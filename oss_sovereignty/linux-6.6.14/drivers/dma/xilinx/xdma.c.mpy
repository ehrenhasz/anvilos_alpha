{
  "module_name": "xdma.c",
  "hash_id": "e683471e286fdc1500c8956783d80c7ce287f2a592dfd6e121e289999e27434f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/xilinx/xdma.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/mod_devicetable.h>\n#include <linux/bitfield.h>\n#include <linux/dmapool.h>\n#include <linux/regmap.h>\n#include <linux/dmaengine.h>\n#include <linux/dma/amd_xdma.h>\n#include <linux/platform_device.h>\n#include <linux/platform_data/amd_xdma.h>\n#include <linux/dma-mapping.h>\n#include <linux/pci.h>\n#include \"../virt-dma.h\"\n#include \"xdma-regs.h\"\n\n \nstatic const struct regmap_config xdma_regmap_config = {\n\t.reg_bits = 32,\n\t.val_bits = 32,\n\t.reg_stride = 4,\n\t.max_register = XDMA_REG_SPACE_LEN,\n};\n\n \nstruct xdma_desc_block {\n\tvoid\t\t*virt_addr;\n\tdma_addr_t\tdma_addr;\n};\n\n \nstruct xdma_chan {\n\tstruct virt_dma_chan\t\tvchan;\n\tvoid\t\t\t\t*xdev_hdl;\n\tu32\t\t\t\tbase;\n\tstruct dma_pool\t\t\t*desc_pool;\n\tbool\t\t\t\tbusy;\n\tenum dma_transfer_direction\tdir;\n\tstruct dma_slave_config\t\tcfg;\n\tu32\t\t\t\tirq;\n};\n\n \nstruct xdma_desc {\n\tstruct virt_dma_desc\t\tvdesc;\n\tstruct xdma_chan\t\t*chan;\n\tenum dma_transfer_direction\tdir;\n\tu64\t\t\t\tdev_addr;\n\tstruct xdma_desc_block\t\t*desc_blocks;\n\tu32\t\t\t\tdblk_num;\n\tu32\t\t\t\tdesc_num;\n\tu32\t\t\t\tcompleted_desc_num;\n};\n\n#define XDMA_DEV_STATUS_REG_DMA\t\tBIT(0)\n#define XDMA_DEV_STATUS_INIT_MSIX\tBIT(1)\n\n \nstruct xdma_device {\n\tstruct platform_device\t*pdev;\n\tstruct dma_device\tdma_dev;\n\tstruct regmap\t\t*rmap;\n\tstruct xdma_chan\t*h2c_chans;\n\tstruct xdma_chan\t*c2h_chans;\n\tu32\t\t\th2c_chan_num;\n\tu32\t\t\tc2h_chan_num;\n\tu32\t\t\tirq_start;\n\tu32\t\t\tirq_num;\n\tu32\t\t\tstatus;\n};\n\n#define xdma_err(xdev, fmt, args...)\t\t\t\t\t\\\n\tdev_err(&(xdev)->pdev->dev, fmt, ##args)\n#define XDMA_CHAN_NUM(_xd) ({\t\t\t\t\t\t\\\n\ttypeof(_xd) (xd) = (_xd);\t\t\t\t\t\\\n\t((xd)->h2c_chan_num + (xd)->c2h_chan_num); })\n\n \nstatic inline void *xdma_blk_last_desc(struct xdma_desc_block *block)\n{\n\treturn block->virt_addr + (XDMA_DESC_ADJACENT - 1) * XDMA_DESC_SIZE;\n}\n\n \nstatic void xdma_link_desc_blocks(struct xdma_desc *sw_desc)\n{\n\tstruct xdma_desc_block *block;\n\tu32 last_blk_desc, desc_control;\n\tstruct xdma_hw_desc *desc;\n\tint i;\n\n\tdesc_control = XDMA_DESC_CONTROL(XDMA_DESC_ADJACENT, 0);\n\tfor (i = 1; i < sw_desc->dblk_num; i++) {\n\t\tblock = &sw_desc->desc_blocks[i - 1];\n\t\tdesc = xdma_blk_last_desc(block);\n\n\t\tif (!(i & XDMA_DESC_BLOCK_MASK)) {\n\t\t\tdesc->control = cpu_to_le32(XDMA_DESC_CONTROL_LAST);\n\t\t\tcontinue;\n\t\t}\n\t\tdesc->control = cpu_to_le32(desc_control);\n\t\tdesc->next_desc = cpu_to_le64(block[1].dma_addr);\n\t}\n\n\t \n\tlast_blk_desc = (sw_desc->desc_num - 1) & XDMA_DESC_ADJACENT_MASK;\n\tif (((sw_desc->dblk_num - 1) & XDMA_DESC_BLOCK_MASK) > 0) {\n\t\tblock = &sw_desc->desc_blocks[sw_desc->dblk_num - 2];\n\t\tdesc = xdma_blk_last_desc(block);\n\t\tdesc_control = XDMA_DESC_CONTROL(last_blk_desc + 1, 0);\n\t\tdesc->control = cpu_to_le32(desc_control);\n\t}\n\n\tblock = &sw_desc->desc_blocks[sw_desc->dblk_num - 1];\n\tdesc = block->virt_addr + last_blk_desc * XDMA_DESC_SIZE;\n\tdesc->control = cpu_to_le32(XDMA_DESC_CONTROL_LAST);\n}\n\nstatic inline struct xdma_chan *to_xdma_chan(struct dma_chan *chan)\n{\n\treturn container_of(chan, struct xdma_chan, vchan.chan);\n}\n\nstatic inline struct xdma_desc *to_xdma_desc(struct virt_dma_desc *vdesc)\n{\n\treturn container_of(vdesc, struct xdma_desc, vdesc);\n}\n\n \nstatic int xdma_channel_init(struct xdma_chan *chan)\n{\n\tstruct xdma_device *xdev = chan->xdev_hdl;\n\tint ret;\n\n\tret = regmap_write(xdev->rmap, chan->base + XDMA_CHAN_CONTROL_W1C,\n\t\t\t   CHAN_CTRL_NON_INCR_ADDR);\n\tif (ret)\n\t\treturn ret;\n\n\tret = regmap_write(xdev->rmap, chan->base + XDMA_CHAN_INTR_ENABLE,\n\t\t\t   CHAN_IM_ALL);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\n \nstatic void xdma_free_desc(struct virt_dma_desc *vdesc)\n{\n\tstruct xdma_desc *sw_desc;\n\tint i;\n\n\tsw_desc = to_xdma_desc(vdesc);\n\tfor (i = 0; i < sw_desc->dblk_num; i++) {\n\t\tif (!sw_desc->desc_blocks[i].virt_addr)\n\t\t\tbreak;\n\t\tdma_pool_free(sw_desc->chan->desc_pool,\n\t\t\t      sw_desc->desc_blocks[i].virt_addr,\n\t\t\t      sw_desc->desc_blocks[i].dma_addr);\n\t}\n\tkfree(sw_desc->desc_blocks);\n\tkfree(sw_desc);\n}\n\n \nstatic struct xdma_desc *\nxdma_alloc_desc(struct xdma_chan *chan, u32 desc_num)\n{\n\tstruct xdma_desc *sw_desc;\n\tstruct xdma_hw_desc *desc;\n\tdma_addr_t dma_addr;\n\tu32 dblk_num;\n\tvoid *addr;\n\tint i, j;\n\n\tsw_desc = kzalloc(sizeof(*sw_desc), GFP_NOWAIT);\n\tif (!sw_desc)\n\t\treturn NULL;\n\n\tsw_desc->chan = chan;\n\tsw_desc->desc_num = desc_num;\n\tdblk_num = DIV_ROUND_UP(desc_num, XDMA_DESC_ADJACENT);\n\tsw_desc->desc_blocks = kcalloc(dblk_num, sizeof(*sw_desc->desc_blocks),\n\t\t\t\t       GFP_NOWAIT);\n\tif (!sw_desc->desc_blocks)\n\t\tgoto failed;\n\n\tsw_desc->dblk_num = dblk_num;\n\tfor (i = 0; i < sw_desc->dblk_num; i++) {\n\t\taddr = dma_pool_alloc(chan->desc_pool, GFP_NOWAIT, &dma_addr);\n\t\tif (!addr)\n\t\t\tgoto failed;\n\n\t\tsw_desc->desc_blocks[i].virt_addr = addr;\n\t\tsw_desc->desc_blocks[i].dma_addr = dma_addr;\n\t\tfor (j = 0, desc = addr; j < XDMA_DESC_ADJACENT; j++)\n\t\t\tdesc[j].control = cpu_to_le32(XDMA_DESC_CONTROL(1, 0));\n\t}\n\n\txdma_link_desc_blocks(sw_desc);\n\n\treturn sw_desc;\n\nfailed:\n\txdma_free_desc(&sw_desc->vdesc);\n\treturn NULL;\n}\n\n \nstatic int xdma_xfer_start(struct xdma_chan *xchan)\n{\n\tstruct virt_dma_desc *vd = vchan_next_desc(&xchan->vchan);\n\tstruct xdma_device *xdev = xchan->xdev_hdl;\n\tstruct xdma_desc_block *block;\n\tu32 val, completed_blocks;\n\tstruct xdma_desc *desc;\n\tint ret;\n\n\t \n\tif (!vd || xchan->busy)\n\t\treturn -EINVAL;\n\n\t \n\tret = regmap_write(xdev->rmap, xchan->base + XDMA_CHAN_CONTROL_W1C,\n\t\t\t   CHAN_CTRL_RUN_STOP);\n\tif (ret)\n\t\treturn ret;\n\n\tdesc = to_xdma_desc(vd);\n\tif (desc->dir != xchan->dir) {\n\t\txdma_err(xdev, \"incorrect request direction\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tcompleted_blocks = desc->completed_desc_num / XDMA_DESC_ADJACENT;\n\tblock = &desc->desc_blocks[completed_blocks];\n\tval = lower_32_bits(block->dma_addr);\n\tret = regmap_write(xdev->rmap, xchan->base + XDMA_SGDMA_DESC_LO, val);\n\tif (ret)\n\t\treturn ret;\n\n\tval = upper_32_bits(block->dma_addr);\n\tret = regmap_write(xdev->rmap, xchan->base + XDMA_SGDMA_DESC_HI, val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (completed_blocks + 1 == desc->dblk_num)\n\t\tval = (desc->desc_num - 1) & XDMA_DESC_ADJACENT_MASK;\n\telse\n\t\tval = XDMA_DESC_ADJACENT - 1;\n\tret = regmap_write(xdev->rmap, xchan->base + XDMA_SGDMA_DESC_ADJ, val);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = regmap_write(xdev->rmap, xchan->base + XDMA_CHAN_CONTROL,\n\t\t\t   CHAN_CTRL_START);\n\tif (ret)\n\t\treturn ret;\n\n\txchan->busy = true;\n\treturn 0;\n}\n\n \nstatic int xdma_alloc_channels(struct xdma_device *xdev,\n\t\t\t       enum dma_transfer_direction dir)\n{\n\tstruct xdma_platdata *pdata = dev_get_platdata(&xdev->pdev->dev);\n\tstruct xdma_chan **chans, *xchan;\n\tu32 base, identifier, target;\n\tu32 *chan_num;\n\tint i, j, ret;\n\n\tif (dir == DMA_MEM_TO_DEV) {\n\t\tbase = XDMA_CHAN_H2C_OFFSET;\n\t\ttarget = XDMA_CHAN_H2C_TARGET;\n\t\tchans = &xdev->h2c_chans;\n\t\tchan_num = &xdev->h2c_chan_num;\n\t} else if (dir == DMA_DEV_TO_MEM) {\n\t\tbase = XDMA_CHAN_C2H_OFFSET;\n\t\ttarget = XDMA_CHAN_C2H_TARGET;\n\t\tchans = &xdev->c2h_chans;\n\t\tchan_num = &xdev->c2h_chan_num;\n\t} else {\n\t\txdma_err(xdev, \"invalid direction specified\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tfor (i = 0, *chan_num = 0; i < pdata->max_dma_channels; i++) {\n\t\tret = regmap_read(xdev->rmap, base + i * XDMA_CHAN_STRIDE,\n\t\t\t\t  &identifier);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tif (XDMA_CHAN_CHECK_TARGET(identifier, target))\n\t\t\t(*chan_num)++;\n\t}\n\n\tif (!*chan_num) {\n\t\txdma_err(xdev, \"does not probe any channel\");\n\t\treturn -EINVAL;\n\t}\n\n\t*chans = devm_kcalloc(&xdev->pdev->dev, *chan_num, sizeof(**chans),\n\t\t\t      GFP_KERNEL);\n\tif (!*chans)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0, j = 0; i < pdata->max_dma_channels; i++) {\n\t\tret = regmap_read(xdev->rmap, base + i * XDMA_CHAN_STRIDE,\n\t\t\t\t  &identifier);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (!XDMA_CHAN_CHECK_TARGET(identifier, target))\n\t\t\tcontinue;\n\n\t\tif (j == *chan_num) {\n\t\t\txdma_err(xdev, \"invalid channel number\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\t \n\t\txchan = &(*chans)[j];\n\t\txchan->xdev_hdl = xdev;\n\t\txchan->base = base + i * XDMA_CHAN_STRIDE;\n\t\txchan->dir = dir;\n\n\t\tret = xdma_channel_init(xchan);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\txchan->vchan.desc_free = xdma_free_desc;\n\t\tvchan_init(&xchan->vchan, &xdev->dma_dev);\n\n\t\tj++;\n\t}\n\n\tdev_info(&xdev->pdev->dev, \"configured %d %s channels\", j,\n\t\t (dir == DMA_MEM_TO_DEV) ? \"H2C\" : \"C2H\");\n\n\treturn 0;\n}\n\n \nstatic void xdma_issue_pending(struct dma_chan *chan)\n{\n\tstruct xdma_chan *xdma_chan = to_xdma_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&xdma_chan->vchan.lock, flags);\n\tif (vchan_issue_pending(&xdma_chan->vchan))\n\t\txdma_xfer_start(xdma_chan);\n\tspin_unlock_irqrestore(&xdma_chan->vchan.lock, flags);\n}\n\n \nstatic struct dma_async_tx_descriptor *\nxdma_prep_device_sg(struct dma_chan *chan, struct scatterlist *sgl,\n\t\t    unsigned int sg_len, enum dma_transfer_direction dir,\n\t\t    unsigned long flags, void *context)\n{\n\tstruct xdma_chan *xdma_chan = to_xdma_chan(chan);\n\tstruct dma_async_tx_descriptor *tx_desc;\n\tu32 desc_num = 0, i, len, rest;\n\tstruct xdma_desc_block *dblk;\n\tstruct xdma_hw_desc *desc;\n\tstruct xdma_desc *sw_desc;\n\tu64 dev_addr, *src, *dst;\n\tstruct scatterlist *sg;\n\tu64 addr;\n\n\tfor_each_sg(sgl, sg, sg_len, i)\n\t\tdesc_num += DIV_ROUND_UP(sg_dma_len(sg), XDMA_DESC_BLEN_MAX);\n\n\tsw_desc = xdma_alloc_desc(xdma_chan, desc_num);\n\tif (!sw_desc)\n\t\treturn NULL;\n\tsw_desc->dir = dir;\n\n\tif (dir == DMA_MEM_TO_DEV) {\n\t\tdev_addr = xdma_chan->cfg.dst_addr;\n\t\tsrc = &addr;\n\t\tdst = &dev_addr;\n\t} else {\n\t\tdev_addr = xdma_chan->cfg.src_addr;\n\t\tsrc = &dev_addr;\n\t\tdst = &addr;\n\t}\n\n\tdblk = sw_desc->desc_blocks;\n\tdesc = dblk->virt_addr;\n\tdesc_num = 1;\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\taddr = sg_dma_address(sg);\n\t\trest = sg_dma_len(sg);\n\n\t\tdo {\n\t\t\tlen = min_t(u32, rest, XDMA_DESC_BLEN_MAX);\n\t\t\t \n\t\t\tdesc->bytes = cpu_to_le32(len);\n\t\t\tdesc->src_addr = cpu_to_le64(*src);\n\t\t\tdesc->dst_addr = cpu_to_le64(*dst);\n\n\t\t\tif (!(desc_num & XDMA_DESC_ADJACENT_MASK)) {\n\t\t\t\tdblk++;\n\t\t\t\tdesc = dblk->virt_addr;\n\t\t\t} else {\n\t\t\t\tdesc++;\n\t\t\t}\n\n\t\t\tdesc_num++;\n\t\t\tdev_addr += len;\n\t\t\taddr += len;\n\t\t\trest -= len;\n\t\t} while (rest);\n\t}\n\n\ttx_desc = vchan_tx_prep(&xdma_chan->vchan, &sw_desc->vdesc, flags);\n\tif (!tx_desc)\n\t\tgoto failed;\n\n\treturn tx_desc;\n\nfailed:\n\txdma_free_desc(&sw_desc->vdesc);\n\n\treturn NULL;\n}\n\n \nstatic int xdma_device_config(struct dma_chan *chan,\n\t\t\t      struct dma_slave_config *cfg)\n{\n\tstruct xdma_chan *xdma_chan = to_xdma_chan(chan);\n\n\tmemcpy(&xdma_chan->cfg, cfg, sizeof(*cfg));\n\n\treturn 0;\n}\n\n \nstatic void xdma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct xdma_chan *xdma_chan = to_xdma_chan(chan);\n\n\tvchan_free_chan_resources(&xdma_chan->vchan);\n\tdma_pool_destroy(xdma_chan->desc_pool);\n\txdma_chan->desc_pool = NULL;\n}\n\n \nstatic int xdma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct xdma_chan *xdma_chan = to_xdma_chan(chan);\n\tstruct xdma_device *xdev = xdma_chan->xdev_hdl;\n\tstruct device *dev = xdev->dma_dev.dev;\n\n\twhile (dev && !dev_is_pci(dev))\n\t\tdev = dev->parent;\n\tif (!dev) {\n\t\txdma_err(xdev, \"unable to find pci device\");\n\t\treturn -EINVAL;\n\t}\n\n\txdma_chan->desc_pool = dma_pool_create(dma_chan_name(chan),\n\t\t\t\t\t       dev, XDMA_DESC_BLOCK_SIZE,\n\t\t\t\t\t       XDMA_DESC_BLOCK_ALIGN, 0);\n\tif (!xdma_chan->desc_pool) {\n\t\txdma_err(xdev, \"unable to allocate descriptor pool\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic irqreturn_t xdma_channel_isr(int irq, void *dev_id)\n{\n\tstruct xdma_chan *xchan = dev_id;\n\tu32 complete_desc_num = 0;\n\tstruct xdma_device *xdev;\n\tstruct virt_dma_desc *vd;\n\tstruct xdma_desc *desc;\n\tint ret;\n\n\tspin_lock(&xchan->vchan.lock);\n\n\t \n\tvd = vchan_next_desc(&xchan->vchan);\n\tif (!vd)\n\t\tgoto out;\n\n\txchan->busy = false;\n\tdesc = to_xdma_desc(vd);\n\txdev = xchan->xdev_hdl;\n\n\tret = regmap_read(xdev->rmap, xchan->base + XDMA_CHAN_COMPLETED_DESC,\n\t\t\t  &complete_desc_num);\n\tif (ret)\n\t\tgoto out;\n\n\tdesc->completed_desc_num += complete_desc_num;\n\t \n\tif (desc->completed_desc_num == desc->desc_num) {\n\t\tlist_del(&vd->node);\n\t\tvchan_cookie_complete(vd);\n\t\tgoto out;\n\t}\n\n\tif (desc->completed_desc_num > desc->desc_num ||\n\t    complete_desc_num != XDMA_DESC_BLOCK_NUM * XDMA_DESC_ADJACENT)\n\t\tgoto out;\n\n\t \n\txdma_xfer_start(xchan);\n\nout:\n\tspin_unlock(&xchan->vchan.lock);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void xdma_irq_fini(struct xdma_device *xdev)\n{\n\tint i;\n\n\t \n\tregmap_write(xdev->rmap, XDMA_IRQ_CHAN_INT_EN_W1C, ~0);\n\n\t \n\tfor (i = 0; i < xdev->h2c_chan_num; i++)\n\t\tfree_irq(xdev->h2c_chans[i].irq, &xdev->h2c_chans[i]);\n\n\tfor (i = 0; i < xdev->c2h_chan_num; i++)\n\t\tfree_irq(xdev->c2h_chans[i].irq, &xdev->c2h_chans[i]);\n}\n\n \nstatic int xdma_set_vector_reg(struct xdma_device *xdev, u32 vec_tbl_start,\n\t\t\t       u32 irq_start, u32 irq_num)\n{\n\tu32 shift, i, val = 0;\n\tint ret;\n\n\t \n\twhile (irq_num > 0) {\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\tshift = XDMA_IRQ_VEC_SHIFT * i;\n\t\t\tval |= irq_start << shift;\n\t\t\tirq_start++;\n\t\t\tirq_num--;\n\t\t\tif (!irq_num)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tret = regmap_write(xdev->rmap, vec_tbl_start, val);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tvec_tbl_start += sizeof(u32);\n\t\tval = 0;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int xdma_irq_init(struct xdma_device *xdev)\n{\n\tu32 irq = xdev->irq_start;\n\tu32 user_irq_start;\n\tint i, j, ret;\n\n\t \n\tif (xdev->irq_num < XDMA_CHAN_NUM(xdev)) {\n\t\txdma_err(xdev, \"not enough irq\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tfor (i = 0; i < xdev->h2c_chan_num; i++) {\n\t\tret = request_irq(irq, xdma_channel_isr, 0,\n\t\t\t\t  \"xdma-h2c-channel\", &xdev->h2c_chans[i]);\n\t\tif (ret) {\n\t\t\txdma_err(xdev, \"H2C channel%d request irq%d failed: %d\",\n\t\t\t\t i, irq, ret);\n\t\t\tgoto failed_init_h2c;\n\t\t}\n\t\txdev->h2c_chans[i].irq = irq;\n\t\tirq++;\n\t}\n\n\t \n\tfor (j = 0; j < xdev->c2h_chan_num; j++) {\n\t\tret = request_irq(irq, xdma_channel_isr, 0,\n\t\t\t\t  \"xdma-c2h-channel\", &xdev->c2h_chans[j]);\n\t\tif (ret) {\n\t\t\txdma_err(xdev, \"C2H channel%d request irq%d failed: %d\",\n\t\t\t\t j, irq, ret);\n\t\t\tgoto failed_init_c2h;\n\t\t}\n\t\txdev->c2h_chans[j].irq = irq;\n\t\tirq++;\n\t}\n\n\t \n\tret = xdma_set_vector_reg(xdev, XDMA_IRQ_CHAN_VEC_NUM, 0,\n\t\t\t\t  XDMA_CHAN_NUM(xdev));\n\tif (ret) {\n\t\txdma_err(xdev, \"failed to set channel vectors: %d\", ret);\n\t\tgoto failed_init_c2h;\n\t}\n\n\t \n\tuser_irq_start = XDMA_CHAN_NUM(xdev);\n\tif (xdev->irq_num > user_irq_start) {\n\t\tret = xdma_set_vector_reg(xdev, XDMA_IRQ_USER_VEC_NUM,\n\t\t\t\t\t  user_irq_start,\n\t\t\t\t\t  xdev->irq_num - user_irq_start);\n\t\tif (ret) {\n\t\t\txdma_err(xdev, \"failed to set user vectors: %d\", ret);\n\t\t\tgoto failed_init_c2h;\n\t\t}\n\t}\n\n\t \n\tret = regmap_write(xdev->rmap, XDMA_IRQ_CHAN_INT_EN_W1S, ~0);\n\tif (ret)\n\t\tgoto failed_init_c2h;\n\n\treturn 0;\n\nfailed_init_c2h:\n\twhile (j--)\n\t\tfree_irq(xdev->c2h_chans[j].irq, &xdev->c2h_chans[j]);\nfailed_init_h2c:\n\twhile (i--)\n\t\tfree_irq(xdev->h2c_chans[i].irq, &xdev->h2c_chans[i]);\n\n\treturn ret;\n}\n\nstatic bool xdma_filter_fn(struct dma_chan *chan, void *param)\n{\n\tstruct xdma_chan *xdma_chan = to_xdma_chan(chan);\n\tstruct xdma_chan_info *chan_info = param;\n\n\treturn chan_info->dir == xdma_chan->dir;\n}\n\n \nvoid xdma_disable_user_irq(struct platform_device *pdev, u32 irq_num)\n{\n\tstruct xdma_device *xdev = platform_get_drvdata(pdev);\n\tu32 index;\n\n\tindex = irq_num - xdev->irq_start;\n\tif (index < XDMA_CHAN_NUM(xdev) || index >= xdev->irq_num) {\n\t\txdma_err(xdev, \"invalid user irq number\");\n\t\treturn;\n\t}\n\tindex -= XDMA_CHAN_NUM(xdev);\n\n\tregmap_write(xdev->rmap, XDMA_IRQ_USER_INT_EN_W1C, 1 << index);\n}\nEXPORT_SYMBOL(xdma_disable_user_irq);\n\n \nint xdma_enable_user_irq(struct platform_device *pdev, u32 irq_num)\n{\n\tstruct xdma_device *xdev = platform_get_drvdata(pdev);\n\tu32 index;\n\tint ret;\n\n\tindex = irq_num - xdev->irq_start;\n\tif (index < XDMA_CHAN_NUM(xdev) || index >= xdev->irq_num) {\n\t\txdma_err(xdev, \"invalid user irq number\");\n\t\treturn -EINVAL;\n\t}\n\tindex -= XDMA_CHAN_NUM(xdev);\n\n\tret = regmap_write(xdev->rmap, XDMA_IRQ_USER_INT_EN_W1S, 1 << index);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(xdma_enable_user_irq);\n\n \nint xdma_get_user_irq(struct platform_device *pdev, u32 user_irq_index)\n{\n\tstruct xdma_device *xdev = platform_get_drvdata(pdev);\n\n\tif (XDMA_CHAN_NUM(xdev) + user_irq_index >= xdev->irq_num) {\n\t\txdma_err(xdev, \"invalid user irq index\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn xdev->irq_start + XDMA_CHAN_NUM(xdev) + user_irq_index;\n}\nEXPORT_SYMBOL(xdma_get_user_irq);\n\n \nstatic int xdma_remove(struct platform_device *pdev)\n{\n\tstruct xdma_device *xdev = platform_get_drvdata(pdev);\n\n\tif (xdev->status & XDMA_DEV_STATUS_INIT_MSIX)\n\t\txdma_irq_fini(xdev);\n\n\tif (xdev->status & XDMA_DEV_STATUS_REG_DMA)\n\t\tdma_async_device_unregister(&xdev->dma_dev);\n\n\treturn 0;\n}\n\n \nstatic int xdma_probe(struct platform_device *pdev)\n{\n\tstruct xdma_platdata *pdata = dev_get_platdata(&pdev->dev);\n\tstruct xdma_device *xdev;\n\tvoid __iomem *reg_base;\n\tstruct resource *res;\n\tint ret = -ENODEV;\n\n\tif (pdata->max_dma_channels > XDMA_MAX_CHANNELS) {\n\t\tdev_err(&pdev->dev, \"invalid max dma channels %d\",\n\t\t\tpdata->max_dma_channels);\n\t\treturn -EINVAL;\n\t}\n\n\txdev = devm_kzalloc(&pdev->dev, sizeof(*xdev), GFP_KERNEL);\n\tif (!xdev)\n\t\treturn -ENOMEM;\n\n\tplatform_set_drvdata(pdev, xdev);\n\txdev->pdev = pdev;\n\n\tres = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\n\tif (!res) {\n\t\txdma_err(xdev, \"failed to get irq resource\");\n\t\tgoto failed;\n\t}\n\txdev->irq_start = res->start;\n\txdev->irq_num = res->end - res->start + 1;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!res) {\n\t\txdma_err(xdev, \"failed to get io resource\");\n\t\tgoto failed;\n\t}\n\n\treg_base = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(reg_base)) {\n\t\txdma_err(xdev, \"ioremap failed\");\n\t\tgoto failed;\n\t}\n\n\txdev->rmap = devm_regmap_init_mmio(&pdev->dev, reg_base,\n\t\t\t\t\t   &xdma_regmap_config);\n\tif (!xdev->rmap) {\n\t\txdma_err(xdev, \"config regmap failed: %d\", ret);\n\t\tgoto failed;\n\t}\n\tINIT_LIST_HEAD(&xdev->dma_dev.channels);\n\n\tret = xdma_alloc_channels(xdev, DMA_MEM_TO_DEV);\n\tif (ret) {\n\t\txdma_err(xdev, \"config H2C channels failed: %d\", ret);\n\t\tgoto failed;\n\t}\n\n\tret = xdma_alloc_channels(xdev, DMA_DEV_TO_MEM);\n\tif (ret) {\n\t\txdma_err(xdev, \"config C2H channels failed: %d\", ret);\n\t\tgoto failed;\n\t}\n\n\tdma_cap_set(DMA_SLAVE, xdev->dma_dev.cap_mask);\n\tdma_cap_set(DMA_PRIVATE, xdev->dma_dev.cap_mask);\n\n\txdev->dma_dev.dev = &pdev->dev;\n\txdev->dma_dev.device_free_chan_resources = xdma_free_chan_resources;\n\txdev->dma_dev.device_alloc_chan_resources = xdma_alloc_chan_resources;\n\txdev->dma_dev.device_tx_status = dma_cookie_status;\n\txdev->dma_dev.device_prep_slave_sg = xdma_prep_device_sg;\n\txdev->dma_dev.device_config = xdma_device_config;\n\txdev->dma_dev.device_issue_pending = xdma_issue_pending;\n\txdev->dma_dev.filter.map = pdata->device_map;\n\txdev->dma_dev.filter.mapcnt = pdata->device_map_cnt;\n\txdev->dma_dev.filter.fn = xdma_filter_fn;\n\n\tret = dma_async_device_register(&xdev->dma_dev);\n\tif (ret) {\n\t\txdma_err(xdev, \"failed to register Xilinx XDMA: %d\", ret);\n\t\tgoto failed;\n\t}\n\txdev->status |= XDMA_DEV_STATUS_REG_DMA;\n\n\tret = xdma_irq_init(xdev);\n\tif (ret) {\n\t\txdma_err(xdev, \"failed to init msix: %d\", ret);\n\t\tgoto failed;\n\t}\n\txdev->status |= XDMA_DEV_STATUS_INIT_MSIX;\n\n\treturn 0;\n\nfailed:\n\txdma_remove(pdev);\n\n\treturn ret;\n}\n\nstatic const struct platform_device_id xdma_id_table[] = {\n\t{ \"xdma\", 0},\n\t{ },\n};\n\nstatic struct platform_driver xdma_driver = {\n\t.driver\t\t= {\n\t\t.name = \"xdma\",\n\t},\n\t.id_table\t= xdma_id_table,\n\t.probe\t\t= xdma_probe,\n\t.remove\t\t= xdma_remove,\n};\n\nmodule_platform_driver(xdma_driver);\n\nMODULE_DESCRIPTION(\"AMD XDMA driver\");\nMODULE_AUTHOR(\"XRT Team <runtimeca39d@amd.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}