{
  "module_name": "xilinx_dma.c",
  "hash_id": "026217bdd1a955e98dd18678b17bebcdb34cc4f0e5175d5abc3a4a08a07c3bc3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/xilinx/xilinx_dma.c",
  "human_readable_source": "\n \n\n#include <linux/bitops.h>\n#include <linux/dmapool.h>\n#include <linux/dma/xilinx_dma.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/iopoll.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n#include <linux/of_irq.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/clk.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n\n#include \"../dmaengine.h\"\n\n \n#define XILINX_DMA_MM2S_CTRL_OFFSET\t\t0x0000\n#define XILINX_DMA_S2MM_CTRL_OFFSET\t\t0x0030\n#define XILINX_VDMA_MM2S_DESC_OFFSET\t\t0x0050\n#define XILINX_VDMA_S2MM_DESC_OFFSET\t\t0x00a0\n\n \n#define XILINX_DMA_REG_DMACR\t\t\t0x0000\n#define XILINX_DMA_DMACR_DELAY_MAX\t\t0xff\n#define XILINX_DMA_DMACR_DELAY_SHIFT\t\t24\n#define XILINX_DMA_DMACR_FRAME_COUNT_MAX\t0xff\n#define XILINX_DMA_DMACR_FRAME_COUNT_SHIFT\t16\n#define XILINX_DMA_DMACR_ERR_IRQ\t\tBIT(14)\n#define XILINX_DMA_DMACR_DLY_CNT_IRQ\t\tBIT(13)\n#define XILINX_DMA_DMACR_FRM_CNT_IRQ\t\tBIT(12)\n#define XILINX_DMA_DMACR_MASTER_SHIFT\t\t8\n#define XILINX_DMA_DMACR_FSYNCSRC_SHIFT\t5\n#define XILINX_DMA_DMACR_FRAMECNT_EN\t\tBIT(4)\n#define XILINX_DMA_DMACR_GENLOCK_EN\t\tBIT(3)\n#define XILINX_DMA_DMACR_RESET\t\t\tBIT(2)\n#define XILINX_DMA_DMACR_CIRC_EN\t\tBIT(1)\n#define XILINX_DMA_DMACR_RUNSTOP\t\tBIT(0)\n#define XILINX_DMA_DMACR_FSYNCSRC_MASK\t\tGENMASK(6, 5)\n#define XILINX_DMA_DMACR_DELAY_MASK\t\tGENMASK(31, 24)\n#define XILINX_DMA_DMACR_FRAME_COUNT_MASK\tGENMASK(23, 16)\n#define XILINX_DMA_DMACR_MASTER_MASK\t\tGENMASK(11, 8)\n\n#define XILINX_DMA_REG_DMASR\t\t\t0x0004\n#define XILINX_DMA_DMASR_EOL_LATE_ERR\t\tBIT(15)\n#define XILINX_DMA_DMASR_ERR_IRQ\t\tBIT(14)\n#define XILINX_DMA_DMASR_DLY_CNT_IRQ\t\tBIT(13)\n#define XILINX_DMA_DMASR_FRM_CNT_IRQ\t\tBIT(12)\n#define XILINX_DMA_DMASR_SOF_LATE_ERR\t\tBIT(11)\n#define XILINX_DMA_DMASR_SG_DEC_ERR\t\tBIT(10)\n#define XILINX_DMA_DMASR_SG_SLV_ERR\t\tBIT(9)\n#define XILINX_DMA_DMASR_EOF_EARLY_ERR\t\tBIT(8)\n#define XILINX_DMA_DMASR_SOF_EARLY_ERR\t\tBIT(7)\n#define XILINX_DMA_DMASR_DMA_DEC_ERR\t\tBIT(6)\n#define XILINX_DMA_DMASR_DMA_SLAVE_ERR\t\tBIT(5)\n#define XILINX_DMA_DMASR_DMA_INT_ERR\t\tBIT(4)\n#define XILINX_DMA_DMASR_SG_MASK\t\tBIT(3)\n#define XILINX_DMA_DMASR_IDLE\t\t\tBIT(1)\n#define XILINX_DMA_DMASR_HALTED\t\tBIT(0)\n#define XILINX_DMA_DMASR_DELAY_MASK\t\tGENMASK(31, 24)\n#define XILINX_DMA_DMASR_FRAME_COUNT_MASK\tGENMASK(23, 16)\n\n#define XILINX_DMA_REG_CURDESC\t\t\t0x0008\n#define XILINX_DMA_REG_TAILDESC\t\t0x0010\n#define XILINX_DMA_REG_REG_INDEX\t\t0x0014\n#define XILINX_DMA_REG_FRMSTORE\t\t0x0018\n#define XILINX_DMA_REG_THRESHOLD\t\t0x001c\n#define XILINX_DMA_REG_FRMPTR_STS\t\t0x0024\n#define XILINX_DMA_REG_PARK_PTR\t\t0x0028\n#define XILINX_DMA_PARK_PTR_WR_REF_SHIFT\t8\n#define XILINX_DMA_PARK_PTR_WR_REF_MASK\t\tGENMASK(12, 8)\n#define XILINX_DMA_PARK_PTR_RD_REF_SHIFT\t0\n#define XILINX_DMA_PARK_PTR_RD_REF_MASK\t\tGENMASK(4, 0)\n#define XILINX_DMA_REG_VDMA_VERSION\t\t0x002c\n\n \n#define XILINX_DMA_REG_VSIZE\t\t\t0x0000\n#define XILINX_DMA_REG_HSIZE\t\t\t0x0004\n\n#define XILINX_DMA_REG_FRMDLY_STRIDE\t\t0x0008\n#define XILINX_DMA_FRMDLY_STRIDE_FRMDLY_SHIFT\t24\n#define XILINX_DMA_FRMDLY_STRIDE_STRIDE_SHIFT\t0\n\n#define XILINX_VDMA_REG_START_ADDRESS(n)\t(0x000c + 4 * (n))\n#define XILINX_VDMA_REG_START_ADDRESS_64(n)\t(0x000c + 8 * (n))\n\n#define XILINX_VDMA_REG_ENABLE_VERTICAL_FLIP\t0x00ec\n#define XILINX_VDMA_ENABLE_VERTICAL_FLIP\tBIT(0)\n\n \n#define XILINX_MCDMA_MAX_CHANS_PER_DEVICE\t0x20\n#define XILINX_DMA_MAX_CHANS_PER_DEVICE\t\t0x2\n#define XILINX_CDMA_MAX_CHANS_PER_DEVICE\t0x1\n\n#define XILINX_DMA_DMAXR_ALL_IRQ_MASK\t\\\n\t\t(XILINX_DMA_DMASR_FRM_CNT_IRQ | \\\n\t\t XILINX_DMA_DMASR_DLY_CNT_IRQ | \\\n\t\t XILINX_DMA_DMASR_ERR_IRQ)\n\n#define XILINX_DMA_DMASR_ALL_ERR_MASK\t\\\n\t\t(XILINX_DMA_DMASR_EOL_LATE_ERR | \\\n\t\t XILINX_DMA_DMASR_SOF_LATE_ERR | \\\n\t\t XILINX_DMA_DMASR_SG_DEC_ERR | \\\n\t\t XILINX_DMA_DMASR_SG_SLV_ERR | \\\n\t\t XILINX_DMA_DMASR_EOF_EARLY_ERR | \\\n\t\t XILINX_DMA_DMASR_SOF_EARLY_ERR | \\\n\t\t XILINX_DMA_DMASR_DMA_DEC_ERR | \\\n\t\t XILINX_DMA_DMASR_DMA_SLAVE_ERR | \\\n\t\t XILINX_DMA_DMASR_DMA_INT_ERR)\n\n \n#define XILINX_DMA_DMASR_ERR_RECOVER_MASK\t\\\n\t\t(XILINX_DMA_DMASR_SOF_LATE_ERR | \\\n\t\t XILINX_DMA_DMASR_EOF_EARLY_ERR | \\\n\t\t XILINX_DMA_DMASR_SOF_EARLY_ERR | \\\n\t\t XILINX_DMA_DMASR_DMA_INT_ERR)\n\n \n#define XILINX_DMA_FLUSH_S2MM\t\t3\n#define XILINX_DMA_FLUSH_MM2S\t\t2\n#define XILINX_DMA_FLUSH_BOTH\t\t1\n\n \n#define XILINX_DMA_LOOP_COUNT\t\t1000000\n\n \n#define XILINX_DMA_REG_SRCDSTADDR\t0x18\n#define XILINX_DMA_REG_BTT\t\t0x28\n\n \n#define XILINX_DMA_MAX_TRANS_LEN_MIN\t8\n#define XILINX_DMA_MAX_TRANS_LEN_MAX\t23\n#define XILINX_DMA_V2_MAX_TRANS_LEN_MAX\t26\n#define XILINX_DMA_CR_COALESCE_MAX\tGENMASK(23, 16)\n#define XILINX_DMA_CR_DELAY_MAX\t\tGENMASK(31, 24)\n#define XILINX_DMA_CR_CYCLIC_BD_EN_MASK\tBIT(4)\n#define XILINX_DMA_CR_COALESCE_SHIFT\t16\n#define XILINX_DMA_CR_DELAY_SHIFT\t24\n#define XILINX_DMA_BD_SOP\t\tBIT(27)\n#define XILINX_DMA_BD_EOP\t\tBIT(26)\n#define XILINX_DMA_BD_COMP_MASK\t\tBIT(31)\n#define XILINX_DMA_COALESCE_MAX\t\t255\n#define XILINX_DMA_NUM_DESCS\t\t512\n#define XILINX_DMA_NUM_APP_WORDS\t5\n\n \n#define XILINX_CDMA_REG_SRCADDR\t\t0x18\n#define XILINX_CDMA_REG_DSTADDR\t\t0x20\n\n \n#define XILINX_CDMA_CR_SGMODE          BIT(3)\n\n#define xilinx_prep_dma_addr_t(addr)\t\\\n\t((dma_addr_t)((u64)addr##_##msb << 32 | (addr)))\n\n \n#define XILINX_MCDMA_MM2S_CTRL_OFFSET\t\t0x0000\n#define XILINX_MCDMA_S2MM_CTRL_OFFSET\t\t0x0500\n#define XILINX_MCDMA_CHEN_OFFSET\t\t0x0008\n#define XILINX_MCDMA_CH_ERR_OFFSET\t\t0x0010\n#define XILINX_MCDMA_RXINT_SER_OFFSET\t\t0x0020\n#define XILINX_MCDMA_TXINT_SER_OFFSET\t\t0x0028\n#define XILINX_MCDMA_CHAN_CR_OFFSET(x)\t\t(0x40 + (x) * 0x40)\n#define XILINX_MCDMA_CHAN_SR_OFFSET(x)\t\t(0x44 + (x) * 0x40)\n#define XILINX_MCDMA_CHAN_CDESC_OFFSET(x)\t(0x48 + (x) * 0x40)\n#define XILINX_MCDMA_CHAN_TDESC_OFFSET(x)\t(0x50 + (x) * 0x40)\n\n \n#define XILINX_MCDMA_COALESCE_SHIFT\t\t16\n#define XILINX_MCDMA_COALESCE_MAX\t\t24\n#define XILINX_MCDMA_IRQ_ALL_MASK\t\tGENMASK(7, 5)\n#define XILINX_MCDMA_COALESCE_MASK\t\tGENMASK(23, 16)\n#define XILINX_MCDMA_CR_RUNSTOP_MASK\t\tBIT(0)\n#define XILINX_MCDMA_IRQ_IOC_MASK\t\tBIT(5)\n#define XILINX_MCDMA_IRQ_DELAY_MASK\t\tBIT(6)\n#define XILINX_MCDMA_IRQ_ERR_MASK\t\tBIT(7)\n#define XILINX_MCDMA_BD_EOP\t\t\tBIT(30)\n#define XILINX_MCDMA_BD_SOP\t\t\tBIT(31)\n\n \nstruct xilinx_vdma_desc_hw {\n\tu32 next_desc;\n\tu32 pad1;\n\tu32 buf_addr;\n\tu32 buf_addr_msb;\n\tu32 vsize;\n\tu32 hsize;\n\tu32 stride;\n} __aligned(64);\n\n \nstruct xilinx_axidma_desc_hw {\n\tu32 next_desc;\n\tu32 next_desc_msb;\n\tu32 buf_addr;\n\tu32 buf_addr_msb;\n\tu32 reserved1;\n\tu32 reserved2;\n\tu32 control;\n\tu32 status;\n\tu32 app[XILINX_DMA_NUM_APP_WORDS];\n} __aligned(64);\n\n \nstruct xilinx_aximcdma_desc_hw {\n\tu32 next_desc;\n\tu32 next_desc_msb;\n\tu32 buf_addr;\n\tu32 buf_addr_msb;\n\tu32 rsvd;\n\tu32 control;\n\tu32 status;\n\tu32 sideband_status;\n\tu32 app[XILINX_DMA_NUM_APP_WORDS];\n} __aligned(64);\n\n \nstruct xilinx_cdma_desc_hw {\n\tu32 next_desc;\n\tu32 next_desc_msb;\n\tu32 src_addr;\n\tu32 src_addr_msb;\n\tu32 dest_addr;\n\tu32 dest_addr_msb;\n\tu32 control;\n\tu32 status;\n} __aligned(64);\n\n \nstruct xilinx_vdma_tx_segment {\n\tstruct xilinx_vdma_desc_hw hw;\n\tstruct list_head node;\n\tdma_addr_t phys;\n} __aligned(64);\n\n \nstruct xilinx_axidma_tx_segment {\n\tstruct xilinx_axidma_desc_hw hw;\n\tstruct list_head node;\n\tdma_addr_t phys;\n} __aligned(64);\n\n \nstruct xilinx_aximcdma_tx_segment {\n\tstruct xilinx_aximcdma_desc_hw hw;\n\tstruct list_head node;\n\tdma_addr_t phys;\n} __aligned(64);\n\n \nstruct xilinx_cdma_tx_segment {\n\tstruct xilinx_cdma_desc_hw hw;\n\tstruct list_head node;\n\tdma_addr_t phys;\n} __aligned(64);\n\n \nstruct xilinx_dma_tx_descriptor {\n\tstruct dma_async_tx_descriptor async_tx;\n\tstruct list_head segments;\n\tstruct list_head node;\n\tbool cyclic;\n\tbool err;\n\tu32 residue;\n};\n\n \nstruct xilinx_dma_chan {\n\tstruct xilinx_dma_device *xdev;\n\tu32 ctrl_offset;\n\tu32 desc_offset;\n\tspinlock_t lock;\n\tstruct list_head pending_list;\n\tstruct list_head active_list;\n\tstruct list_head done_list;\n\tstruct list_head free_seg_list;\n\tstruct dma_chan common;\n\tstruct dma_pool *desc_pool;\n\tstruct device *dev;\n\tint irq;\n\tint id;\n\tenum dma_transfer_direction direction;\n\tint num_frms;\n\tbool has_sg;\n\tbool cyclic;\n\tbool genlock;\n\tbool err;\n\tbool idle;\n\tbool terminating;\n\tstruct tasklet_struct tasklet;\n\tstruct xilinx_vdma_config config;\n\tbool flush_on_fsync;\n\tu32 desc_pendingcount;\n\tbool ext_addr;\n\tu32 desc_submitcount;\n\tstruct xilinx_axidma_tx_segment *seg_v;\n\tstruct xilinx_aximcdma_tx_segment *seg_mv;\n\tdma_addr_t seg_p;\n\tstruct xilinx_axidma_tx_segment *cyclic_seg_v;\n\tdma_addr_t cyclic_seg_p;\n\tvoid (*start_transfer)(struct xilinx_dma_chan *chan);\n\tint (*stop_transfer)(struct xilinx_dma_chan *chan);\n\tu16 tdest;\n\tbool has_vflip;\n\tu8 irq_delay;\n};\n\n \nenum xdma_ip_type {\n\tXDMA_TYPE_AXIDMA = 0,\n\tXDMA_TYPE_CDMA,\n\tXDMA_TYPE_VDMA,\n\tXDMA_TYPE_AXIMCDMA\n};\n\nstruct xilinx_dma_config {\n\tenum xdma_ip_type dmatype;\n\tint (*clk_init)(struct platform_device *pdev, struct clk **axi_clk,\n\t\t\tstruct clk **tx_clk, struct clk **txs_clk,\n\t\t\tstruct clk **rx_clk, struct clk **rxs_clk);\n\tirqreturn_t (*irq_handler)(int irq, void *data);\n\tconst int max_channels;\n};\n\n \nstruct xilinx_dma_device {\n\tvoid __iomem *regs;\n\tstruct device *dev;\n\tstruct dma_device common;\n\tstruct xilinx_dma_chan *chan[XILINX_MCDMA_MAX_CHANS_PER_DEVICE];\n\tu32 flush_on_fsync;\n\tbool ext_addr;\n\tstruct platform_device  *pdev;\n\tconst struct xilinx_dma_config *dma_config;\n\tstruct clk *axi_clk;\n\tstruct clk *tx_clk;\n\tstruct clk *txs_clk;\n\tstruct clk *rx_clk;\n\tstruct clk *rxs_clk;\n\tu32 s2mm_chan_id;\n\tu32 mm2s_chan_id;\n\tu32 max_buffer_len;\n\tbool has_axistream_connected;\n};\n\n \n#define to_xilinx_chan(chan) \\\n\tcontainer_of(chan, struct xilinx_dma_chan, common)\n#define to_dma_tx_descriptor(tx) \\\n\tcontainer_of(tx, struct xilinx_dma_tx_descriptor, async_tx)\n#define xilinx_dma_poll_timeout(chan, reg, val, cond, delay_us, timeout_us) \\\n\treadl_poll_timeout_atomic(chan->xdev->regs + chan->ctrl_offset + reg, \\\n\t\t\t\t  val, cond, delay_us, timeout_us)\n\n \nstatic inline u32 dma_read(struct xilinx_dma_chan *chan, u32 reg)\n{\n\treturn ioread32(chan->xdev->regs + reg);\n}\n\nstatic inline void dma_write(struct xilinx_dma_chan *chan, u32 reg, u32 value)\n{\n\tiowrite32(value, chan->xdev->regs + reg);\n}\n\nstatic inline void vdma_desc_write(struct xilinx_dma_chan *chan, u32 reg,\n\t\t\t\t   u32 value)\n{\n\tdma_write(chan, chan->desc_offset + reg, value);\n}\n\nstatic inline u32 dma_ctrl_read(struct xilinx_dma_chan *chan, u32 reg)\n{\n\treturn dma_read(chan, chan->ctrl_offset + reg);\n}\n\nstatic inline void dma_ctrl_write(struct xilinx_dma_chan *chan, u32 reg,\n\t\t\t\t   u32 value)\n{\n\tdma_write(chan, chan->ctrl_offset + reg, value);\n}\n\nstatic inline void dma_ctrl_clr(struct xilinx_dma_chan *chan, u32 reg,\n\t\t\t\t u32 clr)\n{\n\tdma_ctrl_write(chan, reg, dma_ctrl_read(chan, reg) & ~clr);\n}\n\nstatic inline void dma_ctrl_set(struct xilinx_dma_chan *chan, u32 reg,\n\t\t\t\t u32 set)\n{\n\tdma_ctrl_write(chan, reg, dma_ctrl_read(chan, reg) | set);\n}\n\n \nstatic inline void vdma_desc_write_64(struct xilinx_dma_chan *chan, u32 reg,\n\t\t\t\t      u32 value_lsb, u32 value_msb)\n{\n\t \n\twritel(value_lsb, chan->xdev->regs + chan->desc_offset + reg);\n\n\t \n\twritel(value_msb, chan->xdev->regs + chan->desc_offset + reg + 4);\n}\n\nstatic inline void dma_writeq(struct xilinx_dma_chan *chan, u32 reg, u64 value)\n{\n\tlo_hi_writeq(value, chan->xdev->regs + chan->ctrl_offset + reg);\n}\n\nstatic inline void xilinx_write(struct xilinx_dma_chan *chan, u32 reg,\n\t\t\t\tdma_addr_t addr)\n{\n\tif (chan->ext_addr)\n\t\tdma_writeq(chan, reg, addr);\n\telse\n\t\tdma_ctrl_write(chan, reg, addr);\n}\n\nstatic inline void xilinx_axidma_buf(struct xilinx_dma_chan *chan,\n\t\t\t\t     struct xilinx_axidma_desc_hw *hw,\n\t\t\t\t     dma_addr_t buf_addr, size_t sg_used,\n\t\t\t\t     size_t period_len)\n{\n\tif (chan->ext_addr) {\n\t\thw->buf_addr = lower_32_bits(buf_addr + sg_used + period_len);\n\t\thw->buf_addr_msb = upper_32_bits(buf_addr + sg_used +\n\t\t\t\t\t\t period_len);\n\t} else {\n\t\thw->buf_addr = buf_addr + sg_used + period_len;\n\t}\n}\n\nstatic inline void xilinx_aximcdma_buf(struct xilinx_dma_chan *chan,\n\t\t\t\t       struct xilinx_aximcdma_desc_hw *hw,\n\t\t\t\t       dma_addr_t buf_addr, size_t sg_used)\n{\n\tif (chan->ext_addr) {\n\t\thw->buf_addr = lower_32_bits(buf_addr + sg_used);\n\t\thw->buf_addr_msb = upper_32_bits(buf_addr + sg_used);\n\t} else {\n\t\thw->buf_addr = buf_addr + sg_used;\n\t}\n}\n\n \nstatic void *xilinx_dma_get_metadata_ptr(struct dma_async_tx_descriptor *tx,\n\t\t\t\t\t size_t *payload_len, size_t *max_len)\n{\n\tstruct xilinx_dma_tx_descriptor *desc = to_dma_tx_descriptor(tx);\n\tstruct xilinx_axidma_tx_segment *seg;\n\n\t*max_len = *payload_len = sizeof(u32) * XILINX_DMA_NUM_APP_WORDS;\n\tseg = list_first_entry(&desc->segments,\n\t\t\t       struct xilinx_axidma_tx_segment, node);\n\treturn seg->hw.app;\n}\n\nstatic struct dma_descriptor_metadata_ops xilinx_dma_metadata_ops = {\n\t.get_ptr = xilinx_dma_get_metadata_ptr,\n};\n\n \n\n \nstatic struct xilinx_vdma_tx_segment *\nxilinx_vdma_alloc_tx_segment(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_vdma_tx_segment *segment;\n\tdma_addr_t phys;\n\n\tsegment = dma_pool_zalloc(chan->desc_pool, GFP_ATOMIC, &phys);\n\tif (!segment)\n\t\treturn NULL;\n\n\tsegment->phys = phys;\n\n\treturn segment;\n}\n\n \nstatic struct xilinx_cdma_tx_segment *\nxilinx_cdma_alloc_tx_segment(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_cdma_tx_segment *segment;\n\tdma_addr_t phys;\n\n\tsegment = dma_pool_zalloc(chan->desc_pool, GFP_ATOMIC, &phys);\n\tif (!segment)\n\t\treturn NULL;\n\n\tsegment->phys = phys;\n\n\treturn segment;\n}\n\n \nstatic struct xilinx_axidma_tx_segment *\nxilinx_axidma_alloc_tx_segment(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_axidma_tx_segment *segment = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tif (!list_empty(&chan->free_seg_list)) {\n\t\tsegment = list_first_entry(&chan->free_seg_list,\n\t\t\t\t\t   struct xilinx_axidma_tx_segment,\n\t\t\t\t\t   node);\n\t\tlist_del(&segment->node);\n\t}\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\tif (!segment)\n\t\tdev_dbg(chan->dev, \"Could not find free tx segment\\n\");\n\n\treturn segment;\n}\n\n \nstatic struct xilinx_aximcdma_tx_segment *\nxilinx_aximcdma_alloc_tx_segment(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_aximcdma_tx_segment *segment = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tif (!list_empty(&chan->free_seg_list)) {\n\t\tsegment = list_first_entry(&chan->free_seg_list,\n\t\t\t\t\t   struct xilinx_aximcdma_tx_segment,\n\t\t\t\t\t   node);\n\t\tlist_del(&segment->node);\n\t}\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn segment;\n}\n\nstatic void xilinx_dma_clean_hw_desc(struct xilinx_axidma_desc_hw *hw)\n{\n\tu32 next_desc = hw->next_desc;\n\tu32 next_desc_msb = hw->next_desc_msb;\n\n\tmemset(hw, 0, sizeof(struct xilinx_axidma_desc_hw));\n\n\thw->next_desc = next_desc;\n\thw->next_desc_msb = next_desc_msb;\n}\n\nstatic void xilinx_mcdma_clean_hw_desc(struct xilinx_aximcdma_desc_hw *hw)\n{\n\tu32 next_desc = hw->next_desc;\n\tu32 next_desc_msb = hw->next_desc_msb;\n\n\tmemset(hw, 0, sizeof(struct xilinx_aximcdma_desc_hw));\n\n\thw->next_desc = next_desc;\n\thw->next_desc_msb = next_desc_msb;\n}\n\n \nstatic void xilinx_dma_free_tx_segment(struct xilinx_dma_chan *chan,\n\t\t\t\tstruct xilinx_axidma_tx_segment *segment)\n{\n\txilinx_dma_clean_hw_desc(&segment->hw);\n\n\tlist_add_tail(&segment->node, &chan->free_seg_list);\n}\n\n \nstatic void xilinx_mcdma_free_tx_segment(struct xilinx_dma_chan *chan,\n\t\t\t\t\t struct xilinx_aximcdma_tx_segment *\n\t\t\t\t\t segment)\n{\n\txilinx_mcdma_clean_hw_desc(&segment->hw);\n\n\tlist_add_tail(&segment->node, &chan->free_seg_list);\n}\n\n \nstatic void xilinx_cdma_free_tx_segment(struct xilinx_dma_chan *chan,\n\t\t\t\tstruct xilinx_cdma_tx_segment *segment)\n{\n\tdma_pool_free(chan->desc_pool, segment, segment->phys);\n}\n\n \nstatic void xilinx_vdma_free_tx_segment(struct xilinx_dma_chan *chan,\n\t\t\t\t\tstruct xilinx_vdma_tx_segment *segment)\n{\n\tdma_pool_free(chan->desc_pool, segment, segment->phys);\n}\n\n \nstatic struct xilinx_dma_tx_descriptor *\nxilinx_dma_alloc_tx_descriptor(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_dma_tx_descriptor *desc;\n\n\tdesc = kzalloc(sizeof(*desc), GFP_NOWAIT);\n\tif (!desc)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&desc->segments);\n\n\treturn desc;\n}\n\n \nstatic void\nxilinx_dma_free_tx_descriptor(struct xilinx_dma_chan *chan,\n\t\t\t       struct xilinx_dma_tx_descriptor *desc)\n{\n\tstruct xilinx_vdma_tx_segment *segment, *next;\n\tstruct xilinx_cdma_tx_segment *cdma_segment, *cdma_next;\n\tstruct xilinx_axidma_tx_segment *axidma_segment, *axidma_next;\n\tstruct xilinx_aximcdma_tx_segment *aximcdma_segment, *aximcdma_next;\n\n\tif (!desc)\n\t\treturn;\n\n\tif (chan->xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\n\t\tlist_for_each_entry_safe(segment, next, &desc->segments, node) {\n\t\t\tlist_del(&segment->node);\n\t\t\txilinx_vdma_free_tx_segment(chan, segment);\n\t\t}\n\t} else if (chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\n\t\tlist_for_each_entry_safe(cdma_segment, cdma_next,\n\t\t\t\t\t &desc->segments, node) {\n\t\t\tlist_del(&cdma_segment->node);\n\t\t\txilinx_cdma_free_tx_segment(chan, cdma_segment);\n\t\t}\n\t} else if (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\n\t\tlist_for_each_entry_safe(axidma_segment, axidma_next,\n\t\t\t\t\t &desc->segments, node) {\n\t\t\tlist_del(&axidma_segment->node);\n\t\t\txilinx_dma_free_tx_segment(chan, axidma_segment);\n\t\t}\n\t} else {\n\t\tlist_for_each_entry_safe(aximcdma_segment, aximcdma_next,\n\t\t\t\t\t &desc->segments, node) {\n\t\t\tlist_del(&aximcdma_segment->node);\n\t\t\txilinx_mcdma_free_tx_segment(chan, aximcdma_segment);\n\t\t}\n\t}\n\n\tkfree(desc);\n}\n\n \n\n \nstatic void xilinx_dma_free_desc_list(struct xilinx_dma_chan *chan,\n\t\t\t\t\tstruct list_head *list)\n{\n\tstruct xilinx_dma_tx_descriptor *desc, *next;\n\n\tlist_for_each_entry_safe(desc, next, list, node) {\n\t\tlist_del(&desc->node);\n\t\txilinx_dma_free_tx_descriptor(chan, desc);\n\t}\n}\n\n \nstatic void xilinx_dma_free_descriptors(struct xilinx_dma_chan *chan)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\txilinx_dma_free_desc_list(chan, &chan->pending_list);\n\txilinx_dma_free_desc_list(chan, &chan->done_list);\n\txilinx_dma_free_desc_list(chan, &chan->active_list);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\n \nstatic void xilinx_dma_free_chan_resources(struct dma_chan *dchan)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tunsigned long flags;\n\n\tdev_dbg(chan->dev, \"Free all channel resources.\\n\");\n\n\txilinx_dma_free_descriptors(chan);\n\n\tif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\n\t\tspin_lock_irqsave(&chan->lock, flags);\n\t\tINIT_LIST_HEAD(&chan->free_seg_list);\n\t\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\t\t \n\t\tdma_free_coherent(chan->dev, sizeof(*chan->seg_v) *\n\t\t\t\t  XILINX_DMA_NUM_DESCS, chan->seg_v,\n\t\t\t\t  chan->seg_p);\n\n\t\t \n\t\tdma_free_coherent(chan->dev, sizeof(*chan->cyclic_seg_v),\n\t\t\t\t  chan->cyclic_seg_v, chan->cyclic_seg_p);\n\t}\n\n\tif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA) {\n\t\tspin_lock_irqsave(&chan->lock, flags);\n\t\tINIT_LIST_HEAD(&chan->free_seg_list);\n\t\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\t\t \n\t\tdma_free_coherent(chan->dev, sizeof(*chan->seg_mv) *\n\t\t\t\t  XILINX_DMA_NUM_DESCS, chan->seg_mv,\n\t\t\t\t  chan->seg_p);\n\t}\n\n\tif (chan->xdev->dma_config->dmatype != XDMA_TYPE_AXIDMA &&\n\t    chan->xdev->dma_config->dmatype != XDMA_TYPE_AXIMCDMA) {\n\t\tdma_pool_destroy(chan->desc_pool);\n\t\tchan->desc_pool = NULL;\n\t}\n\n}\n\n \nstatic u32 xilinx_dma_get_residue(struct xilinx_dma_chan *chan,\n\t\t\t\t  struct xilinx_dma_tx_descriptor *desc)\n{\n\tstruct xilinx_cdma_tx_segment *cdma_seg;\n\tstruct xilinx_axidma_tx_segment *axidma_seg;\n\tstruct xilinx_aximcdma_tx_segment *aximcdma_seg;\n\tstruct xilinx_cdma_desc_hw *cdma_hw;\n\tstruct xilinx_axidma_desc_hw *axidma_hw;\n\tstruct xilinx_aximcdma_desc_hw *aximcdma_hw;\n\tstruct list_head *entry;\n\tu32 residue = 0;\n\n\tlist_for_each(entry, &desc->segments) {\n\t\tif (chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\n\t\t\tcdma_seg = list_entry(entry,\n\t\t\t\t\t      struct xilinx_cdma_tx_segment,\n\t\t\t\t\t      node);\n\t\t\tcdma_hw = &cdma_seg->hw;\n\t\t\tresidue += (cdma_hw->control - cdma_hw->status) &\n\t\t\t\t   chan->xdev->max_buffer_len;\n\t\t} else if (chan->xdev->dma_config->dmatype ==\n\t\t\t   XDMA_TYPE_AXIDMA) {\n\t\t\taxidma_seg = list_entry(entry,\n\t\t\t\t\t\tstruct xilinx_axidma_tx_segment,\n\t\t\t\t\t\tnode);\n\t\t\taxidma_hw = &axidma_seg->hw;\n\t\t\tresidue += (axidma_hw->control - axidma_hw->status) &\n\t\t\t\t   chan->xdev->max_buffer_len;\n\t\t} else {\n\t\t\taximcdma_seg =\n\t\t\t\tlist_entry(entry,\n\t\t\t\t\t   struct xilinx_aximcdma_tx_segment,\n\t\t\t\t\t   node);\n\t\t\taximcdma_hw = &aximcdma_seg->hw;\n\t\t\tresidue +=\n\t\t\t\t(aximcdma_hw->control - aximcdma_hw->status) &\n\t\t\t\tchan->xdev->max_buffer_len;\n\t\t}\n\t}\n\n\treturn residue;\n}\n\n \nstatic void xilinx_dma_chan_handle_cyclic(struct xilinx_dma_chan *chan,\n\t\t\t\t\t  struct xilinx_dma_tx_descriptor *desc,\n\t\t\t\t\t  unsigned long *flags)\n{\n\tstruct dmaengine_desc_callback cb;\n\n\tdmaengine_desc_get_callback(&desc->async_tx, &cb);\n\tif (dmaengine_desc_callback_valid(&cb)) {\n\t\tspin_unlock_irqrestore(&chan->lock, *flags);\n\t\tdmaengine_desc_callback_invoke(&cb, NULL);\n\t\tspin_lock_irqsave(&chan->lock, *flags);\n\t}\n}\n\n \nstatic void xilinx_dma_chan_desc_cleanup(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_dma_tx_descriptor *desc, *next;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tlist_for_each_entry_safe(desc, next, &chan->done_list, node) {\n\t\tstruct dmaengine_result result;\n\n\t\tif (desc->cyclic) {\n\t\t\txilinx_dma_chan_handle_cyclic(chan, desc, &flags);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tlist_del(&desc->node);\n\n\t\tif (unlikely(desc->err)) {\n\t\t\tif (chan->direction == DMA_DEV_TO_MEM)\n\t\t\t\tresult.result = DMA_TRANS_READ_FAILED;\n\t\t\telse\n\t\t\t\tresult.result = DMA_TRANS_WRITE_FAILED;\n\t\t} else {\n\t\t\tresult.result = DMA_TRANS_NOERROR;\n\t\t}\n\n\t\tresult.residue = desc->residue;\n\n\t\t \n\t\tspin_unlock_irqrestore(&chan->lock, flags);\n\t\tdmaengine_desc_get_callback_invoke(&desc->async_tx, &result);\n\t\tspin_lock_irqsave(&chan->lock, flags);\n\n\t\t \n\t\tdma_run_dependencies(&desc->async_tx);\n\t\txilinx_dma_free_tx_descriptor(chan, desc);\n\n\t\t \n\t\tif (chan->terminating)\n\t\t\tbreak;\n\t}\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\n \nstatic void xilinx_dma_do_tasklet(struct tasklet_struct *t)\n{\n\tstruct xilinx_dma_chan *chan = from_tasklet(chan, t, tasklet);\n\n\txilinx_dma_chan_desc_cleanup(chan);\n}\n\n \nstatic int xilinx_dma_alloc_chan_resources(struct dma_chan *dchan)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tint i;\n\n\t \n\tif (chan->desc_pool)\n\t\treturn 0;\n\n\t \n\tif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\n\t\t \n\t\tchan->seg_v = dma_alloc_coherent(chan->dev,\n\t\t\t\t\t\t sizeof(*chan->seg_v) * XILINX_DMA_NUM_DESCS,\n\t\t\t\t\t\t &chan->seg_p, GFP_KERNEL);\n\t\tif (!chan->seg_v) {\n\t\t\tdev_err(chan->dev,\n\t\t\t\t\"unable to allocate channel %d descriptors\\n\",\n\t\t\t\tchan->id);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\t \n\t\tchan->cyclic_seg_v = dma_alloc_coherent(chan->dev,\n\t\t\t\t\t\t\tsizeof(*chan->cyclic_seg_v),\n\t\t\t\t\t\t\t&chan->cyclic_seg_p,\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!chan->cyclic_seg_v) {\n\t\t\tdev_err(chan->dev,\n\t\t\t\t\"unable to allocate desc segment for cyclic DMA\\n\");\n\t\t\tdma_free_coherent(chan->dev, sizeof(*chan->seg_v) *\n\t\t\t\tXILINX_DMA_NUM_DESCS, chan->seg_v,\n\t\t\t\tchan->seg_p);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tchan->cyclic_seg_v->phys = chan->cyclic_seg_p;\n\n\t\tfor (i = 0; i < XILINX_DMA_NUM_DESCS; i++) {\n\t\t\tchan->seg_v[i].hw.next_desc =\n\t\t\tlower_32_bits(chan->seg_p + sizeof(*chan->seg_v) *\n\t\t\t\t((i + 1) % XILINX_DMA_NUM_DESCS));\n\t\t\tchan->seg_v[i].hw.next_desc_msb =\n\t\t\tupper_32_bits(chan->seg_p + sizeof(*chan->seg_v) *\n\t\t\t\t((i + 1) % XILINX_DMA_NUM_DESCS));\n\t\t\tchan->seg_v[i].phys = chan->seg_p +\n\t\t\t\tsizeof(*chan->seg_v) * i;\n\t\t\tlist_add_tail(&chan->seg_v[i].node,\n\t\t\t\t      &chan->free_seg_list);\n\t\t}\n\t} else if (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA) {\n\t\t \n\t\tchan->seg_mv = dma_alloc_coherent(chan->dev,\n\t\t\t\t\t\t  sizeof(*chan->seg_mv) *\n\t\t\t\t\t\t  XILINX_DMA_NUM_DESCS,\n\t\t\t\t\t\t  &chan->seg_p, GFP_KERNEL);\n\t\tif (!chan->seg_mv) {\n\t\t\tdev_err(chan->dev,\n\t\t\t\t\"unable to allocate channel %d descriptors\\n\",\n\t\t\t\tchan->id);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tfor (i = 0; i < XILINX_DMA_NUM_DESCS; i++) {\n\t\t\tchan->seg_mv[i].hw.next_desc =\n\t\t\tlower_32_bits(chan->seg_p + sizeof(*chan->seg_mv) *\n\t\t\t\t((i + 1) % XILINX_DMA_NUM_DESCS));\n\t\t\tchan->seg_mv[i].hw.next_desc_msb =\n\t\t\tupper_32_bits(chan->seg_p + sizeof(*chan->seg_mv) *\n\t\t\t\t((i + 1) % XILINX_DMA_NUM_DESCS));\n\t\t\tchan->seg_mv[i].phys = chan->seg_p +\n\t\t\t\tsizeof(*chan->seg_mv) * i;\n\t\t\tlist_add_tail(&chan->seg_mv[i].node,\n\t\t\t\t      &chan->free_seg_list);\n\t\t}\n\t} else if (chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\n\t\tchan->desc_pool = dma_pool_create(\"xilinx_cdma_desc_pool\",\n\t\t\t\t   chan->dev,\n\t\t\t\t   sizeof(struct xilinx_cdma_tx_segment),\n\t\t\t\t   __alignof__(struct xilinx_cdma_tx_segment),\n\t\t\t\t   0);\n\t} else {\n\t\tchan->desc_pool = dma_pool_create(\"xilinx_vdma_desc_pool\",\n\t\t\t\t     chan->dev,\n\t\t\t\t     sizeof(struct xilinx_vdma_tx_segment),\n\t\t\t\t     __alignof__(struct xilinx_vdma_tx_segment),\n\t\t\t\t     0);\n\t}\n\n\tif (!chan->desc_pool &&\n\t    ((chan->xdev->dma_config->dmatype != XDMA_TYPE_AXIDMA) &&\n\t\tchan->xdev->dma_config->dmatype != XDMA_TYPE_AXIMCDMA)) {\n\t\tdev_err(chan->dev,\n\t\t\t\"unable to allocate channel %d descriptor pool\\n\",\n\t\t\tchan->id);\n\t\treturn -ENOMEM;\n\t}\n\n\tdma_cookie_init(dchan);\n\n\tif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\n\t\t \n\t\tdma_ctrl_set(chan, XILINX_DMA_REG_DMACR,\n\t\t\t      XILINX_DMA_DMAXR_ALL_IRQ_MASK);\n\t}\n\n\tif ((chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) && chan->has_sg)\n\t\tdma_ctrl_set(chan, XILINX_DMA_REG_DMACR,\n\t\t\t     XILINX_CDMA_CR_SGMODE);\n\n\treturn 0;\n}\n\n \nstatic int xilinx_dma_calc_copysize(struct xilinx_dma_chan *chan,\n\t\t\t\t    int size, int done)\n{\n\tsize_t copy;\n\n\tcopy = min_t(size_t, size - done,\n\t\t     chan->xdev->max_buffer_len);\n\n\tif ((copy + done < size) &&\n\t    chan->xdev->common.copy_align) {\n\t\t \n\t\tcopy = rounddown(copy,\n\t\t\t\t (1 << chan->xdev->common.copy_align));\n\t}\n\treturn copy;\n}\n\n \nstatic enum dma_status xilinx_dma_tx_status(struct dma_chan *dchan,\n\t\t\t\t\tdma_cookie_t cookie,\n\t\t\t\t\tstruct dma_tx_state *txstate)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tstruct xilinx_dma_tx_descriptor *desc;\n\tenum dma_status ret;\n\tunsigned long flags;\n\tu32 residue = 0;\n\n\tret = dma_cookie_status(dchan, cookie, txstate);\n\tif (ret == DMA_COMPLETE || !txstate)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tif (!list_empty(&chan->active_list)) {\n\t\tdesc = list_last_entry(&chan->active_list,\n\t\t\t\t       struct xilinx_dma_tx_descriptor, node);\n\t\t \n\t\tif (chan->has_sg && chan->xdev->dma_config->dmatype != XDMA_TYPE_VDMA)\n\t\t\tresidue = xilinx_dma_get_residue(chan, desc);\n\t}\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\tdma_set_residue(txstate, residue);\n\n\treturn ret;\n}\n\n \nstatic int xilinx_dma_stop_transfer(struct xilinx_dma_chan *chan)\n{\n\tu32 val;\n\n\tdma_ctrl_clr(chan, XILINX_DMA_REG_DMACR, XILINX_DMA_DMACR_RUNSTOP);\n\n\t \n\treturn xilinx_dma_poll_timeout(chan, XILINX_DMA_REG_DMASR, val,\n\t\t\t\t       val & XILINX_DMA_DMASR_HALTED, 0,\n\t\t\t\t       XILINX_DMA_LOOP_COUNT);\n}\n\n \nstatic int xilinx_cdma_stop_transfer(struct xilinx_dma_chan *chan)\n{\n\tu32 val;\n\n\treturn xilinx_dma_poll_timeout(chan, XILINX_DMA_REG_DMASR, val,\n\t\t\t\t       val & XILINX_DMA_DMASR_IDLE, 0,\n\t\t\t\t       XILINX_DMA_LOOP_COUNT);\n}\n\n \nstatic void xilinx_dma_start(struct xilinx_dma_chan *chan)\n{\n\tint err;\n\tu32 val;\n\n\tdma_ctrl_set(chan, XILINX_DMA_REG_DMACR, XILINX_DMA_DMACR_RUNSTOP);\n\n\t \n\terr = xilinx_dma_poll_timeout(chan, XILINX_DMA_REG_DMASR, val,\n\t\t\t\t      !(val & XILINX_DMA_DMASR_HALTED), 0,\n\t\t\t\t      XILINX_DMA_LOOP_COUNT);\n\n\tif (err) {\n\t\tdev_err(chan->dev, \"Cannot start channel %p: %x\\n\",\n\t\t\tchan, dma_ctrl_read(chan, XILINX_DMA_REG_DMASR));\n\n\t\tchan->err = true;\n\t}\n}\n\n \nstatic void xilinx_vdma_start_transfer(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_vdma_config *config = &chan->config;\n\tstruct xilinx_dma_tx_descriptor *desc;\n\tu32 reg, j;\n\tstruct xilinx_vdma_tx_segment *segment, *last = NULL;\n\tint i = 0;\n\n\t \n\tif (chan->err)\n\t\treturn;\n\n\tif (!chan->idle)\n\t\treturn;\n\n\tif (list_empty(&chan->pending_list))\n\t\treturn;\n\n\tdesc = list_first_entry(&chan->pending_list,\n\t\t\t\tstruct xilinx_dma_tx_descriptor, node);\n\n\t \n\tif (chan->has_vflip) {\n\t\treg = dma_read(chan, XILINX_VDMA_REG_ENABLE_VERTICAL_FLIP);\n\t\treg &= ~XILINX_VDMA_ENABLE_VERTICAL_FLIP;\n\t\treg |= config->vflip_en;\n\t\tdma_write(chan, XILINX_VDMA_REG_ENABLE_VERTICAL_FLIP,\n\t\t\t  reg);\n\t}\n\n\treg = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\n\n\tif (config->frm_cnt_en)\n\t\treg |= XILINX_DMA_DMACR_FRAMECNT_EN;\n\telse\n\t\treg &= ~XILINX_DMA_DMACR_FRAMECNT_EN;\n\n\t \n\tif (config->park)\n\t\treg &= ~XILINX_DMA_DMACR_CIRC_EN;\n\telse\n\t\treg |= XILINX_DMA_DMACR_CIRC_EN;\n\n\tdma_ctrl_write(chan, XILINX_DMA_REG_DMACR, reg);\n\n\tj = chan->desc_submitcount;\n\treg = dma_read(chan, XILINX_DMA_REG_PARK_PTR);\n\tif (chan->direction == DMA_MEM_TO_DEV) {\n\t\treg &= ~XILINX_DMA_PARK_PTR_RD_REF_MASK;\n\t\treg |= j << XILINX_DMA_PARK_PTR_RD_REF_SHIFT;\n\t} else {\n\t\treg &= ~XILINX_DMA_PARK_PTR_WR_REF_MASK;\n\t\treg |= j << XILINX_DMA_PARK_PTR_WR_REF_SHIFT;\n\t}\n\tdma_write(chan, XILINX_DMA_REG_PARK_PTR, reg);\n\n\t \n\txilinx_dma_start(chan);\n\n\tif (chan->err)\n\t\treturn;\n\n\t \n\tif (chan->desc_submitcount < chan->num_frms)\n\t\ti = chan->desc_submitcount;\n\n\tlist_for_each_entry(segment, &desc->segments, node) {\n\t\tif (chan->ext_addr)\n\t\t\tvdma_desc_write_64(chan,\n\t\t\t\t   XILINX_VDMA_REG_START_ADDRESS_64(i++),\n\t\t\t\t   segment->hw.buf_addr,\n\t\t\t\t   segment->hw.buf_addr_msb);\n\t\telse\n\t\t\tvdma_desc_write(chan,\n\t\t\t\t\tXILINX_VDMA_REG_START_ADDRESS(i++),\n\t\t\t\t\tsegment->hw.buf_addr);\n\n\t\tlast = segment;\n\t}\n\n\tif (!last)\n\t\treturn;\n\n\t \n\tvdma_desc_write(chan, XILINX_DMA_REG_HSIZE, last->hw.hsize);\n\tvdma_desc_write(chan, XILINX_DMA_REG_FRMDLY_STRIDE,\n\t\t\tlast->hw.stride);\n\tvdma_desc_write(chan, XILINX_DMA_REG_VSIZE, last->hw.vsize);\n\n\tchan->desc_submitcount++;\n\tchan->desc_pendingcount--;\n\tlist_move_tail(&desc->node, &chan->active_list);\n\tif (chan->desc_submitcount == chan->num_frms)\n\t\tchan->desc_submitcount = 0;\n\n\tchan->idle = false;\n}\n\n \nstatic void xilinx_cdma_start_transfer(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_dma_tx_descriptor *head_desc, *tail_desc;\n\tstruct xilinx_cdma_tx_segment *tail_segment;\n\tu32 ctrl_reg = dma_read(chan, XILINX_DMA_REG_DMACR);\n\n\tif (chan->err)\n\t\treturn;\n\n\tif (!chan->idle)\n\t\treturn;\n\n\tif (list_empty(&chan->pending_list))\n\t\treturn;\n\n\thead_desc = list_first_entry(&chan->pending_list,\n\t\t\t\t     struct xilinx_dma_tx_descriptor, node);\n\ttail_desc = list_last_entry(&chan->pending_list,\n\t\t\t\t    struct xilinx_dma_tx_descriptor, node);\n\ttail_segment = list_last_entry(&tail_desc->segments,\n\t\t\t\t       struct xilinx_cdma_tx_segment, node);\n\n\tif (chan->desc_pendingcount <= XILINX_DMA_COALESCE_MAX) {\n\t\tctrl_reg &= ~XILINX_DMA_CR_COALESCE_MAX;\n\t\tctrl_reg |= chan->desc_pendingcount <<\n\t\t\t\tXILINX_DMA_CR_COALESCE_SHIFT;\n\t\tdma_ctrl_write(chan, XILINX_DMA_REG_DMACR, ctrl_reg);\n\t}\n\n\tif (chan->has_sg) {\n\t\tdma_ctrl_clr(chan, XILINX_DMA_REG_DMACR,\n\t\t\t     XILINX_CDMA_CR_SGMODE);\n\n\t\tdma_ctrl_set(chan, XILINX_DMA_REG_DMACR,\n\t\t\t     XILINX_CDMA_CR_SGMODE);\n\n\t\txilinx_write(chan, XILINX_DMA_REG_CURDESC,\n\t\t\t     head_desc->async_tx.phys);\n\n\t\t \n\t\txilinx_write(chan, XILINX_DMA_REG_TAILDESC,\n\t\t\t     tail_segment->phys);\n\t} else {\n\t\t \n\t\tstruct xilinx_cdma_tx_segment *segment;\n\t\tstruct xilinx_cdma_desc_hw *hw;\n\n\t\tsegment = list_first_entry(&head_desc->segments,\n\t\t\t\t\t   struct xilinx_cdma_tx_segment,\n\t\t\t\t\t   node);\n\n\t\thw = &segment->hw;\n\n\t\txilinx_write(chan, XILINX_CDMA_REG_SRCADDR,\n\t\t\t     xilinx_prep_dma_addr_t(hw->src_addr));\n\t\txilinx_write(chan, XILINX_CDMA_REG_DSTADDR,\n\t\t\t     xilinx_prep_dma_addr_t(hw->dest_addr));\n\n\t\t \n\t\tdma_ctrl_write(chan, XILINX_DMA_REG_BTT,\n\t\t\t\thw->control & chan->xdev->max_buffer_len);\n\t}\n\n\tlist_splice_tail_init(&chan->pending_list, &chan->active_list);\n\tchan->desc_pendingcount = 0;\n\tchan->idle = false;\n}\n\n \nstatic void xilinx_dma_start_transfer(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_dma_tx_descriptor *head_desc, *tail_desc;\n\tstruct xilinx_axidma_tx_segment *tail_segment;\n\tu32 reg;\n\n\tif (chan->err)\n\t\treturn;\n\n\tif (list_empty(&chan->pending_list))\n\t\treturn;\n\n\tif (!chan->idle)\n\t\treturn;\n\n\thead_desc = list_first_entry(&chan->pending_list,\n\t\t\t\t     struct xilinx_dma_tx_descriptor, node);\n\ttail_desc = list_last_entry(&chan->pending_list,\n\t\t\t\t    struct xilinx_dma_tx_descriptor, node);\n\ttail_segment = list_last_entry(&tail_desc->segments,\n\t\t\t\t       struct xilinx_axidma_tx_segment, node);\n\n\treg = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\n\n\tif (chan->desc_pendingcount <= XILINX_DMA_COALESCE_MAX) {\n\t\treg &= ~XILINX_DMA_CR_COALESCE_MAX;\n\t\treg |= chan->desc_pendingcount <<\n\t\t\t\t  XILINX_DMA_CR_COALESCE_SHIFT;\n\t\tdma_ctrl_write(chan, XILINX_DMA_REG_DMACR, reg);\n\t}\n\n\tif (chan->has_sg)\n\t\txilinx_write(chan, XILINX_DMA_REG_CURDESC,\n\t\t\t     head_desc->async_tx.phys);\n\treg  &= ~XILINX_DMA_CR_DELAY_MAX;\n\treg  |= chan->irq_delay << XILINX_DMA_CR_DELAY_SHIFT;\n\tdma_ctrl_write(chan, XILINX_DMA_REG_DMACR, reg);\n\n\txilinx_dma_start(chan);\n\n\tif (chan->err)\n\t\treturn;\n\n\t \n\tif (chan->has_sg) {\n\t\tif (chan->cyclic)\n\t\t\txilinx_write(chan, XILINX_DMA_REG_TAILDESC,\n\t\t\t\t     chan->cyclic_seg_v->phys);\n\t\telse\n\t\t\txilinx_write(chan, XILINX_DMA_REG_TAILDESC,\n\t\t\t\t     tail_segment->phys);\n\t} else {\n\t\tstruct xilinx_axidma_tx_segment *segment;\n\t\tstruct xilinx_axidma_desc_hw *hw;\n\n\t\tsegment = list_first_entry(&head_desc->segments,\n\t\t\t\t\t   struct xilinx_axidma_tx_segment,\n\t\t\t\t\t   node);\n\t\thw = &segment->hw;\n\n\t\txilinx_write(chan, XILINX_DMA_REG_SRCDSTADDR,\n\t\t\t     xilinx_prep_dma_addr_t(hw->buf_addr));\n\n\t\t \n\t\tdma_ctrl_write(chan, XILINX_DMA_REG_BTT,\n\t\t\t       hw->control & chan->xdev->max_buffer_len);\n\t}\n\n\tlist_splice_tail_init(&chan->pending_list, &chan->active_list);\n\tchan->desc_pendingcount = 0;\n\tchan->idle = false;\n}\n\n \nstatic void xilinx_mcdma_start_transfer(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_dma_tx_descriptor *head_desc, *tail_desc;\n\tstruct xilinx_aximcdma_tx_segment *tail_segment;\n\tu32 reg;\n\n\t \n\n\tif (chan->err)\n\t\treturn;\n\n\tif (!chan->idle)\n\t\treturn;\n\n\tif (list_empty(&chan->pending_list))\n\t\treturn;\n\n\thead_desc = list_first_entry(&chan->pending_list,\n\t\t\t\t     struct xilinx_dma_tx_descriptor, node);\n\ttail_desc = list_last_entry(&chan->pending_list,\n\t\t\t\t    struct xilinx_dma_tx_descriptor, node);\n\ttail_segment = list_last_entry(&tail_desc->segments,\n\t\t\t\t       struct xilinx_aximcdma_tx_segment, node);\n\n\treg = dma_ctrl_read(chan, XILINX_MCDMA_CHAN_CR_OFFSET(chan->tdest));\n\n\tif (chan->desc_pendingcount <= XILINX_MCDMA_COALESCE_MAX) {\n\t\treg &= ~XILINX_MCDMA_COALESCE_MASK;\n\t\treg |= chan->desc_pendingcount <<\n\t\t\tXILINX_MCDMA_COALESCE_SHIFT;\n\t}\n\n\treg |= XILINX_MCDMA_IRQ_ALL_MASK;\n\tdma_ctrl_write(chan, XILINX_MCDMA_CHAN_CR_OFFSET(chan->tdest), reg);\n\n\t \n\txilinx_write(chan, XILINX_MCDMA_CHAN_CDESC_OFFSET(chan->tdest),\n\t\t     head_desc->async_tx.phys);\n\n\t \n\treg = dma_ctrl_read(chan, XILINX_MCDMA_CHEN_OFFSET);\n\treg |= BIT(chan->tdest);\n\tdma_ctrl_write(chan, XILINX_MCDMA_CHEN_OFFSET, reg);\n\n\t \n\treg = dma_ctrl_read(chan, XILINX_MCDMA_CHAN_CR_OFFSET(chan->tdest));\n\treg |= XILINX_MCDMA_CR_RUNSTOP_MASK;\n\tdma_ctrl_write(chan, XILINX_MCDMA_CHAN_CR_OFFSET(chan->tdest), reg);\n\n\txilinx_dma_start(chan);\n\n\tif (chan->err)\n\t\treturn;\n\n\t \n\txilinx_write(chan, XILINX_MCDMA_CHAN_TDESC_OFFSET(chan->tdest),\n\t\t     tail_segment->phys);\n\n\tlist_splice_tail_init(&chan->pending_list, &chan->active_list);\n\tchan->desc_pendingcount = 0;\n\tchan->idle = false;\n}\n\n \nstatic void xilinx_dma_issue_pending(struct dma_chan *dchan)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tchan->start_transfer(chan);\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\n \nstatic int xilinx_dma_device_config(struct dma_chan *dchan,\n\t\t\t\t    struct dma_slave_config *config)\n{\n\treturn 0;\n}\n\n \nstatic void xilinx_dma_complete_descriptor(struct xilinx_dma_chan *chan)\n{\n\tstruct xilinx_dma_tx_descriptor *desc, *next;\n\n\t \n\tif (list_empty(&chan->active_list))\n\t\treturn;\n\n\tlist_for_each_entry_safe(desc, next, &chan->active_list, node) {\n\t\tif (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\n\t\t\tstruct xilinx_axidma_tx_segment *seg;\n\n\t\t\tseg = list_last_entry(&desc->segments,\n\t\t\t\t\t      struct xilinx_axidma_tx_segment, node);\n\t\t\tif (!(seg->hw.status & XILINX_DMA_BD_COMP_MASK) && chan->has_sg)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (chan->has_sg && chan->xdev->dma_config->dmatype !=\n\t\t    XDMA_TYPE_VDMA)\n\t\t\tdesc->residue = xilinx_dma_get_residue(chan, desc);\n\t\telse\n\t\t\tdesc->residue = 0;\n\t\tdesc->err = chan->err;\n\n\t\tlist_del(&desc->node);\n\t\tif (!desc->cyclic)\n\t\t\tdma_cookie_complete(&desc->async_tx);\n\t\tlist_add_tail(&desc->node, &chan->done_list);\n\t}\n}\n\n \nstatic int xilinx_dma_reset(struct xilinx_dma_chan *chan)\n{\n\tint err;\n\tu32 tmp;\n\n\tdma_ctrl_set(chan, XILINX_DMA_REG_DMACR, XILINX_DMA_DMACR_RESET);\n\n\t \n\terr = xilinx_dma_poll_timeout(chan, XILINX_DMA_REG_DMACR, tmp,\n\t\t\t\t      !(tmp & XILINX_DMA_DMACR_RESET), 0,\n\t\t\t\t      XILINX_DMA_LOOP_COUNT);\n\n\tif (err) {\n\t\tdev_err(chan->dev, \"reset timeout, cr %x, sr %x\\n\",\n\t\t\tdma_ctrl_read(chan, XILINX_DMA_REG_DMACR),\n\t\t\tdma_ctrl_read(chan, XILINX_DMA_REG_DMASR));\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tchan->err = false;\n\tchan->idle = true;\n\tchan->desc_pendingcount = 0;\n\tchan->desc_submitcount = 0;\n\n\treturn err;\n}\n\n \nstatic int xilinx_dma_chan_reset(struct xilinx_dma_chan *chan)\n{\n\tint err;\n\n\t \n\terr = xilinx_dma_reset(chan);\n\tif (err)\n\t\treturn err;\n\n\t \n\tdma_ctrl_set(chan, XILINX_DMA_REG_DMACR,\n\t\t      XILINX_DMA_DMAXR_ALL_IRQ_MASK);\n\n\treturn 0;\n}\n\n \nstatic irqreturn_t xilinx_mcdma_irq_handler(int irq, void *data)\n{\n\tstruct xilinx_dma_chan *chan = data;\n\tu32 status, ser_offset, chan_sermask, chan_offset = 0, chan_id;\n\n\tif (chan->direction == DMA_DEV_TO_MEM)\n\t\tser_offset = XILINX_MCDMA_RXINT_SER_OFFSET;\n\telse\n\t\tser_offset = XILINX_MCDMA_TXINT_SER_OFFSET;\n\n\t \n\tchan_sermask = dma_ctrl_read(chan, ser_offset);\n\tchan_id = ffs(chan_sermask);\n\n\tif (!chan_id)\n\t\treturn IRQ_NONE;\n\n\tif (chan->direction == DMA_DEV_TO_MEM)\n\t\tchan_offset = chan->xdev->dma_config->max_channels / 2;\n\n\tchan_offset = chan_offset + (chan_id - 1);\n\tchan = chan->xdev->chan[chan_offset];\n\t \n\tstatus = dma_ctrl_read(chan, XILINX_MCDMA_CHAN_SR_OFFSET(chan->tdest));\n\tif (!(status & XILINX_MCDMA_IRQ_ALL_MASK))\n\t\treturn IRQ_NONE;\n\n\tdma_ctrl_write(chan, XILINX_MCDMA_CHAN_SR_OFFSET(chan->tdest),\n\t\t       status & XILINX_MCDMA_IRQ_ALL_MASK);\n\n\tif (status & XILINX_MCDMA_IRQ_ERR_MASK) {\n\t\tdev_err(chan->dev, \"Channel %p has errors %x cdr %x tdr %x\\n\",\n\t\t\tchan,\n\t\t\tdma_ctrl_read(chan, XILINX_MCDMA_CH_ERR_OFFSET),\n\t\t\tdma_ctrl_read(chan, XILINX_MCDMA_CHAN_CDESC_OFFSET\n\t\t\t\t      (chan->tdest)),\n\t\t\tdma_ctrl_read(chan, XILINX_MCDMA_CHAN_TDESC_OFFSET\n\t\t\t\t      (chan->tdest)));\n\t\tchan->err = true;\n\t}\n\n\tif (status & XILINX_MCDMA_IRQ_DELAY_MASK) {\n\t\t \n\t\tdev_dbg(chan->dev, \"Inter-packet latency too long\\n\");\n\t}\n\n\tif (status & XILINX_MCDMA_IRQ_IOC_MASK) {\n\t\tspin_lock(&chan->lock);\n\t\txilinx_dma_complete_descriptor(chan);\n\t\tchan->idle = true;\n\t\tchan->start_transfer(chan);\n\t\tspin_unlock(&chan->lock);\n\t}\n\n\ttasklet_hi_schedule(&chan->tasklet);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t xilinx_dma_irq_handler(int irq, void *data)\n{\n\tstruct xilinx_dma_chan *chan = data;\n\tu32 status;\n\n\t \n\tstatus = dma_ctrl_read(chan, XILINX_DMA_REG_DMASR);\n\tif (!(status & XILINX_DMA_DMAXR_ALL_IRQ_MASK))\n\t\treturn IRQ_NONE;\n\n\tdma_ctrl_write(chan, XILINX_DMA_REG_DMASR,\n\t\t\tstatus & XILINX_DMA_DMAXR_ALL_IRQ_MASK);\n\n\tif (status & XILINX_DMA_DMASR_ERR_IRQ) {\n\t\t \n\t\tu32 errors = status & XILINX_DMA_DMASR_ALL_ERR_MASK;\n\n\t\tdma_ctrl_write(chan, XILINX_DMA_REG_DMASR,\n\t\t\t\terrors & XILINX_DMA_DMASR_ERR_RECOVER_MASK);\n\n\t\tif (!chan->flush_on_fsync ||\n\t\t    (errors & ~XILINX_DMA_DMASR_ERR_RECOVER_MASK)) {\n\t\t\tdev_err(chan->dev,\n\t\t\t\t\"Channel %p has errors %x, cdr %x tdr %x\\n\",\n\t\t\t\tchan, errors,\n\t\t\t\tdma_ctrl_read(chan, XILINX_DMA_REG_CURDESC),\n\t\t\t\tdma_ctrl_read(chan, XILINX_DMA_REG_TAILDESC));\n\t\t\tchan->err = true;\n\t\t}\n\t}\n\n\tif (status & (XILINX_DMA_DMASR_FRM_CNT_IRQ |\n\t\t      XILINX_DMA_DMASR_DLY_CNT_IRQ)) {\n\t\tspin_lock(&chan->lock);\n\t\txilinx_dma_complete_descriptor(chan);\n\t\tchan->idle = true;\n\t\tchan->start_transfer(chan);\n\t\tspin_unlock(&chan->lock);\n\t}\n\n\ttasklet_schedule(&chan->tasklet);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void append_desc_queue(struct xilinx_dma_chan *chan,\n\t\t\t      struct xilinx_dma_tx_descriptor *desc)\n{\n\tstruct xilinx_vdma_tx_segment *tail_segment;\n\tstruct xilinx_dma_tx_descriptor *tail_desc;\n\tstruct xilinx_axidma_tx_segment *axidma_tail_segment;\n\tstruct xilinx_aximcdma_tx_segment *aximcdma_tail_segment;\n\tstruct xilinx_cdma_tx_segment *cdma_tail_segment;\n\n\tif (list_empty(&chan->pending_list))\n\t\tgoto append;\n\n\t \n\ttail_desc = list_last_entry(&chan->pending_list,\n\t\t\t\t    struct xilinx_dma_tx_descriptor, node);\n\tif (chan->xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\n\t\ttail_segment = list_last_entry(&tail_desc->segments,\n\t\t\t\t\t       struct xilinx_vdma_tx_segment,\n\t\t\t\t\t       node);\n\t\ttail_segment->hw.next_desc = (u32)desc->async_tx.phys;\n\t} else if (chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\n\t\tcdma_tail_segment = list_last_entry(&tail_desc->segments,\n\t\t\t\t\t\tstruct xilinx_cdma_tx_segment,\n\t\t\t\t\t\tnode);\n\t\tcdma_tail_segment->hw.next_desc = (u32)desc->async_tx.phys;\n\t} else if (chan->xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\n\t\taxidma_tail_segment = list_last_entry(&tail_desc->segments,\n\t\t\t\t\t       struct xilinx_axidma_tx_segment,\n\t\t\t\t\t       node);\n\t\taxidma_tail_segment->hw.next_desc = (u32)desc->async_tx.phys;\n\t} else {\n\t\taximcdma_tail_segment =\n\t\t\tlist_last_entry(&tail_desc->segments,\n\t\t\t\t\tstruct xilinx_aximcdma_tx_segment,\n\t\t\t\t\tnode);\n\t\taximcdma_tail_segment->hw.next_desc = (u32)desc->async_tx.phys;\n\t}\n\n\t \nappend:\n\tlist_add_tail(&desc->node, &chan->pending_list);\n\tchan->desc_pendingcount++;\n\n\tif (chan->has_sg && (chan->xdev->dma_config->dmatype == XDMA_TYPE_VDMA)\n\t    && unlikely(chan->desc_pendingcount > chan->num_frms)) {\n\t\tdev_dbg(chan->dev, \"desc pendingcount is too high\\n\");\n\t\tchan->desc_pendingcount = chan->num_frms;\n\t}\n}\n\n \nstatic dma_cookie_t xilinx_dma_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct xilinx_dma_tx_descriptor *desc = to_dma_tx_descriptor(tx);\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(tx->chan);\n\tdma_cookie_t cookie;\n\tunsigned long flags;\n\tint err;\n\n\tif (chan->cyclic) {\n\t\txilinx_dma_free_tx_descriptor(chan, desc);\n\t\treturn -EBUSY;\n\t}\n\n\tif (chan->err) {\n\t\t \n\t\terr = xilinx_dma_chan_reset(chan);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tcookie = dma_cookie_assign(tx);\n\n\t \n\tappend_desc_queue(chan, desc);\n\n\tif (desc->cyclic)\n\t\tchan->cyclic = true;\n\n\tchan->terminating = false;\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn cookie;\n}\n\n \nstatic struct dma_async_tx_descriptor *\nxilinx_vdma_dma_prep_interleaved(struct dma_chan *dchan,\n\t\t\t\t struct dma_interleaved_template *xt,\n\t\t\t\t unsigned long flags)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tstruct xilinx_dma_tx_descriptor *desc;\n\tstruct xilinx_vdma_tx_segment *segment;\n\tstruct xilinx_vdma_desc_hw *hw;\n\n\tif (!is_slave_direction(xt->dir))\n\t\treturn NULL;\n\n\tif (!xt->numf || !xt->sgl[0].size)\n\t\treturn NULL;\n\n\tif (xt->frame_size != 1)\n\t\treturn NULL;\n\n\t \n\tdesc = xilinx_dma_alloc_tx_descriptor(chan);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\n\tdesc->async_tx.tx_submit = xilinx_dma_tx_submit;\n\tasync_tx_ack(&desc->async_tx);\n\n\t \n\tsegment = xilinx_vdma_alloc_tx_segment(chan);\n\tif (!segment)\n\t\tgoto error;\n\n\t \n\thw = &segment->hw;\n\thw->vsize = xt->numf;\n\thw->hsize = xt->sgl[0].size;\n\thw->stride = (xt->sgl[0].icg + xt->sgl[0].size) <<\n\t\t\tXILINX_DMA_FRMDLY_STRIDE_STRIDE_SHIFT;\n\thw->stride |= chan->config.frm_dly <<\n\t\t\tXILINX_DMA_FRMDLY_STRIDE_FRMDLY_SHIFT;\n\n\tif (xt->dir != DMA_MEM_TO_DEV) {\n\t\tif (chan->ext_addr) {\n\t\t\thw->buf_addr = lower_32_bits(xt->dst_start);\n\t\t\thw->buf_addr_msb = upper_32_bits(xt->dst_start);\n\t\t} else {\n\t\t\thw->buf_addr = xt->dst_start;\n\t\t}\n\t} else {\n\t\tif (chan->ext_addr) {\n\t\t\thw->buf_addr = lower_32_bits(xt->src_start);\n\t\t\thw->buf_addr_msb = upper_32_bits(xt->src_start);\n\t\t} else {\n\t\t\thw->buf_addr = xt->src_start;\n\t\t}\n\t}\n\n\t \n\tlist_add_tail(&segment->node, &desc->segments);\n\n\t \n\tsegment = list_first_entry(&desc->segments,\n\t\t\t\t   struct xilinx_vdma_tx_segment, node);\n\tdesc->async_tx.phys = segment->phys;\n\n\treturn &desc->async_tx;\n\nerror:\n\txilinx_dma_free_tx_descriptor(chan, desc);\n\treturn NULL;\n}\n\n \nstatic struct dma_async_tx_descriptor *\nxilinx_cdma_prep_memcpy(struct dma_chan *dchan, dma_addr_t dma_dst,\n\t\t\tdma_addr_t dma_src, size_t len, unsigned long flags)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tstruct xilinx_dma_tx_descriptor *desc;\n\tstruct xilinx_cdma_tx_segment *segment;\n\tstruct xilinx_cdma_desc_hw *hw;\n\n\tif (!len || len > chan->xdev->max_buffer_len)\n\t\treturn NULL;\n\n\tdesc = xilinx_dma_alloc_tx_descriptor(chan);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\n\tdesc->async_tx.tx_submit = xilinx_dma_tx_submit;\n\n\t \n\tsegment = xilinx_cdma_alloc_tx_segment(chan);\n\tif (!segment)\n\t\tgoto error;\n\n\thw = &segment->hw;\n\thw->control = len;\n\thw->src_addr = dma_src;\n\thw->dest_addr = dma_dst;\n\tif (chan->ext_addr) {\n\t\thw->src_addr_msb = upper_32_bits(dma_src);\n\t\thw->dest_addr_msb = upper_32_bits(dma_dst);\n\t}\n\n\t \n\tlist_add_tail(&segment->node, &desc->segments);\n\n\tdesc->async_tx.phys = segment->phys;\n\thw->next_desc = segment->phys;\n\n\treturn &desc->async_tx;\n\nerror:\n\txilinx_dma_free_tx_descriptor(chan, desc);\n\treturn NULL;\n}\n\n \nstatic struct dma_async_tx_descriptor *xilinx_dma_prep_slave_sg(\n\tstruct dma_chan *dchan, struct scatterlist *sgl, unsigned int sg_len,\n\tenum dma_transfer_direction direction, unsigned long flags,\n\tvoid *context)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tstruct xilinx_dma_tx_descriptor *desc;\n\tstruct xilinx_axidma_tx_segment *segment = NULL;\n\tu32 *app_w = (u32 *)context;\n\tstruct scatterlist *sg;\n\tsize_t copy;\n\tsize_t sg_used;\n\tunsigned int i;\n\n\tif (!is_slave_direction(direction))\n\t\treturn NULL;\n\n\t \n\tdesc = xilinx_dma_alloc_tx_descriptor(chan);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\n\tdesc->async_tx.tx_submit = xilinx_dma_tx_submit;\n\n\t \n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tsg_used = 0;\n\n\t\t \n\t\twhile (sg_used < sg_dma_len(sg)) {\n\t\t\tstruct xilinx_axidma_desc_hw *hw;\n\n\t\t\t \n\t\t\tsegment = xilinx_axidma_alloc_tx_segment(chan);\n\t\t\tif (!segment)\n\t\t\t\tgoto error;\n\n\t\t\t \n\t\t\tcopy = xilinx_dma_calc_copysize(chan, sg_dma_len(sg),\n\t\t\t\t\t\t\tsg_used);\n\t\t\thw = &segment->hw;\n\n\t\t\t \n\t\t\txilinx_axidma_buf(chan, hw, sg_dma_address(sg),\n\t\t\t\t\t  sg_used, 0);\n\n\t\t\thw->control = copy;\n\n\t\t\tif (chan->direction == DMA_MEM_TO_DEV) {\n\t\t\t\tif (app_w)\n\t\t\t\t\tmemcpy(hw->app, app_w, sizeof(u32) *\n\t\t\t\t\t       XILINX_DMA_NUM_APP_WORDS);\n\t\t\t}\n\n\t\t\tsg_used += copy;\n\n\t\t\t \n\t\t\tlist_add_tail(&segment->node, &desc->segments);\n\t\t}\n\t}\n\n\tsegment = list_first_entry(&desc->segments,\n\t\t\t\t   struct xilinx_axidma_tx_segment, node);\n\tdesc->async_tx.phys = segment->phys;\n\n\t \n\tif (chan->direction == DMA_MEM_TO_DEV) {\n\t\tsegment->hw.control |= XILINX_DMA_BD_SOP;\n\t\tsegment = list_last_entry(&desc->segments,\n\t\t\t\t\t  struct xilinx_axidma_tx_segment,\n\t\t\t\t\t  node);\n\t\tsegment->hw.control |= XILINX_DMA_BD_EOP;\n\t}\n\n\tif (chan->xdev->has_axistream_connected)\n\t\tdesc->async_tx.metadata_ops = &xilinx_dma_metadata_ops;\n\n\treturn &desc->async_tx;\n\nerror:\n\txilinx_dma_free_tx_descriptor(chan, desc);\n\treturn NULL;\n}\n\n \nstatic struct dma_async_tx_descriptor *xilinx_dma_prep_dma_cyclic(\n\tstruct dma_chan *dchan, dma_addr_t buf_addr, size_t buf_len,\n\tsize_t period_len, enum dma_transfer_direction direction,\n\tunsigned long flags)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tstruct xilinx_dma_tx_descriptor *desc;\n\tstruct xilinx_axidma_tx_segment *segment, *head_segment, *prev = NULL;\n\tsize_t copy, sg_used;\n\tunsigned int num_periods;\n\tint i;\n\tu32 reg;\n\n\tif (!period_len)\n\t\treturn NULL;\n\n\tnum_periods = buf_len / period_len;\n\n\tif (!num_periods)\n\t\treturn NULL;\n\n\tif (!is_slave_direction(direction))\n\t\treturn NULL;\n\n\t \n\tdesc = xilinx_dma_alloc_tx_descriptor(chan);\n\tif (!desc)\n\t\treturn NULL;\n\n\tchan->direction = direction;\n\tdma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\n\tdesc->async_tx.tx_submit = xilinx_dma_tx_submit;\n\n\tfor (i = 0; i < num_periods; ++i) {\n\t\tsg_used = 0;\n\n\t\twhile (sg_used < period_len) {\n\t\t\tstruct xilinx_axidma_desc_hw *hw;\n\n\t\t\t \n\t\t\tsegment = xilinx_axidma_alloc_tx_segment(chan);\n\t\t\tif (!segment)\n\t\t\t\tgoto error;\n\n\t\t\t \n\t\t\tcopy = xilinx_dma_calc_copysize(chan, period_len,\n\t\t\t\t\t\t\tsg_used);\n\t\t\thw = &segment->hw;\n\t\t\txilinx_axidma_buf(chan, hw, buf_addr, sg_used,\n\t\t\t\t\t  period_len * i);\n\t\t\thw->control = copy;\n\n\t\t\tif (prev)\n\t\t\t\tprev->hw.next_desc = segment->phys;\n\n\t\t\tprev = segment;\n\t\t\tsg_used += copy;\n\n\t\t\t \n\t\t\tlist_add_tail(&segment->node, &desc->segments);\n\t\t}\n\t}\n\n\thead_segment = list_first_entry(&desc->segments,\n\t\t\t\t   struct xilinx_axidma_tx_segment, node);\n\tdesc->async_tx.phys = head_segment->phys;\n\n\tdesc->cyclic = true;\n\treg = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\n\treg |= XILINX_DMA_CR_CYCLIC_BD_EN_MASK;\n\tdma_ctrl_write(chan, XILINX_DMA_REG_DMACR, reg);\n\n\tsegment = list_last_entry(&desc->segments,\n\t\t\t\t  struct xilinx_axidma_tx_segment,\n\t\t\t\t  node);\n\tsegment->hw.next_desc = (u32) head_segment->phys;\n\n\t \n\tif (direction == DMA_MEM_TO_DEV) {\n\t\thead_segment->hw.control |= XILINX_DMA_BD_SOP;\n\t\tsegment->hw.control |= XILINX_DMA_BD_EOP;\n\t}\n\n\treturn &desc->async_tx;\n\nerror:\n\txilinx_dma_free_tx_descriptor(chan, desc);\n\treturn NULL;\n}\n\n \nstatic struct dma_async_tx_descriptor *\nxilinx_mcdma_prep_slave_sg(struct dma_chan *dchan, struct scatterlist *sgl,\n\t\t\t   unsigned int sg_len,\n\t\t\t   enum dma_transfer_direction direction,\n\t\t\t   unsigned long flags, void *context)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tstruct xilinx_dma_tx_descriptor *desc;\n\tstruct xilinx_aximcdma_tx_segment *segment = NULL;\n\tu32 *app_w = (u32 *)context;\n\tstruct scatterlist *sg;\n\tsize_t copy;\n\tsize_t sg_used;\n\tunsigned int i;\n\n\tif (!is_slave_direction(direction))\n\t\treturn NULL;\n\n\t \n\tdesc = xilinx_dma_alloc_tx_descriptor(chan);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\n\tdesc->async_tx.tx_submit = xilinx_dma_tx_submit;\n\n\t \n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tsg_used = 0;\n\n\t\t \n\t\twhile (sg_used < sg_dma_len(sg)) {\n\t\t\tstruct xilinx_aximcdma_desc_hw *hw;\n\n\t\t\t \n\t\t\tsegment = xilinx_aximcdma_alloc_tx_segment(chan);\n\t\t\tif (!segment)\n\t\t\t\tgoto error;\n\n\t\t\t \n\t\t\tcopy = min_t(size_t, sg_dma_len(sg) - sg_used,\n\t\t\t\t     chan->xdev->max_buffer_len);\n\t\t\thw = &segment->hw;\n\n\t\t\t \n\t\t\txilinx_aximcdma_buf(chan, hw, sg_dma_address(sg),\n\t\t\t\t\t    sg_used);\n\t\t\thw->control = copy;\n\n\t\t\tif (chan->direction == DMA_MEM_TO_DEV && app_w) {\n\t\t\t\tmemcpy(hw->app, app_w, sizeof(u32) *\n\t\t\t\t       XILINX_DMA_NUM_APP_WORDS);\n\t\t\t}\n\n\t\t\tsg_used += copy;\n\t\t\t \n\t\t\tlist_add_tail(&segment->node, &desc->segments);\n\t\t}\n\t}\n\n\tsegment = list_first_entry(&desc->segments,\n\t\t\t\t   struct xilinx_aximcdma_tx_segment, node);\n\tdesc->async_tx.phys = segment->phys;\n\n\t \n\tif (chan->direction == DMA_MEM_TO_DEV) {\n\t\tsegment->hw.control |= XILINX_MCDMA_BD_SOP;\n\t\tsegment = list_last_entry(&desc->segments,\n\t\t\t\t\t  struct xilinx_aximcdma_tx_segment,\n\t\t\t\t\t  node);\n\t\tsegment->hw.control |= XILINX_MCDMA_BD_EOP;\n\t}\n\n\treturn &desc->async_tx;\n\nerror:\n\txilinx_dma_free_tx_descriptor(chan, desc);\n\n\treturn NULL;\n}\n\n \nstatic int xilinx_dma_terminate_all(struct dma_chan *dchan)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tu32 reg;\n\tint err;\n\n\tif (!chan->cyclic) {\n\t\terr = chan->stop_transfer(chan);\n\t\tif (err) {\n\t\t\tdev_err(chan->dev, \"Cannot stop channel %p: %x\\n\",\n\t\t\t\tchan, dma_ctrl_read(chan,\n\t\t\t\tXILINX_DMA_REG_DMASR));\n\t\t\tchan->err = true;\n\t\t}\n\t}\n\n\txilinx_dma_chan_reset(chan);\n\t \n\tchan->terminating = true;\n\txilinx_dma_free_descriptors(chan);\n\tchan->idle = true;\n\n\tif (chan->cyclic) {\n\t\treg = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\n\t\treg &= ~XILINX_DMA_CR_CYCLIC_BD_EN_MASK;\n\t\tdma_ctrl_write(chan, XILINX_DMA_REG_DMACR, reg);\n\t\tchan->cyclic = false;\n\t}\n\n\tif ((chan->xdev->dma_config->dmatype == XDMA_TYPE_CDMA) && chan->has_sg)\n\t\tdma_ctrl_clr(chan, XILINX_DMA_REG_DMACR,\n\t\t\t     XILINX_CDMA_CR_SGMODE);\n\n\treturn 0;\n}\n\nstatic void xilinx_dma_synchronize(struct dma_chan *dchan)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\n\ttasklet_kill(&chan->tasklet);\n}\n\n \nint xilinx_vdma_channel_set_config(struct dma_chan *dchan,\n\t\t\t\t\tstruct xilinx_vdma_config *cfg)\n{\n\tstruct xilinx_dma_chan *chan = to_xilinx_chan(dchan);\n\tu32 dmacr;\n\n\tif (cfg->reset)\n\t\treturn xilinx_dma_chan_reset(chan);\n\n\tdmacr = dma_ctrl_read(chan, XILINX_DMA_REG_DMACR);\n\n\tchan->config.frm_dly = cfg->frm_dly;\n\tchan->config.park = cfg->park;\n\n\t \n\tchan->config.gen_lock = cfg->gen_lock;\n\tchan->config.master = cfg->master;\n\n\tdmacr &= ~XILINX_DMA_DMACR_GENLOCK_EN;\n\tif (cfg->gen_lock && chan->genlock) {\n\t\tdmacr |= XILINX_DMA_DMACR_GENLOCK_EN;\n\t\tdmacr &= ~XILINX_DMA_DMACR_MASTER_MASK;\n\t\tdmacr |= cfg->master << XILINX_DMA_DMACR_MASTER_SHIFT;\n\t}\n\n\tchan->config.frm_cnt_en = cfg->frm_cnt_en;\n\tchan->config.vflip_en = cfg->vflip_en;\n\n\tif (cfg->park)\n\t\tchan->config.park_frm = cfg->park_frm;\n\telse\n\t\tchan->config.park_frm = -1;\n\n\tchan->config.coalesc = cfg->coalesc;\n\tchan->config.delay = cfg->delay;\n\n\tif (cfg->coalesc <= XILINX_DMA_DMACR_FRAME_COUNT_MAX) {\n\t\tdmacr &= ~XILINX_DMA_DMACR_FRAME_COUNT_MASK;\n\t\tdmacr |= cfg->coalesc << XILINX_DMA_DMACR_FRAME_COUNT_SHIFT;\n\t\tchan->config.coalesc = cfg->coalesc;\n\t}\n\n\tif (cfg->delay <= XILINX_DMA_DMACR_DELAY_MAX) {\n\t\tdmacr &= ~XILINX_DMA_DMACR_DELAY_MASK;\n\t\tdmacr |= cfg->delay << XILINX_DMA_DMACR_DELAY_SHIFT;\n\t\tchan->config.delay = cfg->delay;\n\t}\n\n\t \n\tdmacr &= ~XILINX_DMA_DMACR_FSYNCSRC_MASK;\n\tdmacr |= cfg->ext_fsync << XILINX_DMA_DMACR_FSYNCSRC_SHIFT;\n\n\tdma_ctrl_write(chan, XILINX_DMA_REG_DMACR, dmacr);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(xilinx_vdma_channel_set_config);\n\n \n\n \nstatic void xilinx_dma_chan_remove(struct xilinx_dma_chan *chan)\n{\n\t \n\tdma_ctrl_clr(chan, XILINX_DMA_REG_DMACR,\n\t\t      XILINX_DMA_DMAXR_ALL_IRQ_MASK);\n\n\tif (chan->irq > 0)\n\t\tfree_irq(chan->irq, chan);\n\n\ttasklet_kill(&chan->tasklet);\n\n\tlist_del(&chan->common.device_node);\n}\n\nstatic int axidma_clk_init(struct platform_device *pdev, struct clk **axi_clk,\n\t\t\t    struct clk **tx_clk, struct clk **rx_clk,\n\t\t\t    struct clk **sg_clk, struct clk **tmp_clk)\n{\n\tint err;\n\n\t*tmp_clk = NULL;\n\n\t*axi_clk = devm_clk_get(&pdev->dev, \"s_axi_lite_aclk\");\n\tif (IS_ERR(*axi_clk))\n\t\treturn dev_err_probe(&pdev->dev, PTR_ERR(*axi_clk), \"failed to get axi_aclk\\n\");\n\n\t*tx_clk = devm_clk_get(&pdev->dev, \"m_axi_mm2s_aclk\");\n\tif (IS_ERR(*tx_clk))\n\t\t*tx_clk = NULL;\n\n\t*rx_clk = devm_clk_get(&pdev->dev, \"m_axi_s2mm_aclk\");\n\tif (IS_ERR(*rx_clk))\n\t\t*rx_clk = NULL;\n\n\t*sg_clk = devm_clk_get(&pdev->dev, \"m_axi_sg_aclk\");\n\tif (IS_ERR(*sg_clk))\n\t\t*sg_clk = NULL;\n\n\terr = clk_prepare_enable(*axi_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable axi_clk (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = clk_prepare_enable(*tx_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable tx_clk (%d)\\n\", err);\n\t\tgoto err_disable_axiclk;\n\t}\n\n\terr = clk_prepare_enable(*rx_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable rx_clk (%d)\\n\", err);\n\t\tgoto err_disable_txclk;\n\t}\n\n\terr = clk_prepare_enable(*sg_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable sg_clk (%d)\\n\", err);\n\t\tgoto err_disable_rxclk;\n\t}\n\n\treturn 0;\n\nerr_disable_rxclk:\n\tclk_disable_unprepare(*rx_clk);\nerr_disable_txclk:\n\tclk_disable_unprepare(*tx_clk);\nerr_disable_axiclk:\n\tclk_disable_unprepare(*axi_clk);\n\n\treturn err;\n}\n\nstatic int axicdma_clk_init(struct platform_device *pdev, struct clk **axi_clk,\n\t\t\t    struct clk **dev_clk, struct clk **tmp_clk,\n\t\t\t    struct clk **tmp1_clk, struct clk **tmp2_clk)\n{\n\tint err;\n\n\t*tmp_clk = NULL;\n\t*tmp1_clk = NULL;\n\t*tmp2_clk = NULL;\n\n\t*axi_clk = devm_clk_get(&pdev->dev, \"s_axi_lite_aclk\");\n\tif (IS_ERR(*axi_clk))\n\t\treturn dev_err_probe(&pdev->dev, PTR_ERR(*axi_clk), \"failed to get axi_aclk\\n\");\n\n\t*dev_clk = devm_clk_get(&pdev->dev, \"m_axi_aclk\");\n\tif (IS_ERR(*dev_clk))\n\t\treturn dev_err_probe(&pdev->dev, PTR_ERR(*dev_clk), \"failed to get dev_clk\\n\");\n\n\terr = clk_prepare_enable(*axi_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable axi_clk (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = clk_prepare_enable(*dev_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable dev_clk (%d)\\n\", err);\n\t\tgoto err_disable_axiclk;\n\t}\n\n\treturn 0;\n\nerr_disable_axiclk:\n\tclk_disable_unprepare(*axi_clk);\n\n\treturn err;\n}\n\nstatic int axivdma_clk_init(struct platform_device *pdev, struct clk **axi_clk,\n\t\t\t    struct clk **tx_clk, struct clk **txs_clk,\n\t\t\t    struct clk **rx_clk, struct clk **rxs_clk)\n{\n\tint err;\n\n\t*axi_clk = devm_clk_get(&pdev->dev, \"s_axi_lite_aclk\");\n\tif (IS_ERR(*axi_clk))\n\t\treturn dev_err_probe(&pdev->dev, PTR_ERR(*axi_clk), \"failed to get axi_aclk\\n\");\n\n\t*tx_clk = devm_clk_get(&pdev->dev, \"m_axi_mm2s_aclk\");\n\tif (IS_ERR(*tx_clk))\n\t\t*tx_clk = NULL;\n\n\t*txs_clk = devm_clk_get(&pdev->dev, \"m_axis_mm2s_aclk\");\n\tif (IS_ERR(*txs_clk))\n\t\t*txs_clk = NULL;\n\n\t*rx_clk = devm_clk_get(&pdev->dev, \"m_axi_s2mm_aclk\");\n\tif (IS_ERR(*rx_clk))\n\t\t*rx_clk = NULL;\n\n\t*rxs_clk = devm_clk_get(&pdev->dev, \"s_axis_s2mm_aclk\");\n\tif (IS_ERR(*rxs_clk))\n\t\t*rxs_clk = NULL;\n\n\terr = clk_prepare_enable(*axi_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable axi_clk (%d)\\n\",\n\t\t\terr);\n\t\treturn err;\n\t}\n\n\terr = clk_prepare_enable(*tx_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable tx_clk (%d)\\n\", err);\n\t\tgoto err_disable_axiclk;\n\t}\n\n\terr = clk_prepare_enable(*txs_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable txs_clk (%d)\\n\", err);\n\t\tgoto err_disable_txclk;\n\t}\n\n\terr = clk_prepare_enable(*rx_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable rx_clk (%d)\\n\", err);\n\t\tgoto err_disable_txsclk;\n\t}\n\n\terr = clk_prepare_enable(*rxs_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable rxs_clk (%d)\\n\", err);\n\t\tgoto err_disable_rxclk;\n\t}\n\n\treturn 0;\n\nerr_disable_rxclk:\n\tclk_disable_unprepare(*rx_clk);\nerr_disable_txsclk:\n\tclk_disable_unprepare(*txs_clk);\nerr_disable_txclk:\n\tclk_disable_unprepare(*tx_clk);\nerr_disable_axiclk:\n\tclk_disable_unprepare(*axi_clk);\n\n\treturn err;\n}\n\nstatic void xdma_disable_allclks(struct xilinx_dma_device *xdev)\n{\n\tclk_disable_unprepare(xdev->rxs_clk);\n\tclk_disable_unprepare(xdev->rx_clk);\n\tclk_disable_unprepare(xdev->txs_clk);\n\tclk_disable_unprepare(xdev->tx_clk);\n\tclk_disable_unprepare(xdev->axi_clk);\n}\n\n \nstatic int xilinx_dma_chan_probe(struct xilinx_dma_device *xdev,\n\t\t\t\t  struct device_node *node)\n{\n\tstruct xilinx_dma_chan *chan;\n\tbool has_dre = false;\n\tu32 value, width;\n\tint err;\n\n\t \n\tchan = devm_kzalloc(xdev->dev, sizeof(*chan), GFP_KERNEL);\n\tif (!chan)\n\t\treturn -ENOMEM;\n\n\tchan->dev = xdev->dev;\n\tchan->xdev = xdev;\n\tchan->desc_pendingcount = 0x0;\n\tchan->ext_addr = xdev->ext_addr;\n\t \n\tchan->idle = true;\n\n\tspin_lock_init(&chan->lock);\n\tINIT_LIST_HEAD(&chan->pending_list);\n\tINIT_LIST_HEAD(&chan->done_list);\n\tINIT_LIST_HEAD(&chan->active_list);\n\tINIT_LIST_HEAD(&chan->free_seg_list);\n\n\t \n\thas_dre = of_property_read_bool(node, \"xlnx,include-dre\");\n\n\tof_property_read_u8(node, \"xlnx,irq-delay\", &chan->irq_delay);\n\n\tchan->genlock = of_property_read_bool(node, \"xlnx,genlock-mode\");\n\n\terr = of_property_read_u32(node, \"xlnx,datawidth\", &value);\n\tif (err) {\n\t\tdev_err(xdev->dev, \"missing xlnx,datawidth property\\n\");\n\t\treturn err;\n\t}\n\twidth = value >> 3;  \n\n\t \n\tif (width > 8)\n\t\thas_dre = false;\n\n\tif (!has_dre)\n\t\txdev->common.copy_align = (enum dmaengine_alignment)fls(width - 1);\n\n\tif (of_device_is_compatible(node, \"xlnx,axi-vdma-mm2s-channel\") ||\n\t    of_device_is_compatible(node, \"xlnx,axi-dma-mm2s-channel\") ||\n\t    of_device_is_compatible(node, \"xlnx,axi-cdma-channel\")) {\n\t\tchan->direction = DMA_MEM_TO_DEV;\n\t\tchan->id = xdev->mm2s_chan_id++;\n\t\tchan->tdest = chan->id;\n\n\t\tchan->ctrl_offset = XILINX_DMA_MM2S_CTRL_OFFSET;\n\t\tif (xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\n\t\t\tchan->desc_offset = XILINX_VDMA_MM2S_DESC_OFFSET;\n\t\t\tchan->config.park = 1;\n\n\t\t\tif (xdev->flush_on_fsync == XILINX_DMA_FLUSH_BOTH ||\n\t\t\t    xdev->flush_on_fsync == XILINX_DMA_FLUSH_MM2S)\n\t\t\t\tchan->flush_on_fsync = true;\n\t\t}\n\t} else if (of_device_is_compatible(node,\n\t\t\t\t\t   \"xlnx,axi-vdma-s2mm-channel\") ||\n\t\t   of_device_is_compatible(node,\n\t\t\t\t\t   \"xlnx,axi-dma-s2mm-channel\")) {\n\t\tchan->direction = DMA_DEV_TO_MEM;\n\t\tchan->id = xdev->s2mm_chan_id++;\n\t\tchan->tdest = chan->id - xdev->dma_config->max_channels / 2;\n\t\tchan->has_vflip = of_property_read_bool(node,\n\t\t\t\t\t\"xlnx,enable-vert-flip\");\n\t\tif (chan->has_vflip) {\n\t\t\tchan->config.vflip_en = dma_read(chan,\n\t\t\t\tXILINX_VDMA_REG_ENABLE_VERTICAL_FLIP) &\n\t\t\t\tXILINX_VDMA_ENABLE_VERTICAL_FLIP;\n\t\t}\n\n\t\tif (xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA)\n\t\t\tchan->ctrl_offset = XILINX_MCDMA_S2MM_CTRL_OFFSET;\n\t\telse\n\t\t\tchan->ctrl_offset = XILINX_DMA_S2MM_CTRL_OFFSET;\n\n\t\tif (xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\n\t\t\tchan->desc_offset = XILINX_VDMA_S2MM_DESC_OFFSET;\n\t\t\tchan->config.park = 1;\n\n\t\t\tif (xdev->flush_on_fsync == XILINX_DMA_FLUSH_BOTH ||\n\t\t\t    xdev->flush_on_fsync == XILINX_DMA_FLUSH_S2MM)\n\t\t\t\tchan->flush_on_fsync = true;\n\t\t}\n\t} else {\n\t\tdev_err(xdev->dev, \"Invalid channel compatible node\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tchan->irq = of_irq_get(node, chan->tdest);\n\tif (chan->irq < 0)\n\t\treturn dev_err_probe(xdev->dev, chan->irq, \"failed to get irq\\n\");\n\terr = request_irq(chan->irq, xdev->dma_config->irq_handler,\n\t\t\t  IRQF_SHARED, \"xilinx-dma-controller\", chan);\n\tif (err) {\n\t\tdev_err(xdev->dev, \"unable to request IRQ %d\\n\", chan->irq);\n\t\treturn err;\n\t}\n\n\tif (xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\n\t\tchan->start_transfer = xilinx_dma_start_transfer;\n\t\tchan->stop_transfer = xilinx_dma_stop_transfer;\n\t} else if (xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA) {\n\t\tchan->start_transfer = xilinx_mcdma_start_transfer;\n\t\tchan->stop_transfer = xilinx_dma_stop_transfer;\n\t} else if (xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\n\t\tchan->start_transfer = xilinx_cdma_start_transfer;\n\t\tchan->stop_transfer = xilinx_cdma_stop_transfer;\n\t} else {\n\t\tchan->start_transfer = xilinx_vdma_start_transfer;\n\t\tchan->stop_transfer = xilinx_dma_stop_transfer;\n\t}\n\n\t \n\tif (xdev->dma_config->dmatype != XDMA_TYPE_VDMA) {\n\t\tif (xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA ||\n\t\t    dma_ctrl_read(chan, XILINX_DMA_REG_DMASR) &\n\t\t\t    XILINX_DMA_DMASR_SG_MASK)\n\t\t\tchan->has_sg = true;\n\t\tdev_dbg(chan->dev, \"ch %d: SG %s\\n\", chan->id,\n\t\t\tchan->has_sg ? \"enabled\" : \"disabled\");\n\t}\n\n\t \n\ttasklet_setup(&chan->tasklet, xilinx_dma_do_tasklet);\n\n\t \n\tchan->common.device = &xdev->common;\n\n\tlist_add_tail(&chan->common.device_node, &xdev->common.channels);\n\txdev->chan[chan->id] = chan;\n\n\t \n\terr = xilinx_dma_chan_reset(chan);\n\tif (err < 0) {\n\t\tdev_err(xdev->dev, \"Reset channel failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int xilinx_dma_child_probe(struct xilinx_dma_device *xdev,\n\t\t\t\t    struct device_node *node)\n{\n\tint ret, i;\n\tu32 nr_channels = 1;\n\n\tret = of_property_read_u32(node, \"dma-channels\", &nr_channels);\n\tif (xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA && ret < 0)\n\t\tdev_warn(xdev->dev, \"missing dma-channels property\\n\");\n\n\tfor (i = 0; i < nr_channels; i++) {\n\t\tret = xilinx_dma_chan_probe(xdev, node);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct dma_chan *of_dma_xilinx_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t\t\tstruct of_dma *ofdma)\n{\n\tstruct xilinx_dma_device *xdev = ofdma->of_dma_data;\n\tint chan_id = dma_spec->args[0];\n\n\tif (chan_id >= xdev->dma_config->max_channels || !xdev->chan[chan_id])\n\t\treturn NULL;\n\n\treturn dma_get_slave_channel(&xdev->chan[chan_id]->common);\n}\n\nstatic const struct xilinx_dma_config axidma_config = {\n\t.dmatype = XDMA_TYPE_AXIDMA,\n\t.clk_init = axidma_clk_init,\n\t.irq_handler = xilinx_dma_irq_handler,\n\t.max_channels = XILINX_DMA_MAX_CHANS_PER_DEVICE,\n};\n\nstatic const struct xilinx_dma_config aximcdma_config = {\n\t.dmatype = XDMA_TYPE_AXIMCDMA,\n\t.clk_init = axidma_clk_init,\n\t.irq_handler = xilinx_mcdma_irq_handler,\n\t.max_channels = XILINX_MCDMA_MAX_CHANS_PER_DEVICE,\n};\nstatic const struct xilinx_dma_config axicdma_config = {\n\t.dmatype = XDMA_TYPE_CDMA,\n\t.clk_init = axicdma_clk_init,\n\t.irq_handler = xilinx_dma_irq_handler,\n\t.max_channels = XILINX_CDMA_MAX_CHANS_PER_DEVICE,\n};\n\nstatic const struct xilinx_dma_config axivdma_config = {\n\t.dmatype = XDMA_TYPE_VDMA,\n\t.clk_init = axivdma_clk_init,\n\t.irq_handler = xilinx_dma_irq_handler,\n\t.max_channels = XILINX_DMA_MAX_CHANS_PER_DEVICE,\n};\n\nstatic const struct of_device_id xilinx_dma_of_ids[] = {\n\t{ .compatible = \"xlnx,axi-dma-1.00.a\", .data = &axidma_config },\n\t{ .compatible = \"xlnx,axi-cdma-1.00.a\", .data = &axicdma_config },\n\t{ .compatible = \"xlnx,axi-vdma-1.00.a\", .data = &axivdma_config },\n\t{ .compatible = \"xlnx,axi-mcdma-1.00.a\", .data = &aximcdma_config },\n\t{}\n};\nMODULE_DEVICE_TABLE(of, xilinx_dma_of_ids);\n\n \nstatic int xilinx_dma_probe(struct platform_device *pdev)\n{\n\tint (*clk_init)(struct platform_device *, struct clk **, struct clk **,\n\t\t\tstruct clk **, struct clk **, struct clk **)\n\t\t\t\t\t= axivdma_clk_init;\n\tstruct device_node *node = pdev->dev.of_node;\n\tstruct xilinx_dma_device *xdev;\n\tstruct device_node *child, *np = pdev->dev.of_node;\n\tu32 num_frames, addr_width, len_width;\n\tint i, err;\n\n\t \n\txdev = devm_kzalloc(&pdev->dev, sizeof(*xdev), GFP_KERNEL);\n\tif (!xdev)\n\t\treturn -ENOMEM;\n\n\txdev->dev = &pdev->dev;\n\tif (np) {\n\t\tconst struct of_device_id *match;\n\n\t\tmatch = of_match_node(xilinx_dma_of_ids, np);\n\t\tif (match && match->data) {\n\t\t\txdev->dma_config = match->data;\n\t\t\tclk_init = xdev->dma_config->clk_init;\n\t\t}\n\t}\n\n\terr = clk_init(pdev, &xdev->axi_clk, &xdev->tx_clk, &xdev->txs_clk,\n\t\t       &xdev->rx_clk, &xdev->rxs_clk);\n\tif (err)\n\t\treturn err;\n\n\t \n\txdev->regs = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(xdev->regs)) {\n\t\terr = PTR_ERR(xdev->regs);\n\t\tgoto disable_clks;\n\t}\n\t \n\txdev->max_buffer_len = GENMASK(XILINX_DMA_MAX_TRANS_LEN_MAX - 1, 0);\n\txdev->s2mm_chan_id = xdev->dma_config->max_channels / 2;\n\n\tif (xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA ||\n\t    xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA) {\n\t\tif (!of_property_read_u32(node, \"xlnx,sg-length-width\",\n\t\t\t\t\t  &len_width)) {\n\t\t\tif (len_width < XILINX_DMA_MAX_TRANS_LEN_MIN ||\n\t\t\t    len_width > XILINX_DMA_V2_MAX_TRANS_LEN_MAX) {\n\t\t\t\tdev_warn(xdev->dev,\n\t\t\t\t\t \"invalid xlnx,sg-length-width property value. Using default width\\n\");\n\t\t\t} else {\n\t\t\t\tif (len_width > XILINX_DMA_MAX_TRANS_LEN_MAX)\n\t\t\t\t\tdev_warn(xdev->dev, \"Please ensure that IP supports buffer length > 23 bits\\n\");\n\t\t\t\txdev->max_buffer_len =\n\t\t\t\t\tGENMASK(len_width - 1, 0);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\n\t\txdev->has_axistream_connected =\n\t\t\tof_property_read_bool(node, \"xlnx,axistream-connected\");\n\t}\n\n\tif (xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\n\t\terr = of_property_read_u32(node, \"xlnx,num-fstores\",\n\t\t\t\t\t   &num_frames);\n\t\tif (err < 0) {\n\t\t\tdev_err(xdev->dev,\n\t\t\t\t\"missing xlnx,num-fstores property\\n\");\n\t\t\tgoto disable_clks;\n\t\t}\n\n\t\terr = of_property_read_u32(node, \"xlnx,flush-fsync\",\n\t\t\t\t\t   &xdev->flush_on_fsync);\n\t\tif (err < 0)\n\t\t\tdev_warn(xdev->dev,\n\t\t\t\t \"missing xlnx,flush-fsync property\\n\");\n\t}\n\n\terr = of_property_read_u32(node, \"xlnx,addrwidth\", &addr_width);\n\tif (err < 0)\n\t\tdev_warn(xdev->dev, \"missing xlnx,addrwidth property\\n\");\n\n\tif (addr_width > 32)\n\t\txdev->ext_addr = true;\n\telse\n\t\txdev->ext_addr = false;\n\n\t \n\tif (xdev->has_axistream_connected)\n\t\txdev->common.desc_metadata_modes = DESC_METADATA_ENGINE;\n\n\t \n\terr = dma_set_mask_and_coherent(xdev->dev, DMA_BIT_MASK(addr_width));\n\tif (err < 0) {\n\t\tdev_err(xdev->dev, \"DMA mask error %d\\n\", err);\n\t\tgoto disable_clks;\n\t}\n\n\t \n\txdev->common.dev = &pdev->dev;\n\n\tINIT_LIST_HEAD(&xdev->common.channels);\n\tif (!(xdev->dma_config->dmatype == XDMA_TYPE_CDMA)) {\n\t\tdma_cap_set(DMA_SLAVE, xdev->common.cap_mask);\n\t\tdma_cap_set(DMA_PRIVATE, xdev->common.cap_mask);\n\t}\n\n\txdev->common.device_alloc_chan_resources =\n\t\t\t\txilinx_dma_alloc_chan_resources;\n\txdev->common.device_free_chan_resources =\n\t\t\t\txilinx_dma_free_chan_resources;\n\txdev->common.device_terminate_all = xilinx_dma_terminate_all;\n\txdev->common.device_synchronize = xilinx_dma_synchronize;\n\txdev->common.device_tx_status = xilinx_dma_tx_status;\n\txdev->common.device_issue_pending = xilinx_dma_issue_pending;\n\txdev->common.device_config = xilinx_dma_device_config;\n\tif (xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA) {\n\t\tdma_cap_set(DMA_CYCLIC, xdev->common.cap_mask);\n\t\txdev->common.device_prep_slave_sg = xilinx_dma_prep_slave_sg;\n\t\txdev->common.device_prep_dma_cyclic =\n\t\t\t\t\t  xilinx_dma_prep_dma_cyclic;\n\t\t \n\t\txdev->common.residue_granularity =\n\t\t\t\t\t  DMA_RESIDUE_GRANULARITY_SEGMENT;\n\t} else if (xdev->dma_config->dmatype == XDMA_TYPE_CDMA) {\n\t\tdma_cap_set(DMA_MEMCPY, xdev->common.cap_mask);\n\t\txdev->common.device_prep_dma_memcpy = xilinx_cdma_prep_memcpy;\n\t\t \n\t\txdev->common.residue_granularity =\n\t\t\t\t\t  DMA_RESIDUE_GRANULARITY_SEGMENT;\n\t} else if (xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA) {\n\t\txdev->common.device_prep_slave_sg = xilinx_mcdma_prep_slave_sg;\n\t} else {\n\t\txdev->common.device_prep_interleaved_dma =\n\t\t\t\txilinx_vdma_dma_prep_interleaved;\n\t}\n\n\tplatform_set_drvdata(pdev, xdev);\n\n\t \n\tfor_each_child_of_node(node, child) {\n\t\terr = xilinx_dma_child_probe(xdev, child);\n\t\tif (err < 0) {\n\t\t\tof_node_put(child);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tif (xdev->dma_config->dmatype == XDMA_TYPE_VDMA) {\n\t\tfor (i = 0; i < xdev->dma_config->max_channels; i++)\n\t\t\tif (xdev->chan[i])\n\t\t\t\txdev->chan[i]->num_frms = num_frames;\n\t}\n\n\t \n\terr = dma_async_device_register(&xdev->common);\n\tif (err) {\n\t\tdev_err(xdev->dev, \"failed to register the dma device\\n\");\n\t\tgoto error;\n\t}\n\n\terr = of_dma_controller_register(node, of_dma_xilinx_xlate,\n\t\t\t\t\t xdev);\n\tif (err < 0) {\n\t\tdev_err(&pdev->dev, \"Unable to register DMA to DT\\n\");\n\t\tdma_async_device_unregister(&xdev->common);\n\t\tgoto error;\n\t}\n\n\tif (xdev->dma_config->dmatype == XDMA_TYPE_AXIDMA)\n\t\tdev_info(&pdev->dev, \"Xilinx AXI DMA Engine Driver Probed!!\\n\");\n\telse if (xdev->dma_config->dmatype == XDMA_TYPE_CDMA)\n\t\tdev_info(&pdev->dev, \"Xilinx AXI CDMA Engine Driver Probed!!\\n\");\n\telse if (xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA)\n\t\tdev_info(&pdev->dev, \"Xilinx AXI MCDMA Engine Driver Probed!!\\n\");\n\telse\n\t\tdev_info(&pdev->dev, \"Xilinx AXI VDMA Engine Driver Probed!!\\n\");\n\n\treturn 0;\n\nerror:\n\tfor (i = 0; i < xdev->dma_config->max_channels; i++)\n\t\tif (xdev->chan[i])\n\t\t\txilinx_dma_chan_remove(xdev->chan[i]);\ndisable_clks:\n\txdma_disable_allclks(xdev);\n\n\treturn err;\n}\n\n \nstatic int xilinx_dma_remove(struct platform_device *pdev)\n{\n\tstruct xilinx_dma_device *xdev = platform_get_drvdata(pdev);\n\tint i;\n\n\tof_dma_controller_free(pdev->dev.of_node);\n\n\tdma_async_device_unregister(&xdev->common);\n\n\tfor (i = 0; i < xdev->dma_config->max_channels; i++)\n\t\tif (xdev->chan[i])\n\t\t\txilinx_dma_chan_remove(xdev->chan[i]);\n\n\txdma_disable_allclks(xdev);\n\n\treturn 0;\n}\n\nstatic struct platform_driver xilinx_vdma_driver = {\n\t.driver = {\n\t\t.name = \"xilinx-vdma\",\n\t\t.of_match_table = xilinx_dma_of_ids,\n\t},\n\t.probe = xilinx_dma_probe,\n\t.remove = xilinx_dma_remove,\n};\n\nmodule_platform_driver(xilinx_vdma_driver);\n\nMODULE_AUTHOR(\"Xilinx, Inc.\");\nMODULE_DESCRIPTION(\"Xilinx VDMA driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}