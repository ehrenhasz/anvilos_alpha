{
  "module_name": "xilinx_dpdma.c",
  "hash_id": "12831af1ebb6618565aca9148bfe80de2fa7fc4bf2fdf7c9db275c8e2507ee7e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/xilinx/xilinx_dpdma.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/bits.h>\n#include <linux/clk.h>\n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/dma/xilinx_dpdma.h>\n#include <linux/dmaengine.h>\n#include <linux/dmapool.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n#include <linux/platform_device.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/wait.h>\n\n#include <dt-bindings/dma/xlnx-zynqmp-dpdma.h>\n\n#include \"../dmaengine.h\"\n#include \"../virt-dma.h\"\n\n \n#define XILINX_DPDMA_ERR_CTRL\t\t\t\t0x000\n#define XILINX_DPDMA_ISR\t\t\t\t0x004\n#define XILINX_DPDMA_IMR\t\t\t\t0x008\n#define XILINX_DPDMA_IEN\t\t\t\t0x00c\n#define XILINX_DPDMA_IDS\t\t\t\t0x010\n#define XILINX_DPDMA_INTR_DESC_DONE(n)\t\t\tBIT((n) + 0)\n#define XILINX_DPDMA_INTR_DESC_DONE_MASK\t\tGENMASK(5, 0)\n#define XILINX_DPDMA_INTR_NO_OSTAND(n)\t\t\tBIT((n) + 6)\n#define XILINX_DPDMA_INTR_NO_OSTAND_MASK\t\tGENMASK(11, 6)\n#define XILINX_DPDMA_INTR_AXI_ERR(n)\t\t\tBIT((n) + 12)\n#define XILINX_DPDMA_INTR_AXI_ERR_MASK\t\t\tGENMASK(17, 12)\n#define XILINX_DPDMA_INTR_DESC_ERR(n)\t\t\tBIT((n) + 16)\n#define XILINX_DPDMA_INTR_DESC_ERR_MASK\t\t\tGENMASK(23, 18)\n#define XILINX_DPDMA_INTR_WR_CMD_FIFO_FULL\t\tBIT(24)\n#define XILINX_DPDMA_INTR_WR_DATA_FIFO_FULL\t\tBIT(25)\n#define XILINX_DPDMA_INTR_AXI_4K_CROSS\t\t\tBIT(26)\n#define XILINX_DPDMA_INTR_VSYNC\t\t\t\tBIT(27)\n#define XILINX_DPDMA_INTR_CHAN_ERR_MASK\t\t\t0x00041000\n#define XILINX_DPDMA_INTR_CHAN_ERR\t\t\t0x00fff000\n#define XILINX_DPDMA_INTR_GLOBAL_ERR\t\t\t0x07000000\n#define XILINX_DPDMA_INTR_ERR_ALL\t\t\t0x07fff000\n#define XILINX_DPDMA_INTR_CHAN_MASK\t\t\t0x00041041\n#define XILINX_DPDMA_INTR_GLOBAL_MASK\t\t\t0x0f000000\n#define XILINX_DPDMA_INTR_ALL\t\t\t\t0x0fffffff\n#define XILINX_DPDMA_EISR\t\t\t\t0x014\n#define XILINX_DPDMA_EIMR\t\t\t\t0x018\n#define XILINX_DPDMA_EIEN\t\t\t\t0x01c\n#define XILINX_DPDMA_EIDS\t\t\t\t0x020\n#define XILINX_DPDMA_EINTR_INV_APB\t\t\tBIT(0)\n#define XILINX_DPDMA_EINTR_RD_AXI_ERR(n)\t\tBIT((n) + 1)\n#define XILINX_DPDMA_EINTR_RD_AXI_ERR_MASK\t\tGENMASK(6, 1)\n#define XILINX_DPDMA_EINTR_PRE_ERR(n)\t\t\tBIT((n) + 7)\n#define XILINX_DPDMA_EINTR_PRE_ERR_MASK\t\t\tGENMASK(12, 7)\n#define XILINX_DPDMA_EINTR_CRC_ERR(n)\t\t\tBIT((n) + 13)\n#define XILINX_DPDMA_EINTR_CRC_ERR_MASK\t\t\tGENMASK(18, 13)\n#define XILINX_DPDMA_EINTR_WR_AXI_ERR(n)\t\tBIT((n) + 19)\n#define XILINX_DPDMA_EINTR_WR_AXI_ERR_MASK\t\tGENMASK(24, 19)\n#define XILINX_DPDMA_EINTR_DESC_DONE_ERR(n)\t\tBIT((n) + 25)\n#define XILINX_DPDMA_EINTR_DESC_DONE_ERR_MASK\t\tGENMASK(30, 25)\n#define XILINX_DPDMA_EINTR_RD_CMD_FIFO_FULL\t\tBIT(32)\n#define XILINX_DPDMA_EINTR_CHAN_ERR_MASK\t\t0x02082082\n#define XILINX_DPDMA_EINTR_CHAN_ERR\t\t\t0x7ffffffe\n#define XILINX_DPDMA_EINTR_GLOBAL_ERR\t\t\t0x80000001\n#define XILINX_DPDMA_EINTR_ALL\t\t\t\t0xffffffff\n#define XILINX_DPDMA_CNTL\t\t\t\t0x100\n#define XILINX_DPDMA_GBL\t\t\t\t0x104\n#define XILINX_DPDMA_GBL_TRIG_MASK(n)\t\t\t((n) << 0)\n#define XILINX_DPDMA_GBL_RETRIG_MASK(n)\t\t\t((n) << 6)\n#define XILINX_DPDMA_ALC0_CNTL\t\t\t\t0x108\n#define XILINX_DPDMA_ALC0_STATUS\t\t\t0x10c\n#define XILINX_DPDMA_ALC0_MAX\t\t\t\t0x110\n#define XILINX_DPDMA_ALC0_MIN\t\t\t\t0x114\n#define XILINX_DPDMA_ALC0_ACC\t\t\t\t0x118\n#define XILINX_DPDMA_ALC0_ACC_TRAN\t\t\t0x11c\n#define XILINX_DPDMA_ALC1_CNTL\t\t\t\t0x120\n#define XILINX_DPDMA_ALC1_STATUS\t\t\t0x124\n#define XILINX_DPDMA_ALC1_MAX\t\t\t\t0x128\n#define XILINX_DPDMA_ALC1_MIN\t\t\t\t0x12c\n#define XILINX_DPDMA_ALC1_ACC\t\t\t\t0x130\n#define XILINX_DPDMA_ALC1_ACC_TRAN\t\t\t0x134\n\n \n#define XILINX_DPDMA_CH_BASE\t\t\t\t0x200\n#define XILINX_DPDMA_CH_OFFSET\t\t\t\t0x100\n#define XILINX_DPDMA_CH_DESC_START_ADDRE\t\t0x000\n#define XILINX_DPDMA_CH_DESC_START_ADDRE_MASK\t\tGENMASK(15, 0)\n#define XILINX_DPDMA_CH_DESC_START_ADDR\t\t\t0x004\n#define XILINX_DPDMA_CH_DESC_NEXT_ADDRE\t\t\t0x008\n#define XILINX_DPDMA_CH_DESC_NEXT_ADDR\t\t\t0x00c\n#define XILINX_DPDMA_CH_PYLD_CUR_ADDRE\t\t\t0x010\n#define XILINX_DPDMA_CH_PYLD_CUR_ADDR\t\t\t0x014\n#define XILINX_DPDMA_CH_CNTL\t\t\t\t0x018\n#define XILINX_DPDMA_CH_CNTL_ENABLE\t\t\tBIT(0)\n#define XILINX_DPDMA_CH_CNTL_PAUSE\t\t\tBIT(1)\n#define XILINX_DPDMA_CH_CNTL_QOS_DSCR_WR_MASK\t\tGENMASK(5, 2)\n#define XILINX_DPDMA_CH_CNTL_QOS_DSCR_RD_MASK\t\tGENMASK(9, 6)\n#define XILINX_DPDMA_CH_CNTL_QOS_DATA_RD_MASK\t\tGENMASK(13, 10)\n#define XILINX_DPDMA_CH_CNTL_QOS_VID_CLASS\t\t11\n#define XILINX_DPDMA_CH_STATUS\t\t\t\t0x01c\n#define XILINX_DPDMA_CH_STATUS_OTRAN_CNT_MASK\t\tGENMASK(24, 21)\n#define XILINX_DPDMA_CH_VDO\t\t\t\t0x020\n#define XILINX_DPDMA_CH_PYLD_SZ\t\t\t\t0x024\n#define XILINX_DPDMA_CH_DESC_ID\t\t\t\t0x028\n#define XILINX_DPDMA_CH_DESC_ID_MASK\t\t\tGENMASK(15, 0)\n\n \n#define XILINX_DPDMA_DESC_CONTROL_PREEMBLE\t\t0xa5\n#define XILINX_DPDMA_DESC_CONTROL_COMPLETE_INTR\t\tBIT(8)\n#define XILINX_DPDMA_DESC_CONTROL_DESC_UPDATE\t\tBIT(9)\n#define XILINX_DPDMA_DESC_CONTROL_IGNORE_DONE\t\tBIT(10)\n#define XILINX_DPDMA_DESC_CONTROL_FRAG_MODE\t\tBIT(18)\n#define XILINX_DPDMA_DESC_CONTROL_LAST\t\t\tBIT(19)\n#define XILINX_DPDMA_DESC_CONTROL_ENABLE_CRC\t\tBIT(20)\n#define XILINX_DPDMA_DESC_CONTROL_LAST_OF_FRAME\t\tBIT(21)\n#define XILINX_DPDMA_DESC_ID_MASK\t\t\tGENMASK(15, 0)\n#define XILINX_DPDMA_DESC_HSIZE_STRIDE_HSIZE_MASK\tGENMASK(17, 0)\n#define XILINX_DPDMA_DESC_HSIZE_STRIDE_STRIDE_MASK\tGENMASK(31, 18)\n#define XILINX_DPDMA_DESC_ADDR_EXT_NEXT_ADDR_MASK\tGENMASK(15, 0)\n#define XILINX_DPDMA_DESC_ADDR_EXT_SRC_ADDR_MASK\tGENMASK(31, 16)\n\n#define XILINX_DPDMA_ALIGN_BYTES\t\t\t256\n#define XILINX_DPDMA_LINESIZE_ALIGN_BITS\t\t128\n\n#define XILINX_DPDMA_NUM_CHAN\t\t\t\t6\n\nstruct xilinx_dpdma_chan;\n\n \nstruct xilinx_dpdma_hw_desc {\n\tu32 control;\n\tu32 desc_id;\n\tu32 xfer_size;\n\tu32 hsize_stride;\n\tu32 timestamp_lsb;\n\tu32 timestamp_msb;\n\tu32 addr_ext;\n\tu32 next_desc;\n\tu32 src_addr;\n\tu32 addr_ext_23;\n\tu32 addr_ext_45;\n\tu32 src_addr2;\n\tu32 src_addr3;\n\tu32 src_addr4;\n\tu32 src_addr5;\n\tu32 crc;\n} __aligned(XILINX_DPDMA_ALIGN_BYTES);\n\n \nstruct xilinx_dpdma_sw_desc {\n\tstruct xilinx_dpdma_hw_desc hw;\n\tstruct list_head node;\n\tdma_addr_t dma_addr;\n};\n\n \nstruct xilinx_dpdma_tx_desc {\n\tstruct virt_dma_desc vdesc;\n\tstruct xilinx_dpdma_chan *chan;\n\tstruct list_head descriptors;\n\tbool error;\n};\n\n#define to_dpdma_tx_desc(_desc) \\\n\tcontainer_of(_desc, struct xilinx_dpdma_tx_desc, vdesc)\n\n \nstruct xilinx_dpdma_chan {\n\tstruct virt_dma_chan vchan;\n\tvoid __iomem *reg;\n\tunsigned int id;\n\n\twait_queue_head_t wait_to_stop;\n\tbool running;\n\tbool first_frame;\n\tbool video_group;\n\n\tspinlock_t lock;  \n\tstruct dma_pool *desc_pool;\n\tstruct tasklet_struct err_task;\n\n\tstruct {\n\t\tstruct xilinx_dpdma_tx_desc *pending;\n\t\tstruct xilinx_dpdma_tx_desc *active;\n\t} desc;\n\n\tstruct xilinx_dpdma_device *xdev;\n};\n\n#define to_xilinx_chan(_chan) \\\n\tcontainer_of(_chan, struct xilinx_dpdma_chan, vchan.chan)\n\n \nstruct xilinx_dpdma_device {\n\tstruct dma_device common;\n\tvoid __iomem *reg;\n\tstruct device *dev;\n\tint irq;\n\n\tstruct clk *axi_clk;\n\tstruct xilinx_dpdma_chan *chan[XILINX_DPDMA_NUM_CHAN];\n\n\tbool ext_addr;\n};\n\n \n#define XILINX_DPDMA_DEBUGFS_READ_MAX_SIZE\t32\n#define XILINX_DPDMA_DEBUGFS_UINT16_MAX_STR\t\"65535\"\n\n \nenum xilinx_dpdma_testcases {\n\tDPDMA_TC_INTR_DONE,\n\tDPDMA_TC_NONE\n};\n\nstruct xilinx_dpdma_debugfs {\n\tenum xilinx_dpdma_testcases testcase;\n\tu16 xilinx_dpdma_irq_done_count;\n\tunsigned int chan_id;\n};\n\nstatic struct xilinx_dpdma_debugfs dpdma_debugfs;\nstruct xilinx_dpdma_debugfs_request {\n\tconst char *name;\n\tenum xilinx_dpdma_testcases tc;\n\tssize_t (*read)(char *buf);\n\tint (*write)(char *args);\n};\n\nstatic void xilinx_dpdma_debugfs_desc_done_irq(struct xilinx_dpdma_chan *chan)\n{\n\tif (IS_ENABLED(CONFIG_DEBUG_FS) && chan->id == dpdma_debugfs.chan_id)\n\t\tdpdma_debugfs.xilinx_dpdma_irq_done_count++;\n}\n\nstatic ssize_t xilinx_dpdma_debugfs_desc_done_irq_read(char *buf)\n{\n\tsize_t out_str_len;\n\n\tdpdma_debugfs.testcase = DPDMA_TC_NONE;\n\n\tout_str_len = strlen(XILINX_DPDMA_DEBUGFS_UINT16_MAX_STR);\n\tout_str_len = min_t(size_t, XILINX_DPDMA_DEBUGFS_READ_MAX_SIZE,\n\t\t\t    out_str_len);\n\tsnprintf(buf, out_str_len, \"%d\",\n\t\t dpdma_debugfs.xilinx_dpdma_irq_done_count);\n\n\treturn 0;\n}\n\nstatic int xilinx_dpdma_debugfs_desc_done_irq_write(char *args)\n{\n\tchar *arg;\n\tint ret;\n\tu32 id;\n\n\targ = strsep(&args, \" \");\n\tif (!arg || strncasecmp(arg, \"start\", 5))\n\t\treturn -EINVAL;\n\n\targ = strsep(&args, \" \");\n\tif (!arg)\n\t\treturn -EINVAL;\n\n\tret = kstrtou32(arg, 0, &id);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (id < ZYNQMP_DPDMA_VIDEO0 || id > ZYNQMP_DPDMA_AUDIO1)\n\t\treturn -EINVAL;\n\n\tdpdma_debugfs.testcase = DPDMA_TC_INTR_DONE;\n\tdpdma_debugfs.xilinx_dpdma_irq_done_count = 0;\n\tdpdma_debugfs.chan_id = id;\n\n\treturn 0;\n}\n\n \nstatic struct xilinx_dpdma_debugfs_request dpdma_debugfs_reqs[] = {\n\t{\n\t\t.name = \"DESCRIPTOR_DONE_INTR\",\n\t\t.tc = DPDMA_TC_INTR_DONE,\n\t\t.read = xilinx_dpdma_debugfs_desc_done_irq_read,\n\t\t.write = xilinx_dpdma_debugfs_desc_done_irq_write,\n\t},\n};\n\nstatic ssize_t xilinx_dpdma_debugfs_read(struct file *f, char __user *buf,\n\t\t\t\t\t size_t size, loff_t *pos)\n{\n\tenum xilinx_dpdma_testcases testcase;\n\tchar *kern_buff;\n\tint ret = 0;\n\n\tif (*pos != 0 || size <= 0)\n\t\treturn -EINVAL;\n\n\tkern_buff = kzalloc(XILINX_DPDMA_DEBUGFS_READ_MAX_SIZE, GFP_KERNEL);\n\tif (!kern_buff) {\n\t\tdpdma_debugfs.testcase = DPDMA_TC_NONE;\n\t\treturn -ENOMEM;\n\t}\n\n\ttestcase = READ_ONCE(dpdma_debugfs.testcase);\n\tif (testcase != DPDMA_TC_NONE) {\n\t\tret = dpdma_debugfs_reqs[testcase].read(kern_buff);\n\t\tif (ret < 0)\n\t\t\tgoto done;\n\t} else {\n\t\tstrscpy(kern_buff, \"No testcase executed\",\n\t\t\tXILINX_DPDMA_DEBUGFS_READ_MAX_SIZE);\n\t}\n\n\tsize = min(size, strlen(kern_buff));\n\tif (copy_to_user(buf, kern_buff, size))\n\t\tret = -EFAULT;\n\ndone:\n\tkfree(kern_buff);\n\tif (ret)\n\t\treturn ret;\n\n\t*pos = size + 1;\n\treturn size;\n}\n\nstatic ssize_t xilinx_dpdma_debugfs_write(struct file *f,\n\t\t\t\t\t  const char __user *buf, size_t size,\n\t\t\t\t\t  loff_t *pos)\n{\n\tchar *kern_buff, *kern_buff_start;\n\tchar *testcase;\n\tunsigned int i;\n\tint ret;\n\n\tif (*pos != 0 || size <= 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (dpdma_debugfs.testcase != DPDMA_TC_NONE)\n\t\treturn -EBUSY;\n\n\tkern_buff = kzalloc(size, GFP_KERNEL);\n\tif (!kern_buff)\n\t\treturn -ENOMEM;\n\tkern_buff_start = kern_buff;\n\n\tret = strncpy_from_user(kern_buff, buf, size);\n\tif (ret < 0)\n\t\tgoto done;\n\n\t \n\ttestcase = strsep(&kern_buff, \" \");\n\n\tfor (i = 0; i < ARRAY_SIZE(dpdma_debugfs_reqs); i++) {\n\t\tif (!strcasecmp(testcase, dpdma_debugfs_reqs[i].name))\n\t\t\tbreak;\n\t}\n\n\tif (i == ARRAY_SIZE(dpdma_debugfs_reqs)) {\n\t\tret = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tret = dpdma_debugfs_reqs[i].write(kern_buff);\n\tif (ret < 0)\n\t\tgoto done;\n\n\tret = size;\n\ndone:\n\tkfree(kern_buff_start);\n\treturn ret;\n}\n\nstatic const struct file_operations fops_xilinx_dpdma_dbgfs = {\n\t.owner = THIS_MODULE,\n\t.read = xilinx_dpdma_debugfs_read,\n\t.write = xilinx_dpdma_debugfs_write,\n};\n\nstatic void xilinx_dpdma_debugfs_init(struct xilinx_dpdma_device *xdev)\n{\n\tstruct dentry *dent;\n\n\tdpdma_debugfs.testcase = DPDMA_TC_NONE;\n\n\tdent = debugfs_create_file(\"testcase\", 0444, xdev->common.dbg_dev_root,\n\t\t\t\t   NULL, &fops_xilinx_dpdma_dbgfs);\n\tif (IS_ERR(dent))\n\t\tdev_err(xdev->dev, \"Failed to create debugfs testcase file\\n\");\n}\n\n \n\nstatic inline u32 dpdma_read(void __iomem *base, u32 offset)\n{\n\treturn ioread32(base + offset);\n}\n\nstatic inline void dpdma_write(void __iomem *base, u32 offset, u32 val)\n{\n\tiowrite32(val, base + offset);\n}\n\nstatic inline void dpdma_clr(void __iomem *base, u32 offset, u32 clr)\n{\n\tdpdma_write(base, offset, dpdma_read(base, offset) & ~clr);\n}\n\nstatic inline void dpdma_set(void __iomem *base, u32 offset, u32 set)\n{\n\tdpdma_write(base, offset, dpdma_read(base, offset) | set);\n}\n\n \n\n \nstatic void xilinx_dpdma_sw_desc_set_dma_addrs(struct xilinx_dpdma_device *xdev,\n\t\t\t\t\t       struct xilinx_dpdma_sw_desc *sw_desc,\n\t\t\t\t\t       struct xilinx_dpdma_sw_desc *prev,\n\t\t\t\t\t       dma_addr_t dma_addr[],\n\t\t\t\t\t       unsigned int num_src_addr)\n{\n\tstruct xilinx_dpdma_hw_desc *hw_desc = &sw_desc->hw;\n\tunsigned int i;\n\n\thw_desc->src_addr = lower_32_bits(dma_addr[0]);\n\tif (xdev->ext_addr)\n\t\thw_desc->addr_ext |=\n\t\t\tFIELD_PREP(XILINX_DPDMA_DESC_ADDR_EXT_SRC_ADDR_MASK,\n\t\t\t\t   upper_32_bits(dma_addr[0]));\n\n\tfor (i = 1; i < num_src_addr; i++) {\n\t\tu32 *addr = &hw_desc->src_addr2;\n\n\t\taddr[i - 1] = lower_32_bits(dma_addr[i]);\n\n\t\tif (xdev->ext_addr) {\n\t\t\tu32 *addr_ext = &hw_desc->addr_ext_23;\n\t\t\tu32 addr_msb;\n\n\t\t\taddr_msb = upper_32_bits(dma_addr[i]) & GENMASK(15, 0);\n\t\t\taddr_msb <<= 16 * ((i - 1) % 2);\n\t\t\taddr_ext[(i - 1) / 2] |= addr_msb;\n\t\t}\n\t}\n\n\tif (!prev)\n\t\treturn;\n\n\tprev->hw.next_desc = lower_32_bits(sw_desc->dma_addr);\n\tif (xdev->ext_addr)\n\t\tprev->hw.addr_ext |=\n\t\t\tFIELD_PREP(XILINX_DPDMA_DESC_ADDR_EXT_NEXT_ADDR_MASK,\n\t\t\t\t   upper_32_bits(sw_desc->dma_addr));\n}\n\n \nstatic struct xilinx_dpdma_sw_desc *\nxilinx_dpdma_chan_alloc_sw_desc(struct xilinx_dpdma_chan *chan)\n{\n\tstruct xilinx_dpdma_sw_desc *sw_desc;\n\tdma_addr_t dma_addr;\n\n\tsw_desc = dma_pool_zalloc(chan->desc_pool, GFP_ATOMIC, &dma_addr);\n\tif (!sw_desc)\n\t\treturn NULL;\n\n\tsw_desc->dma_addr = dma_addr;\n\n\treturn sw_desc;\n}\n\n \nstatic void\nxilinx_dpdma_chan_free_sw_desc(struct xilinx_dpdma_chan *chan,\n\t\t\t       struct xilinx_dpdma_sw_desc *sw_desc)\n{\n\tdma_pool_free(chan->desc_pool, sw_desc, sw_desc->dma_addr);\n}\n\n \nstatic void xilinx_dpdma_chan_dump_tx_desc(struct xilinx_dpdma_chan *chan,\n\t\t\t\t\t   struct xilinx_dpdma_tx_desc *tx_desc)\n{\n\tstruct xilinx_dpdma_sw_desc *sw_desc;\n\tstruct device *dev = chan->xdev->dev;\n\tunsigned int i = 0;\n\n\tdev_dbg(dev, \"------- TX descriptor dump start -------\\n\");\n\tdev_dbg(dev, \"------- channel ID = %d -------\\n\", chan->id);\n\n\tlist_for_each_entry(sw_desc, &tx_desc->descriptors, node) {\n\t\tstruct xilinx_dpdma_hw_desc *hw_desc = &sw_desc->hw;\n\n\t\tdev_dbg(dev, \"------- HW descriptor %d -------\\n\", i++);\n\t\tdev_dbg(dev, \"descriptor DMA addr: %pad\\n\", &sw_desc->dma_addr);\n\t\tdev_dbg(dev, \"control: 0x%08x\\n\", hw_desc->control);\n\t\tdev_dbg(dev, \"desc_id: 0x%08x\\n\", hw_desc->desc_id);\n\t\tdev_dbg(dev, \"xfer_size: 0x%08x\\n\", hw_desc->xfer_size);\n\t\tdev_dbg(dev, \"hsize_stride: 0x%08x\\n\", hw_desc->hsize_stride);\n\t\tdev_dbg(dev, \"timestamp_lsb: 0x%08x\\n\", hw_desc->timestamp_lsb);\n\t\tdev_dbg(dev, \"timestamp_msb: 0x%08x\\n\", hw_desc->timestamp_msb);\n\t\tdev_dbg(dev, \"addr_ext: 0x%08x\\n\", hw_desc->addr_ext);\n\t\tdev_dbg(dev, \"next_desc: 0x%08x\\n\", hw_desc->next_desc);\n\t\tdev_dbg(dev, \"src_addr: 0x%08x\\n\", hw_desc->src_addr);\n\t\tdev_dbg(dev, \"addr_ext_23: 0x%08x\\n\", hw_desc->addr_ext_23);\n\t\tdev_dbg(dev, \"addr_ext_45: 0x%08x\\n\", hw_desc->addr_ext_45);\n\t\tdev_dbg(dev, \"src_addr2: 0x%08x\\n\", hw_desc->src_addr2);\n\t\tdev_dbg(dev, \"src_addr3: 0x%08x\\n\", hw_desc->src_addr3);\n\t\tdev_dbg(dev, \"src_addr4: 0x%08x\\n\", hw_desc->src_addr4);\n\t\tdev_dbg(dev, \"src_addr5: 0x%08x\\n\", hw_desc->src_addr5);\n\t\tdev_dbg(dev, \"crc: 0x%08x\\n\", hw_desc->crc);\n\t}\n\n\tdev_dbg(dev, \"------- TX descriptor dump end -------\\n\");\n}\n\n \nstatic struct xilinx_dpdma_tx_desc *\nxilinx_dpdma_chan_alloc_tx_desc(struct xilinx_dpdma_chan *chan)\n{\n\tstruct xilinx_dpdma_tx_desc *tx_desc;\n\n\ttx_desc = kzalloc(sizeof(*tx_desc), GFP_NOWAIT);\n\tif (!tx_desc)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&tx_desc->descriptors);\n\ttx_desc->chan = chan;\n\ttx_desc->error = false;\n\n\treturn tx_desc;\n}\n\n \nstatic void xilinx_dpdma_chan_free_tx_desc(struct virt_dma_desc *vdesc)\n{\n\tstruct xilinx_dpdma_sw_desc *sw_desc, *next;\n\tstruct xilinx_dpdma_tx_desc *desc;\n\n\tif (!vdesc)\n\t\treturn;\n\n\tdesc = to_dpdma_tx_desc(vdesc);\n\n\tlist_for_each_entry_safe(sw_desc, next, &desc->descriptors, node) {\n\t\tlist_del(&sw_desc->node);\n\t\txilinx_dpdma_chan_free_sw_desc(desc->chan, sw_desc);\n\t}\n\n\tkfree(desc);\n}\n\n \nstatic struct xilinx_dpdma_tx_desc *\nxilinx_dpdma_chan_prep_interleaved_dma(struct xilinx_dpdma_chan *chan,\n\t\t\t\t       struct dma_interleaved_template *xt)\n{\n\tstruct xilinx_dpdma_tx_desc *tx_desc;\n\tstruct xilinx_dpdma_sw_desc *sw_desc;\n\tstruct xilinx_dpdma_hw_desc *hw_desc;\n\tsize_t hsize = xt->sgl[0].size;\n\tsize_t stride = hsize + xt->sgl[0].icg;\n\n\tif (!IS_ALIGNED(xt->src_start, XILINX_DPDMA_ALIGN_BYTES)) {\n\t\tdev_err(chan->xdev->dev,\n\t\t\t\"chan%u: buffer should be aligned at %d B\\n\",\n\t\t\tchan->id, XILINX_DPDMA_ALIGN_BYTES);\n\t\treturn NULL;\n\t}\n\n\ttx_desc = xilinx_dpdma_chan_alloc_tx_desc(chan);\n\tif (!tx_desc)\n\t\treturn NULL;\n\n\tsw_desc = xilinx_dpdma_chan_alloc_sw_desc(chan);\n\tif (!sw_desc) {\n\t\txilinx_dpdma_chan_free_tx_desc(&tx_desc->vdesc);\n\t\treturn NULL;\n\t}\n\n\txilinx_dpdma_sw_desc_set_dma_addrs(chan->xdev, sw_desc, sw_desc,\n\t\t\t\t\t   &xt->src_start, 1);\n\n\thw_desc = &sw_desc->hw;\n\thsize = ALIGN(hsize, XILINX_DPDMA_LINESIZE_ALIGN_BITS / 8);\n\thw_desc->xfer_size = hsize * xt->numf;\n\thw_desc->hsize_stride =\n\t\tFIELD_PREP(XILINX_DPDMA_DESC_HSIZE_STRIDE_HSIZE_MASK, hsize) |\n\t\tFIELD_PREP(XILINX_DPDMA_DESC_HSIZE_STRIDE_STRIDE_MASK,\n\t\t\t   stride / 16);\n\thw_desc->control |= XILINX_DPDMA_DESC_CONTROL_PREEMBLE;\n\thw_desc->control |= XILINX_DPDMA_DESC_CONTROL_COMPLETE_INTR;\n\thw_desc->control |= XILINX_DPDMA_DESC_CONTROL_IGNORE_DONE;\n\thw_desc->control |= XILINX_DPDMA_DESC_CONTROL_LAST_OF_FRAME;\n\n\tlist_add_tail(&sw_desc->node, &tx_desc->descriptors);\n\n\treturn tx_desc;\n}\n\n \n\n \nstatic void xilinx_dpdma_chan_enable(struct xilinx_dpdma_chan *chan)\n{\n\tu32 reg;\n\n\treg = (XILINX_DPDMA_INTR_CHAN_MASK << chan->id)\n\t    | XILINX_DPDMA_INTR_GLOBAL_MASK;\n\tdpdma_write(chan->xdev->reg, XILINX_DPDMA_IEN, reg);\n\treg = (XILINX_DPDMA_EINTR_CHAN_ERR_MASK << chan->id)\n\t    | XILINX_DPDMA_INTR_GLOBAL_ERR;\n\tdpdma_write(chan->xdev->reg, XILINX_DPDMA_EIEN, reg);\n\n\treg = XILINX_DPDMA_CH_CNTL_ENABLE\n\t    | FIELD_PREP(XILINX_DPDMA_CH_CNTL_QOS_DSCR_WR_MASK,\n\t\t\t XILINX_DPDMA_CH_CNTL_QOS_VID_CLASS)\n\t    | FIELD_PREP(XILINX_DPDMA_CH_CNTL_QOS_DSCR_RD_MASK,\n\t\t\t XILINX_DPDMA_CH_CNTL_QOS_VID_CLASS)\n\t    | FIELD_PREP(XILINX_DPDMA_CH_CNTL_QOS_DATA_RD_MASK,\n\t\t\t XILINX_DPDMA_CH_CNTL_QOS_VID_CLASS);\n\tdpdma_set(chan->reg, XILINX_DPDMA_CH_CNTL, reg);\n}\n\n \nstatic void xilinx_dpdma_chan_disable(struct xilinx_dpdma_chan *chan)\n{\n\tu32 reg;\n\n\treg = XILINX_DPDMA_INTR_CHAN_MASK << chan->id;\n\tdpdma_write(chan->xdev->reg, XILINX_DPDMA_IEN, reg);\n\treg = XILINX_DPDMA_EINTR_CHAN_ERR_MASK << chan->id;\n\tdpdma_write(chan->xdev->reg, XILINX_DPDMA_EIEN, reg);\n\n\tdpdma_clr(chan->reg, XILINX_DPDMA_CH_CNTL, XILINX_DPDMA_CH_CNTL_ENABLE);\n}\n\n \nstatic void xilinx_dpdma_chan_pause(struct xilinx_dpdma_chan *chan)\n{\n\tdpdma_set(chan->reg, XILINX_DPDMA_CH_CNTL, XILINX_DPDMA_CH_CNTL_PAUSE);\n}\n\n \nstatic void xilinx_dpdma_chan_unpause(struct xilinx_dpdma_chan *chan)\n{\n\tdpdma_clr(chan->reg, XILINX_DPDMA_CH_CNTL, XILINX_DPDMA_CH_CNTL_PAUSE);\n}\n\nstatic u32 xilinx_dpdma_chan_video_group_ready(struct xilinx_dpdma_chan *chan)\n{\n\tstruct xilinx_dpdma_device *xdev = chan->xdev;\n\tu32 channels = 0;\n\tunsigned int i;\n\n\tfor (i = ZYNQMP_DPDMA_VIDEO0; i <= ZYNQMP_DPDMA_VIDEO2; i++) {\n\t\tif (xdev->chan[i]->video_group && !xdev->chan[i]->running)\n\t\t\treturn 0;\n\n\t\tif (xdev->chan[i]->video_group)\n\t\t\tchannels |= BIT(i);\n\t}\n\n\treturn channels;\n}\n\n \nstatic void xilinx_dpdma_chan_queue_transfer(struct xilinx_dpdma_chan *chan)\n{\n\tstruct xilinx_dpdma_device *xdev = chan->xdev;\n\tstruct xilinx_dpdma_sw_desc *sw_desc;\n\tstruct xilinx_dpdma_tx_desc *desc;\n\tstruct virt_dma_desc *vdesc;\n\tu32 reg, channels;\n\tbool first_frame;\n\n\tlockdep_assert_held(&chan->lock);\n\n\tif (chan->desc.pending)\n\t\treturn;\n\n\tif (!chan->running) {\n\t\txilinx_dpdma_chan_unpause(chan);\n\t\txilinx_dpdma_chan_enable(chan);\n\t\tchan->first_frame = true;\n\t\tchan->running = true;\n\t}\n\n\tvdesc = vchan_next_desc(&chan->vchan);\n\tif (!vdesc)\n\t\treturn;\n\n\tdesc = to_dpdma_tx_desc(vdesc);\n\tchan->desc.pending = desc;\n\tlist_del(&desc->vdesc.node);\n\n\t \n\tlist_for_each_entry(sw_desc, &desc->descriptors, node)\n\t\tsw_desc->hw.desc_id = desc->vdesc.tx.cookie\n\t\t\t\t    & XILINX_DPDMA_CH_DESC_ID_MASK;\n\n\tsw_desc = list_first_entry(&desc->descriptors,\n\t\t\t\t   struct xilinx_dpdma_sw_desc, node);\n\tdpdma_write(chan->reg, XILINX_DPDMA_CH_DESC_START_ADDR,\n\t\t    lower_32_bits(sw_desc->dma_addr));\n\tif (xdev->ext_addr)\n\t\tdpdma_write(chan->reg, XILINX_DPDMA_CH_DESC_START_ADDRE,\n\t\t\t    FIELD_PREP(XILINX_DPDMA_CH_DESC_START_ADDRE_MASK,\n\t\t\t\t       upper_32_bits(sw_desc->dma_addr)));\n\n\tfirst_frame = chan->first_frame;\n\tchan->first_frame = false;\n\n\tif (chan->video_group) {\n\t\tchannels = xilinx_dpdma_chan_video_group_ready(chan);\n\t\t \n\t\tif (!channels)\n\t\t\treturn;\n\t} else {\n\t\tchannels = BIT(chan->id);\n\t}\n\n\tif (first_frame)\n\t\treg = XILINX_DPDMA_GBL_TRIG_MASK(channels);\n\telse\n\t\treg = XILINX_DPDMA_GBL_RETRIG_MASK(channels);\n\n\tdpdma_write(xdev->reg, XILINX_DPDMA_GBL, reg);\n}\n\n \nstatic u32 xilinx_dpdma_chan_ostand(struct xilinx_dpdma_chan *chan)\n{\n\treturn FIELD_GET(XILINX_DPDMA_CH_STATUS_OTRAN_CNT_MASK,\n\t\t\t dpdma_read(chan->reg, XILINX_DPDMA_CH_STATUS));\n}\n\n \nstatic int xilinx_dpdma_chan_notify_no_ostand(struct xilinx_dpdma_chan *chan)\n{\n\tu32 cnt;\n\n\tcnt = xilinx_dpdma_chan_ostand(chan);\n\tif (cnt) {\n\t\tdev_dbg(chan->xdev->dev,\n\t\t\t\"chan%u: %d outstanding transactions\\n\",\n\t\t\tchan->id, cnt);\n\t\treturn -EWOULDBLOCK;\n\t}\n\n\t \n\tdpdma_write(chan->xdev->reg, XILINX_DPDMA_IDS,\n\t\t    XILINX_DPDMA_INTR_NO_OSTAND(chan->id));\n\twake_up(&chan->wait_to_stop);\n\n\treturn 0;\n}\n\n \nstatic int xilinx_dpdma_chan_wait_no_ostand(struct xilinx_dpdma_chan *chan)\n{\n\tint ret;\n\n\t \n\tret = wait_event_interruptible_timeout(chan->wait_to_stop,\n\t\t\t\t\t       !xilinx_dpdma_chan_ostand(chan),\n\t\t\t\t\t       msecs_to_jiffies(50));\n\tif (ret > 0) {\n\t\tdpdma_write(chan->xdev->reg, XILINX_DPDMA_IEN,\n\t\t\t    XILINX_DPDMA_INTR_NO_OSTAND(chan->id));\n\t\treturn 0;\n\t}\n\n\tdev_err(chan->xdev->dev, \"chan%u: not ready to stop: %d trans\\n\",\n\t\tchan->id, xilinx_dpdma_chan_ostand(chan));\n\n\tif (ret == 0)\n\t\treturn -ETIMEDOUT;\n\n\treturn ret;\n}\n\n \nstatic int xilinx_dpdma_chan_poll_no_ostand(struct xilinx_dpdma_chan *chan)\n{\n\tu32 cnt, loop = 50000;\n\n\t \n\tdo {\n\t\tcnt = xilinx_dpdma_chan_ostand(chan);\n\t\tudelay(1);\n\t} while (loop-- > 0 && cnt);\n\n\tif (loop) {\n\t\tdpdma_write(chan->xdev->reg, XILINX_DPDMA_IEN,\n\t\t\t    XILINX_DPDMA_INTR_NO_OSTAND(chan->id));\n\t\treturn 0;\n\t}\n\n\tdev_err(chan->xdev->dev, \"chan%u: not ready to stop: %d trans\\n\",\n\t\tchan->id, xilinx_dpdma_chan_ostand(chan));\n\n\treturn -ETIMEDOUT;\n}\n\n \nstatic int xilinx_dpdma_chan_stop(struct xilinx_dpdma_chan *chan)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tret = xilinx_dpdma_chan_wait_no_ostand(chan);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\txilinx_dpdma_chan_disable(chan);\n\tchan->running = false;\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn 0;\n}\n\n \nstatic void xilinx_dpdma_chan_done_irq(struct xilinx_dpdma_chan *chan)\n{\n\tstruct xilinx_dpdma_tx_desc *active;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\txilinx_dpdma_debugfs_desc_done_irq(chan);\n\n\tactive = chan->desc.active;\n\tif (active)\n\t\tvchan_cyclic_callback(&active->vdesc);\n\telse\n\t\tdev_warn(chan->xdev->dev,\n\t\t\t \"chan%u: DONE IRQ with no active descriptor!\\n\",\n\t\t\t chan->id);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\n \nstatic void xilinx_dpdma_chan_vsync_irq(struct  xilinx_dpdma_chan *chan)\n{\n\tstruct xilinx_dpdma_tx_desc *pending;\n\tstruct xilinx_dpdma_sw_desc *sw_desc;\n\tunsigned long flags;\n\tu32 desc_id;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tpending = chan->desc.pending;\n\tif (!chan->running || !pending)\n\t\tgoto out;\n\n\tdesc_id = dpdma_read(chan->reg, XILINX_DPDMA_CH_DESC_ID)\n\t\t& XILINX_DPDMA_CH_DESC_ID_MASK;\n\n\t \n\tsw_desc = list_first_entry(&pending->descriptors,\n\t\t\t\t   struct xilinx_dpdma_sw_desc, node);\n\tif (sw_desc->hw.desc_id != desc_id) {\n\t\tdev_dbg(chan->xdev->dev,\n\t\t\t\"chan%u: vsync race lost (%u != %u), retrying\\n\",\n\t\t\tchan->id, sw_desc->hw.desc_id, desc_id);\n\t\tgoto out;\n\t}\n\n\t \n\tif (chan->desc.active)\n\t\tvchan_cookie_complete(&chan->desc.active->vdesc);\n\tchan->desc.active = pending;\n\tchan->desc.pending = NULL;\n\n\txilinx_dpdma_chan_queue_transfer(chan);\n\nout:\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\n \nstatic bool\nxilinx_dpdma_chan_err(struct xilinx_dpdma_chan *chan, u32 isr, u32 eisr)\n{\n\tif (!chan)\n\t\treturn false;\n\n\tif (chan->running &&\n\t    ((isr & (XILINX_DPDMA_INTR_CHAN_ERR_MASK << chan->id)) ||\n\t    (eisr & (XILINX_DPDMA_EINTR_CHAN_ERR_MASK << chan->id))))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void xilinx_dpdma_chan_handle_err(struct xilinx_dpdma_chan *chan)\n{\n\tstruct xilinx_dpdma_device *xdev = chan->xdev;\n\tstruct xilinx_dpdma_tx_desc *active;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tdev_dbg(xdev->dev, \"chan%u: cur desc addr = 0x%04x%08x\\n\",\n\t\tchan->id,\n\t\tdpdma_read(chan->reg, XILINX_DPDMA_CH_DESC_START_ADDRE),\n\t\tdpdma_read(chan->reg, XILINX_DPDMA_CH_DESC_START_ADDR));\n\tdev_dbg(xdev->dev, \"chan%u: cur payload addr = 0x%04x%08x\\n\",\n\t\tchan->id,\n\t\tdpdma_read(chan->reg, XILINX_DPDMA_CH_PYLD_CUR_ADDRE),\n\t\tdpdma_read(chan->reg, XILINX_DPDMA_CH_PYLD_CUR_ADDR));\n\n\txilinx_dpdma_chan_disable(chan);\n\tchan->running = false;\n\n\tif (!chan->desc.active)\n\t\tgoto out_unlock;\n\n\tactive = chan->desc.active;\n\tchan->desc.active = NULL;\n\n\txilinx_dpdma_chan_dump_tx_desc(chan, active);\n\n\tif (active->error)\n\t\tdev_dbg(xdev->dev, \"chan%u: repeated error on desc\\n\",\n\t\t\tchan->id);\n\n\t \n\tif (!chan->desc.pending &&\n\t    list_empty(&chan->vchan.desc_issued)) {\n\t\tactive->error = true;\n\t\tlist_add_tail(&active->vdesc.node,\n\t\t\t      &chan->vchan.desc_issued);\n\t} else {\n\t\txilinx_dpdma_chan_free_tx_desc(&active->vdesc);\n\t}\n\nout_unlock:\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\n \n\nstatic struct dma_async_tx_descriptor *\nxilinx_dpdma_prep_interleaved_dma(struct dma_chan *dchan,\n\t\t\t\t  struct dma_interleaved_template *xt,\n\t\t\t\t  unsigned long flags)\n{\n\tstruct xilinx_dpdma_chan *chan = to_xilinx_chan(dchan);\n\tstruct xilinx_dpdma_tx_desc *desc;\n\n\tif (xt->dir != DMA_MEM_TO_DEV)\n\t\treturn NULL;\n\n\tif (!xt->numf || !xt->sgl[0].size)\n\t\treturn NULL;\n\n\tif (!(flags & DMA_PREP_REPEAT) || !(flags & DMA_PREP_LOAD_EOT))\n\t\treturn NULL;\n\n\tdesc = xilinx_dpdma_chan_prep_interleaved_dma(chan, xt);\n\tif (!desc)\n\t\treturn NULL;\n\n\tvchan_tx_prep(&chan->vchan, &desc->vdesc, flags | DMA_CTRL_ACK);\n\n\treturn &desc->vdesc.tx;\n}\n\n \nstatic int xilinx_dpdma_alloc_chan_resources(struct dma_chan *dchan)\n{\n\tstruct xilinx_dpdma_chan *chan = to_xilinx_chan(dchan);\n\tsize_t align = __alignof__(struct xilinx_dpdma_sw_desc);\n\n\tchan->desc_pool = dma_pool_create(dev_name(chan->xdev->dev),\n\t\t\t\t\t  chan->xdev->dev,\n\t\t\t\t\t  sizeof(struct xilinx_dpdma_sw_desc),\n\t\t\t\t\t  align, 0);\n\tif (!chan->desc_pool) {\n\t\tdev_err(chan->xdev->dev,\n\t\t\t\"chan%u: failed to allocate a descriptor pool\\n\",\n\t\t\tchan->id);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void xilinx_dpdma_free_chan_resources(struct dma_chan *dchan)\n{\n\tstruct xilinx_dpdma_chan *chan = to_xilinx_chan(dchan);\n\n\tvchan_free_chan_resources(&chan->vchan);\n\n\tdma_pool_destroy(chan->desc_pool);\n\tchan->desc_pool = NULL;\n}\n\nstatic void xilinx_dpdma_issue_pending(struct dma_chan *dchan)\n{\n\tstruct xilinx_dpdma_chan *chan = to_xilinx_chan(dchan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->vchan.lock, flags);\n\tif (vchan_issue_pending(&chan->vchan))\n\t\txilinx_dpdma_chan_queue_transfer(chan);\n\tspin_unlock_irqrestore(&chan->vchan.lock, flags);\n}\n\nstatic int xilinx_dpdma_config(struct dma_chan *dchan,\n\t\t\t       struct dma_slave_config *config)\n{\n\tstruct xilinx_dpdma_chan *chan = to_xilinx_chan(dchan);\n\tstruct xilinx_dpdma_peripheral_config *pconfig;\n\tunsigned long flags;\n\n\t \n\n\t \n\tpconfig = config->peripheral_config;\n\tif (WARN_ON(pconfig && config->peripheral_size != sizeof(*pconfig)))\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tif (chan->id <= ZYNQMP_DPDMA_VIDEO2 && pconfig)\n\t\tchan->video_group = pconfig->video_group;\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn 0;\n}\n\nstatic int xilinx_dpdma_pause(struct dma_chan *dchan)\n{\n\txilinx_dpdma_chan_pause(to_xilinx_chan(dchan));\n\n\treturn 0;\n}\n\nstatic int xilinx_dpdma_resume(struct dma_chan *dchan)\n{\n\txilinx_dpdma_chan_unpause(to_xilinx_chan(dchan));\n\n\treturn 0;\n}\n\n \nstatic int xilinx_dpdma_terminate_all(struct dma_chan *dchan)\n{\n\tstruct xilinx_dpdma_chan *chan = to_xilinx_chan(dchan);\n\tstruct xilinx_dpdma_device *xdev = chan->xdev;\n\tLIST_HEAD(descriptors);\n\tunsigned long flags;\n\tunsigned int i;\n\n\t \n\tif (chan->video_group) {\n\t\tfor (i = ZYNQMP_DPDMA_VIDEO0; i <= ZYNQMP_DPDMA_VIDEO2; i++) {\n\t\t\tif (xdev->chan[i]->video_group &&\n\t\t\t    xdev->chan[i]->running) {\n\t\t\t\txilinx_dpdma_chan_pause(xdev->chan[i]);\n\t\t\t\txdev->chan[i]->video_group = false;\n\t\t\t}\n\t\t}\n\t} else {\n\t\txilinx_dpdma_chan_pause(chan);\n\t}\n\n\t \n\tspin_lock_irqsave(&chan->vchan.lock, flags);\n\tvchan_get_all_descriptors(&chan->vchan, &descriptors);\n\tspin_unlock_irqrestore(&chan->vchan.lock, flags);\n\n\tvchan_dma_desc_free_list(&chan->vchan, &descriptors);\n\n\treturn 0;\n}\n\n \nstatic void xilinx_dpdma_synchronize(struct dma_chan *dchan)\n{\n\tstruct xilinx_dpdma_chan *chan = to_xilinx_chan(dchan);\n\tunsigned long flags;\n\n\txilinx_dpdma_chan_stop(chan);\n\n\tspin_lock_irqsave(&chan->vchan.lock, flags);\n\tif (chan->desc.pending) {\n\t\tvchan_terminate_vdesc(&chan->desc.pending->vdesc);\n\t\tchan->desc.pending = NULL;\n\t}\n\tif (chan->desc.active) {\n\t\tvchan_terminate_vdesc(&chan->desc.active->vdesc);\n\t\tchan->desc.active = NULL;\n\t}\n\tspin_unlock_irqrestore(&chan->vchan.lock, flags);\n\n\tvchan_synchronize(&chan->vchan);\n}\n\n \n\n \nstatic bool xilinx_dpdma_err(u32 isr, u32 eisr)\n{\n\tif (isr & XILINX_DPDMA_INTR_GLOBAL_ERR ||\n\t    eisr & XILINX_DPDMA_EINTR_GLOBAL_ERR)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void xilinx_dpdma_handle_err_irq(struct xilinx_dpdma_device *xdev,\n\t\t\t\t\tu32 isr, u32 eisr)\n{\n\tbool err = xilinx_dpdma_err(isr, eisr);\n\tunsigned int i;\n\n\tdev_dbg_ratelimited(xdev->dev,\n\t\t\t    \"error irq: isr = 0x%08x, eisr = 0x%08x\\n\",\n\t\t\t    isr, eisr);\n\n\t \n\tdpdma_write(xdev->reg, XILINX_DPDMA_IDS,\n\t\t    isr & ~XILINX_DPDMA_INTR_GLOBAL_ERR);\n\tdpdma_write(xdev->reg, XILINX_DPDMA_EIDS,\n\t\t    eisr & ~XILINX_DPDMA_EINTR_GLOBAL_ERR);\n\n\tfor (i = 0; i < ARRAY_SIZE(xdev->chan); i++)\n\t\tif (err || xilinx_dpdma_chan_err(xdev->chan[i], isr, eisr))\n\t\t\ttasklet_schedule(&xdev->chan[i]->err_task);\n}\n\n \nstatic void xilinx_dpdma_enable_irq(struct xilinx_dpdma_device *xdev)\n{\n\tdpdma_write(xdev->reg, XILINX_DPDMA_IEN, XILINX_DPDMA_INTR_ALL);\n\tdpdma_write(xdev->reg, XILINX_DPDMA_EIEN, XILINX_DPDMA_EINTR_ALL);\n}\n\n \nstatic void xilinx_dpdma_disable_irq(struct xilinx_dpdma_device *xdev)\n{\n\tdpdma_write(xdev->reg, XILINX_DPDMA_IDS, XILINX_DPDMA_INTR_ALL);\n\tdpdma_write(xdev->reg, XILINX_DPDMA_EIDS, XILINX_DPDMA_EINTR_ALL);\n}\n\n \nstatic void xilinx_dpdma_chan_err_task(struct tasklet_struct *t)\n{\n\tstruct xilinx_dpdma_chan *chan = from_tasklet(chan, t, err_task);\n\tstruct xilinx_dpdma_device *xdev = chan->xdev;\n\tunsigned long flags;\n\n\t \n\txilinx_dpdma_chan_poll_no_ostand(chan);\n\n\txilinx_dpdma_chan_handle_err(chan);\n\n\tdpdma_write(xdev->reg, XILINX_DPDMA_IEN,\n\t\t    XILINX_DPDMA_INTR_CHAN_ERR_MASK << chan->id);\n\tdpdma_write(xdev->reg, XILINX_DPDMA_EIEN,\n\t\t    XILINX_DPDMA_EINTR_CHAN_ERR_MASK << chan->id);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\txilinx_dpdma_chan_queue_transfer(chan);\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\nstatic irqreturn_t xilinx_dpdma_irq_handler(int irq, void *data)\n{\n\tstruct xilinx_dpdma_device *xdev = data;\n\tunsigned long mask;\n\tunsigned int i;\n\tu32 status;\n\tu32 error;\n\n\tstatus = dpdma_read(xdev->reg, XILINX_DPDMA_ISR);\n\terror = dpdma_read(xdev->reg, XILINX_DPDMA_EISR);\n\tif (!status && !error)\n\t\treturn IRQ_NONE;\n\n\tdpdma_write(xdev->reg, XILINX_DPDMA_ISR, status);\n\tdpdma_write(xdev->reg, XILINX_DPDMA_EISR, error);\n\n\tif (status & XILINX_DPDMA_INTR_VSYNC) {\n\t\t \n\t\tfor (i = 0; i < ARRAY_SIZE(xdev->chan); i++) {\n\t\t\tstruct xilinx_dpdma_chan *chan = xdev->chan[i];\n\n\t\t\tif (chan)\n\t\t\t\txilinx_dpdma_chan_vsync_irq(chan);\n\t\t}\n\t}\n\n\tmask = FIELD_GET(XILINX_DPDMA_INTR_DESC_DONE_MASK, status);\n\tif (mask) {\n\t\tfor_each_set_bit(i, &mask, ARRAY_SIZE(xdev->chan))\n\t\t\txilinx_dpdma_chan_done_irq(xdev->chan[i]);\n\t}\n\n\tmask = FIELD_GET(XILINX_DPDMA_INTR_NO_OSTAND_MASK, status);\n\tif (mask) {\n\t\tfor_each_set_bit(i, &mask, ARRAY_SIZE(xdev->chan))\n\t\t\txilinx_dpdma_chan_notify_no_ostand(xdev->chan[i]);\n\t}\n\n\tmask = status & XILINX_DPDMA_INTR_ERR_ALL;\n\tif (mask || error)\n\t\txilinx_dpdma_handle_err_irq(xdev, mask, error);\n\n\treturn IRQ_HANDLED;\n}\n\n \n\nstatic int xilinx_dpdma_chan_init(struct xilinx_dpdma_device *xdev,\n\t\t\t\t  unsigned int chan_id)\n{\n\tstruct xilinx_dpdma_chan *chan;\n\n\tchan = devm_kzalloc(xdev->dev, sizeof(*chan), GFP_KERNEL);\n\tif (!chan)\n\t\treturn -ENOMEM;\n\n\tchan->id = chan_id;\n\tchan->reg = xdev->reg + XILINX_DPDMA_CH_BASE\n\t\t  + XILINX_DPDMA_CH_OFFSET * chan->id;\n\tchan->running = false;\n\tchan->xdev = xdev;\n\n\tspin_lock_init(&chan->lock);\n\tinit_waitqueue_head(&chan->wait_to_stop);\n\n\ttasklet_setup(&chan->err_task, xilinx_dpdma_chan_err_task);\n\n\tchan->vchan.desc_free = xilinx_dpdma_chan_free_tx_desc;\n\tvchan_init(&chan->vchan, &xdev->common);\n\n\txdev->chan[chan->id] = chan;\n\n\treturn 0;\n}\n\nstatic void xilinx_dpdma_chan_remove(struct xilinx_dpdma_chan *chan)\n{\n\tif (!chan)\n\t\treturn;\n\n\ttasklet_kill(&chan->err_task);\n\tlist_del(&chan->vchan.chan.device_node);\n}\n\nstatic struct dma_chan *of_dma_xilinx_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t\t    struct of_dma *ofdma)\n{\n\tstruct xilinx_dpdma_device *xdev = ofdma->of_dma_data;\n\tu32 chan_id = dma_spec->args[0];\n\n\tif (chan_id >= ARRAY_SIZE(xdev->chan))\n\t\treturn NULL;\n\n\tif (!xdev->chan[chan_id])\n\t\treturn NULL;\n\n\treturn dma_get_slave_channel(&xdev->chan[chan_id]->vchan.chan);\n}\n\nstatic void dpdma_hw_init(struct xilinx_dpdma_device *xdev)\n{\n\tunsigned int i;\n\tvoid __iomem *reg;\n\n\t \n\txilinx_dpdma_disable_irq(xdev);\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(xdev->chan); i++) {\n\t\treg = xdev->reg + XILINX_DPDMA_CH_BASE\n\t\t\t\t+ XILINX_DPDMA_CH_OFFSET * i;\n\t\tdpdma_clr(reg, XILINX_DPDMA_CH_CNTL, XILINX_DPDMA_CH_CNTL_ENABLE);\n\t}\n\n\t \n\tdpdma_write(xdev->reg, XILINX_DPDMA_ISR, XILINX_DPDMA_INTR_ALL);\n\tdpdma_write(xdev->reg, XILINX_DPDMA_EISR, XILINX_DPDMA_EINTR_ALL);\n}\n\nstatic int xilinx_dpdma_probe(struct platform_device *pdev)\n{\n\tstruct xilinx_dpdma_device *xdev;\n\tstruct dma_device *ddev;\n\tunsigned int i;\n\tint ret;\n\n\txdev = devm_kzalloc(&pdev->dev, sizeof(*xdev), GFP_KERNEL);\n\tif (!xdev)\n\t\treturn -ENOMEM;\n\n\txdev->dev = &pdev->dev;\n\txdev->ext_addr = sizeof(dma_addr_t) > 4;\n\n\tINIT_LIST_HEAD(&xdev->common.channels);\n\n\tplatform_set_drvdata(pdev, xdev);\n\n\txdev->axi_clk = devm_clk_get(xdev->dev, \"axi_clk\");\n\tif (IS_ERR(xdev->axi_clk))\n\t\treturn PTR_ERR(xdev->axi_clk);\n\n\txdev->reg = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(xdev->reg))\n\t\treturn PTR_ERR(xdev->reg);\n\n\tdpdma_hw_init(xdev);\n\n\txdev->irq = platform_get_irq(pdev, 0);\n\tif (xdev->irq < 0)\n\t\treturn xdev->irq;\n\n\tret = request_irq(xdev->irq, xilinx_dpdma_irq_handler, IRQF_SHARED,\n\t\t\t  dev_name(xdev->dev), xdev);\n\tif (ret) {\n\t\tdev_err(xdev->dev, \"failed to request IRQ\\n\");\n\t\treturn ret;\n\t}\n\n\tddev = &xdev->common;\n\tddev->dev = &pdev->dev;\n\n\tdma_cap_set(DMA_SLAVE, ddev->cap_mask);\n\tdma_cap_set(DMA_PRIVATE, ddev->cap_mask);\n\tdma_cap_set(DMA_INTERLEAVE, ddev->cap_mask);\n\tdma_cap_set(DMA_REPEAT, ddev->cap_mask);\n\tdma_cap_set(DMA_LOAD_EOT, ddev->cap_mask);\n\tddev->copy_align = fls(XILINX_DPDMA_ALIGN_BYTES - 1);\n\n\tddev->device_alloc_chan_resources = xilinx_dpdma_alloc_chan_resources;\n\tddev->device_free_chan_resources = xilinx_dpdma_free_chan_resources;\n\tddev->device_prep_interleaved_dma = xilinx_dpdma_prep_interleaved_dma;\n\t \n\tddev->device_tx_status = dma_cookie_status;\n\tddev->device_issue_pending = xilinx_dpdma_issue_pending;\n\tddev->device_config = xilinx_dpdma_config;\n\tddev->device_pause = xilinx_dpdma_pause;\n\tddev->device_resume = xilinx_dpdma_resume;\n\tddev->device_terminate_all = xilinx_dpdma_terminate_all;\n\tddev->device_synchronize = xilinx_dpdma_synchronize;\n\tddev->src_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_UNDEFINED);\n\tddev->directions = BIT(DMA_MEM_TO_DEV);\n\tddev->residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\n\tfor (i = 0; i < ARRAY_SIZE(xdev->chan); ++i) {\n\t\tret = xilinx_dpdma_chan_init(xdev, i);\n\t\tif (ret < 0) {\n\t\t\tdev_err(xdev->dev, \"failed to initialize channel %u\\n\",\n\t\t\t\ti);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tret = clk_prepare_enable(xdev->axi_clk);\n\tif (ret) {\n\t\tdev_err(xdev->dev, \"failed to enable the axi clock\\n\");\n\t\tgoto error;\n\t}\n\n\tret = dma_async_device_register(ddev);\n\tif (ret) {\n\t\tdev_err(xdev->dev, \"failed to register the dma device\\n\");\n\t\tgoto error_dma_async;\n\t}\n\n\tret = of_dma_controller_register(xdev->dev->of_node,\n\t\t\t\t\t of_dma_xilinx_xlate, ddev);\n\tif (ret) {\n\t\tdev_err(xdev->dev, \"failed to register DMA to DT DMA helper\\n\");\n\t\tgoto error_of_dma;\n\t}\n\n\txilinx_dpdma_enable_irq(xdev);\n\n\txilinx_dpdma_debugfs_init(xdev);\n\n\tdev_info(&pdev->dev, \"Xilinx DPDMA engine is probed\\n\");\n\n\treturn 0;\n\nerror_of_dma:\n\tdma_async_device_unregister(ddev);\nerror_dma_async:\n\tclk_disable_unprepare(xdev->axi_clk);\nerror:\n\tfor (i = 0; i < ARRAY_SIZE(xdev->chan); i++)\n\t\txilinx_dpdma_chan_remove(xdev->chan[i]);\n\n\tfree_irq(xdev->irq, xdev);\n\n\treturn ret;\n}\n\nstatic int xilinx_dpdma_remove(struct platform_device *pdev)\n{\n\tstruct xilinx_dpdma_device *xdev = platform_get_drvdata(pdev);\n\tunsigned int i;\n\n\t \n\tfree_irq(xdev->irq, xdev);\n\n\txilinx_dpdma_disable_irq(xdev);\n\tof_dma_controller_free(pdev->dev.of_node);\n\tdma_async_device_unregister(&xdev->common);\n\tclk_disable_unprepare(xdev->axi_clk);\n\n\tfor (i = 0; i < ARRAY_SIZE(xdev->chan); i++)\n\t\txilinx_dpdma_chan_remove(xdev->chan[i]);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id xilinx_dpdma_of_match[] = {\n\t{ .compatible = \"xlnx,zynqmp-dpdma\",},\n\t{   },\n};\nMODULE_DEVICE_TABLE(of, xilinx_dpdma_of_match);\n\nstatic struct platform_driver xilinx_dpdma_driver = {\n\t.probe\t\t\t= xilinx_dpdma_probe,\n\t.remove\t\t\t= xilinx_dpdma_remove,\n\t.driver\t\t\t= {\n\t\t.name\t\t= \"xilinx-zynqmp-dpdma\",\n\t\t.of_match_table\t= xilinx_dpdma_of_match,\n\t},\n};\n\nmodule_platform_driver(xilinx_dpdma_driver);\n\nMODULE_AUTHOR(\"Xilinx, Inc.\");\nMODULE_DESCRIPTION(\"Xilinx ZynqMP DPDMA driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}