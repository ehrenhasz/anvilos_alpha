{
  "module_name": "fsldma.c",
  "hash_id": "5a5519516ab9da207f3d52e03764700eb12b43156d3d10ccb30f285210beb2ba",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/fsldma.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/dmaengine.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/of_irq.h>\n#include <linux/platform_device.h>\n#include <linux/fsldma.h>\n#include \"dmaengine.h\"\n#include \"fsldma.h\"\n\n#define chan_dbg(chan, fmt, arg...)\t\t\t\t\t\\\n\tdev_dbg(chan->dev, \"%s: \" fmt, chan->name, ##arg)\n#define chan_err(chan, fmt, arg...)\t\t\t\t\t\\\n\tdev_err(chan->dev, \"%s: \" fmt, chan->name, ##arg)\n\nstatic const char msg_ld_oom[] = \"No free memory for link descriptor\";\n\n \n\nstatic void set_sr(struct fsldma_chan *chan, u32 val)\n{\n\tFSL_DMA_OUT(chan, &chan->regs->sr, val, 32);\n}\n\nstatic u32 get_sr(struct fsldma_chan *chan)\n{\n\treturn FSL_DMA_IN(chan, &chan->regs->sr, 32);\n}\n\nstatic void set_mr(struct fsldma_chan *chan, u32 val)\n{\n\tFSL_DMA_OUT(chan, &chan->regs->mr, val, 32);\n}\n\nstatic u32 get_mr(struct fsldma_chan *chan)\n{\n\treturn FSL_DMA_IN(chan, &chan->regs->mr, 32);\n}\n\nstatic void set_cdar(struct fsldma_chan *chan, dma_addr_t addr)\n{\n\tFSL_DMA_OUT(chan, &chan->regs->cdar, addr | FSL_DMA_SNEN, 64);\n}\n\nstatic dma_addr_t get_cdar(struct fsldma_chan *chan)\n{\n\treturn FSL_DMA_IN(chan, &chan->regs->cdar, 64) & ~FSL_DMA_SNEN;\n}\n\nstatic void set_bcr(struct fsldma_chan *chan, u32 val)\n{\n\tFSL_DMA_OUT(chan, &chan->regs->bcr, val, 32);\n}\n\nstatic u32 get_bcr(struct fsldma_chan *chan)\n{\n\treturn FSL_DMA_IN(chan, &chan->regs->bcr, 32);\n}\n\n \n\nstatic void set_desc_cnt(struct fsldma_chan *chan,\n\t\t\t\tstruct fsl_dma_ld_hw *hw, u32 count)\n{\n\thw->count = CPU_TO_DMA(chan, count, 32);\n}\n\nstatic void set_desc_src(struct fsldma_chan *chan,\n\t\t\t struct fsl_dma_ld_hw *hw, dma_addr_t src)\n{\n\tu64 snoop_bits;\n\n\tsnoop_bits = ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX)\n\t\t? ((u64)FSL_DMA_SATR_SREADTYPE_SNOOP_READ << 32) : 0;\n\thw->src_addr = CPU_TO_DMA(chan, snoop_bits | src, 64);\n}\n\nstatic void set_desc_dst(struct fsldma_chan *chan,\n\t\t\t struct fsl_dma_ld_hw *hw, dma_addr_t dst)\n{\n\tu64 snoop_bits;\n\n\tsnoop_bits = ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX)\n\t\t? ((u64)FSL_DMA_DATR_DWRITETYPE_SNOOP_WRITE << 32) : 0;\n\thw->dst_addr = CPU_TO_DMA(chan, snoop_bits | dst, 64);\n}\n\nstatic void set_desc_next(struct fsldma_chan *chan,\n\t\t\t  struct fsl_dma_ld_hw *hw, dma_addr_t next)\n{\n\tu64 snoop_bits;\n\n\tsnoop_bits = ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_83XX)\n\t\t? FSL_DMA_SNEN : 0;\n\thw->next_ln_addr = CPU_TO_DMA(chan, snoop_bits | next, 64);\n}\n\nstatic void set_ld_eol(struct fsldma_chan *chan, struct fsl_desc_sw *desc)\n{\n\tu64 snoop_bits;\n\n\tsnoop_bits = ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_83XX)\n\t\t? FSL_DMA_SNEN : 0;\n\n\tdesc->hw.next_ln_addr = CPU_TO_DMA(chan,\n\t\tDMA_TO_CPU(chan, desc->hw.next_ln_addr, 64) | FSL_DMA_EOL\n\t\t\t| snoop_bits, 64);\n}\n\n \n\nstatic void dma_init(struct fsldma_chan *chan)\n{\n\t \n\tset_mr(chan, 0);\n\n\tswitch (chan->feature & FSL_DMA_IP_MASK) {\n\tcase FSL_DMA_IP_85XX:\n\t\t \n\t\tset_mr(chan, FSL_DMA_MR_BWC | FSL_DMA_MR_EIE\n\t\t\t| FSL_DMA_MR_EOLNIE);\n\t\tbreak;\n\tcase FSL_DMA_IP_83XX:\n\t\t \n\t\tset_mr(chan, FSL_DMA_MR_EOTIE | FSL_DMA_MR_PRC_RM);\n\t\tbreak;\n\t}\n}\n\nstatic int dma_is_idle(struct fsldma_chan *chan)\n{\n\tu32 sr = get_sr(chan);\n\treturn (!(sr & FSL_DMA_SR_CB)) || (sr & FSL_DMA_SR_CH);\n}\n\n \nstatic void dma_start(struct fsldma_chan *chan)\n{\n\tu32 mode;\n\n\tmode = get_mr(chan);\n\n\tif (chan->feature & FSL_DMA_CHAN_PAUSE_EXT) {\n\t\tset_bcr(chan, 0);\n\t\tmode |= FSL_DMA_MR_EMP_EN;\n\t} else {\n\t\tmode &= ~FSL_DMA_MR_EMP_EN;\n\t}\n\n\tif (chan->feature & FSL_DMA_CHAN_START_EXT) {\n\t\tmode |= FSL_DMA_MR_EMS_EN;\n\t} else {\n\t\tmode &= ~FSL_DMA_MR_EMS_EN;\n\t\tmode |= FSL_DMA_MR_CS;\n\t}\n\n\tset_mr(chan, mode);\n}\n\nstatic void dma_halt(struct fsldma_chan *chan)\n{\n\tu32 mode;\n\tint i;\n\n\t \n\tmode = get_mr(chan);\n\n\t \n\tif ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX) {\n\t\tmode |= FSL_DMA_MR_CA;\n\t\tset_mr(chan, mode);\n\n\t\tmode &= ~FSL_DMA_MR_CA;\n\t}\n\n\t \n\tmode &= ~(FSL_DMA_MR_CS | FSL_DMA_MR_EMS_EN);\n\tset_mr(chan, mode);\n\n\t \n\tfor (i = 0; i < 100; i++) {\n\t\tif (dma_is_idle(chan))\n\t\t\treturn;\n\n\t\tudelay(10);\n\t}\n\n\tif (!dma_is_idle(chan))\n\t\tchan_err(chan, \"DMA halt timeout!\\n\");\n}\n\n \nstatic void fsl_chan_set_src_loop_size(struct fsldma_chan *chan, int size)\n{\n\tu32 mode;\n\n\tmode = get_mr(chan);\n\n\tswitch (size) {\n\tcase 0:\n\t\tmode &= ~FSL_DMA_MR_SAHE;\n\t\tbreak;\n\tcase 1:\n\tcase 2:\n\tcase 4:\n\tcase 8:\n\t\tmode &= ~FSL_DMA_MR_SAHTS_MASK;\n\t\tmode |= FSL_DMA_MR_SAHE | (__ilog2(size) << 14);\n\t\tbreak;\n\t}\n\n\tset_mr(chan, mode);\n}\n\n \nstatic void fsl_chan_set_dst_loop_size(struct fsldma_chan *chan, int size)\n{\n\tu32 mode;\n\n\tmode = get_mr(chan);\n\n\tswitch (size) {\n\tcase 0:\n\t\tmode &= ~FSL_DMA_MR_DAHE;\n\t\tbreak;\n\tcase 1:\n\tcase 2:\n\tcase 4:\n\tcase 8:\n\t\tmode &= ~FSL_DMA_MR_DAHTS_MASK;\n\t\tmode |= FSL_DMA_MR_DAHE | (__ilog2(size) << 16);\n\t\tbreak;\n\t}\n\n\tset_mr(chan, mode);\n}\n\n \nstatic void fsl_chan_set_request_count(struct fsldma_chan *chan, int size)\n{\n\tu32 mode;\n\n\tBUG_ON(size > 1024);\n\n\tmode = get_mr(chan);\n\tmode &= ~FSL_DMA_MR_BWC_MASK;\n\tmode |= (__ilog2(size) << 24) & FSL_DMA_MR_BWC_MASK;\n\n\tset_mr(chan, mode);\n}\n\n \nstatic void fsl_chan_toggle_ext_pause(struct fsldma_chan *chan, int enable)\n{\n\tif (enable)\n\t\tchan->feature |= FSL_DMA_CHAN_PAUSE_EXT;\n\telse\n\t\tchan->feature &= ~FSL_DMA_CHAN_PAUSE_EXT;\n}\n\n \nstatic void fsl_chan_toggle_ext_start(struct fsldma_chan *chan, int enable)\n{\n\tif (enable)\n\t\tchan->feature |= FSL_DMA_CHAN_START_EXT;\n\telse\n\t\tchan->feature &= ~FSL_DMA_CHAN_START_EXT;\n}\n\nint fsl_dma_external_start(struct dma_chan *dchan, int enable)\n{\n\tstruct fsldma_chan *chan;\n\n\tif (!dchan)\n\t\treturn -EINVAL;\n\n\tchan = to_fsl_chan(dchan);\n\n\tfsl_chan_toggle_ext_start(chan, enable);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(fsl_dma_external_start);\n\nstatic void append_ld_queue(struct fsldma_chan *chan, struct fsl_desc_sw *desc)\n{\n\tstruct fsl_desc_sw *tail = to_fsl_desc(chan->ld_pending.prev);\n\n\tif (list_empty(&chan->ld_pending))\n\t\tgoto out_splice;\n\n\t \n\tset_desc_next(chan, &tail->hw, desc->async_tx.phys);\n\n\t \nout_splice:\n\tlist_splice_tail_init(&desc->tx_list, &chan->ld_pending);\n}\n\nstatic dma_cookie_t fsl_dma_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct fsldma_chan *chan = to_fsl_chan(tx->chan);\n\tstruct fsl_desc_sw *desc = tx_to_fsl_desc(tx);\n\tstruct fsl_desc_sw *child;\n\tdma_cookie_t cookie = -EINVAL;\n\n\tspin_lock_bh(&chan->desc_lock);\n\n#ifdef CONFIG_PM\n\tif (unlikely(chan->pm_state != RUNNING)) {\n\t\tchan_dbg(chan, \"cannot submit due to suspend\\n\");\n\t\tspin_unlock_bh(&chan->desc_lock);\n\t\treturn -1;\n\t}\n#endif\n\n\t \n\tlist_for_each_entry(child, &desc->tx_list, node) {\n\t\tcookie = dma_cookie_assign(&child->async_tx);\n\t}\n\n\t \n\tappend_ld_queue(chan, desc);\n\n\tspin_unlock_bh(&chan->desc_lock);\n\n\treturn cookie;\n}\n\n \nstatic void fsl_dma_free_descriptor(struct fsldma_chan *chan,\n\t\tstruct fsl_desc_sw *desc)\n{\n\tlist_del(&desc->node);\n\tchan_dbg(chan, \"LD %p free\\n\", desc);\n\tdma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);\n}\n\n \nstatic struct fsl_desc_sw *fsl_dma_alloc_descriptor(struct fsldma_chan *chan)\n{\n\tstruct fsl_desc_sw *desc;\n\tdma_addr_t pdesc;\n\n\tdesc = dma_pool_zalloc(chan->desc_pool, GFP_ATOMIC, &pdesc);\n\tif (!desc) {\n\t\tchan_dbg(chan, \"out of memory for link descriptor\\n\");\n\t\treturn NULL;\n\t}\n\n\tINIT_LIST_HEAD(&desc->tx_list);\n\tdma_async_tx_descriptor_init(&desc->async_tx, &chan->common);\n\tdesc->async_tx.tx_submit = fsl_dma_tx_submit;\n\tdesc->async_tx.phys = pdesc;\n\n\tchan_dbg(chan, \"LD %p allocated\\n\", desc);\n\n\treturn desc;\n}\n\n \nstatic void fsldma_clean_completed_descriptor(struct fsldma_chan *chan)\n{\n\tstruct fsl_desc_sw *desc, *_desc;\n\n\t \n\tlist_for_each_entry_safe(desc, _desc, &chan->ld_completed, node)\n\t\tif (async_tx_test_ack(&desc->async_tx))\n\t\t\tfsl_dma_free_descriptor(chan, desc);\n}\n\n \nstatic dma_cookie_t fsldma_run_tx_complete_actions(struct fsldma_chan *chan,\n\t\tstruct fsl_desc_sw *desc, dma_cookie_t cookie)\n{\n\tstruct dma_async_tx_descriptor *txd = &desc->async_tx;\n\tdma_cookie_t ret = cookie;\n\n\tBUG_ON(txd->cookie < 0);\n\n\tif (txd->cookie > 0) {\n\t\tret = txd->cookie;\n\n\t\tdma_descriptor_unmap(txd);\n\t\t \n\t\tdmaengine_desc_get_callback_invoke(txd, NULL);\n\t}\n\n\t \n\tdma_run_dependencies(txd);\n\n\treturn ret;\n}\n\n \nstatic void fsldma_clean_running_descriptor(struct fsldma_chan *chan,\n\t\tstruct fsl_desc_sw *desc)\n{\n\t \n\tlist_del(&desc->node);\n\n\t \n\tif (!async_tx_test_ack(&desc->async_tx)) {\n\t\t \n\t\tlist_add_tail(&desc->node, &chan->ld_completed);\n\t\treturn;\n\t}\n\n\tdma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);\n}\n\n \nstatic void fsl_chan_xfer_ld_queue(struct fsldma_chan *chan)\n{\n\tstruct fsl_desc_sw *desc;\n\n\t \n\tif (list_empty(&chan->ld_pending)) {\n\t\tchan_dbg(chan, \"no pending LDs\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (!chan->idle) {\n\t\tchan_dbg(chan, \"DMA controller still busy\\n\");\n\t\treturn;\n\t}\n\n\t \n\n\t \n\tchan_dbg(chan, \"idle, starting controller\\n\");\n\tdesc = list_first_entry(&chan->ld_pending, struct fsl_desc_sw, node);\n\tlist_splice_tail_init(&chan->ld_pending, &chan->ld_running);\n\n\t \n\tif ((chan->feature & FSL_DMA_IP_MASK) == FSL_DMA_IP_85XX) {\n\t\tu32 mode;\n\n\t\tmode = get_mr(chan);\n\t\tmode &= ~FSL_DMA_MR_CS;\n\t\tset_mr(chan, mode);\n\t}\n\n\t \n\tset_cdar(chan, desc->async_tx.phys);\n\tget_cdar(chan);\n\n\tdma_start(chan);\n\tchan->idle = false;\n}\n\n \nstatic void fsldma_cleanup_descriptors(struct fsldma_chan *chan)\n{\n\tstruct fsl_desc_sw *desc, *_desc;\n\tdma_cookie_t cookie = 0;\n\tdma_addr_t curr_phys = get_cdar(chan);\n\tint seen_current = 0;\n\n\tfsldma_clean_completed_descriptor(chan);\n\n\t \n\tlist_for_each_entry_safe(desc, _desc, &chan->ld_running, node) {\n\t\t \n\t\tif (seen_current)\n\t\t\tbreak;\n\n\t\t \n\t\tif (desc->async_tx.phys == curr_phys) {\n\t\t\tseen_current = 1;\n\t\t\tif (!dma_is_idle(chan))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tcookie = fsldma_run_tx_complete_actions(chan, desc, cookie);\n\n\t\tfsldma_clean_running_descriptor(chan, desc);\n\t}\n\n\t \n\tfsl_chan_xfer_ld_queue(chan);\n\n\tif (cookie > 0)\n\t\tchan->common.completed_cookie = cookie;\n}\n\n \nstatic int fsl_dma_alloc_chan_resources(struct dma_chan *dchan)\n{\n\tstruct fsldma_chan *chan = to_fsl_chan(dchan);\n\n\t \n\tif (chan->desc_pool)\n\t\treturn 1;\n\n\t \n\tchan->desc_pool = dma_pool_create(chan->name, chan->dev,\n\t\t\t\t\t  sizeof(struct fsl_desc_sw),\n\t\t\t\t\t  __alignof__(struct fsl_desc_sw), 0);\n\tif (!chan->desc_pool) {\n\t\tchan_err(chan, \"unable to allocate descriptor pool\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\treturn 1;\n}\n\n \nstatic void fsldma_free_desc_list(struct fsldma_chan *chan,\n\t\t\t\t  struct list_head *list)\n{\n\tstruct fsl_desc_sw *desc, *_desc;\n\n\tlist_for_each_entry_safe(desc, _desc, list, node)\n\t\tfsl_dma_free_descriptor(chan, desc);\n}\n\nstatic void fsldma_free_desc_list_reverse(struct fsldma_chan *chan,\n\t\t\t\t\t  struct list_head *list)\n{\n\tstruct fsl_desc_sw *desc, *_desc;\n\n\tlist_for_each_entry_safe_reverse(desc, _desc, list, node)\n\t\tfsl_dma_free_descriptor(chan, desc);\n}\n\n \nstatic void fsl_dma_free_chan_resources(struct dma_chan *dchan)\n{\n\tstruct fsldma_chan *chan = to_fsl_chan(dchan);\n\n\tchan_dbg(chan, \"free all channel resources\\n\");\n\tspin_lock_bh(&chan->desc_lock);\n\tfsldma_cleanup_descriptors(chan);\n\tfsldma_free_desc_list(chan, &chan->ld_pending);\n\tfsldma_free_desc_list(chan, &chan->ld_running);\n\tfsldma_free_desc_list(chan, &chan->ld_completed);\n\tspin_unlock_bh(&chan->desc_lock);\n\n\tdma_pool_destroy(chan->desc_pool);\n\tchan->desc_pool = NULL;\n}\n\nstatic struct dma_async_tx_descriptor *\nfsl_dma_prep_memcpy(struct dma_chan *dchan,\n\tdma_addr_t dma_dst, dma_addr_t dma_src,\n\tsize_t len, unsigned long flags)\n{\n\tstruct fsldma_chan *chan;\n\tstruct fsl_desc_sw *first = NULL, *prev = NULL, *new;\n\tsize_t copy;\n\n\tif (!dchan)\n\t\treturn NULL;\n\n\tif (!len)\n\t\treturn NULL;\n\n\tchan = to_fsl_chan(dchan);\n\n\tdo {\n\n\t\t \n\t\tnew = fsl_dma_alloc_descriptor(chan);\n\t\tif (!new) {\n\t\t\tchan_err(chan, \"%s\\n\", msg_ld_oom);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tcopy = min(len, (size_t)FSL_DMA_BCR_MAX_CNT);\n\n\t\tset_desc_cnt(chan, &new->hw, copy);\n\t\tset_desc_src(chan, &new->hw, dma_src);\n\t\tset_desc_dst(chan, &new->hw, dma_dst);\n\n\t\tif (!first)\n\t\t\tfirst = new;\n\t\telse\n\t\t\tset_desc_next(chan, &prev->hw, new->async_tx.phys);\n\n\t\tnew->async_tx.cookie = 0;\n\t\tasync_tx_ack(&new->async_tx);\n\n\t\tprev = new;\n\t\tlen -= copy;\n\t\tdma_src += copy;\n\t\tdma_dst += copy;\n\n\t\t \n\t\tlist_add_tail(&new->node, &first->tx_list);\n\t} while (len);\n\n\tnew->async_tx.flags = flags;  \n\tnew->async_tx.cookie = -EBUSY;\n\n\t \n\tset_ld_eol(chan, new);\n\n\treturn &first->async_tx;\n\nfail:\n\tif (!first)\n\t\treturn NULL;\n\n\tfsldma_free_desc_list_reverse(chan, &first->tx_list);\n\treturn NULL;\n}\n\nstatic int fsl_dma_device_terminate_all(struct dma_chan *dchan)\n{\n\tstruct fsldma_chan *chan;\n\n\tif (!dchan)\n\t\treturn -EINVAL;\n\n\tchan = to_fsl_chan(dchan);\n\n\tspin_lock_bh(&chan->desc_lock);\n\n\t \n\tdma_halt(chan);\n\n\t \n\tfsldma_free_desc_list(chan, &chan->ld_pending);\n\tfsldma_free_desc_list(chan, &chan->ld_running);\n\tfsldma_free_desc_list(chan, &chan->ld_completed);\n\tchan->idle = true;\n\n\tspin_unlock_bh(&chan->desc_lock);\n\treturn 0;\n}\n\nstatic int fsl_dma_device_config(struct dma_chan *dchan,\n\t\t\t\t struct dma_slave_config *config)\n{\n\tstruct fsldma_chan *chan;\n\tint size;\n\n\tif (!dchan)\n\t\treturn -EINVAL;\n\n\tchan = to_fsl_chan(dchan);\n\n\t \n\tif (!chan->set_request_count)\n\t\treturn -ENXIO;\n\n\t \n\tif (config->direction == DMA_MEM_TO_DEV)\n\t\tsize = config->dst_addr_width * config->dst_maxburst;\n\telse\n\t\tsize = config->src_addr_width * config->src_maxburst;\n\n\tchan->set_request_count(chan, size);\n\treturn 0;\n}\n\n\n \nstatic void fsl_dma_memcpy_issue_pending(struct dma_chan *dchan)\n{\n\tstruct fsldma_chan *chan = to_fsl_chan(dchan);\n\n\tspin_lock_bh(&chan->desc_lock);\n\tfsl_chan_xfer_ld_queue(chan);\n\tspin_unlock_bh(&chan->desc_lock);\n}\n\n \nstatic enum dma_status fsl_tx_status(struct dma_chan *dchan,\n\t\t\t\t\tdma_cookie_t cookie,\n\t\t\t\t\tstruct dma_tx_state *txstate)\n{\n\tstruct fsldma_chan *chan = to_fsl_chan(dchan);\n\tenum dma_status ret;\n\n\tret = dma_cookie_status(dchan, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tspin_lock_bh(&chan->desc_lock);\n\tfsldma_cleanup_descriptors(chan);\n\tspin_unlock_bh(&chan->desc_lock);\n\n\treturn dma_cookie_status(dchan, cookie, txstate);\n}\n\n \n \n \n\nstatic irqreturn_t fsldma_chan_irq(int irq, void *data)\n{\n\tstruct fsldma_chan *chan = data;\n\tu32 stat;\n\n\t \n\tstat = get_sr(chan);\n\tset_sr(chan, stat);\n\tchan_dbg(chan, \"irq: stat = 0x%x\\n\", stat);\n\n\t \n\tstat &= ~(FSL_DMA_SR_CB | FSL_DMA_SR_CH);\n\tif (!stat)\n\t\treturn IRQ_NONE;\n\n\tif (stat & FSL_DMA_SR_TE)\n\t\tchan_err(chan, \"Transfer Error!\\n\");\n\n\t \n\tif (stat & FSL_DMA_SR_PE) {\n\t\tchan_dbg(chan, \"irq: Programming Error INT\\n\");\n\t\tstat &= ~FSL_DMA_SR_PE;\n\t\tif (get_bcr(chan) != 0)\n\t\t\tchan_err(chan, \"Programming Error!\\n\");\n\t}\n\n\t \n\tif (stat & FSL_DMA_SR_EOCDI) {\n\t\tchan_dbg(chan, \"irq: End-of-Chain link INT\\n\");\n\t\tstat &= ~FSL_DMA_SR_EOCDI;\n\t}\n\n\t \n\tif (stat & FSL_DMA_SR_EOLNI) {\n\t\tchan_dbg(chan, \"irq: End-of-link INT\\n\");\n\t\tstat &= ~FSL_DMA_SR_EOLNI;\n\t}\n\n\t \n\tif (!dma_is_idle(chan))\n\t\tchan_err(chan, \"irq: controller not idle!\\n\");\n\n\t \n\tif (stat)\n\t\tchan_err(chan, \"irq: unhandled sr 0x%08x\\n\", stat);\n\n\t \n\ttasklet_schedule(&chan->tasklet);\n\tchan_dbg(chan, \"irq: Exit\\n\");\n\treturn IRQ_HANDLED;\n}\n\nstatic void dma_do_tasklet(struct tasklet_struct *t)\n{\n\tstruct fsldma_chan *chan = from_tasklet(chan, t, tasklet);\n\n\tchan_dbg(chan, \"tasklet entry\\n\");\n\n\tspin_lock(&chan->desc_lock);\n\n\t \n\tchan->idle = true;\n\n\t \n\tfsldma_cleanup_descriptors(chan);\n\n\tspin_unlock(&chan->desc_lock);\n\n\tchan_dbg(chan, \"tasklet exit\\n\");\n}\n\nstatic irqreturn_t fsldma_ctrl_irq(int irq, void *data)\n{\n\tstruct fsldma_device *fdev = data;\n\tstruct fsldma_chan *chan;\n\tunsigned int handled = 0;\n\tu32 gsr, mask;\n\tint i;\n\n\tgsr = (fdev->feature & FSL_DMA_BIG_ENDIAN) ? in_be32(fdev->regs)\n\t\t\t\t\t\t   : in_le32(fdev->regs);\n\tmask = 0xff000000;\n\tdev_dbg(fdev->dev, \"IRQ: gsr 0x%.8x\\n\", gsr);\n\n\tfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\n\t\tchan = fdev->chan[i];\n\t\tif (!chan)\n\t\t\tcontinue;\n\n\t\tif (gsr & mask) {\n\t\t\tdev_dbg(fdev->dev, \"IRQ: chan %d\\n\", chan->id);\n\t\t\tfsldma_chan_irq(irq, chan);\n\t\t\thandled++;\n\t\t}\n\n\t\tgsr &= ~mask;\n\t\tmask >>= 8;\n\t}\n\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic void fsldma_free_irqs(struct fsldma_device *fdev)\n{\n\tstruct fsldma_chan *chan;\n\tint i;\n\n\tif (fdev->irq) {\n\t\tdev_dbg(fdev->dev, \"free per-controller IRQ\\n\");\n\t\tfree_irq(fdev->irq, fdev);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\n\t\tchan = fdev->chan[i];\n\t\tif (chan && chan->irq) {\n\t\t\tchan_dbg(chan, \"free per-channel IRQ\\n\");\n\t\t\tfree_irq(chan->irq, chan);\n\t\t}\n\t}\n}\n\nstatic int fsldma_request_irqs(struct fsldma_device *fdev)\n{\n\tstruct fsldma_chan *chan;\n\tint ret;\n\tint i;\n\n\t \n\tif (fdev->irq) {\n\t\tdev_dbg(fdev->dev, \"request per-controller IRQ\\n\");\n\t\tret = request_irq(fdev->irq, fsldma_ctrl_irq, IRQF_SHARED,\n\t\t\t\t  \"fsldma-controller\", fdev);\n\t\treturn ret;\n\t}\n\n\t \n\tfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\n\t\tchan = fdev->chan[i];\n\t\tif (!chan)\n\t\t\tcontinue;\n\n\t\tif (!chan->irq) {\n\t\t\tchan_err(chan, \"interrupts property missing in device tree\\n\");\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_unwind;\n\t\t}\n\n\t\tchan_dbg(chan, \"request per-channel IRQ\\n\");\n\t\tret = request_irq(chan->irq, fsldma_chan_irq, IRQF_SHARED,\n\t\t\t\t  \"fsldma-chan\", chan);\n\t\tif (ret) {\n\t\t\tchan_err(chan, \"unable to request per-channel IRQ\\n\");\n\t\t\tgoto out_unwind;\n\t\t}\n\t}\n\n\treturn 0;\n\nout_unwind:\n\tfor ( ; i >= 0; i--) {\n\t\tchan = fdev->chan[i];\n\t\tif (!chan)\n\t\t\tcontinue;\n\n\t\tif (!chan->irq)\n\t\t\tcontinue;\n\n\t\tfree_irq(chan->irq, chan);\n\t}\n\n\treturn ret;\n}\n\n \n \n \n\nstatic int fsl_dma_chan_probe(struct fsldma_device *fdev,\n\tstruct device_node *node, u32 feature, const char *compatible)\n{\n\tstruct fsldma_chan *chan;\n\tstruct resource res;\n\tint err;\n\n\t \n\tchan = kzalloc(sizeof(*chan), GFP_KERNEL);\n\tif (!chan) {\n\t\terr = -ENOMEM;\n\t\tgoto out_return;\n\t}\n\n\t \n\tchan->regs = of_iomap(node, 0);\n\tif (!chan->regs) {\n\t\tdev_err(fdev->dev, \"unable to ioremap registers\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto out_free_chan;\n\t}\n\n\terr = of_address_to_resource(node, 0, &res);\n\tif (err) {\n\t\tdev_err(fdev->dev, \"unable to find 'reg' property\\n\");\n\t\tgoto out_iounmap_regs;\n\t}\n\n\tchan->feature = feature;\n\tif (!fdev->feature)\n\t\tfdev->feature = chan->feature;\n\n\t \n\tWARN_ON(fdev->feature != chan->feature);\n\n\tchan->dev = fdev->dev;\n\tchan->id = (res.start & 0xfff) < 0x300 ?\n\t\t   ((res.start - 0x100) & 0xfff) >> 7 :\n\t\t   ((res.start - 0x200) & 0xfff) >> 7;\n\tif (chan->id >= FSL_DMA_MAX_CHANS_PER_DEVICE) {\n\t\tdev_err(fdev->dev, \"too many channels for device\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out_iounmap_regs;\n\t}\n\n\tfdev->chan[chan->id] = chan;\n\ttasklet_setup(&chan->tasklet, dma_do_tasklet);\n\tsnprintf(chan->name, sizeof(chan->name), \"chan%d\", chan->id);\n\n\t \n\tdma_init(chan);\n\n\t \n\tset_cdar(chan, 0);\n\n\tswitch (chan->feature & FSL_DMA_IP_MASK) {\n\tcase FSL_DMA_IP_85XX:\n\t\tchan->toggle_ext_pause = fsl_chan_toggle_ext_pause;\n\t\tfallthrough;\n\tcase FSL_DMA_IP_83XX:\n\t\tchan->toggle_ext_start = fsl_chan_toggle_ext_start;\n\t\tchan->set_src_loop_size = fsl_chan_set_src_loop_size;\n\t\tchan->set_dst_loop_size = fsl_chan_set_dst_loop_size;\n\t\tchan->set_request_count = fsl_chan_set_request_count;\n\t}\n\n\tspin_lock_init(&chan->desc_lock);\n\tINIT_LIST_HEAD(&chan->ld_pending);\n\tINIT_LIST_HEAD(&chan->ld_running);\n\tINIT_LIST_HEAD(&chan->ld_completed);\n\tchan->idle = true;\n#ifdef CONFIG_PM\n\tchan->pm_state = RUNNING;\n#endif\n\n\tchan->common.device = &fdev->common;\n\tdma_cookie_init(&chan->common);\n\n\t \n\tchan->irq = irq_of_parse_and_map(node, 0);\n\n\t \n\tlist_add_tail(&chan->common.device_node, &fdev->common.channels);\n\n\tdev_info(fdev->dev, \"#%d (%s), irq %d\\n\", chan->id, compatible,\n\t\t chan->irq ? chan->irq : fdev->irq);\n\n\treturn 0;\n\nout_iounmap_regs:\n\tiounmap(chan->regs);\nout_free_chan:\n\tkfree(chan);\nout_return:\n\treturn err;\n}\n\nstatic void fsl_dma_chan_remove(struct fsldma_chan *chan)\n{\n\tirq_dispose_mapping(chan->irq);\n\tlist_del(&chan->common.device_node);\n\tiounmap(chan->regs);\n\tkfree(chan);\n}\n\nstatic int fsldma_of_probe(struct platform_device *op)\n{\n\tstruct fsldma_device *fdev;\n\tstruct device_node *child;\n\tunsigned int i;\n\tint err;\n\n\tfdev = kzalloc(sizeof(*fdev), GFP_KERNEL);\n\tif (!fdev) {\n\t\terr = -ENOMEM;\n\t\tgoto out_return;\n\t}\n\n\tfdev->dev = &op->dev;\n\tINIT_LIST_HEAD(&fdev->common.channels);\n\n\t \n\tfdev->regs = of_iomap(op->dev.of_node, 0);\n\tif (!fdev->regs) {\n\t\tdev_err(&op->dev, \"unable to ioremap registers\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\t \n\tfdev->irq = irq_of_parse_and_map(op->dev.of_node, 0);\n\n\tdma_cap_set(DMA_MEMCPY, fdev->common.cap_mask);\n\tdma_cap_set(DMA_SLAVE, fdev->common.cap_mask);\n\tfdev->common.device_alloc_chan_resources = fsl_dma_alloc_chan_resources;\n\tfdev->common.device_free_chan_resources = fsl_dma_free_chan_resources;\n\tfdev->common.device_prep_dma_memcpy = fsl_dma_prep_memcpy;\n\tfdev->common.device_tx_status = fsl_tx_status;\n\tfdev->common.device_issue_pending = fsl_dma_memcpy_issue_pending;\n\tfdev->common.device_config = fsl_dma_device_config;\n\tfdev->common.device_terminate_all = fsl_dma_device_terminate_all;\n\tfdev->common.dev = &op->dev;\n\n\tfdev->common.src_addr_widths = FSL_DMA_BUSWIDTHS;\n\tfdev->common.dst_addr_widths = FSL_DMA_BUSWIDTHS;\n\tfdev->common.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\n\tfdev->common.residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\n\tdma_set_mask(&(op->dev), DMA_BIT_MASK(36));\n\n\tplatform_set_drvdata(op, fdev);\n\n\t \n\tfor_each_child_of_node(op->dev.of_node, child) {\n\t\tif (of_device_is_compatible(child, \"fsl,eloplus-dma-channel\")) {\n\t\t\tfsl_dma_chan_probe(fdev, child,\n\t\t\t\tFSL_DMA_IP_85XX | FSL_DMA_BIG_ENDIAN,\n\t\t\t\t\"fsl,eloplus-dma-channel\");\n\t\t}\n\n\t\tif (of_device_is_compatible(child, \"fsl,elo-dma-channel\")) {\n\t\t\tfsl_dma_chan_probe(fdev, child,\n\t\t\t\tFSL_DMA_IP_83XX | FSL_DMA_LITTLE_ENDIAN,\n\t\t\t\t\"fsl,elo-dma-channel\");\n\t\t}\n\t}\n\n\t \n\terr = fsldma_request_irqs(fdev);\n\tif (err) {\n\t\tdev_err(fdev->dev, \"unable to request IRQs\\n\");\n\t\tgoto out_free_fdev;\n\t}\n\n\tdma_async_device_register(&fdev->common);\n\treturn 0;\n\nout_free_fdev:\n\tfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\n\t\tif (fdev->chan[i])\n\t\t\tfsl_dma_chan_remove(fdev->chan[i]);\n\t}\n\tirq_dispose_mapping(fdev->irq);\n\tiounmap(fdev->regs);\nout_free:\n\tkfree(fdev);\nout_return:\n\treturn err;\n}\n\nstatic int fsldma_of_remove(struct platform_device *op)\n{\n\tstruct fsldma_device *fdev;\n\tunsigned int i;\n\n\tfdev = platform_get_drvdata(op);\n\tdma_async_device_unregister(&fdev->common);\n\n\tfsldma_free_irqs(fdev);\n\n\tfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\n\t\tif (fdev->chan[i])\n\t\t\tfsl_dma_chan_remove(fdev->chan[i]);\n\t}\n\tirq_dispose_mapping(fdev->irq);\n\n\tiounmap(fdev->regs);\n\tkfree(fdev);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_PM\nstatic int fsldma_suspend_late(struct device *dev)\n{\n\tstruct fsldma_device *fdev = dev_get_drvdata(dev);\n\tstruct fsldma_chan *chan;\n\tint i;\n\n\tfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\n\t\tchan = fdev->chan[i];\n\t\tif (!chan)\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(&chan->desc_lock);\n\t\tif (unlikely(!chan->idle))\n\t\t\tgoto out;\n\t\tchan->regs_save.mr = get_mr(chan);\n\t\tchan->pm_state = SUSPENDED;\n\t\tspin_unlock_bh(&chan->desc_lock);\n\t}\n\treturn 0;\n\nout:\n\tfor (; i >= 0; i--) {\n\t\tchan = fdev->chan[i];\n\t\tif (!chan)\n\t\t\tcontinue;\n\t\tchan->pm_state = RUNNING;\n\t\tspin_unlock_bh(&chan->desc_lock);\n\t}\n\treturn -EBUSY;\n}\n\nstatic int fsldma_resume_early(struct device *dev)\n{\n\tstruct fsldma_device *fdev = dev_get_drvdata(dev);\n\tstruct fsldma_chan *chan;\n\tu32 mode;\n\tint i;\n\n\tfor (i = 0; i < FSL_DMA_MAX_CHANS_PER_DEVICE; i++) {\n\t\tchan = fdev->chan[i];\n\t\tif (!chan)\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(&chan->desc_lock);\n\t\tmode = chan->regs_save.mr\n\t\t\t& ~FSL_DMA_MR_CS & ~FSL_DMA_MR_CC & ~FSL_DMA_MR_CA;\n\t\tset_mr(chan, mode);\n\t\tchan->pm_state = RUNNING;\n\t\tspin_unlock_bh(&chan->desc_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops fsldma_pm_ops = {\n\t.suspend_late\t= fsldma_suspend_late,\n\t.resume_early\t= fsldma_resume_early,\n};\n#endif\n\nstatic const struct of_device_id fsldma_of_ids[] = {\n\t{ .compatible = \"fsl,elo3-dma\", },\n\t{ .compatible = \"fsl,eloplus-dma\", },\n\t{ .compatible = \"fsl,elo-dma\", },\n\t{}\n};\nMODULE_DEVICE_TABLE(of, fsldma_of_ids);\n\nstatic struct platform_driver fsldma_of_driver = {\n\t.driver = {\n\t\t.name = \"fsl-elo-dma\",\n\t\t.of_match_table = fsldma_of_ids,\n#ifdef CONFIG_PM\n\t\t.pm = &fsldma_pm_ops,\n#endif\n\t},\n\t.probe = fsldma_of_probe,\n\t.remove = fsldma_of_remove,\n};\n\n \n \n \n\nstatic __init int fsldma_init(void)\n{\n\tpr_info(\"Freescale Elo series DMA driver\\n\");\n\treturn platform_driver_register(&fsldma_of_driver);\n}\n\nstatic void __exit fsldma_exit(void)\n{\n\tplatform_driver_unregister(&fsldma_of_driver);\n}\n\nsubsys_initcall(fsldma_init);\nmodule_exit(fsldma_exit);\n\nMODULE_DESCRIPTION(\"Freescale Elo series DMA driver\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}