{
  "module_name": "gpi.c",
  "hash_id": "aef1cc1055ea4d21a82655c99cc707e00ee8696b2bc94ba86f14820c82793840",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/qcom/gpi.c",
  "human_readable_source": "\n \n\n#include <dt-bindings/dma/qcom-gpi.h>\n#include <linux/bitfield.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/module.h>\n#include <linux/of_dma.h>\n#include <linux/platform_device.h>\n#include <linux/dma/qcom-gpi-dma.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n#include \"../dmaengine.h\"\n#include \"../virt-dma.h\"\n\n#define TRE_TYPE_DMA\t\t0x10\n#define TRE_TYPE_GO\t\t0x20\n#define TRE_TYPE_CONFIG0\t0x22\n\n \n#define TRE_FLAGS_CHAIN\t\tBIT(0)\n#define TRE_FLAGS_IEOB\t\tBIT(8)\n#define TRE_FLAGS_IEOT\t\tBIT(9)\n#define TRE_FLAGS_BEI\t\tBIT(10)\n#define TRE_FLAGS_LINK\t\tBIT(11)\n#define TRE_FLAGS_TYPE\t\tGENMASK(23, 16)\n\n \n#define TRE_SPI_C0_WORD_SZ\tGENMASK(4, 0)\n#define TRE_SPI_C0_LOOPBACK\tBIT(8)\n#define TRE_SPI_C0_CS\t\tBIT(11)\n#define TRE_SPI_C0_CPHA\t\tBIT(12)\n#define TRE_SPI_C0_CPOL\t\tBIT(13)\n#define TRE_SPI_C0_TX_PACK\tBIT(24)\n#define TRE_SPI_C0_RX_PACK\tBIT(25)\n\n \n#define TRE_C0_CLK_DIV\t\tGENMASK(11, 0)\n#define TRE_C0_CLK_SRC\t\tGENMASK(19, 16)\n\n \n#define TRE_SPI_GO_CMD\t\tGENMASK(4, 0)\n#define TRE_SPI_GO_CS\t\tGENMASK(10, 8)\n#define TRE_SPI_GO_FRAG\t\tBIT(26)\n\n \n#define TRE_RX_LEN\t\tGENMASK(23, 0)\n\n \n#define TRE_I2C_C0_TLOW\t\tGENMASK(7, 0)\n#define TRE_I2C_C0_THIGH\tGENMASK(15, 8)\n#define TRE_I2C_C0_TCYL\t\tGENMASK(23, 16)\n#define TRE_I2C_C0_TX_PACK\tBIT(24)\n#define TRE_I2C_C0_RX_PACK      BIT(25)\n\n \n#define TRE_I2C_GO_CMD          GENMASK(4, 0)\n#define TRE_I2C_GO_ADDR\t\tGENMASK(14, 8)\n#define TRE_I2C_GO_STRETCH\tBIT(26)\n\n \n#define TRE_DMA_LEN\t\tGENMASK(23, 0)\n\n \n#define GPII_n_CH_k_CNTXT_0_OFFS(n, k)\t(0x20000 + (0x4000 * (n)) + (0x80 * (k)))\n#define GPII_n_CH_k_CNTXT_0_EL_SIZE\tGENMASK(31, 24)\n#define GPII_n_CH_k_CNTXT_0_CHSTATE\tGENMASK(23, 20)\n#define GPII_n_CH_k_CNTXT_0_ERIDX\tGENMASK(18, 14)\n#define GPII_n_CH_k_CNTXT_0_DIR\t\tBIT(3)\n#define GPII_n_CH_k_CNTXT_0_PROTO\tGENMASK(2, 0)\n\n#define GPII_n_CH_k_CNTXT_0(el_size, erindex, dir, chtype_proto)  \\\n\t(FIELD_PREP(GPII_n_CH_k_CNTXT_0_EL_SIZE, el_size)\t| \\\n\t FIELD_PREP(GPII_n_CH_k_CNTXT_0_ERIDX, erindex)\t\t| \\\n\t FIELD_PREP(GPII_n_CH_k_CNTXT_0_DIR, dir)\t\t| \\\n\t FIELD_PREP(GPII_n_CH_k_CNTXT_0_PROTO, chtype_proto))\n\n#define GPI_CHTYPE_DIR_IN\t(0)\n#define GPI_CHTYPE_DIR_OUT\t(1)\n\n#define GPI_CHTYPE_PROTO_GPI\t(0x2)\n\n#define GPII_n_CH_k_DOORBELL_0_OFFS(n, k)\t(0x22000 + (0x4000 * (n)) + (0x8 * (k)))\n#define GPII_n_CH_CMD_OFFS(n)\t\t\t(0x23008 + (0x4000 * (n)))\n#define GPII_n_CH_CMD_OPCODE\t\t\tGENMASK(31, 24)\n#define GPII_n_CH_CMD_CHID\t\t\tGENMASK(7, 0)\n#define GPII_n_CH_CMD(opcode, chid)\t\t\t\t \\\n\t\t     (FIELD_PREP(GPII_n_CH_CMD_OPCODE, opcode) | \\\n\t\t      FIELD_PREP(GPII_n_CH_CMD_CHID, chid))\n\n#define GPII_n_CH_CMD_ALLOCATE\t\t(0)\n#define GPII_n_CH_CMD_START\t\t(1)\n#define GPII_n_CH_CMD_STOP\t\t(2)\n#define GPII_n_CH_CMD_RESET\t\t(9)\n#define GPII_n_CH_CMD_DE_ALLOC\t\t(10)\n#define GPII_n_CH_CMD_UART_SW_STALE\t(32)\n#define GPII_n_CH_CMD_UART_RFR_READY\t(33)\n#define GPII_n_CH_CMD_UART_RFR_NOT_READY (34)\n\n \n#define GPII_n_EV_CH_k_CNTXT_0_OFFS(n, k) (0x21000 + (0x4000 * (n)) + (0x80 * (k)))\n#define GPII_n_EV_k_CNTXT_0_EL_SIZE\tGENMASK(31, 24)\n#define GPII_n_EV_k_CNTXT_0_CHSTATE\tGENMASK(23, 20)\n#define GPII_n_EV_k_CNTXT_0_INTYPE\tBIT(16)\n#define GPII_n_EV_k_CNTXT_0_CHTYPE\tGENMASK(3, 0)\n\n#define GPII_n_EV_k_CNTXT_0(el_size, inttype, chtype)\t\t\\\n\t(FIELD_PREP(GPII_n_EV_k_CNTXT_0_EL_SIZE, el_size) |\t\\\n\t FIELD_PREP(GPII_n_EV_k_CNTXT_0_INTYPE, inttype)  |\t\\\n\t FIELD_PREP(GPII_n_EV_k_CNTXT_0_CHTYPE, chtype))\n\n#define GPI_INTTYPE_IRQ\t\t(1)\n#define GPI_CHTYPE_GPI_EV\t(0x2)\n\nenum CNTXT_OFFS {\n\tCNTXT_0_CONFIG = 0x0,\n\tCNTXT_1_R_LENGTH = 0x4,\n\tCNTXT_2_RING_BASE_LSB = 0x8,\n\tCNTXT_3_RING_BASE_MSB = 0xC,\n\tCNTXT_4_RING_RP_LSB = 0x10,\n\tCNTXT_5_RING_RP_MSB = 0x14,\n\tCNTXT_6_RING_WP_LSB = 0x18,\n\tCNTXT_7_RING_WP_MSB = 0x1C,\n\tCNTXT_8_RING_INT_MOD = 0x20,\n\tCNTXT_9_RING_INTVEC = 0x24,\n\tCNTXT_10_RING_MSI_LSB = 0x28,\n\tCNTXT_11_RING_MSI_MSB = 0x2C,\n\tCNTXT_12_RING_RP_UPDATE_LSB = 0x30,\n\tCNTXT_13_RING_RP_UPDATE_MSB = 0x34,\n};\n\n#define GPII_n_EV_CH_k_DOORBELL_0_OFFS(n, k)\t(0x22100 + (0x4000 * (n)) + (0x8 * (k)))\n#define GPII_n_EV_CH_CMD_OFFS(n)\t\t(0x23010 + (0x4000 * (n)))\n#define GPII_n_EV_CMD_OPCODE\t\t\tGENMASK(31, 24)\n#define GPII_n_EV_CMD_CHID\t\t\tGENMASK(7, 0)\n#define GPII_n_EV_CMD(opcode, chid)\t\t\t\t \\\n\t\t     (FIELD_PREP(GPII_n_EV_CMD_OPCODE, opcode) | \\\n\t\t      FIELD_PREP(GPII_n_EV_CMD_CHID, chid))\n\n#define GPII_n_EV_CH_CMD_ALLOCATE\t\t(0x00)\n#define GPII_n_EV_CH_CMD_RESET\t\t\t(0x09)\n#define GPII_n_EV_CH_CMD_DE_ALLOC\t\t(0x0A)\n\n#define GPII_n_CNTXT_TYPE_IRQ_OFFS(n)\t\t(0x23080 + (0x4000 * (n)))\n\n \n#define GPII_n_CNTXT_TYPE_IRQ_MSK_OFFS(n)\t(0x23088 + (0x4000 * (n)))\n#define GPII_n_CNTXT_TYPE_IRQ_MSK_BMSK\t\tGENMASK(6, 0)\n#define GPII_n_CNTXT_TYPE_IRQ_MSK_GENERAL\tBIT(6)\n#define GPII_n_CNTXT_TYPE_IRQ_MSK_IEOB\t\tBIT(3)\n#define GPII_n_CNTXT_TYPE_IRQ_MSK_GLOB\t\tBIT(2)\n#define GPII_n_CNTXT_TYPE_IRQ_MSK_EV_CTRL\tBIT(1)\n#define GPII_n_CNTXT_TYPE_IRQ_MSK_CH_CTRL\tBIT(0)\n\n#define GPII_n_CNTXT_SRC_GPII_CH_IRQ_OFFS(n)\t(0x23090 + (0x4000 * (n)))\n#define GPII_n_CNTXT_SRC_EV_CH_IRQ_OFFS(n)\t(0x23094 + (0x4000 * (n)))\n\n \n#define GPII_n_CNTXT_SRC_CH_IRQ_MSK_OFFS(n)\t(0x23098 + (0x4000 * (n)))\n#define GPII_n_CNTXT_SRC_CH_IRQ_MSK_BMSK\tGENMASK(1, 0)\n\n \n#define GPII_n_CNTXT_SRC_EV_CH_IRQ_MSK_OFFS(n)\t(0x2309C + (0x4000 * (n)))\n#define GPII_n_CNTXT_SRC_EV_CH_IRQ_MSK_BMSK\tBIT(0)\n\n#define GPII_n_CNTXT_SRC_CH_IRQ_CLR_OFFS(n)\t(0x230A0 + (0x4000 * (n)))\n#define GPII_n_CNTXT_SRC_EV_CH_IRQ_CLR_OFFS(n)\t(0x230A4 + (0x4000 * (n)))\n\n \n#define GPII_n_CNTXT_SRC_IEOB_IRQ_MSK_OFFS(n)\t(0x230B8 + (0x4000 * (n)))\n#define GPII_n_CNTXT_SRC_IEOB_IRQ_MSK_BMSK\tBIT(0)\n\n#define GPII_n_CNTXT_SRC_IEOB_IRQ_CLR_OFFS(n)\t(0x230C0 + (0x4000 * (n)))\n#define GPII_n_CNTXT_GLOB_IRQ_STTS_OFFS(n)\t(0x23100 + (0x4000 * (n)))\n#define GPI_GLOB_IRQ_ERROR_INT_MSK\t\tBIT(0)\n\n \n#define GPII_n_CNTXT_GLOB_IRQ_EN_OFFS(n)\t(0x23108 + (0x4000 * (n)))\n#define GPII_n_CNTXT_GLOB_IRQ_CLR_OFFS(n)\t(0x23110 + (0x4000 * (n)))\n#define GPII_n_CNTXT_GPII_IRQ_STTS_OFFS(n)\t(0x23118 + (0x4000 * (n)))\n\n \n#define GPII_n_CNTXT_GPII_IRQ_EN_OFFS(n)\t(0x23120 + (0x4000 * (n)))\n#define GPII_n_CNTXT_GPII_IRQ_EN_BMSK\t\tGENMASK(3, 0)\n\n#define GPII_n_CNTXT_GPII_IRQ_CLR_OFFS(n)\t(0x23128 + (0x4000 * (n)))\n\n \n#define GPII_n_CNTXT_INTSET_OFFS(n)\t\t(0x23180 + (0x4000 * (n)))\n#define GPII_n_CNTXT_INTSET_BMSK\t\tBIT(0)\n\n#define GPII_n_CNTXT_MSI_BASE_LSB_OFFS(n)\t(0x23188 + (0x4000 * (n)))\n#define GPII_n_CNTXT_MSI_BASE_MSB_OFFS(n)\t(0x2318C + (0x4000 * (n)))\n#define GPII_n_CNTXT_SCRATCH_0_OFFS(n)\t\t(0x23400 + (0x4000 * (n)))\n#define GPII_n_CNTXT_SCRATCH_1_OFFS(n)\t\t(0x23404 + (0x4000 * (n)))\n\n#define GPII_n_ERROR_LOG_OFFS(n)\t\t(0x23200 + (0x4000 * (n)))\n\n \n#define GPII_n_CH_k_QOS_OFFS(n, k)\t\t(0x2005C + (0x4000 * (n)) + (0x80 * (k)))\n\n \n#define GPII_n_CH_k_SCRATCH_0_OFFS(n, k)\t(0x20060 + (0x4000 * (n)) + (0x80 * (k)))\n#define GPII_n_CH_k_SCRATCH_0_SEID\t\tGENMASK(2, 0)\n#define GPII_n_CH_k_SCRATCH_0_PROTO\t\tGENMASK(7, 4)\n#define GPII_n_CH_k_SCRATCH_0_PAIR\t\tGENMASK(20, 16)\n#define GPII_n_CH_k_SCRATCH_0(pair, proto, seid)\t\t\\\n\t\t\t     (FIELD_PREP(GPII_n_CH_k_SCRATCH_0_PAIR, pair)\t| \\\n\t\t\t      FIELD_PREP(GPII_n_CH_k_SCRATCH_0_PROTO, proto)\t| \\\n\t\t\t      FIELD_PREP(GPII_n_CH_k_SCRATCH_0_SEID, seid))\n#define GPII_n_CH_k_SCRATCH_1_OFFS(n, k)\t(0x20064 + (0x4000 * (n)) + (0x80 * (k)))\n#define GPII_n_CH_k_SCRATCH_2_OFFS(n, k)\t(0x20068 + (0x4000 * (n)) + (0x80 * (k)))\n#define GPII_n_CH_k_SCRATCH_3_OFFS(n, k)\t(0x2006C + (0x4000 * (n)) + (0x80 * (k)))\n\nstruct __packed gpi_tre {\n\tu32 dword[4];\n};\n\nenum msm_gpi_tce_code {\n\tMSM_GPI_TCE_SUCCESS = 1,\n\tMSM_GPI_TCE_EOT = 2,\n\tMSM_GPI_TCE_EOB = 4,\n\tMSM_GPI_TCE_UNEXP_ERR = 16,\n};\n\n#define CMD_TIMEOUT_MS\t\t(250)\n\n#define MAX_CHANNELS_PER_GPII\t(2)\n#define GPI_TX_CHAN\t\t(0)\n#define GPI_RX_CHAN\t\t(1)\n#define STATE_IGNORE\t\t(U32_MAX)\n#define EV_FACTOR\t\t(2)\n#define REQ_OF_DMA_ARGS\t\t(5)  \n#define CHAN_TRES\t\t64\n\nstruct __packed xfer_compl_event {\n\tu64 ptr;\n\tu32 length:24;\n\tu8 code;\n\tu16 status;\n\tu8 type;\n\tu8 chid;\n};\n\nstruct __packed immediate_data_event {\n\tu8 data_bytes[8];\n\tu8 length:4;\n\tu8 resvd:4;\n\tu16 tre_index;\n\tu8 code;\n\tu16 status;\n\tu8 type;\n\tu8 chid;\n};\n\nstruct __packed qup_notif_event {\n\tu32 status;\n\tu32 time;\n\tu32 count:24;\n\tu8 resvd;\n\tu16 resvd1;\n\tu8 type;\n\tu8 chid;\n};\n\nstruct __packed gpi_ere {\n\tu32 dword[4];\n};\n\nenum GPI_EV_TYPE {\n\tXFER_COMPLETE_EV_TYPE = 0x22,\n\tIMMEDIATE_DATA_EV_TYPE = 0x30,\n\tQUP_NOTIF_EV_TYPE = 0x31,\n\tSTALE_EV_TYPE = 0xFF,\n};\n\nunion __packed gpi_event {\n\tstruct __packed xfer_compl_event xfer_compl_event;\n\tstruct __packed immediate_data_event immediate_data_event;\n\tstruct __packed qup_notif_event qup_notif_event;\n\tstruct __packed gpi_ere gpi_ere;\n};\n\nenum gpii_irq_settings {\n\tDEFAULT_IRQ_SETTINGS,\n\tMASK_IEOB_SETTINGS,\n};\n\nenum gpi_ev_state {\n\tDEFAULT_EV_CH_STATE = 0,\n\tEV_STATE_NOT_ALLOCATED = DEFAULT_EV_CH_STATE,\n\tEV_STATE_ALLOCATED,\n\tMAX_EV_STATES\n};\n\nstatic const char *const gpi_ev_state_str[MAX_EV_STATES] = {\n\t[EV_STATE_NOT_ALLOCATED] = \"NOT ALLOCATED\",\n\t[EV_STATE_ALLOCATED] = \"ALLOCATED\",\n};\n\n#define TO_GPI_EV_STATE_STR(_state) (((_state) >= MAX_EV_STATES) ? \\\n\t\t\t\t    \"INVALID\" : gpi_ev_state_str[(_state)])\n\nenum gpi_ch_state {\n\tDEFAULT_CH_STATE = 0x0,\n\tCH_STATE_NOT_ALLOCATED = DEFAULT_CH_STATE,\n\tCH_STATE_ALLOCATED = 0x1,\n\tCH_STATE_STARTED = 0x2,\n\tCH_STATE_STOPPED = 0x3,\n\tCH_STATE_STOP_IN_PROC = 0x4,\n\tCH_STATE_ERROR = 0xf,\n\tMAX_CH_STATES\n};\n\nenum gpi_cmd {\n\tGPI_CH_CMD_BEGIN,\n\tGPI_CH_CMD_ALLOCATE = GPI_CH_CMD_BEGIN,\n\tGPI_CH_CMD_START,\n\tGPI_CH_CMD_STOP,\n\tGPI_CH_CMD_RESET,\n\tGPI_CH_CMD_DE_ALLOC,\n\tGPI_CH_CMD_UART_SW_STALE,\n\tGPI_CH_CMD_UART_RFR_READY,\n\tGPI_CH_CMD_UART_RFR_NOT_READY,\n\tGPI_CH_CMD_END = GPI_CH_CMD_UART_RFR_NOT_READY,\n\tGPI_EV_CMD_BEGIN,\n\tGPI_EV_CMD_ALLOCATE = GPI_EV_CMD_BEGIN,\n\tGPI_EV_CMD_RESET,\n\tGPI_EV_CMD_DEALLOC,\n\tGPI_EV_CMD_END = GPI_EV_CMD_DEALLOC,\n\tGPI_MAX_CMD,\n};\n\n#define IS_CHAN_CMD(_cmd) ((_cmd) <= GPI_CH_CMD_END)\n\nstatic const char *const gpi_cmd_str[GPI_MAX_CMD] = {\n\t[GPI_CH_CMD_ALLOCATE] = \"CH ALLOCATE\",\n\t[GPI_CH_CMD_START] = \"CH START\",\n\t[GPI_CH_CMD_STOP] = \"CH STOP\",\n\t[GPI_CH_CMD_RESET] = \"CH_RESET\",\n\t[GPI_CH_CMD_DE_ALLOC] = \"DE ALLOC\",\n\t[GPI_CH_CMD_UART_SW_STALE] = \"UART SW STALE\",\n\t[GPI_CH_CMD_UART_RFR_READY] = \"UART RFR READY\",\n\t[GPI_CH_CMD_UART_RFR_NOT_READY] = \"UART RFR NOT READY\",\n\t[GPI_EV_CMD_ALLOCATE] = \"EV ALLOCATE\",\n\t[GPI_EV_CMD_RESET] = \"EV RESET\",\n\t[GPI_EV_CMD_DEALLOC] = \"EV DEALLOC\",\n};\n\n#define TO_GPI_CMD_STR(_cmd) (((_cmd) >= GPI_MAX_CMD) ? \"INVALID\" : \\\n\t\t\t\t  gpi_cmd_str[(_cmd)])\n\n \nenum gpi_pm_state {\n\tDISABLE_STATE,\n\tCONFIG_STATE,\n\tPREPARE_HARDWARE,\n\tACTIVE_STATE,\n\tPREPARE_TERMINATE,\n\tPAUSE_STATE,\n\tMAX_PM_STATE\n};\n\n#define REG_ACCESS_VALID(_pm_state) ((_pm_state) >= PREPARE_HARDWARE)\n\nstatic const char *const gpi_pm_state_str[MAX_PM_STATE] = {\n\t[DISABLE_STATE] = \"DISABLE\",\n\t[CONFIG_STATE] = \"CONFIG\",\n\t[PREPARE_HARDWARE] = \"PREPARE HARDWARE\",\n\t[ACTIVE_STATE] = \"ACTIVE\",\n\t[PREPARE_TERMINATE] = \"PREPARE TERMINATE\",\n\t[PAUSE_STATE] = \"PAUSE\",\n};\n\n#define TO_GPI_PM_STR(_state) (((_state) >= MAX_PM_STATE) ? \\\n\t\t\t      \"INVALID\" : gpi_pm_state_str[(_state)])\n\nstatic const struct {\n\tenum gpi_cmd gpi_cmd;\n\tu32 opcode;\n\tu32 state;\n} gpi_cmd_info[GPI_MAX_CMD] = {\n\t{\n\t\tGPI_CH_CMD_ALLOCATE,\n\t\tGPII_n_CH_CMD_ALLOCATE,\n\t\tCH_STATE_ALLOCATED,\n\t},\n\t{\n\t\tGPI_CH_CMD_START,\n\t\tGPII_n_CH_CMD_START,\n\t\tCH_STATE_STARTED,\n\t},\n\t{\n\t\tGPI_CH_CMD_STOP,\n\t\tGPII_n_CH_CMD_STOP,\n\t\tCH_STATE_STOPPED,\n\t},\n\t{\n\t\tGPI_CH_CMD_RESET,\n\t\tGPII_n_CH_CMD_RESET,\n\t\tCH_STATE_ALLOCATED,\n\t},\n\t{\n\t\tGPI_CH_CMD_DE_ALLOC,\n\t\tGPII_n_CH_CMD_DE_ALLOC,\n\t\tCH_STATE_NOT_ALLOCATED,\n\t},\n\t{\n\t\tGPI_CH_CMD_UART_SW_STALE,\n\t\tGPII_n_CH_CMD_UART_SW_STALE,\n\t\tSTATE_IGNORE,\n\t},\n\t{\n\t\tGPI_CH_CMD_UART_RFR_READY,\n\t\tGPII_n_CH_CMD_UART_RFR_READY,\n\t\tSTATE_IGNORE,\n\t},\n\t{\n\t\tGPI_CH_CMD_UART_RFR_NOT_READY,\n\t\tGPII_n_CH_CMD_UART_RFR_NOT_READY,\n\t\tSTATE_IGNORE,\n\t},\n\t{\n\t\tGPI_EV_CMD_ALLOCATE,\n\t\tGPII_n_EV_CH_CMD_ALLOCATE,\n\t\tEV_STATE_ALLOCATED,\n\t},\n\t{\n\t\tGPI_EV_CMD_RESET,\n\t\tGPII_n_EV_CH_CMD_RESET,\n\t\tEV_STATE_ALLOCATED,\n\t},\n\t{\n\t\tGPI_EV_CMD_DEALLOC,\n\t\tGPII_n_EV_CH_CMD_DE_ALLOC,\n\t\tEV_STATE_NOT_ALLOCATED,\n\t},\n};\n\nstruct gpi_ring {\n\tvoid *pre_aligned;\n\tsize_t alloc_size;\n\tphys_addr_t phys_addr;\n\tdma_addr_t dma_handle;\n\tvoid *base;\n\tvoid *wp;\n\tvoid *rp;\n\tu32 len;\n\tu32 el_size;\n\tu32 elements;\n\tbool configured;\n};\n\nstruct gpi_dev {\n\tstruct dma_device dma_device;\n\tstruct device *dev;\n\tstruct resource *res;\n\tvoid __iomem *regs;\n\tvoid __iomem *ee_base;  \n\tu32 max_gpii;  \n\tu32 gpii_mask;  \n\tu32 ev_factor;  \n\tstruct gpii *gpiis;\n};\n\nstruct reg_info {\n\tchar *name;\n\tu32 offset;\n\tu32 val;\n};\n\nstruct gchan {\n\tstruct virt_dma_chan vc;\n\tu32 chid;\n\tu32 seid;\n\tu32 protocol;\n\tstruct gpii *gpii;\n\tenum gpi_ch_state ch_state;\n\tenum gpi_pm_state pm_state;\n\tvoid __iomem *ch_cntxt_base_reg;\n\tvoid __iomem *ch_cntxt_db_reg;\n\tvoid __iomem *ch_cmd_reg;\n\tu32 dir;\n\tstruct gpi_ring ch_ring;\n\tvoid *config;\n};\n\nstruct gpii {\n\tu32 gpii_id;\n\tstruct gchan gchan[MAX_CHANNELS_PER_GPII];\n\tstruct gpi_dev *gpi_dev;\n\tint irq;\n\tvoid __iomem *regs;  \n\tvoid __iomem *ev_cntxt_base_reg;\n\tvoid __iomem *ev_cntxt_db_reg;\n\tvoid __iomem *ev_ring_rp_lsb_reg;\n\tvoid __iomem *ev_cmd_reg;\n\tvoid __iomem *ieob_clr_reg;\n\tstruct mutex ctrl_lock;\n\tenum gpi_ev_state ev_state;\n\tbool configured_irq;\n\tenum gpi_pm_state pm_state;\n\trwlock_t pm_lock;\n\tstruct gpi_ring ev_ring;\n\tstruct tasklet_struct ev_task;  \n\tstruct completion cmd_completion;\n\tenum gpi_cmd gpi_cmd;\n\tu32 cntxt_type_irq_msk;\n\tbool ieob_set;\n};\n\n#define MAX_TRE 3\n\nstruct gpi_desc {\n\tstruct virt_dma_desc vd;\n\tsize_t len;\n\tvoid *db;  \n\tstruct gchan *gchan;\n\tstruct gpi_tre tre[MAX_TRE];\n\tu32 num_tre;\n};\n\nstatic const u32 GPII_CHAN_DIR[MAX_CHANNELS_PER_GPII] = {\n\tGPI_CHTYPE_DIR_OUT, GPI_CHTYPE_DIR_IN\n};\n\nstatic irqreturn_t gpi_handle_irq(int irq, void *data);\nstatic void gpi_ring_recycle_ev_element(struct gpi_ring *ring);\nstatic int gpi_ring_add_element(struct gpi_ring *ring, void **wp);\nstatic void gpi_process_events(struct gpii *gpii);\n\nstatic inline struct gchan *to_gchan(struct dma_chan *dma_chan)\n{\n\treturn container_of(dma_chan, struct gchan, vc.chan);\n}\n\nstatic inline struct gpi_desc *to_gpi_desc(struct virt_dma_desc *vd)\n{\n\treturn container_of(vd, struct gpi_desc, vd);\n}\n\nstatic inline phys_addr_t to_physical(const struct gpi_ring *const ring,\n\t\t\t\t      void *addr)\n{\n\treturn ring->phys_addr + (addr - ring->base);\n}\n\nstatic inline void *to_virtual(const struct gpi_ring *const ring, phys_addr_t addr)\n{\n\treturn ring->base + (addr - ring->phys_addr);\n}\n\nstatic inline u32 gpi_read_reg(struct gpii *gpii, void __iomem *addr)\n{\n\treturn readl_relaxed(addr);\n}\n\nstatic inline void gpi_write_reg(struct gpii *gpii, void __iomem *addr, u32 val)\n{\n\twritel_relaxed(val, addr);\n}\n\n \nstatic inline void gpi_write_reg_field(struct gpii *gpii, void __iomem *addr,\n\t\t\t\t       u32 mask, u32 shift, u32 val)\n{\n\tu32 tmp = gpi_read_reg(gpii, addr);\n\n\ttmp &= ~mask;\n\tval = tmp | ((val << shift) & mask);\n\tgpi_write_reg(gpii, addr, val);\n}\n\nstatic __always_inline void\ngpi_update_reg(struct gpii *gpii, u32 offset, u32 mask, u32 val)\n{\n\tvoid __iomem *addr = gpii->regs + offset;\n\tu32 tmp = gpi_read_reg(gpii, addr);\n\n\ttmp &= ~mask;\n\ttmp |= u32_encode_bits(val, mask);\n\n\tgpi_write_reg(gpii, addr, tmp);\n}\n\nstatic void gpi_disable_interrupts(struct gpii *gpii)\n{\n\tgpi_update_reg(gpii, GPII_n_CNTXT_TYPE_IRQ_MSK_OFFS(gpii->gpii_id),\n\t\t       GPII_n_CNTXT_TYPE_IRQ_MSK_BMSK, 0);\n\tgpi_update_reg(gpii, GPII_n_CNTXT_SRC_IEOB_IRQ_MSK_OFFS(gpii->gpii_id),\n\t\t       GPII_n_CNTXT_SRC_IEOB_IRQ_MSK_BMSK, 0);\n\tgpi_update_reg(gpii, GPII_n_CNTXT_SRC_CH_IRQ_MSK_OFFS(gpii->gpii_id),\n\t\t       GPII_n_CNTXT_SRC_CH_IRQ_MSK_BMSK, 0);\n\tgpi_update_reg(gpii, GPII_n_CNTXT_SRC_EV_CH_IRQ_MSK_OFFS(gpii->gpii_id),\n\t\t       GPII_n_CNTXT_SRC_EV_CH_IRQ_MSK_BMSK, 0);\n\tgpi_update_reg(gpii, GPII_n_CNTXT_GLOB_IRQ_EN_OFFS(gpii->gpii_id),\n\t\t       GPII_n_CNTXT_GPII_IRQ_EN_BMSK, 0);\n\tgpi_update_reg(gpii, GPII_n_CNTXT_GPII_IRQ_EN_OFFS(gpii->gpii_id),\n\t\t       GPII_n_CNTXT_GPII_IRQ_EN_BMSK, 0);\n\tgpi_update_reg(gpii, GPII_n_CNTXT_INTSET_OFFS(gpii->gpii_id),\n\t\t       GPII_n_CNTXT_INTSET_BMSK, 0);\n\n\tgpii->cntxt_type_irq_msk = 0;\n\tdevm_free_irq(gpii->gpi_dev->dev, gpii->irq, gpii);\n\tgpii->configured_irq = false;\n}\n\n \nstatic int gpi_config_interrupts(struct gpii *gpii, enum gpii_irq_settings settings, bool mask)\n{\n\tconst u32 enable = (GPII_n_CNTXT_TYPE_IRQ_MSK_GENERAL |\n\t\t\t      GPII_n_CNTXT_TYPE_IRQ_MSK_IEOB |\n\t\t\t      GPII_n_CNTXT_TYPE_IRQ_MSK_GLOB |\n\t\t\t      GPII_n_CNTXT_TYPE_IRQ_MSK_EV_CTRL |\n\t\t\t      GPII_n_CNTXT_TYPE_IRQ_MSK_CH_CTRL);\n\tint ret;\n\n\tif (!gpii->configured_irq) {\n\t\tret = devm_request_irq(gpii->gpi_dev->dev, gpii->irq,\n\t\t\t\t       gpi_handle_irq, IRQF_TRIGGER_HIGH,\n\t\t\t\t       \"gpi-dma\", gpii);\n\t\tif (ret < 0) {\n\t\t\tdev_err(gpii->gpi_dev->dev, \"error request irq:%d ret:%d\\n\",\n\t\t\t\tgpii->irq, ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (settings == MASK_IEOB_SETTINGS) {\n\t\t \n\t\tif (mask)\n\t\t\tgpii->cntxt_type_irq_msk |= GPII_n_CNTXT_TYPE_IRQ_MSK_IEOB;\n\t\telse\n\t\t\tgpii->cntxt_type_irq_msk &= ~(GPII_n_CNTXT_TYPE_IRQ_MSK_IEOB);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_TYPE_IRQ_MSK_OFFS(gpii->gpii_id),\n\t\t\t       GPII_n_CNTXT_TYPE_IRQ_MSK_BMSK, gpii->cntxt_type_irq_msk);\n\t} else {\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_TYPE_IRQ_MSK_OFFS(gpii->gpii_id),\n\t\t\t       GPII_n_CNTXT_TYPE_IRQ_MSK_BMSK, enable);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_SRC_IEOB_IRQ_MSK_OFFS(gpii->gpii_id),\n\t\t\t       GPII_n_CNTXT_SRC_IEOB_IRQ_MSK_BMSK,\n\t\t\t       GPII_n_CNTXT_SRC_IEOB_IRQ_MSK_BMSK);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_SRC_CH_IRQ_MSK_OFFS(gpii->gpii_id),\n\t\t\t       GPII_n_CNTXT_SRC_CH_IRQ_MSK_BMSK,\n\t\t\t       GPII_n_CNTXT_SRC_CH_IRQ_MSK_BMSK);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_SRC_EV_CH_IRQ_MSK_OFFS(gpii->gpii_id),\n\t\t\t       GPII_n_CNTXT_SRC_EV_CH_IRQ_MSK_BMSK,\n\t\t\t       GPII_n_CNTXT_SRC_EV_CH_IRQ_MSK_BMSK);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_GLOB_IRQ_EN_OFFS(gpii->gpii_id),\n\t\t\t       GPII_n_CNTXT_GPII_IRQ_EN_BMSK,\n\t\t\t       GPII_n_CNTXT_GPII_IRQ_EN_BMSK);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_GPII_IRQ_EN_OFFS(gpii->gpii_id),\n\t\t\t       GPII_n_CNTXT_GPII_IRQ_EN_BMSK, GPII_n_CNTXT_GPII_IRQ_EN_BMSK);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_MSI_BASE_LSB_OFFS(gpii->gpii_id), U32_MAX, 0);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_MSI_BASE_MSB_OFFS(gpii->gpii_id), U32_MAX, 0);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_SCRATCH_0_OFFS(gpii->gpii_id), U32_MAX, 0);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_SCRATCH_1_OFFS(gpii->gpii_id), U32_MAX, 0);\n\t\tgpi_update_reg(gpii, GPII_n_CNTXT_INTSET_OFFS(gpii->gpii_id),\n\t\t\t       GPII_n_CNTXT_INTSET_BMSK, 1);\n\t\tgpi_update_reg(gpii, GPII_n_ERROR_LOG_OFFS(gpii->gpii_id), U32_MAX, 0);\n\n\t\tgpii->cntxt_type_irq_msk = enable;\n\t}\n\n\tgpii->configured_irq = true;\n\treturn 0;\n}\n\n \nstatic int gpi_send_cmd(struct gpii *gpii, struct gchan *gchan,\n\t\t\tenum gpi_cmd gpi_cmd)\n{\n\tu32 chid = MAX_CHANNELS_PER_GPII;\n\tunsigned long timeout;\n\tvoid __iomem *cmd_reg;\n\tu32 cmd;\n\n\tif (gpi_cmd >= GPI_MAX_CMD)\n\t\treturn -EINVAL;\n\tif (IS_CHAN_CMD(gpi_cmd))\n\t\tchid = gchan->chid;\n\n\tdev_dbg(gpii->gpi_dev->dev,\n\t\t\"sending cmd: %s:%u\\n\", TO_GPI_CMD_STR(gpi_cmd), chid);\n\n\t \n\treinit_completion(&gpii->cmd_completion);\n\tgpii->gpi_cmd = gpi_cmd;\n\n\tcmd_reg = IS_CHAN_CMD(gpi_cmd) ? gchan->ch_cmd_reg : gpii->ev_cmd_reg;\n\tcmd = IS_CHAN_CMD(gpi_cmd) ? GPII_n_CH_CMD(gpi_cmd_info[gpi_cmd].opcode, chid) :\n\t\t\t\t     GPII_n_EV_CMD(gpi_cmd_info[gpi_cmd].opcode, 0);\n\tgpi_write_reg(gpii, cmd_reg, cmd);\n\ttimeout = wait_for_completion_timeout(&gpii->cmd_completion,\n\t\t\t\t\t      msecs_to_jiffies(CMD_TIMEOUT_MS));\n\tif (!timeout) {\n\t\tdev_err(gpii->gpi_dev->dev, \"cmd: %s completion timeout:%u\\n\",\n\t\t\tTO_GPI_CMD_STR(gpi_cmd), chid);\n\t\treturn -EIO;\n\t}\n\n\t \n\tif (gpi_cmd_info[gpi_cmd].state == STATE_IGNORE)\n\t\treturn 0;\n\n\tif (IS_CHAN_CMD(gpi_cmd) && gchan->ch_state == gpi_cmd_info[gpi_cmd].state)\n\t\treturn 0;\n\n\tif (!IS_CHAN_CMD(gpi_cmd) && gpii->ev_state == gpi_cmd_info[gpi_cmd].state)\n\t\treturn 0;\n\n\treturn -EIO;\n}\n\n \nstatic inline void gpi_write_ch_db(struct gchan *gchan,\n\t\t\t\t   struct gpi_ring *ring, void *wp)\n{\n\tstruct gpii *gpii = gchan->gpii;\n\tphys_addr_t p_wp;\n\n\tp_wp = to_physical(ring, wp);\n\tgpi_write_reg(gpii, gchan->ch_cntxt_db_reg, p_wp);\n}\n\n \nstatic inline void gpi_write_ev_db(struct gpii *gpii,\n\t\t\t\t   struct gpi_ring *ring, void *wp)\n{\n\tphys_addr_t p_wp;\n\n\tp_wp = ring->phys_addr + (wp - ring->base);\n\tgpi_write_reg(gpii, gpii->ev_cntxt_db_reg, p_wp);\n}\n\n \nstatic void gpi_process_ieob(struct gpii *gpii)\n{\n\tgpi_write_reg(gpii, gpii->ieob_clr_reg, BIT(0));\n\n\tgpi_config_interrupts(gpii, MASK_IEOB_SETTINGS, 0);\n\ttasklet_hi_schedule(&gpii->ev_task);\n}\n\n \nstatic void gpi_process_ch_ctrl_irq(struct gpii *gpii)\n{\n\tu32 gpii_id = gpii->gpii_id;\n\tu32 offset = GPII_n_CNTXT_SRC_GPII_CH_IRQ_OFFS(gpii_id);\n\tu32 ch_irq = gpi_read_reg(gpii, gpii->regs + offset);\n\tstruct gchan *gchan;\n\tu32 chid, state;\n\n\t \n\toffset = GPII_n_CNTXT_SRC_CH_IRQ_CLR_OFFS(gpii_id);\n\tgpi_write_reg(gpii, gpii->regs + offset, (u32)ch_irq);\n\n\tfor (chid = 0; chid < MAX_CHANNELS_PER_GPII; chid++) {\n\t\tif (!(BIT(chid) & ch_irq))\n\t\t\tcontinue;\n\n\t\tgchan = &gpii->gchan[chid];\n\t\tstate = gpi_read_reg(gpii, gchan->ch_cntxt_base_reg +\n\t\t\t\t     CNTXT_0_CONFIG);\n\t\tstate = FIELD_GET(GPII_n_CH_k_CNTXT_0_CHSTATE, state);\n\n\t\t \n\t\tif (gpii->gpi_cmd == GPI_CH_CMD_DE_ALLOC)\n\t\t\tstate = DEFAULT_CH_STATE;\n\t\tgchan->ch_state = state;\n\n\t\t \n\t\tif (gchan->ch_state != CH_STATE_STOP_IN_PROC)\n\t\t\tcomplete_all(&gpii->cmd_completion);\n\t}\n}\n\n \nstatic void gpi_process_gen_err_irq(struct gpii *gpii)\n{\n\tu32 gpii_id = gpii->gpii_id;\n\tu32 offset = GPII_n_CNTXT_GPII_IRQ_STTS_OFFS(gpii_id);\n\tu32 irq_stts = gpi_read_reg(gpii, gpii->regs + offset);\n\n\t \n\tdev_dbg(gpii->gpi_dev->dev, \"irq_stts:0x%x\\n\", irq_stts);\n\n\t \n\toffset = GPII_n_CNTXT_GPII_IRQ_CLR_OFFS(gpii_id);\n\tgpi_write_reg(gpii, gpii->regs + offset, irq_stts);\n}\n\n \nstatic void gpi_process_glob_err_irq(struct gpii *gpii)\n{\n\tu32 gpii_id = gpii->gpii_id;\n\tu32 offset = GPII_n_CNTXT_GLOB_IRQ_STTS_OFFS(gpii_id);\n\tu32 irq_stts = gpi_read_reg(gpii, gpii->regs + offset);\n\n\toffset = GPII_n_CNTXT_GLOB_IRQ_CLR_OFFS(gpii_id);\n\tgpi_write_reg(gpii, gpii->regs + offset, irq_stts);\n\n\t \n\tif (irq_stts & ~GPI_GLOB_IRQ_ERROR_INT_MSK) {\n\t\tdev_err(gpii->gpi_dev->dev, \"invalid error status:0x%x\\n\", irq_stts);\n\t\treturn;\n\t}\n\n\toffset = GPII_n_ERROR_LOG_OFFS(gpii_id);\n\tgpi_write_reg(gpii, gpii->regs + offset, 0);\n}\n\n \nstatic irqreturn_t gpi_handle_irq(int irq, void *data)\n{\n\tstruct gpii *gpii = data;\n\tu32 gpii_id = gpii->gpii_id;\n\tu32 type, offset;\n\tunsigned long flags;\n\n\tread_lock_irqsave(&gpii->pm_lock, flags);\n\n\t \n\tif (!REG_ACCESS_VALID(gpii->pm_state)) {\n\t\tdev_err(gpii->gpi_dev->dev, \"receive interrupt while in %s state\\n\",\n\t\t\tTO_GPI_PM_STR(gpii->pm_state));\n\t\tgoto exit_irq;\n\t}\n\n\toffset = GPII_n_CNTXT_TYPE_IRQ_OFFS(gpii->gpii_id);\n\ttype = gpi_read_reg(gpii, gpii->regs + offset);\n\n\tdo {\n\t\t \n\t\tif (type & GPII_n_CNTXT_TYPE_IRQ_MSK_GLOB) {\n\t\t\tgpi_process_glob_err_irq(gpii);\n\t\t\ttype &= ~(GPII_n_CNTXT_TYPE_IRQ_MSK_GLOB);\n\t\t}\n\n\t\t \n\t\tif (type & GPII_n_CNTXT_TYPE_IRQ_MSK_IEOB) {\n\t\t\tgpi_process_ieob(gpii);\n\t\t\ttype &= ~GPII_n_CNTXT_TYPE_IRQ_MSK_IEOB;\n\t\t}\n\n\t\t \n\t\tif (type & GPII_n_CNTXT_TYPE_IRQ_MSK_EV_CTRL) {\n\t\t\tu32 ev_state;\n\t\t\tu32 ev_ch_irq;\n\n\t\t\tdev_dbg(gpii->gpi_dev->dev,\n\t\t\t\t\"processing EV CTRL interrupt\\n\");\n\t\t\toffset = GPII_n_CNTXT_SRC_EV_CH_IRQ_OFFS(gpii_id);\n\t\t\tev_ch_irq = gpi_read_reg(gpii, gpii->regs + offset);\n\n\t\t\toffset = GPII_n_CNTXT_SRC_EV_CH_IRQ_CLR_OFFS\n\t\t\t\t(gpii_id);\n\t\t\tgpi_write_reg(gpii, gpii->regs + offset, ev_ch_irq);\n\t\t\tev_state = gpi_read_reg(gpii, gpii->ev_cntxt_base_reg +\n\t\t\t\t\t\tCNTXT_0_CONFIG);\n\t\t\tev_state = FIELD_GET(GPII_n_EV_k_CNTXT_0_CHSTATE, ev_state);\n\n\t\t\t \n\t\t\tif (gpii->gpi_cmd == GPI_EV_CMD_DEALLOC)\n\t\t\t\tev_state = DEFAULT_EV_CH_STATE;\n\n\t\t\tgpii->ev_state = ev_state;\n\t\t\tdev_dbg(gpii->gpi_dev->dev, \"setting EV state to %s\\n\",\n\t\t\t\tTO_GPI_EV_STATE_STR(gpii->ev_state));\n\t\t\tcomplete_all(&gpii->cmd_completion);\n\t\t\ttype &= ~(GPII_n_CNTXT_TYPE_IRQ_MSK_EV_CTRL);\n\t\t}\n\n\t\t \n\t\tif (type & GPII_n_CNTXT_TYPE_IRQ_MSK_CH_CTRL) {\n\t\t\tdev_dbg(gpii->gpi_dev->dev, \"process CH CTRL interrupts\\n\");\n\t\t\tgpi_process_ch_ctrl_irq(gpii);\n\t\t\ttype &= ~(GPII_n_CNTXT_TYPE_IRQ_MSK_CH_CTRL);\n\t\t}\n\n\t\tif (type) {\n\t\t\tdev_err(gpii->gpi_dev->dev, \"Unhandled interrupt status:0x%x\\n\", type);\n\t\t\tgpi_process_gen_err_irq(gpii);\n\t\t\tgoto exit_irq;\n\t\t}\n\n\t\toffset = GPII_n_CNTXT_TYPE_IRQ_OFFS(gpii->gpii_id);\n\t\ttype = gpi_read_reg(gpii, gpii->regs + offset);\n\t} while (type);\n\nexit_irq:\n\tread_unlock_irqrestore(&gpii->pm_lock, flags);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void gpi_process_imed_data_event(struct gchan *gchan,\n\t\t\t\t\tstruct immediate_data_event *imed_event)\n{\n\tstruct gpii *gpii = gchan->gpii;\n\tstruct gpi_ring *ch_ring = &gchan->ch_ring;\n\tvoid *tre = ch_ring->base + (ch_ring->el_size * imed_event->tre_index);\n\tstruct dmaengine_result result;\n\tstruct gpi_desc *gpi_desc;\n\tstruct virt_dma_desc *vd;\n\tunsigned long flags;\n\tu32 chid;\n\n\t \n\tif (gchan->pm_state != ACTIVE_STATE) {\n\t\tdev_err(gpii->gpi_dev->dev, \"skipping processing event because ch @ %s state\\n\",\n\t\t\tTO_GPI_PM_STR(gchan->pm_state));\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&gchan->vc.lock, flags);\n\tvd = vchan_next_desc(&gchan->vc);\n\tif (!vd) {\n\t\tstruct gpi_ere *gpi_ere;\n\t\tstruct gpi_tre *gpi_tre;\n\n\t\tspin_unlock_irqrestore(&gchan->vc.lock, flags);\n\t\tdev_dbg(gpii->gpi_dev->dev, \"event without a pending descriptor!\\n\");\n\t\tgpi_ere = (struct gpi_ere *)imed_event;\n\t\tdev_dbg(gpii->gpi_dev->dev,\n\t\t\t\"Event: %08x %08x %08x %08x\\n\",\n\t\t\tgpi_ere->dword[0], gpi_ere->dword[1],\n\t\t\tgpi_ere->dword[2], gpi_ere->dword[3]);\n\t\tgpi_tre = tre;\n\t\tdev_dbg(gpii->gpi_dev->dev,\n\t\t\t\"Pending TRE: %08x %08x %08x %08x\\n\",\n\t\t\tgpi_tre->dword[0], gpi_tre->dword[1],\n\t\t\tgpi_tre->dword[2], gpi_tre->dword[3]);\n\t\treturn;\n\t}\n\tgpi_desc = to_gpi_desc(vd);\n\tspin_unlock_irqrestore(&gchan->vc.lock, flags);\n\n\t \n\ttre += ch_ring->el_size;\n\tif (tre >= (ch_ring->base + ch_ring->len))\n\t\ttre = ch_ring->base;\n\tch_ring->rp = tre;\n\n\t \n\tsmp_wmb();\n\n\tchid = imed_event->chid;\n\tif (imed_event->code == MSM_GPI_TCE_EOT && gpii->ieob_set) {\n\t\tif (chid == GPI_RX_CHAN)\n\t\t\tgoto gpi_free_desc;\n\t\telse\n\t\t\treturn;\n\t}\n\n\tif (imed_event->code == MSM_GPI_TCE_UNEXP_ERR)\n\t\tresult.result = DMA_TRANS_ABORTED;\n\telse\n\t\tresult.result = DMA_TRANS_NOERROR;\n\tresult.residue = gpi_desc->len - imed_event->length;\n\n\tdma_cookie_complete(&vd->tx);\n\tdmaengine_desc_get_callback_invoke(&vd->tx, &result);\n\ngpi_free_desc:\n\tspin_lock_irqsave(&gchan->vc.lock, flags);\n\tlist_del(&vd->node);\n\tspin_unlock_irqrestore(&gchan->vc.lock, flags);\n\tkfree(gpi_desc);\n\tgpi_desc = NULL;\n}\n\n \nstatic void gpi_process_xfer_compl_event(struct gchan *gchan,\n\t\t\t\t\t struct xfer_compl_event *compl_event)\n{\n\tstruct gpii *gpii = gchan->gpii;\n\tstruct gpi_ring *ch_ring = &gchan->ch_ring;\n\tvoid *ev_rp = to_virtual(ch_ring, compl_event->ptr);\n\tstruct virt_dma_desc *vd;\n\tstruct gpi_desc *gpi_desc;\n\tstruct dmaengine_result result;\n\tunsigned long flags;\n\tu32 chid;\n\n\t \n\tif (unlikely(gchan->pm_state != ACTIVE_STATE)) {\n\t\tdev_err(gpii->gpi_dev->dev, \"skipping processing event because ch @ %s state\\n\",\n\t\t\tTO_GPI_PM_STR(gchan->pm_state));\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&gchan->vc.lock, flags);\n\tvd = vchan_next_desc(&gchan->vc);\n\tif (!vd) {\n\t\tstruct gpi_ere *gpi_ere;\n\n\t\tspin_unlock_irqrestore(&gchan->vc.lock, flags);\n\t\tdev_err(gpii->gpi_dev->dev, \"Event without a pending descriptor!\\n\");\n\t\tgpi_ere = (struct gpi_ere *)compl_event;\n\t\tdev_err(gpii->gpi_dev->dev,\n\t\t\t\"Event: %08x %08x %08x %08x\\n\",\n\t\t\tgpi_ere->dword[0], gpi_ere->dword[1],\n\t\t\tgpi_ere->dword[2], gpi_ere->dword[3]);\n\t\treturn;\n\t}\n\n\tgpi_desc = to_gpi_desc(vd);\n\tspin_unlock_irqrestore(&gchan->vc.lock, flags);\n\n\t \n\tev_rp += ch_ring->el_size;\n\tif (ev_rp >= (ch_ring->base + ch_ring->len))\n\t\tev_rp = ch_ring->base;\n\tch_ring->rp = ev_rp;\n\n\t \n\tsmp_wmb();\n\n\tchid = compl_event->chid;\n\tif (compl_event->code == MSM_GPI_TCE_EOT && gpii->ieob_set) {\n\t\tif (chid == GPI_RX_CHAN)\n\t\t\tgoto gpi_free_desc;\n\t\telse\n\t\t\treturn;\n\t}\n\n\tif (compl_event->code == MSM_GPI_TCE_UNEXP_ERR) {\n\t\tdev_err(gpii->gpi_dev->dev, \"Error in Transaction\\n\");\n\t\tresult.result = DMA_TRANS_ABORTED;\n\t} else {\n\t\tdev_dbg(gpii->gpi_dev->dev, \"Transaction Success\\n\");\n\t\tresult.result = DMA_TRANS_NOERROR;\n\t}\n\tresult.residue = gpi_desc->len - compl_event->length;\n\tdev_dbg(gpii->gpi_dev->dev, \"Residue %d\\n\", result.residue);\n\n\tdma_cookie_complete(&vd->tx);\n\tdmaengine_desc_get_callback_invoke(&vd->tx, &result);\n\ngpi_free_desc:\n\tspin_lock_irqsave(&gchan->vc.lock, flags);\n\tlist_del(&vd->node);\n\tspin_unlock_irqrestore(&gchan->vc.lock, flags);\n\tkfree(gpi_desc);\n\tgpi_desc = NULL;\n}\n\n \nstatic void gpi_process_events(struct gpii *gpii)\n{\n\tstruct gpi_ring *ev_ring = &gpii->ev_ring;\n\tphys_addr_t cntxt_rp;\n\tvoid *rp;\n\tunion gpi_event *gpi_event;\n\tstruct gchan *gchan;\n\tu32 chid, type;\n\n\tcntxt_rp = gpi_read_reg(gpii, gpii->ev_ring_rp_lsb_reg);\n\trp = to_virtual(ev_ring, cntxt_rp);\n\n\tdo {\n\t\twhile (rp != ev_ring->rp) {\n\t\t\tgpi_event = ev_ring->rp;\n\t\t\tchid = gpi_event->xfer_compl_event.chid;\n\t\t\ttype = gpi_event->xfer_compl_event.type;\n\n\t\t\tdev_dbg(gpii->gpi_dev->dev,\n\t\t\t\t\"Event: CHID:%u, type:%x %08x %08x %08x %08x\\n\",\n\t\t\t\tchid, type, gpi_event->gpi_ere.dword[0],\n\t\t\t\tgpi_event->gpi_ere.dword[1], gpi_event->gpi_ere.dword[2],\n\t\t\t\tgpi_event->gpi_ere.dword[3]);\n\n\t\t\tswitch (type) {\n\t\t\tcase XFER_COMPLETE_EV_TYPE:\n\t\t\t\tgchan = &gpii->gchan[chid];\n\t\t\t\tgpi_process_xfer_compl_event(gchan,\n\t\t\t\t\t\t\t     &gpi_event->xfer_compl_event);\n\t\t\t\tbreak;\n\t\t\tcase STALE_EV_TYPE:\n\t\t\t\tdev_dbg(gpii->gpi_dev->dev, \"stale event, not processing\\n\");\n\t\t\t\tbreak;\n\t\t\tcase IMMEDIATE_DATA_EV_TYPE:\n\t\t\t\tgchan = &gpii->gchan[chid];\n\t\t\t\tgpi_process_imed_data_event(gchan,\n\t\t\t\t\t\t\t    &gpi_event->immediate_data_event);\n\t\t\t\tbreak;\n\t\t\tcase QUP_NOTIF_EV_TYPE:\n\t\t\t\tdev_dbg(gpii->gpi_dev->dev, \"QUP_NOTIF_EV_TYPE\\n\");\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tdev_dbg(gpii->gpi_dev->dev,\n\t\t\t\t\t\"not supported event type:0x%x\\n\", type);\n\t\t\t}\n\t\t\tgpi_ring_recycle_ev_element(ev_ring);\n\t\t}\n\t\tgpi_write_ev_db(gpii, ev_ring, ev_ring->wp);\n\n\t\t \n\t\tgpi_write_reg(gpii, gpii->ieob_clr_reg, BIT(0));\n\n\t\tcntxt_rp = gpi_read_reg(gpii, gpii->ev_ring_rp_lsb_reg);\n\t\trp = to_virtual(ev_ring, cntxt_rp);\n\n\t} while (rp != ev_ring->rp);\n}\n\n \nstatic void gpi_ev_tasklet(unsigned long data)\n{\n\tstruct gpii *gpii = (struct gpii *)data;\n\n\tread_lock(&gpii->pm_lock);\n\tif (!REG_ACCESS_VALID(gpii->pm_state)) {\n\t\tread_unlock(&gpii->pm_lock);\n\t\tdev_err(gpii->gpi_dev->dev, \"not processing any events, pm_state:%s\\n\",\n\t\t\tTO_GPI_PM_STR(gpii->pm_state));\n\t\treturn;\n\t}\n\n\t \n\tgpi_process_events(gpii);\n\n\t \n\tgpi_config_interrupts(gpii, MASK_IEOB_SETTINGS, 1);\n\tread_unlock(&gpii->pm_lock);\n}\n\n \nstatic void gpi_mark_stale_events(struct gchan *gchan)\n{\n\tstruct gpii *gpii = gchan->gpii;\n\tstruct gpi_ring *ev_ring = &gpii->ev_ring;\n\tu32 cntxt_rp, local_rp;\n\tvoid *ev_rp;\n\n\tcntxt_rp = gpi_read_reg(gpii, gpii->ev_ring_rp_lsb_reg);\n\n\tev_rp = ev_ring->rp;\n\tlocal_rp = (u32)to_physical(ev_ring, ev_rp);\n\twhile (local_rp != cntxt_rp) {\n\t\tunion gpi_event *gpi_event = ev_rp;\n\t\tu32 chid = gpi_event->xfer_compl_event.chid;\n\n\t\tif (chid == gchan->chid)\n\t\t\tgpi_event->xfer_compl_event.type = STALE_EV_TYPE;\n\t\tev_rp += ev_ring->el_size;\n\t\tif (ev_rp >= (ev_ring->base + ev_ring->len))\n\t\t\tev_rp = ev_ring->base;\n\t\tcntxt_rp = gpi_read_reg(gpii, gpii->ev_ring_rp_lsb_reg);\n\t\tlocal_rp = (u32)to_physical(ev_ring, ev_rp);\n\t}\n}\n\n \nstatic int gpi_reset_chan(struct gchan *gchan, enum gpi_cmd gpi_cmd)\n{\n\tstruct gpii *gpii = gchan->gpii;\n\tstruct gpi_ring *ch_ring = &gchan->ch_ring;\n\tunsigned long flags;\n\tLIST_HEAD(list);\n\tint ret;\n\n\tret = gpi_send_cmd(gpii, gchan, gpi_cmd);\n\tif (ret) {\n\t\tdev_err(gpii->gpi_dev->dev, \"Error with cmd:%s ret:%d\\n\",\n\t\t\tTO_GPI_CMD_STR(gpi_cmd), ret);\n\t\treturn ret;\n\t}\n\n\t \n\tch_ring->rp = ch_ring->base;\n\tch_ring->wp = ch_ring->base;\n\n\t \n\tsmp_wmb();\n\n\t \n\twrite_lock_irq(&gpii->pm_lock);\n\tgpi_mark_stale_events(gchan);\n\n\t \n\tspin_lock_irqsave(&gchan->vc.lock, flags);\n\tvchan_get_all_descriptors(&gchan->vc, &list);\n\tspin_unlock_irqrestore(&gchan->vc.lock, flags);\n\twrite_unlock_irq(&gpii->pm_lock);\n\tvchan_dma_desc_free_list(&gchan->vc, &list);\n\n\treturn 0;\n}\n\nstatic int gpi_start_chan(struct gchan *gchan)\n{\n\tstruct gpii *gpii = gchan->gpii;\n\tint ret;\n\n\tret = gpi_send_cmd(gpii, gchan, GPI_CH_CMD_START);\n\tif (ret) {\n\t\tdev_err(gpii->gpi_dev->dev, \"Error with cmd:%s ret:%d\\n\",\n\t\t\tTO_GPI_CMD_STR(GPI_CH_CMD_START), ret);\n\t\treturn ret;\n\t}\n\n\t \n\twrite_lock_irq(&gpii->pm_lock);\n\tgchan->pm_state = ACTIVE_STATE;\n\twrite_unlock_irq(&gpii->pm_lock);\n\n\treturn 0;\n}\n\nstatic int gpi_stop_chan(struct gchan *gchan)\n{\n\tstruct gpii *gpii = gchan->gpii;\n\tint ret;\n\n\tret = gpi_send_cmd(gpii, gchan, GPI_CH_CMD_STOP);\n\tif (ret) {\n\t\tdev_err(gpii->gpi_dev->dev, \"Error with cmd:%s ret:%d\\n\",\n\t\t\tTO_GPI_CMD_STR(GPI_CH_CMD_STOP), ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int gpi_alloc_chan(struct gchan *chan, bool send_alloc_cmd)\n{\n\tstruct gpii *gpii = chan->gpii;\n\tstruct gpi_ring *ring = &chan->ch_ring;\n\tint ret;\n\tu32 id = gpii->gpii_id;\n\tu32 chid = chan->chid;\n\tu32 pair_chid = !chid;\n\n\tif (send_alloc_cmd) {\n\t\tret = gpi_send_cmd(gpii, chan, GPI_CH_CMD_ALLOCATE);\n\t\tif (ret) {\n\t\t\tdev_err(gpii->gpi_dev->dev, \"Error with cmd:%s ret:%d\\n\",\n\t\t\t\tTO_GPI_CMD_STR(GPI_CH_CMD_ALLOCATE), ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tgpi_write_reg(gpii, chan->ch_cntxt_base_reg + CNTXT_0_CONFIG,\n\t\t      GPII_n_CH_k_CNTXT_0(ring->el_size, 0, chan->dir, GPI_CHTYPE_PROTO_GPI));\n\tgpi_write_reg(gpii, chan->ch_cntxt_base_reg + CNTXT_1_R_LENGTH, ring->len);\n\tgpi_write_reg(gpii, chan->ch_cntxt_base_reg + CNTXT_2_RING_BASE_LSB, ring->phys_addr);\n\tgpi_write_reg(gpii, chan->ch_cntxt_base_reg + CNTXT_3_RING_BASE_MSB,\n\t\t      upper_32_bits(ring->phys_addr));\n\tgpi_write_reg(gpii, chan->ch_cntxt_db_reg + CNTXT_5_RING_RP_MSB - CNTXT_4_RING_RP_LSB,\n\t\t      upper_32_bits(ring->phys_addr));\n\tgpi_write_reg(gpii, gpii->regs + GPII_n_CH_k_SCRATCH_0_OFFS(id, chid),\n\t\t      GPII_n_CH_k_SCRATCH_0(pair_chid, chan->protocol, chan->seid));\n\tgpi_write_reg(gpii, gpii->regs + GPII_n_CH_k_SCRATCH_1_OFFS(id, chid), 0);\n\tgpi_write_reg(gpii, gpii->regs + GPII_n_CH_k_SCRATCH_2_OFFS(id, chid), 0);\n\tgpi_write_reg(gpii, gpii->regs + GPII_n_CH_k_SCRATCH_3_OFFS(id, chid), 0);\n\tgpi_write_reg(gpii, gpii->regs + GPII_n_CH_k_QOS_OFFS(id, chid), 1);\n\n\t \n\twmb();\n\treturn 0;\n}\n\n \nstatic int gpi_alloc_ev_chan(struct gpii *gpii)\n{\n\tstruct gpi_ring *ring = &gpii->ev_ring;\n\tvoid __iomem *base = gpii->ev_cntxt_base_reg;\n\tint ret;\n\n\tret = gpi_send_cmd(gpii, NULL, GPI_EV_CMD_ALLOCATE);\n\tif (ret) {\n\t\tdev_err(gpii->gpi_dev->dev, \"error with cmd:%s ret:%d\\n\",\n\t\t\tTO_GPI_CMD_STR(GPI_EV_CMD_ALLOCATE), ret);\n\t\treturn ret;\n\t}\n\n\t \n\tgpi_write_reg(gpii, base + CNTXT_0_CONFIG,\n\t\t      GPII_n_EV_k_CNTXT_0(ring->el_size, GPI_INTTYPE_IRQ, GPI_CHTYPE_GPI_EV));\n\tgpi_write_reg(gpii, base + CNTXT_1_R_LENGTH, ring->len);\n\tgpi_write_reg(gpii, base + CNTXT_2_RING_BASE_LSB, lower_32_bits(ring->phys_addr));\n\tgpi_write_reg(gpii, base + CNTXT_3_RING_BASE_MSB, upper_32_bits(ring->phys_addr));\n\tgpi_write_reg(gpii, gpii->ev_cntxt_db_reg + CNTXT_5_RING_RP_MSB - CNTXT_4_RING_RP_LSB,\n\t\t      upper_32_bits(ring->phys_addr));\n\tgpi_write_reg(gpii, base + CNTXT_8_RING_INT_MOD, 0);\n\tgpi_write_reg(gpii, base + CNTXT_10_RING_MSI_LSB, 0);\n\tgpi_write_reg(gpii, base + CNTXT_11_RING_MSI_MSB, 0);\n\tgpi_write_reg(gpii, base + CNTXT_8_RING_INT_MOD, 0);\n\tgpi_write_reg(gpii, base + CNTXT_12_RING_RP_UPDATE_LSB, 0);\n\tgpi_write_reg(gpii, base + CNTXT_13_RING_RP_UPDATE_MSB, 0);\n\n\t \n\tring->wp = (ring->base + ring->len - ring->el_size);\n\n\t \n\twmb();\n\n\t \n\twrite_lock_irq(&gpii->pm_lock);\n\tgpii->pm_state = ACTIVE_STATE;\n\twrite_unlock_irq(&gpii->pm_lock);\n\tgpi_write_ev_db(gpii, ring, ring->wp);\n\n\treturn 0;\n}\n\n \nstatic int gpi_ring_num_elements_avail(const struct gpi_ring * const ring)\n{\n\tint elements = 0;\n\n\tif (ring->wp < ring->rp) {\n\t\telements = ((ring->rp - ring->wp) / ring->el_size) - 1;\n\t} else {\n\t\telements = (ring->rp - ring->base) / ring->el_size;\n\t\telements += ((ring->base + ring->len - ring->wp) / ring->el_size) - 1;\n\t}\n\n\treturn elements;\n}\n\nstatic int gpi_ring_add_element(struct gpi_ring *ring, void **wp)\n{\n\tif (gpi_ring_num_elements_avail(ring) <= 0)\n\t\treturn -ENOMEM;\n\n\t*wp = ring->wp;\n\tring->wp += ring->el_size;\n\tif (ring->wp  >= (ring->base + ring->len))\n\t\tring->wp = ring->base;\n\n\t \n\tsmp_wmb();\n\n\treturn 0;\n}\n\nstatic void gpi_ring_recycle_ev_element(struct gpi_ring *ring)\n{\n\t \n\tring->wp += ring->el_size;\n\tif (ring->wp  >= (ring->base + ring->len))\n\t\tring->wp = ring->base;\n\n\t \n\tring->rp += ring->el_size;\n\tif (ring->rp  >= (ring->base + ring->len))\n\t\tring->rp = ring->base;\n\n\t \n\tsmp_wmb();\n}\n\nstatic void gpi_free_ring(struct gpi_ring *ring,\n\t\t\t  struct gpii *gpii)\n{\n\tdma_free_coherent(gpii->gpi_dev->dev, ring->alloc_size,\n\t\t\t  ring->pre_aligned, ring->dma_handle);\n\tmemset(ring, 0, sizeof(*ring));\n}\n\n \nstatic int gpi_alloc_ring(struct gpi_ring *ring, u32 elements,\n\t\t\t  u32 el_size, struct gpii *gpii)\n{\n\tu64 len = elements * el_size;\n\tint bit;\n\n\t \n\tbit = find_last_bit((unsigned long *)&len, 32);\n\tif (((1 << bit) - 1) & len)\n\t\tbit++;\n\tlen = 1 << bit;\n\tring->alloc_size = (len + (len - 1));\n\tdev_dbg(gpii->gpi_dev->dev,\n\t\t\"#el:%u el_size:%u len:%u actual_len:%llu alloc_size:%zu\\n\",\n\t\t  elements, el_size, (elements * el_size), len,\n\t\t  ring->alloc_size);\n\n\tring->pre_aligned = dma_alloc_coherent(gpii->gpi_dev->dev,\n\t\t\t\t\t       ring->alloc_size,\n\t\t\t\t\t       &ring->dma_handle, GFP_KERNEL);\n\tif (!ring->pre_aligned) {\n\t\tdev_err(gpii->gpi_dev->dev, \"could not alloc size:%zu mem for ring\\n\",\n\t\t\tring->alloc_size);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tring->phys_addr = (ring->dma_handle + (len - 1)) & ~(len - 1);\n\tring->base = ring->pre_aligned + (ring->phys_addr - ring->dma_handle);\n\tring->rp = ring->base;\n\tring->wp = ring->base;\n\tring->len = len;\n\tring->el_size = el_size;\n\tring->elements = ring->len / ring->el_size;\n\tmemset(ring->base, 0, ring->len);\n\tring->configured = true;\n\n\t \n\tsmp_wmb();\n\n\tdev_dbg(gpii->gpi_dev->dev,\n\t\t\"phy_pre:%pad phy_alig:%pa len:%u el_size:%u elements:%u\\n\",\n\t\t&ring->dma_handle, &ring->phys_addr, ring->len,\n\t\tring->el_size, ring->elements);\n\n\treturn 0;\n}\n\n \nstatic void gpi_queue_xfer(struct gpii *gpii, struct gchan *gchan,\n\t\t\t   struct gpi_tre *gpi_tre, void **wp)\n{\n\tstruct gpi_tre *ch_tre;\n\tint ret;\n\n\t \n\tret = gpi_ring_add_element(&gchan->ch_ring, (void **)&ch_tre);\n\tif (unlikely(ret)) {\n\t\tdev_err(gpii->gpi_dev->dev, \"Error adding ring element to xfer ring\\n\");\n\t\treturn;\n\t}\n\n\t \n\tmemcpy(ch_tre, gpi_tre, sizeof(*ch_tre));\n\t*wp = ch_tre;\n}\n\n \nstatic int gpi_terminate_all(struct dma_chan *chan)\n{\n\tstruct gchan *gchan = to_gchan(chan);\n\tstruct gpii *gpii = gchan->gpii;\n\tint schid, echid, i;\n\tint ret = 0;\n\n\tmutex_lock(&gpii->ctrl_lock);\n\n\t \n\tschid = (gchan->protocol == QCOM_GPI_UART) ? gchan->chid : 0;\n\techid = (gchan->protocol == QCOM_GPI_UART) ? schid + 1 : MAX_CHANNELS_PER_GPII;\n\n\t \n\tfor (i = schid; i < echid; i++) {\n\t\tgchan = &gpii->gchan[i];\n\n\t\t \n\t\twrite_lock_irq(&gpii->pm_lock);\n\t\tgchan->pm_state = PREPARE_TERMINATE;\n\t\twrite_unlock_irq(&gpii->pm_lock);\n\n\t\t \n\t\tret = gpi_stop_chan(gchan);\n\t}\n\n\t \n\tfor (i = schid; i < echid; i++) {\n\t\tgchan = &gpii->gchan[i];\n\n\t\tret = gpi_reset_chan(gchan, GPI_CH_CMD_RESET);\n\t\tif (ret) {\n\t\t\tdev_err(gpii->gpi_dev->dev, \"Error resetting channel ret:%d\\n\", ret);\n\t\t\tgoto terminate_exit;\n\t\t}\n\n\t\t \n\t\tret = gpi_alloc_chan(gchan, false);\n\t\tif (ret) {\n\t\t\tdev_err(gpii->gpi_dev->dev, \"Error alloc_channel ret:%d\\n\", ret);\n\t\t\tgoto terminate_exit;\n\t\t}\n\t}\n\n\t \n\tfor (i = schid; i < echid; i++) {\n\t\tgchan = &gpii->gchan[i];\n\n\t\tret = gpi_start_chan(gchan);\n\t\tif (ret) {\n\t\t\tdev_err(gpii->gpi_dev->dev, \"Error Starting Channel ret:%d\\n\", ret);\n\t\t\tgoto terminate_exit;\n\t\t}\n\t}\n\nterminate_exit:\n\tmutex_unlock(&gpii->ctrl_lock);\n\treturn ret;\n}\n\n \nstatic int gpi_pause(struct dma_chan *chan)\n{\n\tstruct gchan *gchan = to_gchan(chan);\n\tstruct gpii *gpii = gchan->gpii;\n\tint i, ret;\n\n\tmutex_lock(&gpii->ctrl_lock);\n\n\t \n\tif (gpii->pm_state == PAUSE_STATE) {\n\t\tdev_dbg(gpii->gpi_dev->dev, \"channel is already paused\\n\");\n\t\tmutex_unlock(&gpii->ctrl_lock);\n\t\treturn 0;\n\t}\n\n\t \n\tfor (i = 0; i < MAX_CHANNELS_PER_GPII; i++) {\n\t\tret = gpi_stop_chan(&gpii->gchan[i]);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&gpii->ctrl_lock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tdisable_irq(gpii->irq);\n\n\t \n\ttasklet_kill(&gpii->ev_task);\n\n\twrite_lock_irq(&gpii->pm_lock);\n\tgpii->pm_state = PAUSE_STATE;\n\twrite_unlock_irq(&gpii->pm_lock);\n\tmutex_unlock(&gpii->ctrl_lock);\n\n\treturn 0;\n}\n\n \nstatic int gpi_resume(struct dma_chan *chan)\n{\n\tstruct gchan *gchan = to_gchan(chan);\n\tstruct gpii *gpii = gchan->gpii;\n\tint i, ret;\n\n\tmutex_lock(&gpii->ctrl_lock);\n\tif (gpii->pm_state == ACTIVE_STATE) {\n\t\tdev_dbg(gpii->gpi_dev->dev, \"channel is already active\\n\");\n\t\tmutex_unlock(&gpii->ctrl_lock);\n\t\treturn 0;\n\t}\n\n\tenable_irq(gpii->irq);\n\n\t \n\tfor (i = 0; i < MAX_CHANNELS_PER_GPII; i++) {\n\t\tret = gpi_send_cmd(gpii, &gpii->gchan[i], GPI_CH_CMD_START);\n\t\tif (ret) {\n\t\t\tdev_err(gpii->gpi_dev->dev, \"Error starting chan, ret:%d\\n\", ret);\n\t\t\tmutex_unlock(&gpii->ctrl_lock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\twrite_lock_irq(&gpii->pm_lock);\n\tgpii->pm_state = ACTIVE_STATE;\n\twrite_unlock_irq(&gpii->pm_lock);\n\tmutex_unlock(&gpii->ctrl_lock);\n\n\treturn 0;\n}\n\nstatic void gpi_desc_free(struct virt_dma_desc *vd)\n{\n\tstruct gpi_desc *gpi_desc = to_gpi_desc(vd);\n\n\tkfree(gpi_desc);\n\tgpi_desc = NULL;\n}\n\nstatic int\ngpi_peripheral_config(struct dma_chan *chan, struct dma_slave_config *config)\n{\n\tstruct gchan *gchan = to_gchan(chan);\n\n\tif (!config->peripheral_config)\n\t\treturn -EINVAL;\n\n\tgchan->config = krealloc(gchan->config, config->peripheral_size, GFP_NOWAIT);\n\tif (!gchan->config)\n\t\treturn -ENOMEM;\n\n\tmemcpy(gchan->config, config->peripheral_config, config->peripheral_size);\n\n\treturn 0;\n}\n\nstatic int gpi_create_i2c_tre(struct gchan *chan, struct gpi_desc *desc,\n\t\t\t      struct scatterlist *sgl, enum dma_transfer_direction direction)\n{\n\tstruct gpi_i2c_config *i2c = chan->config;\n\tstruct device *dev = chan->gpii->gpi_dev->dev;\n\tunsigned int tre_idx = 0;\n\tdma_addr_t address;\n\tstruct gpi_tre *tre;\n\tunsigned int i;\n\n\t \n\tif (i2c->set_config) {\n\t\ttre = &desc->tre[tre_idx];\n\t\ttre_idx++;\n\n\t\ttre->dword[0] = u32_encode_bits(i2c->low_count, TRE_I2C_C0_TLOW);\n\t\ttre->dword[0] |= u32_encode_bits(i2c->high_count, TRE_I2C_C0_THIGH);\n\t\ttre->dword[0] |= u32_encode_bits(i2c->cycle_count, TRE_I2C_C0_TCYL);\n\t\ttre->dword[0] |= u32_encode_bits(i2c->pack_enable, TRE_I2C_C0_TX_PACK);\n\t\ttre->dword[0] |= u32_encode_bits(i2c->pack_enable, TRE_I2C_C0_RX_PACK);\n\n\t\ttre->dword[1] = 0;\n\n\t\ttre->dword[2] = u32_encode_bits(i2c->clk_div, TRE_C0_CLK_DIV);\n\n\t\ttre->dword[3] = u32_encode_bits(TRE_TYPE_CONFIG0, TRE_FLAGS_TYPE);\n\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_CHAIN);\n\t}\n\n\t \n\tif (i2c->op == I2C_WRITE) {\n\t\ttre = &desc->tre[tre_idx];\n\t\ttre_idx++;\n\n\t\tif (i2c->multi_msg)\n\t\t\ttre->dword[0] = u32_encode_bits(I2C_READ, TRE_I2C_GO_CMD);\n\t\telse\n\t\t\ttre->dword[0] = u32_encode_bits(i2c->op, TRE_I2C_GO_CMD);\n\n\t\ttre->dword[0] |= u32_encode_bits(i2c->addr, TRE_I2C_GO_ADDR);\n\t\ttre->dword[0] |= u32_encode_bits(i2c->stretch, TRE_I2C_GO_STRETCH);\n\n\t\ttre->dword[1] = 0;\n\t\ttre->dword[2] = u32_encode_bits(i2c->rx_len, TRE_RX_LEN);\n\n\t\ttre->dword[3] = u32_encode_bits(TRE_TYPE_GO, TRE_FLAGS_TYPE);\n\n\t\tif (i2c->multi_msg)\n\t\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_LINK);\n\t\telse\n\t\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_CHAIN);\n\t}\n\n\tif (i2c->op == I2C_READ || i2c->multi_msg == false) {\n\t\t \n\t\ttre = &desc->tre[tre_idx];\n\t\ttre_idx++;\n\n\t\taddress = sg_dma_address(sgl);\n\t\ttre->dword[0] = lower_32_bits(address);\n\t\ttre->dword[1] = upper_32_bits(address);\n\n\t\ttre->dword[2] = u32_encode_bits(sg_dma_len(sgl), TRE_DMA_LEN);\n\n\t\ttre->dword[3] = u32_encode_bits(TRE_TYPE_DMA, TRE_FLAGS_TYPE);\n\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_IEOT);\n\t}\n\n\tfor (i = 0; i < tre_idx; i++)\n\t\tdev_dbg(dev, \"TRE:%d %x:%x:%x:%x\\n\", i, desc->tre[i].dword[0],\n\t\t\tdesc->tre[i].dword[1], desc->tre[i].dword[2], desc->tre[i].dword[3]);\n\n\treturn tre_idx;\n}\n\nstatic int gpi_create_spi_tre(struct gchan *chan, struct gpi_desc *desc,\n\t\t\t      struct scatterlist *sgl, enum dma_transfer_direction direction)\n{\n\tstruct gpi_spi_config *spi = chan->config;\n\tstruct device *dev = chan->gpii->gpi_dev->dev;\n\tunsigned int tre_idx = 0;\n\tdma_addr_t address;\n\tstruct gpi_tre *tre;\n\tunsigned int i;\n\n\t \n\tif (direction == DMA_MEM_TO_DEV && spi->set_config) {\n\t\ttre = &desc->tre[tre_idx];\n\t\ttre_idx++;\n\n\t\ttre->dword[0] = u32_encode_bits(spi->word_len, TRE_SPI_C0_WORD_SZ);\n\t\ttre->dword[0] |= u32_encode_bits(spi->loopback_en, TRE_SPI_C0_LOOPBACK);\n\t\ttre->dword[0] |= u32_encode_bits(spi->clock_pol_high, TRE_SPI_C0_CPOL);\n\t\ttre->dword[0] |= u32_encode_bits(spi->data_pol_high, TRE_SPI_C0_CPHA);\n\t\ttre->dword[0] |= u32_encode_bits(spi->pack_en, TRE_SPI_C0_TX_PACK);\n\t\ttre->dword[0] |= u32_encode_bits(spi->pack_en, TRE_SPI_C0_RX_PACK);\n\n\t\ttre->dword[1] = 0;\n\n\t\ttre->dword[2] = u32_encode_bits(spi->clk_div, TRE_C0_CLK_DIV);\n\t\ttre->dword[2] |= u32_encode_bits(spi->clk_src, TRE_C0_CLK_SRC);\n\n\t\ttre->dword[3] = u32_encode_bits(TRE_TYPE_CONFIG0, TRE_FLAGS_TYPE);\n\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_CHAIN);\n\t}\n\n\t \n\tif (direction == DMA_MEM_TO_DEV) {\n\t\ttre = &desc->tre[tre_idx];\n\t\ttre_idx++;\n\n\t\ttre->dword[0] = u32_encode_bits(spi->fragmentation, TRE_SPI_GO_FRAG);\n\t\ttre->dword[0] |= u32_encode_bits(spi->cs, TRE_SPI_GO_CS);\n\t\ttre->dword[0] |= u32_encode_bits(spi->cmd, TRE_SPI_GO_CMD);\n\n\t\ttre->dword[1] = 0;\n\n\t\ttre->dword[2] = u32_encode_bits(spi->rx_len, TRE_RX_LEN);\n\n\t\ttre->dword[3] = u32_encode_bits(TRE_TYPE_GO, TRE_FLAGS_TYPE);\n\t\tif (spi->cmd == SPI_RX) {\n\t\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_IEOB);\n\t\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_LINK);\n\t\t} else if (spi->cmd == SPI_TX) {\n\t\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_CHAIN);\n\t\t} else {  \n\t\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_CHAIN);\n\t\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_LINK);\n\t\t}\n\t}\n\n\t \n\ttre = &desc->tre[tre_idx];\n\ttre_idx++;\n\n\taddress = sg_dma_address(sgl);\n\ttre->dword[0] = lower_32_bits(address);\n\ttre->dword[1] = upper_32_bits(address);\n\n\ttre->dword[2] = u32_encode_bits(sg_dma_len(sgl), TRE_DMA_LEN);\n\n\ttre->dword[3] = u32_encode_bits(TRE_TYPE_DMA, TRE_FLAGS_TYPE);\n\tif (direction == DMA_MEM_TO_DEV)\n\t\ttre->dword[3] |= u32_encode_bits(1, TRE_FLAGS_IEOT);\n\n\tfor (i = 0; i < tre_idx; i++)\n\t\tdev_dbg(dev, \"TRE:%d %x:%x:%x:%x\\n\", i, desc->tre[i].dword[0],\n\t\t\tdesc->tre[i].dword[1], desc->tre[i].dword[2], desc->tre[i].dword[3]);\n\n\treturn tre_idx;\n}\n\n \nstatic struct dma_async_tx_descriptor *\ngpi_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\n\t\t  unsigned int sg_len, enum dma_transfer_direction direction,\n\t\t  unsigned long flags, void *context)\n{\n\tstruct gchan *gchan = to_gchan(chan);\n\tstruct gpii *gpii = gchan->gpii;\n\tstruct device *dev = gpii->gpi_dev->dev;\n\tstruct gpi_ring *ch_ring = &gchan->ch_ring;\n\tstruct gpi_desc *gpi_desc;\n\tu32 nr, nr_tre = 0;\n\tu8 set_config;\n\tint i;\n\n\tgpii->ieob_set = false;\n\tif (!is_slave_direction(direction)) {\n\t\tdev_err(gpii->gpi_dev->dev, \"invalid dma direction: %d\\n\", direction);\n\t\treturn NULL;\n\t}\n\n\tif (sg_len > 1) {\n\t\tdev_err(dev, \"Multi sg sent, we support only one atm: %d\\n\", sg_len);\n\t\treturn NULL;\n\t}\n\n\tnr_tre = 3;\n\tset_config = *(u32 *)gchan->config;\n\tif (!set_config)\n\t\tnr_tre = 2;\n\tif (direction == DMA_DEV_TO_MEM)  \n\t\tnr_tre = 1;\n\n\t \n\tnr = gpi_ring_num_elements_avail(ch_ring);\n\tif (nr < nr_tre) {\n\t\tdev_err(dev, \"not enough space in ring, avail:%u required:%u\\n\", nr, nr_tre);\n\t\treturn NULL;\n\t}\n\n\tgpi_desc = kzalloc(sizeof(*gpi_desc), GFP_NOWAIT);\n\tif (!gpi_desc)\n\t\treturn NULL;\n\n\t \n\tif (gchan->protocol == QCOM_GPI_SPI) {\n\t\ti = gpi_create_spi_tre(gchan, gpi_desc, sgl, direction);\n\t} else if (gchan->protocol == QCOM_GPI_I2C) {\n\t\ti = gpi_create_i2c_tre(gchan, gpi_desc, sgl, direction);\n\t} else {\n\t\tdev_err(dev, \"invalid peripheral: %d\\n\", gchan->protocol);\n\t\tkfree(gpi_desc);\n\t\treturn NULL;\n\t}\n\n\t \n\tgpi_desc->gchan = gchan;\n\tgpi_desc->len = sg_dma_len(sgl);\n\tgpi_desc->num_tre  = i;\n\n\treturn vchan_tx_prep(&gchan->vc, &gpi_desc->vd, flags);\n}\n\n \nstatic void gpi_issue_pending(struct dma_chan *chan)\n{\n\tstruct gchan *gchan = to_gchan(chan);\n\tstruct gpii *gpii = gchan->gpii;\n\tunsigned long flags, pm_lock_flags;\n\tstruct virt_dma_desc *vd = NULL;\n\tstruct gpi_desc *gpi_desc;\n\tstruct gpi_ring *ch_ring = &gchan->ch_ring;\n\tvoid *tre, *wp = NULL;\n\tint i;\n\n\tread_lock_irqsave(&gpii->pm_lock, pm_lock_flags);\n\n\t \n\tspin_lock_irqsave(&gchan->vc.lock, flags);\n\tif (vchan_issue_pending(&gchan->vc))\n\t\tvd = list_last_entry(&gchan->vc.desc_issued,\n\t\t\t\t     struct virt_dma_desc, node);\n\tspin_unlock_irqrestore(&gchan->vc.lock, flags);\n\n\t \n\tif (!vd) {\n\t\tread_unlock_irqrestore(&gpii->pm_lock, pm_lock_flags);\n\t\treturn;\n\t}\n\n\tgpi_desc = to_gpi_desc(vd);\n\tfor (i = 0; i < gpi_desc->num_tre; i++) {\n\t\ttre = &gpi_desc->tre[i];\n\t\tgpi_queue_xfer(gpii, gchan, tre, &wp);\n\t}\n\n\tgpi_desc->db = ch_ring->wp;\n\tgpi_write_ch_db(gchan, &gchan->ch_ring, gpi_desc->db);\n\tread_unlock_irqrestore(&gpii->pm_lock, pm_lock_flags);\n}\n\nstatic int gpi_ch_init(struct gchan *gchan)\n{\n\tstruct gpii *gpii = gchan->gpii;\n\tconst int ev_factor = gpii->gpi_dev->ev_factor;\n\tu32 elements;\n\tint i = 0, ret = 0;\n\n\tgchan->pm_state = CONFIG_STATE;\n\n\t \n\tfor (i = 0; i < MAX_CHANNELS_PER_GPII; i++)\n\t\tif (gpii->gchan[i].pm_state != CONFIG_STATE)\n\t\t\tgoto exit_gpi_init;\n\n\t \n\tif (gpii->gchan[0].protocol != gpii->gchan[1].protocol) {\n\t\tdev_err(gpii->gpi_dev->dev, \"protocol did not match protocol %u != %u\\n\",\n\t\t\tgpii->gchan[0].protocol, gpii->gchan[1].protocol);\n\t\tret = -EINVAL;\n\t\tgoto exit_gpi_init;\n\t}\n\n\t \n\telements = CHAN_TRES << ev_factor;\n\tret = gpi_alloc_ring(&gpii->ev_ring, elements,\n\t\t\t     sizeof(union gpi_event), gpii);\n\tif (ret)\n\t\tgoto exit_gpi_init;\n\n\t \n\twrite_lock_irq(&gpii->pm_lock);\n\tgpii->pm_state = PREPARE_HARDWARE;\n\twrite_unlock_irq(&gpii->pm_lock);\n\tret = gpi_config_interrupts(gpii, DEFAULT_IRQ_SETTINGS, 0);\n\tif (ret) {\n\t\tdev_err(gpii->gpi_dev->dev, \"error config. interrupts, ret:%d\\n\", ret);\n\t\tgoto error_config_int;\n\t}\n\n\t \n\tret = gpi_alloc_ev_chan(gpii);\n\tif (ret) {\n\t\tdev_err(gpii->gpi_dev->dev, \"error alloc_ev_chan:%d\\n\", ret);\n\t\tgoto error_alloc_ev_ring;\n\t}\n\n\t \n\tfor (i = 0; i < MAX_CHANNELS_PER_GPII; i++) {\n\t\tret = gpi_alloc_chan(&gpii->gchan[i], true);\n\t\tif (ret) {\n\t\t\tdev_err(gpii->gpi_dev->dev, \"Error allocating chan:%d\\n\", ret);\n\t\t\tgoto error_alloc_chan;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < MAX_CHANNELS_PER_GPII; i++) {\n\t\tret = gpi_start_chan(&gpii->gchan[i]);\n\t\tif (ret) {\n\t\t\tdev_err(gpii->gpi_dev->dev, \"Error start chan:%d\\n\", ret);\n\t\t\tgoto error_start_chan;\n\t\t}\n\t}\n\treturn ret;\n\nerror_start_chan:\n\tfor (i = i - 1; i >= 0; i--) {\n\t\tgpi_stop_chan(&gpii->gchan[i]);\n\t\tgpi_send_cmd(gpii, gchan, GPI_CH_CMD_RESET);\n\t}\n\ti = 2;\nerror_alloc_chan:\n\tfor (i = i - 1; i >= 0; i--)\n\t\tgpi_reset_chan(gchan, GPI_CH_CMD_DE_ALLOC);\nerror_alloc_ev_ring:\n\tgpi_disable_interrupts(gpii);\nerror_config_int:\n\tgpi_free_ring(&gpii->ev_ring, gpii);\nexit_gpi_init:\n\treturn ret;\n}\n\n \nstatic void gpi_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct gchan *gchan = to_gchan(chan);\n\tstruct gpii *gpii = gchan->gpii;\n\tenum gpi_pm_state cur_state;\n\tint ret, i;\n\n\tmutex_lock(&gpii->ctrl_lock);\n\n\tcur_state = gchan->pm_state;\n\n\t \n\twrite_lock_irq(&gpii->pm_lock);\n\tgchan->pm_state = PREPARE_TERMINATE;\n\twrite_unlock_irq(&gpii->pm_lock);\n\n\t \n\tif (cur_state == ACTIVE_STATE) {\n\t\tgpi_stop_chan(gchan);\n\n\t\tret = gpi_send_cmd(gpii, gchan, GPI_CH_CMD_RESET);\n\t\tif (ret)\n\t\t\tdev_err(gpii->gpi_dev->dev, \"error resetting channel:%d\\n\", ret);\n\n\t\tgpi_reset_chan(gchan, GPI_CH_CMD_DE_ALLOC);\n\t}\n\n\t \n\tgpi_free_ring(&gchan->ch_ring, gpii);\n\tvchan_free_chan_resources(&gchan->vc);\n\tkfree(gchan->config);\n\n\twrite_lock_irq(&gpii->pm_lock);\n\tgchan->pm_state = DISABLE_STATE;\n\twrite_unlock_irq(&gpii->pm_lock);\n\n\t \n\tfor (i = 0; i < MAX_CHANNELS_PER_GPII; i++)\n\t\tif (gpii->gchan[i].ch_ring.configured)\n\t\t\tgoto exit_free;\n\n\t \n\tcur_state = gpii->pm_state;\n\twrite_lock_irq(&gpii->pm_lock);\n\tgpii->pm_state = PREPARE_TERMINATE;\n\twrite_unlock_irq(&gpii->pm_lock);\n\n\t \n\ttasklet_kill(&gpii->ev_task);\n\n\t \n\tif (cur_state == ACTIVE_STATE)\n\t\tgpi_send_cmd(gpii, NULL, GPI_EV_CMD_DEALLOC);\n\n\tgpi_free_ring(&gpii->ev_ring, gpii);\n\n\t \n\tif (cur_state == ACTIVE_STATE)\n\t\tgpi_disable_interrupts(gpii);\n\n\t \n\twrite_lock_irq(&gpii->pm_lock);\n\tgpii->pm_state = DISABLE_STATE;\n\twrite_unlock_irq(&gpii->pm_lock);\n\nexit_free:\n\tmutex_unlock(&gpii->ctrl_lock);\n}\n\n \nstatic int gpi_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct gchan *gchan = to_gchan(chan);\n\tstruct gpii *gpii = gchan->gpii;\n\tint ret;\n\n\tmutex_lock(&gpii->ctrl_lock);\n\n\t \n\tret = gpi_alloc_ring(&gchan->ch_ring, CHAN_TRES,\n\t\t\t     sizeof(struct gpi_tre), gpii);\n\tif (ret)\n\t\tgoto xfer_alloc_err;\n\n\tret = gpi_ch_init(gchan);\n\n\tmutex_unlock(&gpii->ctrl_lock);\n\n\treturn ret;\nxfer_alloc_err:\n\tmutex_unlock(&gpii->ctrl_lock);\n\n\treturn ret;\n}\n\nstatic int gpi_find_avail_gpii(struct gpi_dev *gpi_dev, u32 seid)\n{\n\tstruct gchan *tx_chan, *rx_chan;\n\tunsigned int gpii;\n\n\t \n\tfor (gpii = 0; gpii < gpi_dev->max_gpii; gpii++) {\n\t\tif (!((1 << gpii) & gpi_dev->gpii_mask))\n\t\t\tcontinue;\n\n\t\ttx_chan = &gpi_dev->gpiis[gpii].gchan[GPI_TX_CHAN];\n\t\trx_chan = &gpi_dev->gpiis[gpii].gchan[GPI_RX_CHAN];\n\n\t\tif (rx_chan->vc.chan.client_count && rx_chan->seid == seid)\n\t\t\treturn gpii;\n\t\tif (tx_chan->vc.chan.client_count && tx_chan->seid == seid)\n\t\t\treturn gpii;\n\t}\n\n\t \n\tfor (gpii = 0; gpii < gpi_dev->max_gpii; gpii++) {\n\t\tif (!((1 << gpii) & gpi_dev->gpii_mask))\n\t\t\tcontinue;\n\n\t\ttx_chan = &gpi_dev->gpiis[gpii].gchan[GPI_TX_CHAN];\n\t\trx_chan = &gpi_dev->gpiis[gpii].gchan[GPI_RX_CHAN];\n\n\t\t \n\t\tif (tx_chan->vc.chan.client_count ||\n\t\t    rx_chan->vc.chan.client_count)\n\t\t\tcontinue;\n\n\t\t \n\t\treturn gpii;\n\t}\n\n\t \n\treturn -EIO;\n}\n\n \nstatic struct dma_chan *gpi_of_dma_xlate(struct of_phandle_args *args,\n\t\t\t\t\t struct of_dma *of_dma)\n{\n\tstruct gpi_dev *gpi_dev = (struct gpi_dev *)of_dma->of_dma_data;\n\tu32 seid, chid;\n\tint gpii;\n\tstruct gchan *gchan;\n\n\tif (args->args_count < 3) {\n\t\tdev_err(gpi_dev->dev, \"gpii require minimum 2 args, client passed:%d args\\n\",\n\t\t\targs->args_count);\n\t\treturn NULL;\n\t}\n\n\tchid = args->args[0];\n\tif (chid >= MAX_CHANNELS_PER_GPII) {\n\t\tdev_err(gpi_dev->dev, \"gpii channel:%d not valid\\n\", chid);\n\t\treturn NULL;\n\t}\n\n\tseid = args->args[1];\n\n\t \n\tgpii = gpi_find_avail_gpii(gpi_dev, seid);\n\tif (gpii < 0) {\n\t\tdev_err(gpi_dev->dev, \"no available gpii instances\\n\");\n\t\treturn NULL;\n\t}\n\n\tgchan = &gpi_dev->gpiis[gpii].gchan[chid];\n\tif (gchan->vc.chan.client_count) {\n\t\tdev_err(gpi_dev->dev, \"gpii:%d chid:%d seid:%d already configured\\n\",\n\t\t\tgpii, chid, gchan->seid);\n\t\treturn NULL;\n\t}\n\n\tgchan->seid = seid;\n\tgchan->protocol = args->args[2];\n\n\treturn dma_get_slave_channel(&gchan->vc.chan);\n}\n\nstatic int gpi_probe(struct platform_device *pdev)\n{\n\tstruct gpi_dev *gpi_dev;\n\tunsigned int i;\n\tu32 ee_offset;\n\tint ret;\n\n\tgpi_dev = devm_kzalloc(&pdev->dev, sizeof(*gpi_dev), GFP_KERNEL);\n\tif (!gpi_dev)\n\t\treturn -ENOMEM;\n\n\tgpi_dev->dev = &pdev->dev;\n\tgpi_dev->regs = devm_platform_get_and_ioremap_resource(pdev, 0, &gpi_dev->res);\n\tif (IS_ERR(gpi_dev->regs))\n\t\treturn PTR_ERR(gpi_dev->regs);\n\tgpi_dev->ee_base = gpi_dev->regs;\n\n\tret = of_property_read_u32(gpi_dev->dev->of_node, \"dma-channels\",\n\t\t\t\t   &gpi_dev->max_gpii);\n\tif (ret) {\n\t\tdev_err(gpi_dev->dev, \"missing 'max-no-gpii' DT node\\n\");\n\t\treturn ret;\n\t}\n\n\tret = of_property_read_u32(gpi_dev->dev->of_node, \"dma-channel-mask\",\n\t\t\t\t   &gpi_dev->gpii_mask);\n\tif (ret) {\n\t\tdev_err(gpi_dev->dev, \"missing 'gpii-mask' DT node\\n\");\n\t\treturn ret;\n\t}\n\n\tee_offset = (uintptr_t)device_get_match_data(gpi_dev->dev);\n\tgpi_dev->ee_base = gpi_dev->ee_base - ee_offset;\n\n\tgpi_dev->ev_factor = EV_FACTOR;\n\n\tret = dma_set_mask(gpi_dev->dev, DMA_BIT_MASK(64));\n\tif (ret) {\n\t\tdev_err(gpi_dev->dev, \"Error setting dma_mask to 64, ret:%d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tgpi_dev->gpiis = devm_kzalloc(gpi_dev->dev, sizeof(*gpi_dev->gpiis) *\n\t\t\t\t      gpi_dev->max_gpii, GFP_KERNEL);\n\tif (!gpi_dev->gpiis)\n\t\treturn -ENOMEM;\n\n\t \n\tINIT_LIST_HEAD(&gpi_dev->dma_device.channels);\n\tfor (i = 0; i < gpi_dev->max_gpii; i++) {\n\t\tstruct gpii *gpii = &gpi_dev->gpiis[i];\n\t\tint chan;\n\n\t\tif (!((1 << i) & gpi_dev->gpii_mask))\n\t\t\tcontinue;\n\n\t\t \n\t\tgpii->ev_cntxt_base_reg = gpi_dev->ee_base + GPII_n_EV_CH_k_CNTXT_0_OFFS(i, 0);\n\t\tgpii->ev_cntxt_db_reg = gpi_dev->ee_base + GPII_n_EV_CH_k_DOORBELL_0_OFFS(i, 0);\n\t\tgpii->ev_ring_rp_lsb_reg = gpii->ev_cntxt_base_reg + CNTXT_4_RING_RP_LSB;\n\t\tgpii->ev_cmd_reg = gpi_dev->ee_base + GPII_n_EV_CH_CMD_OFFS(i);\n\t\tgpii->ieob_clr_reg = gpi_dev->ee_base + GPII_n_CNTXT_SRC_IEOB_IRQ_CLR_OFFS(i);\n\n\t\t \n\t\tret = platform_get_irq(pdev, i);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tgpii->irq = ret;\n\n\t\t \n\t\tfor (chan = 0; chan < MAX_CHANNELS_PER_GPII; chan++) {\n\t\t\tstruct gchan *gchan = &gpii->gchan[chan];\n\n\t\t\t \n\t\t\tgchan->ch_cntxt_base_reg = gpi_dev->ee_base +\n\t\t\t\tGPII_n_CH_k_CNTXT_0_OFFS(i, chan);\n\t\t\tgchan->ch_cntxt_db_reg = gpi_dev->ee_base +\n\t\t\t\tGPII_n_CH_k_DOORBELL_0_OFFS(i, chan);\n\t\t\tgchan->ch_cmd_reg = gpi_dev->ee_base + GPII_n_CH_CMD_OFFS(i);\n\n\t\t\t \n\t\t\tvchan_init(&gchan->vc, &gpi_dev->dma_device);\n\t\t\tgchan->vc.desc_free = gpi_desc_free;\n\t\t\tgchan->chid = chan;\n\t\t\tgchan->gpii = gpii;\n\t\t\tgchan->dir = GPII_CHAN_DIR[chan];\n\t\t}\n\t\tmutex_init(&gpii->ctrl_lock);\n\t\trwlock_init(&gpii->pm_lock);\n\t\ttasklet_init(&gpii->ev_task, gpi_ev_tasklet,\n\t\t\t     (unsigned long)gpii);\n\t\tinit_completion(&gpii->cmd_completion);\n\t\tgpii->gpii_id = i;\n\t\tgpii->regs = gpi_dev->ee_base;\n\t\tgpii->gpi_dev = gpi_dev;\n\t}\n\n\tplatform_set_drvdata(pdev, gpi_dev);\n\n\t \n\tdma_cap_zero(gpi_dev->dma_device.cap_mask);\n\tdma_cap_set(DMA_SLAVE, gpi_dev->dma_device.cap_mask);\n\n\t \n\tgpi_dev->dma_device.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\n\tgpi_dev->dma_device.residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\tgpi_dev->dma_device.src_addr_widths = DMA_SLAVE_BUSWIDTH_8_BYTES;\n\tgpi_dev->dma_device.dst_addr_widths = DMA_SLAVE_BUSWIDTH_8_BYTES;\n\tgpi_dev->dma_device.device_alloc_chan_resources = gpi_alloc_chan_resources;\n\tgpi_dev->dma_device.device_free_chan_resources = gpi_free_chan_resources;\n\tgpi_dev->dma_device.device_tx_status = dma_cookie_status;\n\tgpi_dev->dma_device.device_issue_pending = gpi_issue_pending;\n\tgpi_dev->dma_device.device_prep_slave_sg = gpi_prep_slave_sg;\n\tgpi_dev->dma_device.device_config = gpi_peripheral_config;\n\tgpi_dev->dma_device.device_terminate_all = gpi_terminate_all;\n\tgpi_dev->dma_device.dev = gpi_dev->dev;\n\tgpi_dev->dma_device.device_pause = gpi_pause;\n\tgpi_dev->dma_device.device_resume = gpi_resume;\n\n\t \n\tret = dma_async_device_register(&gpi_dev->dma_device);\n\tif (ret) {\n\t\tdev_err(gpi_dev->dev, \"async_device_register failed ret:%d\", ret);\n\t\treturn ret;\n\t}\n\n\tret = of_dma_controller_register(gpi_dev->dev->of_node,\n\t\t\t\t\t gpi_of_dma_xlate, gpi_dev);\n\tif (ret) {\n\t\tdev_err(gpi_dev->dev, \"of_dma_controller_reg failed ret:%d\", ret);\n\t\treturn ret;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct of_device_id gpi_of_match[] = {\n\t{ .compatible = \"qcom,sdm845-gpi-dma\", .data = (void *)0x0 },\n\t{ .compatible = \"qcom,sm6350-gpi-dma\", .data = (void *)0x10000 },\n\t \n\t{ .compatible = \"qcom,sc7280-gpi-dma\", .data = (void *)0x10000 },\n\t{ .compatible = \"qcom,sm8150-gpi-dma\", .data = (void *)0x0 },\n\t{ .compatible = \"qcom,sm8250-gpi-dma\", .data = (void *)0x0 },\n\t{ .compatible = \"qcom,sm8350-gpi-dma\", .data = (void *)0x10000 },\n\t{ .compatible = \"qcom,sm8450-gpi-dma\", .data = (void *)0x10000 },\n\t{ },\n};\nMODULE_DEVICE_TABLE(of, gpi_of_match);\n\nstatic struct platform_driver gpi_driver = {\n\t.probe = gpi_probe,\n\t.driver = {\n\t\t.name = KBUILD_MODNAME,\n\t\t.of_match_table = gpi_of_match,\n\t},\n};\n\nstatic int __init gpi_init(void)\n{\n\treturn platform_driver_register(&gpi_driver);\n}\nsubsys_initcall(gpi_init)\n\nMODULE_DESCRIPTION(\"QCOM GPI DMA engine driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}