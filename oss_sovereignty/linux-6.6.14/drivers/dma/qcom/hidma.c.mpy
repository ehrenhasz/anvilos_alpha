{
  "module_name": "hidma.c",
  "hash_id": "ceacc4422c8e8013c824a0bf16edaf5aa5524d83d554078c3746feb3bf60e253",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/qcom/hidma.c",
  "human_readable_source": " \n\n \n\n \n\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/list.h>\n#include <linux/mod_devicetable.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/of_dma.h>\n#include <linux/property.h>\n#include <linux/delay.h>\n#include <linux/acpi.h>\n#include <linux/irq.h>\n#include <linux/atomic.h>\n#include <linux/pm_runtime.h>\n#include <linux/msi.h>\n\n#include \"../dmaengine.h\"\n#include \"hidma.h\"\n\n \n#define HIDMA_AUTOSUSPEND_TIMEOUT\t\t2000\n#define HIDMA_ERR_INFO_SW\t\t\t0xFF\n#define HIDMA_ERR_CODE_UNEXPECTED_TERMINATE\t0x0\n#define HIDMA_NR_DEFAULT_DESC\t\t\t10\n#define HIDMA_MSI_INTS\t\t\t\t11\n\nstatic inline struct hidma_dev *to_hidma_dev(struct dma_device *dmadev)\n{\n\treturn container_of(dmadev, struct hidma_dev, ddev);\n}\n\nstatic inline\nstruct hidma_dev *to_hidma_dev_from_lldev(struct hidma_lldev **_lldevp)\n{\n\treturn container_of(_lldevp, struct hidma_dev, lldev);\n}\n\nstatic inline struct hidma_chan *to_hidma_chan(struct dma_chan *dmach)\n{\n\treturn container_of(dmach, struct hidma_chan, chan);\n}\n\nstatic void hidma_free(struct hidma_dev *dmadev)\n{\n\tINIT_LIST_HEAD(&dmadev->ddev.channels);\n}\n\nstatic unsigned int nr_desc_prm;\nmodule_param(nr_desc_prm, uint, 0644);\nMODULE_PARM_DESC(nr_desc_prm, \"number of descriptors (default: 0)\");\n\nenum hidma_cap {\n\tHIDMA_MSI_CAP = 1,\n\tHIDMA_IDENTITY_CAP,\n};\n\n \nstatic void hidma_process_completed(struct hidma_chan *mchan)\n{\n\tstruct dma_device *ddev = mchan->chan.device;\n\tstruct hidma_dev *mdma = to_hidma_dev(ddev);\n\tstruct dma_async_tx_descriptor *desc;\n\tdma_cookie_t last_cookie;\n\tstruct hidma_desc *mdesc;\n\tstruct hidma_desc *next;\n\tunsigned long irqflags;\n\tstruct list_head list;\n\n\tINIT_LIST_HEAD(&list);\n\n\t \n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\tlist_splice_tail_init(&mchan->completed, &list);\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\t \n\tlist_for_each_entry_safe(mdesc, next, &list, node) {\n\t\tenum dma_status llstat;\n\t\tstruct dmaengine_desc_callback cb;\n\t\tstruct dmaengine_result result;\n\n\t\tdesc = &mdesc->desc;\n\t\tlast_cookie = desc->cookie;\n\n\t\tllstat = hidma_ll_status(mdma->lldev, mdesc->tre_ch);\n\n\t\tspin_lock_irqsave(&mchan->lock, irqflags);\n\t\tif (llstat == DMA_COMPLETE) {\n\t\t\tmchan->last_success = last_cookie;\n\t\t\tresult.result = DMA_TRANS_NOERROR;\n\t\t} else {\n\t\t\tresult.result = DMA_TRANS_ABORTED;\n\t\t}\n\n\t\tdma_cookie_complete(desc);\n\t\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\t\tdmaengine_desc_get_callback(desc, &cb);\n\n\t\tdma_run_dependencies(desc);\n\n\t\tspin_lock_irqsave(&mchan->lock, irqflags);\n\t\tlist_move(&mdesc->node, &mchan->free);\n\t\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\t\tdmaengine_desc_callback_invoke(&cb, &result);\n\t}\n}\n\n \nstatic void hidma_callback(void *data)\n{\n\tstruct hidma_desc *mdesc = data;\n\tstruct hidma_chan *mchan = to_hidma_chan(mdesc->desc.chan);\n\tstruct dma_device *ddev = mchan->chan.device;\n\tstruct hidma_dev *dmadev = to_hidma_dev(ddev);\n\tunsigned long irqflags;\n\tbool queued = false;\n\n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\tif (mdesc->node.next) {\n\t\t \n\t\tlist_move_tail(&mdesc->node, &mchan->completed);\n\t\tqueued = true;\n\n\t\t \n\t\tmchan->running = list_first_entry(&mchan->active,\n\t\t\t\t\t\t  struct hidma_desc, node);\n\t}\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\thidma_process_completed(mchan);\n\n\tif (queued) {\n\t\tpm_runtime_mark_last_busy(dmadev->ddev.dev);\n\t\tpm_runtime_put_autosuspend(dmadev->ddev.dev);\n\t}\n}\n\nstatic int hidma_chan_init(struct hidma_dev *dmadev, u32 dma_sig)\n{\n\tstruct hidma_chan *mchan;\n\tstruct dma_device *ddev;\n\n\tmchan = devm_kzalloc(dmadev->ddev.dev, sizeof(*mchan), GFP_KERNEL);\n\tif (!mchan)\n\t\treturn -ENOMEM;\n\n\tddev = &dmadev->ddev;\n\tmchan->dma_sig = dma_sig;\n\tmchan->dmadev = dmadev;\n\tmchan->chan.device = ddev;\n\tdma_cookie_init(&mchan->chan);\n\n\tINIT_LIST_HEAD(&mchan->free);\n\tINIT_LIST_HEAD(&mchan->prepared);\n\tINIT_LIST_HEAD(&mchan->active);\n\tINIT_LIST_HEAD(&mchan->completed);\n\tINIT_LIST_HEAD(&mchan->queued);\n\n\tspin_lock_init(&mchan->lock);\n\tlist_add_tail(&mchan->chan.device_node, &ddev->channels);\n\treturn 0;\n}\n\nstatic void hidma_issue_task(struct tasklet_struct *t)\n{\n\tstruct hidma_dev *dmadev = from_tasklet(dmadev, t, task);\n\n\tpm_runtime_get_sync(dmadev->ddev.dev);\n\thidma_ll_start(dmadev->lldev);\n}\n\nstatic void hidma_issue_pending(struct dma_chan *dmach)\n{\n\tstruct hidma_chan *mchan = to_hidma_chan(dmach);\n\tstruct hidma_dev *dmadev = mchan->dmadev;\n\tunsigned long flags;\n\tstruct hidma_desc *qdesc, *next;\n\tint status;\n\n\tspin_lock_irqsave(&mchan->lock, flags);\n\tlist_for_each_entry_safe(qdesc, next, &mchan->queued, node) {\n\t\thidma_ll_queue_request(dmadev->lldev, qdesc->tre_ch);\n\t\tlist_move_tail(&qdesc->node, &mchan->active);\n\t}\n\n\tif (!mchan->running) {\n\t\tstruct hidma_desc *desc = list_first_entry(&mchan->active,\n\t\t\t\t\t\t\t   struct hidma_desc,\n\t\t\t\t\t\t\t   node);\n\t\tmchan->running = desc;\n\t}\n\tspin_unlock_irqrestore(&mchan->lock, flags);\n\n\t \n\tstatus = pm_runtime_get(dmadev->ddev.dev);\n\tif (status < 0)\n\t\ttasklet_schedule(&dmadev->task);\n\telse\n\t\thidma_ll_start(dmadev->lldev);\n}\n\nstatic inline bool hidma_txn_is_success(dma_cookie_t cookie,\n\t\tdma_cookie_t last_success, dma_cookie_t last_used)\n{\n\tif (last_success <= last_used) {\n\t\tif ((cookie <= last_success) || (cookie > last_used))\n\t\t\treturn true;\n\t} else {\n\t\tif ((cookie <= last_success) && (cookie > last_used))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic enum dma_status hidma_tx_status(struct dma_chan *dmach,\n\t\t\t\t       dma_cookie_t cookie,\n\t\t\t\t       struct dma_tx_state *txstate)\n{\n\tstruct hidma_chan *mchan = to_hidma_chan(dmach);\n\tenum dma_status ret;\n\n\tret = dma_cookie_status(dmach, cookie, txstate);\n\tif (ret == DMA_COMPLETE) {\n\t\tbool is_success;\n\n\t\tis_success = hidma_txn_is_success(cookie, mchan->last_success,\n\t\t\t\t\t\t  dmach->cookie);\n\t\treturn is_success ? ret : DMA_ERROR;\n\t}\n\n\tif (mchan->paused && (ret == DMA_IN_PROGRESS)) {\n\t\tunsigned long flags;\n\t\tdma_cookie_t runcookie;\n\n\t\tspin_lock_irqsave(&mchan->lock, flags);\n\t\tif (mchan->running)\n\t\t\truncookie = mchan->running->desc.cookie;\n\t\telse\n\t\t\truncookie = -EINVAL;\n\n\t\tif (runcookie == cookie)\n\t\t\tret = DMA_PAUSED;\n\n\t\tspin_unlock_irqrestore(&mchan->lock, flags);\n\t}\n\n\treturn ret;\n}\n\n \nstatic dma_cookie_t hidma_tx_submit(struct dma_async_tx_descriptor *txd)\n{\n\tstruct hidma_chan *mchan = to_hidma_chan(txd->chan);\n\tstruct hidma_dev *dmadev = mchan->dmadev;\n\tstruct hidma_desc *mdesc;\n\tunsigned long irqflags;\n\tdma_cookie_t cookie;\n\n\tpm_runtime_get_sync(dmadev->ddev.dev);\n\tif (!hidma_ll_isenabled(dmadev->lldev)) {\n\t\tpm_runtime_mark_last_busy(dmadev->ddev.dev);\n\t\tpm_runtime_put_autosuspend(dmadev->ddev.dev);\n\t\treturn -ENODEV;\n\t}\n\tpm_runtime_mark_last_busy(dmadev->ddev.dev);\n\tpm_runtime_put_autosuspend(dmadev->ddev.dev);\n\n\tmdesc = container_of(txd, struct hidma_desc, desc);\n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\n\t \n\tlist_move_tail(&mdesc->node, &mchan->queued);\n\n\t \n\tcookie = dma_cookie_assign(txd);\n\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\treturn cookie;\n}\n\nstatic int hidma_alloc_chan_resources(struct dma_chan *dmach)\n{\n\tstruct hidma_chan *mchan = to_hidma_chan(dmach);\n\tstruct hidma_dev *dmadev = mchan->dmadev;\n\tstruct hidma_desc *mdesc, *tmp;\n\tunsigned long irqflags;\n\tLIST_HEAD(descs);\n\tunsigned int i;\n\tint rc = 0;\n\n\tif (mchan->allocated)\n\t\treturn 0;\n\n\t \n\tfor (i = 0; i < dmadev->nr_descriptors; i++) {\n\t\tmdesc = kzalloc(sizeof(struct hidma_desc), GFP_NOWAIT);\n\t\tif (!mdesc) {\n\t\t\trc = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tdma_async_tx_descriptor_init(&mdesc->desc, dmach);\n\t\tmdesc->desc.tx_submit = hidma_tx_submit;\n\n\t\trc = hidma_ll_request(dmadev->lldev, mchan->dma_sig,\n\t\t\t\t      \"DMA engine\", hidma_callback, mdesc,\n\t\t\t\t      &mdesc->tre_ch);\n\t\tif (rc) {\n\t\t\tdev_err(dmach->device->dev,\n\t\t\t\t\"channel alloc failed at %u\\n\", i);\n\t\t\tkfree(mdesc);\n\t\t\tbreak;\n\t\t}\n\t\tlist_add_tail(&mdesc->node, &descs);\n\t}\n\n\tif (rc) {\n\t\t \n\t\tlist_for_each_entry_safe(mdesc, tmp, &descs, node) {\n\t\t\thidma_ll_free(dmadev->lldev, mdesc->tre_ch);\n\t\t\tkfree(mdesc);\n\t\t}\n\t\treturn rc;\n\t}\n\n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\tlist_splice_tail_init(&descs, &mchan->free);\n\tmchan->allocated = true;\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\treturn 1;\n}\n\nstatic struct dma_async_tx_descriptor *\nhidma_prep_dma_memcpy(struct dma_chan *dmach, dma_addr_t dest, dma_addr_t src,\n\t\tsize_t len, unsigned long flags)\n{\n\tstruct hidma_chan *mchan = to_hidma_chan(dmach);\n\tstruct hidma_desc *mdesc = NULL;\n\tstruct hidma_dev *mdma = mchan->dmadev;\n\tunsigned long irqflags;\n\n\t \n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\tif (!list_empty(&mchan->free)) {\n\t\tmdesc = list_first_entry(&mchan->free, struct hidma_desc, node);\n\t\tlist_del(&mdesc->node);\n\t}\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\tif (!mdesc)\n\t\treturn NULL;\n\n\tmdesc->desc.flags = flags;\n\thidma_ll_set_transfer_params(mdma->lldev, mdesc->tre_ch,\n\t\t\t\t     src, dest, len, flags,\n\t\t\t\t     HIDMA_TRE_MEMCPY);\n\n\t \n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\tlist_add_tail(&mdesc->node, &mchan->prepared);\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\treturn &mdesc->desc;\n}\n\nstatic struct dma_async_tx_descriptor *\nhidma_prep_dma_memset(struct dma_chan *dmach, dma_addr_t dest, int value,\n\t\tsize_t len, unsigned long flags)\n{\n\tstruct hidma_chan *mchan = to_hidma_chan(dmach);\n\tstruct hidma_desc *mdesc = NULL;\n\tstruct hidma_dev *mdma = mchan->dmadev;\n\tunsigned long irqflags;\n\tu64 byte_pattern, fill_pattern;\n\n\t \n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\tif (!list_empty(&mchan->free)) {\n\t\tmdesc = list_first_entry(&mchan->free, struct hidma_desc, node);\n\t\tlist_del(&mdesc->node);\n\t}\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\tif (!mdesc)\n\t\treturn NULL;\n\n\tbyte_pattern = (char)value;\n\tfill_pattern =\t(byte_pattern << 56) |\n\t\t\t(byte_pattern << 48) |\n\t\t\t(byte_pattern << 40) |\n\t\t\t(byte_pattern << 32) |\n\t\t\t(byte_pattern << 24) |\n\t\t\t(byte_pattern << 16) |\n\t\t\t(byte_pattern << 8) |\n\t\t\tbyte_pattern;\n\n\tmdesc->desc.flags = flags;\n\thidma_ll_set_transfer_params(mdma->lldev, mdesc->tre_ch,\n\t\t\t\t     fill_pattern, dest, len, flags,\n\t\t\t\t     HIDMA_TRE_MEMSET);\n\n\t \n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\tlist_add_tail(&mdesc->node, &mchan->prepared);\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\treturn &mdesc->desc;\n}\n\nstatic int hidma_terminate_channel(struct dma_chan *chan)\n{\n\tstruct hidma_chan *mchan = to_hidma_chan(chan);\n\tstruct hidma_dev *dmadev = to_hidma_dev(mchan->chan.device);\n\tstruct hidma_desc *tmp, *mdesc;\n\tunsigned long irqflags;\n\tLIST_HEAD(list);\n\tint rc;\n\n\tpm_runtime_get_sync(dmadev->ddev.dev);\n\t \n\thidma_process_completed(mchan);\n\n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\tmchan->last_success = 0;\n\tlist_splice_init(&mchan->active, &list);\n\tlist_splice_init(&mchan->prepared, &list);\n\tlist_splice_init(&mchan->completed, &list);\n\tlist_splice_init(&mchan->queued, &list);\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n\n\t \n\trc = hidma_ll_disable(dmadev->lldev);\n\tif (rc) {\n\t\tdev_err(dmadev->ddev.dev, \"channel did not pause\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tlist_for_each_entry_safe(mdesc, tmp, &list, node) {\n\t\tstruct dma_async_tx_descriptor *txd = &mdesc->desc;\n\n\t\tdma_descriptor_unmap(txd);\n\t\tdmaengine_desc_get_callback_invoke(txd, NULL);\n\t\tdma_run_dependencies(txd);\n\n\t\t \n\t\tlist_move(&mdesc->node, &mchan->free);\n\t}\n\n\trc = hidma_ll_enable(dmadev->lldev);\nout:\n\tpm_runtime_mark_last_busy(dmadev->ddev.dev);\n\tpm_runtime_put_autosuspend(dmadev->ddev.dev);\n\treturn rc;\n}\n\nstatic int hidma_terminate_all(struct dma_chan *chan)\n{\n\tstruct hidma_chan *mchan = to_hidma_chan(chan);\n\tstruct hidma_dev *dmadev = to_hidma_dev(mchan->chan.device);\n\tint rc;\n\n\trc = hidma_terminate_channel(chan);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tpm_runtime_get_sync(dmadev->ddev.dev);\n\trc = hidma_ll_setup(dmadev->lldev);\n\tpm_runtime_mark_last_busy(dmadev->ddev.dev);\n\tpm_runtime_put_autosuspend(dmadev->ddev.dev);\n\treturn rc;\n}\n\nstatic void hidma_free_chan_resources(struct dma_chan *dmach)\n{\n\tstruct hidma_chan *mchan = to_hidma_chan(dmach);\n\tstruct hidma_dev *mdma = mchan->dmadev;\n\tstruct hidma_desc *mdesc, *tmp;\n\tunsigned long irqflags;\n\tLIST_HEAD(descs);\n\n\t \n\thidma_terminate_channel(dmach);\n\n\tspin_lock_irqsave(&mchan->lock, irqflags);\n\n\t \n\tlist_splice_tail_init(&mchan->free, &descs);\n\n\t \n\tlist_for_each_entry_safe(mdesc, tmp, &descs, node) {\n\t\thidma_ll_free(mdma->lldev, mdesc->tre_ch);\n\t\tlist_del(&mdesc->node);\n\t\tkfree(mdesc);\n\t}\n\n\tmchan->allocated = false;\n\tspin_unlock_irqrestore(&mchan->lock, irqflags);\n}\n\nstatic int hidma_pause(struct dma_chan *chan)\n{\n\tstruct hidma_chan *mchan;\n\tstruct hidma_dev *dmadev;\n\n\tmchan = to_hidma_chan(chan);\n\tdmadev = to_hidma_dev(mchan->chan.device);\n\tif (!mchan->paused) {\n\t\tpm_runtime_get_sync(dmadev->ddev.dev);\n\t\tif (hidma_ll_disable(dmadev->lldev))\n\t\t\tdev_warn(dmadev->ddev.dev, \"channel did not stop\\n\");\n\t\tmchan->paused = true;\n\t\tpm_runtime_mark_last_busy(dmadev->ddev.dev);\n\t\tpm_runtime_put_autosuspend(dmadev->ddev.dev);\n\t}\n\treturn 0;\n}\n\nstatic int hidma_resume(struct dma_chan *chan)\n{\n\tstruct hidma_chan *mchan;\n\tstruct hidma_dev *dmadev;\n\tint rc = 0;\n\n\tmchan = to_hidma_chan(chan);\n\tdmadev = to_hidma_dev(mchan->chan.device);\n\tif (mchan->paused) {\n\t\tpm_runtime_get_sync(dmadev->ddev.dev);\n\t\trc = hidma_ll_enable(dmadev->lldev);\n\t\tif (!rc)\n\t\t\tmchan->paused = false;\n\t\telse\n\t\t\tdev_err(dmadev->ddev.dev,\n\t\t\t\t\"failed to resume the channel\");\n\t\tpm_runtime_mark_last_busy(dmadev->ddev.dev);\n\t\tpm_runtime_put_autosuspend(dmadev->ddev.dev);\n\t}\n\treturn rc;\n}\n\nstatic irqreturn_t hidma_chirq_handler(int chirq, void *arg)\n{\n\tstruct hidma_lldev *lldev = arg;\n\n\t \n\treturn hidma_ll_inthandler(chirq, lldev);\n}\n\n#ifdef CONFIG_GENERIC_MSI_IRQ\nstatic irqreturn_t hidma_chirq_handler_msi(int chirq, void *arg)\n{\n\tstruct hidma_lldev **lldevp = arg;\n\tstruct hidma_dev *dmadev = to_hidma_dev_from_lldev(lldevp);\n\n\treturn hidma_ll_inthandler_msi(chirq, *lldevp,\n\t\t\t\t       1 << (chirq - dmadev->msi_virqbase));\n}\n#endif\n\nstatic ssize_t hidma_show_values(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct hidma_dev *mdev = dev_get_drvdata(dev);\n\n\tbuf[0] = 0;\n\n\tif (strcmp(attr->attr.name, \"chid\") == 0)\n\t\tsprintf(buf, \"%d\\n\", mdev->chidx);\n\n\treturn strlen(buf);\n}\n\nstatic inline void  hidma_sysfs_uninit(struct hidma_dev *dev)\n{\n\tdevice_remove_file(dev->ddev.dev, dev->chid_attrs);\n}\n\nstatic struct device_attribute*\nhidma_create_sysfs_entry(struct hidma_dev *dev, char *name, int mode)\n{\n\tstruct device_attribute *attrs;\n\tchar *name_copy;\n\n\tattrs = devm_kmalloc(dev->ddev.dev, sizeof(struct device_attribute),\n\t\t\t     GFP_KERNEL);\n\tif (!attrs)\n\t\treturn NULL;\n\n\tname_copy = devm_kstrdup(dev->ddev.dev, name, GFP_KERNEL);\n\tif (!name_copy)\n\t\treturn NULL;\n\n\tattrs->attr.name = name_copy;\n\tattrs->attr.mode = mode;\n\tattrs->show = hidma_show_values;\n\tsysfs_attr_init(&attrs->attr);\n\n\treturn attrs;\n}\n\nstatic int hidma_sysfs_init(struct hidma_dev *dev)\n{\n\tdev->chid_attrs = hidma_create_sysfs_entry(dev, \"chid\", S_IRUGO);\n\tif (!dev->chid_attrs)\n\t\treturn -ENOMEM;\n\n\treturn device_create_file(dev->ddev.dev, dev->chid_attrs);\n}\n\n#ifdef CONFIG_GENERIC_MSI_IRQ\nstatic void hidma_write_msi_msg(struct msi_desc *desc, struct msi_msg *msg)\n{\n\tstruct device *dev = msi_desc_to_dev(desc);\n\tstruct hidma_dev *dmadev = dev_get_drvdata(dev);\n\n\tif (!desc->msi_index) {\n\t\twritel(msg->address_lo, dmadev->dev_evca + 0x118);\n\t\twritel(msg->address_hi, dmadev->dev_evca + 0x11C);\n\t\twritel(msg->data, dmadev->dev_evca + 0x120);\n\t}\n}\n#endif\n\nstatic void hidma_free_msis(struct hidma_dev *dmadev)\n{\n#ifdef CONFIG_GENERIC_MSI_IRQ\n\tstruct device *dev = dmadev->ddev.dev;\n\tint i, virq;\n\n\tfor (i = 0; i < HIDMA_MSI_INTS; i++) {\n\t\tvirq = msi_get_virq(dev, i);\n\t\tif (virq)\n\t\t\tdevm_free_irq(dev, virq, &dmadev->lldev);\n\t}\n\n\tplatform_msi_domain_free_irqs(dev);\n#endif\n}\n\nstatic int hidma_request_msi(struct hidma_dev *dmadev,\n\t\t\t     struct platform_device *pdev)\n{\n#ifdef CONFIG_GENERIC_MSI_IRQ\n\tint rc, i, virq;\n\n\trc = platform_msi_domain_alloc_irqs(&pdev->dev, HIDMA_MSI_INTS,\n\t\t\t\t\t    hidma_write_msi_msg);\n\tif (rc)\n\t\treturn rc;\n\n\tfor (i = 0; i < HIDMA_MSI_INTS; i++) {\n\t\tvirq = msi_get_virq(&pdev->dev, i);\n\t\trc = devm_request_irq(&pdev->dev, virq,\n\t\t\t\t       hidma_chirq_handler_msi,\n\t\t\t\t       0, \"qcom-hidma-msi\",\n\t\t\t\t       &dmadev->lldev);\n\t\tif (rc)\n\t\t\tbreak;\n\t\tif (!i)\n\t\t\tdmadev->msi_virqbase = virq;\n\t}\n\n\tif (rc) {\n\t\t \n\t\tfor (--i; i >= 0; i--) {\n\t\t\tvirq = msi_get_virq(&pdev->dev, i);\n\t\t\tdevm_free_irq(&pdev->dev, virq, &dmadev->lldev);\n\t\t}\n\t\tdev_warn(&pdev->dev,\n\t\t\t \"failed to request MSI irq, falling back to wired IRQ\\n\");\n\t} else {\n\t\t \n\t\thidma_ll_setup_irq(dmadev->lldev, true);\n\t}\n\treturn rc;\n#else\n\treturn -EINVAL;\n#endif\n}\n\nstatic bool hidma_test_capability(struct device *dev, enum hidma_cap test_cap)\n{\n\tenum hidma_cap cap;\n\n\tcap = (enum hidma_cap) device_get_match_data(dev);\n\treturn cap ? ((cap & test_cap) > 0) : 0;\n}\n\nstatic int hidma_probe(struct platform_device *pdev)\n{\n\tstruct hidma_dev *dmadev;\n\tstruct resource *trca_resource;\n\tstruct resource *evca_resource;\n\tint chirq;\n\tvoid __iomem *evca;\n\tvoid __iomem *trca;\n\tint rc;\n\tbool msi;\n\n\tpm_runtime_set_autosuspend_delay(&pdev->dev, HIDMA_AUTOSUSPEND_TIMEOUT);\n\tpm_runtime_use_autosuspend(&pdev->dev);\n\tpm_runtime_set_active(&pdev->dev);\n\tpm_runtime_enable(&pdev->dev);\n\n\ttrca = devm_platform_get_and_ioremap_resource(pdev, 0, &trca_resource);\n\tif (IS_ERR(trca)) {\n\t\trc = PTR_ERR(trca);\n\t\tgoto bailout;\n\t}\n\n\tevca = devm_platform_get_and_ioremap_resource(pdev, 1, &evca_resource);\n\tif (IS_ERR(evca)) {\n\t\trc = PTR_ERR(evca);\n\t\tgoto bailout;\n\t}\n\n\t \n\tchirq = platform_get_irq(pdev, 0);\n\tif (chirq < 0) {\n\t\trc = chirq;\n\t\tgoto bailout;\n\t}\n\n\tdmadev = devm_kzalloc(&pdev->dev, sizeof(*dmadev), GFP_KERNEL);\n\tif (!dmadev) {\n\t\trc = -ENOMEM;\n\t\tgoto bailout;\n\t}\n\n\tINIT_LIST_HEAD(&dmadev->ddev.channels);\n\tspin_lock_init(&dmadev->lock);\n\tdmadev->ddev.dev = &pdev->dev;\n\tpm_runtime_get_sync(dmadev->ddev.dev);\n\n\tdma_cap_set(DMA_MEMCPY, dmadev->ddev.cap_mask);\n\tdma_cap_set(DMA_MEMSET, dmadev->ddev.cap_mask);\n\tif (WARN_ON(!pdev->dev.dma_mask)) {\n\t\trc = -ENXIO;\n\t\tgoto dmafree;\n\t}\n\n\tdmadev->dev_evca = evca;\n\tdmadev->evca_resource = evca_resource;\n\tdmadev->dev_trca = trca;\n\tdmadev->trca_resource = trca_resource;\n\tdmadev->ddev.device_prep_dma_memcpy = hidma_prep_dma_memcpy;\n\tdmadev->ddev.device_prep_dma_memset = hidma_prep_dma_memset;\n\tdmadev->ddev.device_alloc_chan_resources = hidma_alloc_chan_resources;\n\tdmadev->ddev.device_free_chan_resources = hidma_free_chan_resources;\n\tdmadev->ddev.device_tx_status = hidma_tx_status;\n\tdmadev->ddev.device_issue_pending = hidma_issue_pending;\n\tdmadev->ddev.device_pause = hidma_pause;\n\tdmadev->ddev.device_resume = hidma_resume;\n\tdmadev->ddev.device_terminate_all = hidma_terminate_all;\n\tdmadev->ddev.copy_align = 8;\n\n\t \n\tmsi = hidma_test_capability(&pdev->dev, HIDMA_MSI_CAP);\n\tdevice_property_read_u32(&pdev->dev, \"desc-count\",\n\t\t\t\t &dmadev->nr_descriptors);\n\n\tif (nr_desc_prm) {\n\t\tdev_info(&pdev->dev, \"overriding number of descriptors as %d\\n\",\n\t\t\t nr_desc_prm);\n\t\tdmadev->nr_descriptors = nr_desc_prm;\n\t}\n\n\tif (!dmadev->nr_descriptors)\n\t\tdmadev->nr_descriptors = HIDMA_NR_DEFAULT_DESC;\n\n\tif (hidma_test_capability(&pdev->dev, HIDMA_IDENTITY_CAP))\n\t\tdmadev->chidx = readl(dmadev->dev_trca + 0x40);\n\telse\n\t\tdmadev->chidx = readl(dmadev->dev_trca + 0x28);\n\n\t \n\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (rc) {\n\t\tdev_warn(&pdev->dev, \"unable to set coherent mask to 64\");\n\t\tgoto dmafree;\n\t}\n\n\tdmadev->lldev = hidma_ll_init(dmadev->ddev.dev,\n\t\t\t\t      dmadev->nr_descriptors, dmadev->dev_trca,\n\t\t\t\t      dmadev->dev_evca, dmadev->chidx);\n\tif (!dmadev->lldev) {\n\t\trc = -EPROBE_DEFER;\n\t\tgoto dmafree;\n\t}\n\n\tplatform_set_drvdata(pdev, dmadev);\n\tif (msi)\n\t\trc = hidma_request_msi(dmadev, pdev);\n\n\tif (!msi || rc) {\n\t\thidma_ll_setup_irq(dmadev->lldev, false);\n\t\trc = devm_request_irq(&pdev->dev, chirq, hidma_chirq_handler,\n\t\t\t\t      0, \"qcom-hidma\", dmadev->lldev);\n\t\tif (rc)\n\t\t\tgoto uninit;\n\t}\n\n\tINIT_LIST_HEAD(&dmadev->ddev.channels);\n\trc = hidma_chan_init(dmadev, 0);\n\tif (rc)\n\t\tgoto uninit;\n\n\trc = dma_async_device_register(&dmadev->ddev);\n\tif (rc)\n\t\tgoto uninit;\n\n\tdmadev->irq = chirq;\n\ttasklet_setup(&dmadev->task, hidma_issue_task);\n\thidma_debug_init(dmadev);\n\thidma_sysfs_init(dmadev);\n\tdev_info(&pdev->dev, \"HI-DMA engine driver registration complete\\n\");\n\tpm_runtime_mark_last_busy(dmadev->ddev.dev);\n\tpm_runtime_put_autosuspend(dmadev->ddev.dev);\n\treturn 0;\n\nuninit:\n\tif (msi)\n\t\thidma_free_msis(dmadev);\n\n\thidma_ll_uninit(dmadev->lldev);\ndmafree:\n\tif (dmadev)\n\t\thidma_free(dmadev);\nbailout:\n\tpm_runtime_put_sync(&pdev->dev);\n\tpm_runtime_disable(&pdev->dev);\n\treturn rc;\n}\n\nstatic void hidma_shutdown(struct platform_device *pdev)\n{\n\tstruct hidma_dev *dmadev = platform_get_drvdata(pdev);\n\n\tdev_info(dmadev->ddev.dev, \"HI-DMA engine shutdown\\n\");\n\n\tpm_runtime_get_sync(dmadev->ddev.dev);\n\tif (hidma_ll_disable(dmadev->lldev))\n\t\tdev_warn(dmadev->ddev.dev, \"channel did not stop\\n\");\n\tpm_runtime_mark_last_busy(dmadev->ddev.dev);\n\tpm_runtime_put_autosuspend(dmadev->ddev.dev);\n\n}\n\nstatic int hidma_remove(struct platform_device *pdev)\n{\n\tstruct hidma_dev *dmadev = platform_get_drvdata(pdev);\n\n\tpm_runtime_get_sync(dmadev->ddev.dev);\n\tdma_async_device_unregister(&dmadev->ddev);\n\tif (!dmadev->lldev->msi_support)\n\t\tdevm_free_irq(dmadev->ddev.dev, dmadev->irq, dmadev->lldev);\n\telse\n\t\thidma_free_msis(dmadev);\n\n\ttasklet_kill(&dmadev->task);\n\thidma_sysfs_uninit(dmadev);\n\thidma_debug_uninit(dmadev);\n\thidma_ll_uninit(dmadev->lldev);\n\thidma_free(dmadev);\n\n\tdev_info(&pdev->dev, \"HI-DMA engine removed\\n\");\n\tpm_runtime_put_sync_suspend(&pdev->dev);\n\tpm_runtime_disable(&pdev->dev);\n\n\treturn 0;\n}\n\n#if IS_ENABLED(CONFIG_ACPI)\nstatic const struct acpi_device_id hidma_acpi_ids[] = {\n\t{\"QCOM8061\"},\n\t{\"QCOM8062\", HIDMA_MSI_CAP},\n\t{\"QCOM8063\", (HIDMA_MSI_CAP | HIDMA_IDENTITY_CAP)},\n\t{},\n};\nMODULE_DEVICE_TABLE(acpi, hidma_acpi_ids);\n#endif\n\nstatic const struct of_device_id hidma_match[] = {\n\t{.compatible = \"qcom,hidma-1.0\",},\n\t{.compatible = \"qcom,hidma-1.1\", .data = (void *)(HIDMA_MSI_CAP),},\n\t{.compatible = \"qcom,hidma-1.2\",\n\t .data = (void *)(HIDMA_MSI_CAP | HIDMA_IDENTITY_CAP),},\n\t{},\n};\nMODULE_DEVICE_TABLE(of, hidma_match);\n\nstatic struct platform_driver hidma_driver = {\n\t.probe = hidma_probe,\n\t.remove = hidma_remove,\n\t.shutdown = hidma_shutdown,\n\t.driver = {\n\t\t   .name = \"hidma\",\n\t\t   .of_match_table = hidma_match,\n\t\t   .acpi_match_table = ACPI_PTR(hidma_acpi_ids),\n\t},\n};\n\nmodule_platform_driver(hidma_driver);\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}