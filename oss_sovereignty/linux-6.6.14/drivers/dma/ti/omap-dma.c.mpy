{
  "module_name": "omap-dma.c",
  "hash_id": "fa5d4b7c4372f8a913d79627b6d2054a72d9c8783abfeb9558c5837b1849478c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/ti/omap-dma.c",
  "human_readable_source": "\n \n#include <linux/cpu_pm.h>\n#include <linux/delay.h>\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/omap-dma.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n\n#include \"../virt-dma.h\"\n\n#define OMAP_SDMA_REQUESTS\t127\n#define OMAP_SDMA_CHANNELS\t32\n\nstruct omap_dma_config {\n\tint lch_end;\n\tunsigned int rw_priority:1;\n\tunsigned int needs_busy_check:1;\n\tunsigned int may_lose_context:1;\n\tunsigned int needs_lch_clear:1;\n};\n\nstruct omap_dma_context {\n\tu32 irqenable_l0;\n\tu32 irqenable_l1;\n\tu32 ocp_sysconfig;\n\tu32 gcr;\n};\n\nstruct omap_dmadev {\n\tstruct dma_device ddev;\n\tspinlock_t lock;\n\tvoid __iomem *base;\n\tconst struct omap_dma_reg *reg_map;\n\tstruct omap_system_dma_plat_info *plat;\n\tconst struct omap_dma_config *cfg;\n\tstruct notifier_block nb;\n\tstruct omap_dma_context context;\n\tint lch_count;\n\tDECLARE_BITMAP(lch_bitmap, OMAP_SDMA_CHANNELS);\n\tstruct mutex lch_lock;\t\t \n\tbool legacy;\n\tbool ll123_supported;\n\tstruct dma_pool *desc_pool;\n\tunsigned dma_requests;\n\tspinlock_t irq_lock;\n\tuint32_t irq_enable_mask;\n\tstruct omap_chan **lch_map;\n};\n\nstruct omap_chan {\n\tstruct virt_dma_chan vc;\n\tvoid __iomem *channel_base;\n\tconst struct omap_dma_reg *reg_map;\n\tuint32_t ccr;\n\n\tstruct dma_slave_config\tcfg;\n\tunsigned dma_sig;\n\tbool cyclic;\n\tbool paused;\n\tbool running;\n\n\tint dma_ch;\n\tstruct omap_desc *desc;\n\tunsigned sgidx;\n};\n\n#define DESC_NXT_SV_REFRESH\t(0x1 << 24)\n#define DESC_NXT_SV_REUSE\t(0x2 << 24)\n#define DESC_NXT_DV_REFRESH\t(0x1 << 26)\n#define DESC_NXT_DV_REUSE\t(0x2 << 26)\n#define DESC_NTYPE_TYPE2\t(0x2 << 29)\n\n \nstruct omap_type2_desc {\n\tuint32_t next_desc;\n\tuint32_t en;\n\tuint32_t addr;  \n\tuint16_t fn;\n\tuint16_t cicr;\n\tint16_t cdei;\n\tint16_t csei;\n\tint32_t cdfi;\n\tint32_t csfi;\n} __packed;\n\nstruct omap_sg {\n\tdma_addr_t addr;\n\tuint32_t en;\t\t \n\tuint32_t fn;\t\t \n\tint32_t fi;\t\t \n\tint16_t ei;\t\t \n\n\t \n\tstruct omap_type2_desc *t2_desc;\n\tdma_addr_t t2_desc_paddr;\n};\n\nstruct omap_desc {\n\tstruct virt_dma_desc vd;\n\tbool using_ll;\n\tenum dma_transfer_direction dir;\n\tdma_addr_t dev_addr;\n\tbool polled;\n\n\tint32_t fi;\t\t \n\tint16_t ei;\t\t \n\tuint8_t es;\t\t \n\tuint32_t ccr;\t\t \n\tuint16_t clnk_ctrl;\t \n\tuint16_t cicr;\t\t \n\tuint32_t csdp;\t\t \n\n\tunsigned sglen;\n\tstruct omap_sg sg[];\n};\n\nenum {\n\tCAPS_0_SUPPORT_LL123\t= BIT(20),\t \n\tCAPS_0_SUPPORT_LL4\t= BIT(21),\t \n\n\tCCR_FS\t\t\t= BIT(5),\n\tCCR_READ_PRIORITY\t= BIT(6),\n\tCCR_ENABLE\t\t= BIT(7),\n\tCCR_AUTO_INIT\t\t= BIT(8),\t \n\tCCR_REPEAT\t\t= BIT(9),\t \n\tCCR_OMAP31_DISABLE\t= BIT(10),\t \n\tCCR_SUSPEND_SENSITIVE\t= BIT(8),\t \n\tCCR_RD_ACTIVE\t\t= BIT(9),\t \n\tCCR_WR_ACTIVE\t\t= BIT(10),\t \n\tCCR_SRC_AMODE_CONSTANT\t= 0 << 12,\n\tCCR_SRC_AMODE_POSTINC\t= 1 << 12,\n\tCCR_SRC_AMODE_SGLIDX\t= 2 << 12,\n\tCCR_SRC_AMODE_DBLIDX\t= 3 << 12,\n\tCCR_DST_AMODE_CONSTANT\t= 0 << 14,\n\tCCR_DST_AMODE_POSTINC\t= 1 << 14,\n\tCCR_DST_AMODE_SGLIDX\t= 2 << 14,\n\tCCR_DST_AMODE_DBLIDX\t= 3 << 14,\n\tCCR_CONSTANT_FILL\t= BIT(16),\n\tCCR_TRANSPARENT_COPY\t= BIT(17),\n\tCCR_BS\t\t\t= BIT(18),\n\tCCR_SUPERVISOR\t\t= BIT(22),\n\tCCR_PREFETCH\t\t= BIT(23),\n\tCCR_TRIGGER_SRC\t\t= BIT(24),\n\tCCR_BUFFERING_DISABLE\t= BIT(25),\n\tCCR_WRITE_PRIORITY\t= BIT(26),\n\tCCR_SYNC_ELEMENT\t= 0,\n\tCCR_SYNC_FRAME\t\t= CCR_FS,\n\tCCR_SYNC_BLOCK\t\t= CCR_BS,\n\tCCR_SYNC_PACKET\t\t= CCR_BS | CCR_FS,\n\n\tCSDP_DATA_TYPE_8\t= 0,\n\tCSDP_DATA_TYPE_16\t= 1,\n\tCSDP_DATA_TYPE_32\t= 2,\n\tCSDP_SRC_PORT_EMIFF\t= 0 << 2,  \n\tCSDP_SRC_PORT_EMIFS\t= 1 << 2,  \n\tCSDP_SRC_PORT_OCP_T1\t= 2 << 2,  \n\tCSDP_SRC_PORT_TIPB\t= 3 << 2,  \n\tCSDP_SRC_PORT_OCP_T2\t= 4 << 2,  \n\tCSDP_SRC_PORT_MPUI\t= 5 << 2,  \n\tCSDP_SRC_PACKED\t\t= BIT(6),\n\tCSDP_SRC_BURST_1\t= 0 << 7,\n\tCSDP_SRC_BURST_16\t= 1 << 7,\n\tCSDP_SRC_BURST_32\t= 2 << 7,\n\tCSDP_SRC_BURST_64\t= 3 << 7,\n\tCSDP_DST_PORT_EMIFF\t= 0 << 9,  \n\tCSDP_DST_PORT_EMIFS\t= 1 << 9,  \n\tCSDP_DST_PORT_OCP_T1\t= 2 << 9,  \n\tCSDP_DST_PORT_TIPB\t= 3 << 9,  \n\tCSDP_DST_PORT_OCP_T2\t= 4 << 9,  \n\tCSDP_DST_PORT_MPUI\t= 5 << 9,  \n\tCSDP_DST_PACKED\t\t= BIT(13),\n\tCSDP_DST_BURST_1\t= 0 << 14,\n\tCSDP_DST_BURST_16\t= 1 << 14,\n\tCSDP_DST_BURST_32\t= 2 << 14,\n\tCSDP_DST_BURST_64\t= 3 << 14,\n\tCSDP_WRITE_NON_POSTED\t= 0 << 16,\n\tCSDP_WRITE_POSTED\t= 1 << 16,\n\tCSDP_WRITE_LAST_NON_POSTED = 2 << 16,\n\n\tCICR_TOUT_IE\t\t= BIT(0),\t \n\tCICR_DROP_IE\t\t= BIT(1),\n\tCICR_HALF_IE\t\t= BIT(2),\n\tCICR_FRAME_IE\t\t= BIT(3),\n\tCICR_LAST_IE\t\t= BIT(4),\n\tCICR_BLOCK_IE\t\t= BIT(5),\n\tCICR_PKT_IE\t\t= BIT(7),\t \n\tCICR_TRANS_ERR_IE\t= BIT(8),\t \n\tCICR_SUPERVISOR_ERR_IE\t= BIT(10),\t \n\tCICR_MISALIGNED_ERR_IE\t= BIT(11),\t \n\tCICR_DRAIN_IE\t\t= BIT(12),\t \n\tCICR_SUPER_BLOCK_IE\t= BIT(14),\t \n\n\tCLNK_CTRL_ENABLE_LNK\t= BIT(15),\n\n\tCDP_DST_VALID_INC\t= 0 << 0,\n\tCDP_DST_VALID_RELOAD\t= 1 << 0,\n\tCDP_DST_VALID_REUSE\t= 2 << 0,\n\tCDP_SRC_VALID_INC\t= 0 << 2,\n\tCDP_SRC_VALID_RELOAD\t= 1 << 2,\n\tCDP_SRC_VALID_REUSE\t= 2 << 2,\n\tCDP_NTYPE_TYPE1\t\t= 1 << 4,\n\tCDP_NTYPE_TYPE2\t\t= 2 << 4,\n\tCDP_NTYPE_TYPE3\t\t= 3 << 4,\n\tCDP_TMODE_NORMAL\t= 0 << 8,\n\tCDP_TMODE_LLIST\t\t= 1 << 8,\n\tCDP_FAST\t\t= BIT(10),\n};\n\nstatic const unsigned es_bytes[] = {\n\t[CSDP_DATA_TYPE_8] = 1,\n\t[CSDP_DATA_TYPE_16] = 2,\n\t[CSDP_DATA_TYPE_32] = 4,\n};\n\nstatic bool omap_dma_filter_fn(struct dma_chan *chan, void *param);\nstatic struct of_dma_filter_info omap_dma_info = {\n\t.filter_fn = omap_dma_filter_fn,\n};\n\nstatic inline struct omap_dmadev *to_omap_dma_dev(struct dma_device *d)\n{\n\treturn container_of(d, struct omap_dmadev, ddev);\n}\n\nstatic inline struct omap_chan *to_omap_dma_chan(struct dma_chan *c)\n{\n\treturn container_of(c, struct omap_chan, vc.chan);\n}\n\nstatic inline struct omap_desc *to_omap_dma_desc(struct dma_async_tx_descriptor *t)\n{\n\treturn container_of(t, struct omap_desc, vd.tx);\n}\n\nstatic void omap_dma_desc_free(struct virt_dma_desc *vd)\n{\n\tstruct omap_desc *d = to_omap_dma_desc(&vd->tx);\n\n\tif (d->using_ll) {\n\t\tstruct omap_dmadev *od = to_omap_dma_dev(vd->tx.chan->device);\n\t\tint i;\n\n\t\tfor (i = 0; i < d->sglen; i++) {\n\t\t\tif (d->sg[i].t2_desc)\n\t\t\t\tdma_pool_free(od->desc_pool, d->sg[i].t2_desc,\n\t\t\t\t\t      d->sg[i].t2_desc_paddr);\n\t\t}\n\t}\n\n\tkfree(d);\n}\n\nstatic void omap_dma_fill_type2_desc(struct omap_desc *d, int idx,\n\t\t\t\t     enum dma_transfer_direction dir, bool last)\n{\n\tstruct omap_sg *sg = &d->sg[idx];\n\tstruct omap_type2_desc *t2_desc = sg->t2_desc;\n\n\tif (idx)\n\t\td->sg[idx - 1].t2_desc->next_desc = sg->t2_desc_paddr;\n\tif (last)\n\t\tt2_desc->next_desc = 0xfffffffc;\n\n\tt2_desc->en = sg->en;\n\tt2_desc->addr = sg->addr;\n\tt2_desc->fn = sg->fn & 0xffff;\n\tt2_desc->cicr = d->cicr;\n\tif (!last)\n\t\tt2_desc->cicr &= ~CICR_BLOCK_IE;\n\n\tswitch (dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\tt2_desc->cdei = sg->ei;\n\t\tt2_desc->csei = d->ei;\n\t\tt2_desc->cdfi = sg->fi;\n\t\tt2_desc->csfi = d->fi;\n\n\t\tt2_desc->en |= DESC_NXT_DV_REFRESH;\n\t\tt2_desc->en |= DESC_NXT_SV_REUSE;\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\tt2_desc->cdei = d->ei;\n\t\tt2_desc->csei = sg->ei;\n\t\tt2_desc->cdfi = d->fi;\n\t\tt2_desc->csfi = sg->fi;\n\n\t\tt2_desc->en |= DESC_NXT_SV_REFRESH;\n\t\tt2_desc->en |= DESC_NXT_DV_REUSE;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tt2_desc->en |= DESC_NTYPE_TYPE2;\n}\n\nstatic void omap_dma_write(uint32_t val, unsigned type, void __iomem *addr)\n{\n\tswitch (type) {\n\tcase OMAP_DMA_REG_16BIT:\n\t\twritew_relaxed(val, addr);\n\t\tbreak;\n\tcase OMAP_DMA_REG_2X16BIT:\n\t\twritew_relaxed(val, addr);\n\t\twritew_relaxed(val >> 16, addr + 2);\n\t\tbreak;\n\tcase OMAP_DMA_REG_32BIT:\n\t\twritel_relaxed(val, addr);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t}\n}\n\nstatic unsigned omap_dma_read(unsigned type, void __iomem *addr)\n{\n\tunsigned val;\n\n\tswitch (type) {\n\tcase OMAP_DMA_REG_16BIT:\n\t\tval = readw_relaxed(addr);\n\t\tbreak;\n\tcase OMAP_DMA_REG_2X16BIT:\n\t\tval = readw_relaxed(addr);\n\t\tval |= readw_relaxed(addr + 2) << 16;\n\t\tbreak;\n\tcase OMAP_DMA_REG_32BIT:\n\t\tval = readl_relaxed(addr);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\tval = 0;\n\t}\n\n\treturn val;\n}\n\nstatic void omap_dma_glbl_write(struct omap_dmadev *od, unsigned reg, unsigned val)\n{\n\tconst struct omap_dma_reg *r = od->reg_map + reg;\n\n\tWARN_ON(r->stride);\n\n\tomap_dma_write(val, r->type, od->base + r->offset);\n}\n\nstatic unsigned omap_dma_glbl_read(struct omap_dmadev *od, unsigned reg)\n{\n\tconst struct omap_dma_reg *r = od->reg_map + reg;\n\n\tWARN_ON(r->stride);\n\n\treturn omap_dma_read(r->type, od->base + r->offset);\n}\n\nstatic void omap_dma_chan_write(struct omap_chan *c, unsigned reg, unsigned val)\n{\n\tconst struct omap_dma_reg *r = c->reg_map + reg;\n\n\tomap_dma_write(val, r->type, c->channel_base + r->offset);\n}\n\nstatic unsigned omap_dma_chan_read(struct omap_chan *c, unsigned reg)\n{\n\tconst struct omap_dma_reg *r = c->reg_map + reg;\n\n\treturn omap_dma_read(r->type, c->channel_base + r->offset);\n}\n\nstatic void omap_dma_clear_csr(struct omap_chan *c)\n{\n\tif (dma_omap1())\n\t\tomap_dma_chan_read(c, CSR);\n\telse\n\t\tomap_dma_chan_write(c, CSR, ~0);\n}\n\nstatic unsigned omap_dma_get_csr(struct omap_chan *c)\n{\n\tunsigned val = omap_dma_chan_read(c, CSR);\n\n\tif (!dma_omap1())\n\t\tomap_dma_chan_write(c, CSR, val);\n\n\treturn val;\n}\n\nstatic void omap_dma_clear_lch(struct omap_dmadev *od, int lch)\n{\n\tstruct omap_chan *c;\n\tint i;\n\n\tc = od->lch_map[lch];\n\tif (!c)\n\t\treturn;\n\n\tfor (i = CSDP; i <= od->cfg->lch_end; i++)\n\t\tomap_dma_chan_write(c, i, 0);\n}\n\nstatic void omap_dma_assign(struct omap_dmadev *od, struct omap_chan *c,\n\tunsigned lch)\n{\n\tc->channel_base = od->base + od->plat->channel_stride * lch;\n\n\tod->lch_map[lch] = c;\n}\n\nstatic void omap_dma_start(struct omap_chan *c, struct omap_desc *d)\n{\n\tstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\n\tuint16_t cicr = d->cicr;\n\n\tif (__dma_omap15xx(od->plat->dma_attr))\n\t\tomap_dma_chan_write(c, CPC, 0);\n\telse\n\t\tomap_dma_chan_write(c, CDAC, 0);\n\n\tomap_dma_clear_csr(c);\n\n\tif (d->using_ll) {\n\t\tuint32_t cdp = CDP_TMODE_LLIST | CDP_NTYPE_TYPE2 | CDP_FAST;\n\n\t\tif (d->dir == DMA_DEV_TO_MEM)\n\t\t\tcdp |= (CDP_DST_VALID_RELOAD | CDP_SRC_VALID_REUSE);\n\t\telse\n\t\t\tcdp |= (CDP_DST_VALID_REUSE | CDP_SRC_VALID_RELOAD);\n\t\tomap_dma_chan_write(c, CDP, cdp);\n\n\t\tomap_dma_chan_write(c, CNDP, d->sg[0].t2_desc_paddr);\n\t\tomap_dma_chan_write(c, CCDN, 0);\n\t\tomap_dma_chan_write(c, CCFN, 0xffff);\n\t\tomap_dma_chan_write(c, CCEN, 0xffffff);\n\n\t\tcicr &= ~CICR_BLOCK_IE;\n\t} else if (od->ll123_supported) {\n\t\tomap_dma_chan_write(c, CDP, 0);\n\t}\n\n\t \n\tomap_dma_chan_write(c, CICR, cicr);\n\n\t \n\tomap_dma_chan_write(c, CCR, d->ccr | CCR_ENABLE);\n\n\tc->running = true;\n}\n\nstatic void omap_dma_drain_chan(struct omap_chan *c)\n{\n\tint i;\n\tu32 val;\n\n\t \n\tfor (i = 0; ; i++) {\n\t\tval = omap_dma_chan_read(c, CCR);\n\t\tif (!(val & (CCR_RD_ACTIVE | CCR_WR_ACTIVE)))\n\t\t\tbreak;\n\n\t\tif (i > 100)\n\t\t\tbreak;\n\n\t\tudelay(5);\n\t}\n\n\tif (val & (CCR_RD_ACTIVE | CCR_WR_ACTIVE))\n\t\tdev_err(c->vc.chan.device->dev,\n\t\t\t\"DMA drain did not complete on lch %d\\n\",\n\t\t\tc->dma_ch);\n}\n\nstatic int omap_dma_stop(struct omap_chan *c)\n{\n\tstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\n\tuint32_t val;\n\n\t \n\tomap_dma_chan_write(c, CICR, 0);\n\n\tomap_dma_clear_csr(c);\n\n\tval = omap_dma_chan_read(c, CCR);\n\tif (od->plat->errata & DMA_ERRATA_i541 && val & CCR_TRIGGER_SRC) {\n\t\tuint32_t sysconfig;\n\n\t\tsysconfig = omap_dma_glbl_read(od, OCP_SYSCONFIG);\n\t\tval = sysconfig & ~DMA_SYSCONFIG_MIDLEMODE_MASK;\n\t\tval |= DMA_SYSCONFIG_MIDLEMODE(DMA_IDLEMODE_NO_IDLE);\n\t\tomap_dma_glbl_write(od, OCP_SYSCONFIG, val);\n\n\t\tval = omap_dma_chan_read(c, CCR);\n\t\tval &= ~CCR_ENABLE;\n\t\tomap_dma_chan_write(c, CCR, val);\n\n\t\tif (!(c->ccr & CCR_BUFFERING_DISABLE))\n\t\t\tomap_dma_drain_chan(c);\n\n\t\tomap_dma_glbl_write(od, OCP_SYSCONFIG, sysconfig);\n\t} else {\n\t\tif (!(val & CCR_ENABLE))\n\t\t\treturn -EINVAL;\n\n\t\tval &= ~CCR_ENABLE;\n\t\tomap_dma_chan_write(c, CCR, val);\n\n\t\tif (!(c->ccr & CCR_BUFFERING_DISABLE))\n\t\t\tomap_dma_drain_chan(c);\n\t}\n\n\tmb();\n\n\tif (!__dma_omap15xx(od->plat->dma_attr) && c->cyclic) {\n\t\tval = omap_dma_chan_read(c, CLNK_CTRL);\n\n\t\tif (dma_omap1())\n\t\t\tval |= 1 << 14;  \n\t\telse\n\t\t\tval &= ~CLNK_CTRL_ENABLE_LNK;\n\n\t\tomap_dma_chan_write(c, CLNK_CTRL, val);\n\t}\n\tc->running = false;\n\treturn 0;\n}\n\nstatic void omap_dma_start_sg(struct omap_chan *c, struct omap_desc *d)\n{\n\tstruct omap_sg *sg = d->sg + c->sgidx;\n\tunsigned cxsa, cxei, cxfi;\n\n\tif (d->dir == DMA_DEV_TO_MEM || d->dir == DMA_MEM_TO_MEM) {\n\t\tcxsa = CDSA;\n\t\tcxei = CDEI;\n\t\tcxfi = CDFI;\n\t} else {\n\t\tcxsa = CSSA;\n\t\tcxei = CSEI;\n\t\tcxfi = CSFI;\n\t}\n\n\tomap_dma_chan_write(c, cxsa, sg->addr);\n\tomap_dma_chan_write(c, cxei, sg->ei);\n\tomap_dma_chan_write(c, cxfi, sg->fi);\n\tomap_dma_chan_write(c, CEN, sg->en);\n\tomap_dma_chan_write(c, CFN, sg->fn);\n\n\tomap_dma_start(c, d);\n\tc->sgidx++;\n}\n\nstatic void omap_dma_start_desc(struct omap_chan *c)\n{\n\tstruct virt_dma_desc *vd = vchan_next_desc(&c->vc);\n\tstruct omap_desc *d;\n\tunsigned cxsa, cxei, cxfi;\n\n\tif (!vd) {\n\t\tc->desc = NULL;\n\t\treturn;\n\t}\n\n\tlist_del(&vd->node);\n\n\tc->desc = d = to_omap_dma_desc(&vd->tx);\n\tc->sgidx = 0;\n\n\t \n\tmb();\n\n\tomap_dma_chan_write(c, CCR, d->ccr);\n\tif (dma_omap1())\n\t\tomap_dma_chan_write(c, CCR2, d->ccr >> 16);\n\n\tif (d->dir == DMA_DEV_TO_MEM || d->dir == DMA_MEM_TO_MEM) {\n\t\tcxsa = CSSA;\n\t\tcxei = CSEI;\n\t\tcxfi = CSFI;\n\t} else {\n\t\tcxsa = CDSA;\n\t\tcxei = CDEI;\n\t\tcxfi = CDFI;\n\t}\n\n\tomap_dma_chan_write(c, cxsa, d->dev_addr);\n\tomap_dma_chan_write(c, cxei, d->ei);\n\tomap_dma_chan_write(c, cxfi, d->fi);\n\tomap_dma_chan_write(c, CSDP, d->csdp);\n\tomap_dma_chan_write(c, CLNK_CTRL, d->clnk_ctrl);\n\n\tomap_dma_start_sg(c, d);\n}\n\nstatic void omap_dma_callback(int ch, u16 status, void *data)\n{\n\tstruct omap_chan *c = data;\n\tstruct omap_desc *d;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&c->vc.lock, flags);\n\td = c->desc;\n\tif (d) {\n\t\tif (c->cyclic) {\n\t\t\tvchan_cyclic_callback(&d->vd);\n\t\t} else if (d->using_ll || c->sgidx == d->sglen) {\n\t\t\tomap_dma_start_desc(c);\n\t\t\tvchan_cookie_complete(&d->vd);\n\t\t} else {\n\t\t\tomap_dma_start_sg(c, d);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&c->vc.lock, flags);\n}\n\nstatic irqreturn_t omap_dma_irq(int irq, void *devid)\n{\n\tstruct omap_dmadev *od = devid;\n\tunsigned status, channel;\n\n\tspin_lock(&od->irq_lock);\n\n\tstatus = omap_dma_glbl_read(od, IRQSTATUS_L1);\n\tstatus &= od->irq_enable_mask;\n\tif (status == 0) {\n\t\tspin_unlock(&od->irq_lock);\n\t\treturn IRQ_NONE;\n\t}\n\n\twhile ((channel = ffs(status)) != 0) {\n\t\tunsigned mask, csr;\n\t\tstruct omap_chan *c;\n\n\t\tchannel -= 1;\n\t\tmask = BIT(channel);\n\t\tstatus &= ~mask;\n\n\t\tc = od->lch_map[channel];\n\t\tif (c == NULL) {\n\t\t\t \n\t\t\tdev_err(od->ddev.dev, \"invalid channel %u\\n\", channel);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcsr = omap_dma_get_csr(c);\n\t\tomap_dma_glbl_write(od, IRQSTATUS_L1, mask);\n\n\t\tomap_dma_callback(channel, csr, c);\n\t}\n\n\tspin_unlock(&od->irq_lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int omap_dma_get_lch(struct omap_dmadev *od, int *lch)\n{\n\tint channel;\n\n\tmutex_lock(&od->lch_lock);\n\tchannel = find_first_zero_bit(od->lch_bitmap, od->lch_count);\n\tif (channel >= od->lch_count)\n\t\tgoto out_busy;\n\tset_bit(channel, od->lch_bitmap);\n\tmutex_unlock(&od->lch_lock);\n\n\tomap_dma_clear_lch(od, channel);\n\t*lch = channel;\n\n\treturn 0;\n\nout_busy:\n\tmutex_unlock(&od->lch_lock);\n\t*lch = -EINVAL;\n\n\treturn -EBUSY;\n}\n\nstatic void omap_dma_put_lch(struct omap_dmadev *od, int lch)\n{\n\tomap_dma_clear_lch(od, lch);\n\tmutex_lock(&od->lch_lock);\n\tclear_bit(lch, od->lch_bitmap);\n\tmutex_unlock(&od->lch_lock);\n}\n\nstatic inline bool omap_dma_legacy(struct omap_dmadev *od)\n{\n\treturn IS_ENABLED(CONFIG_ARCH_OMAP1) && od->legacy;\n}\n\nstatic int omap_dma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tstruct device *dev = od->ddev.dev;\n\tint ret;\n\n\tif (omap_dma_legacy(od)) {\n\t\tret = omap_request_dma(c->dma_sig, \"DMA engine\",\n\t\t\t\t       omap_dma_callback, c, &c->dma_ch);\n\t} else {\n\t\tret = omap_dma_get_lch(od, &c->dma_ch);\n\t}\n\n\tdev_dbg(dev, \"allocating channel %u for %u\\n\", c->dma_ch, c->dma_sig);\n\n\tif (ret >= 0) {\n\t\tomap_dma_assign(od, c, c->dma_ch);\n\n\t\tif (!omap_dma_legacy(od)) {\n\t\t\tunsigned val;\n\n\t\t\tspin_lock_irq(&od->irq_lock);\n\t\t\tval = BIT(c->dma_ch);\n\t\t\tomap_dma_glbl_write(od, IRQSTATUS_L1, val);\n\t\t\tod->irq_enable_mask |= val;\n\t\t\tomap_dma_glbl_write(od, IRQENABLE_L1, od->irq_enable_mask);\n\n\t\t\tval = omap_dma_glbl_read(od, IRQENABLE_L0);\n\t\t\tval &= ~BIT(c->dma_ch);\n\t\t\tomap_dma_glbl_write(od, IRQENABLE_L0, val);\n\t\t\tspin_unlock_irq(&od->irq_lock);\n\t\t}\n\t}\n\n\tif (dma_omap1()) {\n\t\tif (__dma_omap16xx(od->plat->dma_attr)) {\n\t\t\tc->ccr = CCR_OMAP31_DISABLE;\n\t\t\t \n\t\t\tc->ccr |= c->dma_ch + 1;\n\t\t} else {\n\t\t\tc->ccr = c->dma_sig & 0x1f;\n\t\t}\n\t} else {\n\t\tc->ccr = c->dma_sig & 0x1f;\n\t\tc->ccr |= (c->dma_sig & ~0x1f) << 14;\n\t}\n\tif (od->plat->errata & DMA_ERRATA_IFRAME_BUFFERING)\n\t\tc->ccr |= CCR_BUFFERING_DISABLE;\n\n\treturn ret;\n}\n\nstatic void omap_dma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\n\tif (!omap_dma_legacy(od)) {\n\t\tspin_lock_irq(&od->irq_lock);\n\t\tod->irq_enable_mask &= ~BIT(c->dma_ch);\n\t\tomap_dma_glbl_write(od, IRQENABLE_L1, od->irq_enable_mask);\n\t\tspin_unlock_irq(&od->irq_lock);\n\t}\n\n\tc->channel_base = NULL;\n\tod->lch_map[c->dma_ch] = NULL;\n\tvchan_free_chan_resources(&c->vc);\n\n\tif (omap_dma_legacy(od))\n\t\tomap_free_dma(c->dma_ch);\n\telse\n\t\tomap_dma_put_lch(od, c->dma_ch);\n\n\tdev_dbg(od->ddev.dev, \"freeing channel %u used for %u\\n\", c->dma_ch,\n\t\tc->dma_sig);\n\tc->dma_sig = 0;\n}\n\nstatic size_t omap_dma_sg_size(struct omap_sg *sg)\n{\n\treturn sg->en * sg->fn;\n}\n\nstatic size_t omap_dma_desc_size(struct omap_desc *d)\n{\n\tunsigned i;\n\tsize_t size;\n\n\tfor (size = i = 0; i < d->sglen; i++)\n\t\tsize += omap_dma_sg_size(&d->sg[i]);\n\n\treturn size * es_bytes[d->es];\n}\n\nstatic size_t omap_dma_desc_size_pos(struct omap_desc *d, dma_addr_t addr)\n{\n\tunsigned i;\n\tsize_t size, es_size = es_bytes[d->es];\n\n\tfor (size = i = 0; i < d->sglen; i++) {\n\t\tsize_t this_size = omap_dma_sg_size(&d->sg[i]) * es_size;\n\n\t\tif (size)\n\t\t\tsize += this_size;\n\t\telse if (addr >= d->sg[i].addr &&\n\t\t\t addr < d->sg[i].addr + this_size)\n\t\t\tsize += d->sg[i].addr + this_size - addr;\n\t}\n\treturn size;\n}\n\n \nstatic uint32_t omap_dma_chan_read_3_3(struct omap_chan *c, unsigned reg)\n{\n\tstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\n\tuint32_t val;\n\n\tval = omap_dma_chan_read(c, reg);\n\tif (val == 0 && od->plat->errata & DMA_ERRATA_3_3)\n\t\tval = omap_dma_chan_read(c, reg);\n\n\treturn val;\n}\n\nstatic dma_addr_t omap_dma_get_src_pos(struct omap_chan *c)\n{\n\tstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\n\tdma_addr_t addr, cdac;\n\n\tif (__dma_omap15xx(od->plat->dma_attr)) {\n\t\taddr = omap_dma_chan_read(c, CPC);\n\t} else {\n\t\taddr = omap_dma_chan_read_3_3(c, CSAC);\n\t\tcdac = omap_dma_chan_read_3_3(c, CDAC);\n\n\t\t \n\t\tif (cdac == 0)\n\t\t\taddr = omap_dma_chan_read(c, CSSA);\n\t}\n\n\tif (dma_omap1())\n\t\taddr |= omap_dma_chan_read(c, CSSA) & 0xffff0000;\n\n\treturn addr;\n}\n\nstatic dma_addr_t omap_dma_get_dst_pos(struct omap_chan *c)\n{\n\tstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\n\tdma_addr_t addr;\n\n\tif (__dma_omap15xx(od->plat->dma_attr)) {\n\t\taddr = omap_dma_chan_read(c, CPC);\n\t} else {\n\t\taddr = omap_dma_chan_read_3_3(c, CDAC);\n\n\t\t \n\t\tif (addr == 0)\n\t\t\taddr = omap_dma_chan_read(c, CDSA);\n\t}\n\n\tif (dma_omap1())\n\t\taddr |= omap_dma_chan_read(c, CDSA) & 0xffff0000;\n\n\treturn addr;\n}\n\nstatic enum dma_status omap_dma_tx_status(struct dma_chan *chan,\n\tdma_cookie_t cookie, struct dma_tx_state *txstate)\n{\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tenum dma_status ret;\n\tunsigned long flags;\n\tstruct omap_desc *d = NULL;\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&c->vc.lock, flags);\n\tif (c->desc && c->desc->vd.tx.cookie == cookie)\n\t\td = c->desc;\n\n\tif (!txstate)\n\t\tgoto out;\n\n\tif (d) {\n\t\tdma_addr_t pos;\n\n\t\tif (d->dir == DMA_MEM_TO_DEV)\n\t\t\tpos = omap_dma_get_src_pos(c);\n\t\telse if (d->dir == DMA_DEV_TO_MEM  || d->dir == DMA_MEM_TO_MEM)\n\t\t\tpos = omap_dma_get_dst_pos(c);\n\t\telse\n\t\t\tpos = 0;\n\n\t\ttxstate->residue = omap_dma_desc_size_pos(d, pos);\n\t} else {\n\t\tstruct virt_dma_desc *vd = vchan_find_desc(&c->vc, cookie);\n\n\t\tif (vd)\n\t\t\ttxstate->residue = omap_dma_desc_size(\n\t\t\t\t\t\tto_omap_dma_desc(&vd->tx));\n\t\telse\n\t\t\ttxstate->residue = 0;\n\t}\n\nout:\n\tif (ret == DMA_IN_PROGRESS && c->paused) {\n\t\tret = DMA_PAUSED;\n\t} else if (d && d->polled && c->running) {\n\t\tuint32_t ccr = omap_dma_chan_read(c, CCR);\n\t\t \n\t\tif (!(ccr & CCR_ENABLE)) {\n\t\t\tret = DMA_COMPLETE;\n\t\t\tomap_dma_start_desc(c);\n\t\t\tvchan_cookie_complete(&d->vd);\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&c->vc.lock, flags);\n\n\treturn ret;\n}\n\nstatic void omap_dma_issue_pending(struct dma_chan *chan)\n{\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&c->vc.lock, flags);\n\tif (vchan_issue_pending(&c->vc) && !c->desc)\n\t\tomap_dma_start_desc(c);\n\tspin_unlock_irqrestore(&c->vc.lock, flags);\n}\n\nstatic struct dma_async_tx_descriptor *omap_dma_prep_slave_sg(\n\tstruct dma_chan *chan, struct scatterlist *sgl, unsigned sglen,\n\tenum dma_transfer_direction dir, unsigned long tx_flags, void *context)\n{\n\tstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tenum dma_slave_buswidth dev_width;\n\tstruct scatterlist *sgent;\n\tstruct omap_desc *d;\n\tdma_addr_t dev_addr;\n\tunsigned i, es, en, frame_bytes;\n\tbool ll_failed = false;\n\tu32 burst;\n\tu32 port_window, port_window_bytes;\n\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\tdev_addr = c->cfg.src_addr;\n\t\tdev_width = c->cfg.src_addr_width;\n\t\tburst = c->cfg.src_maxburst;\n\t\tport_window = c->cfg.src_port_window_size;\n\t} else if (dir == DMA_MEM_TO_DEV) {\n\t\tdev_addr = c->cfg.dst_addr;\n\t\tdev_width = c->cfg.dst_addr_width;\n\t\tburst = c->cfg.dst_maxburst;\n\t\tport_window = c->cfg.dst_port_window_size;\n\t} else {\n\t\tdev_err(chan->device->dev, \"%s: bad direction?\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\t \n\tswitch (dev_width) {\n\tcase DMA_SLAVE_BUSWIDTH_1_BYTE:\n\t\tes = CSDP_DATA_TYPE_8;\n\t\tbreak;\n\tcase DMA_SLAVE_BUSWIDTH_2_BYTES:\n\t\tes = CSDP_DATA_TYPE_16;\n\t\tbreak;\n\tcase DMA_SLAVE_BUSWIDTH_4_BYTES:\n\t\tes = CSDP_DATA_TYPE_32;\n\t\tbreak;\n\tdefault:  \n\t\treturn NULL;\n\t}\n\n\t \n\td = kzalloc(struct_size(d, sg, sglen), GFP_ATOMIC);\n\tif (!d)\n\t\treturn NULL;\n\n\td->dir = dir;\n\td->dev_addr = dev_addr;\n\td->es = es;\n\n\t \n\tif (port_window) {\n\t\tburst = port_window;\n\t\tport_window_bytes = port_window * es_bytes[es];\n\n\t\td->ei = 1;\n\t\t \n\t\td->fi = -(port_window_bytes - 1);\n\t}\n\n\td->ccr = c->ccr | CCR_SYNC_FRAME;\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\td->csdp = CSDP_DST_BURST_64 | CSDP_DST_PACKED;\n\n\t\td->ccr |= CCR_DST_AMODE_POSTINC;\n\t\tif (port_window) {\n\t\t\td->ccr |= CCR_SRC_AMODE_DBLIDX;\n\n\t\t\tif (port_window_bytes >= 64)\n\t\t\t\td->csdp |= CSDP_SRC_BURST_64;\n\t\t\telse if (port_window_bytes >= 32)\n\t\t\t\td->csdp |= CSDP_SRC_BURST_32;\n\t\t\telse if (port_window_bytes >= 16)\n\t\t\t\td->csdp |= CSDP_SRC_BURST_16;\n\n\t\t} else {\n\t\t\td->ccr |= CCR_SRC_AMODE_CONSTANT;\n\t\t}\n\t} else {\n\t\td->csdp = CSDP_SRC_BURST_64 | CSDP_SRC_PACKED;\n\n\t\td->ccr |= CCR_SRC_AMODE_POSTINC;\n\t\tif (port_window) {\n\t\t\td->ccr |= CCR_DST_AMODE_DBLIDX;\n\n\t\t\tif (port_window_bytes >= 64)\n\t\t\t\td->csdp |= CSDP_DST_BURST_64;\n\t\t\telse if (port_window_bytes >= 32)\n\t\t\t\td->csdp |= CSDP_DST_BURST_32;\n\t\t\telse if (port_window_bytes >= 16)\n\t\t\t\td->csdp |= CSDP_DST_BURST_16;\n\t\t} else {\n\t\t\td->ccr |= CCR_DST_AMODE_CONSTANT;\n\t\t}\n\t}\n\n\td->cicr = CICR_DROP_IE | CICR_BLOCK_IE;\n\td->csdp |= es;\n\n\tif (dma_omap1()) {\n\t\td->cicr |= CICR_TOUT_IE;\n\n\t\tif (dir == DMA_DEV_TO_MEM)\n\t\t\td->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_TIPB;\n\t\telse\n\t\t\td->csdp |= CSDP_DST_PORT_TIPB | CSDP_SRC_PORT_EMIFF;\n\t} else {\n\t\tif (dir == DMA_DEV_TO_MEM)\n\t\t\td->ccr |= CCR_TRIGGER_SRC;\n\n\t\td->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\n\n\t\tif (port_window)\n\t\t\td->csdp |= CSDP_WRITE_LAST_NON_POSTED;\n\t}\n\tif (od->plat->errata & DMA_ERRATA_PARALLEL_CHANNELS)\n\t\td->clnk_ctrl = c->dma_ch;\n\n\t \n\ten = burst;\n\tframe_bytes = es_bytes[es] * en;\n\n\tif (sglen >= 2)\n\t\td->using_ll = od->ll123_supported;\n\n\tfor_each_sg(sgl, sgent, sglen, i) {\n\t\tstruct omap_sg *osg = &d->sg[i];\n\n\t\tosg->addr = sg_dma_address(sgent);\n\t\tosg->en = en;\n\t\tosg->fn = sg_dma_len(sgent) / frame_bytes;\n\n\t\tif (d->using_ll) {\n\t\t\tosg->t2_desc = dma_pool_alloc(od->desc_pool, GFP_ATOMIC,\n\t\t\t\t\t\t      &osg->t2_desc_paddr);\n\t\t\tif (!osg->t2_desc) {\n\t\t\t\tdev_err(chan->device->dev,\n\t\t\t\t\t\"t2_desc[%d] allocation failed\\n\", i);\n\t\t\t\tll_failed = true;\n\t\t\t\td->using_ll = false;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tomap_dma_fill_type2_desc(d, i, dir, (i == sglen - 1));\n\t\t}\n\t}\n\n\td->sglen = sglen;\n\n\t \n\tif (ll_failed) {\n\t\tfor (i = 0; i < d->sglen; i++) {\n\t\t\tstruct omap_sg *osg = &d->sg[i];\n\n\t\t\tif (osg->t2_desc) {\n\t\t\t\tdma_pool_free(od->desc_pool, osg->t2_desc,\n\t\t\t\t\t      osg->t2_desc_paddr);\n\t\t\t\tosg->t2_desc = NULL;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn vchan_tx_prep(&c->vc, &d->vd, tx_flags);\n}\n\nstatic struct dma_async_tx_descriptor *omap_dma_prep_dma_cyclic(\n\tstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\n\tsize_t period_len, enum dma_transfer_direction dir, unsigned long flags)\n{\n\tstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tenum dma_slave_buswidth dev_width;\n\tstruct omap_desc *d;\n\tdma_addr_t dev_addr;\n\tunsigned es;\n\tu32 burst;\n\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\tdev_addr = c->cfg.src_addr;\n\t\tdev_width = c->cfg.src_addr_width;\n\t\tburst = c->cfg.src_maxburst;\n\t} else if (dir == DMA_MEM_TO_DEV) {\n\t\tdev_addr = c->cfg.dst_addr;\n\t\tdev_width = c->cfg.dst_addr_width;\n\t\tburst = c->cfg.dst_maxburst;\n\t} else {\n\t\tdev_err(chan->device->dev, \"%s: bad direction?\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\t \n\tswitch (dev_width) {\n\tcase DMA_SLAVE_BUSWIDTH_1_BYTE:\n\t\tes = CSDP_DATA_TYPE_8;\n\t\tbreak;\n\tcase DMA_SLAVE_BUSWIDTH_2_BYTES:\n\t\tes = CSDP_DATA_TYPE_16;\n\t\tbreak;\n\tcase DMA_SLAVE_BUSWIDTH_4_BYTES:\n\t\tes = CSDP_DATA_TYPE_32;\n\t\tbreak;\n\tdefault:  \n\t\treturn NULL;\n\t}\n\n\t \n\td = kzalloc(sizeof(*d) + sizeof(d->sg[0]), GFP_ATOMIC);\n\tif (!d)\n\t\treturn NULL;\n\n\td->dir = dir;\n\td->dev_addr = dev_addr;\n\td->fi = burst;\n\td->es = es;\n\td->sg[0].addr = buf_addr;\n\td->sg[0].en = period_len / es_bytes[es];\n\td->sg[0].fn = buf_len / period_len;\n\td->sglen = 1;\n\n\td->ccr = c->ccr;\n\tif (dir == DMA_DEV_TO_MEM)\n\t\td->ccr |= CCR_DST_AMODE_POSTINC | CCR_SRC_AMODE_CONSTANT;\n\telse\n\t\td->ccr |= CCR_DST_AMODE_CONSTANT | CCR_SRC_AMODE_POSTINC;\n\n\td->cicr = CICR_DROP_IE;\n\tif (flags & DMA_PREP_INTERRUPT)\n\t\td->cicr |= CICR_FRAME_IE;\n\n\td->csdp = es;\n\n\tif (dma_omap1()) {\n\t\td->cicr |= CICR_TOUT_IE;\n\n\t\tif (dir == DMA_DEV_TO_MEM)\n\t\t\td->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_MPUI;\n\t\telse\n\t\t\td->csdp |= CSDP_DST_PORT_MPUI | CSDP_SRC_PORT_EMIFF;\n\t} else {\n\t\tif (burst)\n\t\t\td->ccr |= CCR_SYNC_PACKET;\n\t\telse\n\t\t\td->ccr |= CCR_SYNC_ELEMENT;\n\n\t\tif (dir == DMA_DEV_TO_MEM) {\n\t\t\td->ccr |= CCR_TRIGGER_SRC;\n\t\t\td->csdp |= CSDP_DST_PACKED;\n\t\t} else {\n\t\t\td->csdp |= CSDP_SRC_PACKED;\n\t\t}\n\n\t\td->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\n\n\t\td->csdp |= CSDP_DST_BURST_64 | CSDP_SRC_BURST_64;\n\t}\n\n\tif (__dma_omap15xx(od->plat->dma_attr))\n\t\td->ccr |= CCR_AUTO_INIT | CCR_REPEAT;\n\telse\n\t\td->clnk_ctrl = c->dma_ch | CLNK_CTRL_ENABLE_LNK;\n\n\tc->cyclic = true;\n\n\treturn vchan_tx_prep(&c->vc, &d->vd, flags);\n}\n\nstatic struct dma_async_tx_descriptor *omap_dma_prep_dma_memcpy(\n\tstruct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\n\tsize_t len, unsigned long tx_flags)\n{\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tstruct omap_desc *d;\n\tuint8_t data_type;\n\n\td = kzalloc(sizeof(*d) + sizeof(d->sg[0]), GFP_ATOMIC);\n\tif (!d)\n\t\treturn NULL;\n\n\tdata_type = __ffs((src | dest | len));\n\tif (data_type > CSDP_DATA_TYPE_32)\n\t\tdata_type = CSDP_DATA_TYPE_32;\n\n\td->dir = DMA_MEM_TO_MEM;\n\td->dev_addr = src;\n\td->fi = 0;\n\td->es = data_type;\n\td->sg[0].en = len / BIT(data_type);\n\td->sg[0].fn = 1;\n\td->sg[0].addr = dest;\n\td->sglen = 1;\n\td->ccr = c->ccr;\n\td->ccr |= CCR_DST_AMODE_POSTINC | CCR_SRC_AMODE_POSTINC;\n\n\tif (tx_flags & DMA_PREP_INTERRUPT)\n\t\td->cicr |= CICR_FRAME_IE;\n\telse\n\t\td->polled = true;\n\n\td->csdp = data_type;\n\n\tif (dma_omap1()) {\n\t\td->cicr |= CICR_TOUT_IE;\n\t\td->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_EMIFF;\n\t} else {\n\t\td->csdp |= CSDP_DST_PACKED | CSDP_SRC_PACKED;\n\t\td->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\n\t\td->csdp |= CSDP_DST_BURST_64 | CSDP_SRC_BURST_64;\n\t}\n\n\treturn vchan_tx_prep(&c->vc, &d->vd, tx_flags);\n}\n\nstatic struct dma_async_tx_descriptor *omap_dma_prep_dma_interleaved(\n\tstruct dma_chan *chan, struct dma_interleaved_template *xt,\n\tunsigned long flags)\n{\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tstruct omap_desc *d;\n\tstruct omap_sg *sg;\n\tuint8_t data_type;\n\tsize_t src_icg, dst_icg;\n\n\t \n\tif (is_slave_direction(xt->dir))\n\t\treturn NULL;\n\n\tif (xt->frame_size != 1 || xt->numf == 0)\n\t\treturn NULL;\n\n\td = kzalloc(sizeof(*d) + sizeof(d->sg[0]), GFP_ATOMIC);\n\tif (!d)\n\t\treturn NULL;\n\n\tdata_type = __ffs((xt->src_start | xt->dst_start | xt->sgl[0].size));\n\tif (data_type > CSDP_DATA_TYPE_32)\n\t\tdata_type = CSDP_DATA_TYPE_32;\n\n\tsg = &d->sg[0];\n\td->dir = DMA_MEM_TO_MEM;\n\td->dev_addr = xt->src_start;\n\td->es = data_type;\n\tsg->en = xt->sgl[0].size / BIT(data_type);\n\tsg->fn = xt->numf;\n\tsg->addr = xt->dst_start;\n\td->sglen = 1;\n\td->ccr = c->ccr;\n\n\tsrc_icg = dmaengine_get_src_icg(xt, &xt->sgl[0]);\n\tdst_icg = dmaengine_get_dst_icg(xt, &xt->sgl[0]);\n\tif (src_icg) {\n\t\td->ccr |= CCR_SRC_AMODE_DBLIDX;\n\t\td->ei = 1;\n\t\td->fi = src_icg + 1;\n\t} else if (xt->src_inc) {\n\t\td->ccr |= CCR_SRC_AMODE_POSTINC;\n\t\td->fi = 0;\n\t} else {\n\t\tdev_err(chan->device->dev,\n\t\t\t\"%s: SRC constant addressing is not supported\\n\",\n\t\t\t__func__);\n\t\tkfree(d);\n\t\treturn NULL;\n\t}\n\n\tif (dst_icg) {\n\t\td->ccr |= CCR_DST_AMODE_DBLIDX;\n\t\tsg->ei = 1;\n\t\tsg->fi = dst_icg + 1;\n\t} else if (xt->dst_inc) {\n\t\td->ccr |= CCR_DST_AMODE_POSTINC;\n\t\tsg->fi = 0;\n\t} else {\n\t\tdev_err(chan->device->dev,\n\t\t\t\"%s: DST constant addressing is not supported\\n\",\n\t\t\t__func__);\n\t\tkfree(d);\n\t\treturn NULL;\n\t}\n\n\td->cicr = CICR_DROP_IE | CICR_FRAME_IE;\n\n\td->csdp = data_type;\n\n\tif (dma_omap1()) {\n\t\td->cicr |= CICR_TOUT_IE;\n\t\td->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_EMIFF;\n\t} else {\n\t\td->csdp |= CSDP_DST_PACKED | CSDP_SRC_PACKED;\n\t\td->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\n\t\td->csdp |= CSDP_DST_BURST_64 | CSDP_SRC_BURST_64;\n\t}\n\n\treturn vchan_tx_prep(&c->vc, &d->vd, flags);\n}\n\nstatic int omap_dma_slave_config(struct dma_chan *chan, struct dma_slave_config *cfg)\n{\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\n\tif (cfg->src_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES ||\n\t    cfg->dst_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES)\n\t\treturn -EINVAL;\n\n\tif (cfg->src_maxburst > chan->device->max_burst ||\n\t    cfg->dst_maxburst > chan->device->max_burst)\n\t\treturn -EINVAL;\n\n\tmemcpy(&c->cfg, cfg, sizeof(c->cfg));\n\n\treturn 0;\n}\n\nstatic int omap_dma_terminate_all(struct dma_chan *chan)\n{\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&c->vc.lock, flags);\n\n\t \n\tif (c->desc) {\n\t\tvchan_terminate_vdesc(&c->desc->vd);\n\t\tc->desc = NULL;\n\t\t \n\t\tif (!c->paused)\n\t\t\tomap_dma_stop(c);\n\t}\n\n\tc->cyclic = false;\n\tc->paused = false;\n\n\tvchan_get_all_descriptors(&c->vc, &head);\n\tspin_unlock_irqrestore(&c->vc.lock, flags);\n\tvchan_dma_desc_free_list(&c->vc, &head);\n\n\treturn 0;\n}\n\nstatic void omap_dma_synchronize(struct dma_chan *chan)\n{\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\n\tvchan_synchronize(&c->vc);\n}\n\nstatic int omap_dma_pause(struct dma_chan *chan)\n{\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\n\tunsigned long flags;\n\tint ret = -EINVAL;\n\tbool can_pause = false;\n\n\tspin_lock_irqsave(&od->irq_lock, flags);\n\n\tif (!c->desc)\n\t\tgoto out;\n\n\tif (c->cyclic)\n\t\tcan_pause = true;\n\n\t \n\telse if (c->desc->dir == DMA_DEV_TO_MEM)\n\t\tcan_pause = true;\n\n\tif (can_pause && !c->paused) {\n\t\tret = omap_dma_stop(c);\n\t\tif (!ret)\n\t\t\tc->paused = true;\n\t}\nout:\n\tspin_unlock_irqrestore(&od->irq_lock, flags);\n\n\treturn ret;\n}\n\nstatic int omap_dma_resume(struct dma_chan *chan)\n{\n\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\tstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\n\tunsigned long flags;\n\tint ret = -EINVAL;\n\n\tspin_lock_irqsave(&od->irq_lock, flags);\n\n\tif (c->paused && c->desc) {\n\t\tmb();\n\n\t\t \n\t\tomap_dma_chan_write(c, CLNK_CTRL, c->desc->clnk_ctrl);\n\n\t\tomap_dma_start(c, c->desc);\n\t\tc->paused = false;\n\t\tret = 0;\n\t}\n\tspin_unlock_irqrestore(&od->irq_lock, flags);\n\n\treturn ret;\n}\n\nstatic int omap_dma_chan_init(struct omap_dmadev *od)\n{\n\tstruct omap_chan *c;\n\n\tc = kzalloc(sizeof(*c), GFP_KERNEL);\n\tif (!c)\n\t\treturn -ENOMEM;\n\n\tc->reg_map = od->reg_map;\n\tc->vc.desc_free = omap_dma_desc_free;\n\tvchan_init(&c->vc, &od->ddev);\n\n\treturn 0;\n}\n\nstatic void omap_dma_free(struct omap_dmadev *od)\n{\n\twhile (!list_empty(&od->ddev.channels)) {\n\t\tstruct omap_chan *c = list_first_entry(&od->ddev.channels,\n\t\t\tstruct omap_chan, vc.chan.device_node);\n\n\t\tlist_del(&c->vc.chan.device_node);\n\t\ttasklet_kill(&c->vc.task);\n\t\tkfree(c);\n\t}\n}\n\n \nstatic bool omap_dma_busy(struct omap_dmadev *od)\n{\n\tstruct omap_chan *c;\n\tint lch = -1;\n\n\twhile (1) {\n\t\tlch = find_next_bit(od->lch_bitmap, od->lch_count, lch + 1);\n\t\tif (lch >= od->lch_count)\n\t\t\tbreak;\n\t\tc = od->lch_map[lch];\n\t\tif (!c)\n\t\t\tcontinue;\n\t\tif (omap_dma_chan_read(c, CCR) & CCR_ENABLE)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int omap_dma_busy_notifier(struct notifier_block *nb,\n\t\t\t\t  unsigned long cmd, void *v)\n{\n\tstruct omap_dmadev *od;\n\n\tod = container_of(nb, struct omap_dmadev, nb);\n\n\tswitch (cmd) {\n\tcase CPU_CLUSTER_PM_ENTER:\n\t\tif (omap_dma_busy(od))\n\t\t\treturn NOTIFY_BAD;\n\t\tbreak;\n\tcase CPU_CLUSTER_PM_ENTER_FAILED:\n\tcase CPU_CLUSTER_PM_EXIT:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\n \nstatic void omap_dma_context_save(struct omap_dmadev *od)\n{\n\tod->context.irqenable_l0 = omap_dma_glbl_read(od, IRQENABLE_L0);\n\tod->context.irqenable_l1 = omap_dma_glbl_read(od, IRQENABLE_L1);\n\tod->context.ocp_sysconfig = omap_dma_glbl_read(od, OCP_SYSCONFIG);\n\tod->context.gcr = omap_dma_glbl_read(od, GCR);\n}\n\nstatic void omap_dma_context_restore(struct omap_dmadev *od)\n{\n\tint i;\n\n\tomap_dma_glbl_write(od, GCR, od->context.gcr);\n\tomap_dma_glbl_write(od, OCP_SYSCONFIG, od->context.ocp_sysconfig);\n\tomap_dma_glbl_write(od, IRQENABLE_L0, od->context.irqenable_l0);\n\tomap_dma_glbl_write(od, IRQENABLE_L1, od->context.irqenable_l1);\n\n\t \n\tif (od->plat->errata & DMA_ROMCODE_BUG)\n\t\tomap_dma_glbl_write(od, IRQSTATUS_L0, 0);\n\n\t \n\tfor (i = 0; i < od->lch_count; i++)\n\t\tomap_dma_clear_lch(od, i);\n}\n\n \nstatic int omap_dma_context_notifier(struct notifier_block *nb,\n\t\t\t\t     unsigned long cmd, void *v)\n{\n\tstruct omap_dmadev *od;\n\n\tod = container_of(nb, struct omap_dmadev, nb);\n\n\tswitch (cmd) {\n\tcase CPU_CLUSTER_PM_ENTER:\n\t\tif (omap_dma_busy(od))\n\t\t\treturn NOTIFY_BAD;\n\t\tomap_dma_context_save(od);\n\t\tbreak;\n\tcase CPU_CLUSTER_PM_ENTER_FAILED:\t \n\t\tbreak;\n\tcase CPU_CLUSTER_PM_EXIT:\n\t\tomap_dma_context_restore(od);\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic void omap_dma_init_gcr(struct omap_dmadev *od, int arb_rate,\n\t\t\t      int max_fifo_depth, int tparams)\n{\n\tu32 val;\n\n\t \n\tif (!od->cfg->rw_priority)\n\t\treturn;\n\n\tif (max_fifo_depth == 0)\n\t\tmax_fifo_depth = 1;\n\tif (arb_rate == 0)\n\t\tarb_rate = 1;\n\n\tval = 0xff & max_fifo_depth;\n\tval |= (0x3 & tparams) << 12;\n\tval |= (arb_rate & 0xff) << 16;\n\n\tomap_dma_glbl_write(od, GCR, val);\n}\n\n#define OMAP_DMA_BUSWIDTHS\t(BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) | \\\n\t\t\t\t BIT(DMA_SLAVE_BUSWIDTH_2_BYTES) | \\\n\t\t\t\t BIT(DMA_SLAVE_BUSWIDTH_4_BYTES))\n\n \nstatic const struct omap_dma_config default_cfg;\n\nstatic int omap_dma_probe(struct platform_device *pdev)\n{\n\tconst struct omap_dma_config *conf;\n\tstruct omap_dmadev *od;\n\tint rc, i, irq;\n\tu32 val;\n\n\tod = devm_kzalloc(&pdev->dev, sizeof(*od), GFP_KERNEL);\n\tif (!od)\n\t\treturn -ENOMEM;\n\n\tod->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(od->base))\n\t\treturn PTR_ERR(od->base);\n\n\tconf = of_device_get_match_data(&pdev->dev);\n\tif (conf) {\n\t\tod->cfg = conf;\n\t\tod->plat = dev_get_platdata(&pdev->dev);\n\t\tif (!od->plat) {\n\t\t\tdev_err(&pdev->dev, \"omap_system_dma_plat_info is missing\");\n\t\t\treturn -ENODEV;\n\t\t}\n\t} else if (IS_ENABLED(CONFIG_ARCH_OMAP1)) {\n\t\tod->cfg = &default_cfg;\n\n\t\tod->plat = omap_get_plat_info();\n\t\tif (!od->plat)\n\t\t\treturn -EPROBE_DEFER;\n\t} else {\n\t\treturn -ENODEV;\n\t}\n\n\tod->reg_map = od->plat->reg_map;\n\n\tdma_cap_set(DMA_SLAVE, od->ddev.cap_mask);\n\tdma_cap_set(DMA_CYCLIC, od->ddev.cap_mask);\n\tdma_cap_set(DMA_MEMCPY, od->ddev.cap_mask);\n\tdma_cap_set(DMA_INTERLEAVE, od->ddev.cap_mask);\n\tod->ddev.device_alloc_chan_resources = omap_dma_alloc_chan_resources;\n\tod->ddev.device_free_chan_resources = omap_dma_free_chan_resources;\n\tod->ddev.device_tx_status = omap_dma_tx_status;\n\tod->ddev.device_issue_pending = omap_dma_issue_pending;\n\tod->ddev.device_prep_slave_sg = omap_dma_prep_slave_sg;\n\tod->ddev.device_prep_dma_cyclic = omap_dma_prep_dma_cyclic;\n\tod->ddev.device_prep_dma_memcpy = omap_dma_prep_dma_memcpy;\n\tod->ddev.device_prep_interleaved_dma = omap_dma_prep_dma_interleaved;\n\tod->ddev.device_config = omap_dma_slave_config;\n\tod->ddev.device_pause = omap_dma_pause;\n\tod->ddev.device_resume = omap_dma_resume;\n\tod->ddev.device_terminate_all = omap_dma_terminate_all;\n\tod->ddev.device_synchronize = omap_dma_synchronize;\n\tod->ddev.src_addr_widths = OMAP_DMA_BUSWIDTHS;\n\tod->ddev.dst_addr_widths = OMAP_DMA_BUSWIDTHS;\n\tod->ddev.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\n\tif (__dma_omap15xx(od->plat->dma_attr))\n\t\tod->ddev.residue_granularity =\n\t\t\t\tDMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\telse\n\t\tod->ddev.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\n\tod->ddev.max_burst = SZ_16M - 1;  \n\tod->ddev.dev = &pdev->dev;\n\tINIT_LIST_HEAD(&od->ddev.channels);\n\tmutex_init(&od->lch_lock);\n\tspin_lock_init(&od->lock);\n\tspin_lock_init(&od->irq_lock);\n\n\t \n\tod->dma_requests = OMAP_SDMA_REQUESTS;\n\tif (pdev->dev.of_node && of_property_read_u32(pdev->dev.of_node,\n\t\t\t\t\t\t      \"dma-requests\",\n\t\t\t\t\t\t      &od->dma_requests)) {\n\t\tdev_info(&pdev->dev,\n\t\t\t \"Missing dma-requests property, using %u.\\n\",\n\t\t\t OMAP_SDMA_REQUESTS);\n\t}\n\n\t \n\tif (!pdev->dev.of_node) {\n\t\tod->lch_count = od->plat->dma_attr->lch_count;\n\t\tif (unlikely(!od->lch_count))\n\t\t\tod->lch_count = OMAP_SDMA_CHANNELS;\n\t} else if (of_property_read_u32(pdev->dev.of_node, \"dma-channels\",\n\t\t\t\t\t&od->lch_count)) {\n\t\tdev_info(&pdev->dev,\n\t\t\t \"Missing dma-channels property, using %u.\\n\",\n\t\t\t OMAP_SDMA_CHANNELS);\n\t\tod->lch_count = OMAP_SDMA_CHANNELS;\n\t}\n\n\t \n\tif (pdev->dev.of_node && !of_property_read_u32(pdev->dev.of_node,\n\t\t\t\t\t\t       \"dma-channel-mask\",\n\t\t\t\t\t\t       &val)) {\n\t\t \n\t\tval = ~val;\n\t\tbitmap_from_arr32(od->lch_bitmap, &val, od->lch_count);\n\t}\n\tif (od->plat->dma_attr->dev_caps & HS_CHANNELS_RESERVED)\n\t\tbitmap_set(od->lch_bitmap, 0, 2);\n\n\tod->lch_map = devm_kcalloc(&pdev->dev, od->lch_count,\n\t\t\t\t   sizeof(*od->lch_map),\n\t\t\t\t   GFP_KERNEL);\n\tif (!od->lch_map)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < od->dma_requests; i++) {\n\t\trc = omap_dma_chan_init(od);\n\t\tif (rc) {\n\t\t\tomap_dma_free(od);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\tirq = platform_get_irq(pdev, 1);\n\tif (irq <= 0) {\n\t\tdev_info(&pdev->dev, \"failed to get L1 IRQ: %d\\n\", irq);\n\t\tod->legacy = true;\n\t} else {\n\t\t \n\t\tod->irq_enable_mask = 0;\n\t\tomap_dma_glbl_write(od, IRQENABLE_L1, 0);\n\n\t\trc = devm_request_irq(&pdev->dev, irq, omap_dma_irq,\n\t\t\t\t      IRQF_SHARED, \"omap-dma-engine\", od);\n\t\tif (rc) {\n\t\t\tomap_dma_free(od);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\tif (omap_dma_glbl_read(od, CAPS_0) & CAPS_0_SUPPORT_LL123)\n\t\tod->ll123_supported = true;\n\n\tod->ddev.filter.map = od->plat->slave_map;\n\tod->ddev.filter.mapcnt = od->plat->slavecnt;\n\tod->ddev.filter.fn = omap_dma_filter_fn;\n\n\tif (od->ll123_supported) {\n\t\tod->desc_pool = dma_pool_create(dev_name(&pdev->dev),\n\t\t\t\t\t\t&pdev->dev,\n\t\t\t\t\t\tsizeof(struct omap_type2_desc),\n\t\t\t\t\t\t4, 0);\n\t\tif (!od->desc_pool) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"unable to allocate descriptor pool\\n\");\n\t\t\tod->ll123_supported = false;\n\t\t}\n\t}\n\n\trc = dma_async_device_register(&od->ddev);\n\tif (rc) {\n\t\tpr_warn(\"OMAP-DMA: failed to register slave DMA engine device: %d\\n\",\n\t\t\trc);\n\t\tomap_dma_free(od);\n\t\treturn rc;\n\t}\n\n\tplatform_set_drvdata(pdev, od);\n\n\tif (pdev->dev.of_node) {\n\t\tomap_dma_info.dma_cap = od->ddev.cap_mask;\n\n\t\t \n\t\trc = of_dma_controller_register(pdev->dev.of_node,\n\t\t\t\tof_dma_simple_xlate, &omap_dma_info);\n\t\tif (rc) {\n\t\t\tpr_warn(\"OMAP-DMA: failed to register DMA controller\\n\");\n\t\t\tdma_async_device_unregister(&od->ddev);\n\t\t\tomap_dma_free(od);\n\t\t}\n\t}\n\n\tomap_dma_init_gcr(od, DMA_DEFAULT_ARB_RATE, DMA_DEFAULT_FIFO_DEPTH, 0);\n\n\tif (od->cfg->needs_busy_check) {\n\t\tod->nb.notifier_call = omap_dma_busy_notifier;\n\t\tcpu_pm_register_notifier(&od->nb);\n\t} else if (od->cfg->may_lose_context) {\n\t\tod->nb.notifier_call = omap_dma_context_notifier;\n\t\tcpu_pm_register_notifier(&od->nb);\n\t}\n\n\tdev_info(&pdev->dev, \"OMAP DMA engine driver%s\\n\",\n\t\t od->ll123_supported ? \" (LinkedList1/2/3 supported)\" : \"\");\n\n\treturn rc;\n}\n\nstatic int omap_dma_remove(struct platform_device *pdev)\n{\n\tstruct omap_dmadev *od = platform_get_drvdata(pdev);\n\tint irq;\n\n\tif (od->cfg->may_lose_context)\n\t\tcpu_pm_unregister_notifier(&od->nb);\n\n\tif (pdev->dev.of_node)\n\t\tof_dma_controller_free(pdev->dev.of_node);\n\n\tirq = platform_get_irq(pdev, 1);\n\tdevm_free_irq(&pdev->dev, irq, od);\n\n\tdma_async_device_unregister(&od->ddev);\n\n\tif (!omap_dma_legacy(od)) {\n\t\t \n\t\tomap_dma_glbl_write(od, IRQENABLE_L0, 0);\n\t}\n\n\tif (od->ll123_supported)\n\t\tdma_pool_destroy(od->desc_pool);\n\n\tomap_dma_free(od);\n\n\treturn 0;\n}\n\nstatic const struct omap_dma_config omap2420_data = {\n\t.lch_end = CCFN,\n\t.rw_priority = true,\n\t.needs_lch_clear = true,\n\t.needs_busy_check = true,\n};\n\nstatic const struct omap_dma_config omap2430_data = {\n\t.lch_end = CCFN,\n\t.rw_priority = true,\n\t.needs_lch_clear = true,\n};\n\nstatic const struct omap_dma_config omap3430_data = {\n\t.lch_end = CCFN,\n\t.rw_priority = true,\n\t.needs_lch_clear = true,\n\t.may_lose_context = true,\n};\n\nstatic const struct omap_dma_config omap3630_data = {\n\t.lch_end = CCDN,\n\t.rw_priority = true,\n\t.needs_lch_clear = true,\n\t.may_lose_context = true,\n};\n\nstatic const struct omap_dma_config omap4_data = {\n\t.lch_end = CCDN,\n\t.rw_priority = true,\n\t.needs_lch_clear = true,\n};\n\nstatic const struct of_device_id omap_dma_match[] = {\n\t{ .compatible = \"ti,omap2420-sdma\", .data = &omap2420_data, },\n\t{ .compatible = \"ti,omap2430-sdma\", .data = &omap2430_data, },\n\t{ .compatible = \"ti,omap3430-sdma\", .data = &omap3430_data, },\n\t{ .compatible = \"ti,omap3630-sdma\", .data = &omap3630_data, },\n\t{ .compatible = \"ti,omap4430-sdma\", .data = &omap4_data, },\n\t{},\n};\nMODULE_DEVICE_TABLE(of, omap_dma_match);\n\nstatic struct platform_driver omap_dma_driver = {\n\t.probe\t= omap_dma_probe,\n\t.remove\t= omap_dma_remove,\n\t.driver = {\n\t\t.name = \"omap-dma-engine\",\n\t\t.of_match_table = omap_dma_match,\n\t},\n};\n\nstatic bool omap_dma_filter_fn(struct dma_chan *chan, void *param)\n{\n\tif (chan->device->dev->driver == &omap_dma_driver.driver) {\n\t\tstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\n\t\tstruct omap_chan *c = to_omap_dma_chan(chan);\n\t\tunsigned req = *(unsigned *)param;\n\n\t\tif (req <= od->dma_requests) {\n\t\t\tc->dma_sig = req;\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic int omap_dma_init(void)\n{\n\treturn platform_driver_register(&omap_dma_driver);\n}\nsubsys_initcall(omap_dma_init);\n\nstatic void __exit omap_dma_exit(void)\n{\n\tplatform_driver_unregister(&omap_dma_driver);\n}\nmodule_exit(omap_dma_exit);\n\nMODULE_AUTHOR(\"Russell King\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}