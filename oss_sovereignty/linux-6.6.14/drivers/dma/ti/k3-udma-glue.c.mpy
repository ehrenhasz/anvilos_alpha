{
  "module_name": "k3-udma-glue.c",
  "hash_id": "5bed951d3ab264cbbef475f60ed27c80bf6965213fd308f78f4ed21ffd017954",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/ti/k3-udma-glue.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/atomic.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/io.h>\n#include <linux/init.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/soc/ti/k3-ringacc.h>\n#include <linux/dma/ti-cppi5.h>\n#include <linux/dma/k3-udma-glue.h>\n\n#include \"k3-udma.h\"\n#include \"k3-psil-priv.h\"\n\nstruct k3_udma_glue_common {\n\tstruct device *dev;\n\tstruct device chan_dev;\n\tstruct udma_dev *udmax;\n\tconst struct udma_tisci_rm *tisci_rm;\n\tstruct k3_ringacc *ringacc;\n\tu32 src_thread;\n\tu32 dst_thread;\n\n\tu32  hdesc_size;\n\tbool epib;\n\tu32  psdata_size;\n\tu32  swdata_size;\n\tu32  atype_asel;\n\tstruct psil_endpoint_config *ep_config;\n};\n\nstruct k3_udma_glue_tx_channel {\n\tstruct k3_udma_glue_common common;\n\n\tstruct udma_tchan *udma_tchanx;\n\tint udma_tchan_id;\n\n\tstruct k3_ring *ringtx;\n\tstruct k3_ring *ringtxcq;\n\n\tbool psil_paired;\n\n\tint virq;\n\n\tatomic_t free_pkts;\n\tbool tx_pause_on_err;\n\tbool tx_filt_einfo;\n\tbool tx_filt_pswords;\n\tbool tx_supr_tdpkt;\n\n\tint udma_tflow_id;\n};\n\nstruct k3_udma_glue_rx_flow {\n\tstruct udma_rflow *udma_rflow;\n\tint udma_rflow_id;\n\tstruct k3_ring *ringrx;\n\tstruct k3_ring *ringrxfdq;\n\n\tint virq;\n};\n\nstruct k3_udma_glue_rx_channel {\n\tstruct k3_udma_glue_common common;\n\n\tstruct udma_rchan *udma_rchanx;\n\tint udma_rchan_id;\n\tbool remote;\n\n\tbool psil_paired;\n\n\tu32  swdata_size;\n\tint  flow_id_base;\n\n\tstruct k3_udma_glue_rx_flow *flows;\n\tu32 flow_num;\n\tu32 flows_ready;\n};\n\nstatic void k3_udma_chan_dev_release(struct device *dev)\n{\n\t \n}\n\nstatic struct class k3_udma_glue_devclass = {\n\t.name\t\t= \"k3_udma_glue_chan\",\n\t.dev_release\t= k3_udma_chan_dev_release,\n};\n\n#define K3_UDMAX_TDOWN_TIMEOUT_US 1000\n\nstatic int of_k3_udma_glue_parse(struct device_node *udmax_np,\n\t\t\t\t struct k3_udma_glue_common *common)\n{\n\tcommon->udmax = of_xudma_dev_get(udmax_np, NULL);\n\tif (IS_ERR(common->udmax))\n\t\treturn PTR_ERR(common->udmax);\n\n\tcommon->ringacc = xudma_get_ringacc(common->udmax);\n\tcommon->tisci_rm = xudma_dev_get_tisci_rm(common->udmax);\n\n\treturn 0;\n}\n\nstatic int of_k3_udma_glue_parse_chn(struct device_node *chn_np,\n\t\tconst char *name, struct k3_udma_glue_common *common,\n\t\tbool tx_chn)\n{\n\tstruct of_phandle_args dma_spec;\n\tu32 thread_id;\n\tint ret = 0;\n\tint index;\n\n\tif (unlikely(!name))\n\t\treturn -EINVAL;\n\n\tindex = of_property_match_string(chn_np, \"dma-names\", name);\n\tif (index < 0)\n\t\treturn index;\n\n\tif (of_parse_phandle_with_args(chn_np, \"dmas\", \"#dma-cells\", index,\n\t\t\t\t       &dma_spec))\n\t\treturn -ENOENT;\n\n\tret = of_k3_udma_glue_parse(dma_spec.np, common);\n\tif (ret)\n\t\tgoto out_put_spec;\n\n\tthread_id = dma_spec.args[0];\n\tif (dma_spec.args_count == 2) {\n\t\tif (dma_spec.args[1] > 2 && !xudma_is_pktdma(common->udmax)) {\n\t\t\tdev_err(common->dev, \"Invalid channel atype: %u\\n\",\n\t\t\t\tdma_spec.args[1]);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_put_spec;\n\t\t}\n\t\tif (dma_spec.args[1] > 15 && xudma_is_pktdma(common->udmax)) {\n\t\t\tdev_err(common->dev, \"Invalid channel asel: %u\\n\",\n\t\t\t\tdma_spec.args[1]);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_put_spec;\n\t\t}\n\n\t\tcommon->atype_asel = dma_spec.args[1];\n\t}\n\n\tif (tx_chn && !(thread_id & K3_PSIL_DST_THREAD_ID_OFFSET)) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_spec;\n\t}\n\n\tif (!tx_chn && (thread_id & K3_PSIL_DST_THREAD_ID_OFFSET)) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_spec;\n\t}\n\n\t \n\tcommon->ep_config = psil_get_ep_config(thread_id);\n\tif (IS_ERR(common->ep_config)) {\n\t\tdev_err(common->dev,\n\t\t\t\"No configuration for psi-l thread 0x%04x\\n\",\n\t\t\tthread_id);\n\t\tret = PTR_ERR(common->ep_config);\n\t\tgoto out_put_spec;\n\t}\n\n\tcommon->epib = common->ep_config->needs_epib;\n\tcommon->psdata_size = common->ep_config->psd_size;\n\n\tif (tx_chn)\n\t\tcommon->dst_thread = thread_id;\n\telse\n\t\tcommon->src_thread = thread_id;\n\nout_put_spec:\n\tof_node_put(dma_spec.np);\n\treturn ret;\n};\n\nstatic void k3_udma_glue_dump_tx_chn(struct k3_udma_glue_tx_channel *tx_chn)\n{\n\tstruct device *dev = tx_chn->common.dev;\n\n\tdev_dbg(dev, \"dump_tx_chn:\\n\"\n\t\t\"udma_tchan_id: %d\\n\"\n\t\t\"src_thread: %08x\\n\"\n\t\t\"dst_thread: %08x\\n\",\n\t\ttx_chn->udma_tchan_id,\n\t\ttx_chn->common.src_thread,\n\t\ttx_chn->common.dst_thread);\n}\n\nstatic void k3_udma_glue_dump_tx_rt_chn(struct k3_udma_glue_tx_channel *chn,\n\t\t\t\t\tchar *mark)\n{\n\tstruct device *dev = chn->common.dev;\n\n\tdev_dbg(dev, \"=== dump ===> %s\\n\", mark);\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_CTL_REG,\n\t\txudma_tchanrt_read(chn->udma_tchanx, UDMA_CHAN_RT_CTL_REG));\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\txudma_tchanrt_read(chn->udma_tchanx,\n\t\t\t\t   UDMA_CHAN_RT_PEER_RT_EN_REG));\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_PCNT_REG,\n\t\txudma_tchanrt_read(chn->udma_tchanx, UDMA_CHAN_RT_PCNT_REG));\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_BCNT_REG,\n\t\txudma_tchanrt_read(chn->udma_tchanx, UDMA_CHAN_RT_BCNT_REG));\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_SBCNT_REG,\n\t\txudma_tchanrt_read(chn->udma_tchanx, UDMA_CHAN_RT_SBCNT_REG));\n}\n\nstatic int k3_udma_glue_cfg_tx_chn(struct k3_udma_glue_tx_channel *tx_chn)\n{\n\tconst struct udma_tisci_rm *tisci_rm = tx_chn->common.tisci_rm;\n\tstruct ti_sci_msg_rm_udmap_tx_ch_cfg req;\n\n\tmemset(&req, 0, sizeof(req));\n\n\treq.valid_params = TI_SCI_MSG_VALUE_RM_UDMAP_CH_PAUSE_ON_ERR_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_TX_FILT_EINFO_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_TX_FILT_PSWORDS_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_CHAN_TYPE_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_TX_SUPR_TDPKT_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_FETCH_SIZE_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_CQ_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_ATYPE_VALID;\n\treq.nav_id = tisci_rm->tisci_dev_id;\n\treq.index = tx_chn->udma_tchan_id;\n\tif (tx_chn->tx_pause_on_err)\n\t\treq.tx_pause_on_err = 1;\n\tif (tx_chn->tx_filt_einfo)\n\t\treq.tx_filt_einfo = 1;\n\tif (tx_chn->tx_filt_pswords)\n\t\treq.tx_filt_pswords = 1;\n\treq.tx_chan_type = TI_SCI_RM_UDMAP_CHAN_TYPE_PKT_PBRR;\n\tif (tx_chn->tx_supr_tdpkt)\n\t\treq.tx_supr_tdpkt = 1;\n\treq.tx_fetch_size = tx_chn->common.hdesc_size >> 2;\n\treq.txcq_qnum = k3_ringacc_get_ring_id(tx_chn->ringtxcq);\n\treq.tx_atype = tx_chn->common.atype_asel;\n\n\treturn tisci_rm->tisci_udmap_ops->tx_ch_cfg(tisci_rm->tisci, &req);\n}\n\nstruct k3_udma_glue_tx_channel *k3_udma_glue_request_tx_chn(struct device *dev,\n\t\tconst char *name, struct k3_udma_glue_tx_channel_cfg *cfg)\n{\n\tstruct k3_udma_glue_tx_channel *tx_chn;\n\tint ret;\n\n\ttx_chn = devm_kzalloc(dev, sizeof(*tx_chn), GFP_KERNEL);\n\tif (!tx_chn)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\ttx_chn->common.dev = dev;\n\ttx_chn->common.swdata_size = cfg->swdata_size;\n\ttx_chn->tx_pause_on_err = cfg->tx_pause_on_err;\n\ttx_chn->tx_filt_einfo = cfg->tx_filt_einfo;\n\ttx_chn->tx_filt_pswords = cfg->tx_filt_pswords;\n\ttx_chn->tx_supr_tdpkt = cfg->tx_supr_tdpkt;\n\n\t \n\tret = of_k3_udma_glue_parse_chn(dev->of_node, name,\n\t\t\t\t\t&tx_chn->common, true);\n\tif (ret)\n\t\tgoto err;\n\n\ttx_chn->common.hdesc_size = cppi5_hdesc_calc_size(tx_chn->common.epib,\n\t\t\t\t\t\ttx_chn->common.psdata_size,\n\t\t\t\t\t\ttx_chn->common.swdata_size);\n\n\tif (xudma_is_pktdma(tx_chn->common.udmax))\n\t\ttx_chn->udma_tchan_id = tx_chn->common.ep_config->mapped_channel_id;\n\telse\n\t\ttx_chn->udma_tchan_id = -1;\n\n\t \n\ttx_chn->udma_tchanx = xudma_tchan_get(tx_chn->common.udmax,\n\t\t\t\t\t      tx_chn->udma_tchan_id);\n\tif (IS_ERR(tx_chn->udma_tchanx)) {\n\t\tret = PTR_ERR(tx_chn->udma_tchanx);\n\t\tdev_err(dev, \"UDMAX tchanx get err %d\\n\", ret);\n\t\tgoto err;\n\t}\n\ttx_chn->udma_tchan_id = xudma_tchan_get_id(tx_chn->udma_tchanx);\n\n\ttx_chn->common.chan_dev.class = &k3_udma_glue_devclass;\n\ttx_chn->common.chan_dev.parent = xudma_get_device(tx_chn->common.udmax);\n\tdev_set_name(&tx_chn->common.chan_dev, \"tchan%d-0x%04x\",\n\t\t     tx_chn->udma_tchan_id, tx_chn->common.dst_thread);\n\tret = device_register(&tx_chn->common.chan_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"Channel Device registration failed %d\\n\", ret);\n\t\tput_device(&tx_chn->common.chan_dev);\n\t\ttx_chn->common.chan_dev.parent = NULL;\n\t\tgoto err;\n\t}\n\n\tif (xudma_is_pktdma(tx_chn->common.udmax)) {\n\t\t \n\t\ttx_chn->common.chan_dev.dma_coherent = true;\n\t\tdma_coerce_mask_and_coherent(&tx_chn->common.chan_dev,\n\t\t\t\t\t     DMA_BIT_MASK(48));\n\t}\n\n\tatomic_set(&tx_chn->free_pkts, cfg->txcq_cfg.size);\n\n\tif (xudma_is_pktdma(tx_chn->common.udmax))\n\t\ttx_chn->udma_tflow_id = tx_chn->common.ep_config->default_flow_id;\n\telse\n\t\ttx_chn->udma_tflow_id = tx_chn->udma_tchan_id;\n\n\t \n\tret =  k3_ringacc_request_rings_pair(tx_chn->common.ringacc,\n\t\t\t\t\t     tx_chn->udma_tflow_id, -1,\n\t\t\t\t\t     &tx_chn->ringtx,\n\t\t\t\t\t     &tx_chn->ringtxcq);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to get TX/TXCQ rings %d\\n\", ret);\n\t\tgoto err;\n\t}\n\n\t \n\tcfg->tx_cfg.dma_dev = k3_udma_glue_tx_get_dma_device(tx_chn);\n\tcfg->txcq_cfg.dma_dev = cfg->tx_cfg.dma_dev;\n\n\t \n\tif (xudma_is_pktdma(tx_chn->common.udmax)) {\n\t\tcfg->tx_cfg.asel = tx_chn->common.atype_asel;\n\t\tcfg->txcq_cfg.asel = tx_chn->common.atype_asel;\n\t}\n\n\tret = k3_ringacc_ring_cfg(tx_chn->ringtx, &cfg->tx_cfg);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to cfg ringtx %d\\n\", ret);\n\t\tgoto err;\n\t}\n\n\tret = k3_ringacc_ring_cfg(tx_chn->ringtxcq, &cfg->txcq_cfg);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to cfg ringtx %d\\n\", ret);\n\t\tgoto err;\n\t}\n\n\t \n\ttx_chn->common.src_thread =\n\t\t\txudma_dev_get_psil_base(tx_chn->common.udmax) +\n\t\t\ttx_chn->udma_tchan_id;\n\n\tret = k3_udma_glue_cfg_tx_chn(tx_chn);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to cfg tchan %d\\n\", ret);\n\t\tgoto err;\n\t}\n\n\tk3_udma_glue_dump_tx_chn(tx_chn);\n\n\treturn tx_chn;\n\nerr:\n\tk3_udma_glue_release_tx_chn(tx_chn);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_request_tx_chn);\n\nvoid k3_udma_glue_release_tx_chn(struct k3_udma_glue_tx_channel *tx_chn)\n{\n\tif (tx_chn->psil_paired) {\n\t\txudma_navss_psil_unpair(tx_chn->common.udmax,\n\t\t\t\t\ttx_chn->common.src_thread,\n\t\t\t\t\ttx_chn->common.dst_thread);\n\t\ttx_chn->psil_paired = false;\n\t}\n\n\tif (!IS_ERR_OR_NULL(tx_chn->udma_tchanx))\n\t\txudma_tchan_put(tx_chn->common.udmax,\n\t\t\t\ttx_chn->udma_tchanx);\n\n\tif (tx_chn->ringtxcq)\n\t\tk3_ringacc_ring_free(tx_chn->ringtxcq);\n\n\tif (tx_chn->ringtx)\n\t\tk3_ringacc_ring_free(tx_chn->ringtx);\n\n\tif (tx_chn->common.chan_dev.parent) {\n\t\tdevice_unregister(&tx_chn->common.chan_dev);\n\t\ttx_chn->common.chan_dev.parent = NULL;\n\t}\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_release_tx_chn);\n\nint k3_udma_glue_push_tx_chn(struct k3_udma_glue_tx_channel *tx_chn,\n\t\t\t     struct cppi5_host_desc_t *desc_tx,\n\t\t\t     dma_addr_t desc_dma)\n{\n\tu32 ringtxcq_id;\n\n\tif (!atomic_add_unless(&tx_chn->free_pkts, -1, 0))\n\t\treturn -ENOMEM;\n\n\tringtxcq_id = k3_ringacc_get_ring_id(tx_chn->ringtxcq);\n\tcppi5_desc_set_retpolicy(&desc_tx->hdr, 0, ringtxcq_id);\n\n\treturn k3_ringacc_ring_push(tx_chn->ringtx, &desc_dma);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_push_tx_chn);\n\nint k3_udma_glue_pop_tx_chn(struct k3_udma_glue_tx_channel *tx_chn,\n\t\t\t    dma_addr_t *desc_dma)\n{\n\tint ret;\n\n\tret = k3_ringacc_ring_pop(tx_chn->ringtxcq, desc_dma);\n\tif (!ret)\n\t\tatomic_inc(&tx_chn->free_pkts);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_pop_tx_chn);\n\nint k3_udma_glue_enable_tx_chn(struct k3_udma_glue_tx_channel *tx_chn)\n{\n\tint ret;\n\n\tret = xudma_navss_psil_pair(tx_chn->common.udmax,\n\t\t\t\t    tx_chn->common.src_thread,\n\t\t\t\t    tx_chn->common.dst_thread);\n\tif (ret) {\n\t\tdev_err(tx_chn->common.dev, \"PSI-L request err %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\ttx_chn->psil_paired = true;\n\n\txudma_tchanrt_write(tx_chn->udma_tchanx, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t    UDMA_PEER_RT_EN_ENABLE);\n\n\txudma_tchanrt_write(tx_chn->udma_tchanx, UDMA_CHAN_RT_CTL_REG,\n\t\t\t    UDMA_CHAN_RT_CTL_EN);\n\n\tk3_udma_glue_dump_tx_rt_chn(tx_chn, \"txchn en\");\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_enable_tx_chn);\n\nvoid k3_udma_glue_disable_tx_chn(struct k3_udma_glue_tx_channel *tx_chn)\n{\n\tk3_udma_glue_dump_tx_rt_chn(tx_chn, \"txchn dis1\");\n\n\txudma_tchanrt_write(tx_chn->udma_tchanx, UDMA_CHAN_RT_CTL_REG, 0);\n\n\txudma_tchanrt_write(tx_chn->udma_tchanx,\n\t\t\t    UDMA_CHAN_RT_PEER_RT_EN_REG, 0);\n\tk3_udma_glue_dump_tx_rt_chn(tx_chn, \"txchn dis2\");\n\n\tif (tx_chn->psil_paired) {\n\t\txudma_navss_psil_unpair(tx_chn->common.udmax,\n\t\t\t\t\ttx_chn->common.src_thread,\n\t\t\t\t\ttx_chn->common.dst_thread);\n\t\ttx_chn->psil_paired = false;\n\t}\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_disable_tx_chn);\n\nvoid k3_udma_glue_tdown_tx_chn(struct k3_udma_glue_tx_channel *tx_chn,\n\t\t\t       bool sync)\n{\n\tint i = 0;\n\tu32 val;\n\n\tk3_udma_glue_dump_tx_rt_chn(tx_chn, \"txchn tdown1\");\n\n\txudma_tchanrt_write(tx_chn->udma_tchanx, UDMA_CHAN_RT_CTL_REG,\n\t\t\t    UDMA_CHAN_RT_CTL_EN | UDMA_CHAN_RT_CTL_TDOWN);\n\n\tval = xudma_tchanrt_read(tx_chn->udma_tchanx, UDMA_CHAN_RT_CTL_REG);\n\n\twhile (sync && (val & UDMA_CHAN_RT_CTL_EN)) {\n\t\tval = xudma_tchanrt_read(tx_chn->udma_tchanx,\n\t\t\t\t\t UDMA_CHAN_RT_CTL_REG);\n\t\tudelay(1);\n\t\tif (i > K3_UDMAX_TDOWN_TIMEOUT_US) {\n\t\t\tdev_err(tx_chn->common.dev, \"TX tdown timeout\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\n\tval = xudma_tchanrt_read(tx_chn->udma_tchanx,\n\t\t\t\t UDMA_CHAN_RT_PEER_RT_EN_REG);\n\tif (sync && (val & UDMA_PEER_RT_EN_ENABLE))\n\t\tdev_err(tx_chn->common.dev, \"TX tdown peer not stopped\\n\");\n\tk3_udma_glue_dump_tx_rt_chn(tx_chn, \"txchn tdown2\");\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_tdown_tx_chn);\n\nvoid k3_udma_glue_reset_tx_chn(struct k3_udma_glue_tx_channel *tx_chn,\n\t\t\t       void *data,\n\t\t\t       void (*cleanup)(void *data, dma_addr_t desc_dma))\n{\n\tstruct device *dev = tx_chn->common.dev;\n\tdma_addr_t desc_dma;\n\tint occ_tx, i, ret;\n\n\t \n\tocc_tx = k3_ringacc_ring_get_occ(tx_chn->ringtx);\n\tdev_dbg(dev, \"TX reset occ_tx %u\\n\", occ_tx);\n\n\tfor (i = 0; i < occ_tx; i++) {\n\t\tret = k3_ringacc_ring_pop(tx_chn->ringtx, &desc_dma);\n\t\tif (ret) {\n\t\t\tif (ret != -ENODATA)\n\t\t\t\tdev_err(dev, \"TX reset pop %d\\n\", ret);\n\t\t\tbreak;\n\t\t}\n\t\tcleanup(data, desc_dma);\n\t}\n\n\t \n\tk3_ringacc_ring_reset(tx_chn->ringtxcq);\n\tk3_ringacc_ring_reset_dma(tx_chn->ringtx, occ_tx);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_reset_tx_chn);\n\nu32 k3_udma_glue_tx_get_hdesc_size(struct k3_udma_glue_tx_channel *tx_chn)\n{\n\treturn tx_chn->common.hdesc_size;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_tx_get_hdesc_size);\n\nu32 k3_udma_glue_tx_get_txcq_id(struct k3_udma_glue_tx_channel *tx_chn)\n{\n\treturn k3_ringacc_get_ring_id(tx_chn->ringtxcq);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_tx_get_txcq_id);\n\nint k3_udma_glue_tx_get_irq(struct k3_udma_glue_tx_channel *tx_chn)\n{\n\tif (xudma_is_pktdma(tx_chn->common.udmax)) {\n\t\ttx_chn->virq = xudma_pktdma_tflow_get_irq(tx_chn->common.udmax,\n\t\t\t\t\t\t\t  tx_chn->udma_tflow_id);\n\t} else {\n\t\ttx_chn->virq = k3_ringacc_get_ring_irq_num(tx_chn->ringtxcq);\n\t}\n\n\tif (!tx_chn->virq)\n\t\treturn -ENXIO;\n\n\treturn tx_chn->virq;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_tx_get_irq);\n\nstruct device *\n\tk3_udma_glue_tx_get_dma_device(struct k3_udma_glue_tx_channel *tx_chn)\n{\n\tif (xudma_is_pktdma(tx_chn->common.udmax) &&\n\t    (tx_chn->common.atype_asel == 14 || tx_chn->common.atype_asel == 15))\n\t\treturn &tx_chn->common.chan_dev;\n\n\treturn xudma_get_device(tx_chn->common.udmax);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_tx_get_dma_device);\n\nvoid k3_udma_glue_tx_dma_to_cppi5_addr(struct k3_udma_glue_tx_channel *tx_chn,\n\t\t\t\t       dma_addr_t *addr)\n{\n\tif (!xudma_is_pktdma(tx_chn->common.udmax) ||\n\t    !tx_chn->common.atype_asel)\n\t\treturn;\n\n\t*addr |= (u64)tx_chn->common.atype_asel << K3_ADDRESS_ASEL_SHIFT;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_tx_dma_to_cppi5_addr);\n\nvoid k3_udma_glue_tx_cppi5_to_dma_addr(struct k3_udma_glue_tx_channel *tx_chn,\n\t\t\t\t       dma_addr_t *addr)\n{\n\tif (!xudma_is_pktdma(tx_chn->common.udmax) ||\n\t    !tx_chn->common.atype_asel)\n\t\treturn;\n\n\t*addr &= (u64)GENMASK(K3_ADDRESS_ASEL_SHIFT - 1, 0);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_tx_cppi5_to_dma_addr);\n\nstatic int k3_udma_glue_cfg_rx_chn(struct k3_udma_glue_rx_channel *rx_chn)\n{\n\tconst struct udma_tisci_rm *tisci_rm = rx_chn->common.tisci_rm;\n\tstruct ti_sci_msg_rm_udmap_rx_ch_cfg req;\n\tint ret;\n\n\tmemset(&req, 0, sizeof(req));\n\n\treq.valid_params = TI_SCI_MSG_VALUE_RM_UDMAP_CH_FETCH_SIZE_VALID |\n\t\t\t   TI_SCI_MSG_VALUE_RM_UDMAP_CH_CQ_QNUM_VALID |\n\t\t\t   TI_SCI_MSG_VALUE_RM_UDMAP_CH_CHAN_TYPE_VALID |\n\t\t\t   TI_SCI_MSG_VALUE_RM_UDMAP_CH_ATYPE_VALID;\n\n\treq.nav_id = tisci_rm->tisci_dev_id;\n\treq.index = rx_chn->udma_rchan_id;\n\treq.rx_fetch_size = rx_chn->common.hdesc_size >> 2;\n\t \n\treq.rxcq_qnum = 0xFFFF;\n\tif (!xudma_is_pktdma(rx_chn->common.udmax) && rx_chn->flow_num &&\n\t    rx_chn->flow_id_base != rx_chn->udma_rchan_id) {\n\t\t \n\t\treq.valid_params |= TI_SCI_MSG_VALUE_RM_UDMAP_CH_RX_FLOWID_START_VALID |\n\t\t\t\t    TI_SCI_MSG_VALUE_RM_UDMAP_CH_RX_FLOWID_CNT_VALID;\n\t\treq.flowid_start = rx_chn->flow_id_base;\n\t\treq.flowid_cnt = rx_chn->flow_num;\n\t}\n\treq.rx_chan_type = TI_SCI_RM_UDMAP_CHAN_TYPE_PKT_PBRR;\n\treq.rx_atype = rx_chn->common.atype_asel;\n\n\tret = tisci_rm->tisci_udmap_ops->rx_ch_cfg(tisci_rm->tisci, &req);\n\tif (ret)\n\t\tdev_err(rx_chn->common.dev, \"rchan%d cfg failed %d\\n\",\n\t\t\trx_chn->udma_rchan_id, ret);\n\n\treturn ret;\n}\n\nstatic void k3_udma_glue_release_rx_flow(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t\t\t u32 flow_num)\n{\n\tstruct k3_udma_glue_rx_flow *flow = &rx_chn->flows[flow_num];\n\n\tif (IS_ERR_OR_NULL(flow->udma_rflow))\n\t\treturn;\n\n\tif (flow->ringrxfdq)\n\t\tk3_ringacc_ring_free(flow->ringrxfdq);\n\n\tif (flow->ringrx)\n\t\tk3_ringacc_ring_free(flow->ringrx);\n\n\txudma_rflow_put(rx_chn->common.udmax, flow->udma_rflow);\n\tflow->udma_rflow = NULL;\n\trx_chn->flows_ready--;\n}\n\nstatic int k3_udma_glue_cfg_rx_flow(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t\t    u32 flow_idx,\n\t\t\t\t    struct k3_udma_glue_rx_flow_cfg *flow_cfg)\n{\n\tstruct k3_udma_glue_rx_flow *flow = &rx_chn->flows[flow_idx];\n\tconst struct udma_tisci_rm *tisci_rm = rx_chn->common.tisci_rm;\n\tstruct device *dev = rx_chn->common.dev;\n\tstruct ti_sci_msg_rm_udmap_flow_cfg req;\n\tint rx_ring_id;\n\tint rx_ringfdq_id;\n\tint ret = 0;\n\n\tflow->udma_rflow = xudma_rflow_get(rx_chn->common.udmax,\n\t\t\t\t\t   flow->udma_rflow_id);\n\tif (IS_ERR(flow->udma_rflow)) {\n\t\tret = PTR_ERR(flow->udma_rflow);\n\t\tdev_err(dev, \"UDMAX rflow get err %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tif (flow->udma_rflow_id != xudma_rflow_get_id(flow->udma_rflow)) {\n\t\tret = -ENODEV;\n\t\tgoto err_rflow_put;\n\t}\n\n\tif (xudma_is_pktdma(rx_chn->common.udmax)) {\n\t\trx_ringfdq_id = flow->udma_rflow_id +\n\t\t\t\txudma_get_rflow_ring_offset(rx_chn->common.udmax);\n\t\trx_ring_id = 0;\n\t} else {\n\t\trx_ring_id = flow_cfg->ring_rxq_id;\n\t\trx_ringfdq_id = flow_cfg->ring_rxfdq0_id;\n\t}\n\n\t \n\tret =  k3_ringacc_request_rings_pair(rx_chn->common.ringacc,\n\t\t\t\t\t     rx_ringfdq_id, rx_ring_id,\n\t\t\t\t\t     &flow->ringrxfdq,\n\t\t\t\t\t     &flow->ringrx);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to get RX/RXFDQ rings %d\\n\", ret);\n\t\tgoto err_rflow_put;\n\t}\n\n\t \n\tflow_cfg->rx_cfg.dma_dev = k3_udma_glue_rx_get_dma_device(rx_chn);\n\tflow_cfg->rxfdq_cfg.dma_dev = flow_cfg->rx_cfg.dma_dev;\n\n\t \n\tif (xudma_is_pktdma(rx_chn->common.udmax)) {\n\t\tflow_cfg->rx_cfg.asel = rx_chn->common.atype_asel;\n\t\tflow_cfg->rxfdq_cfg.asel = rx_chn->common.atype_asel;\n\t}\n\n\tret = k3_ringacc_ring_cfg(flow->ringrx, &flow_cfg->rx_cfg);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to cfg ringrx %d\\n\", ret);\n\t\tgoto err_ringrxfdq_free;\n\t}\n\n\tret = k3_ringacc_ring_cfg(flow->ringrxfdq, &flow_cfg->rxfdq_cfg);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to cfg ringrxfdq %d\\n\", ret);\n\t\tgoto err_ringrxfdq_free;\n\t}\n\n\tif (rx_chn->remote) {\n\t\trx_ring_id = TI_SCI_RESOURCE_NULL;\n\t\trx_ringfdq_id = TI_SCI_RESOURCE_NULL;\n\t} else {\n\t\trx_ring_id = k3_ringacc_get_ring_id(flow->ringrx);\n\t\trx_ringfdq_id = k3_ringacc_get_ring_id(flow->ringrxfdq);\n\t}\n\n\tmemset(&req, 0, sizeof(req));\n\n\treq.valid_params =\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_EINFO_PRESENT_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_PSINFO_PRESENT_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_ERROR_HANDLING_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DESC_TYPE_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DEST_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_SRC_TAG_HI_SEL_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_SRC_TAG_LO_SEL_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DEST_TAG_HI_SEL_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DEST_TAG_LO_SEL_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ0_SZ0_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ1_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ2_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ3_QNUM_VALID;\n\treq.nav_id = tisci_rm->tisci_dev_id;\n\treq.flow_index = flow->udma_rflow_id;\n\tif (rx_chn->common.epib)\n\t\treq.rx_einfo_present = 1;\n\tif (rx_chn->common.psdata_size)\n\t\treq.rx_psinfo_present = 1;\n\tif (flow_cfg->rx_error_handling)\n\t\treq.rx_error_handling = 1;\n\treq.rx_desc_type = 0;\n\treq.rx_dest_qnum = rx_ring_id;\n\treq.rx_src_tag_hi_sel = 0;\n\treq.rx_src_tag_lo_sel = flow_cfg->src_tag_lo_sel;\n\treq.rx_dest_tag_hi_sel = 0;\n\treq.rx_dest_tag_lo_sel = 0;\n\treq.rx_fdq0_sz0_qnum = rx_ringfdq_id;\n\treq.rx_fdq1_qnum = rx_ringfdq_id;\n\treq.rx_fdq2_qnum = rx_ringfdq_id;\n\treq.rx_fdq3_qnum = rx_ringfdq_id;\n\n\tret = tisci_rm->tisci_udmap_ops->rx_flow_cfg(tisci_rm->tisci, &req);\n\tif (ret) {\n\t\tdev_err(dev, \"flow%d config failed: %d\\n\", flow->udma_rflow_id,\n\t\t\tret);\n\t\tgoto err_ringrxfdq_free;\n\t}\n\n\trx_chn->flows_ready++;\n\tdev_dbg(dev, \"flow%d config done. ready:%d\\n\",\n\t\tflow->udma_rflow_id, rx_chn->flows_ready);\n\n\treturn 0;\n\nerr_ringrxfdq_free:\n\tk3_ringacc_ring_free(flow->ringrxfdq);\n\tk3_ringacc_ring_free(flow->ringrx);\n\nerr_rflow_put:\n\txudma_rflow_put(rx_chn->common.udmax, flow->udma_rflow);\n\tflow->udma_rflow = NULL;\n\n\treturn ret;\n}\n\nstatic void k3_udma_glue_dump_rx_chn(struct k3_udma_glue_rx_channel *chn)\n{\n\tstruct device *dev = chn->common.dev;\n\n\tdev_dbg(dev, \"dump_rx_chn:\\n\"\n\t\t\"udma_rchan_id: %d\\n\"\n\t\t\"src_thread: %08x\\n\"\n\t\t\"dst_thread: %08x\\n\"\n\t\t\"epib: %d\\n\"\n\t\t\"hdesc_size: %u\\n\"\n\t\t\"psdata_size: %u\\n\"\n\t\t\"swdata_size: %u\\n\"\n\t\t\"flow_id_base: %d\\n\"\n\t\t\"flow_num: %d\\n\",\n\t\tchn->udma_rchan_id,\n\t\tchn->common.src_thread,\n\t\tchn->common.dst_thread,\n\t\tchn->common.epib,\n\t\tchn->common.hdesc_size,\n\t\tchn->common.psdata_size,\n\t\tchn->common.swdata_size,\n\t\tchn->flow_id_base,\n\t\tchn->flow_num);\n}\n\nstatic void k3_udma_glue_dump_rx_rt_chn(struct k3_udma_glue_rx_channel *chn,\n\t\t\t\t\tchar *mark)\n{\n\tstruct device *dev = chn->common.dev;\n\n\tdev_dbg(dev, \"=== dump ===> %s\\n\", mark);\n\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_CTL_REG,\n\t\txudma_rchanrt_read(chn->udma_rchanx, UDMA_CHAN_RT_CTL_REG));\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\txudma_rchanrt_read(chn->udma_rchanx,\n\t\t\t\t   UDMA_CHAN_RT_PEER_RT_EN_REG));\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_PCNT_REG,\n\t\txudma_rchanrt_read(chn->udma_rchanx, UDMA_CHAN_RT_PCNT_REG));\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_BCNT_REG,\n\t\txudma_rchanrt_read(chn->udma_rchanx, UDMA_CHAN_RT_BCNT_REG));\n\tdev_dbg(dev, \"0x%08X: %08X\\n\", UDMA_CHAN_RT_SBCNT_REG,\n\t\txudma_rchanrt_read(chn->udma_rchanx, UDMA_CHAN_RT_SBCNT_REG));\n}\n\nstatic int\nk3_udma_glue_allocate_rx_flows(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t       struct k3_udma_glue_rx_channel_cfg *cfg)\n{\n\tint ret;\n\n\t \n\tif (cfg->flow_id_use_rxchan_id)\n\t\treturn 0;\n\n\t \n\tif (rx_chn->flow_id_base != -1 &&\n\t    !xudma_rflow_is_gp(rx_chn->common.udmax, rx_chn->flow_id_base))\n\t\treturn 0;\n\n\t \n\tret = xudma_alloc_gp_rflow_range(rx_chn->common.udmax,\n\t\t\t\t\t rx_chn->flow_id_base,\n\t\t\t\t\t rx_chn->flow_num);\n\tif (ret < 0) {\n\t\tdev_err(rx_chn->common.dev, \"UDMAX reserve_rflow %d cnt:%d err: %d\\n\",\n\t\t\trx_chn->flow_id_base, rx_chn->flow_num, ret);\n\t\treturn ret;\n\t}\n\trx_chn->flow_id_base = ret;\n\n\treturn 0;\n}\n\nstatic struct k3_udma_glue_rx_channel *\nk3_udma_glue_request_rx_chn_priv(struct device *dev, const char *name,\n\t\t\t\t struct k3_udma_glue_rx_channel_cfg *cfg)\n{\n\tstruct k3_udma_glue_rx_channel *rx_chn;\n\tstruct psil_endpoint_config *ep_cfg;\n\tint ret, i;\n\n\tif (cfg->flow_id_num <= 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (cfg->flow_id_num != 1 &&\n\t    (cfg->def_flow_cfg || cfg->flow_id_use_rxchan_id))\n\t\treturn ERR_PTR(-EINVAL);\n\n\trx_chn = devm_kzalloc(dev, sizeof(*rx_chn), GFP_KERNEL);\n\tif (!rx_chn)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\trx_chn->common.dev = dev;\n\trx_chn->common.swdata_size = cfg->swdata_size;\n\trx_chn->remote = false;\n\n\t \n\tret = of_k3_udma_glue_parse_chn(dev->of_node, name,\n\t\t\t\t\t&rx_chn->common, false);\n\tif (ret)\n\t\tgoto err;\n\n\trx_chn->common.hdesc_size = cppi5_hdesc_calc_size(rx_chn->common.epib,\n\t\t\t\t\t\trx_chn->common.psdata_size,\n\t\t\t\t\t\trx_chn->common.swdata_size);\n\n\tep_cfg = rx_chn->common.ep_config;\n\n\tif (xudma_is_pktdma(rx_chn->common.udmax))\n\t\trx_chn->udma_rchan_id = ep_cfg->mapped_channel_id;\n\telse\n\t\trx_chn->udma_rchan_id = -1;\n\n\t \n\trx_chn->udma_rchanx = xudma_rchan_get(rx_chn->common.udmax,\n\t\t\t\t\t      rx_chn->udma_rchan_id);\n\tif (IS_ERR(rx_chn->udma_rchanx)) {\n\t\tret = PTR_ERR(rx_chn->udma_rchanx);\n\t\tdev_err(dev, \"UDMAX rchanx get err %d\\n\", ret);\n\t\tgoto err;\n\t}\n\trx_chn->udma_rchan_id = xudma_rchan_get_id(rx_chn->udma_rchanx);\n\n\trx_chn->common.chan_dev.class = &k3_udma_glue_devclass;\n\trx_chn->common.chan_dev.parent = xudma_get_device(rx_chn->common.udmax);\n\tdev_set_name(&rx_chn->common.chan_dev, \"rchan%d-0x%04x\",\n\t\t     rx_chn->udma_rchan_id, rx_chn->common.src_thread);\n\tret = device_register(&rx_chn->common.chan_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"Channel Device registration failed %d\\n\", ret);\n\t\tput_device(&rx_chn->common.chan_dev);\n\t\trx_chn->common.chan_dev.parent = NULL;\n\t\tgoto err;\n\t}\n\n\tif (xudma_is_pktdma(rx_chn->common.udmax)) {\n\t\t \n\t\trx_chn->common.chan_dev.dma_coherent = true;\n\t\tdma_coerce_mask_and_coherent(&rx_chn->common.chan_dev,\n\t\t\t\t\t     DMA_BIT_MASK(48));\n\t}\n\n\tif (xudma_is_pktdma(rx_chn->common.udmax)) {\n\t\tint flow_start = cfg->flow_id_base;\n\t\tint flow_end;\n\n\t\tif (flow_start == -1)\n\t\t\tflow_start = ep_cfg->flow_start;\n\n\t\tflow_end = flow_start + cfg->flow_id_num - 1;\n\t\tif (flow_start < ep_cfg->flow_start ||\n\t\t    flow_end > (ep_cfg->flow_start + ep_cfg->flow_num - 1)) {\n\t\t\tdev_err(dev, \"Invalid flow range requested\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\trx_chn->flow_id_base = flow_start;\n\t} else {\n\t\trx_chn->flow_id_base = cfg->flow_id_base;\n\n\t\t \n\t\tif (cfg->flow_id_use_rxchan_id)\n\t\t\trx_chn->flow_id_base = rx_chn->udma_rchan_id;\n\t}\n\n\trx_chn->flow_num = cfg->flow_id_num;\n\n\trx_chn->flows = devm_kcalloc(dev, rx_chn->flow_num,\n\t\t\t\t     sizeof(*rx_chn->flows), GFP_KERNEL);\n\tif (!rx_chn->flows) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tret = k3_udma_glue_allocate_rx_flows(rx_chn, cfg);\n\tif (ret)\n\t\tgoto err;\n\n\tfor (i = 0; i < rx_chn->flow_num; i++)\n\t\trx_chn->flows[i].udma_rflow_id = rx_chn->flow_id_base + i;\n\n\t \n\trx_chn->common.dst_thread =\n\t\t\txudma_dev_get_psil_base(rx_chn->common.udmax) +\n\t\t\trx_chn->udma_rchan_id;\n\n\tret = k3_udma_glue_cfg_rx_chn(rx_chn);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to cfg rchan %d\\n\", ret);\n\t\tgoto err;\n\t}\n\n\t \n\tif (cfg->def_flow_cfg) {\n\t\tret = k3_udma_glue_cfg_rx_flow(rx_chn, 0, cfg->def_flow_cfg);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tk3_udma_glue_dump_rx_chn(rx_chn);\n\n\treturn rx_chn;\n\nerr:\n\tk3_udma_glue_release_rx_chn(rx_chn);\n\treturn ERR_PTR(ret);\n}\n\nstatic struct k3_udma_glue_rx_channel *\nk3_udma_glue_request_remote_rx_chn(struct device *dev, const char *name,\n\t\t\t\t   struct k3_udma_glue_rx_channel_cfg *cfg)\n{\n\tstruct k3_udma_glue_rx_channel *rx_chn;\n\tint ret, i;\n\n\tif (cfg->flow_id_num <= 0 ||\n\t    cfg->flow_id_use_rxchan_id ||\n\t    cfg->def_flow_cfg ||\n\t    cfg->flow_id_base < 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\n\trx_chn = devm_kzalloc(dev, sizeof(*rx_chn), GFP_KERNEL);\n\tif (!rx_chn)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\trx_chn->common.dev = dev;\n\trx_chn->common.swdata_size = cfg->swdata_size;\n\trx_chn->remote = true;\n\trx_chn->udma_rchan_id = -1;\n\trx_chn->flow_num = cfg->flow_id_num;\n\trx_chn->flow_id_base = cfg->flow_id_base;\n\trx_chn->psil_paired = false;\n\n\t \n\tret = of_k3_udma_glue_parse_chn(dev->of_node, name,\n\t\t\t\t\t&rx_chn->common, false);\n\tif (ret)\n\t\tgoto err;\n\n\trx_chn->common.hdesc_size = cppi5_hdesc_calc_size(rx_chn->common.epib,\n\t\t\t\t\t\trx_chn->common.psdata_size,\n\t\t\t\t\t\trx_chn->common.swdata_size);\n\n\trx_chn->flows = devm_kcalloc(dev, rx_chn->flow_num,\n\t\t\t\t     sizeof(*rx_chn->flows), GFP_KERNEL);\n\tif (!rx_chn->flows) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\trx_chn->common.chan_dev.class = &k3_udma_glue_devclass;\n\trx_chn->common.chan_dev.parent = xudma_get_device(rx_chn->common.udmax);\n\tdev_set_name(&rx_chn->common.chan_dev, \"rchan_remote-0x%04x\",\n\t\t     rx_chn->common.src_thread);\n\tret = device_register(&rx_chn->common.chan_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"Channel Device registration failed %d\\n\", ret);\n\t\tput_device(&rx_chn->common.chan_dev);\n\t\trx_chn->common.chan_dev.parent = NULL;\n\t\tgoto err;\n\t}\n\n\tif (xudma_is_pktdma(rx_chn->common.udmax)) {\n\t\t \n\t\trx_chn->common.chan_dev.dma_coherent = true;\n\t\tdma_coerce_mask_and_coherent(&rx_chn->common.chan_dev,\n\t\t\t\t\t     DMA_BIT_MASK(48));\n\t}\n\n\tret = k3_udma_glue_allocate_rx_flows(rx_chn, cfg);\n\tif (ret)\n\t\tgoto err;\n\n\tfor (i = 0; i < rx_chn->flow_num; i++)\n\t\trx_chn->flows[i].udma_rflow_id = rx_chn->flow_id_base + i;\n\n\tk3_udma_glue_dump_rx_chn(rx_chn);\n\n\treturn rx_chn;\n\nerr:\n\tk3_udma_glue_release_rx_chn(rx_chn);\n\treturn ERR_PTR(ret);\n}\n\nstruct k3_udma_glue_rx_channel *\nk3_udma_glue_request_rx_chn(struct device *dev, const char *name,\n\t\t\t    struct k3_udma_glue_rx_channel_cfg *cfg)\n{\n\tif (cfg->remote)\n\t\treturn k3_udma_glue_request_remote_rx_chn(dev, name, cfg);\n\telse\n\t\treturn k3_udma_glue_request_rx_chn_priv(dev, name, cfg);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_request_rx_chn);\n\nvoid k3_udma_glue_release_rx_chn(struct k3_udma_glue_rx_channel *rx_chn)\n{\n\tint i;\n\n\tif (IS_ERR_OR_NULL(rx_chn->common.udmax))\n\t\treturn;\n\n\tif (rx_chn->psil_paired) {\n\t\txudma_navss_psil_unpair(rx_chn->common.udmax,\n\t\t\t\t\trx_chn->common.src_thread,\n\t\t\t\t\trx_chn->common.dst_thread);\n\t\trx_chn->psil_paired = false;\n\t}\n\n\tfor (i = 0; i < rx_chn->flow_num; i++)\n\t\tk3_udma_glue_release_rx_flow(rx_chn, i);\n\n\tif (xudma_rflow_is_gp(rx_chn->common.udmax, rx_chn->flow_id_base))\n\t\txudma_free_gp_rflow_range(rx_chn->common.udmax,\n\t\t\t\t\t  rx_chn->flow_id_base,\n\t\t\t\t\t  rx_chn->flow_num);\n\n\tif (!IS_ERR_OR_NULL(rx_chn->udma_rchanx))\n\t\txudma_rchan_put(rx_chn->common.udmax,\n\t\t\t\trx_chn->udma_rchanx);\n\n\tif (rx_chn->common.chan_dev.parent) {\n\t\tdevice_unregister(&rx_chn->common.chan_dev);\n\t\trx_chn->common.chan_dev.parent = NULL;\n\t}\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_release_rx_chn);\n\nint k3_udma_glue_rx_flow_init(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t      u32 flow_idx,\n\t\t\t      struct k3_udma_glue_rx_flow_cfg *flow_cfg)\n{\n\tif (flow_idx >= rx_chn->flow_num)\n\t\treturn -EINVAL;\n\n\treturn k3_udma_glue_cfg_rx_flow(rx_chn, flow_idx, flow_cfg);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_rx_flow_init);\n\nu32 k3_udma_glue_rx_flow_get_fdq_id(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t\t    u32 flow_idx)\n{\n\tstruct k3_udma_glue_rx_flow *flow;\n\n\tif (flow_idx >= rx_chn->flow_num)\n\t\treturn -EINVAL;\n\n\tflow = &rx_chn->flows[flow_idx];\n\n\treturn k3_ringacc_get_ring_id(flow->ringrxfdq);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_rx_flow_get_fdq_id);\n\nu32 k3_udma_glue_rx_get_flow_id_base(struct k3_udma_glue_rx_channel *rx_chn)\n{\n\treturn rx_chn->flow_id_base;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_rx_get_flow_id_base);\n\nint k3_udma_glue_rx_flow_enable(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t\tu32 flow_idx)\n{\n\tstruct k3_udma_glue_rx_flow *flow = &rx_chn->flows[flow_idx];\n\tconst struct udma_tisci_rm *tisci_rm = rx_chn->common.tisci_rm;\n\tstruct device *dev = rx_chn->common.dev;\n\tstruct ti_sci_msg_rm_udmap_flow_cfg req;\n\tint rx_ring_id;\n\tint rx_ringfdq_id;\n\tint ret = 0;\n\n\tif (!rx_chn->remote)\n\t\treturn -EINVAL;\n\n\trx_ring_id = k3_ringacc_get_ring_id(flow->ringrx);\n\trx_ringfdq_id = k3_ringacc_get_ring_id(flow->ringrxfdq);\n\n\tmemset(&req, 0, sizeof(req));\n\n\treq.valid_params =\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DEST_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ0_SZ0_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ1_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ2_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ3_QNUM_VALID;\n\treq.nav_id = tisci_rm->tisci_dev_id;\n\treq.flow_index = flow->udma_rflow_id;\n\treq.rx_dest_qnum = rx_ring_id;\n\treq.rx_fdq0_sz0_qnum = rx_ringfdq_id;\n\treq.rx_fdq1_qnum = rx_ringfdq_id;\n\treq.rx_fdq2_qnum = rx_ringfdq_id;\n\treq.rx_fdq3_qnum = rx_ringfdq_id;\n\n\tret = tisci_rm->tisci_udmap_ops->rx_flow_cfg(tisci_rm->tisci, &req);\n\tif (ret) {\n\t\tdev_err(dev, \"flow%d enable failed: %d\\n\", flow->udma_rflow_id,\n\t\t\tret);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_rx_flow_enable);\n\nint k3_udma_glue_rx_flow_disable(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t\t u32 flow_idx)\n{\n\tstruct k3_udma_glue_rx_flow *flow = &rx_chn->flows[flow_idx];\n\tconst struct udma_tisci_rm *tisci_rm = rx_chn->common.tisci_rm;\n\tstruct device *dev = rx_chn->common.dev;\n\tstruct ti_sci_msg_rm_udmap_flow_cfg req;\n\tint ret = 0;\n\n\tif (!rx_chn->remote)\n\t\treturn -EINVAL;\n\n\tmemset(&req, 0, sizeof(req));\n\treq.valid_params =\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DEST_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ0_SZ0_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ1_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ2_QNUM_VALID |\n\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ3_QNUM_VALID;\n\treq.nav_id = tisci_rm->tisci_dev_id;\n\treq.flow_index = flow->udma_rflow_id;\n\treq.rx_dest_qnum = TI_SCI_RESOURCE_NULL;\n\treq.rx_fdq0_sz0_qnum = TI_SCI_RESOURCE_NULL;\n\treq.rx_fdq1_qnum = TI_SCI_RESOURCE_NULL;\n\treq.rx_fdq2_qnum = TI_SCI_RESOURCE_NULL;\n\treq.rx_fdq3_qnum = TI_SCI_RESOURCE_NULL;\n\n\tret = tisci_rm->tisci_udmap_ops->rx_flow_cfg(tisci_rm->tisci, &req);\n\tif (ret) {\n\t\tdev_err(dev, \"flow%d disable failed: %d\\n\", flow->udma_rflow_id,\n\t\t\tret);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_rx_flow_disable);\n\nint k3_udma_glue_enable_rx_chn(struct k3_udma_glue_rx_channel *rx_chn)\n{\n\tint ret;\n\n\tif (rx_chn->remote)\n\t\treturn -EINVAL;\n\n\tif (rx_chn->flows_ready < rx_chn->flow_num)\n\t\treturn -EINVAL;\n\n\tret = xudma_navss_psil_pair(rx_chn->common.udmax,\n\t\t\t\t    rx_chn->common.src_thread,\n\t\t\t\t    rx_chn->common.dst_thread);\n\tif (ret) {\n\t\tdev_err(rx_chn->common.dev, \"PSI-L request err %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\trx_chn->psil_paired = true;\n\n\txudma_rchanrt_write(rx_chn->udma_rchanx, UDMA_CHAN_RT_CTL_REG,\n\t\t\t    UDMA_CHAN_RT_CTL_EN);\n\n\txudma_rchanrt_write(rx_chn->udma_rchanx, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t    UDMA_PEER_RT_EN_ENABLE);\n\n\tk3_udma_glue_dump_rx_rt_chn(rx_chn, \"rxrt en\");\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_enable_rx_chn);\n\nvoid k3_udma_glue_disable_rx_chn(struct k3_udma_glue_rx_channel *rx_chn)\n{\n\tk3_udma_glue_dump_rx_rt_chn(rx_chn, \"rxrt dis1\");\n\n\txudma_rchanrt_write(rx_chn->udma_rchanx,\n\t\t\t    UDMA_CHAN_RT_PEER_RT_EN_REG, 0);\n\txudma_rchanrt_write(rx_chn->udma_rchanx, UDMA_CHAN_RT_CTL_REG, 0);\n\n\tk3_udma_glue_dump_rx_rt_chn(rx_chn, \"rxrt dis2\");\n\n\tif (rx_chn->psil_paired) {\n\t\txudma_navss_psil_unpair(rx_chn->common.udmax,\n\t\t\t\t\trx_chn->common.src_thread,\n\t\t\t\t\trx_chn->common.dst_thread);\n\t\trx_chn->psil_paired = false;\n\t}\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_disable_rx_chn);\n\nvoid k3_udma_glue_tdown_rx_chn(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t       bool sync)\n{\n\tint i = 0;\n\tu32 val;\n\n\tif (rx_chn->remote)\n\t\treturn;\n\n\tk3_udma_glue_dump_rx_rt_chn(rx_chn, \"rxrt tdown1\");\n\n\txudma_rchanrt_write(rx_chn->udma_rchanx, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t    UDMA_PEER_RT_EN_ENABLE | UDMA_PEER_RT_EN_TEARDOWN);\n\n\tval = xudma_rchanrt_read(rx_chn->udma_rchanx, UDMA_CHAN_RT_CTL_REG);\n\n\twhile (sync && (val & UDMA_CHAN_RT_CTL_EN)) {\n\t\tval = xudma_rchanrt_read(rx_chn->udma_rchanx,\n\t\t\t\t\t UDMA_CHAN_RT_CTL_REG);\n\t\tudelay(1);\n\t\tif (i > K3_UDMAX_TDOWN_TIMEOUT_US) {\n\t\t\tdev_err(rx_chn->common.dev, \"RX tdown timeout\\n\");\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\n\tval = xudma_rchanrt_read(rx_chn->udma_rchanx,\n\t\t\t\t UDMA_CHAN_RT_PEER_RT_EN_REG);\n\tif (sync && (val & UDMA_PEER_RT_EN_ENABLE))\n\t\tdev_err(rx_chn->common.dev, \"TX tdown peer not stopped\\n\");\n\tk3_udma_glue_dump_rx_rt_chn(rx_chn, \"rxrt tdown2\");\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_tdown_rx_chn);\n\nvoid k3_udma_glue_reset_rx_chn(struct k3_udma_glue_rx_channel *rx_chn,\n\t\tu32 flow_num, void *data,\n\t\tvoid (*cleanup)(void *data, dma_addr_t desc_dma), bool skip_fdq)\n{\n\tstruct k3_udma_glue_rx_flow *flow = &rx_chn->flows[flow_num];\n\tstruct device *dev = rx_chn->common.dev;\n\tdma_addr_t desc_dma;\n\tint occ_rx, i, ret;\n\n\t \n\tocc_rx = k3_ringacc_ring_get_occ(flow->ringrx);\n\tdev_dbg(dev, \"RX reset flow %u occ_rx %u\\n\", flow_num, occ_rx);\n\n\t \n\tif (skip_fdq)\n\t\tgoto do_reset;\n\n\t \n\tocc_rx = k3_ringacc_ring_get_occ(flow->ringrxfdq);\n\tdev_dbg(dev, \"RX reset flow %u occ_rx_fdq %u\\n\", flow_num, occ_rx);\n\n\tfor (i = 0; i < occ_rx; i++) {\n\t\tret = k3_ringacc_ring_pop(flow->ringrxfdq, &desc_dma);\n\t\tif (ret) {\n\t\t\tif (ret != -ENODATA)\n\t\t\t\tdev_err(dev, \"RX reset pop %d\\n\", ret);\n\t\t\tbreak;\n\t\t}\n\t\tcleanup(data, desc_dma);\n\t}\n\n\tk3_ringacc_ring_reset_dma(flow->ringrxfdq, occ_rx);\n\ndo_reset:\n\tk3_ringacc_ring_reset(flow->ringrx);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_reset_rx_chn);\n\nint k3_udma_glue_push_rx_chn(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t     u32 flow_num, struct cppi5_host_desc_t *desc_rx,\n\t\t\t     dma_addr_t desc_dma)\n{\n\tstruct k3_udma_glue_rx_flow *flow = &rx_chn->flows[flow_num];\n\n\treturn k3_ringacc_ring_push(flow->ringrxfdq, &desc_dma);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_push_rx_chn);\n\nint k3_udma_glue_pop_rx_chn(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t    u32 flow_num, dma_addr_t *desc_dma)\n{\n\tstruct k3_udma_glue_rx_flow *flow = &rx_chn->flows[flow_num];\n\n\treturn k3_ringacc_ring_pop(flow->ringrx, desc_dma);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_pop_rx_chn);\n\nint k3_udma_glue_rx_get_irq(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t    u32 flow_num)\n{\n\tstruct k3_udma_glue_rx_flow *flow;\n\n\tflow = &rx_chn->flows[flow_num];\n\n\tif (xudma_is_pktdma(rx_chn->common.udmax)) {\n\t\tflow->virq = xudma_pktdma_rflow_get_irq(rx_chn->common.udmax,\n\t\t\t\t\t\t\tflow->udma_rflow_id);\n\t} else {\n\t\tflow->virq = k3_ringacc_get_ring_irq_num(flow->ringrx);\n\t}\n\n\treturn flow->virq;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_rx_get_irq);\n\nstruct device *\n\tk3_udma_glue_rx_get_dma_device(struct k3_udma_glue_rx_channel *rx_chn)\n{\n\tif (xudma_is_pktdma(rx_chn->common.udmax) &&\n\t    (rx_chn->common.atype_asel == 14 || rx_chn->common.atype_asel == 15))\n\t\treturn &rx_chn->common.chan_dev;\n\n\treturn xudma_get_device(rx_chn->common.udmax);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_rx_get_dma_device);\n\nvoid k3_udma_glue_rx_dma_to_cppi5_addr(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t\t       dma_addr_t *addr)\n{\n\tif (!xudma_is_pktdma(rx_chn->common.udmax) ||\n\t    !rx_chn->common.atype_asel)\n\t\treturn;\n\n\t*addr |= (u64)rx_chn->common.atype_asel << K3_ADDRESS_ASEL_SHIFT;\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_rx_dma_to_cppi5_addr);\n\nvoid k3_udma_glue_rx_cppi5_to_dma_addr(struct k3_udma_glue_rx_channel *rx_chn,\n\t\t\t\t       dma_addr_t *addr)\n{\n\tif (!xudma_is_pktdma(rx_chn->common.udmax) ||\n\t    !rx_chn->common.atype_asel)\n\t\treturn;\n\n\t*addr &= (u64)GENMASK(K3_ADDRESS_ASEL_SHIFT - 1, 0);\n}\nEXPORT_SYMBOL_GPL(k3_udma_glue_rx_cppi5_to_dma_addr);\n\nstatic int __init k3_udma_glue_class_init(void)\n{\n\treturn class_register(&k3_udma_glue_devclass);\n}\n\nmodule_init(k3_udma_glue_class_init);\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}