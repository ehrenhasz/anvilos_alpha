{
  "module_name": "k3-udma.c",
  "hash_id": "f5812b3332c0fbf56b4768dfeac93f6469a39efa7d6d2622278e62a5676d95dc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/ti/k3-udma.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/delay.h>\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/list.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/sys_soc.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n#include <linux/of_irq.h>\n#include <linux/workqueue.h>\n#include <linux/completion.h>\n#include <linux/soc/ti/k3-ringacc.h>\n#include <linux/soc/ti/ti_sci_protocol.h>\n#include <linux/soc/ti/ti_sci_inta_msi.h>\n#include <linux/dma/k3-event-router.h>\n#include <linux/dma/ti-cppi5.h>\n\n#include \"../virt-dma.h\"\n#include \"k3-udma.h\"\n#include \"k3-psil-priv.h\"\n\nstruct udma_static_tr {\n\tu8 elsize;  \n\tu16 elcnt;  \n\tu16 bstcnt;  \n};\n\n#define K3_UDMA_MAX_RFLOWS\t\t1024\n#define K3_UDMA_DEFAULT_RING_SIZE\t16\n\n \n#define UDMA_RFLOW_SRCTAG_NONE\t\t0\n#define UDMA_RFLOW_SRCTAG_CFG_TAG\t1\n#define UDMA_RFLOW_SRCTAG_FLOW_ID\t2\n#define UDMA_RFLOW_SRCTAG_SRC_TAG\t4\n\n#define UDMA_RFLOW_DSTTAG_NONE\t\t0\n#define UDMA_RFLOW_DSTTAG_CFG_TAG\t1\n#define UDMA_RFLOW_DSTTAG_FLOW_ID\t2\n#define UDMA_RFLOW_DSTTAG_DST_TAG_LO\t4\n#define UDMA_RFLOW_DSTTAG_DST_TAG_HI\t5\n\nstruct udma_chan;\n\nenum k3_dma_type {\n\tDMA_TYPE_UDMA = 0,\n\tDMA_TYPE_BCDMA,\n\tDMA_TYPE_PKTDMA,\n};\n\nenum udma_mmr {\n\tMMR_GCFG = 0,\n\tMMR_BCHANRT,\n\tMMR_RCHANRT,\n\tMMR_TCHANRT,\n\tMMR_LAST,\n};\n\nstatic const char * const mmr_names[] = {\n\t[MMR_GCFG] = \"gcfg\",\n\t[MMR_BCHANRT] = \"bchanrt\",\n\t[MMR_RCHANRT] = \"rchanrt\",\n\t[MMR_TCHANRT] = \"tchanrt\",\n};\n\nstruct udma_tchan {\n\tvoid __iomem *reg_rt;\n\n\tint id;\n\tstruct k3_ring *t_ring;  \n\tstruct k3_ring *tc_ring;  \n\tint tflow_id;  \n\n};\n\n#define udma_bchan udma_tchan\n\nstruct udma_rflow {\n\tint id;\n\tstruct k3_ring *fd_ring;  \n\tstruct k3_ring *r_ring;  \n};\n\nstruct udma_rchan {\n\tvoid __iomem *reg_rt;\n\n\tint id;\n};\n\nstruct udma_oes_offsets {\n\t \n\tu32 udma_rchan;\n\n\t \n\tu32 bcdma_bchan_data;\n\tu32 bcdma_bchan_ring;\n\tu32 bcdma_tchan_data;\n\tu32 bcdma_tchan_ring;\n\tu32 bcdma_rchan_data;\n\tu32 bcdma_rchan_ring;\n\n\t \n\tu32 pktdma_tchan_flow;\n\tu32 pktdma_rchan_flow;\n};\n\n#define UDMA_FLAG_PDMA_ACC32\t\tBIT(0)\n#define UDMA_FLAG_PDMA_BURST\t\tBIT(1)\n#define UDMA_FLAG_TDTYPE\t\tBIT(2)\n#define UDMA_FLAG_BURST_SIZE\t\tBIT(3)\n#define UDMA_FLAGS_J7_CLASS\t\t(UDMA_FLAG_PDMA_ACC32 | \\\n\t\t\t\t\t UDMA_FLAG_PDMA_BURST | \\\n\t\t\t\t\t UDMA_FLAG_TDTYPE | \\\n\t\t\t\t\t UDMA_FLAG_BURST_SIZE)\n\nstruct udma_match_data {\n\tenum k3_dma_type type;\n\tu32 psil_base;\n\tbool enable_memcpy_support;\n\tu32 flags;\n\tu32 statictr_z_mask;\n\tu8 burst_size[3];\n\tstruct udma_soc_data *soc_data;\n};\n\nstruct udma_soc_data {\n\tstruct udma_oes_offsets oes;\n\tu32 bcdma_trigger_event_offset;\n};\n\nstruct udma_hwdesc {\n\tsize_t cppi5_desc_size;\n\tvoid *cppi5_desc_vaddr;\n\tdma_addr_t cppi5_desc_paddr;\n\n\t \n\tvoid *tr_req_base;\n\tstruct cppi5_tr_resp_t *tr_resp_base;\n};\n\nstruct udma_rx_flush {\n\tstruct udma_hwdesc hwdescs[2];\n\n\tsize_t buffer_size;\n\tvoid *buffer_vaddr;\n\tdma_addr_t buffer_paddr;\n};\n\nstruct udma_tpl {\n\tu8 levels;\n\tu32 start_idx[3];\n};\n\nstruct udma_dev {\n\tstruct dma_device ddev;\n\tstruct device *dev;\n\tvoid __iomem *mmrs[MMR_LAST];\n\tconst struct udma_match_data *match_data;\n\tconst struct udma_soc_data *soc_data;\n\n\tstruct udma_tpl bchan_tpl;\n\tstruct udma_tpl tchan_tpl;\n\tstruct udma_tpl rchan_tpl;\n\n\tsize_t desc_align;  \n\n\tstruct udma_tisci_rm tisci_rm;\n\n\tstruct k3_ringacc *ringacc;\n\n\tstruct work_struct purge_work;\n\tstruct list_head desc_to_purge;\n\tspinlock_t lock;\n\n\tstruct udma_rx_flush rx_flush;\n\n\tint bchan_cnt;\n\tint tchan_cnt;\n\tint echan_cnt;\n\tint rchan_cnt;\n\tint rflow_cnt;\n\tint tflow_cnt;\n\tunsigned long *bchan_map;\n\tunsigned long *tchan_map;\n\tunsigned long *rchan_map;\n\tunsigned long *rflow_gp_map;\n\tunsigned long *rflow_gp_map_allocated;\n\tunsigned long *rflow_in_use;\n\tunsigned long *tflow_map;\n\n\tstruct udma_bchan *bchans;\n\tstruct udma_tchan *tchans;\n\tstruct udma_rchan *rchans;\n\tstruct udma_rflow *rflows;\n\n\tstruct udma_chan *channels;\n\tu32 psil_base;\n\tu32 atype;\n\tu32 asel;\n};\n\nstruct udma_desc {\n\tstruct virt_dma_desc vd;\n\n\tbool terminated;\n\n\tenum dma_transfer_direction dir;\n\n\tstruct udma_static_tr static_tr;\n\tu32 residue;\n\n\tunsigned int sglen;\n\tunsigned int desc_idx;  \n\tunsigned int tr_idx;\n\n\tu32 metadata_size;\n\tvoid *metadata;  \n\n\tunsigned int hwdesc_count;\n\tstruct udma_hwdesc hwdesc[];\n};\n\nenum udma_chan_state {\n\tUDMA_CHAN_IS_IDLE = 0,  \n\tUDMA_CHAN_IS_ACTIVE,  \n\tUDMA_CHAN_IS_TERMINATING,  \n};\n\nstruct udma_tx_drain {\n\tstruct delayed_work work;\n\tktime_t tstamp;\n\tu32 residue;\n};\n\nstruct udma_chan_config {\n\tbool pkt_mode;  \n\tbool needs_epib;  \n\tu32 psd_size;  \n\tu32 metadata_size;  \n\tu32 hdesc_size;  \n\tbool notdpkt;  \n\tint remote_thread_id;\n\tu32 atype;\n\tu32 asel;\n\tu32 src_thread;\n\tu32 dst_thread;\n\tenum psil_endpoint_type ep_type;\n\tbool enable_acc32;\n\tbool enable_burst;\n\tenum udma_tp_level channel_tpl;  \n\n\tu32 tr_trigger_type;\n\tunsigned long tx_flags;\n\n\t \n\tint mapped_channel_id;\n\t \n\tint default_flow_id;\n\n\tenum dma_transfer_direction dir;\n};\n\nstruct udma_chan {\n\tstruct virt_dma_chan vc;\n\tstruct dma_slave_config\tcfg;\n\tstruct udma_dev *ud;\n\tstruct device *dma_dev;\n\tstruct udma_desc *desc;\n\tstruct udma_desc *terminated_desc;\n\tstruct udma_static_tr static_tr;\n\tchar *name;\n\n\tstruct udma_bchan *bchan;\n\tstruct udma_tchan *tchan;\n\tstruct udma_rchan *rchan;\n\tstruct udma_rflow *rflow;\n\n\tbool psil_paired;\n\n\tint irq_num_ring;\n\tint irq_num_udma;\n\n\tbool cyclic;\n\tbool paused;\n\n\tenum udma_chan_state state;\n\tstruct completion teardown_completed;\n\n\tstruct udma_tx_drain tx_drain;\n\n\t \n\tstruct udma_chan_config config;\n\t \n\tstruct udma_chan_config backup_config;\n\n\t \n\tbool use_dma_pool;\n\tstruct dma_pool *hdesc_pool;\n\n\tu32 id;\n};\n\nstatic inline struct udma_dev *to_udma_dev(struct dma_device *d)\n{\n\treturn container_of(d, struct udma_dev, ddev);\n}\n\nstatic inline struct udma_chan *to_udma_chan(struct dma_chan *c)\n{\n\treturn container_of(c, struct udma_chan, vc.chan);\n}\n\nstatic inline struct udma_desc *to_udma_desc(struct dma_async_tx_descriptor *t)\n{\n\treturn container_of(t, struct udma_desc, vd.tx);\n}\n\n \nstatic inline u32 udma_read(void __iomem *base, int reg)\n{\n\treturn readl(base + reg);\n}\n\nstatic inline void udma_write(void __iomem *base, int reg, u32 val)\n{\n\twritel(val, base + reg);\n}\n\nstatic inline void udma_update_bits(void __iomem *base, int reg,\n\t\t\t\t    u32 mask, u32 val)\n{\n\tu32 tmp, orig;\n\n\torig = readl(base + reg);\n\ttmp = orig & ~mask;\n\ttmp |= (val & mask);\n\n\tif (tmp != orig)\n\t\twritel(tmp, base + reg);\n}\n\n \nstatic inline u32 udma_tchanrt_read(struct udma_chan *uc, int reg)\n{\n\tif (!uc->tchan)\n\t\treturn 0;\n\treturn udma_read(uc->tchan->reg_rt, reg);\n}\n\nstatic inline void udma_tchanrt_write(struct udma_chan *uc, int reg, u32 val)\n{\n\tif (!uc->tchan)\n\t\treturn;\n\tudma_write(uc->tchan->reg_rt, reg, val);\n}\n\nstatic inline void udma_tchanrt_update_bits(struct udma_chan *uc, int reg,\n\t\t\t\t\t    u32 mask, u32 val)\n{\n\tif (!uc->tchan)\n\t\treturn;\n\tudma_update_bits(uc->tchan->reg_rt, reg, mask, val);\n}\n\n \nstatic inline u32 udma_rchanrt_read(struct udma_chan *uc, int reg)\n{\n\tif (!uc->rchan)\n\t\treturn 0;\n\treturn udma_read(uc->rchan->reg_rt, reg);\n}\n\nstatic inline void udma_rchanrt_write(struct udma_chan *uc, int reg, u32 val)\n{\n\tif (!uc->rchan)\n\t\treturn;\n\tudma_write(uc->rchan->reg_rt, reg, val);\n}\n\nstatic inline void udma_rchanrt_update_bits(struct udma_chan *uc, int reg,\n\t\t\t\t\t    u32 mask, u32 val)\n{\n\tif (!uc->rchan)\n\t\treturn;\n\tudma_update_bits(uc->rchan->reg_rt, reg, mask, val);\n}\n\nstatic int navss_psil_pair(struct udma_dev *ud, u32 src_thread, u32 dst_thread)\n{\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\n\tdst_thread |= K3_PSIL_DST_THREAD_ID_OFFSET;\n\treturn tisci_rm->tisci_psil_ops->pair(tisci_rm->tisci,\n\t\t\t\t\t      tisci_rm->tisci_navss_dev_id,\n\t\t\t\t\t      src_thread, dst_thread);\n}\n\nstatic int navss_psil_unpair(struct udma_dev *ud, u32 src_thread,\n\t\t\t     u32 dst_thread)\n{\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\n\tdst_thread |= K3_PSIL_DST_THREAD_ID_OFFSET;\n\treturn tisci_rm->tisci_psil_ops->unpair(tisci_rm->tisci,\n\t\t\t\t\t\ttisci_rm->tisci_navss_dev_id,\n\t\t\t\t\t\tsrc_thread, dst_thread);\n}\n\nstatic void k3_configure_chan_coherency(struct dma_chan *chan, u32 asel)\n{\n\tstruct device *chan_dev = &chan->dev->device;\n\n\tif (asel == 0) {\n\t\t \n\t\tchan->dev->chan_dma_dev = false;\n\n\t\tchan_dev->dma_coherent = false;\n\t\tchan_dev->dma_parms = NULL;\n\t} else if (asel == 14 || asel == 15) {\n\t\tchan->dev->chan_dma_dev = true;\n\n\t\tchan_dev->dma_coherent = true;\n\t\tdma_coerce_mask_and_coherent(chan_dev, DMA_BIT_MASK(48));\n\t\tchan_dev->dma_parms = chan_dev->parent->dma_parms;\n\t} else {\n\t\tdev_warn(chan->device->dev, \"Invalid ASEL value: %u\\n\", asel);\n\n\t\tchan_dev->dma_coherent = false;\n\t\tchan_dev->dma_parms = NULL;\n\t}\n}\n\nstatic u8 udma_get_chan_tpl_index(struct udma_tpl *tpl_map, int chan_id)\n{\n\tint i;\n\n\tfor (i = 0; i < tpl_map->levels; i++) {\n\t\tif (chan_id >= tpl_map->start_idx[i])\n\t\t\treturn i;\n\t}\n\n\treturn 0;\n}\n\nstatic void udma_reset_uchan(struct udma_chan *uc)\n{\n\tmemset(&uc->config, 0, sizeof(uc->config));\n\tuc->config.remote_thread_id = -1;\n\tuc->config.mapped_channel_id = -1;\n\tuc->config.default_flow_id = -1;\n\tuc->state = UDMA_CHAN_IS_IDLE;\n}\n\nstatic void udma_dump_chan_stdata(struct udma_chan *uc)\n{\n\tstruct device *dev = uc->ud->dev;\n\tu32 offset;\n\tint i;\n\n\tif (uc->config.dir == DMA_MEM_TO_DEV || uc->config.dir == DMA_MEM_TO_MEM) {\n\t\tdev_dbg(dev, \"TCHAN State data:\\n\");\n\t\tfor (i = 0; i < 32; i++) {\n\t\t\toffset = UDMA_CHAN_RT_STDATA_REG + i * 4;\n\t\t\tdev_dbg(dev, \"TRT_STDATA[%02d]: 0x%08x\\n\", i,\n\t\t\t\tudma_tchanrt_read(uc, offset));\n\t\t}\n\t}\n\n\tif (uc->config.dir == DMA_DEV_TO_MEM || uc->config.dir == DMA_MEM_TO_MEM) {\n\t\tdev_dbg(dev, \"RCHAN State data:\\n\");\n\t\tfor (i = 0; i < 32; i++) {\n\t\t\toffset = UDMA_CHAN_RT_STDATA_REG + i * 4;\n\t\t\tdev_dbg(dev, \"RRT_STDATA[%02d]: 0x%08x\\n\", i,\n\t\t\t\tudma_rchanrt_read(uc, offset));\n\t\t}\n\t}\n}\n\nstatic inline dma_addr_t udma_curr_cppi5_desc_paddr(struct udma_desc *d,\n\t\t\t\t\t\t    int idx)\n{\n\treturn d->hwdesc[idx].cppi5_desc_paddr;\n}\n\nstatic inline void *udma_curr_cppi5_desc_vaddr(struct udma_desc *d, int idx)\n{\n\treturn d->hwdesc[idx].cppi5_desc_vaddr;\n}\n\nstatic struct udma_desc *udma_udma_desc_from_paddr(struct udma_chan *uc,\n\t\t\t\t\t\t   dma_addr_t paddr)\n{\n\tstruct udma_desc *d = uc->terminated_desc;\n\n\tif (d) {\n\t\tdma_addr_t desc_paddr = udma_curr_cppi5_desc_paddr(d,\n\t\t\t\t\t\t\t\t   d->desc_idx);\n\n\t\tif (desc_paddr != paddr)\n\t\t\td = NULL;\n\t}\n\n\tif (!d) {\n\t\td = uc->desc;\n\t\tif (d) {\n\t\t\tdma_addr_t desc_paddr = udma_curr_cppi5_desc_paddr(d,\n\t\t\t\t\t\t\t\td->desc_idx);\n\n\t\t\tif (desc_paddr != paddr)\n\t\t\t\td = NULL;\n\t\t}\n\t}\n\n\treturn d;\n}\n\nstatic void udma_free_hwdesc(struct udma_chan *uc, struct udma_desc *d)\n{\n\tif (uc->use_dma_pool) {\n\t\tint i;\n\n\t\tfor (i = 0; i < d->hwdesc_count; i++) {\n\t\t\tif (!d->hwdesc[i].cppi5_desc_vaddr)\n\t\t\t\tcontinue;\n\n\t\t\tdma_pool_free(uc->hdesc_pool,\n\t\t\t\t      d->hwdesc[i].cppi5_desc_vaddr,\n\t\t\t\t      d->hwdesc[i].cppi5_desc_paddr);\n\n\t\t\td->hwdesc[i].cppi5_desc_vaddr = NULL;\n\t\t}\n\t} else if (d->hwdesc[0].cppi5_desc_vaddr) {\n\t\tdma_free_coherent(uc->dma_dev, d->hwdesc[0].cppi5_desc_size,\n\t\t\t\t  d->hwdesc[0].cppi5_desc_vaddr,\n\t\t\t\t  d->hwdesc[0].cppi5_desc_paddr);\n\n\t\td->hwdesc[0].cppi5_desc_vaddr = NULL;\n\t}\n}\n\nstatic void udma_purge_desc_work(struct work_struct *work)\n{\n\tstruct udma_dev *ud = container_of(work, typeof(*ud), purge_work);\n\tstruct virt_dma_desc *vd, *_vd;\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&ud->lock, flags);\n\tlist_splice_tail_init(&ud->desc_to_purge, &head);\n\tspin_unlock_irqrestore(&ud->lock, flags);\n\n\tlist_for_each_entry_safe(vd, _vd, &head, node) {\n\t\tstruct udma_chan *uc = to_udma_chan(vd->tx.chan);\n\t\tstruct udma_desc *d = to_udma_desc(&vd->tx);\n\n\t\tudma_free_hwdesc(uc, d);\n\t\tlist_del(&vd->node);\n\t\tkfree(d);\n\t}\n\n\t \n\tif (!list_empty(&ud->desc_to_purge))\n\t\tschedule_work(&ud->purge_work);\n}\n\nstatic void udma_desc_free(struct virt_dma_desc *vd)\n{\n\tstruct udma_dev *ud = to_udma_dev(vd->tx.chan->device);\n\tstruct udma_chan *uc = to_udma_chan(vd->tx.chan);\n\tstruct udma_desc *d = to_udma_desc(&vd->tx);\n\tunsigned long flags;\n\n\tif (uc->terminated_desc == d)\n\t\tuc->terminated_desc = NULL;\n\n\tif (uc->use_dma_pool) {\n\t\tudma_free_hwdesc(uc, d);\n\t\tkfree(d);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&ud->lock, flags);\n\tlist_add_tail(&vd->node, &ud->desc_to_purge);\n\tspin_unlock_irqrestore(&ud->lock, flags);\n\n\tschedule_work(&ud->purge_work);\n}\n\nstatic bool udma_is_chan_running(struct udma_chan *uc)\n{\n\tu32 trt_ctl = 0;\n\tu32 rrt_ctl = 0;\n\n\tif (uc->tchan)\n\t\ttrt_ctl = udma_tchanrt_read(uc, UDMA_CHAN_RT_CTL_REG);\n\tif (uc->rchan)\n\t\trrt_ctl = udma_rchanrt_read(uc, UDMA_CHAN_RT_CTL_REG);\n\n\tif (trt_ctl & UDMA_CHAN_RT_CTL_EN || rrt_ctl & UDMA_CHAN_RT_CTL_EN)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool udma_is_chan_paused(struct udma_chan *uc)\n{\n\tu32 val, pause_mask;\n\n\tswitch (uc->config.dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\tval = udma_rchanrt_read(uc, UDMA_CHAN_RT_PEER_RT_EN_REG);\n\t\tpause_mask = UDMA_PEER_RT_EN_PAUSE;\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\tval = udma_tchanrt_read(uc, UDMA_CHAN_RT_PEER_RT_EN_REG);\n\t\tpause_mask = UDMA_PEER_RT_EN_PAUSE;\n\t\tbreak;\n\tcase DMA_MEM_TO_MEM:\n\t\tval = udma_tchanrt_read(uc, UDMA_CHAN_RT_CTL_REG);\n\t\tpause_mask = UDMA_CHAN_RT_CTL_PAUSE;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\tif (val & pause_mask)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline dma_addr_t udma_get_rx_flush_hwdesc_paddr(struct udma_chan *uc)\n{\n\treturn uc->ud->rx_flush.hwdescs[uc->config.pkt_mode].cppi5_desc_paddr;\n}\n\nstatic int udma_push_to_ring(struct udma_chan *uc, int idx)\n{\n\tstruct udma_desc *d = uc->desc;\n\tstruct k3_ring *ring = NULL;\n\tdma_addr_t paddr;\n\n\tswitch (uc->config.dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\tring = uc->rflow->fd_ring;\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\tcase DMA_MEM_TO_MEM:\n\t\tring = uc->tchan->t_ring;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (idx == -1) {\n\t\tpaddr = udma_get_rx_flush_hwdesc_paddr(uc);\n\t} else {\n\t\tpaddr = udma_curr_cppi5_desc_paddr(d, idx);\n\n\t\twmb();  \n\t}\n\n\treturn k3_ringacc_ring_push(ring, &paddr);\n}\n\nstatic bool udma_desc_is_rx_flush(struct udma_chan *uc, dma_addr_t addr)\n{\n\tif (uc->config.dir != DMA_DEV_TO_MEM)\n\t\treturn false;\n\n\tif (addr == udma_get_rx_flush_hwdesc_paddr(uc))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int udma_pop_from_ring(struct udma_chan *uc, dma_addr_t *addr)\n{\n\tstruct k3_ring *ring = NULL;\n\tint ret;\n\n\tswitch (uc->config.dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\tring = uc->rflow->r_ring;\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\tcase DMA_MEM_TO_MEM:\n\t\tring = uc->tchan->tc_ring;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tret = k3_ringacc_ring_pop(ring, addr);\n\tif (ret)\n\t\treturn ret;\n\n\trmb();  \n\n\t \n\tif (cppi5_desc_is_tdcm(*addr))\n\t\treturn 0;\n\n\t \n\tif (udma_desc_is_rx_flush(uc, *addr))\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nstatic void udma_reset_rings(struct udma_chan *uc)\n{\n\tstruct k3_ring *ring1 = NULL;\n\tstruct k3_ring *ring2 = NULL;\n\n\tswitch (uc->config.dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\tif (uc->rchan) {\n\t\t\tring1 = uc->rflow->fd_ring;\n\t\t\tring2 = uc->rflow->r_ring;\n\t\t}\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\tcase DMA_MEM_TO_MEM:\n\t\tif (uc->tchan) {\n\t\t\tring1 = uc->tchan->t_ring;\n\t\t\tring2 = uc->tchan->tc_ring;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (ring1)\n\t\tk3_ringacc_ring_reset_dma(ring1,\n\t\t\t\t\t  k3_ringacc_ring_get_occ(ring1));\n\tif (ring2)\n\t\tk3_ringacc_ring_reset(ring2);\n\n\t \n\tif (uc->terminated_desc) {\n\t\tudma_desc_free(&uc->terminated_desc->vd);\n\t\tuc->terminated_desc = NULL;\n\t}\n}\n\nstatic void udma_decrement_byte_counters(struct udma_chan *uc, u32 val)\n{\n\tif (uc->desc->dir == DMA_DEV_TO_MEM) {\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_BCNT_REG, val);\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_SBCNT_REG, val);\n\t\tif (uc->config.ep_type != PSIL_EP_NATIVE)\n\t\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_PEER_BCNT_REG, val);\n\t} else {\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_BCNT_REG, val);\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_SBCNT_REG, val);\n\t\tif (!uc->bchan && uc->config.ep_type != PSIL_EP_NATIVE)\n\t\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_PEER_BCNT_REG, val);\n\t}\n}\n\nstatic void udma_reset_counters(struct udma_chan *uc)\n{\n\tu32 val;\n\n\tif (uc->tchan) {\n\t\tval = udma_tchanrt_read(uc, UDMA_CHAN_RT_BCNT_REG);\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_BCNT_REG, val);\n\n\t\tval = udma_tchanrt_read(uc, UDMA_CHAN_RT_SBCNT_REG);\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_SBCNT_REG, val);\n\n\t\tval = udma_tchanrt_read(uc, UDMA_CHAN_RT_PCNT_REG);\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_PCNT_REG, val);\n\n\t\tif (!uc->bchan) {\n\t\t\tval = udma_tchanrt_read(uc, UDMA_CHAN_RT_PEER_BCNT_REG);\n\t\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_PEER_BCNT_REG, val);\n\t\t}\n\t}\n\n\tif (uc->rchan) {\n\t\tval = udma_rchanrt_read(uc, UDMA_CHAN_RT_BCNT_REG);\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_BCNT_REG, val);\n\n\t\tval = udma_rchanrt_read(uc, UDMA_CHAN_RT_SBCNT_REG);\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_SBCNT_REG, val);\n\n\t\tval = udma_rchanrt_read(uc, UDMA_CHAN_RT_PCNT_REG);\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_PCNT_REG, val);\n\n\t\tval = udma_rchanrt_read(uc, UDMA_CHAN_RT_PEER_BCNT_REG);\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_PEER_BCNT_REG, val);\n\t}\n}\n\nstatic int udma_reset_chan(struct udma_chan *uc, bool hard)\n{\n\tswitch (uc->config.dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_PEER_RT_EN_REG, 0);\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_CTL_REG, 0);\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_CTL_REG, 0);\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_PEER_RT_EN_REG, 0);\n\t\tbreak;\n\tcase DMA_MEM_TO_MEM:\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_CTL_REG, 0);\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_CTL_REG, 0);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tudma_reset_counters(uc);\n\n\t \n\tif (hard) {\n\t\tstruct udma_chan_config ucc_backup;\n\t\tint ret;\n\n\t\tmemcpy(&ucc_backup, &uc->config, sizeof(uc->config));\n\t\tuc->ud->ddev.device_free_chan_resources(&uc->vc.chan);\n\n\t\t \n\t\tmemcpy(&uc->config, &ucc_backup, sizeof(uc->config));\n\t\tret = uc->ud->ddev.device_alloc_chan_resources(&uc->vc.chan);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tif (uc->config.dir == DMA_DEV_TO_MEM)\n\t\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_CTL_REG,\n\t\t\t\t\t   UDMA_CHAN_RT_CTL_EN |\n\t\t\t\t\t   UDMA_CHAN_RT_CTL_TDOWN |\n\t\t\t\t\t   UDMA_CHAN_RT_CTL_FTDOWN);\n\t}\n\tuc->state = UDMA_CHAN_IS_IDLE;\n\n\treturn 0;\n}\n\nstatic void udma_start_desc(struct udma_chan *uc)\n{\n\tstruct udma_chan_config *ucc = &uc->config;\n\n\tif (uc->ud->match_data->type == DMA_TYPE_UDMA && ucc->pkt_mode &&\n\t    (uc->cyclic || ucc->dir == DMA_DEV_TO_MEM)) {\n\t\tint i;\n\n\t\t \n\t\tfor (i = 0; i < uc->desc->sglen; i++)\n\t\t\tudma_push_to_ring(uc, i);\n\t} else {\n\t\tudma_push_to_ring(uc, 0);\n\t}\n}\n\nstatic bool udma_chan_needs_reconfiguration(struct udma_chan *uc)\n{\n\t \n\tif (uc->config.ep_type == PSIL_EP_NATIVE)\n\t\treturn false;\n\n\t \n\tif (memcmp(&uc->static_tr, &uc->desc->static_tr, sizeof(uc->static_tr)))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int udma_start(struct udma_chan *uc)\n{\n\tstruct virt_dma_desc *vd = vchan_next_desc(&uc->vc);\n\n\tif (!vd) {\n\t\tuc->desc = NULL;\n\t\treturn -ENOENT;\n\t}\n\n\tlist_del(&vd->node);\n\n\tuc->desc = to_udma_desc(&vd->tx);\n\n\t \n\tif (udma_is_chan_running(uc) && !udma_chan_needs_reconfiguration(uc)) {\n\t\tudma_start_desc(uc);\n\t\tgoto out;\n\t}\n\n\t \n\tudma_reset_chan(uc, false);\n\n\t \n\tudma_start_desc(uc);\n\n\tswitch (uc->desc->dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\t \n\t\tif (uc->config.ep_type == PSIL_EP_PDMA_XY) {\n\t\t\tu32 val = PDMA_STATIC_TR_Y(uc->desc->static_tr.elcnt) |\n\t\t\t\t  PDMA_STATIC_TR_X(uc->desc->static_tr.elsize);\n\t\t\tconst struct udma_match_data *match_data =\n\t\t\t\t\t\t\tuc->ud->match_data;\n\n\t\t\tif (uc->config.enable_acc32)\n\t\t\t\tval |= PDMA_STATIC_TR_XY_ACC32;\n\t\t\tif (uc->config.enable_burst)\n\t\t\t\tval |= PDMA_STATIC_TR_XY_BURST;\n\n\t\t\tudma_rchanrt_write(uc,\n\t\t\t\t\t   UDMA_CHAN_RT_PEER_STATIC_TR_XY_REG,\n\t\t\t\t\t   val);\n\n\t\t\tudma_rchanrt_write(uc,\n\t\t\t\tUDMA_CHAN_RT_PEER_STATIC_TR_Z_REG,\n\t\t\t\tPDMA_STATIC_TR_Z(uc->desc->static_tr.bstcnt,\n\t\t\t\t\t\t match_data->statictr_z_mask));\n\n\t\t\t \n\t\t\tmemcpy(&uc->static_tr, &uc->desc->static_tr,\n\t\t\t       sizeof(uc->static_tr));\n\t\t}\n\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_CTL_REG,\n\t\t\t\t   UDMA_CHAN_RT_CTL_EN);\n\n\t\t \n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t\t   UDMA_PEER_RT_EN_ENABLE);\n\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\t \n\t\tif (uc->config.ep_type == PSIL_EP_PDMA_XY) {\n\t\t\tu32 val = PDMA_STATIC_TR_Y(uc->desc->static_tr.elcnt) |\n\t\t\t\t  PDMA_STATIC_TR_X(uc->desc->static_tr.elsize);\n\n\t\t\tif (uc->config.enable_acc32)\n\t\t\t\tval |= PDMA_STATIC_TR_XY_ACC32;\n\t\t\tif (uc->config.enable_burst)\n\t\t\t\tval |= PDMA_STATIC_TR_XY_BURST;\n\n\t\t\tudma_tchanrt_write(uc,\n\t\t\t\t\t   UDMA_CHAN_RT_PEER_STATIC_TR_XY_REG,\n\t\t\t\t\t   val);\n\n\t\t\t \n\t\t\tmemcpy(&uc->static_tr, &uc->desc->static_tr,\n\t\t\t       sizeof(uc->static_tr));\n\t\t}\n\n\t\t \n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t\t   UDMA_PEER_RT_EN_ENABLE);\n\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_CTL_REG,\n\t\t\t\t   UDMA_CHAN_RT_CTL_EN);\n\n\t\tbreak;\n\tcase DMA_MEM_TO_MEM:\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_CTL_REG,\n\t\t\t\t   UDMA_CHAN_RT_CTL_EN);\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_CTL_REG,\n\t\t\t\t   UDMA_CHAN_RT_CTL_EN);\n\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tuc->state = UDMA_CHAN_IS_ACTIVE;\nout:\n\n\treturn 0;\n}\n\nstatic int udma_stop(struct udma_chan *uc)\n{\n\tenum udma_chan_state old_state = uc->state;\n\n\tuc->state = UDMA_CHAN_IS_TERMINATING;\n\treinit_completion(&uc->teardown_completed);\n\n\tswitch (uc->config.dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\tif (!uc->cyclic && !uc->desc)\n\t\t\tudma_push_to_ring(uc, -1);\n\n\t\tudma_rchanrt_write(uc, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t\t   UDMA_PEER_RT_EN_ENABLE |\n\t\t\t\t   UDMA_PEER_RT_EN_TEARDOWN);\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t\t   UDMA_PEER_RT_EN_ENABLE |\n\t\t\t\t   UDMA_PEER_RT_EN_FLUSH);\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_CTL_REG,\n\t\t\t\t   UDMA_CHAN_RT_CTL_EN |\n\t\t\t\t   UDMA_CHAN_RT_CTL_TDOWN);\n\t\tbreak;\n\tcase DMA_MEM_TO_MEM:\n\t\tudma_tchanrt_write(uc, UDMA_CHAN_RT_CTL_REG,\n\t\t\t\t   UDMA_CHAN_RT_CTL_EN |\n\t\t\t\t   UDMA_CHAN_RT_CTL_TDOWN);\n\t\tbreak;\n\tdefault:\n\t\tuc->state = old_state;\n\t\tcomplete_all(&uc->teardown_completed);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void udma_cyclic_packet_elapsed(struct udma_chan *uc)\n{\n\tstruct udma_desc *d = uc->desc;\n\tstruct cppi5_host_desc_t *h_desc;\n\n\th_desc = d->hwdesc[d->desc_idx].cppi5_desc_vaddr;\n\tcppi5_hdesc_reset_to_original(h_desc);\n\tudma_push_to_ring(uc, d->desc_idx);\n\td->desc_idx = (d->desc_idx + 1) % d->sglen;\n}\n\nstatic inline void udma_fetch_epib(struct udma_chan *uc, struct udma_desc *d)\n{\n\tstruct cppi5_host_desc_t *h_desc = d->hwdesc[0].cppi5_desc_vaddr;\n\n\tmemcpy(d->metadata, h_desc->epib, d->metadata_size);\n}\n\nstatic bool udma_is_desc_really_done(struct udma_chan *uc, struct udma_desc *d)\n{\n\tu32 peer_bcnt, bcnt;\n\n\t \n\tif (uc->config.ep_type == PSIL_EP_NATIVE ||\n\t    uc->config.dir != DMA_MEM_TO_DEV || !(uc->config.tx_flags & DMA_PREP_INTERRUPT))\n\t\treturn true;\n\n\tpeer_bcnt = udma_tchanrt_read(uc, UDMA_CHAN_RT_PEER_BCNT_REG);\n\tbcnt = udma_tchanrt_read(uc, UDMA_CHAN_RT_BCNT_REG);\n\n\t \n\tif (peer_bcnt < bcnt) {\n\t\tuc->tx_drain.residue = bcnt - peer_bcnt;\n\t\tuc->tx_drain.tstamp = ktime_get();\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void udma_check_tx_completion(struct work_struct *work)\n{\n\tstruct udma_chan *uc = container_of(work, typeof(*uc),\n\t\t\t\t\t    tx_drain.work.work);\n\tbool desc_done = true;\n\tu32 residue_diff;\n\tktime_t time_diff;\n\tunsigned long delay;\n\n\twhile (1) {\n\t\tif (uc->desc) {\n\t\t\t \n\t\t\tresidue_diff = uc->tx_drain.residue;\n\t\t\ttime_diff = uc->tx_drain.tstamp;\n\t\t\t \n\t\t\tdesc_done = udma_is_desc_really_done(uc, uc->desc);\n\t\t}\n\n\t\tif (!desc_done) {\n\t\t\t \n\t\t\ttime_diff = ktime_sub(uc->tx_drain.tstamp,\n\t\t\t\t\t      time_diff) + 1;\n\t\t\tresidue_diff -= uc->tx_drain.residue;\n\t\t\tif (residue_diff) {\n\t\t\t\t \n\t\t\t\tdelay = (time_diff / residue_diff) *\n\t\t\t\t\tuc->tx_drain.residue;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tschedule_delayed_work(&uc->tx_drain.work, HZ);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tusleep_range(ktime_to_us(delay),\n\t\t\t\t     ktime_to_us(delay) + 10);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (uc->desc) {\n\t\t\tstruct udma_desc *d = uc->desc;\n\n\t\t\tudma_decrement_byte_counters(uc, d->residue);\n\t\t\tudma_start(uc);\n\t\t\tvchan_cookie_complete(&d->vd);\n\t\t\tbreak;\n\t\t}\n\n\t\tbreak;\n\t}\n}\n\nstatic irqreturn_t udma_ring_irq_handler(int irq, void *data)\n{\n\tstruct udma_chan *uc = data;\n\tstruct udma_desc *d;\n\tdma_addr_t paddr = 0;\n\n\tif (udma_pop_from_ring(uc, &paddr) || !paddr)\n\t\treturn IRQ_HANDLED;\n\n\tspin_lock(&uc->vc.lock);\n\n\t \n\tif (cppi5_desc_is_tdcm(paddr)) {\n\t\tcomplete_all(&uc->teardown_completed);\n\n\t\tif (uc->terminated_desc) {\n\t\t\tudma_desc_free(&uc->terminated_desc->vd);\n\t\t\tuc->terminated_desc = NULL;\n\t\t}\n\n\t\tif (!uc->desc)\n\t\t\tudma_start(uc);\n\n\t\tgoto out;\n\t}\n\n\td = udma_udma_desc_from_paddr(uc, paddr);\n\n\tif (d) {\n\t\tdma_addr_t desc_paddr = udma_curr_cppi5_desc_paddr(d,\n\t\t\t\t\t\t\t\t   d->desc_idx);\n\t\tif (desc_paddr != paddr) {\n\t\t\tdev_err(uc->ud->dev, \"not matching descriptors!\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (d == uc->desc) {\n\t\t\t \n\t\t\tif (uc->cyclic) {\n\t\t\t\tudma_cyclic_packet_elapsed(uc);\n\t\t\t\tvchan_cyclic_callback(&d->vd);\n\t\t\t} else {\n\t\t\t\tif (udma_is_desc_really_done(uc, d)) {\n\t\t\t\t\tudma_decrement_byte_counters(uc, d->residue);\n\t\t\t\t\tudma_start(uc);\n\t\t\t\t\tvchan_cookie_complete(&d->vd);\n\t\t\t\t} else {\n\t\t\t\t\tschedule_delayed_work(&uc->tx_drain.work,\n\t\t\t\t\t\t\t      0);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tdma_cookie_complete(&d->vd.tx);\n\t\t}\n\t}\nout:\n\tspin_unlock(&uc->vc.lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t udma_udma_irq_handler(int irq, void *data)\n{\n\tstruct udma_chan *uc = data;\n\tstruct udma_desc *d;\n\n\tspin_lock(&uc->vc.lock);\n\td = uc->desc;\n\tif (d) {\n\t\td->tr_idx = (d->tr_idx + 1) % d->sglen;\n\n\t\tif (uc->cyclic) {\n\t\t\tvchan_cyclic_callback(&d->vd);\n\t\t} else {\n\t\t\t \n\t\t\tudma_decrement_byte_counters(uc, d->residue);\n\t\t\tudma_start(uc);\n\t\t\tvchan_cookie_complete(&d->vd);\n\t\t}\n\t}\n\n\tspin_unlock(&uc->vc.lock);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int __udma_alloc_gp_rflow_range(struct udma_dev *ud, int from, int cnt)\n{\n\tint start, tmp_from;\n\tDECLARE_BITMAP(tmp, K3_UDMA_MAX_RFLOWS);\n\n\ttmp_from = from;\n\tif (tmp_from < 0)\n\t\ttmp_from = ud->rchan_cnt;\n\t \n\tif (tmp_from < ud->rchan_cnt)\n\t\treturn -EINVAL;\n\n\tif (tmp_from + cnt > ud->rflow_cnt)\n\t\treturn -EINVAL;\n\n\tbitmap_or(tmp, ud->rflow_gp_map, ud->rflow_gp_map_allocated,\n\t\t  ud->rflow_cnt);\n\n\tstart = bitmap_find_next_zero_area(tmp,\n\t\t\t\t\t   ud->rflow_cnt,\n\t\t\t\t\t   tmp_from, cnt, 0);\n\tif (start >= ud->rflow_cnt)\n\t\treturn -ENOMEM;\n\n\tif (from >= 0 && start != from)\n\t\treturn -EEXIST;\n\n\tbitmap_set(ud->rflow_gp_map_allocated, start, cnt);\n\treturn start;\n}\n\nstatic int __udma_free_gp_rflow_range(struct udma_dev *ud, int from, int cnt)\n{\n\tif (from < ud->rchan_cnt)\n\t\treturn -EINVAL;\n\tif (from + cnt > ud->rflow_cnt)\n\t\treturn -EINVAL;\n\n\tbitmap_clear(ud->rflow_gp_map_allocated, from, cnt);\n\treturn 0;\n}\n\nstatic struct udma_rflow *__udma_get_rflow(struct udma_dev *ud, int id)\n{\n\t \n\n\tif (id < 0 || id >= ud->rflow_cnt)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (test_bit(id, ud->rflow_in_use))\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (ud->rflow_gp_map) {\n\t\t \n\t\tif (!test_bit(id, ud->rflow_gp_map) &&\n\t\t    !test_bit(id, ud->rflow_gp_map_allocated))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tdev_dbg(ud->dev, \"get rflow%d\\n\", id);\n\tset_bit(id, ud->rflow_in_use);\n\treturn &ud->rflows[id];\n}\n\nstatic void __udma_put_rflow(struct udma_dev *ud, struct udma_rflow *rflow)\n{\n\tif (!test_bit(rflow->id, ud->rflow_in_use)) {\n\t\tdev_err(ud->dev, \"attempt to put unused rflow%d\\n\", rflow->id);\n\t\treturn;\n\t}\n\n\tdev_dbg(ud->dev, \"put rflow%d\\n\", rflow->id);\n\tclear_bit(rflow->id, ud->rflow_in_use);\n}\n\n#define UDMA_RESERVE_RESOURCE(res)\t\t\t\t\t\\\nstatic struct udma_##res *__udma_reserve_##res(struct udma_dev *ud,\t\\\n\t\t\t\t\t       enum udma_tp_level tpl,\t\\\n\t\t\t\t\t       int id)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tif (id >= 0) {\t\t\t\t\t\t\t\\\n\t\tif (test_bit(id, ud->res##_map)) {\t\t\t\\\n\t\t\tdev_err(ud->dev, \"res##%d is in use\\n\", id);\t\\\n\t\t\treturn ERR_PTR(-ENOENT);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} else {\t\t\t\t\t\t\t\\\n\t\tint start;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tif (tpl >= ud->res##_tpl.levels)\t\t\t\\\n\t\t\ttpl = ud->res##_tpl.levels - 1;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tstart = ud->res##_tpl.start_idx[tpl];\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tid = find_next_zero_bit(ud->res##_map, ud->res##_cnt,\t\\\n\t\t\t\t\tstart);\t\t\t\t\\\n\t\tif (id == ud->res##_cnt) {\t\t\t\t\\\n\t\t\treturn ERR_PTR(-ENOENT);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tset_bit(id, ud->res##_map);\t\t\t\t\t\\\n\treturn &ud->res##s[id];\t\t\t\t\t\t\\\n}\n\nUDMA_RESERVE_RESOURCE(bchan);\nUDMA_RESERVE_RESOURCE(tchan);\nUDMA_RESERVE_RESOURCE(rchan);\n\nstatic int bcdma_get_bchan(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tenum udma_tp_level tpl;\n\tint ret;\n\n\tif (uc->bchan) {\n\t\tdev_dbg(ud->dev, \"chan%d: already have bchan%d allocated\\n\",\n\t\t\tuc->id, uc->bchan->id);\n\t\treturn 0;\n\t}\n\n\t \n\tif (uc->config.tr_trigger_type)\n\t\ttpl = 0;\n\telse\n\t\ttpl = ud->bchan_tpl.levels - 1;\n\n\tuc->bchan = __udma_reserve_bchan(ud, tpl, -1);\n\tif (IS_ERR(uc->bchan)) {\n\t\tret = PTR_ERR(uc->bchan);\n\t\tuc->bchan = NULL;\n\t\treturn ret;\n\t}\n\n\tuc->tchan = uc->bchan;\n\n\treturn 0;\n}\n\nstatic int udma_get_tchan(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tint ret;\n\n\tif (uc->tchan) {\n\t\tdev_dbg(ud->dev, \"chan%d: already have tchan%d allocated\\n\",\n\t\t\tuc->id, uc->tchan->id);\n\t\treturn 0;\n\t}\n\n\t \n\tuc->tchan = __udma_reserve_tchan(ud, uc->config.channel_tpl,\n\t\t\t\t\t uc->config.mapped_channel_id);\n\tif (IS_ERR(uc->tchan)) {\n\t\tret = PTR_ERR(uc->tchan);\n\t\tuc->tchan = NULL;\n\t\treturn ret;\n\t}\n\n\tif (ud->tflow_cnt) {\n\t\tint tflow_id;\n\n\t\t \n\t\tif (uc->config.default_flow_id >= 0)\n\t\t\ttflow_id = uc->config.default_flow_id;\n\t\telse\n\t\t\ttflow_id = uc->tchan->id;\n\n\t\tif (test_bit(tflow_id, ud->tflow_map)) {\n\t\t\tdev_err(ud->dev, \"tflow%d is in use\\n\", tflow_id);\n\t\t\tclear_bit(uc->tchan->id, ud->tchan_map);\n\t\t\tuc->tchan = NULL;\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tuc->tchan->tflow_id = tflow_id;\n\t\tset_bit(tflow_id, ud->tflow_map);\n\t} else {\n\t\tuc->tchan->tflow_id = -1;\n\t}\n\n\treturn 0;\n}\n\nstatic int udma_get_rchan(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tint ret;\n\n\tif (uc->rchan) {\n\t\tdev_dbg(ud->dev, \"chan%d: already have rchan%d allocated\\n\",\n\t\t\tuc->id, uc->rchan->id);\n\t\treturn 0;\n\t}\n\n\t \n\tuc->rchan = __udma_reserve_rchan(ud, uc->config.channel_tpl,\n\t\t\t\t\t uc->config.mapped_channel_id);\n\tif (IS_ERR(uc->rchan)) {\n\t\tret = PTR_ERR(uc->rchan);\n\t\tuc->rchan = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int udma_get_chan_pair(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tint chan_id, end;\n\n\tif ((uc->tchan && uc->rchan) && uc->tchan->id == uc->rchan->id) {\n\t\tdev_info(ud->dev, \"chan%d: already have %d pair allocated\\n\",\n\t\t\t uc->id, uc->tchan->id);\n\t\treturn 0;\n\t}\n\n\tif (uc->tchan) {\n\t\tdev_err(ud->dev, \"chan%d: already have tchan%d allocated\\n\",\n\t\t\tuc->id, uc->tchan->id);\n\t\treturn -EBUSY;\n\t} else if (uc->rchan) {\n\t\tdev_err(ud->dev, \"chan%d: already have rchan%d allocated\\n\",\n\t\t\tuc->id, uc->rchan->id);\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tend = min(ud->tchan_cnt, ud->rchan_cnt);\n\t \n\tchan_id = ud->tchan_tpl.start_idx[ud->tchan_tpl.levels - 1];\n\tfor (; chan_id < end; chan_id++) {\n\t\tif (!test_bit(chan_id, ud->tchan_map) &&\n\t\t    !test_bit(chan_id, ud->rchan_map))\n\t\t\tbreak;\n\t}\n\n\tif (chan_id == end)\n\t\treturn -ENOENT;\n\n\tset_bit(chan_id, ud->tchan_map);\n\tset_bit(chan_id, ud->rchan_map);\n\tuc->tchan = &ud->tchans[chan_id];\n\tuc->rchan = &ud->rchans[chan_id];\n\n\t \n\tuc->tchan->tflow_id = -1;\n\n\treturn 0;\n}\n\nstatic int udma_get_rflow(struct udma_chan *uc, int flow_id)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tint ret;\n\n\tif (!uc->rchan) {\n\t\tdev_err(ud->dev, \"chan%d: does not have rchan??\\n\", uc->id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (uc->rflow) {\n\t\tdev_dbg(ud->dev, \"chan%d: already have rflow%d allocated\\n\",\n\t\t\tuc->id, uc->rflow->id);\n\t\treturn 0;\n\t}\n\n\tuc->rflow = __udma_get_rflow(ud, flow_id);\n\tif (IS_ERR(uc->rflow)) {\n\t\tret = PTR_ERR(uc->rflow);\n\t\tuc->rflow = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void bcdma_put_bchan(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\n\tif (uc->bchan) {\n\t\tdev_dbg(ud->dev, \"chan%d: put bchan%d\\n\", uc->id,\n\t\t\tuc->bchan->id);\n\t\tclear_bit(uc->bchan->id, ud->bchan_map);\n\t\tuc->bchan = NULL;\n\t\tuc->tchan = NULL;\n\t}\n}\n\nstatic void udma_put_rchan(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\n\tif (uc->rchan) {\n\t\tdev_dbg(ud->dev, \"chan%d: put rchan%d\\n\", uc->id,\n\t\t\tuc->rchan->id);\n\t\tclear_bit(uc->rchan->id, ud->rchan_map);\n\t\tuc->rchan = NULL;\n\t}\n}\n\nstatic void udma_put_tchan(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\n\tif (uc->tchan) {\n\t\tdev_dbg(ud->dev, \"chan%d: put tchan%d\\n\", uc->id,\n\t\t\tuc->tchan->id);\n\t\tclear_bit(uc->tchan->id, ud->tchan_map);\n\n\t\tif (uc->tchan->tflow_id >= 0)\n\t\t\tclear_bit(uc->tchan->tflow_id, ud->tflow_map);\n\n\t\tuc->tchan = NULL;\n\t}\n}\n\nstatic void udma_put_rflow(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\n\tif (uc->rflow) {\n\t\tdev_dbg(ud->dev, \"chan%d: put rflow%d\\n\", uc->id,\n\t\t\tuc->rflow->id);\n\t\t__udma_put_rflow(ud, uc->rflow);\n\t\tuc->rflow = NULL;\n\t}\n}\n\nstatic void bcdma_free_bchan_resources(struct udma_chan *uc)\n{\n\tif (!uc->bchan)\n\t\treturn;\n\n\tk3_ringacc_ring_free(uc->bchan->tc_ring);\n\tk3_ringacc_ring_free(uc->bchan->t_ring);\n\tuc->bchan->tc_ring = NULL;\n\tuc->bchan->t_ring = NULL;\n\tk3_configure_chan_coherency(&uc->vc.chan, 0);\n\n\tbcdma_put_bchan(uc);\n}\n\nstatic int bcdma_alloc_bchan_resources(struct udma_chan *uc)\n{\n\tstruct k3_ring_cfg ring_cfg;\n\tstruct udma_dev *ud = uc->ud;\n\tint ret;\n\n\tret = bcdma_get_bchan(uc);\n\tif (ret)\n\t\treturn ret;\n\n\tret = k3_ringacc_request_rings_pair(ud->ringacc, uc->bchan->id, -1,\n\t\t\t\t\t    &uc->bchan->t_ring,\n\t\t\t\t\t    &uc->bchan->tc_ring);\n\tif (ret) {\n\t\tret = -EBUSY;\n\t\tgoto err_ring;\n\t}\n\n\tmemset(&ring_cfg, 0, sizeof(ring_cfg));\n\tring_cfg.size = K3_UDMA_DEFAULT_RING_SIZE;\n\tring_cfg.elm_size = K3_RINGACC_RING_ELSIZE_8;\n\tring_cfg.mode = K3_RINGACC_RING_MODE_RING;\n\n\tk3_configure_chan_coherency(&uc->vc.chan, ud->asel);\n\tring_cfg.asel = ud->asel;\n\tring_cfg.dma_dev = dmaengine_get_dma_device(&uc->vc.chan);\n\n\tret = k3_ringacc_ring_cfg(uc->bchan->t_ring, &ring_cfg);\n\tif (ret)\n\t\tgoto err_ringcfg;\n\n\treturn 0;\n\nerr_ringcfg:\n\tk3_ringacc_ring_free(uc->bchan->tc_ring);\n\tuc->bchan->tc_ring = NULL;\n\tk3_ringacc_ring_free(uc->bchan->t_ring);\n\tuc->bchan->t_ring = NULL;\n\tk3_configure_chan_coherency(&uc->vc.chan, 0);\nerr_ring:\n\tbcdma_put_bchan(uc);\n\n\treturn ret;\n}\n\nstatic void udma_free_tx_resources(struct udma_chan *uc)\n{\n\tif (!uc->tchan)\n\t\treturn;\n\n\tk3_ringacc_ring_free(uc->tchan->t_ring);\n\tk3_ringacc_ring_free(uc->tchan->tc_ring);\n\tuc->tchan->t_ring = NULL;\n\tuc->tchan->tc_ring = NULL;\n\n\tudma_put_tchan(uc);\n}\n\nstatic int udma_alloc_tx_resources(struct udma_chan *uc)\n{\n\tstruct k3_ring_cfg ring_cfg;\n\tstruct udma_dev *ud = uc->ud;\n\tstruct udma_tchan *tchan;\n\tint ring_idx, ret;\n\n\tret = udma_get_tchan(uc);\n\tif (ret)\n\t\treturn ret;\n\n\ttchan = uc->tchan;\n\tif (tchan->tflow_id >= 0)\n\t\tring_idx = tchan->tflow_id;\n\telse\n\t\tring_idx = ud->bchan_cnt + tchan->id;\n\n\tret = k3_ringacc_request_rings_pair(ud->ringacc, ring_idx, -1,\n\t\t\t\t\t    &tchan->t_ring,\n\t\t\t\t\t    &tchan->tc_ring);\n\tif (ret) {\n\t\tret = -EBUSY;\n\t\tgoto err_ring;\n\t}\n\n\tmemset(&ring_cfg, 0, sizeof(ring_cfg));\n\tring_cfg.size = K3_UDMA_DEFAULT_RING_SIZE;\n\tring_cfg.elm_size = K3_RINGACC_RING_ELSIZE_8;\n\tif (ud->match_data->type == DMA_TYPE_UDMA) {\n\t\tring_cfg.mode = K3_RINGACC_RING_MODE_MESSAGE;\n\t} else {\n\t\tring_cfg.mode = K3_RINGACC_RING_MODE_RING;\n\n\t\tk3_configure_chan_coherency(&uc->vc.chan, uc->config.asel);\n\t\tring_cfg.asel = uc->config.asel;\n\t\tring_cfg.dma_dev = dmaengine_get_dma_device(&uc->vc.chan);\n\t}\n\n\tret = k3_ringacc_ring_cfg(tchan->t_ring, &ring_cfg);\n\tret |= k3_ringacc_ring_cfg(tchan->tc_ring, &ring_cfg);\n\n\tif (ret)\n\t\tgoto err_ringcfg;\n\n\treturn 0;\n\nerr_ringcfg:\n\tk3_ringacc_ring_free(uc->tchan->tc_ring);\n\tuc->tchan->tc_ring = NULL;\n\tk3_ringacc_ring_free(uc->tchan->t_ring);\n\tuc->tchan->t_ring = NULL;\nerr_ring:\n\tudma_put_tchan(uc);\n\n\treturn ret;\n}\n\nstatic void udma_free_rx_resources(struct udma_chan *uc)\n{\n\tif (!uc->rchan)\n\t\treturn;\n\n\tif (uc->rflow) {\n\t\tstruct udma_rflow *rflow = uc->rflow;\n\n\t\tk3_ringacc_ring_free(rflow->fd_ring);\n\t\tk3_ringacc_ring_free(rflow->r_ring);\n\t\trflow->fd_ring = NULL;\n\t\trflow->r_ring = NULL;\n\n\t\tudma_put_rflow(uc);\n\t}\n\n\tudma_put_rchan(uc);\n}\n\nstatic int udma_alloc_rx_resources(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tstruct k3_ring_cfg ring_cfg;\n\tstruct udma_rflow *rflow;\n\tint fd_ring_id;\n\tint ret;\n\n\tret = udma_get_rchan(uc);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (uc->config.dir == DMA_MEM_TO_MEM)\n\t\treturn 0;\n\n\tif (uc->config.default_flow_id >= 0)\n\t\tret = udma_get_rflow(uc, uc->config.default_flow_id);\n\telse\n\t\tret = udma_get_rflow(uc, uc->rchan->id);\n\n\tif (ret) {\n\t\tret = -EBUSY;\n\t\tgoto err_rflow;\n\t}\n\n\trflow = uc->rflow;\n\tif (ud->tflow_cnt)\n\t\tfd_ring_id = ud->tflow_cnt + rflow->id;\n\telse\n\t\tfd_ring_id = ud->bchan_cnt + ud->tchan_cnt + ud->echan_cnt +\n\t\t\t     uc->rchan->id;\n\n\tret = k3_ringacc_request_rings_pair(ud->ringacc, fd_ring_id, -1,\n\t\t\t\t\t    &rflow->fd_ring, &rflow->r_ring);\n\tif (ret) {\n\t\tret = -EBUSY;\n\t\tgoto err_ring;\n\t}\n\n\tmemset(&ring_cfg, 0, sizeof(ring_cfg));\n\n\tring_cfg.elm_size = K3_RINGACC_RING_ELSIZE_8;\n\tif (ud->match_data->type == DMA_TYPE_UDMA) {\n\t\tif (uc->config.pkt_mode)\n\t\t\tring_cfg.size = SG_MAX_SEGMENTS;\n\t\telse\n\t\t\tring_cfg.size = K3_UDMA_DEFAULT_RING_SIZE;\n\n\t\tring_cfg.mode = K3_RINGACC_RING_MODE_MESSAGE;\n\t} else {\n\t\tring_cfg.size = K3_UDMA_DEFAULT_RING_SIZE;\n\t\tring_cfg.mode = K3_RINGACC_RING_MODE_RING;\n\n\t\tk3_configure_chan_coherency(&uc->vc.chan, uc->config.asel);\n\t\tring_cfg.asel = uc->config.asel;\n\t\tring_cfg.dma_dev = dmaengine_get_dma_device(&uc->vc.chan);\n\t}\n\n\tret = k3_ringacc_ring_cfg(rflow->fd_ring, &ring_cfg);\n\n\tring_cfg.size = K3_UDMA_DEFAULT_RING_SIZE;\n\tret |= k3_ringacc_ring_cfg(rflow->r_ring, &ring_cfg);\n\n\tif (ret)\n\t\tgoto err_ringcfg;\n\n\treturn 0;\n\nerr_ringcfg:\n\tk3_ringacc_ring_free(rflow->r_ring);\n\trflow->r_ring = NULL;\n\tk3_ringacc_ring_free(rflow->fd_ring);\n\trflow->fd_ring = NULL;\nerr_ring:\n\tudma_put_rflow(uc);\nerr_rflow:\n\tudma_put_rchan(uc);\n\n\treturn ret;\n}\n\n#define TISCI_BCDMA_BCHAN_VALID_PARAMS (\t\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_PAUSE_ON_ERR_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_EXTENDED_CH_TYPE_VALID)\n\n#define TISCI_BCDMA_TCHAN_VALID_PARAMS (\t\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_PAUSE_ON_ERR_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_TX_SUPR_TDPKT_VALID)\n\n#define TISCI_BCDMA_RCHAN_VALID_PARAMS (\t\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_PAUSE_ON_ERR_VALID)\n\n#define TISCI_UDMA_TCHAN_VALID_PARAMS (\t\t\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_PAUSE_ON_ERR_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_TX_FILT_EINFO_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_TX_FILT_PSWORDS_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_CHAN_TYPE_VALID |\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_TX_SUPR_TDPKT_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_FETCH_SIZE_VALID |\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_CQ_QNUM_VALID |\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_ATYPE_VALID)\n\n#define TISCI_UDMA_RCHAN_VALID_PARAMS (\t\t\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_PAUSE_ON_ERR_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_FETCH_SIZE_VALID |\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_CQ_QNUM_VALID |\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_CHAN_TYPE_VALID |\t\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_RX_IGNORE_SHORT_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_RX_IGNORE_LONG_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_RX_FLOWID_START_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_RX_FLOWID_CNT_VALID |\t\\\n\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_ATYPE_VALID)\n\nstatic int udma_tisci_m2m_channel_config(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tconst struct ti_sci_rm_udmap_ops *tisci_ops = tisci_rm->tisci_udmap_ops;\n\tstruct udma_tchan *tchan = uc->tchan;\n\tstruct udma_rchan *rchan = uc->rchan;\n\tu8 burst_size = 0;\n\tint ret;\n\tu8 tpl;\n\n\t \n\tint tc_ring = k3_ringacc_get_ring_id(tchan->tc_ring);\n\tstruct ti_sci_msg_rm_udmap_tx_ch_cfg req_tx = { 0 };\n\tstruct ti_sci_msg_rm_udmap_rx_ch_cfg req_rx = { 0 };\n\n\tif (ud->match_data->flags & UDMA_FLAG_BURST_SIZE) {\n\t\ttpl = udma_get_chan_tpl_index(&ud->tchan_tpl, tchan->id);\n\n\t\tburst_size = ud->match_data->burst_size[tpl];\n\t}\n\n\treq_tx.valid_params = TISCI_UDMA_TCHAN_VALID_PARAMS;\n\treq_tx.nav_id = tisci_rm->tisci_dev_id;\n\treq_tx.index = tchan->id;\n\treq_tx.tx_chan_type = TI_SCI_RM_UDMAP_CHAN_TYPE_3RDP_BCOPY_PBRR;\n\treq_tx.tx_fetch_size = sizeof(struct cppi5_desc_hdr_t) >> 2;\n\treq_tx.txcq_qnum = tc_ring;\n\treq_tx.tx_atype = ud->atype;\n\tif (burst_size) {\n\t\treq_tx.valid_params |= TI_SCI_MSG_VALUE_RM_UDMAP_CH_BURST_SIZE_VALID;\n\t\treq_tx.tx_burst_size = burst_size;\n\t}\n\n\tret = tisci_ops->tx_ch_cfg(tisci_rm->tisci, &req_tx);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"tchan%d cfg failed %d\\n\", tchan->id, ret);\n\t\treturn ret;\n\t}\n\n\treq_rx.valid_params = TISCI_UDMA_RCHAN_VALID_PARAMS;\n\treq_rx.nav_id = tisci_rm->tisci_dev_id;\n\treq_rx.index = rchan->id;\n\treq_rx.rx_fetch_size = sizeof(struct cppi5_desc_hdr_t) >> 2;\n\treq_rx.rxcq_qnum = tc_ring;\n\treq_rx.rx_chan_type = TI_SCI_RM_UDMAP_CHAN_TYPE_3RDP_BCOPY_PBRR;\n\treq_rx.rx_atype = ud->atype;\n\tif (burst_size) {\n\t\treq_rx.valid_params |= TI_SCI_MSG_VALUE_RM_UDMAP_CH_BURST_SIZE_VALID;\n\t\treq_rx.rx_burst_size = burst_size;\n\t}\n\n\tret = tisci_ops->rx_ch_cfg(tisci_rm->tisci, &req_rx);\n\tif (ret)\n\t\tdev_err(ud->dev, \"rchan%d alloc failed %d\\n\", rchan->id, ret);\n\n\treturn ret;\n}\n\nstatic int bcdma_tisci_m2m_channel_config(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tconst struct ti_sci_rm_udmap_ops *tisci_ops = tisci_rm->tisci_udmap_ops;\n\tstruct ti_sci_msg_rm_udmap_tx_ch_cfg req_tx = { 0 };\n\tstruct udma_bchan *bchan = uc->bchan;\n\tu8 burst_size = 0;\n\tint ret;\n\tu8 tpl;\n\n\tif (ud->match_data->flags & UDMA_FLAG_BURST_SIZE) {\n\t\ttpl = udma_get_chan_tpl_index(&ud->bchan_tpl, bchan->id);\n\n\t\tburst_size = ud->match_data->burst_size[tpl];\n\t}\n\n\treq_tx.valid_params = TISCI_BCDMA_BCHAN_VALID_PARAMS;\n\treq_tx.nav_id = tisci_rm->tisci_dev_id;\n\treq_tx.extended_ch_type = TI_SCI_RM_BCDMA_EXTENDED_CH_TYPE_BCHAN;\n\treq_tx.index = bchan->id;\n\tif (burst_size) {\n\t\treq_tx.valid_params |= TI_SCI_MSG_VALUE_RM_UDMAP_CH_BURST_SIZE_VALID;\n\t\treq_tx.tx_burst_size = burst_size;\n\t}\n\n\tret = tisci_ops->tx_ch_cfg(tisci_rm->tisci, &req_tx);\n\tif (ret)\n\t\tdev_err(ud->dev, \"bchan%d cfg failed %d\\n\", bchan->id, ret);\n\n\treturn ret;\n}\n\nstatic int udma_tisci_tx_channel_config(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tconst struct ti_sci_rm_udmap_ops *tisci_ops = tisci_rm->tisci_udmap_ops;\n\tstruct udma_tchan *tchan = uc->tchan;\n\tint tc_ring = k3_ringacc_get_ring_id(tchan->tc_ring);\n\tstruct ti_sci_msg_rm_udmap_tx_ch_cfg req_tx = { 0 };\n\tu32 mode, fetch_size;\n\tint ret;\n\n\tif (uc->config.pkt_mode) {\n\t\tmode = TI_SCI_RM_UDMAP_CHAN_TYPE_PKT_PBRR;\n\t\tfetch_size = cppi5_hdesc_calc_size(uc->config.needs_epib,\n\t\t\t\t\t\t   uc->config.psd_size, 0);\n\t} else {\n\t\tmode = TI_SCI_RM_UDMAP_CHAN_TYPE_3RDP_PBRR;\n\t\tfetch_size = sizeof(struct cppi5_desc_hdr_t);\n\t}\n\n\treq_tx.valid_params = TISCI_UDMA_TCHAN_VALID_PARAMS;\n\treq_tx.nav_id = tisci_rm->tisci_dev_id;\n\treq_tx.index = tchan->id;\n\treq_tx.tx_chan_type = mode;\n\treq_tx.tx_supr_tdpkt = uc->config.notdpkt;\n\treq_tx.tx_fetch_size = fetch_size >> 2;\n\treq_tx.txcq_qnum = tc_ring;\n\treq_tx.tx_atype = uc->config.atype;\n\tif (uc->config.ep_type == PSIL_EP_PDMA_XY &&\n\t    ud->match_data->flags & UDMA_FLAG_TDTYPE) {\n\t\t \n\t\treq_tx.valid_params |=\n\t\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_TX_TDTYPE_VALID;\n\t\treq_tx.tx_tdtype = 1;\n\t}\n\n\tret = tisci_ops->tx_ch_cfg(tisci_rm->tisci, &req_tx);\n\tif (ret)\n\t\tdev_err(ud->dev, \"tchan%d cfg failed %d\\n\", tchan->id, ret);\n\n\treturn ret;\n}\n\nstatic int bcdma_tisci_tx_channel_config(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tconst struct ti_sci_rm_udmap_ops *tisci_ops = tisci_rm->tisci_udmap_ops;\n\tstruct udma_tchan *tchan = uc->tchan;\n\tstruct ti_sci_msg_rm_udmap_tx_ch_cfg req_tx = { 0 };\n\tint ret;\n\n\treq_tx.valid_params = TISCI_BCDMA_TCHAN_VALID_PARAMS;\n\treq_tx.nav_id = tisci_rm->tisci_dev_id;\n\treq_tx.index = tchan->id;\n\treq_tx.tx_supr_tdpkt = uc->config.notdpkt;\n\tif (ud->match_data->flags & UDMA_FLAG_TDTYPE) {\n\t\t \n\t\treq_tx.valid_params |=\n\t\t\t\tTI_SCI_MSG_VALUE_RM_UDMAP_CH_TX_TDTYPE_VALID;\n\t\treq_tx.tx_tdtype = 1;\n\t}\n\n\tret = tisci_ops->tx_ch_cfg(tisci_rm->tisci, &req_tx);\n\tif (ret)\n\t\tdev_err(ud->dev, \"tchan%d cfg failed %d\\n\", tchan->id, ret);\n\n\treturn ret;\n}\n\n#define pktdma_tisci_tx_channel_config bcdma_tisci_tx_channel_config\n\nstatic int udma_tisci_rx_channel_config(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tconst struct ti_sci_rm_udmap_ops *tisci_ops = tisci_rm->tisci_udmap_ops;\n\tstruct udma_rchan *rchan = uc->rchan;\n\tint fd_ring = k3_ringacc_get_ring_id(uc->rflow->fd_ring);\n\tint rx_ring = k3_ringacc_get_ring_id(uc->rflow->r_ring);\n\tstruct ti_sci_msg_rm_udmap_rx_ch_cfg req_rx = { 0 };\n\tstruct ti_sci_msg_rm_udmap_flow_cfg flow_req = { 0 };\n\tu32 mode, fetch_size;\n\tint ret;\n\n\tif (uc->config.pkt_mode) {\n\t\tmode = TI_SCI_RM_UDMAP_CHAN_TYPE_PKT_PBRR;\n\t\tfetch_size = cppi5_hdesc_calc_size(uc->config.needs_epib,\n\t\t\t\t\t\t   uc->config.psd_size, 0);\n\t} else {\n\t\tmode = TI_SCI_RM_UDMAP_CHAN_TYPE_3RDP_PBRR;\n\t\tfetch_size = sizeof(struct cppi5_desc_hdr_t);\n\t}\n\n\treq_rx.valid_params = TISCI_UDMA_RCHAN_VALID_PARAMS;\n\treq_rx.nav_id = tisci_rm->tisci_dev_id;\n\treq_rx.index = rchan->id;\n\treq_rx.rx_fetch_size =  fetch_size >> 2;\n\treq_rx.rxcq_qnum = rx_ring;\n\treq_rx.rx_chan_type = mode;\n\treq_rx.rx_atype = uc->config.atype;\n\n\tret = tisci_ops->rx_ch_cfg(tisci_rm->tisci, &req_rx);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"rchan%d cfg failed %d\\n\", rchan->id, ret);\n\t\treturn ret;\n\t}\n\n\tflow_req.valid_params =\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_EINFO_PRESENT_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_PSINFO_PRESENT_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_ERROR_HANDLING_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DESC_TYPE_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DEST_QNUM_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_SRC_TAG_HI_SEL_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_SRC_TAG_LO_SEL_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DEST_TAG_HI_SEL_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_DEST_TAG_LO_SEL_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ0_SZ0_QNUM_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ1_QNUM_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ2_QNUM_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_FDQ3_QNUM_VALID;\n\n\tflow_req.nav_id = tisci_rm->tisci_dev_id;\n\tflow_req.flow_index = rchan->id;\n\n\tif (uc->config.needs_epib)\n\t\tflow_req.rx_einfo_present = 1;\n\telse\n\t\tflow_req.rx_einfo_present = 0;\n\tif (uc->config.psd_size)\n\t\tflow_req.rx_psinfo_present = 1;\n\telse\n\t\tflow_req.rx_psinfo_present = 0;\n\tflow_req.rx_error_handling = 1;\n\tflow_req.rx_dest_qnum = rx_ring;\n\tflow_req.rx_src_tag_hi_sel = UDMA_RFLOW_SRCTAG_NONE;\n\tflow_req.rx_src_tag_lo_sel = UDMA_RFLOW_SRCTAG_SRC_TAG;\n\tflow_req.rx_dest_tag_hi_sel = UDMA_RFLOW_DSTTAG_DST_TAG_HI;\n\tflow_req.rx_dest_tag_lo_sel = UDMA_RFLOW_DSTTAG_DST_TAG_LO;\n\tflow_req.rx_fdq0_sz0_qnum = fd_ring;\n\tflow_req.rx_fdq1_qnum = fd_ring;\n\tflow_req.rx_fdq2_qnum = fd_ring;\n\tflow_req.rx_fdq3_qnum = fd_ring;\n\n\tret = tisci_ops->rx_flow_cfg(tisci_rm->tisci, &flow_req);\n\n\tif (ret)\n\t\tdev_err(ud->dev, \"flow%d config failed: %d\\n\", rchan->id, ret);\n\n\treturn 0;\n}\n\nstatic int bcdma_tisci_rx_channel_config(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tconst struct ti_sci_rm_udmap_ops *tisci_ops = tisci_rm->tisci_udmap_ops;\n\tstruct udma_rchan *rchan = uc->rchan;\n\tstruct ti_sci_msg_rm_udmap_rx_ch_cfg req_rx = { 0 };\n\tint ret;\n\n\treq_rx.valid_params = TISCI_BCDMA_RCHAN_VALID_PARAMS;\n\treq_rx.nav_id = tisci_rm->tisci_dev_id;\n\treq_rx.index = rchan->id;\n\n\tret = tisci_ops->rx_ch_cfg(tisci_rm->tisci, &req_rx);\n\tif (ret)\n\t\tdev_err(ud->dev, \"rchan%d cfg failed %d\\n\", rchan->id, ret);\n\n\treturn ret;\n}\n\nstatic int pktdma_tisci_rx_channel_config(struct udma_chan *uc)\n{\n\tstruct udma_dev *ud = uc->ud;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tconst struct ti_sci_rm_udmap_ops *tisci_ops = tisci_rm->tisci_udmap_ops;\n\tstruct ti_sci_msg_rm_udmap_rx_ch_cfg req_rx = { 0 };\n\tstruct ti_sci_msg_rm_udmap_flow_cfg flow_req = { 0 };\n\tint ret;\n\n\treq_rx.valid_params = TISCI_BCDMA_RCHAN_VALID_PARAMS;\n\treq_rx.nav_id = tisci_rm->tisci_dev_id;\n\treq_rx.index = uc->rchan->id;\n\n\tret = tisci_ops->rx_ch_cfg(tisci_rm->tisci, &req_rx);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"rchan%d cfg failed %d\\n\", uc->rchan->id, ret);\n\t\treturn ret;\n\t}\n\n\tflow_req.valid_params =\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_EINFO_PRESENT_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_PSINFO_PRESENT_VALID |\n\t\tTI_SCI_MSG_VALUE_RM_UDMAP_FLOW_ERROR_HANDLING_VALID;\n\n\tflow_req.nav_id = tisci_rm->tisci_dev_id;\n\tflow_req.flow_index = uc->rflow->id;\n\n\tif (uc->config.needs_epib)\n\t\tflow_req.rx_einfo_present = 1;\n\telse\n\t\tflow_req.rx_einfo_present = 0;\n\tif (uc->config.psd_size)\n\t\tflow_req.rx_psinfo_present = 1;\n\telse\n\t\tflow_req.rx_psinfo_present = 0;\n\tflow_req.rx_error_handling = 1;\n\n\tret = tisci_ops->rx_flow_cfg(tisci_rm->tisci, &flow_req);\n\n\tif (ret)\n\t\tdev_err(ud->dev, \"flow%d config failed: %d\\n\", uc->rflow->id,\n\t\t\tret);\n\n\treturn ret;\n}\n\nstatic int udma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tstruct udma_dev *ud = to_udma_dev(chan->device);\n\tconst struct udma_soc_data *soc_data = ud->soc_data;\n\tstruct k3_ring *irq_ring;\n\tu32 irq_udma_idx;\n\tint ret;\n\n\tuc->dma_dev = ud->dev;\n\n\tif (uc->config.pkt_mode || uc->config.dir == DMA_MEM_TO_MEM) {\n\t\tuc->use_dma_pool = true;\n\t\t \n\t\tif (uc->config.dir == DMA_MEM_TO_MEM) {\n\t\t\tuc->config.hdesc_size = cppi5_trdesc_calc_size(\n\t\t\t\t\tsizeof(struct cppi5_tr_type15_t), 2);\n\t\t\tuc->config.pkt_mode = false;\n\t\t}\n\t}\n\n\tif (uc->use_dma_pool) {\n\t\tuc->hdesc_pool = dma_pool_create(uc->name, ud->ddev.dev,\n\t\t\t\t\t\t uc->config.hdesc_size,\n\t\t\t\t\t\t ud->desc_align,\n\t\t\t\t\t\t 0);\n\t\tif (!uc->hdesc_pool) {\n\t\t\tdev_err(ud->ddev.dev,\n\t\t\t\t\"Descriptor pool allocation failed\\n\");\n\t\t\tuc->use_dma_pool = false;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_cleanup;\n\t\t}\n\t}\n\n\t \n\treinit_completion(&uc->teardown_completed);\n\tcomplete_all(&uc->teardown_completed);\n\tuc->state = UDMA_CHAN_IS_IDLE;\n\n\tswitch (uc->config.dir) {\n\tcase DMA_MEM_TO_MEM:\n\t\t \n\t\tdev_dbg(uc->ud->dev, \"%s: chan%d as MEM-to-MEM\\n\", __func__,\n\t\t\tuc->id);\n\n\t\tret = udma_get_chan_pair(uc);\n\t\tif (ret)\n\t\t\tgoto err_cleanup;\n\n\t\tret = udma_alloc_tx_resources(uc);\n\t\tif (ret) {\n\t\t\tudma_put_rchan(uc);\n\t\t\tgoto err_cleanup;\n\t\t}\n\n\t\tret = udma_alloc_rx_resources(uc);\n\t\tif (ret) {\n\t\t\tudma_free_tx_resources(uc);\n\t\t\tgoto err_cleanup;\n\t\t}\n\n\t\tuc->config.src_thread = ud->psil_base + uc->tchan->id;\n\t\tuc->config.dst_thread = (ud->psil_base + uc->rchan->id) |\n\t\t\t\t\tK3_PSIL_DST_THREAD_ID_OFFSET;\n\n\t\tirq_ring = uc->tchan->tc_ring;\n\t\tirq_udma_idx = uc->tchan->id;\n\n\t\tret = udma_tisci_m2m_channel_config(uc);\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\t \n\t\tdev_dbg(uc->ud->dev, \"%s: chan%d as MEM-to-DEV\\n\", __func__,\n\t\t\tuc->id);\n\n\t\tret = udma_alloc_tx_resources(uc);\n\t\tif (ret)\n\t\t\tgoto err_cleanup;\n\n\t\tuc->config.src_thread = ud->psil_base + uc->tchan->id;\n\t\tuc->config.dst_thread = uc->config.remote_thread_id;\n\t\tuc->config.dst_thread |= K3_PSIL_DST_THREAD_ID_OFFSET;\n\n\t\tirq_ring = uc->tchan->tc_ring;\n\t\tirq_udma_idx = uc->tchan->id;\n\n\t\tret = udma_tisci_tx_channel_config(uc);\n\t\tbreak;\n\tcase DMA_DEV_TO_MEM:\n\t\t \n\t\tdev_dbg(uc->ud->dev, \"%s: chan%d as DEV-to-MEM\\n\", __func__,\n\t\t\tuc->id);\n\n\t\tret = udma_alloc_rx_resources(uc);\n\t\tif (ret)\n\t\t\tgoto err_cleanup;\n\n\t\tuc->config.src_thread = uc->config.remote_thread_id;\n\t\tuc->config.dst_thread = (ud->psil_base + uc->rchan->id) |\n\t\t\t\t\tK3_PSIL_DST_THREAD_ID_OFFSET;\n\n\t\tirq_ring = uc->rflow->r_ring;\n\t\tirq_udma_idx = soc_data->oes.udma_rchan + uc->rchan->id;\n\n\t\tret = udma_tisci_rx_channel_config(uc);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tdev_err(uc->ud->dev, \"%s: chan%d invalid direction (%u)\\n\",\n\t\t\t__func__, uc->id, uc->config.dir);\n\t\tret = -EINVAL;\n\t\tgoto err_cleanup;\n\n\t}\n\n\t \n\tif (ret)\n\t\tgoto err_res_free;\n\n\tif (udma_is_chan_running(uc)) {\n\t\tdev_warn(ud->dev, \"chan%d: is running!\\n\", uc->id);\n\t\tudma_reset_chan(uc, false);\n\t\tif (udma_is_chan_running(uc)) {\n\t\t\tdev_err(ud->dev, \"chan%d: won't stop!\\n\", uc->id);\n\t\t\tret = -EBUSY;\n\t\t\tgoto err_res_free;\n\t\t}\n\t}\n\n\t \n\tret = navss_psil_pair(ud, uc->config.src_thread, uc->config.dst_thread);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"PSI-L pairing failed: 0x%04x -> 0x%04x\\n\",\n\t\t\tuc->config.src_thread, uc->config.dst_thread);\n\t\tgoto err_res_free;\n\t}\n\n\tuc->psil_paired = true;\n\n\tuc->irq_num_ring = k3_ringacc_get_ring_irq_num(irq_ring);\n\tif (uc->irq_num_ring <= 0) {\n\t\tdev_err(ud->dev, \"Failed to get ring irq (index: %u)\\n\",\n\t\t\tk3_ringacc_get_ring_id(irq_ring));\n\t\tret = -EINVAL;\n\t\tgoto err_psi_free;\n\t}\n\n\tret = request_irq(uc->irq_num_ring, udma_ring_irq_handler,\n\t\t\t  IRQF_TRIGGER_HIGH, uc->name, uc);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"chan%d: ring irq request failed\\n\", uc->id);\n\t\tgoto err_irq_free;\n\t}\n\n\t \n\tif (is_slave_direction(uc->config.dir) && !uc->config.pkt_mode) {\n\t\tuc->irq_num_udma = msi_get_virq(ud->dev, irq_udma_idx);\n\t\tif (uc->irq_num_udma <= 0) {\n\t\t\tdev_err(ud->dev, \"Failed to get udma irq (index: %u)\\n\",\n\t\t\t\tirq_udma_idx);\n\t\t\tfree_irq(uc->irq_num_ring, uc);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_irq_free;\n\t\t}\n\n\t\tret = request_irq(uc->irq_num_udma, udma_udma_irq_handler, 0,\n\t\t\t\t  uc->name, uc);\n\t\tif (ret) {\n\t\t\tdev_err(ud->dev, \"chan%d: UDMA irq request failed\\n\",\n\t\t\t\tuc->id);\n\t\t\tfree_irq(uc->irq_num_ring, uc);\n\t\t\tgoto err_irq_free;\n\t\t}\n\t} else {\n\t\tuc->irq_num_udma = 0;\n\t}\n\n\tudma_reset_rings(uc);\n\n\treturn 0;\n\nerr_irq_free:\n\tuc->irq_num_ring = 0;\n\tuc->irq_num_udma = 0;\nerr_psi_free:\n\tnavss_psil_unpair(ud, uc->config.src_thread, uc->config.dst_thread);\n\tuc->psil_paired = false;\nerr_res_free:\n\tudma_free_tx_resources(uc);\n\tudma_free_rx_resources(uc);\nerr_cleanup:\n\tudma_reset_uchan(uc);\n\n\tif (uc->use_dma_pool) {\n\t\tdma_pool_destroy(uc->hdesc_pool);\n\t\tuc->use_dma_pool = false;\n\t}\n\n\treturn ret;\n}\n\nstatic int bcdma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tstruct udma_dev *ud = to_udma_dev(chan->device);\n\tconst struct udma_oes_offsets *oes = &ud->soc_data->oes;\n\tu32 irq_udma_idx, irq_ring_idx;\n\tint ret;\n\n\t \n\tuc->config.pkt_mode = false;\n\n\t \n\treinit_completion(&uc->teardown_completed);\n\tcomplete_all(&uc->teardown_completed);\n\tuc->state = UDMA_CHAN_IS_IDLE;\n\n\tswitch (uc->config.dir) {\n\tcase DMA_MEM_TO_MEM:\n\t\t \n\t\tdev_dbg(uc->ud->dev, \"%s: chan%d as MEM-to-MEM\\n\", __func__,\n\t\t\tuc->id);\n\n\t\tret = bcdma_alloc_bchan_resources(uc);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tirq_ring_idx = uc->bchan->id + oes->bcdma_bchan_ring;\n\t\tirq_udma_idx = uc->bchan->id + oes->bcdma_bchan_data;\n\n\t\tret = bcdma_tisci_m2m_channel_config(uc);\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\t \n\t\tdev_dbg(uc->ud->dev, \"%s: chan%d as MEM-to-DEV\\n\", __func__,\n\t\t\tuc->id);\n\n\t\tret = udma_alloc_tx_resources(uc);\n\t\tif (ret) {\n\t\t\tuc->config.remote_thread_id = -1;\n\t\t\treturn ret;\n\t\t}\n\n\t\tuc->config.src_thread = ud->psil_base + uc->tchan->id;\n\t\tuc->config.dst_thread = uc->config.remote_thread_id;\n\t\tuc->config.dst_thread |= K3_PSIL_DST_THREAD_ID_OFFSET;\n\n\t\tirq_ring_idx = uc->tchan->id + oes->bcdma_tchan_ring;\n\t\tirq_udma_idx = uc->tchan->id + oes->bcdma_tchan_data;\n\n\t\tret = bcdma_tisci_tx_channel_config(uc);\n\t\tbreak;\n\tcase DMA_DEV_TO_MEM:\n\t\t \n\t\tdev_dbg(uc->ud->dev, \"%s: chan%d as DEV-to-MEM\\n\", __func__,\n\t\t\tuc->id);\n\n\t\tret = udma_alloc_rx_resources(uc);\n\t\tif (ret) {\n\t\t\tuc->config.remote_thread_id = -1;\n\t\t\treturn ret;\n\t\t}\n\n\t\tuc->config.src_thread = uc->config.remote_thread_id;\n\t\tuc->config.dst_thread = (ud->psil_base + uc->rchan->id) |\n\t\t\t\t\tK3_PSIL_DST_THREAD_ID_OFFSET;\n\n\t\tirq_ring_idx = uc->rchan->id + oes->bcdma_rchan_ring;\n\t\tirq_udma_idx = uc->rchan->id + oes->bcdma_rchan_data;\n\n\t\tret = bcdma_tisci_rx_channel_config(uc);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tdev_err(uc->ud->dev, \"%s: chan%d invalid direction (%u)\\n\",\n\t\t\t__func__, uc->id, uc->config.dir);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (ret)\n\t\tgoto err_res_free;\n\n\tif (udma_is_chan_running(uc)) {\n\t\tdev_warn(ud->dev, \"chan%d: is running!\\n\", uc->id);\n\t\tudma_reset_chan(uc, false);\n\t\tif (udma_is_chan_running(uc)) {\n\t\t\tdev_err(ud->dev, \"chan%d: won't stop!\\n\", uc->id);\n\t\t\tret = -EBUSY;\n\t\t\tgoto err_res_free;\n\t\t}\n\t}\n\n\tuc->dma_dev = dmaengine_get_dma_device(chan);\n\tif (uc->config.dir == DMA_MEM_TO_MEM  && !uc->config.tr_trigger_type) {\n\t\tuc->config.hdesc_size = cppi5_trdesc_calc_size(\n\t\t\t\t\tsizeof(struct cppi5_tr_type15_t), 2);\n\n\t\tuc->hdesc_pool = dma_pool_create(uc->name, ud->ddev.dev,\n\t\t\t\t\t\t uc->config.hdesc_size,\n\t\t\t\t\t\t ud->desc_align,\n\t\t\t\t\t\t 0);\n\t\tif (!uc->hdesc_pool) {\n\t\t\tdev_err(ud->ddev.dev,\n\t\t\t\t\"Descriptor pool allocation failed\\n\");\n\t\t\tuc->use_dma_pool = false;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_res_free;\n\t\t}\n\n\t\tuc->use_dma_pool = true;\n\t} else if (uc->config.dir != DMA_MEM_TO_MEM) {\n\t\t \n\t\tret = navss_psil_pair(ud, uc->config.src_thread,\n\t\t\t\t      uc->config.dst_thread);\n\t\tif (ret) {\n\t\t\tdev_err(ud->dev,\n\t\t\t\t\"PSI-L pairing failed: 0x%04x -> 0x%04x\\n\",\n\t\t\t\tuc->config.src_thread, uc->config.dst_thread);\n\t\t\tgoto err_res_free;\n\t\t}\n\n\t\tuc->psil_paired = true;\n\t}\n\n\tuc->irq_num_ring = msi_get_virq(ud->dev, irq_ring_idx);\n\tif (uc->irq_num_ring <= 0) {\n\t\tdev_err(ud->dev, \"Failed to get ring irq (index: %u)\\n\",\n\t\t\tirq_ring_idx);\n\t\tret = -EINVAL;\n\t\tgoto err_psi_free;\n\t}\n\n\tret = request_irq(uc->irq_num_ring, udma_ring_irq_handler,\n\t\t\t  IRQF_TRIGGER_HIGH, uc->name, uc);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"chan%d: ring irq request failed\\n\", uc->id);\n\t\tgoto err_irq_free;\n\t}\n\n\t \n\tif (is_slave_direction(uc->config.dir)) {\n\t\tuc->irq_num_udma = msi_get_virq(ud->dev, irq_udma_idx);\n\t\tif (uc->irq_num_udma <= 0) {\n\t\t\tdev_err(ud->dev, \"Failed to get bcdma irq (index: %u)\\n\",\n\t\t\t\tirq_udma_idx);\n\t\t\tfree_irq(uc->irq_num_ring, uc);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_irq_free;\n\t\t}\n\n\t\tret = request_irq(uc->irq_num_udma, udma_udma_irq_handler, 0,\n\t\t\t\t  uc->name, uc);\n\t\tif (ret) {\n\t\t\tdev_err(ud->dev, \"chan%d: BCDMA irq request failed\\n\",\n\t\t\t\tuc->id);\n\t\t\tfree_irq(uc->irq_num_ring, uc);\n\t\t\tgoto err_irq_free;\n\t\t}\n\t} else {\n\t\tuc->irq_num_udma = 0;\n\t}\n\n\tudma_reset_rings(uc);\n\n\tINIT_DELAYED_WORK_ONSTACK(&uc->tx_drain.work,\n\t\t\t\t  udma_check_tx_completion);\n\treturn 0;\n\nerr_irq_free:\n\tuc->irq_num_ring = 0;\n\tuc->irq_num_udma = 0;\nerr_psi_free:\n\tif (uc->psil_paired)\n\t\tnavss_psil_unpair(ud, uc->config.src_thread,\n\t\t\t\t  uc->config.dst_thread);\n\tuc->psil_paired = false;\nerr_res_free:\n\tbcdma_free_bchan_resources(uc);\n\tudma_free_tx_resources(uc);\n\tudma_free_rx_resources(uc);\n\n\tudma_reset_uchan(uc);\n\n\tif (uc->use_dma_pool) {\n\t\tdma_pool_destroy(uc->hdesc_pool);\n\t\tuc->use_dma_pool = false;\n\t}\n\n\treturn ret;\n}\n\nstatic int bcdma_router_config(struct dma_chan *chan)\n{\n\tstruct k3_event_route_data *router_data = chan->route_data;\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tu32 trigger_event;\n\n\tif (!uc->bchan)\n\t\treturn -EINVAL;\n\n\tif (uc->config.tr_trigger_type != 1 && uc->config.tr_trigger_type != 2)\n\t\treturn -EINVAL;\n\n\ttrigger_event = uc->ud->soc_data->bcdma_trigger_event_offset;\n\ttrigger_event += (uc->bchan->id * 2) + uc->config.tr_trigger_type - 1;\n\n\treturn router_data->set_event(router_data->priv, trigger_event);\n}\n\nstatic int pktdma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tstruct udma_dev *ud = to_udma_dev(chan->device);\n\tconst struct udma_oes_offsets *oes = &ud->soc_data->oes;\n\tu32 irq_ring_idx;\n\tint ret;\n\n\t \n\treinit_completion(&uc->teardown_completed);\n\tcomplete_all(&uc->teardown_completed);\n\tuc->state = UDMA_CHAN_IS_IDLE;\n\n\tswitch (uc->config.dir) {\n\tcase DMA_MEM_TO_DEV:\n\t\t \n\t\tdev_dbg(uc->ud->dev, \"%s: chan%d as MEM-to-DEV\\n\", __func__,\n\t\t\tuc->id);\n\n\t\tret = udma_alloc_tx_resources(uc);\n\t\tif (ret) {\n\t\t\tuc->config.remote_thread_id = -1;\n\t\t\treturn ret;\n\t\t}\n\n\t\tuc->config.src_thread = ud->psil_base + uc->tchan->id;\n\t\tuc->config.dst_thread = uc->config.remote_thread_id;\n\t\tuc->config.dst_thread |= K3_PSIL_DST_THREAD_ID_OFFSET;\n\n\t\tirq_ring_idx = uc->tchan->tflow_id + oes->pktdma_tchan_flow;\n\n\t\tret = pktdma_tisci_tx_channel_config(uc);\n\t\tbreak;\n\tcase DMA_DEV_TO_MEM:\n\t\t \n\t\tdev_dbg(uc->ud->dev, \"%s: chan%d as DEV-to-MEM\\n\", __func__,\n\t\t\tuc->id);\n\n\t\tret = udma_alloc_rx_resources(uc);\n\t\tif (ret) {\n\t\t\tuc->config.remote_thread_id = -1;\n\t\t\treturn ret;\n\t\t}\n\n\t\tuc->config.src_thread = uc->config.remote_thread_id;\n\t\tuc->config.dst_thread = (ud->psil_base + uc->rchan->id) |\n\t\t\t\t\tK3_PSIL_DST_THREAD_ID_OFFSET;\n\n\t\tirq_ring_idx = uc->rflow->id + oes->pktdma_rchan_flow;\n\n\t\tret = pktdma_tisci_rx_channel_config(uc);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tdev_err(uc->ud->dev, \"%s: chan%d invalid direction (%u)\\n\",\n\t\t\t__func__, uc->id, uc->config.dir);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (ret)\n\t\tgoto err_res_free;\n\n\tif (udma_is_chan_running(uc)) {\n\t\tdev_warn(ud->dev, \"chan%d: is running!\\n\", uc->id);\n\t\tudma_reset_chan(uc, false);\n\t\tif (udma_is_chan_running(uc)) {\n\t\t\tdev_err(ud->dev, \"chan%d: won't stop!\\n\", uc->id);\n\t\t\tret = -EBUSY;\n\t\t\tgoto err_res_free;\n\t\t}\n\t}\n\n\tuc->dma_dev = dmaengine_get_dma_device(chan);\n\tuc->hdesc_pool = dma_pool_create(uc->name, uc->dma_dev,\n\t\t\t\t\t uc->config.hdesc_size, ud->desc_align,\n\t\t\t\t\t 0);\n\tif (!uc->hdesc_pool) {\n\t\tdev_err(ud->ddev.dev,\n\t\t\t\"Descriptor pool allocation failed\\n\");\n\t\tuc->use_dma_pool = false;\n\t\tret = -ENOMEM;\n\t\tgoto err_res_free;\n\t}\n\n\tuc->use_dma_pool = true;\n\n\t \n\tret = navss_psil_pair(ud, uc->config.src_thread, uc->config.dst_thread);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"PSI-L pairing failed: 0x%04x -> 0x%04x\\n\",\n\t\t\tuc->config.src_thread, uc->config.dst_thread);\n\t\tgoto err_res_free;\n\t}\n\n\tuc->psil_paired = true;\n\n\tuc->irq_num_ring = msi_get_virq(ud->dev, irq_ring_idx);\n\tif (uc->irq_num_ring <= 0) {\n\t\tdev_err(ud->dev, \"Failed to get ring irq (index: %u)\\n\",\n\t\t\tirq_ring_idx);\n\t\tret = -EINVAL;\n\t\tgoto err_psi_free;\n\t}\n\n\tret = request_irq(uc->irq_num_ring, udma_ring_irq_handler,\n\t\t\t  IRQF_TRIGGER_HIGH, uc->name, uc);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"chan%d: ring irq request failed\\n\", uc->id);\n\t\tgoto err_irq_free;\n\t}\n\n\tuc->irq_num_udma = 0;\n\n\tudma_reset_rings(uc);\n\n\tINIT_DELAYED_WORK_ONSTACK(&uc->tx_drain.work,\n\t\t\t\t  udma_check_tx_completion);\n\n\tif (uc->tchan)\n\t\tdev_dbg(ud->dev,\n\t\t\t\"chan%d: tchan%d, tflow%d, Remote thread: 0x%04x\\n\",\n\t\t\tuc->id, uc->tchan->id, uc->tchan->tflow_id,\n\t\t\tuc->config.remote_thread_id);\n\telse if (uc->rchan)\n\t\tdev_dbg(ud->dev,\n\t\t\t\"chan%d: rchan%d, rflow%d, Remote thread: 0x%04x\\n\",\n\t\t\tuc->id, uc->rchan->id, uc->rflow->id,\n\t\t\tuc->config.remote_thread_id);\n\treturn 0;\n\nerr_irq_free:\n\tuc->irq_num_ring = 0;\nerr_psi_free:\n\tnavss_psil_unpair(ud, uc->config.src_thread, uc->config.dst_thread);\n\tuc->psil_paired = false;\nerr_res_free:\n\tudma_free_tx_resources(uc);\n\tudma_free_rx_resources(uc);\n\n\tudma_reset_uchan(uc);\n\n\tdma_pool_destroy(uc->hdesc_pool);\n\tuc->use_dma_pool = false;\n\n\treturn ret;\n}\n\nstatic int udma_slave_config(struct dma_chan *chan,\n\t\t\t     struct dma_slave_config *cfg)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\n\tmemcpy(&uc->cfg, cfg, sizeof(uc->cfg));\n\n\treturn 0;\n}\n\nstatic struct udma_desc *udma_alloc_tr_desc(struct udma_chan *uc,\n\t\t\t\t\t    size_t tr_size, int tr_count,\n\t\t\t\t\t    enum dma_transfer_direction dir)\n{\n\tstruct udma_hwdesc *hwdesc;\n\tstruct cppi5_desc_hdr_t *tr_desc;\n\tstruct udma_desc *d;\n\tu32 reload_count = 0;\n\tu32 ring_id;\n\n\tswitch (tr_size) {\n\tcase 16:\n\tcase 32:\n\tcase 64:\n\tcase 128:\n\t\tbreak;\n\tdefault:\n\t\tdev_err(uc->ud->dev, \"Unsupported TR size of %zu\\n\", tr_size);\n\t\treturn NULL;\n\t}\n\n\t \n\td = kzalloc(sizeof(*d) + sizeof(d->hwdesc[0]), GFP_NOWAIT);\n\tif (!d)\n\t\treturn NULL;\n\n\td->sglen = tr_count;\n\n\td->hwdesc_count = 1;\n\thwdesc = &d->hwdesc[0];\n\n\t \n\tif (uc->use_dma_pool) {\n\t\thwdesc->cppi5_desc_size = uc->config.hdesc_size;\n\t\thwdesc->cppi5_desc_vaddr = dma_pool_zalloc(uc->hdesc_pool,\n\t\t\t\t\t\tGFP_NOWAIT,\n\t\t\t\t\t\t&hwdesc->cppi5_desc_paddr);\n\t} else {\n\t\thwdesc->cppi5_desc_size = cppi5_trdesc_calc_size(tr_size,\n\t\t\t\t\t\t\t\t tr_count);\n\t\thwdesc->cppi5_desc_size = ALIGN(hwdesc->cppi5_desc_size,\n\t\t\t\t\t\tuc->ud->desc_align);\n\t\thwdesc->cppi5_desc_vaddr = dma_alloc_coherent(uc->ud->dev,\n\t\t\t\t\t\thwdesc->cppi5_desc_size,\n\t\t\t\t\t\t&hwdesc->cppi5_desc_paddr,\n\t\t\t\t\t\tGFP_NOWAIT);\n\t}\n\n\tif (!hwdesc->cppi5_desc_vaddr) {\n\t\tkfree(d);\n\t\treturn NULL;\n\t}\n\n\t \n\thwdesc->tr_req_base = hwdesc->cppi5_desc_vaddr + tr_size;\n\t \n\thwdesc->tr_resp_base = hwdesc->tr_req_base + tr_size * tr_count;\n\n\ttr_desc = hwdesc->cppi5_desc_vaddr;\n\n\tif (uc->cyclic)\n\t\treload_count = CPPI5_INFO0_TRDESC_RLDCNT_INFINITE;\n\n\tif (dir == DMA_DEV_TO_MEM)\n\t\tring_id = k3_ringacc_get_ring_id(uc->rflow->r_ring);\n\telse\n\t\tring_id = k3_ringacc_get_ring_id(uc->tchan->tc_ring);\n\n\tcppi5_trdesc_init(tr_desc, tr_count, tr_size, 0, reload_count);\n\tcppi5_desc_set_pktids(tr_desc, uc->id,\n\t\t\t      CPPI5_INFO1_DESC_FLOWID_DEFAULT);\n\tcppi5_desc_set_retpolicy(tr_desc, 0, ring_id);\n\n\treturn d;\n}\n\n \nstatic int udma_get_tr_counters(size_t len, unsigned long align_to,\n\t\t\t\tu16 *tr0_cnt0, u16 *tr0_cnt1, u16 *tr1_cnt0)\n{\n\tif (len < SZ_64K) {\n\t\t*tr0_cnt0 = len;\n\t\t*tr0_cnt1 = 1;\n\n\t\treturn 1;\n\t}\n\n\tif (align_to > 3)\n\t\talign_to = 3;\n\nrealign:\n\t*tr0_cnt0 = SZ_64K - BIT(align_to);\n\tif (len / *tr0_cnt0 >= SZ_64K) {\n\t\tif (align_to) {\n\t\t\talign_to--;\n\t\t\tgoto realign;\n\t\t}\n\t\treturn -EINVAL;\n\t}\n\n\t*tr0_cnt1 = len / *tr0_cnt0;\n\t*tr1_cnt0 = len % *tr0_cnt0;\n\n\treturn 2;\n}\n\nstatic struct udma_desc *\nudma_prep_slave_sg_tr(struct udma_chan *uc, struct scatterlist *sgl,\n\t\t      unsigned int sglen, enum dma_transfer_direction dir,\n\t\t      unsigned long tx_flags, void *context)\n{\n\tstruct scatterlist *sgent;\n\tstruct udma_desc *d;\n\tstruct cppi5_tr_type1_t *tr_req = NULL;\n\tu16 tr0_cnt0, tr0_cnt1, tr1_cnt0;\n\tunsigned int i;\n\tsize_t tr_size;\n\tint num_tr = 0;\n\tint tr_idx = 0;\n\tu64 asel;\n\n\t \n\tfor_each_sg(sgl, sgent, sglen, i) {\n\t\tif (sg_dma_len(sgent) < SZ_64K)\n\t\t\tnum_tr++;\n\t\telse\n\t\t\tnum_tr += 2;\n\t}\n\n\t \n\ttr_size = sizeof(struct cppi5_tr_type1_t);\n\td = udma_alloc_tr_desc(uc, tr_size, num_tr, dir);\n\tif (!d)\n\t\treturn NULL;\n\n\td->sglen = sglen;\n\n\tif (uc->ud->match_data->type == DMA_TYPE_UDMA)\n\t\tasel = 0;\n\telse\n\t\tasel = (u64)uc->config.asel << K3_ADDRESS_ASEL_SHIFT;\n\n\ttr_req = d->hwdesc[0].tr_req_base;\n\tfor_each_sg(sgl, sgent, sglen, i) {\n\t\tdma_addr_t sg_addr = sg_dma_address(sgent);\n\n\t\tnum_tr = udma_get_tr_counters(sg_dma_len(sgent), __ffs(sg_addr),\n\t\t\t\t\t      &tr0_cnt0, &tr0_cnt1, &tr1_cnt0);\n\t\tif (num_tr < 0) {\n\t\t\tdev_err(uc->ud->dev, \"size %u is not supported\\n\",\n\t\t\t\tsg_dma_len(sgent));\n\t\t\tudma_free_hwdesc(uc, d);\n\t\t\tkfree(d);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tcppi5_tr_init(&tr_req[tr_idx].flags, CPPI5_TR_TYPE1, false,\n\t\t\t      false, CPPI5_TR_EVENT_SIZE_COMPLETION, 0);\n\t\tcppi5_tr_csf_set(&tr_req[tr_idx].flags, CPPI5_TR_CSF_SUPR_EVT);\n\n\t\tsg_addr |= asel;\n\t\ttr_req[tr_idx].addr = sg_addr;\n\t\ttr_req[tr_idx].icnt0 = tr0_cnt0;\n\t\ttr_req[tr_idx].icnt1 = tr0_cnt1;\n\t\ttr_req[tr_idx].dim1 = tr0_cnt0;\n\t\ttr_idx++;\n\n\t\tif (num_tr == 2) {\n\t\t\tcppi5_tr_init(&tr_req[tr_idx].flags, CPPI5_TR_TYPE1,\n\t\t\t\t      false, false,\n\t\t\t\t      CPPI5_TR_EVENT_SIZE_COMPLETION, 0);\n\t\t\tcppi5_tr_csf_set(&tr_req[tr_idx].flags,\n\t\t\t\t\t CPPI5_TR_CSF_SUPR_EVT);\n\n\t\t\ttr_req[tr_idx].addr = sg_addr + tr0_cnt1 * tr0_cnt0;\n\t\t\ttr_req[tr_idx].icnt0 = tr1_cnt0;\n\t\t\ttr_req[tr_idx].icnt1 = 1;\n\t\t\ttr_req[tr_idx].dim1 = tr1_cnt0;\n\t\t\ttr_idx++;\n\t\t}\n\n\t\td->residue += sg_dma_len(sgent);\n\t}\n\n\tcppi5_tr_csf_set(&tr_req[tr_idx - 1].flags,\n\t\t\t CPPI5_TR_CSF_SUPR_EVT | CPPI5_TR_CSF_EOP);\n\n\treturn d;\n}\n\nstatic struct udma_desc *\nudma_prep_slave_sg_triggered_tr(struct udma_chan *uc, struct scatterlist *sgl,\n\t\t\t\tunsigned int sglen,\n\t\t\t\tenum dma_transfer_direction dir,\n\t\t\t\tunsigned long tx_flags, void *context)\n{\n\tstruct scatterlist *sgent;\n\tstruct cppi5_tr_type15_t *tr_req = NULL;\n\tenum dma_slave_buswidth dev_width;\n\tu32 csf = CPPI5_TR_CSF_SUPR_EVT;\n\tu16 tr_cnt0, tr_cnt1;\n\tdma_addr_t dev_addr;\n\tstruct udma_desc *d;\n\tunsigned int i;\n\tsize_t tr_size, sg_len;\n\tint num_tr = 0;\n\tint tr_idx = 0;\n\tu32 burst, trigger_size, port_window;\n\tu64 asel;\n\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\tdev_addr = uc->cfg.src_addr;\n\t\tdev_width = uc->cfg.src_addr_width;\n\t\tburst = uc->cfg.src_maxburst;\n\t\tport_window = uc->cfg.src_port_window_size;\n\t} else if (dir == DMA_MEM_TO_DEV) {\n\t\tdev_addr = uc->cfg.dst_addr;\n\t\tdev_width = uc->cfg.dst_addr_width;\n\t\tburst = uc->cfg.dst_maxburst;\n\t\tport_window = uc->cfg.dst_port_window_size;\n\t} else {\n\t\tdev_err(uc->ud->dev, \"%s: bad direction?\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tif (!burst)\n\t\tburst = 1;\n\n\tif (port_window) {\n\t\tif (port_window != burst) {\n\t\t\tdev_err(uc->ud->dev,\n\t\t\t\t\"The burst must be equal to port_window\\n\");\n\t\t\treturn NULL;\n\t\t}\n\n\t\ttr_cnt0 = dev_width * port_window;\n\t\ttr_cnt1 = 1;\n\t} else {\n\t\ttr_cnt0 = dev_width;\n\t\ttr_cnt1 = burst;\n\t}\n\ttrigger_size = tr_cnt0 * tr_cnt1;\n\n\t \n\tfor_each_sg(sgl, sgent, sglen, i) {\n\t\tsg_len = sg_dma_len(sgent);\n\n\t\tif (sg_len % trigger_size) {\n\t\t\tdev_err(uc->ud->dev,\n\t\t\t\t\"Not aligned SG entry (%zu for %u)\\n\", sg_len,\n\t\t\t\ttrigger_size);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (sg_len / trigger_size < SZ_64K)\n\t\t\tnum_tr++;\n\t\telse\n\t\t\tnum_tr += 2;\n\t}\n\n\t \n\ttr_size = sizeof(struct cppi5_tr_type15_t);\n\td = udma_alloc_tr_desc(uc, tr_size, num_tr, dir);\n\tif (!d)\n\t\treturn NULL;\n\n\td->sglen = sglen;\n\n\tif (uc->ud->match_data->type == DMA_TYPE_UDMA) {\n\t\tasel = 0;\n\t\tcsf |= CPPI5_TR_CSF_EOL_ICNT0;\n\t} else {\n\t\tasel = (u64)uc->config.asel << K3_ADDRESS_ASEL_SHIFT;\n\t\tdev_addr |= asel;\n\t}\n\n\ttr_req = d->hwdesc[0].tr_req_base;\n\tfor_each_sg(sgl, sgent, sglen, i) {\n\t\tu16 tr0_cnt2, tr0_cnt3, tr1_cnt2;\n\t\tdma_addr_t sg_addr = sg_dma_address(sgent);\n\n\t\tsg_len = sg_dma_len(sgent);\n\t\tnum_tr = udma_get_tr_counters(sg_len / trigger_size, 0,\n\t\t\t\t\t      &tr0_cnt2, &tr0_cnt3, &tr1_cnt2);\n\t\tif (num_tr < 0) {\n\t\t\tdev_err(uc->ud->dev, \"size %zu is not supported\\n\",\n\t\t\t\tsg_len);\n\t\t\tudma_free_hwdesc(uc, d);\n\t\t\tkfree(d);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tcppi5_tr_init(&tr_req[tr_idx].flags, CPPI5_TR_TYPE15, false,\n\t\t\t      true, CPPI5_TR_EVENT_SIZE_COMPLETION, 0);\n\t\tcppi5_tr_csf_set(&tr_req[tr_idx].flags, csf);\n\t\tcppi5_tr_set_trigger(&tr_req[tr_idx].flags,\n\t\t\t\t     uc->config.tr_trigger_type,\n\t\t\t\t     CPPI5_TR_TRIGGER_TYPE_ICNT2_DEC, 0, 0);\n\n\t\tsg_addr |= asel;\n\t\tif (dir == DMA_DEV_TO_MEM) {\n\t\t\ttr_req[tr_idx].addr = dev_addr;\n\t\t\ttr_req[tr_idx].icnt0 = tr_cnt0;\n\t\t\ttr_req[tr_idx].icnt1 = tr_cnt1;\n\t\t\ttr_req[tr_idx].icnt2 = tr0_cnt2;\n\t\t\ttr_req[tr_idx].icnt3 = tr0_cnt3;\n\t\t\ttr_req[tr_idx].dim1 = (-1) * tr_cnt0;\n\n\t\t\ttr_req[tr_idx].daddr = sg_addr;\n\t\t\ttr_req[tr_idx].dicnt0 = tr_cnt0;\n\t\t\ttr_req[tr_idx].dicnt1 = tr_cnt1;\n\t\t\ttr_req[tr_idx].dicnt2 = tr0_cnt2;\n\t\t\ttr_req[tr_idx].dicnt3 = tr0_cnt3;\n\t\t\ttr_req[tr_idx].ddim1 = tr_cnt0;\n\t\t\ttr_req[tr_idx].ddim2 = trigger_size;\n\t\t\ttr_req[tr_idx].ddim3 = trigger_size * tr0_cnt2;\n\t\t} else {\n\t\t\ttr_req[tr_idx].addr = sg_addr;\n\t\t\ttr_req[tr_idx].icnt0 = tr_cnt0;\n\t\t\ttr_req[tr_idx].icnt1 = tr_cnt1;\n\t\t\ttr_req[tr_idx].icnt2 = tr0_cnt2;\n\t\t\ttr_req[tr_idx].icnt3 = tr0_cnt3;\n\t\t\ttr_req[tr_idx].dim1 = tr_cnt0;\n\t\t\ttr_req[tr_idx].dim2 = trigger_size;\n\t\t\ttr_req[tr_idx].dim3 = trigger_size * tr0_cnt2;\n\n\t\t\ttr_req[tr_idx].daddr = dev_addr;\n\t\t\ttr_req[tr_idx].dicnt0 = tr_cnt0;\n\t\t\ttr_req[tr_idx].dicnt1 = tr_cnt1;\n\t\t\ttr_req[tr_idx].dicnt2 = tr0_cnt2;\n\t\t\ttr_req[tr_idx].dicnt3 = tr0_cnt3;\n\t\t\ttr_req[tr_idx].ddim1 = (-1) * tr_cnt0;\n\t\t}\n\n\t\ttr_idx++;\n\n\t\tif (num_tr == 2) {\n\t\t\tcppi5_tr_init(&tr_req[tr_idx].flags, CPPI5_TR_TYPE15,\n\t\t\t\t      false, true,\n\t\t\t\t      CPPI5_TR_EVENT_SIZE_COMPLETION, 0);\n\t\t\tcppi5_tr_csf_set(&tr_req[tr_idx].flags, csf);\n\t\t\tcppi5_tr_set_trigger(&tr_req[tr_idx].flags,\n\t\t\t\t\t     uc->config.tr_trigger_type,\n\t\t\t\t\t     CPPI5_TR_TRIGGER_TYPE_ICNT2_DEC,\n\t\t\t\t\t     0, 0);\n\n\t\t\tsg_addr += trigger_size * tr0_cnt2 * tr0_cnt3;\n\t\t\tif (dir == DMA_DEV_TO_MEM) {\n\t\t\t\ttr_req[tr_idx].addr = dev_addr;\n\t\t\t\ttr_req[tr_idx].icnt0 = tr_cnt0;\n\t\t\t\ttr_req[tr_idx].icnt1 = tr_cnt1;\n\t\t\t\ttr_req[tr_idx].icnt2 = tr1_cnt2;\n\t\t\t\ttr_req[tr_idx].icnt3 = 1;\n\t\t\t\ttr_req[tr_idx].dim1 = (-1) * tr_cnt0;\n\n\t\t\t\ttr_req[tr_idx].daddr = sg_addr;\n\t\t\t\ttr_req[tr_idx].dicnt0 = tr_cnt0;\n\t\t\t\ttr_req[tr_idx].dicnt1 = tr_cnt1;\n\t\t\t\ttr_req[tr_idx].dicnt2 = tr1_cnt2;\n\t\t\t\ttr_req[tr_idx].dicnt3 = 1;\n\t\t\t\ttr_req[tr_idx].ddim1 = tr_cnt0;\n\t\t\t\ttr_req[tr_idx].ddim2 = trigger_size;\n\t\t\t} else {\n\t\t\t\ttr_req[tr_idx].addr = sg_addr;\n\t\t\t\ttr_req[tr_idx].icnt0 = tr_cnt0;\n\t\t\t\ttr_req[tr_idx].icnt1 = tr_cnt1;\n\t\t\t\ttr_req[tr_idx].icnt2 = tr1_cnt2;\n\t\t\t\ttr_req[tr_idx].icnt3 = 1;\n\t\t\t\ttr_req[tr_idx].dim1 = tr_cnt0;\n\t\t\t\ttr_req[tr_idx].dim2 = trigger_size;\n\n\t\t\t\ttr_req[tr_idx].daddr = dev_addr;\n\t\t\t\ttr_req[tr_idx].dicnt0 = tr_cnt0;\n\t\t\t\ttr_req[tr_idx].dicnt1 = tr_cnt1;\n\t\t\t\ttr_req[tr_idx].dicnt2 = tr1_cnt2;\n\t\t\t\ttr_req[tr_idx].dicnt3 = 1;\n\t\t\t\ttr_req[tr_idx].ddim1 = (-1) * tr_cnt0;\n\t\t\t}\n\t\t\ttr_idx++;\n\t\t}\n\n\t\td->residue += sg_len;\n\t}\n\n\tcppi5_tr_csf_set(&tr_req[tr_idx - 1].flags, csf | CPPI5_TR_CSF_EOP);\n\n\treturn d;\n}\n\nstatic int udma_configure_statictr(struct udma_chan *uc, struct udma_desc *d,\n\t\t\t\t   enum dma_slave_buswidth dev_width,\n\t\t\t\t   u16 elcnt)\n{\n\tif (uc->config.ep_type != PSIL_EP_PDMA_XY)\n\t\treturn 0;\n\n\t \n\tswitch (dev_width) {\n\tcase DMA_SLAVE_BUSWIDTH_1_BYTE:\n\t\td->static_tr.elsize = 0;\n\t\tbreak;\n\tcase DMA_SLAVE_BUSWIDTH_2_BYTES:\n\t\td->static_tr.elsize = 1;\n\t\tbreak;\n\tcase DMA_SLAVE_BUSWIDTH_3_BYTES:\n\t\td->static_tr.elsize = 2;\n\t\tbreak;\n\tcase DMA_SLAVE_BUSWIDTH_4_BYTES:\n\t\td->static_tr.elsize = 3;\n\t\tbreak;\n\tcase DMA_SLAVE_BUSWIDTH_8_BYTES:\n\t\td->static_tr.elsize = 4;\n\t\tbreak;\n\tdefault:  \n\t\treturn -EINVAL;\n\t}\n\n\td->static_tr.elcnt = elcnt;\n\n\t \n\tif (uc->config.pkt_mode || !uc->cyclic) {\n\t\tunsigned int div = dev_width * elcnt;\n\n\t\tif (uc->cyclic)\n\t\t\td->static_tr.bstcnt = d->residue / d->sglen / div;\n\t\telse\n\t\t\td->static_tr.bstcnt = d->residue / div;\n\n\t\tif (uc->config.dir == DMA_DEV_TO_MEM &&\n\t\t    d->static_tr.bstcnt > uc->ud->match_data->statictr_z_mask)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\td->static_tr.bstcnt = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic struct udma_desc *\nudma_prep_slave_sg_pkt(struct udma_chan *uc, struct scatterlist *sgl,\n\t\t       unsigned int sglen, enum dma_transfer_direction dir,\n\t\t       unsigned long tx_flags, void *context)\n{\n\tstruct scatterlist *sgent;\n\tstruct cppi5_host_desc_t *h_desc = NULL;\n\tstruct udma_desc *d;\n\tu32 ring_id;\n\tunsigned int i;\n\tu64 asel;\n\n\td = kzalloc(struct_size(d, hwdesc, sglen), GFP_NOWAIT);\n\tif (!d)\n\t\treturn NULL;\n\n\td->sglen = sglen;\n\td->hwdesc_count = sglen;\n\n\tif (dir == DMA_DEV_TO_MEM)\n\t\tring_id = k3_ringacc_get_ring_id(uc->rflow->r_ring);\n\telse\n\t\tring_id = k3_ringacc_get_ring_id(uc->tchan->tc_ring);\n\n\tif (uc->ud->match_data->type == DMA_TYPE_UDMA)\n\t\tasel = 0;\n\telse\n\t\tasel = (u64)uc->config.asel << K3_ADDRESS_ASEL_SHIFT;\n\n\tfor_each_sg(sgl, sgent, sglen, i) {\n\t\tstruct udma_hwdesc *hwdesc = &d->hwdesc[i];\n\t\tdma_addr_t sg_addr = sg_dma_address(sgent);\n\t\tstruct cppi5_host_desc_t *desc;\n\t\tsize_t sg_len = sg_dma_len(sgent);\n\n\t\thwdesc->cppi5_desc_vaddr = dma_pool_zalloc(uc->hdesc_pool,\n\t\t\t\t\t\tGFP_NOWAIT,\n\t\t\t\t\t\t&hwdesc->cppi5_desc_paddr);\n\t\tif (!hwdesc->cppi5_desc_vaddr) {\n\t\t\tdev_err(uc->ud->dev,\n\t\t\t\t\"descriptor%d allocation failed\\n\", i);\n\n\t\t\tudma_free_hwdesc(uc, d);\n\t\t\tkfree(d);\n\t\t\treturn NULL;\n\t\t}\n\n\t\td->residue += sg_len;\n\t\thwdesc->cppi5_desc_size = uc->config.hdesc_size;\n\t\tdesc = hwdesc->cppi5_desc_vaddr;\n\n\t\tif (i == 0) {\n\t\t\tcppi5_hdesc_init(desc, 0, 0);\n\t\t\t \n\t\t\tcppi5_desc_set_pktids(&desc->hdr, uc->id,\n\t\t\t\t\t      CPPI5_INFO1_DESC_FLOWID_DEFAULT);\n\t\t\tcppi5_desc_set_retpolicy(&desc->hdr, 0, ring_id);\n\t\t} else {\n\t\t\tcppi5_hdesc_reset_hbdesc(desc);\n\t\t\tcppi5_desc_set_retpolicy(&desc->hdr, 0, 0xffff);\n\t\t}\n\n\t\t \n\t\tsg_addr |= asel;\n\t\tcppi5_hdesc_attach_buf(desc, sg_addr, sg_len, sg_addr, sg_len);\n\n\t\t \n\t\tif (h_desc)\n\t\t\tcppi5_hdesc_link_hbdesc(h_desc,\n\t\t\t\t\t\thwdesc->cppi5_desc_paddr | asel);\n\n\t\tif (uc->ud->match_data->type == DMA_TYPE_PKTDMA ||\n\t\t    dir == DMA_MEM_TO_DEV)\n\t\t\th_desc = desc;\n\t}\n\n\tif (d->residue >= SZ_4M) {\n\t\tdev_err(uc->ud->dev,\n\t\t\t\"%s: Transfer size %u is over the supported 4M range\\n\",\n\t\t\t__func__, d->residue);\n\t\tudma_free_hwdesc(uc, d);\n\t\tkfree(d);\n\t\treturn NULL;\n\t}\n\n\th_desc = d->hwdesc[0].cppi5_desc_vaddr;\n\tcppi5_hdesc_set_pktlen(h_desc, d->residue);\n\n\treturn d;\n}\n\nstatic int udma_attach_metadata(struct dma_async_tx_descriptor *desc,\n\t\t\t\tvoid *data, size_t len)\n{\n\tstruct udma_desc *d = to_udma_desc(desc);\n\tstruct udma_chan *uc = to_udma_chan(desc->chan);\n\tstruct cppi5_host_desc_t *h_desc;\n\tu32 psd_size = len;\n\tu32 flags = 0;\n\n\tif (!uc->config.pkt_mode || !uc->config.metadata_size)\n\t\treturn -ENOTSUPP;\n\n\tif (!data || len > uc->config.metadata_size)\n\t\treturn -EINVAL;\n\n\tif (uc->config.needs_epib && len < CPPI5_INFO0_HDESC_EPIB_SIZE)\n\t\treturn -EINVAL;\n\n\th_desc = d->hwdesc[0].cppi5_desc_vaddr;\n\tif (d->dir == DMA_MEM_TO_DEV)\n\t\tmemcpy(h_desc->epib, data, len);\n\n\tif (uc->config.needs_epib)\n\t\tpsd_size -= CPPI5_INFO0_HDESC_EPIB_SIZE;\n\n\td->metadata = data;\n\td->metadata_size = len;\n\tif (uc->config.needs_epib)\n\t\tflags |= CPPI5_INFO0_HDESC_EPIB_PRESENT;\n\n\tcppi5_hdesc_update_flags(h_desc, flags);\n\tcppi5_hdesc_update_psdata_size(h_desc, psd_size);\n\n\treturn 0;\n}\n\nstatic void *udma_get_metadata_ptr(struct dma_async_tx_descriptor *desc,\n\t\t\t\t   size_t *payload_len, size_t *max_len)\n{\n\tstruct udma_desc *d = to_udma_desc(desc);\n\tstruct udma_chan *uc = to_udma_chan(desc->chan);\n\tstruct cppi5_host_desc_t *h_desc;\n\n\tif (!uc->config.pkt_mode || !uc->config.metadata_size)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\th_desc = d->hwdesc[0].cppi5_desc_vaddr;\n\n\t*max_len = uc->config.metadata_size;\n\n\t*payload_len = cppi5_hdesc_epib_present(&h_desc->hdr) ?\n\t\t       CPPI5_INFO0_HDESC_EPIB_SIZE : 0;\n\t*payload_len += cppi5_hdesc_get_psdata_size(h_desc);\n\n\treturn h_desc->epib;\n}\n\nstatic int udma_set_metadata_len(struct dma_async_tx_descriptor *desc,\n\t\t\t\t size_t payload_len)\n{\n\tstruct udma_desc *d = to_udma_desc(desc);\n\tstruct udma_chan *uc = to_udma_chan(desc->chan);\n\tstruct cppi5_host_desc_t *h_desc;\n\tu32 psd_size = payload_len;\n\tu32 flags = 0;\n\n\tif (!uc->config.pkt_mode || !uc->config.metadata_size)\n\t\treturn -ENOTSUPP;\n\n\tif (payload_len > uc->config.metadata_size)\n\t\treturn -EINVAL;\n\n\tif (uc->config.needs_epib && payload_len < CPPI5_INFO0_HDESC_EPIB_SIZE)\n\t\treturn -EINVAL;\n\n\th_desc = d->hwdesc[0].cppi5_desc_vaddr;\n\n\tif (uc->config.needs_epib) {\n\t\tpsd_size -= CPPI5_INFO0_HDESC_EPIB_SIZE;\n\t\tflags |= CPPI5_INFO0_HDESC_EPIB_PRESENT;\n\t}\n\n\tcppi5_hdesc_update_flags(h_desc, flags);\n\tcppi5_hdesc_update_psdata_size(h_desc, psd_size);\n\n\treturn 0;\n}\n\nstatic struct dma_descriptor_metadata_ops metadata_ops = {\n\t.attach = udma_attach_metadata,\n\t.get_ptr = udma_get_metadata_ptr,\n\t.set_len = udma_set_metadata_len,\n};\n\nstatic struct dma_async_tx_descriptor *\nudma_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\n\t\t   unsigned int sglen, enum dma_transfer_direction dir,\n\t\t   unsigned long tx_flags, void *context)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tenum dma_slave_buswidth dev_width;\n\tstruct udma_desc *d;\n\tu32 burst;\n\n\tif (dir != uc->config.dir &&\n\t    (uc->config.dir == DMA_MEM_TO_MEM && !uc->config.tr_trigger_type)) {\n\t\tdev_err(chan->device->dev,\n\t\t\t\"%s: chan%d is for %s, not supporting %s\\n\",\n\t\t\t__func__, uc->id,\n\t\t\tdmaengine_get_direction_text(uc->config.dir),\n\t\t\tdmaengine_get_direction_text(dir));\n\t\treturn NULL;\n\t}\n\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\tdev_width = uc->cfg.src_addr_width;\n\t\tburst = uc->cfg.src_maxburst;\n\t} else if (dir == DMA_MEM_TO_DEV) {\n\t\tdev_width = uc->cfg.dst_addr_width;\n\t\tburst = uc->cfg.dst_maxburst;\n\t} else {\n\t\tdev_err(chan->device->dev, \"%s: bad direction?\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tif (!burst)\n\t\tburst = 1;\n\n\tuc->config.tx_flags = tx_flags;\n\n\tif (uc->config.pkt_mode)\n\t\td = udma_prep_slave_sg_pkt(uc, sgl, sglen, dir, tx_flags,\n\t\t\t\t\t   context);\n\telse if (is_slave_direction(uc->config.dir))\n\t\td = udma_prep_slave_sg_tr(uc, sgl, sglen, dir, tx_flags,\n\t\t\t\t\t  context);\n\telse\n\t\td = udma_prep_slave_sg_triggered_tr(uc, sgl, sglen, dir,\n\t\t\t\t\t\t    tx_flags, context);\n\n\tif (!d)\n\t\treturn NULL;\n\n\td->dir = dir;\n\td->desc_idx = 0;\n\td->tr_idx = 0;\n\n\t \n\tif (udma_configure_statictr(uc, d, dev_width, burst)) {\n\t\tdev_err(uc->ud->dev,\n\t\t\t\"%s: StaticTR Z is limited to maximum 4095 (%u)\\n\",\n\t\t\t__func__, d->static_tr.bstcnt);\n\n\t\tudma_free_hwdesc(uc, d);\n\t\tkfree(d);\n\t\treturn NULL;\n\t}\n\n\tif (uc->config.metadata_size)\n\t\td->vd.tx.metadata_ops = &metadata_ops;\n\n\treturn vchan_tx_prep(&uc->vc, &d->vd, tx_flags);\n}\n\nstatic struct udma_desc *\nudma_prep_dma_cyclic_tr(struct udma_chan *uc, dma_addr_t buf_addr,\n\t\t\tsize_t buf_len, size_t period_len,\n\t\t\tenum dma_transfer_direction dir, unsigned long flags)\n{\n\tstruct udma_desc *d;\n\tsize_t tr_size, period_addr;\n\tstruct cppi5_tr_type1_t *tr_req;\n\tunsigned int periods = buf_len / period_len;\n\tu16 tr0_cnt0, tr0_cnt1, tr1_cnt0;\n\tunsigned int i;\n\tint num_tr;\n\n\tnum_tr = udma_get_tr_counters(period_len, __ffs(buf_addr), &tr0_cnt0,\n\t\t\t\t      &tr0_cnt1, &tr1_cnt0);\n\tif (num_tr < 0) {\n\t\tdev_err(uc->ud->dev, \"size %zu is not supported\\n\",\n\t\t\tperiod_len);\n\t\treturn NULL;\n\t}\n\n\t \n\ttr_size = sizeof(struct cppi5_tr_type1_t);\n\td = udma_alloc_tr_desc(uc, tr_size, periods * num_tr, dir);\n\tif (!d)\n\t\treturn NULL;\n\n\ttr_req = d->hwdesc[0].tr_req_base;\n\tif (uc->ud->match_data->type == DMA_TYPE_UDMA)\n\t\tperiod_addr = buf_addr;\n\telse\n\t\tperiod_addr = buf_addr |\n\t\t\t((u64)uc->config.asel << K3_ADDRESS_ASEL_SHIFT);\n\n\tfor (i = 0; i < periods; i++) {\n\t\tint tr_idx = i * num_tr;\n\n\t\tcppi5_tr_init(&tr_req[tr_idx].flags, CPPI5_TR_TYPE1, false,\n\t\t\t      false, CPPI5_TR_EVENT_SIZE_COMPLETION, 0);\n\n\t\ttr_req[tr_idx].addr = period_addr;\n\t\ttr_req[tr_idx].icnt0 = tr0_cnt0;\n\t\ttr_req[tr_idx].icnt1 = tr0_cnt1;\n\t\ttr_req[tr_idx].dim1 = tr0_cnt0;\n\n\t\tif (num_tr == 2) {\n\t\t\tcppi5_tr_csf_set(&tr_req[tr_idx].flags,\n\t\t\t\t\t CPPI5_TR_CSF_SUPR_EVT);\n\t\t\ttr_idx++;\n\n\t\t\tcppi5_tr_init(&tr_req[tr_idx].flags, CPPI5_TR_TYPE1,\n\t\t\t\t      false, false,\n\t\t\t\t      CPPI5_TR_EVENT_SIZE_COMPLETION, 0);\n\n\t\t\ttr_req[tr_idx].addr = period_addr + tr0_cnt1 * tr0_cnt0;\n\t\t\ttr_req[tr_idx].icnt0 = tr1_cnt0;\n\t\t\ttr_req[tr_idx].icnt1 = 1;\n\t\t\ttr_req[tr_idx].dim1 = tr1_cnt0;\n\t\t}\n\n\t\tif (!(flags & DMA_PREP_INTERRUPT))\n\t\t\tcppi5_tr_csf_set(&tr_req[tr_idx].flags,\n\t\t\t\t\t CPPI5_TR_CSF_SUPR_EVT);\n\n\t\tperiod_addr += period_len;\n\t}\n\n\treturn d;\n}\n\nstatic struct udma_desc *\nudma_prep_dma_cyclic_pkt(struct udma_chan *uc, dma_addr_t buf_addr,\n\t\t\t size_t buf_len, size_t period_len,\n\t\t\t enum dma_transfer_direction dir, unsigned long flags)\n{\n\tstruct udma_desc *d;\n\tu32 ring_id;\n\tint i;\n\tint periods = buf_len / period_len;\n\n\tif (periods > (K3_UDMA_DEFAULT_RING_SIZE - 1))\n\t\treturn NULL;\n\n\tif (period_len >= SZ_4M)\n\t\treturn NULL;\n\n\td = kzalloc(struct_size(d, hwdesc, periods), GFP_NOWAIT);\n\tif (!d)\n\t\treturn NULL;\n\n\td->hwdesc_count = periods;\n\n\t \n\tif (dir == DMA_DEV_TO_MEM)\n\t\tring_id = k3_ringacc_get_ring_id(uc->rflow->r_ring);\n\telse\n\t\tring_id = k3_ringacc_get_ring_id(uc->tchan->tc_ring);\n\n\tif (uc->ud->match_data->type != DMA_TYPE_UDMA)\n\t\tbuf_addr |= (u64)uc->config.asel << K3_ADDRESS_ASEL_SHIFT;\n\n\tfor (i = 0; i < periods; i++) {\n\t\tstruct udma_hwdesc *hwdesc = &d->hwdesc[i];\n\t\tdma_addr_t period_addr = buf_addr + (period_len * i);\n\t\tstruct cppi5_host_desc_t *h_desc;\n\n\t\thwdesc->cppi5_desc_vaddr = dma_pool_zalloc(uc->hdesc_pool,\n\t\t\t\t\t\tGFP_NOWAIT,\n\t\t\t\t\t\t&hwdesc->cppi5_desc_paddr);\n\t\tif (!hwdesc->cppi5_desc_vaddr) {\n\t\t\tdev_err(uc->ud->dev,\n\t\t\t\t\"descriptor%d allocation failed\\n\", i);\n\n\t\t\tudma_free_hwdesc(uc, d);\n\t\t\tkfree(d);\n\t\t\treturn NULL;\n\t\t}\n\n\t\thwdesc->cppi5_desc_size = uc->config.hdesc_size;\n\t\th_desc = hwdesc->cppi5_desc_vaddr;\n\n\t\tcppi5_hdesc_init(h_desc, 0, 0);\n\t\tcppi5_hdesc_set_pktlen(h_desc, period_len);\n\n\t\t \n\t\tcppi5_desc_set_pktids(&h_desc->hdr, uc->id,\n\t\t\t\t      CPPI5_INFO1_DESC_FLOWID_DEFAULT);\n\t\tcppi5_desc_set_retpolicy(&h_desc->hdr, 0, ring_id);\n\n\t\t \n\t\tcppi5_hdesc_attach_buf(h_desc,\n\t\t\t\t       period_addr, period_len,\n\t\t\t\t       period_addr, period_len);\n\t}\n\n\treturn d;\n}\n\nstatic struct dma_async_tx_descriptor *\nudma_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\n\t\t     size_t period_len, enum dma_transfer_direction dir,\n\t\t     unsigned long flags)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tenum dma_slave_buswidth dev_width;\n\tstruct udma_desc *d;\n\tu32 burst;\n\n\tif (dir != uc->config.dir) {\n\t\tdev_err(chan->device->dev,\n\t\t\t\"%s: chan%d is for %s, not supporting %s\\n\",\n\t\t\t__func__, uc->id,\n\t\t\tdmaengine_get_direction_text(uc->config.dir),\n\t\t\tdmaengine_get_direction_text(dir));\n\t\treturn NULL;\n\t}\n\n\tuc->cyclic = true;\n\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\tdev_width = uc->cfg.src_addr_width;\n\t\tburst = uc->cfg.src_maxburst;\n\t} else if (dir == DMA_MEM_TO_DEV) {\n\t\tdev_width = uc->cfg.dst_addr_width;\n\t\tburst = uc->cfg.dst_maxburst;\n\t} else {\n\t\tdev_err(uc->ud->dev, \"%s: bad direction?\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tif (!burst)\n\t\tburst = 1;\n\n\tif (uc->config.pkt_mode)\n\t\td = udma_prep_dma_cyclic_pkt(uc, buf_addr, buf_len, period_len,\n\t\t\t\t\t     dir, flags);\n\telse\n\t\td = udma_prep_dma_cyclic_tr(uc, buf_addr, buf_len, period_len,\n\t\t\t\t\t    dir, flags);\n\n\tif (!d)\n\t\treturn NULL;\n\n\td->sglen = buf_len / period_len;\n\n\td->dir = dir;\n\td->residue = buf_len;\n\n\t \n\tif (udma_configure_statictr(uc, d, dev_width, burst)) {\n\t\tdev_err(uc->ud->dev,\n\t\t\t\"%s: StaticTR Z is limited to maximum 4095 (%u)\\n\",\n\t\t\t__func__, d->static_tr.bstcnt);\n\n\t\tudma_free_hwdesc(uc, d);\n\t\tkfree(d);\n\t\treturn NULL;\n\t}\n\n\tif (uc->config.metadata_size)\n\t\td->vd.tx.metadata_ops = &metadata_ops;\n\n\treturn vchan_tx_prep(&uc->vc, &d->vd, flags);\n}\n\nstatic struct dma_async_tx_descriptor *\nudma_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\n\t\t     size_t len, unsigned long tx_flags)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tstruct udma_desc *d;\n\tstruct cppi5_tr_type15_t *tr_req;\n\tint num_tr;\n\tsize_t tr_size = sizeof(struct cppi5_tr_type15_t);\n\tu16 tr0_cnt0, tr0_cnt1, tr1_cnt0;\n\tu32 csf = CPPI5_TR_CSF_SUPR_EVT;\n\n\tif (uc->config.dir != DMA_MEM_TO_MEM) {\n\t\tdev_err(chan->device->dev,\n\t\t\t\"%s: chan%d is for %s, not supporting %s\\n\",\n\t\t\t__func__, uc->id,\n\t\t\tdmaengine_get_direction_text(uc->config.dir),\n\t\t\tdmaengine_get_direction_text(DMA_MEM_TO_MEM));\n\t\treturn NULL;\n\t}\n\n\tnum_tr = udma_get_tr_counters(len, __ffs(src | dest), &tr0_cnt0,\n\t\t\t\t      &tr0_cnt1, &tr1_cnt0);\n\tif (num_tr < 0) {\n\t\tdev_err(uc->ud->dev, \"size %zu is not supported\\n\",\n\t\t\tlen);\n\t\treturn NULL;\n\t}\n\n\td = udma_alloc_tr_desc(uc, tr_size, num_tr, DMA_MEM_TO_MEM);\n\tif (!d)\n\t\treturn NULL;\n\n\td->dir = DMA_MEM_TO_MEM;\n\td->desc_idx = 0;\n\td->tr_idx = 0;\n\td->residue = len;\n\n\tif (uc->ud->match_data->type != DMA_TYPE_UDMA) {\n\t\tsrc |= (u64)uc->ud->asel << K3_ADDRESS_ASEL_SHIFT;\n\t\tdest |= (u64)uc->ud->asel << K3_ADDRESS_ASEL_SHIFT;\n\t} else {\n\t\tcsf |= CPPI5_TR_CSF_EOL_ICNT0;\n\t}\n\n\ttr_req = d->hwdesc[0].tr_req_base;\n\n\tcppi5_tr_init(&tr_req[0].flags, CPPI5_TR_TYPE15, false, true,\n\t\t      CPPI5_TR_EVENT_SIZE_COMPLETION, 0);\n\tcppi5_tr_csf_set(&tr_req[0].flags, csf);\n\n\ttr_req[0].addr = src;\n\ttr_req[0].icnt0 = tr0_cnt0;\n\ttr_req[0].icnt1 = tr0_cnt1;\n\ttr_req[0].icnt2 = 1;\n\ttr_req[0].icnt3 = 1;\n\ttr_req[0].dim1 = tr0_cnt0;\n\n\ttr_req[0].daddr = dest;\n\ttr_req[0].dicnt0 = tr0_cnt0;\n\ttr_req[0].dicnt1 = tr0_cnt1;\n\ttr_req[0].dicnt2 = 1;\n\ttr_req[0].dicnt3 = 1;\n\ttr_req[0].ddim1 = tr0_cnt0;\n\n\tif (num_tr == 2) {\n\t\tcppi5_tr_init(&tr_req[1].flags, CPPI5_TR_TYPE15, false, true,\n\t\t\t      CPPI5_TR_EVENT_SIZE_COMPLETION, 0);\n\t\tcppi5_tr_csf_set(&tr_req[1].flags, csf);\n\n\t\ttr_req[1].addr = src + tr0_cnt1 * tr0_cnt0;\n\t\ttr_req[1].icnt0 = tr1_cnt0;\n\t\ttr_req[1].icnt1 = 1;\n\t\ttr_req[1].icnt2 = 1;\n\t\ttr_req[1].icnt3 = 1;\n\n\t\ttr_req[1].daddr = dest + tr0_cnt1 * tr0_cnt0;\n\t\ttr_req[1].dicnt0 = tr1_cnt0;\n\t\ttr_req[1].dicnt1 = 1;\n\t\ttr_req[1].dicnt2 = 1;\n\t\ttr_req[1].dicnt3 = 1;\n\t}\n\n\tcppi5_tr_csf_set(&tr_req[num_tr - 1].flags, csf | CPPI5_TR_CSF_EOP);\n\n\tif (uc->config.metadata_size)\n\t\td->vd.tx.metadata_ops = &metadata_ops;\n\n\treturn vchan_tx_prep(&uc->vc, &d->vd, tx_flags);\n}\n\nstatic void udma_issue_pending(struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&uc->vc.lock, flags);\n\n\t \n\tif (vchan_issue_pending(&uc->vc) && !uc->desc) {\n\t\t \n\t\tif (!(uc->state == UDMA_CHAN_IS_TERMINATING &&\n\t\t      udma_is_chan_running(uc)))\n\t\t\tudma_start(uc);\n\t}\n\n\tspin_unlock_irqrestore(&uc->vc.lock, flags);\n}\n\nstatic enum dma_status udma_tx_status(struct dma_chan *chan,\n\t\t\t\t      dma_cookie_t cookie,\n\t\t\t\t      struct dma_tx_state *txstate)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tenum dma_status ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&uc->vc.lock, flags);\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\n\tif (!udma_is_chan_running(uc))\n\t\tret = DMA_COMPLETE;\n\n\tif (ret == DMA_IN_PROGRESS && udma_is_chan_paused(uc))\n\t\tret = DMA_PAUSED;\n\n\tif (ret == DMA_COMPLETE || !txstate)\n\t\tgoto out;\n\n\tif (uc->desc && uc->desc->vd.tx.cookie == cookie) {\n\t\tu32 peer_bcnt = 0;\n\t\tu32 bcnt = 0;\n\t\tu32 residue = uc->desc->residue;\n\t\tu32 delay = 0;\n\n\t\tif (uc->desc->dir == DMA_MEM_TO_DEV) {\n\t\t\tbcnt = udma_tchanrt_read(uc, UDMA_CHAN_RT_SBCNT_REG);\n\n\t\t\tif (uc->config.ep_type != PSIL_EP_NATIVE) {\n\t\t\t\tpeer_bcnt = udma_tchanrt_read(uc,\n\t\t\t\t\t\tUDMA_CHAN_RT_PEER_BCNT_REG);\n\n\t\t\t\tif (bcnt > peer_bcnt)\n\t\t\t\t\tdelay = bcnt - peer_bcnt;\n\t\t\t}\n\t\t} else if (uc->desc->dir == DMA_DEV_TO_MEM) {\n\t\t\tbcnt = udma_rchanrt_read(uc, UDMA_CHAN_RT_BCNT_REG);\n\n\t\t\tif (uc->config.ep_type != PSIL_EP_NATIVE) {\n\t\t\t\tpeer_bcnt = udma_rchanrt_read(uc,\n\t\t\t\t\t\tUDMA_CHAN_RT_PEER_BCNT_REG);\n\n\t\t\t\tif (peer_bcnt > bcnt)\n\t\t\t\t\tdelay = peer_bcnt - bcnt;\n\t\t\t}\n\t\t} else {\n\t\t\tbcnt = udma_tchanrt_read(uc, UDMA_CHAN_RT_BCNT_REG);\n\t\t}\n\n\t\tif (bcnt && !(bcnt % uc->desc->residue))\n\t\t\tresidue = 0;\n\t\telse\n\t\t\tresidue -= bcnt % uc->desc->residue;\n\n\t\tif (!residue && (uc->config.dir == DMA_DEV_TO_MEM || !delay)) {\n\t\t\tret = DMA_COMPLETE;\n\t\t\tdelay = 0;\n\t\t}\n\n\t\tdma_set_residue(txstate, residue);\n\t\tdma_set_in_flight_bytes(txstate, delay);\n\n\t} else {\n\t\tret = DMA_COMPLETE;\n\t}\n\nout:\n\tspin_unlock_irqrestore(&uc->vc.lock, flags);\n\treturn ret;\n}\n\nstatic int udma_pause(struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\n\t \n\tswitch (uc->config.dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\tudma_rchanrt_update_bits(uc, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t\t\t UDMA_PEER_RT_EN_PAUSE,\n\t\t\t\t\t UDMA_PEER_RT_EN_PAUSE);\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\tudma_tchanrt_update_bits(uc, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t\t\t UDMA_PEER_RT_EN_PAUSE,\n\t\t\t\t\t UDMA_PEER_RT_EN_PAUSE);\n\t\tbreak;\n\tcase DMA_MEM_TO_MEM:\n\t\tudma_tchanrt_update_bits(uc, UDMA_CHAN_RT_CTL_REG,\n\t\t\t\t\t UDMA_CHAN_RT_CTL_PAUSE,\n\t\t\t\t\t UDMA_CHAN_RT_CTL_PAUSE);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int udma_resume(struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\n\t \n\tswitch (uc->config.dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\tudma_rchanrt_update_bits(uc, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t\t\t UDMA_PEER_RT_EN_PAUSE, 0);\n\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\tudma_tchanrt_update_bits(uc, UDMA_CHAN_RT_PEER_RT_EN_REG,\n\t\t\t\t\t UDMA_PEER_RT_EN_PAUSE, 0);\n\t\tbreak;\n\tcase DMA_MEM_TO_MEM:\n\t\tudma_tchanrt_update_bits(uc, UDMA_CHAN_RT_CTL_REG,\n\t\t\t\t\t UDMA_CHAN_RT_CTL_PAUSE, 0);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int udma_terminate_all(struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&uc->vc.lock, flags);\n\n\tif (udma_is_chan_running(uc))\n\t\tudma_stop(uc);\n\n\tif (uc->desc) {\n\t\tuc->terminated_desc = uc->desc;\n\t\tuc->desc = NULL;\n\t\tuc->terminated_desc->terminated = true;\n\t\tcancel_delayed_work(&uc->tx_drain.work);\n\t}\n\n\tuc->paused = false;\n\n\tvchan_get_all_descriptors(&uc->vc, &head);\n\tspin_unlock_irqrestore(&uc->vc.lock, flags);\n\tvchan_dma_desc_free_list(&uc->vc, &head);\n\n\treturn 0;\n}\n\nstatic void udma_synchronize(struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tunsigned long timeout = msecs_to_jiffies(1000);\n\n\tvchan_synchronize(&uc->vc);\n\n\tif (uc->state == UDMA_CHAN_IS_TERMINATING) {\n\t\ttimeout = wait_for_completion_timeout(&uc->teardown_completed,\n\t\t\t\t\t\t      timeout);\n\t\tif (!timeout) {\n\t\t\tdev_warn(uc->ud->dev, \"chan%d teardown timeout!\\n\",\n\t\t\t\t uc->id);\n\t\t\tudma_dump_chan_stdata(uc);\n\t\t\tudma_reset_chan(uc, true);\n\t\t}\n\t}\n\n\tudma_reset_chan(uc, false);\n\tif (udma_is_chan_running(uc))\n\t\tdev_warn(uc->ud->dev, \"chan%d refused to stop!\\n\", uc->id);\n\n\tcancel_delayed_work_sync(&uc->tx_drain.work);\n\tudma_reset_rings(uc);\n}\n\nstatic void udma_desc_pre_callback(struct virt_dma_chan *vc,\n\t\t\t\t   struct virt_dma_desc *vd,\n\t\t\t\t   struct dmaengine_result *result)\n{\n\tstruct udma_chan *uc = to_udma_chan(&vc->chan);\n\tstruct udma_desc *d;\n\n\tif (!vd)\n\t\treturn;\n\n\td = to_udma_desc(&vd->tx);\n\n\tif (d->metadata_size)\n\t\tudma_fetch_epib(uc, d);\n\n\t \n\tif (result) {\n\t\tvoid *desc_vaddr = udma_curr_cppi5_desc_vaddr(d, d->desc_idx);\n\n\t\tif (cppi5_desc_get_type(desc_vaddr) ==\n\t\t    CPPI5_INFO0_DESC_TYPE_VAL_HOST) {\n\t\t\tresult->residue = d->residue -\n\t\t\t\t\t  cppi5_hdesc_get_pktlen(desc_vaddr);\n\t\t\tif (result->residue)\n\t\t\t\tresult->result = DMA_TRANS_ABORTED;\n\t\t\telse\n\t\t\t\tresult->result = DMA_TRANS_NOERROR;\n\t\t} else {\n\t\t\tresult->residue = 0;\n\t\t\tresult->result = DMA_TRANS_NOERROR;\n\t\t}\n\t}\n}\n\n \nstatic void udma_vchan_complete(struct tasklet_struct *t)\n{\n\tstruct virt_dma_chan *vc = from_tasklet(vc, t, task);\n\tstruct virt_dma_desc *vd, *_vd;\n\tstruct dmaengine_desc_callback cb;\n\tLIST_HEAD(head);\n\n\tspin_lock_irq(&vc->lock);\n\tlist_splice_tail_init(&vc->desc_completed, &head);\n\tvd = vc->cyclic;\n\tif (vd) {\n\t\tvc->cyclic = NULL;\n\t\tdmaengine_desc_get_callback(&vd->tx, &cb);\n\t} else {\n\t\tmemset(&cb, 0, sizeof(cb));\n\t}\n\tspin_unlock_irq(&vc->lock);\n\n\tudma_desc_pre_callback(vc, vd, NULL);\n\tdmaengine_desc_callback_invoke(&cb, NULL);\n\n\tlist_for_each_entry_safe(vd, _vd, &head, node) {\n\t\tstruct dmaengine_result result;\n\n\t\tdmaengine_desc_get_callback(&vd->tx, &cb);\n\n\t\tlist_del(&vd->node);\n\n\t\tudma_desc_pre_callback(vc, vd, &result);\n\t\tdmaengine_desc_callback_invoke(&cb, &result);\n\n\t\tvchan_vdesc_fini(vd);\n\t}\n}\n\nstatic void udma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tstruct udma_dev *ud = to_udma_dev(chan->device);\n\n\tudma_terminate_all(chan);\n\tif (uc->terminated_desc) {\n\t\tudma_reset_chan(uc, false);\n\t\tudma_reset_rings(uc);\n\t}\n\n\tcancel_delayed_work_sync(&uc->tx_drain.work);\n\n\tif (uc->irq_num_ring > 0) {\n\t\tfree_irq(uc->irq_num_ring, uc);\n\n\t\tuc->irq_num_ring = 0;\n\t}\n\tif (uc->irq_num_udma > 0) {\n\t\tfree_irq(uc->irq_num_udma, uc);\n\n\t\tuc->irq_num_udma = 0;\n\t}\n\n\t \n\tif (uc->psil_paired) {\n\t\tnavss_psil_unpair(ud, uc->config.src_thread,\n\t\t\t\t  uc->config.dst_thread);\n\t\tuc->psil_paired = false;\n\t}\n\n\tvchan_free_chan_resources(&uc->vc);\n\ttasklet_kill(&uc->vc.task);\n\n\tbcdma_free_bchan_resources(uc);\n\tudma_free_tx_resources(uc);\n\tudma_free_rx_resources(uc);\n\tudma_reset_uchan(uc);\n\n\tif (uc->use_dma_pool) {\n\t\tdma_pool_destroy(uc->hdesc_pool);\n\t\tuc->use_dma_pool = false;\n\t}\n}\n\nstatic struct platform_driver udma_driver;\nstatic struct platform_driver bcdma_driver;\nstatic struct platform_driver pktdma_driver;\n\nstruct udma_filter_param {\n\tint remote_thread_id;\n\tu32 atype;\n\tu32 asel;\n\tu32 tr_trigger_type;\n};\n\nstatic bool udma_dma_filter_fn(struct dma_chan *chan, void *param)\n{\n\tstruct udma_chan_config *ucc;\n\tstruct psil_endpoint_config *ep_config;\n\tstruct udma_filter_param *filter_param;\n\tstruct udma_chan *uc;\n\tstruct udma_dev *ud;\n\n\tif (chan->device->dev->driver != &udma_driver.driver &&\n\t    chan->device->dev->driver != &bcdma_driver.driver &&\n\t    chan->device->dev->driver != &pktdma_driver.driver)\n\t\treturn false;\n\n\tuc = to_udma_chan(chan);\n\tucc = &uc->config;\n\tud = uc->ud;\n\tfilter_param = param;\n\n\tif (filter_param->atype > 2) {\n\t\tdev_err(ud->dev, \"Invalid channel atype: %u\\n\",\n\t\t\tfilter_param->atype);\n\t\treturn false;\n\t}\n\n\tif (filter_param->asel > 15) {\n\t\tdev_err(ud->dev, \"Invalid channel asel: %u\\n\",\n\t\t\tfilter_param->asel);\n\t\treturn false;\n\t}\n\n\tucc->remote_thread_id = filter_param->remote_thread_id;\n\tucc->atype = filter_param->atype;\n\tucc->asel = filter_param->asel;\n\tucc->tr_trigger_type = filter_param->tr_trigger_type;\n\n\tif (ucc->tr_trigger_type) {\n\t\tucc->dir = DMA_MEM_TO_MEM;\n\t\tgoto triggered_bchan;\n\t} else if (ucc->remote_thread_id & K3_PSIL_DST_THREAD_ID_OFFSET) {\n\t\tucc->dir = DMA_MEM_TO_DEV;\n\t} else {\n\t\tucc->dir = DMA_DEV_TO_MEM;\n\t}\n\n\tep_config = psil_get_ep_config(ucc->remote_thread_id);\n\tif (IS_ERR(ep_config)) {\n\t\tdev_err(ud->dev, \"No configuration for psi-l thread 0x%04x\\n\",\n\t\t\tucc->remote_thread_id);\n\t\tucc->dir = DMA_MEM_TO_MEM;\n\t\tucc->remote_thread_id = -1;\n\t\tucc->atype = 0;\n\t\tucc->asel = 0;\n\t\treturn false;\n\t}\n\n\tif (ud->match_data->type == DMA_TYPE_BCDMA &&\n\t    ep_config->pkt_mode) {\n\t\tdev_err(ud->dev,\n\t\t\t\"Only TR mode is supported (psi-l thread 0x%04x)\\n\",\n\t\t\tucc->remote_thread_id);\n\t\tucc->dir = DMA_MEM_TO_MEM;\n\t\tucc->remote_thread_id = -1;\n\t\tucc->atype = 0;\n\t\tucc->asel = 0;\n\t\treturn false;\n\t}\n\n\tucc->pkt_mode = ep_config->pkt_mode;\n\tucc->channel_tpl = ep_config->channel_tpl;\n\tucc->notdpkt = ep_config->notdpkt;\n\tucc->ep_type = ep_config->ep_type;\n\n\tif (ud->match_data->type == DMA_TYPE_PKTDMA &&\n\t    ep_config->mapped_channel_id >= 0) {\n\t\tucc->mapped_channel_id = ep_config->mapped_channel_id;\n\t\tucc->default_flow_id = ep_config->default_flow_id;\n\t} else {\n\t\tucc->mapped_channel_id = -1;\n\t\tucc->default_flow_id = -1;\n\t}\n\n\tif (ucc->ep_type != PSIL_EP_NATIVE) {\n\t\tconst struct udma_match_data *match_data = ud->match_data;\n\n\t\tif (match_data->flags & UDMA_FLAG_PDMA_ACC32)\n\t\t\tucc->enable_acc32 = ep_config->pdma_acc32;\n\t\tif (match_data->flags & UDMA_FLAG_PDMA_BURST)\n\t\t\tucc->enable_burst = ep_config->pdma_burst;\n\t}\n\n\tucc->needs_epib = ep_config->needs_epib;\n\tucc->psd_size = ep_config->psd_size;\n\tucc->metadata_size =\n\t\t\t(ucc->needs_epib ? CPPI5_INFO0_HDESC_EPIB_SIZE : 0) +\n\t\t\tucc->psd_size;\n\n\tif (ucc->pkt_mode)\n\t\tucc->hdesc_size = ALIGN(sizeof(struct cppi5_host_desc_t) +\n\t\t\t\t ucc->metadata_size, ud->desc_align);\n\n\tdev_dbg(ud->dev, \"chan%d: Remote thread: 0x%04x (%s)\\n\", uc->id,\n\t\tucc->remote_thread_id, dmaengine_get_direction_text(ucc->dir));\n\n\treturn true;\n\ntriggered_bchan:\n\tdev_dbg(ud->dev, \"chan%d: triggered channel (type: %u)\\n\", uc->id,\n\t\tucc->tr_trigger_type);\n\n\treturn true;\n\n}\n\nstatic struct dma_chan *udma_of_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t      struct of_dma *ofdma)\n{\n\tstruct udma_dev *ud = ofdma->of_dma_data;\n\tdma_cap_mask_t mask = ud->ddev.cap_mask;\n\tstruct udma_filter_param filter_param;\n\tstruct dma_chan *chan;\n\n\tif (ud->match_data->type == DMA_TYPE_BCDMA) {\n\t\tif (dma_spec->args_count != 3)\n\t\t\treturn NULL;\n\n\t\tfilter_param.tr_trigger_type = dma_spec->args[0];\n\t\tfilter_param.remote_thread_id = dma_spec->args[1];\n\t\tfilter_param.asel = dma_spec->args[2];\n\t\tfilter_param.atype = 0;\n\t} else {\n\t\tif (dma_spec->args_count != 1 && dma_spec->args_count != 2)\n\t\t\treturn NULL;\n\n\t\tfilter_param.remote_thread_id = dma_spec->args[0];\n\t\tfilter_param.tr_trigger_type = 0;\n\t\tif (dma_spec->args_count == 2) {\n\t\t\tif (ud->match_data->type == DMA_TYPE_UDMA) {\n\t\t\t\tfilter_param.atype = dma_spec->args[1];\n\t\t\t\tfilter_param.asel = 0;\n\t\t\t} else {\n\t\t\t\tfilter_param.atype = 0;\n\t\t\t\tfilter_param.asel = dma_spec->args[1];\n\t\t\t}\n\t\t} else {\n\t\t\tfilter_param.atype = 0;\n\t\t\tfilter_param.asel = 0;\n\t\t}\n\t}\n\n\tchan = __dma_request_channel(&mask, udma_dma_filter_fn, &filter_param,\n\t\t\t\t     ofdma->of_node);\n\tif (!chan) {\n\t\tdev_err(ud->dev, \"get channel fail in %s.\\n\", __func__);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn chan;\n}\n\nstatic struct udma_match_data am654_main_data = {\n\t.type = DMA_TYPE_UDMA,\n\t.psil_base = 0x1000,\n\t.enable_memcpy_support = true,\n\t.statictr_z_mask = GENMASK(11, 0),\n\t.burst_size = {\n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\t0,  \n\t},\n};\n\nstatic struct udma_match_data am654_mcu_data = {\n\t.type = DMA_TYPE_UDMA,\n\t.psil_base = 0x6000,\n\t.enable_memcpy_support = false,\n\t.statictr_z_mask = GENMASK(11, 0),\n\t.burst_size = {\n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\t0,  \n\t},\n};\n\nstatic struct udma_match_data j721e_main_data = {\n\t.type = DMA_TYPE_UDMA,\n\t.psil_base = 0x1000,\n\t.enable_memcpy_support = true,\n\t.flags = UDMA_FLAGS_J7_CLASS,\n\t.statictr_z_mask = GENMASK(23, 0),\n\t.burst_size = {\n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_256_BYTES,  \n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_256_BYTES,  \n\t},\n};\n\nstatic struct udma_match_data j721e_mcu_data = {\n\t.type = DMA_TYPE_UDMA,\n\t.psil_base = 0x6000,\n\t.enable_memcpy_support = false,  \n\t.flags = UDMA_FLAGS_J7_CLASS,\n\t.statictr_z_mask = GENMASK(23, 0),\n\t.burst_size = {\n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_128_BYTES,  \n\t\t0,  \n\t},\n};\n\nstatic struct udma_soc_data am62a_dmss_csi_soc_data = {\n\t.oes = {\n\t\t.bcdma_rchan_data = 0xe00,\n\t\t.bcdma_rchan_ring = 0x1000,\n\t},\n};\n\nstatic struct udma_soc_data j721s2_bcdma_csi_soc_data = {\n\t.oes = {\n\t\t.bcdma_tchan_data = 0x800,\n\t\t.bcdma_tchan_ring = 0xa00,\n\t\t.bcdma_rchan_data = 0xe00,\n\t\t.bcdma_rchan_ring = 0x1000,\n\t},\n};\n\nstatic struct udma_match_data am62a_bcdma_csirx_data = {\n\t.type = DMA_TYPE_BCDMA,\n\t.psil_base = 0x3100,\n\t.enable_memcpy_support = false,\n\t.burst_size = {\n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\t0,  \n\t\t0,  \n\t},\n\t.soc_data = &am62a_dmss_csi_soc_data,\n};\n\nstatic struct udma_match_data am64_bcdma_data = {\n\t.type = DMA_TYPE_BCDMA,\n\t.psil_base = 0x2000,  \n\t.enable_memcpy_support = true,  \n\t.flags = UDMA_FLAGS_J7_CLASS,\n\t.statictr_z_mask = GENMASK(23, 0),\n\t.burst_size = {\n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\t0,  \n\t\t0,  \n\t},\n};\n\nstatic struct udma_match_data am64_pktdma_data = {\n\t.type = DMA_TYPE_PKTDMA,\n\t.psil_base = 0x1000,\n\t.enable_memcpy_support = false,  \n\t.flags = UDMA_FLAGS_J7_CLASS,\n\t.statictr_z_mask = GENMASK(23, 0),\n\t.burst_size = {\n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\t0,  \n\t\t0,  \n\t},\n};\n\nstatic struct udma_match_data j721s2_bcdma_csi_data = {\n\t.type = DMA_TYPE_BCDMA,\n\t.psil_base = 0x2000,\n\t.enable_memcpy_support = false,\n\t.burst_size = {\n\t\tTI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES,  \n\t\t0,  \n\t\t0,  \n\t},\n\t.soc_data = &j721s2_bcdma_csi_soc_data,\n};\n\nstatic const struct of_device_id udma_of_match[] = {\n\t{\n\t\t.compatible = \"ti,am654-navss-main-udmap\",\n\t\t.data = &am654_main_data,\n\t},\n\t{\n\t\t.compatible = \"ti,am654-navss-mcu-udmap\",\n\t\t.data = &am654_mcu_data,\n\t}, {\n\t\t.compatible = \"ti,j721e-navss-main-udmap\",\n\t\t.data = &j721e_main_data,\n\t}, {\n\t\t.compatible = \"ti,j721e-navss-mcu-udmap\",\n\t\t.data = &j721e_mcu_data,\n\t},\n\t{\n\t\t.compatible = \"ti,am64-dmss-bcdma\",\n\t\t.data = &am64_bcdma_data,\n\t},\n\t{\n\t\t.compatible = \"ti,am64-dmss-pktdma\",\n\t\t.data = &am64_pktdma_data,\n\t},\n\t{\n\t\t.compatible = \"ti,am62a-dmss-bcdma-csirx\",\n\t\t.data = &am62a_bcdma_csirx_data,\n\t},\n\t{\n\t\t.compatible = \"ti,j721s2-dmss-bcdma-csi\",\n\t\t.data = &j721s2_bcdma_csi_data,\n\t},\n\t{   },\n};\n\nstatic struct udma_soc_data am654_soc_data = {\n\t.oes = {\n\t\t.udma_rchan = 0x200,\n\t},\n};\n\nstatic struct udma_soc_data j721e_soc_data = {\n\t.oes = {\n\t\t.udma_rchan = 0x400,\n\t},\n};\n\nstatic struct udma_soc_data j7200_soc_data = {\n\t.oes = {\n\t\t.udma_rchan = 0x80,\n\t},\n};\n\nstatic struct udma_soc_data am64_soc_data = {\n\t.oes = {\n\t\t.bcdma_bchan_data = 0x2200,\n\t\t.bcdma_bchan_ring = 0x2400,\n\t\t.bcdma_tchan_data = 0x2800,\n\t\t.bcdma_tchan_ring = 0x2a00,\n\t\t.bcdma_rchan_data = 0x2e00,\n\t\t.bcdma_rchan_ring = 0x3000,\n\t\t.pktdma_tchan_flow = 0x1200,\n\t\t.pktdma_rchan_flow = 0x1600,\n\t},\n\t.bcdma_trigger_event_offset = 0xc400,\n};\n\nstatic const struct soc_device_attribute k3_soc_devices[] = {\n\t{ .family = \"AM65X\", .data = &am654_soc_data },\n\t{ .family = \"J721E\", .data = &j721e_soc_data },\n\t{ .family = \"J7200\", .data = &j7200_soc_data },\n\t{ .family = \"AM64X\", .data = &am64_soc_data },\n\t{ .family = \"J721S2\", .data = &j721e_soc_data},\n\t{ .family = \"AM62X\", .data = &am64_soc_data },\n\t{ .family = \"AM62AX\", .data = &am64_soc_data },\n\t{ .family = \"J784S4\", .data = &j721e_soc_data },\n\t{   }\n};\n\nstatic int udma_get_mmrs(struct platform_device *pdev, struct udma_dev *ud)\n{\n\tu32 cap2, cap3, cap4;\n\tint i;\n\n\tud->mmrs[MMR_GCFG] = devm_platform_ioremap_resource_byname(pdev, mmr_names[MMR_GCFG]);\n\tif (IS_ERR(ud->mmrs[MMR_GCFG]))\n\t\treturn PTR_ERR(ud->mmrs[MMR_GCFG]);\n\n\tcap2 = udma_read(ud->mmrs[MMR_GCFG], 0x28);\n\tcap3 = udma_read(ud->mmrs[MMR_GCFG], 0x2c);\n\n\tswitch (ud->match_data->type) {\n\tcase DMA_TYPE_UDMA:\n\t\tud->rflow_cnt = UDMA_CAP3_RFLOW_CNT(cap3);\n\t\tud->tchan_cnt = UDMA_CAP2_TCHAN_CNT(cap2);\n\t\tud->echan_cnt = UDMA_CAP2_ECHAN_CNT(cap2);\n\t\tud->rchan_cnt = UDMA_CAP2_RCHAN_CNT(cap2);\n\t\tbreak;\n\tcase DMA_TYPE_BCDMA:\n\t\tud->bchan_cnt = BCDMA_CAP2_BCHAN_CNT(cap2);\n\t\tud->tchan_cnt = BCDMA_CAP2_TCHAN_CNT(cap2);\n\t\tud->rchan_cnt = BCDMA_CAP2_RCHAN_CNT(cap2);\n\t\tud->rflow_cnt = ud->rchan_cnt;\n\t\tbreak;\n\tcase DMA_TYPE_PKTDMA:\n\t\tcap4 = udma_read(ud->mmrs[MMR_GCFG], 0x30);\n\t\tud->tchan_cnt = UDMA_CAP2_TCHAN_CNT(cap2);\n\t\tud->rchan_cnt = UDMA_CAP2_RCHAN_CNT(cap2);\n\t\tud->rflow_cnt = UDMA_CAP3_RFLOW_CNT(cap3);\n\t\tud->tflow_cnt = PKTDMA_CAP4_TFLOW_CNT(cap4);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 1; i < MMR_LAST; i++) {\n\t\tif (i == MMR_BCHANRT && ud->bchan_cnt == 0)\n\t\t\tcontinue;\n\t\tif (i == MMR_TCHANRT && ud->tchan_cnt == 0)\n\t\t\tcontinue;\n\t\tif (i == MMR_RCHANRT && ud->rchan_cnt == 0)\n\t\t\tcontinue;\n\n\t\tud->mmrs[i] = devm_platform_ioremap_resource_byname(pdev, mmr_names[i]);\n\t\tif (IS_ERR(ud->mmrs[i]))\n\t\t\treturn PTR_ERR(ud->mmrs[i]);\n\t}\n\n\treturn 0;\n}\n\nstatic void udma_mark_resource_ranges(struct udma_dev *ud, unsigned long *map,\n\t\t\t\t      struct ti_sci_resource_desc *rm_desc,\n\t\t\t\t      char *name)\n{\n\tbitmap_clear(map, rm_desc->start, rm_desc->num);\n\tbitmap_clear(map, rm_desc->start_sec, rm_desc->num_sec);\n\tdev_dbg(ud->dev, \"ti_sci resource range for %s: %d:%d | %d:%d\\n\", name,\n\t\trm_desc->start, rm_desc->num, rm_desc->start_sec,\n\t\trm_desc->num_sec);\n}\n\nstatic const char * const range_names[] = {\n\t[RM_RANGE_BCHAN] = \"ti,sci-rm-range-bchan\",\n\t[RM_RANGE_TCHAN] = \"ti,sci-rm-range-tchan\",\n\t[RM_RANGE_RCHAN] = \"ti,sci-rm-range-rchan\",\n\t[RM_RANGE_RFLOW] = \"ti,sci-rm-range-rflow\",\n\t[RM_RANGE_TFLOW] = \"ti,sci-rm-range-tflow\",\n};\n\nstatic int udma_setup_resources(struct udma_dev *ud)\n{\n\tint ret, i, j;\n\tstruct device *dev = ud->dev;\n\tstruct ti_sci_resource *rm_res, irq_res;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tu32 cap3;\n\n\t \n\tcap3 = udma_read(ud->mmrs[MMR_GCFG], 0x2c);\n\tif (of_device_is_compatible(dev->of_node,\n\t\t\t\t    \"ti,am654-navss-main-udmap\")) {\n\t\tud->tchan_tpl.levels = 2;\n\t\tud->tchan_tpl.start_idx[0] = 8;\n\t} else if (of_device_is_compatible(dev->of_node,\n\t\t\t\t\t   \"ti,am654-navss-mcu-udmap\")) {\n\t\tud->tchan_tpl.levels = 2;\n\t\tud->tchan_tpl.start_idx[0] = 2;\n\t} else if (UDMA_CAP3_UCHAN_CNT(cap3)) {\n\t\tud->tchan_tpl.levels = 3;\n\t\tud->tchan_tpl.start_idx[1] = UDMA_CAP3_UCHAN_CNT(cap3);\n\t\tud->tchan_tpl.start_idx[0] = UDMA_CAP3_HCHAN_CNT(cap3);\n\t} else if (UDMA_CAP3_HCHAN_CNT(cap3)) {\n\t\tud->tchan_tpl.levels = 2;\n\t\tud->tchan_tpl.start_idx[0] = UDMA_CAP3_HCHAN_CNT(cap3);\n\t} else {\n\t\tud->tchan_tpl.levels = 1;\n\t}\n\n\tud->rchan_tpl.levels = ud->tchan_tpl.levels;\n\tud->rchan_tpl.start_idx[0] = ud->tchan_tpl.start_idx[0];\n\tud->rchan_tpl.start_idx[1] = ud->tchan_tpl.start_idx[1];\n\n\tud->tchan_map = devm_kmalloc_array(dev, BITS_TO_LONGS(ud->tchan_cnt),\n\t\t\t\t\t   sizeof(unsigned long), GFP_KERNEL);\n\tud->tchans = devm_kcalloc(dev, ud->tchan_cnt, sizeof(*ud->tchans),\n\t\t\t\t  GFP_KERNEL);\n\tud->rchan_map = devm_kmalloc_array(dev, BITS_TO_LONGS(ud->rchan_cnt),\n\t\t\t\t\t   sizeof(unsigned long), GFP_KERNEL);\n\tud->rchans = devm_kcalloc(dev, ud->rchan_cnt, sizeof(*ud->rchans),\n\t\t\t\t  GFP_KERNEL);\n\tud->rflow_gp_map = devm_kmalloc_array(dev, BITS_TO_LONGS(ud->rflow_cnt),\n\t\t\t\t\t      sizeof(unsigned long),\n\t\t\t\t\t      GFP_KERNEL);\n\tud->rflow_gp_map_allocated = devm_kcalloc(dev,\n\t\t\t\t\t\t  BITS_TO_LONGS(ud->rflow_cnt),\n\t\t\t\t\t\t  sizeof(unsigned long),\n\t\t\t\t\t\t  GFP_KERNEL);\n\tud->rflow_in_use = devm_kcalloc(dev, BITS_TO_LONGS(ud->rflow_cnt),\n\t\t\t\t\tsizeof(unsigned long),\n\t\t\t\t\tGFP_KERNEL);\n\tud->rflows = devm_kcalloc(dev, ud->rflow_cnt, sizeof(*ud->rflows),\n\t\t\t\t  GFP_KERNEL);\n\n\tif (!ud->tchan_map || !ud->rchan_map || !ud->rflow_gp_map ||\n\t    !ud->rflow_gp_map_allocated || !ud->tchans || !ud->rchans ||\n\t    !ud->rflows || !ud->rflow_in_use)\n\t\treturn -ENOMEM;\n\n\t \n\tbitmap_set(ud->rflow_gp_map_allocated, 0, ud->rchan_cnt);\n\n\t \n\tbitmap_set(ud->rflow_gp_map, 0, ud->rflow_cnt);\n\n\t \n\tfor (i = 0; i < RM_RANGE_LAST; i++) {\n\t\tif (i == RM_RANGE_BCHAN || i == RM_RANGE_TFLOW)\n\t\t\tcontinue;\n\n\t\ttisci_rm->rm_ranges[i] =\n\t\t\tdevm_ti_sci_get_of_resource(tisci_rm->tisci, dev,\n\t\t\t\t\t\t    tisci_rm->tisci_dev_id,\n\t\t\t\t\t\t    (char *)range_names[i]);\n\t}\n\n\t \n\trm_res = tisci_rm->rm_ranges[RM_RANGE_TCHAN];\n\tif (IS_ERR(rm_res)) {\n\t\tbitmap_zero(ud->tchan_map, ud->tchan_cnt);\n\t\tirq_res.sets = 1;\n\t} else {\n\t\tbitmap_fill(ud->tchan_map, ud->tchan_cnt);\n\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\tudma_mark_resource_ranges(ud, ud->tchan_map,\n\t\t\t\t\t\t  &rm_res->desc[i], \"tchan\");\n\t\tirq_res.sets = rm_res->sets;\n\t}\n\n\t \n\trm_res = tisci_rm->rm_ranges[RM_RANGE_RCHAN];\n\tif (IS_ERR(rm_res)) {\n\t\tbitmap_zero(ud->rchan_map, ud->rchan_cnt);\n\t\tirq_res.sets++;\n\t} else {\n\t\tbitmap_fill(ud->rchan_map, ud->rchan_cnt);\n\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\tudma_mark_resource_ranges(ud, ud->rchan_map,\n\t\t\t\t\t\t  &rm_res->desc[i], \"rchan\");\n\t\tirq_res.sets += rm_res->sets;\n\t}\n\n\tirq_res.desc = kcalloc(irq_res.sets, sizeof(*irq_res.desc), GFP_KERNEL);\n\tif (!irq_res.desc)\n\t\treturn -ENOMEM;\n\trm_res = tisci_rm->rm_ranges[RM_RANGE_TCHAN];\n\tif (IS_ERR(rm_res)) {\n\t\tirq_res.desc[0].start = 0;\n\t\tirq_res.desc[0].num = ud->tchan_cnt;\n\t\ti = 1;\n\t} else {\n\t\tfor (i = 0; i < rm_res->sets; i++) {\n\t\t\tirq_res.desc[i].start = rm_res->desc[i].start;\n\t\t\tirq_res.desc[i].num = rm_res->desc[i].num;\n\t\t\tirq_res.desc[i].start_sec = rm_res->desc[i].start_sec;\n\t\t\tirq_res.desc[i].num_sec = rm_res->desc[i].num_sec;\n\t\t}\n\t}\n\trm_res = tisci_rm->rm_ranges[RM_RANGE_RCHAN];\n\tif (IS_ERR(rm_res)) {\n\t\tirq_res.desc[i].start = 0;\n\t\tirq_res.desc[i].num = ud->rchan_cnt;\n\t} else {\n\t\tfor (j = 0; j < rm_res->sets; j++, i++) {\n\t\t\tif (rm_res->desc[j].num) {\n\t\t\t\tirq_res.desc[i].start = rm_res->desc[j].start +\n\t\t\t\t\t\tud->soc_data->oes.udma_rchan;\n\t\t\t\tirq_res.desc[i].num = rm_res->desc[j].num;\n\t\t\t}\n\t\t\tif (rm_res->desc[j].num_sec) {\n\t\t\t\tirq_res.desc[i].start_sec = rm_res->desc[j].start_sec +\n\t\t\t\t\t\tud->soc_data->oes.udma_rchan;\n\t\t\t\tirq_res.desc[i].num_sec = rm_res->desc[j].num_sec;\n\t\t\t}\n\t\t}\n\t}\n\tret = ti_sci_inta_msi_domain_alloc_irqs(ud->dev, &irq_res);\n\tkfree(irq_res.desc);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"Failed to allocate MSI interrupts\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\trm_res = tisci_rm->rm_ranges[RM_RANGE_RFLOW];\n\tif (IS_ERR(rm_res)) {\n\t\t \n\t\tbitmap_clear(ud->rflow_gp_map, ud->rchan_cnt,\n\t\t\t     ud->rflow_cnt - ud->rchan_cnt);\n\t} else {\n\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\tudma_mark_resource_ranges(ud, ud->rflow_gp_map,\n\t\t\t\t\t\t  &rm_res->desc[i], \"gp-rflow\");\n\t}\n\n\treturn 0;\n}\n\nstatic int bcdma_setup_resources(struct udma_dev *ud)\n{\n\tint ret, i, j;\n\tstruct device *dev = ud->dev;\n\tstruct ti_sci_resource *rm_res, irq_res;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tconst struct udma_oes_offsets *oes = &ud->soc_data->oes;\n\tu32 cap;\n\n\t \n\tcap = udma_read(ud->mmrs[MMR_GCFG], 0x2c);\n\tif (BCDMA_CAP3_UBCHAN_CNT(cap)) {\n\t\tud->bchan_tpl.levels = 3;\n\t\tud->bchan_tpl.start_idx[1] = BCDMA_CAP3_UBCHAN_CNT(cap);\n\t\tud->bchan_tpl.start_idx[0] = BCDMA_CAP3_HBCHAN_CNT(cap);\n\t} else if (BCDMA_CAP3_HBCHAN_CNT(cap)) {\n\t\tud->bchan_tpl.levels = 2;\n\t\tud->bchan_tpl.start_idx[0] = BCDMA_CAP3_HBCHAN_CNT(cap);\n\t} else {\n\t\tud->bchan_tpl.levels = 1;\n\t}\n\n\tcap = udma_read(ud->mmrs[MMR_GCFG], 0x30);\n\tif (BCDMA_CAP4_URCHAN_CNT(cap)) {\n\t\tud->rchan_tpl.levels = 3;\n\t\tud->rchan_tpl.start_idx[1] = BCDMA_CAP4_URCHAN_CNT(cap);\n\t\tud->rchan_tpl.start_idx[0] = BCDMA_CAP4_HRCHAN_CNT(cap);\n\t} else if (BCDMA_CAP4_HRCHAN_CNT(cap)) {\n\t\tud->rchan_tpl.levels = 2;\n\t\tud->rchan_tpl.start_idx[0] = BCDMA_CAP4_HRCHAN_CNT(cap);\n\t} else {\n\t\tud->rchan_tpl.levels = 1;\n\t}\n\n\tif (BCDMA_CAP4_UTCHAN_CNT(cap)) {\n\t\tud->tchan_tpl.levels = 3;\n\t\tud->tchan_tpl.start_idx[1] = BCDMA_CAP4_UTCHAN_CNT(cap);\n\t\tud->tchan_tpl.start_idx[0] = BCDMA_CAP4_HTCHAN_CNT(cap);\n\t} else if (BCDMA_CAP4_HTCHAN_CNT(cap)) {\n\t\tud->tchan_tpl.levels = 2;\n\t\tud->tchan_tpl.start_idx[0] = BCDMA_CAP4_HTCHAN_CNT(cap);\n\t} else {\n\t\tud->tchan_tpl.levels = 1;\n\t}\n\n\tud->bchan_map = devm_kmalloc_array(dev, BITS_TO_LONGS(ud->bchan_cnt),\n\t\t\t\t\t   sizeof(unsigned long), GFP_KERNEL);\n\tud->bchans = devm_kcalloc(dev, ud->bchan_cnt, sizeof(*ud->bchans),\n\t\t\t\t  GFP_KERNEL);\n\tud->tchan_map = devm_kmalloc_array(dev, BITS_TO_LONGS(ud->tchan_cnt),\n\t\t\t\t\t   sizeof(unsigned long), GFP_KERNEL);\n\tud->tchans = devm_kcalloc(dev, ud->tchan_cnt, sizeof(*ud->tchans),\n\t\t\t\t  GFP_KERNEL);\n\tud->rchan_map = devm_kmalloc_array(dev, BITS_TO_LONGS(ud->rchan_cnt),\n\t\t\t\t\t   sizeof(unsigned long), GFP_KERNEL);\n\tud->rchans = devm_kcalloc(dev, ud->rchan_cnt, sizeof(*ud->rchans),\n\t\t\t\t  GFP_KERNEL);\n\t \n\tud->rflow_in_use = devm_kcalloc(dev, BITS_TO_LONGS(ud->rchan_cnt),\n\t\t\t\t\tsizeof(unsigned long),\n\t\t\t\t\tGFP_KERNEL);\n\tud->rflows = devm_kcalloc(dev, ud->rchan_cnt, sizeof(*ud->rflows),\n\t\t\t\t  GFP_KERNEL);\n\n\tif (!ud->bchan_map || !ud->tchan_map || !ud->rchan_map ||\n\t    !ud->rflow_in_use || !ud->bchans || !ud->tchans || !ud->rchans ||\n\t    !ud->rflows)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < RM_RANGE_LAST; i++) {\n\t\tif (i == RM_RANGE_RFLOW || i == RM_RANGE_TFLOW)\n\t\t\tcontinue;\n\t\tif (i == RM_RANGE_BCHAN && ud->bchan_cnt == 0)\n\t\t\tcontinue;\n\t\tif (i == RM_RANGE_TCHAN && ud->tchan_cnt == 0)\n\t\t\tcontinue;\n\t\tif (i == RM_RANGE_RCHAN && ud->rchan_cnt == 0)\n\t\t\tcontinue;\n\n\t\ttisci_rm->rm_ranges[i] =\n\t\t\tdevm_ti_sci_get_of_resource(tisci_rm->tisci, dev,\n\t\t\t\t\t\t    tisci_rm->tisci_dev_id,\n\t\t\t\t\t\t    (char *)range_names[i]);\n\t}\n\n\tirq_res.sets = 0;\n\n\t \n\tif (ud->bchan_cnt) {\n\t\trm_res = tisci_rm->rm_ranges[RM_RANGE_BCHAN];\n\t\tif (IS_ERR(rm_res)) {\n\t\t\tbitmap_zero(ud->bchan_map, ud->bchan_cnt);\n\t\t\tirq_res.sets++;\n\t\t} else {\n\t\t\tbitmap_fill(ud->bchan_map, ud->bchan_cnt);\n\t\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\t\tudma_mark_resource_ranges(ud, ud->bchan_map,\n\t\t\t\t\t\t\t  &rm_res->desc[i],\n\t\t\t\t\t\t\t  \"bchan\");\n\t\t\tirq_res.sets += rm_res->sets;\n\t\t}\n\t}\n\n\t \n\tif (ud->tchan_cnt) {\n\t\trm_res = tisci_rm->rm_ranges[RM_RANGE_TCHAN];\n\t\tif (IS_ERR(rm_res)) {\n\t\t\tbitmap_zero(ud->tchan_map, ud->tchan_cnt);\n\t\t\tirq_res.sets += 2;\n\t\t} else {\n\t\t\tbitmap_fill(ud->tchan_map, ud->tchan_cnt);\n\t\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\t\tudma_mark_resource_ranges(ud, ud->tchan_map,\n\t\t\t\t\t\t\t  &rm_res->desc[i],\n\t\t\t\t\t\t\t  \"tchan\");\n\t\t\tirq_res.sets += rm_res->sets * 2;\n\t\t}\n\t}\n\n\t \n\tif (ud->rchan_cnt) {\n\t\trm_res = tisci_rm->rm_ranges[RM_RANGE_RCHAN];\n\t\tif (IS_ERR(rm_res)) {\n\t\t\tbitmap_zero(ud->rchan_map, ud->rchan_cnt);\n\t\t\tirq_res.sets += 2;\n\t\t} else {\n\t\t\tbitmap_fill(ud->rchan_map, ud->rchan_cnt);\n\t\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\t\tudma_mark_resource_ranges(ud, ud->rchan_map,\n\t\t\t\t\t\t\t  &rm_res->desc[i],\n\t\t\t\t\t\t\t  \"rchan\");\n\t\t\tirq_res.sets += rm_res->sets * 2;\n\t\t}\n\t}\n\n\tirq_res.desc = kcalloc(irq_res.sets, sizeof(*irq_res.desc), GFP_KERNEL);\n\tif (!irq_res.desc)\n\t\treturn -ENOMEM;\n\tif (ud->bchan_cnt) {\n\t\trm_res = tisci_rm->rm_ranges[RM_RANGE_BCHAN];\n\t\tif (IS_ERR(rm_res)) {\n\t\t\tirq_res.desc[0].start = oes->bcdma_bchan_ring;\n\t\t\tirq_res.desc[0].num = ud->bchan_cnt;\n\t\t\ti = 1;\n\t\t} else {\n\t\t\tfor (i = 0; i < rm_res->sets; i++) {\n\t\t\t\tirq_res.desc[i].start = rm_res->desc[i].start +\n\t\t\t\t\t\t\toes->bcdma_bchan_ring;\n\t\t\t\tirq_res.desc[i].num = rm_res->desc[i].num;\n\t\t\t}\n\t\t}\n\t} else {\n\t\ti = 0;\n\t}\n\n\tif (ud->tchan_cnt) {\n\t\trm_res = tisci_rm->rm_ranges[RM_RANGE_TCHAN];\n\t\tif (IS_ERR(rm_res)) {\n\t\t\tirq_res.desc[i].start = oes->bcdma_tchan_data;\n\t\t\tirq_res.desc[i].num = ud->tchan_cnt;\n\t\t\tirq_res.desc[i + 1].start = oes->bcdma_tchan_ring;\n\t\t\tirq_res.desc[i + 1].num = ud->tchan_cnt;\n\t\t\ti += 2;\n\t\t} else {\n\t\t\tfor (j = 0; j < rm_res->sets; j++, i += 2) {\n\t\t\t\tirq_res.desc[i].start = rm_res->desc[j].start +\n\t\t\t\t\t\t\toes->bcdma_tchan_data;\n\t\t\t\tirq_res.desc[i].num = rm_res->desc[j].num;\n\n\t\t\t\tirq_res.desc[i + 1].start = rm_res->desc[j].start +\n\t\t\t\t\t\t\toes->bcdma_tchan_ring;\n\t\t\t\tirq_res.desc[i + 1].num = rm_res->desc[j].num;\n\t\t\t}\n\t\t}\n\t}\n\tif (ud->rchan_cnt) {\n\t\trm_res = tisci_rm->rm_ranges[RM_RANGE_RCHAN];\n\t\tif (IS_ERR(rm_res)) {\n\t\t\tirq_res.desc[i].start = oes->bcdma_rchan_data;\n\t\t\tirq_res.desc[i].num = ud->rchan_cnt;\n\t\t\tirq_res.desc[i + 1].start = oes->bcdma_rchan_ring;\n\t\t\tirq_res.desc[i + 1].num = ud->rchan_cnt;\n\t\t\ti += 2;\n\t\t} else {\n\t\t\tfor (j = 0; j < rm_res->sets; j++, i += 2) {\n\t\t\t\tirq_res.desc[i].start = rm_res->desc[j].start +\n\t\t\t\t\t\t\toes->bcdma_rchan_data;\n\t\t\t\tirq_res.desc[i].num = rm_res->desc[j].num;\n\n\t\t\t\tirq_res.desc[i + 1].start = rm_res->desc[j].start +\n\t\t\t\t\t\t\toes->bcdma_rchan_ring;\n\t\t\t\tirq_res.desc[i + 1].num = rm_res->desc[j].num;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = ti_sci_inta_msi_domain_alloc_irqs(ud->dev, &irq_res);\n\tkfree(irq_res.desc);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"Failed to allocate MSI interrupts\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int pktdma_setup_resources(struct udma_dev *ud)\n{\n\tint ret, i, j;\n\tstruct device *dev = ud->dev;\n\tstruct ti_sci_resource *rm_res, irq_res;\n\tstruct udma_tisci_rm *tisci_rm = &ud->tisci_rm;\n\tconst struct udma_oes_offsets *oes = &ud->soc_data->oes;\n\tu32 cap3;\n\n\t \n\tcap3 = udma_read(ud->mmrs[MMR_GCFG], 0x2c);\n\tif (UDMA_CAP3_UCHAN_CNT(cap3)) {\n\t\tud->tchan_tpl.levels = 3;\n\t\tud->tchan_tpl.start_idx[1] = UDMA_CAP3_UCHAN_CNT(cap3);\n\t\tud->tchan_tpl.start_idx[0] = UDMA_CAP3_HCHAN_CNT(cap3);\n\t} else if (UDMA_CAP3_HCHAN_CNT(cap3)) {\n\t\tud->tchan_tpl.levels = 2;\n\t\tud->tchan_tpl.start_idx[0] = UDMA_CAP3_HCHAN_CNT(cap3);\n\t} else {\n\t\tud->tchan_tpl.levels = 1;\n\t}\n\n\tud->rchan_tpl.levels = ud->tchan_tpl.levels;\n\tud->rchan_tpl.start_idx[0] = ud->tchan_tpl.start_idx[0];\n\tud->rchan_tpl.start_idx[1] = ud->tchan_tpl.start_idx[1];\n\n\tud->tchan_map = devm_kmalloc_array(dev, BITS_TO_LONGS(ud->tchan_cnt),\n\t\t\t\t\t   sizeof(unsigned long), GFP_KERNEL);\n\tud->tchans = devm_kcalloc(dev, ud->tchan_cnt, sizeof(*ud->tchans),\n\t\t\t\t  GFP_KERNEL);\n\tud->rchan_map = devm_kmalloc_array(dev, BITS_TO_LONGS(ud->rchan_cnt),\n\t\t\t\t\t   sizeof(unsigned long), GFP_KERNEL);\n\tud->rchans = devm_kcalloc(dev, ud->rchan_cnt, sizeof(*ud->rchans),\n\t\t\t\t  GFP_KERNEL);\n\tud->rflow_in_use = devm_kcalloc(dev, BITS_TO_LONGS(ud->rflow_cnt),\n\t\t\t\t\tsizeof(unsigned long),\n\t\t\t\t\tGFP_KERNEL);\n\tud->rflows = devm_kcalloc(dev, ud->rflow_cnt, sizeof(*ud->rflows),\n\t\t\t\t  GFP_KERNEL);\n\tud->tflow_map = devm_kmalloc_array(dev, BITS_TO_LONGS(ud->tflow_cnt),\n\t\t\t\t\t   sizeof(unsigned long), GFP_KERNEL);\n\n\tif (!ud->tchan_map || !ud->rchan_map || !ud->tflow_map || !ud->tchans ||\n\t    !ud->rchans || !ud->rflows || !ud->rflow_in_use)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < RM_RANGE_LAST; i++) {\n\t\tif (i == RM_RANGE_BCHAN)\n\t\t\tcontinue;\n\n\t\ttisci_rm->rm_ranges[i] =\n\t\t\tdevm_ti_sci_get_of_resource(tisci_rm->tisci, dev,\n\t\t\t\t\t\t    tisci_rm->tisci_dev_id,\n\t\t\t\t\t\t    (char *)range_names[i]);\n\t}\n\n\t \n\trm_res = tisci_rm->rm_ranges[RM_RANGE_TCHAN];\n\tif (IS_ERR(rm_res)) {\n\t\tbitmap_zero(ud->tchan_map, ud->tchan_cnt);\n\t} else {\n\t\tbitmap_fill(ud->tchan_map, ud->tchan_cnt);\n\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\tudma_mark_resource_ranges(ud, ud->tchan_map,\n\t\t\t\t\t\t  &rm_res->desc[i], \"tchan\");\n\t}\n\n\t \n\trm_res = tisci_rm->rm_ranges[RM_RANGE_RCHAN];\n\tif (IS_ERR(rm_res)) {\n\t\tbitmap_zero(ud->rchan_map, ud->rchan_cnt);\n\t} else {\n\t\tbitmap_fill(ud->rchan_map, ud->rchan_cnt);\n\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\tudma_mark_resource_ranges(ud, ud->rchan_map,\n\t\t\t\t\t\t  &rm_res->desc[i], \"rchan\");\n\t}\n\n\t \n\trm_res = tisci_rm->rm_ranges[RM_RANGE_RFLOW];\n\tif (IS_ERR(rm_res)) {\n\t\t \n\t\tbitmap_zero(ud->rflow_in_use, ud->rflow_cnt);\n\t\tirq_res.sets = 1;\n\t} else {\n\t\tbitmap_fill(ud->rflow_in_use, ud->rflow_cnt);\n\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\tudma_mark_resource_ranges(ud, ud->rflow_in_use,\n\t\t\t\t\t\t  &rm_res->desc[i], \"rflow\");\n\t\tirq_res.sets = rm_res->sets;\n\t}\n\n\t \n\trm_res = tisci_rm->rm_ranges[RM_RANGE_TFLOW];\n\tif (IS_ERR(rm_res)) {\n\t\t \n\t\tbitmap_zero(ud->tflow_map, ud->tflow_cnt);\n\t\tirq_res.sets++;\n\t} else {\n\t\tbitmap_fill(ud->tflow_map, ud->tflow_cnt);\n\t\tfor (i = 0; i < rm_res->sets; i++)\n\t\t\tudma_mark_resource_ranges(ud, ud->tflow_map,\n\t\t\t\t\t\t  &rm_res->desc[i], \"tflow\");\n\t\tirq_res.sets += rm_res->sets;\n\t}\n\n\tirq_res.desc = kcalloc(irq_res.sets, sizeof(*irq_res.desc), GFP_KERNEL);\n\tif (!irq_res.desc)\n\t\treturn -ENOMEM;\n\trm_res = tisci_rm->rm_ranges[RM_RANGE_TFLOW];\n\tif (IS_ERR(rm_res)) {\n\t\tirq_res.desc[0].start = oes->pktdma_tchan_flow;\n\t\tirq_res.desc[0].num = ud->tflow_cnt;\n\t\ti = 1;\n\t} else {\n\t\tfor (i = 0; i < rm_res->sets; i++) {\n\t\t\tirq_res.desc[i].start = rm_res->desc[i].start +\n\t\t\t\t\t\toes->pktdma_tchan_flow;\n\t\t\tirq_res.desc[i].num = rm_res->desc[i].num;\n\t\t}\n\t}\n\trm_res = tisci_rm->rm_ranges[RM_RANGE_RFLOW];\n\tif (IS_ERR(rm_res)) {\n\t\tirq_res.desc[i].start = oes->pktdma_rchan_flow;\n\t\tirq_res.desc[i].num = ud->rflow_cnt;\n\t} else {\n\t\tfor (j = 0; j < rm_res->sets; j++, i++) {\n\t\t\tirq_res.desc[i].start = rm_res->desc[j].start +\n\t\t\t\t\t\toes->pktdma_rchan_flow;\n\t\t\tirq_res.desc[i].num = rm_res->desc[j].num;\n\t\t}\n\t}\n\tret = ti_sci_inta_msi_domain_alloc_irqs(ud->dev, &irq_res);\n\tkfree(irq_res.desc);\n\tif (ret) {\n\t\tdev_err(ud->dev, \"Failed to allocate MSI interrupts\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int setup_resources(struct udma_dev *ud)\n{\n\tstruct device *dev = ud->dev;\n\tint ch_count, ret;\n\n\tswitch (ud->match_data->type) {\n\tcase DMA_TYPE_UDMA:\n\t\tret = udma_setup_resources(ud);\n\t\tbreak;\n\tcase DMA_TYPE_BCDMA:\n\t\tret = bcdma_setup_resources(ud);\n\t\tbreak;\n\tcase DMA_TYPE_PKTDMA:\n\t\tret = pktdma_setup_resources(ud);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\n\tch_count  = ud->bchan_cnt + ud->tchan_cnt + ud->rchan_cnt;\n\tif (ud->bchan_cnt)\n\t\tch_count -= bitmap_weight(ud->bchan_map, ud->bchan_cnt);\n\tch_count -= bitmap_weight(ud->tchan_map, ud->tchan_cnt);\n\tch_count -= bitmap_weight(ud->rchan_map, ud->rchan_cnt);\n\tif (!ch_count)\n\t\treturn -ENODEV;\n\n\tud->channels = devm_kcalloc(dev, ch_count, sizeof(*ud->channels),\n\t\t\t\t    GFP_KERNEL);\n\tif (!ud->channels)\n\t\treturn -ENOMEM;\n\n\tswitch (ud->match_data->type) {\n\tcase DMA_TYPE_UDMA:\n\t\tdev_info(dev,\n\t\t\t \"Channels: %d (tchan: %u, rchan: %u, gp-rflow: %u)\\n\",\n\t\t\t ch_count,\n\t\t\t ud->tchan_cnt - bitmap_weight(ud->tchan_map,\n\t\t\t\t\t\t       ud->tchan_cnt),\n\t\t\t ud->rchan_cnt - bitmap_weight(ud->rchan_map,\n\t\t\t\t\t\t       ud->rchan_cnt),\n\t\t\t ud->rflow_cnt - bitmap_weight(ud->rflow_gp_map,\n\t\t\t\t\t\t       ud->rflow_cnt));\n\t\tbreak;\n\tcase DMA_TYPE_BCDMA:\n\t\tdev_info(dev,\n\t\t\t \"Channels: %d (bchan: %u, tchan: %u, rchan: %u)\\n\",\n\t\t\t ch_count,\n\t\t\t ud->bchan_cnt - bitmap_weight(ud->bchan_map,\n\t\t\t\t\t\t       ud->bchan_cnt),\n\t\t\t ud->tchan_cnt - bitmap_weight(ud->tchan_map,\n\t\t\t\t\t\t       ud->tchan_cnt),\n\t\t\t ud->rchan_cnt - bitmap_weight(ud->rchan_map,\n\t\t\t\t\t\t       ud->rchan_cnt));\n\t\tbreak;\n\tcase DMA_TYPE_PKTDMA:\n\t\tdev_info(dev,\n\t\t\t \"Channels: %d (tchan: %u, rchan: %u)\\n\",\n\t\t\t ch_count,\n\t\t\t ud->tchan_cnt - bitmap_weight(ud->tchan_map,\n\t\t\t\t\t\t       ud->tchan_cnt),\n\t\t\t ud->rchan_cnt - bitmap_weight(ud->rchan_map,\n\t\t\t\t\t\t       ud->rchan_cnt));\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ch_count;\n}\n\nstatic int udma_setup_rx_flush(struct udma_dev *ud)\n{\n\tstruct udma_rx_flush *rx_flush = &ud->rx_flush;\n\tstruct cppi5_desc_hdr_t *tr_desc;\n\tstruct cppi5_tr_type1_t *tr_req;\n\tstruct cppi5_host_desc_t *desc;\n\tstruct device *dev = ud->dev;\n\tstruct udma_hwdesc *hwdesc;\n\tsize_t tr_size;\n\n\t \n\trx_flush->buffer_size = SZ_1K;\n\trx_flush->buffer_vaddr = devm_kzalloc(dev, rx_flush->buffer_size,\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!rx_flush->buffer_vaddr)\n\t\treturn -ENOMEM;\n\n\trx_flush->buffer_paddr = dma_map_single(dev, rx_flush->buffer_vaddr,\n\t\t\t\t\t\trx_flush->buffer_size,\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, rx_flush->buffer_paddr))\n\t\treturn -ENOMEM;\n\n\t \n\thwdesc = &rx_flush->hwdescs[0];\n\ttr_size = sizeof(struct cppi5_tr_type1_t);\n\thwdesc->cppi5_desc_size = cppi5_trdesc_calc_size(tr_size, 1);\n\thwdesc->cppi5_desc_size = ALIGN(hwdesc->cppi5_desc_size,\n\t\t\t\t\tud->desc_align);\n\n\thwdesc->cppi5_desc_vaddr = devm_kzalloc(dev, hwdesc->cppi5_desc_size,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!hwdesc->cppi5_desc_vaddr)\n\t\treturn -ENOMEM;\n\n\thwdesc->cppi5_desc_paddr = dma_map_single(dev, hwdesc->cppi5_desc_vaddr,\n\t\t\t\t\t\t  hwdesc->cppi5_desc_size,\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, hwdesc->cppi5_desc_paddr))\n\t\treturn -ENOMEM;\n\n\t \n\thwdesc->tr_req_base = hwdesc->cppi5_desc_vaddr + tr_size;\n\t \n\thwdesc->tr_resp_base = hwdesc->tr_req_base + tr_size;\n\n\ttr_desc = hwdesc->cppi5_desc_vaddr;\n\tcppi5_trdesc_init(tr_desc, 1, tr_size, 0, 0);\n\tcppi5_desc_set_pktids(tr_desc, 0, CPPI5_INFO1_DESC_FLOWID_DEFAULT);\n\tcppi5_desc_set_retpolicy(tr_desc, 0, 0);\n\n\ttr_req = hwdesc->tr_req_base;\n\tcppi5_tr_init(&tr_req->flags, CPPI5_TR_TYPE1, false, false,\n\t\t      CPPI5_TR_EVENT_SIZE_COMPLETION, 0);\n\tcppi5_tr_csf_set(&tr_req->flags, CPPI5_TR_CSF_SUPR_EVT);\n\n\ttr_req->addr = rx_flush->buffer_paddr;\n\ttr_req->icnt0 = rx_flush->buffer_size;\n\ttr_req->icnt1 = 1;\n\n\tdma_sync_single_for_device(dev, hwdesc->cppi5_desc_paddr,\n\t\t\t\t   hwdesc->cppi5_desc_size, DMA_TO_DEVICE);\n\n\t \n\thwdesc = &rx_flush->hwdescs[1];\n\thwdesc->cppi5_desc_size = ALIGN(sizeof(struct cppi5_host_desc_t) +\n\t\t\t\t\tCPPI5_INFO0_HDESC_EPIB_SIZE +\n\t\t\t\t\tCPPI5_INFO0_HDESC_PSDATA_MAX_SIZE,\n\t\t\t\t\tud->desc_align);\n\n\thwdesc->cppi5_desc_vaddr = devm_kzalloc(dev, hwdesc->cppi5_desc_size,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!hwdesc->cppi5_desc_vaddr)\n\t\treturn -ENOMEM;\n\n\thwdesc->cppi5_desc_paddr = dma_map_single(dev, hwdesc->cppi5_desc_vaddr,\n\t\t\t\t\t\t  hwdesc->cppi5_desc_size,\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, hwdesc->cppi5_desc_paddr))\n\t\treturn -ENOMEM;\n\n\tdesc = hwdesc->cppi5_desc_vaddr;\n\tcppi5_hdesc_init(desc, 0, 0);\n\tcppi5_desc_set_pktids(&desc->hdr, 0, CPPI5_INFO1_DESC_FLOWID_DEFAULT);\n\tcppi5_desc_set_retpolicy(&desc->hdr, 0, 0);\n\n\tcppi5_hdesc_attach_buf(desc,\n\t\t\t       rx_flush->buffer_paddr, rx_flush->buffer_size,\n\t\t\t       rx_flush->buffer_paddr, rx_flush->buffer_size);\n\n\tdma_sync_single_for_device(dev, hwdesc->cppi5_desc_paddr,\n\t\t\t\t   hwdesc->cppi5_desc_size, DMA_TO_DEVICE);\n\treturn 0;\n}\n\n#ifdef CONFIG_DEBUG_FS\nstatic void udma_dbg_summary_show_chan(struct seq_file *s,\n\t\t\t\t       struct dma_chan *chan)\n{\n\tstruct udma_chan *uc = to_udma_chan(chan);\n\tstruct udma_chan_config *ucc = &uc->config;\n\n\tseq_printf(s, \" %-13s| %s\", dma_chan_name(chan),\n\t\t   chan->dbg_client_name ?: \"in-use\");\n\tif (ucc->tr_trigger_type)\n\t\tseq_puts(s, \" (triggered, \");\n\telse\n\t\tseq_printf(s, \" (%s, \",\n\t\t\t   dmaengine_get_direction_text(uc->config.dir));\n\n\tswitch (uc->config.dir) {\n\tcase DMA_MEM_TO_MEM:\n\t\tif (uc->ud->match_data->type == DMA_TYPE_BCDMA) {\n\t\t\tseq_printf(s, \"bchan%d)\\n\", uc->bchan->id);\n\t\t\treturn;\n\t\t}\n\n\t\tseq_printf(s, \"chan%d pair [0x%04x -> 0x%04x], \", uc->tchan->id,\n\t\t\t   ucc->src_thread, ucc->dst_thread);\n\t\tbreak;\n\tcase DMA_DEV_TO_MEM:\n\t\tseq_printf(s, \"rchan%d [0x%04x -> 0x%04x], \", uc->rchan->id,\n\t\t\t   ucc->src_thread, ucc->dst_thread);\n\t\tif (uc->ud->match_data->type == DMA_TYPE_PKTDMA)\n\t\t\tseq_printf(s, \"rflow%d, \", uc->rflow->id);\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\tseq_printf(s, \"tchan%d [0x%04x -> 0x%04x], \", uc->tchan->id,\n\t\t\t   ucc->src_thread, ucc->dst_thread);\n\t\tif (uc->ud->match_data->type == DMA_TYPE_PKTDMA)\n\t\t\tseq_printf(s, \"tflow%d, \", uc->tchan->tflow_id);\n\t\tbreak;\n\tdefault:\n\t\tseq_printf(s, \")\\n\");\n\t\treturn;\n\t}\n\n\tif (ucc->ep_type == PSIL_EP_NATIVE) {\n\t\tseq_printf(s, \"PSI-L Native\");\n\t\tif (ucc->metadata_size) {\n\t\t\tseq_printf(s, \"[%s\", ucc->needs_epib ? \" EPIB\" : \"\");\n\t\t\tif (ucc->psd_size)\n\t\t\t\tseq_printf(s, \" PSDsize:%u\", ucc->psd_size);\n\t\t\tseq_printf(s, \" ]\");\n\t\t}\n\t} else {\n\t\tseq_printf(s, \"PDMA\");\n\t\tif (ucc->enable_acc32 || ucc->enable_burst)\n\t\t\tseq_printf(s, \"[%s%s ]\",\n\t\t\t\t   ucc->enable_acc32 ? \" ACC32\" : \"\",\n\t\t\t\t   ucc->enable_burst ? \" BURST\" : \"\");\n\t}\n\n\tseq_printf(s, \", %s)\\n\", ucc->pkt_mode ? \"Packet mode\" : \"TR mode\");\n}\n\nstatic void udma_dbg_summary_show(struct seq_file *s,\n\t\t\t\t  struct dma_device *dma_dev)\n{\n\tstruct dma_chan *chan;\n\n\tlist_for_each_entry(chan, &dma_dev->channels, device_node) {\n\t\tif (chan->client_count)\n\t\t\tudma_dbg_summary_show_chan(s, chan);\n\t}\n}\n#endif  \n\nstatic enum dmaengine_alignment udma_get_copy_align(struct udma_dev *ud)\n{\n\tconst struct udma_match_data *match_data = ud->match_data;\n\tu8 tpl;\n\n\tif (!match_data->enable_memcpy_support)\n\t\treturn DMAENGINE_ALIGN_8_BYTES;\n\n\t \n\tif (ud->bchan_cnt)\n\t\ttpl = udma_get_chan_tpl_index(&ud->bchan_tpl, 0);\n\telse if (ud->tchan_cnt)\n\t\ttpl = udma_get_chan_tpl_index(&ud->tchan_tpl, 0);\n\telse\n\t\treturn DMAENGINE_ALIGN_8_BYTES;\n\n\tswitch (match_data->burst_size[tpl]) {\n\tcase TI_SCI_RM_UDMAP_CHAN_BURST_SIZE_256_BYTES:\n\t\treturn DMAENGINE_ALIGN_256_BYTES;\n\tcase TI_SCI_RM_UDMAP_CHAN_BURST_SIZE_128_BYTES:\n\t\treturn DMAENGINE_ALIGN_128_BYTES;\n\tcase TI_SCI_RM_UDMAP_CHAN_BURST_SIZE_64_BYTES:\n\tfallthrough;\n\tdefault:\n\t\treturn DMAENGINE_ALIGN_64_BYTES;\n\t}\n}\n\n#define TI_UDMAC_BUSWIDTHS\t(BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) | \\\n\t\t\t\t BIT(DMA_SLAVE_BUSWIDTH_2_BYTES) | \\\n\t\t\t\t BIT(DMA_SLAVE_BUSWIDTH_3_BYTES) | \\\n\t\t\t\t BIT(DMA_SLAVE_BUSWIDTH_4_BYTES) | \\\n\t\t\t\t BIT(DMA_SLAVE_BUSWIDTH_8_BYTES))\n\nstatic int udma_probe(struct platform_device *pdev)\n{\n\tstruct device_node *navss_node = pdev->dev.parent->of_node;\n\tconst struct soc_device_attribute *soc;\n\tstruct device *dev = &pdev->dev;\n\tstruct udma_dev *ud;\n\tconst struct of_device_id *match;\n\tint i, ret;\n\tint ch_count;\n\n\tret = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(48));\n\tif (ret)\n\t\tdev_err(dev, \"failed to set dma mask stuff\\n\");\n\n\tud = devm_kzalloc(dev, sizeof(*ud), GFP_KERNEL);\n\tif (!ud)\n\t\treturn -ENOMEM;\n\n\tmatch = of_match_node(udma_of_match, dev->of_node);\n\tif (!match) {\n\t\tdev_err(dev, \"No compatible match found\\n\");\n\t\treturn -ENODEV;\n\t}\n\tud->match_data = match->data;\n\n\tud->soc_data = ud->match_data->soc_data;\n\tif (!ud->soc_data) {\n\t\tsoc = soc_device_match(k3_soc_devices);\n\t\tif (!soc) {\n\t\t\tdev_err(dev, \"No compatible SoC found\\n\");\n\t\t\treturn -ENODEV;\n\t\t}\n\t\tud->soc_data = soc->data;\n\t}\n\n\tret = udma_get_mmrs(pdev, ud);\n\tif (ret)\n\t\treturn ret;\n\n\tud->tisci_rm.tisci = ti_sci_get_by_phandle(dev->of_node, \"ti,sci\");\n\tif (IS_ERR(ud->tisci_rm.tisci))\n\t\treturn PTR_ERR(ud->tisci_rm.tisci);\n\n\tret = of_property_read_u32(dev->of_node, \"ti,sci-dev-id\",\n\t\t\t\t   &ud->tisci_rm.tisci_dev_id);\n\tif (ret) {\n\t\tdev_err(dev, \"ti,sci-dev-id read failure %d\\n\", ret);\n\t\treturn ret;\n\t}\n\tpdev->id = ud->tisci_rm.tisci_dev_id;\n\n\tret = of_property_read_u32(navss_node, \"ti,sci-dev-id\",\n\t\t\t\t   &ud->tisci_rm.tisci_navss_dev_id);\n\tif (ret) {\n\t\tdev_err(dev, \"NAVSS ti,sci-dev-id read failure %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tif (ud->match_data->type == DMA_TYPE_UDMA) {\n\t\tret = of_property_read_u32(dev->of_node, \"ti,udma-atype\",\n\t\t\t\t\t   &ud->atype);\n\t\tif (!ret && ud->atype > 2) {\n\t\t\tdev_err(dev, \"Invalid atype: %u\\n\", ud->atype);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tret = of_property_read_u32(dev->of_node, \"ti,asel\",\n\t\t\t\t\t   &ud->asel);\n\t\tif (!ret && ud->asel > 15) {\n\t\t\tdev_err(dev, \"Invalid asel: %u\\n\", ud->asel);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tud->tisci_rm.tisci_udmap_ops = &ud->tisci_rm.tisci->ops.rm_udmap_ops;\n\tud->tisci_rm.tisci_psil_ops = &ud->tisci_rm.tisci->ops.rm_psil_ops;\n\n\tif (ud->match_data->type == DMA_TYPE_UDMA) {\n\t\tud->ringacc = of_k3_ringacc_get_by_phandle(dev->of_node, \"ti,ringacc\");\n\t} else {\n\t\tstruct k3_ringacc_init_data ring_init_data;\n\n\t\tring_init_data.tisci = ud->tisci_rm.tisci;\n\t\tring_init_data.tisci_dev_id = ud->tisci_rm.tisci_dev_id;\n\t\tif (ud->match_data->type == DMA_TYPE_BCDMA) {\n\t\t\tring_init_data.num_rings = ud->bchan_cnt +\n\t\t\t\t\t\t   ud->tchan_cnt +\n\t\t\t\t\t\t   ud->rchan_cnt;\n\t\t} else {\n\t\t\tring_init_data.num_rings = ud->rflow_cnt +\n\t\t\t\t\t\t   ud->tflow_cnt;\n\t\t}\n\n\t\tud->ringacc = k3_ringacc_dmarings_init(pdev, &ring_init_data);\n\t}\n\n\tif (IS_ERR(ud->ringacc))\n\t\treturn PTR_ERR(ud->ringacc);\n\n\tdev->msi.domain = of_msi_get_domain(dev, dev->of_node,\n\t\t\t\t\t    DOMAIN_BUS_TI_SCI_INTA_MSI);\n\tif (!dev->msi.domain) {\n\t\treturn -EPROBE_DEFER;\n\t}\n\n\tdma_cap_set(DMA_SLAVE, ud->ddev.cap_mask);\n\t \n\tif (ud->match_data->type != DMA_TYPE_PKTDMA) {\n\t\tdma_cap_set(DMA_CYCLIC, ud->ddev.cap_mask);\n\t\tud->ddev.device_prep_dma_cyclic = udma_prep_dma_cyclic;\n\t}\n\n\tud->ddev.device_config = udma_slave_config;\n\tud->ddev.device_prep_slave_sg = udma_prep_slave_sg;\n\tud->ddev.device_issue_pending = udma_issue_pending;\n\tud->ddev.device_tx_status = udma_tx_status;\n\tud->ddev.device_pause = udma_pause;\n\tud->ddev.device_resume = udma_resume;\n\tud->ddev.device_terminate_all = udma_terminate_all;\n\tud->ddev.device_synchronize = udma_synchronize;\n#ifdef CONFIG_DEBUG_FS\n\tud->ddev.dbg_summary_show = udma_dbg_summary_show;\n#endif\n\n\tswitch (ud->match_data->type) {\n\tcase DMA_TYPE_UDMA:\n\t\tud->ddev.device_alloc_chan_resources =\n\t\t\t\t\tudma_alloc_chan_resources;\n\t\tbreak;\n\tcase DMA_TYPE_BCDMA:\n\t\tud->ddev.device_alloc_chan_resources =\n\t\t\t\t\tbcdma_alloc_chan_resources;\n\t\tud->ddev.device_router_config = bcdma_router_config;\n\t\tbreak;\n\tcase DMA_TYPE_PKTDMA:\n\t\tud->ddev.device_alloc_chan_resources =\n\t\t\t\t\tpktdma_alloc_chan_resources;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tud->ddev.device_free_chan_resources = udma_free_chan_resources;\n\n\tud->ddev.src_addr_widths = TI_UDMAC_BUSWIDTHS;\n\tud->ddev.dst_addr_widths = TI_UDMAC_BUSWIDTHS;\n\tud->ddev.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\n\tud->ddev.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\n\tud->ddev.desc_metadata_modes = DESC_METADATA_CLIENT |\n\t\t\t\t       DESC_METADATA_ENGINE;\n\tif (ud->match_data->enable_memcpy_support &&\n\t    !(ud->match_data->type == DMA_TYPE_BCDMA && ud->bchan_cnt == 0)) {\n\t\tdma_cap_set(DMA_MEMCPY, ud->ddev.cap_mask);\n\t\tud->ddev.device_prep_dma_memcpy = udma_prep_dma_memcpy;\n\t\tud->ddev.directions |= BIT(DMA_MEM_TO_MEM);\n\t}\n\n\tud->ddev.dev = dev;\n\tud->dev = dev;\n\tud->psil_base = ud->match_data->psil_base;\n\n\tINIT_LIST_HEAD(&ud->ddev.channels);\n\tINIT_LIST_HEAD(&ud->desc_to_purge);\n\n\tch_count = setup_resources(ud);\n\tif (ch_count <= 0)\n\t\treturn ch_count;\n\n\tspin_lock_init(&ud->lock);\n\tINIT_WORK(&ud->purge_work, udma_purge_desc_work);\n\n\tud->desc_align = 64;\n\tif (ud->desc_align < dma_get_cache_alignment())\n\t\tud->desc_align = dma_get_cache_alignment();\n\n\tret = udma_setup_rx_flush(ud);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < ud->bchan_cnt; i++) {\n\t\tstruct udma_bchan *bchan = &ud->bchans[i];\n\n\t\tbchan->id = i;\n\t\tbchan->reg_rt = ud->mmrs[MMR_BCHANRT] + i * 0x1000;\n\t}\n\n\tfor (i = 0; i < ud->tchan_cnt; i++) {\n\t\tstruct udma_tchan *tchan = &ud->tchans[i];\n\n\t\ttchan->id = i;\n\t\ttchan->reg_rt = ud->mmrs[MMR_TCHANRT] + i * 0x1000;\n\t}\n\n\tfor (i = 0; i < ud->rchan_cnt; i++) {\n\t\tstruct udma_rchan *rchan = &ud->rchans[i];\n\n\t\trchan->id = i;\n\t\trchan->reg_rt = ud->mmrs[MMR_RCHANRT] + i * 0x1000;\n\t}\n\n\tfor (i = 0; i < ud->rflow_cnt; i++) {\n\t\tstruct udma_rflow *rflow = &ud->rflows[i];\n\n\t\trflow->id = i;\n\t}\n\n\tfor (i = 0; i < ch_count; i++) {\n\t\tstruct udma_chan *uc = &ud->channels[i];\n\n\t\tuc->ud = ud;\n\t\tuc->vc.desc_free = udma_desc_free;\n\t\tuc->id = i;\n\t\tuc->bchan = NULL;\n\t\tuc->tchan = NULL;\n\t\tuc->rchan = NULL;\n\t\tuc->config.remote_thread_id = -1;\n\t\tuc->config.mapped_channel_id = -1;\n\t\tuc->config.default_flow_id = -1;\n\t\tuc->config.dir = DMA_MEM_TO_MEM;\n\t\tuc->name = devm_kasprintf(dev, GFP_KERNEL, \"%s chan%d\",\n\t\t\t\t\t  dev_name(dev), i);\n\n\t\tvchan_init(&uc->vc, &ud->ddev);\n\t\t \n\t\ttasklet_setup(&uc->vc.task, udma_vchan_complete);\n\t\tinit_completion(&uc->teardown_completed);\n\t\tINIT_DELAYED_WORK(&uc->tx_drain.work, udma_check_tx_completion);\n\t}\n\n\t \n\tud->ddev.copy_align = udma_get_copy_align(ud);\n\n\tret = dma_async_device_register(&ud->ddev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to register slave DMA engine: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tplatform_set_drvdata(pdev, ud);\n\n\tret = of_dma_controller_register(dev->of_node, udma_of_xlate, ud);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to register of_dma controller\\n\");\n\t\tdma_async_device_unregister(&ud->ddev);\n\t}\n\n\treturn ret;\n}\n\nstatic int __maybe_unused udma_pm_suspend(struct device *dev)\n{\n\tstruct udma_dev *ud = dev_get_drvdata(dev);\n\tstruct dma_device *dma_dev = &ud->ddev;\n\tstruct dma_chan *chan;\n\tstruct udma_chan *uc;\n\n\tlist_for_each_entry(chan, &dma_dev->channels, device_node) {\n\t\tif (chan->client_count) {\n\t\t\tuc = to_udma_chan(chan);\n\t\t\t \n\t\t\tmemcpy(&uc->backup_config, &uc->config,\n\t\t\t       sizeof(struct udma_chan_config));\n\t\t\tdev_dbg(dev, \"Suspending channel %s\\n\",\n\t\t\t\tdma_chan_name(chan));\n\t\t\tud->ddev.device_free_chan_resources(chan);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int __maybe_unused udma_pm_resume(struct device *dev)\n{\n\tstruct udma_dev *ud = dev_get_drvdata(dev);\n\tstruct dma_device *dma_dev = &ud->ddev;\n\tstruct dma_chan *chan;\n\tstruct udma_chan *uc;\n\tint ret;\n\n\tlist_for_each_entry(chan, &dma_dev->channels, device_node) {\n\t\tif (chan->client_count) {\n\t\t\tuc = to_udma_chan(chan);\n\t\t\t \n\t\t\tmemcpy(&uc->config, &uc->backup_config,\n\t\t\t       sizeof(struct udma_chan_config));\n\t\t\tdev_dbg(dev, \"Resuming channel %s\\n\",\n\t\t\t\tdma_chan_name(chan));\n\t\t\tret = ud->ddev.device_alloc_chan_resources(chan);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops udma_pm_ops = {\n\tSET_LATE_SYSTEM_SLEEP_PM_OPS(udma_pm_suspend, udma_pm_resume)\n};\n\nstatic struct platform_driver udma_driver = {\n\t.driver = {\n\t\t.name\t= \"ti-udma\",\n\t\t.of_match_table = udma_of_match,\n\t\t.suppress_bind_attrs = true,\n\t\t.pm = &udma_pm_ops,\n\t},\n\t.probe\t\t= udma_probe,\n};\n\nmodule_platform_driver(udma_driver);\nMODULE_LICENSE(\"GPL v2\");\n\n \n#include \"k3-udma-private.c\"\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}