{
  "module_name": "mmp_tdma.c",
  "hash_id": "2ae78e89f5d838dc323693cb4470cd383b30abb29530921d6b9897932ab8160d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/mmp_tdma.c",
  "human_readable_source": "\n \n\n#include <linux/err.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/interrupt.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/dmaengine.h>\n#include <linux/platform_device.h>\n#include <linux/device.h>\n#include <linux/genalloc.h>\n#include <linux/of_device.h>\n#include <linux/of_dma.h>\n\n#include \"dmaengine.h\"\n\n \n#define TDBCR\t\t0x00\t \n#define TDSAR\t\t0x10\t \n#define TDDAR\t\t0x20\t \n#define TDNDPR\t\t0x30\t \n#define TDCR\t\t0x40\t \n#define TDCP\t\t0x60\t \n#define TDCDPR\t\t0x70\t \n#define TDIMR\t\t0x80\t \n#define TDISR\t\t0xa0\t \n\n \n#define TDCR_SSZ_8_BITS\t\t(0x0 << 22)\t \n#define TDCR_SSZ_12_BITS\t(0x1 << 22)\n#define TDCR_SSZ_16_BITS\t(0x2 << 22)\n#define TDCR_SSZ_20_BITS\t(0x3 << 22)\n#define TDCR_SSZ_24_BITS\t(0x4 << 22)\n#define TDCR_SSZ_32_BITS\t(0x5 << 22)\n#define TDCR_SSZ_SHIFT\t\t(0x1 << 22)\n#define TDCR_SSZ_MASK\t\t(0x7 << 22)\n#define TDCR_SSPMOD\t\t(0x1 << 21)\t \n#define TDCR_ABR\t\t(0x1 << 20)\t \n#define TDCR_CDE\t\t(0x1 << 17)\t \n#define TDCR_PACKMOD\t\t(0x1 << 16)\t \n#define TDCR_CHANACT\t\t(0x1 << 14)\t \n#define TDCR_FETCHND\t\t(0x1 << 13)\t \n#define TDCR_CHANEN\t\t(0x1 << 12)\t \n#define TDCR_INTMODE\t\t(0x1 << 10)\t \n#define TDCR_CHAINMOD\t\t(0x1 << 9)\t \n#define TDCR_BURSTSZ_MSK\t(0x7 << 6)\t \n#define TDCR_BURSTSZ_4B\t\t(0x0 << 6)\n#define TDCR_BURSTSZ_8B\t\t(0x1 << 6)\n#define TDCR_BURSTSZ_16B\t(0x3 << 6)\n#define TDCR_BURSTSZ_32B\t(0x6 << 6)\n#define TDCR_BURSTSZ_64B\t(0x7 << 6)\n#define TDCR_BURSTSZ_SQU_1B\t\t(0x5 << 6)\n#define TDCR_BURSTSZ_SQU_2B\t\t(0x6 << 6)\n#define TDCR_BURSTSZ_SQU_4B\t\t(0x0 << 6)\n#define TDCR_BURSTSZ_SQU_8B\t\t(0x1 << 6)\n#define TDCR_BURSTSZ_SQU_16B\t(0x3 << 6)\n#define TDCR_BURSTSZ_SQU_32B\t(0x7 << 6)\n#define TDCR_BURSTSZ_128B\t(0x5 << 6)\n#define TDCR_DSTDIR_MSK\t\t(0x3 << 4)\t \n#define TDCR_DSTDIR_ADDR_HOLD\t(0x2 << 4)\t \n#define TDCR_DSTDIR_ADDR_INC\t(0x0 << 4)\t \n#define TDCR_SRCDIR_MSK\t\t(0x3 << 2)\t \n#define TDCR_SRCDIR_ADDR_HOLD\t(0x2 << 2)\t \n#define TDCR_SRCDIR_ADDR_INC\t(0x0 << 2)\t \n#define TDCR_DSTDESCCONT\t(0x1 << 1)\n#define TDCR_SRCDESTCONT\t(0x1 << 0)\n\n \n#define TDIMR_COMP\t\t(0x1 << 0)\n\n \n#define TDISR_COMP\t\t(0x1 << 0)\n\n \nstruct mmp_tdma_desc {\n\tu32 byte_cnt;\n\tu32 src_addr;\n\tu32 dst_addr;\n\tu32 nxt_desc;\n};\n\nenum mmp_tdma_type {\n\tMMP_AUD_TDMA = 0,\n\tPXA910_SQU,\n};\n\n#define TDMA_MAX_XFER_BYTES    SZ_64K\n\nstruct mmp_tdma_chan {\n\tstruct device\t\t\t*dev;\n\tstruct dma_chan\t\t\tchan;\n\tstruct dma_async_tx_descriptor\tdesc;\n\tstruct tasklet_struct\t\ttasklet;\n\n\tstruct mmp_tdma_desc\t\t*desc_arr;\n\tdma_addr_t\t\t\tdesc_arr_phys;\n\tint\t\t\t\tdesc_num;\n\tenum dma_transfer_direction\tdir;\n\tdma_addr_t\t\t\tdev_addr;\n\tu32\t\t\t\tburst_sz;\n\tenum dma_slave_buswidth\t\tbuswidth;\n\tenum dma_status\t\t\tstatus;\n\tstruct dma_slave_config\t\tslave_config;\n\n\tint\t\t\t\tidx;\n\tenum mmp_tdma_type\t\ttype;\n\tint\t\t\t\tirq;\n\tvoid __iomem\t\t\t*reg_base;\n\n\tsize_t\t\t\t\tbuf_len;\n\tsize_t\t\t\t\tperiod_len;\n\tsize_t\t\t\t\tpos;\n\n\tstruct gen_pool\t\t\t*pool;\n};\n\n#define TDMA_CHANNEL_NUM 2\nstruct mmp_tdma_device {\n\tstruct device\t\t\t*dev;\n\tvoid __iomem\t\t\t*base;\n\tstruct dma_device\t\tdevice;\n\tstruct mmp_tdma_chan\t\t*tdmac[TDMA_CHANNEL_NUM];\n};\n\n#define to_mmp_tdma_chan(dchan) container_of(dchan, struct mmp_tdma_chan, chan)\n\nstatic int mmp_tdma_config_write(struct dma_chan *chan,\n\t\t\t\t enum dma_transfer_direction dir,\n\t\t\t\t struct dma_slave_config *dmaengine_cfg);\n\nstatic void mmp_tdma_chan_set_desc(struct mmp_tdma_chan *tdmac, dma_addr_t phys)\n{\n\twritel(phys, tdmac->reg_base + TDNDPR);\n\twritel(readl(tdmac->reg_base + TDCR) | TDCR_FETCHND,\n\t\t\t\t\ttdmac->reg_base + TDCR);\n}\n\nstatic void mmp_tdma_enable_irq(struct mmp_tdma_chan *tdmac, bool enable)\n{\n\tif (enable)\n\t\twritel(TDIMR_COMP, tdmac->reg_base + TDIMR);\n\telse\n\t\twritel(0, tdmac->reg_base + TDIMR);\n}\n\nstatic void mmp_tdma_enable_chan(struct mmp_tdma_chan *tdmac)\n{\n\t \n\twritel(readl(tdmac->reg_base + TDCR) | TDCR_CHANEN,\n\t\t\t\t\ttdmac->reg_base + TDCR);\n\ttdmac->status = DMA_IN_PROGRESS;\n}\n\nstatic int mmp_tdma_disable_chan(struct dma_chan *chan)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\tu32 tdcr;\n\n\ttdcr = readl(tdmac->reg_base + TDCR);\n\ttdcr |= TDCR_ABR;\n\ttdcr &= ~TDCR_CHANEN;\n\twritel(tdcr, tdmac->reg_base + TDCR);\n\n\ttdmac->status = DMA_COMPLETE;\n\n\treturn 0;\n}\n\nstatic int mmp_tdma_resume_chan(struct dma_chan *chan)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\n\twritel(readl(tdmac->reg_base + TDCR) | TDCR_CHANEN,\n\t\t\t\t\ttdmac->reg_base + TDCR);\n\ttdmac->status = DMA_IN_PROGRESS;\n\n\treturn 0;\n}\n\nstatic int mmp_tdma_pause_chan(struct dma_chan *chan)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\n\twritel(readl(tdmac->reg_base + TDCR) & ~TDCR_CHANEN,\n\t\t\t\t\ttdmac->reg_base + TDCR);\n\ttdmac->status = DMA_PAUSED;\n\n\treturn 0;\n}\n\nstatic int mmp_tdma_config_chan(struct dma_chan *chan)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\tunsigned int tdcr = 0;\n\n\tmmp_tdma_disable_chan(chan);\n\n\tif (tdmac->dir == DMA_MEM_TO_DEV)\n\t\ttdcr = TDCR_DSTDIR_ADDR_HOLD | TDCR_SRCDIR_ADDR_INC;\n\telse if (tdmac->dir == DMA_DEV_TO_MEM)\n\t\ttdcr = TDCR_SRCDIR_ADDR_HOLD | TDCR_DSTDIR_ADDR_INC;\n\n\tif (tdmac->type == MMP_AUD_TDMA) {\n\t\ttdcr |= TDCR_PACKMOD;\n\n\t\tswitch (tdmac->burst_sz) {\n\t\tcase 4:\n\t\t\ttdcr |= TDCR_BURSTSZ_4B;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\ttdcr |= TDCR_BURSTSZ_8B;\n\t\t\tbreak;\n\t\tcase 16:\n\t\t\ttdcr |= TDCR_BURSTSZ_16B;\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\ttdcr |= TDCR_BURSTSZ_32B;\n\t\t\tbreak;\n\t\tcase 64:\n\t\t\ttdcr |= TDCR_BURSTSZ_64B;\n\t\t\tbreak;\n\t\tcase 128:\n\t\t\ttdcr |= TDCR_BURSTSZ_128B;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(tdmac->dev, \"unknown burst size.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tswitch (tdmac->buswidth) {\n\t\tcase DMA_SLAVE_BUSWIDTH_1_BYTE:\n\t\t\ttdcr |= TDCR_SSZ_8_BITS;\n\t\t\tbreak;\n\t\tcase DMA_SLAVE_BUSWIDTH_2_BYTES:\n\t\t\ttdcr |= TDCR_SSZ_16_BITS;\n\t\t\tbreak;\n\t\tcase DMA_SLAVE_BUSWIDTH_4_BYTES:\n\t\t\ttdcr |= TDCR_SSZ_32_BITS;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(tdmac->dev, \"unknown bus size.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (tdmac->type == PXA910_SQU) {\n\t\ttdcr |= TDCR_SSPMOD;\n\n\t\tswitch (tdmac->burst_sz) {\n\t\tcase 1:\n\t\t\ttdcr |= TDCR_BURSTSZ_SQU_1B;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\ttdcr |= TDCR_BURSTSZ_SQU_2B;\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\ttdcr |= TDCR_BURSTSZ_SQU_4B;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\ttdcr |= TDCR_BURSTSZ_SQU_8B;\n\t\t\tbreak;\n\t\tcase 16:\n\t\t\ttdcr |= TDCR_BURSTSZ_SQU_16B;\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\ttdcr |= TDCR_BURSTSZ_SQU_32B;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(tdmac->dev, \"unknown burst size.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\twritel(tdcr, tdmac->reg_base + TDCR);\n\treturn 0;\n}\n\nstatic int mmp_tdma_clear_chan_irq(struct mmp_tdma_chan *tdmac)\n{\n\tu32 reg = readl(tdmac->reg_base + TDISR);\n\n\tif (reg & TDISR_COMP) {\n\t\t \n\t\treg &= ~TDISR_COMP;\n\t\twritel(reg, tdmac->reg_base + TDISR);\n\n\t\treturn 0;\n\t}\n\treturn -EAGAIN;\n}\n\nstatic size_t mmp_tdma_get_pos(struct mmp_tdma_chan *tdmac)\n{\n\tsize_t reg;\n\n\tif (tdmac->idx == 0) {\n\t\treg = __raw_readl(tdmac->reg_base + TDSAR);\n\t\treg -= tdmac->desc_arr[0].src_addr;\n\t} else if (tdmac->idx == 1) {\n\t\treg = __raw_readl(tdmac->reg_base + TDDAR);\n\t\treg -= tdmac->desc_arr[0].dst_addr;\n\t} else\n\t\treturn -EINVAL;\n\n\treturn reg;\n}\n\nstatic irqreturn_t mmp_tdma_chan_handler(int irq, void *dev_id)\n{\n\tstruct mmp_tdma_chan *tdmac = dev_id;\n\n\tif (mmp_tdma_clear_chan_irq(tdmac) == 0) {\n\t\ttasklet_schedule(&tdmac->tasklet);\n\t\treturn IRQ_HANDLED;\n\t} else\n\t\treturn IRQ_NONE;\n}\n\nstatic irqreturn_t mmp_tdma_int_handler(int irq, void *dev_id)\n{\n\tstruct mmp_tdma_device *tdev = dev_id;\n\tint i, ret;\n\tint irq_num = 0;\n\n\tfor (i = 0; i < TDMA_CHANNEL_NUM; i++) {\n\t\tstruct mmp_tdma_chan *tdmac = tdev->tdmac[i];\n\n\t\tret = mmp_tdma_chan_handler(irq, tdmac);\n\t\tif (ret == IRQ_HANDLED)\n\t\t\tirq_num++;\n\t}\n\n\tif (irq_num)\n\t\treturn IRQ_HANDLED;\n\telse\n\t\treturn IRQ_NONE;\n}\n\nstatic void dma_do_tasklet(struct tasklet_struct *t)\n{\n\tstruct mmp_tdma_chan *tdmac = from_tasklet(tdmac, t, tasklet);\n\n\tdmaengine_desc_get_callback_invoke(&tdmac->desc, NULL);\n}\n\nstatic void mmp_tdma_free_descriptor(struct mmp_tdma_chan *tdmac)\n{\n\tstruct gen_pool *gpool;\n\tint size = tdmac->desc_num * sizeof(struct mmp_tdma_desc);\n\n\tgpool = tdmac->pool;\n\tif (gpool && tdmac->desc_arr)\n\t\tgen_pool_free(gpool, (unsigned long)tdmac->desc_arr,\n\t\t\t\tsize);\n\ttdmac->desc_arr = NULL;\n\tif (tdmac->status == DMA_ERROR)\n\t\ttdmac->status = DMA_COMPLETE;\n\n\treturn;\n}\n\nstatic dma_cookie_t mmp_tdma_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(tx->chan);\n\n\tmmp_tdma_chan_set_desc(tdmac, tdmac->desc_arr_phys);\n\n\treturn 0;\n}\n\nstatic int mmp_tdma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\tint ret;\n\n\tdma_async_tx_descriptor_init(&tdmac->desc, chan);\n\ttdmac->desc.tx_submit = mmp_tdma_tx_submit;\n\n\tif (tdmac->irq) {\n\t\tret = devm_request_irq(tdmac->dev, tdmac->irq,\n\t\t\tmmp_tdma_chan_handler, 0, \"tdma\", tdmac);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 1;\n}\n\nstatic void mmp_tdma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\n\tif (tdmac->irq)\n\t\tdevm_free_irq(tdmac->dev, tdmac->irq, tdmac);\n\tmmp_tdma_free_descriptor(tdmac);\n\treturn;\n}\n\nstatic struct mmp_tdma_desc *mmp_tdma_alloc_descriptor(struct mmp_tdma_chan *tdmac)\n{\n\tstruct gen_pool *gpool;\n\tint size = tdmac->desc_num * sizeof(struct mmp_tdma_desc);\n\n\tgpool = tdmac->pool;\n\tif (!gpool)\n\t\treturn NULL;\n\n\ttdmac->desc_arr = gen_pool_dma_alloc(gpool, size, &tdmac->desc_arr_phys);\n\n\treturn tdmac->desc_arr;\n}\n\nstatic struct dma_async_tx_descriptor *mmp_tdma_prep_dma_cyclic(\n\t\tstruct dma_chan *chan, dma_addr_t dma_addr, size_t buf_len,\n\t\tsize_t period_len, enum dma_transfer_direction direction,\n\t\tunsigned long flags)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\tstruct mmp_tdma_desc *desc;\n\tint num_periods = buf_len / period_len;\n\tint i = 0, buf = 0;\n\n\tif (!is_slave_direction(direction)) {\n\t\tdev_err(tdmac->dev, \"unsupported transfer direction\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (tdmac->status != DMA_COMPLETE) {\n\t\tdev_err(tdmac->dev, \"controller busy\");\n\t\treturn NULL;\n\t}\n\n\tif (period_len > TDMA_MAX_XFER_BYTES) {\n\t\tdev_err(tdmac->dev,\n\t\t\t\t\"maximum period size exceeded: %zu > %d\\n\",\n\t\t\t\tperiod_len, TDMA_MAX_XFER_BYTES);\n\t\tgoto err_out;\n\t}\n\n\ttdmac->status = DMA_IN_PROGRESS;\n\ttdmac->desc_num = num_periods;\n\tdesc = mmp_tdma_alloc_descriptor(tdmac);\n\tif (!desc)\n\t\tgoto err_out;\n\n\tif (mmp_tdma_config_write(chan, direction, &tdmac->slave_config))\n\t\tgoto err_out;\n\n\twhile (buf < buf_len) {\n\t\tdesc = &tdmac->desc_arr[i];\n\n\t\tif (i + 1 == num_periods)\n\t\t\tdesc->nxt_desc = tdmac->desc_arr_phys;\n\t\telse\n\t\t\tdesc->nxt_desc = tdmac->desc_arr_phys +\n\t\t\t\tsizeof(*desc) * (i + 1);\n\n\t\tif (direction == DMA_MEM_TO_DEV) {\n\t\t\tdesc->src_addr = dma_addr;\n\t\t\tdesc->dst_addr = tdmac->dev_addr;\n\t\t} else {\n\t\t\tdesc->src_addr = tdmac->dev_addr;\n\t\t\tdesc->dst_addr = dma_addr;\n\t\t}\n\t\tdesc->byte_cnt = period_len;\n\t\tdma_addr += period_len;\n\t\tbuf += period_len;\n\t\ti++;\n\t}\n\n\t \n\tif (flags & DMA_PREP_INTERRUPT)\n\t\tmmp_tdma_enable_irq(tdmac, true);\n\n\ttdmac->buf_len = buf_len;\n\ttdmac->period_len = period_len;\n\ttdmac->pos = 0;\n\n\treturn &tdmac->desc;\n\nerr_out:\n\ttdmac->status = DMA_ERROR;\n\treturn NULL;\n}\n\nstatic int mmp_tdma_terminate_all(struct dma_chan *chan)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\n\tmmp_tdma_disable_chan(chan);\n\t \n\tmmp_tdma_enable_irq(tdmac, false);\n\n\treturn 0;\n}\n\nstatic int mmp_tdma_config(struct dma_chan *chan,\n\t\t\t   struct dma_slave_config *dmaengine_cfg)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\n\tmemcpy(&tdmac->slave_config, dmaengine_cfg, sizeof(*dmaengine_cfg));\n\n\treturn 0;\n}\n\nstatic int mmp_tdma_config_write(struct dma_chan *chan,\n\t\t\t\t enum dma_transfer_direction dir,\n\t\t\t\t struct dma_slave_config *dmaengine_cfg)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\ttdmac->dev_addr = dmaengine_cfg->src_addr;\n\t\ttdmac->burst_sz = dmaengine_cfg->src_maxburst;\n\t\ttdmac->buswidth = dmaengine_cfg->src_addr_width;\n\t} else {\n\t\ttdmac->dev_addr = dmaengine_cfg->dst_addr;\n\t\ttdmac->burst_sz = dmaengine_cfg->dst_maxburst;\n\t\ttdmac->buswidth = dmaengine_cfg->dst_addr_width;\n\t}\n\ttdmac->dir = dir;\n\n\treturn mmp_tdma_config_chan(chan);\n}\n\nstatic enum dma_status mmp_tdma_tx_status(struct dma_chan *chan,\n\t\t\tdma_cookie_t cookie, struct dma_tx_state *txstate)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\n\ttdmac->pos = mmp_tdma_get_pos(tdmac);\n\tdma_set_tx_state(txstate, chan->completed_cookie, chan->cookie,\n\t\t\t tdmac->buf_len - tdmac->pos);\n\n\treturn tdmac->status;\n}\n\nstatic void mmp_tdma_issue_pending(struct dma_chan *chan)\n{\n\tstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\n\n\tmmp_tdma_enable_chan(tdmac);\n}\n\nstatic int mmp_tdma_remove(struct platform_device *pdev)\n{\n\tif (pdev->dev.of_node)\n\t\tof_dma_controller_free(pdev->dev.of_node);\n\n\treturn 0;\n}\n\nstatic int mmp_tdma_chan_init(struct mmp_tdma_device *tdev,\n\t\t\t\t\tint idx, int irq,\n\t\t\t\t\tint type, struct gen_pool *pool)\n{\n\tstruct mmp_tdma_chan *tdmac;\n\n\tif (idx >= TDMA_CHANNEL_NUM) {\n\t\tdev_err(tdev->dev, \"too many channels for device!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\ttdmac = devm_kzalloc(tdev->dev, sizeof(*tdmac), GFP_KERNEL);\n\tif (!tdmac)\n\t\treturn -ENOMEM;\n\n\tif (irq)\n\t\ttdmac->irq = irq;\n\ttdmac->dev\t   = tdev->dev;\n\ttdmac->chan.device = &tdev->device;\n\ttdmac->idx\t   = idx;\n\ttdmac->type\t   = type;\n\ttdmac->reg_base\t   = tdev->base + idx * 4;\n\ttdmac->pool\t   = pool;\n\ttdmac->status = DMA_COMPLETE;\n\ttdev->tdmac[tdmac->idx] = tdmac;\n\ttasklet_setup(&tdmac->tasklet, dma_do_tasklet);\n\n\t \n\tlist_add_tail(&tdmac->chan.device_node,\n\t\t\t&tdev->device.channels);\n\treturn 0;\n}\n\nstruct mmp_tdma_filter_param {\n\tunsigned int chan_id;\n};\n\nstatic bool mmp_tdma_filter_fn(struct dma_chan *chan, void *fn_param)\n{\n\tstruct mmp_tdma_filter_param *param = fn_param;\n\n\tif (chan->chan_id != param->chan_id)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic struct dma_chan *mmp_tdma_xlate(struct of_phandle_args *dma_spec,\n\t\t\t       struct of_dma *ofdma)\n{\n\tstruct mmp_tdma_device *tdev = ofdma->of_dma_data;\n\tdma_cap_mask_t mask = tdev->device.cap_mask;\n\tstruct mmp_tdma_filter_param param;\n\n\tif (dma_spec->args_count != 1)\n\t\treturn NULL;\n\n\tparam.chan_id = dma_spec->args[0];\n\n\tif (param.chan_id >= TDMA_CHANNEL_NUM)\n\t\treturn NULL;\n\n\treturn __dma_request_channel(&mask, mmp_tdma_filter_fn, &param,\n\t\t\t\t     ofdma->of_node);\n}\n\nstatic const struct of_device_id mmp_tdma_dt_ids[] = {\n\t{ .compatible = \"marvell,adma-1.0\", .data = (void *)MMP_AUD_TDMA},\n\t{ .compatible = \"marvell,pxa910-squ\", .data = (void *)PXA910_SQU},\n\t{}\n};\nMODULE_DEVICE_TABLE(of, mmp_tdma_dt_ids);\n\nstatic int mmp_tdma_probe(struct platform_device *pdev)\n{\n\tenum mmp_tdma_type type;\n\tconst struct of_device_id *of_id;\n\tstruct mmp_tdma_device *tdev;\n\tint i, ret;\n\tint irq = 0, irq_num = 0;\n\tint chan_num = TDMA_CHANNEL_NUM;\n\tstruct gen_pool *pool = NULL;\n\n\tof_id = of_match_device(mmp_tdma_dt_ids, &pdev->dev);\n\tif (of_id)\n\t\ttype = (enum mmp_tdma_type) of_id->data;\n\telse\n\t\ttype = platform_get_device_id(pdev)->driver_data;\n\n\t \n\ttdev = devm_kzalloc(&pdev->dev, sizeof(*tdev), GFP_KERNEL);\n\tif (!tdev)\n\t\treturn -ENOMEM;\n\n\ttdev->dev = &pdev->dev;\n\n\tfor (i = 0; i < chan_num; i++) {\n\t\tif (platform_get_irq(pdev, i) > 0)\n\t\t\tirq_num++;\n\t}\n\n\ttdev->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(tdev->base))\n\t\treturn PTR_ERR(tdev->base);\n\n\tINIT_LIST_HEAD(&tdev->device.channels);\n\n\tpool = of_gen_pool_get(pdev->dev.of_node, \"asram\", 0);\n\tif (!pool) {\n\t\tdev_err(&pdev->dev, \"asram pool not available\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (irq_num != chan_num) {\n\t\tirq = platform_get_irq(pdev, 0);\n\t\tret = devm_request_irq(&pdev->dev, irq,\n\t\t\tmmp_tdma_int_handler, IRQF_SHARED, \"tdma\", tdev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tfor (i = 0; i < chan_num; i++) {\n\t\tirq = (irq_num != chan_num) ? 0 : platform_get_irq(pdev, i);\n\t\tret = mmp_tdma_chan_init(tdev, i, irq, type, pool);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tdma_cap_set(DMA_SLAVE, tdev->device.cap_mask);\n\tdma_cap_set(DMA_CYCLIC, tdev->device.cap_mask);\n\ttdev->device.dev = &pdev->dev;\n\ttdev->device.device_alloc_chan_resources =\n\t\t\t\t\tmmp_tdma_alloc_chan_resources;\n\ttdev->device.device_free_chan_resources =\n\t\t\t\t\tmmp_tdma_free_chan_resources;\n\ttdev->device.device_prep_dma_cyclic = mmp_tdma_prep_dma_cyclic;\n\ttdev->device.device_tx_status = mmp_tdma_tx_status;\n\ttdev->device.device_issue_pending = mmp_tdma_issue_pending;\n\ttdev->device.device_config = mmp_tdma_config;\n\ttdev->device.device_pause = mmp_tdma_pause_chan;\n\ttdev->device.device_resume = mmp_tdma_resume_chan;\n\ttdev->device.device_terminate_all = mmp_tdma_terminate_all;\n\ttdev->device.copy_align = DMAENGINE_ALIGN_8_BYTES;\n\n\ttdev->device.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\n\tif (type == MMP_AUD_TDMA) {\n\t\ttdev->device.max_burst = SZ_128;\n\t\ttdev->device.src_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\t\ttdev->device.dst_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\t} else if (type == PXA910_SQU) {\n\t\ttdev->device.max_burst = SZ_32;\n\t}\n\ttdev->device.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\n\ttdev->device.descriptor_reuse = true;\n\n\tdma_set_mask(&pdev->dev, DMA_BIT_MASK(64));\n\tplatform_set_drvdata(pdev, tdev);\n\n\tret = dmaenginem_async_device_register(&tdev->device);\n\tif (ret) {\n\t\tdev_err(tdev->device.dev, \"unable to register\\n\");\n\t\treturn ret;\n\t}\n\n\tif (pdev->dev.of_node) {\n\t\tret = of_dma_controller_register(pdev->dev.of_node,\n\t\t\t\t\t\t\tmmp_tdma_xlate, tdev);\n\t\tif (ret) {\n\t\t\tdev_err(tdev->device.dev,\n\t\t\t\t\"failed to register controller\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tdev_info(tdev->device.dev, \"initialized\\n\");\n\treturn 0;\n}\n\nstatic const struct platform_device_id mmp_tdma_id_table[] = {\n\t{ \"mmp-adma\",\tMMP_AUD_TDMA },\n\t{ \"pxa910-squ\",\tPXA910_SQU },\n\t{ },\n};\n\nstatic struct platform_driver mmp_tdma_driver = {\n\t.driver\t\t= {\n\t\t.name\t= \"mmp-tdma\",\n\t\t.of_match_table = mmp_tdma_dt_ids,\n\t},\n\t.id_table\t= mmp_tdma_id_table,\n\t.probe\t\t= mmp_tdma_probe,\n\t.remove\t\t= mmp_tdma_remove,\n};\n\nmodule_platform_driver(mmp_tdma_driver);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"MMP Two-Channel DMA Driver\");\nMODULE_ALIAS(\"platform:mmp-tdma\");\nMODULE_AUTHOR(\"Leo Yan <leoy@marvell.com>\");\nMODULE_AUTHOR(\"Zhangfei Gao <zhangfei.gao@marvell.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}