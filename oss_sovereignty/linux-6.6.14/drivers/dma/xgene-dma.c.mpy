{
  "module_name": "xgene-dma.c",
  "hash_id": "4550dc52f1a36dd036452f82153abf6bf88159aa2775aab9297f32690482cf69",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/xgene-dma.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/clk.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/dmapool.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/irq.h>\n#include <linux/mod_devicetable.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n\n#include \"dmaengine.h\"\n\n \n#define XGENE_DMA_RING_CONFIG\t\t\t0x04\n#define XGENE_DMA_RING_ENABLE\t\t\tBIT(31)\n#define XGENE_DMA_RING_ID\t\t\t0x08\n#define XGENE_DMA_RING_ID_SETUP(v)\t\t((v) | BIT(31))\n#define XGENE_DMA_RING_ID_BUF\t\t\t0x0C\n#define XGENE_DMA_RING_ID_BUF_SETUP(v)\t\t(((v) << 9) | BIT(21))\n#define XGENE_DMA_RING_THRESLD0_SET1\t\t0x30\n#define XGENE_DMA_RING_THRESLD0_SET1_VAL\t0X64\n#define XGENE_DMA_RING_THRESLD1_SET1\t\t0x34\n#define XGENE_DMA_RING_THRESLD1_SET1_VAL\t0xC8\n#define XGENE_DMA_RING_HYSTERESIS\t\t0x68\n#define XGENE_DMA_RING_HYSTERESIS_VAL\t\t0xFFFFFFFF\n#define XGENE_DMA_RING_STATE\t\t\t0x6C\n#define XGENE_DMA_RING_STATE_WR_BASE\t\t0x70\n#define XGENE_DMA_RING_NE_INT_MODE\t\t0x017C\n#define XGENE_DMA_RING_NE_INT_MODE_SET(m, v)\t\\\n\t((m) = ((m) & ~BIT(31 - (v))) | BIT(31 - (v)))\n#define XGENE_DMA_RING_NE_INT_MODE_RESET(m, v)\t\\\n\t((m) &= (~BIT(31 - (v))))\n#define XGENE_DMA_RING_CLKEN\t\t\t0xC208\n#define XGENE_DMA_RING_SRST\t\t\t0xC200\n#define XGENE_DMA_RING_MEM_RAM_SHUTDOWN\t\t0xD070\n#define XGENE_DMA_RING_BLK_MEM_RDY\t\t0xD074\n#define XGENE_DMA_RING_BLK_MEM_RDY_VAL\t\t0xFFFFFFFF\n#define XGENE_DMA_RING_ID_GET(owner, num)\t(((owner) << 6) | (num))\n#define XGENE_DMA_RING_DST_ID(v)\t\t((1 << 10) | (v))\n#define XGENE_DMA_RING_CMD_OFFSET\t\t0x2C\n#define XGENE_DMA_RING_CMD_BASE_OFFSET(v)\t((v) << 6)\n#define XGENE_DMA_RING_COHERENT_SET(m)\t\t\\\n\t(((u32 *)(m))[2] |= BIT(4))\n#define XGENE_DMA_RING_ADDRL_SET(m, v)\t\t\\\n\t(((u32 *)(m))[2] |= (((v) >> 8) << 5))\n#define XGENE_DMA_RING_ADDRH_SET(m, v)\t\t\\\n\t(((u32 *)(m))[3] |= ((v) >> 35))\n#define XGENE_DMA_RING_ACCEPTLERR_SET(m)\t\\\n\t(((u32 *)(m))[3] |= BIT(19))\n#define XGENE_DMA_RING_SIZE_SET(m, v)\t\t\\\n\t(((u32 *)(m))[3] |= ((v) << 23))\n#define XGENE_DMA_RING_RECOMBBUF_SET(m)\t\t\\\n\t(((u32 *)(m))[3] |= BIT(27))\n#define XGENE_DMA_RING_RECOMTIMEOUTL_SET(m)\t\\\n\t(((u32 *)(m))[3] |= (0x7 << 28))\n#define XGENE_DMA_RING_RECOMTIMEOUTH_SET(m)\t\\\n\t(((u32 *)(m))[4] |= 0x3)\n#define XGENE_DMA_RING_SELTHRSH_SET(m)\t\t\\\n\t(((u32 *)(m))[4] |= BIT(3))\n#define XGENE_DMA_RING_TYPE_SET(m, v)\t\t\\\n\t(((u32 *)(m))[4] |= ((v) << 19))\n\n \n#define XGENE_DMA_IPBRR\t\t\t\t0x0\n#define XGENE_DMA_DEV_ID_RD(v)\t\t\t((v) & 0x00000FFF)\n#define XGENE_DMA_BUS_ID_RD(v)\t\t\t(((v) >> 12) & 3)\n#define XGENE_DMA_REV_NO_RD(v)\t\t\t(((v) >> 14) & 3)\n#define XGENE_DMA_GCR\t\t\t\t0x10\n#define XGENE_DMA_CH_SETUP(v)\t\t\t\\\n\t((v) = ((v) & ~0x000FFFFF) | 0x000AAFFF)\n#define XGENE_DMA_ENABLE(v)\t\t\t((v) |= BIT(31))\n#define XGENE_DMA_DISABLE(v)\t\t\t((v) &= ~BIT(31))\n#define XGENE_DMA_RAID6_CONT\t\t\t0x14\n#define XGENE_DMA_RAID6_MULTI_CTRL(v)\t\t((v) << 24)\n#define XGENE_DMA_INT\t\t\t\t0x70\n#define XGENE_DMA_INT_MASK\t\t\t0x74\n#define XGENE_DMA_INT_ALL_MASK\t\t\t0xFFFFFFFF\n#define XGENE_DMA_INT_ALL_UNMASK\t\t0x0\n#define XGENE_DMA_INT_MASK_SHIFT\t\t0x14\n#define XGENE_DMA_RING_INT0_MASK\t\t0x90A0\n#define XGENE_DMA_RING_INT1_MASK\t\t0x90A8\n#define XGENE_DMA_RING_INT2_MASK\t\t0x90B0\n#define XGENE_DMA_RING_INT3_MASK\t\t0x90B8\n#define XGENE_DMA_RING_INT4_MASK\t\t0x90C0\n#define XGENE_DMA_CFG_RING_WQ_ASSOC\t\t0x90E0\n#define XGENE_DMA_ASSOC_RING_MNGR1\t\t0xFFFFFFFF\n#define XGENE_DMA_MEM_RAM_SHUTDOWN\t\t0xD070\n#define XGENE_DMA_BLK_MEM_RDY\t\t\t0xD074\n#define XGENE_DMA_BLK_MEM_RDY_VAL\t\t0xFFFFFFFF\n#define XGENE_DMA_RING_CMD_SM_OFFSET\t\t0x8000\n\n \n#define XGENE_SOC_JTAG1_SHADOW\t\t\t0x18\n#define XGENE_DMA_PQ_DISABLE_MASK\t\tBIT(13)\n\n \n#define XGENE_DMA_DESC_NV_BIT\t\t\tBIT_ULL(50)\n#define XGENE_DMA_DESC_IN_BIT\t\t\tBIT_ULL(55)\n#define XGENE_DMA_DESC_C_BIT\t\t\tBIT_ULL(63)\n#define XGENE_DMA_DESC_DR_BIT\t\t\tBIT_ULL(61)\n#define XGENE_DMA_DESC_ELERR_POS\t\t46\n#define XGENE_DMA_DESC_RTYPE_POS\t\t56\n#define XGENE_DMA_DESC_LERR_POS\t\t\t60\n#define XGENE_DMA_DESC_BUFLEN_POS\t\t48\n#define XGENE_DMA_DESC_HOENQ_NUM_POS\t\t48\n#define XGENE_DMA_DESC_ELERR_RD(m)\t\t\\\n\t(((m) >> XGENE_DMA_DESC_ELERR_POS) & 0x3)\n#define XGENE_DMA_DESC_LERR_RD(m)\t\t\\\n\t(((m) >> XGENE_DMA_DESC_LERR_POS) & 0x7)\n#define XGENE_DMA_DESC_STATUS(elerr, lerr)\t\\\n\t(((elerr) << 4) | (lerr))\n\n \n#define XGENE_DMA_DESC_EMPTY_SIGNATURE\t\t~0ULL\n\n \n#define XGENE_DMA_RING_NUM\t\t512\n#define XGENE_DMA_BUFNUM\t\t0x0\n#define XGENE_DMA_CPU_BUFNUM\t\t0x18\n#define XGENE_DMA_RING_OWNER_DMA\t0x03\n#define XGENE_DMA_RING_OWNER_CPU\t0x0F\n#define XGENE_DMA_RING_TYPE_REGULAR\t0x01\n#define XGENE_DMA_RING_WQ_DESC_SIZE\t32\t \n#define XGENE_DMA_RING_NUM_CONFIG\t5\n#define XGENE_DMA_MAX_CHANNEL\t\t4\n#define XGENE_DMA_XOR_CHANNEL\t\t0\n#define XGENE_DMA_PQ_CHANNEL\t\t1\n#define XGENE_DMA_MAX_BYTE_CNT\t\t0x4000\t \n#define XGENE_DMA_MAX_64B_DESC_BYTE_CNT\t0x14000\t \n#define XGENE_DMA_MAX_XOR_SRC\t\t5\n#define XGENE_DMA_16K_BUFFER_LEN_CODE\t0x0\n#define XGENE_DMA_INVALID_LEN_CODE\t0x7800000000000000ULL\n\n \n#define ERR_DESC_AXI\t\t\t0x01\n#define ERR_BAD_DESC\t\t\t0x02\n#define ERR_READ_DATA_AXI\t\t0x03\n#define ERR_WRITE_DATA_AXI\t\t0x04\n#define ERR_FBP_TIMEOUT\t\t\t0x05\n#define ERR_ECC\t\t\t\t0x06\n#define ERR_DIFF_SIZE\t\t\t0x08\n#define ERR_SCT_GAT_LEN\t\t\t0x09\n#define ERR_CRC_ERR\t\t\t0x11\n#define ERR_CHKSUM\t\t\t0x12\n#define ERR_DIF\t\t\t\t0x13\n\n \n#define ERR_DIF_SIZE_INT\t\t0x0\n#define ERR_GS_ERR_INT\t\t\t0x1\n#define ERR_FPB_TIMEO_INT\t\t0x2\n#define ERR_WFIFO_OVF_INT\t\t0x3\n#define ERR_RFIFO_OVF_INT\t\t0x4\n#define ERR_WR_TIMEO_INT\t\t0x5\n#define ERR_RD_TIMEO_INT\t\t0x6\n#define ERR_WR_ERR_INT\t\t\t0x7\n#define ERR_RD_ERR_INT\t\t\t0x8\n#define ERR_BAD_DESC_INT\t\t0x9\n#define ERR_DESC_DST_INT\t\t0xA\n#define ERR_DESC_SRC_INT\t\t0xB\n\n \n#define FLYBY_2SRC_XOR\t\t\t0x80\n#define FLYBY_3SRC_XOR\t\t\t0x90\n#define FLYBY_4SRC_XOR\t\t\t0xA0\n#define FLYBY_5SRC_XOR\t\t\t0xB0\n\n \n#define XGENE_DMA_FLAG_64B_DESC\t\tBIT(0)\n\n \n#define XGENE_DMA_DESC_DUMP(desc, m)\t\\\n\tprint_hex_dump(KERN_ERR, (m),\t\\\n\t\t\tDUMP_PREFIX_ADDRESS, 16, 8, (desc), 32, 0)\n\n#define to_dma_desc_sw(tx)\t\t\\\n\tcontainer_of(tx, struct xgene_dma_desc_sw, tx)\n#define to_dma_chan(dchan)\t\t\\\n\tcontainer_of(dchan, struct xgene_dma_chan, dma_chan)\n\n#define chan_dbg(chan, fmt, arg...)\t\\\n\tdev_dbg(chan->dev, \"%s: \" fmt, chan->name, ##arg)\n#define chan_err(chan, fmt, arg...)\t\\\n\tdev_err(chan->dev, \"%s: \" fmt, chan->name, ##arg)\n\nstruct xgene_dma_desc_hw {\n\t__le64 m0;\n\t__le64 m1;\n\t__le64 m2;\n\t__le64 m3;\n};\n\nenum xgene_dma_ring_cfgsize {\n\tXGENE_DMA_RING_CFG_SIZE_512B,\n\tXGENE_DMA_RING_CFG_SIZE_2KB,\n\tXGENE_DMA_RING_CFG_SIZE_16KB,\n\tXGENE_DMA_RING_CFG_SIZE_64KB,\n\tXGENE_DMA_RING_CFG_SIZE_512KB,\n\tXGENE_DMA_RING_CFG_SIZE_INVALID\n};\n\nstruct xgene_dma_ring {\n\tstruct xgene_dma *pdma;\n\tu8 buf_num;\n\tu16 id;\n\tu16 num;\n\tu16 head;\n\tu16 owner;\n\tu16 slots;\n\tu16 dst_ring_num;\n\tu32 size;\n\tvoid __iomem *cmd;\n\tvoid __iomem *cmd_base;\n\tdma_addr_t desc_paddr;\n\tu32 state[XGENE_DMA_RING_NUM_CONFIG];\n\tenum xgene_dma_ring_cfgsize cfgsize;\n\tunion {\n\t\tvoid *desc_vaddr;\n\t\tstruct xgene_dma_desc_hw *desc_hw;\n\t};\n};\n\nstruct xgene_dma_desc_sw {\n\tstruct xgene_dma_desc_hw desc1;\n\tstruct xgene_dma_desc_hw desc2;\n\tu32 flags;\n\tstruct list_head node;\n\tstruct list_head tx_list;\n\tstruct dma_async_tx_descriptor tx;\n};\n\n \nstruct xgene_dma_chan {\n\tstruct dma_chan dma_chan;\n\tstruct xgene_dma *pdma;\n\tstruct device *dev;\n\tint id;\n\tint rx_irq;\n\tchar name[10];\n\tspinlock_t lock;\n\tint pending;\n\tint max_outstanding;\n\tstruct list_head ld_pending;\n\tstruct list_head ld_running;\n\tstruct list_head ld_completed;\n\tstruct dma_pool *desc_pool;\n\tstruct tasklet_struct tasklet;\n\tstruct xgene_dma_ring tx_ring;\n\tstruct xgene_dma_ring rx_ring;\n};\n\n \nstruct xgene_dma {\n\tstruct device *dev;\n\tstruct clk *clk;\n\tint err_irq;\n\tint ring_num;\n\tvoid __iomem *csr_dma;\n\tvoid __iomem *csr_ring;\n\tvoid __iomem *csr_ring_cmd;\n\tvoid __iomem *csr_efuse;\n\tstruct dma_device dma_dev[XGENE_DMA_MAX_CHANNEL];\n\tstruct xgene_dma_chan chan[XGENE_DMA_MAX_CHANNEL];\n};\n\nstatic const char * const xgene_dma_desc_err[] = {\n\t[ERR_DESC_AXI] = \"AXI error when reading src/dst link list\",\n\t[ERR_BAD_DESC] = \"ERR or El_ERR fields not set to zero in desc\",\n\t[ERR_READ_DATA_AXI] = \"AXI error when reading data\",\n\t[ERR_WRITE_DATA_AXI] = \"AXI error when writing data\",\n\t[ERR_FBP_TIMEOUT] = \"Timeout on bufpool fetch\",\n\t[ERR_ECC] = \"ECC double bit error\",\n\t[ERR_DIFF_SIZE] = \"Bufpool too small to hold all the DIF result\",\n\t[ERR_SCT_GAT_LEN] = \"Gather and scatter data length not same\",\n\t[ERR_CRC_ERR] = \"CRC error\",\n\t[ERR_CHKSUM] = \"Checksum error\",\n\t[ERR_DIF] = \"DIF error\",\n};\n\nstatic const char * const xgene_dma_err[] = {\n\t[ERR_DIF_SIZE_INT] = \"DIF size error\",\n\t[ERR_GS_ERR_INT] = \"Gather scatter not same size error\",\n\t[ERR_FPB_TIMEO_INT] = \"Free pool time out error\",\n\t[ERR_WFIFO_OVF_INT] = \"Write FIFO over flow error\",\n\t[ERR_RFIFO_OVF_INT] = \"Read FIFO over flow error\",\n\t[ERR_WR_TIMEO_INT] = \"Write time out error\",\n\t[ERR_RD_TIMEO_INT] = \"Read time out error\",\n\t[ERR_WR_ERR_INT] = \"HBF bus write error\",\n\t[ERR_RD_ERR_INT] = \"HBF bus read error\",\n\t[ERR_BAD_DESC_INT] = \"Ring descriptor HE0 not set error\",\n\t[ERR_DESC_DST_INT] = \"HFB reading dst link address error\",\n\t[ERR_DESC_SRC_INT] = \"HFB reading src link address error\",\n};\n\nstatic bool is_pq_enabled(struct xgene_dma *pdma)\n{\n\tu32 val;\n\n\tval = ioread32(pdma->csr_efuse + XGENE_SOC_JTAG1_SHADOW);\n\treturn !(val & XGENE_DMA_PQ_DISABLE_MASK);\n}\n\nstatic u64 xgene_dma_encode_len(size_t len)\n{\n\treturn (len < XGENE_DMA_MAX_BYTE_CNT) ?\n\t\t((u64)len << XGENE_DMA_DESC_BUFLEN_POS) :\n\t\tXGENE_DMA_16K_BUFFER_LEN_CODE;\n}\n\nstatic u8 xgene_dma_encode_xor_flyby(u32 src_cnt)\n{\n\tstatic u8 flyby_type[] = {\n\t\tFLYBY_2SRC_XOR,  \n\t\tFLYBY_2SRC_XOR,  \n\t\tFLYBY_2SRC_XOR,\n\t\tFLYBY_3SRC_XOR,\n\t\tFLYBY_4SRC_XOR,\n\t\tFLYBY_5SRC_XOR\n\t};\n\n\treturn flyby_type[src_cnt];\n}\n\nstatic void xgene_dma_set_src_buffer(__le64 *ext8, size_t *len,\n\t\t\t\t     dma_addr_t *paddr)\n{\n\tsize_t nbytes = (*len < XGENE_DMA_MAX_BYTE_CNT) ?\n\t\t\t*len : XGENE_DMA_MAX_BYTE_CNT;\n\n\t*ext8 |= cpu_to_le64(*paddr);\n\t*ext8 |= cpu_to_le64(xgene_dma_encode_len(nbytes));\n\t*len -= nbytes;\n\t*paddr += nbytes;\n}\n\nstatic __le64 *xgene_dma_lookup_ext8(struct xgene_dma_desc_hw *desc, int idx)\n{\n\tswitch (idx) {\n\tcase 0:\n\t\treturn &desc->m1;\n\tcase 1:\n\t\treturn &desc->m0;\n\tcase 2:\n\t\treturn &desc->m3;\n\tcase 3:\n\t\treturn &desc->m2;\n\tdefault:\n\t\tpr_err(\"Invalid dma descriptor index\\n\");\n\t}\n\n\treturn NULL;\n}\n\nstatic void xgene_dma_init_desc(struct xgene_dma_desc_hw *desc,\n\t\t\t\tu16 dst_ring_num)\n{\n\tdesc->m0 |= cpu_to_le64(XGENE_DMA_DESC_IN_BIT);\n\tdesc->m0 |= cpu_to_le64((u64)XGENE_DMA_RING_OWNER_DMA <<\n\t\t\t\tXGENE_DMA_DESC_RTYPE_POS);\n\tdesc->m1 |= cpu_to_le64(XGENE_DMA_DESC_C_BIT);\n\tdesc->m3 |= cpu_to_le64((u64)dst_ring_num <<\n\t\t\t\tXGENE_DMA_DESC_HOENQ_NUM_POS);\n}\n\nstatic void xgene_dma_prep_xor_desc(struct xgene_dma_chan *chan,\n\t\t\t\t    struct xgene_dma_desc_sw *desc_sw,\n\t\t\t\t    dma_addr_t *dst, dma_addr_t *src,\n\t\t\t\t    u32 src_cnt, size_t *nbytes,\n\t\t\t\t    const u8 *scf)\n{\n\tstruct xgene_dma_desc_hw *desc1, *desc2;\n\tsize_t len = *nbytes;\n\tint i;\n\n\tdesc1 = &desc_sw->desc1;\n\tdesc2 = &desc_sw->desc2;\n\n\t \n\txgene_dma_init_desc(desc1, chan->tx_ring.dst_ring_num);\n\n\t \n\tdesc1->m2 |= cpu_to_le64(XGENE_DMA_DESC_DR_BIT);\n\tdesc1->m3 |= cpu_to_le64(*dst);\n\n\t \n\tdesc1->m0 |= cpu_to_le64(XGENE_DMA_DESC_NV_BIT);\n\n\t \n\tdesc1->m2 |= cpu_to_le64(xgene_dma_encode_xor_flyby(src_cnt));\n\n\t \n\tfor (i = 0; i < src_cnt; i++) {\n\t\tlen = *nbytes;\n\t\txgene_dma_set_src_buffer((i == 0) ? &desc1->m1 :\n\t\t\t\t\t xgene_dma_lookup_ext8(desc2, i - 1),\n\t\t\t\t\t &len, &src[i]);\n\t\tdesc1->m2 |= cpu_to_le64((scf[i] << ((i + 1) * 8)));\n\t}\n\n\t \n\t*nbytes = len;\n\t*dst += XGENE_DMA_MAX_BYTE_CNT;\n\n\t \n\tdesc_sw->flags |= XGENE_DMA_FLAG_64B_DESC;\n}\n\nstatic dma_cookie_t xgene_dma_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct xgene_dma_desc_sw *desc;\n\tstruct xgene_dma_chan *chan;\n\tdma_cookie_t cookie;\n\n\tif (unlikely(!tx))\n\t\treturn -EINVAL;\n\n\tchan = to_dma_chan(tx->chan);\n\tdesc = to_dma_desc_sw(tx);\n\n\tspin_lock_bh(&chan->lock);\n\n\tcookie = dma_cookie_assign(tx);\n\n\t \n\tlist_splice_tail_init(&desc->tx_list, &chan->ld_pending);\n\n\tspin_unlock_bh(&chan->lock);\n\n\treturn cookie;\n}\n\nstatic void xgene_dma_clean_descriptor(struct xgene_dma_chan *chan,\n\t\t\t\t       struct xgene_dma_desc_sw *desc)\n{\n\tlist_del(&desc->node);\n\tchan_dbg(chan, \"LD %p free\\n\", desc);\n\tdma_pool_free(chan->desc_pool, desc, desc->tx.phys);\n}\n\nstatic struct xgene_dma_desc_sw *xgene_dma_alloc_descriptor(\n\t\t\t\t struct xgene_dma_chan *chan)\n{\n\tstruct xgene_dma_desc_sw *desc;\n\tdma_addr_t phys;\n\n\tdesc = dma_pool_zalloc(chan->desc_pool, GFP_NOWAIT, &phys);\n\tif (!desc) {\n\t\tchan_err(chan, \"Failed to allocate LDs\\n\");\n\t\treturn NULL;\n\t}\n\n\tINIT_LIST_HEAD(&desc->tx_list);\n\tdesc->tx.phys = phys;\n\tdesc->tx.tx_submit = xgene_dma_tx_submit;\n\tdma_async_tx_descriptor_init(&desc->tx, &chan->dma_chan);\n\n\tchan_dbg(chan, \"LD %p allocated\\n\", desc);\n\n\treturn desc;\n}\n\n \nstatic void xgene_dma_clean_completed_descriptor(struct xgene_dma_chan *chan)\n{\n\tstruct xgene_dma_desc_sw *desc, *_desc;\n\n\t \n\tlist_for_each_entry_safe(desc, _desc, &chan->ld_completed, node) {\n\t\tif (async_tx_test_ack(&desc->tx))\n\t\t\txgene_dma_clean_descriptor(chan, desc);\n\t}\n}\n\n \nstatic void xgene_dma_run_tx_complete_actions(struct xgene_dma_chan *chan,\n\t\t\t\t\t      struct xgene_dma_desc_sw *desc)\n{\n\tstruct dma_async_tx_descriptor *tx = &desc->tx;\n\n\t \n\n\tif (tx->cookie == 0)\n\t\treturn;\n\n\tdma_cookie_complete(tx);\n\tdma_descriptor_unmap(tx);\n\n\t \n\tdmaengine_desc_get_callback_invoke(tx, NULL);\n\n\t \n\tdma_run_dependencies(tx);\n}\n\n \nstatic void xgene_dma_clean_running_descriptor(struct xgene_dma_chan *chan,\n\t\t\t\t\t       struct xgene_dma_desc_sw *desc)\n{\n\t \n\tlist_del(&desc->node);\n\n\t \n\tif (!async_tx_test_ack(&desc->tx)) {\n\t\t \n\t\tlist_add_tail(&desc->node, &chan->ld_completed);\n\t\treturn;\n\t}\n\n\tchan_dbg(chan, \"LD %p free\\n\", desc);\n\tdma_pool_free(chan->desc_pool, desc, desc->tx.phys);\n}\n\nstatic void xgene_chan_xfer_request(struct xgene_dma_chan *chan,\n\t\t\t\t    struct xgene_dma_desc_sw *desc_sw)\n{\n\tstruct xgene_dma_ring *ring = &chan->tx_ring;\n\tstruct xgene_dma_desc_hw *desc_hw;\n\n\t \n\tdesc_hw = &ring->desc_hw[ring->head];\n\n\t \n\tif (++ring->head == ring->slots)\n\t\tring->head = 0;\n\n\t \n\tmemcpy(desc_hw, &desc_sw->desc1, sizeof(*desc_hw));\n\n\t \n\tif (desc_sw->flags & XGENE_DMA_FLAG_64B_DESC) {\n\t\tdesc_hw = &ring->desc_hw[ring->head];\n\n\t\tif (++ring->head == ring->slots)\n\t\t\tring->head = 0;\n\n\t\tmemcpy(desc_hw, &desc_sw->desc2, sizeof(*desc_hw));\n\t}\n\n\t \n\tchan->pending += ((desc_sw->flags &\n\t\t\t  XGENE_DMA_FLAG_64B_DESC) ? 2 : 1);\n\n\t \n\tiowrite32((desc_sw->flags & XGENE_DMA_FLAG_64B_DESC) ?\n\t\t  2 : 1, ring->cmd);\n}\n\n \nstatic void xgene_chan_xfer_ld_pending(struct xgene_dma_chan *chan)\n{\n\tstruct xgene_dma_desc_sw *desc_sw, *_desc_sw;\n\n\t \n\tif (list_empty(&chan->ld_pending)) {\n\t\tchan_dbg(chan, \"No pending LDs\\n\");\n\t\treturn;\n\t}\n\n\t \n\tlist_for_each_entry_safe(desc_sw, _desc_sw, &chan->ld_pending, node) {\n\t\t \n\t\tif (chan->pending >= chan->max_outstanding)\n\t\t\treturn;\n\n\t\txgene_chan_xfer_request(chan, desc_sw);\n\n\t\t \n\t\tlist_move_tail(&desc_sw->node, &chan->ld_running);\n\t}\n}\n\n \nstatic void xgene_dma_cleanup_descriptors(struct xgene_dma_chan *chan)\n{\n\tstruct xgene_dma_ring *ring = &chan->rx_ring;\n\tstruct xgene_dma_desc_sw *desc_sw, *_desc_sw;\n\tstruct xgene_dma_desc_hw *desc_hw;\n\tstruct list_head ld_completed;\n\tu8 status;\n\n\tINIT_LIST_HEAD(&ld_completed);\n\n\tspin_lock(&chan->lock);\n\n\t \n\txgene_dma_clean_completed_descriptor(chan);\n\n\t \n\tlist_for_each_entry_safe(desc_sw, _desc_sw, &chan->ld_running, node) {\n\t\t \n\t\tdesc_hw = &ring->desc_hw[ring->head];\n\n\t\t \n\t\tif (unlikely(le64_to_cpu(desc_hw->m0) ==\n\t\t\t     XGENE_DMA_DESC_EMPTY_SIGNATURE))\n\t\t\tbreak;\n\n\t\tif (++ring->head == ring->slots)\n\t\t\tring->head = 0;\n\n\t\t \n\t\tstatus = XGENE_DMA_DESC_STATUS(\n\t\t\t\tXGENE_DMA_DESC_ELERR_RD(le64_to_cpu(\n\t\t\t\t\t\t\tdesc_hw->m0)),\n\t\t\t\tXGENE_DMA_DESC_LERR_RD(le64_to_cpu(\n\t\t\t\t\t\t       desc_hw->m0)));\n\t\tif (status) {\n\t\t\t \n\t\t\tchan_err(chan, \"%s\\n\", xgene_dma_desc_err[status]);\n\n\t\t\t \n\t\t\tXGENE_DMA_DESC_DUMP(&desc_sw->desc1,\n\t\t\t\t\t    \"X-Gene DMA TX DESC1: \");\n\n\t\t\tif (desc_sw->flags & XGENE_DMA_FLAG_64B_DESC)\n\t\t\t\tXGENE_DMA_DESC_DUMP(&desc_sw->desc2,\n\t\t\t\t\t\t    \"X-Gene DMA TX DESC2: \");\n\n\t\t\tXGENE_DMA_DESC_DUMP(desc_hw,\n\t\t\t\t\t    \"X-Gene DMA RX ERR DESC: \");\n\t\t}\n\n\t\t \n\t\tiowrite32(-1, ring->cmd);\n\n\t\t \n\t\tdesc_hw->m0 = cpu_to_le64(XGENE_DMA_DESC_EMPTY_SIGNATURE);\n\n\t\t \n\t\tchan->pending -= ((desc_sw->flags &\n\t\t\t\t  XGENE_DMA_FLAG_64B_DESC) ? 2 : 1);\n\n\t\t \n\t\tlist_move_tail(&desc_sw->node, &ld_completed);\n\t}\n\n\t \n\txgene_chan_xfer_ld_pending(chan);\n\n\tspin_unlock(&chan->lock);\n\n\t \n\tlist_for_each_entry_safe(desc_sw, _desc_sw, &ld_completed, node) {\n\t\txgene_dma_run_tx_complete_actions(chan, desc_sw);\n\t\txgene_dma_clean_running_descriptor(chan, desc_sw);\n\t}\n}\n\nstatic int xgene_dma_alloc_chan_resources(struct dma_chan *dchan)\n{\n\tstruct xgene_dma_chan *chan = to_dma_chan(dchan);\n\n\t \n\tif (chan->desc_pool)\n\t\treturn 1;\n\n\tchan->desc_pool = dma_pool_create(chan->name, chan->dev,\n\t\t\t\t\t  sizeof(struct xgene_dma_desc_sw),\n\t\t\t\t\t  0, 0);\n\tif (!chan->desc_pool) {\n\t\tchan_err(chan, \"Failed to allocate descriptor pool\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tchan_dbg(chan, \"Allocate descriptor pool\\n\");\n\n\treturn 1;\n}\n\n \nstatic void xgene_dma_free_desc_list(struct xgene_dma_chan *chan,\n\t\t\t\t     struct list_head *list)\n{\n\tstruct xgene_dma_desc_sw *desc, *_desc;\n\n\tlist_for_each_entry_safe(desc, _desc, list, node)\n\t\txgene_dma_clean_descriptor(chan, desc);\n}\n\nstatic void xgene_dma_free_chan_resources(struct dma_chan *dchan)\n{\n\tstruct xgene_dma_chan *chan = to_dma_chan(dchan);\n\n\tchan_dbg(chan, \"Free all resources\\n\");\n\n\tif (!chan->desc_pool)\n\t\treturn;\n\n\t \n\txgene_dma_cleanup_descriptors(chan);\n\n\tspin_lock_bh(&chan->lock);\n\n\t \n\txgene_dma_free_desc_list(chan, &chan->ld_pending);\n\txgene_dma_free_desc_list(chan, &chan->ld_running);\n\txgene_dma_free_desc_list(chan, &chan->ld_completed);\n\n\tspin_unlock_bh(&chan->lock);\n\n\t \n\tdma_pool_destroy(chan->desc_pool);\n\tchan->desc_pool = NULL;\n}\n\nstatic struct dma_async_tx_descriptor *xgene_dma_prep_xor(\n\tstruct dma_chan *dchan, dma_addr_t dst,\tdma_addr_t *src,\n\tu32 src_cnt, size_t len, unsigned long flags)\n{\n\tstruct xgene_dma_desc_sw *first = NULL, *new;\n\tstruct xgene_dma_chan *chan;\n\tstatic u8 multi[XGENE_DMA_MAX_XOR_SRC] = {\n\t\t\t\t0x01, 0x01, 0x01, 0x01, 0x01};\n\n\tif (unlikely(!dchan || !len))\n\t\treturn NULL;\n\n\tchan = to_dma_chan(dchan);\n\n\tdo {\n\t\t \n\t\tnew = xgene_dma_alloc_descriptor(chan);\n\t\tif (!new)\n\t\t\tgoto fail;\n\n\t\t \n\t\txgene_dma_prep_xor_desc(chan, new, &dst, src,\n\t\t\t\t\tsrc_cnt, &len, multi);\n\n\t\tif (!first)\n\t\t\tfirst = new;\n\n\t\tnew->tx.cookie = 0;\n\t\tasync_tx_ack(&new->tx);\n\n\t\t \n\t\tlist_add_tail(&new->node, &first->tx_list);\n\t} while (len);\n\n\tnew->tx.flags = flags;  \n\tnew->tx.cookie = -EBUSY;\n\tlist_splice(&first->tx_list, &new->tx_list);\n\n\treturn &new->tx;\n\nfail:\n\tif (!first)\n\t\treturn NULL;\n\n\txgene_dma_free_desc_list(chan, &first->tx_list);\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor *xgene_dma_prep_pq(\n\tstruct dma_chan *dchan, dma_addr_t *dst, dma_addr_t *src,\n\tu32 src_cnt, const u8 *scf, size_t len, unsigned long flags)\n{\n\tstruct xgene_dma_desc_sw *first = NULL, *new;\n\tstruct xgene_dma_chan *chan;\n\tsize_t _len = len;\n\tdma_addr_t _src[XGENE_DMA_MAX_XOR_SRC];\n\tstatic u8 multi[XGENE_DMA_MAX_XOR_SRC] = {0x01, 0x01, 0x01, 0x01, 0x01};\n\n\tif (unlikely(!dchan || !len))\n\t\treturn NULL;\n\n\tchan = to_dma_chan(dchan);\n\n\t \n\tmemcpy(_src, src, sizeof(*src) * src_cnt);\n\n\tif (flags & DMA_PREP_PQ_DISABLE_P)\n\t\tlen = 0;\n\n\tif (flags & DMA_PREP_PQ_DISABLE_Q)\n\t\t_len = 0;\n\n\tdo {\n\t\t \n\t\tnew = xgene_dma_alloc_descriptor(chan);\n\t\tif (!new)\n\t\t\tgoto fail;\n\n\t\tif (!first)\n\t\t\tfirst = new;\n\n\t\tnew->tx.cookie = 0;\n\t\tasync_tx_ack(&new->tx);\n\n\t\t \n\t\tlist_add_tail(&new->node, &first->tx_list);\n\n\t\t \n\t\tif (len) {\n\t\t\txgene_dma_prep_xor_desc(chan, new, &dst[0], src,\n\t\t\t\t\t\tsrc_cnt, &len, multi);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (_len) {\n\t\t\txgene_dma_prep_xor_desc(chan, new, &dst[1], _src,\n\t\t\t\t\t\tsrc_cnt, &_len, scf);\n\t\t}\n\t} while (len || _len);\n\n\tnew->tx.flags = flags;  \n\tnew->tx.cookie = -EBUSY;\n\tlist_splice(&first->tx_list, &new->tx_list);\n\n\treturn &new->tx;\n\nfail:\n\tif (!first)\n\t\treturn NULL;\n\n\txgene_dma_free_desc_list(chan, &first->tx_list);\n\treturn NULL;\n}\n\nstatic void xgene_dma_issue_pending(struct dma_chan *dchan)\n{\n\tstruct xgene_dma_chan *chan = to_dma_chan(dchan);\n\n\tspin_lock_bh(&chan->lock);\n\txgene_chan_xfer_ld_pending(chan);\n\tspin_unlock_bh(&chan->lock);\n}\n\nstatic enum dma_status xgene_dma_tx_status(struct dma_chan *dchan,\n\t\t\t\t\t   dma_cookie_t cookie,\n\t\t\t\t\t   struct dma_tx_state *txstate)\n{\n\treturn dma_cookie_status(dchan, cookie, txstate);\n}\n\nstatic void xgene_dma_tasklet_cb(struct tasklet_struct *t)\n{\n\tstruct xgene_dma_chan *chan = from_tasklet(chan, t, tasklet);\n\n\t \n\txgene_dma_cleanup_descriptors(chan);\n\n\t \n\tenable_irq(chan->rx_irq);\n}\n\nstatic irqreturn_t xgene_dma_chan_ring_isr(int irq, void *id)\n{\n\tstruct xgene_dma_chan *chan = (struct xgene_dma_chan *)id;\n\n\tBUG_ON(!chan);\n\n\t \n\tdisable_irq_nosync(chan->rx_irq);\n\n\t \n\ttasklet_schedule(&chan->tasklet);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t xgene_dma_err_isr(int irq, void *id)\n{\n\tstruct xgene_dma *pdma = (struct xgene_dma *)id;\n\tunsigned long int_mask;\n\tu32 val, i;\n\n\tval = ioread32(pdma->csr_dma + XGENE_DMA_INT);\n\n\t \n\tiowrite32(val, pdma->csr_dma + XGENE_DMA_INT);\n\n\t \n\tint_mask = val >> XGENE_DMA_INT_MASK_SHIFT;\n\tfor_each_set_bit(i, &int_mask, ARRAY_SIZE(xgene_dma_err))\n\t\tdev_err(pdma->dev,\n\t\t\t\"Interrupt status 0x%08X %s\\n\", val, xgene_dma_err[i]);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void xgene_dma_wr_ring_state(struct xgene_dma_ring *ring)\n{\n\tint i;\n\n\tiowrite32(ring->num, ring->pdma->csr_ring + XGENE_DMA_RING_STATE);\n\n\tfor (i = 0; i < XGENE_DMA_RING_NUM_CONFIG; i++)\n\t\tiowrite32(ring->state[i], ring->pdma->csr_ring +\n\t\t\t  XGENE_DMA_RING_STATE_WR_BASE + (i * 4));\n}\n\nstatic void xgene_dma_clr_ring_state(struct xgene_dma_ring *ring)\n{\n\tmemset(ring->state, 0, sizeof(u32) * XGENE_DMA_RING_NUM_CONFIG);\n\txgene_dma_wr_ring_state(ring);\n}\n\nstatic void xgene_dma_setup_ring(struct xgene_dma_ring *ring)\n{\n\tvoid *ring_cfg = ring->state;\n\tu64 addr = ring->desc_paddr;\n\tu32 i, val;\n\n\tring->slots = ring->size / XGENE_DMA_RING_WQ_DESC_SIZE;\n\n\t \n\txgene_dma_clr_ring_state(ring);\n\n\t \n\tXGENE_DMA_RING_TYPE_SET(ring_cfg, XGENE_DMA_RING_TYPE_REGULAR);\n\n\tif (ring->owner == XGENE_DMA_RING_OWNER_DMA) {\n\t\t \n\t\tXGENE_DMA_RING_RECOMBBUF_SET(ring_cfg);\n\t\tXGENE_DMA_RING_RECOMTIMEOUTL_SET(ring_cfg);\n\t\tXGENE_DMA_RING_RECOMTIMEOUTH_SET(ring_cfg);\n\t}\n\n\t \n\tXGENE_DMA_RING_SELTHRSH_SET(ring_cfg);\n\tXGENE_DMA_RING_ACCEPTLERR_SET(ring_cfg);\n\tXGENE_DMA_RING_COHERENT_SET(ring_cfg);\n\tXGENE_DMA_RING_ADDRL_SET(ring_cfg, addr);\n\tXGENE_DMA_RING_ADDRH_SET(ring_cfg, addr);\n\tXGENE_DMA_RING_SIZE_SET(ring_cfg, ring->cfgsize);\n\n\t \n\txgene_dma_wr_ring_state(ring);\n\n\t \n\tiowrite32(XGENE_DMA_RING_ID_SETUP(ring->id),\n\t\t  ring->pdma->csr_ring + XGENE_DMA_RING_ID);\n\n\t \n\tiowrite32(XGENE_DMA_RING_ID_BUF_SETUP(ring->num),\n\t\t  ring->pdma->csr_ring + XGENE_DMA_RING_ID_BUF);\n\n\tif (ring->owner != XGENE_DMA_RING_OWNER_CPU)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < ring->slots; i++) {\n\t\tstruct xgene_dma_desc_hw *desc;\n\n\t\tdesc = &ring->desc_hw[i];\n\t\tdesc->m0 = cpu_to_le64(XGENE_DMA_DESC_EMPTY_SIGNATURE);\n\t}\n\n\t \n\tval = ioread32(ring->pdma->csr_ring + XGENE_DMA_RING_NE_INT_MODE);\n\tXGENE_DMA_RING_NE_INT_MODE_SET(val, ring->buf_num);\n\tiowrite32(val, ring->pdma->csr_ring + XGENE_DMA_RING_NE_INT_MODE);\n}\n\nstatic void xgene_dma_clear_ring(struct xgene_dma_ring *ring)\n{\n\tu32 ring_id, val;\n\n\tif (ring->owner == XGENE_DMA_RING_OWNER_CPU) {\n\t\t \n\t\tval = ioread32(ring->pdma->csr_ring +\n\t\t\t       XGENE_DMA_RING_NE_INT_MODE);\n\t\tXGENE_DMA_RING_NE_INT_MODE_RESET(val, ring->buf_num);\n\t\tiowrite32(val, ring->pdma->csr_ring +\n\t\t\t  XGENE_DMA_RING_NE_INT_MODE);\n\t}\n\n\t \n\tring_id = XGENE_DMA_RING_ID_SETUP(ring->id);\n\tiowrite32(ring_id, ring->pdma->csr_ring + XGENE_DMA_RING_ID);\n\n\tiowrite32(0, ring->pdma->csr_ring + XGENE_DMA_RING_ID_BUF);\n\txgene_dma_clr_ring_state(ring);\n}\n\nstatic void xgene_dma_set_ring_cmd(struct xgene_dma_ring *ring)\n{\n\tring->cmd_base = ring->pdma->csr_ring_cmd +\n\t\t\t\tXGENE_DMA_RING_CMD_BASE_OFFSET((ring->num -\n\t\t\t\t\t\t\t  XGENE_DMA_RING_NUM));\n\n\tring->cmd = ring->cmd_base + XGENE_DMA_RING_CMD_OFFSET;\n}\n\nstatic int xgene_dma_get_ring_size(struct xgene_dma_chan *chan,\n\t\t\t\t   enum xgene_dma_ring_cfgsize cfgsize)\n{\n\tint size;\n\n\tswitch (cfgsize) {\n\tcase XGENE_DMA_RING_CFG_SIZE_512B:\n\t\tsize = 0x200;\n\t\tbreak;\n\tcase XGENE_DMA_RING_CFG_SIZE_2KB:\n\t\tsize = 0x800;\n\t\tbreak;\n\tcase XGENE_DMA_RING_CFG_SIZE_16KB:\n\t\tsize = 0x4000;\n\t\tbreak;\n\tcase XGENE_DMA_RING_CFG_SIZE_64KB:\n\t\tsize = 0x10000;\n\t\tbreak;\n\tcase XGENE_DMA_RING_CFG_SIZE_512KB:\n\t\tsize = 0x80000;\n\t\tbreak;\n\tdefault:\n\t\tchan_err(chan, \"Unsupported cfg ring size %d\\n\", cfgsize);\n\t\treturn -EINVAL;\n\t}\n\n\treturn size;\n}\n\nstatic void xgene_dma_delete_ring_one(struct xgene_dma_ring *ring)\n{\n\t \n\txgene_dma_clear_ring(ring);\n\n\t \n\tif (ring->desc_vaddr) {\n\t\tdma_free_coherent(ring->pdma->dev, ring->size,\n\t\t\t\t  ring->desc_vaddr, ring->desc_paddr);\n\t\tring->desc_vaddr = NULL;\n\t}\n}\n\nstatic void xgene_dma_delete_chan_rings(struct xgene_dma_chan *chan)\n{\n\txgene_dma_delete_ring_one(&chan->rx_ring);\n\txgene_dma_delete_ring_one(&chan->tx_ring);\n}\n\nstatic int xgene_dma_create_ring_one(struct xgene_dma_chan *chan,\n\t\t\t\t     struct xgene_dma_ring *ring,\n\t\t\t\t     enum xgene_dma_ring_cfgsize cfgsize)\n{\n\tint ret;\n\n\t \n\tring->pdma = chan->pdma;\n\tring->cfgsize = cfgsize;\n\tring->num = chan->pdma->ring_num++;\n\tring->id = XGENE_DMA_RING_ID_GET(ring->owner, ring->buf_num);\n\n\tret = xgene_dma_get_ring_size(chan, cfgsize);\n\tif (ret <= 0)\n\t\treturn ret;\n\tring->size = ret;\n\n\t \n\tring->desc_vaddr = dma_alloc_coherent(chan->dev, ring->size,\n\t\t\t\t\t      &ring->desc_paddr, GFP_KERNEL);\n\tif (!ring->desc_vaddr) {\n\t\tchan_err(chan, \"Failed to allocate ring desc\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\txgene_dma_set_ring_cmd(ring);\n\txgene_dma_setup_ring(ring);\n\n\treturn 0;\n}\n\nstatic int xgene_dma_create_chan_rings(struct xgene_dma_chan *chan)\n{\n\tstruct xgene_dma_ring *rx_ring = &chan->rx_ring;\n\tstruct xgene_dma_ring *tx_ring = &chan->tx_ring;\n\tint ret;\n\n\t \n\trx_ring->owner = XGENE_DMA_RING_OWNER_CPU;\n\trx_ring->buf_num = XGENE_DMA_CPU_BUFNUM + chan->id;\n\n\tret = xgene_dma_create_ring_one(chan, rx_ring,\n\t\t\t\t\tXGENE_DMA_RING_CFG_SIZE_64KB);\n\tif (ret)\n\t\treturn ret;\n\n\tchan_dbg(chan, \"Rx ring id 0x%X num %d desc 0x%p\\n\",\n\t\t rx_ring->id, rx_ring->num, rx_ring->desc_vaddr);\n\n\t \n\ttx_ring->owner = XGENE_DMA_RING_OWNER_DMA;\n\ttx_ring->buf_num = XGENE_DMA_BUFNUM + chan->id;\n\n\tret = xgene_dma_create_ring_one(chan, tx_ring,\n\t\t\t\t\tXGENE_DMA_RING_CFG_SIZE_64KB);\n\tif (ret) {\n\t\txgene_dma_delete_ring_one(rx_ring);\n\t\treturn ret;\n\t}\n\n\ttx_ring->dst_ring_num = XGENE_DMA_RING_DST_ID(rx_ring->num);\n\n\tchan_dbg(chan,\n\t\t \"Tx ring id 0x%X num %d desc 0x%p\\n\",\n\t\t tx_ring->id, tx_ring->num, tx_ring->desc_vaddr);\n\n\t \n\tchan->max_outstanding = tx_ring->slots;\n\n\treturn ret;\n}\n\nstatic int xgene_dma_init_rings(struct xgene_dma *pdma)\n{\n\tint ret, i, j;\n\n\tfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\n\t\tret = xgene_dma_create_chan_rings(&pdma->chan[i]);\n\t\tif (ret) {\n\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\txgene_dma_delete_chan_rings(&pdma->chan[j]);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void xgene_dma_enable(struct xgene_dma *pdma)\n{\n\tu32 val;\n\n\t \n\tval = ioread32(pdma->csr_dma + XGENE_DMA_GCR);\n\tXGENE_DMA_CH_SETUP(val);\n\tXGENE_DMA_ENABLE(val);\n\tiowrite32(val, pdma->csr_dma + XGENE_DMA_GCR);\n}\n\nstatic void xgene_dma_disable(struct xgene_dma *pdma)\n{\n\tu32 val;\n\n\tval = ioread32(pdma->csr_dma + XGENE_DMA_GCR);\n\tXGENE_DMA_DISABLE(val);\n\tiowrite32(val, pdma->csr_dma + XGENE_DMA_GCR);\n}\n\nstatic void xgene_dma_mask_interrupts(struct xgene_dma *pdma)\n{\n\t \n\tiowrite32(XGENE_DMA_INT_ALL_MASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT0_MASK);\n\tiowrite32(XGENE_DMA_INT_ALL_MASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT1_MASK);\n\tiowrite32(XGENE_DMA_INT_ALL_MASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT2_MASK);\n\tiowrite32(XGENE_DMA_INT_ALL_MASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT3_MASK);\n\tiowrite32(XGENE_DMA_INT_ALL_MASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT4_MASK);\n\n\t \n\tiowrite32(XGENE_DMA_INT_ALL_MASK, pdma->csr_dma + XGENE_DMA_INT_MASK);\n}\n\nstatic void xgene_dma_unmask_interrupts(struct xgene_dma *pdma)\n{\n\t \n\tiowrite32(XGENE_DMA_INT_ALL_UNMASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT0_MASK);\n\tiowrite32(XGENE_DMA_INT_ALL_UNMASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT1_MASK);\n\tiowrite32(XGENE_DMA_INT_ALL_UNMASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT2_MASK);\n\tiowrite32(XGENE_DMA_INT_ALL_UNMASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT3_MASK);\n\tiowrite32(XGENE_DMA_INT_ALL_UNMASK,\n\t\t  pdma->csr_dma + XGENE_DMA_RING_INT4_MASK);\n\n\t \n\tiowrite32(XGENE_DMA_INT_ALL_UNMASK,\n\t\t  pdma->csr_dma + XGENE_DMA_INT_MASK);\n}\n\nstatic void xgene_dma_init_hw(struct xgene_dma *pdma)\n{\n\tu32 val;\n\n\t \n\tiowrite32(XGENE_DMA_ASSOC_RING_MNGR1,\n\t\t  pdma->csr_dma + XGENE_DMA_CFG_RING_WQ_ASSOC);\n\n\t \n\tif (is_pq_enabled(pdma))\n\t\tiowrite32(XGENE_DMA_RAID6_MULTI_CTRL(0x1D),\n\t\t\t  pdma->csr_dma + XGENE_DMA_RAID6_CONT);\n\telse\n\t\tdev_info(pdma->dev, \"PQ is disabled in HW\\n\");\n\n\txgene_dma_enable(pdma);\n\txgene_dma_unmask_interrupts(pdma);\n\n\t \n\tval = ioread32(pdma->csr_dma + XGENE_DMA_IPBRR);\n\n\t \n\tdev_info(pdma->dev,\n\t\t \"X-Gene DMA v%d.%02d.%02d driver registered %d channels\",\n\t\t XGENE_DMA_REV_NO_RD(val), XGENE_DMA_BUS_ID_RD(val),\n\t\t XGENE_DMA_DEV_ID_RD(val), XGENE_DMA_MAX_CHANNEL);\n}\n\nstatic int xgene_dma_init_ring_mngr(struct xgene_dma *pdma)\n{\n\tif (ioread32(pdma->csr_ring + XGENE_DMA_RING_CLKEN) &&\n\t    (!ioread32(pdma->csr_ring + XGENE_DMA_RING_SRST)))\n\t\treturn 0;\n\n\tiowrite32(0x3, pdma->csr_ring + XGENE_DMA_RING_CLKEN);\n\tiowrite32(0x0, pdma->csr_ring + XGENE_DMA_RING_SRST);\n\n\t \n\tiowrite32(0x0, pdma->csr_ring + XGENE_DMA_RING_MEM_RAM_SHUTDOWN);\n\n\t \n\tioread32(pdma->csr_ring + XGENE_DMA_RING_MEM_RAM_SHUTDOWN);\n\n\t \n\tusleep_range(1000, 1100);\n\n\tif (ioread32(pdma->csr_ring + XGENE_DMA_RING_BLK_MEM_RDY)\n\t\t!= XGENE_DMA_RING_BLK_MEM_RDY_VAL) {\n\t\tdev_err(pdma->dev,\n\t\t\t\"Failed to release ring mngr memory from shutdown\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tiowrite32(XGENE_DMA_RING_THRESLD0_SET1_VAL,\n\t\t  pdma->csr_ring + XGENE_DMA_RING_THRESLD0_SET1);\n\tiowrite32(XGENE_DMA_RING_THRESLD1_SET1_VAL,\n\t\t  pdma->csr_ring + XGENE_DMA_RING_THRESLD1_SET1);\n\tiowrite32(XGENE_DMA_RING_HYSTERESIS_VAL,\n\t\t  pdma->csr_ring + XGENE_DMA_RING_HYSTERESIS);\n\n\t \n\tiowrite32(XGENE_DMA_RING_ENABLE,\n\t\t  pdma->csr_ring + XGENE_DMA_RING_CONFIG);\n\n\treturn 0;\n}\n\nstatic int xgene_dma_init_mem(struct xgene_dma *pdma)\n{\n\tint ret;\n\n\tret = xgene_dma_init_ring_mngr(pdma);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tiowrite32(0x0, pdma->csr_dma + XGENE_DMA_MEM_RAM_SHUTDOWN);\n\n\t \n\tioread32(pdma->csr_dma + XGENE_DMA_MEM_RAM_SHUTDOWN);\n\n\t \n\tusleep_range(1000, 1100);\n\n\tif (ioread32(pdma->csr_dma + XGENE_DMA_BLK_MEM_RDY)\n\t\t!= XGENE_DMA_BLK_MEM_RDY_VAL) {\n\t\tdev_err(pdma->dev,\n\t\t\t\"Failed to release DMA memory from shutdown\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\nstatic int xgene_dma_request_irqs(struct xgene_dma *pdma)\n{\n\tstruct xgene_dma_chan *chan;\n\tint ret, i, j;\n\n\t \n\tret = devm_request_irq(pdma->dev, pdma->err_irq, xgene_dma_err_isr,\n\t\t\t       0, \"dma_error\", pdma);\n\tif (ret) {\n\t\tdev_err(pdma->dev,\n\t\t\t\"Failed to register error IRQ %d\\n\", pdma->err_irq);\n\t\treturn ret;\n\t}\n\n\t \n\tfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\n\t\tchan = &pdma->chan[i];\n\t\tirq_set_status_flags(chan->rx_irq, IRQ_DISABLE_UNLAZY);\n\t\tret = devm_request_irq(chan->dev, chan->rx_irq,\n\t\t\t\t       xgene_dma_chan_ring_isr,\n\t\t\t\t       0, chan->name, chan);\n\t\tif (ret) {\n\t\t\tchan_err(chan, \"Failed to register Rx IRQ %d\\n\",\n\t\t\t\t chan->rx_irq);\n\t\t\tdevm_free_irq(pdma->dev, pdma->err_irq, pdma);\n\n\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\tchan = &pdma->chan[i];\n\t\t\t\tirq_clear_status_flags(chan->rx_irq, IRQ_DISABLE_UNLAZY);\n\t\t\t\tdevm_free_irq(chan->dev, chan->rx_irq, chan);\n\t\t\t}\n\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void xgene_dma_free_irqs(struct xgene_dma *pdma)\n{\n\tstruct xgene_dma_chan *chan;\n\tint i;\n\n\t \n\tdevm_free_irq(pdma->dev, pdma->err_irq, pdma);\n\n\tfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\n\t\tchan = &pdma->chan[i];\n\t\tirq_clear_status_flags(chan->rx_irq, IRQ_DISABLE_UNLAZY);\n\t\tdevm_free_irq(chan->dev, chan->rx_irq, chan);\n\t}\n}\n\nstatic void xgene_dma_set_caps(struct xgene_dma_chan *chan,\n\t\t\t       struct dma_device *dma_dev)\n{\n\t \n\tdma_cap_zero(dma_dev->cap_mask);\n\n\t \n\n\t \n\tif ((chan->id == XGENE_DMA_PQ_CHANNEL) &&\n\t    is_pq_enabled(chan->pdma)) {\n\t\tdma_cap_set(DMA_PQ, dma_dev->cap_mask);\n\t\tdma_cap_set(DMA_XOR, dma_dev->cap_mask);\n\t} else if ((chan->id == XGENE_DMA_XOR_CHANNEL) &&\n\t\t   !is_pq_enabled(chan->pdma)) {\n\t\tdma_cap_set(DMA_XOR, dma_dev->cap_mask);\n\t}\n\n\t \n\tdma_dev->dev = chan->dev;\n\tdma_dev->device_alloc_chan_resources = xgene_dma_alloc_chan_resources;\n\tdma_dev->device_free_chan_resources = xgene_dma_free_chan_resources;\n\tdma_dev->device_issue_pending = xgene_dma_issue_pending;\n\tdma_dev->device_tx_status = xgene_dma_tx_status;\n\n\tif (dma_has_cap(DMA_XOR, dma_dev->cap_mask)) {\n\t\tdma_dev->device_prep_dma_xor = xgene_dma_prep_xor;\n\t\tdma_dev->max_xor = XGENE_DMA_MAX_XOR_SRC;\n\t\tdma_dev->xor_align = DMAENGINE_ALIGN_64_BYTES;\n\t}\n\n\tif (dma_has_cap(DMA_PQ, dma_dev->cap_mask)) {\n\t\tdma_dev->device_prep_dma_pq = xgene_dma_prep_pq;\n\t\tdma_dev->max_pq = XGENE_DMA_MAX_XOR_SRC;\n\t\tdma_dev->pq_align = DMAENGINE_ALIGN_64_BYTES;\n\t}\n}\n\nstatic int xgene_dma_async_register(struct xgene_dma *pdma, int id)\n{\n\tstruct xgene_dma_chan *chan = &pdma->chan[id];\n\tstruct dma_device *dma_dev = &pdma->dma_dev[id];\n\tint ret;\n\n\tchan->dma_chan.device = dma_dev;\n\n\tspin_lock_init(&chan->lock);\n\tINIT_LIST_HEAD(&chan->ld_pending);\n\tINIT_LIST_HEAD(&chan->ld_running);\n\tINIT_LIST_HEAD(&chan->ld_completed);\n\ttasklet_setup(&chan->tasklet, xgene_dma_tasklet_cb);\n\n\tchan->pending = 0;\n\tchan->desc_pool = NULL;\n\tdma_cookie_init(&chan->dma_chan);\n\n\t \n\txgene_dma_set_caps(chan, dma_dev);\n\n\t \n\tINIT_LIST_HEAD(&dma_dev->channels);\n\tlist_add_tail(&chan->dma_chan.device_node, &dma_dev->channels);\n\n\t \n\tret = dma_async_device_register(dma_dev);\n\tif (ret) {\n\t\tchan_err(chan, \"Failed to register async device %d\", ret);\n\t\ttasklet_kill(&chan->tasklet);\n\n\t\treturn ret;\n\t}\n\n\t \n\tdev_info(pdma->dev,\n\t\t \"%s: CAPABILITY ( %s%s)\\n\", dma_chan_name(&chan->dma_chan),\n\t\t dma_has_cap(DMA_XOR, dma_dev->cap_mask) ? \"XOR \" : \"\",\n\t\t dma_has_cap(DMA_PQ, dma_dev->cap_mask) ? \"PQ \" : \"\");\n\n\treturn 0;\n}\n\nstatic int xgene_dma_init_async(struct xgene_dma *pdma)\n{\n\tint ret, i, j;\n\n\tfor (i = 0; i < XGENE_DMA_MAX_CHANNEL ; i++) {\n\t\tret = xgene_dma_async_register(pdma, i);\n\t\tif (ret) {\n\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\tdma_async_device_unregister(&pdma->dma_dev[j]);\n\t\t\t\ttasklet_kill(&pdma->chan[j].tasklet);\n\t\t\t}\n\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void xgene_dma_async_unregister(struct xgene_dma *pdma)\n{\n\tint i;\n\n\tfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++)\n\t\tdma_async_device_unregister(&pdma->dma_dev[i]);\n}\n\nstatic void xgene_dma_init_channels(struct xgene_dma *pdma)\n{\n\tstruct xgene_dma_chan *chan;\n\tint i;\n\n\tpdma->ring_num = XGENE_DMA_RING_NUM;\n\n\tfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\n\t\tchan = &pdma->chan[i];\n\t\tchan->dev = pdma->dev;\n\t\tchan->pdma = pdma;\n\t\tchan->id = i;\n\t\tsnprintf(chan->name, sizeof(chan->name), \"dmachan%d\", chan->id);\n\t}\n}\n\nstatic int xgene_dma_get_resources(struct platform_device *pdev,\n\t\t\t\t   struct xgene_dma *pdma)\n{\n\tstruct resource *res;\n\tint irq, i;\n\n\t \n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!res) {\n\t\tdev_err(&pdev->dev, \"Failed to get csr region\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tpdma->csr_dma = devm_ioremap(&pdev->dev, res->start,\n\t\t\t\t     resource_size(res));\n\tif (!pdma->csr_dma) {\n\t\tdev_err(&pdev->dev, \"Failed to ioremap csr region\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\n\tif (!res) {\n\t\tdev_err(&pdev->dev, \"Failed to get ring csr region\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tpdma->csr_ring =  devm_ioremap(&pdev->dev, res->start,\n\t\t\t\t       resource_size(res));\n\tif (!pdma->csr_ring) {\n\t\tdev_err(&pdev->dev, \"Failed to ioremap ring csr region\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 2);\n\tif (!res) {\n\t\tdev_err(&pdev->dev, \"Failed to get ring cmd csr region\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tpdma->csr_ring_cmd = devm_ioremap(&pdev->dev, res->start,\n\t\t\t\t\t  resource_size(res));\n\tif (!pdma->csr_ring_cmd) {\n\t\tdev_err(&pdev->dev, \"Failed to ioremap ring cmd csr region\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpdma->csr_ring_cmd += XGENE_DMA_RING_CMD_SM_OFFSET;\n\n\t \n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 3);\n\tif (!res) {\n\t\tdev_err(&pdev->dev, \"Failed to get efuse csr region\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tpdma->csr_efuse = devm_ioremap(&pdev->dev, res->start,\n\t\t\t\t       resource_size(res));\n\tif (!pdma->csr_efuse) {\n\t\tdev_err(&pdev->dev, \"Failed to ioremap efuse csr region\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tirq = platform_get_irq(pdev, 0);\n\tif (irq <= 0)\n\t\treturn -ENXIO;\n\n\tpdma->err_irq = irq;\n\n\t \n\tfor (i = 1; i <= XGENE_DMA_MAX_CHANNEL; i++) {\n\t\tirq = platform_get_irq(pdev, i);\n\t\tif (irq <= 0)\n\t\t\treturn -ENXIO;\n\n\t\tpdma->chan[i - 1].rx_irq = irq;\n\t}\n\n\treturn 0;\n}\n\nstatic int xgene_dma_probe(struct platform_device *pdev)\n{\n\tstruct xgene_dma *pdma;\n\tint ret, i;\n\n\tpdma = devm_kzalloc(&pdev->dev, sizeof(*pdma), GFP_KERNEL);\n\tif (!pdma)\n\t\treturn -ENOMEM;\n\n\tpdma->dev = &pdev->dev;\n\tplatform_set_drvdata(pdev, pdma);\n\n\tret = xgene_dma_get_resources(pdev, pdma);\n\tif (ret)\n\t\treturn ret;\n\n\tpdma->clk = devm_clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(pdma->clk) && !ACPI_COMPANION(&pdev->dev)) {\n\t\tdev_err(&pdev->dev, \"Failed to get clk\\n\");\n\t\treturn PTR_ERR(pdma->clk);\n\t}\n\n\t \n\tif (!IS_ERR(pdma->clk)) {\n\t\tret = clk_prepare_enable(pdma->clk);\n\t\tif (ret) {\n\t\t\tdev_err(&pdev->dev, \"Failed to enable clk %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tret = xgene_dma_init_mem(pdma);\n\tif (ret)\n\t\tgoto err_clk_enable;\n\n\tret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(42));\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"No usable DMA configuration\\n\");\n\t\tgoto err_dma_mask;\n\t}\n\n\t \n\txgene_dma_init_channels(pdma);\n\n\t \n\tret = xgene_dma_init_rings(pdma);\n\tif (ret)\n\t\tgoto err_clk_enable;\n\n\tret = xgene_dma_request_irqs(pdma);\n\tif (ret)\n\t\tgoto err_request_irq;\n\n\t \n\txgene_dma_init_hw(pdma);\n\n\t \n\tret = xgene_dma_init_async(pdma);\n\tif (ret)\n\t\tgoto err_async_init;\n\n\treturn 0;\n\nerr_async_init:\n\txgene_dma_free_irqs(pdma);\n\nerr_request_irq:\n\tfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++)\n\t\txgene_dma_delete_chan_rings(&pdma->chan[i]);\n\nerr_dma_mask:\nerr_clk_enable:\n\tif (!IS_ERR(pdma->clk))\n\t\tclk_disable_unprepare(pdma->clk);\n\n\treturn ret;\n}\n\nstatic int xgene_dma_remove(struct platform_device *pdev)\n{\n\tstruct xgene_dma *pdma = platform_get_drvdata(pdev);\n\tstruct xgene_dma_chan *chan;\n\tint i;\n\n\txgene_dma_async_unregister(pdma);\n\n\t \n\txgene_dma_mask_interrupts(pdma);\n\txgene_dma_disable(pdma);\n\txgene_dma_free_irqs(pdma);\n\n\tfor (i = 0; i < XGENE_DMA_MAX_CHANNEL; i++) {\n\t\tchan = &pdma->chan[i];\n\t\ttasklet_kill(&chan->tasklet);\n\t\txgene_dma_delete_chan_rings(chan);\n\t}\n\n\tif (!IS_ERR(pdma->clk))\n\t\tclk_disable_unprepare(pdma->clk);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_ACPI\nstatic const struct acpi_device_id xgene_dma_acpi_match_ptr[] = {\n\t{\"APMC0D43\", 0},\n\t{},\n};\nMODULE_DEVICE_TABLE(acpi, xgene_dma_acpi_match_ptr);\n#endif\n\nstatic const struct of_device_id xgene_dma_of_match_ptr[] = {\n\t{.compatible = \"apm,xgene-storm-dma\",},\n\t{},\n};\nMODULE_DEVICE_TABLE(of, xgene_dma_of_match_ptr);\n\nstatic struct platform_driver xgene_dma_driver = {\n\t.probe = xgene_dma_probe,\n\t.remove = xgene_dma_remove,\n\t.driver = {\n\t\t.name = \"X-Gene-DMA\",\n\t\t.of_match_table = xgene_dma_of_match_ptr,\n\t\t.acpi_match_table = ACPI_PTR(xgene_dma_acpi_match_ptr),\n\t},\n};\n\nmodule_platform_driver(xgene_dma_driver);\n\nMODULE_DESCRIPTION(\"APM X-Gene SoC DMA driver\");\nMODULE_AUTHOR(\"Rameshwar Prasad Sahu <rsahu@apm.com>\");\nMODULE_AUTHOR(\"Loc Ho <lho@apm.com>\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(\"1.0\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}