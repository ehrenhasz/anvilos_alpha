{
  "module_name": "img-mdc-dma.c",
  "hash_id": "b87d045897faaa833ff7b0accb27437e70e01f269061c7c74fc8140d3f9d2b88",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/img-mdc-dma.c",
  "human_readable_source": "\n \n\n#include <linux/clk.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/dmapool.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/irq.h>\n#include <linux/kernel.h>\n#include <linux/mfd/syscon.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/regmap.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\n#include \"dmaengine.h\"\n#include \"virt-dma.h\"\n\n#define MDC_MAX_DMA_CHANNELS\t\t\t32\n\n#define MDC_GENERAL_CONFIG\t\t\t0x000\n#define MDC_GENERAL_CONFIG_LIST_IEN\t\tBIT(31)\n#define MDC_GENERAL_CONFIG_IEN\t\t\tBIT(29)\n#define MDC_GENERAL_CONFIG_LEVEL_INT\t\tBIT(28)\n#define MDC_GENERAL_CONFIG_INC_W\t\tBIT(12)\n#define MDC_GENERAL_CONFIG_INC_R\t\tBIT(8)\n#define MDC_GENERAL_CONFIG_PHYSICAL_W\t\tBIT(7)\n#define MDC_GENERAL_CONFIG_WIDTH_W_SHIFT\t4\n#define MDC_GENERAL_CONFIG_WIDTH_W_MASK\t\t0x7\n#define MDC_GENERAL_CONFIG_PHYSICAL_R\t\tBIT(3)\n#define MDC_GENERAL_CONFIG_WIDTH_R_SHIFT\t0\n#define MDC_GENERAL_CONFIG_WIDTH_R_MASK\t\t0x7\n\n#define MDC_READ_PORT_CONFIG\t\t\t0x004\n#define MDC_READ_PORT_CONFIG_STHREAD_SHIFT\t28\n#define MDC_READ_PORT_CONFIG_STHREAD_MASK\t0xf\n#define MDC_READ_PORT_CONFIG_RTHREAD_SHIFT\t24\n#define MDC_READ_PORT_CONFIG_RTHREAD_MASK\t0xf\n#define MDC_READ_PORT_CONFIG_WTHREAD_SHIFT\t16\n#define MDC_READ_PORT_CONFIG_WTHREAD_MASK\t0xf\n#define MDC_READ_PORT_CONFIG_BURST_SIZE_SHIFT\t4\n#define MDC_READ_PORT_CONFIG_BURST_SIZE_MASK\t0xff\n#define MDC_READ_PORT_CONFIG_DREQ_ENABLE\tBIT(1)\n\n#define MDC_READ_ADDRESS\t\t\t0x008\n\n#define MDC_WRITE_ADDRESS\t\t\t0x00c\n\n#define MDC_TRANSFER_SIZE\t\t\t0x010\n#define MDC_TRANSFER_SIZE_MASK\t\t\t0xffffff\n\n#define MDC_LIST_NODE_ADDRESS\t\t\t0x014\n\n#define MDC_CMDS_PROCESSED\t\t\t0x018\n#define MDC_CMDS_PROCESSED_CMDS_PROCESSED_SHIFT\t16\n#define MDC_CMDS_PROCESSED_CMDS_PROCESSED_MASK\t0x3f\n#define MDC_CMDS_PROCESSED_INT_ACTIVE\t\tBIT(8)\n#define MDC_CMDS_PROCESSED_CMDS_DONE_SHIFT\t0\n#define MDC_CMDS_PROCESSED_CMDS_DONE_MASK\t0x3f\n\n#define MDC_CONTROL_AND_STATUS\t\t\t0x01c\n#define MDC_CONTROL_AND_STATUS_CANCEL\t\tBIT(20)\n#define MDC_CONTROL_AND_STATUS_LIST_EN\t\tBIT(4)\n#define MDC_CONTROL_AND_STATUS_EN\t\tBIT(0)\n\n#define MDC_ACTIVE_TRANSFER_SIZE\t\t0x030\n\n#define MDC_GLOBAL_CONFIG_A\t\t\t\t0x900\n#define MDC_GLOBAL_CONFIG_A_THREAD_ID_WIDTH_SHIFT\t16\n#define MDC_GLOBAL_CONFIG_A_THREAD_ID_WIDTH_MASK\t0xff\n#define MDC_GLOBAL_CONFIG_A_DMA_CONTEXTS_SHIFT\t\t8\n#define MDC_GLOBAL_CONFIG_A_DMA_CONTEXTS_MASK\t\t0xff\n#define MDC_GLOBAL_CONFIG_A_SYS_DAT_WIDTH_SHIFT\t\t0\n#define MDC_GLOBAL_CONFIG_A_SYS_DAT_WIDTH_MASK\t\t0xff\n\nstruct mdc_hw_list_desc {\n\tu32 gen_conf;\n\tu32 readport_conf;\n\tu32 read_addr;\n\tu32 write_addr;\n\tu32 xfer_size;\n\tu32 node_addr;\n\tu32 cmds_done;\n\tu32 ctrl_status;\n\t \n\tstruct mdc_hw_list_desc *next_desc;\n};\n\nstruct mdc_tx_desc {\n\tstruct mdc_chan *chan;\n\tstruct virt_dma_desc vd;\n\tdma_addr_t list_phys;\n\tstruct mdc_hw_list_desc *list;\n\tbool cyclic;\n\tbool cmd_loaded;\n\tunsigned int list_len;\n\tunsigned int list_period_len;\n\tsize_t list_xfer_size;\n\tunsigned int list_cmds_done;\n};\n\nstruct mdc_chan {\n\tstruct mdc_dma *mdma;\n\tstruct virt_dma_chan vc;\n\tstruct dma_slave_config config;\n\tstruct mdc_tx_desc *desc;\n\tint irq;\n\tunsigned int periph;\n\tunsigned int thread;\n\tunsigned int chan_nr;\n};\n\nstruct mdc_dma_soc_data {\n\tvoid (*enable_chan)(struct mdc_chan *mchan);\n\tvoid (*disable_chan)(struct mdc_chan *mchan);\n};\n\nstruct mdc_dma {\n\tstruct dma_device dma_dev;\n\tvoid __iomem *regs;\n\tstruct clk *clk;\n\tstruct dma_pool *desc_pool;\n\tstruct regmap *periph_regs;\n\tspinlock_t lock;\n\tunsigned int nr_threads;\n\tunsigned int nr_channels;\n\tunsigned int bus_width;\n\tunsigned int max_burst_mult;\n\tunsigned int max_xfer_size;\n\tconst struct mdc_dma_soc_data *soc;\n\tstruct mdc_chan channels[MDC_MAX_DMA_CHANNELS];\n};\n\nstatic inline u32 mdc_readl(struct mdc_dma *mdma, u32 reg)\n{\n\treturn readl(mdma->regs + reg);\n}\n\nstatic inline void mdc_writel(struct mdc_dma *mdma, u32 val, u32 reg)\n{\n\twritel(val, mdma->regs + reg);\n}\n\nstatic inline u32 mdc_chan_readl(struct mdc_chan *mchan, u32 reg)\n{\n\treturn mdc_readl(mchan->mdma, mchan->chan_nr * 0x040 + reg);\n}\n\nstatic inline void mdc_chan_writel(struct mdc_chan *mchan, u32 val, u32 reg)\n{\n\tmdc_writel(mchan->mdma, val, mchan->chan_nr * 0x040 + reg);\n}\n\nstatic inline struct mdc_chan *to_mdc_chan(struct dma_chan *c)\n{\n\treturn container_of(to_virt_chan(c), struct mdc_chan, vc);\n}\n\nstatic inline struct mdc_tx_desc *to_mdc_desc(struct dma_async_tx_descriptor *t)\n{\n\tstruct virt_dma_desc *vdesc = container_of(t, struct virt_dma_desc, tx);\n\n\treturn container_of(vdesc, struct mdc_tx_desc, vd);\n}\n\nstatic inline struct device *mdma2dev(struct mdc_dma *mdma)\n{\n\treturn mdma->dma_dev.dev;\n}\n\nstatic inline unsigned int to_mdc_width(unsigned int bytes)\n{\n\treturn ffs(bytes) - 1;\n}\n\nstatic inline void mdc_set_read_width(struct mdc_hw_list_desc *ldesc,\n\t\t\t\t      unsigned int bytes)\n{\n\tldesc->gen_conf |= to_mdc_width(bytes) <<\n\t\tMDC_GENERAL_CONFIG_WIDTH_R_SHIFT;\n}\n\nstatic inline void mdc_set_write_width(struct mdc_hw_list_desc *ldesc,\n\t\t\t\t       unsigned int bytes)\n{\n\tldesc->gen_conf |= to_mdc_width(bytes) <<\n\t\tMDC_GENERAL_CONFIG_WIDTH_W_SHIFT;\n}\n\nstatic void mdc_list_desc_config(struct mdc_chan *mchan,\n\t\t\t\t struct mdc_hw_list_desc *ldesc,\n\t\t\t\t enum dma_transfer_direction dir,\n\t\t\t\t dma_addr_t src, dma_addr_t dst, size_t len)\n{\n\tstruct mdc_dma *mdma = mchan->mdma;\n\tunsigned int max_burst, burst_size;\n\n\tldesc->gen_conf = MDC_GENERAL_CONFIG_IEN | MDC_GENERAL_CONFIG_LIST_IEN |\n\t\tMDC_GENERAL_CONFIG_LEVEL_INT | MDC_GENERAL_CONFIG_PHYSICAL_W |\n\t\tMDC_GENERAL_CONFIG_PHYSICAL_R;\n\tldesc->readport_conf =\n\t\t(mchan->thread << MDC_READ_PORT_CONFIG_STHREAD_SHIFT) |\n\t\t(mchan->thread << MDC_READ_PORT_CONFIG_RTHREAD_SHIFT) |\n\t\t(mchan->thread << MDC_READ_PORT_CONFIG_WTHREAD_SHIFT);\n\tldesc->read_addr = src;\n\tldesc->write_addr = dst;\n\tldesc->xfer_size = len - 1;\n\tldesc->node_addr = 0;\n\tldesc->cmds_done = 0;\n\tldesc->ctrl_status = MDC_CONTROL_AND_STATUS_LIST_EN |\n\t\tMDC_CONTROL_AND_STATUS_EN;\n\tldesc->next_desc = NULL;\n\n\tif (IS_ALIGNED(dst, mdma->bus_width) &&\n\t    IS_ALIGNED(src, mdma->bus_width))\n\t\tmax_burst = mdma->bus_width * mdma->max_burst_mult;\n\telse\n\t\tmax_burst = mdma->bus_width * (mdma->max_burst_mult - 1);\n\n\tif (dir == DMA_MEM_TO_DEV) {\n\t\tldesc->gen_conf |= MDC_GENERAL_CONFIG_INC_R;\n\t\tldesc->readport_conf |= MDC_READ_PORT_CONFIG_DREQ_ENABLE;\n\t\tmdc_set_read_width(ldesc, mdma->bus_width);\n\t\tmdc_set_write_width(ldesc, mchan->config.dst_addr_width);\n\t\tburst_size = min(max_burst, mchan->config.dst_maxburst *\n\t\t\t\t mchan->config.dst_addr_width);\n\t} else if (dir == DMA_DEV_TO_MEM) {\n\t\tldesc->gen_conf |= MDC_GENERAL_CONFIG_INC_W;\n\t\tldesc->readport_conf |= MDC_READ_PORT_CONFIG_DREQ_ENABLE;\n\t\tmdc_set_read_width(ldesc, mchan->config.src_addr_width);\n\t\tmdc_set_write_width(ldesc, mdma->bus_width);\n\t\tburst_size = min(max_burst, mchan->config.src_maxburst *\n\t\t\t\t mchan->config.src_addr_width);\n\t} else {\n\t\tldesc->gen_conf |= MDC_GENERAL_CONFIG_INC_R |\n\t\t\tMDC_GENERAL_CONFIG_INC_W;\n\t\tmdc_set_read_width(ldesc, mdma->bus_width);\n\t\tmdc_set_write_width(ldesc, mdma->bus_width);\n\t\tburst_size = max_burst;\n\t}\n\tldesc->readport_conf |= (burst_size - 1) <<\n\t\tMDC_READ_PORT_CONFIG_BURST_SIZE_SHIFT;\n}\n\nstatic void mdc_list_desc_free(struct mdc_tx_desc *mdesc)\n{\n\tstruct mdc_dma *mdma = mdesc->chan->mdma;\n\tstruct mdc_hw_list_desc *curr, *next;\n\tdma_addr_t curr_phys, next_phys;\n\n\tcurr = mdesc->list;\n\tcurr_phys = mdesc->list_phys;\n\twhile (curr) {\n\t\tnext = curr->next_desc;\n\t\tnext_phys = curr->node_addr;\n\t\tdma_pool_free(mdma->desc_pool, curr, curr_phys);\n\t\tcurr = next;\n\t\tcurr_phys = next_phys;\n\t}\n}\n\nstatic void mdc_desc_free(struct virt_dma_desc *vd)\n{\n\tstruct mdc_tx_desc *mdesc = to_mdc_desc(&vd->tx);\n\n\tmdc_list_desc_free(mdesc);\n\tkfree(mdesc);\n}\n\nstatic struct dma_async_tx_descriptor *mdc_prep_dma_memcpy(\n\tstruct dma_chan *chan, dma_addr_t dest, dma_addr_t src, size_t len,\n\tunsigned long flags)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\tstruct mdc_dma *mdma = mchan->mdma;\n\tstruct mdc_tx_desc *mdesc;\n\tstruct mdc_hw_list_desc *curr, *prev = NULL;\n\tdma_addr_t curr_phys;\n\n\tif (!len)\n\t\treturn NULL;\n\n\tmdesc = kzalloc(sizeof(*mdesc), GFP_NOWAIT);\n\tif (!mdesc)\n\t\treturn NULL;\n\tmdesc->chan = mchan;\n\tmdesc->list_xfer_size = len;\n\n\twhile (len > 0) {\n\t\tsize_t xfer_size;\n\n\t\tcurr = dma_pool_alloc(mdma->desc_pool, GFP_NOWAIT, &curr_phys);\n\t\tif (!curr)\n\t\t\tgoto free_desc;\n\n\t\tif (prev) {\n\t\t\tprev->node_addr = curr_phys;\n\t\t\tprev->next_desc = curr;\n\t\t} else {\n\t\t\tmdesc->list_phys = curr_phys;\n\t\t\tmdesc->list = curr;\n\t\t}\n\n\t\txfer_size = min_t(size_t, mdma->max_xfer_size, len);\n\n\t\tmdc_list_desc_config(mchan, curr, DMA_MEM_TO_MEM, src, dest,\n\t\t\t\t     xfer_size);\n\n\t\tprev = curr;\n\n\t\tmdesc->list_len++;\n\t\tsrc += xfer_size;\n\t\tdest += xfer_size;\n\t\tlen -= xfer_size;\n\t}\n\n\treturn vchan_tx_prep(&mchan->vc, &mdesc->vd, flags);\n\nfree_desc:\n\tmdc_desc_free(&mdesc->vd);\n\n\treturn NULL;\n}\n\nstatic int mdc_check_slave_width(struct mdc_chan *mchan,\n\t\t\t\t enum dma_transfer_direction dir)\n{\n\tenum dma_slave_buswidth width;\n\n\tif (dir == DMA_MEM_TO_DEV)\n\t\twidth = mchan->config.dst_addr_width;\n\telse\n\t\twidth = mchan->config.src_addr_width;\n\n\tswitch (width) {\n\tcase DMA_SLAVE_BUSWIDTH_1_BYTE:\n\tcase DMA_SLAVE_BUSWIDTH_2_BYTES:\n\tcase DMA_SLAVE_BUSWIDTH_4_BYTES:\n\tcase DMA_SLAVE_BUSWIDTH_8_BYTES:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (width > mchan->mdma->bus_width)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic struct dma_async_tx_descriptor *mdc_prep_dma_cyclic(\n\tstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\n\tsize_t period_len, enum dma_transfer_direction dir,\n\tunsigned long flags)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\tstruct mdc_dma *mdma = mchan->mdma;\n\tstruct mdc_tx_desc *mdesc;\n\tstruct mdc_hw_list_desc *curr, *prev = NULL;\n\tdma_addr_t curr_phys;\n\n\tif (!buf_len && !period_len)\n\t\treturn NULL;\n\n\tif (!is_slave_direction(dir))\n\t\treturn NULL;\n\n\tif (mdc_check_slave_width(mchan, dir) < 0)\n\t\treturn NULL;\n\n\tmdesc = kzalloc(sizeof(*mdesc), GFP_NOWAIT);\n\tif (!mdesc)\n\t\treturn NULL;\n\tmdesc->chan = mchan;\n\tmdesc->cyclic = true;\n\tmdesc->list_xfer_size = buf_len;\n\tmdesc->list_period_len = DIV_ROUND_UP(period_len,\n\t\t\t\t\t      mdma->max_xfer_size);\n\n\twhile (buf_len > 0) {\n\t\tsize_t remainder = min(period_len, buf_len);\n\n\t\twhile (remainder > 0) {\n\t\t\tsize_t xfer_size;\n\n\t\t\tcurr = dma_pool_alloc(mdma->desc_pool, GFP_NOWAIT,\n\t\t\t\t\t      &curr_phys);\n\t\t\tif (!curr)\n\t\t\t\tgoto free_desc;\n\n\t\t\tif (!prev) {\n\t\t\t\tmdesc->list_phys = curr_phys;\n\t\t\t\tmdesc->list = curr;\n\t\t\t} else {\n\t\t\t\tprev->node_addr = curr_phys;\n\t\t\t\tprev->next_desc = curr;\n\t\t\t}\n\n\t\t\txfer_size = min_t(size_t, mdma->max_xfer_size,\n\t\t\t\t\t  remainder);\n\n\t\t\tif (dir == DMA_MEM_TO_DEV) {\n\t\t\t\tmdc_list_desc_config(mchan, curr, dir,\n\t\t\t\t\t\t     buf_addr,\n\t\t\t\t\t\t     mchan->config.dst_addr,\n\t\t\t\t\t\t     xfer_size);\n\t\t\t} else {\n\t\t\t\tmdc_list_desc_config(mchan, curr, dir,\n\t\t\t\t\t\t     mchan->config.src_addr,\n\t\t\t\t\t\t     buf_addr,\n\t\t\t\t\t\t     xfer_size);\n\t\t\t}\n\n\t\t\tprev = curr;\n\n\t\t\tmdesc->list_len++;\n\t\t\tbuf_addr += xfer_size;\n\t\t\tbuf_len -= xfer_size;\n\t\t\tremainder -= xfer_size;\n\t\t}\n\t}\n\tprev->node_addr = mdesc->list_phys;\n\n\treturn vchan_tx_prep(&mchan->vc, &mdesc->vd, flags);\n\nfree_desc:\n\tmdc_desc_free(&mdesc->vd);\n\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor *mdc_prep_slave_sg(\n\tstruct dma_chan *chan, struct scatterlist *sgl,\n\tunsigned int sg_len, enum dma_transfer_direction dir,\n\tunsigned long flags, void *context)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\tstruct mdc_dma *mdma = mchan->mdma;\n\tstruct mdc_tx_desc *mdesc;\n\tstruct scatterlist *sg;\n\tstruct mdc_hw_list_desc *curr, *prev = NULL;\n\tdma_addr_t curr_phys;\n\tunsigned int i;\n\n\tif (!sgl)\n\t\treturn NULL;\n\n\tif (!is_slave_direction(dir))\n\t\treturn NULL;\n\n\tif (mdc_check_slave_width(mchan, dir) < 0)\n\t\treturn NULL;\n\n\tmdesc = kzalloc(sizeof(*mdesc), GFP_NOWAIT);\n\tif (!mdesc)\n\t\treturn NULL;\n\tmdesc->chan = mchan;\n\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tdma_addr_t buf = sg_dma_address(sg);\n\t\tsize_t buf_len = sg_dma_len(sg);\n\n\t\twhile (buf_len > 0) {\n\t\t\tsize_t xfer_size;\n\n\t\t\tcurr = dma_pool_alloc(mdma->desc_pool, GFP_NOWAIT,\n\t\t\t\t\t      &curr_phys);\n\t\t\tif (!curr)\n\t\t\t\tgoto free_desc;\n\n\t\t\tif (!prev) {\n\t\t\t\tmdesc->list_phys = curr_phys;\n\t\t\t\tmdesc->list = curr;\n\t\t\t} else {\n\t\t\t\tprev->node_addr = curr_phys;\n\t\t\t\tprev->next_desc = curr;\n\t\t\t}\n\n\t\t\txfer_size = min_t(size_t, mdma->max_xfer_size,\n\t\t\t\t\t  buf_len);\n\n\t\t\tif (dir == DMA_MEM_TO_DEV) {\n\t\t\t\tmdc_list_desc_config(mchan, curr, dir, buf,\n\t\t\t\t\t\t     mchan->config.dst_addr,\n\t\t\t\t\t\t     xfer_size);\n\t\t\t} else {\n\t\t\t\tmdc_list_desc_config(mchan, curr, dir,\n\t\t\t\t\t\t     mchan->config.src_addr,\n\t\t\t\t\t\t     buf, xfer_size);\n\t\t\t}\n\n\t\t\tprev = curr;\n\n\t\t\tmdesc->list_len++;\n\t\t\tmdesc->list_xfer_size += xfer_size;\n\t\t\tbuf += xfer_size;\n\t\t\tbuf_len -= xfer_size;\n\t\t}\n\t}\n\n\treturn vchan_tx_prep(&mchan->vc, &mdesc->vd, flags);\n\nfree_desc:\n\tmdc_desc_free(&mdesc->vd);\n\n\treturn NULL;\n}\n\nstatic void mdc_issue_desc(struct mdc_chan *mchan)\n{\n\tstruct mdc_dma *mdma = mchan->mdma;\n\tstruct virt_dma_desc *vd;\n\tstruct mdc_tx_desc *mdesc;\n\tu32 val;\n\n\tvd = vchan_next_desc(&mchan->vc);\n\tif (!vd)\n\t\treturn;\n\n\tlist_del(&vd->node);\n\n\tmdesc = to_mdc_desc(&vd->tx);\n\tmchan->desc = mdesc;\n\n\tdev_dbg(mdma2dev(mdma), \"Issuing descriptor on channel %d\\n\",\n\t\tmchan->chan_nr);\n\n\tmdma->soc->enable_chan(mchan);\n\n\tval = mdc_chan_readl(mchan, MDC_GENERAL_CONFIG);\n\tval |= MDC_GENERAL_CONFIG_LIST_IEN | MDC_GENERAL_CONFIG_IEN |\n\t\tMDC_GENERAL_CONFIG_LEVEL_INT | MDC_GENERAL_CONFIG_PHYSICAL_W |\n\t\tMDC_GENERAL_CONFIG_PHYSICAL_R;\n\tmdc_chan_writel(mchan, val, MDC_GENERAL_CONFIG);\n\tval = (mchan->thread << MDC_READ_PORT_CONFIG_STHREAD_SHIFT) |\n\t\t(mchan->thread << MDC_READ_PORT_CONFIG_RTHREAD_SHIFT) |\n\t\t(mchan->thread << MDC_READ_PORT_CONFIG_WTHREAD_SHIFT);\n\tmdc_chan_writel(mchan, val, MDC_READ_PORT_CONFIG);\n\tmdc_chan_writel(mchan, mdesc->list_phys, MDC_LIST_NODE_ADDRESS);\n\tval = mdc_chan_readl(mchan, MDC_CONTROL_AND_STATUS);\n\tval |= MDC_CONTROL_AND_STATUS_LIST_EN;\n\tmdc_chan_writel(mchan, val, MDC_CONTROL_AND_STATUS);\n}\n\nstatic void mdc_issue_pending(struct dma_chan *chan)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mchan->vc.lock, flags);\n\tif (vchan_issue_pending(&mchan->vc) && !mchan->desc)\n\t\tmdc_issue_desc(mchan);\n\tspin_unlock_irqrestore(&mchan->vc.lock, flags);\n}\n\nstatic enum dma_status mdc_tx_status(struct dma_chan *chan,\n\tdma_cookie_t cookie, struct dma_tx_state *txstate)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\tstruct mdc_tx_desc *mdesc;\n\tstruct virt_dma_desc *vd;\n\tunsigned long flags;\n\tsize_t bytes = 0;\n\tint ret;\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tif (!txstate)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&mchan->vc.lock, flags);\n\tvd = vchan_find_desc(&mchan->vc, cookie);\n\tif (vd) {\n\t\tmdesc = to_mdc_desc(&vd->tx);\n\t\tbytes = mdesc->list_xfer_size;\n\t} else if (mchan->desc && mchan->desc->vd.tx.cookie == cookie) {\n\t\tstruct mdc_hw_list_desc *ldesc;\n\t\tu32 val1, val2, done, processed, residue;\n\t\tint i, cmds;\n\n\t\tmdesc = mchan->desc;\n\n\t\t \n\t\tdo {\n\t\t\tval1 = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED) &\n\t\t\t\t~MDC_CMDS_PROCESSED_INT_ACTIVE;\n\t\t\tresidue = mdc_chan_readl(mchan,\n\t\t\t\t\t\t MDC_ACTIVE_TRANSFER_SIZE);\n\t\t\tval2 = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED) &\n\t\t\t\t~MDC_CMDS_PROCESSED_INT_ACTIVE;\n\t\t} while (val1 != val2);\n\n\t\tdone = (val1 >> MDC_CMDS_PROCESSED_CMDS_DONE_SHIFT) &\n\t\t\tMDC_CMDS_PROCESSED_CMDS_DONE_MASK;\n\t\tprocessed = (val1 >> MDC_CMDS_PROCESSED_CMDS_PROCESSED_SHIFT) &\n\t\t\tMDC_CMDS_PROCESSED_CMDS_PROCESSED_MASK;\n\t\tcmds = (done - processed) %\n\t\t\t(MDC_CMDS_PROCESSED_CMDS_DONE_MASK + 1);\n\n\t\t \n\t\tif (!mdesc->cmd_loaded)\n\t\t\tcmds--;\n\t\telse\n\t\t\tcmds += mdesc->list_cmds_done;\n\n\t\tbytes = mdesc->list_xfer_size;\n\t\tldesc = mdesc->list;\n\t\tfor (i = 0; i < cmds; i++) {\n\t\t\tbytes -= ldesc->xfer_size + 1;\n\t\t\tldesc = ldesc->next_desc;\n\t\t}\n\t\tif (ldesc) {\n\t\t\tif (residue != MDC_TRANSFER_SIZE_MASK)\n\t\t\t\tbytes -= ldesc->xfer_size - residue;\n\t\t\telse\n\t\t\t\tbytes -= ldesc->xfer_size + 1;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&mchan->vc.lock, flags);\n\n\tdma_set_residue(txstate, bytes);\n\n\treturn ret;\n}\n\nstatic unsigned int mdc_get_new_events(struct mdc_chan *mchan)\n{\n\tu32 val, processed, done1, done2;\n\tunsigned int ret;\n\n\tval = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED);\n\tprocessed = (val >> MDC_CMDS_PROCESSED_CMDS_PROCESSED_SHIFT) &\n\t\t\t\tMDC_CMDS_PROCESSED_CMDS_PROCESSED_MASK;\n\t \n\tdo {\n\t\tval = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED);\n\n\t\tdone1 = (val >> MDC_CMDS_PROCESSED_CMDS_DONE_SHIFT) &\n\t\t\tMDC_CMDS_PROCESSED_CMDS_DONE_MASK;\n\n\t\tval &= ~((MDC_CMDS_PROCESSED_CMDS_PROCESSED_MASK <<\n\t\t\t  MDC_CMDS_PROCESSED_CMDS_PROCESSED_SHIFT) |\n\t\t\t MDC_CMDS_PROCESSED_INT_ACTIVE);\n\n\t\tval |= done1 << MDC_CMDS_PROCESSED_CMDS_PROCESSED_SHIFT;\n\n\t\tmdc_chan_writel(mchan, val, MDC_CMDS_PROCESSED);\n\n\t\tval = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED);\n\n\t\tdone2 = (val >> MDC_CMDS_PROCESSED_CMDS_DONE_SHIFT) &\n\t\t\tMDC_CMDS_PROCESSED_CMDS_DONE_MASK;\n\t} while (done1 != done2);\n\n\tif (done1 >= processed)\n\t\tret = done1 - processed;\n\telse\n\t\tret = ((MDC_CMDS_PROCESSED_CMDS_PROCESSED_MASK + 1) -\n\t\t\tprocessed) + done1;\n\n\treturn ret;\n}\n\nstatic int mdc_terminate_all(struct dma_chan *chan)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&mchan->vc.lock, flags);\n\n\tmdc_chan_writel(mchan, MDC_CONTROL_AND_STATUS_CANCEL,\n\t\t\tMDC_CONTROL_AND_STATUS);\n\n\tif (mchan->desc) {\n\t\tvchan_terminate_vdesc(&mchan->desc->vd);\n\t\tmchan->desc = NULL;\n\t}\n\tvchan_get_all_descriptors(&mchan->vc, &head);\n\n\tmdc_get_new_events(mchan);\n\n\tspin_unlock_irqrestore(&mchan->vc.lock, flags);\n\n\tvchan_dma_desc_free_list(&mchan->vc, &head);\n\n\treturn 0;\n}\n\nstatic void mdc_synchronize(struct dma_chan *chan)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\n\tvchan_synchronize(&mchan->vc);\n}\n\nstatic int mdc_slave_config(struct dma_chan *chan,\n\t\t\t    struct dma_slave_config *config)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mchan->vc.lock, flags);\n\tmchan->config = *config;\n\tspin_unlock_irqrestore(&mchan->vc.lock, flags);\n\n\treturn 0;\n}\n\nstatic int mdc_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\tstruct device *dev = mdma2dev(mchan->mdma);\n\n\treturn pm_runtime_get_sync(dev);\n}\n\nstatic void mdc_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\tstruct mdc_dma *mdma = mchan->mdma;\n\tstruct device *dev = mdma2dev(mdma);\n\n\tmdc_terminate_all(chan);\n\tmdma->soc->disable_chan(mchan);\n\tpm_runtime_put(dev);\n}\n\nstatic irqreturn_t mdc_chan_irq(int irq, void *dev_id)\n{\n\tstruct mdc_chan *mchan = (struct mdc_chan *)dev_id;\n\tstruct mdc_tx_desc *mdesc;\n\tunsigned int i, new_events;\n\n\tspin_lock(&mchan->vc.lock);\n\n\tdev_dbg(mdma2dev(mchan->mdma), \"IRQ on channel %d\\n\", mchan->chan_nr);\n\n\tnew_events = mdc_get_new_events(mchan);\n\n\tif (!new_events)\n\t\tgoto out;\n\n\tmdesc = mchan->desc;\n\tif (!mdesc) {\n\t\tdev_warn(mdma2dev(mchan->mdma),\n\t\t\t \"IRQ with no active descriptor on channel %d\\n\",\n\t\t\t mchan->chan_nr);\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < new_events; i++) {\n\t\t \n\t\tif (!mdesc->cmd_loaded) {\n\t\t\tmdesc->cmd_loaded = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tmdesc->list_cmds_done++;\n\t\tif (mdesc->cyclic) {\n\t\t\tmdesc->list_cmds_done %= mdesc->list_len;\n\t\t\tif (mdesc->list_cmds_done % mdesc->list_period_len == 0)\n\t\t\t\tvchan_cyclic_callback(&mdesc->vd);\n\t\t} else if (mdesc->list_cmds_done == mdesc->list_len) {\n\t\t\tmchan->desc = NULL;\n\t\t\tvchan_cookie_complete(&mdesc->vd);\n\t\t\tmdc_issue_desc(mchan);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tspin_unlock(&mchan->vc.lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic struct dma_chan *mdc_of_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t     struct of_dma *ofdma)\n{\n\tstruct mdc_dma *mdma = ofdma->of_dma_data;\n\tstruct dma_chan *chan;\n\n\tif (dma_spec->args_count != 3)\n\t\treturn NULL;\n\n\tlist_for_each_entry(chan, &mdma->dma_dev.channels, device_node) {\n\t\tstruct mdc_chan *mchan = to_mdc_chan(chan);\n\n\t\tif (!(dma_spec->args[1] & BIT(mchan->chan_nr)))\n\t\t\tcontinue;\n\t\tif (dma_get_slave_channel(chan)) {\n\t\t\tmchan->periph = dma_spec->args[0];\n\t\t\tmchan->thread = dma_spec->args[2];\n\t\t\treturn chan;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n#define PISTACHIO_CR_PERIPH_DMA_ROUTE(ch)\t(0x120 + 0x4 * ((ch) / 4))\n#define PISTACHIO_CR_PERIPH_DMA_ROUTE_SHIFT(ch) (8 * ((ch) % 4))\n#define PISTACHIO_CR_PERIPH_DMA_ROUTE_MASK\t0x3f\n\nstatic void pistachio_mdc_enable_chan(struct mdc_chan *mchan)\n{\n\tstruct mdc_dma *mdma = mchan->mdma;\n\n\tregmap_update_bits(mdma->periph_regs,\n\t\t\t   PISTACHIO_CR_PERIPH_DMA_ROUTE(mchan->chan_nr),\n\t\t\t   PISTACHIO_CR_PERIPH_DMA_ROUTE_MASK <<\n\t\t\t   PISTACHIO_CR_PERIPH_DMA_ROUTE_SHIFT(mchan->chan_nr),\n\t\t\t   mchan->periph <<\n\t\t\t   PISTACHIO_CR_PERIPH_DMA_ROUTE_SHIFT(mchan->chan_nr));\n}\n\nstatic void pistachio_mdc_disable_chan(struct mdc_chan *mchan)\n{\n\tstruct mdc_dma *mdma = mchan->mdma;\n\n\tregmap_update_bits(mdma->periph_regs,\n\t\t\t   PISTACHIO_CR_PERIPH_DMA_ROUTE(mchan->chan_nr),\n\t\t\t   PISTACHIO_CR_PERIPH_DMA_ROUTE_MASK <<\n\t\t\t   PISTACHIO_CR_PERIPH_DMA_ROUTE_SHIFT(mchan->chan_nr),\n\t\t\t   0);\n}\n\nstatic const struct mdc_dma_soc_data pistachio_mdc_data = {\n\t.enable_chan = pistachio_mdc_enable_chan,\n\t.disable_chan = pistachio_mdc_disable_chan,\n};\n\nstatic const struct of_device_id mdc_dma_of_match[] = {\n\t{ .compatible = \"img,pistachio-mdc-dma\", .data = &pistachio_mdc_data, },\n\t{ },\n};\nMODULE_DEVICE_TABLE(of, mdc_dma_of_match);\n\nstatic int img_mdc_runtime_suspend(struct device *dev)\n{\n\tstruct mdc_dma *mdma = dev_get_drvdata(dev);\n\n\tclk_disable_unprepare(mdma->clk);\n\n\treturn 0;\n}\n\nstatic int img_mdc_runtime_resume(struct device *dev)\n{\n\tstruct mdc_dma *mdma = dev_get_drvdata(dev);\n\n\treturn clk_prepare_enable(mdma->clk);\n}\n\nstatic int mdc_dma_probe(struct platform_device *pdev)\n{\n\tstruct mdc_dma *mdma;\n\tunsigned int i;\n\tu32 val;\n\tint ret;\n\n\tmdma = devm_kzalloc(&pdev->dev, sizeof(*mdma), GFP_KERNEL);\n\tif (!mdma)\n\t\treturn -ENOMEM;\n\tplatform_set_drvdata(pdev, mdma);\n\n\tmdma->soc = of_device_get_match_data(&pdev->dev);\n\n\tmdma->regs = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(mdma->regs))\n\t\treturn PTR_ERR(mdma->regs);\n\n\tmdma->periph_regs = syscon_regmap_lookup_by_phandle(pdev->dev.of_node,\n\t\t\t\t\t\t\t    \"img,cr-periph\");\n\tif (IS_ERR(mdma->periph_regs))\n\t\treturn PTR_ERR(mdma->periph_regs);\n\n\tmdma->clk = devm_clk_get(&pdev->dev, \"sys\");\n\tif (IS_ERR(mdma->clk))\n\t\treturn PTR_ERR(mdma->clk);\n\n\tdma_cap_zero(mdma->dma_dev.cap_mask);\n\tdma_cap_set(DMA_SLAVE, mdma->dma_dev.cap_mask);\n\tdma_cap_set(DMA_PRIVATE, mdma->dma_dev.cap_mask);\n\tdma_cap_set(DMA_CYCLIC, mdma->dma_dev.cap_mask);\n\tdma_cap_set(DMA_MEMCPY, mdma->dma_dev.cap_mask);\n\n\tval = mdc_readl(mdma, MDC_GLOBAL_CONFIG_A);\n\tmdma->nr_channels = (val >> MDC_GLOBAL_CONFIG_A_DMA_CONTEXTS_SHIFT) &\n\t\tMDC_GLOBAL_CONFIG_A_DMA_CONTEXTS_MASK;\n\tmdma->nr_threads =\n\t\t1 << ((val >> MDC_GLOBAL_CONFIG_A_THREAD_ID_WIDTH_SHIFT) &\n\t\t      MDC_GLOBAL_CONFIG_A_THREAD_ID_WIDTH_MASK);\n\tmdma->bus_width =\n\t\t(1 << ((val >> MDC_GLOBAL_CONFIG_A_SYS_DAT_WIDTH_SHIFT) &\n\t\t       MDC_GLOBAL_CONFIG_A_SYS_DAT_WIDTH_MASK)) / 8;\n\t \n\tmdma->max_xfer_size = MDC_TRANSFER_SIZE_MASK + 1 - mdma->bus_width;\n\n\tof_property_read_u32(pdev->dev.of_node, \"dma-channels\",\n\t\t\t     &mdma->nr_channels);\n\tret = of_property_read_u32(pdev->dev.of_node,\n\t\t\t\t   \"img,max-burst-multiplier\",\n\t\t\t\t   &mdma->max_burst_mult);\n\tif (ret)\n\t\treturn ret;\n\n\tmdma->dma_dev.dev = &pdev->dev;\n\tmdma->dma_dev.device_prep_slave_sg = mdc_prep_slave_sg;\n\tmdma->dma_dev.device_prep_dma_cyclic = mdc_prep_dma_cyclic;\n\tmdma->dma_dev.device_prep_dma_memcpy = mdc_prep_dma_memcpy;\n\tmdma->dma_dev.device_alloc_chan_resources = mdc_alloc_chan_resources;\n\tmdma->dma_dev.device_free_chan_resources = mdc_free_chan_resources;\n\tmdma->dma_dev.device_tx_status = mdc_tx_status;\n\tmdma->dma_dev.device_issue_pending = mdc_issue_pending;\n\tmdma->dma_dev.device_terminate_all = mdc_terminate_all;\n\tmdma->dma_dev.device_synchronize = mdc_synchronize;\n\tmdma->dma_dev.device_config = mdc_slave_config;\n\n\tmdma->dma_dev.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\n\tmdma->dma_dev.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\n\tfor (i = 1; i <= mdma->bus_width; i <<= 1) {\n\t\tmdma->dma_dev.src_addr_widths |= BIT(i);\n\t\tmdma->dma_dev.dst_addr_widths |= BIT(i);\n\t}\n\n\tINIT_LIST_HEAD(&mdma->dma_dev.channels);\n\tfor (i = 0; i < mdma->nr_channels; i++) {\n\t\tstruct mdc_chan *mchan = &mdma->channels[i];\n\n\t\tmchan->mdma = mdma;\n\t\tmchan->chan_nr = i;\n\t\tmchan->irq = platform_get_irq(pdev, i);\n\t\tif (mchan->irq < 0)\n\t\t\treturn mchan->irq;\n\n\t\tret = devm_request_irq(&pdev->dev, mchan->irq, mdc_chan_irq,\n\t\t\t\t       IRQ_TYPE_LEVEL_HIGH,\n\t\t\t\t       dev_name(&pdev->dev), mchan);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tmchan->vc.desc_free = mdc_desc_free;\n\t\tvchan_init(&mchan->vc, &mdma->dma_dev);\n\t}\n\n\tmdma->desc_pool = dmam_pool_create(dev_name(&pdev->dev), &pdev->dev,\n\t\t\t\t\t   sizeof(struct mdc_hw_list_desc),\n\t\t\t\t\t   4, 0);\n\tif (!mdma->desc_pool)\n\t\treturn -ENOMEM;\n\n\tpm_runtime_enable(&pdev->dev);\n\tif (!pm_runtime_enabled(&pdev->dev)) {\n\t\tret = img_mdc_runtime_resume(&pdev->dev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = dma_async_device_register(&mdma->dma_dev);\n\tif (ret)\n\t\tgoto suspend;\n\n\tret = of_dma_controller_register(pdev->dev.of_node, mdc_of_xlate, mdma);\n\tif (ret)\n\t\tgoto unregister;\n\n\tdev_info(&pdev->dev, \"MDC with %u channels and %u threads\\n\",\n\t\t mdma->nr_channels, mdma->nr_threads);\n\n\treturn 0;\n\nunregister:\n\tdma_async_device_unregister(&mdma->dma_dev);\nsuspend:\n\tif (!pm_runtime_enabled(&pdev->dev))\n\t\timg_mdc_runtime_suspend(&pdev->dev);\n\tpm_runtime_disable(&pdev->dev);\n\treturn ret;\n}\n\nstatic int mdc_dma_remove(struct platform_device *pdev)\n{\n\tstruct mdc_dma *mdma = platform_get_drvdata(pdev);\n\tstruct mdc_chan *mchan, *next;\n\n\tof_dma_controller_free(pdev->dev.of_node);\n\tdma_async_device_unregister(&mdma->dma_dev);\n\n\tlist_for_each_entry_safe(mchan, next, &mdma->dma_dev.channels,\n\t\t\t\t vc.chan.device_node) {\n\t\tlist_del(&mchan->vc.chan.device_node);\n\n\t\tdevm_free_irq(&pdev->dev, mchan->irq, mchan);\n\n\t\ttasklet_kill(&mchan->vc.task);\n\t}\n\n\tpm_runtime_disable(&pdev->dev);\n\tif (!pm_runtime_status_suspended(&pdev->dev))\n\t\timg_mdc_runtime_suspend(&pdev->dev);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int img_mdc_suspend_late(struct device *dev)\n{\n\tstruct mdc_dma *mdma = dev_get_drvdata(dev);\n\tint i;\n\n\t \n\tfor (i = 0; i < mdma->nr_channels; i++) {\n\t\tstruct mdc_chan *mchan = &mdma->channels[i];\n\n\t\tif (unlikely(mchan->desc))\n\t\t\treturn -EBUSY;\n\t}\n\n\treturn pm_runtime_force_suspend(dev);\n}\n\nstatic int img_mdc_resume_early(struct device *dev)\n{\n\treturn pm_runtime_force_resume(dev);\n}\n#endif  \n\nstatic const struct dev_pm_ops img_mdc_pm_ops = {\n\tSET_RUNTIME_PM_OPS(img_mdc_runtime_suspend,\n\t\t\t   img_mdc_runtime_resume, NULL)\n\tSET_LATE_SYSTEM_SLEEP_PM_OPS(img_mdc_suspend_late,\n\t\t\t\t     img_mdc_resume_early)\n};\n\nstatic struct platform_driver mdc_dma_driver = {\n\t.driver = {\n\t\t.name = \"img-mdc-dma\",\n\t\t.pm = &img_mdc_pm_ops,\n\t\t.of_match_table = of_match_ptr(mdc_dma_of_match),\n\t},\n\t.probe = mdc_dma_probe,\n\t.remove = mdc_dma_remove,\n};\nmodule_platform_driver(mdc_dma_driver);\n\nMODULE_DESCRIPTION(\"IMG Multi-threaded DMA Controller (MDC) driver\");\nMODULE_AUTHOR(\"Andrew Bresticker <abrestic@chromium.org>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}