{
  "module_name": "dw-edma-core.c",
  "hash_id": "f1549ffc929d7991a797c749e5e48d8ee9d9c5d3bc20c40fe159390c1b2364f8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/dw-edma/dw-edma-core.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/device.h>\n#include <linux/kernel.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/dma/edma.h>\n#include <linux/dma-mapping.h>\n\n#include \"dw-edma-core.h\"\n#include \"dw-edma-v0-core.h\"\n#include \"dw-hdma-v0-core.h\"\n#include \"../dmaengine.h\"\n#include \"../virt-dma.h\"\n\nstatic inline\nstruct device *dchan2dev(struct dma_chan *dchan)\n{\n\treturn &dchan->dev->device;\n}\n\nstatic inline\nstruct device *chan2dev(struct dw_edma_chan *chan)\n{\n\treturn &chan->vc.chan.dev->device;\n}\n\nstatic inline\nstruct dw_edma_desc *vd2dw_edma_desc(struct virt_dma_desc *vd)\n{\n\treturn container_of(vd, struct dw_edma_desc, vd);\n}\n\nstatic inline\nu64 dw_edma_get_pci_address(struct dw_edma_chan *chan, phys_addr_t cpu_addr)\n{\n\tstruct dw_edma_chip *chip = chan->dw->chip;\n\n\tif (chip->ops->pci_address)\n\t\treturn chip->ops->pci_address(chip->dev, cpu_addr);\n\n\treturn cpu_addr;\n}\n\nstatic struct dw_edma_burst *dw_edma_alloc_burst(struct dw_edma_chunk *chunk)\n{\n\tstruct dw_edma_burst *burst;\n\n\tburst = kzalloc(sizeof(*burst), GFP_NOWAIT);\n\tif (unlikely(!burst))\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&burst->list);\n\tif (chunk->burst) {\n\t\t \n\t\tchunk->bursts_alloc++;\n\t\tlist_add_tail(&burst->list, &chunk->burst->list);\n\t} else {\n\t\t \n\t\tchunk->bursts_alloc = 0;\n\t\tchunk->burst = burst;\n\t}\n\n\treturn burst;\n}\n\nstatic struct dw_edma_chunk *dw_edma_alloc_chunk(struct dw_edma_desc *desc)\n{\n\tstruct dw_edma_chip *chip = desc->chan->dw->chip;\n\tstruct dw_edma_chan *chan = desc->chan;\n\tstruct dw_edma_chunk *chunk;\n\n\tchunk = kzalloc(sizeof(*chunk), GFP_NOWAIT);\n\tif (unlikely(!chunk))\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&chunk->list);\n\tchunk->chan = chan;\n\t \n\tchunk->cb = !(desc->chunks_alloc % 2);\n\tif (chan->dir == EDMA_DIR_WRITE) {\n\t\tchunk->ll_region.paddr = chip->ll_region_wr[chan->id].paddr;\n\t\tchunk->ll_region.vaddr = chip->ll_region_wr[chan->id].vaddr;\n\t} else {\n\t\tchunk->ll_region.paddr = chip->ll_region_rd[chan->id].paddr;\n\t\tchunk->ll_region.vaddr = chip->ll_region_rd[chan->id].vaddr;\n\t}\n\n\tif (desc->chunk) {\n\t\t \n\t\tif (!dw_edma_alloc_burst(chunk)) {\n\t\t\tkfree(chunk);\n\t\t\treturn NULL;\n\t\t}\n\t\tdesc->chunks_alloc++;\n\t\tlist_add_tail(&chunk->list, &desc->chunk->list);\n\t} else {\n\t\t \n\t\tchunk->burst = NULL;\n\t\tdesc->chunks_alloc = 0;\n\t\tdesc->chunk = chunk;\n\t}\n\n\treturn chunk;\n}\n\nstatic struct dw_edma_desc *dw_edma_alloc_desc(struct dw_edma_chan *chan)\n{\n\tstruct dw_edma_desc *desc;\n\n\tdesc = kzalloc(sizeof(*desc), GFP_NOWAIT);\n\tif (unlikely(!desc))\n\t\treturn NULL;\n\n\tdesc->chan = chan;\n\tif (!dw_edma_alloc_chunk(desc)) {\n\t\tkfree(desc);\n\t\treturn NULL;\n\t}\n\n\treturn desc;\n}\n\nstatic void dw_edma_free_burst(struct dw_edma_chunk *chunk)\n{\n\tstruct dw_edma_burst *child, *_next;\n\n\t \n\tlist_for_each_entry_safe(child, _next, &chunk->burst->list, list) {\n\t\tlist_del(&child->list);\n\t\tkfree(child);\n\t\tchunk->bursts_alloc--;\n\t}\n\n\t \n\tkfree(child);\n\tchunk->burst = NULL;\n}\n\nstatic void dw_edma_free_chunk(struct dw_edma_desc *desc)\n{\n\tstruct dw_edma_chunk *child, *_next;\n\n\tif (!desc->chunk)\n\t\treturn;\n\n\t \n\tlist_for_each_entry_safe(child, _next, &desc->chunk->list, list) {\n\t\tdw_edma_free_burst(child);\n\t\tlist_del(&child->list);\n\t\tkfree(child);\n\t\tdesc->chunks_alloc--;\n\t}\n\n\t \n\tkfree(child);\n\tdesc->chunk = NULL;\n}\n\nstatic void dw_edma_free_desc(struct dw_edma_desc *desc)\n{\n\tdw_edma_free_chunk(desc);\n\tkfree(desc);\n}\n\nstatic void vchan_free_desc(struct virt_dma_desc *vdesc)\n{\n\tdw_edma_free_desc(vd2dw_edma_desc(vdesc));\n}\n\nstatic int dw_edma_start_transfer(struct dw_edma_chan *chan)\n{\n\tstruct dw_edma *dw = chan->dw;\n\tstruct dw_edma_chunk *child;\n\tstruct dw_edma_desc *desc;\n\tstruct virt_dma_desc *vd;\n\n\tvd = vchan_next_desc(&chan->vc);\n\tif (!vd)\n\t\treturn 0;\n\n\tdesc = vd2dw_edma_desc(vd);\n\tif (!desc)\n\t\treturn 0;\n\n\tchild = list_first_entry_or_null(&desc->chunk->list,\n\t\t\t\t\t struct dw_edma_chunk, list);\n\tif (!child)\n\t\treturn 0;\n\n\tdw_edma_core_start(dw, child, !desc->xfer_sz);\n\tdesc->xfer_sz += child->ll_region.sz;\n\tdw_edma_free_burst(child);\n\tlist_del(&child->list);\n\tkfree(child);\n\tdesc->chunks_alloc--;\n\n\treturn 1;\n}\n\nstatic void dw_edma_device_caps(struct dma_chan *dchan,\n\t\t\t\tstruct dma_slave_caps *caps)\n{\n\tstruct dw_edma_chan *chan = dchan2dw_edma_chan(dchan);\n\n\tif (chan->dw->chip->flags & DW_EDMA_CHIP_LOCAL) {\n\t\tif (chan->dir == EDMA_DIR_READ)\n\t\t\tcaps->directions = BIT(DMA_DEV_TO_MEM);\n\t\telse\n\t\t\tcaps->directions = BIT(DMA_MEM_TO_DEV);\n\t} else {\n\t\tif (chan->dir == EDMA_DIR_WRITE)\n\t\t\tcaps->directions = BIT(DMA_DEV_TO_MEM);\n\t\telse\n\t\t\tcaps->directions = BIT(DMA_MEM_TO_DEV);\n\t}\n}\n\nstatic int dw_edma_device_config(struct dma_chan *dchan,\n\t\t\t\t struct dma_slave_config *config)\n{\n\tstruct dw_edma_chan *chan = dchan2dw_edma_chan(dchan);\n\n\tmemcpy(&chan->config, config, sizeof(*config));\n\tchan->configured = true;\n\n\treturn 0;\n}\n\nstatic int dw_edma_device_pause(struct dma_chan *dchan)\n{\n\tstruct dw_edma_chan *chan = dchan2dw_edma_chan(dchan);\n\tint err = 0;\n\n\tif (!chan->configured)\n\t\terr = -EPERM;\n\telse if (chan->status != EDMA_ST_BUSY)\n\t\terr = -EPERM;\n\telse if (chan->request != EDMA_REQ_NONE)\n\t\terr = -EPERM;\n\telse\n\t\tchan->request = EDMA_REQ_PAUSE;\n\n\treturn err;\n}\n\nstatic int dw_edma_device_resume(struct dma_chan *dchan)\n{\n\tstruct dw_edma_chan *chan = dchan2dw_edma_chan(dchan);\n\tint err = 0;\n\n\tif (!chan->configured) {\n\t\terr = -EPERM;\n\t} else if (chan->status != EDMA_ST_PAUSE) {\n\t\terr = -EPERM;\n\t} else if (chan->request != EDMA_REQ_NONE) {\n\t\terr = -EPERM;\n\t} else {\n\t\tchan->status = EDMA_ST_BUSY;\n\t\tdw_edma_start_transfer(chan);\n\t}\n\n\treturn err;\n}\n\nstatic int dw_edma_device_terminate_all(struct dma_chan *dchan)\n{\n\tstruct dw_edma_chan *chan = dchan2dw_edma_chan(dchan);\n\tint err = 0;\n\n\tif (!chan->configured) {\n\t\t \n\t} else if (chan->status == EDMA_ST_PAUSE) {\n\t\tchan->status = EDMA_ST_IDLE;\n\t\tchan->configured = false;\n\t} else if (chan->status == EDMA_ST_IDLE) {\n\t\tchan->configured = false;\n\t} else if (dw_edma_core_ch_status(chan) == DMA_COMPLETE) {\n\t\t \n\t\tchan->status = EDMA_ST_IDLE;\n\t\tchan->configured = false;\n\t} else if (chan->request > EDMA_REQ_PAUSE) {\n\t\terr = -EPERM;\n\t} else {\n\t\tchan->request = EDMA_REQ_STOP;\n\t}\n\n\treturn err;\n}\n\nstatic void dw_edma_device_issue_pending(struct dma_chan *dchan)\n{\n\tstruct dw_edma_chan *chan = dchan2dw_edma_chan(dchan);\n\tunsigned long flags;\n\n\tif (!chan->configured)\n\t\treturn;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\tif (vchan_issue_pending(&chan->vc) && chan->request == EDMA_REQ_NONE &&\n\t    chan->status == EDMA_ST_IDLE) {\n\t\tchan->status = EDMA_ST_BUSY;\n\t\tdw_edma_start_transfer(chan);\n\t}\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n}\n\nstatic enum dma_status\ndw_edma_device_tx_status(struct dma_chan *dchan, dma_cookie_t cookie,\n\t\t\t struct dma_tx_state *txstate)\n{\n\tstruct dw_edma_chan *chan = dchan2dw_edma_chan(dchan);\n\tstruct dw_edma_desc *desc;\n\tstruct virt_dma_desc *vd;\n\tunsigned long flags;\n\tenum dma_status ret;\n\tu32 residue = 0;\n\n\tret = dma_cookie_status(dchan, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tif (ret == DMA_IN_PROGRESS && chan->status == EDMA_ST_PAUSE)\n\t\tret = DMA_PAUSED;\n\n\tif (!txstate)\n\t\tgoto ret_residue;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\tvd = vchan_find_desc(&chan->vc, cookie);\n\tif (vd) {\n\t\tdesc = vd2dw_edma_desc(vd);\n\t\tif (desc)\n\t\t\tresidue = desc->alloc_sz - desc->xfer_sz;\n\t}\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\nret_residue:\n\tdma_set_residue(txstate, residue);\n\n\treturn ret;\n}\n\nstatic struct dma_async_tx_descriptor *\ndw_edma_device_transfer(struct dw_edma_transfer *xfer)\n{\n\tstruct dw_edma_chan *chan = dchan2dw_edma_chan(xfer->dchan);\n\tenum dma_transfer_direction dir = xfer->direction;\n\tstruct scatterlist *sg = NULL;\n\tstruct dw_edma_chunk *chunk;\n\tstruct dw_edma_burst *burst;\n\tstruct dw_edma_desc *desc;\n\tu64 src_addr, dst_addr;\n\tsize_t fsz = 0;\n\tu32 cnt = 0;\n\tint i;\n\n\tif (!chan->configured)\n\t\treturn NULL;\n\n\t \n\tif (chan->dw->chip->flags & DW_EDMA_CHIP_LOCAL) {\n\t\tif ((chan->dir == EDMA_DIR_READ && dir != DMA_DEV_TO_MEM) ||\n\t\t    (chan->dir == EDMA_DIR_WRITE && dir != DMA_MEM_TO_DEV))\n\t\t\treturn NULL;\n\t} else {\n\t\tif ((chan->dir == EDMA_DIR_WRITE && dir != DMA_DEV_TO_MEM) ||\n\t\t    (chan->dir == EDMA_DIR_READ && dir != DMA_MEM_TO_DEV))\n\t\t\treturn NULL;\n\t}\n\n\tif (xfer->type == EDMA_XFER_CYCLIC) {\n\t\tif (!xfer->xfer.cyclic.len || !xfer->xfer.cyclic.cnt)\n\t\t\treturn NULL;\n\t} else if (xfer->type == EDMA_XFER_SCATTER_GATHER) {\n\t\tif (xfer->xfer.sg.len < 1)\n\t\t\treturn NULL;\n\t} else if (xfer->type == EDMA_XFER_INTERLEAVED) {\n\t\tif (!xfer->xfer.il->numf || xfer->xfer.il->frame_size < 1)\n\t\t\treturn NULL;\n\t\tif (!xfer->xfer.il->src_inc || !xfer->xfer.il->dst_inc)\n\t\t\treturn NULL;\n\t} else {\n\t\treturn NULL;\n\t}\n\n\tdesc = dw_edma_alloc_desc(chan);\n\tif (unlikely(!desc))\n\t\tgoto err_alloc;\n\n\tchunk = dw_edma_alloc_chunk(desc);\n\tif (unlikely(!chunk))\n\t\tgoto err_alloc;\n\n\tif (xfer->type == EDMA_XFER_INTERLEAVED) {\n\t\tsrc_addr = xfer->xfer.il->src_start;\n\t\tdst_addr = xfer->xfer.il->dst_start;\n\t} else {\n\t\tsrc_addr = chan->config.src_addr;\n\t\tdst_addr = chan->config.dst_addr;\n\t}\n\n\tif (dir == DMA_DEV_TO_MEM)\n\t\tsrc_addr = dw_edma_get_pci_address(chan, (phys_addr_t)src_addr);\n\telse\n\t\tdst_addr = dw_edma_get_pci_address(chan, (phys_addr_t)dst_addr);\n\n\tif (xfer->type == EDMA_XFER_CYCLIC) {\n\t\tcnt = xfer->xfer.cyclic.cnt;\n\t} else if (xfer->type == EDMA_XFER_SCATTER_GATHER) {\n\t\tcnt = xfer->xfer.sg.len;\n\t\tsg = xfer->xfer.sg.sgl;\n\t} else if (xfer->type == EDMA_XFER_INTERLEAVED) {\n\t\tcnt = xfer->xfer.il->numf * xfer->xfer.il->frame_size;\n\t\tfsz = xfer->xfer.il->frame_size;\n\t}\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tif (xfer->type == EDMA_XFER_SCATTER_GATHER && !sg)\n\t\t\tbreak;\n\n\t\tif (chunk->bursts_alloc == chan->ll_max) {\n\t\t\tchunk = dw_edma_alloc_chunk(desc);\n\t\t\tif (unlikely(!chunk))\n\t\t\t\tgoto err_alloc;\n\t\t}\n\n\t\tburst = dw_edma_alloc_burst(chunk);\n\t\tif (unlikely(!burst))\n\t\t\tgoto err_alloc;\n\n\t\tif (xfer->type == EDMA_XFER_CYCLIC)\n\t\t\tburst->sz = xfer->xfer.cyclic.len;\n\t\telse if (xfer->type == EDMA_XFER_SCATTER_GATHER)\n\t\t\tburst->sz = sg_dma_len(sg);\n\t\telse if (xfer->type == EDMA_XFER_INTERLEAVED)\n\t\t\tburst->sz = xfer->xfer.il->sgl[i % fsz].size;\n\n\t\tchunk->ll_region.sz += burst->sz;\n\t\tdesc->alloc_sz += burst->sz;\n\n\t\tif (dir == DMA_DEV_TO_MEM) {\n\t\t\tburst->sar = src_addr;\n\t\t\tif (xfer->type == EDMA_XFER_CYCLIC) {\n\t\t\t\tburst->dar = xfer->xfer.cyclic.paddr;\n\t\t\t} else if (xfer->type == EDMA_XFER_SCATTER_GATHER) {\n\t\t\t\tsrc_addr += sg_dma_len(sg);\n\t\t\t\tburst->dar = sg_dma_address(sg);\n\t\t\t\t \n\t\t\t} else if (xfer->type == EDMA_XFER_INTERLEAVED) {\n\t\t\t\tburst->dar = dst_addr;\n\t\t\t}\n\t\t} else {\n\t\t\tburst->dar = dst_addr;\n\t\t\tif (xfer->type == EDMA_XFER_CYCLIC) {\n\t\t\t\tburst->sar = xfer->xfer.cyclic.paddr;\n\t\t\t} else if (xfer->type == EDMA_XFER_SCATTER_GATHER) {\n\t\t\t\tdst_addr += sg_dma_len(sg);\n\t\t\t\tburst->sar = sg_dma_address(sg);\n\t\t\t\t \n\t\t\t}  else if (xfer->type == EDMA_XFER_INTERLEAVED) {\n\t\t\t\tburst->sar = src_addr;\n\t\t\t}\n\t\t}\n\n\t\tif (xfer->type == EDMA_XFER_SCATTER_GATHER) {\n\t\t\tsg = sg_next(sg);\n\t\t} else if (xfer->type == EDMA_XFER_INTERLEAVED) {\n\t\t\tstruct dma_interleaved_template *il = xfer->xfer.il;\n\t\t\tstruct data_chunk *dc = &il->sgl[i % fsz];\n\n\t\t\tsrc_addr += burst->sz;\n\t\t\tif (il->src_sgl)\n\t\t\t\tsrc_addr += dmaengine_get_src_icg(il, dc);\n\n\t\t\tdst_addr += burst->sz;\n\t\t\tif (il->dst_sgl)\n\t\t\t\tdst_addr += dmaengine_get_dst_icg(il, dc);\n\t\t}\n\t}\n\n\treturn vchan_tx_prep(&chan->vc, &desc->vd, xfer->flags);\n\nerr_alloc:\n\tif (desc)\n\t\tdw_edma_free_desc(desc);\n\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor *\ndw_edma_device_prep_slave_sg(struct dma_chan *dchan, struct scatterlist *sgl,\n\t\t\t     unsigned int len,\n\t\t\t     enum dma_transfer_direction direction,\n\t\t\t     unsigned long flags, void *context)\n{\n\tstruct dw_edma_transfer xfer;\n\n\txfer.dchan = dchan;\n\txfer.direction = direction;\n\txfer.xfer.sg.sgl = sgl;\n\txfer.xfer.sg.len = len;\n\txfer.flags = flags;\n\txfer.type = EDMA_XFER_SCATTER_GATHER;\n\n\treturn dw_edma_device_transfer(&xfer);\n}\n\nstatic struct dma_async_tx_descriptor *\ndw_edma_device_prep_dma_cyclic(struct dma_chan *dchan, dma_addr_t paddr,\n\t\t\t       size_t len, size_t count,\n\t\t\t       enum dma_transfer_direction direction,\n\t\t\t       unsigned long flags)\n{\n\tstruct dw_edma_transfer xfer;\n\n\txfer.dchan = dchan;\n\txfer.direction = direction;\n\txfer.xfer.cyclic.paddr = paddr;\n\txfer.xfer.cyclic.len = len;\n\txfer.xfer.cyclic.cnt = count;\n\txfer.flags = flags;\n\txfer.type = EDMA_XFER_CYCLIC;\n\n\treturn dw_edma_device_transfer(&xfer);\n}\n\nstatic struct dma_async_tx_descriptor *\ndw_edma_device_prep_interleaved_dma(struct dma_chan *dchan,\n\t\t\t\t    struct dma_interleaved_template *ilt,\n\t\t\t\t    unsigned long flags)\n{\n\tstruct dw_edma_transfer xfer;\n\n\txfer.dchan = dchan;\n\txfer.direction = ilt->dir;\n\txfer.xfer.il = ilt;\n\txfer.flags = flags;\n\txfer.type = EDMA_XFER_INTERLEAVED;\n\n\treturn dw_edma_device_transfer(&xfer);\n}\n\nstatic void dw_edma_done_interrupt(struct dw_edma_chan *chan)\n{\n\tstruct dw_edma_desc *desc;\n\tstruct virt_dma_desc *vd;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\tvd = vchan_next_desc(&chan->vc);\n\tif (vd) {\n\t\tswitch (chan->request) {\n\t\tcase EDMA_REQ_NONE:\n\t\t\tdesc = vd2dw_edma_desc(vd);\n\t\t\tif (!desc->chunks_alloc) {\n\t\t\t\tlist_del(&vd->node);\n\t\t\t\tvchan_cookie_complete(vd);\n\t\t\t}\n\n\t\t\t \n\t\t\tchan->status = dw_edma_start_transfer(chan) ? EDMA_ST_BUSY : EDMA_ST_IDLE;\n\t\t\tbreak;\n\n\t\tcase EDMA_REQ_STOP:\n\t\t\tlist_del(&vd->node);\n\t\t\tvchan_cookie_complete(vd);\n\t\t\tchan->request = EDMA_REQ_NONE;\n\t\t\tchan->status = EDMA_ST_IDLE;\n\t\t\tbreak;\n\n\t\tcase EDMA_REQ_PAUSE:\n\t\t\tchan->request = EDMA_REQ_NONE;\n\t\t\tchan->status = EDMA_ST_PAUSE;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n}\n\nstatic void dw_edma_abort_interrupt(struct dw_edma_chan *chan)\n{\n\tstruct virt_dma_desc *vd;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\tvd = vchan_next_desc(&chan->vc);\n\tif (vd) {\n\t\tlist_del(&vd->node);\n\t\tvchan_cookie_complete(vd);\n\t}\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\tchan->request = EDMA_REQ_NONE;\n\tchan->status = EDMA_ST_IDLE;\n}\n\nstatic inline irqreturn_t dw_edma_interrupt_write(int irq, void *data)\n{\n\tstruct dw_edma_irq *dw_irq = data;\n\n\treturn dw_edma_core_handle_int(dw_irq, EDMA_DIR_WRITE,\n\t\t\t\t       dw_edma_done_interrupt,\n\t\t\t\t       dw_edma_abort_interrupt);\n}\n\nstatic inline irqreturn_t dw_edma_interrupt_read(int irq, void *data)\n{\n\tstruct dw_edma_irq *dw_irq = data;\n\n\treturn dw_edma_core_handle_int(dw_irq, EDMA_DIR_READ,\n\t\t\t\t       dw_edma_done_interrupt,\n\t\t\t\t       dw_edma_abort_interrupt);\n}\n\nstatic irqreturn_t dw_edma_interrupt_common(int irq, void *data)\n{\n\tirqreturn_t ret = IRQ_NONE;\n\n\tret |= dw_edma_interrupt_write(irq, data);\n\tret |= dw_edma_interrupt_read(irq, data);\n\n\treturn ret;\n}\n\nstatic int dw_edma_alloc_chan_resources(struct dma_chan *dchan)\n{\n\tstruct dw_edma_chan *chan = dchan2dw_edma_chan(dchan);\n\n\tif (chan->status != EDMA_ST_IDLE)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\nstatic void dw_edma_free_chan_resources(struct dma_chan *dchan)\n{\n\tunsigned long timeout = jiffies + msecs_to_jiffies(5000);\n\tint ret;\n\n\twhile (time_before(jiffies, timeout)) {\n\t\tret = dw_edma_device_terminate_all(dchan);\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\tif (time_after_eq(jiffies, timeout))\n\t\t\treturn;\n\n\t\tcpu_relax();\n\t}\n}\n\nstatic int dw_edma_channel_setup(struct dw_edma *dw, u32 wr_alloc, u32 rd_alloc)\n{\n\tstruct dw_edma_chip *chip = dw->chip;\n\tstruct device *dev = chip->dev;\n\tstruct dw_edma_chan *chan;\n\tstruct dw_edma_irq *irq;\n\tstruct dma_device *dma;\n\tu32 i, ch_cnt;\n\tu32 pos;\n\n\tch_cnt = dw->wr_ch_cnt + dw->rd_ch_cnt;\n\tdma = &dw->dma;\n\n\tINIT_LIST_HEAD(&dma->channels);\n\n\tfor (i = 0; i < ch_cnt; i++) {\n\t\tchan = &dw->chan[i];\n\n\t\tchan->dw = dw;\n\n\t\tif (i < dw->wr_ch_cnt) {\n\t\t\tchan->id = i;\n\t\t\tchan->dir = EDMA_DIR_WRITE;\n\t\t} else {\n\t\t\tchan->id = i - dw->wr_ch_cnt;\n\t\t\tchan->dir = EDMA_DIR_READ;\n\t\t}\n\n\t\tchan->configured = false;\n\t\tchan->request = EDMA_REQ_NONE;\n\t\tchan->status = EDMA_ST_IDLE;\n\n\t\tif (chan->dir == EDMA_DIR_WRITE)\n\t\t\tchan->ll_max = (chip->ll_region_wr[chan->id].sz / EDMA_LL_SZ);\n\t\telse\n\t\t\tchan->ll_max = (chip->ll_region_rd[chan->id].sz / EDMA_LL_SZ);\n\t\tchan->ll_max -= 1;\n\n\t\tdev_vdbg(dev, \"L. List:\\tChannel %s[%u] max_cnt=%u\\n\",\n\t\t\t chan->dir == EDMA_DIR_WRITE ? \"write\" : \"read\",\n\t\t\t chan->id, chan->ll_max);\n\n\t\tif (dw->nr_irqs == 1)\n\t\t\tpos = 0;\n\t\telse if (chan->dir == EDMA_DIR_WRITE)\n\t\t\tpos = chan->id % wr_alloc;\n\t\telse\n\t\t\tpos = wr_alloc + chan->id % rd_alloc;\n\n\t\tirq = &dw->irq[pos];\n\n\t\tif (chan->dir == EDMA_DIR_WRITE)\n\t\t\tirq->wr_mask |= BIT(chan->id);\n\t\telse\n\t\t\tirq->rd_mask |= BIT(chan->id);\n\n\t\tirq->dw = dw;\n\t\tmemcpy(&chan->msi, &irq->msi, sizeof(chan->msi));\n\n\t\tdev_vdbg(dev, \"MSI:\\t\\tChannel %s[%u] addr=0x%.8x%.8x, data=0x%.8x\\n\",\n\t\t\t chan->dir == EDMA_DIR_WRITE  ? \"write\" : \"read\", chan->id,\n\t\t\t chan->msi.address_hi, chan->msi.address_lo,\n\t\t\t chan->msi.data);\n\n\t\tchan->vc.desc_free = vchan_free_desc;\n\t\tchan->vc.chan.private = chan->dir == EDMA_DIR_WRITE ?\n\t\t\t\t\t&dw->chip->dt_region_wr[chan->id] :\n\t\t\t\t\t&dw->chip->dt_region_rd[chan->id];\n\n\t\tvchan_init(&chan->vc, dma);\n\n\t\tdw_edma_core_ch_config(chan);\n\t}\n\n\t \n\tdma_cap_zero(dma->cap_mask);\n\tdma_cap_set(DMA_SLAVE, dma->cap_mask);\n\tdma_cap_set(DMA_CYCLIC, dma->cap_mask);\n\tdma_cap_set(DMA_PRIVATE, dma->cap_mask);\n\tdma_cap_set(DMA_INTERLEAVE, dma->cap_mask);\n\tdma->directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\n\tdma->src_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\tdma->dst_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\tdma->residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\n\t \n\tdma->dev = chip->dev;\n\tdma->device_alloc_chan_resources = dw_edma_alloc_chan_resources;\n\tdma->device_free_chan_resources = dw_edma_free_chan_resources;\n\tdma->device_caps = dw_edma_device_caps;\n\tdma->device_config = dw_edma_device_config;\n\tdma->device_pause = dw_edma_device_pause;\n\tdma->device_resume = dw_edma_device_resume;\n\tdma->device_terminate_all = dw_edma_device_terminate_all;\n\tdma->device_issue_pending = dw_edma_device_issue_pending;\n\tdma->device_tx_status = dw_edma_device_tx_status;\n\tdma->device_prep_slave_sg = dw_edma_device_prep_slave_sg;\n\tdma->device_prep_dma_cyclic = dw_edma_device_prep_dma_cyclic;\n\tdma->device_prep_interleaved_dma = dw_edma_device_prep_interleaved_dma;\n\n\tdma_set_max_seg_size(dma->dev, U32_MAX);\n\n\t \n\treturn dma_async_device_register(dma);\n}\n\nstatic inline void dw_edma_dec_irq_alloc(int *nr_irqs, u32 *alloc, u16 cnt)\n{\n\tif (*nr_irqs && *alloc < cnt) {\n\t\t(*alloc)++;\n\t\t(*nr_irqs)--;\n\t}\n}\n\nstatic inline void dw_edma_add_irq_mask(u32 *mask, u32 alloc, u16 cnt)\n{\n\twhile (*mask * alloc < cnt)\n\t\t(*mask)++;\n}\n\nstatic int dw_edma_irq_request(struct dw_edma *dw,\n\t\t\t       u32 *wr_alloc, u32 *rd_alloc)\n{\n\tstruct dw_edma_chip *chip = dw->chip;\n\tstruct device *dev = dw->chip->dev;\n\tu32 wr_mask = 1;\n\tu32 rd_mask = 1;\n\tint i, err = 0;\n\tu32 ch_cnt;\n\tint irq;\n\n\tch_cnt = dw->wr_ch_cnt + dw->rd_ch_cnt;\n\n\tif (chip->nr_irqs < 1 || !chip->ops->irq_vector)\n\t\treturn -EINVAL;\n\n\tdw->irq = devm_kcalloc(dev, chip->nr_irqs, sizeof(*dw->irq), GFP_KERNEL);\n\tif (!dw->irq)\n\t\treturn -ENOMEM;\n\n\tif (chip->nr_irqs == 1) {\n\t\t \n\t\tirq = chip->ops->irq_vector(dev, 0);\n\t\terr = request_irq(irq, dw_edma_interrupt_common,\n\t\t\t\t  IRQF_SHARED, dw->name, &dw->irq[0]);\n\t\tif (err) {\n\t\t\tdw->nr_irqs = 0;\n\t\t\treturn err;\n\t\t}\n\n\t\tif (irq_get_msi_desc(irq))\n\t\t\tget_cached_msi_msg(irq, &dw->irq[0].msi);\n\n\t\tdw->nr_irqs = 1;\n\t} else {\n\t\t \n\t\tint tmp = chip->nr_irqs;\n\n\t\twhile (tmp && (*wr_alloc + *rd_alloc) < ch_cnt) {\n\t\t\tdw_edma_dec_irq_alloc(&tmp, wr_alloc, dw->wr_ch_cnt);\n\t\t\tdw_edma_dec_irq_alloc(&tmp, rd_alloc, dw->rd_ch_cnt);\n\t\t}\n\n\t\tdw_edma_add_irq_mask(&wr_mask, *wr_alloc, dw->wr_ch_cnt);\n\t\tdw_edma_add_irq_mask(&rd_mask, *rd_alloc, dw->rd_ch_cnt);\n\n\t\tfor (i = 0; i < (*wr_alloc + *rd_alloc); i++) {\n\t\t\tirq = chip->ops->irq_vector(dev, i);\n\t\t\terr = request_irq(irq,\n\t\t\t\t\t  i < *wr_alloc ?\n\t\t\t\t\t\tdw_edma_interrupt_write :\n\t\t\t\t\t\tdw_edma_interrupt_read,\n\t\t\t\t\t  IRQF_SHARED, dw->name,\n\t\t\t\t\t  &dw->irq[i]);\n\t\t\tif (err)\n\t\t\t\tgoto err_irq_free;\n\n\t\t\tif (irq_get_msi_desc(irq))\n\t\t\t\tget_cached_msi_msg(irq, &dw->irq[i].msi);\n\t\t}\n\n\t\tdw->nr_irqs = i;\n\t}\n\n\treturn 0;\n\nerr_irq_free:\n\tfor  (i--; i >= 0; i--) {\n\t\tirq = chip->ops->irq_vector(dev, i);\n\t\tfree_irq(irq, &dw->irq[i]);\n\t}\n\n\treturn err;\n}\n\nint dw_edma_probe(struct dw_edma_chip *chip)\n{\n\tstruct device *dev;\n\tstruct dw_edma *dw;\n\tu32 wr_alloc = 0;\n\tu32 rd_alloc = 0;\n\tint i, err;\n\n\tif (!chip)\n\t\treturn -EINVAL;\n\n\tdev = chip->dev;\n\tif (!dev || !chip->ops)\n\t\treturn -EINVAL;\n\n\tdw = devm_kzalloc(dev, sizeof(*dw), GFP_KERNEL);\n\tif (!dw)\n\t\treturn -ENOMEM;\n\n\tdw->chip = chip;\n\n\tif (dw->chip->mf == EDMA_MF_HDMA_NATIVE)\n\t\tdw_hdma_v0_core_register(dw);\n\telse\n\t\tdw_edma_v0_core_register(dw);\n\n\traw_spin_lock_init(&dw->lock);\n\n\tdw->wr_ch_cnt = min_t(u16, chip->ll_wr_cnt,\n\t\t\t      dw_edma_core_ch_count(dw, EDMA_DIR_WRITE));\n\tdw->wr_ch_cnt = min_t(u16, dw->wr_ch_cnt, EDMA_MAX_WR_CH);\n\n\tdw->rd_ch_cnt = min_t(u16, chip->ll_rd_cnt,\n\t\t\t      dw_edma_core_ch_count(dw, EDMA_DIR_READ));\n\tdw->rd_ch_cnt = min_t(u16, dw->rd_ch_cnt, EDMA_MAX_RD_CH);\n\n\tif (!dw->wr_ch_cnt && !dw->rd_ch_cnt)\n\t\treturn -EINVAL;\n\n\tdev_vdbg(dev, \"Channels:\\twrite=%d, read=%d\\n\",\n\t\t dw->wr_ch_cnt, dw->rd_ch_cnt);\n\n\t \n\tdw->chan = devm_kcalloc(dev, dw->wr_ch_cnt + dw->rd_ch_cnt,\n\t\t\t\tsizeof(*dw->chan), GFP_KERNEL);\n\tif (!dw->chan)\n\t\treturn -ENOMEM;\n\n\tsnprintf(dw->name, sizeof(dw->name), \"dw-edma-core:%s\",\n\t\t dev_name(chip->dev));\n\n\t \n\tdw_edma_core_off(dw);\n\n\t \n\terr = dw_edma_irq_request(dw, &wr_alloc, &rd_alloc);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = dw_edma_channel_setup(dw, wr_alloc, rd_alloc);\n\tif (err)\n\t\tgoto err_irq_free;\n\n\t \n\tdw_edma_core_debugfs_on(dw);\n\n\tchip->dw = dw;\n\n\treturn 0;\n\nerr_irq_free:\n\tfor (i = (dw->nr_irqs - 1); i >= 0; i--)\n\t\tfree_irq(chip->ops->irq_vector(dev, i), &dw->irq[i]);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(dw_edma_probe);\n\nint dw_edma_remove(struct dw_edma_chip *chip)\n{\n\tstruct dw_edma_chan *chan, *_chan;\n\tstruct device *dev = chip->dev;\n\tstruct dw_edma *dw = chip->dw;\n\tint i;\n\n\t \n\tif (!dw)\n\t\treturn -ENODEV;\n\n\t \n\tdw_edma_core_off(dw);\n\n\t \n\tfor (i = (dw->nr_irqs - 1); i >= 0; i--)\n\t\tfree_irq(chip->ops->irq_vector(dev, i), &dw->irq[i]);\n\n\t \n\tdma_async_device_unregister(&dw->dma);\n\tlist_for_each_entry_safe(chan, _chan, &dw->dma.channels,\n\t\t\t\t vc.chan.device_node) {\n\t\ttasklet_kill(&chan->vc.task);\n\t\tlist_del(&chan->vc.chan.device_node);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dw_edma_remove);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"Synopsys DesignWare eDMA controller core driver\");\nMODULE_AUTHOR(\"Gustavo Pimentel <gustavo.pimentel@synopsys.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}