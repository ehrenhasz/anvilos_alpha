{
  "module_name": "dma.c",
  "hash_id": "b088ac6ffde9f18a9a857987c5a67098b5711b7cf467055e8dd3d14d97fcd667",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/ioat/dma.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/pci.h>\n#include <linux/interrupt.h>\n#include <linux/dmaengine.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/workqueue.h>\n#include <linux/prefetch.h>\n#include <linux/sizes.h>\n#include \"dma.h\"\n#include \"registers.h\"\n#include \"hw.h\"\n\n#include \"../dmaengine.h\"\n\nstatic int completion_timeout = 200;\nmodule_param(completion_timeout, int, 0644);\nMODULE_PARM_DESC(completion_timeout,\n\t\t\"set ioat completion timeout [msec] (default 200 [msec])\");\nstatic int idle_timeout = 2000;\nmodule_param(idle_timeout, int, 0644);\nMODULE_PARM_DESC(idle_timeout,\n\t\t\"set ioat idle timeout [msec] (default 2000 [msec])\");\n\n#define IDLE_TIMEOUT msecs_to_jiffies(idle_timeout)\n#define COMPLETION_TIMEOUT msecs_to_jiffies(completion_timeout)\n\nstatic char *chanerr_str[] = {\n\t\"DMA Transfer Source Address Error\",\n\t\"DMA Transfer Destination Address Error\",\n\t\"Next Descriptor Address Error\",\n\t\"Descriptor Error\",\n\t\"Chan Address Value Error\",\n\t\"CHANCMD Error\",\n\t\"Chipset Uncorrectable Data Integrity Error\",\n\t\"DMA Uncorrectable Data Integrity Error\",\n\t\"Read Data Error\",\n\t\"Write Data Error\",\n\t\"Descriptor Control Error\",\n\t\"Descriptor Transfer Size Error\",\n\t\"Completion Address Error\",\n\t\"Interrupt Configuration Error\",\n\t\"Super extended descriptor Address Error\",\n\t\"Unaffiliated Error\",\n\t\"CRC or XOR P Error\",\n\t\"XOR Q Error\",\n\t\"Descriptor Count Error\",\n\t\"DIF All F detect Error\",\n\t\"Guard Tag verification Error\",\n\t\"Application Tag verification Error\",\n\t\"Reference Tag verification Error\",\n\t\"Bundle Bit Error\",\n\t\"Result DIF All F detect Error\",\n\t\"Result Guard Tag verification Error\",\n\t\"Result Application Tag verification Error\",\n\t\"Result Reference Tag verification Error\",\n};\n\nstatic void ioat_eh(struct ioatdma_chan *ioat_chan);\n\nstatic void ioat_print_chanerrs(struct ioatdma_chan *ioat_chan, u32 chanerr)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(chanerr_str); i++) {\n\t\tif ((chanerr >> i) & 1) {\n\t\t\tdev_err(to_dev(ioat_chan), \"Err(%d): %s\\n\",\n\t\t\t\ti, chanerr_str[i]);\n\t\t}\n\t}\n}\n\n \nirqreturn_t ioat_dma_do_interrupt(int irq, void *data)\n{\n\tstruct ioatdma_device *instance = data;\n\tstruct ioatdma_chan *ioat_chan;\n\tunsigned long attnstatus;\n\tint bit;\n\tu8 intrctrl;\n\n\tintrctrl = readb(instance->reg_base + IOAT_INTRCTRL_OFFSET);\n\n\tif (!(intrctrl & IOAT_INTRCTRL_MASTER_INT_EN))\n\t\treturn IRQ_NONE;\n\n\tif (!(intrctrl & IOAT_INTRCTRL_INT_STATUS)) {\n\t\twriteb(intrctrl, instance->reg_base + IOAT_INTRCTRL_OFFSET);\n\t\treturn IRQ_NONE;\n\t}\n\n\tattnstatus = readl(instance->reg_base + IOAT_ATTNSTATUS_OFFSET);\n\tfor_each_set_bit(bit, &attnstatus, BITS_PER_LONG) {\n\t\tioat_chan = ioat_chan_by_index(instance, bit);\n\t\tif (test_bit(IOAT_RUN, &ioat_chan->state))\n\t\t\ttasklet_schedule(&ioat_chan->cleanup_task);\n\t}\n\n\twriteb(intrctrl, instance->reg_base + IOAT_INTRCTRL_OFFSET);\n\treturn IRQ_HANDLED;\n}\n\n \nirqreturn_t ioat_dma_do_interrupt_msix(int irq, void *data)\n{\n\tstruct ioatdma_chan *ioat_chan = data;\n\n\tif (test_bit(IOAT_RUN, &ioat_chan->state))\n\t\ttasklet_schedule(&ioat_chan->cleanup_task);\n\n\treturn IRQ_HANDLED;\n}\n\nvoid ioat_stop(struct ioatdma_chan *ioat_chan)\n{\n\tstruct ioatdma_device *ioat_dma = ioat_chan->ioat_dma;\n\tstruct pci_dev *pdev = ioat_dma->pdev;\n\tint chan_id = chan_num(ioat_chan);\n\tstruct msix_entry *msix;\n\n\t \n\tclear_bit(IOAT_RUN, &ioat_chan->state);\n\n\t \n\tswitch (ioat_dma->irq_mode) {\n\tcase IOAT_MSIX:\n\t\tmsix = &ioat_dma->msix_entries[chan_id];\n\t\tsynchronize_irq(msix->vector);\n\t\tbreak;\n\tcase IOAT_MSI:\n\tcase IOAT_INTX:\n\t\tsynchronize_irq(pdev->irq);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tdel_timer_sync(&ioat_chan->timer);\n\n\t \n\ttasklet_kill(&ioat_chan->cleanup_task);\n\n\t \n\tioat_cleanup_event(&ioat_chan->cleanup_task);\n}\n\nstatic void __ioat_issue_pending(struct ioatdma_chan *ioat_chan)\n{\n\tioat_chan->dmacount += ioat_ring_pending(ioat_chan);\n\tioat_chan->issued = ioat_chan->head;\n\twritew(ioat_chan->dmacount,\n\t       ioat_chan->reg_base + IOAT_CHAN_DMACOUNT_OFFSET);\n\tdev_dbg(to_dev(ioat_chan),\n\t\t\"%s: head: %#x tail: %#x issued: %#x count: %#x\\n\",\n\t\t__func__, ioat_chan->head, ioat_chan->tail,\n\t\tioat_chan->issued, ioat_chan->dmacount);\n}\n\nvoid ioat_issue_pending(struct dma_chan *c)\n{\n\tstruct ioatdma_chan *ioat_chan = to_ioat_chan(c);\n\n\tif (ioat_ring_pending(ioat_chan)) {\n\t\tspin_lock_bh(&ioat_chan->prep_lock);\n\t\t__ioat_issue_pending(ioat_chan);\n\t\tspin_unlock_bh(&ioat_chan->prep_lock);\n\t}\n}\n\n \nstatic void ioat_update_pending(struct ioatdma_chan *ioat_chan)\n{\n\tif (ioat_ring_pending(ioat_chan) > ioat_pending_level)\n\t\t__ioat_issue_pending(ioat_chan);\n}\n\nstatic void __ioat_start_null_desc(struct ioatdma_chan *ioat_chan)\n{\n\tstruct ioat_ring_ent *desc;\n\tstruct ioat_dma_descriptor *hw;\n\n\tif (ioat_ring_space(ioat_chan) < 1) {\n\t\tdev_err(to_dev(ioat_chan),\n\t\t\t\"Unable to start null desc - ring full\\n\");\n\t\treturn;\n\t}\n\n\tdev_dbg(to_dev(ioat_chan),\n\t\t\"%s: head: %#x tail: %#x issued: %#x\\n\",\n\t\t__func__, ioat_chan->head, ioat_chan->tail, ioat_chan->issued);\n\tdesc = ioat_get_ring_ent(ioat_chan, ioat_chan->head);\n\n\thw = desc->hw;\n\thw->ctl = 0;\n\thw->ctl_f.null = 1;\n\thw->ctl_f.int_en = 1;\n\thw->ctl_f.compl_write = 1;\n\t \n\thw->size = NULL_DESC_BUFFER_SIZE;\n\thw->src_addr = 0;\n\thw->dst_addr = 0;\n\tasync_tx_ack(&desc->txd);\n\tioat_set_chainaddr(ioat_chan, desc->txd.phys);\n\tdump_desc_dbg(ioat_chan, desc);\n\t \n\twmb();\n\tioat_chan->head += 1;\n\t__ioat_issue_pending(ioat_chan);\n}\n\nvoid ioat_start_null_desc(struct ioatdma_chan *ioat_chan)\n{\n\tspin_lock_bh(&ioat_chan->prep_lock);\n\tif (!test_bit(IOAT_CHAN_DOWN, &ioat_chan->state))\n\t\t__ioat_start_null_desc(ioat_chan);\n\tspin_unlock_bh(&ioat_chan->prep_lock);\n}\n\nstatic void __ioat_restart_chan(struct ioatdma_chan *ioat_chan)\n{\n\t \n\tioat_chan->issued = ioat_chan->tail;\n\tioat_chan->dmacount = 0;\n\tmod_timer(&ioat_chan->timer, jiffies + COMPLETION_TIMEOUT);\n\n\tdev_dbg(to_dev(ioat_chan),\n\t\t\"%s: head: %#x tail: %#x issued: %#x count: %#x\\n\",\n\t\t__func__, ioat_chan->head, ioat_chan->tail,\n\t\tioat_chan->issued, ioat_chan->dmacount);\n\n\tif (ioat_ring_pending(ioat_chan)) {\n\t\tstruct ioat_ring_ent *desc;\n\n\t\tdesc = ioat_get_ring_ent(ioat_chan, ioat_chan->tail);\n\t\tioat_set_chainaddr(ioat_chan, desc->txd.phys);\n\t\t__ioat_issue_pending(ioat_chan);\n\t} else\n\t\t__ioat_start_null_desc(ioat_chan);\n}\n\nstatic int ioat_quiesce(struct ioatdma_chan *ioat_chan, unsigned long tmo)\n{\n\tunsigned long end = jiffies + tmo;\n\tint err = 0;\n\tu32 status;\n\n\tstatus = ioat_chansts(ioat_chan);\n\tif (is_ioat_active(status) || is_ioat_idle(status))\n\t\tioat_suspend(ioat_chan);\n\twhile (is_ioat_active(status) || is_ioat_idle(status)) {\n\t\tif (tmo && time_after(jiffies, end)) {\n\t\t\terr = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t\tstatus = ioat_chansts(ioat_chan);\n\t\tcpu_relax();\n\t}\n\n\treturn err;\n}\n\nstatic int ioat_reset_sync(struct ioatdma_chan *ioat_chan, unsigned long tmo)\n{\n\tunsigned long end = jiffies + tmo;\n\tint err = 0;\n\n\tioat_reset(ioat_chan);\n\twhile (ioat_reset_pending(ioat_chan)) {\n\t\tif (end && time_after(jiffies, end)) {\n\t\t\terr = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t\tcpu_relax();\n\t}\n\n\treturn err;\n}\n\nstatic dma_cookie_t ioat_tx_submit_unlock(struct dma_async_tx_descriptor *tx)\n\t__releases(&ioat_chan->prep_lock)\n{\n\tstruct dma_chan *c = tx->chan;\n\tstruct ioatdma_chan *ioat_chan = to_ioat_chan(c);\n\tdma_cookie_t cookie;\n\n\tcookie = dma_cookie_assign(tx);\n\tdev_dbg(to_dev(ioat_chan), \"%s: cookie: %d\\n\", __func__, cookie);\n\n\tif (!test_and_set_bit(IOAT_CHAN_ACTIVE, &ioat_chan->state))\n\t\tmod_timer(&ioat_chan->timer, jiffies + COMPLETION_TIMEOUT);\n\n\t \n\twmb();\n\n\tioat_chan->head += ioat_chan->produce;\n\n\tioat_update_pending(ioat_chan);\n\tspin_unlock_bh(&ioat_chan->prep_lock);\n\n\treturn cookie;\n}\n\nstatic struct ioat_ring_ent *\nioat_alloc_ring_ent(struct dma_chan *chan, int idx, gfp_t flags)\n{\n\tstruct ioat_dma_descriptor *hw;\n\tstruct ioat_ring_ent *desc;\n\tstruct ioatdma_chan *ioat_chan = to_ioat_chan(chan);\n\tint chunk;\n\tdma_addr_t phys;\n\tu8 *pos;\n\toff_t offs;\n\n\tchunk = idx / IOAT_DESCS_PER_CHUNK;\n\tidx &= (IOAT_DESCS_PER_CHUNK - 1);\n\toffs = idx * IOAT_DESC_SZ;\n\tpos = (u8 *)ioat_chan->descs[chunk].virt + offs;\n\tphys = ioat_chan->descs[chunk].hw + offs;\n\thw = (struct ioat_dma_descriptor *)pos;\n\tmemset(hw, 0, sizeof(*hw));\n\n\tdesc = kmem_cache_zalloc(ioat_cache, flags);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdma_async_tx_descriptor_init(&desc->txd, chan);\n\tdesc->txd.tx_submit = ioat_tx_submit_unlock;\n\tdesc->hw = hw;\n\tdesc->txd.phys = phys;\n\treturn desc;\n}\n\nvoid ioat_free_ring_ent(struct ioat_ring_ent *desc, struct dma_chan *chan)\n{\n\tkmem_cache_free(ioat_cache, desc);\n}\n\nstruct ioat_ring_ent **\nioat_alloc_ring(struct dma_chan *c, int order, gfp_t flags)\n{\n\tstruct ioatdma_chan *ioat_chan = to_ioat_chan(c);\n\tstruct ioatdma_device *ioat_dma = ioat_chan->ioat_dma;\n\tstruct ioat_ring_ent **ring;\n\tint total_descs = 1 << order;\n\tint i, chunks;\n\n\t \n\tring = kcalloc(total_descs, sizeof(*ring), flags);\n\tif (!ring)\n\t\treturn NULL;\n\n\tchunks = (total_descs * IOAT_DESC_SZ) / IOAT_CHUNK_SIZE;\n\tioat_chan->desc_chunks = chunks;\n\n\tfor (i = 0; i < chunks; i++) {\n\t\tstruct ioat_descs *descs = &ioat_chan->descs[i];\n\n\t\tdescs->virt = dma_alloc_coherent(to_dev(ioat_chan),\n\t\t\t\t\tIOAT_CHUNK_SIZE, &descs->hw, flags);\n\t\tif (!descs->virt) {\n\t\t\tint idx;\n\n\t\t\tfor (idx = 0; idx < i; idx++) {\n\t\t\t\tdescs = &ioat_chan->descs[idx];\n\t\t\t\tdma_free_coherent(to_dev(ioat_chan),\n\t\t\t\t\t\tIOAT_CHUNK_SIZE,\n\t\t\t\t\t\tdescs->virt, descs->hw);\n\t\t\t\tdescs->virt = NULL;\n\t\t\t\tdescs->hw = 0;\n\t\t\t}\n\n\t\t\tioat_chan->desc_chunks = 0;\n\t\t\tkfree(ring);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tfor (i = 0; i < total_descs; i++) {\n\t\tring[i] = ioat_alloc_ring_ent(c, i, flags);\n\t\tif (!ring[i]) {\n\t\t\tint idx;\n\n\t\t\twhile (i--)\n\t\t\t\tioat_free_ring_ent(ring[i], c);\n\n\t\t\tfor (idx = 0; idx < ioat_chan->desc_chunks; idx++) {\n\t\t\t\tdma_free_coherent(to_dev(ioat_chan),\n\t\t\t\t\t\t  IOAT_CHUNK_SIZE,\n\t\t\t\t\t\t  ioat_chan->descs[idx].virt,\n\t\t\t\t\t\t  ioat_chan->descs[idx].hw);\n\t\t\t\tioat_chan->descs[idx].virt = NULL;\n\t\t\t\tioat_chan->descs[idx].hw = 0;\n\t\t\t}\n\n\t\t\tioat_chan->desc_chunks = 0;\n\t\t\tkfree(ring);\n\t\t\treturn NULL;\n\t\t}\n\t\tset_desc_id(ring[i], i);\n\t}\n\n\t \n\tfor (i = 0; i < total_descs-1; i++) {\n\t\tstruct ioat_ring_ent *next = ring[i+1];\n\t\tstruct ioat_dma_descriptor *hw = ring[i]->hw;\n\n\t\thw->next = next->txd.phys;\n\t}\n\tring[i]->hw->next = ring[0]->txd.phys;\n\n\t \n\tif (ioat_dma->cap & IOAT_CAP_DPS) {\n\t\tu16 drsctl = IOAT_CHAN_DRSZ_2MB | IOAT_CHAN_DRS_EN;\n\n\t\tif (chunks == 1)\n\t\t\tdrsctl |= IOAT_CHAN_DRS_AUTOWRAP;\n\n\t\twritew(drsctl, ioat_chan->reg_base + IOAT_CHAN_DRSCTL_OFFSET);\n\n\t}\n\n\treturn ring;\n}\n\n \nint ioat_check_space_lock(struct ioatdma_chan *ioat_chan, int num_descs)\n\t__acquires(&ioat_chan->prep_lock)\n{\n\tspin_lock_bh(&ioat_chan->prep_lock);\n\t \n\tif (likely(ioat_ring_space(ioat_chan) > num_descs)) {\n\t\tdev_dbg(to_dev(ioat_chan), \"%s: num_descs: %d (%x:%x:%x)\\n\",\n\t\t\t__func__, num_descs, ioat_chan->head,\n\t\t\tioat_chan->tail, ioat_chan->issued);\n\t\tioat_chan->produce = num_descs;\n\t\treturn 0;   \n\t}\n\tspin_unlock_bh(&ioat_chan->prep_lock);\n\n\tdev_dbg_ratelimited(to_dev(ioat_chan),\n\t\t\t    \"%s: ring full! num_descs: %d (%x:%x:%x)\\n\",\n\t\t\t    __func__, num_descs, ioat_chan->head,\n\t\t\t    ioat_chan->tail, ioat_chan->issued);\n\n\t \n\tif (time_is_before_jiffies(ioat_chan->timer.expires)\n\t    && timer_pending(&ioat_chan->timer)) {\n\t\tmod_timer(&ioat_chan->timer, jiffies + COMPLETION_TIMEOUT);\n\t\tioat_timer_event(&ioat_chan->timer);\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic bool desc_has_ext(struct ioat_ring_ent *desc)\n{\n\tstruct ioat_dma_descriptor *hw = desc->hw;\n\n\tif (hw->ctl_f.op == IOAT_OP_XOR ||\n\t    hw->ctl_f.op == IOAT_OP_XOR_VAL) {\n\t\tstruct ioat_xor_descriptor *xor = desc->xor;\n\n\t\tif (src_cnt_to_sw(xor->ctl_f.src_cnt) > 5)\n\t\t\treturn true;\n\t} else if (hw->ctl_f.op == IOAT_OP_PQ ||\n\t\t   hw->ctl_f.op == IOAT_OP_PQ_VAL) {\n\t\tstruct ioat_pq_descriptor *pq = desc->pq;\n\n\t\tif (src_cnt_to_sw(pq->ctl_f.src_cnt) > 3)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void\nioat_free_sed(struct ioatdma_device *ioat_dma, struct ioat_sed_ent *sed)\n{\n\tif (!sed)\n\t\treturn;\n\n\tdma_pool_free(ioat_dma->sed_hw_pool[sed->hw_pool], sed->hw, sed->dma);\n\tkmem_cache_free(ioat_sed_cache, sed);\n}\n\nstatic u64 ioat_get_current_completion(struct ioatdma_chan *ioat_chan)\n{\n\tu64 phys_complete;\n\tu64 completion;\n\n\tcompletion = *ioat_chan->completion;\n\tphys_complete = ioat_chansts_to_addr(completion);\n\n\tdev_dbg(to_dev(ioat_chan), \"%s: phys_complete: %#llx\\n\", __func__,\n\t\t(unsigned long long) phys_complete);\n\n\treturn phys_complete;\n}\n\nstatic bool ioat_cleanup_preamble(struct ioatdma_chan *ioat_chan,\n\t\t\t\t   u64 *phys_complete)\n{\n\t*phys_complete = ioat_get_current_completion(ioat_chan);\n\tif (*phys_complete == ioat_chan->last_completion)\n\t\treturn false;\n\n\tclear_bit(IOAT_COMPLETION_ACK, &ioat_chan->state);\n\tmod_timer(&ioat_chan->timer, jiffies + COMPLETION_TIMEOUT);\n\n\treturn true;\n}\n\nstatic void\ndesc_get_errstat(struct ioatdma_chan *ioat_chan, struct ioat_ring_ent *desc)\n{\n\tstruct ioat_dma_descriptor *hw = desc->hw;\n\n\tswitch (hw->ctl_f.op) {\n\tcase IOAT_OP_PQ_VAL:\n\tcase IOAT_OP_PQ_VAL_16S:\n\t{\n\t\tstruct ioat_pq_descriptor *pq = desc->pq;\n\n\t\t \n\t\tif (!pq->dwbes_f.wbes)\n\t\t\treturn;\n\n\t\t \n\n\t\tif (pq->dwbes_f.p_val_err)\n\t\t\t*desc->result |= SUM_CHECK_P_RESULT;\n\n\t\tif (pq->dwbes_f.q_val_err)\n\t\t\t*desc->result |= SUM_CHECK_Q_RESULT;\n\n\t\treturn;\n\t}\n\tdefault:\n\t\treturn;\n\t}\n}\n\n \nstatic void __ioat_cleanup(struct ioatdma_chan *ioat_chan, dma_addr_t phys_complete)\n{\n\tstruct ioatdma_device *ioat_dma = ioat_chan->ioat_dma;\n\tstruct ioat_ring_ent *desc;\n\tbool seen_current = false;\n\tint idx = ioat_chan->tail, i;\n\tu16 active;\n\n\tdev_dbg(to_dev(ioat_chan), \"%s: head: %#x tail: %#x issued: %#x\\n\",\n\t\t__func__, ioat_chan->head, ioat_chan->tail, ioat_chan->issued);\n\n\t \n\tif (!phys_complete)\n\t\treturn;\n\n\tactive = ioat_ring_active(ioat_chan);\n\tfor (i = 0; i < active && !seen_current; i++) {\n\t\tstruct dma_async_tx_descriptor *tx;\n\n\t\tprefetch(ioat_get_ring_ent(ioat_chan, idx + i + 1));\n\t\tdesc = ioat_get_ring_ent(ioat_chan, idx + i);\n\t\tdump_desc_dbg(ioat_chan, desc);\n\n\t\t \n\t\tif (ioat_dma->cap & IOAT_CAP_DWBES)\n\t\t\tdesc_get_errstat(ioat_chan, desc);\n\n\t\ttx = &desc->txd;\n\t\tif (tx->cookie) {\n\t\t\tdma_cookie_complete(tx);\n\t\t\tdma_descriptor_unmap(tx);\n\t\t\tdmaengine_desc_get_callback_invoke(tx, NULL);\n\t\t\ttx->callback = NULL;\n\t\t\ttx->callback_result = NULL;\n\t\t}\n\n\t\tif (tx->phys == phys_complete)\n\t\t\tseen_current = true;\n\n\t\t \n\t\tif (desc_has_ext(desc)) {\n\t\t\tBUG_ON(i + 1 >= active);\n\t\t\ti++;\n\t\t}\n\n\t\t \n\t\tif (desc->sed) {\n\t\t\tioat_free_sed(ioat_dma, desc->sed);\n\t\t\tdesc->sed = NULL;\n\t\t}\n\t}\n\n\t \n\tsmp_mb();\n\tioat_chan->tail = idx + i;\n\t \n\tBUG_ON(active && !seen_current);\n\tioat_chan->last_completion = phys_complete;\n\n\tif (active - i == 0) {\n\t\tdev_dbg(to_dev(ioat_chan), \"%s: cancel completion timeout\\n\",\n\t\t\t__func__);\n\t\tmod_timer_pending(&ioat_chan->timer, jiffies + IDLE_TIMEOUT);\n\t}\n\n\t \n\tif (ioat_chan->intr_coalesce != ioat_chan->prev_intr_coalesce) {\n\t\twritew(min((ioat_chan->intr_coalesce * (active - i)),\n\t\t       IOAT_INTRDELAY_MASK),\n\t\t       ioat_chan->ioat_dma->reg_base + IOAT_INTRDELAY_OFFSET);\n\t\tioat_chan->prev_intr_coalesce = ioat_chan->intr_coalesce;\n\t}\n}\n\nstatic void ioat_cleanup(struct ioatdma_chan *ioat_chan)\n{\n\tu64 phys_complete;\n\n\tspin_lock_bh(&ioat_chan->cleanup_lock);\n\n\tif (ioat_cleanup_preamble(ioat_chan, &phys_complete))\n\t\t__ioat_cleanup(ioat_chan, phys_complete);\n\n\tif (is_ioat_halted(*ioat_chan->completion)) {\n\t\tu32 chanerr = readl(ioat_chan->reg_base + IOAT_CHANERR_OFFSET);\n\n\t\tif (chanerr &\n\t\t    (IOAT_CHANERR_HANDLE_MASK | IOAT_CHANERR_RECOVER_MASK)) {\n\t\t\tmod_timer_pending(&ioat_chan->timer, jiffies + IDLE_TIMEOUT);\n\t\t\tioat_eh(ioat_chan);\n\t\t}\n\t}\n\n\tspin_unlock_bh(&ioat_chan->cleanup_lock);\n}\n\nvoid ioat_cleanup_event(struct tasklet_struct *t)\n{\n\tstruct ioatdma_chan *ioat_chan = from_tasklet(ioat_chan, t, cleanup_task);\n\n\tioat_cleanup(ioat_chan);\n\tif (!test_bit(IOAT_RUN, &ioat_chan->state))\n\t\treturn;\n\twritew(IOAT_CHANCTRL_RUN, ioat_chan->reg_base + IOAT_CHANCTRL_OFFSET);\n}\n\nstatic void ioat_restart_channel(struct ioatdma_chan *ioat_chan)\n{\n\tu64 phys_complete;\n\n\t \n\twritel(lower_32_bits(ioat_chan->completion_dma),\n\t       ioat_chan->reg_base + IOAT_CHANCMP_OFFSET_LOW);\n\twritel(upper_32_bits(ioat_chan->completion_dma),\n\t       ioat_chan->reg_base + IOAT_CHANCMP_OFFSET_HIGH);\n\n\tioat_quiesce(ioat_chan, 0);\n\tif (ioat_cleanup_preamble(ioat_chan, &phys_complete))\n\t\t__ioat_cleanup(ioat_chan, phys_complete);\n\n\t__ioat_restart_chan(ioat_chan);\n}\n\n\nstatic void ioat_abort_descs(struct ioatdma_chan *ioat_chan)\n{\n\tstruct ioatdma_device *ioat_dma = ioat_chan->ioat_dma;\n\tstruct ioat_ring_ent *desc;\n\tu16 active;\n\tint idx = ioat_chan->tail, i;\n\n\t \n\tactive = ioat_ring_active(ioat_chan);\n\n\t \n\tfor (i = 1; i < active; i++) {\n\t\tstruct dma_async_tx_descriptor *tx;\n\n\t\tprefetch(ioat_get_ring_ent(ioat_chan, idx + i + 1));\n\t\tdesc = ioat_get_ring_ent(ioat_chan, idx + i);\n\n\t\ttx = &desc->txd;\n\t\tif (tx->cookie) {\n\t\t\tstruct dmaengine_result res;\n\n\t\t\tdma_cookie_complete(tx);\n\t\t\tdma_descriptor_unmap(tx);\n\t\t\tres.result = DMA_TRANS_ABORTED;\n\t\t\tdmaengine_desc_get_callback_invoke(tx, &res);\n\t\t\ttx->callback = NULL;\n\t\t\ttx->callback_result = NULL;\n\t\t}\n\n\t\t \n\t\tif (desc_has_ext(desc)) {\n\t\t\tWARN_ON(i + 1 >= active);\n\t\t\ti++;\n\t\t}\n\n\t\t \n\t\tif (desc->sed) {\n\t\t\tioat_free_sed(ioat_dma, desc->sed);\n\t\t\tdesc->sed = NULL;\n\t\t}\n\t}\n\n\tsmp_mb();  \n\tioat_chan->tail = idx + active;\n\n\tdesc = ioat_get_ring_ent(ioat_chan, ioat_chan->tail);\n\tioat_chan->last_completion = *ioat_chan->completion = desc->txd.phys;\n}\n\nstatic void ioat_eh(struct ioatdma_chan *ioat_chan)\n{\n\tstruct pci_dev *pdev = to_pdev(ioat_chan);\n\tstruct ioat_dma_descriptor *hw;\n\tstruct dma_async_tx_descriptor *tx;\n\tu64 phys_complete;\n\tstruct ioat_ring_ent *desc;\n\tu32 err_handled = 0;\n\tu32 chanerr_int;\n\tu32 chanerr;\n\tbool abort = false;\n\tstruct dmaengine_result res;\n\n\t \n\tif (ioat_cleanup_preamble(ioat_chan, &phys_complete))\n\t\t__ioat_cleanup(ioat_chan, phys_complete);\n\n\tchanerr = readl(ioat_chan->reg_base + IOAT_CHANERR_OFFSET);\n\tpci_read_config_dword(pdev, IOAT_PCI_CHANERR_INT_OFFSET, &chanerr_int);\n\n\tdev_dbg(to_dev(ioat_chan), \"%s: error = %x:%x\\n\",\n\t\t__func__, chanerr, chanerr_int);\n\n\tdesc = ioat_get_ring_ent(ioat_chan, ioat_chan->tail);\n\thw = desc->hw;\n\tdump_desc_dbg(ioat_chan, desc);\n\n\tswitch (hw->ctl_f.op) {\n\tcase IOAT_OP_XOR_VAL:\n\t\tif (chanerr & IOAT_CHANERR_XOR_P_OR_CRC_ERR) {\n\t\t\t*desc->result |= SUM_CHECK_P_RESULT;\n\t\t\terr_handled |= IOAT_CHANERR_XOR_P_OR_CRC_ERR;\n\t\t}\n\t\tbreak;\n\tcase IOAT_OP_PQ_VAL:\n\tcase IOAT_OP_PQ_VAL_16S:\n\t\tif (chanerr & IOAT_CHANERR_XOR_P_OR_CRC_ERR) {\n\t\t\t*desc->result |= SUM_CHECK_P_RESULT;\n\t\t\terr_handled |= IOAT_CHANERR_XOR_P_OR_CRC_ERR;\n\t\t}\n\t\tif (chanerr & IOAT_CHANERR_XOR_Q_ERR) {\n\t\t\t*desc->result |= SUM_CHECK_Q_RESULT;\n\t\t\terr_handled |= IOAT_CHANERR_XOR_Q_ERR;\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (chanerr & IOAT_CHANERR_RECOVER_MASK) {\n\t\tif (chanerr & IOAT_CHANERR_READ_DATA_ERR) {\n\t\t\tres.result = DMA_TRANS_READ_FAILED;\n\t\t\terr_handled |= IOAT_CHANERR_READ_DATA_ERR;\n\t\t} else if (chanerr & IOAT_CHANERR_WRITE_DATA_ERR) {\n\t\t\tres.result = DMA_TRANS_WRITE_FAILED;\n\t\t\terr_handled |= IOAT_CHANERR_WRITE_DATA_ERR;\n\t\t}\n\n\t\tabort = true;\n\t} else\n\t\tres.result = DMA_TRANS_NOERROR;\n\n\t \n\tif (chanerr ^ err_handled || chanerr == 0) {\n\t\tdev_err(to_dev(ioat_chan), \"%s: fatal error (%x:%x)\\n\",\n\t\t\t__func__, chanerr, err_handled);\n\t\tdev_err(to_dev(ioat_chan), \"Errors handled:\\n\");\n\t\tioat_print_chanerrs(ioat_chan, err_handled);\n\t\tdev_err(to_dev(ioat_chan), \"Errors not handled:\\n\");\n\t\tioat_print_chanerrs(ioat_chan, (chanerr & ~err_handled));\n\n\t\tBUG();\n\t}\n\n\t \n\ttx = &desc->txd;\n\tif (tx->cookie) {\n\t\tdma_cookie_complete(tx);\n\t\tdma_descriptor_unmap(tx);\n\t\tdmaengine_desc_get_callback_invoke(tx, &res);\n\t\ttx->callback = NULL;\n\t\ttx->callback_result = NULL;\n\t}\n\n\t \n\t*ioat_chan->completion = desc->txd.phys;\n\n\tspin_lock_bh(&ioat_chan->prep_lock);\n\t \n\tif (abort) {\n\t\tioat_abort_descs(ioat_chan);\n\t\t \n\t\tioat_reset_hw(ioat_chan);\n\t}\n\n\twritel(chanerr, ioat_chan->reg_base + IOAT_CHANERR_OFFSET);\n\tpci_write_config_dword(pdev, IOAT_PCI_CHANERR_INT_OFFSET, chanerr_int);\n\n\tioat_restart_channel(ioat_chan);\n\tspin_unlock_bh(&ioat_chan->prep_lock);\n}\n\nstatic void check_active(struct ioatdma_chan *ioat_chan)\n{\n\tif (ioat_ring_active(ioat_chan)) {\n\t\tmod_timer(&ioat_chan->timer, jiffies + COMPLETION_TIMEOUT);\n\t\treturn;\n\t}\n\n\tif (test_and_clear_bit(IOAT_CHAN_ACTIVE, &ioat_chan->state))\n\t\tmod_timer_pending(&ioat_chan->timer, jiffies + IDLE_TIMEOUT);\n}\n\nstatic void ioat_reboot_chan(struct ioatdma_chan *ioat_chan)\n{\n\tspin_lock_bh(&ioat_chan->prep_lock);\n\tset_bit(IOAT_CHAN_DOWN, &ioat_chan->state);\n\tspin_unlock_bh(&ioat_chan->prep_lock);\n\n\tioat_abort_descs(ioat_chan);\n\tdev_warn(to_dev(ioat_chan), \"Reset channel...\\n\");\n\tioat_reset_hw(ioat_chan);\n\tdev_warn(to_dev(ioat_chan), \"Restart channel...\\n\");\n\tioat_restart_channel(ioat_chan);\n\n\tspin_lock_bh(&ioat_chan->prep_lock);\n\tclear_bit(IOAT_CHAN_DOWN, &ioat_chan->state);\n\tspin_unlock_bh(&ioat_chan->prep_lock);\n}\n\nvoid ioat_timer_event(struct timer_list *t)\n{\n\tstruct ioatdma_chan *ioat_chan = from_timer(ioat_chan, t, timer);\n\tdma_addr_t phys_complete;\n\tu64 status;\n\n\tstatus = ioat_chansts(ioat_chan);\n\n\t \n\tif (is_ioat_halted(status)) {\n\t\tu32 chanerr;\n\n\t\tchanerr = readl(ioat_chan->reg_base + IOAT_CHANERR_OFFSET);\n\t\tdev_err(to_dev(ioat_chan), \"%s: Channel halted (%x)\\n\",\n\t\t\t__func__, chanerr);\n\t\tdev_err(to_dev(ioat_chan), \"Errors:\\n\");\n\t\tioat_print_chanerrs(ioat_chan, chanerr);\n\n\t\tif (test_bit(IOAT_RUN, &ioat_chan->state)) {\n\t\t\tspin_lock_bh(&ioat_chan->cleanup_lock);\n\t\t\tioat_reboot_chan(ioat_chan);\n\t\t\tspin_unlock_bh(&ioat_chan->cleanup_lock);\n\t\t}\n\n\t\treturn;\n\t}\n\n\tspin_lock_bh(&ioat_chan->cleanup_lock);\n\n\t \n\tif (!ioat_ring_active(ioat_chan)) {\n\t\tspin_lock_bh(&ioat_chan->prep_lock);\n\t\tcheck_active(ioat_chan);\n\t\tspin_unlock_bh(&ioat_chan->prep_lock);\n\t\tgoto unlock_out;\n\t}\n\n\t \n\tif (ioat_cleanup_preamble(ioat_chan, &phys_complete)) {\n\t\t \n\t\t__ioat_cleanup(ioat_chan, phys_complete);\n\t\tgoto unlock_out;\n\t}\n\n\t \n\tif (test_bit(IOAT_COMPLETION_ACK, &ioat_chan->state)) {\n\t\tu32 chanerr;\n\n\t\tchanerr = readl(ioat_chan->reg_base + IOAT_CHANERR_OFFSET);\n\t\tdev_err(to_dev(ioat_chan), \"CHANSTS: %#Lx CHANERR: %#x\\n\",\n\t\t\tstatus, chanerr);\n\t\tdev_err(to_dev(ioat_chan), \"Errors:\\n\");\n\t\tioat_print_chanerrs(ioat_chan, chanerr);\n\n\t\tdev_dbg(to_dev(ioat_chan), \"Active descriptors: %d\\n\",\n\t\t\tioat_ring_active(ioat_chan));\n\n\t\tioat_reboot_chan(ioat_chan);\n\n\t\tgoto unlock_out;\n\t}\n\n\t \n\tif (ioat_ring_pending(ioat_chan)) {\n\t\tdev_warn(to_dev(ioat_chan),\n\t\t\t\"Completion timeout with pending descriptors\\n\");\n\t\tspin_lock_bh(&ioat_chan->prep_lock);\n\t\t__ioat_issue_pending(ioat_chan);\n\t\tspin_unlock_bh(&ioat_chan->prep_lock);\n\t}\n\n\tset_bit(IOAT_COMPLETION_ACK, &ioat_chan->state);\n\tmod_timer(&ioat_chan->timer, jiffies + COMPLETION_TIMEOUT);\nunlock_out:\n\tspin_unlock_bh(&ioat_chan->cleanup_lock);\n}\n\nenum dma_status\nioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,\n\t\tstruct dma_tx_state *txstate)\n{\n\tstruct ioatdma_chan *ioat_chan = to_ioat_chan(c);\n\tenum dma_status ret;\n\n\tret = dma_cookie_status(c, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tioat_cleanup(ioat_chan);\n\n\treturn dma_cookie_status(c, cookie, txstate);\n}\n\nint ioat_reset_hw(struct ioatdma_chan *ioat_chan)\n{\n\t \n\tstruct ioatdma_device *ioat_dma = ioat_chan->ioat_dma;\n\tstruct pci_dev *pdev = ioat_dma->pdev;\n\tu32 chanerr;\n\tu16 dev_id;\n\tint err;\n\n\tioat_quiesce(ioat_chan, msecs_to_jiffies(100));\n\n\tchanerr = readl(ioat_chan->reg_base + IOAT_CHANERR_OFFSET);\n\twritel(chanerr, ioat_chan->reg_base + IOAT_CHANERR_OFFSET);\n\n\tif (ioat_dma->version < IOAT_VER_3_3) {\n\t\t \n\t\terr = pci_read_config_dword(pdev,\n\t\t\t\tIOAT_PCI_CHANERR_INT_OFFSET, &chanerr);\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"channel error register unreachable\\n\");\n\t\t\treturn err;\n\t\t}\n\t\tpci_write_config_dword(pdev,\n\t\t\t\tIOAT_PCI_CHANERR_INT_OFFSET, chanerr);\n\n\t\t \n\t\tpci_read_config_word(pdev, IOAT_PCI_DEVICE_ID_OFFSET, &dev_id);\n\t\tif (dev_id == PCI_DEVICE_ID_INTEL_IOAT_TBG0) {\n\t\t\tpci_write_config_dword(pdev,\n\t\t\t\t\t       IOAT_PCI_DMAUNCERRSTS_OFFSET,\n\t\t\t\t\t       0x10);\n\t\t}\n\t}\n\n\tif (is_bwd_ioat(pdev) && (ioat_dma->irq_mode == IOAT_MSIX)) {\n\t\tioat_dma->msixtba0 = readq(ioat_dma->reg_base + 0x1000);\n\t\tioat_dma->msixdata0 = readq(ioat_dma->reg_base + 0x1008);\n\t\tioat_dma->msixpba = readq(ioat_dma->reg_base + 0x1800);\n\t}\n\n\n\terr = ioat_reset_sync(ioat_chan, msecs_to_jiffies(200));\n\tif (!err) {\n\t\tif (is_bwd_ioat(pdev) && (ioat_dma->irq_mode == IOAT_MSIX)) {\n\t\t\twriteq(ioat_dma->msixtba0, ioat_dma->reg_base + 0x1000);\n\t\t\twriteq(ioat_dma->msixdata0, ioat_dma->reg_base + 0x1008);\n\t\t\twriteq(ioat_dma->msixpba, ioat_dma->reg_base + 0x1800);\n\t\t}\n\t}\n\n\tif (err)\n\t\tdev_err(&pdev->dev, \"Failed to reset: %d\\n\", err);\n\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}