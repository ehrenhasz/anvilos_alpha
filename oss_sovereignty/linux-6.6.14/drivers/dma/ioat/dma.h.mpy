{
  "module_name": "dma.h",
  "hash_id": "80b252ccf2a988682d7576a95fa9988a9ccb3fecc7d2ee6540af4bfffb8475d4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/ioat/dma.h",
  "human_readable_source": " \n \n#ifndef IOATDMA_H\n#define IOATDMA_H\n\n#include <linux/dmaengine.h>\n#include <linux/init.h>\n#include <linux/dmapool.h>\n#include <linux/cache.h>\n#include <linux/pci_ids.h>\n#include <linux/circ_buf.h>\n#include <linux/interrupt.h>\n#include \"registers.h\"\n#include \"hw.h\"\n\n#define IOAT_DMA_VERSION  \"5.00\"\n\n#define IOAT_DMA_DCA_ANY_CPU\t\t~0\n\n#define to_ioatdma_device(dev) container_of(dev, struct ioatdma_device, dma_dev)\n#define to_dev(ioat_chan) (&(ioat_chan)->ioat_dma->pdev->dev)\n#define to_pdev(ioat_chan) ((ioat_chan)->ioat_dma->pdev)\n\n#define chan_num(ch) ((int)((ch)->reg_base - (ch)->ioat_dma->reg_base) / 0x80)\n\n \n#define src_cnt_to_sw(x) ((x) + 2)\n#define src_cnt_to_hw(x) ((x) - 2)\n#define ndest_to_sw(x) ((x) + 1)\n#define ndest_to_hw(x) ((x) - 1)\n#define src16_cnt_to_sw(x) ((x) + 9)\n#define src16_cnt_to_hw(x) ((x) - 9)\n\n \n#define NULL_DESC_BUFFER_SIZE 1\n\nenum ioat_irq_mode {\n\tIOAT_NOIRQ = 0,\n\tIOAT_MSIX,\n\tIOAT_MSI,\n\tIOAT_INTX\n};\n\n \nstruct ioatdma_device {\n\tstruct pci_dev *pdev;\n\tvoid __iomem *reg_base;\n\tstruct dma_pool *completion_pool;\n#define MAX_SED_POOLS\t5\n\tstruct dma_pool *sed_hw_pool[MAX_SED_POOLS];\n\tstruct dma_device dma_dev;\n\tu8 version;\n#define IOAT_MAX_CHANS 4\n\tstruct msix_entry msix_entries[IOAT_MAX_CHANS];\n\tstruct ioatdma_chan *idx[IOAT_MAX_CHANS];\n\tstruct dca_provider *dca;\n\tenum ioat_irq_mode irq_mode;\n\tu32 cap;\n\tint chancnt;\n\n\t \n\tu64 msixtba0;\n\tu64 msixdata0;\n\tu32 msixpba;\n};\n\n#define IOAT_MAX_ORDER 16\n#define IOAT_MAX_DESCS (1 << IOAT_MAX_ORDER)\n#define IOAT_CHUNK_SIZE (SZ_512K)\n#define IOAT_DESCS_PER_CHUNK (IOAT_CHUNK_SIZE / IOAT_DESC_SZ)\n\nstruct ioat_descs {\n\tvoid *virt;\n\tdma_addr_t hw;\n};\n\nstruct ioatdma_chan {\n\tstruct dma_chan dma_chan;\n\tvoid __iomem *reg_base;\n\tdma_addr_t last_completion;\n\tspinlock_t cleanup_lock;\n\tunsigned long state;\n\t#define IOAT_CHAN_DOWN 0\n\t#define IOAT_COMPLETION_ACK 1\n\t#define IOAT_RESET_PENDING 2\n\t#define IOAT_KOBJ_INIT_FAIL 3\n\t#define IOAT_RUN 5\n\t#define IOAT_CHAN_ACTIVE 6\n\tstruct timer_list timer;\n\t#define RESET_DELAY msecs_to_jiffies(100)\n\tstruct ioatdma_device *ioat_dma;\n\tdma_addr_t completion_dma;\n\tu64 *completion;\n\tstruct tasklet_struct cleanup_task;\n\tstruct kobject kobj;\n\n \n\tsize_t xfercap_log;\n\tu16 head;\n\tu16 issued;\n\tu16 tail;\n\tu16 dmacount;\n\tu16 alloc_order;\n\tu16 produce;\n\tstruct ioat_ring_ent **ring;\n\tspinlock_t prep_lock;\n\tstruct ioat_descs descs[IOAT_MAX_DESCS / IOAT_DESCS_PER_CHUNK];\n\tint desc_chunks;\n\tint intr_coalesce;\n\tint prev_intr_coalesce;\n};\n\nstruct ioat_sysfs_entry {\n\tstruct attribute attr;\n\tssize_t (*show)(struct dma_chan *, char *);\n\tssize_t (*store)(struct dma_chan *, const char *, size_t);\n};\n\n \nstruct ioat_sed_ent {\n\tstruct ioat_sed_raw_descriptor *hw;\n\tdma_addr_t dma;\n\tstruct ioat_ring_ent *parent;\n\tunsigned int hw_pool;\n};\n\n \n\nstruct ioat_ring_ent {\n\tunion {\n\t\tstruct ioat_dma_descriptor *hw;\n\t\tstruct ioat_xor_descriptor *xor;\n\t\tstruct ioat_xor_ext_descriptor *xor_ex;\n\t\tstruct ioat_pq_descriptor *pq;\n\t\tstruct ioat_pq_ext_descriptor *pq_ex;\n\t\tstruct ioat_pq_update_descriptor *pqu;\n\t\tstruct ioat_raw_descriptor *raw;\n\t};\n\tsize_t len;\n\tstruct dma_async_tx_descriptor txd;\n\tenum sum_check_flags *result;\n\t#ifdef DEBUG\n\tint id;\n\t#endif\n\tstruct ioat_sed_ent *sed;\n};\n\nextern const struct sysfs_ops ioat_sysfs_ops;\nextern struct ioat_sysfs_entry ioat_version_attr;\nextern struct ioat_sysfs_entry ioat_cap_attr;\nextern int ioat_pending_level;\nextern struct kobj_type ioat_ktype;\nextern struct kmem_cache *ioat_cache;\nextern struct kmem_cache *ioat_sed_cache;\n\nstatic inline struct ioatdma_chan *to_ioat_chan(struct dma_chan *c)\n{\n\treturn container_of(c, struct ioatdma_chan, dma_chan);\n}\n\n \n#ifdef DEBUG\n#define set_desc_id(desc, i) ((desc)->id = (i))\n#define desc_id(desc) ((desc)->id)\n#else\n#define set_desc_id(desc, i)\n#define desc_id(desc) (0)\n#endif\n\nstatic inline void\n__dump_desc_dbg(struct ioatdma_chan *ioat_chan, struct ioat_dma_descriptor *hw,\n\t\tstruct dma_async_tx_descriptor *tx, int id)\n{\n\tstruct device *dev = to_dev(ioat_chan);\n\n\tdev_dbg(dev, \"desc[%d]: (%#llx->%#llx) cookie: %d flags: %#x\"\n\t\t\" ctl: %#10.8x (op: %#x int_en: %d compl: %d)\\n\", id,\n\t\t(unsigned long long) tx->phys,\n\t\t(unsigned long long) hw->next, tx->cookie, tx->flags,\n\t\thw->ctl, hw->ctl_f.op, hw->ctl_f.int_en, hw->ctl_f.compl_write);\n}\n\n#define dump_desc_dbg(c, d) \\\n\t({ if (d) __dump_desc_dbg(c, d->hw, &d->txd, desc_id(d)); 0; })\n\nstatic inline struct ioatdma_chan *\nioat_chan_by_index(struct ioatdma_device *ioat_dma, int index)\n{\n\treturn ioat_dma->idx[index];\n}\n\nstatic inline u64 ioat_chansts(struct ioatdma_chan *ioat_chan)\n{\n\treturn readq(ioat_chan->reg_base + IOAT_CHANSTS_OFFSET);\n}\n\nstatic inline u64 ioat_chansts_to_addr(u64 status)\n{\n\treturn status & IOAT_CHANSTS_COMPLETED_DESCRIPTOR_ADDR;\n}\n\nstatic inline u32 ioat_chanerr(struct ioatdma_chan *ioat_chan)\n{\n\treturn readl(ioat_chan->reg_base + IOAT_CHANERR_OFFSET);\n}\n\nstatic inline void ioat_suspend(struct ioatdma_chan *ioat_chan)\n{\n\tu8 ver = ioat_chan->ioat_dma->version;\n\n\twriteb(IOAT_CHANCMD_SUSPEND,\n\t       ioat_chan->reg_base + IOAT_CHANCMD_OFFSET(ver));\n}\n\nstatic inline void ioat_reset(struct ioatdma_chan *ioat_chan)\n{\n\tu8 ver = ioat_chan->ioat_dma->version;\n\n\twriteb(IOAT_CHANCMD_RESET,\n\t       ioat_chan->reg_base + IOAT_CHANCMD_OFFSET(ver));\n}\n\nstatic inline bool ioat_reset_pending(struct ioatdma_chan *ioat_chan)\n{\n\tu8 ver = ioat_chan->ioat_dma->version;\n\tu8 cmd;\n\n\tcmd = readb(ioat_chan->reg_base + IOAT_CHANCMD_OFFSET(ver));\n\treturn (cmd & IOAT_CHANCMD_RESET) == IOAT_CHANCMD_RESET;\n}\n\nstatic inline bool is_ioat_active(unsigned long status)\n{\n\treturn ((status & IOAT_CHANSTS_STATUS) == IOAT_CHANSTS_ACTIVE);\n}\n\nstatic inline bool is_ioat_idle(unsigned long status)\n{\n\treturn ((status & IOAT_CHANSTS_STATUS) == IOAT_CHANSTS_DONE);\n}\n\nstatic inline bool is_ioat_halted(unsigned long status)\n{\n\treturn ((status & IOAT_CHANSTS_STATUS) == IOAT_CHANSTS_HALTED);\n}\n\nstatic inline bool is_ioat_suspended(unsigned long status)\n{\n\treturn ((status & IOAT_CHANSTS_STATUS) == IOAT_CHANSTS_SUSPENDED);\n}\n\n \nstatic inline bool is_ioat_bug(unsigned long err)\n{\n\treturn !!err;\n}\n\n\nstatic inline u32 ioat_ring_size(struct ioatdma_chan *ioat_chan)\n{\n\treturn 1 << ioat_chan->alloc_order;\n}\n\n \nstatic inline u16 ioat_ring_active(struct ioatdma_chan *ioat_chan)\n{\n\treturn CIRC_CNT(ioat_chan->head, ioat_chan->tail,\n\t\t\tioat_ring_size(ioat_chan));\n}\n\n \nstatic inline u16 ioat_ring_pending(struct ioatdma_chan *ioat_chan)\n{\n\treturn CIRC_CNT(ioat_chan->head, ioat_chan->issued,\n\t\t\tioat_ring_size(ioat_chan));\n}\n\nstatic inline u32 ioat_ring_space(struct ioatdma_chan *ioat_chan)\n{\n\treturn ioat_ring_size(ioat_chan) - ioat_ring_active(ioat_chan);\n}\n\nstatic inline u16\nioat_xferlen_to_descs(struct ioatdma_chan *ioat_chan, size_t len)\n{\n\tu16 num_descs = len >> ioat_chan->xfercap_log;\n\n\tnum_descs += !!(len & ((1 << ioat_chan->xfercap_log) - 1));\n\treturn num_descs;\n}\n\nstatic inline struct ioat_ring_ent *\nioat_get_ring_ent(struct ioatdma_chan *ioat_chan, u16 idx)\n{\n\treturn ioat_chan->ring[idx & (ioat_ring_size(ioat_chan) - 1)];\n}\n\nstatic inline void\nioat_set_chainaddr(struct ioatdma_chan *ioat_chan, u64 addr)\n{\n\twritel(addr & 0x00000000FFFFFFFF,\n\t       ioat_chan->reg_base + IOAT2_CHAINADDR_OFFSET_LOW);\n\twritel(addr >> 32,\n\t       ioat_chan->reg_base + IOAT2_CHAINADDR_OFFSET_HIGH);\n}\n\n \nstruct dma_async_tx_descriptor *\nioat_dma_prep_memcpy_lock(struct dma_chan *c, dma_addr_t dma_dest,\n\t\t\t   dma_addr_t dma_src, size_t len, unsigned long flags);\nstruct dma_async_tx_descriptor *\nioat_prep_interrupt_lock(struct dma_chan *c, unsigned long flags);\nstruct dma_async_tx_descriptor *\nioat_prep_xor(struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,\n\t       unsigned int src_cnt, size_t len, unsigned long flags);\nstruct dma_async_tx_descriptor *\nioat_prep_xor_val(struct dma_chan *chan, dma_addr_t *src,\n\t\t    unsigned int src_cnt, size_t len,\n\t\t    enum sum_check_flags *result, unsigned long flags);\nstruct dma_async_tx_descriptor *\nioat_prep_pq(struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,\n\t      unsigned int src_cnt, const unsigned char *scf, size_t len,\n\t      unsigned long flags);\nstruct dma_async_tx_descriptor *\nioat_prep_pq_val(struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,\n\t\t  unsigned int src_cnt, const unsigned char *scf, size_t len,\n\t\t  enum sum_check_flags *pqres, unsigned long flags);\nstruct dma_async_tx_descriptor *\nioat_prep_pqxor(struct dma_chan *chan, dma_addr_t dst, dma_addr_t *src,\n\t\t unsigned int src_cnt, size_t len, unsigned long flags);\nstruct dma_async_tx_descriptor *\nioat_prep_pqxor_val(struct dma_chan *chan, dma_addr_t *src,\n\t\t     unsigned int src_cnt, size_t len,\n\t\t     enum sum_check_flags *result, unsigned long flags);\n\n \nirqreturn_t ioat_dma_do_interrupt(int irq, void *data);\nirqreturn_t ioat_dma_do_interrupt_msix(int irq, void *data);\nstruct ioat_ring_ent **\nioat_alloc_ring(struct dma_chan *c, int order, gfp_t flags);\nvoid ioat_start_null_desc(struct ioatdma_chan *ioat_chan);\nvoid ioat_free_ring_ent(struct ioat_ring_ent *desc, struct dma_chan *chan);\nint ioat_reset_hw(struct ioatdma_chan *ioat_chan);\nenum dma_status\nioat_tx_status(struct dma_chan *c, dma_cookie_t cookie,\n\t\tstruct dma_tx_state *txstate);\nvoid ioat_cleanup_event(struct tasklet_struct *t);\nvoid ioat_timer_event(struct timer_list *t);\nint ioat_check_space_lock(struct ioatdma_chan *ioat_chan, int num_descs);\nvoid ioat_issue_pending(struct dma_chan *chan);\n\n \nbool is_bwd_ioat(struct pci_dev *pdev);\nstruct dca_provider *ioat_dca_init(struct pci_dev *pdev, void __iomem *iobase);\nvoid ioat_kobject_add(struct ioatdma_device *ioat_dma, struct kobj_type *type);\nvoid ioat_kobject_del(struct ioatdma_device *ioat_dma);\nint ioat_dma_setup_interrupts(struct ioatdma_device *ioat_dma);\nvoid ioat_stop(struct ioatdma_chan *ioat_chan);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}