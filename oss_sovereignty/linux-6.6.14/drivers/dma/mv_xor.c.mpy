{
  "module_name": "mv_xor.c",
  "hash_id": "e0c3bd22d0e80395f6d92d59d5a641a0e291c27519ec1a6447668e8f589ba969",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/mv_xor.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/spinlock.h>\n#include <linux/interrupt.h>\n#include <linux/of_device.h>\n#include <linux/platform_device.h>\n#include <linux/memory.h>\n#include <linux/clk.h>\n#include <linux/of.h>\n#include <linux/of_irq.h>\n#include <linux/irqdomain.h>\n#include <linux/cpumask.h>\n#include <linux/platform_data/dma-mv_xor.h>\n\n#include \"dmaengine.h\"\n#include \"mv_xor.h\"\n\nenum mv_xor_type {\n\tXOR_ORION,\n\tXOR_ARMADA_38X,\n\tXOR_ARMADA_37XX,\n};\n\nenum mv_xor_mode {\n\tXOR_MODE_IN_REG,\n\tXOR_MODE_IN_DESC,\n};\n\nstatic void mv_xor_issue_pending(struct dma_chan *chan);\n\n#define to_mv_xor_chan(chan)\t\t\\\n\tcontainer_of(chan, struct mv_xor_chan, dmachan)\n\n#define to_mv_xor_slot(tx)\t\t\\\n\tcontainer_of(tx, struct mv_xor_desc_slot, async_tx)\n\n#define mv_chan_to_devp(chan)           \\\n\t((chan)->dmadev.dev)\n\nstatic void mv_desc_init(struct mv_xor_desc_slot *desc,\n\t\t\t dma_addr_t addr, u32 byte_count,\n\t\t\t enum dma_ctrl_flags flags)\n{\n\tstruct mv_xor_desc *hw_desc = desc->hw_desc;\n\n\thw_desc->status = XOR_DESC_DMA_OWNED;\n\thw_desc->phy_next_desc = 0;\n\t \n\thw_desc->desc_command = (flags & DMA_PREP_INTERRUPT) ?\n\t\t\t\tXOR_DESC_EOD_INT_EN : 0;\n\thw_desc->phy_dest_addr = addr;\n\thw_desc->byte_count = byte_count;\n}\n\nstatic void mv_desc_set_mode(struct mv_xor_desc_slot *desc)\n{\n\tstruct mv_xor_desc *hw_desc = desc->hw_desc;\n\n\tswitch (desc->type) {\n\tcase DMA_XOR:\n\tcase DMA_INTERRUPT:\n\t\thw_desc->desc_command |= XOR_DESC_OPERATION_XOR;\n\t\tbreak;\n\tcase DMA_MEMCPY:\n\t\thw_desc->desc_command |= XOR_DESC_OPERATION_MEMCPY;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\treturn;\n\t}\n}\n\nstatic void mv_desc_set_next_desc(struct mv_xor_desc_slot *desc,\n\t\t\t\t  u32 next_desc_addr)\n{\n\tstruct mv_xor_desc *hw_desc = desc->hw_desc;\n\tBUG_ON(hw_desc->phy_next_desc);\n\thw_desc->phy_next_desc = next_desc_addr;\n}\n\nstatic void mv_desc_set_src_addr(struct mv_xor_desc_slot *desc,\n\t\t\t\t int index, dma_addr_t addr)\n{\n\tstruct mv_xor_desc *hw_desc = desc->hw_desc;\n\thw_desc->phy_src_addr[mv_phy_src_idx(index)] = addr;\n\tif (desc->type == DMA_XOR)\n\t\thw_desc->desc_command |= (1 << index);\n}\n\nstatic u32 mv_chan_get_current_desc(struct mv_xor_chan *chan)\n{\n\treturn readl_relaxed(XOR_CURR_DESC(chan));\n}\n\nstatic void mv_chan_set_next_descriptor(struct mv_xor_chan *chan,\n\t\t\t\t\tu32 next_desc_addr)\n{\n\twritel_relaxed(next_desc_addr, XOR_NEXT_DESC(chan));\n}\n\nstatic void mv_chan_unmask_interrupts(struct mv_xor_chan *chan)\n{\n\tu32 val = readl_relaxed(XOR_INTR_MASK(chan));\n\tval |= XOR_INTR_MASK_VALUE << (chan->idx * 16);\n\twritel_relaxed(val, XOR_INTR_MASK(chan));\n}\n\nstatic u32 mv_chan_get_intr_cause(struct mv_xor_chan *chan)\n{\n\tu32 intr_cause = readl_relaxed(XOR_INTR_CAUSE(chan));\n\tintr_cause = (intr_cause >> (chan->idx * 16)) & 0xFFFF;\n\treturn intr_cause;\n}\n\nstatic void mv_chan_clear_eoc_cause(struct mv_xor_chan *chan)\n{\n\tu32 val;\n\n\tval = XOR_INT_END_OF_DESC | XOR_INT_END_OF_CHAIN | XOR_INT_STOPPED;\n\tval = ~(val << (chan->idx * 16));\n\tdev_dbg(mv_chan_to_devp(chan), \"%s, val 0x%08x\\n\", __func__, val);\n\twritel_relaxed(val, XOR_INTR_CAUSE(chan));\n}\n\nstatic void mv_chan_clear_err_status(struct mv_xor_chan *chan)\n{\n\tu32 val = 0xFFFF0000 >> (chan->idx * 16);\n\twritel_relaxed(val, XOR_INTR_CAUSE(chan));\n}\n\nstatic void mv_chan_set_mode(struct mv_xor_chan *chan,\n\t\t\t     u32 op_mode)\n{\n\tu32 config = readl_relaxed(XOR_CONFIG(chan));\n\n\tconfig &= ~0x7;\n\tconfig |= op_mode;\n\n#if defined(__BIG_ENDIAN)\n\tconfig |= XOR_DESCRIPTOR_SWAP;\n#else\n\tconfig &= ~XOR_DESCRIPTOR_SWAP;\n#endif\n\n\twritel_relaxed(config, XOR_CONFIG(chan));\n}\n\nstatic void mv_chan_activate(struct mv_xor_chan *chan)\n{\n\tdev_dbg(mv_chan_to_devp(chan), \" activate chan.\\n\");\n\n\t \n\twritel(BIT(0), XOR_ACTIVATION(chan));\n}\n\nstatic char mv_chan_is_busy(struct mv_xor_chan *chan)\n{\n\tu32 state = readl_relaxed(XOR_ACTIVATION(chan));\n\n\tstate = (state >> 4) & 0x3;\n\n\treturn (state == 1) ? 1 : 0;\n}\n\n \nstatic void mv_chan_start_new_chain(struct mv_xor_chan *mv_chan,\n\t\t\t\t    struct mv_xor_desc_slot *sw_desc)\n{\n\tdev_dbg(mv_chan_to_devp(mv_chan), \"%s %d: sw_desc %p\\n\",\n\t\t__func__, __LINE__, sw_desc);\n\n\t \n\tmv_chan_set_next_descriptor(mv_chan, sw_desc->async_tx.phys);\n\n\tmv_chan->pending++;\n\tmv_xor_issue_pending(&mv_chan->dmachan);\n}\n\nstatic dma_cookie_t\nmv_desc_run_tx_complete_actions(struct mv_xor_desc_slot *desc,\n\t\t\t\tstruct mv_xor_chan *mv_chan,\n\t\t\t\tdma_cookie_t cookie)\n{\n\tBUG_ON(desc->async_tx.cookie < 0);\n\n\tif (desc->async_tx.cookie > 0) {\n\t\tcookie = desc->async_tx.cookie;\n\n\t\tdma_descriptor_unmap(&desc->async_tx);\n\t\t \n\t\tdmaengine_desc_get_callback_invoke(&desc->async_tx, NULL);\n\t}\n\n\t \n\tdma_run_dependencies(&desc->async_tx);\n\n\treturn cookie;\n}\n\nstatic int\nmv_chan_clean_completed_slots(struct mv_xor_chan *mv_chan)\n{\n\tstruct mv_xor_desc_slot *iter, *_iter;\n\n\tdev_dbg(mv_chan_to_devp(mv_chan), \"%s %d\\n\", __func__, __LINE__);\n\tlist_for_each_entry_safe(iter, _iter, &mv_chan->completed_slots,\n\t\t\t\t node) {\n\n\t\tif (async_tx_test_ack(&iter->async_tx)) {\n\t\t\tlist_move_tail(&iter->node, &mv_chan->free_slots);\n\t\t\tif (!list_empty(&iter->sg_tx_list)) {\n\t\t\t\tlist_splice_tail_init(&iter->sg_tx_list,\n\t\t\t\t\t\t\t&mv_chan->free_slots);\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int\nmv_desc_clean_slot(struct mv_xor_desc_slot *desc,\n\t\t   struct mv_xor_chan *mv_chan)\n{\n\tdev_dbg(mv_chan_to_devp(mv_chan), \"%s %d: desc %p flags %d\\n\",\n\t\t__func__, __LINE__, desc, desc->async_tx.flags);\n\n\t \n\tif (!async_tx_test_ack(&desc->async_tx)) {\n\t\t \n\t\tlist_move_tail(&desc->node, &mv_chan->completed_slots);\n\t\tif (!list_empty(&desc->sg_tx_list)) {\n\t\t\tlist_splice_tail_init(&desc->sg_tx_list,\n\t\t\t\t\t      &mv_chan->completed_slots);\n\t\t}\n\t} else {\n\t\tlist_move_tail(&desc->node, &mv_chan->free_slots);\n\t\tif (!list_empty(&desc->sg_tx_list)) {\n\t\t\tlist_splice_tail_init(&desc->sg_tx_list,\n\t\t\t\t\t      &mv_chan->free_slots);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void mv_chan_slot_cleanup(struct mv_xor_chan *mv_chan)\n{\n\tstruct mv_xor_desc_slot *iter, *_iter;\n\tdma_cookie_t cookie = 0;\n\tint busy = mv_chan_is_busy(mv_chan);\n\tu32 current_desc = mv_chan_get_current_desc(mv_chan);\n\tint current_cleaned = 0;\n\tstruct mv_xor_desc *hw_desc;\n\n\tdev_dbg(mv_chan_to_devp(mv_chan), \"%s %d\\n\", __func__, __LINE__);\n\tdev_dbg(mv_chan_to_devp(mv_chan), \"current_desc %x\\n\", current_desc);\n\tmv_chan_clean_completed_slots(mv_chan);\n\n\t \n\n\tlist_for_each_entry_safe(iter, _iter, &mv_chan->chain,\n\t\t\t\t node) {\n\n\t\t \n\t\thw_desc = iter->hw_desc;\n\t\tif (hw_desc->status & XOR_DESC_SUCCESS) {\n\t\t\tcookie = mv_desc_run_tx_complete_actions(iter, mv_chan,\n\t\t\t\t\t\t\t\t cookie);\n\n\t\t\t \n\t\t\tmv_desc_clean_slot(iter, mv_chan);\n\n\t\t\t \n\t\t\tif (iter->async_tx.phys == current_desc) {\n\t\t\t\tcurrent_cleaned = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tif (iter->async_tx.phys == current_desc) {\n\t\t\t\tcurrent_cleaned = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif ((busy == 0) && !list_empty(&mv_chan->chain)) {\n\t\tif (current_cleaned) {\n\t\t\t \n\t\t\titer = list_entry(mv_chan->chain.next,\n\t\t\t\t\t  struct mv_xor_desc_slot,\n\t\t\t\t\t  node);\n\t\t\tmv_chan_start_new_chain(mv_chan, iter);\n\t\t} else {\n\t\t\tif (!list_is_last(&iter->node, &mv_chan->chain)) {\n\t\t\t\t \n\t\t\t\titer = list_entry(iter->node.next,\n\t\t\t\t\t\t  struct mv_xor_desc_slot,\n\t\t\t\t\t\t  node);\n\t\t\t\tmv_chan_start_new_chain(mv_chan, iter);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\ttasklet_schedule(&mv_chan->irq_tasklet);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (cookie > 0)\n\t\tmv_chan->dmachan.completed_cookie = cookie;\n}\n\nstatic void mv_xor_tasklet(struct tasklet_struct *t)\n{\n\tstruct mv_xor_chan *chan = from_tasklet(chan, t, irq_tasklet);\n\n\tspin_lock(&chan->lock);\n\tmv_chan_slot_cleanup(chan);\n\tspin_unlock(&chan->lock);\n}\n\nstatic struct mv_xor_desc_slot *\nmv_chan_alloc_slot(struct mv_xor_chan *mv_chan)\n{\n\tstruct mv_xor_desc_slot *iter;\n\n\tspin_lock_bh(&mv_chan->lock);\n\n\tif (!list_empty(&mv_chan->free_slots)) {\n\t\titer = list_first_entry(&mv_chan->free_slots,\n\t\t\t\t\tstruct mv_xor_desc_slot,\n\t\t\t\t\tnode);\n\n\t\tlist_move_tail(&iter->node, &mv_chan->allocated_slots);\n\n\t\tspin_unlock_bh(&mv_chan->lock);\n\n\t\t \n\t\tasync_tx_ack(&iter->async_tx);\n\t\titer->async_tx.cookie = -EBUSY;\n\n\t\treturn iter;\n\n\t}\n\n\tspin_unlock_bh(&mv_chan->lock);\n\n\t \n\ttasklet_schedule(&mv_chan->irq_tasklet);\n\n\treturn NULL;\n}\n\n \nstatic dma_cookie_t\nmv_xor_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct mv_xor_desc_slot *sw_desc = to_mv_xor_slot(tx);\n\tstruct mv_xor_chan *mv_chan = to_mv_xor_chan(tx->chan);\n\tstruct mv_xor_desc_slot *old_chain_tail;\n\tdma_cookie_t cookie;\n\tint new_hw_chain = 1;\n\n\tdev_dbg(mv_chan_to_devp(mv_chan),\n\t\t\"%s sw_desc %p: async_tx %p\\n\",\n\t\t__func__, sw_desc, &sw_desc->async_tx);\n\n\tspin_lock_bh(&mv_chan->lock);\n\tcookie = dma_cookie_assign(tx);\n\n\tif (list_empty(&mv_chan->chain))\n\t\tlist_move_tail(&sw_desc->node, &mv_chan->chain);\n\telse {\n\t\tnew_hw_chain = 0;\n\n\t\told_chain_tail = list_entry(mv_chan->chain.prev,\n\t\t\t\t\t    struct mv_xor_desc_slot,\n\t\t\t\t\t    node);\n\t\tlist_move_tail(&sw_desc->node, &mv_chan->chain);\n\n\t\tdev_dbg(mv_chan_to_devp(mv_chan), \"Append to last desc %pa\\n\",\n\t\t\t&old_chain_tail->async_tx.phys);\n\n\t\t \n\t\tmv_desc_set_next_desc(old_chain_tail, sw_desc->async_tx.phys);\n\n\t\t \n\t\tif (!mv_chan_is_busy(mv_chan)) {\n\t\t\tu32 current_desc = mv_chan_get_current_desc(mv_chan);\n\t\t\t \n\t\t\tif (current_desc == old_chain_tail->async_tx.phys)\n\t\t\t\tnew_hw_chain = 1;\n\t\t}\n\t}\n\n\tif (new_hw_chain)\n\t\tmv_chan_start_new_chain(mv_chan, sw_desc);\n\n\tspin_unlock_bh(&mv_chan->lock);\n\n\treturn cookie;\n}\n\n \nstatic int mv_xor_alloc_chan_resources(struct dma_chan *chan)\n{\n\tvoid *virt_desc;\n\tdma_addr_t dma_desc;\n\tint idx;\n\tstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\n\tstruct mv_xor_desc_slot *slot = NULL;\n\tint num_descs_in_pool = MV_XOR_POOL_SIZE/MV_XOR_SLOT_SIZE;\n\n\t \n\tidx = mv_chan->slots_allocated;\n\twhile (idx < num_descs_in_pool) {\n\t\tslot = kzalloc(sizeof(*slot), GFP_KERNEL);\n\t\tif (!slot) {\n\t\t\tdev_info(mv_chan_to_devp(mv_chan),\n\t\t\t\t \"channel only initialized %d descriptor slots\",\n\t\t\t\t idx);\n\t\t\tbreak;\n\t\t}\n\t\tvirt_desc = mv_chan->dma_desc_pool_virt;\n\t\tslot->hw_desc = virt_desc + idx * MV_XOR_SLOT_SIZE;\n\n\t\tdma_async_tx_descriptor_init(&slot->async_tx, chan);\n\t\tslot->async_tx.tx_submit = mv_xor_tx_submit;\n\t\tINIT_LIST_HEAD(&slot->node);\n\t\tINIT_LIST_HEAD(&slot->sg_tx_list);\n\t\tdma_desc = mv_chan->dma_desc_pool;\n\t\tslot->async_tx.phys = dma_desc + idx * MV_XOR_SLOT_SIZE;\n\t\tslot->idx = idx++;\n\n\t\tspin_lock_bh(&mv_chan->lock);\n\t\tmv_chan->slots_allocated = idx;\n\t\tlist_add_tail(&slot->node, &mv_chan->free_slots);\n\t\tspin_unlock_bh(&mv_chan->lock);\n\t}\n\n\tdev_dbg(mv_chan_to_devp(mv_chan),\n\t\t\"allocated %d descriptor slots\\n\",\n\t\tmv_chan->slots_allocated);\n\n\treturn mv_chan->slots_allocated ? : -ENOMEM;\n}\n\n \nstatic int mv_xor_add_io_win(struct mv_xor_chan *mv_chan, u32 addr)\n{\n\tstruct mv_xor_device *xordev = mv_chan->xordev;\n\tvoid __iomem *base = mv_chan->mmr_high_base;\n\tu32 win_enable;\n\tu32 size;\n\tu8 target, attr;\n\tint ret;\n\tint i;\n\n\t \n\tif (xordev->xor_type == XOR_ARMADA_37XX)\n\t\treturn 0;\n\n\t \n\tfor (i = 0; i < WINDOW_COUNT; i++) {\n\t\tif (addr >= xordev->win_start[i] &&\n\t\t    addr <= xordev->win_end[i]) {\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\n\t \n\tret = mvebu_mbus_get_io_win_info(addr, &size, &target, &attr);\n\tif (ret < 0)\n\t\treturn 0;\n\n\t \n\tsize -= 1;\n\taddr &= ~size;\n\n\t \n\twin_enable = readl(base + WINDOW_BAR_ENABLE(0));\n\n\t \n\ti = ffs(~win_enable) - 1;\n\tif (i >= WINDOW_COUNT)\n\t\treturn -ENOMEM;\n\n\twritel((addr & 0xffff0000) | (attr << 8) | target,\n\t       base + WINDOW_BASE(i));\n\twritel(size & 0xffff0000, base + WINDOW_SIZE(i));\n\n\t \n\txordev->win_start[i] = addr;\n\txordev->win_end[i] = addr + size;\n\n\twin_enable |= (1 << i);\n\twin_enable |= 3 << (16 + (2 * i));\n\twritel(win_enable, base + WINDOW_BAR_ENABLE(0));\n\twritel(win_enable, base + WINDOW_BAR_ENABLE(1));\n\n\treturn 0;\n}\n\nstatic struct dma_async_tx_descriptor *\nmv_xor_prep_dma_xor(struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,\n\t\t    unsigned int src_cnt, size_t len, unsigned long flags)\n{\n\tstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\n\tstruct mv_xor_desc_slot *sw_desc;\n\tint ret;\n\n\tif (unlikely(len < MV_XOR_MIN_BYTE_COUNT))\n\t\treturn NULL;\n\n\tBUG_ON(len > MV_XOR_MAX_BYTE_COUNT);\n\n\tdev_dbg(mv_chan_to_devp(mv_chan),\n\t\t\"%s src_cnt: %d len: %zu dest %pad flags: %ld\\n\",\n\t\t__func__, src_cnt, len, &dest, flags);\n\n\t \n\tret = mv_xor_add_io_win(mv_chan, dest);\n\tif (ret)\n\t\treturn NULL;\n\n\tsw_desc = mv_chan_alloc_slot(mv_chan);\n\tif (sw_desc) {\n\t\tsw_desc->type = DMA_XOR;\n\t\tsw_desc->async_tx.flags = flags;\n\t\tmv_desc_init(sw_desc, dest, len, flags);\n\t\tif (mv_chan->op_in_desc == XOR_MODE_IN_DESC)\n\t\t\tmv_desc_set_mode(sw_desc);\n\t\twhile (src_cnt--) {\n\t\t\t \n\t\t\tret = mv_xor_add_io_win(mv_chan, src[src_cnt]);\n\t\t\tif (ret)\n\t\t\t\treturn NULL;\n\t\t\tmv_desc_set_src_addr(sw_desc, src_cnt, src[src_cnt]);\n\t\t}\n\t}\n\n\tdev_dbg(mv_chan_to_devp(mv_chan),\n\t\t\"%s sw_desc %p async_tx %p \\n\",\n\t\t__func__, sw_desc, &sw_desc->async_tx);\n\treturn sw_desc ? &sw_desc->async_tx : NULL;\n}\n\nstatic struct dma_async_tx_descriptor *\nmv_xor_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\n\t\tsize_t len, unsigned long flags)\n{\n\t \n\treturn mv_xor_prep_dma_xor(chan, dest, &src, 1, len, flags);\n}\n\nstatic struct dma_async_tx_descriptor *\nmv_xor_prep_dma_interrupt(struct dma_chan *chan, unsigned long flags)\n{\n\tstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\n\tdma_addr_t src, dest;\n\tsize_t len;\n\n\tsrc = mv_chan->dummy_src_addr;\n\tdest = mv_chan->dummy_dst_addr;\n\tlen = MV_XOR_MIN_BYTE_COUNT;\n\n\t \n\treturn mv_xor_prep_dma_xor(chan, dest, &src, 1, len, flags);\n}\n\nstatic void mv_xor_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\n\tstruct mv_xor_desc_slot *iter, *_iter;\n\tint in_use_descs = 0;\n\n\tspin_lock_bh(&mv_chan->lock);\n\n\tmv_chan_slot_cleanup(mv_chan);\n\n\tlist_for_each_entry_safe(iter, _iter, &mv_chan->chain,\n\t\t\t\t\tnode) {\n\t\tin_use_descs++;\n\t\tlist_move_tail(&iter->node, &mv_chan->free_slots);\n\t}\n\tlist_for_each_entry_safe(iter, _iter, &mv_chan->completed_slots,\n\t\t\t\t node) {\n\t\tin_use_descs++;\n\t\tlist_move_tail(&iter->node, &mv_chan->free_slots);\n\t}\n\tlist_for_each_entry_safe(iter, _iter, &mv_chan->allocated_slots,\n\t\t\t\t node) {\n\t\tin_use_descs++;\n\t\tlist_move_tail(&iter->node, &mv_chan->free_slots);\n\t}\n\tlist_for_each_entry_safe_reverse(\n\t\titer, _iter, &mv_chan->free_slots, node) {\n\t\tlist_del(&iter->node);\n\t\tkfree(iter);\n\t\tmv_chan->slots_allocated--;\n\t}\n\n\tdev_dbg(mv_chan_to_devp(mv_chan), \"%s slots_allocated %d\\n\",\n\t\t__func__, mv_chan->slots_allocated);\n\tspin_unlock_bh(&mv_chan->lock);\n\n\tif (in_use_descs)\n\t\tdev_err(mv_chan_to_devp(mv_chan),\n\t\t\t\"freeing %d in use descriptors!\\n\", in_use_descs);\n}\n\n \nstatic enum dma_status mv_xor_status(struct dma_chan *chan,\n\t\t\t\t\t  dma_cookie_t cookie,\n\t\t\t\t\t  struct dma_tx_state *txstate)\n{\n\tstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\n\tenum dma_status ret;\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tspin_lock_bh(&mv_chan->lock);\n\tmv_chan_slot_cleanup(mv_chan);\n\tspin_unlock_bh(&mv_chan->lock);\n\n\treturn dma_cookie_status(chan, cookie, txstate);\n}\n\nstatic void mv_chan_dump_regs(struct mv_xor_chan *chan)\n{\n\tu32 val;\n\n\tval = readl_relaxed(XOR_CONFIG(chan));\n\tdev_err(mv_chan_to_devp(chan), \"config       0x%08x\\n\", val);\n\n\tval = readl_relaxed(XOR_ACTIVATION(chan));\n\tdev_err(mv_chan_to_devp(chan), \"activation   0x%08x\\n\", val);\n\n\tval = readl_relaxed(XOR_INTR_CAUSE(chan));\n\tdev_err(mv_chan_to_devp(chan), \"intr cause   0x%08x\\n\", val);\n\n\tval = readl_relaxed(XOR_INTR_MASK(chan));\n\tdev_err(mv_chan_to_devp(chan), \"intr mask    0x%08x\\n\", val);\n\n\tval = readl_relaxed(XOR_ERROR_CAUSE(chan));\n\tdev_err(mv_chan_to_devp(chan), \"error cause  0x%08x\\n\", val);\n\n\tval = readl_relaxed(XOR_ERROR_ADDR(chan));\n\tdev_err(mv_chan_to_devp(chan), \"error addr   0x%08x\\n\", val);\n}\n\nstatic void mv_chan_err_interrupt_handler(struct mv_xor_chan *chan,\n\t\t\t\t\t  u32 intr_cause)\n{\n\tif (intr_cause & XOR_INT_ERR_DECODE) {\n\t\tdev_dbg(mv_chan_to_devp(chan), \"ignoring address decode error\\n\");\n\t\treturn;\n\t}\n\n\tdev_err(mv_chan_to_devp(chan), \"error on chan %d. intr cause 0x%08x\\n\",\n\t\tchan->idx, intr_cause);\n\n\tmv_chan_dump_regs(chan);\n\tWARN_ON(1);\n}\n\nstatic irqreturn_t mv_xor_interrupt_handler(int irq, void *data)\n{\n\tstruct mv_xor_chan *chan = data;\n\tu32 intr_cause = mv_chan_get_intr_cause(chan);\n\n\tdev_dbg(mv_chan_to_devp(chan), \"intr cause %x\\n\", intr_cause);\n\n\tif (intr_cause & XOR_INTR_ERRORS)\n\t\tmv_chan_err_interrupt_handler(chan, intr_cause);\n\n\ttasklet_schedule(&chan->irq_tasklet);\n\n\tmv_chan_clear_eoc_cause(chan);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void mv_xor_issue_pending(struct dma_chan *chan)\n{\n\tstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\n\n\tif (mv_chan->pending >= MV_XOR_THRESHOLD) {\n\t\tmv_chan->pending = 0;\n\t\tmv_chan_activate(mv_chan);\n\t}\n}\n\n \n\nstatic int mv_chan_memcpy_self_test(struct mv_xor_chan *mv_chan)\n{\n\tint i, ret;\n\tvoid *src, *dest;\n\tdma_addr_t src_dma, dest_dma;\n\tstruct dma_chan *dma_chan;\n\tdma_cookie_t cookie;\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct dmaengine_unmap_data *unmap;\n\tint err = 0;\n\n\tsrc = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!src)\n\t\treturn -ENOMEM;\n\n\tdest = kzalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!dest) {\n\t\tkfree(src);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfor (i = 0; i < PAGE_SIZE; i++)\n\t\t((u8 *) src)[i] = (u8)i;\n\n\tdma_chan = &mv_chan->dmachan;\n\tif (mv_xor_alloc_chan_resources(dma_chan) < 1) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tunmap = dmaengine_get_unmap_data(dma_chan->device->dev, 2, GFP_KERNEL);\n\tif (!unmap) {\n\t\terr = -ENOMEM;\n\t\tgoto free_resources;\n\t}\n\n\tsrc_dma = dma_map_page(dma_chan->device->dev, virt_to_page(src),\n\t\t\t       offset_in_page(src), PAGE_SIZE,\n\t\t\t       DMA_TO_DEVICE);\n\tunmap->addr[0] = src_dma;\n\n\tret = dma_mapping_error(dma_chan->device->dev, src_dma);\n\tif (ret) {\n\t\terr = -ENOMEM;\n\t\tgoto free_resources;\n\t}\n\tunmap->to_cnt = 1;\n\n\tdest_dma = dma_map_page(dma_chan->device->dev, virt_to_page(dest),\n\t\t\t\toffset_in_page(dest), PAGE_SIZE,\n\t\t\t\tDMA_FROM_DEVICE);\n\tunmap->addr[1] = dest_dma;\n\n\tret = dma_mapping_error(dma_chan->device->dev, dest_dma);\n\tif (ret) {\n\t\terr = -ENOMEM;\n\t\tgoto free_resources;\n\t}\n\tunmap->from_cnt = 1;\n\tunmap->len = PAGE_SIZE;\n\n\ttx = mv_xor_prep_dma_memcpy(dma_chan, dest_dma, src_dma,\n\t\t\t\t    PAGE_SIZE, 0);\n\tif (!tx) {\n\t\tdev_err(dma_chan->device->dev,\n\t\t\t\"Self-test cannot prepare operation, disabling\\n\");\n\t\terr = -ENODEV;\n\t\tgoto free_resources;\n\t}\n\n\tcookie = mv_xor_tx_submit(tx);\n\tif (dma_submit_error(cookie)) {\n\t\tdev_err(dma_chan->device->dev,\n\t\t\t\"Self-test submit error, disabling\\n\");\n\t\terr = -ENODEV;\n\t\tgoto free_resources;\n\t}\n\n\tmv_xor_issue_pending(dma_chan);\n\tasync_tx_ack(tx);\n\tmsleep(1);\n\n\tif (mv_xor_status(dma_chan, cookie, NULL) !=\n\t    DMA_COMPLETE) {\n\t\tdev_err(dma_chan->device->dev,\n\t\t\t\"Self-test copy timed out, disabling\\n\");\n\t\terr = -ENODEV;\n\t\tgoto free_resources;\n\t}\n\n\tdma_sync_single_for_cpu(dma_chan->device->dev, dest_dma,\n\t\t\t\tPAGE_SIZE, DMA_FROM_DEVICE);\n\tif (memcmp(src, dest, PAGE_SIZE)) {\n\t\tdev_err(dma_chan->device->dev,\n\t\t\t\"Self-test copy failed compare, disabling\\n\");\n\t\terr = -ENODEV;\n\t\tgoto free_resources;\n\t}\n\nfree_resources:\n\tdmaengine_unmap_put(unmap);\n\tmv_xor_free_chan_resources(dma_chan);\nout:\n\tkfree(src);\n\tkfree(dest);\n\treturn err;\n}\n\n#define MV_XOR_NUM_SRC_TEST 4  \nstatic int\nmv_chan_xor_self_test(struct mv_xor_chan *mv_chan)\n{\n\tint i, src_idx, ret;\n\tstruct page *dest;\n\tstruct page *xor_srcs[MV_XOR_NUM_SRC_TEST];\n\tdma_addr_t dma_srcs[MV_XOR_NUM_SRC_TEST];\n\tdma_addr_t dest_dma;\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct dmaengine_unmap_data *unmap;\n\tstruct dma_chan *dma_chan;\n\tdma_cookie_t cookie;\n\tu8 cmp_byte = 0;\n\tu32 cmp_word;\n\tint err = 0;\n\tint src_count = MV_XOR_NUM_SRC_TEST;\n\n\tfor (src_idx = 0; src_idx < src_count; src_idx++) {\n\t\txor_srcs[src_idx] = alloc_page(GFP_KERNEL);\n\t\tif (!xor_srcs[src_idx]) {\n\t\t\twhile (src_idx--)\n\t\t\t\t__free_page(xor_srcs[src_idx]);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tdest = alloc_page(GFP_KERNEL);\n\tif (!dest) {\n\t\twhile (src_idx--)\n\t\t\t__free_page(xor_srcs[src_idx]);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfor (src_idx = 0; src_idx < src_count; src_idx++) {\n\t\tu8 *ptr = page_address(xor_srcs[src_idx]);\n\t\tfor (i = 0; i < PAGE_SIZE; i++)\n\t\t\tptr[i] = (1 << src_idx);\n\t}\n\n\tfor (src_idx = 0; src_idx < src_count; src_idx++)\n\t\tcmp_byte ^= (u8) (1 << src_idx);\n\n\tcmp_word = (cmp_byte << 24) | (cmp_byte << 16) |\n\t\t(cmp_byte << 8) | cmp_byte;\n\n\tmemset(page_address(dest), 0, PAGE_SIZE);\n\n\tdma_chan = &mv_chan->dmachan;\n\tif (mv_xor_alloc_chan_resources(dma_chan) < 1) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tunmap = dmaengine_get_unmap_data(dma_chan->device->dev, src_count + 1,\n\t\t\t\t\t GFP_KERNEL);\n\tif (!unmap) {\n\t\terr = -ENOMEM;\n\t\tgoto free_resources;\n\t}\n\n\t \n\tfor (i = 0; i < src_count; i++) {\n\t\tunmap->addr[i] = dma_map_page(dma_chan->device->dev, xor_srcs[i],\n\t\t\t\t\t      0, PAGE_SIZE, DMA_TO_DEVICE);\n\t\tdma_srcs[i] = unmap->addr[i];\n\t\tret = dma_mapping_error(dma_chan->device->dev, unmap->addr[i]);\n\t\tif (ret) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free_resources;\n\t\t}\n\t\tunmap->to_cnt++;\n\t}\n\n\tunmap->addr[src_count] = dma_map_page(dma_chan->device->dev, dest, 0, PAGE_SIZE,\n\t\t\t\t      DMA_FROM_DEVICE);\n\tdest_dma = unmap->addr[src_count];\n\tret = dma_mapping_error(dma_chan->device->dev, unmap->addr[src_count]);\n\tif (ret) {\n\t\terr = -ENOMEM;\n\t\tgoto free_resources;\n\t}\n\tunmap->from_cnt = 1;\n\tunmap->len = PAGE_SIZE;\n\n\ttx = mv_xor_prep_dma_xor(dma_chan, dest_dma, dma_srcs,\n\t\t\t\t src_count, PAGE_SIZE, 0);\n\tif (!tx) {\n\t\tdev_err(dma_chan->device->dev,\n\t\t\t\"Self-test cannot prepare operation, disabling\\n\");\n\t\terr = -ENODEV;\n\t\tgoto free_resources;\n\t}\n\n\tcookie = mv_xor_tx_submit(tx);\n\tif (dma_submit_error(cookie)) {\n\t\tdev_err(dma_chan->device->dev,\n\t\t\t\"Self-test submit error, disabling\\n\");\n\t\terr = -ENODEV;\n\t\tgoto free_resources;\n\t}\n\n\tmv_xor_issue_pending(dma_chan);\n\tasync_tx_ack(tx);\n\tmsleep(8);\n\n\tif (mv_xor_status(dma_chan, cookie, NULL) !=\n\t    DMA_COMPLETE) {\n\t\tdev_err(dma_chan->device->dev,\n\t\t\t\"Self-test xor timed out, disabling\\n\");\n\t\terr = -ENODEV;\n\t\tgoto free_resources;\n\t}\n\n\tdma_sync_single_for_cpu(dma_chan->device->dev, dest_dma,\n\t\t\t\tPAGE_SIZE, DMA_FROM_DEVICE);\n\tfor (i = 0; i < (PAGE_SIZE / sizeof(u32)); i++) {\n\t\tu32 *ptr = page_address(dest);\n\t\tif (ptr[i] != cmp_word) {\n\t\t\tdev_err(dma_chan->device->dev,\n\t\t\t\t\"Self-test xor failed compare, disabling. index %d, data %x, expected %x\\n\",\n\t\t\t\ti, ptr[i], cmp_word);\n\t\t\terr = -ENODEV;\n\t\t\tgoto free_resources;\n\t\t}\n\t}\n\nfree_resources:\n\tdmaengine_unmap_put(unmap);\n\tmv_xor_free_chan_resources(dma_chan);\nout:\n\tsrc_idx = src_count;\n\twhile (src_idx--)\n\t\t__free_page(xor_srcs[src_idx]);\n\t__free_page(dest);\n\treturn err;\n}\n\nstatic int mv_xor_channel_remove(struct mv_xor_chan *mv_chan)\n{\n\tstruct dma_chan *chan, *_chan;\n\tstruct device *dev = mv_chan->dmadev.dev;\n\n\tdma_async_device_unregister(&mv_chan->dmadev);\n\n\tdma_free_coherent(dev, MV_XOR_POOL_SIZE,\n\t\t\t  mv_chan->dma_desc_pool_virt, mv_chan->dma_desc_pool);\n\tdma_unmap_single(dev, mv_chan->dummy_src_addr,\n\t\t\t MV_XOR_MIN_BYTE_COUNT, DMA_FROM_DEVICE);\n\tdma_unmap_single(dev, mv_chan->dummy_dst_addr,\n\t\t\t MV_XOR_MIN_BYTE_COUNT, DMA_TO_DEVICE);\n\n\tlist_for_each_entry_safe(chan, _chan, &mv_chan->dmadev.channels,\n\t\t\t\t device_node) {\n\t\tlist_del(&chan->device_node);\n\t}\n\n\tfree_irq(mv_chan->irq, mv_chan);\n\n\treturn 0;\n}\n\nstatic struct mv_xor_chan *\nmv_xor_channel_add(struct mv_xor_device *xordev,\n\t\t   struct platform_device *pdev,\n\t\t   int idx, dma_cap_mask_t cap_mask, int irq)\n{\n\tint ret = 0;\n\tstruct mv_xor_chan *mv_chan;\n\tstruct dma_device *dma_dev;\n\n\tmv_chan = devm_kzalloc(&pdev->dev, sizeof(*mv_chan), GFP_KERNEL);\n\tif (!mv_chan)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmv_chan->idx = idx;\n\tmv_chan->irq = irq;\n\tif (xordev->xor_type == XOR_ORION)\n\t\tmv_chan->op_in_desc = XOR_MODE_IN_REG;\n\telse\n\t\tmv_chan->op_in_desc = XOR_MODE_IN_DESC;\n\n\tdma_dev = &mv_chan->dmadev;\n\tdma_dev->dev = &pdev->dev;\n\tmv_chan->xordev = xordev;\n\n\t \n\tmv_chan->dummy_src_addr = dma_map_single(dma_dev->dev,\n\t\tmv_chan->dummy_src, MV_XOR_MIN_BYTE_COUNT, DMA_FROM_DEVICE);\n\tmv_chan->dummy_dst_addr = dma_map_single(dma_dev->dev,\n\t\tmv_chan->dummy_dst, MV_XOR_MIN_BYTE_COUNT, DMA_TO_DEVICE);\n\n\t \n\tmv_chan->dma_desc_pool_virt =\n\t  dma_alloc_wc(&pdev->dev, MV_XOR_POOL_SIZE, &mv_chan->dma_desc_pool,\n\t\t       GFP_KERNEL);\n\tif (!mv_chan->dma_desc_pool_virt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tdma_dev->cap_mask = cap_mask;\n\n\tINIT_LIST_HEAD(&dma_dev->channels);\n\n\t \n\tdma_dev->device_alloc_chan_resources = mv_xor_alloc_chan_resources;\n\tdma_dev->device_free_chan_resources = mv_xor_free_chan_resources;\n\tdma_dev->device_tx_status = mv_xor_status;\n\tdma_dev->device_issue_pending = mv_xor_issue_pending;\n\n\t \n\tif (dma_has_cap(DMA_INTERRUPT, dma_dev->cap_mask))\n\t\tdma_dev->device_prep_dma_interrupt = mv_xor_prep_dma_interrupt;\n\tif (dma_has_cap(DMA_MEMCPY, dma_dev->cap_mask))\n\t\tdma_dev->device_prep_dma_memcpy = mv_xor_prep_dma_memcpy;\n\tif (dma_has_cap(DMA_XOR, dma_dev->cap_mask)) {\n\t\tdma_dev->max_xor = 8;\n\t\tdma_dev->device_prep_dma_xor = mv_xor_prep_dma_xor;\n\t}\n\n\tmv_chan->mmr_base = xordev->xor_base;\n\tmv_chan->mmr_high_base = xordev->xor_high_base;\n\ttasklet_setup(&mv_chan->irq_tasklet, mv_xor_tasklet);\n\n\t \n\tmv_chan_clear_err_status(mv_chan);\n\n\tret = request_irq(mv_chan->irq, mv_xor_interrupt_handler,\n\t\t\t  0, dev_name(&pdev->dev), mv_chan);\n\tif (ret)\n\t\tgoto err_free_dma;\n\n\tmv_chan_unmask_interrupts(mv_chan);\n\n\tif (mv_chan->op_in_desc == XOR_MODE_IN_DESC)\n\t\tmv_chan_set_mode(mv_chan, XOR_OPERATION_MODE_IN_DESC);\n\telse\n\t\tmv_chan_set_mode(mv_chan, XOR_OPERATION_MODE_XOR);\n\n\tspin_lock_init(&mv_chan->lock);\n\tINIT_LIST_HEAD(&mv_chan->chain);\n\tINIT_LIST_HEAD(&mv_chan->completed_slots);\n\tINIT_LIST_HEAD(&mv_chan->free_slots);\n\tINIT_LIST_HEAD(&mv_chan->allocated_slots);\n\tmv_chan->dmachan.device = dma_dev;\n\tdma_cookie_init(&mv_chan->dmachan);\n\n\tlist_add_tail(&mv_chan->dmachan.device_node, &dma_dev->channels);\n\n\tif (dma_has_cap(DMA_MEMCPY, dma_dev->cap_mask)) {\n\t\tret = mv_chan_memcpy_self_test(mv_chan);\n\t\tdev_dbg(&pdev->dev, \"memcpy self test returned %d\\n\", ret);\n\t\tif (ret)\n\t\t\tgoto err_free_irq;\n\t}\n\n\tif (dma_has_cap(DMA_XOR, dma_dev->cap_mask)) {\n\t\tret = mv_chan_xor_self_test(mv_chan);\n\t\tdev_dbg(&pdev->dev, \"xor self test returned %d\\n\", ret);\n\t\tif (ret)\n\t\t\tgoto err_free_irq;\n\t}\n\n\tdev_info(&pdev->dev, \"Marvell XOR (%s): ( %s%s%s)\\n\",\n\t\t mv_chan->op_in_desc ? \"Descriptor Mode\" : \"Registers Mode\",\n\t\t dma_has_cap(DMA_XOR, dma_dev->cap_mask) ? \"xor \" : \"\",\n\t\t dma_has_cap(DMA_MEMCPY, dma_dev->cap_mask) ? \"cpy \" : \"\",\n\t\t dma_has_cap(DMA_INTERRUPT, dma_dev->cap_mask) ? \"intr \" : \"\");\n\n\tret = dma_async_device_register(dma_dev);\n\tif (ret)\n\t\tgoto err_free_irq;\n\n\treturn mv_chan;\n\nerr_free_irq:\n\tfree_irq(mv_chan->irq, mv_chan);\nerr_free_dma:\n\tdma_free_coherent(&pdev->dev, MV_XOR_POOL_SIZE,\n\t\t\t  mv_chan->dma_desc_pool_virt, mv_chan->dma_desc_pool);\n\treturn ERR_PTR(ret);\n}\n\nstatic void\nmv_xor_conf_mbus_windows(struct mv_xor_device *xordev,\n\t\t\t const struct mbus_dram_target_info *dram)\n{\n\tvoid __iomem *base = xordev->xor_high_base;\n\tu32 win_enable = 0;\n\tint i;\n\n\tfor (i = 0; i < 8; i++) {\n\t\twritel(0, base + WINDOW_BASE(i));\n\t\twritel(0, base + WINDOW_SIZE(i));\n\t\tif (i < 4)\n\t\t\twritel(0, base + WINDOW_REMAP_HIGH(i));\n\t}\n\n\tfor (i = 0; i < dram->num_cs; i++) {\n\t\tconst struct mbus_dram_window *cs = dram->cs + i;\n\n\t\twritel((cs->base & 0xffff0000) |\n\t\t       (cs->mbus_attr << 8) |\n\t\t       dram->mbus_dram_target_id, base + WINDOW_BASE(i));\n\t\twritel((cs->size - 1) & 0xffff0000, base + WINDOW_SIZE(i));\n\n\t\t \n\t\txordev->win_start[i] = cs->base;\n\t\txordev->win_end[i] = cs->base + cs->size - 1;\n\n\t\twin_enable |= (1 << i);\n\t\twin_enable |= 3 << (16 + (2 * i));\n\t}\n\n\twritel(win_enable, base + WINDOW_BAR_ENABLE(0));\n\twritel(win_enable, base + WINDOW_BAR_ENABLE(1));\n\twritel(0, base + WINDOW_OVERRIDE_CTRL(0));\n\twritel(0, base + WINDOW_OVERRIDE_CTRL(1));\n}\n\nstatic void\nmv_xor_conf_mbus_windows_a3700(struct mv_xor_device *xordev)\n{\n\tvoid __iomem *base = xordev->xor_high_base;\n\tu32 win_enable = 0;\n\tint i;\n\n\tfor (i = 0; i < 8; i++) {\n\t\twritel(0, base + WINDOW_BASE(i));\n\t\twritel(0, base + WINDOW_SIZE(i));\n\t\tif (i < 4)\n\t\t\twritel(0, base + WINDOW_REMAP_HIGH(i));\n\t}\n\t \n\twritel(0xffff0000, base + WINDOW_SIZE(0));\n\twin_enable |= 1;\n\twin_enable |= 3 << 16;\n\n\twritel(win_enable, base + WINDOW_BAR_ENABLE(0));\n\twritel(win_enable, base + WINDOW_BAR_ENABLE(1));\n\twritel(0, base + WINDOW_OVERRIDE_CTRL(0));\n\twritel(0, base + WINDOW_OVERRIDE_CTRL(1));\n}\n\n \nstatic int mv_xor_suspend(struct platform_device *pdev, pm_message_t state)\n{\n\tstruct mv_xor_device *xordev = platform_get_drvdata(pdev);\n\tint i;\n\n\tfor (i = 0; i < MV_XOR_MAX_CHANNELS; i++) {\n\t\tstruct mv_xor_chan *mv_chan = xordev->channels[i];\n\n\t\tif (!mv_chan)\n\t\t\tcontinue;\n\n\t\tmv_chan->saved_config_reg =\n\t\t\treadl_relaxed(XOR_CONFIG(mv_chan));\n\t\tmv_chan->saved_int_mask_reg =\n\t\t\treadl_relaxed(XOR_INTR_MASK(mv_chan));\n\t}\n\n\treturn 0;\n}\n\nstatic int mv_xor_resume(struct platform_device *dev)\n{\n\tstruct mv_xor_device *xordev = platform_get_drvdata(dev);\n\tconst struct mbus_dram_target_info *dram;\n\tint i;\n\n\tfor (i = 0; i < MV_XOR_MAX_CHANNELS; i++) {\n\t\tstruct mv_xor_chan *mv_chan = xordev->channels[i];\n\n\t\tif (!mv_chan)\n\t\t\tcontinue;\n\n\t\twritel_relaxed(mv_chan->saved_config_reg,\n\t\t\t       XOR_CONFIG(mv_chan));\n\t\twritel_relaxed(mv_chan->saved_int_mask_reg,\n\t\t\t       XOR_INTR_MASK(mv_chan));\n\t}\n\n\tif (xordev->xor_type == XOR_ARMADA_37XX) {\n\t\tmv_xor_conf_mbus_windows_a3700(xordev);\n\t\treturn 0;\n\t}\n\n\tdram = mv_mbus_dram_info();\n\tif (dram)\n\t\tmv_xor_conf_mbus_windows(xordev, dram);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id mv_xor_dt_ids[] = {\n\t{ .compatible = \"marvell,orion-xor\", .data = (void *)XOR_ORION },\n\t{ .compatible = \"marvell,armada-380-xor\", .data = (void *)XOR_ARMADA_38X },\n\t{ .compatible = \"marvell,armada-3700-xor\", .data = (void *)XOR_ARMADA_37XX },\n\t{},\n};\n\nstatic unsigned int mv_xor_engine_count;\n\nstatic int mv_xor_probe(struct platform_device *pdev)\n{\n\tconst struct mbus_dram_target_info *dram;\n\tstruct mv_xor_device *xordev;\n\tstruct mv_xor_platform_data *pdata = dev_get_platdata(&pdev->dev);\n\tstruct resource *res;\n\tunsigned int max_engines, max_channels;\n\tint i, ret;\n\n\tdev_notice(&pdev->dev, \"Marvell shared XOR driver\\n\");\n\n\txordev = devm_kzalloc(&pdev->dev, sizeof(*xordev), GFP_KERNEL);\n\tif (!xordev)\n\t\treturn -ENOMEM;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!res)\n\t\treturn -ENODEV;\n\n\txordev->xor_base = devm_ioremap(&pdev->dev, res->start,\n\t\t\t\t\tresource_size(res));\n\tif (!xordev->xor_base)\n\t\treturn -EBUSY;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\n\tif (!res)\n\t\treturn -ENODEV;\n\n\txordev->xor_high_base = devm_ioremap(&pdev->dev, res->start,\n\t\t\t\t\t     resource_size(res));\n\tif (!xordev->xor_high_base)\n\t\treturn -EBUSY;\n\n\tplatform_set_drvdata(pdev, xordev);\n\n\n\t \n\txordev->xor_type = XOR_ORION;\n\tif (pdev->dev.of_node) {\n\t\tconst struct of_device_id *of_id =\n\t\t\tof_match_device(mv_xor_dt_ids,\n\t\t\t\t\t&pdev->dev);\n\n\t\txordev->xor_type = (uintptr_t)of_id->data;\n\t}\n\n\t \n\tif (xordev->xor_type == XOR_ARMADA_37XX) {\n\t\tmv_xor_conf_mbus_windows_a3700(xordev);\n\t} else {\n\t\tdram = mv_mbus_dram_info();\n\t\tif (dram)\n\t\t\tmv_xor_conf_mbus_windows(xordev, dram);\n\t}\n\n\t \n\txordev->clk = clk_get(&pdev->dev, NULL);\n\tif (!IS_ERR(xordev->clk))\n\t\tclk_prepare_enable(xordev->clk);\n\n\t \n\tmax_engines = num_present_cpus();\n\tif (xordev->xor_type == XOR_ARMADA_37XX)\n\t\tmax_channels =\tnum_present_cpus();\n\telse\n\t\tmax_channels = min_t(unsigned int,\n\t\t\t\t     MV_XOR_MAX_CHANNELS,\n\t\t\t\t     DIV_ROUND_UP(num_present_cpus(), 2));\n\n\tif (mv_xor_engine_count >= max_engines)\n\t\treturn 0;\n\n\tif (pdev->dev.of_node) {\n\t\tstruct device_node *np;\n\t\tint i = 0;\n\n\t\tfor_each_child_of_node(pdev->dev.of_node, np) {\n\t\t\tstruct mv_xor_chan *chan;\n\t\t\tdma_cap_mask_t cap_mask;\n\t\t\tint irq;\n\n\t\t\tif (i >= max_channels)\n\t\t\t\tcontinue;\n\n\t\t\tdma_cap_zero(cap_mask);\n\t\t\tdma_cap_set(DMA_MEMCPY, cap_mask);\n\t\t\tdma_cap_set(DMA_XOR, cap_mask);\n\t\t\tdma_cap_set(DMA_INTERRUPT, cap_mask);\n\n\t\t\tirq = irq_of_parse_and_map(np, 0);\n\t\t\tif (!irq) {\n\t\t\t\tret = -ENODEV;\n\t\t\t\tgoto err_channel_add;\n\t\t\t}\n\n\t\t\tchan = mv_xor_channel_add(xordev, pdev, i,\n\t\t\t\t\t\t  cap_mask, irq);\n\t\t\tif (IS_ERR(chan)) {\n\t\t\t\tret = PTR_ERR(chan);\n\t\t\t\tirq_dispose_mapping(irq);\n\t\t\t\tgoto err_channel_add;\n\t\t\t}\n\n\t\t\txordev->channels[i] = chan;\n\t\t\ti++;\n\t\t}\n\t} else if (pdata && pdata->channels) {\n\t\tfor (i = 0; i < max_channels; i++) {\n\t\t\tstruct mv_xor_channel_data *cd;\n\t\t\tstruct mv_xor_chan *chan;\n\t\t\tint irq;\n\n\t\t\tcd = &pdata->channels[i];\n\t\t\tirq = platform_get_irq(pdev, i);\n\t\t\tif (irq < 0) {\n\t\t\t\tret = irq;\n\t\t\t\tgoto err_channel_add;\n\t\t\t}\n\n\t\t\tchan = mv_xor_channel_add(xordev, pdev, i,\n\t\t\t\t\t\t  cd->cap_mask, irq);\n\t\t\tif (IS_ERR(chan)) {\n\t\t\t\tret = PTR_ERR(chan);\n\t\t\t\tgoto err_channel_add;\n\t\t\t}\n\n\t\t\txordev->channels[i] = chan;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_channel_add:\n\tfor (i = 0; i < MV_XOR_MAX_CHANNELS; i++)\n\t\tif (xordev->channels[i]) {\n\t\t\tmv_xor_channel_remove(xordev->channels[i]);\n\t\t\tif (pdev->dev.of_node)\n\t\t\t\tirq_dispose_mapping(xordev->channels[i]->irq);\n\t\t}\n\n\tif (!IS_ERR(xordev->clk)) {\n\t\tclk_disable_unprepare(xordev->clk);\n\t\tclk_put(xordev->clk);\n\t}\n\n\treturn ret;\n}\n\nstatic struct platform_driver mv_xor_driver = {\n\t.probe\t\t= mv_xor_probe,\n\t.suspend        = mv_xor_suspend,\n\t.resume         = mv_xor_resume,\n\t.driver\t\t= {\n\t\t.name\t        = MV_XOR_NAME,\n\t\t.of_match_table = mv_xor_dt_ids,\n\t},\n};\n\nbuiltin_platform_driver(mv_xor_driver);\n\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}