{
  "module_name": "hisi_dma.c",
  "hash_id": "769e6887f26c4e83cd80da5ab4149ab90eb5d4cc3448f0b24afcf2ec389856a9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/hisi_dma.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/dmaengine.h>\n#include <linux/init.h>\n#include <linux/iopoll.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/spinlock.h>\n#include \"virt-dma.h\"\n\n \n#define HISI_DMA_Q_SQ_BASE_L\t\t\t0x0\n#define HISI_DMA_Q_SQ_BASE_H\t\t\t0x4\n#define HISI_DMA_Q_SQ_DEPTH\t\t\t0x8\n#define HISI_DMA_Q_SQ_TAIL_PTR\t\t\t0xc\n#define HISI_DMA_Q_CQ_BASE_L\t\t\t0x10\n#define HISI_DMA_Q_CQ_BASE_H\t\t\t0x14\n#define HISI_DMA_Q_CQ_DEPTH\t\t\t0x18\n#define HISI_DMA_Q_CQ_HEAD_PTR\t\t\t0x1c\n#define HISI_DMA_Q_CTRL0\t\t\t0x20\n#define HISI_DMA_Q_CTRL0_QUEUE_EN\t\tBIT(0)\n#define HISI_DMA_Q_CTRL0_QUEUE_PAUSE\t\tBIT(4)\n#define HISI_DMA_Q_CTRL1\t\t\t0x24\n#define HISI_DMA_Q_CTRL1_QUEUE_RESET\t\tBIT(0)\n#define HISI_DMA_Q_FSM_STS\t\t\t0x30\n#define HISI_DMA_Q_FSM_STS_MASK\t\t\tGENMASK(3, 0)\n#define HISI_DMA_Q_ERR_INT_NUM0\t\t\t0x84\n#define HISI_DMA_Q_ERR_INT_NUM1\t\t\t0x88\n#define HISI_DMA_Q_ERR_INT_NUM2\t\t\t0x8c\n\n \n#define HISI_DMA_HIP08_MODE\t\t\t0x217C\n#define HISI_DMA_HIP08_Q_BASE\t\t\t0x0\n#define HISI_DMA_HIP08_Q_CTRL0_ERR_ABORT_EN\tBIT(2)\n#define HISI_DMA_HIP08_Q_INT_STS\t\t0x40\n#define HISI_DMA_HIP08_Q_INT_MSK\t\t0x44\n#define HISI_DMA_HIP08_Q_INT_STS_MASK\t\tGENMASK(14, 0)\n#define HISI_DMA_HIP08_Q_ERR_INT_NUM3\t\t0x90\n#define HISI_DMA_HIP08_Q_ERR_INT_NUM4\t\t0x94\n#define HISI_DMA_HIP08_Q_ERR_INT_NUM5\t\t0x98\n#define HISI_DMA_HIP08_Q_ERR_INT_NUM6\t\t0x48\n#define HISI_DMA_HIP08_Q_CTRL0_SQCQ_DRCT\tBIT(24)\n\n \n#define HISI_DMA_HIP09_DMA_FLR_DISABLE\t\t0xA00\n#define HISI_DMA_HIP09_DMA_FLR_DISABLE_B\tBIT(0)\n#define HISI_DMA_HIP09_Q_BASE\t\t\t0x2000\n#define HISI_DMA_HIP09_Q_CTRL0_ERR_ABORT_EN\tGENMASK(31, 28)\n#define HISI_DMA_HIP09_Q_CTRL0_SQ_DRCT\t\tBIT(26)\n#define HISI_DMA_HIP09_Q_CTRL0_CQ_DRCT\t\tBIT(27)\n#define HISI_DMA_HIP09_Q_CTRL1_VA_ENABLE\tBIT(2)\n#define HISI_DMA_HIP09_Q_INT_STS\t\t0x40\n#define HISI_DMA_HIP09_Q_INT_MSK\t\t0x44\n#define HISI_DMA_HIP09_Q_INT_STS_MASK\t\t0x1\n#define HISI_DMA_HIP09_Q_ERR_INT_STS\t\t0x48\n#define HISI_DMA_HIP09_Q_ERR_INT_MSK\t\t0x4C\n#define HISI_DMA_HIP09_Q_ERR_INT_STS_MASK\tGENMASK(18, 1)\n#define HISI_DMA_HIP09_PORT_CFG_REG(port_id)\t(0x800 + \\\n\t\t\t\t\t\t(port_id) * 0x20)\n#define HISI_DMA_HIP09_PORT_CFG_LINK_DOWN_MASK_B\tBIT(16)\n\n#define HISI_DMA_HIP09_MAX_PORT_NUM\t\t16\n\n#define HISI_DMA_HIP08_MSI_NUM\t\t\t32\n#define HISI_DMA_HIP08_CHAN_NUM\t\t\t30\n#define HISI_DMA_HIP09_MSI_NUM\t\t\t4\n#define HISI_DMA_HIP09_CHAN_NUM\t\t\t4\n#define HISI_DMA_REVISION_HIP08B\t\t0x21\n#define HISI_DMA_REVISION_HIP09A\t\t0x30\n\n#define HISI_DMA_Q_OFFSET\t\t\t0x100\n#define HISI_DMA_Q_DEPTH_VAL\t\t\t1024\n\n#define PCI_BAR_2\t\t\t\t2\n\n#define HISI_DMA_POLL_Q_STS_DELAY_US\t\t10\n#define HISI_DMA_POLL_Q_STS_TIME_OUT_US\t\t1000\n\n#define HISI_DMA_MAX_DIR_NAME_LEN\t\t128\n\n \nenum hisi_dma_reg_layout {\n\tHISI_DMA_REG_LAYOUT_INVALID = 0,\n\tHISI_DMA_REG_LAYOUT_HIP08,\n\tHISI_DMA_REG_LAYOUT_HIP09\n};\n\nenum hisi_dma_mode {\n\tEP = 0,\n\tRC,\n};\n\nenum hisi_dma_chan_status {\n\tDISABLE = -1,\n\tIDLE = 0,\n\tRUN,\n\tCPL,\n\tPAUSE,\n\tHALT,\n\tABORT,\n\tWAIT,\n\tBUFFCLR,\n};\n\nstruct hisi_dma_sqe {\n\t__le32 dw0;\n#define OPCODE_MASK\t\t\tGENMASK(3, 0)\n#define OPCODE_SMALL_PACKAGE\t\t0x1\n#define OPCODE_M2M\t\t\t0x4\n#define LOCAL_IRQ_EN\t\t\tBIT(8)\n#define ATTR_SRC_MASK\t\t\tGENMASK(14, 12)\n\t__le32 dw1;\n\t__le32 dw2;\n#define ATTR_DST_MASK\t\t\tGENMASK(26, 24)\n\t__le32 length;\n\t__le64 src_addr;\n\t__le64 dst_addr;\n};\n\nstruct hisi_dma_cqe {\n\t__le32 rsv0;\n\t__le32 rsv1;\n\t__le16 sq_head;\n\t__le16 rsv2;\n\t__le16 rsv3;\n\t__le16 w0;\n#define STATUS_MASK\t\t\tGENMASK(15, 1)\n#define STATUS_SUCC\t\t\t0x0\n#define VALID_BIT\t\t\tBIT(0)\n};\n\nstruct hisi_dma_desc {\n\tstruct virt_dma_desc vd;\n\tstruct hisi_dma_sqe sqe;\n};\n\nstruct hisi_dma_chan {\n\tstruct virt_dma_chan vc;\n\tstruct hisi_dma_dev *hdma_dev;\n\tstruct hisi_dma_sqe *sq;\n\tstruct hisi_dma_cqe *cq;\n\tdma_addr_t sq_dma;\n\tdma_addr_t cq_dma;\n\tu32 sq_tail;\n\tu32 cq_head;\n\tu32 qp_num;\n\tenum hisi_dma_chan_status status;\n\tstruct hisi_dma_desc *desc;\n};\n\nstruct hisi_dma_dev {\n\tstruct pci_dev *pdev;\n\tvoid __iomem *base;\n\tstruct dma_device dma_dev;\n\tu32 chan_num;\n\tu32 chan_depth;\n\tenum hisi_dma_reg_layout reg_layout;\n\tvoid __iomem *queue_base;  \n\tstruct hisi_dma_chan chan[];\n};\n\n#ifdef CONFIG_DEBUG_FS\n\nstatic const struct debugfs_reg32 hisi_dma_comm_chan_regs[] = {\n\t{\"DMA_QUEUE_SQ_DEPTH                \", 0x0008ull},\n\t{\"DMA_QUEUE_SQ_TAIL_PTR             \", 0x000Cull},\n\t{\"DMA_QUEUE_CQ_DEPTH                \", 0x0018ull},\n\t{\"DMA_QUEUE_CQ_HEAD_PTR             \", 0x001Cull},\n\t{\"DMA_QUEUE_CTRL0                   \", 0x0020ull},\n\t{\"DMA_QUEUE_CTRL1                   \", 0x0024ull},\n\t{\"DMA_QUEUE_FSM_STS                 \", 0x0030ull},\n\t{\"DMA_QUEUE_SQ_STS                  \", 0x0034ull},\n\t{\"DMA_QUEUE_CQ_TAIL_PTR             \", 0x003Cull},\n\t{\"DMA_QUEUE_INT_STS                 \", 0x0040ull},\n\t{\"DMA_QUEUE_INT_MSK                 \", 0x0044ull},\n\t{\"DMA_QUEUE_INT_RO                  \", 0x006Cull},\n};\n\nstatic const struct debugfs_reg32 hisi_dma_hip08_chan_regs[] = {\n\t{\"DMA_QUEUE_BYTE_CNT                \", 0x0038ull},\n\t{\"DMA_ERR_INT_NUM6                  \", 0x0048ull},\n\t{\"DMA_QUEUE_DESP0                   \", 0x0050ull},\n\t{\"DMA_QUEUE_DESP1                   \", 0x0054ull},\n\t{\"DMA_QUEUE_DESP2                   \", 0x0058ull},\n\t{\"DMA_QUEUE_DESP3                   \", 0x005Cull},\n\t{\"DMA_QUEUE_DESP4                   \", 0x0074ull},\n\t{\"DMA_QUEUE_DESP5                   \", 0x0078ull},\n\t{\"DMA_QUEUE_DESP6                   \", 0x007Cull},\n\t{\"DMA_QUEUE_DESP7                   \", 0x0080ull},\n\t{\"DMA_ERR_INT_NUM0                  \", 0x0084ull},\n\t{\"DMA_ERR_INT_NUM1                  \", 0x0088ull},\n\t{\"DMA_ERR_INT_NUM2                  \", 0x008Cull},\n\t{\"DMA_ERR_INT_NUM3                  \", 0x0090ull},\n\t{\"DMA_ERR_INT_NUM4                  \", 0x0094ull},\n\t{\"DMA_ERR_INT_NUM5                  \", 0x0098ull},\n\t{\"DMA_QUEUE_SQ_STS2                 \", 0x00A4ull},\n};\n\nstatic const struct debugfs_reg32 hisi_dma_hip09_chan_regs[] = {\n\t{\"DMA_QUEUE_ERR_INT_STS             \", 0x0048ull},\n\t{\"DMA_QUEUE_ERR_INT_MSK             \", 0x004Cull},\n\t{\"DFX_SQ_READ_ERR_PTR               \", 0x0068ull},\n\t{\"DFX_DMA_ERR_INT_NUM0              \", 0x0084ull},\n\t{\"DFX_DMA_ERR_INT_NUM1              \", 0x0088ull},\n\t{\"DFX_DMA_ERR_INT_NUM2              \", 0x008Cull},\n\t{\"DFX_DMA_QUEUE_SQ_STS2             \", 0x00A4ull},\n};\n\nstatic const struct debugfs_reg32 hisi_dma_hip08_comm_regs[] = {\n\t{\"DMA_ECC_ERR_ADDR                  \", 0x2004ull},\n\t{\"DMA_ECC_ECC_CNT                   \", 0x2014ull},\n\t{\"COMMON_AND_CH_ERR_STS             \", 0x2030ull},\n\t{\"LOCAL_CPL_ID_STS_0                \", 0x20E0ull},\n\t{\"LOCAL_CPL_ID_STS_1                \", 0x20E4ull},\n\t{\"LOCAL_CPL_ID_STS_2                \", 0x20E8ull},\n\t{\"LOCAL_CPL_ID_STS_3                \", 0x20ECull},\n\t{\"LOCAL_TLP_NUM                     \", 0x2158ull},\n\t{\"SQCQ_TLP_NUM                      \", 0x2164ull},\n\t{\"CPL_NUM                           \", 0x2168ull},\n\t{\"INF_BACK_PRESS_STS                \", 0x2170ull},\n\t{\"DMA_CH_RAS_LEVEL                  \", 0x2184ull},\n\t{\"DMA_CM_RAS_LEVEL                  \", 0x2188ull},\n\t{\"DMA_CH_ERR_STS                    \", 0x2190ull},\n\t{\"DMA_CH_DONE_STS                   \", 0x2194ull},\n\t{\"DMA_SQ_TAG_STS_0                  \", 0x21A0ull},\n\t{\"DMA_SQ_TAG_STS_1                  \", 0x21A4ull},\n\t{\"DMA_SQ_TAG_STS_2                  \", 0x21A8ull},\n\t{\"DMA_SQ_TAG_STS_3                  \", 0x21ACull},\n\t{\"LOCAL_P_ID_STS_0                  \", 0x21B0ull},\n\t{\"LOCAL_P_ID_STS_1                  \", 0x21B4ull},\n\t{\"LOCAL_P_ID_STS_2                  \", 0x21B8ull},\n\t{\"LOCAL_P_ID_STS_3                  \", 0x21BCull},\n\t{\"DMA_PREBUFF_INFO_0                \", 0x2200ull},\n\t{\"DMA_CM_TABLE_INFO_0               \", 0x2220ull},\n\t{\"DMA_CM_CE_RO                      \", 0x2244ull},\n\t{\"DMA_CM_NFE_RO                     \", 0x2248ull},\n\t{\"DMA_CM_FE_RO                      \", 0x224Cull},\n};\n\nstatic const struct debugfs_reg32 hisi_dma_hip09_comm_regs[] = {\n\t{\"COMMON_AND_CH_ERR_STS             \", 0x0030ull},\n\t{\"DMA_PORT_IDLE_STS                 \", 0x0150ull},\n\t{\"DMA_CH_RAS_LEVEL                  \", 0x0184ull},\n\t{\"DMA_CM_RAS_LEVEL                  \", 0x0188ull},\n\t{\"DMA_CM_CE_RO                      \", 0x0244ull},\n\t{\"DMA_CM_NFE_RO                     \", 0x0248ull},\n\t{\"DMA_CM_FE_RO                      \", 0x024Cull},\n\t{\"DFX_INF_BACK_PRESS_STS0           \", 0x1A40ull},\n\t{\"DFX_INF_BACK_PRESS_STS1           \", 0x1A44ull},\n\t{\"DFX_INF_BACK_PRESS_STS2           \", 0x1A48ull},\n\t{\"DFX_DMA_WRR_DISABLE               \", 0x1A4Cull},\n\t{\"DFX_PA_REQ_TLP_NUM                \", 0x1C00ull},\n\t{\"DFX_PA_BACK_TLP_NUM               \", 0x1C04ull},\n\t{\"DFX_PA_RETRY_TLP_NUM              \", 0x1C08ull},\n\t{\"DFX_LOCAL_NP_TLP_NUM              \", 0x1C0Cull},\n\t{\"DFX_LOCAL_CPL_HEAD_TLP_NUM        \", 0x1C10ull},\n\t{\"DFX_LOCAL_CPL_DATA_TLP_NUM        \", 0x1C14ull},\n\t{\"DFX_LOCAL_CPL_EXT_DATA_TLP_NUM    \", 0x1C18ull},\n\t{\"DFX_LOCAL_P_HEAD_TLP_NUM          \", 0x1C1Cull},\n\t{\"DFX_LOCAL_P_ACK_TLP_NUM           \", 0x1C20ull},\n\t{\"DFX_BUF_ALOC_PORT_REQ_NUM         \", 0x1C24ull},\n\t{\"DFX_BUF_ALOC_PORT_RESULT_NUM      \", 0x1C28ull},\n\t{\"DFX_BUF_FAIL_SIZE_NUM             \", 0x1C2Cull},\n\t{\"DFX_BUF_ALOC_SIZE_NUM             \", 0x1C30ull},\n\t{\"DFX_BUF_NP_RELEASE_SIZE_NUM       \", 0x1C34ull},\n\t{\"DFX_BUF_P_RELEASE_SIZE_NUM        \", 0x1C38ull},\n\t{\"DFX_BUF_PORT_RELEASE_SIZE_NUM     \", 0x1C3Cull},\n\t{\"DFX_DMA_PREBUF_MEM0_ECC_ERR_ADDR  \", 0x1CA8ull},\n\t{\"DFX_DMA_PREBUF_MEM0_ECC_CNT       \", 0x1CACull},\n\t{\"DFX_DMA_LOC_NP_OSTB_ECC_ERR_ADDR  \", 0x1CB0ull},\n\t{\"DFX_DMA_LOC_NP_OSTB_ECC_CNT       \", 0x1CB4ull},\n\t{\"DFX_DMA_PREBUF_MEM1_ECC_ERR_ADDR  \", 0x1CC0ull},\n\t{\"DFX_DMA_PREBUF_MEM1_ECC_CNT       \", 0x1CC4ull},\n\t{\"DMA_CH_DONE_STS                   \", 0x02E0ull},\n\t{\"DMA_CH_ERR_STS                    \", 0x0320ull},\n};\n#endif  \n\nstatic enum hisi_dma_reg_layout hisi_dma_get_reg_layout(struct pci_dev *pdev)\n{\n\tif (pdev->revision == HISI_DMA_REVISION_HIP08B)\n\t\treturn HISI_DMA_REG_LAYOUT_HIP08;\n\telse if (pdev->revision >= HISI_DMA_REVISION_HIP09A)\n\t\treturn HISI_DMA_REG_LAYOUT_HIP09;\n\n\treturn HISI_DMA_REG_LAYOUT_INVALID;\n}\n\nstatic u32 hisi_dma_get_chan_num(struct pci_dev *pdev)\n{\n\tif (pdev->revision == HISI_DMA_REVISION_HIP08B)\n\t\treturn HISI_DMA_HIP08_CHAN_NUM;\n\n\treturn HISI_DMA_HIP09_CHAN_NUM;\n}\n\nstatic u32 hisi_dma_get_msi_num(struct pci_dev *pdev)\n{\n\tif (pdev->revision == HISI_DMA_REVISION_HIP08B)\n\t\treturn HISI_DMA_HIP08_MSI_NUM;\n\n\treturn HISI_DMA_HIP09_MSI_NUM;\n}\n\nstatic u32 hisi_dma_get_queue_base(struct pci_dev *pdev)\n{\n\tif (pdev->revision == HISI_DMA_REVISION_HIP08B)\n\t\treturn HISI_DMA_HIP08_Q_BASE;\n\n\treturn HISI_DMA_HIP09_Q_BASE;\n}\n\nstatic inline struct hisi_dma_chan *to_hisi_dma_chan(struct dma_chan *c)\n{\n\treturn container_of(c, struct hisi_dma_chan, vc.chan);\n}\n\nstatic inline struct hisi_dma_desc *to_hisi_dma_desc(struct virt_dma_desc *vd)\n{\n\treturn container_of(vd, struct hisi_dma_desc, vd);\n}\n\nstatic inline void hisi_dma_chan_write(void __iomem *base, u32 reg, u32 index,\n\t\t\t\t       u32 val)\n{\n\twritel_relaxed(val, base + reg + index * HISI_DMA_Q_OFFSET);\n}\n\nstatic inline void hisi_dma_update_bit(void __iomem *addr, u32 pos, bool val)\n{\n\tu32 tmp;\n\n\ttmp = readl_relaxed(addr);\n\ttmp = val ? tmp | pos : tmp & ~pos;\n\twritel_relaxed(tmp, addr);\n}\n\nstatic void hisi_dma_pause_dma(struct hisi_dma_dev *hdma_dev, u32 index,\n\t\t\t       bool pause)\n{\n\tvoid __iomem *addr;\n\n\taddr = hdma_dev->queue_base + HISI_DMA_Q_CTRL0 +\n\t       index * HISI_DMA_Q_OFFSET;\n\thisi_dma_update_bit(addr, HISI_DMA_Q_CTRL0_QUEUE_PAUSE, pause);\n}\n\nstatic void hisi_dma_enable_dma(struct hisi_dma_dev *hdma_dev, u32 index,\n\t\t\t\tbool enable)\n{\n\tvoid __iomem *addr;\n\n\taddr = hdma_dev->queue_base + HISI_DMA_Q_CTRL0 +\n\t       index * HISI_DMA_Q_OFFSET;\n\thisi_dma_update_bit(addr, HISI_DMA_Q_CTRL0_QUEUE_EN, enable);\n}\n\nstatic void hisi_dma_mask_irq(struct hisi_dma_dev *hdma_dev, u32 qp_index)\n{\n\tvoid __iomem *q_base = hdma_dev->queue_base;\n\n\tif (hdma_dev->reg_layout == HISI_DMA_REG_LAYOUT_HIP08)\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP08_Q_INT_MSK,\n\t\t\t\t    qp_index, HISI_DMA_HIP08_Q_INT_STS_MASK);\n\telse {\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP09_Q_INT_MSK,\n\t\t\t\t    qp_index, HISI_DMA_HIP09_Q_INT_STS_MASK);\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP09_Q_ERR_INT_MSK,\n\t\t\t\t    qp_index,\n\t\t\t\t    HISI_DMA_HIP09_Q_ERR_INT_STS_MASK);\n\t}\n}\n\nstatic void hisi_dma_unmask_irq(struct hisi_dma_dev *hdma_dev, u32 qp_index)\n{\n\tvoid __iomem *q_base = hdma_dev->queue_base;\n\n\tif (hdma_dev->reg_layout == HISI_DMA_REG_LAYOUT_HIP08) {\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP08_Q_INT_STS,\n\t\t\t\t    qp_index, HISI_DMA_HIP08_Q_INT_STS_MASK);\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP08_Q_INT_MSK,\n\t\t\t\t    qp_index, 0);\n\t} else {\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP09_Q_INT_STS,\n\t\t\t\t    qp_index, HISI_DMA_HIP09_Q_INT_STS_MASK);\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP09_Q_ERR_INT_STS,\n\t\t\t\t    qp_index,\n\t\t\t\t    HISI_DMA_HIP09_Q_ERR_INT_STS_MASK);\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP09_Q_INT_MSK,\n\t\t\t\t    qp_index, 0);\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP09_Q_ERR_INT_MSK,\n\t\t\t\t    qp_index, 0);\n\t}\n}\n\nstatic void hisi_dma_do_reset(struct hisi_dma_dev *hdma_dev, u32 index)\n{\n\tvoid __iomem *addr;\n\n\taddr = hdma_dev->queue_base +\n\t       HISI_DMA_Q_CTRL1 + index * HISI_DMA_Q_OFFSET;\n\thisi_dma_update_bit(addr, HISI_DMA_Q_CTRL1_QUEUE_RESET, 1);\n}\n\nstatic void hisi_dma_reset_qp_point(struct hisi_dma_dev *hdma_dev, u32 index)\n{\n\tvoid __iomem *q_base = hdma_dev->queue_base;\n\n\thisi_dma_chan_write(q_base, HISI_DMA_Q_SQ_TAIL_PTR, index, 0);\n\thisi_dma_chan_write(q_base, HISI_DMA_Q_CQ_HEAD_PTR, index, 0);\n}\n\nstatic void hisi_dma_reset_or_disable_hw_chan(struct hisi_dma_chan *chan,\n\t\t\t\t\t      bool disable)\n{\n\tstruct hisi_dma_dev *hdma_dev = chan->hdma_dev;\n\tu32 index = chan->qp_num, tmp;\n\tvoid __iomem *addr;\n\tint ret;\n\n\thisi_dma_pause_dma(hdma_dev, index, true);\n\thisi_dma_enable_dma(hdma_dev, index, false);\n\thisi_dma_mask_irq(hdma_dev, index);\n\n\taddr = hdma_dev->queue_base +\n\t       HISI_DMA_Q_FSM_STS + index * HISI_DMA_Q_OFFSET;\n\n\tret = readl_relaxed_poll_timeout(addr, tmp,\n\t\tFIELD_GET(HISI_DMA_Q_FSM_STS_MASK, tmp) != RUN,\n\t\tHISI_DMA_POLL_Q_STS_DELAY_US, HISI_DMA_POLL_Q_STS_TIME_OUT_US);\n\tif (ret) {\n\t\tdev_err(&hdma_dev->pdev->dev, \"disable channel timeout!\\n\");\n\t\tWARN_ON(1);\n\t}\n\n\thisi_dma_do_reset(hdma_dev, index);\n\thisi_dma_reset_qp_point(hdma_dev, index);\n\thisi_dma_pause_dma(hdma_dev, index, false);\n\n\tif (!disable) {\n\t\thisi_dma_enable_dma(hdma_dev, index, true);\n\t\thisi_dma_unmask_irq(hdma_dev, index);\n\t}\n\n\tret = readl_relaxed_poll_timeout(addr, tmp,\n\t\tFIELD_GET(HISI_DMA_Q_FSM_STS_MASK, tmp) == IDLE,\n\t\tHISI_DMA_POLL_Q_STS_DELAY_US, HISI_DMA_POLL_Q_STS_TIME_OUT_US);\n\tif (ret) {\n\t\tdev_err(&hdma_dev->pdev->dev, \"reset channel timeout!\\n\");\n\t\tWARN_ON(1);\n\t}\n}\n\nstatic void hisi_dma_free_chan_resources(struct dma_chan *c)\n{\n\tstruct hisi_dma_chan *chan = to_hisi_dma_chan(c);\n\tstruct hisi_dma_dev *hdma_dev = chan->hdma_dev;\n\n\thisi_dma_reset_or_disable_hw_chan(chan, false);\n\tvchan_free_chan_resources(&chan->vc);\n\n\tmemset(chan->sq, 0, sizeof(struct hisi_dma_sqe) * hdma_dev->chan_depth);\n\tmemset(chan->cq, 0, sizeof(struct hisi_dma_cqe) * hdma_dev->chan_depth);\n\tchan->sq_tail = 0;\n\tchan->cq_head = 0;\n\tchan->status = DISABLE;\n}\n\nstatic void hisi_dma_desc_free(struct virt_dma_desc *vd)\n{\n\tkfree(to_hisi_dma_desc(vd));\n}\n\nstatic struct dma_async_tx_descriptor *\nhisi_dma_prep_dma_memcpy(struct dma_chan *c, dma_addr_t dst, dma_addr_t src,\n\t\t\t size_t len, unsigned long flags)\n{\n\tstruct hisi_dma_chan *chan = to_hisi_dma_chan(c);\n\tstruct hisi_dma_desc *desc;\n\n\tdesc = kzalloc(sizeof(*desc), GFP_NOWAIT);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdesc->sqe.length = cpu_to_le32(len);\n\tdesc->sqe.src_addr = cpu_to_le64(src);\n\tdesc->sqe.dst_addr = cpu_to_le64(dst);\n\n\treturn vchan_tx_prep(&chan->vc, &desc->vd, flags);\n}\n\nstatic enum dma_status\nhisi_dma_tx_status(struct dma_chan *c, dma_cookie_t cookie,\n\t\t   struct dma_tx_state *txstate)\n{\n\treturn dma_cookie_status(c, cookie, txstate);\n}\n\nstatic void hisi_dma_start_transfer(struct hisi_dma_chan *chan)\n{\n\tstruct hisi_dma_sqe *sqe = chan->sq + chan->sq_tail;\n\tstruct hisi_dma_dev *hdma_dev = chan->hdma_dev;\n\tstruct hisi_dma_desc *desc;\n\tstruct virt_dma_desc *vd;\n\n\tvd = vchan_next_desc(&chan->vc);\n\tif (!vd) {\n\t\tchan->desc = NULL;\n\t\treturn;\n\t}\n\tlist_del(&vd->node);\n\tdesc = to_hisi_dma_desc(vd);\n\tchan->desc = desc;\n\n\tmemcpy(sqe, &desc->sqe, sizeof(struct hisi_dma_sqe));\n\n\t \n\tsqe->dw0 = cpu_to_le32(FIELD_PREP(OPCODE_MASK, OPCODE_M2M));\n\tsqe->dw0 |= cpu_to_le32(LOCAL_IRQ_EN);\n\n\t \n\twmb();\n\n\t \n\tchan->sq_tail = (chan->sq_tail + 1) % hdma_dev->chan_depth;\n\n\t \n\thisi_dma_chan_write(hdma_dev->queue_base, HISI_DMA_Q_SQ_TAIL_PTR,\n\t\t\t    chan->qp_num, chan->sq_tail);\n}\n\nstatic void hisi_dma_issue_pending(struct dma_chan *c)\n{\n\tstruct hisi_dma_chan *chan = to_hisi_dma_chan(c);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\n\tif (vchan_issue_pending(&chan->vc) && !chan->desc)\n\t\thisi_dma_start_transfer(chan);\n\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n}\n\nstatic int hisi_dma_terminate_all(struct dma_chan *c)\n{\n\tstruct hisi_dma_chan *chan = to_hisi_dma_chan(c);\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\n\thisi_dma_pause_dma(chan->hdma_dev, chan->qp_num, true);\n\tif (chan->desc) {\n\t\tvchan_terminate_vdesc(&chan->desc->vd);\n\t\tchan->desc = NULL;\n\t}\n\n\tvchan_get_all_descriptors(&chan->vc, &head);\n\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\n\tvchan_dma_desc_free_list(&chan->vc, &head);\n\thisi_dma_pause_dma(chan->hdma_dev, chan->qp_num, false);\n\n\treturn 0;\n}\n\nstatic void hisi_dma_synchronize(struct dma_chan *c)\n{\n\tstruct hisi_dma_chan *chan = to_hisi_dma_chan(c);\n\n\tvchan_synchronize(&chan->vc);\n}\n\nstatic int hisi_dma_alloc_qps_mem(struct hisi_dma_dev *hdma_dev)\n{\n\tsize_t sq_size = sizeof(struct hisi_dma_sqe) * hdma_dev->chan_depth;\n\tsize_t cq_size = sizeof(struct hisi_dma_cqe) * hdma_dev->chan_depth;\n\tstruct device *dev = &hdma_dev->pdev->dev;\n\tstruct hisi_dma_chan *chan;\n\tint i;\n\n\tfor (i = 0; i < hdma_dev->chan_num; i++) {\n\t\tchan = &hdma_dev->chan[i];\n\t\tchan->sq = dmam_alloc_coherent(dev, sq_size, &chan->sq_dma,\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!chan->sq)\n\t\t\treturn -ENOMEM;\n\n\t\tchan->cq = dmam_alloc_coherent(dev, cq_size, &chan->cq_dma,\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!chan->cq)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void hisi_dma_init_hw_qp(struct hisi_dma_dev *hdma_dev, u32 index)\n{\n\tstruct hisi_dma_chan *chan = &hdma_dev->chan[index];\n\tvoid __iomem *q_base = hdma_dev->queue_base;\n\tu32 hw_depth = hdma_dev->chan_depth - 1;\n\tvoid __iomem *addr;\n\tu32 tmp;\n\n\t \n\thisi_dma_chan_write(q_base, HISI_DMA_Q_SQ_BASE_L, index,\n\t\t\t    lower_32_bits(chan->sq_dma));\n\thisi_dma_chan_write(q_base, HISI_DMA_Q_SQ_BASE_H, index,\n\t\t\t    upper_32_bits(chan->sq_dma));\n\thisi_dma_chan_write(q_base, HISI_DMA_Q_CQ_BASE_L, index,\n\t\t\t    lower_32_bits(chan->cq_dma));\n\thisi_dma_chan_write(q_base, HISI_DMA_Q_CQ_BASE_H, index,\n\t\t\t    upper_32_bits(chan->cq_dma));\n\n\t \n\thisi_dma_chan_write(q_base, HISI_DMA_Q_SQ_DEPTH, index, hw_depth);\n\thisi_dma_chan_write(q_base, HISI_DMA_Q_CQ_DEPTH, index, hw_depth);\n\n\t \n\thisi_dma_chan_write(q_base, HISI_DMA_Q_SQ_TAIL_PTR, index, 0);\n\thisi_dma_chan_write(q_base, HISI_DMA_Q_CQ_HEAD_PTR, index, 0);\n\n\t \n\thisi_dma_chan_write(q_base, HISI_DMA_Q_ERR_INT_NUM0, index, 0);\n\thisi_dma_chan_write(q_base, HISI_DMA_Q_ERR_INT_NUM1, index, 0);\n\thisi_dma_chan_write(q_base, HISI_DMA_Q_ERR_INT_NUM2, index, 0);\n\n\tif (hdma_dev->reg_layout == HISI_DMA_REG_LAYOUT_HIP08) {\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP08_Q_ERR_INT_NUM3,\n\t\t\t\t    index, 0);\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP08_Q_ERR_INT_NUM4,\n\t\t\t\t    index, 0);\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP08_Q_ERR_INT_NUM5,\n\t\t\t\t    index, 0);\n\t\thisi_dma_chan_write(q_base, HISI_DMA_HIP08_Q_ERR_INT_NUM6,\n\t\t\t\t    index, 0);\n\t\t \n\t\taddr = q_base + HISI_DMA_Q_CTRL0 + index * HISI_DMA_Q_OFFSET;\n\t\thisi_dma_update_bit(addr, HISI_DMA_HIP08_Q_CTRL0_SQCQ_DRCT, 0);\n\n\t\t \n\t\thisi_dma_update_bit(addr,\n\t\t\t\t    HISI_DMA_HIP08_Q_CTRL0_ERR_ABORT_EN, 0);\n\t} else {\n\t\taddr = q_base + HISI_DMA_Q_CTRL0 + index * HISI_DMA_Q_OFFSET;\n\n\t\t \n\t\thisi_dma_update_bit(addr, HISI_DMA_HIP09_Q_CTRL0_SQ_DRCT, 0);\n\t\thisi_dma_update_bit(addr, HISI_DMA_HIP09_Q_CTRL0_CQ_DRCT, 0);\n\n\t\t \n\n\t\ttmp = readl_relaxed(addr);\n\t\ttmp &= ~HISI_DMA_HIP09_Q_CTRL0_ERR_ABORT_EN;\n\t\twritel_relaxed(tmp, addr);\n\n\t\t \n\t\taddr = q_base + HISI_DMA_HIP09_DMA_FLR_DISABLE +\n\t\t       index * HISI_DMA_Q_OFFSET;\n\t\thisi_dma_update_bit(addr, HISI_DMA_HIP09_DMA_FLR_DISABLE_B, 0);\n\n\t\taddr = q_base + HISI_DMA_Q_CTRL1 + index * HISI_DMA_Q_OFFSET;\n\t\thisi_dma_update_bit(addr, HISI_DMA_HIP09_Q_CTRL1_VA_ENABLE, 1);\n\t}\n}\n\nstatic void hisi_dma_enable_qp(struct hisi_dma_dev *hdma_dev, u32 qp_index)\n{\n\thisi_dma_init_hw_qp(hdma_dev, qp_index);\n\thisi_dma_unmask_irq(hdma_dev, qp_index);\n\thisi_dma_enable_dma(hdma_dev, qp_index, true);\n}\n\nstatic void hisi_dma_disable_qp(struct hisi_dma_dev *hdma_dev, u32 qp_index)\n{\n\thisi_dma_reset_or_disable_hw_chan(&hdma_dev->chan[qp_index], true);\n}\n\nstatic void hisi_dma_enable_qps(struct hisi_dma_dev *hdma_dev)\n{\n\tint i;\n\n\tfor (i = 0; i < hdma_dev->chan_num; i++) {\n\t\thdma_dev->chan[i].qp_num = i;\n\t\thdma_dev->chan[i].hdma_dev = hdma_dev;\n\t\thdma_dev->chan[i].vc.desc_free = hisi_dma_desc_free;\n\t\tvchan_init(&hdma_dev->chan[i].vc, &hdma_dev->dma_dev);\n\t\thisi_dma_enable_qp(hdma_dev, i);\n\t}\n}\n\nstatic void hisi_dma_disable_qps(struct hisi_dma_dev *hdma_dev)\n{\n\tint i;\n\n\tfor (i = 0; i < hdma_dev->chan_num; i++) {\n\t\thisi_dma_disable_qp(hdma_dev, i);\n\t\ttasklet_kill(&hdma_dev->chan[i].vc.task);\n\t}\n}\n\nstatic irqreturn_t hisi_dma_irq(int irq, void *data)\n{\n\tstruct hisi_dma_chan *chan = data;\n\tstruct hisi_dma_dev *hdma_dev = chan->hdma_dev;\n\tstruct hisi_dma_desc *desc;\n\tstruct hisi_dma_cqe *cqe;\n\tvoid __iomem *q_base;\n\n\tspin_lock(&chan->vc.lock);\n\n\tdesc = chan->desc;\n\tcqe = chan->cq + chan->cq_head;\n\tq_base = hdma_dev->queue_base;\n\tif (desc) {\n\t\tchan->cq_head = (chan->cq_head + 1) % hdma_dev->chan_depth;\n\t\thisi_dma_chan_write(q_base, HISI_DMA_Q_CQ_HEAD_PTR,\n\t\t\t\t    chan->qp_num, chan->cq_head);\n\t\tif (FIELD_GET(STATUS_MASK, cqe->w0) == STATUS_SUCC) {\n\t\t\tvchan_cookie_complete(&desc->vd);\n\t\t\thisi_dma_start_transfer(chan);\n\t\t} else {\n\t\t\tdev_err(&hdma_dev->pdev->dev, \"task error!\\n\");\n\t\t}\n\t}\n\n\tspin_unlock(&chan->vc.lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int hisi_dma_request_qps_irq(struct hisi_dma_dev *hdma_dev)\n{\n\tstruct pci_dev *pdev = hdma_dev->pdev;\n\tint i, ret;\n\n\tfor (i = 0; i < hdma_dev->chan_num; i++) {\n\t\tret = devm_request_irq(&pdev->dev, pci_irq_vector(pdev, i),\n\t\t\t\t       hisi_dma_irq, IRQF_SHARED, \"hisi_dma\",\n\t\t\t\t       &hdma_dev->chan[i]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int hisi_dma_enable_hw_channels(struct hisi_dma_dev *hdma_dev)\n{\n\tint ret;\n\n\tret = hisi_dma_alloc_qps_mem(hdma_dev);\n\tif (ret) {\n\t\tdev_err(&hdma_dev->pdev->dev, \"fail to allocate qp memory!\\n\");\n\t\treturn ret;\n\t}\n\n\tret = hisi_dma_request_qps_irq(hdma_dev);\n\tif (ret) {\n\t\tdev_err(&hdma_dev->pdev->dev, \"fail to request qp irq!\\n\");\n\t\treturn ret;\n\t}\n\n\thisi_dma_enable_qps(hdma_dev);\n\n\treturn 0;\n}\n\nstatic void hisi_dma_disable_hw_channels(void *data)\n{\n\thisi_dma_disable_qps(data);\n}\n\nstatic void hisi_dma_set_mode(struct hisi_dma_dev *hdma_dev,\n\t\t\t      enum hisi_dma_mode mode)\n{\n\tif (hdma_dev->reg_layout == HISI_DMA_REG_LAYOUT_HIP08)\n\t\twritel_relaxed(mode == RC ? 1 : 0,\n\t\t\t       hdma_dev->base + HISI_DMA_HIP08_MODE);\n}\n\nstatic void hisi_dma_init_hw(struct hisi_dma_dev *hdma_dev)\n{\n\tvoid __iomem *addr;\n\tint i;\n\n\tif (hdma_dev->reg_layout == HISI_DMA_REG_LAYOUT_HIP09) {\n\t\tfor (i = 0; i < HISI_DMA_HIP09_MAX_PORT_NUM; i++) {\n\t\t\taddr = hdma_dev->base + HISI_DMA_HIP09_PORT_CFG_REG(i);\n\t\t\thisi_dma_update_bit(addr,\n\t\t\t\tHISI_DMA_HIP09_PORT_CFG_LINK_DOWN_MASK_B, 1);\n\t\t}\n\t}\n}\n\nstatic void hisi_dma_init_dma_dev(struct hisi_dma_dev *hdma_dev)\n{\n\tstruct dma_device *dma_dev;\n\n\tdma_dev = &hdma_dev->dma_dev;\n\tdma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);\n\tdma_dev->device_free_chan_resources = hisi_dma_free_chan_resources;\n\tdma_dev->device_prep_dma_memcpy = hisi_dma_prep_dma_memcpy;\n\tdma_dev->device_tx_status = hisi_dma_tx_status;\n\tdma_dev->device_issue_pending = hisi_dma_issue_pending;\n\tdma_dev->device_terminate_all = hisi_dma_terminate_all;\n\tdma_dev->device_synchronize = hisi_dma_synchronize;\n\tdma_dev->directions = BIT(DMA_MEM_TO_MEM);\n\tdma_dev->dev = &hdma_dev->pdev->dev;\n\tINIT_LIST_HEAD(&dma_dev->channels);\n}\n\n \n#ifdef CONFIG_DEBUG_FS\n#include <linux/debugfs.h>\nstatic struct debugfs_reg32 *hisi_dma_get_ch_regs(struct hisi_dma_dev *hdma_dev,\n\t\t\t\t\t\t  u32 *regs_sz)\n{\n\tstruct device *dev = &hdma_dev->pdev->dev;\n\tstruct debugfs_reg32 *regs;\n\tu32 regs_sz_comm;\n\n\tregs_sz_comm = ARRAY_SIZE(hisi_dma_comm_chan_regs);\n\n\tif (hdma_dev->reg_layout == HISI_DMA_REG_LAYOUT_HIP08)\n\t\t*regs_sz = regs_sz_comm + ARRAY_SIZE(hisi_dma_hip08_chan_regs);\n\telse\n\t\t*regs_sz = regs_sz_comm + ARRAY_SIZE(hisi_dma_hip09_chan_regs);\n\n\tregs = devm_kcalloc(dev, *regs_sz, sizeof(struct debugfs_reg32),\n\t\t\t    GFP_KERNEL);\n\tif (!regs)\n\t\treturn NULL;\n\tmemcpy(regs, hisi_dma_comm_chan_regs, sizeof(hisi_dma_comm_chan_regs));\n\n\tif (hdma_dev->reg_layout == HISI_DMA_REG_LAYOUT_HIP08)\n\t\tmemcpy(regs + regs_sz_comm, hisi_dma_hip08_chan_regs,\n\t\t       sizeof(hisi_dma_hip08_chan_regs));\n\telse\n\t\tmemcpy(regs + regs_sz_comm, hisi_dma_hip09_chan_regs,\n\t\t       sizeof(hisi_dma_hip09_chan_regs));\n\n\treturn regs;\n}\n\nstatic int hisi_dma_create_chan_dir(struct hisi_dma_dev *hdma_dev)\n{\n\tchar dir_name[HISI_DMA_MAX_DIR_NAME_LEN];\n\tstruct debugfs_regset32 *regsets;\n\tstruct debugfs_reg32 *regs;\n\tstruct dentry *chan_dir;\n\tstruct device *dev;\n\tu32 regs_sz;\n\tint ret;\n\tint i;\n\n\tdev = &hdma_dev->pdev->dev;\n\n\tregsets = devm_kcalloc(dev, hdma_dev->chan_num,\n\t\t\t       sizeof(*regsets), GFP_KERNEL);\n\tif (!regsets)\n\t\treturn -ENOMEM;\n\n\tregs = hisi_dma_get_ch_regs(hdma_dev, &regs_sz);\n\tif (!regs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < hdma_dev->chan_num; i++) {\n\t\tregsets[i].regs = regs;\n\t\tregsets[i].nregs = regs_sz;\n\t\tregsets[i].base = hdma_dev->queue_base + i * HISI_DMA_Q_OFFSET;\n\t\tregsets[i].dev = dev;\n\n\t\tmemset(dir_name, 0, HISI_DMA_MAX_DIR_NAME_LEN);\n\t\tret = sprintf(dir_name, \"channel%d\", i);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tchan_dir = debugfs_create_dir(dir_name,\n\t\t\t\t\t      hdma_dev->dma_dev.dbg_dev_root);\n\t\tdebugfs_create_regset32(\"regs\", 0444, chan_dir, &regsets[i]);\n\t}\n\n\treturn 0;\n}\n\nstatic void hisi_dma_create_debugfs(struct hisi_dma_dev *hdma_dev)\n{\n\tstruct debugfs_regset32 *regset;\n\tstruct device *dev;\n\tint ret;\n\n\tdev = &hdma_dev->pdev->dev;\n\n\tif (hdma_dev->dma_dev.dbg_dev_root == NULL)\n\t\treturn;\n\n\tregset = devm_kzalloc(dev, sizeof(*regset), GFP_KERNEL);\n\tif (!regset)\n\t\treturn;\n\n\tif (hdma_dev->reg_layout == HISI_DMA_REG_LAYOUT_HIP08) {\n\t\tregset->regs = hisi_dma_hip08_comm_regs;\n\t\tregset->nregs = ARRAY_SIZE(hisi_dma_hip08_comm_regs);\n\t} else {\n\t\tregset->regs = hisi_dma_hip09_comm_regs;\n\t\tregset->nregs = ARRAY_SIZE(hisi_dma_hip09_comm_regs);\n\t}\n\tregset->base = hdma_dev->base;\n\tregset->dev = dev;\n\n\tdebugfs_create_regset32(\"regs\", 0444,\n\t\t\t\thdma_dev->dma_dev.dbg_dev_root, regset);\n\n\tret = hisi_dma_create_chan_dir(hdma_dev);\n\tif (ret < 0)\n\t\tdev_info(&hdma_dev->pdev->dev, \"fail to create debugfs for channels!\\n\");\n}\n#else\nstatic void hisi_dma_create_debugfs(struct hisi_dma_dev *hdma_dev) { }\n#endif  \n \n\nstatic int hisi_dma_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tenum hisi_dma_reg_layout reg_layout;\n\tstruct device *dev = &pdev->dev;\n\tstruct hisi_dma_dev *hdma_dev;\n\tstruct dma_device *dma_dev;\n\tu32 chan_num;\n\tu32 msi_num;\n\tint ret;\n\n\treg_layout = hisi_dma_get_reg_layout(pdev);\n\tif (reg_layout == HISI_DMA_REG_LAYOUT_INVALID) {\n\t\tdev_err(dev, \"unsupported device!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = pcim_enable_device(pdev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to enable device mem!\\n\");\n\t\treturn ret;\n\t}\n\n\tret = pcim_iomap_regions(pdev, 1 << PCI_BAR_2, pci_name(pdev));\n\tif (ret) {\n\t\tdev_err(dev, \"failed to remap I/O region!\\n\");\n\t\treturn ret;\n\t}\n\n\tret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (ret)\n\t\treturn ret;\n\n\tchan_num = hisi_dma_get_chan_num(pdev);\n\thdma_dev = devm_kzalloc(dev, struct_size(hdma_dev, chan, chan_num),\n\t\t\t\tGFP_KERNEL);\n\tif (!hdma_dev)\n\t\treturn -EINVAL;\n\n\thdma_dev->base = pcim_iomap_table(pdev)[PCI_BAR_2];\n\thdma_dev->pdev = pdev;\n\thdma_dev->chan_depth = HISI_DMA_Q_DEPTH_VAL;\n\thdma_dev->chan_num = chan_num;\n\thdma_dev->reg_layout = reg_layout;\n\thdma_dev->queue_base = hdma_dev->base + hisi_dma_get_queue_base(pdev);\n\n\tpci_set_drvdata(pdev, hdma_dev);\n\tpci_set_master(pdev);\n\n\tmsi_num = hisi_dma_get_msi_num(pdev);\n\n\t \n\tret = pci_alloc_irq_vectors(pdev, msi_num, msi_num, PCI_IRQ_MSI);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"Failed to allocate MSI vectors!\\n\");\n\t\treturn ret;\n\t}\n\n\thisi_dma_init_dma_dev(hdma_dev);\n\n\thisi_dma_set_mode(hdma_dev, RC);\n\n\thisi_dma_init_hw(hdma_dev);\n\n\tret = hisi_dma_enable_hw_channels(hdma_dev);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"failed to enable hw channel!\\n\");\n\t\treturn ret;\n\t}\n\n\tret = devm_add_action_or_reset(dev, hisi_dma_disable_hw_channels,\n\t\t\t\t       hdma_dev);\n\tif (ret)\n\t\treturn ret;\n\n\tdma_dev = &hdma_dev->dma_dev;\n\tret = dmaenginem_async_device_register(dma_dev);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"failed to register device!\\n\");\n\t\treturn ret;\n\t}\n\n\thisi_dma_create_debugfs(hdma_dev);\n\n\treturn 0;\n}\n\nstatic const struct pci_device_id hisi_dma_pci_tbl[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_HUAWEI, 0xa122) },\n\t{ 0, }\n};\n\nstatic struct pci_driver hisi_dma_pci_driver = {\n\t.name\t\t= \"hisi_dma\",\n\t.id_table\t= hisi_dma_pci_tbl,\n\t.probe\t\t= hisi_dma_probe,\n};\n\nmodule_pci_driver(hisi_dma_pci_driver);\n\nMODULE_AUTHOR(\"Zhou Wang <wangzhou1@hisilicon.com>\");\nMODULE_AUTHOR(\"Zhenfa Qiu <qiuzhenfa@hisilicon.com>\");\nMODULE_DESCRIPTION(\"HiSilicon Kunpeng DMA controller driver\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DEVICE_TABLE(pci, hisi_dma_pci_tbl);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}