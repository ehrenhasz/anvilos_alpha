{
  "module_name": "timb_dma.c",
  "hash_id": "194a37e7e060644c4b4467f6e773d8f559d6eb5fbe8a321eab5fb5db9a930560",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/timb_dma.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n\n#include <linux/timb_dma.h>\n\n#include \"dmaengine.h\"\n\n#define DRIVER_NAME \"timb-dma\"\n\n \n#define TIMBDMA_ACR\t\t0x34\n#define TIMBDMA_32BIT_ADDR\t0x01\n\n#define TIMBDMA_ISR\t\t0x080000\n#define TIMBDMA_IPR\t\t0x080004\n#define TIMBDMA_IER\t\t0x080008\n\n \n \n#define TIMBDMA_INSTANCE_OFFSET\t\t0x40\n#define TIMBDMA_INSTANCE_TX_OFFSET\t0x18\n\n \n#define TIMBDMA_OFFS_RX_DHAR\t0x00\n#define TIMBDMA_OFFS_RX_DLAR\t0x04\n#define TIMBDMA_OFFS_RX_LR\t0x0C\n#define TIMBDMA_OFFS_RX_BLR\t0x10\n#define TIMBDMA_OFFS_RX_ER\t0x14\n#define TIMBDMA_RX_EN\t\t0x01\n \n#define TIMBDMA_OFFS_RX_BPRR\t0x30\n\n \n#define TIMBDMA_OFFS_TX_DHAR\t0x00\n#define TIMBDMA_OFFS_TX_DLAR\t0x04\n#define TIMBDMA_OFFS_TX_BLR\t0x0C\n#define TIMBDMA_OFFS_TX_LR\t0x14\n\n\n#define TIMB_DMA_DESC_SIZE\t8\n\nstruct timb_dma_desc {\n\tstruct list_head\t\tdesc_node;\n\tstruct dma_async_tx_descriptor\ttxd;\n\tu8\t\t\t\t*desc_list;\n\tunsigned int\t\t\tdesc_list_len;\n\tbool\t\t\t\tinterrupt;\n};\n\nstruct timb_dma_chan {\n\tstruct dma_chan\t\tchan;\n\tvoid __iomem\t\t*membase;\n\tspinlock_t\t\tlock;  \n\tbool\t\t\tongoing;\n\tstruct list_head\tactive_list;\n\tstruct list_head\tqueue;\n\tstruct list_head\tfree_list;\n\tunsigned int\t\tbytes_per_line;\n\tenum dma_transfer_direction\tdirection;\n\tunsigned int\t\tdescs;  \n\tunsigned int\t\tdesc_elems;  \n};\n\nstruct timb_dma {\n\tstruct dma_device\tdma;\n\tvoid __iomem\t\t*membase;\n\tstruct tasklet_struct\ttasklet;\n\tstruct timb_dma_chan\tchannels[];\n};\n\nstatic struct device *chan2dev(struct dma_chan *chan)\n{\n\treturn &chan->dev->device;\n}\nstatic struct device *chan2dmadev(struct dma_chan *chan)\n{\n\treturn chan2dev(chan)->parent->parent;\n}\n\nstatic struct timb_dma *tdchantotd(struct timb_dma_chan *td_chan)\n{\n\tint id = td_chan->chan.chan_id;\n\treturn (struct timb_dma *)((u8 *)td_chan -\n\t\tid * sizeof(struct timb_dma_chan) - sizeof(struct timb_dma));\n}\n\n \nstatic void __td_enable_chan_irq(struct timb_dma_chan *td_chan)\n{\n\tint id = td_chan->chan.chan_id;\n\tstruct timb_dma *td = tdchantotd(td_chan);\n\tu32 ier;\n\n\t \n\tier = ioread32(td->membase + TIMBDMA_IER);\n\tier |= 1 << id;\n\tdev_dbg(chan2dev(&td_chan->chan), \"Enabling irq: %d, IER: 0x%x\\n\", id,\n\t\tier);\n\tiowrite32(ier, td->membase + TIMBDMA_IER);\n}\n\n \nstatic bool __td_dma_done_ack(struct timb_dma_chan *td_chan)\n{\n\tint id = td_chan->chan.chan_id;\n\tstruct timb_dma *td = (struct timb_dma *)((u8 *)td_chan -\n\t\tid * sizeof(struct timb_dma_chan) - sizeof(struct timb_dma));\n\tu32 isr;\n\tbool done = false;\n\n\tdev_dbg(chan2dev(&td_chan->chan), \"Checking irq: %d, td: %p\\n\", id, td);\n\n\tisr = ioread32(td->membase + TIMBDMA_ISR) & (1 << id);\n\tif (isr) {\n\t\tiowrite32(isr, td->membase + TIMBDMA_ISR);\n\t\tdone = true;\n\t}\n\n\treturn done;\n}\n\nstatic int td_fill_desc(struct timb_dma_chan *td_chan, u8 *dma_desc,\n\tstruct scatterlist *sg, bool last)\n{\n\tif (sg_dma_len(sg) > USHRT_MAX) {\n\t\tdev_err(chan2dev(&td_chan->chan), \"Too big sg element\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (sg_dma_len(sg) % sizeof(u32)) {\n\t\tdev_err(chan2dev(&td_chan->chan), \"Incorrect length: %d\\n\",\n\t\t\tsg_dma_len(sg));\n\t\treturn -EINVAL;\n\t}\n\n\tdev_dbg(chan2dev(&td_chan->chan), \"desc: %p, addr: 0x%llx\\n\",\n\t\tdma_desc, (unsigned long long)sg_dma_address(sg));\n\n\tdma_desc[7] = (sg_dma_address(sg) >> 24) & 0xff;\n\tdma_desc[6] = (sg_dma_address(sg) >> 16) & 0xff;\n\tdma_desc[5] = (sg_dma_address(sg) >> 8) & 0xff;\n\tdma_desc[4] = (sg_dma_address(sg) >> 0) & 0xff;\n\n\tdma_desc[3] = (sg_dma_len(sg) >> 8) & 0xff;\n\tdma_desc[2] = (sg_dma_len(sg) >> 0) & 0xff;\n\n\tdma_desc[1] = 0x00;\n\tdma_desc[0] = 0x21 | (last ? 0x02 : 0);  \n\n\treturn 0;\n}\n\n \nstatic void __td_start_dma(struct timb_dma_chan *td_chan)\n{\n\tstruct timb_dma_desc *td_desc;\n\n\tif (td_chan->ongoing) {\n\t\tdev_err(chan2dev(&td_chan->chan),\n\t\t\t\"Transfer already ongoing\\n\");\n\t\treturn;\n\t}\n\n\ttd_desc = list_entry(td_chan->active_list.next, struct timb_dma_desc,\n\t\tdesc_node);\n\n\tdev_dbg(chan2dev(&td_chan->chan),\n\t\t\"td_chan: %p, chan: %d, membase: %p\\n\",\n\t\ttd_chan, td_chan->chan.chan_id, td_chan->membase);\n\n\tif (td_chan->direction == DMA_DEV_TO_MEM) {\n\n\t\t \n\t\tiowrite32(0, td_chan->membase + TIMBDMA_OFFS_RX_DHAR);\n\t\tiowrite32(td_desc->txd.phys, td_chan->membase +\n\t\t\tTIMBDMA_OFFS_RX_DLAR);\n\t\t \n\t\tiowrite32(td_chan->bytes_per_line, td_chan->membase +\n\t\t\tTIMBDMA_OFFS_RX_BPRR);\n\t\t \n\t\tiowrite32(TIMBDMA_RX_EN, td_chan->membase + TIMBDMA_OFFS_RX_ER);\n\t} else {\n\t\t \n\t\tiowrite32(0, td_chan->membase + TIMBDMA_OFFS_TX_DHAR);\n\t\tiowrite32(td_desc->txd.phys, td_chan->membase +\n\t\t\tTIMBDMA_OFFS_TX_DLAR);\n\t}\n\n\ttd_chan->ongoing = true;\n\n\tif (td_desc->interrupt)\n\t\t__td_enable_chan_irq(td_chan);\n}\n\nstatic void __td_finish(struct timb_dma_chan *td_chan)\n{\n\tstruct dmaengine_desc_callback\tcb;\n\tstruct dma_async_tx_descriptor\t*txd;\n\tstruct timb_dma_desc\t\t*td_desc;\n\n\t \n\tif (list_empty(&td_chan->active_list))\n\t\treturn;\n\n\ttd_desc = list_entry(td_chan->active_list.next, struct timb_dma_desc,\n\t\tdesc_node);\n\ttxd = &td_desc->txd;\n\n\tdev_dbg(chan2dev(&td_chan->chan), \"descriptor %u complete\\n\",\n\t\ttxd->cookie);\n\n\t \n\tif (td_chan->direction == DMA_DEV_TO_MEM)\n\t\tiowrite32(0, td_chan->membase + TIMBDMA_OFFS_RX_ER);\n \n\tdma_cookie_complete(txd);\n\ttd_chan->ongoing = false;\n\n\tdmaengine_desc_get_callback(txd, &cb);\n\n\tlist_move(&td_desc->desc_node, &td_chan->free_list);\n\n\tdma_descriptor_unmap(txd);\n\t \n\tdmaengine_desc_callback_invoke(&cb, NULL);\n}\n\nstatic u32 __td_ier_mask(struct timb_dma *td)\n{\n\tint i;\n\tu32 ret = 0;\n\n\tfor (i = 0; i < td->dma.chancnt; i++) {\n\t\tstruct timb_dma_chan *td_chan = td->channels + i;\n\t\tif (td_chan->ongoing) {\n\t\t\tstruct timb_dma_desc *td_desc =\n\t\t\t\tlist_entry(td_chan->active_list.next,\n\t\t\t\tstruct timb_dma_desc, desc_node);\n\t\t\tif (td_desc->interrupt)\n\t\t\t\tret |= 1 << i;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void __td_start_next(struct timb_dma_chan *td_chan)\n{\n\tstruct timb_dma_desc *td_desc;\n\n\tBUG_ON(list_empty(&td_chan->queue));\n\tBUG_ON(td_chan->ongoing);\n\n\ttd_desc = list_entry(td_chan->queue.next, struct timb_dma_desc,\n\t\tdesc_node);\n\n\tdev_dbg(chan2dev(&td_chan->chan), \"%s: started %u\\n\",\n\t\t__func__, td_desc->txd.cookie);\n\n\tlist_move(&td_desc->desc_node, &td_chan->active_list);\n\t__td_start_dma(td_chan);\n}\n\nstatic dma_cookie_t td_tx_submit(struct dma_async_tx_descriptor *txd)\n{\n\tstruct timb_dma_desc *td_desc = container_of(txd, struct timb_dma_desc,\n\t\ttxd);\n\tstruct timb_dma_chan *td_chan = container_of(txd->chan,\n\t\tstruct timb_dma_chan, chan);\n\tdma_cookie_t cookie;\n\n\tspin_lock_bh(&td_chan->lock);\n\tcookie = dma_cookie_assign(txd);\n\n\tif (list_empty(&td_chan->active_list)) {\n\t\tdev_dbg(chan2dev(txd->chan), \"%s: started %u\\n\", __func__,\n\t\t\ttxd->cookie);\n\t\tlist_add_tail(&td_desc->desc_node, &td_chan->active_list);\n\t\t__td_start_dma(td_chan);\n\t} else {\n\t\tdev_dbg(chan2dev(txd->chan), \"tx_submit: queued %u\\n\",\n\t\t\ttxd->cookie);\n\n\t\tlist_add_tail(&td_desc->desc_node, &td_chan->queue);\n\t}\n\n\tspin_unlock_bh(&td_chan->lock);\n\n\treturn cookie;\n}\n\nstatic struct timb_dma_desc *td_alloc_init_desc(struct timb_dma_chan *td_chan)\n{\n\tstruct dma_chan *chan = &td_chan->chan;\n\tstruct timb_dma_desc *td_desc;\n\tint err;\n\n\ttd_desc = kzalloc(sizeof(struct timb_dma_desc), GFP_KERNEL);\n\tif (!td_desc)\n\t\tgoto out;\n\n\ttd_desc->desc_list_len = td_chan->desc_elems * TIMB_DMA_DESC_SIZE;\n\n\ttd_desc->desc_list = kzalloc(td_desc->desc_list_len, GFP_KERNEL);\n\tif (!td_desc->desc_list)\n\t\tgoto err;\n\n\tdma_async_tx_descriptor_init(&td_desc->txd, chan);\n\ttd_desc->txd.tx_submit = td_tx_submit;\n\ttd_desc->txd.flags = DMA_CTRL_ACK;\n\n\ttd_desc->txd.phys = dma_map_single(chan2dmadev(chan),\n\t\ttd_desc->desc_list, td_desc->desc_list_len, DMA_TO_DEVICE);\n\n\terr = dma_mapping_error(chan2dmadev(chan), td_desc->txd.phys);\n\tif (err) {\n\t\tdev_err(chan2dev(chan), \"DMA mapping error: %d\\n\", err);\n\t\tgoto err;\n\t}\n\n\treturn td_desc;\nerr:\n\tkfree(td_desc->desc_list);\n\tkfree(td_desc);\nout:\n\treturn NULL;\n\n}\n\nstatic void td_free_desc(struct timb_dma_desc *td_desc)\n{\n\tdev_dbg(chan2dev(td_desc->txd.chan), \"Freeing desc: %p\\n\", td_desc);\n\tdma_unmap_single(chan2dmadev(td_desc->txd.chan), td_desc->txd.phys,\n\t\ttd_desc->desc_list_len, DMA_TO_DEVICE);\n\n\tkfree(td_desc->desc_list);\n\tkfree(td_desc);\n}\n\nstatic void td_desc_put(struct timb_dma_chan *td_chan,\n\tstruct timb_dma_desc *td_desc)\n{\n\tdev_dbg(chan2dev(&td_chan->chan), \"Putting desc: %p\\n\", td_desc);\n\n\tspin_lock_bh(&td_chan->lock);\n\tlist_add(&td_desc->desc_node, &td_chan->free_list);\n\tspin_unlock_bh(&td_chan->lock);\n}\n\nstatic struct timb_dma_desc *td_desc_get(struct timb_dma_chan *td_chan)\n{\n\tstruct timb_dma_desc *td_desc, *_td_desc;\n\tstruct timb_dma_desc *ret = NULL;\n\n\tspin_lock_bh(&td_chan->lock);\n\tlist_for_each_entry_safe(td_desc, _td_desc, &td_chan->free_list,\n\t\tdesc_node) {\n\t\tif (async_tx_test_ack(&td_desc->txd)) {\n\t\t\tlist_del(&td_desc->desc_node);\n\t\t\tret = td_desc;\n\t\t\tbreak;\n\t\t}\n\t\tdev_dbg(chan2dev(&td_chan->chan), \"desc %p not ACKed\\n\",\n\t\t\ttd_desc);\n\t}\n\tspin_unlock_bh(&td_chan->lock);\n\n\treturn ret;\n}\n\nstatic int td_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct timb_dma_chan *td_chan =\n\t\tcontainer_of(chan, struct timb_dma_chan, chan);\n\tint i;\n\n\tdev_dbg(chan2dev(chan), \"%s: entry\\n\", __func__);\n\n\tBUG_ON(!list_empty(&td_chan->free_list));\n\tfor (i = 0; i < td_chan->descs; i++) {\n\t\tstruct timb_dma_desc *td_desc = td_alloc_init_desc(td_chan);\n\t\tif (!td_desc) {\n\t\t\tif (i)\n\t\t\t\tbreak;\n\t\t\telse {\n\t\t\t\tdev_err(chan2dev(chan),\n\t\t\t\t\t\"Couldn't allocate any descriptors\\n\");\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\ttd_desc_put(td_chan, td_desc);\n\t}\n\n\tspin_lock_bh(&td_chan->lock);\n\tdma_cookie_init(chan);\n\tspin_unlock_bh(&td_chan->lock);\n\n\treturn 0;\n}\n\nstatic void td_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct timb_dma_chan *td_chan =\n\t\tcontainer_of(chan, struct timb_dma_chan, chan);\n\tstruct timb_dma_desc *td_desc, *_td_desc;\n\tLIST_HEAD(list);\n\n\tdev_dbg(chan2dev(chan), \"%s: Entry\\n\", __func__);\n\n\t \n\tBUG_ON(!list_empty(&td_chan->active_list));\n\tBUG_ON(!list_empty(&td_chan->queue));\n\n\tspin_lock_bh(&td_chan->lock);\n\tlist_splice_init(&td_chan->free_list, &list);\n\tspin_unlock_bh(&td_chan->lock);\n\n\tlist_for_each_entry_safe(td_desc, _td_desc, &list, desc_node) {\n\t\tdev_dbg(chan2dev(chan), \"%s: Freeing desc: %p\\n\", __func__,\n\t\t\ttd_desc);\n\t\ttd_free_desc(td_desc);\n\t}\n}\n\nstatic enum dma_status td_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\n\t\t\t\t    struct dma_tx_state *txstate)\n{\n\tenum dma_status ret;\n\n\tdev_dbg(chan2dev(chan), \"%s: Entry\\n\", __func__);\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\n\tdev_dbg(chan2dev(chan), \"%s: exit, ret: %d\\n\", \t__func__, ret);\n\n\treturn ret;\n}\n\nstatic void td_issue_pending(struct dma_chan *chan)\n{\n\tstruct timb_dma_chan *td_chan =\n\t\tcontainer_of(chan, struct timb_dma_chan, chan);\n\n\tdev_dbg(chan2dev(chan), \"%s: Entry\\n\", __func__);\n\tspin_lock_bh(&td_chan->lock);\n\n\tif (!list_empty(&td_chan->active_list))\n\t\t \n\t\tif (__td_dma_done_ack(td_chan))\n\t\t\t__td_finish(td_chan);\n\n\tif (list_empty(&td_chan->active_list) && !list_empty(&td_chan->queue))\n\t\t__td_start_next(td_chan);\n\n\tspin_unlock_bh(&td_chan->lock);\n}\n\nstatic struct dma_async_tx_descriptor *td_prep_slave_sg(struct dma_chan *chan,\n\tstruct scatterlist *sgl, unsigned int sg_len,\n\tenum dma_transfer_direction direction, unsigned long flags,\n\tvoid *context)\n{\n\tstruct timb_dma_chan *td_chan =\n\t\tcontainer_of(chan, struct timb_dma_chan, chan);\n\tstruct timb_dma_desc *td_desc;\n\tstruct scatterlist *sg;\n\tunsigned int i;\n\tunsigned int desc_usage = 0;\n\n\tif (!sgl || !sg_len) {\n\t\tdev_err(chan2dev(chan), \"%s: No SG list\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (td_chan->direction != direction) {\n\t\tdev_err(chan2dev(chan),\n\t\t\t\"Requesting channel in wrong direction\\n\");\n\t\treturn NULL;\n\t}\n\n\ttd_desc = td_desc_get(td_chan);\n\tif (!td_desc) {\n\t\tdev_err(chan2dev(chan), \"Not enough descriptors available\\n\");\n\t\treturn NULL;\n\t}\n\n\ttd_desc->interrupt = (flags & DMA_PREP_INTERRUPT) != 0;\n\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tint err;\n\t\tif (desc_usage > td_desc->desc_list_len) {\n\t\t\tdev_err(chan2dev(chan), \"No descriptor space\\n\");\n\t\t\treturn NULL;\n\t\t}\n\n\t\terr = td_fill_desc(td_chan, td_desc->desc_list + desc_usage, sg,\n\t\t\ti == (sg_len - 1));\n\t\tif (err) {\n\t\t\tdev_err(chan2dev(chan), \"Failed to update desc: %d\\n\",\n\t\t\t\terr);\n\t\t\ttd_desc_put(td_chan, td_desc);\n\t\t\treturn NULL;\n\t\t}\n\t\tdesc_usage += TIMB_DMA_DESC_SIZE;\n\t}\n\n\tdma_sync_single_for_device(chan2dmadev(chan), td_desc->txd.phys,\n\t\ttd_desc->desc_list_len, DMA_TO_DEVICE);\n\n\treturn &td_desc->txd;\n}\n\nstatic int td_terminate_all(struct dma_chan *chan)\n{\n\tstruct timb_dma_chan *td_chan =\n\t\tcontainer_of(chan, struct timb_dma_chan, chan);\n\tstruct timb_dma_desc *td_desc, *_td_desc;\n\n\tdev_dbg(chan2dev(chan), \"%s: Entry\\n\", __func__);\n\n\t \n\tspin_lock_bh(&td_chan->lock);\n\tlist_for_each_entry_safe(td_desc, _td_desc, &td_chan->queue,\n\t\tdesc_node)\n\t\tlist_move(&td_desc->desc_node, &td_chan->free_list);\n\n\t \n\t__td_finish(td_chan);\n\tspin_unlock_bh(&td_chan->lock);\n\n\treturn 0;\n}\n\nstatic void td_tasklet(struct tasklet_struct *t)\n{\n\tstruct timb_dma *td = from_tasklet(td, t, tasklet);\n\tu32 isr;\n\tu32 ipr;\n\tu32 ier;\n\tint i;\n\n\tisr = ioread32(td->membase + TIMBDMA_ISR);\n\tipr = isr & __td_ier_mask(td);\n\n\t \n\tiowrite32(ipr, td->membase + TIMBDMA_ISR);\n\n\tfor (i = 0; i < td->dma.chancnt; i++)\n\t\tif (ipr & (1 << i)) {\n\t\t\tstruct timb_dma_chan *td_chan = td->channels + i;\n\t\t\tspin_lock(&td_chan->lock);\n\t\t\t__td_finish(td_chan);\n\t\t\tif (!list_empty(&td_chan->queue))\n\t\t\t\t__td_start_next(td_chan);\n\t\t\tspin_unlock(&td_chan->lock);\n\t\t}\n\n\tier = __td_ier_mask(td);\n\tiowrite32(ier, td->membase + TIMBDMA_IER);\n}\n\n\nstatic irqreturn_t td_irq(int irq, void *devid)\n{\n\tstruct timb_dma *td = devid;\n\tu32 ipr = ioread32(td->membase + TIMBDMA_IPR);\n\n\tif (ipr) {\n\t\t \n\t\tiowrite32(0, td->membase + TIMBDMA_IER);\n\n\t\ttasklet_schedule(&td->tasklet);\n\n\t\treturn IRQ_HANDLED;\n\t} else\n\t\treturn IRQ_NONE;\n}\n\n\nstatic int td_probe(struct platform_device *pdev)\n{\n\tstruct timb_dma_platform_data *pdata = dev_get_platdata(&pdev->dev);\n\tstruct timb_dma *td;\n\tstruct resource *iomem;\n\tint irq;\n\tint err;\n\tint i;\n\n\tif (!pdata) {\n\t\tdev_err(&pdev->dev, \"No platform data\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tiomem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!iomem)\n\t\treturn -EINVAL;\n\n\tirq = platform_get_irq(pdev, 0);\n\tif (irq < 0)\n\t\treturn irq;\n\n\tif (!request_mem_region(iomem->start, resource_size(iomem),\n\t\tDRIVER_NAME))\n\t\treturn -EBUSY;\n\n\ttd  = kzalloc(struct_size(td, channels, pdata->nr_channels),\n\t\t      GFP_KERNEL);\n\tif (!td) {\n\t\terr = -ENOMEM;\n\t\tgoto err_release_region;\n\t}\n\n\tdev_dbg(&pdev->dev, \"Allocated TD: %p\\n\", td);\n\n\ttd->membase = ioremap(iomem->start, resource_size(iomem));\n\tif (!td->membase) {\n\t\tdev_err(&pdev->dev, \"Failed to remap I/O memory\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_free_mem;\n\t}\n\n\t \n\tiowrite32(TIMBDMA_32BIT_ADDR, td->membase + TIMBDMA_ACR);\n\n\t \n\tiowrite32(0x0, td->membase + TIMBDMA_IER);\n\tiowrite32(0xFFFFFFFF, td->membase + TIMBDMA_ISR);\n\n\ttasklet_setup(&td->tasklet, td_tasklet);\n\n\terr = request_irq(irq, td_irq, IRQF_SHARED, DRIVER_NAME, td);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to request IRQ\\n\");\n\t\tgoto err_tasklet_kill;\n\t}\n\n\ttd->dma.device_alloc_chan_resources\t= td_alloc_chan_resources;\n\ttd->dma.device_free_chan_resources\t= td_free_chan_resources;\n\ttd->dma.device_tx_status\t\t= td_tx_status;\n\ttd->dma.device_issue_pending\t\t= td_issue_pending;\n\n\tdma_cap_set(DMA_SLAVE, td->dma.cap_mask);\n\tdma_cap_set(DMA_PRIVATE, td->dma.cap_mask);\n\ttd->dma.device_prep_slave_sg = td_prep_slave_sg;\n\ttd->dma.device_terminate_all = td_terminate_all;\n\n\ttd->dma.dev = &pdev->dev;\n\n\tINIT_LIST_HEAD(&td->dma.channels);\n\n\tfor (i = 0; i < pdata->nr_channels; i++) {\n\t\tstruct timb_dma_chan *td_chan = &td->channels[i];\n\t\tstruct timb_dma_platform_data_channel *pchan =\n\t\t\tpdata->channels + i;\n\n\t\t \n\t\tif ((i % 2) == pchan->rx) {\n\t\t\tdev_err(&pdev->dev, \"Wrong channel configuration\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free_irq;\n\t\t}\n\n\t\ttd_chan->chan.device = &td->dma;\n\t\tdma_cookie_init(&td_chan->chan);\n\t\tspin_lock_init(&td_chan->lock);\n\t\tINIT_LIST_HEAD(&td_chan->active_list);\n\t\tINIT_LIST_HEAD(&td_chan->queue);\n\t\tINIT_LIST_HEAD(&td_chan->free_list);\n\n\t\ttd_chan->descs = pchan->descriptors;\n\t\ttd_chan->desc_elems = pchan->descriptor_elements;\n\t\ttd_chan->bytes_per_line = pchan->bytes_per_line;\n\t\ttd_chan->direction = pchan->rx ? DMA_DEV_TO_MEM :\n\t\t\tDMA_MEM_TO_DEV;\n\n\t\ttd_chan->membase = td->membase +\n\t\t\t(i / 2) * TIMBDMA_INSTANCE_OFFSET +\n\t\t\t(pchan->rx ? 0 : TIMBDMA_INSTANCE_TX_OFFSET);\n\n\t\tdev_dbg(&pdev->dev, \"Chan: %d, membase: %p\\n\",\n\t\t\ti, td_chan->membase);\n\n\t\tlist_add_tail(&td_chan->chan.device_node, &td->dma.channels);\n\t}\n\n\terr = dma_async_device_register(&td->dma);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to register async device\\n\");\n\t\tgoto err_free_irq;\n\t}\n\n\tplatform_set_drvdata(pdev, td);\n\n\tdev_dbg(&pdev->dev, \"Probe result: %d\\n\", err);\n\treturn err;\n\nerr_free_irq:\n\tfree_irq(irq, td);\nerr_tasklet_kill:\n\ttasklet_kill(&td->tasklet);\n\tiounmap(td->membase);\nerr_free_mem:\n\tkfree(td);\nerr_release_region:\n\trelease_mem_region(iomem->start, resource_size(iomem));\n\n\treturn err;\n\n}\n\nstatic int td_remove(struct platform_device *pdev)\n{\n\tstruct timb_dma *td = platform_get_drvdata(pdev);\n\tstruct resource *iomem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tint irq = platform_get_irq(pdev, 0);\n\n\tdma_async_device_unregister(&td->dma);\n\tfree_irq(irq, td);\n\ttasklet_kill(&td->tasklet);\n\tiounmap(td->membase);\n\tkfree(td);\n\trelease_mem_region(iomem->start, resource_size(iomem));\n\n\tdev_dbg(&pdev->dev, \"Removed...\\n\");\n\treturn 0;\n}\n\nstatic struct platform_driver td_driver = {\n\t.driver = {\n\t\t.name\t= DRIVER_NAME,\n\t},\n\t.probe\t= td_probe,\n\t.remove\t= td_remove,\n};\n\nmodule_platform_driver(td_driver);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"Timberdale DMA controller driver\");\nMODULE_AUTHOR(\"Pelagicore AB <info@pelagicore.com>\");\nMODULE_ALIAS(\"platform:\"DRIVER_NAME);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}