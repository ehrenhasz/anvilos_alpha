{
  "module_name": "lgm-dma.c",
  "hash_id": "a9f9dc404f7637fd5ff978681020113ef25c30da89a5d46c2c4d9641704f5e19",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/lgm/lgm-dma.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/clk.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/err.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/iopoll.h>\n#include <linux/of_dma.h>\n#include <linux/of_irq.h>\n#include <linux/platform_device.h>\n#include <linux/reset.h>\n\n#include \"../dmaengine.h\"\n#include \"../virt-dma.h\"\n\n#define DRIVER_NAME\t\t\t\"lgm-dma\"\n\n#define DMA_ID\t\t\t\t0x0008\n#define DMA_ID_REV\t\t\tGENMASK(7, 0)\n#define DMA_ID_PNR\t\t\tGENMASK(19, 16)\n#define DMA_ID_CHNR\t\t\tGENMASK(26, 20)\n#define DMA_ID_DW_128B\t\t\tBIT(27)\n#define DMA_ID_AW_36B\t\t\tBIT(28)\n#define DMA_VER32\t\t\t0x32\n#define DMA_VER31\t\t\t0x31\n#define DMA_VER22\t\t\t0x0A\n\n#define DMA_CTRL\t\t\t0x0010\n#define DMA_CTRL_RST\t\t\tBIT(0)\n#define DMA_CTRL_DSRAM_PATH\t\tBIT(1)\n#define DMA_CTRL_DBURST_WR\t\tBIT(3)\n#define DMA_CTRL_VLD_DF_ACK\t\tBIT(4)\n#define DMA_CTRL_CH_FL\t\t\tBIT(6)\n#define DMA_CTRL_DS_FOD\t\t\tBIT(7)\n#define DMA_CTRL_DRB\t\t\tBIT(8)\n#define DMA_CTRL_ENBE\t\t\tBIT(9)\n#define DMA_CTRL_DESC_TMOUT_CNT_V31\tGENMASK(27, 16)\n#define DMA_CTRL_DESC_TMOUT_EN_V31\tBIT(30)\n#define DMA_CTRL_PKTARB\t\t\tBIT(31)\n\n#define DMA_CPOLL\t\t\t0x0014\n#define DMA_CPOLL_CNT\t\t\tGENMASK(15, 4)\n#define DMA_CPOLL_EN\t\t\tBIT(31)\n\n#define DMA_CS\t\t\t\t0x0018\n#define DMA_CS_MASK\t\t\tGENMASK(5, 0)\n\n#define DMA_CCTRL\t\t\t0x001C\n#define DMA_CCTRL_ON\t\t\tBIT(0)\n#define DMA_CCTRL_RST\t\t\tBIT(1)\n#define DMA_CCTRL_CH_POLL_EN\t\tBIT(2)\n#define DMA_CCTRL_CH_ABC\t\tBIT(3)  \n#define DMA_CDBA_MSB\t\t\tGENMASK(7, 4)\n#define DMA_CCTRL_DIR_TX\t\tBIT(8)\n#define DMA_CCTRL_CLASS\t\t\tGENMASK(11, 9)\n#define DMA_CCTRL_CLASSH\t\tGENMASK(19, 18)\n#define DMA_CCTRL_WR_NP_EN\t\tBIT(21)\n#define DMA_CCTRL_PDEN\t\t\tBIT(23)\n#define DMA_MAX_CLASS\t\t\t(SZ_32 - 1)\n\n#define DMA_CDBA\t\t\t0x0020\n#define DMA_CDLEN\t\t\t0x0024\n#define DMA_CIS\t\t\t\t0x0028\n#define DMA_CIE\t\t\t\t0x002C\n#define DMA_CI_EOP\t\t\tBIT(1)\n#define DMA_CI_DUR\t\t\tBIT(2)\n#define DMA_CI_DESCPT\t\t\tBIT(3)\n#define DMA_CI_CHOFF\t\t\tBIT(4)\n#define DMA_CI_RDERR\t\t\tBIT(5)\n#define DMA_CI_ALL\t\t\t\t\t\t\t\\\n\t(DMA_CI_EOP | DMA_CI_DUR | DMA_CI_DESCPT | DMA_CI_CHOFF | DMA_CI_RDERR)\n\n#define DMA_PS\t\t\t\t0x0040\n#define DMA_PCTRL\t\t\t0x0044\n#define DMA_PCTRL_RXBL16\t\tBIT(0)\n#define DMA_PCTRL_TXBL16\t\tBIT(1)\n#define DMA_PCTRL_RXBL\t\t\tGENMASK(3, 2)\n#define DMA_PCTRL_RXBL_8\t\t3\n#define DMA_PCTRL_TXBL\t\t\tGENMASK(5, 4)\n#define DMA_PCTRL_TXBL_8\t\t3\n#define DMA_PCTRL_PDEN\t\t\tBIT(6)\n#define DMA_PCTRL_RXBL32\t\tBIT(7)\n#define DMA_PCTRL_RXENDI\t\tGENMASK(9, 8)\n#define DMA_PCTRL_TXENDI\t\tGENMASK(11, 10)\n#define DMA_PCTRL_TXBL32\t\tBIT(15)\n#define DMA_PCTRL_MEM_FLUSH\t\tBIT(16)\n\n#define DMA_IRNEN1\t\t\t0x00E8\n#define DMA_IRNCR1\t\t\t0x00EC\n#define DMA_IRNEN\t\t\t0x00F4\n#define DMA_IRNCR\t\t\t0x00F8\n#define DMA_C_DP_TICK\t\t\t0x100\n#define DMA_C_DP_TICK_TIKNARB\t\tGENMASK(15, 0)\n#define DMA_C_DP_TICK_TIKARB\t\tGENMASK(31, 16)\n\n#define DMA_C_HDRM\t\t\t0x110\n \n#define DMA_C_HDRM_HDR_SUM\t\tBIT(30)\n\n#define DMA_C_BOFF\t\t\t0x120\n#define DMA_C_BOFF_BOF_LEN\t\tGENMASK(7, 0)\n#define DMA_C_BOFF_EN\t\t\tBIT(31)\n\n#define DMA_ORRC\t\t\t0x190\n#define DMA_ORRC_ORRCNT\t\t\tGENMASK(8, 4)\n#define DMA_ORRC_EN\t\t\tBIT(31)\n\n#define DMA_C_ENDIAN\t\t\t0x200\n#define DMA_C_END_DATAENDI\t\tGENMASK(1, 0)\n#define DMA_C_END_DE_EN\t\t\tBIT(7)\n#define DMA_C_END_DESENDI\t\tGENMASK(9, 8)\n#define DMA_C_END_DES_EN\t\tBIT(16)\n\n \n#define DMA_ADDR_36BIT\t\t\tBIT(0)\n#define DMA_DATA_128BIT\t\t\tBIT(1)\n#define DMA_CHAN_FLOW_CTL\t\tBIT(2)\n#define DMA_DESC_FOD\t\t\tBIT(3)\n#define DMA_DESC_IN_SRAM\t\tBIT(4)\n#define DMA_EN_BYTE_EN\t\t\tBIT(5)\n#define DMA_DBURST_WR\t\t\tBIT(6)\n#define DMA_VALID_DESC_FETCH_ACK\tBIT(7)\n#define DMA_DFT_DRB\t\t\tBIT(8)\n\n#define DMA_ORRC_MAX_CNT\t\t(SZ_32 - 1)\n#define DMA_DFT_POLL_CNT\t\tSZ_4\n#define DMA_DFT_BURST_V22\t\tSZ_2\n#define DMA_BURSTL_8DW\t\t\tSZ_8\n#define DMA_BURSTL_16DW\t\t\tSZ_16\n#define DMA_BURSTL_32DW\t\t\tSZ_32\n#define DMA_DFT_BURST\t\t\tDMA_BURSTL_16DW\n#define DMA_MAX_DESC_NUM\t\t(SZ_8K - 1)\n#define DMA_CHAN_BOFF_MAX\t\t(SZ_256 - 1)\n#define DMA_DFT_ENDIAN\t\t\t0\n\n#define DMA_DFT_DESC_TCNT\t\t50\n#define DMA_HDR_LEN_MAX\t\t\t(SZ_16K - 1)\n\n \n#define DMA_TX_CH\t\t\tBIT(0)\n#define DMA_RX_CH\t\t\tBIT(1)\n#define DEVICE_ALLOC_DESC\t\tBIT(2)\n#define CHAN_IN_USE\t\t\tBIT(3)\n#define DMA_HW_DESC\t\t\tBIT(4)\n\n \n#define DESC_DATA_LEN\t\t\tGENMASK(15, 0)\n#define DESC_BYTE_OFF\t\t\tGENMASK(25, 23)\n#define DESC_EOP\t\t\tBIT(28)\n#define DESC_SOP\t\t\tBIT(29)\n#define DESC_C\t\t\t\tBIT(30)\n#define DESC_OWN\t\t\tBIT(31)\n\n#define DMA_CHAN_RST\t\t\t1\n#define DMA_MAX_SIZE\t\t\t(BIT(16) - 1)\n#define MAX_LOWER_CHANS\t\t\t32\n#define MASK_LOWER_CHANS\t\tGENMASK(4, 0)\n#define DMA_OWN\t\t\t\t1\n#define HIGH_4_BITS\t\t\tGENMASK(3, 0)\n#define DMA_DFT_DESC_NUM\t\t1\n#define DMA_PKT_DROP_DIS\t\t0\n\nenum ldma_chan_on_off {\n\tDMA_CH_OFF = 0,\n\tDMA_CH_ON = 1,\n};\n\nenum {\n\tDMA_TYPE_TX = 0,\n\tDMA_TYPE_RX,\n\tDMA_TYPE_MCPY,\n};\n\nstruct ldma_dev;\nstruct ldma_port;\n\nstruct ldma_chan {\n\tstruct virt_dma_chan\tvchan;\n\tstruct ldma_port\t*port;  \n\tchar\t\t\tname[8];  \n\tint\t\t\tnr;  \n\tu32\t\t\tflags;  \n\tenum ldma_chan_on_off\tonoff;\n\tdma_addr_t\t\tdesc_phys;\n\tvoid\t\t\t*desc_base;  \n\tu32\t\t\tdesc_cnt;  \n\tint\t\t\trst;\n\tu32\t\t\thdrm_len;\n\tbool\t\t\thdrm_csum;\n\tu32\t\t\tboff_len;\n\tu32\t\t\tdata_endian;\n\tu32\t\t\tdesc_endian;\n\tbool\t\t\tpden;\n\tbool\t\t\tdesc_rx_np;\n\tbool\t\t\tdata_endian_en;\n\tbool\t\t\tdesc_endian_en;\n\tbool\t\t\tabc_en;\n\tbool\t\t\tdesc_init;\n\tstruct dma_pool\t\t*desc_pool;  \n\tu32\t\t\tdesc_num;\n\tstruct dw2_desc_sw\t*ds;\n\tstruct work_struct\twork;\n\tstruct dma_slave_config config;\n};\n\nstruct ldma_port {\n\tstruct ldma_dev\t\t*ldev;  \n\tu32\t\t\tportid;\n\tu32\t\t\trxbl;\n\tu32\t\t\ttxbl;\n\tu32\t\t\trxendi;\n\tu32\t\t\ttxendi;\n\tu32\t\t\tpkt_drop;\n};\n\n \nstruct ldma_inst_data {\n\tbool\t\t\tdesc_in_sram;\n\tbool\t\t\tchan_fc;\n\tbool\t\t\tdesc_fod;  \n\tbool\t\t\tvalid_desc_fetch_ack;\n\tu32\t\t\torrc;  \n\tconst char\t\t*name;\n\tu32\t\t\ttype;\n};\n\nstruct ldma_dev {\n\tstruct device\t\t*dev;\n\tvoid __iomem\t\t*base;\n\tstruct reset_control\t*rst;\n\tstruct clk\t\t*core_clk;\n\tstruct dma_device\tdma_dev;\n\tu32\t\t\tver;\n\tint\t\t\tirq;\n\tstruct ldma_port\t*ports;\n\tstruct ldma_chan\t*chans;  \n\tspinlock_t\t\tdev_lock;  \n\tu32\t\t\tchan_nrs;\n\tu32\t\t\tport_nrs;\n\tu32\t\t\tchannels_mask;\n\tu32\t\t\tflags;\n\tu32\t\t\tpollcnt;\n\tconst struct ldma_inst_data *inst;\n\tstruct workqueue_struct\t*wq;\n};\n\nstruct dw2_desc {\n\tu32 field;\n\tu32 addr;\n} __packed __aligned(8);\n\nstruct dw2_desc_sw {\n\tstruct virt_dma_desc\tvdesc;\n\tstruct ldma_chan\t*chan;\n\tdma_addr_t\t\tdesc_phys;\n\tsize_t\t\t\tdesc_cnt;\n\tsize_t\t\t\tsize;\n\tstruct dw2_desc\t\t*desc_hw;\n};\n\nstatic inline void\nldma_update_bits(struct ldma_dev *d, u32 mask, u32 val, u32 ofs)\n{\n\tu32 old_val, new_val;\n\n\told_val = readl(d->base +  ofs);\n\tnew_val = (old_val & ~mask) | (val & mask);\n\n\tif (new_val != old_val)\n\t\twritel(new_val, d->base + ofs);\n}\n\nstatic inline struct ldma_chan *to_ldma_chan(struct dma_chan *chan)\n{\n\treturn container_of(chan, struct ldma_chan, vchan.chan);\n}\n\nstatic inline struct ldma_dev *to_ldma_dev(struct dma_device *dma_dev)\n{\n\treturn container_of(dma_dev, struct ldma_dev, dma_dev);\n}\n\nstatic inline struct dw2_desc_sw *to_lgm_dma_desc(struct virt_dma_desc *vdesc)\n{\n\treturn container_of(vdesc, struct dw2_desc_sw, vdesc);\n}\n\nstatic inline bool ldma_chan_tx(struct ldma_chan *c)\n{\n\treturn !!(c->flags & DMA_TX_CH);\n}\n\nstatic inline bool ldma_chan_is_hw_desc(struct ldma_chan *c)\n{\n\treturn !!(c->flags & DMA_HW_DESC);\n}\n\nstatic void ldma_dev_reset(struct ldma_dev *d)\n\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, DMA_CTRL_RST, DMA_CTRL_RST, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_pkt_arb_cfg(struct ldma_dev *d, bool enable)\n{\n\tunsigned long flags;\n\tu32 mask = DMA_CTRL_PKTARB;\n\tu32 val = enable ? DMA_CTRL_PKTARB : 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_sram_desc_cfg(struct ldma_dev *d, bool enable)\n{\n\tunsigned long flags;\n\tu32 mask = DMA_CTRL_DSRAM_PATH;\n\tu32 val = enable ? DMA_CTRL_DSRAM_PATH : 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_chan_flow_ctl_cfg(struct ldma_dev *d, bool enable)\n{\n\tunsigned long flags;\n\tu32 mask, val;\n\n\tif (d->inst->type != DMA_TYPE_TX)\n\t\treturn;\n\n\tmask = DMA_CTRL_CH_FL;\n\tval = enable ? DMA_CTRL_CH_FL : 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_global_polling_enable(struct ldma_dev *d)\n{\n\tunsigned long flags;\n\tu32 mask = DMA_CPOLL_EN | DMA_CPOLL_CNT;\n\tu32 val = DMA_CPOLL_EN;\n\n\tval |= FIELD_PREP(DMA_CPOLL_CNT, d->pollcnt);\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CPOLL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_desc_fetch_on_demand_cfg(struct ldma_dev *d, bool enable)\n{\n\tunsigned long flags;\n\tu32 mask, val;\n\n\tif (d->inst->type == DMA_TYPE_MCPY)\n\t\treturn;\n\n\tmask = DMA_CTRL_DS_FOD;\n\tval = enable ? DMA_CTRL_DS_FOD : 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_byte_enable_cfg(struct ldma_dev *d, bool enable)\n{\n\tunsigned long flags;\n\tu32 mask = DMA_CTRL_ENBE;\n\tu32 val = enable ? DMA_CTRL_ENBE : 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_orrc_cfg(struct ldma_dev *d)\n{\n\tunsigned long flags;\n\tu32 val = 0;\n\tu32 mask;\n\n\tif (d->inst->type == DMA_TYPE_RX)\n\t\treturn;\n\n\tmask = DMA_ORRC_EN | DMA_ORRC_ORRCNT;\n\tif (d->inst->orrc > 0 && d->inst->orrc <= DMA_ORRC_MAX_CNT)\n\t\tval = DMA_ORRC_EN | FIELD_PREP(DMA_ORRC_ORRCNT, d->inst->orrc);\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_ORRC);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_df_tout_cfg(struct ldma_dev *d, bool enable, int tcnt)\n{\n\tu32 mask = DMA_CTRL_DESC_TMOUT_CNT_V31;\n\tunsigned long flags;\n\tu32 val;\n\n\tif (enable)\n\t\tval = DMA_CTRL_DESC_TMOUT_EN_V31 | FIELD_PREP(DMA_CTRL_DESC_TMOUT_CNT_V31, tcnt);\n\telse\n\t\tval = 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_dburst_wr_cfg(struct ldma_dev *d, bool enable)\n{\n\tunsigned long flags;\n\tu32 mask, val;\n\n\tif (d->inst->type != DMA_TYPE_RX && d->inst->type != DMA_TYPE_MCPY)\n\t\treturn;\n\n\tmask = DMA_CTRL_DBURST_WR;\n\tval = enable ? DMA_CTRL_DBURST_WR : 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_vld_fetch_ack_cfg(struct ldma_dev *d, bool enable)\n{\n\tunsigned long flags;\n\tu32 mask, val;\n\n\tif (d->inst->type != DMA_TYPE_TX)\n\t\treturn;\n\n\tmask = DMA_CTRL_VLD_DF_ACK;\n\tval = enable ? DMA_CTRL_VLD_DF_ACK : 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_dev_drb_cfg(struct ldma_dev *d, int enable)\n{\n\tunsigned long flags;\n\tu32 mask = DMA_CTRL_DRB;\n\tu32 val = enable ? DMA_CTRL_DRB : 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, mask, val, DMA_CTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic int ldma_dev_cfg(struct ldma_dev *d)\n{\n\tbool enable;\n\n\tldma_dev_pkt_arb_cfg(d, true);\n\tldma_dev_global_polling_enable(d);\n\n\tenable = !!(d->flags & DMA_DFT_DRB);\n\tldma_dev_drb_cfg(d, enable);\n\n\tenable = !!(d->flags & DMA_EN_BYTE_EN);\n\tldma_dev_byte_enable_cfg(d, enable);\n\n\tenable = !!(d->flags & DMA_CHAN_FLOW_CTL);\n\tldma_dev_chan_flow_ctl_cfg(d, enable);\n\n\tenable = !!(d->flags & DMA_DESC_FOD);\n\tldma_dev_desc_fetch_on_demand_cfg(d, enable);\n\n\tenable = !!(d->flags & DMA_DESC_IN_SRAM);\n\tldma_dev_sram_desc_cfg(d, enable);\n\n\tenable = !!(d->flags & DMA_DBURST_WR);\n\tldma_dev_dburst_wr_cfg(d, enable);\n\n\tenable = !!(d->flags & DMA_VALID_DESC_FETCH_ACK);\n\tldma_dev_vld_fetch_ack_cfg(d, enable);\n\n\tif (d->ver > DMA_VER22) {\n\t\tldma_dev_orrc_cfg(d);\n\t\tldma_dev_df_tout_cfg(d, true, DMA_DFT_DESC_TCNT);\n\t}\n\n\tdev_dbg(d->dev, \"%s Controller 0x%08x configuration done\\n\",\n\t\td->inst->name, readl(d->base + DMA_CTRL));\n\n\treturn 0;\n}\n\nstatic int ldma_chan_cctrl_cfg(struct ldma_chan *c, u32 val)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tu32 class_low, class_high;\n\tunsigned long flags;\n\tu32 reg;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\treg = readl(d->base + DMA_CCTRL);\n\t \n\tif (reg & DMA_CCTRL_DIR_TX)\n\t\tc->flags |= DMA_TX_CH;\n\telse\n\t\tc->flags |= DMA_RX_CH;\n\n\t \n\tclass_low = FIELD_GET(DMA_CCTRL_CLASS, reg);\n\tclass_high = FIELD_GET(DMA_CCTRL_CLASSH, reg);\n\tval &= ~DMA_CCTRL_CLASS;\n\tval |= FIELD_PREP(DMA_CCTRL_CLASS, class_low);\n\tval &= ~DMA_CCTRL_CLASSH;\n\tval |= FIELD_PREP(DMA_CCTRL_CLASSH, class_high);\n\twritel(val, d->base + DMA_CCTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n\n\treturn 0;\n}\n\nstatic void ldma_chan_irq_init(struct ldma_chan *c)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tunsigned long flags;\n\tu32 enofs, crofs;\n\tu32 cn_bit;\n\n\tif (c->nr < MAX_LOWER_CHANS) {\n\t\tenofs = DMA_IRNEN;\n\t\tcrofs = DMA_IRNCR;\n\t} else {\n\t\tenofs = DMA_IRNEN1;\n\t\tcrofs = DMA_IRNCR1;\n\t}\n\n\tcn_bit = BIT(c->nr & MASK_LOWER_CHANS);\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\n\t \n\twritel(0, d->base + DMA_CIE);\n\twritel(DMA_CI_ALL, d->base + DMA_CIS);\n\n\tldma_update_bits(d, cn_bit, 0, enofs);\n\twritel(cn_bit, d->base + crofs);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_chan_set_class(struct ldma_chan *c, u32 val)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tu32 class_val;\n\n\tif (d->inst->type == DMA_TYPE_MCPY || val > DMA_MAX_CLASS)\n\t\treturn;\n\n\t \n\tclass_val = FIELD_PREP(DMA_CCTRL_CLASS, val & 0x7);\n\t \n\tclass_val |= FIELD_PREP(DMA_CCTRL_CLASSH, (val >> 3) & 0x3);\n\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, DMA_CCTRL_CLASS | DMA_CCTRL_CLASSH, class_val,\n\t\t\t DMA_CCTRL);\n}\n\nstatic int ldma_chan_on(struct ldma_chan *c)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tunsigned long flags;\n\n\t \n\tif (WARN_ON(!c->desc_init))\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, DMA_CCTRL_ON, DMA_CCTRL_ON, DMA_CCTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n\n\tc->onoff = DMA_CH_ON;\n\n\treturn 0;\n}\n\nstatic int ldma_chan_off(struct ldma_chan *c)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tunsigned long flags;\n\tu32 val;\n\tint ret;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, DMA_CCTRL_ON, 0, DMA_CCTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n\n\tret = readl_poll_timeout_atomic(d->base + DMA_CCTRL, val,\n\t\t\t\t\t!(val & DMA_CCTRL_ON), 0, 10000);\n\tif (ret)\n\t\treturn ret;\n\n\tc->onoff = DMA_CH_OFF;\n\n\treturn 0;\n}\n\nstatic void ldma_chan_desc_hw_cfg(struct ldma_chan *c, dma_addr_t desc_base,\n\t\t\t\t  int desc_num)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\twritel(lower_32_bits(desc_base), d->base + DMA_CDBA);\n\n\t \n\tif (IS_ENABLED(CONFIG_64BIT)) {\n\t\tu32 hi = upper_32_bits(desc_base) & HIGH_4_BITS;\n\n\t\tldma_update_bits(d, DMA_CDBA_MSB,\n\t\t\t\t FIELD_PREP(DMA_CDBA_MSB, hi), DMA_CCTRL);\n\t}\n\twritel(desc_num, d->base + DMA_CDLEN);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n\n\tc->desc_init = true;\n}\n\nstatic struct dma_async_tx_descriptor *\nldma_chan_desc_cfg(struct dma_chan *chan, dma_addr_t desc_base, int desc_num)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct dw2_desc_sw *ds;\n\n\tif (!desc_num) {\n\t\tdev_err(d->dev, \"Channel %d must allocate descriptor first\\n\",\n\t\t\tc->nr);\n\t\treturn NULL;\n\t}\n\n\tif (desc_num > DMA_MAX_DESC_NUM) {\n\t\tdev_err(d->dev, \"Channel %d descriptor number out of range %d\\n\",\n\t\t\tc->nr, desc_num);\n\t\treturn NULL;\n\t}\n\n\tldma_chan_desc_hw_cfg(c, desc_base, desc_num);\n\n\tc->flags |= DMA_HW_DESC;\n\tc->desc_cnt = desc_num;\n\tc->desc_phys = desc_base;\n\n\tds = kzalloc(sizeof(*ds), GFP_NOWAIT);\n\tif (!ds)\n\t\treturn NULL;\n\n\ttx = &ds->vdesc.tx;\n\tdma_async_tx_descriptor_init(tx, chan);\n\n\treturn tx;\n}\n\nstatic int ldma_chan_reset(struct ldma_chan *c)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tunsigned long flags;\n\tu32 val;\n\tint ret;\n\n\tret = ldma_chan_off(c);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, DMA_CCTRL_RST, DMA_CCTRL_RST, DMA_CCTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n\n\tret = readl_poll_timeout_atomic(d->base + DMA_CCTRL, val,\n\t\t\t\t\t!(val & DMA_CCTRL_RST), 0, 10000);\n\tif (ret)\n\t\treturn ret;\n\n\tc->rst = 1;\n\tc->desc_init = false;\n\n\treturn 0;\n}\n\nstatic void ldma_chan_byte_offset_cfg(struct ldma_chan *c, u32 boff_len)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tu32 mask = DMA_C_BOFF_EN | DMA_C_BOFF_BOF_LEN;\n\tu32 val;\n\n\tif (boff_len > 0 && boff_len <= DMA_CHAN_BOFF_MAX)\n\t\tval = FIELD_PREP(DMA_C_BOFF_BOF_LEN, boff_len) | DMA_C_BOFF_EN;\n\telse\n\t\tval = 0;\n\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, mask, val, DMA_C_BOFF);\n}\n\nstatic void ldma_chan_data_endian_cfg(struct ldma_chan *c, bool enable,\n\t\t\t\t      u32 endian_type)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tu32 mask = DMA_C_END_DE_EN | DMA_C_END_DATAENDI;\n\tu32 val;\n\n\tif (enable)\n\t\tval = DMA_C_END_DE_EN | FIELD_PREP(DMA_C_END_DATAENDI, endian_type);\n\telse\n\t\tval = 0;\n\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, mask, val, DMA_C_ENDIAN);\n}\n\nstatic void ldma_chan_desc_endian_cfg(struct ldma_chan *c, bool enable,\n\t\t\t\t      u32 endian_type)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tu32 mask = DMA_C_END_DES_EN | DMA_C_END_DESENDI;\n\tu32 val;\n\n\tif (enable)\n\t\tval = DMA_C_END_DES_EN | FIELD_PREP(DMA_C_END_DESENDI, endian_type);\n\telse\n\t\tval = 0;\n\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, mask, val, DMA_C_ENDIAN);\n}\n\nstatic void ldma_chan_hdr_mode_cfg(struct ldma_chan *c, u32 hdr_len, bool csum)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tu32 mask, val;\n\n\t \n\tif (!csum && (!hdr_len || hdr_len > DMA_HDR_LEN_MAX))\n\t\treturn;\n\n\tmask = DMA_C_HDRM_HDR_SUM;\n\tval = DMA_C_HDRM_HDR_SUM;\n\n\tif (!csum && hdr_len)\n\t\tval = hdr_len;\n\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, mask, val, DMA_C_HDRM);\n}\n\nstatic void ldma_chan_rxwr_np_cfg(struct ldma_chan *c, bool enable)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tu32 mask, val;\n\n\t \n\tif (ldma_chan_tx(c))\n\t\treturn;\n\n\tmask = DMA_CCTRL_WR_NP_EN;\n\tval = enable ? DMA_CCTRL_WR_NP_EN : 0;\n\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, mask, val, DMA_CCTRL);\n}\n\nstatic void ldma_chan_abc_cfg(struct ldma_chan *c, bool enable)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tu32 mask, val;\n\n\tif (d->ver < DMA_VER32 || ldma_chan_tx(c))\n\t\treturn;\n\n\tmask = DMA_CCTRL_CH_ABC;\n\tval = enable ? DMA_CCTRL_CH_ABC : 0;\n\n\tldma_update_bits(d, DMA_CS_MASK, c->nr, DMA_CS);\n\tldma_update_bits(d, mask, val, DMA_CCTRL);\n}\n\nstatic int ldma_port_cfg(struct ldma_port *p)\n{\n\tunsigned long flags;\n\tstruct ldma_dev *d;\n\tu32 reg;\n\n\td = p->ldev;\n\treg = FIELD_PREP(DMA_PCTRL_TXENDI, p->txendi);\n\treg |= FIELD_PREP(DMA_PCTRL_RXENDI, p->rxendi);\n\n\tif (d->ver == DMA_VER22) {\n\t\treg |= FIELD_PREP(DMA_PCTRL_TXBL, p->txbl);\n\t\treg |= FIELD_PREP(DMA_PCTRL_RXBL, p->rxbl);\n\t} else {\n\t\treg |= FIELD_PREP(DMA_PCTRL_PDEN, p->pkt_drop);\n\n\t\tif (p->txbl == DMA_BURSTL_32DW)\n\t\t\treg |= DMA_PCTRL_TXBL32;\n\t\telse if (p->txbl == DMA_BURSTL_16DW)\n\t\t\treg |= DMA_PCTRL_TXBL16;\n\t\telse\n\t\t\treg |= FIELD_PREP(DMA_PCTRL_TXBL, DMA_PCTRL_TXBL_8);\n\n\t\tif (p->rxbl == DMA_BURSTL_32DW)\n\t\t\treg |= DMA_PCTRL_RXBL32;\n\t\telse if (p->rxbl == DMA_BURSTL_16DW)\n\t\t\treg |= DMA_PCTRL_RXBL16;\n\t\telse\n\t\t\treg |= FIELD_PREP(DMA_PCTRL_RXBL, DMA_PCTRL_RXBL_8);\n\t}\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\twritel(p->portid, d->base + DMA_PS);\n\twritel(reg, d->base + DMA_PCTRL);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n\n\treg = readl(d->base + DMA_PCTRL);  \n\tdev_dbg(d->dev, \"Port Control 0x%08x configuration done\\n\", reg);\n\n\treturn 0;\n}\n\nstatic int ldma_chan_cfg(struct ldma_chan *c)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tunsigned long flags;\n\tu32 reg;\n\n\treg = c->pden ? DMA_CCTRL_PDEN : 0;\n\treg |= c->onoff ? DMA_CCTRL_ON : 0;\n\treg |= c->rst ? DMA_CCTRL_RST : 0;\n\n\tldma_chan_cctrl_cfg(c, reg);\n\tldma_chan_irq_init(c);\n\n\tif (d->ver <= DMA_VER22)\n\t\treturn 0;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\tldma_chan_set_class(c, c->nr);\n\tldma_chan_byte_offset_cfg(c, c->boff_len);\n\tldma_chan_data_endian_cfg(c, c->data_endian_en, c->data_endian);\n\tldma_chan_desc_endian_cfg(c, c->desc_endian_en, c->desc_endian);\n\tldma_chan_hdr_mode_cfg(c, c->hdrm_len, c->hdrm_csum);\n\tldma_chan_rxwr_np_cfg(c, c->desc_rx_np);\n\tldma_chan_abc_cfg(c, c->abc_en);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n\n\tif (ldma_chan_is_hw_desc(c))\n\t\tldma_chan_desc_hw_cfg(c, c->desc_phys, c->desc_cnt);\n\n\treturn 0;\n}\n\nstatic void ldma_dev_init(struct ldma_dev *d)\n{\n\tunsigned long ch_mask = (unsigned long)d->channels_mask;\n\tstruct ldma_port *p;\n\tstruct ldma_chan *c;\n\tint i;\n\tu32 j;\n\n\tspin_lock_init(&d->dev_lock);\n\tldma_dev_reset(d);\n\tldma_dev_cfg(d);\n\n\t \n\tfor (i = 0; i < d->port_nrs; i++) {\n\t\tp = &d->ports[i];\n\t\tldma_port_cfg(p);\n\t}\n\n\t \n\tfor_each_set_bit(j, &ch_mask, d->chan_nrs) {\n\t\tc = &d->chans[j];\n\t\tldma_chan_cfg(c);\n\t}\n}\n\nstatic int ldma_parse_dt(struct ldma_dev *d)\n{\n\tstruct fwnode_handle *fwnode = dev_fwnode(d->dev);\n\tstruct ldma_port *p;\n\tint i;\n\n\tif (fwnode_property_read_bool(fwnode, \"intel,dma-byte-en\"))\n\t\td->flags |= DMA_EN_BYTE_EN;\n\n\tif (fwnode_property_read_bool(fwnode, \"intel,dma-dburst-wr\"))\n\t\td->flags |= DMA_DBURST_WR;\n\n\tif (fwnode_property_read_bool(fwnode, \"intel,dma-drb\"))\n\t\td->flags |= DMA_DFT_DRB;\n\n\tif (fwnode_property_read_u32(fwnode, \"intel,dma-poll-cnt\",\n\t\t\t\t     &d->pollcnt))\n\t\td->pollcnt = DMA_DFT_POLL_CNT;\n\n\tif (d->inst->chan_fc)\n\t\td->flags |= DMA_CHAN_FLOW_CTL;\n\n\tif (d->inst->desc_fod)\n\t\td->flags |= DMA_DESC_FOD;\n\n\tif (d->inst->desc_in_sram)\n\t\td->flags |= DMA_DESC_IN_SRAM;\n\n\tif (d->inst->valid_desc_fetch_ack)\n\t\td->flags |= DMA_VALID_DESC_FETCH_ACK;\n\n\tif (d->ver > DMA_VER22) {\n\t\tif (!d->port_nrs)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < d->port_nrs; i++) {\n\t\t\tp = &d->ports[i];\n\t\t\tp->rxendi = DMA_DFT_ENDIAN;\n\t\t\tp->txendi = DMA_DFT_ENDIAN;\n\t\t\tp->rxbl = DMA_DFT_BURST;\n\t\t\tp->txbl = DMA_DFT_BURST;\n\t\t\tp->pkt_drop = DMA_PKT_DROP_DIS;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void dma_free_desc_resource(struct virt_dma_desc *vdesc)\n{\n\tstruct dw2_desc_sw *ds = to_lgm_dma_desc(vdesc);\n\tstruct ldma_chan *c = ds->chan;\n\n\tdma_pool_free(c->desc_pool, ds->desc_hw, ds->desc_phys);\n\tkfree(ds);\n}\n\nstatic struct dw2_desc_sw *\ndma_alloc_desc_resource(int num, struct ldma_chan *c)\n{\n\tstruct device *dev = c->vchan.chan.device->dev;\n\tstruct dw2_desc_sw *ds;\n\n\tif (num > c->desc_num) {\n\t\tdev_err(dev, \"sg num %d exceed max %d\\n\", num, c->desc_num);\n\t\treturn NULL;\n\t}\n\n\tds = kzalloc(sizeof(*ds), GFP_NOWAIT);\n\tif (!ds)\n\t\treturn NULL;\n\n\tds->chan = c;\n\tds->desc_hw = dma_pool_zalloc(c->desc_pool, GFP_ATOMIC,\n\t\t\t\t      &ds->desc_phys);\n\tif (!ds->desc_hw) {\n\t\tdev_dbg(dev, \"out of memory for link descriptor\\n\");\n\t\tkfree(ds);\n\t\treturn NULL;\n\t}\n\tds->desc_cnt = num;\n\n\treturn ds;\n}\n\nstatic void ldma_chan_irq_en(struct ldma_chan *c)\n{\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&d->dev_lock, flags);\n\twritel(c->nr, d->base + DMA_CS);\n\twritel(DMA_CI_EOP, d->base + DMA_CIE);\n\twritel(BIT(c->nr), d->base + DMA_IRNEN);\n\tspin_unlock_irqrestore(&d->dev_lock, flags);\n}\n\nstatic void ldma_issue_pending(struct dma_chan *chan)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tunsigned long flags;\n\n\tif (d->ver == DMA_VER22) {\n\t\tspin_lock_irqsave(&c->vchan.lock, flags);\n\t\tif (vchan_issue_pending(&c->vchan)) {\n\t\t\tstruct virt_dma_desc *vdesc;\n\n\t\t\t \n\t\t\tvdesc = vchan_next_desc(&c->vchan);\n\t\t\tif (!vdesc) {\n\t\t\t\tc->ds = NULL;\n\t\t\t\tspin_unlock_irqrestore(&c->vchan.lock, flags);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tlist_del(&vdesc->node);\n\t\t\tc->ds = to_lgm_dma_desc(vdesc);\n\t\t\tldma_chan_desc_hw_cfg(c, c->ds->desc_phys, c->ds->desc_cnt);\n\t\t\tldma_chan_irq_en(c);\n\t\t}\n\t\tspin_unlock_irqrestore(&c->vchan.lock, flags);\n\t}\n\tldma_chan_on(c);\n}\n\nstatic void ldma_synchronize(struct dma_chan *chan)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\n\t \n\tcancel_work_sync(&c->work);\n\tvchan_synchronize(&c->vchan);\n\tif (c->ds)\n\t\tdma_free_desc_resource(&c->ds->vdesc);\n}\n\nstatic int ldma_terminate_all(struct dma_chan *chan)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&c->vchan.lock, flags);\n\tvchan_get_all_descriptors(&c->vchan, &head);\n\tspin_unlock_irqrestore(&c->vchan.lock, flags);\n\tvchan_dma_desc_free_list(&c->vchan, &head);\n\n\treturn ldma_chan_reset(c);\n}\n\nstatic int ldma_resume_chan(struct dma_chan *chan)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\n\tldma_chan_on(c);\n\n\treturn 0;\n}\n\nstatic int ldma_pause_chan(struct dma_chan *chan)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\n\treturn ldma_chan_off(c);\n}\n\nstatic enum dma_status\nldma_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\n\t       struct dma_tx_state *txstate)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tenum dma_status status = DMA_COMPLETE;\n\n\tif (d->ver == DMA_VER22)\n\t\tstatus = dma_cookie_status(chan, cookie, txstate);\n\n\treturn status;\n}\n\nstatic void dma_chan_irq(int irq, void *data)\n{\n\tstruct ldma_chan *c = data;\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tu32 stat;\n\n\t \n\twritel(c->nr, d->base + DMA_CS);\n\tstat = readl(d->base + DMA_CIS);\n\tif (!stat)\n\t\treturn;\n\n\twritel(readl(d->base + DMA_CIE) & ~DMA_CI_ALL, d->base + DMA_CIE);\n\twritel(stat, d->base + DMA_CIS);\n\tqueue_work(d->wq, &c->work);\n}\n\nstatic irqreturn_t dma_interrupt(int irq, void *dev_id)\n{\n\tstruct ldma_dev *d = dev_id;\n\tstruct ldma_chan *c;\n\tunsigned long irncr;\n\tu32 cid;\n\n\tirncr = readl(d->base + DMA_IRNCR);\n\tif (!irncr) {\n\t\tdev_err(d->dev, \"dummy interrupt\\n\");\n\t\treturn IRQ_NONE;\n\t}\n\n\tfor_each_set_bit(cid, &irncr, d->chan_nrs) {\n\t\t \n\t\twritel(readl(d->base + DMA_IRNEN) & ~BIT(cid), d->base + DMA_IRNEN);\n\t\t \n\t\twritel(readl(d->base + DMA_IRNCR) | BIT(cid), d->base + DMA_IRNCR);\n\n\t\tc = &d->chans[cid];\n\t\tdma_chan_irq(irq, c);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void prep_slave_burst_len(struct ldma_chan *c)\n{\n\tstruct ldma_port *p = c->port;\n\tstruct dma_slave_config *cfg = &c->config;\n\n\tif (cfg->dst_maxburst)\n\t\tcfg->src_maxburst = cfg->dst_maxburst;\n\n\t \n\tp->txbl = ilog2(cfg->src_maxburst);\n\tp->rxbl = p->txbl;\n}\n\nstatic struct dma_async_tx_descriptor *\nldma_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\n\t\t   unsigned int sglen, enum dma_transfer_direction dir,\n\t\t   unsigned long flags, void *context)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tsize_t len, avail, total = 0;\n\tstruct dw2_desc *hw_ds;\n\tstruct dw2_desc_sw *ds;\n\tstruct scatterlist *sg;\n\tint num = sglen, i;\n\tdma_addr_t addr;\n\n\tif (!sgl)\n\t\treturn NULL;\n\n\tif (d->ver > DMA_VER22)\n\t\treturn ldma_chan_desc_cfg(chan, sgl->dma_address, sglen);\n\n\tfor_each_sg(sgl, sg, sglen, i) {\n\t\tavail = sg_dma_len(sg);\n\t\tif (avail > DMA_MAX_SIZE)\n\t\t\tnum += DIV_ROUND_UP(avail, DMA_MAX_SIZE) - 1;\n\t}\n\n\tds = dma_alloc_desc_resource(num, c);\n\tif (!ds)\n\t\treturn NULL;\n\n\tc->ds = ds;\n\n\tnum = 0;\n\t \n\tfor_each_sg(sgl, sg, sglen, i) {\n\t\taddr = sg_dma_address(sg);\n\t\tavail = sg_dma_len(sg);\n\t\ttotal += avail;\n\n\t\tdo {\n\t\t\tlen = min_t(size_t, avail, DMA_MAX_SIZE);\n\n\t\t\thw_ds = &ds->desc_hw[num];\n\t\t\tswitch (sglen) {\n\t\t\tcase 1:\n\t\t\t\thw_ds->field &= ~DESC_SOP;\n\t\t\t\thw_ds->field |= FIELD_PREP(DESC_SOP, 1);\n\n\t\t\t\thw_ds->field &= ~DESC_EOP;\n\t\t\t\thw_ds->field |= FIELD_PREP(DESC_EOP, 1);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tif (num == 0) {\n\t\t\t\t\thw_ds->field &= ~DESC_SOP;\n\t\t\t\t\thw_ds->field |= FIELD_PREP(DESC_SOP, 1);\n\n\t\t\t\t\thw_ds->field &= ~DESC_EOP;\n\t\t\t\t\thw_ds->field |= FIELD_PREP(DESC_EOP, 0);\n\t\t\t\t} else if (num == (sglen - 1)) {\n\t\t\t\t\thw_ds->field &= ~DESC_SOP;\n\t\t\t\t\thw_ds->field |= FIELD_PREP(DESC_SOP, 0);\n\t\t\t\t\thw_ds->field &= ~DESC_EOP;\n\t\t\t\t\thw_ds->field |= FIELD_PREP(DESC_EOP, 1);\n\t\t\t\t} else {\n\t\t\t\t\thw_ds->field &= ~DESC_SOP;\n\t\t\t\t\thw_ds->field |= FIELD_PREP(DESC_SOP, 0);\n\n\t\t\t\t\thw_ds->field &= ~DESC_EOP;\n\t\t\t\t\thw_ds->field |= FIELD_PREP(DESC_EOP, 0);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t \n\t\t\thw_ds->addr = (u32)addr;\n\n\t\t\thw_ds->field &= ~DESC_DATA_LEN;\n\t\t\thw_ds->field |= FIELD_PREP(DESC_DATA_LEN, len);\n\n\t\t\thw_ds->field &= ~DESC_C;\n\t\t\thw_ds->field |= FIELD_PREP(DESC_C, 0);\n\n\t\t\thw_ds->field &= ~DESC_BYTE_OFF;\n\t\t\thw_ds->field |= FIELD_PREP(DESC_BYTE_OFF, addr & 0x3);\n\n\t\t\t \n\t\t\twmb();\n\t\t\thw_ds->field &= ~DESC_OWN;\n\t\t\thw_ds->field |= FIELD_PREP(DESC_OWN, DMA_OWN);\n\n\t\t\t \n\t\t\twmb();\n\t\t\tnum++;\n\t\t\taddr += len;\n\t\t\tavail -= len;\n\t\t} while (avail);\n\t}\n\n\tds->size = total;\n\tprep_slave_burst_len(c);\n\n\treturn vchan_tx_prep(&c->vchan, &ds->vdesc, DMA_CTRL_ACK);\n}\n\nstatic int\nldma_slave_config(struct dma_chan *chan, struct dma_slave_config *cfg)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\n\tmemcpy(&c->config, cfg, sizeof(c->config));\n\n\treturn 0;\n}\n\nstatic int ldma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\tstruct device *dev = c->vchan.chan.device->dev;\n\tsize_t\tdesc_sz;\n\n\tif (d->ver > DMA_VER22) {\n\t\tc->flags |= CHAN_IN_USE;\n\t\treturn 0;\n\t}\n\n\tif (c->desc_pool)\n\t\treturn c->desc_num;\n\n\tdesc_sz = c->desc_num * sizeof(struct dw2_desc);\n\tc->desc_pool = dma_pool_create(c->name, dev, desc_sz,\n\t\t\t\t       __alignof__(struct dw2_desc), 0);\n\n\tif (!c->desc_pool) {\n\t\tdev_err(dev, \"unable to allocate descriptor pool\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn c->desc_num;\n}\n\nstatic void ldma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct ldma_chan *c = to_ldma_chan(chan);\n\tstruct ldma_dev *d = to_ldma_dev(c->vchan.chan.device);\n\n\tif (d->ver == DMA_VER22) {\n\t\tdma_pool_destroy(c->desc_pool);\n\t\tc->desc_pool = NULL;\n\t\tvchan_free_chan_resources(to_virt_chan(chan));\n\t\tldma_chan_reset(c);\n\t} else {\n\t\tc->flags &= ~CHAN_IN_USE;\n\t}\n}\n\nstatic void dma_work(struct work_struct *work)\n{\n\tstruct ldma_chan *c = container_of(work, struct ldma_chan, work);\n\tstruct dma_async_tx_descriptor *tx = &c->ds->vdesc.tx;\n\tstruct virt_dma_chan *vc = &c->vchan;\n\tstruct dmaengine_desc_callback cb;\n\tstruct virt_dma_desc *vd, *_vd;\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&c->vchan.lock, flags);\n\tlist_splice_tail_init(&vc->desc_completed, &head);\n\tspin_unlock_irqrestore(&c->vchan.lock, flags);\n\tdmaengine_desc_get_callback(tx, &cb);\n\tdma_cookie_complete(tx);\n\tdmaengine_desc_callback_invoke(&cb, NULL);\n\n\tlist_for_each_entry_safe(vd, _vd, &head, node) {\n\t\tdmaengine_desc_get_callback(tx, &cb);\n\t\tdma_cookie_complete(tx);\n\t\tlist_del(&vd->node);\n\t\tdmaengine_desc_callback_invoke(&cb, NULL);\n\n\t\tvchan_vdesc_fini(vd);\n\t}\n\tc->ds = NULL;\n}\n\nstatic void\nupdate_burst_len_v22(struct ldma_chan *c, struct ldma_port *p, u32 burst)\n{\n\tif (ldma_chan_tx(c))\n\t\tp->txbl = ilog2(burst);\n\telse\n\t\tp->rxbl = ilog2(burst);\n}\n\nstatic void\nupdate_burst_len_v3X(struct ldma_chan *c, struct ldma_port *p, u32 burst)\n{\n\tif (ldma_chan_tx(c))\n\t\tp->txbl = burst;\n\telse\n\t\tp->rxbl = burst;\n}\n\nstatic int\nupdate_client_configs(struct of_dma *ofdma, struct of_phandle_args *spec)\n{\n\tstruct ldma_dev *d = ofdma->of_dma_data;\n\tu32 chan_id =  spec->args[0];\n\tu32 port_id =  spec->args[1];\n\tu32 burst = spec->args[2];\n\tstruct ldma_port *p;\n\tstruct ldma_chan *c;\n\n\tif (chan_id >= d->chan_nrs || port_id >= d->port_nrs)\n\t\treturn 0;\n\n\tp = &d->ports[port_id];\n\tc = &d->chans[chan_id];\n\tc->port = p;\n\n\tif (d->ver == DMA_VER22)\n\t\tupdate_burst_len_v22(c, p, burst);\n\telse\n\t\tupdate_burst_len_v3X(c, p, burst);\n\n\tldma_port_cfg(p);\n\n\treturn 1;\n}\n\nstatic struct dma_chan *ldma_xlate(struct of_phandle_args *spec,\n\t\t\t\t   struct of_dma *ofdma)\n{\n\tstruct ldma_dev *d = ofdma->of_dma_data;\n\tu32 chan_id =  spec->args[0];\n\tint ret;\n\n\tif (!spec->args_count)\n\t\treturn NULL;\n\n\t \n\tif (spec->args_count > 1) {\n\t\tret = update_client_configs(ofdma, spec);\n\t\tif (!ret)\n\t\t\treturn NULL;\n\t}\n\n\treturn dma_get_slave_channel(&d->chans[chan_id].vchan.chan);\n}\n\nstatic void ldma_dma_init_v22(int i, struct ldma_dev *d)\n{\n\tstruct ldma_chan *c;\n\n\tc = &d->chans[i];\n\tc->nr = i;  \n\tc->rst = DMA_CHAN_RST;\n\tc->desc_num = DMA_DFT_DESC_NUM;\n\tsnprintf(c->name, sizeof(c->name), \"chan%d\", c->nr);\n\tINIT_WORK(&c->work, dma_work);\n\tc->vchan.desc_free = dma_free_desc_resource;\n\tvchan_init(&c->vchan, &d->dma_dev);\n}\n\nstatic void ldma_dma_init_v3X(int i, struct ldma_dev *d)\n{\n\tstruct ldma_chan *c;\n\n\tc = &d->chans[i];\n\tc->data_endian = DMA_DFT_ENDIAN;\n\tc->desc_endian = DMA_DFT_ENDIAN;\n\tc->data_endian_en = false;\n\tc->desc_endian_en = false;\n\tc->desc_rx_np = false;\n\tc->flags |= DEVICE_ALLOC_DESC;\n\tc->onoff = DMA_CH_OFF;\n\tc->rst = DMA_CHAN_RST;\n\tc->abc_en = true;\n\tc->hdrm_csum = false;\n\tc->boff_len = 0;\n\tc->nr = i;\n\tc->vchan.desc_free = dma_free_desc_resource;\n\tvchan_init(&c->vchan, &d->dma_dev);\n}\n\nstatic int ldma_init_v22(struct ldma_dev *d, struct platform_device *pdev)\n{\n\tint ret;\n\n\tret = device_property_read_u32(d->dev, \"dma-channels\", &d->chan_nrs);\n\tif (ret < 0) {\n\t\tdev_err(d->dev, \"unable to read dma-channels property\\n\");\n\t\treturn ret;\n\t}\n\n\td->irq = platform_get_irq(pdev, 0);\n\tif (d->irq < 0)\n\t\treturn d->irq;\n\n\tret = devm_request_irq(&pdev->dev, d->irq, dma_interrupt, 0,\n\t\t\t       DRIVER_NAME, d);\n\tif (ret)\n\t\treturn ret;\n\n\td->wq = alloc_ordered_workqueue(\"dma_wq\", WQ_MEM_RECLAIM |\n\t\t\tWQ_HIGHPRI);\n\tif (!d->wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void ldma_clk_disable(void *data)\n{\n\tstruct ldma_dev *d = data;\n\n\tclk_disable_unprepare(d->core_clk);\n\treset_control_assert(d->rst);\n}\n\nstatic const struct ldma_inst_data dma0 = {\n\t.name = \"dma0\",\n\t.chan_fc = false,\n\t.desc_fod = false,\n\t.desc_in_sram = false,\n\t.valid_desc_fetch_ack = false,\n};\n\nstatic const struct ldma_inst_data dma2tx = {\n\t.name = \"dma2tx\",\n\t.type = DMA_TYPE_TX,\n\t.orrc = 16,\n\t.chan_fc = true,\n\t.desc_fod = true,\n\t.desc_in_sram = true,\n\t.valid_desc_fetch_ack = true,\n};\n\nstatic const struct ldma_inst_data dma1rx = {\n\t.name = \"dma1rx\",\n\t.type = DMA_TYPE_RX,\n\t.orrc = 16,\n\t.chan_fc = false,\n\t.desc_fod = true,\n\t.desc_in_sram = true,\n\t.valid_desc_fetch_ack = false,\n};\n\nstatic const struct ldma_inst_data dma1tx = {\n\t.name = \"dma1tx\",\n\t.type = DMA_TYPE_TX,\n\t.orrc = 16,\n\t.chan_fc = true,\n\t.desc_fod = true,\n\t.desc_in_sram = true,\n\t.valid_desc_fetch_ack = true,\n};\n\nstatic const struct ldma_inst_data dma0tx = {\n\t.name = \"dma0tx\",\n\t.type = DMA_TYPE_TX,\n\t.orrc = 16,\n\t.chan_fc = true,\n\t.desc_fod = true,\n\t.desc_in_sram = true,\n\t.valid_desc_fetch_ack = true,\n};\n\nstatic const struct ldma_inst_data dma3 = {\n\t.name = \"dma3\",\n\t.type = DMA_TYPE_MCPY,\n\t.orrc = 16,\n\t.chan_fc = false,\n\t.desc_fod = false,\n\t.desc_in_sram = true,\n\t.valid_desc_fetch_ack = false,\n};\n\nstatic const struct ldma_inst_data toe_dma30 = {\n\t.name = \"toe_dma30\",\n\t.type = DMA_TYPE_MCPY,\n\t.orrc = 16,\n\t.chan_fc = false,\n\t.desc_fod = false,\n\t.desc_in_sram = true,\n\t.valid_desc_fetch_ack = true,\n};\n\nstatic const struct ldma_inst_data toe_dma31 = {\n\t.name = \"toe_dma31\",\n\t.type = DMA_TYPE_MCPY,\n\t.orrc = 16,\n\t.chan_fc = false,\n\t.desc_fod = false,\n\t.desc_in_sram = true,\n\t.valid_desc_fetch_ack = true,\n};\n\nstatic const struct of_device_id intel_ldma_match[] = {\n\t{ .compatible = \"intel,lgm-cdma\", .data = &dma0},\n\t{ .compatible = \"intel,lgm-dma2tx\", .data = &dma2tx},\n\t{ .compatible = \"intel,lgm-dma1rx\", .data = &dma1rx},\n\t{ .compatible = \"intel,lgm-dma1tx\", .data = &dma1tx},\n\t{ .compatible = \"intel,lgm-dma0tx\", .data = &dma0tx},\n\t{ .compatible = \"intel,lgm-dma3\", .data = &dma3},\n\t{ .compatible = \"intel,lgm-toe-dma30\", .data = &toe_dma30},\n\t{ .compatible = \"intel,lgm-toe-dma31\", .data = &toe_dma31},\n\t{}\n};\n\nstatic int intel_ldma_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct dma_device *dma_dev;\n\tunsigned long ch_mask;\n\tstruct ldma_chan *c;\n\tstruct ldma_port *p;\n\tstruct ldma_dev *d;\n\tu32 id, bitn = 32, j;\n\tint i, ret;\n\n\td = devm_kzalloc(dev, sizeof(*d), GFP_KERNEL);\n\tif (!d)\n\t\treturn -ENOMEM;\n\n\t \n\td->dev = &pdev->dev;\n\n\td->inst = device_get_match_data(dev);\n\tif (!d->inst) {\n\t\tdev_err(dev, \"No device match found\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\td->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(d->base))\n\t\treturn PTR_ERR(d->base);\n\n\t \n\td->core_clk = devm_clk_get_optional(dev, NULL);\n\tif (IS_ERR(d->core_clk))\n\t\treturn PTR_ERR(d->core_clk);\n\n\td->rst = devm_reset_control_get_optional(dev, NULL);\n\tif (IS_ERR(d->rst))\n\t\treturn PTR_ERR(d->rst);\n\n\tclk_prepare_enable(d->core_clk);\n\treset_control_deassert(d->rst);\n\n\tret = devm_add_action_or_reset(dev, ldma_clk_disable, d);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to devm_add_action_or_reset, %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tid = readl(d->base + DMA_ID);\n\td->chan_nrs = FIELD_GET(DMA_ID_CHNR, id);\n\td->port_nrs = FIELD_GET(DMA_ID_PNR, id);\n\td->ver = FIELD_GET(DMA_ID_REV, id);\n\n\tif (id & DMA_ID_AW_36B)\n\t\td->flags |= DMA_ADDR_36BIT;\n\n\tif (IS_ENABLED(CONFIG_64BIT) && (id & DMA_ID_AW_36B))\n\t\tbitn = 36;\n\n\tif (id & DMA_ID_DW_128B)\n\t\td->flags |= DMA_DATA_128BIT;\n\n\tret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(bitn));\n\tif (ret) {\n\t\tdev_err(dev, \"No usable DMA configuration\\n\");\n\t\treturn ret;\n\t}\n\n\tif (d->ver == DMA_VER22) {\n\t\tret = ldma_init_v22(d, pdev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = device_property_read_u32(dev, \"dma-channel-mask\", &d->channels_mask);\n\tif (ret < 0)\n\t\td->channels_mask = GENMASK(d->chan_nrs - 1, 0);\n\n\tdma_dev = &d->dma_dev;\n\n\tdma_cap_zero(dma_dev->cap_mask);\n\tdma_cap_set(DMA_SLAVE, dma_dev->cap_mask);\n\n\t \n\tINIT_LIST_HEAD(&dma_dev->channels);\n\n\t \n\td->ports = devm_kcalloc(dev, d->port_nrs, sizeof(*p), GFP_KERNEL);\n\tif (!d->ports)\n\t\treturn -ENOMEM;\n\n\t \n\td->chans = devm_kcalloc(d->dev, d->chan_nrs, sizeof(*c), GFP_KERNEL);\n\tif (!d->chans)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < d->port_nrs; i++) {\n\t\tp = &d->ports[i];\n\t\tp->portid = i;\n\t\tp->ldev = d;\n\t}\n\n\tdma_dev->dev = &pdev->dev;\n\n\tch_mask = (unsigned long)d->channels_mask;\n\tfor_each_set_bit(j, &ch_mask, d->chan_nrs) {\n\t\tif (d->ver == DMA_VER22)\n\t\t\tldma_dma_init_v22(j, d);\n\t\telse\n\t\t\tldma_dma_init_v3X(j, d);\n\t}\n\n\tret = ldma_parse_dt(d);\n\tif (ret)\n\t\treturn ret;\n\n\tdma_dev->device_alloc_chan_resources = ldma_alloc_chan_resources;\n\tdma_dev->device_free_chan_resources = ldma_free_chan_resources;\n\tdma_dev->device_terminate_all = ldma_terminate_all;\n\tdma_dev->device_issue_pending = ldma_issue_pending;\n\tdma_dev->device_tx_status = ldma_tx_status;\n\tdma_dev->device_resume = ldma_resume_chan;\n\tdma_dev->device_pause = ldma_pause_chan;\n\tdma_dev->device_prep_slave_sg = ldma_prep_slave_sg;\n\n\tif (d->ver == DMA_VER22) {\n\t\tdma_dev->device_config = ldma_slave_config;\n\t\tdma_dev->device_synchronize = ldma_synchronize;\n\t\tdma_dev->src_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\t\tdma_dev->dst_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\t\tdma_dev->directions = BIT(DMA_MEM_TO_DEV) |\n\t\t\t\t      BIT(DMA_DEV_TO_MEM);\n\t\tdma_dev->residue_granularity =\n\t\t\t\t\tDMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\t}\n\n\tplatform_set_drvdata(pdev, d);\n\n\tldma_dev_init(d);\n\n\tret = dma_async_device_register(dma_dev);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to register slave DMA engine device\\n\");\n\t\treturn ret;\n\t}\n\n\tret = of_dma_controller_register(pdev->dev.of_node, ldma_xlate, d);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to register of DMA controller\\n\");\n\t\tdma_async_device_unregister(dma_dev);\n\t\treturn ret;\n\t}\n\n\tdev_info(dev, \"Init done - rev: %x, ports: %d channels: %d\\n\", d->ver,\n\t\t d->port_nrs, d->chan_nrs);\n\n\treturn 0;\n}\n\nstatic struct platform_driver intel_ldma_driver = {\n\t.probe = intel_ldma_probe,\n\t.driver = {\n\t\t.name = DRIVER_NAME,\n\t\t.of_match_table = intel_ldma_match,\n\t},\n};\n\n \nbuiltin_platform_driver(intel_ldma_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}