{
  "module_name": "dmaengine.c",
  "hash_id": "41ee2780c6e7855facc3cee6ce8eb27863cae069416e8cfca964410da2439f64",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/dmaengine.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/platform_device.h>\n#include <linux/dma-mapping.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/device.h>\n#include <linux/dmaengine.h>\n#include <linux/hardirq.h>\n#include <linux/spinlock.h>\n#include <linux/percpu.h>\n#include <linux/rcupdate.h>\n#include <linux/mutex.h>\n#include <linux/jiffies.h>\n#include <linux/rculist.h>\n#include <linux/idr.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n#include <linux/acpi_dma.h>\n#include <linux/of_dma.h>\n#include <linux/mempool.h>\n#include <linux/numa.h>\n\n#include \"dmaengine.h\"\n\nstatic DEFINE_MUTEX(dma_list_mutex);\nstatic DEFINE_IDA(dma_ida);\nstatic LIST_HEAD(dma_device_list);\nstatic long dmaengine_ref_count;\n\n \n#ifdef CONFIG_DEBUG_FS\n#include <linux/debugfs.h>\n\nstatic struct dentry *rootdir;\n\nstatic void dmaengine_debug_register(struct dma_device *dma_dev)\n{\n\tdma_dev->dbg_dev_root = debugfs_create_dir(dev_name(dma_dev->dev),\n\t\t\t\t\t\t   rootdir);\n\tif (IS_ERR(dma_dev->dbg_dev_root))\n\t\tdma_dev->dbg_dev_root = NULL;\n}\n\nstatic void dmaengine_debug_unregister(struct dma_device *dma_dev)\n{\n\tdebugfs_remove_recursive(dma_dev->dbg_dev_root);\n\tdma_dev->dbg_dev_root = NULL;\n}\n\nstatic void dmaengine_dbg_summary_show(struct seq_file *s,\n\t\t\t\t       struct dma_device *dma_dev)\n{\n\tstruct dma_chan *chan;\n\n\tlist_for_each_entry(chan, &dma_dev->channels, device_node) {\n\t\tif (chan->client_count) {\n\t\t\tseq_printf(s, \" %-13s| %s\", dma_chan_name(chan),\n\t\t\t\t   chan->dbg_client_name ?: \"in-use\");\n\n\t\t\tif (chan->router)\n\t\t\t\tseq_printf(s, \" (via router: %s)\\n\",\n\t\t\t\t\tdev_name(chan->router->dev));\n\t\t\telse\n\t\t\t\tseq_puts(s, \"\\n\");\n\t\t}\n\t}\n}\n\nstatic int dmaengine_summary_show(struct seq_file *s, void *data)\n{\n\tstruct dma_device *dma_dev = NULL;\n\n\tmutex_lock(&dma_list_mutex);\n\tlist_for_each_entry(dma_dev, &dma_device_list, global_node) {\n\t\tseq_printf(s, \"dma%d (%s): number of channels: %u\\n\",\n\t\t\t   dma_dev->dev_id, dev_name(dma_dev->dev),\n\t\t\t   dma_dev->chancnt);\n\n\t\tif (dma_dev->dbg_summary_show)\n\t\t\tdma_dev->dbg_summary_show(s, dma_dev);\n\t\telse\n\t\t\tdmaengine_dbg_summary_show(s, dma_dev);\n\n\t\tif (!list_is_last(&dma_dev->global_node, &dma_device_list))\n\t\t\tseq_puts(s, \"\\n\");\n\t}\n\tmutex_unlock(&dma_list_mutex);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(dmaengine_summary);\n\nstatic void __init dmaengine_debugfs_init(void)\n{\n\trootdir = debugfs_create_dir(\"dmaengine\", NULL);\n\n\t \n\tdebugfs_create_file(\"summary\", 0444, rootdir, NULL,\n\t\t\t    &dmaengine_summary_fops);\n}\n#else\nstatic inline void dmaengine_debugfs_init(void) { }\nstatic inline int dmaengine_debug_register(struct dma_device *dma_dev)\n{\n\treturn 0;\n}\n\nstatic inline void dmaengine_debug_unregister(struct dma_device *dma_dev) { }\n#endif\t \n\n \n\n#define DMA_SLAVE_NAME\t\"slave\"\n\n \nstatic struct dma_chan *dev_to_dma_chan(struct device *dev)\n{\n\tstruct dma_chan_dev *chan_dev;\n\n\tchan_dev = container_of(dev, typeof(*chan_dev), device);\n\treturn chan_dev->chan;\n}\n\nstatic ssize_t memcpy_count_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct dma_chan *chan;\n\tunsigned long count = 0;\n\tint i;\n\tint err;\n\n\tmutex_lock(&dma_list_mutex);\n\tchan = dev_to_dma_chan(dev);\n\tif (chan) {\n\t\tfor_each_possible_cpu(i)\n\t\t\tcount += per_cpu_ptr(chan->local, i)->memcpy_count;\n\t\terr = sysfs_emit(buf, \"%lu\\n\", count);\n\t} else\n\t\terr = -ENODEV;\n\tmutex_unlock(&dma_list_mutex);\n\n\treturn err;\n}\nstatic DEVICE_ATTR_RO(memcpy_count);\n\nstatic ssize_t bytes_transferred_show(struct device *dev,\n\t\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct dma_chan *chan;\n\tunsigned long count = 0;\n\tint i;\n\tint err;\n\n\tmutex_lock(&dma_list_mutex);\n\tchan = dev_to_dma_chan(dev);\n\tif (chan) {\n\t\tfor_each_possible_cpu(i)\n\t\t\tcount += per_cpu_ptr(chan->local, i)->bytes_transferred;\n\t\terr = sysfs_emit(buf, \"%lu\\n\", count);\n\t} else\n\t\terr = -ENODEV;\n\tmutex_unlock(&dma_list_mutex);\n\n\treturn err;\n}\nstatic DEVICE_ATTR_RO(bytes_transferred);\n\nstatic ssize_t in_use_show(struct device *dev, struct device_attribute *attr,\n\t\t\t   char *buf)\n{\n\tstruct dma_chan *chan;\n\tint err;\n\n\tmutex_lock(&dma_list_mutex);\n\tchan = dev_to_dma_chan(dev);\n\tif (chan)\n\t\terr = sysfs_emit(buf, \"%d\\n\", chan->client_count);\n\telse\n\t\terr = -ENODEV;\n\tmutex_unlock(&dma_list_mutex);\n\n\treturn err;\n}\nstatic DEVICE_ATTR_RO(in_use);\n\nstatic struct attribute *dma_dev_attrs[] = {\n\t&dev_attr_memcpy_count.attr,\n\t&dev_attr_bytes_transferred.attr,\n\t&dev_attr_in_use.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(dma_dev);\n\nstatic void chan_dev_release(struct device *dev)\n{\n\tstruct dma_chan_dev *chan_dev;\n\n\tchan_dev = container_of(dev, typeof(*chan_dev), device);\n\tkfree(chan_dev);\n}\n\nstatic struct class dma_devclass = {\n\t.name\t\t= \"dma\",\n\t.dev_groups\t= dma_dev_groups,\n\t.dev_release\t= chan_dev_release,\n};\n\n \n\n \nstatic dma_cap_mask_t dma_cap_mask_all;\n\n \nstruct dma_chan_tbl_ent {\n\tstruct dma_chan *chan;\n};\n\n \nstatic struct dma_chan_tbl_ent __percpu *channel_table[DMA_TX_TYPE_END];\n\nstatic int __init dma_channel_table_init(void)\n{\n\tenum dma_transaction_type cap;\n\tint err = 0;\n\n\tbitmap_fill(dma_cap_mask_all.bits, DMA_TX_TYPE_END);\n\n\t \n\tclear_bit(DMA_INTERRUPT, dma_cap_mask_all.bits);\n\tclear_bit(DMA_PRIVATE, dma_cap_mask_all.bits);\n\tclear_bit(DMA_SLAVE, dma_cap_mask_all.bits);\n\n\tfor_each_dma_cap_mask(cap, dma_cap_mask_all) {\n\t\tchannel_table[cap] = alloc_percpu(struct dma_chan_tbl_ent);\n\t\tif (!channel_table[cap]) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (err) {\n\t\tpr_err(\"dmaengine dma_channel_table_init failure: %d\\n\", err);\n\t\tfor_each_dma_cap_mask(cap, dma_cap_mask_all)\n\t\t\tfree_percpu(channel_table[cap]);\n\t}\n\n\treturn err;\n}\narch_initcall(dma_channel_table_init);\n\n \nstatic bool dma_chan_is_local(struct dma_chan *chan, int cpu)\n{\n\tint node = dev_to_node(chan->device->dev);\n\treturn node == NUMA_NO_NODE ||\n\t\tcpumask_test_cpu(cpu, cpumask_of_node(node));\n}\n\n \nstatic struct dma_chan *min_chan(enum dma_transaction_type cap, int cpu)\n{\n\tstruct dma_device *device;\n\tstruct dma_chan *chan;\n\tstruct dma_chan *min = NULL;\n\tstruct dma_chan *localmin = NULL;\n\n\tlist_for_each_entry(device, &dma_device_list, global_node) {\n\t\tif (!dma_has_cap(cap, device->cap_mask) ||\n\t\t    dma_has_cap(DMA_PRIVATE, device->cap_mask))\n\t\t\tcontinue;\n\t\tlist_for_each_entry(chan, &device->channels, device_node) {\n\t\t\tif (!chan->client_count)\n\t\t\t\tcontinue;\n\t\t\tif (!min || chan->table_count < min->table_count)\n\t\t\t\tmin = chan;\n\n\t\t\tif (dma_chan_is_local(chan, cpu))\n\t\t\t\tif (!localmin ||\n\t\t\t\t    chan->table_count < localmin->table_count)\n\t\t\t\t\tlocalmin = chan;\n\t\t}\n\t}\n\n\tchan = localmin ? localmin : min;\n\n\tif (chan)\n\t\tchan->table_count++;\n\n\treturn chan;\n}\n\n \nstatic void dma_channel_rebalance(void)\n{\n\tstruct dma_chan *chan;\n\tstruct dma_device *device;\n\tint cpu;\n\tint cap;\n\n\t \n\tfor_each_dma_cap_mask(cap, dma_cap_mask_all)\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tper_cpu_ptr(channel_table[cap], cpu)->chan = NULL;\n\n\tlist_for_each_entry(device, &dma_device_list, global_node) {\n\t\tif (dma_has_cap(DMA_PRIVATE, device->cap_mask))\n\t\t\tcontinue;\n\t\tlist_for_each_entry(chan, &device->channels, device_node)\n\t\t\tchan->table_count = 0;\n\t}\n\n\t \n\tif (!dmaengine_ref_count)\n\t\treturn;\n\n\t \n\tfor_each_dma_cap_mask(cap, dma_cap_mask_all)\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tchan = min_chan(cap, cpu);\n\t\t\tper_cpu_ptr(channel_table[cap], cpu)->chan = chan;\n\t\t}\n}\n\nstatic int dma_device_satisfies_mask(struct dma_device *device,\n\t\t\t\t     const dma_cap_mask_t *want)\n{\n\tdma_cap_mask_t has;\n\n\tbitmap_and(has.bits, want->bits, device->cap_mask.bits,\n\t\tDMA_TX_TYPE_END);\n\treturn bitmap_equal(want->bits, has.bits, DMA_TX_TYPE_END);\n}\n\nstatic struct module *dma_chan_to_owner(struct dma_chan *chan)\n{\n\treturn chan->device->owner;\n}\n\n \nstatic void balance_ref_count(struct dma_chan *chan)\n{\n\tstruct module *owner = dma_chan_to_owner(chan);\n\n\twhile (chan->client_count < dmaengine_ref_count) {\n\t\t__module_get(owner);\n\t\tchan->client_count++;\n\t}\n}\n\nstatic void dma_device_release(struct kref *ref)\n{\n\tstruct dma_device *device = container_of(ref, struct dma_device, ref);\n\n\tlist_del_rcu(&device->global_node);\n\tdma_channel_rebalance();\n\n\tif (device->device_release)\n\t\tdevice->device_release(device);\n}\n\nstatic void dma_device_put(struct dma_device *device)\n{\n\tlockdep_assert_held(&dma_list_mutex);\n\tkref_put(&device->ref, dma_device_release);\n}\n\n \nstatic int dma_chan_get(struct dma_chan *chan)\n{\n\tstruct module *owner = dma_chan_to_owner(chan);\n\tint ret;\n\n\t \n\tif (chan->client_count) {\n\t\t__module_get(owner);\n\t\tchan->client_count++;\n\t\treturn 0;\n\t}\n\n\tif (!try_module_get(owner))\n\t\treturn -ENODEV;\n\n\tret = kref_get_unless_zero(&chan->device->ref);\n\tif (!ret) {\n\t\tret = -ENODEV;\n\t\tgoto module_put_out;\n\t}\n\n\t \n\tif (chan->device->device_alloc_chan_resources) {\n\t\tret = chan->device->device_alloc_chan_resources(chan);\n\t\tif (ret < 0)\n\t\t\tgoto err_out;\n\t}\n\n\tchan->client_count++;\n\n\tif (!dma_has_cap(DMA_PRIVATE, chan->device->cap_mask))\n\t\tbalance_ref_count(chan);\n\n\treturn 0;\n\nerr_out:\n\tdma_device_put(chan->device);\nmodule_put_out:\n\tmodule_put(owner);\n\treturn ret;\n}\n\n \nstatic void dma_chan_put(struct dma_chan *chan)\n{\n\t \n\tif (!chan->client_count)\n\t\treturn;\n\n\tchan->client_count--;\n\n\t \n\tif (!chan->client_count && chan->device->device_free_chan_resources) {\n\t\t \n\t\tdmaengine_synchronize(chan);\n\t\tchan->device->device_free_chan_resources(chan);\n\t}\n\n\t \n\tif (chan->router && chan->router->route_free) {\n\t\tchan->router->route_free(chan->router->dev, chan->route_data);\n\t\tchan->router = NULL;\n\t\tchan->route_data = NULL;\n\t}\n\n\tdma_device_put(chan->device);\n\tmodule_put(dma_chan_to_owner(chan));\n}\n\nenum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie)\n{\n\tenum dma_status status;\n\tunsigned long dma_sync_wait_timeout = jiffies + msecs_to_jiffies(5000);\n\n\tdma_async_issue_pending(chan);\n\tdo {\n\t\tstatus = dma_async_is_tx_complete(chan, cookie, NULL, NULL);\n\t\tif (time_after_eq(jiffies, dma_sync_wait_timeout)) {\n\t\t\tdev_err(chan->device->dev, \"%s: timeout!\\n\", __func__);\n\t\t\treturn DMA_ERROR;\n\t\t}\n\t\tif (status != DMA_IN_PROGRESS)\n\t\t\tbreak;\n\t\tcpu_relax();\n\t} while (1);\n\n\treturn status;\n}\nEXPORT_SYMBOL(dma_sync_wait);\n\n \nstruct dma_chan *dma_find_channel(enum dma_transaction_type tx_type)\n{\n\treturn this_cpu_read(channel_table[tx_type]->chan);\n}\nEXPORT_SYMBOL(dma_find_channel);\n\n \nvoid dma_issue_pending_all(void)\n{\n\tstruct dma_device *device;\n\tstruct dma_chan *chan;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(device, &dma_device_list, global_node) {\n\t\tif (dma_has_cap(DMA_PRIVATE, device->cap_mask))\n\t\t\tcontinue;\n\t\tlist_for_each_entry(chan, &device->channels, device_node)\n\t\t\tif (chan->client_count)\n\t\t\t\tdevice->device_issue_pending(chan);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(dma_issue_pending_all);\n\nint dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps)\n{\n\tstruct dma_device *device;\n\n\tif (!chan || !caps)\n\t\treturn -EINVAL;\n\n\tdevice = chan->device;\n\n\t \n\tif (!(test_bit(DMA_SLAVE, device->cap_mask.bits) ||\n\t      test_bit(DMA_CYCLIC, device->cap_mask.bits)))\n\t\treturn -ENXIO;\n\n\t \n\tif (!device->directions)\n\t\treturn -ENXIO;\n\n\tcaps->src_addr_widths = device->src_addr_widths;\n\tcaps->dst_addr_widths = device->dst_addr_widths;\n\tcaps->directions = device->directions;\n\tcaps->min_burst = device->min_burst;\n\tcaps->max_burst = device->max_burst;\n\tcaps->max_sg_burst = device->max_sg_burst;\n\tcaps->residue_granularity = device->residue_granularity;\n\tcaps->descriptor_reuse = device->descriptor_reuse;\n\tcaps->cmd_pause = !!device->device_pause;\n\tcaps->cmd_resume = !!device->device_resume;\n\tcaps->cmd_terminate = !!device->device_terminate_all;\n\n\t \n\tif (device->device_caps)\n\t\tdevice->device_caps(chan, caps);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dma_get_slave_caps);\n\nstatic struct dma_chan *private_candidate(const dma_cap_mask_t *mask,\n\t\t\t\t\t  struct dma_device *dev,\n\t\t\t\t\t  dma_filter_fn fn, void *fn_param)\n{\n\tstruct dma_chan *chan;\n\n\tif (mask && !dma_device_satisfies_mask(dev, mask)) {\n\t\tdev_dbg(dev->dev, \"%s: wrong capabilities\\n\", __func__);\n\t\treturn NULL;\n\t}\n\t \n\tif (dev->chancnt > 1 && !dma_has_cap(DMA_PRIVATE, dev->cap_mask))\n\t\tlist_for_each_entry(chan, &dev->channels, device_node) {\n\t\t\t \n\t\t\tif (chan->client_count)\n\t\t\t\treturn NULL;\n\t\t}\n\n\tlist_for_each_entry(chan, &dev->channels, device_node) {\n\t\tif (chan->client_count) {\n\t\t\tdev_dbg(dev->dev, \"%s: %s busy\\n\",\n\t\t\t\t __func__, dma_chan_name(chan));\n\t\t\tcontinue;\n\t\t}\n\t\tif (fn && !fn(chan, fn_param)) {\n\t\t\tdev_dbg(dev->dev, \"%s: %s filter said false\\n\",\n\t\t\t\t __func__, dma_chan_name(chan));\n\t\t\tcontinue;\n\t\t}\n\t\treturn chan;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct dma_chan *find_candidate(struct dma_device *device,\n\t\t\t\t       const dma_cap_mask_t *mask,\n\t\t\t\t       dma_filter_fn fn, void *fn_param)\n{\n\tstruct dma_chan *chan = private_candidate(mask, device, fn, fn_param);\n\tint err;\n\n\tif (chan) {\n\t\t \n\t\tdma_cap_set(DMA_PRIVATE, device->cap_mask);\n\t\tdevice->privatecnt++;\n\t\terr = dma_chan_get(chan);\n\n\t\tif (err) {\n\t\t\tif (err == -ENODEV) {\n\t\t\t\tdev_dbg(device->dev, \"%s: %s module removed\\n\",\n\t\t\t\t\t__func__, dma_chan_name(chan));\n\t\t\t\tlist_del_rcu(&device->global_node);\n\t\t\t} else\n\t\t\t\tdev_dbg(device->dev,\n\t\t\t\t\t\"%s: failed to get %s: (%d)\\n\",\n\t\t\t\t\t __func__, dma_chan_name(chan), err);\n\n\t\t\tif (--device->privatecnt == 0)\n\t\t\t\tdma_cap_clear(DMA_PRIVATE, device->cap_mask);\n\n\t\t\tchan = ERR_PTR(err);\n\t\t}\n\t}\n\n\treturn chan ? chan : ERR_PTR(-EPROBE_DEFER);\n}\n\n \nstruct dma_chan *dma_get_slave_channel(struct dma_chan *chan)\n{\n\t \n\tmutex_lock(&dma_list_mutex);\n\n\tif (chan->client_count == 0) {\n\t\tstruct dma_device *device = chan->device;\n\t\tint err;\n\n\t\tdma_cap_set(DMA_PRIVATE, device->cap_mask);\n\t\tdevice->privatecnt++;\n\t\terr = dma_chan_get(chan);\n\t\tif (err) {\n\t\t\tdev_dbg(chan->device->dev,\n\t\t\t\t\"%s: failed to get %s: (%d)\\n\",\n\t\t\t\t__func__, dma_chan_name(chan), err);\n\t\t\tchan = NULL;\n\t\t\tif (--device->privatecnt == 0)\n\t\t\t\tdma_cap_clear(DMA_PRIVATE, device->cap_mask);\n\t\t}\n\t} else\n\t\tchan = NULL;\n\n\tmutex_unlock(&dma_list_mutex);\n\n\n\treturn chan;\n}\nEXPORT_SYMBOL_GPL(dma_get_slave_channel);\n\nstruct dma_chan *dma_get_any_slave_channel(struct dma_device *device)\n{\n\tdma_cap_mask_t mask;\n\tstruct dma_chan *chan;\n\n\tdma_cap_zero(mask);\n\tdma_cap_set(DMA_SLAVE, mask);\n\n\t \n\tmutex_lock(&dma_list_mutex);\n\n\tchan = find_candidate(device, &mask, NULL, NULL);\n\n\tmutex_unlock(&dma_list_mutex);\n\n\treturn IS_ERR(chan) ? NULL : chan;\n}\nEXPORT_SYMBOL_GPL(dma_get_any_slave_channel);\n\n \nstruct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,\n\t\t\t\t       dma_filter_fn fn, void *fn_param,\n\t\t\t\t       struct device_node *np)\n{\n\tstruct dma_device *device, *_d;\n\tstruct dma_chan *chan = NULL;\n\n\t \n\tmutex_lock(&dma_list_mutex);\n\tlist_for_each_entry_safe(device, _d, &dma_device_list, global_node) {\n\t\t \n\t\tif (np && device->dev->of_node && np != device->dev->of_node)\n\t\t\tcontinue;\n\n\t\tchan = find_candidate(device, mask, fn, fn_param);\n\t\tif (!IS_ERR(chan))\n\t\t\tbreak;\n\n\t\tchan = NULL;\n\t}\n\tmutex_unlock(&dma_list_mutex);\n\n\tpr_debug(\"%s: %s (%s)\\n\",\n\t\t __func__,\n\t\t chan ? \"success\" : \"fail\",\n\t\t chan ? dma_chan_name(chan) : NULL);\n\n\treturn chan;\n}\nEXPORT_SYMBOL_GPL(__dma_request_channel);\n\nstatic const struct dma_slave_map *dma_filter_match(struct dma_device *device,\n\t\t\t\t\t\t    const char *name,\n\t\t\t\t\t\t    struct device *dev)\n{\n\tint i;\n\n\tif (!device->filter.mapcnt)\n\t\treturn NULL;\n\n\tfor (i = 0; i < device->filter.mapcnt; i++) {\n\t\tconst struct dma_slave_map *map = &device->filter.map[i];\n\n\t\tif (!strcmp(map->devname, dev_name(dev)) &&\n\t\t    !strcmp(map->slave, name))\n\t\t\treturn map;\n\t}\n\n\treturn NULL;\n}\n\n \nstruct dma_chan *dma_request_chan(struct device *dev, const char *name)\n{\n\tstruct dma_device *d, *_d;\n\tstruct dma_chan *chan = NULL;\n\n\t \n\tif (dev->of_node)\n\t\tchan = of_dma_request_slave_channel(dev->of_node, name);\n\n\t \n\tif (has_acpi_companion(dev) && !chan)\n\t\tchan = acpi_dma_request_slave_chan_by_name(dev, name);\n\n\tif (PTR_ERR(chan) == -EPROBE_DEFER)\n\t\treturn chan;\n\n\tif (!IS_ERR_OR_NULL(chan))\n\t\tgoto found;\n\n\t \n\tmutex_lock(&dma_list_mutex);\n\tlist_for_each_entry_safe(d, _d, &dma_device_list, global_node) {\n\t\tdma_cap_mask_t mask;\n\t\tconst struct dma_slave_map *map = dma_filter_match(d, name, dev);\n\n\t\tif (!map)\n\t\t\tcontinue;\n\n\t\tdma_cap_zero(mask);\n\t\tdma_cap_set(DMA_SLAVE, mask);\n\n\t\tchan = find_candidate(d, &mask, d->filter.fn, map->param);\n\t\tif (!IS_ERR(chan))\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&dma_list_mutex);\n\n\tif (IS_ERR(chan))\n\t\treturn chan;\n\tif (!chan)\n\t\treturn ERR_PTR(-EPROBE_DEFER);\n\nfound:\n#ifdef CONFIG_DEBUG_FS\n\tchan->dbg_client_name = kasprintf(GFP_KERNEL, \"%s:%s\", dev_name(dev),\n\t\t\t\t\t  name);\n#endif\n\n\tchan->name = kasprintf(GFP_KERNEL, \"dma:%s\", name);\n\tif (!chan->name)\n\t\treturn chan;\n\tchan->slave = dev;\n\n\tif (sysfs_create_link(&chan->dev->device.kobj, &dev->kobj,\n\t\t\t      DMA_SLAVE_NAME))\n\t\tdev_warn(dev, \"Cannot create DMA %s symlink\\n\", DMA_SLAVE_NAME);\n\tif (sysfs_create_link(&dev->kobj, &chan->dev->device.kobj, chan->name))\n\t\tdev_warn(dev, \"Cannot create DMA %s symlink\\n\", chan->name);\n\n\treturn chan;\n}\nEXPORT_SYMBOL_GPL(dma_request_chan);\n\n \nstruct dma_chan *dma_request_chan_by_mask(const dma_cap_mask_t *mask)\n{\n\tstruct dma_chan *chan;\n\n\tif (!mask)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tchan = __dma_request_channel(mask, NULL, NULL, NULL);\n\tif (!chan) {\n\t\tmutex_lock(&dma_list_mutex);\n\t\tif (list_empty(&dma_device_list))\n\t\t\tchan = ERR_PTR(-EPROBE_DEFER);\n\t\telse\n\t\t\tchan = ERR_PTR(-ENODEV);\n\t\tmutex_unlock(&dma_list_mutex);\n\t}\n\n\treturn chan;\n}\nEXPORT_SYMBOL_GPL(dma_request_chan_by_mask);\n\nvoid dma_release_channel(struct dma_chan *chan)\n{\n\tmutex_lock(&dma_list_mutex);\n\tWARN_ONCE(chan->client_count != 1,\n\t\t  \"chan reference count %d != 1\\n\", chan->client_count);\n\tdma_chan_put(chan);\n\t \n\tif (--chan->device->privatecnt == 0)\n\t\tdma_cap_clear(DMA_PRIVATE, chan->device->cap_mask);\n\n\tif (chan->slave) {\n\t\tsysfs_remove_link(&chan->dev->device.kobj, DMA_SLAVE_NAME);\n\t\tsysfs_remove_link(&chan->slave->kobj, chan->name);\n\t\tkfree(chan->name);\n\t\tchan->name = NULL;\n\t\tchan->slave = NULL;\n\t}\n\n#ifdef CONFIG_DEBUG_FS\n\tkfree(chan->dbg_client_name);\n\tchan->dbg_client_name = NULL;\n#endif\n\tmutex_unlock(&dma_list_mutex);\n}\nEXPORT_SYMBOL_GPL(dma_release_channel);\n\n \nvoid dmaengine_get(void)\n{\n\tstruct dma_device *device, *_d;\n\tstruct dma_chan *chan;\n\tint err;\n\n\tmutex_lock(&dma_list_mutex);\n\tdmaengine_ref_count++;\n\n\t \n\tlist_for_each_entry_safe(device, _d, &dma_device_list, global_node) {\n\t\tif (dma_has_cap(DMA_PRIVATE, device->cap_mask))\n\t\t\tcontinue;\n\t\tlist_for_each_entry(chan, &device->channels, device_node) {\n\t\t\terr = dma_chan_get(chan);\n\t\t\tif (err == -ENODEV) {\n\t\t\t\t \n\t\t\t\tlist_del_rcu(&device->global_node);\n\t\t\t\tbreak;\n\t\t\t} else if (err)\n\t\t\t\tdev_dbg(chan->device->dev,\n\t\t\t\t\t\"%s: failed to get %s: (%d)\\n\",\n\t\t\t\t\t__func__, dma_chan_name(chan), err);\n\t\t}\n\t}\n\n\t \n\tif (dmaengine_ref_count == 1)\n\t\tdma_channel_rebalance();\n\tmutex_unlock(&dma_list_mutex);\n}\nEXPORT_SYMBOL(dmaengine_get);\n\n \nvoid dmaengine_put(void)\n{\n\tstruct dma_device *device, *_d;\n\tstruct dma_chan *chan;\n\n\tmutex_lock(&dma_list_mutex);\n\tdmaengine_ref_count--;\n\tBUG_ON(dmaengine_ref_count < 0);\n\t \n\tlist_for_each_entry_safe(device, _d, &dma_device_list, global_node) {\n\t\tif (dma_has_cap(DMA_PRIVATE, device->cap_mask))\n\t\t\tcontinue;\n\t\tlist_for_each_entry(chan, &device->channels, device_node)\n\t\t\tdma_chan_put(chan);\n\t}\n\tmutex_unlock(&dma_list_mutex);\n}\nEXPORT_SYMBOL(dmaengine_put);\n\nstatic bool device_has_all_tx_types(struct dma_device *device)\n{\n\t \n\t#ifdef CONFIG_ASYNC_TX_DMA\n\tif (!dma_has_cap(DMA_INTERRUPT, device->cap_mask))\n\t\treturn false;\n\t#endif\n\n\t#if IS_ENABLED(CONFIG_ASYNC_MEMCPY)\n\tif (!dma_has_cap(DMA_MEMCPY, device->cap_mask))\n\t\treturn false;\n\t#endif\n\n\t#if IS_ENABLED(CONFIG_ASYNC_XOR)\n\tif (!dma_has_cap(DMA_XOR, device->cap_mask))\n\t\treturn false;\n\n\t#ifndef CONFIG_ASYNC_TX_DISABLE_XOR_VAL_DMA\n\tif (!dma_has_cap(DMA_XOR_VAL, device->cap_mask))\n\t\treturn false;\n\t#endif\n\t#endif\n\n\t#if IS_ENABLED(CONFIG_ASYNC_PQ)\n\tif (!dma_has_cap(DMA_PQ, device->cap_mask))\n\t\treturn false;\n\n\t#ifndef CONFIG_ASYNC_TX_DISABLE_PQ_VAL_DMA\n\tif (!dma_has_cap(DMA_PQ_VAL, device->cap_mask))\n\t\treturn false;\n\t#endif\n\t#endif\n\n\treturn true;\n}\n\nstatic int get_dma_id(struct dma_device *device)\n{\n\tint rc = ida_alloc(&dma_ida, GFP_KERNEL);\n\n\tif (rc < 0)\n\t\treturn rc;\n\tdevice->dev_id = rc;\n\treturn 0;\n}\n\nstatic int __dma_async_device_channel_register(struct dma_device *device,\n\t\t\t\t\t       struct dma_chan *chan)\n{\n\tint rc;\n\n\tchan->local = alloc_percpu(typeof(*chan->local));\n\tif (!chan->local)\n\t\treturn -ENOMEM;\n\tchan->dev = kzalloc(sizeof(*chan->dev), GFP_KERNEL);\n\tif (!chan->dev) {\n\t\trc = -ENOMEM;\n\t\tgoto err_free_local;\n\t}\n\n\t \n\tchan->chan_id = ida_alloc(&device->chan_ida, GFP_KERNEL);\n\tif (chan->chan_id < 0) {\n\t\tpr_err(\"%s: unable to alloc ida for chan: %d\\n\",\n\t\t       __func__, chan->chan_id);\n\t\trc = chan->chan_id;\n\t\tgoto err_free_dev;\n\t}\n\n\tchan->dev->device.class = &dma_devclass;\n\tchan->dev->device.parent = device->dev;\n\tchan->dev->chan = chan;\n\tchan->dev->dev_id = device->dev_id;\n\tdev_set_name(&chan->dev->device, \"dma%dchan%d\",\n\t\t     device->dev_id, chan->chan_id);\n\trc = device_register(&chan->dev->device);\n\tif (rc)\n\t\tgoto err_out_ida;\n\tchan->client_count = 0;\n\tdevice->chancnt++;\n\n\treturn 0;\n\n err_out_ida:\n\tida_free(&device->chan_ida, chan->chan_id);\n err_free_dev:\n\tkfree(chan->dev);\n err_free_local:\n\tfree_percpu(chan->local);\n\tchan->local = NULL;\n\treturn rc;\n}\n\nint dma_async_device_channel_register(struct dma_device *device,\n\t\t\t\t      struct dma_chan *chan)\n{\n\tint rc;\n\n\trc = __dma_async_device_channel_register(device, chan);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tdma_channel_rebalance();\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dma_async_device_channel_register);\n\nstatic void __dma_async_device_channel_unregister(struct dma_device *device,\n\t\t\t\t\t\t  struct dma_chan *chan)\n{\n\tWARN_ONCE(!device->device_release && chan->client_count,\n\t\t  \"%s called while %d clients hold a reference\\n\",\n\t\t  __func__, chan->client_count);\n\tmutex_lock(&dma_list_mutex);\n\tdevice->chancnt--;\n\tchan->dev->chan = NULL;\n\tmutex_unlock(&dma_list_mutex);\n\tida_free(&device->chan_ida, chan->chan_id);\n\tdevice_unregister(&chan->dev->device);\n\tfree_percpu(chan->local);\n}\n\nvoid dma_async_device_channel_unregister(struct dma_device *device,\n\t\t\t\t\t struct dma_chan *chan)\n{\n\t__dma_async_device_channel_unregister(device, chan);\n\tdma_channel_rebalance();\n}\nEXPORT_SYMBOL_GPL(dma_async_device_channel_unregister);\n\n \nint dma_async_device_register(struct dma_device *device)\n{\n\tint rc;\n\tstruct dma_chan* chan;\n\n\tif (!device)\n\t\treturn -ENODEV;\n\n\t \n\tif (!device->dev) {\n\t\tpr_err(\"DMAdevice must have dev\\n\");\n\t\treturn -EIO;\n\t}\n\n\tdevice->owner = device->dev->driver->owner;\n\n#define CHECK_CAP(_name, _type)\t\t\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\t\t\\\n\tif (dma_has_cap(_type, device->cap_mask) && !device->device_prep_##_name) {\t\\\n\t\tdev_err(device->dev,\t\t\t\t\t\t\t\\\n\t\t\t\"Device claims capability %s, but op is not defined\\n\",\t\t\\\n\t\t\t__stringify(_type));\t\t\t\t\t\t\\\n\t\treturn -EIO;\t\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\t\\\n}\n\n\tCHECK_CAP(dma_memcpy,      DMA_MEMCPY);\n\tCHECK_CAP(dma_xor,         DMA_XOR);\n\tCHECK_CAP(dma_xor_val,     DMA_XOR_VAL);\n\tCHECK_CAP(dma_pq,          DMA_PQ);\n\tCHECK_CAP(dma_pq_val,      DMA_PQ_VAL);\n\tCHECK_CAP(dma_memset,      DMA_MEMSET);\n\tCHECK_CAP(dma_interrupt,   DMA_INTERRUPT);\n\tCHECK_CAP(dma_cyclic,      DMA_CYCLIC);\n\tCHECK_CAP(interleaved_dma, DMA_INTERLEAVE);\n\n#undef CHECK_CAP\n\n\tif (!device->device_tx_status) {\n\t\tdev_err(device->dev, \"Device tx_status is not defined\\n\");\n\t\treturn -EIO;\n\t}\n\n\n\tif (!device->device_issue_pending) {\n\t\tdev_err(device->dev, \"Device issue_pending is not defined\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (!device->device_release)\n\t\tdev_dbg(device->dev,\n\t\t\t \"WARN: Device release is not defined so it is not safe to unbind this driver while in use\\n\");\n\n\tkref_init(&device->ref);\n\n\t \n\tif (device_has_all_tx_types(device))\n\t\tdma_cap_set(DMA_ASYNC_TX, device->cap_mask);\n\n\trc = get_dma_id(device);\n\tif (rc != 0)\n\t\treturn rc;\n\n\tida_init(&device->chan_ida);\n\n\t \n\tlist_for_each_entry(chan, &device->channels, device_node) {\n\t\trc = __dma_async_device_channel_register(device, chan);\n\t\tif (rc < 0)\n\t\t\tgoto err_out;\n\t}\n\n\tmutex_lock(&dma_list_mutex);\n\t \n\tif (dmaengine_ref_count && !dma_has_cap(DMA_PRIVATE, device->cap_mask))\n\t\tlist_for_each_entry(chan, &device->channels, device_node) {\n\t\t\t \n\t\t\tif (dma_chan_get(chan) == -ENODEV) {\n\t\t\t\t \n\t\t\t\trc = -ENODEV;\n\t\t\t\tmutex_unlock(&dma_list_mutex);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\tlist_add_tail_rcu(&device->global_node, &dma_device_list);\n\tif (dma_has_cap(DMA_PRIVATE, device->cap_mask))\n\t\tdevice->privatecnt++;\t \n\tdma_channel_rebalance();\n\tmutex_unlock(&dma_list_mutex);\n\n\tdmaengine_debug_register(device);\n\n\treturn 0;\n\nerr_out:\n\t \n\tif (!device->chancnt) {\n\t\tida_free(&dma_ida, device->dev_id);\n\t\treturn rc;\n\t}\n\n\tlist_for_each_entry(chan, &device->channels, device_node) {\n\t\tif (chan->local == NULL)\n\t\t\tcontinue;\n\t\tmutex_lock(&dma_list_mutex);\n\t\tchan->dev->chan = NULL;\n\t\tmutex_unlock(&dma_list_mutex);\n\t\tdevice_unregister(&chan->dev->device);\n\t\tfree_percpu(chan->local);\n\t}\n\treturn rc;\n}\nEXPORT_SYMBOL(dma_async_device_register);\n\n \nvoid dma_async_device_unregister(struct dma_device *device)\n{\n\tstruct dma_chan *chan, *n;\n\n\tdmaengine_debug_unregister(device);\n\n\tlist_for_each_entry_safe(chan, n, &device->channels, device_node)\n\t\t__dma_async_device_channel_unregister(device, chan);\n\n\tmutex_lock(&dma_list_mutex);\n\t \n\tdma_cap_set(DMA_PRIVATE, device->cap_mask);\n\tdma_channel_rebalance();\n\tida_free(&dma_ida, device->dev_id);\n\tdma_device_put(device);\n\tmutex_unlock(&dma_list_mutex);\n}\nEXPORT_SYMBOL(dma_async_device_unregister);\n\nstatic void dmaenginem_async_device_unregister(void *device)\n{\n\tdma_async_device_unregister(device);\n}\n\n \nint dmaenginem_async_device_register(struct dma_device *device)\n{\n\tint ret;\n\n\tret = dma_async_device_register(device);\n\tif (ret)\n\t\treturn ret;\n\n\treturn devm_add_action_or_reset(device->dev, dmaenginem_async_device_unregister, device);\n}\nEXPORT_SYMBOL(dmaenginem_async_device_register);\n\nstruct dmaengine_unmap_pool {\n\tstruct kmem_cache *cache;\n\tconst char *name;\n\tmempool_t *pool;\n\tsize_t size;\n};\n\n#define __UNMAP_POOL(x) { .size = x, .name = \"dmaengine-unmap-\" __stringify(x) }\nstatic struct dmaengine_unmap_pool unmap_pool[] = {\n\t__UNMAP_POOL(2),\n\t#if IS_ENABLED(CONFIG_DMA_ENGINE_RAID)\n\t__UNMAP_POOL(16),\n\t__UNMAP_POOL(128),\n\t__UNMAP_POOL(256),\n\t#endif\n};\n\nstatic struct dmaengine_unmap_pool *__get_unmap_pool(int nr)\n{\n\tint order = get_count_order(nr);\n\n\tswitch (order) {\n\tcase 0 ... 1:\n\t\treturn &unmap_pool[0];\n#if IS_ENABLED(CONFIG_DMA_ENGINE_RAID)\n\tcase 2 ... 4:\n\t\treturn &unmap_pool[1];\n\tcase 5 ... 7:\n\t\treturn &unmap_pool[2];\n\tcase 8:\n\t\treturn &unmap_pool[3];\n#endif\n\tdefault:\n\t\tBUG();\n\t\treturn NULL;\n\t}\n}\n\nstatic void dmaengine_unmap(struct kref *kref)\n{\n\tstruct dmaengine_unmap_data *unmap = container_of(kref, typeof(*unmap), kref);\n\tstruct device *dev = unmap->dev;\n\tint cnt, i;\n\n\tcnt = unmap->to_cnt;\n\tfor (i = 0; i < cnt; i++)\n\t\tdma_unmap_page(dev, unmap->addr[i], unmap->len,\n\t\t\t       DMA_TO_DEVICE);\n\tcnt += unmap->from_cnt;\n\tfor (; i < cnt; i++)\n\t\tdma_unmap_page(dev, unmap->addr[i], unmap->len,\n\t\t\t       DMA_FROM_DEVICE);\n\tcnt += unmap->bidi_cnt;\n\tfor (; i < cnt; i++) {\n\t\tif (unmap->addr[i] == 0)\n\t\t\tcontinue;\n\t\tdma_unmap_page(dev, unmap->addr[i], unmap->len,\n\t\t\t       DMA_BIDIRECTIONAL);\n\t}\n\tcnt = unmap->map_cnt;\n\tmempool_free(unmap, __get_unmap_pool(cnt)->pool);\n}\n\nvoid dmaengine_unmap_put(struct dmaengine_unmap_data *unmap)\n{\n\tif (unmap)\n\t\tkref_put(&unmap->kref, dmaengine_unmap);\n}\nEXPORT_SYMBOL_GPL(dmaengine_unmap_put);\n\nstatic void dmaengine_destroy_unmap_pool(void)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(unmap_pool); i++) {\n\t\tstruct dmaengine_unmap_pool *p = &unmap_pool[i];\n\n\t\tmempool_destroy(p->pool);\n\t\tp->pool = NULL;\n\t\tkmem_cache_destroy(p->cache);\n\t\tp->cache = NULL;\n\t}\n}\n\nstatic int __init dmaengine_init_unmap_pool(void)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(unmap_pool); i++) {\n\t\tstruct dmaengine_unmap_pool *p = &unmap_pool[i];\n\t\tsize_t size;\n\n\t\tsize = sizeof(struct dmaengine_unmap_data) +\n\t\t       sizeof(dma_addr_t) * p->size;\n\n\t\tp->cache = kmem_cache_create(p->name, size, 0,\n\t\t\t\t\t     SLAB_HWCACHE_ALIGN, NULL);\n\t\tif (!p->cache)\n\t\t\tbreak;\n\t\tp->pool = mempool_create_slab_pool(1, p->cache);\n\t\tif (!p->pool)\n\t\t\tbreak;\n\t}\n\n\tif (i == ARRAY_SIZE(unmap_pool))\n\t\treturn 0;\n\n\tdmaengine_destroy_unmap_pool();\n\treturn -ENOMEM;\n}\n\nstruct dmaengine_unmap_data *\ndmaengine_get_unmap_data(struct device *dev, int nr, gfp_t flags)\n{\n\tstruct dmaengine_unmap_data *unmap;\n\n\tunmap = mempool_alloc(__get_unmap_pool(nr)->pool, flags);\n\tif (!unmap)\n\t\treturn NULL;\n\n\tmemset(unmap, 0, sizeof(*unmap));\n\tkref_init(&unmap->kref);\n\tunmap->dev = dev;\n\tunmap->map_cnt = nr;\n\n\treturn unmap;\n}\nEXPORT_SYMBOL(dmaengine_get_unmap_data);\n\nvoid dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx,\n\tstruct dma_chan *chan)\n{\n\ttx->chan = chan;\n\t#ifdef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH\n\tspin_lock_init(&tx->lock);\n\t#endif\n}\nEXPORT_SYMBOL(dma_async_tx_descriptor_init);\n\nstatic inline int desc_check_and_set_metadata_mode(\n\tstruct dma_async_tx_descriptor *desc, enum dma_desc_metadata_mode mode)\n{\n\t \n\tif (!desc->desc_metadata_mode) {\n\t\tif (dmaengine_is_metadata_mode_supported(desc->chan, mode))\n\t\t\tdesc->desc_metadata_mode = mode;\n\t\telse\n\t\t\treturn -ENOTSUPP;\n\t} else if (desc->desc_metadata_mode != mode) {\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint dmaengine_desc_attach_metadata(struct dma_async_tx_descriptor *desc,\n\t\t\t\t   void *data, size_t len)\n{\n\tint ret;\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tret = desc_check_and_set_metadata_mode(desc, DESC_METADATA_CLIENT);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!desc->metadata_ops || !desc->metadata_ops->attach)\n\t\treturn -ENOTSUPP;\n\n\treturn desc->metadata_ops->attach(desc, data, len);\n}\nEXPORT_SYMBOL_GPL(dmaengine_desc_attach_metadata);\n\nvoid *dmaengine_desc_get_metadata_ptr(struct dma_async_tx_descriptor *desc,\n\t\t\t\t      size_t *payload_len, size_t *max_len)\n{\n\tint ret;\n\n\tif (!desc)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tret = desc_check_and_set_metadata_mode(desc, DESC_METADATA_ENGINE);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tif (!desc->metadata_ops || !desc->metadata_ops->get_ptr)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\treturn desc->metadata_ops->get_ptr(desc, payload_len, max_len);\n}\nEXPORT_SYMBOL_GPL(dmaengine_desc_get_metadata_ptr);\n\nint dmaengine_desc_set_metadata_len(struct dma_async_tx_descriptor *desc,\n\t\t\t\t    size_t payload_len)\n{\n\tint ret;\n\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tret = desc_check_and_set_metadata_mode(desc, DESC_METADATA_ENGINE);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!desc->metadata_ops || !desc->metadata_ops->set_len)\n\t\treturn -ENOTSUPP;\n\n\treturn desc->metadata_ops->set_len(desc, payload_len);\n}\nEXPORT_SYMBOL_GPL(dmaengine_desc_set_metadata_len);\n\n \nenum dma_status\ndma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)\n{\n\tunsigned long dma_sync_wait_timeout = jiffies + msecs_to_jiffies(5000);\n\n\tif (!tx)\n\t\treturn DMA_COMPLETE;\n\n\twhile (tx->cookie == -EBUSY) {\n\t\tif (time_after_eq(jiffies, dma_sync_wait_timeout)) {\n\t\t\tdev_err(tx->chan->device->dev,\n\t\t\t\t\"%s timeout waiting for descriptor submission\\n\",\n\t\t\t\t__func__);\n\t\t\treturn DMA_ERROR;\n\t\t}\n\t\tcpu_relax();\n\t}\n\treturn dma_sync_wait(tx->chan, tx->cookie);\n}\nEXPORT_SYMBOL_GPL(dma_wait_for_async_tx);\n\n \nvoid dma_run_dependencies(struct dma_async_tx_descriptor *tx)\n{\n\tstruct dma_async_tx_descriptor *dep = txd_next(tx);\n\tstruct dma_async_tx_descriptor *dep_next;\n\tstruct dma_chan *chan;\n\n\tif (!dep)\n\t\treturn;\n\n\t \n\ttxd_clear_next(tx);\n\tchan = dep->chan;\n\n\t \n\tfor (; dep; dep = dep_next) {\n\t\ttxd_lock(dep);\n\t\ttxd_clear_parent(dep);\n\t\tdep_next = txd_next(dep);\n\t\tif (dep_next && dep_next->chan == chan)\n\t\t\ttxd_clear_next(dep);  \n\t\telse\n\t\t\tdep_next = NULL;  \n\t\ttxd_unlock(dep);\n\n\t\tdep->tx_submit(dep);\n\t}\n\n\tchan->device->device_issue_pending(chan);\n}\nEXPORT_SYMBOL_GPL(dma_run_dependencies);\n\nstatic int __init dma_bus_init(void)\n{\n\tint err = dmaengine_init_unmap_pool();\n\n\tif (err)\n\t\treturn err;\n\n\terr = class_register(&dma_devclass);\n\tif (!err)\n\t\tdmaengine_debugfs_init();\n\n\treturn err;\n}\narch_initcall(dma_bus_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}