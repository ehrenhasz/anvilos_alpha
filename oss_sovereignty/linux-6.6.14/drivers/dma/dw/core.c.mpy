{
  "module_name": "core.c",
  "hash_id": "03c3276518c8d1f4ef2bfb9773576f6351b99b6e30ffc08c7513db69aa8da6cb",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/dw/core.c",
  "human_readable_source": "\n \n\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/pm_runtime.h>\n\n#include \"../dmaengine.h\"\n#include \"internal.h\"\n\n \n\n \n#define DW_DMA_BUSWIDTHS\t\t\t  \\\n\tBIT(DMA_SLAVE_BUSWIDTH_UNDEFINED)\t| \\\n\tBIT(DMA_SLAVE_BUSWIDTH_1_BYTE)\t\t| \\\n\tBIT(DMA_SLAVE_BUSWIDTH_2_BYTES)\t\t| \\\n\tBIT(DMA_SLAVE_BUSWIDTH_4_BYTES)\n\n \n\nstatic struct device *chan2dev(struct dma_chan *chan)\n{\n\treturn &chan->dev->device;\n}\n\nstatic struct dw_desc *dwc_first_active(struct dw_dma_chan *dwc)\n{\n\treturn to_dw_desc(dwc->active_list.next);\n}\n\nstatic dma_cookie_t dwc_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct dw_desc\t\t*desc = txd_to_dw_desc(tx);\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(tx->chan);\n\tdma_cookie_t\t\tcookie;\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\tcookie = dma_cookie_assign(tx);\n\n\t \n\n\tlist_add_tail(&desc->desc_node, &dwc->queue);\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n\tdev_vdbg(chan2dev(tx->chan), \"%s: queued %u\\n\",\n\t\t __func__, desc->txd.cookie);\n\n\treturn cookie;\n}\n\nstatic struct dw_desc *dwc_desc_get(struct dw_dma_chan *dwc)\n{\n\tstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\n\tstruct dw_desc *desc;\n\tdma_addr_t phys;\n\n\tdesc = dma_pool_zalloc(dw->desc_pool, GFP_ATOMIC, &phys);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdwc->descs_allocated++;\n\tINIT_LIST_HEAD(&desc->tx_list);\n\tdma_async_tx_descriptor_init(&desc->txd, &dwc->chan);\n\tdesc->txd.tx_submit = dwc_tx_submit;\n\tdesc->txd.flags = DMA_CTRL_ACK;\n\tdesc->txd.phys = phys;\n\treturn desc;\n}\n\nstatic void dwc_desc_put(struct dw_dma_chan *dwc, struct dw_desc *desc)\n{\n\tstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\n\tstruct dw_desc *child, *_next;\n\n\tif (unlikely(!desc))\n\t\treturn;\n\n\tlist_for_each_entry_safe(child, _next, &desc->tx_list, desc_node) {\n\t\tlist_del(&child->desc_node);\n\t\tdma_pool_free(dw->desc_pool, child, child->txd.phys);\n\t\tdwc->descs_allocated--;\n\t}\n\n\tdma_pool_free(dw->desc_pool, desc, desc->txd.phys);\n\tdwc->descs_allocated--;\n}\n\nstatic void dwc_initialize(struct dw_dma_chan *dwc)\n{\n\tstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\n\n\tdw->initialize_chan(dwc);\n\n\t \n\tchannel_set_bit(dw, MASK.XFER, dwc->mask);\n\tchannel_set_bit(dw, MASK.ERROR, dwc->mask);\n}\n\n \n\nstatic inline void dwc_dump_chan_regs(struct dw_dma_chan *dwc)\n{\n\tdev_err(chan2dev(&dwc->chan),\n\t\t\"  SAR: 0x%x DAR: 0x%x LLP: 0x%x CTL: 0x%x:%08x\\n\",\n\t\tchannel_readl(dwc, SAR),\n\t\tchannel_readl(dwc, DAR),\n\t\tchannel_readl(dwc, LLP),\n\t\tchannel_readl(dwc, CTL_HI),\n\t\tchannel_readl(dwc, CTL_LO));\n}\n\nstatic inline void dwc_chan_disable(struct dw_dma *dw, struct dw_dma_chan *dwc)\n{\n\tchannel_clear_bit(dw, CH_EN, dwc->mask);\n\twhile (dma_readl(dw, CH_EN) & dwc->mask)\n\t\tcpu_relax();\n}\n\n \n\n \nstatic inline void dwc_do_single_block(struct dw_dma_chan *dwc,\n\t\t\t\t       struct dw_desc *desc)\n{\n\tstruct dw_dma\t*dw = to_dw_dma(dwc->chan.device);\n\tu32\t\tctllo;\n\n\t \n\tctllo = lli_read(desc, ctllo) | DWC_CTLL_INT_EN;\n\n\tchannel_writel(dwc, SAR, lli_read(desc, sar));\n\tchannel_writel(dwc, DAR, lli_read(desc, dar));\n\tchannel_writel(dwc, CTL_LO, ctllo);\n\tchannel_writel(dwc, CTL_HI, lli_read(desc, ctlhi));\n\tchannel_set_bit(dw, CH_EN, dwc->mask);\n\n\t \n\tdwc->tx_node_active = dwc->tx_node_active->next;\n}\n\n \nstatic void dwc_dostart(struct dw_dma_chan *dwc, struct dw_desc *first)\n{\n\tstruct dw_dma\t*dw = to_dw_dma(dwc->chan.device);\n\tu8\t\tlms = DWC_LLP_LMS(dwc->dws.m_master);\n\tunsigned long\twas_soft_llp;\n\n\t \n\tif (dma_readl(dw, CH_EN) & dwc->mask) {\n\t\tdev_err(chan2dev(&dwc->chan),\n\t\t\t\"%s: BUG: Attempted to start non-idle channel\\n\",\n\t\t\t__func__);\n\t\tdwc_dump_chan_regs(dwc);\n\n\t\t \n\t\treturn;\n\t}\n\n\tif (dwc->nollp) {\n\t\twas_soft_llp = test_and_set_bit(DW_DMA_IS_SOFT_LLP,\n\t\t\t\t\t\t&dwc->flags);\n\t\tif (was_soft_llp) {\n\t\t\tdev_err(chan2dev(&dwc->chan),\n\t\t\t\t\"BUG: Attempted to start new LLP transfer inside ongoing one\\n\");\n\t\t\treturn;\n\t\t}\n\n\t\tdwc_initialize(dwc);\n\n\t\tfirst->residue = first->total_len;\n\t\tdwc->tx_node_active = &first->tx_list;\n\n\t\t \n\t\tdwc_do_single_block(dwc, first);\n\n\t\treturn;\n\t}\n\n\tdwc_initialize(dwc);\n\n\tchannel_writel(dwc, LLP, first->txd.phys | lms);\n\tchannel_writel(dwc, CTL_LO, DWC_CTLL_LLP_D_EN | DWC_CTLL_LLP_S_EN);\n\tchannel_writel(dwc, CTL_HI, 0);\n\tchannel_set_bit(dw, CH_EN, dwc->mask);\n}\n\nstatic void dwc_dostart_first_queued(struct dw_dma_chan *dwc)\n{\n\tstruct dw_desc *desc;\n\n\tif (list_empty(&dwc->queue))\n\t\treturn;\n\n\tlist_move(dwc->queue.next, &dwc->active_list);\n\tdesc = dwc_first_active(dwc);\n\tdev_vdbg(chan2dev(&dwc->chan), \"%s: started %u\\n\", __func__, desc->txd.cookie);\n\tdwc_dostart(dwc, desc);\n}\n\n \n\nstatic void\ndwc_descriptor_complete(struct dw_dma_chan *dwc, struct dw_desc *desc,\n\t\tbool callback_required)\n{\n\tstruct dma_async_tx_descriptor\t*txd = &desc->txd;\n\tstruct dw_desc\t\t\t*child;\n\tunsigned long\t\t\tflags;\n\tstruct dmaengine_desc_callback\tcb;\n\n\tdev_vdbg(chan2dev(&dwc->chan), \"descriptor %u complete\\n\", txd->cookie);\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\tdma_cookie_complete(txd);\n\tif (callback_required)\n\t\tdmaengine_desc_get_callback(txd, &cb);\n\telse\n\t\tmemset(&cb, 0, sizeof(cb));\n\n\t \n\tlist_for_each_entry(child, &desc->tx_list, desc_node)\n\t\tasync_tx_ack(&child->txd);\n\tasync_tx_ack(&desc->txd);\n\tdwc_desc_put(dwc, desc);\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n\n\tdmaengine_desc_callback_invoke(&cb, NULL);\n}\n\nstatic void dwc_complete_all(struct dw_dma *dw, struct dw_dma_chan *dwc)\n{\n\tstruct dw_desc *desc, *_desc;\n\tLIST_HEAD(list);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\tif (dma_readl(dw, CH_EN) & dwc->mask) {\n\t\tdev_err(chan2dev(&dwc->chan),\n\t\t\t\"BUG: XFER bit set, but channel not idle!\\n\");\n\n\t\t \n\t\tdwc_chan_disable(dw, dwc);\n\t}\n\n\t \n\tlist_splice_init(&dwc->active_list, &list);\n\tdwc_dostart_first_queued(dwc);\n\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n\n\tlist_for_each_entry_safe(desc, _desc, &list, desc_node)\n\t\tdwc_descriptor_complete(dwc, desc, true);\n}\n\n \nstatic inline u32 dwc_get_sent(struct dw_dma_chan *dwc)\n{\n\tstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\n\tu32 ctlhi = channel_readl(dwc, CTL_HI);\n\tu32 ctllo = channel_readl(dwc, CTL_LO);\n\n\treturn dw->block2bytes(dwc, ctlhi, ctllo >> 4 & 7);\n}\n\nstatic void dwc_scan_descriptors(struct dw_dma *dw, struct dw_dma_chan *dwc)\n{\n\tdma_addr_t llp;\n\tstruct dw_desc *desc, *_desc;\n\tstruct dw_desc *child;\n\tu32 status_xfer;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\tllp = channel_readl(dwc, LLP);\n\tstatus_xfer = dma_readl(dw, RAW.XFER);\n\n\tif (status_xfer & dwc->mask) {\n\t\t \n\t\tdma_writel(dw, CLEAR.XFER, dwc->mask);\n\n\t\tif (test_bit(DW_DMA_IS_SOFT_LLP, &dwc->flags)) {\n\t\t\tstruct list_head *head, *active = dwc->tx_node_active;\n\n\t\t\t \n\t\t\tdesc = dwc_first_active(dwc);\n\n\t\t\thead = &desc->tx_list;\n\t\t\tif (active != head) {\n\t\t\t\t \n\t\t\t\tif (active == head->next)\n\t\t\t\t\tdesc->residue -= desc->len;\n\t\t\t\telse\n\t\t\t\t\tdesc->residue -= to_dw_desc(active->prev)->len;\n\n\t\t\t\tchild = to_dw_desc(active);\n\n\t\t\t\t \n\t\t\t\tdwc_do_single_block(dwc, child);\n\n\t\t\t\tspin_unlock_irqrestore(&dwc->lock, flags);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t \n\t\t\tclear_bit(DW_DMA_IS_SOFT_LLP, &dwc->flags);\n\t\t}\n\n\t\tspin_unlock_irqrestore(&dwc->lock, flags);\n\n\t\tdwc_complete_all(dw, dwc);\n\t\treturn;\n\t}\n\n\tif (list_empty(&dwc->active_list)) {\n\t\tspin_unlock_irqrestore(&dwc->lock, flags);\n\t\treturn;\n\t}\n\n\tif (test_bit(DW_DMA_IS_SOFT_LLP, &dwc->flags)) {\n\t\tdev_vdbg(chan2dev(&dwc->chan), \"%s: soft LLP mode\\n\", __func__);\n\t\tspin_unlock_irqrestore(&dwc->lock, flags);\n\t\treturn;\n\t}\n\n\tdev_vdbg(chan2dev(&dwc->chan), \"%s: llp=%pad\\n\", __func__, &llp);\n\n\tlist_for_each_entry_safe(desc, _desc, &dwc->active_list, desc_node) {\n\t\t \n\t\tdesc->residue = desc->total_len;\n\n\t\t \n\t\tif (desc->txd.phys == DWC_LLP_LOC(llp)) {\n\t\t\tspin_unlock_irqrestore(&dwc->lock, flags);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tif (lli_read(desc, llp) == llp) {\n\t\t\t \n\t\t\tdesc->residue -= dwc_get_sent(dwc);\n\t\t\tspin_unlock_irqrestore(&dwc->lock, flags);\n\t\t\treturn;\n\t\t}\n\n\t\tdesc->residue -= desc->len;\n\t\tlist_for_each_entry(child, &desc->tx_list, desc_node) {\n\t\t\tif (lli_read(child, llp) == llp) {\n\t\t\t\t \n\t\t\t\tdesc->residue -= dwc_get_sent(dwc);\n\t\t\t\tspin_unlock_irqrestore(&dwc->lock, flags);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tdesc->residue -= child->len;\n\t\t}\n\n\t\t \n\t\tspin_unlock_irqrestore(&dwc->lock, flags);\n\t\tdwc_descriptor_complete(dwc, desc, true);\n\t\tspin_lock_irqsave(&dwc->lock, flags);\n\t}\n\n\tdev_err(chan2dev(&dwc->chan),\n\t\t\"BUG: All descriptors done, but channel not idle!\\n\");\n\n\t \n\tdwc_chan_disable(dw, dwc);\n\n\tdwc_dostart_first_queued(dwc);\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n}\n\nstatic inline void dwc_dump_lli(struct dw_dma_chan *dwc, struct dw_desc *desc)\n{\n\tdev_crit(chan2dev(&dwc->chan), \"  desc: s0x%x d0x%x l0x%x c0x%x:%x\\n\",\n\t\t lli_read(desc, sar),\n\t\t lli_read(desc, dar),\n\t\t lli_read(desc, llp),\n\t\t lli_read(desc, ctlhi),\n\t\t lli_read(desc, ctllo));\n}\n\nstatic void dwc_handle_error(struct dw_dma *dw, struct dw_dma_chan *dwc)\n{\n\tstruct dw_desc *bad_desc;\n\tstruct dw_desc *child;\n\tunsigned long flags;\n\n\tdwc_scan_descriptors(dw, dwc);\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\n\t \n\tbad_desc = dwc_first_active(dwc);\n\tlist_del_init(&bad_desc->desc_node);\n\tlist_move(dwc->queue.next, dwc->active_list.prev);\n\n\t \n\tdma_writel(dw, CLEAR.ERROR, dwc->mask);\n\tif (!list_empty(&dwc->active_list))\n\t\tdwc_dostart(dwc, dwc_first_active(dwc));\n\n\t \n\tdev_WARN(chan2dev(&dwc->chan), \"Bad descriptor submitted for DMA!\\n\"\n\t\t\t\t       \"  cookie: %d\\n\", bad_desc->txd.cookie);\n\tdwc_dump_lli(dwc, bad_desc);\n\tlist_for_each_entry(child, &bad_desc->tx_list, desc_node)\n\t\tdwc_dump_lli(dwc, child);\n\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n\n\t \n\tdwc_descriptor_complete(dwc, bad_desc, true);\n}\n\nstatic void dw_dma_tasklet(struct tasklet_struct *t)\n{\n\tstruct dw_dma *dw = from_tasklet(dw, t, tasklet);\n\tstruct dw_dma_chan *dwc;\n\tu32 status_xfer;\n\tu32 status_err;\n\tunsigned int i;\n\n\tstatus_xfer = dma_readl(dw, RAW.XFER);\n\tstatus_err = dma_readl(dw, RAW.ERROR);\n\n\tdev_vdbg(dw->dma.dev, \"%s: status_err=%x\\n\", __func__, status_err);\n\n\tfor (i = 0; i < dw->dma.chancnt; i++) {\n\t\tdwc = &dw->chan[i];\n\t\tif (test_bit(DW_DMA_IS_CYCLIC, &dwc->flags))\n\t\t\tdev_vdbg(dw->dma.dev, \"Cyclic xfer is not implemented\\n\");\n\t\telse if (status_err & (1 << i))\n\t\t\tdwc_handle_error(dw, dwc);\n\t\telse if (status_xfer & (1 << i))\n\t\t\tdwc_scan_descriptors(dw, dwc);\n\t}\n\n\t \n\tchannel_set_bit(dw, MASK.XFER, dw->all_chan_mask);\n\tchannel_set_bit(dw, MASK.ERROR, dw->all_chan_mask);\n}\n\nstatic irqreturn_t dw_dma_interrupt(int irq, void *dev_id)\n{\n\tstruct dw_dma *dw = dev_id;\n\tu32 status;\n\n\t \n\tif (!dw->in_use)\n\t\treturn IRQ_NONE;\n\n\tstatus = dma_readl(dw, STATUS_INT);\n\tdev_vdbg(dw->dma.dev, \"%s: status=0x%x\\n\", __func__, status);\n\n\t \n\tif (!status)\n\t\treturn IRQ_NONE;\n\n\t \n\tchannel_clear_bit(dw, MASK.XFER, dw->all_chan_mask);\n\tchannel_clear_bit(dw, MASK.BLOCK, dw->all_chan_mask);\n\tchannel_clear_bit(dw, MASK.ERROR, dw->all_chan_mask);\n\n\tstatus = dma_readl(dw, STATUS_INT);\n\tif (status) {\n\t\tdev_err(dw->dma.dev,\n\t\t\t\"BUG: Unexpected interrupts pending: 0x%x\\n\",\n\t\t\tstatus);\n\n\t\t \n\t\tchannel_clear_bit(dw, MASK.XFER, (1 << 8) - 1);\n\t\tchannel_clear_bit(dw, MASK.BLOCK, (1 << 8) - 1);\n\t\tchannel_clear_bit(dw, MASK.SRC_TRAN, (1 << 8) - 1);\n\t\tchannel_clear_bit(dw, MASK.DST_TRAN, (1 << 8) - 1);\n\t\tchannel_clear_bit(dw, MASK.ERROR, (1 << 8) - 1);\n\t}\n\n\ttasklet_schedule(&dw->tasklet);\n\n\treturn IRQ_HANDLED;\n}\n\n \n\nstatic struct dma_async_tx_descriptor *\ndwc_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\n\t\tsize_t len, unsigned long flags)\n{\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(chan);\n\tstruct dw_dma\t\t*dw = to_dw_dma(chan->device);\n\tstruct dw_desc\t\t*desc;\n\tstruct dw_desc\t\t*first;\n\tstruct dw_desc\t\t*prev;\n\tsize_t\t\t\txfer_count;\n\tsize_t\t\t\toffset;\n\tu8\t\t\tm_master = dwc->dws.m_master;\n\tunsigned int\t\tsrc_width;\n\tunsigned int\t\tdst_width;\n\tunsigned int\t\tdata_width = dw->pdata->data_width[m_master];\n\tu32\t\t\tctllo, ctlhi;\n\tu8\t\t\tlms = DWC_LLP_LMS(m_master);\n\n\tdev_vdbg(chan2dev(chan),\n\t\t\t\"%s: d%pad s%pad l0x%zx f0x%lx\\n\", __func__,\n\t\t\t&dest, &src, len, flags);\n\n\tif (unlikely(!len)) {\n\t\tdev_dbg(chan2dev(chan), \"%s: length is zero!\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tdwc->direction = DMA_MEM_TO_MEM;\n\n\tsrc_width = dst_width = __ffs(data_width | src | dest | len);\n\n\tctllo = dw->prepare_ctllo(dwc)\n\t\t\t| DWC_CTLL_DST_WIDTH(dst_width)\n\t\t\t| DWC_CTLL_SRC_WIDTH(src_width)\n\t\t\t| DWC_CTLL_DST_INC\n\t\t\t| DWC_CTLL_SRC_INC\n\t\t\t| DWC_CTLL_FC_M2M;\n\tprev = first = NULL;\n\n\tfor (offset = 0; offset < len; offset += xfer_count) {\n\t\tdesc = dwc_desc_get(dwc);\n\t\tif (!desc)\n\t\t\tgoto err_desc_get;\n\n\t\tctlhi = dw->bytes2block(dwc, len - offset, src_width, &xfer_count);\n\n\t\tlli_write(desc, sar, src + offset);\n\t\tlli_write(desc, dar, dest + offset);\n\t\tlli_write(desc, ctllo, ctllo);\n\t\tlli_write(desc, ctlhi, ctlhi);\n\t\tdesc->len = xfer_count;\n\n\t\tif (!first) {\n\t\t\tfirst = desc;\n\t\t} else {\n\t\t\tlli_write(prev, llp, desc->txd.phys | lms);\n\t\t\tlist_add_tail(&desc->desc_node, &first->tx_list);\n\t\t}\n\t\tprev = desc;\n\t}\n\n\tif (flags & DMA_PREP_INTERRUPT)\n\t\t \n\t\tlli_set(prev, ctllo, DWC_CTLL_INT_EN);\n\n\tprev->lli.llp = 0;\n\tlli_clear(prev, ctllo, DWC_CTLL_LLP_D_EN | DWC_CTLL_LLP_S_EN);\n\tfirst->txd.flags = flags;\n\tfirst->total_len = len;\n\n\treturn &first->txd;\n\nerr_desc_get:\n\tdwc_desc_put(dwc, first);\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor *\ndwc_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\n\t\tunsigned int sg_len, enum dma_transfer_direction direction,\n\t\tunsigned long flags, void *context)\n{\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(chan);\n\tstruct dw_dma\t\t*dw = to_dw_dma(chan->device);\n\tstruct dma_slave_config\t*sconfig = &dwc->dma_sconfig;\n\tstruct dw_desc\t\t*prev;\n\tstruct dw_desc\t\t*first;\n\tu32\t\t\tctllo, ctlhi;\n\tu8\t\t\tm_master = dwc->dws.m_master;\n\tu8\t\t\tlms = DWC_LLP_LMS(m_master);\n\tdma_addr_t\t\treg;\n\tunsigned int\t\treg_width;\n\tunsigned int\t\tmem_width;\n\tunsigned int\t\tdata_width = dw->pdata->data_width[m_master];\n\tunsigned int\t\ti;\n\tstruct scatterlist\t*sg;\n\tsize_t\t\t\ttotal_len = 0;\n\n\tdev_vdbg(chan2dev(chan), \"%s\\n\", __func__);\n\n\tif (unlikely(!is_slave_direction(direction) || !sg_len))\n\t\treturn NULL;\n\n\tdwc->direction = direction;\n\n\tprev = first = NULL;\n\n\tswitch (direction) {\n\tcase DMA_MEM_TO_DEV:\n\t\treg_width = __ffs(sconfig->dst_addr_width);\n\t\treg = sconfig->dst_addr;\n\t\tctllo = dw->prepare_ctllo(dwc)\n\t\t\t\t| DWC_CTLL_DST_WIDTH(reg_width)\n\t\t\t\t| DWC_CTLL_DST_FIX\n\t\t\t\t| DWC_CTLL_SRC_INC;\n\n\t\tctllo |= sconfig->device_fc ? DWC_CTLL_FC(DW_DMA_FC_P_M2P) :\n\t\t\tDWC_CTLL_FC(DW_DMA_FC_D_M2P);\n\n\t\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\t\tstruct dw_desc\t*desc;\n\t\t\tu32\t\tlen, mem;\n\t\t\tsize_t\t\tdlen;\n\n\t\t\tmem = sg_dma_address(sg);\n\t\t\tlen = sg_dma_len(sg);\n\n\t\t\tmem_width = __ffs(data_width | mem | len);\n\nslave_sg_todev_fill_desc:\n\t\t\tdesc = dwc_desc_get(dwc);\n\t\t\tif (!desc)\n\t\t\t\tgoto err_desc_get;\n\n\t\t\tctlhi = dw->bytes2block(dwc, len, mem_width, &dlen);\n\n\t\t\tlli_write(desc, sar, mem);\n\t\t\tlli_write(desc, dar, reg);\n\t\t\tlli_write(desc, ctlhi, ctlhi);\n\t\t\tlli_write(desc, ctllo, ctllo | DWC_CTLL_SRC_WIDTH(mem_width));\n\t\t\tdesc->len = dlen;\n\n\t\t\tif (!first) {\n\t\t\t\tfirst = desc;\n\t\t\t} else {\n\t\t\t\tlli_write(prev, llp, desc->txd.phys | lms);\n\t\t\t\tlist_add_tail(&desc->desc_node, &first->tx_list);\n\t\t\t}\n\t\t\tprev = desc;\n\n\t\t\tmem += dlen;\n\t\t\tlen -= dlen;\n\t\t\ttotal_len += dlen;\n\n\t\t\tif (len)\n\t\t\t\tgoto slave_sg_todev_fill_desc;\n\t\t}\n\t\tbreak;\n\tcase DMA_DEV_TO_MEM:\n\t\treg_width = __ffs(sconfig->src_addr_width);\n\t\treg = sconfig->src_addr;\n\t\tctllo = dw->prepare_ctllo(dwc)\n\t\t\t\t| DWC_CTLL_SRC_WIDTH(reg_width)\n\t\t\t\t| DWC_CTLL_DST_INC\n\t\t\t\t| DWC_CTLL_SRC_FIX;\n\n\t\tctllo |= sconfig->device_fc ? DWC_CTLL_FC(DW_DMA_FC_P_P2M) :\n\t\t\tDWC_CTLL_FC(DW_DMA_FC_D_P2M);\n\n\t\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\t\tstruct dw_desc\t*desc;\n\t\t\tu32\t\tlen, mem;\n\t\t\tsize_t\t\tdlen;\n\n\t\t\tmem = sg_dma_address(sg);\n\t\t\tlen = sg_dma_len(sg);\n\nslave_sg_fromdev_fill_desc:\n\t\t\tdesc = dwc_desc_get(dwc);\n\t\t\tif (!desc)\n\t\t\t\tgoto err_desc_get;\n\n\t\t\tctlhi = dw->bytes2block(dwc, len, reg_width, &dlen);\n\n\t\t\tlli_write(desc, sar, reg);\n\t\t\tlli_write(desc, dar, mem);\n\t\t\tlli_write(desc, ctlhi, ctlhi);\n\t\t\tmem_width = __ffs(data_width | mem);\n\t\t\tlli_write(desc, ctllo, ctllo | DWC_CTLL_DST_WIDTH(mem_width));\n\t\t\tdesc->len = dlen;\n\n\t\t\tif (!first) {\n\t\t\t\tfirst = desc;\n\t\t\t} else {\n\t\t\t\tlli_write(prev, llp, desc->txd.phys | lms);\n\t\t\t\tlist_add_tail(&desc->desc_node, &first->tx_list);\n\t\t\t}\n\t\t\tprev = desc;\n\n\t\t\tmem += dlen;\n\t\t\tlen -= dlen;\n\t\t\ttotal_len += dlen;\n\n\t\t\tif (len)\n\t\t\t\tgoto slave_sg_fromdev_fill_desc;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn NULL;\n\t}\n\n\tif (flags & DMA_PREP_INTERRUPT)\n\t\t \n\t\tlli_set(prev, ctllo, DWC_CTLL_INT_EN);\n\n\tprev->lli.llp = 0;\n\tlli_clear(prev, ctllo, DWC_CTLL_LLP_D_EN | DWC_CTLL_LLP_S_EN);\n\tfirst->total_len = total_len;\n\n\treturn &first->txd;\n\nerr_desc_get:\n\tdev_err(chan2dev(chan),\n\t\t\"not enough descriptors available. Direction %d\\n\", direction);\n\tdwc_desc_put(dwc, first);\n\treturn NULL;\n}\n\nbool dw_dma_filter(struct dma_chan *chan, void *param)\n{\n\tstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\n\tstruct dw_dma_slave *dws = param;\n\n\tif (dws->dma_dev != chan->device->dev)\n\t\treturn false;\n\n\t \n\tif (dws->channels && !(dws->channels & dwc->mask))\n\t\treturn false;\n\n\t \n\tmemcpy(&dwc->dws, dws, sizeof(struct dw_dma_slave));\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(dw_dma_filter);\n\nstatic int dwc_config(struct dma_chan *chan, struct dma_slave_config *sconfig)\n{\n\tstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\n\tstruct dw_dma *dw = to_dw_dma(chan->device);\n\n\tmemcpy(&dwc->dma_sconfig, sconfig, sizeof(*sconfig));\n\n\tdwc->dma_sconfig.src_maxburst =\n\t\tclamp(dwc->dma_sconfig.src_maxburst, 0U, dwc->max_burst);\n\tdwc->dma_sconfig.dst_maxburst =\n\t\tclamp(dwc->dma_sconfig.dst_maxburst, 0U, dwc->max_burst);\n\n\tdw->encode_maxburst(dwc, &dwc->dma_sconfig.src_maxburst);\n\tdw->encode_maxburst(dwc, &dwc->dma_sconfig.dst_maxburst);\n\n\treturn 0;\n}\n\nstatic void dwc_chan_pause(struct dw_dma_chan *dwc, bool drain)\n{\n\tstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\n\tunsigned int\t\tcount = 20;\t \n\n\tdw->suspend_chan(dwc, drain);\n\n\twhile (!(channel_readl(dwc, CFG_LO) & DWC_CFGL_FIFO_EMPTY) && count--)\n\t\tudelay(2);\n\n\tset_bit(DW_DMA_IS_PAUSED, &dwc->flags);\n}\n\nstatic int dwc_pause(struct dma_chan *chan)\n{\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(chan);\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\tdwc_chan_pause(dwc, false);\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n\n\treturn 0;\n}\n\nstatic inline void dwc_chan_resume(struct dw_dma_chan *dwc, bool drain)\n{\n\tstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\n\n\tdw->resume_chan(dwc, drain);\n\n\tclear_bit(DW_DMA_IS_PAUSED, &dwc->flags);\n}\n\nstatic int dwc_resume(struct dma_chan *chan)\n{\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(chan);\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\n\tif (test_bit(DW_DMA_IS_PAUSED, &dwc->flags))\n\t\tdwc_chan_resume(dwc, false);\n\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n\n\treturn 0;\n}\n\nstatic int dwc_terminate_all(struct dma_chan *chan)\n{\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(chan);\n\tstruct dw_dma\t\t*dw = to_dw_dma(chan->device);\n\tstruct dw_desc\t\t*desc, *_desc;\n\tunsigned long\t\tflags;\n\tLIST_HEAD(list);\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\n\tclear_bit(DW_DMA_IS_SOFT_LLP, &dwc->flags);\n\n\tdwc_chan_pause(dwc, true);\n\n\tdwc_chan_disable(dw, dwc);\n\n\tdwc_chan_resume(dwc, true);\n\n\t \n\tlist_splice_init(&dwc->queue, &list);\n\tlist_splice_init(&dwc->active_list, &list);\n\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n\n\t \n\tlist_for_each_entry_safe(desc, _desc, &list, desc_node)\n\t\tdwc_descriptor_complete(dwc, desc, false);\n\n\treturn 0;\n}\n\nstatic struct dw_desc *dwc_find_desc(struct dw_dma_chan *dwc, dma_cookie_t c)\n{\n\tstruct dw_desc *desc;\n\n\tlist_for_each_entry(desc, &dwc->active_list, desc_node)\n\t\tif (desc->txd.cookie == c)\n\t\t\treturn desc;\n\n\treturn NULL;\n}\n\nstatic u32 dwc_get_residue_and_status(struct dw_dma_chan *dwc, dma_cookie_t cookie,\n\t\t\t\t      enum dma_status *status)\n{\n\tstruct dw_desc *desc;\n\tunsigned long flags;\n\tu32 residue;\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\n\tdesc = dwc_find_desc(dwc, cookie);\n\tif (desc) {\n\t\tif (desc == dwc_first_active(dwc)) {\n\t\t\tresidue = desc->residue;\n\t\t\tif (test_bit(DW_DMA_IS_SOFT_LLP, &dwc->flags) && residue)\n\t\t\t\tresidue -= dwc_get_sent(dwc);\n\t\t\tif (test_bit(DW_DMA_IS_PAUSED, &dwc->flags))\n\t\t\t\t*status = DMA_PAUSED;\n\t\t} else {\n\t\t\tresidue = desc->total_len;\n\t\t}\n\t} else {\n\t\tresidue = 0;\n\t}\n\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n\treturn residue;\n}\n\nstatic enum dma_status\ndwc_tx_status(struct dma_chan *chan,\n\t      dma_cookie_t cookie,\n\t      struct dma_tx_state *txstate)\n{\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(chan);\n\tenum dma_status\t\tret;\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tdwc_scan_descriptors(to_dw_dma(chan->device), dwc);\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tdma_set_residue(txstate, dwc_get_residue_and_status(dwc, cookie, &ret));\n\treturn ret;\n}\n\nstatic void dwc_issue_pending(struct dma_chan *chan)\n{\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(chan);\n\tunsigned long\t\tflags;\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\tif (list_empty(&dwc->active_list))\n\t\tdwc_dostart_first_queued(dwc);\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n}\n\n \n\nvoid do_dw_dma_off(struct dw_dma *dw)\n{\n\tdma_writel(dw, CFG, 0);\n\n\tchannel_clear_bit(dw, MASK.XFER, dw->all_chan_mask);\n\tchannel_clear_bit(dw, MASK.BLOCK, dw->all_chan_mask);\n\tchannel_clear_bit(dw, MASK.SRC_TRAN, dw->all_chan_mask);\n\tchannel_clear_bit(dw, MASK.DST_TRAN, dw->all_chan_mask);\n\tchannel_clear_bit(dw, MASK.ERROR, dw->all_chan_mask);\n\n\twhile (dma_readl(dw, CFG) & DW_CFG_DMA_EN)\n\t\tcpu_relax();\n}\n\nvoid do_dw_dma_on(struct dw_dma *dw)\n{\n\tdma_writel(dw, CFG, DW_CFG_DMA_EN);\n}\n\nstatic int dwc_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(chan);\n\tstruct dw_dma\t\t*dw = to_dw_dma(chan->device);\n\n\tdev_vdbg(chan2dev(chan), \"%s\\n\", __func__);\n\n\t \n\tif (dma_readl(dw, CH_EN) & dwc->mask) {\n\t\tdev_dbg(chan2dev(chan), \"DMA channel not idle?\\n\");\n\t\treturn -EIO;\n\t}\n\n\tdma_cookie_init(chan);\n\n\t \n\n\t \n\tif (chan->private && !dw_dma_filter(chan, chan->private)) {\n\t\tdev_warn(chan2dev(chan), \"Wrong controller-specific data\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!dw->in_use)\n\t\tdo_dw_dma_on(dw);\n\tdw->in_use |= dwc->mask;\n\n\treturn 0;\n}\n\nstatic void dwc_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct dw_dma_chan\t*dwc = to_dw_dma_chan(chan);\n\tstruct dw_dma\t\t*dw = to_dw_dma(chan->device);\n\tunsigned long\t\tflags;\n\n\tdev_dbg(chan2dev(chan), \"%s: descs allocated=%u\\n\", __func__,\n\t\t\tdwc->descs_allocated);\n\n\t \n\tBUG_ON(!list_empty(&dwc->active_list));\n\tBUG_ON(!list_empty(&dwc->queue));\n\tBUG_ON(dma_readl(to_dw_dma(chan->device), CH_EN) & dwc->mask);\n\n\tspin_lock_irqsave(&dwc->lock, flags);\n\n\t \n\tmemset(&dwc->dws, 0, sizeof(struct dw_dma_slave));\n\n\t \n\tchannel_clear_bit(dw, MASK.XFER, dwc->mask);\n\tchannel_clear_bit(dw, MASK.BLOCK, dwc->mask);\n\tchannel_clear_bit(dw, MASK.ERROR, dwc->mask);\n\n\tspin_unlock_irqrestore(&dwc->lock, flags);\n\n\t \n\tdw->in_use &= ~dwc->mask;\n\tif (!dw->in_use)\n\t\tdo_dw_dma_off(dw);\n\n\tdev_vdbg(chan2dev(chan), \"%s: done\\n\", __func__);\n}\n\nstatic void dwc_caps(struct dma_chan *chan, struct dma_slave_caps *caps)\n{\n\tstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\n\n\tcaps->max_burst = dwc->max_burst;\n\n\t \n\tif (dwc->nollp)\n\t\tcaps->max_sg_burst = 1;\n\telse\n\t\tcaps->max_sg_burst = 0;\n}\n\nint do_dma_probe(struct dw_dma_chip *chip)\n{\n\tstruct dw_dma *dw = chip->dw;\n\tstruct dw_dma_platform_data *pdata;\n\tbool\t\t\tautocfg = false;\n\tunsigned int\t\tdw_params;\n\tunsigned int\t\ti;\n\tint\t\t\terr;\n\n\tdw->pdata = devm_kzalloc(chip->dev, sizeof(*dw->pdata), GFP_KERNEL);\n\tif (!dw->pdata)\n\t\treturn -ENOMEM;\n\n\tdw->regs = chip->regs;\n\n\tpm_runtime_get_sync(chip->dev);\n\n\tif (!chip->pdata) {\n\t\tdw_params = dma_readl(dw, DW_PARAMS);\n\t\tdev_dbg(chip->dev, \"DW_PARAMS: 0x%08x\\n\", dw_params);\n\n\t\tautocfg = dw_params >> DW_PARAMS_EN & 1;\n\t\tif (!autocfg) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_pdata;\n\t\t}\n\n\t\t \n\t\tpdata = dw->pdata;\n\n\t\t \n\t\tpdata->nr_channels = (dw_params >> DW_PARAMS_NR_CHAN & 7) + 1;\n\t\tpdata->nr_masters = (dw_params >> DW_PARAMS_NR_MASTER & 3) + 1;\n\t\tfor (i = 0; i < pdata->nr_masters; i++) {\n\t\t\tpdata->data_width[i] =\n\t\t\t\t4 << (dw_params >> DW_PARAMS_DATA_WIDTH(i) & 3);\n\t\t}\n\t\tpdata->block_size = dma_readl(dw, MAX_BLK_SIZE);\n\n\t\t \n\t\tpdata->chan_allocation_order = CHAN_ALLOCATION_ASCENDING;\n\t\tpdata->chan_priority = CHAN_PRIORITY_ASCENDING;\n\t} else if (chip->pdata->nr_channels > DW_DMA_MAX_NR_CHANNELS) {\n\t\terr = -EINVAL;\n\t\tgoto err_pdata;\n\t} else {\n\t\tmemcpy(dw->pdata, chip->pdata, sizeof(*dw->pdata));\n\n\t\t \n\t\tpdata = dw->pdata;\n\t}\n\n\tdw->chan = devm_kcalloc(chip->dev, pdata->nr_channels, sizeof(*dw->chan),\n\t\t\t\tGFP_KERNEL);\n\tif (!dw->chan) {\n\t\terr = -ENOMEM;\n\t\tgoto err_pdata;\n\t}\n\n\t \n\tdw->all_chan_mask = (1 << pdata->nr_channels) - 1;\n\n\t \n\tdw->disable(dw);\n\n\t \n\tdw->set_device_name(dw, chip->id);\n\n\t \n\tdw->desc_pool = dmam_pool_create(dw->name, chip->dev,\n\t\t\t\t\t sizeof(struct dw_desc), 4, 0);\n\tif (!dw->desc_pool) {\n\t\tdev_err(chip->dev, \"No memory for descriptors dma pool\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_pdata;\n\t}\n\n\ttasklet_setup(&dw->tasklet, dw_dma_tasklet);\n\n\terr = request_irq(chip->irq, dw_dma_interrupt, IRQF_SHARED,\n\t\t\t  dw->name, dw);\n\tif (err)\n\t\tgoto err_pdata;\n\n\tINIT_LIST_HEAD(&dw->dma.channels);\n\tfor (i = 0; i < pdata->nr_channels; i++) {\n\t\tstruct dw_dma_chan\t*dwc = &dw->chan[i];\n\n\t\tdwc->chan.device = &dw->dma;\n\t\tdma_cookie_init(&dwc->chan);\n\t\tif (pdata->chan_allocation_order == CHAN_ALLOCATION_ASCENDING)\n\t\t\tlist_add_tail(&dwc->chan.device_node,\n\t\t\t\t\t&dw->dma.channels);\n\t\telse\n\t\t\tlist_add(&dwc->chan.device_node, &dw->dma.channels);\n\n\t\t \n\t\tif (pdata->chan_priority == CHAN_PRIORITY_ASCENDING)\n\t\t\tdwc->priority = pdata->nr_channels - i - 1;\n\t\telse\n\t\t\tdwc->priority = i;\n\n\t\tdwc->ch_regs = &__dw_regs(dw)->CHAN[i];\n\t\tspin_lock_init(&dwc->lock);\n\t\tdwc->mask = 1 << i;\n\n\t\tINIT_LIST_HEAD(&dwc->active_list);\n\t\tINIT_LIST_HEAD(&dwc->queue);\n\n\t\tchannel_clear_bit(dw, CH_EN, dwc->mask);\n\n\t\tdwc->direction = DMA_TRANS_NONE;\n\n\t\t \n\t\tif (autocfg) {\n\t\t\tunsigned int r = DW_DMA_MAX_NR_CHANNELS - i - 1;\n\t\t\tvoid __iomem *addr = &__dw_regs(dw)->DWC_PARAMS[r];\n\t\t\tunsigned int dwc_params = readl(addr);\n\n\t\t\tdev_dbg(chip->dev, \"DWC_PARAMS[%d]: 0x%08x\\n\", i,\n\t\t\t\t\t   dwc_params);\n\n\t\t\t \n\t\t\tdwc->block_size =\n\t\t\t\t(4 << ((pdata->block_size >> 4 * i) & 0xf)) - 1;\n\n\t\t\t \n\t\t\tdwc->nollp =\n\t\t\t\t(dwc_params >> DWC_PARAMS_MBLK_EN & 0x1) == 0 ||\n\t\t\t\t(dwc_params >> DWC_PARAMS_HC_LLP & 0x1) == 1;\n\t\t\tdwc->max_burst =\n\t\t\t\t(0x4 << (dwc_params >> DWC_PARAMS_MSIZE & 0x7));\n\t\t} else {\n\t\t\tdwc->block_size = pdata->block_size;\n\t\t\tdwc->nollp = !pdata->multi_block[i];\n\t\t\tdwc->max_burst = pdata->max_burst[i] ?: DW_DMA_MAX_BURST;\n\t\t}\n\t}\n\n\t \n\tdma_writel(dw, CLEAR.XFER, dw->all_chan_mask);\n\tdma_writel(dw, CLEAR.BLOCK, dw->all_chan_mask);\n\tdma_writel(dw, CLEAR.SRC_TRAN, dw->all_chan_mask);\n\tdma_writel(dw, CLEAR.DST_TRAN, dw->all_chan_mask);\n\tdma_writel(dw, CLEAR.ERROR, dw->all_chan_mask);\n\n\t \n\tdma_cap_set(DMA_SLAVE, dw->dma.cap_mask);\n\tdma_cap_set(DMA_PRIVATE, dw->dma.cap_mask);\n\tdma_cap_set(DMA_MEMCPY, dw->dma.cap_mask);\n\n\tdw->dma.dev = chip->dev;\n\tdw->dma.device_alloc_chan_resources = dwc_alloc_chan_resources;\n\tdw->dma.device_free_chan_resources = dwc_free_chan_resources;\n\n\tdw->dma.device_prep_dma_memcpy = dwc_prep_dma_memcpy;\n\tdw->dma.device_prep_slave_sg = dwc_prep_slave_sg;\n\n\tdw->dma.device_caps = dwc_caps;\n\tdw->dma.device_config = dwc_config;\n\tdw->dma.device_pause = dwc_pause;\n\tdw->dma.device_resume = dwc_resume;\n\tdw->dma.device_terminate_all = dwc_terminate_all;\n\n\tdw->dma.device_tx_status = dwc_tx_status;\n\tdw->dma.device_issue_pending = dwc_issue_pending;\n\n\t \n\tdw->dma.min_burst = DW_DMA_MIN_BURST;\n\tdw->dma.max_burst = DW_DMA_MAX_BURST;\n\tdw->dma.src_addr_widths = DW_DMA_BUSWIDTHS;\n\tdw->dma.dst_addr_widths = DW_DMA_BUSWIDTHS;\n\tdw->dma.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV) |\n\t\t\t     BIT(DMA_MEM_TO_MEM);\n\tdw->dma.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\n\n\t \n\tdma_set_max_seg_size(dw->dma.dev, dw->chan[0].block_size);\n\n\terr = dma_async_device_register(&dw->dma);\n\tif (err)\n\t\tgoto err_dma_register;\n\n\tdev_info(chip->dev, \"DesignWare DMA Controller, %d channels\\n\",\n\t\t pdata->nr_channels);\n\n\tpm_runtime_put_sync_suspend(chip->dev);\n\n\treturn 0;\n\nerr_dma_register:\n\tfree_irq(chip->irq, dw);\nerr_pdata:\n\tpm_runtime_put_sync_suspend(chip->dev);\n\treturn err;\n}\n\nint do_dma_remove(struct dw_dma_chip *chip)\n{\n\tstruct dw_dma\t\t*dw = chip->dw;\n\tstruct dw_dma_chan\t*dwc, *_dwc;\n\n\tpm_runtime_get_sync(chip->dev);\n\n\tdo_dw_dma_off(dw);\n\tdma_async_device_unregister(&dw->dma);\n\n\tfree_irq(chip->irq, dw);\n\ttasklet_kill(&dw->tasklet);\n\n\tlist_for_each_entry_safe(dwc, _dwc, &dw->dma.channels,\n\t\t\tchan.device_node) {\n\t\tlist_del(&dwc->chan.device_node);\n\t\tchannel_clear_bit(dw, CH_EN, dwc->mask);\n\t}\n\n\tpm_runtime_put_sync_suspend(chip->dev);\n\treturn 0;\n}\n\nint do_dw_dma_disable(struct dw_dma_chip *chip)\n{\n\tstruct dw_dma *dw = chip->dw;\n\n\tdw->disable(dw);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(do_dw_dma_disable);\n\nint do_dw_dma_enable(struct dw_dma_chip *chip)\n{\n\tstruct dw_dma *dw = chip->dw;\n\n\tdw->enable(dw);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(do_dw_dma_enable);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"Synopsys DesignWare DMA Controller core driver\");\nMODULE_AUTHOR(\"Haavard Skinnemoen (Atmel)\");\nMODULE_AUTHOR(\"Viresh Kumar <vireshk@kernel.org>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}