{
  "module_name": "pxa_dma.c",
  "hash_id": "e3ecb49ccd1b5890648b412dadcddc9f6736dcc906d6190c3bf7f7e6bb81ab48",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/pxa_dma.c",
  "human_readable_source": "\n \n\n#include <linux/err.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/interrupt.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/dmaengine.h>\n#include <linux/platform_device.h>\n#include <linux/device.h>\n#include <linux/platform_data/mmp_dma.h>\n#include <linux/dmapool.h>\n#include <linux/of_device.h>\n#include <linux/of_dma.h>\n#include <linux/of.h>\n#include <linux/wait.h>\n#include <linux/dma/pxa-dma.h>\n\n#include \"dmaengine.h\"\n#include \"virt-dma.h\"\n\n#define DCSR(n)\t\t(0x0000 + ((n) << 2))\n#define DALGN(n)\t0x00a0\n#define DINT\t\t0x00f0\n#define DDADR(n)\t(0x0200 + ((n) << 4))\n#define DSADR(n)\t(0x0204 + ((n) << 4))\n#define DTADR(n)\t(0x0208 + ((n) << 4))\n#define DCMD(n)\t\t(0x020c + ((n) << 4))\n\n#define PXA_DCSR_RUN\t\tBIT(31)\t \n#define PXA_DCSR_NODESC\t\tBIT(30)\t \n#define PXA_DCSR_STOPIRQEN\tBIT(29)\t \n#define PXA_DCSR_REQPEND\tBIT(8)\t \n#define PXA_DCSR_STOPSTATE\tBIT(3)\t \n#define PXA_DCSR_ENDINTR\tBIT(2)\t \n#define PXA_DCSR_STARTINTR\tBIT(1)\t \n#define PXA_DCSR_BUSERR\t\tBIT(0)\t \n\n#define PXA_DCSR_EORIRQEN\tBIT(28)\t \n#define PXA_DCSR_EORJMPEN\tBIT(27)\t \n#define PXA_DCSR_EORSTOPEN\tBIT(26)\t \n#define PXA_DCSR_SETCMPST\tBIT(25)\t \n#define PXA_DCSR_CLRCMPST\tBIT(24)\t \n#define PXA_DCSR_CMPST\t\tBIT(10)\t \n#define PXA_DCSR_EORINTR\tBIT(9)\t \n\n#define DRCMR_MAPVLD\tBIT(7)\t \n#define DRCMR_CHLNUM\t0x1f\t \n\n#define DDADR_DESCADDR\t0xfffffff0\t \n#define DDADR_STOP\tBIT(0)\t \n\n#define PXA_DCMD_INCSRCADDR\tBIT(31)\t \n#define PXA_DCMD_INCTRGADDR\tBIT(30)\t \n#define PXA_DCMD_FLOWSRC\tBIT(29)\t \n#define PXA_DCMD_FLOWTRG\tBIT(28)\t \n#define PXA_DCMD_STARTIRQEN\tBIT(22)\t \n#define PXA_DCMD_ENDIRQEN\tBIT(21)\t \n#define PXA_DCMD_ENDIAN\t\tBIT(18)\t \n#define PXA_DCMD_BURST8\t\t(1 << 16)\t \n#define PXA_DCMD_BURST16\t(2 << 16)\t \n#define PXA_DCMD_BURST32\t(3 << 16)\t \n#define PXA_DCMD_WIDTH1\t\t(1 << 14)\t \n#define PXA_DCMD_WIDTH2\t\t(2 << 14)\t \n#define PXA_DCMD_WIDTH4\t\t(3 << 14)\t \n#define PXA_DCMD_LENGTH\t\t0x01fff\t\t \n\n#define PDMA_ALIGNMENT\t\t3\n#define PDMA_MAX_DESC_BYTES\t(PXA_DCMD_LENGTH & ~((1 << PDMA_ALIGNMENT) - 1))\n\nstruct pxad_desc_hw {\n\tu32 ddadr;\t \n\tu32 dsadr;\t \n\tu32 dtadr;\t \n\tu32 dcmd;\t \n} __aligned(16);\n\nstruct pxad_desc_sw {\n\tstruct virt_dma_desc\tvd;\t\t \n\tint\t\t\tnb_desc;\t \n\tsize_t\t\t\tlen;\t\t \n\tdma_addr_t\t\tfirst;\t\t \n\n\t \n\tbool\t\t\tmisaligned;\n\tbool\t\t\tcyclic;\n\tstruct dma_pool\t\t*desc_pool;\t \n\n\tstruct pxad_desc_hw\t*hw_desc[];\t \n};\n\nstruct pxad_phy {\n\tint\t\t\tidx;\n\tvoid __iomem\t\t*base;\n\tstruct pxad_chan\t*vchan;\n};\n\nstruct pxad_chan {\n\tstruct virt_dma_chan\tvc;\t\t \n\tu32\t\t\tdrcmr;\t\t \n\tenum pxad_chan_prio\tprio;\t\t \n\t \n\tbool\t\t\tmisaligned;\n\tstruct dma_slave_config\tcfg;\t\t \n\n\t \n\tstruct pxad_phy\t\t*phy;\n\tstruct dma_pool\t\t*desc_pool;\t \n\tdma_cookie_t\t\tbus_error;\n\n\twait_queue_head_t\twq_state;\n};\n\nstruct pxad_device {\n\tstruct dma_device\t\tslave;\n\tint\t\t\t\tnr_chans;\n\tint\t\t\t\tnr_requestors;\n\tvoid __iomem\t\t\t*base;\n\tstruct pxad_phy\t\t\t*phys;\n\tspinlock_t\t\t\tphy_lock;\t \n#ifdef CONFIG_DEBUG_FS\n\tstruct dentry\t\t\t*dbgfs_root;\n\tstruct dentry\t\t\t**dbgfs_chan;\n#endif\n};\n\n#define tx_to_pxad_desc(tx)\t\t\t\t\t\\\n\tcontainer_of(tx, struct pxad_desc_sw, async_tx)\n#define to_pxad_chan(dchan)\t\t\t\t\t\\\n\tcontainer_of(dchan, struct pxad_chan, vc.chan)\n#define to_pxad_dev(dmadev)\t\t\t\t\t\\\n\tcontainer_of(dmadev, struct pxad_device, slave)\n#define to_pxad_sw_desc(_vd)\t\t\t\t\\\n\tcontainer_of((_vd), struct pxad_desc_sw, vd)\n\n#define _phy_readl_relaxed(phy, _reg)\t\t\t\t\t\\\n\treadl_relaxed((phy)->base + _reg((phy)->idx))\n#define phy_readl_relaxed(phy, _reg)\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tu32 _v;\t\t\t\t\t\t\t\\\n\t\t_v = readl_relaxed((phy)->base + _reg((phy)->idx));\t\\\n\t\tdev_vdbg(&phy->vchan->vc.chan.dev->device,\t\t\\\n\t\t\t \"%s(): readl(%s): 0x%08x\\n\", __func__, #_reg,\t\\\n\t\t\t  _v);\t\t\t\t\t\t\\\n\t\t_v;\t\t\t\t\t\t\t\\\n\t})\n#define phy_writel(phy, val, _reg)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\twritel((val), (phy)->base + _reg((phy)->idx));\t\t\\\n\t\tdev_vdbg(&phy->vchan->vc.chan.dev->device,\t\t\\\n\t\t\t \"%s(): writel(0x%08x, %s)\\n\",\t\t\t\\\n\t\t\t __func__, (u32)(val), #_reg);\t\t\t\\\n\t} while (0)\n#define phy_writel_relaxed(phy, val, _reg)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\twritel_relaxed((val), (phy)->base + _reg((phy)->idx));\t\\\n\t\tdev_vdbg(&phy->vchan->vc.chan.dev->device,\t\t\\\n\t\t\t \"%s(): writel_relaxed(0x%08x, %s)\\n\",\t\t\\\n\t\t\t __func__, (u32)(val), #_reg);\t\t\t\\\n\t} while (0)\n\nstatic unsigned int pxad_drcmr(unsigned int line)\n{\n\tif (line < 64)\n\t\treturn 0x100 + line * 4;\n\treturn 0x1000 + line * 4;\n}\n\nstatic bool pxad_filter_fn(struct dma_chan *chan, void *param);\n\n \n#ifdef CONFIG_DEBUG_FS\n#include <linux/debugfs.h>\n#include <linux/uaccess.h>\n#include <linux/seq_file.h>\n\nstatic int requester_chan_show(struct seq_file *s, void *p)\n{\n\tstruct pxad_phy *phy = s->private;\n\tint i;\n\tu32 drcmr;\n\n\tseq_printf(s, \"DMA channel %d requester :\\n\", phy->idx);\n\tfor (i = 0; i < 70; i++) {\n\t\tdrcmr = readl_relaxed(phy->base + pxad_drcmr(i));\n\t\tif ((drcmr & DRCMR_CHLNUM) == phy->idx)\n\t\t\tseq_printf(s, \"\\tRequester %d (MAPVLD=%d)\\n\", i,\n\t\t\t\t   !!(drcmr & DRCMR_MAPVLD));\n\t}\n\treturn 0;\n}\n\nstatic inline int dbg_burst_from_dcmd(u32 dcmd)\n{\n\tint burst = (dcmd >> 16) & 0x3;\n\n\treturn burst ? 4 << burst : 0;\n}\n\nstatic int is_phys_valid(unsigned long addr)\n{\n\treturn pfn_valid(__phys_to_pfn(addr));\n}\n\n#define PXA_DCSR_STR(flag) (dcsr & PXA_DCSR_##flag ? #flag\" \" : \"\")\n#define PXA_DCMD_STR(flag) (dcmd & PXA_DCMD_##flag ? #flag\" \" : \"\")\n\nstatic int descriptors_show(struct seq_file *s, void *p)\n{\n\tstruct pxad_phy *phy = s->private;\n\tint i, max_show = 20, burst, width;\n\tu32 dcmd;\n\tunsigned long phys_desc, ddadr;\n\tstruct pxad_desc_hw *desc;\n\n\tphys_desc = ddadr = _phy_readl_relaxed(phy, DDADR);\n\n\tseq_printf(s, \"DMA channel %d descriptors :\\n\", phy->idx);\n\tseq_printf(s, \"[%03d] First descriptor unknown\\n\", 0);\n\tfor (i = 1; i < max_show && is_phys_valid(phys_desc); i++) {\n\t\tdesc = phys_to_virt(phys_desc);\n\t\tdcmd = desc->dcmd;\n\t\tburst = dbg_burst_from_dcmd(dcmd);\n\t\twidth = (1 << ((dcmd >> 14) & 0x3)) >> 1;\n\n\t\tseq_printf(s, \"[%03d] Desc at %08lx(virt %p)\\n\",\n\t\t\t   i, phys_desc, desc);\n\t\tseq_printf(s, \"\\tDDADR = %08x\\n\", desc->ddadr);\n\t\tseq_printf(s, \"\\tDSADR = %08x\\n\", desc->dsadr);\n\t\tseq_printf(s, \"\\tDTADR = %08x\\n\", desc->dtadr);\n\t\tseq_printf(s, \"\\tDCMD  = %08x (%s%s%s%s%s%s%sburst=%d width=%d len=%d)\\n\",\n\t\t\t   dcmd,\n\t\t\t   PXA_DCMD_STR(INCSRCADDR), PXA_DCMD_STR(INCTRGADDR),\n\t\t\t   PXA_DCMD_STR(FLOWSRC), PXA_DCMD_STR(FLOWTRG),\n\t\t\t   PXA_DCMD_STR(STARTIRQEN), PXA_DCMD_STR(ENDIRQEN),\n\t\t\t   PXA_DCMD_STR(ENDIAN), burst, width,\n\t\t\t   dcmd & PXA_DCMD_LENGTH);\n\t\tphys_desc = desc->ddadr;\n\t}\n\tif (i == max_show)\n\t\tseq_printf(s, \"[%03d] Desc at %08lx ... max display reached\\n\",\n\t\t\t   i, phys_desc);\n\telse\n\t\tseq_printf(s, \"[%03d] Desc at %08lx is %s\\n\",\n\t\t\t   i, phys_desc, phys_desc == DDADR_STOP ?\n\t\t\t   \"DDADR_STOP\" : \"invalid\");\n\n\treturn 0;\n}\n\nstatic int chan_state_show(struct seq_file *s, void *p)\n{\n\tstruct pxad_phy *phy = s->private;\n\tu32 dcsr, dcmd;\n\tint burst, width;\n\tstatic const char * const str_prio[] = {\n\t\t\"high\", \"normal\", \"low\", \"invalid\"\n\t};\n\n\tdcsr = _phy_readl_relaxed(phy, DCSR);\n\tdcmd = _phy_readl_relaxed(phy, DCMD);\n\tburst = dbg_burst_from_dcmd(dcmd);\n\twidth = (1 << ((dcmd >> 14) & 0x3)) >> 1;\n\n\tseq_printf(s, \"DMA channel %d\\n\", phy->idx);\n\tseq_printf(s, \"\\tPriority : %s\\n\",\n\t\t\t  str_prio[(phy->idx & 0xf) / 4]);\n\tseq_printf(s, \"\\tUnaligned transfer bit: %s\\n\",\n\t\t\t  _phy_readl_relaxed(phy, DALGN) & BIT(phy->idx) ?\n\t\t\t  \"yes\" : \"no\");\n\tseq_printf(s, \"\\tDCSR  = %08x (%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s)\\n\",\n\t\t   dcsr, PXA_DCSR_STR(RUN), PXA_DCSR_STR(NODESC),\n\t\t   PXA_DCSR_STR(STOPIRQEN), PXA_DCSR_STR(EORIRQEN),\n\t\t   PXA_DCSR_STR(EORJMPEN), PXA_DCSR_STR(EORSTOPEN),\n\t\t   PXA_DCSR_STR(SETCMPST), PXA_DCSR_STR(CLRCMPST),\n\t\t   PXA_DCSR_STR(CMPST), PXA_DCSR_STR(EORINTR),\n\t\t   PXA_DCSR_STR(REQPEND), PXA_DCSR_STR(STOPSTATE),\n\t\t   PXA_DCSR_STR(ENDINTR), PXA_DCSR_STR(STARTINTR),\n\t\t   PXA_DCSR_STR(BUSERR));\n\n\tseq_printf(s, \"\\tDCMD  = %08x (%s%s%s%s%s%s%sburst=%d width=%d len=%d)\\n\",\n\t\t   dcmd,\n\t\t   PXA_DCMD_STR(INCSRCADDR), PXA_DCMD_STR(INCTRGADDR),\n\t\t   PXA_DCMD_STR(FLOWSRC), PXA_DCMD_STR(FLOWTRG),\n\t\t   PXA_DCMD_STR(STARTIRQEN), PXA_DCMD_STR(ENDIRQEN),\n\t\t   PXA_DCMD_STR(ENDIAN), burst, width, dcmd & PXA_DCMD_LENGTH);\n\tseq_printf(s, \"\\tDSADR = %08x\\n\", _phy_readl_relaxed(phy, DSADR));\n\tseq_printf(s, \"\\tDTADR = %08x\\n\", _phy_readl_relaxed(phy, DTADR));\n\tseq_printf(s, \"\\tDDADR = %08x\\n\", _phy_readl_relaxed(phy, DDADR));\n\n\treturn 0;\n}\n\nstatic int state_show(struct seq_file *s, void *p)\n{\n\tstruct pxad_device *pdev = s->private;\n\n\t \n\tseq_puts(s, \"DMA engine status\\n\");\n\tseq_printf(s, \"\\tChannel number: %d\\n\", pdev->nr_chans);\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(state);\nDEFINE_SHOW_ATTRIBUTE(chan_state);\nDEFINE_SHOW_ATTRIBUTE(descriptors);\nDEFINE_SHOW_ATTRIBUTE(requester_chan);\n\nstatic struct dentry *pxad_dbg_alloc_chan(struct pxad_device *pdev,\n\t\t\t\t\t     int ch, struct dentry *chandir)\n{\n\tchar chan_name[11];\n\tstruct dentry *chan;\n\tvoid *dt;\n\n\tscnprintf(chan_name, sizeof(chan_name), \"%d\", ch);\n\tchan = debugfs_create_dir(chan_name, chandir);\n\tdt = (void *)&pdev->phys[ch];\n\n\tdebugfs_create_file(\"state\", 0400, chan, dt, &chan_state_fops);\n\tdebugfs_create_file(\"descriptors\", 0400, chan, dt, &descriptors_fops);\n\tdebugfs_create_file(\"requesters\", 0400, chan, dt, &requester_chan_fops);\n\n\treturn chan;\n}\n\nstatic void pxad_init_debugfs(struct pxad_device *pdev)\n{\n\tint i;\n\tstruct dentry *chandir;\n\n\tpdev->dbgfs_chan =\n\t\tkmalloc_array(pdev->nr_chans, sizeof(struct dentry *),\n\t\t\t      GFP_KERNEL);\n\tif (!pdev->dbgfs_chan)\n\t\treturn;\n\n\tpdev->dbgfs_root = debugfs_create_dir(dev_name(pdev->slave.dev), NULL);\n\n\tdebugfs_create_file(\"state\", 0400, pdev->dbgfs_root, pdev, &state_fops);\n\n\tchandir = debugfs_create_dir(\"channels\", pdev->dbgfs_root);\n\n\tfor (i = 0; i < pdev->nr_chans; i++)\n\t\tpdev->dbgfs_chan[i] = pxad_dbg_alloc_chan(pdev, i, chandir);\n}\n\nstatic void pxad_cleanup_debugfs(struct pxad_device *pdev)\n{\n\tdebugfs_remove_recursive(pdev->dbgfs_root);\n}\n#else\nstatic inline void pxad_init_debugfs(struct pxad_device *pdev) {}\nstatic inline void pxad_cleanup_debugfs(struct pxad_device *pdev) {}\n#endif\n\nstatic struct pxad_phy *lookup_phy(struct pxad_chan *pchan)\n{\n\tint prio, i;\n\tstruct pxad_device *pdev = to_pxad_dev(pchan->vc.chan.device);\n\tstruct pxad_phy *phy, *found = NULL;\n\tunsigned long flags;\n\n\t \n\n\tspin_lock_irqsave(&pdev->phy_lock, flags);\n\tfor (prio = pchan->prio; prio >= PXAD_PRIO_HIGHEST; prio--) {\n\t\tfor (i = 0; i < pdev->nr_chans; i++) {\n\t\t\tif (prio != (i & 0xf) >> 2)\n\t\t\t\tcontinue;\n\t\t\tphy = &pdev->phys[i];\n\t\t\tif (!phy->vchan) {\n\t\t\t\tphy->vchan = pchan;\n\t\t\t\tfound = phy;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\t}\n\nout_unlock:\n\tspin_unlock_irqrestore(&pdev->phy_lock, flags);\n\tdev_dbg(&pchan->vc.chan.dev->device,\n\t\t\"%s(): phy=%p(%d)\\n\", __func__, found,\n\t\tfound ? found->idx : -1);\n\n\treturn found;\n}\n\nstatic void pxad_free_phy(struct pxad_chan *chan)\n{\n\tstruct pxad_device *pdev = to_pxad_dev(chan->vc.chan.device);\n\tunsigned long flags;\n\tu32 reg;\n\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): freeing\\n\", __func__);\n\tif (!chan->phy)\n\t\treturn;\n\n\t \n\tif (chan->drcmr <= pdev->nr_requestors) {\n\t\treg = pxad_drcmr(chan->drcmr);\n\t\twritel_relaxed(0, chan->phy->base + reg);\n\t}\n\n\tspin_lock_irqsave(&pdev->phy_lock, flags);\n\tchan->phy->vchan = NULL;\n\tchan->phy = NULL;\n\tspin_unlock_irqrestore(&pdev->phy_lock, flags);\n}\n\nstatic bool is_chan_running(struct pxad_chan *chan)\n{\n\tu32 dcsr;\n\tstruct pxad_phy *phy = chan->phy;\n\n\tif (!phy)\n\t\treturn false;\n\tdcsr = phy_readl_relaxed(phy, DCSR);\n\treturn dcsr & PXA_DCSR_RUN;\n}\n\nstatic bool is_running_chan_misaligned(struct pxad_chan *chan)\n{\n\tu32 dalgn;\n\n\tBUG_ON(!chan->phy);\n\tdalgn = phy_readl_relaxed(chan->phy, DALGN);\n\treturn dalgn & (BIT(chan->phy->idx));\n}\n\nstatic void phy_enable(struct pxad_phy *phy, bool misaligned)\n{\n\tstruct pxad_device *pdev;\n\tu32 reg, dalgn;\n\n\tif (!phy->vchan)\n\t\treturn;\n\n\tdev_dbg(&phy->vchan->vc.chan.dev->device,\n\t\t\"%s(); phy=%p(%d) misaligned=%d\\n\", __func__,\n\t\tphy, phy->idx, misaligned);\n\n\tpdev = to_pxad_dev(phy->vchan->vc.chan.device);\n\tif (phy->vchan->drcmr <= pdev->nr_requestors) {\n\t\treg = pxad_drcmr(phy->vchan->drcmr);\n\t\twritel_relaxed(DRCMR_MAPVLD | phy->idx, phy->base + reg);\n\t}\n\n\tdalgn = phy_readl_relaxed(phy, DALGN);\n\tif (misaligned)\n\t\tdalgn |= BIT(phy->idx);\n\telse\n\t\tdalgn &= ~BIT(phy->idx);\n\tphy_writel_relaxed(phy, dalgn, DALGN);\n\n\tphy_writel(phy, PXA_DCSR_STOPIRQEN | PXA_DCSR_ENDINTR |\n\t\t   PXA_DCSR_BUSERR | PXA_DCSR_RUN, DCSR);\n}\n\nstatic void phy_disable(struct pxad_phy *phy)\n{\n\tu32 dcsr;\n\n\tif (!phy)\n\t\treturn;\n\n\tdcsr = phy_readl_relaxed(phy, DCSR);\n\tdev_dbg(&phy->vchan->vc.chan.dev->device,\n\t\t\"%s(): phy=%p(%d)\\n\", __func__, phy, phy->idx);\n\tphy_writel(phy, dcsr & ~PXA_DCSR_RUN & ~PXA_DCSR_STOPIRQEN, DCSR);\n}\n\nstatic void pxad_launch_chan(struct pxad_chan *chan,\n\t\t\t\t struct pxad_desc_sw *desc)\n{\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): desc=%p\\n\", __func__, desc);\n\tif (!chan->phy) {\n\t\tchan->phy = lookup_phy(chan);\n\t\tif (!chan->phy) {\n\t\t\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\t\t\"%s(): no free dma channel\\n\", __func__);\n\t\t\treturn;\n\t\t}\n\t}\n\tchan->bus_error = 0;\n\n\t \n\tphy_writel(chan->phy, desc->first, DDADR);\n\tphy_enable(chan->phy, chan->misaligned);\n\twake_up(&chan->wq_state);\n}\n\nstatic void set_updater_desc(struct pxad_desc_sw *sw_desc,\n\t\t\t     unsigned long flags)\n{\n\tstruct pxad_desc_hw *updater =\n\t\tsw_desc->hw_desc[sw_desc->nb_desc - 1];\n\tdma_addr_t dma = sw_desc->hw_desc[sw_desc->nb_desc - 2]->ddadr;\n\n\tupdater->ddadr = DDADR_STOP;\n\tupdater->dsadr = dma;\n\tupdater->dtadr = dma + 8;\n\tupdater->dcmd = PXA_DCMD_WIDTH4 | PXA_DCMD_BURST32 |\n\t\t(PXA_DCMD_LENGTH & sizeof(u32));\n\tif (flags & DMA_PREP_INTERRUPT)\n\t\tupdater->dcmd |= PXA_DCMD_ENDIRQEN;\n\tif (sw_desc->cyclic)\n\t\tsw_desc->hw_desc[sw_desc->nb_desc - 2]->ddadr = sw_desc->first;\n}\n\nstatic bool is_desc_completed(struct virt_dma_desc *vd)\n{\n\tstruct pxad_desc_sw *sw_desc = to_pxad_sw_desc(vd);\n\tstruct pxad_desc_hw *updater =\n\t\tsw_desc->hw_desc[sw_desc->nb_desc - 1];\n\n\treturn updater->dtadr != (updater->dsadr + 8);\n}\n\nstatic void pxad_desc_chain(struct virt_dma_desc *vd1,\n\t\t\t\tstruct virt_dma_desc *vd2)\n{\n\tstruct pxad_desc_sw *desc1 = to_pxad_sw_desc(vd1);\n\tstruct pxad_desc_sw *desc2 = to_pxad_sw_desc(vd2);\n\tdma_addr_t dma_to_chain;\n\n\tdma_to_chain = desc2->first;\n\tdesc1->hw_desc[desc1->nb_desc - 1]->ddadr = dma_to_chain;\n}\n\nstatic bool pxad_try_hotchain(struct virt_dma_chan *vc,\n\t\t\t\t  struct virt_dma_desc *vd)\n{\n\tstruct virt_dma_desc *vd_last_issued = NULL;\n\tstruct pxad_chan *chan = to_pxad_chan(&vc->chan);\n\n\t \n\tif (is_chan_running(chan)) {\n\t\tBUG_ON(list_empty(&vc->desc_issued));\n\n\t\tif (!is_running_chan_misaligned(chan) &&\n\t\t    to_pxad_sw_desc(vd)->misaligned)\n\t\t\treturn false;\n\n\t\tvd_last_issued = list_entry(vc->desc_issued.prev,\n\t\t\t\t\t    struct virt_dma_desc, node);\n\t\tpxad_desc_chain(vd_last_issued, vd);\n\t\tif (is_chan_running(chan) || is_desc_completed(vd))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic unsigned int clear_chan_irq(struct pxad_phy *phy)\n{\n\tu32 dcsr;\n\tu32 dint = readl(phy->base + DINT);\n\n\tif (!(dint & BIT(phy->idx)))\n\t\treturn PXA_DCSR_RUN;\n\n\t \n\tdcsr = phy_readl_relaxed(phy, DCSR);\n\tphy_writel(phy, dcsr, DCSR);\n\tif ((dcsr & PXA_DCSR_BUSERR) && (phy->vchan))\n\t\tdev_warn(&phy->vchan->vc.chan.dev->device,\n\t\t\t \"%s(chan=%p): PXA_DCSR_BUSERR\\n\",\n\t\t\t __func__, &phy->vchan);\n\n\treturn dcsr & ~PXA_DCSR_RUN;\n}\n\nstatic irqreturn_t pxad_chan_handler(int irq, void *dev_id)\n{\n\tstruct pxad_phy *phy = dev_id;\n\tstruct pxad_chan *chan = phy->vchan;\n\tstruct virt_dma_desc *vd, *tmp;\n\tunsigned int dcsr;\n\tbool vd_completed;\n\tdma_cookie_t last_started = 0;\n\n\tBUG_ON(!chan);\n\n\tdcsr = clear_chan_irq(phy);\n\tif (dcsr & PXA_DCSR_RUN)\n\t\treturn IRQ_NONE;\n\n\tspin_lock(&chan->vc.lock);\n\tlist_for_each_entry_safe(vd, tmp, &chan->vc.desc_issued, node) {\n\t\tvd_completed = is_desc_completed(vd);\n\t\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\t\"%s(): checking txd %p[%x]: completed=%d dcsr=0x%x\\n\",\n\t\t\t__func__, vd, vd->tx.cookie, vd_completed,\n\t\t\tdcsr);\n\t\tlast_started = vd->tx.cookie;\n\t\tif (to_pxad_sw_desc(vd)->cyclic) {\n\t\t\tvchan_cyclic_callback(vd);\n\t\t\tbreak;\n\t\t}\n\t\tif (vd_completed) {\n\t\t\tlist_del(&vd->node);\n\t\t\tvchan_cookie_complete(vd);\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (dcsr & PXA_DCSR_BUSERR) {\n\t\tchan->bus_error = last_started;\n\t\tphy_disable(phy);\n\t}\n\n\tif (!chan->bus_error && dcsr & PXA_DCSR_STOPSTATE) {\n\t\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): channel stopped, submitted_empty=%d issued_empty=%d\",\n\t\t\t__func__,\n\t\t\tlist_empty(&chan->vc.desc_submitted),\n\t\t\tlist_empty(&chan->vc.desc_issued));\n\t\tphy_writel_relaxed(phy, dcsr & ~PXA_DCSR_STOPIRQEN, DCSR);\n\n\t\tif (list_empty(&chan->vc.desc_issued)) {\n\t\t\tchan->misaligned =\n\t\t\t\t!list_empty(&chan->vc.desc_submitted);\n\t\t} else {\n\t\t\tvd = list_first_entry(&chan->vc.desc_issued,\n\t\t\t\t\t      struct virt_dma_desc, node);\n\t\t\tpxad_launch_chan(chan, to_pxad_sw_desc(vd));\n\t\t}\n\t}\n\tspin_unlock(&chan->vc.lock);\n\twake_up(&chan->wq_state);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t pxad_int_handler(int irq, void *dev_id)\n{\n\tstruct pxad_device *pdev = dev_id;\n\tstruct pxad_phy *phy;\n\tu32 dint = readl(pdev->base + DINT);\n\tint i, ret = IRQ_NONE;\n\n\twhile (dint) {\n\t\ti = __ffs(dint);\n\t\tdint &= (dint - 1);\n\t\tphy = &pdev->phys[i];\n\t\tif (pxad_chan_handler(irq, phy) == IRQ_HANDLED)\n\t\t\tret = IRQ_HANDLED;\n\t}\n\n\treturn ret;\n}\n\nstatic int pxad_alloc_chan_resources(struct dma_chan *dchan)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\tstruct pxad_device *pdev = to_pxad_dev(chan->vc.chan.device);\n\n\tif (chan->desc_pool)\n\t\treturn 1;\n\n\tchan->desc_pool = dma_pool_create(dma_chan_name(dchan),\n\t\t\t\t\t  pdev->slave.dev,\n\t\t\t\t\t  sizeof(struct pxad_desc_hw),\n\t\t\t\t\t  __alignof__(struct pxad_desc_hw),\n\t\t\t\t\t  0);\n\tif (!chan->desc_pool) {\n\t\tdev_err(&chan->vc.chan.dev->device,\n\t\t\t\"%s(): unable to allocate descriptor pool\\n\",\n\t\t\t__func__);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 1;\n}\n\nstatic void pxad_free_chan_resources(struct dma_chan *dchan)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\n\tvchan_free_chan_resources(&chan->vc);\n\tdma_pool_destroy(chan->desc_pool);\n\tchan->desc_pool = NULL;\n\n\tchan->drcmr = U32_MAX;\n\tchan->prio = PXAD_PRIO_LOWEST;\n}\n\nstatic void pxad_free_desc(struct virt_dma_desc *vd)\n{\n\tint i;\n\tdma_addr_t dma;\n\tstruct pxad_desc_sw *sw_desc = to_pxad_sw_desc(vd);\n\n\tfor (i = sw_desc->nb_desc - 1; i >= 0; i--) {\n\t\tif (i > 0)\n\t\t\tdma = sw_desc->hw_desc[i - 1]->ddadr;\n\t\telse\n\t\t\tdma = sw_desc->first;\n\t\tdma_pool_free(sw_desc->desc_pool,\n\t\t\t      sw_desc->hw_desc[i], dma);\n\t}\n\tsw_desc->nb_desc = 0;\n\tkfree(sw_desc);\n}\n\nstatic struct pxad_desc_sw *\npxad_alloc_desc(struct pxad_chan *chan, unsigned int nb_hw_desc)\n{\n\tstruct pxad_desc_sw *sw_desc;\n\tdma_addr_t dma;\n\tint i;\n\n\tsw_desc = kzalloc(struct_size(sw_desc, hw_desc, nb_hw_desc),\n\t\t\t  GFP_NOWAIT);\n\tif (!sw_desc)\n\t\treturn NULL;\n\tsw_desc->desc_pool = chan->desc_pool;\n\n\tfor (i = 0; i < nb_hw_desc; i++) {\n\t\tsw_desc->hw_desc[i] = dma_pool_alloc(sw_desc->desc_pool,\n\t\t\t\t\t\t     GFP_NOWAIT, &dma);\n\t\tif (!sw_desc->hw_desc[i]) {\n\t\t\tdev_err(&chan->vc.chan.dev->device,\n\t\t\t\t\"%s(): Couldn't allocate the %dth hw_desc from dma_pool %p\\n\",\n\t\t\t\t__func__, i, sw_desc->desc_pool);\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (i == 0)\n\t\t\tsw_desc->first = dma;\n\t\telse\n\t\t\tsw_desc->hw_desc[i - 1]->ddadr = dma;\n\t\tsw_desc->nb_desc++;\n\t}\n\n\treturn sw_desc;\nerr:\n\tpxad_free_desc(&sw_desc->vd);\n\treturn NULL;\n}\n\nstatic dma_cookie_t pxad_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct virt_dma_chan *vc = to_virt_chan(tx->chan);\n\tstruct pxad_chan *chan = to_pxad_chan(&vc->chan);\n\tstruct virt_dma_desc *vd_chained = NULL,\n\t\t*vd = container_of(tx, struct virt_dma_desc, tx);\n\tdma_cookie_t cookie;\n\tunsigned long flags;\n\n\tset_updater_desc(to_pxad_sw_desc(vd), tx->flags);\n\n\tspin_lock_irqsave(&vc->lock, flags);\n\tcookie = dma_cookie_assign(tx);\n\n\tif (list_empty(&vc->desc_submitted) && pxad_try_hotchain(vc, vd)) {\n\t\tlist_move_tail(&vd->node, &vc->desc_issued);\n\t\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\t\"%s(): txd %p[%x]: submitted (hot linked)\\n\",\n\t\t\t__func__, vd, cookie);\n\t\tgoto out;\n\t}\n\n\t \n\tif (!list_empty(&vc->desc_submitted)) {\n\t\tvd_chained = list_entry(vc->desc_submitted.prev,\n\t\t\t\t\tstruct virt_dma_desc, node);\n\t\t \n\t\tif (chan->misaligned || !to_pxad_sw_desc(vd)->misaligned)\n\t\t\tpxad_desc_chain(vd_chained, vd);\n\t\telse\n\t\t\tvd_chained = NULL;\n\t}\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): txd %p[%x]: submitted (%s linked)\\n\",\n\t\t__func__, vd, cookie, vd_chained ? \"cold\" : \"not\");\n\tlist_move_tail(&vd->node, &vc->desc_submitted);\n\tchan->misaligned |= to_pxad_sw_desc(vd)->misaligned;\n\nout:\n\tspin_unlock_irqrestore(&vc->lock, flags);\n\treturn cookie;\n}\n\nstatic void pxad_issue_pending(struct dma_chan *dchan)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\tstruct virt_dma_desc *vd_first;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\tif (list_empty(&chan->vc.desc_submitted))\n\t\tgoto out;\n\n\tvd_first = list_first_entry(&chan->vc.desc_submitted,\n\t\t\t\t    struct virt_dma_desc, node);\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): txd %p[%x]\", __func__, vd_first, vd_first->tx.cookie);\n\n\tvchan_issue_pending(&chan->vc);\n\tif (!pxad_try_hotchain(&chan->vc, vd_first))\n\t\tpxad_launch_chan(chan, to_pxad_sw_desc(vd_first));\nout:\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n}\n\nstatic inline struct dma_async_tx_descriptor *\npxad_tx_prep(struct virt_dma_chan *vc, struct virt_dma_desc *vd,\n\t\t unsigned long tx_flags)\n{\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct pxad_chan *chan = container_of(vc, struct pxad_chan, vc);\n\n\tINIT_LIST_HEAD(&vd->node);\n\ttx = vchan_tx_prep(vc, vd, tx_flags);\n\ttx->tx_submit = pxad_tx_submit;\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): vc=%p txd=%p[%x] flags=0x%lx\\n\", __func__,\n\t\tvc, vd, vd->tx.cookie,\n\t\ttx_flags);\n\n\treturn tx;\n}\n\nstatic void pxad_get_config(struct pxad_chan *chan,\n\t\t\t    enum dma_transfer_direction dir,\n\t\t\t    u32 *dcmd, u32 *dev_src, u32 *dev_dst)\n{\n\tu32 maxburst = 0, dev_addr = 0;\n\tenum dma_slave_buswidth width = DMA_SLAVE_BUSWIDTH_UNDEFINED;\n\tstruct pxad_device *pdev = to_pxad_dev(chan->vc.chan.device);\n\n\t*dcmd = 0;\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\tmaxburst = chan->cfg.src_maxburst;\n\t\twidth = chan->cfg.src_addr_width;\n\t\tdev_addr = chan->cfg.src_addr;\n\t\t*dev_src = dev_addr;\n\t\t*dcmd |= PXA_DCMD_INCTRGADDR;\n\t\tif (chan->drcmr <= pdev->nr_requestors)\n\t\t\t*dcmd |= PXA_DCMD_FLOWSRC;\n\t}\n\tif (dir == DMA_MEM_TO_DEV) {\n\t\tmaxburst = chan->cfg.dst_maxburst;\n\t\twidth = chan->cfg.dst_addr_width;\n\t\tdev_addr = chan->cfg.dst_addr;\n\t\t*dev_dst = dev_addr;\n\t\t*dcmd |= PXA_DCMD_INCSRCADDR;\n\t\tif (chan->drcmr <= pdev->nr_requestors)\n\t\t\t*dcmd |= PXA_DCMD_FLOWTRG;\n\t}\n\tif (dir == DMA_MEM_TO_MEM)\n\t\t*dcmd |= PXA_DCMD_BURST32 | PXA_DCMD_INCTRGADDR |\n\t\t\tPXA_DCMD_INCSRCADDR;\n\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): dev_addr=0x%x maxburst=%d width=%d  dir=%d\\n\",\n\t\t__func__, dev_addr, maxburst, width, dir);\n\n\tif (width == DMA_SLAVE_BUSWIDTH_1_BYTE)\n\t\t*dcmd |= PXA_DCMD_WIDTH1;\n\telse if (width == DMA_SLAVE_BUSWIDTH_2_BYTES)\n\t\t*dcmd |= PXA_DCMD_WIDTH2;\n\telse if (width == DMA_SLAVE_BUSWIDTH_4_BYTES)\n\t\t*dcmd |= PXA_DCMD_WIDTH4;\n\n\tif (maxburst == 8)\n\t\t*dcmd |= PXA_DCMD_BURST8;\n\telse if (maxburst == 16)\n\t\t*dcmd |= PXA_DCMD_BURST16;\n\telse if (maxburst == 32)\n\t\t*dcmd |= PXA_DCMD_BURST32;\n}\n\nstatic struct dma_async_tx_descriptor *\npxad_prep_memcpy(struct dma_chan *dchan,\n\t\t dma_addr_t dma_dst, dma_addr_t dma_src,\n\t\t size_t len, unsigned long flags)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\tstruct pxad_desc_sw *sw_desc;\n\tstruct pxad_desc_hw *hw_desc;\n\tu32 dcmd;\n\tunsigned int i, nb_desc = 0;\n\tsize_t copy;\n\n\tif (!dchan || !len)\n\t\treturn NULL;\n\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): dma_dst=0x%lx dma_src=0x%lx len=%zu flags=%lx\\n\",\n\t\t__func__, (unsigned long)dma_dst, (unsigned long)dma_src,\n\t\tlen, flags);\n\tpxad_get_config(chan, DMA_MEM_TO_MEM, &dcmd, NULL, NULL);\n\n\tnb_desc = DIV_ROUND_UP(len, PDMA_MAX_DESC_BYTES);\n\tsw_desc = pxad_alloc_desc(chan, nb_desc + 1);\n\tif (!sw_desc)\n\t\treturn NULL;\n\tsw_desc->len = len;\n\n\tif (!IS_ALIGNED(dma_src, 1 << PDMA_ALIGNMENT) ||\n\t    !IS_ALIGNED(dma_dst, 1 << PDMA_ALIGNMENT))\n\t\tsw_desc->misaligned = true;\n\n\ti = 0;\n\tdo {\n\t\thw_desc = sw_desc->hw_desc[i++];\n\t\tcopy = min_t(size_t, len, PDMA_MAX_DESC_BYTES);\n\t\thw_desc->dcmd = dcmd | (PXA_DCMD_LENGTH & copy);\n\t\thw_desc->dsadr = dma_src;\n\t\thw_desc->dtadr = dma_dst;\n\t\tlen -= copy;\n\t\tdma_src += copy;\n\t\tdma_dst += copy;\n\t} while (len);\n\tset_updater_desc(sw_desc, flags);\n\n\treturn pxad_tx_prep(&chan->vc, &sw_desc->vd, flags);\n}\n\nstatic struct dma_async_tx_descriptor *\npxad_prep_slave_sg(struct dma_chan *dchan, struct scatterlist *sgl,\n\t\t   unsigned int sg_len, enum dma_transfer_direction dir,\n\t\t   unsigned long flags, void *context)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\tstruct pxad_desc_sw *sw_desc;\n\tsize_t len, avail;\n\tstruct scatterlist *sg;\n\tdma_addr_t dma;\n\tu32 dcmd, dsadr = 0, dtadr = 0;\n\tunsigned int nb_desc = 0, i, j = 0;\n\n\tif ((sgl == NULL) || (sg_len == 0))\n\t\treturn NULL;\n\n\tpxad_get_config(chan, dir, &dcmd, &dsadr, &dtadr);\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): dir=%d flags=%lx\\n\", __func__, dir, flags);\n\n\tfor_each_sg(sgl, sg, sg_len, i)\n\t\tnb_desc += DIV_ROUND_UP(sg_dma_len(sg), PDMA_MAX_DESC_BYTES);\n\tsw_desc = pxad_alloc_desc(chan, nb_desc + 1);\n\tif (!sw_desc)\n\t\treturn NULL;\n\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tdma = sg_dma_address(sg);\n\t\tavail = sg_dma_len(sg);\n\t\tsw_desc->len += avail;\n\n\t\tdo {\n\t\t\tlen = min_t(size_t, avail, PDMA_MAX_DESC_BYTES);\n\t\t\tif (dma & 0x7)\n\t\t\t\tsw_desc->misaligned = true;\n\n\t\t\tsw_desc->hw_desc[j]->dcmd =\n\t\t\t\tdcmd | (PXA_DCMD_LENGTH & len);\n\t\t\tsw_desc->hw_desc[j]->dsadr = dsadr ? dsadr : dma;\n\t\t\tsw_desc->hw_desc[j++]->dtadr = dtadr ? dtadr : dma;\n\n\t\t\tdma += len;\n\t\t\tavail -= len;\n\t\t} while (avail);\n\t}\n\tset_updater_desc(sw_desc, flags);\n\n\treturn pxad_tx_prep(&chan->vc, &sw_desc->vd, flags);\n}\n\nstatic struct dma_async_tx_descriptor *\npxad_prep_dma_cyclic(struct dma_chan *dchan,\n\t\t     dma_addr_t buf_addr, size_t len, size_t period_len,\n\t\t     enum dma_transfer_direction dir, unsigned long flags)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\tstruct pxad_desc_sw *sw_desc;\n\tstruct pxad_desc_hw **phw_desc;\n\tdma_addr_t dma;\n\tu32 dcmd, dsadr = 0, dtadr = 0;\n\tunsigned int nb_desc = 0;\n\n\tif (!dchan || !len || !period_len)\n\t\treturn NULL;\n\tif ((dir != DMA_DEV_TO_MEM) && (dir != DMA_MEM_TO_DEV)) {\n\t\tdev_err(&chan->vc.chan.dev->device,\n\t\t\t\"Unsupported direction for cyclic DMA\\n\");\n\t\treturn NULL;\n\t}\n\t \n\tif (len % period_len != 0 || period_len > PDMA_MAX_DESC_BYTES ||\n\t    !IS_ALIGNED(period_len, 1 << PDMA_ALIGNMENT))\n\t\treturn NULL;\n\n\tpxad_get_config(chan, dir, &dcmd, &dsadr, &dtadr);\n\tdcmd |= PXA_DCMD_ENDIRQEN | (PXA_DCMD_LENGTH & period_len);\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): buf_addr=0x%lx len=%zu period=%zu dir=%d flags=%lx\\n\",\n\t\t__func__, (unsigned long)buf_addr, len, period_len, dir, flags);\n\n\tnb_desc = DIV_ROUND_UP(period_len, PDMA_MAX_DESC_BYTES);\n\tnb_desc *= DIV_ROUND_UP(len, period_len);\n\tsw_desc = pxad_alloc_desc(chan, nb_desc + 1);\n\tif (!sw_desc)\n\t\treturn NULL;\n\tsw_desc->cyclic = true;\n\tsw_desc->len = len;\n\n\tphw_desc = sw_desc->hw_desc;\n\tdma = buf_addr;\n\tdo {\n\t\tphw_desc[0]->dsadr = dsadr ? dsadr : dma;\n\t\tphw_desc[0]->dtadr = dtadr ? dtadr : dma;\n\t\tphw_desc[0]->dcmd = dcmd;\n\t\tphw_desc++;\n\t\tdma += period_len;\n\t\tlen -= period_len;\n\t} while (len);\n\tset_updater_desc(sw_desc, flags);\n\n\treturn pxad_tx_prep(&chan->vc, &sw_desc->vd, flags);\n}\n\nstatic int pxad_config(struct dma_chan *dchan,\n\t\t       struct dma_slave_config *cfg)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\n\tif (!dchan)\n\t\treturn -EINVAL;\n\n\tchan->cfg = *cfg;\n\treturn 0;\n}\n\nstatic int pxad_terminate_all(struct dma_chan *dchan)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\tstruct pxad_device *pdev = to_pxad_dev(chan->vc.chan.device);\n\tstruct virt_dma_desc *vd = NULL;\n\tunsigned long flags;\n\tstruct pxad_phy *phy;\n\tLIST_HEAD(head);\n\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): vchan %p: terminate all\\n\", __func__, &chan->vc);\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\tvchan_get_all_descriptors(&chan->vc, &head);\n\n\tlist_for_each_entry(vd, &head, node) {\n\t\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\t\"%s(): cancelling txd %p[%x] (completed=%d)\", __func__,\n\t\t\tvd, vd->tx.cookie, is_desc_completed(vd));\n\t}\n\n\tphy = chan->phy;\n\tif (phy) {\n\t\tphy_disable(chan->phy);\n\t\tpxad_free_phy(chan);\n\t\tchan->phy = NULL;\n\t\tspin_lock(&pdev->phy_lock);\n\t\tphy->vchan = NULL;\n\t\tspin_unlock(&pdev->phy_lock);\n\t}\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\tvchan_dma_desc_free_list(&chan->vc, &head);\n\n\treturn 0;\n}\n\nstatic unsigned int pxad_residue(struct pxad_chan *chan,\n\t\t\t\t dma_cookie_t cookie)\n{\n\tstruct virt_dma_desc *vd = NULL;\n\tstruct pxad_desc_sw *sw_desc = NULL;\n\tstruct pxad_desc_hw *hw_desc = NULL;\n\tu32 curr, start, len, end, residue = 0;\n\tunsigned long flags;\n\tbool passed = false;\n\tint i;\n\n\t \n\tif (!chan->phy)\n\t\treturn 0;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\n\tvd = vchan_find_desc(&chan->vc, cookie);\n\tif (!vd)\n\t\tgoto out;\n\n\tsw_desc = to_pxad_sw_desc(vd);\n\tif (sw_desc->hw_desc[0]->dcmd & PXA_DCMD_INCSRCADDR)\n\t\tcurr = phy_readl_relaxed(chan->phy, DSADR);\n\telse\n\t\tcurr = phy_readl_relaxed(chan->phy, DTADR);\n\n\t \n\trmb();\n\tif (is_desc_completed(vd))\n\t\tgoto out;\n\n\tfor (i = 0; i < sw_desc->nb_desc - 1; i++) {\n\t\thw_desc = sw_desc->hw_desc[i];\n\t\tif (sw_desc->hw_desc[0]->dcmd & PXA_DCMD_INCSRCADDR)\n\t\t\tstart = hw_desc->dsadr;\n\t\telse\n\t\t\tstart = hw_desc->dtadr;\n\t\tlen = hw_desc->dcmd & PXA_DCMD_LENGTH;\n\t\tend = start + len;\n\n\t\t \n\n\t\tif (passed) {\n\t\t\tresidue += len;\n\t\t} else if (curr >= start && curr <= end) {\n\t\t\tresidue += end - curr;\n\t\t\tpassed = true;\n\t\t}\n\t}\n\tif (!passed)\n\t\tresidue = sw_desc->len;\n\nout:\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\tdev_dbg(&chan->vc.chan.dev->device,\n\t\t\"%s(): txd %p[%x] sw_desc=%p: %d\\n\",\n\t\t__func__, vd, cookie, sw_desc, residue);\n\treturn residue;\n}\n\nstatic enum dma_status pxad_tx_status(struct dma_chan *dchan,\n\t\t\t\t      dma_cookie_t cookie,\n\t\t\t\t      struct dma_tx_state *txstate)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\tenum dma_status ret;\n\n\tif (cookie == chan->bus_error)\n\t\treturn DMA_ERROR;\n\n\tret = dma_cookie_status(dchan, cookie, txstate);\n\tif (likely(txstate && (ret != DMA_ERROR)))\n\t\tdma_set_residue(txstate, pxad_residue(chan, cookie));\n\n\treturn ret;\n}\n\nstatic void pxad_synchronize(struct dma_chan *dchan)\n{\n\tstruct pxad_chan *chan = to_pxad_chan(dchan);\n\n\twait_event(chan->wq_state, !is_chan_running(chan));\n\tvchan_synchronize(&chan->vc);\n}\n\nstatic void pxad_free_channels(struct dma_device *dmadev)\n{\n\tstruct pxad_chan *c, *cn;\n\n\tlist_for_each_entry_safe(c, cn, &dmadev->channels,\n\t\t\t\t vc.chan.device_node) {\n\t\tlist_del(&c->vc.chan.device_node);\n\t\ttasklet_kill(&c->vc.task);\n\t}\n}\n\nstatic int pxad_remove(struct platform_device *op)\n{\n\tstruct pxad_device *pdev = platform_get_drvdata(op);\n\n\tpxad_cleanup_debugfs(pdev);\n\tpxad_free_channels(&pdev->slave);\n\treturn 0;\n}\n\nstatic int pxad_init_phys(struct platform_device *op,\n\t\t\t  struct pxad_device *pdev,\n\t\t\t  unsigned int nb_phy_chans)\n{\n\tint irq0, irq, nr_irq = 0, i, ret;\n\tstruct pxad_phy *phy;\n\n\tirq0 = platform_get_irq(op, 0);\n\tif (irq0 < 0)\n\t\treturn irq0;\n\n\tpdev->phys = devm_kcalloc(&op->dev, nb_phy_chans,\n\t\t\t\t  sizeof(pdev->phys[0]), GFP_KERNEL);\n\tif (!pdev->phys)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nb_phy_chans; i++)\n\t\tif (platform_get_irq_optional(op, i) > 0)\n\t\t\tnr_irq++;\n\n\tfor (i = 0; i < nb_phy_chans; i++) {\n\t\tphy = &pdev->phys[i];\n\t\tphy->base = pdev->base;\n\t\tphy->idx = i;\n\t\tirq = platform_get_irq_optional(op, i);\n\t\tif ((nr_irq > 1) && (irq > 0))\n\t\t\tret = devm_request_irq(&op->dev, irq,\n\t\t\t\t\t       pxad_chan_handler,\n\t\t\t\t\t       IRQF_SHARED, \"pxa-dma\", phy);\n\t\tif ((nr_irq == 1) && (i == 0))\n\t\t\tret = devm_request_irq(&op->dev, irq0,\n\t\t\t\t\t       pxad_int_handler,\n\t\t\t\t\t       IRQF_SHARED, \"pxa-dma\", pdev);\n\t\tif (ret) {\n\t\t\tdev_err(pdev->slave.dev,\n\t\t\t\t\"%s(): can't request irq %d:%d\\n\", __func__,\n\t\t\t\tirq, ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic const struct of_device_id pxad_dt_ids[] = {\n\t{ .compatible = \"marvell,pdma-1.0\", },\n\t{}\n};\nMODULE_DEVICE_TABLE(of, pxad_dt_ids);\n\nstatic struct dma_chan *pxad_dma_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t\t   struct of_dma *ofdma)\n{\n\tstruct pxad_device *d = ofdma->of_dma_data;\n\tstruct dma_chan *chan;\n\n\tchan = dma_get_any_slave_channel(&d->slave);\n\tif (!chan)\n\t\treturn NULL;\n\n\tto_pxad_chan(chan)->drcmr = dma_spec->args[0];\n\tto_pxad_chan(chan)->prio = dma_spec->args[1];\n\n\treturn chan;\n}\n\nstatic int pxad_init_dmadev(struct platform_device *op,\n\t\t\t    struct pxad_device *pdev,\n\t\t\t    unsigned int nr_phy_chans,\n\t\t\t    unsigned int nr_requestors)\n{\n\tint ret;\n\tunsigned int i;\n\tstruct pxad_chan *c;\n\n\tpdev->nr_chans = nr_phy_chans;\n\tpdev->nr_requestors = nr_requestors;\n\tINIT_LIST_HEAD(&pdev->slave.channels);\n\tpdev->slave.device_alloc_chan_resources = pxad_alloc_chan_resources;\n\tpdev->slave.device_free_chan_resources = pxad_free_chan_resources;\n\tpdev->slave.device_tx_status = pxad_tx_status;\n\tpdev->slave.device_issue_pending = pxad_issue_pending;\n\tpdev->slave.device_config = pxad_config;\n\tpdev->slave.device_synchronize = pxad_synchronize;\n\tpdev->slave.device_terminate_all = pxad_terminate_all;\n\n\tif (op->dev.coherent_dma_mask)\n\t\tdma_set_mask(&op->dev, op->dev.coherent_dma_mask);\n\telse\n\t\tdma_set_mask(&op->dev, DMA_BIT_MASK(32));\n\n\tret = pxad_init_phys(op, pdev, nr_phy_chans);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < nr_phy_chans; i++) {\n\t\tc = devm_kzalloc(&op->dev, sizeof(*c), GFP_KERNEL);\n\t\tif (!c)\n\t\t\treturn -ENOMEM;\n\n\t\tc->drcmr = U32_MAX;\n\t\tc->prio = PXAD_PRIO_LOWEST;\n\t\tc->vc.desc_free = pxad_free_desc;\n\t\tvchan_init(&c->vc, &pdev->slave);\n\t\tinit_waitqueue_head(&c->wq_state);\n\t}\n\n\treturn dmaenginem_async_device_register(&pdev->slave);\n}\n\nstatic int pxad_probe(struct platform_device *op)\n{\n\tstruct pxad_device *pdev;\n\tconst struct of_device_id *of_id;\n\tconst struct dma_slave_map *slave_map = NULL;\n\tstruct mmp_dma_platdata *pdata = dev_get_platdata(&op->dev);\n\tint ret, dma_channels = 0, nb_requestors = 0, slave_map_cnt = 0;\n\tconst enum dma_slave_buswidth widths =\n\t\tDMA_SLAVE_BUSWIDTH_1_BYTE   | DMA_SLAVE_BUSWIDTH_2_BYTES |\n\t\tDMA_SLAVE_BUSWIDTH_4_BYTES;\n\n\tpdev = devm_kzalloc(&op->dev, sizeof(*pdev), GFP_KERNEL);\n\tif (!pdev)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&pdev->phy_lock);\n\n\tpdev->base = devm_platform_ioremap_resource(op, 0);\n\tif (IS_ERR(pdev->base))\n\t\treturn PTR_ERR(pdev->base);\n\n\tof_id = of_match_device(pxad_dt_ids, &op->dev);\n\tif (of_id) {\n\t\t \n\t\tif (of_property_read_u32(op->dev.of_node, \"dma-channels\",\n\t\t\t\t\t &dma_channels))\n\t\t\tof_property_read_u32(op->dev.of_node, \"#dma-channels\",\n\t\t\t\t\t     &dma_channels);\n\t\t \n\t\tret = of_property_read_u32(op->dev.of_node, \"dma-requests\",\n\t\t\t\t\t   &nb_requestors);\n\t\tif (ret)\n\t\t\tret = of_property_read_u32(op->dev.of_node, \"#dma-requests\",\n\t\t\t\t\t\t   &nb_requestors);\n\t\tif (ret) {\n\t\t\tdev_warn(pdev->slave.dev,\n\t\t\t\t \"#dma-requests set to default 32 as missing in OF: %d\",\n\t\t\t\t ret);\n\t\t\tnb_requestors = 32;\n\t\t}\n\t} else if (pdata && pdata->dma_channels) {\n\t\tdma_channels = pdata->dma_channels;\n\t\tnb_requestors = pdata->nb_requestors;\n\t\tslave_map = pdata->slave_map;\n\t\tslave_map_cnt = pdata->slave_map_cnt;\n\t} else {\n\t\tdma_channels = 32;\t \n\t}\n\n\tdma_cap_set(DMA_SLAVE, pdev->slave.cap_mask);\n\tdma_cap_set(DMA_MEMCPY, pdev->slave.cap_mask);\n\tdma_cap_set(DMA_CYCLIC, pdev->slave.cap_mask);\n\tdma_cap_set(DMA_PRIVATE, pdev->slave.cap_mask);\n\tpdev->slave.device_prep_dma_memcpy = pxad_prep_memcpy;\n\tpdev->slave.device_prep_slave_sg = pxad_prep_slave_sg;\n\tpdev->slave.device_prep_dma_cyclic = pxad_prep_dma_cyclic;\n\tpdev->slave.filter.map = slave_map;\n\tpdev->slave.filter.mapcnt = slave_map_cnt;\n\tpdev->slave.filter.fn = pxad_filter_fn;\n\n\tpdev->slave.copy_align = PDMA_ALIGNMENT;\n\tpdev->slave.src_addr_widths = widths;\n\tpdev->slave.dst_addr_widths = widths;\n\tpdev->slave.directions = BIT(DMA_MEM_TO_DEV) | BIT(DMA_DEV_TO_MEM);\n\tpdev->slave.residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\tpdev->slave.descriptor_reuse = true;\n\n\tpdev->slave.dev = &op->dev;\n\tret = pxad_init_dmadev(op, pdev, dma_channels, nb_requestors);\n\tif (ret) {\n\t\tdev_err(pdev->slave.dev, \"unable to register\\n\");\n\t\treturn ret;\n\t}\n\n\tif (op->dev.of_node) {\n\t\t \n\t\tret = of_dma_controller_register(op->dev.of_node,\n\t\t\t\t\t\t pxad_dma_xlate, pdev);\n\t\tif (ret < 0) {\n\t\t\tdev_err(pdev->slave.dev,\n\t\t\t\t\"of_dma_controller_register failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tplatform_set_drvdata(op, pdev);\n\tpxad_init_debugfs(pdev);\n\tdev_info(pdev->slave.dev, \"initialized %d channels on %d requestors\\n\",\n\t\t dma_channels, nb_requestors);\n\treturn 0;\n}\n\nstatic const struct platform_device_id pxad_id_table[] = {\n\t{ \"pxa-dma\", },\n\t{ },\n};\n\nstatic struct platform_driver pxad_driver = {\n\t.driver\t\t= {\n\t\t.name\t= \"pxa-dma\",\n\t\t.of_match_table = pxad_dt_ids,\n\t},\n\t.id_table\t= pxad_id_table,\n\t.probe\t\t= pxad_probe,\n\t.remove\t\t= pxad_remove,\n};\n\nstatic bool pxad_filter_fn(struct dma_chan *chan, void *param)\n{\n\tstruct pxad_chan *c = to_pxad_chan(chan);\n\tstruct pxad_param *p = param;\n\n\tif (chan->device->dev->driver != &pxad_driver.driver)\n\t\treturn false;\n\n\tc->drcmr = p->drcmr;\n\tc->prio = p->prio;\n\n\treturn true;\n}\n\nmodule_platform_driver(pxad_driver);\n\nMODULE_DESCRIPTION(\"Marvell PXA Peripheral DMA Driver\");\nMODULE_AUTHOR(\"Robert Jarzmik <robert.jarzmik@free.fr>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}