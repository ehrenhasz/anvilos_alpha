{
  "module_name": "ste_dma40.c",
  "hash_id": "5f55fe8d0ad27a23aedc44a08cb97e9ff052b84dbb9a2cf7904e5800bbd9c9eb",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/ste_dma40.c",
  "human_readable_source": "\n \n\n#include <linux/dma-mapping.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/dmaengine.h>\n#include <linux/platform_device.h>\n#include <linux/clk.h>\n#include <linux/delay.h>\n#include <linux/log2.h>\n#include <linux/pm.h>\n#include <linux/pm_runtime.h>\n#include <linux/err.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/of_dma.h>\n#include <linux/amba/bus.h>\n#include <linux/regulator/consumer.h>\n\n#include \"dmaengine.h\"\n#include \"ste_dma40.h\"\n#include \"ste_dma40_ll.h\"\n\n \nstruct stedma40_platform_data {\n\tint\t\t\t\t disabled_channels[STEDMA40_MAX_PHYS];\n\tint\t\t\t\t*soft_lli_chans;\n\tint\t\t\t\t num_of_soft_lli_chans;\n\tbool\t\t\t\t use_esram_lcla;\n\tint\t\t\t\t num_of_memcpy_chans;\n\tint\t\t\t\t num_of_phy_chans;\n};\n\n#define D40_NAME \"dma40\"\n\n#define D40_PHY_CHAN -1\n\n \n#define D40_CHAN_POS(chan)  (2 * (chan / 2))\n#define D40_CHAN_POS_MASK(chan) (0x3 << D40_CHAN_POS(chan))\n\n \n#define D40_SUSPEND_MAX_IT 500\n\n \n#define DMA40_AUTOSUSPEND_DELAY\t100\n\n \n#define LCLA_ALIGNMENT 0x40000\n\n \n#define D40_LCLA_LINK_PER_EVENT_GRP 128\n#define D40_LCLA_END D40_LCLA_LINK_PER_EVENT_GRP\n\n \n#define D40_MAX_LOG_CHAN_PER_PHY 32\n\n \n#define MAX_LCLA_ALLOC_ATTEMPTS 256\n\n \n#define D40_ALLOC_FREE\t\tBIT(31)\n#define D40_ALLOC_PHY\t\tBIT(30)\n#define D40_ALLOC_LOG_FREE\t0\n\n#define D40_MEMCPY_MAX_CHANS\t8\n\n \n#define DB8500_DMA_MEMCPY_EV_0\t51\n#define DB8500_DMA_MEMCPY_EV_1\t56\n#define DB8500_DMA_MEMCPY_EV_2\t57\n#define DB8500_DMA_MEMCPY_EV_3\t58\n#define DB8500_DMA_MEMCPY_EV_4\t59\n#define DB8500_DMA_MEMCPY_EV_5\t60\n\nstatic int dma40_memcpy_channels[] = {\n\tDB8500_DMA_MEMCPY_EV_0,\n\tDB8500_DMA_MEMCPY_EV_1,\n\tDB8500_DMA_MEMCPY_EV_2,\n\tDB8500_DMA_MEMCPY_EV_3,\n\tDB8500_DMA_MEMCPY_EV_4,\n\tDB8500_DMA_MEMCPY_EV_5,\n};\n\n \nstatic const struct stedma40_chan_cfg dma40_memcpy_conf_phy = {\n\t.mode = STEDMA40_MODE_PHYSICAL,\n\t.dir = DMA_MEM_TO_MEM,\n\n\t.src_info.data_width = DMA_SLAVE_BUSWIDTH_1_BYTE,\n\t.src_info.psize = STEDMA40_PSIZE_PHY_1,\n\t.src_info.flow_ctrl = STEDMA40_NO_FLOW_CTRL,\n\n\t.dst_info.data_width = DMA_SLAVE_BUSWIDTH_1_BYTE,\n\t.dst_info.psize = STEDMA40_PSIZE_PHY_1,\n\t.dst_info.flow_ctrl = STEDMA40_NO_FLOW_CTRL,\n};\n\n \nstatic const struct stedma40_chan_cfg dma40_memcpy_conf_log = {\n\t.mode = STEDMA40_MODE_LOGICAL,\n\t.dir = DMA_MEM_TO_MEM,\n\n\t.src_info.data_width = DMA_SLAVE_BUSWIDTH_1_BYTE,\n\t.src_info.psize = STEDMA40_PSIZE_LOG_1,\n\t.src_info.flow_ctrl = STEDMA40_NO_FLOW_CTRL,\n\n\t.dst_info.data_width = DMA_SLAVE_BUSWIDTH_1_BYTE,\n\t.dst_info.psize = STEDMA40_PSIZE_LOG_1,\n\t.dst_info.flow_ctrl = STEDMA40_NO_FLOW_CTRL,\n};\n\n \nenum d40_command {\n\tD40_DMA_STOP\t\t= 0,\n\tD40_DMA_RUN\t\t= 1,\n\tD40_DMA_SUSPEND_REQ\t= 2,\n\tD40_DMA_SUSPENDED\t= 3\n};\n\n \n\nenum d40_events {\n\tD40_DEACTIVATE_EVENTLINE\t= 0,\n\tD40_ACTIVATE_EVENTLINE\t\t= 1,\n\tD40_SUSPEND_REQ_EVENTLINE\t= 2,\n\tD40_ROUND_EVENTLINE\t\t= 3\n};\n\n \nstatic __maybe_unused u32 d40_backup_regs[] = {\n\tD40_DREG_LCPA,\n\tD40_DREG_LCLA,\n\tD40_DREG_PRMSE,\n\tD40_DREG_PRMSO,\n\tD40_DREG_PRMOE,\n\tD40_DREG_PRMOO,\n};\n\n#define BACKUP_REGS_SZ ARRAY_SIZE(d40_backup_regs)\n\n \nstatic u32 d40_backup_regs_v4a[] = {\n\tD40_DREG_PSEG1,\n\tD40_DREG_PSEG2,\n\tD40_DREG_PSEG3,\n\tD40_DREG_PSEG4,\n\tD40_DREG_PCEG1,\n\tD40_DREG_PCEG2,\n\tD40_DREG_PCEG3,\n\tD40_DREG_PCEG4,\n\tD40_DREG_RSEG1,\n\tD40_DREG_RSEG2,\n\tD40_DREG_RSEG3,\n\tD40_DREG_RSEG4,\n\tD40_DREG_RCEG1,\n\tD40_DREG_RCEG2,\n\tD40_DREG_RCEG3,\n\tD40_DREG_RCEG4,\n};\n\n#define BACKUP_REGS_SZ_V4A ARRAY_SIZE(d40_backup_regs_v4a)\n\nstatic u32 d40_backup_regs_v4b[] = {\n\tD40_DREG_CPSEG1,\n\tD40_DREG_CPSEG2,\n\tD40_DREG_CPSEG3,\n\tD40_DREG_CPSEG4,\n\tD40_DREG_CPSEG5,\n\tD40_DREG_CPCEG1,\n\tD40_DREG_CPCEG2,\n\tD40_DREG_CPCEG3,\n\tD40_DREG_CPCEG4,\n\tD40_DREG_CPCEG5,\n\tD40_DREG_CRSEG1,\n\tD40_DREG_CRSEG2,\n\tD40_DREG_CRSEG3,\n\tD40_DREG_CRSEG4,\n\tD40_DREG_CRSEG5,\n\tD40_DREG_CRCEG1,\n\tD40_DREG_CRCEG2,\n\tD40_DREG_CRCEG3,\n\tD40_DREG_CRCEG4,\n\tD40_DREG_CRCEG5,\n};\n\n#define BACKUP_REGS_SZ_V4B ARRAY_SIZE(d40_backup_regs_v4b)\n\nstatic __maybe_unused u32 d40_backup_regs_chan[] = {\n\tD40_CHAN_REG_SSCFG,\n\tD40_CHAN_REG_SSELT,\n\tD40_CHAN_REG_SSPTR,\n\tD40_CHAN_REG_SSLNK,\n\tD40_CHAN_REG_SDCFG,\n\tD40_CHAN_REG_SDELT,\n\tD40_CHAN_REG_SDPTR,\n\tD40_CHAN_REG_SDLNK,\n};\n\n#define BACKUP_REGS_SZ_MAX ((BACKUP_REGS_SZ_V4A > BACKUP_REGS_SZ_V4B) ? \\\n\t\t\t     BACKUP_REGS_SZ_V4A : BACKUP_REGS_SZ_V4B)\n\n \nstruct d40_interrupt_lookup {\n\tu32 src;\n\tu32 clr;\n\tbool is_error;\n\tint offset;\n};\n\n\nstatic struct d40_interrupt_lookup il_v4a[] = {\n\t{D40_DREG_LCTIS0, D40_DREG_LCICR0, false,  0},\n\t{D40_DREG_LCTIS1, D40_DREG_LCICR1, false, 32},\n\t{D40_DREG_LCTIS2, D40_DREG_LCICR2, false, 64},\n\t{D40_DREG_LCTIS3, D40_DREG_LCICR3, false, 96},\n\t{D40_DREG_LCEIS0, D40_DREG_LCICR0, true,   0},\n\t{D40_DREG_LCEIS1, D40_DREG_LCICR1, true,  32},\n\t{D40_DREG_LCEIS2, D40_DREG_LCICR2, true,  64},\n\t{D40_DREG_LCEIS3, D40_DREG_LCICR3, true,  96},\n\t{D40_DREG_PCTIS,  D40_DREG_PCICR,  false, D40_PHY_CHAN},\n\t{D40_DREG_PCEIS,  D40_DREG_PCICR,  true,  D40_PHY_CHAN},\n};\n\nstatic struct d40_interrupt_lookup il_v4b[] = {\n\t{D40_DREG_CLCTIS1, D40_DREG_CLCICR1, false,  0},\n\t{D40_DREG_CLCTIS2, D40_DREG_CLCICR2, false, 32},\n\t{D40_DREG_CLCTIS3, D40_DREG_CLCICR3, false, 64},\n\t{D40_DREG_CLCTIS4, D40_DREG_CLCICR4, false, 96},\n\t{D40_DREG_CLCTIS5, D40_DREG_CLCICR5, false, 128},\n\t{D40_DREG_CLCEIS1, D40_DREG_CLCICR1, true,   0},\n\t{D40_DREG_CLCEIS2, D40_DREG_CLCICR2, true,  32},\n\t{D40_DREG_CLCEIS3, D40_DREG_CLCICR3, true,  64},\n\t{D40_DREG_CLCEIS4, D40_DREG_CLCICR4, true,  96},\n\t{D40_DREG_CLCEIS5, D40_DREG_CLCICR5, true,  128},\n\t{D40_DREG_CPCTIS,  D40_DREG_CPCICR,  false, D40_PHY_CHAN},\n\t{D40_DREG_CPCEIS,  D40_DREG_CPCICR,  true,  D40_PHY_CHAN},\n};\n\n \nstruct d40_reg_val {\n\tunsigned int reg;\n\tunsigned int val;\n};\n\nstatic __initdata struct d40_reg_val dma_init_reg_v4a[] = {\n\t \n\t{ .reg = D40_DREG_GCC,    .val = D40_DREG_GCC_ENABLE_ALL},\n\n\t \n\t{ .reg = D40_DREG_LCMIS0, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCMIS1, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCMIS2, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCMIS3, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCICR0, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCICR1, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCICR2, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCICR3, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCTIS0, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCTIS1, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCTIS2, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_LCTIS3, .val = 0xFFFFFFFF}\n};\nstatic __initdata struct d40_reg_val dma_init_reg_v4b[] = {\n\t \n\t{ .reg = D40_DREG_GCC,    .val = D40_DREG_GCC_ENABLE_ALL},\n\n\t \n\t{ .reg = D40_DREG_CLCMIS1, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCMIS2, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCMIS3, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCMIS4, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCMIS5, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCICR1, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCICR2, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCICR3, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCICR4, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCICR5, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCTIS1, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCTIS2, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCTIS3, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCTIS4, .val = 0xFFFFFFFF},\n\t{ .reg = D40_DREG_CLCTIS5, .val = 0xFFFFFFFF}\n};\n\n \nstruct d40_lli_pool {\n\tvoid\t*base;\n\tint\t size;\n\tdma_addr_t\tdma_addr;\n\t \n\tu8\t pre_alloc_lli[3 * sizeof(struct d40_phy_lli)];\n};\n\n \nstruct d40_desc {\n\t \n\tstruct d40_phy_lli_bidir\t lli_phy;\n\t \n\tstruct d40_log_lli_bidir\t lli_log;\n\n\tstruct d40_lli_pool\t\t lli_pool;\n\tint\t\t\t\t lli_len;\n\tint\t\t\t\t lli_current;\n\tint\t\t\t\t lcla_alloc;\n\n\tstruct dma_async_tx_descriptor\t txd;\n\tstruct list_head\t\t node;\n\n\tbool\t\t\t\t is_in_client_list;\n\tbool\t\t\t\t cyclic;\n};\n\n \nstruct d40_lcla_pool {\n\tvoid\t\t*base;\n\tdma_addr_t\tdma_addr;\n\tvoid\t\t*base_unaligned;\n\tint\t\t pages;\n\tspinlock_t\t lock;\n\tstruct d40_desc\t**alloc_map;\n};\n\n \nstruct d40_phy_res {\n\tspinlock_t lock;\n\tbool\t   reserved;\n\tint\t   num;\n\tu32\t   allocated_src;\n\tu32\t   allocated_dst;\n\tbool\t   use_soft_lli;\n};\n\nstruct d40_base;\n\n \nstruct d40_chan {\n\tspinlock_t\t\t\t lock;\n\tint\t\t\t\t log_num;\n\tint\t\t\t\t pending_tx;\n\tbool\t\t\t\t busy;\n\tstruct d40_phy_res\t\t*phy_chan;\n\tstruct dma_chan\t\t\t chan;\n\tstruct tasklet_struct\t\t tasklet;\n\tstruct list_head\t\t client;\n\tstruct list_head\t\t pending_queue;\n\tstruct list_head\t\t active;\n\tstruct list_head\t\t done;\n\tstruct list_head\t\t queue;\n\tstruct list_head\t\t prepare_queue;\n\tstruct stedma40_chan_cfg\t dma_cfg;\n\tstruct dma_slave_config\t\t slave_config;\n\tbool\t\t\t\t configured;\n\tstruct d40_base\t\t\t*base;\n\t \n\tu32\t\t\t\t src_def_cfg;\n\tu32\t\t\t\t dst_def_cfg;\n\tstruct d40_def_lcsp\t\t log_def;\n\tstruct d40_log_lli_full\t\t*lcpa;\n\t \n\tdma_addr_t\t\t\truntime_addr;\n\tenum dma_transfer_direction\truntime_direction;\n};\n\n \nstruct d40_gen_dmac {\n\tu32\t\t\t\t*backup;\n\tu32\t\t\t\t backup_size;\n\tu32\t\t\t\t realtime_en;\n\tu32\t\t\t\t realtime_clear;\n\tu32\t\t\t\t high_prio_en;\n\tu32\t\t\t\t high_prio_clear;\n\tu32\t\t\t\t interrupt_en;\n\tu32\t\t\t\t interrupt_clear;\n\tstruct d40_interrupt_lookup\t*il;\n\tu32\t\t\t\t il_size;\n\tstruct d40_reg_val\t\t*init_reg;\n\tu32\t\t\t\t init_reg_size;\n};\n\n \nstruct d40_base {\n\tspinlock_t\t\t\t interrupt_lock;\n\tspinlock_t\t\t\t execmd_lock;\n\tstruct device\t\t\t *dev;\n\tvoid __iomem\t\t\t *virtbase;\n\tu8\t\t\t\t  rev:4;\n\tstruct clk\t\t\t *clk;\n\tint\t\t\t\t  irq;\n\tint\t\t\t\t  num_memcpy_chans;\n\tint\t\t\t\t  num_phy_chans;\n\tint\t\t\t\t  num_log_chans;\n\tstruct dma_device\t\t  dma_both;\n\tstruct dma_device\t\t  dma_slave;\n\tstruct dma_device\t\t  dma_memcpy;\n\tstruct d40_chan\t\t\t *phy_chans;\n\tstruct d40_chan\t\t\t *log_chans;\n\tstruct d40_chan\t\t\t**lookup_log_chans;\n\tstruct d40_chan\t\t\t**lookup_phy_chans;\n\tstruct stedma40_platform_data\t *plat_data;\n\tstruct regulator\t\t *lcpa_regulator;\n\t \n\tstruct d40_phy_res\t\t *phy_res;\n\tstruct d40_lcla_pool\t\t  lcla_pool;\n\tvoid\t\t\t\t *lcpa_base;\n\tdma_addr_t\t\t\t  phy_lcpa;\n\tresource_size_t\t\t\t  lcpa_size;\n\tstruct kmem_cache\t\t *desc_slab;\n\tu32\t\t\t\t  reg_val_backup[BACKUP_REGS_SZ];\n\tu32\t\t\t\t  reg_val_backup_v4[BACKUP_REGS_SZ_MAX];\n\tu32\t\t\t\t *reg_val_backup_chan;\n\tu32\t\t\t\t *regs_interrupt;\n\tu16\t\t\t\t  gcc_pwr_off_mask;\n\tstruct d40_gen_dmac\t\t  gen_dmac;\n};\n\nstatic struct device *chan2dev(struct d40_chan *d40c)\n{\n\treturn &d40c->chan.dev->device;\n}\n\nstatic bool chan_is_physical(struct d40_chan *chan)\n{\n\treturn chan->log_num == D40_PHY_CHAN;\n}\n\nstatic bool chan_is_logical(struct d40_chan *chan)\n{\n\treturn !chan_is_physical(chan);\n}\n\nstatic void __iomem *chan_base(struct d40_chan *chan)\n{\n\treturn chan->base->virtbase + D40_DREG_PCBASE +\n\t       chan->phy_chan->num * D40_DREG_PCDELTA;\n}\n\n#define d40_err(dev, format, arg...)\t\t\\\n\tdev_err(dev, \"[%s] \" format, __func__, ## arg)\n\n#define chan_err(d40c, format, arg...)\t\t\\\n\td40_err(chan2dev(d40c), format, ## arg)\n\nstatic int d40_set_runtime_config_write(struct dma_chan *chan,\n\t\t\t\t  struct dma_slave_config *config,\n\t\t\t\t  enum dma_transfer_direction direction);\n\nstatic int d40_pool_lli_alloc(struct d40_chan *d40c, struct d40_desc *d40d,\n\t\t\t      int lli_len)\n{\n\tbool is_log = chan_is_logical(d40c);\n\tu32 align;\n\tvoid *base;\n\n\tif (is_log)\n\t\talign = sizeof(struct d40_log_lli);\n\telse\n\t\talign = sizeof(struct d40_phy_lli);\n\n\tif (lli_len == 1) {\n\t\tbase = d40d->lli_pool.pre_alloc_lli;\n\t\td40d->lli_pool.size = sizeof(d40d->lli_pool.pre_alloc_lli);\n\t\td40d->lli_pool.base = NULL;\n\t} else {\n\t\td40d->lli_pool.size = lli_len * 2 * align;\n\n\t\tbase = kmalloc(d40d->lli_pool.size + align, GFP_NOWAIT);\n\t\td40d->lli_pool.base = base;\n\n\t\tif (d40d->lli_pool.base == NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (is_log) {\n\t\td40d->lli_log.src = PTR_ALIGN(base, align);\n\t\td40d->lli_log.dst = d40d->lli_log.src + lli_len;\n\n\t\td40d->lli_pool.dma_addr = 0;\n\t} else {\n\t\td40d->lli_phy.src = PTR_ALIGN(base, align);\n\t\td40d->lli_phy.dst = d40d->lli_phy.src + lli_len;\n\n\t\td40d->lli_pool.dma_addr = dma_map_single(d40c->base->dev,\n\t\t\t\t\t\t\t d40d->lli_phy.src,\n\t\t\t\t\t\t\t d40d->lli_pool.size,\n\t\t\t\t\t\t\t DMA_TO_DEVICE);\n\n\t\tif (dma_mapping_error(d40c->base->dev,\n\t\t\t\t      d40d->lli_pool.dma_addr)) {\n\t\t\tkfree(d40d->lli_pool.base);\n\t\t\td40d->lli_pool.base = NULL;\n\t\t\td40d->lli_pool.dma_addr = 0;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void d40_pool_lli_free(struct d40_chan *d40c, struct d40_desc *d40d)\n{\n\tif (d40d->lli_pool.dma_addr)\n\t\tdma_unmap_single(d40c->base->dev, d40d->lli_pool.dma_addr,\n\t\t\t\t d40d->lli_pool.size, DMA_TO_DEVICE);\n\n\tkfree(d40d->lli_pool.base);\n\td40d->lli_pool.base = NULL;\n\td40d->lli_pool.size = 0;\n\td40d->lli_log.src = NULL;\n\td40d->lli_log.dst = NULL;\n\td40d->lli_phy.src = NULL;\n\td40d->lli_phy.dst = NULL;\n}\n\nstatic int d40_lcla_alloc_one(struct d40_chan *d40c,\n\t\t\t      struct d40_desc *d40d)\n{\n\tunsigned long flags;\n\tint i;\n\tint ret = -EINVAL;\n\n\tspin_lock_irqsave(&d40c->base->lcla_pool.lock, flags);\n\n\t \n\tfor (i = 1 ; i < D40_LCLA_LINK_PER_EVENT_GRP / 2; i++) {\n\t\tint idx = d40c->phy_chan->num * D40_LCLA_LINK_PER_EVENT_GRP + i;\n\n\t\tif (!d40c->base->lcla_pool.alloc_map[idx]) {\n\t\t\td40c->base->lcla_pool.alloc_map[idx] = d40d;\n\t\t\td40d->lcla_alloc++;\n\t\t\tret = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&d40c->base->lcla_pool.lock, flags);\n\n\treturn ret;\n}\n\nstatic int d40_lcla_free_all(struct d40_chan *d40c,\n\t\t\t     struct d40_desc *d40d)\n{\n\tunsigned long flags;\n\tint i;\n\tint ret = -EINVAL;\n\n\tif (chan_is_physical(d40c))\n\t\treturn 0;\n\n\tspin_lock_irqsave(&d40c->base->lcla_pool.lock, flags);\n\n\tfor (i = 1 ; i < D40_LCLA_LINK_PER_EVENT_GRP / 2; i++) {\n\t\tint idx = d40c->phy_chan->num * D40_LCLA_LINK_PER_EVENT_GRP + i;\n\n\t\tif (d40c->base->lcla_pool.alloc_map[idx] == d40d) {\n\t\t\td40c->base->lcla_pool.alloc_map[idx] = NULL;\n\t\t\td40d->lcla_alloc--;\n\t\t\tif (d40d->lcla_alloc == 0) {\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&d40c->base->lcla_pool.lock, flags);\n\n\treturn ret;\n\n}\n\nstatic void d40_desc_remove(struct d40_desc *d40d)\n{\n\tlist_del(&d40d->node);\n}\n\nstatic struct d40_desc *d40_desc_get(struct d40_chan *d40c)\n{\n\tstruct d40_desc *desc = NULL;\n\n\tif (!list_empty(&d40c->client)) {\n\t\tstruct d40_desc *d;\n\t\tstruct d40_desc *_d;\n\n\t\tlist_for_each_entry_safe(d, _d, &d40c->client, node) {\n\t\t\tif (async_tx_test_ack(&d->txd)) {\n\t\t\t\td40_desc_remove(d);\n\t\t\t\tdesc = d;\n\t\t\t\tmemset(desc, 0, sizeof(*desc));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!desc)\n\t\tdesc = kmem_cache_zalloc(d40c->base->desc_slab, GFP_NOWAIT);\n\n\tif (desc)\n\t\tINIT_LIST_HEAD(&desc->node);\n\n\treturn desc;\n}\n\nstatic void d40_desc_free(struct d40_chan *d40c, struct d40_desc *d40d)\n{\n\n\td40_pool_lli_free(d40c, d40d);\n\td40_lcla_free_all(d40c, d40d);\n\tkmem_cache_free(d40c->base->desc_slab, d40d);\n}\n\nstatic void d40_desc_submit(struct d40_chan *d40c, struct d40_desc *desc)\n{\n\tlist_add_tail(&desc->node, &d40c->active);\n}\n\nstatic void d40_phy_lli_load(struct d40_chan *chan, struct d40_desc *desc)\n{\n\tstruct d40_phy_lli *lli_dst = desc->lli_phy.dst;\n\tstruct d40_phy_lli *lli_src = desc->lli_phy.src;\n\tvoid __iomem *base = chan_base(chan);\n\n\twritel(lli_src->reg_cfg, base + D40_CHAN_REG_SSCFG);\n\twritel(lli_src->reg_elt, base + D40_CHAN_REG_SSELT);\n\twritel(lli_src->reg_ptr, base + D40_CHAN_REG_SSPTR);\n\twritel(lli_src->reg_lnk, base + D40_CHAN_REG_SSLNK);\n\n\twritel(lli_dst->reg_cfg, base + D40_CHAN_REG_SDCFG);\n\twritel(lli_dst->reg_elt, base + D40_CHAN_REG_SDELT);\n\twritel(lli_dst->reg_ptr, base + D40_CHAN_REG_SDPTR);\n\twritel(lli_dst->reg_lnk, base + D40_CHAN_REG_SDLNK);\n}\n\nstatic void d40_desc_done(struct d40_chan *d40c, struct d40_desc *desc)\n{\n\tlist_add_tail(&desc->node, &d40c->done);\n}\n\nstatic void d40_log_lli_to_lcxa(struct d40_chan *chan, struct d40_desc *desc)\n{\n\tstruct d40_lcla_pool *pool = &chan->base->lcla_pool;\n\tstruct d40_log_lli_bidir *lli = &desc->lli_log;\n\tint lli_current = desc->lli_current;\n\tint lli_len = desc->lli_len;\n\tbool cyclic = desc->cyclic;\n\tint curr_lcla = -EINVAL;\n\tint first_lcla = 0;\n\tbool use_esram_lcla = chan->base->plat_data->use_esram_lcla;\n\tbool linkback;\n\n\t \n\tlinkback = cyclic && lli_current == 0;\n\n\t \n\tif (linkback || (lli_len - lli_current > 1)) {\n\t\t \n\t\tif (!(chan->phy_chan->use_soft_lli &&\n\t\t\tchan->dma_cfg.dir == DMA_DEV_TO_MEM))\n\t\t\tcurr_lcla = d40_lcla_alloc_one(chan, desc);\n\n\t\tfirst_lcla = curr_lcla;\n\t}\n\n\t \n\tif (!linkback || curr_lcla == -EINVAL) {\n\t\tunsigned int flags = 0;\n\n\t\tif (curr_lcla == -EINVAL)\n\t\t\tflags |= LLI_TERM_INT;\n\n\t\td40_log_lli_lcpa_write(chan->lcpa,\n\t\t\t\t       &lli->dst[lli_current],\n\t\t\t\t       &lli->src[lli_current],\n\t\t\t\t       curr_lcla,\n\t\t\t\t       flags);\n\t\tlli_current++;\n\t}\n\n\tif (curr_lcla < 0)\n\t\tgoto set_current;\n\n\tfor (; lli_current < lli_len; lli_current++) {\n\t\tunsigned int lcla_offset = chan->phy_chan->num * 1024 +\n\t\t\t\t\t   8 * curr_lcla * 2;\n\t\tstruct d40_log_lli *lcla = pool->base + lcla_offset;\n\t\tunsigned int flags = 0;\n\t\tint next_lcla;\n\n\t\tif (lli_current + 1 < lli_len)\n\t\t\tnext_lcla = d40_lcla_alloc_one(chan, desc);\n\t\telse\n\t\t\tnext_lcla = linkback ? first_lcla : -EINVAL;\n\n\t\tif (cyclic || next_lcla == -EINVAL)\n\t\t\tflags |= LLI_TERM_INT;\n\n\t\tif (linkback && curr_lcla == first_lcla) {\n\t\t\t \n\t\t\td40_log_lli_lcpa_write(chan->lcpa,\n\t\t\t\t\t       &lli->dst[lli_current],\n\t\t\t\t\t       &lli->src[lli_current],\n\t\t\t\t\t       next_lcla, flags);\n\t\t}\n\n\t\t \n\t\td40_log_lli_lcla_write(lcla,\n\t\t\t\t       &lli->dst[lli_current],\n\t\t\t\t       &lli->src[lli_current],\n\t\t\t\t       next_lcla, flags);\n\n\t\t \n\t\tif (!use_esram_lcla) {\n\t\t\tdma_sync_single_range_for_device(chan->base->dev,\n\t\t\t\t\t\tpool->dma_addr, lcla_offset,\n\t\t\t\t\t\t2 * sizeof(struct d40_log_lli),\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\t}\n\t\tcurr_lcla = next_lcla;\n\n\t\tif (curr_lcla == -EINVAL || curr_lcla == first_lcla) {\n\t\t\tlli_current++;\n\t\t\tbreak;\n\t\t}\n\t}\n set_current:\n\tdesc->lli_current = lli_current;\n}\n\nstatic void d40_desc_load(struct d40_chan *d40c, struct d40_desc *d40d)\n{\n\tif (chan_is_physical(d40c)) {\n\t\td40_phy_lli_load(d40c, d40d);\n\t\td40d->lli_current = d40d->lli_len;\n\t} else\n\t\td40_log_lli_to_lcxa(d40c, d40d);\n}\n\nstatic struct d40_desc *d40_first_active_get(struct d40_chan *d40c)\n{\n\treturn list_first_entry_or_null(&d40c->active, struct d40_desc, node);\n}\n\n \nstatic void d40_desc_queue(struct d40_chan *d40c, struct d40_desc *desc)\n{\n\td40_desc_remove(desc);\n\tdesc->is_in_client_list = false;\n\tlist_add_tail(&desc->node, &d40c->pending_queue);\n}\n\nstatic struct d40_desc *d40_first_pending(struct d40_chan *d40c)\n{\n\treturn list_first_entry_or_null(&d40c->pending_queue, struct d40_desc,\n\t\t\t\t\tnode);\n}\n\nstatic struct d40_desc *d40_first_queued(struct d40_chan *d40c)\n{\n\treturn list_first_entry_or_null(&d40c->queue, struct d40_desc, node);\n}\n\nstatic struct d40_desc *d40_first_done(struct d40_chan *d40c)\n{\n\treturn list_first_entry_or_null(&d40c->done, struct d40_desc, node);\n}\n\nstatic int d40_psize_2_burst_size(bool is_log, int psize)\n{\n\tif (is_log) {\n\t\tif (psize == STEDMA40_PSIZE_LOG_1)\n\t\t\treturn 1;\n\t} else {\n\t\tif (psize == STEDMA40_PSIZE_PHY_1)\n\t\t\treturn 1;\n\t}\n\n\treturn 2 << psize;\n}\n\n \nstatic int d40_size_2_dmalen(int size, u32 data_width1, u32 data_width2)\n{\n\tint dmalen;\n\tu32 max_w = max(data_width1, data_width2);\n\tu32 min_w = min(data_width1, data_width2);\n\tu32 seg_max = ALIGN(STEDMA40_MAX_SEG_SIZE * min_w, max_w);\n\n\tif (seg_max > STEDMA40_MAX_SEG_SIZE)\n\t\tseg_max -= max_w;\n\n\tif (!IS_ALIGNED(size, max_w))\n\t\treturn -EINVAL;\n\n\tif (size <= seg_max)\n\t\tdmalen = 1;\n\telse {\n\t\tdmalen = size / seg_max;\n\t\tif (dmalen * seg_max < size)\n\t\t\tdmalen++;\n\t}\n\treturn dmalen;\n}\n\nstatic int d40_sg_2_dmalen(struct scatterlist *sgl, int sg_len,\n\t\t\t   u32 data_width1, u32 data_width2)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\tint len = 0;\n\tint ret;\n\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tret = d40_size_2_dmalen(sg_dma_len(sg),\n\t\t\t\t\tdata_width1, data_width2);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tlen += ret;\n\t}\n\treturn len;\n}\n\nstatic int __d40_execute_command_phy(struct d40_chan *d40c,\n\t\t\t\t     enum d40_command command)\n{\n\tu32 status;\n\tint i;\n\tvoid __iomem *active_reg;\n\tint ret = 0;\n\tunsigned long flags;\n\tu32 wmask;\n\n\tif (command == D40_DMA_STOP) {\n\t\tret = __d40_execute_command_phy(d40c, D40_DMA_SUSPEND_REQ);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tspin_lock_irqsave(&d40c->base->execmd_lock, flags);\n\n\tif (d40c->phy_chan->num % 2 == 0)\n\t\tactive_reg = d40c->base->virtbase + D40_DREG_ACTIVE;\n\telse\n\t\tactive_reg = d40c->base->virtbase + D40_DREG_ACTIVO;\n\n\tif (command == D40_DMA_SUSPEND_REQ) {\n\t\tstatus = (readl(active_reg) &\n\t\t\t  D40_CHAN_POS_MASK(d40c->phy_chan->num)) >>\n\t\t\tD40_CHAN_POS(d40c->phy_chan->num);\n\n\t\tif (status == D40_DMA_SUSPENDED || status == D40_DMA_STOP)\n\t\t\tgoto unlock;\n\t}\n\n\twmask = 0xffffffff & ~(D40_CHAN_POS_MASK(d40c->phy_chan->num));\n\twritel(wmask | (command << D40_CHAN_POS(d40c->phy_chan->num)),\n\t       active_reg);\n\n\tif (command == D40_DMA_SUSPEND_REQ) {\n\n\t\tfor (i = 0 ; i < D40_SUSPEND_MAX_IT; i++) {\n\t\t\tstatus = (readl(active_reg) &\n\t\t\t\t  D40_CHAN_POS_MASK(d40c->phy_chan->num)) >>\n\t\t\t\tD40_CHAN_POS(d40c->phy_chan->num);\n\n\t\t\tcpu_relax();\n\t\t\t \n\t\t\tudelay(3);\n\n\t\t\tif (status == D40_DMA_STOP ||\n\t\t\t    status == D40_DMA_SUSPENDED)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (i == D40_SUSPEND_MAX_IT) {\n\t\t\tchan_err(d40c,\n\t\t\t\t\"unable to suspend the chl %d (log: %d) status %x\\n\",\n\t\t\t\td40c->phy_chan->num, d40c->log_num,\n\t\t\t\tstatus);\n\t\t\tdump_stack();\n\t\t\tret = -EBUSY;\n\t\t}\n\n\t}\n unlock:\n\tspin_unlock_irqrestore(&d40c->base->execmd_lock, flags);\n\treturn ret;\n}\n\nstatic void d40_term_all(struct d40_chan *d40c)\n{\n\tstruct d40_desc *d40d;\n\tstruct d40_desc *_d;\n\n\t \n\twhile ((d40d = d40_first_done(d40c))) {\n\t\td40_desc_remove(d40d);\n\t\td40_desc_free(d40c, d40d);\n\t}\n\n\t \n\twhile ((d40d = d40_first_active_get(d40c))) {\n\t\td40_desc_remove(d40d);\n\t\td40_desc_free(d40c, d40d);\n\t}\n\n\t \n\twhile ((d40d = d40_first_queued(d40c))) {\n\t\td40_desc_remove(d40d);\n\t\td40_desc_free(d40c, d40d);\n\t}\n\n\t \n\twhile ((d40d = d40_first_pending(d40c))) {\n\t\td40_desc_remove(d40d);\n\t\td40_desc_free(d40c, d40d);\n\t}\n\n\t \n\tif (!list_empty(&d40c->client))\n\t\tlist_for_each_entry_safe(d40d, _d, &d40c->client, node) {\n\t\t\td40_desc_remove(d40d);\n\t\t\td40_desc_free(d40c, d40d);\n\t\t}\n\n\t \n\tif (!list_empty(&d40c->prepare_queue))\n\t\tlist_for_each_entry_safe(d40d, _d,\n\t\t\t\t\t &d40c->prepare_queue, node) {\n\t\t\td40_desc_remove(d40d);\n\t\t\td40_desc_free(d40c, d40d);\n\t\t}\n\n\td40c->pending_tx = 0;\n}\n\nstatic void __d40_config_set_event(struct d40_chan *d40c,\n\t\t\t\t   enum d40_events event_type, u32 event,\n\t\t\t\t   int reg)\n{\n\tvoid __iomem *addr = chan_base(d40c) + reg;\n\tint tries;\n\tu32 status;\n\n\tswitch (event_type) {\n\n\tcase D40_DEACTIVATE_EVENTLINE:\n\n\t\twritel((D40_DEACTIVATE_EVENTLINE << D40_EVENTLINE_POS(event))\n\t\t       | ~D40_EVENTLINE_MASK(event), addr);\n\t\tbreak;\n\n\tcase D40_SUSPEND_REQ_EVENTLINE:\n\t\tstatus = (readl(addr) & D40_EVENTLINE_MASK(event)) >>\n\t\t\t  D40_EVENTLINE_POS(event);\n\n\t\tif (status == D40_DEACTIVATE_EVENTLINE ||\n\t\t    status == D40_SUSPEND_REQ_EVENTLINE)\n\t\t\tbreak;\n\n\t\twritel((D40_SUSPEND_REQ_EVENTLINE << D40_EVENTLINE_POS(event))\n\t\t       | ~D40_EVENTLINE_MASK(event), addr);\n\n\t\tfor (tries = 0 ; tries < D40_SUSPEND_MAX_IT; tries++) {\n\n\t\t\tstatus = (readl(addr) & D40_EVENTLINE_MASK(event)) >>\n\t\t\t\t  D40_EVENTLINE_POS(event);\n\n\t\t\tcpu_relax();\n\t\t\t \n\t\t\tudelay(3);\n\n\t\t\tif (status == D40_DEACTIVATE_EVENTLINE)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (tries == D40_SUSPEND_MAX_IT) {\n\t\t\tchan_err(d40c,\n\t\t\t\t\"unable to stop the event_line chl %d (log: %d)\"\n\t\t\t\t\"status %x\\n\", d40c->phy_chan->num,\n\t\t\t\t d40c->log_num, status);\n\t\t}\n\t\tbreak;\n\n\tcase D40_ACTIVATE_EVENTLINE:\n\t \n\t\ttries = 100;\n\t\twhile (--tries) {\n\t\t\twritel((D40_ACTIVATE_EVENTLINE <<\n\t\t\t\tD40_EVENTLINE_POS(event)) |\n\t\t\t\t~D40_EVENTLINE_MASK(event), addr);\n\n\t\t\tif (readl(addr) & D40_EVENTLINE_MASK(event))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (tries != 99)\n\t\t\tdev_dbg(chan2dev(d40c),\n\t\t\t\t\"[%s] workaround enable S%cLNK (%d tries)\\n\",\n\t\t\t\t__func__, reg == D40_CHAN_REG_SSLNK ? 'S' : 'D',\n\t\t\t\t100 - tries);\n\n\t\tWARN_ON(!tries);\n\t\tbreak;\n\n\tcase D40_ROUND_EVENTLINE:\n\t\tBUG();\n\t\tbreak;\n\n\t}\n}\n\nstatic void d40_config_set_event(struct d40_chan *d40c,\n\t\t\t\t enum d40_events event_type)\n{\n\tu32 event = D40_TYPE_TO_EVENT(d40c->dma_cfg.dev_type);\n\n\t \n\tif ((d40c->dma_cfg.dir == DMA_DEV_TO_MEM) ||\n\t    (d40c->dma_cfg.dir == DMA_DEV_TO_DEV))\n\t\t__d40_config_set_event(d40c, event_type, event,\n\t\t\t\t       D40_CHAN_REG_SSLNK);\n\n\tif (d40c->dma_cfg.dir !=  DMA_DEV_TO_MEM)\n\t\t__d40_config_set_event(d40c, event_type, event,\n\t\t\t\t       D40_CHAN_REG_SDLNK);\n}\n\nstatic u32 d40_chan_has_events(struct d40_chan *d40c)\n{\n\tvoid __iomem *chanbase = chan_base(d40c);\n\tu32 val;\n\n\tval = readl(chanbase + D40_CHAN_REG_SSLNK);\n\tval |= readl(chanbase + D40_CHAN_REG_SDLNK);\n\n\treturn val;\n}\n\nstatic int\n__d40_execute_command_log(struct d40_chan *d40c, enum d40_command command)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\tu32 active_status;\n\tvoid __iomem *active_reg;\n\n\tif (d40c->phy_chan->num % 2 == 0)\n\t\tactive_reg = d40c->base->virtbase + D40_DREG_ACTIVE;\n\telse\n\t\tactive_reg = d40c->base->virtbase + D40_DREG_ACTIVO;\n\n\n\tspin_lock_irqsave(&d40c->phy_chan->lock, flags);\n\n\tswitch (command) {\n\tcase D40_DMA_STOP:\n\tcase D40_DMA_SUSPEND_REQ:\n\n\t\tactive_status = (readl(active_reg) &\n\t\t\t\t D40_CHAN_POS_MASK(d40c->phy_chan->num)) >>\n\t\t\t\t D40_CHAN_POS(d40c->phy_chan->num);\n\n\t\tif (active_status == D40_DMA_RUN)\n\t\t\td40_config_set_event(d40c, D40_SUSPEND_REQ_EVENTLINE);\n\t\telse\n\t\t\td40_config_set_event(d40c, D40_DEACTIVATE_EVENTLINE);\n\n\t\tif (!d40_chan_has_events(d40c) && (command == D40_DMA_STOP))\n\t\t\tret = __d40_execute_command_phy(d40c, command);\n\n\t\tbreak;\n\n\tcase D40_DMA_RUN:\n\n\t\td40_config_set_event(d40c, D40_ACTIVATE_EVENTLINE);\n\t\tret = __d40_execute_command_phy(d40c, command);\n\t\tbreak;\n\n\tcase D40_DMA_SUSPENDED:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tspin_unlock_irqrestore(&d40c->phy_chan->lock, flags);\n\treturn ret;\n}\n\nstatic int d40_channel_execute_command(struct d40_chan *d40c,\n\t\t\t\t       enum d40_command command)\n{\n\tif (chan_is_logical(d40c))\n\t\treturn __d40_execute_command_log(d40c, command);\n\telse\n\t\treturn __d40_execute_command_phy(d40c, command);\n}\n\nstatic u32 d40_get_prmo(struct d40_chan *d40c)\n{\n\tstatic const unsigned int phy_map[] = {\n\t\t[STEDMA40_PCHAN_BASIC_MODE]\n\t\t\t= D40_DREG_PRMO_PCHAN_BASIC,\n\t\t[STEDMA40_PCHAN_MODULO_MODE]\n\t\t\t= D40_DREG_PRMO_PCHAN_MODULO,\n\t\t[STEDMA40_PCHAN_DOUBLE_DST_MODE]\n\t\t\t= D40_DREG_PRMO_PCHAN_DOUBLE_DST,\n\t};\n\tstatic const unsigned int log_map[] = {\n\t\t[STEDMA40_LCHAN_SRC_PHY_DST_LOG]\n\t\t\t= D40_DREG_PRMO_LCHAN_SRC_PHY_DST_LOG,\n\t\t[STEDMA40_LCHAN_SRC_LOG_DST_PHY]\n\t\t\t= D40_DREG_PRMO_LCHAN_SRC_LOG_DST_PHY,\n\t\t[STEDMA40_LCHAN_SRC_LOG_DST_LOG]\n\t\t\t= D40_DREG_PRMO_LCHAN_SRC_LOG_DST_LOG,\n\t};\n\n\tif (chan_is_physical(d40c))\n\t\treturn phy_map[d40c->dma_cfg.mode_opt];\n\telse\n\t\treturn log_map[d40c->dma_cfg.mode_opt];\n}\n\nstatic void d40_config_write(struct d40_chan *d40c)\n{\n\tu32 addr_base;\n\tu32 var;\n\n\t \n\taddr_base = (d40c->phy_chan->num % 2) * 4;\n\t \n\tvar = ((u32)(chan_is_logical(d40c)) + 1) <<\n\t\tD40_CHAN_POS(d40c->phy_chan->num);\n\twritel(var, d40c->base->virtbase + D40_DREG_PRMSE + addr_base);\n\n\t \n\tvar = d40_get_prmo(d40c) << D40_CHAN_POS(d40c->phy_chan->num);\n\n\twritel(var, d40c->base->virtbase + D40_DREG_PRMOE + addr_base);\n\n\tif (chan_is_logical(d40c)) {\n\t\tint lidx = (d40c->phy_chan->num << D40_SREG_ELEM_LOG_LIDX_POS)\n\t\t\t   & D40_SREG_ELEM_LOG_LIDX_MASK;\n\t\tvoid __iomem *chanbase = chan_base(d40c);\n\n\t\t \n\t\twritel(d40c->src_def_cfg, chanbase + D40_CHAN_REG_SSCFG);\n\t\twritel(d40c->dst_def_cfg, chanbase + D40_CHAN_REG_SDCFG);\n\n\t\t \n\t\twritel(lidx, chanbase + D40_CHAN_REG_SSELT);\n\t\twritel(lidx, chanbase + D40_CHAN_REG_SDELT);\n\n\t\t \n\t\twritel(0, chanbase + D40_CHAN_REG_SSLNK);\n\t\twritel(0, chanbase + D40_CHAN_REG_SDLNK);\n\t}\n}\n\nstatic u32 d40_residue(struct d40_chan *d40c)\n{\n\tu32 num_elt;\n\n\tif (chan_is_logical(d40c))\n\t\tnum_elt = (readl(&d40c->lcpa->lcsp2) & D40_MEM_LCSP2_ECNT_MASK)\n\t\t\t>> D40_MEM_LCSP2_ECNT_POS;\n\telse {\n\t\tu32 val = readl(chan_base(d40c) + D40_CHAN_REG_SDELT);\n\t\tnum_elt = (val & D40_SREG_ELEM_PHY_ECNT_MASK)\n\t\t\t  >> D40_SREG_ELEM_PHY_ECNT_POS;\n\t}\n\n\treturn num_elt * d40c->dma_cfg.dst_info.data_width;\n}\n\nstatic bool d40_tx_is_linked(struct d40_chan *d40c)\n{\n\tbool is_link;\n\n\tif (chan_is_logical(d40c))\n\t\tis_link = readl(&d40c->lcpa->lcsp3) &  D40_MEM_LCSP3_DLOS_MASK;\n\telse\n\t\tis_link = readl(chan_base(d40c) + D40_CHAN_REG_SDLNK)\n\t\t\t  & D40_SREG_LNK_PHYS_LNK_MASK;\n\n\treturn is_link;\n}\n\nstatic int d40_pause(struct dma_chan *chan)\n{\n\tstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\n\tint res = 0;\n\tunsigned long flags;\n\n\tif (d40c->phy_chan == NULL) {\n\t\tchan_err(d40c, \"Channel is not allocated!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!d40c->busy)\n\t\treturn 0;\n\n\tspin_lock_irqsave(&d40c->lock, flags);\n\tpm_runtime_get_sync(d40c->base->dev);\n\n\tres = d40_channel_execute_command(d40c, D40_DMA_SUSPEND_REQ);\n\n\tpm_runtime_mark_last_busy(d40c->base->dev);\n\tpm_runtime_put_autosuspend(d40c->base->dev);\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n\treturn res;\n}\n\nstatic int d40_resume(struct dma_chan *chan)\n{\n\tstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\n\tint res = 0;\n\tunsigned long flags;\n\n\tif (d40c->phy_chan == NULL) {\n\t\tchan_err(d40c, \"Channel is not allocated!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!d40c->busy)\n\t\treturn 0;\n\n\tspin_lock_irqsave(&d40c->lock, flags);\n\tpm_runtime_get_sync(d40c->base->dev);\n\n\t \n\tif (d40_residue(d40c) || d40_tx_is_linked(d40c))\n\t\tres = d40_channel_execute_command(d40c, D40_DMA_RUN);\n\n\tpm_runtime_mark_last_busy(d40c->base->dev);\n\tpm_runtime_put_autosuspend(d40c->base->dev);\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n\treturn res;\n}\n\nstatic dma_cookie_t d40_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct d40_chan *d40c = container_of(tx->chan,\n\t\t\t\t\t     struct d40_chan,\n\t\t\t\t\t     chan);\n\tstruct d40_desc *d40d = container_of(tx, struct d40_desc, txd);\n\tunsigned long flags;\n\tdma_cookie_t cookie;\n\n\tspin_lock_irqsave(&d40c->lock, flags);\n\tcookie = dma_cookie_assign(tx);\n\td40_desc_queue(d40c, d40d);\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n\n\treturn cookie;\n}\n\nstatic int d40_start(struct d40_chan *d40c)\n{\n\treturn d40_channel_execute_command(d40c, D40_DMA_RUN);\n}\n\nstatic struct d40_desc *d40_queue_start(struct d40_chan *d40c)\n{\n\tstruct d40_desc *d40d;\n\tint err;\n\n\t \n\td40d = d40_first_queued(d40c);\n\n\tif (d40d != NULL) {\n\t\tif (!d40c->busy) {\n\t\t\td40c->busy = true;\n\t\t\tpm_runtime_get_sync(d40c->base->dev);\n\t\t}\n\n\t\t \n\t\td40_desc_remove(d40d);\n\n\t\t \n\t\td40_desc_submit(d40c, d40d);\n\n\t\t \n\t\td40_desc_load(d40c, d40d);\n\n\t\t \n\t\terr = d40_start(d40c);\n\n\t\tif (err)\n\t\t\treturn NULL;\n\t}\n\n\treturn d40d;\n}\n\n \nstatic void dma_tc_handle(struct d40_chan *d40c)\n{\n\tstruct d40_desc *d40d;\n\n\t \n\td40d = d40_first_active_get(d40c);\n\n\tif (d40d == NULL)\n\t\treturn;\n\n\tif (d40d->cyclic) {\n\t\t \n\t\tif (d40d->lli_current < d40d->lli_len\n\t\t    && !d40_tx_is_linked(d40c)\n\t\t    && !d40_residue(d40c)) {\n\t\t\td40_lcla_free_all(d40c, d40d);\n\t\t\td40_desc_load(d40c, d40d);\n\t\t\t(void) d40_start(d40c);\n\n\t\t\tif (d40d->lli_current == d40d->lli_len)\n\t\t\t\td40d->lli_current = 0;\n\t\t}\n\t} else {\n\t\td40_lcla_free_all(d40c, d40d);\n\n\t\tif (d40d->lli_current < d40d->lli_len) {\n\t\t\td40_desc_load(d40c, d40d);\n\t\t\t \n\t\t\t(void) d40_start(d40c);\n\t\t\treturn;\n\t\t}\n\n\t\tif (d40_queue_start(d40c) == NULL) {\n\t\t\td40c->busy = false;\n\n\t\t\tpm_runtime_mark_last_busy(d40c->base->dev);\n\t\t\tpm_runtime_put_autosuspend(d40c->base->dev);\n\t\t}\n\n\t\td40_desc_remove(d40d);\n\t\td40_desc_done(d40c, d40d);\n\t}\n\n\td40c->pending_tx++;\n\ttasklet_schedule(&d40c->tasklet);\n\n}\n\nstatic void dma_tasklet(struct tasklet_struct *t)\n{\n\tstruct d40_chan *d40c = from_tasklet(d40c, t, tasklet);\n\tstruct d40_desc *d40d;\n\tunsigned long flags;\n\tbool callback_active;\n\tstruct dmaengine_desc_callback cb;\n\n\tspin_lock_irqsave(&d40c->lock, flags);\n\n\t \n\td40d = d40_first_done(d40c);\n\tif (d40d == NULL) {\n\t\t \n\t\td40d = d40_first_active_get(d40c);\n\t\tif (d40d == NULL || !d40d->cyclic)\n\t\t\tgoto check_pending_tx;\n\t}\n\n\tif (!d40d->cyclic)\n\t\tdma_cookie_complete(&d40d->txd);\n\n\t \n\tif (d40c->pending_tx == 0) {\n\t\tspin_unlock_irqrestore(&d40c->lock, flags);\n\t\treturn;\n\t}\n\n\t \n\tcallback_active = !!(d40d->txd.flags & DMA_PREP_INTERRUPT);\n\tdmaengine_desc_get_callback(&d40d->txd, &cb);\n\n\tif (!d40d->cyclic) {\n\t\tif (async_tx_test_ack(&d40d->txd)) {\n\t\t\td40_desc_remove(d40d);\n\t\t\td40_desc_free(d40c, d40d);\n\t\t} else if (!d40d->is_in_client_list) {\n\t\t\td40_desc_remove(d40d);\n\t\t\td40_lcla_free_all(d40c, d40d);\n\t\t\tlist_add_tail(&d40d->node, &d40c->client);\n\t\t\td40d->is_in_client_list = true;\n\t\t}\n\t}\n\n\td40c->pending_tx--;\n\n\tif (d40c->pending_tx)\n\t\ttasklet_schedule(&d40c->tasklet);\n\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n\n\tif (callback_active)\n\t\tdmaengine_desc_callback_invoke(&cb, NULL);\n\n\treturn;\n check_pending_tx:\n\t \n\tif (d40c->pending_tx > 0)\n\t\td40c->pending_tx--;\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n}\n\nstatic irqreturn_t d40_handle_interrupt(int irq, void *data)\n{\n\tint i;\n\tu32 idx;\n\tu32 row;\n\tlong chan = -1;\n\tstruct d40_chan *d40c;\n\tstruct d40_base *base = data;\n\tu32 *regs = base->regs_interrupt;\n\tstruct d40_interrupt_lookup *il = base->gen_dmac.il;\n\tu32 il_size = base->gen_dmac.il_size;\n\n\tspin_lock(&base->interrupt_lock);\n\n\t \n\tfor (i = 0; i < il_size; i++)\n\t\tregs[i] = readl(base->virtbase + il[i].src);\n\n\tfor (;;) {\n\n\t\tchan = find_next_bit((unsigned long *)regs,\n\t\t\t\t     BITS_PER_LONG * il_size, chan + 1);\n\n\t\t \n\t\tif (chan == BITS_PER_LONG * il_size)\n\t\t\tbreak;\n\n\t\trow = chan / BITS_PER_LONG;\n\t\tidx = chan & (BITS_PER_LONG - 1);\n\n\t\tif (il[row].offset == D40_PHY_CHAN)\n\t\t\td40c = base->lookup_phy_chans[idx];\n\t\telse\n\t\t\td40c = base->lookup_log_chans[il[row].offset + idx];\n\n\t\tif (!d40c) {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\twritel(BIT(idx), base->virtbase + il[row].clr);\n\n\t\tspin_lock(&d40c->lock);\n\n\t\tif (!il[row].is_error)\n\t\t\tdma_tc_handle(d40c);\n\t\telse\n\t\t\td40_err(base->dev, \"IRQ chan: %ld offset %d idx %d\\n\",\n\t\t\t\tchan, il[row].offset, idx);\n\n\t\tspin_unlock(&d40c->lock);\n\t}\n\n\tspin_unlock(&base->interrupt_lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int d40_validate_conf(struct d40_chan *d40c,\n\t\t\t     struct stedma40_chan_cfg *conf)\n{\n\tint res = 0;\n\tbool is_log = conf->mode == STEDMA40_MODE_LOGICAL;\n\n\tif (!conf->dir) {\n\t\tchan_err(d40c, \"Invalid direction.\\n\");\n\t\tres = -EINVAL;\n\t}\n\n\tif ((is_log && conf->dev_type > d40c->base->num_log_chans)  ||\n\t    (!is_log && conf->dev_type > d40c->base->num_phy_chans) ||\n\t    (conf->dev_type < 0)) {\n\t\tchan_err(d40c, \"Invalid device type (%d)\\n\", conf->dev_type);\n\t\tres = -EINVAL;\n\t}\n\n\tif (conf->dir == DMA_DEV_TO_DEV) {\n\t\t \n\t\tchan_err(d40c, \"periph to periph not supported\\n\");\n\t\tres = -EINVAL;\n\t}\n\n\tif (d40_psize_2_burst_size(is_log, conf->src_info.psize) *\n\t    conf->src_info.data_width !=\n\t    d40_psize_2_burst_size(is_log, conf->dst_info.psize) *\n\t    conf->dst_info.data_width) {\n\t\t \n\n\t\tchan_err(d40c, \"src (burst x width) != dst (burst x width)\\n\");\n\t\tres = -EINVAL;\n\t}\n\n\treturn res;\n}\n\nstatic bool d40_alloc_mask_set(struct d40_phy_res *phy,\n\t\t\t       bool is_src, int log_event_line, bool is_log,\n\t\t\t       bool *first_user)\n{\n\tunsigned long flags;\n\tspin_lock_irqsave(&phy->lock, flags);\n\n\t*first_user = ((phy->allocated_src | phy->allocated_dst)\n\t\t\t== D40_ALLOC_FREE);\n\n\tif (!is_log) {\n\t\t \n\t\tif (phy->allocated_src == D40_ALLOC_FREE &&\n\t\t    phy->allocated_dst == D40_ALLOC_FREE) {\n\t\t\tphy->allocated_dst = D40_ALLOC_PHY;\n\t\t\tphy->allocated_src = D40_ALLOC_PHY;\n\t\t\tgoto found_unlock;\n\t\t} else\n\t\t\tgoto not_found_unlock;\n\t}\n\n\t \n\tif (is_src) {\n\t\tif (phy->allocated_src == D40_ALLOC_PHY)\n\t\t\tgoto not_found_unlock;\n\n\t\tif (phy->allocated_src == D40_ALLOC_FREE)\n\t\t\tphy->allocated_src = D40_ALLOC_LOG_FREE;\n\n\t\tif (!(phy->allocated_src & BIT(log_event_line))) {\n\t\t\tphy->allocated_src |= BIT(log_event_line);\n\t\t\tgoto found_unlock;\n\t\t} else\n\t\t\tgoto not_found_unlock;\n\t} else {\n\t\tif (phy->allocated_dst == D40_ALLOC_PHY)\n\t\t\tgoto not_found_unlock;\n\n\t\tif (phy->allocated_dst == D40_ALLOC_FREE)\n\t\t\tphy->allocated_dst = D40_ALLOC_LOG_FREE;\n\n\t\tif (!(phy->allocated_dst & BIT(log_event_line))) {\n\t\t\tphy->allocated_dst |= BIT(log_event_line);\n\t\t\tgoto found_unlock;\n\t\t}\n\t}\n not_found_unlock:\n\tspin_unlock_irqrestore(&phy->lock, flags);\n\treturn false;\n found_unlock:\n\tspin_unlock_irqrestore(&phy->lock, flags);\n\treturn true;\n}\n\nstatic bool d40_alloc_mask_free(struct d40_phy_res *phy, bool is_src,\n\t\t\t       int log_event_line)\n{\n\tunsigned long flags;\n\tbool is_free = false;\n\n\tspin_lock_irqsave(&phy->lock, flags);\n\tif (!log_event_line) {\n\t\tphy->allocated_dst = D40_ALLOC_FREE;\n\t\tphy->allocated_src = D40_ALLOC_FREE;\n\t\tis_free = true;\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (is_src) {\n\t\tphy->allocated_src &= ~BIT(log_event_line);\n\t\tif (phy->allocated_src == D40_ALLOC_LOG_FREE)\n\t\t\tphy->allocated_src = D40_ALLOC_FREE;\n\t} else {\n\t\tphy->allocated_dst &= ~BIT(log_event_line);\n\t\tif (phy->allocated_dst == D40_ALLOC_LOG_FREE)\n\t\t\tphy->allocated_dst = D40_ALLOC_FREE;\n\t}\n\n\tis_free = ((phy->allocated_src | phy->allocated_dst) ==\n\t\t   D40_ALLOC_FREE);\n unlock:\n\tspin_unlock_irqrestore(&phy->lock, flags);\n\n\treturn is_free;\n}\n\nstatic int d40_allocate_channel(struct d40_chan *d40c, bool *first_phy_user)\n{\n\tint dev_type = d40c->dma_cfg.dev_type;\n\tint event_group;\n\tint event_line;\n\tstruct d40_phy_res *phys;\n\tint i;\n\tint j;\n\tint log_num;\n\tint num_phy_chans;\n\tbool is_src;\n\tbool is_log = d40c->dma_cfg.mode == STEDMA40_MODE_LOGICAL;\n\n\tphys = d40c->base->phy_res;\n\tnum_phy_chans = d40c->base->num_phy_chans;\n\n\tif (d40c->dma_cfg.dir == DMA_DEV_TO_MEM) {\n\t\tlog_num = 2 * dev_type;\n\t\tis_src = true;\n\t} else if (d40c->dma_cfg.dir == DMA_MEM_TO_DEV ||\n\t\t   d40c->dma_cfg.dir == DMA_MEM_TO_MEM) {\n\t\t \n\t\tlog_num = 2 * dev_type + 1;\n\t\tis_src = false;\n\t} else\n\t\treturn -EINVAL;\n\n\tevent_group = D40_TYPE_TO_GROUP(dev_type);\n\tevent_line = D40_TYPE_TO_EVENT(dev_type);\n\n\tif (!is_log) {\n\t\tif (d40c->dma_cfg.dir == DMA_MEM_TO_MEM) {\n\t\t\t \n\t\t\tif (d40c->dma_cfg.use_fixed_channel) {\n\t\t\t\ti = d40c->dma_cfg.phy_channel;\n\t\t\t\tif (d40_alloc_mask_set(&phys[i], is_src,\n\t\t\t\t\t\t       0, is_log,\n\t\t\t\t\t\t       first_phy_user))\n\t\t\t\t\tgoto found_phy;\n\t\t\t} else {\n\t\t\t\tfor (i = 0; i < num_phy_chans; i++) {\n\t\t\t\t\tif (d40_alloc_mask_set(&phys[i], is_src,\n\t\t\t\t\t\t       0, is_log,\n\t\t\t\t\t\t       first_phy_user))\n\t\t\t\t\t\tgoto found_phy;\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tfor (j = 0; j < d40c->base->num_phy_chans; j += 8) {\n\t\t\t\tint phy_num = j  + event_group * 2;\n\t\t\t\tfor (i = phy_num; i < phy_num + 2; i++) {\n\t\t\t\t\tif (d40_alloc_mask_set(&phys[i],\n\t\t\t\t\t\t\t       is_src,\n\t\t\t\t\t\t\t       0,\n\t\t\t\t\t\t\t       is_log,\n\t\t\t\t\t\t\t       first_phy_user))\n\t\t\t\t\t\tgoto found_phy;\n\t\t\t\t}\n\t\t\t}\n\t\treturn -EINVAL;\nfound_phy:\n\t\td40c->phy_chan = &phys[i];\n\t\td40c->log_num = D40_PHY_CHAN;\n\t\tgoto out;\n\t}\n\tif (dev_type == -1)\n\t\treturn -EINVAL;\n\n\t \n\tfor (j = 0; j < d40c->base->num_phy_chans; j += 8) {\n\t\tint phy_num = j + event_group * 2;\n\n\t\tif (d40c->dma_cfg.use_fixed_channel) {\n\t\t\ti = d40c->dma_cfg.phy_channel;\n\n\t\t\tif ((i != phy_num) && (i != phy_num + 1)) {\n\t\t\t\tdev_err(chan2dev(d40c),\n\t\t\t\t\t\"invalid fixed phy channel %d\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (d40_alloc_mask_set(&phys[i], is_src, event_line,\n\t\t\t\t\t       is_log, first_phy_user))\n\t\t\t\tgoto found_log;\n\n\t\t\tdev_err(chan2dev(d40c),\n\t\t\t\t\"could not allocate fixed phy channel %d\\n\", i);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tif (is_src) {\n\t\t\tfor (i = phy_num; i < phy_num + 2; i++) {\n\t\t\t\tif (d40_alloc_mask_set(&phys[i], is_src,\n\t\t\t\t\t\t       event_line, is_log,\n\t\t\t\t\t\t       first_phy_user))\n\t\t\t\t\tgoto found_log;\n\t\t\t}\n\t\t} else {\n\t\t\tfor (i = phy_num + 1; i >= phy_num; i--) {\n\t\t\t\tif (d40_alloc_mask_set(&phys[i], is_src,\n\t\t\t\t\t\t       event_line, is_log,\n\t\t\t\t\t\t       first_phy_user))\n\t\t\t\t\tgoto found_log;\n\t\t\t}\n\t\t}\n\t}\n\treturn -EINVAL;\n\nfound_log:\n\td40c->phy_chan = &phys[i];\n\td40c->log_num = log_num;\nout:\n\n\tif (is_log)\n\t\td40c->base->lookup_log_chans[d40c->log_num] = d40c;\n\telse\n\t\td40c->base->lookup_phy_chans[d40c->phy_chan->num] = d40c;\n\n\treturn 0;\n\n}\n\nstatic int d40_config_memcpy(struct d40_chan *d40c)\n{\n\tdma_cap_mask_t cap = d40c->chan.device->cap_mask;\n\n\tif (dma_has_cap(DMA_MEMCPY, cap) && !dma_has_cap(DMA_SLAVE, cap)) {\n\t\td40c->dma_cfg = dma40_memcpy_conf_log;\n\t\td40c->dma_cfg.dev_type = dma40_memcpy_channels[d40c->chan.chan_id];\n\n\t\td40_log_cfg(&d40c->dma_cfg,\n\t\t\t    &d40c->log_def.lcsp1, &d40c->log_def.lcsp3);\n\n\t} else if (dma_has_cap(DMA_MEMCPY, cap) &&\n\t\t   dma_has_cap(DMA_SLAVE, cap)) {\n\t\td40c->dma_cfg = dma40_memcpy_conf_phy;\n\n\t\t \n\t\td40c->dst_def_cfg |= BIT(D40_SREG_CFG_TIM_POS);\n\n\t\t \n\t\td40c->src_def_cfg |= BIT(D40_SREG_CFG_EIM_POS);\n\t\td40c->dst_def_cfg |= BIT(D40_SREG_CFG_EIM_POS);\n\n\t} else {\n\t\tchan_err(d40c, \"No memcpy\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int d40_free_dma(struct d40_chan *d40c)\n{\n\n\tint res = 0;\n\tu32 event = D40_TYPE_TO_EVENT(d40c->dma_cfg.dev_type);\n\tstruct d40_phy_res *phy = d40c->phy_chan;\n\tbool is_src;\n\n\t \n\td40_term_all(d40c);\n\n\tif (phy == NULL) {\n\t\tchan_err(d40c, \"phy == null\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (phy->allocated_src == D40_ALLOC_FREE &&\n\t    phy->allocated_dst == D40_ALLOC_FREE) {\n\t\tchan_err(d40c, \"channel already free\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (d40c->dma_cfg.dir == DMA_MEM_TO_DEV ||\n\t    d40c->dma_cfg.dir == DMA_MEM_TO_MEM)\n\t\tis_src = false;\n\telse if (d40c->dma_cfg.dir == DMA_DEV_TO_MEM)\n\t\tis_src = true;\n\telse {\n\t\tchan_err(d40c, \"Unknown direction\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tpm_runtime_get_sync(d40c->base->dev);\n\tres = d40_channel_execute_command(d40c, D40_DMA_STOP);\n\tif (res) {\n\t\tchan_err(d40c, \"stop failed\\n\");\n\t\tgoto mark_last_busy;\n\t}\n\n\td40_alloc_mask_free(phy, is_src, chan_is_logical(d40c) ? event : 0);\n\n\tif (chan_is_logical(d40c))\n\t\td40c->base->lookup_log_chans[d40c->log_num] = NULL;\n\telse\n\t\td40c->base->lookup_phy_chans[phy->num] = NULL;\n\n\tif (d40c->busy) {\n\t\tpm_runtime_mark_last_busy(d40c->base->dev);\n\t\tpm_runtime_put_autosuspend(d40c->base->dev);\n\t}\n\n\td40c->busy = false;\n\td40c->phy_chan = NULL;\n\td40c->configured = false;\n mark_last_busy:\n\tpm_runtime_mark_last_busy(d40c->base->dev);\n\tpm_runtime_put_autosuspend(d40c->base->dev);\n\treturn res;\n}\n\nstatic bool d40_is_paused(struct d40_chan *d40c)\n{\n\tvoid __iomem *chanbase = chan_base(d40c);\n\tbool is_paused = false;\n\tunsigned long flags;\n\tvoid __iomem *active_reg;\n\tu32 status;\n\tu32 event = D40_TYPE_TO_EVENT(d40c->dma_cfg.dev_type);\n\n\tspin_lock_irqsave(&d40c->lock, flags);\n\n\tif (chan_is_physical(d40c)) {\n\t\tif (d40c->phy_chan->num % 2 == 0)\n\t\t\tactive_reg = d40c->base->virtbase + D40_DREG_ACTIVE;\n\t\telse\n\t\t\tactive_reg = d40c->base->virtbase + D40_DREG_ACTIVO;\n\n\t\tstatus = (readl(active_reg) &\n\t\t\t  D40_CHAN_POS_MASK(d40c->phy_chan->num)) >>\n\t\t\tD40_CHAN_POS(d40c->phy_chan->num);\n\t\tif (status == D40_DMA_SUSPENDED || status == D40_DMA_STOP)\n\t\t\tis_paused = true;\n\t\tgoto unlock;\n\t}\n\n\tif (d40c->dma_cfg.dir == DMA_MEM_TO_DEV ||\n\t    d40c->dma_cfg.dir == DMA_MEM_TO_MEM) {\n\t\tstatus = readl(chanbase + D40_CHAN_REG_SDLNK);\n\t} else if (d40c->dma_cfg.dir == DMA_DEV_TO_MEM) {\n\t\tstatus = readl(chanbase + D40_CHAN_REG_SSLNK);\n\t} else {\n\t\tchan_err(d40c, \"Unknown direction\\n\");\n\t\tgoto unlock;\n\t}\n\n\tstatus = (status & D40_EVENTLINE_MASK(event)) >>\n\t\tD40_EVENTLINE_POS(event);\n\n\tif (status != D40_DMA_RUN)\n\t\tis_paused = true;\n unlock:\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n\treturn is_paused;\n\n}\n\nstatic u32 stedma40_residue(struct dma_chan *chan)\n{\n\tstruct d40_chan *d40c =\n\t\tcontainer_of(chan, struct d40_chan, chan);\n\tu32 bytes_left;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&d40c->lock, flags);\n\tbytes_left = d40_residue(d40c);\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n\n\treturn bytes_left;\n}\n\nstatic int\nd40_prep_sg_log(struct d40_chan *chan, struct d40_desc *desc,\n\t\tstruct scatterlist *sg_src, struct scatterlist *sg_dst,\n\t\tunsigned int sg_len, dma_addr_t src_dev_addr,\n\t\tdma_addr_t dst_dev_addr)\n{\n\tstruct stedma40_chan_cfg *cfg = &chan->dma_cfg;\n\tstruct stedma40_half_channel_info *src_info = &cfg->src_info;\n\tstruct stedma40_half_channel_info *dst_info = &cfg->dst_info;\n\tint ret;\n\n\tret = d40_log_sg_to_lli(sg_src, sg_len,\n\t\t\t\tsrc_dev_addr,\n\t\t\t\tdesc->lli_log.src,\n\t\t\t\tchan->log_def.lcsp1,\n\t\t\t\tsrc_info->data_width,\n\t\t\t\tdst_info->data_width);\n\n\tret = d40_log_sg_to_lli(sg_dst, sg_len,\n\t\t\t\tdst_dev_addr,\n\t\t\t\tdesc->lli_log.dst,\n\t\t\t\tchan->log_def.lcsp3,\n\t\t\t\tdst_info->data_width,\n\t\t\t\tsrc_info->data_width);\n\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic int\nd40_prep_sg_phy(struct d40_chan *chan, struct d40_desc *desc,\n\t\tstruct scatterlist *sg_src, struct scatterlist *sg_dst,\n\t\tunsigned int sg_len, dma_addr_t src_dev_addr,\n\t\tdma_addr_t dst_dev_addr)\n{\n\tstruct stedma40_chan_cfg *cfg = &chan->dma_cfg;\n\tstruct stedma40_half_channel_info *src_info = &cfg->src_info;\n\tstruct stedma40_half_channel_info *dst_info = &cfg->dst_info;\n\tunsigned long flags = 0;\n\tint ret;\n\n\tif (desc->cyclic)\n\t\tflags |= LLI_CYCLIC | LLI_TERM_INT;\n\n\tret = d40_phy_sg_to_lli(sg_src, sg_len, src_dev_addr,\n\t\t\t\tdesc->lli_phy.src,\n\t\t\t\tvirt_to_phys(desc->lli_phy.src),\n\t\t\t\tchan->src_def_cfg,\n\t\t\t\tsrc_info, dst_info, flags);\n\n\tret = d40_phy_sg_to_lli(sg_dst, sg_len, dst_dev_addr,\n\t\t\t\tdesc->lli_phy.dst,\n\t\t\t\tvirt_to_phys(desc->lli_phy.dst),\n\t\t\t\tchan->dst_def_cfg,\n\t\t\t\tdst_info, src_info, flags);\n\n\tdma_sync_single_for_device(chan->base->dev, desc->lli_pool.dma_addr,\n\t\t\t\t   desc->lli_pool.size, DMA_TO_DEVICE);\n\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic struct d40_desc *\nd40_prep_desc(struct d40_chan *chan, struct scatterlist *sg,\n\t      unsigned int sg_len, unsigned long dma_flags)\n{\n\tstruct stedma40_chan_cfg *cfg;\n\tstruct d40_desc *desc;\n\tint ret;\n\n\tdesc = d40_desc_get(chan);\n\tif (!desc)\n\t\treturn NULL;\n\n\tcfg = &chan->dma_cfg;\n\tdesc->lli_len = d40_sg_2_dmalen(sg, sg_len, cfg->src_info.data_width,\n\t\t\t\t\tcfg->dst_info.data_width);\n\tif (desc->lli_len < 0) {\n\t\tchan_err(chan, \"Unaligned size\\n\");\n\t\tgoto free_desc;\n\t}\n\n\tret = d40_pool_lli_alloc(chan, desc, desc->lli_len);\n\tif (ret < 0) {\n\t\tchan_err(chan, \"Could not allocate lli\\n\");\n\t\tgoto free_desc;\n\t}\n\n\tdesc->lli_current = 0;\n\tdesc->txd.flags = dma_flags;\n\tdesc->txd.tx_submit = d40_tx_submit;\n\n\tdma_async_tx_descriptor_init(&desc->txd, &chan->chan);\n\n\treturn desc;\n free_desc:\n\td40_desc_free(chan, desc);\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor *\nd40_prep_sg(struct dma_chan *dchan, struct scatterlist *sg_src,\n\t    struct scatterlist *sg_dst, unsigned int sg_len,\n\t    enum dma_transfer_direction direction, unsigned long dma_flags)\n{\n\tstruct d40_chan *chan = container_of(dchan, struct d40_chan, chan);\n\tdma_addr_t src_dev_addr;\n\tdma_addr_t dst_dev_addr;\n\tstruct d40_desc *desc;\n\tunsigned long flags;\n\tint ret;\n\n\tif (!chan->phy_chan) {\n\t\tchan_err(chan, \"Cannot prepare unallocated channel\\n\");\n\t\treturn NULL;\n\t}\n\n\td40_set_runtime_config_write(dchan, &chan->slave_config, direction);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tdesc = d40_prep_desc(chan, sg_src, sg_len, dma_flags);\n\tif (desc == NULL)\n\t\tgoto unlock;\n\n\tif (sg_next(&sg_src[sg_len - 1]) == sg_src)\n\t\tdesc->cyclic = true;\n\n\tsrc_dev_addr = 0;\n\tdst_dev_addr = 0;\n\tif (direction == DMA_DEV_TO_MEM)\n\t\tsrc_dev_addr = chan->runtime_addr;\n\telse if (direction == DMA_MEM_TO_DEV)\n\t\tdst_dev_addr = chan->runtime_addr;\n\n\tif (chan_is_logical(chan))\n\t\tret = d40_prep_sg_log(chan, desc, sg_src, sg_dst,\n\t\t\t\t      sg_len, src_dev_addr, dst_dev_addr);\n\telse\n\t\tret = d40_prep_sg_phy(chan, desc, sg_src, sg_dst,\n\t\t\t\t      sg_len, src_dev_addr, dst_dev_addr);\n\n\tif (ret) {\n\t\tchan_err(chan, \"Failed to prepare %s sg job: %d\\n\",\n\t\t\t chan_is_logical(chan) ? \"log\" : \"phy\", ret);\n\t\tgoto free_desc;\n\t}\n\n\t \n\tlist_add_tail(&desc->node, &chan->prepare_queue);\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\treturn &desc->txd;\n free_desc:\n\td40_desc_free(chan, desc);\n unlock:\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\treturn NULL;\n}\n\nstatic bool stedma40_filter(struct dma_chan *chan, void *data)\n{\n\tstruct stedma40_chan_cfg *info = data;\n\tstruct d40_chan *d40c =\n\t\tcontainer_of(chan, struct d40_chan, chan);\n\tint err;\n\n\tif (data) {\n\t\terr = d40_validate_conf(d40c, info);\n\t\tif (!err)\n\t\t\td40c->dma_cfg = *info;\n\t} else\n\t\terr = d40_config_memcpy(d40c);\n\n\tif (!err)\n\t\td40c->configured = true;\n\n\treturn err == 0;\n}\n\nstatic void __d40_set_prio_rt(struct d40_chan *d40c, int dev_type, bool src)\n{\n\tbool realtime = d40c->dma_cfg.realtime;\n\tbool highprio = d40c->dma_cfg.high_priority;\n\tu32 rtreg;\n\tu32 event = D40_TYPE_TO_EVENT(dev_type);\n\tu32 group = D40_TYPE_TO_GROUP(dev_type);\n\tu32 bit = BIT(event);\n\tu32 prioreg;\n\tstruct d40_gen_dmac *dmac = &d40c->base->gen_dmac;\n\n\trtreg = realtime ? dmac->realtime_en : dmac->realtime_clear;\n\t \n\tif (!src && chan_is_logical(d40c))\n\t\thighprio = false;\n\n\tprioreg = highprio ? dmac->high_prio_en : dmac->high_prio_clear;\n\n\t \n\tif (!src)\n\t\tbit <<= 16;\n\n\twritel(bit, d40c->base->virtbase + prioreg + group * 4);\n\twritel(bit, d40c->base->virtbase + rtreg + group * 4);\n}\n\nstatic void d40_set_prio_realtime(struct d40_chan *d40c)\n{\n\tif (d40c->base->rev < 3)\n\t\treturn;\n\n\tif ((d40c->dma_cfg.dir ==  DMA_DEV_TO_MEM) ||\n\t    (d40c->dma_cfg.dir == DMA_DEV_TO_DEV))\n\t\t__d40_set_prio_rt(d40c, d40c->dma_cfg.dev_type, true);\n\n\tif ((d40c->dma_cfg.dir ==  DMA_MEM_TO_DEV) ||\n\t    (d40c->dma_cfg.dir == DMA_DEV_TO_DEV))\n\t\t__d40_set_prio_rt(d40c, d40c->dma_cfg.dev_type, false);\n}\n\n#define D40_DT_FLAGS_MODE(flags)       ((flags >> 0) & 0x1)\n#define D40_DT_FLAGS_DIR(flags)        ((flags >> 1) & 0x1)\n#define D40_DT_FLAGS_BIG_ENDIAN(flags) ((flags >> 2) & 0x1)\n#define D40_DT_FLAGS_FIXED_CHAN(flags) ((flags >> 3) & 0x1)\n#define D40_DT_FLAGS_HIGH_PRIO(flags)  ((flags >> 4) & 0x1)\n\nstatic struct dma_chan *d40_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t  struct of_dma *ofdma)\n{\n\tstruct stedma40_chan_cfg cfg;\n\tdma_cap_mask_t cap;\n\tu32 flags;\n\n\tmemset(&cfg, 0, sizeof(struct stedma40_chan_cfg));\n\n\tdma_cap_zero(cap);\n\tdma_cap_set(DMA_SLAVE, cap);\n\n\tcfg.dev_type = dma_spec->args[0];\n\tflags = dma_spec->args[2];\n\n\tswitch (D40_DT_FLAGS_MODE(flags)) {\n\tcase 0: cfg.mode = STEDMA40_MODE_LOGICAL; break;\n\tcase 1: cfg.mode = STEDMA40_MODE_PHYSICAL; break;\n\t}\n\n\tswitch (D40_DT_FLAGS_DIR(flags)) {\n\tcase 0:\n\t\tcfg.dir = DMA_MEM_TO_DEV;\n\t\tcfg.dst_info.big_endian = D40_DT_FLAGS_BIG_ENDIAN(flags);\n\t\tbreak;\n\tcase 1:\n\t\tcfg.dir = DMA_DEV_TO_MEM;\n\t\tcfg.src_info.big_endian = D40_DT_FLAGS_BIG_ENDIAN(flags);\n\t\tbreak;\n\t}\n\n\tif (D40_DT_FLAGS_FIXED_CHAN(flags)) {\n\t\tcfg.phy_channel = dma_spec->args[1];\n\t\tcfg.use_fixed_channel = true;\n\t}\n\n\tif (D40_DT_FLAGS_HIGH_PRIO(flags))\n\t\tcfg.high_priority = true;\n\n\treturn dma_request_channel(cap, stedma40_filter, &cfg);\n}\n\n \nstatic int d40_alloc_chan_resources(struct dma_chan *chan)\n{\n\tint err;\n\tunsigned long flags;\n\tstruct d40_chan *d40c =\n\t\tcontainer_of(chan, struct d40_chan, chan);\n\tbool is_free_phy;\n\tspin_lock_irqsave(&d40c->lock, flags);\n\n\tdma_cookie_init(chan);\n\n\t \n\tif (!d40c->configured) {\n\t\terr = d40_config_memcpy(d40c);\n\t\tif (err) {\n\t\t\tchan_err(d40c, \"Failed to configure memcpy channel\\n\");\n\t\t\tgoto mark_last_busy;\n\t\t}\n\t}\n\n\terr = d40_allocate_channel(d40c, &is_free_phy);\n\tif (err) {\n\t\tchan_err(d40c, \"Failed to allocate channel\\n\");\n\t\td40c->configured = false;\n\t\tgoto mark_last_busy;\n\t}\n\n\tpm_runtime_get_sync(d40c->base->dev);\n\n\td40_set_prio_realtime(d40c);\n\n\tif (chan_is_logical(d40c)) {\n\t\tif (d40c->dma_cfg.dir == DMA_DEV_TO_MEM)\n\t\t\td40c->lcpa = d40c->base->lcpa_base +\n\t\t\t\td40c->dma_cfg.dev_type * D40_LCPA_CHAN_SIZE;\n\t\telse\n\t\t\td40c->lcpa = d40c->base->lcpa_base +\n\t\t\t\td40c->dma_cfg.dev_type *\n\t\t\t\tD40_LCPA_CHAN_SIZE + D40_LCPA_CHAN_DST_DELTA;\n\n\t\t \n\t\td40c->src_def_cfg |= BIT(D40_SREG_CFG_LOG_GIM_POS);\n\t\td40c->dst_def_cfg |= BIT(D40_SREG_CFG_LOG_GIM_POS);\n\t}\n\n\tdev_dbg(chan2dev(d40c), \"allocated %s channel (phy %d%s)\\n\",\n\t\t chan_is_logical(d40c) ? \"logical\" : \"physical\",\n\t\t d40c->phy_chan->num,\n\t\t d40c->dma_cfg.use_fixed_channel ? \", fixed\" : \"\");\n\n\n\t \n\tif (is_free_phy)\n\t\td40_config_write(d40c);\n mark_last_busy:\n\tpm_runtime_mark_last_busy(d40c->base->dev);\n\tpm_runtime_put_autosuspend(d40c->base->dev);\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n\treturn err;\n}\n\nstatic void d40_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct d40_chan *d40c =\n\t\tcontainer_of(chan, struct d40_chan, chan);\n\tint err;\n\tunsigned long flags;\n\n\tif (d40c->phy_chan == NULL) {\n\t\tchan_err(d40c, \"Cannot free unallocated channel\\n\");\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&d40c->lock, flags);\n\n\terr = d40_free_dma(d40c);\n\n\tif (err)\n\t\tchan_err(d40c, \"Failed to free channel\\n\");\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n}\n\nstatic struct dma_async_tx_descriptor *d40_prep_memcpy(struct dma_chan *chan,\n\t\t\t\t\t\t       dma_addr_t dst,\n\t\t\t\t\t\t       dma_addr_t src,\n\t\t\t\t\t\t       size_t size,\n\t\t\t\t\t\t       unsigned long dma_flags)\n{\n\tstruct scatterlist dst_sg;\n\tstruct scatterlist src_sg;\n\n\tsg_init_table(&dst_sg, 1);\n\tsg_init_table(&src_sg, 1);\n\n\tsg_dma_address(&dst_sg) = dst;\n\tsg_dma_address(&src_sg) = src;\n\n\tsg_dma_len(&dst_sg) = size;\n\tsg_dma_len(&src_sg) = size;\n\n\treturn d40_prep_sg(chan, &src_sg, &dst_sg, 1,\n\t\t\t   DMA_MEM_TO_MEM, dma_flags);\n}\n\nstatic struct dma_async_tx_descriptor *\nd40_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\n\t\t  unsigned int sg_len, enum dma_transfer_direction direction,\n\t\t  unsigned long dma_flags, void *context)\n{\n\tif (!is_slave_direction(direction))\n\t\treturn NULL;\n\n\treturn d40_prep_sg(chan, sgl, sgl, sg_len, direction, dma_flags);\n}\n\nstatic struct dma_async_tx_descriptor *\ndma40_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t dma_addr,\n\t\t     size_t buf_len, size_t period_len,\n\t\t     enum dma_transfer_direction direction, unsigned long flags)\n{\n\tunsigned int periods = buf_len / period_len;\n\tstruct dma_async_tx_descriptor *txd;\n\tstruct scatterlist *sg;\n\tint i;\n\n\tsg = kcalloc(periods + 1, sizeof(struct scatterlist), GFP_NOWAIT);\n\tif (!sg)\n\t\treturn NULL;\n\n\tfor (i = 0; i < periods; i++) {\n\t\tsg_dma_address(&sg[i]) = dma_addr;\n\t\tsg_dma_len(&sg[i]) = period_len;\n\t\tdma_addr += period_len;\n\t}\n\n\tsg_chain(sg, periods + 1, sg);\n\n\ttxd = d40_prep_sg(chan, sg, sg, periods, direction,\n\t\t\t  DMA_PREP_INTERRUPT);\n\n\tkfree(sg);\n\n\treturn txd;\n}\n\nstatic enum dma_status d40_tx_status(struct dma_chan *chan,\n\t\t\t\t     dma_cookie_t cookie,\n\t\t\t\t     struct dma_tx_state *txstate)\n{\n\tstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\n\tenum dma_status ret;\n\n\tif (d40c->phy_chan == NULL) {\n\t\tchan_err(d40c, \"Cannot read status of unallocated channel\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\tif (ret != DMA_COMPLETE && txstate)\n\t\tdma_set_residue(txstate, stedma40_residue(chan));\n\n\tif (d40_is_paused(d40c))\n\t\tret = DMA_PAUSED;\n\n\treturn ret;\n}\n\nstatic void d40_issue_pending(struct dma_chan *chan)\n{\n\tstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\n\tunsigned long flags;\n\n\tif (d40c->phy_chan == NULL) {\n\t\tchan_err(d40c, \"Channel is not allocated!\\n\");\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&d40c->lock, flags);\n\n\tlist_splice_tail_init(&d40c->pending_queue, &d40c->queue);\n\n\t \n\tif (!d40c->busy)\n\t\t(void) d40_queue_start(d40c);\n\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n}\n\nstatic int d40_terminate_all(struct dma_chan *chan)\n{\n\tunsigned long flags;\n\tstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\n\tint ret;\n\n\tif (d40c->phy_chan == NULL) {\n\t\tchan_err(d40c, \"Channel is not allocated!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_irqsave(&d40c->lock, flags);\n\n\tpm_runtime_get_sync(d40c->base->dev);\n\tret = d40_channel_execute_command(d40c, D40_DMA_STOP);\n\tif (ret)\n\t\tchan_err(d40c, \"Failed to stop channel\\n\");\n\n\td40_term_all(d40c);\n\tpm_runtime_mark_last_busy(d40c->base->dev);\n\tpm_runtime_put_autosuspend(d40c->base->dev);\n\tif (d40c->busy) {\n\t\tpm_runtime_mark_last_busy(d40c->base->dev);\n\t\tpm_runtime_put_autosuspend(d40c->base->dev);\n\t}\n\td40c->busy = false;\n\n\tspin_unlock_irqrestore(&d40c->lock, flags);\n\treturn 0;\n}\n\nstatic int\ndma40_config_to_halfchannel(struct d40_chan *d40c,\n\t\t\t    struct stedma40_half_channel_info *info,\n\t\t\t    u32 maxburst)\n{\n\tint psize;\n\n\tif (chan_is_logical(d40c)) {\n\t\tif (maxburst >= 16)\n\t\t\tpsize = STEDMA40_PSIZE_LOG_16;\n\t\telse if (maxburst >= 8)\n\t\t\tpsize = STEDMA40_PSIZE_LOG_8;\n\t\telse if (maxburst >= 4)\n\t\t\tpsize = STEDMA40_PSIZE_LOG_4;\n\t\telse\n\t\t\tpsize = STEDMA40_PSIZE_LOG_1;\n\t} else {\n\t\tif (maxburst >= 16)\n\t\t\tpsize = STEDMA40_PSIZE_PHY_16;\n\t\telse if (maxburst >= 8)\n\t\t\tpsize = STEDMA40_PSIZE_PHY_8;\n\t\telse if (maxburst >= 4)\n\t\t\tpsize = STEDMA40_PSIZE_PHY_4;\n\t\telse\n\t\t\tpsize = STEDMA40_PSIZE_PHY_1;\n\t}\n\n\tinfo->psize = psize;\n\tinfo->flow_ctrl = STEDMA40_NO_FLOW_CTRL;\n\n\treturn 0;\n}\n\nstatic int d40_set_runtime_config(struct dma_chan *chan,\n\t\t\t\t  struct dma_slave_config *config)\n{\n\tstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\n\n\tmemcpy(&d40c->slave_config, config, sizeof(*config));\n\n\treturn 0;\n}\n\n \nstatic int d40_set_runtime_config_write(struct dma_chan *chan,\n\t\t\t\t  struct dma_slave_config *config,\n\t\t\t\t  enum dma_transfer_direction direction)\n{\n\tstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\n\tstruct stedma40_chan_cfg *cfg = &d40c->dma_cfg;\n\tenum dma_slave_buswidth src_addr_width, dst_addr_width;\n\tdma_addr_t config_addr;\n\tu32 src_maxburst, dst_maxburst;\n\tint ret;\n\n\tif (d40c->phy_chan == NULL) {\n\t\tchan_err(d40c, \"Channel is not allocated!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tsrc_addr_width = config->src_addr_width;\n\tsrc_maxburst = config->src_maxburst;\n\tdst_addr_width = config->dst_addr_width;\n\tdst_maxburst = config->dst_maxburst;\n\n\tif (direction == DMA_DEV_TO_MEM) {\n\t\tconfig_addr = config->src_addr;\n\n\t\tif (cfg->dir != DMA_DEV_TO_MEM)\n\t\t\tdev_dbg(d40c->base->dev,\n\t\t\t\t\"channel was not configured for peripheral \"\n\t\t\t\t\"to memory transfer (%d) overriding\\n\",\n\t\t\t\tcfg->dir);\n\t\tcfg->dir = DMA_DEV_TO_MEM;\n\n\t\t \n\t\tif (dst_addr_width == DMA_SLAVE_BUSWIDTH_UNDEFINED)\n\t\t\tdst_addr_width = src_addr_width;\n\t\tif (dst_maxburst == 0)\n\t\t\tdst_maxburst = src_maxburst;\n\n\t} else if (direction == DMA_MEM_TO_DEV) {\n\t\tconfig_addr = config->dst_addr;\n\n\t\tif (cfg->dir != DMA_MEM_TO_DEV)\n\t\t\tdev_dbg(d40c->base->dev,\n\t\t\t\t\"channel was not configured for memory \"\n\t\t\t\t\"to peripheral transfer (%d) overriding\\n\",\n\t\t\t\tcfg->dir);\n\t\tcfg->dir = DMA_MEM_TO_DEV;\n\n\t\t \n\t\tif (src_addr_width == DMA_SLAVE_BUSWIDTH_UNDEFINED)\n\t\t\tsrc_addr_width = dst_addr_width;\n\t\tif (src_maxburst == 0)\n\t\t\tsrc_maxburst = dst_maxburst;\n\t} else {\n\t\tdev_err(d40c->base->dev,\n\t\t\t\"unrecognized channel direction %d\\n\",\n\t\t\tdirection);\n\t\treturn -EINVAL;\n\t}\n\n\tif (config_addr <= 0) {\n\t\tdev_err(d40c->base->dev, \"no address supplied\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (src_maxburst * src_addr_width != dst_maxburst * dst_addr_width) {\n\t\tdev_err(d40c->base->dev,\n\t\t\t\"src/dst width/maxburst mismatch: %d*%d != %d*%d\\n\",\n\t\t\tsrc_maxburst,\n\t\t\tsrc_addr_width,\n\t\t\tdst_maxburst,\n\t\t\tdst_addr_width);\n\t\treturn -EINVAL;\n\t}\n\n\tif (src_maxburst > 16) {\n\t\tsrc_maxburst = 16;\n\t\tdst_maxburst = src_maxburst * src_addr_width / dst_addr_width;\n\t} else if (dst_maxburst > 16) {\n\t\tdst_maxburst = 16;\n\t\tsrc_maxburst = dst_maxburst * dst_addr_width / src_addr_width;\n\t}\n\n\t \n\tif (src_addr_width <= DMA_SLAVE_BUSWIDTH_UNDEFINED ||\n\t    src_addr_width >  DMA_SLAVE_BUSWIDTH_8_BYTES   ||\n\t    dst_addr_width <= DMA_SLAVE_BUSWIDTH_UNDEFINED ||\n\t    dst_addr_width >  DMA_SLAVE_BUSWIDTH_8_BYTES   ||\n\t    !is_power_of_2(src_addr_width) ||\n\t    !is_power_of_2(dst_addr_width))\n\t\treturn -EINVAL;\n\n\tcfg->src_info.data_width = src_addr_width;\n\tcfg->dst_info.data_width = dst_addr_width;\n\n\tret = dma40_config_to_halfchannel(d40c, &cfg->src_info,\n\t\t\t\t\t  src_maxburst);\n\tif (ret)\n\t\treturn ret;\n\n\tret = dma40_config_to_halfchannel(d40c, &cfg->dst_info,\n\t\t\t\t\t  dst_maxburst);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (chan_is_logical(d40c))\n\t\td40_log_cfg(cfg, &d40c->log_def.lcsp1, &d40c->log_def.lcsp3);\n\telse\n\t\td40_phy_cfg(cfg, &d40c->src_def_cfg, &d40c->dst_def_cfg);\n\n\t \n\td40c->runtime_addr = config_addr;\n\td40c->runtime_direction = direction;\n\tdev_dbg(d40c->base->dev,\n\t\t\"configured channel %s for %s, data width %d/%d, \"\n\t\t\"maxburst %d/%d elements, LE, no flow control\\n\",\n\t\tdma_chan_name(chan),\n\t\t(direction == DMA_DEV_TO_MEM) ? \"RX\" : \"TX\",\n\t\tsrc_addr_width, dst_addr_width,\n\t\tsrc_maxburst, dst_maxburst);\n\n\treturn 0;\n}\n\n \n\nstatic void __init d40_chan_init(struct d40_base *base, struct dma_device *dma,\n\t\t\t\t struct d40_chan *chans, int offset,\n\t\t\t\t int num_chans)\n{\n\tint i = 0;\n\tstruct d40_chan *d40c;\n\n\tINIT_LIST_HEAD(&dma->channels);\n\n\tfor (i = offset; i < offset + num_chans; i++) {\n\t\td40c = &chans[i];\n\t\td40c->base = base;\n\t\td40c->chan.device = dma;\n\n\t\tspin_lock_init(&d40c->lock);\n\n\t\td40c->log_num = D40_PHY_CHAN;\n\n\t\tINIT_LIST_HEAD(&d40c->done);\n\t\tINIT_LIST_HEAD(&d40c->active);\n\t\tINIT_LIST_HEAD(&d40c->queue);\n\t\tINIT_LIST_HEAD(&d40c->pending_queue);\n\t\tINIT_LIST_HEAD(&d40c->client);\n\t\tINIT_LIST_HEAD(&d40c->prepare_queue);\n\n\t\ttasklet_setup(&d40c->tasklet, dma_tasklet);\n\n\t\tlist_add_tail(&d40c->chan.device_node,\n\t\t\t      &dma->channels);\n\t}\n}\n\nstatic void d40_ops_init(struct d40_base *base, struct dma_device *dev)\n{\n\tif (dma_has_cap(DMA_SLAVE, dev->cap_mask)) {\n\t\tdev->device_prep_slave_sg = d40_prep_slave_sg;\n\t\tdev->directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\n\t}\n\n\tif (dma_has_cap(DMA_MEMCPY, dev->cap_mask)) {\n\t\tdev->device_prep_dma_memcpy = d40_prep_memcpy;\n\t\tdev->directions = BIT(DMA_MEM_TO_MEM);\n\t\t \n\t\tdev->copy_align = DMAENGINE_ALIGN_4_BYTES;\n\t}\n\n\tif (dma_has_cap(DMA_CYCLIC, dev->cap_mask))\n\t\tdev->device_prep_dma_cyclic = dma40_prep_dma_cyclic;\n\n\tdev->device_alloc_chan_resources = d40_alloc_chan_resources;\n\tdev->device_free_chan_resources = d40_free_chan_resources;\n\tdev->device_issue_pending = d40_issue_pending;\n\tdev->device_tx_status = d40_tx_status;\n\tdev->device_config = d40_set_runtime_config;\n\tdev->device_pause = d40_pause;\n\tdev->device_resume = d40_resume;\n\tdev->device_terminate_all = d40_terminate_all;\n\tdev->residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\n\tdev->dev = base->dev;\n}\n\nstatic int __init d40_dmaengine_init(struct d40_base *base,\n\t\t\t\t     int num_reserved_chans)\n{\n\tint err ;\n\n\td40_chan_init(base, &base->dma_slave, base->log_chans,\n\t\t      0, base->num_log_chans);\n\n\tdma_cap_zero(base->dma_slave.cap_mask);\n\tdma_cap_set(DMA_SLAVE, base->dma_slave.cap_mask);\n\tdma_cap_set(DMA_CYCLIC, base->dma_slave.cap_mask);\n\n\td40_ops_init(base, &base->dma_slave);\n\n\terr = dmaenginem_async_device_register(&base->dma_slave);\n\n\tif (err) {\n\t\td40_err(base->dev, \"Failed to register slave channels\\n\");\n\t\tgoto exit;\n\t}\n\n\td40_chan_init(base, &base->dma_memcpy, base->log_chans,\n\t\t      base->num_log_chans, base->num_memcpy_chans);\n\n\tdma_cap_zero(base->dma_memcpy.cap_mask);\n\tdma_cap_set(DMA_MEMCPY, base->dma_memcpy.cap_mask);\n\n\td40_ops_init(base, &base->dma_memcpy);\n\n\terr = dmaenginem_async_device_register(&base->dma_memcpy);\n\n\tif (err) {\n\t\td40_err(base->dev,\n\t\t\t\"Failed to register memcpy only channels\\n\");\n\t\tgoto exit;\n\t}\n\n\td40_chan_init(base, &base->dma_both, base->phy_chans,\n\t\t      0, num_reserved_chans);\n\n\tdma_cap_zero(base->dma_both.cap_mask);\n\tdma_cap_set(DMA_SLAVE, base->dma_both.cap_mask);\n\tdma_cap_set(DMA_MEMCPY, base->dma_both.cap_mask);\n\tdma_cap_set(DMA_CYCLIC, base->dma_slave.cap_mask);\n\n\td40_ops_init(base, &base->dma_both);\n\terr = dmaenginem_async_device_register(&base->dma_both);\n\n\tif (err) {\n\t\td40_err(base->dev,\n\t\t\t\"Failed to register logical and physical capable channels\\n\");\n\t\tgoto exit;\n\t}\n\treturn 0;\n exit:\n\treturn err;\n}\n\n \n#ifdef CONFIG_PM_SLEEP\nstatic int dma40_suspend(struct device *dev)\n{\n\tstruct d40_base *base = dev_get_drvdata(dev);\n\tint ret;\n\n\tret = pm_runtime_force_suspend(dev);\n\tif (ret)\n\t\treturn ret;\n\n\tif (base->lcpa_regulator)\n\t\tret = regulator_disable(base->lcpa_regulator);\n\treturn ret;\n}\n\nstatic int dma40_resume(struct device *dev)\n{\n\tstruct d40_base *base = dev_get_drvdata(dev);\n\tint ret = 0;\n\n\tif (base->lcpa_regulator) {\n\t\tret = regulator_enable(base->lcpa_regulator);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn pm_runtime_force_resume(dev);\n}\n#endif\n\n#ifdef CONFIG_PM\nstatic void dma40_backup(void __iomem *baseaddr, u32 *backup,\n\t\t\t u32 *regaddr, int num, bool save)\n{\n\tint i;\n\n\tfor (i = 0; i < num; i++) {\n\t\tvoid __iomem *addr = baseaddr + regaddr[i];\n\n\t\tif (save)\n\t\t\tbackup[i] = readl_relaxed(addr);\n\t\telse\n\t\t\twritel_relaxed(backup[i], addr);\n\t}\n}\n\nstatic void d40_save_restore_registers(struct d40_base *base, bool save)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < base->num_phy_chans; i++) {\n\t\tvoid __iomem *addr;\n\t\tint idx;\n\n\t\tif (base->phy_res[i].reserved)\n\t\t\tcontinue;\n\n\t\taddr = base->virtbase + D40_DREG_PCBASE + i * D40_DREG_PCDELTA;\n\t\tidx = i * ARRAY_SIZE(d40_backup_regs_chan);\n\n\t\tdma40_backup(addr, &base->reg_val_backup_chan[idx],\n\t\t\t     d40_backup_regs_chan,\n\t\t\t     ARRAY_SIZE(d40_backup_regs_chan),\n\t\t\t     save);\n\t}\n\n\t \n\tdma40_backup(base->virtbase, base->reg_val_backup,\n\t\t     d40_backup_regs, ARRAY_SIZE(d40_backup_regs),\n\t\t     save);\n\n\t \n\tif (base->gen_dmac.backup)\n\t\tdma40_backup(base->virtbase, base->reg_val_backup_v4,\n\t\t\t     base->gen_dmac.backup,\n\t\t\tbase->gen_dmac.backup_size,\n\t\t\tsave);\n}\n\nstatic int dma40_runtime_suspend(struct device *dev)\n{\n\tstruct d40_base *base = dev_get_drvdata(dev);\n\n\td40_save_restore_registers(base, true);\n\n\t \n\tif (base->rev != 1)\n\t\twritel_relaxed(base->gcc_pwr_off_mask,\n\t\t\t       base->virtbase + D40_DREG_GCC);\n\n\treturn 0;\n}\n\nstatic int dma40_runtime_resume(struct device *dev)\n{\n\tstruct d40_base *base = dev_get_drvdata(dev);\n\n\td40_save_restore_registers(base, false);\n\n\twritel_relaxed(D40_DREG_GCC_ENABLE_ALL,\n\t\t       base->virtbase + D40_DREG_GCC);\n\treturn 0;\n}\n#endif\n\nstatic const struct dev_pm_ops dma40_pm_ops = {\n\tSET_LATE_SYSTEM_SLEEP_PM_OPS(dma40_suspend, dma40_resume)\n\tSET_RUNTIME_PM_OPS(dma40_runtime_suspend,\n\t\t\t\tdma40_runtime_resume,\n\t\t\t\tNULL)\n};\n\n \n\nstatic int __init d40_phy_res_init(struct d40_base *base)\n{\n\tint i;\n\tint num_phy_chans_avail = 0;\n\tu32 val[2];\n\tint odd_even_bit = -2;\n\tint gcc = D40_DREG_GCC_ENA;\n\n\tval[0] = readl(base->virtbase + D40_DREG_PRSME);\n\tval[1] = readl(base->virtbase + D40_DREG_PRSMO);\n\n\tfor (i = 0; i < base->num_phy_chans; i++) {\n\t\tbase->phy_res[i].num = i;\n\t\todd_even_bit += 2 * ((i % 2) == 0);\n\t\tif (((val[i % 2] >> odd_even_bit) & 3) == 1) {\n\t\t\t \n\t\t\tbase->phy_res[i].allocated_src = D40_ALLOC_PHY;\n\t\t\tbase->phy_res[i].allocated_dst = D40_ALLOC_PHY;\n\t\t\tbase->phy_res[i].reserved = true;\n\t\t\tgcc |= D40_DREG_GCC_EVTGRP_ENA(D40_PHYS_TO_GROUP(i),\n\t\t\t\t\t\t       D40_DREG_GCC_SRC);\n\t\t\tgcc |= D40_DREG_GCC_EVTGRP_ENA(D40_PHYS_TO_GROUP(i),\n\t\t\t\t\t\t       D40_DREG_GCC_DST);\n\n\n\t\t} else {\n\t\t\tbase->phy_res[i].allocated_src = D40_ALLOC_FREE;\n\t\t\tbase->phy_res[i].allocated_dst = D40_ALLOC_FREE;\n\t\t\tbase->phy_res[i].reserved = false;\n\t\t\tnum_phy_chans_avail++;\n\t\t}\n\t\tspin_lock_init(&base->phy_res[i].lock);\n\t}\n\n\t \n\tfor (i = 0; base->plat_data->disabled_channels[i] != -1; i++) {\n\t\tint chan = base->plat_data->disabled_channels[i];\n\n\t\tbase->phy_res[chan].allocated_src = D40_ALLOC_PHY;\n\t\tbase->phy_res[chan].allocated_dst = D40_ALLOC_PHY;\n\t\tbase->phy_res[chan].reserved = true;\n\t\tgcc |= D40_DREG_GCC_EVTGRP_ENA(D40_PHYS_TO_GROUP(chan),\n\t\t\t\t\t       D40_DREG_GCC_SRC);\n\t\tgcc |= D40_DREG_GCC_EVTGRP_ENA(D40_PHYS_TO_GROUP(chan),\n\t\t\t\t\t       D40_DREG_GCC_DST);\n\t\tnum_phy_chans_avail--;\n\t}\n\n\t \n\tfor (i = 0; i < base->plat_data->num_of_soft_lli_chans; i++) {\n\t\tint chan = base->plat_data->soft_lli_chans[i];\n\n\t\tbase->phy_res[chan].use_soft_lli = true;\n\t}\n\n\tdev_info(base->dev, \"%d of %d physical DMA channels available\\n\",\n\t\t num_phy_chans_avail, base->num_phy_chans);\n\n\t \n\tval[0] = readl(base->virtbase + D40_DREG_PRTYP);\n\n\tfor (i = 0; i < base->num_phy_chans; i++) {\n\n\t\tif (base->phy_res[i].allocated_src == D40_ALLOC_FREE &&\n\t\t    (val[0] & 0x3) != 1)\n\t\t\tdev_info(base->dev,\n\t\t\t\t \"[%s] INFO: channel %d is misconfigured (%d)\\n\",\n\t\t\t\t __func__, i, val[0] & 0x3);\n\n\t\tval[0] = val[0] >> 2;\n\t}\n\n\t \n\twritel(D40_DREG_GCC_ENABLE_ALL, base->virtbase + D40_DREG_GCC);\n\tbase->gcc_pwr_off_mask = gcc;\n\n\treturn num_phy_chans_avail;\n}\n\n \nstatic void d40_drop_kmem_cache_action(void *d)\n{\n\tstruct kmem_cache *desc_slab = d;\n\n\tkmem_cache_destroy(desc_slab);\n}\n\nstatic int __init d40_hw_detect_init(struct platform_device *pdev,\n\t\t\t\t     struct d40_base **retbase)\n{\n\tstruct stedma40_platform_data *plat_data = dev_get_platdata(&pdev->dev);\n\tstruct device *dev = &pdev->dev;\n\tstruct clk *clk;\n\tvoid __iomem *virtbase;\n\tstruct d40_base *base;\n\tint num_log_chans;\n\tint num_phy_chans;\n\tint num_memcpy_chans;\n\tint i;\n\tu32 pid;\n\tu32 cid;\n\tu8 rev;\n\tint ret;\n\n\tclk = devm_clk_get_enabled(dev, NULL);\n\tif (IS_ERR(clk))\n\t\treturn PTR_ERR(clk);\n\n\t \n\tvirtbase = devm_platform_ioremap_resource_byname(pdev, \"base\");\n\tif (IS_ERR(virtbase))\n\t\treturn PTR_ERR(virtbase);\n\n\t \n\tfor (pid = 0, i = 0; i < 4; i++)\n\t\tpid |= (readl(virtbase + SZ_4K - 0x20 + 4 * i)\n\t\t\t& 255) << (i * 8);\n\tfor (cid = 0, i = 0; i < 4; i++)\n\t\tcid |= (readl(virtbase + SZ_4K - 0x10 + 4 * i)\n\t\t\t& 255) << (i * 8);\n\n\tif (cid != AMBA_CID) {\n\t\td40_err(dev, \"Unknown hardware! No PrimeCell ID\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (AMBA_MANF_BITS(pid) != AMBA_VENDOR_ST) {\n\t\td40_err(dev, \"Unknown designer! Got %x wanted %x\\n\",\n\t\t\tAMBA_MANF_BITS(pid),\n\t\t\tAMBA_VENDOR_ST);\n\t\treturn -EINVAL;\n\t}\n\t \n\trev = AMBA_REV_BITS(pid);\n\tif (rev < 2) {\n\t\td40_err(dev, \"hardware revision: %d is not supported\", rev);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (plat_data->num_of_phy_chans)\n\t\tnum_phy_chans = plat_data->num_of_phy_chans;\n\telse\n\t\tnum_phy_chans = 4 * (readl(virtbase + D40_DREG_ICFG) & 0x7) + 4;\n\n\t \n\tif (plat_data->num_of_memcpy_chans)\n\t\tnum_memcpy_chans = plat_data->num_of_memcpy_chans;\n\telse\n\t\tnum_memcpy_chans = ARRAY_SIZE(dma40_memcpy_channels);\n\n\tnum_log_chans = num_phy_chans * D40_MAX_LOG_CHAN_PER_PHY;\n\n\tdev_info(dev,\n\t\t \"hardware rev: %d with %d physical and %d logical channels\\n\",\n\t\t rev, num_phy_chans, num_log_chans);\n\n\tbase = devm_kzalloc(dev,\n\t\tALIGN(sizeof(struct d40_base), 4) +\n\t\t(num_phy_chans + num_log_chans + num_memcpy_chans) *\n\t\tsizeof(struct d40_chan), GFP_KERNEL);\n\n\tif (!base)\n\t\treturn -ENOMEM;\n\n\tbase->rev = rev;\n\tbase->clk = clk;\n\tbase->num_memcpy_chans = num_memcpy_chans;\n\tbase->num_phy_chans = num_phy_chans;\n\tbase->num_log_chans = num_log_chans;\n\tbase->virtbase = virtbase;\n\tbase->plat_data = plat_data;\n\tbase->dev = dev;\n\tbase->phy_chans = ((void *)base) + ALIGN(sizeof(struct d40_base), 4);\n\tbase->log_chans = &base->phy_chans[num_phy_chans];\n\n\tif (base->plat_data->num_of_phy_chans == 14) {\n\t\tbase->gen_dmac.backup = d40_backup_regs_v4b;\n\t\tbase->gen_dmac.backup_size = BACKUP_REGS_SZ_V4B;\n\t\tbase->gen_dmac.interrupt_en = D40_DREG_CPCMIS;\n\t\tbase->gen_dmac.interrupt_clear = D40_DREG_CPCICR;\n\t\tbase->gen_dmac.realtime_en = D40_DREG_CRSEG1;\n\t\tbase->gen_dmac.realtime_clear = D40_DREG_CRCEG1;\n\t\tbase->gen_dmac.high_prio_en = D40_DREG_CPSEG1;\n\t\tbase->gen_dmac.high_prio_clear = D40_DREG_CPCEG1;\n\t\tbase->gen_dmac.il = il_v4b;\n\t\tbase->gen_dmac.il_size = ARRAY_SIZE(il_v4b);\n\t\tbase->gen_dmac.init_reg = dma_init_reg_v4b;\n\t\tbase->gen_dmac.init_reg_size = ARRAY_SIZE(dma_init_reg_v4b);\n\t} else {\n\t\tif (base->rev >= 3) {\n\t\t\tbase->gen_dmac.backup = d40_backup_regs_v4a;\n\t\t\tbase->gen_dmac.backup_size = BACKUP_REGS_SZ_V4A;\n\t\t}\n\t\tbase->gen_dmac.interrupt_en = D40_DREG_PCMIS;\n\t\tbase->gen_dmac.interrupt_clear = D40_DREG_PCICR;\n\t\tbase->gen_dmac.realtime_en = D40_DREG_RSEG1;\n\t\tbase->gen_dmac.realtime_clear = D40_DREG_RCEG1;\n\t\tbase->gen_dmac.high_prio_en = D40_DREG_PSEG1;\n\t\tbase->gen_dmac.high_prio_clear = D40_DREG_PCEG1;\n\t\tbase->gen_dmac.il = il_v4a;\n\t\tbase->gen_dmac.il_size = ARRAY_SIZE(il_v4a);\n\t\tbase->gen_dmac.init_reg = dma_init_reg_v4a;\n\t\tbase->gen_dmac.init_reg_size = ARRAY_SIZE(dma_init_reg_v4a);\n\t}\n\n\tbase->phy_res = devm_kcalloc(dev, num_phy_chans,\n\t\t\t\t     sizeof(*base->phy_res),\n\t\t\t\t     GFP_KERNEL);\n\tif (!base->phy_res)\n\t\treturn -ENOMEM;\n\n\tbase->lookup_phy_chans = devm_kcalloc(dev, num_phy_chans,\n\t\t\t\t\t      sizeof(*base->lookup_phy_chans),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!base->lookup_phy_chans)\n\t\treturn -ENOMEM;\n\n\tbase->lookup_log_chans = devm_kcalloc(dev, num_log_chans,\n\t\t\t\t\t      sizeof(*base->lookup_log_chans),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!base->lookup_log_chans)\n\t\treturn -ENOMEM;\n\n\tbase->reg_val_backup_chan = devm_kmalloc_array(dev, base->num_phy_chans,\n\t\t\t\t\t\t  sizeof(d40_backup_regs_chan),\n\t\t\t\t\t\t  GFP_KERNEL);\n\tif (!base->reg_val_backup_chan)\n\t\treturn -ENOMEM;\n\n\tbase->lcla_pool.alloc_map = devm_kcalloc(dev, num_phy_chans\n\t\t\t\t\t    * D40_LCLA_LINK_PER_EVENT_GRP,\n\t\t\t\t\t    sizeof(*base->lcla_pool.alloc_map),\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!base->lcla_pool.alloc_map)\n\t\treturn -ENOMEM;\n\n\tbase->regs_interrupt = devm_kmalloc_array(dev, base->gen_dmac.il_size,\n\t\t\t\t\t     sizeof(*base->regs_interrupt),\n\t\t\t\t\t     GFP_KERNEL);\n\tif (!base->regs_interrupt)\n\t\treturn -ENOMEM;\n\n\tbase->desc_slab = kmem_cache_create(D40_NAME, sizeof(struct d40_desc),\n\t\t\t\t\t    0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t    NULL);\n\tif (!base->desc_slab)\n\t\treturn -ENOMEM;\n\n\tret = devm_add_action_or_reset(dev, d40_drop_kmem_cache_action,\n\t\t\t\t       base->desc_slab);\n\tif (ret)\n\t\treturn ret;\n\n\t*retbase = base;\n\n\treturn 0;\n}\n\nstatic void __init d40_hw_init(struct d40_base *base)\n{\n\n\tint i;\n\tu32 prmseo[2] = {0, 0};\n\tu32 activeo[2] = {0xFFFFFFFF, 0xFFFFFFFF};\n\tu32 pcmis = 0;\n\tu32 pcicr = 0;\n\tstruct d40_reg_val *dma_init_reg = base->gen_dmac.init_reg;\n\tu32 reg_size = base->gen_dmac.init_reg_size;\n\n\tfor (i = 0; i < reg_size; i++)\n\t\twritel(dma_init_reg[i].val,\n\t\t       base->virtbase + dma_init_reg[i].reg);\n\n\t \n\tfor (i = 0; i < base->num_phy_chans; i++) {\n\n\t\tactiveo[i % 2] = activeo[i % 2] << 2;\n\n\t\tif (base->phy_res[base->num_phy_chans - i - 1].allocated_src\n\t\t    == D40_ALLOC_PHY) {\n\t\t\tactiveo[i % 2] |= 3;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tpcmis = (pcmis << 1) | 1;\n\n\t\t \n\t\tpcicr = (pcicr << 1) | 1;\n\n\t\t \n\t\tprmseo[i % 2] = prmseo[i % 2] << 2;\n\t\tprmseo[i % 2] |= 1;\n\n\t}\n\n\twritel(prmseo[1], base->virtbase + D40_DREG_PRMSE);\n\twritel(prmseo[0], base->virtbase + D40_DREG_PRMSO);\n\twritel(activeo[1], base->virtbase + D40_DREG_ACTIVE);\n\twritel(activeo[0], base->virtbase + D40_DREG_ACTIVO);\n\n\t \n\twritel(pcmis, base->virtbase + base->gen_dmac.interrupt_en);\n\n\t \n\twritel(pcicr, base->virtbase + base->gen_dmac.interrupt_clear);\n\n\t \n\tbase->gen_dmac.init_reg = NULL;\n\tbase->gen_dmac.init_reg_size = 0;\n}\n\nstatic int __init d40_lcla_allocate(struct d40_base *base)\n{\n\tstruct d40_lcla_pool *pool = &base->lcla_pool;\n\tunsigned long *page_list;\n\tint i, j;\n\tint ret;\n\n\t \n\tpage_list = kmalloc_array(MAX_LCLA_ALLOC_ATTEMPTS,\n\t\t\t\t  sizeof(*page_list),\n\t\t\t\t  GFP_KERNEL);\n\tif (!page_list)\n\t\treturn -ENOMEM;\n\n\t \n\tbase->lcla_pool.pages = SZ_1K * base->num_phy_chans / PAGE_SIZE;\n\n\tfor (i = 0; i < MAX_LCLA_ALLOC_ATTEMPTS; i++) {\n\t\tpage_list[i] = __get_free_pages(GFP_KERNEL,\n\t\t\t\t\t\tbase->lcla_pool.pages);\n\t\tif (!page_list[i]) {\n\n\t\t\td40_err(base->dev, \"Failed to allocate %d pages.\\n\",\n\t\t\t\tbase->lcla_pool.pages);\n\t\t\tret = -ENOMEM;\n\n\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\tfree_pages(page_list[j], base->lcla_pool.pages);\n\t\t\tgoto free_page_list;\n\t\t}\n\n\t\tif ((virt_to_phys((void *)page_list[i]) &\n\t\t     (LCLA_ALIGNMENT - 1)) == 0)\n\t\t\tbreak;\n\t}\n\n\tfor (j = 0; j < i; j++)\n\t\tfree_pages(page_list[j], base->lcla_pool.pages);\n\n\tif (i < MAX_LCLA_ALLOC_ATTEMPTS) {\n\t\tbase->lcla_pool.base = (void *)page_list[i];\n\t} else {\n\t\t \n\t\tdev_warn(base->dev,\n\t\t\t \"[%s] Failed to get %d pages @ 18 bit align.\\n\",\n\t\t\t __func__, base->lcla_pool.pages);\n\t\tbase->lcla_pool.base_unaligned = kmalloc(SZ_1K *\n\t\t\t\t\t\t\t base->num_phy_chans +\n\t\t\t\t\t\t\t LCLA_ALIGNMENT,\n\t\t\t\t\t\t\t GFP_KERNEL);\n\t\tif (!base->lcla_pool.base_unaligned) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_page_list;\n\t\t}\n\n\t\tbase->lcla_pool.base = PTR_ALIGN(base->lcla_pool.base_unaligned,\n\t\t\t\t\t\t LCLA_ALIGNMENT);\n\t}\n\n\tpool->dma_addr = dma_map_single(base->dev, pool->base,\n\t\t\t\t\tSZ_1K * base->num_phy_chans,\n\t\t\t\t\tDMA_TO_DEVICE);\n\tif (dma_mapping_error(base->dev, pool->dma_addr)) {\n\t\tpool->dma_addr = 0;\n\t\tret = -ENOMEM;\n\t\tgoto free_page_list;\n\t}\n\n\twritel(virt_to_phys(base->lcla_pool.base),\n\t       base->virtbase + D40_DREG_LCLA);\n\tret = 0;\n free_page_list:\n\tkfree(page_list);\n\treturn ret;\n}\n\nstatic int __init d40_of_probe(struct device *dev,\n\t\t\t       struct device_node *np)\n{\n\tstruct stedma40_platform_data *pdata;\n\tint num_phy = 0, num_memcpy = 0, num_disabled = 0;\n\tconst __be32 *list;\n\n\tpdata = devm_kzalloc(dev, sizeof(*pdata), GFP_KERNEL);\n\tif (!pdata)\n\t\treturn -ENOMEM;\n\n\t \n\tof_property_read_u32(np, \"dma-channels\", &num_phy);\n\tif (num_phy > 0)\n\t\tpdata->num_of_phy_chans = num_phy;\n\n\tlist = of_get_property(np, \"memcpy-channels\", &num_memcpy);\n\tnum_memcpy /= sizeof(*list);\n\n\tif (num_memcpy > D40_MEMCPY_MAX_CHANS || num_memcpy <= 0) {\n\t\td40_err(dev,\n\t\t\t\"Invalid number of memcpy channels specified (%d)\\n\",\n\t\t\tnum_memcpy);\n\t\treturn -EINVAL;\n\t}\n\tpdata->num_of_memcpy_chans = num_memcpy;\n\n\tof_property_read_u32_array(np, \"memcpy-channels\",\n\t\t\t\t   dma40_memcpy_channels,\n\t\t\t\t   num_memcpy);\n\n\tlist = of_get_property(np, \"disabled-channels\", &num_disabled);\n\tnum_disabled /= sizeof(*list);\n\n\tif (num_disabled >= STEDMA40_MAX_PHYS || num_disabled < 0) {\n\t\td40_err(dev,\n\t\t\t\"Invalid number of disabled channels specified (%d)\\n\",\n\t\t\tnum_disabled);\n\t\treturn -EINVAL;\n\t}\n\n\tof_property_read_u32_array(np, \"disabled-channels\",\n\t\t\t\t   pdata->disabled_channels,\n\t\t\t\t   num_disabled);\n\tpdata->disabled_channels[num_disabled] = -1;\n\n\tdev->platform_data = pdata;\n\n\treturn 0;\n}\n\nstatic int __init d40_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct device_node *np = pdev->dev.of_node;\n\tstruct device_node *np_lcpa;\n\tstruct d40_base *base;\n\tstruct resource *res;\n\tstruct resource res_lcpa;\n\tint num_reserved_chans;\n\tu32 val;\n\tint ret;\n\n\tif (d40_of_probe(dev, np)) {\n\t\tret = -ENOMEM;\n\t\tgoto report_failure;\n\t}\n\n\tret = d40_hw_detect_init(pdev, &base);\n\tif (ret)\n\t\tgoto report_failure;\n\n\tnum_reserved_chans = d40_phy_res_init(base);\n\n\tplatform_set_drvdata(pdev, base);\n\n\tspin_lock_init(&base->interrupt_lock);\n\tspin_lock_init(&base->execmd_lock);\n\n\t \n\tnp_lcpa = of_parse_phandle(np, \"sram\", 0);\n\tif (!np_lcpa) {\n\t\tdev_err(dev, \"no LCPA SRAM node\\n\");\n\t\tret = -EINVAL;\n\t\tgoto report_failure;\n\t}\n\t \n\tret = of_address_to_resource(np_lcpa, 0, &res_lcpa);\n\tif (ret) {\n\t\tdev_err(dev, \"no LCPA SRAM resource\\n\");\n\t\tgoto report_failure;\n\t}\n\tbase->lcpa_size = resource_size(&res_lcpa);\n\tbase->phy_lcpa = res_lcpa.start;\n\tdev_info(dev, \"found LCPA SRAM at %pad, size %pa\\n\",\n\t\t &base->phy_lcpa, &base->lcpa_size);\n\n\t \n\tval = readl(base->virtbase + D40_DREG_LCPA);\n\tif (base->phy_lcpa != val && val != 0) {\n\t\tdev_warn(dev,\n\t\t\t \"[%s] Mismatch LCPA dma 0x%x, def %08x\\n\",\n\t\t\t __func__, val, (u32)base->phy_lcpa);\n\t} else\n\t\twritel(base->phy_lcpa, base->virtbase + D40_DREG_LCPA);\n\n\tbase->lcpa_base = devm_ioremap(dev, base->phy_lcpa, base->lcpa_size);\n\tif (!base->lcpa_base) {\n\t\tret = -ENOMEM;\n\t\td40_err(dev, \"Failed to ioremap LCPA region\\n\");\n\t\tgoto report_failure;\n\t}\n\t \n\tif (base->plat_data->use_esram_lcla) {\n\t\tres = platform_get_resource_byname(pdev, IORESOURCE_MEM,\n\t\t\t\t\t\t\t\"lcla_esram\");\n\t\tif (!res) {\n\t\t\tret = -ENOENT;\n\t\t\td40_err(dev,\n\t\t\t\t\"No \\\"lcla_esram\\\" memory resource\\n\");\n\t\t\tgoto report_failure;\n\t\t}\n\t\tbase->lcla_pool.base = devm_ioremap(dev, res->start,\n\t\t\t\t\t\t    resource_size(res));\n\t\tif (!base->lcla_pool.base) {\n\t\t\tret = -ENOMEM;\n\t\t\td40_err(dev, \"Failed to ioremap LCLA region\\n\");\n\t\t\tgoto report_failure;\n\t\t}\n\t\twritel(res->start, base->virtbase + D40_DREG_LCLA);\n\n\t} else {\n\t\tret = d40_lcla_allocate(base);\n\t\tif (ret) {\n\t\t\td40_err(dev, \"Failed to allocate LCLA area\\n\");\n\t\t\tgoto destroy_cache;\n\t\t}\n\t}\n\n\tspin_lock_init(&base->lcla_pool.lock);\n\n\tbase->irq = platform_get_irq(pdev, 0);\n\tif (base->irq < 0) {\n\t\tret = base->irq;\n\t\tgoto destroy_cache;\n\t}\n\n\tret = request_irq(base->irq, d40_handle_interrupt, 0, D40_NAME, base);\n\tif (ret) {\n\t\td40_err(dev, \"No IRQ defined\\n\");\n\t\tgoto destroy_cache;\n\t}\n\n\tif (base->plat_data->use_esram_lcla) {\n\n\t\tbase->lcpa_regulator = regulator_get(base->dev, \"lcla_esram\");\n\t\tif (IS_ERR(base->lcpa_regulator)) {\n\t\t\td40_err(dev, \"Failed to get lcpa_regulator\\n\");\n\t\t\tret = PTR_ERR(base->lcpa_regulator);\n\t\t\tbase->lcpa_regulator = NULL;\n\t\t\tgoto destroy_cache;\n\t\t}\n\n\t\tret = regulator_enable(base->lcpa_regulator);\n\t\tif (ret) {\n\t\t\td40_err(dev,\n\t\t\t\t\"Failed to enable lcpa_regulator\\n\");\n\t\t\tregulator_put(base->lcpa_regulator);\n\t\t\tbase->lcpa_regulator = NULL;\n\t\t\tgoto destroy_cache;\n\t\t}\n\t}\n\n\twritel_relaxed(D40_DREG_GCC_ENABLE_ALL, base->virtbase + D40_DREG_GCC);\n\n\tpm_runtime_irq_safe(base->dev);\n\tpm_runtime_set_autosuspend_delay(base->dev, DMA40_AUTOSUSPEND_DELAY);\n\tpm_runtime_use_autosuspend(base->dev);\n\tpm_runtime_mark_last_busy(base->dev);\n\tpm_runtime_set_active(base->dev);\n\tpm_runtime_enable(base->dev);\n\n\tret = d40_dmaengine_init(base, num_reserved_chans);\n\tif (ret)\n\t\tgoto destroy_cache;\n\n\tret = dma_set_max_seg_size(base->dev, STEDMA40_MAX_SEG_SIZE);\n\tif (ret) {\n\t\td40_err(dev, \"Failed to set dma max seg size\\n\");\n\t\tgoto destroy_cache;\n\t}\n\n\td40_hw_init(base);\n\n\tret = of_dma_controller_register(np, d40_xlate, NULL);\n\tif (ret) {\n\t\tdev_err(dev,\n\t\t\t\"could not register of_dma_controller\\n\");\n\t\tgoto destroy_cache;\n\t}\n\n\tdev_info(base->dev, \"initialized\\n\");\n\treturn 0;\n\n destroy_cache:\n\tif (base->lcla_pool.dma_addr)\n\t\tdma_unmap_single(base->dev, base->lcla_pool.dma_addr,\n\t\t\t\t SZ_1K * base->num_phy_chans,\n\t\t\t\t DMA_TO_DEVICE);\n\n\tif (!base->lcla_pool.base_unaligned && base->lcla_pool.base)\n\t\tfree_pages((unsigned long)base->lcla_pool.base,\n\t\t\t   base->lcla_pool.pages);\n\n\tkfree(base->lcla_pool.base_unaligned);\n\n\tif (base->lcpa_regulator) {\n\t\tregulator_disable(base->lcpa_regulator);\n\t\tregulator_put(base->lcpa_regulator);\n\t}\n\tpm_runtime_disable(base->dev);\n\n report_failure:\n\td40_err(dev, \"probe failed\\n\");\n\treturn ret;\n}\n\nstatic const struct of_device_id d40_match[] = {\n        { .compatible = \"stericsson,dma40\", },\n        {}\n};\n\nstatic struct platform_driver d40_driver = {\n\t.driver = {\n\t\t.name  = D40_NAME,\n\t\t.pm = &dma40_pm_ops,\n\t\t.of_match_table = d40_match,\n\t},\n};\n\nstatic int __init stedma40_init(void)\n{\n\treturn platform_driver_probe(&d40_driver, d40_probe);\n}\nsubsys_initcall(stedma40_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}