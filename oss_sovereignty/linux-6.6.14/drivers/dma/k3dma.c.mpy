{
  "module_name": "k3dma.c",
  "hash_id": "6c744c8380bbe099adab88e9add78df2c05e3e524240968cbbcf9b09cc9fc449",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/k3dma.c",
  "human_readable_source": "\n \n#include <linux/sched.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/dmaengine.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/of_device.h>\n#include <linux/of.h>\n#include <linux/clk.h>\n#include <linux/of_dma.h>\n\n#include \"virt-dma.h\"\n\n#define DRIVER_NAME\t\t\"k3-dma\"\n#define DMA_MAX_SIZE\t\t0x1ffc\n#define DMA_CYCLIC_MAX_PERIOD\t0x1000\n#define LLI_BLOCK_SIZE\t\t(4 * PAGE_SIZE)\n\n#define INT_STAT\t\t0x00\n#define INT_TC1\t\t\t0x04\n#define INT_TC2\t\t\t0x08\n#define INT_ERR1\t\t0x0c\n#define INT_ERR2\t\t0x10\n#define INT_TC1_MASK\t\t0x18\n#define INT_TC2_MASK\t\t0x1c\n#define INT_ERR1_MASK\t\t0x20\n#define INT_ERR2_MASK\t\t0x24\n#define INT_TC1_RAW\t\t0x600\n#define INT_TC2_RAW\t\t0x608\n#define INT_ERR1_RAW\t\t0x610\n#define INT_ERR2_RAW\t\t0x618\n#define CH_PRI\t\t\t0x688\n#define CH_STAT\t\t\t0x690\n#define CX_CUR_CNT\t\t0x704\n#define CX_LLI\t\t\t0x800\n#define CX_CNT1\t\t\t0x80c\n#define CX_CNT0\t\t\t0x810\n#define CX_SRC\t\t\t0x814\n#define CX_DST\t\t\t0x818\n#define CX_CFG\t\t\t0x81c\n\n#define CX_LLI_CHAIN_EN\t\t0x2\n#define CX_CFG_EN\t\t0x1\n#define CX_CFG_NODEIRQ\t\tBIT(1)\n#define CX_CFG_MEM2PER\t\t(0x1 << 2)\n#define CX_CFG_PER2MEM\t\t(0x2 << 2)\n#define CX_CFG_SRCINCR\t\t(0x1 << 31)\n#define CX_CFG_DSTINCR\t\t(0x1 << 30)\n\nstruct k3_desc_hw {\n\tu32 lli;\n\tu32 reserved[3];\n\tu32 count;\n\tu32 saddr;\n\tu32 daddr;\n\tu32 config;\n} __aligned(32);\n\nstruct k3_dma_desc_sw {\n\tstruct virt_dma_desc\tvd;\n\tdma_addr_t\t\tdesc_hw_lli;\n\tsize_t\t\t\tdesc_num;\n\tsize_t\t\t\tsize;\n\tstruct k3_desc_hw\t*desc_hw;\n};\n\nstruct k3_dma_phy;\n\nstruct k3_dma_chan {\n\tu32\t\t\tccfg;\n\tstruct virt_dma_chan\tvc;\n\tstruct k3_dma_phy\t*phy;\n\tstruct list_head\tnode;\n\tdma_addr_t\t\tdev_addr;\n\tenum dma_status\t\tstatus;\n\tbool\t\t\tcyclic;\n\tstruct dma_slave_config\tslave_config;\n};\n\nstruct k3_dma_phy {\n\tu32\t\t\tidx;\n\tvoid __iomem\t\t*base;\n\tstruct k3_dma_chan\t*vchan;\n\tstruct k3_dma_desc_sw\t*ds_run;\n\tstruct k3_dma_desc_sw\t*ds_done;\n};\n\nstruct k3_dma_dev {\n\tstruct dma_device\tslave;\n\tvoid __iomem\t\t*base;\n\tstruct tasklet_struct\ttask;\n\tspinlock_t\t\tlock;\n\tstruct list_head\tchan_pending;\n\tstruct k3_dma_phy\t*phy;\n\tstruct k3_dma_chan\t*chans;\n\tstruct clk\t\t*clk;\n\tstruct dma_pool\t\t*pool;\n\tu32\t\t\tdma_channels;\n\tu32\t\t\tdma_requests;\n\tu32\t\t\tdma_channel_mask;\n\tunsigned int\t\tirq;\n};\n\n\n#define K3_FLAG_NOCLK\tBIT(1)\n\nstruct k3dma_soc_data {\n\tunsigned long flags;\n};\n\n\n#define to_k3_dma(dmadev) container_of(dmadev, struct k3_dma_dev, slave)\n\nstatic int k3_dma_config_write(struct dma_chan *chan,\n\t\t\t       enum dma_transfer_direction dir,\n\t\t\t       struct dma_slave_config *cfg);\n\nstatic struct k3_dma_chan *to_k3_chan(struct dma_chan *chan)\n{\n\treturn container_of(chan, struct k3_dma_chan, vc.chan);\n}\n\nstatic void k3_dma_pause_dma(struct k3_dma_phy *phy, bool on)\n{\n\tu32 val = 0;\n\n\tif (on) {\n\t\tval = readl_relaxed(phy->base + CX_CFG);\n\t\tval |= CX_CFG_EN;\n\t\twritel_relaxed(val, phy->base + CX_CFG);\n\t} else {\n\t\tval = readl_relaxed(phy->base + CX_CFG);\n\t\tval &= ~CX_CFG_EN;\n\t\twritel_relaxed(val, phy->base + CX_CFG);\n\t}\n}\n\nstatic void k3_dma_terminate_chan(struct k3_dma_phy *phy, struct k3_dma_dev *d)\n{\n\tu32 val = 0;\n\n\tk3_dma_pause_dma(phy, false);\n\n\tval = 0x1 << phy->idx;\n\twritel_relaxed(val, d->base + INT_TC1_RAW);\n\twritel_relaxed(val, d->base + INT_TC2_RAW);\n\twritel_relaxed(val, d->base + INT_ERR1_RAW);\n\twritel_relaxed(val, d->base + INT_ERR2_RAW);\n}\n\nstatic void k3_dma_set_desc(struct k3_dma_phy *phy, struct k3_desc_hw *hw)\n{\n\twritel_relaxed(hw->lli, phy->base + CX_LLI);\n\twritel_relaxed(hw->count, phy->base + CX_CNT0);\n\twritel_relaxed(hw->saddr, phy->base + CX_SRC);\n\twritel_relaxed(hw->daddr, phy->base + CX_DST);\n\twritel_relaxed(hw->config, phy->base + CX_CFG);\n}\n\nstatic u32 k3_dma_get_curr_cnt(struct k3_dma_dev *d, struct k3_dma_phy *phy)\n{\n\tu32 cnt = 0;\n\n\tcnt = readl_relaxed(d->base + CX_CUR_CNT + phy->idx * 0x10);\n\tcnt &= 0xffff;\n\treturn cnt;\n}\n\nstatic u32 k3_dma_get_curr_lli(struct k3_dma_phy *phy)\n{\n\treturn readl_relaxed(phy->base + CX_LLI);\n}\n\nstatic u32 k3_dma_get_chan_stat(struct k3_dma_dev *d)\n{\n\treturn readl_relaxed(d->base + CH_STAT);\n}\n\nstatic void k3_dma_enable_dma(struct k3_dma_dev *d, bool on)\n{\n\tif (on) {\n\t\t \n\t\twritel_relaxed(0x0, d->base + CH_PRI);\n\n\t\t \n\t\twritel_relaxed(0xffff, d->base + INT_TC1_MASK);\n\t\twritel_relaxed(0xffff, d->base + INT_TC2_MASK);\n\t\twritel_relaxed(0xffff, d->base + INT_ERR1_MASK);\n\t\twritel_relaxed(0xffff, d->base + INT_ERR2_MASK);\n\t} else {\n\t\t \n\t\twritel_relaxed(0x0, d->base + INT_TC1_MASK);\n\t\twritel_relaxed(0x0, d->base + INT_TC2_MASK);\n\t\twritel_relaxed(0x0, d->base + INT_ERR1_MASK);\n\t\twritel_relaxed(0x0, d->base + INT_ERR2_MASK);\n\t}\n}\n\nstatic irqreturn_t k3_dma_int_handler(int irq, void *dev_id)\n{\n\tstruct k3_dma_dev *d = (struct k3_dma_dev *)dev_id;\n\tstruct k3_dma_phy *p;\n\tstruct k3_dma_chan *c;\n\tu32 stat = readl_relaxed(d->base + INT_STAT);\n\tu32 tc1  = readl_relaxed(d->base + INT_TC1);\n\tu32 tc2  = readl_relaxed(d->base + INT_TC2);\n\tu32 err1 = readl_relaxed(d->base + INT_ERR1);\n\tu32 err2 = readl_relaxed(d->base + INT_ERR2);\n\tu32 i, irq_chan = 0;\n\n\twhile (stat) {\n\t\ti = __ffs(stat);\n\t\tstat &= ~BIT(i);\n\t\tif (likely(tc1 & BIT(i)) || (tc2 & BIT(i))) {\n\n\t\t\tp = &d->phy[i];\n\t\t\tc = p->vchan;\n\t\t\tif (c && (tc1 & BIT(i))) {\n\t\t\t\tspin_lock(&c->vc.lock);\n\t\t\t\tif (p->ds_run != NULL) {\n\t\t\t\t\tvchan_cookie_complete(&p->ds_run->vd);\n\t\t\t\t\tp->ds_done = p->ds_run;\n\t\t\t\t\tp->ds_run = NULL;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&c->vc.lock);\n\t\t\t}\n\t\t\tif (c && (tc2 & BIT(i))) {\n\t\t\t\tspin_lock(&c->vc.lock);\n\t\t\t\tif (p->ds_run != NULL)\n\t\t\t\t\tvchan_cyclic_callback(&p->ds_run->vd);\n\t\t\t\tspin_unlock(&c->vc.lock);\n\t\t\t}\n\t\t\tirq_chan |= BIT(i);\n\t\t}\n\t\tif (unlikely((err1 & BIT(i)) || (err2 & BIT(i))))\n\t\t\tdev_warn(d->slave.dev, \"DMA ERR\\n\");\n\t}\n\n\twritel_relaxed(irq_chan, d->base + INT_TC1_RAW);\n\twritel_relaxed(irq_chan, d->base + INT_TC2_RAW);\n\twritel_relaxed(err1, d->base + INT_ERR1_RAW);\n\twritel_relaxed(err2, d->base + INT_ERR2_RAW);\n\n\tif (irq_chan)\n\t\ttasklet_schedule(&d->task);\n\n\tif (irq_chan || err1 || err2)\n\t\treturn IRQ_HANDLED;\n\n\treturn IRQ_NONE;\n}\n\nstatic int k3_dma_start_txd(struct k3_dma_chan *c)\n{\n\tstruct k3_dma_dev *d = to_k3_dma(c->vc.chan.device);\n\tstruct virt_dma_desc *vd = vchan_next_desc(&c->vc);\n\n\tif (!c->phy)\n\t\treturn -EAGAIN;\n\n\tif (BIT(c->phy->idx) & k3_dma_get_chan_stat(d))\n\t\treturn -EAGAIN;\n\n\t \n\tif (c->phy->ds_run)\n\t\treturn -EAGAIN;\n\n\tif (vd) {\n\t\tstruct k3_dma_desc_sw *ds =\n\t\t\tcontainer_of(vd, struct k3_dma_desc_sw, vd);\n\t\t \n\t\tlist_del(&ds->vd.node);\n\n\t\tc->phy->ds_run = ds;\n\t\tc->phy->ds_done = NULL;\n\t\t \n\t\tk3_dma_set_desc(c->phy, &ds->desc_hw[0]);\n\t\treturn 0;\n\t}\n\tc->phy->ds_run = NULL;\n\tc->phy->ds_done = NULL;\n\treturn -EAGAIN;\n}\n\nstatic void k3_dma_tasklet(struct tasklet_struct *t)\n{\n\tstruct k3_dma_dev *d = from_tasklet(d, t, task);\n\tstruct k3_dma_phy *p;\n\tstruct k3_dma_chan *c, *cn;\n\tunsigned pch, pch_alloc = 0;\n\n\t \n\tlist_for_each_entry_safe(c, cn, &d->slave.channels, vc.chan.device_node) {\n\t\tspin_lock_irq(&c->vc.lock);\n\t\tp = c->phy;\n\t\tif (p && p->ds_done) {\n\t\t\tif (k3_dma_start_txd(c)) {\n\t\t\t\t \n\t\t\t\tdev_dbg(d->slave.dev, \"pchan %u: free\\n\", p->idx);\n\t\t\t\t \n\t\t\t\tc->phy = NULL;\n\t\t\t\tp->vchan = NULL;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(&c->vc.lock);\n\t}\n\n\t \n\tspin_lock_irq(&d->lock);\n\tfor (pch = 0; pch < d->dma_channels; pch++) {\n\t\tif (!(d->dma_channel_mask & (1 << pch)))\n\t\t\tcontinue;\n\n\t\tp = &d->phy[pch];\n\n\t\tif (p->vchan == NULL && !list_empty(&d->chan_pending)) {\n\t\t\tc = list_first_entry(&d->chan_pending,\n\t\t\t\tstruct k3_dma_chan, node);\n\t\t\t \n\t\t\tlist_del_init(&c->node);\n\t\t\tpch_alloc |= 1 << pch;\n\t\t\t \n\t\t\tp->vchan = c;\n\t\t\tc->phy = p;\n\t\t\tdev_dbg(d->slave.dev, \"pchan %u: alloc vchan %p\\n\", pch, &c->vc);\n\t\t}\n\t}\n\tspin_unlock_irq(&d->lock);\n\n\tfor (pch = 0; pch < d->dma_channels; pch++) {\n\t\tif (!(d->dma_channel_mask & (1 << pch)))\n\t\t\tcontinue;\n\n\t\tif (pch_alloc & (1 << pch)) {\n\t\t\tp = &d->phy[pch];\n\t\t\tc = p->vchan;\n\t\t\tif (c) {\n\t\t\t\tspin_lock_irq(&c->vc.lock);\n\t\t\t\tk3_dma_start_txd(c);\n\t\t\t\tspin_unlock_irq(&c->vc.lock);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void k3_dma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_dev *d = to_k3_dma(chan->device);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&d->lock, flags);\n\tlist_del_init(&c->node);\n\tspin_unlock_irqrestore(&d->lock, flags);\n\n\tvchan_free_chan_resources(&c->vc);\n\tc->ccfg = 0;\n}\n\nstatic enum dma_status k3_dma_tx_status(struct dma_chan *chan,\n\tdma_cookie_t cookie, struct dma_tx_state *state)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_dev *d = to_k3_dma(chan->device);\n\tstruct k3_dma_phy *p;\n\tstruct virt_dma_desc *vd;\n\tunsigned long flags;\n\tenum dma_status ret;\n\tsize_t bytes = 0;\n\n\tret = dma_cookie_status(&c->vc.chan, cookie, state);\n\tif (ret == DMA_COMPLETE)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&c->vc.lock, flags);\n\tp = c->phy;\n\tret = c->status;\n\n\t \n\tvd = vchan_find_desc(&c->vc, cookie);\n\tif (vd && !c->cyclic) {\n\t\tbytes = container_of(vd, struct k3_dma_desc_sw, vd)->size;\n\t} else if ((!p) || (!p->ds_run)) {\n\t\tbytes = 0;\n\t} else {\n\t\tstruct k3_dma_desc_sw *ds = p->ds_run;\n\t\tu32 clli = 0, index = 0;\n\n\t\tbytes = k3_dma_get_curr_cnt(d, p);\n\t\tclli = k3_dma_get_curr_lli(p);\n\t\tindex = ((clli - ds->desc_hw_lli) /\n\t\t\t\tsizeof(struct k3_desc_hw)) + 1;\n\t\tfor (; index < ds->desc_num; index++) {\n\t\t\tbytes += ds->desc_hw[index].count;\n\t\t\t \n\t\t\tif (!ds->desc_hw[index].lli)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&c->vc.lock, flags);\n\tdma_set_residue(state, bytes);\n\treturn ret;\n}\n\nstatic void k3_dma_issue_pending(struct dma_chan *chan)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_dev *d = to_k3_dma(chan->device);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&c->vc.lock, flags);\n\t \n\tif (vchan_issue_pending(&c->vc)) {\n\t\tspin_lock(&d->lock);\n\t\tif (!c->phy) {\n\t\t\tif (list_empty(&c->node)) {\n\t\t\t\t \n\t\t\t\tlist_add_tail(&c->node, &d->chan_pending);\n\t\t\t\t \n\t\t\t\ttasklet_schedule(&d->task);\n\t\t\t\tdev_dbg(d->slave.dev, \"vchan %p: issued\\n\", &c->vc);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&d->lock);\n\t} else\n\t\tdev_dbg(d->slave.dev, \"vchan %p: nothing to issue\\n\", &c->vc);\n\tspin_unlock_irqrestore(&c->vc.lock, flags);\n}\n\nstatic void k3_dma_fill_desc(struct k3_dma_desc_sw *ds, dma_addr_t dst,\n\t\t\tdma_addr_t src, size_t len, u32 num, u32 ccfg)\n{\n\tif (num != ds->desc_num - 1)\n\t\tds->desc_hw[num].lli = ds->desc_hw_lli + (num + 1) *\n\t\t\tsizeof(struct k3_desc_hw);\n\n\tds->desc_hw[num].lli |= CX_LLI_CHAIN_EN;\n\tds->desc_hw[num].count = len;\n\tds->desc_hw[num].saddr = src;\n\tds->desc_hw[num].daddr = dst;\n\tds->desc_hw[num].config = ccfg;\n}\n\nstatic struct k3_dma_desc_sw *k3_dma_alloc_desc_resource(int num,\n\t\t\t\t\t\t\tstruct dma_chan *chan)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_desc_sw *ds;\n\tstruct k3_dma_dev *d = to_k3_dma(chan->device);\n\tint lli_limit = LLI_BLOCK_SIZE / sizeof(struct k3_desc_hw);\n\n\tif (num > lli_limit) {\n\t\tdev_dbg(chan->device->dev, \"vch %p: sg num %d exceed max %d\\n\",\n\t\t\t&c->vc, num, lli_limit);\n\t\treturn NULL;\n\t}\n\n\tds = kzalloc(sizeof(*ds), GFP_NOWAIT);\n\tif (!ds)\n\t\treturn NULL;\n\n\tds->desc_hw = dma_pool_zalloc(d->pool, GFP_NOWAIT, &ds->desc_hw_lli);\n\tif (!ds->desc_hw) {\n\t\tdev_dbg(chan->device->dev, \"vch %p: dma alloc fail\\n\", &c->vc);\n\t\tkfree(ds);\n\t\treturn NULL;\n\t}\n\tds->desc_num = num;\n\treturn ds;\n}\n\nstatic struct dma_async_tx_descriptor *k3_dma_prep_memcpy(\n\tstruct dma_chan *chan,\tdma_addr_t dst, dma_addr_t src,\n\tsize_t len, unsigned long flags)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_desc_sw *ds;\n\tsize_t copy = 0;\n\tint num = 0;\n\n\tif (!len)\n\t\treturn NULL;\n\n\tnum = DIV_ROUND_UP(len, DMA_MAX_SIZE);\n\n\tds = k3_dma_alloc_desc_resource(num, chan);\n\tif (!ds)\n\t\treturn NULL;\n\n\tc->cyclic = 0;\n\tds->size = len;\n\tnum = 0;\n\n\tif (!c->ccfg) {\n\t\t \n\t\tc->ccfg = CX_CFG_SRCINCR | CX_CFG_DSTINCR | CX_CFG_EN;\n\t\tc->ccfg |= (0xf << 20) | (0xf << 24);\t \n\t\tc->ccfg |= (0x3 << 12) | (0x3 << 16);\t \n\t}\n\n\tdo {\n\t\tcopy = min_t(size_t, len, DMA_MAX_SIZE);\n\t\tk3_dma_fill_desc(ds, dst, src, copy, num++, c->ccfg);\n\n\t\tsrc += copy;\n\t\tdst += copy;\n\t\tlen -= copy;\n\t} while (len);\n\n\tds->desc_hw[num-1].lli = 0;\t \n\treturn vchan_tx_prep(&c->vc, &ds->vd, flags);\n}\n\nstatic struct dma_async_tx_descriptor *k3_dma_prep_slave_sg(\n\tstruct dma_chan *chan, struct scatterlist *sgl, unsigned int sglen,\n\tenum dma_transfer_direction dir, unsigned long flags, void *context)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_desc_sw *ds;\n\tsize_t len, avail, total = 0;\n\tstruct scatterlist *sg;\n\tdma_addr_t addr, src = 0, dst = 0;\n\tint num = sglen, i;\n\n\tif (sgl == NULL)\n\t\treturn NULL;\n\n\tc->cyclic = 0;\n\n\tfor_each_sg(sgl, sg, sglen, i) {\n\t\tavail = sg_dma_len(sg);\n\t\tif (avail > DMA_MAX_SIZE)\n\t\t\tnum += DIV_ROUND_UP(avail, DMA_MAX_SIZE) - 1;\n\t}\n\n\tds = k3_dma_alloc_desc_resource(num, chan);\n\tif (!ds)\n\t\treturn NULL;\n\tnum = 0;\n\tk3_dma_config_write(chan, dir, &c->slave_config);\n\n\tfor_each_sg(sgl, sg, sglen, i) {\n\t\taddr = sg_dma_address(sg);\n\t\tavail = sg_dma_len(sg);\n\t\ttotal += avail;\n\n\t\tdo {\n\t\t\tlen = min_t(size_t, avail, DMA_MAX_SIZE);\n\n\t\t\tif (dir == DMA_MEM_TO_DEV) {\n\t\t\t\tsrc = addr;\n\t\t\t\tdst = c->dev_addr;\n\t\t\t} else if (dir == DMA_DEV_TO_MEM) {\n\t\t\t\tsrc = c->dev_addr;\n\t\t\t\tdst = addr;\n\t\t\t}\n\n\t\t\tk3_dma_fill_desc(ds, dst, src, len, num++, c->ccfg);\n\n\t\t\taddr += len;\n\t\t\tavail -= len;\n\t\t} while (avail);\n\t}\n\n\tds->desc_hw[num-1].lli = 0;\t \n\tds->size = total;\n\treturn vchan_tx_prep(&c->vc, &ds->vd, flags);\n}\n\nstatic struct dma_async_tx_descriptor *\nk3_dma_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t buf_addr,\n\t\t       size_t buf_len, size_t period_len,\n\t\t       enum dma_transfer_direction dir,\n\t\t       unsigned long flags)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_desc_sw *ds;\n\tsize_t len, avail, total = 0;\n\tdma_addr_t addr, src = 0, dst = 0;\n\tint num = 1, since = 0;\n\tsize_t modulo = DMA_CYCLIC_MAX_PERIOD;\n\tu32 en_tc2 = 0;\n\n\tdev_dbg(chan->device->dev, \"%s: buf %pad, dst %pad, buf len %zu, period_len = %zu, dir %d\\n\",\n\t       __func__, &buf_addr, &to_k3_chan(chan)->dev_addr,\n\t       buf_len, period_len, (int)dir);\n\n\tavail = buf_len;\n\tif (avail > modulo)\n\t\tnum += DIV_ROUND_UP(avail, modulo) - 1;\n\n\tds = k3_dma_alloc_desc_resource(num, chan);\n\tif (!ds)\n\t\treturn NULL;\n\n\tc->cyclic = 1;\n\taddr = buf_addr;\n\tavail = buf_len;\n\ttotal = avail;\n\tnum = 0;\n\tk3_dma_config_write(chan, dir, &c->slave_config);\n\n\tif (period_len < modulo)\n\t\tmodulo = period_len;\n\n\tdo {\n\t\tlen = min_t(size_t, avail, modulo);\n\n\t\tif (dir == DMA_MEM_TO_DEV) {\n\t\t\tsrc = addr;\n\t\t\tdst = c->dev_addr;\n\t\t} else if (dir == DMA_DEV_TO_MEM) {\n\t\t\tsrc = c->dev_addr;\n\t\t\tdst = addr;\n\t\t}\n\t\tsince += len;\n\t\tif (since >= period_len) {\n\t\t\t \n\t\t\ten_tc2 = CX_CFG_NODEIRQ;\n\t\t\tsince -= period_len;\n\t\t} else\n\t\t\ten_tc2 = 0;\n\n\t\tk3_dma_fill_desc(ds, dst, src, len, num++, c->ccfg | en_tc2);\n\n\t\taddr += len;\n\t\tavail -= len;\n\t} while (avail);\n\n\t \n\tds->desc_hw[num - 1].lli |= ds->desc_hw_lli;\n\n\tds->size = total;\n\n\treturn vchan_tx_prep(&c->vc, &ds->vd, flags);\n}\n\nstatic int k3_dma_config(struct dma_chan *chan,\n\t\t\t struct dma_slave_config *cfg)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\n\tmemcpy(&c->slave_config, cfg, sizeof(*cfg));\n\n\treturn 0;\n}\n\nstatic int k3_dma_config_write(struct dma_chan *chan,\n\t\t\t       enum dma_transfer_direction dir,\n\t\t\t       struct dma_slave_config *cfg)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tu32 maxburst = 0, val = 0;\n\tenum dma_slave_buswidth width = DMA_SLAVE_BUSWIDTH_UNDEFINED;\n\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\tc->ccfg = CX_CFG_DSTINCR;\n\t\tc->dev_addr = cfg->src_addr;\n\t\tmaxburst = cfg->src_maxburst;\n\t\twidth = cfg->src_addr_width;\n\t} else if (dir == DMA_MEM_TO_DEV) {\n\t\tc->ccfg = CX_CFG_SRCINCR;\n\t\tc->dev_addr = cfg->dst_addr;\n\t\tmaxburst = cfg->dst_maxburst;\n\t\twidth = cfg->dst_addr_width;\n\t}\n\tswitch (width) {\n\tcase DMA_SLAVE_BUSWIDTH_1_BYTE:\n\tcase DMA_SLAVE_BUSWIDTH_2_BYTES:\n\tcase DMA_SLAVE_BUSWIDTH_4_BYTES:\n\tcase DMA_SLAVE_BUSWIDTH_8_BYTES:\n\t\tval =  __ffs(width);\n\t\tbreak;\n\tdefault:\n\t\tval = 3;\n\t\tbreak;\n\t}\n\tc->ccfg |= (val << 12) | (val << 16);\n\n\tif ((maxburst == 0) || (maxburst > 16))\n\t\tval = 15;\n\telse\n\t\tval = maxburst - 1;\n\tc->ccfg |= (val << 20) | (val << 24);\n\tc->ccfg |= CX_CFG_MEM2PER | CX_CFG_EN;\n\n\t \n\tc->ccfg |= c->vc.chan.chan_id << 4;\n\n\treturn 0;\n}\n\nstatic void k3_dma_free_desc(struct virt_dma_desc *vd)\n{\n\tstruct k3_dma_desc_sw *ds =\n\t\tcontainer_of(vd, struct k3_dma_desc_sw, vd);\n\tstruct k3_dma_dev *d = to_k3_dma(vd->tx.chan->device);\n\n\tdma_pool_free(d->pool, ds->desc_hw, ds->desc_hw_lli);\n\tkfree(ds);\n}\n\nstatic int k3_dma_terminate_all(struct dma_chan *chan)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_dev *d = to_k3_dma(chan->device);\n\tstruct k3_dma_phy *p = c->phy;\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tdev_dbg(d->slave.dev, \"vchan %p: terminate all\\n\", &c->vc);\n\n\t \n\tspin_lock(&d->lock);\n\tlist_del_init(&c->node);\n\tspin_unlock(&d->lock);\n\n\t \n\tspin_lock_irqsave(&c->vc.lock, flags);\n\tvchan_get_all_descriptors(&c->vc, &head);\n\tif (p) {\n\t\t \n\t\tk3_dma_terminate_chan(p, d);\n\t\tc->phy = NULL;\n\t\tp->vchan = NULL;\n\t\tif (p->ds_run) {\n\t\t\tvchan_terminate_vdesc(&p->ds_run->vd);\n\t\t\tp->ds_run = NULL;\n\t\t}\n\t\tp->ds_done = NULL;\n\t}\n\tspin_unlock_irqrestore(&c->vc.lock, flags);\n\tvchan_dma_desc_free_list(&c->vc, &head);\n\n\treturn 0;\n}\n\nstatic void k3_dma_synchronize(struct dma_chan *chan)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\n\tvchan_synchronize(&c->vc);\n}\n\nstatic int k3_dma_transfer_pause(struct dma_chan *chan)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_dev *d = to_k3_dma(chan->device);\n\tstruct k3_dma_phy *p = c->phy;\n\n\tdev_dbg(d->slave.dev, \"vchan %p: pause\\n\", &c->vc);\n\tif (c->status == DMA_IN_PROGRESS) {\n\t\tc->status = DMA_PAUSED;\n\t\tif (p) {\n\t\t\tk3_dma_pause_dma(p, false);\n\t\t} else {\n\t\t\tspin_lock(&d->lock);\n\t\t\tlist_del_init(&c->node);\n\t\t\tspin_unlock(&d->lock);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int k3_dma_transfer_resume(struct dma_chan *chan)\n{\n\tstruct k3_dma_chan *c = to_k3_chan(chan);\n\tstruct k3_dma_dev *d = to_k3_dma(chan->device);\n\tstruct k3_dma_phy *p = c->phy;\n\tunsigned long flags;\n\n\tdev_dbg(d->slave.dev, \"vchan %p: resume\\n\", &c->vc);\n\tspin_lock_irqsave(&c->vc.lock, flags);\n\tif (c->status == DMA_PAUSED) {\n\t\tc->status = DMA_IN_PROGRESS;\n\t\tif (p) {\n\t\t\tk3_dma_pause_dma(p, true);\n\t\t} else if (!list_empty(&c->vc.desc_issued)) {\n\t\t\tspin_lock(&d->lock);\n\t\t\tlist_add_tail(&c->node, &d->chan_pending);\n\t\t\tspin_unlock(&d->lock);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&c->vc.lock, flags);\n\n\treturn 0;\n}\n\nstatic const struct k3dma_soc_data k3_v1_dma_data = {\n\t.flags = 0,\n};\n\nstatic const struct k3dma_soc_data asp_v1_dma_data = {\n\t.flags = K3_FLAG_NOCLK,\n};\n\nstatic const struct of_device_id k3_pdma_dt_ids[] = {\n\t{ .compatible = \"hisilicon,k3-dma-1.0\",\n\t  .data = &k3_v1_dma_data\n\t},\n\t{ .compatible = \"hisilicon,hisi-pcm-asp-dma-1.0\",\n\t  .data = &asp_v1_dma_data\n\t},\n\t{}\n};\nMODULE_DEVICE_TABLE(of, k3_pdma_dt_ids);\n\nstatic struct dma_chan *k3_of_dma_simple_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t\t\tstruct of_dma *ofdma)\n{\n\tstruct k3_dma_dev *d = ofdma->of_dma_data;\n\tunsigned int request = dma_spec->args[0];\n\n\tif (request >= d->dma_requests)\n\t\treturn NULL;\n\n\treturn dma_get_slave_channel(&(d->chans[request].vc.chan));\n}\n\nstatic int k3_dma_probe(struct platform_device *op)\n{\n\tconst struct k3dma_soc_data *soc_data;\n\tstruct k3_dma_dev *d;\n\tconst struct of_device_id *of_id;\n\tint i, ret, irq = 0;\n\n\td = devm_kzalloc(&op->dev, sizeof(*d), GFP_KERNEL);\n\tif (!d)\n\t\treturn -ENOMEM;\n\n\tsoc_data = device_get_match_data(&op->dev);\n\tif (!soc_data)\n\t\treturn -EINVAL;\n\n\td->base = devm_platform_ioremap_resource(op, 0);\n\tif (IS_ERR(d->base))\n\t\treturn PTR_ERR(d->base);\n\n\tof_id = of_match_device(k3_pdma_dt_ids, &op->dev);\n\tif (of_id) {\n\t\tof_property_read_u32((&op->dev)->of_node,\n\t\t\t\t\"dma-channels\", &d->dma_channels);\n\t\tof_property_read_u32((&op->dev)->of_node,\n\t\t\t\t\"dma-requests\", &d->dma_requests);\n\t\tret = of_property_read_u32((&op->dev)->of_node,\n\t\t\t\t\"dma-channel-mask\", &d->dma_channel_mask);\n\t\tif (ret) {\n\t\t\tdev_warn(&op->dev,\n\t\t\t\t \"dma-channel-mask doesn't exist, considering all as available.\\n\");\n\t\t\td->dma_channel_mask = (u32)~0UL;\n\t\t}\n\t}\n\n\tif (!(soc_data->flags & K3_FLAG_NOCLK)) {\n\t\td->clk = devm_clk_get(&op->dev, NULL);\n\t\tif (IS_ERR(d->clk)) {\n\t\t\tdev_err(&op->dev, \"no dma clk\\n\");\n\t\t\treturn PTR_ERR(d->clk);\n\t\t}\n\t}\n\n\tirq = platform_get_irq(op, 0);\n\tret = devm_request_irq(&op->dev, irq,\n\t\t\tk3_dma_int_handler, 0, DRIVER_NAME, d);\n\tif (ret)\n\t\treturn ret;\n\n\td->irq = irq;\n\n\t \n\td->pool = dmam_pool_create(DRIVER_NAME, &op->dev,\n\t\t\t\t\tLLI_BLOCK_SIZE, 32, 0);\n\tif (!d->pool)\n\t\treturn -ENOMEM;\n\n\t \n\td->phy = devm_kcalloc(&op->dev,\n\t\td->dma_channels, sizeof(struct k3_dma_phy), GFP_KERNEL);\n\tif (d->phy == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < d->dma_channels; i++) {\n\t\tstruct k3_dma_phy *p;\n\n\t\tif (!(d->dma_channel_mask & BIT(i)))\n\t\t\tcontinue;\n\n\t\tp = &d->phy[i];\n\t\tp->idx = i;\n\t\tp->base = d->base + i * 0x40;\n\t}\n\n\tINIT_LIST_HEAD(&d->slave.channels);\n\tdma_cap_set(DMA_SLAVE, d->slave.cap_mask);\n\tdma_cap_set(DMA_MEMCPY, d->slave.cap_mask);\n\tdma_cap_set(DMA_CYCLIC, d->slave.cap_mask);\n\td->slave.dev = &op->dev;\n\td->slave.device_free_chan_resources = k3_dma_free_chan_resources;\n\td->slave.device_tx_status = k3_dma_tx_status;\n\td->slave.device_prep_dma_memcpy = k3_dma_prep_memcpy;\n\td->slave.device_prep_slave_sg = k3_dma_prep_slave_sg;\n\td->slave.device_prep_dma_cyclic = k3_dma_prep_dma_cyclic;\n\td->slave.device_issue_pending = k3_dma_issue_pending;\n\td->slave.device_config = k3_dma_config;\n\td->slave.device_pause = k3_dma_transfer_pause;\n\td->slave.device_resume = k3_dma_transfer_resume;\n\td->slave.device_terminate_all = k3_dma_terminate_all;\n\td->slave.device_synchronize = k3_dma_synchronize;\n\td->slave.copy_align = DMAENGINE_ALIGN_8_BYTES;\n\n\t \n\td->chans = devm_kcalloc(&op->dev,\n\t\td->dma_requests, sizeof(struct k3_dma_chan), GFP_KERNEL);\n\tif (d->chans == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < d->dma_requests; i++) {\n\t\tstruct k3_dma_chan *c = &d->chans[i];\n\n\t\tc->status = DMA_IN_PROGRESS;\n\t\tINIT_LIST_HEAD(&c->node);\n\t\tc->vc.desc_free = k3_dma_free_desc;\n\t\tvchan_init(&c->vc, &d->slave);\n\t}\n\n\t \n\tret = clk_prepare_enable(d->clk);\n\tif (ret < 0) {\n\t\tdev_err(&op->dev, \"clk_prepare_enable failed: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tk3_dma_enable_dma(d, true);\n\n\tret = dma_async_device_register(&d->slave);\n\tif (ret)\n\t\tgoto dma_async_register_fail;\n\n\tret = of_dma_controller_register((&op->dev)->of_node,\n\t\t\t\t\tk3_of_dma_simple_xlate, d);\n\tif (ret)\n\t\tgoto of_dma_register_fail;\n\n\tspin_lock_init(&d->lock);\n\tINIT_LIST_HEAD(&d->chan_pending);\n\ttasklet_setup(&d->task, k3_dma_tasklet);\n\tplatform_set_drvdata(op, d);\n\tdev_info(&op->dev, \"initialized\\n\");\n\n\treturn 0;\n\nof_dma_register_fail:\n\tdma_async_device_unregister(&d->slave);\ndma_async_register_fail:\n\tclk_disable_unprepare(d->clk);\n\treturn ret;\n}\n\nstatic int k3_dma_remove(struct platform_device *op)\n{\n\tstruct k3_dma_chan *c, *cn;\n\tstruct k3_dma_dev *d = platform_get_drvdata(op);\n\n\tdma_async_device_unregister(&d->slave);\n\tof_dma_controller_free((&op->dev)->of_node);\n\n\tdevm_free_irq(&op->dev, d->irq, d);\n\n\tlist_for_each_entry_safe(c, cn, &d->slave.channels, vc.chan.device_node) {\n\t\tlist_del(&c->vc.chan.device_node);\n\t\ttasklet_kill(&c->vc.task);\n\t}\n\ttasklet_kill(&d->task);\n\tclk_disable_unprepare(d->clk);\n\treturn 0;\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int k3_dma_suspend_dev(struct device *dev)\n{\n\tstruct k3_dma_dev *d = dev_get_drvdata(dev);\n\tu32 stat = 0;\n\n\tstat = k3_dma_get_chan_stat(d);\n\tif (stat) {\n\t\tdev_warn(d->slave.dev,\n\t\t\t\"chan %d is running fail to suspend\\n\", stat);\n\t\treturn -1;\n\t}\n\tk3_dma_enable_dma(d, false);\n\tclk_disable_unprepare(d->clk);\n\treturn 0;\n}\n\nstatic int k3_dma_resume_dev(struct device *dev)\n{\n\tstruct k3_dma_dev *d = dev_get_drvdata(dev);\n\tint ret = 0;\n\n\tret = clk_prepare_enable(d->clk);\n\tif (ret < 0) {\n\t\tdev_err(d->slave.dev, \"clk_prepare_enable failed: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\tk3_dma_enable_dma(d, true);\n\treturn 0;\n}\n#endif\n\nstatic SIMPLE_DEV_PM_OPS(k3_dma_pmops, k3_dma_suspend_dev, k3_dma_resume_dev);\n\nstatic struct platform_driver k3_pdma_driver = {\n\t.driver\t\t= {\n\t\t.name\t= DRIVER_NAME,\n\t\t.pm\t= &k3_dma_pmops,\n\t\t.of_match_table = k3_pdma_dt_ids,\n\t},\n\t.probe\t\t= k3_dma_probe,\n\t.remove\t\t= k3_dma_remove,\n};\n\nmodule_platform_driver(k3_pdma_driver);\n\nMODULE_DESCRIPTION(\"HiSilicon k3 DMA Driver\");\nMODULE_ALIAS(\"platform:k3dma\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}