{
  "module_name": "perfmon.h",
  "hash_id": "878076ca655858e90341e4c663271c30c2076e53eb829e8e6a153250358e09a8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/idxd/perfmon.h",
  "human_readable_source": " \n \n\n#ifndef _PERFMON_H_\n#define _PERFMON_H_\n\n#include <linux/slab.h>\n#include <linux/pci.h>\n#include <linux/sbitmap.h>\n#include <linux/dmaengine.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/wait.h>\n#include <linux/cdev.h>\n#include <linux/uuid.h>\n#include <linux/idxd.h>\n#include <linux/perf_event.h>\n#include \"registers.h\"\n\nstatic inline struct idxd_pmu *event_to_pmu(struct perf_event *event)\n{\n\tstruct idxd_pmu *idxd_pmu;\n\tstruct pmu *pmu;\n\n\tpmu = event->pmu;\n\tidxd_pmu = container_of(pmu, struct idxd_pmu, pmu);\n\n\treturn idxd_pmu;\n}\n\nstatic inline struct idxd_device *event_to_idxd(struct perf_event *event)\n{\n\tstruct idxd_pmu *idxd_pmu;\n\tstruct pmu *pmu;\n\n\tpmu = event->pmu;\n\tidxd_pmu = container_of(pmu, struct idxd_pmu, pmu);\n\n\treturn idxd_pmu->idxd;\n}\n\nstatic inline struct idxd_device *pmu_to_idxd(struct pmu *pmu)\n{\n\tstruct idxd_pmu *idxd_pmu;\n\n\tidxd_pmu = container_of(pmu, struct idxd_pmu, pmu);\n\n\treturn idxd_pmu->idxd;\n}\n\nenum dsa_perf_events {\n\tDSA_PERF_EVENT_WQ = 0,\n\tDSA_PERF_EVENT_ENGINE,\n\tDSA_PERF_EVENT_ADDR_TRANS,\n\tDSA_PERF_EVENT_OP,\n\tDSA_PERF_EVENT_COMPL,\n\tDSA_PERF_EVENT_MAX,\n};\n\nenum filter_enc {\n\tFLT_WQ = 0,\n\tFLT_TC,\n\tFLT_PG_SZ,\n\tFLT_XFER_SZ,\n\tFLT_ENG,\n\tFLT_MAX,\n};\n\n#define CONFIG_RESET\t\t0x0000000000000001\n#define CNTR_RESET\t\t0x0000000000000002\n#define CNTR_ENABLE\t\t0x0000000000000001\n#define INTR_OVFL\t\t0x0000000000000002\n\n#define COUNTER_FREEZE\t\t0x00000000FFFFFFFF\n#define COUNTER_UNFREEZE\t0x0000000000000000\n#define OVERFLOW_SIZE\t\t32\n\n#define CNTRCFG_ENABLE\t\tBIT(0)\n#define CNTRCFG_IRQ_OVERFLOW\tBIT(1)\n#define CNTRCFG_CATEGORY_SHIFT\t8\n#define CNTRCFG_EVENT_SHIFT\t32\n\n#define PERFMON_TABLE_OFFSET(_idxd)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\ttypeof(_idxd) __idxd = (_idxd);\t\t\t\t\\\n\t((__idxd)->reg_base + (__idxd)->perfmon_offset);\t\\\n})\n#define PERFMON_REG_OFFSET(idxd, offset)\t\t\t\\\n\t(PERFMON_TABLE_OFFSET(idxd) + (offset))\n\n#define PERFCAP_REG(idxd)\t(PERFMON_REG_OFFSET(idxd, IDXD_PERFCAP_OFFSET))\n#define PERFRST_REG(idxd)\t(PERFMON_REG_OFFSET(idxd, IDXD_PERFRST_OFFSET))\n#define OVFSTATUS_REG(idxd)\t(PERFMON_REG_OFFSET(idxd, IDXD_OVFSTATUS_OFFSET))\n#define PERFFRZ_REG(idxd)\t(PERFMON_REG_OFFSET(idxd, IDXD_PERFFRZ_OFFSET))\n\n#define FLTCFG_REG(idxd, cntr, flt)\t\t\t\t\\\n\t(PERFMON_REG_OFFSET(idxd, IDXD_FLTCFG_OFFSET) +\t((cntr) * 32) + ((flt) * 4))\n\n#define CNTRCFG_REG(idxd, cntr)\t\t\t\t\t\\\n\t(PERFMON_REG_OFFSET(idxd, IDXD_CNTRCFG_OFFSET) + ((cntr) * 8))\n#define CNTRDATA_REG(idxd, cntr)\t\t\t\t\t\\\n\t(PERFMON_REG_OFFSET(idxd, IDXD_CNTRDATA_OFFSET) + ((cntr) * 8))\n#define CNTRCAP_REG(idxd, cntr)\t\t\t\t\t\\\n\t(PERFMON_REG_OFFSET(idxd, IDXD_CNTRCAP_OFFSET) + ((cntr) * 8))\n\n#define EVNTCAP_REG(idxd, category) \\\n\t(PERFMON_REG_OFFSET(idxd, IDXD_EVNTCAP_OFFSET) + ((category) * 8))\n\n#define DEFINE_PERFMON_FORMAT_ATTR(_name, _format)\t\t\t\\\nstatic ssize_t __perfmon_idxd_##_name##_show(struct kobject *kobj,\t\\\n\t\t\t\tstruct kobj_attribute *attr,\t\t\\\n\t\t\t\tchar *page)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(_format) >= PAGE_SIZE);\t\t\t\\\n\treturn sprintf(page, _format \"\\n\");\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic struct kobj_attribute format_attr_idxd_##_name =\t\t\t\\\n\t__ATTR(_name, 0444, __perfmon_idxd_##_name##_show, NULL)\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}