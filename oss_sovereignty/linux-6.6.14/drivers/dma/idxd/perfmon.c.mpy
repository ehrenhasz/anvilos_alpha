{
  "module_name": "perfmon.c",
  "hash_id": "a70708622bdebaf4f1f667d00844b6de54dca27b198c16a04c38b2087fe22583",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/idxd/perfmon.c",
  "human_readable_source": "\n \n\n#include <linux/sched/task.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include \"idxd.h\"\n#include \"perfmon.h\"\n\nstatic ssize_t cpumask_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf);\n\nstatic cpumask_t\t\tperfmon_dsa_cpu_mask;\nstatic bool\t\t\tcpuhp_set_up;\nstatic enum cpuhp_state\t\tcpuhp_slot;\n\n \nstatic DEVICE_ATTR_RO(cpumask);\n\nstatic struct attribute *perfmon_cpumask_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL,\n};\n\nstatic struct attribute_group cpumask_attr_group = {\n\t.attrs = perfmon_cpumask_attrs,\n};\n\n \nDEFINE_PERFMON_FORMAT_ATTR(event_category, \"config:0-3\");\nDEFINE_PERFMON_FORMAT_ATTR(event, \"config:4-31\");\n\n \nDEFINE_PERFMON_FORMAT_ATTR(filter_wq, \"config1:0-31\");\nDEFINE_PERFMON_FORMAT_ATTR(filter_tc, \"config1:32-39\");\nDEFINE_PERFMON_FORMAT_ATTR(filter_pgsz, \"config1:40-43\");\nDEFINE_PERFMON_FORMAT_ATTR(filter_sz, \"config1:44-51\");\nDEFINE_PERFMON_FORMAT_ATTR(filter_eng, \"config1:52-59\");\n\n#define PERFMON_FILTERS_START\t2\n#define PERFMON_FILTERS_MAX\t5\n\nstatic struct attribute *perfmon_format_attrs[] = {\n\t&format_attr_idxd_event_category.attr,\n\t&format_attr_idxd_event.attr,\n\t&format_attr_idxd_filter_wq.attr,\n\t&format_attr_idxd_filter_tc.attr,\n\t&format_attr_idxd_filter_pgsz.attr,\n\t&format_attr_idxd_filter_sz.attr,\n\t&format_attr_idxd_filter_eng.attr,\n\tNULL,\n};\n\nstatic struct attribute_group perfmon_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = perfmon_format_attrs,\n};\n\nstatic const struct attribute_group *perfmon_attr_groups[] = {\n\t&perfmon_format_attr_group,\n\t&cpumask_attr_group,\n\tNULL,\n};\n\nstatic ssize_t cpumask_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\treturn cpumap_print_to_pagebuf(true, buf, &perfmon_dsa_cpu_mask);\n}\n\nstatic bool is_idxd_event(struct idxd_pmu *idxd_pmu, struct perf_event *event)\n{\n\treturn &idxd_pmu->pmu == event->pmu;\n}\n\nstatic int perfmon_collect_events(struct idxd_pmu *idxd_pmu,\n\t\t\t\t  struct perf_event *leader,\n\t\t\t\t  bool do_grp)\n{\n\tstruct perf_event *event;\n\tint n, max_count;\n\n\tmax_count = idxd_pmu->n_counters;\n\tn = idxd_pmu->n_events;\n\n\tif (n >= max_count)\n\t\treturn -EINVAL;\n\n\tif (is_idxd_event(idxd_pmu, leader)) {\n\t\tidxd_pmu->event_list[n] = leader;\n\t\tidxd_pmu->event_list[n]->hw.idx = n;\n\t\tn++;\n\t}\n\n\tif (!do_grp)\n\t\treturn n;\n\n\tfor_each_sibling_event(event, leader) {\n\t\tif (!is_idxd_event(idxd_pmu, event) ||\n\t\t    event->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\n\t\tif (n >= max_count)\n\t\t\treturn -EINVAL;\n\n\t\tidxd_pmu->event_list[n] = event;\n\t\tidxd_pmu->event_list[n]->hw.idx = n;\n\t\tn++;\n\t}\n\n\treturn n;\n}\n\nstatic void perfmon_assign_hw_event(struct idxd_pmu *idxd_pmu,\n\t\t\t\t    struct perf_event *event, int idx)\n{\n\tstruct idxd_device *idxd = idxd_pmu->idxd;\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thwc->idx = idx;\n\thwc->config_base = ioread64(CNTRCFG_REG(idxd, idx));\n\thwc->event_base = ioread64(CNTRCFG_REG(idxd, idx));\n}\n\nstatic int perfmon_assign_event(struct idxd_pmu *idxd_pmu,\n\t\t\t\tstruct perf_event *event)\n{\n\tint i;\n\n\tfor (i = 0; i < IDXD_PMU_EVENT_MAX; i++)\n\t\tif (!test_and_set_bit(i, idxd_pmu->used_mask))\n\t\t\treturn i;\n\n\treturn -EINVAL;\n}\n\n \nstatic int perfmon_validate_group(struct idxd_pmu *pmu,\n\t\t\t\t  struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct idxd_pmu *fake_pmu;\n\tint i, ret = 0, n, idx;\n\n\tfake_pmu = kzalloc(sizeof(*fake_pmu), GFP_KERNEL);\n\tif (!fake_pmu)\n\t\treturn -ENOMEM;\n\n\tfake_pmu->pmu.name = pmu->pmu.name;\n\tfake_pmu->n_counters = pmu->n_counters;\n\n\tn = perfmon_collect_events(fake_pmu, leader, true);\n\tif (n < 0) {\n\t\tret = n;\n\t\tgoto out;\n\t}\n\n\tfake_pmu->n_events = n;\n\tn = perfmon_collect_events(fake_pmu, event, false);\n\tif (n < 0) {\n\t\tret = n;\n\t\tgoto out;\n\t}\n\n\tfake_pmu->n_events = n;\n\n\tfor (i = 0; i < n; i++) {\n\t\tevent = fake_pmu->event_list[i];\n\n\t\tidx = perfmon_assign_event(fake_pmu, event);\n\t\tif (idx < 0) {\n\t\t\tret = idx;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tkfree(fake_pmu);\n\n\treturn ret;\n}\n\nstatic int perfmon_pmu_event_init(struct perf_event *event)\n{\n\tstruct idxd_device *idxd;\n\tint ret = 0;\n\n\tidxd = event_to_idxd(event);\n\tevent->hw.idx = -1;\n\n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\t \n\tif (event->attr.sample_period)\n\t\treturn -EINVAL;\n\n\tif (event->cpu < 0)\n\t\treturn -EINVAL;\n\n\tif (event->pmu != &idxd->idxd_pmu->pmu)\n\t\treturn -EINVAL;\n\n\tevent->hw.event_base = ioread64(PERFMON_TABLE_OFFSET(idxd));\n\tevent->cpu = idxd->idxd_pmu->cpu;\n\tevent->hw.config = event->attr.config;\n\n\tif (event->group_leader != event)\n\t\t  \n\t\tret = perfmon_validate_group(idxd->idxd_pmu, event);\n\n\treturn ret;\n}\n\nstatic inline u64 perfmon_pmu_read_counter(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct idxd_device *idxd;\n\tint cntr = hwc->idx;\n\n\tidxd = event_to_idxd(event);\n\n\treturn ioread64(CNTRDATA_REG(idxd, cntr));\n}\n\nstatic void perfmon_pmu_event_update(struct perf_event *event)\n{\n\tstruct idxd_device *idxd = event_to_idxd(event);\n\tu64 prev_raw_count, new_raw_count, delta, p, n;\n\tint shift = 64 - idxd->idxd_pmu->counter_width;\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tdo {\n\t\tnew_raw_count = perfmon_pmu_read_counter(event);\n\t} while (!local64_try_cmpxchg(&hwc->prev_count,\n\t\t\t\t      &prev_raw_count, new_raw_count));\n\tn = (new_raw_count << shift);\n\tp = (prev_raw_count << shift);\n\n\tdelta = ((n - p) >> shift);\n\n\tlocal64_add(delta, &event->count);\n}\n\nvoid perfmon_counter_overflow(struct idxd_device *idxd)\n{\n\tint i, n_counters, max_loop = OVERFLOW_SIZE;\n\tstruct perf_event *event;\n\tunsigned long ovfstatus;\n\n\tn_counters = min(idxd->idxd_pmu->n_counters, OVERFLOW_SIZE);\n\n\tovfstatus = ioread32(OVFSTATUS_REG(idxd));\n\n\t \n\twhile (ovfstatus && max_loop--) {\n\t\t \n\t\tfor_each_set_bit(i, &ovfstatus, n_counters) {\n\t\t\tunsigned long ovfstatus_clear = 0;\n\n\t\t\t \n\t\t\tevent = idxd->idxd_pmu->event_list[i];\n\t\t\tperfmon_pmu_event_update(event);\n\t\t\t \n\t\t\tset_bit(i, &ovfstatus_clear);\n\t\t\tiowrite32(ovfstatus_clear, OVFSTATUS_REG(idxd));\n\t\t}\n\n\t\tovfstatus = ioread32(OVFSTATUS_REG(idxd));\n\t}\n\n\t \n\tWARN_ON_ONCE(ovfstatus);\n}\n\nstatic inline void perfmon_reset_config(struct idxd_device *idxd)\n{\n\tiowrite32(CONFIG_RESET, PERFRST_REG(idxd));\n\tiowrite32(0, OVFSTATUS_REG(idxd));\n\tiowrite32(0, PERFFRZ_REG(idxd));\n}\n\nstatic inline void perfmon_reset_counters(struct idxd_device *idxd)\n{\n\tiowrite32(CNTR_RESET, PERFRST_REG(idxd));\n}\n\nstatic inline void perfmon_reset(struct idxd_device *idxd)\n{\n\tperfmon_reset_config(idxd);\n\tperfmon_reset_counters(idxd);\n}\n\nstatic void perfmon_pmu_event_start(struct perf_event *event, int mode)\n{\n\tu32 flt_wq, flt_tc, flt_pg_sz, flt_xfer_sz, flt_eng = 0;\n\tu64 cntr_cfg, cntrdata, event_enc, event_cat = 0;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunion filter_cfg flt_cfg;\n\tunion event_cfg event_cfg;\n\tstruct idxd_device *idxd;\n\tint cntr;\n\n\tidxd = event_to_idxd(event);\n\n\tevent->hw.idx = hwc->idx;\n\tcntr = hwc->idx;\n\n\t \n\tevent_cfg.val = event->attr.config;\n\tflt_cfg.val = event->attr.config1;\n\tevent_cat = event_cfg.event_cat;\n\tevent_enc = event_cfg.event_enc;\n\n\t \n\tflt_wq = flt_cfg.wq;\n\tflt_tc = flt_cfg.tc;\n\tflt_pg_sz = flt_cfg.pg_sz;\n\tflt_xfer_sz = flt_cfg.xfer_sz;\n\tflt_eng = flt_cfg.eng;\n\n\tif (flt_wq && test_bit(FLT_WQ, &idxd->idxd_pmu->supported_filters))\n\t\tiowrite32(flt_wq, FLTCFG_REG(idxd, cntr, FLT_WQ));\n\tif (flt_tc && test_bit(FLT_TC, &idxd->idxd_pmu->supported_filters))\n\t\tiowrite32(flt_tc, FLTCFG_REG(idxd, cntr, FLT_TC));\n\tif (flt_pg_sz && test_bit(FLT_PG_SZ, &idxd->idxd_pmu->supported_filters))\n\t\tiowrite32(flt_pg_sz, FLTCFG_REG(idxd, cntr, FLT_PG_SZ));\n\tif (flt_xfer_sz && test_bit(FLT_XFER_SZ, &idxd->idxd_pmu->supported_filters))\n\t\tiowrite32(flt_xfer_sz, FLTCFG_REG(idxd, cntr, FLT_XFER_SZ));\n\tif (flt_eng && test_bit(FLT_ENG, &idxd->idxd_pmu->supported_filters))\n\t\tiowrite32(flt_eng, FLTCFG_REG(idxd, cntr, FLT_ENG));\n\n\t \n\tcntrdata = ioread64(CNTRDATA_REG(idxd, cntr));\n\tlocal64_set(&event->hw.prev_count, cntrdata);\n\n\t \n\tcntr_cfg = event_cat << CNTRCFG_CATEGORY_SHIFT;\n\tcntr_cfg |= event_enc << CNTRCFG_EVENT_SHIFT;\n\t \n\tcntr_cfg |= (CNTRCFG_IRQ_OVERFLOW | CNTRCFG_ENABLE);\n\n\tiowrite64(cntr_cfg, CNTRCFG_REG(idxd, cntr));\n}\n\nstatic void perfmon_pmu_event_stop(struct perf_event *event, int mode)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct idxd_device *idxd;\n\tint i, cntr = hwc->idx;\n\tu64 cntr_cfg;\n\n\tidxd = event_to_idxd(event);\n\n\t \n\tfor (i = 0; i < idxd->idxd_pmu->n_events; i++) {\n\t\tif (event != idxd->idxd_pmu->event_list[i])\n\t\t\tcontinue;\n\n\t\tfor (++i; i < idxd->idxd_pmu->n_events; i++)\n\t\t\tidxd->idxd_pmu->event_list[i - 1] = idxd->idxd_pmu->event_list[i];\n\t\t--idxd->idxd_pmu->n_events;\n\t\tbreak;\n\t}\n\n\tcntr_cfg = ioread64(CNTRCFG_REG(idxd, cntr));\n\tcntr_cfg &= ~CNTRCFG_ENABLE;\n\tiowrite64(cntr_cfg, CNTRCFG_REG(idxd, cntr));\n\n\tif (mode == PERF_EF_UPDATE)\n\t\tperfmon_pmu_event_update(event);\n\n\tevent->hw.idx = -1;\n\tclear_bit(cntr, idxd->idxd_pmu->used_mask);\n}\n\nstatic void perfmon_pmu_event_del(struct perf_event *event, int mode)\n{\n\tperfmon_pmu_event_stop(event, PERF_EF_UPDATE);\n}\n\nstatic int perfmon_pmu_event_add(struct perf_event *event, int flags)\n{\n\tstruct idxd_device *idxd = event_to_idxd(event);\n\tstruct idxd_pmu *idxd_pmu = idxd->idxd_pmu;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx, n;\n\n\tn = perfmon_collect_events(idxd_pmu, event, false);\n\tif (n < 0)\n\t\treturn n;\n\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\tif (!(flags & PERF_EF_START))\n\t\thwc->state |= PERF_HES_ARCH;\n\n\tidx = perfmon_assign_event(idxd_pmu, event);\n\tif (idx < 0)\n\t\treturn idx;\n\n\tperfmon_assign_hw_event(idxd_pmu, event, idx);\n\n\tif (flags & PERF_EF_START)\n\t\tperfmon_pmu_event_start(event, 0);\n\n\tidxd_pmu->n_events = n;\n\n\treturn 0;\n}\n\nstatic void enable_perfmon_pmu(struct idxd_device *idxd)\n{\n\tiowrite32(COUNTER_UNFREEZE, PERFFRZ_REG(idxd));\n}\n\nstatic void disable_perfmon_pmu(struct idxd_device *idxd)\n{\n\tiowrite32(COUNTER_FREEZE, PERFFRZ_REG(idxd));\n}\n\nstatic void perfmon_pmu_enable(struct pmu *pmu)\n{\n\tstruct idxd_device *idxd = pmu_to_idxd(pmu);\n\n\tenable_perfmon_pmu(idxd);\n}\n\nstatic void perfmon_pmu_disable(struct pmu *pmu)\n{\n\tstruct idxd_device *idxd = pmu_to_idxd(pmu);\n\n\tdisable_perfmon_pmu(idxd);\n}\n\nstatic void skip_filter(int i)\n{\n\tint j;\n\n\tfor (j = i; j < PERFMON_FILTERS_MAX; j++)\n\t\tperfmon_format_attrs[PERFMON_FILTERS_START + j] =\n\t\t\tperfmon_format_attrs[PERFMON_FILTERS_START + j + 1];\n}\n\nstatic void idxd_pmu_init(struct idxd_pmu *idxd_pmu)\n{\n\tint i;\n\n\tfor (i = 0 ; i < PERFMON_FILTERS_MAX; i++) {\n\t\tif (!test_bit(i, &idxd_pmu->supported_filters))\n\t\t\tskip_filter(i);\n\t}\n\n\tidxd_pmu->pmu.name\t\t= idxd_pmu->name;\n\tidxd_pmu->pmu.attr_groups\t= perfmon_attr_groups;\n\tidxd_pmu->pmu.task_ctx_nr\t= perf_invalid_context;\n\tidxd_pmu->pmu.event_init\t= perfmon_pmu_event_init;\n\tidxd_pmu->pmu.pmu_enable\t= perfmon_pmu_enable,\n\tidxd_pmu->pmu.pmu_disable\t= perfmon_pmu_disable,\n\tidxd_pmu->pmu.add\t\t= perfmon_pmu_event_add;\n\tidxd_pmu->pmu.del\t\t= perfmon_pmu_event_del;\n\tidxd_pmu->pmu.start\t\t= perfmon_pmu_event_start;\n\tidxd_pmu->pmu.stop\t\t= perfmon_pmu_event_stop;\n\tidxd_pmu->pmu.read\t\t= perfmon_pmu_event_update;\n\tidxd_pmu->pmu.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE;\n\tidxd_pmu->pmu.module\t\t= THIS_MODULE;\n}\n\nvoid perfmon_pmu_remove(struct idxd_device *idxd)\n{\n\tif (!idxd->idxd_pmu)\n\t\treturn;\n\n\tcpuhp_state_remove_instance(cpuhp_slot, &idxd->idxd_pmu->cpuhp_node);\n\tperf_pmu_unregister(&idxd->idxd_pmu->pmu);\n\tkfree(idxd->idxd_pmu);\n\tidxd->idxd_pmu = NULL;\n}\n\nstatic int perf_event_cpu_online(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct idxd_pmu *idxd_pmu;\n\n\tidxd_pmu = hlist_entry_safe(node, typeof(*idxd_pmu), cpuhp_node);\n\n\t \n\tif (cpumask_empty(&perfmon_dsa_cpu_mask)) {\n\t\tcpumask_set_cpu(cpu, &perfmon_dsa_cpu_mask);\n\t\tidxd_pmu->cpu = cpu;\n\t}\n\n\treturn 0;\n}\n\nstatic int perf_event_cpu_offline(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct idxd_pmu *idxd_pmu;\n\tunsigned int target;\n\n\tidxd_pmu = hlist_entry_safe(node, typeof(*idxd_pmu), cpuhp_node);\n\n\tif (!cpumask_test_and_clear_cpu(cpu, &perfmon_dsa_cpu_mask))\n\t\treturn 0;\n\n\ttarget = cpumask_any_but(cpu_online_mask, cpu);\n\n\t \n\tif (target < nr_cpu_ids)\n\t\tcpumask_set_cpu(target, &perfmon_dsa_cpu_mask);\n\telse\n\t\ttarget = -1;\n\n\tperf_pmu_migrate_context(&idxd_pmu->pmu, cpu, target);\n\n\treturn 0;\n}\n\nint perfmon_pmu_init(struct idxd_device *idxd)\n{\n\tunion idxd_perfcap perfcap;\n\tstruct idxd_pmu *idxd_pmu;\n\tint rc = -ENODEV;\n\n\t \n\tif (!cpuhp_set_up)\n\t\treturn -ENODEV;\n\n\t \n\tif (idxd->perfmon_offset == 0)\n\t\treturn -ENODEV;\n\n\tidxd_pmu = kzalloc(sizeof(*idxd_pmu), GFP_KERNEL);\n\tif (!idxd_pmu)\n\t\treturn -ENOMEM;\n\n\tidxd_pmu->idxd = idxd;\n\tidxd->idxd_pmu = idxd_pmu;\n\n\tif (idxd->data->type == IDXD_TYPE_DSA) {\n\t\trc = sprintf(idxd_pmu->name, \"dsa%d\", idxd->id);\n\t\tif (rc < 0)\n\t\t\tgoto free;\n\t} else if (idxd->data->type == IDXD_TYPE_IAX) {\n\t\trc = sprintf(idxd_pmu->name, \"iax%d\", idxd->id);\n\t\tif (rc < 0)\n\t\t\tgoto free;\n\t} else {\n\t\tgoto free;\n\t}\n\n\tperfmon_reset(idxd);\n\n\tperfcap.bits = ioread64(PERFCAP_REG(idxd));\n\n\t \n\tif (perfcap.num_perf_counter == 0)\n\t\tgoto free;\n\n\t \n\tif (perfcap.counter_width == 0)\n\t\tgoto free;\n\n\t \n\tif (!perfcap.overflow_interrupt || !perfcap.counter_freeze)\n\t\tgoto free;\n\n\t \n\tif (perfcap.num_event_category == 0)\n\t\tgoto free;\n\n\t \n\tif (perfcap.cap_per_counter)\n\t\tgoto free;\n\n\tidxd_pmu->n_event_categories = perfcap.num_event_category;\n\tidxd_pmu->supported_event_categories = perfcap.global_event_category;\n\tidxd_pmu->per_counter_caps_supported = perfcap.cap_per_counter;\n\n\t \n\tidxd_pmu->supported_filters = perfcap.filter;\n\tif (perfcap.filter)\n\t\tidxd_pmu->n_filters = hweight8(perfcap.filter);\n\n\t \n\tidxd_pmu->n_counters = perfcap.num_perf_counter;\n\tidxd_pmu->counter_width = perfcap.counter_width;\n\n\tidxd_pmu_init(idxd_pmu);\n\n\trc = perf_pmu_register(&idxd_pmu->pmu, idxd_pmu->name, -1);\n\tif (rc)\n\t\tgoto free;\n\n\trc = cpuhp_state_add_instance(cpuhp_slot, &idxd_pmu->cpuhp_node);\n\tif (rc) {\n\t\tperf_pmu_unregister(&idxd->idxd_pmu->pmu);\n\t\tgoto free;\n\t}\nout:\n\treturn rc;\nfree:\n\tkfree(idxd_pmu);\n\tidxd->idxd_pmu = NULL;\n\n\tgoto out;\n}\n\nvoid __init perfmon_init(void)\n{\n\tint rc = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t\t \"driver/dma/idxd/perf:online\",\n\t\t\t\t\t perf_event_cpu_online,\n\t\t\t\t\t perf_event_cpu_offline);\n\tif (WARN_ON(rc < 0))\n\t\treturn;\n\n\tcpuhp_slot = rc;\n\tcpuhp_set_up = true;\n}\n\nvoid __exit perfmon_exit(void)\n{\n\tif (cpuhp_set_up)\n\t\tcpuhp_remove_multi_state(cpuhp_slot);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}