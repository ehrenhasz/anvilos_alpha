{
  "module_name": "idxd.h",
  "hash_id": "2986e19d8b94465190f8e8de2f9cb31a59427536316b1e696a166774eebe2543",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/idxd/idxd.h",
  "human_readable_source": " \n \n#ifndef _IDXD_H_\n#define _IDXD_H_\n\n#include <linux/sbitmap.h>\n#include <linux/dmaengine.h>\n#include <linux/percpu-rwsem.h>\n#include <linux/wait.h>\n#include <linux/cdev.h>\n#include <linux/idr.h>\n#include <linux/pci.h>\n#include <linux/bitmap.h>\n#include <linux/perf_event.h>\n#include <linux/iommu.h>\n#include <uapi/linux/idxd.h>\n#include \"registers.h\"\n\n#define IDXD_DRIVER_VERSION\t\"1.00\"\n\nextern struct kmem_cache *idxd_desc_pool;\nextern bool tc_override;\n\nstruct idxd_wq;\nstruct idxd_dev;\n\nenum idxd_dev_type {\n\tIDXD_DEV_NONE = -1,\n\tIDXD_DEV_DSA = 0,\n\tIDXD_DEV_IAX,\n\tIDXD_DEV_WQ,\n\tIDXD_DEV_GROUP,\n\tIDXD_DEV_ENGINE,\n\tIDXD_DEV_CDEV,\n\tIDXD_DEV_CDEV_FILE,\n\tIDXD_DEV_MAX_TYPE,\n};\n\nstruct idxd_dev {\n\tstruct device conf_dev;\n\tenum idxd_dev_type type;\n};\n\n#define IDXD_REG_TIMEOUT\t50\n#define IDXD_DRAIN_TIMEOUT\t5000\n\nenum idxd_type {\n\tIDXD_TYPE_UNKNOWN = -1,\n\tIDXD_TYPE_DSA = 0,\n\tIDXD_TYPE_IAX,\n\tIDXD_TYPE_MAX,\n};\n\n#define IDXD_NAME_SIZE\t\t128\n#define IDXD_PMU_EVENT_MAX\t64\n\n#define IDXD_ENQCMDS_RETRIES\t\t32\n#define IDXD_ENQCMDS_MAX_RETRIES\t64\n\nstruct idxd_device_driver {\n\tconst char *name;\n\tenum idxd_dev_type *type;\n\tint (*probe)(struct idxd_dev *idxd_dev);\n\tvoid (*remove)(struct idxd_dev *idxd_dev);\n\tstruct device_driver drv;\n};\n\nextern struct idxd_device_driver dsa_drv;\nextern struct idxd_device_driver idxd_drv;\nextern struct idxd_device_driver idxd_dmaengine_drv;\nextern struct idxd_device_driver idxd_user_drv;\n\n#define INVALID_INT_HANDLE\t-1\nstruct idxd_irq_entry {\n\tint id;\n\tint vector;\n\tstruct llist_head pending_llist;\n\tstruct list_head work_list;\n\t \n\tspinlock_t list_lock;\n\tint int_handle;\n\tioasid_t pasid;\n};\n\nstruct idxd_group {\n\tstruct idxd_dev idxd_dev;\n\tstruct idxd_device *idxd;\n\tstruct grpcfg grpcfg;\n\tint id;\n\tint num_engines;\n\tint num_wqs;\n\tbool use_rdbuf_limit;\n\tu8 rdbufs_allowed;\n\tu8 rdbufs_reserved;\n\tint tc_a;\n\tint tc_b;\n\tint desc_progress_limit;\n\tint batch_progress_limit;\n};\n\nstruct idxd_pmu {\n\tstruct idxd_device *idxd;\n\n\tstruct perf_event *event_list[IDXD_PMU_EVENT_MAX];\n\tint n_events;\n\n\tDECLARE_BITMAP(used_mask, IDXD_PMU_EVENT_MAX);\n\n\tstruct pmu pmu;\n\tchar name[IDXD_NAME_SIZE];\n\tint cpu;\n\n\tint n_counters;\n\tint counter_width;\n\tint n_event_categories;\n\n\tbool per_counter_caps_supported;\n\tunsigned long supported_event_categories;\n\n\tunsigned long supported_filters;\n\tint n_filters;\n\n\tstruct hlist_node cpuhp_node;\n};\n\n#define IDXD_MAX_PRIORITY\t0xf\n\nenum {\n\tCOUNTER_FAULTS = 0,\n\tCOUNTER_FAULT_FAILS,\n\tCOUNTER_MAX\n};\n\nenum idxd_wq_state {\n\tIDXD_WQ_DISABLED = 0,\n\tIDXD_WQ_ENABLED,\n};\n\nenum idxd_wq_flag {\n\tWQ_FLAG_DEDICATED = 0,\n\tWQ_FLAG_BLOCK_ON_FAULT,\n\tWQ_FLAG_ATS_DISABLE,\n\tWQ_FLAG_PRS_DISABLE,\n};\n\nenum idxd_wq_type {\n\tIDXD_WQT_NONE = 0,\n\tIDXD_WQT_KERNEL,\n\tIDXD_WQT_USER,\n};\n\nstruct idxd_cdev {\n\tstruct idxd_wq *wq;\n\tstruct cdev cdev;\n\tstruct idxd_dev idxd_dev;\n\tint minor;\n};\n\n#define IDXD_ALLOCATED_BATCH_SIZE\t128U\n#define WQ_NAME_SIZE   1024\n#define WQ_TYPE_SIZE   10\n\n#define WQ_DEFAULT_QUEUE_DEPTH\t\t16\n#define WQ_DEFAULT_MAX_XFER\t\tSZ_2M\n#define WQ_DEFAULT_MAX_BATCH\t\t32\n\nenum idxd_op_type {\n\tIDXD_OP_BLOCK = 0,\n\tIDXD_OP_NONBLOCK = 1,\n};\n\nenum idxd_complete_type {\n\tIDXD_COMPLETE_NORMAL = 0,\n\tIDXD_COMPLETE_ABORT,\n\tIDXD_COMPLETE_DEV_FAIL,\n};\n\nstruct idxd_dma_chan {\n\tstruct dma_chan chan;\n\tstruct idxd_wq *wq;\n};\n\nstruct idxd_wq {\n\tvoid __iomem *portal;\n\tu32 portal_offset;\n\tunsigned int enqcmds_retries;\n\tstruct percpu_ref wq_active;\n\tstruct completion wq_dead;\n\tstruct completion wq_resurrect;\n\tstruct idxd_dev idxd_dev;\n\tstruct idxd_cdev *idxd_cdev;\n\tstruct wait_queue_head err_queue;\n\tstruct workqueue_struct *wq;\n\tstruct idxd_device *idxd;\n\tint id;\n\tstruct idxd_irq_entry ie;\n\tenum idxd_wq_type type;\n\tstruct idxd_group *group;\n\tint client_count;\n\tstruct mutex wq_lock;\t \n\tu32 size;\n\tu32 threshold;\n\tu32 priority;\n\tenum idxd_wq_state state;\n\tunsigned long flags;\n\tunion wqcfg *wqcfg;\n\tunsigned long *opcap_bmap;\n\n\tstruct dsa_hw_desc **hw_descs;\n\tint num_descs;\n\tunion {\n\t\tstruct dsa_completion_record *compls;\n\t\tstruct iax_completion_record *iax_compls;\n\t};\n\tdma_addr_t compls_addr;\n\tint compls_size;\n\tstruct idxd_desc **descs;\n\tstruct sbitmap_queue sbq;\n\tstruct idxd_dma_chan *idxd_chan;\n\tchar name[WQ_NAME_SIZE + 1];\n\tu64 max_xfer_bytes;\n\tu32 max_batch_size;\n\n\t \n\tstruct mutex uc_lock;\n\tstruct xarray upasid_xa;\n};\n\nstruct idxd_engine {\n\tstruct idxd_dev idxd_dev;\n\tint id;\n\tstruct idxd_group *group;\n\tstruct idxd_device *idxd;\n};\n\n \nstruct idxd_hw {\n\tu32 version;\n\tunion gen_cap_reg gen_cap;\n\tunion wq_cap_reg wq_cap;\n\tunion group_cap_reg group_cap;\n\tunion engine_cap_reg engine_cap;\n\tstruct opcap opcap;\n\tu32 cmd_cap;\n\tunion iaa_cap_reg iaa_cap;\n};\n\nenum idxd_device_state {\n\tIDXD_DEV_HALTED = -1,\n\tIDXD_DEV_DISABLED = 0,\n\tIDXD_DEV_ENABLED,\n};\n\nenum idxd_device_flag {\n\tIDXD_FLAG_CONFIGURABLE = 0,\n\tIDXD_FLAG_CMD_RUNNING,\n\tIDXD_FLAG_PASID_ENABLED,\n\tIDXD_FLAG_USER_PASID_ENABLED,\n};\n\nstruct idxd_dma_dev {\n\tstruct idxd_device *idxd;\n\tstruct dma_device dma;\n};\n\nstruct idxd_driver_data {\n\tconst char *name_prefix;\n\tenum idxd_type type;\n\tstruct device_type *dev_type;\n\tint compl_size;\n\tint align;\n\tint evl_cr_off;\n\tint cr_status_off;\n\tint cr_result_off;\n};\n\nstruct idxd_evl {\n\t \n\tspinlock_t lock;\n\tvoid *log;\n\tdma_addr_t dma;\n\t \n\tunsigned int log_size;\n\t \n\tu16 size;\n\tu16 head;\n\tunsigned long *bmap;\n\tbool batch_fail[IDXD_MAX_BATCH_IDENT];\n};\n\nstruct idxd_evl_fault {\n\tstruct work_struct work;\n\tstruct idxd_wq *wq;\n\tu8 status;\n\n\t \n\tstruct __evl_entry entry[];\n};\n\nstruct idxd_device {\n\tstruct idxd_dev idxd_dev;\n\tstruct idxd_driver_data *data;\n\tstruct list_head list;\n\tstruct idxd_hw hw;\n\tenum idxd_device_state state;\n\tunsigned long flags;\n\tint id;\n\tint major;\n\tu32 cmd_status;\n\tstruct idxd_irq_entry ie;\t \n\n\tstruct pci_dev *pdev;\n\tvoid __iomem *reg_base;\n\n\tspinlock_t dev_lock;\t \n\tspinlock_t cmd_lock;\t \n\tstruct completion *cmd_done;\n\tstruct idxd_group **groups;\n\tstruct idxd_wq **wqs;\n\tstruct idxd_engine **engines;\n\n\tstruct iommu_sva *sva;\n\tunsigned int pasid;\n\n\tint num_groups;\n\tint irq_cnt;\n\tbool request_int_handles;\n\n\tu32 msix_perm_offset;\n\tu32 wqcfg_offset;\n\tu32 grpcfg_offset;\n\tu32 perfmon_offset;\n\n\tu64 max_xfer_bytes;\n\tu32 max_batch_size;\n\tint max_groups;\n\tint max_engines;\n\tint max_rdbufs;\n\tint max_wqs;\n\tint max_wq_size;\n\tint rdbuf_limit;\n\tint nr_rdbufs;\t\t \n\tunsigned int wqcfg_size;\n\tunsigned long *wq_enable_map;\n\n\tunion sw_err_reg sw_err;\n\twait_queue_head_t cmd_waitq;\n\n\tstruct idxd_dma_dev *idxd_dma;\n\tstruct workqueue_struct *wq;\n\tstruct work_struct work;\n\n\tstruct idxd_pmu *idxd_pmu;\n\n\tunsigned long *opcap_bmap;\n\tstruct idxd_evl *evl;\n\tstruct kmem_cache *evl_cache;\n\n\tstruct dentry *dbgfs_dir;\n\tstruct dentry *dbgfs_evl_file;\n};\n\nstatic inline unsigned int evl_ent_size(struct idxd_device *idxd)\n{\n\treturn idxd->hw.gen_cap.evl_support ?\n\t       (32 * (1 << idxd->hw.gen_cap.evl_support)) : 0;\n}\n\nstatic inline unsigned int evl_size(struct idxd_device *idxd)\n{\n\treturn idxd->evl->size * evl_ent_size(idxd);\n}\n\n \nstruct idxd_desc {\n\tunion {\n\t\tstruct dsa_hw_desc *hw;\n\t\tstruct iax_hw_desc *iax_hw;\n\t};\n\tdma_addr_t desc_dma;\n\tunion {\n\t\tstruct dsa_completion_record *completion;\n\t\tstruct iax_completion_record *iax_completion;\n\t};\n\tdma_addr_t compl_dma;\n\tstruct dma_async_tx_descriptor txd;\n\tstruct llist_node llnode;\n\tstruct list_head list;\n\tint id;\n\tint cpu;\n\tstruct idxd_wq *wq;\n};\n\n \nenum idxd_completion_status {\n\tIDXD_COMP_DESC_ABORT = 0xff,\n};\n\n#define idxd_confdev(idxd) &idxd->idxd_dev.conf_dev\n#define wq_confdev(wq) &wq->idxd_dev.conf_dev\n#define engine_confdev(engine) &engine->idxd_dev.conf_dev\n#define group_confdev(group) &group->idxd_dev.conf_dev\n#define cdev_dev(cdev) &cdev->idxd_dev.conf_dev\n#define user_ctx_dev(ctx) (&(ctx)->idxd_dev.conf_dev)\n\n#define confdev_to_idxd_dev(dev) container_of(dev, struct idxd_dev, conf_dev)\n#define idxd_dev_to_idxd(idxd_dev) container_of(idxd_dev, struct idxd_device, idxd_dev)\n#define idxd_dev_to_wq(idxd_dev) container_of(idxd_dev, struct idxd_wq, idxd_dev)\n\nstatic inline struct idxd_device *confdev_to_idxd(struct device *dev)\n{\n\tstruct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);\n\n\treturn idxd_dev_to_idxd(idxd_dev);\n}\n\nstatic inline struct idxd_wq *confdev_to_wq(struct device *dev)\n{\n\tstruct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);\n\n\treturn idxd_dev_to_wq(idxd_dev);\n}\n\nstatic inline struct idxd_engine *confdev_to_engine(struct device *dev)\n{\n\tstruct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);\n\n\treturn container_of(idxd_dev, struct idxd_engine, idxd_dev);\n}\n\nstatic inline struct idxd_group *confdev_to_group(struct device *dev)\n{\n\tstruct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);\n\n\treturn container_of(idxd_dev, struct idxd_group, idxd_dev);\n}\n\nstatic inline struct idxd_cdev *dev_to_cdev(struct device *dev)\n{\n\tstruct idxd_dev *idxd_dev = confdev_to_idxd_dev(dev);\n\n\treturn container_of(idxd_dev, struct idxd_cdev, idxd_dev);\n}\n\nstatic inline void idxd_dev_set_type(struct idxd_dev *idev, int type)\n{\n\tif (type >= IDXD_DEV_MAX_TYPE) {\n\t\tidev->type = IDXD_DEV_NONE;\n\t\treturn;\n\t}\n\n\tidev->type = type;\n}\n\nstatic inline struct idxd_irq_entry *idxd_get_ie(struct idxd_device *idxd, int idx)\n{\n\treturn (idx == 0) ? &idxd->ie : &idxd->wqs[idx - 1]->ie;\n}\n\nstatic inline struct idxd_wq *ie_to_wq(struct idxd_irq_entry *ie)\n{\n\treturn container_of(ie, struct idxd_wq, ie);\n}\n\nstatic inline struct idxd_device *ie_to_idxd(struct idxd_irq_entry *ie)\n{\n\treturn container_of(ie, struct idxd_device, ie);\n}\n\nstatic inline void idxd_set_user_intr(struct idxd_device *idxd, bool enable)\n{\n\tunion gencfg_reg reg;\n\n\treg.bits = ioread32(idxd->reg_base + IDXD_GENCFG_OFFSET);\n\treg.user_int_en = enable;\n\tiowrite32(reg.bits, idxd->reg_base + IDXD_GENCFG_OFFSET);\n}\n\nextern struct bus_type dsa_bus_type;\n\nextern bool support_enqcmd;\nextern struct ida idxd_ida;\nextern struct device_type dsa_device_type;\nextern struct device_type iax_device_type;\nextern struct device_type idxd_wq_device_type;\nextern struct device_type idxd_engine_device_type;\nextern struct device_type idxd_group_device_type;\n\nstatic inline bool is_dsa_dev(struct idxd_dev *idxd_dev)\n{\n\treturn idxd_dev->type == IDXD_DEV_DSA;\n}\n\nstatic inline bool is_iax_dev(struct idxd_dev *idxd_dev)\n{\n\treturn idxd_dev->type == IDXD_DEV_IAX;\n}\n\nstatic inline bool is_idxd_dev(struct idxd_dev *idxd_dev)\n{\n\treturn is_dsa_dev(idxd_dev) || is_iax_dev(idxd_dev);\n}\n\nstatic inline bool is_idxd_wq_dev(struct idxd_dev *idxd_dev)\n{\n\treturn idxd_dev->type == IDXD_DEV_WQ;\n}\n\nstatic inline bool is_idxd_wq_dmaengine(struct idxd_wq *wq)\n{\n\tif (wq->type == IDXD_WQT_KERNEL && strcmp(wq->name, \"dmaengine\") == 0)\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline bool is_idxd_wq_user(struct idxd_wq *wq)\n{\n\treturn wq->type == IDXD_WQT_USER;\n}\n\nstatic inline bool is_idxd_wq_kernel(struct idxd_wq *wq)\n{\n\treturn wq->type == IDXD_WQT_KERNEL;\n}\n\nstatic inline bool wq_dedicated(struct idxd_wq *wq)\n{\n\treturn test_bit(WQ_FLAG_DEDICATED, &wq->flags);\n}\n\nstatic inline bool wq_shared(struct idxd_wq *wq)\n{\n\treturn !test_bit(WQ_FLAG_DEDICATED, &wq->flags);\n}\n\nstatic inline bool device_pasid_enabled(struct idxd_device *idxd)\n{\n\treturn test_bit(IDXD_FLAG_PASID_ENABLED, &idxd->flags);\n}\n\nstatic inline bool device_user_pasid_enabled(struct idxd_device *idxd)\n{\n\treturn test_bit(IDXD_FLAG_USER_PASID_ENABLED, &idxd->flags);\n}\n\nstatic inline bool wq_pasid_enabled(struct idxd_wq *wq)\n{\n\treturn (is_idxd_wq_kernel(wq) && device_pasid_enabled(wq->idxd)) ||\n\t       (is_idxd_wq_user(wq) && device_user_pasid_enabled(wq->idxd));\n}\n\nstatic inline bool wq_shared_supported(struct idxd_wq *wq)\n{\n\treturn (support_enqcmd && wq_pasid_enabled(wq));\n}\n\nenum idxd_portal_prot {\n\tIDXD_PORTAL_UNLIMITED = 0,\n\tIDXD_PORTAL_LIMITED,\n};\n\nenum idxd_interrupt_type {\n\tIDXD_IRQ_MSIX = 0,\n\tIDXD_IRQ_IMS,\n};\n\nstatic inline int idxd_get_wq_portal_offset(enum idxd_portal_prot prot)\n{\n\treturn prot * 0x1000;\n}\n\nstatic inline int idxd_get_wq_portal_full_offset(int wq_id,\n\t\t\t\t\t\t enum idxd_portal_prot prot)\n{\n\treturn ((wq_id * 4) << PAGE_SHIFT) + idxd_get_wq_portal_offset(prot);\n}\n\n#define IDXD_PORTAL_MASK\t(PAGE_SIZE - 1)\n\n \nstatic inline void __iomem *idxd_wq_portal_addr(struct idxd_wq *wq)\n{\n\tint ofs = wq->portal_offset;\n\n\twq->portal_offset = (ofs + sizeof(struct dsa_raw_desc)) & IDXD_PORTAL_MASK;\n\treturn wq->portal + ofs;\n}\n\nstatic inline void idxd_wq_get(struct idxd_wq *wq)\n{\n\twq->client_count++;\n}\n\nstatic inline void idxd_wq_put(struct idxd_wq *wq)\n{\n\twq->client_count--;\n}\n\nstatic inline int idxd_wq_refcount(struct idxd_wq *wq)\n{\n\treturn wq->client_count;\n};\n\n \nstatic inline void idxd_set_max_batch_size(int idxd_type, struct idxd_device *idxd,\n\t\t\t\t\t   u32 max_batch_size)\n{\n\tif (idxd_type == IDXD_TYPE_IAX)\n\t\tidxd->max_batch_size = 0;\n\telse\n\t\tidxd->max_batch_size = max_batch_size;\n}\n\nstatic inline void idxd_wq_set_max_batch_size(int idxd_type, struct idxd_wq *wq,\n\t\t\t\t\t      u32 max_batch_size)\n{\n\tif (idxd_type == IDXD_TYPE_IAX)\n\t\twq->max_batch_size = 0;\n\telse\n\t\twq->max_batch_size = max_batch_size;\n}\n\nstatic inline void idxd_wqcfg_set_max_batch_shift(int idxd_type, union wqcfg *wqcfg,\n\t\t\t\t\t\t  u32 max_batch_shift)\n{\n\tif (idxd_type == IDXD_TYPE_IAX)\n\t\twqcfg->max_batch_shift = 0;\n\telse\n\t\twqcfg->max_batch_shift = max_batch_shift;\n}\n\nint __must_check __idxd_driver_register(struct idxd_device_driver *idxd_drv,\n\t\t\t\t\tstruct module *module, const char *mod_name);\n#define idxd_driver_register(driver) \\\n\t__idxd_driver_register(driver, THIS_MODULE, KBUILD_MODNAME)\n\nvoid idxd_driver_unregister(struct idxd_device_driver *idxd_drv);\n\n#define module_idxd_driver(__idxd_driver) \\\n\tmodule_driver(__idxd_driver, idxd_driver_register, idxd_driver_unregister)\n\nint idxd_register_bus_type(void);\nvoid idxd_unregister_bus_type(void);\nint idxd_register_devices(struct idxd_device *idxd);\nvoid idxd_unregister_devices(struct idxd_device *idxd);\nvoid idxd_wqs_quiesce(struct idxd_device *idxd);\nbool idxd_queue_int_handle_resubmit(struct idxd_desc *desc);\nvoid multi_u64_to_bmap(unsigned long *bmap, u64 *val, int count);\n\n \nirqreturn_t idxd_misc_thread(int vec, void *data);\nirqreturn_t idxd_wq_thread(int irq, void *data);\nvoid idxd_mask_error_interrupts(struct idxd_device *idxd);\nvoid idxd_unmask_error_interrupts(struct idxd_device *idxd);\n\n \nint idxd_device_drv_probe(struct idxd_dev *idxd_dev);\nvoid idxd_device_drv_remove(struct idxd_dev *idxd_dev);\nint drv_enable_wq(struct idxd_wq *wq);\nvoid drv_disable_wq(struct idxd_wq *wq);\nint idxd_device_init_reset(struct idxd_device *idxd);\nint idxd_device_enable(struct idxd_device *idxd);\nint idxd_device_disable(struct idxd_device *idxd);\nvoid idxd_device_reset(struct idxd_device *idxd);\nvoid idxd_device_clear_state(struct idxd_device *idxd);\nint idxd_device_config(struct idxd_device *idxd);\nvoid idxd_device_drain_pasid(struct idxd_device *idxd, int pasid);\nint idxd_device_load_config(struct idxd_device *idxd);\nint idxd_device_request_int_handle(struct idxd_device *idxd, int idx, int *handle,\n\t\t\t\t   enum idxd_interrupt_type irq_type);\nint idxd_device_release_int_handle(struct idxd_device *idxd, int handle,\n\t\t\t\t   enum idxd_interrupt_type irq_type);\n\n \nvoid idxd_wqs_unmap_portal(struct idxd_device *idxd);\nint idxd_wq_alloc_resources(struct idxd_wq *wq);\nvoid idxd_wq_free_resources(struct idxd_wq *wq);\nint idxd_wq_enable(struct idxd_wq *wq);\nint idxd_wq_disable(struct idxd_wq *wq, bool reset_config);\nvoid idxd_wq_drain(struct idxd_wq *wq);\nvoid idxd_wq_reset(struct idxd_wq *wq);\nint idxd_wq_map_portal(struct idxd_wq *wq);\nvoid idxd_wq_unmap_portal(struct idxd_wq *wq);\nint idxd_wq_set_pasid(struct idxd_wq *wq, int pasid);\nint idxd_wq_disable_pasid(struct idxd_wq *wq);\nvoid __idxd_wq_quiesce(struct idxd_wq *wq);\nvoid idxd_wq_quiesce(struct idxd_wq *wq);\nint idxd_wq_init_percpu_ref(struct idxd_wq *wq);\nvoid idxd_wq_free_irq(struct idxd_wq *wq);\nint idxd_wq_request_irq(struct idxd_wq *wq);\n\n \nint idxd_submit_desc(struct idxd_wq *wq, struct idxd_desc *desc);\nstruct idxd_desc *idxd_alloc_desc(struct idxd_wq *wq, enum idxd_op_type optype);\nvoid idxd_free_desc(struct idxd_wq *wq, struct idxd_desc *desc);\nint idxd_enqcmds(struct idxd_wq *wq, void __iomem *portal, const void *desc);\n\n \nint idxd_register_dma_device(struct idxd_device *idxd);\nvoid idxd_unregister_dma_device(struct idxd_device *idxd);\nvoid idxd_dma_complete_txd(struct idxd_desc *desc,\n\t\t\t   enum idxd_complete_type comp_type, bool free_desc);\n\n \nint idxd_cdev_register(void);\nvoid idxd_cdev_remove(void);\nint idxd_cdev_get_major(struct idxd_device *idxd);\nint idxd_wq_add_cdev(struct idxd_wq *wq);\nvoid idxd_wq_del_cdev(struct idxd_wq *wq);\nint idxd_copy_cr(struct idxd_wq *wq, ioasid_t pasid, unsigned long addr,\n\t\t void *buf, int len);\nvoid idxd_user_counter_increment(struct idxd_wq *wq, u32 pasid, int index);\n\n \n#if IS_ENABLED(CONFIG_INTEL_IDXD_PERFMON)\nint perfmon_pmu_init(struct idxd_device *idxd);\nvoid perfmon_pmu_remove(struct idxd_device *idxd);\nvoid perfmon_counter_overflow(struct idxd_device *idxd);\nvoid perfmon_init(void);\nvoid perfmon_exit(void);\n#else\nstatic inline int perfmon_pmu_init(struct idxd_device *idxd) { return 0; }\nstatic inline void perfmon_pmu_remove(struct idxd_device *idxd) {}\nstatic inline void perfmon_counter_overflow(struct idxd_device *idxd) {}\nstatic inline void perfmon_init(void) {}\nstatic inline void perfmon_exit(void) {}\n#endif\n\n \nint idxd_device_init_debugfs(struct idxd_device *idxd);\nvoid idxd_device_remove_debugfs(struct idxd_device *idxd);\nint idxd_init_debugfs(void);\nvoid idxd_remove_debugfs(void);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}