{
  "module_name": "sysfs.c",
  "hash_id": "ca9288bc2ad885a59b9e93a0aabb1678816fb75e2e53eb1afd8a6793f88a84d3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/idxd/sysfs.c",
  "human_readable_source": "\n \n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/device.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <uapi/linux/idxd.h>\n#include \"registers.h\"\n#include \"idxd.h\"\n\nstatic char *idxd_wq_type_names[] = {\n\t[IDXD_WQT_NONE]\t\t= \"none\",\n\t[IDXD_WQT_KERNEL]\t= \"kernel\",\n\t[IDXD_WQT_USER]\t\t= \"user\",\n};\n\n \nstatic ssize_t engine_group_id_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_engine *engine = confdev_to_engine(dev);\n\n\tif (engine->group)\n\t\treturn sysfs_emit(buf, \"%d\\n\", engine->group->id);\n\telse\n\t\treturn sysfs_emit(buf, \"%d\\n\", -1);\n}\n\nstatic ssize_t engine_group_id_store(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tstruct idxd_engine *engine = confdev_to_engine(dev);\n\tstruct idxd_device *idxd = engine->idxd;\n\tlong id;\n\tint rc;\n\tstruct idxd_group *prevg;\n\n\trc = kstrtol(buf, 10, &id);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (id > idxd->max_groups - 1 || id < -1)\n\t\treturn -EINVAL;\n\n\tif (id == -1) {\n\t\tif (engine->group) {\n\t\t\tengine->group->num_engines--;\n\t\t\tengine->group = NULL;\n\t\t}\n\t\treturn count;\n\t}\n\n\tprevg = engine->group;\n\n\tif (prevg)\n\t\tprevg->num_engines--;\n\tengine->group = idxd->groups[id];\n\tengine->group->num_engines++;\n\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_engine_group =\n\t\t__ATTR(group_id, 0644, engine_group_id_show,\n\t\t       engine_group_id_store);\n\nstatic struct attribute *idxd_engine_attributes[] = {\n\t&dev_attr_engine_group.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group idxd_engine_attribute_group = {\n\t.attrs = idxd_engine_attributes,\n};\n\nstatic const struct attribute_group *idxd_engine_attribute_groups[] = {\n\t&idxd_engine_attribute_group,\n\tNULL,\n};\n\nstatic void idxd_conf_engine_release(struct device *dev)\n{\n\tstruct idxd_engine *engine = confdev_to_engine(dev);\n\n\tkfree(engine);\n}\n\nstruct device_type idxd_engine_device_type = {\n\t.name = \"engine\",\n\t.release = idxd_conf_engine_release,\n\t.groups = idxd_engine_attribute_groups,\n};\n\n \n\nstatic void idxd_set_free_rdbufs(struct idxd_device *idxd)\n{\n\tint i, rdbufs;\n\n\tfor (i = 0, rdbufs = 0; i < idxd->max_groups; i++) {\n\t\tstruct idxd_group *g = idxd->groups[i];\n\n\t\trdbufs += g->rdbufs_reserved;\n\t}\n\n\tidxd->nr_rdbufs = idxd->max_rdbufs - rdbufs;\n}\n\nstatic ssize_t group_read_buffers_reserved_show(struct device *dev,\n\t\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\t\tchar *buf)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", group->rdbufs_reserved);\n}\n\nstatic ssize_t group_tokens_reserved_show(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\tdev_warn_once(dev, \"attribute deprecated, see read_buffers_reserved.\\n\");\n\treturn group_read_buffers_reserved_show(dev, attr, buf);\n}\n\nstatic ssize_t group_read_buffers_reserved_store(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t const char *buf, size_t count)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tstruct idxd_device *idxd = group->idxd;\n\tunsigned long val;\n\tint rc;\n\n\trc = kstrtoul(buf, 10, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (idxd->data->type == IDXD_TYPE_IAX)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (idxd->state == IDXD_DEV_ENABLED)\n\t\treturn -EPERM;\n\n\tif (val > idxd->max_rdbufs)\n\t\treturn -EINVAL;\n\n\tif (val > idxd->nr_rdbufs + group->rdbufs_reserved)\n\t\treturn -EINVAL;\n\n\tgroup->rdbufs_reserved = val;\n\tidxd_set_free_rdbufs(idxd);\n\treturn count;\n}\n\nstatic ssize_t group_tokens_reserved_store(struct device *dev,\n\t\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tdev_warn_once(dev, \"attribute deprecated, see read_buffers_reserved.\\n\");\n\treturn group_read_buffers_reserved_store(dev, attr, buf, count);\n}\n\nstatic struct device_attribute dev_attr_group_tokens_reserved =\n\t\t__ATTR(tokens_reserved, 0644, group_tokens_reserved_show,\n\t\t       group_tokens_reserved_store);\n\nstatic struct device_attribute dev_attr_group_read_buffers_reserved =\n\t\t__ATTR(read_buffers_reserved, 0644, group_read_buffers_reserved_show,\n\t\t       group_read_buffers_reserved_store);\n\nstatic ssize_t group_read_buffers_allowed_show(struct device *dev,\n\t\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t\t       char *buf)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", group->rdbufs_allowed);\n}\n\nstatic ssize_t group_tokens_allowed_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tdev_warn_once(dev, \"attribute deprecated, see read_buffers_allowed.\\n\");\n\treturn group_read_buffers_allowed_show(dev, attr, buf);\n}\n\nstatic ssize_t group_read_buffers_allowed_store(struct device *dev,\n\t\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tstruct idxd_device *idxd = group->idxd;\n\tunsigned long val;\n\tint rc;\n\n\trc = kstrtoul(buf, 10, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (idxd->data->type == IDXD_TYPE_IAX)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (idxd->state == IDXD_DEV_ENABLED)\n\t\treturn -EPERM;\n\n\tif (val < 4 * group->num_engines ||\n\t    val > group->rdbufs_reserved + idxd->nr_rdbufs)\n\t\treturn -EINVAL;\n\n\tgroup->rdbufs_allowed = val;\n\treturn count;\n}\n\nstatic ssize_t group_tokens_allowed_store(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tdev_warn_once(dev, \"attribute deprecated, see read_buffers_allowed.\\n\");\n\treturn group_read_buffers_allowed_store(dev, attr, buf, count);\n}\n\nstatic struct device_attribute dev_attr_group_tokens_allowed =\n\t\t__ATTR(tokens_allowed, 0644, group_tokens_allowed_show,\n\t\t       group_tokens_allowed_store);\n\nstatic struct device_attribute dev_attr_group_read_buffers_allowed =\n\t\t__ATTR(read_buffers_allowed, 0644, group_read_buffers_allowed_show,\n\t\t       group_read_buffers_allowed_store);\n\nstatic ssize_t group_use_read_buffer_limit_show(struct device *dev,\n\t\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\t\tchar *buf)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", group->use_rdbuf_limit);\n}\n\nstatic ssize_t group_use_token_limit_show(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\tdev_warn_once(dev, \"attribute deprecated, see use_read_buffer_limit.\\n\");\n\treturn group_use_read_buffer_limit_show(dev, attr, buf);\n}\n\nstatic ssize_t group_use_read_buffer_limit_store(struct device *dev,\n\t\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t\t const char *buf, size_t count)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tstruct idxd_device *idxd = group->idxd;\n\tunsigned long val;\n\tint rc;\n\n\trc = kstrtoul(buf, 10, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (idxd->data->type == IDXD_TYPE_IAX)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (idxd->state == IDXD_DEV_ENABLED)\n\t\treturn -EPERM;\n\n\tif (idxd->rdbuf_limit == 0)\n\t\treturn -EPERM;\n\n\tgroup->use_rdbuf_limit = !!val;\n\treturn count;\n}\n\nstatic ssize_t group_use_token_limit_store(struct device *dev,\n\t\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tdev_warn_once(dev, \"attribute deprecated, see use_read_buffer_limit.\\n\");\n\treturn group_use_read_buffer_limit_store(dev, attr, buf, count);\n}\n\nstatic struct device_attribute dev_attr_group_use_token_limit =\n\t\t__ATTR(use_token_limit, 0644, group_use_token_limit_show,\n\t\t       group_use_token_limit_store);\n\nstatic struct device_attribute dev_attr_group_use_read_buffer_limit =\n\t\t__ATTR(use_read_buffer_limit, 0644, group_use_read_buffer_limit_show,\n\t\t       group_use_read_buffer_limit_store);\n\nstatic ssize_t group_engines_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tint i, rc = 0;\n\tstruct idxd_device *idxd = group->idxd;\n\n\tfor (i = 0; i < idxd->max_engines; i++) {\n\t\tstruct idxd_engine *engine = idxd->engines[i];\n\n\t\tif (!engine->group)\n\t\t\tcontinue;\n\n\t\tif (engine->group->id == group->id)\n\t\t\trc += sysfs_emit_at(buf, rc, \"engine%d.%d \", idxd->id, engine->id);\n\t}\n\n\tif (!rc)\n\t\treturn 0;\n\trc--;\n\trc += sysfs_emit_at(buf, rc, \"\\n\");\n\n\treturn rc;\n}\n\nstatic struct device_attribute dev_attr_group_engines =\n\t\t__ATTR(engines, 0444, group_engines_show, NULL);\n\nstatic ssize_t group_work_queues_show(struct device *dev,\n\t\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tint i, rc = 0;\n\tstruct idxd_device *idxd = group->idxd;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\tif (!wq->group)\n\t\t\tcontinue;\n\n\t\tif (wq->group->id == group->id)\n\t\t\trc += sysfs_emit_at(buf, rc, \"wq%d.%d \", idxd->id, wq->id);\n\t}\n\n\tif (!rc)\n\t\treturn 0;\n\trc--;\n\trc += sysfs_emit_at(buf, rc, \"\\n\");\n\n\treturn rc;\n}\n\nstatic struct device_attribute dev_attr_group_work_queues =\n\t\t__ATTR(work_queues, 0444, group_work_queues_show, NULL);\n\nstatic ssize_t group_traffic_class_a_show(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", group->tc_a);\n}\n\nstatic ssize_t group_traffic_class_a_store(struct device *dev,\n\t\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tstruct idxd_device *idxd = group->idxd;\n\tlong val;\n\tint rc;\n\n\trc = kstrtol(buf, 10, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (idxd->state == IDXD_DEV_ENABLED)\n\t\treturn -EPERM;\n\n\tif (idxd->hw.version <= DEVICE_VERSION_2 && !tc_override)\n\t\treturn -EPERM;\n\n\tif (val < 0 || val > 7)\n\t\treturn -EINVAL;\n\n\tgroup->tc_a = val;\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_group_traffic_class_a =\n\t\t__ATTR(traffic_class_a, 0644, group_traffic_class_a_show,\n\t\t       group_traffic_class_a_store);\n\nstatic ssize_t group_traffic_class_b_show(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", group->tc_b);\n}\n\nstatic ssize_t group_traffic_class_b_store(struct device *dev,\n\t\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tstruct idxd_device *idxd = group->idxd;\n\tlong val;\n\tint rc;\n\n\trc = kstrtol(buf, 10, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (idxd->state == IDXD_DEV_ENABLED)\n\t\treturn -EPERM;\n\n\tif (idxd->hw.version <= DEVICE_VERSION_2 && !tc_override)\n\t\treturn -EPERM;\n\n\tif (val < 0 || val > 7)\n\t\treturn -EINVAL;\n\n\tgroup->tc_b = val;\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_group_traffic_class_b =\n\t\t__ATTR(traffic_class_b, 0644, group_traffic_class_b_show,\n\t\t       group_traffic_class_b_store);\n\nstatic ssize_t group_desc_progress_limit_show(struct device *dev,\n\t\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t\t      char *buf)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", group->desc_progress_limit);\n}\n\nstatic ssize_t group_desc_progress_limit_store(struct device *dev,\n\t\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t\t       const char *buf, size_t count)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tint val, rc;\n\n\trc = kstrtoint(buf, 10, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (val & ~GENMASK(1, 0))\n\t\treturn -EINVAL;\n\n\tgroup->desc_progress_limit = val;\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_group_desc_progress_limit =\n\t\t__ATTR(desc_progress_limit, 0644, group_desc_progress_limit_show,\n\t\t       group_desc_progress_limit_store);\n\nstatic ssize_t group_batch_progress_limit_show(struct device *dev,\n\t\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t\t       char *buf)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", group->batch_progress_limit);\n}\n\nstatic ssize_t group_batch_progress_limit_store(struct device *dev,\n\t\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tint val, rc;\n\n\trc = kstrtoint(buf, 10, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (val & ~GENMASK(1, 0))\n\t\treturn -EINVAL;\n\n\tgroup->batch_progress_limit = val;\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_group_batch_progress_limit =\n\t\t__ATTR(batch_progress_limit, 0644, group_batch_progress_limit_show,\n\t\t       group_batch_progress_limit_store);\nstatic struct attribute *idxd_group_attributes[] = {\n\t&dev_attr_group_work_queues.attr,\n\t&dev_attr_group_engines.attr,\n\t&dev_attr_group_use_token_limit.attr,\n\t&dev_attr_group_use_read_buffer_limit.attr,\n\t&dev_attr_group_tokens_allowed.attr,\n\t&dev_attr_group_read_buffers_allowed.attr,\n\t&dev_attr_group_tokens_reserved.attr,\n\t&dev_attr_group_read_buffers_reserved.attr,\n\t&dev_attr_group_traffic_class_a.attr,\n\t&dev_attr_group_traffic_class_b.attr,\n\t&dev_attr_group_desc_progress_limit.attr,\n\t&dev_attr_group_batch_progress_limit.attr,\n\tNULL,\n};\n\nstatic bool idxd_group_attr_progress_limit_invisible(struct attribute *attr,\n\t\t\t\t\t\t     struct idxd_device *idxd)\n{\n\treturn (attr == &dev_attr_group_desc_progress_limit.attr ||\n\t\tattr == &dev_attr_group_batch_progress_limit.attr) &&\n\t\t!idxd->hw.group_cap.progress_limit;\n}\n\nstatic bool idxd_group_attr_read_buffers_invisible(struct attribute *attr,\n\t\t\t\t\t\t   struct idxd_device *idxd)\n{\n\t \n\treturn (attr == &dev_attr_group_use_token_limit.attr ||\n\t\tattr == &dev_attr_group_use_read_buffer_limit.attr ||\n\t\tattr == &dev_attr_group_tokens_allowed.attr ||\n\t\tattr == &dev_attr_group_read_buffers_allowed.attr ||\n\t\tattr == &dev_attr_group_tokens_reserved.attr ||\n\t\tattr == &dev_attr_group_read_buffers_reserved.attr) &&\n\t\tidxd->data->type == IDXD_TYPE_IAX;\n}\n\nstatic umode_t idxd_group_attr_visible(struct kobject *kobj,\n\t\t\t\t       struct attribute *attr, int n)\n{\n\tstruct device *dev = container_of(kobj, struct device, kobj);\n\tstruct idxd_group *group = confdev_to_group(dev);\n\tstruct idxd_device *idxd = group->idxd;\n\n\tif (idxd_group_attr_progress_limit_invisible(attr, idxd))\n\t\treturn 0;\n\n\tif (idxd_group_attr_read_buffers_invisible(attr, idxd))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic const struct attribute_group idxd_group_attribute_group = {\n\t.attrs = idxd_group_attributes,\n\t.is_visible = idxd_group_attr_visible,\n};\n\nstatic const struct attribute_group *idxd_group_attribute_groups[] = {\n\t&idxd_group_attribute_group,\n\tNULL,\n};\n\nstatic void idxd_conf_group_release(struct device *dev)\n{\n\tstruct idxd_group *group = confdev_to_group(dev);\n\n\tkfree(group);\n}\n\nstruct device_type idxd_group_device_type = {\n\t.name = \"group\",\n\t.release = idxd_conf_group_release,\n\t.groups = idxd_group_attribute_groups,\n};\n\n \nstatic ssize_t wq_clients_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", wq->client_count);\n}\n\nstatic struct device_attribute dev_attr_wq_clients =\n\t\t__ATTR(clients, 0444, wq_clients_show, NULL);\n\nstatic ssize_t wq_state_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\tswitch (wq->state) {\n\tcase IDXD_WQ_DISABLED:\n\t\treturn sysfs_emit(buf, \"disabled\\n\");\n\tcase IDXD_WQ_ENABLED:\n\t\treturn sysfs_emit(buf, \"enabled\\n\");\n\t}\n\n\treturn sysfs_emit(buf, \"unknown\\n\");\n}\n\nstatic struct device_attribute dev_attr_wq_state =\n\t\t__ATTR(state, 0444, wq_state_show, NULL);\n\nstatic ssize_t wq_group_id_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\tif (wq->group)\n\t\treturn sysfs_emit(buf, \"%u\\n\", wq->group->id);\n\telse\n\t\treturn sysfs_emit(buf, \"-1\\n\");\n}\n\nstatic ssize_t wq_group_id_store(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tlong id;\n\tint rc;\n\tstruct idxd_group *prevg, *group;\n\n\trc = kstrtol(buf, 10, &id);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\tif (id > idxd->max_groups - 1 || id < -1)\n\t\treturn -EINVAL;\n\n\tif (id == -1) {\n\t\tif (wq->group) {\n\t\t\twq->group->num_wqs--;\n\t\t\twq->group = NULL;\n\t\t}\n\t\treturn count;\n\t}\n\n\tgroup = idxd->groups[id];\n\tprevg = wq->group;\n\n\tif (prevg)\n\t\tprevg->num_wqs--;\n\twq->group = group;\n\tgroup->num_wqs++;\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_group_id =\n\t\t__ATTR(group_id, 0644, wq_group_id_show, wq_group_id_store);\n\nstatic ssize_t wq_mode_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", wq_dedicated(wq) ? \"dedicated\" : \"shared\");\n}\n\nstatic ssize_t wq_mode_store(struct device *dev,\n\t\t\t     struct device_attribute *attr, const char *buf,\n\t\t\t     size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\tif (sysfs_streq(buf, \"dedicated\")) {\n\t\tset_bit(WQ_FLAG_DEDICATED, &wq->flags);\n\t\twq->threshold = 0;\n\t} else if (sysfs_streq(buf, \"shared\")) {\n\t\tclear_bit(WQ_FLAG_DEDICATED, &wq->flags);\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_mode =\n\t\t__ATTR(mode, 0644, wq_mode_show, wq_mode_store);\n\nstatic ssize_t wq_size_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", wq->size);\n}\n\nstatic int total_claimed_wq_size(struct idxd_device *idxd)\n{\n\tint i;\n\tint wq_size = 0;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\twq_size += wq->size;\n\t}\n\n\treturn wq_size;\n}\n\nstatic ssize_t wq_size_store(struct device *dev,\n\t\t\t     struct device_attribute *attr, const char *buf,\n\t\t\t     size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tunsigned long size;\n\tstruct idxd_device *idxd = wq->idxd;\n\tint rc;\n\n\trc = kstrtoul(buf, 10, &size);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (idxd->state == IDXD_DEV_ENABLED)\n\t\treturn -EPERM;\n\n\tif (size + total_claimed_wq_size(idxd) - wq->size > idxd->max_wq_size)\n\t\treturn -EINVAL;\n\n\twq->size = size;\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_size =\n\t\t__ATTR(size, 0644, wq_size_show, wq_size_store);\n\nstatic ssize_t wq_priority_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", wq->priority);\n}\n\nstatic ssize_t wq_priority_store(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tunsigned long prio;\n\tstruct idxd_device *idxd = wq->idxd;\n\tint rc;\n\n\trc = kstrtoul(buf, 10, &prio);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\tif (prio > IDXD_MAX_PRIORITY)\n\t\treturn -EINVAL;\n\n\twq->priority = prio;\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_priority =\n\t\t__ATTR(priority, 0644, wq_priority_show, wq_priority_store);\n\nstatic ssize_t wq_block_on_fault_show(struct device *dev,\n\t\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", test_bit(WQ_FLAG_BLOCK_ON_FAULT, &wq->flags));\n}\n\nstatic ssize_t wq_block_on_fault_store(struct device *dev,\n\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t       const char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tbool bof;\n\tint rc;\n\n\tif (!idxd->hw.gen_cap.block_on_fault)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -ENXIO;\n\n\trc = kstrtobool(buf, &bof);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tif (bof) {\n\t\tif (test_bit(WQ_FLAG_PRS_DISABLE, &wq->flags))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tset_bit(WQ_FLAG_BLOCK_ON_FAULT, &wq->flags);\n\t} else {\n\t\tclear_bit(WQ_FLAG_BLOCK_ON_FAULT, &wq->flags);\n\t}\n\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_block_on_fault =\n\t\t__ATTR(block_on_fault, 0644, wq_block_on_fault_show,\n\t\t       wq_block_on_fault_store);\n\nstatic ssize_t wq_threshold_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", wq->threshold);\n}\n\nstatic ssize_t wq_threshold_store(struct device *dev,\n\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t  const char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tunsigned int val;\n\tint rc;\n\n\trc = kstrtouint(buf, 0, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (val > wq->size || val <= 0)\n\t\treturn -EINVAL;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -ENXIO;\n\n\tif (test_bit(WQ_FLAG_DEDICATED, &wq->flags))\n\t\treturn -EINVAL;\n\n\twq->threshold = val;\n\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_threshold =\n\t\t__ATTR(threshold, 0644, wq_threshold_show, wq_threshold_store);\n\nstatic ssize_t wq_type_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\tswitch (wq->type) {\n\tcase IDXD_WQT_KERNEL:\n\t\treturn sysfs_emit(buf, \"%s\\n\", idxd_wq_type_names[IDXD_WQT_KERNEL]);\n\tcase IDXD_WQT_USER:\n\t\treturn sysfs_emit(buf, \"%s\\n\", idxd_wq_type_names[IDXD_WQT_USER]);\n\tcase IDXD_WQT_NONE:\n\tdefault:\n\t\treturn sysfs_emit(buf, \"%s\\n\", idxd_wq_type_names[IDXD_WQT_NONE]);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic ssize_t wq_type_store(struct device *dev,\n\t\t\t     struct device_attribute *attr, const char *buf,\n\t\t\t     size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tenum idxd_wq_type old_type;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\told_type = wq->type;\n\tif (sysfs_streq(buf, idxd_wq_type_names[IDXD_WQT_NONE]))\n\t\twq->type = IDXD_WQT_NONE;\n\telse if (sysfs_streq(buf, idxd_wq_type_names[IDXD_WQT_KERNEL]))\n\t\twq->type = IDXD_WQT_KERNEL;\n\telse if (sysfs_streq(buf, idxd_wq_type_names[IDXD_WQT_USER]))\n\t\twq->type = IDXD_WQT_USER;\n\telse\n\t\treturn -EINVAL;\n\n\t \n\tif (wq->type != old_type)\n\t\tmemset(wq->name, 0, WQ_NAME_SIZE + 1);\n\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_type =\n\t\t__ATTR(type, 0644, wq_type_show, wq_type_store);\n\nstatic ssize_t wq_name_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", wq->name);\n}\n\nstatic ssize_t wq_name_store(struct device *dev,\n\t\t\t     struct device_attribute *attr, const char *buf,\n\t\t\t     size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tchar *input, *pos;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\tif (strlen(buf) > WQ_NAME_SIZE || strlen(buf) == 0)\n\t\treturn -EINVAL;\n\n\tinput = kstrndup(buf, count, GFP_KERNEL);\n\tif (!input)\n\t\treturn -ENOMEM;\n\n\tpos = strim(input);\n\tmemset(wq->name, 0, WQ_NAME_SIZE + 1);\n\tsprintf(wq->name, \"%s\", pos);\n\tkfree(input);\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_name =\n\t\t__ATTR(name, 0644, wq_name_show, wq_name_store);\n\nstatic ssize_t wq_cdev_minor_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tint minor = -1;\n\n\tmutex_lock(&wq->wq_lock);\n\tif (wq->idxd_cdev)\n\t\tminor = wq->idxd_cdev->minor;\n\tmutex_unlock(&wq->wq_lock);\n\n\tif (minor == -1)\n\t\treturn -ENXIO;\n\treturn sysfs_emit(buf, \"%d\\n\", minor);\n}\n\nstatic struct device_attribute dev_attr_wq_cdev_minor =\n\t\t__ATTR(cdev_minor, 0444, wq_cdev_minor_show, NULL);\n\nstatic int __get_sysfs_u64(const char *buf, u64 *val)\n{\n\tint rc;\n\n\trc = kstrtou64(buf, 0, val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (*val == 0)\n\t\treturn -EINVAL;\n\n\t*val = roundup_pow_of_two(*val);\n\treturn 0;\n}\n\nstatic ssize_t wq_max_transfer_size_show(struct device *dev, struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%llu\\n\", wq->max_xfer_bytes);\n}\n\nstatic ssize_t wq_max_transfer_size_store(struct device *dev, struct device_attribute *attr,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tu64 xfer_size;\n\tint rc;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\trc = __get_sysfs_u64(buf, &xfer_size);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tif (xfer_size > idxd->max_xfer_bytes)\n\t\treturn -EINVAL;\n\n\twq->max_xfer_bytes = xfer_size;\n\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_max_transfer_size =\n\t\t__ATTR(max_transfer_size, 0644,\n\t\t       wq_max_transfer_size_show, wq_max_transfer_size_store);\n\nstatic ssize_t wq_max_batch_size_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", wq->max_batch_size);\n}\n\nstatic ssize_t wq_max_batch_size_store(struct device *dev, struct device_attribute *attr,\n\t\t\t\t       const char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tu64 batch_size;\n\tint rc;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\trc = __get_sysfs_u64(buf, &batch_size);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tif (batch_size > idxd->max_batch_size)\n\t\treturn -EINVAL;\n\n\tidxd_wq_set_max_batch_size(idxd->data->type, wq, (u32)batch_size);\n\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_max_batch_size =\n\t\t__ATTR(max_batch_size, 0644, wq_max_batch_size_show, wq_max_batch_size_store);\n\nstatic ssize_t wq_ats_disable_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", test_bit(WQ_FLAG_ATS_DISABLE, &wq->flags));\n}\n\nstatic ssize_t wq_ats_disable_store(struct device *dev, struct device_attribute *attr,\n\t\t\t\t    const char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tbool ats_dis;\n\tint rc;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\trc = kstrtobool(buf, &ats_dis);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tif (ats_dis)\n\t\tset_bit(WQ_FLAG_ATS_DISABLE, &wq->flags);\n\telse\n\t\tclear_bit(WQ_FLAG_ATS_DISABLE, &wq->flags);\n\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_ats_disable =\n\t\t__ATTR(ats_disable, 0644, wq_ats_disable_show, wq_ats_disable_store);\n\nstatic ssize_t wq_prs_disable_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", test_bit(WQ_FLAG_PRS_DISABLE, &wq->flags));\n}\n\nstatic ssize_t wq_prs_disable_store(struct device *dev, struct device_attribute *attr,\n\t\t\t\t    const char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tbool prs_dis;\n\tint rc;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\trc = kstrtobool(buf, &prs_dis);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tif (prs_dis) {\n\t\tset_bit(WQ_FLAG_PRS_DISABLE, &wq->flags);\n\t\t \n\t\tclear_bit(WQ_FLAG_BLOCK_ON_FAULT, &wq->flags);\n\t} else {\n\t\tclear_bit(WQ_FLAG_PRS_DISABLE, &wq->flags);\n\t}\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_prs_disable =\n\t\t__ATTR(prs_disable, 0644, wq_prs_disable_show, wq_prs_disable_store);\n\nstatic ssize_t wq_occupancy_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tu32 occup, offset;\n\n\tif (!idxd->hw.wq_cap.occupancy)\n\t\treturn -EOPNOTSUPP;\n\n\toffset = WQCFG_OFFSET(idxd, wq->id, WQCFG_OCCUP_IDX);\n\toccup = ioread32(idxd->reg_base + offset) & WQCFG_OCCUP_MASK;\n\n\treturn sysfs_emit(buf, \"%u\\n\", occup);\n}\n\nstatic struct device_attribute dev_attr_wq_occupancy =\n\t\t__ATTR(occupancy, 0444, wq_occupancy_show, NULL);\n\nstatic ssize_t wq_enqcmds_retries_show(struct device *dev,\n\t\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\tif (wq_dedicated(wq))\n\t\treturn -EOPNOTSUPP;\n\n\treturn sysfs_emit(buf, \"%u\\n\", wq->enqcmds_retries);\n}\n\nstatic ssize_t wq_enqcmds_retries_store(struct device *dev, struct device_attribute *attr,\n\t\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tint rc;\n\tunsigned int retries;\n\n\tif (wq_dedicated(wq))\n\t\treturn -EOPNOTSUPP;\n\n\trc = kstrtouint(buf, 10, &retries);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tif (retries > IDXD_ENQCMDS_MAX_RETRIES)\n\t\tretries = IDXD_ENQCMDS_MAX_RETRIES;\n\n\twq->enqcmds_retries = retries;\n\treturn count;\n}\n\nstatic struct device_attribute dev_attr_wq_enqcmds_retries =\n\t\t__ATTR(enqcmds_retries, 0644, wq_enqcmds_retries_show, wq_enqcmds_retries_store);\n\nstatic ssize_t wq_op_config_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\treturn sysfs_emit(buf, \"%*pb\\n\", IDXD_MAX_OPCAP_BITS, wq->opcap_bmap);\n}\n\nstatic int idxd_verify_supported_opcap(struct idxd_device *idxd, unsigned long *opmask)\n{\n\tint bit;\n\n\t \n\tfor_each_set_bit(bit, opmask, IDXD_MAX_OPCAP_BITS) {\n\t\tif (!test_bit(bit, idxd->opcap_bmap))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t wq_op_config_store(struct device *dev, struct device_attribute *attr,\n\t\t\t\t  const char *buf, size_t count)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tunsigned long *opmask;\n\tint rc;\n\n\tif (wq->state != IDXD_WQ_DISABLED)\n\t\treturn -EPERM;\n\n\topmask = bitmap_zalloc(IDXD_MAX_OPCAP_BITS, GFP_KERNEL);\n\tif (!opmask)\n\t\treturn -ENOMEM;\n\n\trc = bitmap_parse(buf, count, opmask, IDXD_MAX_OPCAP_BITS);\n\tif (rc < 0)\n\t\tgoto err;\n\n\trc = idxd_verify_supported_opcap(idxd, opmask);\n\tif (rc < 0)\n\t\tgoto err;\n\n\tbitmap_copy(wq->opcap_bmap, opmask, IDXD_MAX_OPCAP_BITS);\n\n\tbitmap_free(opmask);\n\treturn count;\n\nerr:\n\tbitmap_free(opmask);\n\treturn rc;\n}\n\nstatic struct device_attribute dev_attr_wq_op_config =\n\t\t__ATTR(op_config, 0644, wq_op_config_show, wq_op_config_store);\n\nstatic struct attribute *idxd_wq_attributes[] = {\n\t&dev_attr_wq_clients.attr,\n\t&dev_attr_wq_state.attr,\n\t&dev_attr_wq_group_id.attr,\n\t&dev_attr_wq_mode.attr,\n\t&dev_attr_wq_size.attr,\n\t&dev_attr_wq_priority.attr,\n\t&dev_attr_wq_block_on_fault.attr,\n\t&dev_attr_wq_threshold.attr,\n\t&dev_attr_wq_type.attr,\n\t&dev_attr_wq_name.attr,\n\t&dev_attr_wq_cdev_minor.attr,\n\t&dev_attr_wq_max_transfer_size.attr,\n\t&dev_attr_wq_max_batch_size.attr,\n\t&dev_attr_wq_ats_disable.attr,\n\t&dev_attr_wq_prs_disable.attr,\n\t&dev_attr_wq_occupancy.attr,\n\t&dev_attr_wq_enqcmds_retries.attr,\n\t&dev_attr_wq_op_config.attr,\n\tNULL,\n};\n\n \n#define idxd_wq_attr_invisible(name, cap_field, a, idxd)\t\t\\\n\t((a) == &dev_attr_wq_##name.attr && !(idxd)->hw.wq_cap.cap_field)\n\nstatic bool idxd_wq_attr_max_batch_size_invisible(struct attribute *attr,\n\t\t\t\t\t\t  struct idxd_device *idxd)\n{\n\t \n\treturn attr == &dev_attr_wq_max_batch_size.attr &&\n\t       idxd->data->type == IDXD_TYPE_IAX;\n}\n\nstatic umode_t idxd_wq_attr_visible(struct kobject *kobj,\n\t\t\t\t    struct attribute *attr, int n)\n{\n\tstruct device *dev = container_of(kobj, struct device, kobj);\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\n\tif (idxd_wq_attr_invisible(op_config, op_config, attr, idxd))\n\t\treturn 0;\n\n\tif (idxd_wq_attr_max_batch_size_invisible(attr, idxd))\n\t\treturn 0;\n\n\tif (idxd_wq_attr_invisible(prs_disable, wq_prs_support, attr, idxd))\n\t\treturn 0;\n\n\tif (idxd_wq_attr_invisible(ats_disable, wq_ats_support, attr, idxd))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic const struct attribute_group idxd_wq_attribute_group = {\n\t.attrs = idxd_wq_attributes,\n\t.is_visible = idxd_wq_attr_visible,\n};\n\nstatic const struct attribute_group *idxd_wq_attribute_groups[] = {\n\t&idxd_wq_attribute_group,\n\tNULL,\n};\n\nstatic void idxd_conf_wq_release(struct device *dev)\n{\n\tstruct idxd_wq *wq = confdev_to_wq(dev);\n\n\tbitmap_free(wq->opcap_bmap);\n\tkfree(wq->wqcfg);\n\txa_destroy(&wq->upasid_xa);\n\tkfree(wq);\n}\n\nstruct device_type idxd_wq_device_type = {\n\t.name = \"wq\",\n\t.release = idxd_conf_wq_release,\n\t.groups = idxd_wq_attribute_groups,\n};\n\n \nstatic ssize_t version_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%#x\\n\", idxd->hw.version);\n}\nstatic DEVICE_ATTR_RO(version);\n\nstatic ssize_t max_work_queues_size_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", idxd->max_wq_size);\n}\nstatic DEVICE_ATTR_RO(max_work_queues_size);\n\nstatic ssize_t max_groups_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", idxd->max_groups);\n}\nstatic DEVICE_ATTR_RO(max_groups);\n\nstatic ssize_t max_work_queues_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", idxd->max_wqs);\n}\nstatic DEVICE_ATTR_RO(max_work_queues);\n\nstatic ssize_t max_engines_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", idxd->max_engines);\n}\nstatic DEVICE_ATTR_RO(max_engines);\n\nstatic ssize_t numa_node_show(struct device *dev,\n\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%d\\n\", dev_to_node(&idxd->pdev->dev));\n}\nstatic DEVICE_ATTR_RO(numa_node);\n\nstatic ssize_t max_batch_size_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", idxd->max_batch_size);\n}\nstatic DEVICE_ATTR_RO(max_batch_size);\n\nstatic ssize_t max_transfer_size_show(struct device *dev,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%llu\\n\", idxd->max_xfer_bytes);\n}\nstatic DEVICE_ATTR_RO(max_transfer_size);\n\nstatic ssize_t op_cap_show(struct device *dev,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%*pb\\n\", IDXD_MAX_OPCAP_BITS, idxd->opcap_bmap);\n}\nstatic DEVICE_ATTR_RO(op_cap);\n\nstatic ssize_t gen_cap_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%#llx\\n\", idxd->hw.gen_cap.bits);\n}\nstatic DEVICE_ATTR_RO(gen_cap);\n\nstatic ssize_t configurable_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags));\n}\nstatic DEVICE_ATTR_RO(configurable);\n\nstatic ssize_t clients_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\tint count = 0, i;\n\n\tspin_lock(&idxd->dev_lock);\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\tcount += wq->client_count;\n\t}\n\tspin_unlock(&idxd->dev_lock);\n\n\treturn sysfs_emit(buf, \"%d\\n\", count);\n}\nstatic DEVICE_ATTR_RO(clients);\n\nstatic ssize_t pasid_enabled_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", device_user_pasid_enabled(idxd));\n}\nstatic DEVICE_ATTR_RO(pasid_enabled);\n\nstatic ssize_t state_show(struct device *dev,\n\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\tswitch (idxd->state) {\n\tcase IDXD_DEV_DISABLED:\n\t\treturn sysfs_emit(buf, \"disabled\\n\");\n\tcase IDXD_DEV_ENABLED:\n\t\treturn sysfs_emit(buf, \"enabled\\n\");\n\tcase IDXD_DEV_HALTED:\n\t\treturn sysfs_emit(buf, \"halted\\n\");\n\t}\n\n\treturn sysfs_emit(buf, \"unknown\\n\");\n}\nstatic DEVICE_ATTR_RO(state);\n\nstatic ssize_t errors_show(struct device *dev,\n\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\tDECLARE_BITMAP(swerr_bmap, 256);\n\n\tbitmap_zero(swerr_bmap, 256);\n\tspin_lock(&idxd->dev_lock);\n\tmulti_u64_to_bmap(swerr_bmap, &idxd->sw_err.bits[0], 4);\n\tspin_unlock(&idxd->dev_lock);\n\treturn sysfs_emit(buf, \"%*pb\\n\", 256, swerr_bmap);\n}\nstatic DEVICE_ATTR_RO(errors);\n\nstatic ssize_t max_read_buffers_show(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", idxd->max_rdbufs);\n}\n\nstatic ssize_t max_tokens_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tdev_warn_once(dev, \"attribute deprecated, see max_read_buffers.\\n\");\n\treturn max_read_buffers_show(dev, attr, buf);\n}\n\nstatic DEVICE_ATTR_RO(max_tokens);\t \nstatic DEVICE_ATTR_RO(max_read_buffers);\n\nstatic ssize_t read_buffer_limit_show(struct device *dev,\n\t\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", idxd->rdbuf_limit);\n}\n\nstatic ssize_t token_limit_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tdev_warn_once(dev, \"attribute deprecated, see read_buffer_limit.\\n\");\n\treturn read_buffer_limit_show(dev, attr, buf);\n}\n\nstatic ssize_t read_buffer_limit_store(struct device *dev,\n\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t       const char *buf, size_t count)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\tunsigned long val;\n\tint rc;\n\n\trc = kstrtoul(buf, 10, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (idxd->state == IDXD_DEV_ENABLED)\n\t\treturn -EPERM;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (!idxd->hw.group_cap.rdbuf_limit)\n\t\treturn -EPERM;\n\n\tif (val > idxd->hw.group_cap.total_rdbufs)\n\t\treturn -EINVAL;\n\n\tidxd->rdbuf_limit = val;\n\treturn count;\n}\n\nstatic ssize_t token_limit_store(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t count)\n{\n\tdev_warn_once(dev, \"attribute deprecated, see read_buffer_limit\\n\");\n\treturn read_buffer_limit_store(dev, attr, buf, count);\n}\n\nstatic DEVICE_ATTR_RW(token_limit);\t \nstatic DEVICE_ATTR_RW(read_buffer_limit);\n\nstatic ssize_t cdev_major_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%u\\n\", idxd->major);\n}\nstatic DEVICE_ATTR_RO(cdev_major);\n\nstatic ssize_t cmd_status_show(struct device *dev,\n\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\treturn sysfs_emit(buf, \"%#x\\n\", idxd->cmd_status);\n}\n\nstatic ssize_t cmd_status_store(struct device *dev, struct device_attribute *attr,\n\t\t\t\tconst char *buf, size_t count)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\tidxd->cmd_status = 0;\n\treturn count;\n}\nstatic DEVICE_ATTR_RW(cmd_status);\n\nstatic ssize_t iaa_cap_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\tif (idxd->hw.version < DEVICE_VERSION_2)\n\t\treturn -EOPNOTSUPP;\n\n\treturn sysfs_emit(buf, \"%#llx\\n\", idxd->hw.iaa_cap.bits);\n}\nstatic DEVICE_ATTR_RO(iaa_cap);\n\nstatic ssize_t event_log_size_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\tif (!idxd->evl)\n\t\treturn -EOPNOTSUPP;\n\n\treturn sysfs_emit(buf, \"%u\\n\", idxd->evl->size);\n}\n\nstatic ssize_t event_log_size_store(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    const char *buf, size_t count)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\tunsigned long val;\n\tint rc;\n\n\tif (!idxd->evl)\n\t\treturn -EOPNOTSUPP;\n\n\trc = kstrtoul(buf, 10, &val);\n\tif (rc < 0)\n\t\treturn -EINVAL;\n\n\tif (idxd->state == IDXD_DEV_ENABLED)\n\t\treturn -EPERM;\n\n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\treturn -EPERM;\n\n\tif (val < IDXD_EVL_SIZE_MIN || val > IDXD_EVL_SIZE_MAX ||\n\t    (val * evl_ent_size(idxd) > ULONG_MAX - idxd->evl->dma))\n\t\treturn -EINVAL;\n\n\tidxd->evl->size = val;\n\treturn count;\n}\nstatic DEVICE_ATTR_RW(event_log_size);\n\nstatic bool idxd_device_attr_max_batch_size_invisible(struct attribute *attr,\n\t\t\t\t\t\t      struct idxd_device *idxd)\n{\n\t \n\treturn attr == &dev_attr_max_batch_size.attr &&\n\t       idxd->data->type == IDXD_TYPE_IAX;\n}\n\nstatic bool idxd_device_attr_read_buffers_invisible(struct attribute *attr,\n\t\t\t\t\t\t    struct idxd_device *idxd)\n{\n\t \n\treturn (attr == &dev_attr_max_tokens.attr ||\n\t\tattr == &dev_attr_max_read_buffers.attr ||\n\t\tattr == &dev_attr_token_limit.attr ||\n\t\tattr == &dev_attr_read_buffer_limit.attr) &&\n\t\tidxd->data->type == IDXD_TYPE_IAX;\n}\n\nstatic bool idxd_device_attr_iaa_cap_invisible(struct attribute *attr,\n\t\t\t\t\t       struct idxd_device *idxd)\n{\n\treturn attr == &dev_attr_iaa_cap.attr &&\n\t       (idxd->data->type != IDXD_TYPE_IAX ||\n\t       idxd->hw.version < DEVICE_VERSION_2);\n}\n\nstatic bool idxd_device_attr_event_log_size_invisible(struct attribute *attr,\n\t\t\t\t\t\t      struct idxd_device *idxd)\n{\n\treturn (attr == &dev_attr_event_log_size.attr &&\n\t\t!idxd->hw.gen_cap.evl_support);\n}\n\nstatic umode_t idxd_device_attr_visible(struct kobject *kobj,\n\t\t\t\t\tstruct attribute *attr, int n)\n{\n\tstruct device *dev = container_of(kobj, struct device, kobj);\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\tif (idxd_device_attr_max_batch_size_invisible(attr, idxd))\n\t\treturn 0;\n\n\tif (idxd_device_attr_read_buffers_invisible(attr, idxd))\n\t\treturn 0;\n\n\tif (idxd_device_attr_iaa_cap_invisible(attr, idxd))\n\t\treturn 0;\n\n\tif (idxd_device_attr_event_log_size_invisible(attr, idxd))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic struct attribute *idxd_device_attributes[] = {\n\t&dev_attr_version.attr,\n\t&dev_attr_max_groups.attr,\n\t&dev_attr_max_work_queues.attr,\n\t&dev_attr_max_work_queues_size.attr,\n\t&dev_attr_max_engines.attr,\n\t&dev_attr_numa_node.attr,\n\t&dev_attr_max_batch_size.attr,\n\t&dev_attr_max_transfer_size.attr,\n\t&dev_attr_op_cap.attr,\n\t&dev_attr_gen_cap.attr,\n\t&dev_attr_configurable.attr,\n\t&dev_attr_clients.attr,\n\t&dev_attr_pasid_enabled.attr,\n\t&dev_attr_state.attr,\n\t&dev_attr_errors.attr,\n\t&dev_attr_max_tokens.attr,\n\t&dev_attr_max_read_buffers.attr,\n\t&dev_attr_token_limit.attr,\n\t&dev_attr_read_buffer_limit.attr,\n\t&dev_attr_cdev_major.attr,\n\t&dev_attr_cmd_status.attr,\n\t&dev_attr_iaa_cap.attr,\n\t&dev_attr_event_log_size.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group idxd_device_attribute_group = {\n\t.attrs = idxd_device_attributes,\n\t.is_visible = idxd_device_attr_visible,\n};\n\nstatic const struct attribute_group *idxd_attribute_groups[] = {\n\t&idxd_device_attribute_group,\n\tNULL,\n};\n\nstatic void idxd_conf_device_release(struct device *dev)\n{\n\tstruct idxd_device *idxd = confdev_to_idxd(dev);\n\n\tkfree(idxd->groups);\n\tbitmap_free(idxd->wq_enable_map);\n\tkfree(idxd->wqs);\n\tkfree(idxd->engines);\n\tkfree(idxd->evl);\n\tkmem_cache_destroy(idxd->evl_cache);\n\tida_free(&idxd_ida, idxd->id);\n\tbitmap_free(idxd->opcap_bmap);\n\tkfree(idxd);\n}\n\nstruct device_type dsa_device_type = {\n\t.name = \"dsa\",\n\t.release = idxd_conf_device_release,\n\t.groups = idxd_attribute_groups,\n};\n\nstruct device_type iax_device_type = {\n\t.name = \"iax\",\n\t.release = idxd_conf_device_release,\n\t.groups = idxd_attribute_groups,\n};\n\nstatic int idxd_register_engine_devices(struct idxd_device *idxd)\n{\n\tstruct idxd_engine *engine;\n\tint i, j, rc;\n\n\tfor (i = 0; i < idxd->max_engines; i++) {\n\t\tengine = idxd->engines[i];\n\t\trc = device_add(engine_confdev(engine));\n\t\tif (rc < 0)\n\t\t\tgoto cleanup;\n\t}\n\n\treturn 0;\n\ncleanup:\n\tj = i - 1;\n\tfor (; i < idxd->max_engines; i++) {\n\t\tengine = idxd->engines[i];\n\t\tput_device(engine_confdev(engine));\n\t}\n\n\twhile (j--) {\n\t\tengine = idxd->engines[j];\n\t\tdevice_unregister(engine_confdev(engine));\n\t}\n\treturn rc;\n}\n\nstatic int idxd_register_group_devices(struct idxd_device *idxd)\n{\n\tstruct idxd_group *group;\n\tint i, j, rc;\n\n\tfor (i = 0; i < idxd->max_groups; i++) {\n\t\tgroup = idxd->groups[i];\n\t\trc = device_add(group_confdev(group));\n\t\tif (rc < 0)\n\t\t\tgoto cleanup;\n\t}\n\n\treturn 0;\n\ncleanup:\n\tj = i - 1;\n\tfor (; i < idxd->max_groups; i++) {\n\t\tgroup = idxd->groups[i];\n\t\tput_device(group_confdev(group));\n\t}\n\n\twhile (j--) {\n\t\tgroup = idxd->groups[j];\n\t\tdevice_unregister(group_confdev(group));\n\t}\n\treturn rc;\n}\n\nstatic int idxd_register_wq_devices(struct idxd_device *idxd)\n{\n\tstruct idxd_wq *wq;\n\tint i, rc, j;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\twq = idxd->wqs[i];\n\t\trc = device_add(wq_confdev(wq));\n\t\tif (rc < 0)\n\t\t\tgoto cleanup;\n\t}\n\n\treturn 0;\n\ncleanup:\n\tj = i - 1;\n\tfor (; i < idxd->max_wqs; i++) {\n\t\twq = idxd->wqs[i];\n\t\tput_device(wq_confdev(wq));\n\t}\n\n\twhile (j--) {\n\t\twq = idxd->wqs[j];\n\t\tdevice_unregister(wq_confdev(wq));\n\t}\n\treturn rc;\n}\n\nint idxd_register_devices(struct idxd_device *idxd)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tint rc, i;\n\n\trc = device_add(idxd_confdev(idxd));\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = idxd_register_wq_devices(idxd);\n\tif (rc < 0) {\n\t\tdev_dbg(dev, \"WQ devices registering failed: %d\\n\", rc);\n\t\tgoto err_wq;\n\t}\n\n\trc = idxd_register_engine_devices(idxd);\n\tif (rc < 0) {\n\t\tdev_dbg(dev, \"Engine devices registering failed: %d\\n\", rc);\n\t\tgoto err_engine;\n\t}\n\n\trc = idxd_register_group_devices(idxd);\n\tif (rc < 0) {\n\t\tdev_dbg(dev, \"Group device registering failed: %d\\n\", rc);\n\t\tgoto err_group;\n\t}\n\n\treturn 0;\n\n err_group:\n\tfor (i = 0; i < idxd->max_engines; i++)\n\t\tdevice_unregister(engine_confdev(idxd->engines[i]));\n err_engine:\n\tfor (i = 0; i < idxd->max_wqs; i++)\n\t\tdevice_unregister(wq_confdev(idxd->wqs[i]));\n err_wq:\n\tdevice_del(idxd_confdev(idxd));\n\treturn rc;\n}\n\nvoid idxd_unregister_devices(struct idxd_device *idxd)\n{\n\tint i;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\tdevice_unregister(wq_confdev(wq));\n\t}\n\n\tfor (i = 0; i < idxd->max_engines; i++) {\n\t\tstruct idxd_engine *engine = idxd->engines[i];\n\n\t\tdevice_unregister(engine_confdev(engine));\n\t}\n\n\tfor (i = 0; i < idxd->max_groups; i++) {\n\t\tstruct idxd_group *group = idxd->groups[i];\n\n\t\tdevice_unregister(group_confdev(group));\n\t}\n}\n\nint idxd_register_bus_type(void)\n{\n\treturn bus_register(&dsa_bus_type);\n}\n\nvoid idxd_unregister_bus_type(void)\n{\n\tbus_unregister(&dsa_bus_type);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}