{
  "module_name": "dma.c",
  "hash_id": "5a0dc08af6e679ea795b43ade6921e88c6f31811e107774989391f25b3bcbc35",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/idxd/dma.c",
  "human_readable_source": "\n \n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/device.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/dmaengine.h>\n#include <uapi/linux/idxd.h>\n#include \"../dmaengine.h\"\n#include \"registers.h\"\n#include \"idxd.h\"\n\nstatic inline struct idxd_wq *to_idxd_wq(struct dma_chan *c)\n{\n\tstruct idxd_dma_chan *idxd_chan;\n\n\tidxd_chan = container_of(c, struct idxd_dma_chan, chan);\n\treturn idxd_chan->wq;\n}\n\nvoid idxd_dma_complete_txd(struct idxd_desc *desc,\n\t\t\t   enum idxd_complete_type comp_type,\n\t\t\t   bool free_desc)\n{\n\tstruct idxd_device *idxd = desc->wq->idxd;\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct dmaengine_result res;\n\tint complete = 1;\n\n\tif (desc->completion->status == DSA_COMP_SUCCESS) {\n\t\tres.result = DMA_TRANS_NOERROR;\n\t} else if (desc->completion->status) {\n\t\tif (idxd->request_int_handles && comp_type != IDXD_COMPLETE_ABORT &&\n\t\t    desc->completion->status == DSA_COMP_INT_HANDLE_INVAL &&\n\t\t    idxd_queue_int_handle_resubmit(desc))\n\t\t\treturn;\n\t\tres.result = DMA_TRANS_WRITE_FAILED;\n\t} else if (comp_type == IDXD_COMPLETE_ABORT) {\n\t\tres.result = DMA_TRANS_ABORTED;\n\t} else {\n\t\tcomplete = 0;\n\t}\n\n\ttx = &desc->txd;\n\tif (complete && tx->cookie) {\n\t\tdma_cookie_complete(tx);\n\t\tdma_descriptor_unmap(tx);\n\t\tdmaengine_desc_get_callback_invoke(tx, &res);\n\t\ttx->callback = NULL;\n\t\ttx->callback_result = NULL;\n\t}\n\n\tif (free_desc)\n\t\tidxd_free_desc(desc->wq, desc);\n}\n\nstatic void op_flag_setup(unsigned long flags, u32 *desc_flags)\n{\n\t*desc_flags = IDXD_OP_FLAG_CRAV | IDXD_OP_FLAG_RCR;\n\tif (flags & DMA_PREP_INTERRUPT)\n\t\t*desc_flags |= IDXD_OP_FLAG_RCI;\n}\n\nstatic inline void idxd_prep_desc_common(struct idxd_wq *wq,\n\t\t\t\t\t struct dsa_hw_desc *hw, char opcode,\n\t\t\t\t\t u64 addr_f1, u64 addr_f2, u64 len,\n\t\t\t\t\t u64 compl, u32 flags)\n{\n\thw->flags = flags;\n\thw->opcode = opcode;\n\thw->src_addr = addr_f1;\n\thw->dst_addr = addr_f2;\n\thw->xfer_size = len;\n\t \n\thw->priv = 0;\n\thw->completion_addr = compl;\n}\n\nstatic struct dma_async_tx_descriptor *\nidxd_dma_prep_interrupt(struct dma_chan *c, unsigned long flags)\n{\n\tstruct idxd_wq *wq = to_idxd_wq(c);\n\tu32 desc_flags;\n\tstruct idxd_desc *desc;\n\n\tif (wq->state != IDXD_WQ_ENABLED)\n\t\treturn NULL;\n\n\top_flag_setup(flags, &desc_flags);\n\tdesc = idxd_alloc_desc(wq, IDXD_OP_BLOCK);\n\tif (IS_ERR(desc))\n\t\treturn NULL;\n\n\tidxd_prep_desc_common(wq, desc->hw, DSA_OPCODE_NOOP,\n\t\t\t      0, 0, 0, desc->compl_dma, desc_flags);\n\tdesc->txd.flags = flags;\n\treturn &desc->txd;\n}\n\nstatic struct dma_async_tx_descriptor *\nidxd_dma_submit_memcpy(struct dma_chan *c, dma_addr_t dma_dest,\n\t\t       dma_addr_t dma_src, size_t len, unsigned long flags)\n{\n\tstruct idxd_wq *wq = to_idxd_wq(c);\n\tu32 desc_flags;\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct idxd_desc *desc;\n\n\tif (wq->state != IDXD_WQ_ENABLED)\n\t\treturn NULL;\n\n\tif (len > idxd->max_xfer_bytes)\n\t\treturn NULL;\n\n\top_flag_setup(flags, &desc_flags);\n\tdesc = idxd_alloc_desc(wq, IDXD_OP_BLOCK);\n\tif (IS_ERR(desc))\n\t\treturn NULL;\n\n\tidxd_prep_desc_common(wq, desc->hw, DSA_OPCODE_MEMMOVE,\n\t\t\t      dma_src, dma_dest, len, desc->compl_dma,\n\t\t\t      desc_flags);\n\n\tdesc->txd.flags = flags;\n\n\treturn &desc->txd;\n}\n\nstatic int idxd_dma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct idxd_wq *wq = to_idxd_wq(chan);\n\tstruct device *dev = &wq->idxd->pdev->dev;\n\n\tidxd_wq_get(wq);\n\tdev_dbg(dev, \"%s: client_count: %d\\n\", __func__,\n\t\tidxd_wq_refcount(wq));\n\treturn 0;\n}\n\nstatic void idxd_dma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct idxd_wq *wq = to_idxd_wq(chan);\n\tstruct device *dev = &wq->idxd->pdev->dev;\n\n\tidxd_wq_put(wq);\n\tdev_dbg(dev, \"%s: client_count: %d\\n\", __func__,\n\t\tidxd_wq_refcount(wq));\n}\n\nstatic enum dma_status idxd_dma_tx_status(struct dma_chan *dma_chan,\n\t\t\t\t\t  dma_cookie_t cookie,\n\t\t\t\t\t  struct dma_tx_state *txstate)\n{\n\treturn DMA_OUT_OF_ORDER;\n}\n\n \nstatic void idxd_dma_issue_pending(struct dma_chan *dma_chan)\n{\n}\n\nstatic dma_cookie_t idxd_dma_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct dma_chan *c = tx->chan;\n\tstruct idxd_wq *wq = to_idxd_wq(c);\n\tdma_cookie_t cookie;\n\tint rc;\n\tstruct idxd_desc *desc = container_of(tx, struct idxd_desc, txd);\n\n\tcookie = dma_cookie_assign(tx);\n\n\trc = idxd_submit_desc(wq, desc);\n\tif (rc < 0) {\n\t\tidxd_free_desc(wq, desc);\n\t\treturn rc;\n\t}\n\n\treturn cookie;\n}\n\nstatic void idxd_dma_release(struct dma_device *device)\n{\n\tstruct idxd_dma_dev *idxd_dma = container_of(device, struct idxd_dma_dev, dma);\n\n\tkfree(idxd_dma);\n}\n\nint idxd_register_dma_device(struct idxd_device *idxd)\n{\n\tstruct idxd_dma_dev *idxd_dma;\n\tstruct dma_device *dma;\n\tstruct device *dev = &idxd->pdev->dev;\n\tint rc;\n\n\tidxd_dma = kzalloc_node(sizeof(*idxd_dma), GFP_KERNEL, dev_to_node(dev));\n\tif (!idxd_dma)\n\t\treturn -ENOMEM;\n\n\tdma = &idxd_dma->dma;\n\tINIT_LIST_HEAD(&dma->channels);\n\tdma->dev = dev;\n\n\tdma_cap_set(DMA_INTERRUPT, dma->cap_mask);\n\tdma_cap_set(DMA_PRIVATE, dma->cap_mask);\n\tdma_cap_set(DMA_COMPLETION_NO_ORDER, dma->cap_mask);\n\tdma->device_release = idxd_dma_release;\n\n\tdma->device_prep_dma_interrupt = idxd_dma_prep_interrupt;\n\tif (idxd->hw.opcap.bits[0] & IDXD_OPCAP_MEMMOVE) {\n\t\tdma_cap_set(DMA_MEMCPY, dma->cap_mask);\n\t\tdma->device_prep_dma_memcpy = idxd_dma_submit_memcpy;\n\t}\n\n\tdma->device_tx_status = idxd_dma_tx_status;\n\tdma->device_issue_pending = idxd_dma_issue_pending;\n\tdma->device_alloc_chan_resources = idxd_dma_alloc_chan_resources;\n\tdma->device_free_chan_resources = idxd_dma_free_chan_resources;\n\n\trc = dma_async_device_register(dma);\n\tif (rc < 0) {\n\t\tkfree(idxd_dma);\n\t\treturn rc;\n\t}\n\n\tidxd_dma->idxd = idxd;\n\t \n\tidxd->idxd_dma = idxd_dma;\n\treturn 0;\n}\n\nvoid idxd_unregister_dma_device(struct idxd_device *idxd)\n{\n\tdma_async_device_unregister(&idxd->idxd_dma->dma);\n}\n\nstatic int idxd_register_dma_channel(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct dma_device *dma = &idxd->idxd_dma->dma;\n\tstruct device *dev = &idxd->pdev->dev;\n\tstruct idxd_dma_chan *idxd_chan;\n\tstruct dma_chan *chan;\n\tint rc, i;\n\n\tidxd_chan = kzalloc_node(sizeof(*idxd_chan), GFP_KERNEL, dev_to_node(dev));\n\tif (!idxd_chan)\n\t\treturn -ENOMEM;\n\n\tchan = &idxd_chan->chan;\n\tchan->device = dma;\n\tlist_add_tail(&chan->device_node, &dma->channels);\n\n\tfor (i = 0; i < wq->num_descs; i++) {\n\t\tstruct idxd_desc *desc = wq->descs[i];\n\n\t\tdma_async_tx_descriptor_init(&desc->txd, chan);\n\t\tdesc->txd.tx_submit = idxd_dma_tx_submit;\n\t}\n\n\trc = dma_async_device_channel_register(dma, chan);\n\tif (rc < 0) {\n\t\tkfree(idxd_chan);\n\t\treturn rc;\n\t}\n\n\twq->idxd_chan = idxd_chan;\n\tidxd_chan->wq = wq;\n\tget_device(wq_confdev(wq));\n\n\treturn 0;\n}\n\nstatic void idxd_unregister_dma_channel(struct idxd_wq *wq)\n{\n\tstruct idxd_dma_chan *idxd_chan = wq->idxd_chan;\n\tstruct dma_chan *chan = &idxd_chan->chan;\n\tstruct idxd_dma_dev *idxd_dma = wq->idxd->idxd_dma;\n\n\tdma_async_device_channel_unregister(&idxd_dma->dma, chan);\n\tlist_del(&chan->device_node);\n\tkfree(wq->idxd_chan);\n\twq->idxd_chan = NULL;\n\tput_device(wq_confdev(wq));\n}\n\nstatic int idxd_dmaengine_drv_probe(struct idxd_dev *idxd_dev)\n{\n\tstruct device *dev = &idxd_dev->conf_dev;\n\tstruct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);\n\tstruct idxd_device *idxd = wq->idxd;\n\tint rc;\n\n\tif (idxd->state != IDXD_DEV_ENABLED)\n\t\treturn -ENXIO;\n\n\tmutex_lock(&wq->wq_lock);\n\twq->type = IDXD_WQT_KERNEL;\n\n\trc = drv_enable_wq(wq);\n\tif (rc < 0) {\n\t\tdev_dbg(dev, \"Enable wq %d failed: %d\\n\", wq->id, rc);\n\t\trc = -ENXIO;\n\t\tgoto err;\n\t}\n\n\trc = idxd_register_dma_channel(wq);\n\tif (rc < 0) {\n\t\tidxd->cmd_status = IDXD_SCMD_DMA_CHAN_ERR;\n\t\tdev_dbg(dev, \"Failed to register dma channel\\n\");\n\t\tgoto err_dma;\n\t}\n\n\tidxd->cmd_status = 0;\n\tmutex_unlock(&wq->wq_lock);\n\treturn 0;\n\nerr_dma:\n\tdrv_disable_wq(wq);\nerr:\n\twq->type = IDXD_WQT_NONE;\n\tmutex_unlock(&wq->wq_lock);\n\treturn rc;\n}\n\nstatic void idxd_dmaengine_drv_remove(struct idxd_dev *idxd_dev)\n{\n\tstruct idxd_wq *wq = idxd_dev_to_wq(idxd_dev);\n\n\tmutex_lock(&wq->wq_lock);\n\t__idxd_wq_quiesce(wq);\n\tidxd_unregister_dma_channel(wq);\n\tdrv_disable_wq(wq);\n\tmutex_unlock(&wq->wq_lock);\n}\n\nstatic enum idxd_dev_type dev_types[] = {\n\tIDXD_DEV_WQ,\n\tIDXD_DEV_NONE,\n};\n\nstruct idxd_device_driver idxd_dmaengine_drv = {\n\t.probe = idxd_dmaengine_drv_probe,\n\t.remove = idxd_dmaengine_drv_remove,\n\t.name = \"dmaengine\",\n\t.type = dev_types,\n};\nEXPORT_SYMBOL_GPL(idxd_dmaengine_drv);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}