{
  "module_name": "irq.c",
  "hash_id": "e70f319d7deec39483a547e65bc7eecb3b1dac07870d807ffe716429fec1cef6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/idxd/irq.c",
  "human_readable_source": "\n \n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/dmaengine.h>\n#include <linux/delay.h>\n#include <linux/iommu.h>\n#include <linux/sched/mm.h>\n#include <uapi/linux/idxd.h>\n#include \"../dmaengine.h\"\n#include \"idxd.h\"\n#include \"registers.h\"\n\nenum irq_work_type {\n\tIRQ_WORK_NORMAL = 0,\n\tIRQ_WORK_PROCESS_FAULT,\n};\n\nstruct idxd_resubmit {\n\tstruct work_struct work;\n\tstruct idxd_desc *desc;\n};\n\nstruct idxd_int_handle_revoke {\n\tstruct work_struct work;\n\tstruct idxd_device *idxd;\n};\n\nstatic void idxd_device_reinit(struct work_struct *work)\n{\n\tstruct idxd_device *idxd = container_of(work, struct idxd_device, work);\n\tstruct device *dev = &idxd->pdev->dev;\n\tint rc, i;\n\n\tidxd_device_reset(idxd);\n\trc = idxd_device_config(idxd);\n\tif (rc < 0)\n\t\tgoto out;\n\n\trc = idxd_device_enable(idxd);\n\tif (rc < 0)\n\t\tgoto out;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tif (test_bit(i, idxd->wq_enable_map)) {\n\t\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\t\trc = idxd_wq_enable(wq);\n\t\t\tif (rc < 0) {\n\t\t\t\tclear_bit(i, idxd->wq_enable_map);\n\t\t\t\tdev_warn(dev, \"Unable to re-enable wq %s\\n\",\n\t\t\t\t\t dev_name(wq_confdev(wq)));\n\t\t\t}\n\t\t}\n\t}\n\n\treturn;\n\n out:\n\tidxd_device_clear_state(idxd);\n}\n\n \nstatic void idxd_int_handle_revoke_drain(struct idxd_irq_entry *ie)\n{\n\tstruct idxd_wq *wq = ie_to_wq(ie);\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tstruct dsa_hw_desc desc = {};\n\tvoid __iomem *portal;\n\tint rc;\n\n\t \n\tdesc.flags = IDXD_OP_FLAG_RCI;\n\tdesc.opcode = DSA_OPCODE_DRAIN;\n\tdesc.priv = 1;\n\n\tif (ie->pasid != IOMMU_PASID_INVALID)\n\t\tdesc.pasid = ie->pasid;\n\tdesc.int_handle = ie->int_handle;\n\tportal = idxd_wq_portal_addr(wq);\n\n\t \n\twmb();\n\tif (wq_dedicated(wq)) {\n\t\tiosubmit_cmds512(portal, &desc, 1);\n\t} else {\n\t\trc = idxd_enqcmds(wq, portal, &desc);\n\t\t \n\t\tif (rc < 0)\n\t\t\tdev_warn(dev, \"Failed to submit drain desc on wq %d\\n\", wq->id);\n\t}\n}\n\nstatic void idxd_abort_invalid_int_handle_descs(struct idxd_irq_entry *ie)\n{\n\tLIST_HEAD(flist);\n\tstruct idxd_desc *d, *t;\n\tstruct llist_node *head;\n\n\tspin_lock(&ie->list_lock);\n\thead = llist_del_all(&ie->pending_llist);\n\tif (head) {\n\t\tllist_for_each_entry_safe(d, t, head, llnode)\n\t\t\tlist_add_tail(&d->list, &ie->work_list);\n\t}\n\n\tlist_for_each_entry_safe(d, t, &ie->work_list, list) {\n\t\tif (d->completion->status == DSA_COMP_INT_HANDLE_INVAL)\n\t\t\tlist_move_tail(&d->list, &flist);\n\t}\n\tspin_unlock(&ie->list_lock);\n\n\tlist_for_each_entry_safe(d, t, &flist, list) {\n\t\tlist_del(&d->list);\n\t\tidxd_dma_complete_txd(d, IDXD_COMPLETE_ABORT, true);\n\t}\n}\n\nstatic void idxd_int_handle_revoke(struct work_struct *work)\n{\n\tstruct idxd_int_handle_revoke *revoke =\n\t\tcontainer_of(work, struct idxd_int_handle_revoke, work);\n\tstruct idxd_device *idxd = revoke->idxd;\n\tstruct pci_dev *pdev = idxd->pdev;\n\tstruct device *dev = &pdev->dev;\n\tint i, new_handle, rc;\n\n\tif (!idxd->request_int_handles) {\n\t\tkfree(revoke);\n\t\tdev_warn(dev, \"Unexpected int handle refresh interrupt.\\n\");\n\t\treturn;\n\t}\n\n\t \n\tfor (i = 1; i < idxd->irq_cnt; i++) {\n\t\tstruct idxd_irq_entry *ie = idxd_get_ie(idxd, i);\n\t\tstruct idxd_wq *wq = ie_to_wq(ie);\n\n\t\tif (ie->int_handle == INVALID_INT_HANDLE)\n\t\t\tcontinue;\n\n\t\trc = idxd_device_request_int_handle(idxd, i, &new_handle, IDXD_IRQ_MSIX);\n\t\tif (rc < 0) {\n\t\t\tdev_warn(dev, \"get int handle %d failed: %d\\n\", i, rc);\n\t\t\t \n\t\t\tie->int_handle = INVALID_INT_HANDLE;\n\t\t\tidxd_wq_quiesce(wq);\n\t\t\tidxd_abort_invalid_int_handle_descs(ie);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (ie->int_handle == new_handle)\n\t\t\tcontinue;\n\n\t\tif (wq->state != IDXD_WQ_ENABLED || wq->type != IDXD_WQT_KERNEL) {\n\t\t\t \n\t\t\tie->int_handle = new_handle;\n\t\t\tcontinue;\n\t\t}\n\n\t\tmutex_lock(&wq->wq_lock);\n\t\treinit_completion(&wq->wq_resurrect);\n\n\t\t \n\t\tpercpu_ref_kill(&wq->wq_active);\n\n\t\t \n\t\twait_for_completion(&wq->wq_dead);\n\n\t\tie->int_handle = new_handle;\n\n\t\t \n\t\tpercpu_ref_reinit(&wq->wq_active);\n\t\tcomplete_all(&wq->wq_resurrect);\n\t\tmutex_unlock(&wq->wq_lock);\n\n\t\t \n\t\tif (wq_dedicated(wq))\n\t\t\tudelay(100);\n\t\tidxd_int_handle_revoke_drain(ie);\n\t}\n\tkfree(revoke);\n}\n\nstatic void idxd_evl_fault_work(struct work_struct *work)\n{\n\tstruct idxd_evl_fault *fault = container_of(work, struct idxd_evl_fault, work);\n\tstruct idxd_wq *wq = fault->wq;\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tstruct idxd_evl *evl = idxd->evl;\n\tstruct __evl_entry *entry_head = fault->entry;\n\tvoid *cr = (void *)entry_head + idxd->data->evl_cr_off;\n\tint cr_size = idxd->data->compl_size;\n\tu8 *status = (u8 *)cr + idxd->data->cr_status_off;\n\tu8 *result = (u8 *)cr + idxd->data->cr_result_off;\n\tint copied, copy_size;\n\tbool *bf;\n\n\tswitch (fault->status) {\n\tcase DSA_COMP_CRA_XLAT:\n\t\tif (entry_head->batch && entry_head->first_err_in_batch)\n\t\t\tevl->batch_fail[entry_head->batch_id] = false;\n\n\t\tcopy_size = cr_size;\n\t\tidxd_user_counter_increment(wq, entry_head->pasid, COUNTER_FAULTS);\n\t\tbreak;\n\tcase DSA_COMP_BATCH_EVL_ERR:\n\t\tbf = &evl->batch_fail[entry_head->batch_id];\n\n\t\tcopy_size = entry_head->rcr || *bf ? cr_size : 0;\n\t\tif (*bf) {\n\t\t\tif (*status == DSA_COMP_SUCCESS)\n\t\t\t\t*status = DSA_COMP_BATCH_FAIL;\n\t\t\t*result = 1;\n\t\t\t*bf = false;\n\t\t}\n\t\tidxd_user_counter_increment(wq, entry_head->pasid, COUNTER_FAULTS);\n\t\tbreak;\n\tcase DSA_COMP_DRAIN_EVL:\n\t\tcopy_size = cr_size;\n\t\tbreak;\n\tdefault:\n\t\tcopy_size = 0;\n\t\tdev_dbg_ratelimited(dev, \"Unrecognized error code: %#x\\n\", fault->status);\n\t\tbreak;\n\t}\n\n\tif (copy_size == 0)\n\t\treturn;\n\n\t \n\tcopied = idxd_copy_cr(wq, entry_head->pasid, entry_head->fault_addr,\n\t\t\t      cr, copy_size);\n\t \n\tswitch (fault->status) {\n\tcase DSA_COMP_CRA_XLAT:\n\t\tif (copied != copy_size) {\n\t\t\tidxd_user_counter_increment(wq, entry_head->pasid, COUNTER_FAULT_FAILS);\n\t\t\tdev_dbg_ratelimited(dev, \"Failed to write to completion record: (%d:%d)\\n\",\n\t\t\t\t\t    copy_size, copied);\n\t\t\tif (entry_head->batch)\n\t\t\t\tevl->batch_fail[entry_head->batch_id] = true;\n\t\t}\n\t\tbreak;\n\tcase DSA_COMP_BATCH_EVL_ERR:\n\t\tif (copied != copy_size) {\n\t\t\tidxd_user_counter_increment(wq, entry_head->pasid, COUNTER_FAULT_FAILS);\n\t\t\tdev_dbg_ratelimited(dev, \"Failed to write to batch completion record: (%d:%d)\\n\",\n\t\t\t\t\t    copy_size, copied);\n\t\t}\n\t\tbreak;\n\tcase DSA_COMP_DRAIN_EVL:\n\t\tif (copied != copy_size)\n\t\t\tdev_dbg_ratelimited(dev, \"Failed to write to drain completion record: (%d:%d)\\n\",\n\t\t\t\t\t    copy_size, copied);\n\t\tbreak;\n\t}\n\n\tkmem_cache_free(idxd->evl_cache, fault);\n}\n\nstatic void process_evl_entry(struct idxd_device *idxd,\n\t\t\t      struct __evl_entry *entry_head, unsigned int index)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tstruct idxd_evl *evl = idxd->evl;\n\tu8 status;\n\n\tif (test_bit(index, evl->bmap)) {\n\t\tclear_bit(index, evl->bmap);\n\t} else {\n\t\tstatus = DSA_COMP_STATUS(entry_head->error);\n\n\t\tif (status == DSA_COMP_CRA_XLAT || status == DSA_COMP_DRAIN_EVL ||\n\t\t    status == DSA_COMP_BATCH_EVL_ERR) {\n\t\t\tstruct idxd_evl_fault *fault;\n\t\t\tint ent_size = evl_ent_size(idxd);\n\n\t\t\tif (entry_head->rci)\n\t\t\t\tdev_dbg(dev, \"Completion Int Req set, ignoring!\\n\");\n\n\t\t\tif (!entry_head->rcr && status == DSA_COMP_DRAIN_EVL)\n\t\t\t\treturn;\n\n\t\t\tfault = kmem_cache_alloc(idxd->evl_cache, GFP_ATOMIC);\n\t\t\tif (fault) {\n\t\t\t\tstruct idxd_wq *wq = idxd->wqs[entry_head->wq_idx];\n\n\t\t\t\tfault->wq = wq;\n\t\t\t\tfault->status = status;\n\t\t\t\tmemcpy(&fault->entry, entry_head, ent_size);\n\t\t\t\tINIT_WORK(&fault->work, idxd_evl_fault_work);\n\t\t\t\tqueue_work(wq->wq, &fault->work);\n\t\t\t} else {\n\t\t\t\tdev_warn(dev, \"Failed to service fault work.\\n\");\n\t\t\t}\n\t\t} else {\n\t\t\tdev_warn_ratelimited(dev, \"Device error %#x operation: %#x fault addr: %#llx\\n\",\n\t\t\t\t\t     status, entry_head->operation,\n\t\t\t\t\t     entry_head->fault_addr);\n\t\t}\n\t}\n}\n\nstatic void process_evl_entries(struct idxd_device *idxd)\n{\n\tunion evl_status_reg evl_status;\n\tunsigned int h, t;\n\tstruct idxd_evl *evl = idxd->evl;\n\tstruct __evl_entry *entry_head;\n\tunsigned int ent_size = evl_ent_size(idxd);\n\tu32 size;\n\n\tevl_status.bits = 0;\n\tevl_status.int_pending = 1;\n\n\tspin_lock(&evl->lock);\n\t \n\tiowrite32(evl_status.bits_upper32,\n\t\t  idxd->reg_base + IDXD_EVLSTATUS_OFFSET + sizeof(u32));\n\th = evl->head;\n\tevl_status.bits = ioread64(idxd->reg_base + IDXD_EVLSTATUS_OFFSET);\n\tt = evl_status.tail;\n\tsize = idxd->evl->size;\n\n\twhile (h != t) {\n\t\tentry_head = (struct __evl_entry *)(evl->log + (h * ent_size));\n\t\tprocess_evl_entry(idxd, entry_head, h);\n\t\th = (h + 1) % size;\n\t}\n\n\tevl->head = h;\n\tevl_status.head = h;\n\tiowrite32(evl_status.bits_lower32, idxd->reg_base + IDXD_EVLSTATUS_OFFSET);\n\tspin_unlock(&evl->lock);\n}\n\nirqreturn_t idxd_misc_thread(int vec, void *data)\n{\n\tstruct idxd_irq_entry *irq_entry = data;\n\tstruct idxd_device *idxd = ie_to_idxd(irq_entry);\n\tstruct device *dev = &idxd->pdev->dev;\n\tunion gensts_reg gensts;\n\tu32 val = 0;\n\tint i;\n\tbool err = false;\n\tu32 cause;\n\n\tcause = ioread32(idxd->reg_base + IDXD_INTCAUSE_OFFSET);\n\tif (!cause)\n\t\treturn IRQ_NONE;\n\n\tiowrite32(cause, idxd->reg_base + IDXD_INTCAUSE_OFFSET);\n\n\tif (cause & IDXD_INTC_HALT_STATE)\n\t\tgoto halt;\n\n\tif (cause & IDXD_INTC_ERR) {\n\t\tspin_lock(&idxd->dev_lock);\n\t\tfor (i = 0; i < 4; i++)\n\t\t\tidxd->sw_err.bits[i] = ioread64(idxd->reg_base +\n\t\t\t\t\tIDXD_SWERR_OFFSET + i * sizeof(u64));\n\n\t\tiowrite64(idxd->sw_err.bits[0] & IDXD_SWERR_ACK,\n\t\t\t  idxd->reg_base + IDXD_SWERR_OFFSET);\n\n\t\tif (idxd->sw_err.valid && idxd->sw_err.wq_idx_valid) {\n\t\t\tint id = idxd->sw_err.wq_idx;\n\t\t\tstruct idxd_wq *wq = idxd->wqs[id];\n\n\t\t\tif (wq->type == IDXD_WQT_USER)\n\t\t\t\twake_up_interruptible(&wq->err_queue);\n\t\t} else {\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\t\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\t\t\tif (wq->type == IDXD_WQT_USER)\n\t\t\t\t\twake_up_interruptible(&wq->err_queue);\n\t\t\t}\n\t\t}\n\n\t\tspin_unlock(&idxd->dev_lock);\n\t\tval |= IDXD_INTC_ERR;\n\n\t\tfor (i = 0; i < 4; i++)\n\t\t\tdev_warn(dev, \"err[%d]: %#16.16llx\\n\",\n\t\t\t\t i, idxd->sw_err.bits[i]);\n\t\terr = true;\n\t}\n\n\tif (cause & IDXD_INTC_INT_HANDLE_REVOKED) {\n\t\tstruct idxd_int_handle_revoke *revoke;\n\n\t\tval |= IDXD_INTC_INT_HANDLE_REVOKED;\n\n\t\trevoke = kzalloc(sizeof(*revoke), GFP_ATOMIC);\n\t\tif (revoke) {\n\t\t\trevoke->idxd = idxd;\n\t\t\tINIT_WORK(&revoke->work, idxd_int_handle_revoke);\n\t\t\tqueue_work(idxd->wq, &revoke->work);\n\n\t\t} else {\n\t\t\tdev_err(dev, \"Failed to allocate work for int handle revoke\\n\");\n\t\t\tidxd_wqs_quiesce(idxd);\n\t\t}\n\t}\n\n\tif (cause & IDXD_INTC_CMD) {\n\t\tval |= IDXD_INTC_CMD;\n\t\tcomplete(idxd->cmd_done);\n\t}\n\n\tif (cause & IDXD_INTC_OCCUPY) {\n\t\t \n\t\tval |= IDXD_INTC_OCCUPY;\n\t}\n\n\tif (cause & IDXD_INTC_PERFMON_OVFL) {\n\t\tval |= IDXD_INTC_PERFMON_OVFL;\n\t\tperfmon_counter_overflow(idxd);\n\t}\n\n\tif (cause & IDXD_INTC_EVL) {\n\t\tval |= IDXD_INTC_EVL;\n\t\tprocess_evl_entries(idxd);\n\t}\n\n\tval ^= cause;\n\tif (val)\n\t\tdev_warn_once(dev, \"Unexpected interrupt cause bits set: %#x\\n\",\n\t\t\t      val);\n\n\tif (!err)\n\t\tgoto out;\n\nhalt:\n\tgensts.bits = ioread32(idxd->reg_base + IDXD_GENSTATS_OFFSET);\n\tif (gensts.state == IDXD_DEVICE_STATE_HALT) {\n\t\tidxd->state = IDXD_DEV_HALTED;\n\t\tif (gensts.reset_type == IDXD_DEVICE_RESET_SOFTWARE) {\n\t\t\t \n\t\t\tINIT_WORK(&idxd->work, idxd_device_reinit);\n\t\t\tqueue_work(idxd->wq, &idxd->work);\n\t\t} else {\n\t\t\tidxd->state = IDXD_DEV_HALTED;\n\t\t\tidxd_wqs_quiesce(idxd);\n\t\t\tidxd_wqs_unmap_portal(idxd);\n\t\t\tidxd_device_clear_state(idxd);\n\t\t\tdev_err(&idxd->pdev->dev,\n\t\t\t\t\"idxd halted, need %s.\\n\",\n\t\t\t\tgensts.reset_type == IDXD_DEVICE_RESET_FLR ?\n\t\t\t\t\"FLR\" : \"system reset\");\n\t\t}\n\t}\n\nout:\n\treturn IRQ_HANDLED;\n}\n\nstatic void idxd_int_handle_resubmit_work(struct work_struct *work)\n{\n\tstruct idxd_resubmit *irw = container_of(work, struct idxd_resubmit, work);\n\tstruct idxd_desc *desc = irw->desc;\n\tstruct idxd_wq *wq = desc->wq;\n\tint rc;\n\n\tdesc->completion->status = 0;\n\trc = idxd_submit_desc(wq, desc);\n\tif (rc < 0) {\n\t\tdev_dbg(&wq->idxd->pdev->dev, \"Failed to resubmit desc %d to wq %d.\\n\",\n\t\t\tdesc->id, wq->id);\n\t\t \n\t\tif (rc != -EAGAIN) {\n\t\t\tdesc->completion->status = IDXD_COMP_DESC_ABORT;\n\t\t\tidxd_dma_complete_txd(desc, IDXD_COMPLETE_ABORT, false);\n\t\t}\n\t\tidxd_free_desc(wq, desc);\n\t}\n\tkfree(irw);\n}\n\nbool idxd_queue_int_handle_resubmit(struct idxd_desc *desc)\n{\n\tstruct idxd_wq *wq = desc->wq;\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct idxd_resubmit *irw;\n\n\tirw = kzalloc(sizeof(*irw), GFP_KERNEL);\n\tif (!irw)\n\t\treturn false;\n\n\tirw->desc = desc;\n\tINIT_WORK(&irw->work, idxd_int_handle_resubmit_work);\n\tqueue_work(idxd->wq, &irw->work);\n\treturn true;\n}\n\nstatic void irq_process_pending_llist(struct idxd_irq_entry *irq_entry)\n{\n\tstruct idxd_desc *desc, *t;\n\tstruct llist_node *head;\n\n\thead = llist_del_all(&irq_entry->pending_llist);\n\tif (!head)\n\t\treturn;\n\n\tllist_for_each_entry_safe(desc, t, head, llnode) {\n\t\tu8 status = desc->completion->status & DSA_COMP_STATUS_MASK;\n\n\t\tif (status) {\n\t\t\t \n\t\t\tif (unlikely(desc->completion->status == IDXD_COMP_DESC_ABORT)) {\n\t\t\t\tidxd_dma_complete_txd(desc, IDXD_COMPLETE_ABORT, true);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tidxd_dma_complete_txd(desc, IDXD_COMPLETE_NORMAL, true);\n\t\t} else {\n\t\t\tspin_lock(&irq_entry->list_lock);\n\t\t\tlist_add_tail(&desc->list,\n\t\t\t\t      &irq_entry->work_list);\n\t\t\tspin_unlock(&irq_entry->list_lock);\n\t\t}\n\t}\n}\n\nstatic void irq_process_work_list(struct idxd_irq_entry *irq_entry)\n{\n\tLIST_HEAD(flist);\n\tstruct idxd_desc *desc, *n;\n\n\t \n\tspin_lock(&irq_entry->list_lock);\n\tif (list_empty(&irq_entry->work_list)) {\n\t\tspin_unlock(&irq_entry->list_lock);\n\t\treturn;\n\t}\n\n\tlist_for_each_entry_safe(desc, n, &irq_entry->work_list, list) {\n\t\tif (desc->completion->status) {\n\t\t\tlist_move_tail(&desc->list, &flist);\n\t\t}\n\t}\n\n\tspin_unlock(&irq_entry->list_lock);\n\n\tlist_for_each_entry(desc, &flist, list) {\n\t\t \n\t\tif (unlikely(desc->completion->status == IDXD_COMP_DESC_ABORT)) {\n\t\t\tidxd_dma_complete_txd(desc, IDXD_COMPLETE_ABORT, true);\n\t\t\tcontinue;\n\t\t}\n\n\t\tidxd_dma_complete_txd(desc, IDXD_COMPLETE_NORMAL, true);\n\t}\n}\n\nirqreturn_t idxd_wq_thread(int irq, void *data)\n{\n\tstruct idxd_irq_entry *irq_entry = data;\n\n\t \n\tirq_process_work_list(irq_entry);\n\tirq_process_pending_llist(irq_entry);\n\n\treturn IRQ_HANDLED;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}