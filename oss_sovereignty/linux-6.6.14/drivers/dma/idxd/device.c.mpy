{
  "module_name": "device.c",
  "hash_id": "41e9eff489519987498cf760e45ff6358b7ea17004a52611135fd18e8d659da2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/idxd/device.c",
  "human_readable_source": "\n \n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/dmaengine.h>\n#include <linux/irq.h>\n#include <uapi/linux/idxd.h>\n#include \"../dmaengine.h\"\n#include \"idxd.h\"\n#include \"registers.h\"\n\nstatic void idxd_cmd_exec(struct idxd_device *idxd, int cmd_code, u32 operand,\n\t\t\t  u32 *status);\nstatic void idxd_device_wqs_clear_state(struct idxd_device *idxd);\nstatic void idxd_wq_disable_cleanup(struct idxd_wq *wq);\n\n \nvoid idxd_unmask_error_interrupts(struct idxd_device *idxd)\n{\n\tunion genctrl_reg genctrl;\n\n\tgenctrl.bits = ioread32(idxd->reg_base + IDXD_GENCTRL_OFFSET);\n\tgenctrl.softerr_int_en = 1;\n\tgenctrl.halt_int_en = 1;\n\tiowrite32(genctrl.bits, idxd->reg_base + IDXD_GENCTRL_OFFSET);\n}\n\nvoid idxd_mask_error_interrupts(struct idxd_device *idxd)\n{\n\tunion genctrl_reg genctrl;\n\n\tgenctrl.bits = ioread32(idxd->reg_base + IDXD_GENCTRL_OFFSET);\n\tgenctrl.softerr_int_en = 0;\n\tgenctrl.halt_int_en = 0;\n\tiowrite32(genctrl.bits, idxd->reg_base + IDXD_GENCTRL_OFFSET);\n}\n\nstatic void free_hw_descs(struct idxd_wq *wq)\n{\n\tint i;\n\n\tfor (i = 0; i < wq->num_descs; i++)\n\t\tkfree(wq->hw_descs[i]);\n\n\tkfree(wq->hw_descs);\n}\n\nstatic int alloc_hw_descs(struct idxd_wq *wq, int num)\n{\n\tstruct device *dev = &wq->idxd->pdev->dev;\n\tint i;\n\tint node = dev_to_node(dev);\n\n\twq->hw_descs = kcalloc_node(num, sizeof(struct dsa_hw_desc *),\n\t\t\t\t    GFP_KERNEL, node);\n\tif (!wq->hw_descs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num; i++) {\n\t\twq->hw_descs[i] = kzalloc_node(sizeof(*wq->hw_descs[i]),\n\t\t\t\t\t       GFP_KERNEL, node);\n\t\tif (!wq->hw_descs[i]) {\n\t\t\tfree_hw_descs(wq);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void free_descs(struct idxd_wq *wq)\n{\n\tint i;\n\n\tfor (i = 0; i < wq->num_descs; i++)\n\t\tkfree(wq->descs[i]);\n\n\tkfree(wq->descs);\n}\n\nstatic int alloc_descs(struct idxd_wq *wq, int num)\n{\n\tstruct device *dev = &wq->idxd->pdev->dev;\n\tint i;\n\tint node = dev_to_node(dev);\n\n\twq->descs = kcalloc_node(num, sizeof(struct idxd_desc *),\n\t\t\t\t GFP_KERNEL, node);\n\tif (!wq->descs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num; i++) {\n\t\twq->descs[i] = kzalloc_node(sizeof(*wq->descs[i]),\n\t\t\t\t\t    GFP_KERNEL, node);\n\t\tif (!wq->descs[i]) {\n\t\t\tfree_descs(wq);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint idxd_wq_alloc_resources(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tint rc, num_descs, i;\n\n\tif (wq->type != IDXD_WQT_KERNEL)\n\t\treturn 0;\n\n\tnum_descs = wq_dedicated(wq) ? wq->size : wq->threshold;\n\twq->num_descs = num_descs;\n\n\trc = alloc_hw_descs(wq, num_descs);\n\tif (rc < 0)\n\t\treturn rc;\n\n\twq->compls_size = num_descs * idxd->data->compl_size;\n\twq->compls = dma_alloc_coherent(dev, wq->compls_size, &wq->compls_addr, GFP_KERNEL);\n\tif (!wq->compls) {\n\t\trc = -ENOMEM;\n\t\tgoto fail_alloc_compls;\n\t}\n\n\trc = alloc_descs(wq, num_descs);\n\tif (rc < 0)\n\t\tgoto fail_alloc_descs;\n\n\trc = sbitmap_queue_init_node(&wq->sbq, num_descs, -1, false, GFP_KERNEL,\n\t\t\t\t     dev_to_node(dev));\n\tif (rc < 0)\n\t\tgoto fail_sbitmap_init;\n\n\tfor (i = 0; i < num_descs; i++) {\n\t\tstruct idxd_desc *desc = wq->descs[i];\n\n\t\tdesc->hw = wq->hw_descs[i];\n\t\tif (idxd->data->type == IDXD_TYPE_DSA)\n\t\t\tdesc->completion = &wq->compls[i];\n\t\telse if (idxd->data->type == IDXD_TYPE_IAX)\n\t\t\tdesc->iax_completion = &wq->iax_compls[i];\n\t\tdesc->compl_dma = wq->compls_addr + idxd->data->compl_size * i;\n\t\tdesc->id = i;\n\t\tdesc->wq = wq;\n\t\tdesc->cpu = -1;\n\t}\n\n\treturn 0;\n\n fail_sbitmap_init:\n\tfree_descs(wq);\n fail_alloc_descs:\n\tdma_free_coherent(dev, wq->compls_size, wq->compls, wq->compls_addr);\n fail_alloc_compls:\n\tfree_hw_descs(wq);\n\treturn rc;\n}\n\nvoid idxd_wq_free_resources(struct idxd_wq *wq)\n{\n\tstruct device *dev = &wq->idxd->pdev->dev;\n\n\tif (wq->type != IDXD_WQT_KERNEL)\n\t\treturn;\n\n\tfree_hw_descs(wq);\n\tfree_descs(wq);\n\tdma_free_coherent(dev, wq->compls_size, wq->compls, wq->compls_addr);\n\tsbitmap_queue_free(&wq->sbq);\n}\n\nint idxd_wq_enable(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 status;\n\n\tif (wq->state == IDXD_WQ_ENABLED) {\n\t\tdev_dbg(dev, \"WQ %d already enabled\\n\", wq->id);\n\t\treturn 0;\n\t}\n\n\tidxd_cmd_exec(idxd, IDXD_CMD_ENABLE_WQ, wq->id, &status);\n\n\tif (status != IDXD_CMDSTS_SUCCESS &&\n\t    status != IDXD_CMDSTS_ERR_WQ_ENABLED) {\n\t\tdev_dbg(dev, \"WQ enable failed: %#x\\n\", status);\n\t\treturn -ENXIO;\n\t}\n\n\twq->state = IDXD_WQ_ENABLED;\n\tset_bit(wq->id, idxd->wq_enable_map);\n\tdev_dbg(dev, \"WQ %d enabled\\n\", wq->id);\n\treturn 0;\n}\n\nint idxd_wq_disable(struct idxd_wq *wq, bool reset_config)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 status, operand;\n\n\tdev_dbg(dev, \"Disabling WQ %d\\n\", wq->id);\n\n\tif (wq->state != IDXD_WQ_ENABLED) {\n\t\tdev_dbg(dev, \"WQ %d in wrong state: %d\\n\", wq->id, wq->state);\n\t\treturn 0;\n\t}\n\n\toperand = BIT(wq->id % 16) | ((wq->id / 16) << 16);\n\tidxd_cmd_exec(idxd, IDXD_CMD_DISABLE_WQ, operand, &status);\n\n\tif (status != IDXD_CMDSTS_SUCCESS) {\n\t\tdev_dbg(dev, \"WQ disable failed: %#x\\n\", status);\n\t\treturn -ENXIO;\n\t}\n\n\tif (reset_config)\n\t\tidxd_wq_disable_cleanup(wq);\n\tclear_bit(wq->id, idxd->wq_enable_map);\n\twq->state = IDXD_WQ_DISABLED;\n\tdev_dbg(dev, \"WQ %d disabled\\n\", wq->id);\n\treturn 0;\n}\n\nvoid idxd_wq_drain(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 operand;\n\n\tif (wq->state != IDXD_WQ_ENABLED) {\n\t\tdev_dbg(dev, \"WQ %d in wrong state: %d\\n\", wq->id, wq->state);\n\t\treturn;\n\t}\n\n\tdev_dbg(dev, \"Draining WQ %d\\n\", wq->id);\n\toperand = BIT(wq->id % 16) | ((wq->id / 16) << 16);\n\tidxd_cmd_exec(idxd, IDXD_CMD_DRAIN_WQ, operand, NULL);\n}\n\nvoid idxd_wq_reset(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 operand;\n\n\tif (wq->state != IDXD_WQ_ENABLED) {\n\t\tdev_dbg(dev, \"WQ %d in wrong state: %d\\n\", wq->id, wq->state);\n\t\treturn;\n\t}\n\n\toperand = BIT(wq->id % 16) | ((wq->id / 16) << 16);\n\tidxd_cmd_exec(idxd, IDXD_CMD_RESET_WQ, operand, NULL);\n\tidxd_wq_disable_cleanup(wq);\n}\n\nint idxd_wq_map_portal(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct pci_dev *pdev = idxd->pdev;\n\tstruct device *dev = &pdev->dev;\n\tresource_size_t start;\n\n\tstart = pci_resource_start(pdev, IDXD_WQ_BAR);\n\tstart += idxd_get_wq_portal_full_offset(wq->id, IDXD_PORTAL_LIMITED);\n\n\twq->portal = devm_ioremap(dev, start, IDXD_PORTAL_SIZE);\n\tif (!wq->portal)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid idxd_wq_unmap_portal(struct idxd_wq *wq)\n{\n\tstruct device *dev = &wq->idxd->pdev->dev;\n\n\tdevm_iounmap(dev, wq->portal);\n\twq->portal = NULL;\n\twq->portal_offset = 0;\n}\n\nvoid idxd_wqs_unmap_portal(struct idxd_device *idxd)\n{\n\tint i;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\tif (wq->portal)\n\t\t\tidxd_wq_unmap_portal(wq);\n\t}\n}\n\nstatic void __idxd_wq_set_pasid_locked(struct idxd_wq *wq, int pasid)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tunion wqcfg wqcfg;\n\tunsigned int offset;\n\n\toffset = WQCFG_OFFSET(idxd, wq->id, WQCFG_PASID_IDX);\n\tspin_lock(&idxd->dev_lock);\n\twqcfg.bits[WQCFG_PASID_IDX] = ioread32(idxd->reg_base + offset);\n\twqcfg.pasid_en = 1;\n\twqcfg.pasid = pasid;\n\twq->wqcfg->bits[WQCFG_PASID_IDX] = wqcfg.bits[WQCFG_PASID_IDX];\n\tiowrite32(wqcfg.bits[WQCFG_PASID_IDX], idxd->reg_base + offset);\n\tspin_unlock(&idxd->dev_lock);\n}\n\nint idxd_wq_set_pasid(struct idxd_wq *wq, int pasid)\n{\n\tint rc;\n\n\trc = idxd_wq_disable(wq, false);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t__idxd_wq_set_pasid_locked(wq, pasid);\n\n\trc = idxd_wq_enable(wq);\n\tif (rc < 0)\n\t\treturn rc;\n\n\treturn 0;\n}\n\nint idxd_wq_disable_pasid(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tint rc;\n\tunion wqcfg wqcfg;\n\tunsigned int offset;\n\n\trc = idxd_wq_disable(wq, false);\n\tif (rc < 0)\n\t\treturn rc;\n\n\toffset = WQCFG_OFFSET(idxd, wq->id, WQCFG_PASID_IDX);\n\tspin_lock(&idxd->dev_lock);\n\twqcfg.bits[WQCFG_PASID_IDX] = ioread32(idxd->reg_base + offset);\n\twqcfg.pasid_en = 0;\n\twqcfg.pasid = 0;\n\tiowrite32(wqcfg.bits[WQCFG_PASID_IDX], idxd->reg_base + offset);\n\tspin_unlock(&idxd->dev_lock);\n\n\trc = idxd_wq_enable(wq);\n\tif (rc < 0)\n\t\treturn rc;\n\n\treturn 0;\n}\n\nstatic void idxd_wq_disable_cleanup(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\n\tlockdep_assert_held(&wq->wq_lock);\n\twq->state = IDXD_WQ_DISABLED;\n\tmemset(wq->wqcfg, 0, idxd->wqcfg_size);\n\twq->type = IDXD_WQT_NONE;\n\twq->threshold = 0;\n\twq->priority = 0;\n\twq->enqcmds_retries = IDXD_ENQCMDS_RETRIES;\n\twq->flags = 0;\n\tmemset(wq->name, 0, WQ_NAME_SIZE);\n\twq->max_xfer_bytes = WQ_DEFAULT_MAX_XFER;\n\tidxd_wq_set_max_batch_size(idxd->data->type, wq, WQ_DEFAULT_MAX_BATCH);\n\tif (wq->opcap_bmap)\n\t\tbitmap_copy(wq->opcap_bmap, idxd->opcap_bmap, IDXD_MAX_OPCAP_BITS);\n}\n\nstatic void idxd_wq_device_reset_cleanup(struct idxd_wq *wq)\n{\n\tlockdep_assert_held(&wq->wq_lock);\n\n\twq->size = 0;\n\twq->group = NULL;\n}\n\nstatic void idxd_wq_ref_release(struct percpu_ref *ref)\n{\n\tstruct idxd_wq *wq = container_of(ref, struct idxd_wq, wq_active);\n\n\tcomplete(&wq->wq_dead);\n}\n\nint idxd_wq_init_percpu_ref(struct idxd_wq *wq)\n{\n\tint rc;\n\n\tmemset(&wq->wq_active, 0, sizeof(wq->wq_active));\n\trc = percpu_ref_init(&wq->wq_active, idxd_wq_ref_release,\n\t\t\t     PERCPU_REF_ALLOW_REINIT, GFP_KERNEL);\n\tif (rc < 0)\n\t\treturn rc;\n\treinit_completion(&wq->wq_dead);\n\treinit_completion(&wq->wq_resurrect);\n\treturn 0;\n}\n\nvoid __idxd_wq_quiesce(struct idxd_wq *wq)\n{\n\tlockdep_assert_held(&wq->wq_lock);\n\treinit_completion(&wq->wq_resurrect);\n\tpercpu_ref_kill(&wq->wq_active);\n\tcomplete_all(&wq->wq_resurrect);\n\twait_for_completion(&wq->wq_dead);\n}\n\nvoid idxd_wq_quiesce(struct idxd_wq *wq)\n{\n\tmutex_lock(&wq->wq_lock);\n\t__idxd_wq_quiesce(wq);\n\tmutex_unlock(&wq->wq_lock);\n}\n\n \nstatic inline bool idxd_is_enabled(struct idxd_device *idxd)\n{\n\tunion gensts_reg gensts;\n\n\tgensts.bits = ioread32(idxd->reg_base + IDXD_GENSTATS_OFFSET);\n\n\tif (gensts.state == IDXD_DEVICE_STATE_ENABLED)\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline bool idxd_device_is_halted(struct idxd_device *idxd)\n{\n\tunion gensts_reg gensts;\n\n\tgensts.bits = ioread32(idxd->reg_base + IDXD_GENSTATS_OFFSET);\n\n\treturn (gensts.state == IDXD_DEVICE_STATE_HALT);\n}\n\n \nint idxd_device_init_reset(struct idxd_device *idxd)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tunion idxd_command_reg cmd;\n\n\tif (idxd_device_is_halted(idxd)) {\n\t\tdev_warn(&idxd->pdev->dev, \"Device is HALTED!\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tmemset(&cmd, 0, sizeof(cmd));\n\tcmd.cmd = IDXD_CMD_RESET_DEVICE;\n\tdev_dbg(dev, \"%s: sending reset for init.\\n\", __func__);\n\tspin_lock(&idxd->cmd_lock);\n\tiowrite32(cmd.bits, idxd->reg_base + IDXD_CMD_OFFSET);\n\n\twhile (ioread32(idxd->reg_base + IDXD_CMDSTS_OFFSET) &\n\t       IDXD_CMDSTS_ACTIVE)\n\t\tcpu_relax();\n\tspin_unlock(&idxd->cmd_lock);\n\treturn 0;\n}\n\nstatic void idxd_cmd_exec(struct idxd_device *idxd, int cmd_code, u32 operand,\n\t\t\t  u32 *status)\n{\n\tunion idxd_command_reg cmd;\n\tDECLARE_COMPLETION_ONSTACK(done);\n\tu32 stat;\n\tunsigned long flags;\n\n\tif (idxd_device_is_halted(idxd)) {\n\t\tdev_warn(&idxd->pdev->dev, \"Device is HALTED!\\n\");\n\t\tif (status)\n\t\t\t*status = IDXD_CMDSTS_HW_ERR;\n\t\treturn;\n\t}\n\n\tmemset(&cmd, 0, sizeof(cmd));\n\tcmd.cmd = cmd_code;\n\tcmd.operand = operand;\n\tcmd.int_req = 1;\n\n\tspin_lock_irqsave(&idxd->cmd_lock, flags);\n\twait_event_lock_irq(idxd->cmd_waitq,\n\t\t\t    !test_bit(IDXD_FLAG_CMD_RUNNING, &idxd->flags),\n\t\t\t    idxd->cmd_lock);\n\n\tdev_dbg(&idxd->pdev->dev, \"%s: sending cmd: %#x op: %#x\\n\",\n\t\t__func__, cmd_code, operand);\n\n\tidxd->cmd_status = 0;\n\t__set_bit(IDXD_FLAG_CMD_RUNNING, &idxd->flags);\n\tidxd->cmd_done = &done;\n\tiowrite32(cmd.bits, idxd->reg_base + IDXD_CMD_OFFSET);\n\n\t \n\tspin_unlock_irqrestore(&idxd->cmd_lock, flags);\n\twait_for_completion(&done);\n\tstat = ioread32(idxd->reg_base + IDXD_CMDSTS_OFFSET);\n\tspin_lock(&idxd->cmd_lock);\n\tif (status)\n\t\t*status = stat;\n\tidxd->cmd_status = stat & GENMASK(7, 0);\n\n\t__clear_bit(IDXD_FLAG_CMD_RUNNING, &idxd->flags);\n\t \n\twake_up(&idxd->cmd_waitq);\n\tspin_unlock(&idxd->cmd_lock);\n}\n\nint idxd_device_enable(struct idxd_device *idxd)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 status;\n\n\tif (idxd_is_enabled(idxd)) {\n\t\tdev_dbg(dev, \"Device already enabled\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tidxd_cmd_exec(idxd, IDXD_CMD_ENABLE_DEVICE, 0, &status);\n\n\t \n\tif (status != IDXD_CMDSTS_SUCCESS &&\n\t    status != IDXD_CMDSTS_ERR_DEV_ENABLED) {\n\t\tdev_dbg(dev, \"%s: err_code: %#x\\n\", __func__, status);\n\t\treturn -ENXIO;\n\t}\n\n\tidxd->state = IDXD_DEV_ENABLED;\n\treturn 0;\n}\n\nint idxd_device_disable(struct idxd_device *idxd)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 status;\n\n\tif (!idxd_is_enabled(idxd)) {\n\t\tdev_dbg(dev, \"Device is not enabled\\n\");\n\t\treturn 0;\n\t}\n\n\tidxd_cmd_exec(idxd, IDXD_CMD_DISABLE_DEVICE, 0, &status);\n\n\t \n\tif (status != IDXD_CMDSTS_SUCCESS &&\n\t    !(status & IDXD_CMDSTS_ERR_DIS_DEV_EN)) {\n\t\tdev_dbg(dev, \"%s: err_code: %#x\\n\", __func__, status);\n\t\treturn -ENXIO;\n\t}\n\n\tidxd_device_clear_state(idxd);\n\treturn 0;\n}\n\nvoid idxd_device_reset(struct idxd_device *idxd)\n{\n\tidxd_cmd_exec(idxd, IDXD_CMD_RESET_DEVICE, 0, NULL);\n\tidxd_device_clear_state(idxd);\n\tspin_lock(&idxd->dev_lock);\n\tidxd_unmask_error_interrupts(idxd);\n\tspin_unlock(&idxd->dev_lock);\n}\n\nvoid idxd_device_drain_pasid(struct idxd_device *idxd, int pasid)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 operand;\n\n\toperand = pasid;\n\tdev_dbg(dev, \"cmd: %u operand: %#x\\n\", IDXD_CMD_DRAIN_PASID, operand);\n\tidxd_cmd_exec(idxd, IDXD_CMD_DRAIN_PASID, operand, NULL);\n\tdev_dbg(dev, \"pasid %d drained\\n\", pasid);\n}\n\nint idxd_device_request_int_handle(struct idxd_device *idxd, int idx, int *handle,\n\t\t\t\t   enum idxd_interrupt_type irq_type)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 operand, status;\n\n\tif (!(idxd->hw.cmd_cap & BIT(IDXD_CMD_REQUEST_INT_HANDLE)))\n\t\treturn -EOPNOTSUPP;\n\n\tdev_dbg(dev, \"get int handle, idx %d\\n\", idx);\n\n\toperand = idx & GENMASK(15, 0);\n\tif (irq_type == IDXD_IRQ_IMS)\n\t\toperand |= CMD_INT_HANDLE_IMS;\n\n\tdev_dbg(dev, \"cmd: %u operand: %#x\\n\", IDXD_CMD_REQUEST_INT_HANDLE, operand);\n\n\tidxd_cmd_exec(idxd, IDXD_CMD_REQUEST_INT_HANDLE, operand, &status);\n\n\tif ((status & IDXD_CMDSTS_ERR_MASK) != IDXD_CMDSTS_SUCCESS) {\n\t\tdev_dbg(dev, \"request int handle failed: %#x\\n\", status);\n\t\treturn -ENXIO;\n\t}\n\n\t*handle = (status >> IDXD_CMDSTS_RES_SHIFT) & GENMASK(15, 0);\n\n\tdev_dbg(dev, \"int handle acquired: %u\\n\", *handle);\n\treturn 0;\n}\n\nint idxd_device_release_int_handle(struct idxd_device *idxd, int handle,\n\t\t\t\t   enum idxd_interrupt_type irq_type)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 operand, status;\n\tunion idxd_command_reg cmd;\n\n\tif (!(idxd->hw.cmd_cap & BIT(IDXD_CMD_RELEASE_INT_HANDLE)))\n\t\treturn -EOPNOTSUPP;\n\n\tdev_dbg(dev, \"release int handle, handle %d\\n\", handle);\n\n\tmemset(&cmd, 0, sizeof(cmd));\n\toperand = handle & GENMASK(15, 0);\n\n\tif (irq_type == IDXD_IRQ_IMS)\n\t\toperand |= CMD_INT_HANDLE_IMS;\n\n\tcmd.cmd = IDXD_CMD_RELEASE_INT_HANDLE;\n\tcmd.operand = operand;\n\n\tdev_dbg(dev, \"cmd: %u operand: %#x\\n\", IDXD_CMD_RELEASE_INT_HANDLE, operand);\n\n\tspin_lock(&idxd->cmd_lock);\n\tiowrite32(cmd.bits, idxd->reg_base + IDXD_CMD_OFFSET);\n\n\twhile (ioread32(idxd->reg_base + IDXD_CMDSTS_OFFSET) & IDXD_CMDSTS_ACTIVE)\n\t\tcpu_relax();\n\tstatus = ioread32(idxd->reg_base + IDXD_CMDSTS_OFFSET);\n\tspin_unlock(&idxd->cmd_lock);\n\n\tif ((status & IDXD_CMDSTS_ERR_MASK) != IDXD_CMDSTS_SUCCESS) {\n\t\tdev_dbg(dev, \"release int handle failed: %#x\\n\", status);\n\t\treturn -ENXIO;\n\t}\n\n\tdev_dbg(dev, \"int handle released.\\n\");\n\treturn 0;\n}\n\n \nstatic void idxd_engines_clear_state(struct idxd_device *idxd)\n{\n\tstruct idxd_engine *engine;\n\tint i;\n\n\tlockdep_assert_held(&idxd->dev_lock);\n\tfor (i = 0; i < idxd->max_engines; i++) {\n\t\tengine = idxd->engines[i];\n\t\tengine->group = NULL;\n\t}\n}\n\nstatic void idxd_groups_clear_state(struct idxd_device *idxd)\n{\n\tstruct idxd_group *group;\n\tint i;\n\n\tlockdep_assert_held(&idxd->dev_lock);\n\tfor (i = 0; i < idxd->max_groups; i++) {\n\t\tgroup = idxd->groups[i];\n\t\tmemset(&group->grpcfg, 0, sizeof(group->grpcfg));\n\t\tgroup->num_engines = 0;\n\t\tgroup->num_wqs = 0;\n\t\tgroup->use_rdbuf_limit = false;\n\t\t \n\t\tgroup->rdbufs_allowed = idxd->max_rdbufs;\n\t\tgroup->rdbufs_reserved = 0;\n\t\tif (idxd->hw.version <= DEVICE_VERSION_2 && !tc_override) {\n\t\t\tgroup->tc_a = 1;\n\t\t\tgroup->tc_b = 1;\n\t\t} else {\n\t\t\tgroup->tc_a = -1;\n\t\t\tgroup->tc_b = -1;\n\t\t}\n\t\tgroup->desc_progress_limit = 0;\n\t\tgroup->batch_progress_limit = 0;\n\t}\n}\n\nstatic void idxd_device_wqs_clear_state(struct idxd_device *idxd)\n{\n\tint i;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\tmutex_lock(&wq->wq_lock);\n\t\tidxd_wq_disable_cleanup(wq);\n\t\tidxd_wq_device_reset_cleanup(wq);\n\t\tmutex_unlock(&wq->wq_lock);\n\t}\n}\n\nvoid idxd_device_clear_state(struct idxd_device *idxd)\n{\n\t \n\tif (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags)) {\n\t\t \n\t\tidxd_device_wqs_clear_state(idxd);\n\n\t\tspin_lock(&idxd->dev_lock);\n\t\tidxd_groups_clear_state(idxd);\n\t\tidxd_engines_clear_state(idxd);\n\t} else {\n\t\tspin_lock(&idxd->dev_lock);\n\t}\n\n\tidxd->state = IDXD_DEV_DISABLED;\n\tspin_unlock(&idxd->dev_lock);\n}\n\nstatic int idxd_device_evl_setup(struct idxd_device *idxd)\n{\n\tunion gencfg_reg gencfg;\n\tunion evlcfg_reg evlcfg;\n\tunion genctrl_reg genctrl;\n\tstruct device *dev = &idxd->pdev->dev;\n\tvoid *addr;\n\tdma_addr_t dma_addr;\n\tint size;\n\tstruct idxd_evl *evl = idxd->evl;\n\tunsigned long *bmap;\n\tint rc;\n\n\tif (!evl)\n\t\treturn 0;\n\n\tsize = evl_size(idxd);\n\n\tbmap = bitmap_zalloc(size, GFP_KERNEL);\n\tif (!bmap) {\n\t\trc = -ENOMEM;\n\t\tgoto err_bmap;\n\t}\n\n\t \n\taddr = dma_alloc_coherent(dev, size, &dma_addr, GFP_KERNEL);\n\tif (!addr) {\n\t\trc = -ENOMEM;\n\t\tgoto err_alloc;\n\t}\n\n\tspin_lock(&evl->lock);\n\tevl->log = addr;\n\tevl->dma = dma_addr;\n\tevl->log_size = size;\n\tevl->bmap = bmap;\n\n\tmemset(&evlcfg, 0, sizeof(evlcfg));\n\tevlcfg.bits[0] = dma_addr & GENMASK(63, 12);\n\tevlcfg.size = evl->size;\n\n\tiowrite64(evlcfg.bits[0], idxd->reg_base + IDXD_EVLCFG_OFFSET);\n\tiowrite64(evlcfg.bits[1], idxd->reg_base + IDXD_EVLCFG_OFFSET + 8);\n\n\tgenctrl.bits = ioread32(idxd->reg_base + IDXD_GENCTRL_OFFSET);\n\tgenctrl.evl_int_en = 1;\n\tiowrite32(genctrl.bits, idxd->reg_base + IDXD_GENCTRL_OFFSET);\n\n\tgencfg.bits = ioread32(idxd->reg_base + IDXD_GENCFG_OFFSET);\n\tgencfg.evl_en = 1;\n\tiowrite32(gencfg.bits, idxd->reg_base + IDXD_GENCFG_OFFSET);\n\n\tspin_unlock(&evl->lock);\n\treturn 0;\n\nerr_alloc:\n\tbitmap_free(bmap);\nerr_bmap:\n\treturn rc;\n}\n\nstatic void idxd_device_evl_free(struct idxd_device *idxd)\n{\n\tunion gencfg_reg gencfg;\n\tunion genctrl_reg genctrl;\n\tstruct device *dev = &idxd->pdev->dev;\n\tstruct idxd_evl *evl = idxd->evl;\n\n\tgencfg.bits = ioread32(idxd->reg_base + IDXD_GENCFG_OFFSET);\n\tif (!gencfg.evl_en)\n\t\treturn;\n\n\tspin_lock(&evl->lock);\n\tgencfg.evl_en = 0;\n\tiowrite32(gencfg.bits, idxd->reg_base + IDXD_GENCFG_OFFSET);\n\n\tgenctrl.bits = ioread32(idxd->reg_base + IDXD_GENCTRL_OFFSET);\n\tgenctrl.evl_int_en = 0;\n\tiowrite32(genctrl.bits, idxd->reg_base + IDXD_GENCTRL_OFFSET);\n\n\tiowrite64(0, idxd->reg_base + IDXD_EVLCFG_OFFSET);\n\tiowrite64(0, idxd->reg_base + IDXD_EVLCFG_OFFSET + 8);\n\n\tdma_free_coherent(dev, evl->log_size, evl->log, evl->dma);\n\tbitmap_free(evl->bmap);\n\tevl->log = NULL;\n\tevl->size = IDXD_EVL_SIZE_MIN;\n\tspin_unlock(&evl->lock);\n}\n\nstatic void idxd_group_config_write(struct idxd_group *group)\n{\n\tstruct idxd_device *idxd = group->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tint i;\n\tu32 grpcfg_offset;\n\n\tdev_dbg(dev, \"Writing group %d cfg registers\\n\", group->id);\n\n\t \n\tfor (i = 0; i < GRPWQCFG_STRIDES; i++) {\n\t\tgrpcfg_offset = GRPWQCFG_OFFSET(idxd, group->id, i);\n\t\tiowrite64(group->grpcfg.wqs[i], idxd->reg_base + grpcfg_offset);\n\t\tdev_dbg(dev, \"GRPCFG wq[%d:%d: %#x]: %#llx\\n\",\n\t\t\tgroup->id, i, grpcfg_offset,\n\t\t\tioread64(idxd->reg_base + grpcfg_offset));\n\t}\n\n\t \n\tgrpcfg_offset = GRPENGCFG_OFFSET(idxd, group->id);\n\tiowrite64(group->grpcfg.engines, idxd->reg_base + grpcfg_offset);\n\tdev_dbg(dev, \"GRPCFG engs[%d: %#x]: %#llx\\n\", group->id,\n\t\tgrpcfg_offset, ioread64(idxd->reg_base + grpcfg_offset));\n\n\t \n\tgrpcfg_offset = GRPFLGCFG_OFFSET(idxd, group->id);\n\tiowrite64(group->grpcfg.flags.bits, idxd->reg_base + grpcfg_offset);\n\tdev_dbg(dev, \"GRPFLAGS flags[%d: %#x]: %#llx\\n\",\n\t\tgroup->id, grpcfg_offset,\n\t\tioread64(idxd->reg_base + grpcfg_offset));\n}\n\nstatic int idxd_groups_config_write(struct idxd_device *idxd)\n\n{\n\tunion gencfg_reg reg;\n\tint i;\n\tstruct device *dev = &idxd->pdev->dev;\n\n\t \n\tif (idxd->hw.gen_cap.config_en && idxd->rdbuf_limit) {\n\t\treg.bits = ioread32(idxd->reg_base + IDXD_GENCFG_OFFSET);\n\t\treg.rdbuf_limit = idxd->rdbuf_limit;\n\t\tiowrite32(reg.bits, idxd->reg_base + IDXD_GENCFG_OFFSET);\n\t}\n\n\tdev_dbg(dev, \"GENCFG(%#x): %#x\\n\", IDXD_GENCFG_OFFSET,\n\t\tioread32(idxd->reg_base + IDXD_GENCFG_OFFSET));\n\n\tfor (i = 0; i < idxd->max_groups; i++) {\n\t\tstruct idxd_group *group = idxd->groups[i];\n\n\t\tidxd_group_config_write(group);\n\t}\n\n\treturn 0;\n}\n\nstatic bool idxd_device_pasid_priv_enabled(struct idxd_device *idxd)\n{\n\tstruct pci_dev *pdev = idxd->pdev;\n\n\tif (pdev->pasid_enabled && (pdev->pasid_features & PCI_PASID_CAP_PRIV))\n\t\treturn true;\n\treturn false;\n}\n\nstatic int idxd_wq_config_write(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tu32 wq_offset;\n\tint i, n;\n\n\tif (!wq->group)\n\t\treturn 0;\n\n\t \n\tfor (i = 0; i < WQCFG_STRIDES(idxd); i++) {\n\t\twq_offset = WQCFG_OFFSET(idxd, wq->id, i);\n\t\twq->wqcfg->bits[i] |= ioread32(idxd->reg_base + wq_offset);\n\t}\n\n\tif (wq->size == 0 && wq->type != IDXD_WQT_NONE)\n\t\twq->size = WQ_DEFAULT_QUEUE_DEPTH;\n\n\t \n\twq->wqcfg->wq_size = wq->size;\n\n\t \n\twq->wqcfg->wq_thresh = wq->threshold;\n\n\t \n\tif (wq_dedicated(wq))\n\t\twq->wqcfg->mode = 1;\n\n\t \n\tif (wq_dedicated(wq) && wq->wqcfg->pasid_en &&\n\t    !idxd_device_pasid_priv_enabled(idxd) &&\n\t    wq->type == IDXD_WQT_KERNEL) {\n\t\tidxd->cmd_status = IDXD_SCMD_WQ_NO_PRIV;\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\twq->wqcfg->priority = wq->priority;\n\n\tif (idxd->hw.gen_cap.block_on_fault &&\n\t    test_bit(WQ_FLAG_BLOCK_ON_FAULT, &wq->flags) &&\n\t    !test_bit(WQ_FLAG_PRS_DISABLE, &wq->flags))\n\t\twq->wqcfg->bof = 1;\n\n\tif (idxd->hw.wq_cap.wq_ats_support)\n\t\twq->wqcfg->wq_ats_disable = test_bit(WQ_FLAG_ATS_DISABLE, &wq->flags);\n\n\tif (idxd->hw.wq_cap.wq_prs_support)\n\t\twq->wqcfg->wq_prs_disable = test_bit(WQ_FLAG_PRS_DISABLE, &wq->flags);\n\n\t \n\twq->wqcfg->max_xfer_shift = ilog2(wq->max_xfer_bytes);\n\tidxd_wqcfg_set_max_batch_shift(idxd->data->type, wq->wqcfg, ilog2(wq->max_batch_size));\n\n\t \n\tif (idxd->hw.wq_cap.op_config && wq->opcap_bmap) {\n\t\tmemset(wq->wqcfg->op_config, 0, IDXD_MAX_OPCAP_BITS / 8);\n\t\tfor_each_set_bit(n, wq->opcap_bmap, IDXD_MAX_OPCAP_BITS) {\n\t\t\tint pos = n % BITS_PER_LONG_LONG;\n\t\t\tint idx = n / BITS_PER_LONG_LONG;\n\n\t\t\twq->wqcfg->op_config[idx] |= BIT(pos);\n\t\t}\n\t}\n\n\tdev_dbg(dev, \"WQ %d CFGs\\n\", wq->id);\n\tfor (i = 0; i < WQCFG_STRIDES(idxd); i++) {\n\t\twq_offset = WQCFG_OFFSET(idxd, wq->id, i);\n\t\tiowrite32(wq->wqcfg->bits[i], idxd->reg_base + wq_offset);\n\t\tdev_dbg(dev, \"WQ[%d][%d][%#x]: %#x\\n\",\n\t\t\twq->id, i, wq_offset,\n\t\t\tioread32(idxd->reg_base + wq_offset));\n\t}\n\n\treturn 0;\n}\n\nstatic int idxd_wqs_config_write(struct idxd_device *idxd)\n{\n\tint i, rc;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\trc = idxd_wq_config_write(wq);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic void idxd_group_flags_setup(struct idxd_device *idxd)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < idxd->max_groups; i++) {\n\t\tstruct idxd_group *group = idxd->groups[i];\n\n\t\tif (group->tc_a == -1)\n\t\t\tgroup->tc_a = group->grpcfg.flags.tc_a = 0;\n\t\telse\n\t\t\tgroup->grpcfg.flags.tc_a = group->tc_a;\n\t\tif (group->tc_b == -1)\n\t\t\tgroup->tc_b = group->grpcfg.flags.tc_b = 1;\n\t\telse\n\t\t\tgroup->grpcfg.flags.tc_b = group->tc_b;\n\t\tgroup->grpcfg.flags.use_rdbuf_limit = group->use_rdbuf_limit;\n\t\tgroup->grpcfg.flags.rdbufs_reserved = group->rdbufs_reserved;\n\t\tgroup->grpcfg.flags.rdbufs_allowed = group->rdbufs_allowed;\n\t\tgroup->grpcfg.flags.desc_progress_limit = group->desc_progress_limit;\n\t\tgroup->grpcfg.flags.batch_progress_limit = group->batch_progress_limit;\n\t}\n}\n\nstatic int idxd_engines_setup(struct idxd_device *idxd)\n{\n\tint i, engines = 0;\n\tstruct idxd_engine *eng;\n\tstruct idxd_group *group;\n\n\tfor (i = 0; i < idxd->max_groups; i++) {\n\t\tgroup = idxd->groups[i];\n\t\tgroup->grpcfg.engines = 0;\n\t}\n\n\tfor (i = 0; i < idxd->max_engines; i++) {\n\t\teng = idxd->engines[i];\n\t\tgroup = eng->group;\n\n\t\tif (!group)\n\t\t\tcontinue;\n\n\t\tgroup->grpcfg.engines |= BIT(eng->id);\n\t\tengines++;\n\t}\n\n\tif (!engines)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int idxd_wqs_setup(struct idxd_device *idxd)\n{\n\tstruct idxd_wq *wq;\n\tstruct idxd_group *group;\n\tint i, j, configured = 0;\n\tstruct device *dev = &idxd->pdev->dev;\n\n\tfor (i = 0; i < idxd->max_groups; i++) {\n\t\tgroup = idxd->groups[i];\n\t\tfor (j = 0; j < 4; j++)\n\t\t\tgroup->grpcfg.wqs[j] = 0;\n\t}\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\twq = idxd->wqs[i];\n\t\tgroup = wq->group;\n\n\t\tif (!wq->group)\n\t\t\tcontinue;\n\n\t\tif (wq_shared(wq) && !wq_shared_supported(wq)) {\n\t\t\tidxd->cmd_status = IDXD_SCMD_WQ_NO_SWQ_SUPPORT;\n\t\t\tdev_warn(dev, \"No shared wq support but configured.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tgroup->grpcfg.wqs[wq->id / 64] |= BIT(wq->id % 64);\n\t\tconfigured++;\n\t}\n\n\tif (configured == 0) {\n\t\tidxd->cmd_status = IDXD_SCMD_WQ_NONE_CONFIGURED;\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint idxd_device_config(struct idxd_device *idxd)\n{\n\tint rc;\n\n\tlockdep_assert_held(&idxd->dev_lock);\n\trc = idxd_wqs_setup(idxd);\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = idxd_engines_setup(idxd);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tidxd_group_flags_setup(idxd);\n\n\trc = idxd_wqs_config_write(idxd);\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = idxd_groups_config_write(idxd);\n\tif (rc < 0)\n\t\treturn rc;\n\n\treturn 0;\n}\n\nstatic int idxd_wq_load_config(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tint wqcfg_offset;\n\tint i;\n\n\twqcfg_offset = WQCFG_OFFSET(idxd, wq->id, 0);\n\tmemcpy_fromio(wq->wqcfg, idxd->reg_base + wqcfg_offset, idxd->wqcfg_size);\n\n\twq->size = wq->wqcfg->wq_size;\n\twq->threshold = wq->wqcfg->wq_thresh;\n\n\t \n\tif (wq->wqcfg->mode == 0 || wq->wqcfg->pasid_en)\n\t\treturn -EOPNOTSUPP;\n\n\tset_bit(WQ_FLAG_DEDICATED, &wq->flags);\n\n\twq->priority = wq->wqcfg->priority;\n\n\twq->max_xfer_bytes = 1ULL << wq->wqcfg->max_xfer_shift;\n\tidxd_wq_set_max_batch_size(idxd->data->type, wq, 1U << wq->wqcfg->max_batch_shift);\n\n\tfor (i = 0; i < WQCFG_STRIDES(idxd); i++) {\n\t\twqcfg_offset = WQCFG_OFFSET(idxd, wq->id, i);\n\t\tdev_dbg(dev, \"WQ[%d][%d][%#x]: %#x\\n\", wq->id, i, wqcfg_offset, wq->wqcfg->bits[i]);\n\t}\n\n\treturn 0;\n}\n\nstatic void idxd_group_load_config(struct idxd_group *group)\n{\n\tstruct idxd_device *idxd = group->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tint i, j, grpcfg_offset;\n\n\t \n\tfor (i = 0; i < GRPWQCFG_STRIDES; i++) {\n\t\tstruct idxd_wq *wq;\n\n\t\tgrpcfg_offset = GRPWQCFG_OFFSET(idxd, group->id, i);\n\t\tgroup->grpcfg.wqs[i] = ioread64(idxd->reg_base + grpcfg_offset);\n\t\tdev_dbg(dev, \"GRPCFG wq[%d:%d: %#x]: %#llx\\n\",\n\t\t\tgroup->id, i, grpcfg_offset, group->grpcfg.wqs[i]);\n\n\t\tif (i * 64 >= idxd->max_wqs)\n\t\t\tbreak;\n\n\t\t \n\t\tfor (j = 0; j < 64; j++) {\n\t\t\tint id = i * 64 + j;\n\n\t\t\t \n\t\t\tif (id >= idxd->max_wqs)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (group->grpcfg.wqs[i] & BIT(j)) {\n\t\t\t\twq = idxd->wqs[id];\n\t\t\t\twq->group = group;\n\t\t\t}\n\t\t}\n\t}\n\n\tgrpcfg_offset = GRPENGCFG_OFFSET(idxd, group->id);\n\tgroup->grpcfg.engines = ioread64(idxd->reg_base + grpcfg_offset);\n\tdev_dbg(dev, \"GRPCFG engs[%d: %#x]: %#llx\\n\", group->id,\n\t\tgrpcfg_offset, group->grpcfg.engines);\n\n\t \n\tfor (i = 0; i < 64; i++) {\n\t\tif (i >= idxd->max_engines)\n\t\t\tbreak;\n\n\t\tif (group->grpcfg.engines & BIT(i)) {\n\t\t\tstruct idxd_engine *engine = idxd->engines[i];\n\n\t\t\tengine->group = group;\n\t\t}\n\t}\n\n\tgrpcfg_offset = GRPFLGCFG_OFFSET(idxd, group->id);\n\tgroup->grpcfg.flags.bits = ioread64(idxd->reg_base + grpcfg_offset);\n\tdev_dbg(dev, \"GRPFLAGS flags[%d: %#x]: %#llx\\n\",\n\t\tgroup->id, grpcfg_offset, group->grpcfg.flags.bits);\n}\n\nint idxd_device_load_config(struct idxd_device *idxd)\n{\n\tunion gencfg_reg reg;\n\tint i, rc;\n\n\treg.bits = ioread32(idxd->reg_base + IDXD_GENCFG_OFFSET);\n\tidxd->rdbuf_limit = reg.rdbuf_limit;\n\n\tfor (i = 0; i < idxd->max_groups; i++) {\n\t\tstruct idxd_group *group = idxd->groups[i];\n\n\t\tidxd_group_load_config(group);\n\t}\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\n\t\trc = idxd_wq_load_config(wq);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic void idxd_flush_pending_descs(struct idxd_irq_entry *ie)\n{\n\tstruct idxd_desc *desc, *itr;\n\tstruct llist_node *head;\n\tLIST_HEAD(flist);\n\tenum idxd_complete_type ctype;\n\n\tspin_lock(&ie->list_lock);\n\thead = llist_del_all(&ie->pending_llist);\n\tif (head) {\n\t\tllist_for_each_entry_safe(desc, itr, head, llnode)\n\t\t\tlist_add_tail(&desc->list, &ie->work_list);\n\t}\n\n\tlist_for_each_entry_safe(desc, itr, &ie->work_list, list)\n\t\tlist_move_tail(&desc->list, &flist);\n\tspin_unlock(&ie->list_lock);\n\n\tlist_for_each_entry_safe(desc, itr, &flist, list) {\n\t\tstruct dma_async_tx_descriptor *tx;\n\n\t\tlist_del(&desc->list);\n\t\tctype = desc->completion->status ? IDXD_COMPLETE_NORMAL : IDXD_COMPLETE_ABORT;\n\t\t \n\t\ttx = &desc->txd;\n\t\ttx->callback = NULL;\n\t\ttx->callback_result = NULL;\n\t\tidxd_dma_complete_txd(desc, ctype, true);\n\t}\n}\n\nstatic void idxd_device_set_perm_entry(struct idxd_device *idxd,\n\t\t\t\t       struct idxd_irq_entry *ie)\n{\n\tunion msix_perm mperm;\n\n\tif (ie->pasid == IOMMU_PASID_INVALID)\n\t\treturn;\n\n\tmperm.bits = 0;\n\tmperm.pasid = ie->pasid;\n\tmperm.pasid_en = 1;\n\tiowrite32(mperm.bits, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);\n}\n\nstatic void idxd_device_clear_perm_entry(struct idxd_device *idxd,\n\t\t\t\t\t struct idxd_irq_entry *ie)\n{\n\tiowrite32(0, idxd->reg_base + idxd->msix_perm_offset + ie->id * 8);\n}\n\nvoid idxd_wq_free_irq(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct idxd_irq_entry *ie = &wq->ie;\n\n\tif (wq->type != IDXD_WQT_KERNEL)\n\t\treturn;\n\n\tfree_irq(ie->vector, ie);\n\tidxd_flush_pending_descs(ie);\n\tif (idxd->request_int_handles)\n\t\tidxd_device_release_int_handle(idxd, ie->int_handle, IDXD_IRQ_MSIX);\n\tidxd_device_clear_perm_entry(idxd, ie);\n\tie->vector = -1;\n\tie->int_handle = INVALID_INT_HANDLE;\n\tie->pasid = IOMMU_PASID_INVALID;\n}\n\nint idxd_wq_request_irq(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct pci_dev *pdev = idxd->pdev;\n\tstruct device *dev = &pdev->dev;\n\tstruct idxd_irq_entry *ie;\n\tint rc;\n\n\tif (wq->type != IDXD_WQT_KERNEL)\n\t\treturn 0;\n\n\tie = &wq->ie;\n\tie->vector = pci_irq_vector(pdev, ie->id);\n\tie->pasid = device_pasid_enabled(idxd) ? idxd->pasid : IOMMU_PASID_INVALID;\n\tidxd_device_set_perm_entry(idxd, ie);\n\n\trc = request_threaded_irq(ie->vector, NULL, idxd_wq_thread, 0, \"idxd-portal\", ie);\n\tif (rc < 0) {\n\t\tdev_err(dev, \"Failed to request irq %d.\\n\", ie->vector);\n\t\tgoto err_irq;\n\t}\n\n\tif (idxd->request_int_handles) {\n\t\trc = idxd_device_request_int_handle(idxd, ie->id, &ie->int_handle,\n\t\t\t\t\t\t    IDXD_IRQ_MSIX);\n\t\tif (rc < 0)\n\t\t\tgoto err_int_handle;\n\t} else {\n\t\tie->int_handle = ie->id;\n\t}\n\n\treturn 0;\n\nerr_int_handle:\n\tie->int_handle = INVALID_INT_HANDLE;\n\tfree_irq(ie->vector, ie);\nerr_irq:\n\tidxd_device_clear_perm_entry(idxd, ie);\n\tie->pasid = IOMMU_PASID_INVALID;\n\treturn rc;\n}\n\nint drv_enable_wq(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\tint rc = -ENXIO;\n\n\tlockdep_assert_held(&wq->wq_lock);\n\n\tif (idxd->state != IDXD_DEV_ENABLED) {\n\t\tidxd->cmd_status = IDXD_SCMD_DEV_NOT_ENABLED;\n\t\tgoto err;\n\t}\n\n\tif (wq->state != IDXD_WQ_DISABLED) {\n\t\tdev_dbg(dev, \"wq %d already enabled.\\n\", wq->id);\n\t\tidxd->cmd_status = IDXD_SCMD_WQ_ENABLED;\n\t\trc = -EBUSY;\n\t\tgoto err;\n\t}\n\n\tif (!wq->group) {\n\t\tdev_dbg(dev, \"wq %d not attached to group.\\n\", wq->id);\n\t\tidxd->cmd_status = IDXD_SCMD_WQ_NO_GRP;\n\t\tgoto err;\n\t}\n\n\tif (strlen(wq->name) == 0) {\n\t\tidxd->cmd_status = IDXD_SCMD_WQ_NO_NAME;\n\t\tdev_dbg(dev, \"wq %d name not set.\\n\", wq->id);\n\t\tgoto err;\n\t}\n\n\t \n\tif (wq_shared(wq)) {\n\t\tif (!wq_shared_supported(wq)) {\n\t\t\tidxd->cmd_status = IDXD_SCMD_WQ_NO_SVM;\n\t\t\tdev_dbg(dev, \"PASID not enabled and shared wq.\\n\");\n\t\t\tgoto err;\n\t\t}\n\t\t \n\t\tif (wq->threshold == 0) {\n\t\t\tidxd->cmd_status = IDXD_SCMD_WQ_NO_THRESH;\n\t\t\tdev_dbg(dev, \"Shared wq and threshold 0.\\n\");\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t \n\tif (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags)) {\n\t\tif (wq_pasid_enabled(wq)) {\n\t\t\tif (is_idxd_wq_kernel(wq) || wq_shared(wq)) {\n\t\t\t\tu32 pasid = wq_dedicated(wq) ? idxd->pasid : 0;\n\n\t\t\t\t__idxd_wq_set_pasid_locked(wq, pasid);\n\t\t\t}\n\t\t}\n\t}\n\n\trc = 0;\n\tspin_lock(&idxd->dev_lock);\n\tif (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\trc = idxd_device_config(idxd);\n\tspin_unlock(&idxd->dev_lock);\n\tif (rc < 0) {\n\t\tdev_dbg(dev, \"Writing wq %d config failed: %d\\n\", wq->id, rc);\n\t\tgoto err;\n\t}\n\n\trc = idxd_wq_enable(wq);\n\tif (rc < 0) {\n\t\tdev_dbg(dev, \"wq %d enabling failed: %d\\n\", wq->id, rc);\n\t\tgoto err;\n\t}\n\n\trc = idxd_wq_map_portal(wq);\n\tif (rc < 0) {\n\t\tidxd->cmd_status = IDXD_SCMD_WQ_PORTAL_ERR;\n\t\tdev_dbg(dev, \"wq %d portal mapping failed: %d\\n\", wq->id, rc);\n\t\tgoto err_map_portal;\n\t}\n\n\twq->client_count = 0;\n\n\trc = idxd_wq_request_irq(wq);\n\tif (rc < 0) {\n\t\tidxd->cmd_status = IDXD_SCMD_WQ_IRQ_ERR;\n\t\tdev_dbg(dev, \"WQ %d irq setup failed: %d\\n\", wq->id, rc);\n\t\tgoto err_irq;\n\t}\n\n\trc = idxd_wq_alloc_resources(wq);\n\tif (rc < 0) {\n\t\tidxd->cmd_status = IDXD_SCMD_WQ_RES_ALLOC_ERR;\n\t\tdev_dbg(dev, \"WQ resource alloc failed\\n\");\n\t\tgoto err_res_alloc;\n\t}\n\n\trc = idxd_wq_init_percpu_ref(wq);\n\tif (rc < 0) {\n\t\tidxd->cmd_status = IDXD_SCMD_PERCPU_ERR;\n\t\tdev_dbg(dev, \"percpu_ref setup failed\\n\");\n\t\tgoto err_ref;\n\t}\n\n\treturn 0;\n\nerr_ref:\n\tidxd_wq_free_resources(wq);\nerr_res_alloc:\n\tidxd_wq_free_irq(wq);\nerr_irq:\n\tidxd_wq_unmap_portal(wq);\nerr_map_portal:\n\tif (idxd_wq_disable(wq, false))\n\t\tdev_dbg(dev, \"wq %s disable failed\\n\", dev_name(wq_confdev(wq)));\nerr:\n\treturn rc;\n}\n\nvoid drv_disable_wq(struct idxd_wq *wq)\n{\n\tstruct idxd_device *idxd = wq->idxd;\n\tstruct device *dev = &idxd->pdev->dev;\n\n\tlockdep_assert_held(&wq->wq_lock);\n\n\tif (idxd_wq_refcount(wq))\n\t\tdev_warn(dev, \"Clients has claim on wq %d: %d\\n\",\n\t\t\t wq->id, idxd_wq_refcount(wq));\n\n\tidxd_wq_unmap_portal(wq);\n\tidxd_wq_drain(wq);\n\tidxd_wq_free_irq(wq);\n\tidxd_wq_reset(wq);\n\tidxd_wq_free_resources(wq);\n\tpercpu_ref_exit(&wq->wq_active);\n\twq->type = IDXD_WQT_NONE;\n\twq->client_count = 0;\n}\n\nint idxd_device_drv_probe(struct idxd_dev *idxd_dev)\n{\n\tstruct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);\n\tint rc = 0;\n\n\t \n\tif (idxd->state != IDXD_DEV_DISABLED) {\n\t\tidxd->cmd_status = IDXD_SCMD_DEV_ENABLED;\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tspin_lock(&idxd->dev_lock);\n\tif (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\trc = idxd_device_config(idxd);\n\tspin_unlock(&idxd->dev_lock);\n\tif (rc < 0)\n\t\treturn -ENXIO;\n\n\t \n\tif (idxd->pasid != IOMMU_PASID_INVALID)\n\t\tidxd_set_user_intr(idxd, 1);\n\n\trc = idxd_device_evl_setup(idxd);\n\tif (rc < 0) {\n\t\tidxd->cmd_status = IDXD_SCMD_DEV_EVL_ERR;\n\t\treturn rc;\n\t}\n\n\t \n\trc = idxd_device_enable(idxd);\n\tif (rc < 0) {\n\t\tidxd_device_evl_free(idxd);\n\t\treturn rc;\n\t}\n\n\t \n\trc = idxd_register_dma_device(idxd);\n\tif (rc < 0) {\n\t\tidxd_device_disable(idxd);\n\t\tidxd_device_evl_free(idxd);\n\t\tidxd->cmd_status = IDXD_SCMD_DEV_DMA_ERR;\n\t\treturn rc;\n\t}\n\n\tidxd->cmd_status = 0;\n\treturn 0;\n}\n\nvoid idxd_device_drv_remove(struct idxd_dev *idxd_dev)\n{\n\tstruct device *dev = &idxd_dev->conf_dev;\n\tstruct idxd_device *idxd = idxd_dev_to_idxd(idxd_dev);\n\tint i;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tstruct idxd_wq *wq = idxd->wqs[i];\n\t\tstruct device *wq_dev = wq_confdev(wq);\n\n\t\tif (wq->state == IDXD_WQ_DISABLED)\n\t\t\tcontinue;\n\t\tdev_warn(dev, \"Active wq %d on disable %s.\\n\", i, dev_name(wq_dev));\n\t\tdevice_release_driver(wq_dev);\n\t}\n\n\tidxd_unregister_dma_device(idxd);\n\tidxd_device_disable(idxd);\n\tif (test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags))\n\t\tidxd_device_reset(idxd);\n\tidxd_device_evl_free(idxd);\n}\n\nstatic enum idxd_dev_type dev_types[] = {\n\tIDXD_DEV_DSA,\n\tIDXD_DEV_IAX,\n\tIDXD_DEV_NONE,\n};\n\nstruct idxd_device_driver idxd_drv = {\n\t.type = dev_types,\n\t.probe = idxd_device_drv_probe,\n\t.remove = idxd_device_drv_remove,\n\t.name = \"idxd\",\n};\nEXPORT_SYMBOL_GPL(idxd_drv);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}