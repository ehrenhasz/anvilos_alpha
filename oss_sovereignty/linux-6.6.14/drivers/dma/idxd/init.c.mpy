{
  "module_name": "init.c",
  "hash_id": "0118d39ff344eaa1e766dabaffa22433b879922bed88dd86c48c6cae0668bd06",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/idxd/init.c",
  "human_readable_source": "\n \n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/pci.h>\n#include <linux/interrupt.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/workqueue.h>\n#include <linux/fs.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/device.h>\n#include <linux/idr.h>\n#include <linux/iommu.h>\n#include <uapi/linux/idxd.h>\n#include <linux/dmaengine.h>\n#include \"../dmaengine.h\"\n#include \"registers.h\"\n#include \"idxd.h\"\n#include \"perfmon.h\"\n\nMODULE_VERSION(IDXD_DRIVER_VERSION);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Intel Corporation\");\nMODULE_IMPORT_NS(IDXD);\n\nstatic bool sva = true;\nmodule_param(sva, bool, 0644);\nMODULE_PARM_DESC(sva, \"Toggle SVA support on/off\");\n\nbool tc_override;\nmodule_param(tc_override, bool, 0644);\nMODULE_PARM_DESC(tc_override, \"Override traffic class defaults\");\n\n#define DRV_NAME \"idxd\"\n\nbool support_enqcmd;\nDEFINE_IDA(idxd_ida);\n\nstatic struct idxd_driver_data idxd_driver_data[] = {\n\t[IDXD_TYPE_DSA] = {\n\t\t.name_prefix = \"dsa\",\n\t\t.type = IDXD_TYPE_DSA,\n\t\t.compl_size = sizeof(struct dsa_completion_record),\n\t\t.align = 32,\n\t\t.dev_type = &dsa_device_type,\n\t\t.evl_cr_off = offsetof(struct dsa_evl_entry, cr),\n\t\t.cr_status_off = offsetof(struct dsa_completion_record, status),\n\t\t.cr_result_off = offsetof(struct dsa_completion_record, result),\n\t},\n\t[IDXD_TYPE_IAX] = {\n\t\t.name_prefix = \"iax\",\n\t\t.type = IDXD_TYPE_IAX,\n\t\t.compl_size = sizeof(struct iax_completion_record),\n\t\t.align = 64,\n\t\t.dev_type = &iax_device_type,\n\t\t.evl_cr_off = offsetof(struct iax_evl_entry, cr),\n\t\t.cr_status_off = offsetof(struct iax_completion_record, status),\n\t\t.cr_result_off = offsetof(struct iax_completion_record, error_code),\n\t},\n};\n\nstatic struct pci_device_id idxd_pci_tbl[] = {\n\t \n\t{ PCI_DEVICE_DATA(INTEL, DSA_SPR0, &idxd_driver_data[IDXD_TYPE_DSA]) },\n\n\t \n\t{ PCI_DEVICE_DATA(INTEL, IAX_SPR0, &idxd_driver_data[IDXD_TYPE_IAX]) },\n\t{ 0, }\n};\nMODULE_DEVICE_TABLE(pci, idxd_pci_tbl);\n\nstatic int idxd_setup_interrupts(struct idxd_device *idxd)\n{\n\tstruct pci_dev *pdev = idxd->pdev;\n\tstruct device *dev = &pdev->dev;\n\tstruct idxd_irq_entry *ie;\n\tint i, msixcnt;\n\tint rc = 0;\n\n\tmsixcnt = pci_msix_vec_count(pdev);\n\tif (msixcnt < 0) {\n\t\tdev_err(dev, \"Not MSI-X interrupt capable.\\n\");\n\t\treturn -ENOSPC;\n\t}\n\tidxd->irq_cnt = msixcnt;\n\n\trc = pci_alloc_irq_vectors(pdev, msixcnt, msixcnt, PCI_IRQ_MSIX);\n\tif (rc != msixcnt) {\n\t\tdev_err(dev, \"Failed enabling %d MSIX entries: %d\\n\", msixcnt, rc);\n\t\treturn -ENOSPC;\n\t}\n\tdev_dbg(dev, \"Enabled %d msix vectors\\n\", msixcnt);\n\n\n\tie = idxd_get_ie(idxd, 0);\n\tie->vector = pci_irq_vector(pdev, 0);\n\trc = request_threaded_irq(ie->vector, NULL, idxd_misc_thread, 0, \"idxd-misc\", ie);\n\tif (rc < 0) {\n\t\tdev_err(dev, \"Failed to allocate misc interrupt.\\n\");\n\t\tgoto err_misc_irq;\n\t}\n\tdev_dbg(dev, \"Requested idxd-misc handler on msix vector %d\\n\", ie->vector);\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\tint msix_idx = i + 1;\n\n\t\tie = idxd_get_ie(idxd, msix_idx);\n\t\tie->id = msix_idx;\n\t\tie->int_handle = INVALID_INT_HANDLE;\n\t\tie->pasid = IOMMU_PASID_INVALID;\n\n\t\tspin_lock_init(&ie->list_lock);\n\t\tinit_llist_head(&ie->pending_llist);\n\t\tINIT_LIST_HEAD(&ie->work_list);\n\t}\n\n\tidxd_unmask_error_interrupts(idxd);\n\treturn 0;\n\n err_misc_irq:\n\tidxd_mask_error_interrupts(idxd);\n\tpci_free_irq_vectors(pdev);\n\tdev_err(dev, \"No usable interrupts\\n\");\n\treturn rc;\n}\n\nstatic void idxd_cleanup_interrupts(struct idxd_device *idxd)\n{\n\tstruct pci_dev *pdev = idxd->pdev;\n\tstruct idxd_irq_entry *ie;\n\tint msixcnt;\n\n\tmsixcnt = pci_msix_vec_count(pdev);\n\tif (msixcnt <= 0)\n\t\treturn;\n\n\tie = idxd_get_ie(idxd, 0);\n\tidxd_mask_error_interrupts(idxd);\n\tfree_irq(ie->vector, ie);\n\tpci_free_irq_vectors(pdev);\n}\n\nstatic int idxd_setup_wqs(struct idxd_device *idxd)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tstruct idxd_wq *wq;\n\tstruct device *conf_dev;\n\tint i, rc;\n\n\tidxd->wqs = kcalloc_node(idxd->max_wqs, sizeof(struct idxd_wq *),\n\t\t\t\t GFP_KERNEL, dev_to_node(dev));\n\tif (!idxd->wqs)\n\t\treturn -ENOMEM;\n\n\tidxd->wq_enable_map = bitmap_zalloc_node(idxd->max_wqs, GFP_KERNEL, dev_to_node(dev));\n\tif (!idxd->wq_enable_map) {\n\t\tkfree(idxd->wqs);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\twq = kzalloc_node(sizeof(*wq), GFP_KERNEL, dev_to_node(dev));\n\t\tif (!wq) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tidxd_dev_set_type(&wq->idxd_dev, IDXD_DEV_WQ);\n\t\tconf_dev = wq_confdev(wq);\n\t\twq->id = i;\n\t\twq->idxd = idxd;\n\t\tdevice_initialize(wq_confdev(wq));\n\t\tconf_dev->parent = idxd_confdev(idxd);\n\t\tconf_dev->bus = &dsa_bus_type;\n\t\tconf_dev->type = &idxd_wq_device_type;\n\t\trc = dev_set_name(conf_dev, \"wq%d.%d\", idxd->id, wq->id);\n\t\tif (rc < 0) {\n\t\t\tput_device(conf_dev);\n\t\t\tgoto err;\n\t\t}\n\n\t\tmutex_init(&wq->wq_lock);\n\t\tinit_waitqueue_head(&wq->err_queue);\n\t\tinit_completion(&wq->wq_dead);\n\t\tinit_completion(&wq->wq_resurrect);\n\t\twq->max_xfer_bytes = WQ_DEFAULT_MAX_XFER;\n\t\tidxd_wq_set_max_batch_size(idxd->data->type, wq, WQ_DEFAULT_MAX_BATCH);\n\t\twq->enqcmds_retries = IDXD_ENQCMDS_RETRIES;\n\t\twq->wqcfg = kzalloc_node(idxd->wqcfg_size, GFP_KERNEL, dev_to_node(dev));\n\t\tif (!wq->wqcfg) {\n\t\t\tput_device(conf_dev);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (idxd->hw.wq_cap.op_config) {\n\t\t\twq->opcap_bmap = bitmap_zalloc(IDXD_MAX_OPCAP_BITS, GFP_KERNEL);\n\t\t\tif (!wq->opcap_bmap) {\n\t\t\t\tput_device(conf_dev);\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tbitmap_copy(wq->opcap_bmap, idxd->opcap_bmap, IDXD_MAX_OPCAP_BITS);\n\t\t}\n\t\tmutex_init(&wq->uc_lock);\n\t\txa_init(&wq->upasid_xa);\n\t\tidxd->wqs[i] = wq;\n\t}\n\n\treturn 0;\n\n err:\n\twhile (--i >= 0) {\n\t\twq = idxd->wqs[i];\n\t\tconf_dev = wq_confdev(wq);\n\t\tput_device(conf_dev);\n\t}\n\treturn rc;\n}\n\nstatic int idxd_setup_engines(struct idxd_device *idxd)\n{\n\tstruct idxd_engine *engine;\n\tstruct device *dev = &idxd->pdev->dev;\n\tstruct device *conf_dev;\n\tint i, rc;\n\n\tidxd->engines = kcalloc_node(idxd->max_engines, sizeof(struct idxd_engine *),\n\t\t\t\t     GFP_KERNEL, dev_to_node(dev));\n\tif (!idxd->engines)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < idxd->max_engines; i++) {\n\t\tengine = kzalloc_node(sizeof(*engine), GFP_KERNEL, dev_to_node(dev));\n\t\tif (!engine) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tidxd_dev_set_type(&engine->idxd_dev, IDXD_DEV_ENGINE);\n\t\tconf_dev = engine_confdev(engine);\n\t\tengine->id = i;\n\t\tengine->idxd = idxd;\n\t\tdevice_initialize(conf_dev);\n\t\tconf_dev->parent = idxd_confdev(idxd);\n\t\tconf_dev->bus = &dsa_bus_type;\n\t\tconf_dev->type = &idxd_engine_device_type;\n\t\trc = dev_set_name(conf_dev, \"engine%d.%d\", idxd->id, engine->id);\n\t\tif (rc < 0) {\n\t\t\tput_device(conf_dev);\n\t\t\tgoto err;\n\t\t}\n\n\t\tidxd->engines[i] = engine;\n\t}\n\n\treturn 0;\n\n err:\n\twhile (--i >= 0) {\n\t\tengine = idxd->engines[i];\n\t\tconf_dev = engine_confdev(engine);\n\t\tput_device(conf_dev);\n\t}\n\treturn rc;\n}\n\nstatic int idxd_setup_groups(struct idxd_device *idxd)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tstruct device *conf_dev;\n\tstruct idxd_group *group;\n\tint i, rc;\n\n\tidxd->groups = kcalloc_node(idxd->max_groups, sizeof(struct idxd_group *),\n\t\t\t\t    GFP_KERNEL, dev_to_node(dev));\n\tif (!idxd->groups)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < idxd->max_groups; i++) {\n\t\tgroup = kzalloc_node(sizeof(*group), GFP_KERNEL, dev_to_node(dev));\n\t\tif (!group) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tidxd_dev_set_type(&group->idxd_dev, IDXD_DEV_GROUP);\n\t\tconf_dev = group_confdev(group);\n\t\tgroup->id = i;\n\t\tgroup->idxd = idxd;\n\t\tdevice_initialize(conf_dev);\n\t\tconf_dev->parent = idxd_confdev(idxd);\n\t\tconf_dev->bus = &dsa_bus_type;\n\t\tconf_dev->type = &idxd_group_device_type;\n\t\trc = dev_set_name(conf_dev, \"group%d.%d\", idxd->id, group->id);\n\t\tif (rc < 0) {\n\t\t\tput_device(conf_dev);\n\t\t\tgoto err;\n\t\t}\n\n\t\tidxd->groups[i] = group;\n\t\tif (idxd->hw.version <= DEVICE_VERSION_2 && !tc_override) {\n\t\t\tgroup->tc_a = 1;\n\t\t\tgroup->tc_b = 1;\n\t\t} else {\n\t\t\tgroup->tc_a = -1;\n\t\t\tgroup->tc_b = -1;\n\t\t}\n\t\t \n\t\tgroup->rdbufs_allowed = idxd->max_rdbufs;\n\t}\n\n\treturn 0;\n\n err:\n\twhile (--i >= 0) {\n\t\tgroup = idxd->groups[i];\n\t\tput_device(group_confdev(group));\n\t}\n\treturn rc;\n}\n\nstatic void idxd_cleanup_internals(struct idxd_device *idxd)\n{\n\tint i;\n\n\tfor (i = 0; i < idxd->max_groups; i++)\n\t\tput_device(group_confdev(idxd->groups[i]));\n\tfor (i = 0; i < idxd->max_engines; i++)\n\t\tput_device(engine_confdev(idxd->engines[i]));\n\tfor (i = 0; i < idxd->max_wqs; i++)\n\t\tput_device(wq_confdev(idxd->wqs[i]));\n\tdestroy_workqueue(idxd->wq);\n}\n\nstatic int idxd_init_evl(struct idxd_device *idxd)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tstruct idxd_evl *evl;\n\n\tif (idxd->hw.gen_cap.evl_support == 0)\n\t\treturn 0;\n\n\tevl = kzalloc_node(sizeof(*evl), GFP_KERNEL, dev_to_node(dev));\n\tif (!evl)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&evl->lock);\n\tevl->size = IDXD_EVL_SIZE_MIN;\n\n\tidxd->evl_cache = kmem_cache_create(dev_name(idxd_confdev(idxd)),\n\t\t\t\t\t    sizeof(struct idxd_evl_fault) + evl_ent_size(idxd),\n\t\t\t\t\t    0, 0, NULL);\n\tif (!idxd->evl_cache) {\n\t\tkfree(evl);\n\t\treturn -ENOMEM;\n\t}\n\n\tidxd->evl = evl;\n\treturn 0;\n}\n\nstatic int idxd_setup_internals(struct idxd_device *idxd)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tint rc, i;\n\n\tinit_waitqueue_head(&idxd->cmd_waitq);\n\n\trc = idxd_setup_wqs(idxd);\n\tif (rc < 0)\n\t\tgoto err_wqs;\n\n\trc = idxd_setup_engines(idxd);\n\tif (rc < 0)\n\t\tgoto err_engine;\n\n\trc = idxd_setup_groups(idxd);\n\tif (rc < 0)\n\t\tgoto err_group;\n\n\tidxd->wq = create_workqueue(dev_name(dev));\n\tif (!idxd->wq) {\n\t\trc = -ENOMEM;\n\t\tgoto err_wkq_create;\n\t}\n\n\trc = idxd_init_evl(idxd);\n\tif (rc < 0)\n\t\tgoto err_evl;\n\n\treturn 0;\n\n err_evl:\n\tdestroy_workqueue(idxd->wq);\n err_wkq_create:\n\tfor (i = 0; i < idxd->max_groups; i++)\n\t\tput_device(group_confdev(idxd->groups[i]));\n err_group:\n\tfor (i = 0; i < idxd->max_engines; i++)\n\t\tput_device(engine_confdev(idxd->engines[i]));\n err_engine:\n\tfor (i = 0; i < idxd->max_wqs; i++)\n\t\tput_device(wq_confdev(idxd->wqs[i]));\n err_wqs:\n\treturn rc;\n}\n\nstatic void idxd_read_table_offsets(struct idxd_device *idxd)\n{\n\tunion offsets_reg offsets;\n\tstruct device *dev = &idxd->pdev->dev;\n\n\toffsets.bits[0] = ioread64(idxd->reg_base + IDXD_TABLE_OFFSET);\n\toffsets.bits[1] = ioread64(idxd->reg_base + IDXD_TABLE_OFFSET + sizeof(u64));\n\tidxd->grpcfg_offset = offsets.grpcfg * IDXD_TABLE_MULT;\n\tdev_dbg(dev, \"IDXD Group Config Offset: %#x\\n\", idxd->grpcfg_offset);\n\tidxd->wqcfg_offset = offsets.wqcfg * IDXD_TABLE_MULT;\n\tdev_dbg(dev, \"IDXD Work Queue Config Offset: %#x\\n\", idxd->wqcfg_offset);\n\tidxd->msix_perm_offset = offsets.msix_perm * IDXD_TABLE_MULT;\n\tdev_dbg(dev, \"IDXD MSIX Permission Offset: %#x\\n\", idxd->msix_perm_offset);\n\tidxd->perfmon_offset = offsets.perfmon * IDXD_TABLE_MULT;\n\tdev_dbg(dev, \"IDXD Perfmon Offset: %#x\\n\", idxd->perfmon_offset);\n}\n\nvoid multi_u64_to_bmap(unsigned long *bmap, u64 *val, int count)\n{\n\tint i, j, nr;\n\n\tfor (i = 0, nr = 0; i < count; i++) {\n\t\tfor (j = 0; j < BITS_PER_LONG_LONG; j++) {\n\t\t\tif (val[i] & BIT(j))\n\t\t\t\tset_bit(nr, bmap);\n\t\t\tnr++;\n\t\t}\n\t}\n}\n\nstatic void idxd_read_caps(struct idxd_device *idxd)\n{\n\tstruct device *dev = &idxd->pdev->dev;\n\tint i;\n\n\t \n\tidxd->hw.gen_cap.bits = ioread64(idxd->reg_base + IDXD_GENCAP_OFFSET);\n\tdev_dbg(dev, \"gen_cap: %#llx\\n\", idxd->hw.gen_cap.bits);\n\n\tif (idxd->hw.gen_cap.cmd_cap) {\n\t\tidxd->hw.cmd_cap = ioread32(idxd->reg_base + IDXD_CMDCAP_OFFSET);\n\t\tdev_dbg(dev, \"cmd_cap: %#x\\n\", idxd->hw.cmd_cap);\n\t}\n\n\t \n\tif (idxd->hw.cmd_cap & BIT(IDXD_CMD_REQUEST_INT_HANDLE))\n\t\tidxd->request_int_handles = true;\n\n\tidxd->max_xfer_bytes = 1ULL << idxd->hw.gen_cap.max_xfer_shift;\n\tdev_dbg(dev, \"max xfer size: %llu bytes\\n\", idxd->max_xfer_bytes);\n\tidxd_set_max_batch_size(idxd->data->type, idxd, 1U << idxd->hw.gen_cap.max_batch_shift);\n\tdev_dbg(dev, \"max batch size: %u\\n\", idxd->max_batch_size);\n\tif (idxd->hw.gen_cap.config_en)\n\t\tset_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags);\n\n\t \n\tidxd->hw.group_cap.bits =\n\t\tioread64(idxd->reg_base + IDXD_GRPCAP_OFFSET);\n\tdev_dbg(dev, \"group_cap: %#llx\\n\", idxd->hw.group_cap.bits);\n\tidxd->max_groups = idxd->hw.group_cap.num_groups;\n\tdev_dbg(dev, \"max groups: %u\\n\", idxd->max_groups);\n\tidxd->max_rdbufs = idxd->hw.group_cap.total_rdbufs;\n\tdev_dbg(dev, \"max read buffers: %u\\n\", idxd->max_rdbufs);\n\tidxd->nr_rdbufs = idxd->max_rdbufs;\n\n\t \n\tidxd->hw.engine_cap.bits =\n\t\tioread64(idxd->reg_base + IDXD_ENGCAP_OFFSET);\n\tdev_dbg(dev, \"engine_cap: %#llx\\n\", idxd->hw.engine_cap.bits);\n\tidxd->max_engines = idxd->hw.engine_cap.num_engines;\n\tdev_dbg(dev, \"max engines: %u\\n\", idxd->max_engines);\n\n\t \n\tidxd->hw.wq_cap.bits = ioread64(idxd->reg_base + IDXD_WQCAP_OFFSET);\n\tdev_dbg(dev, \"wq_cap: %#llx\\n\", idxd->hw.wq_cap.bits);\n\tidxd->max_wq_size = idxd->hw.wq_cap.total_wq_size;\n\tdev_dbg(dev, \"total workqueue size: %u\\n\", idxd->max_wq_size);\n\tidxd->max_wqs = idxd->hw.wq_cap.num_wqs;\n\tdev_dbg(dev, \"max workqueues: %u\\n\", idxd->max_wqs);\n\tidxd->wqcfg_size = 1 << (idxd->hw.wq_cap.wqcfg_size + IDXD_WQCFG_MIN);\n\tdev_dbg(dev, \"wqcfg size: %u\\n\", idxd->wqcfg_size);\n\n\t \n\tfor (i = 0; i < 4; i++) {\n\t\tidxd->hw.opcap.bits[i] = ioread64(idxd->reg_base +\n\t\t\t\tIDXD_OPCAP_OFFSET + i * sizeof(u64));\n\t\tdev_dbg(dev, \"opcap[%d]: %#llx\\n\", i, idxd->hw.opcap.bits[i]);\n\t}\n\tmulti_u64_to_bmap(idxd->opcap_bmap, &idxd->hw.opcap.bits[0], 4);\n\n\t \n\tif (idxd->data->type == IDXD_TYPE_IAX && idxd->hw.version >= DEVICE_VERSION_2)\n\t\tidxd->hw.iaa_cap.bits = ioread64(idxd->reg_base + IDXD_IAACAP_OFFSET);\n}\n\nstatic struct idxd_device *idxd_alloc(struct pci_dev *pdev, struct idxd_driver_data *data)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct device *conf_dev;\n\tstruct idxd_device *idxd;\n\tint rc;\n\n\tidxd = kzalloc_node(sizeof(*idxd), GFP_KERNEL, dev_to_node(dev));\n\tif (!idxd)\n\t\treturn NULL;\n\n\tconf_dev = idxd_confdev(idxd);\n\tidxd->pdev = pdev;\n\tidxd->data = data;\n\tidxd_dev_set_type(&idxd->idxd_dev, idxd->data->type);\n\tidxd->id = ida_alloc(&idxd_ida, GFP_KERNEL);\n\tif (idxd->id < 0)\n\t\treturn NULL;\n\n\tidxd->opcap_bmap = bitmap_zalloc_node(IDXD_MAX_OPCAP_BITS, GFP_KERNEL, dev_to_node(dev));\n\tif (!idxd->opcap_bmap) {\n\t\tida_free(&idxd_ida, idxd->id);\n\t\treturn NULL;\n\t}\n\n\tdevice_initialize(conf_dev);\n\tconf_dev->parent = dev;\n\tconf_dev->bus = &dsa_bus_type;\n\tconf_dev->type = idxd->data->dev_type;\n\trc = dev_set_name(conf_dev, \"%s%d\", idxd->data->name_prefix, idxd->id);\n\tif (rc < 0) {\n\t\tput_device(conf_dev);\n\t\treturn NULL;\n\t}\n\n\tspin_lock_init(&idxd->dev_lock);\n\tspin_lock_init(&idxd->cmd_lock);\n\n\treturn idxd;\n}\n\nstatic int idxd_enable_system_pasid(struct idxd_device *idxd)\n{\n\tstruct pci_dev *pdev = idxd->pdev;\n\tstruct device *dev = &pdev->dev;\n\tstruct iommu_domain *domain;\n\tioasid_t pasid;\n\tint ret;\n\n\t \n\tdomain = iommu_get_domain_for_dev(dev);\n\tif (!domain)\n\t\treturn -EPERM;\n\n\tpasid = iommu_alloc_global_pasid(dev);\n\tif (pasid == IOMMU_PASID_INVALID)\n\t\treturn -ENOSPC;\n\n\t \n\tret = iommu_attach_device_pasid(domain, dev, pasid);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to attach device pasid %d, domain type %d\",\n\t\t\tpasid, domain->type);\n\t\tiommu_free_global_pasid(pasid);\n\t\treturn ret;\n\t}\n\n\t \n\tidxd_set_user_intr(idxd, 1);\n\tidxd->pasid = pasid;\n\n\treturn ret;\n}\n\nstatic void idxd_disable_system_pasid(struct idxd_device *idxd)\n{\n\tstruct pci_dev *pdev = idxd->pdev;\n\tstruct device *dev = &pdev->dev;\n\tstruct iommu_domain *domain;\n\n\tdomain = iommu_get_domain_for_dev(dev);\n\tif (!domain)\n\t\treturn;\n\n\tiommu_detach_device_pasid(domain, dev, idxd->pasid);\n\tiommu_free_global_pasid(idxd->pasid);\n\n\tidxd_set_user_intr(idxd, 0);\n\tidxd->sva = NULL;\n\tidxd->pasid = IOMMU_PASID_INVALID;\n}\n\nstatic int idxd_enable_sva(struct pci_dev *pdev)\n{\n\tint ret;\n\n\tret = iommu_dev_enable_feature(&pdev->dev, IOMMU_DEV_FEAT_IOPF);\n\tif (ret)\n\t\treturn ret;\n\n\tret = iommu_dev_enable_feature(&pdev->dev, IOMMU_DEV_FEAT_SVA);\n\tif (ret)\n\t\tiommu_dev_disable_feature(&pdev->dev, IOMMU_DEV_FEAT_IOPF);\n\n\treturn ret;\n}\n\nstatic void idxd_disable_sva(struct pci_dev *pdev)\n{\n\tiommu_dev_disable_feature(&pdev->dev, IOMMU_DEV_FEAT_SVA);\n\tiommu_dev_disable_feature(&pdev->dev, IOMMU_DEV_FEAT_IOPF);\n}\n\nstatic int idxd_probe(struct idxd_device *idxd)\n{\n\tstruct pci_dev *pdev = idxd->pdev;\n\tstruct device *dev = &pdev->dev;\n\tint rc;\n\n\tdev_dbg(dev, \"%s entered and resetting device\\n\", __func__);\n\trc = idxd_device_init_reset(idxd);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tdev_dbg(dev, \"IDXD reset complete\\n\");\n\n\tif (IS_ENABLED(CONFIG_INTEL_IDXD_SVM) && sva) {\n\t\tif (idxd_enable_sva(pdev)) {\n\t\t\tdev_warn(dev, \"Unable to turn on user SVA feature.\\n\");\n\t\t} else {\n\t\t\tset_bit(IDXD_FLAG_USER_PASID_ENABLED, &idxd->flags);\n\n\t\t\trc = idxd_enable_system_pasid(idxd);\n\t\t\tif (rc)\n\t\t\t\tdev_warn(dev, \"No in-kernel DMA with PASID. %d\\n\", rc);\n\t\t\telse\n\t\t\t\tset_bit(IDXD_FLAG_PASID_ENABLED, &idxd->flags);\n\t\t}\n\t} else if (!sva) {\n\t\tdev_warn(dev, \"User forced SVA off via module param.\\n\");\n\t}\n\n\tidxd_read_caps(idxd);\n\tidxd_read_table_offsets(idxd);\n\n\trc = idxd_setup_internals(idxd);\n\tif (rc)\n\t\tgoto err;\n\n\t \n\tif (!test_bit(IDXD_FLAG_CONFIGURABLE, &idxd->flags)) {\n\t\tdev_dbg(dev, \"Loading RO device config\\n\");\n\t\trc = idxd_device_load_config(idxd);\n\t\tif (rc < 0)\n\t\t\tgoto err_config;\n\t}\n\n\trc = idxd_setup_interrupts(idxd);\n\tif (rc)\n\t\tgoto err_config;\n\n\tidxd->major = idxd_cdev_get_major(idxd);\n\n\trc = perfmon_pmu_init(idxd);\n\tif (rc < 0)\n\t\tdev_warn(dev, \"Failed to initialize perfmon. No PMU support: %d\\n\", rc);\n\n\tdev_dbg(dev, \"IDXD device %d probed successfully\\n\", idxd->id);\n\treturn 0;\n\n err_config:\n\tidxd_cleanup_internals(idxd);\n err:\n\tif (device_pasid_enabled(idxd))\n\t\tidxd_disable_system_pasid(idxd);\n\tif (device_user_pasid_enabled(idxd))\n\t\tidxd_disable_sva(pdev);\n\treturn rc;\n}\n\nstatic void idxd_cleanup(struct idxd_device *idxd)\n{\n\tperfmon_pmu_remove(idxd);\n\tidxd_cleanup_interrupts(idxd);\n\tidxd_cleanup_internals(idxd);\n\tif (device_pasid_enabled(idxd))\n\t\tidxd_disable_system_pasid(idxd);\n\tif (device_user_pasid_enabled(idxd))\n\t\tidxd_disable_sva(idxd->pdev);\n}\n\nstatic int idxd_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct idxd_device *idxd;\n\tstruct idxd_driver_data *data = (struct idxd_driver_data *)id->driver_data;\n\tint rc;\n\n\trc = pci_enable_device(pdev);\n\tif (rc)\n\t\treturn rc;\n\n\tdev_dbg(dev, \"Alloc IDXD context\\n\");\n\tidxd = idxd_alloc(pdev, data);\n\tif (!idxd) {\n\t\trc = -ENOMEM;\n\t\tgoto err_idxd_alloc;\n\t}\n\n\tdev_dbg(dev, \"Mapping BARs\\n\");\n\tidxd->reg_base = pci_iomap(pdev, IDXD_MMIO_BAR, 0);\n\tif (!idxd->reg_base) {\n\t\trc = -ENOMEM;\n\t\tgoto err_iomap;\n\t}\n\n\tdev_dbg(dev, \"Set DMA masks\\n\");\n\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (rc)\n\t\tgoto err;\n\n\tdev_dbg(dev, \"Set PCI master\\n\");\n\tpci_set_master(pdev);\n\tpci_set_drvdata(pdev, idxd);\n\n\tidxd->hw.version = ioread32(idxd->reg_base + IDXD_VER_OFFSET);\n\trc = idxd_probe(idxd);\n\tif (rc) {\n\t\tdev_err(dev, \"Intel(R) IDXD DMA Engine init failed\\n\");\n\t\tgoto err;\n\t}\n\n\trc = idxd_register_devices(idxd);\n\tif (rc) {\n\t\tdev_err(dev, \"IDXD sysfs setup failed\\n\");\n\t\tgoto err_dev_register;\n\t}\n\n\trc = idxd_device_init_debugfs(idxd);\n\tif (rc)\n\t\tdev_warn(dev, \"IDXD debugfs failed to setup\\n\");\n\n\tdev_info(&pdev->dev, \"Intel(R) Accelerator Device (v%x)\\n\",\n\t\t idxd->hw.version);\n\n\treturn 0;\n\n err_dev_register:\n\tidxd_cleanup(idxd);\n err:\n\tpci_iounmap(pdev, idxd->reg_base);\n err_iomap:\n\tput_device(idxd_confdev(idxd));\n err_idxd_alloc:\n\tpci_disable_device(pdev);\n\treturn rc;\n}\n\nvoid idxd_wqs_quiesce(struct idxd_device *idxd)\n{\n\tstruct idxd_wq *wq;\n\tint i;\n\n\tfor (i = 0; i < idxd->max_wqs; i++) {\n\t\twq = idxd->wqs[i];\n\t\tif (wq->state == IDXD_WQ_ENABLED && wq->type == IDXD_WQT_KERNEL)\n\t\t\tidxd_wq_quiesce(wq);\n\t}\n}\n\nstatic void idxd_shutdown(struct pci_dev *pdev)\n{\n\tstruct idxd_device *idxd = pci_get_drvdata(pdev);\n\tstruct idxd_irq_entry *irq_entry;\n\tint rc;\n\n\trc = idxd_device_disable(idxd);\n\tif (rc)\n\t\tdev_err(&pdev->dev, \"Disabling device failed\\n\");\n\n\tirq_entry = &idxd->ie;\n\tsynchronize_irq(irq_entry->vector);\n\tidxd_mask_error_interrupts(idxd);\n\tflush_workqueue(idxd->wq);\n}\n\nstatic void idxd_remove(struct pci_dev *pdev)\n{\n\tstruct idxd_device *idxd = pci_get_drvdata(pdev);\n\tstruct idxd_irq_entry *irq_entry;\n\n\tidxd_unregister_devices(idxd);\n\t \n\tget_device(idxd_confdev(idxd));\n\tdevice_unregister(idxd_confdev(idxd));\n\tidxd_shutdown(pdev);\n\tif (device_pasid_enabled(idxd))\n\t\tidxd_disable_system_pasid(idxd);\n\tidxd_device_remove_debugfs(idxd);\n\n\tirq_entry = idxd_get_ie(idxd, 0);\n\tfree_irq(irq_entry->vector, irq_entry);\n\tpci_free_irq_vectors(pdev);\n\tpci_iounmap(pdev, idxd->reg_base);\n\tif (device_user_pasid_enabled(idxd))\n\t\tidxd_disable_sva(pdev);\n\tpci_disable_device(pdev);\n\tdestroy_workqueue(idxd->wq);\n\tperfmon_pmu_remove(idxd);\n\tput_device(idxd_confdev(idxd));\n}\n\nstatic struct pci_driver idxd_pci_driver = {\n\t.name\t\t= DRV_NAME,\n\t.id_table\t= idxd_pci_tbl,\n\t.probe\t\t= idxd_pci_probe,\n\t.remove\t\t= idxd_remove,\n\t.shutdown\t= idxd_shutdown,\n};\n\nstatic int __init idxd_init_module(void)\n{\n\tint err;\n\n\t \n\tif (!cpu_feature_enabled(X86_FEATURE_MOVDIR64B)) {\n\t\tpr_warn(\"idxd driver failed to load without MOVDIR64B.\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!cpu_feature_enabled(X86_FEATURE_ENQCMD))\n\t\tpr_warn(\"Platform does not have ENQCMD(S) support.\\n\");\n\telse\n\t\tsupport_enqcmd = true;\n\n\tperfmon_init();\n\n\terr = idxd_driver_register(&idxd_drv);\n\tif (err < 0)\n\t\tgoto err_idxd_driver_register;\n\n\terr = idxd_driver_register(&idxd_dmaengine_drv);\n\tif (err < 0)\n\t\tgoto err_idxd_dmaengine_driver_register;\n\n\terr = idxd_driver_register(&idxd_user_drv);\n\tif (err < 0)\n\t\tgoto err_idxd_user_driver_register;\n\n\terr = idxd_cdev_register();\n\tif (err)\n\t\tgoto err_cdev_register;\n\n\terr = idxd_init_debugfs();\n\tif (err)\n\t\tgoto err_debugfs;\n\n\terr = pci_register_driver(&idxd_pci_driver);\n\tif (err)\n\t\tgoto err_pci_register;\n\n\treturn 0;\n\nerr_pci_register:\n\tidxd_remove_debugfs();\nerr_debugfs:\n\tidxd_cdev_remove();\nerr_cdev_register:\n\tidxd_driver_unregister(&idxd_user_drv);\nerr_idxd_user_driver_register:\n\tidxd_driver_unregister(&idxd_dmaengine_drv);\nerr_idxd_dmaengine_driver_register:\n\tidxd_driver_unregister(&idxd_drv);\nerr_idxd_driver_register:\n\treturn err;\n}\nmodule_init(idxd_init_module);\n\nstatic void __exit idxd_exit_module(void)\n{\n\tidxd_driver_unregister(&idxd_user_drv);\n\tidxd_driver_unregister(&idxd_dmaengine_drv);\n\tidxd_driver_unregister(&idxd_drv);\n\tpci_unregister_driver(&idxd_pci_driver);\n\tidxd_cdev_remove();\n\tperfmon_exit();\n\tidxd_remove_debugfs();\n}\nmodule_exit(idxd_exit_module);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}