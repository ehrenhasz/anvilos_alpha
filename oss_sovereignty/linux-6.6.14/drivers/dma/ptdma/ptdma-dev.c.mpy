{
  "module_name": "ptdma-dev.c",
  "hash_id": "dc7c50061c67b15a73ab31bc028beb5a967f303c918d07144847635962fdb97a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/ptdma/ptdma-dev.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/dma-mapping.h>\n#include <linux/debugfs.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include \"ptdma.h\"\n\n \nstatic char *pt_error_codes[] = {\n\t\"\",\n\t\"ERR 01: ILLEGAL_ENGINE\",\n\t\"ERR 03: ILLEGAL_FUNCTION_TYPE\",\n\t\"ERR 04: ILLEGAL_FUNCTION_MODE\",\n\t\"ERR 06: ILLEGAL_FUNCTION_SIZE\",\n\t\"ERR 08: ILLEGAL_FUNCTION_RSVD\",\n\t\"ERR 09: ILLEGAL_BUFFER_LENGTH\",\n\t\"ERR 10: VLSB_FAULT\",\n\t\"ERR 11: ILLEGAL_MEM_ADDR\",\n\t\"ERR 12: ILLEGAL_MEM_SEL\",\n\t\"ERR 13: ILLEGAL_CONTEXT_ID\",\n\t\"ERR 15: 0xF Reserved\",\n\t\"ERR 18: CMD_TIMEOUT\",\n\t\"ERR 19: IDMA0_AXI_SLVERR\",\n\t\"ERR 20: IDMA0_AXI_DECERR\",\n\t\"ERR 21: 0x15 Reserved\",\n\t\"ERR 22: IDMA1_AXI_SLAVE_FAULT\",\n\t\"ERR 23: IDMA1_AIXI_DECERR\",\n\t\"ERR 24: 0x18 Reserved\",\n\t\"ERR 27: 0x1B Reserved\",\n\t\"ERR 38: ODMA0_AXI_SLVERR\",\n\t\"ERR 39: ODMA0_AXI_DECERR\",\n\t\"ERR 40: 0x28 Reserved\",\n\t\"ERR 41: ODMA1_AXI_SLVERR\",\n\t\"ERR 42: ODMA1_AXI_DECERR\",\n\t\"ERR 43: LSB_PARITY_ERR\",\n};\n\nstatic void pt_log_error(struct pt_device *d, int e)\n{\n\tdev_err(d->dev, \"PTDMA error: %s (0x%x)\\n\", pt_error_codes[e], e);\n}\n\nvoid pt_start_queue(struct pt_cmd_queue *cmd_q)\n{\n\t \n\tiowrite32(cmd_q->qcontrol | CMD_Q_RUN, cmd_q->reg_control);\n}\n\nvoid pt_stop_queue(struct pt_cmd_queue *cmd_q)\n{\n\t \n\tiowrite32(cmd_q->qcontrol & ~CMD_Q_RUN, cmd_q->reg_control);\n}\n\nstatic int pt_core_execute_cmd(struct ptdma_desc *desc, struct pt_cmd_queue *cmd_q)\n{\n\tbool soc = FIELD_GET(DWORD0_SOC, desc->dw0);\n\tu8 *q_desc = (u8 *)&cmd_q->qbase[cmd_q->qidx];\n\tu32 tail;\n\tunsigned long flags;\n\n\tif (soc) {\n\t\tdesc->dw0 |= FIELD_PREP(DWORD0_IOC, desc->dw0);\n\t\tdesc->dw0 &= ~DWORD0_SOC;\n\t}\n\tspin_lock_irqsave(&cmd_q->q_lock, flags);\n\n\t \n\tmemcpy(q_desc, desc, 32);\n\tcmd_q->qidx = (cmd_q->qidx + 1) % CMD_Q_LEN;\n\n\t \n\twmb();\n\n\t \n\ttail = lower_32_bits(cmd_q->qdma_tail + cmd_q->qidx * Q_DESC_SIZE);\n\tiowrite32(tail, cmd_q->reg_control + 0x0004);\n\n\t \n\tpt_start_queue(cmd_q);\n\tspin_unlock_irqrestore(&cmd_q->q_lock, flags);\n\n\treturn 0;\n}\n\nint pt_core_perform_passthru(struct pt_cmd_queue *cmd_q,\n\t\t\t     struct pt_passthru_engine *pt_engine)\n{\n\tstruct ptdma_desc desc;\n\tstruct pt_device *pt = container_of(cmd_q, struct pt_device, cmd_q);\n\n\tcmd_q->cmd_error = 0;\n\tcmd_q->total_pt_ops++;\n\tmemset(&desc, 0, sizeof(desc));\n\tdesc.dw0 = CMD_DESC_DW0_VAL;\n\tdesc.length = pt_engine->src_len;\n\tdesc.src_lo = lower_32_bits(pt_engine->src_dma);\n\tdesc.dw3.src_hi = upper_32_bits(pt_engine->src_dma);\n\tdesc.dst_lo = lower_32_bits(pt_engine->dst_dma);\n\tdesc.dw5.dst_hi = upper_32_bits(pt_engine->dst_dma);\n\n\tif (cmd_q->int_en)\n\t\tpt_core_enable_queue_interrupts(pt);\n\telse\n\t\tpt_core_disable_queue_interrupts(pt);\n\n\treturn pt_core_execute_cmd(&desc, cmd_q);\n}\n\nstatic void pt_do_cmd_complete(unsigned long data)\n{\n\tstruct pt_tasklet_data *tdata = (struct pt_tasklet_data *)data;\n\tstruct pt_cmd *cmd = tdata->cmd;\n\tstruct pt_cmd_queue *cmd_q = &cmd->pt->cmd_q;\n\tu32 tail;\n\n\tif (cmd_q->cmd_error) {\n\t        \n\t\ttail = lower_32_bits(cmd_q->qdma_tail + cmd_q->qidx * Q_DESC_SIZE);\n\t\tpt_log_error(cmd_q->pt, cmd_q->cmd_error);\n\t\tiowrite32(tail, cmd_q->reg_control + 0x0008);\n\t}\n\n\tcmd->pt_cmd_callback(cmd->data, cmd->ret);\n}\n\nvoid pt_check_status_trans(struct pt_device *pt, struct pt_cmd_queue *cmd_q)\n{\n\tu32 status;\n\n\tstatus = ioread32(cmd_q->reg_control + 0x0010);\n\tif (status) {\n\t\tcmd_q->int_status = status;\n\t\tcmd_q->q_status = ioread32(cmd_q->reg_control + 0x0100);\n\t\tcmd_q->q_int_status = ioread32(cmd_q->reg_control + 0x0104);\n\n\t\t \n\t\tif ((status & INT_ERROR) && !cmd_q->cmd_error)\n\t\t\tcmd_q->cmd_error = CMD_Q_ERROR(cmd_q->q_status);\n\n\t\t \n\t\tiowrite32(status, cmd_q->reg_control + 0x0010);\n\t\tpt_do_cmd_complete((ulong)&pt->tdata);\n\t}\n}\n\nstatic irqreturn_t pt_core_irq_handler(int irq, void *data)\n{\n\tstruct pt_device *pt = data;\n\tstruct pt_cmd_queue *cmd_q = &pt->cmd_q;\n\n\tpt_core_disable_queue_interrupts(pt);\n\tpt->total_interrupts++;\n\tpt_check_status_trans(pt, cmd_q);\n\tpt_core_enable_queue_interrupts(pt);\n\treturn IRQ_HANDLED;\n}\n\nint pt_core_init(struct pt_device *pt)\n{\n\tchar dma_pool_name[MAX_DMAPOOL_NAME_LEN];\n\tstruct pt_cmd_queue *cmd_q = &pt->cmd_q;\n\tu32 dma_addr_lo, dma_addr_hi;\n\tstruct device *dev = pt->dev;\n\tstruct dma_pool *dma_pool;\n\tint ret;\n\n\t \n\tsnprintf(dma_pool_name, sizeof(dma_pool_name), \"%s_q\", dev_name(pt->dev));\n\n\tdma_pool = dma_pool_create(dma_pool_name, dev,\n\t\t\t\t   PT_DMAPOOL_MAX_SIZE,\n\t\t\t\t   PT_DMAPOOL_ALIGN, 0);\n\tif (!dma_pool)\n\t\treturn -ENOMEM;\n\n\t \n\tiowrite32(CMD_CONFIG_VHB_EN, pt->io_regs + CMD_CONFIG_OFFSET);\n\tiowrite32(CMD_QUEUE_PRIO, pt->io_regs + CMD_QUEUE_PRIO_OFFSET);\n\tiowrite32(CMD_TIMEOUT_DISABLE, pt->io_regs + CMD_TIMEOUT_OFFSET);\n\tiowrite32(CMD_CLK_GATE_CONFIG, pt->io_regs + CMD_CLK_GATE_CTL_OFFSET);\n\tiowrite32(CMD_CONFIG_REQID, pt->io_regs + CMD_REQID_CONFIG_OFFSET);\n\n\tcmd_q->pt = pt;\n\tcmd_q->dma_pool = dma_pool;\n\tspin_lock_init(&cmd_q->q_lock);\n\n\t \n\tcmd_q->qsize = Q_SIZE(Q_DESC_SIZE);\n\tcmd_q->qbase = dma_alloc_coherent(dev, cmd_q->qsize,\n\t\t\t\t\t  &cmd_q->qbase_dma,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!cmd_q->qbase) {\n\t\tdev_err(dev, \"unable to allocate command queue\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto e_destroy_pool;\n\t}\n\n\tcmd_q->qidx = 0;\n\n\t \n\tcmd_q->reg_control = pt->io_regs + CMD_Q_STATUS_INCR;\n\n\t \n\tpt_core_disable_queue_interrupts(pt);\n\n\tcmd_q->qcontrol = 0;  \n\tiowrite32(cmd_q->qcontrol, cmd_q->reg_control);\n\n\tioread32(cmd_q->reg_control + 0x0104);\n\tioread32(cmd_q->reg_control + 0x0100);\n\n\t \n\tiowrite32(SUPPORTED_INTERRUPTS, cmd_q->reg_control + 0x0010);\n\n\t \n\tret = request_irq(pt->pt_irq, pt_core_irq_handler, 0, dev_name(pt->dev), pt);\n\tif (ret) {\n\t\tdev_err(dev, \"unable to allocate an IRQ\\n\");\n\t\tgoto e_free_dma;\n\t}\n\n\t \n\tcmd_q->qcontrol &= ~CMD_Q_SIZE;\n\tcmd_q->qcontrol |= FIELD_PREP(CMD_Q_SIZE, QUEUE_SIZE_VAL);\n\n\tcmd_q->qdma_tail = cmd_q->qbase_dma;\n\tdma_addr_lo = lower_32_bits(cmd_q->qdma_tail);\n\tiowrite32((u32)dma_addr_lo, cmd_q->reg_control + 0x0004);\n\tiowrite32((u32)dma_addr_lo, cmd_q->reg_control + 0x0008);\n\n\tdma_addr_hi = upper_32_bits(cmd_q->qdma_tail);\n\tcmd_q->qcontrol |= (dma_addr_hi << 16);\n\tiowrite32(cmd_q->qcontrol, cmd_q->reg_control);\n\n\tpt_core_enable_queue_interrupts(pt);\n\n\t \n\tret = pt_dmaengine_register(pt);\n\tif (ret)\n\t\tgoto e_free_irq;\n\n\t \n\tptdma_debugfs_setup(pt);\n\n\treturn 0;\n\ne_free_irq:\n\tfree_irq(pt->pt_irq, pt);\n\ne_free_dma:\n\tdma_free_coherent(dev, cmd_q->qsize, cmd_q->qbase, cmd_q->qbase_dma);\n\ne_destroy_pool:\n\tdma_pool_destroy(pt->cmd_q.dma_pool);\n\n\treturn ret;\n}\n\nvoid pt_core_destroy(struct pt_device *pt)\n{\n\tstruct device *dev = pt->dev;\n\tstruct pt_cmd_queue *cmd_q = &pt->cmd_q;\n\tstruct pt_cmd *cmd;\n\n\t \n\tpt_dmaengine_unregister(pt);\n\n\t \n\tpt_core_disable_queue_interrupts(pt);\n\n\t \n\tpt_stop_queue(cmd_q);\n\n\t \n\tiowrite32(SUPPORTED_INTERRUPTS, cmd_q->reg_control + 0x0010);\n\tioread32(cmd_q->reg_control + 0x0104);\n\tioread32(cmd_q->reg_control + 0x0100);\n\n\tfree_irq(pt->pt_irq, pt);\n\n\tdma_free_coherent(dev, cmd_q->qsize, cmd_q->qbase,\n\t\t\t  cmd_q->qbase_dma);\n\n\t \n\twhile (!list_empty(&pt->cmd)) {\n\t\t \n\t\tcmd = list_first_entry(&pt->cmd, struct pt_cmd, entry);\n\t\tlist_del(&cmd->entry);\n\t\tcmd->pt_cmd_callback(cmd->data, -ENODEV);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}