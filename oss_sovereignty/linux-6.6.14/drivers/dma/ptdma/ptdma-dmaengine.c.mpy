{
  "module_name": "ptdma-dmaengine.c",
  "hash_id": "aebd6dc6a9319952e4c0ad07b2d3b22ba99d6e1d05f7175dd5f7c74579ff9c63",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/ptdma/ptdma-dmaengine.c",
  "human_readable_source": "\n \n\n#include \"ptdma.h\"\n#include \"../dmaengine.h\"\n#include \"../virt-dma.h\"\n\nstatic inline struct pt_dma_chan *to_pt_chan(struct dma_chan *dma_chan)\n{\n\treturn container_of(dma_chan, struct pt_dma_chan, vc.chan);\n}\n\nstatic inline struct pt_dma_desc *to_pt_desc(struct virt_dma_desc *vd)\n{\n\treturn container_of(vd, struct pt_dma_desc, vd);\n}\n\nstatic void pt_free_chan_resources(struct dma_chan *dma_chan)\n{\n\tstruct pt_dma_chan *chan = to_pt_chan(dma_chan);\n\n\tvchan_free_chan_resources(&chan->vc);\n}\n\nstatic void pt_synchronize(struct dma_chan *dma_chan)\n{\n\tstruct pt_dma_chan *chan = to_pt_chan(dma_chan);\n\n\tvchan_synchronize(&chan->vc);\n}\n\nstatic void pt_do_cleanup(struct virt_dma_desc *vd)\n{\n\tstruct pt_dma_desc *desc = to_pt_desc(vd);\n\tstruct pt_device *pt = desc->pt;\n\n\tkmem_cache_free(pt->dma_desc_cache, desc);\n}\n\nstatic int pt_dma_start_desc(struct pt_dma_desc *desc)\n{\n\tstruct pt_passthru_engine *pt_engine;\n\tstruct pt_device *pt;\n\tstruct pt_cmd *pt_cmd;\n\tstruct pt_cmd_queue *cmd_q;\n\n\tdesc->issued_to_hw = 1;\n\n\tpt_cmd = &desc->pt_cmd;\n\tpt = pt_cmd->pt;\n\tcmd_q = &pt->cmd_q;\n\tpt_engine = &pt_cmd->passthru;\n\n\tpt->tdata.cmd = pt_cmd;\n\n\t \n\tpt_cmd->ret = pt_core_perform_passthru(cmd_q, pt_engine);\n\n\treturn 0;\n}\n\nstatic struct pt_dma_desc *pt_next_dma_desc(struct pt_dma_chan *chan)\n{\n\t \n\tstruct virt_dma_desc *vd = vchan_next_desc(&chan->vc);\n\n\treturn vd ? to_pt_desc(vd) : NULL;\n}\n\nstatic struct pt_dma_desc *pt_handle_active_desc(struct pt_dma_chan *chan,\n\t\t\t\t\t\t struct pt_dma_desc *desc)\n{\n\tstruct dma_async_tx_descriptor *tx_desc;\n\tstruct virt_dma_desc *vd;\n\tunsigned long flags;\n\n\t \n\tdo {\n\t\tif (desc) {\n\t\t\tif (!desc->issued_to_hw) {\n\t\t\t\t \n\t\t\t\tif (desc->status != DMA_ERROR)\n\t\t\t\t\treturn desc;\n\t\t\t}\n\n\t\t\ttx_desc = &desc->vd.tx;\n\t\t\tvd = &desc->vd;\n\t\t} else {\n\t\t\ttx_desc = NULL;\n\t\t}\n\n\t\tspin_lock_irqsave(&chan->vc.lock, flags);\n\n\t\tif (desc) {\n\t\t\tif (desc->status != DMA_COMPLETE) {\n\t\t\t\tif (desc->status != DMA_ERROR)\n\t\t\t\t\tdesc->status = DMA_COMPLETE;\n\n\t\t\t\tdma_cookie_complete(tx_desc);\n\t\t\t\tdma_descriptor_unmap(tx_desc);\n\t\t\t\tlist_del(&desc->vd.node);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\ttx_desc = NULL;\n\t\t\t}\n\t\t}\n\n\t\tdesc = pt_next_dma_desc(chan);\n\n\t\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\n\t\tif (tx_desc) {\n\t\t\tdmaengine_desc_get_callback_invoke(tx_desc, NULL);\n\t\t\tdma_run_dependencies(tx_desc);\n\t\t\tvchan_vdesc_fini(vd);\n\t\t}\n\t} while (desc);\n\n\treturn NULL;\n}\n\nstatic void pt_cmd_callback(void *data, int err)\n{\n\tstruct pt_dma_desc *desc = data;\n\tstruct dma_chan *dma_chan;\n\tstruct pt_dma_chan *chan;\n\tint ret;\n\n\tif (err == -EINPROGRESS)\n\t\treturn;\n\n\tdma_chan = desc->vd.tx.chan;\n\tchan = to_pt_chan(dma_chan);\n\n\tif (err)\n\t\tdesc->status = DMA_ERROR;\n\n\twhile (true) {\n\t\t \n\t\tdesc = pt_handle_active_desc(chan, desc);\n\n\t\t \n\t\tif (!desc)\n\t\t\tbreak;\n\n\t\tret = pt_dma_start_desc(desc);\n\t\tif (!ret)\n\t\t\tbreak;\n\n\t\tdesc->status = DMA_ERROR;\n\t}\n}\n\nstatic struct pt_dma_desc *pt_alloc_dma_desc(struct pt_dma_chan *chan,\n\t\t\t\t\t     unsigned long flags)\n{\n\tstruct pt_dma_desc *desc;\n\n\tdesc = kmem_cache_zalloc(chan->pt->dma_desc_cache, GFP_NOWAIT);\n\tif (!desc)\n\t\treturn NULL;\n\n\tvchan_tx_prep(&chan->vc, &desc->vd, flags);\n\n\tdesc->pt = chan->pt;\n\tdesc->pt->cmd_q.int_en = !!(flags & DMA_PREP_INTERRUPT);\n\tdesc->issued_to_hw = 0;\n\tdesc->status = DMA_IN_PROGRESS;\n\n\treturn desc;\n}\n\nstatic struct pt_dma_desc *pt_create_desc(struct dma_chan *dma_chan,\n\t\t\t\t\t  dma_addr_t dst,\n\t\t\t\t\t  dma_addr_t src,\n\t\t\t\t\t  unsigned int len,\n\t\t\t\t\t  unsigned long flags)\n{\n\tstruct pt_dma_chan *chan = to_pt_chan(dma_chan);\n\tstruct pt_passthru_engine *pt_engine;\n\tstruct pt_dma_desc *desc;\n\tstruct pt_cmd *pt_cmd;\n\n\tdesc = pt_alloc_dma_desc(chan, flags);\n\tif (!desc)\n\t\treturn NULL;\n\n\tpt_cmd = &desc->pt_cmd;\n\tpt_cmd->pt = chan->pt;\n\tpt_engine = &pt_cmd->passthru;\n\tpt_cmd->engine = PT_ENGINE_PASSTHRU;\n\tpt_engine->src_dma = src;\n\tpt_engine->dst_dma = dst;\n\tpt_engine->src_len = len;\n\tpt_cmd->pt_cmd_callback = pt_cmd_callback;\n\tpt_cmd->data = desc;\n\n\tdesc->len = len;\n\n\treturn desc;\n}\n\nstatic struct dma_async_tx_descriptor *\npt_prep_dma_memcpy(struct dma_chan *dma_chan, dma_addr_t dst,\n\t\t   dma_addr_t src, size_t len, unsigned long flags)\n{\n\tstruct pt_dma_desc *desc;\n\n\tdesc = pt_create_desc(dma_chan, dst, src, len, flags);\n\tif (!desc)\n\t\treturn NULL;\n\n\treturn &desc->vd.tx;\n}\n\nstatic struct dma_async_tx_descriptor *\npt_prep_dma_interrupt(struct dma_chan *dma_chan, unsigned long flags)\n{\n\tstruct pt_dma_chan *chan = to_pt_chan(dma_chan);\n\tstruct pt_dma_desc *desc;\n\n\tdesc = pt_alloc_dma_desc(chan, flags);\n\tif (!desc)\n\t\treturn NULL;\n\n\treturn &desc->vd.tx;\n}\n\nstatic void pt_issue_pending(struct dma_chan *dma_chan)\n{\n\tstruct pt_dma_chan *chan = to_pt_chan(dma_chan);\n\tstruct pt_dma_desc *desc;\n\tunsigned long flags;\n\tbool engine_is_idle = true;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\n\tdesc = pt_next_dma_desc(chan);\n\tif (desc)\n\t\tengine_is_idle = false;\n\n\tvchan_issue_pending(&chan->vc);\n\n\tdesc = pt_next_dma_desc(chan);\n\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\n\t \n\tif (engine_is_idle && desc)\n\t\tpt_cmd_callback(desc, 0);\n}\n\nstatic enum dma_status\npt_tx_status(struct dma_chan *c, dma_cookie_t cookie,\n\t\tstruct dma_tx_state *txstate)\n{\n\tstruct pt_device *pt = to_pt_chan(c)->pt;\n\tstruct pt_cmd_queue *cmd_q = &pt->cmd_q;\n\n\tpt_check_status_trans(pt, cmd_q);\n\treturn dma_cookie_status(c, cookie, txstate);\n}\n\nstatic int pt_pause(struct dma_chan *dma_chan)\n{\n\tstruct pt_dma_chan *chan = to_pt_chan(dma_chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\tpt_stop_queue(&chan->pt->cmd_q);\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\n\treturn 0;\n}\n\nstatic int pt_resume(struct dma_chan *dma_chan)\n{\n\tstruct pt_dma_chan *chan = to_pt_chan(dma_chan);\n\tstruct pt_dma_desc *desc = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\tpt_start_queue(&chan->pt->cmd_q);\n\tdesc = pt_next_dma_desc(chan);\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\n\t \n\tif (desc)\n\t\tpt_cmd_callback(desc, 0);\n\n\treturn 0;\n}\n\nstatic int pt_terminate_all(struct dma_chan *dma_chan)\n{\n\tstruct pt_dma_chan *chan = to_pt_chan(dma_chan);\n\tunsigned long flags;\n\tstruct pt_cmd_queue *cmd_q = &chan->pt->cmd_q;\n\tLIST_HEAD(head);\n\n\tiowrite32(SUPPORTED_INTERRUPTS, cmd_q->reg_control + 0x0010);\n\tspin_lock_irqsave(&chan->vc.lock, flags);\n\tvchan_get_all_descriptors(&chan->vc, &head);\n\tspin_unlock_irqrestore(&chan->vc.lock, flags);\n\n\tvchan_dma_desc_free_list(&chan->vc, &head);\n\tvchan_free_chan_resources(&chan->vc);\n\n\treturn 0;\n}\n\nint pt_dmaengine_register(struct pt_device *pt)\n{\n\tstruct pt_dma_chan *chan;\n\tstruct dma_device *dma_dev = &pt->dma_dev;\n\tchar *cmd_cache_name;\n\tchar *desc_cache_name;\n\tint ret;\n\n\tpt->pt_dma_chan = devm_kzalloc(pt->dev, sizeof(*pt->pt_dma_chan),\n\t\t\t\t       GFP_KERNEL);\n\tif (!pt->pt_dma_chan)\n\t\treturn -ENOMEM;\n\n\tcmd_cache_name = devm_kasprintf(pt->dev, GFP_KERNEL,\n\t\t\t\t\t\"%s-dmaengine-cmd-cache\",\n\t\t\t\t\tdev_name(pt->dev));\n\tif (!cmd_cache_name)\n\t\treturn -ENOMEM;\n\n\tdesc_cache_name = devm_kasprintf(pt->dev, GFP_KERNEL,\n\t\t\t\t\t \"%s-dmaengine-desc-cache\",\n\t\t\t\t\t dev_name(pt->dev));\n\tif (!desc_cache_name) {\n\t\tret = -ENOMEM;\n\t\tgoto err_cache;\n\t}\n\n\tpt->dma_desc_cache = kmem_cache_create(desc_cache_name,\n\t\t\t\t\t       sizeof(struct pt_dma_desc), 0,\n\t\t\t\t\t       SLAB_HWCACHE_ALIGN, NULL);\n\tif (!pt->dma_desc_cache) {\n\t\tret = -ENOMEM;\n\t\tgoto err_cache;\n\t}\n\n\tdma_dev->dev = pt->dev;\n\tdma_dev->src_addr_widths = DMA_SLAVE_BUSWIDTH_64_BYTES;\n\tdma_dev->dst_addr_widths = DMA_SLAVE_BUSWIDTH_64_BYTES;\n\tdma_dev->directions = DMA_MEM_TO_MEM;\n\tdma_dev->residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\tdma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);\n\tdma_cap_set(DMA_INTERRUPT, dma_dev->cap_mask);\n\n\t \n\tdma_cap_set(DMA_PRIVATE, dma_dev->cap_mask);\n\n\tINIT_LIST_HEAD(&dma_dev->channels);\n\n\tchan = pt->pt_dma_chan;\n\tchan->pt = pt;\n\n\t \n\tdma_dev->device_free_chan_resources = pt_free_chan_resources;\n\tdma_dev->device_prep_dma_memcpy = pt_prep_dma_memcpy;\n\tdma_dev->device_prep_dma_interrupt = pt_prep_dma_interrupt;\n\tdma_dev->device_issue_pending = pt_issue_pending;\n\tdma_dev->device_tx_status = pt_tx_status;\n\tdma_dev->device_pause = pt_pause;\n\tdma_dev->device_resume = pt_resume;\n\tdma_dev->device_terminate_all = pt_terminate_all;\n\tdma_dev->device_synchronize = pt_synchronize;\n\n\tchan->vc.desc_free = pt_do_cleanup;\n\tvchan_init(&chan->vc, dma_dev);\n\n\tdma_set_mask_and_coherent(pt->dev, DMA_BIT_MASK(64));\n\n\tret = dma_async_device_register(dma_dev);\n\tif (ret)\n\t\tgoto err_reg;\n\n\treturn 0;\n\nerr_reg:\n\tkmem_cache_destroy(pt->dma_desc_cache);\n\nerr_cache:\n\tkmem_cache_destroy(pt->dma_cmd_cache);\n\n\treturn ret;\n}\n\nvoid pt_dmaengine_unregister(struct pt_device *pt)\n{\n\tstruct dma_device *dma_dev = &pt->dma_dev;\n\n\tdma_async_device_unregister(dma_dev);\n\n\tkmem_cache_destroy(pt->dma_desc_cache);\n\tkmem_cache_destroy(pt->dma_cmd_cache);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}