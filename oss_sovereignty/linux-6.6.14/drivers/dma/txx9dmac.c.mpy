{
  "module_name": "txx9dmac.c",
  "hash_id": "6ecc5d4a7a2aa4b3823efbfd8f0b315f70029f1cbfa7bdccec32531d8959e759",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/txx9dmac.c",
  "human_readable_source": "\n \n#include <linux/dma-mapping.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/scatterlist.h>\n\n#include \"dmaengine.h\"\n#include \"txx9dmac.h\"\n\nstatic struct txx9dmac_chan *to_txx9dmac_chan(struct dma_chan *chan)\n{\n\treturn container_of(chan, struct txx9dmac_chan, chan);\n}\n\nstatic struct txx9dmac_cregs __iomem *__dma_regs(const struct txx9dmac_chan *dc)\n{\n\treturn dc->ch_regs;\n}\n\nstatic struct txx9dmac_cregs32 __iomem *__dma_regs32(\n\tconst struct txx9dmac_chan *dc)\n{\n\treturn dc->ch_regs;\n}\n\n#define channel64_readq(dc, name) \\\n\t__raw_readq(&(__dma_regs(dc)->name))\n#define channel64_writeq(dc, name, val) \\\n\t__raw_writeq((val), &(__dma_regs(dc)->name))\n#define channel64_readl(dc, name) \\\n\t__raw_readl(&(__dma_regs(dc)->name))\n#define channel64_writel(dc, name, val) \\\n\t__raw_writel((val), &(__dma_regs(dc)->name))\n\n#define channel32_readl(dc, name) \\\n\t__raw_readl(&(__dma_regs32(dc)->name))\n#define channel32_writel(dc, name, val) \\\n\t__raw_writel((val), &(__dma_regs32(dc)->name))\n\n#define channel_readq(dc, name) channel64_readq(dc, name)\n#define channel_writeq(dc, name, val) channel64_writeq(dc, name, val)\n#define channel_readl(dc, name) \\\n\t(is_dmac64(dc) ? \\\n\t channel64_readl(dc, name) : channel32_readl(dc, name))\n#define channel_writel(dc, name, val) \\\n\t(is_dmac64(dc) ? \\\n\t channel64_writel(dc, name, val) : channel32_writel(dc, name, val))\n\nstatic dma_addr_t channel64_read_CHAR(const struct txx9dmac_chan *dc)\n{\n\tif (sizeof(__dma_regs(dc)->CHAR) == sizeof(u64))\n\t\treturn channel64_readq(dc, CHAR);\n\telse\n\t\treturn channel64_readl(dc, CHAR);\n}\n\nstatic void channel64_write_CHAR(const struct txx9dmac_chan *dc, dma_addr_t val)\n{\n\tif (sizeof(__dma_regs(dc)->CHAR) == sizeof(u64))\n\t\tchannel64_writeq(dc, CHAR, val);\n\telse\n\t\tchannel64_writel(dc, CHAR, val);\n}\n\nstatic void channel64_clear_CHAR(const struct txx9dmac_chan *dc)\n{\n#if defined(CONFIG_32BIT) && !defined(CONFIG_PHYS_ADDR_T_64BIT)\n\tchannel64_writel(dc, CHAR, 0);\n\tchannel64_writel(dc, __pad_CHAR, 0);\n#else\n\tchannel64_writeq(dc, CHAR, 0);\n#endif\n}\n\nstatic dma_addr_t channel_read_CHAR(const struct txx9dmac_chan *dc)\n{\n\tif (is_dmac64(dc))\n\t\treturn channel64_read_CHAR(dc);\n\telse\n\t\treturn channel32_readl(dc, CHAR);\n}\n\nstatic void channel_write_CHAR(const struct txx9dmac_chan *dc, dma_addr_t val)\n{\n\tif (is_dmac64(dc))\n\t\tchannel64_write_CHAR(dc, val);\n\telse\n\t\tchannel32_writel(dc, CHAR, val);\n}\n\nstatic struct txx9dmac_regs __iomem *__txx9dmac_regs(\n\tconst struct txx9dmac_dev *ddev)\n{\n\treturn ddev->regs;\n}\n\nstatic struct txx9dmac_regs32 __iomem *__txx9dmac_regs32(\n\tconst struct txx9dmac_dev *ddev)\n{\n\treturn ddev->regs;\n}\n\n#define dma64_readl(ddev, name) \\\n\t__raw_readl(&(__txx9dmac_regs(ddev)->name))\n#define dma64_writel(ddev, name, val) \\\n\t__raw_writel((val), &(__txx9dmac_regs(ddev)->name))\n\n#define dma32_readl(ddev, name) \\\n\t__raw_readl(&(__txx9dmac_regs32(ddev)->name))\n#define dma32_writel(ddev, name, val) \\\n\t__raw_writel((val), &(__txx9dmac_regs32(ddev)->name))\n\n#define dma_readl(ddev, name) \\\n\t(__is_dmac64(ddev) ? \\\n\tdma64_readl(ddev, name) : dma32_readl(ddev, name))\n#define dma_writel(ddev, name, val) \\\n\t(__is_dmac64(ddev) ? \\\n\tdma64_writel(ddev, name, val) : dma32_writel(ddev, name, val))\n\nstatic struct device *chan2dev(struct dma_chan *chan)\n{\n\treturn &chan->dev->device;\n}\nstatic struct device *chan2parent(struct dma_chan *chan)\n{\n\treturn chan->dev->device.parent;\n}\n\nstatic struct txx9dmac_desc *\ntxd_to_txx9dmac_desc(struct dma_async_tx_descriptor *txd)\n{\n\treturn container_of(txd, struct txx9dmac_desc, txd);\n}\n\nstatic dma_addr_t desc_read_CHAR(const struct txx9dmac_chan *dc,\n\t\t\t\t const struct txx9dmac_desc *desc)\n{\n\treturn is_dmac64(dc) ? desc->hwdesc.CHAR : desc->hwdesc32.CHAR;\n}\n\nstatic void desc_write_CHAR(const struct txx9dmac_chan *dc,\n\t\t\t    struct txx9dmac_desc *desc, dma_addr_t val)\n{\n\tif (is_dmac64(dc))\n\t\tdesc->hwdesc.CHAR = val;\n\telse\n\t\tdesc->hwdesc32.CHAR = val;\n}\n\n#define TXX9_DMA_MAX_COUNT\t0x04000000\n\n#define TXX9_DMA_INITIAL_DESC_COUNT\t64\n\nstatic struct txx9dmac_desc *txx9dmac_first_active(struct txx9dmac_chan *dc)\n{\n\treturn list_entry(dc->active_list.next,\n\t\t\t  struct txx9dmac_desc, desc_node);\n}\n\nstatic struct txx9dmac_desc *txx9dmac_last_active(struct txx9dmac_chan *dc)\n{\n\treturn list_entry(dc->active_list.prev,\n\t\t\t  struct txx9dmac_desc, desc_node);\n}\n\nstatic struct txx9dmac_desc *txx9dmac_first_queued(struct txx9dmac_chan *dc)\n{\n\treturn list_entry(dc->queue.next, struct txx9dmac_desc, desc_node);\n}\n\nstatic struct txx9dmac_desc *txx9dmac_last_child(struct txx9dmac_desc *desc)\n{\n\tif (!list_empty(&desc->tx_list))\n\t\tdesc = list_entry(desc->tx_list.prev, typeof(*desc), desc_node);\n\treturn desc;\n}\n\nstatic dma_cookie_t txx9dmac_tx_submit(struct dma_async_tx_descriptor *tx);\n\nstatic struct txx9dmac_desc *txx9dmac_desc_alloc(struct txx9dmac_chan *dc,\n\t\t\t\t\t\t gfp_t flags)\n{\n\tstruct txx9dmac_dev *ddev = dc->ddev;\n\tstruct txx9dmac_desc *desc;\n\n\tdesc = kzalloc(sizeof(*desc), flags);\n\tif (!desc)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&desc->tx_list);\n\tdma_async_tx_descriptor_init(&desc->txd, &dc->chan);\n\tdesc->txd.tx_submit = txx9dmac_tx_submit;\n\t \n\tdesc->txd.flags = DMA_CTRL_ACK;\n\tdesc->txd.phys = dma_map_single(chan2parent(&dc->chan), &desc->hwdesc,\n\t\t\t\t\tddev->descsize, DMA_TO_DEVICE);\n\treturn desc;\n}\n\nstatic struct txx9dmac_desc *txx9dmac_desc_get(struct txx9dmac_chan *dc)\n{\n\tstruct txx9dmac_desc *desc, *_desc;\n\tstruct txx9dmac_desc *ret = NULL;\n\tunsigned int i = 0;\n\n\tspin_lock_bh(&dc->lock);\n\tlist_for_each_entry_safe(desc, _desc, &dc->free_list, desc_node) {\n\t\tif (async_tx_test_ack(&desc->txd)) {\n\t\t\tlist_del(&desc->desc_node);\n\t\t\tret = desc;\n\t\t\tbreak;\n\t\t}\n\t\tdev_dbg(chan2dev(&dc->chan), \"desc %p not ACKed\\n\", desc);\n\t\ti++;\n\t}\n\tspin_unlock_bh(&dc->lock);\n\n\tdev_vdbg(chan2dev(&dc->chan), \"scanned %u descriptors on freelist\\n\",\n\t\t i);\n\tif (!ret) {\n\t\tret = txx9dmac_desc_alloc(dc, GFP_ATOMIC);\n\t\tif (ret) {\n\t\t\tspin_lock_bh(&dc->lock);\n\t\t\tdc->descs_allocated++;\n\t\t\tspin_unlock_bh(&dc->lock);\n\t\t} else\n\t\t\tdev_err(chan2dev(&dc->chan),\n\t\t\t\t\"not enough descriptors available\\n\");\n\t}\n\treturn ret;\n}\n\nstatic void txx9dmac_sync_desc_for_cpu(struct txx9dmac_chan *dc,\n\t\t\t\t       struct txx9dmac_desc *desc)\n{\n\tstruct txx9dmac_dev *ddev = dc->ddev;\n\tstruct txx9dmac_desc *child;\n\n\tlist_for_each_entry(child, &desc->tx_list, desc_node)\n\t\tdma_sync_single_for_cpu(chan2parent(&dc->chan),\n\t\t\t\tchild->txd.phys, ddev->descsize,\n\t\t\t\tDMA_TO_DEVICE);\n\tdma_sync_single_for_cpu(chan2parent(&dc->chan),\n\t\t\tdesc->txd.phys, ddev->descsize,\n\t\t\tDMA_TO_DEVICE);\n}\n\n \nstatic void txx9dmac_desc_put(struct txx9dmac_chan *dc,\n\t\t\t      struct txx9dmac_desc *desc)\n{\n\tif (desc) {\n\t\tstruct txx9dmac_desc *child;\n\n\t\ttxx9dmac_sync_desc_for_cpu(dc, desc);\n\n\t\tspin_lock_bh(&dc->lock);\n\t\tlist_for_each_entry(child, &desc->tx_list, desc_node)\n\t\t\tdev_vdbg(chan2dev(&dc->chan),\n\t\t\t\t \"moving child desc %p to freelist\\n\",\n\t\t\t\t child);\n\t\tlist_splice_init(&desc->tx_list, &dc->free_list);\n\t\tdev_vdbg(chan2dev(&dc->chan), \"moving desc %p to freelist\\n\",\n\t\t\t desc);\n\t\tlist_add(&desc->desc_node, &dc->free_list);\n\t\tspin_unlock_bh(&dc->lock);\n\t}\n}\n\n \n\nstatic void txx9dmac_dump_regs(struct txx9dmac_chan *dc)\n{\n\tif (is_dmac64(dc))\n\t\tdev_err(chan2dev(&dc->chan),\n\t\t\t\"  CHAR: %#llx SAR: %#llx DAR: %#llx CNTR: %#x\"\n\t\t\t\" SAIR: %#x DAIR: %#x CCR: %#x CSR: %#x\\n\",\n\t\t\t(u64)channel64_read_CHAR(dc),\n\t\t\tchannel64_readq(dc, SAR),\n\t\t\tchannel64_readq(dc, DAR),\n\t\t\tchannel64_readl(dc, CNTR),\n\t\t\tchannel64_readl(dc, SAIR),\n\t\t\tchannel64_readl(dc, DAIR),\n\t\t\tchannel64_readl(dc, CCR),\n\t\t\tchannel64_readl(dc, CSR));\n\telse\n\t\tdev_err(chan2dev(&dc->chan),\n\t\t\t\"  CHAR: %#x SAR: %#x DAR: %#x CNTR: %#x\"\n\t\t\t\" SAIR: %#x DAIR: %#x CCR: %#x CSR: %#x\\n\",\n\t\t\tchannel32_readl(dc, CHAR),\n\t\t\tchannel32_readl(dc, SAR),\n\t\t\tchannel32_readl(dc, DAR),\n\t\t\tchannel32_readl(dc, CNTR),\n\t\t\tchannel32_readl(dc, SAIR),\n\t\t\tchannel32_readl(dc, DAIR),\n\t\t\tchannel32_readl(dc, CCR),\n\t\t\tchannel32_readl(dc, CSR));\n}\n\nstatic void txx9dmac_reset_chan(struct txx9dmac_chan *dc)\n{\n\tchannel_writel(dc, CCR, TXX9_DMA_CCR_CHRST);\n\tif (is_dmac64(dc)) {\n\t\tchannel64_clear_CHAR(dc);\n\t\tchannel_writeq(dc, SAR, 0);\n\t\tchannel_writeq(dc, DAR, 0);\n\t} else {\n\t\tchannel_writel(dc, CHAR, 0);\n\t\tchannel_writel(dc, SAR, 0);\n\t\tchannel_writel(dc, DAR, 0);\n\t}\n\tchannel_writel(dc, CNTR, 0);\n\tchannel_writel(dc, SAIR, 0);\n\tchannel_writel(dc, DAIR, 0);\n\tchannel_writel(dc, CCR, 0);\n}\n\n \nstatic void txx9dmac_dostart(struct txx9dmac_chan *dc,\n\t\t\t     struct txx9dmac_desc *first)\n{\n\tstruct txx9dmac_slave *ds = dc->chan.private;\n\tu32 sai, dai;\n\n\tdev_vdbg(chan2dev(&dc->chan), \"dostart %u %p\\n\",\n\t\t first->txd.cookie, first);\n\t \n\tif (channel_readl(dc, CSR) & TXX9_DMA_CSR_XFACT) {\n\t\tdev_err(chan2dev(&dc->chan),\n\t\t\t\"BUG: Attempted to start non-idle channel\\n\");\n\t\ttxx9dmac_dump_regs(dc);\n\t\t \n\t\treturn;\n\t}\n\n\tif (is_dmac64(dc)) {\n\t\tchannel64_writel(dc, CNTR, 0);\n\t\tchannel64_writel(dc, CSR, 0xffffffff);\n\t\tif (ds) {\n\t\t\tif (ds->tx_reg) {\n\t\t\t\tsai = ds->reg_width;\n\t\t\t\tdai = 0;\n\t\t\t} else {\n\t\t\t\tsai = 0;\n\t\t\t\tdai = ds->reg_width;\n\t\t\t}\n\t\t} else {\n\t\t\tsai = 8;\n\t\t\tdai = 8;\n\t\t}\n\t\tchannel64_writel(dc, SAIR, sai);\n\t\tchannel64_writel(dc, DAIR, dai);\n\t\t \n\t\tchannel64_writel(dc, CCR, dc->ccr);\n\t\t \n\t\tchannel64_write_CHAR(dc, first->txd.phys);\n\t} else {\n\t\tchannel32_writel(dc, CNTR, 0);\n\t\tchannel32_writel(dc, CSR, 0xffffffff);\n\t\tif (ds) {\n\t\t\tif (ds->tx_reg) {\n\t\t\t\tsai = ds->reg_width;\n\t\t\t\tdai = 0;\n\t\t\t} else {\n\t\t\t\tsai = 0;\n\t\t\t\tdai = ds->reg_width;\n\t\t\t}\n\t\t} else {\n\t\t\tsai = 4;\n\t\t\tdai = 4;\n\t\t}\n\t\tchannel32_writel(dc, SAIR, sai);\n\t\tchannel32_writel(dc, DAIR, dai);\n\t\tif (txx9_dma_have_SMPCHN()) {\n\t\t\tchannel32_writel(dc, CCR, dc->ccr);\n\t\t\t \n\t\t\tchannel32_writel(dc, CHAR, first->txd.phys);\n\t\t} else {\n\t\t\tchannel32_writel(dc, CHAR, first->txd.phys);\n\t\t\tchannel32_writel(dc, CCR, dc->ccr);\n\t\t}\n\t}\n}\n\n \n\nstatic void\ntxx9dmac_descriptor_complete(struct txx9dmac_chan *dc,\n\t\t\t     struct txx9dmac_desc *desc)\n{\n\tstruct dmaengine_desc_callback cb;\n\tstruct dma_async_tx_descriptor *txd = &desc->txd;\n\n\tdev_vdbg(chan2dev(&dc->chan), \"descriptor %u %p complete\\n\",\n\t\t txd->cookie, desc);\n\n\tdma_cookie_complete(txd);\n\tdmaengine_desc_get_callback(txd, &cb);\n\n\ttxx9dmac_sync_desc_for_cpu(dc, desc);\n\tlist_splice_init(&desc->tx_list, &dc->free_list);\n\tlist_move(&desc->desc_node, &dc->free_list);\n\n\tdma_descriptor_unmap(txd);\n\t \n\tdmaengine_desc_callback_invoke(&cb, NULL);\n\tdma_run_dependencies(txd);\n}\n\nstatic void txx9dmac_dequeue(struct txx9dmac_chan *dc, struct list_head *list)\n{\n\tstruct txx9dmac_dev *ddev = dc->ddev;\n\tstruct txx9dmac_desc *desc;\n\tstruct txx9dmac_desc *prev = NULL;\n\n\tBUG_ON(!list_empty(list));\n\tdo {\n\t\tdesc = txx9dmac_first_queued(dc);\n\t\tif (prev) {\n\t\t\tdesc_write_CHAR(dc, prev, desc->txd.phys);\n\t\t\tdma_sync_single_for_device(chan2parent(&dc->chan),\n\t\t\t\tprev->txd.phys, ddev->descsize,\n\t\t\t\tDMA_TO_DEVICE);\n\t\t}\n\t\tprev = txx9dmac_last_child(desc);\n\t\tlist_move_tail(&desc->desc_node, list);\n\t\t \n\t\tif ((desc->txd.flags & DMA_PREP_INTERRUPT) &&\n\t\t    !txx9dmac_chan_INTENT(dc))\n\t\t\tbreak;\n\t} while (!list_empty(&dc->queue));\n}\n\nstatic void txx9dmac_complete_all(struct txx9dmac_chan *dc)\n{\n\tstruct txx9dmac_desc *desc, *_desc;\n\tLIST_HEAD(list);\n\n\t \n\tlist_splice_init(&dc->active_list, &list);\n\tif (!list_empty(&dc->queue)) {\n\t\ttxx9dmac_dequeue(dc, &dc->active_list);\n\t\ttxx9dmac_dostart(dc, txx9dmac_first_active(dc));\n\t}\n\n\tlist_for_each_entry_safe(desc, _desc, &list, desc_node)\n\t\ttxx9dmac_descriptor_complete(dc, desc);\n}\n\nstatic void txx9dmac_dump_desc(struct txx9dmac_chan *dc,\n\t\t\t       struct txx9dmac_hwdesc *desc)\n{\n\tif (is_dmac64(dc)) {\n#ifdef TXX9_DMA_USE_SIMPLE_CHAIN\n\t\tdev_crit(chan2dev(&dc->chan),\n\t\t\t \"  desc: ch%#llx s%#llx d%#llx c%#x\\n\",\n\t\t\t (u64)desc->CHAR, desc->SAR, desc->DAR, desc->CNTR);\n#else\n\t\tdev_crit(chan2dev(&dc->chan),\n\t\t\t \"  desc: ch%#llx s%#llx d%#llx c%#x\"\n\t\t\t \" si%#x di%#x cc%#x cs%#x\\n\",\n\t\t\t (u64)desc->CHAR, desc->SAR, desc->DAR, desc->CNTR,\n\t\t\t desc->SAIR, desc->DAIR, desc->CCR, desc->CSR);\n#endif\n\t} else {\n\t\tstruct txx9dmac_hwdesc32 *d = (struct txx9dmac_hwdesc32 *)desc;\n#ifdef TXX9_DMA_USE_SIMPLE_CHAIN\n\t\tdev_crit(chan2dev(&dc->chan),\n\t\t\t \"  desc: ch%#x s%#x d%#x c%#x\\n\",\n\t\t\t d->CHAR, d->SAR, d->DAR, d->CNTR);\n#else\n\t\tdev_crit(chan2dev(&dc->chan),\n\t\t\t \"  desc: ch%#x s%#x d%#x c%#x\"\n\t\t\t \" si%#x di%#x cc%#x cs%#x\\n\",\n\t\t\t d->CHAR, d->SAR, d->DAR, d->CNTR,\n\t\t\t d->SAIR, d->DAIR, d->CCR, d->CSR);\n#endif\n\t}\n}\n\nstatic void txx9dmac_handle_error(struct txx9dmac_chan *dc, u32 csr)\n{\n\tstruct txx9dmac_desc *bad_desc;\n\tstruct txx9dmac_desc *child;\n\tu32 errors;\n\n\t \n\tdev_crit(chan2dev(&dc->chan), \"Abnormal Chain Completion\\n\");\n\ttxx9dmac_dump_regs(dc);\n\n\tbad_desc = txx9dmac_first_active(dc);\n\tlist_del_init(&bad_desc->desc_node);\n\n\t \n\terrors = csr & (TXX9_DMA_CSR_ABCHC |\n\t\t\tTXX9_DMA_CSR_CFERR | TXX9_DMA_CSR_CHERR |\n\t\t\tTXX9_DMA_CSR_DESERR | TXX9_DMA_CSR_SORERR);\n\tchannel_writel(dc, CSR, errors);\n\n\tif (list_empty(&dc->active_list) && !list_empty(&dc->queue))\n\t\ttxx9dmac_dequeue(dc, &dc->active_list);\n\tif (!list_empty(&dc->active_list))\n\t\ttxx9dmac_dostart(dc, txx9dmac_first_active(dc));\n\n\tdev_crit(chan2dev(&dc->chan),\n\t\t \"Bad descriptor submitted for DMA! (cookie: %d)\\n\",\n\t\t bad_desc->txd.cookie);\n\ttxx9dmac_dump_desc(dc, &bad_desc->hwdesc);\n\tlist_for_each_entry(child, &bad_desc->tx_list, desc_node)\n\t\ttxx9dmac_dump_desc(dc, &child->hwdesc);\n\t \n\ttxx9dmac_descriptor_complete(dc, bad_desc);\n}\n\nstatic void txx9dmac_scan_descriptors(struct txx9dmac_chan *dc)\n{\n\tdma_addr_t chain;\n\tstruct txx9dmac_desc *desc, *_desc;\n\tstruct txx9dmac_desc *child;\n\tu32 csr;\n\n\tif (is_dmac64(dc)) {\n\t\tchain = channel64_read_CHAR(dc);\n\t\tcsr = channel64_readl(dc, CSR);\n\t\tchannel64_writel(dc, CSR, csr);\n\t} else {\n\t\tchain = channel32_readl(dc, CHAR);\n\t\tcsr = channel32_readl(dc, CSR);\n\t\tchannel32_writel(dc, CSR, csr);\n\t}\n\t \n\tif (!(csr & (TXX9_DMA_CSR_XFACT | TXX9_DMA_CSR_ABCHC))) {\n\t\t \n\t\ttxx9dmac_complete_all(dc);\n\t\treturn;\n\t}\n\tif (!(csr & TXX9_DMA_CSR_CHNEN))\n\t\tchain = 0;\t \n\n\tdev_vdbg(chan2dev(&dc->chan), \"scan_descriptors: char=%#llx\\n\",\n\t\t (u64)chain);\n\n\tlist_for_each_entry_safe(desc, _desc, &dc->active_list, desc_node) {\n\t\tif (desc_read_CHAR(dc, desc) == chain) {\n\t\t\t \n\t\t\tif (csr & TXX9_DMA_CSR_ABCHC)\n\t\t\t\tgoto scan_done;\n\t\t\treturn;\n\t\t}\n\n\t\tlist_for_each_entry(child, &desc->tx_list, desc_node)\n\t\t\tif (desc_read_CHAR(dc, child) == chain) {\n\t\t\t\t \n\t\t\t\tif (csr & TXX9_DMA_CSR_ABCHC)\n\t\t\t\t\tgoto scan_done;\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t \n\t\ttxx9dmac_descriptor_complete(dc, desc);\n\t}\nscan_done:\n\tif (csr & TXX9_DMA_CSR_ABCHC) {\n\t\ttxx9dmac_handle_error(dc, csr);\n\t\treturn;\n\t}\n\n\tdev_err(chan2dev(&dc->chan),\n\t\t\"BUG: All descriptors done, but channel not idle!\\n\");\n\n\t \n\ttxx9dmac_reset_chan(dc);\n\n\tif (!list_empty(&dc->queue)) {\n\t\ttxx9dmac_dequeue(dc, &dc->active_list);\n\t\ttxx9dmac_dostart(dc, txx9dmac_first_active(dc));\n\t}\n}\n\nstatic void txx9dmac_chan_tasklet(struct tasklet_struct *t)\n{\n\tint irq;\n\tu32 csr;\n\tstruct txx9dmac_chan *dc;\n\n\tdc = from_tasklet(dc, t, tasklet);\n\tcsr = channel_readl(dc, CSR);\n\tdev_vdbg(chan2dev(&dc->chan), \"tasklet: status=%x\\n\", csr);\n\n\tspin_lock(&dc->lock);\n\tif (csr & (TXX9_DMA_CSR_ABCHC | TXX9_DMA_CSR_NCHNC |\n\t\t   TXX9_DMA_CSR_NTRNFC))\n\t\ttxx9dmac_scan_descriptors(dc);\n\tspin_unlock(&dc->lock);\n\tirq = dc->irq;\n\n\tenable_irq(irq);\n}\n\nstatic irqreturn_t txx9dmac_chan_interrupt(int irq, void *dev_id)\n{\n\tstruct txx9dmac_chan *dc = dev_id;\n\n\tdev_vdbg(chan2dev(&dc->chan), \"interrupt: status=%#x\\n\",\n\t\t\tchannel_readl(dc, CSR));\n\n\ttasklet_schedule(&dc->tasklet);\n\t \n\tdisable_irq_nosync(irq);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void txx9dmac_tasklet(struct tasklet_struct *t)\n{\n\tint irq;\n\tu32 csr;\n\tstruct txx9dmac_chan *dc;\n\n\tstruct txx9dmac_dev *ddev = from_tasklet(ddev, t, tasklet);\n\tu32 mcr;\n\tint i;\n\n\tmcr = dma_readl(ddev, MCR);\n\tdev_vdbg(ddev->chan[0]->dma.dev, \"tasklet: mcr=%x\\n\", mcr);\n\tfor (i = 0; i < TXX9_DMA_MAX_NR_CHANNELS; i++) {\n\t\tif ((mcr >> (24 + i)) & 0x11) {\n\t\t\tdc = ddev->chan[i];\n\t\t\tcsr = channel_readl(dc, CSR);\n\t\t\tdev_vdbg(chan2dev(&dc->chan), \"tasklet: status=%x\\n\",\n\t\t\t\t csr);\n\t\t\tspin_lock(&dc->lock);\n\t\t\tif (csr & (TXX9_DMA_CSR_ABCHC | TXX9_DMA_CSR_NCHNC |\n\t\t\t\t   TXX9_DMA_CSR_NTRNFC))\n\t\t\t\ttxx9dmac_scan_descriptors(dc);\n\t\t\tspin_unlock(&dc->lock);\n\t\t}\n\t}\n\tirq = ddev->irq;\n\n\tenable_irq(irq);\n}\n\nstatic irqreturn_t txx9dmac_interrupt(int irq, void *dev_id)\n{\n\tstruct txx9dmac_dev *ddev = dev_id;\n\n\tdev_vdbg(ddev->chan[0]->dma.dev, \"interrupt: status=%#x\\n\",\n\t\t\tdma_readl(ddev, MCR));\n\n\ttasklet_schedule(&ddev->tasklet);\n\t \n\tdisable_irq_nosync(irq);\n\n\treturn IRQ_HANDLED;\n}\n\n \n\nstatic dma_cookie_t txx9dmac_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct txx9dmac_desc *desc = txd_to_txx9dmac_desc(tx);\n\tstruct txx9dmac_chan *dc = to_txx9dmac_chan(tx->chan);\n\tdma_cookie_t cookie;\n\n\tspin_lock_bh(&dc->lock);\n\tcookie = dma_cookie_assign(tx);\n\n\tdev_vdbg(chan2dev(tx->chan), \"tx_submit: queued %u %p\\n\",\n\t\t desc->txd.cookie, desc);\n\n\tlist_add_tail(&desc->desc_node, &dc->queue);\n\tspin_unlock_bh(&dc->lock);\n\n\treturn cookie;\n}\n\nstatic struct dma_async_tx_descriptor *\ntxx9dmac_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\n\t\tsize_t len, unsigned long flags)\n{\n\tstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\n\tstruct txx9dmac_dev *ddev = dc->ddev;\n\tstruct txx9dmac_desc *desc;\n\tstruct txx9dmac_desc *first;\n\tstruct txx9dmac_desc *prev;\n\tsize_t xfer_count;\n\tsize_t offset;\n\n\tdev_vdbg(chan2dev(chan), \"prep_dma_memcpy d%#llx s%#llx l%#zx f%#lx\\n\",\n\t\t (u64)dest, (u64)src, len, flags);\n\n\tif (unlikely(!len)) {\n\t\tdev_dbg(chan2dev(chan), \"prep_dma_memcpy: length is zero!\\n\");\n\t\treturn NULL;\n\t}\n\n\tprev = first = NULL;\n\n\tfor (offset = 0; offset < len; offset += xfer_count) {\n\t\txfer_count = min_t(size_t, len - offset, TXX9_DMA_MAX_COUNT);\n\t\t \n\t\tif (__is_dmac64(ddev)) {\n\t\t\tif (xfer_count > 0x100 &&\n\t\t\t    (xfer_count & 0xff) >= 0xfa &&\n\t\t\t    (xfer_count & 0xff) <= 0xff)\n\t\t\t\txfer_count -= 0x20;\n\t\t} else {\n\t\t\tif (xfer_count > 0x80 &&\n\t\t\t    (xfer_count & 0x7f) >= 0x7e &&\n\t\t\t    (xfer_count & 0x7f) <= 0x7f)\n\t\t\t\txfer_count -= 0x20;\n\t\t}\n\n\t\tdesc = txx9dmac_desc_get(dc);\n\t\tif (!desc) {\n\t\t\ttxx9dmac_desc_put(dc, first);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (__is_dmac64(ddev)) {\n\t\t\tdesc->hwdesc.SAR = src + offset;\n\t\t\tdesc->hwdesc.DAR = dest + offset;\n\t\t\tdesc->hwdesc.CNTR = xfer_count;\n\t\t\ttxx9dmac_desc_set_nosimple(ddev, desc, 8, 8,\n\t\t\t\t\tdc->ccr | TXX9_DMA_CCR_XFACT);\n\t\t} else {\n\t\t\tdesc->hwdesc32.SAR = src + offset;\n\t\t\tdesc->hwdesc32.DAR = dest + offset;\n\t\t\tdesc->hwdesc32.CNTR = xfer_count;\n\t\t\ttxx9dmac_desc_set_nosimple(ddev, desc, 4, 4,\n\t\t\t\t\tdc->ccr | TXX9_DMA_CCR_XFACT);\n\t\t}\n\n\t\t \n\t\tif (!first) {\n\t\t\tfirst = desc;\n\t\t} else {\n\t\t\tdesc_write_CHAR(dc, prev, desc->txd.phys);\n\t\t\tdma_sync_single_for_device(chan2parent(&dc->chan),\n\t\t\t\t\tprev->txd.phys, ddev->descsize,\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\tlist_add_tail(&desc->desc_node, &first->tx_list);\n\t\t}\n\t\tprev = desc;\n\t}\n\n\t \n\tif (flags & DMA_PREP_INTERRUPT)\n\t\ttxx9dmac_desc_set_INTENT(ddev, prev);\n\n\tdesc_write_CHAR(dc, prev, 0);\n\tdma_sync_single_for_device(chan2parent(&dc->chan),\n\t\t\tprev->txd.phys, ddev->descsize,\n\t\t\tDMA_TO_DEVICE);\n\n\tfirst->txd.flags = flags;\n\tfirst->len = len;\n\n\treturn &first->txd;\n}\n\nstatic struct dma_async_tx_descriptor *\ntxx9dmac_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\n\t\tunsigned int sg_len, enum dma_transfer_direction direction,\n\t\tunsigned long flags, void *context)\n{\n\tstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\n\tstruct txx9dmac_dev *ddev = dc->ddev;\n\tstruct txx9dmac_slave *ds = chan->private;\n\tstruct txx9dmac_desc *prev;\n\tstruct txx9dmac_desc *first;\n\tunsigned int i;\n\tstruct scatterlist *sg;\n\n\tdev_vdbg(chan2dev(chan), \"prep_dma_slave\\n\");\n\n\tBUG_ON(!ds || !ds->reg_width);\n\tif (ds->tx_reg)\n\t\tBUG_ON(direction != DMA_MEM_TO_DEV);\n\telse\n\t\tBUG_ON(direction != DMA_DEV_TO_MEM);\n\tif (unlikely(!sg_len))\n\t\treturn NULL;\n\n\tprev = first = NULL;\n\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tstruct txx9dmac_desc *desc;\n\t\tdma_addr_t mem;\n\t\tu32 sai, dai;\n\n\t\tdesc = txx9dmac_desc_get(dc);\n\t\tif (!desc) {\n\t\t\ttxx9dmac_desc_put(dc, first);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tmem = sg_dma_address(sg);\n\n\t\tif (__is_dmac64(ddev)) {\n\t\t\tif (direction == DMA_MEM_TO_DEV) {\n\t\t\t\tdesc->hwdesc.SAR = mem;\n\t\t\t\tdesc->hwdesc.DAR = ds->tx_reg;\n\t\t\t} else {\n\t\t\t\tdesc->hwdesc.SAR = ds->rx_reg;\n\t\t\t\tdesc->hwdesc.DAR = mem;\n\t\t\t}\n\t\t\tdesc->hwdesc.CNTR = sg_dma_len(sg);\n\t\t} else {\n\t\t\tif (direction == DMA_MEM_TO_DEV) {\n\t\t\t\tdesc->hwdesc32.SAR = mem;\n\t\t\t\tdesc->hwdesc32.DAR = ds->tx_reg;\n\t\t\t} else {\n\t\t\t\tdesc->hwdesc32.SAR = ds->rx_reg;\n\t\t\t\tdesc->hwdesc32.DAR = mem;\n\t\t\t}\n\t\t\tdesc->hwdesc32.CNTR = sg_dma_len(sg);\n\t\t}\n\t\tif (direction == DMA_MEM_TO_DEV) {\n\t\t\tsai = ds->reg_width;\n\t\t\tdai = 0;\n\t\t} else {\n\t\t\tsai = 0;\n\t\t\tdai = ds->reg_width;\n\t\t}\n\t\ttxx9dmac_desc_set_nosimple(ddev, desc, sai, dai,\n\t\t\t\t\tdc->ccr | TXX9_DMA_CCR_XFACT);\n\n\t\tif (!first) {\n\t\t\tfirst = desc;\n\t\t} else {\n\t\t\tdesc_write_CHAR(dc, prev, desc->txd.phys);\n\t\t\tdma_sync_single_for_device(chan2parent(&dc->chan),\n\t\t\t\t\tprev->txd.phys,\n\t\t\t\t\tddev->descsize,\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\tlist_add_tail(&desc->desc_node, &first->tx_list);\n\t\t}\n\t\tprev = desc;\n\t}\n\n\t \n\tif (flags & DMA_PREP_INTERRUPT)\n\t\ttxx9dmac_desc_set_INTENT(ddev, prev);\n\n\tdesc_write_CHAR(dc, prev, 0);\n\tdma_sync_single_for_device(chan2parent(&dc->chan),\n\t\t\tprev->txd.phys, ddev->descsize,\n\t\t\tDMA_TO_DEVICE);\n\n\tfirst->txd.flags = flags;\n\tfirst->len = 0;\n\n\treturn &first->txd;\n}\n\nstatic int txx9dmac_terminate_all(struct dma_chan *chan)\n{\n\tstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\n\tstruct txx9dmac_desc *desc, *_desc;\n\tLIST_HEAD(list);\n\n\tdev_vdbg(chan2dev(chan), \"terminate_all\\n\");\n\tspin_lock_bh(&dc->lock);\n\n\ttxx9dmac_reset_chan(dc);\n\n\t \n\tlist_splice_init(&dc->queue, &list);\n\tlist_splice_init(&dc->active_list, &list);\n\n\tspin_unlock_bh(&dc->lock);\n\n\t \n\tlist_for_each_entry_safe(desc, _desc, &list, desc_node)\n\t\ttxx9dmac_descriptor_complete(dc, desc);\n\n\treturn 0;\n}\n\nstatic enum dma_status\ntxx9dmac_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\n\t\t   struct dma_tx_state *txstate)\n{\n\tstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\n\tenum dma_status ret;\n\n\tret = dma_cookie_status(chan, cookie, txstate);\n\tif (ret == DMA_COMPLETE)\n\t\treturn DMA_COMPLETE;\n\n\tspin_lock_bh(&dc->lock);\n\ttxx9dmac_scan_descriptors(dc);\n\tspin_unlock_bh(&dc->lock);\n\n\treturn dma_cookie_status(chan, cookie, txstate);\n}\n\nstatic void txx9dmac_chain_dynamic(struct txx9dmac_chan *dc,\n\t\t\t\t   struct txx9dmac_desc *prev)\n{\n\tstruct txx9dmac_dev *ddev = dc->ddev;\n\tstruct txx9dmac_desc *desc;\n\tLIST_HEAD(list);\n\n\tprev = txx9dmac_last_child(prev);\n\ttxx9dmac_dequeue(dc, &list);\n\tdesc = list_entry(list.next, struct txx9dmac_desc, desc_node);\n\tdesc_write_CHAR(dc, prev, desc->txd.phys);\n\tdma_sync_single_for_device(chan2parent(&dc->chan),\n\t\t\t\t   prev->txd.phys, ddev->descsize,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (!(channel_readl(dc, CSR) & TXX9_DMA_CSR_CHNEN) &&\n\t    channel_read_CHAR(dc) == prev->txd.phys)\n\t\t \n\t\tchannel_write_CHAR(dc, desc->txd.phys);\n\tlist_splice_tail(&list, &dc->active_list);\n}\n\nstatic void txx9dmac_issue_pending(struct dma_chan *chan)\n{\n\tstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\n\n\tspin_lock_bh(&dc->lock);\n\n\tif (!list_empty(&dc->active_list))\n\t\ttxx9dmac_scan_descriptors(dc);\n\tif (!list_empty(&dc->queue)) {\n\t\tif (list_empty(&dc->active_list)) {\n\t\t\ttxx9dmac_dequeue(dc, &dc->active_list);\n\t\t\ttxx9dmac_dostart(dc, txx9dmac_first_active(dc));\n\t\t} else if (txx9_dma_have_SMPCHN()) {\n\t\t\tstruct txx9dmac_desc *prev = txx9dmac_last_active(dc);\n\n\t\t\tif (!(prev->txd.flags & DMA_PREP_INTERRUPT) ||\n\t\t\t    txx9dmac_chan_INTENT(dc))\n\t\t\t\ttxx9dmac_chain_dynamic(dc, prev);\n\t\t}\n\t}\n\n\tspin_unlock_bh(&dc->lock);\n}\n\nstatic int txx9dmac_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\n\tstruct txx9dmac_slave *ds = chan->private;\n\tstruct txx9dmac_desc *desc;\n\tint i;\n\n\tdev_vdbg(chan2dev(chan), \"alloc_chan_resources\\n\");\n\n\t \n\tif (channel_readl(dc, CSR) & TXX9_DMA_CSR_XFACT) {\n\t\tdev_dbg(chan2dev(chan), \"DMA channel not idle?\\n\");\n\t\treturn -EIO;\n\t}\n\n\tdma_cookie_init(chan);\n\n\tdc->ccr = TXX9_DMA_CCR_IMMCHN | TXX9_DMA_CCR_INTENE | CCR_LE;\n\ttxx9dmac_chan_set_SMPCHN(dc);\n\tif (!txx9_dma_have_SMPCHN() || (dc->ccr & TXX9_DMA_CCR_SMPCHN))\n\t\tdc->ccr |= TXX9_DMA_CCR_INTENC;\n\tif (chan->device->device_prep_dma_memcpy) {\n\t\tif (ds)\n\t\t\treturn -EINVAL;\n\t\tdc->ccr |= TXX9_DMA_CCR_XFSZ_X8;\n\t} else {\n\t\tif (!ds ||\n\t\t    (ds->tx_reg && ds->rx_reg) || (!ds->tx_reg && !ds->rx_reg))\n\t\t\treturn -EINVAL;\n\t\tdc->ccr |= TXX9_DMA_CCR_EXTRQ |\n\t\t\tTXX9_DMA_CCR_XFSZ(__ffs(ds->reg_width));\n\t\ttxx9dmac_chan_set_INTENT(dc);\n\t}\n\n\tspin_lock_bh(&dc->lock);\n\ti = dc->descs_allocated;\n\twhile (dc->descs_allocated < TXX9_DMA_INITIAL_DESC_COUNT) {\n\t\tspin_unlock_bh(&dc->lock);\n\n\t\tdesc = txx9dmac_desc_alloc(dc, GFP_KERNEL);\n\t\tif (!desc) {\n\t\t\tdev_info(chan2dev(chan),\n\t\t\t\t\"only allocated %d descriptors\\n\", i);\n\t\t\tspin_lock_bh(&dc->lock);\n\t\t\tbreak;\n\t\t}\n\t\ttxx9dmac_desc_put(dc, desc);\n\n\t\tspin_lock_bh(&dc->lock);\n\t\ti = ++dc->descs_allocated;\n\t}\n\tspin_unlock_bh(&dc->lock);\n\n\tdev_dbg(chan2dev(chan),\n\t\t\"alloc_chan_resources allocated %d descriptors\\n\", i);\n\n\treturn i;\n}\n\nstatic void txx9dmac_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct txx9dmac_chan *dc = to_txx9dmac_chan(chan);\n\tstruct txx9dmac_dev *ddev = dc->ddev;\n\tstruct txx9dmac_desc *desc, *_desc;\n\tLIST_HEAD(list);\n\n\tdev_dbg(chan2dev(chan), \"free_chan_resources (descs allocated=%u)\\n\",\n\t\t\tdc->descs_allocated);\n\n\t \n\tBUG_ON(!list_empty(&dc->active_list));\n\tBUG_ON(!list_empty(&dc->queue));\n\tBUG_ON(channel_readl(dc, CSR) & TXX9_DMA_CSR_XFACT);\n\n\tspin_lock_bh(&dc->lock);\n\tlist_splice_init(&dc->free_list, &list);\n\tdc->descs_allocated = 0;\n\tspin_unlock_bh(&dc->lock);\n\n\tlist_for_each_entry_safe(desc, _desc, &list, desc_node) {\n\t\tdev_vdbg(chan2dev(chan), \"  freeing descriptor %p\\n\", desc);\n\t\tdma_unmap_single(chan2parent(chan), desc->txd.phys,\n\t\t\t\t ddev->descsize, DMA_TO_DEVICE);\n\t\tkfree(desc);\n\t}\n\n\tdev_vdbg(chan2dev(chan), \"free_chan_resources done\\n\");\n}\n\n \n\nstatic void txx9dmac_off(struct txx9dmac_dev *ddev)\n{\n\tdma_writel(ddev, MCR, 0);\n}\n\nstatic int __init txx9dmac_chan_probe(struct platform_device *pdev)\n{\n\tstruct txx9dmac_chan_platform_data *cpdata =\n\t\t\tdev_get_platdata(&pdev->dev);\n\tstruct platform_device *dmac_dev = cpdata->dmac_dev;\n\tstruct txx9dmac_platform_data *pdata = dev_get_platdata(&dmac_dev->dev);\n\tstruct txx9dmac_chan *dc;\n\tint err;\n\tint ch = pdev->id % TXX9_DMA_MAX_NR_CHANNELS;\n\tint irq;\n\n\tdc = devm_kzalloc(&pdev->dev, sizeof(*dc), GFP_KERNEL);\n\tif (!dc)\n\t\treturn -ENOMEM;\n\n\tdc->dma.dev = &pdev->dev;\n\tdc->dma.device_alloc_chan_resources = txx9dmac_alloc_chan_resources;\n\tdc->dma.device_free_chan_resources = txx9dmac_free_chan_resources;\n\tdc->dma.device_terminate_all = txx9dmac_terminate_all;\n\tdc->dma.device_tx_status = txx9dmac_tx_status;\n\tdc->dma.device_issue_pending = txx9dmac_issue_pending;\n\tif (pdata && pdata->memcpy_chan == ch) {\n\t\tdc->dma.device_prep_dma_memcpy = txx9dmac_prep_dma_memcpy;\n\t\tdma_cap_set(DMA_MEMCPY, dc->dma.cap_mask);\n\t} else {\n\t\tdc->dma.device_prep_slave_sg = txx9dmac_prep_slave_sg;\n\t\tdma_cap_set(DMA_SLAVE, dc->dma.cap_mask);\n\t\tdma_cap_set(DMA_PRIVATE, dc->dma.cap_mask);\n\t}\n\n\tINIT_LIST_HEAD(&dc->dma.channels);\n\tdc->ddev = platform_get_drvdata(dmac_dev);\n\tif (dc->ddev->irq < 0) {\n\t\tirq = platform_get_irq(pdev, 0);\n\t\tif (irq < 0)\n\t\t\treturn irq;\n\t\ttasklet_setup(&dc->tasklet, txx9dmac_chan_tasklet);\n\t\tdc->irq = irq;\n\t\terr = devm_request_irq(&pdev->dev, dc->irq,\n\t\t\ttxx9dmac_chan_interrupt, 0, dev_name(&pdev->dev), dc);\n\t\tif (err)\n\t\t\treturn err;\n\t} else\n\t\tdc->irq = -1;\n\tdc->ddev->chan[ch] = dc;\n\tdc->chan.device = &dc->dma;\n\tlist_add_tail(&dc->chan.device_node, &dc->chan.device->channels);\n\tdma_cookie_init(&dc->chan);\n\n\tif (is_dmac64(dc))\n\t\tdc->ch_regs = &__txx9dmac_regs(dc->ddev)->CHAN[ch];\n\telse\n\t\tdc->ch_regs = &__txx9dmac_regs32(dc->ddev)->CHAN[ch];\n\tspin_lock_init(&dc->lock);\n\n\tINIT_LIST_HEAD(&dc->active_list);\n\tINIT_LIST_HEAD(&dc->queue);\n\tINIT_LIST_HEAD(&dc->free_list);\n\n\ttxx9dmac_reset_chan(dc);\n\n\tplatform_set_drvdata(pdev, dc);\n\n\terr = dma_async_device_register(&dc->dma);\n\tif (err)\n\t\treturn err;\n\tdev_dbg(&pdev->dev, \"TXx9 DMA Channel (dma%d%s%s)\\n\",\n\t\tdc->dma.dev_id,\n\t\tdma_has_cap(DMA_MEMCPY, dc->dma.cap_mask) ? \" memcpy\" : \"\",\n\t\tdma_has_cap(DMA_SLAVE, dc->dma.cap_mask) ? \" slave\" : \"\");\n\n\treturn 0;\n}\n\nstatic int txx9dmac_chan_remove(struct platform_device *pdev)\n{\n\tstruct txx9dmac_chan *dc = platform_get_drvdata(pdev);\n\n\n\tdma_async_device_unregister(&dc->dma);\n\tif (dc->irq >= 0) {\n\t\tdevm_free_irq(&pdev->dev, dc->irq, dc);\n\t\ttasklet_kill(&dc->tasklet);\n\t}\n\tdc->ddev->chan[pdev->id % TXX9_DMA_MAX_NR_CHANNELS] = NULL;\n\treturn 0;\n}\n\nstatic int __init txx9dmac_probe(struct platform_device *pdev)\n{\n\tstruct txx9dmac_platform_data *pdata = dev_get_platdata(&pdev->dev);\n\tstruct resource *io;\n\tstruct txx9dmac_dev *ddev;\n\tu32 mcr;\n\tint err;\n\n\tio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (!io)\n\t\treturn -EINVAL;\n\n\tddev = devm_kzalloc(&pdev->dev, sizeof(*ddev), GFP_KERNEL);\n\tif (!ddev)\n\t\treturn -ENOMEM;\n\n\tif (!devm_request_mem_region(&pdev->dev, io->start, resource_size(io),\n\t\t\t\t     dev_name(&pdev->dev)))\n\t\treturn -EBUSY;\n\n\tddev->regs = devm_ioremap(&pdev->dev, io->start, resource_size(io));\n\tif (!ddev->regs)\n\t\treturn -ENOMEM;\n\tddev->have_64bit_regs = pdata->have_64bit_regs;\n\tif (__is_dmac64(ddev))\n\t\tddev->descsize = sizeof(struct txx9dmac_hwdesc);\n\telse\n\t\tddev->descsize = sizeof(struct txx9dmac_hwdesc32);\n\n\t \n\ttxx9dmac_off(ddev);\n\n\tddev->irq = platform_get_irq(pdev, 0);\n\tif (ddev->irq >= 0) {\n\t\ttasklet_setup(&ddev->tasklet, txx9dmac_tasklet);\n\t\terr = devm_request_irq(&pdev->dev, ddev->irq,\n\t\t\ttxx9dmac_interrupt, 0, dev_name(&pdev->dev), ddev);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tmcr = TXX9_DMA_MCR_MSTEN | MCR_LE;\n\tif (pdata && pdata->memcpy_chan >= 0)\n\t\tmcr |= TXX9_DMA_MCR_FIFUM(pdata->memcpy_chan);\n\tdma_writel(ddev, MCR, mcr);\n\n\tplatform_set_drvdata(pdev, ddev);\n\treturn 0;\n}\n\nstatic int txx9dmac_remove(struct platform_device *pdev)\n{\n\tstruct txx9dmac_dev *ddev = platform_get_drvdata(pdev);\n\n\ttxx9dmac_off(ddev);\n\tif (ddev->irq >= 0) {\n\t\tdevm_free_irq(&pdev->dev, ddev->irq, ddev);\n\t\ttasklet_kill(&ddev->tasklet);\n\t}\n\treturn 0;\n}\n\nstatic void txx9dmac_shutdown(struct platform_device *pdev)\n{\n\tstruct txx9dmac_dev *ddev = platform_get_drvdata(pdev);\n\n\ttxx9dmac_off(ddev);\n}\n\nstatic int txx9dmac_suspend_noirq(struct device *dev)\n{\n\tstruct txx9dmac_dev *ddev = dev_get_drvdata(dev);\n\n\ttxx9dmac_off(ddev);\n\treturn 0;\n}\n\nstatic int txx9dmac_resume_noirq(struct device *dev)\n{\n\tstruct txx9dmac_dev *ddev = dev_get_drvdata(dev);\n\tstruct txx9dmac_platform_data *pdata = dev_get_platdata(dev);\n\tu32 mcr;\n\n\tmcr = TXX9_DMA_MCR_MSTEN | MCR_LE;\n\tif (pdata && pdata->memcpy_chan >= 0)\n\t\tmcr |= TXX9_DMA_MCR_FIFUM(pdata->memcpy_chan);\n\tdma_writel(ddev, MCR, mcr);\n\treturn 0;\n\n}\n\nstatic const struct dev_pm_ops txx9dmac_dev_pm_ops = {\n\t.suspend_noirq = txx9dmac_suspend_noirq,\n\t.resume_noirq = txx9dmac_resume_noirq,\n};\n\nstatic struct platform_driver txx9dmac_chan_driver = {\n\t.remove\t\t= txx9dmac_chan_remove,\n\t.driver = {\n\t\t.name\t= \"txx9dmac-chan\",\n\t},\n};\n\nstatic struct platform_driver txx9dmac_driver = {\n\t.remove\t\t= txx9dmac_remove,\n\t.shutdown\t= txx9dmac_shutdown,\n\t.driver = {\n\t\t.name\t= \"txx9dmac\",\n\t\t.pm\t= &txx9dmac_dev_pm_ops,\n\t},\n};\n\nstatic int __init txx9dmac_init(void)\n{\n\tint rc;\n\n\trc = platform_driver_probe(&txx9dmac_driver, txx9dmac_probe);\n\tif (!rc) {\n\t\trc = platform_driver_probe(&txx9dmac_chan_driver,\n\t\t\t\t\t   txx9dmac_chan_probe);\n\t\tif (rc)\n\t\t\tplatform_driver_unregister(&txx9dmac_driver);\n\t}\n\treturn rc;\n}\nmodule_init(txx9dmac_init);\n\nstatic void __exit txx9dmac_exit(void)\n{\n\tplatform_driver_unregister(&txx9dmac_chan_driver);\n\tplatform_driver_unregister(&txx9dmac_driver);\n}\nmodule_exit(txx9dmac_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"TXx9 DMA Controller driver\");\nMODULE_AUTHOR(\"Atsushi Nemoto <anemo@mba.ocn.ne.jp>\");\nMODULE_ALIAS(\"platform:txx9dmac\");\nMODULE_ALIAS(\"platform:txx9dmac-chan\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}