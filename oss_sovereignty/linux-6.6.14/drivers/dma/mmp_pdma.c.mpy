{
  "module_name": "mmp_pdma.c",
  "hash_id": "f69ca7267d5dd736e841730aa9b7d949ad473322e9dfc513d63bad2b6bda9f00",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/mmp_pdma.c",
  "human_readable_source": "\n \n\n#include <linux/err.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/types.h>\n#include <linux/interrupt.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/dmaengine.h>\n#include <linux/platform_device.h>\n#include <linux/device.h>\n#include <linux/platform_data/mmp_dma.h>\n#include <linux/dmapool.h>\n#include <linux/of_device.h>\n#include <linux/of_dma.h>\n#include <linux/of.h>\n\n#include \"dmaengine.h\"\n\n#define DCSR\t\t0x0000\n#define DALGN\t\t0x00a0\n#define DINT\t\t0x00f0\n#define DDADR\t\t0x0200\n#define DSADR(n)\t(0x0204 + ((n) << 4))\n#define DTADR(n)\t(0x0208 + ((n) << 4))\n#define DCMD\t\t0x020c\n\n#define DCSR_RUN\tBIT(31)\t \n#define DCSR_NODESC\tBIT(30)\t \n#define DCSR_STOPIRQEN\tBIT(29)\t \n#define DCSR_REQPEND\tBIT(8)\t \n#define DCSR_STOPSTATE\tBIT(3)\t \n#define DCSR_ENDINTR\tBIT(2)\t \n#define DCSR_STARTINTR\tBIT(1)\t \n#define DCSR_BUSERR\tBIT(0)\t \n\n#define DCSR_EORIRQEN\tBIT(28)\t \n#define DCSR_EORJMPEN\tBIT(27)\t \n#define DCSR_EORSTOPEN\tBIT(26)\t \n#define DCSR_SETCMPST\tBIT(25)\t \n#define DCSR_CLRCMPST\tBIT(24)\t \n#define DCSR_CMPST\tBIT(10)\t \n#define DCSR_EORINTR\tBIT(9)\t \n\n#define DRCMR(n)\t((((n) < 64) ? 0x0100 : 0x1100) + (((n) & 0x3f) << 2))\n#define DRCMR_MAPVLD\tBIT(7)\t \n#define DRCMR_CHLNUM\t0x1f\t \n\n#define DDADR_DESCADDR\t0xfffffff0\t \n#define DDADR_STOP\tBIT(0)\t \n\n#define DCMD_INCSRCADDR\tBIT(31)\t \n#define DCMD_INCTRGADDR\tBIT(30)\t \n#define DCMD_FLOWSRC\tBIT(29)\t \n#define DCMD_FLOWTRG\tBIT(28)\t \n#define DCMD_STARTIRQEN\tBIT(22)\t \n#define DCMD_ENDIRQEN\tBIT(21)\t \n#define DCMD_ENDIAN\tBIT(18)\t \n#define DCMD_BURST8\t(1 << 16)\t \n#define DCMD_BURST16\t(2 << 16)\t \n#define DCMD_BURST32\t(3 << 16)\t \n#define DCMD_WIDTH1\t(1 << 14)\t \n#define DCMD_WIDTH2\t(2 << 14)\t \n#define DCMD_WIDTH4\t(3 << 14)\t \n#define DCMD_LENGTH\t0x01fff\t\t \n\n#define PDMA_MAX_DESC_BYTES\tDCMD_LENGTH\n\nstruct mmp_pdma_desc_hw {\n\tu32 ddadr;\t \n\tu32 dsadr;\t \n\tu32 dtadr;\t \n\tu32 dcmd;\t \n} __aligned(32);\n\nstruct mmp_pdma_desc_sw {\n\tstruct mmp_pdma_desc_hw desc;\n\tstruct list_head node;\n\tstruct list_head tx_list;\n\tstruct dma_async_tx_descriptor async_tx;\n};\n\nstruct mmp_pdma_phy;\n\nstruct mmp_pdma_chan {\n\tstruct device *dev;\n\tstruct dma_chan chan;\n\tstruct dma_async_tx_descriptor desc;\n\tstruct mmp_pdma_phy *phy;\n\tenum dma_transfer_direction dir;\n\tstruct dma_slave_config slave_config;\n\n\tstruct mmp_pdma_desc_sw *cyclic_first;\t \n\n\t \n\tstruct tasklet_struct tasklet;\n\tu32 dcmd;\n\tu32 drcmr;\n\tu32 dev_addr;\n\n\t \n\tspinlock_t desc_lock;\t\t \n\tstruct list_head chain_pending;\t \n\tstruct list_head chain_running;\t \n\tbool idle;\t\t\t \n\tbool byte_align;\n\n\tstruct dma_pool *desc_pool;\t \n};\n\nstruct mmp_pdma_phy {\n\tint idx;\n\tvoid __iomem *base;\n\tstruct mmp_pdma_chan *vchan;\n};\n\nstruct mmp_pdma_device {\n\tint\t\t\t\tdma_channels;\n\tvoid __iomem\t\t\t*base;\n\tstruct device\t\t\t*dev;\n\tstruct dma_device\t\tdevice;\n\tstruct mmp_pdma_phy\t\t*phy;\n\tspinlock_t phy_lock;  \n};\n\n#define tx_to_mmp_pdma_desc(tx)\t\t\t\t\t\\\n\tcontainer_of(tx, struct mmp_pdma_desc_sw, async_tx)\n#define to_mmp_pdma_desc(lh)\t\t\t\t\t\\\n\tcontainer_of(lh, struct mmp_pdma_desc_sw, node)\n#define to_mmp_pdma_chan(dchan)\t\t\t\t\t\\\n\tcontainer_of(dchan, struct mmp_pdma_chan, chan)\n#define to_mmp_pdma_dev(dmadev)\t\t\t\t\t\\\n\tcontainer_of(dmadev, struct mmp_pdma_device, device)\n\nstatic int mmp_pdma_config_write(struct dma_chan *dchan,\n\t\t\t   struct dma_slave_config *cfg,\n\t\t\t   enum dma_transfer_direction direction);\n\nstatic void set_desc(struct mmp_pdma_phy *phy, dma_addr_t addr)\n{\n\tu32 reg = (phy->idx << 4) + DDADR;\n\n\twritel(addr, phy->base + reg);\n}\n\nstatic void enable_chan(struct mmp_pdma_phy *phy)\n{\n\tu32 reg, dalgn;\n\n\tif (!phy->vchan)\n\t\treturn;\n\n\treg = DRCMR(phy->vchan->drcmr);\n\twritel(DRCMR_MAPVLD | phy->idx, phy->base + reg);\n\n\tdalgn = readl(phy->base + DALGN);\n\tif (phy->vchan->byte_align)\n\t\tdalgn |= 1 << phy->idx;\n\telse\n\t\tdalgn &= ~(1 << phy->idx);\n\twritel(dalgn, phy->base + DALGN);\n\n\treg = (phy->idx << 2) + DCSR;\n\twritel(readl(phy->base + reg) | DCSR_RUN, phy->base + reg);\n}\n\nstatic void disable_chan(struct mmp_pdma_phy *phy)\n{\n\tu32 reg;\n\n\tif (!phy)\n\t\treturn;\n\n\treg = (phy->idx << 2) + DCSR;\n\twritel(readl(phy->base + reg) & ~DCSR_RUN, phy->base + reg);\n}\n\nstatic int clear_chan_irq(struct mmp_pdma_phy *phy)\n{\n\tu32 dcsr;\n\tu32 dint = readl(phy->base + DINT);\n\tu32 reg = (phy->idx << 2) + DCSR;\n\n\tif (!(dint & BIT(phy->idx)))\n\t\treturn -EAGAIN;\n\n\t \n\tdcsr = readl(phy->base + reg);\n\twritel(dcsr, phy->base + reg);\n\tif ((dcsr & DCSR_BUSERR) && (phy->vchan))\n\t\tdev_warn(phy->vchan->dev, \"DCSR_BUSERR\\n\");\n\n\treturn 0;\n}\n\nstatic irqreturn_t mmp_pdma_chan_handler(int irq, void *dev_id)\n{\n\tstruct mmp_pdma_phy *phy = dev_id;\n\n\tif (clear_chan_irq(phy) != 0)\n\t\treturn IRQ_NONE;\n\n\ttasklet_schedule(&phy->vchan->tasklet);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t mmp_pdma_int_handler(int irq, void *dev_id)\n{\n\tstruct mmp_pdma_device *pdev = dev_id;\n\tstruct mmp_pdma_phy *phy;\n\tu32 dint = readl(pdev->base + DINT);\n\tint i, ret;\n\tint irq_num = 0;\n\n\twhile (dint) {\n\t\ti = __ffs(dint);\n\t\t \n\t\tif (i >= pdev->dma_channels)\n\t\t\tbreak;\n\t\tdint &= (dint - 1);\n\t\tphy = &pdev->phy[i];\n\t\tret = mmp_pdma_chan_handler(irq, phy);\n\t\tif (ret == IRQ_HANDLED)\n\t\t\tirq_num++;\n\t}\n\n\tif (irq_num)\n\t\treturn IRQ_HANDLED;\n\n\treturn IRQ_NONE;\n}\n\n \nstatic struct mmp_pdma_phy *lookup_phy(struct mmp_pdma_chan *pchan)\n{\n\tint prio, i;\n\tstruct mmp_pdma_device *pdev = to_mmp_pdma_dev(pchan->chan.device);\n\tstruct mmp_pdma_phy *phy, *found = NULL;\n\tunsigned long flags;\n\n\t \n\n\tspin_lock_irqsave(&pdev->phy_lock, flags);\n\tfor (prio = 0; prio <= ((pdev->dma_channels - 1) & 0xf) >> 2; prio++) {\n\t\tfor (i = 0; i < pdev->dma_channels; i++) {\n\t\t\tif (prio != (i & 0xf) >> 2)\n\t\t\t\tcontinue;\n\t\t\tphy = &pdev->phy[i];\n\t\t\tif (!phy->vchan) {\n\t\t\t\tphy->vchan = pchan;\n\t\t\t\tfound = phy;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\t}\n\nout_unlock:\n\tspin_unlock_irqrestore(&pdev->phy_lock, flags);\n\treturn found;\n}\n\nstatic void mmp_pdma_free_phy(struct mmp_pdma_chan *pchan)\n{\n\tstruct mmp_pdma_device *pdev = to_mmp_pdma_dev(pchan->chan.device);\n\tunsigned long flags;\n\tu32 reg;\n\n\tif (!pchan->phy)\n\t\treturn;\n\n\t \n\treg = DRCMR(pchan->drcmr);\n\twritel(0, pchan->phy->base + reg);\n\n\tspin_lock_irqsave(&pdev->phy_lock, flags);\n\tpchan->phy->vchan = NULL;\n\tpchan->phy = NULL;\n\tspin_unlock_irqrestore(&pdev->phy_lock, flags);\n}\n\n \nstatic void start_pending_queue(struct mmp_pdma_chan *chan)\n{\n\tstruct mmp_pdma_desc_sw *desc;\n\n\t \n\tif (!chan->idle) {\n\t\tdev_dbg(chan->dev, \"DMA controller still busy\\n\");\n\t\treturn;\n\t}\n\n\tif (list_empty(&chan->chain_pending)) {\n\t\t \n\t\tmmp_pdma_free_phy(chan);\n\t\tdev_dbg(chan->dev, \"no pending list\\n\");\n\t\treturn;\n\t}\n\n\tif (!chan->phy) {\n\t\tchan->phy = lookup_phy(chan);\n\t\tif (!chan->phy) {\n\t\t\tdev_dbg(chan->dev, \"no free dma channel\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\tdesc = list_first_entry(&chan->chain_pending,\n\t\t\t\tstruct mmp_pdma_desc_sw, node);\n\tlist_splice_tail_init(&chan->chain_pending, &chan->chain_running);\n\n\t \n\tset_desc(chan->phy, desc->async_tx.phys);\n\tenable_chan(chan->phy);\n\tchan->idle = false;\n}\n\n\n \nstatic dma_cookie_t mmp_pdma_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(tx->chan);\n\tstruct mmp_pdma_desc_sw *desc = tx_to_mmp_pdma_desc(tx);\n\tstruct mmp_pdma_desc_sw *child;\n\tunsigned long flags;\n\tdma_cookie_t cookie = -EBUSY;\n\n\tspin_lock_irqsave(&chan->desc_lock, flags);\n\n\tlist_for_each_entry(child, &desc->tx_list, node) {\n\t\tcookie = dma_cookie_assign(&child->async_tx);\n\t}\n\n\t \n\tlist_splice_tail_init(&desc->tx_list, &chan->chain_pending);\n\n\tspin_unlock_irqrestore(&chan->desc_lock, flags);\n\n\treturn cookie;\n}\n\nstatic struct mmp_pdma_desc_sw *\nmmp_pdma_alloc_descriptor(struct mmp_pdma_chan *chan)\n{\n\tstruct mmp_pdma_desc_sw *desc;\n\tdma_addr_t pdesc;\n\n\tdesc = dma_pool_zalloc(chan->desc_pool, GFP_ATOMIC, &pdesc);\n\tif (!desc) {\n\t\tdev_err(chan->dev, \"out of memory for link descriptor\\n\");\n\t\treturn NULL;\n\t}\n\n\tINIT_LIST_HEAD(&desc->tx_list);\n\tdma_async_tx_descriptor_init(&desc->async_tx, &chan->chan);\n\t \n\tdesc->async_tx.tx_submit = mmp_pdma_tx_submit;\n\tdesc->async_tx.phys = pdesc;\n\n\treturn desc;\n}\n\n \n\nstatic int mmp_pdma_alloc_chan_resources(struct dma_chan *dchan)\n{\n\tstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\n\n\tif (chan->desc_pool)\n\t\treturn 1;\n\n\tchan->desc_pool = dma_pool_create(dev_name(&dchan->dev->device),\n\t\t\t\t\t  chan->dev,\n\t\t\t\t\t  sizeof(struct mmp_pdma_desc_sw),\n\t\t\t\t\t  __alignof__(struct mmp_pdma_desc_sw),\n\t\t\t\t\t  0);\n\tif (!chan->desc_pool) {\n\t\tdev_err(chan->dev, \"unable to allocate descriptor pool\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmmp_pdma_free_phy(chan);\n\tchan->idle = true;\n\tchan->dev_addr = 0;\n\treturn 1;\n}\n\nstatic void mmp_pdma_free_desc_list(struct mmp_pdma_chan *chan,\n\t\t\t\t    struct list_head *list)\n{\n\tstruct mmp_pdma_desc_sw *desc, *_desc;\n\n\tlist_for_each_entry_safe(desc, _desc, list, node) {\n\t\tlist_del(&desc->node);\n\t\tdma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);\n\t}\n}\n\nstatic void mmp_pdma_free_chan_resources(struct dma_chan *dchan)\n{\n\tstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->desc_lock, flags);\n\tmmp_pdma_free_desc_list(chan, &chan->chain_pending);\n\tmmp_pdma_free_desc_list(chan, &chan->chain_running);\n\tspin_unlock_irqrestore(&chan->desc_lock, flags);\n\n\tdma_pool_destroy(chan->desc_pool);\n\tchan->desc_pool = NULL;\n\tchan->idle = true;\n\tchan->dev_addr = 0;\n\tmmp_pdma_free_phy(chan);\n\treturn;\n}\n\nstatic struct dma_async_tx_descriptor *\nmmp_pdma_prep_memcpy(struct dma_chan *dchan,\n\t\t     dma_addr_t dma_dst, dma_addr_t dma_src,\n\t\t     size_t len, unsigned long flags)\n{\n\tstruct mmp_pdma_chan *chan;\n\tstruct mmp_pdma_desc_sw *first = NULL, *prev = NULL, *new;\n\tsize_t copy = 0;\n\n\tif (!dchan)\n\t\treturn NULL;\n\n\tif (!len)\n\t\treturn NULL;\n\n\tchan = to_mmp_pdma_chan(dchan);\n\tchan->byte_align = false;\n\n\tif (!chan->dir) {\n\t\tchan->dir = DMA_MEM_TO_MEM;\n\t\tchan->dcmd = DCMD_INCTRGADDR | DCMD_INCSRCADDR;\n\t\tchan->dcmd |= DCMD_BURST32;\n\t}\n\n\tdo {\n\t\t \n\t\tnew = mmp_pdma_alloc_descriptor(chan);\n\t\tif (!new) {\n\t\t\tdev_err(chan->dev, \"no memory for desc\\n\");\n\t\t\tgoto fail;\n\t\t}\n\n\t\tcopy = min_t(size_t, len, PDMA_MAX_DESC_BYTES);\n\t\tif (dma_src & 0x7 || dma_dst & 0x7)\n\t\t\tchan->byte_align = true;\n\n\t\tnew->desc.dcmd = chan->dcmd | (DCMD_LENGTH & copy);\n\t\tnew->desc.dsadr = dma_src;\n\t\tnew->desc.dtadr = dma_dst;\n\n\t\tif (!first)\n\t\t\tfirst = new;\n\t\telse\n\t\t\tprev->desc.ddadr = new->async_tx.phys;\n\n\t\tnew->async_tx.cookie = 0;\n\t\tasync_tx_ack(&new->async_tx);\n\n\t\tprev = new;\n\t\tlen -= copy;\n\n\t\tif (chan->dir == DMA_MEM_TO_DEV) {\n\t\t\tdma_src += copy;\n\t\t} else if (chan->dir == DMA_DEV_TO_MEM) {\n\t\t\tdma_dst += copy;\n\t\t} else if (chan->dir == DMA_MEM_TO_MEM) {\n\t\t\tdma_src += copy;\n\t\t\tdma_dst += copy;\n\t\t}\n\n\t\t \n\t\tlist_add_tail(&new->node, &first->tx_list);\n\t} while (len);\n\n\tfirst->async_tx.flags = flags;  \n\tfirst->async_tx.cookie = -EBUSY;\n\n\t \n\tnew->desc.ddadr = DDADR_STOP;\n\tnew->desc.dcmd |= DCMD_ENDIRQEN;\n\n\tchan->cyclic_first = NULL;\n\n\treturn &first->async_tx;\n\nfail:\n\tif (first)\n\t\tmmp_pdma_free_desc_list(chan, &first->tx_list);\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor *\nmmp_pdma_prep_slave_sg(struct dma_chan *dchan, struct scatterlist *sgl,\n\t\t       unsigned int sg_len, enum dma_transfer_direction dir,\n\t\t       unsigned long flags, void *context)\n{\n\tstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\n\tstruct mmp_pdma_desc_sw *first = NULL, *prev = NULL, *new = NULL;\n\tsize_t len, avail;\n\tstruct scatterlist *sg;\n\tdma_addr_t addr;\n\tint i;\n\n\tif ((sgl == NULL) || (sg_len == 0))\n\t\treturn NULL;\n\n\tchan->byte_align = false;\n\n\tmmp_pdma_config_write(dchan, &chan->slave_config, dir);\n\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\taddr = sg_dma_address(sg);\n\t\tavail = sg_dma_len(sgl);\n\n\t\tdo {\n\t\t\tlen = min_t(size_t, avail, PDMA_MAX_DESC_BYTES);\n\t\t\tif (addr & 0x7)\n\t\t\t\tchan->byte_align = true;\n\n\t\t\t \n\t\t\tnew = mmp_pdma_alloc_descriptor(chan);\n\t\t\tif (!new) {\n\t\t\t\tdev_err(chan->dev, \"no memory for desc\\n\");\n\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tnew->desc.dcmd = chan->dcmd | (DCMD_LENGTH & len);\n\t\t\tif (dir == DMA_MEM_TO_DEV) {\n\t\t\t\tnew->desc.dsadr = addr;\n\t\t\t\tnew->desc.dtadr = chan->dev_addr;\n\t\t\t} else {\n\t\t\t\tnew->desc.dsadr = chan->dev_addr;\n\t\t\t\tnew->desc.dtadr = addr;\n\t\t\t}\n\n\t\t\tif (!first)\n\t\t\t\tfirst = new;\n\t\t\telse\n\t\t\t\tprev->desc.ddadr = new->async_tx.phys;\n\n\t\t\tnew->async_tx.cookie = 0;\n\t\t\tasync_tx_ack(&new->async_tx);\n\t\t\tprev = new;\n\n\t\t\t \n\t\t\tlist_add_tail(&new->node, &first->tx_list);\n\n\t\t\t \n\t\t\taddr += len;\n\t\t\tavail -= len;\n\t\t} while (avail);\n\t}\n\n\tfirst->async_tx.cookie = -EBUSY;\n\tfirst->async_tx.flags = flags;\n\n\t \n\tnew->desc.ddadr = DDADR_STOP;\n\tnew->desc.dcmd |= DCMD_ENDIRQEN;\n\n\tchan->dir = dir;\n\tchan->cyclic_first = NULL;\n\n\treturn &first->async_tx;\n\nfail:\n\tif (first)\n\t\tmmp_pdma_free_desc_list(chan, &first->tx_list);\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor *\nmmp_pdma_prep_dma_cyclic(struct dma_chan *dchan,\n\t\t\t dma_addr_t buf_addr, size_t len, size_t period_len,\n\t\t\t enum dma_transfer_direction direction,\n\t\t\t unsigned long flags)\n{\n\tstruct mmp_pdma_chan *chan;\n\tstruct mmp_pdma_desc_sw *first = NULL, *prev = NULL, *new;\n\tdma_addr_t dma_src, dma_dst;\n\n\tif (!dchan || !len || !period_len)\n\t\treturn NULL;\n\n\t \n\tif (len % period_len != 0)\n\t\treturn NULL;\n\n\tif (period_len > PDMA_MAX_DESC_BYTES)\n\t\treturn NULL;\n\n\tchan = to_mmp_pdma_chan(dchan);\n\tmmp_pdma_config_write(dchan, &chan->slave_config, direction);\n\n\tswitch (direction) {\n\tcase DMA_MEM_TO_DEV:\n\t\tdma_src = buf_addr;\n\t\tdma_dst = chan->dev_addr;\n\t\tbreak;\n\tcase DMA_DEV_TO_MEM:\n\t\tdma_dst = buf_addr;\n\t\tdma_src = chan->dev_addr;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(chan->dev, \"Unsupported direction for cyclic DMA\\n\");\n\t\treturn NULL;\n\t}\n\n\tchan->dir = direction;\n\n\tdo {\n\t\t \n\t\tnew = mmp_pdma_alloc_descriptor(chan);\n\t\tif (!new) {\n\t\t\tdev_err(chan->dev, \"no memory for desc\\n\");\n\t\t\tgoto fail;\n\t\t}\n\n\t\tnew->desc.dcmd = (chan->dcmd | DCMD_ENDIRQEN |\n\t\t\t\t  (DCMD_LENGTH & period_len));\n\t\tnew->desc.dsadr = dma_src;\n\t\tnew->desc.dtadr = dma_dst;\n\n\t\tif (!first)\n\t\t\tfirst = new;\n\t\telse\n\t\t\tprev->desc.ddadr = new->async_tx.phys;\n\n\t\tnew->async_tx.cookie = 0;\n\t\tasync_tx_ack(&new->async_tx);\n\n\t\tprev = new;\n\t\tlen -= period_len;\n\n\t\tif (chan->dir == DMA_MEM_TO_DEV)\n\t\t\tdma_src += period_len;\n\t\telse\n\t\t\tdma_dst += period_len;\n\n\t\t \n\t\tlist_add_tail(&new->node, &first->tx_list);\n\t} while (len);\n\n\tfirst->async_tx.flags = flags;  \n\tfirst->async_tx.cookie = -EBUSY;\n\n\t \n\tnew->desc.ddadr = first->async_tx.phys;\n\tchan->cyclic_first = first;\n\n\treturn &first->async_tx;\n\nfail:\n\tif (first)\n\t\tmmp_pdma_free_desc_list(chan, &first->tx_list);\n\treturn NULL;\n}\n\nstatic int mmp_pdma_config_write(struct dma_chan *dchan,\n\t\t\t   struct dma_slave_config *cfg,\n\t\t\t   enum dma_transfer_direction direction)\n{\n\tstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\n\tu32 maxburst = 0, addr = 0;\n\tenum dma_slave_buswidth width = DMA_SLAVE_BUSWIDTH_UNDEFINED;\n\n\tif (!dchan)\n\t\treturn -EINVAL;\n\n\tif (direction == DMA_DEV_TO_MEM) {\n\t\tchan->dcmd = DCMD_INCTRGADDR | DCMD_FLOWSRC;\n\t\tmaxburst = cfg->src_maxburst;\n\t\twidth = cfg->src_addr_width;\n\t\taddr = cfg->src_addr;\n\t} else if (direction == DMA_MEM_TO_DEV) {\n\t\tchan->dcmd = DCMD_INCSRCADDR | DCMD_FLOWTRG;\n\t\tmaxburst = cfg->dst_maxburst;\n\t\twidth = cfg->dst_addr_width;\n\t\taddr = cfg->dst_addr;\n\t}\n\n\tif (width == DMA_SLAVE_BUSWIDTH_1_BYTE)\n\t\tchan->dcmd |= DCMD_WIDTH1;\n\telse if (width == DMA_SLAVE_BUSWIDTH_2_BYTES)\n\t\tchan->dcmd |= DCMD_WIDTH2;\n\telse if (width == DMA_SLAVE_BUSWIDTH_4_BYTES)\n\t\tchan->dcmd |= DCMD_WIDTH4;\n\n\tif (maxburst == 8)\n\t\tchan->dcmd |= DCMD_BURST8;\n\telse if (maxburst == 16)\n\t\tchan->dcmd |= DCMD_BURST16;\n\telse if (maxburst == 32)\n\t\tchan->dcmd |= DCMD_BURST32;\n\n\tchan->dir = direction;\n\tchan->dev_addr = addr;\n\n\treturn 0;\n}\n\nstatic int mmp_pdma_config(struct dma_chan *dchan,\n\t\t\t   struct dma_slave_config *cfg)\n{\n\tstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\n\n\tmemcpy(&chan->slave_config, cfg, sizeof(*cfg));\n\treturn 0;\n}\n\nstatic int mmp_pdma_terminate_all(struct dma_chan *dchan)\n{\n\tstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\n\tunsigned long flags;\n\n\tif (!dchan)\n\t\treturn -EINVAL;\n\n\tdisable_chan(chan->phy);\n\tmmp_pdma_free_phy(chan);\n\tspin_lock_irqsave(&chan->desc_lock, flags);\n\tmmp_pdma_free_desc_list(chan, &chan->chain_pending);\n\tmmp_pdma_free_desc_list(chan, &chan->chain_running);\n\tspin_unlock_irqrestore(&chan->desc_lock, flags);\n\tchan->idle = true;\n\n\treturn 0;\n}\n\nstatic unsigned int mmp_pdma_residue(struct mmp_pdma_chan *chan,\n\t\t\t\t     dma_cookie_t cookie)\n{\n\tstruct mmp_pdma_desc_sw *sw;\n\tu32 curr, residue = 0;\n\tbool passed = false;\n\tbool cyclic = chan->cyclic_first != NULL;\n\n\t \n\tif (!chan->phy)\n\t\treturn 0;\n\n\tif (chan->dir == DMA_DEV_TO_MEM)\n\t\tcurr = readl(chan->phy->base + DTADR(chan->phy->idx));\n\telse\n\t\tcurr = readl(chan->phy->base + DSADR(chan->phy->idx));\n\n\tlist_for_each_entry(sw, &chan->chain_running, node) {\n\t\tu32 start, end, len;\n\n\t\tif (chan->dir == DMA_DEV_TO_MEM)\n\t\t\tstart = sw->desc.dtadr;\n\t\telse\n\t\t\tstart = sw->desc.dsadr;\n\n\t\tlen = sw->desc.dcmd & DCMD_LENGTH;\n\t\tend = start + len;\n\n\t\t \n\n\t\tif (passed) {\n\t\t\tresidue += len;\n\t\t} else if (curr >= start && curr <= end) {\n\t\t\tresidue += end - curr;\n\t\t\tpassed = true;\n\t\t}\n\n\t\t \n\t\tif (cyclic || !(sw->desc.dcmd & DCMD_ENDIRQEN))\n\t\t\tcontinue;\n\n\t\tif (sw->async_tx.cookie == cookie) {\n\t\t\treturn residue;\n\t\t} else {\n\t\t\tresidue = 0;\n\t\t\tpassed = false;\n\t\t}\n\t}\n\n\t \n\treturn residue;\n}\n\nstatic enum dma_status mmp_pdma_tx_status(struct dma_chan *dchan,\n\t\t\t\t\t  dma_cookie_t cookie,\n\t\t\t\t\t  struct dma_tx_state *txstate)\n{\n\tstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\n\tenum dma_status ret;\n\n\tret = dma_cookie_status(dchan, cookie, txstate);\n\tif (likely(ret != DMA_ERROR))\n\t\tdma_set_residue(txstate, mmp_pdma_residue(chan, cookie));\n\n\treturn ret;\n}\n\n \nstatic void mmp_pdma_issue_pending(struct dma_chan *dchan)\n{\n\tstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->desc_lock, flags);\n\tstart_pending_queue(chan);\n\tspin_unlock_irqrestore(&chan->desc_lock, flags);\n}\n\n \nstatic void dma_do_tasklet(struct tasklet_struct *t)\n{\n\tstruct mmp_pdma_chan *chan = from_tasklet(chan, t, tasklet);\n\tstruct mmp_pdma_desc_sw *desc, *_desc;\n\tLIST_HEAD(chain_cleanup);\n\tunsigned long flags;\n\tstruct dmaengine_desc_callback cb;\n\n\tif (chan->cyclic_first) {\n\t\tspin_lock_irqsave(&chan->desc_lock, flags);\n\t\tdesc = chan->cyclic_first;\n\t\tdmaengine_desc_get_callback(&desc->async_tx, &cb);\n\t\tspin_unlock_irqrestore(&chan->desc_lock, flags);\n\n\t\tdmaengine_desc_callback_invoke(&cb, NULL);\n\n\t\treturn;\n\t}\n\n\t \n\tspin_lock_irqsave(&chan->desc_lock, flags);\n\n\tlist_for_each_entry_safe(desc, _desc, &chan->chain_running, node) {\n\t\t \n\t\tlist_move(&desc->node, &chain_cleanup);\n\n\t\t \n\t\tif (desc->desc.dcmd & DCMD_ENDIRQEN) {\n\t\t\tdma_cookie_t cookie = desc->async_tx.cookie;\n\t\t\tdma_cookie_complete(&desc->async_tx);\n\t\t\tdev_dbg(chan->dev, \"completed_cookie=%d\\n\", cookie);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tchan->idle = list_empty(&chan->chain_running);\n\n\t \n\tstart_pending_queue(chan);\n\tspin_unlock_irqrestore(&chan->desc_lock, flags);\n\n\t \n\tlist_for_each_entry_safe(desc, _desc, &chain_cleanup, node) {\n\t\tstruct dma_async_tx_descriptor *txd = &desc->async_tx;\n\n\t\t \n\t\tlist_del(&desc->node);\n\t\t \n\t\tdmaengine_desc_get_callback(txd, &cb);\n\t\tdmaengine_desc_callback_invoke(&cb, NULL);\n\n\t\tdma_pool_free(chan->desc_pool, desc, txd->phys);\n\t}\n}\n\nstatic int mmp_pdma_remove(struct platform_device *op)\n{\n\tstruct mmp_pdma_device *pdev = platform_get_drvdata(op);\n\tstruct mmp_pdma_phy *phy;\n\tint i, irq = 0, irq_num = 0;\n\n\tif (op->dev.of_node)\n\t\tof_dma_controller_free(op->dev.of_node);\n\n\tfor (i = 0; i < pdev->dma_channels; i++) {\n\t\tif (platform_get_irq(op, i) > 0)\n\t\t\tirq_num++;\n\t}\n\n\tif (irq_num != pdev->dma_channels) {\n\t\tirq = platform_get_irq(op, 0);\n\t\tdevm_free_irq(&op->dev, irq, pdev);\n\t} else {\n\t\tfor (i = 0; i < pdev->dma_channels; i++) {\n\t\t\tphy = &pdev->phy[i];\n\t\t\tirq = platform_get_irq(op, i);\n\t\t\tdevm_free_irq(&op->dev, irq, phy);\n\t\t}\n\t}\n\n\tdma_async_device_unregister(&pdev->device);\n\treturn 0;\n}\n\nstatic int mmp_pdma_chan_init(struct mmp_pdma_device *pdev, int idx, int irq)\n{\n\tstruct mmp_pdma_phy *phy  = &pdev->phy[idx];\n\tstruct mmp_pdma_chan *chan;\n\tint ret;\n\n\tchan = devm_kzalloc(pdev->dev, sizeof(*chan), GFP_KERNEL);\n\tif (chan == NULL)\n\t\treturn -ENOMEM;\n\n\tphy->idx = idx;\n\tphy->base = pdev->base;\n\n\tif (irq) {\n\t\tret = devm_request_irq(pdev->dev, irq, mmp_pdma_chan_handler,\n\t\t\t\t       IRQF_SHARED, \"pdma\", phy);\n\t\tif (ret) {\n\t\t\tdev_err(pdev->dev, \"channel request irq fail!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tspin_lock_init(&chan->desc_lock);\n\tchan->dev = pdev->dev;\n\tchan->chan.device = &pdev->device;\n\ttasklet_setup(&chan->tasklet, dma_do_tasklet);\n\tINIT_LIST_HEAD(&chan->chain_pending);\n\tINIT_LIST_HEAD(&chan->chain_running);\n\n\t \n\tlist_add_tail(&chan->chan.device_node, &pdev->device.channels);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id mmp_pdma_dt_ids[] = {\n\t{ .compatible = \"marvell,pdma-1.0\", },\n\t{}\n};\nMODULE_DEVICE_TABLE(of, mmp_pdma_dt_ids);\n\nstatic struct dma_chan *mmp_pdma_dma_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t\t   struct of_dma *ofdma)\n{\n\tstruct mmp_pdma_device *d = ofdma->of_dma_data;\n\tstruct dma_chan *chan;\n\n\tchan = dma_get_any_slave_channel(&d->device);\n\tif (!chan)\n\t\treturn NULL;\n\n\tto_mmp_pdma_chan(chan)->drcmr = dma_spec->args[0];\n\n\treturn chan;\n}\n\nstatic int mmp_pdma_probe(struct platform_device *op)\n{\n\tstruct mmp_pdma_device *pdev;\n\tconst struct of_device_id *of_id;\n\tstruct mmp_dma_platdata *pdata = dev_get_platdata(&op->dev);\n\tint i, ret, irq = 0;\n\tint dma_channels = 0, irq_num = 0;\n\tconst enum dma_slave_buswidth widths =\n\t\tDMA_SLAVE_BUSWIDTH_1_BYTE   | DMA_SLAVE_BUSWIDTH_2_BYTES |\n\t\tDMA_SLAVE_BUSWIDTH_4_BYTES;\n\n\tpdev = devm_kzalloc(&op->dev, sizeof(*pdev), GFP_KERNEL);\n\tif (!pdev)\n\t\treturn -ENOMEM;\n\n\tpdev->dev = &op->dev;\n\n\tspin_lock_init(&pdev->phy_lock);\n\n\tpdev->base = devm_platform_ioremap_resource(op, 0);\n\tif (IS_ERR(pdev->base))\n\t\treturn PTR_ERR(pdev->base);\n\n\tof_id = of_match_device(mmp_pdma_dt_ids, pdev->dev);\n\tif (of_id) {\n\t\t \n\t\tif (of_property_read_u32(pdev->dev->of_node, \"dma-channels\",\n\t\t\t\t\t &dma_channels))\n\t\t\tof_property_read_u32(pdev->dev->of_node, \"#dma-channels\",\n\t\t\t\t\t     &dma_channels);\n\t} else if (pdata && pdata->dma_channels) {\n\t\tdma_channels = pdata->dma_channels;\n\t} else {\n\t\tdma_channels = 32;\t \n\t}\n\tpdev->dma_channels = dma_channels;\n\n\tfor (i = 0; i < dma_channels; i++) {\n\t\tif (platform_get_irq_optional(op, i) > 0)\n\t\t\tirq_num++;\n\t}\n\n\tpdev->phy = devm_kcalloc(pdev->dev, dma_channels, sizeof(*pdev->phy),\n\t\t\t\t GFP_KERNEL);\n\tif (pdev->phy == NULL)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pdev->device.channels);\n\n\tif (irq_num != dma_channels) {\n\t\t \n\t\tirq = platform_get_irq(op, 0);\n\t\tret = devm_request_irq(pdev->dev, irq, mmp_pdma_int_handler,\n\t\t\t\t       IRQF_SHARED, \"pdma\", pdev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < dma_channels; i++) {\n\t\tirq = (irq_num != dma_channels) ? 0 : platform_get_irq(op, i);\n\t\tret = mmp_pdma_chan_init(pdev, i, irq);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tdma_cap_set(DMA_SLAVE, pdev->device.cap_mask);\n\tdma_cap_set(DMA_MEMCPY, pdev->device.cap_mask);\n\tdma_cap_set(DMA_CYCLIC, pdev->device.cap_mask);\n\tdma_cap_set(DMA_PRIVATE, pdev->device.cap_mask);\n\tpdev->device.dev = &op->dev;\n\tpdev->device.device_alloc_chan_resources = mmp_pdma_alloc_chan_resources;\n\tpdev->device.device_free_chan_resources = mmp_pdma_free_chan_resources;\n\tpdev->device.device_tx_status = mmp_pdma_tx_status;\n\tpdev->device.device_prep_dma_memcpy = mmp_pdma_prep_memcpy;\n\tpdev->device.device_prep_slave_sg = mmp_pdma_prep_slave_sg;\n\tpdev->device.device_prep_dma_cyclic = mmp_pdma_prep_dma_cyclic;\n\tpdev->device.device_issue_pending = mmp_pdma_issue_pending;\n\tpdev->device.device_config = mmp_pdma_config;\n\tpdev->device.device_terminate_all = mmp_pdma_terminate_all;\n\tpdev->device.copy_align = DMAENGINE_ALIGN_8_BYTES;\n\tpdev->device.src_addr_widths = widths;\n\tpdev->device.dst_addr_widths = widths;\n\tpdev->device.directions = BIT(DMA_MEM_TO_DEV) | BIT(DMA_DEV_TO_MEM);\n\tpdev->device.residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR;\n\n\tif (pdev->dev->coherent_dma_mask)\n\t\tdma_set_mask(pdev->dev, pdev->dev->coherent_dma_mask);\n\telse\n\t\tdma_set_mask(pdev->dev, DMA_BIT_MASK(64));\n\n\tret = dma_async_device_register(&pdev->device);\n\tif (ret) {\n\t\tdev_err(pdev->device.dev, \"unable to register\\n\");\n\t\treturn ret;\n\t}\n\n\tif (op->dev.of_node) {\n\t\t \n\t\tret = of_dma_controller_register(op->dev.of_node,\n\t\t\t\t\t\t mmp_pdma_dma_xlate, pdev);\n\t\tif (ret < 0) {\n\t\t\tdev_err(&op->dev, \"of_dma_controller_register failed\\n\");\n\t\t\tdma_async_device_unregister(&pdev->device);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tplatform_set_drvdata(op, pdev);\n\tdev_info(pdev->device.dev, \"initialized %d channels\\n\", dma_channels);\n\treturn 0;\n}\n\nstatic const struct platform_device_id mmp_pdma_id_table[] = {\n\t{ \"mmp-pdma\", },\n\t{ },\n};\n\nstatic struct platform_driver mmp_pdma_driver = {\n\t.driver\t\t= {\n\t\t.name\t= \"mmp-pdma\",\n\t\t.of_match_table = mmp_pdma_dt_ids,\n\t},\n\t.id_table\t= mmp_pdma_id_table,\n\t.probe\t\t= mmp_pdma_probe,\n\t.remove\t\t= mmp_pdma_remove,\n};\n\nmodule_platform_driver(mmp_pdma_driver);\n\nMODULE_DESCRIPTION(\"MARVELL MMP Peripheral DMA Driver\");\nMODULE_AUTHOR(\"Marvell International Ltd.\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}