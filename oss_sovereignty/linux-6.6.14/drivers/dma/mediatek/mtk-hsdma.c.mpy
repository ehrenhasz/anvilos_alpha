{
  "module_name": "mtk-hsdma.c",
  "hash_id": "8e14634b6604732c80044dee5296b391ac82248fdf831a461f9cca80c7d3245c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/mediatek/mtk-hsdma.c",
  "human_readable_source": "\n\n\n \n\n#include <linux/bitops.h>\n#include <linux/clk.h>\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/iopoll.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/refcount.h>\n#include <linux/slab.h>\n\n#include \"../virt-dma.h\"\n\n#define MTK_HSDMA_USEC_POLL\t\t20\n#define MTK_HSDMA_TIMEOUT_POLL\t\t200000\n#define MTK_HSDMA_DMA_BUSWIDTHS\t\tBIT(DMA_SLAVE_BUSWIDTH_4_BYTES)\n\n \n#define MTK_HSDMA_NR_VCHANS\t\t3\n\n \n#define MTK_HSDMA_NR_MAX_PCHANS\t\t1\n\n \n \n#define MTK_DMA_SIZE\t\t\t64\n#define MTK_HSDMA_NEXT_DESP_IDX(x, y)\t(((x) + 1) & ((y) - 1))\n#define MTK_HSDMA_LAST_DESP_IDX(x, y)\t(((x) - 1) & ((y) - 1))\n#define MTK_HSDMA_MAX_LEN\t\t0x3f80\n#define MTK_HSDMA_ALIGN_SIZE\t\t4\n#define MTK_HSDMA_PLEN_MASK\t\t0x3fff\n#define MTK_HSDMA_DESC_PLEN(x)\t\t(((x) & MTK_HSDMA_PLEN_MASK) << 16)\n#define MTK_HSDMA_DESC_PLEN_GET(x)\t(((x) >> 16) & MTK_HSDMA_PLEN_MASK)\n\n \n#define MTK_HSDMA_TX_BASE\t\t0x0\n#define MTK_HSDMA_TX_CNT\t\t0x4\n#define MTK_HSDMA_TX_CPU\t\t0x8\n#define MTK_HSDMA_TX_DMA\t\t0xc\n#define MTK_HSDMA_RX_BASE\t\t0x100\n#define MTK_HSDMA_RX_CNT\t\t0x104\n#define MTK_HSDMA_RX_CPU\t\t0x108\n#define MTK_HSDMA_RX_DMA\t\t0x10c\n\n \n#define MTK_HSDMA_GLO\t\t\t0x204\n#define MTK_HSDMA_GLO_MULTI_DMA\t\tBIT(10)\n#define MTK_HSDMA_TX_WB_DDONE\t\tBIT(6)\n#define MTK_HSDMA_BURST_64BYTES\t\t(0x2 << 4)\n#define MTK_HSDMA_GLO_RX_BUSY\t\tBIT(3)\n#define MTK_HSDMA_GLO_RX_DMA\t\tBIT(2)\n#define MTK_HSDMA_GLO_TX_BUSY\t\tBIT(1)\n#define MTK_HSDMA_GLO_TX_DMA\t\tBIT(0)\n#define MTK_HSDMA_GLO_DMA\t\t(MTK_HSDMA_GLO_TX_DMA |\t\\\n\t\t\t\t\t MTK_HSDMA_GLO_RX_DMA)\n#define MTK_HSDMA_GLO_BUSY\t\t(MTK_HSDMA_GLO_RX_BUSY | \\\n\t\t\t\t\t MTK_HSDMA_GLO_TX_BUSY)\n#define MTK_HSDMA_GLO_DEFAULT\t\t(MTK_HSDMA_GLO_TX_DMA | \\\n\t\t\t\t\t MTK_HSDMA_GLO_RX_DMA | \\\n\t\t\t\t\t MTK_HSDMA_TX_WB_DDONE | \\\n\t\t\t\t\t MTK_HSDMA_BURST_64BYTES | \\\n\t\t\t\t\t MTK_HSDMA_GLO_MULTI_DMA)\n\n \n#define MTK_HSDMA_RESET\t\t\t0x208\n#define MTK_HSDMA_RST_TX\t\tBIT(0)\n#define MTK_HSDMA_RST_RX\t\tBIT(16)\n\n \n#define MTK_HSDMA_DLYINT\t\t0x20c\n#define MTK_HSDMA_RXDLY_INT_EN\t\tBIT(15)\n\n \n#define MTK_HSDMA_RXMAX_PINT(x)\t\t(((x) & 0x7f) << 8)\n\n \n#define MTK_HSDMA_RXMAX_PTIME(x)\t((x) & 0x7f)\n#define MTK_HSDMA_DLYINT_DEFAULT\t(MTK_HSDMA_RXDLY_INT_EN | \\\n\t\t\t\t\t MTK_HSDMA_RXMAX_PINT(20) | \\\n\t\t\t\t\t MTK_HSDMA_RXMAX_PTIME(20))\n#define MTK_HSDMA_INT_STATUS\t\t0x220\n#define MTK_HSDMA_INT_ENABLE\t\t0x228\n#define MTK_HSDMA_INT_RXDONE\t\tBIT(16)\n\nenum mtk_hsdma_vdesc_flag {\n\tMTK_HSDMA_VDESC_FINISHED\t= 0x01,\n};\n\n#define IS_MTK_HSDMA_VDESC_FINISHED(x) ((x) == MTK_HSDMA_VDESC_FINISHED)\n\n \nstruct mtk_hsdma_pdesc {\n\t__le32 desc1;\n\t__le32 desc2;\n\t__le32 desc3;\n\t__le32 desc4;\n} __packed __aligned(4);\n\n \nstruct mtk_hsdma_vdesc {\n\tstruct virt_dma_desc vd;\n\tsize_t len;\n\tsize_t residue;\n\tdma_addr_t dest;\n\tdma_addr_t src;\n};\n\n \nstruct mtk_hsdma_cb {\n\tstruct virt_dma_desc *vd;\n\tenum mtk_hsdma_vdesc_flag flag;\n};\n\n \nstruct mtk_hsdma_ring {\n\tstruct mtk_hsdma_pdesc *txd;\n\tstruct mtk_hsdma_pdesc *rxd;\n\tstruct mtk_hsdma_cb *cb;\n\tdma_addr_t tphys;\n\tdma_addr_t rphys;\n\tu16 cur_tptr;\n\tu16 cur_rptr;\n};\n\n \nstruct mtk_hsdma_pchan {\n\tstruct mtk_hsdma_ring ring;\n\tsize_t sz_ring;\n\tatomic_t nr_free;\n};\n\n \nstruct mtk_hsdma_vchan {\n\tstruct virt_dma_chan vc;\n\tstruct completion issue_completion;\n\tbool issue_synchronize;\n\tstruct list_head desc_hw_processing;\n};\n\n \nstruct mtk_hsdma_soc {\n\t__le32 ddone;\n\t__le32 ls0;\n};\n\n \nstruct mtk_hsdma_device {\n\tstruct dma_device ddev;\n\tvoid __iomem *base;\n\tstruct clk *clk;\n\tu32 irq;\n\n\tu32 dma_requests;\n\tstruct mtk_hsdma_vchan *vc;\n\tstruct mtk_hsdma_pchan *pc;\n\trefcount_t pc_refcnt;\n\n\t \n\tspinlock_t lock;\n\n\tconst struct mtk_hsdma_soc *soc;\n};\n\nstatic struct mtk_hsdma_device *to_hsdma_dev(struct dma_chan *chan)\n{\n\treturn container_of(chan->device, struct mtk_hsdma_device, ddev);\n}\n\nstatic inline struct mtk_hsdma_vchan *to_hsdma_vchan(struct dma_chan *chan)\n{\n\treturn container_of(chan, struct mtk_hsdma_vchan, vc.chan);\n}\n\nstatic struct mtk_hsdma_vdesc *to_hsdma_vdesc(struct virt_dma_desc *vd)\n{\n\treturn container_of(vd, struct mtk_hsdma_vdesc, vd);\n}\n\nstatic struct device *hsdma2dev(struct mtk_hsdma_device *hsdma)\n{\n\treturn hsdma->ddev.dev;\n}\n\nstatic u32 mtk_dma_read(struct mtk_hsdma_device *hsdma, u32 reg)\n{\n\treturn readl(hsdma->base + reg);\n}\n\nstatic void mtk_dma_write(struct mtk_hsdma_device *hsdma, u32 reg, u32 val)\n{\n\twritel(val, hsdma->base + reg);\n}\n\nstatic void mtk_dma_rmw(struct mtk_hsdma_device *hsdma, u32 reg,\n\t\t\tu32 mask, u32 set)\n{\n\tu32 val;\n\n\tval = mtk_dma_read(hsdma, reg);\n\tval &= ~mask;\n\tval |= set;\n\tmtk_dma_write(hsdma, reg, val);\n}\n\nstatic void mtk_dma_set(struct mtk_hsdma_device *hsdma, u32 reg, u32 val)\n{\n\tmtk_dma_rmw(hsdma, reg, 0, val);\n}\n\nstatic void mtk_dma_clr(struct mtk_hsdma_device *hsdma, u32 reg, u32 val)\n{\n\tmtk_dma_rmw(hsdma, reg, val, 0);\n}\n\nstatic void mtk_hsdma_vdesc_free(struct virt_dma_desc *vd)\n{\n\tkfree(container_of(vd, struct mtk_hsdma_vdesc, vd));\n}\n\nstatic int mtk_hsdma_busy_wait(struct mtk_hsdma_device *hsdma)\n{\n\tu32 status = 0;\n\n\treturn readl_poll_timeout(hsdma->base + MTK_HSDMA_GLO, status,\n\t\t\t\t  !(status & MTK_HSDMA_GLO_BUSY),\n\t\t\t\t  MTK_HSDMA_USEC_POLL,\n\t\t\t\t  MTK_HSDMA_TIMEOUT_POLL);\n}\n\nstatic int mtk_hsdma_alloc_pchan(struct mtk_hsdma_device *hsdma,\n\t\t\t\t struct mtk_hsdma_pchan *pc)\n{\n\tstruct mtk_hsdma_ring *ring = &pc->ring;\n\tint err;\n\n\tmemset(pc, 0, sizeof(*pc));\n\n\t \n\tpc->sz_ring = 2 * MTK_DMA_SIZE * sizeof(*ring->txd);\n\tring->txd = dma_alloc_coherent(hsdma2dev(hsdma), pc->sz_ring,\n\t\t\t\t       &ring->tphys, GFP_NOWAIT);\n\tif (!ring->txd)\n\t\treturn -ENOMEM;\n\n\tring->rxd = &ring->txd[MTK_DMA_SIZE];\n\tring->rphys = ring->tphys + MTK_DMA_SIZE * sizeof(*ring->txd);\n\tring->cur_tptr = 0;\n\tring->cur_rptr = MTK_DMA_SIZE - 1;\n\n\tring->cb = kcalloc(MTK_DMA_SIZE, sizeof(*ring->cb), GFP_NOWAIT);\n\tif (!ring->cb) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_dma;\n\t}\n\n\tatomic_set(&pc->nr_free, MTK_DMA_SIZE - 1);\n\n\t \n\tmtk_dma_clr(hsdma, MTK_HSDMA_GLO, MTK_HSDMA_GLO_DMA);\n\terr = mtk_hsdma_busy_wait(hsdma);\n\tif (err)\n\t\tgoto err_free_cb;\n\n\t \n\tmtk_dma_set(hsdma, MTK_HSDMA_RESET,\n\t\t    MTK_HSDMA_RST_TX | MTK_HSDMA_RST_RX);\n\tmtk_dma_clr(hsdma, MTK_HSDMA_RESET,\n\t\t    MTK_HSDMA_RST_TX | MTK_HSDMA_RST_RX);\n\n\t \n\tmtk_dma_write(hsdma, MTK_HSDMA_TX_BASE, ring->tphys);\n\tmtk_dma_write(hsdma, MTK_HSDMA_TX_CNT, MTK_DMA_SIZE);\n\tmtk_dma_write(hsdma, MTK_HSDMA_TX_CPU, ring->cur_tptr);\n\tmtk_dma_write(hsdma, MTK_HSDMA_TX_DMA, 0);\n\tmtk_dma_write(hsdma, MTK_HSDMA_RX_BASE, ring->rphys);\n\tmtk_dma_write(hsdma, MTK_HSDMA_RX_CNT, MTK_DMA_SIZE);\n\tmtk_dma_write(hsdma, MTK_HSDMA_RX_CPU, ring->cur_rptr);\n\tmtk_dma_write(hsdma, MTK_HSDMA_RX_DMA, 0);\n\n\t \n\tmtk_dma_set(hsdma, MTK_HSDMA_GLO, MTK_HSDMA_GLO_DMA);\n\n\t \n\tmtk_dma_write(hsdma, MTK_HSDMA_DLYINT, MTK_HSDMA_DLYINT_DEFAULT);\n\n\t \n\tmtk_dma_set(hsdma, MTK_HSDMA_INT_ENABLE, MTK_HSDMA_INT_RXDONE);\n\n\treturn 0;\n\nerr_free_cb:\n\tkfree(ring->cb);\n\nerr_free_dma:\n\tdma_free_coherent(hsdma2dev(hsdma),\n\t\t\t  pc->sz_ring, ring->txd, ring->tphys);\n\treturn err;\n}\n\nstatic void mtk_hsdma_free_pchan(struct mtk_hsdma_device *hsdma,\n\t\t\t\t struct mtk_hsdma_pchan *pc)\n{\n\tstruct mtk_hsdma_ring *ring = &pc->ring;\n\n\t \n\tmtk_dma_clr(hsdma, MTK_HSDMA_GLO, MTK_HSDMA_GLO_DMA);\n\tmtk_hsdma_busy_wait(hsdma);\n\n\t \n\tmtk_dma_clr(hsdma, MTK_HSDMA_INT_ENABLE, MTK_HSDMA_INT_RXDONE);\n\tmtk_dma_write(hsdma, MTK_HSDMA_TX_BASE, 0);\n\tmtk_dma_write(hsdma, MTK_HSDMA_TX_CNT, 0);\n\tmtk_dma_write(hsdma, MTK_HSDMA_TX_CPU, 0);\n\tmtk_dma_write(hsdma, MTK_HSDMA_RX_BASE, 0);\n\tmtk_dma_write(hsdma, MTK_HSDMA_RX_CNT, 0);\n\tmtk_dma_write(hsdma, MTK_HSDMA_RX_CPU, MTK_DMA_SIZE - 1);\n\n\tkfree(ring->cb);\n\n\tdma_free_coherent(hsdma2dev(hsdma),\n\t\t\t  pc->sz_ring, ring->txd, ring->tphys);\n}\n\nstatic int mtk_hsdma_issue_pending_vdesc(struct mtk_hsdma_device *hsdma,\n\t\t\t\t\t struct mtk_hsdma_pchan *pc,\n\t\t\t\t\t struct mtk_hsdma_vdesc *hvd)\n{\n\tstruct mtk_hsdma_ring *ring = &pc->ring;\n\tstruct mtk_hsdma_pdesc *txd, *rxd;\n\tu16 reserved, prev, tlen, num_sgs;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&hsdma->lock, flags);\n\n\t \n\tnum_sgs = DIV_ROUND_UP(hvd->len, MTK_HSDMA_MAX_LEN);\n\treserved = min_t(u16, num_sgs, atomic_read(&pc->nr_free));\n\n\tif (!reserved) {\n\t\tspin_unlock_irqrestore(&hsdma->lock, flags);\n\t\treturn -ENOSPC;\n\t}\n\n\tatomic_sub(reserved, &pc->nr_free);\n\n\twhile (reserved--) {\n\t\t \n\t\ttlen = (hvd->len > MTK_HSDMA_MAX_LEN) ?\n\t\t       MTK_HSDMA_MAX_LEN : hvd->len;\n\n\t\t \n\t\ttxd = &ring->txd[ring->cur_tptr];\n\t\tWRITE_ONCE(txd->desc1, hvd->src);\n\t\tWRITE_ONCE(txd->desc2,\n\t\t\t   hsdma->soc->ls0 | MTK_HSDMA_DESC_PLEN(tlen));\n\n\t\trxd = &ring->rxd[ring->cur_tptr];\n\t\tWRITE_ONCE(rxd->desc1, hvd->dest);\n\t\tWRITE_ONCE(rxd->desc2, MTK_HSDMA_DESC_PLEN(tlen));\n\n\t\t \n\t\tring->cb[ring->cur_tptr].vd = &hvd->vd;\n\n\t\t \n\t\tring->cur_tptr = MTK_HSDMA_NEXT_DESP_IDX(ring->cur_tptr,\n\t\t\t\t\t\t\t MTK_DMA_SIZE);\n\n\t\t \n\t\thvd->src  += tlen;\n\t\thvd->dest += tlen;\n\t\thvd->len  -= tlen;\n\t}\n\n\t \n\tif (!hvd->len) {\n\t\tprev = MTK_HSDMA_LAST_DESP_IDX(ring->cur_tptr, MTK_DMA_SIZE);\n\t\tring->cb[prev].flag = MTK_HSDMA_VDESC_FINISHED;\n\t}\n\n\t \n\twmb();\n\n\t \n\tmtk_dma_write(hsdma, MTK_HSDMA_TX_CPU, ring->cur_tptr);\n\n\tspin_unlock_irqrestore(&hsdma->lock, flags);\n\n\treturn 0;\n}\n\nstatic void mtk_hsdma_issue_vchan_pending(struct mtk_hsdma_device *hsdma,\n\t\t\t\t\t  struct mtk_hsdma_vchan *hvc)\n{\n\tstruct virt_dma_desc *vd, *vd2;\n\tint err;\n\n\tlockdep_assert_held(&hvc->vc.lock);\n\n\tlist_for_each_entry_safe(vd, vd2, &hvc->vc.desc_issued, node) {\n\t\tstruct mtk_hsdma_vdesc *hvd;\n\n\t\thvd = to_hsdma_vdesc(vd);\n\n\t\t \n\t\terr = mtk_hsdma_issue_pending_vdesc(hsdma, hsdma->pc, hvd);\n\n\t\t \n\t\tif (err == -ENOSPC || hvd->len > 0)\n\t\t\tbreak;\n\n\t\t \n\t\tlist_move_tail(&vd->node, &hvc->desc_hw_processing);\n\t}\n}\n\nstatic void mtk_hsdma_free_rooms_in_ring(struct mtk_hsdma_device *hsdma)\n{\n\tstruct mtk_hsdma_vchan *hvc;\n\tstruct mtk_hsdma_pdesc *rxd;\n\tstruct mtk_hsdma_vdesc *hvd;\n\tstruct mtk_hsdma_pchan *pc;\n\tstruct mtk_hsdma_cb *cb;\n\tint i = MTK_DMA_SIZE;\n\t__le32 desc2;\n\tu32 status;\n\tu16 next;\n\n\t \n\tstatus = mtk_dma_read(hsdma, MTK_HSDMA_INT_STATUS);\n\tif (unlikely(!(status & MTK_HSDMA_INT_RXDONE)))\n\t\tgoto rx_done;\n\n\tpc = hsdma->pc;\n\n\t \n\twhile (i--) {\n\t\tnext = MTK_HSDMA_NEXT_DESP_IDX(pc->ring.cur_rptr,\n\t\t\t\t\t       MTK_DMA_SIZE);\n\t\trxd = &pc->ring.rxd[next];\n\n\t\t \n\t\tdesc2 = READ_ONCE(rxd->desc2);\n\t\tif (!(desc2 & hsdma->soc->ddone))\n\t\t\tbreak;\n\n\t\tcb = &pc->ring.cb[next];\n\t\tif (unlikely(!cb->vd)) {\n\t\t\tdev_err(hsdma2dev(hsdma), \"cb->vd cannot be null\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\thvd = to_hsdma_vdesc(cb->vd);\n\t\thvd->residue -= MTK_HSDMA_DESC_PLEN_GET(rxd->desc2);\n\n\t\t \n\t\tif (IS_MTK_HSDMA_VDESC_FINISHED(cb->flag)) {\n\t\t\thvc = to_hsdma_vchan(cb->vd->tx.chan);\n\n\t\t\tspin_lock(&hvc->vc.lock);\n\n\t\t\t \n\t\t\tlist_del(&cb->vd->node);\n\n\t\t\t \n\t\t\tvchan_cookie_complete(cb->vd);\n\n\t\t\tif (hvc->issue_synchronize &&\n\t\t\t    list_empty(&hvc->desc_hw_processing)) {\n\t\t\t\tcomplete(&hvc->issue_completion);\n\t\t\t\thvc->issue_synchronize = false;\n\t\t\t}\n\t\t\tspin_unlock(&hvc->vc.lock);\n\n\t\t\tcb->flag = 0;\n\t\t}\n\n\t\tcb->vd = NULL;\n\n\t\t \n\t\tWRITE_ONCE(rxd->desc1, 0);\n\t\tWRITE_ONCE(rxd->desc2, 0);\n\t\tpc->ring.cur_rptr = next;\n\n\t\t \n\t\tatomic_inc(&pc->nr_free);\n\t}\n\n\t \n\twmb();\n\n\t \n\tmtk_dma_write(hsdma, MTK_HSDMA_RX_CPU, pc->ring.cur_rptr);\n\n\t \n\tif (atomic_read(&pc->nr_free) >= MTK_DMA_SIZE - 1)\n\t\tmtk_dma_write(hsdma, MTK_HSDMA_INT_STATUS, status);\n\n\t \n\tfor (i = 0; i < hsdma->dma_requests; i++) {\n\t\thvc = &hsdma->vc[i];\n\t\tspin_lock(&hvc->vc.lock);\n\t\tmtk_hsdma_issue_vchan_pending(hsdma, hvc);\n\t\tspin_unlock(&hvc->vc.lock);\n\t}\n\nrx_done:\n\t \n\tmtk_dma_set(hsdma, MTK_HSDMA_INT_ENABLE, MTK_HSDMA_INT_RXDONE);\n}\n\nstatic irqreturn_t mtk_hsdma_irq(int irq, void *devid)\n{\n\tstruct mtk_hsdma_device *hsdma = devid;\n\n\t \n\tmtk_dma_clr(hsdma, MTK_HSDMA_INT_ENABLE, MTK_HSDMA_INT_RXDONE);\n\n\tmtk_hsdma_free_rooms_in_ring(hsdma);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic struct virt_dma_desc *mtk_hsdma_find_active_desc(struct dma_chan *c,\n\t\t\t\t\t\t\tdma_cookie_t cookie)\n{\n\tstruct mtk_hsdma_vchan *hvc = to_hsdma_vchan(c);\n\tstruct virt_dma_desc *vd;\n\n\tlist_for_each_entry(vd, &hvc->desc_hw_processing, node)\n\t\tif (vd->tx.cookie == cookie)\n\t\t\treturn vd;\n\n\tlist_for_each_entry(vd, &hvc->vc.desc_issued, node)\n\t\tif (vd->tx.cookie == cookie)\n\t\t\treturn vd;\n\n\treturn NULL;\n}\n\nstatic enum dma_status mtk_hsdma_tx_status(struct dma_chan *c,\n\t\t\t\t\t   dma_cookie_t cookie,\n\t\t\t\t\t   struct dma_tx_state *txstate)\n{\n\tstruct mtk_hsdma_vchan *hvc = to_hsdma_vchan(c);\n\tstruct mtk_hsdma_vdesc *hvd;\n\tstruct virt_dma_desc *vd;\n\tenum dma_status ret;\n\tunsigned long flags;\n\tsize_t bytes = 0;\n\n\tret = dma_cookie_status(c, cookie, txstate);\n\tif (ret == DMA_COMPLETE || !txstate)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&hvc->vc.lock, flags);\n\tvd = mtk_hsdma_find_active_desc(c, cookie);\n\tspin_unlock_irqrestore(&hvc->vc.lock, flags);\n\n\tif (vd) {\n\t\thvd = to_hsdma_vdesc(vd);\n\t\tbytes = hvd->residue;\n\t}\n\n\tdma_set_residue(txstate, bytes);\n\n\treturn ret;\n}\n\nstatic void mtk_hsdma_issue_pending(struct dma_chan *c)\n{\n\tstruct mtk_hsdma_device *hsdma = to_hsdma_dev(c);\n\tstruct mtk_hsdma_vchan *hvc = to_hsdma_vchan(c);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&hvc->vc.lock, flags);\n\n\tif (vchan_issue_pending(&hvc->vc))\n\t\tmtk_hsdma_issue_vchan_pending(hsdma, hvc);\n\n\tspin_unlock_irqrestore(&hvc->vc.lock, flags);\n}\n\nstatic struct dma_async_tx_descriptor *\nmtk_hsdma_prep_dma_memcpy(struct dma_chan *c, dma_addr_t dest,\n\t\t\t  dma_addr_t src, size_t len, unsigned long flags)\n{\n\tstruct mtk_hsdma_vdesc *hvd;\n\n\thvd = kzalloc(sizeof(*hvd), GFP_NOWAIT);\n\tif (!hvd)\n\t\treturn NULL;\n\n\thvd->len = len;\n\thvd->residue = len;\n\thvd->src = src;\n\thvd->dest = dest;\n\n\treturn vchan_tx_prep(to_virt_chan(c), &hvd->vd, flags);\n}\n\nstatic int mtk_hsdma_free_inactive_desc(struct dma_chan *c)\n{\n\tstruct virt_dma_chan *vc = to_virt_chan(c);\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&vc->lock, flags);\n\tlist_splice_tail_init(&vc->desc_allocated, &head);\n\tlist_splice_tail_init(&vc->desc_submitted, &head);\n\tlist_splice_tail_init(&vc->desc_issued, &head);\n\tspin_unlock_irqrestore(&vc->lock, flags);\n\n\t \n\tvchan_dma_desc_free_list(vc, &head);\n\n\treturn 0;\n}\n\nstatic void mtk_hsdma_free_active_desc(struct dma_chan *c)\n{\n\tstruct mtk_hsdma_vchan *hvc = to_hsdma_vchan(c);\n\tbool sync_needed = false;\n\n\t \n\tspin_lock(&hvc->vc.lock);\n\tif (!list_empty(&hvc->desc_hw_processing)) {\n\t\thvc->issue_synchronize = true;\n\t\tsync_needed = true;\n\t}\n\tspin_unlock(&hvc->vc.lock);\n\n\tif (sync_needed)\n\t\twait_for_completion(&hvc->issue_completion);\n\t \n\tWARN_ONCE(!list_empty(&hvc->desc_hw_processing),\n\t\t  \"Desc pending still in list desc_hw_processing\\n\");\n\n\t \n\tvchan_synchronize(&hvc->vc);\n\n\tWARN_ONCE(!list_empty(&hvc->vc.desc_completed),\n\t\t  \"Desc pending still in list desc_completed\\n\");\n}\n\nstatic int mtk_hsdma_terminate_all(struct dma_chan *c)\n{\n\t \n\tmtk_hsdma_free_inactive_desc(c);\n\n\t \n\tmtk_hsdma_free_active_desc(c);\n\n\treturn 0;\n}\n\nstatic int mtk_hsdma_alloc_chan_resources(struct dma_chan *c)\n{\n\tstruct mtk_hsdma_device *hsdma = to_hsdma_dev(c);\n\tint err;\n\n\t \n\tif (!refcount_read(&hsdma->pc_refcnt)) {\n\t\terr = mtk_hsdma_alloc_pchan(hsdma, hsdma->pc);\n\t\tif (err)\n\t\t\treturn err;\n\t\t \n\t\trefcount_set(&hsdma->pc_refcnt, 1);\n\t} else {\n\t\trefcount_inc(&hsdma->pc_refcnt);\n\t}\n\n\treturn 0;\n}\n\nstatic void mtk_hsdma_free_chan_resources(struct dma_chan *c)\n{\n\tstruct mtk_hsdma_device *hsdma = to_hsdma_dev(c);\n\n\t \n\tmtk_hsdma_terminate_all(c);\n\n\t \n\tif (!refcount_dec_and_test(&hsdma->pc_refcnt))\n\t\treturn;\n\n\tmtk_hsdma_free_pchan(hsdma, hsdma->pc);\n}\n\nstatic int mtk_hsdma_hw_init(struct mtk_hsdma_device *hsdma)\n{\n\tint err;\n\n\tpm_runtime_enable(hsdma2dev(hsdma));\n\tpm_runtime_get_sync(hsdma2dev(hsdma));\n\n\terr = clk_prepare_enable(hsdma->clk);\n\tif (err)\n\t\treturn err;\n\n\tmtk_dma_write(hsdma, MTK_HSDMA_INT_ENABLE, 0);\n\tmtk_dma_write(hsdma, MTK_HSDMA_GLO, MTK_HSDMA_GLO_DEFAULT);\n\n\treturn 0;\n}\n\nstatic int mtk_hsdma_hw_deinit(struct mtk_hsdma_device *hsdma)\n{\n\tmtk_dma_write(hsdma, MTK_HSDMA_GLO, 0);\n\n\tclk_disable_unprepare(hsdma->clk);\n\n\tpm_runtime_put_sync(hsdma2dev(hsdma));\n\tpm_runtime_disable(hsdma2dev(hsdma));\n\n\treturn 0;\n}\n\nstatic const struct mtk_hsdma_soc mt7623_soc = {\n\t.ddone = BIT(31),\n\t.ls0 = BIT(30),\n};\n\nstatic const struct mtk_hsdma_soc mt7622_soc = {\n\t.ddone = BIT(15),\n\t.ls0 = BIT(14),\n};\n\nstatic const struct of_device_id mtk_hsdma_match[] = {\n\t{ .compatible = \"mediatek,mt7623-hsdma\", .data = &mt7623_soc},\n\t{ .compatible = \"mediatek,mt7622-hsdma\", .data = &mt7622_soc},\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, mtk_hsdma_match);\n\nstatic int mtk_hsdma_probe(struct platform_device *pdev)\n{\n\tstruct mtk_hsdma_device *hsdma;\n\tstruct mtk_hsdma_vchan *vc;\n\tstruct dma_device *dd;\n\tint i, err;\n\n\thsdma = devm_kzalloc(&pdev->dev, sizeof(*hsdma), GFP_KERNEL);\n\tif (!hsdma)\n\t\treturn -ENOMEM;\n\n\tdd = &hsdma->ddev;\n\n\thsdma->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(hsdma->base))\n\t\treturn PTR_ERR(hsdma->base);\n\n\thsdma->soc = of_device_get_match_data(&pdev->dev);\n\tif (!hsdma->soc) {\n\t\tdev_err(&pdev->dev, \"No device match found\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\thsdma->clk = devm_clk_get(&pdev->dev, \"hsdma\");\n\tif (IS_ERR(hsdma->clk)) {\n\t\tdev_err(&pdev->dev, \"No clock for %s\\n\",\n\t\t\tdev_name(&pdev->dev));\n\t\treturn PTR_ERR(hsdma->clk);\n\t}\n\n\terr = platform_get_irq(pdev, 0);\n\tif (err < 0)\n\t\treturn err;\n\thsdma->irq = err;\n\n\trefcount_set(&hsdma->pc_refcnt, 0);\n\tspin_lock_init(&hsdma->lock);\n\n\tdma_cap_set(DMA_MEMCPY, dd->cap_mask);\n\n\tdd->copy_align = MTK_HSDMA_ALIGN_SIZE;\n\tdd->device_alloc_chan_resources = mtk_hsdma_alloc_chan_resources;\n\tdd->device_free_chan_resources = mtk_hsdma_free_chan_resources;\n\tdd->device_tx_status = mtk_hsdma_tx_status;\n\tdd->device_issue_pending = mtk_hsdma_issue_pending;\n\tdd->device_prep_dma_memcpy = mtk_hsdma_prep_dma_memcpy;\n\tdd->device_terminate_all = mtk_hsdma_terminate_all;\n\tdd->src_addr_widths = MTK_HSDMA_DMA_BUSWIDTHS;\n\tdd->dst_addr_widths = MTK_HSDMA_DMA_BUSWIDTHS;\n\tdd->directions = BIT(DMA_MEM_TO_MEM);\n\tdd->residue_granularity = DMA_RESIDUE_GRANULARITY_SEGMENT;\n\tdd->dev = &pdev->dev;\n\tINIT_LIST_HEAD(&dd->channels);\n\n\thsdma->dma_requests = MTK_HSDMA_NR_VCHANS;\n\tif (pdev->dev.of_node && of_property_read_u32(pdev->dev.of_node,\n\t\t\t\t\t\t      \"dma-requests\",\n\t\t\t\t\t\t      &hsdma->dma_requests)) {\n\t\tdev_info(&pdev->dev,\n\t\t\t \"Using %u as missing dma-requests property\\n\",\n\t\t\t MTK_HSDMA_NR_VCHANS);\n\t}\n\n\thsdma->pc = devm_kcalloc(&pdev->dev, MTK_HSDMA_NR_MAX_PCHANS,\n\t\t\t\t sizeof(*hsdma->pc), GFP_KERNEL);\n\tif (!hsdma->pc)\n\t\treturn -ENOMEM;\n\n\thsdma->vc = devm_kcalloc(&pdev->dev, hsdma->dma_requests,\n\t\t\t\t sizeof(*hsdma->vc), GFP_KERNEL);\n\tif (!hsdma->vc)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < hsdma->dma_requests; i++) {\n\t\tvc = &hsdma->vc[i];\n\t\tvc->vc.desc_free = mtk_hsdma_vdesc_free;\n\t\tvchan_init(&vc->vc, dd);\n\t\tinit_completion(&vc->issue_completion);\n\t\tINIT_LIST_HEAD(&vc->desc_hw_processing);\n\t}\n\n\terr = dma_async_device_register(dd);\n\tif (err)\n\t\treturn err;\n\n\terr = of_dma_controller_register(pdev->dev.of_node,\n\t\t\t\t\t of_dma_xlate_by_chan_id, hsdma);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"MediaTek HSDMA OF registration failed %d\\n\", err);\n\t\tgoto err_unregister;\n\t}\n\n\tmtk_hsdma_hw_init(hsdma);\n\n\terr = devm_request_irq(&pdev->dev, hsdma->irq,\n\t\t\t       mtk_hsdma_irq, 0,\n\t\t\t       dev_name(&pdev->dev), hsdma);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"request_irq failed with err %d\\n\", err);\n\t\tgoto err_free;\n\t}\n\n\tplatform_set_drvdata(pdev, hsdma);\n\n\tdev_info(&pdev->dev, \"MediaTek HSDMA driver registered\\n\");\n\n\treturn 0;\n\nerr_free:\n\tmtk_hsdma_hw_deinit(hsdma);\n\tof_dma_controller_free(pdev->dev.of_node);\nerr_unregister:\n\tdma_async_device_unregister(dd);\n\n\treturn err;\n}\n\nstatic int mtk_hsdma_remove(struct platform_device *pdev)\n{\n\tstruct mtk_hsdma_device *hsdma = platform_get_drvdata(pdev);\n\tstruct mtk_hsdma_vchan *vc;\n\tint i;\n\n\t \n\tfor (i = 0; i < hsdma->dma_requests; i++) {\n\t\tvc = &hsdma->vc[i];\n\n\t\tlist_del(&vc->vc.chan.device_node);\n\t\ttasklet_kill(&vc->vc.task);\n\t}\n\n\t \n\tmtk_dma_write(hsdma, MTK_HSDMA_INT_ENABLE, 0);\n\n\t \n\tsynchronize_irq(hsdma->irq);\n\n\t \n\tmtk_hsdma_hw_deinit(hsdma);\n\n\tdma_async_device_unregister(&hsdma->ddev);\n\tof_dma_controller_free(pdev->dev.of_node);\n\n\treturn 0;\n}\n\nstatic struct platform_driver mtk_hsdma_driver = {\n\t.probe\t\t= mtk_hsdma_probe,\n\t.remove\t\t= mtk_hsdma_remove,\n\t.driver = {\n\t\t.name\t\t= KBUILD_MODNAME,\n\t\t.of_match_table\t= mtk_hsdma_match,\n\t},\n};\nmodule_platform_driver(mtk_hsdma_driver);\n\nMODULE_DESCRIPTION(\"MediaTek High-Speed DMA Controller Driver\");\nMODULE_AUTHOR(\"Sean Wang <sean.wang@mediatek.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}