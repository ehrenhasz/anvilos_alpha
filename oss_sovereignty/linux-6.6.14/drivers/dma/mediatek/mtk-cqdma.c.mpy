{
  "module_name": "mtk-cqdma.c",
  "hash_id": "2071fcbc3b36ea2b15238d734cc75fb1fe3d7f0301d4f706cde4e46213736206",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/mediatek/mtk-cqdma.c",
  "human_readable_source": "\n\n\n \n\n#include <linux/bitops.h>\n#include <linux/clk.h>\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/iopoll.h>\n#include <linux/interrupt.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/refcount.h>\n#include <linux/slab.h>\n\n#include \"../virt-dma.h\"\n\n#define MTK_CQDMA_USEC_POLL\t\t10\n#define MTK_CQDMA_TIMEOUT_POLL\t\t1000\n#define MTK_CQDMA_DMA_BUSWIDTHS\t\tBIT(DMA_SLAVE_BUSWIDTH_4_BYTES)\n#define MTK_CQDMA_ALIGN_SIZE\t\t1\n\n \n#define MTK_CQDMA_NR_VCHANS\t\t32\n\n \n#define MTK_CQDMA_NR_PCHANS\t\t3\n\n \n#define MTK_CQDMA_INT_FLAG\t\t0x0\n#define MTK_CQDMA_INT_EN\t\t0x4\n#define MTK_CQDMA_EN\t\t\t0x8\n#define MTK_CQDMA_RESET\t\t\t0xc\n#define MTK_CQDMA_FLUSH\t\t\t0x14\n#define MTK_CQDMA_SRC\t\t\t0x1c\n#define MTK_CQDMA_DST\t\t\t0x20\n#define MTK_CQDMA_LEN1\t\t\t0x24\n#define MTK_CQDMA_LEN2\t\t\t0x28\n#define MTK_CQDMA_SRC2\t\t\t0x60\n#define MTK_CQDMA_DST2\t\t\t0x64\n\n \n#define MTK_CQDMA_EN_BIT\t\tBIT(0)\n#define MTK_CQDMA_INT_FLAG_BIT\t\tBIT(0)\n#define MTK_CQDMA_INT_EN_BIT\t\tBIT(0)\n#define MTK_CQDMA_FLUSH_BIT\t\tBIT(0)\n\n#define MTK_CQDMA_WARM_RST_BIT\t\tBIT(0)\n#define MTK_CQDMA_HARD_RST_BIT\t\tBIT(1)\n\n#define MTK_CQDMA_MAX_LEN\t\tGENMASK(27, 0)\n#define MTK_CQDMA_ADDR_LIMIT\t\tGENMASK(31, 0)\n#define MTK_CQDMA_ADDR2_SHFIT\t\t(32)\n\n \nstruct mtk_cqdma_vdesc {\n\tstruct virt_dma_desc vd;\n\tsize_t len;\n\tsize_t residue;\n\tdma_addr_t dest;\n\tdma_addr_t src;\n\tstruct dma_chan *ch;\n\n\tstruct list_head node;\n\tstruct mtk_cqdma_vdesc *parent;\n};\n\n \nstruct mtk_cqdma_pchan {\n\tstruct list_head queue;\n\tvoid __iomem *base;\n\tu32 irq;\n\n\trefcount_t refcnt;\n\n\tstruct tasklet_struct tasklet;\n\n\t \n\tspinlock_t lock;\n};\n\n \nstruct mtk_cqdma_vchan {\n\tstruct virt_dma_chan vc;\n\tstruct mtk_cqdma_pchan *pc;\n\tstruct completion issue_completion;\n\tbool issue_synchronize;\n};\n\n \nstruct mtk_cqdma_device {\n\tstruct dma_device ddev;\n\tstruct clk *clk;\n\n\tu32 dma_requests;\n\tu32 dma_channels;\n\tstruct mtk_cqdma_vchan *vc;\n\tstruct mtk_cqdma_pchan **pc;\n};\n\nstatic struct mtk_cqdma_device *to_cqdma_dev(struct dma_chan *chan)\n{\n\treturn container_of(chan->device, struct mtk_cqdma_device, ddev);\n}\n\nstatic struct mtk_cqdma_vchan *to_cqdma_vchan(struct dma_chan *chan)\n{\n\treturn container_of(chan, struct mtk_cqdma_vchan, vc.chan);\n}\n\nstatic struct mtk_cqdma_vdesc *to_cqdma_vdesc(struct virt_dma_desc *vd)\n{\n\treturn container_of(vd, struct mtk_cqdma_vdesc, vd);\n}\n\nstatic struct device *cqdma2dev(struct mtk_cqdma_device *cqdma)\n{\n\treturn cqdma->ddev.dev;\n}\n\nstatic u32 mtk_dma_read(struct mtk_cqdma_pchan *pc, u32 reg)\n{\n\treturn readl(pc->base + reg);\n}\n\nstatic void mtk_dma_write(struct mtk_cqdma_pchan *pc, u32 reg, u32 val)\n{\n\twritel_relaxed(val, pc->base + reg);\n}\n\nstatic void mtk_dma_rmw(struct mtk_cqdma_pchan *pc, u32 reg,\n\t\t\tu32 mask, u32 set)\n{\n\tu32 val;\n\n\tval = mtk_dma_read(pc, reg);\n\tval &= ~mask;\n\tval |= set;\n\tmtk_dma_write(pc, reg, val);\n}\n\nstatic void mtk_dma_set(struct mtk_cqdma_pchan *pc, u32 reg, u32 val)\n{\n\tmtk_dma_rmw(pc, reg, 0, val);\n}\n\nstatic void mtk_dma_clr(struct mtk_cqdma_pchan *pc, u32 reg, u32 val)\n{\n\tmtk_dma_rmw(pc, reg, val, 0);\n}\n\nstatic void mtk_cqdma_vdesc_free(struct virt_dma_desc *vd)\n{\n\tkfree(to_cqdma_vdesc(vd));\n}\n\nstatic int mtk_cqdma_poll_engine_done(struct mtk_cqdma_pchan *pc, bool atomic)\n{\n\tu32 status = 0;\n\n\tif (!atomic)\n\t\treturn readl_poll_timeout(pc->base + MTK_CQDMA_EN,\n\t\t\t\t\t  status,\n\t\t\t\t\t  !(status & MTK_CQDMA_EN_BIT),\n\t\t\t\t\t  MTK_CQDMA_USEC_POLL,\n\t\t\t\t\t  MTK_CQDMA_TIMEOUT_POLL);\n\n\treturn readl_poll_timeout_atomic(pc->base + MTK_CQDMA_EN,\n\t\t\t\t\t status,\n\t\t\t\t\t !(status & MTK_CQDMA_EN_BIT),\n\t\t\t\t\t MTK_CQDMA_USEC_POLL,\n\t\t\t\t\t MTK_CQDMA_TIMEOUT_POLL);\n}\n\nstatic int mtk_cqdma_hard_reset(struct mtk_cqdma_pchan *pc)\n{\n\tmtk_dma_set(pc, MTK_CQDMA_RESET, MTK_CQDMA_HARD_RST_BIT);\n\tmtk_dma_clr(pc, MTK_CQDMA_RESET, MTK_CQDMA_HARD_RST_BIT);\n\n\treturn mtk_cqdma_poll_engine_done(pc, true);\n}\n\nstatic void mtk_cqdma_start(struct mtk_cqdma_pchan *pc,\n\t\t\t    struct mtk_cqdma_vdesc *cvd)\n{\n\t \n\tif (mtk_cqdma_poll_engine_done(pc, true) < 0)\n\t\tdev_err(cqdma2dev(to_cqdma_dev(cvd->ch)), \"cqdma wait transaction timeout\\n\");\n\n\t \n\tmtk_dma_set(pc, MTK_CQDMA_RESET, MTK_CQDMA_WARM_RST_BIT);\n\tif (mtk_cqdma_poll_engine_done(pc, true) < 0)\n\t\tdev_err(cqdma2dev(to_cqdma_dev(cvd->ch)), \"cqdma warm reset timeout\\n\");\n\n\t \n\tmtk_dma_set(pc, MTK_CQDMA_SRC, cvd->src & MTK_CQDMA_ADDR_LIMIT);\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\tmtk_dma_set(pc, MTK_CQDMA_SRC2, cvd->src >> MTK_CQDMA_ADDR2_SHFIT);\n#else\n\tmtk_dma_set(pc, MTK_CQDMA_SRC2, 0);\n#endif\n\n\t \n\tmtk_dma_set(pc, MTK_CQDMA_DST, cvd->dest & MTK_CQDMA_ADDR_LIMIT);\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\tmtk_dma_set(pc, MTK_CQDMA_DST2, cvd->dest >> MTK_CQDMA_ADDR2_SHFIT);\n#else\n\tmtk_dma_set(pc, MTK_CQDMA_DST2, 0);\n#endif\n\n\t \n\tmtk_dma_set(pc, MTK_CQDMA_LEN1, cvd->len);\n\n\t \n\tmtk_dma_set(pc, MTK_CQDMA_EN, MTK_CQDMA_EN_BIT);\n}\n\nstatic void mtk_cqdma_issue_vchan_pending(struct mtk_cqdma_vchan *cvc)\n{\n\tstruct virt_dma_desc *vd, *vd2;\n\tstruct mtk_cqdma_pchan *pc = cvc->pc;\n\tstruct mtk_cqdma_vdesc *cvd;\n\tbool trigger_engine = false;\n\n\tlockdep_assert_held(&cvc->vc.lock);\n\tlockdep_assert_held(&pc->lock);\n\n\tlist_for_each_entry_safe(vd, vd2, &cvc->vc.desc_issued, node) {\n\t\t \n\t\tif (list_empty(&pc->queue))\n\t\t\ttrigger_engine = true;\n\n\t\tcvd = to_cqdma_vdesc(vd);\n\n\t\t \n\t\tlist_add_tail(&cvd->node, &pc->queue);\n\n\t\t \n\t\tif (trigger_engine)\n\t\t\tmtk_cqdma_start(pc, cvd);\n\n\t\t \n\t\tlist_del(&vd->node);\n\t}\n}\n\n \nstatic bool mtk_cqdma_is_vchan_active(struct mtk_cqdma_vchan *cvc)\n{\n\tstruct mtk_cqdma_vdesc *cvd;\n\n\tlist_for_each_entry(cvd, &cvc->pc->queue, node)\n\t\tif (cvc == to_cqdma_vchan(cvd->ch))\n\t\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic struct mtk_cqdma_vdesc\n*mtk_cqdma_consume_work_queue(struct mtk_cqdma_pchan *pc)\n{\n\tstruct mtk_cqdma_vchan *cvc;\n\tstruct mtk_cqdma_vdesc *cvd, *ret = NULL;\n\n\t \n\tcvd = list_first_entry_or_null(&pc->queue,\n\t\t\t\t       struct mtk_cqdma_vdesc, node);\n\tif (unlikely(!cvd || !cvd->parent))\n\t\treturn NULL;\n\n\tcvc = to_cqdma_vchan(cvd->ch);\n\tret = cvd;\n\n\t \n\tcvd->parent->residue -= cvd->len;\n\n\t \n\tlist_del(&cvd->node);\n\n\tspin_lock(&cvc->vc.lock);\n\n\t \n\tif (!cvd->parent->residue) {\n\t\t \n\t\tvchan_cookie_complete(&cvd->parent->vd);\n\n\t\t \n\t\tif (cvc->issue_synchronize && !mtk_cqdma_is_vchan_active(cvc)) {\n\t\t\tcomplete(&cvc->issue_completion);\n\t\t\tcvc->issue_synchronize = false;\n\t\t}\n\t}\n\n\tspin_unlock(&cvc->vc.lock);\n\n\t \n\tcvd = list_first_entry_or_null(&pc->queue,\n\t\t\t\t       struct mtk_cqdma_vdesc, node);\n\tif (cvd)\n\t\tmtk_cqdma_start(pc, cvd);\n\n\treturn ret;\n}\n\nstatic void mtk_cqdma_tasklet_cb(struct tasklet_struct *t)\n{\n\tstruct mtk_cqdma_pchan *pc = from_tasklet(pc, t, tasklet);\n\tstruct mtk_cqdma_vdesc *cvd = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pc->lock, flags);\n\t \n\tcvd = mtk_cqdma_consume_work_queue(pc);\n\tspin_unlock_irqrestore(&pc->lock, flags);\n\n\t \n\tif (cvd) {\n\t\tdma_run_dependencies(&cvd->vd.tx);\n\n\t\t \n\t\tif (cvd->parent != cvd)\n\t\t\tkfree(cvd);\n\t}\n\n\t \n\tenable_irq(pc->irq);\n}\n\nstatic irqreturn_t mtk_cqdma_irq(int irq, void *devid)\n{\n\tstruct mtk_cqdma_device *cqdma = devid;\n\tirqreturn_t ret = IRQ_NONE;\n\tbool schedule_tasklet = false;\n\tu32 i;\n\n\t \n\tfor (i = 0; i < cqdma->dma_channels; ++i, schedule_tasklet = false) {\n\t\tspin_lock(&cqdma->pc[i]->lock);\n\t\tif (mtk_dma_read(cqdma->pc[i],\n\t\t\t\t MTK_CQDMA_INT_FLAG) & MTK_CQDMA_INT_FLAG_BIT) {\n\t\t\t \n\t\t\tmtk_dma_clr(cqdma->pc[i], MTK_CQDMA_INT_FLAG,\n\t\t\t\t    MTK_CQDMA_INT_FLAG_BIT);\n\n\t\t\tschedule_tasklet = true;\n\t\t\tret = IRQ_HANDLED;\n\t\t}\n\t\tspin_unlock(&cqdma->pc[i]->lock);\n\n\t\tif (schedule_tasklet) {\n\t\t\t \n\t\t\tdisable_irq_nosync(cqdma->pc[i]->irq);\n\n\t\t\t \n\t\t\ttasklet_schedule(&cqdma->pc[i]->tasklet);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic struct virt_dma_desc *mtk_cqdma_find_active_desc(struct dma_chan *c,\n\t\t\t\t\t\t\tdma_cookie_t cookie)\n{\n\tstruct mtk_cqdma_vchan *cvc = to_cqdma_vchan(c);\n\tstruct virt_dma_desc *vd;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cvc->pc->lock, flags);\n\tlist_for_each_entry(vd, &cvc->pc->queue, node)\n\t\tif (vd->tx.cookie == cookie) {\n\t\t\tspin_unlock_irqrestore(&cvc->pc->lock, flags);\n\t\t\treturn vd;\n\t\t}\n\tspin_unlock_irqrestore(&cvc->pc->lock, flags);\n\n\tlist_for_each_entry(vd, &cvc->vc.desc_issued, node)\n\t\tif (vd->tx.cookie == cookie)\n\t\t\treturn vd;\n\n\treturn NULL;\n}\n\nstatic enum dma_status mtk_cqdma_tx_status(struct dma_chan *c,\n\t\t\t\t\t   dma_cookie_t cookie,\n\t\t\t\t\t   struct dma_tx_state *txstate)\n{\n\tstruct mtk_cqdma_vchan *cvc = to_cqdma_vchan(c);\n\tstruct mtk_cqdma_vdesc *cvd;\n\tstruct virt_dma_desc *vd;\n\tenum dma_status ret;\n\tunsigned long flags;\n\tsize_t bytes = 0;\n\n\tret = dma_cookie_status(c, cookie, txstate);\n\tif (ret == DMA_COMPLETE || !txstate)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&cvc->vc.lock, flags);\n\tvd = mtk_cqdma_find_active_desc(c, cookie);\n\tspin_unlock_irqrestore(&cvc->vc.lock, flags);\n\n\tif (vd) {\n\t\tcvd = to_cqdma_vdesc(vd);\n\t\tbytes = cvd->residue;\n\t}\n\n\tdma_set_residue(txstate, bytes);\n\n\treturn ret;\n}\n\nstatic void mtk_cqdma_issue_pending(struct dma_chan *c)\n{\n\tstruct mtk_cqdma_vchan *cvc = to_cqdma_vchan(c);\n\tunsigned long pc_flags;\n\tunsigned long vc_flags;\n\n\t \n\tspin_lock_irqsave(&cvc->pc->lock, pc_flags);\n\tspin_lock_irqsave(&cvc->vc.lock, vc_flags);\n\n\tif (vchan_issue_pending(&cvc->vc))\n\t\tmtk_cqdma_issue_vchan_pending(cvc);\n\n\tspin_unlock_irqrestore(&cvc->vc.lock, vc_flags);\n\tspin_unlock_irqrestore(&cvc->pc->lock, pc_flags);\n}\n\nstatic struct dma_async_tx_descriptor *\nmtk_cqdma_prep_dma_memcpy(struct dma_chan *c, dma_addr_t dest,\n\t\t\t  dma_addr_t src, size_t len, unsigned long flags)\n{\n\tstruct mtk_cqdma_vdesc **cvd;\n\tstruct dma_async_tx_descriptor *tx = NULL, *prev_tx = NULL;\n\tsize_t i, tlen, nr_vd;\n\n\t \n\tnr_vd = DIV_ROUND_UP(len, MTK_CQDMA_MAX_LEN);\n\tcvd = kcalloc(nr_vd, sizeof(*cvd), GFP_NOWAIT);\n\tif (!cvd)\n\t\treturn NULL;\n\n\tfor (i = 0; i < nr_vd; ++i) {\n\t\tcvd[i] = kzalloc(sizeof(*cvd[i]), GFP_NOWAIT);\n\t\tif (!cvd[i]) {\n\t\t\tfor (; i > 0; --i)\n\t\t\t\tkfree(cvd[i - 1]);\n\t\t\treturn NULL;\n\t\t}\n\n\t\t \n\t\tcvd[i]->ch = c;\n\n\t\t \n\t\ttlen = (len > MTK_CQDMA_MAX_LEN) ? MTK_CQDMA_MAX_LEN : len;\n\t\tcvd[i]->len = tlen;\n\t\tcvd[i]->src = src;\n\t\tcvd[i]->dest = dest;\n\n\t\t \n\t\ttx = vchan_tx_prep(to_virt_chan(c), &cvd[i]->vd, flags);\n\t\ttx->next = NULL;\n\n\t\tif (!i) {\n\t\t\tcvd[0]->residue = len;\n\t\t} else {\n\t\t\tprev_tx->next = tx;\n\t\t\tcvd[i]->residue = tlen;\n\t\t}\n\n\t\tcvd[i]->parent = cvd[0];\n\n\t\t \n\t\tsrc += tlen;\n\t\tdest += tlen;\n\t\tlen -= tlen;\n\t\tprev_tx = tx;\n\t}\n\n\treturn &cvd[0]->vd.tx;\n}\n\nstatic void mtk_cqdma_free_inactive_desc(struct dma_chan *c)\n{\n\tstruct virt_dma_chan *vc = to_virt_chan(c);\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\t \n\tspin_lock_irqsave(&vc->lock, flags);\n\tlist_splice_tail_init(&vc->desc_allocated, &head);\n\tlist_splice_tail_init(&vc->desc_submitted, &head);\n\tlist_splice_tail_init(&vc->desc_issued, &head);\n\tspin_unlock_irqrestore(&vc->lock, flags);\n\n\t \n\tvchan_dma_desc_free_list(vc, &head);\n}\n\nstatic void mtk_cqdma_free_active_desc(struct dma_chan *c)\n{\n\tstruct mtk_cqdma_vchan *cvc = to_cqdma_vchan(c);\n\tbool sync_needed = false;\n\tunsigned long pc_flags;\n\tunsigned long vc_flags;\n\n\t \n\tspin_lock_irqsave(&cvc->pc->lock, pc_flags);\n\tspin_lock_irqsave(&cvc->vc.lock, vc_flags);\n\n\t \n\tif (mtk_cqdma_is_vchan_active(cvc)) {\n\t\tcvc->issue_synchronize = true;\n\t\tsync_needed = true;\n\t}\n\n\tspin_unlock_irqrestore(&cvc->vc.lock, vc_flags);\n\tspin_unlock_irqrestore(&cvc->pc->lock, pc_flags);\n\n\t \n\tif (sync_needed)\n\t\twait_for_completion(&cvc->issue_completion);\n\n\t \n\tvchan_synchronize(&cvc->vc);\n\n\tWARN_ONCE(!list_empty(&cvc->vc.desc_completed),\n\t\t  \"Desc pending still in list desc_completed\\n\");\n}\n\nstatic int mtk_cqdma_terminate_all(struct dma_chan *c)\n{\n\t \n\tmtk_cqdma_free_inactive_desc(c);\n\n\t \n\tmtk_cqdma_free_active_desc(c);\n\n\treturn 0;\n}\n\nstatic int mtk_cqdma_alloc_chan_resources(struct dma_chan *c)\n{\n\tstruct mtk_cqdma_device *cqdma = to_cqdma_dev(c);\n\tstruct mtk_cqdma_vchan *vc = to_cqdma_vchan(c);\n\tstruct mtk_cqdma_pchan *pc = NULL;\n\tu32 i, min_refcnt = U32_MAX, refcnt;\n\tunsigned long flags;\n\n\t \n\tfor (i = 0; i < cqdma->dma_channels; ++i) {\n\t\trefcnt = refcount_read(&cqdma->pc[i]->refcnt);\n\t\tif (refcnt < min_refcnt) {\n\t\t\tpc = cqdma->pc[i];\n\t\t\tmin_refcnt = refcnt;\n\t\t}\n\t}\n\n\tif (!pc)\n\t\treturn -ENOSPC;\n\n\tspin_lock_irqsave(&pc->lock, flags);\n\n\tif (!refcount_read(&pc->refcnt)) {\n\t\t \n\t\tmtk_cqdma_hard_reset(pc);\n\n\t\t \n\t\tmtk_dma_set(pc, MTK_CQDMA_INT_EN, MTK_CQDMA_INT_EN_BIT);\n\n\t\t \n\t\trefcount_set(&pc->refcnt, 1);\n\t} else {\n\t\trefcount_inc(&pc->refcnt);\n\t}\n\n\tspin_unlock_irqrestore(&pc->lock, flags);\n\n\tvc->pc = pc;\n\n\treturn 0;\n}\n\nstatic void mtk_cqdma_free_chan_resources(struct dma_chan *c)\n{\n\tstruct mtk_cqdma_vchan *cvc = to_cqdma_vchan(c);\n\tunsigned long flags;\n\n\t \n\tmtk_cqdma_terminate_all(c);\n\n\tspin_lock_irqsave(&cvc->pc->lock, flags);\n\n\t \n\tif (refcount_dec_and_test(&cvc->pc->refcnt)) {\n\t\t \n\t\tmtk_dma_set(cvc->pc, MTK_CQDMA_FLUSH, MTK_CQDMA_FLUSH_BIT);\n\n\t\t \n\t\tif (mtk_cqdma_poll_engine_done(cvc->pc, true) < 0)\n\t\t\tdev_err(cqdma2dev(to_cqdma_dev(c)), \"cqdma flush timeout\\n\");\n\n\t\t \n\t\tmtk_dma_clr(cvc->pc, MTK_CQDMA_FLUSH, MTK_CQDMA_FLUSH_BIT);\n\t\tmtk_dma_clr(cvc->pc, MTK_CQDMA_INT_FLAG,\n\t\t\t    MTK_CQDMA_INT_FLAG_BIT);\n\n\t\t \n\t\tmtk_dma_clr(cvc->pc, MTK_CQDMA_INT_EN, MTK_CQDMA_INT_EN_BIT);\n\t}\n\n\tspin_unlock_irqrestore(&cvc->pc->lock, flags);\n}\n\nstatic int mtk_cqdma_hw_init(struct mtk_cqdma_device *cqdma)\n{\n\tunsigned long flags;\n\tint err;\n\tu32 i;\n\n\tpm_runtime_enable(cqdma2dev(cqdma));\n\tpm_runtime_get_sync(cqdma2dev(cqdma));\n\n\terr = clk_prepare_enable(cqdma->clk);\n\n\tif (err) {\n\t\tpm_runtime_put_sync(cqdma2dev(cqdma));\n\t\tpm_runtime_disable(cqdma2dev(cqdma));\n\t\treturn err;\n\t}\n\n\t \n\tfor (i = 0; i < cqdma->dma_channels; ++i) {\n\t\tspin_lock_irqsave(&cqdma->pc[i]->lock, flags);\n\t\tif (mtk_cqdma_hard_reset(cqdma->pc[i]) < 0) {\n\t\t\tdev_err(cqdma2dev(cqdma), \"cqdma hard reset timeout\\n\");\n\t\t\tspin_unlock_irqrestore(&cqdma->pc[i]->lock, flags);\n\n\t\t\tclk_disable_unprepare(cqdma->clk);\n\t\t\tpm_runtime_put_sync(cqdma2dev(cqdma));\n\t\t\tpm_runtime_disable(cqdma2dev(cqdma));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tspin_unlock_irqrestore(&cqdma->pc[i]->lock, flags);\n\t}\n\n\treturn 0;\n}\n\nstatic void mtk_cqdma_hw_deinit(struct mtk_cqdma_device *cqdma)\n{\n\tunsigned long flags;\n\tu32 i;\n\n\t \n\tfor (i = 0; i < cqdma->dma_channels; ++i) {\n\t\tspin_lock_irqsave(&cqdma->pc[i]->lock, flags);\n\t\tif (mtk_cqdma_hard_reset(cqdma->pc[i]) < 0)\n\t\t\tdev_err(cqdma2dev(cqdma), \"cqdma hard reset timeout\\n\");\n\t\tspin_unlock_irqrestore(&cqdma->pc[i]->lock, flags);\n\t}\n\n\tclk_disable_unprepare(cqdma->clk);\n\n\tpm_runtime_put_sync(cqdma2dev(cqdma));\n\tpm_runtime_disable(cqdma2dev(cqdma));\n}\n\nstatic const struct of_device_id mtk_cqdma_match[] = {\n\t{ .compatible = \"mediatek,mt6765-cqdma\" },\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, mtk_cqdma_match);\n\nstatic int mtk_cqdma_probe(struct platform_device *pdev)\n{\n\tstruct mtk_cqdma_device *cqdma;\n\tstruct mtk_cqdma_vchan *vc;\n\tstruct dma_device *dd;\n\tint err;\n\tu32 i;\n\n\tcqdma = devm_kzalloc(&pdev->dev, sizeof(*cqdma), GFP_KERNEL);\n\tif (!cqdma)\n\t\treturn -ENOMEM;\n\n\tdd = &cqdma->ddev;\n\n\tcqdma->clk = devm_clk_get(&pdev->dev, \"cqdma\");\n\tif (IS_ERR(cqdma->clk)) {\n\t\tdev_err(&pdev->dev, \"No clock for %s\\n\",\n\t\t\tdev_name(&pdev->dev));\n\t\treturn PTR_ERR(cqdma->clk);\n\t}\n\n\tdma_cap_set(DMA_MEMCPY, dd->cap_mask);\n\n\tdd->copy_align = MTK_CQDMA_ALIGN_SIZE;\n\tdd->device_alloc_chan_resources = mtk_cqdma_alloc_chan_resources;\n\tdd->device_free_chan_resources = mtk_cqdma_free_chan_resources;\n\tdd->device_tx_status = mtk_cqdma_tx_status;\n\tdd->device_issue_pending = mtk_cqdma_issue_pending;\n\tdd->device_prep_dma_memcpy = mtk_cqdma_prep_dma_memcpy;\n\tdd->device_terminate_all = mtk_cqdma_terminate_all;\n\tdd->src_addr_widths = MTK_CQDMA_DMA_BUSWIDTHS;\n\tdd->dst_addr_widths = MTK_CQDMA_DMA_BUSWIDTHS;\n\tdd->directions = BIT(DMA_MEM_TO_MEM);\n\tdd->residue_granularity = DMA_RESIDUE_GRANULARITY_SEGMENT;\n\tdd->dev = &pdev->dev;\n\tINIT_LIST_HEAD(&dd->channels);\n\n\tif (pdev->dev.of_node && of_property_read_u32(pdev->dev.of_node,\n\t\t\t\t\t\t      \"dma-requests\",\n\t\t\t\t\t\t      &cqdma->dma_requests)) {\n\t\tdev_info(&pdev->dev,\n\t\t\t \"Using %u as missing dma-requests property\\n\",\n\t\t\t MTK_CQDMA_NR_VCHANS);\n\n\t\tcqdma->dma_requests = MTK_CQDMA_NR_VCHANS;\n\t}\n\n\tif (pdev->dev.of_node && of_property_read_u32(pdev->dev.of_node,\n\t\t\t\t\t\t      \"dma-channels\",\n\t\t\t\t\t\t      &cqdma->dma_channels)) {\n\t\tdev_info(&pdev->dev,\n\t\t\t \"Using %u as missing dma-channels property\\n\",\n\t\t\t MTK_CQDMA_NR_PCHANS);\n\n\t\tcqdma->dma_channels = MTK_CQDMA_NR_PCHANS;\n\t}\n\n\tcqdma->pc = devm_kcalloc(&pdev->dev, cqdma->dma_channels,\n\t\t\t\t sizeof(*cqdma->pc), GFP_KERNEL);\n\tif (!cqdma->pc)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < cqdma->dma_channels; ++i) {\n\t\tcqdma->pc[i] = devm_kcalloc(&pdev->dev, 1,\n\t\t\t\t\t    sizeof(**cqdma->pc), GFP_KERNEL);\n\t\tif (!cqdma->pc[i])\n\t\t\treturn -ENOMEM;\n\n\t\tINIT_LIST_HEAD(&cqdma->pc[i]->queue);\n\t\tspin_lock_init(&cqdma->pc[i]->lock);\n\t\trefcount_set(&cqdma->pc[i]->refcnt, 0);\n\t\tcqdma->pc[i]->base = devm_platform_ioremap_resource(pdev, i);\n\t\tif (IS_ERR(cqdma->pc[i]->base))\n\t\t\treturn PTR_ERR(cqdma->pc[i]->base);\n\n\t\t \n\t\terr = platform_get_irq(pdev, i);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tcqdma->pc[i]->irq = err;\n\n\t\terr = devm_request_irq(&pdev->dev, cqdma->pc[i]->irq,\n\t\t\t\t       mtk_cqdma_irq, 0, dev_name(&pdev->dev),\n\t\t\t\t       cqdma);\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"request_irq failed with err %d\\n\", err);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tcqdma->vc = devm_kcalloc(&pdev->dev, cqdma->dma_requests,\n\t\t\t\t sizeof(*cqdma->vc), GFP_KERNEL);\n\tif (!cqdma->vc)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < cqdma->dma_requests; i++) {\n\t\tvc = &cqdma->vc[i];\n\t\tvc->vc.desc_free = mtk_cqdma_vdesc_free;\n\t\tvchan_init(&vc->vc, dd);\n\t\tinit_completion(&vc->issue_completion);\n\t}\n\n\terr = dma_async_device_register(dd);\n\tif (err)\n\t\treturn err;\n\n\terr = of_dma_controller_register(pdev->dev.of_node,\n\t\t\t\t\t of_dma_xlate_by_chan_id, cqdma);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"MediaTek CQDMA OF registration failed %d\\n\", err);\n\t\tgoto err_unregister;\n\t}\n\n\terr = mtk_cqdma_hw_init(cqdma);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"MediaTek CQDMA HW initialization failed %d\\n\", err);\n\t\tgoto err_unregister;\n\t}\n\n\tplatform_set_drvdata(pdev, cqdma);\n\n\t \n\tfor (i = 0; i < cqdma->dma_channels; ++i)\n\t\ttasklet_setup(&cqdma->pc[i]->tasklet, mtk_cqdma_tasklet_cb);\n\n\tdev_info(&pdev->dev, \"MediaTek CQDMA driver registered\\n\");\n\n\treturn 0;\n\nerr_unregister:\n\tdma_async_device_unregister(dd);\n\n\treturn err;\n}\n\nstatic int mtk_cqdma_remove(struct platform_device *pdev)\n{\n\tstruct mtk_cqdma_device *cqdma = platform_get_drvdata(pdev);\n\tstruct mtk_cqdma_vchan *vc;\n\tunsigned long flags;\n\tint i;\n\n\t \n\tfor (i = 0; i < cqdma->dma_requests; i++) {\n\t\tvc = &cqdma->vc[i];\n\n\t\tlist_del(&vc->vc.chan.device_node);\n\t\ttasklet_kill(&vc->vc.task);\n\t}\n\n\t \n\tfor (i = 0; i < cqdma->dma_channels; i++) {\n\t\tspin_lock_irqsave(&cqdma->pc[i]->lock, flags);\n\t\tmtk_dma_clr(cqdma->pc[i], MTK_CQDMA_INT_EN,\n\t\t\t    MTK_CQDMA_INT_EN_BIT);\n\t\tspin_unlock_irqrestore(&cqdma->pc[i]->lock, flags);\n\n\t\t \n\t\tsynchronize_irq(cqdma->pc[i]->irq);\n\n\t\ttasklet_kill(&cqdma->pc[i]->tasklet);\n\t}\n\n\t \n\tmtk_cqdma_hw_deinit(cqdma);\n\n\tdma_async_device_unregister(&cqdma->ddev);\n\tof_dma_controller_free(pdev->dev.of_node);\n\n\treturn 0;\n}\n\nstatic struct platform_driver mtk_cqdma_driver = {\n\t.probe = mtk_cqdma_probe,\n\t.remove = mtk_cqdma_remove,\n\t.driver = {\n\t\t.name           = KBUILD_MODNAME,\n\t\t.of_match_table = mtk_cqdma_match,\n\t},\n};\nmodule_platform_driver(mtk_cqdma_driver);\n\nMODULE_DESCRIPTION(\"MediaTek CQDMA Controller Driver\");\nMODULE_AUTHOR(\"Shun-Chih Yu <shun-chih.yu@mediatek.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}