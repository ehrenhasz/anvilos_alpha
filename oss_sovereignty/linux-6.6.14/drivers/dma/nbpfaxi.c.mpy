{
  "module_name": "nbpfaxi.c",
  "hash_id": "7f49fb9c2eafefa24686e250deba0d70c32d336239b4ec33b4e373667afa6c83",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/nbpfaxi.c",
  "human_readable_source": "\n \n\n#include <linux/bitmap.h>\n#include <linux/bitops.h>\n#include <linux/clk.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/log2.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n\n#include <dt-bindings/dma/nbpfaxi.h>\n\n#include \"dmaengine.h\"\n\n#define NBPF_REG_CHAN_OFFSET\t0\n#define NBPF_REG_CHAN_SIZE\t0x40\n\n \n#define NBPF_CHAN_CUR_TR_BYTE\t0x20\n\n \n#define NBPF_CHAN_STAT\t0x24\n#define NBPF_CHAN_STAT_EN\t1\n#define NBPF_CHAN_STAT_TACT\t4\n#define NBPF_CHAN_STAT_ERR\t0x10\n#define NBPF_CHAN_STAT_END\t0x20\n#define NBPF_CHAN_STAT_TC\t0x40\n#define NBPF_CHAN_STAT_DER\t0x400\n\n \n#define NBPF_CHAN_CTRL\t0x28\n#define NBPF_CHAN_CTRL_SETEN\t1\n#define NBPF_CHAN_CTRL_CLREN\t2\n#define NBPF_CHAN_CTRL_STG\t4\n#define NBPF_CHAN_CTRL_SWRST\t8\n#define NBPF_CHAN_CTRL_CLRRQ\t0x10\n#define NBPF_CHAN_CTRL_CLREND\t0x20\n#define NBPF_CHAN_CTRL_CLRTC\t0x40\n#define NBPF_CHAN_CTRL_SETSUS\t0x100\n#define NBPF_CHAN_CTRL_CLRSUS\t0x200\n\n \n#define NBPF_CHAN_CFG\t0x2c\n#define NBPF_CHAN_CFG_SEL\t7\t\t \n#define NBPF_CHAN_CFG_REQD\t8\t\t \n#define NBPF_CHAN_CFG_LOEN\t0x10\t\t \n#define NBPF_CHAN_CFG_HIEN\t0x20\t\t \n#define NBPF_CHAN_CFG_LVL\t0x40\t\t \n#define NBPF_CHAN_CFG_AM\t0x700\t\t \n#define NBPF_CHAN_CFG_SDS\t0xf000\t\t \n#define NBPF_CHAN_CFG_DDS\t0xf0000\t\t \n#define NBPF_CHAN_CFG_SAD\t0x100000\t \n#define NBPF_CHAN_CFG_DAD\t0x200000\t \n#define NBPF_CHAN_CFG_TM\t0x400000\t \n#define NBPF_CHAN_CFG_DEM\t0x1000000\t \n#define NBPF_CHAN_CFG_TCM\t0x2000000\t \n#define NBPF_CHAN_CFG_SBE\t0x8000000\t \n#define NBPF_CHAN_CFG_RSEL\t0x10000000\t \n#define NBPF_CHAN_CFG_RSW\t0x20000000\t \n#define NBPF_CHAN_CFG_REN\t0x40000000\t \n#define NBPF_CHAN_CFG_DMS\t0x80000000\t \n\n#define NBPF_CHAN_NXLA\t0x38\n#define NBPF_CHAN_CRLA\t0x3c\n\n \n#define NBPF_HEADER_LV\t1\n#define NBPF_HEADER_LE\t2\n#define NBPF_HEADER_WBD\t4\n#define NBPF_HEADER_DIM\t8\n\n#define NBPF_CTRL\t0x300\n#define NBPF_CTRL_PR\t1\t\t \n#define NBPF_CTRL_LVINT\t2\t\t \n\n#define NBPF_DSTAT_ER\t0x314\n#define NBPF_DSTAT_END\t0x318\n\n#define NBPF_DMA_BUSWIDTHS \\\n\t(BIT(DMA_SLAVE_BUSWIDTH_UNDEFINED) | \\\n\t BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) | \\\n\t BIT(DMA_SLAVE_BUSWIDTH_2_BYTES) | \\\n\t BIT(DMA_SLAVE_BUSWIDTH_4_BYTES) | \\\n\t BIT(DMA_SLAVE_BUSWIDTH_8_BYTES))\n\nstruct nbpf_config {\n\tint num_channels;\n\tint buffer_size;\n};\n\n \n\nstruct nbpf_link_reg {\n\tu32\theader;\n\tu32\tsrc_addr;\n\tu32\tdst_addr;\n\tu32\ttransaction_size;\n\tu32\tconfig;\n\tu32\tinterval;\n\tu32\textension;\n\tu32\tnext;\n} __packed;\n\nstruct nbpf_device;\nstruct nbpf_channel;\nstruct nbpf_desc;\n\nstruct nbpf_link_desc {\n\tstruct nbpf_link_reg *hwdesc;\n\tdma_addr_t hwdesc_dma_addr;\n\tstruct nbpf_desc *desc;\n\tstruct list_head node;\n};\n\n \nstruct nbpf_desc {\n\tstruct dma_async_tx_descriptor async_tx;\n\tbool user_wait;\n\tsize_t length;\n\tstruct nbpf_channel *chan;\n\tstruct list_head sg;\n\tstruct list_head node;\n};\n\n \n#define NBPF_SEGMENTS_PER_DESC 4\n#define NBPF_DESCS_PER_PAGE ((PAGE_SIZE - sizeof(struct list_head)) /\t\\\n\t(sizeof(struct nbpf_desc) +\t\t\t\t\t\\\n\t NBPF_SEGMENTS_PER_DESC *\t\t\t\t\t\\\n\t (sizeof(struct nbpf_link_desc) + sizeof(struct nbpf_link_reg))))\n#define NBPF_SEGMENTS_PER_PAGE (NBPF_SEGMENTS_PER_DESC * NBPF_DESCS_PER_PAGE)\n\nstruct nbpf_desc_page {\n\tstruct list_head node;\n\tstruct nbpf_desc desc[NBPF_DESCS_PER_PAGE];\n\tstruct nbpf_link_desc ldesc[NBPF_SEGMENTS_PER_PAGE];\n\tstruct nbpf_link_reg hwdesc[NBPF_SEGMENTS_PER_PAGE];\n};\n\n \nstruct nbpf_channel {\n\tstruct dma_chan dma_chan;\n\tstruct tasklet_struct tasklet;\n\tvoid __iomem *base;\n\tstruct nbpf_device *nbpf;\n\tchar name[16];\n\tint irq;\n\tdma_addr_t slave_src_addr;\n\tsize_t slave_src_width;\n\tsize_t slave_src_burst;\n\tdma_addr_t slave_dst_addr;\n\tsize_t slave_dst_width;\n\tsize_t slave_dst_burst;\n\tunsigned int terminal;\n\tu32 dmarq_cfg;\n\tunsigned long flags;\n\tspinlock_t lock;\n\tstruct list_head free_links;\n\tstruct list_head free;\n\tstruct list_head queued;\n\tstruct list_head active;\n\tstruct list_head done;\n\tstruct list_head desc_page;\n\tstruct nbpf_desc *running;\n\tbool paused;\n};\n\nstruct nbpf_device {\n\tstruct dma_device dma_dev;\n\tvoid __iomem *base;\n\tu32 max_burst_mem_read;\n\tu32 max_burst_mem_write;\n\tstruct clk *clk;\n\tconst struct nbpf_config *config;\n\tunsigned int eirq;\n\tstruct nbpf_channel chan[];\n};\n\nenum nbpf_model {\n\tNBPF1B4,\n\tNBPF1B8,\n\tNBPF1B16,\n\tNBPF4B4,\n\tNBPF4B8,\n\tNBPF4B16,\n\tNBPF8B4,\n\tNBPF8B8,\n\tNBPF8B16,\n};\n\nstatic struct nbpf_config nbpf_cfg[] = {\n\t[NBPF1B4] = {\n\t\t.num_channels = 1,\n\t\t.buffer_size = 4,\n\t},\n\t[NBPF1B8] = {\n\t\t.num_channels = 1,\n\t\t.buffer_size = 8,\n\t},\n\t[NBPF1B16] = {\n\t\t.num_channels = 1,\n\t\t.buffer_size = 16,\n\t},\n\t[NBPF4B4] = {\n\t\t.num_channels = 4,\n\t\t.buffer_size = 4,\n\t},\n\t[NBPF4B8] = {\n\t\t.num_channels = 4,\n\t\t.buffer_size = 8,\n\t},\n\t[NBPF4B16] = {\n\t\t.num_channels = 4,\n\t\t.buffer_size = 16,\n\t},\n\t[NBPF8B4] = {\n\t\t.num_channels = 8,\n\t\t.buffer_size = 4,\n\t},\n\t[NBPF8B8] = {\n\t\t.num_channels = 8,\n\t\t.buffer_size = 8,\n\t},\n\t[NBPF8B16] = {\n\t\t.num_channels = 8,\n\t\t.buffer_size = 16,\n\t},\n};\n\n#define nbpf_to_chan(d) container_of(d, struct nbpf_channel, dma_chan)\n\n \n\n \n\nstatic inline u32 nbpf_chan_read(struct nbpf_channel *chan,\n\t\t\t\t unsigned int offset)\n{\n\tu32 data = ioread32(chan->base + offset);\n\tdev_dbg(chan->dma_chan.device->dev, \"%s(0x%p + 0x%x) = 0x%x\\n\",\n\t\t__func__, chan->base, offset, data);\n\treturn data;\n}\n\nstatic inline void nbpf_chan_write(struct nbpf_channel *chan,\n\t\t\t\t   unsigned int offset, u32 data)\n{\n\tiowrite32(data, chan->base + offset);\n\tdev_dbg(chan->dma_chan.device->dev, \"%s(0x%p + 0x%x) = 0x%x\\n\",\n\t\t__func__, chan->base, offset, data);\n}\n\nstatic inline u32 nbpf_read(struct nbpf_device *nbpf,\n\t\t\t    unsigned int offset)\n{\n\tu32 data = ioread32(nbpf->base + offset);\n\tdev_dbg(nbpf->dma_dev.dev, \"%s(0x%p + 0x%x) = 0x%x\\n\",\n\t\t__func__, nbpf->base, offset, data);\n\treturn data;\n}\n\nstatic inline void nbpf_write(struct nbpf_device *nbpf,\n\t\t\t      unsigned int offset, u32 data)\n{\n\tiowrite32(data, nbpf->base + offset);\n\tdev_dbg(nbpf->dma_dev.dev, \"%s(0x%p + 0x%x) = 0x%x\\n\",\n\t\t__func__, nbpf->base, offset, data);\n}\n\nstatic void nbpf_chan_halt(struct nbpf_channel *chan)\n{\n\tnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_CLREN);\n}\n\nstatic bool nbpf_status_get(struct nbpf_channel *chan)\n{\n\tu32 status = nbpf_read(chan->nbpf, NBPF_DSTAT_END);\n\n\treturn status & BIT(chan - chan->nbpf->chan);\n}\n\nstatic void nbpf_status_ack(struct nbpf_channel *chan)\n{\n\tnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_CLREND);\n}\n\nstatic u32 nbpf_error_get(struct nbpf_device *nbpf)\n{\n\treturn nbpf_read(nbpf, NBPF_DSTAT_ER);\n}\n\nstatic struct nbpf_channel *nbpf_error_get_channel(struct nbpf_device *nbpf, u32 error)\n{\n\treturn nbpf->chan + __ffs(error);\n}\n\nstatic void nbpf_error_clear(struct nbpf_channel *chan)\n{\n\tu32 status;\n\tint i;\n\n\t \n\tnbpf_chan_halt(chan);\n\n\tfor (i = 1000; i; i--) {\n\t\tstatus = nbpf_chan_read(chan, NBPF_CHAN_STAT);\n\t\tif (!(status & NBPF_CHAN_STAT_TACT))\n\t\t\tbreak;\n\t\tcpu_relax();\n\t}\n\n\tif (!i)\n\t\tdev_err(chan->dma_chan.device->dev,\n\t\t\t\"%s(): abort timeout, channel status 0x%x\\n\", __func__, status);\n\n\tnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_SWRST);\n}\n\nstatic int nbpf_start(struct nbpf_desc *desc)\n{\n\tstruct nbpf_channel *chan = desc->chan;\n\tstruct nbpf_link_desc *ldesc = list_first_entry(&desc->sg, struct nbpf_link_desc, node);\n\n\tnbpf_chan_write(chan, NBPF_CHAN_NXLA, (u32)ldesc->hwdesc_dma_addr);\n\tnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_SETEN | NBPF_CHAN_CTRL_CLRSUS);\n\tchan->paused = false;\n\n\t \n\tif (ldesc->hwdesc->config & NBPF_CHAN_CFG_TM)\n\t\tnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_STG);\n\n\tdev_dbg(chan->nbpf->dma_dev.dev, \"%s(): next 0x%x, cur 0x%x\\n\", __func__,\n\t\tnbpf_chan_read(chan, NBPF_CHAN_NXLA), nbpf_chan_read(chan, NBPF_CHAN_CRLA));\n\n\treturn 0;\n}\n\nstatic void nbpf_chan_prepare(struct nbpf_channel *chan)\n{\n\tchan->dmarq_cfg = (chan->flags & NBPF_SLAVE_RQ_HIGH ? NBPF_CHAN_CFG_HIEN : 0) |\n\t\t(chan->flags & NBPF_SLAVE_RQ_LOW ? NBPF_CHAN_CFG_LOEN : 0) |\n\t\t(chan->flags & NBPF_SLAVE_RQ_LEVEL ?\n\t\t NBPF_CHAN_CFG_LVL | (NBPF_CHAN_CFG_AM & 0x200) : 0) |\n\t\tchan->terminal;\n}\n\nstatic void nbpf_chan_prepare_default(struct nbpf_channel *chan)\n{\n\t \n\tchan->dmarq_cfg = NBPF_CHAN_CFG_AM & 0x400;\n\tchan->terminal = 0;\n\tchan->flags = 0;\n}\n\nstatic void nbpf_chan_configure(struct nbpf_channel *chan)\n{\n\t \n\tnbpf_chan_write(chan, NBPF_CHAN_CFG, NBPF_CHAN_CFG_DMS | chan->dmarq_cfg);\n}\n\nstatic u32 nbpf_xfer_ds(struct nbpf_device *nbpf, size_t size,\n\t\t\tenum dma_transfer_direction direction)\n{\n\tint max_burst = nbpf->config->buffer_size * 8;\n\n\tif (nbpf->max_burst_mem_read || nbpf->max_burst_mem_write) {\n\t\tswitch (direction) {\n\t\tcase DMA_MEM_TO_MEM:\n\t\t\tmax_burst = min_not_zero(nbpf->max_burst_mem_read,\n\t\t\t\t\t\t nbpf->max_burst_mem_write);\n\t\t\tbreak;\n\t\tcase DMA_MEM_TO_DEV:\n\t\t\tif (nbpf->max_burst_mem_read)\n\t\t\t\tmax_burst = nbpf->max_burst_mem_read;\n\t\t\tbreak;\n\t\tcase DMA_DEV_TO_MEM:\n\t\t\tif (nbpf->max_burst_mem_write)\n\t\t\t\tmax_burst = nbpf->max_burst_mem_write;\n\t\t\tbreak;\n\t\tcase DMA_DEV_TO_DEV:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\treturn min_t(int, __ffs(size), ilog2(max_burst));\n}\n\nstatic size_t nbpf_xfer_size(struct nbpf_device *nbpf,\n\t\t\t     enum dma_slave_buswidth width, u32 burst)\n{\n\tsize_t size;\n\n\tif (!burst)\n\t\tburst = 1;\n\n\tswitch (width) {\n\tcase DMA_SLAVE_BUSWIDTH_8_BYTES:\n\t\tsize = 8 * burst;\n\t\tbreak;\n\n\tcase DMA_SLAVE_BUSWIDTH_4_BYTES:\n\t\tsize = 4 * burst;\n\t\tbreak;\n\n\tcase DMA_SLAVE_BUSWIDTH_2_BYTES:\n\t\tsize = 2 * burst;\n\t\tbreak;\n\n\tdefault:\n\t\tpr_warn(\"%s(): invalid bus width %u\\n\", __func__, width);\n\t\tfallthrough;\n\tcase DMA_SLAVE_BUSWIDTH_1_BYTE:\n\t\tsize = burst;\n\t}\n\n\treturn nbpf_xfer_ds(nbpf, size, DMA_TRANS_NONE);\n}\n\n \n\nstatic int nbpf_prep_one(struct nbpf_link_desc *ldesc,\n\t\t\t enum dma_transfer_direction direction,\n\t\t\t dma_addr_t src, dma_addr_t dst, size_t size, bool last)\n{\n\tstruct nbpf_link_reg *hwdesc = ldesc->hwdesc;\n\tstruct nbpf_desc *desc = ldesc->desc;\n\tstruct nbpf_channel *chan = desc->chan;\n\tstruct device *dev = chan->dma_chan.device->dev;\n\tsize_t mem_xfer, slave_xfer;\n\tbool can_burst;\n\n\thwdesc->header = NBPF_HEADER_WBD | NBPF_HEADER_LV |\n\t\t(last ? NBPF_HEADER_LE : 0);\n\n\thwdesc->src_addr = src;\n\thwdesc->dst_addr = dst;\n\thwdesc->transaction_size = size;\n\n\t \n\tmem_xfer = nbpf_xfer_ds(chan->nbpf, size, direction);\n\n\tswitch (direction) {\n\tcase DMA_DEV_TO_MEM:\n\t\tcan_burst = chan->slave_src_width >= 3;\n\t\tslave_xfer = min(mem_xfer, can_burst ?\n\t\t\t\t chan->slave_src_burst : chan->slave_src_width);\n\t\t \n\t\tif (mem_xfer > chan->slave_src_burst && !can_burst)\n\t\t\tmem_xfer = chan->slave_src_burst;\n\t\t \n\t\thwdesc->config = NBPF_CHAN_CFG_SAD | (NBPF_CHAN_CFG_DDS & (mem_xfer << 16)) |\n\t\t\t(NBPF_CHAN_CFG_SDS & (slave_xfer << 12)) | NBPF_CHAN_CFG_REQD |\n\t\t\tNBPF_CHAN_CFG_SBE;\n\t\tbreak;\n\n\tcase DMA_MEM_TO_DEV:\n\t\tslave_xfer = min(mem_xfer, chan->slave_dst_width >= 3 ?\n\t\t\t\t chan->slave_dst_burst : chan->slave_dst_width);\n\t\thwdesc->config = NBPF_CHAN_CFG_DAD | (NBPF_CHAN_CFG_SDS & (mem_xfer << 12)) |\n\t\t\t(NBPF_CHAN_CFG_DDS & (slave_xfer << 16)) | NBPF_CHAN_CFG_REQD;\n\t\tbreak;\n\n\tcase DMA_MEM_TO_MEM:\n\t\thwdesc->config = NBPF_CHAN_CFG_TCM | NBPF_CHAN_CFG_TM |\n\t\t\t(NBPF_CHAN_CFG_SDS & (mem_xfer << 12)) |\n\t\t\t(NBPF_CHAN_CFG_DDS & (mem_xfer << 16));\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\thwdesc->config |= chan->dmarq_cfg | (last ? 0 : NBPF_CHAN_CFG_DEM) |\n\t\tNBPF_CHAN_CFG_DMS;\n\n\tdev_dbg(dev, \"%s(): desc @ %pad: hdr 0x%x, cfg 0x%x, %zu @ %pad -> %pad\\n\",\n\t\t__func__, &ldesc->hwdesc_dma_addr, hwdesc->header,\n\t\thwdesc->config, size, &src, &dst);\n\n\tdma_sync_single_for_device(dev, ldesc->hwdesc_dma_addr, sizeof(*hwdesc),\n\t\t\t\t   DMA_TO_DEVICE);\n\n\treturn 0;\n}\n\nstatic size_t nbpf_bytes_left(struct nbpf_channel *chan)\n{\n\treturn nbpf_chan_read(chan, NBPF_CHAN_CUR_TR_BYTE);\n}\n\nstatic void nbpf_configure(struct nbpf_device *nbpf)\n{\n\tnbpf_write(nbpf, NBPF_CTRL, NBPF_CTRL_LVINT);\n}\n\n \n\n \nstatic void nbpf_issue_pending(struct dma_chan *dchan)\n{\n\tstruct nbpf_channel *chan = nbpf_to_chan(dchan);\n\tunsigned long flags;\n\n\tdev_dbg(dchan->device->dev, \"Entry %s()\\n\", __func__);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tif (list_empty(&chan->queued))\n\t\tgoto unlock;\n\n\tlist_splice_tail_init(&chan->queued, &chan->active);\n\n\tif (!chan->running) {\n\t\tstruct nbpf_desc *desc = list_first_entry(&chan->active,\n\t\t\t\t\t\tstruct nbpf_desc, node);\n\t\tif (!nbpf_start(desc))\n\t\t\tchan->running = desc;\n\t}\n\nunlock:\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\nstatic enum dma_status nbpf_tx_status(struct dma_chan *dchan,\n\t\tdma_cookie_t cookie, struct dma_tx_state *state)\n{\n\tstruct nbpf_channel *chan = nbpf_to_chan(dchan);\n\tenum dma_status status = dma_cookie_status(dchan, cookie, state);\n\n\tif (state) {\n\t\tdma_cookie_t running;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&chan->lock, flags);\n\t\trunning = chan->running ? chan->running->async_tx.cookie : -EINVAL;\n\n\t\tif (cookie == running) {\n\t\t\tstate->residue = nbpf_bytes_left(chan);\n\t\t\tdev_dbg(dchan->device->dev, \"%s(): residue %u\\n\", __func__,\n\t\t\t\tstate->residue);\n\t\t} else if (status == DMA_IN_PROGRESS) {\n\t\t\tstruct nbpf_desc *desc;\n\t\t\tbool found = false;\n\n\t\t\tlist_for_each_entry(desc, &chan->active, node)\n\t\t\t\tif (desc->async_tx.cookie == cookie) {\n\t\t\t\t\tfound = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\tif (!found)\n\t\t\t\tlist_for_each_entry(desc, &chan->queued, node)\n\t\t\t\t\tif (desc->async_tx.cookie == cookie) {\n\t\t\t\t\t\tfound = true;\n\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t}\n\n\t\t\tstate->residue = found ? desc->length : 0;\n\t\t}\n\n\t\tspin_unlock_irqrestore(&chan->lock, flags);\n\t}\n\n\tif (chan->paused)\n\t\tstatus = DMA_PAUSED;\n\n\treturn status;\n}\n\nstatic dma_cookie_t nbpf_tx_submit(struct dma_async_tx_descriptor *tx)\n{\n\tstruct nbpf_desc *desc = container_of(tx, struct nbpf_desc, async_tx);\n\tstruct nbpf_channel *chan = desc->chan;\n\tunsigned long flags;\n\tdma_cookie_t cookie;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tcookie = dma_cookie_assign(tx);\n\tlist_add_tail(&desc->node, &chan->queued);\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\tdev_dbg(chan->dma_chan.device->dev, \"Entry %s(%d)\\n\", __func__, cookie);\n\n\treturn cookie;\n}\n\nstatic int nbpf_desc_page_alloc(struct nbpf_channel *chan)\n{\n\tstruct dma_chan *dchan = &chan->dma_chan;\n\tstruct nbpf_desc_page *dpage = (void *)get_zeroed_page(GFP_KERNEL | GFP_DMA);\n\tstruct nbpf_link_desc *ldesc;\n\tstruct nbpf_link_reg *hwdesc;\n\tstruct nbpf_desc *desc;\n\tLIST_HEAD(head);\n\tLIST_HEAD(lhead);\n\tint i;\n\tstruct device *dev = dchan->device->dev;\n\n\tif (!dpage)\n\t\treturn -ENOMEM;\n\n\tdev_dbg(dev, \"%s(): alloc %lu descriptors, %lu segments, total alloc %zu\\n\",\n\t\t__func__, NBPF_DESCS_PER_PAGE, NBPF_SEGMENTS_PER_PAGE, sizeof(*dpage));\n\n\tfor (i = 0, ldesc = dpage->ldesc, hwdesc = dpage->hwdesc;\n\t     i < ARRAY_SIZE(dpage->ldesc);\n\t     i++, ldesc++, hwdesc++) {\n\t\tldesc->hwdesc = hwdesc;\n\t\tlist_add_tail(&ldesc->node, &lhead);\n\t\tldesc->hwdesc_dma_addr = dma_map_single(dchan->device->dev,\n\t\t\t\t\thwdesc, sizeof(*hwdesc), DMA_TO_DEVICE);\n\n\t\tdev_dbg(dev, \"%s(): mapped 0x%p to %pad\\n\", __func__,\n\t\t\thwdesc, &ldesc->hwdesc_dma_addr);\n\t}\n\n\tfor (i = 0, desc = dpage->desc;\n\t     i < ARRAY_SIZE(dpage->desc);\n\t     i++, desc++) {\n\t\tdma_async_tx_descriptor_init(&desc->async_tx, dchan);\n\t\tdesc->async_tx.tx_submit = nbpf_tx_submit;\n\t\tdesc->chan = chan;\n\t\tINIT_LIST_HEAD(&desc->sg);\n\t\tlist_add_tail(&desc->node, &head);\n\t}\n\n\t \n\tspin_lock_irq(&chan->lock);\n\tlist_splice_tail(&lhead, &chan->free_links);\n\tlist_splice_tail(&head, &chan->free);\n\tlist_add(&dpage->node, &chan->desc_page);\n\tspin_unlock_irq(&chan->lock);\n\n\treturn ARRAY_SIZE(dpage->desc);\n}\n\nstatic void nbpf_desc_put(struct nbpf_desc *desc)\n{\n\tstruct nbpf_channel *chan = desc->chan;\n\tstruct nbpf_link_desc *ldesc, *tmp;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tlist_for_each_entry_safe(ldesc, tmp, &desc->sg, node)\n\t\tlist_move(&ldesc->node, &chan->free_links);\n\n\tlist_add(&desc->node, &chan->free);\n\tspin_unlock_irqrestore(&chan->lock, flags);\n}\n\nstatic void nbpf_scan_acked(struct nbpf_channel *chan)\n{\n\tstruct nbpf_desc *desc, *tmp;\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\tlist_for_each_entry_safe(desc, tmp, &chan->done, node)\n\t\tif (async_tx_test_ack(&desc->async_tx) && desc->user_wait) {\n\t\t\tlist_move(&desc->node, &head);\n\t\t\tdesc->user_wait = false;\n\t\t}\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\tlist_for_each_entry_safe(desc, tmp, &head, node) {\n\t\tlist_del(&desc->node);\n\t\tnbpf_desc_put(desc);\n\t}\n}\n\n \nstatic struct nbpf_desc *nbpf_desc_get(struct nbpf_channel *chan, size_t len)\n{\n\tstruct nbpf_desc *desc = NULL;\n\tstruct nbpf_link_desc *ldesc, *prev = NULL;\n\n\tnbpf_scan_acked(chan);\n\n\tspin_lock_irq(&chan->lock);\n\n\tdo {\n\t\tint i = 0, ret;\n\n\t\tif (list_empty(&chan->free)) {\n\t\t\t \n\t\t\tspin_unlock_irq(&chan->lock);\n\t\t\tret = nbpf_desc_page_alloc(chan);\n\t\t\tif (ret < 0)\n\t\t\t\treturn NULL;\n\t\t\tspin_lock_irq(&chan->lock);\n\t\t\tcontinue;\n\t\t}\n\t\tdesc = list_first_entry(&chan->free, struct nbpf_desc, node);\n\t\tlist_del(&desc->node);\n\n\t\tdo {\n\t\t\tif (list_empty(&chan->free_links)) {\n\t\t\t\t \n\t\t\t\tspin_unlock_irq(&chan->lock);\n\t\t\t\tret = nbpf_desc_page_alloc(chan);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tnbpf_desc_put(desc);\n\t\t\t\t\treturn NULL;\n\t\t\t\t}\n\t\t\t\tspin_lock_irq(&chan->lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tldesc = list_first_entry(&chan->free_links,\n\t\t\t\t\t\t struct nbpf_link_desc, node);\n\t\t\tldesc->desc = desc;\n\t\t\tif (prev)\n\t\t\t\tprev->hwdesc->next = (u32)ldesc->hwdesc_dma_addr;\n\n\t\t\tprev = ldesc;\n\t\t\tlist_move_tail(&ldesc->node, &desc->sg);\n\n\t\t\ti++;\n\t\t} while (i < len);\n\t} while (!desc);\n\n\tprev->hwdesc->next = 0;\n\n\tspin_unlock_irq(&chan->lock);\n\n\treturn desc;\n}\n\nstatic void nbpf_chan_idle(struct nbpf_channel *chan)\n{\n\tstruct nbpf_desc *desc, *tmp;\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&chan->lock, flags);\n\n\tlist_splice_init(&chan->done, &head);\n\tlist_splice_init(&chan->active, &head);\n\tlist_splice_init(&chan->queued, &head);\n\n\tchan->running = NULL;\n\n\tspin_unlock_irqrestore(&chan->lock, flags);\n\n\tlist_for_each_entry_safe(desc, tmp, &head, node) {\n\t\tdev_dbg(chan->nbpf->dma_dev.dev, \"%s(): force-free desc %p cookie %d\\n\",\n\t\t\t__func__, desc, desc->async_tx.cookie);\n\t\tlist_del(&desc->node);\n\t\tnbpf_desc_put(desc);\n\t}\n}\n\nstatic int nbpf_pause(struct dma_chan *dchan)\n{\n\tstruct nbpf_channel *chan = nbpf_to_chan(dchan);\n\n\tdev_dbg(dchan->device->dev, \"Entry %s\\n\", __func__);\n\n\tchan->paused = true;\n\tnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_SETSUS);\n\t \n\tnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_CLREN);\n\n\treturn 0;\n}\n\nstatic int nbpf_terminate_all(struct dma_chan *dchan)\n{\n\tstruct nbpf_channel *chan = nbpf_to_chan(dchan);\n\n\tdev_dbg(dchan->device->dev, \"Entry %s\\n\", __func__);\n\tdev_dbg(dchan->device->dev, \"Terminating\\n\");\n\n\tnbpf_chan_halt(chan);\n\tnbpf_chan_idle(chan);\n\n\treturn 0;\n}\n\nstatic int nbpf_config(struct dma_chan *dchan,\n\t\t       struct dma_slave_config *config)\n{\n\tstruct nbpf_channel *chan = nbpf_to_chan(dchan);\n\n\tdev_dbg(dchan->device->dev, \"Entry %s\\n\", __func__);\n\n\t \n\n\tchan->slave_dst_addr = config->dst_addr;\n\tchan->slave_dst_width = nbpf_xfer_size(chan->nbpf,\n\t\t\t\t\t       config->dst_addr_width, 1);\n\tchan->slave_dst_burst = nbpf_xfer_size(chan->nbpf,\n\t\t\t\t\t       config->dst_addr_width,\n\t\t\t\t\t       config->dst_maxburst);\n\tchan->slave_src_addr = config->src_addr;\n\tchan->slave_src_width = nbpf_xfer_size(chan->nbpf,\n\t\t\t\t\t       config->src_addr_width, 1);\n\tchan->slave_src_burst = nbpf_xfer_size(chan->nbpf,\n\t\t\t\t\t       config->src_addr_width,\n\t\t\t\t\t       config->src_maxburst);\n\n\treturn 0;\n}\n\nstatic struct dma_async_tx_descriptor *nbpf_prep_sg(struct nbpf_channel *chan,\n\t\tstruct scatterlist *src_sg, struct scatterlist *dst_sg,\n\t\tsize_t len, enum dma_transfer_direction direction,\n\t\tunsigned long flags)\n{\n\tstruct nbpf_link_desc *ldesc;\n\tstruct scatterlist *mem_sg;\n\tstruct nbpf_desc *desc;\n\tbool inc_src, inc_dst;\n\tsize_t data_len = 0;\n\tint i = 0;\n\n\tswitch (direction) {\n\tcase DMA_DEV_TO_MEM:\n\t\tmem_sg = dst_sg;\n\t\tinc_src = false;\n\t\tinc_dst = true;\n\t\tbreak;\n\n\tcase DMA_MEM_TO_DEV:\n\t\tmem_sg = src_sg;\n\t\tinc_src = true;\n\t\tinc_dst = false;\n\t\tbreak;\n\n\tdefault:\n\tcase DMA_MEM_TO_MEM:\n\t\tmem_sg = src_sg;\n\t\tinc_src = true;\n\t\tinc_dst = true;\n\t}\n\n\tdesc = nbpf_desc_get(chan, len);\n\tif (!desc)\n\t\treturn NULL;\n\n\tdesc->async_tx.flags = flags;\n\tdesc->async_tx.cookie = -EBUSY;\n\tdesc->user_wait = false;\n\n\t \n\tlist_for_each_entry(ldesc, &desc->sg, node) {\n\t\tint ret = nbpf_prep_one(ldesc, direction,\n\t\t\t\t\tsg_dma_address(src_sg),\n\t\t\t\t\tsg_dma_address(dst_sg),\n\t\t\t\t\tsg_dma_len(mem_sg),\n\t\t\t\t\ti == len - 1);\n\t\tif (ret < 0) {\n\t\t\tnbpf_desc_put(desc);\n\t\t\treturn NULL;\n\t\t}\n\t\tdata_len += sg_dma_len(mem_sg);\n\t\tif (inc_src)\n\t\t\tsrc_sg = sg_next(src_sg);\n\t\tif (inc_dst)\n\t\t\tdst_sg = sg_next(dst_sg);\n\t\tmem_sg = direction == DMA_DEV_TO_MEM ? dst_sg : src_sg;\n\t\ti++;\n\t}\n\n\tdesc->length = data_len;\n\n\t \n\treturn &desc->async_tx;\n}\n\nstatic struct dma_async_tx_descriptor *nbpf_prep_memcpy(\n\tstruct dma_chan *dchan, dma_addr_t dst, dma_addr_t src,\n\tsize_t len, unsigned long flags)\n{\n\tstruct nbpf_channel *chan = nbpf_to_chan(dchan);\n\tstruct scatterlist dst_sg;\n\tstruct scatterlist src_sg;\n\n\tsg_init_table(&dst_sg, 1);\n\tsg_init_table(&src_sg, 1);\n\n\tsg_dma_address(&dst_sg) = dst;\n\tsg_dma_address(&src_sg) = src;\n\n\tsg_dma_len(&dst_sg) = len;\n\tsg_dma_len(&src_sg) = len;\n\n\tdev_dbg(dchan->device->dev, \"%s(): %zu @ %pad -> %pad\\n\",\n\t\t__func__, len, &src, &dst);\n\n\treturn nbpf_prep_sg(chan, &src_sg, &dst_sg, 1,\n\t\t\t    DMA_MEM_TO_MEM, flags);\n}\n\nstatic struct dma_async_tx_descriptor *nbpf_prep_slave_sg(\n\tstruct dma_chan *dchan, struct scatterlist *sgl, unsigned int sg_len,\n\tenum dma_transfer_direction direction, unsigned long flags, void *context)\n{\n\tstruct nbpf_channel *chan = nbpf_to_chan(dchan);\n\tstruct scatterlist slave_sg;\n\n\tdev_dbg(dchan->device->dev, \"Entry %s()\\n\", __func__);\n\n\tsg_init_table(&slave_sg, 1);\n\n\tswitch (direction) {\n\tcase DMA_MEM_TO_DEV:\n\t\tsg_dma_address(&slave_sg) = chan->slave_dst_addr;\n\t\treturn nbpf_prep_sg(chan, sgl, &slave_sg, sg_len,\n\t\t\t\t    direction, flags);\n\n\tcase DMA_DEV_TO_MEM:\n\t\tsg_dma_address(&slave_sg) = chan->slave_src_addr;\n\t\treturn nbpf_prep_sg(chan, &slave_sg, sgl, sg_len,\n\t\t\t\t    direction, flags);\n\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic int nbpf_alloc_chan_resources(struct dma_chan *dchan)\n{\n\tstruct nbpf_channel *chan = nbpf_to_chan(dchan);\n\tint ret;\n\n\tINIT_LIST_HEAD(&chan->free);\n\tINIT_LIST_HEAD(&chan->free_links);\n\tINIT_LIST_HEAD(&chan->queued);\n\tINIT_LIST_HEAD(&chan->active);\n\tINIT_LIST_HEAD(&chan->done);\n\n\tret = nbpf_desc_page_alloc(chan);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tdev_dbg(dchan->device->dev, \"Entry %s(): terminal %u\\n\", __func__,\n\t\tchan->terminal);\n\n\tnbpf_chan_configure(chan);\n\n\treturn ret;\n}\n\nstatic void nbpf_free_chan_resources(struct dma_chan *dchan)\n{\n\tstruct nbpf_channel *chan = nbpf_to_chan(dchan);\n\tstruct nbpf_desc_page *dpage, *tmp;\n\n\tdev_dbg(dchan->device->dev, \"Entry %s()\\n\", __func__);\n\n\tnbpf_chan_halt(chan);\n\tnbpf_chan_idle(chan);\n\t \n\tnbpf_chan_prepare_default(chan);\n\n\tlist_for_each_entry_safe(dpage, tmp, &chan->desc_page, node) {\n\t\tstruct nbpf_link_desc *ldesc;\n\t\tint i;\n\t\tlist_del(&dpage->node);\n\t\tfor (i = 0, ldesc = dpage->ldesc;\n\t\t     i < ARRAY_SIZE(dpage->ldesc);\n\t\t     i++, ldesc++)\n\t\t\tdma_unmap_single(dchan->device->dev, ldesc->hwdesc_dma_addr,\n\t\t\t\t\t sizeof(*ldesc->hwdesc), DMA_TO_DEVICE);\n\t\tfree_page((unsigned long)dpage);\n\t}\n}\n\nstatic struct dma_chan *nbpf_of_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t      struct of_dma *ofdma)\n{\n\tstruct nbpf_device *nbpf = ofdma->of_dma_data;\n\tstruct dma_chan *dchan;\n\tstruct nbpf_channel *chan;\n\n\tif (dma_spec->args_count != 2)\n\t\treturn NULL;\n\n\tdchan = dma_get_any_slave_channel(&nbpf->dma_dev);\n\tif (!dchan)\n\t\treturn NULL;\n\n\tdev_dbg(dchan->device->dev, \"Entry %s(%pOFn)\\n\", __func__,\n\t\tdma_spec->np);\n\n\tchan = nbpf_to_chan(dchan);\n\n\tchan->terminal = dma_spec->args[0];\n\tchan->flags = dma_spec->args[1];\n\n\tnbpf_chan_prepare(chan);\n\tnbpf_chan_configure(chan);\n\n\treturn dchan;\n}\n\nstatic void nbpf_chan_tasklet(struct tasklet_struct *t)\n{\n\tstruct nbpf_channel *chan = from_tasklet(chan, t, tasklet);\n\tstruct nbpf_desc *desc, *tmp;\n\tstruct dmaengine_desc_callback cb;\n\n\twhile (!list_empty(&chan->done)) {\n\t\tbool found = false, must_put, recycling = false;\n\n\t\tspin_lock_irq(&chan->lock);\n\n\t\tlist_for_each_entry_safe(desc, tmp, &chan->done, node) {\n\t\t\tif (!desc->user_wait) {\n\t\t\t\t \n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t} else if (async_tx_test_ack(&desc->async_tx)) {\n\t\t\t\t \n\t\t\t\tlist_del(&desc->node);\n\t\t\t\tspin_unlock_irq(&chan->lock);\n\t\t\t\tnbpf_desc_put(desc);\n\t\t\t\trecycling = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (recycling)\n\t\t\tcontinue;\n\n\t\tif (!found) {\n\t\t\t \n\t\t\tspin_unlock_irq(&chan->lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tdma_cookie_complete(&desc->async_tx);\n\n\t\t \n\t\tif (async_tx_test_ack(&desc->async_tx)) {\n\t\t\tlist_del(&desc->node);\n\t\t\tmust_put = true;\n\t\t} else {\n\t\t\tdesc->user_wait = true;\n\t\t\tmust_put = false;\n\t\t}\n\n\t\tdmaengine_desc_get_callback(&desc->async_tx, &cb);\n\n\t\t \n\t\tspin_unlock_irq(&chan->lock);\n\n\t\tdmaengine_desc_callback_invoke(&cb, NULL);\n\n\t\tif (must_put)\n\t\t\tnbpf_desc_put(desc);\n\t}\n}\n\nstatic irqreturn_t nbpf_chan_irq(int irq, void *dev)\n{\n\tstruct nbpf_channel *chan = dev;\n\tbool done = nbpf_status_get(chan);\n\tstruct nbpf_desc *desc;\n\tirqreturn_t ret;\n\tbool bh = false;\n\n\tif (!done)\n\t\treturn IRQ_NONE;\n\n\tnbpf_status_ack(chan);\n\n\tdev_dbg(&chan->dma_chan.dev->device, \"%s()\\n\", __func__);\n\n\tspin_lock(&chan->lock);\n\tdesc = chan->running;\n\tif (WARN_ON(!desc)) {\n\t\tret = IRQ_NONE;\n\t\tgoto unlock;\n\t} else {\n\t\tret = IRQ_HANDLED;\n\t\tbh = true;\n\t}\n\n\tlist_move_tail(&desc->node, &chan->done);\n\tchan->running = NULL;\n\n\tif (!list_empty(&chan->active)) {\n\t\tdesc = list_first_entry(&chan->active,\n\t\t\t\t\tstruct nbpf_desc, node);\n\t\tif (!nbpf_start(desc))\n\t\t\tchan->running = desc;\n\t}\n\nunlock:\n\tspin_unlock(&chan->lock);\n\n\tif (bh)\n\t\ttasklet_schedule(&chan->tasklet);\n\n\treturn ret;\n}\n\nstatic irqreturn_t nbpf_err_irq(int irq, void *dev)\n{\n\tstruct nbpf_device *nbpf = dev;\n\tu32 error = nbpf_error_get(nbpf);\n\n\tdev_warn(nbpf->dma_dev.dev, \"DMA error IRQ %u\\n\", irq);\n\n\tif (!error)\n\t\treturn IRQ_NONE;\n\n\tdo {\n\t\tstruct nbpf_channel *chan = nbpf_error_get_channel(nbpf, error);\n\t\t \n\t\tnbpf_error_clear(chan);\n\t\tnbpf_chan_idle(chan);\n\t\terror = nbpf_error_get(nbpf);\n\t} while (error);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int nbpf_chan_probe(struct nbpf_device *nbpf, int n)\n{\n\tstruct dma_device *dma_dev = &nbpf->dma_dev;\n\tstruct nbpf_channel *chan = nbpf->chan + n;\n\tint ret;\n\n\tchan->nbpf = nbpf;\n\tchan->base = nbpf->base + NBPF_REG_CHAN_OFFSET + NBPF_REG_CHAN_SIZE * n;\n\tINIT_LIST_HEAD(&chan->desc_page);\n\tspin_lock_init(&chan->lock);\n\tchan->dma_chan.device = dma_dev;\n\tdma_cookie_init(&chan->dma_chan);\n\tnbpf_chan_prepare_default(chan);\n\n\tdev_dbg(dma_dev->dev, \"%s(): channel %d: -> %p\\n\", __func__, n, chan->base);\n\n\tsnprintf(chan->name, sizeof(chan->name), \"nbpf %d\", n);\n\n\ttasklet_setup(&chan->tasklet, nbpf_chan_tasklet);\n\tret = devm_request_irq(dma_dev->dev, chan->irq,\n\t\t\tnbpf_chan_irq, IRQF_SHARED,\n\t\t\tchan->name, chan);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tlist_add_tail(&chan->dma_chan.device_node,\n\t\t      &dma_dev->channels);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id nbpf_match[] = {\n\t{.compatible = \"renesas,nbpfaxi64dmac1b4\",\t.data = &nbpf_cfg[NBPF1B4]},\n\t{.compatible = \"renesas,nbpfaxi64dmac1b8\",\t.data = &nbpf_cfg[NBPF1B8]},\n\t{.compatible = \"renesas,nbpfaxi64dmac1b16\",\t.data = &nbpf_cfg[NBPF1B16]},\n\t{.compatible = \"renesas,nbpfaxi64dmac4b4\",\t.data = &nbpf_cfg[NBPF4B4]},\n\t{.compatible = \"renesas,nbpfaxi64dmac4b8\",\t.data = &nbpf_cfg[NBPF4B8]},\n\t{.compatible = \"renesas,nbpfaxi64dmac4b16\",\t.data = &nbpf_cfg[NBPF4B16]},\n\t{.compatible = \"renesas,nbpfaxi64dmac8b4\",\t.data = &nbpf_cfg[NBPF8B4]},\n\t{.compatible = \"renesas,nbpfaxi64dmac8b8\",\t.data = &nbpf_cfg[NBPF8B8]},\n\t{.compatible = \"renesas,nbpfaxi64dmac8b16\",\t.data = &nbpf_cfg[NBPF8B16]},\n\t{}\n};\nMODULE_DEVICE_TABLE(of, nbpf_match);\n\nstatic int nbpf_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct device_node *np = dev->of_node;\n\tstruct nbpf_device *nbpf;\n\tstruct dma_device *dma_dev;\n\tconst struct nbpf_config *cfg;\n\tint num_channels;\n\tint ret, irq, eirq, i;\n\tint irqbuf[9]  ;\n\tunsigned int irqs = 0;\n\n\tBUILD_BUG_ON(sizeof(struct nbpf_desc_page) > PAGE_SIZE);\n\n\t \n\tif (!np)\n\t\treturn -ENODEV;\n\n\tcfg = of_device_get_match_data(dev);\n\tnum_channels = cfg->num_channels;\n\n\tnbpf = devm_kzalloc(dev, struct_size(nbpf, chan, num_channels),\n\t\t\t    GFP_KERNEL);\n\tif (!nbpf)\n\t\treturn -ENOMEM;\n\n\tdma_dev = &nbpf->dma_dev;\n\tdma_dev->dev = dev;\n\n\tnbpf->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(nbpf->base))\n\t\treturn PTR_ERR(nbpf->base);\n\n\tnbpf->clk = devm_clk_get(dev, NULL);\n\tif (IS_ERR(nbpf->clk))\n\t\treturn PTR_ERR(nbpf->clk);\n\n\tof_property_read_u32(np, \"max-burst-mem-read\",\n\t\t\t     &nbpf->max_burst_mem_read);\n\tof_property_read_u32(np, \"max-burst-mem-write\",\n\t\t\t     &nbpf->max_burst_mem_write);\n\n\tnbpf->config = cfg;\n\n\tfor (i = 0; irqs < ARRAY_SIZE(irqbuf); i++) {\n\t\tirq = platform_get_irq_optional(pdev, i);\n\t\tif (irq < 0 && irq != -ENXIO)\n\t\t\treturn irq;\n\t\tif (irq > 0)\n\t\t\tirqbuf[irqs++] = irq;\n\t}\n\n\t \n\tif (irqs != 1 && irqs != 2 && irqs != num_channels + 1)\n\t\treturn -ENXIO;\n\n\tif (irqs == 1) {\n\t\teirq = irqbuf[0];\n\n\t\tfor (i = 0; i <= num_channels; i++)\n\t\t\tnbpf->chan[i].irq = irqbuf[0];\n\t} else {\n\t\teirq = platform_get_irq_byname(pdev, \"error\");\n\t\tif (eirq < 0)\n\t\t\treturn eirq;\n\n\t\tif (irqs == num_channels + 1) {\n\t\t\tstruct nbpf_channel *chan;\n\n\t\t\tfor (i = 0, chan = nbpf->chan; i <= num_channels;\n\t\t\t     i++, chan++) {\n\t\t\t\t \n\t\t\t\tif (irqbuf[i] == eirq)\n\t\t\t\t\ti++;\n\t\t\t\tchan->irq = irqbuf[i];\n\t\t\t}\n\n\t\t\tif (chan != nbpf->chan + num_channels)\n\t\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\t \n\t\t\tif (irqbuf[0] == eirq)\n\t\t\t\tirq = irqbuf[1];\n\t\t\telse\n\t\t\t\tirq = irqbuf[0];\n\n\t\t\tfor (i = 0; i <= num_channels; i++)\n\t\t\t\tnbpf->chan[i].irq = irq;\n\t\t}\n\t}\n\n\tret = devm_request_irq(dev, eirq, nbpf_err_irq,\n\t\t\t       IRQF_SHARED, \"dma error\", nbpf);\n\tif (ret < 0)\n\t\treturn ret;\n\tnbpf->eirq = eirq;\n\n\tINIT_LIST_HEAD(&dma_dev->channels);\n\n\t \n\tfor (i = 0; i < num_channels; i++) {\n\t\tret = nbpf_chan_probe(nbpf, i);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tdma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);\n\tdma_cap_set(DMA_SLAVE, dma_dev->cap_mask);\n\tdma_cap_set(DMA_PRIVATE, dma_dev->cap_mask);\n\n\t \n\tdma_dev->device_alloc_chan_resources\n\t\t= nbpf_alloc_chan_resources;\n\tdma_dev->device_free_chan_resources = nbpf_free_chan_resources;\n\tdma_dev->device_prep_dma_memcpy = nbpf_prep_memcpy;\n\tdma_dev->device_tx_status = nbpf_tx_status;\n\tdma_dev->device_issue_pending = nbpf_issue_pending;\n\n\t \n\n\t \n\tdma_dev->device_prep_slave_sg = nbpf_prep_slave_sg;\n\tdma_dev->device_config = nbpf_config;\n\tdma_dev->device_pause = nbpf_pause;\n\tdma_dev->device_terminate_all = nbpf_terminate_all;\n\n\tdma_dev->src_addr_widths = NBPF_DMA_BUSWIDTHS;\n\tdma_dev->dst_addr_widths = NBPF_DMA_BUSWIDTHS;\n\tdma_dev->directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\n\n\tplatform_set_drvdata(pdev, nbpf);\n\n\tret = clk_prepare_enable(nbpf->clk);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tnbpf_configure(nbpf);\n\n\tret = dma_async_device_register(dma_dev);\n\tif (ret < 0)\n\t\tgoto e_clk_off;\n\n\tret = of_dma_controller_register(np, nbpf_of_xlate, nbpf);\n\tif (ret < 0)\n\t\tgoto e_dma_dev_unreg;\n\n\treturn 0;\n\ne_dma_dev_unreg:\n\tdma_async_device_unregister(dma_dev);\ne_clk_off:\n\tclk_disable_unprepare(nbpf->clk);\n\n\treturn ret;\n}\n\nstatic int nbpf_remove(struct platform_device *pdev)\n{\n\tstruct nbpf_device *nbpf = platform_get_drvdata(pdev);\n\tint i;\n\n\tdevm_free_irq(&pdev->dev, nbpf->eirq, nbpf);\n\n\tfor (i = 0; i < nbpf->config->num_channels; i++) {\n\t\tstruct nbpf_channel *chan = nbpf->chan + i;\n\n\t\tdevm_free_irq(&pdev->dev, chan->irq, chan);\n\n\t\ttasklet_kill(&chan->tasklet);\n\t}\n\n\tof_dma_controller_free(pdev->dev.of_node);\n\tdma_async_device_unregister(&nbpf->dma_dev);\n\tclk_disable_unprepare(nbpf->clk);\n\n\treturn 0;\n}\n\nstatic const struct platform_device_id nbpf_ids[] = {\n\t{\"nbpfaxi64dmac1b4\",\t(kernel_ulong_t)&nbpf_cfg[NBPF1B4]},\n\t{\"nbpfaxi64dmac1b8\",\t(kernel_ulong_t)&nbpf_cfg[NBPF1B8]},\n\t{\"nbpfaxi64dmac1b16\",\t(kernel_ulong_t)&nbpf_cfg[NBPF1B16]},\n\t{\"nbpfaxi64dmac4b4\",\t(kernel_ulong_t)&nbpf_cfg[NBPF4B4]},\n\t{\"nbpfaxi64dmac4b8\",\t(kernel_ulong_t)&nbpf_cfg[NBPF4B8]},\n\t{\"nbpfaxi64dmac4b16\",\t(kernel_ulong_t)&nbpf_cfg[NBPF4B16]},\n\t{\"nbpfaxi64dmac8b4\",\t(kernel_ulong_t)&nbpf_cfg[NBPF8B4]},\n\t{\"nbpfaxi64dmac8b8\",\t(kernel_ulong_t)&nbpf_cfg[NBPF8B8]},\n\t{\"nbpfaxi64dmac8b16\",\t(kernel_ulong_t)&nbpf_cfg[NBPF8B16]},\n\t{},\n};\nMODULE_DEVICE_TABLE(platform, nbpf_ids);\n\n#ifdef CONFIG_PM\nstatic int nbpf_runtime_suspend(struct device *dev)\n{\n\tstruct nbpf_device *nbpf = dev_get_drvdata(dev);\n\tclk_disable_unprepare(nbpf->clk);\n\treturn 0;\n}\n\nstatic int nbpf_runtime_resume(struct device *dev)\n{\n\tstruct nbpf_device *nbpf = dev_get_drvdata(dev);\n\treturn clk_prepare_enable(nbpf->clk);\n}\n#endif\n\nstatic const struct dev_pm_ops nbpf_pm_ops = {\n\tSET_RUNTIME_PM_OPS(nbpf_runtime_suspend, nbpf_runtime_resume, NULL)\n};\n\nstatic struct platform_driver nbpf_driver = {\n\t.driver = {\n\t\t.name = \"dma-nbpf\",\n\t\t.of_match_table = nbpf_match,\n\t\t.pm = &nbpf_pm_ops,\n\t},\n\t.id_table = nbpf_ids,\n\t.probe = nbpf_probe,\n\t.remove = nbpf_remove,\n};\n\nmodule_platform_driver(nbpf_driver);\n\nMODULE_AUTHOR(\"Guennadi Liakhovetski <g.liakhovetski@gmx.de>\");\nMODULE_DESCRIPTION(\"dmaengine driver for NBPFAXI64* DMACs\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}