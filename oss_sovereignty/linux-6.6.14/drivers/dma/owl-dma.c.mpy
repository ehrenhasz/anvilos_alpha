{
  "module_name": "owl-dma.c",
  "hash_id": "c3173d0f3aada064044f8b7a02c46ef847cd852f47109f144daff1dda1ca6396",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/owl-dma.c",
  "human_readable_source": "\n\n\n\n\n\n\n\n\n\n#include <linux/bitops.h>\n#include <linux/clk.h>\n#include <linux/delay.h>\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_dma.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include \"virt-dma.h\"\n\n#define OWL_DMA_FRAME_MAX_LENGTH\t\t0xfffff\n\n \n#define OWL_DMA_IRQ_PD0\t\t\t\t0x00\n#define OWL_DMA_IRQ_PD1\t\t\t\t0x04\n#define OWL_DMA_IRQ_PD2\t\t\t\t0x08\n#define OWL_DMA_IRQ_PD3\t\t\t\t0x0C\n#define OWL_DMA_IRQ_EN0\t\t\t\t0x10\n#define OWL_DMA_IRQ_EN1\t\t\t\t0x14\n#define OWL_DMA_IRQ_EN2\t\t\t\t0x18\n#define OWL_DMA_IRQ_EN3\t\t\t\t0x1C\n#define OWL_DMA_SECURE_ACCESS_CTL\t\t0x20\n#define OWL_DMA_NIC_QOS\t\t\t\t0x24\n#define OWL_DMA_DBGSEL\t\t\t\t0x28\n#define OWL_DMA_IDLE_STAT\t\t\t0x2C\n\n \n#define OWL_DMA_CHAN_BASE(i)\t\t\t(0x100 + (i) * 0x100)\n#define OWL_DMAX_MODE\t\t\t\t0x00\n#define OWL_DMAX_SOURCE\t\t\t\t0x04\n#define OWL_DMAX_DESTINATION\t\t\t0x08\n#define OWL_DMAX_FRAME_LEN\t\t\t0x0C\n#define OWL_DMAX_FRAME_CNT\t\t\t0x10\n#define OWL_DMAX_REMAIN_FRAME_CNT\t\t0x14\n#define OWL_DMAX_REMAIN_CNT\t\t\t0x18\n#define OWL_DMAX_SOURCE_STRIDE\t\t\t0x1C\n#define OWL_DMAX_DESTINATION_STRIDE\t\t0x20\n#define OWL_DMAX_START\t\t\t\t0x24\n#define OWL_DMAX_PAUSE\t\t\t\t0x28\n#define OWL_DMAX_CHAINED_CTL\t\t\t0x2C\n#define OWL_DMAX_CONSTANT\t\t\t0x30\n#define OWL_DMAX_LINKLIST_CTL\t\t\t0x34\n#define OWL_DMAX_NEXT_DESCRIPTOR\t\t0x38\n#define OWL_DMAX_CURRENT_DESCRIPTOR_NUM\t\t0x3C\n#define OWL_DMAX_INT_CTL\t\t\t0x40\n#define OWL_DMAX_INT_STATUS\t\t\t0x44\n#define OWL_DMAX_CURRENT_SOURCE_POINTER\t\t0x48\n#define OWL_DMAX_CURRENT_DESTINATION_POINTER\t0x4C\n\n \n#define OWL_DMA_MODE_TS(x)\t\t\t(((x) & GENMASK(5, 0)) << 0)\n#define OWL_DMA_MODE_ST(x)\t\t\t(((x) & GENMASK(1, 0)) << 8)\n#define\tOWL_DMA_MODE_ST_DEV\t\t\tOWL_DMA_MODE_ST(0)\n#define\tOWL_DMA_MODE_ST_DCU\t\t\tOWL_DMA_MODE_ST(2)\n#define\tOWL_DMA_MODE_ST_SRAM\t\t\tOWL_DMA_MODE_ST(3)\n#define OWL_DMA_MODE_DT(x)\t\t\t(((x) & GENMASK(1, 0)) << 10)\n#define\tOWL_DMA_MODE_DT_DEV\t\t\tOWL_DMA_MODE_DT(0)\n#define\tOWL_DMA_MODE_DT_DCU\t\t\tOWL_DMA_MODE_DT(2)\n#define\tOWL_DMA_MODE_DT_SRAM\t\t\tOWL_DMA_MODE_DT(3)\n#define OWL_DMA_MODE_SAM(x)\t\t\t(((x) & GENMASK(1, 0)) << 16)\n#define\tOWL_DMA_MODE_SAM_CONST\t\t\tOWL_DMA_MODE_SAM(0)\n#define\tOWL_DMA_MODE_SAM_INC\t\t\tOWL_DMA_MODE_SAM(1)\n#define\tOWL_DMA_MODE_SAM_STRIDE\t\t\tOWL_DMA_MODE_SAM(2)\n#define OWL_DMA_MODE_DAM(x)\t\t\t(((x) & GENMASK(1, 0)) << 18)\n#define\tOWL_DMA_MODE_DAM_CONST\t\t\tOWL_DMA_MODE_DAM(0)\n#define\tOWL_DMA_MODE_DAM_INC\t\t\tOWL_DMA_MODE_DAM(1)\n#define\tOWL_DMA_MODE_DAM_STRIDE\t\t\tOWL_DMA_MODE_DAM(2)\n#define OWL_DMA_MODE_PW(x)\t\t\t(((x) & GENMASK(2, 0)) << 20)\n#define OWL_DMA_MODE_CB\t\t\t\tBIT(23)\n#define OWL_DMA_MODE_NDDBW(x)\t\t\t(((x) & 0x1) << 28)\n#define\tOWL_DMA_MODE_NDDBW_32BIT\t\tOWL_DMA_MODE_NDDBW(0)\n#define\tOWL_DMA_MODE_NDDBW_8BIT\t\t\tOWL_DMA_MODE_NDDBW(1)\n#define OWL_DMA_MODE_CFE\t\t\tBIT(29)\n#define OWL_DMA_MODE_LME\t\t\tBIT(30)\n#define OWL_DMA_MODE_CME\t\t\tBIT(31)\n\n \n#define OWL_DMA_LLC_SAV(x)\t\t\t(((x) & GENMASK(1, 0)) << 8)\n#define\tOWL_DMA_LLC_SAV_INC\t\t\tOWL_DMA_LLC_SAV(0)\n#define\tOWL_DMA_LLC_SAV_LOAD_NEXT\t\tOWL_DMA_LLC_SAV(1)\n#define\tOWL_DMA_LLC_SAV_LOAD_PREV\t\tOWL_DMA_LLC_SAV(2)\n#define OWL_DMA_LLC_DAV(x)\t\t\t(((x) & GENMASK(1, 0)) << 10)\n#define\tOWL_DMA_LLC_DAV_INC\t\t\tOWL_DMA_LLC_DAV(0)\n#define\tOWL_DMA_LLC_DAV_LOAD_NEXT\t\tOWL_DMA_LLC_DAV(1)\n#define\tOWL_DMA_LLC_DAV_LOAD_PREV\t\tOWL_DMA_LLC_DAV(2)\n#define OWL_DMA_LLC_SUSPEND\t\t\tBIT(16)\n\n \n#define OWL_DMA_INTCTL_BLOCK\t\t\tBIT(0)\n#define OWL_DMA_INTCTL_SUPER_BLOCK\t\tBIT(1)\n#define OWL_DMA_INTCTL_FRAME\t\t\tBIT(2)\n#define OWL_DMA_INTCTL_HALF_FRAME\t\tBIT(3)\n#define OWL_DMA_INTCTL_LAST_FRAME\t\tBIT(4)\n\n \n#define OWL_DMA_INTSTAT_BLOCK\t\t\tBIT(0)\n#define OWL_DMA_INTSTAT_SUPER_BLOCK\t\tBIT(1)\n#define OWL_DMA_INTSTAT_FRAME\t\t\tBIT(2)\n#define OWL_DMA_INTSTAT_HALF_FRAME\t\tBIT(3)\n#define OWL_DMA_INTSTAT_LAST_FRAME\t\tBIT(4)\n\n \n#define BIT_FIELD(val, width, shift, newshift)\t\\\n\t\t((((val) >> (shift)) & ((BIT(width)) - 1)) << (newshift))\n\n \n#define FCNT_VAL\t\t\t\t0x1\n\n \nenum owl_dmadesc_offsets {\n\tOWL_DMADESC_NEXT_LLI = 0,\n\tOWL_DMADESC_SADDR,\n\tOWL_DMADESC_DADDR,\n\tOWL_DMADESC_FLEN,\n\tOWL_DMADESC_SRC_STRIDE,\n\tOWL_DMADESC_DST_STRIDE,\n\tOWL_DMADESC_CTRLA,\n\tOWL_DMADESC_CTRLB,\n\tOWL_DMADESC_CONST_NUM,\n\tOWL_DMADESC_SIZE\n};\n\nenum owl_dma_id {\n\tS900_DMA,\n\tS700_DMA,\n};\n\n \nstruct owl_dma_lli {\n\tu32\t\t\thw[OWL_DMADESC_SIZE];\n\tdma_addr_t\t\tphys;\n\tstruct list_head\tnode;\n};\n\n \nstruct owl_dma_txd {\n\tstruct virt_dma_desc\tvd;\n\tstruct list_head\tlli_list;\n\tbool\t\t\tcyclic;\n};\n\n \nstruct owl_dma_pchan {\n\tu32\t\t\tid;\n\tvoid __iomem\t\t*base;\n\tstruct owl_dma_vchan\t*vchan;\n};\n\n \nstruct owl_dma_vchan {\n\tstruct virt_dma_chan\tvc;\n\tstruct owl_dma_pchan\t*pchan;\n\tstruct owl_dma_txd\t*txd;\n\tstruct dma_slave_config cfg;\n\tu8\t\t\tdrq;\n};\n\n \nstruct owl_dma {\n\tstruct dma_device\tdma;\n\tvoid __iomem\t\t*base;\n\tstruct clk\t\t*clk;\n\tspinlock_t\t\tlock;\n\tstruct dma_pool\t\t*lli_pool;\n\tint\t\t\tirq;\n\n\tunsigned int\t\tnr_pchans;\n\tstruct owl_dma_pchan\t*pchans;\n\n\tunsigned int\t\tnr_vchans;\n\tstruct owl_dma_vchan\t*vchans;\n\tenum owl_dma_id\t\tdevid;\n};\n\nstatic void pchan_update(struct owl_dma_pchan *pchan, u32 reg,\n\t\t\t u32 val, bool state)\n{\n\tu32 regval;\n\n\tregval = readl(pchan->base + reg);\n\n\tif (state)\n\t\tregval |= val;\n\telse\n\t\tregval &= ~val;\n\n\twritel(val, pchan->base + reg);\n}\n\nstatic void pchan_writel(struct owl_dma_pchan *pchan, u32 reg, u32 data)\n{\n\twritel(data, pchan->base + reg);\n}\n\nstatic u32 pchan_readl(struct owl_dma_pchan *pchan, u32 reg)\n{\n\treturn readl(pchan->base + reg);\n}\n\nstatic void dma_update(struct owl_dma *od, u32 reg, u32 val, bool state)\n{\n\tu32 regval;\n\n\tregval = readl(od->base + reg);\n\n\tif (state)\n\t\tregval |= val;\n\telse\n\t\tregval &= ~val;\n\n\twritel(val, od->base + reg);\n}\n\nstatic void dma_writel(struct owl_dma *od, u32 reg, u32 data)\n{\n\twritel(data, od->base + reg);\n}\n\nstatic u32 dma_readl(struct owl_dma *od, u32 reg)\n{\n\treturn readl(od->base + reg);\n}\n\nstatic inline struct owl_dma *to_owl_dma(struct dma_device *dd)\n{\n\treturn container_of(dd, struct owl_dma, dma);\n}\n\nstatic struct device *chan2dev(struct dma_chan *chan)\n{\n\treturn &chan->dev->device;\n}\n\nstatic inline struct owl_dma_vchan *to_owl_vchan(struct dma_chan *chan)\n{\n\treturn container_of(chan, struct owl_dma_vchan, vc.chan);\n}\n\nstatic inline struct owl_dma_txd *to_owl_txd(struct dma_async_tx_descriptor *tx)\n{\n\treturn container_of(tx, struct owl_dma_txd, vd.tx);\n}\n\nstatic inline u32 llc_hw_ctrla(u32 mode, u32 llc_ctl)\n{\n\tu32 ctl;\n\n\tctl = BIT_FIELD(mode, 4, 28, 28) |\n\t      BIT_FIELD(mode, 8, 16, 20) |\n\t      BIT_FIELD(mode, 4, 8, 16) |\n\t      BIT_FIELD(mode, 6, 0, 10) |\n\t      BIT_FIELD(llc_ctl, 2, 10, 8) |\n\t      BIT_FIELD(llc_ctl, 2, 8, 6);\n\n\treturn ctl;\n}\n\nstatic inline u32 llc_hw_ctrlb(u32 int_ctl)\n{\n\tu32 ctl;\n\n\t \n\tctl = BIT_FIELD(int_ctl, 7, 0, 18);\n\n\treturn ctl;\n}\n\nstatic u32 llc_hw_flen(struct owl_dma_lli *lli)\n{\n\treturn lli->hw[OWL_DMADESC_FLEN] & GENMASK(19, 0);\n}\n\nstatic void owl_dma_free_lli(struct owl_dma *od,\n\t\t\t     struct owl_dma_lli *lli)\n{\n\tlist_del(&lli->node);\n\tdma_pool_free(od->lli_pool, lli, lli->phys);\n}\n\nstatic struct owl_dma_lli *owl_dma_alloc_lli(struct owl_dma *od)\n{\n\tstruct owl_dma_lli *lli;\n\tdma_addr_t phys;\n\n\tlli = dma_pool_alloc(od->lli_pool, GFP_NOWAIT, &phys);\n\tif (!lli)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&lli->node);\n\tlli->phys = phys;\n\n\treturn lli;\n}\n\nstatic struct owl_dma_lli *owl_dma_add_lli(struct owl_dma_txd *txd,\n\t\t\t\t\t   struct owl_dma_lli *prev,\n\t\t\t\t\t   struct owl_dma_lli *next,\n\t\t\t\t\t   bool is_cyclic)\n{\n\tif (!is_cyclic)\n\t\tlist_add_tail(&next->node, &txd->lli_list);\n\n\tif (prev) {\n\t\tprev->hw[OWL_DMADESC_NEXT_LLI] = next->phys;\n\t\tprev->hw[OWL_DMADESC_CTRLA] |=\n\t\t\t\t\tllc_hw_ctrla(OWL_DMA_MODE_LME, 0);\n\t}\n\n\treturn next;\n}\n\nstatic inline int owl_dma_cfg_lli(struct owl_dma_vchan *vchan,\n\t\t\t\t  struct owl_dma_lli *lli,\n\t\t\t\t  dma_addr_t src, dma_addr_t dst,\n\t\t\t\t  u32 len, enum dma_transfer_direction dir,\n\t\t\t\t  struct dma_slave_config *sconfig,\n\t\t\t\t  bool is_cyclic)\n{\n\tstruct owl_dma *od = to_owl_dma(vchan->vc.chan.device);\n\tu32 mode, ctrlb;\n\n\tmode = OWL_DMA_MODE_PW(0);\n\n\tswitch (dir) {\n\tcase DMA_MEM_TO_MEM:\n\t\tmode |= OWL_DMA_MODE_TS(0) | OWL_DMA_MODE_ST_DCU |\n\t\t\tOWL_DMA_MODE_DT_DCU | OWL_DMA_MODE_SAM_INC |\n\t\t\tOWL_DMA_MODE_DAM_INC;\n\n\t\tbreak;\n\tcase DMA_MEM_TO_DEV:\n\t\tmode |= OWL_DMA_MODE_TS(vchan->drq)\n\t\t\t| OWL_DMA_MODE_ST_DCU | OWL_DMA_MODE_DT_DEV\n\t\t\t| OWL_DMA_MODE_SAM_INC | OWL_DMA_MODE_DAM_CONST;\n\n\t\t \n\t\tif (sconfig->dst_addr_width == DMA_SLAVE_BUSWIDTH_1_BYTE)\n\t\t\tmode |= OWL_DMA_MODE_NDDBW_8BIT;\n\n\t\tbreak;\n\tcase DMA_DEV_TO_MEM:\n\t\t mode |= OWL_DMA_MODE_TS(vchan->drq)\n\t\t\t| OWL_DMA_MODE_ST_DEV | OWL_DMA_MODE_DT_DCU\n\t\t\t| OWL_DMA_MODE_SAM_CONST | OWL_DMA_MODE_DAM_INC;\n\n\t\t \n\t\tif (sconfig->src_addr_width == DMA_SLAVE_BUSWIDTH_1_BYTE)\n\t\t\tmode |= OWL_DMA_MODE_NDDBW_8BIT;\n\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tlli->hw[OWL_DMADESC_CTRLA] = llc_hw_ctrla(mode,\n\t\t\t\t\t\t  OWL_DMA_LLC_SAV_LOAD_NEXT |\n\t\t\t\t\t\t  OWL_DMA_LLC_DAV_LOAD_NEXT);\n\n\tif (is_cyclic)\n\t\tctrlb = llc_hw_ctrlb(OWL_DMA_INTCTL_BLOCK);\n\telse\n\t\tctrlb = llc_hw_ctrlb(OWL_DMA_INTCTL_SUPER_BLOCK);\n\n\tlli->hw[OWL_DMADESC_NEXT_LLI] = 0;  \n\tlli->hw[OWL_DMADESC_SADDR] = src;\n\tlli->hw[OWL_DMADESC_DADDR] = dst;\n\tlli->hw[OWL_DMADESC_SRC_STRIDE] = 0;\n\tlli->hw[OWL_DMADESC_DST_STRIDE] = 0;\n\n\tif (od->devid == S700_DMA) {\n\t\t \n\t\tlli->hw[OWL_DMADESC_FLEN] = len;\n\t\t \n\t\tlli->hw[OWL_DMADESC_CTRLB] = FCNT_VAL | ctrlb;\n\t} else {\n\t\t \n\t\tlli->hw[OWL_DMADESC_FLEN] = len | FCNT_VAL << 20;\n\t\tlli->hw[OWL_DMADESC_CTRLB] = ctrlb;\n\t}\n\n\treturn 0;\n}\n\nstatic struct owl_dma_pchan *owl_dma_get_pchan(struct owl_dma *od,\n\t\t\t\t\t       struct owl_dma_vchan *vchan)\n{\n\tstruct owl_dma_pchan *pchan = NULL;\n\tunsigned long flags;\n\tint i;\n\n\tfor (i = 0; i < od->nr_pchans; i++) {\n\t\tpchan = &od->pchans[i];\n\n\t\tspin_lock_irqsave(&od->lock, flags);\n\t\tif (!pchan->vchan) {\n\t\t\tpchan->vchan = vchan;\n\t\t\tspin_unlock_irqrestore(&od->lock, flags);\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock_irqrestore(&od->lock, flags);\n\t}\n\n\treturn pchan;\n}\n\nstatic int owl_dma_pchan_busy(struct owl_dma *od, struct owl_dma_pchan *pchan)\n{\n\tunsigned int val;\n\n\tval = dma_readl(od, OWL_DMA_IDLE_STAT);\n\n\treturn !(val & (1 << pchan->id));\n}\n\nstatic void owl_dma_terminate_pchan(struct owl_dma *od,\n\t\t\t\t    struct owl_dma_pchan *pchan)\n{\n\tunsigned long flags;\n\tu32 irq_pd;\n\n\tpchan_writel(pchan, OWL_DMAX_START, 0);\n\tpchan_update(pchan, OWL_DMAX_INT_STATUS, 0xff, false);\n\n\tspin_lock_irqsave(&od->lock, flags);\n\tdma_update(od, OWL_DMA_IRQ_EN0, (1 << pchan->id), false);\n\n\tirq_pd = dma_readl(od, OWL_DMA_IRQ_PD0);\n\tif (irq_pd & (1 << pchan->id)) {\n\t\tdev_warn(od->dma.dev,\n\t\t\t \"terminating pchan %d that still has pending irq\\n\",\n\t\t\t pchan->id);\n\t\tdma_writel(od, OWL_DMA_IRQ_PD0, (1 << pchan->id));\n\t}\n\n\tpchan->vchan = NULL;\n\n\tspin_unlock_irqrestore(&od->lock, flags);\n}\n\nstatic void owl_dma_pause_pchan(struct owl_dma_pchan *pchan)\n{\n\tpchan_writel(pchan, 1, OWL_DMAX_PAUSE);\n}\n\nstatic void owl_dma_resume_pchan(struct owl_dma_pchan *pchan)\n{\n\tpchan_writel(pchan, 0, OWL_DMAX_PAUSE);\n}\n\nstatic int owl_dma_start_next_txd(struct owl_dma_vchan *vchan)\n{\n\tstruct owl_dma *od = to_owl_dma(vchan->vc.chan.device);\n\tstruct virt_dma_desc *vd = vchan_next_desc(&vchan->vc);\n\tstruct owl_dma_pchan *pchan = vchan->pchan;\n\tstruct owl_dma_txd *txd = to_owl_txd(&vd->tx);\n\tstruct owl_dma_lli *lli;\n\tunsigned long flags;\n\tu32 int_ctl;\n\n\tlist_del(&vd->node);\n\n\tvchan->txd = txd;\n\n\t \n\twhile (owl_dma_pchan_busy(od, pchan))\n\t\tcpu_relax();\n\n\tlli = list_first_entry(&txd->lli_list,\n\t\t\t       struct owl_dma_lli, node);\n\n\tif (txd->cyclic)\n\t\tint_ctl = OWL_DMA_INTCTL_BLOCK;\n\telse\n\t\tint_ctl = OWL_DMA_INTCTL_SUPER_BLOCK;\n\n\tpchan_writel(pchan, OWL_DMAX_MODE, OWL_DMA_MODE_LME);\n\tpchan_writel(pchan, OWL_DMAX_LINKLIST_CTL,\n\t\t     OWL_DMA_LLC_SAV_LOAD_NEXT | OWL_DMA_LLC_DAV_LOAD_NEXT);\n\tpchan_writel(pchan, OWL_DMAX_NEXT_DESCRIPTOR, lli->phys);\n\tpchan_writel(pchan, OWL_DMAX_INT_CTL, int_ctl);\n\n\t \n\tpchan_update(pchan, OWL_DMAX_INT_STATUS, 0xff, false);\n\n\tspin_lock_irqsave(&od->lock, flags);\n\n\tdma_update(od, OWL_DMA_IRQ_EN0, (1 << pchan->id), true);\n\n\tspin_unlock_irqrestore(&od->lock, flags);\n\n\tdev_dbg(chan2dev(&vchan->vc.chan), \"starting pchan %d\\n\", pchan->id);\n\n\t \n\tpchan_writel(pchan, OWL_DMAX_START, 0x1);\n\n\treturn 0;\n}\n\nstatic void owl_dma_phy_free(struct owl_dma *od, struct owl_dma_vchan *vchan)\n{\n\t \n\towl_dma_terminate_pchan(od, vchan->pchan);\n\n\tvchan->pchan = NULL;\n}\n\nstatic irqreturn_t owl_dma_interrupt(int irq, void *dev_id)\n{\n\tstruct owl_dma *od = dev_id;\n\tstruct owl_dma_vchan *vchan;\n\tstruct owl_dma_pchan *pchan;\n\tunsigned long pending;\n\tint i;\n\tunsigned int global_irq_pending, chan_irq_pending;\n\n\tspin_lock(&od->lock);\n\n\tpending = dma_readl(od, OWL_DMA_IRQ_PD0);\n\n\t \n\tfor_each_set_bit(i, &pending, od->nr_pchans) {\n\t\tpchan = &od->pchans[i];\n\t\tpchan_update(pchan, OWL_DMAX_INT_STATUS, 0xff, false);\n\t}\n\n\t \n\tdma_writel(od, OWL_DMA_IRQ_PD0, pending);\n\n\t \n\tfor (i = 0; i < od->nr_pchans; i++) {\n\t\tpchan = &od->pchans[i];\n\t\tchan_irq_pending = pchan_readl(pchan, OWL_DMAX_INT_CTL) &\n\t\t\t\t   pchan_readl(pchan, OWL_DMAX_INT_STATUS);\n\n\t\t \n\t\tdma_readl(od, OWL_DMA_IRQ_PD0);\n\n\t\tglobal_irq_pending = dma_readl(od, OWL_DMA_IRQ_PD0);\n\n\t\tif (chan_irq_pending && !(global_irq_pending & BIT(i))) {\n\t\t\tdev_dbg(od->dma.dev,\n\t\t\t\t\"global and channel IRQ pending match err\\n\");\n\n\t\t\t \n\t\t\tpchan_update(pchan, OWL_DMAX_INT_STATUS,\n\t\t\t\t     0xff, false);\n\n\t\t\t \n\t\t\tpending |= BIT(i);\n\t\t}\n\t}\n\n\tspin_unlock(&od->lock);\n\n\tfor_each_set_bit(i, &pending, od->nr_pchans) {\n\t\tstruct owl_dma_txd *txd;\n\n\t\tpchan = &od->pchans[i];\n\n\t\tvchan = pchan->vchan;\n\t\tif (!vchan) {\n\t\t\tdev_warn(od->dma.dev, \"no vchan attached on pchan %d\\n\",\n\t\t\t\t pchan->id);\n\t\t\tcontinue;\n\t\t}\n\n\t\tspin_lock(&vchan->vc.lock);\n\n\t\ttxd = vchan->txd;\n\t\tif (txd) {\n\t\t\tvchan->txd = NULL;\n\n\t\t\tvchan_cookie_complete(&txd->vd);\n\n\t\t\t \n\t\t\tif (vchan_next_desc(&vchan->vc))\n\t\t\t\towl_dma_start_next_txd(vchan);\n\t\t\telse\n\t\t\t\towl_dma_phy_free(od, vchan);\n\t\t}\n\n\t\tspin_unlock(&vchan->vc.lock);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void owl_dma_free_txd(struct owl_dma *od, struct owl_dma_txd *txd)\n{\n\tstruct owl_dma_lli *lli, *_lli;\n\n\tif (unlikely(!txd))\n\t\treturn;\n\n\tlist_for_each_entry_safe(lli, _lli, &txd->lli_list, node)\n\t\towl_dma_free_lli(od, lli);\n\n\tkfree(txd);\n}\n\nstatic void owl_dma_desc_free(struct virt_dma_desc *vd)\n{\n\tstruct owl_dma *od = to_owl_dma(vd->tx.chan->device);\n\tstruct owl_dma_txd *txd = to_owl_txd(&vd->tx);\n\n\towl_dma_free_txd(od, txd);\n}\n\nstatic int owl_dma_terminate_all(struct dma_chan *chan)\n{\n\tstruct owl_dma *od = to_owl_dma(chan->device);\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&vchan->vc.lock, flags);\n\n\tif (vchan->pchan)\n\t\towl_dma_phy_free(od, vchan);\n\n\tif (vchan->txd) {\n\t\towl_dma_desc_free(&vchan->txd->vd);\n\t\tvchan->txd = NULL;\n\t}\n\n\tvchan_get_all_descriptors(&vchan->vc, &head);\n\n\tspin_unlock_irqrestore(&vchan->vc.lock, flags);\n\n\tvchan_dma_desc_free_list(&vchan->vc, &head);\n\n\treturn 0;\n}\n\nstatic int owl_dma_config(struct dma_chan *chan,\n\t\t\t  struct dma_slave_config *config)\n{\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\n\t \n\tif (config->src_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES ||\n\t    config->dst_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES)\n\t\treturn -EINVAL;\n\n\tmemcpy(&vchan->cfg, config, sizeof(struct dma_slave_config));\n\n\treturn 0;\n}\n\nstatic int owl_dma_pause(struct dma_chan *chan)\n{\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&vchan->vc.lock, flags);\n\n\towl_dma_pause_pchan(vchan->pchan);\n\n\tspin_unlock_irqrestore(&vchan->vc.lock, flags);\n\n\treturn 0;\n}\n\nstatic int owl_dma_resume(struct dma_chan *chan)\n{\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\tunsigned long flags;\n\n\tif (!vchan->pchan && !vchan->txd)\n\t\treturn 0;\n\n\tdev_dbg(chan2dev(chan), \"vchan %p: resume\\n\", &vchan->vc);\n\n\tspin_lock_irqsave(&vchan->vc.lock, flags);\n\n\towl_dma_resume_pchan(vchan->pchan);\n\n\tspin_unlock_irqrestore(&vchan->vc.lock, flags);\n\n\treturn 0;\n}\n\nstatic u32 owl_dma_getbytes_chan(struct owl_dma_vchan *vchan)\n{\n\tstruct owl_dma_pchan *pchan;\n\tstruct owl_dma_txd *txd;\n\tstruct owl_dma_lli *lli;\n\tunsigned int next_lli_phy;\n\tsize_t bytes;\n\n\tpchan = vchan->pchan;\n\ttxd = vchan->txd;\n\n\tif (!pchan || !txd)\n\t\treturn 0;\n\n\t \n\tbytes = pchan_readl(pchan, OWL_DMAX_REMAIN_CNT);\n\n\t \n\tif (pchan_readl(pchan, OWL_DMAX_MODE) & OWL_DMA_MODE_LME) {\n\t\tnext_lli_phy = pchan_readl(pchan, OWL_DMAX_NEXT_DESCRIPTOR);\n\t\tlist_for_each_entry(lli, &txd->lli_list, node) {\n\t\t\t \n\t\t\tif (lli->phys == next_lli_phy) {\n\t\t\t\tlist_for_each_entry(lli, &txd->lli_list, node)\n\t\t\t\t\tbytes += llc_hw_flen(lli);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn bytes;\n}\n\nstatic enum dma_status owl_dma_tx_status(struct dma_chan *chan,\n\t\t\t\t\t dma_cookie_t cookie,\n\t\t\t\t\t struct dma_tx_state *state)\n{\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\tstruct owl_dma_lli *lli;\n\tstruct virt_dma_desc *vd;\n\tstruct owl_dma_txd *txd;\n\tenum dma_status ret;\n\tunsigned long flags;\n\tsize_t bytes = 0;\n\n\tret = dma_cookie_status(chan, cookie, state);\n\tif (ret == DMA_COMPLETE || !state)\n\t\treturn ret;\n\n\tspin_lock_irqsave(&vchan->vc.lock, flags);\n\n\tvd = vchan_find_desc(&vchan->vc, cookie);\n\tif (vd) {\n\t\ttxd = to_owl_txd(&vd->tx);\n\t\tlist_for_each_entry(lli, &txd->lli_list, node)\n\t\t\tbytes += llc_hw_flen(lli);\n\t} else {\n\t\tbytes = owl_dma_getbytes_chan(vchan);\n\t}\n\n\tspin_unlock_irqrestore(&vchan->vc.lock, flags);\n\n\tdma_set_residue(state, bytes);\n\n\treturn ret;\n}\n\nstatic void owl_dma_phy_alloc_and_start(struct owl_dma_vchan *vchan)\n{\n\tstruct owl_dma *od = to_owl_dma(vchan->vc.chan.device);\n\tstruct owl_dma_pchan *pchan;\n\n\tpchan = owl_dma_get_pchan(od, vchan);\n\tif (!pchan)\n\t\treturn;\n\n\tdev_dbg(od->dma.dev, \"allocated pchan %d\\n\", pchan->id);\n\n\tvchan->pchan = pchan;\n\towl_dma_start_next_txd(vchan);\n}\n\nstatic void owl_dma_issue_pending(struct dma_chan *chan)\n{\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&vchan->vc.lock, flags);\n\tif (vchan_issue_pending(&vchan->vc)) {\n\t\tif (!vchan->pchan)\n\t\t\towl_dma_phy_alloc_and_start(vchan);\n\t}\n\tspin_unlock_irqrestore(&vchan->vc.lock, flags);\n}\n\nstatic struct dma_async_tx_descriptor\n\t\t*owl_dma_prep_memcpy(struct dma_chan *chan,\n\t\t\t\t     dma_addr_t dst, dma_addr_t src,\n\t\t\t\t     size_t len, unsigned long flags)\n{\n\tstruct owl_dma *od = to_owl_dma(chan->device);\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\tstruct owl_dma_txd *txd;\n\tstruct owl_dma_lli *lli, *prev = NULL;\n\tsize_t offset, bytes;\n\tint ret;\n\n\tif (!len)\n\t\treturn NULL;\n\n\ttxd = kzalloc(sizeof(*txd), GFP_NOWAIT);\n\tif (!txd)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&txd->lli_list);\n\n\t \n\tfor (offset = 0; offset < len; offset += bytes) {\n\t\tlli = owl_dma_alloc_lli(od);\n\t\tif (!lli) {\n\t\t\tdev_warn(chan2dev(chan), \"failed to allocate lli\\n\");\n\t\t\tgoto err_txd_free;\n\t\t}\n\n\t\tbytes = min_t(size_t, (len - offset), OWL_DMA_FRAME_MAX_LENGTH);\n\n\t\tret = owl_dma_cfg_lli(vchan, lli, src + offset, dst + offset,\n\t\t\t\t      bytes, DMA_MEM_TO_MEM,\n\t\t\t\t      &vchan->cfg, txd->cyclic);\n\t\tif (ret) {\n\t\t\tdev_warn(chan2dev(chan), \"failed to config lli\\n\");\n\t\t\tgoto err_txd_free;\n\t\t}\n\n\t\tprev = owl_dma_add_lli(txd, prev, lli, false);\n\t}\n\n\treturn vchan_tx_prep(&vchan->vc, &txd->vd, flags);\n\nerr_txd_free:\n\towl_dma_free_txd(od, txd);\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor\n\t\t*owl_dma_prep_slave_sg(struct dma_chan *chan,\n\t\t\t\t       struct scatterlist *sgl,\n\t\t\t\t       unsigned int sg_len,\n\t\t\t\t       enum dma_transfer_direction dir,\n\t\t\t\t       unsigned long flags, void *context)\n{\n\tstruct owl_dma *od = to_owl_dma(chan->device);\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\tstruct dma_slave_config *sconfig = &vchan->cfg;\n\tstruct owl_dma_txd *txd;\n\tstruct owl_dma_lli *lli, *prev = NULL;\n\tstruct scatterlist *sg;\n\tdma_addr_t addr, src = 0, dst = 0;\n\tsize_t len;\n\tint ret, i;\n\n\ttxd = kzalloc(sizeof(*txd), GFP_NOWAIT);\n\tif (!txd)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&txd->lli_list);\n\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\taddr = sg_dma_address(sg);\n\t\tlen = sg_dma_len(sg);\n\n\t\tif (len > OWL_DMA_FRAME_MAX_LENGTH) {\n\t\t\tdev_err(od->dma.dev,\n\t\t\t\t\"frame length exceeds max supported length\");\n\t\t\tgoto err_txd_free;\n\t\t}\n\n\t\tlli = owl_dma_alloc_lli(od);\n\t\tif (!lli) {\n\t\t\tdev_err(chan2dev(chan), \"failed to allocate lli\");\n\t\t\tgoto err_txd_free;\n\t\t}\n\n\t\tif (dir == DMA_MEM_TO_DEV) {\n\t\t\tsrc = addr;\n\t\t\tdst = sconfig->dst_addr;\n\t\t} else {\n\t\t\tsrc = sconfig->src_addr;\n\t\t\tdst = addr;\n\t\t}\n\n\t\tret = owl_dma_cfg_lli(vchan, lli, src, dst, len, dir, sconfig,\n\t\t\t\t      txd->cyclic);\n\t\tif (ret) {\n\t\t\tdev_warn(chan2dev(chan), \"failed to config lli\");\n\t\t\tgoto err_txd_free;\n\t\t}\n\n\t\tprev = owl_dma_add_lli(txd, prev, lli, false);\n\t}\n\n\treturn vchan_tx_prep(&vchan->vc, &txd->vd, flags);\n\nerr_txd_free:\n\towl_dma_free_txd(od, txd);\n\n\treturn NULL;\n}\n\nstatic struct dma_async_tx_descriptor\n\t\t*owl_prep_dma_cyclic(struct dma_chan *chan,\n\t\t\t\t     dma_addr_t buf_addr, size_t buf_len,\n\t\t\t\t     size_t period_len,\n\t\t\t\t     enum dma_transfer_direction dir,\n\t\t\t\t     unsigned long flags)\n{\n\tstruct owl_dma *od = to_owl_dma(chan->device);\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\tstruct dma_slave_config *sconfig = &vchan->cfg;\n\tstruct owl_dma_txd *txd;\n\tstruct owl_dma_lli *lli, *prev = NULL, *first = NULL;\n\tdma_addr_t src = 0, dst = 0;\n\tunsigned int periods = buf_len / period_len;\n\tint ret, i;\n\n\ttxd = kzalloc(sizeof(*txd), GFP_NOWAIT);\n\tif (!txd)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&txd->lli_list);\n\ttxd->cyclic = true;\n\n\tfor (i = 0; i < periods; i++) {\n\t\tlli = owl_dma_alloc_lli(od);\n\t\tif (!lli) {\n\t\t\tdev_warn(chan2dev(chan), \"failed to allocate lli\");\n\t\t\tgoto err_txd_free;\n\t\t}\n\n\t\tif (dir == DMA_MEM_TO_DEV) {\n\t\t\tsrc = buf_addr + (period_len * i);\n\t\t\tdst = sconfig->dst_addr;\n\t\t} else if (dir == DMA_DEV_TO_MEM) {\n\t\t\tsrc = sconfig->src_addr;\n\t\t\tdst = buf_addr + (period_len * i);\n\t\t}\n\n\t\tret = owl_dma_cfg_lli(vchan, lli, src, dst, period_len,\n\t\t\t\t      dir, sconfig, txd->cyclic);\n\t\tif (ret) {\n\t\t\tdev_warn(chan2dev(chan), \"failed to config lli\");\n\t\t\tgoto err_txd_free;\n\t\t}\n\n\t\tif (!first)\n\t\t\tfirst = lli;\n\n\t\tprev = owl_dma_add_lli(txd, prev, lli, false);\n\t}\n\n\t \n\towl_dma_add_lli(txd, prev, first, true);\n\n\treturn vchan_tx_prep(&vchan->vc, &txd->vd, flags);\n\nerr_txd_free:\n\towl_dma_free_txd(od, txd);\n\n\treturn NULL;\n}\n\nstatic void owl_dma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct owl_dma_vchan *vchan = to_owl_vchan(chan);\n\n\t \n\tvchan_free_chan_resources(&vchan->vc);\n}\n\nstatic inline void owl_dma_free(struct owl_dma *od)\n{\n\tstruct owl_dma_vchan *vchan = NULL;\n\tstruct owl_dma_vchan *next;\n\n\tlist_for_each_entry_safe(vchan,\n\t\t\t\t next, &od->dma.channels, vc.chan.device_node) {\n\t\tlist_del(&vchan->vc.chan.device_node);\n\t\ttasklet_kill(&vchan->vc.task);\n\t}\n}\n\nstatic struct dma_chan *owl_dma_of_xlate(struct of_phandle_args *dma_spec,\n\t\t\t\t\t struct of_dma *ofdma)\n{\n\tstruct owl_dma *od = ofdma->of_dma_data;\n\tstruct owl_dma_vchan *vchan;\n\tstruct dma_chan *chan;\n\tu8 drq = dma_spec->args[0];\n\n\tif (drq > od->nr_vchans)\n\t\treturn NULL;\n\n\tchan = dma_get_any_slave_channel(&od->dma);\n\tif (!chan)\n\t\treturn NULL;\n\n\tvchan = to_owl_vchan(chan);\n\tvchan->drq = drq;\n\n\treturn chan;\n}\n\nstatic const struct of_device_id owl_dma_match[] = {\n\t{ .compatible = \"actions,s500-dma\", .data = (void *)S900_DMA,},\n\t{ .compatible = \"actions,s700-dma\", .data = (void *)S700_DMA,},\n\t{ .compatible = \"actions,s900-dma\", .data = (void *)S900_DMA,},\n\t{   },\n};\nMODULE_DEVICE_TABLE(of, owl_dma_match);\n\nstatic int owl_dma_probe(struct platform_device *pdev)\n{\n\tstruct device_node *np = pdev->dev.of_node;\n\tstruct owl_dma *od;\n\tint ret, i, nr_channels, nr_requests;\n\n\tod = devm_kzalloc(&pdev->dev, sizeof(*od), GFP_KERNEL);\n\tif (!od)\n\t\treturn -ENOMEM;\n\n\tod->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(od->base))\n\t\treturn PTR_ERR(od->base);\n\n\tret = of_property_read_u32(np, \"dma-channels\", &nr_channels);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"can't get dma-channels\\n\");\n\t\treturn ret;\n\t}\n\n\tret = of_property_read_u32(np, \"dma-requests\", &nr_requests);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"can't get dma-requests\\n\");\n\t\treturn ret;\n\t}\n\n\tdev_info(&pdev->dev, \"dma-channels %d, dma-requests %d\\n\",\n\t\t nr_channels, nr_requests);\n\n\tod->devid = (uintptr_t)of_device_get_match_data(&pdev->dev);\n\n\tod->nr_pchans = nr_channels;\n\tod->nr_vchans = nr_requests;\n\n\tpdev->dev.coherent_dma_mask = DMA_BIT_MASK(32);\n\n\tplatform_set_drvdata(pdev, od);\n\tspin_lock_init(&od->lock);\n\n\tdma_cap_set(DMA_MEMCPY, od->dma.cap_mask);\n\tdma_cap_set(DMA_SLAVE, od->dma.cap_mask);\n\tdma_cap_set(DMA_CYCLIC, od->dma.cap_mask);\n\n\tod->dma.dev = &pdev->dev;\n\tod->dma.device_free_chan_resources = owl_dma_free_chan_resources;\n\tod->dma.device_tx_status = owl_dma_tx_status;\n\tod->dma.device_issue_pending = owl_dma_issue_pending;\n\tod->dma.device_prep_dma_memcpy = owl_dma_prep_memcpy;\n\tod->dma.device_prep_slave_sg = owl_dma_prep_slave_sg;\n\tod->dma.device_prep_dma_cyclic = owl_prep_dma_cyclic;\n\tod->dma.device_config = owl_dma_config;\n\tod->dma.device_pause = owl_dma_pause;\n\tod->dma.device_resume = owl_dma_resume;\n\tod->dma.device_terminate_all = owl_dma_terminate_all;\n\tod->dma.src_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\tod->dma.dst_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\n\tod->dma.directions = BIT(DMA_MEM_TO_MEM);\n\tod->dma.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\n\n\tINIT_LIST_HEAD(&od->dma.channels);\n\n\tod->clk = devm_clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(od->clk)) {\n\t\tdev_err(&pdev->dev, \"unable to get clock\\n\");\n\t\treturn PTR_ERR(od->clk);\n\t}\n\n\t \n\tod->irq = platform_get_irq(pdev, 0);\n\tret = devm_request_irq(&pdev->dev, od->irq, owl_dma_interrupt, 0,\n\t\t\t       dev_name(&pdev->dev), od);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"unable to request IRQ\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tod->pchans = devm_kcalloc(&pdev->dev, od->nr_pchans,\n\t\t\t\t  sizeof(struct owl_dma_pchan), GFP_KERNEL);\n\tif (!od->pchans)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < od->nr_pchans; i++) {\n\t\tstruct owl_dma_pchan *pchan = &od->pchans[i];\n\n\t\tpchan->id = i;\n\t\tpchan->base = od->base + OWL_DMA_CHAN_BASE(i);\n\t}\n\n\t \n\tod->vchans = devm_kcalloc(&pdev->dev, od->nr_vchans,\n\t\t\t\t  sizeof(struct owl_dma_vchan), GFP_KERNEL);\n\tif (!od->vchans)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < od->nr_vchans; i++) {\n\t\tstruct owl_dma_vchan *vchan = &od->vchans[i];\n\n\t\tvchan->vc.desc_free = owl_dma_desc_free;\n\t\tvchan_init(&vchan->vc, &od->dma);\n\t}\n\n\t \n\tod->lli_pool = dma_pool_create(dev_name(od->dma.dev), od->dma.dev,\n\t\t\t\t       sizeof(struct owl_dma_lli),\n\t\t\t\t       __alignof__(struct owl_dma_lli),\n\t\t\t\t       0);\n\tif (!od->lli_pool) {\n\t\tdev_err(&pdev->dev, \"unable to allocate DMA descriptor pool\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tclk_prepare_enable(od->clk);\n\n\tret = dma_async_device_register(&od->dma);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to register DMA engine device\\n\");\n\t\tgoto err_pool_free;\n\t}\n\n\t \n\tret = of_dma_controller_register(pdev->dev.of_node,\n\t\t\t\t\t owl_dma_of_xlate, od);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"of_dma_controller_register failed\\n\");\n\t\tgoto err_dma_unregister;\n\t}\n\n\treturn 0;\n\nerr_dma_unregister:\n\tdma_async_device_unregister(&od->dma);\nerr_pool_free:\n\tclk_disable_unprepare(od->clk);\n\tdma_pool_destroy(od->lli_pool);\n\n\treturn ret;\n}\n\nstatic int owl_dma_remove(struct platform_device *pdev)\n{\n\tstruct owl_dma *od = platform_get_drvdata(pdev);\n\n\tof_dma_controller_free(pdev->dev.of_node);\n\tdma_async_device_unregister(&od->dma);\n\n\t \n\tdma_writel(od, OWL_DMA_IRQ_EN0, 0x0);\n\n\t \n\tdevm_free_irq(od->dma.dev, od->irq, od);\n\n\towl_dma_free(od);\n\n\tclk_disable_unprepare(od->clk);\n\tdma_pool_destroy(od->lli_pool);\n\n\treturn 0;\n}\n\nstatic struct platform_driver owl_dma_driver = {\n\t.probe\t= owl_dma_probe,\n\t.remove\t= owl_dma_remove,\n\t.driver = {\n\t\t.name = \"dma-owl\",\n\t\t.of_match_table = of_match_ptr(owl_dma_match),\n\t},\n};\n\nstatic int owl_dma_init(void)\n{\n\treturn platform_driver_register(&owl_dma_driver);\n}\nsubsys_initcall(owl_dma_init);\n\nstatic void __exit owl_dma_exit(void)\n{\n\tplatform_driver_unregister(&owl_dma_driver);\n}\nmodule_exit(owl_dma_exit);\n\nMODULE_AUTHOR(\"David Liu <liuwei@actions-semi.com>\");\nMODULE_AUTHOR(\"Manivannan Sadhasivam <manivannan.sadhasivam@linaro.org>\");\nMODULE_DESCRIPTION(\"Actions Semi Owl SoCs DMA driver\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}