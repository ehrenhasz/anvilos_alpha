{
  "module_name": "pch_dma.c",
  "hash_id": "3c4edd2f177623bfc98ac14457bc6dca4e3272c27d159f6df332ebfb46751200",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/pch_dma.c",
  "human_readable_source": "\n \n\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/init.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/pch_dma.h>\n\n#include \"dmaengine.h\"\n\n#define DRV_NAME \"pch-dma\"\n\n#define DMA_CTL0_DISABLE\t\t0x0\n#define DMA_CTL0_SG\t\t\t0x1\n#define DMA_CTL0_ONESHOT\t\t0x2\n#define DMA_CTL0_MODE_MASK_BITS\t\t0x3\n#define DMA_CTL0_DIR_SHIFT_BITS\t\t2\n#define DMA_CTL0_BITS_PER_CH\t\t4\n\n#define DMA_CTL2_START_SHIFT_BITS\t8\n#define DMA_CTL2_IRQ_ENABLE_MASK\t((1UL << DMA_CTL2_START_SHIFT_BITS) - 1)\n\n#define DMA_STATUS_IDLE\t\t\t0x0\n#define DMA_STATUS_DESC_READ\t\t0x1\n#define DMA_STATUS_WAIT\t\t\t0x2\n#define DMA_STATUS_ACCESS\t\t0x3\n#define DMA_STATUS_BITS_PER_CH\t\t2\n#define DMA_STATUS_MASK_BITS\t\t0x3\n#define DMA_STATUS_SHIFT_BITS\t\t16\n#define DMA_STATUS_IRQ(x)\t\t(0x1 << (x))\n#define DMA_STATUS0_ERR(x)\t\t(0x1 << ((x) + 8))\n#define DMA_STATUS2_ERR(x)\t\t(0x1 << (x))\n\n#define DMA_DESC_WIDTH_SHIFT_BITS\t12\n#define DMA_DESC_WIDTH_1_BYTE\t\t(0x3 << DMA_DESC_WIDTH_SHIFT_BITS)\n#define DMA_DESC_WIDTH_2_BYTES\t\t(0x2 << DMA_DESC_WIDTH_SHIFT_BITS)\n#define DMA_DESC_WIDTH_4_BYTES\t\t(0x0 << DMA_DESC_WIDTH_SHIFT_BITS)\n#define DMA_DESC_MAX_COUNT_1_BYTE\t0x3FF\n#define DMA_DESC_MAX_COUNT_2_BYTES\t0x3FF\n#define DMA_DESC_MAX_COUNT_4_BYTES\t0x7FF\n#define DMA_DESC_END_WITHOUT_IRQ\t0x0\n#define DMA_DESC_END_WITH_IRQ\t\t0x1\n#define DMA_DESC_FOLLOW_WITHOUT_IRQ\t0x2\n#define DMA_DESC_FOLLOW_WITH_IRQ\t0x3\n\n#define MAX_CHAN_NR\t\t\t12\n\n#define DMA_MASK_CTL0_MODE\t0x33333333\n#define DMA_MASK_CTL2_MODE\t0x00003333\n\nstatic unsigned int init_nr_desc_per_channel = 64;\nmodule_param(init_nr_desc_per_channel, uint, 0644);\nMODULE_PARM_DESC(init_nr_desc_per_channel,\n\t\t \"initial descriptors per channel (default: 64)\");\n\nstruct pch_dma_desc_regs {\n\tu32\tdev_addr;\n\tu32\tmem_addr;\n\tu32\tsize;\n\tu32\tnext;\n};\n\nstruct pch_dma_regs {\n\tu32\tdma_ctl0;\n\tu32\tdma_ctl1;\n\tu32\tdma_ctl2;\n\tu32\tdma_ctl3;\n\tu32\tdma_sts0;\n\tu32\tdma_sts1;\n\tu32\tdma_sts2;\n\tu32\treserved3;\n\tstruct pch_dma_desc_regs desc[MAX_CHAN_NR];\n};\n\nstruct pch_dma_desc {\n\tstruct pch_dma_desc_regs regs;\n\tstruct dma_async_tx_descriptor txd;\n\tstruct list_head\tdesc_node;\n\tstruct list_head\ttx_list;\n};\n\nstruct pch_dma_chan {\n\tstruct dma_chan\t\tchan;\n\tvoid __iomem *membase;\n\tenum dma_transfer_direction dir;\n\tstruct tasklet_struct\ttasklet;\n\tunsigned long\t\terr_status;\n\n\tspinlock_t\t\tlock;\n\n\tstruct list_head\tactive_list;\n\tstruct list_head\tqueue;\n\tstruct list_head\tfree_list;\n\tunsigned int\t\tdescs_allocated;\n};\n\n#define PDC_DEV_ADDR\t0x00\n#define PDC_MEM_ADDR\t0x04\n#define PDC_SIZE\t0x08\n#define PDC_NEXT\t0x0C\n\n#define channel_readl(pdc, name) \\\n\treadl((pdc)->membase + PDC_##name)\n#define channel_writel(pdc, name, val) \\\n\twritel((val), (pdc)->membase + PDC_##name)\n\nstruct pch_dma {\n\tstruct dma_device\tdma;\n\tvoid __iomem *membase;\n\tstruct dma_pool\t\t*pool;\n\tstruct pch_dma_regs\tregs;\n\tstruct pch_dma_desc_regs ch_regs[MAX_CHAN_NR];\n\tstruct pch_dma_chan\tchannels[MAX_CHAN_NR];\n};\n\n#define PCH_DMA_CTL0\t0x00\n#define PCH_DMA_CTL1\t0x04\n#define PCH_DMA_CTL2\t0x08\n#define PCH_DMA_CTL3\t0x0C\n#define PCH_DMA_STS0\t0x10\n#define PCH_DMA_STS1\t0x14\n#define PCH_DMA_STS2\t0x18\n\n#define dma_readl(pd, name) \\\n\treadl((pd)->membase + PCH_DMA_##name)\n#define dma_writel(pd, name, val) \\\n\twritel((val), (pd)->membase + PCH_DMA_##name)\n\nstatic inline\nstruct pch_dma_desc *to_pd_desc(struct dma_async_tx_descriptor *txd)\n{\n\treturn container_of(txd, struct pch_dma_desc, txd);\n}\n\nstatic inline struct pch_dma_chan *to_pd_chan(struct dma_chan *chan)\n{\n\treturn container_of(chan, struct pch_dma_chan, chan);\n}\n\nstatic inline struct pch_dma *to_pd(struct dma_device *ddev)\n{\n\treturn container_of(ddev, struct pch_dma, dma);\n}\n\nstatic inline struct device *chan2dev(struct dma_chan *chan)\n{\n\treturn &chan->dev->device;\n}\n\nstatic inline struct device *chan2parent(struct dma_chan *chan)\n{\n\treturn chan->dev->device.parent;\n}\n\nstatic inline\nstruct pch_dma_desc *pdc_first_active(struct pch_dma_chan *pd_chan)\n{\n\treturn list_first_entry(&pd_chan->active_list,\n\t\t\t\tstruct pch_dma_desc, desc_node);\n}\n\nstatic inline\nstruct pch_dma_desc *pdc_first_queued(struct pch_dma_chan *pd_chan)\n{\n\treturn list_first_entry(&pd_chan->queue,\n\t\t\t\tstruct pch_dma_desc, desc_node);\n}\n\nstatic void pdc_enable_irq(struct dma_chan *chan, int enable)\n{\n\tstruct pch_dma *pd = to_pd(chan->device);\n\tu32 val;\n\tint pos;\n\n\tif (chan->chan_id < 8)\n\t\tpos = chan->chan_id;\n\telse\n\t\tpos = chan->chan_id + 8;\n\n\tval = dma_readl(pd, CTL2);\n\n\tif (enable)\n\t\tval |= 0x1 << pos;\n\telse\n\t\tval &= ~(0x1 << pos);\n\n\tdma_writel(pd, CTL2, val);\n\n\tdev_dbg(chan2dev(chan), \"pdc_enable_irq: chan %d -> %x\\n\",\n\t\tchan->chan_id, val);\n}\n\nstatic void pdc_set_dir(struct dma_chan *chan)\n{\n\tstruct pch_dma_chan *pd_chan = to_pd_chan(chan);\n\tstruct pch_dma *pd = to_pd(chan->device);\n\tu32 val;\n\tu32 mask_mode;\n\tu32 mask_ctl;\n\n\tif (chan->chan_id < 8) {\n\t\tval = dma_readl(pd, CTL0);\n\n\t\tmask_mode = DMA_CTL0_MODE_MASK_BITS <<\n\t\t\t\t\t(DMA_CTL0_BITS_PER_CH * chan->chan_id);\n\t\tmask_ctl = DMA_MASK_CTL0_MODE & ~(DMA_CTL0_MODE_MASK_BITS <<\n\t\t\t\t       (DMA_CTL0_BITS_PER_CH * chan->chan_id));\n\t\tval &= mask_mode;\n\t\tif (pd_chan->dir == DMA_MEM_TO_DEV)\n\t\t\tval |= 0x1 << (DMA_CTL0_BITS_PER_CH * chan->chan_id +\n\t\t\t\t       DMA_CTL0_DIR_SHIFT_BITS);\n\t\telse\n\t\t\tval &= ~(0x1 << (DMA_CTL0_BITS_PER_CH * chan->chan_id +\n\t\t\t\t\t DMA_CTL0_DIR_SHIFT_BITS));\n\n\t\tval |= mask_ctl;\n\t\tdma_writel(pd, CTL0, val);\n\t} else {\n\t\tint ch = chan->chan_id - 8;  \n\t\tval = dma_readl(pd, CTL3);\n\n\t\tmask_mode = DMA_CTL0_MODE_MASK_BITS <<\n\t\t\t\t\t\t(DMA_CTL0_BITS_PER_CH * ch);\n\t\tmask_ctl = DMA_MASK_CTL2_MODE & ~(DMA_CTL0_MODE_MASK_BITS <<\n\t\t\t\t\t\t (DMA_CTL0_BITS_PER_CH * ch));\n\t\tval &= mask_mode;\n\t\tif (pd_chan->dir == DMA_MEM_TO_DEV)\n\t\t\tval |= 0x1 << (DMA_CTL0_BITS_PER_CH * ch +\n\t\t\t\t       DMA_CTL0_DIR_SHIFT_BITS);\n\t\telse\n\t\t\tval &= ~(0x1 << (DMA_CTL0_BITS_PER_CH * ch +\n\t\t\t\t\t DMA_CTL0_DIR_SHIFT_BITS));\n\t\tval |= mask_ctl;\n\t\tdma_writel(pd, CTL3, val);\n\t}\n\n\tdev_dbg(chan2dev(chan), \"pdc_set_dir: chan %d -> %x\\n\",\n\t\tchan->chan_id, val);\n}\n\nstatic void pdc_set_mode(struct dma_chan *chan, u32 mode)\n{\n\tstruct pch_dma *pd = to_pd(chan->device);\n\tu32 val;\n\tu32 mask_ctl;\n\tu32 mask_dir;\n\n\tif (chan->chan_id < 8) {\n\t\tmask_ctl = DMA_MASK_CTL0_MODE & ~(DMA_CTL0_MODE_MASK_BITS <<\n\t\t\t   (DMA_CTL0_BITS_PER_CH * chan->chan_id));\n\t\tmask_dir = 1 << (DMA_CTL0_BITS_PER_CH * chan->chan_id +\\\n\t\t\t\t DMA_CTL0_DIR_SHIFT_BITS);\n\t\tval = dma_readl(pd, CTL0);\n\t\tval &= mask_dir;\n\t\tval |= mode << (DMA_CTL0_BITS_PER_CH * chan->chan_id);\n\t\tval |= mask_ctl;\n\t\tdma_writel(pd, CTL0, val);\n\t} else {\n\t\tint ch = chan->chan_id - 8;  \n\t\tmask_ctl = DMA_MASK_CTL2_MODE & ~(DMA_CTL0_MODE_MASK_BITS <<\n\t\t\t\t\t\t (DMA_CTL0_BITS_PER_CH * ch));\n\t\tmask_dir = 1 << (DMA_CTL0_BITS_PER_CH * ch +\\\n\t\t\t\t DMA_CTL0_DIR_SHIFT_BITS);\n\t\tval = dma_readl(pd, CTL3);\n\t\tval &= mask_dir;\n\t\tval |= mode << (DMA_CTL0_BITS_PER_CH * ch);\n\t\tval |= mask_ctl;\n\t\tdma_writel(pd, CTL3, val);\n\t}\n\n\tdev_dbg(chan2dev(chan), \"pdc_set_mode: chan %d -> %x\\n\",\n\t\tchan->chan_id, val);\n}\n\nstatic u32 pdc_get_status0(struct pch_dma_chan *pd_chan)\n{\n\tstruct pch_dma *pd = to_pd(pd_chan->chan.device);\n\tu32 val;\n\n\tval = dma_readl(pd, STS0);\n\treturn DMA_STATUS_MASK_BITS & (val >> (DMA_STATUS_SHIFT_BITS +\n\t\t\tDMA_STATUS_BITS_PER_CH * pd_chan->chan.chan_id));\n}\n\nstatic u32 pdc_get_status2(struct pch_dma_chan *pd_chan)\n{\n\tstruct pch_dma *pd = to_pd(pd_chan->chan.device);\n\tu32 val;\n\n\tval = dma_readl(pd, STS2);\n\treturn DMA_STATUS_MASK_BITS & (val >> (DMA_STATUS_SHIFT_BITS +\n\t\t\tDMA_STATUS_BITS_PER_CH * (pd_chan->chan.chan_id - 8)));\n}\n\nstatic bool pdc_is_idle(struct pch_dma_chan *pd_chan)\n{\n\tu32 sts;\n\n\tif (pd_chan->chan.chan_id < 8)\n\t\tsts = pdc_get_status0(pd_chan);\n\telse\n\t\tsts = pdc_get_status2(pd_chan);\n\n\n\tif (sts == DMA_STATUS_IDLE)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic void pdc_dostart(struct pch_dma_chan *pd_chan, struct pch_dma_desc* desc)\n{\n\tif (!pdc_is_idle(pd_chan)) {\n\t\tdev_err(chan2dev(&pd_chan->chan),\n\t\t\t\"BUG: Attempt to start non-idle channel\\n\");\n\t\treturn;\n\t}\n\n\tdev_dbg(chan2dev(&pd_chan->chan), \"chan %d -> dev_addr: %x\\n\",\n\t\tpd_chan->chan.chan_id, desc->regs.dev_addr);\n\tdev_dbg(chan2dev(&pd_chan->chan), \"chan %d -> mem_addr: %x\\n\",\n\t\tpd_chan->chan.chan_id, desc->regs.mem_addr);\n\tdev_dbg(chan2dev(&pd_chan->chan), \"chan %d -> size: %x\\n\",\n\t\tpd_chan->chan.chan_id, desc->regs.size);\n\tdev_dbg(chan2dev(&pd_chan->chan), \"chan %d -> next: %x\\n\",\n\t\tpd_chan->chan.chan_id, desc->regs.next);\n\n\tif (list_empty(&desc->tx_list)) {\n\t\tchannel_writel(pd_chan, DEV_ADDR, desc->regs.dev_addr);\n\t\tchannel_writel(pd_chan, MEM_ADDR, desc->regs.mem_addr);\n\t\tchannel_writel(pd_chan, SIZE, desc->regs.size);\n\t\tchannel_writel(pd_chan, NEXT, desc->regs.next);\n\t\tpdc_set_mode(&pd_chan->chan, DMA_CTL0_ONESHOT);\n\t} else {\n\t\tchannel_writel(pd_chan, NEXT, desc->txd.phys);\n\t\tpdc_set_mode(&pd_chan->chan, DMA_CTL0_SG);\n\t}\n}\n\nstatic void pdc_chain_complete(struct pch_dma_chan *pd_chan,\n\t\t\t       struct pch_dma_desc *desc)\n{\n\tstruct dma_async_tx_descriptor *txd = &desc->txd;\n\tstruct dmaengine_desc_callback cb;\n\n\tdmaengine_desc_get_callback(txd, &cb);\n\tlist_splice_init(&desc->tx_list, &pd_chan->free_list);\n\tlist_move(&desc->desc_node, &pd_chan->free_list);\n\n\tdmaengine_desc_callback_invoke(&cb, NULL);\n}\n\nstatic void pdc_complete_all(struct pch_dma_chan *pd_chan)\n{\n\tstruct pch_dma_desc *desc, *_d;\n\tLIST_HEAD(list);\n\n\tBUG_ON(!pdc_is_idle(pd_chan));\n\n\tif (!list_empty(&pd_chan->queue))\n\t\tpdc_dostart(pd_chan, pdc_first_queued(pd_chan));\n\n\tlist_splice_init(&pd_chan->active_list, &list);\n\tlist_splice_init(&pd_chan->queue, &pd_chan->active_list);\n\n\tlist_for_each_entry_safe(desc, _d, &list, desc_node)\n\t\tpdc_chain_complete(pd_chan, desc);\n}\n\nstatic void pdc_handle_error(struct pch_dma_chan *pd_chan)\n{\n\tstruct pch_dma_desc *bad_desc;\n\n\tbad_desc = pdc_first_active(pd_chan);\n\tlist_del(&bad_desc->desc_node);\n\n\tlist_splice_init(&pd_chan->queue, pd_chan->active_list.prev);\n\n\tif (!list_empty(&pd_chan->active_list))\n\t\tpdc_dostart(pd_chan, pdc_first_active(pd_chan));\n\n\tdev_crit(chan2dev(&pd_chan->chan), \"Bad descriptor submitted\\n\");\n\tdev_crit(chan2dev(&pd_chan->chan), \"descriptor cookie: %d\\n\",\n\t\t bad_desc->txd.cookie);\n\n\tpdc_chain_complete(pd_chan, bad_desc);\n}\n\nstatic void pdc_advance_work(struct pch_dma_chan *pd_chan)\n{\n\tif (list_empty(&pd_chan->active_list) ||\n\t\tlist_is_singular(&pd_chan->active_list)) {\n\t\tpdc_complete_all(pd_chan);\n\t} else {\n\t\tpdc_chain_complete(pd_chan, pdc_first_active(pd_chan));\n\t\tpdc_dostart(pd_chan, pdc_first_active(pd_chan));\n\t}\n}\n\nstatic dma_cookie_t pd_tx_submit(struct dma_async_tx_descriptor *txd)\n{\n\tstruct pch_dma_desc *desc = to_pd_desc(txd);\n\tstruct pch_dma_chan *pd_chan = to_pd_chan(txd->chan);\n\n\tspin_lock(&pd_chan->lock);\n\n\tif (list_empty(&pd_chan->active_list)) {\n\t\tlist_add_tail(&desc->desc_node, &pd_chan->active_list);\n\t\tpdc_dostart(pd_chan, desc);\n\t} else {\n\t\tlist_add_tail(&desc->desc_node, &pd_chan->queue);\n\t}\n\n\tspin_unlock(&pd_chan->lock);\n\treturn 0;\n}\n\nstatic struct pch_dma_desc *pdc_alloc_desc(struct dma_chan *chan, gfp_t flags)\n{\n\tstruct pch_dma_desc *desc = NULL;\n\tstruct pch_dma *pd = to_pd(chan->device);\n\tdma_addr_t addr;\n\n\tdesc = dma_pool_zalloc(pd->pool, flags, &addr);\n\tif (desc) {\n\t\tINIT_LIST_HEAD(&desc->tx_list);\n\t\tdma_async_tx_descriptor_init(&desc->txd, chan);\n\t\tdesc->txd.tx_submit = pd_tx_submit;\n\t\tdesc->txd.flags = DMA_CTRL_ACK;\n\t\tdesc->txd.phys = addr;\n\t}\n\n\treturn desc;\n}\n\nstatic struct pch_dma_desc *pdc_desc_get(struct pch_dma_chan *pd_chan)\n{\n\tstruct pch_dma_desc *desc, *_d;\n\tstruct pch_dma_desc *ret = NULL;\n\tint i = 0;\n\n\tspin_lock(&pd_chan->lock);\n\tlist_for_each_entry_safe(desc, _d, &pd_chan->free_list, desc_node) {\n\t\ti++;\n\t\tif (async_tx_test_ack(&desc->txd)) {\n\t\t\tlist_del(&desc->desc_node);\n\t\t\tret = desc;\n\t\t\tbreak;\n\t\t}\n\t\tdev_dbg(chan2dev(&pd_chan->chan), \"desc %p not ACKed\\n\", desc);\n\t}\n\tspin_unlock(&pd_chan->lock);\n\tdev_dbg(chan2dev(&pd_chan->chan), \"scanned %d descriptors\\n\", i);\n\n\tif (!ret) {\n\t\tret = pdc_alloc_desc(&pd_chan->chan, GFP_ATOMIC);\n\t\tif (ret) {\n\t\t\tspin_lock(&pd_chan->lock);\n\t\t\tpd_chan->descs_allocated++;\n\t\t\tspin_unlock(&pd_chan->lock);\n\t\t} else {\n\t\t\tdev_err(chan2dev(&pd_chan->chan),\n\t\t\t\t\"failed to alloc desc\\n\");\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void pdc_desc_put(struct pch_dma_chan *pd_chan,\n\t\t\t struct pch_dma_desc *desc)\n{\n\tif (desc) {\n\t\tspin_lock(&pd_chan->lock);\n\t\tlist_splice_init(&desc->tx_list, &pd_chan->free_list);\n\t\tlist_add(&desc->desc_node, &pd_chan->free_list);\n\t\tspin_unlock(&pd_chan->lock);\n\t}\n}\n\nstatic int pd_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct pch_dma_chan *pd_chan = to_pd_chan(chan);\n\tstruct pch_dma_desc *desc;\n\tLIST_HEAD(tmp_list);\n\tint i;\n\n\tif (!pdc_is_idle(pd_chan)) {\n\t\tdev_dbg(chan2dev(chan), \"DMA channel not idle ?\\n\");\n\t\treturn -EIO;\n\t}\n\n\tif (!list_empty(&pd_chan->free_list))\n\t\treturn pd_chan->descs_allocated;\n\n\tfor (i = 0; i < init_nr_desc_per_channel; i++) {\n\t\tdesc = pdc_alloc_desc(chan, GFP_KERNEL);\n\n\t\tif (!desc) {\n\t\t\tdev_warn(chan2dev(chan),\n\t\t\t\t\"Only allocated %d initial descriptors\\n\", i);\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_add_tail(&desc->desc_node, &tmp_list);\n\t}\n\n\tspin_lock_irq(&pd_chan->lock);\n\tlist_splice(&tmp_list, &pd_chan->free_list);\n\tpd_chan->descs_allocated = i;\n\tdma_cookie_init(chan);\n\tspin_unlock_irq(&pd_chan->lock);\n\n\tpdc_enable_irq(chan, 1);\n\n\treturn pd_chan->descs_allocated;\n}\n\nstatic void pd_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct pch_dma_chan *pd_chan = to_pd_chan(chan);\n\tstruct pch_dma *pd = to_pd(chan->device);\n\tstruct pch_dma_desc *desc, *_d;\n\tLIST_HEAD(tmp_list);\n\n\tBUG_ON(!pdc_is_idle(pd_chan));\n\tBUG_ON(!list_empty(&pd_chan->active_list));\n\tBUG_ON(!list_empty(&pd_chan->queue));\n\n\tspin_lock_irq(&pd_chan->lock);\n\tlist_splice_init(&pd_chan->free_list, &tmp_list);\n\tpd_chan->descs_allocated = 0;\n\tspin_unlock_irq(&pd_chan->lock);\n\n\tlist_for_each_entry_safe(desc, _d, &tmp_list, desc_node)\n\t\tdma_pool_free(pd->pool, desc, desc->txd.phys);\n\n\tpdc_enable_irq(chan, 0);\n}\n\nstatic enum dma_status pd_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\n\t\t\t\t    struct dma_tx_state *txstate)\n{\n\treturn dma_cookie_status(chan, cookie, txstate);\n}\n\nstatic void pd_issue_pending(struct dma_chan *chan)\n{\n\tstruct pch_dma_chan *pd_chan = to_pd_chan(chan);\n\n\tif (pdc_is_idle(pd_chan)) {\n\t\tspin_lock(&pd_chan->lock);\n\t\tpdc_advance_work(pd_chan);\n\t\tspin_unlock(&pd_chan->lock);\n\t}\n}\n\nstatic struct dma_async_tx_descriptor *pd_prep_slave_sg(struct dma_chan *chan,\n\t\t\tstruct scatterlist *sgl, unsigned int sg_len,\n\t\t\tenum dma_transfer_direction direction, unsigned long flags,\n\t\t\tvoid *context)\n{\n\tstruct pch_dma_chan *pd_chan = to_pd_chan(chan);\n\tstruct pch_dma_slave *pd_slave = chan->private;\n\tstruct pch_dma_desc *first = NULL;\n\tstruct pch_dma_desc *prev = NULL;\n\tstruct pch_dma_desc *desc = NULL;\n\tstruct scatterlist *sg;\n\tdma_addr_t reg;\n\tint i;\n\n\tif (unlikely(!sg_len)) {\n\t\tdev_info(chan2dev(chan), \"prep_slave_sg: length is zero!\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (direction == DMA_DEV_TO_MEM)\n\t\treg = pd_slave->rx_reg;\n\telse if (direction == DMA_MEM_TO_DEV)\n\t\treg = pd_slave->tx_reg;\n\telse\n\t\treturn NULL;\n\n\tpd_chan->dir = direction;\n\tpdc_set_dir(chan);\n\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tdesc = pdc_desc_get(pd_chan);\n\n\t\tif (!desc)\n\t\t\tgoto err_desc_get;\n\n\t\tdesc->regs.dev_addr = reg;\n\t\tdesc->regs.mem_addr = sg_dma_address(sg);\n\t\tdesc->regs.size = sg_dma_len(sg);\n\t\tdesc->regs.next = DMA_DESC_FOLLOW_WITHOUT_IRQ;\n\n\t\tswitch (pd_slave->width) {\n\t\tcase PCH_DMA_WIDTH_1_BYTE:\n\t\t\tif (desc->regs.size > DMA_DESC_MAX_COUNT_1_BYTE)\n\t\t\t\tgoto err_desc_get;\n\t\t\tdesc->regs.size |= DMA_DESC_WIDTH_1_BYTE;\n\t\t\tbreak;\n\t\tcase PCH_DMA_WIDTH_2_BYTES:\n\t\t\tif (desc->regs.size > DMA_DESC_MAX_COUNT_2_BYTES)\n\t\t\t\tgoto err_desc_get;\n\t\t\tdesc->regs.size |= DMA_DESC_WIDTH_2_BYTES;\n\t\t\tbreak;\n\t\tcase PCH_DMA_WIDTH_4_BYTES:\n\t\t\tif (desc->regs.size > DMA_DESC_MAX_COUNT_4_BYTES)\n\t\t\t\tgoto err_desc_get;\n\t\t\tdesc->regs.size |= DMA_DESC_WIDTH_4_BYTES;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto err_desc_get;\n\t\t}\n\n\t\tif (!first) {\n\t\t\tfirst = desc;\n\t\t} else {\n\t\t\tprev->regs.next |= desc->txd.phys;\n\t\t\tlist_add_tail(&desc->desc_node, &first->tx_list);\n\t\t}\n\n\t\tprev = desc;\n\t}\n\n\tif (flags & DMA_PREP_INTERRUPT)\n\t\tdesc->regs.next = DMA_DESC_END_WITH_IRQ;\n\telse\n\t\tdesc->regs.next = DMA_DESC_END_WITHOUT_IRQ;\n\n\tfirst->txd.cookie = -EBUSY;\n\tdesc->txd.flags = flags;\n\n\treturn &first->txd;\n\nerr_desc_get:\n\tdev_err(chan2dev(chan), \"failed to get desc or wrong parameters\\n\");\n\tpdc_desc_put(pd_chan, first);\n\treturn NULL;\n}\n\nstatic int pd_device_terminate_all(struct dma_chan *chan)\n{\n\tstruct pch_dma_chan *pd_chan = to_pd_chan(chan);\n\tstruct pch_dma_desc *desc, *_d;\n\tLIST_HEAD(list);\n\n\tspin_lock_irq(&pd_chan->lock);\n\n\tpdc_set_mode(&pd_chan->chan, DMA_CTL0_DISABLE);\n\n\tlist_splice_init(&pd_chan->active_list, &list);\n\tlist_splice_init(&pd_chan->queue, &list);\n\n\tlist_for_each_entry_safe(desc, _d, &list, desc_node)\n\t\tpdc_chain_complete(pd_chan, desc);\n\n\tspin_unlock_irq(&pd_chan->lock);\n\n\treturn 0;\n}\n\nstatic void pdc_tasklet(struct tasklet_struct *t)\n{\n\tstruct pch_dma_chan *pd_chan = from_tasklet(pd_chan, t, tasklet);\n\tunsigned long flags;\n\n\tif (!pdc_is_idle(pd_chan)) {\n\t\tdev_err(chan2dev(&pd_chan->chan),\n\t\t\t\"BUG: handle non-idle channel in tasklet\\n\");\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&pd_chan->lock, flags);\n\tif (test_and_clear_bit(0, &pd_chan->err_status))\n\t\tpdc_handle_error(pd_chan);\n\telse\n\t\tpdc_advance_work(pd_chan);\n\tspin_unlock_irqrestore(&pd_chan->lock, flags);\n}\n\nstatic irqreturn_t pd_irq(int irq, void *devid)\n{\n\tstruct pch_dma *pd = (struct pch_dma *)devid;\n\tstruct pch_dma_chan *pd_chan;\n\tu32 sts0;\n\tu32 sts2;\n\tint i;\n\tint ret0 = IRQ_NONE;\n\tint ret2 = IRQ_NONE;\n\n\tsts0 = dma_readl(pd, STS0);\n\tsts2 = dma_readl(pd, STS2);\n\n\tdev_dbg(pd->dma.dev, \"pd_irq sts0: %x\\n\", sts0);\n\n\tfor (i = 0; i < pd->dma.chancnt; i++) {\n\t\tpd_chan = &pd->channels[i];\n\n\t\tif (i < 8) {\n\t\t\tif (sts0 & DMA_STATUS_IRQ(i)) {\n\t\t\t\tif (sts0 & DMA_STATUS0_ERR(i))\n\t\t\t\t\tset_bit(0, &pd_chan->err_status);\n\n\t\t\t\ttasklet_schedule(&pd_chan->tasklet);\n\t\t\t\tret0 = IRQ_HANDLED;\n\t\t\t}\n\t\t} else {\n\t\t\tif (sts2 & DMA_STATUS_IRQ(i - 8)) {\n\t\t\t\tif (sts2 & DMA_STATUS2_ERR(i))\n\t\t\t\t\tset_bit(0, &pd_chan->err_status);\n\n\t\t\t\ttasklet_schedule(&pd_chan->tasklet);\n\t\t\t\tret2 = IRQ_HANDLED;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (ret0)\n\t\tdma_writel(pd, STS0, sts0);\n\tif (ret2)\n\t\tdma_writel(pd, STS2, sts2);\n\n\treturn ret0 | ret2;\n}\n\nstatic void __maybe_unused pch_dma_save_regs(struct pch_dma *pd)\n{\n\tstruct pch_dma_chan *pd_chan;\n\tstruct dma_chan *chan, *_c;\n\tint i = 0;\n\n\tpd->regs.dma_ctl0 = dma_readl(pd, CTL0);\n\tpd->regs.dma_ctl1 = dma_readl(pd, CTL1);\n\tpd->regs.dma_ctl2 = dma_readl(pd, CTL2);\n\tpd->regs.dma_ctl3 = dma_readl(pd, CTL3);\n\n\tlist_for_each_entry_safe(chan, _c, &pd->dma.channels, device_node) {\n\t\tpd_chan = to_pd_chan(chan);\n\n\t\tpd->ch_regs[i].dev_addr = channel_readl(pd_chan, DEV_ADDR);\n\t\tpd->ch_regs[i].mem_addr = channel_readl(pd_chan, MEM_ADDR);\n\t\tpd->ch_regs[i].size = channel_readl(pd_chan, SIZE);\n\t\tpd->ch_regs[i].next = channel_readl(pd_chan, NEXT);\n\n\t\ti++;\n\t}\n}\n\nstatic void __maybe_unused pch_dma_restore_regs(struct pch_dma *pd)\n{\n\tstruct pch_dma_chan *pd_chan;\n\tstruct dma_chan *chan, *_c;\n\tint i = 0;\n\n\tdma_writel(pd, CTL0, pd->regs.dma_ctl0);\n\tdma_writel(pd, CTL1, pd->regs.dma_ctl1);\n\tdma_writel(pd, CTL2, pd->regs.dma_ctl2);\n\tdma_writel(pd, CTL3, pd->regs.dma_ctl3);\n\n\tlist_for_each_entry_safe(chan, _c, &pd->dma.channels, device_node) {\n\t\tpd_chan = to_pd_chan(chan);\n\n\t\tchannel_writel(pd_chan, DEV_ADDR, pd->ch_regs[i].dev_addr);\n\t\tchannel_writel(pd_chan, MEM_ADDR, pd->ch_regs[i].mem_addr);\n\t\tchannel_writel(pd_chan, SIZE, pd->ch_regs[i].size);\n\t\tchannel_writel(pd_chan, NEXT, pd->ch_regs[i].next);\n\n\t\ti++;\n\t}\n}\n\nstatic int __maybe_unused pch_dma_suspend(struct device *dev)\n{\n\tstruct pch_dma *pd = dev_get_drvdata(dev);\n\n\tif (pd)\n\t\tpch_dma_save_regs(pd);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused pch_dma_resume(struct device *dev)\n{\n\tstruct pch_dma *pd = dev_get_drvdata(dev);\n\n\tif (pd)\n\t\tpch_dma_restore_regs(pd);\n\n\treturn 0;\n}\n\nstatic int pch_dma_probe(struct pci_dev *pdev,\n\t\t\t\t   const struct pci_device_id *id)\n{\n\tstruct pch_dma *pd;\n\tstruct pch_dma_regs *regs;\n\tunsigned int nr_channels;\n\tint err;\n\tint i;\n\n\tnr_channels = id->driver_data;\n\tpd = kzalloc(sizeof(*pd), GFP_KERNEL);\n\tif (!pd)\n\t\treturn -ENOMEM;\n\n\tpci_set_drvdata(pdev, pd);\n\n\terr = pci_enable_device(pdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot enable PCI device\\n\");\n\t\tgoto err_free_mem;\n\t}\n\n\tif (!(pci_resource_flags(pdev, 1) & IORESOURCE_MEM)) {\n\t\tdev_err(&pdev->dev, \"Cannot find proper base address\\n\");\n\t\terr = -ENODEV;\n\t\tgoto err_disable_pdev;\n\t}\n\n\terr = pci_request_regions(pdev, DRV_NAME);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot obtain PCI resources\\n\");\n\t\tgoto err_disable_pdev;\n\t}\n\n\terr = dma_set_mask(&pdev->dev, DMA_BIT_MASK(32));\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot set proper DMA config\\n\");\n\t\tgoto err_free_res;\n\t}\n\n\tregs = pd->membase = pci_iomap(pdev, 1, 0);\n\tif (!pd->membase) {\n\t\tdev_err(&pdev->dev, \"Cannot map MMIO registers\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_free_res;\n\t}\n\n\tpci_set_master(pdev);\n\tpd->dma.dev = &pdev->dev;\n\n\terr = request_irq(pdev->irq, pd_irq, IRQF_SHARED, DRV_NAME, pd);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to request IRQ\\n\");\n\t\tgoto err_iounmap;\n\t}\n\n\tpd->pool = dma_pool_create(\"pch_dma_desc_pool\", &pdev->dev,\n\t\t\t\t   sizeof(struct pch_dma_desc), 4, 0);\n\tif (!pd->pool) {\n\t\tdev_err(&pdev->dev, \"Failed to alloc DMA descriptors\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_free_irq;\n\t}\n\n\n\tINIT_LIST_HEAD(&pd->dma.channels);\n\n\tfor (i = 0; i < nr_channels; i++) {\n\t\tstruct pch_dma_chan *pd_chan = &pd->channels[i];\n\n\t\tpd_chan->chan.device = &pd->dma;\n\t\tdma_cookie_init(&pd_chan->chan);\n\n\t\tpd_chan->membase = &regs->desc[i];\n\n\t\tspin_lock_init(&pd_chan->lock);\n\n\t\tINIT_LIST_HEAD(&pd_chan->active_list);\n\t\tINIT_LIST_HEAD(&pd_chan->queue);\n\t\tINIT_LIST_HEAD(&pd_chan->free_list);\n\n\t\ttasklet_setup(&pd_chan->tasklet, pdc_tasklet);\n\t\tlist_add_tail(&pd_chan->chan.device_node, &pd->dma.channels);\n\t}\n\n\tdma_cap_zero(pd->dma.cap_mask);\n\tdma_cap_set(DMA_PRIVATE, pd->dma.cap_mask);\n\tdma_cap_set(DMA_SLAVE, pd->dma.cap_mask);\n\n\tpd->dma.device_alloc_chan_resources = pd_alloc_chan_resources;\n\tpd->dma.device_free_chan_resources = pd_free_chan_resources;\n\tpd->dma.device_tx_status = pd_tx_status;\n\tpd->dma.device_issue_pending = pd_issue_pending;\n\tpd->dma.device_prep_slave_sg = pd_prep_slave_sg;\n\tpd->dma.device_terminate_all = pd_device_terminate_all;\n\n\terr = dma_async_device_register(&pd->dma);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to register DMA device\\n\");\n\t\tgoto err_free_pool;\n\t}\n\n\treturn 0;\n\nerr_free_pool:\n\tdma_pool_destroy(pd->pool);\nerr_free_irq:\n\tfree_irq(pdev->irq, pd);\nerr_iounmap:\n\tpci_iounmap(pdev, pd->membase);\nerr_free_res:\n\tpci_release_regions(pdev);\nerr_disable_pdev:\n\tpci_disable_device(pdev);\nerr_free_mem:\n\tkfree(pd);\n\treturn err;\n}\n\nstatic void pch_dma_remove(struct pci_dev *pdev)\n{\n\tstruct pch_dma *pd = pci_get_drvdata(pdev);\n\tstruct pch_dma_chan *pd_chan;\n\tstruct dma_chan *chan, *_c;\n\n\tif (pd) {\n\t\tdma_async_device_unregister(&pd->dma);\n\n\t\tfree_irq(pdev->irq, pd);\n\n\t\tlist_for_each_entry_safe(chan, _c, &pd->dma.channels,\n\t\t\t\t\t device_node) {\n\t\t\tpd_chan = to_pd_chan(chan);\n\n\t\t\ttasklet_kill(&pd_chan->tasklet);\n\t\t}\n\n\t\tdma_pool_destroy(pd->pool);\n\t\tpci_iounmap(pdev, pd->membase);\n\t\tpci_release_regions(pdev);\n\t\tpci_disable_device(pdev);\n\t\tkfree(pd);\n\t}\n}\n\n \n#define PCI_DEVICE_ID_EG20T_PCH_DMA_8CH        0x8810\n#define PCI_DEVICE_ID_EG20T_PCH_DMA_4CH        0x8815\n#define PCI_DEVICE_ID_ML7213_DMA1_8CH\t0x8026\n#define PCI_DEVICE_ID_ML7213_DMA2_8CH\t0x802B\n#define PCI_DEVICE_ID_ML7213_DMA3_4CH\t0x8034\n#define PCI_DEVICE_ID_ML7213_DMA4_12CH\t0x8032\n#define PCI_DEVICE_ID_ML7223_DMA1_4CH\t0x800B\n#define PCI_DEVICE_ID_ML7223_DMA2_4CH\t0x800E\n#define PCI_DEVICE_ID_ML7223_DMA3_4CH\t0x8017\n#define PCI_DEVICE_ID_ML7223_DMA4_4CH\t0x803B\n#define PCI_DEVICE_ID_ML7831_DMA1_8CH\t0x8810\n#define PCI_DEVICE_ID_ML7831_DMA2_4CH\t0x8815\n\nstatic const struct pci_device_id pch_dma_id_table[] = {\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_EG20T_PCH_DMA_8CH), 8 },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_EG20T_PCH_DMA_4CH), 4 },\n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7213_DMA1_8CH), 8},  \n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7213_DMA2_8CH), 8},  \n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7213_DMA3_4CH), 4},  \n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7213_DMA4_12CH), 12},  \n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7223_DMA1_4CH), 4},  \n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7223_DMA2_4CH), 4},  \n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7223_DMA3_4CH), 4},  \n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7223_DMA4_4CH), 4},  \n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7831_DMA1_8CH), 8},  \n\t{ PCI_VDEVICE(ROHM, PCI_DEVICE_ID_ML7831_DMA2_4CH), 4},  \n\t{ 0, },\n};\n\nstatic SIMPLE_DEV_PM_OPS(pch_dma_pm_ops, pch_dma_suspend, pch_dma_resume);\n\nstatic struct pci_driver pch_dma_driver = {\n\t.name\t\t= DRV_NAME,\n\t.id_table\t= pch_dma_id_table,\n\t.probe\t\t= pch_dma_probe,\n\t.remove\t\t= pch_dma_remove,\n\t.driver.pm\t= &pch_dma_pm_ops,\n};\n\nmodule_pci_driver(pch_dma_driver);\n\nMODULE_DESCRIPTION(\"Intel EG20T PCH / LAPIS Semicon ML7213/ML7223/ML7831 IOH \"\n\t\t   \"DMA controller driver\");\nMODULE_AUTHOR(\"Yong Wang <yong.y.wang@intel.com>\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DEVICE_TABLE(pci, pch_dma_id_table);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}