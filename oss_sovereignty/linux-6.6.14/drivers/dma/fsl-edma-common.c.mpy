{
  "module_name": "fsl-edma-common.c",
  "hash_id": "19a980d8aedfe3f3161f3fe0dd81573eb3589354fd7c4e9dd05c2c433fe30bcf",
  "original_prompt": "Ingested from linux-6.6.14/drivers/dma/fsl-edma-common.c",
  "human_readable_source": "\n\n\n\n\n#include <linux/dmapool.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/dma-mapping.h>\n#include <linux/pm_runtime.h>\n#include <linux/pm_domain.h>\n\n#include \"fsl-edma-common.h\"\n\n#define EDMA_CR\t\t\t0x00\n#define EDMA_ES\t\t\t0x04\n#define EDMA_ERQ\t\t0x0C\n#define EDMA_EEI\t\t0x14\n#define EDMA_SERQ\t\t0x1B\n#define EDMA_CERQ\t\t0x1A\n#define EDMA_SEEI\t\t0x19\n#define EDMA_CEEI\t\t0x18\n#define EDMA_CINT\t\t0x1F\n#define EDMA_CERR\t\t0x1E\n#define EDMA_SSRT\t\t0x1D\n#define EDMA_CDNE\t\t0x1C\n#define EDMA_INTR\t\t0x24\n#define EDMA_ERR\t\t0x2C\n\n#define EDMA64_ERQH\t\t0x08\n#define EDMA64_EEIH\t\t0x10\n#define EDMA64_SERQ\t\t0x18\n#define EDMA64_CERQ\t\t0x19\n#define EDMA64_SEEI\t\t0x1a\n#define EDMA64_CEEI\t\t0x1b\n#define EDMA64_CINT\t\t0x1c\n#define EDMA64_CERR\t\t0x1d\n#define EDMA64_SSRT\t\t0x1e\n#define EDMA64_CDNE\t\t0x1f\n#define EDMA64_INTH\t\t0x20\n#define EDMA64_INTL\t\t0x24\n#define EDMA64_ERRH\t\t0x28\n#define EDMA64_ERRL\t\t0x2c\n\nvoid fsl_edma_tx_chan_handler(struct fsl_edma_chan *fsl_chan)\n{\n\tspin_lock(&fsl_chan->vchan.lock);\n\n\tif (!fsl_chan->edesc) {\n\t\t \n\t\tspin_unlock(&fsl_chan->vchan.lock);\n\t\treturn;\n\t}\n\n\tif (!fsl_chan->edesc->iscyclic) {\n\t\tlist_del(&fsl_chan->edesc->vdesc.node);\n\t\tvchan_cookie_complete(&fsl_chan->edesc->vdesc);\n\t\tfsl_chan->edesc = NULL;\n\t\tfsl_chan->status = DMA_COMPLETE;\n\t\tfsl_chan->idle = true;\n\t} else {\n\t\tvchan_cyclic_callback(&fsl_chan->edesc->vdesc);\n\t}\n\n\tif (!fsl_chan->edesc)\n\t\tfsl_edma_xfer_desc(fsl_chan);\n\n\tspin_unlock(&fsl_chan->vchan.lock);\n}\n\nstatic void fsl_edma3_enable_request(struct fsl_edma_chan *fsl_chan)\n{\n\tu32 val, flags;\n\n\tflags = fsl_edma_drvflags(fsl_chan);\n\tval = edma_readl_chreg(fsl_chan, ch_sbr);\n\t \n\tif (flags & FSL_EDMA_DRV_QUIRK_SWAPPED) {\n\t\tif (!fsl_chan->is_rxchan)\n\t\t\tval |= EDMA_V3_CH_SBR_RD;\n\t\telse\n\t\t\tval |= EDMA_V3_CH_SBR_WR;\n\t} else {\n\t\tif (fsl_chan->is_rxchan)\n\t\t\tval |= EDMA_V3_CH_SBR_RD;\n\t\telse\n\t\t\tval |= EDMA_V3_CH_SBR_WR;\n\t}\n\n\tif (fsl_chan->is_remote)\n\t\tval &= ~(EDMA_V3_CH_SBR_RD | EDMA_V3_CH_SBR_WR);\n\n\tedma_writel_chreg(fsl_chan, val, ch_sbr);\n\n\tif (flags & FSL_EDMA_DRV_HAS_CHMUX) {\n\t\t \n\t\tif (!edma_readl_chreg(fsl_chan, ch_mux))\n\t\t\tedma_writel_chreg(fsl_chan, fsl_chan->srcid, ch_mux);\n\t}\n\n\tval = edma_readl_chreg(fsl_chan, ch_csr);\n\tval |= EDMA_V3_CH_CSR_ERQ;\n\tedma_writel_chreg(fsl_chan, val, ch_csr);\n}\n\nstatic void fsl_edma_enable_request(struct fsl_edma_chan *fsl_chan)\n{\n\tstruct edma_regs *regs = &fsl_chan->edma->regs;\n\tu32 ch = fsl_chan->vchan.chan.chan_id;\n\n\tif (fsl_edma_drvflags(fsl_chan) & FSL_EDMA_DRV_SPLIT_REG)\n\t\treturn fsl_edma3_enable_request(fsl_chan);\n\n\tif (fsl_chan->edma->drvdata->flags & FSL_EDMA_DRV_WRAP_IO) {\n\t\tedma_writeb(fsl_chan->edma, EDMA_SEEI_SEEI(ch), regs->seei);\n\t\tedma_writeb(fsl_chan->edma, ch, regs->serq);\n\t} else {\n\t\t \n\t\tiowrite8(EDMA_SEEI_SEEI(ch), regs->seei);\n\t\tiowrite8(ch, regs->serq);\n\t}\n}\n\nstatic void fsl_edma3_disable_request(struct fsl_edma_chan *fsl_chan)\n{\n\tu32 val = edma_readl_chreg(fsl_chan, ch_csr);\n\tu32 flags;\n\n\tflags = fsl_edma_drvflags(fsl_chan);\n\n\tif (flags & FSL_EDMA_DRV_HAS_CHMUX)\n\t\tedma_writel_chreg(fsl_chan, 0, ch_mux);\n\n\tval &= ~EDMA_V3_CH_CSR_ERQ;\n\tedma_writel_chreg(fsl_chan, val, ch_csr);\n}\n\nvoid fsl_edma_disable_request(struct fsl_edma_chan *fsl_chan)\n{\n\tstruct edma_regs *regs = &fsl_chan->edma->regs;\n\tu32 ch = fsl_chan->vchan.chan.chan_id;\n\n\tif (fsl_edma_drvflags(fsl_chan) & FSL_EDMA_DRV_SPLIT_REG)\n\t\treturn fsl_edma3_disable_request(fsl_chan);\n\n\tif (fsl_chan->edma->drvdata->flags & FSL_EDMA_DRV_WRAP_IO) {\n\t\tedma_writeb(fsl_chan->edma, ch, regs->cerq);\n\t\tedma_writeb(fsl_chan->edma, EDMA_CEEI_CEEI(ch), regs->ceei);\n\t} else {\n\t\t \n\t\tiowrite8(ch, regs->cerq);\n\t\tiowrite8(EDMA_CEEI_CEEI(ch), regs->ceei);\n\t}\n}\n\nstatic void mux_configure8(struct fsl_edma_chan *fsl_chan, void __iomem *addr,\n\t\t\t   u32 off, u32 slot, bool enable)\n{\n\tu8 val8;\n\n\tif (enable)\n\t\tval8 = EDMAMUX_CHCFG_ENBL | slot;\n\telse\n\t\tval8 = EDMAMUX_CHCFG_DIS;\n\n\tiowrite8(val8, addr + off);\n}\n\nstatic void mux_configure32(struct fsl_edma_chan *fsl_chan, void __iomem *addr,\n\t\t\t    u32 off, u32 slot, bool enable)\n{\n\tu32 val;\n\n\tif (enable)\n\t\tval = EDMAMUX_CHCFG_ENBL << 24 | slot;\n\telse\n\t\tval = EDMAMUX_CHCFG_DIS;\n\n\tiowrite32(val, addr + off * 4);\n}\n\nvoid fsl_edma_chan_mux(struct fsl_edma_chan *fsl_chan,\n\t\t       unsigned int slot, bool enable)\n{\n\tu32 ch = fsl_chan->vchan.chan.chan_id;\n\tvoid __iomem *muxaddr;\n\tunsigned int chans_per_mux, ch_off;\n\tint endian_diff[4] = {3, 1, -1, -3};\n\tu32 dmamux_nr = fsl_chan->edma->drvdata->dmamuxs;\n\n\tif (!dmamux_nr)\n\t\treturn;\n\n\tchans_per_mux = fsl_chan->edma->n_chans / dmamux_nr;\n\tch_off = fsl_chan->vchan.chan.chan_id % chans_per_mux;\n\n\tif (fsl_chan->edma->drvdata->flags & FSL_EDMA_DRV_MUX_SWAP)\n\t\tch_off += endian_diff[ch_off % 4];\n\n\tmuxaddr = fsl_chan->edma->muxbase[ch / chans_per_mux];\n\tslot = EDMAMUX_CHCFG_SOURCE(slot);\n\n\tif (fsl_chan->edma->drvdata->flags & FSL_EDMA_DRV_CONFIG32)\n\t\tmux_configure32(fsl_chan, muxaddr, ch_off, slot, enable);\n\telse\n\t\tmux_configure8(fsl_chan, muxaddr, ch_off, slot, enable);\n}\n\nstatic unsigned int fsl_edma_get_tcd_attr(enum dma_slave_buswidth addr_width)\n{\n\tu32 val;\n\n\tif (addr_width == DMA_SLAVE_BUSWIDTH_UNDEFINED)\n\t\taddr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\n\n\tval = ffs(addr_width) - 1;\n\treturn val | (val << 8);\n}\n\nvoid fsl_edma_free_desc(struct virt_dma_desc *vdesc)\n{\n\tstruct fsl_edma_desc *fsl_desc;\n\tint i;\n\n\tfsl_desc = to_fsl_edma_desc(vdesc);\n\tfor (i = 0; i < fsl_desc->n_tcds; i++)\n\t\tdma_pool_free(fsl_desc->echan->tcd_pool, fsl_desc->tcd[i].vtcd,\n\t\t\t      fsl_desc->tcd[i].ptcd);\n\tkfree(fsl_desc);\n}\n\nint fsl_edma_terminate_all(struct dma_chan *chan)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\n\tfsl_edma_disable_request(fsl_chan);\n\tfsl_chan->edesc = NULL;\n\tfsl_chan->idle = true;\n\tvchan_get_all_descriptors(&fsl_chan->vchan, &head);\n\tspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\n\tvchan_dma_desc_free_list(&fsl_chan->vchan, &head);\n\n\tif (fsl_edma_drvflags(fsl_chan) & FSL_EDMA_DRV_HAS_PD)\n\t\tpm_runtime_allow(fsl_chan->pd_dev);\n\n\treturn 0;\n}\n\nint fsl_edma_pause(struct dma_chan *chan)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\n\tif (fsl_chan->edesc) {\n\t\tfsl_edma_disable_request(fsl_chan);\n\t\tfsl_chan->status = DMA_PAUSED;\n\t\tfsl_chan->idle = true;\n\t}\n\tspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\n\treturn 0;\n}\n\nint fsl_edma_resume(struct dma_chan *chan)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\n\tif (fsl_chan->edesc) {\n\t\tfsl_edma_enable_request(fsl_chan);\n\t\tfsl_chan->status = DMA_IN_PROGRESS;\n\t\tfsl_chan->idle = false;\n\t}\n\tspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\n\treturn 0;\n}\n\nstatic void fsl_edma_unprep_slave_dma(struct fsl_edma_chan *fsl_chan)\n{\n\tif (fsl_chan->dma_dir != DMA_NONE)\n\t\tdma_unmap_resource(fsl_chan->vchan.chan.device->dev,\n\t\t\t\t   fsl_chan->dma_dev_addr,\n\t\t\t\t   fsl_chan->dma_dev_size,\n\t\t\t\t   fsl_chan->dma_dir, 0);\n\tfsl_chan->dma_dir = DMA_NONE;\n}\n\nstatic bool fsl_edma_prep_slave_dma(struct fsl_edma_chan *fsl_chan,\n\t\t\t\t    enum dma_transfer_direction dir)\n{\n\tstruct device *dev = fsl_chan->vchan.chan.device->dev;\n\tenum dma_data_direction dma_dir;\n\tphys_addr_t addr = 0;\n\tu32 size = 0;\n\n\tswitch (dir) {\n\tcase DMA_MEM_TO_DEV:\n\t\tdma_dir = DMA_FROM_DEVICE;\n\t\taddr = fsl_chan->cfg.dst_addr;\n\t\tsize = fsl_chan->cfg.dst_maxburst;\n\t\tbreak;\n\tcase DMA_DEV_TO_MEM:\n\t\tdma_dir = DMA_TO_DEVICE;\n\t\taddr = fsl_chan->cfg.src_addr;\n\t\tsize = fsl_chan->cfg.src_maxburst;\n\t\tbreak;\n\tdefault:\n\t\tdma_dir = DMA_NONE;\n\t\tbreak;\n\t}\n\n\t \n\tif (fsl_chan->dma_dir == dma_dir)\n\t\treturn true;\n\n\tfsl_edma_unprep_slave_dma(fsl_chan);\n\n\tfsl_chan->dma_dev_addr = dma_map_resource(dev, addr, size, dma_dir, 0);\n\tif (dma_mapping_error(dev, fsl_chan->dma_dev_addr))\n\t\treturn false;\n\tfsl_chan->dma_dev_size = size;\n\tfsl_chan->dma_dir = dma_dir;\n\n\treturn true;\n}\n\nint fsl_edma_slave_config(struct dma_chan *chan,\n\t\t\t\t struct dma_slave_config *cfg)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\n\tmemcpy(&fsl_chan->cfg, cfg, sizeof(*cfg));\n\tfsl_edma_unprep_slave_dma(fsl_chan);\n\n\treturn 0;\n}\n\nstatic size_t fsl_edma_desc_residue(struct fsl_edma_chan *fsl_chan,\n\t\tstruct virt_dma_desc *vdesc, bool in_progress)\n{\n\tstruct fsl_edma_desc *edesc = fsl_chan->edesc;\n\tenum dma_transfer_direction dir = edesc->dirn;\n\tdma_addr_t cur_addr, dma_addr;\n\tsize_t len, size;\n\tu32 nbytes = 0;\n\tint i;\n\n\t \n\tfor (len = i = 0; i < fsl_chan->edesc->n_tcds; i++) {\n\t\tnbytes = le32_to_cpu(edesc->tcd[i].vtcd->nbytes);\n\t\tif (nbytes & (EDMA_V3_TCD_NBYTES_DMLOE | EDMA_V3_TCD_NBYTES_SMLOE))\n\t\t\tnbytes = EDMA_V3_TCD_NBYTES_MLOFF_NBYTES(nbytes);\n\t\tlen += nbytes * le16_to_cpu(edesc->tcd[i].vtcd->biter);\n\t}\n\n\tif (!in_progress)\n\t\treturn len;\n\n\tif (dir == DMA_MEM_TO_DEV)\n\t\tcur_addr = edma_read_tcdreg(fsl_chan, saddr);\n\telse\n\t\tcur_addr = edma_read_tcdreg(fsl_chan, daddr);\n\n\t \n\tfor (i = 0; i < fsl_chan->edesc->n_tcds; i++) {\n\t\tnbytes = le32_to_cpu(edesc->tcd[i].vtcd->nbytes);\n\t\tif (nbytes & (EDMA_V3_TCD_NBYTES_DMLOE | EDMA_V3_TCD_NBYTES_SMLOE))\n\t\t\tnbytes = EDMA_V3_TCD_NBYTES_MLOFF_NBYTES(nbytes);\n\n\t\tsize = nbytes * le16_to_cpu(edesc->tcd[i].vtcd->biter);\n\n\t\tif (dir == DMA_MEM_TO_DEV)\n\t\t\tdma_addr = le32_to_cpu(edesc->tcd[i].vtcd->saddr);\n\t\telse\n\t\t\tdma_addr = le32_to_cpu(edesc->tcd[i].vtcd->daddr);\n\n\t\tlen -= size;\n\t\tif (cur_addr >= dma_addr && cur_addr < dma_addr + size) {\n\t\t\tlen += dma_addr + size - cur_addr;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn len;\n}\n\nenum dma_status fsl_edma_tx_status(struct dma_chan *chan,\n\t\tdma_cookie_t cookie, struct dma_tx_state *txstate)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\tstruct virt_dma_desc *vdesc;\n\tenum dma_status status;\n\tunsigned long flags;\n\n\tstatus = dma_cookie_status(chan, cookie, txstate);\n\tif (status == DMA_COMPLETE)\n\t\treturn status;\n\n\tif (!txstate)\n\t\treturn fsl_chan->status;\n\n\tspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\n\tvdesc = vchan_find_desc(&fsl_chan->vchan, cookie);\n\tif (fsl_chan->edesc && cookie == fsl_chan->edesc->vdesc.tx.cookie)\n\t\ttxstate->residue =\n\t\t\tfsl_edma_desc_residue(fsl_chan, vdesc, true);\n\telse if (vdesc)\n\t\ttxstate->residue =\n\t\t\tfsl_edma_desc_residue(fsl_chan, vdesc, false);\n\telse\n\t\ttxstate->residue = 0;\n\n\tspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\n\n\treturn fsl_chan->status;\n}\n\nstatic void fsl_edma_set_tcd_regs(struct fsl_edma_chan *fsl_chan,\n\t\t\t\t  struct fsl_edma_hw_tcd *tcd)\n{\n\tu16 csr = 0;\n\n\t \n\tedma_write_tcdreg(fsl_chan, 0, csr);\n\n\tedma_write_tcdreg(fsl_chan, tcd->saddr, saddr);\n\tedma_write_tcdreg(fsl_chan, tcd->daddr, daddr);\n\n\tedma_write_tcdreg(fsl_chan, tcd->attr, attr);\n\tedma_write_tcdreg(fsl_chan, tcd->soff, soff);\n\n\tedma_write_tcdreg(fsl_chan, tcd->nbytes, nbytes);\n\tedma_write_tcdreg(fsl_chan, tcd->slast, slast);\n\n\tedma_write_tcdreg(fsl_chan, tcd->citer, citer);\n\tedma_write_tcdreg(fsl_chan, tcd->biter, biter);\n\tedma_write_tcdreg(fsl_chan, tcd->doff, doff);\n\n\tedma_write_tcdreg(fsl_chan, tcd->dlast_sga, dlast_sga);\n\n\tcsr = le16_to_cpu(tcd->csr);\n\n\tif (fsl_chan->is_sw) {\n\t\tcsr |= EDMA_TCD_CSR_START;\n\t\ttcd->csr = cpu_to_le16(csr);\n\t}\n\n\t \n\tif (((fsl_edma_drvflags(fsl_chan) & FSL_EDMA_DRV_CLEAR_DONE_E_SG) &&\n\t\t(csr & EDMA_TCD_CSR_E_SG)) ||\n\t    ((fsl_edma_drvflags(fsl_chan) & FSL_EDMA_DRV_CLEAR_DONE_E_LINK) &&\n\t\t(csr & EDMA_TCD_CSR_E_LINK)))\n\t\tedma_writel_chreg(fsl_chan, edma_readl_chreg(fsl_chan, ch_csr), ch_csr);\n\n\n\tedma_write_tcdreg(fsl_chan, tcd->csr, csr);\n}\n\nstatic inline\nvoid fsl_edma_fill_tcd(struct fsl_edma_chan *fsl_chan,\n\t\t       struct fsl_edma_hw_tcd *tcd, u32 src, u32 dst,\n\t\t       u16 attr, u16 soff, u32 nbytes, u32 slast, u16 citer,\n\t\t       u16 biter, u16 doff, u32 dlast_sga, bool major_int,\n\t\t       bool disable_req, bool enable_sg)\n{\n\tstruct dma_slave_config *cfg = &fsl_chan->cfg;\n\tu16 csr = 0;\n\tu32 burst;\n\n\t \n\ttcd->saddr = cpu_to_le32(src);\n\ttcd->daddr = cpu_to_le32(dst);\n\n\ttcd->attr = cpu_to_le16(attr);\n\n\ttcd->soff = cpu_to_le16(soff);\n\n\tif (fsl_chan->is_multi_fifo) {\n\t\t \n\t\tburst = cfg->direction == DMA_DEV_TO_MEM ?\n\t\t\t\tcfg->src_addr_width : cfg->dst_addr_width;\n\t\tnbytes |= EDMA_V3_TCD_NBYTES_MLOFF(-(burst * 4));\n\t\t \n\t\tif (cfg->direction == DMA_MEM_TO_DEV) {\n\t\t\tnbytes |= EDMA_V3_TCD_NBYTES_DMLOE;\n\t\t\tnbytes &= ~EDMA_V3_TCD_NBYTES_SMLOE;\n\t\t} else {\n\t\t\tnbytes |= EDMA_V3_TCD_NBYTES_SMLOE;\n\t\t\tnbytes &= ~EDMA_V3_TCD_NBYTES_DMLOE;\n\t\t}\n\t}\n\n\ttcd->nbytes = cpu_to_le32(nbytes);\n\ttcd->slast = cpu_to_le32(slast);\n\n\ttcd->citer = cpu_to_le16(EDMA_TCD_CITER_CITER(citer));\n\ttcd->doff = cpu_to_le16(doff);\n\n\ttcd->dlast_sga = cpu_to_le32(dlast_sga);\n\n\ttcd->biter = cpu_to_le16(EDMA_TCD_BITER_BITER(biter));\n\tif (major_int)\n\t\tcsr |= EDMA_TCD_CSR_INT_MAJOR;\n\n\tif (disable_req)\n\t\tcsr |= EDMA_TCD_CSR_D_REQ;\n\n\tif (enable_sg)\n\t\tcsr |= EDMA_TCD_CSR_E_SG;\n\n\tif (fsl_chan->is_rxchan)\n\t\tcsr |= EDMA_TCD_CSR_ACTIVE;\n\n\tif (fsl_chan->is_sw)\n\t\tcsr |= EDMA_TCD_CSR_START;\n\n\ttcd->csr = cpu_to_le16(csr);\n}\n\nstatic struct fsl_edma_desc *fsl_edma_alloc_desc(struct fsl_edma_chan *fsl_chan,\n\t\tint sg_len)\n{\n\tstruct fsl_edma_desc *fsl_desc;\n\tint i;\n\n\tfsl_desc = kzalloc(struct_size(fsl_desc, tcd, sg_len), GFP_NOWAIT);\n\tif (!fsl_desc)\n\t\treturn NULL;\n\n\tfsl_desc->echan = fsl_chan;\n\tfsl_desc->n_tcds = sg_len;\n\tfor (i = 0; i < sg_len; i++) {\n\t\tfsl_desc->tcd[i].vtcd = dma_pool_alloc(fsl_chan->tcd_pool,\n\t\t\t\t\tGFP_NOWAIT, &fsl_desc->tcd[i].ptcd);\n\t\tif (!fsl_desc->tcd[i].vtcd)\n\t\t\tgoto err;\n\t}\n\treturn fsl_desc;\n\nerr:\n\twhile (--i >= 0)\n\t\tdma_pool_free(fsl_chan->tcd_pool, fsl_desc->tcd[i].vtcd,\n\t\t\t\tfsl_desc->tcd[i].ptcd);\n\tkfree(fsl_desc);\n\treturn NULL;\n}\n\nstruct dma_async_tx_descriptor *fsl_edma_prep_dma_cyclic(\n\t\tstruct dma_chan *chan, dma_addr_t dma_addr, size_t buf_len,\n\t\tsize_t period_len, enum dma_transfer_direction direction,\n\t\tunsigned long flags)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\tstruct fsl_edma_desc *fsl_desc;\n\tdma_addr_t dma_buf_next;\n\tbool major_int = true;\n\tint sg_len, i;\n\tu32 src_addr, dst_addr, last_sg, nbytes;\n\tu16 soff, doff, iter;\n\n\tif (!is_slave_direction(direction))\n\t\treturn NULL;\n\n\tif (!fsl_edma_prep_slave_dma(fsl_chan, direction))\n\t\treturn NULL;\n\n\tsg_len = buf_len / period_len;\n\tfsl_desc = fsl_edma_alloc_desc(fsl_chan, sg_len);\n\tif (!fsl_desc)\n\t\treturn NULL;\n\tfsl_desc->iscyclic = true;\n\tfsl_desc->dirn = direction;\n\n\tdma_buf_next = dma_addr;\n\tif (direction == DMA_MEM_TO_DEV) {\n\t\tfsl_chan->attr =\n\t\t\tfsl_edma_get_tcd_attr(fsl_chan->cfg.dst_addr_width);\n\t\tnbytes = fsl_chan->cfg.dst_addr_width *\n\t\t\tfsl_chan->cfg.dst_maxburst;\n\t} else {\n\t\tfsl_chan->attr =\n\t\t\tfsl_edma_get_tcd_attr(fsl_chan->cfg.src_addr_width);\n\t\tnbytes = fsl_chan->cfg.src_addr_width *\n\t\t\tfsl_chan->cfg.src_maxburst;\n\t}\n\n\titer = period_len / nbytes;\n\n\tfor (i = 0; i < sg_len; i++) {\n\t\tif (dma_buf_next >= dma_addr + buf_len)\n\t\t\tdma_buf_next = dma_addr;\n\n\t\t \n\t\tlast_sg = fsl_desc->tcd[(i + 1) % sg_len].ptcd;\n\n\t\tif (direction == DMA_MEM_TO_DEV) {\n\t\t\tsrc_addr = dma_buf_next;\n\t\t\tdst_addr = fsl_chan->dma_dev_addr;\n\t\t\tsoff = fsl_chan->cfg.dst_addr_width;\n\t\t\tdoff = fsl_chan->is_multi_fifo ? 4 : 0;\n\t\t} else if (direction == DMA_DEV_TO_MEM) {\n\t\t\tsrc_addr = fsl_chan->dma_dev_addr;\n\t\t\tdst_addr = dma_buf_next;\n\t\t\tsoff = fsl_chan->is_multi_fifo ? 4 : 0;\n\t\t\tdoff = fsl_chan->cfg.src_addr_width;\n\t\t} else {\n\t\t\t \n\t\t\tsrc_addr = fsl_chan->cfg.src_addr;\n\t\t\tdst_addr = fsl_chan->cfg.dst_addr;\n\t\t\tsoff = doff = 0;\n\t\t\tmajor_int = false;\n\t\t}\n\n\t\tfsl_edma_fill_tcd(fsl_chan, fsl_desc->tcd[i].vtcd, src_addr, dst_addr,\n\t\t\t\t  fsl_chan->attr, soff, nbytes, 0, iter,\n\t\t\t\t  iter, doff, last_sg, major_int, false, true);\n\t\tdma_buf_next += period_len;\n\t}\n\n\treturn vchan_tx_prep(&fsl_chan->vchan, &fsl_desc->vdesc, flags);\n}\n\nstruct dma_async_tx_descriptor *fsl_edma_prep_slave_sg(\n\t\tstruct dma_chan *chan, struct scatterlist *sgl,\n\t\tunsigned int sg_len, enum dma_transfer_direction direction,\n\t\tunsigned long flags, void *context)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\tstruct fsl_edma_desc *fsl_desc;\n\tstruct scatterlist *sg;\n\tu32 src_addr, dst_addr, last_sg, nbytes;\n\tu16 soff, doff, iter;\n\tint i;\n\n\tif (!is_slave_direction(direction))\n\t\treturn NULL;\n\n\tif (!fsl_edma_prep_slave_dma(fsl_chan, direction))\n\t\treturn NULL;\n\n\tfsl_desc = fsl_edma_alloc_desc(fsl_chan, sg_len);\n\tif (!fsl_desc)\n\t\treturn NULL;\n\tfsl_desc->iscyclic = false;\n\tfsl_desc->dirn = direction;\n\n\tif (direction == DMA_MEM_TO_DEV) {\n\t\tfsl_chan->attr =\n\t\t\tfsl_edma_get_tcd_attr(fsl_chan->cfg.dst_addr_width);\n\t\tnbytes = fsl_chan->cfg.dst_addr_width *\n\t\t\tfsl_chan->cfg.dst_maxburst;\n\t} else {\n\t\tfsl_chan->attr =\n\t\t\tfsl_edma_get_tcd_attr(fsl_chan->cfg.src_addr_width);\n\t\tnbytes = fsl_chan->cfg.src_addr_width *\n\t\t\tfsl_chan->cfg.src_maxburst;\n\t}\n\n\tfor_each_sg(sgl, sg, sg_len, i) {\n\t\tif (direction == DMA_MEM_TO_DEV) {\n\t\t\tsrc_addr = sg_dma_address(sg);\n\t\t\tdst_addr = fsl_chan->dma_dev_addr;\n\t\t\tsoff = fsl_chan->cfg.dst_addr_width;\n\t\t\tdoff = 0;\n\t\t} else if (direction == DMA_DEV_TO_MEM) {\n\t\t\tsrc_addr = fsl_chan->dma_dev_addr;\n\t\t\tdst_addr = sg_dma_address(sg);\n\t\t\tsoff = 0;\n\t\t\tdoff = fsl_chan->cfg.src_addr_width;\n\t\t} else {\n\t\t\t \n\t\t\tsrc_addr = fsl_chan->cfg.src_addr;\n\t\t\tdst_addr = fsl_chan->cfg.dst_addr;\n\t\t\tsoff = 0;\n\t\t\tdoff = 0;\n\t\t}\n\n\t\t \n\t\tif (sg_dma_len(sg) % nbytes) {\n\t\t\tu32 width = (direction == DMA_DEV_TO_MEM) ? doff : soff;\n\t\t\tu32 burst = (direction == DMA_DEV_TO_MEM) ?\n\t\t\t\t\t\tfsl_chan->cfg.src_maxburst :\n\t\t\t\t\t\tfsl_chan->cfg.dst_maxburst;\n\t\t\tint j;\n\n\t\t\tfor (j = burst; j > 1; j--) {\n\t\t\t\tif (!(sg_dma_len(sg) % (j * width))) {\n\t\t\t\t\tnbytes = j * width;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\t \n\t\t\tif (j == 1)\n\t\t\t\tnbytes = width;\n\t\t}\n\t\titer = sg_dma_len(sg) / nbytes;\n\t\tif (i < sg_len - 1) {\n\t\t\tlast_sg = fsl_desc->tcd[(i + 1)].ptcd;\n\t\t\tfsl_edma_fill_tcd(fsl_chan, fsl_desc->tcd[i].vtcd, src_addr,\n\t\t\t\t\t  dst_addr, fsl_chan->attr, soff,\n\t\t\t\t\t  nbytes, 0, iter, iter, doff, last_sg,\n\t\t\t\t\t  false, false, true);\n\t\t} else {\n\t\t\tlast_sg = 0;\n\t\t\tfsl_edma_fill_tcd(fsl_chan, fsl_desc->tcd[i].vtcd, src_addr,\n\t\t\t\t\t  dst_addr, fsl_chan->attr, soff,\n\t\t\t\t\t  nbytes, 0, iter, iter, doff, last_sg,\n\t\t\t\t\t  true, true, false);\n\t\t}\n\t}\n\n\treturn vchan_tx_prep(&fsl_chan->vchan, &fsl_desc->vdesc, flags);\n}\n\nstruct dma_async_tx_descriptor *fsl_edma_prep_memcpy(struct dma_chan *chan,\n\t\t\t\t\t\t     dma_addr_t dma_dst, dma_addr_t dma_src,\n\t\t\t\t\t\t     size_t len, unsigned long flags)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\tstruct fsl_edma_desc *fsl_desc;\n\n\tfsl_desc = fsl_edma_alloc_desc(fsl_chan, 1);\n\tif (!fsl_desc)\n\t\treturn NULL;\n\tfsl_desc->iscyclic = false;\n\n\tfsl_chan->is_sw = true;\n\n\t \n\tfsl_edma_fill_tcd(fsl_chan, fsl_desc->tcd[0].vtcd, dma_src, dma_dst,\n\t\t\tfsl_edma_get_tcd_attr(DMA_SLAVE_BUSWIDTH_32_BYTES),\n\t\t\t32, len, 0, 1, 1, 32, 0, true, true, false);\n\n\treturn vchan_tx_prep(&fsl_chan->vchan, &fsl_desc->vdesc, flags);\n}\n\nvoid fsl_edma_xfer_desc(struct fsl_edma_chan *fsl_chan)\n{\n\tstruct virt_dma_desc *vdesc;\n\n\tlockdep_assert_held(&fsl_chan->vchan.lock);\n\n\tvdesc = vchan_next_desc(&fsl_chan->vchan);\n\tif (!vdesc)\n\t\treturn;\n\tfsl_chan->edesc = to_fsl_edma_desc(vdesc);\n\tfsl_edma_set_tcd_regs(fsl_chan, fsl_chan->edesc->tcd[0].vtcd);\n\tfsl_edma_enable_request(fsl_chan);\n\tfsl_chan->status = DMA_IN_PROGRESS;\n\tfsl_chan->idle = false;\n}\n\nvoid fsl_edma_issue_pending(struct dma_chan *chan)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\n\n\tif (unlikely(fsl_chan->pm_state != RUNNING)) {\n\t\tspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\n\t\t \n\t\treturn;\n\t}\n\n\tif (vchan_issue_pending(&fsl_chan->vchan) && !fsl_chan->edesc)\n\t\tfsl_edma_xfer_desc(fsl_chan);\n\n\tspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\n}\n\nint fsl_edma_alloc_chan_resources(struct dma_chan *chan)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\n\tfsl_chan->tcd_pool = dma_pool_create(\"tcd_pool\", chan->device->dev,\n\t\t\t\tsizeof(struct fsl_edma_hw_tcd),\n\t\t\t\t32, 0);\n\treturn 0;\n}\n\nvoid fsl_edma_free_chan_resources(struct dma_chan *chan)\n{\n\tstruct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);\n\tstruct fsl_edma_engine *edma = fsl_chan->edma;\n\tunsigned long flags;\n\tLIST_HEAD(head);\n\n\tspin_lock_irqsave(&fsl_chan->vchan.lock, flags);\n\tfsl_edma_disable_request(fsl_chan);\n\tif (edma->drvdata->dmamuxs)\n\t\tfsl_edma_chan_mux(fsl_chan, 0, false);\n\tfsl_chan->edesc = NULL;\n\tvchan_get_all_descriptors(&fsl_chan->vchan, &head);\n\tfsl_edma_unprep_slave_dma(fsl_chan);\n\tspin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);\n\n\tvchan_dma_desc_free_list(&fsl_chan->vchan, &head);\n\tdma_pool_destroy(fsl_chan->tcd_pool);\n\tfsl_chan->tcd_pool = NULL;\n\tfsl_chan->is_sw = false;\n\tfsl_chan->srcid = 0;\n}\n\nvoid fsl_edma_cleanup_vchan(struct dma_device *dmadev)\n{\n\tstruct fsl_edma_chan *chan, *_chan;\n\n\tlist_for_each_entry_safe(chan, _chan,\n\t\t\t\t&dmadev->channels, vchan.chan.device_node) {\n\t\tlist_del(&chan->vchan.chan.device_node);\n\t\ttasklet_kill(&chan->vchan.task);\n\t}\n}\n\n \nvoid fsl_edma_setup_regs(struct fsl_edma_engine *edma)\n{\n\tbool is64 = !!(edma->drvdata->flags & FSL_EDMA_DRV_EDMA64);\n\n\tedma->regs.cr = edma->membase + EDMA_CR;\n\tedma->regs.es = edma->membase + EDMA_ES;\n\tedma->regs.erql = edma->membase + EDMA_ERQ;\n\tedma->regs.eeil = edma->membase + EDMA_EEI;\n\n\tedma->regs.serq = edma->membase + (is64 ? EDMA64_SERQ : EDMA_SERQ);\n\tedma->regs.cerq = edma->membase + (is64 ? EDMA64_CERQ : EDMA_CERQ);\n\tedma->regs.seei = edma->membase + (is64 ? EDMA64_SEEI : EDMA_SEEI);\n\tedma->regs.ceei = edma->membase + (is64 ? EDMA64_CEEI : EDMA_CEEI);\n\tedma->regs.cint = edma->membase + (is64 ? EDMA64_CINT : EDMA_CINT);\n\tedma->regs.cerr = edma->membase + (is64 ? EDMA64_CERR : EDMA_CERR);\n\tedma->regs.ssrt = edma->membase + (is64 ? EDMA64_SSRT : EDMA_SSRT);\n\tedma->regs.cdne = edma->membase + (is64 ? EDMA64_CDNE : EDMA_CDNE);\n\tedma->regs.intl = edma->membase + (is64 ? EDMA64_INTL : EDMA_INTR);\n\tedma->regs.errl = edma->membase + (is64 ? EDMA64_ERRL : EDMA_ERR);\n\n\tif (is64) {\n\t\tedma->regs.erqh = edma->membase + EDMA64_ERQH;\n\t\tedma->regs.eeih = edma->membase + EDMA64_EEIH;\n\t\tedma->regs.errh = edma->membase + EDMA64_ERRH;\n\t\tedma->regs.inth = edma->membase + EDMA64_INTH;\n\t}\n}\n\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}