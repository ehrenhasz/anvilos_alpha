{
  "module_name": "vduse_dev.c",
  "hash_id": "76ae010d9b4b0923643eecabab886cbd50808fbdf8e57c30397b27fb601bc8c1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vdpa/vdpa_user/vduse_dev.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/cdev.h>\n#include <linux/device.h>\n#include <linux/eventfd.h>\n#include <linux/slab.h>\n#include <linux/wait.h>\n#include <linux/dma-map-ops.h>\n#include <linux/poll.h>\n#include <linux/file.h>\n#include <linux/uio.h>\n#include <linux/vdpa.h>\n#include <linux/nospec.h>\n#include <linux/vmalloc.h>\n#include <linux/sched/mm.h>\n#include <uapi/linux/vduse.h>\n#include <uapi/linux/vdpa.h>\n#include <uapi/linux/virtio_config.h>\n#include <uapi/linux/virtio_ids.h>\n#include <uapi/linux/virtio_blk.h>\n#include <linux/mod_devicetable.h>\n\n#include \"iova_domain.h\"\n\n#define DRV_AUTHOR   \"Yongji Xie <xieyongji@bytedance.com>\"\n#define DRV_DESC     \"vDPA Device in Userspace\"\n#define DRV_LICENSE  \"GPL v2\"\n\n#define VDUSE_DEV_MAX (1U << MINORBITS)\n#define VDUSE_MAX_BOUNCE_SIZE (1024 * 1024 * 1024)\n#define VDUSE_MIN_BOUNCE_SIZE (1024 * 1024)\n#define VDUSE_BOUNCE_SIZE (64 * 1024 * 1024)\n \n#define VDUSE_IOVA_SIZE (VDUSE_MAX_BOUNCE_SIZE + 128 * 1024 * 1024)\n#define VDUSE_MSG_DEFAULT_TIMEOUT 30\n\n#define IRQ_UNBOUND -1\n\nstruct vduse_virtqueue {\n\tu16 index;\n\tu16 num_max;\n\tu32 num;\n\tu64 desc_addr;\n\tu64 driver_addr;\n\tu64 device_addr;\n\tstruct vdpa_vq_state state;\n\tbool ready;\n\tbool kicked;\n\tspinlock_t kick_lock;\n\tspinlock_t irq_lock;\n\tstruct eventfd_ctx *kickfd;\n\tstruct vdpa_callback cb;\n\tstruct work_struct inject;\n\tstruct work_struct kick;\n\tint irq_effective_cpu;\n\tstruct cpumask irq_affinity;\n\tstruct kobject kobj;\n};\n\nstruct vduse_dev;\n\nstruct vduse_vdpa {\n\tstruct vdpa_device vdpa;\n\tstruct vduse_dev *dev;\n};\n\nstruct vduse_umem {\n\tunsigned long iova;\n\tunsigned long npages;\n\tstruct page **pages;\n\tstruct mm_struct *mm;\n};\n\nstruct vduse_dev {\n\tstruct vduse_vdpa *vdev;\n\tstruct device *dev;\n\tstruct vduse_virtqueue **vqs;\n\tstruct vduse_iova_domain *domain;\n\tchar *name;\n\tstruct mutex lock;\n\tspinlock_t msg_lock;\n\tu64 msg_unique;\n\tu32 msg_timeout;\n\twait_queue_head_t waitq;\n\tstruct list_head send_list;\n\tstruct list_head recv_list;\n\tstruct vdpa_callback config_cb;\n\tstruct work_struct inject;\n\tspinlock_t irq_lock;\n\tstruct rw_semaphore rwsem;\n\tint minor;\n\tbool broken;\n\tbool connected;\n\tu64 api_version;\n\tu64 device_features;\n\tu64 driver_features;\n\tu32 device_id;\n\tu32 vendor_id;\n\tu32 generation;\n\tu32 config_size;\n\tvoid *config;\n\tu8 status;\n\tu32 vq_num;\n\tu32 vq_align;\n\tstruct vduse_umem *umem;\n\tstruct mutex mem_lock;\n\tunsigned int bounce_size;\n\tstruct mutex domain_lock;\n};\n\nstruct vduse_dev_msg {\n\tstruct vduse_dev_request req;\n\tstruct vduse_dev_response resp;\n\tstruct list_head list;\n\twait_queue_head_t waitq;\n\tbool completed;\n};\n\nstruct vduse_control {\n\tu64 api_version;\n};\n\nstatic DEFINE_MUTEX(vduse_lock);\nstatic DEFINE_IDR(vduse_idr);\n\nstatic dev_t vduse_major;\nstatic struct class *vduse_class;\nstatic struct cdev vduse_ctrl_cdev;\nstatic struct cdev vduse_cdev;\nstatic struct workqueue_struct *vduse_irq_wq;\nstatic struct workqueue_struct *vduse_irq_bound_wq;\n\nstatic u32 allowed_device_id[] = {\n\tVIRTIO_ID_BLOCK,\n};\n\nstatic inline struct vduse_dev *vdpa_to_vduse(struct vdpa_device *vdpa)\n{\n\tstruct vduse_vdpa *vdev = container_of(vdpa, struct vduse_vdpa, vdpa);\n\n\treturn vdev->dev;\n}\n\nstatic inline struct vduse_dev *dev_to_vduse(struct device *dev)\n{\n\tstruct vdpa_device *vdpa = dev_to_vdpa(dev);\n\n\treturn vdpa_to_vduse(vdpa);\n}\n\nstatic struct vduse_dev_msg *vduse_find_msg(struct list_head *head,\n\t\t\t\t\t    uint32_t request_id)\n{\n\tstruct vduse_dev_msg *msg;\n\n\tlist_for_each_entry(msg, head, list) {\n\t\tif (msg->req.request_id == request_id) {\n\t\t\tlist_del(&msg->list);\n\t\t\treturn msg;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct vduse_dev_msg *vduse_dequeue_msg(struct list_head *head)\n{\n\tstruct vduse_dev_msg *msg = NULL;\n\n\tif (!list_empty(head)) {\n\t\tmsg = list_first_entry(head, struct vduse_dev_msg, list);\n\t\tlist_del(&msg->list);\n\t}\n\n\treturn msg;\n}\n\nstatic void vduse_enqueue_msg(struct list_head *head,\n\t\t\t      struct vduse_dev_msg *msg)\n{\n\tlist_add_tail(&msg->list, head);\n}\n\nstatic void vduse_dev_broken(struct vduse_dev *dev)\n{\n\tstruct vduse_dev_msg *msg, *tmp;\n\n\tif (unlikely(dev->broken))\n\t\treturn;\n\n\tlist_splice_init(&dev->recv_list, &dev->send_list);\n\tlist_for_each_entry_safe(msg, tmp, &dev->send_list, list) {\n\t\tlist_del(&msg->list);\n\t\tmsg->completed = 1;\n\t\tmsg->resp.result = VDUSE_REQ_RESULT_FAILED;\n\t\twake_up(&msg->waitq);\n\t}\n\tdev->broken = true;\n\twake_up(&dev->waitq);\n}\n\nstatic int vduse_dev_msg_sync(struct vduse_dev *dev,\n\t\t\t      struct vduse_dev_msg *msg)\n{\n\tint ret;\n\n\tif (unlikely(dev->broken))\n\t\treturn -EIO;\n\n\tinit_waitqueue_head(&msg->waitq);\n\tspin_lock(&dev->msg_lock);\n\tif (unlikely(dev->broken)) {\n\t\tspin_unlock(&dev->msg_lock);\n\t\treturn -EIO;\n\t}\n\tmsg->req.request_id = dev->msg_unique++;\n\tvduse_enqueue_msg(&dev->send_list, msg);\n\twake_up(&dev->waitq);\n\tspin_unlock(&dev->msg_lock);\n\tif (dev->msg_timeout)\n\t\tret = wait_event_killable_timeout(msg->waitq, msg->completed,\n\t\t\t\t\t\t  (long)dev->msg_timeout * HZ);\n\telse\n\t\tret = wait_event_killable(msg->waitq, msg->completed);\n\n\tspin_lock(&dev->msg_lock);\n\tif (!msg->completed) {\n\t\tlist_del(&msg->list);\n\t\tmsg->resp.result = VDUSE_REQ_RESULT_FAILED;\n\t\t \n\t\tif (!ret)\n\t\t\tvduse_dev_broken(dev);\n\t}\n\tret = (msg->resp.result == VDUSE_REQ_RESULT_OK) ? 0 : -EIO;\n\tspin_unlock(&dev->msg_lock);\n\n\treturn ret;\n}\n\nstatic int vduse_dev_get_vq_state_packed(struct vduse_dev *dev,\n\t\t\t\t\t struct vduse_virtqueue *vq,\n\t\t\t\t\t struct vdpa_vq_state_packed *packed)\n{\n\tstruct vduse_dev_msg msg = { 0 };\n\tint ret;\n\n\tmsg.req.type = VDUSE_GET_VQ_STATE;\n\tmsg.req.vq_state.index = vq->index;\n\n\tret = vduse_dev_msg_sync(dev, &msg);\n\tif (ret)\n\t\treturn ret;\n\n\tpacked->last_avail_counter =\n\t\t\tmsg.resp.vq_state.packed.last_avail_counter & 0x0001;\n\tpacked->last_avail_idx =\n\t\t\tmsg.resp.vq_state.packed.last_avail_idx & 0x7FFF;\n\tpacked->last_used_counter =\n\t\t\tmsg.resp.vq_state.packed.last_used_counter & 0x0001;\n\tpacked->last_used_idx =\n\t\t\tmsg.resp.vq_state.packed.last_used_idx & 0x7FFF;\n\n\treturn 0;\n}\n\nstatic int vduse_dev_get_vq_state_split(struct vduse_dev *dev,\n\t\t\t\t\tstruct vduse_virtqueue *vq,\n\t\t\t\t\tstruct vdpa_vq_state_split *split)\n{\n\tstruct vduse_dev_msg msg = { 0 };\n\tint ret;\n\n\tmsg.req.type = VDUSE_GET_VQ_STATE;\n\tmsg.req.vq_state.index = vq->index;\n\n\tret = vduse_dev_msg_sync(dev, &msg);\n\tif (ret)\n\t\treturn ret;\n\n\tsplit->avail_index = msg.resp.vq_state.split.avail_index;\n\n\treturn 0;\n}\n\nstatic int vduse_dev_set_status(struct vduse_dev *dev, u8 status)\n{\n\tstruct vduse_dev_msg msg = { 0 };\n\n\tmsg.req.type = VDUSE_SET_STATUS;\n\tmsg.req.s.status = status;\n\n\treturn vduse_dev_msg_sync(dev, &msg);\n}\n\nstatic int vduse_dev_update_iotlb(struct vduse_dev *dev,\n\t\t\t\t  u64 start, u64 last)\n{\n\tstruct vduse_dev_msg msg = { 0 };\n\n\tif (last < start)\n\t\treturn -EINVAL;\n\n\tmsg.req.type = VDUSE_UPDATE_IOTLB;\n\tmsg.req.iova.start = start;\n\tmsg.req.iova.last = last;\n\n\treturn vduse_dev_msg_sync(dev, &msg);\n}\n\nstatic ssize_t vduse_dev_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct vduse_dev *dev = file->private_data;\n\tstruct vduse_dev_msg *msg;\n\tint size = sizeof(struct vduse_dev_request);\n\tssize_t ret;\n\n\tif (iov_iter_count(to) < size)\n\t\treturn -EINVAL;\n\n\tspin_lock(&dev->msg_lock);\n\twhile (1) {\n\t\tmsg = vduse_dequeue_msg(&dev->send_list);\n\t\tif (msg)\n\t\t\tbreak;\n\n\t\tret = -EAGAIN;\n\t\tif (file->f_flags & O_NONBLOCK)\n\t\t\tgoto unlock;\n\n\t\tspin_unlock(&dev->msg_lock);\n\t\tret = wait_event_interruptible_exclusive(dev->waitq,\n\t\t\t\t\t!list_empty(&dev->send_list));\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tspin_lock(&dev->msg_lock);\n\t}\n\tspin_unlock(&dev->msg_lock);\n\tret = copy_to_iter(&msg->req, size, to);\n\tspin_lock(&dev->msg_lock);\n\tif (ret != size) {\n\t\tret = -EFAULT;\n\t\tvduse_enqueue_msg(&dev->send_list, msg);\n\t\tgoto unlock;\n\t}\n\tvduse_enqueue_msg(&dev->recv_list, msg);\nunlock:\n\tspin_unlock(&dev->msg_lock);\n\n\treturn ret;\n}\n\nstatic bool is_mem_zero(const char *ptr, int size)\n{\n\tint i;\n\n\tfor (i = 0; i < size; i++) {\n\t\tif (ptr[i])\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic ssize_t vduse_dev_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct vduse_dev *dev = file->private_data;\n\tstruct vduse_dev_response resp;\n\tstruct vduse_dev_msg *msg;\n\tsize_t ret;\n\n\tret = copy_from_iter(&resp, sizeof(resp), from);\n\tif (ret != sizeof(resp))\n\t\treturn -EINVAL;\n\n\tif (!is_mem_zero((const char *)resp.reserved, sizeof(resp.reserved)))\n\t\treturn -EINVAL;\n\n\tspin_lock(&dev->msg_lock);\n\tmsg = vduse_find_msg(&dev->recv_list, resp.request_id);\n\tif (!msg) {\n\t\tret = -ENOENT;\n\t\tgoto unlock;\n\t}\n\n\tmemcpy(&msg->resp, &resp, sizeof(resp));\n\tmsg->completed = 1;\n\twake_up(&msg->waitq);\nunlock:\n\tspin_unlock(&dev->msg_lock);\n\n\treturn ret;\n}\n\nstatic __poll_t vduse_dev_poll(struct file *file, poll_table *wait)\n{\n\tstruct vduse_dev *dev = file->private_data;\n\t__poll_t mask = 0;\n\n\tpoll_wait(file, &dev->waitq, wait);\n\n\tspin_lock(&dev->msg_lock);\n\n\tif (unlikely(dev->broken))\n\t\tmask |= EPOLLERR;\n\tif (!list_empty(&dev->send_list))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\tif (!list_empty(&dev->recv_list))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\n\tspin_unlock(&dev->msg_lock);\n\n\treturn mask;\n}\n\nstatic void vduse_dev_reset(struct vduse_dev *dev)\n{\n\tint i;\n\tstruct vduse_iova_domain *domain = dev->domain;\n\n\t \n\tif (domain && domain->bounce_map)\n\t\tvduse_domain_reset_bounce_map(domain);\n\n\tdown_write(&dev->rwsem);\n\n\tdev->status = 0;\n\tdev->driver_features = 0;\n\tdev->generation++;\n\tspin_lock(&dev->irq_lock);\n\tdev->config_cb.callback = NULL;\n\tdev->config_cb.private = NULL;\n\tspin_unlock(&dev->irq_lock);\n\tflush_work(&dev->inject);\n\n\tfor (i = 0; i < dev->vq_num; i++) {\n\t\tstruct vduse_virtqueue *vq = dev->vqs[i];\n\n\t\tvq->ready = false;\n\t\tvq->desc_addr = 0;\n\t\tvq->driver_addr = 0;\n\t\tvq->device_addr = 0;\n\t\tvq->num = 0;\n\t\tmemset(&vq->state, 0, sizeof(vq->state));\n\n\t\tspin_lock(&vq->kick_lock);\n\t\tvq->kicked = false;\n\t\tif (vq->kickfd)\n\t\t\teventfd_ctx_put(vq->kickfd);\n\t\tvq->kickfd = NULL;\n\t\tspin_unlock(&vq->kick_lock);\n\n\t\tspin_lock(&vq->irq_lock);\n\t\tvq->cb.callback = NULL;\n\t\tvq->cb.private = NULL;\n\t\tvq->cb.trigger = NULL;\n\t\tspin_unlock(&vq->irq_lock);\n\t\tflush_work(&vq->inject);\n\t\tflush_work(&vq->kick);\n\t}\n\n\tup_write(&dev->rwsem);\n}\n\nstatic int vduse_vdpa_set_vq_address(struct vdpa_device *vdpa, u16 idx,\n\t\t\t\tu64 desc_area, u64 driver_area,\n\t\t\t\tu64 device_area)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tstruct vduse_virtqueue *vq = dev->vqs[idx];\n\n\tvq->desc_addr = desc_area;\n\tvq->driver_addr = driver_area;\n\tvq->device_addr = device_area;\n\n\treturn 0;\n}\n\nstatic void vduse_vq_kick(struct vduse_virtqueue *vq)\n{\n\tspin_lock(&vq->kick_lock);\n\tif (!vq->ready)\n\t\tgoto unlock;\n\n\tif (vq->kickfd)\n\t\teventfd_signal(vq->kickfd, 1);\n\telse\n\t\tvq->kicked = true;\nunlock:\n\tspin_unlock(&vq->kick_lock);\n}\n\nstatic void vduse_vq_kick_work(struct work_struct *work)\n{\n\tstruct vduse_virtqueue *vq = container_of(work,\n\t\t\t\t\tstruct vduse_virtqueue, kick);\n\n\tvduse_vq_kick(vq);\n}\n\nstatic void vduse_vdpa_kick_vq(struct vdpa_device *vdpa, u16 idx)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tstruct vduse_virtqueue *vq = dev->vqs[idx];\n\n\tif (!eventfd_signal_allowed()) {\n\t\tschedule_work(&vq->kick);\n\t\treturn;\n\t}\n\tvduse_vq_kick(vq);\n}\n\nstatic void vduse_vdpa_set_vq_cb(struct vdpa_device *vdpa, u16 idx,\n\t\t\t      struct vdpa_callback *cb)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tstruct vduse_virtqueue *vq = dev->vqs[idx];\n\n\tspin_lock(&vq->irq_lock);\n\tvq->cb.callback = cb->callback;\n\tvq->cb.private = cb->private;\n\tvq->cb.trigger = cb->trigger;\n\tspin_unlock(&vq->irq_lock);\n}\n\nstatic void vduse_vdpa_set_vq_num(struct vdpa_device *vdpa, u16 idx, u32 num)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tstruct vduse_virtqueue *vq = dev->vqs[idx];\n\n\tvq->num = num;\n}\n\nstatic void vduse_vdpa_set_vq_ready(struct vdpa_device *vdpa,\n\t\t\t\t\tu16 idx, bool ready)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tstruct vduse_virtqueue *vq = dev->vqs[idx];\n\n\tvq->ready = ready;\n}\n\nstatic bool vduse_vdpa_get_vq_ready(struct vdpa_device *vdpa, u16 idx)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tstruct vduse_virtqueue *vq = dev->vqs[idx];\n\n\treturn vq->ready;\n}\n\nstatic int vduse_vdpa_set_vq_state(struct vdpa_device *vdpa, u16 idx,\n\t\t\t\tconst struct vdpa_vq_state *state)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tstruct vduse_virtqueue *vq = dev->vqs[idx];\n\n\tif (dev->driver_features & BIT_ULL(VIRTIO_F_RING_PACKED)) {\n\t\tvq->state.packed.last_avail_counter =\n\t\t\t\tstate->packed.last_avail_counter;\n\t\tvq->state.packed.last_avail_idx = state->packed.last_avail_idx;\n\t\tvq->state.packed.last_used_counter =\n\t\t\t\tstate->packed.last_used_counter;\n\t\tvq->state.packed.last_used_idx = state->packed.last_used_idx;\n\t} else\n\t\tvq->state.split.avail_index = state->split.avail_index;\n\n\treturn 0;\n}\n\nstatic int vduse_vdpa_get_vq_state(struct vdpa_device *vdpa, u16 idx,\n\t\t\t\tstruct vdpa_vq_state *state)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tstruct vduse_virtqueue *vq = dev->vqs[idx];\n\n\tif (dev->driver_features & BIT_ULL(VIRTIO_F_RING_PACKED))\n\t\treturn vduse_dev_get_vq_state_packed(dev, vq, &state->packed);\n\n\treturn vduse_dev_get_vq_state_split(dev, vq, &state->split);\n}\n\nstatic u32 vduse_vdpa_get_vq_align(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\treturn dev->vq_align;\n}\n\nstatic u64 vduse_vdpa_get_device_features(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\treturn dev->device_features;\n}\n\nstatic int vduse_vdpa_set_driver_features(struct vdpa_device *vdpa, u64 features)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\tdev->driver_features = features;\n\treturn 0;\n}\n\nstatic u64 vduse_vdpa_get_driver_features(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\treturn dev->driver_features;\n}\n\nstatic void vduse_vdpa_set_config_cb(struct vdpa_device *vdpa,\n\t\t\t\t  struct vdpa_callback *cb)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\tspin_lock(&dev->irq_lock);\n\tdev->config_cb.callback = cb->callback;\n\tdev->config_cb.private = cb->private;\n\tspin_unlock(&dev->irq_lock);\n}\n\nstatic u16 vduse_vdpa_get_vq_num_max(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tu16 num_max = 0;\n\tint i;\n\n\tfor (i = 0; i < dev->vq_num; i++)\n\t\tif (num_max < dev->vqs[i]->num_max)\n\t\t\tnum_max = dev->vqs[i]->num_max;\n\n\treturn num_max;\n}\n\nstatic u32 vduse_vdpa_get_device_id(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\treturn dev->device_id;\n}\n\nstatic u32 vduse_vdpa_get_vendor_id(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\treturn dev->vendor_id;\n}\n\nstatic u8 vduse_vdpa_get_status(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\treturn dev->status;\n}\n\nstatic void vduse_vdpa_set_status(struct vdpa_device *vdpa, u8 status)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\tif (vduse_dev_set_status(dev, status))\n\t\treturn;\n\n\tdev->status = status;\n}\n\nstatic size_t vduse_vdpa_get_config_size(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\treturn dev->config_size;\n}\n\nstatic void vduse_vdpa_get_config(struct vdpa_device *vdpa, unsigned int offset,\n\t\t\t\t  void *buf, unsigned int len)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\t \n\tmemset(buf, 0, len);\n\n\tif (offset > dev->config_size)\n\t\treturn;\n\n\tif (len > dev->config_size - offset)\n\t\tlen = dev->config_size - offset;\n\n\tmemcpy(buf, dev->config + offset, len);\n}\n\nstatic void vduse_vdpa_set_config(struct vdpa_device *vdpa, unsigned int offset,\n\t\t\tconst void *buf, unsigned int len)\n{\n\t \n}\n\nstatic int vduse_vdpa_reset(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tint ret = vduse_dev_set_status(dev, 0);\n\n\tvduse_dev_reset(dev);\n\n\treturn ret;\n}\n\nstatic u32 vduse_vdpa_get_generation(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\treturn dev->generation;\n}\n\nstatic int vduse_vdpa_set_vq_affinity(struct vdpa_device *vdpa, u16 idx,\n\t\t\t\t      const struct cpumask *cpu_mask)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\tif (cpu_mask)\n\t\tcpumask_copy(&dev->vqs[idx]->irq_affinity, cpu_mask);\n\telse\n\t\tcpumask_setall(&dev->vqs[idx]->irq_affinity);\n\n\treturn 0;\n}\n\nstatic const struct cpumask *\nvduse_vdpa_get_vq_affinity(struct vdpa_device *vdpa, u16 idx)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\treturn &dev->vqs[idx]->irq_affinity;\n}\n\nstatic int vduse_vdpa_set_map(struct vdpa_device *vdpa,\n\t\t\t\tunsigned int asid,\n\t\t\t\tstruct vhost_iotlb *iotlb)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\tint ret;\n\n\tret = vduse_domain_set_map(dev->domain, iotlb);\n\tif (ret)\n\t\treturn ret;\n\n\tret = vduse_dev_update_iotlb(dev, 0ULL, ULLONG_MAX);\n\tif (ret) {\n\t\tvduse_domain_clear_map(dev->domain, iotlb);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void vduse_vdpa_free(struct vdpa_device *vdpa)\n{\n\tstruct vduse_dev *dev = vdpa_to_vduse(vdpa);\n\n\tdev->vdev = NULL;\n}\n\nstatic const struct vdpa_config_ops vduse_vdpa_config_ops = {\n\t.set_vq_address\t\t= vduse_vdpa_set_vq_address,\n\t.kick_vq\t\t= vduse_vdpa_kick_vq,\n\t.set_vq_cb\t\t= vduse_vdpa_set_vq_cb,\n\t.set_vq_num             = vduse_vdpa_set_vq_num,\n\t.set_vq_ready\t\t= vduse_vdpa_set_vq_ready,\n\t.get_vq_ready\t\t= vduse_vdpa_get_vq_ready,\n\t.set_vq_state\t\t= vduse_vdpa_set_vq_state,\n\t.get_vq_state\t\t= vduse_vdpa_get_vq_state,\n\t.get_vq_align\t\t= vduse_vdpa_get_vq_align,\n\t.get_device_features\t= vduse_vdpa_get_device_features,\n\t.set_driver_features\t= vduse_vdpa_set_driver_features,\n\t.get_driver_features\t= vduse_vdpa_get_driver_features,\n\t.set_config_cb\t\t= vduse_vdpa_set_config_cb,\n\t.get_vq_num_max\t\t= vduse_vdpa_get_vq_num_max,\n\t.get_device_id\t\t= vduse_vdpa_get_device_id,\n\t.get_vendor_id\t\t= vduse_vdpa_get_vendor_id,\n\t.get_status\t\t= vduse_vdpa_get_status,\n\t.set_status\t\t= vduse_vdpa_set_status,\n\t.get_config_size\t= vduse_vdpa_get_config_size,\n\t.get_config\t\t= vduse_vdpa_get_config,\n\t.set_config\t\t= vduse_vdpa_set_config,\n\t.get_generation\t\t= vduse_vdpa_get_generation,\n\t.set_vq_affinity\t= vduse_vdpa_set_vq_affinity,\n\t.get_vq_affinity\t= vduse_vdpa_get_vq_affinity,\n\t.reset\t\t\t= vduse_vdpa_reset,\n\t.set_map\t\t= vduse_vdpa_set_map,\n\t.free\t\t\t= vduse_vdpa_free,\n};\n\nstatic dma_addr_t vduse_dev_map_page(struct device *dev, struct page *page,\n\t\t\t\t     unsigned long offset, size_t size,\n\t\t\t\t     enum dma_data_direction dir,\n\t\t\t\t     unsigned long attrs)\n{\n\tstruct vduse_dev *vdev = dev_to_vduse(dev);\n\tstruct vduse_iova_domain *domain = vdev->domain;\n\n\treturn vduse_domain_map_page(domain, page, offset, size, dir, attrs);\n}\n\nstatic void vduse_dev_unmap_page(struct device *dev, dma_addr_t dma_addr,\n\t\t\t\tsize_t size, enum dma_data_direction dir,\n\t\t\t\tunsigned long attrs)\n{\n\tstruct vduse_dev *vdev = dev_to_vduse(dev);\n\tstruct vduse_iova_domain *domain = vdev->domain;\n\n\treturn vduse_domain_unmap_page(domain, dma_addr, size, dir, attrs);\n}\n\nstatic void *vduse_dev_alloc_coherent(struct device *dev, size_t size,\n\t\t\t\t\tdma_addr_t *dma_addr, gfp_t flag,\n\t\t\t\t\tunsigned long attrs)\n{\n\tstruct vduse_dev *vdev = dev_to_vduse(dev);\n\tstruct vduse_iova_domain *domain = vdev->domain;\n\tunsigned long iova;\n\tvoid *addr;\n\n\t*dma_addr = DMA_MAPPING_ERROR;\n\taddr = vduse_domain_alloc_coherent(domain, size,\n\t\t\t\t(dma_addr_t *)&iova, flag, attrs);\n\tif (!addr)\n\t\treturn NULL;\n\n\t*dma_addr = (dma_addr_t)iova;\n\n\treturn addr;\n}\n\nstatic void vduse_dev_free_coherent(struct device *dev, size_t size,\n\t\t\t\t\tvoid *vaddr, dma_addr_t dma_addr,\n\t\t\t\t\tunsigned long attrs)\n{\n\tstruct vduse_dev *vdev = dev_to_vduse(dev);\n\tstruct vduse_iova_domain *domain = vdev->domain;\n\n\tvduse_domain_free_coherent(domain, size, vaddr, dma_addr, attrs);\n}\n\nstatic size_t vduse_dev_max_mapping_size(struct device *dev)\n{\n\tstruct vduse_dev *vdev = dev_to_vduse(dev);\n\tstruct vduse_iova_domain *domain = vdev->domain;\n\n\treturn domain->bounce_size;\n}\n\nstatic const struct dma_map_ops vduse_dev_dma_ops = {\n\t.map_page = vduse_dev_map_page,\n\t.unmap_page = vduse_dev_unmap_page,\n\t.alloc = vduse_dev_alloc_coherent,\n\t.free = vduse_dev_free_coherent,\n\t.max_mapping_size = vduse_dev_max_mapping_size,\n};\n\nstatic unsigned int perm_to_file_flags(u8 perm)\n{\n\tunsigned int flags = 0;\n\n\tswitch (perm) {\n\tcase VDUSE_ACCESS_WO:\n\t\tflags |= O_WRONLY;\n\t\tbreak;\n\tcase VDUSE_ACCESS_RO:\n\t\tflags |= O_RDONLY;\n\t\tbreak;\n\tcase VDUSE_ACCESS_RW:\n\t\tflags |= O_RDWR;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"invalidate vhost IOTLB permission\\n\");\n\t\tbreak;\n\t}\n\n\treturn flags;\n}\n\nstatic int vduse_kickfd_setup(struct vduse_dev *dev,\n\t\t\tstruct vduse_vq_eventfd *eventfd)\n{\n\tstruct eventfd_ctx *ctx = NULL;\n\tstruct vduse_virtqueue *vq;\n\tu32 index;\n\n\tif (eventfd->index >= dev->vq_num)\n\t\treturn -EINVAL;\n\n\tindex = array_index_nospec(eventfd->index, dev->vq_num);\n\tvq = dev->vqs[index];\n\tif (eventfd->fd >= 0) {\n\t\tctx = eventfd_ctx_fdget(eventfd->fd);\n\t\tif (IS_ERR(ctx))\n\t\t\treturn PTR_ERR(ctx);\n\t} else if (eventfd->fd != VDUSE_EVENTFD_DEASSIGN)\n\t\treturn 0;\n\n\tspin_lock(&vq->kick_lock);\n\tif (vq->kickfd)\n\t\teventfd_ctx_put(vq->kickfd);\n\tvq->kickfd = ctx;\n\tif (vq->ready && vq->kicked && vq->kickfd) {\n\t\teventfd_signal(vq->kickfd, 1);\n\t\tvq->kicked = false;\n\t}\n\tspin_unlock(&vq->kick_lock);\n\n\treturn 0;\n}\n\nstatic bool vduse_dev_is_ready(struct vduse_dev *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < dev->vq_num; i++)\n\t\tif (!dev->vqs[i]->num_max)\n\t\t\treturn false;\n\n\treturn true;\n}\n\nstatic void vduse_dev_irq_inject(struct work_struct *work)\n{\n\tstruct vduse_dev *dev = container_of(work, struct vduse_dev, inject);\n\n\tspin_lock_bh(&dev->irq_lock);\n\tif (dev->config_cb.callback)\n\t\tdev->config_cb.callback(dev->config_cb.private);\n\tspin_unlock_bh(&dev->irq_lock);\n}\n\nstatic void vduse_vq_irq_inject(struct work_struct *work)\n{\n\tstruct vduse_virtqueue *vq = container_of(work,\n\t\t\t\t\tstruct vduse_virtqueue, inject);\n\n\tspin_lock_bh(&vq->irq_lock);\n\tif (vq->ready && vq->cb.callback)\n\t\tvq->cb.callback(vq->cb.private);\n\tspin_unlock_bh(&vq->irq_lock);\n}\n\nstatic bool vduse_vq_signal_irqfd(struct vduse_virtqueue *vq)\n{\n\tbool signal = false;\n\n\tif (!vq->cb.trigger)\n\t\treturn false;\n\n\tspin_lock_irq(&vq->irq_lock);\n\tif (vq->ready && vq->cb.trigger) {\n\t\teventfd_signal(vq->cb.trigger, 1);\n\t\tsignal = true;\n\t}\n\tspin_unlock_irq(&vq->irq_lock);\n\n\treturn signal;\n}\n\nstatic int vduse_dev_queue_irq_work(struct vduse_dev *dev,\n\t\t\t\t    struct work_struct *irq_work,\n\t\t\t\t    int irq_effective_cpu)\n{\n\tint ret = -EINVAL;\n\n\tdown_read(&dev->rwsem);\n\tif (!(dev->status & VIRTIO_CONFIG_S_DRIVER_OK))\n\t\tgoto unlock;\n\n\tret = 0;\n\tif (irq_effective_cpu == IRQ_UNBOUND)\n\t\tqueue_work(vduse_irq_wq, irq_work);\n\telse\n\t\tqueue_work_on(irq_effective_cpu,\n\t\t\t      vduse_irq_bound_wq, irq_work);\nunlock:\n\tup_read(&dev->rwsem);\n\n\treturn ret;\n}\n\nstatic int vduse_dev_dereg_umem(struct vduse_dev *dev,\n\t\t\t\tu64 iova, u64 size)\n{\n\tint ret;\n\n\tmutex_lock(&dev->mem_lock);\n\tret = -ENOENT;\n\tif (!dev->umem)\n\t\tgoto unlock;\n\n\tret = -EINVAL;\n\tif (!dev->domain)\n\t\tgoto unlock;\n\n\tif (dev->umem->iova != iova || size != dev->domain->bounce_size)\n\t\tgoto unlock;\n\n\tvduse_domain_remove_user_bounce_pages(dev->domain);\n\tunpin_user_pages_dirty_lock(dev->umem->pages,\n\t\t\t\t    dev->umem->npages, true);\n\tatomic64_sub(dev->umem->npages, &dev->umem->mm->pinned_vm);\n\tmmdrop(dev->umem->mm);\n\tvfree(dev->umem->pages);\n\tkfree(dev->umem);\n\tdev->umem = NULL;\n\tret = 0;\nunlock:\n\tmutex_unlock(&dev->mem_lock);\n\treturn ret;\n}\n\nstatic int vduse_dev_reg_umem(struct vduse_dev *dev,\n\t\t\t      u64 iova, u64 uaddr, u64 size)\n{\n\tstruct page **page_list = NULL;\n\tstruct vduse_umem *umem = NULL;\n\tlong pinned = 0;\n\tunsigned long npages, lock_limit;\n\tint ret;\n\n\tif (!dev->domain || !dev->domain->bounce_map ||\n\t    size != dev->domain->bounce_size ||\n\t    iova != 0 || uaddr & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&dev->mem_lock);\n\tret = -EEXIST;\n\tif (dev->umem)\n\t\tgoto unlock;\n\n\tret = -ENOMEM;\n\tnpages = size >> PAGE_SHIFT;\n\tpage_list = __vmalloc(array_size(npages, sizeof(struct page *)),\n\t\t\t      GFP_KERNEL_ACCOUNT);\n\tumem = kzalloc(sizeof(*umem), GFP_KERNEL);\n\tif (!page_list || !umem)\n\t\tgoto unlock;\n\n\tmmap_read_lock(current->mm);\n\n\tlock_limit = PFN_DOWN(rlimit(RLIMIT_MEMLOCK));\n\tif (npages + atomic64_read(&current->mm->pinned_vm) > lock_limit)\n\t\tgoto out;\n\n\tpinned = pin_user_pages(uaddr, npages, FOLL_LONGTERM | FOLL_WRITE,\n\t\t\t\tpage_list);\n\tif (pinned != npages) {\n\t\tret = pinned < 0 ? pinned : -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tret = vduse_domain_add_user_bounce_pages(dev->domain,\n\t\t\t\t\t\t page_list, pinned);\n\tif (ret)\n\t\tgoto out;\n\n\tatomic64_add(npages, &current->mm->pinned_vm);\n\n\tumem->pages = page_list;\n\tumem->npages = pinned;\n\tumem->iova = iova;\n\tumem->mm = current->mm;\n\tmmgrab(current->mm);\n\n\tdev->umem = umem;\nout:\n\tif (ret && pinned > 0)\n\t\tunpin_user_pages(page_list, pinned);\n\n\tmmap_read_unlock(current->mm);\nunlock:\n\tif (ret) {\n\t\tvfree(page_list);\n\t\tkfree(umem);\n\t}\n\tmutex_unlock(&dev->mem_lock);\n\treturn ret;\n}\n\nstatic void vduse_vq_update_effective_cpu(struct vduse_virtqueue *vq)\n{\n\tint curr_cpu = vq->irq_effective_cpu;\n\n\twhile (true) {\n\t\tcurr_cpu = cpumask_next(curr_cpu, &vq->irq_affinity);\n\t\tif (cpu_online(curr_cpu))\n\t\t\tbreak;\n\n\t\tif (curr_cpu >= nr_cpu_ids)\n\t\t\tcurr_cpu = IRQ_UNBOUND;\n\t}\n\n\tvq->irq_effective_cpu = curr_cpu;\n}\n\nstatic long vduse_dev_ioctl(struct file *file, unsigned int cmd,\n\t\t\t    unsigned long arg)\n{\n\tstruct vduse_dev *dev = file->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tint ret;\n\n\tif (unlikely(dev->broken))\n\t\treturn -EPERM;\n\n\tswitch (cmd) {\n\tcase VDUSE_IOTLB_GET_FD: {\n\t\tstruct vduse_iotlb_entry entry;\n\t\tstruct vhost_iotlb_map *map;\n\t\tstruct vdpa_map_file *map_file;\n\t\tstruct file *f = NULL;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&entry, argp, sizeof(entry)))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (entry.start > entry.last)\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->domain_lock);\n\t\tif (!dev->domain) {\n\t\t\tmutex_unlock(&dev->domain_lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_lock(&dev->domain->iotlb_lock);\n\t\tmap = vhost_iotlb_itree_first(dev->domain->iotlb,\n\t\t\t\t\t      entry.start, entry.last);\n\t\tif (map) {\n\t\t\tmap_file = (struct vdpa_map_file *)map->opaque;\n\t\t\tf = get_file(map_file->file);\n\t\t\tentry.offset = map_file->offset;\n\t\t\tentry.start = map->start;\n\t\t\tentry.last = map->last;\n\t\t\tentry.perm = map->perm;\n\t\t}\n\t\tspin_unlock(&dev->domain->iotlb_lock);\n\t\tmutex_unlock(&dev->domain_lock);\n\t\tret = -EINVAL;\n\t\tif (!f)\n\t\t\tbreak;\n\n\t\tret = -EFAULT;\n\t\tif (copy_to_user(argp, &entry, sizeof(entry))) {\n\t\t\tfput(f);\n\t\t\tbreak;\n\t\t}\n\t\tret = receive_fd(f, perm_to_file_flags(entry.perm));\n\t\tfput(f);\n\t\tbreak;\n\t}\n\tcase VDUSE_DEV_GET_FEATURES:\n\t\t \n\t\tret = put_user(dev->driver_features, (u64 __user *)argp);\n\t\tbreak;\n\tcase VDUSE_DEV_SET_CONFIG: {\n\t\tstruct vduse_config_data config;\n\t\tunsigned long size = offsetof(struct vduse_config_data,\n\t\t\t\t\t      buffer);\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&config, argp, size))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (config.offset > dev->config_size ||\n\t\t    config.length == 0 ||\n\t\t    config.length > dev->config_size - config.offset)\n\t\t\tbreak;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(dev->config + config.offset, argp + size,\n\t\t\t\t   config.length))\n\t\t\tbreak;\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\tcase VDUSE_DEV_INJECT_CONFIG_IRQ:\n\t\tret = vduse_dev_queue_irq_work(dev, &dev->inject, IRQ_UNBOUND);\n\t\tbreak;\n\tcase VDUSE_VQ_SETUP: {\n\t\tstruct vduse_vq_config config;\n\t\tu32 index;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&config, argp, sizeof(config)))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (config.index >= dev->vq_num)\n\t\t\tbreak;\n\n\t\tif (!is_mem_zero((const char *)config.reserved,\n\t\t\t\t sizeof(config.reserved)))\n\t\t\tbreak;\n\n\t\tindex = array_index_nospec(config.index, dev->vq_num);\n\t\tdev->vqs[index]->num_max = config.max_size;\n\t\tret = 0;\n\t\tbreak;\n\t}\n\tcase VDUSE_VQ_GET_INFO: {\n\t\tstruct vduse_vq_info vq_info;\n\t\tstruct vduse_virtqueue *vq;\n\t\tu32 index;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&vq_info, argp, sizeof(vq_info)))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (vq_info.index >= dev->vq_num)\n\t\t\tbreak;\n\n\t\tindex = array_index_nospec(vq_info.index, dev->vq_num);\n\t\tvq = dev->vqs[index];\n\t\tvq_info.desc_addr = vq->desc_addr;\n\t\tvq_info.driver_addr = vq->driver_addr;\n\t\tvq_info.device_addr = vq->device_addr;\n\t\tvq_info.num = vq->num;\n\n\t\tif (dev->driver_features & BIT_ULL(VIRTIO_F_RING_PACKED)) {\n\t\t\tvq_info.packed.last_avail_counter =\n\t\t\t\tvq->state.packed.last_avail_counter;\n\t\t\tvq_info.packed.last_avail_idx =\n\t\t\t\tvq->state.packed.last_avail_idx;\n\t\t\tvq_info.packed.last_used_counter =\n\t\t\t\tvq->state.packed.last_used_counter;\n\t\t\tvq_info.packed.last_used_idx =\n\t\t\t\tvq->state.packed.last_used_idx;\n\t\t} else\n\t\t\tvq_info.split.avail_index =\n\t\t\t\tvq->state.split.avail_index;\n\n\t\tvq_info.ready = vq->ready;\n\n\t\tret = -EFAULT;\n\t\tif (copy_to_user(argp, &vq_info, sizeof(vq_info)))\n\t\t\tbreak;\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\tcase VDUSE_VQ_SETUP_KICKFD: {\n\t\tstruct vduse_vq_eventfd eventfd;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&eventfd, argp, sizeof(eventfd)))\n\t\t\tbreak;\n\n\t\tret = vduse_kickfd_setup(dev, &eventfd);\n\t\tbreak;\n\t}\n\tcase VDUSE_VQ_INJECT_IRQ: {\n\t\tu32 index;\n\n\t\tret = -EFAULT;\n\t\tif (get_user(index, (u32 __user *)argp))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (index >= dev->vq_num)\n\t\t\tbreak;\n\n\t\tret = 0;\n\t\tindex = array_index_nospec(index, dev->vq_num);\n\t\tif (!vduse_vq_signal_irqfd(dev->vqs[index])) {\n\t\t\tvduse_vq_update_effective_cpu(dev->vqs[index]);\n\t\t\tret = vduse_dev_queue_irq_work(dev,\n\t\t\t\t\t\t&dev->vqs[index]->inject,\n\t\t\t\t\t\tdev->vqs[index]->irq_effective_cpu);\n\t\t}\n\t\tbreak;\n\t}\n\tcase VDUSE_IOTLB_REG_UMEM: {\n\t\tstruct vduse_iova_umem umem;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&umem, argp, sizeof(umem)))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (!is_mem_zero((const char *)umem.reserved,\n\t\t\t\t sizeof(umem.reserved)))\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->domain_lock);\n\t\tret = vduse_dev_reg_umem(dev, umem.iova,\n\t\t\t\t\t umem.uaddr, umem.size);\n\t\tmutex_unlock(&dev->domain_lock);\n\t\tbreak;\n\t}\n\tcase VDUSE_IOTLB_DEREG_UMEM: {\n\t\tstruct vduse_iova_umem umem;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&umem, argp, sizeof(umem)))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (!is_mem_zero((const char *)umem.reserved,\n\t\t\t\t sizeof(umem.reserved)))\n\t\t\tbreak;\n\t\tmutex_lock(&dev->domain_lock);\n\t\tret = vduse_dev_dereg_umem(dev, umem.iova,\n\t\t\t\t\t   umem.size);\n\t\tmutex_unlock(&dev->domain_lock);\n\t\tbreak;\n\t}\n\tcase VDUSE_IOTLB_GET_INFO: {\n\t\tstruct vduse_iova_info info;\n\t\tstruct vhost_iotlb_map *map;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&info, argp, sizeof(info)))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (info.start > info.last)\n\t\t\tbreak;\n\n\t\tif (!is_mem_zero((const char *)info.reserved,\n\t\t\t\t sizeof(info.reserved)))\n\t\t\tbreak;\n\n\t\tmutex_lock(&dev->domain_lock);\n\t\tif (!dev->domain) {\n\t\t\tmutex_unlock(&dev->domain_lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_lock(&dev->domain->iotlb_lock);\n\t\tmap = vhost_iotlb_itree_first(dev->domain->iotlb,\n\t\t\t\t\t      info.start, info.last);\n\t\tif (map) {\n\t\t\tinfo.start = map->start;\n\t\t\tinfo.last = map->last;\n\t\t\tinfo.capability = 0;\n\t\t\tif (dev->domain->bounce_map && map->start == 0 &&\n\t\t\t    map->last == dev->domain->bounce_size - 1)\n\t\t\t\tinfo.capability |= VDUSE_IOVA_CAP_UMEM;\n\t\t}\n\t\tspin_unlock(&dev->domain->iotlb_lock);\n\t\tmutex_unlock(&dev->domain_lock);\n\t\tif (!map)\n\t\t\tbreak;\n\n\t\tret = -EFAULT;\n\t\tif (copy_to_user(argp, &info, sizeof(info)))\n\t\t\tbreak;\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int vduse_dev_release(struct inode *inode, struct file *file)\n{\n\tstruct vduse_dev *dev = file->private_data;\n\n\tmutex_lock(&dev->domain_lock);\n\tif (dev->domain)\n\t\tvduse_dev_dereg_umem(dev, 0, dev->domain->bounce_size);\n\tmutex_unlock(&dev->domain_lock);\n\tspin_lock(&dev->msg_lock);\n\t \n\tlist_splice_init(&dev->recv_list, &dev->send_list);\n\tspin_unlock(&dev->msg_lock);\n\tdev->connected = false;\n\n\treturn 0;\n}\n\nstatic struct vduse_dev *vduse_dev_get_from_minor(int minor)\n{\n\tstruct vduse_dev *dev;\n\n\tmutex_lock(&vduse_lock);\n\tdev = idr_find(&vduse_idr, minor);\n\tmutex_unlock(&vduse_lock);\n\n\treturn dev;\n}\n\nstatic int vduse_dev_open(struct inode *inode, struct file *file)\n{\n\tint ret;\n\tstruct vduse_dev *dev = vduse_dev_get_from_minor(iminor(inode));\n\n\tif (!dev)\n\t\treturn -ENODEV;\n\n\tret = -EBUSY;\n\tmutex_lock(&dev->lock);\n\tif (dev->connected)\n\t\tgoto unlock;\n\n\tret = 0;\n\tdev->connected = true;\n\tfile->private_data = dev;\nunlock:\n\tmutex_unlock(&dev->lock);\n\n\treturn ret;\n}\n\nstatic const struct file_operations vduse_dev_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= vduse_dev_open,\n\t.release\t= vduse_dev_release,\n\t.read_iter\t= vduse_dev_read_iter,\n\t.write_iter\t= vduse_dev_write_iter,\n\t.poll\t\t= vduse_dev_poll,\n\t.unlocked_ioctl\t= vduse_dev_ioctl,\n\t.compat_ioctl\t= compat_ptr_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic ssize_t irq_cb_affinity_show(struct vduse_virtqueue *vq, char *buf)\n{\n\treturn sprintf(buf, \"%*pb\\n\", cpumask_pr_args(&vq->irq_affinity));\n}\n\nstatic ssize_t irq_cb_affinity_store(struct vduse_virtqueue *vq,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tcpumask_var_t new_value;\n\tint ret;\n\n\tif (!zalloc_cpumask_var(&new_value, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tret = cpumask_parse(buf, new_value);\n\tif (ret)\n\t\tgoto free_mask;\n\n\tret = -EINVAL;\n\tif (!cpumask_intersects(new_value, cpu_online_mask))\n\t\tgoto free_mask;\n\n\tcpumask_copy(&vq->irq_affinity, new_value);\n\tret = count;\nfree_mask:\n\tfree_cpumask_var(new_value);\n\treturn ret;\n}\n\nstruct vq_sysfs_entry {\n\tstruct attribute attr;\n\tssize_t (*show)(struct vduse_virtqueue *vq, char *buf);\n\tssize_t (*store)(struct vduse_virtqueue *vq, const char *buf,\n\t\t\t size_t count);\n};\n\nstatic struct vq_sysfs_entry irq_cb_affinity_attr = __ATTR_RW(irq_cb_affinity);\n\nstatic struct attribute *vq_attrs[] = {\n\t&irq_cb_affinity_attr.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(vq);\n\nstatic ssize_t vq_attr_show(struct kobject *kobj, struct attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct vduse_virtqueue *vq = container_of(kobj,\n\t\t\t\t\tstruct vduse_virtqueue, kobj);\n\tstruct vq_sysfs_entry *entry = container_of(attr,\n\t\t\t\t\tstruct vq_sysfs_entry, attr);\n\n\tif (!entry->show)\n\t\treturn -EIO;\n\n\treturn entry->show(vq, buf);\n}\n\nstatic ssize_t vq_attr_store(struct kobject *kobj, struct attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\tstruct vduse_virtqueue *vq = container_of(kobj,\n\t\t\t\t\tstruct vduse_virtqueue, kobj);\n\tstruct vq_sysfs_entry *entry = container_of(attr,\n\t\t\t\t\tstruct vq_sysfs_entry, attr);\n\n\tif (!entry->store)\n\t\treturn -EIO;\n\n\treturn entry->store(vq, buf, count);\n}\n\nstatic const struct sysfs_ops vq_sysfs_ops = {\n\t.show = vq_attr_show,\n\t.store = vq_attr_store,\n};\n\nstatic void vq_release(struct kobject *kobj)\n{\n\tstruct vduse_virtqueue *vq = container_of(kobj,\n\t\t\t\t\tstruct vduse_virtqueue, kobj);\n\tkfree(vq);\n}\n\nstatic const struct kobj_type vq_type = {\n\t.release\t= vq_release,\n\t.sysfs_ops\t= &vq_sysfs_ops,\n\t.default_groups\t= vq_groups,\n};\n\nstatic void vduse_dev_deinit_vqs(struct vduse_dev *dev)\n{\n\tint i;\n\n\tif (!dev->vqs)\n\t\treturn;\n\n\tfor (i = 0; i < dev->vq_num; i++)\n\t\tkobject_put(&dev->vqs[i]->kobj);\n\tkfree(dev->vqs);\n}\n\nstatic int vduse_dev_init_vqs(struct vduse_dev *dev, u32 vq_align, u32 vq_num)\n{\n\tint ret, i;\n\n\tdev->vq_align = vq_align;\n\tdev->vq_num = vq_num;\n\tdev->vqs = kcalloc(dev->vq_num, sizeof(*dev->vqs), GFP_KERNEL);\n\tif (!dev->vqs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < vq_num; i++) {\n\t\tdev->vqs[i] = kzalloc(sizeof(*dev->vqs[i]), GFP_KERNEL);\n\t\tif (!dev->vqs[i]) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tdev->vqs[i]->index = i;\n\t\tdev->vqs[i]->irq_effective_cpu = IRQ_UNBOUND;\n\t\tINIT_WORK(&dev->vqs[i]->inject, vduse_vq_irq_inject);\n\t\tINIT_WORK(&dev->vqs[i]->kick, vduse_vq_kick_work);\n\t\tspin_lock_init(&dev->vqs[i]->kick_lock);\n\t\tspin_lock_init(&dev->vqs[i]->irq_lock);\n\t\tcpumask_setall(&dev->vqs[i]->irq_affinity);\n\n\t\tkobject_init(&dev->vqs[i]->kobj, &vq_type);\n\t\tret = kobject_add(&dev->vqs[i]->kobj,\n\t\t\t\t  &dev->dev->kobj, \"vq%d\", i);\n\t\tif (ret) {\n\t\t\tkfree(dev->vqs[i]);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn 0;\nerr:\n\twhile (i--)\n\t\tkobject_put(&dev->vqs[i]->kobj);\n\tkfree(dev->vqs);\n\tdev->vqs = NULL;\n\treturn ret;\n}\n\nstatic struct vduse_dev *vduse_dev_create(void)\n{\n\tstruct vduse_dev *dev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\n\tif (!dev)\n\t\treturn NULL;\n\n\tmutex_init(&dev->lock);\n\tmutex_init(&dev->mem_lock);\n\tmutex_init(&dev->domain_lock);\n\tspin_lock_init(&dev->msg_lock);\n\tINIT_LIST_HEAD(&dev->send_list);\n\tINIT_LIST_HEAD(&dev->recv_list);\n\tspin_lock_init(&dev->irq_lock);\n\tinit_rwsem(&dev->rwsem);\n\n\tINIT_WORK(&dev->inject, vduse_dev_irq_inject);\n\tinit_waitqueue_head(&dev->waitq);\n\n\treturn dev;\n}\n\nstatic void vduse_dev_destroy(struct vduse_dev *dev)\n{\n\tkfree(dev);\n}\n\nstatic struct vduse_dev *vduse_find_dev(const char *name)\n{\n\tstruct vduse_dev *dev;\n\tint id;\n\n\tidr_for_each_entry(&vduse_idr, dev, id)\n\t\tif (!strcmp(dev->name, name))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\n\nstatic int vduse_destroy_dev(char *name)\n{\n\tstruct vduse_dev *dev = vduse_find_dev(name);\n\n\tif (!dev)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&dev->lock);\n\tif (dev->vdev || dev->connected) {\n\t\tmutex_unlock(&dev->lock);\n\t\treturn -EBUSY;\n\t}\n\tdev->connected = true;\n\tmutex_unlock(&dev->lock);\n\n\tvduse_dev_reset(dev);\n\tdevice_destroy(vduse_class, MKDEV(MAJOR(vduse_major), dev->minor));\n\tidr_remove(&vduse_idr, dev->minor);\n\tkvfree(dev->config);\n\tvduse_dev_deinit_vqs(dev);\n\tif (dev->domain)\n\t\tvduse_domain_destroy(dev->domain);\n\tkfree(dev->name);\n\tvduse_dev_destroy(dev);\n\tmodule_put(THIS_MODULE);\n\n\treturn 0;\n}\n\nstatic bool device_is_allowed(u32 device_id)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(allowed_device_id); i++)\n\t\tif (allowed_device_id[i] == device_id)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool features_is_valid(u64 features)\n{\n\tif (!(features & (1ULL << VIRTIO_F_ACCESS_PLATFORM)))\n\t\treturn false;\n\n\t \n\tif (features & (1ULL << VIRTIO_BLK_F_CONFIG_WCE))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool vduse_validate_config(struct vduse_dev_config *config)\n{\n\tif (!is_mem_zero((const char *)config->reserved,\n\t\t\t sizeof(config->reserved)))\n\t\treturn false;\n\n\tif (config->vq_align > PAGE_SIZE)\n\t\treturn false;\n\n\tif (config->config_size > PAGE_SIZE)\n\t\treturn false;\n\n\tif (config->vq_num > 0xffff)\n\t\treturn false;\n\n\tif (!config->name[0])\n\t\treturn false;\n\n\tif (!device_is_allowed(config->device_id))\n\t\treturn false;\n\n\tif (!features_is_valid(config->features))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic ssize_t msg_timeout_show(struct device *device,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct vduse_dev *dev = dev_get_drvdata(device);\n\n\treturn sysfs_emit(buf, \"%u\\n\", dev->msg_timeout);\n}\n\nstatic ssize_t msg_timeout_store(struct device *device,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t count)\n{\n\tstruct vduse_dev *dev = dev_get_drvdata(device);\n\tint ret;\n\n\tret = kstrtouint(buf, 10, &dev->msg_timeout);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn count;\n}\n\nstatic DEVICE_ATTR_RW(msg_timeout);\n\nstatic ssize_t bounce_size_show(struct device *device,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct vduse_dev *dev = dev_get_drvdata(device);\n\n\treturn sysfs_emit(buf, \"%u\\n\", dev->bounce_size);\n}\n\nstatic ssize_t bounce_size_store(struct device *device,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t count)\n{\n\tstruct vduse_dev *dev = dev_get_drvdata(device);\n\tunsigned int bounce_size;\n\tint ret;\n\n\tret = -EPERM;\n\tmutex_lock(&dev->domain_lock);\n\tif (dev->domain)\n\t\tgoto unlock;\n\n\tret = kstrtouint(buf, 10, &bounce_size);\n\tif (ret < 0)\n\t\tgoto unlock;\n\n\tret = -EINVAL;\n\tif (bounce_size > VDUSE_MAX_BOUNCE_SIZE ||\n\t    bounce_size < VDUSE_MIN_BOUNCE_SIZE)\n\t\tgoto unlock;\n\n\tdev->bounce_size = bounce_size & PAGE_MASK;\n\tret = count;\nunlock:\n\tmutex_unlock(&dev->domain_lock);\n\treturn ret;\n}\n\nstatic DEVICE_ATTR_RW(bounce_size);\n\nstatic struct attribute *vduse_dev_attrs[] = {\n\t&dev_attr_msg_timeout.attr,\n\t&dev_attr_bounce_size.attr,\n\tNULL\n};\n\nATTRIBUTE_GROUPS(vduse_dev);\n\nstatic int vduse_create_dev(struct vduse_dev_config *config,\n\t\t\t    void *config_buf, u64 api_version)\n{\n\tint ret;\n\tstruct vduse_dev *dev;\n\n\tret = -EEXIST;\n\tif (vduse_find_dev(config->name))\n\t\tgoto err;\n\n\tret = -ENOMEM;\n\tdev = vduse_dev_create();\n\tif (!dev)\n\t\tgoto err;\n\n\tdev->api_version = api_version;\n\tdev->device_features = config->features;\n\tdev->device_id = config->device_id;\n\tdev->vendor_id = config->vendor_id;\n\tdev->name = kstrdup(config->name, GFP_KERNEL);\n\tif (!dev->name)\n\t\tgoto err_str;\n\n\tdev->bounce_size = VDUSE_BOUNCE_SIZE;\n\tdev->config = config_buf;\n\tdev->config_size = config->config_size;\n\n\tret = idr_alloc(&vduse_idr, dev, 1, VDUSE_DEV_MAX, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto err_idr;\n\n\tdev->minor = ret;\n\tdev->msg_timeout = VDUSE_MSG_DEFAULT_TIMEOUT;\n\tdev->dev = device_create_with_groups(vduse_class, NULL,\n\t\t\t\tMKDEV(MAJOR(vduse_major), dev->minor),\n\t\t\t\tdev, vduse_dev_groups, \"%s\", config->name);\n\tif (IS_ERR(dev->dev)) {\n\t\tret = PTR_ERR(dev->dev);\n\t\tgoto err_dev;\n\t}\n\n\tret = vduse_dev_init_vqs(dev, config->vq_align, config->vq_num);\n\tif (ret)\n\t\tgoto err_vqs;\n\n\t__module_get(THIS_MODULE);\n\n\treturn 0;\nerr_vqs:\n\tdevice_destroy(vduse_class, MKDEV(MAJOR(vduse_major), dev->minor));\nerr_dev:\n\tidr_remove(&vduse_idr, dev->minor);\nerr_idr:\n\tkfree(dev->name);\nerr_str:\n\tvduse_dev_destroy(dev);\nerr:\n\treturn ret;\n}\n\nstatic long vduse_ioctl(struct file *file, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tint ret;\n\tvoid __user *argp = (void __user *)arg;\n\tstruct vduse_control *control = file->private_data;\n\n\tmutex_lock(&vduse_lock);\n\tswitch (cmd) {\n\tcase VDUSE_GET_API_VERSION:\n\t\tret = put_user(control->api_version, (u64 __user *)argp);\n\t\tbreak;\n\tcase VDUSE_SET_API_VERSION: {\n\t\tu64 api_version;\n\n\t\tret = -EFAULT;\n\t\tif (get_user(api_version, (u64 __user *)argp))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (api_version > VDUSE_API_VERSION)\n\t\t\tbreak;\n\n\t\tret = 0;\n\t\tcontrol->api_version = api_version;\n\t\tbreak;\n\t}\n\tcase VDUSE_CREATE_DEV: {\n\t\tstruct vduse_dev_config config;\n\t\tunsigned long size = offsetof(struct vduse_dev_config, config);\n\t\tvoid *buf;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&config, argp, size))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (vduse_validate_config(&config) == false)\n\t\t\tbreak;\n\n\t\tbuf = vmemdup_user(argp + size, config.config_size);\n\t\tif (IS_ERR(buf)) {\n\t\t\tret = PTR_ERR(buf);\n\t\t\tbreak;\n\t\t}\n\t\tconfig.name[VDUSE_NAME_MAX - 1] = '\\0';\n\t\tret = vduse_create_dev(&config, buf, control->api_version);\n\t\tif (ret)\n\t\t\tkvfree(buf);\n\t\tbreak;\n\t}\n\tcase VDUSE_DESTROY_DEV: {\n\t\tchar name[VDUSE_NAME_MAX];\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(name, argp, VDUSE_NAME_MAX))\n\t\t\tbreak;\n\n\t\tname[VDUSE_NAME_MAX - 1] = '\\0';\n\t\tret = vduse_destroy_dev(name);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\tmutex_unlock(&vduse_lock);\n\n\treturn ret;\n}\n\nstatic int vduse_release(struct inode *inode, struct file *file)\n{\n\tstruct vduse_control *control = file->private_data;\n\n\tkfree(control);\n\treturn 0;\n}\n\nstatic int vduse_open(struct inode *inode, struct file *file)\n{\n\tstruct vduse_control *control;\n\n\tcontrol = kmalloc(sizeof(struct vduse_control), GFP_KERNEL);\n\tif (!control)\n\t\treturn -ENOMEM;\n\n\tcontrol->api_version = VDUSE_API_VERSION;\n\tfile->private_data = control;\n\n\treturn 0;\n}\n\nstatic const struct file_operations vduse_ctrl_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= vduse_open,\n\t.release\t= vduse_release,\n\t.unlocked_ioctl\t= vduse_ioctl,\n\t.compat_ioctl\t= compat_ptr_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic char *vduse_devnode(const struct device *dev, umode_t *mode)\n{\n\treturn kasprintf(GFP_KERNEL, \"vduse/%s\", dev_name(dev));\n}\n\nstruct vduse_mgmt_dev {\n\tstruct vdpa_mgmt_dev mgmt_dev;\n\tstruct device dev;\n};\n\nstatic struct vduse_mgmt_dev *vduse_mgmt;\n\nstatic int vduse_dev_init_vdpa(struct vduse_dev *dev, const char *name)\n{\n\tstruct vduse_vdpa *vdev;\n\tint ret;\n\n\tif (dev->vdev)\n\t\treturn -EEXIST;\n\n\tvdev = vdpa_alloc_device(struct vduse_vdpa, vdpa, dev->dev,\n\t\t\t\t &vduse_vdpa_config_ops, 1, 1, name, true);\n\tif (IS_ERR(vdev))\n\t\treturn PTR_ERR(vdev);\n\n\tdev->vdev = vdev;\n\tvdev->dev = dev;\n\tvdev->vdpa.dev.dma_mask = &vdev->vdpa.dev.coherent_dma_mask;\n\tret = dma_set_mask_and_coherent(&vdev->vdpa.dev, DMA_BIT_MASK(64));\n\tif (ret) {\n\t\tput_device(&vdev->vdpa.dev);\n\t\treturn ret;\n\t}\n\tset_dma_ops(&vdev->vdpa.dev, &vduse_dev_dma_ops);\n\tvdev->vdpa.dma_dev = &vdev->vdpa.dev;\n\tvdev->vdpa.mdev = &vduse_mgmt->mgmt_dev;\n\n\treturn 0;\n}\n\nstatic int vdpa_dev_add(struct vdpa_mgmt_dev *mdev, const char *name,\n\t\t\tconst struct vdpa_dev_set_config *config)\n{\n\tstruct vduse_dev *dev;\n\tint ret;\n\n\tmutex_lock(&vduse_lock);\n\tdev = vduse_find_dev(name);\n\tif (!dev || !vduse_dev_is_ready(dev)) {\n\t\tmutex_unlock(&vduse_lock);\n\t\treturn -EINVAL;\n\t}\n\tret = vduse_dev_init_vdpa(dev, name);\n\tmutex_unlock(&vduse_lock);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&dev->domain_lock);\n\tif (!dev->domain)\n\t\tdev->domain = vduse_domain_create(VDUSE_IOVA_SIZE - 1,\n\t\t\t\t\t\t  dev->bounce_size);\n\tmutex_unlock(&dev->domain_lock);\n\tif (!dev->domain) {\n\t\tput_device(&dev->vdev->vdpa.dev);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = _vdpa_register_device(&dev->vdev->vdpa, dev->vq_num);\n\tif (ret) {\n\t\tput_device(&dev->vdev->vdpa.dev);\n\t\tmutex_lock(&dev->domain_lock);\n\t\tvduse_domain_destroy(dev->domain);\n\t\tdev->domain = NULL;\n\t\tmutex_unlock(&dev->domain_lock);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void vdpa_dev_del(struct vdpa_mgmt_dev *mdev, struct vdpa_device *dev)\n{\n\t_vdpa_unregister_device(dev);\n}\n\nstatic const struct vdpa_mgmtdev_ops vdpa_dev_mgmtdev_ops = {\n\t.dev_add = vdpa_dev_add,\n\t.dev_del = vdpa_dev_del,\n};\n\nstatic struct virtio_device_id id_table[] = {\n\t{ VIRTIO_ID_BLOCK, VIRTIO_DEV_ANY_ID },\n\t{ 0 },\n};\n\nstatic void vduse_mgmtdev_release(struct device *dev)\n{\n\tstruct vduse_mgmt_dev *mgmt_dev;\n\n\tmgmt_dev = container_of(dev, struct vduse_mgmt_dev, dev);\n\tkfree(mgmt_dev);\n}\n\nstatic int vduse_mgmtdev_init(void)\n{\n\tint ret;\n\n\tvduse_mgmt = kzalloc(sizeof(*vduse_mgmt), GFP_KERNEL);\n\tif (!vduse_mgmt)\n\t\treturn -ENOMEM;\n\n\tret = dev_set_name(&vduse_mgmt->dev, \"vduse\");\n\tif (ret) {\n\t\tkfree(vduse_mgmt);\n\t\treturn ret;\n\t}\n\n\tvduse_mgmt->dev.release = vduse_mgmtdev_release;\n\n\tret = device_register(&vduse_mgmt->dev);\n\tif (ret)\n\t\tgoto dev_reg_err;\n\n\tvduse_mgmt->mgmt_dev.id_table = id_table;\n\tvduse_mgmt->mgmt_dev.ops = &vdpa_dev_mgmtdev_ops;\n\tvduse_mgmt->mgmt_dev.device = &vduse_mgmt->dev;\n\tret = vdpa_mgmtdev_register(&vduse_mgmt->mgmt_dev);\n\tif (ret)\n\t\tdevice_unregister(&vduse_mgmt->dev);\n\n\treturn ret;\n\ndev_reg_err:\n\tput_device(&vduse_mgmt->dev);\n\treturn ret;\n}\n\nstatic void vduse_mgmtdev_exit(void)\n{\n\tvdpa_mgmtdev_unregister(&vduse_mgmt->mgmt_dev);\n\tdevice_unregister(&vduse_mgmt->dev);\n}\n\nstatic int vduse_init(void)\n{\n\tint ret;\n\tstruct device *dev;\n\n\tvduse_class = class_create(\"vduse\");\n\tif (IS_ERR(vduse_class))\n\t\treturn PTR_ERR(vduse_class);\n\n\tvduse_class->devnode = vduse_devnode;\n\n\tret = alloc_chrdev_region(&vduse_major, 0, VDUSE_DEV_MAX, \"vduse\");\n\tif (ret)\n\t\tgoto err_chardev_region;\n\n\t \n\tcdev_init(&vduse_ctrl_cdev, &vduse_ctrl_fops);\n\tvduse_ctrl_cdev.owner = THIS_MODULE;\n\tret = cdev_add(&vduse_ctrl_cdev, vduse_major, 1);\n\tif (ret)\n\t\tgoto err_ctrl_cdev;\n\n\tdev = device_create(vduse_class, NULL, vduse_major, NULL, \"control\");\n\tif (IS_ERR(dev)) {\n\t\tret = PTR_ERR(dev);\n\t\tgoto err_device;\n\t}\n\n\t \n\tcdev_init(&vduse_cdev, &vduse_dev_fops);\n\tvduse_cdev.owner = THIS_MODULE;\n\tret = cdev_add(&vduse_cdev, MKDEV(MAJOR(vduse_major), 1),\n\t\t       VDUSE_DEV_MAX - 1);\n\tif (ret)\n\t\tgoto err_cdev;\n\n\tret = -ENOMEM;\n\tvduse_irq_wq = alloc_workqueue(\"vduse-irq\",\n\t\t\t\tWQ_HIGHPRI | WQ_SYSFS | WQ_UNBOUND, 0);\n\tif (!vduse_irq_wq)\n\t\tgoto err_wq;\n\n\tvduse_irq_bound_wq = alloc_workqueue(\"vduse-irq-bound\", WQ_HIGHPRI, 0);\n\tif (!vduse_irq_bound_wq)\n\t\tgoto err_bound_wq;\n\n\tret = vduse_domain_init();\n\tif (ret)\n\t\tgoto err_domain;\n\n\tret = vduse_mgmtdev_init();\n\tif (ret)\n\t\tgoto err_mgmtdev;\n\n\treturn 0;\nerr_mgmtdev:\n\tvduse_domain_exit();\nerr_domain:\n\tdestroy_workqueue(vduse_irq_bound_wq);\nerr_bound_wq:\n\tdestroy_workqueue(vduse_irq_wq);\nerr_wq:\n\tcdev_del(&vduse_cdev);\nerr_cdev:\n\tdevice_destroy(vduse_class, vduse_major);\nerr_device:\n\tcdev_del(&vduse_ctrl_cdev);\nerr_ctrl_cdev:\n\tunregister_chrdev_region(vduse_major, VDUSE_DEV_MAX);\nerr_chardev_region:\n\tclass_destroy(vduse_class);\n\treturn ret;\n}\nmodule_init(vduse_init);\n\nstatic void vduse_exit(void)\n{\n\tvduse_mgmtdev_exit();\n\tvduse_domain_exit();\n\tdestroy_workqueue(vduse_irq_bound_wq);\n\tdestroy_workqueue(vduse_irq_wq);\n\tcdev_del(&vduse_cdev);\n\tdevice_destroy(vduse_class, vduse_major);\n\tcdev_del(&vduse_ctrl_cdev);\n\tunregister_chrdev_region(vduse_major, VDUSE_DEV_MAX);\n\tclass_destroy(vduse_class);\n}\nmodule_exit(vduse_exit);\n\nMODULE_LICENSE(DRV_LICENSE);\nMODULE_AUTHOR(DRV_AUTHOR);\nMODULE_DESCRIPTION(DRV_DESC);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}