{
  "module_name": "iova_domain.c",
  "hash_id": "eb59b5ca90efd8bf1c93a9e36a7ea32883ec2a5be360f4acbf1c5b0a172cd3b6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vdpa/vdpa_user/iova_domain.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/file.h>\n#include <linux/anon_inodes.h>\n#include <linux/highmem.h>\n#include <linux/vmalloc.h>\n#include <linux/vdpa.h>\n\n#include \"iova_domain.h\"\n\nstatic int vduse_iotlb_add_range(struct vduse_iova_domain *domain,\n\t\t\t\t u64 start, u64 last,\n\t\t\t\t u64 addr, unsigned int perm,\n\t\t\t\t struct file *file, u64 offset)\n{\n\tstruct vdpa_map_file *map_file;\n\tint ret;\n\n\tmap_file = kmalloc(sizeof(*map_file), GFP_ATOMIC);\n\tif (!map_file)\n\t\treturn -ENOMEM;\n\n\tmap_file->file = get_file(file);\n\tmap_file->offset = offset;\n\n\tret = vhost_iotlb_add_range_ctx(domain->iotlb, start, last,\n\t\t\t\t\taddr, perm, map_file);\n\tif (ret) {\n\t\tfput(map_file->file);\n\t\tkfree(map_file);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic void vduse_iotlb_del_range(struct vduse_iova_domain *domain,\n\t\t\t\t  u64 start, u64 last)\n{\n\tstruct vdpa_map_file *map_file;\n\tstruct vhost_iotlb_map *map;\n\n\twhile ((map = vhost_iotlb_itree_first(domain->iotlb, start, last))) {\n\t\tmap_file = (struct vdpa_map_file *)map->opaque;\n\t\tfput(map_file->file);\n\t\tkfree(map_file);\n\t\tvhost_iotlb_map_free(domain->iotlb, map);\n\t}\n}\n\nint vduse_domain_set_map(struct vduse_iova_domain *domain,\n\t\t\t struct vhost_iotlb *iotlb)\n{\n\tstruct vdpa_map_file *map_file;\n\tstruct vhost_iotlb_map *map;\n\tu64 start = 0ULL, last = ULLONG_MAX;\n\tint ret;\n\n\tspin_lock(&domain->iotlb_lock);\n\tvduse_iotlb_del_range(domain, start, last);\n\n\tfor (map = vhost_iotlb_itree_first(iotlb, start, last); map;\n\t     map = vhost_iotlb_itree_next(map, start, last)) {\n\t\tmap_file = (struct vdpa_map_file *)map->opaque;\n\t\tret = vduse_iotlb_add_range(domain, map->start, map->last,\n\t\t\t\t\t    map->addr, map->perm,\n\t\t\t\t\t    map_file->file,\n\t\t\t\t\t    map_file->offset);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\tspin_unlock(&domain->iotlb_lock);\n\n\treturn 0;\nerr:\n\tvduse_iotlb_del_range(domain, start, last);\n\tspin_unlock(&domain->iotlb_lock);\n\treturn ret;\n}\n\nvoid vduse_domain_clear_map(struct vduse_iova_domain *domain,\n\t\t\t    struct vhost_iotlb *iotlb)\n{\n\tstruct vhost_iotlb_map *map;\n\tu64 start = 0ULL, last = ULLONG_MAX;\n\n\tspin_lock(&domain->iotlb_lock);\n\tfor (map = vhost_iotlb_itree_first(iotlb, start, last); map;\n\t     map = vhost_iotlb_itree_next(map, start, last)) {\n\t\tvduse_iotlb_del_range(domain, map->start, map->last);\n\t}\n\tspin_unlock(&domain->iotlb_lock);\n}\n\nstatic int vduse_domain_map_bounce_page(struct vduse_iova_domain *domain,\n\t\t\t\t\t u64 iova, u64 size, u64 paddr)\n{\n\tstruct vduse_bounce_map *map;\n\tu64 last = iova + size - 1;\n\n\twhile (iova <= last) {\n\t\tmap = &domain->bounce_maps[iova >> PAGE_SHIFT];\n\t\tif (!map->bounce_page) {\n\t\t\tmap->bounce_page = alloc_page(GFP_ATOMIC);\n\t\t\tif (!map->bounce_page)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tmap->orig_phys = paddr;\n\t\tpaddr += PAGE_SIZE;\n\t\tiova += PAGE_SIZE;\n\t}\n\treturn 0;\n}\n\nstatic void vduse_domain_unmap_bounce_page(struct vduse_iova_domain *domain,\n\t\t\t\t\t   u64 iova, u64 size)\n{\n\tstruct vduse_bounce_map *map;\n\tu64 last = iova + size - 1;\n\n\twhile (iova <= last) {\n\t\tmap = &domain->bounce_maps[iova >> PAGE_SHIFT];\n\t\tmap->orig_phys = INVALID_PHYS_ADDR;\n\t\tiova += PAGE_SIZE;\n\t}\n}\n\nstatic void do_bounce(phys_addr_t orig, void *addr, size_t size,\n\t\t      enum dma_data_direction dir)\n{\n\tunsigned long pfn = PFN_DOWN(orig);\n\tunsigned int offset = offset_in_page(orig);\n\tstruct page *page;\n\tunsigned int sz = 0;\n\n\twhile (size) {\n\t\tsz = min_t(size_t, PAGE_SIZE - offset, size);\n\n\t\tpage = pfn_to_page(pfn);\n\t\tif (dir == DMA_TO_DEVICE)\n\t\t\tmemcpy_from_page(addr, page, offset, sz);\n\t\telse\n\t\t\tmemcpy_to_page(page, offset, addr, sz);\n\n\t\tsize -= sz;\n\t\tpfn++;\n\t\taddr += sz;\n\t\toffset = 0;\n\t}\n}\n\nstatic void vduse_domain_bounce(struct vduse_iova_domain *domain,\n\t\t\t\tdma_addr_t iova, size_t size,\n\t\t\t\tenum dma_data_direction dir)\n{\n\tstruct vduse_bounce_map *map;\n\tunsigned int offset;\n\tvoid *addr;\n\tsize_t sz;\n\n\tif (iova >= domain->bounce_size)\n\t\treturn;\n\n\twhile (size) {\n\t\tmap = &domain->bounce_maps[iova >> PAGE_SHIFT];\n\t\toffset = offset_in_page(iova);\n\t\tsz = min_t(size_t, PAGE_SIZE - offset, size);\n\n\t\tif (WARN_ON(!map->bounce_page ||\n\t\t\t    map->orig_phys == INVALID_PHYS_ADDR))\n\t\t\treturn;\n\n\t\taddr = kmap_local_page(map->bounce_page);\n\t\tdo_bounce(map->orig_phys + offset, addr + offset, sz, dir);\n\t\tkunmap_local(addr);\n\t\tsize -= sz;\n\t\tiova += sz;\n\t}\n}\n\nstatic struct page *\nvduse_domain_get_coherent_page(struct vduse_iova_domain *domain, u64 iova)\n{\n\tu64 start = iova & PAGE_MASK;\n\tu64 last = start + PAGE_SIZE - 1;\n\tstruct vhost_iotlb_map *map;\n\tstruct page *page = NULL;\n\n\tspin_lock(&domain->iotlb_lock);\n\tmap = vhost_iotlb_itree_first(domain->iotlb, start, last);\n\tif (!map)\n\t\tgoto out;\n\n\tpage = pfn_to_page((map->addr + iova - map->start) >> PAGE_SHIFT);\n\tget_page(page);\nout:\n\tspin_unlock(&domain->iotlb_lock);\n\n\treturn page;\n}\n\nstatic struct page *\nvduse_domain_get_bounce_page(struct vduse_iova_domain *domain, u64 iova)\n{\n\tstruct vduse_bounce_map *map;\n\tstruct page *page = NULL;\n\n\tread_lock(&domain->bounce_lock);\n\tmap = &domain->bounce_maps[iova >> PAGE_SHIFT];\n\tif (domain->user_bounce_pages || !map->bounce_page)\n\t\tgoto out;\n\n\tpage = map->bounce_page;\n\tget_page(page);\nout:\n\tread_unlock(&domain->bounce_lock);\n\n\treturn page;\n}\n\nstatic void\nvduse_domain_free_kernel_bounce_pages(struct vduse_iova_domain *domain)\n{\n\tstruct vduse_bounce_map *map;\n\tunsigned long pfn, bounce_pfns;\n\n\tbounce_pfns = domain->bounce_size >> PAGE_SHIFT;\n\n\tfor (pfn = 0; pfn < bounce_pfns; pfn++) {\n\t\tmap = &domain->bounce_maps[pfn];\n\t\tif (WARN_ON(map->orig_phys != INVALID_PHYS_ADDR))\n\t\t\tcontinue;\n\n\t\tif (!map->bounce_page)\n\t\t\tcontinue;\n\n\t\t__free_page(map->bounce_page);\n\t\tmap->bounce_page = NULL;\n\t}\n}\n\nint vduse_domain_add_user_bounce_pages(struct vduse_iova_domain *domain,\n\t\t\t\t       struct page **pages, int count)\n{\n\tstruct vduse_bounce_map *map;\n\tint i, ret;\n\n\t \n\tif (count != (domain->bounce_size >> PAGE_SHIFT))\n\t\treturn -EINVAL;\n\n\twrite_lock(&domain->bounce_lock);\n\tret = -EEXIST;\n\tif (domain->user_bounce_pages)\n\t\tgoto out;\n\n\tfor (i = 0; i < count; i++) {\n\t\tmap = &domain->bounce_maps[i];\n\t\tif (map->bounce_page) {\n\t\t\t \n\t\t\tif (map->orig_phys != INVALID_PHYS_ADDR)\n\t\t\t\tmemcpy_to_page(pages[i], 0,\n\t\t\t\t\t       page_address(map->bounce_page),\n\t\t\t\t\t       PAGE_SIZE);\n\t\t\t__free_page(map->bounce_page);\n\t\t}\n\t\tmap->bounce_page = pages[i];\n\t\tget_page(pages[i]);\n\t}\n\tdomain->user_bounce_pages = true;\n\tret = 0;\nout:\n\twrite_unlock(&domain->bounce_lock);\n\n\treturn ret;\n}\n\nvoid vduse_domain_remove_user_bounce_pages(struct vduse_iova_domain *domain)\n{\n\tstruct vduse_bounce_map *map;\n\tunsigned long i, count;\n\n\twrite_lock(&domain->bounce_lock);\n\tif (!domain->user_bounce_pages)\n\t\tgoto out;\n\n\tcount = domain->bounce_size >> PAGE_SHIFT;\n\tfor (i = 0; i < count; i++) {\n\t\tstruct page *page = NULL;\n\n\t\tmap = &domain->bounce_maps[i];\n\t\tif (WARN_ON(!map->bounce_page))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (map->orig_phys != INVALID_PHYS_ADDR) {\n\t\t\tpage = alloc_page(GFP_ATOMIC | __GFP_NOFAIL);\n\t\t\tmemcpy_from_page(page_address(page),\n\t\t\t\t\t map->bounce_page, 0, PAGE_SIZE);\n\t\t}\n\t\tput_page(map->bounce_page);\n\t\tmap->bounce_page = page;\n\t}\n\tdomain->user_bounce_pages = false;\nout:\n\twrite_unlock(&domain->bounce_lock);\n}\n\nvoid vduse_domain_reset_bounce_map(struct vduse_iova_domain *domain)\n{\n\tif (!domain->bounce_map)\n\t\treturn;\n\n\tspin_lock(&domain->iotlb_lock);\n\tif (!domain->bounce_map)\n\t\tgoto unlock;\n\n\tvduse_iotlb_del_range(domain, 0, domain->bounce_size - 1);\n\tdomain->bounce_map = 0;\nunlock:\n\tspin_unlock(&domain->iotlb_lock);\n}\n\nstatic int vduse_domain_init_bounce_map(struct vduse_iova_domain *domain)\n{\n\tint ret = 0;\n\n\tif (domain->bounce_map)\n\t\treturn 0;\n\n\tspin_lock(&domain->iotlb_lock);\n\tif (domain->bounce_map)\n\t\tgoto unlock;\n\n\tret = vduse_iotlb_add_range(domain, 0, domain->bounce_size - 1,\n\t\t\t\t    0, VHOST_MAP_RW, domain->file, 0);\n\tif (ret)\n\t\tgoto unlock;\n\n\tdomain->bounce_map = 1;\nunlock:\n\tspin_unlock(&domain->iotlb_lock);\n\treturn ret;\n}\n\nstatic dma_addr_t\nvduse_domain_alloc_iova(struct iova_domain *iovad,\n\t\t\tunsigned long size, unsigned long limit)\n{\n\tunsigned long shift = iova_shift(iovad);\n\tunsigned long iova_len = iova_align(iovad, size) >> shift;\n\tunsigned long iova_pfn;\n\n\tiova_pfn = alloc_iova_fast(iovad, iova_len, limit >> shift, true);\n\n\treturn (dma_addr_t)iova_pfn << shift;\n}\n\nstatic void vduse_domain_free_iova(struct iova_domain *iovad,\n\t\t\t\t   dma_addr_t iova, size_t size)\n{\n\tunsigned long shift = iova_shift(iovad);\n\tunsigned long iova_len = iova_align(iovad, size) >> shift;\n\n\tfree_iova_fast(iovad, iova >> shift, iova_len);\n}\n\ndma_addr_t vduse_domain_map_page(struct vduse_iova_domain *domain,\n\t\t\t\t struct page *page, unsigned long offset,\n\t\t\t\t size_t size, enum dma_data_direction dir,\n\t\t\t\t unsigned long attrs)\n{\n\tstruct iova_domain *iovad = &domain->stream_iovad;\n\tunsigned long limit = domain->bounce_size - 1;\n\tphys_addr_t pa = page_to_phys(page) + offset;\n\tdma_addr_t iova = vduse_domain_alloc_iova(iovad, size, limit);\n\n\tif (!iova)\n\t\treturn DMA_MAPPING_ERROR;\n\n\tif (vduse_domain_init_bounce_map(domain))\n\t\tgoto err;\n\n\tread_lock(&domain->bounce_lock);\n\tif (vduse_domain_map_bounce_page(domain, (u64)iova, (u64)size, pa))\n\t\tgoto err_unlock;\n\n\tif (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)\n\t\tvduse_domain_bounce(domain, iova, size, DMA_TO_DEVICE);\n\n\tread_unlock(&domain->bounce_lock);\n\n\treturn iova;\nerr_unlock:\n\tread_unlock(&domain->bounce_lock);\nerr:\n\tvduse_domain_free_iova(iovad, iova, size);\n\treturn DMA_MAPPING_ERROR;\n}\n\nvoid vduse_domain_unmap_page(struct vduse_iova_domain *domain,\n\t\t\t     dma_addr_t dma_addr, size_t size,\n\t\t\t     enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct iova_domain *iovad = &domain->stream_iovad;\n\n\tread_lock(&domain->bounce_lock);\n\tif (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)\n\t\tvduse_domain_bounce(domain, dma_addr, size, DMA_FROM_DEVICE);\n\n\tvduse_domain_unmap_bounce_page(domain, (u64)dma_addr, (u64)size);\n\tread_unlock(&domain->bounce_lock);\n\tvduse_domain_free_iova(iovad, dma_addr, size);\n}\n\nvoid *vduse_domain_alloc_coherent(struct vduse_iova_domain *domain,\n\t\t\t\t  size_t size, dma_addr_t *dma_addr,\n\t\t\t\t  gfp_t flag, unsigned long attrs)\n{\n\tstruct iova_domain *iovad = &domain->consistent_iovad;\n\tunsigned long limit = domain->iova_limit;\n\tdma_addr_t iova = vduse_domain_alloc_iova(iovad, size, limit);\n\tvoid *orig = alloc_pages_exact(size, flag);\n\n\tif (!iova || !orig)\n\t\tgoto err;\n\n\tspin_lock(&domain->iotlb_lock);\n\tif (vduse_iotlb_add_range(domain, (u64)iova, (u64)iova + size - 1,\n\t\t\t\t  virt_to_phys(orig), VHOST_MAP_RW,\n\t\t\t\t  domain->file, (u64)iova)) {\n\t\tspin_unlock(&domain->iotlb_lock);\n\t\tgoto err;\n\t}\n\tspin_unlock(&domain->iotlb_lock);\n\n\t*dma_addr = iova;\n\n\treturn orig;\nerr:\n\t*dma_addr = DMA_MAPPING_ERROR;\n\tif (orig)\n\t\tfree_pages_exact(orig, size);\n\tif (iova)\n\t\tvduse_domain_free_iova(iovad, iova, size);\n\n\treturn NULL;\n}\n\nvoid vduse_domain_free_coherent(struct vduse_iova_domain *domain, size_t size,\n\t\t\t\tvoid *vaddr, dma_addr_t dma_addr,\n\t\t\t\tunsigned long attrs)\n{\n\tstruct iova_domain *iovad = &domain->consistent_iovad;\n\tstruct vhost_iotlb_map *map;\n\tstruct vdpa_map_file *map_file;\n\tphys_addr_t pa;\n\n\tspin_lock(&domain->iotlb_lock);\n\tmap = vhost_iotlb_itree_first(domain->iotlb, (u64)dma_addr,\n\t\t\t\t      (u64)dma_addr + size - 1);\n\tif (WARN_ON(!map)) {\n\t\tspin_unlock(&domain->iotlb_lock);\n\t\treturn;\n\t}\n\tmap_file = (struct vdpa_map_file *)map->opaque;\n\tfput(map_file->file);\n\tkfree(map_file);\n\tpa = map->addr;\n\tvhost_iotlb_map_free(domain->iotlb, map);\n\tspin_unlock(&domain->iotlb_lock);\n\n\tvduse_domain_free_iova(iovad, dma_addr, size);\n\tfree_pages_exact(phys_to_virt(pa), size);\n}\n\nstatic vm_fault_t vduse_domain_mmap_fault(struct vm_fault *vmf)\n{\n\tstruct vduse_iova_domain *domain = vmf->vma->vm_private_data;\n\tunsigned long iova = vmf->pgoff << PAGE_SHIFT;\n\tstruct page *page;\n\n\tif (!domain)\n\t\treturn VM_FAULT_SIGBUS;\n\n\tif (iova < domain->bounce_size)\n\t\tpage = vduse_domain_get_bounce_page(domain, iova);\n\telse\n\t\tpage = vduse_domain_get_coherent_page(domain, iova);\n\n\tif (!page)\n\t\treturn VM_FAULT_SIGBUS;\n\n\tvmf->page = page;\n\n\treturn 0;\n}\n\nstatic const struct vm_operations_struct vduse_domain_mmap_ops = {\n\t.fault = vduse_domain_mmap_fault,\n};\n\nstatic int vduse_domain_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct vduse_iova_domain *domain = file->private_data;\n\n\tvm_flags_set(vma, VM_DONTDUMP | VM_DONTEXPAND);\n\tvma->vm_private_data = domain;\n\tvma->vm_ops = &vduse_domain_mmap_ops;\n\n\treturn 0;\n}\n\nstatic int vduse_domain_release(struct inode *inode, struct file *file)\n{\n\tstruct vduse_iova_domain *domain = file->private_data;\n\n\tspin_lock(&domain->iotlb_lock);\n\tvduse_iotlb_del_range(domain, 0, ULLONG_MAX);\n\tvduse_domain_remove_user_bounce_pages(domain);\n\tvduse_domain_free_kernel_bounce_pages(domain);\n\tspin_unlock(&domain->iotlb_lock);\n\tput_iova_domain(&domain->stream_iovad);\n\tput_iova_domain(&domain->consistent_iovad);\n\tvhost_iotlb_free(domain->iotlb);\n\tvfree(domain->bounce_maps);\n\tkfree(domain);\n\n\treturn 0;\n}\n\nstatic const struct file_operations vduse_domain_fops = {\n\t.owner = THIS_MODULE,\n\t.mmap = vduse_domain_mmap,\n\t.release = vduse_domain_release,\n};\n\nvoid vduse_domain_destroy(struct vduse_iova_domain *domain)\n{\n\tfput(domain->file);\n}\n\nstruct vduse_iova_domain *\nvduse_domain_create(unsigned long iova_limit, size_t bounce_size)\n{\n\tstruct vduse_iova_domain *domain;\n\tstruct file *file;\n\tstruct vduse_bounce_map *map;\n\tunsigned long pfn, bounce_pfns;\n\tint ret;\n\n\tbounce_pfns = PAGE_ALIGN(bounce_size) >> PAGE_SHIFT;\n\tif (iova_limit <= bounce_size)\n\t\treturn NULL;\n\n\tdomain = kzalloc(sizeof(*domain), GFP_KERNEL);\n\tif (!domain)\n\t\treturn NULL;\n\n\tdomain->iotlb = vhost_iotlb_alloc(0, 0);\n\tif (!domain->iotlb)\n\t\tgoto err_iotlb;\n\n\tdomain->iova_limit = iova_limit;\n\tdomain->bounce_size = PAGE_ALIGN(bounce_size);\n\tdomain->bounce_maps = vzalloc(bounce_pfns *\n\t\t\t\tsizeof(struct vduse_bounce_map));\n\tif (!domain->bounce_maps)\n\t\tgoto err_map;\n\n\tfor (pfn = 0; pfn < bounce_pfns; pfn++) {\n\t\tmap = &domain->bounce_maps[pfn];\n\t\tmap->orig_phys = INVALID_PHYS_ADDR;\n\t}\n\tfile = anon_inode_getfile(\"[vduse-domain]\", &vduse_domain_fops,\n\t\t\t\tdomain, O_RDWR);\n\tif (IS_ERR(file))\n\t\tgoto err_file;\n\n\tdomain->file = file;\n\trwlock_init(&domain->bounce_lock);\n\tspin_lock_init(&domain->iotlb_lock);\n\tinit_iova_domain(&domain->stream_iovad,\n\t\t\tPAGE_SIZE, IOVA_START_PFN);\n\tret = iova_domain_init_rcaches(&domain->stream_iovad);\n\tif (ret)\n\t\tgoto err_iovad_stream;\n\tinit_iova_domain(&domain->consistent_iovad,\n\t\t\tPAGE_SIZE, bounce_pfns);\n\tret = iova_domain_init_rcaches(&domain->consistent_iovad);\n\tif (ret)\n\t\tgoto err_iovad_consistent;\n\n\treturn domain;\nerr_iovad_consistent:\n\tput_iova_domain(&domain->stream_iovad);\nerr_iovad_stream:\n\tfput(file);\nerr_file:\n\tvfree(domain->bounce_maps);\nerr_map:\n\tvhost_iotlb_free(domain->iotlb);\nerr_iotlb:\n\tkfree(domain);\n\treturn NULL;\n}\n\nint vduse_domain_init(void)\n{\n\treturn iova_cache_get();\n}\n\nvoid vduse_domain_exit(void)\n{\n\tiova_cache_put();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}