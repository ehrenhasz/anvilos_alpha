{
  "module_name": "mlx5_vnet.c",
  "hash_id": "175e66b659fb93662fe67feccf8debd824732f8ae42bad52ffa7bc211de15d9c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vdpa/mlx5/net/mlx5_vnet.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/vdpa.h>\n#include <linux/vringh.h>\n#include <uapi/linux/virtio_net.h>\n#include <uapi/linux/virtio_ids.h>\n#include <uapi/linux/vdpa.h>\n#include <linux/virtio_config.h>\n#include <linux/auxiliary_bus.h>\n#include <linux/mlx5/cq.h>\n#include <linux/mlx5/qp.h>\n#include <linux/mlx5/device.h>\n#include <linux/mlx5/driver.h>\n#include <linux/mlx5/vport.h>\n#include <linux/mlx5/fs.h>\n#include <linux/mlx5/mlx5_ifc_vdpa.h>\n#include <linux/mlx5/mpfs.h>\n#include \"mlx5_vdpa.h\"\n#include \"mlx5_vnet.h\"\n\nMODULE_AUTHOR(\"Eli Cohen <eli@mellanox.com>\");\nMODULE_DESCRIPTION(\"Mellanox VDPA driver\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\n#define VALID_FEATURES_MASK                                                                        \\\n\t(BIT_ULL(VIRTIO_NET_F_CSUM) | BIT_ULL(VIRTIO_NET_F_GUEST_CSUM) |                                   \\\n\t BIT_ULL(VIRTIO_NET_F_CTRL_GUEST_OFFLOADS) | BIT_ULL(VIRTIO_NET_F_MTU) | BIT_ULL(VIRTIO_NET_F_MAC) |   \\\n\t BIT_ULL(VIRTIO_NET_F_GUEST_TSO4) | BIT_ULL(VIRTIO_NET_F_GUEST_TSO6) |                             \\\n\t BIT_ULL(VIRTIO_NET_F_GUEST_ECN) | BIT_ULL(VIRTIO_NET_F_GUEST_UFO) | BIT_ULL(VIRTIO_NET_F_HOST_TSO4) | \\\n\t BIT_ULL(VIRTIO_NET_F_HOST_TSO6) | BIT_ULL(VIRTIO_NET_F_HOST_ECN) | BIT_ULL(VIRTIO_NET_F_HOST_UFO) |   \\\n\t BIT_ULL(VIRTIO_NET_F_MRG_RXBUF) | BIT_ULL(VIRTIO_NET_F_STATUS) | BIT_ULL(VIRTIO_NET_F_CTRL_VQ) |      \\\n\t BIT_ULL(VIRTIO_NET_F_CTRL_RX) | BIT_ULL(VIRTIO_NET_F_CTRL_VLAN) |                                 \\\n\t BIT_ULL(VIRTIO_NET_F_CTRL_RX_EXTRA) | BIT_ULL(VIRTIO_NET_F_GUEST_ANNOUNCE) |                      \\\n\t BIT_ULL(VIRTIO_NET_F_MQ) | BIT_ULL(VIRTIO_NET_F_CTRL_MAC_ADDR) | BIT_ULL(VIRTIO_NET_F_HASH_REPORT) |  \\\n\t BIT_ULL(VIRTIO_NET_F_RSS) | BIT_ULL(VIRTIO_NET_F_RSC_EXT) | BIT_ULL(VIRTIO_NET_F_STANDBY) |           \\\n\t BIT_ULL(VIRTIO_NET_F_SPEED_DUPLEX) | BIT_ULL(VIRTIO_F_NOTIFY_ON_EMPTY) |                          \\\n\t BIT_ULL(VIRTIO_F_ANY_LAYOUT) | BIT_ULL(VIRTIO_F_VERSION_1) | BIT_ULL(VIRTIO_F_ACCESS_PLATFORM) |      \\\n\t BIT_ULL(VIRTIO_F_RING_PACKED) | BIT_ULL(VIRTIO_F_ORDER_PLATFORM) | BIT_ULL(VIRTIO_F_SR_IOV))\n\n#define VALID_STATUS_MASK                                                                          \\\n\t(VIRTIO_CONFIG_S_ACKNOWLEDGE | VIRTIO_CONFIG_S_DRIVER | VIRTIO_CONFIG_S_DRIVER_OK |        \\\n\t VIRTIO_CONFIG_S_FEATURES_OK | VIRTIO_CONFIG_S_NEEDS_RESET | VIRTIO_CONFIG_S_FAILED)\n\n#define MLX5_FEATURE(_mvdev, _feature) (!!((_mvdev)->actual_features & BIT_ULL(_feature)))\n\n#define MLX5V_UNTAGGED 0x1000\n\nstruct mlx5_vdpa_cq_buf {\n\tstruct mlx5_frag_buf_ctrl fbc;\n\tstruct mlx5_frag_buf frag_buf;\n\tint cqe_size;\n\tint nent;\n};\n\nstruct mlx5_vdpa_cq {\n\tstruct mlx5_core_cq mcq;\n\tstruct mlx5_vdpa_cq_buf buf;\n\tstruct mlx5_db db;\n\tint cqe;\n};\n\nstruct mlx5_vdpa_umem {\n\tstruct mlx5_frag_buf_ctrl fbc;\n\tstruct mlx5_frag_buf frag_buf;\n\tint size;\n\tu32 id;\n};\n\nstruct mlx5_vdpa_qp {\n\tstruct mlx5_core_qp mqp;\n\tstruct mlx5_frag_buf frag_buf;\n\tstruct mlx5_db db;\n\tu16 head;\n\tbool fw;\n};\n\nstruct mlx5_vq_restore_info {\n\tu32 num_ent;\n\tu64 desc_addr;\n\tu64 device_addr;\n\tu64 driver_addr;\n\tu16 avail_index;\n\tu16 used_index;\n\tstruct msi_map map;\n\tbool ready;\n\tbool restore;\n};\n\nstruct mlx5_vdpa_virtqueue {\n\tbool ready;\n\tu64 desc_addr;\n\tu64 device_addr;\n\tu64 driver_addr;\n\tu32 num_ent;\n\n\t \n\tstruct mlx5_vdpa_cq cq;\n\tstruct mlx5_vdpa_qp fwqp;\n\tstruct mlx5_vdpa_qp vqqp;\n\n\t \n\tstruct mlx5_vdpa_umem umem1;\n\tstruct mlx5_vdpa_umem umem2;\n\tstruct mlx5_vdpa_umem umem3;\n\n\tu32 counter_set_id;\n\tbool initialized;\n\tint index;\n\tu32 virtq_id;\n\tstruct mlx5_vdpa_net *ndev;\n\tu16 avail_idx;\n\tu16 used_idx;\n\tint fw_state;\n\tstruct msi_map map;\n\n\t \n\tstruct mlx5_vq_restore_info ri;\n};\n\nstatic bool is_index_valid(struct mlx5_vdpa_dev *mvdev, u16 idx)\n{\n\tif (!(mvdev->actual_features & BIT_ULL(VIRTIO_NET_F_MQ))) {\n\t\tif (!(mvdev->actual_features & BIT_ULL(VIRTIO_NET_F_CTRL_VQ)))\n\t\t\treturn idx < 2;\n\t\telse\n\t\t\treturn idx < 3;\n\t}\n\n\treturn idx <= mvdev->max_idx;\n}\n\nstatic void free_resources(struct mlx5_vdpa_net *ndev);\nstatic void init_mvqs(struct mlx5_vdpa_net *ndev);\nstatic int setup_driver(struct mlx5_vdpa_dev *mvdev);\nstatic void teardown_driver(struct mlx5_vdpa_net *ndev);\n\nstatic bool mlx5_vdpa_debug;\n\n#define MLX5_CVQ_MAX_ENT 16\n\n#define MLX5_LOG_VIO_FLAG(_feature)                                                                \\\n\tdo {                                                                                       \\\n\t\tif (features & BIT_ULL(_feature))                                                  \\\n\t\t\tmlx5_vdpa_info(mvdev, \"%s\\n\", #_feature);                                  \\\n\t} while (0)\n\n#define MLX5_LOG_VIO_STAT(_status)                                                                 \\\n\tdo {                                                                                       \\\n\t\tif (status & (_status))                                                            \\\n\t\t\tmlx5_vdpa_info(mvdev, \"%s\\n\", #_status);                                   \\\n\t} while (0)\n\n \nstatic inline bool mlx5_vdpa_is_little_endian(struct mlx5_vdpa_dev *mvdev)\n{\n\treturn virtio_legacy_is_little_endian() ||\n\t\t(mvdev->actual_features & BIT_ULL(VIRTIO_F_VERSION_1));\n}\n\nstatic u16 mlx5vdpa16_to_cpu(struct mlx5_vdpa_dev *mvdev, __virtio16 val)\n{\n\treturn __virtio16_to_cpu(mlx5_vdpa_is_little_endian(mvdev), val);\n}\n\nstatic __virtio16 cpu_to_mlx5vdpa16(struct mlx5_vdpa_dev *mvdev, u16 val)\n{\n\treturn __cpu_to_virtio16(mlx5_vdpa_is_little_endian(mvdev), val);\n}\n\nstatic u16 ctrl_vq_idx(struct mlx5_vdpa_dev *mvdev)\n{\n\tif (!(mvdev->actual_features & BIT_ULL(VIRTIO_NET_F_MQ)))\n\t\treturn 2;\n\n\treturn mvdev->max_vqs;\n}\n\nstatic bool is_ctrl_vq_idx(struct mlx5_vdpa_dev *mvdev, u16 idx)\n{\n\treturn idx == ctrl_vq_idx(mvdev);\n}\n\nstatic void print_status(struct mlx5_vdpa_dev *mvdev, u8 status, bool set)\n{\n\tif (status & ~VALID_STATUS_MASK)\n\t\tmlx5_vdpa_warn(mvdev, \"Warning: there are invalid status bits 0x%x\\n\",\n\t\t\t       status & ~VALID_STATUS_MASK);\n\n\tif (!mlx5_vdpa_debug)\n\t\treturn;\n\n\tmlx5_vdpa_info(mvdev, \"driver status %s\", set ? \"set\" : \"get\");\n\tif (set && !status) {\n\t\tmlx5_vdpa_info(mvdev, \"driver resets the device\\n\");\n\t\treturn;\n\t}\n\n\tMLX5_LOG_VIO_STAT(VIRTIO_CONFIG_S_ACKNOWLEDGE);\n\tMLX5_LOG_VIO_STAT(VIRTIO_CONFIG_S_DRIVER);\n\tMLX5_LOG_VIO_STAT(VIRTIO_CONFIG_S_DRIVER_OK);\n\tMLX5_LOG_VIO_STAT(VIRTIO_CONFIG_S_FEATURES_OK);\n\tMLX5_LOG_VIO_STAT(VIRTIO_CONFIG_S_NEEDS_RESET);\n\tMLX5_LOG_VIO_STAT(VIRTIO_CONFIG_S_FAILED);\n}\n\nstatic void print_features(struct mlx5_vdpa_dev *mvdev, u64 features, bool set)\n{\n\tif (features & ~VALID_FEATURES_MASK)\n\t\tmlx5_vdpa_warn(mvdev, \"There are invalid feature bits 0x%llx\\n\",\n\t\t\t       features & ~VALID_FEATURES_MASK);\n\n\tif (!mlx5_vdpa_debug)\n\t\treturn;\n\n\tmlx5_vdpa_info(mvdev, \"driver %s feature bits:\\n\", set ? \"sets\" : \"reads\");\n\tif (!features)\n\t\tmlx5_vdpa_info(mvdev, \"all feature bits are cleared\\n\");\n\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_CSUM);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_GUEST_CSUM);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_CTRL_GUEST_OFFLOADS);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_MTU);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_MAC);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_GUEST_TSO4);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_GUEST_TSO6);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_GUEST_ECN);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_GUEST_UFO);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_HOST_TSO4);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_HOST_TSO6);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_HOST_ECN);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_HOST_UFO);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_MRG_RXBUF);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_STATUS);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_CTRL_VQ);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_CTRL_RX);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_CTRL_VLAN);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_CTRL_RX_EXTRA);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_GUEST_ANNOUNCE);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_MQ);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_CTRL_MAC_ADDR);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_HASH_REPORT);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_RSS);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_RSC_EXT);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_STANDBY);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_NET_F_SPEED_DUPLEX);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_F_NOTIFY_ON_EMPTY);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_F_ANY_LAYOUT);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_F_VERSION_1);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_F_ACCESS_PLATFORM);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_F_RING_PACKED);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_F_ORDER_PLATFORM);\n\tMLX5_LOG_VIO_FLAG(VIRTIO_F_SR_IOV);\n}\n\nstatic int create_tis(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_vdpa_dev *mvdev = &ndev->mvdev;\n\tu32 in[MLX5_ST_SZ_DW(create_tis_in)] = {};\n\tvoid *tisc;\n\tint err;\n\n\ttisc = MLX5_ADDR_OF(create_tis_in, in, ctx);\n\tMLX5_SET(tisc, tisc, transport_domain, ndev->res.tdn);\n\terr = mlx5_vdpa_create_tis(mvdev, in, &ndev->res.tisn);\n\tif (err)\n\t\tmlx5_vdpa_warn(mvdev, \"create TIS (%d)\\n\", err);\n\n\treturn err;\n}\n\nstatic void destroy_tis(struct mlx5_vdpa_net *ndev)\n{\n\tmlx5_vdpa_destroy_tis(&ndev->mvdev, ndev->res.tisn);\n}\n\n#define MLX5_VDPA_CQE_SIZE 64\n#define MLX5_VDPA_LOG_CQE_SIZE ilog2(MLX5_VDPA_CQE_SIZE)\n\nstatic int cq_frag_buf_alloc(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_cq_buf *buf, int nent)\n{\n\tstruct mlx5_frag_buf *frag_buf = &buf->frag_buf;\n\tu8 log_wq_stride = MLX5_VDPA_LOG_CQE_SIZE;\n\tu8 log_wq_sz = MLX5_VDPA_LOG_CQE_SIZE;\n\tint err;\n\n\terr = mlx5_frag_buf_alloc_node(ndev->mvdev.mdev, nent * MLX5_VDPA_CQE_SIZE, frag_buf,\n\t\t\t\t       ndev->mvdev.mdev->priv.numa_node);\n\tif (err)\n\t\treturn err;\n\n\tmlx5_init_fbc(frag_buf->frags, log_wq_stride, log_wq_sz, &buf->fbc);\n\n\tbuf->cqe_size = MLX5_VDPA_CQE_SIZE;\n\tbuf->nent = nent;\n\n\treturn 0;\n}\n\nstatic int umem_frag_buf_alloc(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_umem *umem, int size)\n{\n\tstruct mlx5_frag_buf *frag_buf = &umem->frag_buf;\n\n\treturn mlx5_frag_buf_alloc_node(ndev->mvdev.mdev, size, frag_buf,\n\t\t\t\t\tndev->mvdev.mdev->priv.numa_node);\n}\n\nstatic void cq_frag_buf_free(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_cq_buf *buf)\n{\n\tmlx5_frag_buf_free(ndev->mvdev.mdev, &buf->frag_buf);\n}\n\nstatic void *get_cqe(struct mlx5_vdpa_cq *vcq, int n)\n{\n\treturn mlx5_frag_buf_get_wqe(&vcq->buf.fbc, n);\n}\n\nstatic void cq_frag_buf_init(struct mlx5_vdpa_cq *vcq, struct mlx5_vdpa_cq_buf *buf)\n{\n\tstruct mlx5_cqe64 *cqe64;\n\tvoid *cqe;\n\tint i;\n\n\tfor (i = 0; i < buf->nent; i++) {\n\t\tcqe = get_cqe(vcq, i);\n\t\tcqe64 = cqe;\n\t\tcqe64->op_own = MLX5_CQE_INVALID << 4;\n\t}\n}\n\nstatic void *get_sw_cqe(struct mlx5_vdpa_cq *cq, int n)\n{\n\tstruct mlx5_cqe64 *cqe64 = get_cqe(cq, n & (cq->cqe - 1));\n\n\tif (likely(get_cqe_opcode(cqe64) != MLX5_CQE_INVALID) &&\n\t    !((cqe64->op_own & MLX5_CQE_OWNER_MASK) ^ !!(n & cq->cqe)))\n\t\treturn cqe64;\n\n\treturn NULL;\n}\n\nstatic void rx_post(struct mlx5_vdpa_qp *vqp, int n)\n{\n\tvqp->head += n;\n\tvqp->db.db[0] = cpu_to_be32(vqp->head);\n}\n\nstatic void qp_prepare(struct mlx5_vdpa_net *ndev, bool fw, void *in,\n\t\t       struct mlx5_vdpa_virtqueue *mvq, u32 num_ent)\n{\n\tstruct mlx5_vdpa_qp *vqp;\n\t__be64 *pas;\n\tvoid *qpc;\n\n\tvqp = fw ? &mvq->fwqp : &mvq->vqqp;\n\tMLX5_SET(create_qp_in, in, uid, ndev->mvdev.res.uid);\n\tqpc = MLX5_ADDR_OF(create_qp_in, in, qpc);\n\tif (vqp->fw) {\n\t\t \n\t\tqpc = MLX5_ADDR_OF(create_qp_in, in, qpc);\n\t\tMLX5_SET(qpc, qpc, rq_type, MLX5_ZERO_LEN_RQ);\n\t\tMLX5_SET(qpc, qpc, no_sq, 1);\n\t\treturn;\n\t}\n\n\tMLX5_SET(qpc, qpc, st, MLX5_QP_ST_RC);\n\tMLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);\n\tMLX5_SET(qpc, qpc, pd, ndev->mvdev.res.pdn);\n\tMLX5_SET(qpc, qpc, mtu, MLX5_QPC_MTU_256_BYTES);\n\tMLX5_SET(qpc, qpc, uar_page, ndev->mvdev.res.uar->index);\n\tMLX5_SET(qpc, qpc, log_page_size, vqp->frag_buf.page_shift - MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET(qpc, qpc, no_sq, 1);\n\tMLX5_SET(qpc, qpc, cqn_rcv, mvq->cq.mcq.cqn);\n\tMLX5_SET(qpc, qpc, log_rq_size, ilog2(num_ent));\n\tMLX5_SET(qpc, qpc, rq_type, MLX5_NON_ZERO_RQ);\n\tpas = (__be64 *)MLX5_ADDR_OF(create_qp_in, in, pas);\n\tmlx5_fill_page_frag_array(&vqp->frag_buf, pas);\n}\n\nstatic int rq_buf_alloc(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_qp *vqp, u32 num_ent)\n{\n\treturn mlx5_frag_buf_alloc_node(ndev->mvdev.mdev,\n\t\t\t\t\tnum_ent * sizeof(struct mlx5_wqe_data_seg), &vqp->frag_buf,\n\t\t\t\t\tndev->mvdev.mdev->priv.numa_node);\n}\n\nstatic void rq_buf_free(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_qp *vqp)\n{\n\tmlx5_frag_buf_free(ndev->mvdev.mdev, &vqp->frag_buf);\n}\n\nstatic int qp_create(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq,\n\t\t     struct mlx5_vdpa_qp *vqp)\n{\n\tstruct mlx5_core_dev *mdev = ndev->mvdev.mdev;\n\tint inlen = MLX5_ST_SZ_BYTES(create_qp_in);\n\tu32 out[MLX5_ST_SZ_DW(create_qp_out)] = {};\n\tvoid *qpc;\n\tvoid *in;\n\tint err;\n\n\tif (!vqp->fw) {\n\t\tvqp = &mvq->vqqp;\n\t\terr = rq_buf_alloc(ndev, vqp, mvq->num_ent);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = mlx5_db_alloc(ndev->mvdev.mdev, &vqp->db);\n\t\tif (err)\n\t\t\tgoto err_db;\n\t\tinlen += vqp->frag_buf.npages * sizeof(__be64);\n\t}\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_kzalloc;\n\t}\n\n\tqp_prepare(ndev, vqp->fw, in, mvq, mvq->num_ent);\n\tqpc = MLX5_ADDR_OF(create_qp_in, in, qpc);\n\tMLX5_SET(qpc, qpc, st, MLX5_QP_ST_RC);\n\tMLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);\n\tMLX5_SET(qpc, qpc, pd, ndev->mvdev.res.pdn);\n\tMLX5_SET(qpc, qpc, mtu, MLX5_QPC_MTU_256_BYTES);\n\tif (!vqp->fw)\n\t\tMLX5_SET64(qpc, qpc, dbr_addr, vqp->db.dma);\n\tMLX5_SET(create_qp_in, in, opcode, MLX5_CMD_OP_CREATE_QP);\n\terr = mlx5_cmd_exec(mdev, in, inlen, out, sizeof(out));\n\tkfree(in);\n\tif (err)\n\t\tgoto err_kzalloc;\n\n\tvqp->mqp.uid = ndev->mvdev.res.uid;\n\tvqp->mqp.qpn = MLX5_GET(create_qp_out, out, qpn);\n\n\tif (!vqp->fw)\n\t\trx_post(vqp, mvq->num_ent);\n\n\treturn 0;\n\nerr_kzalloc:\n\tif (!vqp->fw)\n\t\tmlx5_db_free(ndev->mvdev.mdev, &vqp->db);\nerr_db:\n\tif (!vqp->fw)\n\t\trq_buf_free(ndev, vqp);\n\n\treturn err;\n}\n\nstatic void qp_destroy(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_qp *vqp)\n{\n\tu32 in[MLX5_ST_SZ_DW(destroy_qp_in)] = {};\n\n\tMLX5_SET(destroy_qp_in, in, opcode, MLX5_CMD_OP_DESTROY_QP);\n\tMLX5_SET(destroy_qp_in, in, qpn, vqp->mqp.qpn);\n\tMLX5_SET(destroy_qp_in, in, uid, ndev->mvdev.res.uid);\n\tif (mlx5_cmd_exec_in(ndev->mvdev.mdev, destroy_qp, in))\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"destroy qp 0x%x\\n\", vqp->mqp.qpn);\n\tif (!vqp->fw) {\n\t\tmlx5_db_free(ndev->mvdev.mdev, &vqp->db);\n\t\trq_buf_free(ndev, vqp);\n\t}\n}\n\nstatic void *next_cqe_sw(struct mlx5_vdpa_cq *cq)\n{\n\treturn get_sw_cqe(cq, cq->mcq.cons_index);\n}\n\nstatic int mlx5_vdpa_poll_one(struct mlx5_vdpa_cq *vcq)\n{\n\tstruct mlx5_cqe64 *cqe64;\n\n\tcqe64 = next_cqe_sw(vcq);\n\tif (!cqe64)\n\t\treturn -EAGAIN;\n\n\tvcq->mcq.cons_index++;\n\treturn 0;\n}\n\nstatic void mlx5_vdpa_handle_completions(struct mlx5_vdpa_virtqueue *mvq, int num)\n{\n\tstruct mlx5_vdpa_net *ndev = mvq->ndev;\n\tstruct vdpa_callback *event_cb;\n\n\tevent_cb = &ndev->event_cbs[mvq->index];\n\tmlx5_cq_set_ci(&mvq->cq.mcq);\n\n\t \n\tdma_wmb();\n\trx_post(&mvq->vqqp, num);\n\tif (event_cb->callback)\n\t\tevent_cb->callback(event_cb->private);\n}\n\nstatic void mlx5_vdpa_cq_comp(struct mlx5_core_cq *mcq, struct mlx5_eqe *eqe)\n{\n\tstruct mlx5_vdpa_virtqueue *mvq = container_of(mcq, struct mlx5_vdpa_virtqueue, cq.mcq);\n\tstruct mlx5_vdpa_net *ndev = mvq->ndev;\n\tvoid __iomem *uar_page = ndev->mvdev.res.uar->map;\n\tint num = 0;\n\n\twhile (!mlx5_vdpa_poll_one(&mvq->cq)) {\n\t\tnum++;\n\t\tif (num > mvq->num_ent / 2) {\n\t\t\t \n\t\t\tmlx5_vdpa_handle_completions(mvq, num);\n\t\t\tnum = 0;\n\t\t}\n\t}\n\n\tif (num)\n\t\tmlx5_vdpa_handle_completions(mvq, num);\n\n\tmlx5_cq_arm(&mvq->cq.mcq, MLX5_CQ_DB_REQ_NOT, uar_page, mvq->cq.mcq.cons_index);\n}\n\nstatic int cq_create(struct mlx5_vdpa_net *ndev, u16 idx, u32 num_ent)\n{\n\tstruct mlx5_vdpa_virtqueue *mvq = &ndev->vqs[idx];\n\tstruct mlx5_core_dev *mdev = ndev->mvdev.mdev;\n\tvoid __iomem *uar_page = ndev->mvdev.res.uar->map;\n\tu32 out[MLX5_ST_SZ_DW(create_cq_out)];\n\tstruct mlx5_vdpa_cq *vcq = &mvq->cq;\n\t__be64 *pas;\n\tint inlen;\n\tvoid *cqc;\n\tvoid *in;\n\tint err;\n\tint eqn;\n\n\terr = mlx5_db_alloc(mdev, &vcq->db);\n\tif (err)\n\t\treturn err;\n\n\tvcq->mcq.set_ci_db = vcq->db.db;\n\tvcq->mcq.arm_db = vcq->db.db + 1;\n\tvcq->mcq.cqe_sz = 64;\n\n\terr = cq_frag_buf_alloc(ndev, &vcq->buf, num_ent);\n\tif (err)\n\t\tgoto err_db;\n\n\tcq_frag_buf_init(vcq, &vcq->buf);\n\n\tinlen = MLX5_ST_SZ_BYTES(create_cq_in) +\n\t\tMLX5_FLD_SZ_BYTES(create_cq_in, pas[0]) * vcq->buf.frag_buf.npages;\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_vzalloc;\n\t}\n\n\tMLX5_SET(create_cq_in, in, uid, ndev->mvdev.res.uid);\n\tpas = (__be64 *)MLX5_ADDR_OF(create_cq_in, in, pas);\n\tmlx5_fill_page_frag_array(&vcq->buf.frag_buf, pas);\n\n\tcqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);\n\tMLX5_SET(cqc, cqc, log_page_size, vcq->buf.frag_buf.page_shift - MLX5_ADAPTER_PAGE_SHIFT);\n\n\t \n\terr = mlx5_comp_eqn_get(mdev, 0, &eqn);\n\tif (err)\n\t\tgoto err_vec;\n\n\tcqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);\n\tMLX5_SET(cqc, cqc, log_cq_size, ilog2(num_ent));\n\tMLX5_SET(cqc, cqc, uar_page, ndev->mvdev.res.uar->index);\n\tMLX5_SET(cqc, cqc, c_eqn_or_apu_element, eqn);\n\tMLX5_SET64(cqc, cqc, dbr_addr, vcq->db.dma);\n\n\terr = mlx5_core_create_cq(mdev, &vcq->mcq, in, inlen, out, sizeof(out));\n\tif (err)\n\t\tgoto err_vec;\n\n\tvcq->mcq.comp = mlx5_vdpa_cq_comp;\n\tvcq->cqe = num_ent;\n\tvcq->mcq.set_ci_db = vcq->db.db;\n\tvcq->mcq.arm_db = vcq->db.db + 1;\n\tmlx5_cq_arm(&mvq->cq.mcq, MLX5_CQ_DB_REQ_NOT, uar_page, mvq->cq.mcq.cons_index);\n\tkfree(in);\n\treturn 0;\n\nerr_vec:\n\tkfree(in);\nerr_vzalloc:\n\tcq_frag_buf_free(ndev, &vcq->buf);\nerr_db:\n\tmlx5_db_free(ndev->mvdev.mdev, &vcq->db);\n\treturn err;\n}\n\nstatic void cq_destroy(struct mlx5_vdpa_net *ndev, u16 idx)\n{\n\tstruct mlx5_vdpa_virtqueue *mvq = &ndev->vqs[idx];\n\tstruct mlx5_core_dev *mdev = ndev->mvdev.mdev;\n\tstruct mlx5_vdpa_cq *vcq = &mvq->cq;\n\n\tif (mlx5_core_destroy_cq(mdev, &vcq->mcq)) {\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"destroy CQ 0x%x\\n\", vcq->mcq.cqn);\n\t\treturn;\n\t}\n\tcq_frag_buf_free(ndev, &vcq->buf);\n\tmlx5_db_free(ndev->mvdev.mdev, &vcq->db);\n}\n\nstatic int read_umem_params(struct mlx5_vdpa_net *ndev)\n{\n\tu32 in[MLX5_ST_SZ_DW(query_hca_cap_in)] = {};\n\tu16 opmod = (MLX5_CAP_VDPA_EMULATION << 1) | (HCA_CAP_OPMOD_GET_CUR & 0x01);\n\tstruct mlx5_core_dev *mdev = ndev->mvdev.mdev;\n\tint out_size;\n\tvoid *caps;\n\tvoid *out;\n\tint err;\n\n\tout_size = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tout = kzalloc(out_size, GFP_KERNEL);\n\tif (!out)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(query_hca_cap_in, in, opcode, MLX5_CMD_OP_QUERY_HCA_CAP);\n\tMLX5_SET(query_hca_cap_in, in, op_mod, opmod);\n\terr = mlx5_cmd_exec_inout(mdev, query_hca_cap, in, out);\n\tif (err) {\n\t\tmlx5_vdpa_warn(&ndev->mvdev,\n\t\t\t\"Failed reading vdpa umem capabilities with err %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\tcaps =  MLX5_ADDR_OF(query_hca_cap_out, out, capability);\n\n\tndev->umem_1_buffer_param_a = MLX5_GET(virtio_emulation_cap, caps, umem_1_buffer_param_a);\n\tndev->umem_1_buffer_param_b = MLX5_GET(virtio_emulation_cap, caps, umem_1_buffer_param_b);\n\n\tndev->umem_2_buffer_param_a = MLX5_GET(virtio_emulation_cap, caps, umem_2_buffer_param_a);\n\tndev->umem_2_buffer_param_b = MLX5_GET(virtio_emulation_cap, caps, umem_2_buffer_param_b);\n\n\tndev->umem_3_buffer_param_a = MLX5_GET(virtio_emulation_cap, caps, umem_3_buffer_param_a);\n\tndev->umem_3_buffer_param_b = MLX5_GET(virtio_emulation_cap, caps, umem_3_buffer_param_b);\n\nout:\n\tkfree(out);\n\treturn 0;\n}\n\nstatic void set_umem_size(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq, int num,\n\t\t\t  struct mlx5_vdpa_umem **umemp)\n{\n\tu32 p_a;\n\tu32 p_b;\n\n\tswitch (num) {\n\tcase 1:\n\t\tp_a = ndev->umem_1_buffer_param_a;\n\t\tp_b = ndev->umem_1_buffer_param_b;\n\t\t*umemp = &mvq->umem1;\n\t\tbreak;\n\tcase 2:\n\t\tp_a = ndev->umem_2_buffer_param_a;\n\t\tp_b = ndev->umem_2_buffer_param_b;\n\t\t*umemp = &mvq->umem2;\n\t\tbreak;\n\tcase 3:\n\t\tp_a = ndev->umem_3_buffer_param_a;\n\t\tp_b = ndev->umem_3_buffer_param_b;\n\t\t*umemp = &mvq->umem3;\n\t\tbreak;\n\t}\n\n\t(*umemp)->size = p_a * mvq->num_ent + p_b;\n}\n\nstatic void umem_frag_buf_free(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_umem *umem)\n{\n\tmlx5_frag_buf_free(ndev->mvdev.mdev, &umem->frag_buf);\n}\n\nstatic int create_umem(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq, int num)\n{\n\tint inlen;\n\tu32 out[MLX5_ST_SZ_DW(create_umem_out)] = {};\n\tvoid *um;\n\tvoid *in;\n\tint err;\n\t__be64 *pas;\n\tstruct mlx5_vdpa_umem *umem;\n\n\tset_umem_size(ndev, mvq, num, &umem);\n\terr = umem_frag_buf_alloc(ndev, umem, umem->size);\n\tif (err)\n\t\treturn err;\n\n\tinlen = MLX5_ST_SZ_BYTES(create_umem_in) + MLX5_ST_SZ_BYTES(mtt) * umem->frag_buf.npages;\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_in;\n\t}\n\n\tMLX5_SET(create_umem_in, in, opcode, MLX5_CMD_OP_CREATE_UMEM);\n\tMLX5_SET(create_umem_in, in, uid, ndev->mvdev.res.uid);\n\tum = MLX5_ADDR_OF(create_umem_in, in, umem);\n\tMLX5_SET(umem, um, log_page_size, umem->frag_buf.page_shift - MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET64(umem, um, num_of_mtt, umem->frag_buf.npages);\n\n\tpas = (__be64 *)MLX5_ADDR_OF(umem, um, mtt[0]);\n\tmlx5_fill_page_frag_array_perm(&umem->frag_buf, pas, MLX5_MTT_PERM_RW);\n\n\terr = mlx5_cmd_exec(ndev->mvdev.mdev, in, inlen, out, sizeof(out));\n\tif (err) {\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"create umem(%d)\\n\", err);\n\t\tgoto err_cmd;\n\t}\n\n\tkfree(in);\n\tumem->id = MLX5_GET(create_umem_out, out, umem_id);\n\n\treturn 0;\n\nerr_cmd:\n\tkfree(in);\nerr_in:\n\tumem_frag_buf_free(ndev, umem);\n\treturn err;\n}\n\nstatic void umem_destroy(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq, int num)\n{\n\tu32 in[MLX5_ST_SZ_DW(destroy_umem_in)] = {};\n\tu32 out[MLX5_ST_SZ_DW(destroy_umem_out)] = {};\n\tstruct mlx5_vdpa_umem *umem;\n\n\tswitch (num) {\n\tcase 1:\n\t\tumem = &mvq->umem1;\n\t\tbreak;\n\tcase 2:\n\t\tumem = &mvq->umem2;\n\t\tbreak;\n\tcase 3:\n\t\tumem = &mvq->umem3;\n\t\tbreak;\n\t}\n\n\tMLX5_SET(destroy_umem_in, in, opcode, MLX5_CMD_OP_DESTROY_UMEM);\n\tMLX5_SET(destroy_umem_in, in, umem_id, umem->id);\n\tif (mlx5_cmd_exec(ndev->mvdev.mdev, in, sizeof(in), out, sizeof(out)))\n\t\treturn;\n\n\tumem_frag_buf_free(ndev, umem);\n}\n\nstatic int umems_create(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tint num;\n\tint err;\n\n\tfor (num = 1; num <= 3; num++) {\n\t\terr = create_umem(ndev, mvq, num);\n\t\tif (err)\n\t\t\tgoto err_umem;\n\t}\n\treturn 0;\n\nerr_umem:\n\tfor (num--; num > 0; num--)\n\t\tumem_destroy(ndev, mvq, num);\n\n\treturn err;\n}\n\nstatic void umems_destroy(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tint num;\n\n\tfor (num = 3; num > 0; num--)\n\t\tumem_destroy(ndev, mvq, num);\n}\n\nstatic int get_queue_type(struct mlx5_vdpa_net *ndev)\n{\n\tu32 type_mask;\n\n\ttype_mask = MLX5_CAP_DEV_VDPA_EMULATION(ndev->mvdev.mdev, virtio_queue_type);\n\n\t \n\tif (type_mask & MLX5_VIRTIO_EMULATION_CAP_VIRTIO_QUEUE_TYPE_SPLIT)\n\t\treturn MLX5_VIRTIO_EMULATION_VIRTIO_QUEUE_TYPE_SPLIT;\n\n\tWARN_ON(!(type_mask & MLX5_VIRTIO_EMULATION_CAP_VIRTIO_QUEUE_TYPE_PACKED));\n\n\treturn MLX5_VIRTIO_EMULATION_VIRTIO_QUEUE_TYPE_PACKED;\n}\n\nstatic bool vq_is_tx(u16 idx)\n{\n\treturn idx % 2;\n}\n\nenum {\n\tMLX5_VIRTIO_NET_F_MRG_RXBUF = 2,\n\tMLX5_VIRTIO_NET_F_HOST_ECN = 4,\n\tMLX5_VIRTIO_NET_F_GUEST_ECN = 6,\n\tMLX5_VIRTIO_NET_F_GUEST_TSO6 = 7,\n\tMLX5_VIRTIO_NET_F_GUEST_TSO4 = 8,\n\tMLX5_VIRTIO_NET_F_GUEST_CSUM = 9,\n\tMLX5_VIRTIO_NET_F_CSUM = 10,\n\tMLX5_VIRTIO_NET_F_HOST_TSO6 = 11,\n\tMLX5_VIRTIO_NET_F_HOST_TSO4 = 12,\n};\n\nstatic u16 get_features(u64 features)\n{\n\treturn (!!(features & BIT_ULL(VIRTIO_NET_F_MRG_RXBUF)) << MLX5_VIRTIO_NET_F_MRG_RXBUF) |\n\t       (!!(features & BIT_ULL(VIRTIO_NET_F_HOST_ECN)) << MLX5_VIRTIO_NET_F_HOST_ECN) |\n\t       (!!(features & BIT_ULL(VIRTIO_NET_F_GUEST_ECN)) << MLX5_VIRTIO_NET_F_GUEST_ECN) |\n\t       (!!(features & BIT_ULL(VIRTIO_NET_F_GUEST_TSO6)) << MLX5_VIRTIO_NET_F_GUEST_TSO6) |\n\t       (!!(features & BIT_ULL(VIRTIO_NET_F_GUEST_TSO4)) << MLX5_VIRTIO_NET_F_GUEST_TSO4) |\n\t       (!!(features & BIT_ULL(VIRTIO_NET_F_CSUM)) << MLX5_VIRTIO_NET_F_CSUM) |\n\t       (!!(features & BIT_ULL(VIRTIO_NET_F_HOST_TSO6)) << MLX5_VIRTIO_NET_F_HOST_TSO6) |\n\t       (!!(features & BIT_ULL(VIRTIO_NET_F_HOST_TSO4)) << MLX5_VIRTIO_NET_F_HOST_TSO4);\n}\n\nstatic bool counters_supported(const struct mlx5_vdpa_dev *mvdev)\n{\n\treturn MLX5_CAP_GEN_64(mvdev->mdev, general_obj_types) &\n\t       BIT_ULL(MLX5_OBJ_TYPE_VIRTIO_Q_COUNTERS);\n}\n\nstatic bool msix_mode_supported(struct mlx5_vdpa_dev *mvdev)\n{\n\treturn MLX5_CAP_DEV_VDPA_EMULATION(mvdev->mdev, event_mode) &\n\t\t(1 << MLX5_VIRTIO_Q_EVENT_MODE_MSIX_MODE) &&\n\t\tpci_msix_can_alloc_dyn(mvdev->mdev->pdev);\n}\n\nstatic int create_virtqueue(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_virtio_net_q_in);\n\tu32 out[MLX5_ST_SZ_DW(create_virtio_net_q_out)] = {};\n\tvoid *obj_context;\n\tu16 mlx_features;\n\tvoid *cmd_hdr;\n\tvoid *vq_ctx;\n\tvoid *in;\n\tint err;\n\n\terr = umems_create(ndev, mvq);\n\tif (err)\n\t\treturn err;\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc;\n\t}\n\n\tmlx_features = get_features(ndev->mvdev.actual_features);\n\tcmd_hdr = MLX5_ADDR_OF(create_virtio_net_q_in, in, general_obj_in_cmd_hdr);\n\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, opcode, MLX5_CMD_OP_CREATE_GENERAL_OBJECT);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_type, MLX5_OBJ_TYPE_VIRTIO_NET_Q);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, uid, ndev->mvdev.res.uid);\n\n\tobj_context = MLX5_ADDR_OF(create_virtio_net_q_in, in, obj_context);\n\tMLX5_SET(virtio_net_q_object, obj_context, hw_available_index, mvq->avail_idx);\n\tMLX5_SET(virtio_net_q_object, obj_context, hw_used_index, mvq->used_idx);\n\tMLX5_SET(virtio_net_q_object, obj_context, queue_feature_bit_mask_12_3,\n\t\t mlx_features >> 3);\n\tMLX5_SET(virtio_net_q_object, obj_context, queue_feature_bit_mask_2_0,\n\t\t mlx_features & 7);\n\tvq_ctx = MLX5_ADDR_OF(virtio_net_q_object, obj_context, virtio_q_context);\n\tMLX5_SET(virtio_q, vq_ctx, virtio_q_type, get_queue_type(ndev));\n\n\tif (vq_is_tx(mvq->index))\n\t\tMLX5_SET(virtio_net_q_object, obj_context, tisn_or_qpn, ndev->res.tisn);\n\n\tif (mvq->map.virq) {\n\t\tMLX5_SET(virtio_q, vq_ctx, event_mode, MLX5_VIRTIO_Q_EVENT_MODE_MSIX_MODE);\n\t\tMLX5_SET(virtio_q, vq_ctx, event_qpn_or_msix, mvq->map.index);\n\t} else {\n\t\tMLX5_SET(virtio_q, vq_ctx, event_mode, MLX5_VIRTIO_Q_EVENT_MODE_QP_MODE);\n\t\tMLX5_SET(virtio_q, vq_ctx, event_qpn_or_msix, mvq->fwqp.mqp.qpn);\n\t}\n\n\tMLX5_SET(virtio_q, vq_ctx, queue_index, mvq->index);\n\tMLX5_SET(virtio_q, vq_ctx, queue_size, mvq->num_ent);\n\tMLX5_SET(virtio_q, vq_ctx, virtio_version_1_0,\n\t\t !!(ndev->mvdev.actual_features & BIT_ULL(VIRTIO_F_VERSION_1)));\n\tMLX5_SET64(virtio_q, vq_ctx, desc_addr, mvq->desc_addr);\n\tMLX5_SET64(virtio_q, vq_ctx, used_addr, mvq->device_addr);\n\tMLX5_SET64(virtio_q, vq_ctx, available_addr, mvq->driver_addr);\n\tMLX5_SET(virtio_q, vq_ctx, virtio_q_mkey, ndev->mvdev.mr.mkey);\n\tMLX5_SET(virtio_q, vq_ctx, umem_1_id, mvq->umem1.id);\n\tMLX5_SET(virtio_q, vq_ctx, umem_1_size, mvq->umem1.size);\n\tMLX5_SET(virtio_q, vq_ctx, umem_2_id, mvq->umem2.id);\n\tMLX5_SET(virtio_q, vq_ctx, umem_2_size, mvq->umem2.size);\n\tMLX5_SET(virtio_q, vq_ctx, umem_3_id, mvq->umem3.id);\n\tMLX5_SET(virtio_q, vq_ctx, umem_3_size, mvq->umem3.size);\n\tMLX5_SET(virtio_q, vq_ctx, pd, ndev->mvdev.res.pdn);\n\tif (counters_supported(&ndev->mvdev))\n\t\tMLX5_SET(virtio_q, vq_ctx, counter_set_id, mvq->counter_set_id);\n\n\terr = mlx5_cmd_exec(ndev->mvdev.mdev, in, inlen, out, sizeof(out));\n\tif (err)\n\t\tgoto err_cmd;\n\n\tmvq->fw_state = MLX5_VIRTIO_NET_Q_OBJECT_STATE_INIT;\n\tkfree(in);\n\tmvq->virtq_id = MLX5_GET(general_obj_out_cmd_hdr, out, obj_id);\n\n\treturn 0;\n\nerr_cmd:\n\tkfree(in);\nerr_alloc:\n\tumems_destroy(ndev, mvq);\n\treturn err;\n}\n\nstatic void destroy_virtqueue(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tu32 in[MLX5_ST_SZ_DW(destroy_virtio_net_q_in)] = {};\n\tu32 out[MLX5_ST_SZ_DW(destroy_virtio_net_q_out)] = {};\n\n\tMLX5_SET(destroy_virtio_net_q_in, in, general_obj_out_cmd_hdr.opcode,\n\t\t MLX5_CMD_OP_DESTROY_GENERAL_OBJECT);\n\tMLX5_SET(destroy_virtio_net_q_in, in, general_obj_out_cmd_hdr.obj_id, mvq->virtq_id);\n\tMLX5_SET(destroy_virtio_net_q_in, in, general_obj_out_cmd_hdr.uid, ndev->mvdev.res.uid);\n\tMLX5_SET(destroy_virtio_net_q_in, in, general_obj_out_cmd_hdr.obj_type,\n\t\t MLX5_OBJ_TYPE_VIRTIO_NET_Q);\n\tif (mlx5_cmd_exec(ndev->mvdev.mdev, in, sizeof(in), out, sizeof(out))) {\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"destroy virtqueue 0x%x\\n\", mvq->virtq_id);\n\t\treturn;\n\t}\n\tmvq->fw_state = MLX5_VIRTIO_NET_Q_OBJECT_NONE;\n\tumems_destroy(ndev, mvq);\n}\n\nstatic u32 get_rqpn(struct mlx5_vdpa_virtqueue *mvq, bool fw)\n{\n\treturn fw ? mvq->vqqp.mqp.qpn : mvq->fwqp.mqp.qpn;\n}\n\nstatic u32 get_qpn(struct mlx5_vdpa_virtqueue *mvq, bool fw)\n{\n\treturn fw ? mvq->fwqp.mqp.qpn : mvq->vqqp.mqp.qpn;\n}\n\nstatic void alloc_inout(struct mlx5_vdpa_net *ndev, int cmd, void **in, int *inlen, void **out,\n\t\t\tint *outlen, u32 qpn, u32 rqpn)\n{\n\tvoid *qpc;\n\tvoid *pp;\n\n\tswitch (cmd) {\n\tcase MLX5_CMD_OP_2RST_QP:\n\t\t*inlen = MLX5_ST_SZ_BYTES(qp_2rst_in);\n\t\t*outlen = MLX5_ST_SZ_BYTES(qp_2rst_out);\n\t\t*in = kzalloc(*inlen, GFP_KERNEL);\n\t\t*out = kzalloc(*outlen, GFP_KERNEL);\n\t\tif (!*in || !*out)\n\t\t\tgoto outerr;\n\n\t\tMLX5_SET(qp_2rst_in, *in, opcode, cmd);\n\t\tMLX5_SET(qp_2rst_in, *in, uid, ndev->mvdev.res.uid);\n\t\tMLX5_SET(qp_2rst_in, *in, qpn, qpn);\n\t\tbreak;\n\tcase MLX5_CMD_OP_RST2INIT_QP:\n\t\t*inlen = MLX5_ST_SZ_BYTES(rst2init_qp_in);\n\t\t*outlen = MLX5_ST_SZ_BYTES(rst2init_qp_out);\n\t\t*in = kzalloc(*inlen, GFP_KERNEL);\n\t\t*out = kzalloc(MLX5_ST_SZ_BYTES(rst2init_qp_out), GFP_KERNEL);\n\t\tif (!*in || !*out)\n\t\t\tgoto outerr;\n\n\t\tMLX5_SET(rst2init_qp_in, *in, opcode, cmd);\n\t\tMLX5_SET(rst2init_qp_in, *in, uid, ndev->mvdev.res.uid);\n\t\tMLX5_SET(rst2init_qp_in, *in, qpn, qpn);\n\t\tqpc = MLX5_ADDR_OF(rst2init_qp_in, *in, qpc);\n\t\tMLX5_SET(qpc, qpc, remote_qpn, rqpn);\n\t\tMLX5_SET(qpc, qpc, rwe, 1);\n\t\tpp = MLX5_ADDR_OF(qpc, qpc, primary_address_path);\n\t\tMLX5_SET(ads, pp, vhca_port_num, 1);\n\t\tbreak;\n\tcase MLX5_CMD_OP_INIT2RTR_QP:\n\t\t*inlen = MLX5_ST_SZ_BYTES(init2rtr_qp_in);\n\t\t*outlen = MLX5_ST_SZ_BYTES(init2rtr_qp_out);\n\t\t*in = kzalloc(*inlen, GFP_KERNEL);\n\t\t*out = kzalloc(MLX5_ST_SZ_BYTES(init2rtr_qp_out), GFP_KERNEL);\n\t\tif (!*in || !*out)\n\t\t\tgoto outerr;\n\n\t\tMLX5_SET(init2rtr_qp_in, *in, opcode, cmd);\n\t\tMLX5_SET(init2rtr_qp_in, *in, uid, ndev->mvdev.res.uid);\n\t\tMLX5_SET(init2rtr_qp_in, *in, qpn, qpn);\n\t\tqpc = MLX5_ADDR_OF(rst2init_qp_in, *in, qpc);\n\t\tMLX5_SET(qpc, qpc, mtu, MLX5_QPC_MTU_256_BYTES);\n\t\tMLX5_SET(qpc, qpc, log_msg_max, 30);\n\t\tMLX5_SET(qpc, qpc, remote_qpn, rqpn);\n\t\tpp = MLX5_ADDR_OF(qpc, qpc, primary_address_path);\n\t\tMLX5_SET(ads, pp, fl, 1);\n\t\tbreak;\n\tcase MLX5_CMD_OP_RTR2RTS_QP:\n\t\t*inlen = MLX5_ST_SZ_BYTES(rtr2rts_qp_in);\n\t\t*outlen = MLX5_ST_SZ_BYTES(rtr2rts_qp_out);\n\t\t*in = kzalloc(*inlen, GFP_KERNEL);\n\t\t*out = kzalloc(MLX5_ST_SZ_BYTES(rtr2rts_qp_out), GFP_KERNEL);\n\t\tif (!*in || !*out)\n\t\t\tgoto outerr;\n\n\t\tMLX5_SET(rtr2rts_qp_in, *in, opcode, cmd);\n\t\tMLX5_SET(rtr2rts_qp_in, *in, uid, ndev->mvdev.res.uid);\n\t\tMLX5_SET(rtr2rts_qp_in, *in, qpn, qpn);\n\t\tqpc = MLX5_ADDR_OF(rst2init_qp_in, *in, qpc);\n\t\tpp = MLX5_ADDR_OF(qpc, qpc, primary_address_path);\n\t\tMLX5_SET(ads, pp, ack_timeout, 14);\n\t\tMLX5_SET(qpc, qpc, retry_count, 7);\n\t\tMLX5_SET(qpc, qpc, rnr_retry, 7);\n\t\tbreak;\n\tdefault:\n\t\tgoto outerr_nullify;\n\t}\n\n\treturn;\n\nouterr:\n\tkfree(*in);\n\tkfree(*out);\nouterr_nullify:\n\t*in = NULL;\n\t*out = NULL;\n}\n\nstatic void free_inout(void *in, void *out)\n{\n\tkfree(in);\n\tkfree(out);\n}\n\n \nstatic int modify_qp(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq, bool fw, int cmd)\n{\n\tint outlen;\n\tint inlen;\n\tvoid *out;\n\tvoid *in;\n\tint err;\n\n\talloc_inout(ndev, cmd, &in, &inlen, &out, &outlen, get_qpn(mvq, fw), get_rqpn(mvq, fw));\n\tif (!in || !out)\n\t\treturn -ENOMEM;\n\n\terr = mlx5_cmd_exec(ndev->mvdev.mdev, in, inlen, out, outlen);\n\tfree_inout(in, out);\n\treturn err;\n}\n\nstatic int connect_qps(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tint err;\n\n\terr = modify_qp(ndev, mvq, true, MLX5_CMD_OP_2RST_QP);\n\tif (err)\n\t\treturn err;\n\n\terr = modify_qp(ndev, mvq, false, MLX5_CMD_OP_2RST_QP);\n\tif (err)\n\t\treturn err;\n\n\terr = modify_qp(ndev, mvq, true, MLX5_CMD_OP_RST2INIT_QP);\n\tif (err)\n\t\treturn err;\n\n\terr = modify_qp(ndev, mvq, false, MLX5_CMD_OP_RST2INIT_QP);\n\tif (err)\n\t\treturn err;\n\n\terr = modify_qp(ndev, mvq, true, MLX5_CMD_OP_INIT2RTR_QP);\n\tif (err)\n\t\treturn err;\n\n\terr = modify_qp(ndev, mvq, false, MLX5_CMD_OP_INIT2RTR_QP);\n\tif (err)\n\t\treturn err;\n\n\treturn modify_qp(ndev, mvq, true, MLX5_CMD_OP_RTR2RTS_QP);\n}\n\nstruct mlx5_virtq_attr {\n\tu8 state;\n\tu16 available_index;\n\tu16 used_index;\n};\n\nstatic int query_virtqueue(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq,\n\t\t\t   struct mlx5_virtq_attr *attr)\n{\n\tint outlen = MLX5_ST_SZ_BYTES(query_virtio_net_q_out);\n\tu32 in[MLX5_ST_SZ_DW(query_virtio_net_q_in)] = {};\n\tvoid *out;\n\tvoid *obj_context;\n\tvoid *cmd_hdr;\n\tint err;\n\n\tout = kzalloc(outlen, GFP_KERNEL);\n\tif (!out)\n\t\treturn -ENOMEM;\n\n\tcmd_hdr = MLX5_ADDR_OF(query_virtio_net_q_in, in, general_obj_in_cmd_hdr);\n\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, opcode, MLX5_CMD_OP_QUERY_GENERAL_OBJECT);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_type, MLX5_OBJ_TYPE_VIRTIO_NET_Q);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_id, mvq->virtq_id);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, uid, ndev->mvdev.res.uid);\n\terr = mlx5_cmd_exec(ndev->mvdev.mdev, in, sizeof(in), out, outlen);\n\tif (err)\n\t\tgoto err_cmd;\n\n\tobj_context = MLX5_ADDR_OF(query_virtio_net_q_out, out, obj_context);\n\tmemset(attr, 0, sizeof(*attr));\n\tattr->state = MLX5_GET(virtio_net_q_object, obj_context, state);\n\tattr->available_index = MLX5_GET(virtio_net_q_object, obj_context, hw_available_index);\n\tattr->used_index = MLX5_GET(virtio_net_q_object, obj_context, hw_used_index);\n\tkfree(out);\n\treturn 0;\n\nerr_cmd:\n\tkfree(out);\n\treturn err;\n}\n\nstatic bool is_valid_state_change(int oldstate, int newstate)\n{\n\tswitch (oldstate) {\n\tcase MLX5_VIRTIO_NET_Q_OBJECT_STATE_INIT:\n\t\treturn newstate == MLX5_VIRTIO_NET_Q_OBJECT_STATE_RDY;\n\tcase MLX5_VIRTIO_NET_Q_OBJECT_STATE_RDY:\n\t\treturn newstate == MLX5_VIRTIO_NET_Q_OBJECT_STATE_SUSPEND;\n\tcase MLX5_VIRTIO_NET_Q_OBJECT_STATE_SUSPEND:\n\tcase MLX5_VIRTIO_NET_Q_OBJECT_STATE_ERR:\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int modify_virtqueue(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq, int state)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(modify_virtio_net_q_in);\n\tu32 out[MLX5_ST_SZ_DW(modify_virtio_net_q_out)] = {};\n\tvoid *obj_context;\n\tvoid *cmd_hdr;\n\tvoid *in;\n\tint err;\n\n\tif (mvq->fw_state == MLX5_VIRTIO_NET_Q_OBJECT_NONE)\n\t\treturn 0;\n\n\tif (!is_valid_state_change(mvq->fw_state, state))\n\t\treturn -EINVAL;\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tcmd_hdr = MLX5_ADDR_OF(modify_virtio_net_q_in, in, general_obj_in_cmd_hdr);\n\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, opcode, MLX5_CMD_OP_MODIFY_GENERAL_OBJECT);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_type, MLX5_OBJ_TYPE_VIRTIO_NET_Q);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_id, mvq->virtq_id);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, uid, ndev->mvdev.res.uid);\n\n\tobj_context = MLX5_ADDR_OF(modify_virtio_net_q_in, in, obj_context);\n\tMLX5_SET64(virtio_net_q_object, obj_context, modify_field_select,\n\t\t   MLX5_VIRTQ_MODIFY_MASK_STATE);\n\tMLX5_SET(virtio_net_q_object, obj_context, state, state);\n\terr = mlx5_cmd_exec(ndev->mvdev.mdev, in, inlen, out, sizeof(out));\n\tkfree(in);\n\tif (!err)\n\t\tmvq->fw_state = state;\n\n\treturn err;\n}\n\nstatic int counter_set_alloc(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tu32 in[MLX5_ST_SZ_DW(create_virtio_q_counters_in)] = {};\n\tu32 out[MLX5_ST_SZ_DW(create_virtio_q_counters_out)] = {};\n\tvoid *cmd_hdr;\n\tint err;\n\n\tif (!counters_supported(&ndev->mvdev))\n\t\treturn 0;\n\n\tcmd_hdr = MLX5_ADDR_OF(create_virtio_q_counters_in, in, hdr);\n\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, opcode, MLX5_CMD_OP_CREATE_GENERAL_OBJECT);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_type, MLX5_OBJ_TYPE_VIRTIO_Q_COUNTERS);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, uid, ndev->mvdev.res.uid);\n\n\terr = mlx5_cmd_exec(ndev->mvdev.mdev, in, sizeof(in), out, sizeof(out));\n\tif (err)\n\t\treturn err;\n\n\tmvq->counter_set_id = MLX5_GET(general_obj_out_cmd_hdr, out, obj_id);\n\n\treturn 0;\n}\n\nstatic void counter_set_dealloc(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tu32 in[MLX5_ST_SZ_DW(destroy_virtio_q_counters_in)] = {};\n\tu32 out[MLX5_ST_SZ_DW(destroy_virtio_q_counters_out)] = {};\n\n\tif (!counters_supported(&ndev->mvdev))\n\t\treturn;\n\n\tMLX5_SET(destroy_virtio_q_counters_in, in, hdr.opcode, MLX5_CMD_OP_DESTROY_GENERAL_OBJECT);\n\tMLX5_SET(destroy_virtio_q_counters_in, in, hdr.obj_id, mvq->counter_set_id);\n\tMLX5_SET(destroy_virtio_q_counters_in, in, hdr.uid, ndev->mvdev.res.uid);\n\tMLX5_SET(destroy_virtio_q_counters_in, in, hdr.obj_type, MLX5_OBJ_TYPE_VIRTIO_Q_COUNTERS);\n\tif (mlx5_cmd_exec(ndev->mvdev.mdev, in, sizeof(in), out, sizeof(out)))\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"dealloc counter set 0x%x\\n\", mvq->counter_set_id);\n}\n\nstatic irqreturn_t mlx5_vdpa_int_handler(int irq, void *priv)\n{\n\tstruct vdpa_callback *cb = priv;\n\n\tif (cb->callback)\n\t\treturn cb->callback(cb->private);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void alloc_vector(struct mlx5_vdpa_net *ndev,\n\t\t\t struct mlx5_vdpa_virtqueue *mvq)\n{\n\tstruct mlx5_vdpa_irq_pool *irqp = &ndev->irqp;\n\tstruct mlx5_vdpa_irq_pool_entry *ent;\n\tint err;\n\tint i;\n\n\tfor (i = 0; i < irqp->num_ent; i++) {\n\t\tent = &irqp->entries[i];\n\t\tif (!ent->used) {\n\t\t\tsnprintf(ent->name, MLX5_VDPA_IRQ_NAME_LEN, \"%s-vq-%d\",\n\t\t\t\t dev_name(&ndev->mvdev.vdev.dev), mvq->index);\n\t\t\tent->dev_id = &ndev->event_cbs[mvq->index];\n\t\t\terr = request_irq(ent->map.virq, mlx5_vdpa_int_handler, 0,\n\t\t\t\t\t  ent->name, ent->dev_id);\n\t\t\tif (err)\n\t\t\t\treturn;\n\n\t\t\tent->used = true;\n\t\t\tmvq->map = ent->map;\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic void dealloc_vector(struct mlx5_vdpa_net *ndev,\n\t\t\t   struct mlx5_vdpa_virtqueue *mvq)\n{\n\tstruct mlx5_vdpa_irq_pool *irqp = &ndev->irqp;\n\tint i;\n\n\tfor (i = 0; i < irqp->num_ent; i++)\n\t\tif (mvq->map.virq == irqp->entries[i].map.virq) {\n\t\t\tfree_irq(mvq->map.virq, irqp->entries[i].dev_id);\n\t\t\tirqp->entries[i].used = false;\n\t\t\treturn;\n\t\t}\n}\n\nstatic int setup_vq(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tu16 idx = mvq->index;\n\tint err;\n\n\tif (!mvq->num_ent)\n\t\treturn 0;\n\n\tif (mvq->initialized)\n\t\treturn 0;\n\n\terr = cq_create(ndev, idx, mvq->num_ent);\n\tif (err)\n\t\treturn err;\n\n\terr = qp_create(ndev, mvq, &mvq->fwqp);\n\tif (err)\n\t\tgoto err_fwqp;\n\n\terr = qp_create(ndev, mvq, &mvq->vqqp);\n\tif (err)\n\t\tgoto err_vqqp;\n\n\terr = connect_qps(ndev, mvq);\n\tif (err)\n\t\tgoto err_connect;\n\n\terr = counter_set_alloc(ndev, mvq);\n\tif (err)\n\t\tgoto err_connect;\n\n\talloc_vector(ndev, mvq);\n\terr = create_virtqueue(ndev, mvq);\n\tif (err)\n\t\tgoto err_vq;\n\n\tif (mvq->ready) {\n\t\terr = modify_virtqueue(ndev, mvq, MLX5_VIRTIO_NET_Q_OBJECT_STATE_RDY);\n\t\tif (err) {\n\t\t\tmlx5_vdpa_warn(&ndev->mvdev, \"failed to modify to ready vq idx %d(%d)\\n\",\n\t\t\t\t       idx, err);\n\t\t\tgoto err_modify;\n\t\t}\n\t}\n\n\tmvq->initialized = true;\n\treturn 0;\n\nerr_modify:\n\tdestroy_virtqueue(ndev, mvq);\nerr_vq:\n\tdealloc_vector(ndev, mvq);\n\tcounter_set_dealloc(ndev, mvq);\nerr_connect:\n\tqp_destroy(ndev, &mvq->vqqp);\nerr_vqqp:\n\tqp_destroy(ndev, &mvq->fwqp);\nerr_fwqp:\n\tcq_destroy(ndev, idx);\n\treturn err;\n}\n\nstatic void suspend_vq(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tstruct mlx5_virtq_attr attr;\n\n\tif (!mvq->initialized)\n\t\treturn;\n\n\tif (mvq->fw_state != MLX5_VIRTIO_NET_Q_OBJECT_STATE_RDY)\n\t\treturn;\n\n\tif (modify_virtqueue(ndev, mvq, MLX5_VIRTIO_NET_Q_OBJECT_STATE_SUSPEND))\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"modify to suspend failed\\n\");\n\n\tif (query_virtqueue(ndev, mvq, &attr)) {\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"failed to query virtqueue\\n\");\n\t\treturn;\n\t}\n\tmvq->avail_idx = attr.available_index;\n\tmvq->used_idx = attr.used_index;\n}\n\nstatic void suspend_vqs(struct mlx5_vdpa_net *ndev)\n{\n\tint i;\n\n\tfor (i = 0; i < ndev->mvdev.max_vqs; i++)\n\t\tsuspend_vq(ndev, &ndev->vqs[i]);\n}\n\nstatic void teardown_vq(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tif (!mvq->initialized)\n\t\treturn;\n\n\tsuspend_vq(ndev, mvq);\n\tdestroy_virtqueue(ndev, mvq);\n\tdealloc_vector(ndev, mvq);\n\tcounter_set_dealloc(ndev, mvq);\n\tqp_destroy(ndev, &mvq->vqqp);\n\tqp_destroy(ndev, &mvq->fwqp);\n\tcq_destroy(ndev, mvq->index);\n\tmvq->initialized = false;\n}\n\nstatic int create_rqt(struct mlx5_vdpa_net *ndev)\n{\n\tint rqt_table_size = roundup_pow_of_two(ndev->rqt_size);\n\tint act_sz = roundup_pow_of_two(ndev->cur_num_vqs / 2);\n\t__be32 *list;\n\tvoid *rqtc;\n\tint inlen;\n\tvoid *in;\n\tint i, j;\n\tint err;\n\n\tinlen = MLX5_ST_SZ_BYTES(create_rqt_in) + rqt_table_size * MLX5_ST_SZ_BYTES(rq_num);\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(create_rqt_in, in, uid, ndev->mvdev.res.uid);\n\trqtc = MLX5_ADDR_OF(create_rqt_in, in, rqt_context);\n\n\tMLX5_SET(rqtc, rqtc, list_q_type, MLX5_RQTC_LIST_Q_TYPE_VIRTIO_NET_Q);\n\tMLX5_SET(rqtc, rqtc, rqt_max_size, rqt_table_size);\n\tlist = MLX5_ADDR_OF(rqtc, rqtc, rq_num[0]);\n\tfor (i = 0, j = 0; i < act_sz; i++, j += 2)\n\t\tlist[i] = cpu_to_be32(ndev->vqs[j % ndev->cur_num_vqs].virtq_id);\n\n\tMLX5_SET(rqtc, rqtc, rqt_actual_size, act_sz);\n\terr = mlx5_vdpa_create_rqt(&ndev->mvdev, in, inlen, &ndev->res.rqtn);\n\tkfree(in);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n#define MLX5_MODIFY_RQT_NUM_RQS ((u64)1)\n\nstatic int modify_rqt(struct mlx5_vdpa_net *ndev, int num)\n{\n\tint act_sz = roundup_pow_of_two(num / 2);\n\t__be32 *list;\n\tvoid *rqtc;\n\tint inlen;\n\tvoid *in;\n\tint i, j;\n\tint err;\n\n\tinlen = MLX5_ST_SZ_BYTES(modify_rqt_in) + act_sz * MLX5_ST_SZ_BYTES(rq_num);\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(modify_rqt_in, in, uid, ndev->mvdev.res.uid);\n\tMLX5_SET64(modify_rqt_in, in, bitmask, MLX5_MODIFY_RQT_NUM_RQS);\n\trqtc = MLX5_ADDR_OF(modify_rqt_in, in, ctx);\n\tMLX5_SET(rqtc, rqtc, list_q_type, MLX5_RQTC_LIST_Q_TYPE_VIRTIO_NET_Q);\n\n\tlist = MLX5_ADDR_OF(rqtc, rqtc, rq_num[0]);\n\tfor (i = 0, j = 0; i < act_sz; i++, j = j + 2)\n\t\tlist[i] = cpu_to_be32(ndev->vqs[j % num].virtq_id);\n\n\tMLX5_SET(rqtc, rqtc, rqt_actual_size, act_sz);\n\terr = mlx5_vdpa_modify_rqt(&ndev->mvdev, in, inlen, ndev->res.rqtn);\n\tkfree(in);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic void destroy_rqt(struct mlx5_vdpa_net *ndev)\n{\n\tmlx5_vdpa_destroy_rqt(&ndev->mvdev, ndev->res.rqtn);\n}\n\nstatic int create_tir(struct mlx5_vdpa_net *ndev)\n{\n#define HASH_IP_L4PORTS                                                                            \\\n\t(MLX5_HASH_FIELD_SEL_SRC_IP | MLX5_HASH_FIELD_SEL_DST_IP | MLX5_HASH_FIELD_SEL_L4_SPORT |  \\\n\t MLX5_HASH_FIELD_SEL_L4_DPORT)\n\tstatic const u8 rx_hash_toeplitz_key[] = { 0x2c, 0xc6, 0x81, 0xd1, 0x5b, 0xdb, 0xf4, 0xf7,\n\t\t\t\t\t\t   0xfc, 0xa2, 0x83, 0x19, 0xdb, 0x1a, 0x3e, 0x94,\n\t\t\t\t\t\t   0x6b, 0x9e, 0x38, 0xd9, 0x2c, 0x9c, 0x03, 0xd1,\n\t\t\t\t\t\t   0xad, 0x99, 0x44, 0xa7, 0xd9, 0x56, 0x3d, 0x59,\n\t\t\t\t\t\t   0x06, 0x3c, 0x25, 0xf3, 0xfc, 0x1f, 0xdc, 0x2a };\n\tvoid *rss_key;\n\tvoid *outer;\n\tvoid *tirc;\n\tvoid *in;\n\tint err;\n\n\tin = kzalloc(MLX5_ST_SZ_BYTES(create_tir_in), GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(create_tir_in, in, uid, ndev->mvdev.res.uid);\n\ttirc = MLX5_ADDR_OF(create_tir_in, in, ctx);\n\tMLX5_SET(tirc, tirc, disp_type, MLX5_TIRC_DISP_TYPE_INDIRECT);\n\n\tMLX5_SET(tirc, tirc, rx_hash_symmetric, 1);\n\tMLX5_SET(tirc, tirc, rx_hash_fn, MLX5_RX_HASH_FN_TOEPLITZ);\n\trss_key = MLX5_ADDR_OF(tirc, tirc, rx_hash_toeplitz_key);\n\tmemcpy(rss_key, rx_hash_toeplitz_key, sizeof(rx_hash_toeplitz_key));\n\n\touter = MLX5_ADDR_OF(tirc, tirc, rx_hash_field_selector_outer);\n\tMLX5_SET(rx_hash_field_select, outer, l3_prot_type, MLX5_L3_PROT_TYPE_IPV4);\n\tMLX5_SET(rx_hash_field_select, outer, l4_prot_type, MLX5_L4_PROT_TYPE_TCP);\n\tMLX5_SET(rx_hash_field_select, outer, selected_fields, HASH_IP_L4PORTS);\n\n\tMLX5_SET(tirc, tirc, indirect_table, ndev->res.rqtn);\n\tMLX5_SET(tirc, tirc, transport_domain, ndev->res.tdn);\n\n\terr = mlx5_vdpa_create_tir(&ndev->mvdev, in, &ndev->res.tirn);\n\tkfree(in);\n\tif (err)\n\t\treturn err;\n\n\tmlx5_vdpa_add_tirn(ndev);\n\treturn err;\n}\n\nstatic void destroy_tir(struct mlx5_vdpa_net *ndev)\n{\n\tmlx5_vdpa_remove_tirn(ndev);\n\tmlx5_vdpa_destroy_tir(&ndev->mvdev, ndev->res.tirn);\n}\n\n#define MAX_STEERING_ENT 0x8000\n#define MAX_STEERING_GROUPS 2\n\n#if defined(CONFIG_MLX5_VDPA_STEERING_DEBUG)\n       #define NUM_DESTS 2\n#else\n       #define NUM_DESTS 1\n#endif\n\nstatic int add_steering_counters(struct mlx5_vdpa_net *ndev,\n\t\t\t\t struct macvlan_node *node,\n\t\t\t\t struct mlx5_flow_act *flow_act,\n\t\t\t\t struct mlx5_flow_destination *dests)\n{\n#if defined(CONFIG_MLX5_VDPA_STEERING_DEBUG)\n\tint err;\n\n\tnode->ucast_counter.counter = mlx5_fc_create(ndev->mvdev.mdev, false);\n\tif (IS_ERR(node->ucast_counter.counter))\n\t\treturn PTR_ERR(node->ucast_counter.counter);\n\n\tnode->mcast_counter.counter = mlx5_fc_create(ndev->mvdev.mdev, false);\n\tif (IS_ERR(node->mcast_counter.counter)) {\n\t\terr = PTR_ERR(node->mcast_counter.counter);\n\t\tgoto err_mcast_counter;\n\t}\n\n\tdests[1].type = MLX5_FLOW_DESTINATION_TYPE_COUNTER;\n\tflow_act->action |= MLX5_FLOW_CONTEXT_ACTION_COUNT;\n\treturn 0;\n\nerr_mcast_counter:\n\tmlx5_fc_destroy(ndev->mvdev.mdev, node->ucast_counter.counter);\n\treturn err;\n#else\n\treturn 0;\n#endif\n}\n\nstatic void remove_steering_counters(struct mlx5_vdpa_net *ndev,\n\t\t\t\t     struct macvlan_node *node)\n{\n#if defined(CONFIG_MLX5_VDPA_STEERING_DEBUG)\n\tmlx5_fc_destroy(ndev->mvdev.mdev, node->mcast_counter.counter);\n\tmlx5_fc_destroy(ndev->mvdev.mdev, node->ucast_counter.counter);\n#endif\n}\n\nstatic int mlx5_vdpa_add_mac_vlan_rules(struct mlx5_vdpa_net *ndev, u8 *mac,\n\t\t\t\t\tstruct macvlan_node *node)\n{\n\tstruct mlx5_flow_destination dests[NUM_DESTS] = {};\n\tstruct mlx5_flow_act flow_act = {};\n\tstruct mlx5_flow_spec *spec;\n\tvoid *headers_c;\n\tvoid *headers_v;\n\tu8 *dmac_c;\n\tu8 *dmac_v;\n\tint err;\n\tu16 vid;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn -ENOMEM;\n\n\tvid = key2vid(node->macvlan);\n\tspec->match_criteria_enable = MLX5_MATCH_OUTER_HEADERS;\n\theaders_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, outer_headers);\n\theaders_v = MLX5_ADDR_OF(fte_match_param, spec->match_value, outer_headers);\n\tdmac_c = MLX5_ADDR_OF(fte_match_param, headers_c, outer_headers.dmac_47_16);\n\tdmac_v = MLX5_ADDR_OF(fte_match_param, headers_v, outer_headers.dmac_47_16);\n\teth_broadcast_addr(dmac_c);\n\tether_addr_copy(dmac_v, mac);\n\tif (ndev->mvdev.actual_features & BIT_ULL(VIRTIO_NET_F_CTRL_VLAN)) {\n\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_c, cvlan_tag, 1);\n\t\tMLX5_SET_TO_ONES(fte_match_set_lyr_2_4, headers_c, first_vid);\n\t}\n\tif (node->tagged) {\n\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_v, cvlan_tag, 1);\n\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_v, first_vid, vid);\n\t}\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;\n\tdests[0].type = MLX5_FLOW_DESTINATION_TYPE_TIR;\n\tdests[0].tir_num = ndev->res.tirn;\n\terr = add_steering_counters(ndev, node, &flow_act, dests);\n\tif (err)\n\t\tgoto out_free;\n\n#if defined(CONFIG_MLX5_VDPA_STEERING_DEBUG)\n\tdests[1].counter_id = mlx5_fc_id(node->ucast_counter.counter);\n#endif\n\tnode->ucast_rule = mlx5_add_flow_rules(ndev->rxft, spec, &flow_act, dests, NUM_DESTS);\n\tif (IS_ERR(node->ucast_rule)) {\n\t\terr = PTR_ERR(node->ucast_rule);\n\t\tgoto err_ucast;\n\t}\n\n#if defined(CONFIG_MLX5_VDPA_STEERING_DEBUG)\n\tdests[1].counter_id = mlx5_fc_id(node->mcast_counter.counter);\n#endif\n\n\tmemset(dmac_c, 0, ETH_ALEN);\n\tmemset(dmac_v, 0, ETH_ALEN);\n\tdmac_c[0] = 1;\n\tdmac_v[0] = 1;\n\tnode->mcast_rule = mlx5_add_flow_rules(ndev->rxft, spec, &flow_act, dests, NUM_DESTS);\n\tif (IS_ERR(node->mcast_rule)) {\n\t\terr = PTR_ERR(node->mcast_rule);\n\t\tgoto err_mcast;\n\t}\n\tkvfree(spec);\n\tmlx5_vdpa_add_rx_counters(ndev, node);\n\treturn 0;\n\nerr_mcast:\n\tmlx5_del_flow_rules(node->ucast_rule);\nerr_ucast:\n\tremove_steering_counters(ndev, node);\nout_free:\n\tkvfree(spec);\n\treturn err;\n}\n\nstatic void mlx5_vdpa_del_mac_vlan_rules(struct mlx5_vdpa_net *ndev,\n\t\t\t\t\t struct macvlan_node *node)\n{\n\tmlx5_vdpa_remove_rx_counters(ndev, node);\n\tmlx5_del_flow_rules(node->ucast_rule);\n\tmlx5_del_flow_rules(node->mcast_rule);\n}\n\nstatic u64 search_val(u8 *mac, u16 vlan, bool tagged)\n{\n\tu64 val;\n\n\tif (!tagged)\n\t\tvlan = MLX5V_UNTAGGED;\n\n\tval = (u64)vlan << 48 |\n\t      (u64)mac[0] << 40 |\n\t      (u64)mac[1] << 32 |\n\t      (u64)mac[2] << 24 |\n\t      (u64)mac[3] << 16 |\n\t      (u64)mac[4] << 8 |\n\t      (u64)mac[5];\n\n\treturn val;\n}\n\nstatic struct macvlan_node *mac_vlan_lookup(struct mlx5_vdpa_net *ndev, u64 value)\n{\n\tstruct macvlan_node *pos;\n\tu32 idx;\n\n\tidx = hash_64(value, 8); \n\thlist_for_each_entry(pos, &ndev->macvlan_hash[idx], hlist) {\n\t\tif (pos->macvlan == value)\n\t\t\treturn pos;\n\t}\n\treturn NULL;\n}\n\nstatic int mac_vlan_add(struct mlx5_vdpa_net *ndev, u8 *mac, u16 vid, bool tagged)\n{\n\tstruct macvlan_node *ptr;\n\tu64 val;\n\tu32 idx;\n\tint err;\n\n\tval = search_val(mac, vid, tagged);\n\tif (mac_vlan_lookup(ndev, val))\n\t\treturn -EEXIST;\n\n\tptr = kzalloc(sizeof(*ptr), GFP_KERNEL);\n\tif (!ptr)\n\t\treturn -ENOMEM;\n\n\tptr->tagged = tagged;\n\tptr->macvlan = val;\n\tptr->ndev = ndev;\n\terr = mlx5_vdpa_add_mac_vlan_rules(ndev, ndev->config.mac, ptr);\n\tif (err)\n\t\tgoto err_add;\n\n\tidx = hash_64(val, 8);\n\thlist_add_head(&ptr->hlist, &ndev->macvlan_hash[idx]);\n\treturn 0;\n\nerr_add:\n\tkfree(ptr);\n\treturn err;\n}\n\nstatic void mac_vlan_del(struct mlx5_vdpa_net *ndev, u8 *mac, u16 vlan, bool tagged)\n{\n\tstruct macvlan_node *ptr;\n\n\tptr = mac_vlan_lookup(ndev, search_val(mac, vlan, tagged));\n\tif (!ptr)\n\t\treturn;\n\n\thlist_del(&ptr->hlist);\n\tmlx5_vdpa_del_mac_vlan_rules(ndev, ptr);\n\tremove_steering_counters(ndev, ptr);\n\tkfree(ptr);\n}\n\nstatic void clear_mac_vlan_table(struct mlx5_vdpa_net *ndev)\n{\n\tstruct macvlan_node *pos;\n\tstruct hlist_node *n;\n\tint i;\n\n\tfor (i = 0; i < MLX5V_MACVLAN_SIZE; i++) {\n\t\thlist_for_each_entry_safe(pos, n, &ndev->macvlan_hash[i], hlist) {\n\t\t\thlist_del(&pos->hlist);\n\t\t\tmlx5_vdpa_del_mac_vlan_rules(ndev, pos);\n\t\t\tremove_steering_counters(ndev, pos);\n\t\t\tkfree(pos);\n\t\t}\n\t}\n}\n\nstatic int setup_steering(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_flow_table_attr ft_attr = {};\n\tstruct mlx5_flow_namespace *ns;\n\tint err;\n\n\tft_attr.max_fte = MAX_STEERING_ENT;\n\tft_attr.autogroup.max_num_groups = MAX_STEERING_GROUPS;\n\n\tns = mlx5_get_flow_namespace(ndev->mvdev.mdev, MLX5_FLOW_NAMESPACE_BYPASS);\n\tif (!ns) {\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"failed to get flow namespace\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tndev->rxft = mlx5_create_auto_grouped_flow_table(ns, &ft_attr);\n\tif (IS_ERR(ndev->rxft)) {\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"failed to create flow table\\n\");\n\t\treturn PTR_ERR(ndev->rxft);\n\t}\n\tmlx5_vdpa_add_rx_flow_table(ndev);\n\n\terr = mac_vlan_add(ndev, ndev->config.mac, 0, false);\n\tif (err)\n\t\tgoto err_add;\n\n\treturn 0;\n\nerr_add:\n\tmlx5_vdpa_remove_rx_flow_table(ndev);\n\tmlx5_destroy_flow_table(ndev->rxft);\n\treturn err;\n}\n\nstatic void teardown_steering(struct mlx5_vdpa_net *ndev)\n{\n\tclear_mac_vlan_table(ndev);\n\tmlx5_vdpa_remove_rx_flow_table(ndev);\n\tmlx5_destroy_flow_table(ndev->rxft);\n}\n\nstatic virtio_net_ctrl_ack handle_ctrl_mac(struct mlx5_vdpa_dev *mvdev, u8 cmd)\n{\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_control_vq *cvq = &mvdev->cvq;\n\tvirtio_net_ctrl_ack status = VIRTIO_NET_ERR;\n\tstruct mlx5_core_dev *pfmdev;\n\tsize_t read;\n\tu8 mac[ETH_ALEN], mac_back[ETH_ALEN];\n\n\tpfmdev = pci_get_drvdata(pci_physfn(mvdev->mdev->pdev));\n\tswitch (cmd) {\n\tcase VIRTIO_NET_CTRL_MAC_ADDR_SET:\n\t\tread = vringh_iov_pull_iotlb(&cvq->vring, &cvq->riov, (void *)mac, ETH_ALEN);\n\t\tif (read != ETH_ALEN)\n\t\t\tbreak;\n\n\t\tif (!memcmp(ndev->config.mac, mac, 6)) {\n\t\t\tstatus = VIRTIO_NET_OK;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (is_zero_ether_addr(mac))\n\t\t\tbreak;\n\n\t\tif (!is_zero_ether_addr(ndev->config.mac)) {\n\t\t\tif (mlx5_mpfs_del_mac(pfmdev, ndev->config.mac)) {\n\t\t\t\tmlx5_vdpa_warn(mvdev, \"failed to delete old MAC %pM from MPFS table\\n\",\n\t\t\t\t\t       ndev->config.mac);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (mlx5_mpfs_add_mac(pfmdev, mac)) {\n\t\t\tmlx5_vdpa_warn(mvdev, \"failed to insert new MAC %pM into MPFS table\\n\",\n\t\t\t\t       mac);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tmemcpy(mac_back, ndev->config.mac, ETH_ALEN);\n\n\t\tmemcpy(ndev->config.mac, mac, ETH_ALEN);\n\n\t\t \n\t\tmac_vlan_del(ndev, mac_back, 0, false);\n\n\t\tif (mac_vlan_add(ndev, ndev->config.mac, 0, false)) {\n\t\t\tmlx5_vdpa_warn(mvdev, \"failed to insert forward rules, try to restore\\n\");\n\n\t\t\t \n\t\t\tif (is_zero_ether_addr(mac_back)) {\n\t\t\t\tmlx5_vdpa_warn(mvdev, \"restore mac failed: Original MAC is zero\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (mlx5_mpfs_del_mac(pfmdev, ndev->config.mac)) {\n\t\t\t\tmlx5_vdpa_warn(mvdev, \"restore mac failed: delete MAC %pM from MPFS table failed\\n\",\n\t\t\t\t\t       ndev->config.mac);\n\t\t\t}\n\n\t\t\tif (mlx5_mpfs_add_mac(pfmdev, mac_back)) {\n\t\t\t\tmlx5_vdpa_warn(mvdev, \"restore mac failed: insert old MAC %pM into MPFS table failed\\n\",\n\t\t\t\t\t       mac_back);\n\t\t\t}\n\n\t\t\tmemcpy(ndev->config.mac, mac_back, ETH_ALEN);\n\n\t\t\tif (mac_vlan_add(ndev, ndev->config.mac, 0, false))\n\t\t\t\tmlx5_vdpa_warn(mvdev, \"restore forward rules failed: insert forward rules failed\\n\");\n\n\t\t\tbreak;\n\t\t}\n\n\t\tstatus = VIRTIO_NET_OK;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn status;\n}\n\nstatic int change_num_qps(struct mlx5_vdpa_dev *mvdev, int newqps)\n{\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tint cur_qps = ndev->cur_num_vqs / 2;\n\tint err;\n\tint i;\n\n\tif (cur_qps > newqps) {\n\t\terr = modify_rqt(ndev, 2 * newqps);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tfor (i = ndev->cur_num_vqs - 1; i >= 2 * newqps; i--)\n\t\t\tteardown_vq(ndev, &ndev->vqs[i]);\n\n\t\tndev->cur_num_vqs = 2 * newqps;\n\t} else {\n\t\tndev->cur_num_vqs = 2 * newqps;\n\t\tfor (i = cur_qps * 2; i < 2 * newqps; i++) {\n\t\t\terr = setup_vq(ndev, &ndev->vqs[i]);\n\t\t\tif (err)\n\t\t\t\tgoto clean_added;\n\t\t}\n\t\terr = modify_rqt(ndev, 2 * newqps);\n\t\tif (err)\n\t\t\tgoto clean_added;\n\t}\n\treturn 0;\n\nclean_added:\n\tfor (--i; i >= 2 * cur_qps; --i)\n\t\tteardown_vq(ndev, &ndev->vqs[i]);\n\n\tndev->cur_num_vqs = 2 * cur_qps;\n\n\treturn err;\n}\n\nstatic virtio_net_ctrl_ack handle_ctrl_mq(struct mlx5_vdpa_dev *mvdev, u8 cmd)\n{\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tvirtio_net_ctrl_ack status = VIRTIO_NET_ERR;\n\tstruct mlx5_control_vq *cvq = &mvdev->cvq;\n\tstruct virtio_net_ctrl_mq mq;\n\tsize_t read;\n\tu16 newqps;\n\n\tswitch (cmd) {\n\tcase VIRTIO_NET_CTRL_MQ_VQ_PAIRS_SET:\n\t\t \n\t\tif (!MLX5_FEATURE(mvdev, VIRTIO_NET_F_MQ))\n\t\t\tbreak;\n\n\t\tread = vringh_iov_pull_iotlb(&cvq->vring, &cvq->riov, (void *)&mq, sizeof(mq));\n\t\tif (read != sizeof(mq))\n\t\t\tbreak;\n\n\t\tnewqps = mlx5vdpa16_to_cpu(mvdev, mq.virtqueue_pairs);\n\t\tif (newqps < VIRTIO_NET_CTRL_MQ_VQ_PAIRS_MIN ||\n\t\t    newqps > ndev->rqt_size)\n\t\t\tbreak;\n\n\t\tif (ndev->cur_num_vqs == 2 * newqps) {\n\t\t\tstatus = VIRTIO_NET_OK;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!change_num_qps(mvdev, newqps))\n\t\t\tstatus = VIRTIO_NET_OK;\n\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn status;\n}\n\nstatic virtio_net_ctrl_ack handle_ctrl_vlan(struct mlx5_vdpa_dev *mvdev, u8 cmd)\n{\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tvirtio_net_ctrl_ack status = VIRTIO_NET_ERR;\n\tstruct mlx5_control_vq *cvq = &mvdev->cvq;\n\t__virtio16 vlan;\n\tsize_t read;\n\tu16 id;\n\n\tif (!(ndev->mvdev.actual_features & BIT_ULL(VIRTIO_NET_F_CTRL_VLAN)))\n\t\treturn status;\n\n\tswitch (cmd) {\n\tcase VIRTIO_NET_CTRL_VLAN_ADD:\n\t\tread = vringh_iov_pull_iotlb(&cvq->vring, &cvq->riov, &vlan, sizeof(vlan));\n\t\tif (read != sizeof(vlan))\n\t\t\tbreak;\n\n\t\tid = mlx5vdpa16_to_cpu(mvdev, vlan);\n\t\tif (mac_vlan_add(ndev, ndev->config.mac, id, true))\n\t\t\tbreak;\n\n\t\tstatus = VIRTIO_NET_OK;\n\t\tbreak;\n\tcase VIRTIO_NET_CTRL_VLAN_DEL:\n\t\tread = vringh_iov_pull_iotlb(&cvq->vring, &cvq->riov, &vlan, sizeof(vlan));\n\t\tif (read != sizeof(vlan))\n\t\t\tbreak;\n\n\t\tid = mlx5vdpa16_to_cpu(mvdev, vlan);\n\t\tmac_vlan_del(ndev, ndev->config.mac, id, true);\n\t\tstatus = VIRTIO_NET_OK;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn status;\n}\n\nstatic void mlx5_cvq_kick_handler(struct work_struct *work)\n{\n\tvirtio_net_ctrl_ack status = VIRTIO_NET_ERR;\n\tstruct virtio_net_ctrl_hdr ctrl;\n\tstruct mlx5_vdpa_wq_ent *wqent;\n\tstruct mlx5_vdpa_dev *mvdev;\n\tstruct mlx5_control_vq *cvq;\n\tstruct mlx5_vdpa_net *ndev;\n\tsize_t read, write;\n\tint err;\n\n\twqent = container_of(work, struct mlx5_vdpa_wq_ent, work);\n\tmvdev = wqent->mvdev;\n\tndev = to_mlx5_vdpa_ndev(mvdev);\n\tcvq = &mvdev->cvq;\n\n\tdown_write(&ndev->reslock);\n\n\tif (!(mvdev->status & VIRTIO_CONFIG_S_DRIVER_OK))\n\t\tgoto out;\n\n\tif (!(ndev->mvdev.actual_features & BIT_ULL(VIRTIO_NET_F_CTRL_VQ)))\n\t\tgoto out;\n\n\tif (!cvq->ready)\n\t\tgoto out;\n\n\twhile (true) {\n\t\terr = vringh_getdesc_iotlb(&cvq->vring, &cvq->riov, &cvq->wiov, &cvq->head,\n\t\t\t\t\t   GFP_ATOMIC);\n\t\tif (err <= 0)\n\t\t\tbreak;\n\n\t\tread = vringh_iov_pull_iotlb(&cvq->vring, &cvq->riov, &ctrl, sizeof(ctrl));\n\t\tif (read != sizeof(ctrl))\n\t\t\tbreak;\n\n\t\tcvq->received_desc++;\n\t\tswitch (ctrl.class) {\n\t\tcase VIRTIO_NET_CTRL_MAC:\n\t\t\tstatus = handle_ctrl_mac(mvdev, ctrl.cmd);\n\t\t\tbreak;\n\t\tcase VIRTIO_NET_CTRL_MQ:\n\t\t\tstatus = handle_ctrl_mq(mvdev, ctrl.cmd);\n\t\t\tbreak;\n\t\tcase VIRTIO_NET_CTRL_VLAN:\n\t\t\tstatus = handle_ctrl_vlan(mvdev, ctrl.cmd);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tsmp_wmb();\n\n\t\twrite = vringh_iov_push_iotlb(&cvq->vring, &cvq->wiov, &status, sizeof(status));\n\t\tvringh_complete_iotlb(&cvq->vring, cvq->head, write);\n\t\tvringh_kiov_cleanup(&cvq->riov);\n\t\tvringh_kiov_cleanup(&cvq->wiov);\n\n\t\tif (vringh_need_notify_iotlb(&cvq->vring))\n\t\t\tvringh_notify(&cvq->vring);\n\n\t\tcvq->completed_desc++;\n\t\tqueue_work(mvdev->wq, &wqent->work);\n\t\tbreak;\n\t}\n\nout:\n\tup_write(&ndev->reslock);\n}\n\nstatic void mlx5_vdpa_kick_vq(struct vdpa_device *vdev, u16 idx)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\n\tif (!is_index_valid(mvdev, idx))\n\t\treturn;\n\n\tif (unlikely(is_ctrl_vq_idx(mvdev, idx))) {\n\t\tif (!mvdev->wq || !mvdev->cvq.ready)\n\t\t\treturn;\n\n\t\tqueue_work(mvdev->wq, &ndev->cvq_ent.work);\n\t\treturn;\n\t}\n\n\tmvq = &ndev->vqs[idx];\n\tif (unlikely(!mvq->ready))\n\t\treturn;\n\n\tiowrite16(idx, ndev->mvdev.res.kick_addr);\n}\n\nstatic int mlx5_vdpa_set_vq_address(struct vdpa_device *vdev, u16 idx, u64 desc_area,\n\t\t\t\t    u64 driver_area, u64 device_area)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\n\tif (!is_index_valid(mvdev, idx))\n\t\treturn -EINVAL;\n\n\tif (is_ctrl_vq_idx(mvdev, idx)) {\n\t\tmvdev->cvq.desc_addr = desc_area;\n\t\tmvdev->cvq.device_addr = device_area;\n\t\tmvdev->cvq.driver_addr = driver_area;\n\t\treturn 0;\n\t}\n\n\tmvq = &ndev->vqs[idx];\n\tmvq->desc_addr = desc_area;\n\tmvq->device_addr = device_area;\n\tmvq->driver_addr = driver_area;\n\treturn 0;\n}\n\nstatic void mlx5_vdpa_set_vq_num(struct vdpa_device *vdev, u16 idx, u32 num)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\n\tif (!is_index_valid(mvdev, idx) || is_ctrl_vq_idx(mvdev, idx))\n\t\treturn;\n\n\tmvq = &ndev->vqs[idx];\n\tmvq->num_ent = num;\n}\n\nstatic void mlx5_vdpa_set_vq_cb(struct vdpa_device *vdev, u16 idx, struct vdpa_callback *cb)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\n\tndev->event_cbs[idx] = *cb;\n\tif (is_ctrl_vq_idx(mvdev, idx))\n\t\tmvdev->cvq.event_cb = *cb;\n}\n\nstatic void mlx5_cvq_notify(struct vringh *vring)\n{\n\tstruct mlx5_control_vq *cvq = container_of(vring, struct mlx5_control_vq, vring);\n\n\tif (!cvq->event_cb.callback)\n\t\treturn;\n\n\tcvq->event_cb.callback(cvq->event_cb.private);\n}\n\nstatic void set_cvq_ready(struct mlx5_vdpa_dev *mvdev, bool ready)\n{\n\tstruct mlx5_control_vq *cvq = &mvdev->cvq;\n\n\tcvq->ready = ready;\n\tif (!ready)\n\t\treturn;\n\n\tcvq->vring.notify = mlx5_cvq_notify;\n}\n\nstatic void mlx5_vdpa_set_vq_ready(struct vdpa_device *vdev, u16 idx, bool ready)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\tint err;\n\n\tif (!mvdev->actual_features)\n\t\treturn;\n\n\tif (!is_index_valid(mvdev, idx))\n\t\treturn;\n\n\tif (is_ctrl_vq_idx(mvdev, idx)) {\n\t\tset_cvq_ready(mvdev, ready);\n\t\treturn;\n\t}\n\n\tmvq = &ndev->vqs[idx];\n\tif (!ready) {\n\t\tsuspend_vq(ndev, mvq);\n\t} else {\n\t\terr = modify_virtqueue(ndev, mvq, MLX5_VIRTIO_NET_Q_OBJECT_STATE_RDY);\n\t\tif (err) {\n\t\t\tmlx5_vdpa_warn(mvdev, \"modify VQ %d to ready failed (%d)\\n\", idx, err);\n\t\t\tready = false;\n\t\t}\n\t}\n\n\n\tmvq->ready = ready;\n}\n\nstatic bool mlx5_vdpa_get_vq_ready(struct vdpa_device *vdev, u16 idx)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\n\tif (!is_index_valid(mvdev, idx))\n\t\treturn false;\n\n\tif (is_ctrl_vq_idx(mvdev, idx))\n\t\treturn mvdev->cvq.ready;\n\n\treturn ndev->vqs[idx].ready;\n}\n\nstatic int mlx5_vdpa_set_vq_state(struct vdpa_device *vdev, u16 idx,\n\t\t\t\t  const struct vdpa_vq_state *state)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\n\tif (!is_index_valid(mvdev, idx))\n\t\treturn -EINVAL;\n\n\tif (is_ctrl_vq_idx(mvdev, idx)) {\n\t\tmvdev->cvq.vring.last_avail_idx = state->split.avail_index;\n\t\treturn 0;\n\t}\n\n\tmvq = &ndev->vqs[idx];\n\tif (mvq->fw_state == MLX5_VIRTIO_NET_Q_OBJECT_STATE_RDY) {\n\t\tmlx5_vdpa_warn(mvdev, \"can't modify available index\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmvq->used_idx = state->split.avail_index;\n\tmvq->avail_idx = state->split.avail_index;\n\treturn 0;\n}\n\nstatic int mlx5_vdpa_get_vq_state(struct vdpa_device *vdev, u16 idx, struct vdpa_vq_state *state)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\tstruct mlx5_virtq_attr attr;\n\tint err;\n\n\tif (!is_index_valid(mvdev, idx))\n\t\treturn -EINVAL;\n\n\tif (is_ctrl_vq_idx(mvdev, idx)) {\n\t\tstate->split.avail_index = mvdev->cvq.vring.last_avail_idx;\n\t\treturn 0;\n\t}\n\n\tmvq = &ndev->vqs[idx];\n\t \n\tif (!mvq->initialized) {\n\t\t \n\t\tstate->split.avail_index = mvq->used_idx;\n\t\treturn 0;\n\t}\n\n\terr = query_virtqueue(ndev, mvq, &attr);\n\tif (err) {\n\t\tmlx5_vdpa_warn(mvdev, \"failed to query virtqueue\\n\");\n\t\treturn err;\n\t}\n\tstate->split.avail_index = attr.used_index;\n\treturn 0;\n}\n\nstatic u32 mlx5_vdpa_get_vq_align(struct vdpa_device *vdev)\n{\n\treturn PAGE_SIZE;\n}\n\nstatic u32 mlx5_vdpa_get_vq_group(struct vdpa_device *vdev, u16 idx)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\n\tif (is_ctrl_vq_idx(mvdev, idx))\n\t\treturn MLX5_VDPA_CVQ_GROUP;\n\n\treturn MLX5_VDPA_DATAVQ_GROUP;\n}\n\nstatic u64 mlx_to_vritio_features(u16 dev_features)\n{\n\tu64 result = 0;\n\n\tif (dev_features & BIT_ULL(MLX5_VIRTIO_NET_F_MRG_RXBUF))\n\t\tresult |= BIT_ULL(VIRTIO_NET_F_MRG_RXBUF);\n\tif (dev_features & BIT_ULL(MLX5_VIRTIO_NET_F_HOST_ECN))\n\t\tresult |= BIT_ULL(VIRTIO_NET_F_HOST_ECN);\n\tif (dev_features & BIT_ULL(MLX5_VIRTIO_NET_F_GUEST_ECN))\n\t\tresult |= BIT_ULL(VIRTIO_NET_F_GUEST_ECN);\n\tif (dev_features & BIT_ULL(MLX5_VIRTIO_NET_F_GUEST_TSO6))\n\t\tresult |= BIT_ULL(VIRTIO_NET_F_GUEST_TSO6);\n\tif (dev_features & BIT_ULL(MLX5_VIRTIO_NET_F_GUEST_TSO4))\n\t\tresult |= BIT_ULL(VIRTIO_NET_F_GUEST_TSO4);\n\tif (dev_features & BIT_ULL(MLX5_VIRTIO_NET_F_GUEST_CSUM))\n\t\tresult |= BIT_ULL(VIRTIO_NET_F_GUEST_CSUM);\n\tif (dev_features & BIT_ULL(MLX5_VIRTIO_NET_F_CSUM))\n\t\tresult |= BIT_ULL(VIRTIO_NET_F_CSUM);\n\tif (dev_features & BIT_ULL(MLX5_VIRTIO_NET_F_HOST_TSO6))\n\t\tresult |= BIT_ULL(VIRTIO_NET_F_HOST_TSO6);\n\tif (dev_features & BIT_ULL(MLX5_VIRTIO_NET_F_HOST_TSO4))\n\t\tresult |= BIT_ULL(VIRTIO_NET_F_HOST_TSO4);\n\n\treturn result;\n}\n\nstatic u64 get_supported_features(struct mlx5_core_dev *mdev)\n{\n\tu64 mlx_vdpa_features = 0;\n\tu16 dev_features;\n\n\tdev_features = MLX5_CAP_DEV_VDPA_EMULATION(mdev, device_features_bits_mask);\n\tmlx_vdpa_features |= mlx_to_vritio_features(dev_features);\n\tif (MLX5_CAP_DEV_VDPA_EMULATION(mdev, virtio_version_1_0))\n\t\tmlx_vdpa_features |= BIT_ULL(VIRTIO_F_VERSION_1);\n\tmlx_vdpa_features |= BIT_ULL(VIRTIO_F_ACCESS_PLATFORM);\n\tmlx_vdpa_features |= BIT_ULL(VIRTIO_NET_F_CTRL_VQ);\n\tmlx_vdpa_features |= BIT_ULL(VIRTIO_NET_F_CTRL_MAC_ADDR);\n\tmlx_vdpa_features |= BIT_ULL(VIRTIO_NET_F_MQ);\n\tmlx_vdpa_features |= BIT_ULL(VIRTIO_NET_F_STATUS);\n\tmlx_vdpa_features |= BIT_ULL(VIRTIO_NET_F_MTU);\n\tmlx_vdpa_features |= BIT_ULL(VIRTIO_NET_F_CTRL_VLAN);\n\tmlx_vdpa_features |= BIT_ULL(VIRTIO_NET_F_MAC);\n\n\treturn mlx_vdpa_features;\n}\n\nstatic u64 mlx5_vdpa_get_device_features(struct vdpa_device *vdev)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\n\tprint_features(mvdev, ndev->mvdev.mlx_features, false);\n\treturn ndev->mvdev.mlx_features;\n}\n\nstatic int verify_driver_features(struct mlx5_vdpa_dev *mvdev, u64 features)\n{\n\t \n\tif (!(features & BIT_ULL(VIRTIO_F_ACCESS_PLATFORM)))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif ((features & (BIT_ULL(VIRTIO_NET_F_MQ) | BIT_ULL(VIRTIO_NET_F_CTRL_VQ))) ==\n            BIT_ULL(VIRTIO_NET_F_MQ))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int setup_virtqueues(struct mlx5_vdpa_dev *mvdev)\n{\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tint err;\n\tint i;\n\n\tfor (i = 0; i < mvdev->max_vqs; i++) {\n\t\terr = setup_vq(ndev, &ndev->vqs[i]);\n\t\tif (err)\n\t\t\tgoto err_vq;\n\t}\n\n\treturn 0;\n\nerr_vq:\n\tfor (--i; i >= 0; i--)\n\t\tteardown_vq(ndev, &ndev->vqs[i]);\n\n\treturn err;\n}\n\nstatic void teardown_virtqueues(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\tint i;\n\n\tfor (i = ndev->mvdev.max_vqs - 1; i >= 0; i--) {\n\t\tmvq = &ndev->vqs[i];\n\t\tif (!mvq->initialized)\n\t\t\tcontinue;\n\n\t\tteardown_vq(ndev, mvq);\n\t}\n}\n\nstatic void update_cvq_info(struct mlx5_vdpa_dev *mvdev)\n{\n\tif (MLX5_FEATURE(mvdev, VIRTIO_NET_F_CTRL_VQ)) {\n\t\tif (MLX5_FEATURE(mvdev, VIRTIO_NET_F_MQ)) {\n\t\t\t \n\t\t\tmvdev->max_idx = mvdev->max_vqs;\n\t\t} else {\n\t\t\t \n\t\t\tmvdev->max_idx = 2;\n\t\t}\n\t} else {\n\t\t \n\t\tmvdev->max_idx = 1;\n\t}\n}\n\nstatic u8 query_vport_state(struct mlx5_core_dev *mdev, u8 opmod, u16 vport)\n{\n\tu32 out[MLX5_ST_SZ_DW(query_vport_state_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(query_vport_state_in)] = {};\n\tint err;\n\n\tMLX5_SET(query_vport_state_in, in, opcode, MLX5_CMD_OP_QUERY_VPORT_STATE);\n\tMLX5_SET(query_vport_state_in, in, op_mod, opmod);\n\tMLX5_SET(query_vport_state_in, in, vport_number, vport);\n\tif (vport)\n\t\tMLX5_SET(query_vport_state_in, in, other_vport, 1);\n\n\terr = mlx5_cmd_exec_inout(mdev, query_vport_state, in, out);\n\tif (err)\n\t\treturn 0;\n\n\treturn MLX5_GET(query_vport_state_out, out, state);\n}\n\nstatic bool get_link_state(struct mlx5_vdpa_dev *mvdev)\n{\n\tif (query_vport_state(mvdev->mdev, MLX5_VPORT_STATE_OP_MOD_VNIC_VPORT, 0) ==\n\t    VPORT_STATE_UP)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void update_carrier(struct work_struct *work)\n{\n\tstruct mlx5_vdpa_wq_ent *wqent;\n\tstruct mlx5_vdpa_dev *mvdev;\n\tstruct mlx5_vdpa_net *ndev;\n\n\twqent = container_of(work, struct mlx5_vdpa_wq_ent, work);\n\tmvdev = wqent->mvdev;\n\tndev = to_mlx5_vdpa_ndev(mvdev);\n\tif (get_link_state(mvdev))\n\t\tndev->config.status |= cpu_to_mlx5vdpa16(mvdev, VIRTIO_NET_S_LINK_UP);\n\telse\n\t\tndev->config.status &= cpu_to_mlx5vdpa16(mvdev, ~VIRTIO_NET_S_LINK_UP);\n\n\tif (ndev->config_cb.callback)\n\t\tndev->config_cb.callback(ndev->config_cb.private);\n\n\tkfree(wqent);\n}\n\nstatic int queue_link_work(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_vdpa_wq_ent *wqent;\n\n\twqent = kzalloc(sizeof(*wqent), GFP_ATOMIC);\n\tif (!wqent)\n\t\treturn -ENOMEM;\n\n\twqent->mvdev = &ndev->mvdev;\n\tINIT_WORK(&wqent->work, update_carrier);\n\tqueue_work(ndev->mvdev.wq, &wqent->work);\n\treturn 0;\n}\n\nstatic int event_handler(struct notifier_block *nb, unsigned long event, void *param)\n{\n\tstruct mlx5_vdpa_net *ndev = container_of(nb, struct mlx5_vdpa_net, nb);\n\tstruct mlx5_eqe *eqe = param;\n\tint ret = NOTIFY_DONE;\n\n\tif (event == MLX5_EVENT_TYPE_PORT_CHANGE) {\n\t\tswitch (eqe->sub_type) {\n\t\tcase MLX5_PORT_CHANGE_SUBTYPE_DOWN:\n\t\tcase MLX5_PORT_CHANGE_SUBTYPE_ACTIVE:\n\t\t\tif (queue_link_work(ndev))\n\t\t\t\treturn NOTIFY_DONE;\n\n\t\t\tret = NOTIFY_OK;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn NOTIFY_DONE;\n\t\t}\n\t\treturn ret;\n\t}\n\treturn ret;\n}\n\nstatic void register_link_notifier(struct mlx5_vdpa_net *ndev)\n{\n\tif (!(ndev->mvdev.actual_features & BIT_ULL(VIRTIO_NET_F_STATUS)))\n\t\treturn;\n\n\tndev->nb.notifier_call = event_handler;\n\tmlx5_notifier_register(ndev->mvdev.mdev, &ndev->nb);\n\tndev->nb_registered = true;\n\tqueue_link_work(ndev);\n}\n\nstatic void unregister_link_notifier(struct mlx5_vdpa_net *ndev)\n{\n\tif (!ndev->nb_registered)\n\t\treturn;\n\n\tndev->nb_registered = false;\n\tmlx5_notifier_unregister(ndev->mvdev.mdev, &ndev->nb);\n\tif (ndev->mvdev.wq)\n\t\tflush_workqueue(ndev->mvdev.wq);\n}\n\nstatic int mlx5_vdpa_set_driver_features(struct vdpa_device *vdev, u64 features)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tint err;\n\n\tprint_features(mvdev, features, true);\n\n\terr = verify_driver_features(mvdev, features);\n\tif (err)\n\t\treturn err;\n\n\tndev->mvdev.actual_features = features & ndev->mvdev.mlx_features;\n\tif (ndev->mvdev.actual_features & BIT_ULL(VIRTIO_NET_F_MQ))\n\t\tndev->rqt_size = mlx5vdpa16_to_cpu(mvdev, ndev->config.max_virtqueue_pairs);\n\telse\n\t\tndev->rqt_size = 1;\n\n\t \n\tndev->cur_num_vqs = 2;\n\n\tupdate_cvq_info(mvdev);\n\treturn err;\n}\n\nstatic void mlx5_vdpa_set_config_cb(struct vdpa_device *vdev, struct vdpa_callback *cb)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\n\tndev->config_cb = *cb;\n}\n\n#define MLX5_VDPA_MAX_VQ_ENTRIES 256\nstatic u16 mlx5_vdpa_get_vq_num_max(struct vdpa_device *vdev)\n{\n\treturn MLX5_VDPA_MAX_VQ_ENTRIES;\n}\n\nstatic u32 mlx5_vdpa_get_device_id(struct vdpa_device *vdev)\n{\n\treturn VIRTIO_ID_NET;\n}\n\nstatic u32 mlx5_vdpa_get_vendor_id(struct vdpa_device *vdev)\n{\n\treturn PCI_VENDOR_ID_MELLANOX;\n}\n\nstatic u8 mlx5_vdpa_get_status(struct vdpa_device *vdev)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\n\tprint_status(mvdev, ndev->mvdev.status, false);\n\treturn ndev->mvdev.status;\n}\n\nstatic int save_channel_info(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq)\n{\n\tstruct mlx5_vq_restore_info *ri = &mvq->ri;\n\tstruct mlx5_virtq_attr attr = {};\n\tint err;\n\n\tif (mvq->initialized) {\n\t\terr = query_virtqueue(ndev, mvq, &attr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tri->avail_index = attr.available_index;\n\tri->used_index = attr.used_index;\n\tri->ready = mvq->ready;\n\tri->num_ent = mvq->num_ent;\n\tri->desc_addr = mvq->desc_addr;\n\tri->device_addr = mvq->device_addr;\n\tri->driver_addr = mvq->driver_addr;\n\tri->map = mvq->map;\n\tri->restore = true;\n\treturn 0;\n}\n\nstatic int save_channels_info(struct mlx5_vdpa_net *ndev)\n{\n\tint i;\n\n\tfor (i = 0; i < ndev->mvdev.max_vqs; i++) {\n\t\tmemset(&ndev->vqs[i].ri, 0, sizeof(ndev->vqs[i].ri));\n\t\tsave_channel_info(ndev, &ndev->vqs[i]);\n\t}\n\treturn 0;\n}\n\nstatic void mlx5_clear_vqs(struct mlx5_vdpa_net *ndev)\n{\n\tint i;\n\n\tfor (i = 0; i < ndev->mvdev.max_vqs; i++)\n\t\tmemset(&ndev->vqs[i], 0, offsetof(struct mlx5_vdpa_virtqueue, ri));\n}\n\nstatic void restore_channels_info(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\tstruct mlx5_vq_restore_info *ri;\n\tint i;\n\n\tmlx5_clear_vqs(ndev);\n\tinit_mvqs(ndev);\n\tfor (i = 0; i < ndev->mvdev.max_vqs; i++) {\n\t\tmvq = &ndev->vqs[i];\n\t\tri = &mvq->ri;\n\t\tif (!ri->restore)\n\t\t\tcontinue;\n\n\t\tmvq->avail_idx = ri->avail_index;\n\t\tmvq->used_idx = ri->used_index;\n\t\tmvq->ready = ri->ready;\n\t\tmvq->num_ent = ri->num_ent;\n\t\tmvq->desc_addr = ri->desc_addr;\n\t\tmvq->device_addr = ri->device_addr;\n\t\tmvq->driver_addr = ri->driver_addr;\n\t\tmvq->map = ri->map;\n\t}\n}\n\nstatic int mlx5_vdpa_change_map(struct mlx5_vdpa_dev *mvdev,\n\t\t\t\tstruct vhost_iotlb *iotlb, unsigned int asid)\n{\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tint err;\n\n\tsuspend_vqs(ndev);\n\terr = save_channels_info(ndev);\n\tif (err)\n\t\tgoto err_mr;\n\n\tteardown_driver(ndev);\n\tmlx5_vdpa_destroy_mr_asid(mvdev, asid);\n\terr = mlx5_vdpa_create_mr(mvdev, iotlb, asid);\n\tif (err)\n\t\tgoto err_mr;\n\n\tif (!(mvdev->status & VIRTIO_CONFIG_S_DRIVER_OK) || mvdev->suspended)\n\t\tgoto err_mr;\n\n\trestore_channels_info(ndev);\n\terr = setup_driver(mvdev);\n\tif (err)\n\t\tgoto err_setup;\n\n\treturn 0;\n\nerr_setup:\n\tmlx5_vdpa_destroy_mr_asid(mvdev, asid);\nerr_mr:\n\treturn err;\n}\n\n \nstatic int setup_driver(struct mlx5_vdpa_dev *mvdev)\n{\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tint err;\n\n\tWARN_ON(!rwsem_is_locked(&ndev->reslock));\n\n\tif (ndev->setup) {\n\t\tmlx5_vdpa_warn(mvdev, \"setup driver called for already setup driver\\n\");\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\tmlx5_vdpa_add_debugfs(ndev);\n\n\terr = read_umem_params(ndev);\n\tif (err)\n\t\tgoto err_setup;\n\n\terr = setup_virtqueues(mvdev);\n\tif (err) {\n\t\tmlx5_vdpa_warn(mvdev, \"setup_virtqueues\\n\");\n\t\tgoto err_setup;\n\t}\n\n\terr = create_rqt(ndev);\n\tif (err) {\n\t\tmlx5_vdpa_warn(mvdev, \"create_rqt\\n\");\n\t\tgoto err_rqt;\n\t}\n\n\terr = create_tir(ndev);\n\tif (err) {\n\t\tmlx5_vdpa_warn(mvdev, \"create_tir\\n\");\n\t\tgoto err_tir;\n\t}\n\n\terr = setup_steering(ndev);\n\tif (err) {\n\t\tmlx5_vdpa_warn(mvdev, \"setup_steering\\n\");\n\t\tgoto err_fwd;\n\t}\n\tndev->setup = true;\n\n\treturn 0;\n\nerr_fwd:\n\tdestroy_tir(ndev);\nerr_tir:\n\tdestroy_rqt(ndev);\nerr_rqt:\n\tteardown_virtqueues(ndev);\nerr_setup:\n\tmlx5_vdpa_remove_debugfs(ndev);\nout:\n\treturn err;\n}\n\n \nstatic void teardown_driver(struct mlx5_vdpa_net *ndev)\n{\n\n\tWARN_ON(!rwsem_is_locked(&ndev->reslock));\n\n\tif (!ndev->setup)\n\t\treturn;\n\n\tmlx5_vdpa_remove_debugfs(ndev);\n\tteardown_steering(ndev);\n\tdestroy_tir(ndev);\n\tdestroy_rqt(ndev);\n\tteardown_virtqueues(ndev);\n\tndev->setup = false;\n}\n\nstatic void clear_vqs_ready(struct mlx5_vdpa_net *ndev)\n{\n\tint i;\n\n\tfor (i = 0; i < ndev->mvdev.max_vqs; i++)\n\t\tndev->vqs[i].ready = false;\n\n\tndev->mvdev.cvq.ready = false;\n}\n\nstatic int setup_cvq_vring(struct mlx5_vdpa_dev *mvdev)\n{\n\tstruct mlx5_control_vq *cvq = &mvdev->cvq;\n\tint err = 0;\n\n\tif (mvdev->actual_features & BIT_ULL(VIRTIO_NET_F_CTRL_VQ)) {\n\t\tu16 idx = cvq->vring.last_avail_idx;\n\n\t\terr = vringh_init_iotlb(&cvq->vring, mvdev->actual_features,\n\t\t\t\t\tMLX5_CVQ_MAX_ENT, false,\n\t\t\t\t\t(struct vring_desc *)(uintptr_t)cvq->desc_addr,\n\t\t\t\t\t(struct vring_avail *)(uintptr_t)cvq->driver_addr,\n\t\t\t\t\t(struct vring_used *)(uintptr_t)cvq->device_addr);\n\n\t\tif (!err)\n\t\t\tcvq->vring.last_avail_idx = cvq->vring.last_used_idx = idx;\n\t}\n\treturn err;\n}\n\nstatic void mlx5_vdpa_set_status(struct vdpa_device *vdev, u8 status)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tint err;\n\n\tprint_status(mvdev, status, true);\n\n\tdown_write(&ndev->reslock);\n\n\tif ((status ^ ndev->mvdev.status) & VIRTIO_CONFIG_S_DRIVER_OK) {\n\t\tif (status & VIRTIO_CONFIG_S_DRIVER_OK) {\n\t\t\terr = setup_cvq_vring(mvdev);\n\t\t\tif (err) {\n\t\t\t\tmlx5_vdpa_warn(mvdev, \"failed to setup control VQ vring\\n\");\n\t\t\t\tgoto err_setup;\n\t\t\t}\n\t\t\tregister_link_notifier(ndev);\n\t\t\terr = setup_driver(mvdev);\n\t\t\tif (err) {\n\t\t\t\tmlx5_vdpa_warn(mvdev, \"failed to setup driver\\n\");\n\t\t\t\tgoto err_driver;\n\t\t\t}\n\t\t} else {\n\t\t\tmlx5_vdpa_warn(mvdev, \"did not expect DRIVER_OK to be cleared\\n\");\n\t\t\tgoto err_clear;\n\t\t}\n\t}\n\n\tndev->mvdev.status = status;\n\tup_write(&ndev->reslock);\n\treturn;\n\nerr_driver:\n\tunregister_link_notifier(ndev);\nerr_setup:\n\tmlx5_vdpa_destroy_mr(&ndev->mvdev);\n\tndev->mvdev.status |= VIRTIO_CONFIG_S_FAILED;\nerr_clear:\n\tup_write(&ndev->reslock);\n}\n\nstatic void init_group_to_asid_map(struct mlx5_vdpa_dev *mvdev)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < MLX5_VDPA_NUMVQ_GROUPS; i++)\n\t\tmvdev->group2asid[i] = 0;\n}\n\nstatic int mlx5_vdpa_reset(struct vdpa_device *vdev)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\n\tprint_status(mvdev, 0, true);\n\tmlx5_vdpa_info(mvdev, \"performing device reset\\n\");\n\n\tdown_write(&ndev->reslock);\n\tunregister_link_notifier(ndev);\n\tteardown_driver(ndev);\n\tclear_vqs_ready(ndev);\n\tmlx5_vdpa_destroy_mr(&ndev->mvdev);\n\tndev->mvdev.status = 0;\n\tndev->mvdev.suspended = false;\n\tndev->cur_num_vqs = 0;\n\tndev->mvdev.cvq.received_desc = 0;\n\tndev->mvdev.cvq.completed_desc = 0;\n\tmemset(ndev->event_cbs, 0, sizeof(*ndev->event_cbs) * (mvdev->max_vqs + 1));\n\tndev->mvdev.actual_features = 0;\n\tinit_group_to_asid_map(mvdev);\n\t++mvdev->generation;\n\n\tif (MLX5_CAP_GEN(mvdev->mdev, umem_uid_0)) {\n\t\tif (mlx5_vdpa_create_mr(mvdev, NULL, 0))\n\t\t\tmlx5_vdpa_warn(mvdev, \"create MR failed\\n\");\n\t}\n\tup_write(&ndev->reslock);\n\n\treturn 0;\n}\n\nstatic size_t mlx5_vdpa_get_config_size(struct vdpa_device *vdev)\n{\n\treturn sizeof(struct virtio_net_config);\n}\n\nstatic void mlx5_vdpa_get_config(struct vdpa_device *vdev, unsigned int offset, void *buf,\n\t\t\t\t unsigned int len)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\n\tif (offset + len <= sizeof(struct virtio_net_config))\n\t\tmemcpy(buf, (u8 *)&ndev->config + offset, len);\n}\n\nstatic void mlx5_vdpa_set_config(struct vdpa_device *vdev, unsigned int offset, const void *buf,\n\t\t\t\t unsigned int len)\n{\n\t \n}\n\nstatic u32 mlx5_vdpa_get_generation(struct vdpa_device *vdev)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\n\treturn mvdev->generation;\n}\n\nstatic int set_map_data(struct mlx5_vdpa_dev *mvdev, struct vhost_iotlb *iotlb,\n\t\t\tunsigned int asid)\n{\n\tbool change_map;\n\tint err;\n\n\terr = mlx5_vdpa_handle_set_map(mvdev, iotlb, &change_map, asid);\n\tif (err) {\n\t\tmlx5_vdpa_warn(mvdev, \"set map failed(%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\tif (change_map)\n\t\terr = mlx5_vdpa_change_map(mvdev, iotlb, asid);\n\n\treturn err;\n}\n\nstatic int mlx5_vdpa_set_map(struct vdpa_device *vdev, unsigned int asid,\n\t\t\t     struct vhost_iotlb *iotlb)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tint err = -EINVAL;\n\n\tdown_write(&ndev->reslock);\n\terr = set_map_data(mvdev, iotlb, asid);\n\tup_write(&ndev->reslock);\n\treturn err;\n}\n\nstatic struct device *mlx5_get_vq_dma_dev(struct vdpa_device *vdev, u16 idx)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\n\tif (is_ctrl_vq_idx(mvdev, idx))\n\t\treturn &vdev->dev;\n\n\treturn mvdev->vdev.dma_dev;\n}\n\nstatic void free_irqs(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_vdpa_irq_pool_entry *ent;\n\tint i;\n\n\tif (!msix_mode_supported(&ndev->mvdev))\n\t\treturn;\n\n\tif (!ndev->irqp.entries)\n\t\treturn;\n\n\tfor (i = ndev->irqp.num_ent - 1; i >= 0; i--) {\n\t\tent = ndev->irqp.entries + i;\n\t\tif (ent->map.virq)\n\t\t\tpci_msix_free_irq(ndev->mvdev.mdev->pdev, ent->map);\n\t}\n\tkfree(ndev->irqp.entries);\n}\n\nstatic void mlx5_vdpa_free(struct vdpa_device *vdev)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_core_dev *pfmdev;\n\tstruct mlx5_vdpa_net *ndev;\n\n\tndev = to_mlx5_vdpa_ndev(mvdev);\n\n\tfree_resources(ndev);\n\tmlx5_vdpa_destroy_mr(mvdev);\n\tif (!is_zero_ether_addr(ndev->config.mac)) {\n\t\tpfmdev = pci_get_drvdata(pci_physfn(mvdev->mdev->pdev));\n\t\tmlx5_mpfs_del_mac(pfmdev, ndev->config.mac);\n\t}\n\tmlx5_vdpa_free_resources(&ndev->mvdev);\n\tfree_irqs(ndev);\n\tkfree(ndev->event_cbs);\n\tkfree(ndev->vqs);\n}\n\nstatic struct vdpa_notification_area mlx5_get_vq_notification(struct vdpa_device *vdev, u16 idx)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct vdpa_notification_area ret = {};\n\tstruct mlx5_vdpa_net *ndev;\n\tphys_addr_t addr;\n\n\tif (!is_index_valid(mvdev, idx) || is_ctrl_vq_idx(mvdev, idx))\n\t\treturn ret;\n\n\t \n\tif (MLX5_CAP_GEN(mvdev->mdev, log_min_sf_size) + 12 < PAGE_SHIFT)\n\t\treturn ret;\n\n\tndev = to_mlx5_vdpa_ndev(mvdev);\n\taddr = (phys_addr_t)ndev->mvdev.res.phys_kick_addr;\n\tret.addr = addr;\n\tret.size = PAGE_SIZE;\n\treturn ret;\n}\n\nstatic int mlx5_get_vq_irq(struct vdpa_device *vdev, u16 idx)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\n\tif (!is_index_valid(mvdev, idx))\n\t\treturn -EINVAL;\n\n\tif (is_ctrl_vq_idx(mvdev, idx))\n\t\treturn -EOPNOTSUPP;\n\n\tmvq = &ndev->vqs[idx];\n\tif (!mvq->map.virq)\n\t\treturn -EOPNOTSUPP;\n\n\treturn mvq->map.virq;\n}\n\nstatic u64 mlx5_vdpa_get_driver_features(struct vdpa_device *vdev)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\n\treturn mvdev->actual_features;\n}\n\nstatic int counter_set_query(struct mlx5_vdpa_net *ndev, struct mlx5_vdpa_virtqueue *mvq,\n\t\t\t     u64 *received_desc, u64 *completed_desc)\n{\n\tu32 in[MLX5_ST_SZ_DW(query_virtio_q_counters_in)] = {};\n\tu32 out[MLX5_ST_SZ_DW(query_virtio_q_counters_out)] = {};\n\tvoid *cmd_hdr;\n\tvoid *ctx;\n\tint err;\n\n\tif (!counters_supported(&ndev->mvdev))\n\t\treturn -EOPNOTSUPP;\n\n\tif (mvq->fw_state != MLX5_VIRTIO_NET_Q_OBJECT_STATE_RDY)\n\t\treturn -EAGAIN;\n\n\tcmd_hdr = MLX5_ADDR_OF(query_virtio_q_counters_in, in, hdr);\n\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, opcode, MLX5_CMD_OP_QUERY_GENERAL_OBJECT);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_type, MLX5_OBJ_TYPE_VIRTIO_Q_COUNTERS);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, uid, ndev->mvdev.res.uid);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_id, mvq->counter_set_id);\n\n\terr = mlx5_cmd_exec(ndev->mvdev.mdev, in, sizeof(in), out, sizeof(out));\n\tif (err)\n\t\treturn err;\n\n\tctx = MLX5_ADDR_OF(query_virtio_q_counters_out, out, counters);\n\t*received_desc = MLX5_GET64(virtio_q_counters, ctx, received_desc);\n\t*completed_desc = MLX5_GET64(virtio_q_counters, ctx, completed_desc);\n\treturn 0;\n}\n\nstatic int mlx5_vdpa_get_vendor_vq_stats(struct vdpa_device *vdev, u16 idx,\n\t\t\t\t\t struct sk_buff *msg,\n\t\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\tstruct mlx5_control_vq *cvq;\n\tu64 received_desc;\n\tu64 completed_desc;\n\tint err = 0;\n\n\tdown_read(&ndev->reslock);\n\tif (!is_index_valid(mvdev, idx)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"virtqueue index is not valid\");\n\t\terr = -EINVAL;\n\t\tgoto out_err;\n\t}\n\n\tif (idx == ctrl_vq_idx(mvdev)) {\n\t\tcvq = &mvdev->cvq;\n\t\treceived_desc = cvq->received_desc;\n\t\tcompleted_desc = cvq->completed_desc;\n\t\tgoto out;\n\t}\n\n\tmvq = &ndev->vqs[idx];\n\terr = counter_set_query(ndev, mvq, &received_desc, &completed_desc);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"failed to query hardware\");\n\t\tgoto out_err;\n\t}\n\nout:\n\terr = -EMSGSIZE;\n\tif (nla_put_string(msg, VDPA_ATTR_DEV_VENDOR_ATTR_NAME, \"received_desc\"))\n\t\tgoto out_err;\n\n\tif (nla_put_u64_64bit(msg, VDPA_ATTR_DEV_VENDOR_ATTR_VALUE, received_desc,\n\t\t\t      VDPA_ATTR_PAD))\n\t\tgoto out_err;\n\n\tif (nla_put_string(msg, VDPA_ATTR_DEV_VENDOR_ATTR_NAME, \"completed_desc\"))\n\t\tgoto out_err;\n\n\tif (nla_put_u64_64bit(msg, VDPA_ATTR_DEV_VENDOR_ATTR_VALUE, completed_desc,\n\t\t\t      VDPA_ATTR_PAD))\n\t\tgoto out_err;\n\n\terr = 0;\nout_err:\n\tup_read(&ndev->reslock);\n\treturn err;\n}\n\nstatic void mlx5_vdpa_cvq_suspend(struct mlx5_vdpa_dev *mvdev)\n{\n\tstruct mlx5_control_vq *cvq;\n\n\tif (!(mvdev->actual_features & BIT_ULL(VIRTIO_NET_F_CTRL_VQ)))\n\t\treturn;\n\n\tcvq = &mvdev->cvq;\n\tcvq->ready = false;\n}\n\nstatic int mlx5_vdpa_suspend(struct vdpa_device *vdev)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\tint i;\n\n\tmlx5_vdpa_info(mvdev, \"suspending device\\n\");\n\n\tdown_write(&ndev->reslock);\n\tunregister_link_notifier(ndev);\n\tfor (i = 0; i < ndev->cur_num_vqs; i++) {\n\t\tmvq = &ndev->vqs[i];\n\t\tsuspend_vq(ndev, mvq);\n\t}\n\tmlx5_vdpa_cvq_suspend(mvdev);\n\tmvdev->suspended = true;\n\tup_write(&ndev->reslock);\n\treturn 0;\n}\n\nstatic int mlx5_set_group_asid(struct vdpa_device *vdev, u32 group,\n\t\t\t       unsigned int asid)\n{\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);\n\n\tif (group >= MLX5_VDPA_NUMVQ_GROUPS)\n\t\treturn -EINVAL;\n\n\tmvdev->group2asid[group] = asid;\n\treturn 0;\n}\n\nstatic const struct vdpa_config_ops mlx5_vdpa_ops = {\n\t.set_vq_address = mlx5_vdpa_set_vq_address,\n\t.set_vq_num = mlx5_vdpa_set_vq_num,\n\t.kick_vq = mlx5_vdpa_kick_vq,\n\t.set_vq_cb = mlx5_vdpa_set_vq_cb,\n\t.set_vq_ready = mlx5_vdpa_set_vq_ready,\n\t.get_vq_ready = mlx5_vdpa_get_vq_ready,\n\t.set_vq_state = mlx5_vdpa_set_vq_state,\n\t.get_vq_state = mlx5_vdpa_get_vq_state,\n\t.get_vendor_vq_stats = mlx5_vdpa_get_vendor_vq_stats,\n\t.get_vq_notification = mlx5_get_vq_notification,\n\t.get_vq_irq = mlx5_get_vq_irq,\n\t.get_vq_align = mlx5_vdpa_get_vq_align,\n\t.get_vq_group = mlx5_vdpa_get_vq_group,\n\t.get_device_features = mlx5_vdpa_get_device_features,\n\t.set_driver_features = mlx5_vdpa_set_driver_features,\n\t.get_driver_features = mlx5_vdpa_get_driver_features,\n\t.set_config_cb = mlx5_vdpa_set_config_cb,\n\t.get_vq_num_max = mlx5_vdpa_get_vq_num_max,\n\t.get_device_id = mlx5_vdpa_get_device_id,\n\t.get_vendor_id = mlx5_vdpa_get_vendor_id,\n\t.get_status = mlx5_vdpa_get_status,\n\t.set_status = mlx5_vdpa_set_status,\n\t.reset = mlx5_vdpa_reset,\n\t.get_config_size = mlx5_vdpa_get_config_size,\n\t.get_config = mlx5_vdpa_get_config,\n\t.set_config = mlx5_vdpa_set_config,\n\t.get_generation = mlx5_vdpa_get_generation,\n\t.set_map = mlx5_vdpa_set_map,\n\t.set_group_asid = mlx5_set_group_asid,\n\t.get_vq_dma_dev = mlx5_get_vq_dma_dev,\n\t.free = mlx5_vdpa_free,\n\t.suspend = mlx5_vdpa_suspend,\n};\n\nstatic int query_mtu(struct mlx5_core_dev *mdev, u16 *mtu)\n{\n\tu16 hw_mtu;\n\tint err;\n\n\terr = mlx5_query_nic_vport_mtu(mdev, &hw_mtu);\n\tif (err)\n\t\treturn err;\n\n\t*mtu = hw_mtu - MLX5V_ETH_HARD_MTU;\n\treturn 0;\n}\n\nstatic int alloc_resources(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_vdpa_net_resources *res = &ndev->res;\n\tint err;\n\n\tif (res->valid) {\n\t\tmlx5_vdpa_warn(&ndev->mvdev, \"resources already allocated\\n\");\n\t\treturn -EEXIST;\n\t}\n\n\terr = mlx5_vdpa_alloc_transport_domain(&ndev->mvdev, &res->tdn);\n\tif (err)\n\t\treturn err;\n\n\terr = create_tis(ndev);\n\tif (err)\n\t\tgoto err_tis;\n\n\tres->valid = true;\n\n\treturn 0;\n\nerr_tis:\n\tmlx5_vdpa_dealloc_transport_domain(&ndev->mvdev, res->tdn);\n\treturn err;\n}\n\nstatic void free_resources(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_vdpa_net_resources *res = &ndev->res;\n\n\tif (!res->valid)\n\t\treturn;\n\n\tdestroy_tis(ndev);\n\tmlx5_vdpa_dealloc_transport_domain(&ndev->mvdev, res->tdn);\n\tres->valid = false;\n}\n\nstatic void init_mvqs(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_vdpa_virtqueue *mvq;\n\tint i;\n\n\tfor (i = 0; i < ndev->mvdev.max_vqs; ++i) {\n\t\tmvq = &ndev->vqs[i];\n\t\tmemset(mvq, 0, offsetof(struct mlx5_vdpa_virtqueue, ri));\n\t\tmvq->index = i;\n\t\tmvq->ndev = ndev;\n\t\tmvq->fwqp.fw = true;\n\t\tmvq->fw_state = MLX5_VIRTIO_NET_Q_OBJECT_NONE;\n\t}\n\tfor (; i < ndev->mvdev.max_vqs; i++) {\n\t\tmvq = &ndev->vqs[i];\n\t\tmemset(mvq, 0, offsetof(struct mlx5_vdpa_virtqueue, ri));\n\t\tmvq->index = i;\n\t\tmvq->ndev = ndev;\n\t}\n}\n\nstruct mlx5_vdpa_mgmtdev {\n\tstruct vdpa_mgmt_dev mgtdev;\n\tstruct mlx5_adev *madev;\n\tstruct mlx5_vdpa_net *ndev;\n};\n\nstatic int config_func_mtu(struct mlx5_core_dev *mdev, u16 mtu)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(modify_nic_vport_context_in);\n\tvoid *in;\n\tint err;\n\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(modify_nic_vport_context_in, in, field_select.mtu, 1);\n\tMLX5_SET(modify_nic_vport_context_in, in, nic_vport_context.mtu,\n\t\t mtu + MLX5V_ETH_HARD_MTU);\n\tMLX5_SET(modify_nic_vport_context_in, in, opcode,\n\t\t MLX5_CMD_OP_MODIFY_NIC_VPORT_CONTEXT);\n\n\terr = mlx5_cmd_exec_in(mdev, modify_nic_vport_context, in);\n\n\tkvfree(in);\n\treturn err;\n}\n\nstatic void allocate_irqs(struct mlx5_vdpa_net *ndev)\n{\n\tstruct mlx5_vdpa_irq_pool_entry *ent;\n\tint i;\n\n\tif (!msix_mode_supported(&ndev->mvdev))\n\t\treturn;\n\n\tif (!ndev->mvdev.mdev->pdev)\n\t\treturn;\n\n\tndev->irqp.entries = kcalloc(ndev->mvdev.max_vqs, sizeof(*ndev->irqp.entries), GFP_KERNEL);\n\tif (!ndev->irqp.entries)\n\t\treturn;\n\n\n\tfor (i = 0; i < ndev->mvdev.max_vqs; i++) {\n\t\tent = ndev->irqp.entries + i;\n\t\tsnprintf(ent->name, MLX5_VDPA_IRQ_NAME_LEN, \"%s-vq-%d\",\n\t\t\t dev_name(&ndev->mvdev.vdev.dev), i);\n\t\tent->map = pci_msix_alloc_irq_at(ndev->mvdev.mdev->pdev, MSI_ANY_INDEX, NULL);\n\t\tif (!ent->map.virq)\n\t\t\treturn;\n\n\t\tndev->irqp.num_ent++;\n\t}\n}\n\nstatic int mlx5_vdpa_dev_add(struct vdpa_mgmt_dev *v_mdev, const char *name,\n\t\t\t     const struct vdpa_dev_set_config *add_config)\n{\n\tstruct mlx5_vdpa_mgmtdev *mgtdev = container_of(v_mdev, struct mlx5_vdpa_mgmtdev, mgtdev);\n\tstruct virtio_net_config *config;\n\tstruct mlx5_core_dev *pfmdev;\n\tstruct mlx5_vdpa_dev *mvdev;\n\tstruct mlx5_vdpa_net *ndev;\n\tstruct mlx5_core_dev *mdev;\n\tu64 device_features;\n\tu32 max_vqs;\n\tu16 mtu;\n\tint err;\n\n\tif (mgtdev->ndev)\n\t\treturn -ENOSPC;\n\n\tmdev = mgtdev->madev->mdev;\n\tdevice_features = mgtdev->mgtdev.supported_features;\n\tif (add_config->mask & BIT_ULL(VDPA_ATTR_DEV_FEATURES)) {\n\t\tif (add_config->device_features & ~device_features) {\n\t\t\tdev_warn(mdev->device,\n\t\t\t\t \"The provisioned features 0x%llx are not supported by this device with features 0x%llx\\n\",\n\t\t\t\t add_config->device_features, device_features);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdevice_features &= add_config->device_features;\n\t} else {\n\t\tdevice_features &= ~BIT_ULL(VIRTIO_NET_F_MRG_RXBUF);\n\t}\n\tif (!(device_features & BIT_ULL(VIRTIO_F_VERSION_1) &&\n\t      device_features & BIT_ULL(VIRTIO_F_ACCESS_PLATFORM))) {\n\t\tdev_warn(mdev->device,\n\t\t\t \"Must provision minimum features 0x%llx for this device\",\n\t\t\t BIT_ULL(VIRTIO_F_VERSION_1) | BIT_ULL(VIRTIO_F_ACCESS_PLATFORM));\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!(MLX5_CAP_DEV_VDPA_EMULATION(mdev, virtio_queue_type) &\n\t    MLX5_VIRTIO_EMULATION_CAP_VIRTIO_QUEUE_TYPE_SPLIT)) {\n\t\tdev_warn(mdev->device, \"missing support for split virtqueues\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmax_vqs = min_t(int, MLX5_CAP_DEV_VDPA_EMULATION(mdev, max_num_virtio_queues),\n\t\t\t1 << MLX5_CAP_GEN(mdev, log_max_rqt_size));\n\tif (max_vqs < 2) {\n\t\tdev_warn(mdev->device,\n\t\t\t \"%d virtqueues are supported. At least 2 are required\\n\",\n\t\t\t max_vqs);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (add_config->mask & BIT_ULL(VDPA_ATTR_DEV_NET_CFG_MAX_VQP)) {\n\t\tif (add_config->net.max_vq_pairs > max_vqs / 2)\n\t\t\treturn -EINVAL;\n\t\tmax_vqs = min_t(u32, max_vqs, 2 * add_config->net.max_vq_pairs);\n\t} else {\n\t\tmax_vqs = 2;\n\t}\n\n\tndev = vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mlx5_vdpa_ops,\n\t\t\t\t MLX5_VDPA_NUMVQ_GROUPS, MLX5_VDPA_NUM_AS, name, false);\n\tif (IS_ERR(ndev))\n\t\treturn PTR_ERR(ndev);\n\n\tndev->mvdev.max_vqs = max_vqs;\n\tmvdev = &ndev->mvdev;\n\tmvdev->mdev = mdev;\n\n\tndev->vqs = kcalloc(max_vqs, sizeof(*ndev->vqs), GFP_KERNEL);\n\tndev->event_cbs = kcalloc(max_vqs + 1, sizeof(*ndev->event_cbs), GFP_KERNEL);\n\tif (!ndev->vqs || !ndev->event_cbs) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc;\n\t}\n\n\tinit_mvqs(ndev);\n\tallocate_irqs(ndev);\n\tinit_rwsem(&ndev->reslock);\n\tconfig = &ndev->config;\n\n\tif (add_config->mask & BIT_ULL(VDPA_ATTR_DEV_NET_CFG_MTU)) {\n\t\terr = config_func_mtu(mdev, add_config->net.mtu);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\tif (device_features & BIT_ULL(VIRTIO_NET_F_MTU)) {\n\t\terr = query_mtu(mdev, &mtu);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\n\t\tndev->config.mtu = cpu_to_mlx5vdpa16(mvdev, mtu);\n\t}\n\n\tif (device_features & BIT_ULL(VIRTIO_NET_F_STATUS)) {\n\t\tif (get_link_state(mvdev))\n\t\t\tndev->config.status |= cpu_to_mlx5vdpa16(mvdev, VIRTIO_NET_S_LINK_UP);\n\t\telse\n\t\t\tndev->config.status &= cpu_to_mlx5vdpa16(mvdev, ~VIRTIO_NET_S_LINK_UP);\n\t}\n\n\tif (add_config->mask & (1 << VDPA_ATTR_DEV_NET_CFG_MACADDR)) {\n\t\tmemcpy(ndev->config.mac, add_config->net.mac, ETH_ALEN);\n\t \n\t} else if ((add_config->mask & BIT_ULL(VDPA_ATTR_DEV_FEATURES)) == 0 ||\n\t\t   device_features & BIT_ULL(VIRTIO_NET_F_MAC)) {\n\t\terr = mlx5_query_nic_vport_mac_address(mdev, 0, 0, config->mac);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\tif (!is_zero_ether_addr(config->mac)) {\n\t\tpfmdev = pci_get_drvdata(pci_physfn(mdev->pdev));\n\t\terr = mlx5_mpfs_add_mac(pfmdev, config->mac);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t} else if ((add_config->mask & BIT_ULL(VDPA_ATTR_DEV_FEATURES)) == 0) {\n\t\t \n\t\tdevice_features &= ~BIT_ULL(VIRTIO_NET_F_MAC);\n\t} else if (device_features & BIT_ULL(VIRTIO_NET_F_MAC)) {\n\t\t \n\t\tmlx5_vdpa_warn(&ndev->mvdev,\n\t\t\t       \"No mac address provisioned?\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_alloc;\n\t}\n\n\tif (device_features & BIT_ULL(VIRTIO_NET_F_MQ))\n\t\tconfig->max_virtqueue_pairs = cpu_to_mlx5vdpa16(mvdev, max_vqs / 2);\n\n\tndev->mvdev.mlx_features = device_features;\n\tmvdev->vdev.dma_dev = &mdev->pdev->dev;\n\terr = mlx5_vdpa_alloc_resources(&ndev->mvdev);\n\tif (err)\n\t\tgoto err_mpfs;\n\n\tif (MLX5_CAP_GEN(mvdev->mdev, umem_uid_0)) {\n\t\terr = mlx5_vdpa_create_mr(mvdev, NULL, 0);\n\t\tif (err)\n\t\t\tgoto err_res;\n\t}\n\n\terr = alloc_resources(ndev);\n\tif (err)\n\t\tgoto err_mr;\n\n\tndev->cvq_ent.mvdev = mvdev;\n\tINIT_WORK(&ndev->cvq_ent.work, mlx5_cvq_kick_handler);\n\tmvdev->wq = create_singlethread_workqueue(\"mlx5_vdpa_wq\");\n\tif (!mvdev->wq) {\n\t\terr = -ENOMEM;\n\t\tgoto err_res2;\n\t}\n\n\tmvdev->vdev.mdev = &mgtdev->mgtdev;\n\terr = _vdpa_register_device(&mvdev->vdev, max_vqs + 1);\n\tif (err)\n\t\tgoto err_reg;\n\n\tmgtdev->ndev = ndev;\n\treturn 0;\n\nerr_reg:\n\tdestroy_workqueue(mvdev->wq);\nerr_res2:\n\tfree_resources(ndev);\nerr_mr:\n\tmlx5_vdpa_destroy_mr(mvdev);\nerr_res:\n\tmlx5_vdpa_free_resources(&ndev->mvdev);\nerr_mpfs:\n\tif (!is_zero_ether_addr(config->mac))\n\t\tmlx5_mpfs_del_mac(pfmdev, config->mac);\nerr_alloc:\n\tput_device(&mvdev->vdev.dev);\n\treturn err;\n}\n\nstatic void mlx5_vdpa_dev_del(struct vdpa_mgmt_dev *v_mdev, struct vdpa_device *dev)\n{\n\tstruct mlx5_vdpa_mgmtdev *mgtdev = container_of(v_mdev, struct mlx5_vdpa_mgmtdev, mgtdev);\n\tstruct mlx5_vdpa_dev *mvdev = to_mvdev(dev);\n\tstruct mlx5_vdpa_net *ndev = to_mlx5_vdpa_ndev(mvdev);\n\tstruct workqueue_struct *wq;\n\n\tunregister_link_notifier(ndev);\n\t_vdpa_unregister_device(dev);\n\twq = mvdev->wq;\n\tmvdev->wq = NULL;\n\tdestroy_workqueue(wq);\n\tmgtdev->ndev = NULL;\n}\n\nstatic const struct vdpa_mgmtdev_ops mdev_ops = {\n\t.dev_add = mlx5_vdpa_dev_add,\n\t.dev_del = mlx5_vdpa_dev_del,\n};\n\nstatic struct virtio_device_id id_table[] = {\n\t{ VIRTIO_ID_NET, VIRTIO_DEV_ANY_ID },\n\t{ 0 },\n};\n\nstatic int mlx5v_probe(struct auxiliary_device *adev,\n\t\t       const struct auxiliary_device_id *id)\n\n{\n\tstruct mlx5_adev *madev = container_of(adev, struct mlx5_adev, adev);\n\tstruct mlx5_core_dev *mdev = madev->mdev;\n\tstruct mlx5_vdpa_mgmtdev *mgtdev;\n\tint err;\n\n\tmgtdev = kzalloc(sizeof(*mgtdev), GFP_KERNEL);\n\tif (!mgtdev)\n\t\treturn -ENOMEM;\n\n\tmgtdev->mgtdev.ops = &mdev_ops;\n\tmgtdev->mgtdev.device = mdev->device;\n\tmgtdev->mgtdev.id_table = id_table;\n\tmgtdev->mgtdev.config_attr_mask = BIT_ULL(VDPA_ATTR_DEV_NET_CFG_MACADDR) |\n\t\t\t\t\t  BIT_ULL(VDPA_ATTR_DEV_NET_CFG_MAX_VQP) |\n\t\t\t\t\t  BIT_ULL(VDPA_ATTR_DEV_NET_CFG_MTU) |\n\t\t\t\t\t  BIT_ULL(VDPA_ATTR_DEV_FEATURES);\n\tmgtdev->mgtdev.max_supported_vqs =\n\t\tMLX5_CAP_DEV_VDPA_EMULATION(mdev, max_num_virtio_queues) + 1;\n\tmgtdev->mgtdev.supported_features = get_supported_features(mdev);\n\tmgtdev->madev = madev;\n\n\terr = vdpa_mgmtdev_register(&mgtdev->mgtdev);\n\tif (err)\n\t\tgoto reg_err;\n\n\tauxiliary_set_drvdata(adev, mgtdev);\n\n\treturn 0;\n\nreg_err:\n\tkfree(mgtdev);\n\treturn err;\n}\n\nstatic void mlx5v_remove(struct auxiliary_device *adev)\n{\n\tstruct mlx5_vdpa_mgmtdev *mgtdev;\n\n\tmgtdev = auxiliary_get_drvdata(adev);\n\tvdpa_mgmtdev_unregister(&mgtdev->mgtdev);\n\tkfree(mgtdev);\n}\n\nstatic const struct auxiliary_device_id mlx5v_id_table[] = {\n\t{ .name = MLX5_ADEV_NAME \".vnet\", },\n\t{},\n};\n\nMODULE_DEVICE_TABLE(auxiliary, mlx5v_id_table);\n\nstatic struct auxiliary_driver mlx5v_driver = {\n\t.name = \"vnet\",\n\t.probe = mlx5v_probe,\n\t.remove = mlx5v_remove,\n\t.id_table = mlx5v_id_table,\n};\n\nmodule_auxiliary_driver(mlx5v_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}