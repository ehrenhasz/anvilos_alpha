{
  "module_name": "mr.c",
  "hash_id": "83713cde4a70ec600bc938b1226c78a4c13a4c6ff2a966df968dbc831c60c361",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vdpa/mlx5/core/mr.c",
  "human_readable_source": "\n \n\n#include <linux/vhost_types.h>\n#include <linux/vdpa.h>\n#include <linux/gcd.h>\n#include <linux/string.h>\n#include <linux/mlx5/qp.h>\n#include \"mlx5_vdpa.h\"\n\n \n#define MLX5_DIV_ROUND_UP_POW2(_n, _s) \\\n({ \\\n\tu64 __s = _s; \\\n\tu64 _res; \\\n\t_res = (((_n) + (1 << (__s)) - 1) >> (__s)); \\\n\t_res; \\\n})\n\nstatic int get_octo_len(u64 len, int page_shift)\n{\n\tu64 page_size = 1ULL << page_shift;\n\tint npages;\n\n\tnpages = ALIGN(len, page_size) >> page_shift;\n\treturn (npages + 1) / 2;\n}\n\nstatic void mlx5_set_access_mode(void *mkc, int mode)\n{\n\tMLX5_SET(mkc, mkc, access_mode_1_0, mode & 0x3);\n\tMLX5_SET(mkc, mkc, access_mode_4_2, mode >> 2);\n}\n\nstatic void populate_mtts(struct mlx5_vdpa_direct_mr *mr, __be64 *mtt)\n{\n\tstruct scatterlist *sg;\n\tint nsg = mr->nsg;\n\tu64 dma_addr;\n\tu64 dma_len;\n\tint j = 0;\n\tint i;\n\n\tfor_each_sg(mr->sg_head.sgl, sg, mr->nent, i) {\n\t\tfor (dma_addr = sg_dma_address(sg), dma_len = sg_dma_len(sg);\n\t\t     nsg && dma_len;\n\t\t     nsg--, dma_addr += BIT(mr->log_size), dma_len -= BIT(mr->log_size))\n\t\t\tmtt[j++] = cpu_to_be64(dma_addr);\n\t}\n}\n\nstatic int create_direct_mr(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_direct_mr *mr)\n{\n\tint inlen;\n\tvoid *mkc;\n\tvoid *in;\n\tint err;\n\n\tinlen = MLX5_ST_SZ_BYTES(create_mkey_in) + roundup(MLX5_ST_SZ_BYTES(mtt) * mr->nsg, 16);\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(create_mkey_in, in, uid, mvdev->res.uid);\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\tMLX5_SET(mkc, mkc, lw, !!(mr->perm & VHOST_MAP_WO));\n\tMLX5_SET(mkc, mkc, lr, !!(mr->perm & VHOST_MAP_RO));\n\tmlx5_set_access_mode(mkc, MLX5_MKC_ACCESS_MODE_MTT);\n\tMLX5_SET(mkc, mkc, qpn, 0xffffff);\n\tMLX5_SET(mkc, mkc, pd, mvdev->res.pdn);\n\tMLX5_SET64(mkc, mkc, start_addr, mr->offset);\n\tMLX5_SET64(mkc, mkc, len, mr->end - mr->start);\n\tMLX5_SET(mkc, mkc, log_page_size, mr->log_size);\n\tMLX5_SET(mkc, mkc, translations_octword_size,\n\t\t get_octo_len(mr->end - mr->start, mr->log_size));\n\tMLX5_SET(create_mkey_in, in, translations_octword_actual_size,\n\t\t get_octo_len(mr->end - mr->start, mr->log_size));\n\tpopulate_mtts(mr, MLX5_ADDR_OF(create_mkey_in, in, klm_pas_mtt));\n\terr = mlx5_vdpa_create_mkey(mvdev, &mr->mr, in, inlen);\n\tkvfree(in);\n\tif (err) {\n\t\tmlx5_vdpa_warn(mvdev, \"Failed to create direct MR\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void destroy_direct_mr(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_direct_mr *mr)\n{\n\tmlx5_vdpa_destroy_mkey(mvdev, mr->mr);\n}\n\nstatic u64 map_start(struct vhost_iotlb_map *map, struct mlx5_vdpa_direct_mr *mr)\n{\n\treturn max_t(u64, map->start, mr->start);\n}\n\nstatic u64 map_end(struct vhost_iotlb_map *map, struct mlx5_vdpa_direct_mr *mr)\n{\n\treturn min_t(u64, map->last + 1, mr->end);\n}\n\nstatic u64 maplen(struct vhost_iotlb_map *map, struct mlx5_vdpa_direct_mr *mr)\n{\n\treturn map_end(map, mr) - map_start(map, mr);\n}\n\n#define MLX5_VDPA_INVALID_START_ADDR ((u64)-1)\n#define MLX5_VDPA_INVALID_LEN ((u64)-1)\n\nstatic u64 indir_start_addr(struct mlx5_vdpa_mr *mkey)\n{\n\tstruct mlx5_vdpa_direct_mr *s;\n\n\ts = list_first_entry_or_null(&mkey->head, struct mlx5_vdpa_direct_mr, list);\n\tif (!s)\n\t\treturn MLX5_VDPA_INVALID_START_ADDR;\n\n\treturn s->start;\n}\n\nstatic u64 indir_len(struct mlx5_vdpa_mr *mkey)\n{\n\tstruct mlx5_vdpa_direct_mr *s;\n\tstruct mlx5_vdpa_direct_mr *e;\n\n\ts = list_first_entry_or_null(&mkey->head, struct mlx5_vdpa_direct_mr, list);\n\tif (!s)\n\t\treturn MLX5_VDPA_INVALID_LEN;\n\n\te = list_last_entry(&mkey->head, struct mlx5_vdpa_direct_mr, list);\n\n\treturn e->end - s->start;\n}\n\n#define LOG_MAX_KLM_SIZE 30\n#define MAX_KLM_SIZE BIT(LOG_MAX_KLM_SIZE)\n\nstatic u32 klm_bcount(u64 size)\n{\n\treturn (u32)size;\n}\n\nstatic void fill_indir(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_mr *mkey, void *in)\n{\n\tstruct mlx5_vdpa_direct_mr *dmr;\n\tstruct mlx5_klm *klmarr;\n\tstruct mlx5_klm *klm;\n\tbool first = true;\n\tu64 preve;\n\tint i;\n\n\tklmarr = MLX5_ADDR_OF(create_mkey_in, in, klm_pas_mtt);\n\ti = 0;\n\tlist_for_each_entry(dmr, &mkey->head, list) {\nagain:\n\t\tklm = &klmarr[i++];\n\t\tif (first) {\n\t\t\tpreve = dmr->start;\n\t\t\tfirst = false;\n\t\t}\n\n\t\tif (preve == dmr->start) {\n\t\t\tklm->key = cpu_to_be32(dmr->mr);\n\t\t\tklm->bcount = cpu_to_be32(klm_bcount(dmr->end - dmr->start));\n\t\t\tpreve = dmr->end;\n\t\t} else {\n\t\t\tklm->key = cpu_to_be32(mvdev->res.null_mkey);\n\t\t\tklm->bcount = cpu_to_be32(klm_bcount(dmr->start - preve));\n\t\t\tpreve = dmr->start;\n\t\t\tgoto again;\n\t\t}\n\t}\n}\n\nstatic int klm_byte_size(int nklms)\n{\n\treturn 16 * ALIGN(nklms, 4);\n}\n\nstatic int create_indirect_key(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_mr *mr)\n{\n\tint inlen;\n\tvoid *mkc;\n\tvoid *in;\n\tint err;\n\tu64 start;\n\tu64 len;\n\n\tstart = indir_start_addr(mr);\n\tlen = indir_len(mr);\n\tif (start == MLX5_VDPA_INVALID_START_ADDR || len == MLX5_VDPA_INVALID_LEN)\n\t\treturn -EINVAL;\n\n\tinlen = MLX5_ST_SZ_BYTES(create_mkey_in) + klm_byte_size(mr->num_klms);\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(create_mkey_in, in, uid, mvdev->res.uid);\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\tMLX5_SET(mkc, mkc, lw, 1);\n\tMLX5_SET(mkc, mkc, lr, 1);\n\tmlx5_set_access_mode(mkc, MLX5_MKC_ACCESS_MODE_KLMS);\n\tMLX5_SET(mkc, mkc, qpn, 0xffffff);\n\tMLX5_SET(mkc, mkc, pd, mvdev->res.pdn);\n\tMLX5_SET64(mkc, mkc, start_addr, start);\n\tMLX5_SET64(mkc, mkc, len, len);\n\tMLX5_SET(mkc, mkc, translations_octword_size, klm_byte_size(mr->num_klms) / 16);\n\tMLX5_SET(create_mkey_in, in, translations_octword_actual_size, mr->num_klms);\n\tfill_indir(mvdev, mr, in);\n\terr = mlx5_vdpa_create_mkey(mvdev, &mr->mkey, in, inlen);\n\tkfree(in);\n\treturn err;\n}\n\nstatic void destroy_indirect_key(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_mr *mkey)\n{\n\tmlx5_vdpa_destroy_mkey(mvdev, mkey->mkey);\n}\n\nstatic int map_direct_mr(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_direct_mr *mr,\n\t\t\t struct vhost_iotlb *iotlb)\n{\n\tstruct vhost_iotlb_map *map;\n\tunsigned long lgcd = 0;\n\tint log_entity_size;\n\tunsigned long size;\n\tu64 start = 0;\n\tint err;\n\tstruct page *pg;\n\tunsigned int nsg;\n\tint sglen;\n\tu64 pa;\n\tu64 paend;\n\tstruct scatterlist *sg;\n\tstruct device *dma = mvdev->vdev.dma_dev;\n\n\tfor (map = vhost_iotlb_itree_first(iotlb, mr->start, mr->end - 1);\n\t     map; map = vhost_iotlb_itree_next(map, start, mr->end - 1)) {\n\t\tsize = maplen(map, mr);\n\t\tlgcd = gcd(lgcd, size);\n\t\tstart += size;\n\t}\n\tlog_entity_size = ilog2(lgcd);\n\n\tsglen = 1 << log_entity_size;\n\tnsg = MLX5_DIV_ROUND_UP_POW2(mr->end - mr->start, log_entity_size);\n\n\terr = sg_alloc_table(&mr->sg_head, nsg, GFP_KERNEL);\n\tif (err)\n\t\treturn err;\n\n\tsg = mr->sg_head.sgl;\n\tfor (map = vhost_iotlb_itree_first(iotlb, mr->start, mr->end - 1);\n\t     map; map = vhost_iotlb_itree_next(map, mr->start, mr->end - 1)) {\n\t\tpaend = map->addr + maplen(map, mr);\n\t\tfor (pa = map->addr; pa < paend; pa += sglen) {\n\t\t\tpg = pfn_to_page(__phys_to_pfn(pa));\n\t\t\tif (!sg) {\n\t\t\t\tmlx5_vdpa_warn(mvdev, \"sg null. start 0x%llx, end 0x%llx\\n\",\n\t\t\t\t\t       map->start, map->last + 1);\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto err_map;\n\t\t\t}\n\t\t\tsg_set_page(sg, pg, sglen, 0);\n\t\t\tsg = sg_next(sg);\n\t\t\tif (!sg)\n\t\t\t\tgoto done;\n\t\t}\n\t}\ndone:\n\tmr->log_size = log_entity_size;\n\tmr->nsg = nsg;\n\tmr->nent = dma_map_sg_attrs(dma, mr->sg_head.sgl, mr->nsg, DMA_BIDIRECTIONAL, 0);\n\tif (!mr->nent) {\n\t\terr = -ENOMEM;\n\t\tgoto err_map;\n\t}\n\n\terr = create_direct_mr(mvdev, mr);\n\tif (err)\n\t\tgoto err_direct;\n\n\treturn 0;\n\nerr_direct:\n\tdma_unmap_sg_attrs(dma, mr->sg_head.sgl, mr->nsg, DMA_BIDIRECTIONAL, 0);\nerr_map:\n\tsg_free_table(&mr->sg_head);\n\treturn err;\n}\n\nstatic void unmap_direct_mr(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_direct_mr *mr)\n{\n\tstruct device *dma = mvdev->vdev.dma_dev;\n\n\tdestroy_direct_mr(mvdev, mr);\n\tdma_unmap_sg_attrs(dma, mr->sg_head.sgl, mr->nsg, DMA_BIDIRECTIONAL, 0);\n\tsg_free_table(&mr->sg_head);\n}\n\nstatic int add_direct_chain(struct mlx5_vdpa_dev *mvdev, u64 start, u64 size, u8 perm,\n\t\t\t    struct vhost_iotlb *iotlb)\n{\n\tstruct mlx5_vdpa_mr *mr = &mvdev->mr;\n\tstruct mlx5_vdpa_direct_mr *dmr;\n\tstruct mlx5_vdpa_direct_mr *n;\n\tLIST_HEAD(tmp);\n\tu64 st;\n\tu64 sz;\n\tint err;\n\n\tst = start;\n\twhile (size) {\n\t\tsz = (u32)min_t(u64, MAX_KLM_SIZE, size);\n\t\tdmr = kzalloc(sizeof(*dmr), GFP_KERNEL);\n\t\tif (!dmr) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_alloc;\n\t\t}\n\n\t\tdmr->start = st;\n\t\tdmr->end = st + sz;\n\t\tdmr->perm = perm;\n\t\terr = map_direct_mr(mvdev, dmr, iotlb);\n\t\tif (err) {\n\t\t\tkfree(dmr);\n\t\t\tgoto err_alloc;\n\t\t}\n\n\t\tlist_add_tail(&dmr->list, &tmp);\n\t\tsize -= sz;\n\t\tmr->num_directs++;\n\t\tmr->num_klms++;\n\t\tst += sz;\n\t}\n\tlist_splice_tail(&tmp, &mr->head);\n\treturn 0;\n\nerr_alloc:\n\tlist_for_each_entry_safe(dmr, n, &mr->head, list) {\n\t\tlist_del_init(&dmr->list);\n\t\tunmap_direct_mr(mvdev, dmr);\n\t\tkfree(dmr);\n\t}\n\treturn err;\n}\n\n \nstatic int create_user_mr(struct mlx5_vdpa_dev *mvdev, struct vhost_iotlb *iotlb)\n{\n\tstruct mlx5_vdpa_mr *mr = &mvdev->mr;\n\tstruct mlx5_vdpa_direct_mr *dmr;\n\tstruct mlx5_vdpa_direct_mr *n;\n\tstruct vhost_iotlb_map *map;\n\tu32 pperm = U16_MAX;\n\tu64 last = U64_MAX;\n\tu64 ps = U64_MAX;\n\tu64 pe = U64_MAX;\n\tu64 start = 0;\n\tint err = 0;\n\tint nnuls;\n\n\tINIT_LIST_HEAD(&mr->head);\n\tfor (map = vhost_iotlb_itree_first(iotlb, start, last); map;\n\t     map = vhost_iotlb_itree_next(map, start, last)) {\n\t\tstart = map->start;\n\t\tif (pe == map->start && pperm == map->perm) {\n\t\t\tpe = map->last + 1;\n\t\t} else {\n\t\t\tif (ps != U64_MAX) {\n\t\t\t\tif (pe < map->start) {\n\t\t\t\t\t \n\t\t\t\t\tnnuls = MLX5_DIV_ROUND_UP_POW2(map->start - pe,\n\t\t\t\t\t\t\t\t       LOG_MAX_KLM_SIZE);\n\t\t\t\t\tmr->num_klms += nnuls;\n\t\t\t\t}\n\t\t\t\terr = add_direct_chain(mvdev, ps, pe - ps, pperm, iotlb);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto err_chain;\n\t\t\t}\n\t\t\tps = map->start;\n\t\t\tpe = map->last + 1;\n\t\t\tpperm = map->perm;\n\t\t}\n\t}\n\terr = add_direct_chain(mvdev, ps, pe - ps, pperm, iotlb);\n\tif (err)\n\t\tgoto err_chain;\n\n\t \n\terr = create_indirect_key(mvdev, mr);\n\tif (err)\n\t\tgoto err_chain;\n\n\tmr->user_mr = true;\n\treturn 0;\n\nerr_chain:\n\tlist_for_each_entry_safe_reverse(dmr, n, &mr->head, list) {\n\t\tlist_del_init(&dmr->list);\n\t\tunmap_direct_mr(mvdev, dmr);\n\t\tkfree(dmr);\n\t}\n\treturn err;\n}\n\nstatic int create_dma_mr(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_mr *mr)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\tvoid *mkc;\n\tu32 *in;\n\tint err;\n\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\n\tMLX5_SET(mkc, mkc, access_mode_1_0, MLX5_MKC_ACCESS_MODE_PA);\n\tMLX5_SET(mkc, mkc, length64, 1);\n\tMLX5_SET(mkc, mkc, lw, 1);\n\tMLX5_SET(mkc, mkc, lr, 1);\n\tMLX5_SET(mkc, mkc, pd, mvdev->res.pdn);\n\tMLX5_SET(mkc, mkc, qpn, 0xffffff);\n\n\terr = mlx5_vdpa_create_mkey(mvdev, &mr->mkey, in, inlen);\n\tif (!err)\n\t\tmr->user_mr = false;\n\n\tkfree(in);\n\treturn err;\n}\n\nstatic void destroy_dma_mr(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_mr *mr)\n{\n\tmlx5_vdpa_destroy_mkey(mvdev, mr->mkey);\n}\n\nstatic int dup_iotlb(struct mlx5_vdpa_dev *mvdev, struct vhost_iotlb *src)\n{\n\tstruct vhost_iotlb_map *map;\n\tu64 start = 0, last = ULLONG_MAX;\n\tint err;\n\n\tif (!src) {\n\t\terr = vhost_iotlb_add_range(mvdev->cvq.iotlb, start, last, start, VHOST_ACCESS_RW);\n\t\treturn err;\n\t}\n\n\tfor (map = vhost_iotlb_itree_first(src, start, last); map;\n\t\tmap = vhost_iotlb_itree_next(map, start, last)) {\n\t\terr = vhost_iotlb_add_range(mvdev->cvq.iotlb, map->start, map->last,\n\t\t\t\t\t    map->addr, map->perm);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic void prune_iotlb(struct mlx5_vdpa_dev *mvdev)\n{\n\tvhost_iotlb_del_range(mvdev->cvq.iotlb, 0, ULLONG_MAX);\n}\n\nstatic void destroy_user_mr(struct mlx5_vdpa_dev *mvdev, struct mlx5_vdpa_mr *mr)\n{\n\tstruct mlx5_vdpa_direct_mr *dmr;\n\tstruct mlx5_vdpa_direct_mr *n;\n\n\tdestroy_indirect_key(mvdev, mr);\n\tlist_for_each_entry_safe_reverse(dmr, n, &mr->head, list) {\n\t\tlist_del_init(&dmr->list);\n\t\tunmap_direct_mr(mvdev, dmr);\n\t\tkfree(dmr);\n\t}\n}\n\nstatic void _mlx5_vdpa_destroy_cvq_mr(struct mlx5_vdpa_dev *mvdev, unsigned int asid)\n{\n\tif (mvdev->group2asid[MLX5_VDPA_CVQ_GROUP] != asid)\n\t\treturn;\n\n\tprune_iotlb(mvdev);\n}\n\nstatic void _mlx5_vdpa_destroy_dvq_mr(struct mlx5_vdpa_dev *mvdev, unsigned int asid)\n{\n\tstruct mlx5_vdpa_mr *mr = &mvdev->mr;\n\n\tif (mvdev->group2asid[MLX5_VDPA_DATAVQ_GROUP] != asid)\n\t\treturn;\n\n\tif (!mr->initialized)\n\t\treturn;\n\n\tif (mr->user_mr)\n\t\tdestroy_user_mr(mvdev, mr);\n\telse\n\t\tdestroy_dma_mr(mvdev, mr);\n\n\tmr->initialized = false;\n}\n\nvoid mlx5_vdpa_destroy_mr_asid(struct mlx5_vdpa_dev *mvdev, unsigned int asid)\n{\n\tstruct mlx5_vdpa_mr *mr = &mvdev->mr;\n\n\tmutex_lock(&mr->mkey_mtx);\n\n\t_mlx5_vdpa_destroy_dvq_mr(mvdev, asid);\n\t_mlx5_vdpa_destroy_cvq_mr(mvdev, asid);\n\n\tmutex_unlock(&mr->mkey_mtx);\n}\n\nvoid mlx5_vdpa_destroy_mr(struct mlx5_vdpa_dev *mvdev)\n{\n\tmlx5_vdpa_destroy_mr_asid(mvdev, mvdev->group2asid[MLX5_VDPA_CVQ_GROUP]);\n\tmlx5_vdpa_destroy_mr_asid(mvdev, mvdev->group2asid[MLX5_VDPA_DATAVQ_GROUP]);\n}\n\nstatic int _mlx5_vdpa_create_cvq_mr(struct mlx5_vdpa_dev *mvdev,\n\t\t\t\t    struct vhost_iotlb *iotlb,\n\t\t\t\t    unsigned int asid)\n{\n\tif (mvdev->group2asid[MLX5_VDPA_CVQ_GROUP] != asid)\n\t\treturn 0;\n\n\treturn dup_iotlb(mvdev, iotlb);\n}\n\nstatic int _mlx5_vdpa_create_dvq_mr(struct mlx5_vdpa_dev *mvdev,\n\t\t\t\t    struct vhost_iotlb *iotlb,\n\t\t\t\t    unsigned int asid)\n{\n\tstruct mlx5_vdpa_mr *mr = &mvdev->mr;\n\tint err;\n\n\tif (mvdev->group2asid[MLX5_VDPA_DATAVQ_GROUP] != asid)\n\t\treturn 0;\n\n\tif (mr->initialized)\n\t\treturn 0;\n\n\tif (iotlb)\n\t\terr = create_user_mr(mvdev, iotlb);\n\telse\n\t\terr = create_dma_mr(mvdev, mr);\n\n\tif (err)\n\t\treturn err;\n\n\tmr->initialized = true;\n\n\treturn 0;\n}\n\nstatic int _mlx5_vdpa_create_mr(struct mlx5_vdpa_dev *mvdev,\n\t\t\t\tstruct vhost_iotlb *iotlb, unsigned int asid)\n{\n\tint err;\n\n\terr = _mlx5_vdpa_create_dvq_mr(mvdev, iotlb, asid);\n\tif (err)\n\t\treturn err;\n\n\terr = _mlx5_vdpa_create_cvq_mr(mvdev, iotlb, asid);\n\tif (err)\n\t\tgoto out_err;\n\n\treturn 0;\n\nout_err:\n\t_mlx5_vdpa_destroy_dvq_mr(mvdev, asid);\n\n\treturn err;\n}\n\nint mlx5_vdpa_create_mr(struct mlx5_vdpa_dev *mvdev, struct vhost_iotlb *iotlb,\n\t\t\tunsigned int asid)\n{\n\tint err;\n\n\tmutex_lock(&mvdev->mr.mkey_mtx);\n\terr = _mlx5_vdpa_create_mr(mvdev, iotlb, asid);\n\tmutex_unlock(&mvdev->mr.mkey_mtx);\n\treturn err;\n}\n\nint mlx5_vdpa_handle_set_map(struct mlx5_vdpa_dev *mvdev, struct vhost_iotlb *iotlb,\n\t\t\t     bool *change_map, unsigned int asid)\n{\n\tstruct mlx5_vdpa_mr *mr = &mvdev->mr;\n\tint err = 0;\n\n\t*change_map = false;\n\tmutex_lock(&mr->mkey_mtx);\n\tif (mr->initialized) {\n\t\tmlx5_vdpa_info(mvdev, \"memory map update\\n\");\n\t\t*change_map = true;\n\t}\n\tif (!*change_map)\n\t\terr = _mlx5_vdpa_create_mr(mvdev, iotlb, asid);\n\tmutex_unlock(&mr->mkey_mtx);\n\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}