{
  "module_name": "ifcvf_base.c",
  "hash_id": "a01a661d44704be9096866973ead21dd05f5ff62bfbe58d7a719f46c8b8eebd7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vdpa/ifcvf/ifcvf_base.c",
  "human_readable_source": "\n \n\n#include \"ifcvf_base.h\"\n\nu16 ifcvf_set_vq_vector(struct ifcvf_hw *hw, u16 qid, int vector)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = hw->common_cfg;\n\n\tvp_iowrite16(qid, &cfg->queue_select);\n\tvp_iowrite16(vector, &cfg->queue_msix_vector);\n\n\treturn vp_ioread16(&cfg->queue_msix_vector);\n}\n\nu16 ifcvf_set_config_vector(struct ifcvf_hw *hw, int vector)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = hw->common_cfg;\n\n\tvp_iowrite16(vector,  &cfg->msix_config);\n\n\treturn vp_ioread16(&cfg->msix_config);\n}\n\nstatic void __iomem *get_cap_addr(struct ifcvf_hw *hw,\n\t\t\t\t  struct virtio_pci_cap *cap)\n{\n\tu32 length, offset;\n\tu8 bar;\n\n\tlength = le32_to_cpu(cap->length);\n\toffset = le32_to_cpu(cap->offset);\n\tbar = cap->bar;\n\n\tif (bar >= IFCVF_PCI_MAX_RESOURCE) {\n\t\tIFCVF_DBG(hw->pdev,\n\t\t\t  \"Invalid bar number %u to get capabilities\\n\", bar);\n\t\treturn NULL;\n\t}\n\n\tif (offset + length > pci_resource_len(hw->pdev, bar)) {\n\t\tIFCVF_DBG(hw->pdev,\n\t\t\t  \"offset(%u) + len(%u) overflows bar%u's capability\\n\",\n\t\t\t  offset, length, bar);\n\t\treturn NULL;\n\t}\n\n\treturn hw->base[bar] + offset;\n}\n\nstatic int ifcvf_read_config_range(struct pci_dev *dev,\n\t\t\t\t   uint32_t *val, int size, int where)\n{\n\tint ret, i;\n\n\tfor (i = 0; i < size; i += 4) {\n\t\tret = pci_read_config_dword(dev, where + i, val + i / 4);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic u16 ifcvf_get_vq_size(struct ifcvf_hw *hw, u16 qid)\n{\n\tu16 queue_size;\n\n\tvp_iowrite16(qid, &hw->common_cfg->queue_select);\n\tqueue_size = vp_ioread16(&hw->common_cfg->queue_size);\n\n\treturn queue_size;\n}\n\n \nu16 ifcvf_get_max_vq_size(struct ifcvf_hw *hw)\n{\n\tu16 queue_size, max_size, qid;\n\n\tmax_size = ifcvf_get_vq_size(hw, 0);\n\tfor (qid = 1; qid < hw->nr_vring; qid++) {\n\t\tqueue_size = ifcvf_get_vq_size(hw, qid);\n\t\t \n\t\tif (!queue_size)\n\t\t\tcontinue;\n\n\t\tmax_size = min(queue_size, max_size);\n\t}\n\n\treturn max_size;\n}\n\nint ifcvf_init_hw(struct ifcvf_hw *hw, struct pci_dev *pdev)\n{\n\tstruct virtio_pci_cap cap;\n\tu16 notify_off;\n\tint ret;\n\tu8 pos;\n\tu32 i;\n\n\tret = pci_read_config_byte(pdev, PCI_CAPABILITY_LIST, &pos);\n\tif (ret < 0) {\n\t\tIFCVF_ERR(pdev, \"Failed to read PCI capability list\\n\");\n\t\treturn -EIO;\n\t}\n\thw->pdev = pdev;\n\n\twhile (pos) {\n\t\tret = ifcvf_read_config_range(pdev, (u32 *)&cap,\n\t\t\t\t\t      sizeof(cap), pos);\n\t\tif (ret < 0) {\n\t\t\tIFCVF_ERR(pdev,\n\t\t\t\t  \"Failed to get PCI capability at %x\\n\", pos);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cap.cap_vndr != PCI_CAP_ID_VNDR)\n\t\t\tgoto next;\n\n\t\tswitch (cap.cfg_type) {\n\t\tcase VIRTIO_PCI_CAP_COMMON_CFG:\n\t\t\thw->common_cfg = get_cap_addr(hw, &cap);\n\t\t\tIFCVF_DBG(pdev, \"hw->common_cfg = %p\\n\",\n\t\t\t\t  hw->common_cfg);\n\t\t\tbreak;\n\t\tcase VIRTIO_PCI_CAP_NOTIFY_CFG:\n\t\t\tpci_read_config_dword(pdev, pos + sizeof(cap),\n\t\t\t\t\t      &hw->notify_off_multiplier);\n\t\t\thw->notify_bar = cap.bar;\n\t\t\thw->notify_base = get_cap_addr(hw, &cap);\n\t\t\thw->notify_base_pa = pci_resource_start(pdev, cap.bar) +\n\t\t\t\t\tle32_to_cpu(cap.offset);\n\t\t\tIFCVF_DBG(pdev, \"hw->notify_base = %p\\n\",\n\t\t\t\t  hw->notify_base);\n\t\t\tbreak;\n\t\tcase VIRTIO_PCI_CAP_ISR_CFG:\n\t\t\thw->isr = get_cap_addr(hw, &cap);\n\t\t\tIFCVF_DBG(pdev, \"hw->isr = %p\\n\", hw->isr);\n\t\t\tbreak;\n\t\tcase VIRTIO_PCI_CAP_DEVICE_CFG:\n\t\t\thw->dev_cfg = get_cap_addr(hw, &cap);\n\t\t\thw->cap_dev_config_size = le32_to_cpu(cap.length);\n\t\t\tIFCVF_DBG(pdev, \"hw->dev_cfg = %p\\n\", hw->dev_cfg);\n\t\t\tbreak;\n\t\t}\n\nnext:\n\t\tpos = cap.cap_next;\n\t}\n\n\tif (hw->common_cfg == NULL || hw->notify_base == NULL ||\n\t    hw->isr == NULL || hw->dev_cfg == NULL) {\n\t\tIFCVF_ERR(pdev, \"Incomplete PCI capabilities\\n\");\n\t\treturn -EIO;\n\t}\n\n\thw->nr_vring = vp_ioread16(&hw->common_cfg->num_queues);\n\thw->vring = kzalloc(sizeof(struct vring_info) * hw->nr_vring, GFP_KERNEL);\n\tif (!hw->vring)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < hw->nr_vring; i++) {\n\t\tvp_iowrite16(i, &hw->common_cfg->queue_select);\n\t\tnotify_off = vp_ioread16(&hw->common_cfg->queue_notify_off);\n\t\thw->vring[i].notify_addr = hw->notify_base +\n\t\t\tnotify_off * hw->notify_off_multiplier;\n\t\thw->vring[i].notify_pa = hw->notify_base_pa +\n\t\t\tnotify_off * hw->notify_off_multiplier;\n\t\thw->vring[i].irq = -EINVAL;\n\t}\n\n\thw->lm_cfg = hw->base[IFCVF_LM_BAR];\n\n\tIFCVF_DBG(pdev,\n\t\t  \"PCI capability mapping: common cfg: %p, notify base: %p\\n, isr cfg: %p, device cfg: %p, multiplier: %u\\n\",\n\t\t  hw->common_cfg, hw->notify_base, hw->isr,\n\t\t  hw->dev_cfg, hw->notify_off_multiplier);\n\n\thw->vqs_reused_irq = -EINVAL;\n\thw->config_irq = -EINVAL;\n\n\treturn 0;\n}\n\nu8 ifcvf_get_status(struct ifcvf_hw *hw)\n{\n\treturn vp_ioread8(&hw->common_cfg->device_status);\n}\n\nvoid ifcvf_set_status(struct ifcvf_hw *hw, u8 status)\n{\n\tvp_iowrite8(status, &hw->common_cfg->device_status);\n}\n\nvoid ifcvf_reset(struct ifcvf_hw *hw)\n{\n\tifcvf_set_status(hw, 0);\n\twhile (ifcvf_get_status(hw))\n\t\tmsleep(1);\n}\n\nu64 ifcvf_get_hw_features(struct ifcvf_hw *hw)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = hw->common_cfg;\n\tu32 features_lo, features_hi;\n\tu64 features;\n\n\tvp_iowrite32(0, &cfg->device_feature_select);\n\tfeatures_lo = vp_ioread32(&cfg->device_feature);\n\n\tvp_iowrite32(1, &cfg->device_feature_select);\n\tfeatures_hi = vp_ioread32(&cfg->device_feature);\n\n\tfeatures = ((u64)features_hi << 32) | features_lo;\n\n\treturn features;\n}\n\n \nu64 ifcvf_get_dev_features(struct ifcvf_hw *hw)\n{\n\treturn hw->dev_features;\n}\n\nu64 ifcvf_get_driver_features(struct ifcvf_hw *hw)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = hw->common_cfg;\n\tu32 features_lo, features_hi;\n\tu64 features;\n\n\tvp_iowrite32(0, &cfg->device_feature_select);\n\tfeatures_lo = vp_ioread32(&cfg->guest_feature);\n\n\tvp_iowrite32(1, &cfg->device_feature_select);\n\tfeatures_hi = vp_ioread32(&cfg->guest_feature);\n\n\tfeatures = ((u64)features_hi << 32) | features_lo;\n\n\treturn features;\n}\n\nint ifcvf_verify_min_features(struct ifcvf_hw *hw, u64 features)\n{\n\tif (!(features & BIT_ULL(VIRTIO_F_ACCESS_PLATFORM)) && features) {\n\t\tIFCVF_ERR(hw->pdev, \"VIRTIO_F_ACCESS_PLATFORM is not negotiated\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nu32 ifcvf_get_config_size(struct ifcvf_hw *hw)\n{\n\tu32 net_config_size = sizeof(struct virtio_net_config);\n\tu32 blk_config_size = sizeof(struct virtio_blk_config);\n\tu32 cap_size = hw->cap_dev_config_size;\n\tu32 config_size;\n\n\t \n\tswitch (hw->dev_type) {\n\tcase VIRTIO_ID_NET:\n\t\tconfig_size = min(cap_size, net_config_size);\n\t\tbreak;\n\tcase VIRTIO_ID_BLOCK:\n\t\tconfig_size = min(cap_size, blk_config_size);\n\t\tbreak;\n\tdefault:\n\t\tconfig_size = 0;\n\t\tIFCVF_ERR(hw->pdev, \"VIRTIO ID %u not supported\\n\", hw->dev_type);\n\t}\n\n\treturn config_size;\n}\n\nvoid ifcvf_read_dev_config(struct ifcvf_hw *hw, u64 offset,\n\t\t\t   void *dst, int length)\n{\n\tu8 old_gen, new_gen, *p;\n\tint i;\n\n\tWARN_ON(offset + length > hw->config_size);\n\tdo {\n\t\told_gen = vp_ioread8(&hw->common_cfg->config_generation);\n\t\tp = dst;\n\t\tfor (i = 0; i < length; i++)\n\t\t\t*p++ = vp_ioread8(hw->dev_cfg + offset + i);\n\n\t\tnew_gen = vp_ioread8(&hw->common_cfg->config_generation);\n\t} while (old_gen != new_gen);\n}\n\nvoid ifcvf_write_dev_config(struct ifcvf_hw *hw, u64 offset,\n\t\t\t    const void *src, int length)\n{\n\tconst u8 *p;\n\tint i;\n\n\tp = src;\n\tWARN_ON(offset + length > hw->config_size);\n\tfor (i = 0; i < length; i++)\n\t\tvp_iowrite8(*p++, hw->dev_cfg + offset + i);\n}\n\nvoid ifcvf_set_driver_features(struct ifcvf_hw *hw, u64 features)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = hw->common_cfg;\n\n\tvp_iowrite32(0, &cfg->guest_feature_select);\n\tvp_iowrite32((u32)features, &cfg->guest_feature);\n\n\tvp_iowrite32(1, &cfg->guest_feature_select);\n\tvp_iowrite32(features >> 32, &cfg->guest_feature);\n}\n\nu16 ifcvf_get_vq_state(struct ifcvf_hw *hw, u16 qid)\n{\n\tstruct ifcvf_lm_cfg  __iomem *lm_cfg = hw->lm_cfg;\n\tu16 last_avail_idx;\n\n\tlast_avail_idx = vp_ioread16(&lm_cfg->vq_state_region + qid * 2);\n\n\treturn last_avail_idx;\n}\n\nint ifcvf_set_vq_state(struct ifcvf_hw *hw, u16 qid, u16 num)\n{\n\tstruct ifcvf_lm_cfg  __iomem *lm_cfg = hw->lm_cfg;\n\n\tvp_iowrite16(num, &lm_cfg->vq_state_region + qid * 2);\n\n\treturn 0;\n}\n\nvoid ifcvf_set_vq_num(struct ifcvf_hw *hw, u16 qid, u32 num)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = hw->common_cfg;\n\n\tvp_iowrite16(qid, &cfg->queue_select);\n\tvp_iowrite16(num, &cfg->queue_size);\n}\n\nint ifcvf_set_vq_address(struct ifcvf_hw *hw, u16 qid, u64 desc_area,\n\t\t\t u64 driver_area, u64 device_area)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = hw->common_cfg;\n\n\tvp_iowrite16(qid, &cfg->queue_select);\n\tvp_iowrite64_twopart(desc_area, &cfg->queue_desc_lo,\n\t\t\t     &cfg->queue_desc_hi);\n\tvp_iowrite64_twopart(driver_area, &cfg->queue_avail_lo,\n\t\t\t     &cfg->queue_avail_hi);\n\tvp_iowrite64_twopart(device_area, &cfg->queue_used_lo,\n\t\t\t     &cfg->queue_used_hi);\n\n\treturn 0;\n}\n\nbool ifcvf_get_vq_ready(struct ifcvf_hw *hw, u16 qid)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = hw->common_cfg;\n\tu16 queue_enable;\n\n\tvp_iowrite16(qid, &cfg->queue_select);\n\tqueue_enable = vp_ioread16(&cfg->queue_enable);\n\n\treturn (bool)queue_enable;\n}\n\nvoid ifcvf_set_vq_ready(struct ifcvf_hw *hw, u16 qid, bool ready)\n{\n\tstruct virtio_pci_common_cfg __iomem *cfg = hw->common_cfg;\n\n\tvp_iowrite16(qid, &cfg->queue_select);\n\tvp_iowrite16(ready, &cfg->queue_enable);\n}\n\nstatic void ifcvf_reset_vring(struct ifcvf_hw *hw)\n{\n\tu16 qid;\n\n\tfor (qid = 0; qid < hw->nr_vring; qid++) {\n\t\thw->vring[qid].cb.callback = NULL;\n\t\thw->vring[qid].cb.private = NULL;\n\t\tifcvf_set_vq_vector(hw, qid, VIRTIO_MSI_NO_VECTOR);\n\t}\n}\n\nstatic void ifcvf_reset_config_handler(struct ifcvf_hw *hw)\n{\n\thw->config_cb.callback = NULL;\n\thw->config_cb.private = NULL;\n\tifcvf_set_config_vector(hw, VIRTIO_MSI_NO_VECTOR);\n}\n\nstatic void ifcvf_synchronize_irq(struct ifcvf_hw *hw)\n{\n\tu32 nvectors = hw->num_msix_vectors;\n\tstruct pci_dev *pdev = hw->pdev;\n\tint i, irq;\n\n\tfor (i = 0; i < nvectors; i++) {\n\t\tirq = pci_irq_vector(pdev, i);\n\t\tif (irq >= 0)\n\t\t\tsynchronize_irq(irq);\n\t}\n}\n\nvoid ifcvf_stop(struct ifcvf_hw *hw)\n{\n\tifcvf_synchronize_irq(hw);\n\tifcvf_reset_vring(hw);\n\tifcvf_reset_config_handler(hw);\n}\n\nvoid ifcvf_notify_queue(struct ifcvf_hw *hw, u16 qid)\n{\n\tvp_iowrite16(qid, hw->vring[qid].notify_addr);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}