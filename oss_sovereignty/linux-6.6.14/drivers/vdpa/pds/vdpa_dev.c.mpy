{
  "module_name": "vdpa_dev.c",
  "hash_id": "db37c6cefe27797b0a02ac12a3abaaff6e558cb2a73f696ce2f2641c140f4585",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vdpa/pds/vdpa_dev.c",
  "human_readable_source": "\n \n\n#include <linux/pci.h>\n#include <linux/vdpa.h>\n#include <uapi/linux/vdpa.h>\n#include <linux/virtio_pci_modern.h>\n\n#include <linux/pds/pds_common.h>\n#include <linux/pds/pds_core_if.h>\n#include <linux/pds/pds_adminq.h>\n#include <linux/pds/pds_auxbus.h>\n\n#include \"vdpa_dev.h\"\n#include \"aux_drv.h\"\n#include \"cmds.h\"\n#include \"debugfs.h\"\n\nstatic u64 pds_vdpa_get_driver_features(struct vdpa_device *vdpa_dev);\n\nstatic struct pds_vdpa_device *vdpa_to_pdsv(struct vdpa_device *vdpa_dev)\n{\n\treturn container_of(vdpa_dev, struct pds_vdpa_device, vdpa_dev);\n}\n\nstatic int pds_vdpa_notify_handler(struct notifier_block *nb,\n\t\t\t\t   unsigned long ecode,\n\t\t\t\t   void *data)\n{\n\tstruct pds_vdpa_device *pdsv = container_of(nb, struct pds_vdpa_device, nb);\n\tstruct device *dev = &pdsv->vdpa_aux->padev->aux_dev.dev;\n\n\tdev_dbg(dev, \"%s: event code %lu\\n\", __func__, ecode);\n\n\tif (ecode == PDS_EVENT_RESET || ecode == PDS_EVENT_LINK_CHANGE) {\n\t\tif (pdsv->config_cb.callback)\n\t\t\tpdsv->config_cb.callback(pdsv->config_cb.private);\n\t}\n\n\treturn 0;\n}\n\nstatic int pds_vdpa_register_event_handler(struct pds_vdpa_device *pdsv)\n{\n\tstruct device *dev = &pdsv->vdpa_aux->padev->aux_dev.dev;\n\tstruct notifier_block *nb = &pdsv->nb;\n\tint err;\n\n\tif (!nb->notifier_call) {\n\t\tnb->notifier_call = pds_vdpa_notify_handler;\n\t\terr = pdsc_register_notify(nb);\n\t\tif (err) {\n\t\t\tnb->notifier_call = NULL;\n\t\t\tdev_err(dev, \"failed to register pds event handler: %ps\\n\",\n\t\t\t\tERR_PTR(err));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdev_dbg(dev, \"pds event handler registered\\n\");\n\t}\n\n\treturn 0;\n}\n\nstatic void pds_vdpa_unregister_event_handler(struct pds_vdpa_device *pdsv)\n{\n\tif (pdsv->nb.notifier_call) {\n\t\tpdsc_unregister_notify(&pdsv->nb);\n\t\tpdsv->nb.notifier_call = NULL;\n\t}\n}\n\nstatic int pds_vdpa_set_vq_address(struct vdpa_device *vdpa_dev, u16 qid,\n\t\t\t\t   u64 desc_addr, u64 driver_addr, u64 device_addr)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\tpdsv->vqs[qid].desc_addr = desc_addr;\n\tpdsv->vqs[qid].avail_addr = driver_addr;\n\tpdsv->vqs[qid].used_addr = device_addr;\n\n\treturn 0;\n}\n\nstatic void pds_vdpa_set_vq_num(struct vdpa_device *vdpa_dev, u16 qid, u32 num)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\tpdsv->vqs[qid].q_len = num;\n}\n\nstatic void pds_vdpa_kick_vq(struct vdpa_device *vdpa_dev, u16 qid)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\tiowrite16(qid, pdsv->vqs[qid].notify);\n}\n\nstatic void pds_vdpa_set_vq_cb(struct vdpa_device *vdpa_dev, u16 qid,\n\t\t\t       struct vdpa_callback *cb)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\tpdsv->vqs[qid].event_cb = *cb;\n}\n\nstatic irqreturn_t pds_vdpa_isr(int irq, void *data)\n{\n\tstruct pds_vdpa_vq_info *vq;\n\n\tvq = data;\n\tif (vq->event_cb.callback)\n\t\tvq->event_cb.callback(vq->event_cb.private);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void pds_vdpa_release_irq(struct pds_vdpa_device *pdsv, int qid)\n{\n\tif (pdsv->vqs[qid].irq == VIRTIO_MSI_NO_VECTOR)\n\t\treturn;\n\n\tfree_irq(pdsv->vqs[qid].irq, &pdsv->vqs[qid]);\n\tpdsv->vqs[qid].irq = VIRTIO_MSI_NO_VECTOR;\n}\n\nstatic void pds_vdpa_set_vq_ready(struct vdpa_device *vdpa_dev, u16 qid, bool ready)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tstruct device *dev = &pdsv->vdpa_dev.dev;\n\tu64 driver_features;\n\tu16 invert_idx = 0;\n\tint err;\n\n\tdev_dbg(dev, \"%s: qid %d ready %d => %d\\n\",\n\t\t__func__, qid, pdsv->vqs[qid].ready, ready);\n\tif (ready == pdsv->vqs[qid].ready)\n\t\treturn;\n\n\tdriver_features = pds_vdpa_get_driver_features(vdpa_dev);\n\tif (driver_features & BIT_ULL(VIRTIO_F_RING_PACKED))\n\t\tinvert_idx = PDS_VDPA_PACKED_INVERT_IDX;\n\n\tif (ready) {\n\t\t \n\t\terr = pds_vdpa_cmd_init_vq(pdsv, qid, invert_idx, &pdsv->vqs[qid]);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"Failed to init vq %d: %pe\\n\",\n\t\t\t\tqid, ERR_PTR(err));\n\t\t\tready = false;\n\t\t}\n\t} else {\n\t\terr = pds_vdpa_cmd_reset_vq(pdsv, qid, invert_idx, &pdsv->vqs[qid]);\n\t\tif (err)\n\t\t\tdev_err(dev, \"%s: reset_vq failed qid %d: %pe\\n\",\n\t\t\t\t__func__, qid, ERR_PTR(err));\n\t}\n\n\tpdsv->vqs[qid].ready = ready;\n}\n\nstatic bool pds_vdpa_get_vq_ready(struct vdpa_device *vdpa_dev, u16 qid)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\treturn pdsv->vqs[qid].ready;\n}\n\nstatic int pds_vdpa_set_vq_state(struct vdpa_device *vdpa_dev, u16 qid,\n\t\t\t\t const struct vdpa_vq_state *state)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tstruct pds_auxiliary_dev *padev = pdsv->vdpa_aux->padev;\n\tstruct device *dev = &padev->aux_dev.dev;\n\tu64 driver_features;\n\tu16 avail;\n\tu16 used;\n\n\tif (pdsv->vqs[qid].ready) {\n\t\tdev_err(dev, \"Setting device position is denied while vq is enabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdriver_features = pds_vdpa_get_driver_features(vdpa_dev);\n\tif (driver_features & BIT_ULL(VIRTIO_F_RING_PACKED)) {\n\t\tavail = state->packed.last_avail_idx |\n\t\t\t(state->packed.last_avail_counter << 15);\n\t\tused = state->packed.last_used_idx |\n\t\t       (state->packed.last_used_counter << 15);\n\n\t\t \n\t\tavail ^= PDS_VDPA_PACKED_INVERT_IDX;\n\t\tused ^= PDS_VDPA_PACKED_INVERT_IDX;\n\t} else {\n\t\tavail = state->split.avail_index;\n\t\t \n\t\tused = avail;\n\t}\n\n\tif (used != avail) {\n\t\tdev_dbg(dev, \"Setting used equal to avail, for interoperability\\n\");\n\t\tused = avail;\n\t}\n\n\tpdsv->vqs[qid].avail_idx = avail;\n\tpdsv->vqs[qid].used_idx = used;\n\n\treturn 0;\n}\n\nstatic int pds_vdpa_get_vq_state(struct vdpa_device *vdpa_dev, u16 qid,\n\t\t\t\t struct vdpa_vq_state *state)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tstruct pds_auxiliary_dev *padev = pdsv->vdpa_aux->padev;\n\tstruct device *dev = &padev->aux_dev.dev;\n\tu64 driver_features;\n\tu16 avail;\n\tu16 used;\n\n\tif (pdsv->vqs[qid].ready) {\n\t\tdev_err(dev, \"Getting device position is denied while vq is enabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tavail = pdsv->vqs[qid].avail_idx;\n\tused = pdsv->vqs[qid].used_idx;\n\n\tdriver_features = pds_vdpa_get_driver_features(vdpa_dev);\n\tif (driver_features & BIT_ULL(VIRTIO_F_RING_PACKED)) {\n\t\tavail ^= PDS_VDPA_PACKED_INVERT_IDX;\n\t\tused ^= PDS_VDPA_PACKED_INVERT_IDX;\n\n\t\tstate->packed.last_avail_idx = avail & 0x7fff;\n\t\tstate->packed.last_avail_counter = avail >> 15;\n\t\tstate->packed.last_used_idx = used & 0x7fff;\n\t\tstate->packed.last_used_counter = used >> 15;\n\t} else {\n\t\tstate->split.avail_index = avail;\n\t\t \n\t}\n\n\treturn 0;\n}\n\nstatic struct vdpa_notification_area\npds_vdpa_get_vq_notification(struct vdpa_device *vdpa_dev, u16 qid)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tstruct virtio_pci_modern_device *vd_mdev;\n\tstruct vdpa_notification_area area;\n\n\tarea.addr = pdsv->vqs[qid].notify_pa;\n\n\tvd_mdev = &pdsv->vdpa_aux->vd_mdev;\n\tif (!vd_mdev->notify_offset_multiplier)\n\t\tarea.size = PDS_PAGE_SIZE;\n\telse\n\t\tarea.size = vd_mdev->notify_offset_multiplier;\n\n\treturn area;\n}\n\nstatic int pds_vdpa_get_vq_irq(struct vdpa_device *vdpa_dev, u16 qid)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\treturn pdsv->vqs[qid].irq;\n}\n\nstatic u32 pds_vdpa_get_vq_align(struct vdpa_device *vdpa_dev)\n{\n\treturn PDS_PAGE_SIZE;\n}\n\nstatic u32 pds_vdpa_get_vq_group(struct vdpa_device *vdpa_dev, u16 idx)\n{\n\treturn 0;\n}\n\nstatic u64 pds_vdpa_get_device_features(struct vdpa_device *vdpa_dev)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\treturn pdsv->supported_features;\n}\n\nstatic int pds_vdpa_set_driver_features(struct vdpa_device *vdpa_dev, u64 features)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tstruct device *dev = &pdsv->vdpa_dev.dev;\n\tu64 driver_features;\n\tu64 nego_features;\n\tu64 hw_features;\n\tu64 missing;\n\n\tif (!(features & BIT_ULL(VIRTIO_F_ACCESS_PLATFORM)) && features) {\n\t\tdev_err(dev, \"VIRTIO_F_ACCESS_PLATFORM is not negotiated\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tnego_features = features & pdsv->supported_features;\n\tmissing = features & ~nego_features;\n\tif (missing) {\n\t\tdev_err(dev, \"Can't support all requested features in %#llx, missing %#llx features\\n\",\n\t\t\tfeatures, missing);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tdriver_features = pds_vdpa_get_driver_features(vdpa_dev);\n\tpdsv->negotiated_features = nego_features;\n\tdev_dbg(dev, \"%s: %#llx => %#llx\\n\",\n\t\t__func__, driver_features, nego_features);\n\n\t \n\thw_features = le64_to_cpu(pdsv->vdpa_aux->ident.hw_features);\n\tif (!(hw_features & BIT_ULL(VIRTIO_NET_F_MAC)))\n\t\tnego_features &= ~BIT_ULL(VIRTIO_NET_F_MAC);\n\n\tif (driver_features == nego_features)\n\t\treturn 0;\n\n\tvp_modern_set_features(&pdsv->vdpa_aux->vd_mdev, nego_features);\n\n\treturn 0;\n}\n\nstatic u64 pds_vdpa_get_driver_features(struct vdpa_device *vdpa_dev)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\treturn pdsv->negotiated_features;\n}\n\nstatic void pds_vdpa_set_config_cb(struct vdpa_device *vdpa_dev,\n\t\t\t\t   struct vdpa_callback *cb)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\tpdsv->config_cb.callback = cb->callback;\n\tpdsv->config_cb.private = cb->private;\n}\n\nstatic u16 pds_vdpa_get_vq_num_max(struct vdpa_device *vdpa_dev)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\t \n\treturn min_t(u16, 1024, BIT(le16_to_cpu(pdsv->vdpa_aux->ident.max_qlen)));\n}\n\nstatic u32 pds_vdpa_get_device_id(struct vdpa_device *vdpa_dev)\n{\n\treturn VIRTIO_ID_NET;\n}\n\nstatic u32 pds_vdpa_get_vendor_id(struct vdpa_device *vdpa_dev)\n{\n\treturn PCI_VENDOR_ID_PENSANDO;\n}\n\nstatic u8 pds_vdpa_get_status(struct vdpa_device *vdpa_dev)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\n\treturn vp_modern_get_status(&pdsv->vdpa_aux->vd_mdev);\n}\n\nstatic int pds_vdpa_request_irqs(struct pds_vdpa_device *pdsv)\n{\n\tstruct pci_dev *pdev = pdsv->vdpa_aux->padev->vf_pdev;\n\tstruct pds_vdpa_aux *vdpa_aux = pdsv->vdpa_aux;\n\tstruct device *dev = &pdsv->vdpa_dev.dev;\n\tint max_vq, nintrs, qid, err;\n\n\tmax_vq = vdpa_aux->vdpa_mdev.max_supported_vqs;\n\n\tnintrs = pci_alloc_irq_vectors(pdev, max_vq, max_vq, PCI_IRQ_MSIX);\n\tif (nintrs < 0) {\n\t\tdev_err(dev, \"Couldn't get %d msix vectors: %pe\\n\",\n\t\t\tmax_vq, ERR_PTR(nintrs));\n\t\treturn nintrs;\n\t}\n\n\tfor (qid = 0; qid < pdsv->num_vqs; ++qid) {\n\t\tint irq = pci_irq_vector(pdev, qid);\n\n\t\tsnprintf(pdsv->vqs[qid].irq_name, sizeof(pdsv->vqs[qid].irq_name),\n\t\t\t \"vdpa-%s-%d\", dev_name(dev), qid);\n\n\t\terr = request_irq(irq, pds_vdpa_isr, 0,\n\t\t\t\t  pdsv->vqs[qid].irq_name,\n\t\t\t\t  &pdsv->vqs[qid]);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"%s: no irq for qid %d: %pe\\n\",\n\t\t\t\t__func__, qid, ERR_PTR(err));\n\t\t\tgoto err_release;\n\t\t}\n\n\t\tpdsv->vqs[qid].irq = irq;\n\t}\n\n\tvdpa_aux->nintrs = nintrs;\n\n\treturn 0;\n\nerr_release:\n\twhile (qid--)\n\t\tpds_vdpa_release_irq(pdsv, qid);\n\n\tpci_free_irq_vectors(pdev);\n\n\tvdpa_aux->nintrs = 0;\n\n\treturn err;\n}\n\nstatic void pds_vdpa_release_irqs(struct pds_vdpa_device *pdsv)\n{\n\tstruct pci_dev *pdev = pdsv->vdpa_aux->padev->vf_pdev;\n\tstruct pds_vdpa_aux *vdpa_aux = pdsv->vdpa_aux;\n\tint qid;\n\n\tif (!vdpa_aux->nintrs)\n\t\treturn;\n\n\tfor (qid = 0; qid < pdsv->num_vqs; qid++)\n\t\tpds_vdpa_release_irq(pdsv, qid);\n\n\tpci_free_irq_vectors(pdev);\n\n\tvdpa_aux->nintrs = 0;\n}\n\nstatic void pds_vdpa_set_status(struct vdpa_device *vdpa_dev, u8 status)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tstruct device *dev = &pdsv->vdpa_dev.dev;\n\tu8 old_status;\n\tint i;\n\n\told_status = pds_vdpa_get_status(vdpa_dev);\n\tdev_dbg(dev, \"%s: old %#x new %#x\\n\", __func__, old_status, status);\n\n\tif (status & ~old_status & VIRTIO_CONFIG_S_DRIVER_OK) {\n\t\tif (pds_vdpa_request_irqs(pdsv))\n\t\t\tstatus = old_status | VIRTIO_CONFIG_S_FAILED;\n\t}\n\n\tpds_vdpa_cmd_set_status(pdsv, status);\n\n\tif (status == 0) {\n\t\tstruct vdpa_callback null_cb = { };\n\n\t\tpds_vdpa_set_config_cb(vdpa_dev, &null_cb);\n\t\tpds_vdpa_cmd_reset(pdsv);\n\n\t\tfor (i = 0; i < pdsv->num_vqs; i++) {\n\t\t\tpdsv->vqs[i].avail_idx = 0;\n\t\t\tpdsv->vqs[i].used_idx = 0;\n\t\t}\n\n\t\tpds_vdpa_cmd_set_mac(pdsv, pdsv->mac);\n\t}\n\n\tif (status & ~old_status & VIRTIO_CONFIG_S_FEATURES_OK) {\n\t\tfor (i = 0; i < pdsv->num_vqs; i++) {\n\t\t\tpdsv->vqs[i].notify =\n\t\t\t\tvp_modern_map_vq_notify(&pdsv->vdpa_aux->vd_mdev,\n\t\t\t\t\t\t\ti, &pdsv->vqs[i].notify_pa);\n\t\t}\n\t}\n\n\tif (old_status & ~status & VIRTIO_CONFIG_S_DRIVER_OK)\n\t\tpds_vdpa_release_irqs(pdsv);\n}\n\nstatic void pds_vdpa_init_vqs_entry(struct pds_vdpa_device *pdsv, int qid,\n\t\t\t\t    void __iomem *notify)\n{\n\tmemset(&pdsv->vqs[qid], 0, sizeof(pdsv->vqs[0]));\n\tpdsv->vqs[qid].qid = qid;\n\tpdsv->vqs[qid].pdsv = pdsv;\n\tpdsv->vqs[qid].ready = false;\n\tpdsv->vqs[qid].irq = VIRTIO_MSI_NO_VECTOR;\n\tpdsv->vqs[qid].notify = notify;\n}\n\nstatic int pds_vdpa_reset(struct vdpa_device *vdpa_dev)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tstruct device *dev;\n\tint err = 0;\n\tu8 status;\n\tint i;\n\n\tdev = &pdsv->vdpa_aux->padev->aux_dev.dev;\n\tstatus = pds_vdpa_get_status(vdpa_dev);\n\n\tif (status == 0)\n\t\treturn 0;\n\n\tif (status & VIRTIO_CONFIG_S_DRIVER_OK) {\n\t\t \n\t\tfor (i = 0; i < pdsv->num_vqs && !err; i++) {\n\t\t\terr = pds_vdpa_cmd_reset_vq(pdsv, i, 0, &pdsv->vqs[i]);\n\t\t\tif (err)\n\t\t\t\tdev_err(dev, \"%s: reset_vq failed qid %d: %pe\\n\",\n\t\t\t\t\t__func__, i, ERR_PTR(err));\n\t\t}\n\t}\n\n\tpds_vdpa_set_status(vdpa_dev, 0);\n\n\tif (status & VIRTIO_CONFIG_S_DRIVER_OK) {\n\t\t \n\t\tfor (i = 0; i < pdsv->num_vqs && !err; i++)\n\t\t\tpds_vdpa_init_vqs_entry(pdsv, i, pdsv->vqs[i].notify);\n\t}\n\n\treturn 0;\n}\n\nstatic size_t pds_vdpa_get_config_size(struct vdpa_device *vdpa_dev)\n{\n\treturn sizeof(struct virtio_net_config);\n}\n\nstatic void pds_vdpa_get_config(struct vdpa_device *vdpa_dev,\n\t\t\t\tunsigned int offset,\n\t\t\t\tvoid *buf, unsigned int len)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tvoid __iomem *device;\n\n\tif (offset + len > sizeof(struct virtio_net_config)) {\n\t\tWARN(true, \"%s: bad read, offset %d len %d\\n\", __func__, offset, len);\n\t\treturn;\n\t}\n\n\tdevice = pdsv->vdpa_aux->vd_mdev.device;\n\tmemcpy_fromio(buf, device + offset, len);\n}\n\nstatic void pds_vdpa_set_config(struct vdpa_device *vdpa_dev,\n\t\t\t\tunsigned int offset, const void *buf,\n\t\t\t\tunsigned int len)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tvoid __iomem *device;\n\n\tif (offset + len > sizeof(struct virtio_net_config)) {\n\t\tWARN(true, \"%s: bad read, offset %d len %d\\n\", __func__, offset, len);\n\t\treturn;\n\t}\n\n\tdevice = pdsv->vdpa_aux->vd_mdev.device;\n\tmemcpy_toio(device + offset, buf, len);\n}\n\nstatic const struct vdpa_config_ops pds_vdpa_ops = {\n\t.set_vq_address\t\t= pds_vdpa_set_vq_address,\n\t.set_vq_num\t\t= pds_vdpa_set_vq_num,\n\t.kick_vq\t\t= pds_vdpa_kick_vq,\n\t.set_vq_cb\t\t= pds_vdpa_set_vq_cb,\n\t.set_vq_ready\t\t= pds_vdpa_set_vq_ready,\n\t.get_vq_ready\t\t= pds_vdpa_get_vq_ready,\n\t.set_vq_state\t\t= pds_vdpa_set_vq_state,\n\t.get_vq_state\t\t= pds_vdpa_get_vq_state,\n\t.get_vq_notification\t= pds_vdpa_get_vq_notification,\n\t.get_vq_irq\t\t= pds_vdpa_get_vq_irq,\n\t.get_vq_align\t\t= pds_vdpa_get_vq_align,\n\t.get_vq_group\t\t= pds_vdpa_get_vq_group,\n\n\t.get_device_features\t= pds_vdpa_get_device_features,\n\t.set_driver_features\t= pds_vdpa_set_driver_features,\n\t.get_driver_features\t= pds_vdpa_get_driver_features,\n\t.set_config_cb\t\t= pds_vdpa_set_config_cb,\n\t.get_vq_num_max\t\t= pds_vdpa_get_vq_num_max,\n\t.get_device_id\t\t= pds_vdpa_get_device_id,\n\t.get_vendor_id\t\t= pds_vdpa_get_vendor_id,\n\t.get_status\t\t= pds_vdpa_get_status,\n\t.set_status\t\t= pds_vdpa_set_status,\n\t.reset\t\t\t= pds_vdpa_reset,\n\t.get_config_size\t= pds_vdpa_get_config_size,\n\t.get_config\t\t= pds_vdpa_get_config,\n\t.set_config\t\t= pds_vdpa_set_config,\n};\nstatic struct virtio_device_id pds_vdpa_id_table[] = {\n\t{VIRTIO_ID_NET, VIRTIO_DEV_ANY_ID},\n\t{0},\n};\n\nstatic int pds_vdpa_dev_add(struct vdpa_mgmt_dev *mdev, const char *name,\n\t\t\t    const struct vdpa_dev_set_config *add_config)\n{\n\tstruct pds_vdpa_aux *vdpa_aux;\n\tstruct pds_vdpa_device *pdsv;\n\tstruct vdpa_mgmt_dev *mgmt;\n\tu16 fw_max_vqs, vq_pairs;\n\tstruct device *dma_dev;\n\tstruct pci_dev *pdev;\n\tstruct device *dev;\n\tint err;\n\tint i;\n\n\tvdpa_aux = container_of(mdev, struct pds_vdpa_aux, vdpa_mdev);\n\tdev = &vdpa_aux->padev->aux_dev.dev;\n\tmgmt = &vdpa_aux->vdpa_mdev;\n\n\tif (vdpa_aux->pdsv) {\n\t\tdev_warn(dev, \"Multiple vDPA devices on a VF is not supported.\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tpdsv = vdpa_alloc_device(struct pds_vdpa_device, vdpa_dev,\n\t\t\t\t dev, &pds_vdpa_ops, 1, 1, name, false);\n\tif (IS_ERR(pdsv)) {\n\t\tdev_err(dev, \"Failed to allocate vDPA structure: %pe\\n\", pdsv);\n\t\treturn PTR_ERR(pdsv);\n\t}\n\n\tvdpa_aux->pdsv = pdsv;\n\tpdsv->vdpa_aux = vdpa_aux;\n\n\tpdev = vdpa_aux->padev->vf_pdev;\n\tdma_dev = &pdev->dev;\n\tpdsv->vdpa_dev.dma_dev = dma_dev;\n\n\tpdsv->supported_features = mgmt->supported_features;\n\n\tif (add_config->mask & BIT_ULL(VDPA_ATTR_DEV_FEATURES)) {\n\t\tu64 unsupp_features =\n\t\t\tadd_config->device_features & ~pdsv->supported_features;\n\n\t\tif (unsupp_features) {\n\t\t\tdev_err(dev, \"Unsupported features: %#llx\\n\", unsupp_features);\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto err_unmap;\n\t\t}\n\n\t\tpdsv->supported_features = add_config->device_features;\n\t}\n\n\terr = pds_vdpa_cmd_reset(pdsv);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to reset hw: %pe\\n\", ERR_PTR(err));\n\t\tgoto err_unmap;\n\t}\n\n\terr = pds_vdpa_init_hw(pdsv);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to init hw: %pe\\n\", ERR_PTR(err));\n\t\tgoto err_unmap;\n\t}\n\n\tfw_max_vqs = le16_to_cpu(pdsv->vdpa_aux->ident.max_vqs);\n\tvq_pairs = fw_max_vqs / 2;\n\n\t \n\tif (add_config->mask & (1 << VDPA_ATTR_DEV_NET_CFG_MAX_VQP))\n\t\tvq_pairs = add_config->net.max_vq_pairs;\n\n\tpdsv->num_vqs = 2 * vq_pairs;\n\tif (pdsv->supported_features & BIT_ULL(VIRTIO_NET_F_CTRL_VQ))\n\t\tpdsv->num_vqs++;\n\n\tif (pdsv->num_vqs > fw_max_vqs) {\n\t\tdev_err(dev, \"%s: queue count requested %u greater than max %u\\n\",\n\t\t\t__func__, pdsv->num_vqs, fw_max_vqs);\n\t\terr = -ENOSPC;\n\t\tgoto err_unmap;\n\t}\n\n\tif (pdsv->num_vqs != fw_max_vqs) {\n\t\terr = pds_vdpa_cmd_set_max_vq_pairs(pdsv, vq_pairs);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"Failed to set max_vq_pairs: %pe\\n\",\n\t\t\t\tERR_PTR(err));\n\t\t\tgoto err_unmap;\n\t\t}\n\t}\n\n\t \n\tif (add_config->mask & BIT_ULL(VDPA_ATTR_DEV_NET_CFG_MACADDR)) {\n\t\tether_addr_copy(pdsv->mac, add_config->net.mac);\n\t} else {\n\t\tstruct virtio_net_config __iomem *vc;\n\n\t\tvc = pdsv->vdpa_aux->vd_mdev.device;\n\t\tmemcpy_fromio(pdsv->mac, vc->mac, sizeof(pdsv->mac));\n\t\tif (is_zero_ether_addr(pdsv->mac) &&\n\t\t    (pdsv->supported_features & BIT_ULL(VIRTIO_NET_F_MAC))) {\n\t\t\teth_random_addr(pdsv->mac);\n\t\t\tdev_info(dev, \"setting random mac %pM\\n\", pdsv->mac);\n\t\t}\n\t}\n\tpds_vdpa_cmd_set_mac(pdsv, pdsv->mac);\n\n\tfor (i = 0; i < pdsv->num_vqs; i++) {\n\t\tvoid __iomem *notify;\n\n\t\tnotify = vp_modern_map_vq_notify(&pdsv->vdpa_aux->vd_mdev,\n\t\t\t\t\t\t i, &pdsv->vqs[i].notify_pa);\n\t\tpds_vdpa_init_vqs_entry(pdsv, i, notify);\n\t}\n\n\tpdsv->vdpa_dev.mdev = &vdpa_aux->vdpa_mdev;\n\n\terr = pds_vdpa_register_event_handler(pdsv);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to register for PDS events: %pe\\n\", ERR_PTR(err));\n\t\tgoto err_unmap;\n\t}\n\n\t \n\terr = _vdpa_register_device(&pdsv->vdpa_dev, pdsv->num_vqs);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to register to vDPA bus: %pe\\n\", ERR_PTR(err));\n\t\tgoto err_unevent;\n\t}\n\n\tpds_vdpa_debugfs_add_vdpadev(vdpa_aux);\n\n\treturn 0;\n\nerr_unevent:\n\tpds_vdpa_unregister_event_handler(pdsv);\nerr_unmap:\n\tput_device(&pdsv->vdpa_dev.dev);\n\tvdpa_aux->pdsv = NULL;\n\treturn err;\n}\n\nstatic void pds_vdpa_dev_del(struct vdpa_mgmt_dev *mdev,\n\t\t\t     struct vdpa_device *vdpa_dev)\n{\n\tstruct pds_vdpa_device *pdsv = vdpa_to_pdsv(vdpa_dev);\n\tstruct pds_vdpa_aux *vdpa_aux;\n\n\tpds_vdpa_unregister_event_handler(pdsv);\n\n\tvdpa_aux = container_of(mdev, struct pds_vdpa_aux, vdpa_mdev);\n\t_vdpa_unregister_device(vdpa_dev);\n\n\tpds_vdpa_cmd_reset(vdpa_aux->pdsv);\n\tpds_vdpa_debugfs_reset_vdpadev(vdpa_aux);\n\n\tvdpa_aux->pdsv = NULL;\n\n\tdev_info(&vdpa_aux->padev->aux_dev.dev, \"Removed vdpa device\\n\");\n}\n\nstatic const struct vdpa_mgmtdev_ops pds_vdpa_mgmt_dev_ops = {\n\t.dev_add = pds_vdpa_dev_add,\n\t.dev_del = pds_vdpa_dev_del\n};\n\nint pds_vdpa_get_mgmt_info(struct pds_vdpa_aux *vdpa_aux)\n{\n\tunion pds_core_adminq_cmd cmd = {\n\t\t.vdpa_ident.opcode = PDS_VDPA_CMD_IDENT,\n\t\t.vdpa_ident.vf_id = cpu_to_le16(vdpa_aux->vf_id),\n\t};\n\tunion pds_core_adminq_comp comp = {};\n\tstruct vdpa_mgmt_dev *mgmt;\n\tstruct pci_dev *pf_pdev;\n\tstruct device *pf_dev;\n\tstruct pci_dev *pdev;\n\tdma_addr_t ident_pa;\n\tstruct device *dev;\n\tu16 dev_intrs;\n\tu16 max_vqs;\n\tint err;\n\n\tdev = &vdpa_aux->padev->aux_dev.dev;\n\tpdev = vdpa_aux->padev->vf_pdev;\n\tmgmt = &vdpa_aux->vdpa_mdev;\n\n\t \n\tpf_pdev = pci_physfn(vdpa_aux->padev->vf_pdev);\n\tpf_dev = &pf_pdev->dev;\n\tident_pa = dma_map_single(pf_dev, &vdpa_aux->ident,\n\t\t\t\t  sizeof(vdpa_aux->ident), DMA_FROM_DEVICE);\n\tif (dma_mapping_error(pf_dev, ident_pa)) {\n\t\tdev_err(dev, \"Failed to map ident space\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tcmd.vdpa_ident.ident_pa = cpu_to_le64(ident_pa);\n\tcmd.vdpa_ident.len = cpu_to_le32(sizeof(vdpa_aux->ident));\n\terr = pds_client_adminq_cmd(vdpa_aux->padev, &cmd,\n\t\t\t\t    sizeof(cmd.vdpa_ident), &comp, 0);\n\tdma_unmap_single(pf_dev, ident_pa,\n\t\t\t sizeof(vdpa_aux->ident), DMA_FROM_DEVICE);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to ident hw, status %d: %pe\\n\",\n\t\t\tcomp.status, ERR_PTR(err));\n\t\treturn err;\n\t}\n\n\tmax_vqs = le16_to_cpu(vdpa_aux->ident.max_vqs);\n\tdev_intrs = pci_msix_vec_count(pdev);\n\tdev_dbg(dev, \"ident.max_vqs %d dev_intrs %d\\n\", max_vqs, dev_intrs);\n\n\tmax_vqs = min_t(u16, dev_intrs, max_vqs);\n\tmgmt->max_supported_vqs = min_t(u16, PDS_VDPA_MAX_QUEUES, max_vqs);\n\tvdpa_aux->nintrs = 0;\n\n\tmgmt->ops = &pds_vdpa_mgmt_dev_ops;\n\tmgmt->id_table = pds_vdpa_id_table;\n\tmgmt->device = dev;\n\tmgmt->supported_features = le64_to_cpu(vdpa_aux->ident.hw_features);\n\n\t \n\tmgmt->supported_features |= BIT_ULL(VIRTIO_NET_F_MAC);\n\n\tmgmt->config_attr_mask = BIT_ULL(VDPA_ATTR_DEV_NET_CFG_MACADDR);\n\tmgmt->config_attr_mask |= BIT_ULL(VDPA_ATTR_DEV_NET_CFG_MAX_VQP);\n\tmgmt->config_attr_mask |= BIT_ULL(VDPA_ATTR_DEV_FEATURES);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}