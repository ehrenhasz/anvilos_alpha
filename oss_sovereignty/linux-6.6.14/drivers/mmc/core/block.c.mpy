{
  "module_name": "block.c",
  "hash_id": "354d590010032d2dcaa0adbc334df669b132180f48ff2365ad13064d7ebbd03d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/mmc/core/block.c",
  "human_readable_source": "\n \n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/init.h>\n\n#include <linux/kernel.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/errno.h>\n#include <linux/hdreg.h>\n#include <linux/kdev_t.h>\n#include <linux/kref.h>\n#include <linux/blkdev.h>\n#include <linux/cdev.h>\n#include <linux/mutex.h>\n#include <linux/scatterlist.h>\n#include <linux/string_helpers.h>\n#include <linux/delay.h>\n#include <linux/capability.h>\n#include <linux/compat.h>\n#include <linux/pm_runtime.h>\n#include <linux/idr.h>\n#include <linux/debugfs.h>\n\n#include <linux/mmc/ioctl.h>\n#include <linux/mmc/card.h>\n#include <linux/mmc/host.h>\n#include <linux/mmc/mmc.h>\n#include <linux/mmc/sd.h>\n\n#include <linux/uaccess.h>\n\n#include \"queue.h\"\n#include \"block.h\"\n#include \"core.h\"\n#include \"card.h\"\n#include \"crypto.h\"\n#include \"host.h\"\n#include \"bus.h\"\n#include \"mmc_ops.h\"\n#include \"quirks.h\"\n#include \"sd_ops.h\"\n\nMODULE_ALIAS(\"mmc:block\");\n#ifdef MODULE_PARAM_PREFIX\n#undef MODULE_PARAM_PREFIX\n#endif\n#define MODULE_PARAM_PREFIX \"mmcblk.\"\n\n \n#define MMC_BLK_TIMEOUT_MS  (10 * 1000)\n#define MMC_EXTRACT_INDEX_FROM_ARG(x) ((x & 0x00FF0000) >> 16)\n#define MMC_EXTRACT_VALUE_FROM_ARG(x) ((x & 0x0000FF00) >> 8)\n\nstatic DEFINE_MUTEX(block_mutex);\n\n \nstatic int perdev_minors = CONFIG_MMC_BLOCK_MINORS;\n\n \nstatic int max_devices;\n\n#define MAX_DEVICES 256\n\nstatic DEFINE_IDA(mmc_blk_ida);\nstatic DEFINE_IDA(mmc_rpmb_ida);\n\nstruct mmc_blk_busy_data {\n\tstruct mmc_card *card;\n\tu32 status;\n};\n\n \nstruct mmc_blk_data {\n\tstruct device\t*parent;\n\tstruct gendisk\t*disk;\n\tstruct mmc_queue queue;\n\tstruct list_head part;\n\tstruct list_head rpmbs;\n\n\tunsigned int\tflags;\n#define MMC_BLK_CMD23\t(1 << 0)\t \n#define MMC_BLK_REL_WR\t(1 << 1)\t \n\n\tstruct kref\tkref;\n\tunsigned int\tread_only;\n\tunsigned int\tpart_type;\n\tunsigned int\treset_done;\n#define MMC_BLK_READ\t\tBIT(0)\n#define MMC_BLK_WRITE\t\tBIT(1)\n#define MMC_BLK_DISCARD\t\tBIT(2)\n#define MMC_BLK_SECDISCARD\tBIT(3)\n#define MMC_BLK_CQE_RECOVERY\tBIT(4)\n#define MMC_BLK_TRIM\t\tBIT(5)\n\n\t \n\tunsigned int\tpart_curr;\n#define MMC_BLK_PART_INVALID\tUINT_MAX\t \n\tint\tarea_type;\n\n\t \n\tstruct dentry *status_dentry;\n\tstruct dentry *ext_csd_dentry;\n};\n\n \nstatic dev_t mmc_rpmb_devt;\n\n \nstatic struct bus_type mmc_rpmb_bus_type = {\n\t.name = \"mmc_rpmb\",\n};\n\n \nstruct mmc_rpmb_data {\n\tstruct device dev;\n\tstruct cdev chrdev;\n\tint id;\n\tunsigned int part_index;\n\tstruct mmc_blk_data *md;\n\tstruct list_head node;\n};\n\nstatic DEFINE_MUTEX(open_lock);\n\nmodule_param(perdev_minors, int, 0444);\nMODULE_PARM_DESC(perdev_minors, \"Minors numbers to allocate per device\");\n\nstatic inline int mmc_blk_part_switch(struct mmc_card *card,\n\t\t\t\t      unsigned int part_type);\nstatic void mmc_blk_rw_rq_prep(struct mmc_queue_req *mqrq,\n\t\t\t       struct mmc_card *card,\n\t\t\t       int recovery_mode,\n\t\t\t       struct mmc_queue *mq);\nstatic void mmc_blk_hsq_req_done(struct mmc_request *mrq);\nstatic int mmc_spi_err_check(struct mmc_card *card);\nstatic int mmc_blk_busy_cb(void *cb_data, bool *busy);\n\nstatic struct mmc_blk_data *mmc_blk_get(struct gendisk *disk)\n{\n\tstruct mmc_blk_data *md;\n\n\tmutex_lock(&open_lock);\n\tmd = disk->private_data;\n\tif (md && !kref_get_unless_zero(&md->kref))\n\t\tmd = NULL;\n\tmutex_unlock(&open_lock);\n\n\treturn md;\n}\n\nstatic inline int mmc_get_devidx(struct gendisk *disk)\n{\n\tint devidx = disk->first_minor / perdev_minors;\n\treturn devidx;\n}\n\nstatic void mmc_blk_kref_release(struct kref *ref)\n{\n\tstruct mmc_blk_data *md = container_of(ref, struct mmc_blk_data, kref);\n\tint devidx;\n\n\tdevidx = mmc_get_devidx(md->disk);\n\tida_simple_remove(&mmc_blk_ida, devidx);\n\n\tmutex_lock(&open_lock);\n\tmd->disk->private_data = NULL;\n\tmutex_unlock(&open_lock);\n\n\tput_disk(md->disk);\n\tkfree(md);\n}\n\nstatic void mmc_blk_put(struct mmc_blk_data *md)\n{\n\tkref_put(&md->kref, mmc_blk_kref_release);\n}\n\nstatic ssize_t power_ro_lock_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tint ret;\n\tstruct mmc_blk_data *md = mmc_blk_get(dev_to_disk(dev));\n\tstruct mmc_card *card = md->queue.card;\n\tint locked = 0;\n\n\tif (card->ext_csd.boot_ro_lock & EXT_CSD_BOOT_WP_B_PERM_WP_EN)\n\t\tlocked = 2;\n\telse if (card->ext_csd.boot_ro_lock & EXT_CSD_BOOT_WP_B_PWR_WP_EN)\n\t\tlocked = 1;\n\n\tret = snprintf(buf, PAGE_SIZE, \"%d\\n\", locked);\n\n\tmmc_blk_put(md);\n\n\treturn ret;\n}\n\nstatic ssize_t power_ro_lock_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t count)\n{\n\tint ret;\n\tstruct mmc_blk_data *md, *part_md;\n\tstruct mmc_queue *mq;\n\tstruct request *req;\n\tunsigned long set;\n\n\tif (kstrtoul(buf, 0, &set))\n\t\treturn -EINVAL;\n\n\tif (set != 1)\n\t\treturn count;\n\n\tmd = mmc_blk_get(dev_to_disk(dev));\n\tmq = &md->queue;\n\n\t \n\treq = blk_mq_alloc_request(mq->queue, REQ_OP_DRV_OUT, 0);\n\tif (IS_ERR(req)) {\n\t\tcount = PTR_ERR(req);\n\t\tgoto out_put;\n\t}\n\treq_to_mmc_queue_req(req)->drv_op = MMC_DRV_OP_BOOT_WP;\n\treq_to_mmc_queue_req(req)->drv_op_result = -EIO;\n\tblk_execute_rq(req, false);\n\tret = req_to_mmc_queue_req(req)->drv_op_result;\n\tblk_mq_free_request(req);\n\n\tif (!ret) {\n\t\tpr_info(\"%s: Locking boot partition ro until next power on\\n\",\n\t\t\tmd->disk->disk_name);\n\t\tset_disk_ro(md->disk, 1);\n\n\t\tlist_for_each_entry(part_md, &md->part, part)\n\t\t\tif (part_md->area_type == MMC_BLK_DATA_AREA_BOOT) {\n\t\t\t\tpr_info(\"%s: Locking boot partition ro until next power on\\n\", part_md->disk->disk_name);\n\t\t\t\tset_disk_ro(part_md->disk, 1);\n\t\t\t}\n\t}\nout_put:\n\tmmc_blk_put(md);\n\treturn count;\n}\n\nstatic DEVICE_ATTR(ro_lock_until_next_power_on, 0,\n\t\tpower_ro_lock_show, power_ro_lock_store);\n\nstatic ssize_t force_ro_show(struct device *dev, struct device_attribute *attr,\n\t\t\t     char *buf)\n{\n\tint ret;\n\tstruct mmc_blk_data *md = mmc_blk_get(dev_to_disk(dev));\n\n\tret = snprintf(buf, PAGE_SIZE, \"%d\\n\",\n\t\t       get_disk_ro(dev_to_disk(dev)) ^\n\t\t       md->read_only);\n\tmmc_blk_put(md);\n\treturn ret;\n}\n\nstatic ssize_t force_ro_store(struct device *dev, struct device_attribute *attr,\n\t\t\t      const char *buf, size_t count)\n{\n\tint ret;\n\tchar *end;\n\tstruct mmc_blk_data *md = mmc_blk_get(dev_to_disk(dev));\n\tunsigned long set = simple_strtoul(buf, &end, 0);\n\tif (end == buf) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tset_disk_ro(dev_to_disk(dev), set || md->read_only);\n\tret = count;\nout:\n\tmmc_blk_put(md);\n\treturn ret;\n}\n\nstatic DEVICE_ATTR(force_ro, 0644, force_ro_show, force_ro_store);\n\nstatic struct attribute *mmc_disk_attrs[] = {\n\t&dev_attr_force_ro.attr,\n\t&dev_attr_ro_lock_until_next_power_on.attr,\n\tNULL,\n};\n\nstatic umode_t mmc_disk_attrs_is_visible(struct kobject *kobj,\n\t\tstruct attribute *a, int n)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct mmc_blk_data *md = mmc_blk_get(dev_to_disk(dev));\n\tumode_t mode = a->mode;\n\n\tif (a == &dev_attr_ro_lock_until_next_power_on.attr &&\n\t    (md->area_type & MMC_BLK_DATA_AREA_BOOT) &&\n\t    md->queue.card->ext_csd.boot_ro_lockable) {\n\t\tmode = S_IRUGO;\n\t\tif (!(md->queue.card->ext_csd.boot_ro_lock &\n\t\t\t\tEXT_CSD_BOOT_WP_B_PWR_WP_DIS))\n\t\t\tmode |= S_IWUSR;\n\t}\n\n\tmmc_blk_put(md);\n\treturn mode;\n}\n\nstatic const struct attribute_group mmc_disk_attr_group = {\n\t.is_visible\t= mmc_disk_attrs_is_visible,\n\t.attrs\t\t= mmc_disk_attrs,\n};\n\nstatic const struct attribute_group *mmc_disk_attr_groups[] = {\n\t&mmc_disk_attr_group,\n\tNULL,\n};\n\nstatic int mmc_blk_open(struct gendisk *disk, blk_mode_t mode)\n{\n\tstruct mmc_blk_data *md = mmc_blk_get(disk);\n\tint ret = -ENXIO;\n\n\tmutex_lock(&block_mutex);\n\tif (md) {\n\t\tret = 0;\n\t\tif ((mode & BLK_OPEN_WRITE) && md->read_only) {\n\t\t\tmmc_blk_put(md);\n\t\t\tret = -EROFS;\n\t\t}\n\t}\n\tmutex_unlock(&block_mutex);\n\n\treturn ret;\n}\n\nstatic void mmc_blk_release(struct gendisk *disk)\n{\n\tstruct mmc_blk_data *md = disk->private_data;\n\n\tmutex_lock(&block_mutex);\n\tmmc_blk_put(md);\n\tmutex_unlock(&block_mutex);\n}\n\nstatic int\nmmc_blk_getgeo(struct block_device *bdev, struct hd_geometry *geo)\n{\n\tgeo->cylinders = get_capacity(bdev->bd_disk) / (4 * 16);\n\tgeo->heads = 4;\n\tgeo->sectors = 16;\n\treturn 0;\n}\n\nstruct mmc_blk_ioc_data {\n\tstruct mmc_ioc_cmd ic;\n\tunsigned char *buf;\n\tu64 buf_bytes;\n\tstruct mmc_rpmb_data *rpmb;\n};\n\nstatic struct mmc_blk_ioc_data *mmc_blk_ioctl_copy_from_user(\n\tstruct mmc_ioc_cmd __user *user)\n{\n\tstruct mmc_blk_ioc_data *idata;\n\tint err;\n\n\tidata = kmalloc(sizeof(*idata), GFP_KERNEL);\n\tif (!idata) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (copy_from_user(&idata->ic, user, sizeof(idata->ic))) {\n\t\terr = -EFAULT;\n\t\tgoto idata_err;\n\t}\n\n\tidata->buf_bytes = (u64) idata->ic.blksz * idata->ic.blocks;\n\tif (idata->buf_bytes > MMC_IOC_MAX_BYTES) {\n\t\terr = -EOVERFLOW;\n\t\tgoto idata_err;\n\t}\n\n\tif (!idata->buf_bytes) {\n\t\tidata->buf = NULL;\n\t\treturn idata;\n\t}\n\n\tidata->buf = memdup_user((void __user *)(unsigned long)\n\t\t\t\t idata->ic.data_ptr, idata->buf_bytes);\n\tif (IS_ERR(idata->buf)) {\n\t\terr = PTR_ERR(idata->buf);\n\t\tgoto idata_err;\n\t}\n\n\treturn idata;\n\nidata_err:\n\tkfree(idata);\nout:\n\treturn ERR_PTR(err);\n}\n\nstatic int mmc_blk_ioctl_copy_to_user(struct mmc_ioc_cmd __user *ic_ptr,\n\t\t\t\t      struct mmc_blk_ioc_data *idata)\n{\n\tstruct mmc_ioc_cmd *ic = &idata->ic;\n\n\tif (copy_to_user(&(ic_ptr->response), ic->response,\n\t\t\t sizeof(ic->response)))\n\t\treturn -EFAULT;\n\n\tif (!idata->ic.write_flag) {\n\t\tif (copy_to_user((void __user *)(unsigned long)ic->data_ptr,\n\t\t\t\t idata->buf, idata->buf_bytes))\n\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic int __mmc_blk_ioctl_cmd(struct mmc_card *card, struct mmc_blk_data *md,\n\t\t\t       struct mmc_blk_ioc_data *idata)\n{\n\tstruct mmc_command cmd = {}, sbc = {};\n\tstruct mmc_data data = {};\n\tstruct mmc_request mrq = {};\n\tstruct scatterlist sg;\n\tbool r1b_resp;\n\tunsigned int busy_timeout_ms;\n\tint err;\n\tunsigned int target_part;\n\n\tif (!card || !md || !idata)\n\t\treturn -EINVAL;\n\n\t \n\tif (idata->rpmb) {\n\t\t \n\t\ttarget_part = idata->rpmb->part_index;\n\t\ttarget_part |= EXT_CSD_PART_CONFIG_ACC_RPMB;\n\t} else {\n\t\ttarget_part = md->part_type;\n\t}\n\n\tcmd.opcode = idata->ic.opcode;\n\tcmd.arg = idata->ic.arg;\n\tcmd.flags = idata->ic.flags;\n\n\tif (idata->buf_bytes) {\n\t\tdata.sg = &sg;\n\t\tdata.sg_len = 1;\n\t\tdata.blksz = idata->ic.blksz;\n\t\tdata.blocks = idata->ic.blocks;\n\n\t\tsg_init_one(data.sg, idata->buf, idata->buf_bytes);\n\n\t\tif (idata->ic.write_flag)\n\t\t\tdata.flags = MMC_DATA_WRITE;\n\t\telse\n\t\t\tdata.flags = MMC_DATA_READ;\n\n\t\t \n\t\tmmc_set_data_timeout(&data, card);\n\n\t\t \n\t\tif (idata->ic.data_timeout_ns)\n\t\t\tdata.timeout_ns = idata->ic.data_timeout_ns;\n\n\t\tmrq.data = &data;\n\t}\n\n\tmrq.cmd = &cmd;\n\n\terr = mmc_blk_part_switch(card, target_part);\n\tif (err)\n\t\treturn err;\n\n\tif (idata->ic.is_acmd) {\n\t\terr = mmc_app_cmd(card->host, card);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (idata->rpmb) {\n\t\tsbc.opcode = MMC_SET_BLOCK_COUNT;\n\t\t \n\t\tsbc.arg = data.blocks | (idata->ic.write_flag & BIT(31));\n\t\tsbc.flags = MMC_RSP_R1 | MMC_CMD_AC;\n\t\tmrq.sbc = &sbc;\n\t}\n\n\tif ((MMC_EXTRACT_INDEX_FROM_ARG(cmd.arg) == EXT_CSD_SANITIZE_START) &&\n\t    (cmd.opcode == MMC_SWITCH))\n\t\treturn mmc_sanitize(card, idata->ic.cmd_timeout_ms);\n\n\t \n\tbusy_timeout_ms = idata->ic.cmd_timeout_ms ? : MMC_BLK_TIMEOUT_MS;\n\tr1b_resp = (cmd.flags & MMC_RSP_R1B) == MMC_RSP_R1B;\n\tif (r1b_resp)\n\t\tmmc_prepare_busy_cmd(card->host, &cmd, busy_timeout_ms);\n\n\tmmc_wait_for_req(card->host, &mrq);\n\tmemcpy(&idata->ic.response, cmd.resp, sizeof(cmd.resp));\n\n\tif (cmd.error) {\n\t\tdev_err(mmc_dev(card->host), \"%s: cmd error %d\\n\",\n\t\t\t\t\t\t__func__, cmd.error);\n\t\treturn cmd.error;\n\t}\n\tif (data.error) {\n\t\tdev_err(mmc_dev(card->host), \"%s: data error %d\\n\",\n\t\t\t\t\t\t__func__, data.error);\n\t\treturn data.error;\n\t}\n\n\t \n\tif ((MMC_EXTRACT_INDEX_FROM_ARG(cmd.arg) == EXT_CSD_PART_CONFIG) &&\n\t    (cmd.opcode == MMC_SWITCH)) {\n\t\tstruct mmc_blk_data *main_md = dev_get_drvdata(&card->dev);\n\t\tu8 value = MMC_EXTRACT_VALUE_FROM_ARG(cmd.arg);\n\n\t\t \n\t\tcard->ext_csd.part_config = value;\n\t\tmain_md->part_curr = value & EXT_CSD_PART_CONFIG_ACC_MASK;\n\t}\n\n\t \n\tif ((MMC_EXTRACT_INDEX_FROM_ARG(cmd.arg) == EXT_CSD_CACHE_CTRL) &&\n\t    (cmd.opcode == MMC_SWITCH)) {\n\t\tu8 value = MMC_EXTRACT_VALUE_FROM_ARG(cmd.arg) & 1;\n\n\t\tcard->ext_csd.cache_ctrl = value;\n\t}\n\n\t \n\tif (idata->ic.postsleep_min_us)\n\t\tusleep_range(idata->ic.postsleep_min_us, idata->ic.postsleep_max_us);\n\n\tif (mmc_host_is_spi(card->host)) {\n\t\tif (idata->ic.write_flag || r1b_resp || cmd.flags & MMC_RSP_SPI_BUSY)\n\t\t\treturn mmc_spi_err_check(card);\n\t\treturn err;\n\t}\n\n\t \n\tif (idata->rpmb || idata->ic.write_flag || r1b_resp) {\n\t\tstruct mmc_blk_busy_data cb_data = {\n\t\t\t.card = card,\n\t\t};\n\n\t\terr = __mmc_poll_for_busy(card->host, 0, busy_timeout_ms,\n\t\t\t\t\t  &mmc_blk_busy_cb, &cb_data);\n\n\t\tidata->ic.response[0] = cb_data.status;\n\t}\n\n\treturn err;\n}\n\nstatic int mmc_blk_ioctl_cmd(struct mmc_blk_data *md,\n\t\t\t     struct mmc_ioc_cmd __user *ic_ptr,\n\t\t\t     struct mmc_rpmb_data *rpmb)\n{\n\tstruct mmc_blk_ioc_data *idata;\n\tstruct mmc_blk_ioc_data *idatas[1];\n\tstruct mmc_queue *mq;\n\tstruct mmc_card *card;\n\tint err = 0, ioc_err = 0;\n\tstruct request *req;\n\n\tidata = mmc_blk_ioctl_copy_from_user(ic_ptr);\n\tif (IS_ERR(idata))\n\t\treturn PTR_ERR(idata);\n\t \n\tidata->rpmb = rpmb;\n\n\tcard = md->queue.card;\n\tif (IS_ERR(card)) {\n\t\terr = PTR_ERR(card);\n\t\tgoto cmd_done;\n\t}\n\n\t \n\tmq = &md->queue;\n\treq = blk_mq_alloc_request(mq->queue,\n\t\tidata->ic.write_flag ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN, 0);\n\tif (IS_ERR(req)) {\n\t\terr = PTR_ERR(req);\n\t\tgoto cmd_done;\n\t}\n\tidatas[0] = idata;\n\treq_to_mmc_queue_req(req)->drv_op =\n\t\trpmb ? MMC_DRV_OP_IOCTL_RPMB : MMC_DRV_OP_IOCTL;\n\treq_to_mmc_queue_req(req)->drv_op_result = -EIO;\n\treq_to_mmc_queue_req(req)->drv_op_data = idatas;\n\treq_to_mmc_queue_req(req)->ioc_count = 1;\n\tblk_execute_rq(req, false);\n\tioc_err = req_to_mmc_queue_req(req)->drv_op_result;\n\terr = mmc_blk_ioctl_copy_to_user(ic_ptr, idata);\n\tblk_mq_free_request(req);\n\ncmd_done:\n\tkfree(idata->buf);\n\tkfree(idata);\n\treturn ioc_err ? ioc_err : err;\n}\n\nstatic int mmc_blk_ioctl_multi_cmd(struct mmc_blk_data *md,\n\t\t\t\t   struct mmc_ioc_multi_cmd __user *user,\n\t\t\t\t   struct mmc_rpmb_data *rpmb)\n{\n\tstruct mmc_blk_ioc_data **idata = NULL;\n\tstruct mmc_ioc_cmd __user *cmds = user->cmds;\n\tstruct mmc_card *card;\n\tstruct mmc_queue *mq;\n\tint err = 0, ioc_err = 0;\n\t__u64 num_of_cmds;\n\tunsigned int i, n;\n\tstruct request *req;\n\n\tif (copy_from_user(&num_of_cmds, &user->num_of_cmds,\n\t\t\t   sizeof(num_of_cmds)))\n\t\treturn -EFAULT;\n\n\tif (!num_of_cmds)\n\t\treturn 0;\n\n\tif (num_of_cmds > MMC_IOC_MAX_CMDS)\n\t\treturn -EINVAL;\n\n\tn = num_of_cmds;\n\tidata = kcalloc(n, sizeof(*idata), GFP_KERNEL);\n\tif (!idata)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < n; i++) {\n\t\tidata[i] = mmc_blk_ioctl_copy_from_user(&cmds[i]);\n\t\tif (IS_ERR(idata[i])) {\n\t\t\terr = PTR_ERR(idata[i]);\n\t\t\tn = i;\n\t\t\tgoto cmd_err;\n\t\t}\n\t\t \n\t\tidata[i]->rpmb = rpmb;\n\t}\n\n\tcard = md->queue.card;\n\tif (IS_ERR(card)) {\n\t\terr = PTR_ERR(card);\n\t\tgoto cmd_err;\n\t}\n\n\n\t \n\tmq = &md->queue;\n\treq = blk_mq_alloc_request(mq->queue,\n\t\tidata[0]->ic.write_flag ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN, 0);\n\tif (IS_ERR(req)) {\n\t\terr = PTR_ERR(req);\n\t\tgoto cmd_err;\n\t}\n\treq_to_mmc_queue_req(req)->drv_op =\n\t\trpmb ? MMC_DRV_OP_IOCTL_RPMB : MMC_DRV_OP_IOCTL;\n\treq_to_mmc_queue_req(req)->drv_op_result = -EIO;\n\treq_to_mmc_queue_req(req)->drv_op_data = idata;\n\treq_to_mmc_queue_req(req)->ioc_count = n;\n\tblk_execute_rq(req, false);\n\tioc_err = req_to_mmc_queue_req(req)->drv_op_result;\n\n\t \n\tfor (i = 0; i < n && !err; i++)\n\t\terr = mmc_blk_ioctl_copy_to_user(&cmds[i], idata[i]);\n\n\tblk_mq_free_request(req);\n\ncmd_err:\n\tfor (i = 0; i < n; i++) {\n\t\tkfree(idata[i]->buf);\n\t\tkfree(idata[i]);\n\t}\n\tkfree(idata);\n\treturn ioc_err ? ioc_err : err;\n}\n\nstatic int mmc_blk_check_blkdev(struct block_device *bdev)\n{\n\t \n\tif (!capable(CAP_SYS_RAWIO) || bdev_is_partition(bdev))\n\t\treturn -EPERM;\n\treturn 0;\n}\n\nstatic int mmc_blk_ioctl(struct block_device *bdev, blk_mode_t mode,\n\tunsigned int cmd, unsigned long arg)\n{\n\tstruct mmc_blk_data *md;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase MMC_IOC_CMD:\n\t\tret = mmc_blk_check_blkdev(bdev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmd = mmc_blk_get(bdev->bd_disk);\n\t\tif (!md)\n\t\t\treturn -EINVAL;\n\t\tret = mmc_blk_ioctl_cmd(md,\n\t\t\t\t\t(struct mmc_ioc_cmd __user *)arg,\n\t\t\t\t\tNULL);\n\t\tmmc_blk_put(md);\n\t\treturn ret;\n\tcase MMC_IOC_MULTI_CMD:\n\t\tret = mmc_blk_check_blkdev(bdev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmd = mmc_blk_get(bdev->bd_disk);\n\t\tif (!md)\n\t\t\treturn -EINVAL;\n\t\tret = mmc_blk_ioctl_multi_cmd(md,\n\t\t\t\t\t(struct mmc_ioc_multi_cmd __user *)arg,\n\t\t\t\t\tNULL);\n\t\tmmc_blk_put(md);\n\t\treturn ret;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n#ifdef CONFIG_COMPAT\nstatic int mmc_blk_compat_ioctl(struct block_device *bdev, blk_mode_t mode,\n\tunsigned int cmd, unsigned long arg)\n{\n\treturn mmc_blk_ioctl(bdev, mode, cmd, (unsigned long) compat_ptr(arg));\n}\n#endif\n\nstatic int mmc_blk_alternative_gpt_sector(struct gendisk *disk,\n\t\t\t\t\t  sector_t *sector)\n{\n\tstruct mmc_blk_data *md;\n\tint ret;\n\n\tmd = mmc_blk_get(disk);\n\tif (!md)\n\t\treturn -EINVAL;\n\n\tif (md->queue.card)\n\t\tret = mmc_card_alternative_gpt_sector(md->queue.card, sector);\n\telse\n\t\tret = -ENODEV;\n\n\tmmc_blk_put(md);\n\n\treturn ret;\n}\n\nstatic const struct block_device_operations mmc_bdops = {\n\t.open\t\t\t= mmc_blk_open,\n\t.release\t\t= mmc_blk_release,\n\t.getgeo\t\t\t= mmc_blk_getgeo,\n\t.owner\t\t\t= THIS_MODULE,\n\t.ioctl\t\t\t= mmc_blk_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t\t= mmc_blk_compat_ioctl,\n#endif\n\t.alternative_gpt_sector\t= mmc_blk_alternative_gpt_sector,\n};\n\nstatic int mmc_blk_part_switch_pre(struct mmc_card *card,\n\t\t\t\t   unsigned int part_type)\n{\n\tconst unsigned int mask = EXT_CSD_PART_CONFIG_ACC_RPMB;\n\tint ret = 0;\n\n\tif ((part_type & mask) == mask) {\n\t\tif (card->ext_csd.cmdq_en) {\n\t\t\tret = mmc_cmdq_disable(card);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tmmc_retune_pause(card->host);\n\t}\n\n\treturn ret;\n}\n\nstatic int mmc_blk_part_switch_post(struct mmc_card *card,\n\t\t\t\t    unsigned int part_type)\n{\n\tconst unsigned int mask = EXT_CSD_PART_CONFIG_ACC_RPMB;\n\tint ret = 0;\n\n\tif ((part_type & mask) == mask) {\n\t\tmmc_retune_unpause(card->host);\n\t\tif (card->reenable_cmdq && !card->ext_csd.cmdq_en)\n\t\t\tret = mmc_cmdq_enable(card);\n\t}\n\n\treturn ret;\n}\n\nstatic inline int mmc_blk_part_switch(struct mmc_card *card,\n\t\t\t\t      unsigned int part_type)\n{\n\tint ret = 0;\n\tstruct mmc_blk_data *main_md = dev_get_drvdata(&card->dev);\n\n\tif (main_md->part_curr == part_type)\n\t\treturn 0;\n\n\tif (mmc_card_mmc(card)) {\n\t\tu8 part_config = card->ext_csd.part_config;\n\n\t\tret = mmc_blk_part_switch_pre(card, part_type);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tpart_config &= ~EXT_CSD_PART_CONFIG_ACC_MASK;\n\t\tpart_config |= part_type;\n\n\t\tret = mmc_switch(card, EXT_CSD_CMD_SET_NORMAL,\n\t\t\t\t EXT_CSD_PART_CONFIG, part_config,\n\t\t\t\t card->ext_csd.part_time);\n\t\tif (ret) {\n\t\t\tmmc_blk_part_switch_post(card, part_type);\n\t\t\treturn ret;\n\t\t}\n\n\t\tcard->ext_csd.part_config = part_config;\n\n\t\tret = mmc_blk_part_switch_post(card, main_md->part_curr);\n\t}\n\n\tmain_md->part_curr = part_type;\n\treturn ret;\n}\n\nstatic int mmc_sd_num_wr_blocks(struct mmc_card *card, u32 *written_blocks)\n{\n\tint err;\n\tu32 result;\n\t__be32 *blocks;\n\n\tstruct mmc_request mrq = {};\n\tstruct mmc_command cmd = {};\n\tstruct mmc_data data = {};\n\n\tstruct scatterlist sg;\n\n\terr = mmc_app_cmd(card->host, card);\n\tif (err)\n\t\treturn err;\n\n\tcmd.opcode = SD_APP_SEND_NUM_WR_BLKS;\n\tcmd.arg = 0;\n\tcmd.flags = MMC_RSP_SPI_R1 | MMC_RSP_R1 | MMC_CMD_ADTC;\n\n\tdata.blksz = 4;\n\tdata.blocks = 1;\n\tdata.flags = MMC_DATA_READ;\n\tdata.sg = &sg;\n\tdata.sg_len = 1;\n\tmmc_set_data_timeout(&data, card);\n\n\tmrq.cmd = &cmd;\n\tmrq.data = &data;\n\n\tblocks = kmalloc(4, GFP_KERNEL);\n\tif (!blocks)\n\t\treturn -ENOMEM;\n\n\tsg_init_one(&sg, blocks, 4);\n\n\tmmc_wait_for_req(card->host, &mrq);\n\n\tresult = ntohl(*blocks);\n\tkfree(blocks);\n\n\tif (cmd.error || data.error)\n\t\treturn -EIO;\n\n\t*written_blocks = result;\n\n\treturn 0;\n}\n\nstatic unsigned int mmc_blk_clock_khz(struct mmc_host *host)\n{\n\tif (host->actual_clock)\n\t\treturn host->actual_clock / 1000;\n\n\t \n\tif (host->ios.clock)\n\t\treturn host->ios.clock / 2000;\n\n\t \n\tWARN_ON_ONCE(1);\n\treturn 100;  \n}\n\nstatic unsigned int mmc_blk_data_timeout_ms(struct mmc_host *host,\n\t\t\t\t\t    struct mmc_data *data)\n{\n\tunsigned int ms = DIV_ROUND_UP(data->timeout_ns, 1000000);\n\tunsigned int khz;\n\n\tif (data->timeout_clks) {\n\t\tkhz = mmc_blk_clock_khz(host);\n\t\tms += DIV_ROUND_UP(data->timeout_clks, khz);\n\t}\n\n\treturn ms;\n}\n\n \nstatic int mmc_blk_reset(struct mmc_blk_data *md, struct mmc_host *host,\n\t\t\t int type)\n{\n\tint err;\n\tstruct mmc_blk_data *main_md = dev_get_drvdata(&host->card->dev);\n\n\tif (md->reset_done & type)\n\t\treturn -EEXIST;\n\n\tmd->reset_done |= type;\n\terr = mmc_hw_reset(host->card);\n\t \n\tmain_md->part_curr = err ? MMC_BLK_PART_INVALID : main_md->part_type;\n\tif (err)\n\t\treturn err;\n\t \n\tif (mmc_blk_part_switch(host->card, md->part_type))\n\t\t \n\t\treturn -ENODEV;\n\treturn 0;\n}\n\nstatic inline void mmc_blk_reset_success(struct mmc_blk_data *md, int type)\n{\n\tmd->reset_done &= ~type;\n}\n\n \nstatic void mmc_blk_issue_drv_op(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_queue_req *mq_rq;\n\tstruct mmc_card *card = mq->card;\n\tstruct mmc_blk_data *md = mq->blkdata;\n\tstruct mmc_blk_ioc_data **idata;\n\tbool rpmb_ioctl;\n\tu8 **ext_csd;\n\tu32 status;\n\tint ret;\n\tint i;\n\n\tmq_rq = req_to_mmc_queue_req(req);\n\trpmb_ioctl = (mq_rq->drv_op == MMC_DRV_OP_IOCTL_RPMB);\n\n\tswitch (mq_rq->drv_op) {\n\tcase MMC_DRV_OP_IOCTL:\n\t\tif (card->ext_csd.cmdq_en) {\n\t\t\tret = mmc_cmdq_disable(card);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase MMC_DRV_OP_IOCTL_RPMB:\n\t\tidata = mq_rq->drv_op_data;\n\t\tfor (i = 0, ret = 0; i < mq_rq->ioc_count; i++) {\n\t\t\tret = __mmc_blk_ioctl_cmd(card, md, idata[i]);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (rpmb_ioctl)\n\t\t\tmmc_blk_part_switch(card, 0);\n\t\telse if (card->reenable_cmdq && !card->ext_csd.cmdq_en)\n\t\t\tmmc_cmdq_enable(card);\n\t\tbreak;\n\tcase MMC_DRV_OP_BOOT_WP:\n\t\tret = mmc_switch(card, EXT_CSD_CMD_SET_NORMAL, EXT_CSD_BOOT_WP,\n\t\t\t\t card->ext_csd.boot_ro_lock |\n\t\t\t\t EXT_CSD_BOOT_WP_B_PWR_WP_EN,\n\t\t\t\t card->ext_csd.part_time);\n\t\tif (ret)\n\t\t\tpr_err(\"%s: Locking boot partition ro until next power on failed: %d\\n\",\n\t\t\t       md->disk->disk_name, ret);\n\t\telse\n\t\t\tcard->ext_csd.boot_ro_lock |=\n\t\t\t\tEXT_CSD_BOOT_WP_B_PWR_WP_EN;\n\t\tbreak;\n\tcase MMC_DRV_OP_GET_CARD_STATUS:\n\t\tret = mmc_send_status(card, &status);\n\t\tif (!ret)\n\t\t\tret = status;\n\t\tbreak;\n\tcase MMC_DRV_OP_GET_EXT_CSD:\n\t\text_csd = mq_rq->drv_op_data;\n\t\tret = mmc_get_ext_csd(card, ext_csd);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s: unknown driver specific operation\\n\",\n\t\t       md->disk->disk_name);\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\tmq_rq->drv_op_result = ret;\n\tblk_mq_end_request(req, ret ? BLK_STS_IOERR : BLK_STS_OK);\n}\n\nstatic void mmc_blk_issue_erase_rq(struct mmc_queue *mq, struct request *req,\n\t\t\t\t   int type, unsigned int erase_arg)\n{\n\tstruct mmc_blk_data *md = mq->blkdata;\n\tstruct mmc_card *card = md->queue.card;\n\tunsigned int from, nr;\n\tint err = 0;\n\tblk_status_t status = BLK_STS_OK;\n\n\tif (!mmc_can_erase(card)) {\n\t\tstatus = BLK_STS_NOTSUPP;\n\t\tgoto fail;\n\t}\n\n\tfrom = blk_rq_pos(req);\n\tnr = blk_rq_sectors(req);\n\n\tdo {\n\t\terr = 0;\n\t\tif (card->quirks & MMC_QUIRK_INAND_CMD38) {\n\t\t\terr = mmc_switch(card, EXT_CSD_CMD_SET_NORMAL,\n\t\t\t\t\t INAND_CMD38_ARG_EXT_CSD,\n\t\t\t\t\t erase_arg == MMC_TRIM_ARG ?\n\t\t\t\t\t INAND_CMD38_ARG_TRIM :\n\t\t\t\t\t INAND_CMD38_ARG_ERASE,\n\t\t\t\t\t card->ext_csd.generic_cmd6_time);\n\t\t}\n\t\tif (!err)\n\t\t\terr = mmc_erase(card, from, nr, erase_arg);\n\t} while (err == -EIO && !mmc_blk_reset(md, card->host, type));\n\tif (err)\n\t\tstatus = BLK_STS_IOERR;\n\telse\n\t\tmmc_blk_reset_success(md, type);\nfail:\n\tblk_mq_end_request(req, status);\n}\n\nstatic void mmc_blk_issue_trim_rq(struct mmc_queue *mq, struct request *req)\n{\n\tmmc_blk_issue_erase_rq(mq, req, MMC_BLK_TRIM, MMC_TRIM_ARG);\n}\n\nstatic void mmc_blk_issue_discard_rq(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_blk_data *md = mq->blkdata;\n\tstruct mmc_card *card = md->queue.card;\n\tunsigned int arg = card->erase_arg;\n\n\tif (mmc_card_broken_sd_discard(card))\n\t\targ = SD_ERASE_ARG;\n\n\tmmc_blk_issue_erase_rq(mq, req, MMC_BLK_DISCARD, arg);\n}\n\nstatic void mmc_blk_issue_secdiscard_rq(struct mmc_queue *mq,\n\t\t\t\t       struct request *req)\n{\n\tstruct mmc_blk_data *md = mq->blkdata;\n\tstruct mmc_card *card = md->queue.card;\n\tunsigned int from, nr, arg;\n\tint err = 0, type = MMC_BLK_SECDISCARD;\n\tblk_status_t status = BLK_STS_OK;\n\n\tif (!(mmc_can_secure_erase_trim(card))) {\n\t\tstatus = BLK_STS_NOTSUPP;\n\t\tgoto out;\n\t}\n\n\tfrom = blk_rq_pos(req);\n\tnr = blk_rq_sectors(req);\n\n\tif (mmc_can_trim(card) && !mmc_erase_group_aligned(card, from, nr))\n\t\targ = MMC_SECURE_TRIM1_ARG;\n\telse\n\t\targ = MMC_SECURE_ERASE_ARG;\n\nretry:\n\tif (card->quirks & MMC_QUIRK_INAND_CMD38) {\n\t\terr = mmc_switch(card, EXT_CSD_CMD_SET_NORMAL,\n\t\t\t\t INAND_CMD38_ARG_EXT_CSD,\n\t\t\t\t arg == MMC_SECURE_TRIM1_ARG ?\n\t\t\t\t INAND_CMD38_ARG_SECTRIM1 :\n\t\t\t\t INAND_CMD38_ARG_SECERASE,\n\t\t\t\t card->ext_csd.generic_cmd6_time);\n\t\tif (err)\n\t\t\tgoto out_retry;\n\t}\n\n\terr = mmc_erase(card, from, nr, arg);\n\tif (err == -EIO)\n\t\tgoto out_retry;\n\tif (err) {\n\t\tstatus = BLK_STS_IOERR;\n\t\tgoto out;\n\t}\n\n\tif (arg == MMC_SECURE_TRIM1_ARG) {\n\t\tif (card->quirks & MMC_QUIRK_INAND_CMD38) {\n\t\t\terr = mmc_switch(card, EXT_CSD_CMD_SET_NORMAL,\n\t\t\t\t\t INAND_CMD38_ARG_EXT_CSD,\n\t\t\t\t\t INAND_CMD38_ARG_SECTRIM2,\n\t\t\t\t\t card->ext_csd.generic_cmd6_time);\n\t\t\tif (err)\n\t\t\t\tgoto out_retry;\n\t\t}\n\n\t\terr = mmc_erase(card, from, nr, MMC_SECURE_TRIM2_ARG);\n\t\tif (err == -EIO)\n\t\t\tgoto out_retry;\n\t\tif (err) {\n\t\t\tstatus = BLK_STS_IOERR;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout_retry:\n\tif (err && !mmc_blk_reset(md, card->host, type))\n\t\tgoto retry;\n\tif (!err)\n\t\tmmc_blk_reset_success(md, type);\nout:\n\tblk_mq_end_request(req, status);\n}\n\nstatic void mmc_blk_issue_flush(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_blk_data *md = mq->blkdata;\n\tstruct mmc_card *card = md->queue.card;\n\tint ret = 0;\n\n\tret = mmc_flush_cache(card->host);\n\tblk_mq_end_request(req, ret ? BLK_STS_IOERR : BLK_STS_OK);\n}\n\n \nstatic inline void mmc_apply_rel_rw(struct mmc_blk_request *brq,\n\t\t\t\t    struct mmc_card *card,\n\t\t\t\t    struct request *req)\n{\n\tif (!(card->ext_csd.rel_param & EXT_CSD_WR_REL_PARAM_EN)) {\n\t\t \n\t\tif (!IS_ALIGNED(blk_rq_pos(req), card->ext_csd.rel_sectors))\n\t\t\tbrq->data.blocks = 1;\n\n\t\tif (brq->data.blocks > card->ext_csd.rel_sectors)\n\t\t\tbrq->data.blocks = card->ext_csd.rel_sectors;\n\t\telse if (brq->data.blocks < card->ext_csd.rel_sectors)\n\t\t\tbrq->data.blocks = 1;\n\t}\n}\n\n#define CMD_ERRORS_EXCL_OOR\t\t\t\t\t\t\\\n\t(R1_ADDRESS_ERROR |\t \t\t\\\n\t R1_BLOCK_LEN_ERROR |\t \\\n\t R1_WP_VIOLATION |\t \t\\\n\t R1_CARD_ECC_FAILED |\t \t\t\t\\\n\t R1_CC_ERROR |\t\t \t\t\\\n\t R1_ERROR)\t\t \n\n#define CMD_ERRORS\t\t\t\t\t\t\t\\\n\t(CMD_ERRORS_EXCL_OOR |\t\t\t\t\t\t\\\n\t R1_OUT_OF_RANGE)\t \t\\\n\nstatic void mmc_blk_eval_resp_error(struct mmc_blk_request *brq)\n{\n\tu32 val;\n\n\t \n\n\tif (!brq->stop.error) {\n\t\tbool oor_with_open_end;\n\t\t \n\n\t\tval = brq->stop.resp[0] & CMD_ERRORS;\n\t\toor_with_open_end = val & R1_OUT_OF_RANGE && !brq->mrq.sbc;\n\n\t\tif (val && !oor_with_open_end)\n\t\t\tbrq->stop.error = -EIO;\n\t}\n}\n\nstatic void mmc_blk_data_prep(struct mmc_queue *mq, struct mmc_queue_req *mqrq,\n\t\t\t      int recovery_mode, bool *do_rel_wr_p,\n\t\t\t      bool *do_data_tag_p)\n{\n\tstruct mmc_blk_data *md = mq->blkdata;\n\tstruct mmc_card *card = md->queue.card;\n\tstruct mmc_blk_request *brq = &mqrq->brq;\n\tstruct request *req = mmc_queue_req_to_req(mqrq);\n\tbool do_rel_wr, do_data_tag;\n\n\t \n\tdo_rel_wr = (req->cmd_flags & REQ_FUA) &&\n\t\t    rq_data_dir(req) == WRITE &&\n\t\t    (md->flags & MMC_BLK_REL_WR);\n\n\tmemset(brq, 0, sizeof(struct mmc_blk_request));\n\n\tmmc_crypto_prepare_req(mqrq);\n\n\tbrq->mrq.data = &brq->data;\n\tbrq->mrq.tag = req->tag;\n\n\tbrq->stop.opcode = MMC_STOP_TRANSMISSION;\n\tbrq->stop.arg = 0;\n\n\tif (rq_data_dir(req) == READ) {\n\t\tbrq->data.flags = MMC_DATA_READ;\n\t\tbrq->stop.flags = MMC_RSP_SPI_R1 | MMC_RSP_R1 | MMC_CMD_AC;\n\t} else {\n\t\tbrq->data.flags = MMC_DATA_WRITE;\n\t\tbrq->stop.flags = MMC_RSP_SPI_R1B | MMC_RSP_R1B | MMC_CMD_AC;\n\t}\n\n\tbrq->data.blksz = 512;\n\tbrq->data.blocks = blk_rq_sectors(req);\n\tbrq->data.blk_addr = blk_rq_pos(req);\n\n\t \n\n\t \n\tif (brq->data.blocks > card->host->max_blk_count)\n\t\tbrq->data.blocks = card->host->max_blk_count;\n\n\tif (brq->data.blocks > 1) {\n\t\t \n\t\tif (mmc_host_is_spi(card->host) && (rq_data_dir(req) == READ) &&\n\t\t    (blk_rq_pos(req) + blk_rq_sectors(req) ==\n\t\t     get_capacity(md->disk)))\n\t\t\tbrq->data.blocks--;\n\n\t\t \n\t\tif (recovery_mode)\n\t\t\tbrq->data.blocks = queue_physical_block_size(mq->queue) >> 9;\n\n\t\t \n\t\tif (card->host->ops->multi_io_quirk)\n\t\t\tbrq->data.blocks = card->host->ops->multi_io_quirk(card,\n\t\t\t\t\t\t(rq_data_dir(req) == READ) ?\n\t\t\t\t\t\tMMC_DATA_READ : MMC_DATA_WRITE,\n\t\t\t\t\t\tbrq->data.blocks);\n\t}\n\n\tif (do_rel_wr) {\n\t\tmmc_apply_rel_rw(brq, card, req);\n\t\tbrq->data.flags |= MMC_DATA_REL_WR;\n\t}\n\n\t \n\tdo_data_tag = card->ext_csd.data_tag_unit_size &&\n\t\t      (req->cmd_flags & REQ_META) &&\n\t\t      (rq_data_dir(req) == WRITE) &&\n\t\t      ((brq->data.blocks * brq->data.blksz) >=\n\t\t       card->ext_csd.data_tag_unit_size);\n\n\tif (do_data_tag)\n\t\tbrq->data.flags |= MMC_DATA_DAT_TAG;\n\n\tmmc_set_data_timeout(&brq->data, card);\n\n\tbrq->data.sg = mqrq->sg;\n\tbrq->data.sg_len = mmc_queue_map_sg(mq, mqrq);\n\n\t \n\tif (brq->data.blocks != blk_rq_sectors(req)) {\n\t\tint i, data_size = brq->data.blocks << 9;\n\t\tstruct scatterlist *sg;\n\n\t\tfor_each_sg(brq->data.sg, sg, brq->data.sg_len, i) {\n\t\t\tdata_size -= sg->length;\n\t\t\tif (data_size <= 0) {\n\t\t\t\tsg->length += data_size;\n\t\t\t\ti++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbrq->data.sg_len = i;\n\t}\n\n\tif (do_rel_wr_p)\n\t\t*do_rel_wr_p = do_rel_wr;\n\n\tif (do_data_tag_p)\n\t\t*do_data_tag_p = do_data_tag;\n}\n\n#define MMC_CQE_RETRIES 2\n\nstatic void mmc_blk_cqe_complete_rq(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_request *mrq = &mqrq->brq.mrq;\n\tstruct request_queue *q = req->q;\n\tstruct mmc_host *host = mq->card->host;\n\tenum mmc_issue_type issue_type = mmc_issue_type(mq, req);\n\tunsigned long flags;\n\tbool put_card;\n\tint err;\n\n\tmmc_cqe_post_req(host, mrq);\n\n\tif (mrq->cmd && mrq->cmd->error)\n\t\terr = mrq->cmd->error;\n\telse if (mrq->data && mrq->data->error)\n\t\terr = mrq->data->error;\n\telse\n\t\terr = 0;\n\n\tif (err) {\n\t\tif (mqrq->retries++ < MMC_CQE_RETRIES)\n\t\t\tblk_mq_requeue_request(req, true);\n\t\telse\n\t\t\tblk_mq_end_request(req, BLK_STS_IOERR);\n\t} else if (mrq->data) {\n\t\tif (blk_update_request(req, BLK_STS_OK, mrq->data->bytes_xfered))\n\t\t\tblk_mq_requeue_request(req, true);\n\t\telse\n\t\t\t__blk_mq_end_request(req, BLK_STS_OK);\n\t} else if (mq->in_recovery) {\n\t\tblk_mq_requeue_request(req, true);\n\t} else {\n\t\tblk_mq_end_request(req, BLK_STS_OK);\n\t}\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\n\tmq->in_flight[issue_type] -= 1;\n\n\tput_card = (mmc_tot_in_flight(mq) == 0);\n\n\tmmc_cqe_check_busy(mq);\n\n\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\tif (!mq->cqe_busy)\n\t\tblk_mq_run_hw_queues(q, true);\n\n\tif (put_card)\n\t\tmmc_put_card(mq->card, &mq->ctx);\n}\n\nvoid mmc_blk_cqe_recovery(struct mmc_queue *mq)\n{\n\tstruct mmc_card *card = mq->card;\n\tstruct mmc_host *host = card->host;\n\tint err;\n\n\tpr_debug(\"%s: CQE recovery start\\n\", mmc_hostname(host));\n\n\terr = mmc_cqe_recovery(host);\n\tif (err)\n\t\tmmc_blk_reset(mq->blkdata, host, MMC_BLK_CQE_RECOVERY);\n\tmmc_blk_reset_success(mq->blkdata, MMC_BLK_CQE_RECOVERY);\n\n\tpr_debug(\"%s: CQE recovery done\\n\", mmc_hostname(host));\n}\n\nstatic void mmc_blk_cqe_req_done(struct mmc_request *mrq)\n{\n\tstruct mmc_queue_req *mqrq = container_of(mrq, struct mmc_queue_req,\n\t\t\t\t\t\t  brq.mrq);\n\tstruct request *req = mmc_queue_req_to_req(mqrq);\n\tstruct request_queue *q = req->q;\n\tstruct mmc_queue *mq = q->queuedata;\n\n\t \n\tif (mq->in_recovery)\n\t\tmmc_blk_cqe_complete_rq(mq, req);\n\telse if (likely(!blk_should_fake_timeout(req->q)))\n\t\tblk_mq_complete_request(req);\n}\n\nstatic int mmc_blk_cqe_start_req(struct mmc_host *host, struct mmc_request *mrq)\n{\n\tmrq->done\t\t= mmc_blk_cqe_req_done;\n\tmrq->recovery_notifier\t= mmc_cqe_recovery_notifier;\n\n\treturn mmc_cqe_start_req(host, mrq);\n}\n\nstatic struct mmc_request *mmc_blk_cqe_prep_dcmd(struct mmc_queue_req *mqrq,\n\t\t\t\t\t\t struct request *req)\n{\n\tstruct mmc_blk_request *brq = &mqrq->brq;\n\n\tmemset(brq, 0, sizeof(*brq));\n\n\tbrq->mrq.cmd = &brq->cmd;\n\tbrq->mrq.tag = req->tag;\n\n\treturn &brq->mrq;\n}\n\nstatic int mmc_blk_cqe_issue_flush(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_request *mrq = mmc_blk_cqe_prep_dcmd(mqrq, req);\n\n\tmrq->cmd->opcode = MMC_SWITCH;\n\tmrq->cmd->arg = (MMC_SWITCH_MODE_WRITE_BYTE << 24) |\n\t\t\t(EXT_CSD_FLUSH_CACHE << 16) |\n\t\t\t(1 << 8) |\n\t\t\tEXT_CSD_CMD_SET_NORMAL;\n\tmrq->cmd->flags = MMC_CMD_AC | MMC_RSP_R1B;\n\n\treturn mmc_blk_cqe_start_req(mq->card->host, mrq);\n}\n\nstatic int mmc_blk_hsq_issue_rw_rq(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_host *host = mq->card->host;\n\tint err;\n\n\tmmc_blk_rw_rq_prep(mqrq, mq->card, 0, mq);\n\tmqrq->brq.mrq.done = mmc_blk_hsq_req_done;\n\tmmc_pre_req(host, &mqrq->brq.mrq);\n\n\terr = mmc_cqe_start_req(host, &mqrq->brq.mrq);\n\tif (err)\n\t\tmmc_post_req(host, &mqrq->brq.mrq, err);\n\n\treturn err;\n}\n\nstatic int mmc_blk_cqe_issue_rw_rq(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_host *host = mq->card->host;\n\n\tif (host->hsq_enabled)\n\t\treturn mmc_blk_hsq_issue_rw_rq(mq, req);\n\n\tmmc_blk_data_prep(mq, mqrq, 0, NULL, NULL);\n\n\treturn mmc_blk_cqe_start_req(mq->card->host, &mqrq->brq.mrq);\n}\n\nstatic void mmc_blk_rw_rq_prep(struct mmc_queue_req *mqrq,\n\t\t\t       struct mmc_card *card,\n\t\t\t       int recovery_mode,\n\t\t\t       struct mmc_queue *mq)\n{\n\tu32 readcmd, writecmd;\n\tstruct mmc_blk_request *brq = &mqrq->brq;\n\tstruct request *req = mmc_queue_req_to_req(mqrq);\n\tstruct mmc_blk_data *md = mq->blkdata;\n\tbool do_rel_wr, do_data_tag;\n\n\tmmc_blk_data_prep(mq, mqrq, recovery_mode, &do_rel_wr, &do_data_tag);\n\n\tbrq->mrq.cmd = &brq->cmd;\n\n\tbrq->cmd.arg = blk_rq_pos(req);\n\tif (!mmc_card_blockaddr(card))\n\t\tbrq->cmd.arg <<= 9;\n\tbrq->cmd.flags = MMC_RSP_SPI_R1 | MMC_RSP_R1 | MMC_CMD_ADTC;\n\n\tif (brq->data.blocks > 1 || do_rel_wr) {\n\t\t \n\t\tif (!mmc_host_is_spi(card->host) ||\n\t\t    rq_data_dir(req) == READ)\n\t\t\tbrq->mrq.stop = &brq->stop;\n\t\treadcmd = MMC_READ_MULTIPLE_BLOCK;\n\t\twritecmd = MMC_WRITE_MULTIPLE_BLOCK;\n\t} else {\n\t\tbrq->mrq.stop = NULL;\n\t\treadcmd = MMC_READ_SINGLE_BLOCK;\n\t\twritecmd = MMC_WRITE_BLOCK;\n\t}\n\tbrq->cmd.opcode = rq_data_dir(req) == READ ? readcmd : writecmd;\n\n\t \n\tif ((md->flags & MMC_BLK_CMD23) && mmc_op_multi(brq->cmd.opcode) &&\n\t    (do_rel_wr || !(card->quirks & MMC_QUIRK_BLK_NO_CMD23) ||\n\t     do_data_tag)) {\n\t\tbrq->sbc.opcode = MMC_SET_BLOCK_COUNT;\n\t\tbrq->sbc.arg = brq->data.blocks |\n\t\t\t(do_rel_wr ? (1 << 31) : 0) |\n\t\t\t(do_data_tag ? (1 << 29) : 0);\n\t\tbrq->sbc.flags = MMC_RSP_R1 | MMC_CMD_AC;\n\t\tbrq->mrq.sbc = &brq->sbc;\n\t}\n}\n\n#define MMC_MAX_RETRIES\t\t5\n#define MMC_DATA_RETRIES\t2\n#define MMC_NO_RETRIES\t\t(MMC_MAX_RETRIES + 1)\n\nstatic int mmc_blk_send_stop(struct mmc_card *card, unsigned int timeout)\n{\n\tstruct mmc_command cmd = {\n\t\t.opcode = MMC_STOP_TRANSMISSION,\n\t\t.flags = MMC_RSP_SPI_R1 | MMC_RSP_R1 | MMC_CMD_AC,\n\t\t \n\t\t.busy_timeout = timeout,\n\t};\n\n\treturn mmc_wait_for_cmd(card->host, &cmd, 5);\n}\n\nstatic int mmc_blk_fix_state(struct mmc_card *card, struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_blk_request *brq = &mqrq->brq;\n\tunsigned int timeout = mmc_blk_data_timeout_ms(card->host, &brq->data);\n\tint err;\n\n\tmmc_retune_hold_now(card->host);\n\n\tmmc_blk_send_stop(card, timeout);\n\n\terr = mmc_poll_for_busy(card, timeout, false, MMC_BUSY_IO);\n\n\tmmc_retune_release(card->host);\n\n\treturn err;\n}\n\n#define MMC_READ_SINGLE_RETRIES\t2\n\n \nstatic void mmc_blk_read_single(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_request *mrq = &mqrq->brq.mrq;\n\tstruct mmc_card *card = mq->card;\n\tstruct mmc_host *host = card->host;\n\tblk_status_t error = BLK_STS_OK;\n\tsize_t bytes_per_read = queue_physical_block_size(mq->queue);\n\n\tdo {\n\t\tu32 status;\n\t\tint err;\n\t\tint retries = 0;\n\n\t\twhile (retries++ <= MMC_READ_SINGLE_RETRIES) {\n\t\t\tmmc_blk_rw_rq_prep(mqrq, card, 1, mq);\n\n\t\t\tmmc_wait_for_req(host, mrq);\n\n\t\t\terr = mmc_send_status(card, &status);\n\t\t\tif (err)\n\t\t\t\tgoto error_exit;\n\n\t\t\tif (!mmc_host_is_spi(host) &&\n\t\t\t    !mmc_ready_for_data(status)) {\n\t\t\t\terr = mmc_blk_fix_state(card, req);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto error_exit;\n\t\t\t}\n\n\t\t\tif (!mrq->cmd->error)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (mrq->cmd->error ||\n\t\t    mrq->data->error ||\n\t\t    (!mmc_host_is_spi(host) &&\n\t\t     (mrq->cmd->resp[0] & CMD_ERRORS || status & CMD_ERRORS)))\n\t\t\terror = BLK_STS_IOERR;\n\t\telse\n\t\t\terror = BLK_STS_OK;\n\n\t} while (blk_update_request(req, error, bytes_per_read));\n\n\treturn;\n\nerror_exit:\n\tmrq->data->bytes_xfered = 0;\n\tblk_update_request(req, BLK_STS_IOERR, bytes_per_read);\n\t \n\tif (mqrq->retries > MMC_MAX_RETRIES - 1)\n\t\tmqrq->retries = MMC_MAX_RETRIES - 1;\n}\n\nstatic inline bool mmc_blk_oor_valid(struct mmc_blk_request *brq)\n{\n\treturn !!brq->mrq.sbc;\n}\n\nstatic inline u32 mmc_blk_stop_err_bits(struct mmc_blk_request *brq)\n{\n\treturn mmc_blk_oor_valid(brq) ? CMD_ERRORS : CMD_ERRORS_EXCL_OOR;\n}\n\n \nstatic bool mmc_blk_status_error(struct request *req, u32 status)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_blk_request *brq = &mqrq->brq;\n\tstruct mmc_queue *mq = req->q->queuedata;\n\tu32 stop_err_bits;\n\n\tif (mmc_host_is_spi(mq->card->host))\n\t\treturn false;\n\n\tstop_err_bits = mmc_blk_stop_err_bits(brq);\n\n\treturn brq->cmd.resp[0]  & CMD_ERRORS    ||\n\t       brq->stop.resp[0] & stop_err_bits ||\n\t       status            & stop_err_bits ||\n\t       (rq_data_dir(req) == WRITE && !mmc_ready_for_data(status));\n}\n\nstatic inline bool mmc_blk_cmd_started(struct mmc_blk_request *brq)\n{\n\treturn !brq->sbc.error && !brq->cmd.error &&\n\t       !(brq->cmd.resp[0] & CMD_ERRORS);\n}\n\n \nstatic void mmc_blk_mq_rw_recovery(struct mmc_queue *mq, struct request *req)\n{\n\tint type = rq_data_dir(req) == READ ? MMC_BLK_READ : MMC_BLK_WRITE;\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_blk_request *brq = &mqrq->brq;\n\tstruct mmc_blk_data *md = mq->blkdata;\n\tstruct mmc_card *card = mq->card;\n\tu32 status;\n\tu32 blocks;\n\tint err;\n\n\t \n\terr = __mmc_send_status(card, &status, 0);\n\tif (err || mmc_blk_status_error(req, status))\n\t\tbrq->data.bytes_xfered = 0;\n\n\tmmc_retune_release(card->host);\n\n\t \n\tif (err)\n\t\terr = __mmc_send_status(card, &status, 0);\n\n\t \n\tif (err && mmc_detect_card_removed(card->host))\n\t\treturn;\n\n\t \n\tif (!mmc_host_is_spi(mq->card->host) &&\n\t    (err || !mmc_ready_for_data(status)))\n\t\terr = mmc_blk_fix_state(mq->card, req);\n\n\t \n\tif (!err && mmc_blk_cmd_started(brq) && mmc_card_sd(card) &&\n\t    rq_data_dir(req) == WRITE) {\n\t\tif (mmc_sd_num_wr_blocks(card, &blocks))\n\t\t\tbrq->data.bytes_xfered = 0;\n\t\telse\n\t\t\tbrq->data.bytes_xfered = blocks << 9;\n\t}\n\n\t \n\tif (!mmc_host_is_spi(mq->card->host) &&\n\t    err && mmc_blk_reset(md, card->host, type)) {\n\t\tpr_err(\"%s: recovery failed!\\n\", req->q->disk->disk_name);\n\t\tmqrq->retries = MMC_NO_RETRIES;\n\t\treturn;\n\t}\n\n\t \n\tif (brq->data.bytes_xfered)\n\t\treturn;\n\n\t \n\tif (mqrq->retries + 1 == MMC_MAX_RETRIES &&\n\t    mmc_blk_reset(md, card->host, type))\n\t\treturn;\n\n\t \n\tif (brq->sbc.error || brq->cmd.error)\n\t\treturn;\n\n\t \n\tif (mqrq->retries < MMC_MAX_RETRIES - MMC_DATA_RETRIES) {\n\t\tmqrq->retries = MMC_MAX_RETRIES - MMC_DATA_RETRIES;\n\t\treturn;\n\t}\n\n\tif (rq_data_dir(req) == READ && brq->data.blocks >\n\t\t\tqueue_physical_block_size(mq->queue) >> 9) {\n\t\t \n\t\tmmc_blk_read_single(mq, req);\n\t\treturn;\n\t}\n}\n\nstatic inline bool mmc_blk_rq_error(struct mmc_blk_request *brq)\n{\n\tmmc_blk_eval_resp_error(brq);\n\n\treturn brq->sbc.error || brq->cmd.error || brq->stop.error ||\n\t       brq->data.error || brq->cmd.resp[0] & CMD_ERRORS;\n}\n\nstatic int mmc_spi_err_check(struct mmc_card *card)\n{\n\tu32 status = 0;\n\tint err;\n\n\t \n\terr = __mmc_send_status(card, &status, 0);\n\tif (err)\n\t\treturn err;\n\t \n\tif (status)\n\t\treturn -EIO;\n\treturn 0;\n}\n\nstatic int mmc_blk_busy_cb(void *cb_data, bool *busy)\n{\n\tstruct mmc_blk_busy_data *data = cb_data;\n\tu32 status = 0;\n\tint err;\n\n\terr = mmc_send_status(data->card, &status);\n\tif (err)\n\t\treturn err;\n\n\t \n\tdata->status |= status;\n\n\t*busy = !mmc_ready_for_data(status);\n\treturn 0;\n}\n\nstatic int mmc_blk_card_busy(struct mmc_card *card, struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_blk_busy_data cb_data;\n\tint err;\n\n\tif (rq_data_dir(req) == READ)\n\t\treturn 0;\n\n\tif (mmc_host_is_spi(card->host)) {\n\t\terr = mmc_spi_err_check(card);\n\t\tif (err)\n\t\t\tmqrq->brq.data.bytes_xfered = 0;\n\t\treturn err;\n\t}\n\n\tcb_data.card = card;\n\tcb_data.status = 0;\n\terr = __mmc_poll_for_busy(card->host, 0, MMC_BLK_TIMEOUT_MS,\n\t\t\t\t  &mmc_blk_busy_cb, &cb_data);\n\n\t \n\tif (cb_data.status & mmc_blk_stop_err_bits(&mqrq->brq)) {\n\t\tmqrq->brq.data.bytes_xfered = 0;\n\t\terr = err ? err : -EIO;\n\t}\n\n\t \n\tif (mmc_card_mmc(card) && cb_data.status & R1_EXCEPTION_EVENT)\n\t\tmqrq->brq.cmd.resp[0] |= R1_EXCEPTION_EVENT;\n\n\treturn err;\n}\n\nstatic inline void mmc_blk_rw_reset_success(struct mmc_queue *mq,\n\t\t\t\t\t    struct request *req)\n{\n\tint type = rq_data_dir(req) == READ ? MMC_BLK_READ : MMC_BLK_WRITE;\n\n\tmmc_blk_reset_success(mq->blkdata, type);\n}\n\nstatic void mmc_blk_mq_complete_rq(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tunsigned int nr_bytes = mqrq->brq.data.bytes_xfered;\n\n\tif (nr_bytes) {\n\t\tif (blk_update_request(req, BLK_STS_OK, nr_bytes))\n\t\t\tblk_mq_requeue_request(req, true);\n\t\telse\n\t\t\t__blk_mq_end_request(req, BLK_STS_OK);\n\t} else if (!blk_rq_bytes(req)) {\n\t\t__blk_mq_end_request(req, BLK_STS_IOERR);\n\t} else if (mqrq->retries++ < MMC_MAX_RETRIES) {\n\t\tblk_mq_requeue_request(req, true);\n\t} else {\n\t\tif (mmc_card_removed(mq->card))\n\t\t\treq->rq_flags |= RQF_QUIET;\n\t\tblk_mq_end_request(req, BLK_STS_IOERR);\n\t}\n}\n\nstatic bool mmc_blk_urgent_bkops_needed(struct mmc_queue *mq,\n\t\t\t\t\tstruct mmc_queue_req *mqrq)\n{\n\treturn mmc_card_mmc(mq->card) && !mmc_host_is_spi(mq->card->host) &&\n\t       (mqrq->brq.cmd.resp[0] & R1_EXCEPTION_EVENT ||\n\t\tmqrq->brq.stop.resp[0] & R1_EXCEPTION_EVENT);\n}\n\nstatic void mmc_blk_urgent_bkops(struct mmc_queue *mq,\n\t\t\t\t struct mmc_queue_req *mqrq)\n{\n\tif (mmc_blk_urgent_bkops_needed(mq, mqrq))\n\t\tmmc_run_bkops(mq->card);\n}\n\nstatic void mmc_blk_hsq_req_done(struct mmc_request *mrq)\n{\n\tstruct mmc_queue_req *mqrq =\n\t\tcontainer_of(mrq, struct mmc_queue_req, brq.mrq);\n\tstruct request *req = mmc_queue_req_to_req(mqrq);\n\tstruct request_queue *q = req->q;\n\tstruct mmc_queue *mq = q->queuedata;\n\tstruct mmc_host *host = mq->card->host;\n\tunsigned long flags;\n\n\tif (mmc_blk_rq_error(&mqrq->brq) ||\n\t    mmc_blk_urgent_bkops_needed(mq, mqrq)) {\n\t\tspin_lock_irqsave(&mq->lock, flags);\n\t\tmq->recovery_needed = true;\n\t\tmq->recovery_req = req;\n\t\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\t\thost->cqe_ops->cqe_recovery_start(host);\n\n\t\tschedule_work(&mq->recovery_work);\n\t\treturn;\n\t}\n\n\tmmc_blk_rw_reset_success(mq, req);\n\n\t \n\tif (mq->in_recovery)\n\t\tmmc_blk_cqe_complete_rq(mq, req);\n\telse if (likely(!blk_should_fake_timeout(req->q)))\n\t\tblk_mq_complete_request(req);\n}\n\nvoid mmc_blk_mq_complete(struct request *req)\n{\n\tstruct mmc_queue *mq = req->q->queuedata;\n\tstruct mmc_host *host = mq->card->host;\n\n\tif (host->cqe_enabled)\n\t\tmmc_blk_cqe_complete_rq(mq, req);\n\telse if (likely(!blk_should_fake_timeout(req->q)))\n\t\tmmc_blk_mq_complete_rq(mq, req);\n}\n\nstatic void mmc_blk_mq_poll_completion(struct mmc_queue *mq,\n\t\t\t\t       struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_host *host = mq->card->host;\n\n\tif (mmc_blk_rq_error(&mqrq->brq) ||\n\t    mmc_blk_card_busy(mq->card, req)) {\n\t\tmmc_blk_mq_rw_recovery(mq, req);\n\t} else {\n\t\tmmc_blk_rw_reset_success(mq, req);\n\t\tmmc_retune_release(host);\n\t}\n\n\tmmc_blk_urgent_bkops(mq, mqrq);\n}\n\nstatic void mmc_blk_mq_dec_in_flight(struct mmc_queue *mq, enum mmc_issue_type issue_type)\n{\n\tunsigned long flags;\n\tbool put_card;\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\n\tmq->in_flight[issue_type] -= 1;\n\n\tput_card = (mmc_tot_in_flight(mq) == 0);\n\n\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\tif (put_card)\n\t\tmmc_put_card(mq->card, &mq->ctx);\n}\n\nstatic void mmc_blk_mq_post_req(struct mmc_queue *mq, struct request *req,\n\t\t\t\tbool can_sleep)\n{\n\tenum mmc_issue_type issue_type = mmc_issue_type(mq, req);\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_request *mrq = &mqrq->brq.mrq;\n\tstruct mmc_host *host = mq->card->host;\n\n\tmmc_post_req(host, mrq, 0);\n\n\t \n\tif (mq->in_recovery) {\n\t\tmmc_blk_mq_complete_rq(mq, req);\n\t} else if (likely(!blk_should_fake_timeout(req->q))) {\n\t\tif (can_sleep)\n\t\t\tblk_mq_complete_request_direct(req, mmc_blk_mq_complete);\n\t\telse\n\t\t\tblk_mq_complete_request(req);\n\t}\n\n\tmmc_blk_mq_dec_in_flight(mq, issue_type);\n}\n\nvoid mmc_blk_mq_recovery(struct mmc_queue *mq)\n{\n\tstruct request *req = mq->recovery_req;\n\tstruct mmc_host *host = mq->card->host;\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\n\tmq->recovery_req = NULL;\n\tmq->rw_wait = false;\n\n\tif (mmc_blk_rq_error(&mqrq->brq)) {\n\t\tmmc_retune_hold_now(host);\n\t\tmmc_blk_mq_rw_recovery(mq, req);\n\t}\n\n\tmmc_blk_urgent_bkops(mq, mqrq);\n\n\tmmc_blk_mq_post_req(mq, req, true);\n}\n\nstatic void mmc_blk_mq_complete_prev_req(struct mmc_queue *mq,\n\t\t\t\t\t struct request **prev_req)\n{\n\tif (mmc_host_done_complete(mq->card->host))\n\t\treturn;\n\n\tmutex_lock(&mq->complete_lock);\n\n\tif (!mq->complete_req)\n\t\tgoto out_unlock;\n\n\tmmc_blk_mq_poll_completion(mq, mq->complete_req);\n\n\tif (prev_req)\n\t\t*prev_req = mq->complete_req;\n\telse\n\t\tmmc_blk_mq_post_req(mq, mq->complete_req, true);\n\n\tmq->complete_req = NULL;\n\nout_unlock:\n\tmutex_unlock(&mq->complete_lock);\n}\n\nvoid mmc_blk_mq_complete_work(struct work_struct *work)\n{\n\tstruct mmc_queue *mq = container_of(work, struct mmc_queue,\n\t\t\t\t\t    complete_work);\n\n\tmmc_blk_mq_complete_prev_req(mq, NULL);\n}\n\nstatic void mmc_blk_mq_req_done(struct mmc_request *mrq)\n{\n\tstruct mmc_queue_req *mqrq = container_of(mrq, struct mmc_queue_req,\n\t\t\t\t\t\t  brq.mrq);\n\tstruct request *req = mmc_queue_req_to_req(mqrq);\n\tstruct request_queue *q = req->q;\n\tstruct mmc_queue *mq = q->queuedata;\n\tstruct mmc_host *host = mq->card->host;\n\tunsigned long flags;\n\n\tif (!mmc_host_done_complete(host)) {\n\t\tbool waiting;\n\n\t\t \n\t\tspin_lock_irqsave(&mq->lock, flags);\n\t\tmq->complete_req = req;\n\t\tmq->rw_wait = false;\n\t\twaiting = mq->waiting;\n\t\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\t\t \n\t\tif (waiting)\n\t\t\twake_up(&mq->wait);\n\t\telse\n\t\t\tqueue_work(mq->card->complete_wq, &mq->complete_work);\n\n\t\treturn;\n\t}\n\n\t \n\tif (mmc_blk_rq_error(&mqrq->brq) ||\n\t    mmc_blk_urgent_bkops_needed(mq, mqrq)) {\n\t\tspin_lock_irqsave(&mq->lock, flags);\n\t\tmq->recovery_needed = true;\n\t\tmq->recovery_req = req;\n\t\tspin_unlock_irqrestore(&mq->lock, flags);\n\t\twake_up(&mq->wait);\n\t\tschedule_work(&mq->recovery_work);\n\t\treturn;\n\t}\n\n\tmmc_blk_rw_reset_success(mq, req);\n\n\tmq->rw_wait = false;\n\twake_up(&mq->wait);\n\n\t \n\tmmc_blk_mq_post_req(mq, req, false);\n}\n\nstatic bool mmc_blk_rw_wait_cond(struct mmc_queue *mq, int *err)\n{\n\tunsigned long flags;\n\tbool done;\n\n\t \n\tspin_lock_irqsave(&mq->lock, flags);\n\tif (mq->recovery_needed) {\n\t\t*err = -EBUSY;\n\t\tdone = true;\n\t} else {\n\t\tdone = !mq->rw_wait;\n\t}\n\tmq->waiting = !done;\n\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\treturn done;\n}\n\nstatic int mmc_blk_rw_wait(struct mmc_queue *mq, struct request **prev_req)\n{\n\tint err = 0;\n\n\twait_event(mq->wait, mmc_blk_rw_wait_cond(mq, &err));\n\n\t \n\tmmc_blk_mq_complete_prev_req(mq, prev_req);\n\n\treturn err;\n}\n\nstatic int mmc_blk_mq_issue_rw_rq(struct mmc_queue *mq,\n\t\t\t\t  struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_host *host = mq->card->host;\n\tstruct request *prev_req = NULL;\n\tint err = 0;\n\n\tmmc_blk_rw_rq_prep(mqrq, mq->card, 0, mq);\n\n\tmqrq->brq.mrq.done = mmc_blk_mq_req_done;\n\n\tmmc_pre_req(host, &mqrq->brq.mrq);\n\n\terr = mmc_blk_rw_wait(mq, &prev_req);\n\tif (err)\n\t\tgoto out_post_req;\n\n\tmq->rw_wait = true;\n\n\terr = mmc_start_request(host, &mqrq->brq.mrq);\n\n\tif (prev_req)\n\t\tmmc_blk_mq_post_req(mq, prev_req, true);\n\n\tif (err)\n\t\tmq->rw_wait = false;\n\n\t \n\tif (err || mmc_host_done_complete(host))\n\t\tmmc_retune_release(host);\n\nout_post_req:\n\tif (err)\n\t\tmmc_post_req(host, &mqrq->brq.mrq, err);\n\n\treturn err;\n}\n\nstatic int mmc_blk_wait_for_idle(struct mmc_queue *mq, struct mmc_host *host)\n{\n\tif (host->cqe_enabled)\n\t\treturn host->cqe_ops->cqe_wait_for_idle(host);\n\n\treturn mmc_blk_rw_wait(mq, NULL);\n}\n\nenum mmc_issued mmc_blk_mq_issue_rq(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_blk_data *md = mq->blkdata;\n\tstruct mmc_card *card = md->queue.card;\n\tstruct mmc_host *host = card->host;\n\tint ret;\n\n\tret = mmc_blk_part_switch(card, md->part_type);\n\tif (ret)\n\t\treturn MMC_REQ_FAILED_TO_START;\n\n\tswitch (mmc_issue_type(mq, req)) {\n\tcase MMC_ISSUE_SYNC:\n\t\tret = mmc_blk_wait_for_idle(mq, host);\n\t\tif (ret)\n\t\t\treturn MMC_REQ_BUSY;\n\t\tswitch (req_op(req)) {\n\t\tcase REQ_OP_DRV_IN:\n\t\tcase REQ_OP_DRV_OUT:\n\t\t\tmmc_blk_issue_drv_op(mq, req);\n\t\t\tbreak;\n\t\tcase REQ_OP_DISCARD:\n\t\t\tmmc_blk_issue_discard_rq(mq, req);\n\t\t\tbreak;\n\t\tcase REQ_OP_SECURE_ERASE:\n\t\t\tmmc_blk_issue_secdiscard_rq(mq, req);\n\t\t\tbreak;\n\t\tcase REQ_OP_WRITE_ZEROES:\n\t\t\tmmc_blk_issue_trim_rq(mq, req);\n\t\t\tbreak;\n\t\tcase REQ_OP_FLUSH:\n\t\t\tmmc_blk_issue_flush(mq, req);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn MMC_REQ_FAILED_TO_START;\n\t\t}\n\t\treturn MMC_REQ_FINISHED;\n\tcase MMC_ISSUE_DCMD:\n\tcase MMC_ISSUE_ASYNC:\n\t\tswitch (req_op(req)) {\n\t\tcase REQ_OP_FLUSH:\n\t\t\tif (!mmc_cache_enabled(host)) {\n\t\t\t\tblk_mq_end_request(req, BLK_STS_OK);\n\t\t\t\treturn MMC_REQ_FINISHED;\n\t\t\t}\n\t\t\tret = mmc_blk_cqe_issue_flush(mq, req);\n\t\t\tbreak;\n\t\tcase REQ_OP_WRITE:\n\t\t\tcard->written_flag = true;\n\t\t\tfallthrough;\n\t\tcase REQ_OP_READ:\n\t\t\tif (host->cqe_enabled)\n\t\t\t\tret = mmc_blk_cqe_issue_rw_rq(mq, req);\n\t\t\telse\n\t\t\t\tret = mmc_blk_mq_issue_rw_rq(mq, req);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tif (!ret)\n\t\t\treturn MMC_REQ_STARTED;\n\t\treturn ret == -EBUSY ? MMC_REQ_BUSY : MMC_REQ_FAILED_TO_START;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn MMC_REQ_FAILED_TO_START;\n\t}\n}\n\nstatic inline int mmc_blk_readonly(struct mmc_card *card)\n{\n\treturn mmc_card_readonly(card) ||\n\t       !(card->csd.cmdclass & CCC_BLOCK_WRITE);\n}\n\nstatic struct mmc_blk_data *mmc_blk_alloc_req(struct mmc_card *card,\n\t\t\t\t\t      struct device *parent,\n\t\t\t\t\t      sector_t size,\n\t\t\t\t\t      bool default_ro,\n\t\t\t\t\t      const char *subname,\n\t\t\t\t\t      int area_type,\n\t\t\t\t\t      unsigned int part_type)\n{\n\tstruct mmc_blk_data *md;\n\tint devidx, ret;\n\tchar cap_str[10];\n\tbool cache_enabled = false;\n\tbool fua_enabled = false;\n\n\tdevidx = ida_simple_get(&mmc_blk_ida, 0, max_devices, GFP_KERNEL);\n\tif (devidx < 0) {\n\t\t \n\t\tif (devidx == -ENOSPC)\n\t\t\tdev_err(mmc_dev(card->host),\n\t\t\t\t\"no more device IDs available\\n\");\n\n\t\treturn ERR_PTR(devidx);\n\t}\n\n\tmd = kzalloc(sizeof(struct mmc_blk_data), GFP_KERNEL);\n\tif (!md) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmd->area_type = area_type;\n\n\t \n\tmd->read_only = mmc_blk_readonly(card);\n\n\tmd->disk = mmc_init_queue(&md->queue, card);\n\tif (IS_ERR(md->disk)) {\n\t\tret = PTR_ERR(md->disk);\n\t\tgoto err_kfree;\n\t}\n\n\tINIT_LIST_HEAD(&md->part);\n\tINIT_LIST_HEAD(&md->rpmbs);\n\tkref_init(&md->kref);\n\n\tmd->queue.blkdata = md;\n\tmd->part_type = part_type;\n\n\tmd->disk->major\t= MMC_BLOCK_MAJOR;\n\tmd->disk->minors = perdev_minors;\n\tmd->disk->first_minor = devidx * perdev_minors;\n\tmd->disk->fops = &mmc_bdops;\n\tmd->disk->private_data = md;\n\tmd->parent = parent;\n\tset_disk_ro(md->disk, md->read_only || default_ro);\n\tif (area_type & (MMC_BLK_DATA_AREA_RPMB | MMC_BLK_DATA_AREA_BOOT))\n\t\tmd->disk->flags |= GENHD_FL_NO_PART;\n\n\t \n\n\tsnprintf(md->disk->disk_name, sizeof(md->disk->disk_name),\n\t\t \"mmcblk%u%s\", card->host->index, subname ? subname : \"\");\n\n\tset_capacity(md->disk, size);\n\n\tif (mmc_host_cmd23(card->host)) {\n\t\tif ((mmc_card_mmc(card) &&\n\t\t     card->csd.mmca_vsn >= CSD_SPEC_VER_3) ||\n\t\t    (mmc_card_sd(card) &&\n\t\t     card->scr.cmds & SD_SCR_CMD23_SUPPORT))\n\t\t\tmd->flags |= MMC_BLK_CMD23;\n\t}\n\n\tif (md->flags & MMC_BLK_CMD23 &&\n\t    ((card->ext_csd.rel_param & EXT_CSD_WR_REL_PARAM_EN) ||\n\t     card->ext_csd.rel_sectors)) {\n\t\tmd->flags |= MMC_BLK_REL_WR;\n\t\tfua_enabled = true;\n\t\tcache_enabled = true;\n\t}\n\tif (mmc_cache_enabled(card->host))\n\t\tcache_enabled  = true;\n\n\tblk_queue_write_cache(md->queue.queue, cache_enabled, fua_enabled);\n\n\tstring_get_size((u64)size, 512, STRING_UNITS_2,\n\t\t\tcap_str, sizeof(cap_str));\n\tpr_info(\"%s: %s %s %s%s\\n\",\n\t\tmd->disk->disk_name, mmc_card_id(card), mmc_card_name(card),\n\t\tcap_str, md->read_only ? \" (ro)\" : \"\");\n\n\t \n\tif (area_type == MMC_BLK_DATA_AREA_MAIN)\n\t\tdev_set_drvdata(&card->dev, md);\n\tret = device_add_disk(md->parent, md->disk, mmc_disk_attr_groups);\n\tif (ret)\n\t\tgoto err_put_disk;\n\treturn md;\n\n err_put_disk:\n\tput_disk(md->disk);\n\tblk_mq_free_tag_set(&md->queue.tag_set);\n err_kfree:\n\tkfree(md);\n out:\n\tida_simple_remove(&mmc_blk_ida, devidx);\n\treturn ERR_PTR(ret);\n}\n\nstatic struct mmc_blk_data *mmc_blk_alloc(struct mmc_card *card)\n{\n\tsector_t size;\n\n\tif (!mmc_card_sd(card) && mmc_card_blockaddr(card)) {\n\t\t \n\t\tsize = card->ext_csd.sectors;\n\t} else {\n\t\t \n\t\tsize = (typeof(sector_t))card->csd.capacity\n\t\t\t<< (card->csd.read_blkbits - 9);\n\t}\n\n\treturn mmc_blk_alloc_req(card, &card->dev, size, false, NULL,\n\t\t\t\t\tMMC_BLK_DATA_AREA_MAIN, 0);\n}\n\nstatic int mmc_blk_alloc_part(struct mmc_card *card,\n\t\t\t      struct mmc_blk_data *md,\n\t\t\t      unsigned int part_type,\n\t\t\t      sector_t size,\n\t\t\t      bool default_ro,\n\t\t\t      const char *subname,\n\t\t\t      int area_type)\n{\n\tstruct mmc_blk_data *part_md;\n\n\tpart_md = mmc_blk_alloc_req(card, disk_to_dev(md->disk), size, default_ro,\n\t\t\t\t    subname, area_type, part_type);\n\tif (IS_ERR(part_md))\n\t\treturn PTR_ERR(part_md);\n\tlist_add(&part_md->part, &md->part);\n\n\treturn 0;\n}\n\n \nstatic long mmc_rpmb_ioctl(struct file *filp, unsigned int cmd,\n\t\t\t   unsigned long arg)\n{\n\tstruct mmc_rpmb_data *rpmb = filp->private_data;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase MMC_IOC_CMD:\n\t\tret = mmc_blk_ioctl_cmd(rpmb->md,\n\t\t\t\t\t(struct mmc_ioc_cmd __user *)arg,\n\t\t\t\t\trpmb);\n\t\tbreak;\n\tcase MMC_IOC_MULTI_CMD:\n\t\tret = mmc_blk_ioctl_multi_cmd(rpmb->md,\n\t\t\t\t\t(struct mmc_ioc_multi_cmd __user *)arg,\n\t\t\t\t\trpmb);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\nstatic long mmc_rpmb_ioctl_compat(struct file *filp, unsigned int cmd,\n\t\t\t      unsigned long arg)\n{\n\treturn mmc_rpmb_ioctl(filp, cmd, (unsigned long)compat_ptr(arg));\n}\n#endif\n\nstatic int mmc_rpmb_chrdev_open(struct inode *inode, struct file *filp)\n{\n\tstruct mmc_rpmb_data *rpmb = container_of(inode->i_cdev,\n\t\t\t\t\t\t  struct mmc_rpmb_data, chrdev);\n\n\tget_device(&rpmb->dev);\n\tfilp->private_data = rpmb;\n\tmmc_blk_get(rpmb->md->disk);\n\n\treturn nonseekable_open(inode, filp);\n}\n\nstatic int mmc_rpmb_chrdev_release(struct inode *inode, struct file *filp)\n{\n\tstruct mmc_rpmb_data *rpmb = container_of(inode->i_cdev,\n\t\t\t\t\t\t  struct mmc_rpmb_data, chrdev);\n\n\tmmc_blk_put(rpmb->md);\n\tput_device(&rpmb->dev);\n\n\treturn 0;\n}\n\nstatic const struct file_operations mmc_rpmb_fileops = {\n\t.release = mmc_rpmb_chrdev_release,\n\t.open = mmc_rpmb_chrdev_open,\n\t.owner = THIS_MODULE,\n\t.llseek = no_llseek,\n\t.unlocked_ioctl = mmc_rpmb_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = mmc_rpmb_ioctl_compat,\n#endif\n};\n\nstatic void mmc_blk_rpmb_device_release(struct device *dev)\n{\n\tstruct mmc_rpmb_data *rpmb = dev_get_drvdata(dev);\n\n\tida_simple_remove(&mmc_rpmb_ida, rpmb->id);\n\tkfree(rpmb);\n}\n\nstatic int mmc_blk_alloc_rpmb_part(struct mmc_card *card,\n\t\t\t\t   struct mmc_blk_data *md,\n\t\t\t\t   unsigned int part_index,\n\t\t\t\t   sector_t size,\n\t\t\t\t   const char *subname)\n{\n\tint devidx, ret;\n\tchar rpmb_name[DISK_NAME_LEN];\n\tchar cap_str[10];\n\tstruct mmc_rpmb_data *rpmb;\n\n\t \n\tdevidx = ida_simple_get(&mmc_rpmb_ida, 0, max_devices, GFP_KERNEL);\n\tif (devidx < 0)\n\t\treturn devidx;\n\n\trpmb = kzalloc(sizeof(*rpmb), GFP_KERNEL);\n\tif (!rpmb) {\n\t\tida_simple_remove(&mmc_rpmb_ida, devidx);\n\t\treturn -ENOMEM;\n\t}\n\n\tsnprintf(rpmb_name, sizeof(rpmb_name),\n\t\t \"mmcblk%u%s\", card->host->index, subname ? subname : \"\");\n\n\trpmb->id = devidx;\n\trpmb->part_index = part_index;\n\trpmb->dev.init_name = rpmb_name;\n\trpmb->dev.bus = &mmc_rpmb_bus_type;\n\trpmb->dev.devt = MKDEV(MAJOR(mmc_rpmb_devt), rpmb->id);\n\trpmb->dev.parent = &card->dev;\n\trpmb->dev.release = mmc_blk_rpmb_device_release;\n\tdevice_initialize(&rpmb->dev);\n\tdev_set_drvdata(&rpmb->dev, rpmb);\n\trpmb->md = md;\n\n\tcdev_init(&rpmb->chrdev, &mmc_rpmb_fileops);\n\trpmb->chrdev.owner = THIS_MODULE;\n\tret = cdev_device_add(&rpmb->chrdev, &rpmb->dev);\n\tif (ret) {\n\t\tpr_err(\"%s: could not add character device\\n\", rpmb_name);\n\t\tgoto out_put_device;\n\t}\n\n\tlist_add(&rpmb->node, &md->rpmbs);\n\n\tstring_get_size((u64)size, 512, STRING_UNITS_2,\n\t\t\tcap_str, sizeof(cap_str));\n\n\tpr_info(\"%s: %s %s %s, chardev (%d:%d)\\n\",\n\t\trpmb_name, mmc_card_id(card), mmc_card_name(card), cap_str,\n\t\tMAJOR(mmc_rpmb_devt), rpmb->id);\n\n\treturn 0;\n\nout_put_device:\n\tput_device(&rpmb->dev);\n\treturn ret;\n}\n\nstatic void mmc_blk_remove_rpmb_part(struct mmc_rpmb_data *rpmb)\n\n{\n\tcdev_device_del(&rpmb->chrdev, &rpmb->dev);\n\tput_device(&rpmb->dev);\n}\n\n \n\nstatic int mmc_blk_alloc_parts(struct mmc_card *card, struct mmc_blk_data *md)\n{\n\tint idx, ret;\n\n\tif (!mmc_card_mmc(card))\n\t\treturn 0;\n\n\tfor (idx = 0; idx < card->nr_parts; idx++) {\n\t\tif (card->part[idx].area_type & MMC_BLK_DATA_AREA_RPMB) {\n\t\t\t \n\t\t\tret = mmc_blk_alloc_rpmb_part(card, md,\n\t\t\t\tcard->part[idx].part_cfg,\n\t\t\t\tcard->part[idx].size >> 9,\n\t\t\t\tcard->part[idx].name);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t} else if (card->part[idx].size) {\n\t\t\tret = mmc_blk_alloc_part(card, md,\n\t\t\t\tcard->part[idx].part_cfg,\n\t\t\t\tcard->part[idx].size >> 9,\n\t\t\t\tcard->part[idx].force_ro,\n\t\t\t\tcard->part[idx].name,\n\t\t\t\tcard->part[idx].area_type);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void mmc_blk_remove_req(struct mmc_blk_data *md)\n{\n\t \n\tdel_gendisk(md->disk);\n\tmmc_cleanup_queue(&md->queue);\n\tmmc_blk_put(md);\n}\n\nstatic void mmc_blk_remove_parts(struct mmc_card *card,\n\t\t\t\t struct mmc_blk_data *md)\n{\n\tstruct list_head *pos, *q;\n\tstruct mmc_blk_data *part_md;\n\tstruct mmc_rpmb_data *rpmb;\n\n\t \n\tlist_for_each_safe(pos, q, &md->rpmbs) {\n\t\trpmb = list_entry(pos, struct mmc_rpmb_data, node);\n\t\tlist_del(pos);\n\t\tmmc_blk_remove_rpmb_part(rpmb);\n\t}\n\t \n\tlist_for_each_safe(pos, q, &md->part) {\n\t\tpart_md = list_entry(pos, struct mmc_blk_data, part);\n\t\tlist_del(pos);\n\t\tmmc_blk_remove_req(part_md);\n\t}\n}\n\n#ifdef CONFIG_DEBUG_FS\n\nstatic int mmc_dbg_card_status_get(void *data, u64 *val)\n{\n\tstruct mmc_card *card = data;\n\tstruct mmc_blk_data *md = dev_get_drvdata(&card->dev);\n\tstruct mmc_queue *mq = &md->queue;\n\tstruct request *req;\n\tint ret;\n\n\t \n\treq = blk_mq_alloc_request(mq->queue, REQ_OP_DRV_IN, 0);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\treq_to_mmc_queue_req(req)->drv_op = MMC_DRV_OP_GET_CARD_STATUS;\n\treq_to_mmc_queue_req(req)->drv_op_result = -EIO;\n\tblk_execute_rq(req, false);\n\tret = req_to_mmc_queue_req(req)->drv_op_result;\n\tif (ret >= 0) {\n\t\t*val = ret;\n\t\tret = 0;\n\t}\n\tblk_mq_free_request(req);\n\n\treturn ret;\n}\nDEFINE_DEBUGFS_ATTRIBUTE(mmc_dbg_card_status_fops, mmc_dbg_card_status_get,\n\t\t\t NULL, \"%08llx\\n\");\n\n \n#define EXT_CSD_STR_LEN 1025\n\nstatic int mmc_ext_csd_open(struct inode *inode, struct file *filp)\n{\n\tstruct mmc_card *card = inode->i_private;\n\tstruct mmc_blk_data *md = dev_get_drvdata(&card->dev);\n\tstruct mmc_queue *mq = &md->queue;\n\tstruct request *req;\n\tchar *buf;\n\tssize_t n = 0;\n\tu8 *ext_csd;\n\tint err, i;\n\n\tbuf = kmalloc(EXT_CSD_STR_LEN + 1, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\t \n\treq = blk_mq_alloc_request(mq->queue, REQ_OP_DRV_IN, 0);\n\tif (IS_ERR(req)) {\n\t\terr = PTR_ERR(req);\n\t\tgoto out_free;\n\t}\n\treq_to_mmc_queue_req(req)->drv_op = MMC_DRV_OP_GET_EXT_CSD;\n\treq_to_mmc_queue_req(req)->drv_op_result = -EIO;\n\treq_to_mmc_queue_req(req)->drv_op_data = &ext_csd;\n\tblk_execute_rq(req, false);\n\terr = req_to_mmc_queue_req(req)->drv_op_result;\n\tblk_mq_free_request(req);\n\tif (err) {\n\t\tpr_err(\"FAILED %d\\n\", err);\n\t\tgoto out_free;\n\t}\n\n\tfor (i = 0; i < 512; i++)\n\t\tn += sprintf(buf + n, \"%02x\", ext_csd[i]);\n\tn += sprintf(buf + n, \"\\n\");\n\n\tif (n != EXT_CSD_STR_LEN) {\n\t\terr = -EINVAL;\n\t\tkfree(ext_csd);\n\t\tgoto out_free;\n\t}\n\n\tfilp->private_data = buf;\n\tkfree(ext_csd);\n\treturn 0;\n\nout_free:\n\tkfree(buf);\n\treturn err;\n}\n\nstatic ssize_t mmc_ext_csd_read(struct file *filp, char __user *ubuf,\n\t\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tchar *buf = filp->private_data;\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos,\n\t\t\t\t       buf, EXT_CSD_STR_LEN);\n}\n\nstatic int mmc_ext_csd_release(struct inode *inode, struct file *file)\n{\n\tkfree(file->private_data);\n\treturn 0;\n}\n\nstatic const struct file_operations mmc_dbg_ext_csd_fops = {\n\t.open\t\t= mmc_ext_csd_open,\n\t.read\t\t= mmc_ext_csd_read,\n\t.release\t= mmc_ext_csd_release,\n\t.llseek\t\t= default_llseek,\n};\n\nstatic void mmc_blk_add_debugfs(struct mmc_card *card, struct mmc_blk_data *md)\n{\n\tstruct dentry *root;\n\n\tif (!card->debugfs_root)\n\t\treturn;\n\n\troot = card->debugfs_root;\n\n\tif (mmc_card_mmc(card) || mmc_card_sd(card)) {\n\t\tmd->status_dentry =\n\t\t\tdebugfs_create_file_unsafe(\"status\", 0400, root,\n\t\t\t\t\t\t   card,\n\t\t\t\t\t\t   &mmc_dbg_card_status_fops);\n\t}\n\n\tif (mmc_card_mmc(card)) {\n\t\tmd->ext_csd_dentry =\n\t\t\tdebugfs_create_file(\"ext_csd\", S_IRUSR, root, card,\n\t\t\t\t\t    &mmc_dbg_ext_csd_fops);\n\t}\n}\n\nstatic void mmc_blk_remove_debugfs(struct mmc_card *card,\n\t\t\t\t   struct mmc_blk_data *md)\n{\n\tif (!card->debugfs_root)\n\t\treturn;\n\n\tdebugfs_remove(md->status_dentry);\n\tmd->status_dentry = NULL;\n\n\tdebugfs_remove(md->ext_csd_dentry);\n\tmd->ext_csd_dentry = NULL;\n}\n\n#else\n\nstatic void mmc_blk_add_debugfs(struct mmc_card *card, struct mmc_blk_data *md)\n{\n}\n\nstatic void mmc_blk_remove_debugfs(struct mmc_card *card,\n\t\t\t\t   struct mmc_blk_data *md)\n{\n}\n\n#endif  \n\nstatic int mmc_blk_probe(struct mmc_card *card)\n{\n\tstruct mmc_blk_data *md;\n\tint ret = 0;\n\n\t \n\tif (!(card->csd.cmdclass & CCC_BLOCK_READ))\n\t\treturn -ENODEV;\n\n\tmmc_fixup_device(card, mmc_blk_fixups);\n\n\tcard->complete_wq = alloc_workqueue(\"mmc_complete\",\n\t\t\t\t\tWQ_MEM_RECLAIM | WQ_HIGHPRI, 0);\n\tif (!card->complete_wq) {\n\t\tpr_err(\"Failed to create mmc completion workqueue\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmd = mmc_blk_alloc(card);\n\tif (IS_ERR(md)) {\n\t\tret = PTR_ERR(md);\n\t\tgoto out_free;\n\t}\n\n\tret = mmc_blk_alloc_parts(card, md);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tmmc_blk_add_debugfs(card, md);\n\n\tpm_runtime_set_autosuspend_delay(&card->dev, 3000);\n\tpm_runtime_use_autosuspend(&card->dev);\n\n\t \n\tif (!mmc_card_sd_combo(card)) {\n\t\tpm_runtime_set_active(&card->dev);\n\t\tpm_runtime_enable(&card->dev);\n\t}\n\n\treturn 0;\n\nout:\n\tmmc_blk_remove_parts(card, md);\n\tmmc_blk_remove_req(md);\nout_free:\n\tdestroy_workqueue(card->complete_wq);\n\treturn ret;\n}\n\nstatic void mmc_blk_remove(struct mmc_card *card)\n{\n\tstruct mmc_blk_data *md = dev_get_drvdata(&card->dev);\n\n\tmmc_blk_remove_debugfs(card, md);\n\tmmc_blk_remove_parts(card, md);\n\tpm_runtime_get_sync(&card->dev);\n\tif (md->part_curr != md->part_type) {\n\t\tmmc_claim_host(card->host);\n\t\tmmc_blk_part_switch(card, md->part_type);\n\t\tmmc_release_host(card->host);\n\t}\n\tif (!mmc_card_sd_combo(card))\n\t\tpm_runtime_disable(&card->dev);\n\tpm_runtime_put_noidle(&card->dev);\n\tmmc_blk_remove_req(md);\n\tdestroy_workqueue(card->complete_wq);\n}\n\nstatic int _mmc_blk_suspend(struct mmc_card *card)\n{\n\tstruct mmc_blk_data *part_md;\n\tstruct mmc_blk_data *md = dev_get_drvdata(&card->dev);\n\n\tif (md) {\n\t\tmmc_queue_suspend(&md->queue);\n\t\tlist_for_each_entry(part_md, &md->part, part) {\n\t\t\tmmc_queue_suspend(&part_md->queue);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void mmc_blk_shutdown(struct mmc_card *card)\n{\n\t_mmc_blk_suspend(card);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int mmc_blk_suspend(struct device *dev)\n{\n\tstruct mmc_card *card = mmc_dev_to_card(dev);\n\n\treturn _mmc_blk_suspend(card);\n}\n\nstatic int mmc_blk_resume(struct device *dev)\n{\n\tstruct mmc_blk_data *part_md;\n\tstruct mmc_blk_data *md = dev_get_drvdata(dev);\n\n\tif (md) {\n\t\t \n\t\tmd->part_curr = md->part_type;\n\t\tmmc_queue_resume(&md->queue);\n\t\tlist_for_each_entry(part_md, &md->part, part) {\n\t\t\tmmc_queue_resume(&part_md->queue);\n\t\t}\n\t}\n\treturn 0;\n}\n#endif\n\nstatic SIMPLE_DEV_PM_OPS(mmc_blk_pm_ops, mmc_blk_suspend, mmc_blk_resume);\n\nstatic struct mmc_driver mmc_driver = {\n\t.drv\t\t= {\n\t\t.name\t= \"mmcblk\",\n\t\t.pm\t= &mmc_blk_pm_ops,\n\t},\n\t.probe\t\t= mmc_blk_probe,\n\t.remove\t\t= mmc_blk_remove,\n\t.shutdown\t= mmc_blk_shutdown,\n};\n\nstatic int __init mmc_blk_init(void)\n{\n\tint res;\n\n\tres  = bus_register(&mmc_rpmb_bus_type);\n\tif (res < 0) {\n\t\tpr_err(\"mmcblk: could not register RPMB bus type\\n\");\n\t\treturn res;\n\t}\n\tres = alloc_chrdev_region(&mmc_rpmb_devt, 0, MAX_DEVICES, \"rpmb\");\n\tif (res < 0) {\n\t\tpr_err(\"mmcblk: failed to allocate rpmb chrdev region\\n\");\n\t\tgoto out_bus_unreg;\n\t}\n\n\tif (perdev_minors != CONFIG_MMC_BLOCK_MINORS)\n\t\tpr_info(\"mmcblk: using %d minors per device\\n\", perdev_minors);\n\n\tmax_devices = min(MAX_DEVICES, (1 << MINORBITS) / perdev_minors);\n\n\tres = register_blkdev(MMC_BLOCK_MAJOR, \"mmc\");\n\tif (res)\n\t\tgoto out_chrdev_unreg;\n\n\tres = mmc_register_driver(&mmc_driver);\n\tif (res)\n\t\tgoto out_blkdev_unreg;\n\n\treturn 0;\n\nout_blkdev_unreg:\n\tunregister_blkdev(MMC_BLOCK_MAJOR, \"mmc\");\nout_chrdev_unreg:\n\tunregister_chrdev_region(mmc_rpmb_devt, MAX_DEVICES);\nout_bus_unreg:\n\tbus_unregister(&mmc_rpmb_bus_type);\n\treturn res;\n}\n\nstatic void __exit mmc_blk_exit(void)\n{\n\tmmc_unregister_driver(&mmc_driver);\n\tunregister_blkdev(MMC_BLOCK_MAJOR, \"mmc\");\n\tunregister_chrdev_region(mmc_rpmb_devt, MAX_DEVICES);\n\tbus_unregister(&mmc_rpmb_bus_type);\n}\n\nmodule_init(mmc_blk_init);\nmodule_exit(mmc_blk_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Multimedia Card (MMC) block device driver\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}