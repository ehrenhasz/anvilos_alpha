{
  "module_name": "queue.c",
  "hash_id": "46e07b3f9dafa58f88940826dcbf97b486eee9d248f07f5abd5d8808c9bfb303",
  "original_prompt": "Ingested from linux-6.6.14/drivers/mmc/core/queue.c",
  "human_readable_source": "\n \n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/blkdev.h>\n#include <linux/freezer.h>\n#include <linux/scatterlist.h>\n#include <linux/dma-mapping.h>\n#include <linux/backing-dev.h>\n\n#include <linux/mmc/card.h>\n#include <linux/mmc/host.h>\n\n#include \"queue.h\"\n#include \"block.h\"\n#include \"core.h\"\n#include \"card.h\"\n#include \"crypto.h\"\n#include \"host.h\"\n\n#define MMC_DMA_MAP_MERGE_SEGMENTS\t512\n\nstatic inline bool mmc_cqe_dcmd_busy(struct mmc_queue *mq)\n{\n\t \n\treturn mq->in_flight[MMC_ISSUE_DCMD];\n}\n\nvoid mmc_cqe_check_busy(struct mmc_queue *mq)\n{\n\tif ((mq->cqe_busy & MMC_CQE_DCMD_BUSY) && !mmc_cqe_dcmd_busy(mq))\n\t\tmq->cqe_busy &= ~MMC_CQE_DCMD_BUSY;\n}\n\nstatic inline bool mmc_cqe_can_dcmd(struct mmc_host *host)\n{\n\treturn host->caps2 & MMC_CAP2_CQE_DCMD;\n}\n\nstatic enum mmc_issue_type mmc_cqe_issue_type(struct mmc_host *host,\n\t\t\t\t\t      struct request *req)\n{\n\tswitch (req_op(req)) {\n\tcase REQ_OP_DRV_IN:\n\tcase REQ_OP_DRV_OUT:\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_SECURE_ERASE:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\treturn MMC_ISSUE_SYNC;\n\tcase REQ_OP_FLUSH:\n\t\treturn mmc_cqe_can_dcmd(host) ? MMC_ISSUE_DCMD : MMC_ISSUE_SYNC;\n\tdefault:\n\t\treturn MMC_ISSUE_ASYNC;\n\t}\n}\n\nenum mmc_issue_type mmc_issue_type(struct mmc_queue *mq, struct request *req)\n{\n\tstruct mmc_host *host = mq->card->host;\n\n\tif (host->cqe_enabled && !host->hsq_enabled)\n\t\treturn mmc_cqe_issue_type(host, req);\n\n\tif (req_op(req) == REQ_OP_READ || req_op(req) == REQ_OP_WRITE)\n\t\treturn MMC_ISSUE_ASYNC;\n\n\treturn MMC_ISSUE_SYNC;\n}\n\nstatic void __mmc_cqe_recovery_notifier(struct mmc_queue *mq)\n{\n\tif (!mq->recovery_needed) {\n\t\tmq->recovery_needed = true;\n\t\tschedule_work(&mq->recovery_work);\n\t}\n}\n\nvoid mmc_cqe_recovery_notifier(struct mmc_request *mrq)\n{\n\tstruct mmc_queue_req *mqrq = container_of(mrq, struct mmc_queue_req,\n\t\t\t\t\t\t  brq.mrq);\n\tstruct request *req = mmc_queue_req_to_req(mqrq);\n\tstruct request_queue *q = req->q;\n\tstruct mmc_queue *mq = q->queuedata;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\t__mmc_cqe_recovery_notifier(mq);\n\tspin_unlock_irqrestore(&mq->lock, flags);\n}\n\nstatic enum blk_eh_timer_return mmc_cqe_timed_out(struct request *req)\n{\n\tstruct mmc_queue_req *mqrq = req_to_mmc_queue_req(req);\n\tstruct mmc_request *mrq = &mqrq->brq.mrq;\n\tstruct mmc_queue *mq = req->q->queuedata;\n\tstruct mmc_host *host = mq->card->host;\n\tenum mmc_issue_type issue_type = mmc_issue_type(mq, req);\n\tbool recovery_needed = false;\n\n\tswitch (issue_type) {\n\tcase MMC_ISSUE_ASYNC:\n\tcase MMC_ISSUE_DCMD:\n\t\tif (host->cqe_ops->cqe_timeout(host, mrq, &recovery_needed)) {\n\t\t\tif (recovery_needed)\n\t\t\t\tmmc_cqe_recovery_notifier(mrq);\n\t\t\treturn BLK_EH_RESET_TIMER;\n\t\t}\n\t\t \n\t\treturn BLK_EH_DONE;\n\tdefault:\n\t\t \n\t\treturn BLK_EH_RESET_TIMER;\n\t}\n}\n\nstatic enum blk_eh_timer_return mmc_mq_timed_out(struct request *req)\n{\n\tstruct request_queue *q = req->q;\n\tstruct mmc_queue *mq = q->queuedata;\n\tstruct mmc_card *card = mq->card;\n\tstruct mmc_host *host = card->host;\n\tunsigned long flags;\n\tbool ignore_tout;\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\tignore_tout = mq->recovery_needed || !host->cqe_enabled || host->hsq_enabled;\n\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\treturn ignore_tout ? BLK_EH_RESET_TIMER : mmc_cqe_timed_out(req);\n}\n\nstatic void mmc_mq_recovery_handler(struct work_struct *work)\n{\n\tstruct mmc_queue *mq = container_of(work, struct mmc_queue,\n\t\t\t\t\t    recovery_work);\n\tstruct request_queue *q = mq->queue;\n\tstruct mmc_host *host = mq->card->host;\n\n\tmmc_get_card(mq->card, &mq->ctx);\n\n\tmq->in_recovery = true;\n\n\tif (host->cqe_enabled && !host->hsq_enabled)\n\t\tmmc_blk_cqe_recovery(mq);\n\telse\n\t\tmmc_blk_mq_recovery(mq);\n\n\tmq->in_recovery = false;\n\n\tspin_lock_irq(&mq->lock);\n\tmq->recovery_needed = false;\n\tspin_unlock_irq(&mq->lock);\n\n\tif (host->hsq_enabled)\n\t\thost->cqe_ops->cqe_recovery_finish(host);\n\n\tmmc_put_card(mq->card, &mq->ctx);\n\n\tblk_mq_run_hw_queues(q, true);\n}\n\nstatic struct scatterlist *mmc_alloc_sg(unsigned short sg_len, gfp_t gfp)\n{\n\tstruct scatterlist *sg;\n\n\tsg = kmalloc_array(sg_len, sizeof(*sg), gfp);\n\tif (sg)\n\t\tsg_init_table(sg, sg_len);\n\n\treturn sg;\n}\n\nstatic void mmc_queue_setup_discard(struct request_queue *q,\n\t\t\t\t    struct mmc_card *card)\n{\n\tunsigned max_discard;\n\n\tmax_discard = mmc_calc_max_discard(card);\n\tif (!max_discard)\n\t\treturn;\n\n\tblk_queue_max_discard_sectors(q, max_discard);\n\tq->limits.discard_granularity = card->pref_erase << 9;\n\t \n\tif (card->pref_erase > max_discard)\n\t\tq->limits.discard_granularity = SECTOR_SIZE;\n\tif (mmc_can_secure_erase_trim(card))\n\t\tblk_queue_max_secure_erase_sectors(q, max_discard);\n\tif (mmc_can_trim(card) && card->erased_byte == 0)\n\t\tblk_queue_max_write_zeroes_sectors(q, max_discard);\n}\n\nstatic unsigned short mmc_get_max_segments(struct mmc_host *host)\n{\n\treturn host->can_dma_map_merge ? MMC_DMA_MAP_MERGE_SEGMENTS :\n\t\t\t\t\t host->max_segs;\n}\n\nstatic int mmc_mq_init_request(struct blk_mq_tag_set *set, struct request *req,\n\t\t\t       unsigned int hctx_idx, unsigned int numa_node)\n{\n\tstruct mmc_queue_req *mq_rq = req_to_mmc_queue_req(req);\n\tstruct mmc_queue *mq = set->driver_data;\n\tstruct mmc_card *card = mq->card;\n\tstruct mmc_host *host = card->host;\n\n\tmq_rq->sg = mmc_alloc_sg(mmc_get_max_segments(host), GFP_KERNEL);\n\tif (!mq_rq->sg)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void mmc_mq_exit_request(struct blk_mq_tag_set *set, struct request *req,\n\t\t\t\tunsigned int hctx_idx)\n{\n\tstruct mmc_queue_req *mq_rq = req_to_mmc_queue_req(req);\n\n\tkfree(mq_rq->sg);\n\tmq_rq->sg = NULL;\n}\n\nstatic blk_status_t mmc_mq_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t    const struct blk_mq_queue_data *bd)\n{\n\tstruct request *req = bd->rq;\n\tstruct request_queue *q = req->q;\n\tstruct mmc_queue *mq = q->queuedata;\n\tstruct mmc_card *card = mq->card;\n\tstruct mmc_host *host = card->host;\n\tenum mmc_issue_type issue_type;\n\tenum mmc_issued issued;\n\tbool get_card, cqe_retune_ok;\n\tblk_status_t ret;\n\n\tif (mmc_card_removed(mq->card)) {\n\t\treq->rq_flags |= RQF_QUIET;\n\t\treturn BLK_STS_IOERR;\n\t}\n\n\tissue_type = mmc_issue_type(mq, req);\n\n\tspin_lock_irq(&mq->lock);\n\n\tif (mq->recovery_needed || mq->busy) {\n\t\tspin_unlock_irq(&mq->lock);\n\t\treturn BLK_STS_RESOURCE;\n\t}\n\n\tswitch (issue_type) {\n\tcase MMC_ISSUE_DCMD:\n\t\tif (mmc_cqe_dcmd_busy(mq)) {\n\t\t\tmq->cqe_busy |= MMC_CQE_DCMD_BUSY;\n\t\t\tspin_unlock_irq(&mq->lock);\n\t\t\treturn BLK_STS_RESOURCE;\n\t\t}\n\t\tbreak;\n\tcase MMC_ISSUE_ASYNC:\n\t\t \n\t\tif (host->hsq_enabled && mq->in_flight[issue_type] > 2) {\n\t\t\tspin_unlock_irq(&mq->lock);\n\t\t\treturn BLK_STS_RESOURCE;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treq->timeout = 600 * HZ;\n\t\tbreak;\n\t}\n\n\t \n\tmq->busy = true;\n\n\tmq->in_flight[issue_type] += 1;\n\tget_card = (mmc_tot_in_flight(mq) == 1);\n\tcqe_retune_ok = (mmc_cqe_qcnt(mq) == 1);\n\n\tspin_unlock_irq(&mq->lock);\n\n\tif (!(req->rq_flags & RQF_DONTPREP)) {\n\t\treq_to_mmc_queue_req(req)->retries = 0;\n\t\treq->rq_flags |= RQF_DONTPREP;\n\t}\n\n\tif (get_card)\n\t\tmmc_get_card(card, &mq->ctx);\n\n\tif (host->cqe_enabled) {\n\t\thost->retune_now = host->need_retune && cqe_retune_ok &&\n\t\t\t\t   !host->hold_retune;\n\t}\n\n\tblk_mq_start_request(req);\n\n\tissued = mmc_blk_mq_issue_rq(mq, req);\n\n\tswitch (issued) {\n\tcase MMC_REQ_BUSY:\n\t\tret = BLK_STS_RESOURCE;\n\t\tbreak;\n\tcase MMC_REQ_FAILED_TO_START:\n\t\tret = BLK_STS_IOERR;\n\t\tbreak;\n\tdefault:\n\t\tret = BLK_STS_OK;\n\t\tbreak;\n\t}\n\n\tif (issued != MMC_REQ_STARTED) {\n\t\tbool put_card = false;\n\n\t\tspin_lock_irq(&mq->lock);\n\t\tmq->in_flight[issue_type] -= 1;\n\t\tif (mmc_tot_in_flight(mq) == 0)\n\t\t\tput_card = true;\n\t\tmq->busy = false;\n\t\tspin_unlock_irq(&mq->lock);\n\t\tif (put_card)\n\t\t\tmmc_put_card(card, &mq->ctx);\n\t} else {\n\t\tWRITE_ONCE(mq->busy, false);\n\t}\n\n\treturn ret;\n}\n\nstatic const struct blk_mq_ops mmc_mq_ops = {\n\t.queue_rq\t= mmc_mq_queue_rq,\n\t.init_request\t= mmc_mq_init_request,\n\t.exit_request\t= mmc_mq_exit_request,\n\t.complete\t= mmc_blk_mq_complete,\n\t.timeout\t= mmc_mq_timed_out,\n};\n\nstatic void mmc_setup_queue(struct mmc_queue *mq, struct mmc_card *card)\n{\n\tstruct mmc_host *host = card->host;\n\tunsigned block_size = 512;\n\n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, mq->queue);\n\tblk_queue_flag_clear(QUEUE_FLAG_ADD_RANDOM, mq->queue);\n\tif (mmc_can_erase(card))\n\t\tmmc_queue_setup_discard(mq->queue, card);\n\n\tif (!mmc_dev(host)->dma_mask || !*mmc_dev(host)->dma_mask)\n\t\tblk_queue_bounce_limit(mq->queue, BLK_BOUNCE_HIGH);\n\tblk_queue_max_hw_sectors(mq->queue,\n\t\tmin(host->max_blk_count, host->max_req_size / 512));\n\tif (host->can_dma_map_merge)\n\t\tWARN(!blk_queue_can_use_dma_map_merging(mq->queue,\n\t\t\t\t\t\t\tmmc_dev(host)),\n\t\t     \"merging was advertised but not possible\");\n\tblk_queue_max_segments(mq->queue, mmc_get_max_segments(host));\n\n\tif (mmc_card_mmc(card) && card->ext_csd.data_sector_size) {\n\t\tblock_size = card->ext_csd.data_sector_size;\n\t\tWARN_ON(block_size != 512 && block_size != 4096);\n\t}\n\n\tblk_queue_logical_block_size(mq->queue, block_size);\n\t \n\tif (!host->can_dma_map_merge)\n\t\tblk_queue_max_segment_size(mq->queue,\n\t\t\tround_down(host->max_seg_size, block_size));\n\n\tdma_set_max_seg_size(mmc_dev(host), queue_max_segment_size(mq->queue));\n\n\tINIT_WORK(&mq->recovery_work, mmc_mq_recovery_handler);\n\tINIT_WORK(&mq->complete_work, mmc_blk_mq_complete_work);\n\n\tmutex_init(&mq->complete_lock);\n\n\tinit_waitqueue_head(&mq->wait);\n\n\tmmc_crypto_setup_queue(mq->queue, host);\n}\n\nstatic inline bool mmc_merge_capable(struct mmc_host *host)\n{\n\treturn host->caps2 & MMC_CAP2_MERGE_CAPABLE;\n}\n\n \n#define MMC_QUEUE_DEPTH 64\n\n \nstruct gendisk *mmc_init_queue(struct mmc_queue *mq, struct mmc_card *card)\n{\n\tstruct mmc_host *host = card->host;\n\tstruct gendisk *disk;\n\tint ret;\n\n\tmq->card = card;\n\t\n\tspin_lock_init(&mq->lock);\n\n\tmemset(&mq->tag_set, 0, sizeof(mq->tag_set));\n\tmq->tag_set.ops = &mmc_mq_ops;\n\t \n\tif (host->cqe_enabled && !host->hsq_enabled)\n\t\tmq->tag_set.queue_depth =\n\t\t\tmin_t(int, card->ext_csd.cmdq_depth, host->cqe_qdepth);\n\telse\n\t\tmq->tag_set.queue_depth = MMC_QUEUE_DEPTH;\n\tmq->tag_set.numa_node = NUMA_NO_NODE;\n\tmq->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;\n\tmq->tag_set.nr_hw_queues = 1;\n\tmq->tag_set.cmd_size = sizeof(struct mmc_queue_req);\n\tmq->tag_set.driver_data = mq;\n\n\t \n\tif (mmc_merge_capable(host) &&\n\t    host->max_segs < MMC_DMA_MAP_MERGE_SEGMENTS &&\n\t    dma_get_merge_boundary(mmc_dev(host)))\n\t\thost->can_dma_map_merge = 1;\n\telse\n\t\thost->can_dma_map_merge = 0;\n\n\tret = blk_mq_alloc_tag_set(&mq->tag_set);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\t\t\n\n\tdisk = blk_mq_alloc_disk(&mq->tag_set, mq);\n\tif (IS_ERR(disk)) {\n\t\tblk_mq_free_tag_set(&mq->tag_set);\n\t\treturn disk;\n\t}\n\tmq->queue = disk->queue;\n\n\tif (mmc_host_is_spi(host) && host->use_spi_crc)\n\t\tblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, mq->queue);\n\tblk_queue_rq_timeout(mq->queue, 60 * HZ);\n\n\tmmc_setup_queue(mq, card);\n\treturn disk;\n}\n\nvoid mmc_queue_suspend(struct mmc_queue *mq)\n{\n\tblk_mq_quiesce_queue(mq->queue);\n\n\t \n\tmmc_claim_host(mq->card->host);\n\tmmc_release_host(mq->card->host);\n}\n\nvoid mmc_queue_resume(struct mmc_queue *mq)\n{\n\tblk_mq_unquiesce_queue(mq->queue);\n}\n\nvoid mmc_cleanup_queue(struct mmc_queue *mq)\n{\n\tstruct request_queue *q = mq->queue;\n\n\t \n\tif (blk_queue_quiesced(q))\n\t\tblk_mq_unquiesce_queue(q);\n\n\t \n\tcancel_work_sync(&mq->recovery_work);\n\n\tblk_mq_free_tag_set(&mq->tag_set);\n\n\t \n\tflush_work(&mq->complete_work);\n\n\tmq->card = NULL;\n}\n\n \nunsigned int mmc_queue_map_sg(struct mmc_queue *mq, struct mmc_queue_req *mqrq)\n{\n\tstruct request *req = mmc_queue_req_to_req(mqrq);\n\n\treturn blk_rq_map_sg(mq->queue, req, mqrq->sg);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}