{
  "module_name": "pcie-xilinx-nwl.c",
  "hash_id": "d4248440a136f65967202498614c8577eff6259c3e7adbef50e36015fcda8ab4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/pci/controller/pcie-xilinx-nwl.c",
  "human_readable_source": "\n \n\n#include <linux/clk.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/irqdomain.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/msi.h>\n#include <linux/of_address.h>\n#include <linux/of_pci.h>\n#include <linux/of_platform.h>\n#include <linux/pci.h>\n#include <linux/pci-ecam.h>\n#include <linux/platform_device.h>\n#include <linux/irqchip/chained_irq.h>\n\n#include \"../pci.h\"\n\n \n#define BRCFG_PCIE_RX0\t\t\t0x00000000\n#define BRCFG_PCIE_RX1\t\t\t0x00000004\n#define BRCFG_INTERRUPT\t\t\t0x00000010\n#define BRCFG_PCIE_RX_MSG_FILTER\t0x00000020\n\n \n#define E_BREG_CAPABILITIES\t\t0x00000200\n#define E_BREG_CONTROL\t\t\t0x00000208\n#define E_BREG_BASE_LO\t\t\t0x00000210\n#define E_BREG_BASE_HI\t\t\t0x00000214\n#define E_ECAM_CAPABILITIES\t\t0x00000220\n#define E_ECAM_CONTROL\t\t\t0x00000228\n#define E_ECAM_BASE_LO\t\t\t0x00000230\n#define E_ECAM_BASE_HI\t\t\t0x00000234\n\n \n#define I_MSII_CAPABILITIES\t\t0x00000300\n#define I_MSII_CONTROL\t\t\t0x00000308\n#define I_MSII_BASE_LO\t\t\t0x00000310\n#define I_MSII_BASE_HI\t\t\t0x00000314\n\n#define I_ISUB_CONTROL\t\t\t0x000003E8\n#define SET_ISUB_CONTROL\t\tBIT(0)\n \n#define MSGF_MISC_STATUS\t\t0x00000400\n#define MSGF_MISC_MASK\t\t\t0x00000404\n#define MSGF_LEG_STATUS\t\t\t0x00000420\n#define MSGF_LEG_MASK\t\t\t0x00000424\n#define MSGF_MSI_STATUS_LO\t\t0x00000440\n#define MSGF_MSI_STATUS_HI\t\t0x00000444\n#define MSGF_MSI_MASK_LO\t\t0x00000448\n#define MSGF_MSI_MASK_HI\t\t0x0000044C\n\n \n#define CFG_ENABLE_PM_MSG_FWD\t\tBIT(1)\n#define CFG_ENABLE_INT_MSG_FWD\t\tBIT(2)\n#define CFG_ENABLE_ERR_MSG_FWD\t\tBIT(3)\n#define CFG_ENABLE_MSG_FILTER_MASK\t(CFG_ENABLE_PM_MSG_FWD | \\\n\t\t\t\t\tCFG_ENABLE_INT_MSG_FWD | \\\n\t\t\t\t\tCFG_ENABLE_ERR_MSG_FWD)\n\n \n#define MSGF_MISC_SR_RXMSG_AVAIL\tBIT(0)\n#define MSGF_MISC_SR_RXMSG_OVER\t\tBIT(1)\n#define MSGF_MISC_SR_SLAVE_ERR\t\tBIT(4)\n#define MSGF_MISC_SR_MASTER_ERR\t\tBIT(5)\n#define MSGF_MISC_SR_I_ADDR_ERR\t\tBIT(6)\n#define MSGF_MISC_SR_E_ADDR_ERR\t\tBIT(7)\n#define MSGF_MISC_SR_FATAL_AER\t\tBIT(16)\n#define MSGF_MISC_SR_NON_FATAL_AER\tBIT(17)\n#define MSGF_MISC_SR_CORR_AER\t\tBIT(18)\n#define MSGF_MISC_SR_UR_DETECT\t\tBIT(20)\n#define MSGF_MISC_SR_NON_FATAL_DEV\tBIT(22)\n#define MSGF_MISC_SR_FATAL_DEV\t\tBIT(23)\n#define MSGF_MISC_SR_LINK_DOWN\t\tBIT(24)\n#define MSGF_MSIC_SR_LINK_AUTO_BWIDTH\tBIT(25)\n#define MSGF_MSIC_SR_LINK_BWIDTH\tBIT(26)\n\n#define MSGF_MISC_SR_MASKALL\t\t(MSGF_MISC_SR_RXMSG_AVAIL | \\\n\t\t\t\t\tMSGF_MISC_SR_RXMSG_OVER | \\\n\t\t\t\t\tMSGF_MISC_SR_SLAVE_ERR | \\\n\t\t\t\t\tMSGF_MISC_SR_MASTER_ERR | \\\n\t\t\t\t\tMSGF_MISC_SR_I_ADDR_ERR | \\\n\t\t\t\t\tMSGF_MISC_SR_E_ADDR_ERR | \\\n\t\t\t\t\tMSGF_MISC_SR_FATAL_AER | \\\n\t\t\t\t\tMSGF_MISC_SR_NON_FATAL_AER | \\\n\t\t\t\t\tMSGF_MISC_SR_CORR_AER | \\\n\t\t\t\t\tMSGF_MISC_SR_UR_DETECT | \\\n\t\t\t\t\tMSGF_MISC_SR_NON_FATAL_DEV | \\\n\t\t\t\t\tMSGF_MISC_SR_FATAL_DEV | \\\n\t\t\t\t\tMSGF_MISC_SR_LINK_DOWN | \\\n\t\t\t\t\tMSGF_MSIC_SR_LINK_AUTO_BWIDTH | \\\n\t\t\t\t\tMSGF_MSIC_SR_LINK_BWIDTH)\n\n \n#define MSGF_LEG_SR_INTA\t\tBIT(0)\n#define MSGF_LEG_SR_INTB\t\tBIT(1)\n#define MSGF_LEG_SR_INTC\t\tBIT(2)\n#define MSGF_LEG_SR_INTD\t\tBIT(3)\n#define MSGF_LEG_SR_MASKALL\t\t(MSGF_LEG_SR_INTA | MSGF_LEG_SR_INTB | \\\n\t\t\t\t\tMSGF_LEG_SR_INTC | MSGF_LEG_SR_INTD)\n\n \n#define MSGF_MSI_SR_LO_MASK\t\tGENMASK(31, 0)\n#define MSGF_MSI_SR_HI_MASK\t\tGENMASK(31, 0)\n\n#define MSII_PRESENT\t\t\tBIT(0)\n#define MSII_ENABLE\t\t\tBIT(0)\n#define MSII_STATUS_ENABLE\t\tBIT(15)\n\n \n#define BRCFG_INTERRUPT_MASK\t\tBIT(0)\n#define BREG_PRESENT\t\t\tBIT(0)\n#define BREG_ENABLE\t\t\tBIT(0)\n#define BREG_ENABLE_FORCE\t\tBIT(1)\n\n \n#define E_ECAM_PRESENT\t\t\tBIT(0)\n#define E_ECAM_CR_ENABLE\t\tBIT(0)\n#define E_ECAM_SIZE_LOC\t\t\tGENMASK(20, 16)\n#define E_ECAM_SIZE_SHIFT\t\t16\n#define NWL_ECAM_VALUE_DEFAULT\t\t12\n\n#define CFG_DMA_REG_BAR\t\t\tGENMASK(2, 0)\n#define CFG_PCIE_CACHE\t\t\tGENMASK(7, 0)\n\n#define INT_PCI_MSI_NR\t\t\t(2 * 32)\n\n \n#define PS_LINKUP_OFFSET\t\t0x00000238\n#define PCIE_PHY_LINKUP_BIT\t\tBIT(0)\n#define PHY_RDY_LINKUP_BIT\t\tBIT(1)\n\n \n#define LINK_WAIT_MAX_RETRIES          10\n#define LINK_WAIT_USLEEP_MIN           90000\n#define LINK_WAIT_USLEEP_MAX           100000\n\nstruct nwl_msi {\t\t\t \n\tstruct irq_domain *msi_domain;\n\tDECLARE_BITMAP(bitmap, INT_PCI_MSI_NR);\n\tstruct irq_domain *dev_domain;\n\tstruct mutex lock;\t\t \n\tint irq_msi0;\n\tint irq_msi1;\n};\n\nstruct nwl_pcie {\n\tstruct device *dev;\n\tvoid __iomem *breg_base;\n\tvoid __iomem *pcireg_base;\n\tvoid __iomem *ecam_base;\n\tphys_addr_t phys_breg_base;\t \n\tphys_addr_t phys_pcie_reg_base;\t \n\tphys_addr_t phys_ecam_base;\t \n\tu32 breg_size;\n\tu32 pcie_reg_size;\n\tu32 ecam_size;\n\tint irq_intx;\n\tint irq_misc;\n\tu32 ecam_value;\n\tu8 last_busno;\n\tstruct nwl_msi msi;\n\tstruct irq_domain *legacy_irq_domain;\n\tstruct clk *clk;\n\traw_spinlock_t leg_mask_lock;\n};\n\nstatic inline u32 nwl_bridge_readl(struct nwl_pcie *pcie, u32 off)\n{\n\treturn readl(pcie->breg_base + off);\n}\n\nstatic inline void nwl_bridge_writel(struct nwl_pcie *pcie, u32 val, u32 off)\n{\n\twritel(val, pcie->breg_base + off);\n}\n\nstatic bool nwl_pcie_link_up(struct nwl_pcie *pcie)\n{\n\tif (readl(pcie->pcireg_base + PS_LINKUP_OFFSET) & PCIE_PHY_LINKUP_BIT)\n\t\treturn true;\n\treturn false;\n}\n\nstatic bool nwl_phy_link_up(struct nwl_pcie *pcie)\n{\n\tif (readl(pcie->pcireg_base + PS_LINKUP_OFFSET) & PHY_RDY_LINKUP_BIT)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int nwl_wait_for_link(struct nwl_pcie *pcie)\n{\n\tstruct device *dev = pcie->dev;\n\tint retries;\n\n\t \n\tfor (retries = 0; retries < LINK_WAIT_MAX_RETRIES; retries++) {\n\t\tif (nwl_phy_link_up(pcie))\n\t\t\treturn 0;\n\t\tusleep_range(LINK_WAIT_USLEEP_MIN, LINK_WAIT_USLEEP_MAX);\n\t}\n\n\tdev_err(dev, \"PHY link never came up\\n\");\n\treturn -ETIMEDOUT;\n}\n\nstatic bool nwl_pcie_valid_device(struct pci_bus *bus, unsigned int devfn)\n{\n\tstruct nwl_pcie *pcie = bus->sysdata;\n\n\t \n\tif (!pci_is_root_bus(bus)) {\n\t\tif (!nwl_pcie_link_up(pcie))\n\t\t\treturn false;\n\t} else if (devfn > 0)\n\t\t \n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic void __iomem *nwl_pcie_map_bus(struct pci_bus *bus, unsigned int devfn,\n\t\t\t\t      int where)\n{\n\tstruct nwl_pcie *pcie = bus->sysdata;\n\n\tif (!nwl_pcie_valid_device(bus, devfn))\n\t\treturn NULL;\n\n\treturn pcie->ecam_base + PCIE_ECAM_OFFSET(bus->number, devfn, where);\n}\n\n \nstatic struct pci_ops nwl_pcie_ops = {\n\t.map_bus = nwl_pcie_map_bus,\n\t.read  = pci_generic_config_read,\n\t.write = pci_generic_config_write,\n};\n\nstatic irqreturn_t nwl_pcie_misc_handler(int irq, void *data)\n{\n\tstruct nwl_pcie *pcie = data;\n\tstruct device *dev = pcie->dev;\n\tu32 misc_stat;\n\n\t \n\tmisc_stat = nwl_bridge_readl(pcie, MSGF_MISC_STATUS) &\n\t\t\t\t     MSGF_MISC_SR_MASKALL;\n\tif (!misc_stat)\n\t\treturn IRQ_NONE;\n\n\tif (misc_stat & MSGF_MISC_SR_RXMSG_OVER)\n\t\tdev_err(dev, \"Received Message FIFO Overflow\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_SLAVE_ERR)\n\t\tdev_err(dev, \"Slave error\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_MASTER_ERR)\n\t\tdev_err(dev, \"Master error\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_I_ADDR_ERR)\n\t\tdev_err(dev, \"In Misc Ingress address translation error\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_E_ADDR_ERR)\n\t\tdev_err(dev, \"In Misc Egress address translation error\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_FATAL_AER)\n\t\tdev_err(dev, \"Fatal Error in AER Capability\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_NON_FATAL_AER)\n\t\tdev_err(dev, \"Non-Fatal Error in AER Capability\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_CORR_AER)\n\t\tdev_err(dev, \"Correctable Error in AER Capability\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_UR_DETECT)\n\t\tdev_err(dev, \"Unsupported request Detected\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_NON_FATAL_DEV)\n\t\tdev_err(dev, \"Non-Fatal Error Detected\\n\");\n\n\tif (misc_stat & MSGF_MISC_SR_FATAL_DEV)\n\t\tdev_err(dev, \"Fatal Error Detected\\n\");\n\n\tif (misc_stat & MSGF_MSIC_SR_LINK_AUTO_BWIDTH)\n\t\tdev_info(dev, \"Link Autonomous Bandwidth Management Status bit set\\n\");\n\n\tif (misc_stat & MSGF_MSIC_SR_LINK_BWIDTH)\n\t\tdev_info(dev, \"Link Bandwidth Management Status bit set\\n\");\n\n\t \n\tnwl_bridge_writel(pcie, misc_stat, MSGF_MISC_STATUS);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void nwl_pcie_leg_handler(struct irq_desc *desc)\n{\n\tstruct irq_chip *chip = irq_desc_get_chip(desc);\n\tstruct nwl_pcie *pcie;\n\tunsigned long status;\n\tu32 bit;\n\n\tchained_irq_enter(chip, desc);\n\tpcie = irq_desc_get_handler_data(desc);\n\n\twhile ((status = nwl_bridge_readl(pcie, MSGF_LEG_STATUS) &\n\t\t\t\tMSGF_LEG_SR_MASKALL) != 0) {\n\t\tfor_each_set_bit(bit, &status, PCI_NUM_INTX)\n\t\t\tgeneric_handle_domain_irq(pcie->legacy_irq_domain, bit);\n\t}\n\n\tchained_irq_exit(chip, desc);\n}\n\nstatic void nwl_pcie_handle_msi_irq(struct nwl_pcie *pcie, u32 status_reg)\n{\n\tstruct nwl_msi *msi = &pcie->msi;\n\tunsigned long status;\n\tu32 bit;\n\n\twhile ((status = nwl_bridge_readl(pcie, status_reg)) != 0) {\n\t\tfor_each_set_bit(bit, &status, 32) {\n\t\t\tnwl_bridge_writel(pcie, 1 << bit, status_reg);\n\t\t\tgeneric_handle_domain_irq(msi->dev_domain, bit);\n\t\t}\n\t}\n}\n\nstatic void nwl_pcie_msi_handler_high(struct irq_desc *desc)\n{\n\tstruct irq_chip *chip = irq_desc_get_chip(desc);\n\tstruct nwl_pcie *pcie = irq_desc_get_handler_data(desc);\n\n\tchained_irq_enter(chip, desc);\n\tnwl_pcie_handle_msi_irq(pcie, MSGF_MSI_STATUS_HI);\n\tchained_irq_exit(chip, desc);\n}\n\nstatic void nwl_pcie_msi_handler_low(struct irq_desc *desc)\n{\n\tstruct irq_chip *chip = irq_desc_get_chip(desc);\n\tstruct nwl_pcie *pcie = irq_desc_get_handler_data(desc);\n\n\tchained_irq_enter(chip, desc);\n\tnwl_pcie_handle_msi_irq(pcie, MSGF_MSI_STATUS_LO);\n\tchained_irq_exit(chip, desc);\n}\n\nstatic void nwl_mask_leg_irq(struct irq_data *data)\n{\n\tstruct nwl_pcie *pcie = irq_data_get_irq_chip_data(data);\n\tunsigned long flags;\n\tu32 mask;\n\tu32 val;\n\n\tmask = 1 << (data->hwirq - 1);\n\traw_spin_lock_irqsave(&pcie->leg_mask_lock, flags);\n\tval = nwl_bridge_readl(pcie, MSGF_LEG_MASK);\n\tnwl_bridge_writel(pcie, (val & (~mask)), MSGF_LEG_MASK);\n\traw_spin_unlock_irqrestore(&pcie->leg_mask_lock, flags);\n}\n\nstatic void nwl_unmask_leg_irq(struct irq_data *data)\n{\n\tstruct nwl_pcie *pcie = irq_data_get_irq_chip_data(data);\n\tunsigned long flags;\n\tu32 mask;\n\tu32 val;\n\n\tmask = 1 << (data->hwirq - 1);\n\traw_spin_lock_irqsave(&pcie->leg_mask_lock, flags);\n\tval = nwl_bridge_readl(pcie, MSGF_LEG_MASK);\n\tnwl_bridge_writel(pcie, (val | mask), MSGF_LEG_MASK);\n\traw_spin_unlock_irqrestore(&pcie->leg_mask_lock, flags);\n}\n\nstatic struct irq_chip nwl_leg_irq_chip = {\n\t.name = \"nwl_pcie:legacy\",\n\t.irq_enable = nwl_unmask_leg_irq,\n\t.irq_disable = nwl_mask_leg_irq,\n\t.irq_mask = nwl_mask_leg_irq,\n\t.irq_unmask = nwl_unmask_leg_irq,\n};\n\nstatic int nwl_legacy_map(struct irq_domain *domain, unsigned int irq,\n\t\t\t  irq_hw_number_t hwirq)\n{\n\tirq_set_chip_and_handler(irq, &nwl_leg_irq_chip, handle_level_irq);\n\tirq_set_chip_data(irq, domain->host_data);\n\tirq_set_status_flags(irq, IRQ_LEVEL);\n\n\treturn 0;\n}\n\nstatic const struct irq_domain_ops legacy_domain_ops = {\n\t.map = nwl_legacy_map,\n\t.xlate = pci_irqd_intx_xlate,\n};\n\n#ifdef CONFIG_PCI_MSI\nstatic struct irq_chip nwl_msi_irq_chip = {\n\t.name = \"nwl_pcie:msi\",\n\t.irq_enable = pci_msi_unmask_irq,\n\t.irq_disable = pci_msi_mask_irq,\n\t.irq_mask = pci_msi_mask_irq,\n\t.irq_unmask = pci_msi_unmask_irq,\n};\n\nstatic struct msi_domain_info nwl_msi_domain_info = {\n\t.flags = (MSI_FLAG_USE_DEF_DOM_OPS | MSI_FLAG_USE_DEF_CHIP_OPS |\n\t\t  MSI_FLAG_MULTI_PCI_MSI),\n\t.chip = &nwl_msi_irq_chip,\n};\n#endif\n\nstatic void nwl_compose_msi_msg(struct irq_data *data, struct msi_msg *msg)\n{\n\tstruct nwl_pcie *pcie = irq_data_get_irq_chip_data(data);\n\tphys_addr_t msi_addr = pcie->phys_pcie_reg_base;\n\n\tmsg->address_lo = lower_32_bits(msi_addr);\n\tmsg->address_hi = upper_32_bits(msi_addr);\n\tmsg->data = data->hwirq;\n}\n\nstatic int nwl_msi_set_affinity(struct irq_data *irq_data,\n\t\t\t\tconst struct cpumask *mask, bool force)\n{\n\treturn -EINVAL;\n}\n\nstatic struct irq_chip nwl_irq_chip = {\n\t.name = \"Xilinx MSI\",\n\t.irq_compose_msi_msg = nwl_compose_msi_msg,\n\t.irq_set_affinity = nwl_msi_set_affinity,\n};\n\nstatic int nwl_irq_domain_alloc(struct irq_domain *domain, unsigned int virq,\n\t\t\t\tunsigned int nr_irqs, void *args)\n{\n\tstruct nwl_pcie *pcie = domain->host_data;\n\tstruct nwl_msi *msi = &pcie->msi;\n\tint bit;\n\tint i;\n\n\tmutex_lock(&msi->lock);\n\tbit = bitmap_find_free_region(msi->bitmap, INT_PCI_MSI_NR,\n\t\t\t\t      get_count_order(nr_irqs));\n\tif (bit < 0) {\n\t\tmutex_unlock(&msi->lock);\n\t\treturn -ENOSPC;\n\t}\n\n\tfor (i = 0; i < nr_irqs; i++) {\n\t\tirq_domain_set_info(domain, virq + i, bit + i, &nwl_irq_chip,\n\t\t\t\t    domain->host_data, handle_simple_irq,\n\t\t\t\t    NULL, NULL);\n\t}\n\tmutex_unlock(&msi->lock);\n\treturn 0;\n}\n\nstatic void nwl_irq_domain_free(struct irq_domain *domain, unsigned int virq,\n\t\t\t\tunsigned int nr_irqs)\n{\n\tstruct irq_data *data = irq_domain_get_irq_data(domain, virq);\n\tstruct nwl_pcie *pcie = irq_data_get_irq_chip_data(data);\n\tstruct nwl_msi *msi = &pcie->msi;\n\n\tmutex_lock(&msi->lock);\n\tbitmap_release_region(msi->bitmap, data->hwirq,\n\t\t\t      get_count_order(nr_irqs));\n\tmutex_unlock(&msi->lock);\n}\n\nstatic const struct irq_domain_ops dev_msi_domain_ops = {\n\t.alloc  = nwl_irq_domain_alloc,\n\t.free   = nwl_irq_domain_free,\n};\n\nstatic int nwl_pcie_init_msi_irq_domain(struct nwl_pcie *pcie)\n{\n#ifdef CONFIG_PCI_MSI\n\tstruct device *dev = pcie->dev;\n\tstruct fwnode_handle *fwnode = of_node_to_fwnode(dev->of_node);\n\tstruct nwl_msi *msi = &pcie->msi;\n\n\tmsi->dev_domain = irq_domain_add_linear(NULL, INT_PCI_MSI_NR,\n\t\t\t\t\t\t&dev_msi_domain_ops, pcie);\n\tif (!msi->dev_domain) {\n\t\tdev_err(dev, \"failed to create dev IRQ domain\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tmsi->msi_domain = pci_msi_create_irq_domain(fwnode,\n\t\t\t\t\t\t    &nwl_msi_domain_info,\n\t\t\t\t\t\t    msi->dev_domain);\n\tif (!msi->msi_domain) {\n\t\tdev_err(dev, \"failed to create msi IRQ domain\\n\");\n\t\tirq_domain_remove(msi->dev_domain);\n\t\treturn -ENOMEM;\n\t}\n#endif\n\treturn 0;\n}\n\nstatic int nwl_pcie_init_irq_domain(struct nwl_pcie *pcie)\n{\n\tstruct device *dev = pcie->dev;\n\tstruct device_node *node = dev->of_node;\n\tstruct device_node *legacy_intc_node;\n\n\tlegacy_intc_node = of_get_next_child(node, NULL);\n\tif (!legacy_intc_node) {\n\t\tdev_err(dev, \"No legacy intc node found\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tpcie->legacy_irq_domain = irq_domain_add_linear(legacy_intc_node,\n\t\t\t\t\t\t\tPCI_NUM_INTX,\n\t\t\t\t\t\t\t&legacy_domain_ops,\n\t\t\t\t\t\t\tpcie);\n\tof_node_put(legacy_intc_node);\n\tif (!pcie->legacy_irq_domain) {\n\t\tdev_err(dev, \"failed to create IRQ domain\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\traw_spin_lock_init(&pcie->leg_mask_lock);\n\tnwl_pcie_init_msi_irq_domain(pcie);\n\treturn 0;\n}\n\nstatic int nwl_pcie_enable_msi(struct nwl_pcie *pcie)\n{\n\tstruct device *dev = pcie->dev;\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tstruct nwl_msi *msi = &pcie->msi;\n\tunsigned long base;\n\tint ret;\n\n\tmutex_init(&msi->lock);\n\n\t \n\tmsi->irq_msi1 = platform_get_irq_byname(pdev, \"msi1\");\n\tif (msi->irq_msi1 < 0)\n\t\treturn -EINVAL;\n\n\tirq_set_chained_handler_and_data(msi->irq_msi1,\n\t\t\t\t\t nwl_pcie_msi_handler_high, pcie);\n\n\t \n\tmsi->irq_msi0 = platform_get_irq_byname(pdev, \"msi0\");\n\tif (msi->irq_msi0 < 0)\n\t\treturn -EINVAL;\n\n\tirq_set_chained_handler_and_data(msi->irq_msi0,\n\t\t\t\t\t nwl_pcie_msi_handler_low, pcie);\n\n\t \n\tret = nwl_bridge_readl(pcie, I_MSII_CAPABILITIES) & MSII_PRESENT;\n\tif (!ret) {\n\t\tdev_err(dev, \"MSI not present\\n\");\n\t\treturn -EIO;\n\t}\n\n\t \n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, I_MSII_CONTROL) |\n\t\t\t  MSII_ENABLE, I_MSII_CONTROL);\n\n\t \n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, I_MSII_CONTROL) |\n\t\t\t  MSII_STATUS_ENABLE, I_MSII_CONTROL);\n\n\t \n\tbase = pcie->phys_pcie_reg_base;\n\tnwl_bridge_writel(pcie, lower_32_bits(base), I_MSII_BASE_LO);\n\tnwl_bridge_writel(pcie, upper_32_bits(base), I_MSII_BASE_HI);\n\n\t \n\tnwl_bridge_writel(pcie, 0, MSGF_MSI_MASK_HI);\n\n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie,  MSGF_MSI_STATUS_HI) &\n\t\t\t  MSGF_MSI_SR_HI_MASK, MSGF_MSI_STATUS_HI);\n\n\tnwl_bridge_writel(pcie, MSGF_MSI_SR_HI_MASK, MSGF_MSI_MASK_HI);\n\n\t \n\tnwl_bridge_writel(pcie, 0, MSGF_MSI_MASK_LO);\n\n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, MSGF_MSI_STATUS_LO) &\n\t\t\t  MSGF_MSI_SR_LO_MASK, MSGF_MSI_STATUS_LO);\n\n\tnwl_bridge_writel(pcie, MSGF_MSI_SR_LO_MASK, MSGF_MSI_MASK_LO);\n\n\treturn 0;\n}\n\nstatic int nwl_pcie_bridge_init(struct nwl_pcie *pcie)\n{\n\tstruct device *dev = pcie->dev;\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tu32 breg_val, ecam_val, first_busno = 0;\n\tint err;\n\n\tbreg_val = nwl_bridge_readl(pcie, E_BREG_CAPABILITIES) & BREG_PRESENT;\n\tif (!breg_val) {\n\t\tdev_err(dev, \"BREG is not present\\n\");\n\t\treturn breg_val;\n\t}\n\n\t \n\tnwl_bridge_writel(pcie, lower_32_bits(pcie->phys_breg_base),\n\t\t\t  E_BREG_BASE_LO);\n\tnwl_bridge_writel(pcie, upper_32_bits(pcie->phys_breg_base),\n\t\t\t  E_BREG_BASE_HI);\n\n\t \n\tnwl_bridge_writel(pcie, ~BREG_ENABLE_FORCE & BREG_ENABLE,\n\t\t\t  E_BREG_CONTROL);\n\n\t \n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, BRCFG_PCIE_RX0) |\n\t\t\t  CFG_DMA_REG_BAR, BRCFG_PCIE_RX0);\n\n\t \n\tnwl_bridge_writel(pcie, SET_ISUB_CONTROL, I_ISUB_CONTROL);\n\n\t \n\tnwl_bridge_writel(pcie, CFG_ENABLE_MSG_FILTER_MASK,\n\t\t\t  BRCFG_PCIE_RX_MSG_FILTER);\n\n\t \n\tif (of_dma_is_coherent(dev->of_node))\n\t\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, BRCFG_PCIE_RX1) |\n\t\t\t\t  CFG_PCIE_CACHE, BRCFG_PCIE_RX1);\n\n\terr = nwl_wait_for_link(pcie);\n\tif (err)\n\t\treturn err;\n\n\tecam_val = nwl_bridge_readl(pcie, E_ECAM_CAPABILITIES) & E_ECAM_PRESENT;\n\tif (!ecam_val) {\n\t\tdev_err(dev, \"ECAM is not present\\n\");\n\t\treturn ecam_val;\n\t}\n\n\t \n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, E_ECAM_CONTROL) |\n\t\t\t  E_ECAM_CR_ENABLE, E_ECAM_CONTROL);\n\n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, E_ECAM_CONTROL) |\n\t\t\t  (pcie->ecam_value << E_ECAM_SIZE_SHIFT),\n\t\t\t  E_ECAM_CONTROL);\n\n\tnwl_bridge_writel(pcie, lower_32_bits(pcie->phys_ecam_base),\n\t\t\t  E_ECAM_BASE_LO);\n\tnwl_bridge_writel(pcie, upper_32_bits(pcie->phys_ecam_base),\n\t\t\t  E_ECAM_BASE_HI);\n\n\t \n\tecam_val = nwl_bridge_readl(pcie, E_ECAM_CONTROL);\n\tpcie->last_busno = (ecam_val & E_ECAM_SIZE_LOC) >> E_ECAM_SIZE_SHIFT;\n\t \n\tecam_val = first_busno;\n\tecam_val |= (first_busno + 1) << 8;\n\tecam_val |= (pcie->last_busno << E_ECAM_SIZE_SHIFT);\n\twritel(ecam_val, (pcie->ecam_base + PCI_PRIMARY_BUS));\n\n\tif (nwl_pcie_link_up(pcie))\n\t\tdev_info(dev, \"Link is UP\\n\");\n\telse\n\t\tdev_info(dev, \"Link is DOWN\\n\");\n\n\t \n\tpcie->irq_misc = platform_get_irq_byname(pdev, \"misc\");\n\tif (pcie->irq_misc < 0)\n\t\treturn -EINVAL;\n\n\terr = devm_request_irq(dev, pcie->irq_misc,\n\t\t\t       nwl_pcie_misc_handler, IRQF_SHARED,\n\t\t\t       \"nwl_pcie:misc\", pcie);\n\tif (err) {\n\t\tdev_err(dev, \"fail to register misc IRQ#%d\\n\",\n\t\t\tpcie->irq_misc);\n\t\treturn err;\n\t}\n\n\t \n\tnwl_bridge_writel(pcie, (u32)~MSGF_MISC_SR_MASKALL, MSGF_MISC_MASK);\n\n\t \n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, MSGF_MISC_STATUS) &\n\t\t\t  MSGF_MISC_SR_MASKALL, MSGF_MISC_STATUS);\n\n\t \n\tnwl_bridge_writel(pcie, MSGF_MISC_SR_MASKALL, MSGF_MISC_MASK);\n\n\t \n\tnwl_bridge_writel(pcie, (u32)~MSGF_LEG_SR_MASKALL, MSGF_LEG_MASK);\n\n\t \n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, MSGF_LEG_STATUS) &\n\t\t\t  MSGF_LEG_SR_MASKALL, MSGF_LEG_STATUS);\n\n\t \n\tnwl_bridge_writel(pcie, MSGF_LEG_SR_MASKALL, MSGF_LEG_MASK);\n\n\t \n\tnwl_bridge_writel(pcie, nwl_bridge_readl(pcie, BRCFG_INTERRUPT) |\n\t\t\t  BRCFG_INTERRUPT_MASK, BRCFG_INTERRUPT);\n\n\treturn 0;\n}\n\nstatic int nwl_pcie_parse_dt(struct nwl_pcie *pcie,\n\t\t\t     struct platform_device *pdev)\n{\n\tstruct device *dev = pcie->dev;\n\tstruct resource *res;\n\n\tres = platform_get_resource_byname(pdev, IORESOURCE_MEM, \"breg\");\n\tpcie->breg_base = devm_ioremap_resource(dev, res);\n\tif (IS_ERR(pcie->breg_base))\n\t\treturn PTR_ERR(pcie->breg_base);\n\tpcie->phys_breg_base = res->start;\n\n\tres = platform_get_resource_byname(pdev, IORESOURCE_MEM, \"pcireg\");\n\tpcie->pcireg_base = devm_ioremap_resource(dev, res);\n\tif (IS_ERR(pcie->pcireg_base))\n\t\treturn PTR_ERR(pcie->pcireg_base);\n\tpcie->phys_pcie_reg_base = res->start;\n\n\tres = platform_get_resource_byname(pdev, IORESOURCE_MEM, \"cfg\");\n\tpcie->ecam_base = devm_pci_remap_cfg_resource(dev, res);\n\tif (IS_ERR(pcie->ecam_base))\n\t\treturn PTR_ERR(pcie->ecam_base);\n\tpcie->phys_ecam_base = res->start;\n\n\t \n\tpcie->irq_intx = platform_get_irq_byname(pdev, \"intx\");\n\tif (pcie->irq_intx < 0)\n\t\treturn pcie->irq_intx;\n\n\tirq_set_chained_handler_and_data(pcie->irq_intx,\n\t\t\t\t\t nwl_pcie_leg_handler, pcie);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id nwl_pcie_of_match[] = {\n\t{ .compatible = \"xlnx,nwl-pcie-2.11\", },\n\t{}\n};\n\nstatic int nwl_pcie_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct nwl_pcie *pcie;\n\tstruct pci_host_bridge *bridge;\n\tint err;\n\n\tbridge = devm_pci_alloc_host_bridge(dev, sizeof(*pcie));\n\tif (!bridge)\n\t\treturn -ENODEV;\n\n\tpcie = pci_host_bridge_priv(bridge);\n\n\tpcie->dev = dev;\n\tpcie->ecam_value = NWL_ECAM_VALUE_DEFAULT;\n\n\terr = nwl_pcie_parse_dt(pcie, pdev);\n\tif (err) {\n\t\tdev_err(dev, \"Parsing DT failed\\n\");\n\t\treturn err;\n\t}\n\n\tpcie->clk = devm_clk_get(dev, NULL);\n\tif (IS_ERR(pcie->clk))\n\t\treturn PTR_ERR(pcie->clk);\n\n\terr = clk_prepare_enable(pcie->clk);\n\tif (err) {\n\t\tdev_err(dev, \"can't enable PCIe ref clock\\n\");\n\t\treturn err;\n\t}\n\n\terr = nwl_pcie_bridge_init(pcie);\n\tif (err) {\n\t\tdev_err(dev, \"HW Initialization failed\\n\");\n\t\treturn err;\n\t}\n\n\terr = nwl_pcie_init_irq_domain(pcie);\n\tif (err) {\n\t\tdev_err(dev, \"Failed creating IRQ Domain\\n\");\n\t\treturn err;\n\t}\n\n\tbridge->sysdata = pcie;\n\tbridge->ops = &nwl_pcie_ops;\n\n\tif (IS_ENABLED(CONFIG_PCI_MSI)) {\n\t\terr = nwl_pcie_enable_msi(pcie);\n\t\tif (err < 0) {\n\t\t\tdev_err(dev, \"failed to enable MSI support: %d\\n\", err);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn pci_host_probe(bridge);\n}\n\nstatic struct platform_driver nwl_pcie_driver = {\n\t.driver = {\n\t\t.name = \"nwl-pcie\",\n\t\t.suppress_bind_attrs = true,\n\t\t.of_match_table = nwl_pcie_of_match,\n\t},\n\t.probe = nwl_pcie_probe,\n};\nbuiltin_platform_driver(nwl_pcie_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}