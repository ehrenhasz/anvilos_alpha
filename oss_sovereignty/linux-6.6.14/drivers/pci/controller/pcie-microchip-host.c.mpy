{
  "module_name": "pcie-microchip-host.c",
  "hash_id": "78df223c05cf4c0c7f01fdf994da14d1afd2f26222c7c999289c2324737a00dc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/pci/controller/pcie-microchip-host.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/clk.h>\n#include <linux/irqchip/chained_irq.h>\n#include <linux/irqdomain.h>\n#include <linux/module.h>\n#include <linux/msi.h>\n#include <linux/of_address.h>\n#include <linux/of_pci.h>\n#include <linux/pci-ecam.h>\n#include <linux/platform_device.h>\n\n#include \"../pci.h\"\n\n \n#define MC_MAX_NUM_MSI_IRQS\t\t\t32\n\n \n#define MC_PCIE1_BRIDGE_ADDR\t\t\t0x00008000u\n#define MC_PCIE1_CTRL_ADDR\t\t\t0x0000a000u\n\n#define MC_PCIE_BRIDGE_ADDR\t\t\t(MC_PCIE1_BRIDGE_ADDR)\n#define MC_PCIE_CTRL_ADDR\t\t\t(MC_PCIE1_CTRL_ADDR)\n\n \n#define PCIE_PCI_IRQ_DW0\t\t\t0xa8\n#define  MSIX_CAP_MASK\t\t\t\tBIT(31)\n#define  NUM_MSI_MSGS_MASK\t\t\tGENMASK(6, 4)\n#define  NUM_MSI_MSGS_SHIFT\t\t\t4\n\n#define IMASK_LOCAL\t\t\t\t0x180\n#define  DMA_END_ENGINE_0_MASK\t\t\t0x00000000u\n#define  DMA_END_ENGINE_0_SHIFT\t\t\t0\n#define  DMA_END_ENGINE_1_MASK\t\t\t0x00000000u\n#define  DMA_END_ENGINE_1_SHIFT\t\t\t1\n#define  DMA_ERROR_ENGINE_0_MASK\t\t0x00000100u\n#define  DMA_ERROR_ENGINE_0_SHIFT\t\t8\n#define  DMA_ERROR_ENGINE_1_MASK\t\t0x00000200u\n#define  DMA_ERROR_ENGINE_1_SHIFT\t\t9\n#define  A_ATR_EVT_POST_ERR_MASK\t\t0x00010000u\n#define  A_ATR_EVT_POST_ERR_SHIFT\t\t16\n#define  A_ATR_EVT_FETCH_ERR_MASK\t\t0x00020000u\n#define  A_ATR_EVT_FETCH_ERR_SHIFT\t\t17\n#define  A_ATR_EVT_DISCARD_ERR_MASK\t\t0x00040000u\n#define  A_ATR_EVT_DISCARD_ERR_SHIFT\t\t18\n#define  A_ATR_EVT_DOORBELL_MASK\t\t0x00000000u\n#define  A_ATR_EVT_DOORBELL_SHIFT\t\t19\n#define  P_ATR_EVT_POST_ERR_MASK\t\t0x00100000u\n#define  P_ATR_EVT_POST_ERR_SHIFT\t\t20\n#define  P_ATR_EVT_FETCH_ERR_MASK\t\t0x00200000u\n#define  P_ATR_EVT_FETCH_ERR_SHIFT\t\t21\n#define  P_ATR_EVT_DISCARD_ERR_MASK\t\t0x00400000u\n#define  P_ATR_EVT_DISCARD_ERR_SHIFT\t\t22\n#define  P_ATR_EVT_DOORBELL_MASK\t\t0x00000000u\n#define  P_ATR_EVT_DOORBELL_SHIFT\t\t23\n#define  PM_MSI_INT_INTA_MASK\t\t\t0x01000000u\n#define  PM_MSI_INT_INTA_SHIFT\t\t\t24\n#define  PM_MSI_INT_INTB_MASK\t\t\t0x02000000u\n#define  PM_MSI_INT_INTB_SHIFT\t\t\t25\n#define  PM_MSI_INT_INTC_MASK\t\t\t0x04000000u\n#define  PM_MSI_INT_INTC_SHIFT\t\t\t26\n#define  PM_MSI_INT_INTD_MASK\t\t\t0x08000000u\n#define  PM_MSI_INT_INTD_SHIFT\t\t\t27\n#define  PM_MSI_INT_INTX_MASK\t\t\t0x0f000000u\n#define  PM_MSI_INT_INTX_SHIFT\t\t\t24\n#define  PM_MSI_INT_MSI_MASK\t\t\t0x10000000u\n#define  PM_MSI_INT_MSI_SHIFT\t\t\t28\n#define  PM_MSI_INT_AER_EVT_MASK\t\t0x20000000u\n#define  PM_MSI_INT_AER_EVT_SHIFT\t\t29\n#define  PM_MSI_INT_EVENTS_MASK\t\t\t0x40000000u\n#define  PM_MSI_INT_EVENTS_SHIFT\t\t30\n#define  PM_MSI_INT_SYS_ERR_MASK\t\t0x80000000u\n#define  PM_MSI_INT_SYS_ERR_SHIFT\t\t31\n#define  NUM_LOCAL_EVENTS\t\t\t15\n#define ISTATUS_LOCAL\t\t\t\t0x184\n#define IMASK_HOST\t\t\t\t0x188\n#define ISTATUS_HOST\t\t\t\t0x18c\n#define IMSI_ADDR\t\t\t\t0x190\n#define ISTATUS_MSI\t\t\t\t0x194\n\n \n#define ATR0_PCIE_WIN0_SRCADDR_PARAM\t\t0x600u\n#define  ATR0_PCIE_ATR_SIZE\t\t\t0x25\n#define  ATR0_PCIE_ATR_SIZE_SHIFT\t\t1\n#define ATR0_PCIE_WIN0_SRC_ADDR\t\t\t0x604u\n#define ATR0_PCIE_WIN0_TRSL_ADDR_LSB\t\t0x608u\n#define ATR0_PCIE_WIN0_TRSL_ADDR_UDW\t\t0x60cu\n#define ATR0_PCIE_WIN0_TRSL_PARAM\t\t0x610u\n\n \n#define ATR0_AXI4_SLV0_SRCADDR_PARAM\t\t0x800u\n#define  ATR_SIZE_SHIFT\t\t\t\t1\n#define  ATR_IMPL_ENABLE\t\t\t1\n#define ATR0_AXI4_SLV0_SRC_ADDR\t\t\t0x804u\n#define ATR0_AXI4_SLV0_TRSL_ADDR_LSB\t\t0x808u\n#define ATR0_AXI4_SLV0_TRSL_ADDR_UDW\t\t0x80cu\n#define ATR0_AXI4_SLV0_TRSL_PARAM\t\t0x810u\n#define  PCIE_TX_RX_INTERFACE\t\t\t0x00000000u\n#define  PCIE_CONFIG_INTERFACE\t\t\t0x00000001u\n\n#define ATR_ENTRY_SIZE\t\t\t\t32\n\n \n#define SEC_ERROR_EVENT_CNT\t\t\t0x20\n#define DED_ERROR_EVENT_CNT\t\t\t0x24\n#define SEC_ERROR_INT\t\t\t\t0x28\n#define  SEC_ERROR_INT_TX_RAM_SEC_ERR_INT\tGENMASK(3, 0)\n#define  SEC_ERROR_INT_RX_RAM_SEC_ERR_INT\tGENMASK(7, 4)\n#define  SEC_ERROR_INT_PCIE2AXI_RAM_SEC_ERR_INT\tGENMASK(11, 8)\n#define  SEC_ERROR_INT_AXI2PCIE_RAM_SEC_ERR_INT\tGENMASK(15, 12)\n#define  SEC_ERROR_INT_ALL_RAM_SEC_ERR_INT\tGENMASK(15, 0)\n#define  NUM_SEC_ERROR_INTS\t\t\t(4)\n#define SEC_ERROR_INT_MASK\t\t\t0x2c\n#define DED_ERROR_INT\t\t\t\t0x30\n#define  DED_ERROR_INT_TX_RAM_DED_ERR_INT\tGENMASK(3, 0)\n#define  DED_ERROR_INT_RX_RAM_DED_ERR_INT\tGENMASK(7, 4)\n#define  DED_ERROR_INT_PCIE2AXI_RAM_DED_ERR_INT\tGENMASK(11, 8)\n#define  DED_ERROR_INT_AXI2PCIE_RAM_DED_ERR_INT\tGENMASK(15, 12)\n#define  DED_ERROR_INT_ALL_RAM_DED_ERR_INT\tGENMASK(15, 0)\n#define  NUM_DED_ERROR_INTS\t\t\t(4)\n#define DED_ERROR_INT_MASK\t\t\t0x34\n#define ECC_CONTROL\t\t\t\t0x38\n#define  ECC_CONTROL_TX_RAM_INJ_ERROR_0\t\tBIT(0)\n#define  ECC_CONTROL_TX_RAM_INJ_ERROR_1\t\tBIT(1)\n#define  ECC_CONTROL_TX_RAM_INJ_ERROR_2\t\tBIT(2)\n#define  ECC_CONTROL_TX_RAM_INJ_ERROR_3\t\tBIT(3)\n#define  ECC_CONTROL_RX_RAM_INJ_ERROR_0\t\tBIT(4)\n#define  ECC_CONTROL_RX_RAM_INJ_ERROR_1\t\tBIT(5)\n#define  ECC_CONTROL_RX_RAM_INJ_ERROR_2\t\tBIT(6)\n#define  ECC_CONTROL_RX_RAM_INJ_ERROR_3\t\tBIT(7)\n#define  ECC_CONTROL_PCIE2AXI_RAM_INJ_ERROR_0\tBIT(8)\n#define  ECC_CONTROL_PCIE2AXI_RAM_INJ_ERROR_1\tBIT(9)\n#define  ECC_CONTROL_PCIE2AXI_RAM_INJ_ERROR_2\tBIT(10)\n#define  ECC_CONTROL_PCIE2AXI_RAM_INJ_ERROR_3\tBIT(11)\n#define  ECC_CONTROL_AXI2PCIE_RAM_INJ_ERROR_0\tBIT(12)\n#define  ECC_CONTROL_AXI2PCIE_RAM_INJ_ERROR_1\tBIT(13)\n#define  ECC_CONTROL_AXI2PCIE_RAM_INJ_ERROR_2\tBIT(14)\n#define  ECC_CONTROL_AXI2PCIE_RAM_INJ_ERROR_3\tBIT(15)\n#define  ECC_CONTROL_TX_RAM_ECC_BYPASS\t\tBIT(24)\n#define  ECC_CONTROL_RX_RAM_ECC_BYPASS\t\tBIT(25)\n#define  ECC_CONTROL_PCIE2AXI_RAM_ECC_BYPASS\tBIT(26)\n#define  ECC_CONTROL_AXI2PCIE_RAM_ECC_BYPASS\tBIT(27)\n#define PCIE_EVENT_INT\t\t\t\t0x14c\n#define  PCIE_EVENT_INT_L2_EXIT_INT\t\tBIT(0)\n#define  PCIE_EVENT_INT_HOTRST_EXIT_INT\t\tBIT(1)\n#define  PCIE_EVENT_INT_DLUP_EXIT_INT\t\tBIT(2)\n#define  PCIE_EVENT_INT_MASK\t\t\tGENMASK(2, 0)\n#define  PCIE_EVENT_INT_L2_EXIT_INT_MASK\tBIT(16)\n#define  PCIE_EVENT_INT_HOTRST_EXIT_INT_MASK\tBIT(17)\n#define  PCIE_EVENT_INT_DLUP_EXIT_INT_MASK\tBIT(18)\n#define  PCIE_EVENT_INT_ENB_MASK\t\tGENMASK(18, 16)\n#define  PCIE_EVENT_INT_ENB_SHIFT\t\t16\n#define  NUM_PCIE_EVENTS\t\t\t(3)\n\n \n#define MC_MSI_CAP_CTRL_OFFSET\t\t\t0xe0u\n\n \n#define EVENT_PCIE_L2_EXIT\t\t\t0\n#define EVENT_PCIE_HOTRST_EXIT\t\t\t1\n#define EVENT_PCIE_DLUP_EXIT\t\t\t2\n#define EVENT_SEC_TX_RAM_SEC_ERR\t\t3\n#define EVENT_SEC_RX_RAM_SEC_ERR\t\t4\n#define EVENT_SEC_PCIE2AXI_RAM_SEC_ERR\t\t5\n#define EVENT_SEC_AXI2PCIE_RAM_SEC_ERR\t\t6\n#define EVENT_DED_TX_RAM_DED_ERR\t\t7\n#define EVENT_DED_RX_RAM_DED_ERR\t\t8\n#define EVENT_DED_PCIE2AXI_RAM_DED_ERR\t\t9\n#define EVENT_DED_AXI2PCIE_RAM_DED_ERR\t\t10\n#define EVENT_LOCAL_DMA_END_ENGINE_0\t\t11\n#define EVENT_LOCAL_DMA_END_ENGINE_1\t\t12\n#define EVENT_LOCAL_DMA_ERROR_ENGINE_0\t\t13\n#define EVENT_LOCAL_DMA_ERROR_ENGINE_1\t\t14\n#define EVENT_LOCAL_A_ATR_EVT_POST_ERR\t\t15\n#define EVENT_LOCAL_A_ATR_EVT_FETCH_ERR\t\t16\n#define EVENT_LOCAL_A_ATR_EVT_DISCARD_ERR\t17\n#define EVENT_LOCAL_A_ATR_EVT_DOORBELL\t\t18\n#define EVENT_LOCAL_P_ATR_EVT_POST_ERR\t\t19\n#define EVENT_LOCAL_P_ATR_EVT_FETCH_ERR\t\t20\n#define EVENT_LOCAL_P_ATR_EVT_DISCARD_ERR\t21\n#define EVENT_LOCAL_P_ATR_EVT_DOORBELL\t\t22\n#define EVENT_LOCAL_PM_MSI_INT_INTX\t\t23\n#define EVENT_LOCAL_PM_MSI_INT_MSI\t\t24\n#define EVENT_LOCAL_PM_MSI_INT_AER_EVT\t\t25\n#define EVENT_LOCAL_PM_MSI_INT_EVENTS\t\t26\n#define EVENT_LOCAL_PM_MSI_INT_SYS_ERR\t\t27\n#define NUM_EVENTS\t\t\t\t28\n\n#define PCIE_EVENT_CAUSE(x, s)\t\\\n\t[EVENT_PCIE_ ## x] = { __stringify(x), s }\n\n#define SEC_ERROR_CAUSE(x, s) \\\n\t[EVENT_SEC_ ## x] = { __stringify(x), s }\n\n#define DED_ERROR_CAUSE(x, s) \\\n\t[EVENT_DED_ ## x] = { __stringify(x), s }\n\n#define LOCAL_EVENT_CAUSE(x, s) \\\n\t[EVENT_LOCAL_ ## x] = { __stringify(x), s }\n\n#define PCIE_EVENT(x) \\\n\t.base = MC_PCIE_CTRL_ADDR, \\\n\t.offset = PCIE_EVENT_INT, \\\n\t.mask_offset = PCIE_EVENT_INT, \\\n\t.mask_high = 1, \\\n\t.mask = PCIE_EVENT_INT_ ## x ## _INT, \\\n\t.enb_mask = PCIE_EVENT_INT_ENB_MASK\n\n#define SEC_EVENT(x) \\\n\t.base = MC_PCIE_CTRL_ADDR, \\\n\t.offset = SEC_ERROR_INT, \\\n\t.mask_offset = SEC_ERROR_INT_MASK, \\\n\t.mask = SEC_ERROR_INT_ ## x ## _INT, \\\n\t.mask_high = 1, \\\n\t.enb_mask = 0\n\n#define DED_EVENT(x) \\\n\t.base = MC_PCIE_CTRL_ADDR, \\\n\t.offset = DED_ERROR_INT, \\\n\t.mask_offset = DED_ERROR_INT_MASK, \\\n\t.mask_high = 1, \\\n\t.mask = DED_ERROR_INT_ ## x ## _INT, \\\n\t.enb_mask = 0\n\n#define LOCAL_EVENT(x) \\\n\t.base = MC_PCIE_BRIDGE_ADDR, \\\n\t.offset = ISTATUS_LOCAL, \\\n\t.mask_offset = IMASK_LOCAL, \\\n\t.mask_high = 0, \\\n\t.mask = x ## _MASK, \\\n\t.enb_mask = 0\n\n#define PCIE_EVENT_TO_EVENT_MAP(x) \\\n\t{ PCIE_EVENT_INT_ ## x ## _INT, EVENT_PCIE_ ## x }\n\n#define SEC_ERROR_TO_EVENT_MAP(x) \\\n\t{ SEC_ERROR_INT_ ## x ## _INT, EVENT_SEC_ ## x }\n\n#define DED_ERROR_TO_EVENT_MAP(x) \\\n\t{ DED_ERROR_INT_ ## x ## _INT, EVENT_DED_ ## x }\n\n#define LOCAL_STATUS_TO_EVENT_MAP(x) \\\n\t{ x ## _MASK, EVENT_LOCAL_ ## x }\n\nstruct event_map {\n\tu32 reg_mask;\n\tu32 event_bit;\n};\n\nstruct mc_msi {\n\tstruct mutex lock;\t\t \n\tstruct irq_domain *msi_domain;\n\tstruct irq_domain *dev_domain;\n\tu32 num_vectors;\n\tu64 vector_phy;\n\tDECLARE_BITMAP(used, MC_MAX_NUM_MSI_IRQS);\n};\n\nstruct mc_pcie {\n\tvoid __iomem *axi_base_addr;\n\tstruct device *dev;\n\tstruct irq_domain *intx_domain;\n\tstruct irq_domain *event_domain;\n\traw_spinlock_t lock;\n\tstruct mc_msi msi;\n};\n\nstruct cause {\n\tconst char *sym;\n\tconst char *str;\n};\n\nstatic const struct cause event_cause[NUM_EVENTS] = {\n\tPCIE_EVENT_CAUSE(L2_EXIT, \"L2 exit event\"),\n\tPCIE_EVENT_CAUSE(HOTRST_EXIT, \"Hot reset exit event\"),\n\tPCIE_EVENT_CAUSE(DLUP_EXIT, \"DLUP exit event\"),\n\tSEC_ERROR_CAUSE(TX_RAM_SEC_ERR,  \"sec error in tx buffer\"),\n\tSEC_ERROR_CAUSE(RX_RAM_SEC_ERR,  \"sec error in rx buffer\"),\n\tSEC_ERROR_CAUSE(PCIE2AXI_RAM_SEC_ERR,  \"sec error in pcie2axi buffer\"),\n\tSEC_ERROR_CAUSE(AXI2PCIE_RAM_SEC_ERR,  \"sec error in axi2pcie buffer\"),\n\tDED_ERROR_CAUSE(TX_RAM_DED_ERR,  \"ded error in tx buffer\"),\n\tDED_ERROR_CAUSE(RX_RAM_DED_ERR,  \"ded error in rx buffer\"),\n\tDED_ERROR_CAUSE(PCIE2AXI_RAM_DED_ERR,  \"ded error in pcie2axi buffer\"),\n\tDED_ERROR_CAUSE(AXI2PCIE_RAM_DED_ERR,  \"ded error in axi2pcie buffer\"),\n\tLOCAL_EVENT_CAUSE(DMA_ERROR_ENGINE_0, \"dma engine 0 error\"),\n\tLOCAL_EVENT_CAUSE(DMA_ERROR_ENGINE_1, \"dma engine 1 error\"),\n\tLOCAL_EVENT_CAUSE(A_ATR_EVT_POST_ERR, \"axi write request error\"),\n\tLOCAL_EVENT_CAUSE(A_ATR_EVT_FETCH_ERR, \"axi read request error\"),\n\tLOCAL_EVENT_CAUSE(A_ATR_EVT_DISCARD_ERR, \"axi read timeout\"),\n\tLOCAL_EVENT_CAUSE(P_ATR_EVT_POST_ERR, \"pcie write request error\"),\n\tLOCAL_EVENT_CAUSE(P_ATR_EVT_FETCH_ERR, \"pcie read request error\"),\n\tLOCAL_EVENT_CAUSE(P_ATR_EVT_DISCARD_ERR, \"pcie read timeout\"),\n\tLOCAL_EVENT_CAUSE(PM_MSI_INT_AER_EVT, \"aer event\"),\n\tLOCAL_EVENT_CAUSE(PM_MSI_INT_EVENTS, \"pm/ltr/hotplug event\"),\n\tLOCAL_EVENT_CAUSE(PM_MSI_INT_SYS_ERR, \"system error\"),\n};\n\nstatic struct event_map pcie_event_to_event[] = {\n\tPCIE_EVENT_TO_EVENT_MAP(L2_EXIT),\n\tPCIE_EVENT_TO_EVENT_MAP(HOTRST_EXIT),\n\tPCIE_EVENT_TO_EVENT_MAP(DLUP_EXIT),\n};\n\nstatic struct event_map sec_error_to_event[] = {\n\tSEC_ERROR_TO_EVENT_MAP(TX_RAM_SEC_ERR),\n\tSEC_ERROR_TO_EVENT_MAP(RX_RAM_SEC_ERR),\n\tSEC_ERROR_TO_EVENT_MAP(PCIE2AXI_RAM_SEC_ERR),\n\tSEC_ERROR_TO_EVENT_MAP(AXI2PCIE_RAM_SEC_ERR),\n};\n\nstatic struct event_map ded_error_to_event[] = {\n\tDED_ERROR_TO_EVENT_MAP(TX_RAM_DED_ERR),\n\tDED_ERROR_TO_EVENT_MAP(RX_RAM_DED_ERR),\n\tDED_ERROR_TO_EVENT_MAP(PCIE2AXI_RAM_DED_ERR),\n\tDED_ERROR_TO_EVENT_MAP(AXI2PCIE_RAM_DED_ERR),\n};\n\nstatic struct event_map local_status_to_event[] = {\n\tLOCAL_STATUS_TO_EVENT_MAP(DMA_END_ENGINE_0),\n\tLOCAL_STATUS_TO_EVENT_MAP(DMA_END_ENGINE_1),\n\tLOCAL_STATUS_TO_EVENT_MAP(DMA_ERROR_ENGINE_0),\n\tLOCAL_STATUS_TO_EVENT_MAP(DMA_ERROR_ENGINE_1),\n\tLOCAL_STATUS_TO_EVENT_MAP(A_ATR_EVT_POST_ERR),\n\tLOCAL_STATUS_TO_EVENT_MAP(A_ATR_EVT_FETCH_ERR),\n\tLOCAL_STATUS_TO_EVENT_MAP(A_ATR_EVT_DISCARD_ERR),\n\tLOCAL_STATUS_TO_EVENT_MAP(A_ATR_EVT_DOORBELL),\n\tLOCAL_STATUS_TO_EVENT_MAP(P_ATR_EVT_POST_ERR),\n\tLOCAL_STATUS_TO_EVENT_MAP(P_ATR_EVT_FETCH_ERR),\n\tLOCAL_STATUS_TO_EVENT_MAP(P_ATR_EVT_DISCARD_ERR),\n\tLOCAL_STATUS_TO_EVENT_MAP(P_ATR_EVT_DOORBELL),\n\tLOCAL_STATUS_TO_EVENT_MAP(PM_MSI_INT_INTX),\n\tLOCAL_STATUS_TO_EVENT_MAP(PM_MSI_INT_MSI),\n\tLOCAL_STATUS_TO_EVENT_MAP(PM_MSI_INT_AER_EVT),\n\tLOCAL_STATUS_TO_EVENT_MAP(PM_MSI_INT_EVENTS),\n\tLOCAL_STATUS_TO_EVENT_MAP(PM_MSI_INT_SYS_ERR),\n};\n\nstatic struct {\n\tu32 base;\n\tu32 offset;\n\tu32 mask;\n\tu32 shift;\n\tu32 enb_mask;\n\tu32 mask_high;\n\tu32 mask_offset;\n} event_descs[] = {\n\t{ PCIE_EVENT(L2_EXIT) },\n\t{ PCIE_EVENT(HOTRST_EXIT) },\n\t{ PCIE_EVENT(DLUP_EXIT) },\n\t{ SEC_EVENT(TX_RAM_SEC_ERR) },\n\t{ SEC_EVENT(RX_RAM_SEC_ERR) },\n\t{ SEC_EVENT(PCIE2AXI_RAM_SEC_ERR) },\n\t{ SEC_EVENT(AXI2PCIE_RAM_SEC_ERR) },\n\t{ DED_EVENT(TX_RAM_DED_ERR) },\n\t{ DED_EVENT(RX_RAM_DED_ERR) },\n\t{ DED_EVENT(PCIE2AXI_RAM_DED_ERR) },\n\t{ DED_EVENT(AXI2PCIE_RAM_DED_ERR) },\n\t{ LOCAL_EVENT(DMA_END_ENGINE_0) },\n\t{ LOCAL_EVENT(DMA_END_ENGINE_1) },\n\t{ LOCAL_EVENT(DMA_ERROR_ENGINE_0) },\n\t{ LOCAL_EVENT(DMA_ERROR_ENGINE_1) },\n\t{ LOCAL_EVENT(A_ATR_EVT_POST_ERR) },\n\t{ LOCAL_EVENT(A_ATR_EVT_FETCH_ERR) },\n\t{ LOCAL_EVENT(A_ATR_EVT_DISCARD_ERR) },\n\t{ LOCAL_EVENT(A_ATR_EVT_DOORBELL) },\n\t{ LOCAL_EVENT(P_ATR_EVT_POST_ERR) },\n\t{ LOCAL_EVENT(P_ATR_EVT_FETCH_ERR) },\n\t{ LOCAL_EVENT(P_ATR_EVT_DISCARD_ERR) },\n\t{ LOCAL_EVENT(P_ATR_EVT_DOORBELL) },\n\t{ LOCAL_EVENT(PM_MSI_INT_INTX) },\n\t{ LOCAL_EVENT(PM_MSI_INT_MSI) },\n\t{ LOCAL_EVENT(PM_MSI_INT_AER_EVT) },\n\t{ LOCAL_EVENT(PM_MSI_INT_EVENTS) },\n\t{ LOCAL_EVENT(PM_MSI_INT_SYS_ERR) },\n};\n\nstatic char poss_clks[][5] = { \"fic0\", \"fic1\", \"fic2\", \"fic3\" };\n\nstatic struct mc_pcie *port;\n\nstatic void mc_pcie_enable_msi(struct mc_pcie *port, void __iomem *ecam)\n{\n\tstruct mc_msi *msi = &port->msi;\n\tu16 reg;\n\tu8 queue_size;\n\n\t \n\treg = readw_relaxed(ecam + MC_MSI_CAP_CTRL_OFFSET + PCI_MSI_FLAGS);\n\treg |= PCI_MSI_FLAGS_ENABLE;\n\twritew_relaxed(reg, ecam + MC_MSI_CAP_CTRL_OFFSET + PCI_MSI_FLAGS);\n\n\t \n\tqueue_size = FIELD_GET(PCI_MSI_FLAGS_QMASK, reg);\n\treg |= FIELD_PREP(PCI_MSI_FLAGS_QSIZE, queue_size);\n\twritew_relaxed(reg, ecam + MC_MSI_CAP_CTRL_OFFSET + PCI_MSI_FLAGS);\n\n\t \n\twritel_relaxed(lower_32_bits(msi->vector_phy),\n\t\t       ecam + MC_MSI_CAP_CTRL_OFFSET + PCI_MSI_ADDRESS_LO);\n\twritel_relaxed(upper_32_bits(msi->vector_phy),\n\t\t       ecam + MC_MSI_CAP_CTRL_OFFSET + PCI_MSI_ADDRESS_HI);\n}\n\nstatic void mc_handle_msi(struct irq_desc *desc)\n{\n\tstruct mc_pcie *port = irq_desc_get_handler_data(desc);\n\tstruct irq_chip *chip = irq_desc_get_chip(desc);\n\tstruct device *dev = port->dev;\n\tstruct mc_msi *msi = &port->msi;\n\tvoid __iomem *bridge_base_addr =\n\t\tport->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tunsigned long status;\n\tu32 bit;\n\tint ret;\n\n\tchained_irq_enter(chip, desc);\n\n\tstatus = readl_relaxed(bridge_base_addr + ISTATUS_LOCAL);\n\tif (status & PM_MSI_INT_MSI_MASK) {\n\t\twritel_relaxed(status & PM_MSI_INT_MSI_MASK, bridge_base_addr + ISTATUS_LOCAL);\n\t\tstatus = readl_relaxed(bridge_base_addr + ISTATUS_MSI);\n\t\tfor_each_set_bit(bit, &status, msi->num_vectors) {\n\t\t\tret = generic_handle_domain_irq(msi->dev_domain, bit);\n\t\t\tif (ret)\n\t\t\t\tdev_err_ratelimited(dev, \"bad MSI IRQ %d\\n\",\n\t\t\t\t\t\t    bit);\n\t\t}\n\t}\n\n\tchained_irq_exit(chip, desc);\n}\n\nstatic void mc_msi_bottom_irq_ack(struct irq_data *data)\n{\n\tstruct mc_pcie *port = irq_data_get_irq_chip_data(data);\n\tvoid __iomem *bridge_base_addr =\n\t\tport->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tu32 bitpos = data->hwirq;\n\n\twritel_relaxed(BIT(bitpos), bridge_base_addr + ISTATUS_MSI);\n}\n\nstatic void mc_compose_msi_msg(struct irq_data *data, struct msi_msg *msg)\n{\n\tstruct mc_pcie *port = irq_data_get_irq_chip_data(data);\n\tphys_addr_t addr = port->msi.vector_phy;\n\n\tmsg->address_lo = lower_32_bits(addr);\n\tmsg->address_hi = upper_32_bits(addr);\n\tmsg->data = data->hwirq;\n\n\tdev_dbg(port->dev, \"msi#%x address_hi %#x address_lo %#x\\n\",\n\t\t(int)data->hwirq, msg->address_hi, msg->address_lo);\n}\n\nstatic int mc_msi_set_affinity(struct irq_data *irq_data,\n\t\t\t       const struct cpumask *mask, bool force)\n{\n\treturn -EINVAL;\n}\n\nstatic struct irq_chip mc_msi_bottom_irq_chip = {\n\t.name = \"Microchip MSI\",\n\t.irq_ack = mc_msi_bottom_irq_ack,\n\t.irq_compose_msi_msg = mc_compose_msi_msg,\n\t.irq_set_affinity = mc_msi_set_affinity,\n};\n\nstatic int mc_irq_msi_domain_alloc(struct irq_domain *domain, unsigned int virq,\n\t\t\t\t   unsigned int nr_irqs, void *args)\n{\n\tstruct mc_pcie *port = domain->host_data;\n\tstruct mc_msi *msi = &port->msi;\n\tunsigned long bit;\n\n\tmutex_lock(&msi->lock);\n\tbit = find_first_zero_bit(msi->used, msi->num_vectors);\n\tif (bit >= msi->num_vectors) {\n\t\tmutex_unlock(&msi->lock);\n\t\treturn -ENOSPC;\n\t}\n\n\tset_bit(bit, msi->used);\n\n\tirq_domain_set_info(domain, virq, bit, &mc_msi_bottom_irq_chip,\n\t\t\t    domain->host_data, handle_edge_irq, NULL, NULL);\n\n\tmutex_unlock(&msi->lock);\n\n\treturn 0;\n}\n\nstatic void mc_irq_msi_domain_free(struct irq_domain *domain, unsigned int virq,\n\t\t\t\t   unsigned int nr_irqs)\n{\n\tstruct irq_data *d = irq_domain_get_irq_data(domain, virq);\n\tstruct mc_pcie *port = irq_data_get_irq_chip_data(d);\n\tstruct mc_msi *msi = &port->msi;\n\n\tmutex_lock(&msi->lock);\n\n\tif (test_bit(d->hwirq, msi->used))\n\t\t__clear_bit(d->hwirq, msi->used);\n\telse\n\t\tdev_err(port->dev, \"trying to free unused MSI%lu\\n\", d->hwirq);\n\n\tmutex_unlock(&msi->lock);\n}\n\nstatic const struct irq_domain_ops msi_domain_ops = {\n\t.alloc\t= mc_irq_msi_domain_alloc,\n\t.free\t= mc_irq_msi_domain_free,\n};\n\nstatic struct irq_chip mc_msi_irq_chip = {\n\t.name = \"Microchip PCIe MSI\",\n\t.irq_ack = irq_chip_ack_parent,\n\t.irq_mask = pci_msi_mask_irq,\n\t.irq_unmask = pci_msi_unmask_irq,\n};\n\nstatic struct msi_domain_info mc_msi_domain_info = {\n\t.flags = (MSI_FLAG_USE_DEF_DOM_OPS | MSI_FLAG_USE_DEF_CHIP_OPS |\n\t\t  MSI_FLAG_PCI_MSIX),\n\t.chip = &mc_msi_irq_chip,\n};\n\nstatic int mc_allocate_msi_domains(struct mc_pcie *port)\n{\n\tstruct device *dev = port->dev;\n\tstruct fwnode_handle *fwnode = of_node_to_fwnode(dev->of_node);\n\tstruct mc_msi *msi = &port->msi;\n\n\tmutex_init(&port->msi.lock);\n\n\tmsi->dev_domain = irq_domain_add_linear(NULL, msi->num_vectors,\n\t\t\t\t\t\t&msi_domain_ops, port);\n\tif (!msi->dev_domain) {\n\t\tdev_err(dev, \"failed to create IRQ domain\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmsi->msi_domain = pci_msi_create_irq_domain(fwnode, &mc_msi_domain_info,\n\t\t\t\t\t\t    msi->dev_domain);\n\tif (!msi->msi_domain) {\n\t\tdev_err(dev, \"failed to create MSI domain\\n\");\n\t\tirq_domain_remove(msi->dev_domain);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void mc_handle_intx(struct irq_desc *desc)\n{\n\tstruct mc_pcie *port = irq_desc_get_handler_data(desc);\n\tstruct irq_chip *chip = irq_desc_get_chip(desc);\n\tstruct device *dev = port->dev;\n\tvoid __iomem *bridge_base_addr =\n\t\tport->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tunsigned long status;\n\tu32 bit;\n\tint ret;\n\n\tchained_irq_enter(chip, desc);\n\n\tstatus = readl_relaxed(bridge_base_addr + ISTATUS_LOCAL);\n\tif (status & PM_MSI_INT_INTX_MASK) {\n\t\tstatus &= PM_MSI_INT_INTX_MASK;\n\t\tstatus >>= PM_MSI_INT_INTX_SHIFT;\n\t\tfor_each_set_bit(bit, &status, PCI_NUM_INTX) {\n\t\t\tret = generic_handle_domain_irq(port->intx_domain, bit);\n\t\t\tif (ret)\n\t\t\t\tdev_err_ratelimited(dev, \"bad INTx IRQ %d\\n\",\n\t\t\t\t\t\t    bit);\n\t\t}\n\t}\n\n\tchained_irq_exit(chip, desc);\n}\n\nstatic void mc_ack_intx_irq(struct irq_data *data)\n{\n\tstruct mc_pcie *port = irq_data_get_irq_chip_data(data);\n\tvoid __iomem *bridge_base_addr =\n\t\tport->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tu32 mask = BIT(data->hwirq + PM_MSI_INT_INTX_SHIFT);\n\n\twritel_relaxed(mask, bridge_base_addr + ISTATUS_LOCAL);\n}\n\nstatic void mc_mask_intx_irq(struct irq_data *data)\n{\n\tstruct mc_pcie *port = irq_data_get_irq_chip_data(data);\n\tvoid __iomem *bridge_base_addr =\n\t\tport->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tunsigned long flags;\n\tu32 mask = BIT(data->hwirq + PM_MSI_INT_INTX_SHIFT);\n\tu32 val;\n\n\traw_spin_lock_irqsave(&port->lock, flags);\n\tval = readl_relaxed(bridge_base_addr + IMASK_LOCAL);\n\tval &= ~mask;\n\twritel_relaxed(val, bridge_base_addr + IMASK_LOCAL);\n\traw_spin_unlock_irqrestore(&port->lock, flags);\n}\n\nstatic void mc_unmask_intx_irq(struct irq_data *data)\n{\n\tstruct mc_pcie *port = irq_data_get_irq_chip_data(data);\n\tvoid __iomem *bridge_base_addr =\n\t\tport->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tunsigned long flags;\n\tu32 mask = BIT(data->hwirq + PM_MSI_INT_INTX_SHIFT);\n\tu32 val;\n\n\traw_spin_lock_irqsave(&port->lock, flags);\n\tval = readl_relaxed(bridge_base_addr + IMASK_LOCAL);\n\tval |= mask;\n\twritel_relaxed(val, bridge_base_addr + IMASK_LOCAL);\n\traw_spin_unlock_irqrestore(&port->lock, flags);\n}\n\nstatic struct irq_chip mc_intx_irq_chip = {\n\t.name = \"Microchip PCIe INTx\",\n\t.irq_ack = mc_ack_intx_irq,\n\t.irq_mask = mc_mask_intx_irq,\n\t.irq_unmask = mc_unmask_intx_irq,\n};\n\nstatic int mc_pcie_intx_map(struct irq_domain *domain, unsigned int irq,\n\t\t\t    irq_hw_number_t hwirq)\n{\n\tirq_set_chip_and_handler(irq, &mc_intx_irq_chip, handle_level_irq);\n\tirq_set_chip_data(irq, domain->host_data);\n\n\treturn 0;\n}\n\nstatic const struct irq_domain_ops intx_domain_ops = {\n\t.map = mc_pcie_intx_map,\n};\n\nstatic inline u32 reg_to_event(u32 reg, struct event_map field)\n{\n\treturn (reg & field.reg_mask) ? BIT(field.event_bit) : 0;\n}\n\nstatic u32 pcie_events(struct mc_pcie *port)\n{\n\tvoid __iomem *ctrl_base_addr = port->axi_base_addr + MC_PCIE_CTRL_ADDR;\n\tu32 reg = readl_relaxed(ctrl_base_addr + PCIE_EVENT_INT);\n\tu32 val = 0;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(pcie_event_to_event); i++)\n\t\tval |= reg_to_event(reg, pcie_event_to_event[i]);\n\n\treturn val;\n}\n\nstatic u32 sec_errors(struct mc_pcie *port)\n{\n\tvoid __iomem *ctrl_base_addr = port->axi_base_addr + MC_PCIE_CTRL_ADDR;\n\tu32 reg = readl_relaxed(ctrl_base_addr + SEC_ERROR_INT);\n\tu32 val = 0;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(sec_error_to_event); i++)\n\t\tval |= reg_to_event(reg, sec_error_to_event[i]);\n\n\treturn val;\n}\n\nstatic u32 ded_errors(struct mc_pcie *port)\n{\n\tvoid __iomem *ctrl_base_addr = port->axi_base_addr + MC_PCIE_CTRL_ADDR;\n\tu32 reg = readl_relaxed(ctrl_base_addr + DED_ERROR_INT);\n\tu32 val = 0;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(ded_error_to_event); i++)\n\t\tval |= reg_to_event(reg, ded_error_to_event[i]);\n\n\treturn val;\n}\n\nstatic u32 local_events(struct mc_pcie *port)\n{\n\tvoid __iomem *bridge_base_addr = port->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tu32 reg = readl_relaxed(bridge_base_addr + ISTATUS_LOCAL);\n\tu32 val = 0;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(local_status_to_event); i++)\n\t\tval |= reg_to_event(reg, local_status_to_event[i]);\n\n\treturn val;\n}\n\nstatic u32 get_events(struct mc_pcie *port)\n{\n\tu32 events = 0;\n\n\tevents |= pcie_events(port);\n\tevents |= sec_errors(port);\n\tevents |= ded_errors(port);\n\tevents |= local_events(port);\n\n\treturn events;\n}\n\nstatic irqreturn_t mc_event_handler(int irq, void *dev_id)\n{\n\tstruct mc_pcie *port = dev_id;\n\tstruct device *dev = port->dev;\n\tstruct irq_data *data;\n\n\tdata = irq_domain_get_irq_data(port->event_domain, irq);\n\n\tif (event_cause[data->hwirq].str)\n\t\tdev_err_ratelimited(dev, \"%s\\n\", event_cause[data->hwirq].str);\n\telse\n\t\tdev_err_ratelimited(dev, \"bad event IRQ %ld\\n\", data->hwirq);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void mc_handle_event(struct irq_desc *desc)\n{\n\tstruct mc_pcie *port = irq_desc_get_handler_data(desc);\n\tunsigned long events;\n\tu32 bit;\n\tstruct irq_chip *chip = irq_desc_get_chip(desc);\n\n\tchained_irq_enter(chip, desc);\n\n\tevents = get_events(port);\n\n\tfor_each_set_bit(bit, &events, NUM_EVENTS)\n\t\tgeneric_handle_domain_irq(port->event_domain, bit);\n\n\tchained_irq_exit(chip, desc);\n}\n\nstatic void mc_ack_event_irq(struct irq_data *data)\n{\n\tstruct mc_pcie *port = irq_data_get_irq_chip_data(data);\n\tu32 event = data->hwirq;\n\tvoid __iomem *addr;\n\tu32 mask;\n\n\taddr = port->axi_base_addr + event_descs[event].base +\n\t\tevent_descs[event].offset;\n\tmask = event_descs[event].mask;\n\tmask |= event_descs[event].enb_mask;\n\n\twritel_relaxed(mask, addr);\n}\n\nstatic void mc_mask_event_irq(struct irq_data *data)\n{\n\tstruct mc_pcie *port = irq_data_get_irq_chip_data(data);\n\tu32 event = data->hwirq;\n\tvoid __iomem *addr;\n\tu32 mask;\n\tu32 val;\n\n\taddr = port->axi_base_addr + event_descs[event].base +\n\t\tevent_descs[event].mask_offset;\n\tmask = event_descs[event].mask;\n\tif (event_descs[event].enb_mask) {\n\t\tmask <<= PCIE_EVENT_INT_ENB_SHIFT;\n\t\tmask &= PCIE_EVENT_INT_ENB_MASK;\n\t}\n\n\tif (!event_descs[event].mask_high)\n\t\tmask = ~mask;\n\n\traw_spin_lock(&port->lock);\n\tval = readl_relaxed(addr);\n\tif (event_descs[event].mask_high)\n\t\tval |= mask;\n\telse\n\t\tval &= mask;\n\n\twritel_relaxed(val, addr);\n\traw_spin_unlock(&port->lock);\n}\n\nstatic void mc_unmask_event_irq(struct irq_data *data)\n{\n\tstruct mc_pcie *port = irq_data_get_irq_chip_data(data);\n\tu32 event = data->hwirq;\n\tvoid __iomem *addr;\n\tu32 mask;\n\tu32 val;\n\n\taddr = port->axi_base_addr + event_descs[event].base +\n\t\tevent_descs[event].mask_offset;\n\tmask = event_descs[event].mask;\n\n\tif (event_descs[event].enb_mask)\n\t\tmask <<= PCIE_EVENT_INT_ENB_SHIFT;\n\n\tif (event_descs[event].mask_high)\n\t\tmask = ~mask;\n\n\tif (event_descs[event].enb_mask)\n\t\tmask &= PCIE_EVENT_INT_ENB_MASK;\n\n\traw_spin_lock(&port->lock);\n\tval = readl_relaxed(addr);\n\tif (event_descs[event].mask_high)\n\t\tval &= mask;\n\telse\n\t\tval |= mask;\n\twritel_relaxed(val, addr);\n\traw_spin_unlock(&port->lock);\n}\n\nstatic struct irq_chip mc_event_irq_chip = {\n\t.name = \"Microchip PCIe EVENT\",\n\t.irq_ack = mc_ack_event_irq,\n\t.irq_mask = mc_mask_event_irq,\n\t.irq_unmask = mc_unmask_event_irq,\n};\n\nstatic int mc_pcie_event_map(struct irq_domain *domain, unsigned int irq,\n\t\t\t     irq_hw_number_t hwirq)\n{\n\tirq_set_chip_and_handler(irq, &mc_event_irq_chip, handle_level_irq);\n\tirq_set_chip_data(irq, domain->host_data);\n\n\treturn 0;\n}\n\nstatic const struct irq_domain_ops event_domain_ops = {\n\t.map = mc_pcie_event_map,\n};\n\nstatic inline void mc_pcie_deinit_clk(void *data)\n{\n\tstruct clk *clk = data;\n\n\tclk_disable_unprepare(clk);\n}\n\nstatic inline struct clk *mc_pcie_init_clk(struct device *dev, const char *id)\n{\n\tstruct clk *clk;\n\tint ret;\n\n\tclk = devm_clk_get_optional(dev, id);\n\tif (IS_ERR(clk))\n\t\treturn clk;\n\tif (!clk)\n\t\treturn clk;\n\n\tret = clk_prepare_enable(clk);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tdevm_add_action_or_reset(dev, mc_pcie_deinit_clk, clk);\n\n\treturn clk;\n}\n\nstatic int mc_pcie_init_clks(struct device *dev)\n{\n\tint i;\n\tstruct clk *fic;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(poss_clks); i++) {\n\t\tfic = mc_pcie_init_clk(dev, poss_clks[i]);\n\t\tif (IS_ERR(fic))\n\t\t\treturn PTR_ERR(fic);\n\t}\n\n\treturn 0;\n}\n\nstatic int mc_pcie_init_irq_domains(struct mc_pcie *port)\n{\n\tstruct device *dev = port->dev;\n\tstruct device_node *node = dev->of_node;\n\tstruct device_node *pcie_intc_node;\n\n\t \n\tpcie_intc_node = of_get_next_child(node, NULL);\n\tif (!pcie_intc_node) {\n\t\tdev_err(dev, \"failed to find PCIe Intc node\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tport->event_domain = irq_domain_add_linear(pcie_intc_node, NUM_EVENTS,\n\t\t\t\t\t\t   &event_domain_ops, port);\n\tif (!port->event_domain) {\n\t\tdev_err(dev, \"failed to get event domain\\n\");\n\t\tof_node_put(pcie_intc_node);\n\t\treturn -ENOMEM;\n\t}\n\n\tirq_domain_update_bus_token(port->event_domain, DOMAIN_BUS_NEXUS);\n\n\tport->intx_domain = irq_domain_add_linear(pcie_intc_node, PCI_NUM_INTX,\n\t\t\t\t\t\t  &intx_domain_ops, port);\n\tif (!port->intx_domain) {\n\t\tdev_err(dev, \"failed to get an INTx IRQ domain\\n\");\n\t\tof_node_put(pcie_intc_node);\n\t\treturn -ENOMEM;\n\t}\n\n\tirq_domain_update_bus_token(port->intx_domain, DOMAIN_BUS_WIRED);\n\n\tof_node_put(pcie_intc_node);\n\traw_spin_lock_init(&port->lock);\n\n\treturn mc_allocate_msi_domains(port);\n}\n\nstatic void mc_pcie_setup_window(void __iomem *bridge_base_addr, u32 index,\n\t\t\t\t phys_addr_t axi_addr, phys_addr_t pci_addr,\n\t\t\t\t size_t size)\n{\n\tu32 atr_sz = ilog2(size) - 1;\n\tu32 val;\n\n\tif (index == 0)\n\t\tval = PCIE_CONFIG_INTERFACE;\n\telse\n\t\tval = PCIE_TX_RX_INTERFACE;\n\n\twritel(val, bridge_base_addr + (index * ATR_ENTRY_SIZE) +\n\t       ATR0_AXI4_SLV0_TRSL_PARAM);\n\n\tval = lower_32_bits(axi_addr) | (atr_sz << ATR_SIZE_SHIFT) |\n\t\t\t    ATR_IMPL_ENABLE;\n\twritel(val, bridge_base_addr + (index * ATR_ENTRY_SIZE) +\n\t       ATR0_AXI4_SLV0_SRCADDR_PARAM);\n\n\tval = upper_32_bits(axi_addr);\n\twritel(val, bridge_base_addr + (index * ATR_ENTRY_SIZE) +\n\t       ATR0_AXI4_SLV0_SRC_ADDR);\n\n\tval = lower_32_bits(pci_addr);\n\twritel(val, bridge_base_addr + (index * ATR_ENTRY_SIZE) +\n\t       ATR0_AXI4_SLV0_TRSL_ADDR_LSB);\n\n\tval = upper_32_bits(pci_addr);\n\twritel(val, bridge_base_addr + (index * ATR_ENTRY_SIZE) +\n\t       ATR0_AXI4_SLV0_TRSL_ADDR_UDW);\n\n\tval = readl(bridge_base_addr + ATR0_PCIE_WIN0_SRCADDR_PARAM);\n\tval |= (ATR0_PCIE_ATR_SIZE << ATR0_PCIE_ATR_SIZE_SHIFT);\n\twritel(val, bridge_base_addr + ATR0_PCIE_WIN0_SRCADDR_PARAM);\n\twritel(0, bridge_base_addr + ATR0_PCIE_WIN0_SRC_ADDR);\n}\n\nstatic int mc_pcie_setup_windows(struct platform_device *pdev,\n\t\t\t\t struct mc_pcie *port)\n{\n\tvoid __iomem *bridge_base_addr =\n\t\tport->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tstruct pci_host_bridge *bridge = platform_get_drvdata(pdev);\n\tstruct resource_entry *entry;\n\tu64 pci_addr;\n\tu32 index = 1;\n\n\tresource_list_for_each_entry(entry, &bridge->windows) {\n\t\tif (resource_type(entry->res) == IORESOURCE_MEM) {\n\t\t\tpci_addr = entry->res->start - entry->offset;\n\t\t\tmc_pcie_setup_window(bridge_base_addr, index,\n\t\t\t\t\t     entry->res->start, pci_addr,\n\t\t\t\t\t     resource_size(entry->res));\n\t\t\tindex++;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic inline void mc_clear_secs(struct mc_pcie *port)\n{\n\tvoid __iomem *ctrl_base_addr = port->axi_base_addr + MC_PCIE_CTRL_ADDR;\n\n\twritel_relaxed(SEC_ERROR_INT_ALL_RAM_SEC_ERR_INT, ctrl_base_addr +\n\t\t       SEC_ERROR_INT);\n\twritel_relaxed(0, ctrl_base_addr + SEC_ERROR_EVENT_CNT);\n}\n\nstatic inline void mc_clear_deds(struct mc_pcie *port)\n{\n\tvoid __iomem *ctrl_base_addr = port->axi_base_addr + MC_PCIE_CTRL_ADDR;\n\n\twritel_relaxed(DED_ERROR_INT_ALL_RAM_DED_ERR_INT, ctrl_base_addr +\n\t\t       DED_ERROR_INT);\n\twritel_relaxed(0, ctrl_base_addr + DED_ERROR_EVENT_CNT);\n}\n\nstatic void mc_disable_interrupts(struct mc_pcie *port)\n{\n\tvoid __iomem *bridge_base_addr = port->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tvoid __iomem *ctrl_base_addr = port->axi_base_addr + MC_PCIE_CTRL_ADDR;\n\tu32 val;\n\n\t \n\tval = ECC_CONTROL_TX_RAM_ECC_BYPASS |\n\t      ECC_CONTROL_RX_RAM_ECC_BYPASS |\n\t      ECC_CONTROL_PCIE2AXI_RAM_ECC_BYPASS |\n\t      ECC_CONTROL_AXI2PCIE_RAM_ECC_BYPASS;\n\twritel_relaxed(val, ctrl_base_addr + ECC_CONTROL);\n\n\t \n\twritel_relaxed(SEC_ERROR_INT_ALL_RAM_SEC_ERR_INT, ctrl_base_addr +\n\t\t       SEC_ERROR_INT_MASK);\n\tmc_clear_secs(port);\n\n\t \n\twritel_relaxed(DED_ERROR_INT_ALL_RAM_DED_ERR_INT, ctrl_base_addr +\n\t\t       DED_ERROR_INT_MASK);\n\tmc_clear_deds(port);\n\n\t \n\twritel_relaxed(0, bridge_base_addr + IMASK_LOCAL);\n\twritel_relaxed(GENMASK(31, 0), bridge_base_addr + ISTATUS_LOCAL);\n\twritel_relaxed(GENMASK(31, 0), bridge_base_addr + ISTATUS_MSI);\n\n\t \n\tval = PCIE_EVENT_INT_L2_EXIT_INT |\n\t      PCIE_EVENT_INT_HOTRST_EXIT_INT |\n\t      PCIE_EVENT_INT_DLUP_EXIT_INT |\n\t      PCIE_EVENT_INT_L2_EXIT_INT_MASK |\n\t      PCIE_EVENT_INT_HOTRST_EXIT_INT_MASK |\n\t      PCIE_EVENT_INT_DLUP_EXIT_INT_MASK;\n\twritel_relaxed(val, ctrl_base_addr + PCIE_EVENT_INT);\n\n\t \n\twritel_relaxed(0, bridge_base_addr + IMASK_HOST);\n\twritel_relaxed(GENMASK(31, 0), bridge_base_addr + ISTATUS_HOST);\n}\n\nstatic int mc_init_interrupts(struct platform_device *pdev, struct mc_pcie *port)\n{\n\tstruct device *dev = &pdev->dev;\n\tint irq;\n\tint i, intx_irq, msi_irq, event_irq;\n\tint ret;\n\n\tret = mc_pcie_init_irq_domains(port);\n\tif (ret) {\n\t\tdev_err(dev, \"failed creating IRQ domains\\n\");\n\t\treturn ret;\n\t}\n\n\tirq = platform_get_irq(pdev, 0);\n\tif (irq < 0)\n\t\treturn -ENODEV;\n\n\tfor (i = 0; i < NUM_EVENTS; i++) {\n\t\tevent_irq = irq_create_mapping(port->event_domain, i);\n\t\tif (!event_irq) {\n\t\t\tdev_err(dev, \"failed to map hwirq %d\\n\", i);\n\t\t\treturn -ENXIO;\n\t\t}\n\n\t\tret = devm_request_irq(dev, event_irq, mc_event_handler,\n\t\t\t\t       0, event_cause[i].sym, port);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"failed to request IRQ %d\\n\", event_irq);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tintx_irq = irq_create_mapping(port->event_domain,\n\t\t\t\t      EVENT_LOCAL_PM_MSI_INT_INTX);\n\tif (!intx_irq) {\n\t\tdev_err(dev, \"failed to map INTx interrupt\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tirq_set_chained_handler_and_data(intx_irq, mc_handle_intx, port);\n\n\tmsi_irq = irq_create_mapping(port->event_domain,\n\t\t\t\t     EVENT_LOCAL_PM_MSI_INT_MSI);\n\tif (!msi_irq)\n\t\treturn -ENXIO;\n\n\t \n\tirq_set_chained_handler_and_data(msi_irq, mc_handle_msi, port);\n\n\t \n\tirq_set_chained_handler_and_data(irq, mc_handle_event, port);\n\n\treturn 0;\n}\n\nstatic int mc_platform_init(struct pci_config_window *cfg)\n{\n\tstruct device *dev = cfg->parent;\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tvoid __iomem *bridge_base_addr =\n\t\tport->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\tint ret;\n\n\t \n\tmc_pcie_setup_window(bridge_base_addr, 0, cfg->res.start,\n\t\t\t     cfg->res.start,\n\t\t\t     resource_size(&cfg->res));\n\n\t \n\tmc_pcie_enable_msi(port, cfg->win);\n\n\t \n\tret = mc_pcie_setup_windows(pdev, port);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = mc_init_interrupts(pdev, port);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic int mc_host_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tvoid __iomem *bridge_base_addr;\n\tint ret;\n\tu32 val;\n\n\tport = devm_kzalloc(dev, sizeof(*port), GFP_KERNEL);\n\tif (!port)\n\t\treturn -ENOMEM;\n\n\tport->dev = dev;\n\n\tport->axi_base_addr = devm_platform_ioremap_resource(pdev, 1);\n\tif (IS_ERR(port->axi_base_addr))\n\t\treturn PTR_ERR(port->axi_base_addr);\n\n\tmc_disable_interrupts(port);\n\n\tbridge_base_addr = port->axi_base_addr + MC_PCIE_BRIDGE_ADDR;\n\n\t \n\tval = readl(bridge_base_addr + PCIE_PCI_IRQ_DW0);\n\tval &= ~MSIX_CAP_MASK;\n\twritel(val, bridge_base_addr + PCIE_PCI_IRQ_DW0);\n\n\t \n\tval = readl(bridge_base_addr + PCIE_PCI_IRQ_DW0);\n\tval &= NUM_MSI_MSGS_MASK;\n\tval >>= NUM_MSI_MSGS_SHIFT;\n\n\tport->msi.num_vectors = 1 << val;\n\n\t \n\tport->msi.vector_phy = readl_relaxed(bridge_base_addr + IMSI_ADDR);\n\n\tret = mc_pcie_init_clks(dev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to get clock resources, error %d\\n\", ret);\n\t\treturn -ENODEV;\n\t}\n\n\treturn pci_host_common_probe(pdev);\n}\n\nstatic const struct pci_ecam_ops mc_ecam_ops = {\n\t.init = mc_platform_init,\n\t.pci_ops = {\n\t\t.map_bus = pci_ecam_map_bus,\n\t\t.read = pci_generic_config_read,\n\t\t.write = pci_generic_config_write,\n\t}\n};\n\nstatic const struct of_device_id mc_pcie_of_match[] = {\n\t{\n\t\t.compatible = \"microchip,pcie-host-1.0\",\n\t\t.data = &mc_ecam_ops,\n\t},\n\t{},\n};\n\nMODULE_DEVICE_TABLE(of, mc_pcie_of_match);\n\nstatic struct platform_driver mc_pcie_driver = {\n\t.probe = mc_host_probe,\n\t.driver = {\n\t\t.name = \"microchip-pcie\",\n\t\t.of_match_table = mc_pcie_of_match,\n\t\t.suppress_bind_attrs = true,\n\t},\n};\n\nbuiltin_platform_driver(mc_pcie_driver);\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Microchip PCIe host controller driver\");\nMODULE_AUTHOR(\"Daire McNamara <daire.mcnamara@microchip.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}