{
  "module_name": "pcie-xilinx-cpm.c",
  "hash_id": "1f2fdf84a8251adc1f0de2daa540485296e671330ec305c4720dfc72821ad9ee",
  "original_prompt": "Ingested from linux-6.6.14/drivers/pci/controller/pcie-xilinx-cpm.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/irqchip.h>\n#include <linux/irqchip/chained_irq.h>\n#include <linux/irqdomain.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/of_address.h>\n#include <linux/of_pci.h>\n#include <linux/of_platform.h>\n#include <linux/pci.h>\n#include <linux/platform_device.h>\n#include <linux/pci-ecam.h>\n\n#include \"../pci.h\"\n\n \n#define XILINX_CPM_PCIE_REG_IDR\t\t0x00000E10\n#define XILINX_CPM_PCIE_REG_IMR\t\t0x00000E14\n#define XILINX_CPM_PCIE_REG_PSCR\t0x00000E1C\n#define XILINX_CPM_PCIE_REG_RPSC\t0x00000E20\n#define XILINX_CPM_PCIE_REG_RPEFR\t0x00000E2C\n#define XILINX_CPM_PCIE_REG_IDRN\t0x00000E38\n#define XILINX_CPM_PCIE_REG_IDRN_MASK\t0x00000E3C\n#define XILINX_CPM_PCIE_MISC_IR_STATUS\t0x00000340\n#define XILINX_CPM_PCIE_MISC_IR_ENABLE\t0x00000348\n#define XILINX_CPM_PCIE_MISC_IR_LOCAL\tBIT(1)\n\n#define XILINX_CPM_PCIE_IR_STATUS       0x000002A0\n#define XILINX_CPM_PCIE_IR_ENABLE       0x000002A8\n#define XILINX_CPM_PCIE_IR_LOCAL        BIT(0)\n\n \n#define XILINX_CPM_PCIE_INTR_LINK_DOWN\t\t0\n#define XILINX_CPM_PCIE_INTR_HOT_RESET\t\t3\n#define XILINX_CPM_PCIE_INTR_CFG_PCIE_TIMEOUT\t4\n#define XILINX_CPM_PCIE_INTR_CFG_TIMEOUT\t8\n#define XILINX_CPM_PCIE_INTR_CORRECTABLE\t9\n#define XILINX_CPM_PCIE_INTR_NONFATAL\t\t10\n#define XILINX_CPM_PCIE_INTR_FATAL\t\t11\n#define XILINX_CPM_PCIE_INTR_CFG_ERR_POISON\t12\n#define XILINX_CPM_PCIE_INTR_PME_TO_ACK_RCVD\t15\n#define XILINX_CPM_PCIE_INTR_INTX\t\t16\n#define XILINX_CPM_PCIE_INTR_PM_PME_RCVD\t17\n#define XILINX_CPM_PCIE_INTR_SLV_UNSUPP\t\t20\n#define XILINX_CPM_PCIE_INTR_SLV_UNEXP\t\t21\n#define XILINX_CPM_PCIE_INTR_SLV_COMPL\t\t22\n#define XILINX_CPM_PCIE_INTR_SLV_ERRP\t\t23\n#define XILINX_CPM_PCIE_INTR_SLV_CMPABT\t\t24\n#define XILINX_CPM_PCIE_INTR_SLV_ILLBUR\t\t25\n#define XILINX_CPM_PCIE_INTR_MST_DECERR\t\t26\n#define XILINX_CPM_PCIE_INTR_MST_SLVERR\t\t27\n#define XILINX_CPM_PCIE_INTR_SLV_PCIE_TIMEOUT\t28\n\n#define IMR(x) BIT(XILINX_CPM_PCIE_INTR_ ##x)\n\n#define XILINX_CPM_PCIE_IMR_ALL_MASK\t\t\t\\\n\t(\t\t\t\t\t\t\\\n\t\tIMR(LINK_DOWN)\t\t|\t\t\\\n\t\tIMR(HOT_RESET)\t\t|\t\t\\\n\t\tIMR(CFG_PCIE_TIMEOUT)\t|\t\t\\\n\t\tIMR(CFG_TIMEOUT)\t|\t\t\\\n\t\tIMR(CORRECTABLE)\t|\t\t\\\n\t\tIMR(NONFATAL)\t\t|\t\t\\\n\t\tIMR(FATAL)\t\t|\t\t\\\n\t\tIMR(CFG_ERR_POISON)\t|\t\t\\\n\t\tIMR(PME_TO_ACK_RCVD)\t|\t\t\\\n\t\tIMR(INTX)\t\t|\t\t\\\n\t\tIMR(PM_PME_RCVD)\t|\t\t\\\n\t\tIMR(SLV_UNSUPP)\t\t|\t\t\\\n\t\tIMR(SLV_UNEXP)\t\t|\t\t\\\n\t\tIMR(SLV_COMPL)\t\t|\t\t\\\n\t\tIMR(SLV_ERRP)\t\t|\t\t\\\n\t\tIMR(SLV_CMPABT)\t\t|\t\t\\\n\t\tIMR(SLV_ILLBUR)\t\t|\t\t\\\n\t\tIMR(MST_DECERR)\t\t|\t\t\\\n\t\tIMR(MST_SLVERR)\t\t|\t\t\\\n\t\tIMR(SLV_PCIE_TIMEOUT)\t\t\t\\\n\t)\n\n#define XILINX_CPM_PCIE_IDR_ALL_MASK\t\t0xFFFFFFFF\n#define XILINX_CPM_PCIE_IDRN_MASK\t\tGENMASK(19, 16)\n#define XILINX_CPM_PCIE_IDRN_SHIFT\t\t16\n\n \n#define XILINX_CPM_PCIE_RPEFR_ERR_VALID\t\tBIT(18)\n#define XILINX_CPM_PCIE_RPEFR_REQ_ID\t\tGENMASK(15, 0)\n#define XILINX_CPM_PCIE_RPEFR_ALL_MASK\t\t0xFFFFFFFF\n\n \n#define XILINX_CPM_PCIE_REG_RPSC_BEN\t\tBIT(0)\n\n \n#define XILINX_CPM_PCIE_REG_PSCR_LNKUP\t\tBIT(11)\n\nenum xilinx_cpm_version {\n\tCPM,\n\tCPM5,\n};\n\n \nstruct xilinx_cpm_variant {\n\tenum xilinx_cpm_version version;\n};\n\n \nstruct xilinx_cpm_pcie {\n\tstruct device\t\t\t*dev;\n\tvoid __iomem\t\t\t*reg_base;\n\tvoid __iomem\t\t\t*cpm_base;\n\tstruct irq_domain\t\t*intx_domain;\n\tstruct irq_domain\t\t*cpm_domain;\n\tstruct pci_config_window\t*cfg;\n\tint\t\t\t\tintx_irq;\n\tint\t\t\t\tirq;\n\traw_spinlock_t\t\t\tlock;\n\tconst struct xilinx_cpm_variant   *variant;\n};\n\nstatic u32 pcie_read(struct xilinx_cpm_pcie *port, u32 reg)\n{\n\treturn readl_relaxed(port->reg_base + reg);\n}\n\nstatic void pcie_write(struct xilinx_cpm_pcie *port,\n\t\t       u32 val, u32 reg)\n{\n\twritel_relaxed(val, port->reg_base + reg);\n}\n\nstatic bool cpm_pcie_link_up(struct xilinx_cpm_pcie *port)\n{\n\treturn (pcie_read(port, XILINX_CPM_PCIE_REG_PSCR) &\n\t\tXILINX_CPM_PCIE_REG_PSCR_LNKUP);\n}\n\nstatic void cpm_pcie_clear_err_interrupts(struct xilinx_cpm_pcie *port)\n{\n\tunsigned long val = pcie_read(port, XILINX_CPM_PCIE_REG_RPEFR);\n\n\tif (val & XILINX_CPM_PCIE_RPEFR_ERR_VALID) {\n\t\tdev_dbg(port->dev, \"Requester ID %lu\\n\",\n\t\t\tval & XILINX_CPM_PCIE_RPEFR_REQ_ID);\n\t\tpcie_write(port, XILINX_CPM_PCIE_RPEFR_ALL_MASK,\n\t\t\t   XILINX_CPM_PCIE_REG_RPEFR);\n\t}\n}\n\nstatic void xilinx_cpm_mask_leg_irq(struct irq_data *data)\n{\n\tstruct xilinx_cpm_pcie *port = irq_data_get_irq_chip_data(data);\n\tunsigned long flags;\n\tu32 mask;\n\tu32 val;\n\n\tmask = BIT(data->hwirq + XILINX_CPM_PCIE_IDRN_SHIFT);\n\traw_spin_lock_irqsave(&port->lock, flags);\n\tval = pcie_read(port, XILINX_CPM_PCIE_REG_IDRN_MASK);\n\tpcie_write(port, (val & (~mask)), XILINX_CPM_PCIE_REG_IDRN_MASK);\n\traw_spin_unlock_irqrestore(&port->lock, flags);\n}\n\nstatic void xilinx_cpm_unmask_leg_irq(struct irq_data *data)\n{\n\tstruct xilinx_cpm_pcie *port = irq_data_get_irq_chip_data(data);\n\tunsigned long flags;\n\tu32 mask;\n\tu32 val;\n\n\tmask = BIT(data->hwirq + XILINX_CPM_PCIE_IDRN_SHIFT);\n\traw_spin_lock_irqsave(&port->lock, flags);\n\tval = pcie_read(port, XILINX_CPM_PCIE_REG_IDRN_MASK);\n\tpcie_write(port, (val | mask), XILINX_CPM_PCIE_REG_IDRN_MASK);\n\traw_spin_unlock_irqrestore(&port->lock, flags);\n}\n\nstatic struct irq_chip xilinx_cpm_leg_irq_chip = {\n\t.name\t\t= \"INTx\",\n\t.irq_mask\t= xilinx_cpm_mask_leg_irq,\n\t.irq_unmask\t= xilinx_cpm_unmask_leg_irq,\n};\n\n \nstatic int xilinx_cpm_pcie_intx_map(struct irq_domain *domain,\n\t\t\t\t    unsigned int irq, irq_hw_number_t hwirq)\n{\n\tirq_set_chip_and_handler(irq, &xilinx_cpm_leg_irq_chip,\n\t\t\t\t handle_level_irq);\n\tirq_set_chip_data(irq, domain->host_data);\n\tirq_set_status_flags(irq, IRQ_LEVEL);\n\n\treturn 0;\n}\n\n \nstatic const struct irq_domain_ops intx_domain_ops = {\n\t.map = xilinx_cpm_pcie_intx_map,\n};\n\nstatic void xilinx_cpm_pcie_intx_flow(struct irq_desc *desc)\n{\n\tstruct xilinx_cpm_pcie *port = irq_desc_get_handler_data(desc);\n\tstruct irq_chip *chip = irq_desc_get_chip(desc);\n\tunsigned long val;\n\tint i;\n\n\tchained_irq_enter(chip, desc);\n\n\tval = FIELD_GET(XILINX_CPM_PCIE_IDRN_MASK,\n\t\t\tpcie_read(port, XILINX_CPM_PCIE_REG_IDRN));\n\n\tfor_each_set_bit(i, &val, PCI_NUM_INTX)\n\t\tgeneric_handle_domain_irq(port->intx_domain, i);\n\n\tchained_irq_exit(chip, desc);\n}\n\nstatic void xilinx_cpm_mask_event_irq(struct irq_data *d)\n{\n\tstruct xilinx_cpm_pcie *port = irq_data_get_irq_chip_data(d);\n\tu32 val;\n\n\traw_spin_lock(&port->lock);\n\tval = pcie_read(port, XILINX_CPM_PCIE_REG_IMR);\n\tval &= ~BIT(d->hwirq);\n\tpcie_write(port, val, XILINX_CPM_PCIE_REG_IMR);\n\traw_spin_unlock(&port->lock);\n}\n\nstatic void xilinx_cpm_unmask_event_irq(struct irq_data *d)\n{\n\tstruct xilinx_cpm_pcie *port = irq_data_get_irq_chip_data(d);\n\tu32 val;\n\n\traw_spin_lock(&port->lock);\n\tval = pcie_read(port, XILINX_CPM_PCIE_REG_IMR);\n\tval |= BIT(d->hwirq);\n\tpcie_write(port, val, XILINX_CPM_PCIE_REG_IMR);\n\traw_spin_unlock(&port->lock);\n}\n\nstatic struct irq_chip xilinx_cpm_event_irq_chip = {\n\t.name\t\t= \"RC-Event\",\n\t.irq_mask\t= xilinx_cpm_mask_event_irq,\n\t.irq_unmask\t= xilinx_cpm_unmask_event_irq,\n};\n\nstatic int xilinx_cpm_pcie_event_map(struct irq_domain *domain,\n\t\t\t\t     unsigned int irq, irq_hw_number_t hwirq)\n{\n\tirq_set_chip_and_handler(irq, &xilinx_cpm_event_irq_chip,\n\t\t\t\t handle_level_irq);\n\tirq_set_chip_data(irq, domain->host_data);\n\tirq_set_status_flags(irq, IRQ_LEVEL);\n\treturn 0;\n}\n\nstatic const struct irq_domain_ops event_domain_ops = {\n\t.map = xilinx_cpm_pcie_event_map,\n};\n\nstatic void xilinx_cpm_pcie_event_flow(struct irq_desc *desc)\n{\n\tstruct xilinx_cpm_pcie *port = irq_desc_get_handler_data(desc);\n\tstruct irq_chip *chip = irq_desc_get_chip(desc);\n\tunsigned long val;\n\tint i;\n\n\tchained_irq_enter(chip, desc);\n\tval =  pcie_read(port, XILINX_CPM_PCIE_REG_IDR);\n\tval &= pcie_read(port, XILINX_CPM_PCIE_REG_IMR);\n\tfor_each_set_bit(i, &val, 32)\n\t\tgeneric_handle_domain_irq(port->cpm_domain, i);\n\tpcie_write(port, val, XILINX_CPM_PCIE_REG_IDR);\n\n\tif (port->variant->version == CPM5) {\n\t\tval = readl_relaxed(port->cpm_base + XILINX_CPM_PCIE_IR_STATUS);\n\t\tif (val)\n\t\t\twritel_relaxed(val, port->cpm_base +\n\t\t\t\t\t    XILINX_CPM_PCIE_IR_STATUS);\n\t}\n\n\t \n\tval = readl_relaxed(port->cpm_base + XILINX_CPM_PCIE_MISC_IR_STATUS);\n\tif (val)\n\t\twritel_relaxed(val,\n\t\t\t       port->cpm_base + XILINX_CPM_PCIE_MISC_IR_STATUS);\n\n\tchained_irq_exit(chip, desc);\n}\n\n#define _IC(x, s)                              \\\n\t[XILINX_CPM_PCIE_INTR_ ## x] = { __stringify(x), s }\n\nstatic const struct {\n\tconst char      *sym;\n\tconst char      *str;\n} intr_cause[32] = {\n\t_IC(LINK_DOWN,\t\t\"Link Down\"),\n\t_IC(HOT_RESET,\t\t\"Hot reset\"),\n\t_IC(CFG_TIMEOUT,\t\"ECAM access timeout\"),\n\t_IC(CORRECTABLE,\t\"Correctable error message\"),\n\t_IC(NONFATAL,\t\t\"Non fatal error message\"),\n\t_IC(FATAL,\t\t\"Fatal error message\"),\n\t_IC(SLV_UNSUPP,\t\t\"Slave unsupported request\"),\n\t_IC(SLV_UNEXP,\t\t\"Slave unexpected completion\"),\n\t_IC(SLV_COMPL,\t\t\"Slave completion timeout\"),\n\t_IC(SLV_ERRP,\t\t\"Slave Error Poison\"),\n\t_IC(SLV_CMPABT,\t\t\"Slave Completer Abort\"),\n\t_IC(SLV_ILLBUR,\t\t\"Slave Illegal Burst\"),\n\t_IC(MST_DECERR,\t\t\"Master decode error\"),\n\t_IC(MST_SLVERR,\t\t\"Master slave error\"),\n\t_IC(CFG_PCIE_TIMEOUT,\t\"PCIe ECAM access timeout\"),\n\t_IC(CFG_ERR_POISON,\t\"ECAM poisoned completion received\"),\n\t_IC(PME_TO_ACK_RCVD,\t\"PME_TO_ACK message received\"),\n\t_IC(PM_PME_RCVD,\t\"PM_PME message received\"),\n\t_IC(SLV_PCIE_TIMEOUT,\t\"PCIe completion timeout received\"),\n};\n\nstatic irqreturn_t xilinx_cpm_pcie_intr_handler(int irq, void *dev_id)\n{\n\tstruct xilinx_cpm_pcie *port = dev_id;\n\tstruct device *dev = port->dev;\n\tstruct irq_data *d;\n\n\td = irq_domain_get_irq_data(port->cpm_domain, irq);\n\n\tswitch (d->hwirq) {\n\tcase XILINX_CPM_PCIE_INTR_CORRECTABLE:\n\tcase XILINX_CPM_PCIE_INTR_NONFATAL:\n\tcase XILINX_CPM_PCIE_INTR_FATAL:\n\t\tcpm_pcie_clear_err_interrupts(port);\n\t\tfallthrough;\n\n\tdefault:\n\t\tif (intr_cause[d->hwirq].str)\n\t\t\tdev_warn(dev, \"%s\\n\", intr_cause[d->hwirq].str);\n\t\telse\n\t\t\tdev_warn(dev, \"Unknown IRQ %ld\\n\", d->hwirq);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void xilinx_cpm_free_irq_domains(struct xilinx_cpm_pcie *port)\n{\n\tif (port->intx_domain) {\n\t\tirq_domain_remove(port->intx_domain);\n\t\tport->intx_domain = NULL;\n\t}\n\n\tif (port->cpm_domain) {\n\t\tirq_domain_remove(port->cpm_domain);\n\t\tport->cpm_domain = NULL;\n\t}\n}\n\n \nstatic int xilinx_cpm_pcie_init_irq_domain(struct xilinx_cpm_pcie *port)\n{\n\tstruct device *dev = port->dev;\n\tstruct device_node *node = dev->of_node;\n\tstruct device_node *pcie_intc_node;\n\n\t \n\tpcie_intc_node = of_get_next_child(node, NULL);\n\tif (!pcie_intc_node) {\n\t\tdev_err(dev, \"No PCIe Intc node found\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tport->cpm_domain = irq_domain_add_linear(pcie_intc_node, 32,\n\t\t\t\t\t\t &event_domain_ops,\n\t\t\t\t\t\t port);\n\tif (!port->cpm_domain)\n\t\tgoto out;\n\n\tirq_domain_update_bus_token(port->cpm_domain, DOMAIN_BUS_NEXUS);\n\n\tport->intx_domain = irq_domain_add_linear(pcie_intc_node, PCI_NUM_INTX,\n\t\t\t\t\t\t  &intx_domain_ops,\n\t\t\t\t\t\t  port);\n\tif (!port->intx_domain)\n\t\tgoto out;\n\n\tirq_domain_update_bus_token(port->intx_domain, DOMAIN_BUS_WIRED);\n\n\tof_node_put(pcie_intc_node);\n\traw_spin_lock_init(&port->lock);\n\n\treturn 0;\nout:\n\txilinx_cpm_free_irq_domains(port);\n\tof_node_put(pcie_intc_node);\n\tdev_err(dev, \"Failed to allocate IRQ domains\\n\");\n\n\treturn -ENOMEM;\n}\n\nstatic int xilinx_cpm_setup_irq(struct xilinx_cpm_pcie *port)\n{\n\tstruct device *dev = port->dev;\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tint i, irq;\n\n\tport->irq = platform_get_irq(pdev, 0);\n\tif (port->irq < 0)\n\t\treturn port->irq;\n\n\tfor (i = 0; i < ARRAY_SIZE(intr_cause); i++) {\n\t\tint err;\n\n\t\tif (!intr_cause[i].str)\n\t\t\tcontinue;\n\n\t\tirq = irq_create_mapping(port->cpm_domain, i);\n\t\tif (!irq) {\n\t\t\tdev_err(dev, \"Failed to map interrupt\\n\");\n\t\t\treturn -ENXIO;\n\t\t}\n\n\t\terr = devm_request_irq(dev, irq, xilinx_cpm_pcie_intr_handler,\n\t\t\t\t       0, intr_cause[i].sym, port);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"Failed to request IRQ %d\\n\", irq);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tport->intx_irq = irq_create_mapping(port->cpm_domain,\n\t\t\t\t\t    XILINX_CPM_PCIE_INTR_INTX);\n\tif (!port->intx_irq) {\n\t\tdev_err(dev, \"Failed to map INTx interrupt\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tirq_set_chained_handler_and_data(port->intx_irq,\n\t\t\t\t\t xilinx_cpm_pcie_intx_flow, port);\n\n\t \n\tirq_set_chained_handler_and_data(port->irq,\n\t\t\t\t\t xilinx_cpm_pcie_event_flow, port);\n\n\treturn 0;\n}\n\n \nstatic void xilinx_cpm_pcie_init_port(struct xilinx_cpm_pcie *port)\n{\n\tif (cpm_pcie_link_up(port))\n\t\tdev_info(port->dev, \"PCIe Link is UP\\n\");\n\telse\n\t\tdev_info(port->dev, \"PCIe Link is DOWN\\n\");\n\n\t \n\tpcie_write(port, ~XILINX_CPM_PCIE_IDR_ALL_MASK,\n\t\t   XILINX_CPM_PCIE_REG_IMR);\n\n\t \n\tpcie_write(port, pcie_read(port, XILINX_CPM_PCIE_REG_IDR) &\n\t\t   XILINX_CPM_PCIE_IMR_ALL_MASK,\n\t\t   XILINX_CPM_PCIE_REG_IDR);\n\n\t \n\twritel(XILINX_CPM_PCIE_MISC_IR_LOCAL,\n\t       port->cpm_base + XILINX_CPM_PCIE_MISC_IR_ENABLE);\n\n\tif (port->variant->version == CPM5) {\n\t\twritel(XILINX_CPM_PCIE_IR_LOCAL,\n\t\t       port->cpm_base + XILINX_CPM_PCIE_IR_ENABLE);\n\t}\n\n\t \n\tpcie_write(port, pcie_read(port, XILINX_CPM_PCIE_REG_RPSC) |\n\t\t   XILINX_CPM_PCIE_REG_RPSC_BEN,\n\t\t   XILINX_CPM_PCIE_REG_RPSC);\n}\n\n \nstatic int xilinx_cpm_pcie_parse_dt(struct xilinx_cpm_pcie *port,\n\t\t\t\t    struct resource *bus_range)\n{\n\tstruct device *dev = port->dev;\n\tstruct platform_device *pdev = to_platform_device(dev);\n\tstruct resource *res;\n\n\tport->cpm_base = devm_platform_ioremap_resource_byname(pdev,\n\t\t\t\t\t\t\t       \"cpm_slcr\");\n\tif (IS_ERR(port->cpm_base))\n\t\treturn PTR_ERR(port->cpm_base);\n\n\tres = platform_get_resource_byname(pdev, IORESOURCE_MEM, \"cfg\");\n\tif (!res)\n\t\treturn -ENXIO;\n\n\tport->cfg = pci_ecam_create(dev, res, bus_range,\n\t\t\t\t    &pci_generic_ecam_ops);\n\tif (IS_ERR(port->cfg))\n\t\treturn PTR_ERR(port->cfg);\n\n\tif (port->variant->version == CPM5) {\n\t\tport->reg_base = devm_platform_ioremap_resource_byname(pdev,\n\t\t\t\t\t\t\t\t    \"cpm_csr\");\n\t\tif (IS_ERR(port->reg_base))\n\t\t\treturn PTR_ERR(port->reg_base);\n\t} else {\n\t\tport->reg_base = port->cfg->win;\n\t}\n\n\treturn 0;\n}\n\nstatic void xilinx_cpm_free_interrupts(struct xilinx_cpm_pcie *port)\n{\n\tirq_set_chained_handler_and_data(port->intx_irq, NULL, NULL);\n\tirq_set_chained_handler_and_data(port->irq, NULL, NULL);\n}\n\n \nstatic int xilinx_cpm_pcie_probe(struct platform_device *pdev)\n{\n\tstruct xilinx_cpm_pcie *port;\n\tstruct device *dev = &pdev->dev;\n\tstruct pci_host_bridge *bridge;\n\tstruct resource_entry *bus;\n\tint err;\n\n\tbridge = devm_pci_alloc_host_bridge(dev, sizeof(*port));\n\tif (!bridge)\n\t\treturn -ENODEV;\n\n\tport = pci_host_bridge_priv(bridge);\n\n\tport->dev = dev;\n\n\terr = xilinx_cpm_pcie_init_irq_domain(port);\n\tif (err)\n\t\treturn err;\n\n\tbus = resource_list_first_type(&bridge->windows, IORESOURCE_BUS);\n\tif (!bus)\n\t\treturn -ENODEV;\n\n\tport->variant = of_device_get_match_data(dev);\n\n\terr = xilinx_cpm_pcie_parse_dt(port, bus->res);\n\tif (err) {\n\t\tdev_err(dev, \"Parsing DT failed\\n\");\n\t\tgoto err_parse_dt;\n\t}\n\n\txilinx_cpm_pcie_init_port(port);\n\n\terr = xilinx_cpm_setup_irq(port);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to set up interrupts\\n\");\n\t\tgoto err_setup_irq;\n\t}\n\n\tbridge->sysdata = port->cfg;\n\tbridge->ops = (struct pci_ops *)&pci_generic_ecam_ops.pci_ops;\n\n\terr = pci_host_probe(bridge);\n\tif (err < 0)\n\t\tgoto err_host_bridge;\n\n\treturn 0;\n\nerr_host_bridge:\n\txilinx_cpm_free_interrupts(port);\nerr_setup_irq:\n\tpci_ecam_free(port->cfg);\nerr_parse_dt:\n\txilinx_cpm_free_irq_domains(port);\n\treturn err;\n}\n\nstatic const struct xilinx_cpm_variant cpm_host = {\n\t.version = CPM,\n};\n\nstatic const struct xilinx_cpm_variant cpm5_host = {\n\t.version = CPM5,\n};\n\nstatic const struct of_device_id xilinx_cpm_pcie_of_match[] = {\n\t{\n\t\t.compatible = \"xlnx,versal-cpm-host-1.00\",\n\t\t.data = &cpm_host,\n\t},\n\t{\n\t\t.compatible = \"xlnx,versal-cpm5-host\",\n\t\t.data = &cpm5_host,\n\t},\n\t{}\n};\n\nstatic struct platform_driver xilinx_cpm_pcie_driver = {\n\t.driver = {\n\t\t.name = \"xilinx-cpm-pcie\",\n\t\t.of_match_table = xilinx_cpm_pcie_of_match,\n\t\t.suppress_bind_attrs = true,\n\t},\n\t.probe = xilinx_cpm_pcie_probe,\n};\n\nbuiltin_platform_driver(xilinx_cpm_pcie_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}