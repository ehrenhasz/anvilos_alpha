{
  "module_name": "p2pdma.c",
  "hash_id": "d1469fe4cbacdb0889454d434b242b7cf7d5d21795548c6135c65c7f8942bc82",
  "original_prompt": "Ingested from linux-6.6.14/drivers/pci/p2pdma.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"pci-p2pdma: \" fmt\n#include <linux/ctype.h>\n#include <linux/dma-map-ops.h>\n#include <linux/pci-p2pdma.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/genalloc.h>\n#include <linux/memremap.h>\n#include <linux/percpu-refcount.h>\n#include <linux/random.h>\n#include <linux/seq_buf.h>\n#include <linux/xarray.h>\n\nstruct pci_p2pdma {\n\tstruct gen_pool *pool;\n\tbool p2pmem_published;\n\tstruct xarray map_types;\n};\n\nstruct pci_p2pdma_pagemap {\n\tstruct dev_pagemap pgmap;\n\tstruct pci_dev *provider;\n\tu64 bus_offset;\n};\n\nstatic struct pci_p2pdma_pagemap *to_p2p_pgmap(struct dev_pagemap *pgmap)\n{\n\treturn container_of(pgmap, struct pci_p2pdma_pagemap, pgmap);\n}\n\nstatic ssize_t size_show(struct device *dev, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct pci_p2pdma *p2pdma;\n\tsize_t size = 0;\n\n\trcu_read_lock();\n\tp2pdma = rcu_dereference(pdev->p2pdma);\n\tif (p2pdma && p2pdma->pool)\n\t\tsize = gen_pool_size(p2pdma->pool);\n\trcu_read_unlock();\n\n\treturn sysfs_emit(buf, \"%zd\\n\", size);\n}\nstatic DEVICE_ATTR_RO(size);\n\nstatic ssize_t available_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct pci_p2pdma *p2pdma;\n\tsize_t avail = 0;\n\n\trcu_read_lock();\n\tp2pdma = rcu_dereference(pdev->p2pdma);\n\tif (p2pdma && p2pdma->pool)\n\t\tavail = gen_pool_avail(p2pdma->pool);\n\trcu_read_unlock();\n\n\treturn sysfs_emit(buf, \"%zd\\n\", avail);\n}\nstatic DEVICE_ATTR_RO(available);\n\nstatic ssize_t published_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct pci_p2pdma *p2pdma;\n\tbool published = false;\n\n\trcu_read_lock();\n\tp2pdma = rcu_dereference(pdev->p2pdma);\n\tif (p2pdma)\n\t\tpublished = p2pdma->p2pmem_published;\n\trcu_read_unlock();\n\n\treturn sysfs_emit(buf, \"%d\\n\", published);\n}\nstatic DEVICE_ATTR_RO(published);\n\nstatic int p2pmem_alloc_mmap(struct file *filp, struct kobject *kobj,\n\t\tstruct bin_attribute *attr, struct vm_area_struct *vma)\n{\n\tstruct pci_dev *pdev = to_pci_dev(kobj_to_dev(kobj));\n\tsize_t len = vma->vm_end - vma->vm_start;\n\tstruct pci_p2pdma *p2pdma;\n\tstruct percpu_ref *ref;\n\tunsigned long vaddr;\n\tvoid *kaddr;\n\tint ret;\n\n\t \n\tif ((vma->vm_flags & VM_MAYSHARE) != VM_MAYSHARE) {\n\t\tpci_info_ratelimited(pdev,\n\t\t\t\t     \"%s: fail, attempted private mapping\\n\",\n\t\t\t\t     current->comm);\n\t\treturn -EINVAL;\n\t}\n\n\tif (vma->vm_pgoff) {\n\t\tpci_info_ratelimited(pdev,\n\t\t\t\t     \"%s: fail, attempted mapping with non-zero offset\\n\",\n\t\t\t\t     current->comm);\n\t\treturn -EINVAL;\n\t}\n\n\trcu_read_lock();\n\tp2pdma = rcu_dereference(pdev->p2pdma);\n\tif (!p2pdma) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tkaddr = (void *)gen_pool_alloc_owner(p2pdma->pool, len, (void **)&ref);\n\tif (!kaddr) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (unlikely(!percpu_ref_tryget_live_rcu(ref))) {\n\t\tret = -ENODEV;\n\t\tgoto out_free_mem;\n\t}\n\trcu_read_unlock();\n\n\tfor (vaddr = vma->vm_start; vaddr < vma->vm_end; vaddr += PAGE_SIZE) {\n\t\tret = vm_insert_page(vma, vaddr, virt_to_page(kaddr));\n\t\tif (ret) {\n\t\t\tgen_pool_free(p2pdma->pool, (uintptr_t)kaddr, len);\n\t\t\treturn ret;\n\t\t}\n\t\tpercpu_ref_get(ref);\n\t\tput_page(virt_to_page(kaddr));\n\t\tkaddr += PAGE_SIZE;\n\t\tlen -= PAGE_SIZE;\n\t}\n\n\tpercpu_ref_put(ref);\n\n\treturn 0;\nout_free_mem:\n\tgen_pool_free(p2pdma->pool, (uintptr_t)kaddr, len);\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic struct bin_attribute p2pmem_alloc_attr = {\n\t.attr = { .name = \"allocate\", .mode = 0660 },\n\t.mmap = p2pmem_alloc_mmap,\n\t \n\t.size = SZ_1T,\n};\n\nstatic struct attribute *p2pmem_attrs[] = {\n\t&dev_attr_size.attr,\n\t&dev_attr_available.attr,\n\t&dev_attr_published.attr,\n\tNULL,\n};\n\nstatic struct bin_attribute *p2pmem_bin_attrs[] = {\n\t&p2pmem_alloc_attr,\n\tNULL,\n};\n\nstatic const struct attribute_group p2pmem_group = {\n\t.attrs = p2pmem_attrs,\n\t.bin_attrs = p2pmem_bin_attrs,\n\t.name = \"p2pmem\",\n};\n\nstatic void p2pdma_page_free(struct page *page)\n{\n\tstruct pci_p2pdma_pagemap *pgmap = to_p2p_pgmap(page->pgmap);\n\t \n\tstruct pci_p2pdma *p2pdma =\n\t\trcu_dereference_protected(pgmap->provider->p2pdma, 1);\n\tstruct percpu_ref *ref;\n\n\tgen_pool_free_owner(p2pdma->pool, (uintptr_t)page_to_virt(page),\n\t\t\t    PAGE_SIZE, (void **)&ref);\n\tpercpu_ref_put(ref);\n}\n\nstatic const struct dev_pagemap_ops p2pdma_pgmap_ops = {\n\t.page_free = p2pdma_page_free,\n};\n\nstatic void pci_p2pdma_release(void *data)\n{\n\tstruct pci_dev *pdev = data;\n\tstruct pci_p2pdma *p2pdma;\n\n\tp2pdma = rcu_dereference_protected(pdev->p2pdma, 1);\n\tif (!p2pdma)\n\t\treturn;\n\n\t \n\tpdev->p2pdma = NULL;\n\tsynchronize_rcu();\n\n\tgen_pool_destroy(p2pdma->pool);\n\tsysfs_remove_group(&pdev->dev.kobj, &p2pmem_group);\n\txa_destroy(&p2pdma->map_types);\n}\n\nstatic int pci_p2pdma_setup(struct pci_dev *pdev)\n{\n\tint error = -ENOMEM;\n\tstruct pci_p2pdma *p2p;\n\n\tp2p = devm_kzalloc(&pdev->dev, sizeof(*p2p), GFP_KERNEL);\n\tif (!p2p)\n\t\treturn -ENOMEM;\n\n\txa_init(&p2p->map_types);\n\n\tp2p->pool = gen_pool_create(PAGE_SHIFT, dev_to_node(&pdev->dev));\n\tif (!p2p->pool)\n\t\tgoto out;\n\n\terror = devm_add_action_or_reset(&pdev->dev, pci_p2pdma_release, pdev);\n\tif (error)\n\t\tgoto out_pool_destroy;\n\n\terror = sysfs_create_group(&pdev->dev.kobj, &p2pmem_group);\n\tif (error)\n\t\tgoto out_pool_destroy;\n\n\trcu_assign_pointer(pdev->p2pdma, p2p);\n\treturn 0;\n\nout_pool_destroy:\n\tgen_pool_destroy(p2p->pool);\nout:\n\tdevm_kfree(&pdev->dev, p2p);\n\treturn error;\n}\n\nstatic void pci_p2pdma_unmap_mappings(void *data)\n{\n\tstruct pci_dev *pdev = data;\n\n\t \n\tsysfs_remove_file_from_group(&pdev->dev.kobj, &p2pmem_alloc_attr.attr,\n\t\t\t\t     p2pmem_group.name);\n}\n\n \nint pci_p2pdma_add_resource(struct pci_dev *pdev, int bar, size_t size,\n\t\t\t    u64 offset)\n{\n\tstruct pci_p2pdma_pagemap *p2p_pgmap;\n\tstruct dev_pagemap *pgmap;\n\tstruct pci_p2pdma *p2pdma;\n\tvoid *addr;\n\tint error;\n\n\tif (!(pci_resource_flags(pdev, bar) & IORESOURCE_MEM))\n\t\treturn -EINVAL;\n\n\tif (offset >= pci_resource_len(pdev, bar))\n\t\treturn -EINVAL;\n\n\tif (!size)\n\t\tsize = pci_resource_len(pdev, bar) - offset;\n\n\tif (size + offset > pci_resource_len(pdev, bar))\n\t\treturn -EINVAL;\n\n\tif (!pdev->p2pdma) {\n\t\terror = pci_p2pdma_setup(pdev);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\tp2p_pgmap = devm_kzalloc(&pdev->dev, sizeof(*p2p_pgmap), GFP_KERNEL);\n\tif (!p2p_pgmap)\n\t\treturn -ENOMEM;\n\n\tpgmap = &p2p_pgmap->pgmap;\n\tpgmap->range.start = pci_resource_start(pdev, bar) + offset;\n\tpgmap->range.end = pgmap->range.start + size - 1;\n\tpgmap->nr_range = 1;\n\tpgmap->type = MEMORY_DEVICE_PCI_P2PDMA;\n\tpgmap->ops = &p2pdma_pgmap_ops;\n\n\tp2p_pgmap->provider = pdev;\n\tp2p_pgmap->bus_offset = pci_bus_address(pdev, bar) -\n\t\tpci_resource_start(pdev, bar);\n\n\taddr = devm_memremap_pages(&pdev->dev, pgmap);\n\tif (IS_ERR(addr)) {\n\t\terror = PTR_ERR(addr);\n\t\tgoto pgmap_free;\n\t}\n\n\terror = devm_add_action_or_reset(&pdev->dev, pci_p2pdma_unmap_mappings,\n\t\t\t\t\t pdev);\n\tif (error)\n\t\tgoto pages_free;\n\n\tp2pdma = rcu_dereference_protected(pdev->p2pdma, 1);\n\terror = gen_pool_add_owner(p2pdma->pool, (unsigned long)addr,\n\t\t\tpci_bus_address(pdev, bar) + offset,\n\t\t\trange_len(&pgmap->range), dev_to_node(&pdev->dev),\n\t\t\t&pgmap->ref);\n\tif (error)\n\t\tgoto pages_free;\n\n\tpci_info(pdev, \"added peer-to-peer DMA memory %#llx-%#llx\\n\",\n\t\t pgmap->range.start, pgmap->range.end);\n\n\treturn 0;\n\npages_free:\n\tdevm_memunmap_pages(&pdev->dev, pgmap);\npgmap_free:\n\tdevm_kfree(&pdev->dev, pgmap);\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(pci_p2pdma_add_resource);\n\n \nstatic struct pci_dev *find_parent_pci_dev(struct device *dev)\n{\n\tstruct device *parent;\n\n\tdev = get_device(dev);\n\n\twhile (dev) {\n\t\tif (dev_is_pci(dev))\n\t\t\treturn to_pci_dev(dev);\n\n\t\tparent = get_device(dev->parent);\n\t\tput_device(dev);\n\t\tdev = parent;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic int pci_bridge_has_acs_redir(struct pci_dev *pdev)\n{\n\tint pos;\n\tu16 ctrl;\n\n\tpos = pdev->acs_cap;\n\tif (!pos)\n\t\treturn 0;\n\n\tpci_read_config_word(pdev, pos + PCI_ACS_CTRL, &ctrl);\n\n\tif (ctrl & (PCI_ACS_RR | PCI_ACS_CR | PCI_ACS_EC))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void seq_buf_print_bus_devfn(struct seq_buf *buf, struct pci_dev *pdev)\n{\n\tif (!buf)\n\t\treturn;\n\n\tseq_buf_printf(buf, \"%s;\", pci_name(pdev));\n}\n\nstatic bool cpu_supports_p2pdma(void)\n{\n#ifdef CONFIG_X86\n\tstruct cpuinfo_x86 *c = &cpu_data(0);\n\n\t \n\tif (c->x86_vendor == X86_VENDOR_AMD && c->x86 >= 0x17)\n\t\treturn true;\n#endif\n\n\treturn false;\n}\n\nstatic const struct pci_p2pdma_whitelist_entry {\n\tunsigned short vendor;\n\tunsigned short device;\n\tenum {\n\t\tREQ_SAME_HOST_BRIDGE\t= 1 << 0,\n\t} flags;\n} pci_p2pdma_whitelist[] = {\n\t \n\t{PCI_VENDOR_ID_INTEL,\t0x3c00, REQ_SAME_HOST_BRIDGE},\n\t{PCI_VENDOR_ID_INTEL,\t0x3c01, REQ_SAME_HOST_BRIDGE},\n\t \n\t{PCI_VENDOR_ID_INTEL,\t0x2f00, REQ_SAME_HOST_BRIDGE},\n\t{PCI_VENDOR_ID_INTEL,\t0x2f01, REQ_SAME_HOST_BRIDGE},\n\t \n\t{PCI_VENDOR_ID_INTEL,\t0x2030, 0},\n\t{PCI_VENDOR_ID_INTEL,\t0x2031, 0},\n\t{PCI_VENDOR_ID_INTEL,\t0x2032, 0},\n\t{PCI_VENDOR_ID_INTEL,\t0x2033, 0},\n\t{PCI_VENDOR_ID_INTEL,\t0x2020, 0},\n\t{PCI_VENDOR_ID_INTEL,\t0x09a2, 0},\n\t{}\n};\n\n \nstatic struct pci_dev *pci_host_bridge_dev(struct pci_host_bridge *host)\n{\n\tstruct pci_dev *root;\n\n\troot = list_first_entry_or_null(&host->bus->devices,\n\t\t\t\t\tstruct pci_dev, bus_list);\n\n\tif (!root)\n\t\treturn NULL;\n\n\tif (root->devfn == PCI_DEVFN(0, 0))\n\t\treturn root;\n\n\tif (pci_pcie_type(root) == PCI_EXP_TYPE_ROOT_PORT)\n\t\treturn root;\n\n\treturn NULL;\n}\n\nstatic bool __host_bridge_whitelist(struct pci_host_bridge *host,\n\t\t\t\t    bool same_host_bridge, bool warn)\n{\n\tstruct pci_dev *root = pci_host_bridge_dev(host);\n\tconst struct pci_p2pdma_whitelist_entry *entry;\n\tunsigned short vendor, device;\n\n\tif (!root)\n\t\treturn false;\n\n\tvendor = root->vendor;\n\tdevice = root->device;\n\n\tfor (entry = pci_p2pdma_whitelist; entry->vendor; entry++) {\n\t\tif (vendor != entry->vendor || device != entry->device)\n\t\t\tcontinue;\n\t\tif (entry->flags & REQ_SAME_HOST_BRIDGE && !same_host_bridge)\n\t\t\treturn false;\n\n\t\treturn true;\n\t}\n\n\tif (warn)\n\t\tpci_warn(root, \"Host bridge not in P2PDMA whitelist: %04x:%04x\\n\",\n\t\t\t vendor, device);\n\n\treturn false;\n}\n\n \nstatic bool host_bridge_whitelist(struct pci_dev *a, struct pci_dev *b,\n\t\t\t\t  bool warn)\n{\n\tstruct pci_host_bridge *host_a = pci_find_host_bridge(a->bus);\n\tstruct pci_host_bridge *host_b = pci_find_host_bridge(b->bus);\n\n\tif (host_a == host_b)\n\t\treturn __host_bridge_whitelist(host_a, true, warn);\n\n\tif (__host_bridge_whitelist(host_a, false, warn) &&\n\t    __host_bridge_whitelist(host_b, false, warn))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic unsigned long map_types_idx(struct pci_dev *client)\n{\n\treturn (pci_domain_nr(client->bus) << 16) | pci_dev_id(client);\n}\n\n \nstatic enum pci_p2pdma_map_type\ncalc_map_type_and_dist(struct pci_dev *provider, struct pci_dev *client,\n\t\tint *dist, bool verbose)\n{\n\tenum pci_p2pdma_map_type map_type = PCI_P2PDMA_MAP_THRU_HOST_BRIDGE;\n\tstruct pci_dev *a = provider, *b = client, *bb;\n\tbool acs_redirects = false;\n\tstruct pci_p2pdma *p2pdma;\n\tstruct seq_buf acs_list;\n\tint acs_cnt = 0;\n\tint dist_a = 0;\n\tint dist_b = 0;\n\tchar buf[128];\n\n\tseq_buf_init(&acs_list, buf, sizeof(buf));\n\n\t \n\twhile (a) {\n\t\tdist_b = 0;\n\n\t\tif (pci_bridge_has_acs_redir(a)) {\n\t\t\tseq_buf_print_bus_devfn(&acs_list, a);\n\t\t\tacs_cnt++;\n\t\t}\n\n\t\tbb = b;\n\n\t\twhile (bb) {\n\t\t\tif (a == bb)\n\t\t\t\tgoto check_b_path_acs;\n\n\t\t\tbb = pci_upstream_bridge(bb);\n\t\t\tdist_b++;\n\t\t}\n\n\t\ta = pci_upstream_bridge(a);\n\t\tdist_a++;\n\t}\n\n\t*dist = dist_a + dist_b;\n\tgoto map_through_host_bridge;\n\ncheck_b_path_acs:\n\tbb = b;\n\n\twhile (bb) {\n\t\tif (a == bb)\n\t\t\tbreak;\n\n\t\tif (pci_bridge_has_acs_redir(bb)) {\n\t\t\tseq_buf_print_bus_devfn(&acs_list, bb);\n\t\t\tacs_cnt++;\n\t\t}\n\n\t\tbb = pci_upstream_bridge(bb);\n\t}\n\n\t*dist = dist_a + dist_b;\n\n\tif (!acs_cnt) {\n\t\tmap_type = PCI_P2PDMA_MAP_BUS_ADDR;\n\t\tgoto done;\n\t}\n\n\tif (verbose) {\n\t\tacs_list.buffer[acs_list.len-1] = 0;  \n\t\tpci_warn(client, \"ACS redirect is set between the client and provider (%s)\\n\",\n\t\t\t pci_name(provider));\n\t\tpci_warn(client, \"to disable ACS redirect for this path, add the kernel parameter: pci=disable_acs_redir=%s\\n\",\n\t\t\t acs_list.buffer);\n\t}\n\tacs_redirects = true;\n\nmap_through_host_bridge:\n\tif (!cpu_supports_p2pdma() &&\n\t    !host_bridge_whitelist(provider, client, acs_redirects)) {\n\t\tif (verbose)\n\t\t\tpci_warn(client, \"cannot be used for peer-to-peer DMA as the client and provider (%s) do not share an upstream bridge or whitelisted host bridge\\n\",\n\t\t\t\t pci_name(provider));\n\t\tmap_type = PCI_P2PDMA_MAP_NOT_SUPPORTED;\n\t}\ndone:\n\trcu_read_lock();\n\tp2pdma = rcu_dereference(provider->p2pdma);\n\tif (p2pdma)\n\t\txa_store(&p2pdma->map_types, map_types_idx(client),\n\t\t\t xa_mk_value(map_type), GFP_KERNEL);\n\trcu_read_unlock();\n\treturn map_type;\n}\n\n \nint pci_p2pdma_distance_many(struct pci_dev *provider, struct device **clients,\n\t\t\t     int num_clients, bool verbose)\n{\n\tenum pci_p2pdma_map_type map;\n\tbool not_supported = false;\n\tstruct pci_dev *pci_client;\n\tint total_dist = 0;\n\tint i, distance;\n\n\tif (num_clients == 0)\n\t\treturn -1;\n\n\tfor (i = 0; i < num_clients; i++) {\n\t\tpci_client = find_parent_pci_dev(clients[i]);\n\t\tif (!pci_client) {\n\t\t\tif (verbose)\n\t\t\t\tdev_warn(clients[i],\n\t\t\t\t\t \"cannot be used for peer-to-peer DMA as it is not a PCI device\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tmap = calc_map_type_and_dist(provider, pci_client, &distance,\n\t\t\t\t\t     verbose);\n\n\t\tpci_dev_put(pci_client);\n\n\t\tif (map == PCI_P2PDMA_MAP_NOT_SUPPORTED)\n\t\t\tnot_supported = true;\n\n\t\tif (not_supported && !verbose)\n\t\t\tbreak;\n\n\t\ttotal_dist += distance;\n\t}\n\n\tif (not_supported)\n\t\treturn -1;\n\n\treturn total_dist;\n}\nEXPORT_SYMBOL_GPL(pci_p2pdma_distance_many);\n\n \nbool pci_has_p2pmem(struct pci_dev *pdev)\n{\n\tstruct pci_p2pdma *p2pdma;\n\tbool res;\n\n\trcu_read_lock();\n\tp2pdma = rcu_dereference(pdev->p2pdma);\n\tres = p2pdma && p2pdma->p2pmem_published;\n\trcu_read_unlock();\n\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(pci_has_p2pmem);\n\n \nstruct pci_dev *pci_p2pmem_find_many(struct device **clients, int num_clients)\n{\n\tstruct pci_dev *pdev = NULL;\n\tint distance;\n\tint closest_distance = INT_MAX;\n\tstruct pci_dev **closest_pdevs;\n\tint dev_cnt = 0;\n\tconst int max_devs = PAGE_SIZE / sizeof(*closest_pdevs);\n\tint i;\n\n\tclosest_pdevs = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!closest_pdevs)\n\t\treturn NULL;\n\n\tfor_each_pci_dev(pdev) {\n\t\tif (!pci_has_p2pmem(pdev))\n\t\t\tcontinue;\n\n\t\tdistance = pci_p2pdma_distance_many(pdev, clients,\n\t\t\t\t\t\t    num_clients, false);\n\t\tif (distance < 0 || distance > closest_distance)\n\t\t\tcontinue;\n\n\t\tif (distance == closest_distance && dev_cnt >= max_devs)\n\t\t\tcontinue;\n\n\t\tif (distance < closest_distance) {\n\t\t\tfor (i = 0; i < dev_cnt; i++)\n\t\t\t\tpci_dev_put(closest_pdevs[i]);\n\n\t\t\tdev_cnt = 0;\n\t\t\tclosest_distance = distance;\n\t\t}\n\n\t\tclosest_pdevs[dev_cnt++] = pci_dev_get(pdev);\n\t}\n\n\tif (dev_cnt)\n\t\tpdev = pci_dev_get(closest_pdevs[get_random_u32_below(dev_cnt)]);\n\n\tfor (i = 0; i < dev_cnt; i++)\n\t\tpci_dev_put(closest_pdevs[i]);\n\n\tkfree(closest_pdevs);\n\treturn pdev;\n}\nEXPORT_SYMBOL_GPL(pci_p2pmem_find_many);\n\n \nvoid *pci_alloc_p2pmem(struct pci_dev *pdev, size_t size)\n{\n\tvoid *ret = NULL;\n\tstruct percpu_ref *ref;\n\tstruct pci_p2pdma *p2pdma;\n\n\t \n\trcu_read_lock();\n\tp2pdma = rcu_dereference(pdev->p2pdma);\n\tif (unlikely(!p2pdma))\n\t\tgoto out;\n\n\tret = (void *)gen_pool_alloc_owner(p2pdma->pool, size, (void **) &ref);\n\tif (!ret)\n\t\tgoto out;\n\n\tif (unlikely(!percpu_ref_tryget_live_rcu(ref))) {\n\t\tgen_pool_free(p2pdma->pool, (unsigned long) ret, size);\n\t\tret = NULL;\n\t\tgoto out;\n\t}\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(pci_alloc_p2pmem);\n\n \nvoid pci_free_p2pmem(struct pci_dev *pdev, void *addr, size_t size)\n{\n\tstruct percpu_ref *ref;\n\tstruct pci_p2pdma *p2pdma = rcu_dereference_protected(pdev->p2pdma, 1);\n\n\tgen_pool_free_owner(p2pdma->pool, (uintptr_t)addr, size,\n\t\t\t(void **) &ref);\n\tpercpu_ref_put(ref);\n}\nEXPORT_SYMBOL_GPL(pci_free_p2pmem);\n\n \npci_bus_addr_t pci_p2pmem_virt_to_bus(struct pci_dev *pdev, void *addr)\n{\n\tstruct pci_p2pdma *p2pdma;\n\n\tif (!addr)\n\t\treturn 0;\n\n\tp2pdma = rcu_dereference_protected(pdev->p2pdma, 1);\n\tif (!p2pdma)\n\t\treturn 0;\n\n\t \n\treturn gen_pool_virt_to_phys(p2pdma->pool, (unsigned long)addr);\n}\nEXPORT_SYMBOL_GPL(pci_p2pmem_virt_to_bus);\n\n \nstruct scatterlist *pci_p2pmem_alloc_sgl(struct pci_dev *pdev,\n\t\t\t\t\t unsigned int *nents, u32 length)\n{\n\tstruct scatterlist *sg;\n\tvoid *addr;\n\n\tsg = kmalloc(sizeof(*sg), GFP_KERNEL);\n\tif (!sg)\n\t\treturn NULL;\n\n\tsg_init_table(sg, 1);\n\n\taddr = pci_alloc_p2pmem(pdev, length);\n\tif (!addr)\n\t\tgoto out_free_sg;\n\n\tsg_set_buf(sg, addr, length);\n\t*nents = 1;\n\treturn sg;\n\nout_free_sg:\n\tkfree(sg);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(pci_p2pmem_alloc_sgl);\n\n \nvoid pci_p2pmem_free_sgl(struct pci_dev *pdev, struct scatterlist *sgl)\n{\n\tstruct scatterlist *sg;\n\tint count;\n\n\tfor_each_sg(sgl, sg, INT_MAX, count) {\n\t\tif (!sg)\n\t\t\tbreak;\n\n\t\tpci_free_p2pmem(pdev, sg_virt(sg), sg->length);\n\t}\n\tkfree(sgl);\n}\nEXPORT_SYMBOL_GPL(pci_p2pmem_free_sgl);\n\n \nvoid pci_p2pmem_publish(struct pci_dev *pdev, bool publish)\n{\n\tstruct pci_p2pdma *p2pdma;\n\n\trcu_read_lock();\n\tp2pdma = rcu_dereference(pdev->p2pdma);\n\tif (p2pdma)\n\t\tp2pdma->p2pmem_published = publish;\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(pci_p2pmem_publish);\n\nstatic enum pci_p2pdma_map_type pci_p2pdma_map_type(struct dev_pagemap *pgmap,\n\t\t\t\t\t\t    struct device *dev)\n{\n\tenum pci_p2pdma_map_type type = PCI_P2PDMA_MAP_NOT_SUPPORTED;\n\tstruct pci_dev *provider = to_p2p_pgmap(pgmap)->provider;\n\tstruct pci_dev *client;\n\tstruct pci_p2pdma *p2pdma;\n\tint dist;\n\n\tif (!provider->p2pdma)\n\t\treturn PCI_P2PDMA_MAP_NOT_SUPPORTED;\n\n\tif (!dev_is_pci(dev))\n\t\treturn PCI_P2PDMA_MAP_NOT_SUPPORTED;\n\n\tclient = to_pci_dev(dev);\n\n\trcu_read_lock();\n\tp2pdma = rcu_dereference(provider->p2pdma);\n\n\tif (p2pdma)\n\t\ttype = xa_to_value(xa_load(&p2pdma->map_types,\n\t\t\t\t\t   map_types_idx(client)));\n\trcu_read_unlock();\n\n\tif (type == PCI_P2PDMA_MAP_UNKNOWN)\n\t\treturn calc_map_type_and_dist(provider, client, &dist, true);\n\n\treturn type;\n}\n\n \nenum pci_p2pdma_map_type\npci_p2pdma_map_segment(struct pci_p2pdma_map_state *state, struct device *dev,\n\t\t       struct scatterlist *sg)\n{\n\tif (state->pgmap != sg_page(sg)->pgmap) {\n\t\tstate->pgmap = sg_page(sg)->pgmap;\n\t\tstate->map = pci_p2pdma_map_type(state->pgmap, dev);\n\t\tstate->bus_off = to_p2p_pgmap(state->pgmap)->bus_offset;\n\t}\n\n\tif (state->map == PCI_P2PDMA_MAP_BUS_ADDR) {\n\t\tsg->dma_address = sg_phys(sg) + state->bus_off;\n\t\tsg_dma_len(sg) = sg->length;\n\t\tsg_dma_mark_bus_address(sg);\n\t}\n\n\treturn state->map;\n}\n\n \nint pci_p2pdma_enable_store(const char *page, struct pci_dev **p2p_dev,\n\t\t\t    bool *use_p2pdma)\n{\n\tstruct device *dev;\n\n\tdev = bus_find_device_by_name(&pci_bus_type, NULL, page);\n\tif (dev) {\n\t\t*use_p2pdma = true;\n\t\t*p2p_dev = to_pci_dev(dev);\n\n\t\tif (!pci_has_p2pmem(*p2p_dev)) {\n\t\t\tpci_err(*p2p_dev,\n\t\t\t\t\"PCI device has no peer-to-peer memory: %s\\n\",\n\t\t\t\tpage);\n\t\t\tpci_dev_put(*p2p_dev);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\treturn 0;\n\t} else if ((page[0] == '0' || page[0] == '1') && !iscntrl(page[1])) {\n\t\t \n\t} else if (!kstrtobool(page, use_p2pdma)) {\n\t\treturn 0;\n\t}\n\n\tpr_err(\"No such PCI device: %.*s\\n\", (int)strcspn(page, \"\\n\"), page);\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL_GPL(pci_p2pdma_enable_store);\n\n \nssize_t pci_p2pdma_enable_show(char *page, struct pci_dev *p2p_dev,\n\t\t\t       bool use_p2pdma)\n{\n\tif (!use_p2pdma)\n\t\treturn sprintf(page, \"0\\n\");\n\n\tif (!p2p_dev)\n\t\treturn sprintf(page, \"1\\n\");\n\n\treturn sprintf(page, \"%s\\n\", pci_name(p2p_dev));\n}\nEXPORT_SYMBOL_GPL(pci_p2pdma_enable_show);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}