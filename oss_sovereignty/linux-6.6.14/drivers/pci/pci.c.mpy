{
  "module_name": "pci.c",
  "hash_id": "2fe8d998a9a838fbdac13ccfa5e61df158bf0f6f959342206017ddb5ae12828d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/pci/pci.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/kernel.h>\n#include <linux/delay.h>\n#include <linux/dmi.h>\n#include <linux/init.h>\n#include <linux/msi.h>\n#include <linux/of.h>\n#include <linux/pci.h>\n#include <linux/pm.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/log2.h>\n#include <linux/logic_pio.h>\n#include <linux/pm_wakeup.h>\n#include <linux/interrupt.h>\n#include <linux/device.h>\n#include <linux/pm_runtime.h>\n#include <linux/pci_hotplug.h>\n#include <linux/vmalloc.h>\n#include <asm/dma.h>\n#include <linux/aer.h>\n#include <linux/bitfield.h>\n#include \"pci.h\"\n\nDEFINE_MUTEX(pci_slot_mutex);\n\nconst char *pci_power_names[] = {\n\t\"error\", \"D0\", \"D1\", \"D2\", \"D3hot\", \"D3cold\", \"unknown\",\n};\nEXPORT_SYMBOL_GPL(pci_power_names);\n\n#ifdef CONFIG_X86_32\nint isa_dma_bridge_buggy;\nEXPORT_SYMBOL(isa_dma_bridge_buggy);\n#endif\n\nint pci_pci_problems;\nEXPORT_SYMBOL(pci_pci_problems);\n\nunsigned int pci_pm_d3hot_delay;\n\nstatic void pci_pme_list_scan(struct work_struct *work);\n\nstatic LIST_HEAD(pci_pme_list);\nstatic DEFINE_MUTEX(pci_pme_list_mutex);\nstatic DECLARE_DELAYED_WORK(pci_pme_work, pci_pme_list_scan);\n\nstruct pci_pme_device {\n\tstruct list_head list;\n\tstruct pci_dev *dev;\n};\n\n#define PME_TIMEOUT 1000  \n\n \n#define PCI_RESET_WAIT 1000  \n\n \n#define PCIE_RESET_READY_POLL_MS 60000  \n\nstatic void pci_dev_d3_sleep(struct pci_dev *dev)\n{\n\tunsigned int delay_ms = max(dev->d3hot_delay, pci_pm_d3hot_delay);\n\tunsigned int upper;\n\n\tif (delay_ms) {\n\t\t \n\t\tupper = max(DIV_ROUND_CLOSEST(delay_ms, 5), 1U);\n\t\tusleep_range(delay_ms * USEC_PER_MSEC,\n\t\t\t     (delay_ms + upper) * USEC_PER_MSEC);\n\t}\n}\n\nbool pci_reset_supported(struct pci_dev *dev)\n{\n\treturn dev->reset_methods[0] != 0;\n}\n\n#ifdef CONFIG_PCI_DOMAINS\nint pci_domains_supported = 1;\n#endif\n\n#define DEFAULT_CARDBUS_IO_SIZE\t\t(256)\n#define DEFAULT_CARDBUS_MEM_SIZE\t(64*1024*1024)\n \nunsigned long pci_cardbus_io_size = DEFAULT_CARDBUS_IO_SIZE;\nunsigned long pci_cardbus_mem_size = DEFAULT_CARDBUS_MEM_SIZE;\n\n#define DEFAULT_HOTPLUG_IO_SIZE\t\t(256)\n#define DEFAULT_HOTPLUG_MMIO_SIZE\t(2*1024*1024)\n#define DEFAULT_HOTPLUG_MMIO_PREF_SIZE\t(2*1024*1024)\n \nunsigned long pci_hotplug_io_size  = DEFAULT_HOTPLUG_IO_SIZE;\n \nunsigned long pci_hotplug_mmio_size = DEFAULT_HOTPLUG_MMIO_SIZE;\nunsigned long pci_hotplug_mmio_pref_size = DEFAULT_HOTPLUG_MMIO_PREF_SIZE;\n\n#define DEFAULT_HOTPLUG_BUS_SIZE\t1\nunsigned long pci_hotplug_bus_size = DEFAULT_HOTPLUG_BUS_SIZE;\n\n\n \n#ifdef CONFIG_PCIE_BUS_TUNE_OFF\nenum pcie_bus_config_types pcie_bus_config = PCIE_BUS_TUNE_OFF;\n#elif defined CONFIG_PCIE_BUS_SAFE\nenum pcie_bus_config_types pcie_bus_config = PCIE_BUS_SAFE;\n#elif defined CONFIG_PCIE_BUS_PERFORMANCE\nenum pcie_bus_config_types pcie_bus_config = PCIE_BUS_PERFORMANCE;\n#elif defined CONFIG_PCIE_BUS_PEER2PEER\nenum pcie_bus_config_types pcie_bus_config = PCIE_BUS_PEER2PEER;\n#else\nenum pcie_bus_config_types pcie_bus_config = PCIE_BUS_DEFAULT;\n#endif\n\n \nu8 pci_dfl_cache_line_size = L1_CACHE_BYTES >> 2;\nu8 pci_cache_line_size;\n\n \nunsigned int pcibios_max_latency = 255;\n\n \nstatic bool pcie_ari_disabled;\n\n \nstatic bool pcie_ats_disabled;\n\n \nbool pci_early_dump;\n\nbool pci_ats_disabled(void)\n{\n\treturn pcie_ats_disabled;\n}\nEXPORT_SYMBOL_GPL(pci_ats_disabled);\n\n \nstatic bool pci_bridge_d3_disable;\n \nstatic bool pci_bridge_d3_force;\n\nstatic int __init pcie_port_pm_setup(char *str)\n{\n\tif (!strcmp(str, \"off\"))\n\t\tpci_bridge_d3_disable = true;\n\telse if (!strcmp(str, \"force\"))\n\t\tpci_bridge_d3_force = true;\n\treturn 1;\n}\n__setup(\"pcie_port_pm=\", pcie_port_pm_setup);\n\n \nunsigned char pci_bus_max_busnr(struct pci_bus *bus)\n{\n\tstruct pci_bus *tmp;\n\tunsigned char max, n;\n\n\tmax = bus->busn_res.end;\n\tlist_for_each_entry(tmp, &bus->children, node) {\n\t\tn = pci_bus_max_busnr(tmp);\n\t\tif (n > max)\n\t\t\tmax = n;\n\t}\n\treturn max;\n}\nEXPORT_SYMBOL_GPL(pci_bus_max_busnr);\n\n \nint pci_status_get_and_clear_errors(struct pci_dev *pdev)\n{\n\tu16 status;\n\tint ret;\n\n\tret = pci_read_config_word(pdev, PCI_STATUS, &status);\n\tif (ret != PCIBIOS_SUCCESSFUL)\n\t\treturn -EIO;\n\n\tstatus &= PCI_STATUS_ERROR_BITS;\n\tif (status)\n\t\tpci_write_config_word(pdev, PCI_STATUS, status);\n\n\treturn status;\n}\nEXPORT_SYMBOL_GPL(pci_status_get_and_clear_errors);\n\n#ifdef CONFIG_HAS_IOMEM\nstatic void __iomem *__pci_ioremap_resource(struct pci_dev *pdev, int bar,\n\t\t\t\t\t    bool write_combine)\n{\n\tstruct resource *res = &pdev->resource[bar];\n\tresource_size_t start = res->start;\n\tresource_size_t size = resource_size(res);\n\n\t \n\tif (res->flags & IORESOURCE_UNSET || !(res->flags & IORESOURCE_MEM)) {\n\t\tpci_err(pdev, \"can't ioremap BAR %d: %pR\\n\", bar, res);\n\t\treturn NULL;\n\t}\n\n\tif (write_combine)\n\t\treturn ioremap_wc(start, size);\n\n\treturn ioremap(start, size);\n}\n\nvoid __iomem *pci_ioremap_bar(struct pci_dev *pdev, int bar)\n{\n\treturn __pci_ioremap_resource(pdev, bar, false);\n}\nEXPORT_SYMBOL_GPL(pci_ioremap_bar);\n\nvoid __iomem *pci_ioremap_wc_bar(struct pci_dev *pdev, int bar)\n{\n\treturn __pci_ioremap_resource(pdev, bar, true);\n}\nEXPORT_SYMBOL_GPL(pci_ioremap_wc_bar);\n#endif\n\n \nstatic int pci_dev_str_match_path(struct pci_dev *dev, const char *path,\n\t\t\t\t  const char **endptr)\n{\n\tint ret;\n\tunsigned int seg, bus, slot, func;\n\tchar *wpath, *p;\n\tchar end;\n\n\t*endptr = strchrnul(path, ';');\n\n\twpath = kmemdup_nul(path, *endptr - path, GFP_ATOMIC);\n\tif (!wpath)\n\t\treturn -ENOMEM;\n\n\twhile (1) {\n\t\tp = strrchr(wpath, '/');\n\t\tif (!p)\n\t\t\tbreak;\n\t\tret = sscanf(p, \"/%x.%x%c\", &slot, &func, &end);\n\t\tif (ret != 2) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_and_exit;\n\t\t}\n\n\t\tif (dev->devfn != PCI_DEVFN(slot, func)) {\n\t\t\tret = 0;\n\t\t\tgoto free_and_exit;\n\t\t}\n\n\t\t \n\t\tdev = pci_upstream_bridge(dev);\n\t\tif (!dev) {\n\t\t\tret = 0;\n\t\t\tgoto free_and_exit;\n\t\t}\n\n\t\t*p = 0;\n\t}\n\n\tret = sscanf(wpath, \"%x:%x:%x.%x%c\", &seg, &bus, &slot,\n\t\t     &func, &end);\n\tif (ret != 4) {\n\t\tseg = 0;\n\t\tret = sscanf(wpath, \"%x:%x.%x%c\", &bus, &slot, &func, &end);\n\t\tif (ret != 3) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto free_and_exit;\n\t\t}\n\t}\n\n\tret = (seg == pci_domain_nr(dev->bus) &&\n\t       bus == dev->bus->number &&\n\t       dev->devfn == PCI_DEVFN(slot, func));\n\nfree_and_exit:\n\tkfree(wpath);\n\treturn ret;\n}\n\n \nstatic int pci_dev_str_match(struct pci_dev *dev, const char *p,\n\t\t\t     const char **endptr)\n{\n\tint ret;\n\tint count;\n\tunsigned short vendor, device, subsystem_vendor, subsystem_device;\n\n\tif (strncmp(p, \"pci:\", 4) == 0) {\n\t\t \n\t\tp += 4;\n\t\tret = sscanf(p, \"%hx:%hx:%hx:%hx%n\", &vendor, &device,\n\t\t\t     &subsystem_vendor, &subsystem_device, &count);\n\t\tif (ret != 4) {\n\t\t\tret = sscanf(p, \"%hx:%hx%n\", &vendor, &device, &count);\n\t\t\tif (ret != 2)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tsubsystem_vendor = 0;\n\t\t\tsubsystem_device = 0;\n\t\t}\n\n\t\tp += count;\n\n\t\tif ((!vendor || vendor == dev->vendor) &&\n\t\t    (!device || device == dev->device) &&\n\t\t    (!subsystem_vendor ||\n\t\t\t    subsystem_vendor == dev->subsystem_vendor) &&\n\t\t    (!subsystem_device ||\n\t\t\t    subsystem_device == dev->subsystem_device))\n\t\t\tgoto found;\n\t} else {\n\t\t \n\t\tret = pci_dev_str_match_path(dev, p, &p);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\telse if (ret)\n\t\t\tgoto found;\n\t}\n\n\t*endptr = p;\n\treturn 0;\n\nfound:\n\t*endptr = p;\n\treturn 1;\n}\n\nstatic u8 __pci_find_next_cap_ttl(struct pci_bus *bus, unsigned int devfn,\n\t\t\t\t  u8 pos, int cap, int *ttl)\n{\n\tu8 id;\n\tu16 ent;\n\n\tpci_bus_read_config_byte(bus, devfn, pos, &pos);\n\n\twhile ((*ttl)--) {\n\t\tif (pos < 0x40)\n\t\t\tbreak;\n\t\tpos &= ~3;\n\t\tpci_bus_read_config_word(bus, devfn, pos, &ent);\n\n\t\tid = ent & 0xff;\n\t\tif (id == 0xff)\n\t\t\tbreak;\n\t\tif (id == cap)\n\t\t\treturn pos;\n\t\tpos = (ent >> 8);\n\t}\n\treturn 0;\n}\n\nstatic u8 __pci_find_next_cap(struct pci_bus *bus, unsigned int devfn,\n\t\t\t      u8 pos, int cap)\n{\n\tint ttl = PCI_FIND_CAP_TTL;\n\n\treturn __pci_find_next_cap_ttl(bus, devfn, pos, cap, &ttl);\n}\n\nu8 pci_find_next_capability(struct pci_dev *dev, u8 pos, int cap)\n{\n\treturn __pci_find_next_cap(dev->bus, dev->devfn,\n\t\t\t\t   pos + PCI_CAP_LIST_NEXT, cap);\n}\nEXPORT_SYMBOL_GPL(pci_find_next_capability);\n\nstatic u8 __pci_bus_find_cap_start(struct pci_bus *bus,\n\t\t\t\t    unsigned int devfn, u8 hdr_type)\n{\n\tu16 status;\n\n\tpci_bus_read_config_word(bus, devfn, PCI_STATUS, &status);\n\tif (!(status & PCI_STATUS_CAP_LIST))\n\t\treturn 0;\n\n\tswitch (hdr_type) {\n\tcase PCI_HEADER_TYPE_NORMAL:\n\tcase PCI_HEADER_TYPE_BRIDGE:\n\t\treturn PCI_CAPABILITY_LIST;\n\tcase PCI_HEADER_TYPE_CARDBUS:\n\t\treturn PCI_CB_CAPABILITY_LIST;\n\t}\n\n\treturn 0;\n}\n\n \nu8 pci_find_capability(struct pci_dev *dev, int cap)\n{\n\tu8 pos;\n\n\tpos = __pci_bus_find_cap_start(dev->bus, dev->devfn, dev->hdr_type);\n\tif (pos)\n\t\tpos = __pci_find_next_cap(dev->bus, dev->devfn, pos, cap);\n\n\treturn pos;\n}\nEXPORT_SYMBOL(pci_find_capability);\n\n \nu8 pci_bus_find_capability(struct pci_bus *bus, unsigned int devfn, int cap)\n{\n\tu8 hdr_type, pos;\n\n\tpci_bus_read_config_byte(bus, devfn, PCI_HEADER_TYPE, &hdr_type);\n\n\tpos = __pci_bus_find_cap_start(bus, devfn, hdr_type & 0x7f);\n\tif (pos)\n\t\tpos = __pci_find_next_cap(bus, devfn, pos, cap);\n\n\treturn pos;\n}\nEXPORT_SYMBOL(pci_bus_find_capability);\n\n \nu16 pci_find_next_ext_capability(struct pci_dev *dev, u16 start, int cap)\n{\n\tu32 header;\n\tint ttl;\n\tu16 pos = PCI_CFG_SPACE_SIZE;\n\n\t \n\tttl = (PCI_CFG_SPACE_EXP_SIZE - PCI_CFG_SPACE_SIZE) / 8;\n\n\tif (dev->cfg_size <= PCI_CFG_SPACE_SIZE)\n\t\treturn 0;\n\n\tif (start)\n\t\tpos = start;\n\n\tif (pci_read_config_dword(dev, pos, &header) != PCIBIOS_SUCCESSFUL)\n\t\treturn 0;\n\n\t \n\tif (header == 0)\n\t\treturn 0;\n\n\twhile (ttl-- > 0) {\n\t\tif (PCI_EXT_CAP_ID(header) == cap && pos != start)\n\t\t\treturn pos;\n\n\t\tpos = PCI_EXT_CAP_NEXT(header);\n\t\tif (pos < PCI_CFG_SPACE_SIZE)\n\t\t\tbreak;\n\n\t\tif (pci_read_config_dword(dev, pos, &header) != PCIBIOS_SUCCESSFUL)\n\t\t\tbreak;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(pci_find_next_ext_capability);\n\n \nu16 pci_find_ext_capability(struct pci_dev *dev, int cap)\n{\n\treturn pci_find_next_ext_capability(dev, 0, cap);\n}\nEXPORT_SYMBOL_GPL(pci_find_ext_capability);\n\n \nu64 pci_get_dsn(struct pci_dev *dev)\n{\n\tu32 dword;\n\tu64 dsn;\n\tint pos;\n\n\tpos = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_DSN);\n\tif (!pos)\n\t\treturn 0;\n\n\t \n\tpos += 4;\n\tpci_read_config_dword(dev, pos, &dword);\n\tdsn = (u64)dword;\n\tpci_read_config_dword(dev, pos + 4, &dword);\n\tdsn |= ((u64)dword) << 32;\n\n\treturn dsn;\n}\nEXPORT_SYMBOL_GPL(pci_get_dsn);\n\nstatic u8 __pci_find_next_ht_cap(struct pci_dev *dev, u8 pos, int ht_cap)\n{\n\tint rc, ttl = PCI_FIND_CAP_TTL;\n\tu8 cap, mask;\n\n\tif (ht_cap == HT_CAPTYPE_SLAVE || ht_cap == HT_CAPTYPE_HOST)\n\t\tmask = HT_3BIT_CAP_MASK;\n\telse\n\t\tmask = HT_5BIT_CAP_MASK;\n\n\tpos = __pci_find_next_cap_ttl(dev->bus, dev->devfn, pos,\n\t\t\t\t      PCI_CAP_ID_HT, &ttl);\n\twhile (pos) {\n\t\trc = pci_read_config_byte(dev, pos + 3, &cap);\n\t\tif (rc != PCIBIOS_SUCCESSFUL)\n\t\t\treturn 0;\n\n\t\tif ((cap & mask) == ht_cap)\n\t\t\treturn pos;\n\n\t\tpos = __pci_find_next_cap_ttl(dev->bus, dev->devfn,\n\t\t\t\t\t      pos + PCI_CAP_LIST_NEXT,\n\t\t\t\t\t      PCI_CAP_ID_HT, &ttl);\n\t}\n\n\treturn 0;\n}\n\n \nu8 pci_find_next_ht_capability(struct pci_dev *dev, u8 pos, int ht_cap)\n{\n\treturn __pci_find_next_ht_cap(dev, pos + PCI_CAP_LIST_NEXT, ht_cap);\n}\nEXPORT_SYMBOL_GPL(pci_find_next_ht_capability);\n\n \nu8 pci_find_ht_capability(struct pci_dev *dev, int ht_cap)\n{\n\tu8 pos;\n\n\tpos = __pci_bus_find_cap_start(dev->bus, dev->devfn, dev->hdr_type);\n\tif (pos)\n\t\tpos = __pci_find_next_ht_cap(dev, pos, ht_cap);\n\n\treturn pos;\n}\nEXPORT_SYMBOL_GPL(pci_find_ht_capability);\n\n \nu16 pci_find_vsec_capability(struct pci_dev *dev, u16 vendor, int cap)\n{\n\tu16 vsec = 0;\n\tu32 header;\n\tint ret;\n\n\tif (vendor != dev->vendor)\n\t\treturn 0;\n\n\twhile ((vsec = pci_find_next_ext_capability(dev, vsec,\n\t\t\t\t\t\t     PCI_EXT_CAP_ID_VNDR))) {\n\t\tret = pci_read_config_dword(dev, vsec + PCI_VNDR_HEADER, &header);\n\t\tif (ret != PCIBIOS_SUCCESSFUL)\n\t\t\tcontinue;\n\n\t\tif (PCI_VNDR_HEADER_ID(header) == cap)\n\t\t\treturn vsec;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(pci_find_vsec_capability);\n\n \nu16 pci_find_dvsec_capability(struct pci_dev *dev, u16 vendor, u16 dvsec)\n{\n\tint pos;\n\n\tpos = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_DVSEC);\n\tif (!pos)\n\t\treturn 0;\n\n\twhile (pos) {\n\t\tu16 v, id;\n\n\t\tpci_read_config_word(dev, pos + PCI_DVSEC_HEADER1, &v);\n\t\tpci_read_config_word(dev, pos + PCI_DVSEC_HEADER2, &id);\n\t\tif (vendor == v && dvsec == id)\n\t\t\treturn pos;\n\n\t\tpos = pci_find_next_ext_capability(dev, pos, PCI_EXT_CAP_ID_DVSEC);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(pci_find_dvsec_capability);\n\n \nstruct resource *pci_find_parent_resource(const struct pci_dev *dev,\n\t\t\t\t\t  struct resource *res)\n{\n\tconst struct pci_bus *bus = dev->bus;\n\tstruct resource *r;\n\n\tpci_bus_for_each_resource(bus, r) {\n\t\tif (!r)\n\t\t\tcontinue;\n\t\tif (resource_contains(r, res)) {\n\n\t\t\t \n\t\t\tif (r->flags & IORESOURCE_PREFETCH &&\n\t\t\t    !(res->flags & IORESOURCE_PREFETCH))\n\t\t\t\treturn NULL;\n\n\t\t\t \n\t\t\treturn r;\n\t\t}\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(pci_find_parent_resource);\n\n \nstruct resource *pci_find_resource(struct pci_dev *dev, struct resource *res)\n{\n\tint i;\n\n\tfor (i = 0; i < PCI_STD_NUM_BARS; i++) {\n\t\tstruct resource *r = &dev->resource[i];\n\n\t\tif (r->start && resource_contains(r, res))\n\t\t\treturn r;\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(pci_find_resource);\n\n \nint pci_wait_for_pending(struct pci_dev *dev, int pos, u16 mask)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < 4; i++) {\n\t\tu16 status;\n\t\tif (i)\n\t\t\tmsleep((1 << (i - 1)) * 100);\n\n\t\tpci_read_config_word(dev, pos, &status);\n\t\tif (!(status & mask))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int pci_acs_enable;\n\n \nvoid pci_request_acs(void)\n{\n\tpci_acs_enable = 1;\n}\n\nstatic const char *disable_acs_redir_param;\n\n \nstatic void pci_disable_acs_redir(struct pci_dev *dev)\n{\n\tint ret = 0;\n\tconst char *p;\n\tint pos;\n\tu16 ctrl;\n\n\tif (!disable_acs_redir_param)\n\t\treturn;\n\n\tp = disable_acs_redir_param;\n\twhile (*p) {\n\t\tret = pci_dev_str_match(dev, p, &p);\n\t\tif (ret < 0) {\n\t\t\tpr_info_once(\"PCI: Can't parse disable_acs_redir parameter: %s\\n\",\n\t\t\t\t     disable_acs_redir_param);\n\n\t\t\tbreak;\n\t\t} else if (ret == 1) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\n\t\tif (*p != ';' && *p != ',') {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tp++;\n\t}\n\n\tif (ret != 1)\n\t\treturn;\n\n\tif (!pci_dev_specific_disable_acs_redir(dev))\n\t\treturn;\n\n\tpos = dev->acs_cap;\n\tif (!pos) {\n\t\tpci_warn(dev, \"cannot disable ACS redirect for this hardware as it does not have ACS capabilities\\n\");\n\t\treturn;\n\t}\n\n\tpci_read_config_word(dev, pos + PCI_ACS_CTRL, &ctrl);\n\n\t \n\tctrl &= ~(PCI_ACS_RR | PCI_ACS_CR | PCI_ACS_EC);\n\n\tpci_write_config_word(dev, pos + PCI_ACS_CTRL, ctrl);\n\n\tpci_info(dev, \"disabled ACS redirect\\n\");\n}\n\n \nstatic void pci_std_enable_acs(struct pci_dev *dev)\n{\n\tint pos;\n\tu16 cap;\n\tu16 ctrl;\n\n\tpos = dev->acs_cap;\n\tif (!pos)\n\t\treturn;\n\n\tpci_read_config_word(dev, pos + PCI_ACS_CAP, &cap);\n\tpci_read_config_word(dev, pos + PCI_ACS_CTRL, &ctrl);\n\n\t \n\tctrl |= (cap & PCI_ACS_SV);\n\n\t \n\tctrl |= (cap & PCI_ACS_RR);\n\n\t \n\tctrl |= (cap & PCI_ACS_CR);\n\n\t \n\tctrl |= (cap & PCI_ACS_UF);\n\n\t \n\tif (pci_ats_disabled() || dev->external_facing || dev->untrusted)\n\t\tctrl |= (cap & PCI_ACS_TB);\n\n\tpci_write_config_word(dev, pos + PCI_ACS_CTRL, ctrl);\n}\n\n \nstatic void pci_enable_acs(struct pci_dev *dev)\n{\n\tif (!pci_acs_enable)\n\t\tgoto disable_acs_redir;\n\n\tif (!pci_dev_specific_enable_acs(dev))\n\t\tgoto disable_acs_redir;\n\n\tpci_std_enable_acs(dev);\n\ndisable_acs_redir:\n\t \n\tpci_disable_acs_redir(dev);\n}\n\n \nstatic void pci_restore_bars(struct pci_dev *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < PCI_BRIDGE_RESOURCES; i++)\n\t\tpci_update_resource(dev, i);\n}\n\nstatic inline bool platform_pci_power_manageable(struct pci_dev *dev)\n{\n\tif (pci_use_mid_pm())\n\t\treturn true;\n\n\treturn acpi_pci_power_manageable(dev);\n}\n\nstatic inline int platform_pci_set_power_state(struct pci_dev *dev,\n\t\t\t\t\t       pci_power_t t)\n{\n\tif (pci_use_mid_pm())\n\t\treturn mid_pci_set_power_state(dev, t);\n\n\treturn acpi_pci_set_power_state(dev, t);\n}\n\nstatic inline pci_power_t platform_pci_get_power_state(struct pci_dev *dev)\n{\n\tif (pci_use_mid_pm())\n\t\treturn mid_pci_get_power_state(dev);\n\n\treturn acpi_pci_get_power_state(dev);\n}\n\nstatic inline void platform_pci_refresh_power_state(struct pci_dev *dev)\n{\n\tif (!pci_use_mid_pm())\n\t\tacpi_pci_refresh_power_state(dev);\n}\n\nstatic inline pci_power_t platform_pci_choose_state(struct pci_dev *dev)\n{\n\tif (pci_use_mid_pm())\n\t\treturn PCI_POWER_ERROR;\n\n\treturn acpi_pci_choose_state(dev);\n}\n\nstatic inline int platform_pci_set_wakeup(struct pci_dev *dev, bool enable)\n{\n\tif (pci_use_mid_pm())\n\t\treturn PCI_POWER_ERROR;\n\n\treturn acpi_pci_wakeup(dev, enable);\n}\n\nstatic inline bool platform_pci_need_resume(struct pci_dev *dev)\n{\n\tif (pci_use_mid_pm())\n\t\treturn false;\n\n\treturn acpi_pci_need_resume(dev);\n}\n\nstatic inline bool platform_pci_bridge_d3(struct pci_dev *dev)\n{\n\tif (pci_use_mid_pm())\n\t\treturn false;\n\n\treturn acpi_pci_bridge_d3(dev);\n}\n\n \nvoid pci_update_current_state(struct pci_dev *dev, pci_power_t state)\n{\n\tif (platform_pci_get_power_state(dev) == PCI_D3cold) {\n\t\tdev->current_state = PCI_D3cold;\n\t} else if (dev->pm_cap) {\n\t\tu16 pmcsr;\n\n\t\tpci_read_config_word(dev, dev->pm_cap + PCI_PM_CTRL, &pmcsr);\n\t\tif (PCI_POSSIBLE_ERROR(pmcsr)) {\n\t\t\tdev->current_state = PCI_D3cold;\n\t\t\treturn;\n\t\t}\n\t\tdev->current_state = pmcsr & PCI_PM_CTRL_STATE_MASK;\n\t} else {\n\t\tdev->current_state = state;\n\t}\n}\n\n \nvoid pci_refresh_power_state(struct pci_dev *dev)\n{\n\tplatform_pci_refresh_power_state(dev);\n\tpci_update_current_state(dev, dev->current_state);\n}\n\n \nint pci_platform_power_transition(struct pci_dev *dev, pci_power_t state)\n{\n\tint error;\n\n\terror = platform_pci_set_power_state(dev, state);\n\tif (!error)\n\t\tpci_update_current_state(dev, state);\n\telse if (!dev->pm_cap)  \n\t\tdev->current_state = PCI_D0;\n\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(pci_platform_power_transition);\n\nstatic int pci_resume_one(struct pci_dev *pci_dev, void *ign)\n{\n\tpm_request_resume(&pci_dev->dev);\n\treturn 0;\n}\n\n \nvoid pci_resume_bus(struct pci_bus *bus)\n{\n\tif (bus)\n\t\tpci_walk_bus(bus, pci_resume_one, NULL);\n}\n\nstatic int pci_dev_wait(struct pci_dev *dev, char *reset_type, int timeout)\n{\n\tint delay = 1;\n\tbool retrain = false;\n\tstruct pci_dev *bridge;\n\n\tif (pci_is_pcie(dev)) {\n\t\tbridge = pci_upstream_bridge(dev);\n\t\tif (bridge)\n\t\t\tretrain = true;\n\t}\n\n\t \n\tfor (;;) {\n\t\tu32 id;\n\n\t\tpci_read_config_dword(dev, PCI_COMMAND, &id);\n\t\tif (!PCI_POSSIBLE_ERROR(id))\n\t\t\tbreak;\n\n\t\tif (delay > timeout) {\n\t\t\tpci_warn(dev, \"not ready %dms after %s; giving up\\n\",\n\t\t\t\t delay - 1, reset_type);\n\t\t\treturn -ENOTTY;\n\t\t}\n\n\t\tif (delay > PCI_RESET_WAIT) {\n\t\t\tif (retrain) {\n\t\t\t\tretrain = false;\n\t\t\t\tif (pcie_failed_link_retrain(bridge)) {\n\t\t\t\t\tdelay = 1;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tpci_info(dev, \"not ready %dms after %s; waiting\\n\",\n\t\t\t\t delay - 1, reset_type);\n\t\t}\n\n\t\tmsleep(delay);\n\t\tdelay *= 2;\n\t}\n\n\tif (delay > PCI_RESET_WAIT)\n\t\tpci_info(dev, \"ready %dms after %s\\n\", delay - 1,\n\t\t\t reset_type);\n\n\treturn 0;\n}\n\n \nint pci_power_up(struct pci_dev *dev)\n{\n\tbool need_restore;\n\tpci_power_t state;\n\tu16 pmcsr;\n\n\tplatform_pci_set_power_state(dev, PCI_D0);\n\n\tif (!dev->pm_cap) {\n\t\tstate = platform_pci_get_power_state(dev);\n\t\tif (state == PCI_UNKNOWN)\n\t\t\tdev->current_state = PCI_D0;\n\t\telse\n\t\t\tdev->current_state = state;\n\n\t\treturn -EIO;\n\t}\n\n\tpci_read_config_word(dev, dev->pm_cap + PCI_PM_CTRL, &pmcsr);\n\tif (PCI_POSSIBLE_ERROR(pmcsr)) {\n\t\tpci_err(dev, \"Unable to change power state from %s to D0, device inaccessible\\n\",\n\t\t\tpci_power_name(dev->current_state));\n\t\tdev->current_state = PCI_D3cold;\n\t\treturn -EIO;\n\t}\n\n\tstate = pmcsr & PCI_PM_CTRL_STATE_MASK;\n\n\tneed_restore = (state == PCI_D3hot || dev->current_state >= PCI_D3hot) &&\n\t\t\t!(pmcsr & PCI_PM_CTRL_NO_SOFT_RESET);\n\n\tif (state == PCI_D0)\n\t\tgoto end;\n\n\t \n\tpci_write_config_word(dev, dev->pm_cap + PCI_PM_CTRL, 0);\n\n\t \n\tif (state == PCI_D3hot)\n\t\tpci_dev_d3_sleep(dev);\n\telse if (state == PCI_D2)\n\t\tudelay(PCI_PM_D2_DELAY);\n\nend:\n\tdev->current_state = PCI_D0;\n\tif (need_restore)\n\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic int pci_set_full_power_state(struct pci_dev *dev)\n{\n\tu16 pmcsr;\n\tint ret;\n\n\tret = pci_power_up(dev);\n\tif (ret < 0) {\n\t\tif (dev->current_state == PCI_D0)\n\t\t\treturn 0;\n\n\t\treturn ret;\n\t}\n\n\tpci_read_config_word(dev, dev->pm_cap + PCI_PM_CTRL, &pmcsr);\n\tdev->current_state = pmcsr & PCI_PM_CTRL_STATE_MASK;\n\tif (dev->current_state != PCI_D0) {\n\t\tpci_info_ratelimited(dev, \"Refused to change power state from %s to D0\\n\",\n\t\t\t\t     pci_power_name(dev->current_state));\n\t} else if (ret > 0) {\n\t\t \n\t\tpci_restore_bars(dev);\n\t}\n\n\tif (dev->bus->self)\n\t\tpcie_aspm_pm_state_change(dev->bus->self);\n\n\treturn 0;\n}\n\n \nstatic int __pci_dev_set_current_state(struct pci_dev *dev, void *data)\n{\n\tpci_power_t state = *(pci_power_t *)data;\n\n\tdev->current_state = state;\n\treturn 0;\n}\n\n \nvoid pci_bus_set_current_state(struct pci_bus *bus, pci_power_t state)\n{\n\tif (bus)\n\t\tpci_walk_bus(bus, __pci_dev_set_current_state, &state);\n}\n\n \nstatic int pci_set_low_power_state(struct pci_dev *dev, pci_power_t state)\n{\n\tu16 pmcsr;\n\n\tif (!dev->pm_cap)\n\t\treturn -EIO;\n\n\t \n\tif (dev->current_state <= PCI_D3cold && dev->current_state > state) {\n\t\tpci_dbg(dev, \"Invalid power transition (from %s to %s)\\n\",\n\t\t\tpci_power_name(dev->current_state),\n\t\t\tpci_power_name(state));\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif ((state == PCI_D1 && !dev->d1_support)\n\t   || (state == PCI_D2 && !dev->d2_support))\n\t\treturn -EIO;\n\n\tpci_read_config_word(dev, dev->pm_cap + PCI_PM_CTRL, &pmcsr);\n\tif (PCI_POSSIBLE_ERROR(pmcsr)) {\n\t\tpci_err(dev, \"Unable to change power state from %s to %s, device inaccessible\\n\",\n\t\t\tpci_power_name(dev->current_state),\n\t\t\tpci_power_name(state));\n\t\tdev->current_state = PCI_D3cold;\n\t\treturn -EIO;\n\t}\n\n\tpmcsr &= ~PCI_PM_CTRL_STATE_MASK;\n\tpmcsr |= state;\n\n\t \n\tpci_write_config_word(dev, dev->pm_cap + PCI_PM_CTRL, pmcsr);\n\n\t \n\tif (state == PCI_D3hot)\n\t\tpci_dev_d3_sleep(dev);\n\telse if (state == PCI_D2)\n\t\tudelay(PCI_PM_D2_DELAY);\n\n\tpci_read_config_word(dev, dev->pm_cap + PCI_PM_CTRL, &pmcsr);\n\tdev->current_state = pmcsr & PCI_PM_CTRL_STATE_MASK;\n\tif (dev->current_state != state)\n\t\tpci_info_ratelimited(dev, \"Refused to change power state from %s to %s\\n\",\n\t\t\t\t     pci_power_name(dev->current_state),\n\t\t\t\t     pci_power_name(state));\n\n\tif (dev->bus->self)\n\t\tpcie_aspm_pm_state_change(dev->bus->self);\n\n\treturn 0;\n}\n\n \nint pci_set_power_state(struct pci_dev *dev, pci_power_t state)\n{\n\tint error;\n\n\t \n\tif (state > PCI_D3cold)\n\t\tstate = PCI_D3cold;\n\telse if (state < PCI_D0)\n\t\tstate = PCI_D0;\n\telse if ((state == PCI_D1 || state == PCI_D2) && pci_no_d1d2(dev))\n\n\t\t \n\t\treturn 0;\n\n\t \n\tif (dev->current_state == state)\n\t\treturn 0;\n\n\tif (state == PCI_D0)\n\t\treturn pci_set_full_power_state(dev);\n\n\t \n\tif (state >= PCI_D3hot && (dev->dev_flags & PCI_DEV_FLAGS_NO_D3))\n\t\treturn 0;\n\n\tif (state == PCI_D3cold) {\n\t\t \n\t\terror = pci_set_low_power_state(dev, PCI_D3hot);\n\n\t\tif (pci_platform_power_transition(dev, PCI_D3cold))\n\t\t\treturn error;\n\n\t\t \n\t\tif (dev->current_state == PCI_D3cold)\n\t\t\tpci_bus_set_current_state(dev->subordinate, PCI_D3cold);\n\t} else {\n\t\terror = pci_set_low_power_state(dev, state);\n\n\t\tif (pci_platform_power_transition(dev, state))\n\t\t\treturn error;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(pci_set_power_state);\n\n#define PCI_EXP_SAVE_REGS\t7\n\nstatic struct pci_cap_saved_state *_pci_find_saved_cap(struct pci_dev *pci_dev,\n\t\t\t\t\t\t       u16 cap, bool extended)\n{\n\tstruct pci_cap_saved_state *tmp;\n\n\thlist_for_each_entry(tmp, &pci_dev->saved_cap_space, next) {\n\t\tif (tmp->cap.cap_extended == extended && tmp->cap.cap_nr == cap)\n\t\t\treturn tmp;\n\t}\n\treturn NULL;\n}\n\nstruct pci_cap_saved_state *pci_find_saved_cap(struct pci_dev *dev, char cap)\n{\n\treturn _pci_find_saved_cap(dev, cap, false);\n}\n\nstruct pci_cap_saved_state *pci_find_saved_ext_cap(struct pci_dev *dev, u16 cap)\n{\n\treturn _pci_find_saved_cap(dev, cap, true);\n}\n\nstatic int pci_save_pcie_state(struct pci_dev *dev)\n{\n\tint i = 0;\n\tstruct pci_cap_saved_state *save_state;\n\tu16 *cap;\n\n\tif (!pci_is_pcie(dev))\n\t\treturn 0;\n\n\tsave_state = pci_find_saved_cap(dev, PCI_CAP_ID_EXP);\n\tif (!save_state) {\n\t\tpci_err(dev, \"buffer not found in %s\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\n\tcap = (u16 *)&save_state->cap.data[0];\n\tpcie_capability_read_word(dev, PCI_EXP_DEVCTL, &cap[i++]);\n\tpcie_capability_read_word(dev, PCI_EXP_LNKCTL, &cap[i++]);\n\tpcie_capability_read_word(dev, PCI_EXP_SLTCTL, &cap[i++]);\n\tpcie_capability_read_word(dev, PCI_EXP_RTCTL,  &cap[i++]);\n\tpcie_capability_read_word(dev, PCI_EXP_DEVCTL2, &cap[i++]);\n\tpcie_capability_read_word(dev, PCI_EXP_LNKCTL2, &cap[i++]);\n\tpcie_capability_read_word(dev, PCI_EXP_SLTCTL2, &cap[i++]);\n\n\treturn 0;\n}\n\nvoid pci_bridge_reconfigure_ltr(struct pci_dev *dev)\n{\n#ifdef CONFIG_PCIEASPM\n\tstruct pci_dev *bridge;\n\tu32 ctl;\n\n\tbridge = pci_upstream_bridge(dev);\n\tif (bridge && bridge->ltr_path) {\n\t\tpcie_capability_read_dword(bridge, PCI_EXP_DEVCTL2, &ctl);\n\t\tif (!(ctl & PCI_EXP_DEVCTL2_LTR_EN)) {\n\t\t\tpci_dbg(bridge, \"re-enabling LTR\\n\");\n\t\t\tpcie_capability_set_word(bridge, PCI_EXP_DEVCTL2,\n\t\t\t\t\t\t PCI_EXP_DEVCTL2_LTR_EN);\n\t\t}\n\t}\n#endif\n}\n\nstatic void pci_restore_pcie_state(struct pci_dev *dev)\n{\n\tint i = 0;\n\tstruct pci_cap_saved_state *save_state;\n\tu16 *cap;\n\n\tsave_state = pci_find_saved_cap(dev, PCI_CAP_ID_EXP);\n\tif (!save_state)\n\t\treturn;\n\n\t \n\tpci_bridge_reconfigure_ltr(dev);\n\n\tcap = (u16 *)&save_state->cap.data[0];\n\tpcie_capability_write_word(dev, PCI_EXP_DEVCTL, cap[i++]);\n\tpcie_capability_write_word(dev, PCI_EXP_LNKCTL, cap[i++]);\n\tpcie_capability_write_word(dev, PCI_EXP_SLTCTL, cap[i++]);\n\tpcie_capability_write_word(dev, PCI_EXP_RTCTL, cap[i++]);\n\tpcie_capability_write_word(dev, PCI_EXP_DEVCTL2, cap[i++]);\n\tpcie_capability_write_word(dev, PCI_EXP_LNKCTL2, cap[i++]);\n\tpcie_capability_write_word(dev, PCI_EXP_SLTCTL2, cap[i++]);\n}\n\nstatic int pci_save_pcix_state(struct pci_dev *dev)\n{\n\tint pos;\n\tstruct pci_cap_saved_state *save_state;\n\n\tpos = pci_find_capability(dev, PCI_CAP_ID_PCIX);\n\tif (!pos)\n\t\treturn 0;\n\n\tsave_state = pci_find_saved_cap(dev, PCI_CAP_ID_PCIX);\n\tif (!save_state) {\n\t\tpci_err(dev, \"buffer not found in %s\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\n\tpci_read_config_word(dev, pos + PCI_X_CMD,\n\t\t\t     (u16 *)save_state->cap.data);\n\n\treturn 0;\n}\n\nstatic void pci_restore_pcix_state(struct pci_dev *dev)\n{\n\tint i = 0, pos;\n\tstruct pci_cap_saved_state *save_state;\n\tu16 *cap;\n\n\tsave_state = pci_find_saved_cap(dev, PCI_CAP_ID_PCIX);\n\tpos = pci_find_capability(dev, PCI_CAP_ID_PCIX);\n\tif (!save_state || !pos)\n\t\treturn;\n\tcap = (u16 *)&save_state->cap.data[0];\n\n\tpci_write_config_word(dev, pos + PCI_X_CMD, cap[i++]);\n}\n\nstatic void pci_save_ltr_state(struct pci_dev *dev)\n{\n\tint ltr;\n\tstruct pci_cap_saved_state *save_state;\n\tu32 *cap;\n\n\tif (!pci_is_pcie(dev))\n\t\treturn;\n\n\tltr = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_LTR);\n\tif (!ltr)\n\t\treturn;\n\n\tsave_state = pci_find_saved_ext_cap(dev, PCI_EXT_CAP_ID_LTR);\n\tif (!save_state) {\n\t\tpci_err(dev, \"no suspend buffer for LTR; ASPM issues possible after resume\\n\");\n\t\treturn;\n\t}\n\n\t \n\tcap = &save_state->cap.data[0];\n\tpci_read_config_dword(dev, ltr + PCI_LTR_MAX_SNOOP_LAT, cap);\n}\n\nstatic void pci_restore_ltr_state(struct pci_dev *dev)\n{\n\tstruct pci_cap_saved_state *save_state;\n\tint ltr;\n\tu32 *cap;\n\n\tsave_state = pci_find_saved_ext_cap(dev, PCI_EXT_CAP_ID_LTR);\n\tltr = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_LTR);\n\tif (!save_state || !ltr)\n\t\treturn;\n\n\t \n\tcap = &save_state->cap.data[0];\n\tpci_write_config_dword(dev, ltr + PCI_LTR_MAX_SNOOP_LAT, *cap);\n}\n\n \nint pci_save_state(struct pci_dev *dev)\n{\n\tint i;\n\t \n\tfor (i = 0; i < 16; i++) {\n\t\tpci_read_config_dword(dev, i * 4, &dev->saved_config_space[i]);\n\t\tpci_dbg(dev, \"save config %#04x: %#010x\\n\",\n\t\t\ti * 4, dev->saved_config_space[i]);\n\t}\n\tdev->state_saved = true;\n\n\ti = pci_save_pcie_state(dev);\n\tif (i != 0)\n\t\treturn i;\n\n\ti = pci_save_pcix_state(dev);\n\tif (i != 0)\n\t\treturn i;\n\n\tpci_save_ltr_state(dev);\n\tpci_save_dpc_state(dev);\n\tpci_save_aer_state(dev);\n\tpci_save_ptm_state(dev);\n\treturn pci_save_vc_state(dev);\n}\nEXPORT_SYMBOL(pci_save_state);\n\nstatic void pci_restore_config_dword(struct pci_dev *pdev, int offset,\n\t\t\t\t     u32 saved_val, int retry, bool force)\n{\n\tu32 val;\n\n\tpci_read_config_dword(pdev, offset, &val);\n\tif (!force && val == saved_val)\n\t\treturn;\n\n\tfor (;;) {\n\t\tpci_dbg(pdev, \"restore config %#04x: %#010x -> %#010x\\n\",\n\t\t\toffset, val, saved_val);\n\t\tpci_write_config_dword(pdev, offset, saved_val);\n\t\tif (retry-- <= 0)\n\t\t\treturn;\n\n\t\tpci_read_config_dword(pdev, offset, &val);\n\t\tif (val == saved_val)\n\t\t\treturn;\n\n\t\tmdelay(1);\n\t}\n}\n\nstatic void pci_restore_config_space_range(struct pci_dev *pdev,\n\t\t\t\t\t   int start, int end, int retry,\n\t\t\t\t\t   bool force)\n{\n\tint index;\n\n\tfor (index = end; index >= start; index--)\n\t\tpci_restore_config_dword(pdev, 4 * index,\n\t\t\t\t\t pdev->saved_config_space[index],\n\t\t\t\t\t retry, force);\n}\n\nstatic void pci_restore_config_space(struct pci_dev *pdev)\n{\n\tif (pdev->hdr_type == PCI_HEADER_TYPE_NORMAL) {\n\t\tpci_restore_config_space_range(pdev, 10, 15, 0, false);\n\t\t \n\t\tpci_restore_config_space_range(pdev, 4, 9, 10, false);\n\t\tpci_restore_config_space_range(pdev, 0, 3, 0, false);\n\t} else if (pdev->hdr_type == PCI_HEADER_TYPE_BRIDGE) {\n\t\tpci_restore_config_space_range(pdev, 12, 15, 0, false);\n\n\t\t \n\t\tpci_restore_config_space_range(pdev, 9, 11, 0, true);\n\t\tpci_restore_config_space_range(pdev, 0, 8, 0, false);\n\t} else {\n\t\tpci_restore_config_space_range(pdev, 0, 15, 0, false);\n\t}\n}\n\nstatic void pci_restore_rebar_state(struct pci_dev *pdev)\n{\n\tunsigned int pos, nbars, i;\n\tu32 ctrl;\n\n\tpos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_REBAR);\n\tif (!pos)\n\t\treturn;\n\n\tpci_read_config_dword(pdev, pos + PCI_REBAR_CTRL, &ctrl);\n\tnbars = (ctrl & PCI_REBAR_CTRL_NBAR_MASK) >>\n\t\t    PCI_REBAR_CTRL_NBAR_SHIFT;\n\n\tfor (i = 0; i < nbars; i++, pos += 8) {\n\t\tstruct resource *res;\n\t\tint bar_idx, size;\n\n\t\tpci_read_config_dword(pdev, pos + PCI_REBAR_CTRL, &ctrl);\n\t\tbar_idx = ctrl & PCI_REBAR_CTRL_BAR_IDX;\n\t\tres = pdev->resource + bar_idx;\n\t\tsize = pci_rebar_bytes_to_size(resource_size(res));\n\t\tctrl &= ~PCI_REBAR_CTRL_BAR_SIZE;\n\t\tctrl |= size << PCI_REBAR_CTRL_BAR_SHIFT;\n\t\tpci_write_config_dword(pdev, pos + PCI_REBAR_CTRL, ctrl);\n\t}\n}\n\n \nvoid pci_restore_state(struct pci_dev *dev)\n{\n\tif (!dev->state_saved)\n\t\treturn;\n\n\t \n\tpci_restore_ltr_state(dev);\n\n\tpci_restore_pcie_state(dev);\n\tpci_restore_pasid_state(dev);\n\tpci_restore_pri_state(dev);\n\tpci_restore_ats_state(dev);\n\tpci_restore_vc_state(dev);\n\tpci_restore_rebar_state(dev);\n\tpci_restore_dpc_state(dev);\n\tpci_restore_ptm_state(dev);\n\n\tpci_aer_clear_status(dev);\n\tpci_restore_aer_state(dev);\n\n\tpci_restore_config_space(dev);\n\n\tpci_restore_pcix_state(dev);\n\tpci_restore_msi_state(dev);\n\n\t \n\tpci_enable_acs(dev);\n\tpci_restore_iov_state(dev);\n\n\tdev->state_saved = false;\n}\nEXPORT_SYMBOL(pci_restore_state);\n\nstruct pci_saved_state {\n\tu32 config_space[16];\n\tstruct pci_cap_saved_data cap[];\n};\n\n \nstruct pci_saved_state *pci_store_saved_state(struct pci_dev *dev)\n{\n\tstruct pci_saved_state *state;\n\tstruct pci_cap_saved_state *tmp;\n\tstruct pci_cap_saved_data *cap;\n\tsize_t size;\n\n\tif (!dev->state_saved)\n\t\treturn NULL;\n\n\tsize = sizeof(*state) + sizeof(struct pci_cap_saved_data);\n\n\thlist_for_each_entry(tmp, &dev->saved_cap_space, next)\n\t\tsize += sizeof(struct pci_cap_saved_data) + tmp->cap.size;\n\n\tstate = kzalloc(size, GFP_KERNEL);\n\tif (!state)\n\t\treturn NULL;\n\n\tmemcpy(state->config_space, dev->saved_config_space,\n\t       sizeof(state->config_space));\n\n\tcap = state->cap;\n\thlist_for_each_entry(tmp, &dev->saved_cap_space, next) {\n\t\tsize_t len = sizeof(struct pci_cap_saved_data) + tmp->cap.size;\n\t\tmemcpy(cap, &tmp->cap, len);\n\t\tcap = (struct pci_cap_saved_data *)((u8 *)cap + len);\n\t}\n\t \n\n\treturn state;\n}\nEXPORT_SYMBOL_GPL(pci_store_saved_state);\n\n \nint pci_load_saved_state(struct pci_dev *dev,\n\t\t\t struct pci_saved_state *state)\n{\n\tstruct pci_cap_saved_data *cap;\n\n\tdev->state_saved = false;\n\n\tif (!state)\n\t\treturn 0;\n\n\tmemcpy(dev->saved_config_space, state->config_space,\n\t       sizeof(state->config_space));\n\n\tcap = state->cap;\n\twhile (cap->size) {\n\t\tstruct pci_cap_saved_state *tmp;\n\n\t\ttmp = _pci_find_saved_cap(dev, cap->cap_nr, cap->cap_extended);\n\t\tif (!tmp || tmp->cap.size != cap->size)\n\t\t\treturn -EINVAL;\n\n\t\tmemcpy(tmp->cap.data, cap->data, tmp->cap.size);\n\t\tcap = (struct pci_cap_saved_data *)((u8 *)cap +\n\t\t       sizeof(struct pci_cap_saved_data) + cap->size);\n\t}\n\n\tdev->state_saved = true;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(pci_load_saved_state);\n\n \nint pci_load_and_free_saved_state(struct pci_dev *dev,\n\t\t\t\t  struct pci_saved_state **state)\n{\n\tint ret = pci_load_saved_state(dev, *state);\n\tkfree(*state);\n\t*state = NULL;\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(pci_load_and_free_saved_state);\n\nint __weak pcibios_enable_device(struct pci_dev *dev, int bars)\n{\n\treturn pci_enable_resources(dev, bars);\n}\n\nstatic int do_pci_enable_device(struct pci_dev *dev, int bars)\n{\n\tint err;\n\tstruct pci_dev *bridge;\n\tu16 cmd;\n\tu8 pin;\n\n\terr = pci_set_power_state(dev, PCI_D0);\n\tif (err < 0 && err != -EIO)\n\t\treturn err;\n\n\tbridge = pci_upstream_bridge(dev);\n\tif (bridge)\n\t\tpcie_aspm_powersave_config_link(bridge);\n\n\terr = pcibios_enable_device(dev, bars);\n\tif (err < 0)\n\t\treturn err;\n\tpci_fixup_device(pci_fixup_enable, dev);\n\n\tif (dev->msi_enabled || dev->msix_enabled)\n\t\treturn 0;\n\n\tpci_read_config_byte(dev, PCI_INTERRUPT_PIN, &pin);\n\tif (pin) {\n\t\tpci_read_config_word(dev, PCI_COMMAND, &cmd);\n\t\tif (cmd & PCI_COMMAND_INTX_DISABLE)\n\t\t\tpci_write_config_word(dev, PCI_COMMAND,\n\t\t\t\t\t      cmd & ~PCI_COMMAND_INTX_DISABLE);\n\t}\n\n\treturn 0;\n}\n\n \nint pci_reenable_device(struct pci_dev *dev)\n{\n\tif (pci_is_enabled(dev))\n\t\treturn do_pci_enable_device(dev, (1 << PCI_NUM_RESOURCES) - 1);\n\treturn 0;\n}\nEXPORT_SYMBOL(pci_reenable_device);\n\nstatic void pci_enable_bridge(struct pci_dev *dev)\n{\n\tstruct pci_dev *bridge;\n\tint retval;\n\n\tbridge = pci_upstream_bridge(dev);\n\tif (bridge)\n\t\tpci_enable_bridge(bridge);\n\n\tif (pci_is_enabled(dev)) {\n\t\tif (!dev->is_busmaster)\n\t\t\tpci_set_master(dev);\n\t\treturn;\n\t}\n\n\tretval = pci_enable_device(dev);\n\tif (retval)\n\t\tpci_err(dev, \"Error enabling bridge (%d), continuing\\n\",\n\t\t\tretval);\n\tpci_set_master(dev);\n}\n\nstatic int pci_enable_device_flags(struct pci_dev *dev, unsigned long flags)\n{\n\tstruct pci_dev *bridge;\n\tint err;\n\tint i, bars = 0;\n\n\t \n\tpci_update_current_state(dev, dev->current_state);\n\n\tif (atomic_inc_return(&dev->enable_cnt) > 1)\n\t\treturn 0;\t\t \n\n\tbridge = pci_upstream_bridge(dev);\n\tif (bridge)\n\t\tpci_enable_bridge(bridge);\n\n\t \n\tfor (i = 0; i <= PCI_ROM_RESOURCE; i++)\n\t\tif (dev->resource[i].flags & flags)\n\t\t\tbars |= (1 << i);\n\tfor (i = PCI_BRIDGE_RESOURCES; i < DEVICE_COUNT_RESOURCE; i++)\n\t\tif (dev->resource[i].flags & flags)\n\t\t\tbars |= (1 << i);\n\n\terr = do_pci_enable_device(dev, bars);\n\tif (err < 0)\n\t\tatomic_dec(&dev->enable_cnt);\n\treturn err;\n}\n\n \nint pci_enable_device_io(struct pci_dev *dev)\n{\n\treturn pci_enable_device_flags(dev, IORESOURCE_IO);\n}\nEXPORT_SYMBOL(pci_enable_device_io);\n\n \nint pci_enable_device_mem(struct pci_dev *dev)\n{\n\treturn pci_enable_device_flags(dev, IORESOURCE_MEM);\n}\nEXPORT_SYMBOL(pci_enable_device_mem);\n\n \nint pci_enable_device(struct pci_dev *dev)\n{\n\treturn pci_enable_device_flags(dev, IORESOURCE_MEM | IORESOURCE_IO);\n}\nEXPORT_SYMBOL(pci_enable_device);\n\n \nstruct pci_devres {\n\tunsigned int enabled:1;\n\tunsigned int pinned:1;\n\tunsigned int orig_intx:1;\n\tunsigned int restore_intx:1;\n\tunsigned int mwi:1;\n\tu32 region_mask;\n};\n\nstatic void pcim_release(struct device *gendev, void *res)\n{\n\tstruct pci_dev *dev = to_pci_dev(gendev);\n\tstruct pci_devres *this = res;\n\tint i;\n\n\tfor (i = 0; i < DEVICE_COUNT_RESOURCE; i++)\n\t\tif (this->region_mask & (1 << i))\n\t\t\tpci_release_region(dev, i);\n\n\tif (this->mwi)\n\t\tpci_clear_mwi(dev);\n\n\tif (this->restore_intx)\n\t\tpci_intx(dev, this->orig_intx);\n\n\tif (this->enabled && !this->pinned)\n\t\tpci_disable_device(dev);\n}\n\nstatic struct pci_devres *get_pci_dr(struct pci_dev *pdev)\n{\n\tstruct pci_devres *dr, *new_dr;\n\n\tdr = devres_find(&pdev->dev, pcim_release, NULL, NULL);\n\tif (dr)\n\t\treturn dr;\n\n\tnew_dr = devres_alloc(pcim_release, sizeof(*new_dr), GFP_KERNEL);\n\tif (!new_dr)\n\t\treturn NULL;\n\treturn devres_get(&pdev->dev, new_dr, NULL, NULL);\n}\n\nstatic struct pci_devres *find_pci_dr(struct pci_dev *pdev)\n{\n\tif (pci_is_managed(pdev))\n\t\treturn devres_find(&pdev->dev, pcim_release, NULL, NULL);\n\treturn NULL;\n}\n\n \nint pcim_enable_device(struct pci_dev *pdev)\n{\n\tstruct pci_devres *dr;\n\tint rc;\n\n\tdr = get_pci_dr(pdev);\n\tif (unlikely(!dr))\n\t\treturn -ENOMEM;\n\tif (dr->enabled)\n\t\treturn 0;\n\n\trc = pci_enable_device(pdev);\n\tif (!rc) {\n\t\tpdev->is_managed = 1;\n\t\tdr->enabled = 1;\n\t}\n\treturn rc;\n}\nEXPORT_SYMBOL(pcim_enable_device);\n\n \nvoid pcim_pin_device(struct pci_dev *pdev)\n{\n\tstruct pci_devres *dr;\n\n\tdr = find_pci_dr(pdev);\n\tWARN_ON(!dr || !dr->enabled);\n\tif (dr)\n\t\tdr->pinned = 1;\n}\nEXPORT_SYMBOL(pcim_pin_device);\n\n \nint __weak pcibios_device_add(struct pci_dev *dev)\n{\n\treturn 0;\n}\n\n \nvoid __weak pcibios_release_device(struct pci_dev *dev) {}\n\n \nvoid __weak pcibios_disable_device(struct pci_dev *dev) {}\n\n \nvoid __weak pcibios_penalize_isa_irq(int irq, int active) {}\n\nstatic void do_pci_disable_device(struct pci_dev *dev)\n{\n\tu16 pci_command;\n\n\tpci_read_config_word(dev, PCI_COMMAND, &pci_command);\n\tif (pci_command & PCI_COMMAND_MASTER) {\n\t\tpci_command &= ~PCI_COMMAND_MASTER;\n\t\tpci_write_config_word(dev, PCI_COMMAND, pci_command);\n\t}\n\n\tpcibios_disable_device(dev);\n}\n\n \nvoid pci_disable_enabled_device(struct pci_dev *dev)\n{\n\tif (pci_is_enabled(dev))\n\t\tdo_pci_disable_device(dev);\n}\n\n \nvoid pci_disable_device(struct pci_dev *dev)\n{\n\tstruct pci_devres *dr;\n\n\tdr = find_pci_dr(dev);\n\tif (dr)\n\t\tdr->enabled = 0;\n\n\tdev_WARN_ONCE(&dev->dev, atomic_read(&dev->enable_cnt) <= 0,\n\t\t      \"disabling already-disabled device\");\n\n\tif (atomic_dec_return(&dev->enable_cnt) != 0)\n\t\treturn;\n\n\tdo_pci_disable_device(dev);\n\n\tdev->is_busmaster = 0;\n}\nEXPORT_SYMBOL(pci_disable_device);\n\n \nint __weak pcibios_set_pcie_reset_state(struct pci_dev *dev,\n\t\t\t\t\tenum pcie_reset_state state)\n{\n\treturn -EINVAL;\n}\n\n \nint pci_set_pcie_reset_state(struct pci_dev *dev, enum pcie_reset_state state)\n{\n\treturn pcibios_set_pcie_reset_state(dev, state);\n}\nEXPORT_SYMBOL_GPL(pci_set_pcie_reset_state);\n\n#ifdef CONFIG_PCIEAER\nvoid pcie_clear_device_status(struct pci_dev *dev)\n{\n\tu16 sta;\n\n\tpcie_capability_read_word(dev, PCI_EXP_DEVSTA, &sta);\n\tpcie_capability_write_word(dev, PCI_EXP_DEVSTA, sta);\n}\n#endif\n\n \nvoid pcie_clear_root_pme_status(struct pci_dev *dev)\n{\n\tpcie_capability_set_dword(dev, PCI_EXP_RTSTA, PCI_EXP_RTSTA_PME);\n}\n\n \nbool pci_check_pme_status(struct pci_dev *dev)\n{\n\tint pmcsr_pos;\n\tu16 pmcsr;\n\tbool ret = false;\n\n\tif (!dev->pm_cap)\n\t\treturn false;\n\n\tpmcsr_pos = dev->pm_cap + PCI_PM_CTRL;\n\tpci_read_config_word(dev, pmcsr_pos, &pmcsr);\n\tif (!(pmcsr & PCI_PM_CTRL_PME_STATUS))\n\t\treturn false;\n\n\t \n\tpmcsr |= PCI_PM_CTRL_PME_STATUS;\n\tif (pmcsr & PCI_PM_CTRL_PME_ENABLE) {\n\t\t \n\t\tpmcsr &= ~PCI_PM_CTRL_PME_ENABLE;\n\t\tret = true;\n\t}\n\n\tpci_write_config_word(dev, pmcsr_pos, pmcsr);\n\n\treturn ret;\n}\n\n \nstatic int pci_pme_wakeup(struct pci_dev *dev, void *pme_poll_reset)\n{\n\tif (pme_poll_reset && dev->pme_poll)\n\t\tdev->pme_poll = false;\n\n\tif (pci_check_pme_status(dev)) {\n\t\tpci_wakeup_event(dev);\n\t\tpm_request_resume(&dev->dev);\n\t}\n\treturn 0;\n}\n\n \nvoid pci_pme_wakeup_bus(struct pci_bus *bus)\n{\n\tif (bus)\n\t\tpci_walk_bus(bus, pci_pme_wakeup, (void *)true);\n}\n\n\n \nbool pci_pme_capable(struct pci_dev *dev, pci_power_t state)\n{\n\tif (!dev->pm_cap)\n\t\treturn false;\n\n\treturn !!(dev->pme_support & (1 << state));\n}\nEXPORT_SYMBOL(pci_pme_capable);\n\nstatic void pci_pme_list_scan(struct work_struct *work)\n{\n\tstruct pci_pme_device *pme_dev, *n;\n\n\tmutex_lock(&pci_pme_list_mutex);\n\tlist_for_each_entry_safe(pme_dev, n, &pci_pme_list, list) {\n\t\tstruct pci_dev *pdev = pme_dev->dev;\n\n\t\tif (pdev->pme_poll) {\n\t\t\tstruct pci_dev *bridge = pdev->bus->self;\n\t\t\tstruct device *dev = &pdev->dev;\n\t\t\tint pm_status;\n\n\t\t\t \n\t\t\tif (bridge && bridge->current_state != PCI_D0)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tpm_status = pm_runtime_get_if_active(dev, true);\n\t\t\tif (!pm_status)\n\t\t\t\tcontinue;\n\n\t\t\tif (pdev->current_state != PCI_D3cold)\n\t\t\t\tpci_pme_wakeup(pdev, NULL);\n\n\t\t\tif (pm_status > 0)\n\t\t\t\tpm_runtime_put(dev);\n\t\t} else {\n\t\t\tlist_del(&pme_dev->list);\n\t\t\tkfree(pme_dev);\n\t\t}\n\t}\n\tif (!list_empty(&pci_pme_list))\n\t\tqueue_delayed_work(system_freezable_wq, &pci_pme_work,\n\t\t\t\t   msecs_to_jiffies(PME_TIMEOUT));\n\tmutex_unlock(&pci_pme_list_mutex);\n}\n\nstatic void __pci_pme_active(struct pci_dev *dev, bool enable)\n{\n\tu16 pmcsr;\n\n\tif (!dev->pme_support)\n\t\treturn;\n\n\tpci_read_config_word(dev, dev->pm_cap + PCI_PM_CTRL, &pmcsr);\n\t \n\tpmcsr |= PCI_PM_CTRL_PME_STATUS | PCI_PM_CTRL_PME_ENABLE;\n\tif (!enable)\n\t\tpmcsr &= ~PCI_PM_CTRL_PME_ENABLE;\n\n\tpci_write_config_word(dev, dev->pm_cap + PCI_PM_CTRL, pmcsr);\n}\n\n \nvoid pci_pme_restore(struct pci_dev *dev)\n{\n\tu16 pmcsr;\n\n\tif (!dev->pme_support)\n\t\treturn;\n\n\tpci_read_config_word(dev, dev->pm_cap + PCI_PM_CTRL, &pmcsr);\n\tif (dev->wakeup_prepared) {\n\t\tpmcsr |= PCI_PM_CTRL_PME_ENABLE;\n\t\tpmcsr &= ~PCI_PM_CTRL_PME_STATUS;\n\t} else {\n\t\tpmcsr &= ~PCI_PM_CTRL_PME_ENABLE;\n\t\tpmcsr |= PCI_PM_CTRL_PME_STATUS;\n\t}\n\tpci_write_config_word(dev, dev->pm_cap + PCI_PM_CTRL, pmcsr);\n}\n\n \nvoid pci_pme_active(struct pci_dev *dev, bool enable)\n{\n\t__pci_pme_active(dev, enable);\n\n\t \n\n\tif (dev->pme_poll) {\n\t\tstruct pci_pme_device *pme_dev;\n\t\tif (enable) {\n\t\t\tpme_dev = kmalloc(sizeof(struct pci_pme_device),\n\t\t\t\t\t  GFP_KERNEL);\n\t\t\tif (!pme_dev) {\n\t\t\t\tpci_warn(dev, \"can't enable PME#\\n\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tpme_dev->dev = dev;\n\t\t\tmutex_lock(&pci_pme_list_mutex);\n\t\t\tlist_add(&pme_dev->list, &pci_pme_list);\n\t\t\tif (list_is_singular(&pci_pme_list))\n\t\t\t\tqueue_delayed_work(system_freezable_wq,\n\t\t\t\t\t\t   &pci_pme_work,\n\t\t\t\t\t\t   msecs_to_jiffies(PME_TIMEOUT));\n\t\t\tmutex_unlock(&pci_pme_list_mutex);\n\t\t} else {\n\t\t\tmutex_lock(&pci_pme_list_mutex);\n\t\t\tlist_for_each_entry(pme_dev, &pci_pme_list, list) {\n\t\t\t\tif (pme_dev->dev == dev) {\n\t\t\t\t\tlist_del(&pme_dev->list);\n\t\t\t\t\tkfree(pme_dev);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tmutex_unlock(&pci_pme_list_mutex);\n\t\t}\n\t}\n\n\tpci_dbg(dev, \"PME# %s\\n\", enable ? \"enabled\" : \"disabled\");\n}\nEXPORT_SYMBOL(pci_pme_active);\n\n \nstatic int __pci_enable_wake(struct pci_dev *dev, pci_power_t state, bool enable)\n{\n\tint ret = 0;\n\n\t \n\tif (!pci_power_manageable(dev))\n\t\treturn 0;\n\n\t \n\tif (!!enable == !!dev->wakeup_prepared)\n\t\treturn 0;\n\n\t \n\n\tif (enable) {\n\t\tint error;\n\n\t\t \n\t\tif (pci_pme_capable(dev, state) || pci_pme_capable(dev, PCI_D3cold))\n\t\t\tpci_pme_active(dev, true);\n\t\telse\n\t\t\tret = 1;\n\t\terror = platform_pci_set_wakeup(dev, true);\n\t\tif (ret)\n\t\t\tret = error;\n\t\tif (!ret)\n\t\t\tdev->wakeup_prepared = true;\n\t} else {\n\t\tplatform_pci_set_wakeup(dev, false);\n\t\tpci_pme_active(dev, false);\n\t\tdev->wakeup_prepared = false;\n\t}\n\n\treturn ret;\n}\n\n \nint pci_enable_wake(struct pci_dev *pci_dev, pci_power_t state, bool enable)\n{\n\tif (enable && !device_may_wakeup(&pci_dev->dev))\n\t\treturn -EINVAL;\n\n\treturn __pci_enable_wake(pci_dev, state, enable);\n}\nEXPORT_SYMBOL(pci_enable_wake);\n\n \nint pci_wake_from_d3(struct pci_dev *dev, bool enable)\n{\n\treturn pci_pme_capable(dev, PCI_D3cold) ?\n\t\t\tpci_enable_wake(dev, PCI_D3cold, enable) :\n\t\t\tpci_enable_wake(dev, PCI_D3hot, enable);\n}\nEXPORT_SYMBOL(pci_wake_from_d3);\n\n \nstatic pci_power_t pci_target_state(struct pci_dev *dev, bool wakeup)\n{\n\tif (platform_pci_power_manageable(dev)) {\n\t\t \n\t\tpci_power_t state = platform_pci_choose_state(dev);\n\n\t\tswitch (state) {\n\t\tcase PCI_POWER_ERROR:\n\t\tcase PCI_UNKNOWN:\n\t\t\treturn PCI_D3hot;\n\n\t\tcase PCI_D1:\n\t\tcase PCI_D2:\n\t\t\tif (pci_no_d1d2(dev))\n\t\t\t\treturn PCI_D3hot;\n\t\t}\n\n\t\treturn state;\n\t}\n\n\t \n\tif (dev->current_state == PCI_D3cold)\n\t\treturn PCI_D3cold;\n\telse if (!dev->pm_cap)\n\t\treturn PCI_D0;\n\n\tif (wakeup && dev->pme_support) {\n\t\tpci_power_t state = PCI_D3hot;\n\n\t\t \n\t\twhile (state && !(dev->pme_support & (1 << state)))\n\t\t\tstate--;\n\n\t\tif (state)\n\t\t\treturn state;\n\t\telse if (dev->pme_support & 1)\n\t\t\treturn PCI_D0;\n\t}\n\n\treturn PCI_D3hot;\n}\n\n \nint pci_prepare_to_sleep(struct pci_dev *dev)\n{\n\tbool wakeup = device_may_wakeup(&dev->dev);\n\tpci_power_t target_state = pci_target_state(dev, wakeup);\n\tint error;\n\n\tif (target_state == PCI_POWER_ERROR)\n\t\treturn -EIO;\n\n\tpci_enable_wake(dev, target_state, wakeup);\n\n\terror = pci_set_power_state(dev, target_state);\n\n\tif (error)\n\t\tpci_enable_wake(dev, target_state, false);\n\n\treturn error;\n}\nEXPORT_SYMBOL(pci_prepare_to_sleep);\n\n \nint pci_back_from_sleep(struct pci_dev *dev)\n{\n\tint ret = pci_set_power_state(dev, PCI_D0);\n\n\tif (ret)\n\t\treturn ret;\n\n\tpci_enable_wake(dev, PCI_D0, false);\n\treturn 0;\n}\nEXPORT_SYMBOL(pci_back_from_sleep);\n\n \nint pci_finish_runtime_suspend(struct pci_dev *dev)\n{\n\tpci_power_t target_state;\n\tint error;\n\n\ttarget_state = pci_target_state(dev, device_can_wakeup(&dev->dev));\n\tif (target_state == PCI_POWER_ERROR)\n\t\treturn -EIO;\n\n\t__pci_enable_wake(dev, target_state, pci_dev_run_wake(dev));\n\n\terror = pci_set_power_state(dev, target_state);\n\n\tif (error)\n\t\tpci_enable_wake(dev, target_state, false);\n\n\treturn error;\n}\n\n \nbool pci_dev_run_wake(struct pci_dev *dev)\n{\n\tstruct pci_bus *bus = dev->bus;\n\n\tif (!dev->pme_support)\n\t\treturn false;\n\n\t \n\tif (!pci_pme_capable(dev, pci_target_state(dev, true)))\n\t\treturn false;\n\n\tif (device_can_wakeup(&dev->dev))\n\t\treturn true;\n\n\twhile (bus->parent) {\n\t\tstruct pci_dev *bridge = bus->self;\n\n\t\tif (device_can_wakeup(&bridge->dev))\n\t\t\treturn true;\n\n\t\tbus = bus->parent;\n\t}\n\n\t \n\tif (bus->bridge)\n\t\treturn device_can_wakeup(bus->bridge);\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(pci_dev_run_wake);\n\n \nbool pci_dev_need_resume(struct pci_dev *pci_dev)\n{\n\tstruct device *dev = &pci_dev->dev;\n\tpci_power_t target_state;\n\n\tif (!pm_runtime_suspended(dev) || platform_pci_need_resume(pci_dev))\n\t\treturn true;\n\n\ttarget_state = pci_target_state(pci_dev, device_may_wakeup(dev));\n\n\t \n\treturn target_state != pci_dev->current_state &&\n\t\ttarget_state != PCI_D3cold &&\n\t\tpci_dev->current_state != PCI_D3hot;\n}\n\n \nvoid pci_dev_adjust_pme(struct pci_dev *pci_dev)\n{\n\tstruct device *dev = &pci_dev->dev;\n\n\tspin_lock_irq(&dev->power.lock);\n\n\tif (pm_runtime_suspended(dev) && !device_may_wakeup(dev) &&\n\t    pci_dev->current_state < PCI_D3cold)\n\t\t__pci_pme_active(pci_dev, false);\n\n\tspin_unlock_irq(&dev->power.lock);\n}\n\n \nvoid pci_dev_complete_resume(struct pci_dev *pci_dev)\n{\n\tstruct device *dev = &pci_dev->dev;\n\n\tif (!pci_dev_run_wake(pci_dev))\n\t\treturn;\n\n\tspin_lock_irq(&dev->power.lock);\n\n\tif (pm_runtime_suspended(dev) && pci_dev->current_state < PCI_D3cold)\n\t\t__pci_pme_active(pci_dev, true);\n\n\tspin_unlock_irq(&dev->power.lock);\n}\n\n \npci_power_t pci_choose_state(struct pci_dev *dev, pm_message_t state)\n{\n\tif (state.event == PM_EVENT_ON)\n\t\treturn PCI_D0;\n\n\treturn pci_target_state(dev, false);\n}\nEXPORT_SYMBOL(pci_choose_state);\n\nvoid pci_config_pm_runtime_get(struct pci_dev *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct device *parent = dev->parent;\n\n\tif (parent)\n\t\tpm_runtime_get_sync(parent);\n\tpm_runtime_get_noresume(dev);\n\t \n\tpm_runtime_barrier(dev);\n\t \n\tif (pdev->current_state == PCI_D3cold)\n\t\tpm_runtime_resume(dev);\n}\n\nvoid pci_config_pm_runtime_put(struct pci_dev *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct device *parent = dev->parent;\n\n\tpm_runtime_put(dev);\n\tif (parent)\n\t\tpm_runtime_put_sync(parent);\n}\n\nstatic const struct dmi_system_id bridge_d3_blacklist[] = {\n#ifdef CONFIG_X86\n\t{\n\t\t \n\t\t.ident = \"X299 DESIGNARE EX-CF\",\n\t\t.matches = {\n\t\t\tDMI_MATCH(DMI_BOARD_VENDOR, \"Gigabyte Technology Co., Ltd.\"),\n\t\t\tDMI_MATCH(DMI_BOARD_NAME, \"X299 DESIGNARE EX-CF\"),\n\t\t},\n\t},\n\t{\n\t\t \n\t\t.ident = \"Elo Continental Z2\",\n\t\t.matches = {\n\t\t\tDMI_MATCH(DMI_BOARD_VENDOR, \"Elo Touch Solutions\"),\n\t\t\tDMI_MATCH(DMI_BOARD_NAME, \"Geminilake\"),\n\t\t\tDMI_MATCH(DMI_BOARD_VERSION, \"Continental Z2\"),\n\t\t},\n\t},\n#endif\n\t{ }\n};\n\n \nbool pci_bridge_d3_possible(struct pci_dev *bridge)\n{\n\tif (!pci_is_pcie(bridge))\n\t\treturn false;\n\n\tswitch (pci_pcie_type(bridge)) {\n\tcase PCI_EXP_TYPE_ROOT_PORT:\n\tcase PCI_EXP_TYPE_UPSTREAM:\n\tcase PCI_EXP_TYPE_DOWNSTREAM:\n\t\tif (pci_bridge_d3_disable)\n\t\t\treturn false;\n\n\t\t \n\t\tif (bridge->is_hotplug_bridge && !pciehp_is_native(bridge))\n\t\t\treturn false;\n\n\t\tif (pci_bridge_d3_force)\n\t\t\treturn true;\n\n\t\t \n\t\tif (bridge->is_thunderbolt)\n\t\t\treturn true;\n\n\t\t \n\t\tif (platform_pci_bridge_d3(bridge))\n\t\t\treturn true;\n\n\t\t \n\t\tif (bridge->is_hotplug_bridge)\n\t\t\treturn false;\n\n\t\tif (dmi_check_system(bridge_d3_blacklist))\n\t\t\treturn false;\n\n\t\t \n\t\tif (dmi_get_bios_year() >= 2015)\n\t\t\treturn true;\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\nstatic int pci_dev_check_d3cold(struct pci_dev *dev, void *data)\n{\n\tbool *d3cold_ok = data;\n\n\tif ( \n\t    dev->no_d3cold || !dev->d3cold_allowed ||\n\n\t     \n\t    (device_may_wakeup(&dev->dev) &&\n\t     !pci_pme_capable(dev, PCI_D3cold)) ||\n\n\t     \n\t    !pci_power_manageable(dev))\n\n\t\t*d3cold_ok = false;\n\n\treturn !*d3cold_ok;\n}\n\n \nvoid pci_bridge_d3_update(struct pci_dev *dev)\n{\n\tbool remove = !device_is_registered(&dev->dev);\n\tstruct pci_dev *bridge;\n\tbool d3cold_ok = true;\n\n\tbridge = pci_upstream_bridge(dev);\n\tif (!bridge || !pci_bridge_d3_possible(bridge))\n\t\treturn;\n\n\t \n\tif (remove && bridge->bridge_d3)\n\t\treturn;\n\n\t \n\tif (!remove)\n\t\tpci_dev_check_d3cold(dev, &d3cold_ok);\n\n\t \n\tif (d3cold_ok && !bridge->bridge_d3)\n\t\tpci_walk_bus(bridge->subordinate, pci_dev_check_d3cold,\n\t\t\t     &d3cold_ok);\n\n\tif (bridge->bridge_d3 != d3cold_ok) {\n\t\tbridge->bridge_d3 = d3cold_ok;\n\t\t \n\t\tpci_bridge_d3_update(bridge);\n\t}\n}\n\n \nvoid pci_d3cold_enable(struct pci_dev *dev)\n{\n\tif (dev->no_d3cold) {\n\t\tdev->no_d3cold = false;\n\t\tpci_bridge_d3_update(dev);\n\t}\n}\nEXPORT_SYMBOL_GPL(pci_d3cold_enable);\n\n \nvoid pci_d3cold_disable(struct pci_dev *dev)\n{\n\tif (!dev->no_d3cold) {\n\t\tdev->no_d3cold = true;\n\t\tpci_bridge_d3_update(dev);\n\t}\n}\nEXPORT_SYMBOL_GPL(pci_d3cold_disable);\n\n \nvoid pci_pm_init(struct pci_dev *dev)\n{\n\tint pm;\n\tu16 status;\n\tu16 pmc;\n\n\tpm_runtime_forbid(&dev->dev);\n\tpm_runtime_set_active(&dev->dev);\n\tpm_runtime_enable(&dev->dev);\n\tdevice_enable_async_suspend(&dev->dev);\n\tdev->wakeup_prepared = false;\n\n\tdev->pm_cap = 0;\n\tdev->pme_support = 0;\n\n\t \n\tpm = pci_find_capability(dev, PCI_CAP_ID_PM);\n\tif (!pm)\n\t\treturn;\n\t \n\tpci_read_config_word(dev, pm + PCI_PM_PMC, &pmc);\n\n\tif ((pmc & PCI_PM_CAP_VER_MASK) > 3) {\n\t\tpci_err(dev, \"unsupported PM cap regs version (%u)\\n\",\n\t\t\tpmc & PCI_PM_CAP_VER_MASK);\n\t\treturn;\n\t}\n\n\tdev->pm_cap = pm;\n\tdev->d3hot_delay = PCI_PM_D3HOT_WAIT;\n\tdev->d3cold_delay = PCI_PM_D3COLD_WAIT;\n\tdev->bridge_d3 = pci_bridge_d3_possible(dev);\n\tdev->d3cold_allowed = true;\n\n\tdev->d1_support = false;\n\tdev->d2_support = false;\n\tif (!pci_no_d1d2(dev)) {\n\t\tif (pmc & PCI_PM_CAP_D1)\n\t\t\tdev->d1_support = true;\n\t\tif (pmc & PCI_PM_CAP_D2)\n\t\t\tdev->d2_support = true;\n\n\t\tif (dev->d1_support || dev->d2_support)\n\t\t\tpci_info(dev, \"supports%s%s\\n\",\n\t\t\t\t   dev->d1_support ? \" D1\" : \"\",\n\t\t\t\t   dev->d2_support ? \" D2\" : \"\");\n\t}\n\n\tpmc &= PCI_PM_CAP_PME_MASK;\n\tif (pmc) {\n\t\tpci_info(dev, \"PME# supported from%s%s%s%s%s\\n\",\n\t\t\t (pmc & PCI_PM_CAP_PME_D0) ? \" D0\" : \"\",\n\t\t\t (pmc & PCI_PM_CAP_PME_D1) ? \" D1\" : \"\",\n\t\t\t (pmc & PCI_PM_CAP_PME_D2) ? \" D2\" : \"\",\n\t\t\t (pmc & PCI_PM_CAP_PME_D3hot) ? \" D3hot\" : \"\",\n\t\t\t (pmc & PCI_PM_CAP_PME_D3cold) ? \" D3cold\" : \"\");\n\t\tdev->pme_support = pmc >> PCI_PM_CAP_PME_SHIFT;\n\t\tdev->pme_poll = true;\n\t\t \n\t\tdevice_set_wakeup_capable(&dev->dev, true);\n\t\t \n\t\tpci_pme_active(dev, false);\n\t}\n\n\tpci_read_config_word(dev, PCI_STATUS, &status);\n\tif (status & PCI_STATUS_IMM_READY)\n\t\tdev->imm_ready = 1;\n}\n\nstatic unsigned long pci_ea_flags(struct pci_dev *dev, u8 prop)\n{\n\tunsigned long flags = IORESOURCE_PCI_FIXED | IORESOURCE_PCI_EA_BEI;\n\n\tswitch (prop) {\n\tcase PCI_EA_P_MEM:\n\tcase PCI_EA_P_VF_MEM:\n\t\tflags |= IORESOURCE_MEM;\n\t\tbreak;\n\tcase PCI_EA_P_MEM_PREFETCH:\n\tcase PCI_EA_P_VF_MEM_PREFETCH:\n\t\tflags |= IORESOURCE_MEM | IORESOURCE_PREFETCH;\n\t\tbreak;\n\tcase PCI_EA_P_IO:\n\t\tflags |= IORESOURCE_IO;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treturn flags;\n}\n\nstatic struct resource *pci_ea_get_resource(struct pci_dev *dev, u8 bei,\n\t\t\t\t\t    u8 prop)\n{\n\tif (bei <= PCI_EA_BEI_BAR5 && prop <= PCI_EA_P_IO)\n\t\treturn &dev->resource[bei];\n#ifdef CONFIG_PCI_IOV\n\telse if (bei >= PCI_EA_BEI_VF_BAR0 && bei <= PCI_EA_BEI_VF_BAR5 &&\n\t\t (prop == PCI_EA_P_VF_MEM || prop == PCI_EA_P_VF_MEM_PREFETCH))\n\t\treturn &dev->resource[PCI_IOV_RESOURCES +\n\t\t\t\t      bei - PCI_EA_BEI_VF_BAR0];\n#endif\n\telse if (bei == PCI_EA_BEI_ROM)\n\t\treturn &dev->resource[PCI_ROM_RESOURCE];\n\telse\n\t\treturn NULL;\n}\n\n \nstatic int pci_ea_read(struct pci_dev *dev, int offset)\n{\n\tstruct resource *res;\n\tint ent_size, ent_offset = offset;\n\tresource_size_t start, end;\n\tunsigned long flags;\n\tu32 dw0, bei, base, max_offset;\n\tu8 prop;\n\tbool support_64 = (sizeof(resource_size_t) >= 8);\n\n\tpci_read_config_dword(dev, ent_offset, &dw0);\n\tent_offset += 4;\n\n\t \n\tent_size = ((dw0 & PCI_EA_ES) + 1) << 2;\n\n\tif (!(dw0 & PCI_EA_ENABLE))  \n\t\tgoto out;\n\n\tbei = (dw0 & PCI_EA_BEI) >> 4;\n\tprop = (dw0 & PCI_EA_PP) >> 8;\n\n\t \n\tif (prop > PCI_EA_P_BRIDGE_IO && prop < PCI_EA_P_MEM_RESERVED)\n\t\tprop = (dw0 & PCI_EA_SP) >> 16;\n\tif (prop > PCI_EA_P_BRIDGE_IO)\n\t\tgoto out;\n\n\tres = pci_ea_get_resource(dev, bei, prop);\n\tif (!res) {\n\t\tpci_err(dev, \"Unsupported EA entry BEI: %u\\n\", bei);\n\t\tgoto out;\n\t}\n\n\tflags = pci_ea_flags(dev, prop);\n\tif (!flags) {\n\t\tpci_err(dev, \"Unsupported EA properties: %#x\\n\", prop);\n\t\tgoto out;\n\t}\n\n\t \n\tpci_read_config_dword(dev, ent_offset, &base);\n\tstart = (base & PCI_EA_FIELD_MASK);\n\tent_offset += 4;\n\n\t \n\tpci_read_config_dword(dev, ent_offset, &max_offset);\n\tent_offset += 4;\n\n\t \n\tif (base & PCI_EA_IS_64) {\n\t\tu32 base_upper;\n\n\t\tpci_read_config_dword(dev, ent_offset, &base_upper);\n\t\tent_offset += 4;\n\n\t\tflags |= IORESOURCE_MEM_64;\n\n\t\t \n\t\tif (!support_64 && base_upper)\n\t\t\tgoto out;\n\n\t\tif (support_64)\n\t\t\tstart |= ((u64)base_upper << 32);\n\t}\n\n\tend = start + (max_offset | 0x03);\n\n\t \n\tif (max_offset & PCI_EA_IS_64) {\n\t\tu32 max_offset_upper;\n\n\t\tpci_read_config_dword(dev, ent_offset, &max_offset_upper);\n\t\tent_offset += 4;\n\n\t\tflags |= IORESOURCE_MEM_64;\n\n\t\t \n\t\tif (!support_64 && max_offset_upper)\n\t\t\tgoto out;\n\n\t\tif (support_64)\n\t\t\tend += ((u64)max_offset_upper << 32);\n\t}\n\n\tif (end < start) {\n\t\tpci_err(dev, \"EA Entry crosses address boundary\\n\");\n\t\tgoto out;\n\t}\n\n\tif (ent_size != ent_offset - offset) {\n\t\tpci_err(dev, \"EA Entry Size (%d) does not match length read (%d)\\n\",\n\t\t\tent_size, ent_offset - offset);\n\t\tgoto out;\n\t}\n\n\tres->name = pci_name(dev);\n\tres->start = start;\n\tres->end = end;\n\tres->flags = flags;\n\n\tif (bei <= PCI_EA_BEI_BAR5)\n\t\tpci_info(dev, \"BAR %d: %pR (from Enhanced Allocation, properties %#02x)\\n\",\n\t\t\t   bei, res, prop);\n\telse if (bei == PCI_EA_BEI_ROM)\n\t\tpci_info(dev, \"ROM: %pR (from Enhanced Allocation, properties %#02x)\\n\",\n\t\t\t   res, prop);\n\telse if (bei >= PCI_EA_BEI_VF_BAR0 && bei <= PCI_EA_BEI_VF_BAR5)\n\t\tpci_info(dev, \"VF BAR %d: %pR (from Enhanced Allocation, properties %#02x)\\n\",\n\t\t\t   bei - PCI_EA_BEI_VF_BAR0, res, prop);\n\telse\n\t\tpci_info(dev, \"BEI %d res: %pR (from Enhanced Allocation, properties %#02x)\\n\",\n\t\t\t   bei, res, prop);\n\nout:\n\treturn offset + ent_size;\n}\n\n \nvoid pci_ea_init(struct pci_dev *dev)\n{\n\tint ea;\n\tu8 num_ent;\n\tint offset;\n\tint i;\n\n\t \n\tea = pci_find_capability(dev, PCI_CAP_ID_EA);\n\tif (!ea)\n\t\treturn;\n\n\t \n\tpci_bus_read_config_byte(dev->bus, dev->devfn, ea + PCI_EA_NUM_ENT,\n\t\t\t\t\t&num_ent);\n\tnum_ent &= PCI_EA_NUM_ENT_MASK;\n\n\toffset = ea + PCI_EA_FIRST_ENT;\n\n\t \n\tif (dev->hdr_type == PCI_HEADER_TYPE_BRIDGE)\n\t\toffset += 4;\n\n\t \n\tfor (i = 0; i < num_ent; ++i)\n\t\toffset = pci_ea_read(dev, offset);\n}\n\nstatic void pci_add_saved_cap(struct pci_dev *pci_dev,\n\tstruct pci_cap_saved_state *new_cap)\n{\n\thlist_add_head(&new_cap->next, &pci_dev->saved_cap_space);\n}\n\n \nstatic int _pci_add_cap_save_buffer(struct pci_dev *dev, u16 cap,\n\t\t\t\t    bool extended, unsigned int size)\n{\n\tint pos;\n\tstruct pci_cap_saved_state *save_state;\n\n\tif (extended)\n\t\tpos = pci_find_ext_capability(dev, cap);\n\telse\n\t\tpos = pci_find_capability(dev, cap);\n\n\tif (!pos)\n\t\treturn 0;\n\n\tsave_state = kzalloc(sizeof(*save_state) + size, GFP_KERNEL);\n\tif (!save_state)\n\t\treturn -ENOMEM;\n\n\tsave_state->cap.cap_nr = cap;\n\tsave_state->cap.cap_extended = extended;\n\tsave_state->cap.size = size;\n\tpci_add_saved_cap(dev, save_state);\n\n\treturn 0;\n}\n\nint pci_add_cap_save_buffer(struct pci_dev *dev, char cap, unsigned int size)\n{\n\treturn _pci_add_cap_save_buffer(dev, cap, false, size);\n}\n\nint pci_add_ext_cap_save_buffer(struct pci_dev *dev, u16 cap, unsigned int size)\n{\n\treturn _pci_add_cap_save_buffer(dev, cap, true, size);\n}\n\n \nvoid pci_allocate_cap_save_buffers(struct pci_dev *dev)\n{\n\tint error;\n\n\terror = pci_add_cap_save_buffer(dev, PCI_CAP_ID_EXP,\n\t\t\t\t\tPCI_EXP_SAVE_REGS * sizeof(u16));\n\tif (error)\n\t\tpci_err(dev, \"unable to preallocate PCI Express save buffer\\n\");\n\n\terror = pci_add_cap_save_buffer(dev, PCI_CAP_ID_PCIX, sizeof(u16));\n\tif (error)\n\t\tpci_err(dev, \"unable to preallocate PCI-X save buffer\\n\");\n\n\terror = pci_add_ext_cap_save_buffer(dev, PCI_EXT_CAP_ID_LTR,\n\t\t\t\t\t    2 * sizeof(u16));\n\tif (error)\n\t\tpci_err(dev, \"unable to allocate suspend buffer for LTR\\n\");\n\n\tpci_allocate_vc_save_buffers(dev);\n}\n\nvoid pci_free_cap_save_buffers(struct pci_dev *dev)\n{\n\tstruct pci_cap_saved_state *tmp;\n\tstruct hlist_node *n;\n\n\thlist_for_each_entry_safe(tmp, n, &dev->saved_cap_space, next)\n\t\tkfree(tmp);\n}\n\n \nvoid pci_configure_ari(struct pci_dev *dev)\n{\n\tu32 cap;\n\tstruct pci_dev *bridge;\n\n\tif (pcie_ari_disabled || !pci_is_pcie(dev) || dev->devfn)\n\t\treturn;\n\n\tbridge = dev->bus->self;\n\tif (!bridge)\n\t\treturn;\n\n\tpcie_capability_read_dword(bridge, PCI_EXP_DEVCAP2, &cap);\n\tif (!(cap & PCI_EXP_DEVCAP2_ARI))\n\t\treturn;\n\n\tif (pci_find_ext_capability(dev, PCI_EXT_CAP_ID_ARI)) {\n\t\tpcie_capability_set_word(bridge, PCI_EXP_DEVCTL2,\n\t\t\t\t\t PCI_EXP_DEVCTL2_ARI);\n\t\tbridge->ari_enabled = 1;\n\t} else {\n\t\tpcie_capability_clear_word(bridge, PCI_EXP_DEVCTL2,\n\t\t\t\t\t   PCI_EXP_DEVCTL2_ARI);\n\t\tbridge->ari_enabled = 0;\n\t}\n}\n\nstatic bool pci_acs_flags_enabled(struct pci_dev *pdev, u16 acs_flags)\n{\n\tint pos;\n\tu16 cap, ctrl;\n\n\tpos = pdev->acs_cap;\n\tif (!pos)\n\t\treturn false;\n\n\t \n\tpci_read_config_word(pdev, pos + PCI_ACS_CAP, &cap);\n\tacs_flags &= (cap | PCI_ACS_EC);\n\n\tpci_read_config_word(pdev, pos + PCI_ACS_CTRL, &ctrl);\n\treturn (ctrl & acs_flags) == acs_flags;\n}\n\n \nbool pci_acs_enabled(struct pci_dev *pdev, u16 acs_flags)\n{\n\tint ret;\n\n\tret = pci_dev_specific_acs_enabled(pdev, acs_flags);\n\tif (ret >= 0)\n\t\treturn ret > 0;\n\n\t \n\tif (!pci_is_pcie(pdev))\n\t\treturn false;\n\n\tswitch (pci_pcie_type(pdev)) {\n\t \n\tcase PCI_EXP_TYPE_PCIE_BRIDGE:\n\t \n\tcase PCI_EXP_TYPE_PCI_BRIDGE:\n\tcase PCI_EXP_TYPE_RC_EC:\n\t\treturn false;\n\t \n\tcase PCI_EXP_TYPE_DOWNSTREAM:\n\tcase PCI_EXP_TYPE_ROOT_PORT:\n\t\treturn pci_acs_flags_enabled(pdev, acs_flags);\n\t \n\tcase PCI_EXP_TYPE_ENDPOINT:\n\tcase PCI_EXP_TYPE_UPSTREAM:\n\tcase PCI_EXP_TYPE_LEG_END:\n\tcase PCI_EXP_TYPE_RC_END:\n\t\tif (!pdev->multifunction)\n\t\t\tbreak;\n\n\t\treturn pci_acs_flags_enabled(pdev, acs_flags);\n\t}\n\n\t \n\treturn true;\n}\n\n \nbool pci_acs_path_enabled(struct pci_dev *start,\n\t\t\t  struct pci_dev *end, u16 acs_flags)\n{\n\tstruct pci_dev *pdev, *parent = start;\n\n\tdo {\n\t\tpdev = parent;\n\n\t\tif (!pci_acs_enabled(pdev, acs_flags))\n\t\t\treturn false;\n\n\t\tif (pci_is_root_bus(pdev->bus))\n\t\t\treturn (end == NULL);\n\n\t\tparent = pdev->bus->self;\n\t} while (pdev != end);\n\n\treturn true;\n}\n\n \nvoid pci_acs_init(struct pci_dev *dev)\n{\n\tdev->acs_cap = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_ACS);\n\n\t \n\tpci_enable_acs(dev);\n}\n\n \nstatic int pci_rebar_find_pos(struct pci_dev *pdev, int bar)\n{\n\tunsigned int pos, nbars, i;\n\tu32 ctrl;\n\n\tpos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_REBAR);\n\tif (!pos)\n\t\treturn -ENOTSUPP;\n\n\tpci_read_config_dword(pdev, pos + PCI_REBAR_CTRL, &ctrl);\n\tnbars = (ctrl & PCI_REBAR_CTRL_NBAR_MASK) >>\n\t\t    PCI_REBAR_CTRL_NBAR_SHIFT;\n\n\tfor (i = 0; i < nbars; i++, pos += 8) {\n\t\tint bar_idx;\n\n\t\tpci_read_config_dword(pdev, pos + PCI_REBAR_CTRL, &ctrl);\n\t\tbar_idx = ctrl & PCI_REBAR_CTRL_BAR_IDX;\n\t\tif (bar_idx == bar)\n\t\t\treturn pos;\n\t}\n\n\treturn -ENOENT;\n}\n\n \nu32 pci_rebar_get_possible_sizes(struct pci_dev *pdev, int bar)\n{\n\tint pos;\n\tu32 cap;\n\n\tpos = pci_rebar_find_pos(pdev, bar);\n\tif (pos < 0)\n\t\treturn 0;\n\n\tpci_read_config_dword(pdev, pos + PCI_REBAR_CAP, &cap);\n\tcap = FIELD_GET(PCI_REBAR_CAP_SIZES, cap);\n\n\t \n\tif (pdev->vendor == PCI_VENDOR_ID_ATI && pdev->device == 0x731f &&\n\t    bar == 0 && cap == 0x700)\n\t\treturn 0x3f00;\n\n\treturn cap;\n}\nEXPORT_SYMBOL(pci_rebar_get_possible_sizes);\n\n \nint pci_rebar_get_current_size(struct pci_dev *pdev, int bar)\n{\n\tint pos;\n\tu32 ctrl;\n\n\tpos = pci_rebar_find_pos(pdev, bar);\n\tif (pos < 0)\n\t\treturn pos;\n\n\tpci_read_config_dword(pdev, pos + PCI_REBAR_CTRL, &ctrl);\n\treturn (ctrl & PCI_REBAR_CTRL_BAR_SIZE) >> PCI_REBAR_CTRL_BAR_SHIFT;\n}\n\n \nint pci_rebar_set_size(struct pci_dev *pdev, int bar, int size)\n{\n\tint pos;\n\tu32 ctrl;\n\n\tpos = pci_rebar_find_pos(pdev, bar);\n\tif (pos < 0)\n\t\treturn pos;\n\n\tpci_read_config_dword(pdev, pos + PCI_REBAR_CTRL, &ctrl);\n\tctrl &= ~PCI_REBAR_CTRL_BAR_SIZE;\n\tctrl |= size << PCI_REBAR_CTRL_BAR_SHIFT;\n\tpci_write_config_dword(pdev, pos + PCI_REBAR_CTRL, ctrl);\n\treturn 0;\n}\n\n \nint pci_enable_atomic_ops_to_root(struct pci_dev *dev, u32 cap_mask)\n{\n\tstruct pci_bus *bus = dev->bus;\n\tstruct pci_dev *bridge;\n\tu32 cap, ctl2;\n\n\t \n\tif (dev->is_virtfn)\n\t\treturn -EINVAL;\n\n\tif (!pci_is_pcie(dev))\n\t\treturn -EINVAL;\n\n\t \n\n\tswitch (pci_pcie_type(dev)) {\n\tcase PCI_EXP_TYPE_ENDPOINT:\n\tcase PCI_EXP_TYPE_LEG_END:\n\tcase PCI_EXP_TYPE_RC_END:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\twhile (bus->parent) {\n\t\tbridge = bus->self;\n\n\t\tpcie_capability_read_dword(bridge, PCI_EXP_DEVCAP2, &cap);\n\n\t\tswitch (pci_pcie_type(bridge)) {\n\t\t \n\t\tcase PCI_EXP_TYPE_UPSTREAM:\n\t\tcase PCI_EXP_TYPE_DOWNSTREAM:\n\t\t\tif (!(cap & PCI_EXP_DEVCAP2_ATOMIC_ROUTE))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\n\t\t \n\t\tcase PCI_EXP_TYPE_ROOT_PORT:\n\t\t\tif ((cap & cap_mask) != cap_mask)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (pci_pcie_type(bridge) == PCI_EXP_TYPE_UPSTREAM) {\n\t\t\tpcie_capability_read_dword(bridge, PCI_EXP_DEVCTL2,\n\t\t\t\t\t\t   &ctl2);\n\t\t\tif (ctl2 & PCI_EXP_DEVCTL2_ATOMIC_EGRESS_BLOCK)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tbus = bus->parent;\n\t}\n\n\tpcie_capability_set_word(dev, PCI_EXP_DEVCTL2,\n\t\t\t\t PCI_EXP_DEVCTL2_ATOMIC_REQ);\n\treturn 0;\n}\nEXPORT_SYMBOL(pci_enable_atomic_ops_to_root);\n\n \nu8 pci_swizzle_interrupt_pin(const struct pci_dev *dev, u8 pin)\n{\n\tint slot;\n\n\tif (pci_ari_enabled(dev->bus))\n\t\tslot = 0;\n\telse\n\t\tslot = PCI_SLOT(dev->devfn);\n\n\treturn (((pin - 1) + slot) % 4) + 1;\n}\n\nint pci_get_interrupt_pin(struct pci_dev *dev, struct pci_dev **bridge)\n{\n\tu8 pin;\n\n\tpin = dev->pin;\n\tif (!pin)\n\t\treturn -1;\n\n\twhile (!pci_is_root_bus(dev->bus)) {\n\t\tpin = pci_swizzle_interrupt_pin(dev, pin);\n\t\tdev = dev->bus->self;\n\t}\n\t*bridge = dev;\n\treturn pin;\n}\n\n \nu8 pci_common_swizzle(struct pci_dev *dev, u8 *pinp)\n{\n\tu8 pin = *pinp;\n\n\twhile (!pci_is_root_bus(dev->bus)) {\n\t\tpin = pci_swizzle_interrupt_pin(dev, pin);\n\t\tdev = dev->bus->self;\n\t}\n\t*pinp = pin;\n\treturn PCI_SLOT(dev->devfn);\n}\nEXPORT_SYMBOL_GPL(pci_common_swizzle);\n\n \nvoid pci_release_region(struct pci_dev *pdev, int bar)\n{\n\tstruct pci_devres *dr;\n\n\tif (pci_resource_len(pdev, bar) == 0)\n\t\treturn;\n\tif (pci_resource_flags(pdev, bar) & IORESOURCE_IO)\n\t\trelease_region(pci_resource_start(pdev, bar),\n\t\t\t\tpci_resource_len(pdev, bar));\n\telse if (pci_resource_flags(pdev, bar) & IORESOURCE_MEM)\n\t\trelease_mem_region(pci_resource_start(pdev, bar),\n\t\t\t\tpci_resource_len(pdev, bar));\n\n\tdr = find_pci_dr(pdev);\n\tif (dr)\n\t\tdr->region_mask &= ~(1 << bar);\n}\nEXPORT_SYMBOL(pci_release_region);\n\n \nstatic int __pci_request_region(struct pci_dev *pdev, int bar,\n\t\t\t\tconst char *res_name, int exclusive)\n{\n\tstruct pci_devres *dr;\n\n\tif (pci_resource_len(pdev, bar) == 0)\n\t\treturn 0;\n\n\tif (pci_resource_flags(pdev, bar) & IORESOURCE_IO) {\n\t\tif (!request_region(pci_resource_start(pdev, bar),\n\t\t\t    pci_resource_len(pdev, bar), res_name))\n\t\t\tgoto err_out;\n\t} else if (pci_resource_flags(pdev, bar) & IORESOURCE_MEM) {\n\t\tif (!__request_mem_region(pci_resource_start(pdev, bar),\n\t\t\t\t\tpci_resource_len(pdev, bar), res_name,\n\t\t\t\t\texclusive))\n\t\t\tgoto err_out;\n\t}\n\n\tdr = find_pci_dr(pdev);\n\tif (dr)\n\t\tdr->region_mask |= 1 << bar;\n\n\treturn 0;\n\nerr_out:\n\tpci_warn(pdev, \"BAR %d: can't reserve %pR\\n\", bar,\n\t\t &pdev->resource[bar]);\n\treturn -EBUSY;\n}\n\n \nint pci_request_region(struct pci_dev *pdev, int bar, const char *res_name)\n{\n\treturn __pci_request_region(pdev, bar, res_name, 0);\n}\nEXPORT_SYMBOL(pci_request_region);\n\n \nvoid pci_release_selected_regions(struct pci_dev *pdev, int bars)\n{\n\tint i;\n\n\tfor (i = 0; i < PCI_STD_NUM_BARS; i++)\n\t\tif (bars & (1 << i))\n\t\t\tpci_release_region(pdev, i);\n}\nEXPORT_SYMBOL(pci_release_selected_regions);\n\nstatic int __pci_request_selected_regions(struct pci_dev *pdev, int bars,\n\t\t\t\t\t  const char *res_name, int excl)\n{\n\tint i;\n\n\tfor (i = 0; i < PCI_STD_NUM_BARS; i++)\n\t\tif (bars & (1 << i))\n\t\t\tif (__pci_request_region(pdev, i, res_name, excl))\n\t\t\t\tgoto err_out;\n\treturn 0;\n\nerr_out:\n\twhile (--i >= 0)\n\t\tif (bars & (1 << i))\n\t\t\tpci_release_region(pdev, i);\n\n\treturn -EBUSY;\n}\n\n\n \nint pci_request_selected_regions(struct pci_dev *pdev, int bars,\n\t\t\t\t const char *res_name)\n{\n\treturn __pci_request_selected_regions(pdev, bars, res_name, 0);\n}\nEXPORT_SYMBOL(pci_request_selected_regions);\n\nint pci_request_selected_regions_exclusive(struct pci_dev *pdev, int bars,\n\t\t\t\t\t   const char *res_name)\n{\n\treturn __pci_request_selected_regions(pdev, bars, res_name,\n\t\t\tIORESOURCE_EXCLUSIVE);\n}\nEXPORT_SYMBOL(pci_request_selected_regions_exclusive);\n\n \n\nvoid pci_release_regions(struct pci_dev *pdev)\n{\n\tpci_release_selected_regions(pdev, (1 << PCI_STD_NUM_BARS) - 1);\n}\nEXPORT_SYMBOL(pci_release_regions);\n\n \nint pci_request_regions(struct pci_dev *pdev, const char *res_name)\n{\n\treturn pci_request_selected_regions(pdev,\n\t\t\t((1 << PCI_STD_NUM_BARS) - 1), res_name);\n}\nEXPORT_SYMBOL(pci_request_regions);\n\n \nint pci_request_regions_exclusive(struct pci_dev *pdev, const char *res_name)\n{\n\treturn pci_request_selected_regions_exclusive(pdev,\n\t\t\t\t((1 << PCI_STD_NUM_BARS) - 1), res_name);\n}\nEXPORT_SYMBOL(pci_request_regions_exclusive);\n\n \nint pci_register_io_range(struct fwnode_handle *fwnode, phys_addr_t addr,\n\t\t\tresource_size_t\tsize)\n{\n\tint ret = 0;\n#ifdef PCI_IOBASE\n\tstruct logic_pio_hwaddr *range;\n\n\tif (!size || addr + size < addr)\n\t\treturn -EINVAL;\n\n\trange = kzalloc(sizeof(*range), GFP_ATOMIC);\n\tif (!range)\n\t\treturn -ENOMEM;\n\n\trange->fwnode = fwnode;\n\trange->size = size;\n\trange->hw_start = addr;\n\trange->flags = LOGIC_PIO_CPU_MMIO;\n\n\tret = logic_pio_register_range(range);\n\tif (ret)\n\t\tkfree(range);\n\n\t \n\tif (ret == -EEXIST)\n\t\tret = 0;\n#endif\n\n\treturn ret;\n}\n\nphys_addr_t pci_pio_to_address(unsigned long pio)\n{\n#ifdef PCI_IOBASE\n\tif (pio < MMIO_UPPER_LIMIT)\n\t\treturn logic_pio_to_hwaddr(pio);\n#endif\n\n\treturn (phys_addr_t) OF_BAD_ADDR;\n}\nEXPORT_SYMBOL_GPL(pci_pio_to_address);\n\nunsigned long __weak pci_address_to_pio(phys_addr_t address)\n{\n#ifdef PCI_IOBASE\n\treturn logic_pio_trans_cpuaddr(address);\n#else\n\tif (address > IO_SPACE_LIMIT)\n\t\treturn (unsigned long)-1;\n\n\treturn (unsigned long) address;\n#endif\n}\n\n \n#ifndef pci_remap_iospace\nint pci_remap_iospace(const struct resource *res, phys_addr_t phys_addr)\n{\n#if defined(PCI_IOBASE) && defined(CONFIG_MMU)\n\tunsigned long vaddr = (unsigned long)PCI_IOBASE + res->start;\n\n\tif (!(res->flags & IORESOURCE_IO))\n\t\treturn -EINVAL;\n\n\tif (res->end > IO_SPACE_LIMIT)\n\t\treturn -EINVAL;\n\n\treturn ioremap_page_range(vaddr, vaddr + resource_size(res), phys_addr,\n\t\t\t\t  pgprot_device(PAGE_KERNEL));\n#else\n\t \n\tWARN_ONCE(1, \"This architecture does not support memory mapped I/O\\n\");\n\treturn -ENODEV;\n#endif\n}\nEXPORT_SYMBOL(pci_remap_iospace);\n#endif\n\n \nvoid pci_unmap_iospace(struct resource *res)\n{\n#if defined(PCI_IOBASE) && defined(CONFIG_MMU)\n\tunsigned long vaddr = (unsigned long)PCI_IOBASE + res->start;\n\n\tvunmap_range(vaddr, vaddr + resource_size(res));\n#endif\n}\nEXPORT_SYMBOL(pci_unmap_iospace);\n\nstatic void devm_pci_unmap_iospace(struct device *dev, void *ptr)\n{\n\tstruct resource **res = ptr;\n\n\tpci_unmap_iospace(*res);\n}\n\n \nint devm_pci_remap_iospace(struct device *dev, const struct resource *res,\n\t\t\t   phys_addr_t phys_addr)\n{\n\tconst struct resource **ptr;\n\tint error;\n\n\tptr = devres_alloc(devm_pci_unmap_iospace, sizeof(*ptr), GFP_KERNEL);\n\tif (!ptr)\n\t\treturn -ENOMEM;\n\n\terror = pci_remap_iospace(res, phys_addr);\n\tif (error) {\n\t\tdevres_free(ptr);\n\t} else\t{\n\t\t*ptr = res;\n\t\tdevres_add(dev, ptr);\n\t}\n\n\treturn error;\n}\nEXPORT_SYMBOL(devm_pci_remap_iospace);\n\n \nvoid __iomem *devm_pci_remap_cfgspace(struct device *dev,\n\t\t\t\t      resource_size_t offset,\n\t\t\t\t      resource_size_t size)\n{\n\tvoid __iomem **ptr, *addr;\n\n\tptr = devres_alloc(devm_ioremap_release, sizeof(*ptr), GFP_KERNEL);\n\tif (!ptr)\n\t\treturn NULL;\n\n\taddr = pci_remap_cfgspace(offset, size);\n\tif (addr) {\n\t\t*ptr = addr;\n\t\tdevres_add(dev, ptr);\n\t} else\n\t\tdevres_free(ptr);\n\n\treturn addr;\n}\nEXPORT_SYMBOL(devm_pci_remap_cfgspace);\n\n \nvoid __iomem *devm_pci_remap_cfg_resource(struct device *dev,\n\t\t\t\t\t  struct resource *res)\n{\n\tresource_size_t size;\n\tconst char *name;\n\tvoid __iomem *dest_ptr;\n\n\tBUG_ON(!dev);\n\n\tif (!res || resource_type(res) != IORESOURCE_MEM) {\n\t\tdev_err(dev, \"invalid resource\\n\");\n\t\treturn IOMEM_ERR_PTR(-EINVAL);\n\t}\n\n\tsize = resource_size(res);\n\n\tif (res->name)\n\t\tname = devm_kasprintf(dev, GFP_KERNEL, \"%s %s\", dev_name(dev),\n\t\t\t\t      res->name);\n\telse\n\t\tname = devm_kstrdup(dev, dev_name(dev), GFP_KERNEL);\n\tif (!name)\n\t\treturn IOMEM_ERR_PTR(-ENOMEM);\n\n\tif (!devm_request_mem_region(dev, res->start, size, name)) {\n\t\tdev_err(dev, \"can't request region for resource %pR\\n\", res);\n\t\treturn IOMEM_ERR_PTR(-EBUSY);\n\t}\n\n\tdest_ptr = devm_pci_remap_cfgspace(dev, res->start, size);\n\tif (!dest_ptr) {\n\t\tdev_err(dev, \"ioremap failed for resource %pR\\n\", res);\n\t\tdevm_release_mem_region(dev, res->start, size);\n\t\tdest_ptr = IOMEM_ERR_PTR(-ENOMEM);\n\t}\n\n\treturn dest_ptr;\n}\nEXPORT_SYMBOL(devm_pci_remap_cfg_resource);\n\nstatic void __pci_set_master(struct pci_dev *dev, bool enable)\n{\n\tu16 old_cmd, cmd;\n\n\tpci_read_config_word(dev, PCI_COMMAND, &old_cmd);\n\tif (enable)\n\t\tcmd = old_cmd | PCI_COMMAND_MASTER;\n\telse\n\t\tcmd = old_cmd & ~PCI_COMMAND_MASTER;\n\tif (cmd != old_cmd) {\n\t\tpci_dbg(dev, \"%s bus mastering\\n\",\n\t\t\tenable ? \"enabling\" : \"disabling\");\n\t\tpci_write_config_word(dev, PCI_COMMAND, cmd);\n\t}\n\tdev->is_busmaster = enable;\n}\n\n \nchar * __weak __init pcibios_setup(char *str)\n{\n\treturn str;\n}\n\n \nvoid __weak pcibios_set_master(struct pci_dev *dev)\n{\n\tu8 lat;\n\n\t \n\tif (pci_is_pcie(dev))\n\t\treturn;\n\n\tpci_read_config_byte(dev, PCI_LATENCY_TIMER, &lat);\n\tif (lat < 16)\n\t\tlat = (64 <= pcibios_max_latency) ? 64 : pcibios_max_latency;\n\telse if (lat > pcibios_max_latency)\n\t\tlat = pcibios_max_latency;\n\telse\n\t\treturn;\n\n\tpci_write_config_byte(dev, PCI_LATENCY_TIMER, lat);\n}\n\n \nvoid pci_set_master(struct pci_dev *dev)\n{\n\t__pci_set_master(dev, true);\n\tpcibios_set_master(dev);\n}\nEXPORT_SYMBOL(pci_set_master);\n\n \nvoid pci_clear_master(struct pci_dev *dev)\n{\n\t__pci_set_master(dev, false);\n}\nEXPORT_SYMBOL(pci_clear_master);\n\n \nint pci_set_cacheline_size(struct pci_dev *dev)\n{\n\tu8 cacheline_size;\n\n\tif (!pci_cache_line_size)\n\t\treturn -EINVAL;\n\n\t \n\tpci_read_config_byte(dev, PCI_CACHE_LINE_SIZE, &cacheline_size);\n\tif (cacheline_size >= pci_cache_line_size &&\n\t    (cacheline_size % pci_cache_line_size) == 0)\n\t\treturn 0;\n\n\t \n\tpci_write_config_byte(dev, PCI_CACHE_LINE_SIZE, pci_cache_line_size);\n\t \n\tpci_read_config_byte(dev, PCI_CACHE_LINE_SIZE, &cacheline_size);\n\tif (cacheline_size == pci_cache_line_size)\n\t\treturn 0;\n\n\tpci_dbg(dev, \"cache line size of %d is not supported\\n\",\n\t\t   pci_cache_line_size << 2);\n\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL_GPL(pci_set_cacheline_size);\n\n \nint pci_set_mwi(struct pci_dev *dev)\n{\n#ifdef PCI_DISABLE_MWI\n\treturn 0;\n#else\n\tint rc;\n\tu16 cmd;\n\n\trc = pci_set_cacheline_size(dev);\n\tif (rc)\n\t\treturn rc;\n\n\tpci_read_config_word(dev, PCI_COMMAND, &cmd);\n\tif (!(cmd & PCI_COMMAND_INVALIDATE)) {\n\t\tpci_dbg(dev, \"enabling Mem-Wr-Inval\\n\");\n\t\tcmd |= PCI_COMMAND_INVALIDATE;\n\t\tpci_write_config_word(dev, PCI_COMMAND, cmd);\n\t}\n\treturn 0;\n#endif\n}\nEXPORT_SYMBOL(pci_set_mwi);\n\n \nint pcim_set_mwi(struct pci_dev *dev)\n{\n\tstruct pci_devres *dr;\n\n\tdr = find_pci_dr(dev);\n\tif (!dr)\n\t\treturn -ENOMEM;\n\n\tdr->mwi = 1;\n\treturn pci_set_mwi(dev);\n}\nEXPORT_SYMBOL(pcim_set_mwi);\n\n \nint pci_try_set_mwi(struct pci_dev *dev)\n{\n#ifdef PCI_DISABLE_MWI\n\treturn 0;\n#else\n\treturn pci_set_mwi(dev);\n#endif\n}\nEXPORT_SYMBOL(pci_try_set_mwi);\n\n \nvoid pci_clear_mwi(struct pci_dev *dev)\n{\n#ifndef PCI_DISABLE_MWI\n\tu16 cmd;\n\n\tpci_read_config_word(dev, PCI_COMMAND, &cmd);\n\tif (cmd & PCI_COMMAND_INVALIDATE) {\n\t\tcmd &= ~PCI_COMMAND_INVALIDATE;\n\t\tpci_write_config_word(dev, PCI_COMMAND, cmd);\n\t}\n#endif\n}\nEXPORT_SYMBOL(pci_clear_mwi);\n\n \nvoid pci_disable_parity(struct pci_dev *dev)\n{\n\tu16 cmd;\n\n\tpci_read_config_word(dev, PCI_COMMAND, &cmd);\n\tif (cmd & PCI_COMMAND_PARITY) {\n\t\tcmd &= ~PCI_COMMAND_PARITY;\n\t\tpci_write_config_word(dev, PCI_COMMAND, cmd);\n\t}\n}\n\n \nvoid pci_intx(struct pci_dev *pdev, int enable)\n{\n\tu16 pci_command, new;\n\n\tpci_read_config_word(pdev, PCI_COMMAND, &pci_command);\n\n\tif (enable)\n\t\tnew = pci_command & ~PCI_COMMAND_INTX_DISABLE;\n\telse\n\t\tnew = pci_command | PCI_COMMAND_INTX_DISABLE;\n\n\tif (new != pci_command) {\n\t\tstruct pci_devres *dr;\n\n\t\tpci_write_config_word(pdev, PCI_COMMAND, new);\n\n\t\tdr = find_pci_dr(pdev);\n\t\tif (dr && !dr->restore_intx) {\n\t\t\tdr->restore_intx = 1;\n\t\t\tdr->orig_intx = !enable;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(pci_intx);\n\nstatic bool pci_check_and_set_intx_mask(struct pci_dev *dev, bool mask)\n{\n\tstruct pci_bus *bus = dev->bus;\n\tbool mask_updated = true;\n\tu32 cmd_status_dword;\n\tu16 origcmd, newcmd;\n\tunsigned long flags;\n\tbool irq_pending;\n\n\t \n\tBUILD_BUG_ON(PCI_COMMAND % 4);\n\tBUILD_BUG_ON(PCI_COMMAND + 2 != PCI_STATUS);\n\n\traw_spin_lock_irqsave(&pci_lock, flags);\n\n\tbus->ops->read(bus, dev->devfn, PCI_COMMAND, 4, &cmd_status_dword);\n\n\tirq_pending = (cmd_status_dword >> 16) & PCI_STATUS_INTERRUPT;\n\n\t \n\tif (mask != irq_pending) {\n\t\tmask_updated = false;\n\t\tgoto done;\n\t}\n\n\torigcmd = cmd_status_dword;\n\tnewcmd = origcmd & ~PCI_COMMAND_INTX_DISABLE;\n\tif (mask)\n\t\tnewcmd |= PCI_COMMAND_INTX_DISABLE;\n\tif (newcmd != origcmd)\n\t\tbus->ops->write(bus, dev->devfn, PCI_COMMAND, 2, newcmd);\n\ndone:\n\traw_spin_unlock_irqrestore(&pci_lock, flags);\n\n\treturn mask_updated;\n}\n\n \nbool pci_check_and_mask_intx(struct pci_dev *dev)\n{\n\treturn pci_check_and_set_intx_mask(dev, true);\n}\nEXPORT_SYMBOL_GPL(pci_check_and_mask_intx);\n\n \nbool pci_check_and_unmask_intx(struct pci_dev *dev)\n{\n\treturn pci_check_and_set_intx_mask(dev, false);\n}\nEXPORT_SYMBOL_GPL(pci_check_and_unmask_intx);\n\n \nint pci_wait_for_pending_transaction(struct pci_dev *dev)\n{\n\tif (!pci_is_pcie(dev))\n\t\treturn 1;\n\n\treturn pci_wait_for_pending(dev, pci_pcie_cap(dev) + PCI_EXP_DEVSTA,\n\t\t\t\t    PCI_EXP_DEVSTA_TRPND);\n}\nEXPORT_SYMBOL(pci_wait_for_pending_transaction);\n\n \nint pcie_flr(struct pci_dev *dev)\n{\n\tif (!pci_wait_for_pending_transaction(dev))\n\t\tpci_err(dev, \"timed out waiting for pending transaction; performing function level reset anyway\\n\");\n\n\tpcie_capability_set_word(dev, PCI_EXP_DEVCTL, PCI_EXP_DEVCTL_BCR_FLR);\n\n\tif (dev->imm_ready)\n\t\treturn 0;\n\n\t \n\tmsleep(100);\n\n\treturn pci_dev_wait(dev, \"FLR\", PCIE_RESET_READY_POLL_MS);\n}\nEXPORT_SYMBOL_GPL(pcie_flr);\n\n \nint pcie_reset_flr(struct pci_dev *dev, bool probe)\n{\n\tif (dev->dev_flags & PCI_DEV_FLAGS_NO_FLR_RESET)\n\t\treturn -ENOTTY;\n\n\tif (!(dev->devcap & PCI_EXP_DEVCAP_FLR))\n\t\treturn -ENOTTY;\n\n\tif (probe)\n\t\treturn 0;\n\n\treturn pcie_flr(dev);\n}\nEXPORT_SYMBOL_GPL(pcie_reset_flr);\n\nstatic int pci_af_flr(struct pci_dev *dev, bool probe)\n{\n\tint pos;\n\tu8 cap;\n\n\tpos = pci_find_capability(dev, PCI_CAP_ID_AF);\n\tif (!pos)\n\t\treturn -ENOTTY;\n\n\tif (dev->dev_flags & PCI_DEV_FLAGS_NO_FLR_RESET)\n\t\treturn -ENOTTY;\n\n\tpci_read_config_byte(dev, pos + PCI_AF_CAP, &cap);\n\tif (!(cap & PCI_AF_CAP_TP) || !(cap & PCI_AF_CAP_FLR))\n\t\treturn -ENOTTY;\n\n\tif (probe)\n\t\treturn 0;\n\n\t \n\tif (!pci_wait_for_pending(dev, pos + PCI_AF_CTRL,\n\t\t\t\t PCI_AF_STATUS_TP << 8))\n\t\tpci_err(dev, \"timed out waiting for pending transaction; performing AF function level reset anyway\\n\");\n\n\tpci_write_config_byte(dev, pos + PCI_AF_CTRL, PCI_AF_CTRL_FLR);\n\n\tif (dev->imm_ready)\n\t\treturn 0;\n\n\t \n\tmsleep(100);\n\n\treturn pci_dev_wait(dev, \"AF_FLR\", PCIE_RESET_READY_POLL_MS);\n}\n\n \nstatic int pci_pm_reset(struct pci_dev *dev, bool probe)\n{\n\tu16 csr;\n\n\tif (!dev->pm_cap || dev->dev_flags & PCI_DEV_FLAGS_NO_PM_RESET)\n\t\treturn -ENOTTY;\n\n\tpci_read_config_word(dev, dev->pm_cap + PCI_PM_CTRL, &csr);\n\tif (csr & PCI_PM_CTRL_NO_SOFT_RESET)\n\t\treturn -ENOTTY;\n\n\tif (probe)\n\t\treturn 0;\n\n\tif (dev->current_state != PCI_D0)\n\t\treturn -EINVAL;\n\n\tcsr &= ~PCI_PM_CTRL_STATE_MASK;\n\tcsr |= PCI_D3hot;\n\tpci_write_config_word(dev, dev->pm_cap + PCI_PM_CTRL, csr);\n\tpci_dev_d3_sleep(dev);\n\n\tcsr &= ~PCI_PM_CTRL_STATE_MASK;\n\tcsr |= PCI_D0;\n\tpci_write_config_word(dev, dev->pm_cap + PCI_PM_CTRL, csr);\n\tpci_dev_d3_sleep(dev);\n\n\treturn pci_dev_wait(dev, \"PM D3hot->D0\", PCIE_RESET_READY_POLL_MS);\n}\n\n \nstatic int pcie_wait_for_link_status(struct pci_dev *pdev,\n\t\t\t\t     bool use_lt, bool active)\n{\n\tu16 lnksta_mask, lnksta_match;\n\tunsigned long end_jiffies;\n\tu16 lnksta;\n\n\tlnksta_mask = use_lt ? PCI_EXP_LNKSTA_LT : PCI_EXP_LNKSTA_DLLLA;\n\tlnksta_match = active ? lnksta_mask : 0;\n\n\tend_jiffies = jiffies + msecs_to_jiffies(PCIE_LINK_RETRAIN_TIMEOUT_MS);\n\tdo {\n\t\tpcie_capability_read_word(pdev, PCI_EXP_LNKSTA, &lnksta);\n\t\tif ((lnksta & lnksta_mask) == lnksta_match)\n\t\t\treturn 0;\n\t\tmsleep(1);\n\t} while (time_before(jiffies, end_jiffies));\n\n\treturn -ETIMEDOUT;\n}\n\n \nint pcie_retrain_link(struct pci_dev *pdev, bool use_lt)\n{\n\tint rc;\n\n\t \n\trc = pcie_wait_for_link_status(pdev, use_lt, !use_lt);\n\tif (rc)\n\t\treturn rc;\n\n\tpcie_capability_set_word(pdev, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_RL);\n\tif (pdev->clear_retrain_link) {\n\t\t \n\t\tpcie_capability_clear_word(pdev, PCI_EXP_LNKCTL, PCI_EXP_LNKCTL_RL);\n\t}\n\n\treturn pcie_wait_for_link_status(pdev, use_lt, !use_lt);\n}\n\n \nstatic bool pcie_wait_for_link_delay(struct pci_dev *pdev, bool active,\n\t\t\t\t     int delay)\n{\n\tint rc;\n\n\t \n\tif (!pdev->link_active_reporting) {\n\t\tmsleep(PCIE_LINK_RETRAIN_TIMEOUT_MS + delay);\n\t\treturn true;\n\t}\n\n\t \n\tif (active)\n\t\tmsleep(20);\n\trc = pcie_wait_for_link_status(pdev, false, active);\n\tif (active) {\n\t\tif (rc)\n\t\t\trc = pcie_failed_link_retrain(pdev);\n\t\tif (rc)\n\t\t\treturn false;\n\n\t\tmsleep(delay);\n\t\treturn true;\n\t}\n\n\tif (rc)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nbool pcie_wait_for_link(struct pci_dev *pdev, bool active)\n{\n\treturn pcie_wait_for_link_delay(pdev, active, 100);\n}\n\n \nstatic int pci_bus_max_d3cold_delay(const struct pci_bus *bus)\n{\n\tconst struct pci_dev *pdev;\n\tint min_delay = 100;\n\tint max_delay = 0;\n\n\tlist_for_each_entry(pdev, &bus->devices, bus_list) {\n\t\tif (pdev->d3cold_delay < min_delay)\n\t\t\tmin_delay = pdev->d3cold_delay;\n\t\tif (pdev->d3cold_delay > max_delay)\n\t\t\tmax_delay = pdev->d3cold_delay;\n\t}\n\n\treturn max(min_delay, max_delay);\n}\n\n \nint pci_bridge_wait_for_secondary_bus(struct pci_dev *dev, char *reset_type)\n{\n\tstruct pci_dev *child;\n\tint delay;\n\n\tif (pci_dev_is_disconnected(dev))\n\t\treturn 0;\n\n\tif (!pci_is_bridge(dev))\n\t\treturn 0;\n\n\tdown_read(&pci_bus_sem);\n\n\t \n\tif (!dev->subordinate || list_empty(&dev->subordinate->devices)) {\n\t\tup_read(&pci_bus_sem);\n\t\treturn 0;\n\t}\n\n\t \n\tdelay = pci_bus_max_d3cold_delay(dev->subordinate);\n\tif (!delay) {\n\t\tup_read(&pci_bus_sem);\n\t\treturn 0;\n\t}\n\n\tchild = list_first_entry(&dev->subordinate->devices, struct pci_dev,\n\t\t\t\t bus_list);\n\tup_read(&pci_bus_sem);\n\n\t \n\tif (!pci_is_pcie(dev)) {\n\t\tpci_dbg(dev, \"waiting %d ms for secondary bus\\n\", 1000 + delay);\n\t\tmsleep(1000 + delay);\n\t\treturn 0;\n\t}\n\n\t \n\tif (!pcie_downstream_port(dev))\n\t\treturn 0;\n\n\tif (pcie_get_speed_cap(dev) <= PCIE_SPEED_5_0GT) {\n\t\tu16 status;\n\n\t\tpci_dbg(dev, \"waiting %d ms for downstream link\\n\", delay);\n\t\tmsleep(delay);\n\n\t\tif (!pci_dev_wait(child, reset_type, PCI_RESET_WAIT - delay))\n\t\t\treturn 0;\n\n\t\t \n\t\tif (!dev->link_active_reporting)\n\t\t\treturn -ENOTTY;\n\n\t\tpcie_capability_read_word(dev, PCI_EXP_LNKSTA, &status);\n\t\tif (!(status & PCI_EXP_LNKSTA_DLLLA))\n\t\t\treturn -ENOTTY;\n\n\t\treturn pci_dev_wait(child, reset_type,\n\t\t\t\t    PCIE_RESET_READY_POLL_MS - PCI_RESET_WAIT);\n\t}\n\n\tpci_dbg(dev, \"waiting %d ms for downstream link, after activation\\n\",\n\t\tdelay);\n\tif (!pcie_wait_for_link_delay(dev, true, delay)) {\n\t\t \n\t\tpci_info(dev, \"Data Link Layer Link Active not set in 1000 msec\\n\");\n\t\treturn -ENOTTY;\n\t}\n\n\treturn pci_dev_wait(child, reset_type,\n\t\t\t    PCIE_RESET_READY_POLL_MS - delay);\n}\n\nvoid pci_reset_secondary_bus(struct pci_dev *dev)\n{\n\tu16 ctrl;\n\n\tpci_read_config_word(dev, PCI_BRIDGE_CONTROL, &ctrl);\n\tctrl |= PCI_BRIDGE_CTL_BUS_RESET;\n\tpci_write_config_word(dev, PCI_BRIDGE_CONTROL, ctrl);\n\n\t \n\tmsleep(2);\n\n\tctrl &= ~PCI_BRIDGE_CTL_BUS_RESET;\n\tpci_write_config_word(dev, PCI_BRIDGE_CONTROL, ctrl);\n}\n\nvoid __weak pcibios_reset_secondary_bus(struct pci_dev *dev)\n{\n\tpci_reset_secondary_bus(dev);\n}\n\n \nint pci_bridge_secondary_bus_reset(struct pci_dev *dev)\n{\n\tpcibios_reset_secondary_bus(dev);\n\n\treturn pci_bridge_wait_for_secondary_bus(dev, \"bus reset\");\n}\nEXPORT_SYMBOL_GPL(pci_bridge_secondary_bus_reset);\n\nstatic int pci_parent_bus_reset(struct pci_dev *dev, bool probe)\n{\n\tstruct pci_dev *pdev;\n\n\tif (pci_is_root_bus(dev->bus) || dev->subordinate ||\n\t    !dev->bus->self || dev->dev_flags & PCI_DEV_FLAGS_NO_BUS_RESET)\n\t\treturn -ENOTTY;\n\n\tlist_for_each_entry(pdev, &dev->bus->devices, bus_list)\n\t\tif (pdev != dev)\n\t\t\treturn -ENOTTY;\n\n\tif (probe)\n\t\treturn 0;\n\n\treturn pci_bridge_secondary_bus_reset(dev->bus->self);\n}\n\nstatic int pci_reset_hotplug_slot(struct hotplug_slot *hotplug, bool probe)\n{\n\tint rc = -ENOTTY;\n\n\tif (!hotplug || !try_module_get(hotplug->owner))\n\t\treturn rc;\n\n\tif (hotplug->ops->reset_slot)\n\t\trc = hotplug->ops->reset_slot(hotplug, probe);\n\n\tmodule_put(hotplug->owner);\n\n\treturn rc;\n}\n\nstatic int pci_dev_reset_slot_function(struct pci_dev *dev, bool probe)\n{\n\tif (dev->multifunction || dev->subordinate || !dev->slot ||\n\t    dev->dev_flags & PCI_DEV_FLAGS_NO_BUS_RESET)\n\t\treturn -ENOTTY;\n\n\treturn pci_reset_hotplug_slot(dev->slot->hotplug, probe);\n}\n\nstatic int pci_reset_bus_function(struct pci_dev *dev, bool probe)\n{\n\tint rc;\n\n\trc = pci_dev_reset_slot_function(dev, probe);\n\tif (rc != -ENOTTY)\n\t\treturn rc;\n\treturn pci_parent_bus_reset(dev, probe);\n}\n\nvoid pci_dev_lock(struct pci_dev *dev)\n{\n\t \n\tdevice_lock(&dev->dev);\n\tpci_cfg_access_lock(dev);\n}\nEXPORT_SYMBOL_GPL(pci_dev_lock);\n\n \nint pci_dev_trylock(struct pci_dev *dev)\n{\n\tif (device_trylock(&dev->dev)) {\n\t\tif (pci_cfg_access_trylock(dev))\n\t\t\treturn 1;\n\t\tdevice_unlock(&dev->dev);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(pci_dev_trylock);\n\nvoid pci_dev_unlock(struct pci_dev *dev)\n{\n\tpci_cfg_access_unlock(dev);\n\tdevice_unlock(&dev->dev);\n}\nEXPORT_SYMBOL_GPL(pci_dev_unlock);\n\nstatic void pci_dev_save_and_disable(struct pci_dev *dev)\n{\n\tconst struct pci_error_handlers *err_handler =\n\t\t\tdev->driver ? dev->driver->err_handler : NULL;\n\n\t \n\tif (err_handler && err_handler->reset_prepare)\n\t\terr_handler->reset_prepare(dev);\n\n\t \n\tpci_set_power_state(dev, PCI_D0);\n\n\tpci_save_state(dev);\n\t \n\tpci_write_config_word(dev, PCI_COMMAND, PCI_COMMAND_INTX_DISABLE);\n}\n\nstatic void pci_dev_restore(struct pci_dev *dev)\n{\n\tconst struct pci_error_handlers *err_handler =\n\t\t\tdev->driver ? dev->driver->err_handler : NULL;\n\n\tpci_restore_state(dev);\n\n\t \n\tif (err_handler && err_handler->reset_done)\n\t\terr_handler->reset_done(dev);\n}\n\n \nstatic const struct pci_reset_fn_method pci_reset_fn_methods[] = {\n\t{ },\n\t{ pci_dev_specific_reset, .name = \"device_specific\" },\n\t{ pci_dev_acpi_reset, .name = \"acpi\" },\n\t{ pcie_reset_flr, .name = \"flr\" },\n\t{ pci_af_flr, .name = \"af_flr\" },\n\t{ pci_pm_reset, .name = \"pm\" },\n\t{ pci_reset_bus_function, .name = \"bus\" },\n};\n\nstatic ssize_t reset_method_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tssize_t len = 0;\n\tint i, m;\n\n\tfor (i = 0; i < PCI_NUM_RESET_METHODS; i++) {\n\t\tm = pdev->reset_methods[i];\n\t\tif (!m)\n\t\t\tbreak;\n\n\t\tlen += sysfs_emit_at(buf, len, \"%s%s\", len ? \" \" : \"\",\n\t\t\t\t     pci_reset_fn_methods[m].name);\n\t}\n\n\tif (len)\n\t\tlen += sysfs_emit_at(buf, len, \"\\n\");\n\n\treturn len;\n}\n\nstatic int reset_method_lookup(const char *name)\n{\n\tint m;\n\n\tfor (m = 1; m < PCI_NUM_RESET_METHODS; m++) {\n\t\tif (sysfs_streq(name, pci_reset_fn_methods[m].name))\n\t\t\treturn m;\n\t}\n\n\treturn 0;\t \n}\n\nstatic ssize_t reset_method_store(struct device *dev,\n\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t  const char *buf, size_t count)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tchar *options, *name;\n\tint m, n;\n\tu8 reset_methods[PCI_NUM_RESET_METHODS] = { 0 };\n\n\tif (sysfs_streq(buf, \"\")) {\n\t\tpdev->reset_methods[0] = 0;\n\t\tpci_warn(pdev, \"All device reset methods disabled by user\");\n\t\treturn count;\n\t}\n\n\tif (sysfs_streq(buf, \"default\")) {\n\t\tpci_init_reset_methods(pdev);\n\t\treturn count;\n\t}\n\n\toptions = kstrndup(buf, count, GFP_KERNEL);\n\tif (!options)\n\t\treturn -ENOMEM;\n\n\tn = 0;\n\twhile ((name = strsep(&options, \" \")) != NULL) {\n\t\tif (sysfs_streq(name, \"\"))\n\t\t\tcontinue;\n\n\t\tname = strim(name);\n\n\t\tm = reset_method_lookup(name);\n\t\tif (!m) {\n\t\t\tpci_err(pdev, \"Invalid reset method '%s'\", name);\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (pci_reset_fn_methods[m].reset_fn(pdev, PCI_RESET_PROBE)) {\n\t\t\tpci_err(pdev, \"Unsupported reset method '%s'\", name);\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (n == PCI_NUM_RESET_METHODS - 1) {\n\t\t\tpci_err(pdev, \"Too many reset methods\\n\");\n\t\t\tgoto error;\n\t\t}\n\n\t\treset_methods[n++] = m;\n\t}\n\n\treset_methods[n] = 0;\n\n\t \n\tif (pci_reset_fn_methods[1].reset_fn(pdev, PCI_RESET_PROBE) == 0 &&\n\t    reset_methods[0] != 1)\n\t\tpci_warn(pdev, \"Device-specific reset disabled/de-prioritized by user\");\n\tmemcpy(pdev->reset_methods, reset_methods, sizeof(pdev->reset_methods));\n\tkfree(options);\n\treturn count;\n\nerror:\n\t \n\tkfree(options);\n\treturn -EINVAL;\n}\nstatic DEVICE_ATTR_RW(reset_method);\n\nstatic struct attribute *pci_dev_reset_method_attrs[] = {\n\t&dev_attr_reset_method.attr,\n\tNULL,\n};\n\nstatic umode_t pci_dev_reset_method_attr_is_visible(struct kobject *kobj,\n\t\t\t\t\t\t    struct attribute *a, int n)\n{\n\tstruct pci_dev *pdev = to_pci_dev(kobj_to_dev(kobj));\n\n\tif (!pci_reset_supported(pdev))\n\t\treturn 0;\n\n\treturn a->mode;\n}\n\nconst struct attribute_group pci_dev_reset_method_attr_group = {\n\t.attrs = pci_dev_reset_method_attrs,\n\t.is_visible = pci_dev_reset_method_attr_is_visible,\n};\n\n \nint __pci_reset_function_locked(struct pci_dev *dev)\n{\n\tint i, m, rc;\n\n\tmight_sleep();\n\n\t \n\tfor (i = 0; i < PCI_NUM_RESET_METHODS; i++) {\n\t\tm = dev->reset_methods[i];\n\t\tif (!m)\n\t\t\treturn -ENOTTY;\n\n\t\trc = pci_reset_fn_methods[m].reset_fn(dev, PCI_RESET_DO_RESET);\n\t\tif (!rc)\n\t\t\treturn 0;\n\t\tif (rc != -ENOTTY)\n\t\t\treturn rc;\n\t}\n\n\treturn -ENOTTY;\n}\nEXPORT_SYMBOL_GPL(__pci_reset_function_locked);\n\n \nvoid pci_init_reset_methods(struct pci_dev *dev)\n{\n\tint m, i, rc;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(pci_reset_fn_methods) != PCI_NUM_RESET_METHODS);\n\n\tmight_sleep();\n\n\ti = 0;\n\tfor (m = 1; m < PCI_NUM_RESET_METHODS; m++) {\n\t\trc = pci_reset_fn_methods[m].reset_fn(dev, PCI_RESET_PROBE);\n\t\tif (!rc)\n\t\t\tdev->reset_methods[i++] = m;\n\t\telse if (rc != -ENOTTY)\n\t\t\tbreak;\n\t}\n\n\tdev->reset_methods[i] = 0;\n}\n\n \nint pci_reset_function(struct pci_dev *dev)\n{\n\tint rc;\n\n\tif (!pci_reset_supported(dev))\n\t\treturn -ENOTTY;\n\n\tpci_dev_lock(dev);\n\tpci_dev_save_and_disable(dev);\n\n\trc = __pci_reset_function_locked(dev);\n\n\tpci_dev_restore(dev);\n\tpci_dev_unlock(dev);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(pci_reset_function);\n\n \nint pci_reset_function_locked(struct pci_dev *dev)\n{\n\tint rc;\n\n\tif (!pci_reset_supported(dev))\n\t\treturn -ENOTTY;\n\n\tpci_dev_save_and_disable(dev);\n\n\trc = __pci_reset_function_locked(dev);\n\n\tpci_dev_restore(dev);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(pci_reset_function_locked);\n\n \nint pci_try_reset_function(struct pci_dev *dev)\n{\n\tint rc;\n\n\tif (!pci_reset_supported(dev))\n\t\treturn -ENOTTY;\n\n\tif (!pci_dev_trylock(dev))\n\t\treturn -EAGAIN;\n\n\tpci_dev_save_and_disable(dev);\n\trc = __pci_reset_function_locked(dev);\n\tpci_dev_restore(dev);\n\tpci_dev_unlock(dev);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(pci_try_reset_function);\n\n \nstatic bool pci_bus_resettable(struct pci_bus *bus)\n{\n\tstruct pci_dev *dev;\n\n\n\tif (bus->self && (bus->self->dev_flags & PCI_DEV_FLAGS_NO_BUS_RESET))\n\t\treturn false;\n\n\tlist_for_each_entry(dev, &bus->devices, bus_list) {\n\t\tif (dev->dev_flags & PCI_DEV_FLAGS_NO_BUS_RESET ||\n\t\t    (dev->subordinate && !pci_bus_resettable(dev->subordinate)))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic void pci_bus_lock(struct pci_bus *bus)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &bus->devices, bus_list) {\n\t\tpci_dev_lock(dev);\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_lock(dev->subordinate);\n\t}\n}\n\n \nstatic void pci_bus_unlock(struct pci_bus *bus)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &bus->devices, bus_list) {\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_unlock(dev->subordinate);\n\t\tpci_dev_unlock(dev);\n\t}\n}\n\n \nstatic int pci_bus_trylock(struct pci_bus *bus)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &bus->devices, bus_list) {\n\t\tif (!pci_dev_trylock(dev))\n\t\t\tgoto unlock;\n\t\tif (dev->subordinate) {\n\t\t\tif (!pci_bus_trylock(dev->subordinate)) {\n\t\t\t\tpci_dev_unlock(dev);\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\n\treturn 1;\n\nunlock:\n\tlist_for_each_entry_continue_reverse(dev, &bus->devices, bus_list) {\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_unlock(dev->subordinate);\n\t\tpci_dev_unlock(dev);\n\t}\n\treturn 0;\n}\n\n \nstatic bool pci_slot_resettable(struct pci_slot *slot)\n{\n\tstruct pci_dev *dev;\n\n\tif (slot->bus->self &&\n\t    (slot->bus->self->dev_flags & PCI_DEV_FLAGS_NO_BUS_RESET))\n\t\treturn false;\n\n\tlist_for_each_entry(dev, &slot->bus->devices, bus_list) {\n\t\tif (!dev->slot || dev->slot != slot)\n\t\t\tcontinue;\n\t\tif (dev->dev_flags & PCI_DEV_FLAGS_NO_BUS_RESET ||\n\t\t    (dev->subordinate && !pci_bus_resettable(dev->subordinate)))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic void pci_slot_lock(struct pci_slot *slot)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &slot->bus->devices, bus_list) {\n\t\tif (!dev->slot || dev->slot != slot)\n\t\t\tcontinue;\n\t\tpci_dev_lock(dev);\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_lock(dev->subordinate);\n\t}\n}\n\n \nstatic void pci_slot_unlock(struct pci_slot *slot)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &slot->bus->devices, bus_list) {\n\t\tif (!dev->slot || dev->slot != slot)\n\t\t\tcontinue;\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_unlock(dev->subordinate);\n\t\tpci_dev_unlock(dev);\n\t}\n}\n\n \nstatic int pci_slot_trylock(struct pci_slot *slot)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &slot->bus->devices, bus_list) {\n\t\tif (!dev->slot || dev->slot != slot)\n\t\t\tcontinue;\n\t\tif (!pci_dev_trylock(dev))\n\t\t\tgoto unlock;\n\t\tif (dev->subordinate) {\n\t\t\tif (!pci_bus_trylock(dev->subordinate)) {\n\t\t\t\tpci_dev_unlock(dev);\n\t\t\t\tgoto unlock;\n\t\t\t}\n\t\t}\n\t}\n\treturn 1;\n\nunlock:\n\tlist_for_each_entry_continue_reverse(dev,\n\t\t\t\t\t     &slot->bus->devices, bus_list) {\n\t\tif (!dev->slot || dev->slot != slot)\n\t\t\tcontinue;\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_unlock(dev->subordinate);\n\t\tpci_dev_unlock(dev);\n\t}\n\treturn 0;\n}\n\n \nstatic void pci_bus_save_and_disable_locked(struct pci_bus *bus)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &bus->devices, bus_list) {\n\t\tpci_dev_save_and_disable(dev);\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_save_and_disable_locked(dev->subordinate);\n\t}\n}\n\n \nstatic void pci_bus_restore_locked(struct pci_bus *bus)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &bus->devices, bus_list) {\n\t\tpci_dev_restore(dev);\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_restore_locked(dev->subordinate);\n\t}\n}\n\n \nstatic void pci_slot_save_and_disable_locked(struct pci_slot *slot)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &slot->bus->devices, bus_list) {\n\t\tif (!dev->slot || dev->slot != slot)\n\t\t\tcontinue;\n\t\tpci_dev_save_and_disable(dev);\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_save_and_disable_locked(dev->subordinate);\n\t}\n}\n\n \nstatic void pci_slot_restore_locked(struct pci_slot *slot)\n{\n\tstruct pci_dev *dev;\n\n\tlist_for_each_entry(dev, &slot->bus->devices, bus_list) {\n\t\tif (!dev->slot || dev->slot != slot)\n\t\t\tcontinue;\n\t\tpci_dev_restore(dev);\n\t\tif (dev->subordinate)\n\t\t\tpci_bus_restore_locked(dev->subordinate);\n\t}\n}\n\nstatic int pci_slot_reset(struct pci_slot *slot, bool probe)\n{\n\tint rc;\n\n\tif (!slot || !pci_slot_resettable(slot))\n\t\treturn -ENOTTY;\n\n\tif (!probe)\n\t\tpci_slot_lock(slot);\n\n\tmight_sleep();\n\n\trc = pci_reset_hotplug_slot(slot->hotplug, probe);\n\n\tif (!probe)\n\t\tpci_slot_unlock(slot);\n\n\treturn rc;\n}\n\n \nint pci_probe_reset_slot(struct pci_slot *slot)\n{\n\treturn pci_slot_reset(slot, PCI_RESET_PROBE);\n}\nEXPORT_SYMBOL_GPL(pci_probe_reset_slot);\n\n \nstatic int __pci_reset_slot(struct pci_slot *slot)\n{\n\tint rc;\n\n\trc = pci_slot_reset(slot, PCI_RESET_PROBE);\n\tif (rc)\n\t\treturn rc;\n\n\tif (pci_slot_trylock(slot)) {\n\t\tpci_slot_save_and_disable_locked(slot);\n\t\tmight_sleep();\n\t\trc = pci_reset_hotplug_slot(slot->hotplug, PCI_RESET_DO_RESET);\n\t\tpci_slot_restore_locked(slot);\n\t\tpci_slot_unlock(slot);\n\t} else\n\t\trc = -EAGAIN;\n\n\treturn rc;\n}\n\nstatic int pci_bus_reset(struct pci_bus *bus, bool probe)\n{\n\tint ret;\n\n\tif (!bus->self || !pci_bus_resettable(bus))\n\t\treturn -ENOTTY;\n\n\tif (probe)\n\t\treturn 0;\n\n\tpci_bus_lock(bus);\n\n\tmight_sleep();\n\n\tret = pci_bridge_secondary_bus_reset(bus->self);\n\n\tpci_bus_unlock(bus);\n\n\treturn ret;\n}\n\n \nint pci_bus_error_reset(struct pci_dev *bridge)\n{\n\tstruct pci_bus *bus = bridge->subordinate;\n\tstruct pci_slot *slot;\n\n\tif (!bus)\n\t\treturn -ENOTTY;\n\n\tmutex_lock(&pci_slot_mutex);\n\tif (list_empty(&bus->slots))\n\t\tgoto bus_reset;\n\n\tlist_for_each_entry(slot, &bus->slots, list)\n\t\tif (pci_probe_reset_slot(slot))\n\t\t\tgoto bus_reset;\n\n\tlist_for_each_entry(slot, &bus->slots, list)\n\t\tif (pci_slot_reset(slot, PCI_RESET_DO_RESET))\n\t\t\tgoto bus_reset;\n\n\tmutex_unlock(&pci_slot_mutex);\n\treturn 0;\nbus_reset:\n\tmutex_unlock(&pci_slot_mutex);\n\treturn pci_bus_reset(bridge->subordinate, PCI_RESET_DO_RESET);\n}\n\n \nint pci_probe_reset_bus(struct pci_bus *bus)\n{\n\treturn pci_bus_reset(bus, PCI_RESET_PROBE);\n}\nEXPORT_SYMBOL_GPL(pci_probe_reset_bus);\n\n \nstatic int __pci_reset_bus(struct pci_bus *bus)\n{\n\tint rc;\n\n\trc = pci_bus_reset(bus, PCI_RESET_PROBE);\n\tif (rc)\n\t\treturn rc;\n\n\tif (pci_bus_trylock(bus)) {\n\t\tpci_bus_save_and_disable_locked(bus);\n\t\tmight_sleep();\n\t\trc = pci_bridge_secondary_bus_reset(bus->self);\n\t\tpci_bus_restore_locked(bus);\n\t\tpci_bus_unlock(bus);\n\t} else\n\t\trc = -EAGAIN;\n\n\treturn rc;\n}\n\n \nint pci_reset_bus(struct pci_dev *pdev)\n{\n\treturn (!pci_probe_reset_slot(pdev->slot)) ?\n\t    __pci_reset_slot(pdev->slot) : __pci_reset_bus(pdev->bus);\n}\nEXPORT_SYMBOL_GPL(pci_reset_bus);\n\n \nint pcix_get_max_mmrbc(struct pci_dev *dev)\n{\n\tint cap;\n\tu32 stat;\n\n\tcap = pci_find_capability(dev, PCI_CAP_ID_PCIX);\n\tif (!cap)\n\t\treturn -EINVAL;\n\n\tif (pci_read_config_dword(dev, cap + PCI_X_STATUS, &stat))\n\t\treturn -EINVAL;\n\n\treturn 512 << ((stat & PCI_X_STATUS_MAX_READ) >> 21);\n}\nEXPORT_SYMBOL(pcix_get_max_mmrbc);\n\n \nint pcix_get_mmrbc(struct pci_dev *dev)\n{\n\tint cap;\n\tu16 cmd;\n\n\tcap = pci_find_capability(dev, PCI_CAP_ID_PCIX);\n\tif (!cap)\n\t\treturn -EINVAL;\n\n\tif (pci_read_config_word(dev, cap + PCI_X_CMD, &cmd))\n\t\treturn -EINVAL;\n\n\treturn 512 << ((cmd & PCI_X_CMD_MAX_READ) >> 2);\n}\nEXPORT_SYMBOL(pcix_get_mmrbc);\n\n \nint pcix_set_mmrbc(struct pci_dev *dev, int mmrbc)\n{\n\tint cap;\n\tu32 stat, v, o;\n\tu16 cmd;\n\n\tif (mmrbc < 512 || mmrbc > 4096 || !is_power_of_2(mmrbc))\n\t\treturn -EINVAL;\n\n\tv = ffs(mmrbc) - 10;\n\n\tcap = pci_find_capability(dev, PCI_CAP_ID_PCIX);\n\tif (!cap)\n\t\treturn -EINVAL;\n\n\tif (pci_read_config_dword(dev, cap + PCI_X_STATUS, &stat))\n\t\treturn -EINVAL;\n\n\tif (v > (stat & PCI_X_STATUS_MAX_READ) >> 21)\n\t\treturn -E2BIG;\n\n\tif (pci_read_config_word(dev, cap + PCI_X_CMD, &cmd))\n\t\treturn -EINVAL;\n\n\to = (cmd & PCI_X_CMD_MAX_READ) >> 2;\n\tif (o != v) {\n\t\tif (v > o && (dev->bus->bus_flags & PCI_BUS_FLAGS_NO_MMRBC))\n\t\t\treturn -EIO;\n\n\t\tcmd &= ~PCI_X_CMD_MAX_READ;\n\t\tcmd |= v << 2;\n\t\tif (pci_write_config_word(dev, cap + PCI_X_CMD, cmd))\n\t\t\treturn -EIO;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(pcix_set_mmrbc);\n\n \nint pcie_get_readrq(struct pci_dev *dev)\n{\n\tu16 ctl;\n\n\tpcie_capability_read_word(dev, PCI_EXP_DEVCTL, &ctl);\n\n\treturn 128 << ((ctl & PCI_EXP_DEVCTL_READRQ) >> 12);\n}\nEXPORT_SYMBOL(pcie_get_readrq);\n\n \nint pcie_set_readrq(struct pci_dev *dev, int rq)\n{\n\tu16 v;\n\tint ret;\n\tstruct pci_host_bridge *bridge = pci_find_host_bridge(dev->bus);\n\n\tif (rq < 128 || rq > 4096 || !is_power_of_2(rq))\n\t\treturn -EINVAL;\n\n\t \n\tif (pcie_bus_config == PCIE_BUS_PERFORMANCE) {\n\t\tint mps = pcie_get_mps(dev);\n\n\t\tif (mps < rq)\n\t\t\trq = mps;\n\t}\n\n\tv = (ffs(rq) - 8) << 12;\n\n\tif (bridge->no_inc_mrrs) {\n\t\tint max_mrrs = pcie_get_readrq(dev);\n\n\t\tif (rq > max_mrrs) {\n\t\t\tpci_info(dev, \"can't set Max_Read_Request_Size to %d; max is %d\\n\", rq, max_mrrs);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tret = pcie_capability_clear_and_set_word(dev, PCI_EXP_DEVCTL,\n\t\t\t\t\t\t  PCI_EXP_DEVCTL_READRQ, v);\n\n\treturn pcibios_err_to_errno(ret);\n}\nEXPORT_SYMBOL(pcie_set_readrq);\n\n \nint pcie_get_mps(struct pci_dev *dev)\n{\n\tu16 ctl;\n\n\tpcie_capability_read_word(dev, PCI_EXP_DEVCTL, &ctl);\n\n\treturn 128 << ((ctl & PCI_EXP_DEVCTL_PAYLOAD) >> 5);\n}\nEXPORT_SYMBOL(pcie_get_mps);\n\n \nint pcie_set_mps(struct pci_dev *dev, int mps)\n{\n\tu16 v;\n\tint ret;\n\n\tif (mps < 128 || mps > 4096 || !is_power_of_2(mps))\n\t\treturn -EINVAL;\n\n\tv = ffs(mps) - 8;\n\tif (v > dev->pcie_mpss)\n\t\treturn -EINVAL;\n\tv <<= 5;\n\n\tret = pcie_capability_clear_and_set_word(dev, PCI_EXP_DEVCTL,\n\t\t\t\t\t\t  PCI_EXP_DEVCTL_PAYLOAD, v);\n\n\treturn pcibios_err_to_errno(ret);\n}\nEXPORT_SYMBOL(pcie_set_mps);\n\n \nu32 pcie_bandwidth_available(struct pci_dev *dev, struct pci_dev **limiting_dev,\n\t\t\t     enum pci_bus_speed *speed,\n\t\t\t     enum pcie_link_width *width)\n{\n\tu16 lnksta;\n\tenum pci_bus_speed next_speed;\n\tenum pcie_link_width next_width;\n\tu32 bw, next_bw;\n\n\tif (speed)\n\t\t*speed = PCI_SPEED_UNKNOWN;\n\tif (width)\n\t\t*width = PCIE_LNK_WIDTH_UNKNOWN;\n\n\tbw = 0;\n\n\twhile (dev) {\n\t\tpcie_capability_read_word(dev, PCI_EXP_LNKSTA, &lnksta);\n\n\t\tnext_speed = pcie_link_speed[lnksta & PCI_EXP_LNKSTA_CLS];\n\t\tnext_width = FIELD_GET(PCI_EXP_LNKSTA_NLW, lnksta);\n\n\t\tnext_bw = next_width * PCIE_SPEED2MBS_ENC(next_speed);\n\n\t\t \n\t\tif (!bw || next_bw <= bw) {\n\t\t\tbw = next_bw;\n\n\t\t\tif (limiting_dev)\n\t\t\t\t*limiting_dev = dev;\n\t\t\tif (speed)\n\t\t\t\t*speed = next_speed;\n\t\t\tif (width)\n\t\t\t\t*width = next_width;\n\t\t}\n\n\t\tdev = pci_upstream_bridge(dev);\n\t}\n\n\treturn bw;\n}\nEXPORT_SYMBOL(pcie_bandwidth_available);\n\n \nenum pci_bus_speed pcie_get_speed_cap(struct pci_dev *dev)\n{\n\tu32 lnkcap2, lnkcap;\n\n\t \n\tpcie_capability_read_dword(dev, PCI_EXP_LNKCAP2, &lnkcap2);\n\n\t \n\tif (lnkcap2)\n\t\treturn PCIE_LNKCAP2_SLS2SPEED(lnkcap2);\n\n\tpcie_capability_read_dword(dev, PCI_EXP_LNKCAP, &lnkcap);\n\tif ((lnkcap & PCI_EXP_LNKCAP_SLS) == PCI_EXP_LNKCAP_SLS_5_0GB)\n\t\treturn PCIE_SPEED_5_0GT;\n\telse if ((lnkcap & PCI_EXP_LNKCAP_SLS) == PCI_EXP_LNKCAP_SLS_2_5GB)\n\t\treturn PCIE_SPEED_2_5GT;\n\n\treturn PCI_SPEED_UNKNOWN;\n}\nEXPORT_SYMBOL(pcie_get_speed_cap);\n\n \nenum pcie_link_width pcie_get_width_cap(struct pci_dev *dev)\n{\n\tu32 lnkcap;\n\n\tpcie_capability_read_dword(dev, PCI_EXP_LNKCAP, &lnkcap);\n\tif (lnkcap)\n\t\treturn FIELD_GET(PCI_EXP_LNKCAP_MLW, lnkcap);\n\n\treturn PCIE_LNK_WIDTH_UNKNOWN;\n}\nEXPORT_SYMBOL(pcie_get_width_cap);\n\n \nu32 pcie_bandwidth_capable(struct pci_dev *dev, enum pci_bus_speed *speed,\n\t\t\t   enum pcie_link_width *width)\n{\n\t*speed = pcie_get_speed_cap(dev);\n\t*width = pcie_get_width_cap(dev);\n\n\tif (*speed == PCI_SPEED_UNKNOWN || *width == PCIE_LNK_WIDTH_UNKNOWN)\n\t\treturn 0;\n\n\treturn *width * PCIE_SPEED2MBS_ENC(*speed);\n}\n\n \nvoid __pcie_print_link_status(struct pci_dev *dev, bool verbose)\n{\n\tenum pcie_link_width width, width_cap;\n\tenum pci_bus_speed speed, speed_cap;\n\tstruct pci_dev *limiting_dev = NULL;\n\tu32 bw_avail, bw_cap;\n\n\tbw_cap = pcie_bandwidth_capable(dev, &speed_cap, &width_cap);\n\tbw_avail = pcie_bandwidth_available(dev, &limiting_dev, &speed, &width);\n\n\tif (bw_avail >= bw_cap && verbose)\n\t\tpci_info(dev, \"%u.%03u Gb/s available PCIe bandwidth (%s x%d link)\\n\",\n\t\t\t bw_cap / 1000, bw_cap % 1000,\n\t\t\t pci_speed_string(speed_cap), width_cap);\n\telse if (bw_avail < bw_cap)\n\t\tpci_info(dev, \"%u.%03u Gb/s available PCIe bandwidth, limited by %s x%d link at %s (capable of %u.%03u Gb/s with %s x%d link)\\n\",\n\t\t\t bw_avail / 1000, bw_avail % 1000,\n\t\t\t pci_speed_string(speed), width,\n\t\t\t limiting_dev ? pci_name(limiting_dev) : \"<unknown>\",\n\t\t\t bw_cap / 1000, bw_cap % 1000,\n\t\t\t pci_speed_string(speed_cap), width_cap);\n}\n\n \nvoid pcie_print_link_status(struct pci_dev *dev)\n{\n\t__pcie_print_link_status(dev, true);\n}\nEXPORT_SYMBOL(pcie_print_link_status);\n\n \nint pci_select_bars(struct pci_dev *dev, unsigned long flags)\n{\n\tint i, bars = 0;\n\tfor (i = 0; i < PCI_NUM_RESOURCES; i++)\n\t\tif (pci_resource_flags(dev, i) & flags)\n\t\t\tbars |= (1 << i);\n\treturn bars;\n}\nEXPORT_SYMBOL(pci_select_bars);\n\n \nstatic arch_set_vga_state_t arch_set_vga_state;\n\nvoid __init pci_register_set_vga_state(arch_set_vga_state_t func)\n{\n\tarch_set_vga_state = func;\t \n}\n\nstatic int pci_set_vga_state_arch(struct pci_dev *dev, bool decode,\n\t\t\t\t  unsigned int command_bits, u32 flags)\n{\n\tif (arch_set_vga_state)\n\t\treturn arch_set_vga_state(dev, decode, command_bits,\n\t\t\t\t\t\tflags);\n\treturn 0;\n}\n\n \nint pci_set_vga_state(struct pci_dev *dev, bool decode,\n\t\t      unsigned int command_bits, u32 flags)\n{\n\tstruct pci_bus *bus;\n\tstruct pci_dev *bridge;\n\tu16 cmd;\n\tint rc;\n\n\tWARN_ON((flags & PCI_VGA_STATE_CHANGE_DECODES) && (command_bits & ~(PCI_COMMAND_IO|PCI_COMMAND_MEMORY)));\n\n\t \n\trc = pci_set_vga_state_arch(dev, decode, command_bits, flags);\n\tif (rc)\n\t\treturn rc;\n\n\tif (flags & PCI_VGA_STATE_CHANGE_DECODES) {\n\t\tpci_read_config_word(dev, PCI_COMMAND, &cmd);\n\t\tif (decode)\n\t\t\tcmd |= command_bits;\n\t\telse\n\t\t\tcmd &= ~command_bits;\n\t\tpci_write_config_word(dev, PCI_COMMAND, cmd);\n\t}\n\n\tif (!(flags & PCI_VGA_STATE_CHANGE_BRIDGE))\n\t\treturn 0;\n\n\tbus = dev->bus;\n\twhile (bus) {\n\t\tbridge = bus->self;\n\t\tif (bridge) {\n\t\t\tpci_read_config_word(bridge, PCI_BRIDGE_CONTROL,\n\t\t\t\t\t     &cmd);\n\t\t\tif (decode)\n\t\t\t\tcmd |= PCI_BRIDGE_CTL_VGA;\n\t\t\telse\n\t\t\t\tcmd &= ~PCI_BRIDGE_CTL_VGA;\n\t\t\tpci_write_config_word(bridge, PCI_BRIDGE_CONTROL,\n\t\t\t\t\t      cmd);\n\t\t}\n\t\tbus = bus->parent;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_ACPI\nbool pci_pr3_present(struct pci_dev *pdev)\n{\n\tstruct acpi_device *adev;\n\n\tif (acpi_disabled)\n\t\treturn false;\n\n\tadev = ACPI_COMPANION(&pdev->dev);\n\tif (!adev)\n\t\treturn false;\n\n\treturn adev->power.flags.power_resources &&\n\t\tacpi_has_method(adev->handle, \"_PR3\");\n}\nEXPORT_SYMBOL_GPL(pci_pr3_present);\n#endif\n\n \nvoid pci_add_dma_alias(struct pci_dev *dev, u8 devfn_from,\n\t\t       unsigned int nr_devfns)\n{\n\tint devfn_to;\n\n\tnr_devfns = min(nr_devfns, (unsigned int)MAX_NR_DEVFNS - devfn_from);\n\tdevfn_to = devfn_from + nr_devfns - 1;\n\n\tif (!dev->dma_alias_mask)\n\t\tdev->dma_alias_mask = bitmap_zalloc(MAX_NR_DEVFNS, GFP_KERNEL);\n\tif (!dev->dma_alias_mask) {\n\t\tpci_warn(dev, \"Unable to allocate DMA alias mask\\n\");\n\t\treturn;\n\t}\n\n\tbitmap_set(dev->dma_alias_mask, devfn_from, nr_devfns);\n\n\tif (nr_devfns == 1)\n\t\tpci_info(dev, \"Enabling fixed DMA alias to %02x.%d\\n\",\n\t\t\t\tPCI_SLOT(devfn_from), PCI_FUNC(devfn_from));\n\telse if (nr_devfns > 1)\n\t\tpci_info(dev, \"Enabling fixed DMA alias for devfn range from %02x.%d to %02x.%d\\n\",\n\t\t\t\tPCI_SLOT(devfn_from), PCI_FUNC(devfn_from),\n\t\t\t\tPCI_SLOT(devfn_to), PCI_FUNC(devfn_to));\n}\n\nbool pci_devs_are_dma_aliases(struct pci_dev *dev1, struct pci_dev *dev2)\n{\n\treturn (dev1->dma_alias_mask &&\n\t\ttest_bit(dev2->devfn, dev1->dma_alias_mask)) ||\n\t       (dev2->dma_alias_mask &&\n\t\ttest_bit(dev1->devfn, dev2->dma_alias_mask)) ||\n\t       pci_real_dma_dev(dev1) == dev2 ||\n\t       pci_real_dma_dev(dev2) == dev1;\n}\n\nbool pci_device_is_present(struct pci_dev *pdev)\n{\n\tu32 v;\n\n\t \n\tpdev = pci_physfn(pdev);\n\tif (pci_dev_is_disconnected(pdev))\n\t\treturn false;\n\treturn pci_bus_read_dev_vendor_id(pdev->bus, pdev->devfn, &v, 0);\n}\nEXPORT_SYMBOL_GPL(pci_device_is_present);\n\nvoid pci_ignore_hotplug(struct pci_dev *dev)\n{\n\tstruct pci_dev *bridge = dev->bus->self;\n\n\tdev->ignore_hotplug = 1;\n\t \n\tif (bridge)\n\t\tbridge->ignore_hotplug = 1;\n}\nEXPORT_SYMBOL_GPL(pci_ignore_hotplug);\n\n \nstruct pci_dev __weak *pci_real_dma_dev(struct pci_dev *dev)\n{\n\treturn dev;\n}\n\nresource_size_t __weak pcibios_default_alignment(void)\n{\n\treturn 0;\n}\n\n \nvoid __weak pci_resource_to_user(const struct pci_dev *dev, int bar,\n\t\t\t\t const struct resource *rsrc,\n\t\t\t\t resource_size_t *start, resource_size_t *end)\n{\n\t*start = rsrc->start;\n\t*end = rsrc->end;\n}\n\nstatic char *resource_alignment_param;\nstatic DEFINE_SPINLOCK(resource_alignment_lock);\n\n \nstatic resource_size_t pci_specified_resource_alignment(struct pci_dev *dev,\n\t\t\t\t\t\t\tbool *resize)\n{\n\tint align_order, count;\n\tresource_size_t align = pcibios_default_alignment();\n\tconst char *p;\n\tint ret;\n\n\tspin_lock(&resource_alignment_lock);\n\tp = resource_alignment_param;\n\tif (!p || !*p)\n\t\tgoto out;\n\tif (pci_has_flag(PCI_PROBE_ONLY)) {\n\t\talign = 0;\n\t\tpr_info_once(\"PCI: Ignoring requested alignments (PCI_PROBE_ONLY)\\n\");\n\t\tgoto out;\n\t}\n\n\twhile (*p) {\n\t\tcount = 0;\n\t\tif (sscanf(p, \"%d%n\", &align_order, &count) == 1 &&\n\t\t    p[count] == '@') {\n\t\t\tp += count + 1;\n\t\t\tif (align_order > 63) {\n\t\t\t\tpr_err(\"PCI: Invalid requested alignment (order %d)\\n\",\n\t\t\t\t       align_order);\n\t\t\t\talign_order = PAGE_SHIFT;\n\t\t\t}\n\t\t} else {\n\t\t\talign_order = PAGE_SHIFT;\n\t\t}\n\n\t\tret = pci_dev_str_match(dev, p, &p);\n\t\tif (ret == 1) {\n\t\t\t*resize = true;\n\t\t\talign = 1ULL << align_order;\n\t\t\tbreak;\n\t\t} else if (ret < 0) {\n\t\t\tpr_err(\"PCI: Can't parse resource_alignment parameter: %s\\n\",\n\t\t\t       p);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (*p != ';' && *p != ',') {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tp++;\n\t}\nout:\n\tspin_unlock(&resource_alignment_lock);\n\treturn align;\n}\n\nstatic void pci_request_resource_alignment(struct pci_dev *dev, int bar,\n\t\t\t\t\t   resource_size_t align, bool resize)\n{\n\tstruct resource *r = &dev->resource[bar];\n\tresource_size_t size;\n\n\tif (!(r->flags & IORESOURCE_MEM))\n\t\treturn;\n\n\tif (r->flags & IORESOURCE_PCI_FIXED) {\n\t\tpci_info(dev, \"BAR%d %pR: ignoring requested alignment %#llx\\n\",\n\t\t\t bar, r, (unsigned long long)align);\n\t\treturn;\n\t}\n\n\tsize = resource_size(r);\n\tif (size >= align)\n\t\treturn;\n\n\t \n\n\tpci_info(dev, \"BAR%d %pR: requesting alignment to %#llx\\n\",\n\t\t bar, r, (unsigned long long)align);\n\n\tif (resize) {\n\t\tr->start = 0;\n\t\tr->end = align - 1;\n\t} else {\n\t\tr->flags &= ~IORESOURCE_SIZEALIGN;\n\t\tr->flags |= IORESOURCE_STARTALIGN;\n\t\tr->start = align;\n\t\tr->end = r->start + size - 1;\n\t}\n\tr->flags |= IORESOURCE_UNSET;\n}\n\n \nvoid pci_reassigndev_resource_alignment(struct pci_dev *dev)\n{\n\tint i;\n\tstruct resource *r;\n\tresource_size_t align;\n\tu16 command;\n\tbool resize = false;\n\n\t \n\tif (dev->is_virtfn)\n\t\treturn;\n\n\t \n\talign = pci_specified_resource_alignment(dev, &resize);\n\tif (!align)\n\t\treturn;\n\n\tif (dev->hdr_type == PCI_HEADER_TYPE_NORMAL &&\n\t    (dev->class >> 8) == PCI_CLASS_BRIDGE_HOST) {\n\t\tpci_warn(dev, \"Can't reassign resources to host bridge\\n\");\n\t\treturn;\n\t}\n\n\tpci_read_config_word(dev, PCI_COMMAND, &command);\n\tcommand &= ~PCI_COMMAND_MEMORY;\n\tpci_write_config_word(dev, PCI_COMMAND, command);\n\n\tfor (i = 0; i <= PCI_ROM_RESOURCE; i++)\n\t\tpci_request_resource_alignment(dev, i, align, resize);\n\n\t \n\tif (dev->hdr_type == PCI_HEADER_TYPE_BRIDGE) {\n\t\tfor (i = PCI_BRIDGE_RESOURCES; i < PCI_NUM_RESOURCES; i++) {\n\t\t\tr = &dev->resource[i];\n\t\t\tif (!(r->flags & IORESOURCE_MEM))\n\t\t\t\tcontinue;\n\t\t\tr->flags |= IORESOURCE_UNSET;\n\t\t\tr->end = resource_size(r) - 1;\n\t\t\tr->start = 0;\n\t\t}\n\t\tpci_disable_bridge_window(dev);\n\t}\n}\n\nstatic ssize_t resource_alignment_show(const struct bus_type *bus, char *buf)\n{\n\tsize_t count = 0;\n\n\tspin_lock(&resource_alignment_lock);\n\tif (resource_alignment_param)\n\t\tcount = sysfs_emit(buf, \"%s\\n\", resource_alignment_param);\n\tspin_unlock(&resource_alignment_lock);\n\n\treturn count;\n}\n\nstatic ssize_t resource_alignment_store(const struct bus_type *bus,\n\t\t\t\t\tconst char *buf, size_t count)\n{\n\tchar *param, *old, *end;\n\n\tif (count >= (PAGE_SIZE - 1))\n\t\treturn -EINVAL;\n\n\tparam = kstrndup(buf, count, GFP_KERNEL);\n\tif (!param)\n\t\treturn -ENOMEM;\n\n\tend = strchr(param, '\\n');\n\tif (end)\n\t\t*end = '\\0';\n\n\tspin_lock(&resource_alignment_lock);\n\told = resource_alignment_param;\n\tif (strlen(param)) {\n\t\tresource_alignment_param = param;\n\t} else {\n\t\tkfree(param);\n\t\tresource_alignment_param = NULL;\n\t}\n\tspin_unlock(&resource_alignment_lock);\n\n\tkfree(old);\n\n\treturn count;\n}\n\nstatic BUS_ATTR_RW(resource_alignment);\n\nstatic int __init pci_resource_alignment_sysfs_init(void)\n{\n\treturn bus_create_file(&pci_bus_type,\n\t\t\t\t\t&bus_attr_resource_alignment);\n}\nlate_initcall(pci_resource_alignment_sysfs_init);\n\nstatic void pci_no_domains(void)\n{\n#ifdef CONFIG_PCI_DOMAINS\n\tpci_domains_supported = 0;\n#endif\n}\n\n#ifdef CONFIG_PCI_DOMAINS_GENERIC\nstatic DEFINE_IDA(pci_domain_nr_static_ida);\nstatic DEFINE_IDA(pci_domain_nr_dynamic_ida);\n\nstatic void of_pci_reserve_static_domain_nr(void)\n{\n\tstruct device_node *np;\n\tint domain_nr;\n\n\tfor_each_node_by_type(np, \"pci\") {\n\t\tdomain_nr = of_get_pci_domain_nr(np);\n\t\tif (domain_nr < 0)\n\t\t\tcontinue;\n\t\t \n\t\tida_alloc_range(&pci_domain_nr_dynamic_ida,\n\t\t\t\tdomain_nr, domain_nr, GFP_KERNEL);\n\t}\n}\n\nstatic int of_pci_bus_find_domain_nr(struct device *parent)\n{\n\tstatic bool static_domains_reserved = false;\n\tint domain_nr;\n\n\t \n\tif (!static_domains_reserved) {\n\t\tof_pci_reserve_static_domain_nr();\n\t\tstatic_domains_reserved = true;\n\t}\n\n\tif (parent) {\n\t\t \n\t\tdomain_nr = of_get_pci_domain_nr(parent->of_node);\n\t\tif (domain_nr >= 0)\n\t\t\treturn ida_alloc_range(&pci_domain_nr_static_ida,\n\t\t\t\t\t       domain_nr, domain_nr,\n\t\t\t\t\t       GFP_KERNEL);\n\t}\n\n\t \n\treturn ida_alloc(&pci_domain_nr_dynamic_ida, GFP_KERNEL);\n}\n\nstatic void of_pci_bus_release_domain_nr(struct pci_bus *bus, struct device *parent)\n{\n\tif (bus->domain_nr < 0)\n\t\treturn;\n\n\t \n\tif (of_get_pci_domain_nr(parent->of_node) == bus->domain_nr)\n\t\tida_free(&pci_domain_nr_static_ida, bus->domain_nr);\n\telse\n\t\tida_free(&pci_domain_nr_dynamic_ida, bus->domain_nr);\n}\n\nint pci_bus_find_domain_nr(struct pci_bus *bus, struct device *parent)\n{\n\treturn acpi_disabled ? of_pci_bus_find_domain_nr(parent) :\n\t\t\t       acpi_pci_bus_find_domain_nr(bus);\n}\n\nvoid pci_bus_release_domain_nr(struct pci_bus *bus, struct device *parent)\n{\n\tif (!acpi_disabled)\n\t\treturn;\n\tof_pci_bus_release_domain_nr(bus, parent);\n}\n#endif\n\n \nint __weak pci_ext_cfg_avail(void)\n{\n\treturn 1;\n}\n\nvoid __weak pci_fixup_cardbus(struct pci_bus *bus)\n{\n}\nEXPORT_SYMBOL(pci_fixup_cardbus);\n\nstatic int __init pci_setup(char *str)\n{\n\twhile (str) {\n\t\tchar *k = strchr(str, ',');\n\t\tif (k)\n\t\t\t*k++ = 0;\n\t\tif (*str && (str = pcibios_setup(str)) && *str) {\n\t\t\tif (!strcmp(str, \"nomsi\")) {\n\t\t\t\tpci_no_msi();\n\t\t\t} else if (!strncmp(str, \"noats\", 5)) {\n\t\t\t\tpr_info(\"PCIe: ATS is disabled\\n\");\n\t\t\t\tpcie_ats_disabled = true;\n\t\t\t} else if (!strcmp(str, \"noaer\")) {\n\t\t\t\tpci_no_aer();\n\t\t\t} else if (!strcmp(str, \"earlydump\")) {\n\t\t\t\tpci_early_dump = true;\n\t\t\t} else if (!strncmp(str, \"realloc=\", 8)) {\n\t\t\t\tpci_realloc_get_opt(str + 8);\n\t\t\t} else if (!strncmp(str, \"realloc\", 7)) {\n\t\t\t\tpci_realloc_get_opt(\"on\");\n\t\t\t} else if (!strcmp(str, \"nodomains\")) {\n\t\t\t\tpci_no_domains();\n\t\t\t} else if (!strncmp(str, \"noari\", 5)) {\n\t\t\t\tpcie_ari_disabled = true;\n\t\t\t} else if (!strncmp(str, \"cbiosize=\", 9)) {\n\t\t\t\tpci_cardbus_io_size = memparse(str + 9, &str);\n\t\t\t} else if (!strncmp(str, \"cbmemsize=\", 10)) {\n\t\t\t\tpci_cardbus_mem_size = memparse(str + 10, &str);\n\t\t\t} else if (!strncmp(str, \"resource_alignment=\", 19)) {\n\t\t\t\tresource_alignment_param = str + 19;\n\t\t\t} else if (!strncmp(str, \"ecrc=\", 5)) {\n\t\t\t\tpcie_ecrc_get_policy(str + 5);\n\t\t\t} else if (!strncmp(str, \"hpiosize=\", 9)) {\n\t\t\t\tpci_hotplug_io_size = memparse(str + 9, &str);\n\t\t\t} else if (!strncmp(str, \"hpmmiosize=\", 11)) {\n\t\t\t\tpci_hotplug_mmio_size = memparse(str + 11, &str);\n\t\t\t} else if (!strncmp(str, \"hpmmioprefsize=\", 15)) {\n\t\t\t\tpci_hotplug_mmio_pref_size = memparse(str + 15, &str);\n\t\t\t} else if (!strncmp(str, \"hpmemsize=\", 10)) {\n\t\t\t\tpci_hotplug_mmio_size = memparse(str + 10, &str);\n\t\t\t\tpci_hotplug_mmio_pref_size = pci_hotplug_mmio_size;\n\t\t\t} else if (!strncmp(str, \"hpbussize=\", 10)) {\n\t\t\t\tpci_hotplug_bus_size =\n\t\t\t\t\tsimple_strtoul(str + 10, &str, 0);\n\t\t\t\tif (pci_hotplug_bus_size > 0xff)\n\t\t\t\t\tpci_hotplug_bus_size = DEFAULT_HOTPLUG_BUS_SIZE;\n\t\t\t} else if (!strncmp(str, \"pcie_bus_tune_off\", 17)) {\n\t\t\t\tpcie_bus_config = PCIE_BUS_TUNE_OFF;\n\t\t\t} else if (!strncmp(str, \"pcie_bus_safe\", 13)) {\n\t\t\t\tpcie_bus_config = PCIE_BUS_SAFE;\n\t\t\t} else if (!strncmp(str, \"pcie_bus_perf\", 13)) {\n\t\t\t\tpcie_bus_config = PCIE_BUS_PERFORMANCE;\n\t\t\t} else if (!strncmp(str, \"pcie_bus_peer2peer\", 18)) {\n\t\t\t\tpcie_bus_config = PCIE_BUS_PEER2PEER;\n\t\t\t} else if (!strncmp(str, \"pcie_scan_all\", 13)) {\n\t\t\t\tpci_add_flags(PCI_SCAN_ALL_PCIE_DEVS);\n\t\t\t} else if (!strncmp(str, \"disable_acs_redir=\", 18)) {\n\t\t\t\tdisable_acs_redir_param = str + 18;\n\t\t\t} else {\n\t\t\t\tpr_err(\"PCI: Unknown option `%s'\\n\", str);\n\t\t\t}\n\t\t}\n\t\tstr = k;\n\t}\n\treturn 0;\n}\nearly_param(\"pci\", pci_setup);\n\n \nstatic int __init pci_realloc_setup_params(void)\n{\n\tresource_alignment_param = kstrdup(resource_alignment_param,\n\t\t\t\t\t   GFP_KERNEL);\n\tdisable_acs_redir_param = kstrdup(disable_acs_redir_param, GFP_KERNEL);\n\n\treturn 0;\n}\npure_initcall(pci_realloc_setup_params);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}