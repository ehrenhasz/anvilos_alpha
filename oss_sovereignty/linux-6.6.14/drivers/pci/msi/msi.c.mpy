{
  "module_name": "msi.c",
  "hash_id": "0c2b77f2496988bbd46e4ed8310f04b1bc410e18ad8ba149110190e194d7b9fc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/pci/msi/msi.c",
  "human_readable_source": "\n \n#include <linux/err.h>\n#include <linux/export.h>\n#include <linux/irq.h>\n\n#include \"../pci.h\"\n#include \"msi.h\"\n\nint pci_msi_enable = 1;\nint pci_msi_ignore_mask;\n\n \nstatic int pci_msi_supported(struct pci_dev *dev, int nvec)\n{\n\tstruct pci_bus *bus;\n\n\t \n\tif (!pci_msi_enable)\n\t\treturn 0;\n\n\tif (!dev || dev->no_msi)\n\t\treturn 0;\n\n\t \n\tif (nvec < 1)\n\t\treturn 0;\n\n\t \n\tfor (bus = dev->bus; bus; bus = bus->parent)\n\t\tif (bus->bus_flags & PCI_BUS_FLAGS_NO_MSI)\n\t\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic void pcim_msi_release(void *pcidev)\n{\n\tstruct pci_dev *dev = pcidev;\n\n\tdev->is_msi_managed = false;\n\tpci_free_irq_vectors(dev);\n}\n\n \nstatic int pcim_setup_msi_release(struct pci_dev *dev)\n{\n\tint ret;\n\n\tif (!pci_is_managed(dev) || dev->is_msi_managed)\n\t\treturn 0;\n\n\tret = devm_add_action(&dev->dev, pcim_msi_release, dev);\n\tif (!ret)\n\t\tdev->is_msi_managed = true;\n\treturn ret;\n}\n\n \nstatic int pci_setup_msi_context(struct pci_dev *dev)\n{\n\tint ret = msi_setup_device_data(&dev->dev);\n\n\tif (!ret)\n\t\tret = pcim_setup_msi_release(dev);\n\treturn ret;\n}\n\n \n\nvoid pci_msi_update_mask(struct msi_desc *desc, u32 clear, u32 set)\n{\n\traw_spinlock_t *lock = &to_pci_dev(desc->dev)->msi_lock;\n\tunsigned long flags;\n\n\tif (!desc->pci.msi_attrib.can_mask)\n\t\treturn;\n\n\traw_spin_lock_irqsave(lock, flags);\n\tdesc->pci.msi_mask &= ~clear;\n\tdesc->pci.msi_mask |= set;\n\tpci_write_config_dword(msi_desc_to_pci_dev(desc), desc->pci.mask_pos,\n\t\t\t       desc->pci.msi_mask);\n\traw_spin_unlock_irqrestore(lock, flags);\n}\n\n \nvoid pci_msi_mask_irq(struct irq_data *data)\n{\n\tstruct msi_desc *desc = irq_data_get_msi_desc(data);\n\n\t__pci_msi_mask_desc(desc, BIT(data->irq - desc->irq));\n}\nEXPORT_SYMBOL_GPL(pci_msi_mask_irq);\n\n \nvoid pci_msi_unmask_irq(struct irq_data *data)\n{\n\tstruct msi_desc *desc = irq_data_get_msi_desc(data);\n\n\t__pci_msi_unmask_desc(desc, BIT(data->irq - desc->irq));\n}\nEXPORT_SYMBOL_GPL(pci_msi_unmask_irq);\n\nvoid __pci_read_msi_msg(struct msi_desc *entry, struct msi_msg *msg)\n{\n\tstruct pci_dev *dev = msi_desc_to_pci_dev(entry);\n\n\tBUG_ON(dev->current_state != PCI_D0);\n\n\tif (entry->pci.msi_attrib.is_msix) {\n\t\tvoid __iomem *base = pci_msix_desc_addr(entry);\n\n\t\tif (WARN_ON_ONCE(entry->pci.msi_attrib.is_virtual))\n\t\t\treturn;\n\n\t\tmsg->address_lo = readl(base + PCI_MSIX_ENTRY_LOWER_ADDR);\n\t\tmsg->address_hi = readl(base + PCI_MSIX_ENTRY_UPPER_ADDR);\n\t\tmsg->data = readl(base + PCI_MSIX_ENTRY_DATA);\n\t} else {\n\t\tint pos = dev->msi_cap;\n\t\tu16 data;\n\n\t\tpci_read_config_dword(dev, pos + PCI_MSI_ADDRESS_LO,\n\t\t\t\t      &msg->address_lo);\n\t\tif (entry->pci.msi_attrib.is_64) {\n\t\t\tpci_read_config_dword(dev, pos + PCI_MSI_ADDRESS_HI,\n\t\t\t\t\t      &msg->address_hi);\n\t\t\tpci_read_config_word(dev, pos + PCI_MSI_DATA_64, &data);\n\t\t} else {\n\t\t\tmsg->address_hi = 0;\n\t\t\tpci_read_config_word(dev, pos + PCI_MSI_DATA_32, &data);\n\t\t}\n\t\tmsg->data = data;\n\t}\n}\n\nstatic inline void pci_write_msg_msi(struct pci_dev *dev, struct msi_desc *desc,\n\t\t\t\t     struct msi_msg *msg)\n{\n\tint pos = dev->msi_cap;\n\tu16 msgctl;\n\n\tpci_read_config_word(dev, pos + PCI_MSI_FLAGS, &msgctl);\n\tmsgctl &= ~PCI_MSI_FLAGS_QSIZE;\n\tmsgctl |= desc->pci.msi_attrib.multiple << 4;\n\tpci_write_config_word(dev, pos + PCI_MSI_FLAGS, msgctl);\n\n\tpci_write_config_dword(dev, pos + PCI_MSI_ADDRESS_LO, msg->address_lo);\n\tif (desc->pci.msi_attrib.is_64) {\n\t\tpci_write_config_dword(dev, pos + PCI_MSI_ADDRESS_HI,  msg->address_hi);\n\t\tpci_write_config_word(dev, pos + PCI_MSI_DATA_64, msg->data);\n\t} else {\n\t\tpci_write_config_word(dev, pos + PCI_MSI_DATA_32, msg->data);\n\t}\n\t \n\tpci_read_config_word(dev, pos + PCI_MSI_FLAGS, &msgctl);\n}\n\nstatic inline void pci_write_msg_msix(struct msi_desc *desc, struct msi_msg *msg)\n{\n\tvoid __iomem *base = pci_msix_desc_addr(desc);\n\tu32 ctrl = desc->pci.msix_ctrl;\n\tbool unmasked = !(ctrl & PCI_MSIX_ENTRY_CTRL_MASKBIT);\n\n\tif (desc->pci.msi_attrib.is_virtual)\n\t\treturn;\n\t \n\tif (unmasked)\n\t\tpci_msix_write_vector_ctrl(desc, ctrl | PCI_MSIX_ENTRY_CTRL_MASKBIT);\n\n\twritel(msg->address_lo, base + PCI_MSIX_ENTRY_LOWER_ADDR);\n\twritel(msg->address_hi, base + PCI_MSIX_ENTRY_UPPER_ADDR);\n\twritel(msg->data, base + PCI_MSIX_ENTRY_DATA);\n\n\tif (unmasked)\n\t\tpci_msix_write_vector_ctrl(desc, ctrl);\n\n\t \n\treadl(base + PCI_MSIX_ENTRY_DATA);\n}\n\nvoid __pci_write_msi_msg(struct msi_desc *entry, struct msi_msg *msg)\n{\n\tstruct pci_dev *dev = msi_desc_to_pci_dev(entry);\n\n\tif (dev->current_state != PCI_D0 || pci_dev_is_disconnected(dev)) {\n\t\t \n\t} else if (entry->pci.msi_attrib.is_msix) {\n\t\tpci_write_msg_msix(entry, msg);\n\t} else {\n\t\tpci_write_msg_msi(dev, entry, msg);\n\t}\n\n\tentry->msg = *msg;\n\n\tif (entry->write_msi_msg)\n\t\tentry->write_msi_msg(entry, entry->write_msi_msg_data);\n}\n\nvoid pci_write_msi_msg(unsigned int irq, struct msi_msg *msg)\n{\n\tstruct msi_desc *entry = irq_get_msi_desc(irq);\n\n\t__pci_write_msi_msg(entry, msg);\n}\nEXPORT_SYMBOL_GPL(pci_write_msi_msg);\n\n\n \n\nstatic void pci_intx_for_msi(struct pci_dev *dev, int enable)\n{\n\tif (!(dev->dev_flags & PCI_DEV_FLAGS_MSI_INTX_DISABLE_BUG))\n\t\tpci_intx(dev, enable);\n}\n\nstatic void pci_msi_set_enable(struct pci_dev *dev, int enable)\n{\n\tu16 control;\n\n\tpci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &control);\n\tcontrol &= ~PCI_MSI_FLAGS_ENABLE;\n\tif (enable)\n\t\tcontrol |= PCI_MSI_FLAGS_ENABLE;\n\tpci_write_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, control);\n}\n\nstatic int msi_setup_msi_desc(struct pci_dev *dev, int nvec,\n\t\t\t      struct irq_affinity_desc *masks)\n{\n\tstruct msi_desc desc;\n\tu16 control;\n\n\t \n\tmemset(&desc, 0, sizeof(desc));\n\n\tpci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &control);\n\t \n\tif (dev->dev_flags & PCI_DEV_FLAGS_HAS_MSI_MASKING)\n\t\tcontrol |= PCI_MSI_FLAGS_MASKBIT;\n\t \n\tif (pci_msi_ignore_mask)\n\t\tcontrol &= ~PCI_MSI_FLAGS_MASKBIT;\n\n\tdesc.nvec_used\t\t\t= nvec;\n\tdesc.pci.msi_attrib.is_64\t= !!(control & PCI_MSI_FLAGS_64BIT);\n\tdesc.pci.msi_attrib.can_mask\t= !!(control & PCI_MSI_FLAGS_MASKBIT);\n\tdesc.pci.msi_attrib.default_irq\t= dev->irq;\n\tdesc.pci.msi_attrib.multi_cap\t= (control & PCI_MSI_FLAGS_QMASK) >> 1;\n\tdesc.pci.msi_attrib.multiple\t= ilog2(__roundup_pow_of_two(nvec));\n\tdesc.affinity\t\t\t= masks;\n\n\tif (control & PCI_MSI_FLAGS_64BIT)\n\t\tdesc.pci.mask_pos = dev->msi_cap + PCI_MSI_MASK_64;\n\telse\n\t\tdesc.pci.mask_pos = dev->msi_cap + PCI_MSI_MASK_32;\n\n\t \n\tif (desc.pci.msi_attrib.can_mask)\n\t\tpci_read_config_dword(dev, desc.pci.mask_pos, &desc.pci.msi_mask);\n\n\treturn msi_insert_msi_desc(&dev->dev, &desc);\n}\n\nstatic int msi_verify_entries(struct pci_dev *dev)\n{\n\tstruct msi_desc *entry;\n\n\tif (!dev->no_64bit_msi)\n\t\treturn 0;\n\n\tmsi_for_each_desc(entry, &dev->dev, MSI_DESC_ALL) {\n\t\tif (entry->msg.address_hi) {\n\t\t\tpci_err(dev, \"arch assigned 64-bit MSI address %#x%08x but device only supports 32 bits\\n\",\n\t\t\t\tentry->msg.address_hi, entry->msg.address_lo);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn !entry ? 0 : -EIO;\n}\n\n \nstatic int msi_capability_init(struct pci_dev *dev, int nvec,\n\t\t\t       struct irq_affinity *affd)\n{\n\tstruct irq_affinity_desc *masks = NULL;\n\tstruct msi_desc *entry;\n\tint ret;\n\n\t \n\tif (nvec > 1 && !pci_msi_domain_supports(dev, MSI_FLAG_MULTI_PCI_MSI, ALLOW_LEGACY))\n\t\treturn 1;\n\n\t \n\tpci_msi_set_enable(dev, 0);\n\tdev->msi_enabled = 1;\n\n\tif (affd)\n\t\tmasks = irq_create_affinity_masks(nvec, affd);\n\n\tmsi_lock_descs(&dev->dev);\n\tret = msi_setup_msi_desc(dev, nvec, masks);\n\tif (ret)\n\t\tgoto fail;\n\n\t \n\tentry = msi_first_desc(&dev->dev, MSI_DESC_ALL);\n\tpci_msi_mask(entry, msi_multi_mask(entry));\n\n\t \n\tret = pci_msi_setup_msi_irqs(dev, nvec, PCI_CAP_ID_MSI);\n\tif (ret)\n\t\tgoto err;\n\n\tret = msi_verify_entries(dev);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\tpci_intx_for_msi(dev, 0);\n\tpci_msi_set_enable(dev, 1);\n\n\tpcibios_free_irq(dev);\n\tdev->irq = entry->irq;\n\tgoto unlock;\n\nerr:\n\tpci_msi_unmask(entry, msi_multi_mask(entry));\n\tpci_free_msi_irqs(dev);\nfail:\n\tdev->msi_enabled = 0;\nunlock:\n\tmsi_unlock_descs(&dev->dev);\n\tkfree(masks);\n\treturn ret;\n}\n\nint __pci_enable_msi_range(struct pci_dev *dev, int minvec, int maxvec,\n\t\t\t   struct irq_affinity *affd)\n{\n\tint nvec;\n\tint rc;\n\n\tif (!pci_msi_supported(dev, minvec) || dev->current_state != PCI_D0)\n\t\treturn -EINVAL;\n\n\t \n\tif (dev->msix_enabled) {\n\t\tpci_info(dev, \"can't enable MSI (MSI-X already enabled)\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (maxvec < minvec)\n\t\treturn -ERANGE;\n\n\tif (WARN_ON_ONCE(dev->msi_enabled))\n\t\treturn -EINVAL;\n\n\tnvec = pci_msi_vec_count(dev);\n\tif (nvec < 0)\n\t\treturn nvec;\n\tif (nvec < minvec)\n\t\treturn -ENOSPC;\n\n\tif (nvec > maxvec)\n\t\tnvec = maxvec;\n\n\trc = pci_setup_msi_context(dev);\n\tif (rc)\n\t\treturn rc;\n\n\tif (!pci_setup_msi_device_domain(dev))\n\t\treturn -ENODEV;\n\n\tfor (;;) {\n\t\tif (affd) {\n\t\t\tnvec = irq_calc_affinity_vectors(minvec, nvec, affd);\n\t\t\tif (nvec < minvec)\n\t\t\t\treturn -ENOSPC;\n\t\t}\n\n\t\trc = msi_capability_init(dev, nvec, affd);\n\t\tif (rc == 0)\n\t\t\treturn nvec;\n\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t\tif (rc < minvec)\n\t\t\treturn -ENOSPC;\n\n\t\tnvec = rc;\n\t}\n}\n\n \nint pci_msi_vec_count(struct pci_dev *dev)\n{\n\tint ret;\n\tu16 msgctl;\n\n\tif (!dev->msi_cap)\n\t\treturn -EINVAL;\n\n\tpci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &msgctl);\n\tret = 1 << ((msgctl & PCI_MSI_FLAGS_QMASK) >> 1);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(pci_msi_vec_count);\n\n \nbool __weak arch_restore_msi_irqs(struct pci_dev *dev)\n{\n\treturn true;\n}\n\nvoid __pci_restore_msi_state(struct pci_dev *dev)\n{\n\tstruct msi_desc *entry;\n\tu16 control;\n\n\tif (!dev->msi_enabled)\n\t\treturn;\n\n\tentry = irq_get_msi_desc(dev->irq);\n\n\tpci_intx_for_msi(dev, 0);\n\tpci_msi_set_enable(dev, 0);\n\tif (arch_restore_msi_irqs(dev))\n\t\t__pci_write_msi_msg(entry, &entry->msg);\n\n\tpci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &control);\n\tpci_msi_update_mask(entry, 0, 0);\n\tcontrol &= ~PCI_MSI_FLAGS_QSIZE;\n\tcontrol |= (entry->pci.msi_attrib.multiple << 4) | PCI_MSI_FLAGS_ENABLE;\n\tpci_write_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, control);\n}\n\nvoid pci_msi_shutdown(struct pci_dev *dev)\n{\n\tstruct msi_desc *desc;\n\n\tif (!pci_msi_enable || !dev || !dev->msi_enabled)\n\t\treturn;\n\n\tpci_msi_set_enable(dev, 0);\n\tpci_intx_for_msi(dev, 1);\n\tdev->msi_enabled = 0;\n\n\t \n\tdesc = msi_first_desc(&dev->dev, MSI_DESC_ALL);\n\tif (!WARN_ON_ONCE(!desc))\n\t\tpci_msi_unmask(desc, msi_multi_mask(desc));\n\n\t \n\tdev->irq = desc->pci.msi_attrib.default_irq;\n\tpcibios_alloc_irq(dev);\n}\n\n \n\nstatic void pci_msix_clear_and_set_ctrl(struct pci_dev *dev, u16 clear, u16 set)\n{\n\tu16 ctrl;\n\n\tpci_read_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, &ctrl);\n\tctrl &= ~clear;\n\tctrl |= set;\n\tpci_write_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, ctrl);\n}\n\nstatic void __iomem *msix_map_region(struct pci_dev *dev,\n\t\t\t\t     unsigned int nr_entries)\n{\n\tresource_size_t phys_addr;\n\tu32 table_offset;\n\tunsigned long flags;\n\tu8 bir;\n\n\tpci_read_config_dword(dev, dev->msix_cap + PCI_MSIX_TABLE,\n\t\t\t      &table_offset);\n\tbir = (u8)(table_offset & PCI_MSIX_TABLE_BIR);\n\tflags = pci_resource_flags(dev, bir);\n\tif (!flags || (flags & IORESOURCE_UNSET))\n\t\treturn NULL;\n\n\ttable_offset &= PCI_MSIX_TABLE_OFFSET;\n\tphys_addr = pci_resource_start(dev, bir) + table_offset;\n\n\treturn ioremap(phys_addr, nr_entries * PCI_MSIX_ENTRY_SIZE);\n}\n\n \nvoid msix_prepare_msi_desc(struct pci_dev *dev, struct msi_desc *desc)\n{\n\tdesc->nvec_used\t\t\t\t= 1;\n\tdesc->pci.msi_attrib.is_msix\t\t= 1;\n\tdesc->pci.msi_attrib.is_64\t\t= 1;\n\tdesc->pci.msi_attrib.default_irq\t= dev->irq;\n\tdesc->pci.mask_base\t\t\t= dev->msix_base;\n\tdesc->pci.msi_attrib.can_mask\t\t= !pci_msi_ignore_mask &&\n\t\t\t\t\t\t  !desc->pci.msi_attrib.is_virtual;\n\n\tif (desc->pci.msi_attrib.can_mask) {\n\t\tvoid __iomem *addr = pci_msix_desc_addr(desc);\n\n\t\tdesc->pci.msix_ctrl = readl(addr + PCI_MSIX_ENTRY_VECTOR_CTRL);\n\t}\n}\n\nstatic int msix_setup_msi_descs(struct pci_dev *dev, struct msix_entry *entries,\n\t\t\t\tint nvec, struct irq_affinity_desc *masks)\n{\n\tint ret = 0, i, vec_count = pci_msix_vec_count(dev);\n\tstruct irq_affinity_desc *curmsk;\n\tstruct msi_desc desc;\n\n\tmemset(&desc, 0, sizeof(desc));\n\n\tfor (i = 0, curmsk = masks; i < nvec; i++, curmsk++) {\n\t\tdesc.msi_index = entries ? entries[i].entry : i;\n\t\tdesc.affinity = masks ? curmsk : NULL;\n\t\tdesc.pci.msi_attrib.is_virtual = desc.msi_index >= vec_count;\n\n\t\tmsix_prepare_msi_desc(dev, &desc);\n\n\t\tret = msi_insert_msi_desc(&dev->dev, &desc);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic void msix_update_entries(struct pci_dev *dev, struct msix_entry *entries)\n{\n\tstruct msi_desc *desc;\n\n\tif (entries) {\n\t\tmsi_for_each_desc(desc, &dev->dev, MSI_DESC_ALL) {\n\t\t\tentries->vector = desc->irq;\n\t\t\tentries++;\n\t\t}\n\t}\n}\n\nstatic void msix_mask_all(void __iomem *base, int tsize)\n{\n\tu32 ctrl = PCI_MSIX_ENTRY_CTRL_MASKBIT;\n\tint i;\n\n\tif (pci_msi_ignore_mask)\n\t\treturn;\n\n\tfor (i = 0; i < tsize; i++, base += PCI_MSIX_ENTRY_SIZE)\n\t\twritel(ctrl, base + PCI_MSIX_ENTRY_VECTOR_CTRL);\n}\n\nstatic int msix_setup_interrupts(struct pci_dev *dev, struct msix_entry *entries,\n\t\t\t\t int nvec, struct irq_affinity *affd)\n{\n\tstruct irq_affinity_desc *masks = NULL;\n\tint ret;\n\n\tif (affd)\n\t\tmasks = irq_create_affinity_masks(nvec, affd);\n\n\tmsi_lock_descs(&dev->dev);\n\tret = msix_setup_msi_descs(dev, entries, nvec, masks);\n\tif (ret)\n\t\tgoto out_free;\n\n\tret = pci_msi_setup_msi_irqs(dev, nvec, PCI_CAP_ID_MSIX);\n\tif (ret)\n\t\tgoto out_free;\n\n\t \n\tret = msi_verify_entries(dev);\n\tif (ret)\n\t\tgoto out_free;\n\n\tmsix_update_entries(dev, entries);\n\tgoto out_unlock;\n\nout_free:\n\tpci_free_msi_irqs(dev);\nout_unlock:\n\tmsi_unlock_descs(&dev->dev);\n\tkfree(masks);\n\treturn ret;\n}\n\n \nstatic int msix_capability_init(struct pci_dev *dev, struct msix_entry *entries,\n\t\t\t\tint nvec, struct irq_affinity *affd)\n{\n\tint ret, tsize;\n\tu16 control;\n\n\t \n\tpci_msix_clear_and_set_ctrl(dev, 0, PCI_MSIX_FLAGS_MASKALL |\n\t\t\t\t    PCI_MSIX_FLAGS_ENABLE);\n\n\t \n\tdev->msix_enabled = 1;\n\n\tpci_read_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, &control);\n\t \n\ttsize = msix_table_size(control);\n\tdev->msix_base = msix_map_region(dev, tsize);\n\tif (!dev->msix_base) {\n\t\tret = -ENOMEM;\n\t\tgoto out_disable;\n\t}\n\n\tret = msix_setup_interrupts(dev, entries, nvec, affd);\n\tif (ret)\n\t\tgoto out_disable;\n\n\t \n\tpci_intx_for_msi(dev, 0);\n\n\t \n\tmsix_mask_all(dev->msix_base, tsize);\n\tpci_msix_clear_and_set_ctrl(dev, PCI_MSIX_FLAGS_MASKALL, 0);\n\n\tpcibios_free_irq(dev);\n\treturn 0;\n\nout_disable:\n\tdev->msix_enabled = 0;\n\tpci_msix_clear_and_set_ctrl(dev, PCI_MSIX_FLAGS_MASKALL | PCI_MSIX_FLAGS_ENABLE, 0);\n\n\treturn ret;\n}\n\nstatic bool pci_msix_validate_entries(struct pci_dev *dev, struct msix_entry *entries, int nvec)\n{\n\tbool nogap;\n\tint i, j;\n\n\tif (!entries)\n\t\treturn true;\n\n\tnogap = pci_msi_domain_supports(dev, MSI_FLAG_MSIX_CONTIGUOUS, DENY_LEGACY);\n\n\tfor (i = 0; i < nvec; i++) {\n\t\t \n\t\tfor (j = i + 1; j < nvec; j++) {\n\t\t\tif (entries[i].entry == entries[j].entry)\n\t\t\t\treturn false;\n\t\t}\n\t\t \n\t\tif (nogap && entries[i].entry != i)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nint __pci_enable_msix_range(struct pci_dev *dev, struct msix_entry *entries, int minvec,\n\t\t\t    int maxvec, struct irq_affinity *affd, int flags)\n{\n\tint hwsize, rc, nvec = maxvec;\n\n\tif (maxvec < minvec)\n\t\treturn -ERANGE;\n\n\tif (dev->msi_enabled) {\n\t\tpci_info(dev, \"can't enable MSI-X (MSI already enabled)\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (WARN_ON_ONCE(dev->msix_enabled))\n\t\treturn -EINVAL;\n\n\t \n\tif (!pci_msi_domain_supports(dev, MSI_FLAG_PCI_MSIX, ALLOW_LEGACY))\n\t\treturn -ENOTSUPP;\n\n\tif (!pci_msi_supported(dev, nvec) || dev->current_state != PCI_D0)\n\t\treturn -EINVAL;\n\n\thwsize = pci_msix_vec_count(dev);\n\tif (hwsize < 0)\n\t\treturn hwsize;\n\n\tif (!pci_msix_validate_entries(dev, entries, nvec))\n\t\treturn -EINVAL;\n\n\tif (hwsize < nvec) {\n\t\t \n\t\tif (flags & PCI_IRQ_VIRTUAL)\n\t\t\thwsize = nvec;\n\t\telse\n\t\t\tnvec = hwsize;\n\t}\n\n\tif (nvec < minvec)\n\t\treturn -ENOSPC;\n\n\trc = pci_setup_msi_context(dev);\n\tif (rc)\n\t\treturn rc;\n\n\tif (!pci_setup_msix_device_domain(dev, hwsize))\n\t\treturn -ENODEV;\n\n\tfor (;;) {\n\t\tif (affd) {\n\t\t\tnvec = irq_calc_affinity_vectors(minvec, nvec, affd);\n\t\t\tif (nvec < minvec)\n\t\t\t\treturn -ENOSPC;\n\t\t}\n\n\t\trc = msix_capability_init(dev, entries, nvec, affd);\n\t\tif (rc == 0)\n\t\t\treturn nvec;\n\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t\tif (rc < minvec)\n\t\t\treturn -ENOSPC;\n\n\t\tnvec = rc;\n\t}\n}\n\nvoid __pci_restore_msix_state(struct pci_dev *dev)\n{\n\tstruct msi_desc *entry;\n\tbool write_msg;\n\n\tif (!dev->msix_enabled)\n\t\treturn;\n\n\t \n\tpci_intx_for_msi(dev, 0);\n\tpci_msix_clear_and_set_ctrl(dev, 0,\n\t\t\t\tPCI_MSIX_FLAGS_ENABLE | PCI_MSIX_FLAGS_MASKALL);\n\n\twrite_msg = arch_restore_msi_irqs(dev);\n\n\tmsi_lock_descs(&dev->dev);\n\tmsi_for_each_desc(entry, &dev->dev, MSI_DESC_ALL) {\n\t\tif (write_msg)\n\t\t\t__pci_write_msi_msg(entry, &entry->msg);\n\t\tpci_msix_write_vector_ctrl(entry, entry->pci.msix_ctrl);\n\t}\n\tmsi_unlock_descs(&dev->dev);\n\n\tpci_msix_clear_and_set_ctrl(dev, PCI_MSIX_FLAGS_MASKALL, 0);\n}\n\nvoid pci_msix_shutdown(struct pci_dev *dev)\n{\n\tstruct msi_desc *desc;\n\n\tif (!pci_msi_enable || !dev || !dev->msix_enabled)\n\t\treturn;\n\n\tif (pci_dev_is_disconnected(dev)) {\n\t\tdev->msix_enabled = 0;\n\t\treturn;\n\t}\n\n\t \n\tmsi_for_each_desc(desc, &dev->dev, MSI_DESC_ALL)\n\t\tpci_msix_mask(desc);\n\n\tpci_msix_clear_and_set_ctrl(dev, PCI_MSIX_FLAGS_ENABLE, 0);\n\tpci_intx_for_msi(dev, 1);\n\tdev->msix_enabled = 0;\n\tpcibios_alloc_irq(dev);\n}\n\n \n\nvoid pci_free_msi_irqs(struct pci_dev *dev)\n{\n\tpci_msi_teardown_msi_irqs(dev);\n\n\tif (dev->msix_base) {\n\t\tiounmap(dev->msix_base);\n\t\tdev->msix_base = NULL;\n\t}\n}\n\n \n\nstruct pci_dev *msi_desc_to_pci_dev(struct msi_desc *desc)\n{\n\treturn to_pci_dev(desc->dev);\n}\nEXPORT_SYMBOL(msi_desc_to_pci_dev);\n\nvoid pci_no_msi(void)\n{\n\tpci_msi_enable = 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}