{
  "module_name": "ntb_transport.c",
  "hash_id": "ca5ece66f9e307e89d406180d3fc09d271a389fd91af38762dd2da71ccd5b4d1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ntb/ntb_transport.c",
  "human_readable_source": " \n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/errno.h>\n#include <linux/export.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/uaccess.h>\n#include \"linux/ntb.h\"\n#include \"linux/ntb_transport.h\"\n\n#define NTB_TRANSPORT_VERSION\t4\n#define NTB_TRANSPORT_VER\t\"4\"\n#define NTB_TRANSPORT_NAME\t\"ntb_transport\"\n#define NTB_TRANSPORT_DESC\t\"Software Queue-Pair Transport over NTB\"\n#define NTB_TRANSPORT_MIN_SPADS (MW0_SZ_HIGH + 2)\n\nMODULE_DESCRIPTION(NTB_TRANSPORT_DESC);\nMODULE_VERSION(NTB_TRANSPORT_VER);\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_AUTHOR(\"Intel Corporation\");\n\nstatic unsigned long max_mw_size;\nmodule_param(max_mw_size, ulong, 0644);\nMODULE_PARM_DESC(max_mw_size, \"Limit size of large memory windows\");\n\nstatic unsigned int transport_mtu = 0x10000;\nmodule_param(transport_mtu, uint, 0644);\nMODULE_PARM_DESC(transport_mtu, \"Maximum size of NTB transport packets\");\n\nstatic unsigned char max_num_clients;\nmodule_param(max_num_clients, byte, 0644);\nMODULE_PARM_DESC(max_num_clients, \"Maximum number of NTB transport clients\");\n\nstatic unsigned int copy_bytes = 1024;\nmodule_param(copy_bytes, uint, 0644);\nMODULE_PARM_DESC(copy_bytes, \"Threshold under which NTB will use the CPU to copy instead of DMA\");\n\nstatic bool use_dma;\nmodule_param(use_dma, bool, 0644);\nMODULE_PARM_DESC(use_dma, \"Use DMA engine to perform large data copy\");\n\nstatic bool use_msi;\n#ifdef CONFIG_NTB_MSI\nmodule_param(use_msi, bool, 0644);\nMODULE_PARM_DESC(use_msi, \"Use MSI interrupts instead of doorbells\");\n#endif\n\nstatic struct dentry *nt_debugfs_dir;\n\n \n#define PIDX\t\tNTB_DEF_PEER_IDX\n\nstruct ntb_queue_entry {\n\t \n\tstruct list_head entry;\n\t \n\tvoid *cb_data;\n\tvoid *buf;\n\tunsigned int len;\n\tunsigned int flags;\n\tint retries;\n\tint errors;\n\tunsigned int tx_index;\n\tunsigned int rx_index;\n\n\tstruct ntb_transport_qp *qp;\n\tunion {\n\t\tstruct ntb_payload_header __iomem *tx_hdr;\n\t\tstruct ntb_payload_header *rx_hdr;\n\t};\n};\n\nstruct ntb_rx_info {\n\tunsigned int entry;\n};\n\nstruct ntb_transport_qp {\n\tstruct ntb_transport_ctx *transport;\n\tstruct ntb_dev *ndev;\n\tvoid *cb_data;\n\tstruct dma_chan *tx_dma_chan;\n\tstruct dma_chan *rx_dma_chan;\n\n\tbool client_ready;\n\tbool link_is_up;\n\tbool active;\n\n\tu8 qp_num;\t \n\tu64 qp_bit;\n\n\tstruct ntb_rx_info __iomem *rx_info;\n\tstruct ntb_rx_info *remote_rx_info;\n\n\tvoid (*tx_handler)(struct ntb_transport_qp *qp, void *qp_data,\n\t\t\t   void *data, int len);\n\tstruct list_head tx_free_q;\n\tspinlock_t ntb_tx_free_q_lock;\n\tvoid __iomem *tx_mw;\n\tphys_addr_t tx_mw_phys;\n\tsize_t tx_mw_size;\n\tdma_addr_t tx_mw_dma_addr;\n\tunsigned int tx_index;\n\tunsigned int tx_max_entry;\n\tunsigned int tx_max_frame;\n\n\tvoid (*rx_handler)(struct ntb_transport_qp *qp, void *qp_data,\n\t\t\t   void *data, int len);\n\tstruct list_head rx_post_q;\n\tstruct list_head rx_pend_q;\n\tstruct list_head rx_free_q;\n\t \n\tspinlock_t ntb_rx_q_lock;\n\tvoid *rx_buff;\n\tunsigned int rx_index;\n\tunsigned int rx_max_entry;\n\tunsigned int rx_max_frame;\n\tunsigned int rx_alloc_entry;\n\tdma_cookie_t last_cookie;\n\tstruct tasklet_struct rxc_db_work;\n\n\tvoid (*event_handler)(void *data, int status);\n\tstruct delayed_work link_work;\n\tstruct work_struct link_cleanup;\n\n\tstruct dentry *debugfs_dir;\n\tstruct dentry *debugfs_stats;\n\n\t \n\tu64 rx_bytes;\n\tu64 rx_pkts;\n\tu64 rx_ring_empty;\n\tu64 rx_err_no_buf;\n\tu64 rx_err_oflow;\n\tu64 rx_err_ver;\n\tu64 rx_memcpy;\n\tu64 rx_async;\n\tu64 tx_bytes;\n\tu64 tx_pkts;\n\tu64 tx_ring_full;\n\tu64 tx_err_no_buf;\n\tu64 tx_memcpy;\n\tu64 tx_async;\n\n\tbool use_msi;\n\tint msi_irq;\n\tstruct ntb_msi_desc msi_desc;\n\tstruct ntb_msi_desc peer_msi_desc;\n};\n\nstruct ntb_transport_mw {\n\tphys_addr_t phys_addr;\n\tresource_size_t phys_size;\n\tvoid __iomem *vbase;\n\tsize_t xlat_size;\n\tsize_t buff_size;\n\tsize_t alloc_size;\n\tvoid *alloc_addr;\n\tvoid *virt_addr;\n\tdma_addr_t dma_addr;\n};\n\nstruct ntb_transport_client_dev {\n\tstruct list_head entry;\n\tstruct ntb_transport_ctx *nt;\n\tstruct device dev;\n};\n\nstruct ntb_transport_ctx {\n\tstruct list_head entry;\n\tstruct list_head client_devs;\n\n\tstruct ntb_dev *ndev;\n\n\tstruct ntb_transport_mw *mw_vec;\n\tstruct ntb_transport_qp *qp_vec;\n\tunsigned int mw_count;\n\tunsigned int qp_count;\n\tu64 qp_bitmap;\n\tu64 qp_bitmap_free;\n\n\tbool use_msi;\n\tunsigned int msi_spad_offset;\n\tu64 msi_db_mask;\n\n\tbool link_is_up;\n\tstruct delayed_work link_work;\n\tstruct work_struct link_cleanup;\n\n\tstruct dentry *debugfs_node_dir;\n};\n\nenum {\n\tDESC_DONE_FLAG = BIT(0),\n\tLINK_DOWN_FLAG = BIT(1),\n};\n\nstruct ntb_payload_header {\n\tunsigned int ver;\n\tunsigned int len;\n\tunsigned int flags;\n};\n\nenum {\n\tVERSION = 0,\n\tQP_LINKS,\n\tNUM_QPS,\n\tNUM_MWS,\n\tMW0_SZ_HIGH,\n\tMW0_SZ_LOW,\n};\n\n#define dev_client_dev(__dev) \\\n\tcontainer_of((__dev), struct ntb_transport_client_dev, dev)\n\n#define drv_client(__drv) \\\n\tcontainer_of((__drv), struct ntb_transport_client, driver)\n\n#define QP_TO_MW(nt, qp)\t((qp) % nt->mw_count)\n#define NTB_QP_DEF_NUM_ENTRIES\t100\n#define NTB_LINK_DOWN_TIMEOUT\t10\n\nstatic void ntb_transport_rxc_db(unsigned long data);\nstatic const struct ntb_ctx_ops ntb_transport_ops;\nstatic struct ntb_client ntb_transport_client;\nstatic int ntb_async_tx_submit(struct ntb_transport_qp *qp,\n\t\t\t       struct ntb_queue_entry *entry);\nstatic void ntb_memcpy_tx(struct ntb_queue_entry *entry, void __iomem *offset);\nstatic int ntb_async_rx_submit(struct ntb_queue_entry *entry, void *offset);\nstatic void ntb_memcpy_rx(struct ntb_queue_entry *entry, void *offset);\n\n\nstatic int ntb_transport_bus_match(struct device *dev,\n\t\t\t\t   struct device_driver *drv)\n{\n\treturn !strncmp(dev_name(dev), drv->name, strlen(drv->name));\n}\n\nstatic int ntb_transport_bus_probe(struct device *dev)\n{\n\tconst struct ntb_transport_client *client;\n\tint rc;\n\n\tget_device(dev);\n\n\tclient = drv_client(dev->driver);\n\trc = client->probe(dev);\n\tif (rc)\n\t\tput_device(dev);\n\n\treturn rc;\n}\n\nstatic void ntb_transport_bus_remove(struct device *dev)\n{\n\tconst struct ntb_transport_client *client;\n\n\tclient = drv_client(dev->driver);\n\tclient->remove(dev);\n\n\tput_device(dev);\n}\n\nstatic struct bus_type ntb_transport_bus = {\n\t.name = \"ntb_transport\",\n\t.match = ntb_transport_bus_match,\n\t.probe = ntb_transport_bus_probe,\n\t.remove = ntb_transport_bus_remove,\n};\n\nstatic LIST_HEAD(ntb_transport_list);\n\nstatic int ntb_bus_init(struct ntb_transport_ctx *nt)\n{\n\tlist_add_tail(&nt->entry, &ntb_transport_list);\n\treturn 0;\n}\n\nstatic void ntb_bus_remove(struct ntb_transport_ctx *nt)\n{\n\tstruct ntb_transport_client_dev *client_dev, *cd;\n\n\tlist_for_each_entry_safe(client_dev, cd, &nt->client_devs, entry) {\n\t\tdev_err(client_dev->dev.parent, \"%s still attached to bus, removing\\n\",\n\t\t\tdev_name(&client_dev->dev));\n\t\tlist_del(&client_dev->entry);\n\t\tdevice_unregister(&client_dev->dev);\n\t}\n\n\tlist_del(&nt->entry);\n}\n\nstatic void ntb_transport_client_release(struct device *dev)\n{\n\tstruct ntb_transport_client_dev *client_dev;\n\n\tclient_dev = dev_client_dev(dev);\n\tkfree(client_dev);\n}\n\n \nvoid ntb_transport_unregister_client_dev(char *device_name)\n{\n\tstruct ntb_transport_client_dev *client, *cd;\n\tstruct ntb_transport_ctx *nt;\n\n\tlist_for_each_entry(nt, &ntb_transport_list, entry)\n\t\tlist_for_each_entry_safe(client, cd, &nt->client_devs, entry)\n\t\t\tif (!strncmp(dev_name(&client->dev), device_name,\n\t\t\t\t     strlen(device_name))) {\n\t\t\t\tlist_del(&client->entry);\n\t\t\t\tdevice_unregister(&client->dev);\n\t\t\t}\n}\nEXPORT_SYMBOL_GPL(ntb_transport_unregister_client_dev);\n\n \nint ntb_transport_register_client_dev(char *device_name)\n{\n\tstruct ntb_transport_client_dev *client_dev;\n\tstruct ntb_transport_ctx *nt;\n\tint node;\n\tint rc, i = 0;\n\n\tif (list_empty(&ntb_transport_list))\n\t\treturn -ENODEV;\n\n\tlist_for_each_entry(nt, &ntb_transport_list, entry) {\n\t\tstruct device *dev;\n\n\t\tnode = dev_to_node(&nt->ndev->dev);\n\n\t\tclient_dev = kzalloc_node(sizeof(*client_dev),\n\t\t\t\t\t  GFP_KERNEL, node);\n\t\tif (!client_dev) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tdev = &client_dev->dev;\n\n\t\t \n\t\tdev_set_name(dev, \"%s%d\", device_name, i);\n\t\tdev->bus = &ntb_transport_bus;\n\t\tdev->release = ntb_transport_client_release;\n\t\tdev->parent = &nt->ndev->dev;\n\n\t\trc = device_register(dev);\n\t\tif (rc) {\n\t\t\tput_device(dev);\n\t\t\tgoto err;\n\t\t}\n\n\t\tlist_add_tail(&client_dev->entry, &nt->client_devs);\n\t\ti++;\n\t}\n\n\treturn 0;\n\nerr:\n\tntb_transport_unregister_client_dev(device_name);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(ntb_transport_register_client_dev);\n\n \nint ntb_transport_register_client(struct ntb_transport_client *drv)\n{\n\tdrv->driver.bus = &ntb_transport_bus;\n\n\tif (list_empty(&ntb_transport_list))\n\t\treturn -ENODEV;\n\n\treturn driver_register(&drv->driver);\n}\nEXPORT_SYMBOL_GPL(ntb_transport_register_client);\n\n \nvoid ntb_transport_unregister_client(struct ntb_transport_client *drv)\n{\n\tdriver_unregister(&drv->driver);\n}\nEXPORT_SYMBOL_GPL(ntb_transport_unregister_client);\n\nstatic ssize_t debugfs_read(struct file *filp, char __user *ubuf, size_t count,\n\t\t\t    loff_t *offp)\n{\n\tstruct ntb_transport_qp *qp;\n\tchar *buf;\n\tssize_t ret, out_offset, out_count;\n\n\tqp = filp->private_data;\n\n\tif (!qp || !qp->link_is_up)\n\t\treturn 0;\n\n\tout_count = 1000;\n\n\tbuf = kmalloc(out_count, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tout_offset = 0;\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"\\nNTB QP stats:\\n\\n\");\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_bytes - \\t%llu\\n\", qp->rx_bytes);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_pkts - \\t%llu\\n\", qp->rx_pkts);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_memcpy - \\t%llu\\n\", qp->rx_memcpy);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_async - \\t%llu\\n\", qp->rx_async);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_ring_empty - %llu\\n\", qp->rx_ring_empty);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_err_no_buf - %llu\\n\", qp->rx_err_no_buf);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_err_oflow - \\t%llu\\n\", qp->rx_err_oflow);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_err_ver - \\t%llu\\n\", qp->rx_err_ver);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_buff - \\t0x%p\\n\", qp->rx_buff);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_index - \\t%u\\n\", qp->rx_index);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_max_entry - \\t%u\\n\", qp->rx_max_entry);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"rx_alloc_entry - \\t%u\\n\\n\", qp->rx_alloc_entry);\n\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"tx_bytes - \\t%llu\\n\", qp->tx_bytes);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"tx_pkts - \\t%llu\\n\", qp->tx_pkts);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"tx_memcpy - \\t%llu\\n\", qp->tx_memcpy);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"tx_async - \\t%llu\\n\", qp->tx_async);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"tx_ring_full - \\t%llu\\n\", qp->tx_ring_full);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"tx_err_no_buf - %llu\\n\", qp->tx_err_no_buf);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"tx_mw - \\t0x%p\\n\", qp->tx_mw);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"tx_index (H) - \\t%u\\n\", qp->tx_index);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"RRI (T) - \\t%u\\n\",\n\t\t\t       qp->remote_rx_info->entry);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"tx_max_entry - \\t%u\\n\", qp->tx_max_entry);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"free tx - \\t%u\\n\",\n\t\t\t       ntb_transport_tx_free_entry(qp));\n\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"\\n\");\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"Using TX DMA - \\t%s\\n\",\n\t\t\t       qp->tx_dma_chan ? \"Yes\" : \"No\");\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"Using RX DMA - \\t%s\\n\",\n\t\t\t       qp->rx_dma_chan ? \"Yes\" : \"No\");\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"QP Link - \\t%s\\n\",\n\t\t\t       qp->link_is_up ? \"Up\" : \"Down\");\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"\\n\");\n\n\tif (out_offset > out_count)\n\t\tout_offset = out_count;\n\n\tret = simple_read_from_buffer(ubuf, count, offp, buf, out_offset);\n\tkfree(buf);\n\treturn ret;\n}\n\nstatic const struct file_operations ntb_qp_debugfs_stats = {\n\t.owner = THIS_MODULE,\n\t.open = simple_open,\n\t.read = debugfs_read,\n};\n\nstatic void ntb_list_add(spinlock_t *lock, struct list_head *entry,\n\t\t\t struct list_head *list)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(lock, flags);\n\tlist_add_tail(entry, list);\n\tspin_unlock_irqrestore(lock, flags);\n}\n\nstatic struct ntb_queue_entry *ntb_list_rm(spinlock_t *lock,\n\t\t\t\t\t   struct list_head *list)\n{\n\tstruct ntb_queue_entry *entry;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(lock, flags);\n\tif (list_empty(list)) {\n\t\tentry = NULL;\n\t\tgoto out;\n\t}\n\tentry = list_first_entry(list, struct ntb_queue_entry, entry);\n\tlist_del(&entry->entry);\n\nout:\n\tspin_unlock_irqrestore(lock, flags);\n\n\treturn entry;\n}\n\nstatic struct ntb_queue_entry *ntb_list_mv(spinlock_t *lock,\n\t\t\t\t\t   struct list_head *list,\n\t\t\t\t\t   struct list_head *to_list)\n{\n\tstruct ntb_queue_entry *entry;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(lock, flags);\n\n\tif (list_empty(list)) {\n\t\tentry = NULL;\n\t} else {\n\t\tentry = list_first_entry(list, struct ntb_queue_entry, entry);\n\t\tlist_move_tail(&entry->entry, to_list);\n\t}\n\n\tspin_unlock_irqrestore(lock, flags);\n\n\treturn entry;\n}\n\nstatic int ntb_transport_setup_qp_mw(struct ntb_transport_ctx *nt,\n\t\t\t\t     unsigned int qp_num)\n{\n\tstruct ntb_transport_qp *qp = &nt->qp_vec[qp_num];\n\tstruct ntb_transport_mw *mw;\n\tstruct ntb_dev *ndev = nt->ndev;\n\tstruct ntb_queue_entry *entry;\n\tunsigned int rx_size, num_qps_mw;\n\tunsigned int mw_num, mw_count, qp_count;\n\tunsigned int i;\n\tint node;\n\n\tmw_count = nt->mw_count;\n\tqp_count = nt->qp_count;\n\n\tmw_num = QP_TO_MW(nt, qp_num);\n\tmw = &nt->mw_vec[mw_num];\n\n\tif (!mw->virt_addr)\n\t\treturn -ENOMEM;\n\n\tif (mw_num < qp_count % mw_count)\n\t\tnum_qps_mw = qp_count / mw_count + 1;\n\telse\n\t\tnum_qps_mw = qp_count / mw_count;\n\n\trx_size = (unsigned int)mw->xlat_size / num_qps_mw;\n\tqp->rx_buff = mw->virt_addr + rx_size * (qp_num / mw_count);\n\trx_size -= sizeof(struct ntb_rx_info);\n\n\tqp->remote_rx_info = qp->rx_buff + rx_size;\n\n\t \n\tqp->rx_max_frame = min(transport_mtu, rx_size / 2);\n\tqp->rx_max_entry = rx_size / qp->rx_max_frame;\n\tqp->rx_index = 0;\n\n\t \n\tnode = dev_to_node(&ndev->dev);\n\tfor (i = qp->rx_alloc_entry; i < qp->rx_max_entry; i++) {\n\t\tentry = kzalloc_node(sizeof(*entry), GFP_KERNEL, node);\n\t\tif (!entry)\n\t\t\treturn -ENOMEM;\n\n\t\tentry->qp = qp;\n\t\tntb_list_add(&qp->ntb_rx_q_lock, &entry->entry,\n\t\t\t     &qp->rx_free_q);\n\t\tqp->rx_alloc_entry++;\n\t}\n\n\tqp->remote_rx_info->entry = qp->rx_max_entry - 1;\n\n\t \n\tfor (i = 0; i < qp->rx_max_entry; i++) {\n\t\tvoid *offset = (qp->rx_buff + qp->rx_max_frame * (i + 1) -\n\t\t\t\tsizeof(struct ntb_payload_header));\n\t\tmemset(offset, 0, sizeof(struct ntb_payload_header));\n\t}\n\n\tqp->rx_pkts = 0;\n\tqp->tx_pkts = 0;\n\tqp->tx_index = 0;\n\n\treturn 0;\n}\n\nstatic irqreturn_t ntb_transport_isr(int irq, void *dev)\n{\n\tstruct ntb_transport_qp *qp = dev;\n\n\ttasklet_schedule(&qp->rxc_db_work);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void ntb_transport_setup_qp_peer_msi(struct ntb_transport_ctx *nt,\n\t\t\t\t\t    unsigned int qp_num)\n{\n\tstruct ntb_transport_qp *qp = &nt->qp_vec[qp_num];\n\tint spad = qp_num * 2 + nt->msi_spad_offset;\n\n\tif (!nt->use_msi)\n\t\treturn;\n\n\tif (spad >= ntb_spad_count(nt->ndev))\n\t\treturn;\n\n\tqp->peer_msi_desc.addr_offset =\n\t\tntb_peer_spad_read(qp->ndev, PIDX, spad);\n\tqp->peer_msi_desc.data =\n\t\tntb_peer_spad_read(qp->ndev, PIDX, spad + 1);\n\n\tdev_dbg(&qp->ndev->pdev->dev, \"QP%d Peer MSI addr=%x data=%x\\n\",\n\t\tqp_num, qp->peer_msi_desc.addr_offset, qp->peer_msi_desc.data);\n\n\tif (qp->peer_msi_desc.addr_offset) {\n\t\tqp->use_msi = true;\n\t\tdev_info(&qp->ndev->pdev->dev,\n\t\t\t \"Using MSI interrupts for QP%d\\n\", qp_num);\n\t}\n}\n\nstatic void ntb_transport_setup_qp_msi(struct ntb_transport_ctx *nt,\n\t\t\t\t       unsigned int qp_num)\n{\n\tstruct ntb_transport_qp *qp = &nt->qp_vec[qp_num];\n\tint spad = qp_num * 2 + nt->msi_spad_offset;\n\tint rc;\n\n\tif (!nt->use_msi)\n\t\treturn;\n\n\tif (spad >= ntb_spad_count(nt->ndev)) {\n\t\tdev_warn_once(&qp->ndev->pdev->dev,\n\t\t\t      \"Not enough SPADS to use MSI interrupts\\n\");\n\t\treturn;\n\t}\n\n\tntb_spad_write(qp->ndev, spad, 0);\n\tntb_spad_write(qp->ndev, spad + 1, 0);\n\n\tif (!qp->msi_irq) {\n\t\tqp->msi_irq = ntbm_msi_request_irq(qp->ndev, ntb_transport_isr,\n\t\t\t\t\t\t   KBUILD_MODNAME, qp,\n\t\t\t\t\t\t   &qp->msi_desc);\n\t\tif (qp->msi_irq < 0) {\n\t\t\tdev_warn(&qp->ndev->pdev->dev,\n\t\t\t\t \"Unable to allocate MSI interrupt for qp%d\\n\",\n\t\t\t\t qp_num);\n\t\t\treturn;\n\t\t}\n\t}\n\n\trc = ntb_spad_write(qp->ndev, spad, qp->msi_desc.addr_offset);\n\tif (rc)\n\t\tgoto err_free_interrupt;\n\n\trc = ntb_spad_write(qp->ndev, spad + 1, qp->msi_desc.data);\n\tif (rc)\n\t\tgoto err_free_interrupt;\n\n\tdev_dbg(&qp->ndev->pdev->dev, \"QP%d MSI %d addr=%x data=%x\\n\",\n\t\tqp_num, qp->msi_irq, qp->msi_desc.addr_offset,\n\t\tqp->msi_desc.data);\n\n\treturn;\n\nerr_free_interrupt:\n\tdevm_free_irq(&nt->ndev->dev, qp->msi_irq, qp);\n}\n\nstatic void ntb_transport_msi_peer_desc_changed(struct ntb_transport_ctx *nt)\n{\n\tint i;\n\n\tdev_dbg(&nt->ndev->pdev->dev, \"Peer MSI descriptors changed\");\n\n\tfor (i = 0; i < nt->qp_count; i++)\n\t\tntb_transport_setup_qp_peer_msi(nt, i);\n}\n\nstatic void ntb_transport_msi_desc_changed(void *data)\n{\n\tstruct ntb_transport_ctx *nt = data;\n\tint i;\n\n\tdev_dbg(&nt->ndev->pdev->dev, \"MSI descriptors changed\");\n\n\tfor (i = 0; i < nt->qp_count; i++)\n\t\tntb_transport_setup_qp_msi(nt, i);\n\n\tntb_peer_db_set(nt->ndev, nt->msi_db_mask);\n}\n\nstatic void ntb_free_mw(struct ntb_transport_ctx *nt, int num_mw)\n{\n\tstruct ntb_transport_mw *mw = &nt->mw_vec[num_mw];\n\tstruct pci_dev *pdev = nt->ndev->pdev;\n\n\tif (!mw->virt_addr)\n\t\treturn;\n\n\tntb_mw_clear_trans(nt->ndev, PIDX, num_mw);\n\tdma_free_coherent(&pdev->dev, mw->alloc_size,\n\t\t\t  mw->alloc_addr, mw->dma_addr);\n\tmw->xlat_size = 0;\n\tmw->buff_size = 0;\n\tmw->alloc_size = 0;\n\tmw->alloc_addr = NULL;\n\tmw->virt_addr = NULL;\n}\n\nstatic int ntb_alloc_mw_buffer(struct ntb_transport_mw *mw,\n\t\t\t       struct device *dma_dev, size_t align)\n{\n\tdma_addr_t dma_addr;\n\tvoid *alloc_addr, *virt_addr;\n\tint rc;\n\n\talloc_addr = dma_alloc_coherent(dma_dev, mw->alloc_size,\n\t\t\t\t\t&dma_addr, GFP_KERNEL);\n\tif (!alloc_addr) {\n\t\tdev_err(dma_dev, \"Unable to alloc MW buff of size %zu\\n\",\n\t\t\tmw->alloc_size);\n\t\treturn -ENOMEM;\n\t}\n\tvirt_addr = alloc_addr;\n\n\t \n\tif (!IS_ALIGNED(dma_addr, align)) {\n\t\tif (mw->alloc_size > mw->buff_size) {\n\t\t\tvirt_addr = PTR_ALIGN(alloc_addr, align);\n\t\t\tdma_addr = ALIGN(dma_addr, align);\n\t\t} else {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tmw->alloc_addr = alloc_addr;\n\tmw->virt_addr = virt_addr;\n\tmw->dma_addr = dma_addr;\n\n\treturn 0;\n\nerr:\n\tdma_free_coherent(dma_dev, mw->alloc_size, alloc_addr, dma_addr);\n\n\treturn rc;\n}\n\nstatic int ntb_set_mw(struct ntb_transport_ctx *nt, int num_mw,\n\t\t      resource_size_t size)\n{\n\tstruct ntb_transport_mw *mw = &nt->mw_vec[num_mw];\n\tstruct pci_dev *pdev = nt->ndev->pdev;\n\tsize_t xlat_size, buff_size;\n\tresource_size_t xlat_align;\n\tresource_size_t xlat_align_size;\n\tint rc;\n\n\tif (!size)\n\t\treturn -EINVAL;\n\n\trc = ntb_mw_get_align(nt->ndev, PIDX, num_mw, &xlat_align,\n\t\t\t      &xlat_align_size, NULL);\n\tif (rc)\n\t\treturn rc;\n\n\txlat_size = round_up(size, xlat_align_size);\n\tbuff_size = round_up(size, xlat_align);\n\n\t \n\tif (mw->xlat_size == xlat_size)\n\t\treturn 0;\n\n\tif (mw->buff_size)\n\t\tntb_free_mw(nt, num_mw);\n\n\t \n\tmw->xlat_size = xlat_size;\n\tmw->buff_size = buff_size;\n\tmw->alloc_size = buff_size;\n\n\trc = ntb_alloc_mw_buffer(mw, &pdev->dev, xlat_align);\n\tif (rc) {\n\t\tmw->alloc_size *= 2;\n\t\trc = ntb_alloc_mw_buffer(mw, &pdev->dev, xlat_align);\n\t\tif (rc) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"Unable to alloc aligned MW buff\\n\");\n\t\t\tmw->xlat_size = 0;\n\t\t\tmw->buff_size = 0;\n\t\t\tmw->alloc_size = 0;\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t \n\trc = ntb_mw_set_trans(nt->ndev, PIDX, num_mw, mw->dma_addr,\n\t\t\t      mw->xlat_size);\n\tif (rc) {\n\t\tdev_err(&pdev->dev, \"Unable to set mw%d translation\", num_mw);\n\t\tntb_free_mw(nt, num_mw);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic void ntb_qp_link_context_reset(struct ntb_transport_qp *qp)\n{\n\tqp->link_is_up = false;\n\tqp->active = false;\n\n\tqp->tx_index = 0;\n\tqp->rx_index = 0;\n\tqp->rx_bytes = 0;\n\tqp->rx_pkts = 0;\n\tqp->rx_ring_empty = 0;\n\tqp->rx_err_no_buf = 0;\n\tqp->rx_err_oflow = 0;\n\tqp->rx_err_ver = 0;\n\tqp->rx_memcpy = 0;\n\tqp->rx_async = 0;\n\tqp->tx_bytes = 0;\n\tqp->tx_pkts = 0;\n\tqp->tx_ring_full = 0;\n\tqp->tx_err_no_buf = 0;\n\tqp->tx_memcpy = 0;\n\tqp->tx_async = 0;\n}\n\nstatic void ntb_qp_link_down_reset(struct ntb_transport_qp *qp)\n{\n\tntb_qp_link_context_reset(qp);\n\tif (qp->remote_rx_info)\n\t\tqp->remote_rx_info->entry = qp->rx_max_entry - 1;\n}\n\nstatic void ntb_qp_link_cleanup(struct ntb_transport_qp *qp)\n{\n\tstruct ntb_transport_ctx *nt = qp->transport;\n\tstruct pci_dev *pdev = nt->ndev->pdev;\n\n\tdev_info(&pdev->dev, \"qp %d: Link Cleanup\\n\", qp->qp_num);\n\n\tcancel_delayed_work_sync(&qp->link_work);\n\tntb_qp_link_down_reset(qp);\n\n\tif (qp->event_handler)\n\t\tqp->event_handler(qp->cb_data, qp->link_is_up);\n}\n\nstatic void ntb_qp_link_cleanup_work(struct work_struct *work)\n{\n\tstruct ntb_transport_qp *qp = container_of(work,\n\t\t\t\t\t\t   struct ntb_transport_qp,\n\t\t\t\t\t\t   link_cleanup);\n\tstruct ntb_transport_ctx *nt = qp->transport;\n\n\tntb_qp_link_cleanup(qp);\n\n\tif (nt->link_is_up)\n\t\tschedule_delayed_work(&qp->link_work,\n\t\t\t\t      msecs_to_jiffies(NTB_LINK_DOWN_TIMEOUT));\n}\n\nstatic void ntb_qp_link_down(struct ntb_transport_qp *qp)\n{\n\tschedule_work(&qp->link_cleanup);\n}\n\nstatic void ntb_transport_link_cleanup(struct ntb_transport_ctx *nt)\n{\n\tstruct ntb_transport_qp *qp;\n\tu64 qp_bitmap_alloc;\n\tunsigned int i, count;\n\n\tqp_bitmap_alloc = nt->qp_bitmap & ~nt->qp_bitmap_free;\n\n\t \n\tfor (i = 0; i < nt->qp_count; i++)\n\t\tif (qp_bitmap_alloc & BIT_ULL(i)) {\n\t\t\tqp = &nt->qp_vec[i];\n\t\t\tntb_qp_link_cleanup(qp);\n\t\t\tcancel_work_sync(&qp->link_cleanup);\n\t\t\tcancel_delayed_work_sync(&qp->link_work);\n\t\t}\n\n\tif (!nt->link_is_up)\n\t\tcancel_delayed_work_sync(&nt->link_work);\n\n\tfor (i = 0; i < nt->mw_count; i++)\n\t\tntb_free_mw(nt, i);\n\n\t \n\tcount = ntb_spad_count(nt->ndev);\n\tfor (i = 0; i < count; i++)\n\t\tntb_spad_write(nt->ndev, i, 0);\n}\n\nstatic void ntb_transport_link_cleanup_work(struct work_struct *work)\n{\n\tstruct ntb_transport_ctx *nt =\n\t\tcontainer_of(work, struct ntb_transport_ctx, link_cleanup);\n\n\tntb_transport_link_cleanup(nt);\n}\n\nstatic void ntb_transport_event_callback(void *data)\n{\n\tstruct ntb_transport_ctx *nt = data;\n\n\tif (ntb_link_is_up(nt->ndev, NULL, NULL) == 1)\n\t\tschedule_delayed_work(&nt->link_work, 0);\n\telse\n\t\tschedule_work(&nt->link_cleanup);\n}\n\nstatic void ntb_transport_link_work(struct work_struct *work)\n{\n\tstruct ntb_transport_ctx *nt =\n\t\tcontainer_of(work, struct ntb_transport_ctx, link_work.work);\n\tstruct ntb_dev *ndev = nt->ndev;\n\tstruct pci_dev *pdev = ndev->pdev;\n\tresource_size_t size;\n\tu32 val;\n\tint rc = 0, i, spad;\n\n\t \n\n\tif (nt->use_msi) {\n\t\trc = ntb_msi_setup_mws(ndev);\n\t\tif (rc) {\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"Failed to register MSI memory window: %d\\n\",\n\t\t\t\t rc);\n\t\t\tnt->use_msi = false;\n\t\t}\n\t}\n\n\tfor (i = 0; i < nt->qp_count; i++)\n\t\tntb_transport_setup_qp_msi(nt, i);\n\n\tfor (i = 0; i < nt->mw_count; i++) {\n\t\tsize = nt->mw_vec[i].phys_size;\n\n\t\tif (max_mw_size && size > max_mw_size)\n\t\t\tsize = max_mw_size;\n\n\t\tspad = MW0_SZ_HIGH + (i * 2);\n\t\tntb_peer_spad_write(ndev, PIDX, spad, upper_32_bits(size));\n\n\t\tspad = MW0_SZ_LOW + (i * 2);\n\t\tntb_peer_spad_write(ndev, PIDX, spad, lower_32_bits(size));\n\t}\n\n\tntb_peer_spad_write(ndev, PIDX, NUM_MWS, nt->mw_count);\n\n\tntb_peer_spad_write(ndev, PIDX, NUM_QPS, nt->qp_count);\n\n\tntb_peer_spad_write(ndev, PIDX, VERSION, NTB_TRANSPORT_VERSION);\n\n\t \n\tval = ntb_spad_read(ndev, VERSION);\n\tdev_dbg(&pdev->dev, \"Remote version = %d\\n\", val);\n\tif (val != NTB_TRANSPORT_VERSION)\n\t\tgoto out;\n\n\tval = ntb_spad_read(ndev, NUM_QPS);\n\tdev_dbg(&pdev->dev, \"Remote max number of qps = %d\\n\", val);\n\tif (val != nt->qp_count)\n\t\tgoto out;\n\n\tval = ntb_spad_read(ndev, NUM_MWS);\n\tdev_dbg(&pdev->dev, \"Remote number of mws = %d\\n\", val);\n\tif (val != nt->mw_count)\n\t\tgoto out;\n\n\tfor (i = 0; i < nt->mw_count; i++) {\n\t\tu64 val64;\n\n\t\tval = ntb_spad_read(ndev, MW0_SZ_HIGH + (i * 2));\n\t\tval64 = (u64)val << 32;\n\n\t\tval = ntb_spad_read(ndev, MW0_SZ_LOW + (i * 2));\n\t\tval64 |= val;\n\n\t\tdev_dbg(&pdev->dev, \"Remote MW%d size = %#llx\\n\", i, val64);\n\n\t\trc = ntb_set_mw(nt, i, val64);\n\t\tif (rc)\n\t\t\tgoto out1;\n\t}\n\n\tnt->link_is_up = true;\n\n\tfor (i = 0; i < nt->qp_count; i++) {\n\t\tstruct ntb_transport_qp *qp = &nt->qp_vec[i];\n\n\t\tntb_transport_setup_qp_mw(nt, i);\n\t\tntb_transport_setup_qp_peer_msi(nt, i);\n\n\t\tif (qp->client_ready)\n\t\t\tschedule_delayed_work(&qp->link_work, 0);\n\t}\n\n\treturn;\n\nout1:\n\tfor (i = 0; i < nt->mw_count; i++)\n\t\tntb_free_mw(nt, i);\n\n\t \n\tif (rc < 0)\n\t\treturn;\n\nout:\n\tif (ntb_link_is_up(ndev, NULL, NULL) == 1)\n\t\tschedule_delayed_work(&nt->link_work,\n\t\t\t\t      msecs_to_jiffies(NTB_LINK_DOWN_TIMEOUT));\n}\n\nstatic void ntb_qp_link_work(struct work_struct *work)\n{\n\tstruct ntb_transport_qp *qp = container_of(work,\n\t\t\t\t\t\t   struct ntb_transport_qp,\n\t\t\t\t\t\t   link_work.work);\n\tstruct pci_dev *pdev = qp->ndev->pdev;\n\tstruct ntb_transport_ctx *nt = qp->transport;\n\tint val;\n\n\tWARN_ON(!nt->link_is_up);\n\n\tval = ntb_spad_read(nt->ndev, QP_LINKS);\n\n\tntb_peer_spad_write(nt->ndev, PIDX, QP_LINKS, val | BIT(qp->qp_num));\n\n\t \n\tdev_dbg_ratelimited(&pdev->dev, \"Remote QP link status = %x\\n\", val);\n\n\t \n\tif (val & BIT(qp->qp_num)) {\n\t\tdev_info(&pdev->dev, \"qp %d: Link Up\\n\", qp->qp_num);\n\t\tqp->link_is_up = true;\n\t\tqp->active = true;\n\n\t\tif (qp->event_handler)\n\t\t\tqp->event_handler(qp->cb_data, qp->link_is_up);\n\n\t\tif (qp->active)\n\t\t\ttasklet_schedule(&qp->rxc_db_work);\n\t} else if (nt->link_is_up)\n\t\tschedule_delayed_work(&qp->link_work,\n\t\t\t\t      msecs_to_jiffies(NTB_LINK_DOWN_TIMEOUT));\n}\n\nstatic int ntb_transport_init_queue(struct ntb_transport_ctx *nt,\n\t\t\t\t    unsigned int qp_num)\n{\n\tstruct ntb_transport_qp *qp;\n\tphys_addr_t mw_base;\n\tresource_size_t mw_size;\n\tunsigned int num_qps_mw, tx_size;\n\tunsigned int mw_num, mw_count, qp_count;\n\tu64 qp_offset;\n\n\tmw_count = nt->mw_count;\n\tqp_count = nt->qp_count;\n\n\tmw_num = QP_TO_MW(nt, qp_num);\n\n\tqp = &nt->qp_vec[qp_num];\n\tqp->qp_num = qp_num;\n\tqp->transport = nt;\n\tqp->ndev = nt->ndev;\n\tqp->client_ready = false;\n\tqp->event_handler = NULL;\n\tntb_qp_link_context_reset(qp);\n\n\tif (mw_num < qp_count % mw_count)\n\t\tnum_qps_mw = qp_count / mw_count + 1;\n\telse\n\t\tnum_qps_mw = qp_count / mw_count;\n\n\tmw_base = nt->mw_vec[mw_num].phys_addr;\n\tmw_size = nt->mw_vec[mw_num].phys_size;\n\n\tif (max_mw_size && mw_size > max_mw_size)\n\t\tmw_size = max_mw_size;\n\n\ttx_size = (unsigned int)mw_size / num_qps_mw;\n\tqp_offset = tx_size * (qp_num / mw_count);\n\n\tqp->tx_mw_size = tx_size;\n\tqp->tx_mw = nt->mw_vec[mw_num].vbase + qp_offset;\n\tif (!qp->tx_mw)\n\t\treturn -EINVAL;\n\n\tqp->tx_mw_phys = mw_base + qp_offset;\n\tif (!qp->tx_mw_phys)\n\t\treturn -EINVAL;\n\n\ttx_size -= sizeof(struct ntb_rx_info);\n\tqp->rx_info = qp->tx_mw + tx_size;\n\n\t \n\tqp->tx_max_frame = min(transport_mtu, tx_size / 2);\n\tqp->tx_max_entry = tx_size / qp->tx_max_frame;\n\n\tif (nt->debugfs_node_dir) {\n\t\tchar debugfs_name[4];\n\n\t\tsnprintf(debugfs_name, 4, \"qp%d\", qp_num);\n\t\tqp->debugfs_dir = debugfs_create_dir(debugfs_name,\n\t\t\t\t\t\t     nt->debugfs_node_dir);\n\n\t\tqp->debugfs_stats = debugfs_create_file(\"stats\", S_IRUSR,\n\t\t\t\t\t\t\tqp->debugfs_dir, qp,\n\t\t\t\t\t\t\t&ntb_qp_debugfs_stats);\n\t} else {\n\t\tqp->debugfs_dir = NULL;\n\t\tqp->debugfs_stats = NULL;\n\t}\n\n\tINIT_DELAYED_WORK(&qp->link_work, ntb_qp_link_work);\n\tINIT_WORK(&qp->link_cleanup, ntb_qp_link_cleanup_work);\n\n\tspin_lock_init(&qp->ntb_rx_q_lock);\n\tspin_lock_init(&qp->ntb_tx_free_q_lock);\n\n\tINIT_LIST_HEAD(&qp->rx_post_q);\n\tINIT_LIST_HEAD(&qp->rx_pend_q);\n\tINIT_LIST_HEAD(&qp->rx_free_q);\n\tINIT_LIST_HEAD(&qp->tx_free_q);\n\n\ttasklet_init(&qp->rxc_db_work, ntb_transport_rxc_db,\n\t\t     (unsigned long)qp);\n\n\treturn 0;\n}\n\nstatic int ntb_transport_probe(struct ntb_client *self, struct ntb_dev *ndev)\n{\n\tstruct ntb_transport_ctx *nt;\n\tstruct ntb_transport_mw *mw;\n\tunsigned int mw_count, qp_count, spad_count, max_mw_count_for_spads;\n\tu64 qp_bitmap;\n\tint node;\n\tint rc, i;\n\n\tmw_count = ntb_peer_mw_count(ndev);\n\n\tif (!ndev->ops->mw_set_trans) {\n\t\tdev_err(&ndev->dev, \"Inbound MW based NTB API is required\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (ntb_db_is_unsafe(ndev))\n\t\tdev_dbg(&ndev->dev,\n\t\t\t\"doorbell is unsafe, proceed anyway...\\n\");\n\tif (ntb_spad_is_unsafe(ndev))\n\t\tdev_dbg(&ndev->dev,\n\t\t\t\"scratchpad is unsafe, proceed anyway...\\n\");\n\n\tif (ntb_peer_port_count(ndev) != NTB_DEF_PEER_CNT)\n\t\tdev_warn(&ndev->dev, \"Multi-port NTB devices unsupported\\n\");\n\n\tnode = dev_to_node(&ndev->dev);\n\n\tnt = kzalloc_node(sizeof(*nt), GFP_KERNEL, node);\n\tif (!nt)\n\t\treturn -ENOMEM;\n\n\tnt->ndev = ndev;\n\n\t \n\tif (use_msi && mw_count > 1) {\n\t\trc = ntb_msi_init(ndev, ntb_transport_msi_desc_changed);\n\t\tif (!rc) {\n\t\t\tmw_count -= 1;\n\t\t\tnt->use_msi = true;\n\t\t}\n\t}\n\n\tspad_count = ntb_spad_count(ndev);\n\n\t \n\n\tif (spad_count < NTB_TRANSPORT_MIN_SPADS) {\n\t\tnt->mw_count = 0;\n\t\trc = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tmax_mw_count_for_spads = (spad_count - MW0_SZ_HIGH) / 2;\n\tnt->mw_count = min(mw_count, max_mw_count_for_spads);\n\n\tnt->msi_spad_offset = nt->mw_count * 2 + MW0_SZ_HIGH;\n\n\tnt->mw_vec = kcalloc_node(mw_count, sizeof(*nt->mw_vec),\n\t\t\t\t  GFP_KERNEL, node);\n\tif (!nt->mw_vec) {\n\t\trc = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tfor (i = 0; i < mw_count; i++) {\n\t\tmw = &nt->mw_vec[i];\n\n\t\trc = ntb_peer_mw_get_addr(ndev, i, &mw->phys_addr,\n\t\t\t\t\t  &mw->phys_size);\n\t\tif (rc)\n\t\t\tgoto err1;\n\n\t\tmw->vbase = ioremap_wc(mw->phys_addr, mw->phys_size);\n\t\tif (!mw->vbase) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err1;\n\t\t}\n\n\t\tmw->buff_size = 0;\n\t\tmw->xlat_size = 0;\n\t\tmw->virt_addr = NULL;\n\t\tmw->dma_addr = 0;\n\t}\n\n\tqp_bitmap = ntb_db_valid_mask(ndev);\n\n\tqp_count = ilog2(qp_bitmap);\n\tif (nt->use_msi) {\n\t\tqp_count -= 1;\n\t\tnt->msi_db_mask = 1 << qp_count;\n\t\tntb_db_clear_mask(ndev, nt->msi_db_mask);\n\t}\n\n\tif (max_num_clients && max_num_clients < qp_count)\n\t\tqp_count = max_num_clients;\n\telse if (nt->mw_count < qp_count)\n\t\tqp_count = nt->mw_count;\n\n\tqp_bitmap &= BIT_ULL(qp_count) - 1;\n\n\tnt->qp_count = qp_count;\n\tnt->qp_bitmap = qp_bitmap;\n\tnt->qp_bitmap_free = qp_bitmap;\n\n\tnt->qp_vec = kcalloc_node(qp_count, sizeof(*nt->qp_vec),\n\t\t\t\t  GFP_KERNEL, node);\n\tif (!nt->qp_vec) {\n\t\trc = -ENOMEM;\n\t\tgoto err1;\n\t}\n\n\tif (nt_debugfs_dir) {\n\t\tnt->debugfs_node_dir =\n\t\t\tdebugfs_create_dir(pci_name(ndev->pdev),\n\t\t\t\t\t   nt_debugfs_dir);\n\t}\n\n\tfor (i = 0; i < qp_count; i++) {\n\t\trc = ntb_transport_init_queue(nt, i);\n\t\tif (rc)\n\t\t\tgoto err2;\n\t}\n\n\tINIT_DELAYED_WORK(&nt->link_work, ntb_transport_link_work);\n\tINIT_WORK(&nt->link_cleanup, ntb_transport_link_cleanup_work);\n\n\trc = ntb_set_ctx(ndev, nt, &ntb_transport_ops);\n\tif (rc)\n\t\tgoto err2;\n\n\tINIT_LIST_HEAD(&nt->client_devs);\n\trc = ntb_bus_init(nt);\n\tif (rc)\n\t\tgoto err3;\n\n\tnt->link_is_up = false;\n\tntb_link_enable(ndev, NTB_SPEED_AUTO, NTB_WIDTH_AUTO);\n\tntb_link_event(ndev);\n\n\treturn 0;\n\nerr3:\n\tntb_clear_ctx(ndev);\nerr2:\n\tkfree(nt->qp_vec);\nerr1:\n\twhile (i--) {\n\t\tmw = &nt->mw_vec[i];\n\t\tiounmap(mw->vbase);\n\t}\n\tkfree(nt->mw_vec);\nerr:\n\tkfree(nt);\n\treturn rc;\n}\n\nstatic void ntb_transport_free(struct ntb_client *self, struct ntb_dev *ndev)\n{\n\tstruct ntb_transport_ctx *nt = ndev->ctx;\n\tstruct ntb_transport_qp *qp;\n\tu64 qp_bitmap_alloc;\n\tint i;\n\n\tntb_transport_link_cleanup(nt);\n\tcancel_work_sync(&nt->link_cleanup);\n\tcancel_delayed_work_sync(&nt->link_work);\n\n\tqp_bitmap_alloc = nt->qp_bitmap & ~nt->qp_bitmap_free;\n\n\t \n\tfor (i = 0; i < nt->qp_count; i++) {\n\t\tqp = &nt->qp_vec[i];\n\t\tif (qp_bitmap_alloc & BIT_ULL(i))\n\t\t\tntb_transport_free_queue(qp);\n\t\tdebugfs_remove_recursive(qp->debugfs_dir);\n\t}\n\n\tntb_link_disable(ndev);\n\tntb_clear_ctx(ndev);\n\n\tntb_bus_remove(nt);\n\n\tfor (i = nt->mw_count; i--; ) {\n\t\tntb_free_mw(nt, i);\n\t\tiounmap(nt->mw_vec[i].vbase);\n\t}\n\n\tkfree(nt->qp_vec);\n\tkfree(nt->mw_vec);\n\tkfree(nt);\n}\n\nstatic void ntb_complete_rxc(struct ntb_transport_qp *qp)\n{\n\tstruct ntb_queue_entry *entry;\n\tvoid *cb_data;\n\tunsigned int len;\n\tunsigned long irqflags;\n\n\tspin_lock_irqsave(&qp->ntb_rx_q_lock, irqflags);\n\n\twhile (!list_empty(&qp->rx_post_q)) {\n\t\tentry = list_first_entry(&qp->rx_post_q,\n\t\t\t\t\t struct ntb_queue_entry, entry);\n\t\tif (!(entry->flags & DESC_DONE_FLAG))\n\t\t\tbreak;\n\n\t\tentry->rx_hdr->flags = 0;\n\t\tiowrite32(entry->rx_index, &qp->rx_info->entry);\n\n\t\tcb_data = entry->cb_data;\n\t\tlen = entry->len;\n\n\t\tlist_move_tail(&entry->entry, &qp->rx_free_q);\n\n\t\tspin_unlock_irqrestore(&qp->ntb_rx_q_lock, irqflags);\n\n\t\tif (qp->rx_handler && qp->client_ready)\n\t\t\tqp->rx_handler(qp, qp->cb_data, cb_data, len);\n\n\t\tspin_lock_irqsave(&qp->ntb_rx_q_lock, irqflags);\n\t}\n\n\tspin_unlock_irqrestore(&qp->ntb_rx_q_lock, irqflags);\n}\n\nstatic void ntb_rx_copy_callback(void *data,\n\t\t\t\t const struct dmaengine_result *res)\n{\n\tstruct ntb_queue_entry *entry = data;\n\n\t \n\tif (res) {\n\t\tenum dmaengine_tx_result dma_err = res->result;\n\n\t\tswitch (dma_err) {\n\t\tcase DMA_TRANS_READ_FAILED:\n\t\tcase DMA_TRANS_WRITE_FAILED:\n\t\t\tentry->errors++;\n\t\t\tfallthrough;\n\t\tcase DMA_TRANS_ABORTED:\n\t\t{\n\t\t\tstruct ntb_transport_qp *qp = entry->qp;\n\t\t\tvoid *offset = qp->rx_buff + qp->rx_max_frame *\n\t\t\t\t\tqp->rx_index;\n\n\t\t\tntb_memcpy_rx(entry, offset);\n\t\t\tqp->rx_memcpy++;\n\t\t\treturn;\n\t\t}\n\n\t\tcase DMA_TRANS_NOERROR:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tentry->flags |= DESC_DONE_FLAG;\n\n\tntb_complete_rxc(entry->qp);\n}\n\nstatic void ntb_memcpy_rx(struct ntb_queue_entry *entry, void *offset)\n{\n\tvoid *buf = entry->buf;\n\tsize_t len = entry->len;\n\n\tmemcpy(buf, offset, len);\n\n\t \n\twmb();\n\n\tntb_rx_copy_callback(entry, NULL);\n}\n\nstatic int ntb_async_rx_submit(struct ntb_queue_entry *entry, void *offset)\n{\n\tstruct dma_async_tx_descriptor *txd;\n\tstruct ntb_transport_qp *qp = entry->qp;\n\tstruct dma_chan *chan = qp->rx_dma_chan;\n\tstruct dma_device *device;\n\tsize_t pay_off, buff_off, len;\n\tstruct dmaengine_unmap_data *unmap;\n\tdma_cookie_t cookie;\n\tvoid *buf = entry->buf;\n\n\tlen = entry->len;\n\tdevice = chan->device;\n\tpay_off = (size_t)offset & ~PAGE_MASK;\n\tbuff_off = (size_t)buf & ~PAGE_MASK;\n\n\tif (!is_dma_copy_aligned(device, pay_off, buff_off, len))\n\t\tgoto err;\n\n\tunmap = dmaengine_get_unmap_data(device->dev, 2, GFP_NOWAIT);\n\tif (!unmap)\n\t\tgoto err;\n\n\tunmap->len = len;\n\tunmap->addr[0] = dma_map_page(device->dev, virt_to_page(offset),\n\t\t\t\t      pay_off, len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(device->dev, unmap->addr[0]))\n\t\tgoto err_get_unmap;\n\n\tunmap->to_cnt = 1;\n\n\tunmap->addr[1] = dma_map_page(device->dev, virt_to_page(buf),\n\t\t\t\t      buff_off, len, DMA_FROM_DEVICE);\n\tif (dma_mapping_error(device->dev, unmap->addr[1]))\n\t\tgoto err_get_unmap;\n\n\tunmap->from_cnt = 1;\n\n\ttxd = device->device_prep_dma_memcpy(chan, unmap->addr[1],\n\t\t\t\t\t     unmap->addr[0], len,\n\t\t\t\t\t     DMA_PREP_INTERRUPT);\n\tif (!txd)\n\t\tgoto err_get_unmap;\n\n\ttxd->callback_result = ntb_rx_copy_callback;\n\ttxd->callback_param = entry;\n\tdma_set_unmap(txd, unmap);\n\n\tcookie = dmaengine_submit(txd);\n\tif (dma_submit_error(cookie))\n\t\tgoto err_set_unmap;\n\n\tdmaengine_unmap_put(unmap);\n\n\tqp->last_cookie = cookie;\n\n\tqp->rx_async++;\n\n\treturn 0;\n\nerr_set_unmap:\n\tdmaengine_unmap_put(unmap);\nerr_get_unmap:\n\tdmaengine_unmap_put(unmap);\nerr:\n\treturn -ENXIO;\n}\n\nstatic void ntb_async_rx(struct ntb_queue_entry *entry, void *offset)\n{\n\tstruct ntb_transport_qp *qp = entry->qp;\n\tstruct dma_chan *chan = qp->rx_dma_chan;\n\tint res;\n\n\tif (!chan)\n\t\tgoto err;\n\n\tif (entry->len < copy_bytes)\n\t\tgoto err;\n\n\tres = ntb_async_rx_submit(entry, offset);\n\tif (res < 0)\n\t\tgoto err;\n\n\tif (!entry->retries)\n\t\tqp->rx_async++;\n\n\treturn;\n\nerr:\n\tntb_memcpy_rx(entry, offset);\n\tqp->rx_memcpy++;\n}\n\nstatic int ntb_process_rxc(struct ntb_transport_qp *qp)\n{\n\tstruct ntb_payload_header *hdr;\n\tstruct ntb_queue_entry *entry;\n\tvoid *offset;\n\n\toffset = qp->rx_buff + qp->rx_max_frame * qp->rx_index;\n\thdr = offset + qp->rx_max_frame - sizeof(struct ntb_payload_header);\n\n\tdev_dbg(&qp->ndev->pdev->dev, \"qp %d: RX ver %u len %d flags %x\\n\",\n\t\tqp->qp_num, hdr->ver, hdr->len, hdr->flags);\n\n\tif (!(hdr->flags & DESC_DONE_FLAG)) {\n\t\tdev_dbg(&qp->ndev->pdev->dev, \"done flag not set\\n\");\n\t\tqp->rx_ring_empty++;\n\t\treturn -EAGAIN;\n\t}\n\n\tif (hdr->flags & LINK_DOWN_FLAG) {\n\t\tdev_dbg(&qp->ndev->pdev->dev, \"link down flag set\\n\");\n\t\tntb_qp_link_down(qp);\n\t\thdr->flags = 0;\n\t\treturn -EAGAIN;\n\t}\n\n\tif (hdr->ver != (u32)qp->rx_pkts) {\n\t\tdev_dbg(&qp->ndev->pdev->dev,\n\t\t\t\"version mismatch, expected %llu - got %u\\n\",\n\t\t\tqp->rx_pkts, hdr->ver);\n\t\tqp->rx_err_ver++;\n\t\treturn -EIO;\n\t}\n\n\tentry = ntb_list_mv(&qp->ntb_rx_q_lock, &qp->rx_pend_q, &qp->rx_post_q);\n\tif (!entry) {\n\t\tdev_dbg(&qp->ndev->pdev->dev, \"no receive buffer\\n\");\n\t\tqp->rx_err_no_buf++;\n\t\treturn -EAGAIN;\n\t}\n\n\tentry->rx_hdr = hdr;\n\tentry->rx_index = qp->rx_index;\n\n\tif (hdr->len > entry->len) {\n\t\tdev_dbg(&qp->ndev->pdev->dev,\n\t\t\t\"receive buffer overflow! Wanted %d got %d\\n\",\n\t\t\thdr->len, entry->len);\n\t\tqp->rx_err_oflow++;\n\n\t\tentry->len = -EIO;\n\t\tentry->flags |= DESC_DONE_FLAG;\n\n\t\tntb_complete_rxc(qp);\n\t} else {\n\t\tdev_dbg(&qp->ndev->pdev->dev,\n\t\t\t\"RX OK index %u ver %u size %d into buf size %d\\n\",\n\t\t\tqp->rx_index, hdr->ver, hdr->len, entry->len);\n\n\t\tqp->rx_bytes += hdr->len;\n\t\tqp->rx_pkts++;\n\n\t\tentry->len = hdr->len;\n\n\t\tntb_async_rx(entry, offset);\n\t}\n\n\tqp->rx_index++;\n\tqp->rx_index %= qp->rx_max_entry;\n\n\treturn 0;\n}\n\nstatic void ntb_transport_rxc_db(unsigned long data)\n{\n\tstruct ntb_transport_qp *qp = (void *)data;\n\tint rc, i;\n\n\tdev_dbg(&qp->ndev->pdev->dev, \"%s: doorbell %d received\\n\",\n\t\t__func__, qp->qp_num);\n\n\t \n\tfor (i = 0; i < qp->rx_max_entry; i++) {\n\t\trc = ntb_process_rxc(qp);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\n\tif (i && qp->rx_dma_chan)\n\t\tdma_async_issue_pending(qp->rx_dma_chan);\n\n\tif (i == qp->rx_max_entry) {\n\t\t \n\t\tif (qp->active)\n\t\t\ttasklet_schedule(&qp->rxc_db_work);\n\t} else if (ntb_db_read(qp->ndev) & BIT_ULL(qp->qp_num)) {\n\t\t \n\t\tntb_db_clear(qp->ndev, BIT_ULL(qp->qp_num));\n\t\t \n\t\tntb_db_read(qp->ndev);\n\n\t\t \n\t\tif (qp->active)\n\t\t\ttasklet_schedule(&qp->rxc_db_work);\n\t}\n}\n\nstatic void ntb_tx_copy_callback(void *data,\n\t\t\t\t const struct dmaengine_result *res)\n{\n\tstruct ntb_queue_entry *entry = data;\n\tstruct ntb_transport_qp *qp = entry->qp;\n\tstruct ntb_payload_header __iomem *hdr = entry->tx_hdr;\n\n\t \n\tif (res) {\n\t\tenum dmaengine_tx_result dma_err = res->result;\n\n\t\tswitch (dma_err) {\n\t\tcase DMA_TRANS_READ_FAILED:\n\t\tcase DMA_TRANS_WRITE_FAILED:\n\t\t\tentry->errors++;\n\t\t\tfallthrough;\n\t\tcase DMA_TRANS_ABORTED:\n\t\t{\n\t\t\tvoid __iomem *offset =\n\t\t\t\tqp->tx_mw + qp->tx_max_frame *\n\t\t\t\tentry->tx_index;\n\n\t\t\t \n\t\t\tntb_memcpy_tx(entry, offset);\n\t\t\tqp->tx_memcpy++;\n\t\t\treturn;\n\t\t}\n\n\t\tcase DMA_TRANS_NOERROR:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tiowrite32(entry->flags | DESC_DONE_FLAG, &hdr->flags);\n\n\tif (qp->use_msi)\n\t\tntb_msi_peer_trigger(qp->ndev, PIDX, &qp->peer_msi_desc);\n\telse\n\t\tntb_peer_db_set(qp->ndev, BIT_ULL(qp->qp_num));\n\n\t \n\tif (entry->len > 0) {\n\t\tqp->tx_bytes += entry->len;\n\n\t\tif (qp->tx_handler)\n\t\t\tqp->tx_handler(qp, qp->cb_data, entry->cb_data,\n\t\t\t\t       entry->len);\n\t}\n\n\tntb_list_add(&qp->ntb_tx_free_q_lock, &entry->entry, &qp->tx_free_q);\n}\n\nstatic void ntb_memcpy_tx(struct ntb_queue_entry *entry, void __iomem *offset)\n{\n#ifdef ARCH_HAS_NOCACHE_UACCESS\n\t \n\t__copy_from_user_inatomic_nocache(offset, entry->buf, entry->len);\n#else\n\tmemcpy_toio(offset, entry->buf, entry->len);\n#endif\n\n\t \n\twmb();\n\n\tntb_tx_copy_callback(entry, NULL);\n}\n\nstatic int ntb_async_tx_submit(struct ntb_transport_qp *qp,\n\t\t\t       struct ntb_queue_entry *entry)\n{\n\tstruct dma_async_tx_descriptor *txd;\n\tstruct dma_chan *chan = qp->tx_dma_chan;\n\tstruct dma_device *device;\n\tsize_t len = entry->len;\n\tvoid *buf = entry->buf;\n\tsize_t dest_off, buff_off;\n\tstruct dmaengine_unmap_data *unmap;\n\tdma_addr_t dest;\n\tdma_cookie_t cookie;\n\n\tdevice = chan->device;\n\tdest = qp->tx_mw_dma_addr + qp->tx_max_frame * entry->tx_index;\n\tbuff_off = (size_t)buf & ~PAGE_MASK;\n\tdest_off = (size_t)dest & ~PAGE_MASK;\n\n\tif (!is_dma_copy_aligned(device, buff_off, dest_off, len))\n\t\tgoto err;\n\n\tunmap = dmaengine_get_unmap_data(device->dev, 1, GFP_NOWAIT);\n\tif (!unmap)\n\t\tgoto err;\n\n\tunmap->len = len;\n\tunmap->addr[0] = dma_map_page(device->dev, virt_to_page(buf),\n\t\t\t\t      buff_off, len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(device->dev, unmap->addr[0]))\n\t\tgoto err_get_unmap;\n\n\tunmap->to_cnt = 1;\n\n\ttxd = device->device_prep_dma_memcpy(chan, dest, unmap->addr[0], len,\n\t\t\t\t\t     DMA_PREP_INTERRUPT);\n\tif (!txd)\n\t\tgoto err_get_unmap;\n\n\ttxd->callback_result = ntb_tx_copy_callback;\n\ttxd->callback_param = entry;\n\tdma_set_unmap(txd, unmap);\n\n\tcookie = dmaengine_submit(txd);\n\tif (dma_submit_error(cookie))\n\t\tgoto err_set_unmap;\n\n\tdmaengine_unmap_put(unmap);\n\n\tdma_async_issue_pending(chan);\n\n\treturn 0;\nerr_set_unmap:\n\tdmaengine_unmap_put(unmap);\nerr_get_unmap:\n\tdmaengine_unmap_put(unmap);\nerr:\n\treturn -ENXIO;\n}\n\nstatic void ntb_async_tx(struct ntb_transport_qp *qp,\n\t\t\t struct ntb_queue_entry *entry)\n{\n\tstruct ntb_payload_header __iomem *hdr;\n\tstruct dma_chan *chan = qp->tx_dma_chan;\n\tvoid __iomem *offset;\n\tint res;\n\n\tentry->tx_index = qp->tx_index;\n\toffset = qp->tx_mw + qp->tx_max_frame * entry->tx_index;\n\thdr = offset + qp->tx_max_frame - sizeof(struct ntb_payload_header);\n\tentry->tx_hdr = hdr;\n\n\tiowrite32(entry->len, &hdr->len);\n\tiowrite32((u32)qp->tx_pkts, &hdr->ver);\n\n\tif (!chan)\n\t\tgoto err;\n\n\tif (entry->len < copy_bytes)\n\t\tgoto err;\n\n\tres = ntb_async_tx_submit(qp, entry);\n\tif (res < 0)\n\t\tgoto err;\n\n\tif (!entry->retries)\n\t\tqp->tx_async++;\n\n\treturn;\n\nerr:\n\tntb_memcpy_tx(entry, offset);\n\tqp->tx_memcpy++;\n}\n\nstatic int ntb_process_tx(struct ntb_transport_qp *qp,\n\t\t\t  struct ntb_queue_entry *entry)\n{\n\tif (!ntb_transport_tx_free_entry(qp)) {\n\t\tqp->tx_ring_full++;\n\t\treturn -EAGAIN;\n\t}\n\n\tif (entry->len > qp->tx_max_frame - sizeof(struct ntb_payload_header)) {\n\t\tif (qp->tx_handler)\n\t\t\tqp->tx_handler(qp, qp->cb_data, NULL, -EIO);\n\n\t\tntb_list_add(&qp->ntb_tx_free_q_lock, &entry->entry,\n\t\t\t     &qp->tx_free_q);\n\t\treturn 0;\n\t}\n\n\tntb_async_tx(qp, entry);\n\n\tqp->tx_index++;\n\tqp->tx_index %= qp->tx_max_entry;\n\n\tqp->tx_pkts++;\n\n\treturn 0;\n}\n\nstatic void ntb_send_link_down(struct ntb_transport_qp *qp)\n{\n\tstruct pci_dev *pdev = qp->ndev->pdev;\n\tstruct ntb_queue_entry *entry;\n\tint i, rc;\n\n\tif (!qp->link_is_up)\n\t\treturn;\n\n\tdev_info(&pdev->dev, \"qp %d: Send Link Down\\n\", qp->qp_num);\n\n\tfor (i = 0; i < NTB_LINK_DOWN_TIMEOUT; i++) {\n\t\tentry = ntb_list_rm(&qp->ntb_tx_free_q_lock, &qp->tx_free_q);\n\t\tif (entry)\n\t\t\tbreak;\n\t\tmsleep(100);\n\t}\n\n\tif (!entry)\n\t\treturn;\n\n\tentry->cb_data = NULL;\n\tentry->buf = NULL;\n\tentry->len = 0;\n\tentry->flags = LINK_DOWN_FLAG;\n\n\trc = ntb_process_tx(qp, entry);\n\tif (rc)\n\t\tdev_err(&pdev->dev, \"ntb: QP%d unable to send linkdown msg\\n\",\n\t\t\tqp->qp_num);\n\n\tntb_qp_link_down_reset(qp);\n}\n\nstatic bool ntb_dma_filter_fn(struct dma_chan *chan, void *node)\n{\n\treturn dev_to_node(&chan->dev->device) == (int)(unsigned long)node;\n}\n\n \nstruct ntb_transport_qp *\nntb_transport_create_queue(void *data, struct device *client_dev,\n\t\t\t   const struct ntb_queue_handlers *handlers)\n{\n\tstruct ntb_dev *ndev;\n\tstruct pci_dev *pdev;\n\tstruct ntb_transport_ctx *nt;\n\tstruct ntb_queue_entry *entry;\n\tstruct ntb_transport_qp *qp;\n\tu64 qp_bit;\n\tunsigned int free_queue;\n\tdma_cap_mask_t dma_mask;\n\tint node;\n\tint i;\n\n\tndev = dev_ntb(client_dev->parent);\n\tpdev = ndev->pdev;\n\tnt = ndev->ctx;\n\n\tnode = dev_to_node(&ndev->dev);\n\n\tfree_queue = ffs(nt->qp_bitmap_free);\n\tif (!free_queue)\n\t\tgoto err;\n\n\t \n\tfree_queue--;\n\n\tqp = &nt->qp_vec[free_queue];\n\tqp_bit = BIT_ULL(qp->qp_num);\n\n\tnt->qp_bitmap_free &= ~qp_bit;\n\n\tqp->cb_data = data;\n\tqp->rx_handler = handlers->rx_handler;\n\tqp->tx_handler = handlers->tx_handler;\n\tqp->event_handler = handlers->event_handler;\n\n\tdma_cap_zero(dma_mask);\n\tdma_cap_set(DMA_MEMCPY, dma_mask);\n\n\tif (use_dma) {\n\t\tqp->tx_dma_chan =\n\t\t\tdma_request_channel(dma_mask, ntb_dma_filter_fn,\n\t\t\t\t\t    (void *)(unsigned long)node);\n\t\tif (!qp->tx_dma_chan)\n\t\t\tdev_info(&pdev->dev, \"Unable to allocate TX DMA channel\\n\");\n\n\t\tqp->rx_dma_chan =\n\t\t\tdma_request_channel(dma_mask, ntb_dma_filter_fn,\n\t\t\t\t\t    (void *)(unsigned long)node);\n\t\tif (!qp->rx_dma_chan)\n\t\t\tdev_info(&pdev->dev, \"Unable to allocate RX DMA channel\\n\");\n\t} else {\n\t\tqp->tx_dma_chan = NULL;\n\t\tqp->rx_dma_chan = NULL;\n\t}\n\n\tqp->tx_mw_dma_addr = 0;\n\tif (qp->tx_dma_chan) {\n\t\tqp->tx_mw_dma_addr =\n\t\t\tdma_map_resource(qp->tx_dma_chan->device->dev,\n\t\t\t\t\t qp->tx_mw_phys, qp->tx_mw_size,\n\t\t\t\t\t DMA_FROM_DEVICE, 0);\n\t\tif (dma_mapping_error(qp->tx_dma_chan->device->dev,\n\t\t\t\t      qp->tx_mw_dma_addr)) {\n\t\t\tqp->tx_mw_dma_addr = 0;\n\t\t\tgoto err1;\n\t\t}\n\t}\n\n\tdev_dbg(&pdev->dev, \"Using %s memcpy for TX\\n\",\n\t\tqp->tx_dma_chan ? \"DMA\" : \"CPU\");\n\n\tdev_dbg(&pdev->dev, \"Using %s memcpy for RX\\n\",\n\t\tqp->rx_dma_chan ? \"DMA\" : \"CPU\");\n\n\tfor (i = 0; i < NTB_QP_DEF_NUM_ENTRIES; i++) {\n\t\tentry = kzalloc_node(sizeof(*entry), GFP_KERNEL, node);\n\t\tif (!entry)\n\t\t\tgoto err1;\n\n\t\tentry->qp = qp;\n\t\tntb_list_add(&qp->ntb_rx_q_lock, &entry->entry,\n\t\t\t     &qp->rx_free_q);\n\t}\n\tqp->rx_alloc_entry = NTB_QP_DEF_NUM_ENTRIES;\n\n\tfor (i = 0; i < qp->tx_max_entry; i++) {\n\t\tentry = kzalloc_node(sizeof(*entry), GFP_KERNEL, node);\n\t\tif (!entry)\n\t\t\tgoto err2;\n\n\t\tentry->qp = qp;\n\t\tntb_list_add(&qp->ntb_tx_free_q_lock, &entry->entry,\n\t\t\t     &qp->tx_free_q);\n\t}\n\n\tntb_db_clear(qp->ndev, qp_bit);\n\tntb_db_clear_mask(qp->ndev, qp_bit);\n\n\tdev_info(&pdev->dev, \"NTB Transport QP %d created\\n\", qp->qp_num);\n\n\treturn qp;\n\nerr2:\n\twhile ((entry = ntb_list_rm(&qp->ntb_tx_free_q_lock, &qp->tx_free_q)))\n\t\tkfree(entry);\nerr1:\n\tqp->rx_alloc_entry = 0;\n\twhile ((entry = ntb_list_rm(&qp->ntb_rx_q_lock, &qp->rx_free_q)))\n\t\tkfree(entry);\n\tif (qp->tx_mw_dma_addr)\n\t\tdma_unmap_resource(qp->tx_dma_chan->device->dev,\n\t\t\t\t   qp->tx_mw_dma_addr, qp->tx_mw_size,\n\t\t\t\t   DMA_FROM_DEVICE, 0);\n\tif (qp->tx_dma_chan)\n\t\tdma_release_channel(qp->tx_dma_chan);\n\tif (qp->rx_dma_chan)\n\t\tdma_release_channel(qp->rx_dma_chan);\n\tnt->qp_bitmap_free |= qp_bit;\nerr:\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ntb_transport_create_queue);\n\n \nvoid ntb_transport_free_queue(struct ntb_transport_qp *qp)\n{\n\tstruct pci_dev *pdev;\n\tstruct ntb_queue_entry *entry;\n\tu64 qp_bit;\n\n\tif (!qp)\n\t\treturn;\n\n\tpdev = qp->ndev->pdev;\n\n\tqp->active = false;\n\n\tif (qp->tx_dma_chan) {\n\t\tstruct dma_chan *chan = qp->tx_dma_chan;\n\t\t \n\t\tqp->tx_dma_chan = NULL;\n\n\t\t \n\t\tdma_sync_wait(chan, qp->last_cookie);\n\t\tdmaengine_terminate_all(chan);\n\n\t\tdma_unmap_resource(chan->device->dev,\n\t\t\t\t   qp->tx_mw_dma_addr, qp->tx_mw_size,\n\t\t\t\t   DMA_FROM_DEVICE, 0);\n\n\t\tdma_release_channel(chan);\n\t}\n\n\tif (qp->rx_dma_chan) {\n\t\tstruct dma_chan *chan = qp->rx_dma_chan;\n\t\t \n\t\tqp->rx_dma_chan = NULL;\n\n\t\t \n\t\tdma_sync_wait(chan, qp->last_cookie);\n\t\tdmaengine_terminate_all(chan);\n\t\tdma_release_channel(chan);\n\t}\n\n\tqp_bit = BIT_ULL(qp->qp_num);\n\n\tntb_db_set_mask(qp->ndev, qp_bit);\n\ttasklet_kill(&qp->rxc_db_work);\n\n\tcancel_delayed_work_sync(&qp->link_work);\n\n\tqp->cb_data = NULL;\n\tqp->rx_handler = NULL;\n\tqp->tx_handler = NULL;\n\tqp->event_handler = NULL;\n\n\twhile ((entry = ntb_list_rm(&qp->ntb_rx_q_lock, &qp->rx_free_q)))\n\t\tkfree(entry);\n\n\twhile ((entry = ntb_list_rm(&qp->ntb_rx_q_lock, &qp->rx_pend_q))) {\n\t\tdev_warn(&pdev->dev, \"Freeing item from non-empty rx_pend_q\\n\");\n\t\tkfree(entry);\n\t}\n\n\twhile ((entry = ntb_list_rm(&qp->ntb_rx_q_lock, &qp->rx_post_q))) {\n\t\tdev_warn(&pdev->dev, \"Freeing item from non-empty rx_post_q\\n\");\n\t\tkfree(entry);\n\t}\n\n\twhile ((entry = ntb_list_rm(&qp->ntb_tx_free_q_lock, &qp->tx_free_q)))\n\t\tkfree(entry);\n\n\tqp->transport->qp_bitmap_free |= qp_bit;\n\n\tdev_info(&pdev->dev, \"NTB Transport QP %d freed\\n\", qp->qp_num);\n}\nEXPORT_SYMBOL_GPL(ntb_transport_free_queue);\n\n \nvoid *ntb_transport_rx_remove(struct ntb_transport_qp *qp, unsigned int *len)\n{\n\tstruct ntb_queue_entry *entry;\n\tvoid *buf;\n\n\tif (!qp || qp->client_ready)\n\t\treturn NULL;\n\n\tentry = ntb_list_rm(&qp->ntb_rx_q_lock, &qp->rx_pend_q);\n\tif (!entry)\n\t\treturn NULL;\n\n\tbuf = entry->cb_data;\n\t*len = entry->len;\n\n\tntb_list_add(&qp->ntb_rx_q_lock, &entry->entry, &qp->rx_free_q);\n\n\treturn buf;\n}\nEXPORT_SYMBOL_GPL(ntb_transport_rx_remove);\n\n \nint ntb_transport_rx_enqueue(struct ntb_transport_qp *qp, void *cb, void *data,\n\t\t\t     unsigned int len)\n{\n\tstruct ntb_queue_entry *entry;\n\n\tif (!qp)\n\t\treturn -EINVAL;\n\n\tentry = ntb_list_rm(&qp->ntb_rx_q_lock, &qp->rx_free_q);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\tentry->cb_data = cb;\n\tentry->buf = data;\n\tentry->len = len;\n\tentry->flags = 0;\n\tentry->retries = 0;\n\tentry->errors = 0;\n\tentry->rx_index = 0;\n\n\tntb_list_add(&qp->ntb_rx_q_lock, &entry->entry, &qp->rx_pend_q);\n\n\tif (qp->active)\n\t\ttasklet_schedule(&qp->rxc_db_work);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ntb_transport_rx_enqueue);\n\n \nint ntb_transport_tx_enqueue(struct ntb_transport_qp *qp, void *cb, void *data,\n\t\t\t     unsigned int len)\n{\n\tstruct ntb_queue_entry *entry;\n\tint rc;\n\n\tif (!qp || !len)\n\t\treturn -EINVAL;\n\n\t \n\tif (!qp->link_is_up)\n\t\treturn 0;\n\n\tentry = ntb_list_rm(&qp->ntb_tx_free_q_lock, &qp->tx_free_q);\n\tif (!entry) {\n\t\tqp->tx_err_no_buf++;\n\t\treturn -EBUSY;\n\t}\n\n\tentry->cb_data = cb;\n\tentry->buf = data;\n\tentry->len = len;\n\tentry->flags = 0;\n\tentry->errors = 0;\n\tentry->retries = 0;\n\tentry->tx_index = 0;\n\n\trc = ntb_process_tx(qp, entry);\n\tif (rc)\n\t\tntb_list_add(&qp->ntb_tx_free_q_lock, &entry->entry,\n\t\t\t     &qp->tx_free_q);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(ntb_transport_tx_enqueue);\n\n \nvoid ntb_transport_link_up(struct ntb_transport_qp *qp)\n{\n\tif (!qp)\n\t\treturn;\n\n\tqp->client_ready = true;\n\n\tif (qp->transport->link_is_up)\n\t\tschedule_delayed_work(&qp->link_work, 0);\n}\nEXPORT_SYMBOL_GPL(ntb_transport_link_up);\n\n \nvoid ntb_transport_link_down(struct ntb_transport_qp *qp)\n{\n\tint val;\n\n\tif (!qp)\n\t\treturn;\n\n\tqp->client_ready = false;\n\n\tval = ntb_spad_read(qp->ndev, QP_LINKS);\n\n\tntb_peer_spad_write(qp->ndev, PIDX, QP_LINKS, val & ~BIT(qp->qp_num));\n\n\tif (qp->link_is_up)\n\t\tntb_send_link_down(qp);\n\telse\n\t\tcancel_delayed_work_sync(&qp->link_work);\n}\nEXPORT_SYMBOL_GPL(ntb_transport_link_down);\n\n \nbool ntb_transport_link_query(struct ntb_transport_qp *qp)\n{\n\tif (!qp)\n\t\treturn false;\n\n\treturn qp->link_is_up;\n}\nEXPORT_SYMBOL_GPL(ntb_transport_link_query);\n\n \nunsigned char ntb_transport_qp_num(struct ntb_transport_qp *qp)\n{\n\tif (!qp)\n\t\treturn 0;\n\n\treturn qp->qp_num;\n}\nEXPORT_SYMBOL_GPL(ntb_transport_qp_num);\n\n \nunsigned int ntb_transport_max_size(struct ntb_transport_qp *qp)\n{\n\tunsigned int max_size;\n\tunsigned int copy_align;\n\tstruct dma_chan *rx_chan, *tx_chan;\n\n\tif (!qp)\n\t\treturn 0;\n\n\trx_chan = qp->rx_dma_chan;\n\ttx_chan = qp->tx_dma_chan;\n\n\tcopy_align = max(rx_chan ? rx_chan->device->copy_align : 0,\n\t\t\t tx_chan ? tx_chan->device->copy_align : 0);\n\n\t \n\tmax_size = qp->tx_max_frame - sizeof(struct ntb_payload_header);\n\tmax_size = round_down(max_size, 1 << copy_align);\n\n\treturn max_size;\n}\nEXPORT_SYMBOL_GPL(ntb_transport_max_size);\n\nunsigned int ntb_transport_tx_free_entry(struct ntb_transport_qp *qp)\n{\n\tunsigned int head = qp->tx_index;\n\tunsigned int tail = qp->remote_rx_info->entry;\n\n\treturn tail >= head ? tail - head : qp->tx_max_entry + tail - head;\n}\nEXPORT_SYMBOL_GPL(ntb_transport_tx_free_entry);\n\nstatic void ntb_transport_doorbell_callback(void *data, int vector)\n{\n\tstruct ntb_transport_ctx *nt = data;\n\tstruct ntb_transport_qp *qp;\n\tu64 db_bits;\n\tunsigned int qp_num;\n\n\tif (ntb_db_read(nt->ndev) & nt->msi_db_mask) {\n\t\tntb_transport_msi_peer_desc_changed(nt);\n\t\tntb_db_clear(nt->ndev, nt->msi_db_mask);\n\t}\n\n\tdb_bits = (nt->qp_bitmap & ~nt->qp_bitmap_free &\n\t\t   ntb_db_vector_mask(nt->ndev, vector));\n\n\twhile (db_bits) {\n\t\tqp_num = __ffs(db_bits);\n\t\tqp = &nt->qp_vec[qp_num];\n\n\t\tif (qp->active)\n\t\t\ttasklet_schedule(&qp->rxc_db_work);\n\n\t\tdb_bits &= ~BIT_ULL(qp_num);\n\t}\n}\n\nstatic const struct ntb_ctx_ops ntb_transport_ops = {\n\t.link_event = ntb_transport_event_callback,\n\t.db_event = ntb_transport_doorbell_callback,\n};\n\nstatic struct ntb_client ntb_transport_client = {\n\t.ops = {\n\t\t.probe = ntb_transport_probe,\n\t\t.remove = ntb_transport_free,\n\t},\n};\n\nstatic int __init ntb_transport_init(void)\n{\n\tint rc;\n\n\tpr_info(\"%s, version %s\\n\", NTB_TRANSPORT_DESC, NTB_TRANSPORT_VER);\n\n\tif (debugfs_initialized())\n\t\tnt_debugfs_dir = debugfs_create_dir(KBUILD_MODNAME, NULL);\n\n\trc = bus_register(&ntb_transport_bus);\n\tif (rc)\n\t\tgoto err_bus;\n\n\trc = ntb_register_client(&ntb_transport_client);\n\tif (rc)\n\t\tgoto err_client;\n\n\treturn 0;\n\nerr_client:\n\tbus_unregister(&ntb_transport_bus);\nerr_bus:\n\tdebugfs_remove_recursive(nt_debugfs_dir);\n\treturn rc;\n}\nmodule_init(ntb_transport_init);\n\nstatic void __exit ntb_transport_exit(void)\n{\n\tntb_unregister_client(&ntb_transport_client);\n\tbus_unregister(&ntb_transport_bus);\n\tdebugfs_remove_recursive(nt_debugfs_dir);\n}\nmodule_exit(ntb_transport_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}