{
  "module_name": "ntb_hw_gen1.c",
  "hash_id": "a94f40998ea1c85e58b89ec077f3486ead3c79647a5366e3bd26ea67eea36aa0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ntb/hw/intel/ntb_hw_gen1.c",
  "human_readable_source": " \n\n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n#include <linux/ntb.h>\n\n#include \"ntb_hw_intel.h\"\n#include \"ntb_hw_gen1.h\"\n#include \"ntb_hw_gen3.h\"\n#include \"ntb_hw_gen4.h\"\n\n#define NTB_NAME\t\"ntb_hw_intel\"\n#define NTB_DESC\t\"Intel(R) PCI-E Non-Transparent Bridge Driver\"\n#define NTB_VER\t\t\"2.0\"\n\nMODULE_DESCRIPTION(NTB_DESC);\nMODULE_VERSION(NTB_VER);\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_AUTHOR(\"Intel Corporation\");\n\n#define bar0_off(base, bar) ((base) + ((bar) << 2))\n#define bar2_off(base, bar) bar0_off(base, (bar) - 2)\n\nstatic const struct intel_ntb_reg xeon_reg;\nstatic const struct intel_ntb_alt_reg xeon_pri_reg;\nstatic const struct intel_ntb_alt_reg xeon_sec_reg;\nstatic const struct intel_ntb_alt_reg xeon_b2b_reg;\nstatic const struct intel_ntb_xlat_reg xeon_pri_xlat;\nstatic const struct intel_ntb_xlat_reg xeon_sec_xlat;\nstatic const struct ntb_dev_ops intel_ntb_ops;\n\nstatic const struct file_operations intel_ntb_debugfs_info;\nstatic struct dentry *debugfs_dir;\n\nstatic int b2b_mw_idx = -1;\nmodule_param(b2b_mw_idx, int, 0644);\nMODULE_PARM_DESC(b2b_mw_idx, \"Use this mw idx to access the peer ntb.  A \"\n\t\t \"value of zero or positive starts from first mw idx, and a \"\n\t\t \"negative value starts from last mw idx.  Both sides MUST \"\n\t\t \"set the same value here!\");\n\nstatic unsigned int b2b_mw_share;\nmodule_param(b2b_mw_share, uint, 0644);\nMODULE_PARM_DESC(b2b_mw_share, \"If the b2b mw is large enough, configure the \"\n\t\t \"ntb so that the peer ntb only occupies the first half of \"\n\t\t \"the mw, so the second half can still be used as a mw.  Both \"\n\t\t \"sides MUST set the same value here!\");\n\nmodule_param_named(xeon_b2b_usd_bar2_addr64,\n\t\t   xeon_b2b_usd_addr.bar2_addr64, ullong, 0644);\nMODULE_PARM_DESC(xeon_b2b_usd_bar2_addr64,\n\t\t \"XEON B2B USD BAR 2 64-bit address\");\n\nmodule_param_named(xeon_b2b_usd_bar4_addr64,\n\t\t   xeon_b2b_usd_addr.bar4_addr64, ullong, 0644);\nMODULE_PARM_DESC(xeon_b2b_usd_bar4_addr64,\n\t\t \"XEON B2B USD BAR 4 64-bit address\");\n\nmodule_param_named(xeon_b2b_usd_bar4_addr32,\n\t\t   xeon_b2b_usd_addr.bar4_addr32, ullong, 0644);\nMODULE_PARM_DESC(xeon_b2b_usd_bar4_addr32,\n\t\t \"XEON B2B USD split-BAR 4 32-bit address\");\n\nmodule_param_named(xeon_b2b_usd_bar5_addr32,\n\t\t   xeon_b2b_usd_addr.bar5_addr32, ullong, 0644);\nMODULE_PARM_DESC(xeon_b2b_usd_bar5_addr32,\n\t\t \"XEON B2B USD split-BAR 5 32-bit address\");\n\nmodule_param_named(xeon_b2b_dsd_bar2_addr64,\n\t\t   xeon_b2b_dsd_addr.bar2_addr64, ullong, 0644);\nMODULE_PARM_DESC(xeon_b2b_dsd_bar2_addr64,\n\t\t \"XEON B2B DSD BAR 2 64-bit address\");\n\nmodule_param_named(xeon_b2b_dsd_bar4_addr64,\n\t\t   xeon_b2b_dsd_addr.bar4_addr64, ullong, 0644);\nMODULE_PARM_DESC(xeon_b2b_dsd_bar4_addr64,\n\t\t \"XEON B2B DSD BAR 4 64-bit address\");\n\nmodule_param_named(xeon_b2b_dsd_bar4_addr32,\n\t\t   xeon_b2b_dsd_addr.bar4_addr32, ullong, 0644);\nMODULE_PARM_DESC(xeon_b2b_dsd_bar4_addr32,\n\t\t \"XEON B2B DSD split-BAR 4 32-bit address\");\n\nmodule_param_named(xeon_b2b_dsd_bar5_addr32,\n\t\t   xeon_b2b_dsd_addr.bar5_addr32, ullong, 0644);\nMODULE_PARM_DESC(xeon_b2b_dsd_bar5_addr32,\n\t\t \"XEON B2B DSD split-BAR 5 32-bit address\");\n\n\nstatic int xeon_init_isr(struct intel_ntb_dev *ndev);\n\nstatic inline void ndev_reset_unsafe_flags(struct intel_ntb_dev *ndev)\n{\n\tndev->unsafe_flags = 0;\n\tndev->unsafe_flags_ignore = 0;\n\n\t \n\tif (ndev->hwerr_flags & NTB_HWERR_SDOORBELL_LOCKUP)\n\t\tif (!ntb_topo_is_b2b(ndev->ntb.topo))\n\t\t\tndev->unsafe_flags |= NTB_UNSAFE_DB;\n\n\t \n\tif (ndev->hwerr_flags & NTB_HWERR_SB01BASE_LOCKUP) {\n\t\tndev->unsafe_flags |= NTB_UNSAFE_DB;\n\t\tndev->unsafe_flags |= NTB_UNSAFE_SPAD;\n\t}\n}\n\nstatic inline int ndev_is_unsafe(struct intel_ntb_dev *ndev,\n\t\t\t\t unsigned long flag)\n{\n\treturn !!(flag & ndev->unsafe_flags & ~ndev->unsafe_flags_ignore);\n}\n\nstatic inline int ndev_ignore_unsafe(struct intel_ntb_dev *ndev,\n\t\t\t\t     unsigned long flag)\n{\n\tflag &= ndev->unsafe_flags;\n\tndev->unsafe_flags_ignore |= flag;\n\n\treturn !!flag;\n}\n\nint ndev_mw_to_bar(struct intel_ntb_dev *ndev, int idx)\n{\n\tif (idx < 0 || idx >= ndev->mw_count)\n\t\treturn -EINVAL;\n\treturn ndev->reg->mw_bar[idx];\n}\n\nvoid ndev_db_addr(struct intel_ntb_dev *ndev,\n\t\t\t       phys_addr_t *db_addr, resource_size_t *db_size,\n\t\t\t       phys_addr_t reg_addr, unsigned long reg)\n{\n\tif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\n\t\tpr_warn_once(\"%s: NTB unsafe doorbell access\", __func__);\n\n\tif (db_addr) {\n\t\t*db_addr = reg_addr + reg;\n\t\tdev_dbg(&ndev->ntb.pdev->dev, \"Peer db addr %llx\\n\", *db_addr);\n\t}\n\n\tif (db_size) {\n\t\t*db_size = ndev->reg->db_size;\n\t\tdev_dbg(&ndev->ntb.pdev->dev, \"Peer db size %llx\\n\", *db_size);\n\t}\n}\n\nu64 ndev_db_read(struct intel_ntb_dev *ndev,\n\t\t\t       void __iomem *mmio)\n{\n\tif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\n\t\tpr_warn_once(\"%s: NTB unsafe doorbell access\", __func__);\n\n\treturn ndev->reg->db_ioread(mmio);\n}\n\nint ndev_db_write(struct intel_ntb_dev *ndev, u64 db_bits,\n\t\t\t\tvoid __iomem *mmio)\n{\n\tif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\n\t\tpr_warn_once(\"%s: NTB unsafe doorbell access\", __func__);\n\n\tif (db_bits & ~ndev->db_valid_mask)\n\t\treturn -EINVAL;\n\n\tndev->reg->db_iowrite(db_bits, mmio);\n\n\treturn 0;\n}\n\nstatic inline int ndev_db_set_mask(struct intel_ntb_dev *ndev, u64 db_bits,\n\t\t\t\t   void __iomem *mmio)\n{\n\tunsigned long irqflags;\n\n\tif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\n\t\tpr_warn_once(\"%s: NTB unsafe doorbell access\", __func__);\n\n\tif (db_bits & ~ndev->db_valid_mask)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&ndev->db_mask_lock, irqflags);\n\t{\n\t\tndev->db_mask |= db_bits;\n\t\tndev->reg->db_iowrite(ndev->db_mask, mmio);\n\t}\n\tspin_unlock_irqrestore(&ndev->db_mask_lock, irqflags);\n\n\treturn 0;\n}\n\nstatic inline int ndev_db_clear_mask(struct intel_ntb_dev *ndev, u64 db_bits,\n\t\t\t\t     void __iomem *mmio)\n{\n\tunsigned long irqflags;\n\n\tif (ndev_is_unsafe(ndev, NTB_UNSAFE_DB))\n\t\tpr_warn_once(\"%s: NTB unsafe doorbell access\", __func__);\n\n\tif (db_bits & ~ndev->db_valid_mask)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&ndev->db_mask_lock, irqflags);\n\t{\n\t\tndev->db_mask &= ~db_bits;\n\t\tndev->reg->db_iowrite(ndev->db_mask, mmio);\n\t}\n\tspin_unlock_irqrestore(&ndev->db_mask_lock, irqflags);\n\n\treturn 0;\n}\n\nstatic inline u64 ndev_vec_mask(struct intel_ntb_dev *ndev, int db_vector)\n{\n\tu64 shift, mask;\n\n\tshift = ndev->db_vec_shift;\n\tmask = BIT_ULL(shift) - 1;\n\n\treturn mask << (shift * db_vector);\n}\n\nstatic inline int ndev_spad_addr(struct intel_ntb_dev *ndev, int idx,\n\t\t\t\t phys_addr_t *spad_addr, phys_addr_t reg_addr,\n\t\t\t\t unsigned long reg)\n{\n\tif (ndev_is_unsafe(ndev, NTB_UNSAFE_SPAD))\n\t\tpr_warn_once(\"%s: NTB unsafe scratchpad access\", __func__);\n\n\tif (idx < 0 || idx >= ndev->spad_count)\n\t\treturn -EINVAL;\n\n\tif (spad_addr) {\n\t\t*spad_addr = reg_addr + reg + (idx << 2);\n\t\tdev_dbg(&ndev->ntb.pdev->dev, \"Peer spad addr %llx\\n\",\n\t\t\t*spad_addr);\n\t}\n\n\treturn 0;\n}\n\nstatic inline u32 ndev_spad_read(struct intel_ntb_dev *ndev, int idx,\n\t\t\t\t void __iomem *mmio)\n{\n\tif (ndev_is_unsafe(ndev, NTB_UNSAFE_SPAD))\n\t\tpr_warn_once(\"%s: NTB unsafe scratchpad access\", __func__);\n\n\tif (idx < 0 || idx >= ndev->spad_count)\n\t\treturn 0;\n\n\treturn ioread32(mmio + (idx << 2));\n}\n\nstatic inline int ndev_spad_write(struct intel_ntb_dev *ndev, int idx, u32 val,\n\t\t\t\t  void __iomem *mmio)\n{\n\tif (ndev_is_unsafe(ndev, NTB_UNSAFE_SPAD))\n\t\tpr_warn_once(\"%s: NTB unsafe scratchpad access\", __func__);\n\n\tif (idx < 0 || idx >= ndev->spad_count)\n\t\treturn -EINVAL;\n\n\tiowrite32(val, mmio + (idx << 2));\n\n\treturn 0;\n}\n\nstatic irqreturn_t ndev_interrupt(struct intel_ntb_dev *ndev, int vec)\n{\n\tu64 vec_mask;\n\n\tvec_mask = ndev_vec_mask(ndev, vec);\n\n\tif ((ndev->hwerr_flags & NTB_HWERR_MSIX_VECTOR32_BAD) && (vec == 31))\n\t\tvec_mask |= ndev->db_link_mask;\n\n\tdev_dbg(&ndev->ntb.pdev->dev, \"vec %d vec_mask %llx\\n\", vec, vec_mask);\n\n\tndev->last_ts = jiffies;\n\n\tif (vec_mask & ndev->db_link_mask) {\n\t\tif (ndev->reg->poll_link(ndev))\n\t\t\tntb_link_event(&ndev->ntb);\n\t}\n\n\tif (vec_mask & ndev->db_valid_mask)\n\t\tntb_db_event(&ndev->ntb, vec);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t ndev_vec_isr(int irq, void *dev)\n{\n\tstruct intel_ntb_vec *nvec = dev;\n\n\tdev_dbg(&nvec->ndev->ntb.pdev->dev, \"irq: %d  nvec->num: %d\\n\",\n\t\tirq, nvec->num);\n\n\treturn ndev_interrupt(nvec->ndev, nvec->num);\n}\n\nstatic irqreturn_t ndev_irq_isr(int irq, void *dev)\n{\n\tstruct intel_ntb_dev *ndev = dev;\n\n\treturn ndev_interrupt(ndev, irq - ndev->ntb.pdev->irq);\n}\n\nint ndev_init_isr(struct intel_ntb_dev *ndev,\n\t\t\t int msix_min, int msix_max,\n\t\t\t int msix_shift, int total_shift)\n{\n\tstruct pci_dev *pdev;\n\tint rc, i, msix_count, node;\n\n\tpdev = ndev->ntb.pdev;\n\n\tnode = dev_to_node(&pdev->dev);\n\n\t \n\tndev->db_mask = ndev->db_valid_mask;\n\tndev->reg->db_iowrite(ndev->db_mask,\n\t\t\t      ndev->self_mmio +\n\t\t\t      ndev->self_reg->db_mask);\n\n\t \n\n\tndev->vec = kcalloc_node(msix_max, sizeof(*ndev->vec),\n\t\t\t\t GFP_KERNEL, node);\n\tif (!ndev->vec)\n\t\tgoto err_msix_vec_alloc;\n\n\tndev->msix = kcalloc_node(msix_max, sizeof(*ndev->msix),\n\t\t\t\t  GFP_KERNEL, node);\n\tif (!ndev->msix)\n\t\tgoto err_msix_alloc;\n\n\tfor (i = 0; i < msix_max; ++i)\n\t\tndev->msix[i].entry = i;\n\n\tmsix_count = pci_enable_msix_range(pdev, ndev->msix,\n\t\t\t\t\t   msix_min, msix_max);\n\tif (msix_count < 0)\n\t\tgoto err_msix_enable;\n\n\tfor (i = 0; i < msix_count; ++i) {\n\t\tndev->vec[i].ndev = ndev;\n\t\tndev->vec[i].num = i;\n\t\trc = request_irq(ndev->msix[i].vector, ndev_vec_isr, 0,\n\t\t\t\t \"ndev_vec_isr\", &ndev->vec[i]);\n\t\tif (rc)\n\t\t\tgoto err_msix_request;\n\t}\n\n\tdev_dbg(&pdev->dev, \"Using %d msix interrupts\\n\", msix_count);\n\tndev->db_vec_count = msix_count;\n\tndev->db_vec_shift = msix_shift;\n\treturn 0;\n\nerr_msix_request:\n\twhile (i-- > 0)\n\t\tfree_irq(ndev->msix[i].vector, &ndev->vec[i]);\n\tpci_disable_msix(pdev);\nerr_msix_enable:\n\tkfree(ndev->msix);\nerr_msix_alloc:\n\tkfree(ndev->vec);\nerr_msix_vec_alloc:\n\tndev->msix = NULL;\n\tndev->vec = NULL;\n\n\t \n\n\trc = pci_enable_msi(pdev);\n\tif (rc)\n\t\tgoto err_msi_enable;\n\n\trc = request_irq(pdev->irq, ndev_irq_isr, 0,\n\t\t\t \"ndev_irq_isr\", ndev);\n\tif (rc)\n\t\tgoto err_msi_request;\n\n\tdev_dbg(&pdev->dev, \"Using msi interrupts\\n\");\n\tndev->db_vec_count = 1;\n\tndev->db_vec_shift = total_shift;\n\treturn 0;\n\nerr_msi_request:\n\tpci_disable_msi(pdev);\nerr_msi_enable:\n\n\t \n\n\tpci_intx(pdev, 1);\n\n\trc = request_irq(pdev->irq, ndev_irq_isr, IRQF_SHARED,\n\t\t\t \"ndev_irq_isr\", ndev);\n\tif (rc)\n\t\tgoto err_intx_request;\n\n\tdev_dbg(&pdev->dev, \"Using intx interrupts\\n\");\n\tndev->db_vec_count = 1;\n\tndev->db_vec_shift = total_shift;\n\treturn 0;\n\nerr_intx_request:\n\treturn rc;\n}\n\nstatic void ndev_deinit_isr(struct intel_ntb_dev *ndev)\n{\n\tstruct pci_dev *pdev;\n\tint i;\n\n\tpdev = ndev->ntb.pdev;\n\n\t \n\tndev->db_mask = ndev->db_valid_mask;\n\tndev->reg->db_iowrite(ndev->db_mask,\n\t\t\t      ndev->self_mmio +\n\t\t\t      ndev->self_reg->db_mask);\n\n\tif (ndev->msix) {\n\t\ti = ndev->db_vec_count;\n\t\twhile (i--)\n\t\t\tfree_irq(ndev->msix[i].vector, &ndev->vec[i]);\n\t\tpci_disable_msix(pdev);\n\t\tkfree(ndev->msix);\n\t\tkfree(ndev->vec);\n\t} else {\n\t\tfree_irq(pdev->irq, ndev);\n\t\tif (pci_dev_msi_enabled(pdev))\n\t\t\tpci_disable_msi(pdev);\n\t}\n}\n\nstatic ssize_t ndev_ntb_debugfs_read(struct file *filp, char __user *ubuf,\n\t\t\t\t     size_t count, loff_t *offp)\n{\n\tstruct intel_ntb_dev *ndev;\n\tstruct pci_dev *pdev;\n\tvoid __iomem *mmio;\n\tchar *buf;\n\tsize_t buf_size;\n\tssize_t ret, off;\n\tunion { u64 v64; u32 v32; u16 v16; u8 v8; } u;\n\n\tndev = filp->private_data;\n\tpdev = ndev->ntb.pdev;\n\tmmio = ndev->self_mmio;\n\n\tbuf_size = min(count, 0x800ul);\n\n\tbuf = kmalloc(buf_size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\toff = 0;\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"NTB Device Information:\\n\");\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Connection Topology -\\t%s\\n\",\n\t\t\t ntb_topo_string(ndev->ntb.topo));\n\n\tif (ndev->b2b_idx != UINT_MAX) {\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"B2B MW Idx -\\t\\t%u\\n\", ndev->b2b_idx);\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"B2B Offset -\\t\\t%#lx\\n\", ndev->b2b_off);\n\t}\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"BAR4 Split -\\t\\t%s\\n\",\n\t\t\t ndev->bar4_split ? \"yes\" : \"no\");\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"NTB CTL -\\t\\t%#06x\\n\", ndev->ntb_ctl);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"LNK STA -\\t\\t%#06x\\n\", ndev->lnk_sta);\n\n\tif (!ndev->reg->link_is_up(ndev)) {\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"Link Status -\\t\\tDown\\n\");\n\t} else {\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"Link Status -\\t\\tUp\\n\");\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"Link Speed -\\t\\tPCI-E Gen %u\\n\",\n\t\t\t\t NTB_LNK_STA_SPEED(ndev->lnk_sta));\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"Link Width -\\t\\tx%u\\n\",\n\t\t\t\t NTB_LNK_STA_WIDTH(ndev->lnk_sta));\n\t}\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Memory Window Count -\\t%u\\n\", ndev->mw_count);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Scratchpad Count -\\t%u\\n\", ndev->spad_count);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Count -\\t%u\\n\", ndev->db_count);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Vector Count -\\t%u\\n\", ndev->db_vec_count);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Vector Shift -\\t%u\\n\", ndev->db_vec_shift);\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Valid Mask -\\t%#llx\\n\", ndev->db_valid_mask);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Link Mask -\\t%#llx\\n\", ndev->db_link_mask);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Mask Cached -\\t%#llx\\n\", ndev->db_mask);\n\n\tu.v64 = ndev_db_read(ndev, mmio + ndev->self_reg->db_mask);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Mask -\\t\\t%#llx\\n\", u.v64);\n\n\tu.v64 = ndev_db_read(ndev, mmio + ndev->self_reg->db_bell);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Bell -\\t\\t%#llx\\n\", u.v64);\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"\\nNTB Window Size:\\n\");\n\n\tpci_read_config_byte(pdev, XEON_PBAR23SZ_OFFSET, &u.v8);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"PBAR23SZ %hhu\\n\", u.v8);\n\tif (!ndev->bar4_split) {\n\t\tpci_read_config_byte(pdev, XEON_PBAR45SZ_OFFSET, &u.v8);\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"PBAR45SZ %hhu\\n\", u.v8);\n\t} else {\n\t\tpci_read_config_byte(pdev, XEON_PBAR4SZ_OFFSET, &u.v8);\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"PBAR4SZ %hhu\\n\", u.v8);\n\t\tpci_read_config_byte(pdev, XEON_PBAR5SZ_OFFSET, &u.v8);\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"PBAR5SZ %hhu\\n\", u.v8);\n\t}\n\n\tpci_read_config_byte(pdev, XEON_SBAR23SZ_OFFSET, &u.v8);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"SBAR23SZ %hhu\\n\", u.v8);\n\tif (!ndev->bar4_split) {\n\t\tpci_read_config_byte(pdev, XEON_SBAR45SZ_OFFSET, &u.v8);\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"SBAR45SZ %hhu\\n\", u.v8);\n\t} else {\n\t\tpci_read_config_byte(pdev, XEON_SBAR4SZ_OFFSET, &u.v8);\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"SBAR4SZ %hhu\\n\", u.v8);\n\t\tpci_read_config_byte(pdev, XEON_SBAR5SZ_OFFSET, &u.v8);\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"SBAR5SZ %hhu\\n\", u.v8);\n\t}\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"\\nNTB Incoming XLAT:\\n\");\n\n\tu.v64 = ioread64(mmio + bar2_off(ndev->xlat_reg->bar2_xlat, 2));\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"XLAT23 -\\t\\t%#018llx\\n\", u.v64);\n\n\tif (ndev->bar4_split) {\n\t\tu.v32 = ioread32(mmio + bar2_off(ndev->xlat_reg->bar2_xlat, 4));\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"XLAT4 -\\t\\t\\t%#06x\\n\", u.v32);\n\n\t\tu.v32 = ioread32(mmio + bar2_off(ndev->xlat_reg->bar2_xlat, 5));\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"XLAT5 -\\t\\t\\t%#06x\\n\", u.v32);\n\t} else {\n\t\tu.v64 = ioread64(mmio + bar2_off(ndev->xlat_reg->bar2_xlat, 4));\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"XLAT45 -\\t\\t%#018llx\\n\", u.v64);\n\t}\n\n\tu.v64 = ioread64(mmio + bar2_off(ndev->xlat_reg->bar2_limit, 2));\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"LMT23 -\\t\\t\\t%#018llx\\n\", u.v64);\n\n\tif (ndev->bar4_split) {\n\t\tu.v32 = ioread32(mmio + bar2_off(ndev->xlat_reg->bar2_limit, 4));\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"LMT4 -\\t\\t\\t%#06x\\n\", u.v32);\n\t\tu.v32 = ioread32(mmio + bar2_off(ndev->xlat_reg->bar2_limit, 5));\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"LMT5 -\\t\\t\\t%#06x\\n\", u.v32);\n\t} else {\n\t\tu.v64 = ioread64(mmio + bar2_off(ndev->xlat_reg->bar2_limit, 4));\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"LMT45 -\\t\\t\\t%#018llx\\n\", u.v64);\n\t}\n\n\tif (pdev_is_gen1(pdev)) {\n\t\tif (ntb_topo_is_b2b(ndev->ntb.topo)) {\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"\\nNTB Outgoing B2B XLAT:\\n\");\n\n\t\t\tu.v64 = ioread64(mmio + XEON_PBAR23XLAT_OFFSET);\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"B2B XLAT23 -\\t\\t%#018llx\\n\", u.v64);\n\n\t\t\tif (ndev->bar4_split) {\n\t\t\t\tu.v32 = ioread32(mmio + XEON_PBAR4XLAT_OFFSET);\n\t\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t\t \"B2B XLAT4 -\\t\\t%#06x\\n\",\n\t\t\t\t\t\t u.v32);\n\t\t\t\tu.v32 = ioread32(mmio + XEON_PBAR5XLAT_OFFSET);\n\t\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t\t \"B2B XLAT5 -\\t\\t%#06x\\n\",\n\t\t\t\t\t\t u.v32);\n\t\t\t} else {\n\t\t\t\tu.v64 = ioread64(mmio + XEON_PBAR45XLAT_OFFSET);\n\t\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t\t \"B2B XLAT45 -\\t\\t%#018llx\\n\",\n\t\t\t\t\t\t u.v64);\n\t\t\t}\n\n\t\t\tu.v64 = ioread64(mmio + XEON_PBAR23LMT_OFFSET);\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"B2B LMT23 -\\t\\t%#018llx\\n\", u.v64);\n\n\t\t\tif (ndev->bar4_split) {\n\t\t\t\tu.v32 = ioread32(mmio + XEON_PBAR4LMT_OFFSET);\n\t\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t\t \"B2B LMT4 -\\t\\t%#06x\\n\",\n\t\t\t\t\t\t u.v32);\n\t\t\t\tu.v32 = ioread32(mmio + XEON_PBAR5LMT_OFFSET);\n\t\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t\t \"B2B LMT5 -\\t\\t%#06x\\n\",\n\t\t\t\t\t\t u.v32);\n\t\t\t} else {\n\t\t\t\tu.v64 = ioread64(mmio + XEON_PBAR45LMT_OFFSET);\n\t\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t\t \"B2B LMT45 -\\t\\t%#018llx\\n\",\n\t\t\t\t\t\t u.v64);\n\t\t\t}\n\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"\\nNTB Secondary BAR:\\n\");\n\n\t\t\tu.v64 = ioread64(mmio + XEON_SBAR0BASE_OFFSET);\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"SBAR01 -\\t\\t%#018llx\\n\", u.v64);\n\n\t\t\tu.v64 = ioread64(mmio + XEON_SBAR23BASE_OFFSET);\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"SBAR23 -\\t\\t%#018llx\\n\", u.v64);\n\n\t\t\tif (ndev->bar4_split) {\n\t\t\t\tu.v32 = ioread32(mmio + XEON_SBAR4BASE_OFFSET);\n\t\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t\t \"SBAR4 -\\t\\t\\t%#06x\\n\", u.v32);\n\t\t\t\tu.v32 = ioread32(mmio + XEON_SBAR5BASE_OFFSET);\n\t\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t\t \"SBAR5 -\\t\\t\\t%#06x\\n\", u.v32);\n\t\t\t} else {\n\t\t\t\tu.v64 = ioread64(mmio + XEON_SBAR45BASE_OFFSET);\n\t\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t\t \"SBAR45 -\\t\\t%#018llx\\n\",\n\t\t\t\t\t\t u.v64);\n\t\t\t}\n\t\t}\n\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"\\nXEON NTB Statistics:\\n\");\n\n\t\tu.v16 = ioread16(mmio + XEON_USMEMMISS_OFFSET);\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"Upstream Memory Miss -\\t%u\\n\", u.v16);\n\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"\\nXEON NTB Hardware Errors:\\n\");\n\n\t\tif (!pci_read_config_word(pdev,\n\t\t\t\t\t  XEON_DEVSTS_OFFSET, &u.v16))\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"DEVSTS -\\t\\t%#06x\\n\", u.v16);\n\n\t\tif (!pci_read_config_word(pdev,\n\t\t\t\t\t  XEON_LINK_STATUS_OFFSET, &u.v16))\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"LNKSTS -\\t\\t%#06x\\n\", u.v16);\n\n\t\tif (!pci_read_config_dword(pdev,\n\t\t\t\t\t   XEON_UNCERRSTS_OFFSET, &u.v32))\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"UNCERRSTS -\\t\\t%#06x\\n\", u.v32);\n\n\t\tif (!pci_read_config_dword(pdev,\n\t\t\t\t\t   XEON_CORERRSTS_OFFSET, &u.v32))\n\t\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t\t \"CORERRSTS -\\t\\t%#06x\\n\", u.v32);\n\t}\n\n\tret = simple_read_from_buffer(ubuf, count, offp, buf, off);\n\tkfree(buf);\n\treturn ret;\n}\n\nstatic ssize_t ndev_debugfs_read(struct file *filp, char __user *ubuf,\n\t\t\t\t size_t count, loff_t *offp)\n{\n\tstruct intel_ntb_dev *ndev = filp->private_data;\n\n\tif (pdev_is_gen1(ndev->ntb.pdev))\n\t\treturn ndev_ntb_debugfs_read(filp, ubuf, count, offp);\n\telse if (pdev_is_gen3(ndev->ntb.pdev))\n\t\treturn ndev_ntb3_debugfs_read(filp, ubuf, count, offp);\n\telse if (pdev_is_gen4(ndev->ntb.pdev) || pdev_is_gen5(ndev->ntb.pdev))\n\t\treturn ndev_ntb4_debugfs_read(filp, ubuf, count, offp);\n\n\treturn -ENXIO;\n}\n\nstatic void ndev_init_debugfs(struct intel_ntb_dev *ndev)\n{\n\tif (!debugfs_dir) {\n\t\tndev->debugfs_dir = NULL;\n\t\tndev->debugfs_info = NULL;\n\t} else {\n\t\tndev->debugfs_dir =\n\t\t\tdebugfs_create_dir(pci_name(ndev->ntb.pdev),\n\t\t\t\t\t   debugfs_dir);\n\t\tif (!ndev->debugfs_dir)\n\t\t\tndev->debugfs_info = NULL;\n\t\telse\n\t\t\tndev->debugfs_info =\n\t\t\t\tdebugfs_create_file(\"info\", S_IRUSR,\n\t\t\t\t\t\t    ndev->debugfs_dir, ndev,\n\t\t\t\t\t\t    &intel_ntb_debugfs_info);\n\t}\n}\n\nstatic void ndev_deinit_debugfs(struct intel_ntb_dev *ndev)\n{\n\tdebugfs_remove_recursive(ndev->debugfs_dir);\n}\n\nint intel_ntb_mw_count(struct ntb_dev *ntb, int pidx)\n{\n\tif (pidx != NTB_DEF_PEER_IDX)\n\t\treturn -EINVAL;\n\n\treturn ntb_ndev(ntb)->mw_count;\n}\n\nint intel_ntb_mw_get_align(struct ntb_dev *ntb, int pidx, int idx,\n\t\t\t   resource_size_t *addr_align,\n\t\t\t   resource_size_t *size_align,\n\t\t\t   resource_size_t *size_max)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\tresource_size_t bar_size, mw_size;\n\tint bar;\n\n\tif (pidx != NTB_DEF_PEER_IDX)\n\t\treturn -EINVAL;\n\n\tif (idx >= ndev->b2b_idx && !ndev->b2b_off)\n\t\tidx += 1;\n\n\tbar = ndev_mw_to_bar(ndev, idx);\n\tif (bar < 0)\n\t\treturn bar;\n\n\tbar_size = pci_resource_len(ndev->ntb.pdev, bar);\n\n\tif (idx == ndev->b2b_idx)\n\t\tmw_size = bar_size - ndev->b2b_off;\n\telse\n\t\tmw_size = bar_size;\n\n\tif (addr_align)\n\t\t*addr_align = pci_resource_len(ndev->ntb.pdev, bar);\n\n\tif (size_align)\n\t\t*size_align = 1;\n\n\tif (size_max)\n\t\t*size_max = mw_size;\n\n\treturn 0;\n}\n\nstatic int intel_ntb_mw_set_trans(struct ntb_dev *ntb, int pidx, int idx,\n\t\t\t\t  dma_addr_t addr, resource_size_t size)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\tunsigned long base_reg, xlat_reg, limit_reg;\n\tresource_size_t bar_size, mw_size;\n\tvoid __iomem *mmio;\n\tu64 base, limit, reg_val;\n\tint bar;\n\n\tif (pidx != NTB_DEF_PEER_IDX)\n\t\treturn -EINVAL;\n\n\tif (idx >= ndev->b2b_idx && !ndev->b2b_off)\n\t\tidx += 1;\n\n\tbar = ndev_mw_to_bar(ndev, idx);\n\tif (bar < 0)\n\t\treturn bar;\n\n\tbar_size = pci_resource_len(ndev->ntb.pdev, bar);\n\n\tif (idx == ndev->b2b_idx)\n\t\tmw_size = bar_size - ndev->b2b_off;\n\telse\n\t\tmw_size = bar_size;\n\n\t \n\tif (addr & (bar_size - 1))\n\t\treturn -EINVAL;\n\n\t \n\tif (size > mw_size)\n\t\treturn -EINVAL;\n\n\tmmio = ndev->self_mmio;\n\tbase_reg = bar0_off(ndev->xlat_reg->bar0_base, bar);\n\txlat_reg = bar2_off(ndev->xlat_reg->bar2_xlat, bar);\n\tlimit_reg = bar2_off(ndev->xlat_reg->bar2_limit, bar);\n\n\tif (bar < 4 || !ndev->bar4_split) {\n\t\tbase = ioread64(mmio + base_reg) & NTB_BAR_MASK_64;\n\n\t\t \n\t\tif (limit_reg && size != mw_size)\n\t\t\tlimit = base + size;\n\t\telse\n\t\t\tlimit = 0;\n\n\t\t \n\t\tiowrite64(addr, mmio + xlat_reg);\n\t\treg_val = ioread64(mmio + xlat_reg);\n\t\tif (reg_val != addr) {\n\t\t\tiowrite64(0, mmio + xlat_reg);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\t \n\t\tiowrite64(limit, mmio + limit_reg);\n\t\treg_val = ioread64(mmio + limit_reg);\n\t\tif (reg_val != limit) {\n\t\t\tiowrite64(base, mmio + limit_reg);\n\t\t\tiowrite64(0, mmio + xlat_reg);\n\t\t\treturn -EIO;\n\t\t}\n\t} else {\n\t\t \n\t\tif (addr & (~0ull << 32))\n\t\t\treturn -EINVAL;\n\t\tif ((addr + size) & (~0ull << 32))\n\t\t\treturn -EINVAL;\n\n\t\tbase = ioread32(mmio + base_reg) & NTB_BAR_MASK_32;\n\n\t\t \n\t\tif (limit_reg && size != mw_size)\n\t\t\tlimit = base + size;\n\t\telse\n\t\t\tlimit = 0;\n\n\t\t \n\t\tiowrite32(addr, mmio + xlat_reg);\n\t\treg_val = ioread32(mmio + xlat_reg);\n\t\tif (reg_val != addr) {\n\t\t\tiowrite32(0, mmio + xlat_reg);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\t \n\t\tiowrite32(limit, mmio + limit_reg);\n\t\treg_val = ioread32(mmio + limit_reg);\n\t\tif (reg_val != limit) {\n\t\t\tiowrite32(base, mmio + limit_reg);\n\t\t\tiowrite32(0, mmio + xlat_reg);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nu64 intel_ntb_link_is_up(struct ntb_dev *ntb, enum ntb_speed *speed,\n\t\t\t enum ntb_width *width)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\tif (ndev->reg->link_is_up(ndev)) {\n\t\tif (speed)\n\t\t\t*speed = NTB_LNK_STA_SPEED(ndev->lnk_sta);\n\t\tif (width)\n\t\t\t*width = NTB_LNK_STA_WIDTH(ndev->lnk_sta);\n\t\treturn 1;\n\t} else {\n\t\t \n\t\tif (speed)\n\t\t\t*speed = NTB_SPEED_NONE;\n\t\tif (width)\n\t\t\t*width = NTB_WIDTH_NONE;\n\t\treturn 0;\n\t}\n}\n\nstatic int intel_ntb_link_enable(struct ntb_dev *ntb,\n\t\t\t\t enum ntb_speed max_speed,\n\t\t\t\t enum ntb_width max_width)\n{\n\tstruct intel_ntb_dev *ndev;\n\tu32 ntb_ctl;\n\n\tndev = container_of(ntb, struct intel_ntb_dev, ntb);\n\n\tif (ndev->ntb.topo == NTB_TOPO_SEC)\n\t\treturn -EINVAL;\n\n\tdev_dbg(&ntb->pdev->dev,\n\t\t\"Enabling link with max_speed %d max_width %d\\n\",\n\t\tmax_speed, max_width);\n\tif (max_speed != NTB_SPEED_AUTO)\n\t\tdev_dbg(&ntb->pdev->dev, \"ignoring max_speed %d\\n\", max_speed);\n\tif (max_width != NTB_WIDTH_AUTO)\n\t\tdev_dbg(&ntb->pdev->dev, \"ignoring max_width %d\\n\", max_width);\n\n\tntb_ctl = ioread32(ndev->self_mmio + ndev->reg->ntb_ctl);\n\tntb_ctl &= ~(NTB_CTL_DISABLE | NTB_CTL_CFG_LOCK);\n\tntb_ctl |= NTB_CTL_P2S_BAR2_SNOOP | NTB_CTL_S2P_BAR2_SNOOP;\n\tntb_ctl |= NTB_CTL_P2S_BAR4_SNOOP | NTB_CTL_S2P_BAR4_SNOOP;\n\tif (ndev->bar4_split)\n\t\tntb_ctl |= NTB_CTL_P2S_BAR5_SNOOP | NTB_CTL_S2P_BAR5_SNOOP;\n\tiowrite32(ntb_ctl, ndev->self_mmio + ndev->reg->ntb_ctl);\n\n\treturn 0;\n}\n\nint intel_ntb_link_disable(struct ntb_dev *ntb)\n{\n\tstruct intel_ntb_dev *ndev;\n\tu32 ntb_cntl;\n\n\tndev = container_of(ntb, struct intel_ntb_dev, ntb);\n\n\tif (ndev->ntb.topo == NTB_TOPO_SEC)\n\t\treturn -EINVAL;\n\n\tdev_dbg(&ntb->pdev->dev, \"Disabling link\\n\");\n\n\t \n\tntb_cntl = ioread32(ndev->self_mmio + ndev->reg->ntb_ctl);\n\tntb_cntl &= ~(NTB_CTL_P2S_BAR2_SNOOP | NTB_CTL_S2P_BAR2_SNOOP);\n\tntb_cntl &= ~(NTB_CTL_P2S_BAR4_SNOOP | NTB_CTL_S2P_BAR4_SNOOP);\n\tif (ndev->bar4_split)\n\t\tntb_cntl &= ~(NTB_CTL_P2S_BAR5_SNOOP | NTB_CTL_S2P_BAR5_SNOOP);\n\tntb_cntl |= NTB_CTL_DISABLE | NTB_CTL_CFG_LOCK;\n\tiowrite32(ntb_cntl, ndev->self_mmio + ndev->reg->ntb_ctl);\n\n\treturn 0;\n}\n\nint intel_ntb_peer_mw_count(struct ntb_dev *ntb)\n{\n\t \n\treturn ntb_ndev(ntb)->mw_count;\n}\n\nint intel_ntb_peer_mw_get_addr(struct ntb_dev *ntb, int idx,\n\t\t\t       phys_addr_t *base, resource_size_t *size)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\tint bar;\n\n\tif (idx >= ndev->b2b_idx && !ndev->b2b_off)\n\t\tidx += 1;\n\n\tbar = ndev_mw_to_bar(ndev, idx);\n\tif (bar < 0)\n\t\treturn bar;\n\n\tif (base)\n\t\t*base = pci_resource_start(ndev->ntb.pdev, bar) +\n\t\t\t(idx == ndev->b2b_idx ? ndev->b2b_off : 0);\n\n\tif (size)\n\t\t*size = pci_resource_len(ndev->ntb.pdev, bar) -\n\t\t\t(idx == ndev->b2b_idx ? ndev->b2b_off : 0);\n\n\treturn 0;\n}\n\nstatic int intel_ntb_db_is_unsafe(struct ntb_dev *ntb)\n{\n\treturn ndev_ignore_unsafe(ntb_ndev(ntb), NTB_UNSAFE_DB);\n}\n\nu64 intel_ntb_db_valid_mask(struct ntb_dev *ntb)\n{\n\treturn ntb_ndev(ntb)->db_valid_mask;\n}\n\nint intel_ntb_db_vector_count(struct ntb_dev *ntb)\n{\n\tstruct intel_ntb_dev *ndev;\n\n\tndev = container_of(ntb, struct intel_ntb_dev, ntb);\n\n\treturn ndev->db_vec_count;\n}\n\nu64 intel_ntb_db_vector_mask(struct ntb_dev *ntb, int db_vector)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\tif (db_vector < 0 || db_vector > ndev->db_vec_count)\n\t\treturn 0;\n\n\treturn ndev->db_valid_mask & ndev_vec_mask(ndev, db_vector);\n}\n\nstatic u64 intel_ntb_db_read(struct ntb_dev *ntb)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_db_read(ndev,\n\t\t\t    ndev->self_mmio +\n\t\t\t    ndev->self_reg->db_bell);\n}\n\nstatic int intel_ntb_db_clear(struct ntb_dev *ntb, u64 db_bits)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_db_write(ndev, db_bits,\n\t\t\t     ndev->self_mmio +\n\t\t\t     ndev->self_reg->db_bell);\n}\n\nint intel_ntb_db_set_mask(struct ntb_dev *ntb, u64 db_bits)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_db_set_mask(ndev, db_bits,\n\t\t\t\tndev->self_mmio +\n\t\t\t\tndev->self_reg->db_mask);\n}\n\nint intel_ntb_db_clear_mask(struct ntb_dev *ntb, u64 db_bits)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_db_clear_mask(ndev, db_bits,\n\t\t\t\t  ndev->self_mmio +\n\t\t\t\t  ndev->self_reg->db_mask);\n}\n\nstatic int intel_ntb_peer_db_addr(struct ntb_dev *ntb, phys_addr_t *db_addr,\n\t\t\t   resource_size_t *db_size, u64 *db_data, int db_bit)\n{\n\tu64 db_bits;\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\tif (unlikely(db_bit >= BITS_PER_LONG_LONG))\n\t\treturn -EINVAL;\n\n\tdb_bits = BIT_ULL(db_bit);\n\n\tif (unlikely(db_bits & ~ntb_ndev(ntb)->db_valid_mask))\n\t\treturn -EINVAL;\n\n\tndev_db_addr(ndev, db_addr, db_size, ndev->peer_addr,\n\t\t\t    ndev->peer_reg->db_bell);\n\n\tif (db_data)\n\t\t*db_data = db_bits;\n\n\n\treturn 0;\n}\n\nstatic int intel_ntb_peer_db_set(struct ntb_dev *ntb, u64 db_bits)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_db_write(ndev, db_bits,\n\t\t\t     ndev->peer_mmio +\n\t\t\t     ndev->peer_reg->db_bell);\n}\n\nint intel_ntb_spad_is_unsafe(struct ntb_dev *ntb)\n{\n\treturn ndev_ignore_unsafe(ntb_ndev(ntb), NTB_UNSAFE_SPAD);\n}\n\nint intel_ntb_spad_count(struct ntb_dev *ntb)\n{\n\tstruct intel_ntb_dev *ndev;\n\n\tndev = container_of(ntb, struct intel_ntb_dev, ntb);\n\n\treturn ndev->spad_count;\n}\n\nu32 intel_ntb_spad_read(struct ntb_dev *ntb, int idx)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_spad_read(ndev, idx,\n\t\t\t      ndev->self_mmio +\n\t\t\t      ndev->self_reg->spad);\n}\n\nint intel_ntb_spad_write(struct ntb_dev *ntb, int idx, u32 val)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_spad_write(ndev, idx, val,\n\t\t\t       ndev->self_mmio +\n\t\t\t       ndev->self_reg->spad);\n}\n\nint intel_ntb_peer_spad_addr(struct ntb_dev *ntb, int pidx, int sidx,\n\t\t\t     phys_addr_t *spad_addr)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_spad_addr(ndev, sidx, spad_addr, ndev->peer_addr,\n\t\t\t      ndev->peer_reg->spad);\n}\n\nu32 intel_ntb_peer_spad_read(struct ntb_dev *ntb, int pidx, int sidx)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_spad_read(ndev, sidx,\n\t\t\t      ndev->peer_mmio +\n\t\t\t      ndev->peer_reg->spad);\n}\n\nint intel_ntb_peer_spad_write(struct ntb_dev *ntb, int pidx, int sidx,\n\t\t\t      u32 val)\n{\n\tstruct intel_ntb_dev *ndev = ntb_ndev(ntb);\n\n\treturn ndev_spad_write(ndev, sidx, val,\n\t\t\t       ndev->peer_mmio +\n\t\t\t       ndev->peer_reg->spad);\n}\n\nstatic u64 xeon_db_ioread(const void __iomem *mmio)\n{\n\treturn (u64)ioread16(mmio);\n}\n\nstatic void xeon_db_iowrite(u64 bits, void __iomem *mmio)\n{\n\tiowrite16((u16)bits, mmio);\n}\n\nstatic int xeon_poll_link(struct intel_ntb_dev *ndev)\n{\n\tu16 reg_val;\n\tint rc;\n\n\tndev->reg->db_iowrite(ndev->db_link_mask,\n\t\t\t      ndev->self_mmio +\n\t\t\t      ndev->self_reg->db_bell);\n\n\trc = pci_read_config_word(ndev->ntb.pdev,\n\t\t\t\t  XEON_LINK_STATUS_OFFSET, &reg_val);\n\tif (rc)\n\t\treturn 0;\n\n\tif (reg_val == ndev->lnk_sta)\n\t\treturn 0;\n\n\tndev->lnk_sta = reg_val;\n\n\treturn 1;\n}\n\nint xeon_link_is_up(struct intel_ntb_dev *ndev)\n{\n\tif (ndev->ntb.topo == NTB_TOPO_SEC)\n\t\treturn 1;\n\n\treturn NTB_LNK_STA_ACTIVE(ndev->lnk_sta);\n}\n\nenum ntb_topo xeon_ppd_topo(struct intel_ntb_dev *ndev, u8 ppd)\n{\n\tswitch (ppd & XEON_PPD_TOPO_MASK) {\n\tcase XEON_PPD_TOPO_B2B_USD:\n\t\treturn NTB_TOPO_B2B_USD;\n\n\tcase XEON_PPD_TOPO_B2B_DSD:\n\t\treturn NTB_TOPO_B2B_DSD;\n\n\tcase XEON_PPD_TOPO_PRI_USD:\n\tcase XEON_PPD_TOPO_PRI_DSD:  \n\t\treturn NTB_TOPO_PRI;\n\n\tcase XEON_PPD_TOPO_SEC_USD:\n\tcase XEON_PPD_TOPO_SEC_DSD:  \n\t\treturn NTB_TOPO_SEC;\n\t}\n\n\treturn NTB_TOPO_NONE;\n}\n\nstatic inline int xeon_ppd_bar4_split(struct intel_ntb_dev *ndev, u8 ppd)\n{\n\tif (ppd & XEON_PPD_SPLIT_BAR_MASK) {\n\t\tdev_dbg(&ndev->ntb.pdev->dev, \"PPD %d split bar\\n\", ppd);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int xeon_init_isr(struct intel_ntb_dev *ndev)\n{\n\treturn ndev_init_isr(ndev, XEON_DB_MSIX_VECTOR_COUNT,\n\t\t\t     XEON_DB_MSIX_VECTOR_COUNT,\n\t\t\t     XEON_DB_MSIX_VECTOR_SHIFT,\n\t\t\t     XEON_DB_TOTAL_SHIFT);\n}\n\nstatic void xeon_deinit_isr(struct intel_ntb_dev *ndev)\n{\n\tndev_deinit_isr(ndev);\n}\n\nstatic int xeon_setup_b2b_mw(struct intel_ntb_dev *ndev,\n\t\t\t     const struct intel_b2b_addr *addr,\n\t\t\t     const struct intel_b2b_addr *peer_addr)\n{\n\tstruct pci_dev *pdev;\n\tvoid __iomem *mmio;\n\tresource_size_t bar_size;\n\tphys_addr_t bar_addr;\n\tint b2b_bar;\n\tu8 bar_sz;\n\n\tpdev = ndev->ntb.pdev;\n\tmmio = ndev->self_mmio;\n\n\tif (ndev->b2b_idx == UINT_MAX) {\n\t\tdev_dbg(&pdev->dev, \"not using b2b mw\\n\");\n\t\tb2b_bar = 0;\n\t\tndev->b2b_off = 0;\n\t} else {\n\t\tb2b_bar = ndev_mw_to_bar(ndev, ndev->b2b_idx);\n\t\tif (b2b_bar < 0)\n\t\t\treturn -EIO;\n\n\t\tdev_dbg(&pdev->dev, \"using b2b mw bar %d\\n\", b2b_bar);\n\n\t\tbar_size = pci_resource_len(ndev->ntb.pdev, b2b_bar);\n\n\t\tdev_dbg(&pdev->dev, \"b2b bar size %#llx\\n\", bar_size);\n\n\t\tif (b2b_mw_share && XEON_B2B_MIN_SIZE <= bar_size >> 1) {\n\t\t\tdev_dbg(&pdev->dev, \"b2b using first half of bar\\n\");\n\t\t\tndev->b2b_off = bar_size >> 1;\n\t\t} else if (XEON_B2B_MIN_SIZE <= bar_size) {\n\t\t\tdev_dbg(&pdev->dev, \"b2b using whole bar\\n\");\n\t\t\tndev->b2b_off = 0;\n\t\t\t--ndev->mw_count;\n\t\t} else {\n\t\t\tdev_dbg(&pdev->dev, \"b2b bar size is too small\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t \n\tpci_read_config_byte(pdev, XEON_PBAR23SZ_OFFSET, &bar_sz);\n\tdev_dbg(&pdev->dev, \"PBAR23SZ %#x\\n\", bar_sz);\n\tif (b2b_bar == 2) {\n\t\tif (ndev->b2b_off)\n\t\t\tbar_sz -= 1;\n\t\telse\n\t\t\tbar_sz = 0;\n\t}\n\tpci_write_config_byte(pdev, XEON_SBAR23SZ_OFFSET, bar_sz);\n\tpci_read_config_byte(pdev, XEON_SBAR23SZ_OFFSET, &bar_sz);\n\tdev_dbg(&pdev->dev, \"SBAR23SZ %#x\\n\", bar_sz);\n\n\tif (!ndev->bar4_split) {\n\t\tpci_read_config_byte(pdev, XEON_PBAR45SZ_OFFSET, &bar_sz);\n\t\tdev_dbg(&pdev->dev, \"PBAR45SZ %#x\\n\", bar_sz);\n\t\tif (b2b_bar == 4) {\n\t\t\tif (ndev->b2b_off)\n\t\t\t\tbar_sz -= 1;\n\t\t\telse\n\t\t\t\tbar_sz = 0;\n\t\t}\n\t\tpci_write_config_byte(pdev, XEON_SBAR45SZ_OFFSET, bar_sz);\n\t\tpci_read_config_byte(pdev, XEON_SBAR45SZ_OFFSET, &bar_sz);\n\t\tdev_dbg(&pdev->dev, \"SBAR45SZ %#x\\n\", bar_sz);\n\t} else {\n\t\tpci_read_config_byte(pdev, XEON_PBAR4SZ_OFFSET, &bar_sz);\n\t\tdev_dbg(&pdev->dev, \"PBAR4SZ %#x\\n\", bar_sz);\n\t\tif (b2b_bar == 4) {\n\t\t\tif (ndev->b2b_off)\n\t\t\t\tbar_sz -= 1;\n\t\t\telse\n\t\t\t\tbar_sz = 0;\n\t\t}\n\t\tpci_write_config_byte(pdev, XEON_SBAR4SZ_OFFSET, bar_sz);\n\t\tpci_read_config_byte(pdev, XEON_SBAR4SZ_OFFSET, &bar_sz);\n\t\tdev_dbg(&pdev->dev, \"SBAR4SZ %#x\\n\", bar_sz);\n\n\t\tpci_read_config_byte(pdev, XEON_PBAR5SZ_OFFSET, &bar_sz);\n\t\tdev_dbg(&pdev->dev, \"PBAR5SZ %#x\\n\", bar_sz);\n\t\tif (b2b_bar == 5) {\n\t\t\tif (ndev->b2b_off)\n\t\t\t\tbar_sz -= 1;\n\t\t\telse\n\t\t\t\tbar_sz = 0;\n\t\t}\n\t\tpci_write_config_byte(pdev, XEON_SBAR5SZ_OFFSET, bar_sz);\n\t\tpci_read_config_byte(pdev, XEON_SBAR5SZ_OFFSET, &bar_sz);\n\t\tdev_dbg(&pdev->dev, \"SBAR5SZ %#x\\n\", bar_sz);\n\t}\n\n\t \n\tif (b2b_bar == 0)\n\t\tbar_addr = addr->bar0_addr;\n\telse if (b2b_bar == 2)\n\t\tbar_addr = addr->bar2_addr64;\n\telse if (b2b_bar == 4 && !ndev->bar4_split)\n\t\tbar_addr = addr->bar4_addr64;\n\telse if (b2b_bar == 4)\n\t\tbar_addr = addr->bar4_addr32;\n\telse if (b2b_bar == 5)\n\t\tbar_addr = addr->bar5_addr32;\n\telse\n\t\treturn -EIO;\n\n\tdev_dbg(&pdev->dev, \"SBAR01 %#018llx\\n\", bar_addr);\n\tiowrite64(bar_addr, mmio + XEON_SBAR0BASE_OFFSET);\n\n\t \n\n\tbar_addr = addr->bar2_addr64 + (b2b_bar == 2 ? ndev->b2b_off : 0);\n\tiowrite64(bar_addr, mmio + XEON_SBAR23BASE_OFFSET);\n\tbar_addr = ioread64(mmio + XEON_SBAR23BASE_OFFSET);\n\tdev_dbg(&pdev->dev, \"SBAR23 %#018llx\\n\", bar_addr);\n\n\tif (!ndev->bar4_split) {\n\t\tbar_addr = addr->bar4_addr64 +\n\t\t\t(b2b_bar == 4 ? ndev->b2b_off : 0);\n\t\tiowrite64(bar_addr, mmio + XEON_SBAR45BASE_OFFSET);\n\t\tbar_addr = ioread64(mmio + XEON_SBAR45BASE_OFFSET);\n\t\tdev_dbg(&pdev->dev, \"SBAR45 %#018llx\\n\", bar_addr);\n\t} else {\n\t\tbar_addr = addr->bar4_addr32 +\n\t\t\t(b2b_bar == 4 ? ndev->b2b_off : 0);\n\t\tiowrite32(bar_addr, mmio + XEON_SBAR4BASE_OFFSET);\n\t\tbar_addr = ioread32(mmio + XEON_SBAR4BASE_OFFSET);\n\t\tdev_dbg(&pdev->dev, \"SBAR4 %#010llx\\n\", bar_addr);\n\n\t\tbar_addr = addr->bar5_addr32 +\n\t\t\t(b2b_bar == 5 ? ndev->b2b_off : 0);\n\t\tiowrite32(bar_addr, mmio + XEON_SBAR5BASE_OFFSET);\n\t\tbar_addr = ioread32(mmio + XEON_SBAR5BASE_OFFSET);\n\t\tdev_dbg(&pdev->dev, \"SBAR5 %#010llx\\n\", bar_addr);\n\t}\n\n\t \n\n\tbar_addr = addr->bar2_addr64 + (b2b_bar == 2 ? ndev->b2b_off : 0);\n\tiowrite64(bar_addr, mmio + XEON_SBAR23LMT_OFFSET);\n\tbar_addr = ioread64(mmio + XEON_SBAR23LMT_OFFSET);\n\tdev_dbg(&pdev->dev, \"SBAR23LMT %#018llx\\n\", bar_addr);\n\n\tif (!ndev->bar4_split) {\n\t\tbar_addr = addr->bar4_addr64 +\n\t\t\t(b2b_bar == 4 ? ndev->b2b_off : 0);\n\t\tiowrite64(bar_addr, mmio + XEON_SBAR45LMT_OFFSET);\n\t\tbar_addr = ioread64(mmio + XEON_SBAR45LMT_OFFSET);\n\t\tdev_dbg(&pdev->dev, \"SBAR45LMT %#018llx\\n\", bar_addr);\n\t} else {\n\t\tbar_addr = addr->bar4_addr32 +\n\t\t\t(b2b_bar == 4 ? ndev->b2b_off : 0);\n\t\tiowrite32(bar_addr, mmio + XEON_SBAR4LMT_OFFSET);\n\t\tbar_addr = ioread32(mmio + XEON_SBAR4LMT_OFFSET);\n\t\tdev_dbg(&pdev->dev, \"SBAR4LMT %#010llx\\n\", bar_addr);\n\n\t\tbar_addr = addr->bar5_addr32 +\n\t\t\t(b2b_bar == 5 ? ndev->b2b_off : 0);\n\t\tiowrite32(bar_addr, mmio + XEON_SBAR5LMT_OFFSET);\n\t\tbar_addr = ioread32(mmio + XEON_SBAR5LMT_OFFSET);\n\t\tdev_dbg(&pdev->dev, \"SBAR5LMT %#05llx\\n\", bar_addr);\n\t}\n\n\t \n\tiowrite64(0, mmio + XEON_SBAR23XLAT_OFFSET);\n\n\tif (!ndev->bar4_split) {\n\t\tiowrite64(0, mmio + XEON_SBAR45XLAT_OFFSET);\n\t} else {\n\t\tiowrite32(0, mmio + XEON_SBAR4XLAT_OFFSET);\n\t\tiowrite32(0, mmio + XEON_SBAR5XLAT_OFFSET);\n\t}\n\n\t \n\tiowrite64(0, mmio + XEON_PBAR23LMT_OFFSET);\n\tif (!ndev->bar4_split) {\n\t\tiowrite64(0, mmio + XEON_PBAR45LMT_OFFSET);\n\t} else {\n\t\tiowrite32(0, mmio + XEON_PBAR4LMT_OFFSET);\n\t\tiowrite32(0, mmio + XEON_PBAR5LMT_OFFSET);\n\t}\n\n\t \n\tbar_addr = peer_addr->bar2_addr64;\n\tiowrite64(bar_addr, mmio + XEON_PBAR23XLAT_OFFSET);\n\tbar_addr = ioread64(mmio + XEON_PBAR23XLAT_OFFSET);\n\tdev_dbg(&pdev->dev, \"PBAR23XLAT %#018llx\\n\", bar_addr);\n\n\tif (!ndev->bar4_split) {\n\t\tbar_addr = peer_addr->bar4_addr64;\n\t\tiowrite64(bar_addr, mmio + XEON_PBAR45XLAT_OFFSET);\n\t\tbar_addr = ioread64(mmio + XEON_PBAR45XLAT_OFFSET);\n\t\tdev_dbg(&pdev->dev, \"PBAR45XLAT %#018llx\\n\", bar_addr);\n\t} else {\n\t\tbar_addr = peer_addr->bar4_addr32;\n\t\tiowrite32(bar_addr, mmio + XEON_PBAR4XLAT_OFFSET);\n\t\tbar_addr = ioread32(mmio + XEON_PBAR4XLAT_OFFSET);\n\t\tdev_dbg(&pdev->dev, \"PBAR4XLAT %#010llx\\n\", bar_addr);\n\n\t\tbar_addr = peer_addr->bar5_addr32;\n\t\tiowrite32(bar_addr, mmio + XEON_PBAR5XLAT_OFFSET);\n\t\tbar_addr = ioread32(mmio + XEON_PBAR5XLAT_OFFSET);\n\t\tdev_dbg(&pdev->dev, \"PBAR5XLAT %#010llx\\n\", bar_addr);\n\t}\n\n\t \n\tif (b2b_bar == 0)\n\t\tbar_addr = peer_addr->bar0_addr;\n\telse if (b2b_bar == 2)\n\t\tbar_addr = peer_addr->bar2_addr64;\n\telse if (b2b_bar == 4 && !ndev->bar4_split)\n\t\tbar_addr = peer_addr->bar4_addr64;\n\telse if (b2b_bar == 4)\n\t\tbar_addr = peer_addr->bar4_addr32;\n\telse if (b2b_bar == 5)\n\t\tbar_addr = peer_addr->bar5_addr32;\n\telse\n\t\treturn -EIO;\n\n\t \n\tdev_dbg(&pdev->dev, \"B2BXLAT %#018llx\\n\", bar_addr);\n\tiowrite32(bar_addr, mmio + XEON_B2B_XLAT_OFFSETL);\n\tiowrite32(bar_addr >> 32, mmio + XEON_B2B_XLAT_OFFSETU);\n\n\tif (b2b_bar) {\n\t\t \n\t\tndev->peer_mmio = pci_iomap(pdev, b2b_bar,\n\t\t\t\t\t    XEON_B2B_MIN_SIZE);\n\t\tif (!ndev->peer_mmio)\n\t\t\treturn -EIO;\n\n\t\tndev->peer_addr = pci_resource_start(pdev, b2b_bar);\n\t}\n\n\treturn 0;\n}\n\nstatic int xeon_init_ntb(struct intel_ntb_dev *ndev)\n{\n\tstruct device *dev = &ndev->ntb.pdev->dev;\n\tint rc;\n\tu32 ntb_ctl;\n\n\tif (ndev->bar4_split)\n\t\tndev->mw_count = HSX_SPLIT_BAR_MW_COUNT;\n\telse\n\t\tndev->mw_count = XEON_MW_COUNT;\n\n\tndev->spad_count = XEON_SPAD_COUNT;\n\tndev->db_count = XEON_DB_COUNT;\n\tndev->db_link_mask = XEON_DB_LINK_BIT;\n\n\tswitch (ndev->ntb.topo) {\n\tcase NTB_TOPO_PRI:\n\t\tif (ndev->hwerr_flags & NTB_HWERR_SDOORBELL_LOCKUP) {\n\t\t\tdev_err(dev, \"NTB Primary config disabled\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tntb_ctl = ioread32(ndev->self_mmio + ndev->reg->ntb_ctl);\n\t\tntb_ctl &= ~NTB_CTL_DISABLE;\n\t\tiowrite32(ntb_ctl, ndev->self_mmio + ndev->reg->ntb_ctl);\n\n\t\t \n\t\tndev->spad_count >>= 1;\n\t\tndev->self_reg = &xeon_pri_reg;\n\t\tndev->peer_reg = &xeon_sec_reg;\n\t\tndev->xlat_reg = &xeon_sec_xlat;\n\t\tbreak;\n\n\tcase NTB_TOPO_SEC:\n\t\tif (ndev->hwerr_flags & NTB_HWERR_SDOORBELL_LOCKUP) {\n\t\t\tdev_err(dev, \"NTB Secondary config disabled\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t \n\t\tndev->spad_count >>= 1;\n\t\tndev->self_reg = &xeon_sec_reg;\n\t\tndev->peer_reg = &xeon_pri_reg;\n\t\tndev->xlat_reg = &xeon_pri_xlat;\n\t\tbreak;\n\n\tcase NTB_TOPO_B2B_USD:\n\tcase NTB_TOPO_B2B_DSD:\n\t\tndev->self_reg = &xeon_pri_reg;\n\t\tndev->peer_reg = &xeon_b2b_reg;\n\t\tndev->xlat_reg = &xeon_sec_xlat;\n\n\t\tif (ndev->hwerr_flags & NTB_HWERR_SDOORBELL_LOCKUP) {\n\t\t\tndev->peer_reg = &xeon_pri_reg;\n\n\t\t\tif (b2b_mw_idx < 0)\n\t\t\t\tndev->b2b_idx = b2b_mw_idx + ndev->mw_count;\n\t\t\telse\n\t\t\t\tndev->b2b_idx = b2b_mw_idx;\n\n\t\t\tif (ndev->b2b_idx >= ndev->mw_count) {\n\t\t\t\tdev_dbg(dev,\n\t\t\t\t\t\"b2b_mw_idx %d invalid for mw_count %u\\n\",\n\t\t\t\t\tb2b_mw_idx, ndev->mw_count);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tdev_dbg(dev, \"setting up b2b mw idx %d means %d\\n\",\n\t\t\t\tb2b_mw_idx, ndev->b2b_idx);\n\n\t\t} else if (ndev->hwerr_flags & NTB_HWERR_B2BDOORBELL_BIT14) {\n\t\t\tdev_warn(dev, \"Reduce doorbell count by 1\\n\");\n\t\t\tndev->db_count -= 1;\n\t\t}\n\n\t\tif (ndev->ntb.topo == NTB_TOPO_B2B_USD) {\n\t\t\trc = xeon_setup_b2b_mw(ndev,\n\t\t\t\t\t       &xeon_b2b_dsd_addr,\n\t\t\t\t\t       &xeon_b2b_usd_addr);\n\t\t} else {\n\t\t\trc = xeon_setup_b2b_mw(ndev,\n\t\t\t\t\t       &xeon_b2b_usd_addr,\n\t\t\t\t\t       &xeon_b2b_dsd_addr);\n\t\t}\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\t \n\t\tiowrite16(PCI_COMMAND_MEMORY | PCI_COMMAND_MASTER,\n\t\t\t  ndev->self_mmio + XEON_SPCICMD_OFFSET);\n\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tndev->db_valid_mask = BIT_ULL(ndev->db_count) - 1;\n\n\tndev->reg->db_iowrite(ndev->db_valid_mask,\n\t\t\t      ndev->self_mmio +\n\t\t\t      ndev->self_reg->db_mask);\n\n\treturn 0;\n}\n\nstatic int xeon_init_dev(struct intel_ntb_dev *ndev)\n{\n\tstruct pci_dev *pdev;\n\tu8 ppd;\n\tint rc, mem;\n\n\tpdev = ndev->ntb.pdev;\n\n\tswitch (pdev->device) {\n\t \n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_JSF:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_JSF:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_JSF:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_SNB:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_SNB:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_SNB:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_IVT:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_IVT:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_IVT:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_HSX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_HSX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_HSX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_BDX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_BDX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_BDX:\n\t\tndev->hwerr_flags |= NTB_HWERR_SDOORBELL_LOCKUP;\n\t\tbreak;\n\t}\n\n\tswitch (pdev->device) {\n\t \n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_IVT:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_IVT:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_IVT:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_HSX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_HSX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_HSX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_BDX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_BDX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_BDX:\n\t\tndev->hwerr_flags |= NTB_HWERR_SB01BASE_LOCKUP;\n\t\tbreak;\n\t}\n\n\tswitch (pdev->device) {\n\t \n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_JSF:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_JSF:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_JSF:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_SNB:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_SNB:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_SNB:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_IVT:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_IVT:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_IVT:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_HSX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_HSX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_HSX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_SS_BDX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_PS_BDX:\n\tcase PCI_DEVICE_ID_INTEL_NTB_B2B_BDX:\n\t\tndev->hwerr_flags |= NTB_HWERR_B2BDOORBELL_BIT14;\n\t\tbreak;\n\t}\n\n\tndev->reg = &xeon_reg;\n\n\trc = pci_read_config_byte(pdev, XEON_PPD_OFFSET, &ppd);\n\tif (rc)\n\t\treturn -EIO;\n\n\tndev->ntb.topo = xeon_ppd_topo(ndev, ppd);\n\tdev_dbg(&pdev->dev, \"ppd %#x topo %s\\n\", ppd,\n\t\tntb_topo_string(ndev->ntb.topo));\n\tif (ndev->ntb.topo == NTB_TOPO_NONE)\n\t\treturn -EINVAL;\n\n\tif (ndev->ntb.topo != NTB_TOPO_SEC) {\n\t\tndev->bar4_split = xeon_ppd_bar4_split(ndev, ppd);\n\t\tdev_dbg(&pdev->dev, \"ppd %#x bar4_split %d\\n\",\n\t\t\tppd, ndev->bar4_split);\n\t} else {\n\t\t \n\t\tmem = pci_select_bars(pdev, IORESOURCE_MEM);\n\t\tndev->bar4_split = hweight32(mem) ==\n\t\t\tHSX_SPLIT_BAR_MW_COUNT + 1;\n\t\tdev_dbg(&pdev->dev, \"mem %#x bar4_split %d\\n\",\n\t\t\tmem, ndev->bar4_split);\n\t}\n\n\trc = xeon_init_ntb(ndev);\n\tif (rc)\n\t\treturn rc;\n\n\treturn xeon_init_isr(ndev);\n}\n\nstatic void xeon_deinit_dev(struct intel_ntb_dev *ndev)\n{\n\txeon_deinit_isr(ndev);\n}\n\nstatic int intel_ntb_init_pci(struct intel_ntb_dev *ndev, struct pci_dev *pdev)\n{\n\tint rc;\n\n\tpci_set_drvdata(pdev, ndev);\n\n\trc = pci_enable_device(pdev);\n\tif (rc)\n\t\tgoto err_pci_enable;\n\n\trc = pci_request_regions(pdev, NTB_NAME);\n\tif (rc)\n\t\tgoto err_pci_regions;\n\n\tpci_set_master(pdev);\n\n\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (rc) {\n\t\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (rc)\n\t\t\tgoto err_dma_mask;\n\t\tdev_warn(&pdev->dev, \"Cannot DMA highmem\\n\");\n\t}\n\n\tndev->self_mmio = pci_iomap(pdev, 0, 0);\n\tif (!ndev->self_mmio) {\n\t\trc = -EIO;\n\t\tgoto err_mmio;\n\t}\n\tndev->peer_mmio = ndev->self_mmio;\n\tndev->peer_addr = pci_resource_start(pdev, 0);\n\n\treturn 0;\n\nerr_mmio:\nerr_dma_mask:\n\tpci_release_regions(pdev);\nerr_pci_regions:\n\tpci_disable_device(pdev);\nerr_pci_enable:\n\tpci_set_drvdata(pdev, NULL);\n\treturn rc;\n}\n\nstatic void intel_ntb_deinit_pci(struct intel_ntb_dev *ndev)\n{\n\tstruct pci_dev *pdev = ndev->ntb.pdev;\n\n\tif (ndev->peer_mmio && ndev->peer_mmio != ndev->self_mmio)\n\t\tpci_iounmap(pdev, ndev->peer_mmio);\n\tpci_iounmap(pdev, ndev->self_mmio);\n\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tpci_set_drvdata(pdev, NULL);\n}\n\nstatic inline void ndev_init_struct(struct intel_ntb_dev *ndev,\n\t\t\t\t    struct pci_dev *pdev)\n{\n\tndev->ntb.pdev = pdev;\n\tndev->ntb.topo = NTB_TOPO_NONE;\n\tndev->ntb.ops = &intel_ntb_ops;\n\n\tndev->b2b_off = 0;\n\tndev->b2b_idx = UINT_MAX;\n\n\tndev->bar4_split = 0;\n\n\tndev->mw_count = 0;\n\tndev->spad_count = 0;\n\tndev->db_count = 0;\n\tndev->db_vec_count = 0;\n\tndev->db_vec_shift = 0;\n\n\tndev->ntb_ctl = 0;\n\tndev->lnk_sta = 0;\n\n\tndev->db_valid_mask = 0;\n\tndev->db_link_mask = 0;\n\tndev->db_mask = 0;\n\n\tspin_lock_init(&ndev->db_mask_lock);\n}\n\nstatic int intel_ntb_pci_probe(struct pci_dev *pdev,\n\t\t\t       const struct pci_device_id *id)\n{\n\tstruct intel_ntb_dev *ndev;\n\tint rc, node;\n\n\tnode = dev_to_node(&pdev->dev);\n\tndev = kzalloc_node(sizeof(*ndev), GFP_KERNEL, node);\n\tif (!ndev) {\n\t\trc = -ENOMEM;\n\t\tgoto err_ndev;\n\t}\n\n\tndev_init_struct(ndev, pdev);\n\n\tif (pdev_is_gen1(pdev)) {\n\t\trc = intel_ntb_init_pci(ndev, pdev);\n\t\tif (rc)\n\t\t\tgoto err_init_pci;\n\n\t\trc = xeon_init_dev(ndev);\n\t\tif (rc)\n\t\t\tgoto err_init_dev;\n\t} else if (pdev_is_gen3(pdev)) {\n\t\tndev->ntb.ops = &intel_ntb3_ops;\n\t\trc = intel_ntb_init_pci(ndev, pdev);\n\t\tif (rc)\n\t\t\tgoto err_init_pci;\n\n\t\trc = gen3_init_dev(ndev);\n\t\tif (rc)\n\t\t\tgoto err_init_dev;\n\t} else if (pdev_is_gen4(pdev) || pdev_is_gen5(pdev)) {\n\t\tndev->ntb.ops = &intel_ntb4_ops;\n\t\trc = intel_ntb_init_pci(ndev, pdev);\n\t\tif (rc)\n\t\t\tgoto err_init_pci;\n\n\t\trc = gen4_init_dev(ndev);\n\t\tif (rc)\n\t\t\tgoto err_init_dev;\n\t} else {\n\t\trc = -EINVAL;\n\t\tgoto err_init_pci;\n\t}\n\n\tndev_reset_unsafe_flags(ndev);\n\n\tndev->reg->poll_link(ndev);\n\n\tndev_init_debugfs(ndev);\n\n\trc = ntb_register_device(&ndev->ntb);\n\tif (rc)\n\t\tgoto err_register;\n\n\tdev_info(&pdev->dev, \"NTB device registered.\\n\");\n\n\treturn 0;\n\nerr_register:\n\tndev_deinit_debugfs(ndev);\n\tif (pdev_is_gen1(pdev) || pdev_is_gen3(pdev) ||\n\t    pdev_is_gen4(pdev) || pdev_is_gen5(pdev))\n\t\txeon_deinit_dev(ndev);\nerr_init_dev:\n\tintel_ntb_deinit_pci(ndev);\nerr_init_pci:\n\tkfree(ndev);\nerr_ndev:\n\treturn rc;\n}\n\nstatic void intel_ntb_pci_remove(struct pci_dev *pdev)\n{\n\tstruct intel_ntb_dev *ndev = pci_get_drvdata(pdev);\n\n\tntb_unregister_device(&ndev->ntb);\n\tndev_deinit_debugfs(ndev);\n\tif (pdev_is_gen1(pdev) || pdev_is_gen3(pdev) ||\n\t    pdev_is_gen4(pdev) || pdev_is_gen5(pdev))\n\t\txeon_deinit_dev(ndev);\n\tintel_ntb_deinit_pci(ndev);\n\tkfree(ndev);\n}\n\nstatic const struct intel_ntb_reg xeon_reg = {\n\t.poll_link\t\t= xeon_poll_link,\n\t.link_is_up\t\t= xeon_link_is_up,\n\t.db_ioread\t\t= xeon_db_ioread,\n\t.db_iowrite\t\t= xeon_db_iowrite,\n\t.db_size\t\t= sizeof(u32),\n\t.ntb_ctl\t\t= XEON_NTBCNTL_OFFSET,\n\t.mw_bar\t\t\t= {2, 4, 5},\n};\n\nstatic const struct intel_ntb_alt_reg xeon_pri_reg = {\n\t.db_bell\t\t= XEON_PDOORBELL_OFFSET,\n\t.db_mask\t\t= XEON_PDBMSK_OFFSET,\n\t.spad\t\t\t= XEON_SPAD_OFFSET,\n};\n\nstatic const struct intel_ntb_alt_reg xeon_sec_reg = {\n\t.db_bell\t\t= XEON_SDOORBELL_OFFSET,\n\t.db_mask\t\t= XEON_SDBMSK_OFFSET,\n\t \n\t.spad\t\t\t= XEON_SPAD_OFFSET + (XEON_SPAD_COUNT << 1),\n};\n\nstatic const struct intel_ntb_alt_reg xeon_b2b_reg = {\n\t.db_bell\t\t= XEON_B2B_DOORBELL_OFFSET,\n\t.spad\t\t\t= XEON_B2B_SPAD_OFFSET,\n};\n\nstatic const struct intel_ntb_xlat_reg xeon_pri_xlat = {\n\t \n\t.bar2_limit\t\t= XEON_PBAR23LMT_OFFSET,\n\t.bar2_xlat\t\t= XEON_PBAR23XLAT_OFFSET,\n};\n\nstatic const struct intel_ntb_xlat_reg xeon_sec_xlat = {\n\t.bar0_base\t\t= XEON_SBAR0BASE_OFFSET,\n\t.bar2_limit\t\t= XEON_SBAR23LMT_OFFSET,\n\t.bar2_xlat\t\t= XEON_SBAR23XLAT_OFFSET,\n};\n\nstruct intel_b2b_addr xeon_b2b_usd_addr = {\n\t.bar2_addr64\t\t= XEON_B2B_BAR2_ADDR64,\n\t.bar4_addr64\t\t= XEON_B2B_BAR4_ADDR64,\n\t.bar4_addr32\t\t= XEON_B2B_BAR4_ADDR32,\n\t.bar5_addr32\t\t= XEON_B2B_BAR5_ADDR32,\n};\n\nstruct intel_b2b_addr xeon_b2b_dsd_addr = {\n\t.bar2_addr64\t\t= XEON_B2B_BAR2_ADDR64,\n\t.bar4_addr64\t\t= XEON_B2B_BAR4_ADDR64,\n\t.bar4_addr32\t\t= XEON_B2B_BAR4_ADDR32,\n\t.bar5_addr32\t\t= XEON_B2B_BAR5_ADDR32,\n};\n\n \nstatic const struct ntb_dev_ops intel_ntb_ops = {\n\t.mw_count\t\t= intel_ntb_mw_count,\n\t.mw_get_align\t\t= intel_ntb_mw_get_align,\n\t.mw_set_trans\t\t= intel_ntb_mw_set_trans,\n\t.peer_mw_count\t\t= intel_ntb_peer_mw_count,\n\t.peer_mw_get_addr\t= intel_ntb_peer_mw_get_addr,\n\t.link_is_up\t\t= intel_ntb_link_is_up,\n\t.link_enable\t\t= intel_ntb_link_enable,\n\t.link_disable\t\t= intel_ntb_link_disable,\n\t.db_is_unsafe\t\t= intel_ntb_db_is_unsafe,\n\t.db_valid_mask\t\t= intel_ntb_db_valid_mask,\n\t.db_vector_count\t= intel_ntb_db_vector_count,\n\t.db_vector_mask\t\t= intel_ntb_db_vector_mask,\n\t.db_read\t\t= intel_ntb_db_read,\n\t.db_clear\t\t= intel_ntb_db_clear,\n\t.db_set_mask\t\t= intel_ntb_db_set_mask,\n\t.db_clear_mask\t\t= intel_ntb_db_clear_mask,\n\t.peer_db_addr\t\t= intel_ntb_peer_db_addr,\n\t.peer_db_set\t\t= intel_ntb_peer_db_set,\n\t.spad_is_unsafe\t\t= intel_ntb_spad_is_unsafe,\n\t.spad_count\t\t= intel_ntb_spad_count,\n\t.spad_read\t\t= intel_ntb_spad_read,\n\t.spad_write\t\t= intel_ntb_spad_write,\n\t.peer_spad_addr\t\t= intel_ntb_peer_spad_addr,\n\t.peer_spad_read\t\t= intel_ntb_peer_spad_read,\n\t.peer_spad_write\t= intel_ntb_peer_spad_write,\n};\n\nstatic const struct file_operations intel_ntb_debugfs_info = {\n\t.owner = THIS_MODULE,\n\t.open = simple_open,\n\t.read = ndev_debugfs_read,\n};\n\nstatic const struct pci_device_id intel_ntb_pci_tbl[] = {\n\t \n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_B2B_JSF)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_B2B_SNB)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_B2B_IVT)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_B2B_HSX)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_B2B_BDX)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_PS_JSF)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_PS_SNB)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_PS_IVT)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_PS_HSX)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_PS_BDX)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_SS_JSF)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_SS_SNB)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_SS_IVT)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_SS_HSX)},\n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_SS_BDX)},\n\n\t \n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_B2B_SKX)},\n\n\t \n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_B2B_ICX)},\n\t \n\t{PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_NTB_B2B_GNR)},\n\t{0}\n};\nMODULE_DEVICE_TABLE(pci, intel_ntb_pci_tbl);\n\nstatic struct pci_driver intel_ntb_pci_driver = {\n\t.name = KBUILD_MODNAME,\n\t.id_table = intel_ntb_pci_tbl,\n\t.probe = intel_ntb_pci_probe,\n\t.remove = intel_ntb_pci_remove,\n};\n\nstatic int __init intel_ntb_pci_driver_init(void)\n{\n\tint ret;\n\tpr_info(\"%s %s\\n\", NTB_DESC, NTB_VER);\n\n\tif (debugfs_initialized())\n\t\tdebugfs_dir = debugfs_create_dir(KBUILD_MODNAME, NULL);\n\n\tret = pci_register_driver(&intel_ntb_pci_driver);\n\tif (ret)\n\t\tdebugfs_remove_recursive(debugfs_dir);\n\n\treturn ret;\n}\nmodule_init(intel_ntb_pci_driver_init);\n\nstatic void __exit intel_ntb_pci_driver_exit(void)\n{\n\tpci_unregister_driver(&intel_ntb_pci_driver);\n\n\tdebugfs_remove_recursive(debugfs_dir);\n}\nmodule_exit(intel_ntb_pci_driver_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}