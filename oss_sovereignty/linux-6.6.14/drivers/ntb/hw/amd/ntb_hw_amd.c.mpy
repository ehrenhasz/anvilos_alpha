{
  "module_name": "ntb_hw_amd.c",
  "hash_id": "a1c5dd7345ba1c27e13254b883eda4e68f4cd6442263b201e4591ee48b216346",
  "original_prompt": "Ingested from linux-6.6.14/drivers/ntb/hw/amd/ntb_hw_amd.c",
  "human_readable_source": " \n\n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/acpi.h>\n#include <linux/pci.h>\n#include <linux/random.h>\n#include <linux/slab.h>\n#include <linux/ntb.h>\n\n#include \"ntb_hw_amd.h\"\n\n#define NTB_NAME\t\"ntb_hw_amd\"\n#define NTB_DESC\t\"AMD(R) PCI-E Non-Transparent Bridge Driver\"\n#define NTB_VER\t\t\"1.0\"\n\nMODULE_DESCRIPTION(NTB_DESC);\nMODULE_VERSION(NTB_VER);\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_AUTHOR(\"AMD Inc.\");\n\nstatic const struct file_operations amd_ntb_debugfs_info;\nstatic struct dentry *debugfs_dir;\n\nstatic int ndev_mw_to_bar(struct amd_ntb_dev *ndev, int idx)\n{\n\tif (idx < 0 || idx > ndev->mw_count)\n\t\treturn -EINVAL;\n\n\treturn ndev->dev_data->mw_idx << idx;\n}\n\nstatic int amd_ntb_mw_count(struct ntb_dev *ntb, int pidx)\n{\n\tif (pidx != NTB_DEF_PEER_IDX)\n\t\treturn -EINVAL;\n\n\treturn ntb_ndev(ntb)->mw_count;\n}\n\nstatic int amd_ntb_mw_get_align(struct ntb_dev *ntb, int pidx, int idx,\n\t\t\t\tresource_size_t *addr_align,\n\t\t\t\tresource_size_t *size_align,\n\t\t\t\tresource_size_t *size_max)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tint bar;\n\n\tif (pidx != NTB_DEF_PEER_IDX)\n\t\treturn -EINVAL;\n\n\tbar = ndev_mw_to_bar(ndev, idx);\n\tif (bar < 0)\n\t\treturn bar;\n\n\tif (addr_align)\n\t\t*addr_align = SZ_4K;\n\n\tif (size_align)\n\t\t*size_align = 1;\n\n\tif (size_max)\n\t\t*size_max = pci_resource_len(ndev->ntb.pdev, bar);\n\n\treturn 0;\n}\n\nstatic int amd_ntb_mw_set_trans(struct ntb_dev *ntb, int pidx, int idx,\n\t\t\t\tdma_addr_t addr, resource_size_t size)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tunsigned long xlat_reg, limit_reg = 0;\n\tresource_size_t mw_size;\n\tvoid __iomem *mmio, *peer_mmio;\n\tu64 base_addr, limit, reg_val;\n\tint bar;\n\n\tif (pidx != NTB_DEF_PEER_IDX)\n\t\treturn -EINVAL;\n\n\tbar = ndev_mw_to_bar(ndev, idx);\n\tif (bar < 0)\n\t\treturn bar;\n\n\tmw_size = pci_resource_len(ntb->pdev, bar);\n\n\t \n\tif (size > mw_size)\n\t\treturn -EINVAL;\n\n\tmmio = ndev->self_mmio;\n\tpeer_mmio = ndev->peer_mmio;\n\n\tbase_addr = pci_resource_start(ntb->pdev, bar);\n\n\tif (bar != 1) {\n\t\txlat_reg = AMD_BAR23XLAT_OFFSET + ((bar - 2) << 2);\n\t\tlimit_reg = AMD_BAR23LMT_OFFSET + ((bar - 2) << 2);\n\n\t\t \n\t\tlimit = size;\n\n\t\t \n\t\twrite64(addr, peer_mmio + xlat_reg);\n\t\treg_val = read64(peer_mmio + xlat_reg);\n\t\tif (reg_val != addr) {\n\t\t\twrite64(0, peer_mmio + xlat_reg);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\t \n\t\twrite64(limit, peer_mmio + limit_reg);\n\t\treg_val = read64(peer_mmio + limit_reg);\n\t\tif (reg_val != limit) {\n\t\t\twrite64(base_addr, mmio + limit_reg);\n\t\t\twrite64(0, peer_mmio + xlat_reg);\n\t\t\treturn -EIO;\n\t\t}\n\t} else {\n\t\txlat_reg = AMD_BAR1XLAT_OFFSET;\n\t\tlimit_reg = AMD_BAR1LMT_OFFSET;\n\n\t\t \n\t\tlimit = size;\n\n\t\t \n\t\twrite64(addr, peer_mmio + xlat_reg);\n\t\treg_val = read64(peer_mmio + xlat_reg);\n\t\tif (reg_val != addr) {\n\t\t\twrite64(0, peer_mmio + xlat_reg);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\t \n\t\twritel(limit, peer_mmio + limit_reg);\n\t\treg_val = readl(peer_mmio + limit_reg);\n\t\tif (reg_val != limit) {\n\t\t\twritel(base_addr, mmio + limit_reg);\n\t\t\twritel(0, peer_mmio + xlat_reg);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amd_ntb_get_link_status(struct amd_ntb_dev *ndev)\n{\n\tstruct pci_dev *pdev = NULL;\n\tstruct pci_dev *pci_swds = NULL;\n\tstruct pci_dev *pci_swus = NULL;\n\tu32 stat;\n\tint rc;\n\n\tif (ndev->ntb.topo == NTB_TOPO_SEC) {\n\t\t \n\t\tpci_swds = pci_upstream_bridge(ndev->ntb.pdev);\n\t\tif (pci_swds) {\n\t\t\t \n\t\t\tpci_swus = pci_upstream_bridge(pci_swds);\n\t\t\tif (pci_swus) {\n\t\t\t\trc = pcie_capability_read_dword(pci_swus,\n\t\t\t\t\t\t\t\tPCI_EXP_LNKCTL,\n\t\t\t\t\t\t\t\t&stat);\n\t\t\t\tif (rc)\n\t\t\t\t\treturn 0;\n\t\t\t} else {\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t} else if (ndev->ntb.topo == NTB_TOPO_PRI) {\n\t\t \n\t\tpdev = ndev->ntb.pdev;\n\t\trc = pcie_capability_read_dword(pdev, PCI_EXP_LNKCTL, &stat);\n\t\tif (rc)\n\t\t\treturn 0;\n\t} else {\n\t\t \n\t\treturn 0;\n\t}\n\n\tndev->lnk_sta = stat;\n\n\treturn 1;\n}\n\nstatic int amd_link_is_up(struct amd_ntb_dev *ndev)\n{\n\tint ret;\n\n\t \n\tret = amd_poll_link(ndev);\n\tif (ret) {\n\t\t \n\t\tif (ndev->ntb.topo == NTB_TOPO_PRI) {\n\t\t\tif ((ndev->peer_sta & AMD_LINK_UP_EVENT) ||\n\t\t\t    (ndev->peer_sta == 0))\n\t\t\t\treturn ret;\n\t\t\telse if (ndev->peer_sta & AMD_LINK_DOWN_EVENT) {\n\t\t\t\t \n\t\t\t\tamd_clear_side_info_reg(ndev, true);\n\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t} else {  \n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic u64 amd_ntb_link_is_up(struct ntb_dev *ntb,\n\t\t\t      enum ntb_speed *speed,\n\t\t\t      enum ntb_width *width)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tint ret = 0;\n\n\tif (amd_link_is_up(ndev)) {\n\t\tif (speed)\n\t\t\t*speed = NTB_LNK_STA_SPEED(ndev->lnk_sta);\n\t\tif (width)\n\t\t\t*width = NTB_LNK_STA_WIDTH(ndev->lnk_sta);\n\n\t\tdev_dbg(&ntb->pdev->dev, \"link is up.\\n\");\n\n\t\tret = 1;\n\t} else {\n\t\tif (speed)\n\t\t\t*speed = NTB_SPEED_NONE;\n\t\tif (width)\n\t\t\t*width = NTB_WIDTH_NONE;\n\n\t\tdev_dbg(&ntb->pdev->dev, \"link is down.\\n\");\n\t}\n\n\treturn ret;\n}\n\nstatic int amd_ntb_link_enable(struct ntb_dev *ntb,\n\t\t\t       enum ntb_speed max_speed,\n\t\t\t       enum ntb_width max_width)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\n\t \n\tndev->int_mask &= ~AMD_EVENT_INTMASK;\n\twritel(ndev->int_mask, mmio + AMD_INTMASK_OFFSET);\n\n\tif (ndev->ntb.topo == NTB_TOPO_SEC)\n\t\treturn -EINVAL;\n\tdev_dbg(&ntb->pdev->dev, \"Enabling Link.\\n\");\n\n\treturn 0;\n}\n\nstatic int amd_ntb_link_disable(struct ntb_dev *ntb)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\n\t \n\tndev->int_mask |= AMD_EVENT_INTMASK;\n\twritel(ndev->int_mask, mmio + AMD_INTMASK_OFFSET);\n\n\tif (ndev->ntb.topo == NTB_TOPO_SEC)\n\t\treturn -EINVAL;\n\tdev_dbg(&ntb->pdev->dev, \"Enabling Link.\\n\");\n\n\treturn 0;\n}\n\nstatic int amd_ntb_peer_mw_count(struct ntb_dev *ntb)\n{\n\t \n\treturn ntb_ndev(ntb)->mw_count;\n}\n\nstatic int amd_ntb_peer_mw_get_addr(struct ntb_dev *ntb, int idx,\n\t\t\t\t    phys_addr_t *base, resource_size_t *size)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tint bar;\n\n\tbar = ndev_mw_to_bar(ndev, idx);\n\tif (bar < 0)\n\t\treturn bar;\n\n\tif (base)\n\t\t*base = pci_resource_start(ndev->ntb.pdev, bar);\n\n\tif (size)\n\t\t*size = pci_resource_len(ndev->ntb.pdev, bar);\n\n\treturn 0;\n}\n\nstatic u64 amd_ntb_db_valid_mask(struct ntb_dev *ntb)\n{\n\treturn ntb_ndev(ntb)->db_valid_mask;\n}\n\nstatic int amd_ntb_db_vector_count(struct ntb_dev *ntb)\n{\n\treturn ntb_ndev(ntb)->db_count;\n}\n\nstatic u64 amd_ntb_db_vector_mask(struct ntb_dev *ntb, int db_vector)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\n\tif (db_vector < 0 || db_vector > ndev->db_count)\n\t\treturn 0;\n\n\treturn ntb_ndev(ntb)->db_valid_mask & (1ULL << db_vector);\n}\n\nstatic u64 amd_ntb_db_read(struct ntb_dev *ntb)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\n\treturn (u64)readw(mmio + AMD_DBSTAT_OFFSET);\n}\n\nstatic int amd_ntb_db_clear(struct ntb_dev *ntb, u64 db_bits)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\n\twritew((u16)db_bits, mmio + AMD_DBSTAT_OFFSET);\n\n\treturn 0;\n}\n\nstatic int amd_ntb_db_set_mask(struct ntb_dev *ntb, u64 db_bits)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tunsigned long flags;\n\n\tif (db_bits & ~ndev->db_valid_mask)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&ndev->db_mask_lock, flags);\n\tndev->db_mask |= db_bits;\n\twritew((u16)ndev->db_mask, mmio + AMD_DBMASK_OFFSET);\n\tspin_unlock_irqrestore(&ndev->db_mask_lock, flags);\n\n\treturn 0;\n}\n\nstatic int amd_ntb_db_clear_mask(struct ntb_dev *ntb, u64 db_bits)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tunsigned long flags;\n\n\tif (db_bits & ~ndev->db_valid_mask)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&ndev->db_mask_lock, flags);\n\tndev->db_mask &= ~db_bits;\n\twritew((u16)ndev->db_mask, mmio + AMD_DBMASK_OFFSET);\n\tspin_unlock_irqrestore(&ndev->db_mask_lock, flags);\n\n\treturn 0;\n}\n\nstatic int amd_ntb_peer_db_set(struct ntb_dev *ntb, u64 db_bits)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\n\twritew((u16)db_bits, mmio + AMD_DBREQ_OFFSET);\n\n\treturn 0;\n}\n\nstatic int amd_ntb_spad_count(struct ntb_dev *ntb)\n{\n\treturn ntb_ndev(ntb)->spad_count;\n}\n\nstatic u32 amd_ntb_spad_read(struct ntb_dev *ntb, int idx)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tu32 offset;\n\n\tif (idx < 0 || idx >= ndev->spad_count)\n\t\treturn 0;\n\n\toffset = ndev->self_spad + (idx << 2);\n\treturn readl(mmio + AMD_SPAD_OFFSET + offset);\n}\n\nstatic int amd_ntb_spad_write(struct ntb_dev *ntb,\n\t\t\t      int idx, u32 val)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tu32 offset;\n\n\tif (idx < 0 || idx >= ndev->spad_count)\n\t\treturn -EINVAL;\n\n\toffset = ndev->self_spad + (idx << 2);\n\twritel(val, mmio + AMD_SPAD_OFFSET + offset);\n\n\treturn 0;\n}\n\nstatic u32 amd_ntb_peer_spad_read(struct ntb_dev *ntb, int pidx, int sidx)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tu32 offset;\n\n\tif (sidx < 0 || sidx >= ndev->spad_count)\n\t\treturn -EINVAL;\n\n\toffset = ndev->peer_spad + (sidx << 2);\n\treturn readl(mmio + AMD_SPAD_OFFSET + offset);\n}\n\nstatic int amd_ntb_peer_spad_write(struct ntb_dev *ntb, int pidx,\n\t\t\t\t   int sidx, u32 val)\n{\n\tstruct amd_ntb_dev *ndev = ntb_ndev(ntb);\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tu32 offset;\n\n\tif (sidx < 0 || sidx >= ndev->spad_count)\n\t\treturn -EINVAL;\n\n\toffset = ndev->peer_spad + (sidx << 2);\n\twritel(val, mmio + AMD_SPAD_OFFSET + offset);\n\n\treturn 0;\n}\n\nstatic const struct ntb_dev_ops amd_ntb_ops = {\n\t.mw_count\t\t= amd_ntb_mw_count,\n\t.mw_get_align\t\t= amd_ntb_mw_get_align,\n\t.mw_set_trans\t\t= amd_ntb_mw_set_trans,\n\t.peer_mw_count\t\t= amd_ntb_peer_mw_count,\n\t.peer_mw_get_addr\t= amd_ntb_peer_mw_get_addr,\n\t.link_is_up\t\t= amd_ntb_link_is_up,\n\t.link_enable\t\t= amd_ntb_link_enable,\n\t.link_disable\t\t= amd_ntb_link_disable,\n\t.db_valid_mask\t\t= amd_ntb_db_valid_mask,\n\t.db_vector_count\t= amd_ntb_db_vector_count,\n\t.db_vector_mask\t\t= amd_ntb_db_vector_mask,\n\t.db_read\t\t= amd_ntb_db_read,\n\t.db_clear\t\t= amd_ntb_db_clear,\n\t.db_set_mask\t\t= amd_ntb_db_set_mask,\n\t.db_clear_mask\t\t= amd_ntb_db_clear_mask,\n\t.peer_db_set\t\t= amd_ntb_peer_db_set,\n\t.spad_count\t\t= amd_ntb_spad_count,\n\t.spad_read\t\t= amd_ntb_spad_read,\n\t.spad_write\t\t= amd_ntb_spad_write,\n\t.peer_spad_read\t\t= amd_ntb_peer_spad_read,\n\t.peer_spad_write\t= amd_ntb_peer_spad_write,\n};\n\nstatic void amd_ack_smu(struct amd_ntb_dev *ndev, u32 bit)\n{\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tint reg;\n\n\treg = readl(mmio + AMD_SMUACK_OFFSET);\n\treg |= bit;\n\twritel(reg, mmio + AMD_SMUACK_OFFSET);\n}\n\nstatic void amd_handle_event(struct amd_ntb_dev *ndev, int vec)\n{\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tstruct device *dev = &ndev->ntb.pdev->dev;\n\tu32 status;\n\n\tstatus = readl(mmio + AMD_INTSTAT_OFFSET);\n\tif (!(status & AMD_EVENT_INTMASK))\n\t\treturn;\n\n\tdev_dbg(dev, \"status = 0x%x and vec = %d\\n\", status, vec);\n\n\tstatus &= AMD_EVENT_INTMASK;\n\tswitch (status) {\n\tcase AMD_PEER_FLUSH_EVENT:\n\t\tndev->peer_sta |= AMD_PEER_FLUSH_EVENT;\n\t\tdev_info(dev, \"Flush is done.\\n\");\n\t\tbreak;\n\tcase AMD_PEER_RESET_EVENT:\n\tcase AMD_LINK_DOWN_EVENT:\n\t\tndev->peer_sta |= status;\n\t\tif (status == AMD_LINK_DOWN_EVENT)\n\t\t\tndev->peer_sta &= ~AMD_LINK_UP_EVENT;\n\n\t\tamd_ack_smu(ndev, status);\n\n\t\t \n\t\tntb_link_event(&ndev->ntb);\n\t\t \n\t\tschedule_delayed_work(&ndev->hb_timer, AMD_LINK_HB_TIMEOUT);\n\n\t\tbreak;\n\tcase AMD_PEER_D3_EVENT:\n\tcase AMD_PEER_PMETO_EVENT:\n\tcase AMD_LINK_UP_EVENT:\n\t\tndev->peer_sta |= status;\n\t\tif (status == AMD_LINK_UP_EVENT)\n\t\t\tndev->peer_sta &= ~AMD_LINK_DOWN_EVENT;\n\t\telse if (status == AMD_PEER_D3_EVENT)\n\t\t\tndev->peer_sta &= ~AMD_PEER_D0_EVENT;\n\n\t\tamd_ack_smu(ndev, status);\n\n\t\t \n\t\tntb_link_event(&ndev->ntb);\n\n\t\tbreak;\n\tcase AMD_PEER_D0_EVENT:\n\t\tmmio = ndev->peer_mmio;\n\t\tstatus = readl(mmio + AMD_PMESTAT_OFFSET);\n\t\t \n\t\tif (status & 0x1)\n\t\t\tdev_info(dev, \"Wakeup is done.\\n\");\n\n\t\tndev->peer_sta |= AMD_PEER_D0_EVENT;\n\t\tndev->peer_sta &= ~AMD_PEER_D3_EVENT;\n\t\tamd_ack_smu(ndev, AMD_PEER_D0_EVENT);\n\n\t\t \n\t\tschedule_delayed_work(&ndev->hb_timer,\n\t\t\t\t      AMD_LINK_HB_TIMEOUT);\n\t\tbreak;\n\tdefault:\n\t\tdev_info(dev, \"event status = 0x%x.\\n\", status);\n\t\tbreak;\n\t}\n\n\t \n\twritel(status, mmio + AMD_INTSTAT_OFFSET);\n}\n\nstatic void amd_handle_db_event(struct amd_ntb_dev *ndev, int vec)\n{\n\tstruct device *dev = &ndev->ntb.pdev->dev;\n\tu64 status;\n\n\tstatus = amd_ntb_db_read(&ndev->ntb);\n\n\tdev_dbg(dev, \"status = 0x%llx and vec = %d\\n\", status, vec);\n\n\t \n\tif (status & BIT(ndev->db_last_bit)) {\n\t\tntb_db_clear(&ndev->ntb, BIT(ndev->db_last_bit));\n\t\t \n\t\tntb_link_event(&ndev->ntb);\n\n\t\t \n\t\tschedule_delayed_work(&ndev->hb_timer, AMD_LINK_HB_TIMEOUT);\n\t}\n}\n\nstatic irqreturn_t ndev_interrupt(struct amd_ntb_dev *ndev, int vec)\n{\n\tdev_dbg(&ndev->ntb.pdev->dev, \"vec %d\\n\", vec);\n\n\tif (vec > (AMD_DB_CNT - 1) || (ndev->msix_vec_count == 1))\n\t\tamd_handle_event(ndev, vec);\n\n\tif (vec < AMD_DB_CNT) {\n\t\tamd_handle_db_event(ndev, vec);\n\t\tntb_db_event(&ndev->ntb, vec);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t ndev_vec_isr(int irq, void *dev)\n{\n\tstruct amd_ntb_vec *nvec = dev;\n\n\treturn ndev_interrupt(nvec->ndev, nvec->num);\n}\n\nstatic irqreturn_t ndev_irq_isr(int irq, void *dev)\n{\n\tstruct amd_ntb_dev *ndev = dev;\n\n\treturn ndev_interrupt(ndev, irq - ndev->ntb.pdev->irq);\n}\n\nstatic int ndev_init_isr(struct amd_ntb_dev *ndev,\n\t\t\t int msix_min, int msix_max)\n{\n\tstruct pci_dev *pdev;\n\tint rc, i, msix_count, node;\n\n\tpdev = ndev->ntb.pdev;\n\n\tnode = dev_to_node(&pdev->dev);\n\n\tndev->db_mask = ndev->db_valid_mask;\n\n\t \n\tndev->vec = kcalloc_node(msix_max, sizeof(*ndev->vec),\n\t\t\t\t GFP_KERNEL, node);\n\tif (!ndev->vec)\n\t\tgoto err_msix_vec_alloc;\n\n\tndev->msix = kcalloc_node(msix_max, sizeof(*ndev->msix),\n\t\t\t\t  GFP_KERNEL, node);\n\tif (!ndev->msix)\n\t\tgoto err_msix_alloc;\n\n\tfor (i = 0; i < msix_max; ++i)\n\t\tndev->msix[i].entry = i;\n\n\tmsix_count = pci_enable_msix_range(pdev, ndev->msix,\n\t\t\t\t\t   msix_min, msix_max);\n\tif (msix_count < 0)\n\t\tgoto err_msix_enable;\n\n\t \n\tif (msix_count < msix_min) {\n\t\tpci_disable_msix(pdev);\n\t\tgoto err_msix_enable;\n\t}\n\n\tfor (i = 0; i < msix_count; ++i) {\n\t\tndev->vec[i].ndev = ndev;\n\t\tndev->vec[i].num = i;\n\t\trc = request_irq(ndev->msix[i].vector, ndev_vec_isr, 0,\n\t\t\t\t \"ndev_vec_isr\", &ndev->vec[i]);\n\t\tif (rc)\n\t\t\tgoto err_msix_request;\n\t}\n\n\tdev_dbg(&pdev->dev, \"Using msix interrupts\\n\");\n\tndev->db_count = msix_min;\n\tndev->msix_vec_count = msix_max;\n\treturn 0;\n\nerr_msix_request:\n\twhile (i-- > 0)\n\t\tfree_irq(ndev->msix[i].vector, &ndev->vec[i]);\n\tpci_disable_msix(pdev);\nerr_msix_enable:\n\tkfree(ndev->msix);\nerr_msix_alloc:\n\tkfree(ndev->vec);\nerr_msix_vec_alloc:\n\tndev->msix = NULL;\n\tndev->vec = NULL;\n\n\t \n\trc = pci_enable_msi(pdev);\n\tif (rc)\n\t\tgoto err_msi_enable;\n\n\trc = request_irq(pdev->irq, ndev_irq_isr, 0,\n\t\t\t \"ndev_irq_isr\", ndev);\n\tif (rc)\n\t\tgoto err_msi_request;\n\n\tdev_dbg(&pdev->dev, \"Using msi interrupts\\n\");\n\tndev->db_count = 1;\n\tndev->msix_vec_count = 1;\n\treturn 0;\n\nerr_msi_request:\n\tpci_disable_msi(pdev);\nerr_msi_enable:\n\n\t \n\tpci_intx(pdev, 1);\n\n\trc = request_irq(pdev->irq, ndev_irq_isr, IRQF_SHARED,\n\t\t\t \"ndev_irq_isr\", ndev);\n\tif (rc)\n\t\tgoto err_intx_request;\n\n\tdev_dbg(&pdev->dev, \"Using intx interrupts\\n\");\n\tndev->db_count = 1;\n\tndev->msix_vec_count = 1;\n\treturn 0;\n\nerr_intx_request:\n\treturn rc;\n}\n\nstatic void ndev_deinit_isr(struct amd_ntb_dev *ndev)\n{\n\tstruct pci_dev *pdev;\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tint i;\n\n\tpdev = ndev->ntb.pdev;\n\n\t \n\tndev->db_mask = ndev->db_valid_mask;\n\twritel(ndev->db_mask, mmio + AMD_DBMASK_OFFSET);\n\n\tif (ndev->msix) {\n\t\ti = ndev->msix_vec_count;\n\t\twhile (i--)\n\t\t\tfree_irq(ndev->msix[i].vector, &ndev->vec[i]);\n\t\tpci_disable_msix(pdev);\n\t\tkfree(ndev->msix);\n\t\tkfree(ndev->vec);\n\t} else {\n\t\tfree_irq(pdev->irq, ndev);\n\t\tif (pci_dev_msi_enabled(pdev))\n\t\t\tpci_disable_msi(pdev);\n\t\telse\n\t\t\tpci_intx(pdev, 0);\n\t}\n}\n\nstatic ssize_t ndev_debugfs_read(struct file *filp, char __user *ubuf,\n\t\t\t\t size_t count, loff_t *offp)\n{\n\tstruct amd_ntb_dev *ndev;\n\tvoid __iomem *mmio;\n\tchar *buf;\n\tsize_t buf_size;\n\tssize_t ret, off;\n\tunion { u64 v64; u32 v32; u16 v16; } u;\n\n\tndev = filp->private_data;\n\tmmio = ndev->self_mmio;\n\n\tbuf_size = min(count, 0x800ul);\n\n\tbuf = kmalloc(buf_size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\toff = 0;\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"NTB Device Information:\\n\");\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Connection Topology -\\t%s\\n\",\n\t\t\t ntb_topo_string(ndev->ntb.topo));\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"LNK STA -\\t\\t%#06x\\n\", ndev->lnk_sta);\n\n\tif (!amd_link_is_up(ndev)) {\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"Link Status -\\t\\tDown\\n\");\n\t} else {\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"Link Status -\\t\\tUp\\n\");\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"Link Speed -\\t\\tPCI-E Gen %u\\n\",\n\t\t\t\t NTB_LNK_STA_SPEED(ndev->lnk_sta));\n\t\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t\t \"Link Width -\\t\\tx%u\\n\",\n\t\t\t\t NTB_LNK_STA_WIDTH(ndev->lnk_sta));\n\t}\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Memory Window Count -\\t%u\\n\", ndev->mw_count);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Scratchpad Count -\\t%u\\n\", ndev->spad_count);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Count -\\t%u\\n\", ndev->db_count);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"MSIX Vector Count -\\t%u\\n\", ndev->msix_vec_count);\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Valid Mask -\\t%#llx\\n\", ndev->db_valid_mask);\n\n\tu.v32 = readl(ndev->self_mmio + AMD_DBMASK_OFFSET);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Mask -\\t\\t\\t%#06x\\n\", u.v32);\n\n\tu.v32 = readl(mmio + AMD_DBSTAT_OFFSET);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"Doorbell Bell -\\t\\t\\t%#06x\\n\", u.v32);\n\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"\\nNTB Incoming XLAT:\\n\");\n\n\tu.v64 = read64(mmio + AMD_BAR1XLAT_OFFSET);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"XLAT1 -\\t\\t%#018llx\\n\", u.v64);\n\n\tu.v64 = read64(ndev->self_mmio + AMD_BAR23XLAT_OFFSET);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"XLAT23 -\\t\\t%#018llx\\n\", u.v64);\n\n\tu.v64 = read64(ndev->self_mmio + AMD_BAR45XLAT_OFFSET);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"XLAT45 -\\t\\t%#018llx\\n\", u.v64);\n\n\tu.v32 = readl(mmio + AMD_BAR1LMT_OFFSET);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"LMT1 -\\t\\t\\t%#06x\\n\", u.v32);\n\n\tu.v64 = read64(ndev->self_mmio + AMD_BAR23LMT_OFFSET);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"LMT23 -\\t\\t\\t%#018llx\\n\", u.v64);\n\n\tu.v64 = read64(ndev->self_mmio + AMD_BAR45LMT_OFFSET);\n\toff += scnprintf(buf + off, buf_size - off,\n\t\t\t \"LMT45 -\\t\\t\\t%#018llx\\n\", u.v64);\n\n\tret = simple_read_from_buffer(ubuf, count, offp, buf, off);\n\tkfree(buf);\n\treturn ret;\n}\n\nstatic void ndev_init_debugfs(struct amd_ntb_dev *ndev)\n{\n\tif (!debugfs_dir) {\n\t\tndev->debugfs_dir = NULL;\n\t\tndev->debugfs_info = NULL;\n\t} else {\n\t\tndev->debugfs_dir =\n\t\t\tdebugfs_create_dir(pci_name(ndev->ntb.pdev),\n\t\t\t\t\t   debugfs_dir);\n\t\tndev->debugfs_info =\n\t\t\tdebugfs_create_file(\"info\", S_IRUSR,\n\t\t\t\t\t    ndev->debugfs_dir, ndev,\n\t\t\t\t\t    &amd_ntb_debugfs_info);\n\t}\n}\n\nstatic void ndev_deinit_debugfs(struct amd_ntb_dev *ndev)\n{\n\tdebugfs_remove_recursive(ndev->debugfs_dir);\n}\n\nstatic inline void ndev_init_struct(struct amd_ntb_dev *ndev,\n\t\t\t\t    struct pci_dev *pdev)\n{\n\tndev->ntb.pdev = pdev;\n\tndev->ntb.topo = NTB_TOPO_NONE;\n\tndev->ntb.ops = &amd_ntb_ops;\n\tndev->int_mask = AMD_EVENT_INTMASK;\n\tspin_lock_init(&ndev->db_mask_lock);\n}\n\nstatic int amd_poll_link(struct amd_ntb_dev *ndev)\n{\n\tvoid __iomem *mmio = ndev->peer_mmio;\n\tu32 reg;\n\n\treg = readl(mmio + AMD_SIDEINFO_OFFSET);\n\treg &= AMD_SIDE_READY;\n\n\tdev_dbg(&ndev->ntb.pdev->dev, \"%s: reg_val = 0x%x.\\n\", __func__, reg);\n\n\tndev->cntl_sta = reg;\n\n\tamd_ntb_get_link_status(ndev);\n\n\treturn ndev->cntl_sta;\n}\n\nstatic void amd_link_hb(struct work_struct *work)\n{\n\tstruct amd_ntb_dev *ndev = hb_ndev(work);\n\n\tif (amd_poll_link(ndev))\n\t\tntb_link_event(&ndev->ntb);\n\n\tif (!amd_link_is_up(ndev))\n\t\tschedule_delayed_work(&ndev->hb_timer, AMD_LINK_HB_TIMEOUT);\n}\n\nstatic int amd_init_isr(struct amd_ntb_dev *ndev)\n{\n\treturn ndev_init_isr(ndev, AMD_DB_CNT, AMD_MSIX_VECTOR_CNT);\n}\n\nstatic void amd_set_side_info_reg(struct amd_ntb_dev *ndev, bool peer)\n{\n\tvoid __iomem *mmio = NULL;\n\tunsigned int reg;\n\n\tif (peer)\n\t\tmmio = ndev->peer_mmio;\n\telse\n\t\tmmio = ndev->self_mmio;\n\n\treg = readl(mmio + AMD_SIDEINFO_OFFSET);\n\tif (!(reg & AMD_SIDE_READY)) {\n\t\treg |= AMD_SIDE_READY;\n\t\twritel(reg, mmio + AMD_SIDEINFO_OFFSET);\n\t}\n}\n\nstatic void amd_clear_side_info_reg(struct amd_ntb_dev *ndev, bool peer)\n{\n\tvoid __iomem *mmio = NULL;\n\tunsigned int reg;\n\n\tif (peer)\n\t\tmmio = ndev->peer_mmio;\n\telse\n\t\tmmio = ndev->self_mmio;\n\n\treg = readl(mmio + AMD_SIDEINFO_OFFSET);\n\tif (reg & AMD_SIDE_READY) {\n\t\treg &= ~AMD_SIDE_READY;\n\t\twritel(reg, mmio + AMD_SIDEINFO_OFFSET);\n\t\treadl(mmio + AMD_SIDEINFO_OFFSET);\n\t}\n}\n\nstatic void amd_init_side_info(struct amd_ntb_dev *ndev)\n{\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tu32 ntb_ctl;\n\n\tamd_set_side_info_reg(ndev, false);\n\n\tntb_ctl = readl(mmio + AMD_CNTL_OFFSET);\n\tntb_ctl |= (PMM_REG_CTL | SMM_REG_CTL);\n\twritel(ntb_ctl, mmio + AMD_CNTL_OFFSET);\n}\n\nstatic void amd_deinit_side_info(struct amd_ntb_dev *ndev)\n{\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tu32 ntb_ctl;\n\n\tamd_clear_side_info_reg(ndev, false);\n\n\tntb_ctl = readl(mmio + AMD_CNTL_OFFSET);\n\tntb_ctl &= ~(PMM_REG_CTL | SMM_REG_CTL);\n\twritel(ntb_ctl, mmio + AMD_CNTL_OFFSET);\n}\n\nstatic int amd_init_ntb(struct amd_ntb_dev *ndev)\n{\n\tvoid __iomem *mmio = ndev->self_mmio;\n\n\tndev->mw_count = ndev->dev_data->mw_count;\n\tndev->spad_count = AMD_SPADS_CNT;\n\tndev->db_count = AMD_DB_CNT;\n\n\tswitch (ndev->ntb.topo) {\n\tcase NTB_TOPO_PRI:\n\tcase NTB_TOPO_SEC:\n\t\tndev->spad_count >>= 1;\n\t\tif (ndev->ntb.topo == NTB_TOPO_PRI) {\n\t\t\tndev->self_spad = 0;\n\t\t\tndev->peer_spad = 0x20;\n\t\t} else {\n\t\t\tndev->self_spad = 0x20;\n\t\t\tndev->peer_spad = 0;\n\t\t}\n\n\t\tINIT_DELAYED_WORK(&ndev->hb_timer, amd_link_hb);\n\t\tschedule_delayed_work(&ndev->hb_timer, AMD_LINK_HB_TIMEOUT);\n\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&ndev->ntb.pdev->dev,\n\t\t\t\"AMD NTB does not support B2B mode.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\twritel(ndev->int_mask, mmio + AMD_INTMASK_OFFSET);\n\n\treturn 0;\n}\n\nstatic enum ntb_topo amd_get_topo(struct amd_ntb_dev *ndev)\n{\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tu32 info;\n\n\tinfo = readl(mmio + AMD_SIDEINFO_OFFSET);\n\tif (info & AMD_SIDE_MASK)\n\t\treturn NTB_TOPO_SEC;\n\telse\n\t\treturn NTB_TOPO_PRI;\n}\n\nstatic int amd_init_dev(struct amd_ntb_dev *ndev)\n{\n\tvoid __iomem *mmio = ndev->self_mmio;\n\tstruct pci_dev *pdev;\n\tint rc = 0;\n\n\tpdev = ndev->ntb.pdev;\n\n\tndev->ntb.topo = amd_get_topo(ndev);\n\tdev_dbg(&pdev->dev, \"AMD NTB topo is %s\\n\",\n\t\tntb_topo_string(ndev->ntb.topo));\n\n\trc = amd_init_ntb(ndev);\n\tif (rc)\n\t\treturn rc;\n\n\trc = amd_init_isr(ndev);\n\tif (rc) {\n\t\tdev_err(&pdev->dev, \"fail to init isr.\\n\");\n\t\treturn rc;\n\t}\n\n\tndev->db_valid_mask = BIT_ULL(ndev->db_count) - 1;\n\t \n\tndev->db_last_bit =\n\t\t\tfind_last_bit((unsigned long *)&ndev->db_valid_mask,\n\t\t\t\t      hweight64(ndev->db_valid_mask));\n\twritew((u16)~BIT(ndev->db_last_bit), mmio + AMD_DBMASK_OFFSET);\n\t \n\tndev->db_count -= 1;\n\tndev->db_valid_mask = BIT_ULL(ndev->db_count) - 1;\n\n\t \n\tndev->int_mask &= ~(AMD_LINK_UP_EVENT | AMD_LINK_DOWN_EVENT);\n\twritel(ndev->int_mask, mmio + AMD_INTMASK_OFFSET);\n\n\treturn 0;\n}\n\nstatic void amd_deinit_dev(struct amd_ntb_dev *ndev)\n{\n\tcancel_delayed_work_sync(&ndev->hb_timer);\n\n\tndev_deinit_isr(ndev);\n}\n\nstatic int amd_ntb_init_pci(struct amd_ntb_dev *ndev,\n\t\t\t    struct pci_dev *pdev)\n{\n\tint rc;\n\n\tpci_set_drvdata(pdev, ndev);\n\n\trc = pci_enable_device(pdev);\n\tif (rc)\n\t\tgoto err_pci_enable;\n\n\trc = pci_request_regions(pdev, NTB_NAME);\n\tif (rc)\n\t\tgoto err_pci_regions;\n\n\tpci_set_master(pdev);\n\n\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (rc) {\n\t\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (rc)\n\t\t\tgoto err_dma_mask;\n\t\tdev_warn(&pdev->dev, \"Cannot DMA highmem\\n\");\n\t}\n\n\tndev->self_mmio = pci_iomap(pdev, 0, 0);\n\tif (!ndev->self_mmio) {\n\t\trc = -EIO;\n\t\tgoto err_dma_mask;\n\t}\n\tndev->peer_mmio = ndev->self_mmio + AMD_PEER_OFFSET;\n\n\treturn 0;\n\nerr_dma_mask:\n\tpci_release_regions(pdev);\nerr_pci_regions:\n\tpci_disable_device(pdev);\nerr_pci_enable:\n\tpci_set_drvdata(pdev, NULL);\n\treturn rc;\n}\n\nstatic void amd_ntb_deinit_pci(struct amd_ntb_dev *ndev)\n{\n\tstruct pci_dev *pdev = ndev->ntb.pdev;\n\n\tpci_iounmap(pdev, ndev->self_mmio);\n\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tpci_set_drvdata(pdev, NULL);\n}\n\nstatic int amd_ntb_pci_probe(struct pci_dev *pdev,\n\t\t\t     const struct pci_device_id *id)\n{\n\tstruct amd_ntb_dev *ndev;\n\tint rc, node;\n\n\tnode = dev_to_node(&pdev->dev);\n\n\tndev = kzalloc_node(sizeof(*ndev), GFP_KERNEL, node);\n\tif (!ndev) {\n\t\trc = -ENOMEM;\n\t\tgoto err_ndev;\n\t}\n\n\tndev->dev_data = (struct ntb_dev_data *)id->driver_data;\n\n\tndev_init_struct(ndev, pdev);\n\n\trc = amd_ntb_init_pci(ndev, pdev);\n\tif (rc)\n\t\tgoto err_init_pci;\n\n\trc = amd_init_dev(ndev);\n\tif (rc)\n\t\tgoto err_init_dev;\n\n\t \n\tamd_init_side_info(ndev);\n\n\tamd_poll_link(ndev);\n\n\tndev_init_debugfs(ndev);\n\n\trc = ntb_register_device(&ndev->ntb);\n\tif (rc)\n\t\tgoto err_register;\n\n\tdev_info(&pdev->dev, \"NTB device registered.\\n\");\n\n\treturn 0;\n\nerr_register:\n\tndev_deinit_debugfs(ndev);\n\tamd_deinit_dev(ndev);\nerr_init_dev:\n\tamd_ntb_deinit_pci(ndev);\nerr_init_pci:\n\tkfree(ndev);\nerr_ndev:\n\treturn rc;\n}\n\nstatic void amd_ntb_pci_remove(struct pci_dev *pdev)\n{\n\tstruct amd_ntb_dev *ndev = pci_get_drvdata(pdev);\n\n\t \n\tamd_deinit_side_info(ndev);\n\tntb_peer_db_set(&ndev->ntb, BIT_ULL(ndev->db_last_bit));\n\tntb_unregister_device(&ndev->ntb);\n\tndev_deinit_debugfs(ndev);\n\tamd_deinit_dev(ndev);\n\tamd_ntb_deinit_pci(ndev);\n\tkfree(ndev);\n}\n\nstatic void amd_ntb_pci_shutdown(struct pci_dev *pdev)\n{\n\tstruct amd_ntb_dev *ndev = pci_get_drvdata(pdev);\n\n\t \n\tntb_link_event(&ndev->ntb);\n\n\tamd_deinit_side_info(ndev);\n\tntb_peer_db_set(&ndev->ntb, BIT_ULL(ndev->db_last_bit));\n\tntb_unregister_device(&ndev->ntb);\n\tndev_deinit_debugfs(ndev);\n\tamd_deinit_dev(ndev);\n\tamd_ntb_deinit_pci(ndev);\n\tkfree(ndev);\n}\n\nstatic const struct file_operations amd_ntb_debugfs_info = {\n\t.owner = THIS_MODULE,\n\t.open = simple_open,\n\t.read = ndev_debugfs_read,\n};\n\nstatic const struct ntb_dev_data dev_data[] = {\n\t{  \n\t\t.mw_count = 3,\n\t\t.mw_idx = 1,\n\t},\n\t{  \n\t\t.mw_count = 2,\n\t\t.mw_idx = 2,\n\t},\n};\n\nstatic const struct pci_device_id amd_ntb_pci_tbl[] = {\n\t{ PCI_VDEVICE(AMD, 0x145b), (kernel_ulong_t)&dev_data[0] },\n\t{ PCI_VDEVICE(AMD, 0x148b), (kernel_ulong_t)&dev_data[1] },\n\t{ PCI_VDEVICE(AMD, 0x14c0), (kernel_ulong_t)&dev_data[1] },\n\t{ PCI_VDEVICE(AMD, 0x14c3), (kernel_ulong_t)&dev_data[1] },\n\t{ PCI_VDEVICE(HYGON, 0x145b), (kernel_ulong_t)&dev_data[0] },\n\t{ 0, }\n};\nMODULE_DEVICE_TABLE(pci, amd_ntb_pci_tbl);\n\nstatic struct pci_driver amd_ntb_pci_driver = {\n\t.name\t\t= KBUILD_MODNAME,\n\t.id_table\t= amd_ntb_pci_tbl,\n\t.probe\t\t= amd_ntb_pci_probe,\n\t.remove\t\t= amd_ntb_pci_remove,\n\t.shutdown\t= amd_ntb_pci_shutdown,\n};\n\nstatic int __init amd_ntb_pci_driver_init(void)\n{\n\tint ret;\n\tpr_info(\"%s %s\\n\", NTB_DESC, NTB_VER);\n\n\tif (debugfs_initialized())\n\t\tdebugfs_dir = debugfs_create_dir(KBUILD_MODNAME, NULL);\n\n\tret = pci_register_driver(&amd_ntb_pci_driver);\n\tif (ret)\n\t\tdebugfs_remove_recursive(debugfs_dir);\n\n\treturn ret;\n}\nmodule_init(amd_ntb_pci_driver_init);\n\nstatic void __exit amd_ntb_pci_driver_exit(void)\n{\n\tpci_unregister_driver(&amd_ntb_pci_driver);\n\tdebugfs_remove_recursive(debugfs_dir);\n}\nmodule_exit(amd_ntb_pci_driver_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}