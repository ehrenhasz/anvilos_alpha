{
  "module_name": "sba_iommu.c",
  "hash_id": "107167478b0f130d92855ce5fa72f90d60d414fdeb863f8da1684eba2b3f80d5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/parisc/sba_iommu.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n\n#include <linux/mm.h>\n#include <linux/string.h>\n#include <linux/pci.h>\n#include <linux/dma-map-ops.h>\n#include <linux/scatterlist.h>\n#include <linux/iommu-helper.h>\n \n#include <linux/io-64-nonatomic-lo-hi.h>\n\n#include <asm/byteorder.h>\n#include <asm/io.h>\n#include <asm/dma.h>\t\t \n\n#include <asm/hardware.h>\t \n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/module.h>\n\n#include <asm/ropes.h>\n#include <asm/page.h>\t\t \n#include <asm/pdc.h>\t\t \n#include <asm/pdcpat.h>\t\t \n#include <asm/parisc-device.h>\n\n#include \"iommu.h\"\n\n#define MODULE_NAME \"SBA\"\n\n \n#undef DEBUG_SBA_INIT\n#undef DEBUG_SBA_RUN\n#undef DEBUG_SBA_RUN_SG\n#undef DEBUG_SBA_RESOURCE\n#undef ASSERT_PDIR_SANITY\n#undef DEBUG_LARGE_SG_ENTRIES\n#undef DEBUG_DMB_TRAP\n\n#ifdef DEBUG_SBA_INIT\n#define DBG_INIT(x...)\tprintk(x)\n#else\n#define DBG_INIT(x...)\n#endif\n\n#ifdef DEBUG_SBA_RUN\n#define DBG_RUN(x...)\tprintk(x)\n#else\n#define DBG_RUN(x...)\n#endif\n\n#ifdef DEBUG_SBA_RUN_SG\n#define DBG_RUN_SG(x...)\tprintk(x)\n#else\n#define DBG_RUN_SG(x...)\n#endif\n\n\n#ifdef DEBUG_SBA_RESOURCE\n#define DBG_RES(x...)\tprintk(x)\n#else\n#define DBG_RES(x...)\n#endif\n\n#define DEFAULT_DMA_HINT_REG\t0\n\nstruct sba_device *sba_list;\nEXPORT_SYMBOL_GPL(sba_list);\n\nstatic unsigned long ioc_needs_fdc = 0;\n\n \nstatic unsigned int global_ioc_cnt = 0;\n\n \nstatic unsigned long piranha_bad_128k = 0;\n\n \n#define SBA_DEV(d) ((struct sba_device *) (d))\n\n#ifdef CONFIG_AGP_PARISC\n#define SBA_AGP_SUPPORT\n#endif  \n\n#ifdef SBA_AGP_SUPPORT\nstatic int sba_reserve_agpgart = 1;\nmodule_param(sba_reserve_agpgart, int, 0444);\nMODULE_PARM_DESC(sba_reserve_agpgart, \"Reserve half of IO pdir as AGPGART\");\n#endif\n\nstatic struct proc_dir_entry *proc_runway_root __ro_after_init;\nstatic struct proc_dir_entry *proc_mckinley_root __ro_after_init;\n\n \n#define READ_REG32(addr)\treadl(addr)\n#define READ_REG64(addr)\treadq(addr)\n#define WRITE_REG32(val, addr)\twritel((val), (addr))\n#define WRITE_REG64(val, addr)\twriteq((val), (addr))\n\n#ifdef CONFIG_64BIT\n#define READ_REG(addr)\t\tREAD_REG64(addr)\n#define WRITE_REG(value, addr)\tWRITE_REG64(value, addr)\n#else\n#define READ_REG(addr)\t\tREAD_REG32(addr)\n#define WRITE_REG(value, addr)\tWRITE_REG32(value, addr)\n#endif\n\n#ifdef DEBUG_SBA_INIT\n\n \n\n \nstatic void\nsba_dump_ranges(void __iomem *hpa)\n{\n\tDBG_INIT(\"SBA at 0x%p\\n\", hpa);\n\tDBG_INIT(\"IOS_DIST_BASE   : %Lx\\n\", READ_REG64(hpa+IOS_DIST_BASE));\n\tDBG_INIT(\"IOS_DIST_MASK   : %Lx\\n\", READ_REG64(hpa+IOS_DIST_MASK));\n\tDBG_INIT(\"IOS_DIST_ROUTE  : %Lx\\n\", READ_REG64(hpa+IOS_DIST_ROUTE));\n\tDBG_INIT(\"\\n\");\n\tDBG_INIT(\"IOS_DIRECT_BASE : %Lx\\n\", READ_REG64(hpa+IOS_DIRECT_BASE));\n\tDBG_INIT(\"IOS_DIRECT_MASK : %Lx\\n\", READ_REG64(hpa+IOS_DIRECT_MASK));\n\tDBG_INIT(\"IOS_DIRECT_ROUTE: %Lx\\n\", READ_REG64(hpa+IOS_DIRECT_ROUTE));\n}\n\n \nstatic void sba_dump_tlb(void __iomem *hpa)\n{\n\tDBG_INIT(\"IO TLB at 0x%p\\n\", hpa);\n\tDBG_INIT(\"IOC_IBASE    : 0x%Lx\\n\", READ_REG64(hpa+IOC_IBASE));\n\tDBG_INIT(\"IOC_IMASK    : 0x%Lx\\n\", READ_REG64(hpa+IOC_IMASK));\n\tDBG_INIT(\"IOC_TCNFG    : 0x%Lx\\n\", READ_REG64(hpa+IOC_TCNFG));\n\tDBG_INIT(\"IOC_PDIR_BASE: 0x%Lx\\n\", READ_REG64(hpa+IOC_PDIR_BASE));\n\tDBG_INIT(\"\\n\");\n}\n#else\n#define sba_dump_ranges(x)\n#define sba_dump_tlb(x)\n#endif\t \n\n\n#ifdef ASSERT_PDIR_SANITY\n\n \nstatic void\nsba_dump_pdir_entry(struct ioc *ioc, char *msg, uint pide)\n{\n\t \n\t__le64 *ptr = &(ioc->pdir_base[pide & (~0U * BITS_PER_LONG)]);\n\tunsigned long *rptr = (unsigned long *) &(ioc->res_map[(pide >>3) & ~(sizeof(unsigned long) - 1)]);\n\tuint rcnt;\n\n\tprintk(KERN_DEBUG \"SBA: %s rp %p bit %d rval 0x%lx\\n\",\n\t\t msg,\n\t\t rptr, pide & (BITS_PER_LONG - 1), *rptr);\n\n\trcnt = 0;\n\twhile (rcnt < BITS_PER_LONG) {\n\t\tprintk(KERN_DEBUG \"%s %2d %p %016Lx\\n\",\n\t\t\t(rcnt == (pide & (BITS_PER_LONG - 1)))\n\t\t\t\t? \"    -->\" : \"       \",\n\t\t\trcnt, ptr, *ptr );\n\t\trcnt++;\n\t\tptr++;\n\t}\n\tprintk(KERN_DEBUG \"%s\", msg);\n}\n\n\n \nstatic int\nsba_check_pdir(struct ioc *ioc, char *msg)\n{\n\tu32 *rptr_end = (u32 *) &(ioc->res_map[ioc->res_size]);\n\tu32 *rptr = (u32 *) ioc->res_map;\t \n\tu64 *pptr = ioc->pdir_base;\t \n\tuint pide = 0;\n\n\twhile (rptr < rptr_end) {\n\t\tu32 rval = *rptr;\n\t\tint rcnt = 32;\t \n\n\t\twhile (rcnt) {\n\t\t\t \n\t\t\tu32 pde = ((u32) (((char *)pptr)[7])) << 24;\n\t\t\tif ((rval ^ pde) & 0x80000000)\n\t\t\t{\n\t\t\t\t \n\t\t\t\tsba_dump_pdir_entry(ioc, msg, pide);\n\t\t\t\treturn(1);\n\t\t\t}\n\t\t\trcnt--;\n\t\t\trval <<= 1;\t \n\t\t\tpptr++;\n\t\t\tpide++;\n\t\t}\n\t\trptr++;\t \n\t}\n\t \n\treturn 0;\n}\n\n\n \nstatic void\nsba_dump_sg( struct ioc *ioc, struct scatterlist *startsg, int nents)\n{\n\twhile (nents-- > 0) {\n\t\tprintk(KERN_DEBUG \" %d : %08lx/%05x %p/%05x\\n\",\n\t\t\t\tnents,\n\t\t\t\t(unsigned long) sg_dma_address(startsg),\n\t\t\t\tsg_dma_len(startsg),\n\t\t\t\tsg_virt(startsg), startsg->length);\n\t\tstartsg++;\n\t}\n}\n\n#endif  \n\n\n\n\n \n#define PAGES_PER_RANGE 1\t \n\n \n\n#ifdef ZX1_SUPPORT\n \n#define SBA_IOVA(ioc,iovp,offset,hint_reg) ((ioc->ibase) | (iovp) | (offset))\n#define SBA_IOVP(ioc,iova) ((iova) & (ioc)->iovp_mask)\n#else\n \n#define SBA_IOVA(ioc,iovp,offset,hint_reg) ((iovp) | (offset))\n#define SBA_IOVP(ioc,iova) (iova)\n#endif\n\n#define PDIR_INDEX(iovp)   ((iovp)>>IOVP_SHIFT)\n\n#define RESMAP_MASK(n)    (~0UL << (BITS_PER_LONG - (n)))\n#define RESMAP_IDX_MASK   (sizeof(unsigned long) - 1)\n\nstatic unsigned long ptr_to_pide(struct ioc *ioc, unsigned long *res_ptr,\n\t\t\t\t unsigned int bitshiftcnt)\n{\n\treturn (((unsigned long)res_ptr - (unsigned long)ioc->res_map) << 3)\n\t\t+ bitshiftcnt;\n}\n\n \nstatic unsigned long\nsba_search_bitmap(struct ioc *ioc, struct device *dev,\n\t\t  unsigned long bits_wanted)\n{\n\tunsigned long *res_ptr = ioc->res_hint;\n\tunsigned long *res_end = (unsigned long *) &(ioc->res_map[ioc->res_size]);\n\tunsigned long pide = ~0UL, tpide;\n\tunsigned long boundary_size;\n\tunsigned long shift;\n\tint ret;\n\n\tboundary_size = dma_get_seg_boundary_nr_pages(dev, IOVP_SHIFT);\n\n#if defined(ZX1_SUPPORT)\n\tBUG_ON(ioc->ibase & ~IOVP_MASK);\n\tshift = ioc->ibase >> IOVP_SHIFT;\n#else\n\tshift = 0;\n#endif\n\n\tif (bits_wanted > (BITS_PER_LONG/2)) {\n\t\t \n\t\tfor(; res_ptr < res_end; ++res_ptr) {\n\t\t\ttpide = ptr_to_pide(ioc, res_ptr, 0);\n\t\t\tret = iommu_is_span_boundary(tpide, bits_wanted,\n\t\t\t\t\t\t     shift,\n\t\t\t\t\t\t     boundary_size);\n\t\t\tif ((*res_ptr == 0) && !ret) {\n\t\t\t\t*res_ptr = RESMAP_MASK(bits_wanted);\n\t\t\t\tpide = tpide;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t \n\t\tres_ptr++;\n\t\tioc->res_bitshift = 0;\n\t} else {\n\t\t \n\t\tunsigned long o = 1 << get_order(bits_wanted << PAGE_SHIFT);\n\t\tuint bitshiftcnt = ALIGN(ioc->res_bitshift, o);\n\t\tunsigned long mask;\n\n\t\tif (bitshiftcnt >= BITS_PER_LONG) {\n\t\t\tbitshiftcnt = 0;\n\t\t\tres_ptr++;\n\t\t}\n\t\tmask = RESMAP_MASK(bits_wanted) >> bitshiftcnt;\n\n\t\tDBG_RES(\"%s() o %ld %p\", __func__, o, res_ptr);\n\t\twhile(res_ptr < res_end)\n\t\t{ \n\t\t\tDBG_RES(\"    %p %lx %lx\\n\", res_ptr, mask, *res_ptr);\n\t\t\tWARN_ON(mask == 0);\n\t\t\ttpide = ptr_to_pide(ioc, res_ptr, bitshiftcnt);\n\t\t\tret = iommu_is_span_boundary(tpide, bits_wanted,\n\t\t\t\t\t\t     shift,\n\t\t\t\t\t\t     boundary_size);\n\t\t\tif ((((*res_ptr) & mask) == 0) && !ret) {\n\t\t\t\t*res_ptr |= mask;      \n\t\t\t\tpide = tpide;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmask >>= o;\n\t\t\tbitshiftcnt += o;\n\t\t\tif (mask == 0) {\n\t\t\t\tmask = RESMAP_MASK(bits_wanted);\n\t\t\t\tbitshiftcnt=0;\n\t\t\t\tres_ptr++;\n\t\t\t}\n\t\t}\n\t\t \n\t\tioc->res_bitshift = bitshiftcnt + bits_wanted;\n\t}\n\n\t \n\tif (res_end <= res_ptr) {\n\t\tioc->res_hint = (unsigned long *) ioc->res_map;\n\t\tioc->res_bitshift = 0;\n\t} else {\n\t\tioc->res_hint = res_ptr;\n\t}\n\treturn (pide);\n}\n\n\n \nstatic int\nsba_alloc_range(struct ioc *ioc, struct device *dev, size_t size)\n{\n\tunsigned int pages_needed = size >> IOVP_SHIFT;\n#ifdef SBA_COLLECT_STATS\n\tunsigned long cr_start = mfctl(16);\n#endif\n\tunsigned long pide;\n\n\tpide = sba_search_bitmap(ioc, dev, pages_needed);\n\tif (pide >= (ioc->res_size << 3)) {\n\t\tpide = sba_search_bitmap(ioc, dev, pages_needed);\n\t\tif (pide >= (ioc->res_size << 3))\n\t\t\tpanic(\"%s: I/O MMU @ %p is out of mapping resources\\n\",\n\t\t\t      __FILE__, ioc->ioc_hpa);\n\t}\n\n#ifdef ASSERT_PDIR_SANITY\n\t \n\tif(0x00 != ((u8 *) ioc->pdir_base)[pide*sizeof(u64) + 7]) {\n\t\tsba_dump_pdir_entry(ioc, \"sba_search_bitmap() botched it?\", pide);\n\t}\n#endif\n\n\tDBG_RES(\"%s(%x) %d -> %lx hint %x/%x\\n\",\n\t\t__func__, size, pages_needed, pide,\n\t\t(uint) ((unsigned long) ioc->res_hint - (unsigned long) ioc->res_map),\n\t\tioc->res_bitshift );\n\n#ifdef SBA_COLLECT_STATS\n\t{\n\t\tunsigned long cr_end = mfctl(16);\n\t\tunsigned long tmp = cr_end - cr_start;\n\t\t \n\t\tcr_start = (cr_end < cr_start) ?  -(tmp) : (tmp);\n\t}\n\tioc->avg_search[ioc->avg_idx++] = cr_start;\n\tioc->avg_idx &= SBA_SEARCH_SAMPLE - 1;\n\n\tioc->used_pages += pages_needed;\n#endif\n\n\treturn (pide);\n}\n\n\n \nstatic void\nsba_free_range(struct ioc *ioc, dma_addr_t iova, size_t size)\n{\n\tunsigned long iovp = SBA_IOVP(ioc, iova);\n\tunsigned int pide = PDIR_INDEX(iovp);\n\tunsigned int ridx = pide >> 3;\t \n\tunsigned long *res_ptr = (unsigned long *) &((ioc)->res_map[ridx & ~RESMAP_IDX_MASK]);\n\n\tint bits_not_wanted = size >> IOVP_SHIFT;\n\n\t \n\tunsigned long m = RESMAP_MASK(bits_not_wanted) >> (pide & (BITS_PER_LONG - 1));\n\n\tDBG_RES(\"%s( ,%x,%x) %x/%lx %x %p %lx\\n\",\n\t\t__func__, (uint) iova, size,\n\t\tbits_not_wanted, m, pide, res_ptr, *res_ptr);\n\n#ifdef SBA_COLLECT_STATS\n\tioc->used_pages -= bits_not_wanted;\n#endif\n\n\t*res_ptr &= ~m;\n}\n\n\n \n\n#ifdef SBA_HINT_SUPPORT\n#define SBA_DMA_HINT(ioc, val) ((val) << (ioc)->hint_shift_pdir)\n#endif\n\ntypedef unsigned long space_t;\n#define KERNEL_SPACE 0\n\n \n\nstatic void\nsba_io_pdir_entry(__le64 *pdir_ptr, space_t sid, unsigned long vba,\n\t\t  unsigned long hint)\n{\n\tu64 pa;  \n\tregister unsigned ci;  \n\n\tpa = lpa(vba);\n\tpa &= IOVP_MASK;\n\n\tasm(\"lci 0(%1), %0\" : \"=r\" (ci) : \"r\" (vba));\n\tpa |= (ci >> PAGE_SHIFT) & 0xff;   \n\n\tpa |= SBA_PDIR_VALID_BIT;\t \n\t*pdir_ptr = cpu_to_le64(pa);\t \n\n\t \n\tasm_io_fdc(pdir_ptr);\n}\n\n\n \nstatic void\nsba_mark_invalid(struct ioc *ioc, dma_addr_t iova, size_t byte_cnt)\n{\n\tu32 iovp = (u32) SBA_IOVP(ioc,iova);\n\t__le64 *pdir_ptr = &ioc->pdir_base[PDIR_INDEX(iovp)];\n\n#ifdef ASSERT_PDIR_SANITY\n\t \n\tif (0x80 != (((u8 *) pdir_ptr)[7])) {\n\t\tsba_dump_pdir_entry(ioc,\"sba_mark_invalid()\", PDIR_INDEX(iovp));\n\t}\n#endif\n\n\tif (byte_cnt > IOVP_SIZE)\n\t{\n#if 0\n\t\tunsigned long entries_per_cacheline = ioc_needs_fdc ?\n\t\t\t\tL1_CACHE_ALIGN(((unsigned long) pdir_ptr))\n\t\t\t\t\t- (unsigned long) pdir_ptr;\n\t\t\t\t: 262144;\n#endif\n\n\t\t \n\t\tiovp |= get_order(byte_cnt) + PAGE_SHIFT;\n\n\t\tdo {\n\t\t\t \n\t\t\t((u8 *) pdir_ptr)[7] = 0;\n\t\t\tasm_io_fdc(pdir_ptr);\n\t\t\tif (ioc_needs_fdc) {\n#if 0\n\t\t\t\tentries_per_cacheline = L1_CACHE_SHIFT - 3;\n#endif\n\t\t\t}\n\t\t\tpdir_ptr++;\n\t\t\tbyte_cnt -= IOVP_SIZE;\n\t\t} while (byte_cnt > IOVP_SIZE);\n\t} else\n\t\tiovp |= IOVP_SHIFT;      \n\n\t \n\t((u8 *) pdir_ptr)[7] = 0;\n\tasm_io_fdc(pdir_ptr);\n\n\tWRITE_REG( SBA_IOVA(ioc, iovp, 0, 0), ioc->ioc_hpa+IOC_PCOM);\n}\n\n \nstatic int sba_dma_supported( struct device *dev, u64 mask)\n{\n\tstruct ioc *ioc;\n\n\tif (dev == NULL) {\n\t\tprintk(KERN_ERR MODULE_NAME \": EISA/ISA/et al not supported\\n\");\n\t\tBUG();\n\t\treturn(0);\n\t}\n\n\tioc = GET_IOC(dev);\n\tif (!ioc)\n\t\treturn 0;\n\n\t \n\treturn((int)(mask >= (ioc->ibase - 1 +\n\t\t\t(ioc->pdir_size / sizeof(u64) * IOVP_SIZE) )));\n}\n\n\n \nstatic dma_addr_t\nsba_map_single(struct device *dev, void *addr, size_t size,\n\t       enum dma_data_direction direction)\n{\n\tstruct ioc *ioc;\n\tunsigned long flags; \n\tdma_addr_t iovp;\n\tdma_addr_t offset;\n\t__le64 *pdir_start;\n\tint pide;\n\n\tioc = GET_IOC(dev);\n\tif (!ioc)\n\t\treturn DMA_MAPPING_ERROR;\n\n\t \n\toffset = ((dma_addr_t) (long) addr) & ~IOVP_MASK;\n\n\t \n\tsize = (size + offset + ~IOVP_MASK) & IOVP_MASK;\n\n\tspin_lock_irqsave(&ioc->res_lock, flags);\n#ifdef ASSERT_PDIR_SANITY\n\tsba_check_pdir(ioc,\"Check before sba_map_single()\");\n#endif\n\n#ifdef SBA_COLLECT_STATS\n\tioc->msingle_calls++;\n\tioc->msingle_pages += size >> IOVP_SHIFT;\n#endif\n\tpide = sba_alloc_range(ioc, dev, size);\n\tiovp = (dma_addr_t) pide << IOVP_SHIFT;\n\n\tDBG_RUN(\"%s() 0x%p -> 0x%lx\\n\",\n\t\t__func__, addr, (long) iovp | offset);\n\n\tpdir_start = &(ioc->pdir_base[pide]);\n\n\twhile (size > 0) {\n\t\tsba_io_pdir_entry(pdir_start, KERNEL_SPACE, (unsigned long) addr, 0);\n\n\t\tDBG_RUN(\"\tpdir 0x%p %02x%02x%02x%02x%02x%02x%02x%02x\\n\",\n\t\t\tpdir_start,\n\t\t\t(u8) (((u8 *) pdir_start)[7]),\n\t\t\t(u8) (((u8 *) pdir_start)[6]),\n\t\t\t(u8) (((u8 *) pdir_start)[5]),\n\t\t\t(u8) (((u8 *) pdir_start)[4]),\n\t\t\t(u8) (((u8 *) pdir_start)[3]),\n\t\t\t(u8) (((u8 *) pdir_start)[2]),\n\t\t\t(u8) (((u8 *) pdir_start)[1]),\n\t\t\t(u8) (((u8 *) pdir_start)[0])\n\t\t\t);\n\n\t\taddr += IOVP_SIZE;\n\t\tsize -= IOVP_SIZE;\n\t\tpdir_start++;\n\t}\n\n\t \n\tasm_io_sync();\n\n#ifdef ASSERT_PDIR_SANITY\n\tsba_check_pdir(ioc,\"Check after sba_map_single()\");\n#endif\n\tspin_unlock_irqrestore(&ioc->res_lock, flags);\n\n\t \n\treturn SBA_IOVA(ioc, iovp, offset, DEFAULT_DMA_HINT_REG);\n}\n\n\nstatic dma_addr_t\nsba_map_page(struct device *dev, struct page *page, unsigned long offset,\n\t\tsize_t size, enum dma_data_direction direction,\n\t\tunsigned long attrs)\n{\n\treturn sba_map_single(dev, page_address(page) + offset, size,\n\t\t\tdirection);\n}\n\n\n \nstatic void\nsba_unmap_page(struct device *dev, dma_addr_t iova, size_t size,\n\t\tenum dma_data_direction direction, unsigned long attrs)\n{\n\tstruct ioc *ioc;\n#if DELAYED_RESOURCE_CNT > 0\n\tstruct sba_dma_pair *d;\n#endif\n\tunsigned long flags; \n\tdma_addr_t offset;\n\n\tDBG_RUN(\"%s() iovp 0x%lx/%x\\n\", __func__, (long) iova, size);\n\n\tioc = GET_IOC(dev);\n\tif (!ioc) {\n\t\tWARN_ON(!ioc);\n\t\treturn;\n\t}\n\toffset = iova & ~IOVP_MASK;\n\tiova ^= offset;         \n\tsize += offset;\n\tsize = ALIGN(size, IOVP_SIZE);\n\n\tspin_lock_irqsave(&ioc->res_lock, flags);\n\n#ifdef SBA_COLLECT_STATS\n\tioc->usingle_calls++;\n\tioc->usingle_pages += size >> IOVP_SHIFT;\n#endif\n\n\tsba_mark_invalid(ioc, iova, size);\n\n#if DELAYED_RESOURCE_CNT > 0\n\t \n\td = &(ioc->saved[ioc->saved_cnt]);\n\td->iova = iova;\n\td->size = size;\n\tif (++(ioc->saved_cnt) >= DELAYED_RESOURCE_CNT) {\n\t\tint cnt = ioc->saved_cnt;\n\t\twhile (cnt--) {\n\t\t\tsba_free_range(ioc, d->iova, d->size);\n\t\t\td--;\n\t\t}\n\t\tioc->saved_cnt = 0;\n\n\t\tREAD_REG(ioc->ioc_hpa+IOC_PCOM);\t \n\t}\n#else  \n\tsba_free_range(ioc, iova, size);\n\n\t \n\tasm_io_sync();\n\n\tREAD_REG(ioc->ioc_hpa+IOC_PCOM);\t \n#endif  \n\n\tspin_unlock_irqrestore(&ioc->res_lock, flags);\n\n\t \n}\n\n\n \nstatic void *sba_alloc(struct device *hwdev, size_t size, dma_addr_t *dma_handle,\n\t\tgfp_t gfp, unsigned long attrs)\n{\n\tvoid *ret;\n\n\tif (!hwdev) {\n\t\t \n\t\t*dma_handle = 0;\n\t\treturn NULL;\n\t}\n\n        ret = (void *) __get_free_pages(gfp, get_order(size));\n\n\tif (ret) {\n\t\tmemset(ret, 0, size);\n\t\t*dma_handle = sba_map_single(hwdev, ret, size, 0);\n\t}\n\n\treturn ret;\n}\n\n\n \nstatic void\nsba_free(struct device *hwdev, size_t size, void *vaddr,\n\t\t    dma_addr_t dma_handle, unsigned long attrs)\n{\n\tsba_unmap_page(hwdev, dma_handle, size, 0, 0);\n\tfree_pages((unsigned long) vaddr, get_order(size));\n}\n\n\n \n#define PIDE_FLAG 0x80000000UL\n\n#ifdef SBA_COLLECT_STATS\n#define IOMMU_MAP_STATS\n#endif\n#include \"iommu-helpers.h\"\n\n#ifdef DEBUG_LARGE_SG_ENTRIES\nint dump_run_sg = 0;\n#endif\n\n\n \nstatic int\nsba_map_sg(struct device *dev, struct scatterlist *sglist, int nents,\n\t   enum dma_data_direction direction, unsigned long attrs)\n{\n\tstruct ioc *ioc;\n\tint filled = 0;\n\tunsigned long flags;\n\n\tDBG_RUN_SG(\"%s() START %d entries\\n\", __func__, nents);\n\n\tioc = GET_IOC(dev);\n\tif (!ioc)\n\t\treturn -EINVAL;\n\n\t \n\tif (nents == 1) {\n\t\tsg_dma_address(sglist) = sba_map_single(dev, sg_virt(sglist),\n\t\t\t\t\t\tsglist->length, direction);\n\t\tsg_dma_len(sglist)     = sglist->length;\n\t\treturn 1;\n\t}\n\n\tspin_lock_irqsave(&ioc->res_lock, flags);\n\n#ifdef ASSERT_PDIR_SANITY\n\tif (sba_check_pdir(ioc,\"Check before sba_map_sg()\"))\n\t{\n\t\tsba_dump_sg(ioc, sglist, nents);\n\t\tpanic(\"Check before sba_map_sg()\");\n\t}\n#endif\n\n#ifdef SBA_COLLECT_STATS\n\tioc->msg_calls++;\n#endif\n\n\t \n\tiommu_coalesce_chunks(ioc, dev, sglist, nents, sba_alloc_range);\n\n\t \n\tfilled = iommu_fill_pdir(ioc, sglist, nents, 0, sba_io_pdir_entry);\n\n\t \n\tasm_io_sync();\n\n#ifdef ASSERT_PDIR_SANITY\n\tif (sba_check_pdir(ioc,\"Check after sba_map_sg()\"))\n\t{\n\t\tsba_dump_sg(ioc, sglist, nents);\n\t\tpanic(\"Check after sba_map_sg()\\n\");\n\t}\n#endif\n\n\tspin_unlock_irqrestore(&ioc->res_lock, flags);\n\n\tDBG_RUN_SG(\"%s() DONE %d mappings\\n\", __func__, filled);\n\n\treturn filled;\n}\n\n\n \nstatic void \nsba_unmap_sg(struct device *dev, struct scatterlist *sglist, int nents,\n\t     enum dma_data_direction direction, unsigned long attrs)\n{\n\tstruct ioc *ioc;\n#ifdef ASSERT_PDIR_SANITY\n\tunsigned long flags;\n#endif\n\n\tDBG_RUN_SG(\"%s() START %d entries,  %p,%x\\n\",\n\t\t__func__, nents, sg_virt(sglist), sglist->length);\n\n\tioc = GET_IOC(dev);\n\tif (!ioc) {\n\t\tWARN_ON(!ioc);\n\t\treturn;\n\t}\n\n#ifdef SBA_COLLECT_STATS\n\tioc->usg_calls++;\n#endif\n\n#ifdef ASSERT_PDIR_SANITY\n\tspin_lock_irqsave(&ioc->res_lock, flags);\n\tsba_check_pdir(ioc,\"Check before sba_unmap_sg()\");\n\tspin_unlock_irqrestore(&ioc->res_lock, flags);\n#endif\n\n\twhile (nents && sg_dma_len(sglist)) {\n\n\t\tsba_unmap_page(dev, sg_dma_address(sglist), sg_dma_len(sglist),\n\t\t\t\tdirection, 0);\n#ifdef SBA_COLLECT_STATS\n\t\tioc->usg_pages += ((sg_dma_address(sglist) & ~IOVP_MASK) + sg_dma_len(sglist) + IOVP_SIZE - 1) >> PAGE_SHIFT;\n\t\tioc->usingle_calls--;\t \n#endif\n\t\t++sglist;\n\t\tnents--;\n\t}\n\n\tDBG_RUN_SG(\"%s() DONE (nents %d)\\n\", __func__,  nents);\n\n#ifdef ASSERT_PDIR_SANITY\n\tspin_lock_irqsave(&ioc->res_lock, flags);\n\tsba_check_pdir(ioc,\"Check after sba_unmap_sg()\");\n\tspin_unlock_irqrestore(&ioc->res_lock, flags);\n#endif\n\n}\n\nstatic const struct dma_map_ops sba_ops = {\n\t.dma_supported =\tsba_dma_supported,\n\t.alloc =\t\tsba_alloc,\n\t.free =\t\t\tsba_free,\n\t.map_page =\t\tsba_map_page,\n\t.unmap_page =\t\tsba_unmap_page,\n\t.map_sg =\t\tsba_map_sg,\n\t.unmap_sg =\t\tsba_unmap_sg,\n\t.get_sgtable =\t\tdma_common_get_sgtable,\n\t.alloc_pages =\t\tdma_common_alloc_pages,\n\t.free_pages =\t\tdma_common_free_pages,\n};\n\n\n \n\nstatic void\nsba_get_pat_resources(struct sba_device *sba_dev)\n{\n#if 0\n \nPAT_MOD(mod)->mod_info.mod_pages   = PAT_GET_MOD_PAGES(temp);\n\tFIXME : ???\nPAT_MOD(mod)->mod_info.dvi         = PAT_GET_DVI(temp);\n\tTells where the dvi bits are located in the address.\nPAT_MOD(mod)->mod_info.ioc         = PAT_GET_IOC(temp);\n\tFIXME : ???\n#endif\n}\n\n\n \n#define PIRANHA_ADDR_MASK\t0x00160000UL  \n#define PIRANHA_ADDR_VAL\t0x00060000UL  \nstatic void *\nsba_alloc_pdir(unsigned int pdir_size)\n{\n        unsigned long pdir_base;\n\tunsigned long pdir_order = get_order(pdir_size);\n\n\tpdir_base = __get_free_pages(GFP_KERNEL, pdir_order);\n\tif (NULL == (void *) pdir_base)\t{\n\t\tpanic(\"%s() could not allocate I/O Page Table\\n\",\n\t\t\t__func__);\n\t}\n\n\t \n\tif ( ((boot_cpu_data.pdc.cpuid >> 5) & 0x7f) != 0x13\n\t\t\t|| (boot_cpu_data.pdc.versions > 0x202)\n\t\t\t|| (boot_cpu_data.pdc.capabilities & 0x08L) )\n\t\treturn (void *) pdir_base;\n\n\t \n\tif (pdir_order <= (19-12)) {\n\t\tif (((virt_to_phys(pdir_base)+pdir_size-1) & PIRANHA_ADDR_MASK) == PIRANHA_ADDR_VAL) {\n\t\t\t \n\t\t\tunsigned long new_pdir = __get_free_pages(GFP_KERNEL, (19-12));\n\t\t\t \n\t\t\tfree_pages(pdir_base, pdir_order);\n\n\t\t\tpdir_base = new_pdir;\n\n\t\t\t \n\t\t\twhile (pdir_order < (19-12)) {\n\t\t\t\tnew_pdir += pdir_size;\n\t\t\t\tfree_pages(new_pdir, pdir_order);\n\t\t\t\tpdir_order +=1;\n\t\t\t\tpdir_size <<=1;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tunsigned long new_pdir = __get_free_pages(GFP_KERNEL, pdir_order+1);  \n\n\t\t \n\t\tfree_pages( pdir_base, pdir_order);\n\n\t\t \n\t\tfree_pages(new_pdir, 20-12);\n\n\t\tpdir_base = new_pdir + 1024*1024;\n\n\t\tif (pdir_order > (20-12)) {\n\t\t\t \n\t\t\tpiranha_bad_128k = 1;\n\n\t\t\tnew_pdir += 3*1024*1024;\n\t\t\t \n\t\t\tfree_pages(new_pdir, 20-12);\n\n\t\t\t \n\t\t\tfree_pages(new_pdir - 128*1024 , 17-12);\n\n\t\t\tpdir_size -= 128*1024;\n\t\t}\n\t}\n\n\tmemset((void *) pdir_base, 0, pdir_size);\n\treturn (void *) pdir_base;\n}\n\nstruct ibase_data_struct {\n\tstruct ioc *ioc;\n\tint ioc_num;\n};\n\nstatic int setup_ibase_imask_callback(struct device *dev, void *data)\n{\n\tstruct parisc_device *lba = to_parisc_device(dev);\n\tstruct ibase_data_struct *ibd = data;\n\tint rope_num = (lba->hpa.start >> 13) & 0xf;\n\tif (rope_num >> 3 == ibd->ioc_num)\n\t\tlba_set_iregs(lba, ibd->ioc->ibase, ibd->ioc->imask);\n\treturn 0;\n}\n\n \nstatic void \nsetup_ibase_imask(struct parisc_device *sba, struct ioc *ioc, int ioc_num)\n{\n\tstruct ibase_data_struct ibase_data = {\n\t\t.ioc\t\t= ioc,\n\t\t.ioc_num\t= ioc_num,\n\t};\n\n\tdevice_for_each_child(&sba->dev, &ibase_data,\n\t\t\t      setup_ibase_imask_callback);\n}\n\n#ifdef SBA_AGP_SUPPORT\nstatic int\nsba_ioc_find_quicksilver(struct device *dev, void *data)\n{\n\tint *agp_found = data;\n\tstruct parisc_device *lba = to_parisc_device(dev);\n\n\tif (IS_QUICKSILVER(lba))\n\t\t*agp_found = 1;\n\treturn 0;\n}\n#endif\n\nstatic void\nsba_ioc_init_pluto(struct parisc_device *sba, struct ioc *ioc, int ioc_num)\n{\n\tu32 iova_space_mask;\n\tu32 iova_space_size;\n\tint iov_order, tcnfg;\n#ifdef SBA_AGP_SUPPORT\n\tint agp_found = 0;\n#endif\n\t \n\tioc->ibase = READ_REG(ioc->ioc_hpa + IOC_IBASE) & ~0x1fffffULL;\n\tiova_space_size = ~(READ_REG(ioc->ioc_hpa + IOC_IMASK) & 0xFFFFFFFFUL) + 1;\n\n\tif ((ioc->ibase < 0xfed00000UL) && ((ioc->ibase + iova_space_size) > 0xfee00000UL)) {\n\t\tprintk(\"WARNING: IOV space overlaps local config and interrupt message, truncating\\n\");\n\t\tiova_space_size /= 2;\n\t}\n\n\t \n\tiov_order = get_order(iova_space_size >> (IOVP_SHIFT - PAGE_SHIFT));\n\tioc->pdir_size = (iova_space_size / IOVP_SIZE) * sizeof(u64);\n\n\tDBG_INIT(\"%s() hpa 0x%p IOV %dMB (%d bits)\\n\",\n\t\t__func__, ioc->ioc_hpa, iova_space_size >> 20,\n\t\tiov_order + PAGE_SHIFT);\n\n\tioc->pdir_base = (void *) __get_free_pages(GFP_KERNEL,\n\t\t\t\t\t\t   get_order(ioc->pdir_size));\n\tif (!ioc->pdir_base)\n\t\tpanic(\"Couldn't allocate I/O Page Table\\n\");\n\n\tmemset(ioc->pdir_base, 0, ioc->pdir_size);\n\n\tDBG_INIT(\"%s() pdir %p size %x\\n\",\n\t\t\t__func__, ioc->pdir_base, ioc->pdir_size);\n\n#ifdef SBA_HINT_SUPPORT\n\tioc->hint_shift_pdir = iov_order + PAGE_SHIFT;\n\tioc->hint_mask_pdir = ~(0x3 << (iov_order + PAGE_SHIFT));\n\n\tDBG_INIT(\"\thint_shift_pdir %x hint_mask_pdir %lx\\n\",\n\t\tioc->hint_shift_pdir, ioc->hint_mask_pdir);\n#endif\n\n\tWARN_ON((((unsigned long) ioc->pdir_base) & PAGE_MASK) != (unsigned long) ioc->pdir_base);\n\tWRITE_REG(virt_to_phys(ioc->pdir_base), ioc->ioc_hpa + IOC_PDIR_BASE);\n\n\t \n\tiova_space_mask =  0xffffffff;\n\tiova_space_mask <<= (iov_order + PAGE_SHIFT);\n\tioc->imask = iova_space_mask;\n#ifdef ZX1_SUPPORT\n\tioc->iovp_mask = ~(iova_space_mask + PAGE_SIZE - 1);\n#endif\n\tsba_dump_tlb(ioc->ioc_hpa);\n\n\tsetup_ibase_imask(sba, ioc, ioc_num);\n\n\tWRITE_REG(ioc->imask, ioc->ioc_hpa + IOC_IMASK);\n\n#ifdef CONFIG_64BIT\n\t \n\tioc->imask |= 0xFFFFFFFF00000000UL;\n#endif\n\n\t \n\tswitch (PAGE_SHIFT) {\n\t\tcase 12: tcnfg = 0; break;\t \n\t\tcase 13: tcnfg = 1; break;\t \n\t\tcase 14: tcnfg = 2; break;\t \n\t\tcase 16: tcnfg = 3; break;\t \n\t\tdefault:\n\t\t\tpanic(__FILE__ \"Unsupported system page size %d\",\n\t\t\t\t1 << PAGE_SHIFT);\n\t\t\tbreak;\n\t}\n\tWRITE_REG(tcnfg, ioc->ioc_hpa + IOC_TCNFG);\n\n\t \n\tWRITE_REG(ioc->ibase | 1, ioc->ioc_hpa + IOC_IBASE);\n\n\t \n\tWRITE_REG(ioc->ibase | 31, ioc->ioc_hpa + IOC_PCOM);\n\n#ifdef SBA_AGP_SUPPORT\n\n\t \n\tdevice_for_each_child(&sba->dev, &agp_found, sba_ioc_find_quicksilver);\n\n\tif (agp_found && sba_reserve_agpgart) {\n\t\tprintk(KERN_INFO \"%s: reserving %dMb of IOVA space for agpgart\\n\",\n\t\t       __func__, (iova_space_size/2) >> 20);\n\t\tioc->pdir_size /= 2;\n\t\tioc->pdir_base[PDIR_INDEX(iova_space_size/2)] = SBA_AGPGART_COOKIE;\n\t}\n#endif  \n}\n\nstatic void\nsba_ioc_init(struct parisc_device *sba, struct ioc *ioc, int ioc_num)\n{\n\tu32 iova_space_size, iova_space_mask;\n\tunsigned int pdir_size, iov_order, tcnfg;\n\n\t \n\n\tiova_space_size = (u32) (totalram_pages()/global_ioc_cnt);\n\n\t \n\tif (iova_space_size < (1 << (20 - PAGE_SHIFT))) {\n\t\tiova_space_size = 1 << (20 - PAGE_SHIFT);\n\t}\n\telse if (iova_space_size > (1 << (30 - PAGE_SHIFT))) {\n\t\tiova_space_size = 1 << (30 - PAGE_SHIFT);\n\t}\n\n\t \n\tiov_order = get_order(iova_space_size << PAGE_SHIFT);\n\n\t \n\tiova_space_size = 1 << (iov_order + PAGE_SHIFT);\n\n\tioc->pdir_size = pdir_size = (iova_space_size/IOVP_SIZE) * sizeof(u64);\n\n\tDBG_INIT(\"%s() hpa %px mem %ldMB IOV %dMB (%d bits)\\n\",\n\t\t\t__func__,\n\t\t\tioc->ioc_hpa,\n\t\t\t(unsigned long) totalram_pages() >> (20 - PAGE_SHIFT),\n\t\t\tiova_space_size>>20,\n\t\t\tiov_order + PAGE_SHIFT);\n\n\tioc->pdir_base = sba_alloc_pdir(pdir_size);\n\n\tDBG_INIT(\"%s() pdir %p size %x\\n\",\n\t\t\t__func__, ioc->pdir_base, pdir_size);\n\n#ifdef SBA_HINT_SUPPORT\n\t \n\tioc->hint_shift_pdir = iov_order + PAGE_SHIFT;\n\tioc->hint_mask_pdir = ~(0x3 << (iov_order + PAGE_SHIFT));\n\n\tDBG_INIT(\"\thint_shift_pdir %x hint_mask_pdir %lx\\n\",\n\t\t\tioc->hint_shift_pdir, ioc->hint_mask_pdir);\n#endif\n\n\tWRITE_REG64(virt_to_phys(ioc->pdir_base), ioc->ioc_hpa + IOC_PDIR_BASE);\n\n\t \n\tiova_space_mask =  0xffffffff;\n\tiova_space_mask <<= (iov_order + PAGE_SHIFT);\n\n\t \n\tioc->ibase = 0;\n\tioc->imask = iova_space_mask;\t \n#ifdef ZX1_SUPPORT\n\tioc->iovp_mask = ~(iova_space_mask + PAGE_SIZE - 1);\n#endif\n\n\tDBG_INIT(\"%s() IOV base %#lx mask %#0lx\\n\",\n\t\t__func__, ioc->ibase, ioc->imask);\n\n\t \n\n\tsetup_ibase_imask(sba, ioc, ioc_num);\n\n\t \n\tWRITE_REG(ioc->ibase | 1, ioc->ioc_hpa+IOC_IBASE);\n\tWRITE_REG(ioc->imask, ioc->ioc_hpa+IOC_IMASK);\n\n\t \n\tswitch (PAGE_SHIFT) {\n\t\tcase 12: tcnfg = 0; break;\t \n\t\tcase 13: tcnfg = 1; break;\t \n\t\tcase 14: tcnfg = 2; break;\t \n\t\tcase 16: tcnfg = 3; break;\t \n\t\tdefault:\n\t\t\tpanic(__FILE__ \"Unsupported system page size %d\",\n\t\t\t\t1 << PAGE_SHIFT);\n\t\t\tbreak;\n\t}\n\t \n\tWRITE_REG(tcnfg, ioc->ioc_hpa+IOC_TCNFG);\n\n\t \n\tWRITE_REG(0 | 31, ioc->ioc_hpa+IOC_PCOM);\n\n\tioc->ibase = 0;  \t\n\n\tDBG_INIT(\"%s() DONE\\n\", __func__);\n}\n\n\n\n \n\nstatic void __iomem *ioc_remap(struct sba_device *sba_dev, unsigned int offset)\n{\n\treturn ioremap(sba_dev->dev->hpa.start + offset, SBA_FUNC_SIZE);\n}\n\nstatic void sba_hw_init(struct sba_device *sba_dev)\n{ \n\tint i;\n\tint num_ioc;\n\tu64 ioc_ctl;\n\n\tif (!is_pdc_pat()) {\n\t\t \n\t\tif (PAGE0->mem_kbd.cl_class == CL_KEYBD) {\n\t\t\tpdc_io_reset_devices();\n\t\t}\n\n\t}\n\n\n#if 0\nprintk(\"sba_hw_init(): mem_boot 0x%x 0x%x 0x%x 0x%x\\n\", PAGE0->mem_boot.hpa,\n\tPAGE0->mem_boot.spa, PAGE0->mem_boot.pad, PAGE0->mem_boot.cl_class);\n\n\t \n\tif ((PAGE0->mem_boot.cl_class != CL_RANDOM)\n\t\t&& (PAGE0->mem_boot.cl_class != CL_SEQU)) {\n\t\t\tpdc_io_reset();\n\t}\n#endif\n\n\tif (!IS_PLUTO(sba_dev->dev)) {\n\t\tioc_ctl = READ_REG(sba_dev->sba_hpa+IOC_CTRL);\n\t\tDBG_INIT(\"%s() hpa %px ioc_ctl 0x%Lx ->\",\n\t\t\t__func__, sba_dev->sba_hpa, ioc_ctl);\n\t\tioc_ctl &= ~(IOC_CTRL_RM | IOC_CTRL_NC | IOC_CTRL_CE);\n\t\tioc_ctl |= IOC_CTRL_DD | IOC_CTRL_D4 | IOC_CTRL_TC;\n\t\t\t \n\t\t\t \n\n\t\tWRITE_REG(ioc_ctl, sba_dev->sba_hpa+IOC_CTRL);\n\n#ifdef DEBUG_SBA_INIT\n\t\tioc_ctl = READ_REG64(sba_dev->sba_hpa+IOC_CTRL);\n\t\tDBG_INIT(\" 0x%Lx\\n\", ioc_ctl);\n#endif\n\t}  \n\n\tif (IS_ASTRO(sba_dev->dev)) {\n\t\tint err;\n\t\tsba_dev->ioc[0].ioc_hpa = ioc_remap(sba_dev, ASTRO_IOC_OFFSET);\n\t\tnum_ioc = 1;\n\n\t\tsba_dev->chip_resv.name = \"Astro Intr Ack\";\n\t\tsba_dev->chip_resv.start = PCI_F_EXTEND | 0xfef00000UL;\n\t\tsba_dev->chip_resv.end   = PCI_F_EXTEND | (0xff000000UL - 1) ;\n\t\terr = request_resource(&iomem_resource, &(sba_dev->chip_resv));\n\t\tBUG_ON(err < 0);\n\n\t} else if (IS_PLUTO(sba_dev->dev)) {\n\t\tint err;\n\n\t\tsba_dev->ioc[0].ioc_hpa = ioc_remap(sba_dev, PLUTO_IOC_OFFSET);\n\t\tnum_ioc = 1;\n\n\t\tsba_dev->chip_resv.name = \"Pluto Intr/PIOP/VGA\";\n\t\tsba_dev->chip_resv.start = PCI_F_EXTEND | 0xfee00000UL;\n\t\tsba_dev->chip_resv.end   = PCI_F_EXTEND | (0xff200000UL - 1);\n\t\terr = request_resource(&iomem_resource, &(sba_dev->chip_resv));\n\t\tWARN_ON(err < 0);\n\n\t\tsba_dev->iommu_resv.name = \"IOVA Space\";\n\t\tsba_dev->iommu_resv.start = 0x40000000UL;\n\t\tsba_dev->iommu_resv.end   = 0x50000000UL - 1;\n\t\terr = request_resource(&iomem_resource, &(sba_dev->iommu_resv));\n\t\tWARN_ON(err < 0);\n\t} else {\n\t\t \n\t\tsba_dev->ioc[0].ioc_hpa = ioc_remap(sba_dev, IKE_IOC_OFFSET(0));\n\t\tsba_dev->ioc[1].ioc_hpa = ioc_remap(sba_dev, IKE_IOC_OFFSET(1));\n\t\tnum_ioc = 2;\n\n\t\t \n\t}\n\t \n\n\tsba_dev->num_ioc = num_ioc;\n\tfor (i = 0; i < num_ioc; i++) {\n\t\tvoid __iomem *ioc_hpa = sba_dev->ioc[i].ioc_hpa;\n\t\tunsigned int j;\n\n\t\tfor (j=0; j < sizeof(u64) * ROPES_PER_IOC; j+=sizeof(u64)) {\n\n\t\t\t \n\t\t\tif (IS_PLUTO(sba_dev->dev)) {\n\t\t\t\tvoid __iomem *rope_cfg;\n\t\t\t\tunsigned long cfg_val;\n\n\t\t\t\trope_cfg = ioc_hpa + IOC_ROPE0_CFG + j;\n\t\t\t\tcfg_val = READ_REG(rope_cfg);\n\t\t\t\tcfg_val &= ~IOC_ROPE_AO;\n\t\t\t\tWRITE_REG(cfg_val, rope_cfg);\n\t\t\t}\n\n\t\t\t \n\t\t\tWRITE_REG(HF_ENABLE, ioc_hpa + ROPE0_CTL + j);\n\t\t}\n\n\t\t \n\t\tREAD_REG(sba_dev->ioc[i].ioc_hpa + ROPE7_CTL);\n\n\t\tDBG_INIT(\"\tioc[%d] ROPE_CFG %#lx  ROPE_DBG %lx\\n\",\n\t\t\t\ti,\n\t\t\t\t(unsigned long) READ_REG(sba_dev->ioc[i].ioc_hpa + 0x40),\n\t\t\t\t(unsigned long) READ_REG(sba_dev->ioc[i].ioc_hpa + 0x50)\n\t\t\t);\n\t\tDBG_INIT(\"\tSTATUS_CONTROL %#lx  FLUSH_CTRL %#lx\\n\",\n\t\t\t\t(unsigned long) READ_REG(sba_dev->ioc[i].ioc_hpa + 0x108),\n\t\t\t\t(unsigned long) READ_REG(sba_dev->ioc[i].ioc_hpa + 0x400)\n\t\t\t);\n\n\t\tif (IS_PLUTO(sba_dev->dev)) {\n\t\t\tsba_ioc_init_pluto(sba_dev->dev, &(sba_dev->ioc[i]), i);\n\t\t} else {\n\t\t\tsba_ioc_init(sba_dev->dev, &(sba_dev->ioc[i]), i);\n\t\t}\n\t}\n}\n\nstatic void\nsba_common_init(struct sba_device *sba_dev)\n{\n\tint i;\n\n\t \n\tsba_dev->next = sba_list;\n\tsba_list = sba_dev;\n\n\tfor(i=0; i< sba_dev->num_ioc; i++) {\n\t\tint res_size;\n#ifdef DEBUG_DMB_TRAP\n\t\textern void iterate_pages(unsigned long , unsigned long ,\n\t\t\t\t\t  void (*)(pte_t * , unsigned long),\n\t\t\t\t\t  unsigned long );\n\t\tvoid set_data_memory_break(pte_t * , unsigned long);\n#endif\n\t\t \n\t\tres_size = sba_dev->ioc[i].pdir_size/sizeof(u64);  \n\n\t\t \n\t\tif (piranha_bad_128k) {\n\t\t\tres_size -= (128*1024)/sizeof(u64);\n\t\t}\n\n\t\tres_size >>= 3;   \n\t\tDBG_INIT(\"%s() res_size 0x%x\\n\",\n\t\t\t__func__, res_size);\n\n\t\tsba_dev->ioc[i].res_size = res_size;\n\t\tsba_dev->ioc[i].res_map = (char *) __get_free_pages(GFP_KERNEL, get_order(res_size));\n\n#ifdef DEBUG_DMB_TRAP\n\t\titerate_pages( sba_dev->ioc[i].res_map, res_size,\n\t\t\t\tset_data_memory_break, 0);\n#endif\n\n\t\tif (NULL == sba_dev->ioc[i].res_map)\n\t\t{\n\t\t\tpanic(\"%s:%s() could not allocate resource map\\n\",\n\t\t\t      __FILE__, __func__ );\n\t\t}\n\n\t\tmemset(sba_dev->ioc[i].res_map, 0, res_size);\n\t\t \n\t\tsba_dev->ioc[i].res_hint = (unsigned long *)\n\t\t\t\t&(sba_dev->ioc[i].res_map[L1_CACHE_BYTES]);\n\n#ifdef ASSERT_PDIR_SANITY\n\t\t \n\t\tsba_dev->ioc[i].res_map[0] = 0x80;\n\t\tsba_dev->ioc[i].pdir_base[0] = (__force __le64) 0xeeffc0addbba0080ULL;\n#endif\n\n\t\t \n\t\tif (piranha_bad_128k) {\n\t\t\t \n\n\t\t\tint idx_start = (1408*1024/sizeof(u64)) >> 3;\n\t\t\tint idx_end   = (1536*1024/sizeof(u64)) >> 3;\n\t\t\tlong *p_start = (long *) &(sba_dev->ioc[i].res_map[idx_start]);\n\t\t\tlong *p_end   = (long *) &(sba_dev->ioc[i].res_map[idx_end]);\n\n\t\t\t \n\t\t\twhile (p_start < p_end)\n\t\t\t\t*p_start++ = -1;\n\t\t\t\t\n\t\t}\n\n#ifdef DEBUG_DMB_TRAP\n\t\titerate_pages( sba_dev->ioc[i].res_map, res_size,\n\t\t\t\tset_data_memory_break, 0);\n\t\titerate_pages( sba_dev->ioc[i].pdir_base, sba_dev->ioc[i].pdir_size,\n\t\t\t\tset_data_memory_break, 0);\n#endif\n\n\t\tDBG_INIT(\"%s() %d res_map %x %p\\n\",\n\t\t\t__func__, i, res_size, sba_dev->ioc[i].res_map);\n\t}\n\n\tspin_lock_init(&sba_dev->sba_lock);\n\tioc_needs_fdc = boot_cpu_data.pdc.capabilities & PDC_MODEL_IOPDIR_FDC;\n\n#ifdef DEBUG_SBA_INIT\n\t \n\tif (ioc_needs_fdc) {\n\t\tprintk(KERN_INFO MODULE_NAME \" FDC/SYNC required.\\n\");\n\t} else {\n\t\tprintk(KERN_INFO MODULE_NAME \" IOC has cache coherent PDIR.\\n\");\n\t}\n#endif\n}\n\n#ifdef CONFIG_PROC_FS\nstatic int sba_proc_info(struct seq_file *m, void *p)\n{\n\tstruct sba_device *sba_dev = sba_list;\n\tstruct ioc *ioc = &sba_dev->ioc[0];\t \n\tint total_pages = (int) (ioc->res_size << 3);  \n#ifdef SBA_COLLECT_STATS\n\tunsigned long avg = 0, min, max;\n#endif\n\tint i;\n\n\tseq_printf(m, \"%s rev %d.%d\\n\",\n\t\t   sba_dev->name,\n\t\t   (sba_dev->hw_rev & 0x7) + 1,\n\t\t   (sba_dev->hw_rev & 0x18) >> 3);\n\tseq_printf(m, \"IO PDIR size    : %d bytes (%d entries)\\n\",\n\t\t   (int)((ioc->res_size << 3) * sizeof(u64)),  \n\t\t   total_pages);\n\n\tseq_printf(m, \"Resource bitmap : %d bytes (%d pages)\\n\",\n\t\t   ioc->res_size, ioc->res_size << 3);    \n\n\tseq_printf(m, \"LMMIO_BASE/MASK/ROUTE %08x %08x %08x\\n\",\n\t\t   READ_REG32(sba_dev->sba_hpa + LMMIO_DIST_BASE),\n\t\t   READ_REG32(sba_dev->sba_hpa + LMMIO_DIST_MASK),\n\t\t   READ_REG32(sba_dev->sba_hpa + LMMIO_DIST_ROUTE));\n\n\tfor (i=0; i<4; i++)\n\t\tseq_printf(m, \"DIR%d_BASE/MASK/ROUTE %08x %08x %08x\\n\",\n\t\t\t   i,\n\t\t\t   READ_REG32(sba_dev->sba_hpa + LMMIO_DIRECT0_BASE  + i*0x18),\n\t\t\t   READ_REG32(sba_dev->sba_hpa + LMMIO_DIRECT0_MASK  + i*0x18),\n\t\t\t   READ_REG32(sba_dev->sba_hpa + LMMIO_DIRECT0_ROUTE + i*0x18));\n\n#ifdef SBA_COLLECT_STATS\n\tseq_printf(m, \"IO PDIR entries : %ld free  %ld used (%d%%)\\n\",\n\t\t   total_pages - ioc->used_pages, ioc->used_pages,\n\t\t   (int)(ioc->used_pages * 100 / total_pages));\n\n\tmin = max = ioc->avg_search[0];\n\tfor (i = 0; i < SBA_SEARCH_SAMPLE; i++) {\n\t\tavg += ioc->avg_search[i];\n\t\tif (ioc->avg_search[i] > max) max = ioc->avg_search[i];\n\t\tif (ioc->avg_search[i] < min) min = ioc->avg_search[i];\n\t}\n\tavg /= SBA_SEARCH_SAMPLE;\n\tseq_printf(m, \"  Bitmap search : %ld/%ld/%ld (min/avg/max CPU Cycles)\\n\",\n\t\t   min, avg, max);\n\n\tseq_printf(m, \"pci_map_single(): %12ld calls  %12ld pages (avg %d/1000)\\n\",\n\t\t   ioc->msingle_calls, ioc->msingle_pages,\n\t\t   (int)((ioc->msingle_pages * 1000)/ioc->msingle_calls));\n\n\t \n\tmin = ioc->usingle_calls;\n\tmax = ioc->usingle_pages - ioc->usg_pages;\n\tseq_printf(m, \"pci_unmap_single: %12ld calls  %12ld pages (avg %d/1000)\\n\",\n\t\t   min, max, (int)((max * 1000)/min));\n\n\tseq_printf(m, \"pci_map_sg()    : %12ld calls  %12ld pages (avg %d/1000)\\n\",\n\t\t   ioc->msg_calls, ioc->msg_pages,\n\t\t   (int)((ioc->msg_pages * 1000)/ioc->msg_calls));\n\n\tseq_printf(m, \"pci_unmap_sg()  : %12ld calls  %12ld pages (avg %d/1000)\\n\",\n\t\t   ioc->usg_calls, ioc->usg_pages,\n\t\t   (int)((ioc->usg_pages * 1000)/ioc->usg_calls));\n#endif\n\n\treturn 0;\n}\n\nstatic int\nsba_proc_bitmap_info(struct seq_file *m, void *p)\n{\n\tstruct sba_device *sba_dev = sba_list;\n\tstruct ioc *ioc = &sba_dev->ioc[0];\t \n\n\tseq_hex_dump(m, \"   \", DUMP_PREFIX_NONE, 32, 4, ioc->res_map,\n\t\t     ioc->res_size, false);\n\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n#endif  \n\nstatic const struct parisc_device_id sba_tbl[] __initconst = {\n\t{ HPHW_IOA, HVERSION_REV_ANY_ID, ASTRO_RUNWAY_PORT, 0xb },\n\t{ HPHW_BCPORT, HVERSION_REV_ANY_ID, IKE_MERCED_PORT, 0xc },\n\t{ HPHW_BCPORT, HVERSION_REV_ANY_ID, REO_MERCED_PORT, 0xc },\n\t{ HPHW_BCPORT, HVERSION_REV_ANY_ID, REOG_MERCED_PORT, 0xc },\n\t{ HPHW_IOA, HVERSION_REV_ANY_ID, PLUTO_MCKINLEY_PORT, 0xc },\n\t{ 0, }\n};\n\nstatic int sba_driver_callback(struct parisc_device *);\n\nstatic struct parisc_driver sba_driver __refdata = {\n\t.name =\t\tMODULE_NAME,\n\t.id_table =\tsba_tbl,\n\t.probe =\tsba_driver_callback,\n};\n\n \nstatic int __init sba_driver_callback(struct parisc_device *dev)\n{\n\tstruct sba_device *sba_dev;\n\tu32 func_class;\n\tint i;\n\tchar *version;\n\tvoid __iomem *sba_addr = ioremap(dev->hpa.start, SBA_FUNC_SIZE);\n\tstruct proc_dir_entry *root __maybe_unused;\n\n\tsba_dump_ranges(sba_addr);\n\n\t \n\tfunc_class = READ_REG(sba_addr + SBA_FCLASS);\n\n\tif (IS_ASTRO(dev)) {\n\t\tunsigned long fclass;\n\t\tstatic char astro_rev[]=\"Astro ?.?\";\n\n\t\t \n\t\tfclass = READ_REG(sba_addr);\n\n\t\tastro_rev[6] = '1' + (char) (fclass & 0x7);\n\t\tastro_rev[8] = '0' + (char) ((fclass & 0x18) >> 3);\n\t\tversion = astro_rev;\n\n\t} else if (IS_IKE(dev)) {\n\t\tstatic char ike_rev[] = \"Ike rev ?\";\n\t\tike_rev[8] = '0' + (char) (func_class & 0xff);\n\t\tversion = ike_rev;\n\t} else if (IS_PLUTO(dev)) {\n\t\tstatic char pluto_rev[]=\"Pluto ?.?\";\n\t\tpluto_rev[6] = '0' + (char) ((func_class & 0xf0) >> 4); \n\t\tpluto_rev[8] = '0' + (char) (func_class & 0x0f); \n\t\tversion = pluto_rev;\n\t} else {\n\t\tstatic char reo_rev[] = \"REO rev ?\";\n\t\treo_rev[8] = '0' + (char) (func_class & 0xff);\n\t\tversion = reo_rev;\n\t}\n\n\tif (!global_ioc_cnt) {\n\t\tglobal_ioc_cnt = count_parisc_driver(&sba_driver);\n\n\t\t \n\t\tif ((!IS_ASTRO(dev)) || (!IS_PLUTO(dev)))\n\t\t\tglobal_ioc_cnt *= 2;\n\t}\n\n\tprintk(KERN_INFO \"%s found %s at 0x%llx\\n\",\n\t\tMODULE_NAME, version, (unsigned long long)dev->hpa.start);\n\n\tsba_dev = kzalloc(sizeof(struct sba_device), GFP_KERNEL);\n\tif (!sba_dev) {\n\t\tprintk(KERN_ERR MODULE_NAME \" - couldn't alloc sba_device\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tparisc_set_drvdata(dev, sba_dev);\n\n\tfor(i=0; i<MAX_IOC; i++)\n\t\tspin_lock_init(&(sba_dev->ioc[i].res_lock));\n\n\tsba_dev->dev = dev;\n\tsba_dev->hw_rev = func_class;\n\tsba_dev->name = dev->name;\n\tsba_dev->sba_hpa = sba_addr;\n\n\tsba_get_pat_resources(sba_dev);\n\tsba_hw_init(sba_dev);\n\tsba_common_init(sba_dev);\n\n\thppa_dma_ops = &sba_ops;\n\n\tswitch (dev->id.hversion) {\n\tcase PLUTO_MCKINLEY_PORT:\n\t\tif (!proc_mckinley_root)\n\t\t\tproc_mckinley_root = proc_mkdir(\"bus/mckinley\", NULL);\n\t\troot = proc_mckinley_root;\n\t\tbreak;\n\tcase ASTRO_RUNWAY_PORT:\n\tcase IKE_MERCED_PORT:\n\tdefault:\n\t\tif (!proc_runway_root)\n\t\t\tproc_runway_root = proc_mkdir(\"bus/runway\", NULL);\n\t\troot = proc_runway_root;\n\t\tbreak;\n\t}\n\n\tproc_create_single(\"sba_iommu\", 0, root, sba_proc_info);\n\tproc_create_single(\"sba_iommu-bitmap\", 0, root, sba_proc_bitmap_info);\n\treturn 0;\n}\n\n \nstatic int __init sba_init(void)\n{\n\treturn register_parisc_driver(&sba_driver);\n}\narch_initcall(sba_init);\n\n\n \nvoid * sba_get_iommu(struct parisc_device *pci_hba)\n{\n\tstruct parisc_device *sba_dev = parisc_parent(pci_hba);\n\tstruct sba_device *sba = dev_get_drvdata(&sba_dev->dev);\n\tchar t = sba_dev->id.hw_type;\n\tint iocnum = (pci_hba->hw_path >> 3);\t \n\n\tWARN_ON((t != HPHW_IOA) && (t != HPHW_BCPORT));\n\n\treturn &(sba->ioc[iocnum]);\n}\n\n\n \nvoid sba_directed_lmmio(struct parisc_device *pci_hba, struct resource *r)\n{\n\tstruct parisc_device *sba_dev = parisc_parent(pci_hba);\n\tstruct sba_device *sba = dev_get_drvdata(&sba_dev->dev);\n\tchar t = sba_dev->id.hw_type;\n\tint i;\n\tint rope = (pci_hba->hw_path & (ROPES_PER_IOC-1));   \n\n\tBUG_ON((t!=HPHW_IOA) && (t!=HPHW_BCPORT));\n\n\tr->start = r->end = 0;\n\n\t \n\tfor (i=0; i<4; i++) {\n\t\tint base, size;\n\t\tvoid __iomem *reg = sba->sba_hpa + i*0x18;\n\n\t\tbase = READ_REG32(reg + LMMIO_DIRECT0_BASE);\n\t\tif ((base & 1) == 0)\n\t\t\tcontinue;\t \n\n\t\tsize = READ_REG32(reg + LMMIO_DIRECT0_ROUTE);\n\n\t\tif ((size & (ROPES_PER_IOC-1)) != rope)\n\t\t\tcontinue;\t \n\t\t\n\t\tr->start = (base & ~1UL) | PCI_F_EXTEND;\n\t\tsize = ~ READ_REG32(reg + LMMIO_DIRECT0_MASK);\n\t\tr->end = r->start + size;\n\t\tr->flags = IORESOURCE_MEM;\n\t}\n}\n\n\n \nvoid sba_distributed_lmmio(struct parisc_device *pci_hba, struct resource *r )\n{\n\tstruct parisc_device *sba_dev = parisc_parent(pci_hba);\n\tstruct sba_device *sba = dev_get_drvdata(&sba_dev->dev);\n\tchar t = sba_dev->id.hw_type;\n\tint base, size;\n\tint rope = (pci_hba->hw_path & (ROPES_PER_IOC-1));   \n\n\tBUG_ON((t!=HPHW_IOA) && (t!=HPHW_BCPORT));\n\n\tr->start = r->end = 0;\n\n\tbase = READ_REG32(sba->sba_hpa + LMMIO_DIST_BASE);\n\tif ((base & 1) == 0) {\n\t\tBUG();\t \n\t\treturn;\n\t}\n\n\tr->start = (base & ~1UL) | PCI_F_EXTEND;\n\n\tsize = (~READ_REG32(sba->sba_hpa + LMMIO_DIST_MASK)) / ROPES_PER_IOC;\n\tr->start += rope * (size + 1);\t \n\tr->end = r->start + size;\n\tr->flags = IORESOURCE_MEM;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}