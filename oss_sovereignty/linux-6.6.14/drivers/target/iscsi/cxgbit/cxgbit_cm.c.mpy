{
  "module_name": "cxgbit_cm.c",
  "hash_id": "a9bc30d423370c7bbbaf700e65e8750314a3bd12cf7d6af9d22b2a61bfbe52d9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/target/iscsi/cxgbit/cxgbit_cm.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/list.h>\n#include <linux/workqueue.h>\n#include <linux/skbuff.h>\n#include <linux/timer.h>\n#include <linux/notifier.h>\n#include <linux/inetdevice.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/if_vlan.h>\n\n#include <net/neighbour.h>\n#include <net/netevent.h>\n#include <net/route.h>\n#include <net/tcp.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n\n#include <libcxgb_cm.h>\n#include \"cxgbit.h\"\n#include \"clip_tbl.h\"\n\nstatic void cxgbit_init_wr_wait(struct cxgbit_wr_wait *wr_waitp)\n{\n\twr_waitp->ret = 0;\n\treinit_completion(&wr_waitp->completion);\n}\n\nstatic void\ncxgbit_wake_up(struct cxgbit_wr_wait *wr_waitp, const char *func, u8 ret)\n{\n\tif (ret == CPL_ERR_NONE)\n\t\twr_waitp->ret = 0;\n\telse\n\t\twr_waitp->ret = -EIO;\n\n\tif (wr_waitp->ret)\n\t\tpr_err(\"%s: err:%u\", func, ret);\n\n\tcomplete(&wr_waitp->completion);\n}\n\nstatic int\ncxgbit_wait_for_reply(struct cxgbit_device *cdev,\n\t\t      struct cxgbit_wr_wait *wr_waitp, u32 tid, u32 timeout,\n\t\t      const char *func)\n{\n\tint ret;\n\n\tif (!test_bit(CDEV_STATE_UP, &cdev->flags)) {\n\t\twr_waitp->ret = -EIO;\n\t\tgoto out;\n\t}\n\n\tret = wait_for_completion_timeout(&wr_waitp->completion, timeout * HZ);\n\tif (!ret) {\n\t\tpr_info(\"%s - Device %s not responding tid %u\\n\",\n\t\t\tfunc, pci_name(cdev->lldi.pdev), tid);\n\t\twr_waitp->ret = -ETIMEDOUT;\n\t}\nout:\n\tif (wr_waitp->ret)\n\t\tpr_info(\"%s: FW reply %d tid %u\\n\",\n\t\t\tpci_name(cdev->lldi.pdev), wr_waitp->ret, tid);\n\treturn wr_waitp->ret;\n}\n\nstatic int cxgbit_np_hashfn(const struct cxgbit_np *cnp)\n{\n\treturn ((unsigned long)cnp >> 10) & (NP_INFO_HASH_SIZE - 1);\n}\n\nstatic struct np_info *\ncxgbit_np_hash_add(struct cxgbit_device *cdev, struct cxgbit_np *cnp,\n\t\t   unsigned int stid)\n{\n\tstruct np_info *p = kzalloc(sizeof(*p), GFP_KERNEL);\n\n\tif (p) {\n\t\tint bucket = cxgbit_np_hashfn(cnp);\n\n\t\tp->cnp = cnp;\n\t\tp->stid = stid;\n\t\tspin_lock(&cdev->np_lock);\n\t\tp->next = cdev->np_hash_tab[bucket];\n\t\tcdev->np_hash_tab[bucket] = p;\n\t\tspin_unlock(&cdev->np_lock);\n\t}\n\n\treturn p;\n}\n\nstatic int\ncxgbit_np_hash_find(struct cxgbit_device *cdev, struct cxgbit_np *cnp)\n{\n\tint stid = -1, bucket = cxgbit_np_hashfn(cnp);\n\tstruct np_info *p;\n\n\tspin_lock(&cdev->np_lock);\n\tfor (p = cdev->np_hash_tab[bucket]; p; p = p->next) {\n\t\tif (p->cnp == cnp) {\n\t\t\tstid = p->stid;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&cdev->np_lock);\n\n\treturn stid;\n}\n\nstatic int cxgbit_np_hash_del(struct cxgbit_device *cdev, struct cxgbit_np *cnp)\n{\n\tint stid = -1, bucket = cxgbit_np_hashfn(cnp);\n\tstruct np_info *p, **prev = &cdev->np_hash_tab[bucket];\n\n\tspin_lock(&cdev->np_lock);\n\tfor (p = *prev; p; prev = &p->next, p = p->next) {\n\t\tif (p->cnp == cnp) {\n\t\t\tstid = p->stid;\n\t\t\t*prev = p->next;\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&cdev->np_lock);\n\n\treturn stid;\n}\n\nvoid _cxgbit_free_cnp(struct kref *kref)\n{\n\tstruct cxgbit_np *cnp;\n\n\tcnp = container_of(kref, struct cxgbit_np, kref);\n\tkfree(cnp);\n}\n\nstatic int\ncxgbit_create_server6(struct cxgbit_device *cdev, unsigned int stid,\n\t\t      struct cxgbit_np *cnp)\n{\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)\n\t\t\t\t     &cnp->com.local_addr;\n\tint addr_type;\n\tint ret;\n\n\tpr_debug(\"%s: dev = %s; stid = %u; sin6_port = %u\\n\",\n\t\t __func__, cdev->lldi.ports[0]->name, stid, sin6->sin6_port);\n\n\taddr_type = ipv6_addr_type((const struct in6_addr *)\n\t\t\t\t   &sin6->sin6_addr);\n\tif (addr_type != IPV6_ADDR_ANY) {\n\t\tret = cxgb4_clip_get(cdev->lldi.ports[0],\n\t\t\t\t     (const u32 *)&sin6->sin6_addr.s6_addr, 1);\n\t\tif (ret) {\n\t\t\tpr_err(\"Unable to find clip table entry. laddr %pI6. Error:%d.\\n\",\n\t\t\t       sin6->sin6_addr.s6_addr, ret);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tcxgbit_get_cnp(cnp);\n\tcxgbit_init_wr_wait(&cnp->com.wr_wait);\n\n\tret = cxgb4_create_server6(cdev->lldi.ports[0],\n\t\t\t\t   stid, &sin6->sin6_addr,\n\t\t\t\t   sin6->sin6_port,\n\t\t\t\t   cdev->lldi.rxq_ids[0]);\n\tif (!ret)\n\t\tret = cxgbit_wait_for_reply(cdev, &cnp->com.wr_wait,\n\t\t\t\t\t    0, 10, __func__);\n\telse if (ret > 0)\n\t\tret = net_xmit_errno(ret);\n\telse\n\t\tcxgbit_put_cnp(cnp);\n\n\tif (ret) {\n\t\tif (ret != -ETIMEDOUT)\n\t\t\tcxgb4_clip_release(cdev->lldi.ports[0],\n\t\t\t\t   (const u32 *)&sin6->sin6_addr.s6_addr, 1);\n\n\t\tpr_err(\"create server6 err %d stid %d laddr %pI6 lport %d\\n\",\n\t\t       ret, stid, sin6->sin6_addr.s6_addr,\n\t\t       ntohs(sin6->sin6_port));\n\t}\n\n\treturn ret;\n}\n\nstatic int\ncxgbit_create_server4(struct cxgbit_device *cdev, unsigned int stid,\n\t\t      struct cxgbit_np *cnp)\n{\n\tstruct sockaddr_in *sin = (struct sockaddr_in *)\n\t\t\t\t   &cnp->com.local_addr;\n\tint ret;\n\n\tpr_debug(\"%s: dev = %s; stid = %u; sin_port = %u\\n\",\n\t\t __func__, cdev->lldi.ports[0]->name, stid, sin->sin_port);\n\n\tcxgbit_get_cnp(cnp);\n\tcxgbit_init_wr_wait(&cnp->com.wr_wait);\n\n\tret = cxgb4_create_server(cdev->lldi.ports[0],\n\t\t\t\t  stid, sin->sin_addr.s_addr,\n\t\t\t\t  sin->sin_port, 0,\n\t\t\t\t  cdev->lldi.rxq_ids[0]);\n\tif (!ret)\n\t\tret = cxgbit_wait_for_reply(cdev,\n\t\t\t\t\t    &cnp->com.wr_wait,\n\t\t\t\t\t    0, 10, __func__);\n\telse if (ret > 0)\n\t\tret = net_xmit_errno(ret);\n\telse\n\t\tcxgbit_put_cnp(cnp);\n\n\tif (ret)\n\t\tpr_err(\"create server failed err %d stid %d laddr %pI4 lport %d\\n\",\n\t\t       ret, stid, &sin->sin_addr, ntohs(sin->sin_port));\n\treturn ret;\n}\n\nstruct cxgbit_device *cxgbit_find_device(struct net_device *ndev, u8 *port_id)\n{\n\tstruct cxgbit_device *cdev;\n\tu8 i;\n\n\tlist_for_each_entry(cdev, &cdev_list_head, list) {\n\t\tstruct cxgb4_lld_info *lldi = &cdev->lldi;\n\n\t\tfor (i = 0; i < lldi->nports; i++) {\n\t\t\tif (lldi->ports[i] == ndev) {\n\t\t\t\tif (port_id)\n\t\t\t\t\t*port_id = i;\n\t\t\t\treturn cdev;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct net_device *cxgbit_get_real_dev(struct net_device *ndev)\n{\n\tif (ndev->priv_flags & IFF_BONDING) {\n\t\tpr_err(\"Bond devices are not supported. Interface:%s\\n\",\n\t\t       ndev->name);\n\t\treturn NULL;\n\t}\n\n\tif (is_vlan_dev(ndev))\n\t\treturn vlan_dev_real_dev(ndev);\n\n\treturn ndev;\n}\n\nstatic struct net_device *cxgbit_ipv4_netdev(__be32 saddr)\n{\n\tstruct net_device *ndev;\n\n\tndev = __ip_dev_find(&init_net, saddr, false);\n\tif (!ndev)\n\t\treturn NULL;\n\n\treturn cxgbit_get_real_dev(ndev);\n}\n\nstatic struct net_device *cxgbit_ipv6_netdev(struct in6_addr *addr6)\n{\n\tstruct net_device *ndev = NULL;\n\tbool found = false;\n\n\tif (IS_ENABLED(CONFIG_IPV6)) {\n\t\tfor_each_netdev_rcu(&init_net, ndev)\n\t\t\tif (ipv6_chk_addr(&init_net, addr6, ndev, 1)) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t}\n\tif (!found)\n\t\treturn NULL;\n\treturn cxgbit_get_real_dev(ndev);\n}\n\nstatic struct cxgbit_device *cxgbit_find_np_cdev(struct cxgbit_np *cnp)\n{\n\tstruct sockaddr_storage *sockaddr = &cnp->com.local_addr;\n\tint ss_family = sockaddr->ss_family;\n\tstruct net_device *ndev = NULL;\n\tstruct cxgbit_device *cdev = NULL;\n\n\trcu_read_lock();\n\tif (ss_family == AF_INET) {\n\t\tstruct sockaddr_in *sin;\n\n\t\tsin = (struct sockaddr_in *)sockaddr;\n\t\tndev = cxgbit_ipv4_netdev(sin->sin_addr.s_addr);\n\t} else if (ss_family == AF_INET6) {\n\t\tstruct sockaddr_in6 *sin6;\n\n\t\tsin6 = (struct sockaddr_in6 *)sockaddr;\n\t\tndev = cxgbit_ipv6_netdev(&sin6->sin6_addr);\n\t}\n\tif (!ndev)\n\t\tgoto out;\n\n\tcdev = cxgbit_find_device(ndev, NULL);\nout:\n\trcu_read_unlock();\n\treturn cdev;\n}\n\nstatic bool cxgbit_inaddr_any(struct cxgbit_np *cnp)\n{\n\tstruct sockaddr_storage *sockaddr = &cnp->com.local_addr;\n\tint ss_family = sockaddr->ss_family;\n\tint addr_type;\n\n\tif (ss_family == AF_INET) {\n\t\tstruct sockaddr_in *sin;\n\n\t\tsin = (struct sockaddr_in *)sockaddr;\n\t\tif (sin->sin_addr.s_addr == htonl(INADDR_ANY))\n\t\t\treturn true;\n\t} else if (ss_family == AF_INET6) {\n\t\tstruct sockaddr_in6 *sin6;\n\n\t\tsin6 = (struct sockaddr_in6 *)sockaddr;\n\t\taddr_type = ipv6_addr_type((const struct in6_addr *)\n\t\t\t\t&sin6->sin6_addr);\n\t\tif (addr_type == IPV6_ADDR_ANY)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int\n__cxgbit_setup_cdev_np(struct cxgbit_device *cdev, struct cxgbit_np *cnp)\n{\n\tint stid, ret;\n\tint ss_family = cnp->com.local_addr.ss_family;\n\n\tif (!test_bit(CDEV_STATE_UP, &cdev->flags))\n\t\treturn -EINVAL;\n\n\tstid = cxgb4_alloc_stid(cdev->lldi.tids, ss_family, cnp);\n\tif (stid < 0)\n\t\treturn -EINVAL;\n\n\tif (!cxgbit_np_hash_add(cdev, cnp, stid)) {\n\t\tcxgb4_free_stid(cdev->lldi.tids, stid, ss_family);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ss_family == AF_INET)\n\t\tret = cxgbit_create_server4(cdev, stid, cnp);\n\telse\n\t\tret = cxgbit_create_server6(cdev, stid, cnp);\n\n\tif (ret) {\n\t\tif (ret != -ETIMEDOUT)\n\t\t\tcxgb4_free_stid(cdev->lldi.tids, stid,\n\t\t\t\t\tss_family);\n\t\tcxgbit_np_hash_del(cdev, cnp);\n\t\treturn ret;\n\t}\n\treturn ret;\n}\n\nstatic int cxgbit_setup_cdev_np(struct cxgbit_np *cnp)\n{\n\tstruct cxgbit_device *cdev;\n\tint ret = -1;\n\n\tmutex_lock(&cdev_list_lock);\n\tcdev = cxgbit_find_np_cdev(cnp);\n\tif (!cdev)\n\t\tgoto out;\n\n\tif (cxgbit_np_hash_find(cdev, cnp) >= 0)\n\t\tgoto out;\n\n\tif (__cxgbit_setup_cdev_np(cdev, cnp))\n\t\tgoto out;\n\n\tcnp->com.cdev = cdev;\n\tret = 0;\nout:\n\tmutex_unlock(&cdev_list_lock);\n\treturn ret;\n}\n\nstatic int cxgbit_setup_all_np(struct cxgbit_np *cnp)\n{\n\tstruct cxgbit_device *cdev;\n\tint ret;\n\tu32 count = 0;\n\n\tmutex_lock(&cdev_list_lock);\n\tlist_for_each_entry(cdev, &cdev_list_head, list) {\n\t\tif (cxgbit_np_hash_find(cdev, cnp) >= 0) {\n\t\t\tmutex_unlock(&cdev_list_lock);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tlist_for_each_entry(cdev, &cdev_list_head, list) {\n\t\tret = __cxgbit_setup_cdev_np(cdev, cnp);\n\t\tif (ret == -ETIMEDOUT)\n\t\t\tbreak;\n\t\tif (ret != 0)\n\t\t\tcontinue;\n\t\tcount++;\n\t}\n\tmutex_unlock(&cdev_list_lock);\n\n\treturn count ? 0 : -1;\n}\n\nint cxgbit_setup_np(struct iscsi_np *np, struct sockaddr_storage *ksockaddr)\n{\n\tstruct cxgbit_np *cnp;\n\tint ret;\n\n\tif ((ksockaddr->ss_family != AF_INET) &&\n\t    (ksockaddr->ss_family != AF_INET6))\n\t\treturn -EINVAL;\n\n\tcnp = kzalloc(sizeof(*cnp), GFP_KERNEL);\n\tif (!cnp)\n\t\treturn -ENOMEM;\n\n\tinit_waitqueue_head(&cnp->accept_wait);\n\tinit_completion(&cnp->com.wr_wait.completion);\n\tinit_completion(&cnp->accept_comp);\n\tINIT_LIST_HEAD(&cnp->np_accept_list);\n\tspin_lock_init(&cnp->np_accept_lock);\n\tkref_init(&cnp->kref);\n\tmemcpy(&np->np_sockaddr, ksockaddr,\n\t       sizeof(struct sockaddr_storage));\n\tmemcpy(&cnp->com.local_addr, &np->np_sockaddr,\n\t       sizeof(cnp->com.local_addr));\n\n\tcnp->np = np;\n\tcnp->com.cdev = NULL;\n\n\tif (cxgbit_inaddr_any(cnp))\n\t\tret = cxgbit_setup_all_np(cnp);\n\telse\n\t\tret = cxgbit_setup_cdev_np(cnp);\n\n\tif (ret) {\n\t\tcxgbit_put_cnp(cnp);\n\t\treturn -EINVAL;\n\t}\n\n\tnp->np_context = cnp;\n\tcnp->com.state = CSK_STATE_LISTEN;\n\treturn 0;\n}\n\nstatic void\ncxgbit_set_conn_info(struct iscsi_np *np, struct iscsit_conn *conn,\n\t\t     struct cxgbit_sock *csk)\n{\n\tconn->login_family = np->np_sockaddr.ss_family;\n\tconn->login_sockaddr = csk->com.remote_addr;\n\tconn->local_sockaddr = csk->com.local_addr;\n}\n\nint cxgbit_accept_np(struct iscsi_np *np, struct iscsit_conn *conn)\n{\n\tstruct cxgbit_np *cnp = np->np_context;\n\tstruct cxgbit_sock *csk;\n\tint ret = 0;\n\naccept_wait:\n\tret = wait_for_completion_interruptible(&cnp->accept_comp);\n\tif (ret)\n\t\treturn -ENODEV;\n\n\tspin_lock_bh(&np->np_thread_lock);\n\tif (np->np_thread_state >= ISCSI_NP_THREAD_RESET) {\n\t\tspin_unlock_bh(&np->np_thread_lock);\n\t\t \n\t\treturn -ENODEV;\n\t}\n\tspin_unlock_bh(&np->np_thread_lock);\n\n\tspin_lock_bh(&cnp->np_accept_lock);\n\tif (list_empty(&cnp->np_accept_list)) {\n\t\tspin_unlock_bh(&cnp->np_accept_lock);\n\t\tgoto accept_wait;\n\t}\n\n\tcsk = list_first_entry(&cnp->np_accept_list,\n\t\t\t       struct cxgbit_sock,\n\t\t\t       accept_node);\n\n\tlist_del_init(&csk->accept_node);\n\tspin_unlock_bh(&cnp->np_accept_lock);\n\tconn->context = csk;\n\tcsk->conn = conn;\n\n\tcxgbit_set_conn_info(np, conn, csk);\n\treturn 0;\n}\n\nstatic int\n__cxgbit_free_cdev_np(struct cxgbit_device *cdev, struct cxgbit_np *cnp)\n{\n\tint stid, ret;\n\tbool ipv6 = false;\n\n\tstid = cxgbit_np_hash_del(cdev, cnp);\n\tif (stid < 0)\n\t\treturn -EINVAL;\n\tif (!test_bit(CDEV_STATE_UP, &cdev->flags))\n\t\treturn -EINVAL;\n\n\tif (cnp->np->np_sockaddr.ss_family == AF_INET6)\n\t\tipv6 = true;\n\n\tcxgbit_get_cnp(cnp);\n\tcxgbit_init_wr_wait(&cnp->com.wr_wait);\n\tret = cxgb4_remove_server(cdev->lldi.ports[0], stid,\n\t\t\t\t  cdev->lldi.rxq_ids[0], ipv6);\n\n\tif (ret > 0)\n\t\tret = net_xmit_errno(ret);\n\n\tif (ret) {\n\t\tcxgbit_put_cnp(cnp);\n\t\treturn ret;\n\t}\n\n\tret = cxgbit_wait_for_reply(cdev, &cnp->com.wr_wait,\n\t\t\t\t    0, 10, __func__);\n\tif (ret == -ETIMEDOUT)\n\t\treturn ret;\n\n\tif (ipv6 && cnp->com.cdev) {\n\t\tstruct sockaddr_in6 *sin6;\n\n\t\tsin6 = (struct sockaddr_in6 *)&cnp->com.local_addr;\n\t\tcxgb4_clip_release(cdev->lldi.ports[0],\n\t\t\t\t   (const u32 *)&sin6->sin6_addr.s6_addr,\n\t\t\t\t   1);\n\t}\n\n\tcxgb4_free_stid(cdev->lldi.tids, stid,\n\t\t\tcnp->com.local_addr.ss_family);\n\treturn 0;\n}\n\nstatic void cxgbit_free_all_np(struct cxgbit_np *cnp)\n{\n\tstruct cxgbit_device *cdev;\n\tint ret;\n\n\tmutex_lock(&cdev_list_lock);\n\tlist_for_each_entry(cdev, &cdev_list_head, list) {\n\t\tret = __cxgbit_free_cdev_np(cdev, cnp);\n\t\tif (ret == -ETIMEDOUT)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&cdev_list_lock);\n}\n\nstatic void cxgbit_free_cdev_np(struct cxgbit_np *cnp)\n{\n\tstruct cxgbit_device *cdev;\n\tbool found = false;\n\n\tmutex_lock(&cdev_list_lock);\n\tlist_for_each_entry(cdev, &cdev_list_head, list) {\n\t\tif (cdev == cnp->com.cdev) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\tgoto out;\n\n\t__cxgbit_free_cdev_np(cdev, cnp);\nout:\n\tmutex_unlock(&cdev_list_lock);\n}\n\nstatic void __cxgbit_free_conn(struct cxgbit_sock *csk);\n\nvoid cxgbit_free_np(struct iscsi_np *np)\n{\n\tstruct cxgbit_np *cnp = np->np_context;\n\tstruct cxgbit_sock *csk, *tmp;\n\n\tcnp->com.state = CSK_STATE_DEAD;\n\tif (cnp->com.cdev)\n\t\tcxgbit_free_cdev_np(cnp);\n\telse\n\t\tcxgbit_free_all_np(cnp);\n\n\tspin_lock_bh(&cnp->np_accept_lock);\n\tlist_for_each_entry_safe(csk, tmp, &cnp->np_accept_list, accept_node) {\n\t\tlist_del_init(&csk->accept_node);\n\t\t__cxgbit_free_conn(csk);\n\t}\n\tspin_unlock_bh(&cnp->np_accept_lock);\n\n\tnp->np_context = NULL;\n\tcxgbit_put_cnp(cnp);\n}\n\nstatic void cxgbit_send_halfclose(struct cxgbit_sock *csk)\n{\n\tstruct sk_buff *skb;\n\tu32 len = roundup(sizeof(struct cpl_close_con_req), 16);\n\n\tskb = alloc_skb(len, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tcxgb_mk_close_con_req(skb, len, csk->tid, csk->txq_idx,\n\t\t\t      NULL, NULL);\n\n\tcxgbit_skcb_flags(skb) |= SKCBF_TX_FLAG_COMPL;\n\t__skb_queue_tail(&csk->txq, skb);\n\tcxgbit_push_tx_frames(csk);\n}\n\nstatic void cxgbit_arp_failure_discard(void *handle, struct sk_buff *skb)\n{\n\tstruct cxgbit_sock *csk = handle;\n\n\tpr_debug(\"%s cxgbit_device %p\\n\", __func__, handle);\n\tkfree_skb(skb);\n\tcxgbit_put_csk(csk);\n}\n\nstatic void cxgbit_abort_arp_failure(void *handle, struct sk_buff *skb)\n{\n\tstruct cxgbit_device *cdev = handle;\n\tstruct cpl_abort_req *req = cplhdr(skb);\n\n\tpr_debug(\"%s cdev %p\\n\", __func__, cdev);\n\treq->cmd = CPL_ABORT_NO_RST;\n\tcxgbit_ofld_send(cdev, skb);\n}\n\nstatic int cxgbit_send_abort_req(struct cxgbit_sock *csk)\n{\n\tstruct sk_buff *skb;\n\tu32 len = roundup(sizeof(struct cpl_abort_req), 16);\n\n\tpr_debug(\"%s: csk %p tid %u; state %d\\n\",\n\t\t __func__, csk, csk->tid, csk->com.state);\n\n\t__skb_queue_purge(&csk->txq);\n\n\tif (!test_and_set_bit(CSK_TX_DATA_SENT, &csk->com.flags))\n\t\tcxgbit_send_tx_flowc_wr(csk);\n\n\tskb = __skb_dequeue(&csk->skbq);\n\tcxgb_mk_abort_req(skb, len, csk->tid, csk->txq_idx,\n\t\t\t  csk->com.cdev, cxgbit_abort_arp_failure);\n\n\treturn cxgbit_l2t_send(csk->com.cdev, skb, csk->l2t);\n}\n\nstatic void\n__cxgbit_abort_conn(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\t__kfree_skb(skb);\n\n\tif (csk->com.state != CSK_STATE_ESTABLISHED)\n\t\tgoto no_abort;\n\n\tset_bit(CSK_ABORT_RPL_WAIT, &csk->com.flags);\n\tcsk->com.state = CSK_STATE_ABORTING;\n\n\tcxgbit_send_abort_req(csk);\n\n\treturn;\n\nno_abort:\n\tcxgbit_wake_up(&csk->com.wr_wait, __func__, CPL_ERR_NONE);\n\tcxgbit_put_csk(csk);\n}\n\nvoid cxgbit_abort_conn(struct cxgbit_sock *csk)\n{\n\tstruct sk_buff *skb = alloc_skb(0, GFP_KERNEL | __GFP_NOFAIL);\n\n\tcxgbit_get_csk(csk);\n\tcxgbit_init_wr_wait(&csk->com.wr_wait);\n\n\tspin_lock_bh(&csk->lock);\n\tif (csk->lock_owner) {\n\t\tcxgbit_skcb_rx_backlog_fn(skb) = __cxgbit_abort_conn;\n\t\t__skb_queue_tail(&csk->backlogq, skb);\n\t} else {\n\t\t__cxgbit_abort_conn(csk, skb);\n\t}\n\tspin_unlock_bh(&csk->lock);\n\n\tcxgbit_wait_for_reply(csk->com.cdev, &csk->com.wr_wait,\n\t\t\t      csk->tid, 600, __func__);\n}\n\nstatic void __cxgbit_free_conn(struct cxgbit_sock *csk)\n{\n\tstruct iscsit_conn *conn = csk->conn;\n\tbool release = false;\n\n\tpr_debug(\"%s: state %d\\n\",\n\t\t __func__, csk->com.state);\n\n\tspin_lock_bh(&csk->lock);\n\tswitch (csk->com.state) {\n\tcase CSK_STATE_ESTABLISHED:\n\t\tif (conn && (conn->conn_state == TARG_CONN_STATE_IN_LOGOUT)) {\n\t\t\tcsk->com.state = CSK_STATE_CLOSING;\n\t\t\tcxgbit_send_halfclose(csk);\n\t\t} else {\n\t\t\tcsk->com.state = CSK_STATE_ABORTING;\n\t\t\tcxgbit_send_abort_req(csk);\n\t\t}\n\t\tbreak;\n\tcase CSK_STATE_CLOSING:\n\t\tcsk->com.state = CSK_STATE_MORIBUND;\n\t\tcxgbit_send_halfclose(csk);\n\t\tbreak;\n\tcase CSK_STATE_DEAD:\n\t\trelease = true;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s: csk %p; state %d\\n\",\n\t\t       __func__, csk, csk->com.state);\n\t}\n\tspin_unlock_bh(&csk->lock);\n\n\tif (release)\n\t\tcxgbit_put_csk(csk);\n}\n\nvoid cxgbit_free_conn(struct iscsit_conn *conn)\n{\n\t__cxgbit_free_conn(conn->context);\n}\n\nstatic void cxgbit_set_emss(struct cxgbit_sock *csk, u16 opt)\n{\n\tcsk->emss = csk->com.cdev->lldi.mtus[TCPOPT_MSS_G(opt)] -\n\t\t\t((csk->com.remote_addr.ss_family == AF_INET) ?\n\t\t\tsizeof(struct iphdr) : sizeof(struct ipv6hdr)) -\n\t\t\tsizeof(struct tcphdr);\n\tcsk->mss = csk->emss;\n\tif (TCPOPT_TSTAMP_G(opt))\n\t\tcsk->emss -= round_up(TCPOLEN_TIMESTAMP, 4);\n\tif (csk->emss < 128)\n\t\tcsk->emss = 128;\n\tif (csk->emss & 7)\n\t\tpr_info(\"Warning: misaligned mtu idx %u mss %u emss=%u\\n\",\n\t\t\tTCPOPT_MSS_G(opt), csk->mss, csk->emss);\n\tpr_debug(\"%s mss_idx %u mss %u emss=%u\\n\", __func__, TCPOPT_MSS_G(opt),\n\t\t csk->mss, csk->emss);\n}\n\nstatic void cxgbit_free_skb(struct cxgbit_sock *csk)\n{\n\tstruct sk_buff *skb;\n\n\t__skb_queue_purge(&csk->txq);\n\t__skb_queue_purge(&csk->rxq);\n\t__skb_queue_purge(&csk->backlogq);\n\t__skb_queue_purge(&csk->ppodq);\n\t__skb_queue_purge(&csk->skbq);\n\n\twhile ((skb = cxgbit_sock_dequeue_wr(csk)))\n\t\tkfree_skb(skb);\n\n\t__kfree_skb(csk->lro_hskb);\n}\n\nvoid _cxgbit_free_csk(struct kref *kref)\n{\n\tstruct cxgbit_sock *csk;\n\tstruct cxgbit_device *cdev;\n\n\tcsk = container_of(kref, struct cxgbit_sock, kref);\n\n\tpr_debug(\"%s csk %p state %d\\n\", __func__, csk, csk->com.state);\n\n\tif (csk->com.local_addr.ss_family == AF_INET6) {\n\t\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)\n\t\t\t\t\t     &csk->com.local_addr;\n\t\tcxgb4_clip_release(csk->com.cdev->lldi.ports[0],\n\t\t\t\t   (const u32 *)\n\t\t\t\t   &sin6->sin6_addr.s6_addr, 1);\n\t}\n\n\tcxgb4_remove_tid(csk->com.cdev->lldi.tids, 0, csk->tid,\n\t\t\t csk->com.local_addr.ss_family);\n\tdst_release(csk->dst);\n\tcxgb4_l2t_release(csk->l2t);\n\n\tcdev = csk->com.cdev;\n\tspin_lock_bh(&cdev->cskq.lock);\n\tlist_del(&csk->list);\n\tspin_unlock_bh(&cdev->cskq.lock);\n\n\tcxgbit_free_skb(csk);\n\tcxgbit_put_cnp(csk->cnp);\n\tcxgbit_put_cdev(cdev);\n\n\tkfree(csk);\n}\n\nstatic void cxgbit_set_tcp_window(struct cxgbit_sock *csk, struct port_info *pi)\n{\n\tunsigned int linkspeed;\n\tu8 scale;\n\n\tlinkspeed = pi->link_cfg.speed;\n\tscale = linkspeed / SPEED_10000;\n\n#define CXGBIT_10G_RCV_WIN (256 * 1024)\n\tcsk->rcv_win = CXGBIT_10G_RCV_WIN;\n\tif (scale)\n\t\tcsk->rcv_win *= scale;\n\tcsk->rcv_win = min(csk->rcv_win, RCV_BUFSIZ_M << 10);\n\n#define CXGBIT_10G_SND_WIN (256 * 1024)\n\tcsk->snd_win = CXGBIT_10G_SND_WIN;\n\tif (scale)\n\t\tcsk->snd_win *= scale;\n\tcsk->snd_win = min(csk->snd_win, 512U * 1024);\n\n\tpr_debug(\"%s snd_win %d rcv_win %d\\n\",\n\t\t __func__, csk->snd_win, csk->rcv_win);\n}\n\n#ifdef CONFIG_CHELSIO_T4_DCB\nstatic u8 cxgbit_get_iscsi_dcb_state(struct net_device *ndev)\n{\n\treturn ndev->dcbnl_ops->getstate(ndev);\n}\n\nstatic int cxgbit_select_priority(int pri_mask)\n{\n\tif (!pri_mask)\n\t\treturn 0;\n\n\treturn (ffs(pri_mask) - 1);\n}\n\nstatic u8 cxgbit_get_iscsi_dcb_priority(struct net_device *ndev, u16 local_port)\n{\n\tint ret;\n\tu8 caps;\n\n\tstruct dcb_app iscsi_dcb_app = {\n\t\t.protocol = local_port\n\t};\n\n\tret = (int)ndev->dcbnl_ops->getcap(ndev, DCB_CAP_ATTR_DCBX, &caps);\n\n\tif (ret)\n\t\treturn 0;\n\n\tif (caps & DCB_CAP_DCBX_VER_IEEE) {\n\t\tiscsi_dcb_app.selector = IEEE_8021QAZ_APP_SEL_STREAM;\n\t\tret = dcb_ieee_getapp_mask(ndev, &iscsi_dcb_app);\n\t\tif (!ret) {\n\t\t\tiscsi_dcb_app.selector = IEEE_8021QAZ_APP_SEL_ANY;\n\t\t\tret = dcb_ieee_getapp_mask(ndev, &iscsi_dcb_app);\n\t\t}\n\t} else if (caps & DCB_CAP_DCBX_VER_CEE) {\n\t\tiscsi_dcb_app.selector = DCB_APP_IDTYPE_PORTNUM;\n\n\t\tret = dcb_getapp(ndev, &iscsi_dcb_app);\n\t}\n\n\tpr_info(\"iSCSI priority is set to %u\\n\", cxgbit_select_priority(ret));\n\n\treturn cxgbit_select_priority(ret);\n}\n#endif\n\nstatic int\ncxgbit_offload_init(struct cxgbit_sock *csk, int iptype, __u8 *peer_ip,\n\t\t    u16 local_port, struct dst_entry *dst,\n\t\t    struct cxgbit_device *cdev)\n{\n\tstruct neighbour *n;\n\tint ret, step;\n\tstruct net_device *ndev;\n\tu16 rxq_idx, port_id;\n#ifdef CONFIG_CHELSIO_T4_DCB\n\tu8 priority = 0;\n#endif\n\n\tn = dst_neigh_lookup(dst, peer_ip);\n\tif (!n)\n\t\treturn -ENODEV;\n\n\trcu_read_lock();\n\tif (!(n->nud_state & NUD_VALID))\n\t\tneigh_event_send(n, NULL);\n\n\tret = -ENOMEM;\n\tif (n->dev->flags & IFF_LOOPBACK) {\n\t\tif (iptype == 4)\n\t\t\tndev = cxgbit_ipv4_netdev(*(__be32 *)peer_ip);\n\t\telse if (IS_ENABLED(CONFIG_IPV6))\n\t\t\tndev = cxgbit_ipv6_netdev((struct in6_addr *)peer_ip);\n\t\telse\n\t\t\tndev = NULL;\n\n\t\tif (!ndev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out;\n\t\t}\n\n\t\tcsk->l2t = cxgb4_l2t_get(cdev->lldi.l2t,\n\t\t\t\t\t n, ndev, 0);\n\t\tif (!csk->l2t)\n\t\t\tgoto out;\n\t\tcsk->mtu = ndev->mtu;\n\t\tcsk->tx_chan = cxgb4_port_chan(ndev);\n\t\tcsk->smac_idx =\n\t\t\t       ((struct port_info *)netdev_priv(ndev))->smt_idx;\n\t\tstep = cdev->lldi.ntxq /\n\t\t\tcdev->lldi.nchan;\n\t\tcsk->txq_idx = cxgb4_port_idx(ndev) * step;\n\t\tstep = cdev->lldi.nrxq /\n\t\t\tcdev->lldi.nchan;\n\t\tcsk->ctrlq_idx = cxgb4_port_idx(ndev);\n\t\tcsk->rss_qid = cdev->lldi.rxq_ids[\n\t\t\t\tcxgb4_port_idx(ndev) * step];\n\t\tcsk->port_id = cxgb4_port_idx(ndev);\n\t\tcxgbit_set_tcp_window(csk,\n\t\t\t\t      (struct port_info *)netdev_priv(ndev));\n\t} else {\n\t\tndev = cxgbit_get_real_dev(n->dev);\n\t\tif (!ndev) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto out;\n\t\t}\n\n#ifdef CONFIG_CHELSIO_T4_DCB\n\t\tif (cxgbit_get_iscsi_dcb_state(ndev))\n\t\t\tpriority = cxgbit_get_iscsi_dcb_priority(ndev,\n\t\t\t\t\t\t\t\t local_port);\n\n\t\tcsk->dcb_priority = priority;\n\n\t\tcsk->l2t = cxgb4_l2t_get(cdev->lldi.l2t, n, ndev, priority);\n#else\n\t\tcsk->l2t = cxgb4_l2t_get(cdev->lldi.l2t, n, ndev, 0);\n#endif\n\t\tif (!csk->l2t)\n\t\t\tgoto out;\n\t\tport_id = cxgb4_port_idx(ndev);\n\t\tcsk->mtu = dst_mtu(dst);\n\t\tcsk->tx_chan = cxgb4_port_chan(ndev);\n\t\tcsk->smac_idx =\n\t\t\t       ((struct port_info *)netdev_priv(ndev))->smt_idx;\n\t\tstep = cdev->lldi.ntxq /\n\t\t\tcdev->lldi.nports;\n\t\tcsk->txq_idx = (port_id * step) +\n\t\t\t\t(cdev->selectq[port_id][0]++ % step);\n\t\tcsk->ctrlq_idx = cxgb4_port_idx(ndev);\n\t\tstep = cdev->lldi.nrxq /\n\t\t\tcdev->lldi.nports;\n\t\trxq_idx = (port_id * step) +\n\t\t\t\t(cdev->selectq[port_id][1]++ % step);\n\t\tcsk->rss_qid = cdev->lldi.rxq_ids[rxq_idx];\n\t\tcsk->port_id = port_id;\n\t\tcxgbit_set_tcp_window(csk,\n\t\t\t\t      (struct port_info *)netdev_priv(ndev));\n\t}\n\tret = 0;\nout:\n\trcu_read_unlock();\n\tneigh_release(n);\n\treturn ret;\n}\n\nint cxgbit_ofld_send(struct cxgbit_device *cdev, struct sk_buff *skb)\n{\n\tint ret = 0;\n\n\tif (!test_bit(CDEV_STATE_UP, &cdev->flags)) {\n\t\tkfree_skb(skb);\n\t\tpr_err(\"%s - device not up - dropping\\n\", __func__);\n\t\treturn -EIO;\n\t}\n\n\tret = cxgb4_ofld_send(cdev->lldi.ports[0], skb);\n\tif (ret < 0)\n\t\tkfree_skb(skb);\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic void cxgbit_release_tid(struct cxgbit_device *cdev, u32 tid)\n{\n\tu32 len = roundup(sizeof(struct cpl_tid_release), 16);\n\tstruct sk_buff *skb;\n\n\tskb = alloc_skb(len, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn;\n\n\tcxgb_mk_tid_release(skb, len, tid, 0);\n\tcxgbit_ofld_send(cdev, skb);\n}\n\nint\ncxgbit_l2t_send(struct cxgbit_device *cdev, struct sk_buff *skb,\n\t\tstruct l2t_entry *l2e)\n{\n\tint ret = 0;\n\n\tif (!test_bit(CDEV_STATE_UP, &cdev->flags)) {\n\t\tkfree_skb(skb);\n\t\tpr_err(\"%s - device not up - dropping\\n\", __func__);\n\t\treturn -EIO;\n\t}\n\n\tret = cxgb4_l2t_send(cdev->lldi.ports[0], skb, l2e);\n\tif (ret < 0)\n\t\tkfree_skb(skb);\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic void cxgbit_send_rx_credits(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tif (csk->com.state != CSK_STATE_ESTABLISHED) {\n\t\t__kfree_skb(skb);\n\t\treturn;\n\t}\n\n\tcxgbit_ofld_send(csk->com.cdev, skb);\n}\n\n \nint cxgbit_rx_data_ack(struct cxgbit_sock *csk)\n{\n\tstruct sk_buff *skb;\n\tu32 len = roundup(sizeof(struct cpl_rx_data_ack), 16);\n\tu32 credit_dack;\n\n\tskb = alloc_skb(len, GFP_KERNEL);\n\tif (!skb)\n\t\treturn -1;\n\n\tcredit_dack = RX_DACK_CHANGE_F | RX_DACK_MODE_V(3) |\n\t\t      RX_CREDITS_V(csk->rx_credits);\n\n\tcxgb_mk_rx_data_ack(skb, len, csk->tid, csk->ctrlq_idx,\n\t\t\t    credit_dack);\n\n\tcsk->rx_credits = 0;\n\n\tspin_lock_bh(&csk->lock);\n\tif (csk->lock_owner) {\n\t\tcxgbit_skcb_rx_backlog_fn(skb) = cxgbit_send_rx_credits;\n\t\t__skb_queue_tail(&csk->backlogq, skb);\n\t\tspin_unlock_bh(&csk->lock);\n\t\treturn 0;\n\t}\n\n\tcxgbit_send_rx_credits(csk, skb);\n\tspin_unlock_bh(&csk->lock);\n\n\treturn 0;\n}\n\n#define FLOWC_WR_NPARAMS_MIN    9\n#define FLOWC_WR_NPARAMS_MAX\t11\nstatic int cxgbit_alloc_csk_skb(struct cxgbit_sock *csk)\n{\n\tstruct sk_buff *skb;\n\tu32 len, flowclen;\n\tu8 i;\n\n\tflowclen = offsetof(struct fw_flowc_wr,\n\t\t\t    mnemval[FLOWC_WR_NPARAMS_MAX]);\n\n\tlen = max_t(u32, sizeof(struct cpl_abort_req),\n\t\t    sizeof(struct cpl_abort_rpl));\n\n\tlen = max(len, flowclen);\n\tlen = roundup(len, 16);\n\n\tfor (i = 0; i < 3; i++) {\n\t\tskb = alloc_skb(len, GFP_ATOMIC);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t\t__skb_queue_tail(&csk->skbq, skb);\n\t}\n\n\tskb = alloc_skb(LRO_SKB_MIN_HEADROOM, GFP_ATOMIC);\n\tif (!skb)\n\t\tgoto out;\n\n\tmemset(skb->data, 0, LRO_SKB_MIN_HEADROOM);\n\tcsk->lro_hskb = skb;\n\n\treturn 0;\nout:\n\t__skb_queue_purge(&csk->skbq);\n\treturn -ENOMEM;\n}\n\nstatic void\ncxgbit_pass_accept_rpl(struct cxgbit_sock *csk, struct cpl_pass_accept_req *req)\n{\n\tstruct sk_buff *skb;\n\tconst struct tcphdr *tcph;\n\tstruct cpl_t5_pass_accept_rpl *rpl5;\n\tstruct cxgb4_lld_info *lldi = &csk->com.cdev->lldi;\n\tunsigned int len = roundup(sizeof(*rpl5), 16);\n\tunsigned int mtu_idx;\n\tu64 opt0;\n\tu32 opt2, hlen;\n\tu32 wscale;\n\tu32 win;\n\n\tpr_debug(\"%s csk %p tid %u\\n\", __func__, csk, csk->tid);\n\n\tskb = alloc_skb(len, GFP_ATOMIC);\n\tif (!skb) {\n\t\tcxgbit_put_csk(csk);\n\t\treturn;\n\t}\n\n\trpl5 = __skb_put_zero(skb, len);\n\n\tINIT_TP_WR(rpl5, csk->tid);\n\tOPCODE_TID(rpl5) = cpu_to_be32(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL,\n\t\t\t\t\t\t     csk->tid));\n\tcxgb_best_mtu(csk->com.cdev->lldi.mtus, csk->mtu, &mtu_idx,\n\t\t      req->tcpopt.tstamp,\n\t\t      (csk->com.remote_addr.ss_family == AF_INET) ? 0 : 1);\n\twscale = cxgb_compute_wscale(csk->rcv_win);\n\t \n\twin = csk->rcv_win >> 10;\n\tif (win > RCV_BUFSIZ_M)\n\t\twin = RCV_BUFSIZ_M;\n\topt0 =  TCAM_BYPASS_F |\n\t\tWND_SCALE_V(wscale) |\n\t\tMSS_IDX_V(mtu_idx) |\n\t\tL2T_IDX_V(csk->l2t->idx) |\n\t\tTX_CHAN_V(csk->tx_chan) |\n\t\tSMAC_SEL_V(csk->smac_idx) |\n\t\tDSCP_V(csk->tos >> 2) |\n\t\tULP_MODE_V(ULP_MODE_ISCSI) |\n\t\tRCV_BUFSIZ_V(win);\n\n\topt2 = RX_CHANNEL_V(0) |\n\t\tRSS_QUEUE_VALID_F | RSS_QUEUE_V(csk->rss_qid);\n\n\tif (!is_t5(lldi->adapter_type))\n\t\topt2 |= RX_FC_DISABLE_F;\n\n\tif (req->tcpopt.tstamp)\n\t\topt2 |= TSTAMPS_EN_F;\n\tif (req->tcpopt.sack)\n\t\topt2 |= SACK_EN_F;\n\tif (wscale)\n\t\topt2 |= WND_SCALE_EN_F;\n\n\thlen = ntohl(req->hdr_len);\n\n\tif (is_t5(lldi->adapter_type))\n\t\ttcph = (struct tcphdr *)((u8 *)(req + 1) +\n\t\t       ETH_HDR_LEN_G(hlen) + IP_HDR_LEN_G(hlen));\n\telse\n\t\ttcph = (struct tcphdr *)((u8 *)(req + 1) +\n\t\t       T6_ETH_HDR_LEN_G(hlen) + T6_IP_HDR_LEN_G(hlen));\n\n\tif (tcph->ece && tcph->cwr)\n\t\topt2 |= CCTRL_ECN_V(1);\n\n\topt2 |= CONG_CNTRL_V(CONG_ALG_NEWRENO);\n\n\topt2 |= T5_ISS_F;\n\trpl5->iss = cpu_to_be32((get_random_u32() & ~7UL) - 1);\n\n\topt2 |= T5_OPT_2_VALID_F;\n\n\trpl5->opt0 = cpu_to_be64(opt0);\n\trpl5->opt2 = cpu_to_be32(opt2);\n\tset_wr_txq(skb, CPL_PRIORITY_SETUP, csk->ctrlq_idx);\n\tt4_set_arp_err_handler(skb, csk, cxgbit_arp_failure_discard);\n\tcxgbit_l2t_send(csk->com.cdev, skb, csk->l2t);\n}\n\nstatic void\ncxgbit_pass_accept_req(struct cxgbit_device *cdev, struct sk_buff *skb)\n{\n\tstruct cxgbit_sock *csk = NULL;\n\tstruct cxgbit_np *cnp;\n\tstruct cpl_pass_accept_req *req = cplhdr(skb);\n\tunsigned int stid = PASS_OPEN_TID_G(ntohl(req->tos_stid));\n\tstruct tid_info *t = cdev->lldi.tids;\n\tunsigned int tid = GET_TID(req);\n\tu16 peer_mss = ntohs(req->tcpopt.mss);\n\tunsigned short hdrs;\n\n\tstruct dst_entry *dst;\n\t__u8 local_ip[16], peer_ip[16];\n\t__be16 local_port, peer_port;\n\tint ret;\n\tint iptype;\n\n\tpr_debug(\"%s: cdev = %p; stid = %u; tid = %u\\n\",\n\t\t __func__, cdev, stid, tid);\n\n\tcnp = lookup_stid(t, stid);\n\tif (!cnp) {\n\t\tpr_err(\"%s connect request on invalid stid %d\\n\",\n\t\t       __func__, stid);\n\t\tgoto rel_skb;\n\t}\n\n\tif (cnp->com.state != CSK_STATE_LISTEN) {\n\t\tpr_err(\"%s - listening parent not in CSK_STATE_LISTEN\\n\",\n\t\t       __func__);\n\t\tgoto reject;\n\t}\n\n\tcsk = lookup_tid(t, tid);\n\tif (csk) {\n\t\tpr_err(\"%s csk not null tid %u\\n\",\n\t\t       __func__, tid);\n\t\tgoto rel_skb;\n\t}\n\n\tcxgb_get_4tuple(req, cdev->lldi.adapter_type, &iptype, local_ip,\n\t\t\tpeer_ip, &local_port, &peer_port);\n\n\t \n\tif (iptype == 4)  {\n\t\tpr_debug(\"%s parent sock %p tid %u laddr %pI4 raddr %pI4 \"\n\t\t\t \"lport %d rport %d peer_mss %d\\n\"\n\t\t\t , __func__, cnp, tid,\n\t\t\t local_ip, peer_ip, ntohs(local_port),\n\t\t\t ntohs(peer_port), peer_mss);\n\t\tdst = cxgb_find_route(&cdev->lldi, cxgbit_get_real_dev,\n\t\t\t\t      *(__be32 *)local_ip,\n\t\t\t\t      *(__be32 *)peer_ip,\n\t\t\t\t      local_port, peer_port,\n\t\t\t\t      PASS_OPEN_TOS_G(ntohl(req->tos_stid)));\n\t} else {\n\t\tpr_debug(\"%s parent sock %p tid %u laddr %pI6 raddr %pI6 \"\n\t\t\t \"lport %d rport %d peer_mss %d\\n\"\n\t\t\t , __func__, cnp, tid,\n\t\t\t local_ip, peer_ip, ntohs(local_port),\n\t\t\t ntohs(peer_port), peer_mss);\n\t\tdst = cxgb_find_route6(&cdev->lldi, cxgbit_get_real_dev,\n\t\t\t\t       local_ip, peer_ip,\n\t\t\t\t       local_port, peer_port,\n\t\t\t\t       PASS_OPEN_TOS_G(ntohl(req->tos_stid)),\n\t\t\t\t       ((struct sockaddr_in6 *)\n\t\t\t\t\t&cnp->com.local_addr)->sin6_scope_id);\n\t}\n\tif (!dst) {\n\t\tpr_err(\"%s - failed to find dst entry!\\n\",\n\t\t       __func__);\n\t\tgoto reject;\n\t}\n\n\tcsk = kzalloc(sizeof(*csk), GFP_ATOMIC);\n\tif (!csk) {\n\t\tdst_release(dst);\n\t\tgoto rel_skb;\n\t}\n\n\tret = cxgbit_offload_init(csk, iptype, peer_ip, ntohs(local_port),\n\t\t\t\t  dst, cdev);\n\tif (ret) {\n\t\tpr_err(\"%s - failed to allocate l2t entry!\\n\",\n\t\t       __func__);\n\t\tdst_release(dst);\n\t\tkfree(csk);\n\t\tgoto reject;\n\t}\n\n\tkref_init(&csk->kref);\n\tinit_completion(&csk->com.wr_wait.completion);\n\n\tINIT_LIST_HEAD(&csk->accept_node);\n\n\thdrs = (iptype == 4 ? sizeof(struct iphdr) : sizeof(struct ipv6hdr)) +\n\t\tsizeof(struct tcphdr) +\t(req->tcpopt.tstamp ? 12 : 0);\n\tif (peer_mss && csk->mtu > (peer_mss + hdrs))\n\t\tcsk->mtu = peer_mss + hdrs;\n\n\tcsk->com.state = CSK_STATE_CONNECTING;\n\tcsk->com.cdev = cdev;\n\tcsk->cnp = cnp;\n\tcsk->tos = PASS_OPEN_TOS_G(ntohl(req->tos_stid));\n\tcsk->dst = dst;\n\tcsk->tid = tid;\n\tcsk->wr_cred = cdev->lldi.wr_cred -\n\t\t\tDIV_ROUND_UP(sizeof(struct cpl_abort_req), 16);\n\tcsk->wr_max_cred = csk->wr_cred;\n\tcsk->wr_una_cred = 0;\n\n\tif (iptype == 4) {\n\t\tstruct sockaddr_in *sin = (struct sockaddr_in *)\n\t\t\t\t\t  &csk->com.local_addr;\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_port = local_port;\n\t\tsin->sin_addr.s_addr = *(__be32 *)local_ip;\n\n\t\tsin = (struct sockaddr_in *)&csk->com.remote_addr;\n\t\tsin->sin_family = AF_INET;\n\t\tsin->sin_port = peer_port;\n\t\tsin->sin_addr.s_addr = *(__be32 *)peer_ip;\n\t} else {\n\t\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)\n\t\t\t\t\t    &csk->com.local_addr;\n\n\t\tsin6->sin6_family = PF_INET6;\n\t\tsin6->sin6_port = local_port;\n\t\tmemcpy(sin6->sin6_addr.s6_addr, local_ip, 16);\n\t\tcxgb4_clip_get(cdev->lldi.ports[0],\n\t\t\t       (const u32 *)&sin6->sin6_addr.s6_addr,\n\t\t\t       1);\n\n\t\tsin6 = (struct sockaddr_in6 *)&csk->com.remote_addr;\n\t\tsin6->sin6_family = PF_INET6;\n\t\tsin6->sin6_port = peer_port;\n\t\tmemcpy(sin6->sin6_addr.s6_addr, peer_ip, 16);\n\t}\n\n\tskb_queue_head_init(&csk->rxq);\n\tskb_queue_head_init(&csk->txq);\n\tskb_queue_head_init(&csk->ppodq);\n\tskb_queue_head_init(&csk->backlogq);\n\tskb_queue_head_init(&csk->skbq);\n\tcxgbit_sock_reset_wr_list(csk);\n\tspin_lock_init(&csk->lock);\n\tinit_waitqueue_head(&csk->waitq);\n\tcsk->lock_owner = false;\n\n\tif (cxgbit_alloc_csk_skb(csk)) {\n\t\tdst_release(dst);\n\t\tkfree(csk);\n\t\tgoto rel_skb;\n\t}\n\n\tcxgbit_get_cnp(cnp);\n\tcxgbit_get_cdev(cdev);\n\n\tspin_lock(&cdev->cskq.lock);\n\tlist_add_tail(&csk->list, &cdev->cskq.list);\n\tspin_unlock(&cdev->cskq.lock);\n\tcxgb4_insert_tid(t, csk, tid, csk->com.local_addr.ss_family);\n\tcxgbit_pass_accept_rpl(csk, req);\n\tgoto rel_skb;\n\nreject:\n\tcxgbit_release_tid(cdev, tid);\nrel_skb:\n\t__kfree_skb(skb);\n}\n\nstatic u32\ncxgbit_tx_flowc_wr_credits(struct cxgbit_sock *csk, u32 *nparamsp,\n\t\t\t   u32 *flowclenp)\n{\n\tu32 nparams, flowclen16, flowclen;\n\n\tnparams = FLOWC_WR_NPARAMS_MIN;\n\n\tif (csk->snd_wscale)\n\t\tnparams++;\n\n#ifdef CONFIG_CHELSIO_T4_DCB\n\tnparams++;\n#endif\n\tflowclen = offsetof(struct fw_flowc_wr, mnemval[nparams]);\n\tflowclen16 = DIV_ROUND_UP(flowclen, 16);\n\tflowclen = flowclen16 * 16;\n\t \n\tif (nparamsp)\n\t\t*nparamsp = nparams;\n\tif (flowclenp)\n\t\t*flowclenp = flowclen;\n\treturn flowclen16;\n}\n\nu32 cxgbit_send_tx_flowc_wr(struct cxgbit_sock *csk)\n{\n\tstruct cxgbit_device *cdev = csk->com.cdev;\n\tstruct fw_flowc_wr *flowc;\n\tu32 nparams, flowclen16, flowclen;\n\tstruct sk_buff *skb;\n\tu8 index;\n\n#ifdef CONFIG_CHELSIO_T4_DCB\n\tu16 vlan = ((struct l2t_entry *)csk->l2t)->vlan;\n#endif\n\n\tflowclen16 = cxgbit_tx_flowc_wr_credits(csk, &nparams, &flowclen);\n\n\tskb = __skb_dequeue(&csk->skbq);\n\tflowc = __skb_put_zero(skb, flowclen);\n\n\tflowc->op_to_nparams = cpu_to_be32(FW_WR_OP_V(FW_FLOWC_WR) |\n\t\t\t\t\t   FW_FLOWC_WR_NPARAMS_V(nparams));\n\tflowc->flowid_len16 = cpu_to_be32(FW_WR_LEN16_V(flowclen16) |\n\t\t\t\t\t  FW_WR_FLOWID_V(csk->tid));\n\tflowc->mnemval[0].mnemonic = FW_FLOWC_MNEM_PFNVFN;\n\tflowc->mnemval[0].val = cpu_to_be32(FW_PFVF_CMD_PFN_V\n\t\t\t\t\t    (csk->com.cdev->lldi.pf));\n\tflowc->mnemval[1].mnemonic = FW_FLOWC_MNEM_CH;\n\tflowc->mnemval[1].val = cpu_to_be32(csk->tx_chan);\n\tflowc->mnemval[2].mnemonic = FW_FLOWC_MNEM_PORT;\n\tflowc->mnemval[2].val = cpu_to_be32(csk->tx_chan);\n\tflowc->mnemval[3].mnemonic = FW_FLOWC_MNEM_IQID;\n\tflowc->mnemval[3].val = cpu_to_be32(csk->rss_qid);\n\tflowc->mnemval[4].mnemonic = FW_FLOWC_MNEM_SNDNXT;\n\tflowc->mnemval[4].val = cpu_to_be32(csk->snd_nxt);\n\tflowc->mnemval[5].mnemonic = FW_FLOWC_MNEM_RCVNXT;\n\tflowc->mnemval[5].val = cpu_to_be32(csk->rcv_nxt);\n\tflowc->mnemval[6].mnemonic = FW_FLOWC_MNEM_SNDBUF;\n\tflowc->mnemval[6].val = cpu_to_be32(csk->snd_win);\n\tflowc->mnemval[7].mnemonic = FW_FLOWC_MNEM_MSS;\n\tflowc->mnemval[7].val = cpu_to_be32(csk->emss);\n\n\tflowc->mnemval[8].mnemonic = FW_FLOWC_MNEM_TXDATAPLEN_MAX;\n\tif (test_bit(CDEV_ISO_ENABLE, &cdev->flags))\n\t\tflowc->mnemval[8].val = cpu_to_be32(CXGBIT_MAX_ISO_PAYLOAD);\n\telse\n\t\tflowc->mnemval[8].val = cpu_to_be32(16384);\n\n\tindex = 9;\n\n\tif (csk->snd_wscale) {\n\t\tflowc->mnemval[index].mnemonic = FW_FLOWC_MNEM_RCV_SCALE;\n\t\tflowc->mnemval[index].val = cpu_to_be32(csk->snd_wscale);\n\t\tindex++;\n\t}\n\n#ifdef CONFIG_CHELSIO_T4_DCB\n\tflowc->mnemval[index].mnemonic = FW_FLOWC_MNEM_DCBPRIO;\n\tif (vlan == VLAN_NONE) {\n\t\tpr_warn(\"csk %u without VLAN Tag on DCB Link\\n\", csk->tid);\n\t\tflowc->mnemval[index].val = cpu_to_be32(0);\n\t} else\n\t\tflowc->mnemval[index].val = cpu_to_be32(\n\t\t\t\t(vlan & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT);\n#endif\n\n\tpr_debug(\"%s: csk %p; tx_chan = %u; rss_qid = %u; snd_seq = %u;\"\n\t\t \" rcv_seq = %u; snd_win = %u; emss = %u\\n\",\n\t\t __func__, csk, csk->tx_chan, csk->rss_qid, csk->snd_nxt,\n\t\t csk->rcv_nxt, csk->snd_win, csk->emss);\n\tset_wr_txq(skb, CPL_PRIORITY_DATA, csk->txq_idx);\n\tcxgbit_ofld_send(csk->com.cdev, skb);\n\treturn flowclen16;\n}\n\nstatic int\ncxgbit_send_tcb_skb(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tspin_lock_bh(&csk->lock);\n\tif (unlikely(csk->com.state != CSK_STATE_ESTABLISHED)) {\n\t\tspin_unlock_bh(&csk->lock);\n\t\tpr_err(\"%s: csk 0x%p, tid %u, state %u\\n\",\n\t\t       __func__, csk, csk->tid, csk->com.state);\n\t\t__kfree_skb(skb);\n\t\treturn -1;\n\t}\n\n\tcxgbit_get_csk(csk);\n\tcxgbit_init_wr_wait(&csk->com.wr_wait);\n\tcxgbit_ofld_send(csk->com.cdev, skb);\n\tspin_unlock_bh(&csk->lock);\n\n\treturn 0;\n}\n\nint cxgbit_setup_conn_digest(struct cxgbit_sock *csk)\n{\n\tstruct sk_buff *skb;\n\tstruct cpl_set_tcb_field *req;\n\tu8 hcrc = csk->submode & CXGBIT_SUBMODE_HCRC;\n\tu8 dcrc = csk->submode & CXGBIT_SUBMODE_DCRC;\n\tunsigned int len = roundup(sizeof(*req), 16);\n\tint ret;\n\n\tskb = alloc_skb(len, GFP_KERNEL);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\t \n\treq = __skb_put_zero(skb, len);\n\n\tINIT_TP_WR(req, csk->tid);\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, csk->tid));\n\treq->reply_ctrl = htons(NO_REPLY_V(0) | QUEUENO_V(csk->rss_qid));\n\treq->word_cookie = htons(0);\n\treq->mask = cpu_to_be64(0x3 << 4);\n\treq->val = cpu_to_be64(((hcrc ? ULP_CRC_HEADER : 0) |\n\t\t\t\t(dcrc ? ULP_CRC_DATA : 0)) << 4);\n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, csk->ctrlq_idx);\n\n\tif (cxgbit_send_tcb_skb(csk, skb))\n\t\treturn -1;\n\n\tret = cxgbit_wait_for_reply(csk->com.cdev,\n\t\t\t\t    &csk->com.wr_wait,\n\t\t\t\t    csk->tid, 5, __func__);\n\tif (ret)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nint cxgbit_setup_conn_pgidx(struct cxgbit_sock *csk, u32 pg_idx)\n{\n\tstruct sk_buff *skb;\n\tstruct cpl_set_tcb_field *req;\n\tunsigned int len = roundup(sizeof(*req), 16);\n\tint ret;\n\n\tskb = alloc_skb(len, GFP_KERNEL);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\treq = __skb_put_zero(skb, len);\n\n\tINIT_TP_WR(req, csk->tid);\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, csk->tid));\n\treq->reply_ctrl = htons(NO_REPLY_V(0) | QUEUENO_V(csk->rss_qid));\n\treq->word_cookie = htons(0);\n\treq->mask = cpu_to_be64(0x3 << 8);\n\treq->val = cpu_to_be64(pg_idx << 8);\n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, csk->ctrlq_idx);\n\n\tif (cxgbit_send_tcb_skb(csk, skb))\n\t\treturn -1;\n\n\tret = cxgbit_wait_for_reply(csk->com.cdev,\n\t\t\t\t    &csk->com.wr_wait,\n\t\t\t\t    csk->tid, 5, __func__);\n\tif (ret)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic void\ncxgbit_pass_open_rpl(struct cxgbit_device *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_pass_open_rpl *rpl = cplhdr(skb);\n\tstruct tid_info *t = cdev->lldi.tids;\n\tunsigned int stid = GET_TID(rpl);\n\tstruct cxgbit_np *cnp = lookup_stid(t, stid);\n\n\tpr_debug(\"%s: cnp = %p; stid = %u; status = %d\\n\",\n\t\t __func__, cnp, stid, rpl->status);\n\n\tif (!cnp) {\n\t\tpr_info(\"%s stid %d lookup failure\\n\", __func__, stid);\n\t\tgoto rel_skb;\n\t}\n\n\tcxgbit_wake_up(&cnp->com.wr_wait, __func__, rpl->status);\n\tcxgbit_put_cnp(cnp);\nrel_skb:\n\t__kfree_skb(skb);\n}\n\nstatic void\ncxgbit_close_listsrv_rpl(struct cxgbit_device *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_close_listsvr_rpl *rpl = cplhdr(skb);\n\tstruct tid_info *t = cdev->lldi.tids;\n\tunsigned int stid = GET_TID(rpl);\n\tstruct cxgbit_np *cnp = lookup_stid(t, stid);\n\n\tpr_debug(\"%s: cnp = %p; stid = %u; status = %d\\n\",\n\t\t __func__, cnp, stid, rpl->status);\n\n\tif (!cnp) {\n\t\tpr_info(\"%s stid %d lookup failure\\n\", __func__, stid);\n\t\tgoto rel_skb;\n\t}\n\n\tcxgbit_wake_up(&cnp->com.wr_wait, __func__, rpl->status);\n\tcxgbit_put_cnp(cnp);\nrel_skb:\n\t__kfree_skb(skb);\n}\n\nstatic void\ncxgbit_pass_establish(struct cxgbit_device *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_pass_establish *req = cplhdr(skb);\n\tstruct tid_info *t = cdev->lldi.tids;\n\tunsigned int tid = GET_TID(req);\n\tstruct cxgbit_sock *csk;\n\tstruct cxgbit_np *cnp;\n\tu16 tcp_opt = be16_to_cpu(req->tcp_opt);\n\tu32 snd_isn = be32_to_cpu(req->snd_isn);\n\tu32 rcv_isn = be32_to_cpu(req->rcv_isn);\n\n\tcsk = lookup_tid(t, tid);\n\tif (unlikely(!csk)) {\n\t\tpr_err(\"can't find connection for tid %u.\\n\", tid);\n\t\tgoto rel_skb;\n\t}\n\tcnp = csk->cnp;\n\n\tpr_debug(\"%s: csk %p; tid %u; cnp %p\\n\",\n\t\t __func__, csk, tid, cnp);\n\n\tcsk->write_seq = snd_isn;\n\tcsk->snd_una = snd_isn;\n\tcsk->snd_nxt = snd_isn;\n\n\tcsk->rcv_nxt = rcv_isn;\n\n\tcsk->snd_wscale = TCPOPT_SND_WSCALE_G(tcp_opt);\n\tcxgbit_set_emss(csk, tcp_opt);\n\tdst_confirm(csk->dst);\n\tcsk->com.state = CSK_STATE_ESTABLISHED;\n\tspin_lock_bh(&cnp->np_accept_lock);\n\tlist_add_tail(&csk->accept_node, &cnp->np_accept_list);\n\tspin_unlock_bh(&cnp->np_accept_lock);\n\tcomplete(&cnp->accept_comp);\nrel_skb:\n\t__kfree_skb(skb);\n}\n\nstatic void cxgbit_queue_rx_skb(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tcxgbit_skcb_flags(skb) = 0;\n\tspin_lock_bh(&csk->rxq.lock);\n\t__skb_queue_tail(&csk->rxq, skb);\n\tspin_unlock_bh(&csk->rxq.lock);\n\twake_up(&csk->waitq);\n}\n\nstatic void cxgbit_peer_close(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tpr_debug(\"%s: csk %p; tid %u; state %d\\n\",\n\t\t __func__, csk, csk->tid, csk->com.state);\n\n\tswitch (csk->com.state) {\n\tcase CSK_STATE_ESTABLISHED:\n\t\tcsk->com.state = CSK_STATE_CLOSING;\n\t\tcxgbit_queue_rx_skb(csk, skb);\n\t\treturn;\n\tcase CSK_STATE_CLOSING:\n\t\t \n\t\tcsk->com.state = CSK_STATE_MORIBUND;\n\t\tbreak;\n\tcase CSK_STATE_MORIBUND:\n\t\tcsk->com.state = CSK_STATE_DEAD;\n\t\tcxgbit_put_csk(csk);\n\t\tbreak;\n\tcase CSK_STATE_ABORTING:\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"%s: cpl_peer_close in bad state %d\\n\",\n\t\t\t__func__, csk->com.state);\n\t}\n\n\t__kfree_skb(skb);\n}\n\nstatic void cxgbit_close_con_rpl(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tpr_debug(\"%s: csk %p; tid %u; state %d\\n\",\n\t\t __func__, csk, csk->tid, csk->com.state);\n\n\tswitch (csk->com.state) {\n\tcase CSK_STATE_CLOSING:\n\t\tcsk->com.state = CSK_STATE_MORIBUND;\n\t\tbreak;\n\tcase CSK_STATE_MORIBUND:\n\t\tcsk->com.state = CSK_STATE_DEAD;\n\t\tcxgbit_put_csk(csk);\n\t\tbreak;\n\tcase CSK_STATE_ABORTING:\n\tcase CSK_STATE_DEAD:\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"%s: cpl_close_con_rpl in bad state %d\\n\",\n\t\t\t__func__, csk->com.state);\n\t}\n\n\t__kfree_skb(skb);\n}\n\nstatic void cxgbit_abort_req_rss(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tstruct cpl_abort_req_rss *hdr = cplhdr(skb);\n\tunsigned int tid = GET_TID(hdr);\n\tstruct sk_buff *rpl_skb;\n\tbool release = false;\n\tbool wakeup_thread = false;\n\tu32 len = roundup(sizeof(struct cpl_abort_rpl), 16);\n\n\tpr_debug(\"%s: csk %p; tid %u; state %d\\n\",\n\t\t __func__, csk, tid, csk->com.state);\n\n\tif (cxgb_is_neg_adv(hdr->status)) {\n\t\tpr_err(\"%s: got neg advise %d on tid %u\\n\",\n\t\t       __func__, hdr->status, tid);\n\t\tgoto rel_skb;\n\t}\n\n\tswitch (csk->com.state) {\n\tcase CSK_STATE_CONNECTING:\n\tcase CSK_STATE_MORIBUND:\n\t\tcsk->com.state = CSK_STATE_DEAD;\n\t\trelease = true;\n\t\tbreak;\n\tcase CSK_STATE_ESTABLISHED:\n\t\tcsk->com.state = CSK_STATE_DEAD;\n\t\twakeup_thread = true;\n\t\tbreak;\n\tcase CSK_STATE_CLOSING:\n\t\tcsk->com.state = CSK_STATE_DEAD;\n\t\tif (!csk->conn)\n\t\t\trelease = true;\n\t\tbreak;\n\tcase CSK_STATE_ABORTING:\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"%s: cpl_abort_req_rss in bad state %d\\n\",\n\t\t\t__func__, csk->com.state);\n\t\tcsk->com.state = CSK_STATE_DEAD;\n\t}\n\n\t__skb_queue_purge(&csk->txq);\n\n\tif (!test_and_set_bit(CSK_TX_DATA_SENT, &csk->com.flags))\n\t\tcxgbit_send_tx_flowc_wr(csk);\n\n\trpl_skb = __skb_dequeue(&csk->skbq);\n\n\tcxgb_mk_abort_rpl(rpl_skb, len, csk->tid, csk->txq_idx);\n\tcxgbit_ofld_send(csk->com.cdev, rpl_skb);\n\n\tif (wakeup_thread) {\n\t\tcxgbit_queue_rx_skb(csk, skb);\n\t\treturn;\n\t}\n\n\tif (release)\n\t\tcxgbit_put_csk(csk);\nrel_skb:\n\t__kfree_skb(skb);\n}\n\nstatic void cxgbit_abort_rpl_rss(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tstruct cpl_abort_rpl_rss *rpl = cplhdr(skb);\n\n\tpr_debug(\"%s: csk %p; tid %u; state %d\\n\",\n\t\t __func__, csk, csk->tid, csk->com.state);\n\n\tswitch (csk->com.state) {\n\tcase CSK_STATE_ABORTING:\n\t\tcsk->com.state = CSK_STATE_DEAD;\n\t\tif (test_bit(CSK_ABORT_RPL_WAIT, &csk->com.flags))\n\t\t\tcxgbit_wake_up(&csk->com.wr_wait, __func__,\n\t\t\t\t       rpl->status);\n\t\tcxgbit_put_csk(csk);\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"%s: cpl_abort_rpl_rss in state %d\\n\",\n\t\t\t__func__, csk->com.state);\n\t}\n\n\t__kfree_skb(skb);\n}\n\nstatic bool cxgbit_credit_err(const struct cxgbit_sock *csk)\n{\n\tconst struct sk_buff *skb = csk->wr_pending_head;\n\tu32 credit = 0;\n\n\tif (unlikely(csk->wr_cred > csk->wr_max_cred)) {\n\t\tpr_err(\"csk 0x%p, tid %u, credit %u > %u\\n\",\n\t\t       csk, csk->tid, csk->wr_cred, csk->wr_max_cred);\n\t\treturn true;\n\t}\n\n\twhile (skb) {\n\t\tcredit += (__force u32)skb->csum;\n\t\tskb = cxgbit_skcb_tx_wr_next(skb);\n\t}\n\n\tif (unlikely((csk->wr_cred + credit) != csk->wr_max_cred)) {\n\t\tpr_err(\"csk 0x%p, tid %u, credit %u + %u != %u.\\n\",\n\t\t       csk, csk->tid, csk->wr_cred,\n\t\t       credit, csk->wr_max_cred);\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void cxgbit_fw4_ack(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tstruct cpl_fw4_ack *rpl = (struct cpl_fw4_ack *)cplhdr(skb);\n\tu32 credits = rpl->credits;\n\tu32 snd_una = ntohl(rpl->snd_una);\n\n\tcsk->wr_cred += credits;\n\tif (csk->wr_una_cred > (csk->wr_max_cred - csk->wr_cred))\n\t\tcsk->wr_una_cred = csk->wr_max_cred - csk->wr_cred;\n\n\twhile (credits) {\n\t\tstruct sk_buff *p = cxgbit_sock_peek_wr(csk);\n\t\tu32 csum;\n\n\t\tif (unlikely(!p)) {\n\t\t\tpr_err(\"csk 0x%p,%u, cr %u,%u+%u, empty.\\n\",\n\t\t\t       csk, csk->tid, credits,\n\t\t\t       csk->wr_cred, csk->wr_una_cred);\n\t\t\tbreak;\n\t\t}\n\n\t\tcsum = (__force u32)p->csum;\n\t\tif (unlikely(credits < csum)) {\n\t\t\tpr_warn(\"csk 0x%p,%u, cr %u,%u+%u, < %u.\\n\",\n\t\t\t\tcsk,  csk->tid,\n\t\t\t\tcredits, csk->wr_cred, csk->wr_una_cred,\n\t\t\t\tcsum);\n\t\t\tp->csum = (__force __wsum)(csum - credits);\n\t\t\tbreak;\n\t\t}\n\n\t\tcxgbit_sock_dequeue_wr(csk);\n\t\tcredits -= csum;\n\t\tkfree_skb(p);\n\t}\n\n\tif (unlikely(cxgbit_credit_err(csk))) {\n\t\tcxgbit_queue_rx_skb(csk, skb);\n\t\treturn;\n\t}\n\n\tif (rpl->seq_vld & CPL_FW4_ACK_FLAGS_SEQVAL) {\n\t\tif (unlikely(before(snd_una, csk->snd_una))) {\n\t\t\tpr_warn(\"csk 0x%p,%u, snd_una %u/%u.\",\n\t\t\t\tcsk, csk->tid, snd_una,\n\t\t\t\tcsk->snd_una);\n\t\t\tgoto rel_skb;\n\t\t}\n\n\t\tif (csk->snd_una != snd_una) {\n\t\t\tcsk->snd_una = snd_una;\n\t\t\tdst_confirm(csk->dst);\n\t\t}\n\t}\n\n\tif (skb_queue_len(&csk->txq))\n\t\tcxgbit_push_tx_frames(csk);\n\nrel_skb:\n\t__kfree_skb(skb);\n}\n\nstatic void cxgbit_set_tcb_rpl(struct cxgbit_device *cdev, struct sk_buff *skb)\n{\n\tstruct cxgbit_sock *csk;\n\tstruct cpl_set_tcb_rpl *rpl = (struct cpl_set_tcb_rpl *)skb->data;\n\tunsigned int tid = GET_TID(rpl);\n\tstruct cxgb4_lld_info *lldi = &cdev->lldi;\n\tstruct tid_info *t = lldi->tids;\n\n\tcsk = lookup_tid(t, tid);\n\tif (unlikely(!csk)) {\n\t\tpr_err(\"can't find connection for tid %u.\\n\", tid);\n\t\tgoto rel_skb;\n\t} else {\n\t\tcxgbit_wake_up(&csk->com.wr_wait, __func__, rpl->status);\n\t}\n\n\tcxgbit_put_csk(csk);\nrel_skb:\n\t__kfree_skb(skb);\n}\n\nstatic void cxgbit_rx_data(struct cxgbit_device *cdev, struct sk_buff *skb)\n{\n\tstruct cxgbit_sock *csk;\n\tstruct cpl_rx_data *cpl = cplhdr(skb);\n\tunsigned int tid = GET_TID(cpl);\n\tstruct cxgb4_lld_info *lldi = &cdev->lldi;\n\tstruct tid_info *t = lldi->tids;\n\n\tcsk = lookup_tid(t, tid);\n\tif (unlikely(!csk)) {\n\t\tpr_err(\"can't find conn. for tid %u.\\n\", tid);\n\t\tgoto rel_skb;\n\t}\n\n\tcxgbit_queue_rx_skb(csk, skb);\n\treturn;\nrel_skb:\n\t__kfree_skb(skb);\n}\n\nstatic void\n__cxgbit_process_rx_cpl(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tspin_lock(&csk->lock);\n\tif (csk->lock_owner) {\n\t\t__skb_queue_tail(&csk->backlogq, skb);\n\t\tspin_unlock(&csk->lock);\n\t\treturn;\n\t}\n\n\tcxgbit_skcb_rx_backlog_fn(skb)(csk, skb);\n\tspin_unlock(&csk->lock);\n}\n\nstatic void cxgbit_process_rx_cpl(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tcxgbit_get_csk(csk);\n\t__cxgbit_process_rx_cpl(csk, skb);\n\tcxgbit_put_csk(csk);\n}\n\nstatic void cxgbit_rx_cpl(struct cxgbit_device *cdev, struct sk_buff *skb)\n{\n\tstruct cxgbit_sock *csk;\n\tstruct cpl_tx_data *cpl = cplhdr(skb);\n\tstruct cxgb4_lld_info *lldi = &cdev->lldi;\n\tstruct tid_info *t = lldi->tids;\n\tunsigned int tid = GET_TID(cpl);\n\tu8 opcode = cxgbit_skcb_rx_opcode(skb);\n\tbool ref = true;\n\n\tswitch (opcode) {\n\tcase CPL_FW4_ACK:\n\t\t\tcxgbit_skcb_rx_backlog_fn(skb) = cxgbit_fw4_ack;\n\t\t\tref = false;\n\t\t\tbreak;\n\tcase CPL_PEER_CLOSE:\n\t\t\tcxgbit_skcb_rx_backlog_fn(skb) = cxgbit_peer_close;\n\t\t\tbreak;\n\tcase CPL_CLOSE_CON_RPL:\n\t\t\tcxgbit_skcb_rx_backlog_fn(skb) = cxgbit_close_con_rpl;\n\t\t\tbreak;\n\tcase CPL_ABORT_REQ_RSS:\n\t\t\tcxgbit_skcb_rx_backlog_fn(skb) = cxgbit_abort_req_rss;\n\t\t\tbreak;\n\tcase CPL_ABORT_RPL_RSS:\n\t\t\tcxgbit_skcb_rx_backlog_fn(skb) = cxgbit_abort_rpl_rss;\n\t\t\tbreak;\n\tdefault:\n\t\tgoto rel_skb;\n\t}\n\n\tcsk = lookup_tid(t, tid);\n\tif (unlikely(!csk)) {\n\t\tpr_err(\"can't find conn. for tid %u.\\n\", tid);\n\t\tgoto rel_skb;\n\t}\n\n\tif (ref)\n\t\tcxgbit_process_rx_cpl(csk, skb);\n\telse\n\t\t__cxgbit_process_rx_cpl(csk, skb);\n\n\treturn;\nrel_skb:\n\t__kfree_skb(skb);\n}\n\ncxgbit_cplhandler_func cxgbit_cplhandlers[NUM_CPL_CMDS] = {\n\t[CPL_PASS_OPEN_RPL]\t= cxgbit_pass_open_rpl,\n\t[CPL_CLOSE_LISTSRV_RPL] = cxgbit_close_listsrv_rpl,\n\t[CPL_PASS_ACCEPT_REQ]\t= cxgbit_pass_accept_req,\n\t[CPL_PASS_ESTABLISH]\t= cxgbit_pass_establish,\n\t[CPL_SET_TCB_RPL]\t= cxgbit_set_tcb_rpl,\n\t[CPL_RX_DATA]\t\t= cxgbit_rx_data,\n\t[CPL_FW4_ACK]\t\t= cxgbit_rx_cpl,\n\t[CPL_PEER_CLOSE]\t= cxgbit_rx_cpl,\n\t[CPL_CLOSE_CON_RPL]\t= cxgbit_rx_cpl,\n\t[CPL_ABORT_REQ_RSS]\t= cxgbit_rx_cpl,\n\t[CPL_ABORT_RPL_RSS]\t= cxgbit_rx_cpl,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}