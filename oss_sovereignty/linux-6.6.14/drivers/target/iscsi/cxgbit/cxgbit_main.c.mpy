{
  "module_name": "cxgbit_main.c",
  "hash_id": "df25011bbc52f9a285ef4bbdc5cadbaa483e6dcc756f2ec20931571e49b4929c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/target/iscsi/cxgbit/cxgbit_main.c",
  "human_readable_source": "\n \n\n#define DRV_NAME \"cxgbit\"\n#define DRV_VERSION \"1.0.0-ko\"\n#define pr_fmt(fmt) DRV_NAME \": \" fmt\n\n#include \"cxgbit.h\"\n\n#ifdef CONFIG_CHELSIO_T4_DCB\n#include <net/dcbevent.h>\n#include \"cxgb4_dcb.h\"\n#endif\n\nLIST_HEAD(cdev_list_head);\n \nDEFINE_MUTEX(cdev_list_lock);\n\nvoid _cxgbit_free_cdev(struct kref *kref)\n{\n\tstruct cxgbit_device *cdev;\n\n\tcdev = container_of(kref, struct cxgbit_device, kref);\n\n\tcxgbi_ppm_release(cdev2ppm(cdev));\n\tkfree(cdev);\n}\n\nstatic void cxgbit_set_mdsl(struct cxgbit_device *cdev)\n{\n\tstruct cxgb4_lld_info *lldi = &cdev->lldi;\n\tu32 mdsl;\n\n#define CXGBIT_T5_MAX_PDU_LEN 16224\n#define CXGBIT_PDU_NONPAYLOAD_LEN 312  \n\tif (is_t5(lldi->adapter_type)) {\n\t\tmdsl = min_t(u32, lldi->iscsi_iolen - CXGBIT_PDU_NONPAYLOAD_LEN,\n\t\t\t     CXGBIT_T5_MAX_PDU_LEN - CXGBIT_PDU_NONPAYLOAD_LEN);\n\t} else {\n\t\tmdsl = lldi->iscsi_iolen - CXGBIT_PDU_NONPAYLOAD_LEN;\n\t\tmdsl = min(mdsl, 16384U);\n\t}\n\n\tmdsl = round_down(mdsl, 4);\n\tmdsl = min_t(u32, mdsl, 4 * PAGE_SIZE);\n\tmdsl = min_t(u32, mdsl, (MAX_SKB_FRAGS - 1) * PAGE_SIZE);\n\n\tcdev->mdsl = mdsl;\n}\n\nstatic void *cxgbit_uld_add(const struct cxgb4_lld_info *lldi)\n{\n\tstruct cxgbit_device *cdev;\n\n\tif (is_t4(lldi->adapter_type))\n\t\treturn ERR_PTR(-ENODEV);\n\n\tcdev = kzalloc(sizeof(*cdev), GFP_KERNEL);\n\tif (!cdev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tkref_init(&cdev->kref);\n\tspin_lock_init(&cdev->np_lock);\n\n\tcdev->lldi = *lldi;\n\n\tcxgbit_set_mdsl(cdev);\n\n\tif (cxgbit_ddp_init(cdev) < 0) {\n\t\tkfree(cdev);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!test_bit(CDEV_DDP_ENABLE, &cdev->flags))\n\t\tpr_info(\"cdev %s ddp init failed\\n\",\n\t\t\tpci_name(lldi->pdev));\n\n\tif (lldi->fw_vers >= 0x10d2b00)\n\t\tset_bit(CDEV_ISO_ENABLE, &cdev->flags);\n\n\tspin_lock_init(&cdev->cskq.lock);\n\tINIT_LIST_HEAD(&cdev->cskq.list);\n\n\tmutex_lock(&cdev_list_lock);\n\tlist_add_tail(&cdev->list, &cdev_list_head);\n\tmutex_unlock(&cdev_list_lock);\n\n\tpr_info(\"cdev %s added for iSCSI target transport\\n\",\n\t\tpci_name(lldi->pdev));\n\n\treturn cdev;\n}\n\nstatic void cxgbit_close_conn(struct cxgbit_device *cdev)\n{\n\tstruct cxgbit_sock *csk;\n\tstruct sk_buff *skb;\n\tbool wakeup_thread = false;\n\n\tspin_lock_bh(&cdev->cskq.lock);\n\tlist_for_each_entry(csk, &cdev->cskq.list, list) {\n\t\tskb = alloc_skb(0, GFP_ATOMIC);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(&csk->rxq.lock);\n\t\t__skb_queue_tail(&csk->rxq, skb);\n\t\tif (skb_queue_len(&csk->rxq) == 1)\n\t\t\twakeup_thread = true;\n\t\tspin_unlock_bh(&csk->rxq.lock);\n\n\t\tif (wakeup_thread) {\n\t\t\twake_up(&csk->waitq);\n\t\t\twakeup_thread = false;\n\t\t}\n\t}\n\tspin_unlock_bh(&cdev->cskq.lock);\n}\n\nstatic void cxgbit_detach_cdev(struct cxgbit_device *cdev)\n{\n\tbool free_cdev = false;\n\n\tspin_lock_bh(&cdev->cskq.lock);\n\tif (list_empty(&cdev->cskq.list))\n\t\tfree_cdev = true;\n\tspin_unlock_bh(&cdev->cskq.lock);\n\n\tif (free_cdev) {\n\t\tmutex_lock(&cdev_list_lock);\n\t\tlist_del(&cdev->list);\n\t\tmutex_unlock(&cdev_list_lock);\n\n\t\tcxgbit_put_cdev(cdev);\n\t} else {\n\t\tcxgbit_close_conn(cdev);\n\t}\n}\n\nstatic int cxgbit_uld_state_change(void *handle, enum cxgb4_state state)\n{\n\tstruct cxgbit_device *cdev = handle;\n\n\tswitch (state) {\n\tcase CXGB4_STATE_UP:\n\t\tset_bit(CDEV_STATE_UP, &cdev->flags);\n\t\tpr_info(\"cdev %s state UP.\\n\", pci_name(cdev->lldi.pdev));\n\t\tbreak;\n\tcase CXGB4_STATE_START_RECOVERY:\n\t\tclear_bit(CDEV_STATE_UP, &cdev->flags);\n\t\tcxgbit_close_conn(cdev);\n\t\tpr_info(\"cdev %s state RECOVERY.\\n\", pci_name(cdev->lldi.pdev));\n\t\tbreak;\n\tcase CXGB4_STATE_DOWN:\n\t\tpr_info(\"cdev %s state DOWN.\\n\", pci_name(cdev->lldi.pdev));\n\t\tbreak;\n\tcase CXGB4_STATE_DETACH:\n\t\tclear_bit(CDEV_STATE_UP, &cdev->flags);\n\t\tpr_info(\"cdev %s state DETACH.\\n\", pci_name(cdev->lldi.pdev));\n\t\tcxgbit_detach_cdev(cdev);\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"cdev %s unknown state %d.\\n\",\n\t\t\tpci_name(cdev->lldi.pdev), state);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void\ncxgbit_process_ddpvld(struct cxgbit_sock *csk, struct cxgbit_lro_pdu_cb *pdu_cb,\n\t\t      u32 ddpvld)\n{\n\n\tif (ddpvld & (1 << CPL_RX_ISCSI_DDP_STATUS_HCRC_SHIFT)) {\n\t\tpr_info(\"tid 0x%x, status 0x%x, hcrc bad.\\n\", csk->tid, ddpvld);\n\t\tpdu_cb->flags |= PDUCBF_RX_HCRC_ERR;\n\t}\n\n\tif (ddpvld & (1 << CPL_RX_ISCSI_DDP_STATUS_DCRC_SHIFT)) {\n\t\tpr_info(\"tid 0x%x, status 0x%x, dcrc bad.\\n\", csk->tid, ddpvld);\n\t\tpdu_cb->flags |= PDUCBF_RX_DCRC_ERR;\n\t}\n\n\tif (ddpvld & (1 << CPL_RX_ISCSI_DDP_STATUS_PAD_SHIFT))\n\t\tpr_info(\"tid 0x%x, status 0x%x, pad bad.\\n\", csk->tid, ddpvld);\n\n\tif ((ddpvld & (1 << CPL_RX_ISCSI_DDP_STATUS_DDP_SHIFT)) &&\n\t    (!(pdu_cb->flags & PDUCBF_RX_DATA))) {\n\t\tpdu_cb->flags |= PDUCBF_RX_DATA_DDPD;\n\t}\n}\n\nstatic void\ncxgbit_lro_add_packet_rsp(struct sk_buff *skb, u8 op, const __be64 *rsp)\n{\n\tstruct cxgbit_lro_cb *lro_cb = cxgbit_skb_lro_cb(skb);\n\tstruct cxgbit_lro_pdu_cb *pdu_cb = cxgbit_skb_lro_pdu_cb(skb,\n\t\t\t\t\t\tlro_cb->pdu_idx);\n\tstruct cpl_rx_iscsi_ddp *cpl = (struct cpl_rx_iscsi_ddp *)(rsp + 1);\n\n\tcxgbit_process_ddpvld(lro_cb->csk, pdu_cb, be32_to_cpu(cpl->ddpvld));\n\n\tpdu_cb->flags |= PDUCBF_RX_STATUS;\n\tpdu_cb->ddigest = ntohl(cpl->ulp_crc);\n\tpdu_cb->pdulen = ntohs(cpl->len);\n\n\tif (pdu_cb->flags & PDUCBF_RX_HDR)\n\t\tpdu_cb->complete = true;\n\n\tlro_cb->pdu_totallen += pdu_cb->pdulen;\n\tlro_cb->complete = true;\n\tlro_cb->pdu_idx++;\n}\n\nstatic void\ncxgbit_copy_frags(struct sk_buff *skb, const struct pkt_gl *gl,\n\t\t  unsigned int offset)\n{\n\tu8 skb_frag_idx = skb_shinfo(skb)->nr_frags;\n\tu8 i;\n\n\t \n\t__skb_fill_page_desc(skb, skb_frag_idx, gl->frags[0].page,\n\t\t\t     gl->frags[0].offset + offset,\n\t\t\t     gl->frags[0].size - offset);\n\tfor (i = 1; i < gl->nfrags; i++)\n\t\t__skb_fill_page_desc(skb, skb_frag_idx + i,\n\t\t\t\t     gl->frags[i].page,\n\t\t\t\t     gl->frags[i].offset,\n\t\t\t\t     gl->frags[i].size);\n\n\tskb_shinfo(skb)->nr_frags += gl->nfrags;\n\n\t \n\tget_page(gl->frags[gl->nfrags - 1].page);\n}\n\nstatic void\ncxgbit_lro_add_packet_gl(struct sk_buff *skb, u8 op, const struct pkt_gl *gl)\n{\n\tstruct cxgbit_lro_cb *lro_cb = cxgbit_skb_lro_cb(skb);\n\tstruct cxgbit_lro_pdu_cb *pdu_cb = cxgbit_skb_lro_pdu_cb(skb,\n\t\t\t\t\t\tlro_cb->pdu_idx);\n\tu32 len, offset;\n\n\tif (op == CPL_ISCSI_HDR) {\n\t\tstruct cpl_iscsi_hdr *cpl = (struct cpl_iscsi_hdr *)gl->va;\n\n\t\toffset = sizeof(struct cpl_iscsi_hdr);\n\t\tpdu_cb->flags |= PDUCBF_RX_HDR;\n\t\tpdu_cb->seq = ntohl(cpl->seq);\n\t\tlen = ntohs(cpl->len);\n\t\tpdu_cb->hdr = gl->va + offset;\n\t\tpdu_cb->hlen = len;\n\t\tpdu_cb->hfrag_idx = skb_shinfo(skb)->nr_frags;\n\n\t\tif (unlikely(gl->nfrags > 1))\n\t\t\tcxgbit_skcb_flags(skb) = 0;\n\n\t\tlro_cb->complete = false;\n\t} else if (op == CPL_ISCSI_DATA) {\n\t\tstruct cpl_iscsi_data *cpl = (struct cpl_iscsi_data *)gl->va;\n\n\t\toffset = sizeof(struct cpl_iscsi_data);\n\t\tpdu_cb->flags |= PDUCBF_RX_DATA;\n\t\tlen = ntohs(cpl->len);\n\t\tpdu_cb->dlen = len;\n\t\tpdu_cb->doffset = lro_cb->offset;\n\t\tpdu_cb->nr_dfrags = gl->nfrags;\n\t\tpdu_cb->dfrag_idx = skb_shinfo(skb)->nr_frags;\n\t\tlro_cb->complete = false;\n\t} else {\n\t\tstruct cpl_rx_iscsi_cmp *cpl;\n\n\t\tcpl = (struct cpl_rx_iscsi_cmp *)gl->va;\n\t\toffset = sizeof(struct cpl_rx_iscsi_cmp);\n\t\tpdu_cb->flags |= (PDUCBF_RX_HDR | PDUCBF_RX_STATUS);\n\t\tlen = be16_to_cpu(cpl->len);\n\t\tpdu_cb->hdr = gl->va + offset;\n\t\tpdu_cb->hlen = len;\n\t\tpdu_cb->hfrag_idx = skb_shinfo(skb)->nr_frags;\n\t\tpdu_cb->ddigest = be32_to_cpu(cpl->ulp_crc);\n\t\tpdu_cb->pdulen = ntohs(cpl->len);\n\n\t\tif (unlikely(gl->nfrags > 1))\n\t\t\tcxgbit_skcb_flags(skb) = 0;\n\n\t\tcxgbit_process_ddpvld(lro_cb->csk, pdu_cb,\n\t\t\t\t      be32_to_cpu(cpl->ddpvld));\n\n\t\tif (pdu_cb->flags & PDUCBF_RX_DATA_DDPD) {\n\t\t\tpdu_cb->flags |= PDUCBF_RX_DDP_CMP;\n\t\t\tpdu_cb->complete = true;\n\t\t} else if (pdu_cb->flags & PDUCBF_RX_DATA) {\n\t\t\tpdu_cb->complete = true;\n\t\t}\n\n\t\tlro_cb->pdu_totallen += pdu_cb->hlen + pdu_cb->dlen;\n\t\tlro_cb->complete = true;\n\t\tlro_cb->pdu_idx++;\n\t}\n\n\tcxgbit_copy_frags(skb, gl, offset);\n\n\tpdu_cb->frags += gl->nfrags;\n\tlro_cb->offset += len;\n\tskb->len += len;\n\tskb->data_len += len;\n\tskb->truesize += len;\n}\n\nstatic struct sk_buff *\ncxgbit_lro_init_skb(struct cxgbit_sock *csk, u8 op, const struct pkt_gl *gl,\n\t\t    const __be64 *rsp, struct napi_struct *napi)\n{\n\tstruct sk_buff *skb;\n\tstruct cxgbit_lro_cb *lro_cb;\n\n\tskb = napi_alloc_skb(napi, LRO_SKB_MAX_HEADROOM);\n\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemset(skb->data, 0, LRO_SKB_MAX_HEADROOM);\n\n\tcxgbit_skcb_flags(skb) |= SKCBF_RX_LRO;\n\n\tlro_cb = cxgbit_skb_lro_cb(skb);\n\n\tcxgbit_get_csk(csk);\n\n\tlro_cb->csk = csk;\n\n\treturn skb;\n}\n\nstatic void cxgbit_queue_lro_skb(struct cxgbit_sock *csk, struct sk_buff *skb)\n{\n\tbool wakeup_thread = false;\n\n\tspin_lock(&csk->rxq.lock);\n\t__skb_queue_tail(&csk->rxq, skb);\n\tif (skb_queue_len(&csk->rxq) == 1)\n\t\twakeup_thread = true;\n\tspin_unlock(&csk->rxq.lock);\n\n\tif (wakeup_thread)\n\t\twake_up(&csk->waitq);\n}\n\nstatic void cxgbit_lro_flush(struct t4_lro_mgr *lro_mgr, struct sk_buff *skb)\n{\n\tstruct cxgbit_lro_cb *lro_cb = cxgbit_skb_lro_cb(skb);\n\tstruct cxgbit_sock *csk = lro_cb->csk;\n\n\tcsk->lro_skb = NULL;\n\n\t__skb_unlink(skb, &lro_mgr->lroq);\n\tcxgbit_queue_lro_skb(csk, skb);\n\n\tcxgbit_put_csk(csk);\n\n\tlro_mgr->lro_pkts++;\n\tlro_mgr->lro_session_cnt--;\n}\n\nstatic void cxgbit_uld_lro_flush(struct t4_lro_mgr *lro_mgr)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = skb_peek(&lro_mgr->lroq)))\n\t\tcxgbit_lro_flush(lro_mgr, skb);\n}\n\nstatic int\ncxgbit_lro_receive(struct cxgbit_sock *csk, u8 op, const __be64 *rsp,\n\t\t   const struct pkt_gl *gl, struct t4_lro_mgr *lro_mgr,\n\t\t   struct napi_struct *napi)\n{\n\tstruct sk_buff *skb;\n\tstruct cxgbit_lro_cb *lro_cb;\n\n\tif (!csk) {\n\t\tpr_err(\"%s: csk NULL, op 0x%x.\\n\", __func__, op);\n\t\tgoto out;\n\t}\n\n\tif (csk->lro_skb)\n\t\tgoto add_packet;\n\nstart_lro:\n\tif (lro_mgr->lro_session_cnt >= MAX_LRO_SESSIONS) {\n\t\tcxgbit_uld_lro_flush(lro_mgr);\n\t\tgoto start_lro;\n\t}\n\n\tskb = cxgbit_lro_init_skb(csk, op, gl, rsp, napi);\n\tif (unlikely(!skb))\n\t\tgoto out;\n\n\tcsk->lro_skb = skb;\n\n\t__skb_queue_tail(&lro_mgr->lroq, skb);\n\tlro_mgr->lro_session_cnt++;\n\nadd_packet:\n\tskb = csk->lro_skb;\n\tlro_cb = cxgbit_skb_lro_cb(skb);\n\n\tif ((gl && (((skb_shinfo(skb)->nr_frags + gl->nfrags) >\n\t    MAX_SKB_FRAGS) || (lro_cb->pdu_totallen >= LRO_FLUSH_LEN_MAX))) ||\n\t    (lro_cb->pdu_idx >= MAX_SKB_FRAGS)) {\n\t\tcxgbit_lro_flush(lro_mgr, skb);\n\t\tgoto start_lro;\n\t}\n\n\tif (gl)\n\t\tcxgbit_lro_add_packet_gl(skb, op, gl);\n\telse\n\t\tcxgbit_lro_add_packet_rsp(skb, op, rsp);\n\n\tlro_mgr->lro_merged++;\n\n\treturn 0;\n\nout:\n\treturn -1;\n}\n\nstatic int\ncxgbit_uld_lro_rx_handler(void *hndl, const __be64 *rsp,\n\t\t\t  const struct pkt_gl *gl, struct t4_lro_mgr *lro_mgr,\n\t\t\t  struct napi_struct *napi)\n{\n\tstruct cxgbit_device *cdev = hndl;\n\tstruct cxgb4_lld_info *lldi = &cdev->lldi;\n\tstruct cpl_tx_data *rpl = NULL;\n\tstruct cxgbit_sock *csk = NULL;\n\tunsigned int tid = 0;\n\tstruct sk_buff *skb;\n\tunsigned int op = *(u8 *)rsp;\n\tbool lro_flush = true;\n\n\tswitch (op) {\n\tcase CPL_ISCSI_HDR:\n\tcase CPL_ISCSI_DATA:\n\tcase CPL_RX_ISCSI_CMP:\n\tcase CPL_RX_ISCSI_DDP:\n\tcase CPL_FW4_ACK:\n\t\tlro_flush = false;\n\t\tfallthrough;\n\tcase CPL_ABORT_RPL_RSS:\n\tcase CPL_PASS_ESTABLISH:\n\tcase CPL_PEER_CLOSE:\n\tcase CPL_CLOSE_CON_RPL:\n\tcase CPL_ABORT_REQ_RSS:\n\tcase CPL_SET_TCB_RPL:\n\tcase CPL_RX_DATA:\n\t\trpl = gl ? (struct cpl_tx_data *)gl->va :\n\t\t\t   (struct cpl_tx_data *)(rsp + 1);\n\t\ttid = GET_TID(rpl);\n\t\tcsk = lookup_tid(lldi->tids, tid);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (csk && csk->lro_skb && lro_flush)\n\t\tcxgbit_lro_flush(lro_mgr, csk->lro_skb);\n\n\tif (!gl) {\n\t\tunsigned int len;\n\n\t\tif (op == CPL_RX_ISCSI_DDP) {\n\t\t\tif (!cxgbit_lro_receive(csk, op, rsp, NULL, lro_mgr,\n\t\t\t\t\t\tnapi))\n\t\t\t\treturn 0;\n\t\t}\n\n\t\tlen = 64 - sizeof(struct rsp_ctrl) - 8;\n\t\tskb = napi_alloc_skb(napi, len);\n\t\tif (!skb)\n\t\t\tgoto nomem;\n\t\t__skb_put(skb, len);\n\t\tskb_copy_to_linear_data(skb, &rsp[1], len);\n\t} else {\n\t\tif (unlikely(op != *(u8 *)gl->va)) {\n\t\t\tpr_info(\"? FL 0x%p,RSS%#llx,FL %#llx,len %u.\\n\",\n\t\t\t\tgl->va, be64_to_cpu(*rsp),\n\t\t\t\tget_unaligned_be64(gl->va),\n\t\t\t\tgl->tot_len);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif ((op == CPL_ISCSI_HDR) || (op == CPL_ISCSI_DATA) ||\n\t\t    (op == CPL_RX_ISCSI_CMP)) {\n\t\t\tif (!cxgbit_lro_receive(csk, op, rsp, gl, lro_mgr,\n\t\t\t\t\t\tnapi))\n\t\t\t\treturn 0;\n\t\t}\n\n#define RX_PULL_LEN 128\n\t\tskb = cxgb4_pktgl_to_skb(gl, RX_PULL_LEN, RX_PULL_LEN);\n\t\tif (unlikely(!skb))\n\t\t\tgoto nomem;\n\t}\n\n\trpl = (struct cpl_tx_data *)skb->data;\n\top = rpl->ot.opcode;\n\tcxgbit_skcb_rx_opcode(skb) = op;\n\n\tpr_debug(\"cdev %p, opcode 0x%x(0x%x,0x%x), skb %p.\\n\",\n\t\t cdev, op, rpl->ot.opcode_tid,\n\t\t ntohl(rpl->ot.opcode_tid), skb);\n\n\tif (op < NUM_CPL_CMDS && cxgbit_cplhandlers[op]) {\n\t\tcxgbit_cplhandlers[op](cdev, skb);\n\t} else {\n\t\tpr_err(\"No handler for opcode 0x%x.\\n\", op);\n\t\t__kfree_skb(skb);\n\t}\n\treturn 0;\nnomem:\n\tpr_err(\"%s OOM bailing out.\\n\", __func__);\n\treturn 1;\n}\n\n#ifdef CONFIG_CHELSIO_T4_DCB\nstruct cxgbit_dcb_work {\n\tstruct dcb_app_type dcb_app;\n\tstruct work_struct work;\n};\n\nstatic void\ncxgbit_update_dcb_priority(struct cxgbit_device *cdev, u8 port_id,\n\t\t\t   u8 dcb_priority, u16 port_num)\n{\n\tstruct cxgbit_sock *csk;\n\tstruct sk_buff *skb;\n\tu16 local_port;\n\tbool wakeup_thread = false;\n\n\tspin_lock_bh(&cdev->cskq.lock);\n\tlist_for_each_entry(csk, &cdev->cskq.list, list) {\n\t\tif (csk->port_id != port_id)\n\t\t\tcontinue;\n\n\t\tif (csk->com.local_addr.ss_family == AF_INET6) {\n\t\t\tstruct sockaddr_in6 *sock_in6;\n\n\t\t\tsock_in6 = (struct sockaddr_in6 *)&csk->com.local_addr;\n\t\t\tlocal_port = ntohs(sock_in6->sin6_port);\n\t\t} else {\n\t\t\tstruct sockaddr_in *sock_in;\n\n\t\t\tsock_in = (struct sockaddr_in *)&csk->com.local_addr;\n\t\t\tlocal_port = ntohs(sock_in->sin_port);\n\t\t}\n\n\t\tif (local_port != port_num)\n\t\t\tcontinue;\n\n\t\tif (csk->dcb_priority == dcb_priority)\n\t\t\tcontinue;\n\n\t\tskb = alloc_skb(0, GFP_ATOMIC);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tspin_lock(&csk->rxq.lock);\n\t\t__skb_queue_tail(&csk->rxq, skb);\n\t\tif (skb_queue_len(&csk->rxq) == 1)\n\t\t\twakeup_thread = true;\n\t\tspin_unlock(&csk->rxq.lock);\n\n\t\tif (wakeup_thread) {\n\t\t\twake_up(&csk->waitq);\n\t\t\twakeup_thread = false;\n\t\t}\n\t}\n\tspin_unlock_bh(&cdev->cskq.lock);\n}\n\nstatic void cxgbit_dcb_workfn(struct work_struct *work)\n{\n\tstruct cxgbit_dcb_work *dcb_work;\n\tstruct net_device *ndev;\n\tstruct cxgbit_device *cdev = NULL;\n\tstruct dcb_app_type *iscsi_app;\n\tu8 priority, port_id = 0xff;\n\n\tdcb_work = container_of(work, struct cxgbit_dcb_work, work);\n\tiscsi_app = &dcb_work->dcb_app;\n\n\tif (iscsi_app->dcbx & DCB_CAP_DCBX_VER_IEEE) {\n\t\tif ((iscsi_app->app.selector != IEEE_8021QAZ_APP_SEL_STREAM) &&\n\t\t    (iscsi_app->app.selector != IEEE_8021QAZ_APP_SEL_ANY))\n\t\t\tgoto out;\n\n\t\tpriority = iscsi_app->app.priority;\n\n\t} else if (iscsi_app->dcbx & DCB_CAP_DCBX_VER_CEE) {\n\t\tif (iscsi_app->app.selector != DCB_APP_IDTYPE_PORTNUM)\n\t\t\tgoto out;\n\n\t\tif (!iscsi_app->app.priority)\n\t\t\tgoto out;\n\n\t\tpriority = ffs(iscsi_app->app.priority) - 1;\n\t} else {\n\t\tgoto out;\n\t}\n\n\tpr_debug(\"priority for ifid %d is %u\\n\",\n\t\t iscsi_app->ifindex, priority);\n\n\tndev = dev_get_by_index(&init_net, iscsi_app->ifindex);\n\n\tif (!ndev)\n\t\tgoto out;\n\n\tmutex_lock(&cdev_list_lock);\n\tcdev = cxgbit_find_device(ndev, &port_id);\n\n\tdev_put(ndev);\n\n\tif (!cdev) {\n\t\tmutex_unlock(&cdev_list_lock);\n\t\tgoto out;\n\t}\n\n\tcxgbit_update_dcb_priority(cdev, port_id, priority,\n\t\t\t\t   iscsi_app->app.protocol);\n\tmutex_unlock(&cdev_list_lock);\nout:\n\tkfree(dcb_work);\n}\n\nstatic int\ncxgbit_dcbevent_notify(struct notifier_block *nb, unsigned long action,\n\t\t       void *data)\n{\n\tstruct cxgbit_dcb_work *dcb_work;\n\tstruct dcb_app_type *dcb_app = data;\n\n\tdcb_work = kzalloc(sizeof(*dcb_work), GFP_ATOMIC);\n\tif (!dcb_work)\n\t\treturn NOTIFY_DONE;\n\n\tdcb_work->dcb_app = *dcb_app;\n\tINIT_WORK(&dcb_work->work, cxgbit_dcb_workfn);\n\tschedule_work(&dcb_work->work);\n\treturn NOTIFY_OK;\n}\n#endif\n\nstatic enum target_prot_op cxgbit_get_sup_prot_ops(struct iscsit_conn *conn)\n{\n\treturn TARGET_PROT_NORMAL;\n}\n\nstatic struct iscsit_transport cxgbit_transport = {\n\t.name\t\t\t= DRV_NAME,\n\t.transport_type\t\t= ISCSI_CXGBIT,\n\t.rdma_shutdown\t\t= false,\n\t.priv_size\t\t= sizeof(struct cxgbit_cmd),\n\t.owner\t\t\t= THIS_MODULE,\n\t.iscsit_setup_np\t= cxgbit_setup_np,\n\t.iscsit_accept_np\t= cxgbit_accept_np,\n\t.iscsit_free_np\t\t= cxgbit_free_np,\n\t.iscsit_free_conn\t= cxgbit_free_conn,\n\t.iscsit_get_login_rx\t= cxgbit_get_login_rx,\n\t.iscsit_put_login_tx\t= cxgbit_put_login_tx,\n\t.iscsit_immediate_queue\t= iscsit_immediate_queue,\n\t.iscsit_response_queue\t= iscsit_response_queue,\n\t.iscsit_get_dataout\t= iscsit_build_r2ts_for_cmd,\n\t.iscsit_queue_data_in\t= iscsit_queue_rsp,\n\t.iscsit_queue_status\t= iscsit_queue_rsp,\n\t.iscsit_xmit_pdu\t= cxgbit_xmit_pdu,\n\t.iscsit_get_r2t_ttt\t= cxgbit_get_r2t_ttt,\n\t.iscsit_get_rx_pdu\t= cxgbit_get_rx_pdu,\n\t.iscsit_validate_params\t= cxgbit_validate_params,\n\t.iscsit_unmap_cmd\t= cxgbit_unmap_cmd,\n\t.iscsit_aborted_task\t= iscsit_aborted_task,\n\t.iscsit_get_sup_prot_ops = cxgbit_get_sup_prot_ops,\n};\n\nstatic struct cxgb4_uld_info cxgbit_uld_info = {\n\t.name\t\t= DRV_NAME,\n\t.nrxq\t\t= MAX_ULD_QSETS,\n\t.ntxq\t\t= MAX_ULD_QSETS,\n\t.rxq_size\t= 1024,\n\t.lro\t\t= true,\n\t.add\t\t= cxgbit_uld_add,\n\t.state_change\t= cxgbit_uld_state_change,\n\t.lro_rx_handler = cxgbit_uld_lro_rx_handler,\n\t.lro_flush\t= cxgbit_uld_lro_flush,\n};\n\n#ifdef CONFIG_CHELSIO_T4_DCB\nstatic struct notifier_block cxgbit_dcbevent_nb = {\n\t.notifier_call = cxgbit_dcbevent_notify,\n};\n#endif\n\nstatic int __init cxgbit_init(void)\n{\n\tcxgb4_register_uld(CXGB4_ULD_ISCSIT, &cxgbit_uld_info);\n\tiscsit_register_transport(&cxgbit_transport);\n\n#ifdef CONFIG_CHELSIO_T4_DCB\n\tpr_info(\"%s dcb enabled.\\n\", DRV_NAME);\n\tregister_dcbevent_notifier(&cxgbit_dcbevent_nb);\n#endif\n\tBUILD_BUG_ON(sizeof_field(struct sk_buff, cb) <\n\t\t     sizeof(union cxgbit_skb_cb));\n\treturn 0;\n}\n\nstatic void __exit cxgbit_exit(void)\n{\n\tstruct cxgbit_device *cdev, *tmp;\n\n#ifdef CONFIG_CHELSIO_T4_DCB\n\tunregister_dcbevent_notifier(&cxgbit_dcbevent_nb);\n#endif\n\tmutex_lock(&cdev_list_lock);\n\tlist_for_each_entry_safe(cdev, tmp, &cdev_list_head, list) {\n\t\tlist_del(&cdev->list);\n\t\tcxgbit_put_cdev(cdev);\n\t}\n\tmutex_unlock(&cdev_list_lock);\n\tiscsit_unregister_transport(&cxgbit_transport);\n\tcxgb4_unregister_uld(CXGB4_ULD_ISCSIT);\n}\n\nmodule_init(cxgbit_init);\nmodule_exit(cxgbit_exit);\n\nMODULE_DESCRIPTION(\"Chelsio iSCSI target offload driver\");\nMODULE_AUTHOR(\"Chelsio Communications\");\nMODULE_VERSION(DRV_VERSION);\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}