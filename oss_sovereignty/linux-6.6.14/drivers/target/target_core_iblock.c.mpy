{
  "module_name": "target_core_iblock.c",
  "hash_id": "bfba6f4020be42596ee069989f7497626df537c547e99cd763d55f09e7c5d3e6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/target/target_core_iblock.c",
  "human_readable_source": "\n \n\n#include <linux/string.h>\n#include <linux/parser.h>\n#include <linux/timer.h>\n#include <linux/fs.h>\n#include <linux/blkdev.h>\n#include <linux/blk-integrity.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/bio.h>\n#include <linux/file.h>\n#include <linux/module.h>\n#include <linux/scatterlist.h>\n#include <linux/pr.h>\n#include <scsi/scsi_proto.h>\n#include <scsi/scsi_common.h>\n#include <asm/unaligned.h>\n\n#include <target/target_core_base.h>\n#include <target/target_core_backend.h>\n\n#include \"target_core_iblock.h\"\n#include \"target_core_pr.h\"\n\n#define IBLOCK_MAX_BIO_PER_TASK\t 32\t \n#define IBLOCK_BIO_POOL_SIZE\t128\n\nstatic inline struct iblock_dev *IBLOCK_DEV(struct se_device *dev)\n{\n\treturn container_of(dev, struct iblock_dev, dev);\n}\n\n\nstatic int iblock_attach_hba(struct se_hba *hba, u32 host_id)\n{\n\tpr_debug(\"CORE_HBA[%d] - TCM iBlock HBA Driver %s on\"\n\t\t\" Generic Target Core Stack %s\\n\", hba->hba_id,\n\t\tIBLOCK_VERSION, TARGET_CORE_VERSION);\n\treturn 0;\n}\n\nstatic void iblock_detach_hba(struct se_hba *hba)\n{\n}\n\nstatic struct se_device *iblock_alloc_device(struct se_hba *hba, const char *name)\n{\n\tstruct iblock_dev *ib_dev = NULL;\n\n\tib_dev = kzalloc(sizeof(struct iblock_dev), GFP_KERNEL);\n\tif (!ib_dev) {\n\t\tpr_err(\"Unable to allocate struct iblock_dev\\n\");\n\t\treturn NULL;\n\t}\n\n\tib_dev->ibd_plug = kcalloc(nr_cpu_ids, sizeof(*ib_dev->ibd_plug),\n\t\t\t\t   GFP_KERNEL);\n\tif (!ib_dev->ibd_plug)\n\t\tgoto free_dev;\n\n\tpr_debug( \"IBLOCK: Allocated ib_dev for %s\\n\", name);\n\n\treturn &ib_dev->dev;\n\nfree_dev:\n\tkfree(ib_dev);\n\treturn NULL;\n}\n\nstatic bool iblock_configure_unmap(struct se_device *dev)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\n\treturn target_configure_unmap_from_queue(&dev->dev_attrib,\n\t\t\t\t\t\t ib_dev->ibd_bd);\n}\n\nstatic int iblock_configure_device(struct se_device *dev)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tstruct request_queue *q;\n\tstruct block_device *bd = NULL;\n\tstruct blk_integrity *bi;\n\tblk_mode_t mode = BLK_OPEN_READ;\n\tunsigned int max_write_zeroes_sectors;\n\tint ret;\n\n\tif (!(ib_dev->ibd_flags & IBDF_HAS_UDEV_PATH)) {\n\t\tpr_err(\"Missing udev_path= parameters for IBLOCK\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = bioset_init(&ib_dev->ibd_bio_set, IBLOCK_BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);\n\tif (ret) {\n\t\tpr_err(\"IBLOCK: Unable to create bioset\\n\");\n\t\tgoto out;\n\t}\n\n\tpr_debug( \"IBLOCK: Claiming struct block_device: %s\\n\",\n\t\t\tib_dev->ibd_udev_path);\n\n\tif (!ib_dev->ibd_readonly)\n\t\tmode |= BLK_OPEN_WRITE;\n\telse\n\t\tdev->dev_flags |= DF_READ_ONLY;\n\n\tbd = blkdev_get_by_path(ib_dev->ibd_udev_path, mode, ib_dev, NULL);\n\tif (IS_ERR(bd)) {\n\t\tret = PTR_ERR(bd);\n\t\tgoto out_free_bioset;\n\t}\n\tib_dev->ibd_bd = bd;\n\n\tq = bdev_get_queue(bd);\n\n\tdev->dev_attrib.hw_block_size = bdev_logical_block_size(bd);\n\tdev->dev_attrib.hw_max_sectors = mult_frac(queue_max_hw_sectors(q),\n\t\t\tSECTOR_SIZE,\n\t\t\tdev->dev_attrib.hw_block_size);\n\tdev->dev_attrib.hw_queue_depth = q->nr_requests;\n\n\t \n\tmax_write_zeroes_sectors = bdev_write_zeroes_sectors(bd);\n\tif (max_write_zeroes_sectors)\n\t\tdev->dev_attrib.max_write_same_len = max_write_zeroes_sectors;\n\telse\n\t\tdev->dev_attrib.max_write_same_len = 0xFFFF;\n\n\tif (bdev_nonrot(bd))\n\t\tdev->dev_attrib.is_nonrot = 1;\n\n\tbi = bdev_get_integrity(bd);\n\tif (bi) {\n\t\tstruct bio_set *bs = &ib_dev->ibd_bio_set;\n\n\t\tif (!strcmp(bi->profile->name, \"T10-DIF-TYPE3-IP\") ||\n\t\t    !strcmp(bi->profile->name, \"T10-DIF-TYPE1-IP\")) {\n\t\t\tpr_err(\"IBLOCK export of blk_integrity: %s not\"\n\t\t\t       \" supported\\n\", bi->profile->name);\n\t\t\tret = -ENOSYS;\n\t\t\tgoto out_blkdev_put;\n\t\t}\n\n\t\tif (!strcmp(bi->profile->name, \"T10-DIF-TYPE3-CRC\")) {\n\t\t\tdev->dev_attrib.pi_prot_type = TARGET_DIF_TYPE3_PROT;\n\t\t} else if (!strcmp(bi->profile->name, \"T10-DIF-TYPE1-CRC\")) {\n\t\t\tdev->dev_attrib.pi_prot_type = TARGET_DIF_TYPE1_PROT;\n\t\t}\n\n\t\tif (dev->dev_attrib.pi_prot_type) {\n\t\t\tif (bioset_integrity_create(bs, IBLOCK_BIO_POOL_SIZE) < 0) {\n\t\t\t\tpr_err(\"Unable to allocate bioset for PI\\n\");\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out_blkdev_put;\n\t\t\t}\n\t\t\tpr_debug(\"IBLOCK setup BIP bs->bio_integrity_pool: %p\\n\",\n\t\t\t\t &bs->bio_integrity_pool);\n\t\t}\n\t\tdev->dev_attrib.hw_pi_prot_type = dev->dev_attrib.pi_prot_type;\n\t}\n\n\treturn 0;\n\nout_blkdev_put:\n\tblkdev_put(ib_dev->ibd_bd, ib_dev);\nout_free_bioset:\n\tbioset_exit(&ib_dev->ibd_bio_set);\nout:\n\treturn ret;\n}\n\nstatic void iblock_dev_call_rcu(struct rcu_head *p)\n{\n\tstruct se_device *dev = container_of(p, struct se_device, rcu_head);\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\n\tkfree(ib_dev->ibd_plug);\n\tkfree(ib_dev);\n}\n\nstatic void iblock_free_device(struct se_device *dev)\n{\n\tcall_rcu(&dev->rcu_head, iblock_dev_call_rcu);\n}\n\nstatic void iblock_destroy_device(struct se_device *dev)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\n\tif (ib_dev->ibd_bd != NULL)\n\t\tblkdev_put(ib_dev->ibd_bd, ib_dev);\n\tbioset_exit(&ib_dev->ibd_bio_set);\n}\n\nstatic struct se_dev_plug *iblock_plug_device(struct se_device *se_dev)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(se_dev);\n\tstruct iblock_dev_plug *ib_dev_plug;\n\n\t \n\tib_dev_plug = &ib_dev->ibd_plug[raw_smp_processor_id()];\n\tif (test_and_set_bit(IBD_PLUGF_PLUGGED, &ib_dev_plug->flags))\n\t\treturn NULL;\n\n\tblk_start_plug(&ib_dev_plug->blk_plug);\n\treturn &ib_dev_plug->se_plug;\n}\n\nstatic void iblock_unplug_device(struct se_dev_plug *se_plug)\n{\n\tstruct iblock_dev_plug *ib_dev_plug = container_of(se_plug,\n\t\t\t\t\tstruct iblock_dev_plug, se_plug);\n\n\tblk_finish_plug(&ib_dev_plug->blk_plug);\n\tclear_bit(IBD_PLUGF_PLUGGED, &ib_dev_plug->flags);\n}\n\nstatic sector_t iblock_get_blocks(struct se_device *dev)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tu32 block_size = bdev_logical_block_size(ib_dev->ibd_bd);\n\tunsigned long long blocks_long =\n\t\tdiv_u64(bdev_nr_bytes(ib_dev->ibd_bd), block_size) - 1;\n\n\tif (block_size == dev->dev_attrib.block_size)\n\t\treturn blocks_long;\n\n\tswitch (block_size) {\n\tcase 4096:\n\t\tswitch (dev->dev_attrib.block_size) {\n\t\tcase 2048:\n\t\t\tblocks_long <<= 1;\n\t\t\tbreak;\n\t\tcase 1024:\n\t\t\tblocks_long <<= 2;\n\t\t\tbreak;\n\t\tcase 512:\n\t\t\tblocks_long <<= 3;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 2048:\n\t\tswitch (dev->dev_attrib.block_size) {\n\t\tcase 4096:\n\t\t\tblocks_long >>= 1;\n\t\t\tbreak;\n\t\tcase 1024:\n\t\t\tblocks_long <<= 1;\n\t\t\tbreak;\n\t\tcase 512:\n\t\t\tblocks_long <<= 2;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 1024:\n\t\tswitch (dev->dev_attrib.block_size) {\n\t\tcase 4096:\n\t\t\tblocks_long >>= 2;\n\t\t\tbreak;\n\t\tcase 2048:\n\t\t\tblocks_long >>= 1;\n\t\t\tbreak;\n\t\tcase 512:\n\t\t\tblocks_long <<= 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 512:\n\t\tswitch (dev->dev_attrib.block_size) {\n\t\tcase 4096:\n\t\t\tblocks_long >>= 3;\n\t\t\tbreak;\n\t\tcase 2048:\n\t\t\tblocks_long >>= 2;\n\t\t\tbreak;\n\t\tcase 1024:\n\t\t\tblocks_long >>= 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn blocks_long;\n}\n\nstatic void iblock_complete_cmd(struct se_cmd *cmd, blk_status_t blk_status)\n{\n\tstruct iblock_req *ibr = cmd->priv;\n\tu8 status;\n\n\tif (!refcount_dec_and_test(&ibr->pending))\n\t\treturn;\n\n\tif (blk_status == BLK_STS_RESV_CONFLICT)\n\t\tstatus = SAM_STAT_RESERVATION_CONFLICT;\n\telse if (atomic_read(&ibr->ib_bio_err_cnt))\n\t\tstatus = SAM_STAT_CHECK_CONDITION;\n\telse\n\t\tstatus = SAM_STAT_GOOD;\n\n\ttarget_complete_cmd(cmd, status);\n\tkfree(ibr);\n}\n\nstatic void iblock_bio_done(struct bio *bio)\n{\n\tstruct se_cmd *cmd = bio->bi_private;\n\tstruct iblock_req *ibr = cmd->priv;\n\tblk_status_t blk_status = bio->bi_status;\n\n\tif (bio->bi_status) {\n\t\tpr_err(\"bio error: %p,  err: %d\\n\", bio, bio->bi_status);\n\t\t \n\t\tatomic_inc(&ibr->ib_bio_err_cnt);\n\t\tsmp_mb__after_atomic();\n\t}\n\n\tbio_put(bio);\n\n\tiblock_complete_cmd(cmd, blk_status);\n}\n\nstatic struct bio *iblock_get_bio(struct se_cmd *cmd, sector_t lba, u32 sg_num,\n\t\t\t\t  blk_opf_t opf)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(cmd->se_dev);\n\tstruct bio *bio;\n\n\t \n\tbio = bio_alloc_bioset(ib_dev->ibd_bd, bio_max_segs(sg_num), opf,\n\t\t\t       GFP_NOIO, &ib_dev->ibd_bio_set);\n\tif (!bio) {\n\t\tpr_err(\"Unable to allocate memory for bio\\n\");\n\t\treturn NULL;\n\t}\n\n\tbio->bi_private = cmd;\n\tbio->bi_end_io = &iblock_bio_done;\n\tbio->bi_iter.bi_sector = lba;\n\n\treturn bio;\n}\n\nstatic void iblock_submit_bios(struct bio_list *list)\n{\n\tstruct blk_plug plug;\n\tstruct bio *bio;\n\t \n\tblk_start_plug(&plug);\n\twhile ((bio = bio_list_pop(list)))\n\t\tsubmit_bio(bio);\n\tblk_finish_plug(&plug);\n}\n\nstatic void iblock_end_io_flush(struct bio *bio)\n{\n\tstruct se_cmd *cmd = bio->bi_private;\n\n\tif (bio->bi_status)\n\t\tpr_err(\"IBLOCK: cache flush failed: %d\\n\", bio->bi_status);\n\n\tif (cmd) {\n\t\tif (bio->bi_status)\n\t\t\ttarget_complete_cmd(cmd, SAM_STAT_CHECK_CONDITION);\n\t\telse\n\t\t\ttarget_complete_cmd(cmd, SAM_STAT_GOOD);\n\t}\n\n\tbio_put(bio);\n}\n\n \nstatic sense_reason_t\niblock_execute_sync_cache(struct se_cmd *cmd)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(cmd->se_dev);\n\tint immed = (cmd->t_task_cdb[1] & 0x2);\n\tstruct bio *bio;\n\n\t \n\tif (immed)\n\t\ttarget_complete_cmd(cmd, SAM_STAT_GOOD);\n\n\tbio = bio_alloc(ib_dev->ibd_bd, 0, REQ_OP_WRITE | REQ_PREFLUSH,\n\t\t\tGFP_KERNEL);\n\tbio->bi_end_io = iblock_end_io_flush;\n\tif (!immed)\n\t\tbio->bi_private = cmd;\n\tsubmit_bio(bio);\n\treturn 0;\n}\n\nstatic sense_reason_t\niblock_execute_unmap(struct se_cmd *cmd, sector_t lba, sector_t nolb)\n{\n\tstruct block_device *bdev = IBLOCK_DEV(cmd->se_dev)->ibd_bd;\n\tstruct se_device *dev = cmd->se_dev;\n\tint ret;\n\n\tret = blkdev_issue_discard(bdev,\n\t\t\t\t   target_to_linux_sector(dev, lba),\n\t\t\t\t   target_to_linux_sector(dev,  nolb),\n\t\t\t\t   GFP_KERNEL);\n\tif (ret < 0) {\n\t\tpr_err(\"blkdev_issue_discard() failed: %d\\n\", ret);\n\t\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\t}\n\n\treturn 0;\n}\n\nstatic sense_reason_t\niblock_execute_zero_out(struct block_device *bdev, struct se_cmd *cmd)\n{\n\tstruct se_device *dev = cmd->se_dev;\n\tstruct scatterlist *sg = &cmd->t_data_sg[0];\n\tunsigned char *buf, *not_zero;\n\tint ret;\n\n\tbuf = kmap(sg_page(sg)) + sg->offset;\n\tif (!buf)\n\t\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\t \n\tnot_zero = memchr_inv(buf, 0x00, cmd->data_length);\n\tkunmap(sg_page(sg));\n\n\tif (not_zero)\n\t\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\n\tret = blkdev_issue_zeroout(bdev,\n\t\t\t\ttarget_to_linux_sector(dev, cmd->t_task_lba),\n\t\t\t\ttarget_to_linux_sector(dev,\n\t\t\t\t\tsbc_get_write_same_sectors(cmd)),\n\t\t\t\tGFP_KERNEL, BLKDEV_ZERO_NOUNMAP);\n\tif (ret)\n\t\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\n\ttarget_complete_cmd(cmd, SAM_STAT_GOOD);\n\treturn 0;\n}\n\nstatic sense_reason_t\niblock_execute_write_same(struct se_cmd *cmd)\n{\n\tstruct block_device *bdev = IBLOCK_DEV(cmd->se_dev)->ibd_bd;\n\tstruct iblock_req *ibr;\n\tstruct scatterlist *sg;\n\tstruct bio *bio;\n\tstruct bio_list list;\n\tstruct se_device *dev = cmd->se_dev;\n\tsector_t block_lba = target_to_linux_sector(dev, cmd->t_task_lba);\n\tsector_t sectors = target_to_linux_sector(dev,\n\t\t\t\t\tsbc_get_write_same_sectors(cmd));\n\n\tif (cmd->prot_op) {\n\t\tpr_err(\"WRITE_SAME: Protection information with IBLOCK\"\n\t\t       \" backends not supported\\n\");\n\t\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\t}\n\n\tif (!cmd->t_data_nents)\n\t\treturn TCM_INVALID_CDB_FIELD;\n\n\tsg = &cmd->t_data_sg[0];\n\n\tif (cmd->t_data_nents > 1 ||\n\t    sg->length != cmd->se_dev->dev_attrib.block_size) {\n\t\tpr_err(\"WRITE_SAME: Illegal SGL t_data_nents: %u length: %u\"\n\t\t\t\" block_size: %u\\n\", cmd->t_data_nents, sg->length,\n\t\t\tcmd->se_dev->dev_attrib.block_size);\n\t\treturn TCM_INVALID_CDB_FIELD;\n\t}\n\n\tif (bdev_write_zeroes_sectors(bdev)) {\n\t\tif (!iblock_execute_zero_out(bdev, cmd))\n\t\t\treturn 0;\n\t}\n\n\tibr = kzalloc(sizeof(struct iblock_req), GFP_KERNEL);\n\tif (!ibr)\n\t\tgoto fail;\n\tcmd->priv = ibr;\n\n\tbio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);\n\tif (!bio)\n\t\tgoto fail_free_ibr;\n\n\tbio_list_init(&list);\n\tbio_list_add(&list, bio);\n\n\trefcount_set(&ibr->pending, 1);\n\n\twhile (sectors) {\n\t\twhile (bio_add_page(bio, sg_page(sg), sg->length, sg->offset)\n\t\t\t\t!= sg->length) {\n\n\t\t\tbio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE);\n\t\t\tif (!bio)\n\t\t\t\tgoto fail_put_bios;\n\n\t\t\trefcount_inc(&ibr->pending);\n\t\t\tbio_list_add(&list, bio);\n\t\t}\n\n\t\t \n\t\tblock_lba += sg->length >> SECTOR_SHIFT;\n\t\tsectors -= sg->length >> SECTOR_SHIFT;\n\t}\n\n\tiblock_submit_bios(&list);\n\treturn 0;\n\nfail_put_bios:\n\twhile ((bio = bio_list_pop(&list)))\n\t\tbio_put(bio);\nfail_free_ibr:\n\tkfree(ibr);\nfail:\n\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n}\n\nenum {\n\tOpt_udev_path, Opt_readonly, Opt_force, Opt_err\n};\n\nstatic match_table_t tokens = {\n\t{Opt_udev_path, \"udev_path=%s\"},\n\t{Opt_readonly, \"readonly=%d\"},\n\t{Opt_force, \"force=%d\"},\n\t{Opt_err, NULL}\n};\n\nstatic ssize_t iblock_set_configfs_dev_params(struct se_device *dev,\n\t\tconst char *page, ssize_t count)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tchar *orig, *ptr, *arg_p, *opts;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint ret = 0, token;\n\tunsigned long tmp_readonly;\n\n\topts = kstrdup(page, GFP_KERNEL);\n\tif (!opts)\n\t\treturn -ENOMEM;\n\n\torig = opts;\n\n\twhile ((ptr = strsep(&opts, \",\\n\")) != NULL) {\n\t\tif (!*ptr)\n\t\t\tcontinue;\n\n\t\ttoken = match_token(ptr, tokens, args);\n\t\tswitch (token) {\n\t\tcase Opt_udev_path:\n\t\t\tif (ib_dev->ibd_bd) {\n\t\t\t\tpr_err(\"Unable to set udev_path= while\"\n\t\t\t\t\t\" ib_dev->ibd_bd exists\\n\");\n\t\t\t\tret = -EEXIST;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (match_strlcpy(ib_dev->ibd_udev_path, &args[0],\n\t\t\t\tSE_UDEV_PATH_LEN) == 0) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpr_debug(\"IBLOCK: Referencing UDEV path: %s\\n\",\n\t\t\t\t\tib_dev->ibd_udev_path);\n\t\t\tib_dev->ibd_flags |= IBDF_HAS_UDEV_PATH;\n\t\t\tbreak;\n\t\tcase Opt_readonly:\n\t\t\targ_p = match_strdup(&args[0]);\n\t\t\tif (!arg_p) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret = kstrtoul(arg_p, 0, &tmp_readonly);\n\t\t\tkfree(arg_p);\n\t\t\tif (ret < 0) {\n\t\t\t\tpr_err(\"kstrtoul() failed for\"\n\t\t\t\t\t\t\" readonly=\\n\");\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tib_dev->ibd_readonly = tmp_readonly;\n\t\t\tpr_debug(\"IBLOCK: readonly: %d\\n\", ib_dev->ibd_readonly);\n\t\t\tbreak;\n\t\tcase Opt_force:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tkfree(orig);\n\treturn (!ret) ? count : ret;\n}\n\nstatic ssize_t iblock_show_configfs_dev_params(struct se_device *dev, char *b)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tstruct block_device *bd = ib_dev->ibd_bd;\n\tssize_t bl = 0;\n\n\tif (bd)\n\t\tbl += sprintf(b + bl, \"iBlock device: %pg\", bd);\n\tif (ib_dev->ibd_flags & IBDF_HAS_UDEV_PATH)\n\t\tbl += sprintf(b + bl, \"  UDEV PATH: %s\",\n\t\t\t\tib_dev->ibd_udev_path);\n\tbl += sprintf(b + bl, \"  readonly: %d\\n\", ib_dev->ibd_readonly);\n\n\tbl += sprintf(b + bl, \"        \");\n\tif (bd) {\n\t\tbl += sprintf(b + bl, \"Major: %d Minor: %d  %s\\n\",\n\t\t\tMAJOR(bd->bd_dev), MINOR(bd->bd_dev),\n\t\t\t\"CLAIMED: IBLOCK\");\n\t} else {\n\t\tbl += sprintf(b + bl, \"Major: 0 Minor: 0\\n\");\n\t}\n\n\treturn bl;\n}\n\nstatic int\niblock_alloc_bip(struct se_cmd *cmd, struct bio *bio,\n\t\t struct sg_mapping_iter *miter)\n{\n\tstruct se_device *dev = cmd->se_dev;\n\tstruct blk_integrity *bi;\n\tstruct bio_integrity_payload *bip;\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tint rc;\n\tsize_t resid, len;\n\n\tbi = bdev_get_integrity(ib_dev->ibd_bd);\n\tif (!bi) {\n\t\tpr_err(\"Unable to locate bio_integrity\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tbip = bio_integrity_alloc(bio, GFP_NOIO, bio_max_segs(cmd->t_prot_nents));\n\tif (IS_ERR(bip)) {\n\t\tpr_err(\"Unable to allocate bio_integrity_payload\\n\");\n\t\treturn PTR_ERR(bip);\n\t}\n\n\t \n\tbip_set_seed(bip, bio->bi_iter.bi_sector >>\n\t\t\t\t  (bi->interval_exp - SECTOR_SHIFT));\n\n\tpr_debug(\"IBLOCK BIP Size: %u Sector: %llu\\n\", bip->bip_iter.bi_size,\n\t\t (unsigned long long)bip->bip_iter.bi_sector);\n\n\tresid = bio_integrity_bytes(bi, bio_sectors(bio));\n\twhile (resid > 0 && sg_miter_next(miter)) {\n\n\t\tlen = min_t(size_t, miter->length, resid);\n\t\trc = bio_integrity_add_page(bio, miter->page, len,\n\t\t\t\t\t    offset_in_page(miter->addr));\n\t\tif (rc != len) {\n\t\t\tpr_err(\"bio_integrity_add_page() failed; %d\\n\", rc);\n\t\t\tsg_miter_stop(miter);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tpr_debug(\"Added bio integrity page: %p length: %zu offset: %lu\\n\",\n\t\t\t  miter->page, len, offset_in_page(miter->addr));\n\n\t\tresid -= len;\n\t\tif (len < miter->length)\n\t\t\tmiter->consumed -= miter->length - len;\n\t}\n\tsg_miter_stop(miter);\n\n\treturn 0;\n}\n\nstatic sense_reason_t\niblock_execute_rw(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,\n\t\t  enum dma_data_direction data_direction)\n{\n\tstruct se_device *dev = cmd->se_dev;\n\tsector_t block_lba = target_to_linux_sector(dev, cmd->t_task_lba);\n\tstruct iblock_req *ibr;\n\tstruct bio *bio;\n\tstruct bio_list list;\n\tstruct scatterlist *sg;\n\tu32 sg_num = sgl_nents;\n\tblk_opf_t opf;\n\tunsigned bio_cnt;\n\tint i, rc;\n\tstruct sg_mapping_iter prot_miter;\n\tunsigned int miter_dir;\n\n\tif (data_direction == DMA_TO_DEVICE) {\n\t\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\n\t\t \n\t\topf = REQ_OP_WRITE | REQ_SYNC | REQ_IDLE;\n\t\t \n\t\tmiter_dir = SG_MITER_TO_SG;\n\t\tif (bdev_fua(ib_dev->ibd_bd)) {\n\t\t\tif (cmd->se_cmd_flags & SCF_FUA)\n\t\t\t\topf |= REQ_FUA;\n\t\t\telse if (!bdev_write_cache(ib_dev->ibd_bd))\n\t\t\t\topf |= REQ_FUA;\n\t\t}\n\t} else {\n\t\topf = REQ_OP_READ;\n\t\tmiter_dir = SG_MITER_FROM_SG;\n\t}\n\n\tibr = kzalloc(sizeof(struct iblock_req), GFP_KERNEL);\n\tif (!ibr)\n\t\tgoto fail;\n\tcmd->priv = ibr;\n\n\tif (!sgl_nents) {\n\t\trefcount_set(&ibr->pending, 1);\n\t\tiblock_complete_cmd(cmd, BLK_STS_OK);\n\t\treturn 0;\n\t}\n\n\tbio = iblock_get_bio(cmd, block_lba, sgl_nents, opf);\n\tif (!bio)\n\t\tgoto fail_free_ibr;\n\n\tbio_list_init(&list);\n\tbio_list_add(&list, bio);\n\n\trefcount_set(&ibr->pending, 2);\n\tbio_cnt = 1;\n\n\tif (cmd->prot_type && dev->dev_attrib.pi_prot_type)\n\t\tsg_miter_start(&prot_miter, cmd->t_prot_sg, cmd->t_prot_nents,\n\t\t\t       miter_dir);\n\n\tfor_each_sg(sgl, sg, sgl_nents, i) {\n\t\t \n\t\twhile (bio_add_page(bio, sg_page(sg), sg->length, sg->offset)\n\t\t\t\t!= sg->length) {\n\t\t\tif (cmd->prot_type && dev->dev_attrib.pi_prot_type) {\n\t\t\t\trc = iblock_alloc_bip(cmd, bio, &prot_miter);\n\t\t\t\tif (rc)\n\t\t\t\t\tgoto fail_put_bios;\n\t\t\t}\n\n\t\t\tif (bio_cnt >= IBLOCK_MAX_BIO_PER_TASK) {\n\t\t\t\tiblock_submit_bios(&list);\n\t\t\t\tbio_cnt = 0;\n\t\t\t}\n\n\t\t\tbio = iblock_get_bio(cmd, block_lba, sg_num, opf);\n\t\t\tif (!bio)\n\t\t\t\tgoto fail_put_bios;\n\n\t\t\trefcount_inc(&ibr->pending);\n\t\t\tbio_list_add(&list, bio);\n\t\t\tbio_cnt++;\n\t\t}\n\n\t\t \n\t\tblock_lba += sg->length >> SECTOR_SHIFT;\n\t\tsg_num--;\n\t}\n\n\tif (cmd->prot_type && dev->dev_attrib.pi_prot_type) {\n\t\trc = iblock_alloc_bip(cmd, bio, &prot_miter);\n\t\tif (rc)\n\t\t\tgoto fail_put_bios;\n\t}\n\n\tiblock_submit_bios(&list);\n\tiblock_complete_cmd(cmd, BLK_STS_OK);\n\treturn 0;\n\nfail_put_bios:\n\twhile ((bio = bio_list_pop(&list)))\n\t\tbio_put(bio);\nfail_free_ibr:\n\tkfree(ibr);\nfail:\n\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n}\n\nstatic sense_reason_t iblock_execute_pr_out(struct se_cmd *cmd, u8 sa, u64 key,\n\t\t\t\t\t    u64 sa_key, u8 type, bool aptpl)\n{\n\tstruct se_device *dev = cmd->se_dev;\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tstruct block_device *bdev = ib_dev->ibd_bd;\n\tconst struct pr_ops *ops = bdev->bd_disk->fops->pr_ops;\n\tint ret;\n\n\tif (!ops) {\n\t\tpr_err(\"Block device does not support pr_ops but iblock device has been configured for PR passthrough.\\n\");\n\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t}\n\n\tswitch (sa) {\n\tcase PRO_REGISTER:\n\tcase PRO_REGISTER_AND_IGNORE_EXISTING_KEY:\n\t\tif (!ops->pr_register) {\n\t\t\tpr_err(\"block device does not support pr_register.\\n\");\n\t\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t\t}\n\n\t\t \n\t\tif (!aptpl)\n\t\t\tpr_info(\"APTPL not set by initiator, but will be used.\\n\");\n\n\t\tret = ops->pr_register(bdev, key, sa_key,\n\t\t\t\tsa == PRO_REGISTER ? 0 : PR_FL_IGNORE_KEY);\n\t\tbreak;\n\tcase PRO_RESERVE:\n\t\tif (!ops->pr_reserve) {\n\t\t\tpr_err(\"block_device does not support pr_reserve.\\n\");\n\t\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t\t}\n\n\t\tret = ops->pr_reserve(bdev, key, scsi_pr_type_to_block(type), 0);\n\t\tbreak;\n\tcase PRO_CLEAR:\n\t\tif (!ops->pr_clear) {\n\t\t\tpr_err(\"block_device does not support pr_clear.\\n\");\n\t\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t\t}\n\n\t\tret = ops->pr_clear(bdev, key);\n\t\tbreak;\n\tcase PRO_PREEMPT:\n\tcase PRO_PREEMPT_AND_ABORT:\n\t\tif (!ops->pr_clear) {\n\t\t\tpr_err(\"block_device does not support pr_preempt.\\n\");\n\t\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t\t}\n\n\t\tret = ops->pr_preempt(bdev, key, sa_key,\n\t\t\t\t      scsi_pr_type_to_block(type),\n\t\t\t\t      sa == PRO_PREEMPT_AND_ABORT);\n\t\tbreak;\n\tcase PRO_RELEASE:\n\t\tif (!ops->pr_clear) {\n\t\t\tpr_err(\"block_device does not support pr_pclear.\\n\");\n\t\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t\t}\n\n\t\tret = ops->pr_release(bdev, key, scsi_pr_type_to_block(type));\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Unknown PERSISTENT_RESERVE_OUT SA: 0x%02x\\n\", sa);\n\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t}\n\n\tif (!ret)\n\t\treturn TCM_NO_SENSE;\n\telse if (ret == PR_STS_RESERVATION_CONFLICT)\n\t\treturn TCM_RESERVATION_CONFLICT;\n\telse\n\t\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n}\n\nstatic void iblock_pr_report_caps(unsigned char *param_data)\n{\n\tu16 len = 8;\n\n\tput_unaligned_be16(len, &param_data[0]);\n\t \n\t \n\tparam_data[2] |= 0x01;\n\t \n\tparam_data[3] |= 0x80;\n\t \n\tparam_data[3] |= 0x10;  \n\t \n\tparam_data[3] |= 0x01;\n\t \n\tparam_data[4] |= 0x80;  \n\tparam_data[4] |= 0x40;  \n\tparam_data[4] |= 0x20;  \n\tparam_data[4] |= 0x08;  \n\tparam_data[4] |= 0x02;  \n\tparam_data[5] |= 0x01;  \n}\n\nstatic sense_reason_t iblock_pr_read_keys(struct se_cmd *cmd,\n\t\t\t\t\t  unsigned char *param_data)\n{\n\tstruct se_device *dev = cmd->se_dev;\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tstruct block_device *bdev = ib_dev->ibd_bd;\n\tconst struct pr_ops *ops = bdev->bd_disk->fops->pr_ops;\n\tint i, len, paths, data_offset;\n\tstruct pr_keys *keys;\n\tsense_reason_t ret;\n\n\tif (!ops) {\n\t\tpr_err(\"Block device does not support pr_ops but iblock device has been configured for PR passthrough.\\n\");\n\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t}\n\n\tif (!ops->pr_read_keys) {\n\t\tpr_err(\"Block device does not support read_keys.\\n\");\n\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t}\n\n\t \n\tpaths = 16;\nretry:\n\tlen = 8 * paths;\n\tkeys = kzalloc(sizeof(*keys) + len, GFP_KERNEL);\n\tif (!keys)\n\t\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\n\tkeys->num_keys = paths;\n\tif (!ops->pr_read_keys(bdev, keys)) {\n\t\tif (keys->num_keys > paths) {\n\t\t\tkfree(keys);\n\t\t\tpaths *= 2;\n\t\t\tgoto retry;\n\t\t}\n\t} else {\n\t\tret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\t\tgoto free_keys;\n\t}\n\n\tret = TCM_NO_SENSE;\n\n\tput_unaligned_be32(keys->generation, &param_data[0]);\n\tif (!keys->num_keys) {\n\t\tput_unaligned_be32(0, &param_data[4]);\n\t\tgoto free_keys;\n\t}\n\n\tput_unaligned_be32(8 * keys->num_keys, &param_data[4]);\n\n\tdata_offset = 8;\n\tfor (i = 0; i < keys->num_keys; i++) {\n\t\tif (data_offset + 8 > cmd->data_length)\n\t\t\tbreak;\n\n\t\tput_unaligned_be64(keys->keys[i], &param_data[data_offset]);\n\t\tdata_offset += 8;\n\t}\n\nfree_keys:\n\tkfree(keys);\n\treturn ret;\n}\n\nstatic sense_reason_t iblock_pr_read_reservation(struct se_cmd *cmd,\n\t\t\t\t\t\t unsigned char *param_data)\n{\n\tstruct se_device *dev = cmd->se_dev;\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tstruct block_device *bdev = ib_dev->ibd_bd;\n\tconst struct pr_ops *ops = bdev->bd_disk->fops->pr_ops;\n\tstruct pr_held_reservation rsv = { };\n\n\tif (!ops) {\n\t\tpr_err(\"Block device does not support pr_ops but iblock device has been configured for PR passthrough.\\n\");\n\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t}\n\n\tif (!ops->pr_read_reservation) {\n\t\tpr_err(\"Block device does not support read_keys.\\n\");\n\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t}\n\n\tif (ops->pr_read_reservation(bdev, &rsv))\n\t\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\n\tput_unaligned_be32(rsv.generation, &param_data[0]);\n\tif (!block_pr_type_to_scsi(rsv.type)) {\n\t\tput_unaligned_be32(0, &param_data[4]);\n\t\treturn TCM_NO_SENSE;\n\t}\n\n\tput_unaligned_be32(16, &param_data[4]);\n\n\tif (cmd->data_length < 16)\n\t\treturn TCM_NO_SENSE;\n\tput_unaligned_be64(rsv.key, &param_data[8]);\n\n\tif (cmd->data_length < 22)\n\t\treturn TCM_NO_SENSE;\n\tparam_data[21] = block_pr_type_to_scsi(rsv.type);\n\n\treturn TCM_NO_SENSE;\n}\n\nstatic sense_reason_t iblock_execute_pr_in(struct se_cmd *cmd, u8 sa,\n\t\t\t\t\t   unsigned char *param_data)\n{\n\tsense_reason_t ret = TCM_NO_SENSE;\n\n\tswitch (sa) {\n\tcase PRI_REPORT_CAPABILITIES:\n\t\tiblock_pr_report_caps(param_data);\n\t\tbreak;\n\tcase PRI_READ_KEYS:\n\t\tret = iblock_pr_read_keys(cmd, param_data);\n\t\tbreak;\n\tcase PRI_READ_RESERVATION:\n\t\tret = iblock_pr_read_reservation(cmd, param_data);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Unknown PERSISTENT_RESERVE_IN SA: 0x%02x\\n\", sa);\n\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t}\n\n\treturn ret;\n}\n\nstatic sector_t iblock_get_alignment_offset_lbas(struct se_device *dev)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tstruct block_device *bd = ib_dev->ibd_bd;\n\tint ret;\n\n\tret = bdev_alignment_offset(bd);\n\tif (ret == -1)\n\t\treturn 0;\n\n\t \n\treturn ret / bdev_logical_block_size(bd);\n}\n\nstatic unsigned int iblock_get_lbppbe(struct se_device *dev)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tstruct block_device *bd = ib_dev->ibd_bd;\n\tunsigned int logs_per_phys =\n\t\tbdev_physical_block_size(bd) / bdev_logical_block_size(bd);\n\n\treturn ilog2(logs_per_phys);\n}\n\nstatic unsigned int iblock_get_io_min(struct se_device *dev)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tstruct block_device *bd = ib_dev->ibd_bd;\n\n\treturn bdev_io_min(bd);\n}\n\nstatic unsigned int iblock_get_io_opt(struct se_device *dev)\n{\n\tstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\n\tstruct block_device *bd = ib_dev->ibd_bd;\n\n\treturn bdev_io_opt(bd);\n}\n\nstatic struct exec_cmd_ops iblock_exec_cmd_ops = {\n\t.execute_rw\t\t= iblock_execute_rw,\n\t.execute_sync_cache\t= iblock_execute_sync_cache,\n\t.execute_write_same\t= iblock_execute_write_same,\n\t.execute_unmap\t\t= iblock_execute_unmap,\n\t.execute_pr_out\t\t= iblock_execute_pr_out,\n\t.execute_pr_in\t\t= iblock_execute_pr_in,\n};\n\nstatic sense_reason_t\niblock_parse_cdb(struct se_cmd *cmd)\n{\n\treturn sbc_parse_cdb(cmd, &iblock_exec_cmd_ops);\n}\n\nstatic bool iblock_get_write_cache(struct se_device *dev)\n{\n\treturn bdev_write_cache(IBLOCK_DEV(dev)->ibd_bd);\n}\n\nstatic const struct target_backend_ops iblock_ops = {\n\t.name\t\t\t= \"iblock\",\n\t.inquiry_prod\t\t= \"IBLOCK\",\n\t.transport_flags_changeable = TRANSPORT_FLAG_PASSTHROUGH_PGR,\n\t.inquiry_rev\t\t= IBLOCK_VERSION,\n\t.owner\t\t\t= THIS_MODULE,\n\t.attach_hba\t\t= iblock_attach_hba,\n\t.detach_hba\t\t= iblock_detach_hba,\n\t.alloc_device\t\t= iblock_alloc_device,\n\t.configure_device\t= iblock_configure_device,\n\t.destroy_device\t\t= iblock_destroy_device,\n\t.free_device\t\t= iblock_free_device,\n\t.configure_unmap\t= iblock_configure_unmap,\n\t.plug_device\t\t= iblock_plug_device,\n\t.unplug_device\t\t= iblock_unplug_device,\n\t.parse_cdb\t\t= iblock_parse_cdb,\n\t.set_configfs_dev_params = iblock_set_configfs_dev_params,\n\t.show_configfs_dev_params = iblock_show_configfs_dev_params,\n\t.get_device_type\t= sbc_get_device_type,\n\t.get_blocks\t\t= iblock_get_blocks,\n\t.get_alignment_offset_lbas = iblock_get_alignment_offset_lbas,\n\t.get_lbppbe\t\t= iblock_get_lbppbe,\n\t.get_io_min\t\t= iblock_get_io_min,\n\t.get_io_opt\t\t= iblock_get_io_opt,\n\t.get_write_cache\t= iblock_get_write_cache,\n\t.tb_dev_attrib_attrs\t= sbc_attrib_attrs,\n};\n\nstatic int __init iblock_module_init(void)\n{\n\treturn transport_backend_register(&iblock_ops);\n}\n\nstatic void __exit iblock_module_exit(void)\n{\n\ttarget_backend_unregister(&iblock_ops);\n}\n\nMODULE_DESCRIPTION(\"TCM IBLOCK subsystem plugin\");\nMODULE_AUTHOR(\"nab@Linux-iSCSI.org\");\nMODULE_LICENSE(\"GPL\");\n\nmodule_init(iblock_module_init);\nmodule_exit(iblock_module_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}