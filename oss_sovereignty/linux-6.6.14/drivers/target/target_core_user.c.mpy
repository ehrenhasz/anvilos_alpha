{
  "module_name": "target_core_user.c",
  "hash_id": "a0299e4a0d28184dfaa878041b6e14c160d0e224e7d4cae34181f340f2022919",
  "original_prompt": "Ingested from linux-6.6.14/drivers/target/target_core_user.c",
  "human_readable_source": "\n \n\n#include <linux/spinlock.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/timer.h>\n#include <linux/parser.h>\n#include <linux/vmalloc.h>\n#include <linux/uio_driver.h>\n#include <linux/xarray.h>\n#include <linux/stringify.h>\n#include <linux/bitops.h>\n#include <linux/highmem.h>\n#include <linux/configfs.h>\n#include <linux/mutex.h>\n#include <linux/workqueue.h>\n#include <linux/pagemap.h>\n#include <net/genetlink.h>\n#include <scsi/scsi_common.h>\n#include <scsi/scsi_proto.h>\n#include <target/target_core_base.h>\n#include <target/target_core_fabric.h>\n#include <target/target_core_backend.h>\n\n#include <linux/target_core_user.h>\n\n \n\n#define TCMU_TIME_OUT (30 * MSEC_PER_SEC)\n\n \n#define MB_CMDR_SIZE_DEF (8 * 1024 * 1024)\n \n#define CMDR_OFF ((__u32)sizeof(struct tcmu_mailbox))\n#define CMDR_SIZE_DEF (MB_CMDR_SIZE_DEF - CMDR_OFF)\n\n \n#define DATA_PAGES_PER_BLK_DEF 1\n#define DATA_AREA_PAGES_DEF (256 * 1024)\n\n#define TCMU_MBS_TO_PAGES(_mbs) ((size_t)_mbs << (20 - PAGE_SHIFT))\n#define TCMU_PAGES_TO_MBS(_pages) (_pages >> (20 - PAGE_SHIFT))\n\n \n#define TCMU_GLOBAL_MAX_PAGES_DEF (512 * 1024)\n\nstatic u8 tcmu_kern_cmd_reply_supported;\nstatic u8 tcmu_netlink_blocked;\n\nstatic struct device *tcmu_root_device;\n\nstruct tcmu_hba {\n\tu32 host_id;\n};\n\n#define TCMU_CONFIG_LEN 256\n\nstatic DEFINE_MUTEX(tcmu_nl_cmd_mutex);\nstatic LIST_HEAD(tcmu_nl_cmd_list);\n\nstruct tcmu_dev;\n\nstruct tcmu_nl_cmd {\n\t \n\tstruct completion complete;\n\tstruct list_head nl_list;\n\tstruct tcmu_dev *udev;\n\tint cmd;\n\tint status;\n};\n\nstruct tcmu_dev {\n\tstruct list_head node;\n\tstruct kref kref;\n\n\tstruct se_device se_dev;\n\tstruct se_dev_plug se_plug;\n\n\tchar *name;\n\tstruct se_hba *hba;\n\n#define TCMU_DEV_BIT_OPEN 0\n#define TCMU_DEV_BIT_BROKEN 1\n#define TCMU_DEV_BIT_BLOCKED 2\n#define TCMU_DEV_BIT_TMR_NOTIFY 3\n#define TCMU_DEV_BIT_PLUGGED 4\n\tunsigned long flags;\n\n\tstruct uio_info uio_info;\n\n\tstruct inode *inode;\n\n\tuint64_t dev_size;\n\n\tstruct tcmu_mailbox *mb_addr;\n\tvoid *cmdr;\n\tu32 cmdr_size;\n\tu32 cmdr_last_cleaned;\n\t \n\t \n\tsize_t data_off;\n\tint data_area_mb;\n\tuint32_t max_blocks;\n\tsize_t mmap_pages;\n\n\tstruct mutex cmdr_lock;\n\tstruct list_head qfull_queue;\n\tstruct list_head tmr_queue;\n\n\tuint32_t dbi_max;\n\tuint32_t dbi_thresh;\n\tunsigned long *data_bitmap;\n\tstruct xarray data_pages;\n\tuint32_t data_pages_per_blk;\n\tuint32_t data_blk_size;\n\n\tstruct xarray commands;\n\n\tstruct timer_list cmd_timer;\n\tunsigned int cmd_time_out;\n\tstruct list_head inflight_queue;\n\n\tstruct timer_list qfull_timer;\n\tint qfull_time_out;\n\n\tstruct list_head timedout_entry;\n\n\tstruct tcmu_nl_cmd curr_nl_cmd;\n\n\tchar dev_config[TCMU_CONFIG_LEN];\n\n\tint nl_reply_supported;\n};\n\n#define TCMU_DEV(_se_dev) container_of(_se_dev, struct tcmu_dev, se_dev)\n\nstruct tcmu_cmd {\n\tstruct se_cmd *se_cmd;\n\tstruct tcmu_dev *tcmu_dev;\n\tstruct list_head queue_entry;\n\n\tuint16_t cmd_id;\n\n\t \n\tuint32_t dbi_cnt;\n\tuint32_t dbi_bidi_cnt;\n\tuint32_t dbi_cur;\n\tuint32_t *dbi;\n\n\tuint32_t data_len_bidi;\n\n\tunsigned long deadline;\n\n#define TCMU_CMD_BIT_EXPIRED 0\n#define TCMU_CMD_BIT_KEEP_BUF 1\n\tunsigned long flags;\n};\n\nstruct tcmu_tmr {\n\tstruct list_head queue_entry;\n\n\tuint8_t tmr_type;\n\tuint32_t tmr_cmd_cnt;\n\tint16_t tmr_cmd_ids[];\n};\n\n \nstatic DEFINE_MUTEX(root_udev_mutex);\nstatic LIST_HEAD(root_udev);\n\nstatic DEFINE_SPINLOCK(timed_out_udevs_lock);\nstatic LIST_HEAD(timed_out_udevs);\n\nstatic struct kmem_cache *tcmu_cmd_cache;\n\nstatic atomic_t global_page_count = ATOMIC_INIT(0);\nstatic struct delayed_work tcmu_unmap_work;\nstatic int tcmu_global_max_pages = TCMU_GLOBAL_MAX_PAGES_DEF;\n\nstatic int tcmu_set_global_max_data_area(const char *str,\n\t\t\t\t\t const struct kernel_param *kp)\n{\n\tint ret, max_area_mb;\n\n\tret = kstrtoint(str, 10, &max_area_mb);\n\tif (ret)\n\t\treturn -EINVAL;\n\n\tif (max_area_mb <= 0) {\n\t\tpr_err(\"global_max_data_area must be larger than 0.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ttcmu_global_max_pages = TCMU_MBS_TO_PAGES(max_area_mb);\n\tif (atomic_read(&global_page_count) > tcmu_global_max_pages)\n\t\tschedule_delayed_work(&tcmu_unmap_work, 0);\n\telse\n\t\tcancel_delayed_work_sync(&tcmu_unmap_work);\n\n\treturn 0;\n}\n\nstatic int tcmu_get_global_max_data_area(char *buffer,\n\t\t\t\t\t const struct kernel_param *kp)\n{\n\treturn sprintf(buffer, \"%d\\n\", TCMU_PAGES_TO_MBS(tcmu_global_max_pages));\n}\n\nstatic const struct kernel_param_ops tcmu_global_max_data_area_op = {\n\t.set = tcmu_set_global_max_data_area,\n\t.get = tcmu_get_global_max_data_area,\n};\n\nmodule_param_cb(global_max_data_area_mb, &tcmu_global_max_data_area_op, NULL,\n\t\tS_IWUSR | S_IRUGO);\nMODULE_PARM_DESC(global_max_data_area_mb,\n\t\t \"Max MBs allowed to be allocated to all the tcmu device's \"\n\t\t \"data areas.\");\n\nstatic int tcmu_get_block_netlink(char *buffer,\n\t\t\t\t  const struct kernel_param *kp)\n{\n\treturn sprintf(buffer, \"%s\\n\", tcmu_netlink_blocked ?\n\t\t       \"blocked\" : \"unblocked\");\n}\n\nstatic int tcmu_set_block_netlink(const char *str,\n\t\t\t\t  const struct kernel_param *kp)\n{\n\tint ret;\n\tu8 val;\n\n\tret = kstrtou8(str, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (val > 1) {\n\t\tpr_err(\"Invalid block netlink value %u\\n\", val);\n\t\treturn -EINVAL;\n\t}\n\n\ttcmu_netlink_blocked = val;\n\treturn 0;\n}\n\nstatic const struct kernel_param_ops tcmu_block_netlink_op = {\n\t.set = tcmu_set_block_netlink,\n\t.get = tcmu_get_block_netlink,\n};\n\nmodule_param_cb(block_netlink, &tcmu_block_netlink_op, NULL, S_IWUSR | S_IRUGO);\nMODULE_PARM_DESC(block_netlink, \"Block new netlink commands.\");\n\nstatic int tcmu_fail_netlink_cmd(struct tcmu_nl_cmd *nl_cmd)\n{\n\tstruct tcmu_dev *udev = nl_cmd->udev;\n\n\tif (!tcmu_netlink_blocked) {\n\t\tpr_err(\"Could not reset device's netlink interface. Netlink is not blocked.\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tif (nl_cmd->cmd != TCMU_CMD_UNSPEC) {\n\t\tpr_debug(\"Aborting nl cmd %d on %s\\n\", nl_cmd->cmd, udev->name);\n\t\tnl_cmd->status = -EINTR;\n\t\tlist_del(&nl_cmd->nl_list);\n\t\tcomplete(&nl_cmd->complete);\n\t}\n\treturn 0;\n}\n\nstatic int tcmu_set_reset_netlink(const char *str,\n\t\t\t\t  const struct kernel_param *kp)\n{\n\tstruct tcmu_nl_cmd *nl_cmd, *tmp_cmd;\n\tint ret;\n\tu8 val;\n\n\tret = kstrtou8(str, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (val != 1) {\n\t\tpr_err(\"Invalid reset netlink value %u\\n\", val);\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&tcmu_nl_cmd_mutex);\n\tlist_for_each_entry_safe(nl_cmd, tmp_cmd, &tcmu_nl_cmd_list, nl_list) {\n\t\tret = tcmu_fail_netlink_cmd(nl_cmd);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&tcmu_nl_cmd_mutex);\n\n\treturn ret;\n}\n\nstatic const struct kernel_param_ops tcmu_reset_netlink_op = {\n\t.set = tcmu_set_reset_netlink,\n};\n\nmodule_param_cb(reset_netlink, &tcmu_reset_netlink_op, NULL, S_IWUSR);\nMODULE_PARM_DESC(reset_netlink, \"Reset netlink commands.\");\n\n \nenum tcmu_multicast_groups {\n\tTCMU_MCGRP_CONFIG,\n};\n\nstatic const struct genl_multicast_group tcmu_mcgrps[] = {\n\t[TCMU_MCGRP_CONFIG] = { .name = \"config\", },\n};\n\nstatic struct nla_policy tcmu_attr_policy[TCMU_ATTR_MAX+1] = {\n\t[TCMU_ATTR_DEVICE]\t= { .type = NLA_STRING },\n\t[TCMU_ATTR_MINOR]\t= { .type = NLA_U32 },\n\t[TCMU_ATTR_CMD_STATUS]\t= { .type = NLA_S32 },\n\t[TCMU_ATTR_DEVICE_ID]\t= { .type = NLA_U32 },\n\t[TCMU_ATTR_SUPP_KERN_CMD_REPLY] = { .type = NLA_U8 },\n};\n\nstatic int tcmu_genl_cmd_done(struct genl_info *info, int completed_cmd)\n{\n\tstruct tcmu_dev *udev = NULL;\n\tstruct tcmu_nl_cmd *nl_cmd;\n\tint dev_id, rc, ret = 0;\n\n\tif (!info->attrs[TCMU_ATTR_CMD_STATUS] ||\n\t    !info->attrs[TCMU_ATTR_DEVICE_ID]) {\n\t\tprintk(KERN_ERR \"TCMU_ATTR_CMD_STATUS or TCMU_ATTR_DEVICE_ID not set, doing nothing\\n\");\n\t\treturn -EINVAL;\n        }\n\n\tdev_id = nla_get_u32(info->attrs[TCMU_ATTR_DEVICE_ID]);\n\trc = nla_get_s32(info->attrs[TCMU_ATTR_CMD_STATUS]);\n\n\tmutex_lock(&tcmu_nl_cmd_mutex);\n\tlist_for_each_entry(nl_cmd, &tcmu_nl_cmd_list, nl_list) {\n\t\tif (nl_cmd->udev->se_dev.dev_index == dev_id) {\n\t\t\tudev = nl_cmd->udev;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!udev) {\n\t\tpr_err(\"tcmu nl cmd %u/%d completion could not find device with dev id %u.\\n\",\n\t\t       completed_cmd, rc, dev_id);\n\t\tret = -ENODEV;\n\t\tgoto unlock;\n\t}\n\tlist_del(&nl_cmd->nl_list);\n\n\tpr_debug(\"%s genl cmd done got id %d curr %d done %d rc %d stat %d\\n\",\n\t\t udev->name, dev_id, nl_cmd->cmd, completed_cmd, rc,\n\t\t nl_cmd->status);\n\n\tif (nl_cmd->cmd != completed_cmd) {\n\t\tpr_err(\"Mismatched commands on %s (Expecting reply for %d. Current %d).\\n\",\n\t\t       udev->name, completed_cmd, nl_cmd->cmd);\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tnl_cmd->status = rc;\n\tcomplete(&nl_cmd->complete);\nunlock:\n\tmutex_unlock(&tcmu_nl_cmd_mutex);\n\treturn ret;\n}\n\nstatic int tcmu_genl_rm_dev_done(struct sk_buff *skb, struct genl_info *info)\n{\n\treturn tcmu_genl_cmd_done(info, TCMU_CMD_REMOVED_DEVICE);\n}\n\nstatic int tcmu_genl_add_dev_done(struct sk_buff *skb, struct genl_info *info)\n{\n\treturn tcmu_genl_cmd_done(info, TCMU_CMD_ADDED_DEVICE);\n}\n\nstatic int tcmu_genl_reconfig_dev_done(struct sk_buff *skb,\n\t\t\t\t       struct genl_info *info)\n{\n\treturn tcmu_genl_cmd_done(info, TCMU_CMD_RECONFIG_DEVICE);\n}\n\nstatic int tcmu_genl_set_features(struct sk_buff *skb, struct genl_info *info)\n{\n\tif (info->attrs[TCMU_ATTR_SUPP_KERN_CMD_REPLY]) {\n\t\ttcmu_kern_cmd_reply_supported  =\n\t\t\tnla_get_u8(info->attrs[TCMU_ATTR_SUPP_KERN_CMD_REPLY]);\n\t\tprintk(KERN_INFO \"tcmu daemon: command reply support %u.\\n\",\n\t\t       tcmu_kern_cmd_reply_supported);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct genl_small_ops tcmu_genl_ops[] = {\n\t{\n\t\t.cmd\t= TCMU_CMD_SET_FEATURES,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.flags\t= GENL_ADMIN_PERM,\n\t\t.doit\t= tcmu_genl_set_features,\n\t},\n\t{\n\t\t.cmd\t= TCMU_CMD_ADDED_DEVICE_DONE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.flags\t= GENL_ADMIN_PERM,\n\t\t.doit\t= tcmu_genl_add_dev_done,\n\t},\n\t{\n\t\t.cmd\t= TCMU_CMD_REMOVED_DEVICE_DONE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.flags\t= GENL_ADMIN_PERM,\n\t\t.doit\t= tcmu_genl_rm_dev_done,\n\t},\n\t{\n\t\t.cmd\t= TCMU_CMD_RECONFIG_DEVICE_DONE,\n\t\t.validate = GENL_DONT_VALIDATE_STRICT | GENL_DONT_VALIDATE_DUMP,\n\t\t.flags\t= GENL_ADMIN_PERM,\n\t\t.doit\t= tcmu_genl_reconfig_dev_done,\n\t},\n};\n\n \nstatic struct genl_family tcmu_genl_family __ro_after_init = {\n\t.module = THIS_MODULE,\n\t.hdrsize = 0,\n\t.name = \"TCM-USER\",\n\t.version = 2,\n\t.maxattr = TCMU_ATTR_MAX,\n\t.policy = tcmu_attr_policy,\n\t.mcgrps = tcmu_mcgrps,\n\t.n_mcgrps = ARRAY_SIZE(tcmu_mcgrps),\n\t.netnsok = true,\n\t.small_ops = tcmu_genl_ops,\n\t.n_small_ops = ARRAY_SIZE(tcmu_genl_ops),\n\t.resv_start_op = TCMU_CMD_SET_FEATURES + 1,\n};\n\n#define tcmu_cmd_set_dbi_cur(cmd, index) ((cmd)->dbi_cur = (index))\n#define tcmu_cmd_reset_dbi_cur(cmd) tcmu_cmd_set_dbi_cur(cmd, 0)\n#define tcmu_cmd_set_dbi(cmd, index) ((cmd)->dbi[(cmd)->dbi_cur++] = (index))\n#define tcmu_cmd_get_dbi(cmd) ((cmd)->dbi[(cmd)->dbi_cur++])\n\nstatic void tcmu_cmd_free_data(struct tcmu_cmd *tcmu_cmd, uint32_t len)\n{\n\tstruct tcmu_dev *udev = tcmu_cmd->tcmu_dev;\n\tuint32_t i;\n\n\tfor (i = 0; i < len; i++)\n\t\tclear_bit(tcmu_cmd->dbi[i], udev->data_bitmap);\n}\n\nstatic inline int tcmu_get_empty_block(struct tcmu_dev *udev,\n\t\t\t\t       struct tcmu_cmd *tcmu_cmd,\n\t\t\t\t       int prev_dbi, int length, int *iov_cnt)\n{\n\tXA_STATE(xas, &udev->data_pages, 0);\n\tstruct page *page;\n\tint i, cnt, dbi, dpi;\n\tint page_cnt = DIV_ROUND_UP(length, PAGE_SIZE);\n\n\tdbi = find_first_zero_bit(udev->data_bitmap, udev->dbi_thresh);\n\tif (dbi == udev->dbi_thresh)\n\t\treturn -1;\n\n\tdpi = dbi * udev->data_pages_per_blk;\n\t \n\txas_set(&xas, dpi);\n\trcu_read_lock();\n\tfor (cnt = 0; xas_next(&xas) && cnt < page_cnt;)\n\t\tcnt++;\n\trcu_read_unlock();\n\n\tfor (i = cnt; i < page_cnt; i++) {\n\t\t \n\t\tpage = alloc_page(GFP_NOIO | __GFP_ZERO);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tif (xa_store(&udev->data_pages, dpi + i, page, GFP_NOIO)) {\n\t\t\t__free_page(page);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (atomic_add_return(i - cnt, &global_page_count) >\n\t\t\t      tcmu_global_max_pages)\n\t\tschedule_delayed_work(&tcmu_unmap_work, 0);\n\n\tif (i && dbi > udev->dbi_max)\n\t\tudev->dbi_max = dbi;\n\n\tset_bit(dbi, udev->data_bitmap);\n\ttcmu_cmd_set_dbi(tcmu_cmd, dbi);\n\n\tif (dbi != prev_dbi + 1)\n\t\t*iov_cnt += 1;\n\n\treturn i == page_cnt ? dbi : -1;\n}\n\nstatic int tcmu_get_empty_blocks(struct tcmu_dev *udev,\n\t\t\t\t struct tcmu_cmd *tcmu_cmd, int length)\n{\n\t \n\tint dbi = -2;\n\tint blk_data_len, iov_cnt = 0;\n\tuint32_t blk_size = udev->data_blk_size;\n\n\tfor (; length > 0; length -= blk_size) {\n\t\tblk_data_len = min_t(uint32_t, length, blk_size);\n\t\tdbi = tcmu_get_empty_block(udev, tcmu_cmd, dbi, blk_data_len,\n\t\t\t\t\t   &iov_cnt);\n\t\tif (dbi < 0)\n\t\t\treturn -1;\n\t}\n\treturn iov_cnt;\n}\n\nstatic inline void tcmu_free_cmd(struct tcmu_cmd *tcmu_cmd)\n{\n\tkfree(tcmu_cmd->dbi);\n\tkmem_cache_free(tcmu_cmd_cache, tcmu_cmd);\n}\n\nstatic inline void tcmu_cmd_set_block_cnts(struct tcmu_cmd *cmd)\n{\n\tint i, len;\n\tstruct se_cmd *se_cmd = cmd->se_cmd;\n\tuint32_t blk_size = cmd->tcmu_dev->data_blk_size;\n\n\tcmd->dbi_cnt = DIV_ROUND_UP(se_cmd->data_length, blk_size);\n\n\tif (se_cmd->se_cmd_flags & SCF_BIDI) {\n\t\tBUG_ON(!(se_cmd->t_bidi_data_sg && se_cmd->t_bidi_data_nents));\n\t\tfor (i = 0, len = 0; i < se_cmd->t_bidi_data_nents; i++)\n\t\t\tlen += se_cmd->t_bidi_data_sg[i].length;\n\t\tcmd->dbi_bidi_cnt = DIV_ROUND_UP(len, blk_size);\n\t\tcmd->dbi_cnt += cmd->dbi_bidi_cnt;\n\t\tcmd->data_len_bidi = len;\n\t}\n}\n\nstatic int new_block_to_iov(struct tcmu_dev *udev, struct tcmu_cmd *cmd,\n\t\t\t    struct iovec **iov, int prev_dbi, int len)\n{\n\t \n\tint dbi = tcmu_cmd_get_dbi(cmd);\n\n\t \n\tlen = min_t(int,  len, udev->data_blk_size);\n\n\t \n\tif (dbi != prev_dbi + 1) {\n\t\t \n\t\tif (prev_dbi >= 0)\n\t\t\t(*iov)++;\n\t\t \n\t\t(*iov)->iov_base = (void __user *)\n\t\t\t\t   (udev->data_off + dbi * udev->data_blk_size);\n\t}\n\t(*iov)->iov_len += len;\n\n\treturn dbi;\n}\n\nstatic void tcmu_setup_iovs(struct tcmu_dev *udev, struct tcmu_cmd *cmd,\n\t\t\t    struct iovec **iov, int data_length)\n{\n\t \n\tint dbi = -2;\n\n\t \n\tfor (; data_length > 0; data_length -= udev->data_blk_size)\n\t\tdbi = new_block_to_iov(udev, cmd, iov, dbi, data_length);\n}\n\nstatic struct tcmu_cmd *tcmu_alloc_cmd(struct se_cmd *se_cmd)\n{\n\tstruct se_device *se_dev = se_cmd->se_dev;\n\tstruct tcmu_dev *udev = TCMU_DEV(se_dev);\n\tstruct tcmu_cmd *tcmu_cmd;\n\n\ttcmu_cmd = kmem_cache_zalloc(tcmu_cmd_cache, GFP_NOIO);\n\tif (!tcmu_cmd)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&tcmu_cmd->queue_entry);\n\ttcmu_cmd->se_cmd = se_cmd;\n\ttcmu_cmd->tcmu_dev = udev;\n\n\ttcmu_cmd_set_block_cnts(tcmu_cmd);\n\ttcmu_cmd->dbi = kcalloc(tcmu_cmd->dbi_cnt, sizeof(uint32_t),\n\t\t\t\tGFP_NOIO);\n\tif (!tcmu_cmd->dbi) {\n\t\tkmem_cache_free(tcmu_cmd_cache, tcmu_cmd);\n\t\treturn NULL;\n\t}\n\n\treturn tcmu_cmd;\n}\n\nstatic inline void tcmu_flush_dcache_range(void *vaddr, size_t size)\n{\n\tunsigned long offset = offset_in_page(vaddr);\n\tvoid *start = vaddr - offset;\n\n\tsize = round_up(size+offset, PAGE_SIZE);\n\n\twhile (size) {\n\t\tflush_dcache_page(vmalloc_to_page(start));\n\t\tstart += PAGE_SIZE;\n\t\tsize -= PAGE_SIZE;\n\t}\n}\n\n \nstatic inline size_t spc_used(size_t head, size_t tail, size_t size)\n{\n\tint diff = head - tail;\n\n\tif (diff >= 0)\n\t\treturn diff;\n\telse\n\t\treturn size + diff;\n}\n\nstatic inline size_t spc_free(size_t head, size_t tail, size_t size)\n{\n\t \n\treturn (size - spc_used(head, tail, size) - 1);\n}\n\nstatic inline size_t head_to_end(size_t head, size_t size)\n{\n\treturn size - head;\n}\n\n#define UPDATE_HEAD(head, used, size) smp_store_release(&head, ((head % size) + used) % size)\n\n#define TCMU_SG_TO_DATA_AREA 1\n#define TCMU_DATA_AREA_TO_SG 2\n\nstatic inline void tcmu_copy_data(struct tcmu_dev *udev,\n\t\t\t\t  struct tcmu_cmd *tcmu_cmd, uint32_t direction,\n\t\t\t\t  struct scatterlist *sg, unsigned int sg_nents,\n\t\t\t\t  struct iovec **iov, size_t data_len)\n{\n\t \n\tint dbi = -2;\n\tsize_t page_remaining, cp_len;\n\tint page_cnt, page_inx, dpi;\n\tstruct sg_mapping_iter sg_iter;\n\tunsigned int sg_flags;\n\tstruct page *page;\n\tvoid *data_page_start, *data_addr;\n\n\tif (direction == TCMU_SG_TO_DATA_AREA)\n\t\tsg_flags = SG_MITER_ATOMIC | SG_MITER_FROM_SG;\n\telse\n\t\tsg_flags = SG_MITER_ATOMIC | SG_MITER_TO_SG;\n\tsg_miter_start(&sg_iter, sg, sg_nents, sg_flags);\n\n\twhile (data_len) {\n\t\tif (direction == TCMU_SG_TO_DATA_AREA)\n\t\t\tdbi = new_block_to_iov(udev, tcmu_cmd, iov, dbi,\n\t\t\t\t\t       data_len);\n\t\telse\n\t\t\tdbi = tcmu_cmd_get_dbi(tcmu_cmd);\n\n\t\tpage_cnt = DIV_ROUND_UP(data_len, PAGE_SIZE);\n\t\tif (page_cnt > udev->data_pages_per_blk)\n\t\t\tpage_cnt = udev->data_pages_per_blk;\n\n\t\tdpi = dbi * udev->data_pages_per_blk;\n\t\tfor (page_inx = 0; page_inx < page_cnt && data_len;\n\t\t     page_inx++, dpi++) {\n\t\t\tpage = xa_load(&udev->data_pages, dpi);\n\n\t\t\tif (direction == TCMU_DATA_AREA_TO_SG)\n\t\t\t\tflush_dcache_page(page);\n\t\t\tdata_page_start = kmap_atomic(page);\n\t\t\tpage_remaining = PAGE_SIZE;\n\n\t\t\twhile (page_remaining && data_len) {\n\t\t\t\tif (!sg_miter_next(&sg_iter)) {\n\t\t\t\t\t \n\t\t\t\t\tdata_len = 0;\n\t\t\t\t\tpr_debug(\"%s: aborting data copy due to exhausted sg_list\\n\",\n\t\t\t\t\t\t __func__);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tcp_len = min3(sg_iter.length, page_remaining,\n\t\t\t\t\t      data_len);\n\n\t\t\t\tdata_addr = data_page_start +\n\t\t\t\t\t    PAGE_SIZE - page_remaining;\n\t\t\t\tif (direction == TCMU_SG_TO_DATA_AREA)\n\t\t\t\t\tmemcpy(data_addr, sg_iter.addr, cp_len);\n\t\t\t\telse\n\t\t\t\t\tmemcpy(sg_iter.addr, data_addr, cp_len);\n\n\t\t\t\tdata_len -= cp_len;\n\t\t\t\tpage_remaining -= cp_len;\n\t\t\t\tsg_iter.consumed = cp_len;\n\t\t\t}\n\t\t\tsg_miter_stop(&sg_iter);\n\n\t\t\tkunmap_atomic(data_page_start);\n\t\t\tif (direction == TCMU_SG_TO_DATA_AREA)\n\t\t\t\tflush_dcache_page(page);\n\t\t}\n\t}\n}\n\nstatic void scatter_data_area(struct tcmu_dev *udev, struct tcmu_cmd *tcmu_cmd,\n\t\t\t      struct iovec **iov)\n{\n\tstruct se_cmd *se_cmd = tcmu_cmd->se_cmd;\n\n\ttcmu_copy_data(udev, tcmu_cmd, TCMU_SG_TO_DATA_AREA, se_cmd->t_data_sg,\n\t\t       se_cmd->t_data_nents, iov, se_cmd->data_length);\n}\n\nstatic void gather_data_area(struct tcmu_dev *udev, struct tcmu_cmd *tcmu_cmd,\n\t\t\t     bool bidi, uint32_t read_len)\n{\n\tstruct se_cmd *se_cmd = tcmu_cmd->se_cmd;\n\tstruct scatterlist *data_sg;\n\tunsigned int data_nents;\n\n\tif (!bidi) {\n\t\tdata_sg = se_cmd->t_data_sg;\n\t\tdata_nents = se_cmd->t_data_nents;\n\t} else {\n\t\t \n\t\ttcmu_cmd_set_dbi_cur(tcmu_cmd,\n\t\t\t\t     tcmu_cmd->dbi_cnt - tcmu_cmd->dbi_bidi_cnt);\n\n\t\tdata_sg = se_cmd->t_bidi_data_sg;\n\t\tdata_nents = se_cmd->t_bidi_data_nents;\n\t}\n\n\ttcmu_copy_data(udev, tcmu_cmd, TCMU_DATA_AREA_TO_SG, data_sg,\n\t\t       data_nents, NULL, read_len);\n}\n\nstatic inline size_t spc_bitmap_free(unsigned long *bitmap, uint32_t thresh)\n{\n\treturn thresh - bitmap_weight(bitmap, thresh);\n}\n\n \nstatic bool is_ring_space_avail(struct tcmu_dev *udev, size_t cmd_size)\n{\n\tstruct tcmu_mailbox *mb = udev->mb_addr;\n\tsize_t space, cmd_needed;\n\tu32 cmd_head;\n\n\ttcmu_flush_dcache_range(mb, sizeof(*mb));\n\n\tcmd_head = mb->cmd_head % udev->cmdr_size;  \n\n\t \n\tif (head_to_end(cmd_head, udev->cmdr_size) >= cmd_size)\n\t\tcmd_needed = cmd_size;\n\telse\n\t\tcmd_needed = cmd_size + head_to_end(cmd_head, udev->cmdr_size);\n\n\tspace = spc_free(cmd_head, udev->cmdr_last_cleaned, udev->cmdr_size);\n\tif (space < cmd_needed) {\n\t\tpr_debug(\"no cmd space: %u %u %u\\n\", cmd_head,\n\t\t       udev->cmdr_last_cleaned, udev->cmdr_size);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nstatic int tcmu_alloc_data_space(struct tcmu_dev *udev, struct tcmu_cmd *cmd,\n\t\t\t\t  int *iov_bidi_cnt)\n{\n\tint space, iov_cnt = 0, ret = 0;\n\n\tif (!cmd->dbi_cnt)\n\t\tgoto wr_iov_cnts;\n\n\t \n\tspace = spc_bitmap_free(udev->data_bitmap, udev->dbi_thresh);\n\tif (space < cmd->dbi_cnt) {\n\t\tunsigned long blocks_left =\n\t\t\t\t(udev->max_blocks - udev->dbi_thresh) + space;\n\n\t\tif (blocks_left < cmd->dbi_cnt) {\n\t\t\tpr_debug(\"no data space: only %lu available, but ask for %u\\n\",\n\t\t\t\t\tblocks_left * udev->data_blk_size,\n\t\t\t\t\tcmd->dbi_cnt * udev->data_blk_size);\n\t\t\treturn -1;\n\t\t}\n\n\t\tudev->dbi_thresh += cmd->dbi_cnt;\n\t\tif (udev->dbi_thresh > udev->max_blocks)\n\t\t\tudev->dbi_thresh = udev->max_blocks;\n\t}\n\n\tiov_cnt = tcmu_get_empty_blocks(udev, cmd, cmd->se_cmd->data_length);\n\tif (iov_cnt < 0)\n\t\treturn -1;\n\n\tif (cmd->dbi_bidi_cnt) {\n\t\tret = tcmu_get_empty_blocks(udev, cmd, cmd->data_len_bidi);\n\t\tif (ret < 0)\n\t\t\treturn -1;\n\t}\nwr_iov_cnts:\n\t*iov_bidi_cnt = ret;\n\treturn iov_cnt + ret;\n}\n\nstatic inline size_t tcmu_cmd_get_base_cmd_size(size_t iov_cnt)\n{\n\treturn max(offsetof(struct tcmu_cmd_entry, req.iov[iov_cnt]),\n\t\t\tsizeof(struct tcmu_cmd_entry));\n}\n\nstatic inline size_t tcmu_cmd_get_cmd_size(struct tcmu_cmd *tcmu_cmd,\n\t\t\t\t\t   size_t base_command_size)\n{\n\tstruct se_cmd *se_cmd = tcmu_cmd->se_cmd;\n\tsize_t command_size;\n\n\tcommand_size = base_command_size +\n\t\tround_up(scsi_command_size(se_cmd->t_task_cdb),\n\t\t\t\tTCMU_OP_ALIGN_SIZE);\n\n\tWARN_ON(command_size & (TCMU_OP_ALIGN_SIZE-1));\n\n\treturn command_size;\n}\n\nstatic void tcmu_setup_cmd_timer(struct tcmu_cmd *tcmu_cmd, unsigned int tmo,\n\t\t\t\t struct timer_list *timer)\n{\n\tif (!tmo)\n\t\treturn;\n\n\ttcmu_cmd->deadline = round_jiffies_up(jiffies + msecs_to_jiffies(tmo));\n\tif (!timer_pending(timer))\n\t\tmod_timer(timer, tcmu_cmd->deadline);\n\n\tpr_debug(\"Timeout set up for cmd %p, dev = %s, tmo = %lu\\n\", tcmu_cmd,\n\t\t tcmu_cmd->tcmu_dev->name, tmo / MSEC_PER_SEC);\n}\n\nstatic int add_to_qfull_queue(struct tcmu_cmd *tcmu_cmd)\n{\n\tstruct tcmu_dev *udev = tcmu_cmd->tcmu_dev;\n\tunsigned int tmo;\n\n\t \n\tif (!udev->qfull_time_out)\n\t\treturn -ETIMEDOUT;\n\telse if (udev->qfull_time_out > 0)\n\t\ttmo = udev->qfull_time_out;\n\telse if (udev->cmd_time_out)\n\t\ttmo = udev->cmd_time_out;\n\telse\n\t\ttmo = TCMU_TIME_OUT;\n\n\ttcmu_setup_cmd_timer(tcmu_cmd, tmo, &udev->qfull_timer);\n\n\tlist_add_tail(&tcmu_cmd->queue_entry, &udev->qfull_queue);\n\tpr_debug(\"adding cmd %p on dev %s to ring space wait queue\\n\",\n\t\t tcmu_cmd, udev->name);\n\treturn 0;\n}\n\nstatic uint32_t ring_insert_padding(struct tcmu_dev *udev, size_t cmd_size)\n{\n\tstruct tcmu_cmd_entry_hdr *hdr;\n\tstruct tcmu_mailbox *mb = udev->mb_addr;\n\tuint32_t cmd_head = mb->cmd_head % udev->cmdr_size;  \n\n\t \n\tif (head_to_end(cmd_head, udev->cmdr_size) < cmd_size) {\n\t\tsize_t pad_size = head_to_end(cmd_head, udev->cmdr_size);\n\n\t\thdr = udev->cmdr + cmd_head;\n\t\ttcmu_hdr_set_op(&hdr->len_op, TCMU_OP_PAD);\n\t\ttcmu_hdr_set_len(&hdr->len_op, pad_size);\n\t\thdr->cmd_id = 0;  \n\t\thdr->kflags = 0;\n\t\thdr->uflags = 0;\n\t\ttcmu_flush_dcache_range(hdr, sizeof(*hdr));\n\n\t\tUPDATE_HEAD(mb->cmd_head, pad_size, udev->cmdr_size);\n\t\ttcmu_flush_dcache_range(mb, sizeof(*mb));\n\n\t\tcmd_head = mb->cmd_head % udev->cmdr_size;  \n\t\tWARN_ON(cmd_head != 0);\n\t}\n\n\treturn cmd_head;\n}\n\nstatic void tcmu_unplug_device(struct se_dev_plug *se_plug)\n{\n\tstruct se_device *se_dev = se_plug->se_dev;\n\tstruct tcmu_dev *udev = TCMU_DEV(se_dev);\n\n\tclear_bit(TCMU_DEV_BIT_PLUGGED, &udev->flags);\n\tuio_event_notify(&udev->uio_info);\n}\n\nstatic struct se_dev_plug *tcmu_plug_device(struct se_device *se_dev)\n{\n\tstruct tcmu_dev *udev = TCMU_DEV(se_dev);\n\n\tif (!test_and_set_bit(TCMU_DEV_BIT_PLUGGED, &udev->flags))\n\t\treturn &udev->se_plug;\n\n\treturn NULL;\n}\n\n \nstatic int queue_cmd_ring(struct tcmu_cmd *tcmu_cmd, sense_reason_t *scsi_err)\n{\n\tstruct tcmu_dev *udev = tcmu_cmd->tcmu_dev;\n\tstruct se_cmd *se_cmd = tcmu_cmd->se_cmd;\n\tsize_t base_command_size, command_size;\n\tstruct tcmu_mailbox *mb = udev->mb_addr;\n\tstruct tcmu_cmd_entry *entry;\n\tstruct iovec *iov;\n\tint iov_cnt, iov_bidi_cnt;\n\tuint32_t cmd_id, cmd_head;\n\tuint64_t cdb_off;\n\tuint32_t blk_size = udev->data_blk_size;\n\t \n\tsize_t data_length = (size_t)tcmu_cmd->dbi_cnt * blk_size;\n\n\t*scsi_err = TCM_NO_SENSE;\n\n\tif (test_bit(TCMU_DEV_BIT_BLOCKED, &udev->flags)) {\n\t\t*scsi_err = TCM_LUN_BUSY;\n\t\treturn -1;\n\t}\n\n\tif (test_bit(TCMU_DEV_BIT_BROKEN, &udev->flags)) {\n\t\t*scsi_err = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\t\treturn -1;\n\t}\n\n\tif (!list_empty(&udev->qfull_queue))\n\t\tgoto queue;\n\n\tif (data_length > (size_t)udev->max_blocks * blk_size) {\n\t\tpr_warn(\"TCMU: Request of size %zu is too big for %zu data area\\n\",\n\t\t\tdata_length, (size_t)udev->max_blocks * blk_size);\n\t\t*scsi_err = TCM_INVALID_CDB_FIELD;\n\t\treturn -1;\n\t}\n\n\tiov_cnt = tcmu_alloc_data_space(udev, tcmu_cmd, &iov_bidi_cnt);\n\tif (iov_cnt < 0)\n\t\tgoto free_and_queue;\n\n\t \n\tbase_command_size = tcmu_cmd_get_base_cmd_size(iov_cnt);\n\tcommand_size = tcmu_cmd_get_cmd_size(tcmu_cmd, base_command_size);\n\n\tif (command_size > (udev->cmdr_size / 2)) {\n\t\tpr_warn(\"TCMU: Request of size %zu is too big for %u cmd ring\\n\",\n\t\t\tcommand_size, udev->cmdr_size);\n\t\ttcmu_cmd_free_data(tcmu_cmd, tcmu_cmd->dbi_cur);\n\t\t*scsi_err = TCM_INVALID_CDB_FIELD;\n\t\treturn -1;\n\t}\n\n\tif (!is_ring_space_avail(udev, command_size))\n\t\t \n\t\tgoto free_and_queue;\n\n\tif (xa_alloc(&udev->commands, &cmd_id, tcmu_cmd, XA_LIMIT(1, 0xffff),\n\t\t     GFP_NOWAIT) < 0) {\n\t\tpr_err(\"tcmu: Could not allocate cmd id.\\n\");\n\n\t\ttcmu_cmd_free_data(tcmu_cmd, tcmu_cmd->dbi_cnt);\n\t\t*scsi_err = TCM_OUT_OF_RESOURCES;\n\t\treturn -1;\n\t}\n\ttcmu_cmd->cmd_id = cmd_id;\n\n\tpr_debug(\"allocated cmd id %u for cmd %p dev %s\\n\", tcmu_cmd->cmd_id,\n\t\t tcmu_cmd, udev->name);\n\n\tcmd_head = ring_insert_padding(udev, command_size);\n\n\tentry = udev->cmdr + cmd_head;\n\tmemset(entry, 0, command_size);\n\ttcmu_hdr_set_op(&entry->hdr.len_op, TCMU_OP_CMD);\n\n\t \n\ttcmu_cmd_reset_dbi_cur(tcmu_cmd);\n\tiov = &entry->req.iov[0];\n\n\tif (se_cmd->data_direction == DMA_TO_DEVICE ||\n\t    se_cmd->se_cmd_flags & SCF_BIDI)\n\t\tscatter_data_area(udev, tcmu_cmd, &iov);\n\telse\n\t\ttcmu_setup_iovs(udev, tcmu_cmd, &iov, se_cmd->data_length);\n\n\tentry->req.iov_cnt = iov_cnt - iov_bidi_cnt;\n\n\t \n\tif (se_cmd->se_cmd_flags & SCF_BIDI) {\n\t\tiov++;\n\t\ttcmu_setup_iovs(udev, tcmu_cmd, &iov, tcmu_cmd->data_len_bidi);\n\t\tentry->req.iov_bidi_cnt = iov_bidi_cnt;\n\t}\n\n\ttcmu_setup_cmd_timer(tcmu_cmd, udev->cmd_time_out, &udev->cmd_timer);\n\n\tentry->hdr.cmd_id = tcmu_cmd->cmd_id;\n\n\ttcmu_hdr_set_len(&entry->hdr.len_op, command_size);\n\n\t \n\tcdb_off = CMDR_OFF + cmd_head + base_command_size;\n\tmemcpy((void *) mb + cdb_off, se_cmd->t_task_cdb, scsi_command_size(se_cmd->t_task_cdb));\n\tentry->req.cdb_off = cdb_off;\n\ttcmu_flush_dcache_range(entry, command_size);\n\n\tUPDATE_HEAD(mb->cmd_head, command_size, udev->cmdr_size);\n\ttcmu_flush_dcache_range(mb, sizeof(*mb));\n\n\tlist_add_tail(&tcmu_cmd->queue_entry, &udev->inflight_queue);\n\n\tif (!test_bit(TCMU_DEV_BIT_PLUGGED, &udev->flags))\n\t\tuio_event_notify(&udev->uio_info);\n\n\treturn 0;\n\nfree_and_queue:\n\ttcmu_cmd_free_data(tcmu_cmd, tcmu_cmd->dbi_cur);\n\ttcmu_cmd_reset_dbi_cur(tcmu_cmd);\n\nqueue:\n\tif (add_to_qfull_queue(tcmu_cmd)) {\n\t\t*scsi_err = TCM_OUT_OF_RESOURCES;\n\t\treturn -1;\n\t}\n\n\treturn 1;\n}\n\n \nstatic int\nqueue_tmr_ring(struct tcmu_dev *udev, struct tcmu_tmr *tmr)\n{\n\tstruct tcmu_tmr_entry *entry;\n\tint cmd_size;\n\tint id_list_sz;\n\tstruct tcmu_mailbox *mb = udev->mb_addr;\n\tuint32_t cmd_head;\n\n\tif (test_bit(TCMU_DEV_BIT_BROKEN, &udev->flags))\n\t\tgoto out_free;\n\n\tid_list_sz = sizeof(tmr->tmr_cmd_ids[0]) * tmr->tmr_cmd_cnt;\n\tcmd_size = round_up(sizeof(*entry) + id_list_sz, TCMU_OP_ALIGN_SIZE);\n\n\tif (!list_empty(&udev->tmr_queue) ||\n\t    !is_ring_space_avail(udev, cmd_size)) {\n\t\tlist_add_tail(&tmr->queue_entry, &udev->tmr_queue);\n\t\tpr_debug(\"adding tmr %p on dev %s to TMR ring space wait queue\\n\",\n\t\t\t tmr, udev->name);\n\t\treturn 1;\n\t}\n\n\tcmd_head = ring_insert_padding(udev, cmd_size);\n\n\tentry = udev->cmdr + cmd_head;\n\tmemset(entry, 0, cmd_size);\n\ttcmu_hdr_set_op(&entry->hdr.len_op, TCMU_OP_TMR);\n\ttcmu_hdr_set_len(&entry->hdr.len_op, cmd_size);\n\tentry->tmr_type = tmr->tmr_type;\n\tentry->cmd_cnt = tmr->tmr_cmd_cnt;\n\tmemcpy(&entry->cmd_ids[0], &tmr->tmr_cmd_ids[0], id_list_sz);\n\ttcmu_flush_dcache_range(entry, cmd_size);\n\n\tUPDATE_HEAD(mb->cmd_head, cmd_size, udev->cmdr_size);\n\ttcmu_flush_dcache_range(mb, sizeof(*mb));\n\n\tuio_event_notify(&udev->uio_info);\n\nout_free:\n\tkfree(tmr);\n\n\treturn 0;\n}\n\nstatic sense_reason_t\ntcmu_queue_cmd(struct se_cmd *se_cmd)\n{\n\tstruct se_device *se_dev = se_cmd->se_dev;\n\tstruct tcmu_dev *udev = TCMU_DEV(se_dev);\n\tstruct tcmu_cmd *tcmu_cmd;\n\tsense_reason_t scsi_ret = TCM_CHECK_CONDITION_ABORT_CMD;\n\tint ret = -1;\n\n\ttcmu_cmd = tcmu_alloc_cmd(se_cmd);\n\tif (!tcmu_cmd)\n\t\treturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\n\n\tmutex_lock(&udev->cmdr_lock);\n\tif (!(se_cmd->transport_state & CMD_T_ABORTED))\n\t\tret = queue_cmd_ring(tcmu_cmd, &scsi_ret);\n\tif (ret < 0)\n\t\ttcmu_free_cmd(tcmu_cmd);\n\telse\n\t\tse_cmd->priv = tcmu_cmd;\n\tmutex_unlock(&udev->cmdr_lock);\n\treturn scsi_ret;\n}\n\nstatic void tcmu_set_next_deadline(struct list_head *queue,\n\t\t\t\t   struct timer_list *timer)\n{\n\tstruct tcmu_cmd *cmd;\n\n\tif (!list_empty(queue)) {\n\t\tcmd = list_first_entry(queue, struct tcmu_cmd, queue_entry);\n\t\tmod_timer(timer, cmd->deadline);\n\t} else\n\t\tdel_timer(timer);\n}\n\nstatic int\ntcmu_tmr_type(enum tcm_tmreq_table tmf)\n{\n\tswitch (tmf) {\n\tcase TMR_ABORT_TASK:\t\treturn TCMU_TMR_ABORT_TASK;\n\tcase TMR_ABORT_TASK_SET:\treturn TCMU_TMR_ABORT_TASK_SET;\n\tcase TMR_CLEAR_ACA:\t\treturn TCMU_TMR_CLEAR_ACA;\n\tcase TMR_CLEAR_TASK_SET:\treturn TCMU_TMR_CLEAR_TASK_SET;\n\tcase TMR_LUN_RESET:\t\treturn TCMU_TMR_LUN_RESET;\n\tcase TMR_TARGET_WARM_RESET:\treturn TCMU_TMR_TARGET_WARM_RESET;\n\tcase TMR_TARGET_COLD_RESET:\treturn TCMU_TMR_TARGET_COLD_RESET;\n\tcase TMR_LUN_RESET_PRO:\t\treturn TCMU_TMR_LUN_RESET_PRO;\n\tdefault:\t\t\treturn TCMU_TMR_UNKNOWN;\n\t}\n}\n\nstatic void\ntcmu_tmr_notify(struct se_device *se_dev, enum tcm_tmreq_table tmf,\n\t\tstruct list_head *cmd_list)\n{\n\tint i = 0, cmd_cnt = 0;\n\tbool unqueued = false;\n\tstruct tcmu_cmd *cmd;\n\tstruct se_cmd *se_cmd;\n\tstruct tcmu_tmr *tmr;\n\tstruct tcmu_dev *udev = TCMU_DEV(se_dev);\n\n\tmutex_lock(&udev->cmdr_lock);\n\n\t \n\tlist_for_each_entry(se_cmd, cmd_list, state_list) {\n\t\ti++;\n\t\tif (!se_cmd->priv)\n\t\t\tcontinue;\n\t\tcmd = se_cmd->priv;\n\t\t \n\t\tif (cmd->cmd_id) {\n\t\t\tcmd_cnt++;\n\t\t\tcontinue;\n\t\t}\n\t\tpr_debug(\"Removing aborted command %p from queue on dev %s.\\n\",\n\t\t\t cmd, udev->name);\n\n\t\tlist_del_init(&cmd->queue_entry);\n\t\ttcmu_free_cmd(cmd);\n\t\tse_cmd->priv = NULL;\n\t\ttarget_complete_cmd(se_cmd, SAM_STAT_TASK_ABORTED);\n\t\tunqueued = true;\n\t}\n\tif (unqueued)\n\t\ttcmu_set_next_deadline(&udev->qfull_queue, &udev->qfull_timer);\n\n\tif (!test_bit(TCMU_DEV_BIT_TMR_NOTIFY, &udev->flags))\n\t\tgoto unlock;\n\n\tpr_debug(\"TMR event %d on dev %s, aborted cmds %d, afflicted cmd_ids %d\\n\",\n\t\t tcmu_tmr_type(tmf), udev->name, i, cmd_cnt);\n\n\ttmr = kmalloc(struct_size(tmr, tmr_cmd_ids, cmd_cnt), GFP_NOIO);\n\tif (!tmr)\n\t\tgoto unlock;\n\n\ttmr->tmr_type = tcmu_tmr_type(tmf);\n\ttmr->tmr_cmd_cnt = cmd_cnt;\n\n\tif (cmd_cnt != 0) {\n\t\tcmd_cnt = 0;\n\t\tlist_for_each_entry(se_cmd, cmd_list, state_list) {\n\t\t\tif (!se_cmd->priv)\n\t\t\t\tcontinue;\n\t\t\tcmd = se_cmd->priv;\n\t\t\tif (cmd->cmd_id)\n\t\t\t\ttmr->tmr_cmd_ids[cmd_cnt++] = cmd->cmd_id;\n\t\t}\n\t}\n\n\tqueue_tmr_ring(udev, tmr);\n\nunlock:\n\tmutex_unlock(&udev->cmdr_lock);\n}\n\nstatic bool tcmu_handle_completion(struct tcmu_cmd *cmd,\n\t\t\t\t   struct tcmu_cmd_entry *entry, bool keep_buf)\n{\n\tstruct se_cmd *se_cmd = cmd->se_cmd;\n\tstruct tcmu_dev *udev = cmd->tcmu_dev;\n\tbool read_len_valid = false;\n\tbool ret = true;\n\tuint32_t read_len;\n\n\t \n\tif (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags)) {\n\t\tWARN_ON_ONCE(se_cmd);\n\t\tgoto out;\n\t}\n\tif (test_bit(TCMU_CMD_BIT_KEEP_BUF, &cmd->flags)) {\n\t\tpr_err(\"cmd_id %u already completed with KEEP_BUF, ring is broken\\n\",\n\t\t       entry->hdr.cmd_id);\n\t\tset_bit(TCMU_DEV_BIT_BROKEN, &udev->flags);\n\t\tret = false;\n\t\tgoto out;\n\t}\n\n\tlist_del_init(&cmd->queue_entry);\n\n\ttcmu_cmd_reset_dbi_cur(cmd);\n\n\tif (entry->hdr.uflags & TCMU_UFLAG_UNKNOWN_OP) {\n\t\tpr_warn(\"TCMU: Userspace set UNKNOWN_OP flag on se_cmd %p\\n\",\n\t\t\tcmd->se_cmd);\n\t\tentry->rsp.scsi_status = SAM_STAT_CHECK_CONDITION;\n\t\tgoto done;\n\t}\n\n\tread_len = se_cmd->data_length;\n\tif (se_cmd->data_direction == DMA_FROM_DEVICE &&\n\t    (entry->hdr.uflags & TCMU_UFLAG_READ_LEN) && entry->rsp.read_len) {\n\t\tread_len_valid = true;\n\t\tif (entry->rsp.read_len < read_len)\n\t\t\tread_len = entry->rsp.read_len;\n\t}\n\n\tif (entry->rsp.scsi_status == SAM_STAT_CHECK_CONDITION) {\n\t\ttransport_copy_sense_to_cmd(se_cmd, entry->rsp.sense_buffer);\n\t\tif (!read_len_valid )\n\t\t\tgoto done;\n\t\telse\n\t\t\tse_cmd->se_cmd_flags |= SCF_TREAT_READ_AS_NORMAL;\n\t}\n\tif (se_cmd->se_cmd_flags & SCF_BIDI) {\n\t\t \n\t\tgather_data_area(udev, cmd, true, read_len);\n\t} else if (se_cmd->data_direction == DMA_FROM_DEVICE) {\n\t\tgather_data_area(udev, cmd, false, read_len);\n\t} else if (se_cmd->data_direction == DMA_TO_DEVICE) {\n\t\t \n\t} else if (se_cmd->data_direction != DMA_NONE) {\n\t\tpr_warn(\"TCMU: data direction was %d!\\n\",\n\t\t\tse_cmd->data_direction);\n\t}\n\ndone:\n\tse_cmd->priv = NULL;\n\tif (read_len_valid) {\n\t\tpr_debug(\"read_len = %d\\n\", read_len);\n\t\ttarget_complete_cmd_with_length(cmd->se_cmd,\n\t\t\t\t\tentry->rsp.scsi_status, read_len);\n\t} else\n\t\ttarget_complete_cmd(cmd->se_cmd, entry->rsp.scsi_status);\n\nout:\n\tif (!keep_buf) {\n\t\ttcmu_cmd_free_data(cmd, cmd->dbi_cnt);\n\t\ttcmu_free_cmd(cmd);\n\t} else {\n\t\t \n\t\tclear_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags);\n\t\tset_bit(TCMU_CMD_BIT_KEEP_BUF, &cmd->flags);\n\t}\n\treturn ret;\n}\n\nstatic int tcmu_run_tmr_queue(struct tcmu_dev *udev)\n{\n\tstruct tcmu_tmr *tmr, *tmp;\n\tLIST_HEAD(tmrs);\n\n\tif (list_empty(&udev->tmr_queue))\n\t\treturn 1;\n\n\tpr_debug(\"running %s's tmr queue\\n\", udev->name);\n\n\tlist_splice_init(&udev->tmr_queue, &tmrs);\n\n\tlist_for_each_entry_safe(tmr, tmp, &tmrs, queue_entry) {\n\t\tlist_del_init(&tmr->queue_entry);\n\n\t\tpr_debug(\"removing tmr %p on dev %s from queue\\n\",\n\t\t\t tmr, udev->name);\n\n\t\tif (queue_tmr_ring(udev, tmr)) {\n\t\t\tpr_debug(\"ran out of space during tmr queue run\\n\");\n\t\t\t \n\t\t\tlist_splice_tail(&tmrs, &udev->tmr_queue);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\nstatic bool tcmu_handle_completions(struct tcmu_dev *udev)\n{\n\tstruct tcmu_mailbox *mb;\n\tstruct tcmu_cmd *cmd;\n\tbool free_space = false;\n\n\tif (test_bit(TCMU_DEV_BIT_BROKEN, &udev->flags)) {\n\t\tpr_err(\"ring broken, not handling completions\\n\");\n\t\treturn false;\n\t}\n\n\tmb = udev->mb_addr;\n\ttcmu_flush_dcache_range(mb, sizeof(*mb));\n\n\twhile (udev->cmdr_last_cleaned != READ_ONCE(mb->cmd_tail)) {\n\n\t\tstruct tcmu_cmd_entry *entry = udev->cmdr + udev->cmdr_last_cleaned;\n\t\tbool keep_buf;\n\n\t\t \n\t\tsize_t ring_left = head_to_end(udev->cmdr_last_cleaned,\n\t\t\t\t\t       udev->cmdr_size);\n\t\ttcmu_flush_dcache_range(entry, ring_left < sizeof(*entry) ?\n\t\t\t\t\tring_left : sizeof(*entry));\n\n\t\tfree_space = true;\n\n\t\tif (tcmu_hdr_get_op(entry->hdr.len_op) == TCMU_OP_PAD ||\n\t\t    tcmu_hdr_get_op(entry->hdr.len_op) == TCMU_OP_TMR) {\n\t\t\tUPDATE_HEAD(udev->cmdr_last_cleaned,\n\t\t\t\t    tcmu_hdr_get_len(entry->hdr.len_op),\n\t\t\t\t    udev->cmdr_size);\n\t\t\tcontinue;\n\t\t}\n\t\tWARN_ON(tcmu_hdr_get_op(entry->hdr.len_op) != TCMU_OP_CMD);\n\n\t\tkeep_buf = !!(entry->hdr.uflags & TCMU_UFLAG_KEEP_BUF);\n\t\tif (keep_buf)\n\t\t\tcmd = xa_load(&udev->commands, entry->hdr.cmd_id);\n\t\telse\n\t\t\tcmd = xa_erase(&udev->commands, entry->hdr.cmd_id);\n\t\tif (!cmd) {\n\t\t\tpr_err(\"cmd_id %u not found, ring is broken\\n\",\n\t\t\t       entry->hdr.cmd_id);\n\t\t\tset_bit(TCMU_DEV_BIT_BROKEN, &udev->flags);\n\t\t\treturn false;\n\t\t}\n\n\t\tif (!tcmu_handle_completion(cmd, entry, keep_buf))\n\t\t\tbreak;\n\n\t\tUPDATE_HEAD(udev->cmdr_last_cleaned,\n\t\t\t    tcmu_hdr_get_len(entry->hdr.len_op),\n\t\t\t    udev->cmdr_size);\n\t}\n\tif (free_space)\n\t\tfree_space = tcmu_run_tmr_queue(udev);\n\n\tif (atomic_read(&global_page_count) > tcmu_global_max_pages &&\n\t    xa_empty(&udev->commands) && list_empty(&udev->qfull_queue)) {\n\t\t \n\t\tschedule_delayed_work(&tcmu_unmap_work, 0);\n\t}\n\tif (udev->cmd_time_out)\n\t\ttcmu_set_next_deadline(&udev->inflight_queue, &udev->cmd_timer);\n\n\treturn free_space;\n}\n\nstatic void tcmu_check_expired_ring_cmd(struct tcmu_cmd *cmd)\n{\n\tstruct se_cmd *se_cmd;\n\n\tif (!time_after_eq(jiffies, cmd->deadline))\n\t\treturn;\n\n\tset_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags);\n\tlist_del_init(&cmd->queue_entry);\n\tse_cmd = cmd->se_cmd;\n\tse_cmd->priv = NULL;\n\tcmd->se_cmd = NULL;\n\n\tpr_debug(\"Timing out inflight cmd %u on dev %s.\\n\",\n\t\t cmd->cmd_id, cmd->tcmu_dev->name);\n\n\ttarget_complete_cmd(se_cmd, SAM_STAT_CHECK_CONDITION);\n}\n\nstatic void tcmu_check_expired_queue_cmd(struct tcmu_cmd *cmd)\n{\n\tstruct se_cmd *se_cmd;\n\n\tif (!time_after_eq(jiffies, cmd->deadline))\n\t\treturn;\n\n\tpr_debug(\"Timing out queued cmd %p on dev %s.\\n\",\n\t\t  cmd, cmd->tcmu_dev->name);\n\n\tlist_del_init(&cmd->queue_entry);\n\tse_cmd = cmd->se_cmd;\n\ttcmu_free_cmd(cmd);\n\n\tse_cmd->priv = NULL;\n\ttarget_complete_cmd(se_cmd, SAM_STAT_TASK_SET_FULL);\n}\n\nstatic void tcmu_device_timedout(struct tcmu_dev *udev)\n{\n\tspin_lock(&timed_out_udevs_lock);\n\tif (list_empty(&udev->timedout_entry))\n\t\tlist_add_tail(&udev->timedout_entry, &timed_out_udevs);\n\tspin_unlock(&timed_out_udevs_lock);\n\n\tschedule_delayed_work(&tcmu_unmap_work, 0);\n}\n\nstatic void tcmu_cmd_timedout(struct timer_list *t)\n{\n\tstruct tcmu_dev *udev = from_timer(udev, t, cmd_timer);\n\n\tpr_debug(\"%s cmd timeout has expired\\n\", udev->name);\n\ttcmu_device_timedout(udev);\n}\n\nstatic void tcmu_qfull_timedout(struct timer_list *t)\n{\n\tstruct tcmu_dev *udev = from_timer(udev, t, qfull_timer);\n\n\tpr_debug(\"%s qfull timeout has expired\\n\", udev->name);\n\ttcmu_device_timedout(udev);\n}\n\nstatic int tcmu_attach_hba(struct se_hba *hba, u32 host_id)\n{\n\tstruct tcmu_hba *tcmu_hba;\n\n\ttcmu_hba = kzalloc(sizeof(struct tcmu_hba), GFP_KERNEL);\n\tif (!tcmu_hba)\n\t\treturn -ENOMEM;\n\n\ttcmu_hba->host_id = host_id;\n\thba->hba_ptr = tcmu_hba;\n\n\treturn 0;\n}\n\nstatic void tcmu_detach_hba(struct se_hba *hba)\n{\n\tkfree(hba->hba_ptr);\n\thba->hba_ptr = NULL;\n}\n\nstatic struct se_device *tcmu_alloc_device(struct se_hba *hba, const char *name)\n{\n\tstruct tcmu_dev *udev;\n\n\tudev = kzalloc(sizeof(struct tcmu_dev), GFP_KERNEL);\n\tif (!udev)\n\t\treturn NULL;\n\tkref_init(&udev->kref);\n\n\tudev->name = kstrdup(name, GFP_KERNEL);\n\tif (!udev->name) {\n\t\tkfree(udev);\n\t\treturn NULL;\n\t}\n\n\tudev->hba = hba;\n\tudev->cmd_time_out = TCMU_TIME_OUT;\n\tudev->qfull_time_out = -1;\n\n\tudev->data_pages_per_blk = DATA_PAGES_PER_BLK_DEF;\n\tudev->max_blocks = DATA_AREA_PAGES_DEF / udev->data_pages_per_blk;\n\tudev->cmdr_size = CMDR_SIZE_DEF;\n\tudev->data_area_mb = TCMU_PAGES_TO_MBS(DATA_AREA_PAGES_DEF);\n\n\tmutex_init(&udev->cmdr_lock);\n\n\tINIT_LIST_HEAD(&udev->node);\n\tINIT_LIST_HEAD(&udev->timedout_entry);\n\tINIT_LIST_HEAD(&udev->qfull_queue);\n\tINIT_LIST_HEAD(&udev->tmr_queue);\n\tINIT_LIST_HEAD(&udev->inflight_queue);\n\txa_init_flags(&udev->commands, XA_FLAGS_ALLOC1);\n\n\ttimer_setup(&udev->qfull_timer, tcmu_qfull_timedout, 0);\n\ttimer_setup(&udev->cmd_timer, tcmu_cmd_timedout, 0);\n\n\txa_init(&udev->data_pages);\n\n\treturn &udev->se_dev;\n}\n\nstatic void tcmu_dev_call_rcu(struct rcu_head *p)\n{\n\tstruct se_device *dev = container_of(p, struct se_device, rcu_head);\n\tstruct tcmu_dev *udev = TCMU_DEV(dev);\n\n\tkfree(udev->uio_info.name);\n\tkfree(udev->name);\n\tkfree(udev);\n}\n\nstatic int tcmu_check_and_free_pending_cmd(struct tcmu_cmd *cmd)\n{\n\tif (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags) ||\n\t    test_bit(TCMU_CMD_BIT_KEEP_BUF, &cmd->flags)) {\n\t\tkmem_cache_free(tcmu_cmd_cache, cmd);\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic u32 tcmu_blocks_release(struct tcmu_dev *udev, unsigned long first,\n\t\t\t\tunsigned long last)\n{\n\tstruct page *page;\n\tunsigned long dpi;\n\tu32 pages_freed = 0;\n\n\tfirst = first * udev->data_pages_per_blk;\n\tlast = (last + 1) * udev->data_pages_per_blk - 1;\n\txa_for_each_range(&udev->data_pages, dpi, page, first, last) {\n\t\txa_erase(&udev->data_pages, dpi);\n\t\t \n\t\tlock_page(page);\n\t\tunlock_page(page);\n\t\t__free_page(page);\n\t\tpages_freed++;\n\t}\n\n\tatomic_sub(pages_freed, &global_page_count);\n\n\treturn pages_freed;\n}\n\nstatic void tcmu_remove_all_queued_tmr(struct tcmu_dev *udev)\n{\n\tstruct tcmu_tmr *tmr, *tmp;\n\n\tlist_for_each_entry_safe(tmr, tmp, &udev->tmr_queue, queue_entry) {\n\t\tlist_del_init(&tmr->queue_entry);\n\t\tkfree(tmr);\n\t}\n}\n\nstatic void tcmu_dev_kref_release(struct kref *kref)\n{\n\tstruct tcmu_dev *udev = container_of(kref, struct tcmu_dev, kref);\n\tstruct se_device *dev = &udev->se_dev;\n\tstruct tcmu_cmd *cmd;\n\tbool all_expired = true;\n\tunsigned long i;\n\n\tvfree(udev->mb_addr);\n\tudev->mb_addr = NULL;\n\n\tspin_lock_bh(&timed_out_udevs_lock);\n\tif (!list_empty(&udev->timedout_entry))\n\t\tlist_del(&udev->timedout_entry);\n\tspin_unlock_bh(&timed_out_udevs_lock);\n\n\t \n\tmutex_lock(&udev->cmdr_lock);\n\txa_for_each(&udev->commands, i, cmd) {\n\t\tif (tcmu_check_and_free_pending_cmd(cmd) != 0)\n\t\t\tall_expired = false;\n\t}\n\t \n\ttcmu_remove_all_queued_tmr(udev);\n\tif (!list_empty(&udev->qfull_queue))\n\t\tall_expired = false;\n\txa_destroy(&udev->commands);\n\tWARN_ON(!all_expired);\n\n\ttcmu_blocks_release(udev, 0, udev->dbi_max);\n\tbitmap_free(udev->data_bitmap);\n\tmutex_unlock(&udev->cmdr_lock);\n\n\tpr_debug(\"dev_kref_release\\n\");\n\n\tcall_rcu(&dev->rcu_head, tcmu_dev_call_rcu);\n}\n\nstatic void run_qfull_queue(struct tcmu_dev *udev, bool fail)\n{\n\tstruct tcmu_cmd *tcmu_cmd, *tmp_cmd;\n\tLIST_HEAD(cmds);\n\tsense_reason_t scsi_ret;\n\tint ret;\n\n\tif (list_empty(&udev->qfull_queue))\n\t\treturn;\n\n\tpr_debug(\"running %s's cmdr queue forcefail %d\\n\", udev->name, fail);\n\n\tlist_splice_init(&udev->qfull_queue, &cmds);\n\n\tlist_for_each_entry_safe(tcmu_cmd, tmp_cmd, &cmds, queue_entry) {\n\t\tlist_del_init(&tcmu_cmd->queue_entry);\n\n\t\tpr_debug(\"removing cmd %p on dev %s from queue\\n\",\n\t\t\t tcmu_cmd, udev->name);\n\n\t\tif (fail) {\n\t\t\t \n\t\t\ttcmu_cmd->se_cmd->priv = NULL;\n\t\t\ttarget_complete_cmd(tcmu_cmd->se_cmd, SAM_STAT_BUSY);\n\t\t\ttcmu_free_cmd(tcmu_cmd);\n\t\t\tcontinue;\n\t\t}\n\n\t\tret = queue_cmd_ring(tcmu_cmd, &scsi_ret);\n\t\tif (ret < 0) {\n\t\t\tpr_debug(\"cmd %p on dev %s failed with %u\\n\",\n\t\t\t\t tcmu_cmd, udev->name, scsi_ret);\n\t\t\t \n\t\t\ttcmu_cmd->se_cmd->priv = NULL;\n\t\t\ttarget_complete_cmd(tcmu_cmd->se_cmd,\n\t\t\t\t\t    SAM_STAT_CHECK_CONDITION);\n\t\t\ttcmu_free_cmd(tcmu_cmd);\n\t\t} else if (ret > 0) {\n\t\t\tpr_debug(\"ran out of space during cmdr queue run\\n\");\n\t\t\t \n\t\t\tlist_splice_tail(&cmds, &udev->qfull_queue);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\ttcmu_set_next_deadline(&udev->qfull_queue, &udev->qfull_timer);\n}\n\nstatic int tcmu_irqcontrol(struct uio_info *info, s32 irq_on)\n{\n\tstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\n\n\tmutex_lock(&udev->cmdr_lock);\n\tif (tcmu_handle_completions(udev))\n\t\trun_qfull_queue(udev, false);\n\tmutex_unlock(&udev->cmdr_lock);\n\n\treturn 0;\n}\n\n \nstatic int tcmu_find_mem_index(struct vm_area_struct *vma)\n{\n\tstruct tcmu_dev *udev = vma->vm_private_data;\n\tstruct uio_info *info = &udev->uio_info;\n\n\tif (vma->vm_pgoff < MAX_UIO_MAPS) {\n\t\tif (info->mem[vma->vm_pgoff].size == 0)\n\t\t\treturn -1;\n\t\treturn (int)vma->vm_pgoff;\n\t}\n\treturn -1;\n}\n\nstatic struct page *tcmu_try_get_data_page(struct tcmu_dev *udev, uint32_t dpi)\n{\n\tstruct page *page;\n\n\tmutex_lock(&udev->cmdr_lock);\n\tpage = xa_load(&udev->data_pages, dpi);\n\tif (likely(page)) {\n\t\tget_page(page);\n\t\tlock_page(page);\n\t\tmutex_unlock(&udev->cmdr_lock);\n\t\treturn page;\n\t}\n\n\t \n\tpr_err(\"Invalid addr to data page mapping (dpi %u) on device %s\\n\",\n\t       dpi, udev->name);\n\tmutex_unlock(&udev->cmdr_lock);\n\n\treturn NULL;\n}\n\nstatic void tcmu_vma_open(struct vm_area_struct *vma)\n{\n\tstruct tcmu_dev *udev = vma->vm_private_data;\n\n\tpr_debug(\"vma_open\\n\");\n\n\tkref_get(&udev->kref);\n}\n\nstatic void tcmu_vma_close(struct vm_area_struct *vma)\n{\n\tstruct tcmu_dev *udev = vma->vm_private_data;\n\n\tpr_debug(\"vma_close\\n\");\n\n\t \n\tkref_put(&udev->kref, tcmu_dev_kref_release);\n}\n\nstatic vm_fault_t tcmu_vma_fault(struct vm_fault *vmf)\n{\n\tstruct tcmu_dev *udev = vmf->vma->vm_private_data;\n\tstruct uio_info *info = &udev->uio_info;\n\tstruct page *page;\n\tunsigned long offset;\n\tvoid *addr;\n\tvm_fault_t ret = 0;\n\n\tint mi = tcmu_find_mem_index(vmf->vma);\n\tif (mi < 0)\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\toffset = (vmf->pgoff - mi) << PAGE_SHIFT;\n\n\tif (offset < udev->data_off) {\n\t\t \n\t\taddr = (void *)(unsigned long)info->mem[mi].addr + offset;\n\t\tpage = vmalloc_to_page(addr);\n\t\tget_page(page);\n\t} else {\n\t\tuint32_t dpi;\n\n\t\t \n\t\tdpi = (offset - udev->data_off) / PAGE_SIZE;\n\t\tpage = tcmu_try_get_data_page(udev, dpi);\n\t\tif (!page)\n\t\t\treturn VM_FAULT_SIGBUS;\n\t\tret = VM_FAULT_LOCKED;\n\t}\n\n\tvmf->page = page;\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct tcmu_vm_ops = {\n\t.open = tcmu_vma_open,\n\t.close = tcmu_vma_close,\n\t.fault = tcmu_vma_fault,\n};\n\nstatic int tcmu_mmap(struct uio_info *info, struct vm_area_struct *vma)\n{\n\tstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\n\n\tvm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);\n\tvma->vm_ops = &tcmu_vm_ops;\n\n\tvma->vm_private_data = udev;\n\n\t \n\tif (vma_pages(vma) != udev->mmap_pages)\n\t\treturn -EINVAL;\n\n\ttcmu_vma_open(vma);\n\n\treturn 0;\n}\n\nstatic int tcmu_open(struct uio_info *info, struct inode *inode)\n{\n\tstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\n\n\t \n\tif (test_and_set_bit(TCMU_DEV_BIT_OPEN, &udev->flags))\n\t\treturn -EBUSY;\n\n\tudev->inode = inode;\n\n\tpr_debug(\"open\\n\");\n\n\treturn 0;\n}\n\nstatic int tcmu_release(struct uio_info *info, struct inode *inode)\n{\n\tstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\n\tstruct tcmu_cmd *cmd;\n\tunsigned long i;\n\tbool freed = false;\n\n\tmutex_lock(&udev->cmdr_lock);\n\n\txa_for_each(&udev->commands, i, cmd) {\n\t\t \n\t\tif (!test_bit(TCMU_CMD_BIT_KEEP_BUF, &cmd->flags))\n\t\t\tcontinue;\n\t\tpr_debug(\"removing KEEP_BUF cmd %u on dev %s from ring\\n\",\n\t\t\t cmd->cmd_id, udev->name);\n\t\tfreed = true;\n\n\t\txa_erase(&udev->commands, i);\n\t\ttcmu_cmd_free_data(cmd, cmd->dbi_cnt);\n\t\ttcmu_free_cmd(cmd);\n\t}\n\t \n\tif (freed && list_empty(&udev->tmr_queue))\n\t\trun_qfull_queue(udev, false);\n\n\tmutex_unlock(&udev->cmdr_lock);\n\n\tclear_bit(TCMU_DEV_BIT_OPEN, &udev->flags);\n\n\tpr_debug(\"close\\n\");\n\n\treturn 0;\n}\n\nstatic int tcmu_init_genl_cmd_reply(struct tcmu_dev *udev, int cmd)\n{\n\tstruct tcmu_nl_cmd *nl_cmd = &udev->curr_nl_cmd;\n\n\tif (!tcmu_kern_cmd_reply_supported)\n\t\treturn 0;\n\n\tif (udev->nl_reply_supported <= 0)\n\t\treturn 0;\n\n\tmutex_lock(&tcmu_nl_cmd_mutex);\n\n\tif (tcmu_netlink_blocked) {\n\t\tmutex_unlock(&tcmu_nl_cmd_mutex);\n\t\tpr_warn(\"Failing nl cmd %d on %s. Interface is blocked.\\n\", cmd,\n\t\t\tudev->name);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (nl_cmd->cmd != TCMU_CMD_UNSPEC) {\n\t\tmutex_unlock(&tcmu_nl_cmd_mutex);\n\t\tpr_warn(\"netlink cmd %d already executing on %s\\n\",\n\t\t\t nl_cmd->cmd, udev->name);\n\t\treturn -EBUSY;\n\t}\n\n\tmemset(nl_cmd, 0, sizeof(*nl_cmd));\n\tnl_cmd->cmd = cmd;\n\tnl_cmd->udev = udev;\n\tinit_completion(&nl_cmd->complete);\n\tINIT_LIST_HEAD(&nl_cmd->nl_list);\n\n\tlist_add_tail(&nl_cmd->nl_list, &tcmu_nl_cmd_list);\n\n\tmutex_unlock(&tcmu_nl_cmd_mutex);\n\treturn 0;\n}\n\nstatic void tcmu_destroy_genl_cmd_reply(struct tcmu_dev *udev)\n{\n\tstruct tcmu_nl_cmd *nl_cmd = &udev->curr_nl_cmd;\n\n\tif (!tcmu_kern_cmd_reply_supported)\n\t\treturn;\n\n\tif (udev->nl_reply_supported <= 0)\n\t\treturn;\n\n\tmutex_lock(&tcmu_nl_cmd_mutex);\n\n\tlist_del(&nl_cmd->nl_list);\n\tmemset(nl_cmd, 0, sizeof(*nl_cmd));\n\n\tmutex_unlock(&tcmu_nl_cmd_mutex);\n}\n\nstatic int tcmu_wait_genl_cmd_reply(struct tcmu_dev *udev)\n{\n\tstruct tcmu_nl_cmd *nl_cmd = &udev->curr_nl_cmd;\n\tint ret;\n\n\tif (!tcmu_kern_cmd_reply_supported)\n\t\treturn 0;\n\n\tif (udev->nl_reply_supported <= 0)\n\t\treturn 0;\n\n\tpr_debug(\"sleeping for nl reply\\n\");\n\twait_for_completion(&nl_cmd->complete);\n\n\tmutex_lock(&tcmu_nl_cmd_mutex);\n\tnl_cmd->cmd = TCMU_CMD_UNSPEC;\n\tret = nl_cmd->status;\n\tmutex_unlock(&tcmu_nl_cmd_mutex);\n\n\treturn ret;\n}\n\nstatic int tcmu_netlink_event_init(struct tcmu_dev *udev,\n\t\t\t\t   enum tcmu_genl_cmd cmd,\n\t\t\t\t   struct sk_buff **buf, void **hdr)\n{\n\tstruct sk_buff *skb;\n\tvoid *msg_header;\n\tint ret = -ENOMEM;\n\n\tskb = genlmsg_new(NLMSG_GOODSIZE, GFP_KERNEL);\n\tif (!skb)\n\t\treturn ret;\n\n\tmsg_header = genlmsg_put(skb, 0, 0, &tcmu_genl_family, 0, cmd);\n\tif (!msg_header)\n\t\tgoto free_skb;\n\n\tret = nla_put_string(skb, TCMU_ATTR_DEVICE, udev->uio_info.name);\n\tif (ret < 0)\n\t\tgoto free_skb;\n\n\tret = nla_put_u32(skb, TCMU_ATTR_MINOR, udev->uio_info.uio_dev->minor);\n\tif (ret < 0)\n\t\tgoto free_skb;\n\n\tret = nla_put_u32(skb, TCMU_ATTR_DEVICE_ID, udev->se_dev.dev_index);\n\tif (ret < 0)\n\t\tgoto free_skb;\n\n\t*buf = skb;\n\t*hdr = msg_header;\n\treturn ret;\n\nfree_skb:\n\tnlmsg_free(skb);\n\treturn ret;\n}\n\nstatic int tcmu_netlink_event_send(struct tcmu_dev *udev,\n\t\t\t\t   enum tcmu_genl_cmd cmd,\n\t\t\t\t   struct sk_buff *skb, void *msg_header)\n{\n\tint ret;\n\n\tgenlmsg_end(skb, msg_header);\n\n\tret = tcmu_init_genl_cmd_reply(udev, cmd);\n\tif (ret) {\n\t\tnlmsg_free(skb);\n\t\treturn ret;\n\t}\n\n\tret = genlmsg_multicast_allns(&tcmu_genl_family, skb, 0,\n\t\t\t\t      TCMU_MCGRP_CONFIG, GFP_KERNEL);\n\n\t \n\tif (ret == 0 ||\n\t   (ret == -ESRCH && cmd == TCMU_CMD_ADDED_DEVICE))\n\t\treturn tcmu_wait_genl_cmd_reply(udev);\n\telse\n\t\ttcmu_destroy_genl_cmd_reply(udev);\n\n\treturn ret;\n}\n\nstatic int tcmu_send_dev_add_event(struct tcmu_dev *udev)\n{\n\tstruct sk_buff *skb = NULL;\n\tvoid *msg_header = NULL;\n\tint ret = 0;\n\n\tret = tcmu_netlink_event_init(udev, TCMU_CMD_ADDED_DEVICE, &skb,\n\t\t\t\t      &msg_header);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn tcmu_netlink_event_send(udev, TCMU_CMD_ADDED_DEVICE, skb,\n\t\t\t\t       msg_header);\n}\n\nstatic int tcmu_send_dev_remove_event(struct tcmu_dev *udev)\n{\n\tstruct sk_buff *skb = NULL;\n\tvoid *msg_header = NULL;\n\tint ret = 0;\n\n\tret = tcmu_netlink_event_init(udev, TCMU_CMD_REMOVED_DEVICE,\n\t\t\t\t      &skb, &msg_header);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn tcmu_netlink_event_send(udev, TCMU_CMD_REMOVED_DEVICE,\n\t\t\t\t       skb, msg_header);\n}\n\nstatic int tcmu_update_uio_info(struct tcmu_dev *udev)\n{\n\tstruct tcmu_hba *hba = udev->hba->hba_ptr;\n\tstruct uio_info *info;\n\tchar *str;\n\n\tinfo = &udev->uio_info;\n\n\tif (udev->dev_config[0])\n\t\tstr = kasprintf(GFP_KERNEL, \"tcm-user/%u/%s/%s\", hba->host_id,\n\t\t\t\tudev->name, udev->dev_config);\n\telse\n\t\tstr = kasprintf(GFP_KERNEL, \"tcm-user/%u/%s\", hba->host_id,\n\t\t\t\tudev->name);\n\tif (!str)\n\t\treturn -ENOMEM;\n\n\t \n\tkfree(info->name);\n\tinfo->name = str;\n\n\treturn 0;\n}\n\nstatic int tcmu_configure_device(struct se_device *dev)\n{\n\tstruct tcmu_dev *udev = TCMU_DEV(dev);\n\tstruct uio_info *info;\n\tstruct tcmu_mailbox *mb;\n\tsize_t data_size;\n\tint ret = 0;\n\n\tret = tcmu_update_uio_info(udev);\n\tif (ret)\n\t\treturn ret;\n\n\tinfo = &udev->uio_info;\n\n\tmutex_lock(&udev->cmdr_lock);\n\tudev->data_bitmap = bitmap_zalloc(udev->max_blocks, GFP_KERNEL);\n\tmutex_unlock(&udev->cmdr_lock);\n\tif (!udev->data_bitmap) {\n\t\tret = -ENOMEM;\n\t\tgoto err_bitmap_alloc;\n\t}\n\n\tmb = vzalloc(udev->cmdr_size + CMDR_OFF);\n\tif (!mb) {\n\t\tret = -ENOMEM;\n\t\tgoto err_vzalloc;\n\t}\n\n\t \n\tudev->mb_addr = mb;\n\tudev->cmdr = (void *)mb + CMDR_OFF;\n\tudev->data_off = udev->cmdr_size + CMDR_OFF;\n\tdata_size = TCMU_MBS_TO_PAGES(udev->data_area_mb) << PAGE_SHIFT;\n\tudev->mmap_pages = (data_size + udev->cmdr_size + CMDR_OFF) >> PAGE_SHIFT;\n\tudev->data_blk_size = udev->data_pages_per_blk * PAGE_SIZE;\n\tudev->dbi_thresh = 0;  \n\n\t \n\tmb->version = TCMU_MAILBOX_VERSION;\n\tmb->flags = TCMU_MAILBOX_FLAG_CAP_OOOC |\n\t\t    TCMU_MAILBOX_FLAG_CAP_READ_LEN |\n\t\t    TCMU_MAILBOX_FLAG_CAP_TMR |\n\t\t    TCMU_MAILBOX_FLAG_CAP_KEEP_BUF;\n\tmb->cmdr_off = CMDR_OFF;\n\tmb->cmdr_size = udev->cmdr_size;\n\n\tWARN_ON(!PAGE_ALIGNED(udev->data_off));\n\tWARN_ON(data_size % PAGE_SIZE);\n\n\tinfo->version = __stringify(TCMU_MAILBOX_VERSION);\n\n\tinfo->mem[0].name = \"tcm-user command & data buffer\";\n\tinfo->mem[0].addr = (phys_addr_t)(uintptr_t)udev->mb_addr;\n\tinfo->mem[0].size = data_size + udev->cmdr_size + CMDR_OFF;\n\tinfo->mem[0].memtype = UIO_MEM_NONE;\n\n\tinfo->irqcontrol = tcmu_irqcontrol;\n\tinfo->irq = UIO_IRQ_CUSTOM;\n\n\tinfo->mmap = tcmu_mmap;\n\tinfo->open = tcmu_open;\n\tinfo->release = tcmu_release;\n\n\tret = uio_register_device(tcmu_root_device, info);\n\tif (ret)\n\t\tgoto err_register;\n\n\t \n\tif (dev->dev_attrib.hw_block_size == 0)\n\t\tdev->dev_attrib.hw_block_size = 512;\n\t \n\tif (!dev->dev_attrib.hw_max_sectors)\n\t\tdev->dev_attrib.hw_max_sectors = 128;\n\tif (!dev->dev_attrib.emulate_write_cache)\n\t\tdev->dev_attrib.emulate_write_cache = 0;\n\tdev->dev_attrib.hw_queue_depth = 128;\n\n\t \n\tif (udev->nl_reply_supported >= 0)\n\t\tudev->nl_reply_supported = tcmu_kern_cmd_reply_supported;\n\n\t \n\tkref_get(&udev->kref);\n\n\tret = tcmu_send_dev_add_event(udev);\n\tif (ret)\n\t\tgoto err_netlink;\n\n\tmutex_lock(&root_udev_mutex);\n\tlist_add(&udev->node, &root_udev);\n\tmutex_unlock(&root_udev_mutex);\n\n\treturn 0;\n\nerr_netlink:\n\tkref_put(&udev->kref, tcmu_dev_kref_release);\n\tuio_unregister_device(&udev->uio_info);\nerr_register:\n\tvfree(udev->mb_addr);\n\tudev->mb_addr = NULL;\nerr_vzalloc:\n\tbitmap_free(udev->data_bitmap);\n\tudev->data_bitmap = NULL;\nerr_bitmap_alloc:\n\tkfree(info->name);\n\tinfo->name = NULL;\n\n\treturn ret;\n}\n\nstatic void tcmu_free_device(struct se_device *dev)\n{\n\tstruct tcmu_dev *udev = TCMU_DEV(dev);\n\n\t \n\tkref_put(&udev->kref, tcmu_dev_kref_release);\n}\n\nstatic void tcmu_destroy_device(struct se_device *dev)\n{\n\tstruct tcmu_dev *udev = TCMU_DEV(dev);\n\n\tdel_timer_sync(&udev->cmd_timer);\n\tdel_timer_sync(&udev->qfull_timer);\n\n\tmutex_lock(&root_udev_mutex);\n\tlist_del(&udev->node);\n\tmutex_unlock(&root_udev_mutex);\n\n\ttcmu_send_dev_remove_event(udev);\n\n\tuio_unregister_device(&udev->uio_info);\n\n\t \n\tkref_put(&udev->kref, tcmu_dev_kref_release);\n}\n\nstatic void tcmu_unblock_dev(struct tcmu_dev *udev)\n{\n\tmutex_lock(&udev->cmdr_lock);\n\tclear_bit(TCMU_DEV_BIT_BLOCKED, &udev->flags);\n\tmutex_unlock(&udev->cmdr_lock);\n}\n\nstatic void tcmu_block_dev(struct tcmu_dev *udev)\n{\n\tmutex_lock(&udev->cmdr_lock);\n\n\tif (test_and_set_bit(TCMU_DEV_BIT_BLOCKED, &udev->flags))\n\t\tgoto unlock;\n\n\t \n\ttcmu_handle_completions(udev);\n\t \n\trun_qfull_queue(udev, true);\n\nunlock:\n\tmutex_unlock(&udev->cmdr_lock);\n}\n\nstatic void tcmu_reset_ring(struct tcmu_dev *udev, u8 err_level)\n{\n\tstruct tcmu_mailbox *mb;\n\tstruct tcmu_cmd *cmd;\n\tunsigned long i;\n\n\tmutex_lock(&udev->cmdr_lock);\n\n\txa_for_each(&udev->commands, i, cmd) {\n\t\tpr_debug(\"removing cmd %u on dev %s from ring %s\\n\",\n\t\t\t cmd->cmd_id, udev->name,\n\t\t\t test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags) ?\n\t\t\t \"(is expired)\" :\n\t\t\t (test_bit(TCMU_CMD_BIT_KEEP_BUF, &cmd->flags) ?\n\t\t\t \"(is keep buffer)\" : \"\"));\n\n\t\txa_erase(&udev->commands, i);\n\t\tif (!test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags) &&\n\t\t    !test_bit(TCMU_CMD_BIT_KEEP_BUF, &cmd->flags)) {\n\t\t\tWARN_ON(!cmd->se_cmd);\n\t\t\tlist_del_init(&cmd->queue_entry);\n\t\t\tcmd->se_cmd->priv = NULL;\n\t\t\tif (err_level == 1) {\n\t\t\t\t \n\t\t\t\ttarget_complete_cmd(cmd->se_cmd, SAM_STAT_BUSY);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\ttarget_complete_cmd(cmd->se_cmd,\n\t\t\t\t\t\t    SAM_STAT_CHECK_CONDITION);\n\t\t\t}\n\t\t}\n\t\ttcmu_cmd_free_data(cmd, cmd->dbi_cnt);\n\t\ttcmu_free_cmd(cmd);\n\t}\n\n\tmb = udev->mb_addr;\n\ttcmu_flush_dcache_range(mb, sizeof(*mb));\n\tpr_debug(\"mb last %u head %u tail %u\\n\", udev->cmdr_last_cleaned,\n\t\t mb->cmd_tail, mb->cmd_head);\n\n\tudev->cmdr_last_cleaned = 0;\n\tmb->cmd_tail = 0;\n\tmb->cmd_head = 0;\n\ttcmu_flush_dcache_range(mb, sizeof(*mb));\n\tclear_bit(TCMU_DEV_BIT_BROKEN, &udev->flags);\n\n\tdel_timer(&udev->cmd_timer);\n\n\t \n\ttcmu_remove_all_queued_tmr(udev);\n\n\trun_qfull_queue(udev, false);\n\n\tmutex_unlock(&udev->cmdr_lock);\n}\n\nenum {\n\tOpt_dev_config, Opt_dev_size, Opt_hw_block_size, Opt_hw_max_sectors,\n\tOpt_nl_reply_supported, Opt_max_data_area_mb, Opt_data_pages_per_blk,\n\tOpt_cmd_ring_size_mb, Opt_err,\n};\n\nstatic match_table_t tokens = {\n\t{Opt_dev_config, \"dev_config=%s\"},\n\t{Opt_dev_size, \"dev_size=%s\"},\n\t{Opt_hw_block_size, \"hw_block_size=%d\"},\n\t{Opt_hw_max_sectors, \"hw_max_sectors=%d\"},\n\t{Opt_nl_reply_supported, \"nl_reply_supported=%d\"},\n\t{Opt_max_data_area_mb, \"max_data_area_mb=%d\"},\n\t{Opt_data_pages_per_blk, \"data_pages_per_blk=%d\"},\n\t{Opt_cmd_ring_size_mb, \"cmd_ring_size_mb=%d\"},\n\t{Opt_err, NULL}\n};\n\nstatic int tcmu_set_dev_attrib(substring_t *arg, u32 *dev_attrib)\n{\n\tint val, ret;\n\n\tret = match_int(arg, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"match_int() failed for dev attrib. Error %d.\\n\",\n\t\t       ret);\n\t\treturn ret;\n\t}\n\n\tif (val <= 0) {\n\t\tpr_err(\"Invalid dev attrib value %d. Must be greater than zero.\\n\",\n\t\t       val);\n\t\treturn -EINVAL;\n\t}\n\t*dev_attrib = val;\n\treturn 0;\n}\n\nstatic int tcmu_set_max_blocks_param(struct tcmu_dev *udev, substring_t *arg)\n{\n\tint val, ret;\n\tuint32_t pages_per_blk = udev->data_pages_per_blk;\n\n\tret = match_int(arg, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"match_int() failed for max_data_area_mb=. Error %d.\\n\",\n\t\t       ret);\n\t\treturn ret;\n\t}\n\tif (val <= 0) {\n\t\tpr_err(\"Invalid max_data_area %d.\\n\", val);\n\t\treturn -EINVAL;\n\t}\n\tif (val > TCMU_PAGES_TO_MBS(tcmu_global_max_pages)) {\n\t\tpr_err(\"%d is too large. Adjusting max_data_area_mb to global limit of %u\\n\",\n\t\t       val, TCMU_PAGES_TO_MBS(tcmu_global_max_pages));\n\t\tval = TCMU_PAGES_TO_MBS(tcmu_global_max_pages);\n\t}\n\tif (TCMU_MBS_TO_PAGES(val) < pages_per_blk) {\n\t\tpr_err(\"Invalid max_data_area %d (%zu pages): smaller than data_pages_per_blk (%u pages).\\n\",\n\t\t       val, TCMU_MBS_TO_PAGES(val), pages_per_blk);\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&udev->cmdr_lock);\n\tif (udev->data_bitmap) {\n\t\tpr_err(\"Cannot set max_data_area_mb after it has been enabled.\\n\");\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tudev->data_area_mb = val;\n\tudev->max_blocks = TCMU_MBS_TO_PAGES(val) / pages_per_blk;\n\nunlock:\n\tmutex_unlock(&udev->cmdr_lock);\n\treturn ret;\n}\n\nstatic int tcmu_set_data_pages_per_blk(struct tcmu_dev *udev, substring_t *arg)\n{\n\tint val, ret;\n\n\tret = match_int(arg, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"match_int() failed for data_pages_per_blk=. Error %d.\\n\",\n\t\t       ret);\n\t\treturn ret;\n\t}\n\n\tif (val > TCMU_MBS_TO_PAGES(udev->data_area_mb)) {\n\t\tpr_err(\"Invalid data_pages_per_blk %d: greater than max_data_area_mb %d -> %zd pages).\\n\",\n\t\t       val, udev->data_area_mb,\n\t\t       TCMU_MBS_TO_PAGES(udev->data_area_mb));\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&udev->cmdr_lock);\n\tif (udev->data_bitmap) {\n\t\tpr_err(\"Cannot set data_pages_per_blk after it has been enabled.\\n\");\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tudev->data_pages_per_blk = val;\n\tudev->max_blocks = TCMU_MBS_TO_PAGES(udev->data_area_mb) / val;\n\nunlock:\n\tmutex_unlock(&udev->cmdr_lock);\n\treturn ret;\n}\n\nstatic int tcmu_set_cmd_ring_size(struct tcmu_dev *udev, substring_t *arg)\n{\n\tint val, ret;\n\n\tret = match_int(arg, &val);\n\tif (ret < 0) {\n\t\tpr_err(\"match_int() failed for cmd_ring_size_mb=. Error %d.\\n\",\n\t\t       ret);\n\t\treturn ret;\n\t}\n\n\tif (val <= 0) {\n\t\tpr_err(\"Invalid cmd_ring_size_mb %d.\\n\", val);\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&udev->cmdr_lock);\n\tif (udev->data_bitmap) {\n\t\tpr_err(\"Cannot set cmd_ring_size_mb after it has been enabled.\\n\");\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tudev->cmdr_size = (val << 20) - CMDR_OFF;\n\tif (val > (MB_CMDR_SIZE_DEF >> 20)) {\n\t\tpr_err(\"%d is too large. Adjusting cmd_ring_size_mb to global limit of %u\\n\",\n\t\t       val, (MB_CMDR_SIZE_DEF >> 20));\n\t\tudev->cmdr_size = CMDR_SIZE_DEF;\n\t}\n\nunlock:\n\tmutex_unlock(&udev->cmdr_lock);\n\treturn ret;\n}\n\nstatic ssize_t tcmu_set_configfs_dev_params(struct se_device *dev,\n\t\tconst char *page, ssize_t count)\n{\n\tstruct tcmu_dev *udev = TCMU_DEV(dev);\n\tchar *orig, *ptr, *opts;\n\tsubstring_t args[MAX_OPT_ARGS];\n\tint ret = 0, token;\n\n\topts = kstrdup(page, GFP_KERNEL);\n\tif (!opts)\n\t\treturn -ENOMEM;\n\n\torig = opts;\n\n\twhile ((ptr = strsep(&opts, \",\\n\")) != NULL) {\n\t\tif (!*ptr)\n\t\t\tcontinue;\n\n\t\ttoken = match_token(ptr, tokens, args);\n\t\tswitch (token) {\n\t\tcase Opt_dev_config:\n\t\t\tif (match_strlcpy(udev->dev_config, &args[0],\n\t\t\t\t\t  TCMU_CONFIG_LEN) == 0) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpr_debug(\"TCMU: Referencing Path: %s\\n\", udev->dev_config);\n\t\t\tbreak;\n\t\tcase Opt_dev_size:\n\t\t\tret = match_u64(&args[0], &udev->dev_size);\n\t\t\tif (ret < 0)\n\t\t\t\tpr_err(\"match_u64() failed for dev_size=. Error %d.\\n\",\n\t\t\t\t       ret);\n\t\t\tbreak;\n\t\tcase Opt_hw_block_size:\n\t\t\tret = tcmu_set_dev_attrib(&args[0],\n\t\t\t\t\t&(dev->dev_attrib.hw_block_size));\n\t\t\tbreak;\n\t\tcase Opt_hw_max_sectors:\n\t\t\tret = tcmu_set_dev_attrib(&args[0],\n\t\t\t\t\t&(dev->dev_attrib.hw_max_sectors));\n\t\t\tbreak;\n\t\tcase Opt_nl_reply_supported:\n\t\t\tret = match_int(&args[0], &udev->nl_reply_supported);\n\t\t\tif (ret < 0)\n\t\t\t\tpr_err(\"match_int() failed for nl_reply_supported=. Error %d.\\n\",\n\t\t\t\t       ret);\n\t\t\tbreak;\n\t\tcase Opt_max_data_area_mb:\n\t\t\tret = tcmu_set_max_blocks_param(udev, &args[0]);\n\t\t\tbreak;\n\t\tcase Opt_data_pages_per_blk:\n\t\t\tret = tcmu_set_data_pages_per_blk(udev, &args[0]);\n\t\t\tbreak;\n\t\tcase Opt_cmd_ring_size_mb:\n\t\t\tret = tcmu_set_cmd_ring_size(udev, &args[0]);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tkfree(orig);\n\treturn (!ret) ? count : ret;\n}\n\nstatic ssize_t tcmu_show_configfs_dev_params(struct se_device *dev, char *b)\n{\n\tstruct tcmu_dev *udev = TCMU_DEV(dev);\n\tssize_t bl = 0;\n\n\tbl = sprintf(b + bl, \"Config: %s \",\n\t\t     udev->dev_config[0] ? udev->dev_config : \"NULL\");\n\tbl += sprintf(b + bl, \"Size: %llu \", udev->dev_size);\n\tbl += sprintf(b + bl, \"MaxDataAreaMB: %u \", udev->data_area_mb);\n\tbl += sprintf(b + bl, \"DataPagesPerBlk: %u \", udev->data_pages_per_blk);\n\tbl += sprintf(b + bl, \"CmdRingSizeMB: %u\\n\",\n\t\t      (udev->cmdr_size + CMDR_OFF) >> 20);\n\n\treturn bl;\n}\n\nstatic sector_t tcmu_get_blocks(struct se_device *dev)\n{\n\tstruct tcmu_dev *udev = TCMU_DEV(dev);\n\n\treturn div_u64(udev->dev_size - dev->dev_attrib.block_size,\n\t\t       dev->dev_attrib.block_size);\n}\n\nstatic sense_reason_t\ntcmu_parse_cdb(struct se_cmd *cmd)\n{\n\treturn passthrough_parse_cdb(cmd, tcmu_queue_cmd);\n}\n\nstatic ssize_t tcmu_cmd_time_out_show(struct config_item *item, char *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\n\treturn snprintf(page, PAGE_SIZE, \"%lu\\n\", udev->cmd_time_out / MSEC_PER_SEC);\n}\n\nstatic ssize_t tcmu_cmd_time_out_store(struct config_item *item, const char *page,\n\t\t\t\t       size_t count)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = container_of(da->da_dev,\n\t\t\t\t\tstruct tcmu_dev, se_dev);\n\tu32 val;\n\tint ret;\n\n\tif (da->da_dev->export_count) {\n\t\tpr_err(\"Unable to set tcmu cmd_time_out while exports exist\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = kstrtou32(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tudev->cmd_time_out = val * MSEC_PER_SEC;\n\treturn count;\n}\nCONFIGFS_ATTR(tcmu_, cmd_time_out);\n\nstatic ssize_t tcmu_qfull_time_out_show(struct config_item *item, char *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\n\treturn snprintf(page, PAGE_SIZE, \"%ld\\n\", udev->qfull_time_out <= 0 ?\n\t\t\tudev->qfull_time_out :\n\t\t\tudev->qfull_time_out / MSEC_PER_SEC);\n}\n\nstatic ssize_t tcmu_qfull_time_out_store(struct config_item *item,\n\t\t\t\t\t const char *page, size_t count)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\ts32 val;\n\tint ret;\n\n\tret = kstrtos32(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (val >= 0) {\n\t\tudev->qfull_time_out = val * MSEC_PER_SEC;\n\t} else if (val == -1) {\n\t\tudev->qfull_time_out = val;\n\t} else {\n\t\tprintk(KERN_ERR \"Invalid qfull timeout value %d\\n\", val);\n\t\treturn -EINVAL;\n\t}\n\treturn count;\n}\nCONFIGFS_ATTR(tcmu_, qfull_time_out);\n\nstatic ssize_t tcmu_max_data_area_mb_show(struct config_item *item, char *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\n\treturn snprintf(page, PAGE_SIZE, \"%u\\n\", udev->data_area_mb);\n}\nCONFIGFS_ATTR_RO(tcmu_, max_data_area_mb);\n\nstatic ssize_t tcmu_data_pages_per_blk_show(struct config_item *item,\n\t\t\t\t\t    char *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\n\treturn snprintf(page, PAGE_SIZE, \"%u\\n\", udev->data_pages_per_blk);\n}\nCONFIGFS_ATTR_RO(tcmu_, data_pages_per_blk);\n\nstatic ssize_t tcmu_cmd_ring_size_mb_show(struct config_item *item, char *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\n\treturn snprintf(page, PAGE_SIZE, \"%u\\n\",\n\t\t\t(udev->cmdr_size + CMDR_OFF) >> 20);\n}\nCONFIGFS_ATTR_RO(tcmu_, cmd_ring_size_mb);\n\nstatic ssize_t tcmu_dev_config_show(struct config_item *item, char *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\n\treturn snprintf(page, PAGE_SIZE, \"%s\\n\", udev->dev_config);\n}\n\nstatic int tcmu_send_dev_config_event(struct tcmu_dev *udev,\n\t\t\t\t      const char *reconfig_data)\n{\n\tstruct sk_buff *skb = NULL;\n\tvoid *msg_header = NULL;\n\tint ret = 0;\n\n\tret = tcmu_netlink_event_init(udev, TCMU_CMD_RECONFIG_DEVICE,\n\t\t\t\t      &skb, &msg_header);\n\tif (ret < 0)\n\t\treturn ret;\n\tret = nla_put_string(skb, TCMU_ATTR_DEV_CFG, reconfig_data);\n\tif (ret < 0) {\n\t\tnlmsg_free(skb);\n\t\treturn ret;\n\t}\n\treturn tcmu_netlink_event_send(udev, TCMU_CMD_RECONFIG_DEVICE,\n\t\t\t\t       skb, msg_header);\n}\n\n\nstatic ssize_t tcmu_dev_config_store(struct config_item *item, const char *page,\n\t\t\t\t     size_t count)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\tint ret, len;\n\n\tlen = strlen(page);\n\tif (!len || len > TCMU_CONFIG_LEN - 1)\n\t\treturn -EINVAL;\n\n\t \n\tif (target_dev_configured(&udev->se_dev)) {\n\t\tret = tcmu_send_dev_config_event(udev, page);\n\t\tif (ret) {\n\t\t\tpr_err(\"Unable to reconfigure device\\n\");\n\t\t\treturn ret;\n\t\t}\n\t\tstrscpy(udev->dev_config, page, TCMU_CONFIG_LEN);\n\n\t\tret = tcmu_update_uio_info(udev);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\treturn count;\n\t}\n\tstrscpy(udev->dev_config, page, TCMU_CONFIG_LEN);\n\n\treturn count;\n}\nCONFIGFS_ATTR(tcmu_, dev_config);\n\nstatic ssize_t tcmu_dev_size_show(struct config_item *item, char *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\n\treturn snprintf(page, PAGE_SIZE, \"%llu\\n\", udev->dev_size);\n}\n\nstatic int tcmu_send_dev_size_event(struct tcmu_dev *udev, u64 size)\n{\n\tstruct sk_buff *skb = NULL;\n\tvoid *msg_header = NULL;\n\tint ret = 0;\n\n\tret = tcmu_netlink_event_init(udev, TCMU_CMD_RECONFIG_DEVICE,\n\t\t\t\t      &skb, &msg_header);\n\tif (ret < 0)\n\t\treturn ret;\n\tret = nla_put_u64_64bit(skb, TCMU_ATTR_DEV_SIZE,\n\t\t\t\tsize, TCMU_ATTR_PAD);\n\tif (ret < 0) {\n\t\tnlmsg_free(skb);\n\t\treturn ret;\n\t}\n\treturn tcmu_netlink_event_send(udev, TCMU_CMD_RECONFIG_DEVICE,\n\t\t\t\t       skb, msg_header);\n}\n\nstatic ssize_t tcmu_dev_size_store(struct config_item *item, const char *page,\n\t\t\t\t   size_t count)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\tu64 val;\n\tint ret;\n\n\tret = kstrtou64(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tif (target_dev_configured(&udev->se_dev)) {\n\t\tret = tcmu_send_dev_size_event(udev, val);\n\t\tif (ret) {\n\t\t\tpr_err(\"Unable to reconfigure device\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\tudev->dev_size = val;\n\treturn count;\n}\nCONFIGFS_ATTR(tcmu_, dev_size);\n\nstatic ssize_t tcmu_nl_reply_supported_show(struct config_item *item,\n\t\tchar *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\n\treturn snprintf(page, PAGE_SIZE, \"%d\\n\", udev->nl_reply_supported);\n}\n\nstatic ssize_t tcmu_nl_reply_supported_store(struct config_item *item,\n\t\tconst char *page, size_t count)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\ts8 val;\n\tint ret;\n\n\tret = kstrtos8(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tudev->nl_reply_supported = val;\n\treturn count;\n}\nCONFIGFS_ATTR(tcmu_, nl_reply_supported);\n\nstatic ssize_t tcmu_emulate_write_cache_show(struct config_item *item,\n\t\t\t\t\t     char *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\tstruct se_dev_attrib, da_group);\n\n\treturn snprintf(page, PAGE_SIZE, \"%i\\n\", da->emulate_write_cache);\n}\n\nstatic int tcmu_send_emulate_write_cache(struct tcmu_dev *udev, u8 val)\n{\n\tstruct sk_buff *skb = NULL;\n\tvoid *msg_header = NULL;\n\tint ret = 0;\n\n\tret = tcmu_netlink_event_init(udev, TCMU_CMD_RECONFIG_DEVICE,\n\t\t\t\t      &skb, &msg_header);\n\tif (ret < 0)\n\t\treturn ret;\n\tret = nla_put_u8(skb, TCMU_ATTR_WRITECACHE, val);\n\tif (ret < 0) {\n\t\tnlmsg_free(skb);\n\t\treturn ret;\n\t}\n\treturn tcmu_netlink_event_send(udev, TCMU_CMD_RECONFIG_DEVICE,\n\t\t\t\t       skb, msg_header);\n}\n\nstatic ssize_t tcmu_emulate_write_cache_store(struct config_item *item,\n\t\t\t\t\t      const char *page, size_t count)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\tu8 val;\n\tint ret;\n\n\tret = kstrtou8(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tif (target_dev_configured(&udev->se_dev)) {\n\t\tret = tcmu_send_emulate_write_cache(udev, val);\n\t\tif (ret) {\n\t\t\tpr_err(\"Unable to reconfigure device\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tda->emulate_write_cache = val;\n\treturn count;\n}\nCONFIGFS_ATTR(tcmu_, emulate_write_cache);\n\nstatic ssize_t tcmu_tmr_notification_show(struct config_item *item, char *page)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\n\treturn snprintf(page, PAGE_SIZE, \"%i\\n\",\n\t\t\ttest_bit(TCMU_DEV_BIT_TMR_NOTIFY, &udev->flags));\n}\n\nstatic ssize_t tcmu_tmr_notification_store(struct config_item *item,\n\t\t\t\t\t   const char *page, size_t count)\n{\n\tstruct se_dev_attrib *da = container_of(to_config_group(item),\n\t\t\t\t\tstruct se_dev_attrib, da_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(da->da_dev);\n\tu8 val;\n\tint ret;\n\n\tret = kstrtou8(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (val > 1)\n\t\treturn -EINVAL;\n\n\tif (val)\n\t\tset_bit(TCMU_DEV_BIT_TMR_NOTIFY, &udev->flags);\n\telse\n\t\tclear_bit(TCMU_DEV_BIT_TMR_NOTIFY, &udev->flags);\n\treturn count;\n}\nCONFIGFS_ATTR(tcmu_, tmr_notification);\n\nstatic ssize_t tcmu_block_dev_show(struct config_item *item, char *page)\n{\n\tstruct se_device *se_dev = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_device,\n\t\t\t\t\t\tdev_action_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(se_dev);\n\n\tif (test_bit(TCMU_DEV_BIT_BLOCKED, &udev->flags))\n\t\treturn snprintf(page, PAGE_SIZE, \"%s\\n\", \"blocked\");\n\telse\n\t\treturn snprintf(page, PAGE_SIZE, \"%s\\n\", \"unblocked\");\n}\n\nstatic ssize_t tcmu_block_dev_store(struct config_item *item, const char *page,\n\t\t\t\t    size_t count)\n{\n\tstruct se_device *se_dev = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_device,\n\t\t\t\t\t\tdev_action_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(se_dev);\n\tu8 val;\n\tint ret;\n\n\tif (!target_dev_configured(&udev->se_dev)) {\n\t\tpr_err(\"Device is not configured.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = kstrtou8(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (val > 1) {\n\t\tpr_err(\"Invalid block value %d\\n\", val);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!val)\n\t\ttcmu_unblock_dev(udev);\n\telse\n\t\ttcmu_block_dev(udev);\n\treturn count;\n}\nCONFIGFS_ATTR(tcmu_, block_dev);\n\nstatic ssize_t tcmu_reset_ring_store(struct config_item *item, const char *page,\n\t\t\t\t     size_t count)\n{\n\tstruct se_device *se_dev = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_device,\n\t\t\t\t\t\tdev_action_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(se_dev);\n\tu8 val;\n\tint ret;\n\n\tif (!target_dev_configured(&udev->se_dev)) {\n\t\tpr_err(\"Device is not configured.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = kstrtou8(page, 0, &val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (val != 1 && val != 2) {\n\t\tpr_err(\"Invalid reset ring value %d\\n\", val);\n\t\treturn -EINVAL;\n\t}\n\n\ttcmu_reset_ring(udev, val);\n\treturn count;\n}\nCONFIGFS_ATTR_WO(tcmu_, reset_ring);\n\nstatic ssize_t tcmu_free_kept_buf_store(struct config_item *item, const char *page,\n\t\t\t\t\tsize_t count)\n{\n\tstruct se_device *se_dev = container_of(to_config_group(item),\n\t\t\t\t\t\tstruct se_device,\n\t\t\t\t\t\tdev_action_group);\n\tstruct tcmu_dev *udev = TCMU_DEV(se_dev);\n\tstruct tcmu_cmd *cmd;\n\tu16 cmd_id;\n\tint ret;\n\n\tif (!target_dev_configured(&udev->se_dev)) {\n\t\tpr_err(\"Device is not configured.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = kstrtou16(page, 0, &cmd_id);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tmutex_lock(&udev->cmdr_lock);\n\n\t{\n\t\tXA_STATE(xas, &udev->commands, cmd_id);\n\n\t\txas_lock(&xas);\n\t\tcmd = xas_load(&xas);\n\t\tif (!cmd) {\n\t\t\tpr_err(\"free_kept_buf: cmd_id %d not found\\n\", cmd_id);\n\t\t\tcount = -EINVAL;\n\t\t\txas_unlock(&xas);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (!test_bit(TCMU_CMD_BIT_KEEP_BUF, &cmd->flags)) {\n\t\t\tpr_err(\"free_kept_buf: cmd_id %d was not completed with KEEP_BUF\\n\",\n\t\t\t       cmd_id);\n\t\t\tcount = -EINVAL;\n\t\t\txas_unlock(&xas);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\txas_store(&xas, NULL);\n\t\txas_unlock(&xas);\n\t}\n\n\ttcmu_cmd_free_data(cmd, cmd->dbi_cnt);\n\ttcmu_free_cmd(cmd);\n\t \n\tif (list_empty(&udev->tmr_queue))\n\t\trun_qfull_queue(udev, false);\n\nout_unlock:\n\tmutex_unlock(&udev->cmdr_lock);\n\treturn count;\n}\nCONFIGFS_ATTR_WO(tcmu_, free_kept_buf);\n\nstatic struct configfs_attribute *tcmu_attrib_attrs[] = {\n\t&tcmu_attr_cmd_time_out,\n\t&tcmu_attr_qfull_time_out,\n\t&tcmu_attr_max_data_area_mb,\n\t&tcmu_attr_data_pages_per_blk,\n\t&tcmu_attr_cmd_ring_size_mb,\n\t&tcmu_attr_dev_config,\n\t&tcmu_attr_dev_size,\n\t&tcmu_attr_emulate_write_cache,\n\t&tcmu_attr_tmr_notification,\n\t&tcmu_attr_nl_reply_supported,\n\tNULL,\n};\n\nstatic struct configfs_attribute **tcmu_attrs;\n\nstatic struct configfs_attribute *tcmu_action_attrs[] = {\n\t&tcmu_attr_block_dev,\n\t&tcmu_attr_reset_ring,\n\t&tcmu_attr_free_kept_buf,\n\tNULL,\n};\n\nstatic struct target_backend_ops tcmu_ops = {\n\t.name\t\t\t= \"user\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.transport_flags_default = TRANSPORT_FLAG_PASSTHROUGH,\n\t.transport_flags_changeable = TRANSPORT_FLAG_PASSTHROUGH_PGR |\n\t\t\t\t      TRANSPORT_FLAG_PASSTHROUGH_ALUA,\n\t.attach_hba\t\t= tcmu_attach_hba,\n\t.detach_hba\t\t= tcmu_detach_hba,\n\t.alloc_device\t\t= tcmu_alloc_device,\n\t.configure_device\t= tcmu_configure_device,\n\t.destroy_device\t\t= tcmu_destroy_device,\n\t.free_device\t\t= tcmu_free_device,\n\t.unplug_device\t\t= tcmu_unplug_device,\n\t.plug_device\t\t= tcmu_plug_device,\n\t.parse_cdb\t\t= tcmu_parse_cdb,\n\t.tmr_notify\t\t= tcmu_tmr_notify,\n\t.set_configfs_dev_params = tcmu_set_configfs_dev_params,\n\t.show_configfs_dev_params = tcmu_show_configfs_dev_params,\n\t.get_device_type\t= sbc_get_device_type,\n\t.get_blocks\t\t= tcmu_get_blocks,\n\t.tb_dev_action_attrs\t= tcmu_action_attrs,\n};\n\nstatic void find_free_blocks(void)\n{\n\tstruct tcmu_dev *udev;\n\tloff_t off;\n\tu32 pages_freed, total_pages_freed = 0;\n\tu32 start, end, block, total_blocks_freed = 0;\n\n\tif (atomic_read(&global_page_count) <= tcmu_global_max_pages)\n\t\treturn;\n\n\tmutex_lock(&root_udev_mutex);\n\tlist_for_each_entry(udev, &root_udev, node) {\n\t\tmutex_lock(&udev->cmdr_lock);\n\n\t\tif (!target_dev_configured(&udev->se_dev)) {\n\t\t\tmutex_unlock(&udev->cmdr_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (tcmu_handle_completions(udev))\n\t\t\trun_qfull_queue(udev, false);\n\n\t\t \n\t\tif (!udev->dbi_thresh) {\n\t\t\tmutex_unlock(&udev->cmdr_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tend = udev->dbi_max + 1;\n\t\tblock = find_last_bit(udev->data_bitmap, end);\n\t\tif (block == udev->dbi_max) {\n\t\t\t \n\t\t\tmutex_unlock(&udev->cmdr_lock);\n\t\t\tcontinue;\n\t\t} else if (block == end) {\n\t\t\t \n\t\t\tudev->dbi_thresh = start = 0;\n\t\t\tudev->dbi_max = 0;\n\t\t} else {\n\t\t\tudev->dbi_thresh = start = block + 1;\n\t\t\tudev->dbi_max = block;\n\t\t}\n\n\t\t \n\t\tpages_freed = tcmu_blocks_release(udev, start, end - 1);\n\n\t\t \n\t\toff = udev->data_off + (loff_t)start * udev->data_blk_size;\n\t\tunmap_mapping_range(udev->inode->i_mapping, off, 0, 1);\n\n\t\tmutex_unlock(&udev->cmdr_lock);\n\n\t\ttotal_pages_freed += pages_freed;\n\t\ttotal_blocks_freed += end - start;\n\t\tpr_debug(\"Freed %u pages (total %u) from %u blocks (total %u) from %s.\\n\",\n\t\t\t pages_freed, total_pages_freed, end - start,\n\t\t\t total_blocks_freed, udev->name);\n\t}\n\tmutex_unlock(&root_udev_mutex);\n\n\tif (atomic_read(&global_page_count) > tcmu_global_max_pages)\n\t\tschedule_delayed_work(&tcmu_unmap_work, msecs_to_jiffies(5000));\n}\n\nstatic void check_timedout_devices(void)\n{\n\tstruct tcmu_dev *udev, *tmp_dev;\n\tstruct tcmu_cmd *cmd, *tmp_cmd;\n\tLIST_HEAD(devs);\n\n\tspin_lock_bh(&timed_out_udevs_lock);\n\tlist_splice_init(&timed_out_udevs, &devs);\n\n\tlist_for_each_entry_safe(udev, tmp_dev, &devs, timedout_entry) {\n\t\tlist_del_init(&udev->timedout_entry);\n\t\tspin_unlock_bh(&timed_out_udevs_lock);\n\n\t\tmutex_lock(&udev->cmdr_lock);\n\n\t\t \n\t\tif (udev->cmd_time_out) {\n\t\t\tlist_for_each_entry_safe(cmd, tmp_cmd,\n\t\t\t\t\t\t &udev->inflight_queue,\n\t\t\t\t\t\t queue_entry) {\n\t\t\t\ttcmu_check_expired_ring_cmd(cmd);\n\t\t\t}\n\t\t\ttcmu_set_next_deadline(&udev->inflight_queue,\n\t\t\t\t\t       &udev->cmd_timer);\n\t\t}\n\t\tlist_for_each_entry_safe(cmd, tmp_cmd, &udev->qfull_queue,\n\t\t\t\t\t queue_entry) {\n\t\t\ttcmu_check_expired_queue_cmd(cmd);\n\t\t}\n\t\ttcmu_set_next_deadline(&udev->qfull_queue, &udev->qfull_timer);\n\n\t\tmutex_unlock(&udev->cmdr_lock);\n\n\t\tspin_lock_bh(&timed_out_udevs_lock);\n\t}\n\n\tspin_unlock_bh(&timed_out_udevs_lock);\n}\n\nstatic void tcmu_unmap_work_fn(struct work_struct *work)\n{\n\tcheck_timedout_devices();\n\tfind_free_blocks();\n}\n\nstatic int __init tcmu_module_init(void)\n{\n\tint ret, i, k, len = 0;\n\n\tBUILD_BUG_ON((sizeof(struct tcmu_cmd_entry) % TCMU_OP_ALIGN_SIZE) != 0);\n\n\tINIT_DELAYED_WORK(&tcmu_unmap_work, tcmu_unmap_work_fn);\n\n\ttcmu_cmd_cache = kmem_cache_create(\"tcmu_cmd_cache\",\n\t\t\t\tsizeof(struct tcmu_cmd),\n\t\t\t\t__alignof__(struct tcmu_cmd),\n\t\t\t\t0, NULL);\n\tif (!tcmu_cmd_cache)\n\t\treturn -ENOMEM;\n\n\ttcmu_root_device = root_device_register(\"tcm_user\");\n\tif (IS_ERR(tcmu_root_device)) {\n\t\tret = PTR_ERR(tcmu_root_device);\n\t\tgoto out_free_cache;\n\t}\n\n\tret = genl_register_family(&tcmu_genl_family);\n\tif (ret < 0) {\n\t\tgoto out_unreg_device;\n\t}\n\n\tfor (i = 0; passthrough_attrib_attrs[i] != NULL; i++)\n\t\tlen += sizeof(struct configfs_attribute *);\n\tfor (i = 0; passthrough_pr_attrib_attrs[i] != NULL; i++)\n\t\tlen += sizeof(struct configfs_attribute *);\n\tfor (i = 0; tcmu_attrib_attrs[i] != NULL; i++)\n\t\tlen += sizeof(struct configfs_attribute *);\n\tlen += sizeof(struct configfs_attribute *);\n\n\ttcmu_attrs = kzalloc(len, GFP_KERNEL);\n\tif (!tcmu_attrs) {\n\t\tret = -ENOMEM;\n\t\tgoto out_unreg_genl;\n\t}\n\n\tfor (i = 0; passthrough_attrib_attrs[i] != NULL; i++)\n\t\ttcmu_attrs[i] = passthrough_attrib_attrs[i];\n\tfor (k = 0; passthrough_pr_attrib_attrs[k] != NULL; k++)\n\t\ttcmu_attrs[i++] = passthrough_pr_attrib_attrs[k];\n\tfor (k = 0; tcmu_attrib_attrs[k] != NULL; k++)\n\t\ttcmu_attrs[i++] = tcmu_attrib_attrs[k];\n\ttcmu_ops.tb_dev_attrib_attrs = tcmu_attrs;\n\n\tret = transport_backend_register(&tcmu_ops);\n\tif (ret)\n\t\tgoto out_attrs;\n\n\treturn 0;\n\nout_attrs:\n\tkfree(tcmu_attrs);\nout_unreg_genl:\n\tgenl_unregister_family(&tcmu_genl_family);\nout_unreg_device:\n\troot_device_unregister(tcmu_root_device);\nout_free_cache:\n\tkmem_cache_destroy(tcmu_cmd_cache);\n\n\treturn ret;\n}\n\nstatic void __exit tcmu_module_exit(void)\n{\n\tcancel_delayed_work_sync(&tcmu_unmap_work);\n\ttarget_backend_unregister(&tcmu_ops);\n\tkfree(tcmu_attrs);\n\tgenl_unregister_family(&tcmu_genl_family);\n\troot_device_unregister(tcmu_root_device);\n\tkmem_cache_destroy(tcmu_cmd_cache);\n}\n\nMODULE_DESCRIPTION(\"TCM USER subsystem plugin\");\nMODULE_AUTHOR(\"Shaohua Li <shli@kernel.org>\");\nMODULE_AUTHOR(\"Andy Grover <agrover@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n\nmodule_init(tcmu_module_init);\nmodule_exit(tcmu_module_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}