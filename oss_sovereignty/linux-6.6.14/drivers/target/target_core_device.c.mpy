{
  "module_name": "target_core_device.c",
  "hash_id": "f8aceb022ed901cca21a6d25769493db4a203fa3005fa412509de671d32d6597",
  "original_prompt": "Ingested from linux-6.6.14/drivers/target/target_core_device.c",
  "human_readable_source": "\n \n\n#include <linux/net.h>\n#include <linux/string.h>\n#include <linux/delay.h>\n#include <linux/timer.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/kthread.h>\n#include <linux/in.h>\n#include <linux/export.h>\n#include <linux/t10-pi.h>\n#include <asm/unaligned.h>\n#include <net/sock.h>\n#include <net/tcp.h>\n#include <scsi/scsi_common.h>\n#include <scsi/scsi_proto.h>\n\n#include <target/target_core_base.h>\n#include <target/target_core_backend.h>\n#include <target/target_core_fabric.h>\n\n#include \"target_core_internal.h\"\n#include \"target_core_alua.h\"\n#include \"target_core_pr.h\"\n#include \"target_core_ua.h\"\n\nstatic DEFINE_MUTEX(device_mutex);\nstatic LIST_HEAD(device_list);\nstatic DEFINE_IDR(devices_idr);\n\nstatic struct se_hba *lun0_hba;\n \nstruct se_device *g_lun0_dev;\n\nsense_reason_t\ntransport_lookup_cmd_lun(struct se_cmd *se_cmd)\n{\n\tstruct se_lun *se_lun = NULL;\n\tstruct se_session *se_sess = se_cmd->se_sess;\n\tstruct se_node_acl *nacl = se_sess->se_node_acl;\n\tstruct se_dev_entry *deve;\n\tsense_reason_t ret = TCM_NO_SENSE;\n\n\trcu_read_lock();\n\tdeve = target_nacl_find_deve(nacl, se_cmd->orig_fe_lun);\n\tif (deve) {\n\t\tatomic_long_inc(&deve->total_cmds);\n\n\t\tif (se_cmd->data_direction == DMA_TO_DEVICE)\n\t\t\tatomic_long_add(se_cmd->data_length,\n\t\t\t\t\t&deve->write_bytes);\n\t\telse if (se_cmd->data_direction == DMA_FROM_DEVICE)\n\t\t\tatomic_long_add(se_cmd->data_length,\n\t\t\t\t\t&deve->read_bytes);\n\n\t\tif ((se_cmd->data_direction == DMA_TO_DEVICE) &&\n\t\t    deve->lun_access_ro) {\n\t\t\tpr_err(\"TARGET_CORE[%s]: Detected WRITE_PROTECTED LUN\"\n\t\t\t\t\" Access for 0x%08llx\\n\",\n\t\t\t\tse_cmd->se_tfo->fabric_name,\n\t\t\t\tse_cmd->orig_fe_lun);\n\t\t\trcu_read_unlock();\n\t\t\treturn TCM_WRITE_PROTECTED;\n\t\t}\n\n\t\tse_lun = deve->se_lun;\n\n\t\tif (!percpu_ref_tryget_live(&se_lun->lun_ref)) {\n\t\t\tse_lun = NULL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tse_cmd->se_lun = se_lun;\n\t\tse_cmd->pr_res_key = deve->pr_res_key;\n\t\tse_cmd->se_cmd_flags |= SCF_SE_LUN_CMD;\n\t\tse_cmd->lun_ref_active = true;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\n\tif (!se_lun) {\n\t\t \n\t\tif (se_cmd->orig_fe_lun != 0) {\n\t\t\tpr_err(\"TARGET_CORE[%s]: Detected NON_EXISTENT_LUN\"\n\t\t\t\t\" Access for 0x%08llx from %s\\n\",\n\t\t\t\tse_cmd->se_tfo->fabric_name,\n\t\t\t\tse_cmd->orig_fe_lun,\n\t\t\t\tnacl->initiatorname);\n\t\t\treturn TCM_NON_EXISTENT_LUN;\n\t\t}\n\n\t\t \n\t\tif ((se_cmd->data_direction != DMA_FROM_DEVICE) &&\n\t\t    (se_cmd->data_direction != DMA_NONE))\n\t\t\treturn TCM_WRITE_PROTECTED;\n\n\t\tse_lun = se_sess->se_tpg->tpg_virt_lun0;\n\t\tif (!percpu_ref_tryget_live(&se_lun->lun_ref))\n\t\t\treturn TCM_NON_EXISTENT_LUN;\n\n\t\tse_cmd->se_lun = se_sess->se_tpg->tpg_virt_lun0;\n\t\tse_cmd->se_cmd_flags |= SCF_SE_LUN_CMD;\n\t\tse_cmd->lun_ref_active = true;\n\t}\n\t \n\tse_cmd->se_dev = rcu_dereference_raw(se_lun->lun_se_dev);\n\tatomic_long_inc(&se_cmd->se_dev->num_cmds);\n\n\tif (se_cmd->data_direction == DMA_TO_DEVICE)\n\t\tatomic_long_add(se_cmd->data_length,\n\t\t\t\t&se_cmd->se_dev->write_bytes);\n\telse if (se_cmd->data_direction == DMA_FROM_DEVICE)\n\t\tatomic_long_add(se_cmd->data_length,\n\t\t\t\t&se_cmd->se_dev->read_bytes);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(transport_lookup_cmd_lun);\n\nint transport_lookup_tmr_lun(struct se_cmd *se_cmd)\n{\n\tstruct se_dev_entry *deve;\n\tstruct se_lun *se_lun = NULL;\n\tstruct se_session *se_sess = se_cmd->se_sess;\n\tstruct se_node_acl *nacl = se_sess->se_node_acl;\n\tstruct se_tmr_req *se_tmr = se_cmd->se_tmr_req;\n\tunsigned long flags;\n\n\trcu_read_lock();\n\tdeve = target_nacl_find_deve(nacl, se_cmd->orig_fe_lun);\n\tif (deve) {\n\t\tse_lun = deve->se_lun;\n\n\t\tif (!percpu_ref_tryget_live(&se_lun->lun_ref)) {\n\t\t\tse_lun = NULL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tse_cmd->se_lun = se_lun;\n\t\tse_cmd->pr_res_key = deve->pr_res_key;\n\t\tse_cmd->se_cmd_flags |= SCF_SE_LUN_CMD;\n\t\tse_cmd->lun_ref_active = true;\n\t}\nout_unlock:\n\trcu_read_unlock();\n\n\tif (!se_lun) {\n\t\tpr_debug(\"TARGET_CORE[%s]: Detected NON_EXISTENT_LUN\"\n\t\t\t\" Access for 0x%08llx for %s\\n\",\n\t\t\tse_cmd->se_tfo->fabric_name,\n\t\t\tse_cmd->orig_fe_lun,\n\t\t\tnacl->initiatorname);\n\t\treturn -ENODEV;\n\t}\n\tse_cmd->se_dev = rcu_dereference_raw(se_lun->lun_se_dev);\n\tse_tmr->tmr_dev = rcu_dereference_raw(se_lun->lun_se_dev);\n\n\tspin_lock_irqsave(&se_tmr->tmr_dev->se_tmr_lock, flags);\n\tlist_add_tail(&se_tmr->tmr_list, &se_tmr->tmr_dev->dev_tmr_list);\n\tspin_unlock_irqrestore(&se_tmr->tmr_dev->se_tmr_lock, flags);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(transport_lookup_tmr_lun);\n\nbool target_lun_is_rdonly(struct se_cmd *cmd)\n{\n\tstruct se_session *se_sess = cmd->se_sess;\n\tstruct se_dev_entry *deve;\n\tbool ret;\n\n\trcu_read_lock();\n\tdeve = target_nacl_find_deve(se_sess->se_node_acl, cmd->orig_fe_lun);\n\tret = deve && deve->lun_access_ro;\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL(target_lun_is_rdonly);\n\n \nstruct se_dev_entry *core_get_se_deve_from_rtpi(\n\tstruct se_node_acl *nacl,\n\tu16 rtpi)\n{\n\tstruct se_dev_entry *deve;\n\tstruct se_lun *lun;\n\tstruct se_portal_group *tpg = nacl->se_tpg;\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(deve, &nacl->lun_entry_hlist, link) {\n\t\tlun = deve->se_lun;\n\t\tif (!lun) {\n\t\t\tpr_err(\"%s device entries device pointer is\"\n\t\t\t\t\" NULL, but Initiator has access.\\n\",\n\t\t\t\ttpg->se_tpg_tfo->fabric_name);\n\t\t\tcontinue;\n\t\t}\n\t\tif (lun->lun_tpg->tpg_rtpi != rtpi)\n\t\t\tcontinue;\n\n\t\tkref_get(&deve->pr_kref);\n\t\trcu_read_unlock();\n\n\t\treturn deve;\n\t}\n\trcu_read_unlock();\n\n\treturn NULL;\n}\n\nvoid core_free_device_list_for_node(\n\tstruct se_node_acl *nacl,\n\tstruct se_portal_group *tpg)\n{\n\tstruct se_dev_entry *deve;\n\n\tmutex_lock(&nacl->lun_entry_mutex);\n\thlist_for_each_entry_rcu(deve, &nacl->lun_entry_hlist, link)\n\t\tcore_disable_device_list_for_node(deve->se_lun, deve, nacl, tpg);\n\tmutex_unlock(&nacl->lun_entry_mutex);\n}\n\nvoid core_update_device_list_access(\n\tu64 mapped_lun,\n\tbool lun_access_ro,\n\tstruct se_node_acl *nacl)\n{\n\tstruct se_dev_entry *deve;\n\n\tmutex_lock(&nacl->lun_entry_mutex);\n\tdeve = target_nacl_find_deve(nacl, mapped_lun);\n\tif (deve)\n\t\tdeve->lun_access_ro = lun_access_ro;\n\tmutex_unlock(&nacl->lun_entry_mutex);\n}\n\n \nstruct se_dev_entry *target_nacl_find_deve(struct se_node_acl *nacl, u64 mapped_lun)\n{\n\tstruct se_dev_entry *deve;\n\n\thlist_for_each_entry_rcu(deve, &nacl->lun_entry_hlist, link)\n\t\tif (deve->mapped_lun == mapped_lun)\n\t\t\treturn deve;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(target_nacl_find_deve);\n\nvoid target_pr_kref_release(struct kref *kref)\n{\n\tstruct se_dev_entry *deve = container_of(kref, struct se_dev_entry,\n\t\t\t\t\t\t pr_kref);\n\tcomplete(&deve->pr_comp);\n}\n\n \nvoid target_dev_ua_allocate(struct se_device *dev, u8 asc, u8 ascq)\n{\n\tstruct se_dev_entry *se_deve;\n\tstruct se_lun *lun;\n\n\tspin_lock(&dev->se_port_lock);\n\tlist_for_each_entry(lun, &dev->dev_sep_list, lun_dev_link) {\n\n\t\tspin_lock(&lun->lun_deve_lock);\n\t\tlist_for_each_entry(se_deve, &lun->lun_deve_list, lun_link)\n\t\t\tcore_scsi3_ua_allocate(se_deve, asc, ascq);\n\t\tspin_unlock(&lun->lun_deve_lock);\n\t}\n\tspin_unlock(&dev->se_port_lock);\n}\n\nstatic void\ntarget_luns_data_has_changed(struct se_node_acl *nacl, struct se_dev_entry *new,\n\t\t\t     bool skip_new)\n{\n\tstruct se_dev_entry *tmp;\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(tmp, &nacl->lun_entry_hlist, link) {\n\t\tif (skip_new && tmp == new)\n\t\t\tcontinue;\n\t\tcore_scsi3_ua_allocate(tmp, 0x3F,\n\t\t\t\t       ASCQ_3FH_REPORTED_LUNS_DATA_HAS_CHANGED);\n\t}\n\trcu_read_unlock();\n}\n\nint core_enable_device_list_for_node(\n\tstruct se_lun *lun,\n\tstruct se_lun_acl *lun_acl,\n\tu64 mapped_lun,\n\tbool lun_access_ro,\n\tstruct se_node_acl *nacl,\n\tstruct se_portal_group *tpg)\n{\n\tstruct se_dev_entry *orig, *new;\n\n\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\tif (!new) {\n\t\tpr_err(\"Unable to allocate se_dev_entry memory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock_init(&new->ua_lock);\n\tINIT_LIST_HEAD(&new->ua_list);\n\tINIT_LIST_HEAD(&new->lun_link);\n\n\tnew->mapped_lun = mapped_lun;\n\tkref_init(&new->pr_kref);\n\tinit_completion(&new->pr_comp);\n\n\tnew->lun_access_ro = lun_access_ro;\n\tnew->creation_time = get_jiffies_64();\n\tnew->attach_count++;\n\n\tmutex_lock(&nacl->lun_entry_mutex);\n\torig = target_nacl_find_deve(nacl, mapped_lun);\n\tif (orig && orig->se_lun) {\n\t\tstruct se_lun *orig_lun = orig->se_lun;\n\n\t\tif (orig_lun != lun) {\n\t\t\tpr_err(\"Existing orig->se_lun doesn't match new lun\"\n\t\t\t       \" for dynamic -> explicit NodeACL conversion:\"\n\t\t\t\t\" %s\\n\", nacl->initiatorname);\n\t\t\tmutex_unlock(&nacl->lun_entry_mutex);\n\t\t\tkfree(new);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (orig->se_lun_acl != NULL) {\n\t\t\tpr_warn_ratelimited(\"Detected existing explicit\"\n\t\t\t\t\" se_lun_acl->se_lun_group reference for %s\"\n\t\t\t\t\" mapped_lun: %llu, failing\\n\",\n\t\t\t\t nacl->initiatorname, mapped_lun);\n\t\t\tmutex_unlock(&nacl->lun_entry_mutex);\n\t\t\tkfree(new);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tnew->se_lun = lun;\n\t\tnew->se_lun_acl = lun_acl;\n\t\thlist_del_rcu(&orig->link);\n\t\thlist_add_head_rcu(&new->link, &nacl->lun_entry_hlist);\n\t\tmutex_unlock(&nacl->lun_entry_mutex);\n\n\t\tspin_lock(&lun->lun_deve_lock);\n\t\tlist_del(&orig->lun_link);\n\t\tlist_add_tail(&new->lun_link, &lun->lun_deve_list);\n\t\tspin_unlock(&lun->lun_deve_lock);\n\n\t\tkref_put(&orig->pr_kref, target_pr_kref_release);\n\t\twait_for_completion(&orig->pr_comp);\n\n\t\ttarget_luns_data_has_changed(nacl, new, true);\n\t\tkfree_rcu(orig, rcu_head);\n\t\treturn 0;\n\t}\n\n\tnew->se_lun = lun;\n\tnew->se_lun_acl = lun_acl;\n\thlist_add_head_rcu(&new->link, &nacl->lun_entry_hlist);\n\tmutex_unlock(&nacl->lun_entry_mutex);\n\n\tspin_lock(&lun->lun_deve_lock);\n\tlist_add_tail(&new->lun_link, &lun->lun_deve_list);\n\tspin_unlock(&lun->lun_deve_lock);\n\n\ttarget_luns_data_has_changed(nacl, new, true);\n\treturn 0;\n}\n\nvoid core_disable_device_list_for_node(\n\tstruct se_lun *lun,\n\tstruct se_dev_entry *orig,\n\tstruct se_node_acl *nacl,\n\tstruct se_portal_group *tpg)\n{\n\t \n\tstruct se_device *dev = rcu_dereference_raw(lun->lun_se_dev);\n\n\tlockdep_assert_held(&nacl->lun_entry_mutex);\n\n\t \n\tspin_lock(&lun->lun_deve_lock);\n\tlist_del(&orig->lun_link);\n\tspin_unlock(&lun->lun_deve_lock);\n\t \n\tcore_scsi3_ua_release_all(orig);\n\n\thlist_del_rcu(&orig->link);\n\tclear_bit(DEF_PR_REG_ACTIVE, &orig->deve_flags);\n\torig->lun_access_ro = false;\n\torig->creation_time = 0;\n\torig->attach_count--;\n\t \n\tkref_put(&orig->pr_kref, target_pr_kref_release);\n\twait_for_completion(&orig->pr_comp);\n\n\tkfree_rcu(orig, rcu_head);\n\n\tcore_scsi3_free_pr_reg_from_nacl(dev, nacl);\n\ttarget_luns_data_has_changed(nacl, NULL, false);\n}\n\n \nvoid core_clear_lun_from_tpg(struct se_lun *lun, struct se_portal_group *tpg)\n{\n\tstruct se_node_acl *nacl;\n\tstruct se_dev_entry *deve;\n\n\tmutex_lock(&tpg->acl_node_mutex);\n\tlist_for_each_entry(nacl, &tpg->acl_node_list, acl_list) {\n\n\t\tmutex_lock(&nacl->lun_entry_mutex);\n\t\thlist_for_each_entry_rcu(deve, &nacl->lun_entry_hlist, link) {\n\t\t\tif (lun != deve->se_lun)\n\t\t\t\tcontinue;\n\n\t\t\tcore_disable_device_list_for_node(lun, deve, nacl, tpg);\n\t\t}\n\t\tmutex_unlock(&nacl->lun_entry_mutex);\n\t}\n\tmutex_unlock(&tpg->acl_node_mutex);\n}\n\nstatic void se_release_vpd_for_dev(struct se_device *dev)\n{\n\tstruct t10_vpd *vpd, *vpd_tmp;\n\n\tspin_lock(&dev->t10_wwn.t10_vpd_lock);\n\tlist_for_each_entry_safe(vpd, vpd_tmp,\n\t\t\t&dev->t10_wwn.t10_vpd_list, vpd_list) {\n\t\tlist_del(&vpd->vpd_list);\n\t\tkfree(vpd);\n\t}\n\tspin_unlock(&dev->t10_wwn.t10_vpd_lock);\n}\n\nstatic u32 se_dev_align_max_sectors(u32 max_sectors, u32 block_size)\n{\n\tu32 aligned_max_sectors;\n\tu32 alignment;\n\t \n\talignment = max(1ul, PAGE_SIZE / block_size);\n\taligned_max_sectors = rounddown(max_sectors, alignment);\n\n\tif (max_sectors != aligned_max_sectors)\n\t\tpr_info(\"Rounding down aligned max_sectors from %u to %u\\n\",\n\t\t\tmax_sectors, aligned_max_sectors);\n\n\treturn aligned_max_sectors;\n}\n\nint core_dev_add_lun(\n\tstruct se_portal_group *tpg,\n\tstruct se_device *dev,\n\tstruct se_lun *lun)\n{\n\tint rc;\n\n\trc = core_tpg_add_lun(tpg, lun, false, dev);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tpr_debug(\"%s_TPG[%u]_LUN[%llu] - Activated %s Logical Unit from\"\n\t\t\" CORE HBA: %u\\n\", tpg->se_tpg_tfo->fabric_name,\n\t\ttpg->se_tpg_tfo->tpg_get_tag(tpg), lun->unpacked_lun,\n\t\ttpg->se_tpg_tfo->fabric_name, dev->se_hba->hba_id);\n\t \n\tif (tpg->se_tpg_tfo->tpg_check_demo_mode(tpg)) {\n\t\tstruct se_node_acl *acl;\n\n\t\tmutex_lock(&tpg->acl_node_mutex);\n\t\tlist_for_each_entry(acl, &tpg->acl_node_list, acl_list) {\n\t\t\tif (acl->dynamic_node_acl &&\n\t\t\t    (!tpg->se_tpg_tfo->tpg_check_demo_mode_login_only ||\n\t\t\t     !tpg->se_tpg_tfo->tpg_check_demo_mode_login_only(tpg))) {\n\t\t\t\tcore_tpg_add_node_to_devs(acl, tpg, lun);\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&tpg->acl_node_mutex);\n\t}\n\n\treturn 0;\n}\n\n \nvoid core_dev_del_lun(\n\tstruct se_portal_group *tpg,\n\tstruct se_lun *lun)\n{\n\tpr_debug(\"%s_TPG[%u]_LUN[%llu] - Deactivating %s Logical Unit from\"\n\t\t\" device object\\n\", tpg->se_tpg_tfo->fabric_name,\n\t\ttpg->se_tpg_tfo->tpg_get_tag(tpg), lun->unpacked_lun,\n\t\ttpg->se_tpg_tfo->fabric_name);\n\n\tcore_tpg_remove_lun(tpg, lun);\n}\n\nstruct se_lun_acl *core_dev_init_initiator_node_lun_acl(\n\tstruct se_portal_group *tpg,\n\tstruct se_node_acl *nacl,\n\tu64 mapped_lun,\n\tint *ret)\n{\n\tstruct se_lun_acl *lacl;\n\n\tif (strlen(nacl->initiatorname) >= TRANSPORT_IQN_LEN) {\n\t\tpr_err(\"%s InitiatorName exceeds maximum size.\\n\",\n\t\t\ttpg->se_tpg_tfo->fabric_name);\n\t\t*ret = -EOVERFLOW;\n\t\treturn NULL;\n\t}\n\tlacl = kzalloc(sizeof(struct se_lun_acl), GFP_KERNEL);\n\tif (!lacl) {\n\t\tpr_err(\"Unable to allocate memory for struct se_lun_acl.\\n\");\n\t\t*ret = -ENOMEM;\n\t\treturn NULL;\n\t}\n\n\tlacl->mapped_lun = mapped_lun;\n\tlacl->se_lun_nacl = nacl;\n\n\treturn lacl;\n}\n\nint core_dev_add_initiator_node_lun_acl(\n\tstruct se_portal_group *tpg,\n\tstruct se_lun_acl *lacl,\n\tstruct se_lun *lun,\n\tbool lun_access_ro)\n{\n\tstruct se_node_acl *nacl = lacl->se_lun_nacl;\n\t \n\tstruct se_device *dev = rcu_dereference_raw(lun->lun_se_dev);\n\n\tif (!nacl)\n\t\treturn -EINVAL;\n\n\tif (lun->lun_access_ro)\n\t\tlun_access_ro = true;\n\n\tlacl->se_lun = lun;\n\n\tif (core_enable_device_list_for_node(lun, lacl, lacl->mapped_lun,\n\t\t\tlun_access_ro, nacl, tpg) < 0)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"%s_TPG[%hu]_LUN[%llu->%llu] - Added %s ACL for \"\n\t\t\" InitiatorNode: %s\\n\", tpg->se_tpg_tfo->fabric_name,\n\t\ttpg->se_tpg_tfo->tpg_get_tag(tpg), lun->unpacked_lun, lacl->mapped_lun,\n\t\tlun_access_ro ? \"RO\" : \"RW\",\n\t\tnacl->initiatorname);\n\t \n\tcore_scsi3_check_aptpl_registration(dev, tpg, lun, nacl,\n\t\t\t\t\t    lacl->mapped_lun);\n\treturn 0;\n}\n\nint core_dev_del_initiator_node_lun_acl(\n\tstruct se_lun *lun,\n\tstruct se_lun_acl *lacl)\n{\n\tstruct se_portal_group *tpg = lun->lun_tpg;\n\tstruct se_node_acl *nacl;\n\tstruct se_dev_entry *deve;\n\n\tnacl = lacl->se_lun_nacl;\n\tif (!nacl)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&nacl->lun_entry_mutex);\n\tdeve = target_nacl_find_deve(nacl, lacl->mapped_lun);\n\tif (deve)\n\t\tcore_disable_device_list_for_node(lun, deve, nacl, tpg);\n\tmutex_unlock(&nacl->lun_entry_mutex);\n\n\tpr_debug(\"%s_TPG[%hu]_LUN[%llu] - Removed ACL for\"\n\t\t\" InitiatorNode: %s Mapped LUN: %llu\\n\",\n\t\ttpg->se_tpg_tfo->fabric_name,\n\t\ttpg->se_tpg_tfo->tpg_get_tag(tpg), lun->unpacked_lun,\n\t\tnacl->initiatorname, lacl->mapped_lun);\n\n\treturn 0;\n}\n\nvoid core_dev_free_initiator_node_lun_acl(\n\tstruct se_portal_group *tpg,\n\tstruct se_lun_acl *lacl)\n{\n\tpr_debug(\"%s_TPG[%hu] - Freeing ACL for %s InitiatorNode: %s\"\n\t\t\" Mapped LUN: %llu\\n\", tpg->se_tpg_tfo->fabric_name,\n\t\ttpg->se_tpg_tfo->tpg_get_tag(tpg),\n\t\ttpg->se_tpg_tfo->fabric_name,\n\t\tlacl->se_lun_nacl->initiatorname, lacl->mapped_lun);\n\n\tkfree(lacl);\n}\n\nstatic void scsi_dump_inquiry(struct se_device *dev)\n{\n\tstruct t10_wwn *wwn = &dev->t10_wwn;\n\tint device_type = dev->transport->get_device_type(dev);\n\n\t \n\tpr_debug(\"  Vendor: %-\" __stringify(INQUIRY_VENDOR_LEN) \"s\\n\",\n\t\twwn->vendor);\n\tpr_debug(\"  Model: %-\" __stringify(INQUIRY_MODEL_LEN) \"s\\n\",\n\t\twwn->model);\n\tpr_debug(\"  Revision: %-\" __stringify(INQUIRY_REVISION_LEN) \"s\\n\",\n\t\twwn->revision);\n\tpr_debug(\"  Type:   %s \", scsi_device_type(device_type));\n}\n\nstruct se_device *target_alloc_device(struct se_hba *hba, const char *name)\n{\n\tstruct se_device *dev;\n\tstruct se_lun *xcopy_lun;\n\tint i;\n\n\tdev = hba->backend->ops->alloc_device(hba, name);\n\tif (!dev)\n\t\treturn NULL;\n\n\tdev->queues = kcalloc(nr_cpu_ids, sizeof(*dev->queues), GFP_KERNEL);\n\tif (!dev->queues) {\n\t\tdev->transport->free_device(dev);\n\t\treturn NULL;\n\t}\n\n\tdev->queue_cnt = nr_cpu_ids;\n\tfor (i = 0; i < dev->queue_cnt; i++) {\n\t\tstruct se_device_queue *q;\n\n\t\tq = &dev->queues[i];\n\t\tINIT_LIST_HEAD(&q->state_list);\n\t\tspin_lock_init(&q->lock);\n\n\t\tinit_llist_head(&q->sq.cmd_list);\n\t\tINIT_WORK(&q->sq.work, target_queued_submit_work);\n\t}\n\n\tdev->se_hba = hba;\n\tdev->transport = hba->backend->ops;\n\tdev->transport_flags = dev->transport->transport_flags_default;\n\tdev->prot_length = sizeof(struct t10_pi_tuple);\n\tdev->hba_index = hba->hba_index;\n\n\tINIT_LIST_HEAD(&dev->dev_sep_list);\n\tINIT_LIST_HEAD(&dev->dev_tmr_list);\n\tINIT_LIST_HEAD(&dev->delayed_cmd_list);\n\tINIT_LIST_HEAD(&dev->qf_cmd_list);\n\tspin_lock_init(&dev->delayed_cmd_lock);\n\tspin_lock_init(&dev->dev_reservation_lock);\n\tspin_lock_init(&dev->se_port_lock);\n\tspin_lock_init(&dev->se_tmr_lock);\n\tspin_lock_init(&dev->qf_cmd_lock);\n\tsema_init(&dev->caw_sem, 1);\n\tINIT_LIST_HEAD(&dev->t10_wwn.t10_vpd_list);\n\tspin_lock_init(&dev->t10_wwn.t10_vpd_lock);\n\tINIT_LIST_HEAD(&dev->t10_pr.registration_list);\n\tINIT_LIST_HEAD(&dev->t10_pr.aptpl_reg_list);\n\tspin_lock_init(&dev->t10_pr.registration_lock);\n\tspin_lock_init(&dev->t10_pr.aptpl_reg_lock);\n\tINIT_LIST_HEAD(&dev->t10_alua.tg_pt_gps_list);\n\tspin_lock_init(&dev->t10_alua.tg_pt_gps_lock);\n\tINIT_LIST_HEAD(&dev->t10_alua.lba_map_list);\n\tspin_lock_init(&dev->t10_alua.lba_map_lock);\n\n\tINIT_WORK(&dev->delayed_cmd_work, target_do_delayed_work);\n\tmutex_init(&dev->lun_reset_mutex);\n\n\tdev->t10_wwn.t10_dev = dev;\n\t \n\tdev->t10_wwn.company_id = 0x001405;\n\n\tdev->t10_alua.t10_dev = dev;\n\n\tdev->dev_attrib.da_dev = dev;\n\tdev->dev_attrib.emulate_model_alias = DA_EMULATE_MODEL_ALIAS;\n\tdev->dev_attrib.emulate_dpo = 1;\n\tdev->dev_attrib.emulate_fua_write = 1;\n\tdev->dev_attrib.emulate_fua_read = 1;\n\tdev->dev_attrib.emulate_write_cache = DA_EMULATE_WRITE_CACHE;\n\tdev->dev_attrib.emulate_ua_intlck_ctrl = TARGET_UA_INTLCK_CTRL_CLEAR;\n\tdev->dev_attrib.emulate_tas = DA_EMULATE_TAS;\n\tdev->dev_attrib.emulate_tpu = DA_EMULATE_TPU;\n\tdev->dev_attrib.emulate_tpws = DA_EMULATE_TPWS;\n\tdev->dev_attrib.emulate_caw = DA_EMULATE_CAW;\n\tdev->dev_attrib.emulate_3pc = DA_EMULATE_3PC;\n\tdev->dev_attrib.emulate_pr = DA_EMULATE_PR;\n\tdev->dev_attrib.emulate_rsoc = DA_EMULATE_RSOC;\n\tdev->dev_attrib.pi_prot_type = TARGET_DIF_TYPE0_PROT;\n\tdev->dev_attrib.enforce_pr_isids = DA_ENFORCE_PR_ISIDS;\n\tdev->dev_attrib.force_pr_aptpl = DA_FORCE_PR_APTPL;\n\tdev->dev_attrib.is_nonrot = DA_IS_NONROT;\n\tdev->dev_attrib.emulate_rest_reord = DA_EMULATE_REST_REORD;\n\tdev->dev_attrib.max_unmap_lba_count = DA_MAX_UNMAP_LBA_COUNT;\n\tdev->dev_attrib.max_unmap_block_desc_count =\n\t\tDA_MAX_UNMAP_BLOCK_DESC_COUNT;\n\tdev->dev_attrib.unmap_granularity = DA_UNMAP_GRANULARITY_DEFAULT;\n\tdev->dev_attrib.unmap_granularity_alignment =\n\t\t\t\tDA_UNMAP_GRANULARITY_ALIGNMENT_DEFAULT;\n\tdev->dev_attrib.unmap_zeroes_data =\n\t\t\t\tDA_UNMAP_ZEROES_DATA_DEFAULT;\n\tdev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN;\n\n\txcopy_lun = &dev->xcopy_lun;\n\trcu_assign_pointer(xcopy_lun->lun_se_dev, dev);\n\tinit_completion(&xcopy_lun->lun_shutdown_comp);\n\tINIT_LIST_HEAD(&xcopy_lun->lun_deve_list);\n\tINIT_LIST_HEAD(&xcopy_lun->lun_dev_link);\n\tmutex_init(&xcopy_lun->lun_tg_pt_md_mutex);\n\txcopy_lun->lun_tpg = &xcopy_pt_tpg;\n\n\t \n\tstrscpy(dev->t10_wwn.vendor, \"LIO-ORG\", sizeof(dev->t10_wwn.vendor));\n\tstrscpy(dev->t10_wwn.model, dev->transport->inquiry_prod,\n\t\tsizeof(dev->t10_wwn.model));\n\tstrscpy(dev->t10_wwn.revision, dev->transport->inquiry_rev,\n\t\tsizeof(dev->t10_wwn.revision));\n\n\treturn dev;\n}\n\n \nbool target_configure_unmap_from_queue(struct se_dev_attrib *attrib,\n\t\t\t\t       struct block_device *bdev)\n{\n\tint block_size = bdev_logical_block_size(bdev);\n\n\tif (!bdev_max_discard_sectors(bdev))\n\t\treturn false;\n\n\tattrib->max_unmap_lba_count =\n\t\tbdev_max_discard_sectors(bdev) >> (ilog2(block_size) - 9);\n\t \n\tattrib->max_unmap_block_desc_count = 1;\n\tattrib->unmap_granularity = bdev_discard_granularity(bdev) / block_size;\n\tattrib->unmap_granularity_alignment =\n\t\tbdev_discard_alignment(bdev) / block_size;\n\treturn true;\n}\nEXPORT_SYMBOL(target_configure_unmap_from_queue);\n\n \nsector_t target_to_linux_sector(struct se_device *dev, sector_t lb)\n{\n\tswitch (dev->dev_attrib.block_size) {\n\tcase 4096:\n\t\treturn lb << 3;\n\tcase 2048:\n\t\treturn lb << 2;\n\tcase 1024:\n\t\treturn lb << 1;\n\tdefault:\n\t\treturn lb;\n\t}\n}\nEXPORT_SYMBOL(target_to_linux_sector);\n\nstruct devices_idr_iter {\n\tint (*fn)(struct se_device *dev, void *data);\n\tvoid *data;\n};\n\nstatic int target_devices_idr_iter(int id, void *p, void *data)\n\t __must_hold(&device_mutex)\n{\n\tstruct devices_idr_iter *iter = data;\n\tstruct se_device *dev = p;\n\tstruct config_item *item;\n\tint ret;\n\n\t \n\tif (!target_dev_configured(dev))\n\t\treturn 0;\n\n\titem = config_item_get_unless_zero(&dev->dev_group.cg_item);\n\tif (!item)\n\t\treturn 0;\n\tmutex_unlock(&device_mutex);\n\n\tret = iter->fn(dev, iter->data);\n\tconfig_item_put(item);\n\n\tmutex_lock(&device_mutex);\n\treturn ret;\n}\n\n \nint target_for_each_device(int (*fn)(struct se_device *dev, void *data),\n\t\t\t   void *data)\n{\n\tstruct devices_idr_iter iter = { .fn = fn, .data = data };\n\tint ret;\n\n\tmutex_lock(&device_mutex);\n\tret = idr_for_each(&devices_idr, target_devices_idr_iter, &iter);\n\tmutex_unlock(&device_mutex);\n\treturn ret;\n}\n\nint target_configure_device(struct se_device *dev)\n{\n\tstruct se_hba *hba = dev->se_hba;\n\tint ret, id;\n\n\tif (target_dev_configured(dev)) {\n\t\tpr_err(\"se_dev->se_dev_ptr already set for storage\"\n\t\t\t\t\" object\\n\");\n\t\treturn -EEXIST;\n\t}\n\n\t \n\tmutex_lock(&device_mutex);\n\t \n\tid = idr_alloc_cyclic(&devices_idr, dev, 0, INT_MAX, GFP_KERNEL);\n\tmutex_unlock(&device_mutex);\n\tif (id < 0) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tdev->dev_index = id;\n\n\tret = dev->transport->configure_device(dev);\n\tif (ret)\n\t\tgoto out_free_index;\n\n\tif (dev->transport->configure_unmap &&\n\t    dev->transport->configure_unmap(dev)) {\n\t\tpr_debug(\"Discard support available, but disabled by default.\\n\");\n\t}\n\n\t \n\tdev->dev_attrib.block_size = dev->dev_attrib.hw_block_size;\n\tdev->dev_attrib.queue_depth = dev->dev_attrib.hw_queue_depth;\n\n\t \n\tdev->dev_attrib.hw_max_sectors =\n\t\tse_dev_align_max_sectors(dev->dev_attrib.hw_max_sectors,\n\t\t\t\t\t dev->dev_attrib.hw_block_size);\n\tdev->dev_attrib.optimal_sectors = dev->dev_attrib.hw_max_sectors;\n\n\tdev->creation_time = get_jiffies_64();\n\n\tret = core_setup_alua(dev);\n\tif (ret)\n\t\tgoto out_destroy_device;\n\n\t \n\tINIT_WORK(&dev->qf_work_queue, target_qf_do_work);\n\n\tscsi_dump_inquiry(dev);\n\n\tspin_lock(&hba->device_lock);\n\thba->dev_count++;\n\tspin_unlock(&hba->device_lock);\n\n\tdev->dev_flags |= DF_CONFIGURED;\n\n\treturn 0;\n\nout_destroy_device:\n\tdev->transport->destroy_device(dev);\nout_free_index:\n\tmutex_lock(&device_mutex);\n\tidr_remove(&devices_idr, dev->dev_index);\n\tmutex_unlock(&device_mutex);\nout:\n\tse_release_vpd_for_dev(dev);\n\treturn ret;\n}\n\nvoid target_free_device(struct se_device *dev)\n{\n\tstruct se_hba *hba = dev->se_hba;\n\n\tWARN_ON(!list_empty(&dev->dev_sep_list));\n\n\tif (target_dev_configured(dev)) {\n\t\tdev->transport->destroy_device(dev);\n\n\t\tmutex_lock(&device_mutex);\n\t\tidr_remove(&devices_idr, dev->dev_index);\n\t\tmutex_unlock(&device_mutex);\n\n\t\tspin_lock(&hba->device_lock);\n\t\thba->dev_count--;\n\t\tspin_unlock(&hba->device_lock);\n\t}\n\n\tcore_alua_free_lu_gp_mem(dev);\n\tcore_alua_set_lba_map(dev, NULL, 0, 0);\n\tcore_scsi3_free_all_registrations(dev);\n\tse_release_vpd_for_dev(dev);\n\n\tif (dev->transport->free_prot)\n\t\tdev->transport->free_prot(dev);\n\n\tkfree(dev->queues);\n\tdev->transport->free_device(dev);\n}\n\nint core_dev_setup_virtual_lun0(void)\n{\n\tstruct se_hba *hba;\n\tstruct se_device *dev;\n\tchar buf[] = \"rd_pages=8,rd_nullio=1,rd_dummy=1\";\n\tint ret;\n\n\thba = core_alloc_hba(\"rd_mcp\", 0, HBA_FLAGS_INTERNAL_USE);\n\tif (IS_ERR(hba))\n\t\treturn PTR_ERR(hba);\n\n\tdev = target_alloc_device(hba, \"virt_lun0\");\n\tif (!dev) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_hba;\n\t}\n\n\thba->backend->ops->set_configfs_dev_params(dev, buf, sizeof(buf));\n\n\tret = target_configure_device(dev);\n\tif (ret)\n\t\tgoto out_free_se_dev;\n\n\tlun0_hba = hba;\n\tg_lun0_dev = dev;\n\treturn 0;\n\nout_free_se_dev:\n\ttarget_free_device(dev);\nout_free_hba:\n\tcore_delete_hba(hba);\n\treturn ret;\n}\n\n\nvoid core_dev_release_virtual_lun0(void)\n{\n\tstruct se_hba *hba = lun0_hba;\n\n\tif (!hba)\n\t\treturn;\n\n\tif (g_lun0_dev)\n\t\ttarget_free_device(g_lun0_dev);\n\tcore_delete_hba(hba);\n}\n\n \nsense_reason_t\npassthrough_parse_cdb(struct se_cmd *cmd,\n\tsense_reason_t (*exec_cmd)(struct se_cmd *cmd))\n{\n\tunsigned char *cdb = cmd->t_task_cdb;\n\tstruct se_device *dev = cmd->se_dev;\n\tunsigned int size;\n\n\t \n\tif (cdb[0] == REPORT_LUNS) {\n\t\tcmd->execute_cmd = spc_emulate_report_luns;\n\t\treturn TCM_NO_SENSE;\n\t}\n\n\t \n\tif (!dev->dev_attrib.emulate_pr &&\n\t    ((cdb[0] == PERSISTENT_RESERVE_IN) ||\n\t     (cdb[0] == PERSISTENT_RESERVE_OUT) ||\n\t     (cdb[0] == RELEASE || cdb[0] == RELEASE_10) ||\n\t     (cdb[0] == RESERVE || cdb[0] == RESERVE_10))) {\n\t\treturn TCM_UNSUPPORTED_SCSI_OPCODE;\n\t}\n\n\t \n\tif (!(dev->transport_flags &\n\t      TRANSPORT_FLAG_PASSTHROUGH_PGR)) {\n\t\tif (cdb[0] == PERSISTENT_RESERVE_IN) {\n\t\t\tcmd->execute_cmd = target_scsi3_emulate_pr_in;\n\t\t\tsize = get_unaligned_be16(&cdb[7]);\n\t\t\treturn target_cmd_size_check(cmd, size);\n\t\t}\n\t\tif (cdb[0] == PERSISTENT_RESERVE_OUT) {\n\t\t\tcmd->execute_cmd = target_scsi3_emulate_pr_out;\n\t\t\tsize = get_unaligned_be32(&cdb[5]);\n\t\t\treturn target_cmd_size_check(cmd, size);\n\t\t}\n\n\t\tif (cdb[0] == RELEASE || cdb[0] == RELEASE_10) {\n\t\t\tcmd->execute_cmd = target_scsi2_reservation_release;\n\t\t\tif (cdb[0] == RELEASE_10)\n\t\t\t\tsize = get_unaligned_be16(&cdb[7]);\n\t\t\telse\n\t\t\t\tsize = cmd->data_length;\n\t\t\treturn target_cmd_size_check(cmd, size);\n\t\t}\n\t\tif (cdb[0] == RESERVE || cdb[0] == RESERVE_10) {\n\t\t\tcmd->execute_cmd = target_scsi2_reservation_reserve;\n\t\t\tif (cdb[0] == RESERVE_10)\n\t\t\t\tsize = get_unaligned_be16(&cdb[7]);\n\t\t\telse\n\t\t\t\tsize = cmd->data_length;\n\t\t\treturn target_cmd_size_check(cmd, size);\n\t\t}\n\t}\n\n\t \n\tswitch (cdb[0]) {\n\tcase READ_6:\n\tcase READ_10:\n\tcase READ_12:\n\tcase READ_16:\n\tcase WRITE_6:\n\tcase WRITE_10:\n\tcase WRITE_12:\n\tcase WRITE_16:\n\tcase WRITE_VERIFY:\n\tcase WRITE_VERIFY_12:\n\tcase WRITE_VERIFY_16:\n\tcase COMPARE_AND_WRITE:\n\tcase XDWRITEREAD_10:\n\t\tcmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;\n\t\tbreak;\n\tcase VARIABLE_LENGTH_CMD:\n\t\tswitch (get_unaligned_be16(&cdb[8])) {\n\t\tcase READ_32:\n\t\tcase WRITE_32:\n\t\tcase WRITE_VERIFY_32:\n\t\tcase XDWRITEREAD_32:\n\t\t\tcmd->se_cmd_flags |= SCF_SCSI_DATA_CDB;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tcmd->execute_cmd = exec_cmd;\n\n\treturn TCM_NO_SENSE;\n}\nEXPORT_SYMBOL(passthrough_parse_cdb);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}