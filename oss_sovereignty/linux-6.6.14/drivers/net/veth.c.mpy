{
  "module_name": "veth.c",
  "hash_id": "44b5ce04a0af72cb2f573fd09ac5c773cd77d2da80943d8ad22ab5a792ddd59c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/veth.c",
  "human_readable_source": "\n \n\n#include <linux/netdevice.h>\n#include <linux/slab.h>\n#include <linux/ethtool.h>\n#include <linux/etherdevice.h>\n#include <linux/u64_stats_sync.h>\n\n#include <net/rtnetlink.h>\n#include <net/dst.h>\n#include <net/xfrm.h>\n#include <net/xdp.h>\n#include <linux/veth.h>\n#include <linux/module.h>\n#include <linux/bpf.h>\n#include <linux/filter.h>\n#include <linux/ptr_ring.h>\n#include <linux/bpf_trace.h>\n#include <linux/net_tstamp.h>\n#include <net/page_pool/helpers.h>\n\n#define DRV_NAME\t\"veth\"\n#define DRV_VERSION\t\"1.0\"\n\n#define VETH_XDP_FLAG\t\tBIT(0)\n#define VETH_RING_SIZE\t\t256\n#define VETH_XDP_HEADROOM\t(XDP_PACKET_HEADROOM + NET_IP_ALIGN)\n\n#define VETH_XDP_TX_BULK_SIZE\t16\n#define VETH_XDP_BATCH\t\t16\n\nstruct veth_stats {\n\tu64\trx_drops;\n\t \n\tu64\txdp_packets;\n\tu64\txdp_bytes;\n\tu64\txdp_redirect;\n\tu64\txdp_drops;\n\tu64\txdp_tx;\n\tu64\txdp_tx_err;\n\tu64\tpeer_tq_xdp_xmit;\n\tu64\tpeer_tq_xdp_xmit_err;\n};\n\nstruct veth_rq_stats {\n\tstruct veth_stats\tvs;\n\tstruct u64_stats_sync\tsyncp;\n};\n\nstruct veth_rq {\n\tstruct napi_struct\txdp_napi;\n\tstruct napi_struct __rcu *napi;  \n\tstruct net_device\t*dev;\n\tstruct bpf_prog __rcu\t*xdp_prog;\n\tstruct xdp_mem_info\txdp_mem;\n\tstruct veth_rq_stats\tstats;\n\tbool\t\t\trx_notify_masked;\n\tstruct ptr_ring\t\txdp_ring;\n\tstruct xdp_rxq_info\txdp_rxq;\n\tstruct page_pool\t*page_pool;\n};\n\nstruct veth_priv {\n\tstruct net_device __rcu\t*peer;\n\tatomic64_t\t\tdropped;\n\tstruct bpf_prog\t\t*_xdp_prog;\n\tstruct veth_rq\t\t*rq;\n\tunsigned int\t\trequested_headroom;\n};\n\nstruct veth_xdp_tx_bq {\n\tstruct xdp_frame *q[VETH_XDP_TX_BULK_SIZE];\n\tunsigned int count;\n};\n\n \n\nstruct veth_q_stat_desc {\n\tchar\tdesc[ETH_GSTRING_LEN];\n\tsize_t\toffset;\n};\n\n#define VETH_RQ_STAT(m)\toffsetof(struct veth_stats, m)\n\nstatic const struct veth_q_stat_desc veth_rq_stats_desc[] = {\n\t{ \"xdp_packets\",\tVETH_RQ_STAT(xdp_packets) },\n\t{ \"xdp_bytes\",\t\tVETH_RQ_STAT(xdp_bytes) },\n\t{ \"drops\",\t\tVETH_RQ_STAT(rx_drops) },\n\t{ \"xdp_redirect\",\tVETH_RQ_STAT(xdp_redirect) },\n\t{ \"xdp_drops\",\t\tVETH_RQ_STAT(xdp_drops) },\n\t{ \"xdp_tx\",\t\tVETH_RQ_STAT(xdp_tx) },\n\t{ \"xdp_tx_errors\",\tVETH_RQ_STAT(xdp_tx_err) },\n};\n\n#define VETH_RQ_STATS_LEN\tARRAY_SIZE(veth_rq_stats_desc)\n\nstatic const struct veth_q_stat_desc veth_tq_stats_desc[] = {\n\t{ \"xdp_xmit\",\t\tVETH_RQ_STAT(peer_tq_xdp_xmit) },\n\t{ \"xdp_xmit_errors\",\tVETH_RQ_STAT(peer_tq_xdp_xmit_err) },\n};\n\n#define VETH_TQ_STATS_LEN\tARRAY_SIZE(veth_tq_stats_desc)\n\nstatic struct {\n\tconst char string[ETH_GSTRING_LEN];\n} ethtool_stats_keys[] = {\n\t{ \"peer_ifindex\" },\n};\n\nstruct veth_xdp_buff {\n\tstruct xdp_buff xdp;\n\tstruct sk_buff *skb;\n};\n\nstatic int veth_get_link_ksettings(struct net_device *dev,\n\t\t\t\t   struct ethtool_link_ksettings *cmd)\n{\n\tcmd->base.speed\t\t= SPEED_10000;\n\tcmd->base.duplex\t= DUPLEX_FULL;\n\tcmd->base.port\t\t= PORT_TP;\n\tcmd->base.autoneg\t= AUTONEG_DISABLE;\n\treturn 0;\n}\n\nstatic void veth_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\n{\n\tstrscpy(info->driver, DRV_NAME, sizeof(info->driver));\n\tstrscpy(info->version, DRV_VERSION, sizeof(info->version));\n}\n\nstatic void veth_get_strings(struct net_device *dev, u32 stringset, u8 *buf)\n{\n\tu8 *p = buf;\n\tint i, j;\n\n\tswitch(stringset) {\n\tcase ETH_SS_STATS:\n\t\tmemcpy(p, &ethtool_stats_keys, sizeof(ethtool_stats_keys));\n\t\tp += sizeof(ethtool_stats_keys);\n\t\tfor (i = 0; i < dev->real_num_rx_queues; i++)\n\t\t\tfor (j = 0; j < VETH_RQ_STATS_LEN; j++)\n\t\t\t\tethtool_sprintf(&p, \"rx_queue_%u_%.18s\",\n\t\t\t\t\t\ti, veth_rq_stats_desc[j].desc);\n\n\t\tfor (i = 0; i < dev->real_num_tx_queues; i++)\n\t\t\tfor (j = 0; j < VETH_TQ_STATS_LEN; j++)\n\t\t\t\tethtool_sprintf(&p, \"tx_queue_%u_%.18s\",\n\t\t\t\t\t\ti, veth_tq_stats_desc[j].desc);\n\n\t\tpage_pool_ethtool_stats_get_strings(p);\n\t\tbreak;\n\t}\n}\n\nstatic int veth_get_sset_count(struct net_device *dev, int sset)\n{\n\tswitch (sset) {\n\tcase ETH_SS_STATS:\n\t\treturn ARRAY_SIZE(ethtool_stats_keys) +\n\t\t       VETH_RQ_STATS_LEN * dev->real_num_rx_queues +\n\t\t       VETH_TQ_STATS_LEN * dev->real_num_tx_queues +\n\t\t       page_pool_ethtool_stats_get_count();\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic void veth_get_page_pool_stats(struct net_device *dev, u64 *data)\n{\n#ifdef CONFIG_PAGE_POOL_STATS\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct page_pool_stats pp_stats = {};\n\tint i;\n\n\tfor (i = 0; i < dev->real_num_rx_queues; i++) {\n\t\tif (!priv->rq[i].page_pool)\n\t\t\tcontinue;\n\t\tpage_pool_get_stats(priv->rq[i].page_pool, &pp_stats);\n\t}\n\tpage_pool_ethtool_stats_get(data, &pp_stats);\n#endif  \n}\n\nstatic void veth_get_ethtool_stats(struct net_device *dev,\n\t\tstruct ethtool_stats *stats, u64 *data)\n{\n\tstruct veth_priv *rcv_priv, *priv = netdev_priv(dev);\n\tstruct net_device *peer = rtnl_dereference(priv->peer);\n\tint i, j, idx, pp_idx;\n\n\tdata[0] = peer ? peer->ifindex : 0;\n\tidx = 1;\n\tfor (i = 0; i < dev->real_num_rx_queues; i++) {\n\t\tconst struct veth_rq_stats *rq_stats = &priv->rq[i].stats;\n\t\tconst void *stats_base = (void *)&rq_stats->vs;\n\t\tunsigned int start;\n\t\tsize_t offset;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&rq_stats->syncp);\n\t\t\tfor (j = 0; j < VETH_RQ_STATS_LEN; j++) {\n\t\t\t\toffset = veth_rq_stats_desc[j].offset;\n\t\t\t\tdata[idx + j] = *(u64 *)(stats_base + offset);\n\t\t\t}\n\t\t} while (u64_stats_fetch_retry(&rq_stats->syncp, start));\n\t\tidx += VETH_RQ_STATS_LEN;\n\t}\n\tpp_idx = idx;\n\n\tif (!peer)\n\t\tgoto page_pool_stats;\n\n\trcv_priv = netdev_priv(peer);\n\tfor (i = 0; i < peer->real_num_rx_queues; i++) {\n\t\tconst struct veth_rq_stats *rq_stats = &rcv_priv->rq[i].stats;\n\t\tconst void *base = (void *)&rq_stats->vs;\n\t\tunsigned int start, tx_idx = idx;\n\t\tsize_t offset;\n\n\t\ttx_idx += (i % dev->real_num_tx_queues) * VETH_TQ_STATS_LEN;\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&rq_stats->syncp);\n\t\t\tfor (j = 0; j < VETH_TQ_STATS_LEN; j++) {\n\t\t\t\toffset = veth_tq_stats_desc[j].offset;\n\t\t\t\tdata[tx_idx + j] += *(u64 *)(base + offset);\n\t\t\t}\n\t\t} while (u64_stats_fetch_retry(&rq_stats->syncp, start));\n\t}\n\tpp_idx = idx + dev->real_num_tx_queues * VETH_TQ_STATS_LEN;\n\npage_pool_stats:\n\tveth_get_page_pool_stats(dev, &data[pp_idx]);\n}\n\nstatic void veth_get_channels(struct net_device *dev,\n\t\t\t      struct ethtool_channels *channels)\n{\n\tchannels->tx_count = dev->real_num_tx_queues;\n\tchannels->rx_count = dev->real_num_rx_queues;\n\tchannels->max_tx = dev->num_tx_queues;\n\tchannels->max_rx = dev->num_rx_queues;\n}\n\nstatic int veth_set_channels(struct net_device *dev,\n\t\t\t     struct ethtool_channels *ch);\n\nstatic const struct ethtool_ops veth_ethtool_ops = {\n\t.get_drvinfo\t\t= veth_get_drvinfo,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_strings\t\t= veth_get_strings,\n\t.get_sset_count\t\t= veth_get_sset_count,\n\t.get_ethtool_stats\t= veth_get_ethtool_stats,\n\t.get_link_ksettings\t= veth_get_link_ksettings,\n\t.get_ts_info\t\t= ethtool_op_get_ts_info,\n\t.get_channels\t\t= veth_get_channels,\n\t.set_channels\t\t= veth_set_channels,\n};\n\n \n\nstatic bool veth_is_xdp_frame(void *ptr)\n{\n\treturn (unsigned long)ptr & VETH_XDP_FLAG;\n}\n\nstatic struct xdp_frame *veth_ptr_to_xdp(void *ptr)\n{\n\treturn (void *)((unsigned long)ptr & ~VETH_XDP_FLAG);\n}\n\nstatic void *veth_xdp_to_ptr(struct xdp_frame *xdp)\n{\n\treturn (void *)((unsigned long)xdp | VETH_XDP_FLAG);\n}\n\nstatic void veth_ptr_free(void *ptr)\n{\n\tif (veth_is_xdp_frame(ptr))\n\t\txdp_return_frame(veth_ptr_to_xdp(ptr));\n\telse\n\t\tkfree_skb(ptr);\n}\n\nstatic void __veth_xdp_flush(struct veth_rq *rq)\n{\n\t \n\tsmp_mb();\n\tif (!READ_ONCE(rq->rx_notify_masked) &&\n\t    napi_schedule_prep(&rq->xdp_napi)) {\n\t\tWRITE_ONCE(rq->rx_notify_masked, true);\n\t\t__napi_schedule(&rq->xdp_napi);\n\t}\n}\n\nstatic int veth_xdp_rx(struct veth_rq *rq, struct sk_buff *skb)\n{\n\tif (unlikely(ptr_ring_produce(&rq->xdp_ring, skb))) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\treturn NET_RX_SUCCESS;\n}\n\nstatic int veth_forward_skb(struct net_device *dev, struct sk_buff *skb,\n\t\t\t    struct veth_rq *rq, bool xdp)\n{\n\treturn __dev_forward_skb(dev, skb) ?: xdp ?\n\t\tveth_xdp_rx(rq, skb) :\n\t\t__netif_rx(skb);\n}\n\n \nstatic bool veth_skb_is_eligible_for_gro(const struct net_device *dev,\n\t\t\t\t\t const struct net_device *rcv,\n\t\t\t\t\t const struct sk_buff *skb)\n{\n\treturn !(dev->features & NETIF_F_ALL_TSO) ||\n\t\t(skb->destructor == sock_wfree &&\n\t\t rcv->features & (NETIF_F_GRO_FRAGLIST | NETIF_F_GRO_UDP_FWD));\n}\n\nstatic netdev_tx_t veth_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct veth_priv *rcv_priv, *priv = netdev_priv(dev);\n\tstruct veth_rq *rq = NULL;\n\tint ret = NETDEV_TX_OK;\n\tstruct net_device *rcv;\n\tint length = skb->len;\n\tbool use_napi = false;\n\tint rxq;\n\n\trcu_read_lock();\n\trcv = rcu_dereference(priv->peer);\n\tif (unlikely(!rcv) || !pskb_may_pull(skb, ETH_HLEN)) {\n\t\tkfree_skb(skb);\n\t\tgoto drop;\n\t}\n\n\trcv_priv = netdev_priv(rcv);\n\trxq = skb_get_queue_mapping(skb);\n\tif (rxq < rcv->real_num_rx_queues) {\n\t\trq = &rcv_priv->rq[rxq];\n\n\t\t \n\t\tuse_napi = rcu_access_pointer(rq->napi) &&\n\t\t\t   veth_skb_is_eligible_for_gro(dev, rcv, skb);\n\t}\n\n\tskb_tx_timestamp(skb);\n\tif (likely(veth_forward_skb(rcv, skb, rq, use_napi) == NET_RX_SUCCESS)) {\n\t\tif (!use_napi)\n\t\t\tdev_sw_netstats_tx_add(dev, 1, length);\n\t\telse\n\t\t\t__veth_xdp_flush(rq);\n\t} else {\ndrop:\n\t\tatomic64_inc(&priv->dropped);\n\t\tret = NET_XMIT_DROP;\n\t}\n\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic void veth_stats_rx(struct veth_stats *result, struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint i;\n\n\tresult->peer_tq_xdp_xmit_err = 0;\n\tresult->xdp_packets = 0;\n\tresult->xdp_tx_err = 0;\n\tresult->xdp_bytes = 0;\n\tresult->rx_drops = 0;\n\tfor (i = 0; i < dev->num_rx_queues; i++) {\n\t\tu64 packets, bytes, drops, xdp_tx_err, peer_tq_xdp_xmit_err;\n\t\tstruct veth_rq_stats *stats = &priv->rq[i].stats;\n\t\tunsigned int start;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&stats->syncp);\n\t\t\tpeer_tq_xdp_xmit_err = stats->vs.peer_tq_xdp_xmit_err;\n\t\t\txdp_tx_err = stats->vs.xdp_tx_err;\n\t\t\tpackets = stats->vs.xdp_packets;\n\t\t\tbytes = stats->vs.xdp_bytes;\n\t\t\tdrops = stats->vs.rx_drops;\n\t\t} while (u64_stats_fetch_retry(&stats->syncp, start));\n\t\tresult->peer_tq_xdp_xmit_err += peer_tq_xdp_xmit_err;\n\t\tresult->xdp_tx_err += xdp_tx_err;\n\t\tresult->xdp_packets += packets;\n\t\tresult->xdp_bytes += bytes;\n\t\tresult->rx_drops += drops;\n\t}\n}\n\nstatic void veth_get_stats64(struct net_device *dev,\n\t\t\t     struct rtnl_link_stats64 *tot)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct net_device *peer;\n\tstruct veth_stats rx;\n\n\ttot->tx_dropped = atomic64_read(&priv->dropped);\n\tdev_fetch_sw_netstats(tot, dev->tstats);\n\n\tveth_stats_rx(&rx, dev);\n\ttot->tx_dropped += rx.xdp_tx_err;\n\ttot->rx_dropped = rx.rx_drops + rx.peer_tq_xdp_xmit_err;\n\ttot->rx_bytes += rx.xdp_bytes;\n\ttot->rx_packets += rx.xdp_packets;\n\n\trcu_read_lock();\n\tpeer = rcu_dereference(priv->peer);\n\tif (peer) {\n\t\tstruct rtnl_link_stats64 tot_peer = {};\n\n\t\tdev_fetch_sw_netstats(&tot_peer, peer->tstats);\n\t\ttot->rx_bytes += tot_peer.tx_bytes;\n\t\ttot->rx_packets += tot_peer.tx_packets;\n\n\t\tveth_stats_rx(&rx, peer);\n\t\ttot->tx_dropped += rx.peer_tq_xdp_xmit_err;\n\t\ttot->rx_dropped += rx.xdp_tx_err;\n\t\ttot->tx_bytes += rx.xdp_bytes;\n\t\ttot->tx_packets += rx.xdp_packets;\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic void veth_set_multicast_list(struct net_device *dev)\n{\n}\n\nstatic int veth_select_rxq(struct net_device *dev)\n{\n\treturn smp_processor_id() % dev->real_num_rx_queues;\n}\n\nstatic struct net_device *veth_peer_dev(struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\n\t \n\treturn rcu_dereference(priv->peer);\n}\n\nstatic int veth_xdp_xmit(struct net_device *dev, int n,\n\t\t\t struct xdp_frame **frames,\n\t\t\t u32 flags, bool ndo_xmit)\n{\n\tstruct veth_priv *rcv_priv, *priv = netdev_priv(dev);\n\tint i, ret = -ENXIO, nxmit = 0;\n\tstruct net_device *rcv;\n\tunsigned int max_len;\n\tstruct veth_rq *rq;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\trcv = rcu_dereference(priv->peer);\n\tif (unlikely(!rcv))\n\t\tgoto out;\n\n\trcv_priv = netdev_priv(rcv);\n\trq = &rcv_priv->rq[veth_select_rxq(rcv)];\n\t \n\tif (!rcu_access_pointer(rq->napi))\n\t\tgoto out;\n\n\tmax_len = rcv->mtu + rcv->hard_header_len + VLAN_HLEN;\n\n\tspin_lock(&rq->xdp_ring.producer_lock);\n\tfor (i = 0; i < n; i++) {\n\t\tstruct xdp_frame *frame = frames[i];\n\t\tvoid *ptr = veth_xdp_to_ptr(frame);\n\n\t\tif (unlikely(xdp_get_frame_len(frame) > max_len ||\n\t\t\t     __ptr_ring_produce(&rq->xdp_ring, ptr)))\n\t\t\tbreak;\n\t\tnxmit++;\n\t}\n\tspin_unlock(&rq->xdp_ring.producer_lock);\n\n\tif (flags & XDP_XMIT_FLUSH)\n\t\t__veth_xdp_flush(rq);\n\n\tret = nxmit;\n\tif (ndo_xmit) {\n\t\tu64_stats_update_begin(&rq->stats.syncp);\n\t\trq->stats.vs.peer_tq_xdp_xmit += nxmit;\n\t\trq->stats.vs.peer_tq_xdp_xmit_err += n - nxmit;\n\t\tu64_stats_update_end(&rq->stats.syncp);\n\t}\n\nout:\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int veth_ndo_xdp_xmit(struct net_device *dev, int n,\n\t\t\t     struct xdp_frame **frames, u32 flags)\n{\n\tint err;\n\n\terr = veth_xdp_xmit(dev, n, frames, flags, true);\n\tif (err < 0) {\n\t\tstruct veth_priv *priv = netdev_priv(dev);\n\n\t\tatomic64_add(n, &priv->dropped);\n\t}\n\n\treturn err;\n}\n\nstatic void veth_xdp_flush_bq(struct veth_rq *rq, struct veth_xdp_tx_bq *bq)\n{\n\tint sent, i, err = 0, drops;\n\n\tsent = veth_xdp_xmit(rq->dev, bq->count, bq->q, 0, false);\n\tif (sent < 0) {\n\t\terr = sent;\n\t\tsent = 0;\n\t}\n\n\tfor (i = sent; unlikely(i < bq->count); i++)\n\t\txdp_return_frame(bq->q[i]);\n\n\tdrops = bq->count - sent;\n\ttrace_xdp_bulk_tx(rq->dev, sent, drops, err);\n\n\tu64_stats_update_begin(&rq->stats.syncp);\n\trq->stats.vs.xdp_tx += sent;\n\trq->stats.vs.xdp_tx_err += drops;\n\tu64_stats_update_end(&rq->stats.syncp);\n\n\tbq->count = 0;\n}\n\nstatic void veth_xdp_flush(struct veth_rq *rq, struct veth_xdp_tx_bq *bq)\n{\n\tstruct veth_priv *rcv_priv, *priv = netdev_priv(rq->dev);\n\tstruct net_device *rcv;\n\tstruct veth_rq *rcv_rq;\n\n\trcu_read_lock();\n\tveth_xdp_flush_bq(rq, bq);\n\trcv = rcu_dereference(priv->peer);\n\tif (unlikely(!rcv))\n\t\tgoto out;\n\n\trcv_priv = netdev_priv(rcv);\n\trcv_rq = &rcv_priv->rq[veth_select_rxq(rcv)];\n\t \n\tif (unlikely(!rcu_access_pointer(rcv_rq->xdp_prog)))\n\t\tgoto out;\n\n\t__veth_xdp_flush(rcv_rq);\nout:\n\trcu_read_unlock();\n}\n\nstatic int veth_xdp_tx(struct veth_rq *rq, struct xdp_buff *xdp,\n\t\t       struct veth_xdp_tx_bq *bq)\n{\n\tstruct xdp_frame *frame = xdp_convert_buff_to_frame(xdp);\n\n\tif (unlikely(!frame))\n\t\treturn -EOVERFLOW;\n\n\tif (unlikely(bq->count == VETH_XDP_TX_BULK_SIZE))\n\t\tveth_xdp_flush_bq(rq, bq);\n\n\tbq->q[bq->count++] = frame;\n\n\treturn 0;\n}\n\nstatic struct xdp_frame *veth_xdp_rcv_one(struct veth_rq *rq,\n\t\t\t\t\t  struct xdp_frame *frame,\n\t\t\t\t\t  struct veth_xdp_tx_bq *bq,\n\t\t\t\t\t  struct veth_stats *stats)\n{\n\tstruct xdp_frame orig_frame;\n\tstruct bpf_prog *xdp_prog;\n\n\trcu_read_lock();\n\txdp_prog = rcu_dereference(rq->xdp_prog);\n\tif (likely(xdp_prog)) {\n\t\tstruct veth_xdp_buff vxbuf;\n\t\tstruct xdp_buff *xdp = &vxbuf.xdp;\n\t\tu32 act;\n\n\t\txdp_convert_frame_to_buff(frame, xdp);\n\t\txdp->rxq = &rq->xdp_rxq;\n\t\tvxbuf.skb = NULL;\n\n\t\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\t\tswitch (act) {\n\t\tcase XDP_PASS:\n\t\t\tif (xdp_update_frame_from_buff(xdp, frame))\n\t\t\t\tgoto err_xdp;\n\t\t\tbreak;\n\t\tcase XDP_TX:\n\t\t\torig_frame = *frame;\n\t\t\txdp->rxq->mem = frame->mem;\n\t\t\tif (unlikely(veth_xdp_tx(rq, xdp, bq) < 0)) {\n\t\t\t\ttrace_xdp_exception(rq->dev, xdp_prog, act);\n\t\t\t\tframe = &orig_frame;\n\t\t\t\tstats->rx_drops++;\n\t\t\t\tgoto err_xdp;\n\t\t\t}\n\t\t\tstats->xdp_tx++;\n\t\t\trcu_read_unlock();\n\t\t\tgoto xdp_xmit;\n\t\tcase XDP_REDIRECT:\n\t\t\torig_frame = *frame;\n\t\t\txdp->rxq->mem = frame->mem;\n\t\t\tif (xdp_do_redirect(rq->dev, xdp, xdp_prog)) {\n\t\t\t\tframe = &orig_frame;\n\t\t\t\tstats->rx_drops++;\n\t\t\t\tgoto err_xdp;\n\t\t\t}\n\t\t\tstats->xdp_redirect++;\n\t\t\trcu_read_unlock();\n\t\t\tgoto xdp_xmit;\n\t\tdefault:\n\t\t\tbpf_warn_invalid_xdp_action(rq->dev, xdp_prog, act);\n\t\t\tfallthrough;\n\t\tcase XDP_ABORTED:\n\t\t\ttrace_xdp_exception(rq->dev, xdp_prog, act);\n\t\t\tfallthrough;\n\t\tcase XDP_DROP:\n\t\t\tstats->xdp_drops++;\n\t\t\tgoto err_xdp;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn frame;\nerr_xdp:\n\trcu_read_unlock();\n\txdp_return_frame(frame);\nxdp_xmit:\n\treturn NULL;\n}\n\n \nstatic void veth_xdp_rcv_bulk_skb(struct veth_rq *rq, void **frames,\n\t\t\t\t  int n_xdpf, struct veth_xdp_tx_bq *bq,\n\t\t\t\t  struct veth_stats *stats)\n{\n\tvoid *skbs[VETH_XDP_BATCH];\n\tint i;\n\n\tif (xdp_alloc_skb_bulk(skbs, n_xdpf,\n\t\t\t       GFP_ATOMIC | __GFP_ZERO) < 0) {\n\t\tfor (i = 0; i < n_xdpf; i++)\n\t\t\txdp_return_frame(frames[i]);\n\t\tstats->rx_drops += n_xdpf;\n\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < n_xdpf; i++) {\n\t\tstruct sk_buff *skb = skbs[i];\n\n\t\tskb = __xdp_build_skb_from_frame(frames[i], skb,\n\t\t\t\t\t\t rq->dev);\n\t\tif (!skb) {\n\t\t\txdp_return_frame(frames[i]);\n\t\t\tstats->rx_drops++;\n\t\t\tcontinue;\n\t\t}\n\t\tnapi_gro_receive(&rq->xdp_napi, skb);\n\t}\n}\n\nstatic void veth_xdp_get(struct xdp_buff *xdp)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\tint i;\n\n\tget_page(virt_to_page(xdp->data));\n\tif (likely(!xdp_buff_has_frags(xdp)))\n\t\treturn;\n\n\tfor (i = 0; i < sinfo->nr_frags; i++)\n\t\t__skb_frag_ref(&sinfo->frags[i]);\n}\n\nstatic int veth_convert_skb_to_xdp_buff(struct veth_rq *rq,\n\t\t\t\t\tstruct xdp_buff *xdp,\n\t\t\t\t\tstruct sk_buff **pskb)\n{\n\tstruct sk_buff *skb = *pskb;\n\tu32 frame_sz;\n\n\tif (skb_shared(skb) || skb_head_is_locked(skb) ||\n\t    skb_shinfo(skb)->nr_frags ||\n\t    skb_headroom(skb) < XDP_PACKET_HEADROOM) {\n\t\tu32 size, len, max_head_size, off;\n\t\tstruct sk_buff *nskb;\n\t\tstruct page *page;\n\t\tint i, head_off;\n\n\t\t \n\t\tmax_head_size = SKB_WITH_OVERHEAD(PAGE_SIZE -\n\t\t\t\t\t\t  VETH_XDP_HEADROOM);\n\t\tif (skb->len > PAGE_SIZE * MAX_SKB_FRAGS + max_head_size)\n\t\t\tgoto drop;\n\n\t\t \n\t\tpage = page_pool_dev_alloc_pages(rq->page_pool);\n\t\tif (!page)\n\t\t\tgoto drop;\n\n\t\tnskb = napi_build_skb(page_address(page), PAGE_SIZE);\n\t\tif (!nskb) {\n\t\t\tpage_pool_put_full_page(rq->page_pool, page, true);\n\t\t\tgoto drop;\n\t\t}\n\n\t\tskb_reserve(nskb, VETH_XDP_HEADROOM);\n\t\tskb_copy_header(nskb, skb);\n\t\tskb_mark_for_recycle(nskb);\n\n\t\tsize = min_t(u32, skb->len, max_head_size);\n\t\tif (skb_copy_bits(skb, 0, nskb->data, size)) {\n\t\t\tconsume_skb(nskb);\n\t\t\tgoto drop;\n\t\t}\n\t\tskb_put(nskb, size);\n\n\t\thead_off = skb_headroom(nskb) - skb_headroom(skb);\n\t\tskb_headers_offset_update(nskb, head_off);\n\n\t\t \n\t\toff = size;\n\t\tlen = skb->len - off;\n\n\t\tfor (i = 0; i < MAX_SKB_FRAGS && off < skb->len; i++) {\n\t\t\tpage = page_pool_dev_alloc_pages(rq->page_pool);\n\t\t\tif (!page) {\n\t\t\t\tconsume_skb(nskb);\n\t\t\t\tgoto drop;\n\t\t\t}\n\n\t\t\tsize = min_t(u32, len, PAGE_SIZE);\n\t\t\tskb_add_rx_frag(nskb, i, page, 0, size, PAGE_SIZE);\n\t\t\tif (skb_copy_bits(skb, off, page_address(page),\n\t\t\t\t\t  size)) {\n\t\t\t\tconsume_skb(nskb);\n\t\t\t\tgoto drop;\n\t\t\t}\n\n\t\t\tlen -= size;\n\t\t\toff += size;\n\t\t}\n\n\t\tconsume_skb(skb);\n\t\tskb = nskb;\n\t}\n\n\t \n\tframe_sz = skb_end_pointer(skb) - skb->head;\n\tframe_sz += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\txdp_init_buff(xdp, frame_sz, &rq->xdp_rxq);\n\txdp_prepare_buff(xdp, skb->head, skb_headroom(skb),\n\t\t\t skb_headlen(skb), true);\n\n\tif (skb_is_nonlinear(skb)) {\n\t\tskb_shinfo(skb)->xdp_frags_size = skb->data_len;\n\t\txdp_buff_set_frags_flag(xdp);\n\t} else {\n\t\txdp_buff_clear_frags_flag(xdp);\n\t}\n\t*pskb = skb;\n\n\treturn 0;\ndrop:\n\tconsume_skb(skb);\n\t*pskb = NULL;\n\n\treturn -ENOMEM;\n}\n\nstatic struct sk_buff *veth_xdp_rcv_skb(struct veth_rq *rq,\n\t\t\t\t\tstruct sk_buff *skb,\n\t\t\t\t\tstruct veth_xdp_tx_bq *bq,\n\t\t\t\t\tstruct veth_stats *stats)\n{\n\tvoid *orig_data, *orig_data_end;\n\tstruct bpf_prog *xdp_prog;\n\tstruct veth_xdp_buff vxbuf;\n\tstruct xdp_buff *xdp = &vxbuf.xdp;\n\tu32 act, metalen;\n\tint off;\n\n\tskb_prepare_for_gro(skb);\n\n\trcu_read_lock();\n\txdp_prog = rcu_dereference(rq->xdp_prog);\n\tif (unlikely(!xdp_prog)) {\n\t\trcu_read_unlock();\n\t\tgoto out;\n\t}\n\n\t__skb_push(skb, skb->data - skb_mac_header(skb));\n\tif (veth_convert_skb_to_xdp_buff(rq, xdp, &skb))\n\t\tgoto drop;\n\tvxbuf.skb = skb;\n\n\torig_data = xdp->data;\n\torig_data_end = xdp->data_end;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\tveth_xdp_get(xdp);\n\t\tconsume_skb(skb);\n\t\txdp->rxq->mem = rq->xdp_mem;\n\t\tif (unlikely(veth_xdp_tx(rq, xdp, bq) < 0)) {\n\t\t\ttrace_xdp_exception(rq->dev, xdp_prog, act);\n\t\t\tstats->rx_drops++;\n\t\t\tgoto err_xdp;\n\t\t}\n\t\tstats->xdp_tx++;\n\t\trcu_read_unlock();\n\t\tgoto xdp_xmit;\n\tcase XDP_REDIRECT:\n\t\tveth_xdp_get(xdp);\n\t\tconsume_skb(skb);\n\t\txdp->rxq->mem = rq->xdp_mem;\n\t\tif (xdp_do_redirect(rq->dev, xdp, xdp_prog)) {\n\t\t\tstats->rx_drops++;\n\t\t\tgoto err_xdp;\n\t\t}\n\t\tstats->xdp_redirect++;\n\t\trcu_read_unlock();\n\t\tgoto xdp_xmit;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rq->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(rq->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tstats->xdp_drops++;\n\t\tgoto xdp_drop;\n\t}\n\trcu_read_unlock();\n\n\t \n\toff = orig_data - xdp->data;\n\tif (off > 0)\n\t\t__skb_push(skb, off);\n\telse if (off < 0)\n\t\t__skb_pull(skb, -off);\n\n\tskb_reset_mac_header(skb);\n\n\t \n\toff = xdp->data_end - orig_data_end;\n\tif (off != 0)\n\t\t__skb_put(skb, off);  \n\n\t \n\tif (xdp_buff_has_frags(xdp))\n\t\tskb->data_len = skb_shinfo(skb)->xdp_frags_size;\n\telse\n\t\tskb->data_len = 0;\n\n\tskb->protocol = eth_type_trans(skb, rq->dev);\n\n\tmetalen = xdp->data - xdp->data_meta;\n\tif (metalen)\n\t\tskb_metadata_set(skb, metalen);\nout:\n\treturn skb;\ndrop:\n\tstats->rx_drops++;\nxdp_drop:\n\trcu_read_unlock();\n\tkfree_skb(skb);\n\treturn NULL;\nerr_xdp:\n\trcu_read_unlock();\n\txdp_return_buff(xdp);\nxdp_xmit:\n\treturn NULL;\n}\n\nstatic int veth_xdp_rcv(struct veth_rq *rq, int budget,\n\t\t\tstruct veth_xdp_tx_bq *bq,\n\t\t\tstruct veth_stats *stats)\n{\n\tint i, done = 0, n_xdpf = 0;\n\tvoid *xdpf[VETH_XDP_BATCH];\n\n\tfor (i = 0; i < budget; i++) {\n\t\tvoid *ptr = __ptr_ring_consume(&rq->xdp_ring);\n\n\t\tif (!ptr)\n\t\t\tbreak;\n\n\t\tif (veth_is_xdp_frame(ptr)) {\n\t\t\t \n\t\t\tstruct xdp_frame *frame = veth_ptr_to_xdp(ptr);\n\n\t\t\tstats->xdp_bytes += xdp_get_frame_len(frame);\n\t\t\tframe = veth_xdp_rcv_one(rq, frame, bq, stats);\n\t\t\tif (frame) {\n\t\t\t\t \n\t\t\t\txdpf[n_xdpf++] = frame;\n\t\t\t\tif (n_xdpf == VETH_XDP_BATCH) {\n\t\t\t\t\tveth_xdp_rcv_bulk_skb(rq, xdpf, n_xdpf,\n\t\t\t\t\t\t\t      bq, stats);\n\t\t\t\t\tn_xdpf = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tstruct sk_buff *skb = ptr;\n\n\t\t\tstats->xdp_bytes += skb->len;\n\t\t\tskb = veth_xdp_rcv_skb(rq, skb, bq, stats);\n\t\t\tif (skb) {\n\t\t\t\tif (skb_shared(skb) || skb_unclone(skb, GFP_ATOMIC))\n\t\t\t\t\tnetif_receive_skb(skb);\n\t\t\t\telse\n\t\t\t\t\tnapi_gro_receive(&rq->xdp_napi, skb);\n\t\t\t}\n\t\t}\n\t\tdone++;\n\t}\n\n\tif (n_xdpf)\n\t\tveth_xdp_rcv_bulk_skb(rq, xdpf, n_xdpf, bq, stats);\n\n\tu64_stats_update_begin(&rq->stats.syncp);\n\trq->stats.vs.xdp_redirect += stats->xdp_redirect;\n\trq->stats.vs.xdp_bytes += stats->xdp_bytes;\n\trq->stats.vs.xdp_drops += stats->xdp_drops;\n\trq->stats.vs.rx_drops += stats->rx_drops;\n\trq->stats.vs.xdp_packets += done;\n\tu64_stats_update_end(&rq->stats.syncp);\n\n\treturn done;\n}\n\nstatic int veth_poll(struct napi_struct *napi, int budget)\n{\n\tstruct veth_rq *rq =\n\t\tcontainer_of(napi, struct veth_rq, xdp_napi);\n\tstruct veth_stats stats = {};\n\tstruct veth_xdp_tx_bq bq;\n\tint done;\n\n\tbq.count = 0;\n\n\txdp_set_return_frame_no_direct();\n\tdone = veth_xdp_rcv(rq, budget, &bq, &stats);\n\n\tif (stats.xdp_redirect > 0)\n\t\txdp_do_flush();\n\n\tif (done < budget && napi_complete_done(napi, done)) {\n\t\t \n\t\tsmp_store_mb(rq->rx_notify_masked, false);\n\t\tif (unlikely(!__ptr_ring_empty(&rq->xdp_ring))) {\n\t\t\tif (napi_schedule_prep(&rq->xdp_napi)) {\n\t\t\t\tWRITE_ONCE(rq->rx_notify_masked, true);\n\t\t\t\t__napi_schedule(&rq->xdp_napi);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (stats.xdp_tx > 0)\n\t\tveth_xdp_flush(rq, &bq);\n\txdp_clear_return_frame_no_direct();\n\n\treturn done;\n}\n\nstatic int veth_create_page_pool(struct veth_rq *rq)\n{\n\tstruct page_pool_params pp_params = {\n\t\t.order = 0,\n\t\t.pool_size = VETH_RING_SIZE,\n\t\t.nid = NUMA_NO_NODE,\n\t\t.dev = &rq->dev->dev,\n\t};\n\n\trq->page_pool = page_pool_create(&pp_params);\n\tif (IS_ERR(rq->page_pool)) {\n\t\tint err = PTR_ERR(rq->page_pool);\n\n\t\trq->page_pool = NULL;\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int __veth_napi_enable_range(struct net_device *dev, int start, int end)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint err, i;\n\n\tfor (i = start; i < end; i++) {\n\t\terr = veth_create_page_pool(&priv->rq[i]);\n\t\tif (err)\n\t\t\tgoto err_page_pool;\n\t}\n\n\tfor (i = start; i < end; i++) {\n\t\tstruct veth_rq *rq = &priv->rq[i];\n\n\t\terr = ptr_ring_init(&rq->xdp_ring, VETH_RING_SIZE, GFP_KERNEL);\n\t\tif (err)\n\t\t\tgoto err_xdp_ring;\n\t}\n\n\tfor (i = start; i < end; i++) {\n\t\tstruct veth_rq *rq = &priv->rq[i];\n\n\t\tnapi_enable(&rq->xdp_napi);\n\t\trcu_assign_pointer(priv->rq[i].napi, &priv->rq[i].xdp_napi);\n\t}\n\n\treturn 0;\n\nerr_xdp_ring:\n\tfor (i--; i >= start; i--)\n\t\tptr_ring_cleanup(&priv->rq[i].xdp_ring, veth_ptr_free);\n\ti = end;\nerr_page_pool:\n\tfor (i--; i >= start; i--) {\n\t\tpage_pool_destroy(priv->rq[i].page_pool);\n\t\tpriv->rq[i].page_pool = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic int __veth_napi_enable(struct net_device *dev)\n{\n\treturn __veth_napi_enable_range(dev, 0, dev->real_num_rx_queues);\n}\n\nstatic void veth_napi_del_range(struct net_device *dev, int start, int end)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint i;\n\n\tfor (i = start; i < end; i++) {\n\t\tstruct veth_rq *rq = &priv->rq[i];\n\n\t\trcu_assign_pointer(priv->rq[i].napi, NULL);\n\t\tnapi_disable(&rq->xdp_napi);\n\t\t__netif_napi_del(&rq->xdp_napi);\n\t}\n\tsynchronize_net();\n\n\tfor (i = start; i < end; i++) {\n\t\tstruct veth_rq *rq = &priv->rq[i];\n\n\t\trq->rx_notify_masked = false;\n\t\tptr_ring_cleanup(&rq->xdp_ring, veth_ptr_free);\n\t}\n\n\tfor (i = start; i < end; i++) {\n\t\tpage_pool_destroy(priv->rq[i].page_pool);\n\t\tpriv->rq[i].page_pool = NULL;\n\t}\n}\n\nstatic void veth_napi_del(struct net_device *dev)\n{\n\tveth_napi_del_range(dev, 0, dev->real_num_rx_queues);\n}\n\nstatic bool veth_gro_requested(const struct net_device *dev)\n{\n\treturn !!(dev->wanted_features & NETIF_F_GRO);\n}\n\nstatic int veth_enable_xdp_range(struct net_device *dev, int start, int end,\n\t\t\t\t bool napi_already_on)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint err, i;\n\n\tfor (i = start; i < end; i++) {\n\t\tstruct veth_rq *rq = &priv->rq[i];\n\n\t\tif (!napi_already_on)\n\t\t\tnetif_napi_add(dev, &rq->xdp_napi, veth_poll);\n\t\terr = xdp_rxq_info_reg(&rq->xdp_rxq, dev, i, rq->xdp_napi.napi_id);\n\t\tif (err < 0)\n\t\t\tgoto err_rxq_reg;\n\n\t\terr = xdp_rxq_info_reg_mem_model(&rq->xdp_rxq,\n\t\t\t\t\t\t MEM_TYPE_PAGE_SHARED,\n\t\t\t\t\t\t NULL);\n\t\tif (err < 0)\n\t\t\tgoto err_reg_mem;\n\n\t\t \n\t\trq->xdp_mem = rq->xdp_rxq.mem;\n\t}\n\treturn 0;\n\nerr_reg_mem:\n\txdp_rxq_info_unreg(&priv->rq[i].xdp_rxq);\nerr_rxq_reg:\n\tfor (i--; i >= start; i--) {\n\t\tstruct veth_rq *rq = &priv->rq[i];\n\n\t\txdp_rxq_info_unreg(&rq->xdp_rxq);\n\t\tif (!napi_already_on)\n\t\t\tnetif_napi_del(&rq->xdp_napi);\n\t}\n\n\treturn err;\n}\n\nstatic void veth_disable_xdp_range(struct net_device *dev, int start, int end,\n\t\t\t\t   bool delete_napi)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint i;\n\n\tfor (i = start; i < end; i++) {\n\t\tstruct veth_rq *rq = &priv->rq[i];\n\n\t\trq->xdp_rxq.mem = rq->xdp_mem;\n\t\txdp_rxq_info_unreg(&rq->xdp_rxq);\n\n\t\tif (delete_napi)\n\t\t\tnetif_napi_del(&rq->xdp_napi);\n\t}\n}\n\nstatic int veth_enable_xdp(struct net_device *dev)\n{\n\tbool napi_already_on = veth_gro_requested(dev) && (dev->flags & IFF_UP);\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint err, i;\n\n\tif (!xdp_rxq_info_is_reg(&priv->rq[0].xdp_rxq)) {\n\t\terr = veth_enable_xdp_range(dev, 0, dev->real_num_rx_queues, napi_already_on);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!napi_already_on) {\n\t\t\terr = __veth_napi_enable(dev);\n\t\t\tif (err) {\n\t\t\t\tveth_disable_xdp_range(dev, 0, dev->real_num_rx_queues, true);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\tif (!veth_gro_requested(dev)) {\n\t\t\t\t \n\t\t\t\tdev->features |= NETIF_F_GRO;\n\t\t\t\tnetdev_features_change(dev);\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (i = 0; i < dev->real_num_rx_queues; i++) {\n\t\trcu_assign_pointer(priv->rq[i].xdp_prog, priv->_xdp_prog);\n\t\trcu_assign_pointer(priv->rq[i].napi, &priv->rq[i].xdp_napi);\n\t}\n\n\treturn 0;\n}\n\nstatic void veth_disable_xdp(struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < dev->real_num_rx_queues; i++)\n\t\trcu_assign_pointer(priv->rq[i].xdp_prog, NULL);\n\n\tif (!netif_running(dev) || !veth_gro_requested(dev)) {\n\t\tveth_napi_del(dev);\n\n\t\t \n\t\tif (!veth_gro_requested(dev) && netif_running(dev)) {\n\t\t\tdev->features &= ~NETIF_F_GRO;\n\t\t\tnetdev_features_change(dev);\n\t\t}\n\t}\n\n\tveth_disable_xdp_range(dev, 0, dev->real_num_rx_queues, false);\n}\n\nstatic int veth_napi_enable_range(struct net_device *dev, int start, int end)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint err, i;\n\n\tfor (i = start; i < end; i++) {\n\t\tstruct veth_rq *rq = &priv->rq[i];\n\n\t\tnetif_napi_add(dev, &rq->xdp_napi, veth_poll);\n\t}\n\n\terr = __veth_napi_enable_range(dev, start, end);\n\tif (err) {\n\t\tfor (i = start; i < end; i++) {\n\t\t\tstruct veth_rq *rq = &priv->rq[i];\n\n\t\t\tnetif_napi_del(&rq->xdp_napi);\n\t\t}\n\t\treturn err;\n\t}\n\treturn err;\n}\n\nstatic int veth_napi_enable(struct net_device *dev)\n{\n\treturn veth_napi_enable_range(dev, 0, dev->real_num_rx_queues);\n}\n\nstatic void veth_disable_range_safe(struct net_device *dev, int start, int end)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\n\tif (start >= end)\n\t\treturn;\n\n\tif (priv->_xdp_prog) {\n\t\tveth_napi_del_range(dev, start, end);\n\t\tveth_disable_xdp_range(dev, start, end, false);\n\t} else if (veth_gro_requested(dev)) {\n\t\tveth_napi_del_range(dev, start, end);\n\t}\n}\n\nstatic int veth_enable_range_safe(struct net_device *dev, int start, int end)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint err;\n\n\tif (start >= end)\n\t\treturn 0;\n\n\tif (priv->_xdp_prog) {\n\t\t \n\t\terr = veth_enable_xdp_range(dev, start, end, false);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = __veth_napi_enable_range(dev, start, end);\n\t\tif (err) {\n\t\t\t \n\t\t\tveth_disable_xdp_range(dev, start, end, true);\n\t\t\treturn err;\n\t\t}\n\t} else if (veth_gro_requested(dev)) {\n\t\treturn veth_napi_enable_range(dev, start, end);\n\t}\n\treturn 0;\n}\n\nstatic void veth_set_xdp_features(struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct net_device *peer;\n\n\tpeer = rtnl_dereference(priv->peer);\n\tif (peer && peer->real_num_tx_queues <= dev->real_num_rx_queues) {\n\t\tstruct veth_priv *priv_peer = netdev_priv(peer);\n\t\txdp_features_t val = NETDEV_XDP_ACT_BASIC |\n\t\t\t\t     NETDEV_XDP_ACT_REDIRECT |\n\t\t\t\t     NETDEV_XDP_ACT_RX_SG;\n\n\t\tif (priv_peer->_xdp_prog || veth_gro_requested(peer))\n\t\t\tval |= NETDEV_XDP_ACT_NDO_XMIT |\n\t\t\t       NETDEV_XDP_ACT_NDO_XMIT_SG;\n\t\txdp_set_features_flag(dev, val);\n\t} else {\n\t\txdp_clear_features_flag(dev);\n\t}\n}\n\nstatic int veth_set_channels(struct net_device *dev,\n\t\t\t     struct ethtool_channels *ch)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tunsigned int old_rx_count, new_rx_count;\n\tstruct veth_priv *peer_priv;\n\tstruct net_device *peer;\n\tint err;\n\n\t \n\tif (!ch->rx_count || !ch->tx_count)\n\t\treturn -EINVAL;\n\n\t \n\tpeer = rtnl_dereference(priv->peer);\n\tpeer_priv = peer ? netdev_priv(peer) : NULL;\n\tif (priv->_xdp_prog && peer && ch->rx_count < peer->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (peer && peer_priv && peer_priv->_xdp_prog && ch->tx_count > peer->real_num_rx_queues)\n\t\treturn -EINVAL;\n\n\told_rx_count = dev->real_num_rx_queues;\n\tnew_rx_count = ch->rx_count;\n\tif (netif_running(dev)) {\n\t\t \n\t\tnetif_carrier_off(dev);\n\t\tif (peer)\n\t\t\tnetif_carrier_off(peer);\n\n\t\t \n\t\terr = veth_enable_range_safe(dev, old_rx_count, new_rx_count);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = netif_set_real_num_rx_queues(dev, ch->rx_count);\n\tif (err)\n\t\tgoto revert;\n\n\terr = netif_set_real_num_tx_queues(dev, ch->tx_count);\n\tif (err) {\n\t\tint err2 = netif_set_real_num_rx_queues(dev, old_rx_count);\n\n\t\t \n\t\tif (err2)\n\t\t\tpr_warn(\"Can't restore rx queues config %d -> %d %d\",\n\t\t\t\tnew_rx_count, old_rx_count, err2);\n\t\telse\n\t\t\tgoto revert;\n\t}\n\nout:\n\tif (netif_running(dev)) {\n\t\t \n\t\tveth_disable_range_safe(dev, new_rx_count, old_rx_count);\n\t\tnetif_carrier_on(dev);\n\t\tif (peer)\n\t\t\tnetif_carrier_on(peer);\n\t}\n\n\t \n\tveth_set_xdp_features(dev);\n\tif (peer)\n\t\tveth_set_xdp_features(peer);\n\n\treturn err;\n\nrevert:\n\tnew_rx_count = old_rx_count;\n\told_rx_count = ch->rx_count;\n\tgoto out;\n}\n\nstatic int veth_open(struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct net_device *peer = rtnl_dereference(priv->peer);\n\tint err;\n\n\tif (!peer)\n\t\treturn -ENOTCONN;\n\n\tif (priv->_xdp_prog) {\n\t\terr = veth_enable_xdp(dev);\n\t\tif (err)\n\t\t\treturn err;\n\t} else if (veth_gro_requested(dev)) {\n\t\terr = veth_napi_enable(dev);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (peer->flags & IFF_UP) {\n\t\tnetif_carrier_on(dev);\n\t\tnetif_carrier_on(peer);\n\t}\n\n\tveth_set_xdp_features(dev);\n\n\treturn 0;\n}\n\nstatic int veth_close(struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct net_device *peer = rtnl_dereference(priv->peer);\n\n\tnetif_carrier_off(dev);\n\tif (peer)\n\t\tnetif_carrier_off(peer);\n\n\tif (priv->_xdp_prog)\n\t\tveth_disable_xdp(dev);\n\telse if (veth_gro_requested(dev))\n\t\tveth_napi_del(dev);\n\n\treturn 0;\n}\n\nstatic int is_valid_veth_mtu(int mtu)\n{\n\treturn mtu >= ETH_MIN_MTU && mtu <= ETH_MAX_MTU;\n}\n\nstatic int veth_alloc_queues(struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tint i;\n\n\tpriv->rq = kcalloc(dev->num_rx_queues, sizeof(*priv->rq), GFP_KERNEL_ACCOUNT);\n\tif (!priv->rq)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < dev->num_rx_queues; i++) {\n\t\tpriv->rq[i].dev = dev;\n\t\tu64_stats_init(&priv->rq[i].stats.syncp);\n\t}\n\n\treturn 0;\n}\n\nstatic void veth_free_queues(struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\n\tkfree(priv->rq);\n}\n\nstatic int veth_dev_init(struct net_device *dev)\n{\n\treturn veth_alloc_queues(dev);\n}\n\nstatic void veth_dev_free(struct net_device *dev)\n{\n\tveth_free_queues(dev);\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void veth_poll_controller(struct net_device *dev)\n{\n\t \n}\n#endif\t \n\nstatic int veth_get_iflink(const struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct net_device *peer;\n\tint iflink;\n\n\trcu_read_lock();\n\tpeer = rcu_dereference(priv->peer);\n\tiflink = peer ? peer->ifindex : 0;\n\trcu_read_unlock();\n\n\treturn iflink;\n}\n\nstatic netdev_features_t veth_fix_features(struct net_device *dev,\n\t\t\t\t\t   netdev_features_t features)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct net_device *peer;\n\n\tpeer = rtnl_dereference(priv->peer);\n\tif (peer) {\n\t\tstruct veth_priv *peer_priv = netdev_priv(peer);\n\n\t\tif (peer_priv->_xdp_prog)\n\t\t\tfeatures &= ~NETIF_F_GSO_SOFTWARE;\n\t}\n\tif (priv->_xdp_prog)\n\t\tfeatures |= NETIF_F_GRO;\n\n\treturn features;\n}\n\nstatic int veth_set_features(struct net_device *dev,\n\t\t\t     netdev_features_t features)\n{\n\tnetdev_features_t changed = features ^ dev->features;\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct net_device *peer;\n\tint err;\n\n\tif (!(changed & NETIF_F_GRO) || !(dev->flags & IFF_UP) || priv->_xdp_prog)\n\t\treturn 0;\n\n\tpeer = rtnl_dereference(priv->peer);\n\tif (features & NETIF_F_GRO) {\n\t\terr = veth_napi_enable(dev);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (peer)\n\t\t\txdp_features_set_redirect_target(peer, true);\n\t} else {\n\t\tif (peer)\n\t\t\txdp_features_clear_redirect_target(peer);\n\t\tveth_napi_del(dev);\n\t}\n\treturn 0;\n}\n\nstatic void veth_set_rx_headroom(struct net_device *dev, int new_hr)\n{\n\tstruct veth_priv *peer_priv, *priv = netdev_priv(dev);\n\tstruct net_device *peer;\n\n\tif (new_hr < 0)\n\t\tnew_hr = 0;\n\n\trcu_read_lock();\n\tpeer = rcu_dereference(priv->peer);\n\tif (unlikely(!peer))\n\t\tgoto out;\n\n\tpeer_priv = netdev_priv(peer);\n\tpriv->requested_headroom = new_hr;\n\tnew_hr = max(priv->requested_headroom, peer_priv->requested_headroom);\n\tdev->needed_headroom = new_hr;\n\tpeer->needed_headroom = new_hr;\n\nout:\n\trcu_read_unlock();\n}\n\nstatic int veth_xdp_set(struct net_device *dev, struct bpf_prog *prog,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct bpf_prog *old_prog;\n\tstruct net_device *peer;\n\tunsigned int max_mtu;\n\tint err;\n\n\told_prog = priv->_xdp_prog;\n\tpriv->_xdp_prog = prog;\n\tpeer = rtnl_dereference(priv->peer);\n\n\tif (prog) {\n\t\tif (!peer) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Cannot set XDP when peer is detached\");\n\t\t\terr = -ENOTCONN;\n\t\t\tgoto err;\n\t\t}\n\n\t\tmax_mtu = SKB_WITH_OVERHEAD(PAGE_SIZE - VETH_XDP_HEADROOM) -\n\t\t\t  peer->hard_header_len;\n\t\t \n\t\tif (prog->aux->xdp_has_frags)\n\t\t\tmax_mtu += PAGE_SIZE * MAX_SKB_FRAGS;\n\n\t\tif (peer->mtu > max_mtu) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Peer MTU is too large to set XDP\");\n\t\t\terr = -ERANGE;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (dev->real_num_rx_queues < peer->real_num_tx_queues) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"XDP expects number of rx queues not less than peer tx queues\");\n\t\t\terr = -ENOSPC;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (dev->flags & IFF_UP) {\n\t\t\terr = veth_enable_xdp(dev);\n\t\t\tif (err) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Setup for XDP failed\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\tif (!old_prog) {\n\t\t\tpeer->hw_features &= ~NETIF_F_GSO_SOFTWARE;\n\t\t\tpeer->max_mtu = max_mtu;\n\t\t}\n\n\t\txdp_features_set_redirect_target(peer, true);\n\t}\n\n\tif (old_prog) {\n\t\tif (!prog) {\n\t\t\tif (peer && !veth_gro_requested(dev))\n\t\t\t\txdp_features_clear_redirect_target(peer);\n\n\t\t\tif (dev->flags & IFF_UP)\n\t\t\t\tveth_disable_xdp(dev);\n\n\t\t\tif (peer) {\n\t\t\t\tpeer->hw_features |= NETIF_F_GSO_SOFTWARE;\n\t\t\t\tpeer->max_mtu = ETH_MAX_MTU;\n\t\t\t}\n\t\t}\n\t\tbpf_prog_put(old_prog);\n\t}\n\n\tif ((!!old_prog ^ !!prog) && peer)\n\t\tnetdev_update_features(peer);\n\n\treturn 0;\nerr:\n\tpriv->_xdp_prog = old_prog;\n\n\treturn err;\n}\n\nstatic int veth_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn veth_xdp_set(dev, xdp->prog, xdp->extack);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int veth_xdp_rx_timestamp(const struct xdp_md *ctx, u64 *timestamp)\n{\n\tstruct veth_xdp_buff *_ctx = (void *)ctx;\n\n\tif (!_ctx->skb)\n\t\treturn -ENODATA;\n\n\t*timestamp = skb_hwtstamps(_ctx->skb)->hwtstamp;\n\treturn 0;\n}\n\nstatic int veth_xdp_rx_hash(const struct xdp_md *ctx, u32 *hash,\n\t\t\t    enum xdp_rss_hash_type *rss_type)\n{\n\tstruct veth_xdp_buff *_ctx = (void *)ctx;\n\tstruct sk_buff *skb = _ctx->skb;\n\n\tif (!skb)\n\t\treturn -ENODATA;\n\n\t*hash = skb_get_hash(skb);\n\t*rss_type = skb->l4_hash ? XDP_RSS_TYPE_L4_ANY : XDP_RSS_TYPE_NONE;\n\n\treturn 0;\n}\n\nstatic const struct net_device_ops veth_netdev_ops = {\n\t.ndo_init            = veth_dev_init,\n\t.ndo_open            = veth_open,\n\t.ndo_stop            = veth_close,\n\t.ndo_start_xmit      = veth_xmit,\n\t.ndo_get_stats64     = veth_get_stats64,\n\t.ndo_set_rx_mode     = veth_set_multicast_list,\n\t.ndo_set_mac_address = eth_mac_addr,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= veth_poll_controller,\n#endif\n\t.ndo_get_iflink\t\t= veth_get_iflink,\n\t.ndo_fix_features\t= veth_fix_features,\n\t.ndo_set_features\t= veth_set_features,\n\t.ndo_features_check\t= passthru_features_check,\n\t.ndo_set_rx_headroom\t= veth_set_rx_headroom,\n\t.ndo_bpf\t\t= veth_xdp,\n\t.ndo_xdp_xmit\t\t= veth_ndo_xdp_xmit,\n\t.ndo_get_peer_dev\t= veth_peer_dev,\n};\n\nstatic const struct xdp_metadata_ops veth_xdp_metadata_ops = {\n\t.xmo_rx_timestamp\t\t= veth_xdp_rx_timestamp,\n\t.xmo_rx_hash\t\t\t= veth_xdp_rx_hash,\n};\n\n#define VETH_FEATURES (NETIF_F_SG | NETIF_F_FRAGLIST | NETIF_F_HW_CSUM | \\\n\t\t       NETIF_F_RXCSUM | NETIF_F_SCTP_CRC | NETIF_F_HIGHDMA | \\\n\t\t       NETIF_F_GSO_SOFTWARE | NETIF_F_GSO_ENCAP_ALL | \\\n\t\t       NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX | \\\n\t\t       NETIF_F_HW_VLAN_STAG_TX | NETIF_F_HW_VLAN_STAG_RX )\n\nstatic void veth_setup(struct net_device *dev)\n{\n\tether_setup(dev);\n\n\tdev->priv_flags &= ~IFF_TX_SKB_SHARING;\n\tdev->priv_flags |= IFF_LIVE_ADDR_CHANGE;\n\tdev->priv_flags |= IFF_NO_QUEUE;\n\tdev->priv_flags |= IFF_PHONY_HEADROOM;\n\n\tdev->netdev_ops = &veth_netdev_ops;\n\tdev->xdp_metadata_ops = &veth_xdp_metadata_ops;\n\tdev->ethtool_ops = &veth_ethtool_ops;\n\tdev->features |= NETIF_F_LLTX;\n\tdev->features |= VETH_FEATURES;\n\tdev->vlan_features = dev->features &\n\t\t\t     ~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t       NETIF_F_HW_VLAN_STAG_TX |\n\t\t\t       NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t       NETIF_F_HW_VLAN_STAG_RX);\n\tdev->needs_free_netdev = true;\n\tdev->priv_destructor = veth_dev_free;\n\tdev->pcpu_stat_type = NETDEV_PCPU_STAT_TSTATS;\n\tdev->max_mtu = ETH_MAX_MTU;\n\n\tdev->hw_features = VETH_FEATURES;\n\tdev->hw_enc_features = VETH_FEATURES;\n\tdev->mpls_features = NETIF_F_HW_CSUM | NETIF_F_GSO_SOFTWARE;\n\tnetif_set_tso_max_size(dev, GSO_MAX_SIZE);\n}\n\n \n\nstatic int veth_validate(struct nlattr *tb[], struct nlattr *data[],\n\t\t\t struct netlink_ext_ack *extack)\n{\n\tif (tb[IFLA_ADDRESS]) {\n\t\tif (nla_len(tb[IFLA_ADDRESS]) != ETH_ALEN)\n\t\t\treturn -EINVAL;\n\t\tif (!is_valid_ether_addr(nla_data(tb[IFLA_ADDRESS])))\n\t\t\treturn -EADDRNOTAVAIL;\n\t}\n\tif (tb[IFLA_MTU]) {\n\t\tif (!is_valid_veth_mtu(nla_get_u32(tb[IFLA_MTU])))\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic struct rtnl_link_ops veth_link_ops;\n\nstatic void veth_disable_gro(struct net_device *dev)\n{\n\tdev->features &= ~NETIF_F_GRO;\n\tdev->wanted_features &= ~NETIF_F_GRO;\n\tnetdev_update_features(dev);\n}\n\nstatic int veth_init_queues(struct net_device *dev, struct nlattr *tb[])\n{\n\tint err;\n\n\tif (!tb[IFLA_NUM_TX_QUEUES] && dev->num_tx_queues > 1) {\n\t\terr = netif_set_real_num_tx_queues(dev, 1);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (!tb[IFLA_NUM_RX_QUEUES] && dev->num_rx_queues > 1) {\n\t\terr = netif_set_real_num_rx_queues(dev, 1);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic int veth_newlink(struct net *src_net, struct net_device *dev,\n\t\t\tstruct nlattr *tb[], struct nlattr *data[],\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tint err;\n\tstruct net_device *peer;\n\tstruct veth_priv *priv;\n\tchar ifname[IFNAMSIZ];\n\tstruct nlattr *peer_tb[IFLA_MAX + 1], **tbp;\n\tunsigned char name_assign_type;\n\tstruct ifinfomsg *ifmp;\n\tstruct net *net;\n\n\t \n\tif (data != NULL && data[VETH_INFO_PEER] != NULL) {\n\t\tstruct nlattr *nla_peer;\n\n\t\tnla_peer = data[VETH_INFO_PEER];\n\t\tifmp = nla_data(nla_peer);\n\t\terr = rtnl_nla_parse_ifinfomsg(peer_tb, nla_peer, extack);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = veth_validate(peer_tb, NULL, extack);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\ttbp = peer_tb;\n\t} else {\n\t\tifmp = NULL;\n\t\ttbp = tb;\n\t}\n\n\tif (ifmp && tbp[IFLA_IFNAME]) {\n\t\tnla_strscpy(ifname, tbp[IFLA_IFNAME], IFNAMSIZ);\n\t\tname_assign_type = NET_NAME_USER;\n\t} else {\n\t\tsnprintf(ifname, IFNAMSIZ, DRV_NAME \"%%d\");\n\t\tname_assign_type = NET_NAME_ENUM;\n\t}\n\n\tnet = rtnl_link_get_net(src_net, tbp);\n\tif (IS_ERR(net))\n\t\treturn PTR_ERR(net);\n\n\tpeer = rtnl_create_link(net, ifname, name_assign_type,\n\t\t\t\t&veth_link_ops, tbp, extack);\n\tif (IS_ERR(peer)) {\n\t\tput_net(net);\n\t\treturn PTR_ERR(peer);\n\t}\n\n\tif (!ifmp || !tbp[IFLA_ADDRESS])\n\t\teth_hw_addr_random(peer);\n\n\tif (ifmp && (dev->ifindex != 0))\n\t\tpeer->ifindex = ifmp->ifi_index;\n\n\tnetif_inherit_tso_max(peer, dev);\n\n\terr = register_netdevice(peer);\n\tput_net(net);\n\tnet = NULL;\n\tif (err < 0)\n\t\tgoto err_register_peer;\n\n\t \n\tveth_disable_gro(peer);\n\tnetif_carrier_off(peer);\n\n\terr = rtnl_configure_link(peer, ifmp, 0, NULL);\n\tif (err < 0)\n\t\tgoto err_configure_peer;\n\n\t \n\n\tif (tb[IFLA_ADDRESS] == NULL)\n\t\teth_hw_addr_random(dev);\n\n\tif (tb[IFLA_IFNAME])\n\t\tnla_strscpy(dev->name, tb[IFLA_IFNAME], IFNAMSIZ);\n\telse\n\t\tsnprintf(dev->name, IFNAMSIZ, DRV_NAME \"%%d\");\n\n\terr = register_netdevice(dev);\n\tif (err < 0)\n\t\tgoto err_register_dev;\n\n\tnetif_carrier_off(dev);\n\n\t \n\n\tpriv = netdev_priv(dev);\n\trcu_assign_pointer(priv->peer, peer);\n\terr = veth_init_queues(dev, tb);\n\tif (err)\n\t\tgoto err_queues;\n\n\tpriv = netdev_priv(peer);\n\trcu_assign_pointer(priv->peer, dev);\n\terr = veth_init_queues(peer, tb);\n\tif (err)\n\t\tgoto err_queues;\n\n\tveth_disable_gro(dev);\n\t \n\tveth_set_xdp_features(dev);\n\tveth_set_xdp_features(peer);\n\n\treturn 0;\n\nerr_queues:\n\tunregister_netdevice(dev);\nerr_register_dev:\n\t \nerr_configure_peer:\n\tunregister_netdevice(peer);\n\treturn err;\n\nerr_register_peer:\n\tfree_netdev(peer);\n\treturn err;\n}\n\nstatic void veth_dellink(struct net_device *dev, struct list_head *head)\n{\n\tstruct veth_priv *priv;\n\tstruct net_device *peer;\n\n\tpriv = netdev_priv(dev);\n\tpeer = rtnl_dereference(priv->peer);\n\n\t \n\tRCU_INIT_POINTER(priv->peer, NULL);\n\tunregister_netdevice_queue(dev, head);\n\n\tif (peer) {\n\t\tpriv = netdev_priv(peer);\n\t\tRCU_INIT_POINTER(priv->peer, NULL);\n\t\tunregister_netdevice_queue(peer, head);\n\t}\n}\n\nstatic const struct nla_policy veth_policy[VETH_INFO_MAX + 1] = {\n\t[VETH_INFO_PEER]\t= { .len = sizeof(struct ifinfomsg) },\n};\n\nstatic struct net *veth_get_link_net(const struct net_device *dev)\n{\n\tstruct veth_priv *priv = netdev_priv(dev);\n\tstruct net_device *peer = rtnl_dereference(priv->peer);\n\n\treturn peer ? dev_net(peer) : dev_net(dev);\n}\n\nstatic unsigned int veth_get_num_queues(void)\n{\n\t \n\tint queues = num_possible_cpus();\n\n\tif (queues > 4096)\n\t\tqueues = 4096;\n\treturn queues;\n}\n\nstatic struct rtnl_link_ops veth_link_ops = {\n\t.kind\t\t= DRV_NAME,\n\t.priv_size\t= sizeof(struct veth_priv),\n\t.setup\t\t= veth_setup,\n\t.validate\t= veth_validate,\n\t.newlink\t= veth_newlink,\n\t.dellink\t= veth_dellink,\n\t.policy\t\t= veth_policy,\n\t.maxtype\t= VETH_INFO_MAX,\n\t.get_link_net\t= veth_get_link_net,\n\t.get_num_tx_queues\t= veth_get_num_queues,\n\t.get_num_rx_queues\t= veth_get_num_queues,\n};\n\n \n\nstatic __init int veth_init(void)\n{\n\treturn rtnl_link_register(&veth_link_ops);\n}\n\nstatic __exit void veth_exit(void)\n{\n\trtnl_link_unregister(&veth_link_ops);\n}\n\nmodule_init(veth_init);\nmodule_exit(veth_exit);\n\nMODULE_DESCRIPTION(\"Virtual Ethernet Tunnel\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_ALIAS_RTNL_LINK(DRV_NAME);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}