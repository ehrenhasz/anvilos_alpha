{
  "module_name": "tap.c",
  "hash_id": "7103cfda64591cd5e71cf8552b1590aec63c7c5133bcc5199706eb5903626149",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/tap.c",
  "human_readable_source": "\n#include <linux/etherdevice.h>\n#include <linux/if_tap.h>\n#include <linux/if_vlan.h>\n#include <linux/interrupt.h>\n#include <linux/nsproxy.h>\n#include <linux/compat.h>\n#include <linux/if_tun.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/cache.h>\n#include <linux/sched/signal.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/wait.h>\n#include <linux/cdev.h>\n#include <linux/idr.h>\n#include <linux/fs.h>\n#include <linux/uio.h>\n\n#include <net/gso.h>\n#include <net/net_namespace.h>\n#include <net/rtnetlink.h>\n#include <net/sock.h>\n#include <net/xdp.h>\n#include <linux/virtio_net.h>\n#include <linux/skb_array.h>\n\n#define TAP_IFFEATURES (IFF_VNET_HDR | IFF_MULTI_QUEUE)\n\n#define TAP_VNET_LE 0x80000000\n#define TAP_VNET_BE 0x40000000\n\n#ifdef CONFIG_TUN_VNET_CROSS_LE\nstatic inline bool tap_legacy_is_little_endian(struct tap_queue *q)\n{\n\treturn q->flags & TAP_VNET_BE ? false :\n\t\tvirtio_legacy_is_little_endian();\n}\n\nstatic long tap_get_vnet_be(struct tap_queue *q, int __user *sp)\n{\n\tint s = !!(q->flags & TAP_VNET_BE);\n\n\tif (put_user(s, sp))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long tap_set_vnet_be(struct tap_queue *q, int __user *sp)\n{\n\tint s;\n\n\tif (get_user(s, sp))\n\t\treturn -EFAULT;\n\n\tif (s)\n\t\tq->flags |= TAP_VNET_BE;\n\telse\n\t\tq->flags &= ~TAP_VNET_BE;\n\n\treturn 0;\n}\n#else\nstatic inline bool tap_legacy_is_little_endian(struct tap_queue *q)\n{\n\treturn virtio_legacy_is_little_endian();\n}\n\nstatic long tap_get_vnet_be(struct tap_queue *q, int __user *argp)\n{\n\treturn -EINVAL;\n}\n\nstatic long tap_set_vnet_be(struct tap_queue *q, int __user *argp)\n{\n\treturn -EINVAL;\n}\n#endif  \n\nstatic inline bool tap_is_little_endian(struct tap_queue *q)\n{\n\treturn q->flags & TAP_VNET_LE ||\n\t\ttap_legacy_is_little_endian(q);\n}\n\nstatic inline u16 tap16_to_cpu(struct tap_queue *q, __virtio16 val)\n{\n\treturn __virtio16_to_cpu(tap_is_little_endian(q), val);\n}\n\nstatic inline __virtio16 cpu_to_tap16(struct tap_queue *q, u16 val)\n{\n\treturn __cpu_to_virtio16(tap_is_little_endian(q), val);\n}\n\nstatic struct proto tap_proto = {\n\t.name = \"tap\",\n\t.owner = THIS_MODULE,\n\t.obj_size = sizeof(struct tap_queue),\n};\n\n#define TAP_NUM_DEVS (1U << MINORBITS)\n\nstatic LIST_HEAD(major_list);\n\nstruct major_info {\n\tstruct rcu_head rcu;\n\tdev_t major;\n\tstruct idr minor_idr;\n\tspinlock_t minor_lock;\n\tconst char *device_name;\n\tstruct list_head next;\n};\n\n#define GOODCOPY_LEN 128\n\nstatic const struct proto_ops tap_socket_ops;\n\n#define RX_OFFLOADS (NETIF_F_GRO | NETIF_F_LRO)\n#define TAP_FEATURES (NETIF_F_GSO | NETIF_F_SG | NETIF_F_FRAGLIST)\n\nstatic struct tap_dev *tap_dev_get_rcu(const struct net_device *dev)\n{\n\treturn rcu_dereference(dev->rx_handler_data);\n}\n\n \n\nstatic int tap_enable_queue(struct tap_dev *tap, struct file *file,\n\t\t\t    struct tap_queue *q)\n{\n\tint err = -EINVAL;\n\n\tASSERT_RTNL();\n\n\tif (q->enabled)\n\t\tgoto out;\n\n\terr = 0;\n\trcu_assign_pointer(tap->taps[tap->numvtaps], q);\n\tq->queue_index = tap->numvtaps;\n\tq->enabled = true;\n\n\ttap->numvtaps++;\nout:\n\treturn err;\n}\n\n \nstatic int tap_set_queue(struct tap_dev *tap, struct file *file,\n\t\t\t struct tap_queue *q)\n{\n\tif (tap->numqueues == MAX_TAP_QUEUES)\n\t\treturn -EBUSY;\n\n\trcu_assign_pointer(q->tap, tap);\n\trcu_assign_pointer(tap->taps[tap->numvtaps], q);\n\tsock_hold(&q->sk);\n\n\tq->file = file;\n\tq->queue_index = tap->numvtaps;\n\tq->enabled = true;\n\tfile->private_data = q;\n\tlist_add_tail(&q->next, &tap->queue_list);\n\n\ttap->numvtaps++;\n\ttap->numqueues++;\n\n\treturn 0;\n}\n\nstatic int tap_disable_queue(struct tap_queue *q)\n{\n\tstruct tap_dev *tap;\n\tstruct tap_queue *nq;\n\n\tASSERT_RTNL();\n\tif (!q->enabled)\n\t\treturn -EINVAL;\n\n\ttap = rtnl_dereference(q->tap);\n\n\tif (tap) {\n\t\tint index = q->queue_index;\n\t\tBUG_ON(index >= tap->numvtaps);\n\t\tnq = rtnl_dereference(tap->taps[tap->numvtaps - 1]);\n\t\tnq->queue_index = index;\n\n\t\trcu_assign_pointer(tap->taps[index], nq);\n\t\tRCU_INIT_POINTER(tap->taps[tap->numvtaps - 1], NULL);\n\t\tq->enabled = false;\n\n\t\ttap->numvtaps--;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void tap_put_queue(struct tap_queue *q)\n{\n\tstruct tap_dev *tap;\n\n\trtnl_lock();\n\ttap = rtnl_dereference(q->tap);\n\n\tif (tap) {\n\t\tif (q->enabled)\n\t\t\tBUG_ON(tap_disable_queue(q));\n\n\t\ttap->numqueues--;\n\t\tRCU_INIT_POINTER(q->tap, NULL);\n\t\tsock_put(&q->sk);\n\t\tlist_del_init(&q->next);\n\t}\n\n\trtnl_unlock();\n\n\tsynchronize_rcu();\n\tsock_put(&q->sk);\n}\n\n \nstatic struct tap_queue *tap_get_queue(struct tap_dev *tap,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tstruct tap_queue *queue = NULL;\n\t \n\tint numvtaps = READ_ONCE(tap->numvtaps);\n\t__u32 rxq;\n\n\tif (!numvtaps)\n\t\tgoto out;\n\n\tif (numvtaps == 1)\n\t\tgoto single;\n\n\t \n\trxq = skb_get_hash(skb);\n\tif (rxq) {\n\t\tqueue = rcu_dereference(tap->taps[rxq % numvtaps]);\n\t\tgoto out;\n\t}\n\n\tif (likely(skb_rx_queue_recorded(skb))) {\n\t\trxq = skb_get_rx_queue(skb);\n\n\t\twhile (unlikely(rxq >= numvtaps))\n\t\t\trxq -= numvtaps;\n\n\t\tqueue = rcu_dereference(tap->taps[rxq]);\n\t\tgoto out;\n\t}\n\nsingle:\n\tqueue = rcu_dereference(tap->taps[0]);\nout:\n\treturn queue;\n}\n\n \nvoid tap_del_queues(struct tap_dev *tap)\n{\n\tstruct tap_queue *q, *tmp;\n\n\tASSERT_RTNL();\n\tlist_for_each_entry_safe(q, tmp, &tap->queue_list, next) {\n\t\tlist_del_init(&q->next);\n\t\tRCU_INIT_POINTER(q->tap, NULL);\n\t\tif (q->enabled)\n\t\t\ttap->numvtaps--;\n\t\ttap->numqueues--;\n\t\tsock_put(&q->sk);\n\t}\n\tBUG_ON(tap->numvtaps);\n\tBUG_ON(tap->numqueues);\n\t \n\ttap->numvtaps = MAX_TAP_QUEUES;\n}\nEXPORT_SYMBOL_GPL(tap_del_queues);\n\nrx_handler_result_t tap_handle_frame(struct sk_buff **pskb)\n{\n\tstruct sk_buff *skb = *pskb;\n\tstruct net_device *dev = skb->dev;\n\tstruct tap_dev *tap;\n\tstruct tap_queue *q;\n\tnetdev_features_t features = TAP_FEATURES;\n\tenum skb_drop_reason drop_reason;\n\n\ttap = tap_dev_get_rcu(dev);\n\tif (!tap)\n\t\treturn RX_HANDLER_PASS;\n\n\tq = tap_get_queue(tap, skb);\n\tif (!q)\n\t\treturn RX_HANDLER_PASS;\n\n\tskb_push(skb, ETH_HLEN);\n\n\t \n\tif (q->flags & IFF_VNET_HDR)\n\t\tfeatures |= tap->tap_features;\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs = __skb_gso_segment(skb, features, false);\n\t\tstruct sk_buff *next;\n\n\t\tif (IS_ERR(segs)) {\n\t\t\tdrop_reason = SKB_DROP_REASON_SKB_GSO_SEG;\n\t\t\tgoto drop;\n\t\t}\n\n\t\tif (!segs) {\n\t\t\tif (ptr_ring_produce(&q->ring, skb)) {\n\t\t\t\tdrop_reason = SKB_DROP_REASON_FULL_RING;\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\tgoto wake_up;\n\t\t}\n\n\t\tconsume_skb(skb);\n\t\tskb_list_walk_safe(segs, skb, next) {\n\t\t\tskb_mark_not_on_list(skb);\n\t\t\tif (ptr_ring_produce(&q->ring, skb)) {\n\t\t\t\tdrop_reason = SKB_DROP_REASON_FULL_RING;\n\t\t\t\tkfree_skb_reason(skb, drop_reason);\n\t\t\t\tkfree_skb_list_reason(next, drop_reason);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t    !(features & NETIF_F_CSUM_MASK) &&\n\t\t    skb_checksum_help(skb)) {\n\t\t\tdrop_reason = SKB_DROP_REASON_SKB_CSUM;\n\t\t\tgoto drop;\n\t\t}\n\t\tif (ptr_ring_produce(&q->ring, skb)) {\n\t\t\tdrop_reason = SKB_DROP_REASON_FULL_RING;\n\t\t\tgoto drop;\n\t\t}\n\t}\n\nwake_up:\n\twake_up_interruptible_poll(sk_sleep(&q->sk), EPOLLIN | EPOLLRDNORM | EPOLLRDBAND);\n\treturn RX_HANDLER_CONSUMED;\n\ndrop:\n\t \n\tif (tap->count_rx_dropped)\n\t\ttap->count_rx_dropped(tap);\n\tkfree_skb_reason(skb, drop_reason);\n\treturn RX_HANDLER_CONSUMED;\n}\nEXPORT_SYMBOL_GPL(tap_handle_frame);\n\nstatic struct major_info *tap_get_major(int major)\n{\n\tstruct major_info *tap_major;\n\n\tlist_for_each_entry_rcu(tap_major, &major_list, next) {\n\t\tif (tap_major->major == major)\n\t\t\treturn tap_major;\n\t}\n\n\treturn NULL;\n}\n\nint tap_get_minor(dev_t major, struct tap_dev *tap)\n{\n\tint retval = -ENOMEM;\n\tstruct major_info *tap_major;\n\n\trcu_read_lock();\n\ttap_major = tap_get_major(MAJOR(major));\n\tif (!tap_major) {\n\t\tretval = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tspin_lock(&tap_major->minor_lock);\n\tretval = idr_alloc(&tap_major->minor_idr, tap, 1, TAP_NUM_DEVS, GFP_ATOMIC);\n\tif (retval >= 0) {\n\t\ttap->minor = retval;\n\t} else if (retval == -ENOSPC) {\n\t\tnetdev_err(tap->dev, \"Too many tap devices\\n\");\n\t\tretval = -EINVAL;\n\t}\n\tspin_unlock(&tap_major->minor_lock);\n\nunlock:\n\trcu_read_unlock();\n\treturn retval < 0 ? retval : 0;\n}\nEXPORT_SYMBOL_GPL(tap_get_minor);\n\nvoid tap_free_minor(dev_t major, struct tap_dev *tap)\n{\n\tstruct major_info *tap_major;\n\n\trcu_read_lock();\n\ttap_major = tap_get_major(MAJOR(major));\n\tif (!tap_major) {\n\t\tgoto unlock;\n\t}\n\n\tspin_lock(&tap_major->minor_lock);\n\tif (tap->minor) {\n\t\tidr_remove(&tap_major->minor_idr, tap->minor);\n\t\ttap->minor = 0;\n\t}\n\tspin_unlock(&tap_major->minor_lock);\n\nunlock:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(tap_free_minor);\n\nstatic struct tap_dev *dev_get_by_tap_file(int major, int minor)\n{\n\tstruct net_device *dev = NULL;\n\tstruct tap_dev *tap;\n\tstruct major_info *tap_major;\n\n\trcu_read_lock();\n\ttap_major = tap_get_major(major);\n\tif (!tap_major) {\n\t\ttap = NULL;\n\t\tgoto unlock;\n\t}\n\n\tspin_lock(&tap_major->minor_lock);\n\ttap = idr_find(&tap_major->minor_idr, minor);\n\tif (tap) {\n\t\tdev = tap->dev;\n\t\tdev_hold(dev);\n\t}\n\tspin_unlock(&tap_major->minor_lock);\n\nunlock:\n\trcu_read_unlock();\n\treturn tap;\n}\n\nstatic void tap_sock_write_space(struct sock *sk)\n{\n\twait_queue_head_t *wqueue;\n\n\tif (!sock_writeable(sk) ||\n\t    !test_and_clear_bit(SOCKWQ_ASYNC_NOSPACE, &sk->sk_socket->flags))\n\t\treturn;\n\n\twqueue = sk_sleep(sk);\n\tif (wqueue && waitqueue_active(wqueue))\n\t\twake_up_interruptible_poll(wqueue, EPOLLOUT | EPOLLWRNORM | EPOLLWRBAND);\n}\n\nstatic void tap_sock_destruct(struct sock *sk)\n{\n\tstruct tap_queue *q = container_of(sk, struct tap_queue, sk);\n\n\tptr_ring_cleanup(&q->ring, __skb_array_destroy_skb);\n}\n\nstatic int tap_open(struct inode *inode, struct file *file)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct tap_dev *tap;\n\tstruct tap_queue *q;\n\tint err = -ENODEV;\n\n\trtnl_lock();\n\ttap = dev_get_by_tap_file(imajor(inode), iminor(inode));\n\tif (!tap)\n\t\tgoto err;\n\n\terr = -ENOMEM;\n\tq = (struct tap_queue *)sk_alloc(net, AF_UNSPEC, GFP_KERNEL,\n\t\t\t\t\t     &tap_proto, 0);\n\tif (!q)\n\t\tgoto err;\n\tif (ptr_ring_init(&q->ring, tap->dev->tx_queue_len, GFP_KERNEL)) {\n\t\tsk_free(&q->sk);\n\t\tgoto err;\n\t}\n\n\tinit_waitqueue_head(&q->sock.wq.wait);\n\tq->sock.type = SOCK_RAW;\n\tq->sock.state = SS_CONNECTED;\n\tq->sock.file = file;\n\tq->sock.ops = &tap_socket_ops;\n\tsock_init_data_uid(&q->sock, &q->sk, current_fsuid());\n\tq->sk.sk_write_space = tap_sock_write_space;\n\tq->sk.sk_destruct = tap_sock_destruct;\n\tq->flags = IFF_VNET_HDR | IFF_NO_PI | IFF_TAP;\n\tq->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\n\n\t \n\tif ((tap->dev->features & NETIF_F_HIGHDMA) && (tap->dev->features & NETIF_F_SG))\n\t\tsock_set_flag(&q->sk, SOCK_ZEROCOPY);\n\n\terr = tap_set_queue(tap, file, q);\n\tif (err) {\n\t\t \n\t\tgoto err_put;\n\t}\n\n\t \n\tfile->f_mode |= FMODE_NOWAIT;\n\n\tdev_put(tap->dev);\n\n\trtnl_unlock();\n\treturn err;\n\nerr_put:\n\tsock_put(&q->sk);\nerr:\n\tif (tap)\n\t\tdev_put(tap->dev);\n\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic int tap_release(struct inode *inode, struct file *file)\n{\n\tstruct tap_queue *q = file->private_data;\n\ttap_put_queue(q);\n\treturn 0;\n}\n\nstatic __poll_t tap_poll(struct file *file, poll_table *wait)\n{\n\tstruct tap_queue *q = file->private_data;\n\t__poll_t mask = EPOLLERR;\n\n\tif (!q)\n\t\tgoto out;\n\n\tmask = 0;\n\tpoll_wait(file, &q->sock.wq.wait, wait);\n\n\tif (!ptr_ring_empty(&q->ring))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\tif (sock_writeable(&q->sk) ||\n\t    (!test_and_set_bit(SOCKWQ_ASYNC_NOSPACE, &q->sock.flags) &&\n\t     sock_writeable(&q->sk)))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\nout:\n\treturn mask;\n}\n\nstatic inline struct sk_buff *tap_alloc_skb(struct sock *sk, size_t prepad,\n\t\t\t\t\t    size_t len, size_t linear,\n\t\t\t\t\t\tint noblock, int *err)\n{\n\tstruct sk_buff *skb;\n\n\t \n\tif (prepad + len < PAGE_SIZE || !linear)\n\t\tlinear = len;\n\n\tif (len - linear > MAX_SKB_FRAGS * (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER))\n\t\tlinear = len - MAX_SKB_FRAGS * (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER);\n\tskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\n\t\t\t\t   err, PAGE_ALLOC_COSTLY_ORDER);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, prepad);\n\tskb_put(skb, linear);\n\tskb->data_len = len - linear;\n\tskb->len += len - linear;\n\n\treturn skb;\n}\n\n \n#define TAP_RESERVE HH_DATA_OFF(ETH_HLEN)\n\n \nstatic ssize_t tap_get_user(struct tap_queue *q, void *msg_control,\n\t\t\t    struct iov_iter *from, int noblock)\n{\n\tint good_linear = SKB_MAX_HEAD(TAP_RESERVE);\n\tstruct sk_buff *skb;\n\tstruct tap_dev *tap;\n\tunsigned long total_len = iov_iter_count(from);\n\tunsigned long len = total_len;\n\tint err;\n\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\tint vnet_hdr_len = 0;\n\tint copylen = 0;\n\tint depth;\n\tbool zerocopy = false;\n\tsize_t linear;\n\tenum skb_drop_reason drop_reason;\n\n\tif (q->flags & IFF_VNET_HDR) {\n\t\tvnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);\n\n\t\terr = -EINVAL;\n\t\tif (len < vnet_hdr_len)\n\t\t\tgoto err;\n\t\tlen -= vnet_hdr_len;\n\n\t\terr = -EFAULT;\n\t\tif (!copy_from_iter_full(&vnet_hdr, sizeof(vnet_hdr), from))\n\t\t\tgoto err;\n\t\tiov_iter_advance(from, vnet_hdr_len - sizeof(vnet_hdr));\n\t\tif ((vnet_hdr.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t\t     tap16_to_cpu(q, vnet_hdr.csum_start) +\n\t\t     tap16_to_cpu(q, vnet_hdr.csum_offset) + 2 >\n\t\t\t     tap16_to_cpu(q, vnet_hdr.hdr_len))\n\t\t\tvnet_hdr.hdr_len = cpu_to_tap16(q,\n\t\t\t\t tap16_to_cpu(q, vnet_hdr.csum_start) +\n\t\t\t\t tap16_to_cpu(q, vnet_hdr.csum_offset) + 2);\n\t\terr = -EINVAL;\n\t\tif (tap16_to_cpu(q, vnet_hdr.hdr_len) > len)\n\t\t\tgoto err;\n\t}\n\n\terr = -EINVAL;\n\tif (unlikely(len < ETH_HLEN))\n\t\tgoto err;\n\n\tif (msg_control && sock_flag(&q->sk, SOCK_ZEROCOPY)) {\n\t\tstruct iov_iter i;\n\n\t\tcopylen = vnet_hdr.hdr_len ?\n\t\t\ttap16_to_cpu(q, vnet_hdr.hdr_len) : GOODCOPY_LEN;\n\t\tif (copylen > good_linear)\n\t\t\tcopylen = good_linear;\n\t\telse if (copylen < ETH_HLEN)\n\t\t\tcopylen = ETH_HLEN;\n\t\tlinear = copylen;\n\t\ti = *from;\n\t\tiov_iter_advance(&i, copylen);\n\t\tif (iov_iter_npages(&i, INT_MAX) <= MAX_SKB_FRAGS)\n\t\t\tzerocopy = true;\n\t}\n\n\tif (!zerocopy) {\n\t\tcopylen = len;\n\t\tlinear = tap16_to_cpu(q, vnet_hdr.hdr_len);\n\t\tif (linear > good_linear)\n\t\t\tlinear = good_linear;\n\t\telse if (linear < ETH_HLEN)\n\t\t\tlinear = ETH_HLEN;\n\t}\n\n\tskb = tap_alloc_skb(&q->sk, TAP_RESERVE, copylen,\n\t\t\t    linear, noblock, &err);\n\tif (!skb)\n\t\tgoto err;\n\n\tif (zerocopy)\n\t\terr = zerocopy_sg_from_iter(skb, from);\n\telse\n\t\terr = skb_copy_datagram_from_iter(skb, 0, from, len);\n\n\tif (err) {\n\t\tdrop_reason = SKB_DROP_REASON_SKB_UCOPY_FAULT;\n\t\tgoto err_kfree;\n\t}\n\n\tskb_set_network_header(skb, ETH_HLEN);\n\tskb_reset_mac_header(skb);\n\tskb->protocol = eth_hdr(skb)->h_proto;\n\n\trcu_read_lock();\n\ttap = rcu_dereference(q->tap);\n\tif (!tap) {\n\t\tkfree_skb(skb);\n\t\trcu_read_unlock();\n\t\treturn total_len;\n\t}\n\tskb->dev = tap->dev;\n\n\tif (vnet_hdr_len) {\n\t\terr = virtio_net_hdr_to_skb(skb, &vnet_hdr,\n\t\t\t\t\t    tap_is_little_endian(q));\n\t\tif (err) {\n\t\t\trcu_read_unlock();\n\t\t\tdrop_reason = SKB_DROP_REASON_DEV_HDR;\n\t\t\tgoto err_kfree;\n\t\t}\n\t}\n\n\tskb_probe_transport_header(skb);\n\n\t \n\tif (eth_type_vlan(skb->protocol) &&\n\t    vlan_get_protocol_and_depth(skb, skb->protocol, &depth) != 0)\n\t\tskb_set_network_header(skb, depth);\n\n\t \n\tif (zerocopy) {\n\t\tskb_zcopy_init(skb, msg_control);\n\t} else if (msg_control) {\n\t\tstruct ubuf_info *uarg = msg_control;\n\t\tuarg->callback(NULL, uarg, false);\n\t}\n\n\tdev_queue_xmit(skb);\n\trcu_read_unlock();\n\treturn total_len;\n\nerr_kfree:\n\tkfree_skb_reason(skb, drop_reason);\n\nerr:\n\trcu_read_lock();\n\ttap = rcu_dereference(q->tap);\n\tif (tap && tap->count_tx_dropped)\n\t\ttap->count_tx_dropped(tap);\n\trcu_read_unlock();\n\n\treturn err;\n}\n\nstatic ssize_t tap_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct tap_queue *q = file->private_data;\n\tint noblock = 0;\n\n\tif ((file->f_flags & O_NONBLOCK) || (iocb->ki_flags & IOCB_NOWAIT))\n\t\tnoblock = 1;\n\n\treturn tap_get_user(q, NULL, from, noblock);\n}\n\n \nstatic ssize_t tap_put_user(struct tap_queue *q,\n\t\t\t    const struct sk_buff *skb,\n\t\t\t    struct iov_iter *iter)\n{\n\tint ret;\n\tint vnet_hdr_len = 0;\n\tint vlan_offset = 0;\n\tint total;\n\n\tif (q->flags & IFF_VNET_HDR) {\n\t\tint vlan_hlen = skb_vlan_tag_present(skb) ? VLAN_HLEN : 0;\n\t\tstruct virtio_net_hdr vnet_hdr;\n\n\t\tvnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);\n\t\tif (iov_iter_count(iter) < vnet_hdr_len)\n\t\t\treturn -EINVAL;\n\n\t\tif (virtio_net_hdr_from_skb(skb, &vnet_hdr,\n\t\t\t\t\t    tap_is_little_endian(q), true,\n\t\t\t\t\t    vlan_hlen))\n\t\t\tBUG();\n\n\t\tif (copy_to_iter(&vnet_hdr, sizeof(vnet_hdr), iter) !=\n\t\t    sizeof(vnet_hdr))\n\t\t\treturn -EFAULT;\n\n\t\tiov_iter_advance(iter, vnet_hdr_len - sizeof(vnet_hdr));\n\t}\n\ttotal = vnet_hdr_len;\n\ttotal += skb->len;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tstruct {\n\t\t\t__be16 h_vlan_proto;\n\t\t\t__be16 h_vlan_TCI;\n\t\t} veth;\n\t\tveth.h_vlan_proto = skb->vlan_proto;\n\t\tveth.h_vlan_TCI = htons(skb_vlan_tag_get(skb));\n\n\t\tvlan_offset = offsetof(struct vlan_ethhdr, h_vlan_proto);\n\t\ttotal += VLAN_HLEN;\n\n\t\tret = skb_copy_datagram_iter(skb, 0, iter, vlan_offset);\n\t\tif (ret || !iov_iter_count(iter))\n\t\t\tgoto done;\n\n\t\tret = copy_to_iter(&veth, sizeof(veth), iter);\n\t\tif (ret != sizeof(veth) || !iov_iter_count(iter))\n\t\t\tgoto done;\n\t}\n\n\tret = skb_copy_datagram_iter(skb, vlan_offset, iter,\n\t\t\t\t     skb->len - vlan_offset);\n\ndone:\n\treturn ret ? ret : total;\n}\n\nstatic ssize_t tap_do_read(struct tap_queue *q,\n\t\t\t   struct iov_iter *to,\n\t\t\t   int noblock, struct sk_buff *skb)\n{\n\tDEFINE_WAIT(wait);\n\tssize_t ret = 0;\n\n\tif (!iov_iter_count(to)) {\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\n\tif (skb)\n\t\tgoto put;\n\n\twhile (1) {\n\t\tif (!noblock)\n\t\t\tprepare_to_wait(sk_sleep(&q->sk), &wait,\n\t\t\t\t\tTASK_INTERRUPTIBLE);\n\n\t\t \n\t\tskb = ptr_ring_consume(&q->ring);\n\t\tif (skb)\n\t\t\tbreak;\n\t\tif (noblock) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tschedule();\n\t}\n\tif (!noblock)\n\t\tfinish_wait(sk_sleep(&q->sk), &wait);\n\nput:\n\tif (skb) {\n\t\tret = tap_put_user(q, skb, to);\n\t\tif (unlikely(ret < 0))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\tconsume_skb(skb);\n\t}\n\treturn ret;\n}\n\nstatic ssize_t tap_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct tap_queue *q = file->private_data;\n\tssize_t len = iov_iter_count(to), ret;\n\tint noblock = 0;\n\n\tif ((file->f_flags & O_NONBLOCK) || (iocb->ki_flags & IOCB_NOWAIT))\n\t\tnoblock = 1;\n\n\tret = tap_do_read(q, to, noblock, NULL);\n\tret = min_t(ssize_t, ret, len);\n\tif (ret > 0)\n\t\tiocb->ki_pos = ret;\n\treturn ret;\n}\n\nstatic struct tap_dev *tap_get_tap_dev(struct tap_queue *q)\n{\n\tstruct tap_dev *tap;\n\n\tASSERT_RTNL();\n\ttap = rtnl_dereference(q->tap);\n\tif (tap)\n\t\tdev_hold(tap->dev);\n\n\treturn tap;\n}\n\nstatic void tap_put_tap_dev(struct tap_dev *tap)\n{\n\tdev_put(tap->dev);\n}\n\nstatic int tap_ioctl_set_queue(struct file *file, unsigned int flags)\n{\n\tstruct tap_queue *q = file->private_data;\n\tstruct tap_dev *tap;\n\tint ret;\n\n\ttap = tap_get_tap_dev(q);\n\tif (!tap)\n\t\treturn -EINVAL;\n\n\tif (flags & IFF_ATTACH_QUEUE)\n\t\tret = tap_enable_queue(tap, file, q);\n\telse if (flags & IFF_DETACH_QUEUE)\n\t\tret = tap_disable_queue(q);\n\telse\n\t\tret = -EINVAL;\n\n\ttap_put_tap_dev(tap);\n\treturn ret;\n}\n\nstatic int set_offload(struct tap_queue *q, unsigned long arg)\n{\n\tstruct tap_dev *tap;\n\tnetdev_features_t features;\n\tnetdev_features_t feature_mask = 0;\n\n\ttap = rtnl_dereference(q->tap);\n\tif (!tap)\n\t\treturn -ENOLINK;\n\n\tfeatures = tap->dev->features;\n\n\tif (arg & TUN_F_CSUM) {\n\t\tfeature_mask = NETIF_F_HW_CSUM;\n\n\t\tif (arg & (TUN_F_TSO4 | TUN_F_TSO6)) {\n\t\t\tif (arg & TUN_F_TSO_ECN)\n\t\t\t\tfeature_mask |= NETIF_F_TSO_ECN;\n\t\t\tif (arg & TUN_F_TSO4)\n\t\t\t\tfeature_mask |= NETIF_F_TSO;\n\t\t\tif (arg & TUN_F_TSO6)\n\t\t\t\tfeature_mask |= NETIF_F_TSO6;\n\t\t}\n\n\t\t \n\t\tif ((arg & (TUN_F_USO4 | TUN_F_USO6)) == (TUN_F_USO4 | TUN_F_USO6))\n\t\t\tfeatures |= NETIF_F_GSO_UDP_L4;\n\t}\n\n\t \n\tif (feature_mask & (NETIF_F_TSO | NETIF_F_TSO6) ||\n\t    (feature_mask & (TUN_F_USO4 | TUN_F_USO6)) == (TUN_F_USO4 | TUN_F_USO6))\n\t\tfeatures |= RX_OFFLOADS;\n\telse\n\t\tfeatures &= ~RX_OFFLOADS;\n\n\t \n\ttap->tap_features = feature_mask;\n\tif (tap->update_features)\n\t\ttap->update_features(tap, features);\n\n\treturn 0;\n}\n\n \nstatic long tap_ioctl(struct file *file, unsigned int cmd,\n\t\t      unsigned long arg)\n{\n\tstruct tap_queue *q = file->private_data;\n\tstruct tap_dev *tap;\n\tvoid __user *argp = (void __user *)arg;\n\tstruct ifreq __user *ifr = argp;\n\tunsigned int __user *up = argp;\n\tunsigned short u;\n\tint __user *sp = argp;\n\tstruct sockaddr sa;\n\tint s;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase TUNSETIFF:\n\t\t \n\t\tif (get_user(u, &ifr->ifr_flags))\n\t\t\treturn -EFAULT;\n\n\t\tret = 0;\n\t\tif ((u & ~TAP_IFFEATURES) != (IFF_NO_PI | IFF_TAP))\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tq->flags = (q->flags & ~TAP_IFFEATURES) | u;\n\n\t\treturn ret;\n\n\tcase TUNGETIFF:\n\t\trtnl_lock();\n\t\ttap = tap_get_tap_dev(q);\n\t\tif (!tap) {\n\t\t\trtnl_unlock();\n\t\t\treturn -ENOLINK;\n\t\t}\n\n\t\tret = 0;\n\t\tu = q->flags;\n\t\tif (copy_to_user(&ifr->ifr_name, tap->dev->name, IFNAMSIZ) ||\n\t\t    put_user(u, &ifr->ifr_flags))\n\t\t\tret = -EFAULT;\n\t\ttap_put_tap_dev(tap);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\tcase TUNSETQUEUE:\n\t\tif (get_user(u, &ifr->ifr_flags))\n\t\t\treturn -EFAULT;\n\t\trtnl_lock();\n\t\tret = tap_ioctl_set_queue(file, u);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\tcase TUNGETFEATURES:\n\t\tif (put_user(IFF_TAP | IFF_NO_PI | TAP_IFFEATURES, up))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tcase TUNSETSNDBUF:\n\t\tif (get_user(s, sp))\n\t\t\treturn -EFAULT;\n\t\tif (s <= 0)\n\t\t\treturn -EINVAL;\n\n\t\tq->sk.sk_sndbuf = s;\n\t\treturn 0;\n\n\tcase TUNGETVNETHDRSZ:\n\t\ts = q->vnet_hdr_sz;\n\t\tif (put_user(s, sp))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tcase TUNSETVNETHDRSZ:\n\t\tif (get_user(s, sp))\n\t\t\treturn -EFAULT;\n\t\tif (s < (int)sizeof(struct virtio_net_hdr))\n\t\t\treturn -EINVAL;\n\n\t\tq->vnet_hdr_sz = s;\n\t\treturn 0;\n\n\tcase TUNGETVNETLE:\n\t\ts = !!(q->flags & TAP_VNET_LE);\n\t\tif (put_user(s, sp))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tcase TUNSETVNETLE:\n\t\tif (get_user(s, sp))\n\t\t\treturn -EFAULT;\n\t\tif (s)\n\t\t\tq->flags |= TAP_VNET_LE;\n\t\telse\n\t\t\tq->flags &= ~TAP_VNET_LE;\n\t\treturn 0;\n\n\tcase TUNGETVNETBE:\n\t\treturn tap_get_vnet_be(q, sp);\n\n\tcase TUNSETVNETBE:\n\t\treturn tap_set_vnet_be(q, sp);\n\n\tcase TUNSETOFFLOAD:\n\t\t \n\t\tif (arg & ~(TUN_F_CSUM | TUN_F_TSO4 | TUN_F_TSO6 |\n\t\t\t    TUN_F_TSO_ECN | TUN_F_UFO |\n\t\t\t    TUN_F_USO4 | TUN_F_USO6))\n\t\t\treturn -EINVAL;\n\n\t\trtnl_lock();\n\t\tret = set_offload(q, arg);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\tcase SIOCGIFHWADDR:\n\t\trtnl_lock();\n\t\ttap = tap_get_tap_dev(q);\n\t\tif (!tap) {\n\t\t\trtnl_unlock();\n\t\t\treturn -ENOLINK;\n\t\t}\n\t\tret = 0;\n\t\tdev_get_mac_address(&sa, dev_net(tap->dev), tap->dev->name);\n\t\tif (copy_to_user(&ifr->ifr_name, tap->dev->name, IFNAMSIZ) ||\n\t\t    copy_to_user(&ifr->ifr_hwaddr, &sa, sizeof(sa)))\n\t\t\tret = -EFAULT;\n\t\ttap_put_tap_dev(tap);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\tcase SIOCSIFHWADDR:\n\t\tif (copy_from_user(&sa, &ifr->ifr_hwaddr, sizeof(sa)))\n\t\t\treturn -EFAULT;\n\t\trtnl_lock();\n\t\ttap = tap_get_tap_dev(q);\n\t\tif (!tap) {\n\t\t\trtnl_unlock();\n\t\t\treturn -ENOLINK;\n\t\t}\n\t\tret = dev_set_mac_address_user(tap->dev, &sa, NULL);\n\t\ttap_put_tap_dev(tap);\n\t\trtnl_unlock();\n\t\treturn ret;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic const struct file_operations tap_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= tap_open,\n\t.release\t= tap_release,\n\t.read_iter\t= tap_read_iter,\n\t.write_iter\t= tap_write_iter,\n\t.poll\t\t= tap_poll,\n\t.llseek\t\t= no_llseek,\n\t.unlocked_ioctl\t= tap_ioctl,\n\t.compat_ioctl\t= compat_ptr_ioctl,\n};\n\nstatic int tap_get_user_xdp(struct tap_queue *q, struct xdp_buff *xdp)\n{\n\tstruct tun_xdp_hdr *hdr = xdp->data_hard_start;\n\tstruct virtio_net_hdr *gso = &hdr->gso;\n\tint buflen = hdr->buflen;\n\tint vnet_hdr_len = 0;\n\tstruct tap_dev *tap;\n\tstruct sk_buff *skb;\n\tint err, depth;\n\n\tif (q->flags & IFF_VNET_HDR)\n\t\tvnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);\n\n\tskb = build_skb(xdp->data_hard_start, buflen);\n\tif (!skb) {\n\t\terr = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\tskb_put(skb, xdp->data_end - xdp->data);\n\n\tskb_set_network_header(skb, ETH_HLEN);\n\tskb_reset_mac_header(skb);\n\tskb->protocol = eth_hdr(skb)->h_proto;\n\n\tif (vnet_hdr_len) {\n\t\terr = virtio_net_hdr_to_skb(skb, gso, tap_is_little_endian(q));\n\t\tif (err)\n\t\t\tgoto err_kfree;\n\t}\n\n\t \n\tif (eth_type_vlan(skb->protocol) &&\n\t    vlan_get_protocol_and_depth(skb, skb->protocol, &depth) != 0)\n\t\tskb_set_network_header(skb, depth);\n\n\trcu_read_lock();\n\ttap = rcu_dereference(q->tap);\n\tif (tap) {\n\t\tskb->dev = tap->dev;\n\t\tskb_probe_transport_header(skb);\n\t\tdev_queue_xmit(skb);\n\t} else {\n\t\tkfree_skb(skb);\n\t}\n\trcu_read_unlock();\n\n\treturn 0;\n\nerr_kfree:\n\tkfree_skb(skb);\nerr:\n\trcu_read_lock();\n\ttap = rcu_dereference(q->tap);\n\tif (tap && tap->count_tx_dropped)\n\t\ttap->count_tx_dropped(tap);\n\trcu_read_unlock();\n\treturn err;\n}\n\nstatic int tap_sendmsg(struct socket *sock, struct msghdr *m,\n\t\t       size_t total_len)\n{\n\tstruct tap_queue *q = container_of(sock, struct tap_queue, sock);\n\tstruct tun_msg_ctl *ctl = m->msg_control;\n\tstruct xdp_buff *xdp;\n\tint i;\n\n\tif (m->msg_controllen == sizeof(struct tun_msg_ctl) &&\n\t    ctl && ctl->type == TUN_MSG_PTR) {\n\t\tfor (i = 0; i < ctl->num; i++) {\n\t\t\txdp = &((struct xdp_buff *)ctl->ptr)[i];\n\t\t\ttap_get_user_xdp(q, xdp);\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn tap_get_user(q, ctl ? ctl->ptr : NULL, &m->msg_iter,\n\t\t\t    m->msg_flags & MSG_DONTWAIT);\n}\n\nstatic int tap_recvmsg(struct socket *sock, struct msghdr *m,\n\t\t       size_t total_len, int flags)\n{\n\tstruct tap_queue *q = container_of(sock, struct tap_queue, sock);\n\tstruct sk_buff *skb = m->msg_control;\n\tint ret;\n\tif (flags & ~(MSG_DONTWAIT|MSG_TRUNC)) {\n\t\tkfree_skb(skb);\n\t\treturn -EINVAL;\n\t}\n\tret = tap_do_read(q, &m->msg_iter, flags & MSG_DONTWAIT, skb);\n\tif (ret > total_len) {\n\t\tm->msg_flags |= MSG_TRUNC;\n\t\tret = flags & MSG_TRUNC ? ret : total_len;\n\t}\n\treturn ret;\n}\n\nstatic int tap_peek_len(struct socket *sock)\n{\n\tstruct tap_queue *q = container_of(sock, struct tap_queue,\n\t\t\t\t\t       sock);\n\treturn PTR_RING_PEEK_CALL(&q->ring, __skb_array_len_with_tag);\n}\n\n \nstatic const struct proto_ops tap_socket_ops = {\n\t.sendmsg = tap_sendmsg,\n\t.recvmsg = tap_recvmsg,\n\t.peek_len = tap_peek_len,\n};\n\n \nstruct socket *tap_get_socket(struct file *file)\n{\n\tstruct tap_queue *q;\n\tif (file->f_op != &tap_fops)\n\t\treturn ERR_PTR(-EINVAL);\n\tq = file->private_data;\n\tif (!q)\n\t\treturn ERR_PTR(-EBADFD);\n\treturn &q->sock;\n}\nEXPORT_SYMBOL_GPL(tap_get_socket);\n\nstruct ptr_ring *tap_get_ptr_ring(struct file *file)\n{\n\tstruct tap_queue *q;\n\n\tif (file->f_op != &tap_fops)\n\t\treturn ERR_PTR(-EINVAL);\n\tq = file->private_data;\n\tif (!q)\n\t\treturn ERR_PTR(-EBADFD);\n\treturn &q->ring;\n}\nEXPORT_SYMBOL_GPL(tap_get_ptr_ring);\n\nint tap_queue_resize(struct tap_dev *tap)\n{\n\tstruct net_device *dev = tap->dev;\n\tstruct tap_queue *q;\n\tstruct ptr_ring **rings;\n\tint n = tap->numqueues;\n\tint ret, i = 0;\n\n\trings = kmalloc_array(n, sizeof(*rings), GFP_KERNEL);\n\tif (!rings)\n\t\treturn -ENOMEM;\n\n\tlist_for_each_entry(q, &tap->queue_list, next)\n\t\trings[i++] = &q->ring;\n\n\tret = ptr_ring_resize_multiple(rings, n,\n\t\t\t\t       dev->tx_queue_len, GFP_KERNEL,\n\t\t\t\t       __skb_array_destroy_skb);\n\n\tkfree(rings);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(tap_queue_resize);\n\nstatic int tap_list_add(dev_t major, const char *device_name)\n{\n\tstruct major_info *tap_major;\n\n\ttap_major = kzalloc(sizeof(*tap_major), GFP_ATOMIC);\n\tif (!tap_major)\n\t\treturn -ENOMEM;\n\n\ttap_major->major = MAJOR(major);\n\n\tidr_init(&tap_major->minor_idr);\n\tspin_lock_init(&tap_major->minor_lock);\n\n\ttap_major->device_name = device_name;\n\n\tlist_add_tail_rcu(&tap_major->next, &major_list);\n\treturn 0;\n}\n\nint tap_create_cdev(struct cdev *tap_cdev, dev_t *tap_major,\n\t\t    const char *device_name, struct module *module)\n{\n\tint err;\n\n\terr = alloc_chrdev_region(tap_major, 0, TAP_NUM_DEVS, device_name);\n\tif (err)\n\t\tgoto out1;\n\n\tcdev_init(tap_cdev, &tap_fops);\n\ttap_cdev->owner = module;\n\terr = cdev_add(tap_cdev, *tap_major, TAP_NUM_DEVS);\n\tif (err)\n\t\tgoto out2;\n\n\terr =  tap_list_add(*tap_major, device_name);\n\tif (err)\n\t\tgoto out3;\n\n\treturn 0;\n\nout3:\n\tcdev_del(tap_cdev);\nout2:\n\tunregister_chrdev_region(*tap_major, TAP_NUM_DEVS);\nout1:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(tap_create_cdev);\n\nvoid tap_destroy_cdev(dev_t major, struct cdev *tap_cdev)\n{\n\tstruct major_info *tap_major, *tmp;\n\n\tcdev_del(tap_cdev);\n\tunregister_chrdev_region(major, TAP_NUM_DEVS);\n\tlist_for_each_entry_safe(tap_major, tmp, &major_list, next) {\n\t\tif (tap_major->major == MAJOR(major)) {\n\t\t\tidr_destroy(&tap_major->minor_idr);\n\t\t\tlist_del_rcu(&tap_major->next);\n\t\t\tkfree_rcu(tap_major, rcu);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(tap_destroy_cdev);\n\nMODULE_AUTHOR(\"Arnd Bergmann <arnd@arndb.de>\");\nMODULE_AUTHOR(\"Sainath Grandhi <sainath.grandhi@intel.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}