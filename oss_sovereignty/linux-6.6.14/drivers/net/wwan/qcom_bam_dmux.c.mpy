{
  "module_name": "qcom_bam_dmux.c",
  "hash_id": "19990ab7a78f740abce2610355ff676268778f9953fa391e9316e48b54dfe577",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wwan/qcom_bam_dmux.c",
  "human_readable_source": "\n \n\n#include <linux/atomic.h>\n#include <linux/bitops.h>\n#include <linux/completion.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/if_arp.h>\n#include <linux/interrupt.h>\n#include <linux/mod_devicetable.h>\n#include <linux/module.h>\n#include <linux/netdevice.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/soc/qcom/smem_state.h>\n#include <linux/spinlock.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n#include <net/pkt_sched.h>\n\n#define BAM_DMUX_BUFFER_SIZE\t\tSZ_2K\n#define BAM_DMUX_HDR_SIZE\t\tsizeof(struct bam_dmux_hdr)\n#define BAM_DMUX_MAX_DATA_SIZE\t\t(BAM_DMUX_BUFFER_SIZE - BAM_DMUX_HDR_SIZE)\n#define BAM_DMUX_NUM_SKB\t\t32\n\n#define BAM_DMUX_HDR_MAGIC\t\t0x33fc\n\n#define BAM_DMUX_AUTOSUSPEND_DELAY\t1000\n#define BAM_DMUX_REMOTE_TIMEOUT\t\tmsecs_to_jiffies(2000)\n\nenum {\n\tBAM_DMUX_CMD_DATA,\n\tBAM_DMUX_CMD_OPEN,\n\tBAM_DMUX_CMD_CLOSE,\n};\n\nenum {\n\tBAM_DMUX_CH_DATA_0,\n\tBAM_DMUX_CH_DATA_1,\n\tBAM_DMUX_CH_DATA_2,\n\tBAM_DMUX_CH_DATA_3,\n\tBAM_DMUX_CH_DATA_4,\n\tBAM_DMUX_CH_DATA_5,\n\tBAM_DMUX_CH_DATA_6,\n\tBAM_DMUX_CH_DATA_7,\n\tBAM_DMUX_NUM_CH\n};\n\nstruct bam_dmux_hdr {\n\tu16 magic;\n\tu8 signal;\n\tu8 cmd;\n\tu8 pad;\n\tu8 ch;\n\tu16 len;\n};\n\nstruct bam_dmux_skb_dma {\n\tstruct bam_dmux *dmux;\n\tstruct sk_buff *skb;\n\tdma_addr_t addr;\n};\n\nstruct bam_dmux {\n\tstruct device *dev;\n\n\tint pc_irq;\n\tbool pc_state, pc_ack_state;\n\tstruct qcom_smem_state *pc, *pc_ack;\n\tu32 pc_mask, pc_ack_mask;\n\twait_queue_head_t pc_wait;\n\tstruct completion pc_ack_completion;\n\n\tstruct dma_chan *rx, *tx;\n\tstruct bam_dmux_skb_dma rx_skbs[BAM_DMUX_NUM_SKB];\n\tstruct bam_dmux_skb_dma tx_skbs[BAM_DMUX_NUM_SKB];\n\tspinlock_t tx_lock;  \n\tunsigned int tx_next_skb;\n\tatomic_long_t tx_deferred_skb;\n\tstruct work_struct tx_wakeup_work;\n\n\tDECLARE_BITMAP(remote_channels, BAM_DMUX_NUM_CH);\n\tstruct work_struct register_netdev_work;\n\tstruct net_device *netdevs[BAM_DMUX_NUM_CH];\n};\n\nstruct bam_dmux_netdev {\n\tstruct bam_dmux *dmux;\n\tu8 ch;\n};\n\nstatic void bam_dmux_pc_vote(struct bam_dmux *dmux, bool enable)\n{\n\treinit_completion(&dmux->pc_ack_completion);\n\tqcom_smem_state_update_bits(dmux->pc, dmux->pc_mask,\n\t\t\t\t    enable ? dmux->pc_mask : 0);\n}\n\nstatic void bam_dmux_pc_ack(struct bam_dmux *dmux)\n{\n\tqcom_smem_state_update_bits(dmux->pc_ack, dmux->pc_ack_mask,\n\t\t\t\t    dmux->pc_ack_state ? 0 : dmux->pc_ack_mask);\n\tdmux->pc_ack_state = !dmux->pc_ack_state;\n}\n\nstatic bool bam_dmux_skb_dma_map(struct bam_dmux_skb_dma *skb_dma,\n\t\t\t\t enum dma_data_direction dir)\n{\n\tstruct device *dev = skb_dma->dmux->dev;\n\n\tskb_dma->addr = dma_map_single(dev, skb_dma->skb->data, skb_dma->skb->len, dir);\n\tif (dma_mapping_error(dev, skb_dma->addr)) {\n\t\tdev_err(dev, \"Failed to DMA map buffer\\n\");\n\t\tskb_dma->addr = 0;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void bam_dmux_skb_dma_unmap(struct bam_dmux_skb_dma *skb_dma,\n\t\t\t\t   enum dma_data_direction dir)\n{\n\tdma_unmap_single(skb_dma->dmux->dev, skb_dma->addr, skb_dma->skb->len, dir);\n\tskb_dma->addr = 0;\n}\n\nstatic void bam_dmux_tx_wake_queues(struct bam_dmux *dmux)\n{\n\tint i;\n\n\tdev_dbg(dmux->dev, \"wake queues\\n\");\n\n\tfor (i = 0; i < BAM_DMUX_NUM_CH; ++i) {\n\t\tstruct net_device *netdev = dmux->netdevs[i];\n\n\t\tif (netdev && netif_running(netdev))\n\t\t\tnetif_wake_queue(netdev);\n\t}\n}\n\nstatic void bam_dmux_tx_stop_queues(struct bam_dmux *dmux)\n{\n\tint i;\n\n\tdev_dbg(dmux->dev, \"stop queues\\n\");\n\n\tfor (i = 0; i < BAM_DMUX_NUM_CH; ++i) {\n\t\tstruct net_device *netdev = dmux->netdevs[i];\n\n\t\tif (netdev)\n\t\t\tnetif_stop_queue(netdev);\n\t}\n}\n\nstatic void bam_dmux_tx_done(struct bam_dmux_skb_dma *skb_dma)\n{\n\tstruct bam_dmux *dmux = skb_dma->dmux;\n\tunsigned long flags;\n\n\tpm_runtime_mark_last_busy(dmux->dev);\n\tpm_runtime_put_autosuspend(dmux->dev);\n\n\tif (skb_dma->addr)\n\t\tbam_dmux_skb_dma_unmap(skb_dma, DMA_TO_DEVICE);\n\n\tspin_lock_irqsave(&dmux->tx_lock, flags);\n\tskb_dma->skb = NULL;\n\tif (skb_dma == &dmux->tx_skbs[dmux->tx_next_skb % BAM_DMUX_NUM_SKB])\n\t\tbam_dmux_tx_wake_queues(dmux);\n\tspin_unlock_irqrestore(&dmux->tx_lock, flags);\n}\n\nstatic void bam_dmux_tx_callback(void *data)\n{\n\tstruct bam_dmux_skb_dma *skb_dma = data;\n\tstruct sk_buff *skb = skb_dma->skb;\n\n\tbam_dmux_tx_done(skb_dma);\n\tdev_consume_skb_any(skb);\n}\n\nstatic bool bam_dmux_skb_dma_submit_tx(struct bam_dmux_skb_dma *skb_dma)\n{\n\tstruct bam_dmux *dmux = skb_dma->dmux;\n\tstruct dma_async_tx_descriptor *desc;\n\n\tdesc = dmaengine_prep_slave_single(dmux->tx, skb_dma->addr,\n\t\t\t\t\t   skb_dma->skb->len, DMA_MEM_TO_DEV,\n\t\t\t\t\t   DMA_PREP_INTERRUPT);\n\tif (!desc) {\n\t\tdev_err(dmux->dev, \"Failed to prepare TX DMA buffer\\n\");\n\t\treturn false;\n\t}\n\n\tdesc->callback = bam_dmux_tx_callback;\n\tdesc->callback_param = skb_dma;\n\tdesc->cookie = dmaengine_submit(desc);\n\treturn true;\n}\n\nstatic struct bam_dmux_skb_dma *\nbam_dmux_tx_queue(struct bam_dmux *dmux, struct sk_buff *skb)\n{\n\tstruct bam_dmux_skb_dma *skb_dma;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dmux->tx_lock, flags);\n\n\tskb_dma = &dmux->tx_skbs[dmux->tx_next_skb % BAM_DMUX_NUM_SKB];\n\tif (skb_dma->skb) {\n\t\tbam_dmux_tx_stop_queues(dmux);\n\t\tspin_unlock_irqrestore(&dmux->tx_lock, flags);\n\t\treturn NULL;\n\t}\n\tskb_dma->skb = skb;\n\n\tdmux->tx_next_skb++;\n\tif (dmux->tx_skbs[dmux->tx_next_skb % BAM_DMUX_NUM_SKB].skb)\n\t\tbam_dmux_tx_stop_queues(dmux);\n\n\tspin_unlock_irqrestore(&dmux->tx_lock, flags);\n\treturn skb_dma;\n}\n\nstatic int bam_dmux_send_cmd(struct bam_dmux_netdev *bndev, u8 cmd)\n{\n\tstruct bam_dmux *dmux = bndev->dmux;\n\tstruct bam_dmux_skb_dma *skb_dma;\n\tstruct bam_dmux_hdr *hdr;\n\tstruct sk_buff *skb;\n\tint ret;\n\n\tskb = alloc_skb(sizeof(*hdr), GFP_KERNEL);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\thdr = skb_put_zero(skb, sizeof(*hdr));\n\thdr->magic = BAM_DMUX_HDR_MAGIC;\n\thdr->cmd = cmd;\n\thdr->ch = bndev->ch;\n\n\tskb_dma = bam_dmux_tx_queue(dmux, skb);\n\tif (!skb_dma) {\n\t\tret = -EAGAIN;\n\t\tgoto free_skb;\n\t}\n\n\tret = pm_runtime_get_sync(dmux->dev);\n\tif (ret < 0)\n\t\tgoto tx_fail;\n\n\tif (!bam_dmux_skb_dma_map(skb_dma, DMA_TO_DEVICE)) {\n\t\tret = -ENOMEM;\n\t\tgoto tx_fail;\n\t}\n\n\tif (!bam_dmux_skb_dma_submit_tx(skb_dma)) {\n\t\tret = -EIO;\n\t\tgoto tx_fail;\n\t}\n\n\tdma_async_issue_pending(dmux->tx);\n\treturn 0;\n\ntx_fail:\n\tbam_dmux_tx_done(skb_dma);\nfree_skb:\n\tdev_kfree_skb(skb);\n\treturn ret;\n}\n\nstatic int bam_dmux_netdev_open(struct net_device *netdev)\n{\n\tstruct bam_dmux_netdev *bndev = netdev_priv(netdev);\n\tint ret;\n\n\tret = bam_dmux_send_cmd(bndev, BAM_DMUX_CMD_OPEN);\n\tif (ret)\n\t\treturn ret;\n\n\tnetif_start_queue(netdev);\n\treturn 0;\n}\n\nstatic int bam_dmux_netdev_stop(struct net_device *netdev)\n{\n\tstruct bam_dmux_netdev *bndev = netdev_priv(netdev);\n\n\tnetif_stop_queue(netdev);\n\tbam_dmux_send_cmd(bndev, BAM_DMUX_CMD_CLOSE);\n\treturn 0;\n}\n\nstatic unsigned int needed_room(unsigned int avail, unsigned int needed)\n{\n\tif (avail >= needed)\n\t\treturn 0;\n\treturn needed - avail;\n}\n\nstatic int bam_dmux_tx_prepare_skb(struct bam_dmux_netdev *bndev,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tunsigned int head = needed_room(skb_headroom(skb), BAM_DMUX_HDR_SIZE);\n\tunsigned int pad = sizeof(u32) - skb->len % sizeof(u32);\n\tunsigned int tail = needed_room(skb_tailroom(skb), pad);\n\tstruct bam_dmux_hdr *hdr;\n\tint ret;\n\n\tif (head || tail || skb_cloned(skb)) {\n\t\tret = pskb_expand_head(skb, head, tail, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\thdr = skb_push(skb, sizeof(*hdr));\n\thdr->magic = BAM_DMUX_HDR_MAGIC;\n\thdr->signal = 0;\n\thdr->cmd = BAM_DMUX_CMD_DATA;\n\thdr->pad = pad;\n\thdr->ch = bndev->ch;\n\thdr->len = skb->len - sizeof(*hdr);\n\tif (pad)\n\t\tskb_put_zero(skb, pad);\n\n\treturn 0;\n}\n\nstatic netdev_tx_t bam_dmux_netdev_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t      struct net_device *netdev)\n{\n\tstruct bam_dmux_netdev *bndev = netdev_priv(netdev);\n\tstruct bam_dmux *dmux = bndev->dmux;\n\tstruct bam_dmux_skb_dma *skb_dma;\n\tint active, ret;\n\n\tskb_dma = bam_dmux_tx_queue(dmux, skb);\n\tif (!skb_dma)\n\t\treturn NETDEV_TX_BUSY;\n\n\tactive = pm_runtime_get(dmux->dev);\n\tif (active < 0 && active != -EINPROGRESS)\n\t\tgoto drop;\n\n\tret = bam_dmux_tx_prepare_skb(bndev, skb);\n\tif (ret)\n\t\tgoto drop;\n\n\tif (!bam_dmux_skb_dma_map(skb_dma, DMA_TO_DEVICE))\n\t\tgoto drop;\n\n\tif (active <= 0) {\n\t\t \n\t\tif (!atomic_long_fetch_or(BIT(skb_dma - dmux->tx_skbs),\n\t\t\t\t\t  &dmux->tx_deferred_skb))\n\t\t\tqueue_pm_work(&dmux->tx_wakeup_work);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tif (!bam_dmux_skb_dma_submit_tx(skb_dma))\n\t\tgoto drop;\n\n\tdma_async_issue_pending(dmux->tx);\n\treturn NETDEV_TX_OK;\n\ndrop:\n\tbam_dmux_tx_done(skb_dma);\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void bam_dmux_tx_wakeup_work(struct work_struct *work)\n{\n\tstruct bam_dmux *dmux = container_of(work, struct bam_dmux, tx_wakeup_work);\n\tunsigned long pending;\n\tint ret, i;\n\n\tret = pm_runtime_resume_and_get(dmux->dev);\n\tif (ret < 0) {\n\t\tdev_err(dmux->dev, \"Failed to resume: %d\\n\", ret);\n\t\treturn;\n\t}\n\n\tpending = atomic_long_xchg(&dmux->tx_deferred_skb, 0);\n\tif (!pending)\n\t\tgoto out;\n\n\tdev_dbg(dmux->dev, \"pending skbs after wakeup: %#lx\\n\", pending);\n\tfor_each_set_bit(i, &pending, BAM_DMUX_NUM_SKB) {\n\t\tbam_dmux_skb_dma_submit_tx(&dmux->tx_skbs[i]);\n\t}\n\tdma_async_issue_pending(dmux->tx);\n\nout:\n\tpm_runtime_mark_last_busy(dmux->dev);\n\tpm_runtime_put_autosuspend(dmux->dev);\n}\n\nstatic const struct net_device_ops bam_dmux_ops = {\n\t.ndo_open\t= bam_dmux_netdev_open,\n\t.ndo_stop\t= bam_dmux_netdev_stop,\n\t.ndo_start_xmit\t= bam_dmux_netdev_start_xmit,\n};\n\nstatic const struct device_type wwan_type = {\n\t.name = \"wwan\",\n};\n\nstatic void bam_dmux_netdev_setup(struct net_device *dev)\n{\n\tdev->netdev_ops = &bam_dmux_ops;\n\n\tdev->type = ARPHRD_RAWIP;\n\tSET_NETDEV_DEVTYPE(dev, &wwan_type);\n\tdev->flags = IFF_POINTOPOINT | IFF_NOARP;\n\n\tdev->mtu = ETH_DATA_LEN;\n\tdev->max_mtu = BAM_DMUX_MAX_DATA_SIZE;\n\tdev->needed_headroom = sizeof(struct bam_dmux_hdr);\n\tdev->needed_tailroom = sizeof(u32);  \n\tdev->tx_queue_len = DEFAULT_TX_QUEUE_LEN;\n\n\t \n\tdev->addr_assign_type = NET_ADDR_RANDOM;\n\teth_random_addr(dev->perm_addr);\n}\n\nstatic void bam_dmux_register_netdev_work(struct work_struct *work)\n{\n\tstruct bam_dmux *dmux = container_of(work, struct bam_dmux, register_netdev_work);\n\tstruct bam_dmux_netdev *bndev;\n\tstruct net_device *netdev;\n\tint ch, ret;\n\n\tfor_each_set_bit(ch, dmux->remote_channels, BAM_DMUX_NUM_CH) {\n\t\tif (dmux->netdevs[ch])\n\t\t\tcontinue;\n\n\t\tnetdev = alloc_netdev(sizeof(*bndev), \"wwan%d\", NET_NAME_ENUM,\n\t\t\t\t      bam_dmux_netdev_setup);\n\t\tif (!netdev)\n\t\t\treturn;\n\n\t\tSET_NETDEV_DEV(netdev, dmux->dev);\n\t\tnetdev->dev_port = ch;\n\n\t\tbndev = netdev_priv(netdev);\n\t\tbndev->dmux = dmux;\n\t\tbndev->ch = ch;\n\n\t\tret = register_netdev(netdev);\n\t\tif (ret) {\n\t\t\tdev_err(dmux->dev, \"Failed to register netdev for channel %u: %d\\n\",\n\t\t\t\tch, ret);\n\t\t\tfree_netdev(netdev);\n\t\t\treturn;\n\t\t}\n\n\t\tdmux->netdevs[ch] = netdev;\n\t}\n}\n\nstatic void bam_dmux_rx_callback(void *data);\n\nstatic bool bam_dmux_skb_dma_submit_rx(struct bam_dmux_skb_dma *skb_dma)\n{\n\tstruct bam_dmux *dmux = skb_dma->dmux;\n\tstruct dma_async_tx_descriptor *desc;\n\n\tdesc = dmaengine_prep_slave_single(dmux->rx, skb_dma->addr,\n\t\t\t\t\t   skb_dma->skb->len, DMA_DEV_TO_MEM,\n\t\t\t\t\t   DMA_PREP_INTERRUPT);\n\tif (!desc) {\n\t\tdev_err(dmux->dev, \"Failed to prepare RX DMA buffer\\n\");\n\t\treturn false;\n\t}\n\n\tdesc->callback = bam_dmux_rx_callback;\n\tdesc->callback_param = skb_dma;\n\tdesc->cookie = dmaengine_submit(desc);\n\treturn true;\n}\n\nstatic bool bam_dmux_skb_dma_queue_rx(struct bam_dmux_skb_dma *skb_dma, gfp_t gfp)\n{\n\tif (!skb_dma->skb) {\n\t\tskb_dma->skb = __netdev_alloc_skb(NULL, BAM_DMUX_BUFFER_SIZE, gfp);\n\t\tif (!skb_dma->skb)\n\t\t\treturn false;\n\t\tskb_put(skb_dma->skb, BAM_DMUX_BUFFER_SIZE);\n\t}\n\n\treturn bam_dmux_skb_dma_map(skb_dma, DMA_FROM_DEVICE) &&\n\t       bam_dmux_skb_dma_submit_rx(skb_dma);\n}\n\nstatic void bam_dmux_cmd_data(struct bam_dmux_skb_dma *skb_dma)\n{\n\tstruct bam_dmux *dmux = skb_dma->dmux;\n\tstruct sk_buff *skb = skb_dma->skb;\n\tstruct bam_dmux_hdr *hdr = (struct bam_dmux_hdr *)skb->data;\n\tstruct net_device *netdev = dmux->netdevs[hdr->ch];\n\n\tif (!netdev || !netif_running(netdev)) {\n\t\tdev_warn(dmux->dev, \"Data for inactive channel %u\\n\", hdr->ch);\n\t\treturn;\n\t}\n\n\tif (hdr->len > BAM_DMUX_MAX_DATA_SIZE) {\n\t\tdev_err(dmux->dev, \"Data larger than buffer? (%u > %u)\\n\",\n\t\t\thdr->len, (u16)BAM_DMUX_MAX_DATA_SIZE);\n\t\treturn;\n\t}\n\n\tskb_dma->skb = NULL;  \n\n\tskb_pull(skb, sizeof(*hdr));\n\tskb_trim(skb, hdr->len);\n\tskb->dev = netdev;\n\n\t \n\tswitch (skb->data[0] & 0xf0) {\n\tcase 0x40:\n\t\tskb->protocol = htons(ETH_P_IP);\n\t\tbreak;\n\tcase 0x60:\n\t\tskb->protocol = htons(ETH_P_IPV6);\n\t\tbreak;\n\tdefault:\n\t\tskb->protocol = htons(ETH_P_MAP);\n\t\tbreak;\n\t}\n\n\tnetif_receive_skb(skb);\n}\n\nstatic void bam_dmux_cmd_open(struct bam_dmux *dmux, struct bam_dmux_hdr *hdr)\n{\n\tstruct net_device *netdev = dmux->netdevs[hdr->ch];\n\n\tdev_dbg(dmux->dev, \"open channel: %u\\n\", hdr->ch);\n\n\tif (__test_and_set_bit(hdr->ch, dmux->remote_channels)) {\n\t\tdev_warn(dmux->dev, \"Channel already open: %u\\n\", hdr->ch);\n\t\treturn;\n\t}\n\n\tif (netdev) {\n\t\tnetif_device_attach(netdev);\n\t} else {\n\t\t \n\t\tschedule_work(&dmux->register_netdev_work);\n\t}\n}\n\nstatic void bam_dmux_cmd_close(struct bam_dmux *dmux, struct bam_dmux_hdr *hdr)\n{\n\tstruct net_device *netdev = dmux->netdevs[hdr->ch];\n\n\tdev_dbg(dmux->dev, \"close channel: %u\\n\", hdr->ch);\n\n\tif (!__test_and_clear_bit(hdr->ch, dmux->remote_channels)) {\n\t\tdev_err(dmux->dev, \"Channel not open: %u\\n\", hdr->ch);\n\t\treturn;\n\t}\n\n\tif (netdev)\n\t\tnetif_device_detach(netdev);\n}\n\nstatic void bam_dmux_rx_callback(void *data)\n{\n\tstruct bam_dmux_skb_dma *skb_dma = data;\n\tstruct bam_dmux *dmux = skb_dma->dmux;\n\tstruct sk_buff *skb = skb_dma->skb;\n\tstruct bam_dmux_hdr *hdr = (struct bam_dmux_hdr *)skb->data;\n\n\tbam_dmux_skb_dma_unmap(skb_dma, DMA_FROM_DEVICE);\n\n\tif (hdr->magic != BAM_DMUX_HDR_MAGIC) {\n\t\tdev_err(dmux->dev, \"Invalid magic in header: %#x\\n\", hdr->magic);\n\t\tgoto out;\n\t}\n\n\tif (hdr->ch >= BAM_DMUX_NUM_CH) {\n\t\tdev_dbg(dmux->dev, \"Unsupported channel: %u\\n\", hdr->ch);\n\t\tgoto out;\n\t}\n\n\tswitch (hdr->cmd) {\n\tcase BAM_DMUX_CMD_DATA:\n\t\tbam_dmux_cmd_data(skb_dma);\n\t\tbreak;\n\tcase BAM_DMUX_CMD_OPEN:\n\t\tbam_dmux_cmd_open(dmux, hdr);\n\t\tbreak;\n\tcase BAM_DMUX_CMD_CLOSE:\n\t\tbam_dmux_cmd_close(dmux, hdr);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dmux->dev, \"Unsupported command %u on channel %u\\n\",\n\t\t\thdr->cmd, hdr->ch);\n\t\tbreak;\n\t}\n\nout:\n\tif (bam_dmux_skb_dma_queue_rx(skb_dma, GFP_ATOMIC))\n\t\tdma_async_issue_pending(dmux->rx);\n}\n\nstatic bool bam_dmux_power_on(struct bam_dmux *dmux)\n{\n\tstruct device *dev = dmux->dev;\n\tstruct dma_slave_config dma_rx_conf = {\n\t\t.direction = DMA_DEV_TO_MEM,\n\t\t.src_maxburst = BAM_DMUX_BUFFER_SIZE,\n\t};\n\tint i;\n\n\tdmux->rx = dma_request_chan(dev, \"rx\");\n\tif (IS_ERR(dmux->rx)) {\n\t\tdev_err(dev, \"Failed to request RX DMA channel: %pe\\n\", dmux->rx);\n\t\tdmux->rx = NULL;\n\t\treturn false;\n\t}\n\tdmaengine_slave_config(dmux->rx, &dma_rx_conf);\n\n\tfor (i = 0; i < BAM_DMUX_NUM_SKB; i++) {\n\t\tif (!bam_dmux_skb_dma_queue_rx(&dmux->rx_skbs[i], GFP_KERNEL))\n\t\t\treturn false;\n\t}\n\tdma_async_issue_pending(dmux->rx);\n\n\treturn true;\n}\n\nstatic void bam_dmux_free_skbs(struct bam_dmux_skb_dma skbs[],\n\t\t\t       enum dma_data_direction dir)\n{\n\tint i;\n\n\tfor (i = 0; i < BAM_DMUX_NUM_SKB; i++) {\n\t\tstruct bam_dmux_skb_dma *skb_dma = &skbs[i];\n\n\t\tif (skb_dma->addr)\n\t\t\tbam_dmux_skb_dma_unmap(skb_dma, dir);\n\t\tif (skb_dma->skb) {\n\t\t\tdev_kfree_skb(skb_dma->skb);\n\t\t\tskb_dma->skb = NULL;\n\t\t}\n\t}\n}\n\nstatic void bam_dmux_power_off(struct bam_dmux *dmux)\n{\n\tif (dmux->tx) {\n\t\tdmaengine_terminate_sync(dmux->tx);\n\t\tdma_release_channel(dmux->tx);\n\t\tdmux->tx = NULL;\n\t}\n\n\tif (dmux->rx) {\n\t\tdmaengine_terminate_sync(dmux->rx);\n\t\tdma_release_channel(dmux->rx);\n\t\tdmux->rx = NULL;\n\t}\n\n\tbam_dmux_free_skbs(dmux->rx_skbs, DMA_FROM_DEVICE);\n}\n\nstatic irqreturn_t bam_dmux_pc_irq(int irq, void *data)\n{\n\tstruct bam_dmux *dmux = data;\n\tbool new_state = !dmux->pc_state;\n\n\tdev_dbg(dmux->dev, \"pc: %u\\n\", new_state);\n\n\tif (new_state) {\n\t\tif (bam_dmux_power_on(dmux))\n\t\t\tbam_dmux_pc_ack(dmux);\n\t\telse\n\t\t\tbam_dmux_power_off(dmux);\n\t} else {\n\t\tbam_dmux_power_off(dmux);\n\t\tbam_dmux_pc_ack(dmux);\n\t}\n\n\tdmux->pc_state = new_state;\n\twake_up_all(&dmux->pc_wait);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t bam_dmux_pc_ack_irq(int irq, void *data)\n{\n\tstruct bam_dmux *dmux = data;\n\n\tdev_dbg(dmux->dev, \"pc ack\\n\");\n\tcomplete_all(&dmux->pc_ack_completion);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int bam_dmux_runtime_suspend(struct device *dev)\n{\n\tstruct bam_dmux *dmux = dev_get_drvdata(dev);\n\n\tdev_dbg(dev, \"runtime suspend\\n\");\n\tbam_dmux_pc_vote(dmux, false);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused bam_dmux_runtime_resume(struct device *dev)\n{\n\tstruct bam_dmux *dmux = dev_get_drvdata(dev);\n\n\tdev_dbg(dev, \"runtime resume\\n\");\n\n\t \n\tif (!wait_for_completion_timeout(&dmux->pc_ack_completion,\n\t\t\t\t\t BAM_DMUX_REMOTE_TIMEOUT))\n\t\treturn -ETIMEDOUT;\n\n\t \n\tbam_dmux_pc_vote(dmux, true);\n\n\t \n\tif (!wait_for_completion_timeout(&dmux->pc_ack_completion,\n\t\t\t\t\t BAM_DMUX_REMOTE_TIMEOUT)) {\n\t\tbam_dmux_pc_vote(dmux, false);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\t \n\tif (!wait_event_timeout(dmux->pc_wait, dmux->pc_state,\n\t\t\t\tBAM_DMUX_REMOTE_TIMEOUT)) {\n\t\tbam_dmux_pc_vote(dmux, false);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\t \n\tif (!dmux->rx) {\n\t\tbam_dmux_pc_vote(dmux, false);\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tif (dmux->tx)\n\t\treturn 0;\n\n\tdmux->tx = dma_request_chan(dev, \"tx\");\n\tif (IS_ERR(dmux->tx)) {\n\t\tdev_err(dev, \"Failed to request TX DMA channel: %pe\\n\", dmux->tx);\n\t\tdmux->tx = NULL;\n\t\tbam_dmux_runtime_suspend(dev);\n\t\treturn -ENXIO;\n\t}\n\n\treturn 0;\n}\n\nstatic int bam_dmux_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct bam_dmux *dmux;\n\tint ret, pc_ack_irq, i;\n\tunsigned int bit;\n\n\tdmux = devm_kzalloc(dev, sizeof(*dmux), GFP_KERNEL);\n\tif (!dmux)\n\t\treturn -ENOMEM;\n\n\tdmux->dev = dev;\n\tplatform_set_drvdata(pdev, dmux);\n\n\tdmux->pc_irq = platform_get_irq_byname(pdev, \"pc\");\n\tif (dmux->pc_irq < 0)\n\t\treturn dmux->pc_irq;\n\n\tpc_ack_irq = platform_get_irq_byname(pdev, \"pc-ack\");\n\tif (pc_ack_irq < 0)\n\t\treturn pc_ack_irq;\n\n\tdmux->pc = devm_qcom_smem_state_get(dev, \"pc\", &bit);\n\tif (IS_ERR(dmux->pc))\n\t\treturn dev_err_probe(dev, PTR_ERR(dmux->pc),\n\t\t\t\t     \"Failed to get pc state\\n\");\n\tdmux->pc_mask = BIT(bit);\n\n\tdmux->pc_ack = devm_qcom_smem_state_get(dev, \"pc-ack\", &bit);\n\tif (IS_ERR(dmux->pc_ack))\n\t\treturn dev_err_probe(dev, PTR_ERR(dmux->pc_ack),\n\t\t\t\t     \"Failed to get pc-ack state\\n\");\n\tdmux->pc_ack_mask = BIT(bit);\n\n\tinit_waitqueue_head(&dmux->pc_wait);\n\tinit_completion(&dmux->pc_ack_completion);\n\tcomplete_all(&dmux->pc_ack_completion);\n\n\tspin_lock_init(&dmux->tx_lock);\n\tINIT_WORK(&dmux->tx_wakeup_work, bam_dmux_tx_wakeup_work);\n\tINIT_WORK(&dmux->register_netdev_work, bam_dmux_register_netdev_work);\n\n\tfor (i = 0; i < BAM_DMUX_NUM_SKB; i++) {\n\t\tdmux->rx_skbs[i].dmux = dmux;\n\t\tdmux->tx_skbs[i].dmux = dmux;\n\t}\n\n\t \n\tpm_runtime_set_autosuspend_delay(dev, BAM_DMUX_AUTOSUSPEND_DELAY);\n\tpm_runtime_use_autosuspend(dev);\n\tpm_runtime_enable(dev);\n\n\tret = devm_request_threaded_irq(dev, pc_ack_irq, NULL, bam_dmux_pc_ack_irq,\n\t\t\t\t\tIRQF_ONESHOT, NULL, dmux);\n\tif (ret)\n\t\treturn ret;\n\n\tret = devm_request_threaded_irq(dev, dmux->pc_irq, NULL, bam_dmux_pc_irq,\n\t\t\t\t\tIRQF_ONESHOT, NULL, dmux);\n\tif (ret)\n\t\treturn ret;\n\n\tret = irq_get_irqchip_state(dmux->pc_irq, IRQCHIP_STATE_LINE_LEVEL,\n\t\t\t\t    &dmux->pc_state);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (dmux->pc_state) {\n\t\tif (bam_dmux_power_on(dmux))\n\t\t\tbam_dmux_pc_ack(dmux);\n\t\telse\n\t\t\tbam_dmux_power_off(dmux);\n\t}\n\n\treturn 0;\n}\n\nstatic int bam_dmux_remove(struct platform_device *pdev)\n{\n\tstruct bam_dmux *dmux = platform_get_drvdata(pdev);\n\tstruct device *dev = dmux->dev;\n\tLIST_HEAD(list);\n\tint i;\n\n\t \n\tcancel_work_sync(&dmux->register_netdev_work);\n\trtnl_lock();\n\tfor (i = 0; i < BAM_DMUX_NUM_CH; ++i)\n\t\tif (dmux->netdevs[i])\n\t\t\tunregister_netdevice_queue(dmux->netdevs[i], &list);\n\tunregister_netdevice_many(&list);\n\trtnl_unlock();\n\tcancel_work_sync(&dmux->tx_wakeup_work);\n\n\t \n\tpm_runtime_disable(dev);\n\tpm_runtime_dont_use_autosuspend(dev);\n\tbam_dmux_runtime_suspend(dev);\n\tpm_runtime_set_suspended(dev);\n\n\t \n\tif (!wait_event_timeout(dmux->pc_wait, !dmux->rx, BAM_DMUX_REMOTE_TIMEOUT))\n\t\tdev_err(dev, \"Timed out waiting for remote side to suspend\\n\");\n\n\t \n\tdisable_irq(dmux->pc_irq);\n\tbam_dmux_power_off(dmux);\n\tbam_dmux_free_skbs(dmux->tx_skbs, DMA_TO_DEVICE);\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops bam_dmux_pm_ops = {\n\tSET_RUNTIME_PM_OPS(bam_dmux_runtime_suspend, bam_dmux_runtime_resume, NULL)\n};\n\nstatic const struct of_device_id bam_dmux_of_match[] = {\n\t{ .compatible = \"qcom,bam-dmux\" },\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, bam_dmux_of_match);\n\nstatic struct platform_driver bam_dmux_driver = {\n\t.probe = bam_dmux_probe,\n\t.remove = bam_dmux_remove,\n\t.driver = {\n\t\t.name = \"bam-dmux\",\n\t\t.pm = &bam_dmux_pm_ops,\n\t\t.of_match_table = bam_dmux_of_match,\n\t},\n};\nmodule_platform_driver(bam_dmux_driver);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"Qualcomm BAM-DMUX WWAN Network Driver\");\nMODULE_AUTHOR(\"Stephan Gerhold <stephan@gerhold.net>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}