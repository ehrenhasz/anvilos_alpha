{
  "module_name": "t7xx_hif_cldma.c",
  "hash_id": "ea5488c855c78cb6675f688102c2fcbb7d2168713e078c571e0a6649bbe8cab0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wwan/t7xx/t7xx_hif_cldma.c",
  "human_readable_source": "\n \n\n#include <linux/bits.h>\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/device.h>\n#include <linux/dmapool.h>\n#include <linux/dma-mapping.h>\n#include <linux/dma-direction.h>\n#include <linux/gfp.h>\n#include <linux/io.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/iopoll.h>\n#include <linux/irqreturn.h>\n#include <linux/kernel.h>\n#include <linux/kthread.h>\n#include <linux/list.h>\n#include <linux/netdevice.h>\n#include <linux/pci.h>\n#include <linux/pm_runtime.h>\n#include <linux/sched.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/types.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n\n#include \"t7xx_cldma.h\"\n#include \"t7xx_hif_cldma.h\"\n#include \"t7xx_mhccif.h\"\n#include \"t7xx_pci.h\"\n#include \"t7xx_pcie_mac.h\"\n#include \"t7xx_port_proxy.h\"\n#include \"t7xx_reg.h\"\n#include \"t7xx_state_monitor.h\"\n\n#define MAX_TX_BUDGET\t\t\t16\n#define MAX_RX_BUDGET\t\t\t16\n\n#define CHECK_Q_STOP_TIMEOUT_US\t\t1000000\n#define CHECK_Q_STOP_STEP_US\t\t10000\n\n#define CLDMA_JUMBO_BUFF_SZ\t\t(63 * 1024 + sizeof(struct ccci_header))\n\nstatic void md_cd_queue_struct_reset(struct cldma_queue *queue, struct cldma_ctrl *md_ctrl,\n\t\t\t\t     enum mtk_txrx tx_rx, unsigned int index)\n{\n\tqueue->dir = tx_rx;\n\tqueue->index = index;\n\tqueue->md_ctrl = md_ctrl;\n\tqueue->tr_ring = NULL;\n\tqueue->tr_done = NULL;\n\tqueue->tx_next = NULL;\n}\n\nstatic void md_cd_queue_struct_init(struct cldma_queue *queue, struct cldma_ctrl *md_ctrl,\n\t\t\t\t    enum mtk_txrx tx_rx, unsigned int index)\n{\n\tmd_cd_queue_struct_reset(queue, md_ctrl, tx_rx, index);\n\tinit_waitqueue_head(&queue->req_wq);\n\tspin_lock_init(&queue->ring_lock);\n}\n\nstatic void t7xx_cldma_gpd_set_data_ptr(struct cldma_gpd *gpd, dma_addr_t data_ptr)\n{\n\tgpd->data_buff_bd_ptr_h = cpu_to_le32(upper_32_bits(data_ptr));\n\tgpd->data_buff_bd_ptr_l = cpu_to_le32(lower_32_bits(data_ptr));\n}\n\nstatic void t7xx_cldma_gpd_set_next_ptr(struct cldma_gpd *gpd, dma_addr_t next_ptr)\n{\n\tgpd->next_gpd_ptr_h = cpu_to_le32(upper_32_bits(next_ptr));\n\tgpd->next_gpd_ptr_l = cpu_to_le32(lower_32_bits(next_ptr));\n}\n\nstatic int t7xx_cldma_alloc_and_map_skb(struct cldma_ctrl *md_ctrl, struct cldma_request *req,\n\t\t\t\t\tsize_t size, gfp_t gfp_mask)\n{\n\treq->skb = __dev_alloc_skb(size, gfp_mask);\n\tif (!req->skb)\n\t\treturn -ENOMEM;\n\n\treq->mapped_buff = dma_map_single(md_ctrl->dev, req->skb->data, size, DMA_FROM_DEVICE);\n\tif (dma_mapping_error(md_ctrl->dev, req->mapped_buff)) {\n\t\tdev_kfree_skb_any(req->skb);\n\t\treq->skb = NULL;\n\t\treq->mapped_buff = 0;\n\t\tdev_err(md_ctrl->dev, \"DMA mapping failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int t7xx_cldma_gpd_rx_from_q(struct cldma_queue *queue, int budget, bool *over_budget)\n{\n\tstruct cldma_ctrl *md_ctrl = queue->md_ctrl;\n\tunsigned int hwo_polling_count = 0;\n\tstruct t7xx_cldma_hw *hw_info;\n\tbool rx_not_done = true;\n\tunsigned long flags;\n\tint count = 0;\n\n\thw_info = &md_ctrl->hw_info;\n\n\tdo {\n\t\tstruct cldma_request *req;\n\t\tstruct cldma_gpd *gpd;\n\t\tstruct sk_buff *skb;\n\t\tint ret;\n\n\t\treq = queue->tr_done;\n\t\tif (!req)\n\t\t\treturn -ENODATA;\n\n\t\tgpd = req->gpd;\n\t\tif ((gpd->flags & GPD_FLAGS_HWO) || !req->skb) {\n\t\t\tdma_addr_t gpd_addr;\n\n\t\t\tif (!pci_device_is_present(to_pci_dev(md_ctrl->dev))) {\n\t\t\t\tdev_err(md_ctrl->dev, \"PCIe Link disconnected\\n\");\n\t\t\t\treturn -ENODEV;\n\t\t\t}\n\n\t\t\tgpd_addr = ioread64(hw_info->ap_pdn_base + REG_CLDMA_DL_CURRENT_ADDRL_0 +\n\t\t\t\t\t    queue->index * sizeof(u64));\n\t\t\tif (req->gpd_addr == gpd_addr || hwo_polling_count++ >= 100)\n\t\t\t\treturn 0;\n\n\t\t\tudelay(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\thwo_polling_count = 0;\n\t\tskb = req->skb;\n\n\t\tif (req->mapped_buff) {\n\t\t\tdma_unmap_single(md_ctrl->dev, req->mapped_buff,\n\t\t\t\t\t queue->tr_ring->pkt_size, DMA_FROM_DEVICE);\n\t\t\treq->mapped_buff = 0;\n\t\t}\n\n\t\tskb->len = 0;\n\t\tskb_reset_tail_pointer(skb);\n\t\tskb_put(skb, le16_to_cpu(gpd->data_buff_len));\n\n\t\tret = md_ctrl->recv_skb(queue, skb);\n\t\t \n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\treq->skb = NULL;\n\t\tt7xx_cldma_gpd_set_data_ptr(gpd, 0);\n\n\t\tspin_lock_irqsave(&queue->ring_lock, flags);\n\t\tqueue->tr_done = list_next_entry_circular(req, &queue->tr_ring->gpd_ring, entry);\n\t\tspin_unlock_irqrestore(&queue->ring_lock, flags);\n\t\treq = queue->rx_refill;\n\n\t\tret = t7xx_cldma_alloc_and_map_skb(md_ctrl, req, queue->tr_ring->pkt_size, GFP_KERNEL);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tgpd = req->gpd;\n\t\tt7xx_cldma_gpd_set_data_ptr(gpd, req->mapped_buff);\n\t\tgpd->data_buff_len = 0;\n\t\tgpd->flags = GPD_FLAGS_IOC | GPD_FLAGS_HWO;\n\n\t\tspin_lock_irqsave(&queue->ring_lock, flags);\n\t\tqueue->rx_refill = list_next_entry_circular(req, &queue->tr_ring->gpd_ring, entry);\n\t\tspin_unlock_irqrestore(&queue->ring_lock, flags);\n\n\t\trx_not_done = ++count < budget || !need_resched();\n\t} while (rx_not_done);\n\n\t*over_budget = true;\n\treturn 0;\n}\n\nstatic int t7xx_cldma_gpd_rx_collect(struct cldma_queue *queue, int budget)\n{\n\tstruct cldma_ctrl *md_ctrl = queue->md_ctrl;\n\tstruct t7xx_cldma_hw *hw_info;\n\tunsigned int pending_rx_int;\n\tbool over_budget = false;\n\tunsigned long flags;\n\tint ret;\n\n\thw_info = &md_ctrl->hw_info;\n\n\tdo {\n\t\tret = t7xx_cldma_gpd_rx_from_q(queue, budget, &over_budget);\n\t\tif (ret == -ENODATA)\n\t\t\treturn 0;\n\t\telse if (ret)\n\t\t\treturn ret;\n\n\t\tpending_rx_int = 0;\n\n\t\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\t\tif (md_ctrl->rxq_active & BIT(queue->index)) {\n\t\t\tif (!t7xx_cldma_hw_queue_status(hw_info, queue->index, MTK_RX))\n\t\t\t\tt7xx_cldma_hw_resume_queue(hw_info, queue->index, MTK_RX);\n\n\t\t\tpending_rx_int = t7xx_cldma_hw_int_status(hw_info, BIT(queue->index),\n\t\t\t\t\t\t\t\t  MTK_RX);\n\t\t\tif (pending_rx_int) {\n\t\t\t\tt7xx_cldma_hw_rx_done(hw_info, pending_rx_int);\n\n\t\t\t\tif (over_budget) {\n\t\t\t\t\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\t\t\t\t\treturn -EAGAIN;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\t} while (pending_rx_int);\n\n\treturn 0;\n}\n\nstatic void t7xx_cldma_rx_done(struct work_struct *work)\n{\n\tstruct cldma_queue *queue = container_of(work, struct cldma_queue, cldma_work);\n\tstruct cldma_ctrl *md_ctrl = queue->md_ctrl;\n\tint value;\n\n\tvalue = t7xx_cldma_gpd_rx_collect(queue, queue->budget);\n\tif (value && md_ctrl->rxq_active & BIT(queue->index)) {\n\t\tqueue_work(queue->worker, &queue->cldma_work);\n\t\treturn;\n\t}\n\n\tt7xx_cldma_clear_ip_busy(&md_ctrl->hw_info);\n\tt7xx_cldma_hw_irq_en_txrx(&md_ctrl->hw_info, queue->index, MTK_RX);\n\tt7xx_cldma_hw_irq_en_eq(&md_ctrl->hw_info, queue->index, MTK_RX);\n\tpm_runtime_mark_last_busy(md_ctrl->dev);\n\tpm_runtime_put_autosuspend(md_ctrl->dev);\n}\n\nstatic int t7xx_cldma_gpd_tx_collect(struct cldma_queue *queue)\n{\n\tstruct cldma_ctrl *md_ctrl = queue->md_ctrl;\n\tunsigned int dma_len, count = 0;\n\tstruct cldma_request *req;\n\tstruct cldma_gpd *gpd;\n\tunsigned long flags;\n\tdma_addr_t dma_free;\n\tstruct sk_buff *skb;\n\n\twhile (!kthread_should_stop()) {\n\t\tspin_lock_irqsave(&queue->ring_lock, flags);\n\t\treq = queue->tr_done;\n\t\tif (!req) {\n\t\t\tspin_unlock_irqrestore(&queue->ring_lock, flags);\n\t\t\tbreak;\n\t\t}\n\t\tgpd = req->gpd;\n\t\tif ((gpd->flags & GPD_FLAGS_HWO) || !req->skb) {\n\t\t\tspin_unlock_irqrestore(&queue->ring_lock, flags);\n\t\t\tbreak;\n\t\t}\n\t\tqueue->budget++;\n\t\tdma_free = req->mapped_buff;\n\t\tdma_len = le16_to_cpu(gpd->data_buff_len);\n\t\tskb = req->skb;\n\t\treq->skb = NULL;\n\t\tqueue->tr_done = list_next_entry_circular(req, &queue->tr_ring->gpd_ring, entry);\n\t\tspin_unlock_irqrestore(&queue->ring_lock, flags);\n\n\t\tcount++;\n\t\tdma_unmap_single(md_ctrl->dev, dma_free, dma_len, DMA_TO_DEVICE);\n\t\tdev_kfree_skb_any(skb);\n\t}\n\n\tif (count)\n\t\twake_up_nr(&queue->req_wq, count);\n\n\treturn count;\n}\n\nstatic void t7xx_cldma_txq_empty_hndl(struct cldma_queue *queue)\n{\n\tstruct cldma_ctrl *md_ctrl = queue->md_ctrl;\n\tstruct cldma_request *req;\n\tdma_addr_t ul_curr_addr;\n\tunsigned long flags;\n\tbool pending_gpd;\n\n\tif (!(md_ctrl->txq_active & BIT(queue->index)))\n\t\treturn;\n\n\tspin_lock_irqsave(&queue->ring_lock, flags);\n\treq = list_prev_entry_circular(queue->tx_next, &queue->tr_ring->gpd_ring, entry);\n\tspin_unlock_irqrestore(&queue->ring_lock, flags);\n\n\tpending_gpd = (req->gpd->flags & GPD_FLAGS_HWO) && req->skb;\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tif (pending_gpd) {\n\t\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\n\t\t \n\t\tul_curr_addr = ioread64(hw_info->ap_pdn_base + REG_CLDMA_UL_CURRENT_ADDRL_0 +\n\t\t\t\t\tqueue->index * sizeof(u64));\n\t\tif (req->gpd_addr != ul_curr_addr) {\n\t\t\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\t\t\tdev_err(md_ctrl->dev, \"CLDMA%d queue %d is not empty\\n\",\n\t\t\t\tmd_ctrl->hif_id, queue->index);\n\t\t\treturn;\n\t\t}\n\n\t\tt7xx_cldma_hw_resume_queue(hw_info, queue->index, MTK_TX);\n\t}\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n}\n\nstatic void t7xx_cldma_tx_done(struct work_struct *work)\n{\n\tstruct cldma_queue *queue = container_of(work, struct cldma_queue, cldma_work);\n\tstruct cldma_ctrl *md_ctrl = queue->md_ctrl;\n\tstruct t7xx_cldma_hw *hw_info;\n\tunsigned int l2_tx_int;\n\tunsigned long flags;\n\n\thw_info = &md_ctrl->hw_info;\n\tt7xx_cldma_gpd_tx_collect(queue);\n\tl2_tx_int = t7xx_cldma_hw_int_status(hw_info, BIT(queue->index) | EQ_STA_BIT(queue->index),\n\t\t\t\t\t     MTK_TX);\n\tif (l2_tx_int & EQ_STA_BIT(queue->index)) {\n\t\tt7xx_cldma_hw_tx_done(hw_info, EQ_STA_BIT(queue->index));\n\t\tt7xx_cldma_txq_empty_hndl(queue);\n\t}\n\n\tif (l2_tx_int & BIT(queue->index)) {\n\t\tt7xx_cldma_hw_tx_done(hw_info, BIT(queue->index));\n\t\tqueue_work(queue->worker, &queue->cldma_work);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tif (md_ctrl->txq_active & BIT(queue->index)) {\n\t\tt7xx_cldma_clear_ip_busy(hw_info);\n\t\tt7xx_cldma_hw_irq_en_eq(hw_info, queue->index, MTK_TX);\n\t\tt7xx_cldma_hw_irq_en_txrx(hw_info, queue->index, MTK_TX);\n\t}\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\n\tpm_runtime_mark_last_busy(md_ctrl->dev);\n\tpm_runtime_put_autosuspend(md_ctrl->dev);\n}\n\nstatic void t7xx_cldma_ring_free(struct cldma_ctrl *md_ctrl,\n\t\t\t\t struct cldma_ring *ring, enum dma_data_direction tx_rx)\n{\n\tstruct cldma_request *req_cur, *req_next;\n\n\tlist_for_each_entry_safe(req_cur, req_next, &ring->gpd_ring, entry) {\n\t\tif (req_cur->mapped_buff && req_cur->skb) {\n\t\t\tdma_unmap_single(md_ctrl->dev, req_cur->mapped_buff,\n\t\t\t\t\t ring->pkt_size, tx_rx);\n\t\t\treq_cur->mapped_buff = 0;\n\t\t}\n\n\t\tdev_kfree_skb_any(req_cur->skb);\n\n\t\tif (req_cur->gpd)\n\t\t\tdma_pool_free(md_ctrl->gpd_dmapool, req_cur->gpd, req_cur->gpd_addr);\n\n\t\tlist_del(&req_cur->entry);\n\t\tkfree(req_cur);\n\t}\n}\n\nstatic struct cldma_request *t7xx_alloc_rx_request(struct cldma_ctrl *md_ctrl, size_t pkt_size)\n{\n\tstruct cldma_request *req;\n\tint val;\n\n\treq = kzalloc(sizeof(*req), GFP_KERNEL);\n\tif (!req)\n\t\treturn NULL;\n\n\treq->gpd = dma_pool_zalloc(md_ctrl->gpd_dmapool, GFP_KERNEL, &req->gpd_addr);\n\tif (!req->gpd)\n\t\tgoto err_free_req;\n\n\tval = t7xx_cldma_alloc_and_map_skb(md_ctrl, req, pkt_size, GFP_KERNEL);\n\tif (val)\n\t\tgoto err_free_pool;\n\n\treturn req;\n\nerr_free_pool:\n\tdma_pool_free(md_ctrl->gpd_dmapool, req->gpd, req->gpd_addr);\n\nerr_free_req:\n\tkfree(req);\n\n\treturn NULL;\n}\n\nstatic int t7xx_cldma_rx_ring_init(struct cldma_ctrl *md_ctrl, struct cldma_ring *ring)\n{\n\tstruct cldma_request *req;\n\tstruct cldma_gpd *gpd;\n\tint i;\n\n\tINIT_LIST_HEAD(&ring->gpd_ring);\n\tring->length = MAX_RX_BUDGET;\n\n\tfor (i = 0; i < ring->length; i++) {\n\t\treq = t7xx_alloc_rx_request(md_ctrl, ring->pkt_size);\n\t\tif (!req) {\n\t\t\tt7xx_cldma_ring_free(md_ctrl, ring, DMA_FROM_DEVICE);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tgpd = req->gpd;\n\t\tt7xx_cldma_gpd_set_data_ptr(gpd, req->mapped_buff);\n\t\tgpd->rx_data_allow_len = cpu_to_le16(ring->pkt_size);\n\t\tgpd->flags = GPD_FLAGS_IOC | GPD_FLAGS_HWO;\n\t\tINIT_LIST_HEAD(&req->entry);\n\t\tlist_add_tail(&req->entry, &ring->gpd_ring);\n\t}\n\n\t \n\tlist_for_each_entry(req, &ring->gpd_ring, entry) {\n\t\tt7xx_cldma_gpd_set_next_ptr(gpd, req->gpd_addr);\n\t\tgpd = req->gpd;\n\t}\n\n\treturn 0;\n}\n\nstatic struct cldma_request *t7xx_alloc_tx_request(struct cldma_ctrl *md_ctrl)\n{\n\tstruct cldma_request *req;\n\n\treq = kzalloc(sizeof(*req), GFP_KERNEL);\n\tif (!req)\n\t\treturn NULL;\n\n\treq->gpd = dma_pool_zalloc(md_ctrl->gpd_dmapool, GFP_KERNEL, &req->gpd_addr);\n\tif (!req->gpd) {\n\t\tkfree(req);\n\t\treturn NULL;\n\t}\n\n\treturn req;\n}\n\nstatic int t7xx_cldma_tx_ring_init(struct cldma_ctrl *md_ctrl, struct cldma_ring *ring)\n{\n\tstruct cldma_request *req;\n\tstruct cldma_gpd *gpd;\n\tint i;\n\n\tINIT_LIST_HEAD(&ring->gpd_ring);\n\tring->length = MAX_TX_BUDGET;\n\n\tfor (i = 0; i < ring->length; i++) {\n\t\treq = t7xx_alloc_tx_request(md_ctrl);\n\t\tif (!req) {\n\t\t\tt7xx_cldma_ring_free(md_ctrl, ring, DMA_TO_DEVICE);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tgpd = req->gpd;\n\t\tgpd->flags = GPD_FLAGS_IOC;\n\t\tINIT_LIST_HEAD(&req->entry);\n\t\tlist_add_tail(&req->entry, &ring->gpd_ring);\n\t}\n\n\t \n\tlist_for_each_entry(req, &ring->gpd_ring, entry) {\n\t\tt7xx_cldma_gpd_set_next_ptr(gpd, req->gpd_addr);\n\t\tgpd = req->gpd;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void t7xx_cldma_q_reset(struct cldma_queue *queue)\n{\n\tstruct cldma_request *req;\n\n\treq = list_first_entry(&queue->tr_ring->gpd_ring, struct cldma_request, entry);\n\tqueue->tr_done = req;\n\tqueue->budget = queue->tr_ring->length;\n\n\tif (queue->dir == MTK_TX)\n\t\tqueue->tx_next = req;\n\telse\n\t\tqueue->rx_refill = req;\n}\n\nstatic void t7xx_cldma_rxq_init(struct cldma_queue *queue)\n{\n\tstruct cldma_ctrl *md_ctrl = queue->md_ctrl;\n\n\tqueue->dir = MTK_RX;\n\tqueue->tr_ring = &md_ctrl->rx_ring[queue->index];\n\tt7xx_cldma_q_reset(queue);\n}\n\nstatic void t7xx_cldma_txq_init(struct cldma_queue *queue)\n{\n\tstruct cldma_ctrl *md_ctrl = queue->md_ctrl;\n\n\tqueue->dir = MTK_TX;\n\tqueue->tr_ring = &md_ctrl->tx_ring[queue->index];\n\tt7xx_cldma_q_reset(queue);\n}\n\nstatic void t7xx_cldma_enable_irq(struct cldma_ctrl *md_ctrl)\n{\n\tt7xx_pcie_mac_set_int(md_ctrl->t7xx_dev, md_ctrl->hw_info.phy_interrupt_id);\n}\n\nstatic void t7xx_cldma_disable_irq(struct cldma_ctrl *md_ctrl)\n{\n\tt7xx_pcie_mac_clear_int(md_ctrl->t7xx_dev, md_ctrl->hw_info.phy_interrupt_id);\n}\n\nstatic void t7xx_cldma_irq_work_cb(struct cldma_ctrl *md_ctrl)\n{\n\tunsigned long l2_tx_int_msk, l2_rx_int_msk, l2_tx_int, l2_rx_int, val;\n\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\tint i;\n\n\t \n\tl2_tx_int = ioread32(hw_info->ap_pdn_base + REG_CLDMA_L2TISAR0);\n\tl2_rx_int = ioread32(hw_info->ap_pdn_base + REG_CLDMA_L2RISAR0);\n\tl2_tx_int_msk = ioread32(hw_info->ap_pdn_base + REG_CLDMA_L2TIMR0);\n\tl2_rx_int_msk = ioread32(hw_info->ap_ao_base + REG_CLDMA_L2RIMR0);\n\tl2_tx_int &= ~l2_tx_int_msk;\n\tl2_rx_int &= ~l2_rx_int_msk;\n\n\tif (l2_tx_int) {\n\t\tif (l2_tx_int & (TQ_ERR_INT_BITMASK | TQ_ACTIVE_START_ERR_INT_BITMASK)) {\n\t\t\t \n\t\t\tval = ioread32(hw_info->ap_pdn_base + REG_CLDMA_L3TISAR0);\n\t\t\tiowrite32(val, hw_info->ap_pdn_base + REG_CLDMA_L3TISAR0);\n\t\t\tval = ioread32(hw_info->ap_pdn_base + REG_CLDMA_L3TISAR1);\n\t\t\tiowrite32(val, hw_info->ap_pdn_base + REG_CLDMA_L3TISAR1);\n\t\t}\n\n\t\tt7xx_cldma_hw_tx_done(hw_info, l2_tx_int);\n\t\tif (l2_tx_int & (TXRX_STATUS_BITMASK | EMPTY_STATUS_BITMASK)) {\n\t\t\tfor_each_set_bit(i, &l2_tx_int, L2_INT_BIT_COUNT) {\n\t\t\t\tif (i < CLDMA_TXQ_NUM) {\n\t\t\t\t\tpm_runtime_get(md_ctrl->dev);\n\t\t\t\t\tt7xx_cldma_hw_irq_dis_eq(hw_info, i, MTK_TX);\n\t\t\t\t\tt7xx_cldma_hw_irq_dis_txrx(hw_info, i, MTK_TX);\n\t\t\t\t\tqueue_work(md_ctrl->txq[i].worker,\n\t\t\t\t\t\t   &md_ctrl->txq[i].cldma_work);\n\t\t\t\t} else {\n\t\t\t\t\tt7xx_cldma_txq_empty_hndl(&md_ctrl->txq[i - CLDMA_TXQ_NUM]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (l2_rx_int) {\n\t\tif (l2_rx_int & (RQ_ERR_INT_BITMASK | RQ_ACTIVE_START_ERR_INT_BITMASK)) {\n\t\t\t \n\t\t\tval = ioread32(hw_info->ap_pdn_base + REG_CLDMA_L3RISAR0);\n\t\t\tiowrite32(val, hw_info->ap_pdn_base + REG_CLDMA_L3RISAR0);\n\t\t\tval = ioread32(hw_info->ap_pdn_base + REG_CLDMA_L3RISAR1);\n\t\t\tiowrite32(val, hw_info->ap_pdn_base + REG_CLDMA_L3RISAR1);\n\t\t}\n\n\t\tt7xx_cldma_hw_rx_done(hw_info, l2_rx_int);\n\t\tif (l2_rx_int & (TXRX_STATUS_BITMASK | EMPTY_STATUS_BITMASK)) {\n\t\t\tl2_rx_int |= l2_rx_int >> CLDMA_RXQ_NUM;\n\t\t\tfor_each_set_bit(i, &l2_rx_int, CLDMA_RXQ_NUM) {\n\t\t\t\tpm_runtime_get(md_ctrl->dev);\n\t\t\t\tt7xx_cldma_hw_irq_dis_eq(hw_info, i, MTK_RX);\n\t\t\t\tt7xx_cldma_hw_irq_dis_txrx(hw_info, i, MTK_RX);\n\t\t\t\tqueue_work(md_ctrl->rxq[i].worker, &md_ctrl->rxq[i].cldma_work);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic bool t7xx_cldma_qs_are_active(struct cldma_ctrl *md_ctrl)\n{\n\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\tunsigned int tx_active;\n\tunsigned int rx_active;\n\n\tif (!pci_device_is_present(to_pci_dev(md_ctrl->dev)))\n\t\treturn false;\n\n\ttx_active = t7xx_cldma_hw_queue_status(hw_info, CLDMA_ALL_Q, MTK_TX);\n\trx_active = t7xx_cldma_hw_queue_status(hw_info, CLDMA_ALL_Q, MTK_RX);\n\n\treturn tx_active || rx_active;\n}\n\n \nint t7xx_cldma_stop(struct cldma_ctrl *md_ctrl)\n{\n\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\tbool active;\n\tint i, ret;\n\n\tmd_ctrl->rxq_active = 0;\n\tt7xx_cldma_hw_stop_all_qs(hw_info, MTK_RX);\n\tmd_ctrl->txq_active = 0;\n\tt7xx_cldma_hw_stop_all_qs(hw_info, MTK_TX);\n\tmd_ctrl->txq_started = 0;\n\tt7xx_cldma_disable_irq(md_ctrl);\n\tt7xx_cldma_hw_stop(hw_info, MTK_RX);\n\tt7xx_cldma_hw_stop(hw_info, MTK_TX);\n\tt7xx_cldma_hw_tx_done(hw_info, CLDMA_L2TISAR0_ALL_INT_MASK);\n\tt7xx_cldma_hw_rx_done(hw_info, CLDMA_L2RISAR0_ALL_INT_MASK);\n\n\tif (md_ctrl->is_late_init) {\n\t\tfor (i = 0; i < CLDMA_TXQ_NUM; i++)\n\t\t\tflush_work(&md_ctrl->txq[i].cldma_work);\n\n\t\tfor (i = 0; i < CLDMA_RXQ_NUM; i++)\n\t\t\tflush_work(&md_ctrl->rxq[i].cldma_work);\n\t}\n\n\tret = read_poll_timeout(t7xx_cldma_qs_are_active, active, !active, CHECK_Q_STOP_STEP_US,\n\t\t\t\tCHECK_Q_STOP_TIMEOUT_US, true, md_ctrl);\n\tif (ret)\n\t\tdev_err(md_ctrl->dev, \"Could not stop CLDMA%d queues\", md_ctrl->hif_id);\n\n\treturn ret;\n}\n\nstatic void t7xx_cldma_late_release(struct cldma_ctrl *md_ctrl)\n{\n\tint i;\n\n\tif (!md_ctrl->is_late_init)\n\t\treturn;\n\n\tfor (i = 0; i < CLDMA_TXQ_NUM; i++)\n\t\tt7xx_cldma_ring_free(md_ctrl, &md_ctrl->tx_ring[i], DMA_TO_DEVICE);\n\n\tfor (i = 0; i < CLDMA_RXQ_NUM; i++)\n\t\tt7xx_cldma_ring_free(md_ctrl, &md_ctrl->rx_ring[i], DMA_FROM_DEVICE);\n\n\tdma_pool_destroy(md_ctrl->gpd_dmapool);\n\tmd_ctrl->gpd_dmapool = NULL;\n\tmd_ctrl->is_late_init = false;\n}\n\nvoid t7xx_cldma_reset(struct cldma_ctrl *md_ctrl)\n{\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tmd_ctrl->txq_active = 0;\n\tmd_ctrl->rxq_active = 0;\n\tt7xx_cldma_disable_irq(md_ctrl);\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\n\tfor (i = 0; i < CLDMA_TXQ_NUM; i++) {\n\t\tcancel_work_sync(&md_ctrl->txq[i].cldma_work);\n\n\t\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\t\tmd_cd_queue_struct_reset(&md_ctrl->txq[i], md_ctrl, MTK_TX, i);\n\t\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\t}\n\n\tfor (i = 0; i < CLDMA_RXQ_NUM; i++) {\n\t\tcancel_work_sync(&md_ctrl->rxq[i].cldma_work);\n\n\t\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\t\tmd_cd_queue_struct_reset(&md_ctrl->rxq[i], md_ctrl, MTK_RX, i);\n\t\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\t}\n\n\tt7xx_cldma_late_release(md_ctrl);\n}\n\n \nvoid t7xx_cldma_start(struct cldma_ctrl *md_ctrl)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tif (md_ctrl->is_late_init) {\n\t\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\t\tint i;\n\n\t\tt7xx_cldma_enable_irq(md_ctrl);\n\n\t\tfor (i = 0; i < CLDMA_TXQ_NUM; i++) {\n\t\t\tif (md_ctrl->txq[i].tr_done)\n\t\t\t\tt7xx_cldma_hw_set_start_addr(hw_info, i,\n\t\t\t\t\t\t\t     md_ctrl->txq[i].tr_done->gpd_addr,\n\t\t\t\t\t\t\t     MTK_TX);\n\t\t}\n\n\t\tfor (i = 0; i < CLDMA_RXQ_NUM; i++) {\n\t\t\tif (md_ctrl->rxq[i].tr_done)\n\t\t\t\tt7xx_cldma_hw_set_start_addr(hw_info, i,\n\t\t\t\t\t\t\t     md_ctrl->rxq[i].tr_done->gpd_addr,\n\t\t\t\t\t\t\t     MTK_RX);\n\t\t}\n\n\t\t \n\t\tt7xx_cldma_hw_start_queue(hw_info, CLDMA_ALL_Q, MTK_RX);\n\t\tt7xx_cldma_hw_start(hw_info);\n\t\tmd_ctrl->txq_started = 0;\n\t\tmd_ctrl->txq_active |= TXRX_STATUS_BITMASK;\n\t\tmd_ctrl->rxq_active |= TXRX_STATUS_BITMASK;\n\t}\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n}\n\nstatic void t7xx_cldma_clear_txq(struct cldma_ctrl *md_ctrl, int qnum)\n{\n\tstruct cldma_queue *txq = &md_ctrl->txq[qnum];\n\tstruct cldma_request *req;\n\tstruct cldma_gpd *gpd;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&txq->ring_lock, flags);\n\tt7xx_cldma_q_reset(txq);\n\tlist_for_each_entry(req, &txq->tr_ring->gpd_ring, entry) {\n\t\tgpd = req->gpd;\n\t\tgpd->flags &= ~GPD_FLAGS_HWO;\n\t\tt7xx_cldma_gpd_set_data_ptr(gpd, 0);\n\t\tgpd->data_buff_len = 0;\n\t\tdev_kfree_skb_any(req->skb);\n\t\treq->skb = NULL;\n\t}\n\tspin_unlock_irqrestore(&txq->ring_lock, flags);\n}\n\nstatic int t7xx_cldma_clear_rxq(struct cldma_ctrl *md_ctrl, int qnum)\n{\n\tstruct cldma_queue *rxq = &md_ctrl->rxq[qnum];\n\tstruct cldma_request *req;\n\tstruct cldma_gpd *gpd;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&rxq->ring_lock, flags);\n\tt7xx_cldma_q_reset(rxq);\n\tlist_for_each_entry(req, &rxq->tr_ring->gpd_ring, entry) {\n\t\tgpd = req->gpd;\n\t\tgpd->flags = GPD_FLAGS_IOC | GPD_FLAGS_HWO;\n\t\tgpd->data_buff_len = 0;\n\n\t\tif (req->skb) {\n\t\t\treq->skb->len = 0;\n\t\t\tskb_reset_tail_pointer(req->skb);\n\t\t}\n\t}\n\n\tlist_for_each_entry(req, &rxq->tr_ring->gpd_ring, entry) {\n\t\tif (req->skb)\n\t\t\tcontinue;\n\n\t\tret = t7xx_cldma_alloc_and_map_skb(md_ctrl, req, rxq->tr_ring->pkt_size, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tt7xx_cldma_gpd_set_data_ptr(req->gpd, req->mapped_buff);\n\t}\n\tspin_unlock_irqrestore(&rxq->ring_lock, flags);\n\n\treturn ret;\n}\n\nvoid t7xx_cldma_clear_all_qs(struct cldma_ctrl *md_ctrl, enum mtk_txrx tx_rx)\n{\n\tint i;\n\n\tif (tx_rx == MTK_TX) {\n\t\tfor (i = 0; i < CLDMA_TXQ_NUM; i++)\n\t\t\tt7xx_cldma_clear_txq(md_ctrl, i);\n\t} else {\n\t\tfor (i = 0; i < CLDMA_RXQ_NUM; i++)\n\t\t\tt7xx_cldma_clear_rxq(md_ctrl, i);\n\t}\n}\n\nvoid t7xx_cldma_stop_all_qs(struct cldma_ctrl *md_ctrl, enum mtk_txrx tx_rx)\n{\n\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tt7xx_cldma_hw_irq_dis_eq(hw_info, CLDMA_ALL_Q, tx_rx);\n\tt7xx_cldma_hw_irq_dis_txrx(hw_info, CLDMA_ALL_Q, tx_rx);\n\tif (tx_rx == MTK_RX)\n\t\tmd_ctrl->rxq_active &= ~TXRX_STATUS_BITMASK;\n\telse\n\t\tmd_ctrl->txq_active &= ~TXRX_STATUS_BITMASK;\n\tt7xx_cldma_hw_stop_all_qs(hw_info, tx_rx);\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n}\n\nstatic int t7xx_cldma_gpd_handle_tx_request(struct cldma_queue *queue, struct cldma_request *tx_req,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\tstruct cldma_ctrl *md_ctrl = queue->md_ctrl;\n\tstruct cldma_gpd *gpd = tx_req->gpd;\n\tunsigned long flags;\n\n\t \n\ttx_req->mapped_buff = dma_map_single(md_ctrl->dev, skb->data, skb->len, DMA_TO_DEVICE);\n\n\tif (dma_mapping_error(md_ctrl->dev, tx_req->mapped_buff)) {\n\t\tdev_err(md_ctrl->dev, \"DMA mapping failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tt7xx_cldma_gpd_set_data_ptr(gpd, tx_req->mapped_buff);\n\tgpd->data_buff_len = cpu_to_le16(skb->len);\n\n\t \n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tif (md_ctrl->txq_active & BIT(queue->index))\n\t\tgpd->flags |= GPD_FLAGS_HWO;\n\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\n\ttx_req->skb = skb;\n\treturn 0;\n}\n\n \nstatic void t7xx_cldma_hw_start_send(struct cldma_ctrl *md_ctrl, int qno,\n\t\t\t\t     struct cldma_request *prev_req)\n{\n\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\n\t \n\tif (!t7xx_cldma_tx_addr_is_set(hw_info, qno)) {\n\t\tt7xx_cldma_hw_init(hw_info);\n\t\tt7xx_cldma_hw_set_start_addr(hw_info, qno, prev_req->gpd_addr, MTK_TX);\n\t\tmd_ctrl->txq_started &= ~BIT(qno);\n\t}\n\n\tif (!t7xx_cldma_hw_queue_status(hw_info, qno, MTK_TX)) {\n\t\tif (md_ctrl->txq_started & BIT(qno))\n\t\t\tt7xx_cldma_hw_resume_queue(hw_info, qno, MTK_TX);\n\t\telse\n\t\t\tt7xx_cldma_hw_start_queue(hw_info, qno, MTK_TX);\n\n\t\tmd_ctrl->txq_started |= BIT(qno);\n\t}\n}\n\n \nvoid t7xx_cldma_set_recv_skb(struct cldma_ctrl *md_ctrl,\n\t\t\t     int (*recv_skb)(struct cldma_queue *queue, struct sk_buff *skb))\n{\n\tmd_ctrl->recv_skb = recv_skb;\n}\n\n \nint t7xx_cldma_send_skb(struct cldma_ctrl *md_ctrl, int qno, struct sk_buff *skb)\n{\n\tstruct cldma_request *tx_req;\n\tstruct cldma_queue *queue;\n\tunsigned long flags;\n\tint ret;\n\n\tif (qno >= CLDMA_TXQ_NUM)\n\t\treturn -EINVAL;\n\n\tret = pm_runtime_resume_and_get(md_ctrl->dev);\n\tif (ret < 0 && ret != -EACCES)\n\t\treturn ret;\n\n\tt7xx_pci_disable_sleep(md_ctrl->t7xx_dev);\n\tqueue = &md_ctrl->txq[qno];\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tif (!(md_ctrl->txq_active & BIT(qno))) {\n\t\tret = -EIO;\n\t\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\t\tgoto allow_sleep;\n\t}\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\n\tdo {\n\t\tspin_lock_irqsave(&queue->ring_lock, flags);\n\t\ttx_req = queue->tx_next;\n\t\tif (queue->budget > 0 && !tx_req->skb) {\n\t\t\tstruct list_head *gpd_ring = &queue->tr_ring->gpd_ring;\n\n\t\t\tqueue->budget--;\n\t\t\tt7xx_cldma_gpd_handle_tx_request(queue, tx_req, skb);\n\t\t\tqueue->tx_next = list_next_entry_circular(tx_req, gpd_ring, entry);\n\t\t\tspin_unlock_irqrestore(&queue->ring_lock, flags);\n\n\t\t\tif (!t7xx_pci_sleep_disable_complete(md_ctrl->t7xx_dev)) {\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\t\t\tt7xx_cldma_hw_start_send(md_ctrl, qno, tx_req);\n\t\t\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irqrestore(&queue->ring_lock, flags);\n\n\t\tif (!t7xx_pci_sleep_disable_complete(md_ctrl->t7xx_dev)) {\n\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!t7xx_cldma_hw_queue_status(&md_ctrl->hw_info, qno, MTK_TX)) {\n\t\t\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\t\t\tt7xx_cldma_hw_resume_queue(&md_ctrl->hw_info, qno, MTK_TX);\n\t\t\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\t\t}\n\n\t\tret = wait_event_interruptible_exclusive(queue->req_wq, queue->budget > 0);\n\t} while (!ret);\n\nallow_sleep:\n\tt7xx_pci_enable_sleep(md_ctrl->t7xx_dev);\n\tpm_runtime_mark_last_busy(md_ctrl->dev);\n\tpm_runtime_put_autosuspend(md_ctrl->dev);\n\treturn ret;\n}\n\nstatic int t7xx_cldma_late_init(struct cldma_ctrl *md_ctrl)\n{\n\tchar dma_pool_name[32];\n\tint i, j, ret;\n\n\tif (md_ctrl->is_late_init) {\n\t\tdev_err(md_ctrl->dev, \"CLDMA late init was already done\\n\");\n\t\treturn -EALREADY;\n\t}\n\n\tsnprintf(dma_pool_name, sizeof(dma_pool_name), \"cldma_req_hif%d\", md_ctrl->hif_id);\n\n\tmd_ctrl->gpd_dmapool = dma_pool_create(dma_pool_name, md_ctrl->dev,\n\t\t\t\t\t       sizeof(struct cldma_gpd), GPD_DMAPOOL_ALIGN, 0);\n\tif (!md_ctrl->gpd_dmapool) {\n\t\tdev_err(md_ctrl->dev, \"DMA pool alloc fail\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < CLDMA_TXQ_NUM; i++) {\n\t\tret = t7xx_cldma_tx_ring_init(md_ctrl, &md_ctrl->tx_ring[i]);\n\t\tif (ret) {\n\t\t\tdev_err(md_ctrl->dev, \"control TX ring init fail\\n\");\n\t\t\tgoto err_free_tx_ring;\n\t\t}\n\n\t\tmd_ctrl->tx_ring[i].pkt_size = CLDMA_MTU;\n\t}\n\n\tfor (j = 0; j < CLDMA_RXQ_NUM; j++) {\n\t\tmd_ctrl->rx_ring[j].pkt_size = CLDMA_MTU;\n\n\t\tif (j == CLDMA_RXQ_NUM - 1)\n\t\t\tmd_ctrl->rx_ring[j].pkt_size = CLDMA_JUMBO_BUFF_SZ;\n\n\t\tret = t7xx_cldma_rx_ring_init(md_ctrl, &md_ctrl->rx_ring[j]);\n\t\tif (ret) {\n\t\t\tdev_err(md_ctrl->dev, \"Control RX ring init fail\\n\");\n\t\t\tgoto err_free_rx_ring;\n\t\t}\n\t}\n\n\tfor (i = 0; i < CLDMA_TXQ_NUM; i++)\n\t\tt7xx_cldma_txq_init(&md_ctrl->txq[i]);\n\n\tfor (j = 0; j < CLDMA_RXQ_NUM; j++)\n\t\tt7xx_cldma_rxq_init(&md_ctrl->rxq[j]);\n\n\tmd_ctrl->is_late_init = true;\n\treturn 0;\n\nerr_free_rx_ring:\n\twhile (j--)\n\t\tt7xx_cldma_ring_free(md_ctrl, &md_ctrl->rx_ring[j], DMA_FROM_DEVICE);\n\nerr_free_tx_ring:\n\twhile (i--)\n\t\tt7xx_cldma_ring_free(md_ctrl, &md_ctrl->tx_ring[i], DMA_TO_DEVICE);\n\n\treturn ret;\n}\n\nstatic void __iomem *t7xx_pcie_addr_transfer(void __iomem *addr, u32 addr_trs1, u32 phy_addr)\n{\n\treturn addr + phy_addr - addr_trs1;\n}\n\nstatic void t7xx_hw_info_init(struct cldma_ctrl *md_ctrl)\n{\n\tstruct t7xx_addr_base *pbase = &md_ctrl->t7xx_dev->base_addr;\n\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\tu32 phy_ao_base, phy_pd_base;\n\n\thw_info->hw_mode = MODE_BIT_64;\n\n\tif (md_ctrl->hif_id == CLDMA_ID_MD) {\n\t\tphy_ao_base = CLDMA1_AO_BASE;\n\t\tphy_pd_base = CLDMA1_PD_BASE;\n\t\thw_info->phy_interrupt_id = CLDMA1_INT;\n\t} else {\n\t\tphy_ao_base = CLDMA0_AO_BASE;\n\t\tphy_pd_base = CLDMA0_PD_BASE;\n\t\thw_info->phy_interrupt_id = CLDMA0_INT;\n\t}\n\n\thw_info->ap_ao_base = t7xx_pcie_addr_transfer(pbase->pcie_ext_reg_base,\n\t\t\t\t\t\t      pbase->pcie_dev_reg_trsl_addr, phy_ao_base);\n\thw_info->ap_pdn_base = t7xx_pcie_addr_transfer(pbase->pcie_ext_reg_base,\n\t\t\t\t\t\t       pbase->pcie_dev_reg_trsl_addr, phy_pd_base);\n}\n\nstatic int t7xx_cldma_default_recv_skb(struct cldma_queue *queue, struct sk_buff *skb)\n{\n\tdev_kfree_skb_any(skb);\n\treturn 0;\n}\n\nint t7xx_cldma_alloc(enum cldma_id hif_id, struct t7xx_pci_dev *t7xx_dev)\n{\n\tstruct device *dev = &t7xx_dev->pdev->dev;\n\tstruct cldma_ctrl *md_ctrl;\n\n\tmd_ctrl = devm_kzalloc(dev, sizeof(*md_ctrl), GFP_KERNEL);\n\tif (!md_ctrl)\n\t\treturn -ENOMEM;\n\n\tmd_ctrl->t7xx_dev = t7xx_dev;\n\tmd_ctrl->dev = dev;\n\tmd_ctrl->hif_id = hif_id;\n\tmd_ctrl->recv_skb = t7xx_cldma_default_recv_skb;\n\tt7xx_hw_info_init(md_ctrl);\n\tt7xx_dev->md->md_ctrl[hif_id] = md_ctrl;\n\treturn 0;\n}\n\nstatic void t7xx_cldma_resume_early(struct t7xx_pci_dev *t7xx_dev, void *entity_param)\n{\n\tstruct cldma_ctrl *md_ctrl = entity_param;\n\tstruct t7xx_cldma_hw *hw_info;\n\tunsigned long flags;\n\tint qno_t;\n\n\thw_info = &md_ctrl->hw_info;\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tt7xx_cldma_hw_restore(hw_info);\n\tfor (qno_t = 0; qno_t < CLDMA_TXQ_NUM; qno_t++) {\n\t\tt7xx_cldma_hw_set_start_addr(hw_info, qno_t, md_ctrl->txq[qno_t].tx_next->gpd_addr,\n\t\t\t\t\t     MTK_TX);\n\t\tt7xx_cldma_hw_set_start_addr(hw_info, qno_t, md_ctrl->rxq[qno_t].tr_done->gpd_addr,\n\t\t\t\t\t     MTK_RX);\n\t}\n\tt7xx_cldma_enable_irq(md_ctrl);\n\tt7xx_cldma_hw_start_queue(hw_info, CLDMA_ALL_Q, MTK_RX);\n\tmd_ctrl->rxq_active |= TXRX_STATUS_BITMASK;\n\tt7xx_cldma_hw_irq_en_eq(hw_info, CLDMA_ALL_Q, MTK_RX);\n\tt7xx_cldma_hw_irq_en_txrx(hw_info, CLDMA_ALL_Q, MTK_RX);\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n}\n\nstatic int t7xx_cldma_resume(struct t7xx_pci_dev *t7xx_dev, void *entity_param)\n{\n\tstruct cldma_ctrl *md_ctrl = entity_param;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tmd_ctrl->txq_active |= TXRX_STATUS_BITMASK;\n\tt7xx_cldma_hw_irq_en_txrx(&md_ctrl->hw_info, CLDMA_ALL_Q, MTK_TX);\n\tt7xx_cldma_hw_irq_en_eq(&md_ctrl->hw_info, CLDMA_ALL_Q, MTK_TX);\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\n\tif (md_ctrl->hif_id == CLDMA_ID_MD)\n\t\tt7xx_mhccif_mask_clr(t7xx_dev, D2H_SW_INT_MASK);\n\n\treturn 0;\n}\n\nstatic void t7xx_cldma_suspend_late(struct t7xx_pci_dev *t7xx_dev, void *entity_param)\n{\n\tstruct cldma_ctrl *md_ctrl = entity_param;\n\tstruct t7xx_cldma_hw *hw_info;\n\tunsigned long flags;\n\n\thw_info = &md_ctrl->hw_info;\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tt7xx_cldma_hw_irq_dis_eq(hw_info, CLDMA_ALL_Q, MTK_RX);\n\tt7xx_cldma_hw_irq_dis_txrx(hw_info, CLDMA_ALL_Q, MTK_RX);\n\tmd_ctrl->rxq_active &= ~TXRX_STATUS_BITMASK;\n\tt7xx_cldma_hw_stop_all_qs(hw_info, MTK_RX);\n\tt7xx_cldma_clear_ip_busy(hw_info);\n\tt7xx_cldma_disable_irq(md_ctrl);\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n}\n\nstatic int t7xx_cldma_suspend(struct t7xx_pci_dev *t7xx_dev, void *entity_param)\n{\n\tstruct cldma_ctrl *md_ctrl = entity_param;\n\tstruct t7xx_cldma_hw *hw_info;\n\tunsigned long flags;\n\n\tif (md_ctrl->hif_id == CLDMA_ID_MD)\n\t\tt7xx_mhccif_mask_set(t7xx_dev, D2H_SW_INT_MASK);\n\n\thw_info = &md_ctrl->hw_info;\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tt7xx_cldma_hw_irq_dis_eq(hw_info, CLDMA_ALL_Q, MTK_TX);\n\tt7xx_cldma_hw_irq_dis_txrx(hw_info, CLDMA_ALL_Q, MTK_TX);\n\tmd_ctrl->txq_active &= ~TXRX_STATUS_BITMASK;\n\tt7xx_cldma_hw_stop_all_qs(hw_info, MTK_TX);\n\tmd_ctrl->txq_started = 0;\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n\n\treturn 0;\n}\n\nstatic int t7xx_cldma_pm_init(struct cldma_ctrl *md_ctrl)\n{\n\tmd_ctrl->pm_entity = kzalloc(sizeof(*md_ctrl->pm_entity), GFP_KERNEL);\n\tif (!md_ctrl->pm_entity)\n\t\treturn -ENOMEM;\n\n\tmd_ctrl->pm_entity->entity_param = md_ctrl;\n\n\tif (md_ctrl->hif_id == CLDMA_ID_MD)\n\t\tmd_ctrl->pm_entity->id = PM_ENTITY_ID_CTRL1;\n\telse\n\t\tmd_ctrl->pm_entity->id = PM_ENTITY_ID_CTRL2;\n\n\tmd_ctrl->pm_entity->suspend = t7xx_cldma_suspend;\n\tmd_ctrl->pm_entity->suspend_late = t7xx_cldma_suspend_late;\n\tmd_ctrl->pm_entity->resume = t7xx_cldma_resume;\n\tmd_ctrl->pm_entity->resume_early = t7xx_cldma_resume_early;\n\n\treturn t7xx_pci_pm_entity_register(md_ctrl->t7xx_dev, md_ctrl->pm_entity);\n}\n\nstatic int t7xx_cldma_pm_uninit(struct cldma_ctrl *md_ctrl)\n{\n\tif (!md_ctrl->pm_entity)\n\t\treturn -EINVAL;\n\n\tt7xx_pci_pm_entity_unregister(md_ctrl->t7xx_dev, md_ctrl->pm_entity);\n\tkfree(md_ctrl->pm_entity);\n\tmd_ctrl->pm_entity = NULL;\n\treturn 0;\n}\n\nvoid t7xx_cldma_hif_hw_init(struct cldma_ctrl *md_ctrl)\n{\n\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md_ctrl->cldma_lock, flags);\n\tt7xx_cldma_hw_stop(hw_info, MTK_TX);\n\tt7xx_cldma_hw_stop(hw_info, MTK_RX);\n\tt7xx_cldma_hw_rx_done(hw_info, EMPTY_STATUS_BITMASK | TXRX_STATUS_BITMASK);\n\tt7xx_cldma_hw_tx_done(hw_info, EMPTY_STATUS_BITMASK | TXRX_STATUS_BITMASK);\n\tt7xx_cldma_hw_init(hw_info);\n\tspin_unlock_irqrestore(&md_ctrl->cldma_lock, flags);\n}\n\nstatic irqreturn_t t7xx_cldma_isr_handler(int irq, void *data)\n{\n\tstruct cldma_ctrl *md_ctrl = data;\n\tu32 interrupt;\n\n\tinterrupt = md_ctrl->hw_info.phy_interrupt_id;\n\tt7xx_pcie_mac_clear_int(md_ctrl->t7xx_dev, interrupt);\n\tt7xx_cldma_irq_work_cb(md_ctrl);\n\tt7xx_pcie_mac_clear_int_status(md_ctrl->t7xx_dev, interrupt);\n\tt7xx_pcie_mac_set_int(md_ctrl->t7xx_dev, interrupt);\n\treturn IRQ_HANDLED;\n}\n\nstatic void t7xx_cldma_destroy_wqs(struct cldma_ctrl *md_ctrl)\n{\n\tint i;\n\n\tfor (i = 0; i < CLDMA_TXQ_NUM; i++) {\n\t\tif (md_ctrl->txq[i].worker) {\n\t\t\tdestroy_workqueue(md_ctrl->txq[i].worker);\n\t\t\tmd_ctrl->txq[i].worker = NULL;\n\t\t}\n\t}\n\n\tfor (i = 0; i < CLDMA_RXQ_NUM; i++) {\n\t\tif (md_ctrl->rxq[i].worker) {\n\t\t\tdestroy_workqueue(md_ctrl->rxq[i].worker);\n\t\t\tmd_ctrl->rxq[i].worker = NULL;\n\t\t}\n\t}\n}\n\n \nint t7xx_cldma_init(struct cldma_ctrl *md_ctrl)\n{\n\tstruct t7xx_cldma_hw *hw_info = &md_ctrl->hw_info;\n\tint ret, i;\n\n\tmd_ctrl->txq_active = 0;\n\tmd_ctrl->rxq_active = 0;\n\tmd_ctrl->is_late_init = false;\n\n\tret = t7xx_cldma_pm_init(md_ctrl);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_init(&md_ctrl->cldma_lock);\n\n\tfor (i = 0; i < CLDMA_TXQ_NUM; i++) {\n\t\tmd_cd_queue_struct_init(&md_ctrl->txq[i], md_ctrl, MTK_TX, i);\n\t\tmd_ctrl->txq[i].worker =\n\t\t\talloc_ordered_workqueue(\"md_hif%d_tx%d_worker\",\n\t\t\t\t\tWQ_MEM_RECLAIM | (i ? 0 : WQ_HIGHPRI),\n\t\t\t\t\tmd_ctrl->hif_id, i);\n\t\tif (!md_ctrl->txq[i].worker)\n\t\t\tgoto err_workqueue;\n\n\t\tINIT_WORK(&md_ctrl->txq[i].cldma_work, t7xx_cldma_tx_done);\n\t}\n\n\tfor (i = 0; i < CLDMA_RXQ_NUM; i++) {\n\t\tmd_cd_queue_struct_init(&md_ctrl->rxq[i], md_ctrl, MTK_RX, i);\n\t\tINIT_WORK(&md_ctrl->rxq[i].cldma_work, t7xx_cldma_rx_done);\n\n\t\tmd_ctrl->rxq[i].worker =\n\t\t\talloc_ordered_workqueue(\"md_hif%d_rx%d_worker\",\n\t\t\t\t\t\tWQ_MEM_RECLAIM,\n\t\t\t\t\t\tmd_ctrl->hif_id, i);\n\t\tif (!md_ctrl->rxq[i].worker)\n\t\t\tgoto err_workqueue;\n\t}\n\n\tt7xx_pcie_mac_clear_int(md_ctrl->t7xx_dev, hw_info->phy_interrupt_id);\n\tmd_ctrl->t7xx_dev->intr_handler[hw_info->phy_interrupt_id] = t7xx_cldma_isr_handler;\n\tmd_ctrl->t7xx_dev->intr_thread[hw_info->phy_interrupt_id] = NULL;\n\tmd_ctrl->t7xx_dev->callback_param[hw_info->phy_interrupt_id] = md_ctrl;\n\tt7xx_pcie_mac_clear_int_status(md_ctrl->t7xx_dev, hw_info->phy_interrupt_id);\n\treturn 0;\n\nerr_workqueue:\n\tt7xx_cldma_destroy_wqs(md_ctrl);\n\tt7xx_cldma_pm_uninit(md_ctrl);\n\treturn -ENOMEM;\n}\n\nvoid t7xx_cldma_switch_cfg(struct cldma_ctrl *md_ctrl)\n{\n\tt7xx_cldma_late_release(md_ctrl);\n\tt7xx_cldma_late_init(md_ctrl);\n}\n\nvoid t7xx_cldma_exit(struct cldma_ctrl *md_ctrl)\n{\n\tt7xx_cldma_stop(md_ctrl);\n\tt7xx_cldma_late_release(md_ctrl);\n\tt7xx_cldma_destroy_wqs(md_ctrl);\n\tt7xx_cldma_pm_uninit(md_ctrl);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}