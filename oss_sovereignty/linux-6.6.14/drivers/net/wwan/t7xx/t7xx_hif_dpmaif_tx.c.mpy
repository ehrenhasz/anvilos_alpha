{
  "module_name": "t7xx_hif_dpmaif_tx.c",
  "hash_id": "3077765eb9a9a7bac4d2247cab203364ab662613996b2c36df47ff9f894d6716",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wwan/t7xx/t7xx_hif_dpmaif_tx.c",
  "human_readable_source": "\n \n\n#include <linux/atomic.h>\n#include <linux/bitfield.h>\n#include <linux/delay.h>\n#include <linux/device.h>\n#include <linux/dma-direction.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/gfp.h>\n#include <linux/kernel.h>\n#include <linux/kthread.h>\n#include <linux/list.h>\n#include <linux/minmax.h>\n#include <linux/netdevice.h>\n#include <linux/pm_runtime.h>\n#include <linux/sched.h>\n#include <linux/spinlock.h>\n#include <linux/skbuff.h>\n#include <linux/types.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n\n#include \"t7xx_dpmaif.h\"\n#include \"t7xx_hif_dpmaif.h\"\n#include \"t7xx_hif_dpmaif_tx.h\"\n#include \"t7xx_pci.h\"\n\n#define DPMAIF_SKB_TX_BURST_CNT\t5\n#define DPMAIF_DRB_LIST_LEN\t6144\n\n \n#define DES_DTYP_PD\t\t0\n#define DES_DTYP_MSG\t\t1\n\nstatic unsigned int t7xx_dpmaif_update_drb_rd_idx(struct dpmaif_ctrl *dpmaif_ctrl,\n\t\t\t\t\t\t  unsigned int q_num)\n{\n\tstruct dpmaif_tx_queue *txq = &dpmaif_ctrl->txq[q_num];\n\tunsigned int old_sw_rd_idx, new_hw_rd_idx, drb_cnt;\n\tunsigned long flags;\n\n\tif (!txq->que_started)\n\t\treturn 0;\n\n\told_sw_rd_idx = txq->drb_rd_idx;\n\tnew_hw_rd_idx = t7xx_dpmaif_ul_get_rd_idx(&dpmaif_ctrl->hw_info, q_num);\n\tif (new_hw_rd_idx >= DPMAIF_DRB_LIST_LEN) {\n\t\tdev_err(dpmaif_ctrl->dev, \"Out of range read index: %u\\n\", new_hw_rd_idx);\n\t\treturn 0;\n\t}\n\n\tif (old_sw_rd_idx <= new_hw_rd_idx)\n\t\tdrb_cnt = new_hw_rd_idx - old_sw_rd_idx;\n\telse\n\t\tdrb_cnt = txq->drb_size_cnt - old_sw_rd_idx + new_hw_rd_idx;\n\n\tspin_lock_irqsave(&txq->tx_lock, flags);\n\ttxq->drb_rd_idx = new_hw_rd_idx;\n\tspin_unlock_irqrestore(&txq->tx_lock, flags);\n\n\treturn drb_cnt;\n}\n\nstatic unsigned int t7xx_dpmaif_release_tx_buffer(struct dpmaif_ctrl *dpmaif_ctrl,\n\t\t\t\t\t\t  unsigned int q_num, unsigned int release_cnt)\n{\n\tstruct dpmaif_tx_queue *txq = &dpmaif_ctrl->txq[q_num];\n\tstruct dpmaif_callbacks *cb = dpmaif_ctrl->callbacks;\n\tstruct dpmaif_drb_skb *cur_drb_skb, *drb_skb_base;\n\tstruct dpmaif_drb *cur_drb, *drb_base;\n\tunsigned int drb_cnt, i, cur_idx;\n\tunsigned long flags;\n\n\tdrb_skb_base = txq->drb_skb_base;\n\tdrb_base = txq->drb_base;\n\n\tspin_lock_irqsave(&txq->tx_lock, flags);\n\tdrb_cnt = txq->drb_size_cnt;\n\tcur_idx = txq->drb_release_rd_idx;\n\tspin_unlock_irqrestore(&txq->tx_lock, flags);\n\n\tfor (i = 0; i < release_cnt; i++) {\n\t\tcur_drb = drb_base + cur_idx;\n\t\tif (FIELD_GET(DRB_HDR_DTYP, le32_to_cpu(cur_drb->header)) == DES_DTYP_PD) {\n\t\t\tcur_drb_skb = drb_skb_base + cur_idx;\n\t\t\tif (!cur_drb_skb->is_msg)\n\t\t\t\tdma_unmap_single(dpmaif_ctrl->dev, cur_drb_skb->bus_addr,\n\t\t\t\t\t\t cur_drb_skb->data_len, DMA_TO_DEVICE);\n\n\t\t\tif (!FIELD_GET(DRB_HDR_CONT, le32_to_cpu(cur_drb->header))) {\n\t\t\t\tif (!cur_drb_skb->skb) {\n\t\t\t\t\tdev_err(dpmaif_ctrl->dev,\n\t\t\t\t\t\t\"txq%u: DRB check fail, invalid skb\\n\", q_num);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tdev_kfree_skb_any(cur_drb_skb->skb);\n\t\t\t}\n\n\t\t\tcur_drb_skb->skb = NULL;\n\t\t}\n\n\t\tspin_lock_irqsave(&txq->tx_lock, flags);\n\t\tcur_idx = t7xx_ring_buf_get_next_wr_idx(drb_cnt, cur_idx);\n\t\ttxq->drb_release_rd_idx = cur_idx;\n\t\tspin_unlock_irqrestore(&txq->tx_lock, flags);\n\n\t\tif (atomic_inc_return(&txq->tx_budget) > txq->drb_size_cnt / 8)\n\t\t\tcb->state_notify(dpmaif_ctrl->t7xx_dev, DMPAIF_TXQ_STATE_IRQ, txq->index);\n\t}\n\n\tif (FIELD_GET(DRB_HDR_CONT, le32_to_cpu(cur_drb->header)))\n\t\tdev_err(dpmaif_ctrl->dev, \"txq%u: DRB not marked as the last one\\n\", q_num);\n\n\treturn i;\n}\n\nstatic int t7xx_dpmaif_tx_release(struct dpmaif_ctrl *dpmaif_ctrl,\n\t\t\t\t  unsigned int q_num, unsigned int budget)\n{\n\tstruct dpmaif_tx_queue *txq = &dpmaif_ctrl->txq[q_num];\n\tunsigned int rel_cnt, real_rel_cnt;\n\n\t \n\tt7xx_dpmaif_update_drb_rd_idx(dpmaif_ctrl, q_num);\n\n\trel_cnt = t7xx_ring_buf_rd_wr_count(txq->drb_size_cnt, txq->drb_release_rd_idx,\n\t\t\t\t\t    txq->drb_rd_idx, DPMAIF_READ);\n\n\treal_rel_cnt = min_not_zero(budget, rel_cnt);\n\tif (real_rel_cnt)\n\t\treal_rel_cnt = t7xx_dpmaif_release_tx_buffer(dpmaif_ctrl, q_num, real_rel_cnt);\n\n\treturn real_rel_cnt < rel_cnt ? -EAGAIN : 0;\n}\n\nstatic bool t7xx_dpmaif_drb_ring_not_empty(struct dpmaif_tx_queue *txq)\n{\n\treturn !!t7xx_dpmaif_update_drb_rd_idx(txq->dpmaif_ctrl, txq->index);\n}\n\nstatic void t7xx_dpmaif_tx_done(struct work_struct *work)\n{\n\tstruct dpmaif_tx_queue *txq = container_of(work, struct dpmaif_tx_queue, dpmaif_tx_work);\n\tstruct dpmaif_ctrl *dpmaif_ctrl = txq->dpmaif_ctrl;\n\tstruct dpmaif_hw_info *hw_info;\n\tint ret;\n\n\tret = pm_runtime_resume_and_get(dpmaif_ctrl->dev);\n\tif (ret < 0 && ret != -EACCES)\n\t\treturn;\n\n\t \n\tt7xx_pci_disable_sleep(dpmaif_ctrl->t7xx_dev);\n\tif (t7xx_pci_sleep_disable_complete(dpmaif_ctrl->t7xx_dev)) {\n\t\thw_info = &dpmaif_ctrl->hw_info;\n\t\tret = t7xx_dpmaif_tx_release(dpmaif_ctrl, txq->index, txq->drb_size_cnt);\n\t\tif (ret == -EAGAIN ||\n\t\t    (t7xx_dpmaif_ul_clr_done(hw_info, txq->index) &&\n\t\t     t7xx_dpmaif_drb_ring_not_empty(txq))) {\n\t\t\tqueue_work(dpmaif_ctrl->txq[txq->index].worker,\n\t\t\t\t   &dpmaif_ctrl->txq[txq->index].dpmaif_tx_work);\n\t\t\t \n\t\t\tt7xx_dpmaif_clr_ip_busy_sts(hw_info);\n\t\t} else {\n\t\t\tt7xx_dpmaif_clr_ip_busy_sts(hw_info);\n\t\t\tt7xx_dpmaif_unmask_ulq_intr(hw_info, txq->index);\n\t\t}\n\t}\n\n\tt7xx_pci_enable_sleep(dpmaif_ctrl->t7xx_dev);\n\tpm_runtime_mark_last_busy(dpmaif_ctrl->dev);\n\tpm_runtime_put_autosuspend(dpmaif_ctrl->dev);\n}\n\nstatic void t7xx_setup_msg_drb(struct dpmaif_ctrl *dpmaif_ctrl, unsigned int q_num,\n\t\t\t       unsigned int cur_idx, unsigned int pkt_len, unsigned int count_l,\n\t\t\t       unsigned int channel_id)\n{\n\tstruct dpmaif_drb *drb_base = dpmaif_ctrl->txq[q_num].drb_base;\n\tstruct dpmaif_drb *drb = drb_base + cur_idx;\n\n\tdrb->header = cpu_to_le32(FIELD_PREP(DRB_HDR_DTYP, DES_DTYP_MSG) |\n\t\t\t\t  FIELD_PREP(DRB_HDR_CONT, 1) |\n\t\t\t\t  FIELD_PREP(DRB_HDR_DATA_LEN, pkt_len));\n\n\tdrb->msg.msg_hdr = cpu_to_le32(FIELD_PREP(DRB_MSG_COUNT_L, count_l) |\n\t\t\t\t       FIELD_PREP(DRB_MSG_CHANNEL_ID, channel_id) |\n\t\t\t\t       FIELD_PREP(DRB_MSG_L4_CHK, 1));\n}\n\nstatic void t7xx_setup_payload_drb(struct dpmaif_ctrl *dpmaif_ctrl, unsigned int q_num,\n\t\t\t\t   unsigned int cur_idx, dma_addr_t data_addr,\n\t\t\t\t   unsigned int pkt_size, bool last_one)\n{\n\tstruct dpmaif_drb *drb_base = dpmaif_ctrl->txq[q_num].drb_base;\n\tstruct dpmaif_drb *drb = drb_base + cur_idx;\n\tu32 header;\n\n\theader = FIELD_PREP(DRB_HDR_DTYP, DES_DTYP_PD) | FIELD_PREP(DRB_HDR_DATA_LEN, pkt_size);\n\tif (!last_one)\n\t\theader |= FIELD_PREP(DRB_HDR_CONT, 1);\n\n\tdrb->header = cpu_to_le32(header);\n\tdrb->pd.data_addr_l = cpu_to_le32(lower_32_bits(data_addr));\n\tdrb->pd.data_addr_h = cpu_to_le32(upper_32_bits(data_addr));\n}\n\nstatic void t7xx_record_drb_skb(struct dpmaif_ctrl *dpmaif_ctrl, unsigned int q_num,\n\t\t\t\tunsigned int cur_idx, struct sk_buff *skb, bool is_msg,\n\t\t\t\tbool is_frag, bool is_last_one, dma_addr_t bus_addr,\n\t\t\t\tunsigned int data_len)\n{\n\tstruct dpmaif_drb_skb *drb_skb_base = dpmaif_ctrl->txq[q_num].drb_skb_base;\n\tstruct dpmaif_drb_skb *drb_skb = drb_skb_base + cur_idx;\n\n\tdrb_skb->skb = skb;\n\tdrb_skb->bus_addr = bus_addr;\n\tdrb_skb->data_len = data_len;\n\tdrb_skb->index = cur_idx;\n\tdrb_skb->is_msg = is_msg;\n\tdrb_skb->is_frag = is_frag;\n\tdrb_skb->is_last = is_last_one;\n}\n\nstatic int t7xx_dpmaif_add_skb_to_ring(struct dpmaif_ctrl *dpmaif_ctrl, struct sk_buff *skb)\n{\n\tstruct dpmaif_callbacks *cb = dpmaif_ctrl->callbacks;\n\tunsigned int wr_cnt, send_cnt, payload_cnt;\n\tunsigned int cur_idx, drb_wr_idx_backup;\n\tstruct skb_shared_info *shinfo;\n\tstruct dpmaif_tx_queue *txq;\n\tstruct t7xx_skb_cb *skb_cb;\n\tunsigned long flags;\n\n\tskb_cb = T7XX_SKB_CB(skb);\n\ttxq = &dpmaif_ctrl->txq[skb_cb->txq_number];\n\tif (!txq->que_started || dpmaif_ctrl->state != DPMAIF_STATE_PWRON)\n\t\treturn -ENODEV;\n\n\tatomic_set(&txq->tx_processing, 1);\n\t  \n\tsmp_mb();\n\n\tshinfo = skb_shinfo(skb);\n\tif (shinfo->frag_list)\n\t\tdev_warn_ratelimited(dpmaif_ctrl->dev, \"frag_list not supported\\n\");\n\n\tpayload_cnt = shinfo->nr_frags + 1;\n\t \n\tsend_cnt = payload_cnt + 1;\n\n\tspin_lock_irqsave(&txq->tx_lock, flags);\n\tcur_idx = txq->drb_wr_idx;\n\tdrb_wr_idx_backup = cur_idx;\n\ttxq->drb_wr_idx += send_cnt;\n\tif (txq->drb_wr_idx >= txq->drb_size_cnt)\n\t\ttxq->drb_wr_idx -= txq->drb_size_cnt;\n\tt7xx_setup_msg_drb(dpmaif_ctrl, txq->index, cur_idx, skb->len, 0, skb_cb->netif_idx);\n\tt7xx_record_drb_skb(dpmaif_ctrl, txq->index, cur_idx, skb, true, 0, 0, 0, 0);\n\tspin_unlock_irqrestore(&txq->tx_lock, flags);\n\n\tfor (wr_cnt = 0; wr_cnt < payload_cnt; wr_cnt++) {\n\t\tbool is_frag, is_last_one = wr_cnt == payload_cnt - 1;\n\t\tunsigned int data_len;\n\t\tdma_addr_t bus_addr;\n\t\tvoid *data_addr;\n\n\t\tif (!wr_cnt) {\n\t\t\tdata_len = skb_headlen(skb);\n\t\t\tdata_addr = skb->data;\n\t\t\tis_frag = false;\n\t\t} else {\n\t\t\tskb_frag_t *frag = shinfo->frags + wr_cnt - 1;\n\n\t\t\tdata_len = skb_frag_size(frag);\n\t\t\tdata_addr = skb_frag_address(frag);\n\t\t\tis_frag = true;\n\t\t}\n\n\t\tbus_addr = dma_map_single(dpmaif_ctrl->dev, data_addr, data_len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dpmaif_ctrl->dev, bus_addr))\n\t\t\tgoto unmap_buffers;\n\n\t\tcur_idx = t7xx_ring_buf_get_next_wr_idx(txq->drb_size_cnt, cur_idx);\n\n\t\tspin_lock_irqsave(&txq->tx_lock, flags);\n\t\tt7xx_setup_payload_drb(dpmaif_ctrl, txq->index, cur_idx, bus_addr, data_len,\n\t\t\t\t       is_last_one);\n\t\tt7xx_record_drb_skb(dpmaif_ctrl, txq->index, cur_idx, skb, false, is_frag,\n\t\t\t\t    is_last_one, bus_addr, data_len);\n\t\tspin_unlock_irqrestore(&txq->tx_lock, flags);\n\t}\n\n\tif (atomic_sub_return(send_cnt, &txq->tx_budget) <= (MAX_SKB_FRAGS + 2))\n\t\tcb->state_notify(dpmaif_ctrl->t7xx_dev, DMPAIF_TXQ_STATE_FULL, txq->index);\n\n\tatomic_set(&txq->tx_processing, 0);\n\n\treturn 0;\n\nunmap_buffers:\n\twhile (wr_cnt--) {\n\t\tstruct dpmaif_drb_skb *drb_skb = txq->drb_skb_base;\n\n\t\tcur_idx = cur_idx ? cur_idx - 1 : txq->drb_size_cnt - 1;\n\t\tdrb_skb += cur_idx;\n\t\tdma_unmap_single(dpmaif_ctrl->dev, drb_skb->bus_addr,\n\t\t\t\t drb_skb->data_len, DMA_TO_DEVICE);\n\t}\n\n\ttxq->drb_wr_idx = drb_wr_idx_backup;\n\tatomic_set(&txq->tx_processing, 0);\n\n\treturn -ENOMEM;\n}\n\nstatic bool t7xx_tx_lists_are_all_empty(const struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tint i;\n\n\tfor (i = 0; i < DPMAIF_TXQ_NUM; i++) {\n\t\tif (!skb_queue_empty(&dpmaif_ctrl->txq[i].tx_skb_head))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic struct dpmaif_tx_queue *t7xx_select_tx_queue(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tstruct dpmaif_tx_queue *txq;\n\n\ttxq = &dpmaif_ctrl->txq[DPMAIF_TX_DEFAULT_QUEUE];\n\tif (!txq->que_started)\n\t\treturn NULL;\n\n\treturn txq;\n}\n\nstatic unsigned int t7xx_txq_drb_wr_available(struct dpmaif_tx_queue *txq)\n{\n\treturn t7xx_ring_buf_rd_wr_count(txq->drb_size_cnt, txq->drb_release_rd_idx,\n\t\t\t\t\t txq->drb_wr_idx, DPMAIF_WRITE);\n}\n\nstatic unsigned int t7xx_skb_drb_cnt(struct sk_buff *skb)\n{\n\t \n\treturn skb_shinfo(skb)->nr_frags + 2;\n}\n\nstatic int t7xx_txq_burst_send_skb(struct dpmaif_tx_queue *txq)\n{\n\tunsigned int drb_remain_cnt, i;\n\tunsigned int send_drb_cnt;\n\tint drb_cnt = 0;\n\tint ret = 0;\n\n\tdrb_remain_cnt = t7xx_txq_drb_wr_available(txq);\n\n\tfor (i = 0; i < DPMAIF_SKB_TX_BURST_CNT; i++) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = skb_peek(&txq->tx_skb_head);\n\t\tif (!skb)\n\t\t\tbreak;\n\n\t\tsend_drb_cnt = t7xx_skb_drb_cnt(skb);\n\t\tif (drb_remain_cnt < send_drb_cnt) {\n\t\t\tdrb_remain_cnt = t7xx_txq_drb_wr_available(txq);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdrb_remain_cnt -= send_drb_cnt;\n\n\t\tret = t7xx_dpmaif_add_skb_to_ring(txq->dpmaif_ctrl, skb);\n\t\tif (ret < 0) {\n\t\t\tdev_err(txq->dpmaif_ctrl->dev,\n\t\t\t\t\"Failed to add skb to device's ring: %d\\n\", ret);\n\t\t\tbreak;\n\t\t}\n\n\t\tdrb_cnt += send_drb_cnt;\n\t\tskb_unlink(skb, &txq->tx_skb_head);\n\t}\n\n\tif (drb_cnt > 0)\n\t\treturn drb_cnt;\n\n\treturn ret;\n}\n\nstatic void t7xx_do_tx_hw_push(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tbool wait_disable_sleep = true;\n\n\tdo {\n\t\tstruct dpmaif_tx_queue *txq;\n\t\tint drb_send_cnt;\n\n\t\ttxq = t7xx_select_tx_queue(dpmaif_ctrl);\n\t\tif (!txq)\n\t\t\treturn;\n\n\t\tdrb_send_cnt = t7xx_txq_burst_send_skb(txq);\n\t\tif (drb_send_cnt <= 0) {\n\t\t\tusleep_range(10, 20);\n\t\t\tcond_resched();\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (wait_disable_sleep) {\n\t\t\tif (!t7xx_pci_sleep_disable_complete(dpmaif_ctrl->t7xx_dev))\n\t\t\t\treturn;\n\n\t\t\twait_disable_sleep = false;\n\t\t}\n\n\t\tt7xx_dpmaif_ul_update_hw_drb_cnt(&dpmaif_ctrl->hw_info, txq->index,\n\t\t\t\t\t\t drb_send_cnt * DPMAIF_UL_DRB_SIZE_WORD);\n\n\t\tcond_resched();\n\t} while (!t7xx_tx_lists_are_all_empty(dpmaif_ctrl) && !kthread_should_stop() &&\n\t\t (dpmaif_ctrl->state == DPMAIF_STATE_PWRON));\n}\n\nstatic int t7xx_dpmaif_tx_hw_push_thread(void *arg)\n{\n\tstruct dpmaif_ctrl *dpmaif_ctrl = arg;\n\tint ret;\n\n\twhile (!kthread_should_stop()) {\n\t\tif (t7xx_tx_lists_are_all_empty(dpmaif_ctrl) ||\n\t\t    dpmaif_ctrl->state != DPMAIF_STATE_PWRON) {\n\t\t\tif (wait_event_interruptible(dpmaif_ctrl->tx_wq,\n\t\t\t\t\t\t     (!t7xx_tx_lists_are_all_empty(dpmaif_ctrl) &&\n\t\t\t\t\t\t     dpmaif_ctrl->state == DPMAIF_STATE_PWRON) ||\n\t\t\t\t\t\t     kthread_should_stop()))\n\t\t\t\tcontinue;\n\n\t\t\tif (kthread_should_stop())\n\t\t\t\tbreak;\n\t\t}\n\n\t\tret = pm_runtime_resume_and_get(dpmaif_ctrl->dev);\n\t\tif (ret < 0 && ret != -EACCES)\n\t\t\treturn ret;\n\n\t\tt7xx_pci_disable_sleep(dpmaif_ctrl->t7xx_dev);\n\t\tt7xx_do_tx_hw_push(dpmaif_ctrl);\n\t\tt7xx_pci_enable_sleep(dpmaif_ctrl->t7xx_dev);\n\t\tpm_runtime_mark_last_busy(dpmaif_ctrl->dev);\n\t\tpm_runtime_put_autosuspend(dpmaif_ctrl->dev);\n\t}\n\n\treturn 0;\n}\n\nint t7xx_dpmaif_tx_thread_init(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tinit_waitqueue_head(&dpmaif_ctrl->tx_wq);\n\tdpmaif_ctrl->tx_thread = kthread_run(t7xx_dpmaif_tx_hw_push_thread,\n\t\t\t\t\t     dpmaif_ctrl, \"dpmaif_tx_hw_push\");\n\treturn PTR_ERR_OR_ZERO(dpmaif_ctrl->tx_thread);\n}\n\nvoid t7xx_dpmaif_tx_thread_rel(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tif (dpmaif_ctrl->tx_thread)\n\t\tkthread_stop(dpmaif_ctrl->tx_thread);\n}\n\n \nint t7xx_dpmaif_tx_send_skb(struct dpmaif_ctrl *dpmaif_ctrl, unsigned int txq_number,\n\t\t\t    struct sk_buff *skb)\n{\n\tstruct dpmaif_tx_queue *txq = &dpmaif_ctrl->txq[txq_number];\n\tstruct dpmaif_callbacks *cb = dpmaif_ctrl->callbacks;\n\tstruct t7xx_skb_cb *skb_cb;\n\n\tif (atomic_read(&txq->tx_budget) <= t7xx_skb_drb_cnt(skb)) {\n\t\tcb->state_notify(dpmaif_ctrl->t7xx_dev, DMPAIF_TXQ_STATE_FULL, txq_number);\n\t\treturn -EBUSY;\n\t}\n\n\tskb_cb = T7XX_SKB_CB(skb);\n\tskb_cb->txq_number = txq_number;\n\tskb_queue_tail(&txq->tx_skb_head, skb);\n\twake_up(&dpmaif_ctrl->tx_wq);\n\n\treturn 0;\n}\n\nvoid t7xx_dpmaif_irq_tx_done(struct dpmaif_ctrl *dpmaif_ctrl, unsigned int que_mask)\n{\n\tint i;\n\n\tfor (i = 0; i < DPMAIF_TXQ_NUM; i++) {\n\t\tif (que_mask & BIT(i))\n\t\t\tqueue_work(dpmaif_ctrl->txq[i].worker, &dpmaif_ctrl->txq[i].dpmaif_tx_work);\n\t}\n}\n\nstatic int t7xx_dpmaif_tx_drb_buf_init(struct dpmaif_tx_queue *txq)\n{\n\tsize_t brb_skb_size, brb_pd_size;\n\n\tbrb_pd_size = DPMAIF_DRB_LIST_LEN * sizeof(struct dpmaif_drb);\n\tbrb_skb_size = DPMAIF_DRB_LIST_LEN * sizeof(struct dpmaif_drb_skb);\n\n\ttxq->drb_size_cnt = DPMAIF_DRB_LIST_LEN;\n\n\t \n\ttxq->drb_base = dma_alloc_coherent(txq->dpmaif_ctrl->dev, brb_pd_size,\n\t\t\t\t\t   &txq->drb_bus_addr, GFP_KERNEL | __GFP_ZERO);\n\tif (!txq->drb_base)\n\t\treturn -ENOMEM;\n\n\t \n\ttxq->drb_skb_base = devm_kzalloc(txq->dpmaif_ctrl->dev, brb_skb_size, GFP_KERNEL);\n\tif (!txq->drb_skb_base) {\n\t\tdma_free_coherent(txq->dpmaif_ctrl->dev, brb_pd_size,\n\t\t\t\t  txq->drb_base, txq->drb_bus_addr);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void t7xx_dpmaif_tx_free_drb_skb(struct dpmaif_tx_queue *txq)\n{\n\tstruct dpmaif_drb_skb *drb_skb, *drb_skb_base = txq->drb_skb_base;\n\tunsigned int i;\n\n\tif (!drb_skb_base)\n\t\treturn;\n\n\tfor (i = 0; i < txq->drb_size_cnt; i++) {\n\t\tdrb_skb = drb_skb_base + i;\n\t\tif (!drb_skb->skb)\n\t\t\tcontinue;\n\n\t\tif (!drb_skb->is_msg)\n\t\t\tdma_unmap_single(txq->dpmaif_ctrl->dev, drb_skb->bus_addr,\n\t\t\t\t\t drb_skb->data_len, DMA_TO_DEVICE);\n\n\t\tif (drb_skb->is_last) {\n\t\t\tdev_kfree_skb(drb_skb->skb);\n\t\t\tdrb_skb->skb = NULL;\n\t\t}\n\t}\n}\n\nstatic void t7xx_dpmaif_tx_drb_buf_rel(struct dpmaif_tx_queue *txq)\n{\n\tif (txq->drb_base)\n\t\tdma_free_coherent(txq->dpmaif_ctrl->dev,\n\t\t\t\t  txq->drb_size_cnt * sizeof(struct dpmaif_drb),\n\t\t\t\t  txq->drb_base, txq->drb_bus_addr);\n\n\tt7xx_dpmaif_tx_free_drb_skb(txq);\n}\n\n \nint t7xx_dpmaif_txq_init(struct dpmaif_tx_queue *txq)\n{\n\tint ret;\n\n\tskb_queue_head_init(&txq->tx_skb_head);\n\tinit_waitqueue_head(&txq->req_wq);\n\tatomic_set(&txq->tx_budget, DPMAIF_DRB_LIST_LEN);\n\n\tret = t7xx_dpmaif_tx_drb_buf_init(txq);\n\tif (ret) {\n\t\tdev_err(txq->dpmaif_ctrl->dev, \"Failed to initialize DRB buffers: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\ttxq->worker = alloc_ordered_workqueue(\"md_dpmaif_tx%d_worker\",\n\t\t\t\tWQ_MEM_RECLAIM | (txq->index ? 0 : WQ_HIGHPRI),\n\t\t\t\ttxq->index);\n\tif (!txq->worker)\n\t\treturn -ENOMEM;\n\n\tINIT_WORK(&txq->dpmaif_tx_work, t7xx_dpmaif_tx_done);\n\tspin_lock_init(&txq->tx_lock);\n\n\treturn 0;\n}\n\nvoid t7xx_dpmaif_txq_free(struct dpmaif_tx_queue *txq)\n{\n\tif (txq->worker)\n\t\tdestroy_workqueue(txq->worker);\n\n\tskb_queue_purge(&txq->tx_skb_head);\n\tt7xx_dpmaif_tx_drb_buf_rel(txq);\n}\n\nvoid t7xx_dpmaif_tx_stop(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tint i;\n\n\tfor (i = 0; i < DPMAIF_TXQ_NUM; i++) {\n\t\tstruct dpmaif_tx_queue *txq;\n\t\tint count = 0;\n\n\t\ttxq = &dpmaif_ctrl->txq[i];\n\t\ttxq->que_started = false;\n\t\t \n\t\tsmp_mb();\n\n\t\t \n\t\twhile (atomic_read(&txq->tx_processing)) {\n\t\t\tif (++count >= DPMAIF_MAX_CHECK_COUNT) {\n\t\t\t\tdev_err(dpmaif_ctrl->dev, \"TX queue stop failed\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void t7xx_dpmaif_txq_flush_rel(struct dpmaif_tx_queue *txq)\n{\n\ttxq->que_started = false;\n\n\tcancel_work_sync(&txq->dpmaif_tx_work);\n\tflush_work(&txq->dpmaif_tx_work);\n\tt7xx_dpmaif_tx_free_drb_skb(txq);\n\n\ttxq->drb_rd_idx = 0;\n\ttxq->drb_wr_idx = 0;\n\ttxq->drb_release_rd_idx = 0;\n}\n\nvoid t7xx_dpmaif_tx_clear(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tint i;\n\n\tfor (i = 0; i < DPMAIF_TXQ_NUM; i++)\n\t\tt7xx_dpmaif_txq_flush_rel(&dpmaif_ctrl->txq[i]);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}