{
  "module_name": "t7xx_modem_ops.c",
  "hash_id": "18ebb571481542d2ae4c71918f5f30212a63bf4de6527e625ccbd28a3471d579",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wwan/t7xx/t7xx_modem_ops.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/bits.h>\n#include <linux/bitfield.h>\n#include <linux/device.h>\n#include <linux/delay.h>\n#include <linux/gfp.h>\n#include <linux/io.h>\n#include <linux/irqreturn.h>\n#include <linux/kthread.h>\n#include <linux/skbuff.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n\n#include \"t7xx_cldma.h\"\n#include \"t7xx_hif_cldma.h\"\n#include \"t7xx_mhccif.h\"\n#include \"t7xx_modem_ops.h\"\n#include \"t7xx_netdev.h\"\n#include \"t7xx_pci.h\"\n#include \"t7xx_pcie_mac.h\"\n#include \"t7xx_port.h\"\n#include \"t7xx_port_proxy.h\"\n#include \"t7xx_reg.h\"\n#include \"t7xx_state_monitor.h\"\n\n#define RT_ID_MD_PORT_ENUM\t0\n#define RT_ID_AP_PORT_ENUM\t1\n \n#define MD_FEATURE_QUERY_ID\t0x49434343\n\n#define FEATURE_VER\t\tGENMASK(7, 4)\n#define FEATURE_MSK\t\tGENMASK(3, 0)\n\n#define RGU_RESET_DELAY_MS\t10\n#define PORT_RESET_DELAY_MS\t2000\n#define EX_HS_TIMEOUT_MS\t5000\n#define EX_HS_POLL_DELAY_MS\t10\n\nenum mtk_feature_support_type {\n\tMTK_FEATURE_DOES_NOT_EXIST,\n\tMTK_FEATURE_NOT_SUPPORTED,\n\tMTK_FEATURE_MUST_BE_SUPPORTED,\n};\n\nstatic unsigned int t7xx_get_interrupt_status(struct t7xx_pci_dev *t7xx_dev)\n{\n\treturn t7xx_mhccif_read_sw_int_sts(t7xx_dev) & D2H_SW_INT_MASK;\n}\n\n \nint t7xx_pci_mhccif_isr(struct t7xx_pci_dev *t7xx_dev)\n{\n\tstruct t7xx_modem *md = t7xx_dev->md;\n\tstruct t7xx_fsm_ctl *ctl;\n\tunsigned int int_sta;\n\tint ret = 0;\n\tu32 mask;\n\n\tctl = md->fsm_ctl;\n\tif (!ctl) {\n\t\tdev_err_ratelimited(&t7xx_dev->pdev->dev,\n\t\t\t\t    \"MHCCIF interrupt received before initializing MD monitor\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_bh(&md->exp_lock);\n\tint_sta = t7xx_get_interrupt_status(t7xx_dev);\n\tmd->exp_id |= int_sta;\n\tif (md->exp_id & D2H_INT_EXCEPTION_INIT) {\n\t\tif (ctl->md_state == MD_STATE_INVALID ||\n\t\t    ctl->md_state == MD_STATE_WAITING_FOR_HS1 ||\n\t\t    ctl->md_state == MD_STATE_WAITING_FOR_HS2 ||\n\t\t    ctl->md_state == MD_STATE_READY) {\n\t\t\tmd->exp_id &= ~D2H_INT_EXCEPTION_INIT;\n\t\t\tret = t7xx_fsm_recv_md_intr(ctl, MD_IRQ_CCIF_EX);\n\t\t}\n\t} else if (md->exp_id & D2H_INT_PORT_ENUM) {\n\t\tmd->exp_id &= ~D2H_INT_PORT_ENUM;\n\n\t\tif (ctl->curr_state == FSM_STATE_INIT || ctl->curr_state == FSM_STATE_PRE_START ||\n\t\t    ctl->curr_state == FSM_STATE_STOPPED)\n\t\t\tret = t7xx_fsm_recv_md_intr(ctl, MD_IRQ_PORT_ENUM);\n\t} else if (ctl->md_state == MD_STATE_WAITING_FOR_HS1) {\n\t\tmask = t7xx_mhccif_mask_get(t7xx_dev);\n\t\tif ((md->exp_id & D2H_INT_ASYNC_MD_HK) && !(mask & D2H_INT_ASYNC_MD_HK)) {\n\t\t\tmd->exp_id &= ~D2H_INT_ASYNC_MD_HK;\n\t\t\tqueue_work(md->handshake_wq, &md->handshake_work);\n\t\t}\n\t}\n\tspin_unlock_bh(&md->exp_lock);\n\n\treturn ret;\n}\n\nstatic void t7xx_clr_device_irq_via_pcie(struct t7xx_pci_dev *t7xx_dev)\n{\n\tstruct t7xx_addr_base *pbase_addr = &t7xx_dev->base_addr;\n\tvoid __iomem *reset_pcie_reg;\n\tu32 val;\n\n\treset_pcie_reg = pbase_addr->pcie_ext_reg_base + TOPRGU_CH_PCIE_IRQ_STA -\n\t\t\t  pbase_addr->pcie_dev_reg_trsl_addr;\n\tval = ioread32(reset_pcie_reg);\n\tiowrite32(val, reset_pcie_reg);\n}\n\nvoid t7xx_clear_rgu_irq(struct t7xx_pci_dev *t7xx_dev)\n{\n\t \n\tt7xx_clr_device_irq_via_pcie(t7xx_dev);\n\t \n\tt7xx_pcie_mac_clear_int_status(t7xx_dev, SAP_RGU_INT);\n}\n\nstatic int t7xx_acpi_reset(struct t7xx_pci_dev *t7xx_dev, char *fn_name)\n{\n#ifdef CONFIG_ACPI\n\tstruct acpi_buffer buffer = { ACPI_ALLOCATE_BUFFER, NULL };\n\tstruct device *dev = &t7xx_dev->pdev->dev;\n\tacpi_status acpi_ret;\n\tacpi_handle handle;\n\n\thandle = ACPI_HANDLE(dev);\n\tif (!handle) {\n\t\tdev_err(dev, \"ACPI handle not found\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (!acpi_has_method(handle, fn_name)) {\n\t\tdev_err(dev, \"%s method not found\\n\", fn_name);\n\t\treturn -EFAULT;\n\t}\n\n\tacpi_ret = acpi_evaluate_object(handle, fn_name, NULL, &buffer);\n\tif (ACPI_FAILURE(acpi_ret)) {\n\t\tdev_err(dev, \"%s method fail: %s\\n\", fn_name, acpi_format_exception(acpi_ret));\n\t\treturn -EFAULT;\n\t}\n\n\tkfree(buffer.pointer);\n\n#endif\n\treturn 0;\n}\n\nint t7xx_acpi_fldr_func(struct t7xx_pci_dev *t7xx_dev)\n{\n\treturn t7xx_acpi_reset(t7xx_dev, \"_RST\");\n}\n\nstatic void t7xx_reset_device_via_pmic(struct t7xx_pci_dev *t7xx_dev)\n{\n\tu32 val;\n\n\tval = ioread32(IREG_BASE(t7xx_dev) + T7XX_PCIE_MISC_DEV_STATUS);\n\tif (val & MISC_RESET_TYPE_PLDR)\n\t\tt7xx_acpi_reset(t7xx_dev, \"MRST._RST\");\n\telse if (val & MISC_RESET_TYPE_FLDR)\n\t\tt7xx_acpi_fldr_func(t7xx_dev);\n}\n\nstatic irqreturn_t t7xx_rgu_isr_thread(int irq, void *data)\n{\n\tstruct t7xx_pci_dev *t7xx_dev = data;\n\n\tmsleep(RGU_RESET_DELAY_MS);\n\tt7xx_reset_device_via_pmic(t7xx_dev);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t t7xx_rgu_isr_handler(int irq, void *data)\n{\n\tstruct t7xx_pci_dev *t7xx_dev = data;\n\tstruct t7xx_modem *modem;\n\n\tt7xx_clear_rgu_irq(t7xx_dev);\n\tif (!t7xx_dev->rgu_pci_irq_en)\n\t\treturn IRQ_HANDLED;\n\n\tmodem = t7xx_dev->md;\n\tmodem->rgu_irq_asserted = true;\n\tt7xx_pcie_mac_clear_int(t7xx_dev, SAP_RGU_INT);\n\treturn IRQ_WAKE_THREAD;\n}\n\nstatic void t7xx_pcie_register_rgu_isr(struct t7xx_pci_dev *t7xx_dev)\n{\n\t \n\tt7xx_pcie_mac_clear_int(t7xx_dev, SAP_RGU_INT);\n\tt7xx_pcie_mac_clear_int_status(t7xx_dev, SAP_RGU_INT);\n\n\tt7xx_dev->intr_handler[SAP_RGU_INT] = t7xx_rgu_isr_handler;\n\tt7xx_dev->intr_thread[SAP_RGU_INT] = t7xx_rgu_isr_thread;\n\tt7xx_dev->callback_param[SAP_RGU_INT] = t7xx_dev;\n\tt7xx_pcie_mac_set_int(t7xx_dev, SAP_RGU_INT);\n}\n\n \n\n \nstatic void t7xx_cldma_exception(struct cldma_ctrl *md_ctrl, enum hif_ex_stage stage)\n{\n\tswitch (stage) {\n\tcase HIF_EX_INIT:\n\t\tt7xx_cldma_stop_all_qs(md_ctrl, MTK_TX);\n\t\tt7xx_cldma_clear_all_qs(md_ctrl, MTK_TX);\n\t\tbreak;\n\n\tcase HIF_EX_CLEARQ_DONE:\n\t\t \n\t\tt7xx_cldma_stop_all_qs(md_ctrl, MTK_RX);\n\t\tt7xx_cldma_stop(md_ctrl);\n\n\t\tif (md_ctrl->hif_id == CLDMA_ID_MD)\n\t\t\tt7xx_cldma_hw_reset(md_ctrl->t7xx_dev->base_addr.infracfg_ao_base);\n\n\t\tt7xx_cldma_clear_all_qs(md_ctrl, MTK_RX);\n\t\tbreak;\n\n\tcase HIF_EX_ALLQ_RESET:\n\t\tt7xx_cldma_hw_init(&md_ctrl->hw_info);\n\t\tt7xx_cldma_start(md_ctrl);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void t7xx_md_exception(struct t7xx_modem *md, enum hif_ex_stage stage)\n{\n\tstruct t7xx_pci_dev *t7xx_dev = md->t7xx_dev;\n\n\tif (stage == HIF_EX_CLEARQ_DONE) {\n\t\t \n\t\tmsleep(PORT_RESET_DELAY_MS);\n\t\tt7xx_port_proxy_reset(md->port_prox);\n\t}\n\n\tt7xx_cldma_exception(md->md_ctrl[CLDMA_ID_MD], stage);\n\tt7xx_cldma_exception(md->md_ctrl[CLDMA_ID_AP], stage);\n\n\tif (stage == HIF_EX_INIT)\n\t\tt7xx_mhccif_h2d_swint_trigger(t7xx_dev, H2D_CH_EXCEPTION_ACK);\n\telse if (stage == HIF_EX_CLEARQ_DONE)\n\t\tt7xx_mhccif_h2d_swint_trigger(t7xx_dev, H2D_CH_EXCEPTION_CLEARQ_ACK);\n}\n\nstatic int t7xx_wait_hif_ex_hk_event(struct t7xx_modem *md, int event_id)\n{\n\tunsigned int waited_time_ms = 0;\n\n\tdo {\n\t\tif (md->exp_id & event_id)\n\t\t\treturn 0;\n\n\t\twaited_time_ms += EX_HS_POLL_DELAY_MS;\n\t\tmsleep(EX_HS_POLL_DELAY_MS);\n\t} while (waited_time_ms < EX_HS_TIMEOUT_MS);\n\n\treturn -EFAULT;\n}\n\nstatic void t7xx_md_sys_sw_init(struct t7xx_pci_dev *t7xx_dev)\n{\n\t \n\tt7xx_mhccif_mask_set(t7xx_dev, D2H_SW_INT_MASK);\n\tt7xx_mhccif_mask_clr(t7xx_dev, D2H_INT_PORT_ENUM);\n\n\t \n\tt7xx_dev->rgu_pci_irq_en = true;\n\tt7xx_pcie_register_rgu_isr(t7xx_dev);\n}\n\nstruct feature_query {\n\t__le32 head_pattern;\n\tu8 feature_set[FEATURE_COUNT];\n\t__le32 tail_pattern;\n};\n\nstatic void t7xx_prepare_host_rt_data_query(struct t7xx_sys_info *core)\n{\n\tstruct feature_query *ft_query;\n\tstruct sk_buff *skb;\n\n\tskb = t7xx_ctrl_alloc_skb(sizeof(*ft_query));\n\tif (!skb)\n\t\treturn;\n\n\tft_query = skb_put(skb, sizeof(*ft_query));\n\tft_query->head_pattern = cpu_to_le32(MD_FEATURE_QUERY_ID);\n\tmemcpy(ft_query->feature_set, core->feature_set, FEATURE_COUNT);\n\tft_query->tail_pattern = cpu_to_le32(MD_FEATURE_QUERY_ID);\n\n\t \n\tt7xx_port_send_ctl_skb(core->ctl_port, skb, CTL_ID_HS1_MSG, 0);\n}\n\nstatic int t7xx_prepare_device_rt_data(struct t7xx_sys_info *core, struct device *dev,\n\t\t\t\t       void *data)\n{\n\tstruct feature_query *md_feature = data;\n\tstruct mtk_runtime_feature *rt_feature;\n\tunsigned int i, rt_data_len = 0;\n\tstruct sk_buff *skb;\n\n\t \n\tif (le32_to_cpu(md_feature->head_pattern) != MD_FEATURE_QUERY_ID ||\n\t    le32_to_cpu(md_feature->tail_pattern) != MD_FEATURE_QUERY_ID) {\n\t\tdev_err(dev, \"Invalid feature pattern: head 0x%x, tail 0x%x\\n\",\n\t\t\tle32_to_cpu(md_feature->head_pattern),\n\t\t\tle32_to_cpu(md_feature->tail_pattern));\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < FEATURE_COUNT; i++) {\n\t\tif (FIELD_GET(FEATURE_MSK, md_feature->feature_set[i]) !=\n\t\t    MTK_FEATURE_MUST_BE_SUPPORTED)\n\t\t\trt_data_len += sizeof(*rt_feature);\n\t}\n\n\tskb = t7xx_ctrl_alloc_skb(rt_data_len);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\trt_feature = skb_put(skb, rt_data_len);\n\tmemset(rt_feature, 0, rt_data_len);\n\n\t \n\tfor (i = 0; i < FEATURE_COUNT; i++) {\n\t\tu8 md_feature_mask = FIELD_GET(FEATURE_MSK, md_feature->feature_set[i]);\n\n\t\tif (md_feature_mask == MTK_FEATURE_MUST_BE_SUPPORTED)\n\t\t\tcontinue;\n\n\t\trt_feature->feature_id = i;\n\t\tif (md_feature_mask == MTK_FEATURE_DOES_NOT_EXIST)\n\t\t\trt_feature->support_info = md_feature->feature_set[i];\n\n\t\trt_feature++;\n\t}\n\n\t \n\tt7xx_port_send_ctl_skb(core->ctl_port, skb, CTL_ID_HS3_MSG, 0);\n\treturn 0;\n}\n\nstatic int t7xx_parse_host_rt_data(struct t7xx_fsm_ctl *ctl, struct t7xx_sys_info *core,\n\t\t\t\t   struct device *dev, void *data, int data_length)\n{\n\tenum mtk_feature_support_type ft_spt_st, ft_spt_cfg;\n\tstruct mtk_runtime_feature *rt_feature;\n\tint i, offset;\n\n\toffset = sizeof(struct feature_query);\n\tfor (i = 0; i < FEATURE_COUNT && offset < data_length; i++) {\n\t\trt_feature = data + offset;\n\t\toffset += sizeof(*rt_feature) + le32_to_cpu(rt_feature->data_len);\n\n\t\tft_spt_cfg = FIELD_GET(FEATURE_MSK, core->feature_set[i]);\n\t\tif (ft_spt_cfg != MTK_FEATURE_MUST_BE_SUPPORTED)\n\t\t\tcontinue;\n\n\t\tft_spt_st = FIELD_GET(FEATURE_MSK, rt_feature->support_info);\n\t\tif (ft_spt_st != MTK_FEATURE_MUST_BE_SUPPORTED)\n\t\t\treturn -EINVAL;\n\n\t\tif (i == RT_ID_MD_PORT_ENUM || i == RT_ID_AP_PORT_ENUM)\n\t\t\tt7xx_port_enum_msg_handler(ctl->md, rt_feature->data);\n\t}\n\n\treturn 0;\n}\n\nstatic int t7xx_core_reset(struct t7xx_modem *md)\n{\n\tstruct device *dev = &md->t7xx_dev->pdev->dev;\n\tstruct t7xx_fsm_ctl *ctl = md->fsm_ctl;\n\n\tmd->core_md.ready = false;\n\n\tif (!ctl) {\n\t\tdev_err(dev, \"FSM is not initialized\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (md->core_md.handshake_ongoing) {\n\t\tint ret = t7xx_fsm_append_event(ctl, FSM_EVENT_MD_HS2_EXIT, NULL, 0);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tmd->core_md.handshake_ongoing = false;\n\treturn 0;\n}\n\nstatic void t7xx_core_hk_handler(struct t7xx_modem *md, struct t7xx_sys_info *core_info,\n\t\t\t\t struct t7xx_fsm_ctl *ctl,\n\t\t\t\t enum t7xx_fsm_event_state event_id,\n\t\t\t\t enum t7xx_fsm_event_state err_detect)\n{\n\tstruct t7xx_fsm_event *event = NULL, *event_next;\n\tstruct device *dev = &md->t7xx_dev->pdev->dev;\n\tunsigned long flags;\n\tint ret;\n\n\tt7xx_prepare_host_rt_data_query(core_info);\n\n\twhile (!kthread_should_stop()) {\n\t\tbool event_received = false;\n\n\t\tspin_lock_irqsave(&ctl->event_lock, flags);\n\t\tlist_for_each_entry_safe(event, event_next, &ctl->event_queue, entry) {\n\t\t\tif (event->event_id == err_detect) {\n\t\t\t\tlist_del(&event->entry);\n\t\t\t\tspin_unlock_irqrestore(&ctl->event_lock, flags);\n\t\t\t\tdev_err(dev, \"Core handshake error event received\\n\");\n\t\t\t\tgoto err_free_event;\n\t\t\t} else if (event->event_id == event_id) {\n\t\t\t\tlist_del(&event->entry);\n\t\t\t\tevent_received = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&ctl->event_lock, flags);\n\n\t\tif (event_received)\n\t\t\tbreak;\n\n\t\twait_event_interruptible(ctl->event_wq, !list_empty(&ctl->event_queue) ||\n\t\t\t\t\t kthread_should_stop());\n\t\tif (kthread_should_stop())\n\t\t\tgoto err_free_event;\n\t}\n\n\tif (!event || ctl->exp_flg)\n\t\tgoto err_free_event;\n\n\tret = t7xx_parse_host_rt_data(ctl, core_info, dev, event->data, event->length);\n\tif (ret) {\n\t\tdev_err(dev, \"Host failure parsing runtime data: %d\\n\", ret);\n\t\tgoto err_free_event;\n\t}\n\n\tif (ctl->exp_flg)\n\t\tgoto err_free_event;\n\n\tret = t7xx_prepare_device_rt_data(core_info, dev, event->data);\n\tif (ret) {\n\t\tdev_err(dev, \"Device failure parsing runtime data: %d\", ret);\n\t\tgoto err_free_event;\n\t}\n\n\tcore_info->ready = true;\n\tcore_info->handshake_ongoing = false;\n\twake_up(&ctl->async_hk_wq);\nerr_free_event:\n\tkfree(event);\n}\n\nstatic void t7xx_md_hk_wq(struct work_struct *work)\n{\n\tstruct t7xx_modem *md = container_of(work, struct t7xx_modem, handshake_work);\n\tstruct t7xx_fsm_ctl *ctl = md->fsm_ctl;\n\n\t \n\tt7xx_fsm_clr_event(ctl, FSM_EVENT_MD_HS2_EXIT);\n\tt7xx_cldma_switch_cfg(md->md_ctrl[CLDMA_ID_MD]);\n\tt7xx_cldma_start(md->md_ctrl[CLDMA_ID_MD]);\n\tt7xx_fsm_broadcast_state(ctl, MD_STATE_WAITING_FOR_HS2);\n\tmd->core_md.handshake_ongoing = true;\n\tt7xx_core_hk_handler(md, &md->core_md, ctl, FSM_EVENT_MD_HS2, FSM_EVENT_MD_HS2_EXIT);\n}\n\nstatic void t7xx_ap_hk_wq(struct work_struct *work)\n{\n\tstruct t7xx_modem *md = container_of(work, struct t7xx_modem, ap_handshake_work);\n\tstruct t7xx_fsm_ctl *ctl = md->fsm_ctl;\n\n\t  \n\tt7xx_fsm_clr_event(ctl, FSM_EVENT_AP_HS2_EXIT);\n\tt7xx_cldma_stop(md->md_ctrl[CLDMA_ID_AP]);\n\tt7xx_cldma_switch_cfg(md->md_ctrl[CLDMA_ID_AP]);\n\tt7xx_cldma_start(md->md_ctrl[CLDMA_ID_AP]);\n\tmd->core_ap.handshake_ongoing = true;\n\tt7xx_core_hk_handler(md, &md->core_ap, ctl, FSM_EVENT_AP_HS2, FSM_EVENT_AP_HS2_EXIT);\n}\n\nvoid t7xx_md_event_notify(struct t7xx_modem *md, enum md_event_id evt_id)\n{\n\tstruct t7xx_fsm_ctl *ctl = md->fsm_ctl;\n\tunsigned int int_sta;\n\tunsigned long flags;\n\n\tswitch (evt_id) {\n\tcase FSM_PRE_START:\n\t\tt7xx_mhccif_mask_clr(md->t7xx_dev, D2H_INT_PORT_ENUM | D2H_INT_ASYNC_MD_HK |\n\t\t\t\t\t\t   D2H_INT_ASYNC_AP_HK);\n\t\tbreak;\n\n\tcase FSM_START:\n\t\tt7xx_mhccif_mask_set(md->t7xx_dev, D2H_INT_PORT_ENUM);\n\n\t\tspin_lock_irqsave(&md->exp_lock, flags);\n\t\tint_sta = t7xx_get_interrupt_status(md->t7xx_dev);\n\t\tmd->exp_id |= int_sta;\n\t\tif (md->exp_id & D2H_INT_EXCEPTION_INIT) {\n\t\t\tctl->exp_flg = true;\n\t\t\tmd->exp_id &= ~D2H_INT_EXCEPTION_INIT;\n\t\t\tmd->exp_id &= ~D2H_INT_ASYNC_MD_HK;\n\t\t\tmd->exp_id &= ~D2H_INT_ASYNC_AP_HK;\n\t\t} else if (ctl->exp_flg) {\n\t\t\tmd->exp_id &= ~D2H_INT_ASYNC_MD_HK;\n\t\t\tmd->exp_id &= ~D2H_INT_ASYNC_AP_HK;\n\t\t} else {\n\t\t\tvoid __iomem *mhccif_base = md->t7xx_dev->base_addr.mhccif_rc_base;\n\n\t\t\tif (md->exp_id & D2H_INT_ASYNC_MD_HK) {\n\t\t\t\tqueue_work(md->handshake_wq, &md->handshake_work);\n\t\t\t\tmd->exp_id &= ~D2H_INT_ASYNC_MD_HK;\n\t\t\t\tiowrite32(D2H_INT_ASYNC_MD_HK, mhccif_base + REG_EP2RC_SW_INT_ACK);\n\t\t\t\tt7xx_mhccif_mask_set(md->t7xx_dev, D2H_INT_ASYNC_MD_HK);\n\t\t\t}\n\n\t\t\tif (md->exp_id & D2H_INT_ASYNC_AP_HK) {\n\t\t\t\tqueue_work(md->handshake_wq, &md->ap_handshake_work);\n\t\t\t\tmd->exp_id &= ~D2H_INT_ASYNC_AP_HK;\n\t\t\t\tiowrite32(D2H_INT_ASYNC_AP_HK, mhccif_base + REG_EP2RC_SW_INT_ACK);\n\t\t\t\tt7xx_mhccif_mask_set(md->t7xx_dev, D2H_INT_ASYNC_AP_HK);\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&md->exp_lock, flags);\n\n\t\tt7xx_mhccif_mask_clr(md->t7xx_dev,\n\t\t\t\t     D2H_INT_EXCEPTION_INIT |\n\t\t\t\t     D2H_INT_EXCEPTION_INIT_DONE |\n\t\t\t\t     D2H_INT_EXCEPTION_CLEARQ_DONE |\n\t\t\t\t     D2H_INT_EXCEPTION_ALLQ_RESET);\n\t\tbreak;\n\n\tcase FSM_READY:\n\t\tt7xx_mhccif_mask_set(md->t7xx_dev, D2H_INT_ASYNC_MD_HK);\n\t\tt7xx_mhccif_mask_set(md->t7xx_dev, D2H_INT_ASYNC_AP_HK);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nvoid t7xx_md_exception_handshake(struct t7xx_modem *md)\n{\n\tstruct device *dev = &md->t7xx_dev->pdev->dev;\n\tint ret;\n\n\tt7xx_md_exception(md, HIF_EX_INIT);\n\tret = t7xx_wait_hif_ex_hk_event(md, D2H_INT_EXCEPTION_INIT_DONE);\n\tif (ret)\n\t\tdev_err(dev, \"EX CCIF HS timeout, RCH 0x%lx\\n\", D2H_INT_EXCEPTION_INIT_DONE);\n\n\tt7xx_md_exception(md, HIF_EX_INIT_DONE);\n\tret = t7xx_wait_hif_ex_hk_event(md, D2H_INT_EXCEPTION_CLEARQ_DONE);\n\tif (ret)\n\t\tdev_err(dev, \"EX CCIF HS timeout, RCH 0x%lx\\n\", D2H_INT_EXCEPTION_CLEARQ_DONE);\n\n\tt7xx_md_exception(md, HIF_EX_CLEARQ_DONE);\n\tret = t7xx_wait_hif_ex_hk_event(md, D2H_INT_EXCEPTION_ALLQ_RESET);\n\tif (ret)\n\t\tdev_err(dev, \"EX CCIF HS timeout, RCH 0x%lx\\n\", D2H_INT_EXCEPTION_ALLQ_RESET);\n\n\tt7xx_md_exception(md, HIF_EX_ALLQ_RESET);\n}\n\nstatic struct t7xx_modem *t7xx_md_alloc(struct t7xx_pci_dev *t7xx_dev)\n{\n\tstruct device *dev = &t7xx_dev->pdev->dev;\n\tstruct t7xx_modem *md;\n\n\tmd = devm_kzalloc(dev, sizeof(*md), GFP_KERNEL);\n\tif (!md)\n\t\treturn NULL;\n\n\tmd->t7xx_dev = t7xx_dev;\n\tt7xx_dev->md = md;\n\tspin_lock_init(&md->exp_lock);\n\tmd->handshake_wq = alloc_workqueue(\"%s\", WQ_UNBOUND | WQ_MEM_RECLAIM | WQ_HIGHPRI,\n\t\t\t\t\t   0, \"md_hk_wq\");\n\tif (!md->handshake_wq)\n\t\treturn NULL;\n\n\tINIT_WORK(&md->handshake_work, t7xx_md_hk_wq);\n\tmd->core_md.feature_set[RT_ID_MD_PORT_ENUM] &= ~FEATURE_MSK;\n\tmd->core_md.feature_set[RT_ID_MD_PORT_ENUM] |=\n\t\tFIELD_PREP(FEATURE_MSK, MTK_FEATURE_MUST_BE_SUPPORTED);\n\n\tINIT_WORK(&md->ap_handshake_work, t7xx_ap_hk_wq);\n\tmd->core_ap.feature_set[RT_ID_AP_PORT_ENUM] &= ~FEATURE_MSK;\n\tmd->core_ap.feature_set[RT_ID_AP_PORT_ENUM] |=\n\t\tFIELD_PREP(FEATURE_MSK, MTK_FEATURE_MUST_BE_SUPPORTED);\n\n\treturn md;\n}\n\nint t7xx_md_reset(struct t7xx_pci_dev *t7xx_dev)\n{\n\tstruct t7xx_modem *md = t7xx_dev->md;\n\n\tmd->md_init_finish = false;\n\tmd->exp_id = 0;\n\tt7xx_fsm_reset(md);\n\tt7xx_cldma_reset(md->md_ctrl[CLDMA_ID_MD]);\n\tt7xx_cldma_reset(md->md_ctrl[CLDMA_ID_AP]);\n\tt7xx_port_proxy_reset(md->port_prox);\n\tmd->md_init_finish = true;\n\treturn t7xx_core_reset(md);\n}\n\n \nint t7xx_md_init(struct t7xx_pci_dev *t7xx_dev)\n{\n\tstruct t7xx_modem *md;\n\tint ret;\n\n\tmd = t7xx_md_alloc(t7xx_dev);\n\tif (!md)\n\t\treturn -ENOMEM;\n\n\tret = t7xx_cldma_alloc(CLDMA_ID_MD, t7xx_dev);\n\tif (ret)\n\t\tgoto err_destroy_hswq;\n\n\tret = t7xx_cldma_alloc(CLDMA_ID_AP, t7xx_dev);\n\tif (ret)\n\t\tgoto err_destroy_hswq;\n\n\tret = t7xx_fsm_init(md);\n\tif (ret)\n\t\tgoto err_destroy_hswq;\n\n\tret = t7xx_ccmni_init(t7xx_dev);\n\tif (ret)\n\t\tgoto err_uninit_fsm;\n\n\tret = t7xx_cldma_init(md->md_ctrl[CLDMA_ID_MD]);\n\tif (ret)\n\t\tgoto err_uninit_ccmni;\n\n\tret = t7xx_cldma_init(md->md_ctrl[CLDMA_ID_AP]);\n\tif (ret)\n\t\tgoto err_uninit_md_cldma;\n\n\tret = t7xx_port_proxy_init(md);\n\tif (ret)\n\t\tgoto err_uninit_ap_cldma;\n\n\tret = t7xx_fsm_append_cmd(md->fsm_ctl, FSM_CMD_START, 0);\n\tif (ret)  \n\t\tgoto err_uninit_proxy;\n\n\tt7xx_md_sys_sw_init(t7xx_dev);\n\tmd->md_init_finish = true;\n\treturn 0;\n\nerr_uninit_proxy:\n\tt7xx_port_proxy_uninit(md->port_prox);\n\nerr_uninit_ap_cldma:\n\tt7xx_cldma_exit(md->md_ctrl[CLDMA_ID_AP]);\n\nerr_uninit_md_cldma:\n\tt7xx_cldma_exit(md->md_ctrl[CLDMA_ID_MD]);\n\nerr_uninit_ccmni:\n\tt7xx_ccmni_exit(t7xx_dev);\n\nerr_uninit_fsm:\n\tt7xx_fsm_uninit(md);\n\nerr_destroy_hswq:\n\tdestroy_workqueue(md->handshake_wq);\n\tdev_err(&t7xx_dev->pdev->dev, \"Modem init failed\\n\");\n\treturn ret;\n}\n\nvoid t7xx_md_exit(struct t7xx_pci_dev *t7xx_dev)\n{\n\tstruct t7xx_modem *md = t7xx_dev->md;\n\n\tt7xx_pcie_mac_clear_int(t7xx_dev, SAP_RGU_INT);\n\n\tif (!md->md_init_finish)\n\t\treturn;\n\n\tt7xx_fsm_append_cmd(md->fsm_ctl, FSM_CMD_PRE_STOP, FSM_CMD_FLAG_WAIT_FOR_COMPLETION);\n\tt7xx_port_proxy_uninit(md->port_prox);\n\tt7xx_cldma_exit(md->md_ctrl[CLDMA_ID_AP]);\n\tt7xx_cldma_exit(md->md_ctrl[CLDMA_ID_MD]);\n\tt7xx_ccmni_exit(t7xx_dev);\n\tt7xx_fsm_uninit(md);\n\tdestroy_workqueue(md->handshake_wq);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}