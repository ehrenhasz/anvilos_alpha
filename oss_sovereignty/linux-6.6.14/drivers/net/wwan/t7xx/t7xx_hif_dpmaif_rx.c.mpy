{
  "module_name": "t7xx_hif_dpmaif_rx.c",
  "hash_id": "cfaddbee0516c9a2851e42f6bbc66b234a972de5d4188635e4954ed11b206eb3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wwan/t7xx/t7xx_hif_dpmaif_rx.c",
  "human_readable_source": "\n \n\n#include <linux/atomic.h>\n#include <linux/bitfield.h>\n#include <linux/bitops.h>\n#include <linux/device.h>\n#include <linux/dma-direction.h>\n#include <linux/dma-mapping.h>\n#include <linux/gfp.h>\n#include <linux/err.h>\n#include <linux/iopoll.h>\n#include <linux/jiffies.h>\n#include <linux/kernel.h>\n#include <linux/kthread.h>\n#include <linux/list.h>\n#include <linux/minmax.h>\n#include <linux/mm.h>\n#include <linux/netdevice.h>\n#include <linux/pm_runtime.h>\n#include <linux/sched.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n\n#include \"t7xx_dpmaif.h\"\n#include \"t7xx_hif_dpmaif.h\"\n#include \"t7xx_hif_dpmaif_rx.h\"\n#include \"t7xx_netdev.h\"\n#include \"t7xx_pci.h\"\n\n#define DPMAIF_BAT_COUNT\t\t8192\n#define DPMAIF_FRG_COUNT\t\t4814\n#define DPMAIF_PIT_COUNT\t\t(DPMAIF_BAT_COUNT * 2)\n\n#define DPMAIF_BAT_CNT_THRESHOLD\t30\n#define DPMAIF_PIT_CNT_THRESHOLD\t60\n#define DPMAIF_RX_PUSH_THRESHOLD_MASK\tGENMASK(2, 0)\n#define DPMAIF_NOTIFY_RELEASE_COUNT\t128\n#define DPMAIF_POLL_PIT_TIME_US\t\t20\n#define DPMAIF_POLL_PIT_MAX_TIME_US\t2000\n#define DPMAIF_WQ_TIME_LIMIT_MS\t\t2\n#define DPMAIF_CS_RESULT_PASS\t\t0\n\n \n#define DES_PT_PD\t\t\t0\n#define DES_PT_MSG\t\t\t1\n \n#define PKT_BUF_FRAG\t\t\t1\n\nstatic unsigned int t7xx_normal_pit_bid(const struct dpmaif_pit *pit_info)\n{\n\tu32 value;\n\n\tvalue = FIELD_GET(PD_PIT_H_BID, le32_to_cpu(pit_info->pd.footer));\n\tvalue <<= 13;\n\tvalue += FIELD_GET(PD_PIT_BUFFER_ID, le32_to_cpu(pit_info->header));\n\treturn value;\n}\n\nstatic int t7xx_dpmaif_update_bat_wr_idx(struct dpmaif_ctrl *dpmaif_ctrl,\n\t\t\t\t\t const unsigned int q_num, const unsigned int bat_cnt)\n{\n\tstruct dpmaif_rx_queue *rxq = &dpmaif_ctrl->rxq[q_num];\n\tstruct dpmaif_bat_request *bat_req = rxq->bat_req;\n\tunsigned int old_rl_idx, new_wr_idx, old_wr_idx;\n\n\tif (!rxq->que_started) {\n\t\tdev_err(dpmaif_ctrl->dev, \"RX queue %d has not been started\\n\", rxq->index);\n\t\treturn -EINVAL;\n\t}\n\n\told_rl_idx = bat_req->bat_release_rd_idx;\n\told_wr_idx = bat_req->bat_wr_idx;\n\tnew_wr_idx = old_wr_idx + bat_cnt;\n\n\tif (old_rl_idx > old_wr_idx && new_wr_idx >= old_rl_idx)\n\t\tgoto err_flow;\n\n\tif (new_wr_idx >= bat_req->bat_size_cnt) {\n\t\tnew_wr_idx -= bat_req->bat_size_cnt;\n\t\tif (new_wr_idx >= old_rl_idx)\n\t\t\tgoto err_flow;\n\t}\n\n\tbat_req->bat_wr_idx = new_wr_idx;\n\treturn 0;\n\nerr_flow:\n\tdev_err(dpmaif_ctrl->dev, \"RX BAT flow check fail\\n\");\n\treturn -EINVAL;\n}\n\nstatic bool t7xx_alloc_and_map_skb_info(const struct dpmaif_ctrl *dpmaif_ctrl,\n\t\t\t\t\tconst unsigned int size, struct dpmaif_bat_skb *cur_skb)\n{\n\tdma_addr_t data_bus_addr;\n\tstruct sk_buff *skb;\n\n\tskb = __dev_alloc_skb(size, GFP_KERNEL);\n\tif (!skb)\n\t\treturn false;\n\n\tdata_bus_addr = dma_map_single(dpmaif_ctrl->dev, skb->data, size, DMA_FROM_DEVICE);\n\tif (dma_mapping_error(dpmaif_ctrl->dev, data_bus_addr)) {\n\t\tdev_err_ratelimited(dpmaif_ctrl->dev, \"DMA mapping error\\n\");\n\t\tdev_kfree_skb_any(skb);\n\t\treturn false;\n\t}\n\n\tcur_skb->skb = skb;\n\tcur_skb->data_bus_addr = data_bus_addr;\n\tcur_skb->data_len = size;\n\n\treturn true;\n}\n\nstatic void t7xx_unmap_bat_skb(struct device *dev, struct dpmaif_bat_skb *bat_skb_base,\n\t\t\t       unsigned int index)\n{\n\tstruct dpmaif_bat_skb *bat_skb = bat_skb_base + index;\n\n\tif (bat_skb->skb) {\n\t\tdma_unmap_single(dev, bat_skb->data_bus_addr, bat_skb->data_len, DMA_FROM_DEVICE);\n\t\tdev_kfree_skb(bat_skb->skb);\n\t\tbat_skb->skb = NULL;\n\t}\n}\n\n \nint t7xx_dpmaif_rx_buf_alloc(struct dpmaif_ctrl *dpmaif_ctrl,\n\t\t\t     const struct dpmaif_bat_request *bat_req,\n\t\t\t     const unsigned int q_num, const unsigned int buf_cnt,\n\t\t\t     const bool initial)\n{\n\tunsigned int i, bat_cnt, bat_max_cnt, bat_start_idx;\n\tint ret;\n\n\tif (!buf_cnt || buf_cnt > bat_req->bat_size_cnt)\n\t\treturn -EINVAL;\n\n\t \n\tbat_max_cnt = bat_req->bat_size_cnt;\n\n\tbat_cnt = t7xx_ring_buf_rd_wr_count(bat_max_cnt, bat_req->bat_release_rd_idx,\n\t\t\t\t\t    bat_req->bat_wr_idx, DPMAIF_WRITE);\n\tif (buf_cnt > bat_cnt)\n\t\treturn -ENOMEM;\n\n\tbat_start_idx = bat_req->bat_wr_idx;\n\n\tfor (i = 0; i < buf_cnt; i++) {\n\t\tunsigned int cur_bat_idx = bat_start_idx + i;\n\t\tstruct dpmaif_bat_skb *cur_skb;\n\t\tstruct dpmaif_bat *cur_bat;\n\n\t\tif (cur_bat_idx >= bat_max_cnt)\n\t\t\tcur_bat_idx -= bat_max_cnt;\n\n\t\tcur_skb = (struct dpmaif_bat_skb *)bat_req->bat_skb + cur_bat_idx;\n\t\tif (!cur_skb->skb &&\n\t\t    !t7xx_alloc_and_map_skb_info(dpmaif_ctrl, bat_req->pkt_buf_sz, cur_skb))\n\t\t\tbreak;\n\n\t\tcur_bat = (struct dpmaif_bat *)bat_req->bat_base + cur_bat_idx;\n\t\tcur_bat->buffer_addr_ext = upper_32_bits(cur_skb->data_bus_addr);\n\t\tcur_bat->p_buffer_addr = lower_32_bits(cur_skb->data_bus_addr);\n\t}\n\n\tif (!i)\n\t\treturn -ENOMEM;\n\n\tret = t7xx_dpmaif_update_bat_wr_idx(dpmaif_ctrl, q_num, i);\n\tif (ret)\n\t\tgoto err_unmap_skbs;\n\n\tif (!initial) {\n\t\tunsigned int hw_wr_idx;\n\n\t\tret = t7xx_dpmaif_dl_snd_hw_bat_cnt(&dpmaif_ctrl->hw_info, i);\n\t\tif (ret)\n\t\t\tgoto err_unmap_skbs;\n\n\t\thw_wr_idx = t7xx_dpmaif_dl_get_bat_wr_idx(&dpmaif_ctrl->hw_info,\n\t\t\t\t\t\t\t  DPF_RX_QNO_DFT);\n\t\tif (hw_wr_idx != bat_req->bat_wr_idx) {\n\t\t\tret = -EFAULT;\n\t\t\tdev_err(dpmaif_ctrl->dev, \"Write index mismatch in RX ring\\n\");\n\t\t\tgoto err_unmap_skbs;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_unmap_skbs:\n\twhile (--i > 0)\n\t\tt7xx_unmap_bat_skb(dpmaif_ctrl->dev, bat_req->bat_skb, i);\n\n\treturn ret;\n}\n\nstatic int t7xx_dpmaifq_release_pit_entry(struct dpmaif_rx_queue *rxq,\n\t\t\t\t\t  const unsigned int rel_entry_num)\n{\n\tstruct dpmaif_hw_info *hw_info = &rxq->dpmaif_ctrl->hw_info;\n\tunsigned int old_rel_idx, new_rel_idx, hw_wr_idx;\n\tint ret;\n\n\tif (!rxq->que_started)\n\t\treturn 0;\n\n\tif (rel_entry_num >= rxq->pit_size_cnt) {\n\t\tdev_err(rxq->dpmaif_ctrl->dev, \"Invalid PIT release index\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\told_rel_idx = rxq->pit_release_rd_idx;\n\tnew_rel_idx = old_rel_idx + rel_entry_num;\n\thw_wr_idx = rxq->pit_wr_idx;\n\tif (hw_wr_idx < old_rel_idx && new_rel_idx >= rxq->pit_size_cnt)\n\t\tnew_rel_idx -= rxq->pit_size_cnt;\n\n\tret = t7xx_dpmaif_dlq_add_pit_remain_cnt(hw_info, rxq->index, rel_entry_num);\n\tif (ret) {\n\t\tdev_err(rxq->dpmaif_ctrl->dev, \"PIT release failure: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\trxq->pit_release_rd_idx = new_rel_idx;\n\treturn 0;\n}\n\nstatic void t7xx_dpmaif_set_bat_mask(struct dpmaif_bat_request *bat_req, unsigned int idx)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bat_req->mask_lock, flags);\n\tset_bit(idx, bat_req->bat_bitmap);\n\tspin_unlock_irqrestore(&bat_req->mask_lock, flags);\n}\n\nstatic int t7xx_frag_bat_cur_bid_check(struct dpmaif_rx_queue *rxq,\n\t\t\t\t       const unsigned int cur_bid)\n{\n\tstruct dpmaif_bat_request *bat_frag = rxq->bat_frag;\n\tstruct dpmaif_bat_page *bat_page;\n\n\tif (cur_bid >= DPMAIF_FRG_COUNT)\n\t\treturn -EINVAL;\n\n\tbat_page = bat_frag->bat_skb + cur_bid;\n\tif (!bat_page->page)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void t7xx_unmap_bat_page(struct device *dev, struct dpmaif_bat_page *bat_page_base,\n\t\t\t\tunsigned int index)\n{\n\tstruct dpmaif_bat_page *bat_page = bat_page_base + index;\n\n\tif (bat_page->page) {\n\t\tdma_unmap_page(dev, bat_page->data_bus_addr, bat_page->data_len, DMA_FROM_DEVICE);\n\t\tput_page(bat_page->page);\n\t\tbat_page->page = NULL;\n\t}\n}\n\n \nint t7xx_dpmaif_rx_frag_alloc(struct dpmaif_ctrl *dpmaif_ctrl, struct dpmaif_bat_request *bat_req,\n\t\t\t      const unsigned int buf_cnt, const bool initial)\n{\n\tunsigned int buf_space, cur_bat_idx = bat_req->bat_wr_idx;\n\tstruct dpmaif_bat_page *bat_skb = bat_req->bat_skb;\n\tint ret = 0, i;\n\n\tif (!buf_cnt || buf_cnt > bat_req->bat_size_cnt)\n\t\treturn -EINVAL;\n\n\tbuf_space = t7xx_ring_buf_rd_wr_count(bat_req->bat_size_cnt,\n\t\t\t\t\t      bat_req->bat_release_rd_idx, bat_req->bat_wr_idx,\n\t\t\t\t\t      DPMAIF_WRITE);\n\tif (buf_cnt > buf_space) {\n\t\tdev_err(dpmaif_ctrl->dev,\n\t\t\t\"Requested more buffers than the space available in RX frag ring\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < buf_cnt; i++) {\n\t\tstruct dpmaif_bat_page *cur_page = bat_skb + cur_bat_idx;\n\t\tstruct dpmaif_bat *cur_bat;\n\t\tdma_addr_t data_base_addr;\n\n\t\tif (!cur_page->page) {\n\t\t\tunsigned long offset;\n\t\t\tstruct page *page;\n\t\t\tvoid *data;\n\n\t\t\tdata = netdev_alloc_frag(bat_req->pkt_buf_sz);\n\t\t\tif (!data)\n\t\t\t\tbreak;\n\n\t\t\tpage = virt_to_head_page(data);\n\t\t\toffset = data - page_address(page);\n\n\t\t\tdata_base_addr = dma_map_page(dpmaif_ctrl->dev, page, offset,\n\t\t\t\t\t\t      bat_req->pkt_buf_sz, DMA_FROM_DEVICE);\n\t\t\tif (dma_mapping_error(dpmaif_ctrl->dev, data_base_addr)) {\n\t\t\t\tput_page(virt_to_head_page(data));\n\t\t\t\tdev_err(dpmaif_ctrl->dev, \"DMA mapping fail\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcur_page->page = page;\n\t\t\tcur_page->data_bus_addr = data_base_addr;\n\t\t\tcur_page->offset = offset;\n\t\t\tcur_page->data_len = bat_req->pkt_buf_sz;\n\t\t}\n\n\t\tdata_base_addr = cur_page->data_bus_addr;\n\t\tcur_bat = (struct dpmaif_bat *)bat_req->bat_base + cur_bat_idx;\n\t\tcur_bat->buffer_addr_ext = upper_32_bits(data_base_addr);\n\t\tcur_bat->p_buffer_addr = lower_32_bits(data_base_addr);\n\t\tcur_bat_idx = t7xx_ring_buf_get_next_wr_idx(bat_req->bat_size_cnt, cur_bat_idx);\n\t}\n\n\tbat_req->bat_wr_idx = cur_bat_idx;\n\n\tif (!initial)\n\t\tt7xx_dpmaif_dl_snd_hw_frg_cnt(&dpmaif_ctrl->hw_info, i);\n\n\tif (i < buf_cnt) {\n\t\tret = -ENOMEM;\n\t\tif (initial) {\n\t\t\twhile (--i > 0)\n\t\t\t\tt7xx_unmap_bat_page(dpmaif_ctrl->dev, bat_req->bat_skb, i);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int t7xx_dpmaif_set_frag_to_skb(const struct dpmaif_rx_queue *rxq,\n\t\t\t\t       const struct dpmaif_pit *pkt_info,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tunsigned long long data_bus_addr, data_base_addr;\n\tstruct device *dev = rxq->dpmaif_ctrl->dev;\n\tstruct dpmaif_bat_page *page_info;\n\tunsigned int data_len;\n\tint data_offset;\n\n\tpage_info = rxq->bat_frag->bat_skb;\n\tpage_info += t7xx_normal_pit_bid(pkt_info);\n\tdma_unmap_page(dev, page_info->data_bus_addr, page_info->data_len, DMA_FROM_DEVICE);\n\n\tif (!page_info->page)\n\t\treturn -EINVAL;\n\n\tdata_bus_addr = le32_to_cpu(pkt_info->pd.data_addr_h);\n\tdata_bus_addr = (data_bus_addr << 32) + le32_to_cpu(pkt_info->pd.data_addr_l);\n\tdata_base_addr = page_info->data_bus_addr;\n\tdata_offset = data_bus_addr - data_base_addr;\n\tdata_offset += page_info->offset;\n\tdata_len = FIELD_GET(PD_PIT_DATA_LEN, le32_to_cpu(pkt_info->header));\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page_info->page,\n\t\t\tdata_offset, data_len, page_info->data_len);\n\n\tpage_info->page = NULL;\n\tpage_info->offset = 0;\n\tpage_info->data_len = 0;\n\treturn 0;\n}\n\nstatic int t7xx_dpmaif_get_frag(struct dpmaif_rx_queue *rxq,\n\t\t\t\tconst struct dpmaif_pit *pkt_info,\n\t\t\t\tconst struct dpmaif_cur_rx_skb_info *skb_info)\n{\n\tunsigned int cur_bid = t7xx_normal_pit_bid(pkt_info);\n\tint ret;\n\n\tret = t7xx_frag_bat_cur_bid_check(rxq, cur_bid);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = t7xx_dpmaif_set_frag_to_skb(rxq, pkt_info, skb_info->cur_skb);\n\tif (ret < 0) {\n\t\tdev_err(rxq->dpmaif_ctrl->dev, \"Failed to set frag data to skb: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tt7xx_dpmaif_set_bat_mask(rxq->bat_frag, cur_bid);\n\treturn 0;\n}\n\nstatic int t7xx_bat_cur_bid_check(struct dpmaif_rx_queue *rxq, const unsigned int cur_bid)\n{\n\tstruct dpmaif_bat_skb *bat_skb = rxq->bat_req->bat_skb;\n\n\tbat_skb += cur_bid;\n\tif (cur_bid >= DPMAIF_BAT_COUNT || !bat_skb->skb)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int t7xx_dpmaif_read_pit_seq(const struct dpmaif_pit *pit)\n{\n\treturn FIELD_GET(PD_PIT_PIT_SEQ, le32_to_cpu(pit->pd.footer));\n}\n\nstatic int t7xx_dpmaif_check_pit_seq(struct dpmaif_rx_queue *rxq,\n\t\t\t\t     const struct dpmaif_pit *pit)\n{\n\tunsigned int cur_pit_seq, expect_pit_seq = rxq->expect_pit_seq;\n\n\tif (read_poll_timeout_atomic(t7xx_dpmaif_read_pit_seq, cur_pit_seq,\n\t\t\t\t     cur_pit_seq == expect_pit_seq, DPMAIF_POLL_PIT_TIME_US,\n\t\t\t\t     DPMAIF_POLL_PIT_MAX_TIME_US, false, pit))\n\t\treturn -EFAULT;\n\n\trxq->expect_pit_seq++;\n\tif (rxq->expect_pit_seq >= DPMAIF_DL_PIT_SEQ_VALUE)\n\t\trxq->expect_pit_seq = 0;\n\n\treturn 0;\n}\n\nstatic unsigned int t7xx_dpmaif_avail_pkt_bat_cnt(struct dpmaif_bat_request *bat_req)\n{\n\tunsigned int zero_index;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bat_req->mask_lock, flags);\n\n\tzero_index = find_next_zero_bit(bat_req->bat_bitmap, bat_req->bat_size_cnt,\n\t\t\t\t\tbat_req->bat_release_rd_idx);\n\n\tif (zero_index < bat_req->bat_size_cnt) {\n\t\tspin_unlock_irqrestore(&bat_req->mask_lock, flags);\n\t\treturn zero_index - bat_req->bat_release_rd_idx;\n\t}\n\n\t \n\tzero_index = find_first_zero_bit(bat_req->bat_bitmap, bat_req->bat_release_rd_idx);\n\tspin_unlock_irqrestore(&bat_req->mask_lock, flags);\n\treturn bat_req->bat_size_cnt - bat_req->bat_release_rd_idx + zero_index;\n}\n\nstatic int t7xx_dpmaif_release_bat_entry(const struct dpmaif_rx_queue *rxq,\n\t\t\t\t\t const unsigned int rel_entry_num,\n\t\t\t\t\t const enum bat_type buf_type)\n{\n\tstruct dpmaif_hw_info *hw_info = &rxq->dpmaif_ctrl->hw_info;\n\tunsigned int old_rel_idx, new_rel_idx, hw_rd_idx, i;\n\tstruct dpmaif_bat_request *bat;\n\tunsigned long flags;\n\n\tif (!rxq->que_started || !rel_entry_num)\n\t\treturn -EINVAL;\n\n\tif (buf_type == BAT_TYPE_FRAG) {\n\t\tbat = rxq->bat_frag;\n\t\thw_rd_idx = t7xx_dpmaif_dl_get_frg_rd_idx(hw_info, rxq->index);\n\t} else {\n\t\tbat = rxq->bat_req;\n\t\thw_rd_idx = t7xx_dpmaif_dl_get_bat_rd_idx(hw_info, rxq->index);\n\t}\n\n\tif (rel_entry_num >= bat->bat_size_cnt)\n\t\treturn -EINVAL;\n\n\told_rel_idx = bat->bat_release_rd_idx;\n\tnew_rel_idx = old_rel_idx + rel_entry_num;\n\n\t \n\tif (bat->bat_wr_idx == old_rel_idx)\n\t\treturn 0;\n\n\tif (hw_rd_idx >= old_rel_idx) {\n\t\tif (new_rel_idx > hw_rd_idx)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (new_rel_idx >= bat->bat_size_cnt) {\n\t\tnew_rel_idx -= bat->bat_size_cnt;\n\t\tif (new_rel_idx > hw_rd_idx)\n\t\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_irqsave(&bat->mask_lock, flags);\n\tfor (i = 0; i < rel_entry_num; i++) {\n\t\tunsigned int index = bat->bat_release_rd_idx + i;\n\n\t\tif (index >= bat->bat_size_cnt)\n\t\t\tindex -= bat->bat_size_cnt;\n\n\t\tclear_bit(index, bat->bat_bitmap);\n\t}\n\tspin_unlock_irqrestore(&bat->mask_lock, flags);\n\n\tbat->bat_release_rd_idx = new_rel_idx;\n\treturn rel_entry_num;\n}\n\nstatic int t7xx_dpmaif_pit_release_and_add(struct dpmaif_rx_queue *rxq)\n{\n\tint ret;\n\n\tif (rxq->pit_remain_release_cnt < DPMAIF_PIT_CNT_THRESHOLD)\n\t\treturn 0;\n\n\tret = t7xx_dpmaifq_release_pit_entry(rxq, rxq->pit_remain_release_cnt);\n\tif (ret)\n\t\treturn ret;\n\n\trxq->pit_remain_release_cnt = 0;\n\treturn 0;\n}\n\nstatic int t7xx_dpmaif_bat_release_and_add(const struct dpmaif_rx_queue *rxq)\n{\n\tunsigned int bid_cnt;\n\tint ret;\n\n\tbid_cnt = t7xx_dpmaif_avail_pkt_bat_cnt(rxq->bat_req);\n\tif (bid_cnt < DPMAIF_BAT_CNT_THRESHOLD)\n\t\treturn 0;\n\n\tret = t7xx_dpmaif_release_bat_entry(rxq, bid_cnt, BAT_TYPE_NORMAL);\n\tif (ret <= 0) {\n\t\tdev_err(rxq->dpmaif_ctrl->dev, \"Release PKT BAT failed: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = t7xx_dpmaif_rx_buf_alloc(rxq->dpmaif_ctrl, rxq->bat_req, rxq->index, bid_cnt, false);\n\tif (ret < 0)\n\t\tdev_err(rxq->dpmaif_ctrl->dev, \"Allocate new RX buffer failed: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int t7xx_dpmaif_frag_bat_release_and_add(const struct dpmaif_rx_queue *rxq)\n{\n\tunsigned int bid_cnt;\n\tint ret;\n\n\tbid_cnt = t7xx_dpmaif_avail_pkt_bat_cnt(rxq->bat_frag);\n\tif (bid_cnt < DPMAIF_BAT_CNT_THRESHOLD)\n\t\treturn 0;\n\n\tret = t7xx_dpmaif_release_bat_entry(rxq, bid_cnt, BAT_TYPE_FRAG);\n\tif (ret <= 0) {\n\t\tdev_err(rxq->dpmaif_ctrl->dev, \"Release BAT entry failed: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\treturn t7xx_dpmaif_rx_frag_alloc(rxq->dpmaif_ctrl, rxq->bat_frag, bid_cnt, false);\n}\n\nstatic void t7xx_dpmaif_parse_msg_pit(const struct dpmaif_rx_queue *rxq,\n\t\t\t\t      const struct dpmaif_pit *msg_pit,\n\t\t\t\t      struct dpmaif_cur_rx_skb_info *skb_info)\n{\n\tint header = le32_to_cpu(msg_pit->header);\n\n\tskb_info->cur_chn_idx = FIELD_GET(MSG_PIT_CHANNEL_ID, header);\n\tskb_info->check_sum = FIELD_GET(MSG_PIT_CHECKSUM, header);\n\tskb_info->pit_dp = FIELD_GET(MSG_PIT_DP, header);\n\tskb_info->pkt_type = FIELD_GET(MSG_PIT_IP, le32_to_cpu(msg_pit->msg.params_3));\n}\n\nstatic int t7xx_dpmaif_set_data_to_skb(const struct dpmaif_rx_queue *rxq,\n\t\t\t\t       const struct dpmaif_pit *pkt_info,\n\t\t\t\t       struct dpmaif_cur_rx_skb_info *skb_info)\n{\n\tunsigned long long data_bus_addr, data_base_addr;\n\tstruct device *dev = rxq->dpmaif_ctrl->dev;\n\tstruct dpmaif_bat_skb *bat_skb;\n\tunsigned int data_len;\n\tstruct sk_buff *skb;\n\tint data_offset;\n\n\tbat_skb = rxq->bat_req->bat_skb;\n\tbat_skb += t7xx_normal_pit_bid(pkt_info);\n\tdma_unmap_single(dev, bat_skb->data_bus_addr, bat_skb->data_len, DMA_FROM_DEVICE);\n\n\tdata_bus_addr = le32_to_cpu(pkt_info->pd.data_addr_h);\n\tdata_bus_addr = (data_bus_addr << 32) + le32_to_cpu(pkt_info->pd.data_addr_l);\n\tdata_base_addr = bat_skb->data_bus_addr;\n\tdata_offset = data_bus_addr - data_base_addr;\n\tdata_len = FIELD_GET(PD_PIT_DATA_LEN, le32_to_cpu(pkt_info->header));\n\tskb = bat_skb->skb;\n\tskb->len = 0;\n\tskb_reset_tail_pointer(skb);\n\tskb_reserve(skb, data_offset);\n\n\tif (skb->tail + data_len > skb->end) {\n\t\tdev_err(dev, \"No buffer space available\\n\");\n\t\treturn -ENOBUFS;\n\t}\n\n\tskb_put(skb, data_len);\n\tskb_info->cur_skb = skb;\n\tbat_skb->skb = NULL;\n\treturn 0;\n}\n\nstatic int t7xx_dpmaif_get_rx_pkt(struct dpmaif_rx_queue *rxq,\n\t\t\t\t  const struct dpmaif_pit *pkt_info,\n\t\t\t\t  struct dpmaif_cur_rx_skb_info *skb_info)\n{\n\tunsigned int cur_bid = t7xx_normal_pit_bid(pkt_info);\n\tint ret;\n\n\tret = t7xx_bat_cur_bid_check(rxq, cur_bid);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = t7xx_dpmaif_set_data_to_skb(rxq, pkt_info, skb_info);\n\tif (ret < 0) {\n\t\tdev_err(rxq->dpmaif_ctrl->dev, \"RX set data to skb failed: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tt7xx_dpmaif_set_bat_mask(rxq->bat_req, cur_bid);\n\treturn 0;\n}\n\nstatic int t7xx_dpmaifq_rx_notify_hw(struct dpmaif_rx_queue *rxq)\n{\n\tstruct dpmaif_ctrl *dpmaif_ctrl = rxq->dpmaif_ctrl;\n\tint ret;\n\n\tqueue_work(dpmaif_ctrl->bat_release_wq, &dpmaif_ctrl->bat_release_work);\n\n\tret = t7xx_dpmaif_pit_release_and_add(rxq);\n\tif (ret < 0)\n\t\tdev_err(dpmaif_ctrl->dev, \"RXQ%u update PIT failed: %d\\n\", rxq->index, ret);\n\n\treturn ret;\n}\n\nstatic void t7xx_dpmaif_rx_skb(struct dpmaif_rx_queue *rxq,\n\t\t\t       struct dpmaif_cur_rx_skb_info *skb_info)\n{\n\tstruct dpmaif_ctrl *dpmaif_ctrl = rxq->dpmaif_ctrl;\n\tstruct sk_buff *skb = skb_info->cur_skb;\n\tstruct t7xx_skb_cb *skb_cb;\n\tu8 netif_id;\n\n\tskb_info->cur_skb = NULL;\n\n\tif (skb_info->pit_dp) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn;\n\t}\n\n\tskb->ip_summed = skb_info->check_sum == DPMAIF_CS_RESULT_PASS ? CHECKSUM_UNNECESSARY :\n\t\t\t\t\t\t\t\t\tCHECKSUM_NONE;\n\tnetif_id = FIELD_GET(NETIF_MASK, skb_info->cur_chn_idx);\n\tskb_cb = T7XX_SKB_CB(skb);\n\tskb_cb->netif_idx = netif_id;\n\tskb_cb->rx_pkt_type = skb_info->pkt_type;\n\tdpmaif_ctrl->callbacks->recv_skb(dpmaif_ctrl->t7xx_dev->ccmni_ctlb, skb, &rxq->napi);\n}\n\nstatic int t7xx_dpmaif_rx_start(struct dpmaif_rx_queue *rxq, const unsigned int pit_cnt,\n\t\t\t\tconst unsigned int budget, int *once_more)\n{\n\tunsigned int cur_pit, pit_len, rx_cnt, recv_skb_cnt = 0;\n\tstruct device *dev = rxq->dpmaif_ctrl->dev;\n\tstruct dpmaif_cur_rx_skb_info *skb_info;\n\tint ret = 0;\n\n\tpit_len = rxq->pit_size_cnt;\n\tskb_info = &rxq->rx_data_info;\n\tcur_pit = rxq->pit_rd_idx;\n\n\tfor (rx_cnt = 0; rx_cnt < pit_cnt; rx_cnt++) {\n\t\tstruct dpmaif_pit *pkt_info;\n\t\tu32 val;\n\n\t\tif (!skb_info->msg_pit_received && recv_skb_cnt >= budget)\n\t\t\tbreak;\n\n\t\tpkt_info = (struct dpmaif_pit *)rxq->pit_base + cur_pit;\n\t\tif (t7xx_dpmaif_check_pit_seq(rxq, pkt_info)) {\n\t\t\tdev_err_ratelimited(dev, \"RXQ%u checks PIT SEQ fail\\n\", rxq->index);\n\t\t\t*once_more = 1;\n\t\t\treturn recv_skb_cnt;\n\t\t}\n\n\t\tval = FIELD_GET(PD_PIT_PACKET_TYPE, le32_to_cpu(pkt_info->header));\n\t\tif (val == DES_PT_MSG) {\n\t\t\tif (skb_info->msg_pit_received)\n\t\t\t\tdev_err(dev, \"RXQ%u received repeated PIT\\n\", rxq->index);\n\n\t\t\tskb_info->msg_pit_received = true;\n\t\t\tt7xx_dpmaif_parse_msg_pit(rxq, pkt_info, skb_info);\n\t\t} else {  \n\t\t\tval = FIELD_GET(PD_PIT_BUFFER_TYPE, le32_to_cpu(pkt_info->header));\n\t\t\tif (val != PKT_BUF_FRAG)\n\t\t\t\tret = t7xx_dpmaif_get_rx_pkt(rxq, pkt_info, skb_info);\n\t\t\telse if (!skb_info->cur_skb)\n\t\t\t\tret = -EINVAL;\n\t\t\telse\n\t\t\t\tret = t7xx_dpmaif_get_frag(rxq, pkt_info, skb_info);\n\n\t\t\tif (ret < 0) {\n\t\t\t\tskb_info->err_payload = 1;\n\t\t\t\tdev_err_ratelimited(dev, \"RXQ%u error payload\\n\", rxq->index);\n\t\t\t}\n\n\t\t\tval = FIELD_GET(PD_PIT_CONT, le32_to_cpu(pkt_info->header));\n\t\t\tif (!val) {\n\t\t\t\tif (!skb_info->err_payload) {\n\t\t\t\t\tt7xx_dpmaif_rx_skb(rxq, skb_info);\n\t\t\t\t} else if (skb_info->cur_skb) {\n\t\t\t\t\tdev_kfree_skb_any(skb_info->cur_skb);\n\t\t\t\t\tskb_info->cur_skb = NULL;\n\t\t\t\t}\n\n\t\t\t\tmemset(skb_info, 0, sizeof(*skb_info));\n\t\t\t\trecv_skb_cnt++;\n\t\t\t}\n\t\t}\n\n\t\tcur_pit = t7xx_ring_buf_get_next_wr_idx(pit_len, cur_pit);\n\t\trxq->pit_rd_idx = cur_pit;\n\t\trxq->pit_remain_release_cnt++;\n\n\t\tif (rx_cnt > 0 && !(rx_cnt % DPMAIF_NOTIFY_RELEASE_COUNT)) {\n\t\t\tret = t7xx_dpmaifq_rx_notify_hw(rxq);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!ret)\n\t\tret = t7xx_dpmaifq_rx_notify_hw(rxq);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn recv_skb_cnt;\n}\n\nstatic unsigned int t7xx_dpmaifq_poll_pit(struct dpmaif_rx_queue *rxq)\n{\n\tunsigned int hw_wr_idx, pit_cnt;\n\n\tif (!rxq->que_started)\n\t\treturn 0;\n\n\thw_wr_idx = t7xx_dpmaif_dl_dlq_pit_get_wr_idx(&rxq->dpmaif_ctrl->hw_info, rxq->index);\n\tpit_cnt = t7xx_ring_buf_rd_wr_count(rxq->pit_size_cnt, rxq->pit_rd_idx, hw_wr_idx,\n\t\t\t\t\t    DPMAIF_READ);\n\trxq->pit_wr_idx = hw_wr_idx;\n\treturn pit_cnt;\n}\n\nstatic int t7xx_dpmaif_napi_rx_data_collect(struct dpmaif_ctrl *dpmaif_ctrl,\n\t\t\t\t\t    const unsigned int q_num,\n\t\t\t\t\t    const unsigned int budget, int *once_more)\n{\n\tstruct dpmaif_rx_queue *rxq = &dpmaif_ctrl->rxq[q_num];\n\tunsigned int cnt;\n\tint ret = 0;\n\n\tcnt = t7xx_dpmaifq_poll_pit(rxq);\n\tif (!cnt)\n\t\treturn ret;\n\n\tret = t7xx_dpmaif_rx_start(rxq, cnt, budget, once_more);\n\tif (ret < 0)\n\t\tdev_err(dpmaif_ctrl->dev, \"dlq%u rx ERR:%d\\n\", rxq->index, ret);\n\n\treturn ret;\n}\n\nint t7xx_dpmaif_napi_rx_poll(struct napi_struct *napi, const int budget)\n{\n\tstruct dpmaif_rx_queue *rxq = container_of(napi, struct dpmaif_rx_queue, napi);\n\tstruct t7xx_pci_dev *t7xx_dev = rxq->dpmaif_ctrl->t7xx_dev;\n\tint ret, once_more = 0, work_done = 0;\n\n\tatomic_set(&rxq->rx_processing, 1);\n\t \n\tsmp_mb();\n\n\tif (!rxq->que_started) {\n\t\tatomic_set(&rxq->rx_processing, 0);\n\t\tpm_runtime_put_autosuspend(rxq->dpmaif_ctrl->dev);\n\t\tdev_err(rxq->dpmaif_ctrl->dev, \"Work RXQ: %d has not been started\\n\", rxq->index);\n\t\treturn work_done;\n\t}\n\n\tif (!rxq->sleep_lock_pending)\n\t\tt7xx_pci_disable_sleep(t7xx_dev);\n\n\tret = try_wait_for_completion(&t7xx_dev->sleep_lock_acquire);\n\tif (!ret) {\n\t\tnapi_complete_done(napi, work_done);\n\t\trxq->sleep_lock_pending = true;\n\t\tnapi_reschedule(napi);\n\t\treturn work_done;\n\t}\n\n\trxq->sleep_lock_pending = false;\n\twhile (work_done < budget) {\n\t\tint each_budget = budget - work_done;\n\t\tint rx_cnt = t7xx_dpmaif_napi_rx_data_collect(rxq->dpmaif_ctrl, rxq->index,\n\t\t\t\t\t\t\t      each_budget, &once_more);\n\t\tif (rx_cnt > 0)\n\t\t\twork_done += rx_cnt;\n\t\telse\n\t\t\tbreak;\n\t}\n\n\tif (once_more) {\n\t\tnapi_gro_flush(napi, false);\n\t\twork_done = budget;\n\t\tt7xx_dpmaif_clr_ip_busy_sts(&rxq->dpmaif_ctrl->hw_info);\n\t} else if (work_done < budget) {\n\t\tnapi_complete_done(napi, work_done);\n\t\tt7xx_dpmaif_clr_ip_busy_sts(&rxq->dpmaif_ctrl->hw_info);\n\t\tt7xx_dpmaif_dlq_unmask_rx_done(&rxq->dpmaif_ctrl->hw_info, rxq->index);\n\t\tt7xx_pci_enable_sleep(rxq->dpmaif_ctrl->t7xx_dev);\n\t\tpm_runtime_mark_last_busy(rxq->dpmaif_ctrl->dev);\n\t\tpm_runtime_put_autosuspend(rxq->dpmaif_ctrl->dev);\n\t\tatomic_set(&rxq->rx_processing, 0);\n\t} else {\n\t\tt7xx_dpmaif_clr_ip_busy_sts(&rxq->dpmaif_ctrl->hw_info);\n\t}\n\n\treturn work_done;\n}\n\nvoid t7xx_dpmaif_irq_rx_done(struct dpmaif_ctrl *dpmaif_ctrl, const unsigned int que_mask)\n{\n\tstruct dpmaif_rx_queue *rxq;\n\tstruct dpmaif_ctrl *ctrl;\n\tint qno, ret;\n\n\tqno = ffs(que_mask) - 1;\n\tif (qno < 0 || qno > DPMAIF_RXQ_NUM - 1) {\n\t\tdev_err(dpmaif_ctrl->dev, \"Invalid RXQ number: %u\\n\", qno);\n\t\treturn;\n\t}\n\n\trxq = &dpmaif_ctrl->rxq[qno];\n\tctrl = rxq->dpmaif_ctrl;\n\t \n\tret = pm_runtime_resume_and_get(ctrl->dev);\n\tif (ret < 0 && ret != -EACCES) {\n\t\tdev_err(ctrl->dev, \"Failed to resume device: %d\\n\", ret);\n\t\treturn;\n\t}\n\tnapi_schedule(&rxq->napi);\n}\n\nstatic void t7xx_dpmaif_base_free(const struct dpmaif_ctrl *dpmaif_ctrl,\n\t\t\t\t  const struct dpmaif_bat_request *bat_req)\n{\n\tif (bat_req->bat_base)\n\t\tdma_free_coherent(dpmaif_ctrl->dev,\n\t\t\t\t  bat_req->bat_size_cnt * sizeof(struct dpmaif_bat),\n\t\t\t\t  bat_req->bat_base, bat_req->bat_bus_addr);\n}\n\n \nint t7xx_dpmaif_bat_alloc(const struct dpmaif_ctrl *dpmaif_ctrl, struct dpmaif_bat_request *bat_req,\n\t\t\t  const enum bat_type buf_type)\n{\n\tint sw_buf_size;\n\n\tif (buf_type == BAT_TYPE_FRAG) {\n\t\tsw_buf_size = sizeof(struct dpmaif_bat_page);\n\t\tbat_req->bat_size_cnt = DPMAIF_FRG_COUNT;\n\t\tbat_req->pkt_buf_sz = DPMAIF_HW_FRG_PKTBUF;\n\t} else {\n\t\tsw_buf_size = sizeof(struct dpmaif_bat_skb);\n\t\tbat_req->bat_size_cnt = DPMAIF_BAT_COUNT;\n\t\tbat_req->pkt_buf_sz = DPMAIF_HW_BAT_PKTBUF;\n\t}\n\n\tbat_req->type = buf_type;\n\tbat_req->bat_wr_idx = 0;\n\tbat_req->bat_release_rd_idx = 0;\n\n\tbat_req->bat_base = dma_alloc_coherent(dpmaif_ctrl->dev,\n\t\t\t\t\t       bat_req->bat_size_cnt * sizeof(struct dpmaif_bat),\n\t\t\t\t\t       &bat_req->bat_bus_addr, GFP_KERNEL | __GFP_ZERO);\n\tif (!bat_req->bat_base)\n\t\treturn -ENOMEM;\n\n\t \n\tbat_req->bat_skb = devm_kzalloc(dpmaif_ctrl->dev, bat_req->bat_size_cnt * sw_buf_size,\n\t\t\t\t\tGFP_KERNEL);\n\tif (!bat_req->bat_skb)\n\t\tgoto err_free_dma_mem;\n\n\tbat_req->bat_bitmap = bitmap_zalloc(bat_req->bat_size_cnt, GFP_KERNEL);\n\tif (!bat_req->bat_bitmap)\n\t\tgoto err_free_dma_mem;\n\n\tspin_lock_init(&bat_req->mask_lock);\n\tatomic_set(&bat_req->refcnt, 0);\n\treturn 0;\n\nerr_free_dma_mem:\n\tt7xx_dpmaif_base_free(dpmaif_ctrl, bat_req);\n\n\treturn -ENOMEM;\n}\n\nvoid t7xx_dpmaif_bat_free(const struct dpmaif_ctrl *dpmaif_ctrl, struct dpmaif_bat_request *bat_req)\n{\n\tif (!bat_req || !atomic_dec_and_test(&bat_req->refcnt))\n\t\treturn;\n\n\tbitmap_free(bat_req->bat_bitmap);\n\tbat_req->bat_bitmap = NULL;\n\n\tif (bat_req->bat_skb) {\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i < bat_req->bat_size_cnt; i++) {\n\t\t\tif (bat_req->type == BAT_TYPE_FRAG)\n\t\t\t\tt7xx_unmap_bat_page(dpmaif_ctrl->dev, bat_req->bat_skb, i);\n\t\t\telse\n\t\t\t\tt7xx_unmap_bat_skb(dpmaif_ctrl->dev, bat_req->bat_skb, i);\n\t\t}\n\t}\n\n\tt7xx_dpmaif_base_free(dpmaif_ctrl, bat_req);\n}\n\nstatic int t7xx_dpmaif_rx_alloc(struct dpmaif_rx_queue *rxq)\n{\n\trxq->pit_size_cnt = DPMAIF_PIT_COUNT;\n\trxq->pit_rd_idx = 0;\n\trxq->pit_wr_idx = 0;\n\trxq->pit_release_rd_idx = 0;\n\trxq->expect_pit_seq = 0;\n\trxq->pit_remain_release_cnt = 0;\n\tmemset(&rxq->rx_data_info, 0, sizeof(rxq->rx_data_info));\n\n\trxq->pit_base = dma_alloc_coherent(rxq->dpmaif_ctrl->dev,\n\t\t\t\t\t   rxq->pit_size_cnt * sizeof(struct dpmaif_pit),\n\t\t\t\t\t   &rxq->pit_bus_addr, GFP_KERNEL | __GFP_ZERO);\n\tif (!rxq->pit_base)\n\t\treturn -ENOMEM;\n\n\trxq->bat_req = &rxq->dpmaif_ctrl->bat_req;\n\tatomic_inc(&rxq->bat_req->refcnt);\n\n\trxq->bat_frag = &rxq->dpmaif_ctrl->bat_frag;\n\tatomic_inc(&rxq->bat_frag->refcnt);\n\treturn 0;\n}\n\nstatic void t7xx_dpmaif_rx_buf_free(const struct dpmaif_rx_queue *rxq)\n{\n\tif (!rxq->dpmaif_ctrl)\n\t\treturn;\n\n\tt7xx_dpmaif_bat_free(rxq->dpmaif_ctrl, rxq->bat_req);\n\tt7xx_dpmaif_bat_free(rxq->dpmaif_ctrl, rxq->bat_frag);\n\n\tif (rxq->pit_base)\n\t\tdma_free_coherent(rxq->dpmaif_ctrl->dev,\n\t\t\t\t  rxq->pit_size_cnt * sizeof(struct dpmaif_pit),\n\t\t\t\t  rxq->pit_base, rxq->pit_bus_addr);\n}\n\nint t7xx_dpmaif_rxq_init(struct dpmaif_rx_queue *queue)\n{\n\tint ret;\n\n\tret = t7xx_dpmaif_rx_alloc(queue);\n\tif (ret < 0)\n\t\tdev_err(queue->dpmaif_ctrl->dev, \"Failed to allocate RX buffers: %d\\n\", ret);\n\n\treturn ret;\n}\n\nvoid t7xx_dpmaif_rxq_free(struct dpmaif_rx_queue *queue)\n{\n\tt7xx_dpmaif_rx_buf_free(queue);\n}\n\nstatic void t7xx_dpmaif_bat_release_work(struct work_struct *work)\n{\n\tstruct dpmaif_ctrl *dpmaif_ctrl = container_of(work, struct dpmaif_ctrl, bat_release_work);\n\tstruct dpmaif_rx_queue *rxq;\n\tint ret;\n\n\tret = pm_runtime_resume_and_get(dpmaif_ctrl->dev);\n\tif (ret < 0 && ret != -EACCES)\n\t\treturn;\n\n\tt7xx_pci_disable_sleep(dpmaif_ctrl->t7xx_dev);\n\n\t \n\trxq = &dpmaif_ctrl->rxq[DPF_RX_QNO_DFT];\n\tif (t7xx_pci_sleep_disable_complete(dpmaif_ctrl->t7xx_dev)) {\n\t\tt7xx_dpmaif_bat_release_and_add(rxq);\n\t\tt7xx_dpmaif_frag_bat_release_and_add(rxq);\n\t}\n\n\tt7xx_pci_enable_sleep(dpmaif_ctrl->t7xx_dev);\n\tpm_runtime_mark_last_busy(dpmaif_ctrl->dev);\n\tpm_runtime_put_autosuspend(dpmaif_ctrl->dev);\n}\n\nint t7xx_dpmaif_bat_rel_wq_alloc(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tdpmaif_ctrl->bat_release_wq = alloc_workqueue(\"dpmaif_bat_release_work_queue\",\n\t\t\t\t\t\t      WQ_MEM_RECLAIM, 1);\n\tif (!dpmaif_ctrl->bat_release_wq)\n\t\treturn -ENOMEM;\n\n\tINIT_WORK(&dpmaif_ctrl->bat_release_work, t7xx_dpmaif_bat_release_work);\n\treturn 0;\n}\n\nvoid t7xx_dpmaif_bat_wq_rel(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tflush_work(&dpmaif_ctrl->bat_release_work);\n\n\tif (dpmaif_ctrl->bat_release_wq) {\n\t\tdestroy_workqueue(dpmaif_ctrl->bat_release_wq);\n\t\tdpmaif_ctrl->bat_release_wq = NULL;\n\t}\n}\n\n \nvoid t7xx_dpmaif_rx_stop(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < DPMAIF_RXQ_NUM; i++) {\n\t\tstruct dpmaif_rx_queue *rxq = &dpmaif_ctrl->rxq[i];\n\t\tint timeout, value;\n\n\t\ttimeout = readx_poll_timeout_atomic(atomic_read, &rxq->rx_processing, value,\n\t\t\t\t\t\t    !value, 0, DPMAIF_CHECK_INIT_TIMEOUT_US);\n\t\tif (timeout)\n\t\t\tdev_err(dpmaif_ctrl->dev, \"Stop RX SW failed\\n\");\n\n\t\t \n\t\tsmp_mb();\n\t\trxq->que_started = false;\n\t}\n}\n\nstatic void t7xx_dpmaif_stop_rxq(struct dpmaif_rx_queue *rxq)\n{\n\tint cnt, j = 0;\n\n\trxq->que_started = false;\n\n\tdo {\n\t\tcnt = t7xx_ring_buf_rd_wr_count(rxq->pit_size_cnt, rxq->pit_rd_idx,\n\t\t\t\t\t\trxq->pit_wr_idx, DPMAIF_READ);\n\n\t\tif (++j >= DPMAIF_MAX_CHECK_COUNT) {\n\t\t\tdev_err(rxq->dpmaif_ctrl->dev, \"Stop RX SW failed, %d\\n\", cnt);\n\t\t\tbreak;\n\t\t}\n\t} while (cnt);\n\n\tmemset(rxq->pit_base, 0, rxq->pit_size_cnt * sizeof(struct dpmaif_pit));\n\tmemset(rxq->bat_req->bat_base, 0, rxq->bat_req->bat_size_cnt * sizeof(struct dpmaif_bat));\n\tbitmap_zero(rxq->bat_req->bat_bitmap, rxq->bat_req->bat_size_cnt);\n\tmemset(&rxq->rx_data_info, 0, sizeof(rxq->rx_data_info));\n\n\trxq->pit_rd_idx = 0;\n\trxq->pit_wr_idx = 0;\n\trxq->pit_release_rd_idx = 0;\n\trxq->expect_pit_seq = 0;\n\trxq->pit_remain_release_cnt = 0;\n\trxq->bat_req->bat_release_rd_idx = 0;\n\trxq->bat_req->bat_wr_idx = 0;\n\trxq->bat_frag->bat_release_rd_idx = 0;\n\trxq->bat_frag->bat_wr_idx = 0;\n}\n\nvoid t7xx_dpmaif_rx_clear(struct dpmaif_ctrl *dpmaif_ctrl)\n{\n\tint i;\n\n\tfor (i = 0; i < DPMAIF_RXQ_NUM; i++)\n\t\tt7xx_dpmaif_stop_rxq(&dpmaif_ctrl->rxq[i]);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}