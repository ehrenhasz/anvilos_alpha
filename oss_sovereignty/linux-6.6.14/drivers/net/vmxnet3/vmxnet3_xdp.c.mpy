{
  "module_name": "vmxnet3_xdp.c",
  "hash_id": "9d4c2194df10dfce332d38414c4ce20febc5b6267f7d0ce706c6d045cb338af1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/vmxnet3/vmxnet3_xdp.c",
  "human_readable_source": "\n \n\n#include \"vmxnet3_int.h\"\n#include \"vmxnet3_xdp.h\"\n\nstatic void\nvmxnet3_xdp_exchange_program(struct vmxnet3_adapter *adapter,\n\t\t\t     struct bpf_prog *prog)\n{\n\trcu_assign_pointer(adapter->xdp_bpf_prog, prog);\n}\n\nstatic inline struct vmxnet3_tx_queue *\nvmxnet3_xdp_get_tq(struct vmxnet3_adapter *adapter)\n{\n\tstruct vmxnet3_tx_queue *tq;\n\tint tq_number;\n\tint cpu;\n\n\ttq_number = adapter->num_tx_queues;\n\tcpu = smp_processor_id();\n\tif (likely(cpu < tq_number))\n\t\ttq = &adapter->tx_queue[cpu];\n\telse\n\t\ttq = &adapter->tx_queue[reciprocal_scale(cpu, tq_number)];\n\n\treturn tq;\n}\n\nstatic int\nvmxnet3_xdp_set(struct net_device *netdev, struct netdev_bpf *bpf,\n\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tstruct bpf_prog *new_bpf_prog = bpf->prog;\n\tstruct bpf_prog *old_bpf_prog;\n\tbool need_update;\n\tbool running;\n\tint err;\n\n\tif (new_bpf_prog && netdev->mtu > VMXNET3_XDP_MAX_MTU) {\n\t\tNL_SET_ERR_MSG_FMT_MOD(extack, \"MTU %u too large for XDP\",\n\t\t\t\t       netdev->mtu);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (adapter->netdev->features & NETIF_F_LRO) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"LRO is not supported with XDP\");\n\t\tadapter->netdev->features &= ~NETIF_F_LRO;\n\t}\n\n\told_bpf_prog = rcu_dereference(adapter->xdp_bpf_prog);\n\tif (!new_bpf_prog && !old_bpf_prog)\n\t\treturn 0;\n\n\trunning = netif_running(netdev);\n\tneed_update = !!old_bpf_prog != !!new_bpf_prog;\n\n\tif (running && need_update)\n\t\tvmxnet3_quiesce_dev(adapter);\n\n\tvmxnet3_xdp_exchange_program(adapter, new_bpf_prog);\n\tif (old_bpf_prog)\n\t\tbpf_prog_put(old_bpf_prog);\n\n\tif (!running || !need_update)\n\t\treturn 0;\n\n\tif (new_bpf_prog)\n\t\txdp_features_set_redirect_target(netdev, false);\n\telse\n\t\txdp_features_clear_redirect_target(netdev);\n\n\tvmxnet3_reset_dev(adapter);\n\tvmxnet3_rq_destroy_all(adapter);\n\tvmxnet3_adjust_rx_ring_size(adapter);\n\terr = vmxnet3_rq_create_all(adapter);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"failed to re-create rx queues for XDP.\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\terr = vmxnet3_activate_dev(adapter);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"failed to activate device for XDP.\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n\n\treturn 0;\n}\n\n \nint\nvmxnet3_xdp(struct net_device *netdev, struct netdev_bpf *bpf)\n{\n\tswitch (bpf->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn vmxnet3_xdp_set(netdev, bpf, bpf->extack);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nvmxnet3_xdp_xmit_frame(struct vmxnet3_adapter *adapter,\n\t\t       struct xdp_frame *xdpf,\n\t\t       struct vmxnet3_tx_queue *tq, bool dma_map)\n{\n\tstruct vmxnet3_tx_buf_info *tbi = NULL;\n\tunion Vmxnet3_GenericDesc *gdesc;\n\tstruct vmxnet3_tx_ctx ctx;\n\tint tx_num_deferred;\n\tstruct page *page;\n\tu32 buf_size;\n\tu32 dw2;\n\n\tdw2 = (tq->tx_ring.gen ^ 0x1) << VMXNET3_TXD_GEN_SHIFT;\n\tdw2 |= xdpf->len;\n\tctx.sop_txd = tq->tx_ring.base + tq->tx_ring.next2fill;\n\tgdesc = ctx.sop_txd;\n\n\tbuf_size = xdpf->len;\n\ttbi = tq->buf_info + tq->tx_ring.next2fill;\n\n\tif (vmxnet3_cmd_ring_desc_avail(&tq->tx_ring) == 0) {\n\t\ttq->stats.tx_ring_full++;\n\t\treturn -ENOSPC;\n\t}\n\n\ttbi->map_type = VMXNET3_MAP_XDP;\n\tif (dma_map) {  \n\t\ttbi->dma_addr = dma_map_single(&adapter->pdev->dev,\n\t\t\t\t\t       xdpf->data, buf_size,\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&adapter->pdev->dev, tbi->dma_addr))\n\t\t\treturn -EFAULT;\n\t\ttbi->map_type |= VMXNET3_MAP_SINGLE;\n\t} else {  \n\t\tpage = virt_to_page(xdpf->data);\n\t\ttbi->dma_addr = page_pool_get_dma_addr(page) +\n\t\t\t\tVMXNET3_XDP_HEADROOM;\n\t\tdma_sync_single_for_device(&adapter->pdev->dev,\n\t\t\t\t\t   tbi->dma_addr, buf_size,\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t}\n\ttbi->xdpf = xdpf;\n\ttbi->len = buf_size;\n\n\tgdesc = tq->tx_ring.base + tq->tx_ring.next2fill;\n\tWARN_ON_ONCE(gdesc->txd.gen == tq->tx_ring.gen);\n\n\tgdesc->txd.addr = cpu_to_le64(tbi->dma_addr);\n\tgdesc->dword[2] = cpu_to_le32(dw2);\n\n\t \n\tgdesc->dword[3] = cpu_to_le32(VMXNET3_TXD_CQ | VMXNET3_TXD_EOP);\n\n\tgdesc->txd.om = 0;\n\tgdesc->txd.msscof = 0;\n\tgdesc->txd.hlen = 0;\n\tgdesc->txd.ti = 0;\n\n\ttx_num_deferred = le32_to_cpu(tq->shared->txNumDeferred);\n\tle32_add_cpu(&tq->shared->txNumDeferred, 1);\n\ttx_num_deferred++;\n\n\tvmxnet3_cmd_ring_adv_next2fill(&tq->tx_ring);\n\n\t \n\ttbi->sop_idx = ctx.sop_txd - tq->tx_ring.base;\n\n\tdma_wmb();\n\tgdesc->dword[2] = cpu_to_le32(le32_to_cpu(gdesc->dword[2]) ^\n\t\t\t\t\t\t  VMXNET3_TXD_GEN);\n\n\t \n\tif (tx_num_deferred >= le32_to_cpu(tq->shared->txThreshold)) {\n\t\ttq->shared->txNumDeferred = 0;\n\t\tVMXNET3_WRITE_BAR0_REG(adapter,\n\t\t\t\t       VMXNET3_REG_TXPROD + tq->qid * 8,\n\t\t\t\t       tq->tx_ring.next2fill);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nvmxnet3_xdp_xmit_back(struct vmxnet3_adapter *adapter,\n\t\t      struct xdp_frame *xdpf)\n{\n\tstruct vmxnet3_tx_queue *tq;\n\tstruct netdev_queue *nq;\n\tint err;\n\n\ttq = vmxnet3_xdp_get_tq(adapter);\n\tif (tq->stopped)\n\t\treturn -ENETDOWN;\n\n\tnq = netdev_get_tx_queue(adapter->netdev, tq->qid);\n\n\t__netif_tx_lock(nq, smp_processor_id());\n\terr = vmxnet3_xdp_xmit_frame(adapter, xdpf, tq, false);\n\t__netif_tx_unlock(nq);\n\n\treturn err;\n}\n\n \nint\nvmxnet3_xdp_xmit(struct net_device *dev,\n\t\t int n, struct xdp_frame **frames, u32 flags)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(dev);\n\tstruct vmxnet3_tx_queue *tq;\n\tint i;\n\n\tif (unlikely(test_bit(VMXNET3_STATE_BIT_QUIESCED, &adapter->state)))\n\t\treturn -ENETDOWN;\n\tif (unlikely(test_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state)))\n\t\treturn -EINVAL;\n\n\ttq = vmxnet3_xdp_get_tq(adapter);\n\tif (tq->stopped)\n\t\treturn -ENETDOWN;\n\n\tfor (i = 0; i < n; i++) {\n\t\tif (vmxnet3_xdp_xmit_frame(adapter, frames[i], tq, true)) {\n\t\t\ttq->stats.xdp_xmit_err++;\n\t\t\tbreak;\n\t\t}\n\t}\n\ttq->stats.xdp_xmit += i;\n\n\treturn i;\n}\n\nstatic int\nvmxnet3_run_xdp(struct vmxnet3_rx_queue *rq, struct xdp_buff *xdp,\n\t\tstruct bpf_prog *prog)\n{\n\tstruct xdp_frame *xdpf;\n\tstruct page *page;\n\tint err;\n\tu32 act;\n\n\trq->stats.xdp_packets++;\n\tact = bpf_prog_run_xdp(prog, xdp);\n\tpage = virt_to_page(xdp->data_hard_start);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\treturn act;\n\tcase XDP_REDIRECT:\n\t\terr = xdp_do_redirect(rq->adapter->netdev, xdp, prog);\n\t\tif (!err) {\n\t\t\trq->stats.xdp_redirects++;\n\t\t} else {\n\t\t\trq->stats.xdp_drops++;\n\t\t\tpage_pool_recycle_direct(rq->page_pool, page);\n\t\t}\n\t\treturn act;\n\tcase XDP_TX:\n\t\txdpf = xdp_convert_buff_to_frame(xdp);\n\t\tif (unlikely(!xdpf ||\n\t\t\t     vmxnet3_xdp_xmit_back(rq->adapter, xdpf))) {\n\t\t\trq->stats.xdp_drops++;\n\t\t\tpage_pool_recycle_direct(rq->page_pool, page);\n\t\t} else {\n\t\t\trq->stats.xdp_tx++;\n\t\t}\n\t\treturn act;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rq->adapter->netdev, prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(rq->adapter->netdev, prog, act);\n\t\trq->stats.xdp_aborted++;\n\t\tbreak;\n\tcase XDP_DROP:\n\t\trq->stats.xdp_drops++;\n\t\tbreak;\n\t}\n\n\tpage_pool_recycle_direct(rq->page_pool, page);\n\n\treturn act;\n}\n\nstatic struct sk_buff *\nvmxnet3_build_skb(struct vmxnet3_rx_queue *rq, struct page *page,\n\t\t  const struct xdp_buff *xdp)\n{\n\tstruct sk_buff *skb;\n\n\tskb = build_skb(page_address(page), PAGE_SIZE);\n\tif (unlikely(!skb)) {\n\t\tpage_pool_recycle_direct(rq->page_pool, page);\n\t\trq->stats.rx_buf_alloc_failure++;\n\t\treturn NULL;\n\t}\n\n\t \n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\tskb_put(skb, xdp->data_end - xdp->data);\n\tskb_mark_for_recycle(skb);\n\n\treturn skb;\n}\n\n \nint\nvmxnet3_process_xdp_small(struct vmxnet3_adapter *adapter,\n\t\t\t  struct vmxnet3_rx_queue *rq,\n\t\t\t  void *data, int len,\n\t\t\t  struct sk_buff **skb_xdp_pass)\n{\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_buff xdp;\n\tstruct page *page;\n\tint act;\n\n\tpage = page_pool_alloc_pages(rq->page_pool, GFP_ATOMIC);\n\tif (unlikely(!page)) {\n\t\trq->stats.rx_buf_alloc_failure++;\n\t\treturn XDP_DROP;\n\t}\n\n\txdp_init_buff(&xdp, PAGE_SIZE, &rq->xdp_rxq);\n\txdp_prepare_buff(&xdp, page_address(page), rq->page_pool->p.offset,\n\t\t\t len, false);\n\txdp_buff_clear_frags_flag(&xdp);\n\n\t \n\tmemcpy(xdp.data, data, len);\n\n\txdp_prog = rcu_dereference(rq->adapter->xdp_bpf_prog);\n\tif (!xdp_prog) {\n\t\tact = XDP_PASS;\n\t\tgoto out_skb;\n\t}\n\tact = vmxnet3_run_xdp(rq, &xdp, xdp_prog);\n\tif (act != XDP_PASS)\n\t\treturn act;\n\nout_skb:\n\t*skb_xdp_pass = vmxnet3_build_skb(rq, page, &xdp);\n\tif (!*skb_xdp_pass)\n\t\treturn XDP_DROP;\n\n\t \n\treturn likely(*skb_xdp_pass) ? act : XDP_DROP;\n}\n\nint\nvmxnet3_process_xdp(struct vmxnet3_adapter *adapter,\n\t\t    struct vmxnet3_rx_queue *rq,\n\t\t    struct Vmxnet3_RxCompDesc *rcd,\n\t\t    struct vmxnet3_rx_buf_info *rbi,\n\t\t    struct Vmxnet3_RxDesc *rxd,\n\t\t    struct sk_buff **skb_xdp_pass)\n{\n\tstruct bpf_prog *xdp_prog;\n\tdma_addr_t new_dma_addr;\n\tstruct xdp_buff xdp;\n\tstruct page *page;\n\tvoid *new_data;\n\tint act;\n\n\tpage = rbi->page;\n\tdma_sync_single_for_cpu(&adapter->pdev->dev,\n\t\t\t\tpage_pool_get_dma_addr(page) +\n\t\t\t\trq->page_pool->p.offset, rcd->len,\n\t\t\t\tpage_pool_get_dma_dir(rq->page_pool));\n\n\txdp_init_buff(&xdp, rbi->len, &rq->xdp_rxq);\n\txdp_prepare_buff(&xdp, page_address(page), rq->page_pool->p.offset,\n\t\t\t rcd->len, false);\n\txdp_buff_clear_frags_flag(&xdp);\n\n\txdp_prog = rcu_dereference(rq->adapter->xdp_bpf_prog);\n\tif (!xdp_prog) {\n\t\tact = XDP_PASS;\n\t\tgoto out_skb;\n\t}\n\tact = vmxnet3_run_xdp(rq, &xdp, xdp_prog);\n\n\tif (act == XDP_PASS) {\nout_skb:\n\t\t*skb_xdp_pass = vmxnet3_build_skb(rq, page, &xdp);\n\t\tif (!*skb_xdp_pass)\n\t\t\tact = XDP_DROP;\n\t}\n\n\tnew_data = vmxnet3_pp_get_buff(rq->page_pool, &new_dma_addr,\n\t\t\t\t       GFP_ATOMIC);\n\tif (!new_data) {\n\t\trq->stats.rx_buf_alloc_failure++;\n\t\treturn XDP_DROP;\n\t}\n\trbi->page = virt_to_page(new_data);\n\trbi->dma_addr = new_dma_addr;\n\trxd->addr = cpu_to_le64(rbi->dma_addr);\n\trxd->len = rbi->len;\n\n\treturn act;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}