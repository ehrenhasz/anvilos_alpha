{
  "module_name": "main.c",
  "hash_id": "6ba6a90ef74404855ad5ef48201835c207aee14b33d407871761eee3274d6190",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/thunderbolt/main.c",
  "human_readable_source": "\n \n\n#include <linux/atomic.h>\n#include <linux/highmem.h>\n#include <linux/if_vlan.h>\n#include <linux/jhash.h>\n#include <linux/module.h>\n#include <linux/etherdevice.h>\n#include <linux/rtnetlink.h>\n#include <linux/sizes.h>\n#include <linux/thunderbolt.h>\n#include <linux/uuid.h>\n#include <linux/workqueue.h>\n\n#include <net/ip6_checksum.h>\n\n#include \"trace.h\"\n\n \n#define TBNET_LOGIN_DELAY\t4500\n#define TBNET_LOGIN_TIMEOUT\t500\n#define TBNET_LOGOUT_TIMEOUT\t1000\n\n#define TBNET_RING_SIZE\t\t256\n#define TBNET_LOGIN_RETRIES\t60\n#define TBNET_LOGOUT_RETRIES\t10\n#define TBNET_E2E\t\tBIT(0)\n#define TBNET_MATCH_FRAGS_ID\tBIT(1)\n#define TBNET_64K_FRAMES\tBIT(2)\n#define TBNET_MAX_MTU\t\tSZ_64K\n#define TBNET_FRAME_SIZE\tSZ_4K\n#define TBNET_MAX_PAYLOAD_SIZE\t\\\n\t(TBNET_FRAME_SIZE - sizeof(struct thunderbolt_ip_frame_header))\n \n#define TBNET_RX_MAX_SIZE\t\\\n\t(TBNET_FRAME_SIZE + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n#define TBNET_RX_PAGE_ORDER\tget_order(TBNET_RX_MAX_SIZE)\n#define TBNET_RX_PAGE_SIZE\t(PAGE_SIZE << TBNET_RX_PAGE_ORDER)\n\n#define TBNET_L0_PORT_NUM(route) ((route) & GENMASK(5, 0))\n\n \nstruct thunderbolt_ip_frame_header {\n\t__le32 frame_size;\n\t__le16 frame_index;\n\t__le16 frame_id;\n\t__le32 frame_count;\n};\n\nenum thunderbolt_ip_frame_pdf {\n\tTBIP_PDF_FRAME_START = 1,\n\tTBIP_PDF_FRAME_END,\n};\n\nenum thunderbolt_ip_type {\n\tTBIP_LOGIN,\n\tTBIP_LOGIN_RESPONSE,\n\tTBIP_LOGOUT,\n\tTBIP_STATUS,\n};\n\nstruct thunderbolt_ip_header {\n\tu32 route_hi;\n\tu32 route_lo;\n\tu32 length_sn;\n\tuuid_t uuid;\n\tuuid_t initiator_uuid;\n\tuuid_t target_uuid;\n\tu32 type;\n\tu32 command_id;\n};\n\n#define TBIP_HDR_LENGTH_MASK\t\tGENMASK(5, 0)\n#define TBIP_HDR_SN_MASK\t\tGENMASK(28, 27)\n#define TBIP_HDR_SN_SHIFT\t\t27\n\nstruct thunderbolt_ip_login {\n\tstruct thunderbolt_ip_header hdr;\n\tu32 proto_version;\n\tu32 transmit_path;\n\tu32 reserved[4];\n};\n\n#define TBIP_LOGIN_PROTO_VERSION\t1\n\nstruct thunderbolt_ip_login_response {\n\tstruct thunderbolt_ip_header hdr;\n\tu32 status;\n\tu32 receiver_mac[2];\n\tu32 receiver_mac_len;\n\tu32 reserved[4];\n};\n\nstruct thunderbolt_ip_logout {\n\tstruct thunderbolt_ip_header hdr;\n};\n\nstruct thunderbolt_ip_status {\n\tstruct thunderbolt_ip_header hdr;\n\tu32 status;\n};\n\nstruct tbnet_stats {\n\tu64 tx_packets;\n\tu64 rx_packets;\n\tu64 tx_bytes;\n\tu64 rx_bytes;\n\tu64 rx_errors;\n\tu64 tx_errors;\n\tu64 rx_length_errors;\n\tu64 rx_over_errors;\n\tu64 rx_crc_errors;\n\tu64 rx_missed_errors;\n};\n\nstruct tbnet_frame {\n\tstruct net_device *dev;\n\tstruct page *page;\n\tstruct ring_frame frame;\n};\n\nstruct tbnet_ring {\n\tstruct tbnet_frame frames[TBNET_RING_SIZE];\n\tunsigned int cons;\n\tunsigned int prod;\n\tstruct tb_ring *ring;\n};\n\n \nstruct tbnet {\n\tconst struct tb_service *svc;\n\tstruct tb_xdomain *xd;\n\tstruct tb_protocol_handler handler;\n\tstruct net_device *dev;\n\tstruct napi_struct napi;\n\tstruct tbnet_stats stats;\n\tstruct sk_buff *skb;\n\tatomic_t command_id;\n\tbool login_sent;\n\tbool login_received;\n\tint local_transmit_path;\n\tint remote_transmit_path;\n\tstruct mutex connection_lock;\n\tint login_retries;\n\tstruct delayed_work login_work;\n\tstruct work_struct connected_work;\n\tstruct work_struct disconnect_work;\n\tstruct thunderbolt_ip_frame_header rx_hdr;\n\tstruct tbnet_ring rx_ring;\n\tatomic_t frame_id;\n\tstruct tbnet_ring tx_ring;\n};\n\n \nstatic const uuid_t tbnet_dir_uuid =\n\tUUID_INIT(0xc66189ca, 0x1cce, 0x4195,\n\t\t  0xbd, 0xb8, 0x49, 0x59, 0x2e, 0x5f, 0x5a, 0x4f);\n\n \nstatic const uuid_t tbnet_svc_uuid =\n\tUUID_INIT(0x798f589e, 0x3616, 0x8a47,\n\t\t  0x97, 0xc6, 0x56, 0x64, 0xa9, 0x20, 0xc8, 0xdd);\n\nstatic struct tb_property_dir *tbnet_dir;\n\nstatic bool tbnet_e2e = true;\nmodule_param_named(e2e, tbnet_e2e, bool, 0444);\nMODULE_PARM_DESC(e2e, \"USB4NET full end-to-end flow control (default: true)\");\n\nstatic void tbnet_fill_header(struct thunderbolt_ip_header *hdr, u64 route,\n\tu8 sequence, const uuid_t *initiator_uuid, const uuid_t *target_uuid,\n\tenum thunderbolt_ip_type type, size_t size, u32 command_id)\n{\n\tu32 length_sn;\n\n\t \n\tlength_sn = (size - 3 * 4) / 4;\n\tlength_sn |= (sequence << TBIP_HDR_SN_SHIFT) & TBIP_HDR_SN_MASK;\n\n\thdr->route_hi = upper_32_bits(route);\n\thdr->route_lo = lower_32_bits(route);\n\thdr->length_sn = length_sn;\n\tuuid_copy(&hdr->uuid, &tbnet_svc_uuid);\n\tuuid_copy(&hdr->initiator_uuid, initiator_uuid);\n\tuuid_copy(&hdr->target_uuid, target_uuid);\n\thdr->type = type;\n\thdr->command_id = command_id;\n}\n\nstatic int tbnet_login_response(struct tbnet *net, u64 route, u8 sequence,\n\t\t\t\tu32 command_id)\n{\n\tstruct thunderbolt_ip_login_response reply;\n\tstruct tb_xdomain *xd = net->xd;\n\n\tmemset(&reply, 0, sizeof(reply));\n\ttbnet_fill_header(&reply.hdr, route, sequence, xd->local_uuid,\n\t\t\t  xd->remote_uuid, TBIP_LOGIN_RESPONSE, sizeof(reply),\n\t\t\t  command_id);\n\tmemcpy(reply.receiver_mac, net->dev->dev_addr, ETH_ALEN);\n\treply.receiver_mac_len = ETH_ALEN;\n\n\treturn tb_xdomain_response(xd, &reply, sizeof(reply),\n\t\t\t\t   TB_CFG_PKG_XDOMAIN_RESP);\n}\n\nstatic int tbnet_login_request(struct tbnet *net, u8 sequence)\n{\n\tstruct thunderbolt_ip_login_response reply;\n\tstruct thunderbolt_ip_login request;\n\tstruct tb_xdomain *xd = net->xd;\n\n\tmemset(&request, 0, sizeof(request));\n\ttbnet_fill_header(&request.hdr, xd->route, sequence, xd->local_uuid,\n\t\t\t  xd->remote_uuid, TBIP_LOGIN, sizeof(request),\n\t\t\t  atomic_inc_return(&net->command_id));\n\n\trequest.proto_version = TBIP_LOGIN_PROTO_VERSION;\n\trequest.transmit_path = net->local_transmit_path;\n\n\treturn tb_xdomain_request(xd, &request, sizeof(request),\n\t\t\t\t  TB_CFG_PKG_XDOMAIN_RESP, &reply,\n\t\t\t\t  sizeof(reply), TB_CFG_PKG_XDOMAIN_RESP,\n\t\t\t\t  TBNET_LOGIN_TIMEOUT);\n}\n\nstatic int tbnet_logout_response(struct tbnet *net, u64 route, u8 sequence,\n\t\t\t\t u32 command_id)\n{\n\tstruct thunderbolt_ip_status reply;\n\tstruct tb_xdomain *xd = net->xd;\n\n\tmemset(&reply, 0, sizeof(reply));\n\ttbnet_fill_header(&reply.hdr, route, sequence, xd->local_uuid,\n\t\t\t  xd->remote_uuid, TBIP_STATUS, sizeof(reply),\n\t\t\t  atomic_inc_return(&net->command_id));\n\treturn tb_xdomain_response(xd, &reply, sizeof(reply),\n\t\t\t\t   TB_CFG_PKG_XDOMAIN_RESP);\n}\n\nstatic int tbnet_logout_request(struct tbnet *net)\n{\n\tstruct thunderbolt_ip_logout request;\n\tstruct thunderbolt_ip_status reply;\n\tstruct tb_xdomain *xd = net->xd;\n\n\tmemset(&request, 0, sizeof(request));\n\ttbnet_fill_header(&request.hdr, xd->route, 0, xd->local_uuid,\n\t\t\t  xd->remote_uuid, TBIP_LOGOUT, sizeof(request),\n\t\t\t  atomic_inc_return(&net->command_id));\n\n\treturn tb_xdomain_request(xd, &request, sizeof(request),\n\t\t\t\t  TB_CFG_PKG_XDOMAIN_RESP, &reply,\n\t\t\t\t  sizeof(reply), TB_CFG_PKG_XDOMAIN_RESP,\n\t\t\t\t  TBNET_LOGOUT_TIMEOUT);\n}\n\nstatic void start_login(struct tbnet *net)\n{\n\tnetdev_dbg(net->dev, \"login started\\n\");\n\n\tmutex_lock(&net->connection_lock);\n\tnet->login_sent = false;\n\tnet->login_received = false;\n\tmutex_unlock(&net->connection_lock);\n\n\tqueue_delayed_work(system_long_wq, &net->login_work,\n\t\t\t   msecs_to_jiffies(1000));\n}\n\nstatic void stop_login(struct tbnet *net)\n{\n\tcancel_delayed_work_sync(&net->login_work);\n\tcancel_work_sync(&net->connected_work);\n\n\tnetdev_dbg(net->dev, \"login stopped\\n\");\n}\n\nstatic inline unsigned int tbnet_frame_size(const struct tbnet_frame *tf)\n{\n\treturn tf->frame.size ? : TBNET_FRAME_SIZE;\n}\n\nstatic void tbnet_free_buffers(struct tbnet_ring *ring)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < TBNET_RING_SIZE; i++) {\n\t\tstruct device *dma_dev = tb_ring_dma_device(ring->ring);\n\t\tstruct tbnet_frame *tf = &ring->frames[i];\n\t\tenum dma_data_direction dir;\n\t\tunsigned int order;\n\t\tsize_t size;\n\n\t\tif (!tf->page)\n\t\t\tcontinue;\n\n\t\tif (ring->ring->is_tx) {\n\t\t\tdir = DMA_TO_DEVICE;\n\t\t\torder = 0;\n\t\t\tsize = TBNET_FRAME_SIZE;\n\t\t} else {\n\t\t\tdir = DMA_FROM_DEVICE;\n\t\t\torder = TBNET_RX_PAGE_ORDER;\n\t\t\tsize = TBNET_RX_PAGE_SIZE;\n\t\t}\n\n\t\ttrace_tbnet_free_frame(i, tf->page, tf->frame.buffer_phy, dir);\n\n\t\tif (tf->frame.buffer_phy)\n\t\t\tdma_unmap_page(dma_dev, tf->frame.buffer_phy, size,\n\t\t\t\t       dir);\n\n\t\t__free_pages(tf->page, order);\n\t\ttf->page = NULL;\n\t}\n\n\tring->cons = 0;\n\tring->prod = 0;\n}\n\nstatic void tbnet_tear_down(struct tbnet *net, bool send_logout)\n{\n\tnetif_carrier_off(net->dev);\n\tnetif_stop_queue(net->dev);\n\n\tstop_login(net);\n\n\tmutex_lock(&net->connection_lock);\n\n\tif (net->login_sent && net->login_received) {\n\t\tint ret, retries = TBNET_LOGOUT_RETRIES;\n\n\t\twhile (send_logout && retries-- > 0) {\n\t\t\tnetdev_dbg(net->dev, \"sending logout request %u\\n\",\n\t\t\t\t   retries);\n\t\t\tret = tbnet_logout_request(net);\n\t\t\tif (ret != -ETIMEDOUT)\n\t\t\t\tbreak;\n\t\t}\n\n\t\ttb_ring_stop(net->rx_ring.ring);\n\t\ttb_ring_stop(net->tx_ring.ring);\n\t\ttbnet_free_buffers(&net->rx_ring);\n\t\ttbnet_free_buffers(&net->tx_ring);\n\n\t\tret = tb_xdomain_disable_paths(net->xd,\n\t\t\t\t\t       net->local_transmit_path,\n\t\t\t\t\t       net->rx_ring.ring->hop,\n\t\t\t\t\t       net->remote_transmit_path,\n\t\t\t\t\t       net->tx_ring.ring->hop);\n\t\tif (ret)\n\t\t\tnetdev_warn(net->dev, \"failed to disable DMA paths\\n\");\n\n\t\ttb_xdomain_release_in_hopid(net->xd, net->remote_transmit_path);\n\t\tnet->remote_transmit_path = 0;\n\t}\n\n\tnet->login_retries = 0;\n\tnet->login_sent = false;\n\tnet->login_received = false;\n\n\tnetdev_dbg(net->dev, \"network traffic stopped\\n\");\n\n\tmutex_unlock(&net->connection_lock);\n}\n\nstatic int tbnet_handle_packet(const void *buf, size_t size, void *data)\n{\n\tconst struct thunderbolt_ip_login *pkg = buf;\n\tstruct tbnet *net = data;\n\tu32 command_id;\n\tint ret = 0;\n\tu32 sequence;\n\tu64 route;\n\n\t \n\tif (size < sizeof(struct thunderbolt_ip_header))\n\t\treturn 0;\n\tif (!uuid_equal(&pkg->hdr.initiator_uuid, net->xd->remote_uuid))\n\t\treturn 0;\n\tif (!uuid_equal(&pkg->hdr.target_uuid, net->xd->local_uuid))\n\t\treturn 0;\n\n\troute = ((u64)pkg->hdr.route_hi << 32) | pkg->hdr.route_lo;\n\troute &= ~BIT_ULL(63);\n\tif (route != net->xd->route)\n\t\treturn 0;\n\n\tsequence = pkg->hdr.length_sn & TBIP_HDR_SN_MASK;\n\tsequence >>= TBIP_HDR_SN_SHIFT;\n\tcommand_id = pkg->hdr.command_id;\n\n\tswitch (pkg->hdr.type) {\n\tcase TBIP_LOGIN:\n\t\tnetdev_dbg(net->dev, \"remote login request received\\n\");\n\t\tif (!netif_running(net->dev))\n\t\t\tbreak;\n\n\t\tret = tbnet_login_response(net, route, sequence,\n\t\t\t\t\t   pkg->hdr.command_id);\n\t\tif (!ret) {\n\t\t\tnetdev_dbg(net->dev, \"remote login response sent\\n\");\n\n\t\t\tmutex_lock(&net->connection_lock);\n\t\t\tnet->login_received = true;\n\t\t\tnet->remote_transmit_path = pkg->transmit_path;\n\n\t\t\t \n\t\t\tif (net->login_retries >= TBNET_LOGIN_RETRIES ||\n\t\t\t    !net->login_sent) {\n\t\t\t\tnet->login_retries = 0;\n\t\t\t\tqueue_delayed_work(system_long_wq,\n\t\t\t\t\t\t   &net->login_work, 0);\n\t\t\t}\n\t\t\tmutex_unlock(&net->connection_lock);\n\n\t\t\tqueue_work(system_long_wq, &net->connected_work);\n\t\t}\n\t\tbreak;\n\n\tcase TBIP_LOGOUT:\n\t\tnetdev_dbg(net->dev, \"remote logout request received\\n\");\n\t\tret = tbnet_logout_response(net, route, sequence, command_id);\n\t\tif (!ret) {\n\t\t\tnetdev_dbg(net->dev, \"remote logout response sent\\n\");\n\t\t\tqueue_work(system_long_wq, &net->disconnect_work);\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (ret)\n\t\tnetdev_warn(net->dev, \"failed to send ThunderboltIP response\\n\");\n\n\treturn 1;\n}\n\nstatic unsigned int tbnet_available_buffers(const struct tbnet_ring *ring)\n{\n\treturn ring->prod - ring->cons;\n}\n\nstatic int tbnet_alloc_rx_buffers(struct tbnet *net, unsigned int nbuffers)\n{\n\tstruct tbnet_ring *ring = &net->rx_ring;\n\tint ret;\n\n\twhile (nbuffers--) {\n\t\tstruct device *dma_dev = tb_ring_dma_device(ring->ring);\n\t\tunsigned int index = ring->prod & (TBNET_RING_SIZE - 1);\n\t\tstruct tbnet_frame *tf = &ring->frames[index];\n\t\tdma_addr_t dma_addr;\n\n\t\tif (tf->page)\n\t\t\tbreak;\n\n\t\t \n\t\ttf->page = dev_alloc_pages(TBNET_RX_PAGE_ORDER);\n\t\tif (!tf->page) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tdma_addr = dma_map_page(dma_dev, tf->page, 0,\n\t\t\t\t\tTBNET_RX_PAGE_SIZE, DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(dma_dev, dma_addr)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\ttf->frame.buffer_phy = dma_addr;\n\t\ttf->dev = net->dev;\n\n\t\ttrace_tbnet_alloc_rx_frame(index, tf->page, dma_addr,\n\t\t\t\t\t   DMA_FROM_DEVICE);\n\n\t\ttb_ring_rx(ring->ring, &tf->frame);\n\n\t\tring->prod++;\n\t}\n\n\treturn 0;\n\nerr_free:\n\ttbnet_free_buffers(ring);\n\treturn ret;\n}\n\nstatic struct tbnet_frame *tbnet_get_tx_buffer(struct tbnet *net)\n{\n\tstruct tbnet_ring *ring = &net->tx_ring;\n\tstruct device *dma_dev = tb_ring_dma_device(ring->ring);\n\tstruct tbnet_frame *tf;\n\tunsigned int index;\n\n\tif (!tbnet_available_buffers(ring))\n\t\treturn NULL;\n\n\tindex = ring->cons++ & (TBNET_RING_SIZE - 1);\n\n\ttf = &ring->frames[index];\n\ttf->frame.size = 0;\n\n\tdma_sync_single_for_cpu(dma_dev, tf->frame.buffer_phy,\n\t\t\t\ttbnet_frame_size(tf), DMA_TO_DEVICE);\n\n\treturn tf;\n}\n\nstatic void tbnet_tx_callback(struct tb_ring *ring, struct ring_frame *frame,\n\t\t\t      bool canceled)\n{\n\tstruct tbnet_frame *tf = container_of(frame, typeof(*tf), frame);\n\tstruct tbnet *net = netdev_priv(tf->dev);\n\n\t \n\tnet->tx_ring.prod++;\n\n\tif (tbnet_available_buffers(&net->tx_ring) >= TBNET_RING_SIZE / 2)\n\t\tnetif_wake_queue(net->dev);\n}\n\nstatic int tbnet_alloc_tx_buffers(struct tbnet *net)\n{\n\tstruct tbnet_ring *ring = &net->tx_ring;\n\tstruct device *dma_dev = tb_ring_dma_device(ring->ring);\n\tunsigned int i;\n\n\tfor (i = 0; i < TBNET_RING_SIZE; i++) {\n\t\tstruct tbnet_frame *tf = &ring->frames[i];\n\t\tdma_addr_t dma_addr;\n\n\t\ttf->page = alloc_page(GFP_KERNEL);\n\t\tif (!tf->page) {\n\t\t\ttbnet_free_buffers(ring);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdma_addr = dma_map_page(dma_dev, tf->page, 0, TBNET_FRAME_SIZE,\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dma_dev, dma_addr)) {\n\t\t\t__free_page(tf->page);\n\t\t\ttf->page = NULL;\n\t\t\ttbnet_free_buffers(ring);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttf->dev = net->dev;\n\t\ttf->frame.buffer_phy = dma_addr;\n\t\ttf->frame.callback = tbnet_tx_callback;\n\t\ttf->frame.sof = TBIP_PDF_FRAME_START;\n\t\ttf->frame.eof = TBIP_PDF_FRAME_END;\n\n\t\ttrace_tbnet_alloc_tx_frame(i, tf->page, dma_addr, DMA_TO_DEVICE);\n\t}\n\n\tring->cons = 0;\n\tring->prod = TBNET_RING_SIZE - 1;\n\n\treturn 0;\n}\n\nstatic void tbnet_connected_work(struct work_struct *work)\n{\n\tstruct tbnet *net = container_of(work, typeof(*net), connected_work);\n\tbool connected;\n\tint ret;\n\n\tif (netif_carrier_ok(net->dev))\n\t\treturn;\n\n\tmutex_lock(&net->connection_lock);\n\tconnected = net->login_sent && net->login_received;\n\tmutex_unlock(&net->connection_lock);\n\n\tif (!connected)\n\t\treturn;\n\n\tnetdev_dbg(net->dev, \"login successful, enabling paths\\n\");\n\n\tret = tb_xdomain_alloc_in_hopid(net->xd, net->remote_transmit_path);\n\tif (ret != net->remote_transmit_path) {\n\t\tnetdev_err(net->dev, \"failed to allocate Rx HopID\\n\");\n\t\treturn;\n\t}\n\n\t \n\ttb_ring_start(net->tx_ring.ring);\n\ttb_ring_start(net->rx_ring.ring);\n\n\tret = tbnet_alloc_rx_buffers(net, TBNET_RING_SIZE);\n\tif (ret)\n\t\tgoto err_stop_rings;\n\n\tret = tbnet_alloc_tx_buffers(net);\n\tif (ret)\n\t\tgoto err_free_rx_buffers;\n\n\tret = tb_xdomain_enable_paths(net->xd, net->local_transmit_path,\n\t\t\t\t      net->rx_ring.ring->hop,\n\t\t\t\t      net->remote_transmit_path,\n\t\t\t\t      net->tx_ring.ring->hop);\n\tif (ret) {\n\t\tnetdev_err(net->dev, \"failed to enable DMA paths\\n\");\n\t\tgoto err_free_tx_buffers;\n\t}\n\n\tnetif_carrier_on(net->dev);\n\tnetif_start_queue(net->dev);\n\n\tnetdev_dbg(net->dev, \"network traffic started\\n\");\n\treturn;\n\nerr_free_tx_buffers:\n\ttbnet_free_buffers(&net->tx_ring);\nerr_free_rx_buffers:\n\ttbnet_free_buffers(&net->rx_ring);\nerr_stop_rings:\n\ttb_ring_stop(net->rx_ring.ring);\n\ttb_ring_stop(net->tx_ring.ring);\n\ttb_xdomain_release_in_hopid(net->xd, net->remote_transmit_path);\n}\n\nstatic void tbnet_login_work(struct work_struct *work)\n{\n\tstruct tbnet *net = container_of(work, typeof(*net), login_work.work);\n\tunsigned long delay = msecs_to_jiffies(TBNET_LOGIN_DELAY);\n\tint ret;\n\n\tif (netif_carrier_ok(net->dev))\n\t\treturn;\n\n\tnetdev_dbg(net->dev, \"sending login request, retries=%u\\n\",\n\t\t   net->login_retries);\n\n\tret = tbnet_login_request(net, net->login_retries % 4);\n\tif (ret) {\n\t\tnetdev_dbg(net->dev, \"sending login request failed, ret=%d\\n\",\n\t\t\t   ret);\n\t\tif (net->login_retries++ < TBNET_LOGIN_RETRIES) {\n\t\t\tqueue_delayed_work(system_long_wq, &net->login_work,\n\t\t\t\t\t   delay);\n\t\t} else {\n\t\t\tnetdev_info(net->dev, \"ThunderboltIP login timed out\\n\");\n\t\t}\n\t} else {\n\t\tnetdev_dbg(net->dev, \"received login reply\\n\");\n\n\t\tnet->login_retries = 0;\n\n\t\tmutex_lock(&net->connection_lock);\n\t\tnet->login_sent = true;\n\t\tmutex_unlock(&net->connection_lock);\n\n\t\tqueue_work(system_long_wq, &net->connected_work);\n\t}\n}\n\nstatic void tbnet_disconnect_work(struct work_struct *work)\n{\n\tstruct tbnet *net = container_of(work, typeof(*net), disconnect_work);\n\n\ttbnet_tear_down(net, false);\n}\n\nstatic bool tbnet_check_frame(struct tbnet *net, const struct tbnet_frame *tf,\n\t\t\t      const struct thunderbolt_ip_frame_header *hdr)\n{\n\tu32 frame_id, frame_count, frame_size, frame_index;\n\tunsigned int size;\n\n\tif (tf->frame.flags & RING_DESC_CRC_ERROR) {\n\t\tnet->stats.rx_crc_errors++;\n\t\treturn false;\n\t} else if (tf->frame.flags & RING_DESC_BUFFER_OVERRUN) {\n\t\tnet->stats.rx_over_errors++;\n\t\treturn false;\n\t}\n\n\t \n\tsize = tbnet_frame_size(tf);\n\tif (size <= sizeof(*hdr)) {\n\t\tnet->stats.rx_length_errors++;\n\t\treturn false;\n\t}\n\n\tframe_count = le32_to_cpu(hdr->frame_count);\n\tframe_size = le32_to_cpu(hdr->frame_size);\n\tframe_index = le16_to_cpu(hdr->frame_index);\n\tframe_id = le16_to_cpu(hdr->frame_id);\n\n\tif ((frame_size > size - sizeof(*hdr)) || !frame_size) {\n\t\tnet->stats.rx_length_errors++;\n\t\treturn false;\n\t}\n\n\t \n\tif (net->skb && net->rx_hdr.frame_count) {\n\t\t \n\t\tif (frame_count != le32_to_cpu(net->rx_hdr.frame_count)) {\n\t\t\tnet->stats.rx_length_errors++;\n\t\t\treturn false;\n\t\t}\n\n\t\t \n\t\tif (frame_index != le16_to_cpu(net->rx_hdr.frame_index) + 1 ||\n\t\t    frame_id != le16_to_cpu(net->rx_hdr.frame_id)) {\n\t\t\tnet->stats.rx_missed_errors++;\n\t\t\treturn false;\n\t\t}\n\n\t\tif (net->skb->len + frame_size > TBNET_MAX_MTU) {\n\t\t\tnet->stats.rx_length_errors++;\n\t\t\treturn false;\n\t\t}\n\n\t\treturn true;\n\t}\n\n\t \n\tif (frame_count == 0 || frame_count > TBNET_RING_SIZE / 4) {\n\t\tnet->stats.rx_length_errors++;\n\t\treturn false;\n\t}\n\tif (frame_index != 0) {\n\t\tnet->stats.rx_missed_errors++;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int tbnet_poll(struct napi_struct *napi, int budget)\n{\n\tstruct tbnet *net = container_of(napi, struct tbnet, napi);\n\tunsigned int cleaned_count = tbnet_available_buffers(&net->rx_ring);\n\tstruct device *dma_dev = tb_ring_dma_device(net->rx_ring.ring);\n\tunsigned int rx_packets = 0;\n\n\twhile (rx_packets < budget) {\n\t\tconst struct thunderbolt_ip_frame_header *hdr;\n\t\tunsigned int hdr_size = sizeof(*hdr);\n\t\tstruct sk_buff *skb = NULL;\n\t\tstruct ring_frame *frame;\n\t\tstruct tbnet_frame *tf;\n\t\tstruct page *page;\n\t\tbool last = true;\n\t\tu32 frame_size;\n\n\t\t \n\t\tif (cleaned_count >= MAX_SKB_FRAGS) {\n\t\t\ttbnet_alloc_rx_buffers(net, cleaned_count);\n\t\t\tcleaned_count = 0;\n\t\t}\n\n\t\tframe = tb_ring_poll(net->rx_ring.ring);\n\t\tif (!frame)\n\t\t\tbreak;\n\n\t\tdma_unmap_page(dma_dev, frame->buffer_phy,\n\t\t\t       TBNET_RX_PAGE_SIZE, DMA_FROM_DEVICE);\n\n\t\ttf = container_of(frame, typeof(*tf), frame);\n\n\t\tpage = tf->page;\n\t\ttf->page = NULL;\n\t\tnet->rx_ring.cons++;\n\t\tcleaned_count++;\n\n\t\thdr = page_address(page);\n\t\tif (!tbnet_check_frame(net, tf, hdr)) {\n\t\t\ttrace_tbnet_invalid_rx_ip_frame(hdr->frame_size,\n\t\t\t\thdr->frame_id, hdr->frame_index, hdr->frame_count);\n\t\t\t__free_pages(page, TBNET_RX_PAGE_ORDER);\n\t\t\tdev_kfree_skb_any(net->skb);\n\t\t\tnet->skb = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\ttrace_tbnet_rx_ip_frame(hdr->frame_size, hdr->frame_id,\n\t\t\t\t\thdr->frame_index, hdr->frame_count);\n\t\tframe_size = le32_to_cpu(hdr->frame_size);\n\n\t\tskb = net->skb;\n\t\tif (!skb) {\n\t\t\tskb = build_skb(page_address(page),\n\t\t\t\t\tTBNET_RX_PAGE_SIZE);\n\t\t\tif (!skb) {\n\t\t\t\t__free_pages(page, TBNET_RX_PAGE_ORDER);\n\t\t\t\tnet->stats.rx_errors++;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tskb_reserve(skb, hdr_size);\n\t\t\tskb_put(skb, frame_size);\n\n\t\t\tnet->skb = skb;\n\t\t} else {\n\t\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t\tpage, hdr_size, frame_size,\n\t\t\t\t\tTBNET_RX_PAGE_SIZE - hdr_size);\n\t\t}\n\n\t\tnet->rx_hdr.frame_size = hdr->frame_size;\n\t\tnet->rx_hdr.frame_count = hdr->frame_count;\n\t\tnet->rx_hdr.frame_index = hdr->frame_index;\n\t\tnet->rx_hdr.frame_id = hdr->frame_id;\n\t\tlast = le16_to_cpu(net->rx_hdr.frame_index) ==\n\t\t       le32_to_cpu(net->rx_hdr.frame_count) - 1;\n\n\t\trx_packets++;\n\t\tnet->stats.rx_bytes += frame_size;\n\n\t\tif (last) {\n\t\t\tskb->protocol = eth_type_trans(skb, net->dev);\n\t\t\ttrace_tbnet_rx_skb(skb);\n\t\t\tnapi_gro_receive(&net->napi, skb);\n\t\t\tnet->skb = NULL;\n\t\t}\n\t}\n\n\tnet->stats.rx_packets += rx_packets;\n\n\tif (cleaned_count)\n\t\ttbnet_alloc_rx_buffers(net, cleaned_count);\n\n\tif (rx_packets >= budget)\n\t\treturn budget;\n\n\tnapi_complete_done(napi, rx_packets);\n\t \n\ttb_ring_poll_complete(net->rx_ring.ring);\n\n\treturn rx_packets;\n}\n\nstatic void tbnet_start_poll(void *data)\n{\n\tstruct tbnet *net = data;\n\n\tnapi_schedule(&net->napi);\n}\n\nstatic int tbnet_open(struct net_device *dev)\n{\n\tstruct tbnet *net = netdev_priv(dev);\n\tstruct tb_xdomain *xd = net->xd;\n\tu16 sof_mask, eof_mask;\n\tstruct tb_ring *ring;\n\tunsigned int flags;\n\tint hopid;\n\n\tnetif_carrier_off(dev);\n\n\tring = tb_ring_alloc_tx(xd->tb->nhi, -1, TBNET_RING_SIZE,\n\t\t\t\tRING_FLAG_FRAME);\n\tif (!ring) {\n\t\tnetdev_err(dev, \"failed to allocate Tx ring\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tnet->tx_ring.ring = ring;\n\n\thopid = tb_xdomain_alloc_out_hopid(xd, -1);\n\tif (hopid < 0) {\n\t\tnetdev_err(dev, \"failed to allocate Tx HopID\\n\");\n\t\ttb_ring_free(net->tx_ring.ring);\n\t\tnet->tx_ring.ring = NULL;\n\t\treturn hopid;\n\t}\n\tnet->local_transmit_path = hopid;\n\n\tsof_mask = BIT(TBIP_PDF_FRAME_START);\n\teof_mask = BIT(TBIP_PDF_FRAME_END);\n\n\tflags = RING_FLAG_FRAME;\n\t \n\tif (tbnet_e2e && net->svc->prtcstns & TBNET_E2E)\n\t\tflags |= RING_FLAG_E2E;\n\n\tring = tb_ring_alloc_rx(xd->tb->nhi, -1, TBNET_RING_SIZE, flags,\n\t\t\t\tnet->tx_ring.ring->hop, sof_mask,\n\t\t\t\teof_mask, tbnet_start_poll, net);\n\tif (!ring) {\n\t\tnetdev_err(dev, \"failed to allocate Rx ring\\n\");\n\t\ttb_xdomain_release_out_hopid(xd, hopid);\n\t\ttb_ring_free(net->tx_ring.ring);\n\t\tnet->tx_ring.ring = NULL;\n\t\treturn -ENOMEM;\n\t}\n\tnet->rx_ring.ring = ring;\n\n\tnapi_enable(&net->napi);\n\tstart_login(net);\n\n\treturn 0;\n}\n\nstatic int tbnet_stop(struct net_device *dev)\n{\n\tstruct tbnet *net = netdev_priv(dev);\n\n\tnapi_disable(&net->napi);\n\n\tcancel_work_sync(&net->disconnect_work);\n\ttbnet_tear_down(net, true);\n\n\ttb_ring_free(net->rx_ring.ring);\n\tnet->rx_ring.ring = NULL;\n\n\ttb_xdomain_release_out_hopid(net->xd, net->local_transmit_path);\n\ttb_ring_free(net->tx_ring.ring);\n\tnet->tx_ring.ring = NULL;\n\n\treturn 0;\n}\n\nstatic bool tbnet_xmit_csum_and_map(struct tbnet *net, struct sk_buff *skb,\n\tstruct tbnet_frame **frames, u32 frame_count)\n{\n\tstruct thunderbolt_ip_frame_header *hdr = page_address(frames[0]->page);\n\tstruct device *dma_dev = tb_ring_dma_device(net->tx_ring.ring);\n\tunsigned int i, len, offset = skb_transport_offset(skb);\n\t \n\tu32 paylen = skb->len - skb_transport_offset(skb);\n\t__wsum wsum = (__force __wsum)htonl(paylen);\n\t__be16 protocol = skb->protocol;\n\tvoid *data = skb->data;\n\tvoid *dest = hdr + 1;\n\t__sum16 *tucso;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL) {\n\t\t \n\t\tfor (i = 0; i < frame_count; i++) {\n\t\t\thdr = page_address(frames[i]->page);\n\t\t\thdr->frame_count = cpu_to_le32(frame_count);\n\t\t\ttrace_tbnet_tx_ip_frame(hdr->frame_size, hdr->frame_id,\n\t\t\t\t\t\thdr->frame_index, hdr->frame_count);\n\t\t\tdma_sync_single_for_device(dma_dev,\n\t\t\t\tframes[i]->frame.buffer_phy,\n\t\t\t\ttbnet_frame_size(frames[i]), DMA_TO_DEVICE);\n\t\t}\n\n\t\treturn true;\n\t}\n\n\tif (protocol == htons(ETH_P_8021Q)) {\n\t\tstruct vlan_hdr *vhdr, vh;\n\n\t\tvhdr = skb_header_pointer(skb, ETH_HLEN, sizeof(vh), &vh);\n\t\tif (!vhdr)\n\t\t\treturn false;\n\n\t\tprotocol = vhdr->h_vlan_encapsulated_proto;\n\t}\n\n\t \n\tif (protocol == htons(ETH_P_IP)) {\n\t\t__sum16 *ipcso = dest + ((void *)&(ip_hdr(skb)->check) - data);\n\n\t\t*ipcso = 0;\n\t\t*ipcso = ip_fast_csum(dest + skb_network_offset(skb),\n\t\t\t\t      ip_hdr(skb)->ihl);\n\n\t\tif (ip_hdr(skb)->protocol == IPPROTO_TCP)\n\t\t\ttucso = dest + ((void *)&(tcp_hdr(skb)->check) - data);\n\t\telse if (ip_hdr(skb)->protocol == IPPROTO_UDP)\n\t\t\ttucso = dest + ((void *)&(udp_hdr(skb)->check) - data);\n\t\telse\n\t\t\treturn false;\n\n\t\t*tucso = ~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t    ip_hdr(skb)->daddr, 0,\n\t\t\t\t\t    ip_hdr(skb)->protocol, 0);\n\t} else if (skb_is_gso(skb) && skb_is_gso_v6(skb)) {\n\t\ttucso = dest + ((void *)&(tcp_hdr(skb)->check) - data);\n\t\t*tucso = ~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t  &ipv6_hdr(skb)->daddr, 0,\n\t\t\t\t\t  IPPROTO_TCP, 0);\n\t} else if (protocol == htons(ETH_P_IPV6)) {\n\t\ttucso = dest + skb_checksum_start_offset(skb) + skb->csum_offset;\n\t\t*tucso = ~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t  &ipv6_hdr(skb)->daddr, 0,\n\t\t\t\t\t  ipv6_hdr(skb)->nexthdr, 0);\n\t} else {\n\t\treturn false;\n\t}\n\n\t \n\tfor (i = 0; i < frame_count; i++) {\n\t\thdr = page_address(frames[i]->page);\n\t\tdest = (void *)(hdr + 1) + offset;\n\t\tlen = le32_to_cpu(hdr->frame_size) - offset;\n\t\twsum = csum_partial(dest, len, wsum);\n\t\thdr->frame_count = cpu_to_le32(frame_count);\n\t\ttrace_tbnet_tx_ip_frame(hdr->frame_size, hdr->frame_id,\n\t\t\t\t\thdr->frame_index, hdr->frame_count);\n\n\t\toffset = 0;\n\t}\n\n\t*tucso = csum_fold(wsum);\n\n\t \n\tfor (i = 0; i < frame_count; i++) {\n\t\tdma_sync_single_for_device(dma_dev, frames[i]->frame.buffer_phy,\n\t\t\ttbnet_frame_size(frames[i]), DMA_TO_DEVICE);\n\t}\n\n\treturn true;\n}\n\nstatic void *tbnet_kmap_frag(struct sk_buff *skb, unsigned int frag_num,\n\t\t\t     unsigned int *len)\n{\n\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[frag_num];\n\n\t*len = skb_frag_size(frag);\n\treturn kmap_local_page(skb_frag_page(frag)) + skb_frag_off(frag);\n}\n\nstatic netdev_tx_t tbnet_start_xmit(struct sk_buff *skb,\n\t\t\t\t    struct net_device *dev)\n{\n\tstruct tbnet *net = netdev_priv(dev);\n\tstruct tbnet_frame *frames[MAX_SKB_FRAGS];\n\tu16 frame_id = atomic_read(&net->frame_id);\n\tstruct thunderbolt_ip_frame_header *hdr;\n\tunsigned int len = skb_headlen(skb);\n\tunsigned int data_len = skb->len;\n\tunsigned int nframes, i;\n\tunsigned int frag = 0;\n\tvoid *src = skb->data;\n\tu32 frame_index = 0;\n\tbool unmap = false;\n\tvoid *dest;\n\n\ttrace_tbnet_tx_skb(skb);\n\n\tnframes = DIV_ROUND_UP(data_len, TBNET_MAX_PAYLOAD_SIZE);\n\tif (tbnet_available_buffers(&net->tx_ring) < nframes) {\n\t\tnetif_stop_queue(net->dev);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tframes[frame_index] = tbnet_get_tx_buffer(net);\n\tif (!frames[frame_index])\n\t\tgoto err_drop;\n\n\thdr = page_address(frames[frame_index]->page);\n\tdest = hdr + 1;\n\n\t \n\twhile (data_len > TBNET_MAX_PAYLOAD_SIZE) {\n\t\tunsigned int size_left = TBNET_MAX_PAYLOAD_SIZE;\n\n\t\thdr->frame_size = cpu_to_le32(TBNET_MAX_PAYLOAD_SIZE);\n\t\thdr->frame_index = cpu_to_le16(frame_index);\n\t\thdr->frame_id = cpu_to_le16(frame_id);\n\n\t\tdo {\n\t\t\tif (len > size_left) {\n\t\t\t\t \n\t\t\t\tmemcpy(dest, src, size_left);\n\t\t\t\tlen -= size_left;\n\t\t\t\tdest += size_left;\n\t\t\t\tsrc += size_left;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmemcpy(dest, src, len);\n\t\t\tsize_left -= len;\n\t\t\tdest += len;\n\n\t\t\tif (unmap) {\n\t\t\t\tkunmap_local(src);\n\t\t\t\tunmap = false;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (frag < skb_shinfo(skb)->nr_frags) {\n\t\t\t\t \n\t\t\t\tsrc = tbnet_kmap_frag(skb, frag++, &len);\n\t\t\t\tunmap = true;\n\t\t\t} else if (unlikely(size_left > 0)) {\n\t\t\t\tgoto err_drop;\n\t\t\t}\n\t\t} while (size_left > 0);\n\n\t\tdata_len -= TBNET_MAX_PAYLOAD_SIZE;\n\t\tframe_index++;\n\n\t\tframes[frame_index] = tbnet_get_tx_buffer(net);\n\t\tif (!frames[frame_index])\n\t\t\tgoto err_drop;\n\n\t\thdr = page_address(frames[frame_index]->page);\n\t\tdest = hdr + 1;\n\t}\n\n\thdr->frame_size = cpu_to_le32(data_len);\n\thdr->frame_index = cpu_to_le16(frame_index);\n\thdr->frame_id = cpu_to_le16(frame_id);\n\n\tframes[frame_index]->frame.size = data_len + sizeof(*hdr);\n\n\t \n\twhile (len < data_len) {\n\t\tmemcpy(dest, src, len);\n\t\tdata_len -= len;\n\t\tdest += len;\n\n\t\tif (unmap) {\n\t\t\tkunmap_local(src);\n\t\t\tunmap = false;\n\t\t}\n\n\t\tif (frag < skb_shinfo(skb)->nr_frags) {\n\t\t\tsrc = tbnet_kmap_frag(skb, frag++, &len);\n\t\t\tunmap = true;\n\t\t} else if (unlikely(data_len > 0)) {\n\t\t\tgoto err_drop;\n\t\t}\n\t}\n\n\tmemcpy(dest, src, data_len);\n\n\tif (unmap)\n\t\tkunmap_local(src);\n\n\tif (!tbnet_xmit_csum_and_map(net, skb, frames, frame_index + 1))\n\t\tgoto err_drop;\n\n\tfor (i = 0; i < frame_index + 1; i++)\n\t\ttb_ring_tx(net->tx_ring.ring, &frames[i]->frame);\n\n\tif (net->svc->prtcstns & TBNET_MATCH_FRAGS_ID)\n\t\tatomic_inc(&net->frame_id);\n\n\tnet->stats.tx_packets++;\n\tnet->stats.tx_bytes += skb->len;\n\n\ttrace_tbnet_consume_skb(skb);\n\tdev_consume_skb_any(skb);\n\n\treturn NETDEV_TX_OK;\n\nerr_drop:\n\t \n\tnet->tx_ring.cons -= frame_index;\n\n\tdev_kfree_skb_any(skb);\n\tnet->stats.tx_errors++;\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic void tbnet_get_stats64(struct net_device *dev,\n\t\t\t      struct rtnl_link_stats64 *stats)\n{\n\tstruct tbnet *net = netdev_priv(dev);\n\n\tstats->tx_packets = net->stats.tx_packets;\n\tstats->rx_packets = net->stats.rx_packets;\n\tstats->tx_bytes = net->stats.tx_bytes;\n\tstats->rx_bytes = net->stats.rx_bytes;\n\tstats->rx_errors = net->stats.rx_errors + net->stats.rx_length_errors +\n\t\tnet->stats.rx_over_errors + net->stats.rx_crc_errors +\n\t\tnet->stats.rx_missed_errors;\n\tstats->tx_errors = net->stats.tx_errors;\n\tstats->rx_length_errors = net->stats.rx_length_errors;\n\tstats->rx_over_errors = net->stats.rx_over_errors;\n\tstats->rx_crc_errors = net->stats.rx_crc_errors;\n\tstats->rx_missed_errors = net->stats.rx_missed_errors;\n}\n\nstatic const struct net_device_ops tbnet_netdev_ops = {\n\t.ndo_open = tbnet_open,\n\t.ndo_stop = tbnet_stop,\n\t.ndo_start_xmit = tbnet_start_xmit,\n\t.ndo_get_stats64 = tbnet_get_stats64,\n};\n\nstatic void tbnet_generate_mac(struct net_device *dev)\n{\n\tconst struct tbnet *net = netdev_priv(dev);\n\tconst struct tb_xdomain *xd = net->xd;\n\tu8 addr[ETH_ALEN];\n\tu8 phy_port;\n\tu32 hash;\n\n\tphy_port = tb_phy_port_from_link(TBNET_L0_PORT_NUM(xd->route));\n\n\t \n\taddr[0] = phy_port << 4 | 0x02;\n\thash = jhash2((u32 *)xd->local_uuid, 4, 0);\n\tmemcpy(addr + 1, &hash, sizeof(hash));\n\thash = jhash2((u32 *)xd->local_uuid, 4, hash);\n\taddr[5] = hash & 0xff;\n\teth_hw_addr_set(dev, addr);\n}\n\nstatic int tbnet_probe(struct tb_service *svc, const struct tb_service_id *id)\n{\n\tstruct tb_xdomain *xd = tb_service_parent(svc);\n\tstruct net_device *dev;\n\tstruct tbnet *net;\n\tint ret;\n\n\tdev = alloc_etherdev(sizeof(*net));\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tSET_NETDEV_DEV(dev, &svc->dev);\n\n\tnet = netdev_priv(dev);\n\tINIT_DELAYED_WORK(&net->login_work, tbnet_login_work);\n\tINIT_WORK(&net->connected_work, tbnet_connected_work);\n\tINIT_WORK(&net->disconnect_work, tbnet_disconnect_work);\n\tmutex_init(&net->connection_lock);\n\tatomic_set(&net->command_id, 0);\n\tatomic_set(&net->frame_id, 0);\n\tnet->svc = svc;\n\tnet->dev = dev;\n\tnet->xd = xd;\n\n\ttbnet_generate_mac(dev);\n\n\tstrcpy(dev->name, \"thunderbolt%d\");\n\tdev->netdev_ops = &tbnet_netdev_ops;\n\n\t \n\tdev->hw_features = NETIF_F_SG | NETIF_F_ALL_TSO | NETIF_F_GRO |\n\t\t\t   NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\n\tdev->features = dev->hw_features | NETIF_F_HIGHDMA;\n\tdev->hard_header_len += sizeof(struct thunderbolt_ip_frame_header);\n\n\tnetif_napi_add(dev, &net->napi, tbnet_poll);\n\n\t \n\tdev->min_mtu = ETH_MIN_MTU;\n\tdev->max_mtu = TBNET_MAX_MTU - ETH_HLEN;\n\n\tnet->handler.uuid = &tbnet_svc_uuid;\n\tnet->handler.callback = tbnet_handle_packet;\n\tnet->handler.data = net;\n\ttb_register_protocol_handler(&net->handler);\n\n\ttb_service_set_drvdata(svc, net);\n\n\tret = register_netdev(dev);\n\tif (ret) {\n\t\ttb_unregister_protocol_handler(&net->handler);\n\t\tfree_netdev(dev);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void tbnet_remove(struct tb_service *svc)\n{\n\tstruct tbnet *net = tb_service_get_drvdata(svc);\n\n\tunregister_netdev(net->dev);\n\ttb_unregister_protocol_handler(&net->handler);\n\tfree_netdev(net->dev);\n}\n\nstatic void tbnet_shutdown(struct tb_service *svc)\n{\n\ttbnet_tear_down(tb_service_get_drvdata(svc), true);\n}\n\nstatic int tbnet_suspend(struct device *dev)\n{\n\tstruct tb_service *svc = tb_to_service(dev);\n\tstruct tbnet *net = tb_service_get_drvdata(svc);\n\n\tstop_login(net);\n\tif (netif_running(net->dev)) {\n\t\tnetif_device_detach(net->dev);\n\t\ttbnet_tear_down(net, true);\n\t}\n\n\ttb_unregister_protocol_handler(&net->handler);\n\treturn 0;\n}\n\nstatic int tbnet_resume(struct device *dev)\n{\n\tstruct tb_service *svc = tb_to_service(dev);\n\tstruct tbnet *net = tb_service_get_drvdata(svc);\n\n\ttb_register_protocol_handler(&net->handler);\n\n\tnetif_carrier_off(net->dev);\n\tif (netif_running(net->dev)) {\n\t\tnetif_device_attach(net->dev);\n\t\tstart_login(net);\n\t}\n\n\treturn 0;\n}\n\nstatic DEFINE_SIMPLE_DEV_PM_OPS(tbnet_pm_ops, tbnet_suspend, tbnet_resume);\n\nstatic const struct tb_service_id tbnet_ids[] = {\n\t{ TB_SERVICE(\"network\", 1) },\n\t{ },\n};\nMODULE_DEVICE_TABLE(tbsvc, tbnet_ids);\n\nstatic struct tb_service_driver tbnet_driver = {\n\t.driver = {\n\t\t.owner = THIS_MODULE,\n\t\t.name = \"thunderbolt-net\",\n\t\t.pm = pm_sleep_ptr(&tbnet_pm_ops),\n\t},\n\t.probe = tbnet_probe,\n\t.remove = tbnet_remove,\n\t.shutdown = tbnet_shutdown,\n\t.id_table = tbnet_ids,\n};\n\nstatic int __init tbnet_init(void)\n{\n\tunsigned int flags;\n\tint ret;\n\n\ttbnet_dir = tb_property_create_dir(&tbnet_dir_uuid);\n\tif (!tbnet_dir)\n\t\treturn -ENOMEM;\n\n\ttb_property_add_immediate(tbnet_dir, \"prtcid\", 1);\n\ttb_property_add_immediate(tbnet_dir, \"prtcvers\", 1);\n\ttb_property_add_immediate(tbnet_dir, \"prtcrevs\", 1);\n\n\tflags = TBNET_MATCH_FRAGS_ID | TBNET_64K_FRAMES;\n\tif (tbnet_e2e)\n\t\tflags |= TBNET_E2E;\n\ttb_property_add_immediate(tbnet_dir, \"prtcstns\", flags);\n\n\tret = tb_register_property_dir(\"network\", tbnet_dir);\n\tif (ret)\n\t\tgoto err_free_dir;\n\n\tret = tb_register_service_driver(&tbnet_driver);\n\tif (ret)\n\t\tgoto err_unregister;\n\n\treturn 0;\n\nerr_unregister:\n\ttb_unregister_property_dir(\"network\", tbnet_dir);\nerr_free_dir:\n\ttb_property_free_dir(tbnet_dir);\n\n\treturn ret;\n}\nmodule_init(tbnet_init);\n\nstatic void __exit tbnet_exit(void)\n{\n\ttb_unregister_service_driver(&tbnet_driver);\n\ttb_unregister_property_dir(\"network\", tbnet_dir);\n\ttb_property_free_dir(tbnet_dir);\n}\nmodule_exit(tbnet_exit);\n\nMODULE_AUTHOR(\"Amir Levy <amir.jer.levy@intel.com>\");\nMODULE_AUTHOR(\"Michael Jamet <michael.jamet@intel.com>\");\nMODULE_AUTHOR(\"Mika Westerberg <mika.westerberg@linux.intel.com>\");\nMODULE_DESCRIPTION(\"Thunderbolt/USB4 network driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}