{
  "module_name": "ipa_mem.c",
  "hash_id": "9adbdbbdf13143610ab2d19ab00d773b090712ef6da726dd86664bc6afb4d5ca",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ipa/ipa_mem.c",
  "human_readable_source": "\n\n \n\n#include <linux/types.h>\n#include <linux/bitfield.h>\n#include <linux/bug.h>\n#include <linux/dma-mapping.h>\n#include <linux/iommu.h>\n#include <linux/io.h>\n#include <linux/soc/qcom/smem.h>\n\n#include \"ipa.h\"\n#include \"ipa_reg.h\"\n#include \"ipa_data.h\"\n#include \"ipa_cmd.h\"\n#include \"ipa_mem.h\"\n#include \"ipa_table.h\"\n#include \"gsi_trans.h\"\n\n \n#define IPA_MEM_CANARY_VAL\t\tcpu_to_le32(0xdeadbeef)\n\n \n#define QCOM_SMEM_HOST_MODEM\t1\n\nconst struct ipa_mem *ipa_mem_find(struct ipa *ipa, enum ipa_mem_id mem_id)\n{\n\tu32 i;\n\n\tfor (i = 0; i < ipa->mem_count; i++) {\n\t\tconst struct ipa_mem *mem = &ipa->mem[i];\n\n\t\tif (mem->id == mem_id)\n\t\t\treturn mem;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void\nipa_mem_zero_region_add(struct gsi_trans *trans, enum ipa_mem_id mem_id)\n{\n\tstruct ipa *ipa = container_of(trans->gsi, struct ipa, gsi);\n\tconst struct ipa_mem *mem = ipa_mem_find(ipa, mem_id);\n\tdma_addr_t addr = ipa->zero_addr;\n\n\tif (!mem->size)\n\t\treturn;\n\n\tipa_cmd_dma_shared_mem_add(trans, mem->offset, mem->size, addr, true);\n}\n\n \nint ipa_mem_setup(struct ipa *ipa)\n{\n\tdma_addr_t addr = ipa->zero_addr;\n\tconst struct reg *reg;\n\tconst struct ipa_mem *mem;\n\tstruct gsi_trans *trans;\n\tu32 offset;\n\tu16 size;\n\tu32 val;\n\n\t \n\ttrans = ipa_cmd_trans_alloc(ipa, 4);\n\tif (!trans) {\n\t\tdev_err(&ipa->pdev->dev, \"no transaction for memory setup\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tmem = ipa_mem_find(ipa, IPA_MEM_MODEM_HEADER);\n\toffset = mem->offset;\n\tsize = mem->size;\n\tmem = ipa_mem_find(ipa, IPA_MEM_AP_HEADER);\n\tif (mem)\n\t\tsize += mem->size;\n\n\tipa_cmd_hdr_init_local_add(trans, offset, size, addr);\n\n\tipa_mem_zero_region_add(trans, IPA_MEM_MODEM_PROC_CTX);\n\tipa_mem_zero_region_add(trans, IPA_MEM_AP_PROC_CTX);\n\tipa_mem_zero_region_add(trans, IPA_MEM_MODEM);\n\n\tgsi_trans_commit_wait(trans);\n\n\t \n\tmem = ipa_mem_find(ipa, IPA_MEM_MODEM_PROC_CTX);\n\toffset = ipa->mem_offset + mem->offset;\n\n\treg = ipa_reg(ipa, LOCAL_PKT_PROC_CNTXT);\n\tval = reg_encode(reg, IPA_BASE_ADDR, offset);\n\tiowrite32(val, ipa->reg_virt + reg_offset(reg));\n\n\treturn 0;\n}\n\n \nstatic bool ipa_mem_id_valid(struct ipa *ipa, enum ipa_mem_id mem_id)\n{\n\tenum ipa_version version = ipa->version;\n\n\tswitch (mem_id) {\n\tcase IPA_MEM_UC_SHARED:\n\tcase IPA_MEM_UC_INFO:\n\tcase IPA_MEM_V4_FILTER_HASHED:\n\tcase IPA_MEM_V4_FILTER:\n\tcase IPA_MEM_V6_FILTER_HASHED:\n\tcase IPA_MEM_V6_FILTER:\n\tcase IPA_MEM_V4_ROUTE_HASHED:\n\tcase IPA_MEM_V4_ROUTE:\n\tcase IPA_MEM_V6_ROUTE_HASHED:\n\tcase IPA_MEM_V6_ROUTE:\n\tcase IPA_MEM_MODEM_HEADER:\n\tcase IPA_MEM_AP_HEADER:\n\tcase IPA_MEM_MODEM_PROC_CTX:\n\tcase IPA_MEM_AP_PROC_CTX:\n\tcase IPA_MEM_MODEM:\n\tcase IPA_MEM_UC_EVENT_RING:\n\tcase IPA_MEM_PDN_CONFIG:\n\tcase IPA_MEM_STATS_QUOTA_MODEM:\n\tcase IPA_MEM_STATS_QUOTA_AP:\n\tcase IPA_MEM_END_MARKER:\t \n\t\tbreak;\n\n\tcase IPA_MEM_STATS_TETHERING:\n\tcase IPA_MEM_STATS_DROP:\n\t\tif (version < IPA_VERSION_4_0)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase IPA_MEM_STATS_V4_FILTER:\n\tcase IPA_MEM_STATS_V6_FILTER:\n\tcase IPA_MEM_STATS_V4_ROUTE:\n\tcase IPA_MEM_STATS_V6_ROUTE:\n\t\tif (version < IPA_VERSION_4_0 || version > IPA_VERSION_4_2)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase IPA_MEM_AP_V4_FILTER:\n\tcase IPA_MEM_AP_V6_FILTER:\n\t\tif (version != IPA_VERSION_5_0)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase IPA_MEM_NAT_TABLE:\n\tcase IPA_MEM_STATS_FILTER_ROUTE:\n\t\tif (version < IPA_VERSION_4_5)\n\t\t\treturn false;\n\t\tbreak;\n\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic bool ipa_mem_id_required(struct ipa *ipa, enum ipa_mem_id mem_id)\n{\n\tswitch (mem_id) {\n\tcase IPA_MEM_UC_SHARED:\n\tcase IPA_MEM_UC_INFO:\n\tcase IPA_MEM_V4_FILTER_HASHED:\n\tcase IPA_MEM_V4_FILTER:\n\tcase IPA_MEM_V6_FILTER_HASHED:\n\tcase IPA_MEM_V6_FILTER:\n\tcase IPA_MEM_V4_ROUTE_HASHED:\n\tcase IPA_MEM_V4_ROUTE:\n\tcase IPA_MEM_V6_ROUTE_HASHED:\n\tcase IPA_MEM_V6_ROUTE:\n\tcase IPA_MEM_MODEM_HEADER:\n\tcase IPA_MEM_MODEM_PROC_CTX:\n\tcase IPA_MEM_AP_PROC_CTX:\n\tcase IPA_MEM_MODEM:\n\t\treturn true;\n\n\tcase IPA_MEM_PDN_CONFIG:\n\tcase IPA_MEM_STATS_QUOTA_MODEM:\n\t\treturn ipa->version >= IPA_VERSION_4_0;\n\n\tcase IPA_MEM_STATS_TETHERING:\n\t\treturn ipa->version >= IPA_VERSION_4_0 &&\n\t\t\tipa->version != IPA_VERSION_5_0;\n\n\tdefault:\n\t\treturn false;\t\t \n\t}\n}\n\nstatic bool ipa_mem_valid_one(struct ipa *ipa, const struct ipa_mem *mem)\n{\n\tstruct device *dev = &ipa->pdev->dev;\n\tenum ipa_mem_id mem_id = mem->id;\n\tu16 size_multiple;\n\n\t \n\tif (!ipa_mem_id_valid(ipa, mem_id)) {\n\t\tdev_err(dev, \"region id %u not valid\\n\", mem_id);\n\t\treturn false;\n\t}\n\n\tif (!mem->size && !mem->canary_count) {\n\t\tdev_err(dev, \"empty memory region %u\\n\", mem_id);\n\t\treturn false;\n\t}\n\n\t \n\tsize_multiple = mem_id == IPA_MEM_MODEM ? 4 : 8;\n\tif (mem->size % size_multiple)\n\t\tdev_err(dev, \"region %u size not a multiple of %u bytes\\n\",\n\t\t\tmem_id, size_multiple);\n\telse if (mem->offset % 8)\n\t\tdev_err(dev, \"region %u offset not 8-byte aligned\\n\", mem_id);\n\telse if (mem->offset < mem->canary_count * sizeof(__le32))\n\t\tdev_err(dev, \"region %u offset too small for %hu canaries\\n\",\n\t\t\tmem_id, mem->canary_count);\n\telse if (mem_id == IPA_MEM_END_MARKER && mem->size)\n\t\tdev_err(dev, \"non-zero end marker region size\\n\");\n\telse\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool ipa_mem_valid(struct ipa *ipa, const struct ipa_mem_data *mem_data)\n{\n\tDECLARE_BITMAP(regions, IPA_MEM_COUNT) = { };\n\tstruct device *dev = &ipa->pdev->dev;\n\tenum ipa_mem_id mem_id;\n\tu32 i;\n\n\tif (mem_data->local_count > IPA_MEM_COUNT) {\n\t\tdev_err(dev, \"too many memory regions (%u > %u)\\n\",\n\t\t\tmem_data->local_count, IPA_MEM_COUNT);\n\t\treturn false;\n\t}\n\n\tfor (i = 0; i < mem_data->local_count; i++) {\n\t\tconst struct ipa_mem *mem = &mem_data->local[i];\n\n\t\tif (__test_and_set_bit(mem->id, regions)) {\n\t\t\tdev_err(dev, \"duplicate memory region %u\\n\", mem->id);\n\t\t\treturn false;\n\t\t}\n\n\t\t \n\t\tif (!ipa_mem_valid_one(ipa, mem))\n\t\t\treturn false;\n\t}\n\n\t \n\tfor_each_clear_bit(mem_id, regions, IPA_MEM_COUNT) {\n\t\tif (ipa_mem_id_required(ipa, mem_id))\n\t\t\tdev_err(dev, \"required memory region %u missing\\n\",\n\t\t\t\tmem_id);\n\t}\n\n\treturn true;\n}\n\n \nstatic bool ipa_mem_size_valid(struct ipa *ipa)\n{\n\tstruct device *dev = &ipa->pdev->dev;\n\tu32 limit = ipa->mem_size;\n\tu32 i;\n\n\tfor (i = 0; i < ipa->mem_count; i++) {\n\t\tconst struct ipa_mem *mem = &ipa->mem[i];\n\n\t\tif (mem->offset + mem->size <= limit)\n\t\t\tcontinue;\n\n\t\tdev_err(dev, \"region %u ends beyond memory limit (0x%08x)\\n\",\n\t\t\tmem->id, limit);\n\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nint ipa_mem_config(struct ipa *ipa)\n{\n\tstruct device *dev = &ipa->pdev->dev;\n\tconst struct ipa_mem *mem;\n\tconst struct reg *reg;\n\tdma_addr_t addr;\n\tu32 mem_size;\n\tvoid *virt;\n\tu32 val;\n\tu32 i;\n\n\t \n\treg = ipa_reg(ipa, SHARED_MEM_SIZE);\n\tval = ioread32(ipa->reg_virt + reg_offset(reg));\n\n\t \n\tipa->mem_offset = 8 * reg_decode(reg, MEM_BADDR, val);\n\n\t \n\tmem_size = 8 * reg_decode(reg, MEM_SIZE, val);\n\n\t \n\tif (ipa->mem_offset + mem_size < ipa->mem_size) {\n\t\tdev_warn(dev, \"limiting IPA memory size to 0x%08x\\n\",\n\t\t\t mem_size);\n\t\tipa->mem_size = mem_size;\n\t} else if (ipa->mem_offset + mem_size > ipa->mem_size) {\n\t\tdev_dbg(dev, \"ignoring larger reported memory size: 0x%08x\\n\",\n\t\t\tmem_size);\n\t}\n\n\t \n\tif (!ipa_mem_size_valid(ipa))\n\t\treturn -EINVAL;\n\n\t \n\tvirt = dma_alloc_coherent(dev, IPA_MEM_MAX, &addr, GFP_KERNEL);\n\tif (!virt)\n\t\treturn -ENOMEM;\n\tipa->zero_addr = addr;\n\tipa->zero_virt = virt;\n\tipa->zero_size = IPA_MEM_MAX;\n\n\t \n\tfor (i = 0; i < ipa->mem_count; i++) {\n\t\tu16 canary_count = ipa->mem[i].canary_count;\n\t\t__le32 *canary;\n\n\t\tif (!canary_count)\n\t\t\tcontinue;\n\n\t\t \n\t\tcanary = ipa->mem_virt + ipa->mem_offset + ipa->mem[i].offset;\n\t\tdo\n\t\t\t*--canary = IPA_MEM_CANARY_VAL;\n\t\twhile (--canary_count);\n\t}\n\n\t \n\tmem = ipa_mem_find(ipa, IPA_MEM_UC_EVENT_RING);\n\tif (mem && mem->offset % 1024) {\n\t\tdev_err(dev, \"microcontroller ring not 1024-byte aligned\\n\");\n\t\tgoto err_dma_free;\n\t}\n\n\treturn 0;\n\nerr_dma_free:\n\tdma_free_coherent(dev, IPA_MEM_MAX, ipa->zero_virt, ipa->zero_addr);\n\n\treturn -EINVAL;\n}\n\n \nvoid ipa_mem_deconfig(struct ipa *ipa)\n{\n\tstruct device *dev = &ipa->pdev->dev;\n\n\tdma_free_coherent(dev, ipa->zero_size, ipa->zero_virt, ipa->zero_addr);\n\tipa->zero_size = 0;\n\tipa->zero_virt = NULL;\n\tipa->zero_addr = 0;\n}\n\n \nint ipa_mem_zero_modem(struct ipa *ipa)\n{\n\tstruct gsi_trans *trans;\n\n\t \n\ttrans = ipa_cmd_trans_alloc(ipa, 3);\n\tif (!trans) {\n\t\tdev_err(&ipa->pdev->dev,\n\t\t\t\"no transaction to zero modem memory\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tipa_mem_zero_region_add(trans, IPA_MEM_MODEM_HEADER);\n\tipa_mem_zero_region_add(trans, IPA_MEM_MODEM_PROC_CTX);\n\tipa_mem_zero_region_add(trans, IPA_MEM_MODEM);\n\n\tgsi_trans_commit_wait(trans);\n\n\treturn 0;\n}\n\n \nstatic int ipa_imem_init(struct ipa *ipa, unsigned long addr, size_t size)\n{\n\tstruct device *dev = &ipa->pdev->dev;\n\tstruct iommu_domain *domain;\n\tunsigned long iova;\n\tphys_addr_t phys;\n\tint ret;\n\n\tif (!size)\n\t\treturn 0;\t \n\n\tdomain = iommu_get_domain_for_dev(dev);\n\tif (!domain) {\n\t\tdev_err(dev, \"no IOMMU domain found for IMEM\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tphys = addr & PAGE_MASK;\n\tsize = PAGE_ALIGN(size + addr - phys);\n\tiova = phys;\t \n\n\tret = iommu_map(domain, iova, phys, size, IOMMU_READ | IOMMU_WRITE,\n\t\t\tGFP_KERNEL);\n\tif (ret)\n\t\treturn ret;\n\n\tipa->imem_iova = iova;\n\tipa->imem_size = size;\n\n\treturn 0;\n}\n\nstatic void ipa_imem_exit(struct ipa *ipa)\n{\n\tstruct iommu_domain *domain;\n\tstruct device *dev;\n\n\tif (!ipa->imem_size)\n\t\treturn;\n\n\tdev = &ipa->pdev->dev;\n\tdomain = iommu_get_domain_for_dev(dev);\n\tif (domain) {\n\t\tsize_t size;\n\n\t\tsize = iommu_unmap(domain, ipa->imem_iova, ipa->imem_size);\n\t\tif (size != ipa->imem_size)\n\t\t\tdev_warn(dev, \"unmapped %zu IMEM bytes, expected %zu\\n\",\n\t\t\t\t size, ipa->imem_size);\n\t} else {\n\t\tdev_err(dev, \"couldn't get IPA IOMMU domain for IMEM\\n\");\n\t}\n\n\tipa->imem_size = 0;\n\tipa->imem_iova = 0;\n}\n\n \nstatic int ipa_smem_init(struct ipa *ipa, u32 item, size_t size)\n{\n\tstruct device *dev = &ipa->pdev->dev;\n\tstruct iommu_domain *domain;\n\tunsigned long iova;\n\tphys_addr_t phys;\n\tphys_addr_t addr;\n\tsize_t actual;\n\tvoid *virt;\n\tint ret;\n\n\tif (!size)\n\t\treturn 0;\t \n\n\t \n\tret = qcom_smem_alloc(QCOM_SMEM_HOST_MODEM, item, size);\n\tif (ret && ret != -EEXIST) {\n\t\tdev_err(dev, \"error %d allocating size %zu SMEM item %u\\n\",\n\t\t\tret, size, item);\n\t\treturn ret;\n\t}\n\n\t \n\tvirt = qcom_smem_get(QCOM_SMEM_HOST_MODEM, item, &actual);\n\tif (IS_ERR(virt)) {\n\t\tret = PTR_ERR(virt);\n\t\tdev_err(dev, \"error %d getting SMEM item %u\\n\", ret, item);\n\t\treturn ret;\n\t}\n\n\t \n\tif (ret && actual != size) {\n\t\tdev_err(dev, \"SMEM item %u has size %zu, expected %zu\\n\",\n\t\t\titem, actual, size);\n\t\treturn -EINVAL;\n\t}\n\n\tdomain = iommu_get_domain_for_dev(dev);\n\tif (!domain) {\n\t\tdev_err(dev, \"no IOMMU domain found for SMEM\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\taddr = qcom_smem_virt_to_phys(virt);\n\tphys = addr & PAGE_MASK;\n\tsize = PAGE_ALIGN(size + addr - phys);\n\tiova = phys;\t \n\n\tret = iommu_map(domain, iova, phys, size, IOMMU_READ | IOMMU_WRITE,\n\t\t\tGFP_KERNEL);\n\tif (ret)\n\t\treturn ret;\n\n\tipa->smem_iova = iova;\n\tipa->smem_size = size;\n\n\treturn 0;\n}\n\nstatic void ipa_smem_exit(struct ipa *ipa)\n{\n\tstruct device *dev = &ipa->pdev->dev;\n\tstruct iommu_domain *domain;\n\n\tdomain = iommu_get_domain_for_dev(dev);\n\tif (domain) {\n\t\tsize_t size;\n\n\t\tsize = iommu_unmap(domain, ipa->smem_iova, ipa->smem_size);\n\t\tif (size != ipa->smem_size)\n\t\t\tdev_warn(dev, \"unmapped %zu SMEM bytes, expected %zu\\n\",\n\t\t\t\t size, ipa->smem_size);\n\n\t} else {\n\t\tdev_err(dev, \"couldn't get IPA IOMMU domain for SMEM\\n\");\n\t}\n\n\tipa->smem_size = 0;\n\tipa->smem_iova = 0;\n}\n\n \nint ipa_mem_init(struct ipa *ipa, const struct ipa_mem_data *mem_data)\n{\n\tstruct device *dev = &ipa->pdev->dev;\n\tstruct resource *res;\n\tint ret;\n\n\t \n\tif (!ipa_mem_valid(ipa, mem_data))\n\t\treturn -EINVAL;\n\n\tipa->mem_count = mem_data->local_count;\n\tipa->mem = mem_data->local;\n\n\t \n\tif (!ipa_table_mem_valid(ipa, false))\n\t\treturn -EINVAL;\n\tif (!ipa_table_mem_valid(ipa, true))\n\t\treturn -EINVAL;\n\n\tret = dma_set_mask_and_coherent(&ipa->pdev->dev, DMA_BIT_MASK(64));\n\tif (ret) {\n\t\tdev_err(dev, \"error %d setting DMA mask\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tres = platform_get_resource_byname(ipa->pdev, IORESOURCE_MEM,\n\t\t\t\t\t   \"ipa-shared\");\n\tif (!res) {\n\t\tdev_err(dev,\n\t\t\t\"DT error getting \\\"ipa-shared\\\" memory property\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tipa->mem_virt = memremap(res->start, resource_size(res), MEMREMAP_WC);\n\tif (!ipa->mem_virt) {\n\t\tdev_err(dev, \"unable to remap \\\"ipa-shared\\\" memory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tipa->mem_addr = res->start;\n\tipa->mem_size = resource_size(res);\n\n\tret = ipa_imem_init(ipa, mem_data->imem_addr, mem_data->imem_size);\n\tif (ret)\n\t\tgoto err_unmap;\n\n\tret = ipa_smem_init(ipa, mem_data->smem_id, mem_data->smem_size);\n\tif (ret)\n\t\tgoto err_imem_exit;\n\n\treturn 0;\n\nerr_imem_exit:\n\tipa_imem_exit(ipa);\nerr_unmap:\n\tmemunmap(ipa->mem_virt);\n\n\treturn ret;\n}\n\n \nvoid ipa_mem_exit(struct ipa *ipa)\n{\n\tipa_smem_exit(ipa);\n\tipa_imem_exit(ipa);\n\tmemunmap(ipa->mem_virt);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}