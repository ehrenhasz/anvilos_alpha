{
  "module_name": "ipa_endpoint.c",
  "hash_id": "35129803156420f305f81b0a9ebd6d0155f6278c1afc5f65130c567e6ebe7b5d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ipa/ipa_endpoint.c",
  "human_readable_source": "\n\n \n\n#include <linux/types.h>\n#include <linux/device.h>\n#include <linux/slab.h>\n#include <linux/bitfield.h>\n#include <linux/if_rmnet.h>\n#include <linux/dma-direction.h>\n\n#include \"gsi.h\"\n#include \"gsi_trans.h\"\n#include \"ipa.h\"\n#include \"ipa_data.h\"\n#include \"ipa_endpoint.h\"\n#include \"ipa_cmd.h\"\n#include \"ipa_mem.h\"\n#include \"ipa_modem.h\"\n#include \"ipa_table.h\"\n#include \"ipa_gsi.h\"\n#include \"ipa_power.h\"\n\n \n#define IPA_REPLENISH_BATCH\t16\t\t \n\n \n#define IPA_RX_BUFFER_OVERHEAD\t(PAGE_SIZE - SKB_MAX_ORDER(NET_SKB_PAD, 0))\n\n \n#define IPA_ENDPOINT_QMAP_METADATA_MASK\t\t0x000000ff  \n\n#define IPA_ENDPOINT_RESET_AGGR_RETRY_MAX\t3\n\n \nenum ipa_status_opcode {\t\t\t\t \n\tIPA_STATUS_OPCODE_PACKET\t\t= 1,\n\tIPA_STATUS_OPCODE_NEW_RULE_PACKET\t= 2,\n\tIPA_STATUS_OPCODE_DROPPED_PACKET\t= 4,\n\tIPA_STATUS_OPCODE_SUSPENDED_PACKET\t= 8,\n\tIPA_STATUS_OPCODE_LOG\t\t\t= 16,\n\tIPA_STATUS_OPCODE_DCMP\t\t\t= 32,\n\tIPA_STATUS_OPCODE_PACKET_2ND_PASS\t= 64,\n};\n\n \nenum ipa_status_exception {\t\t\t\t \n\t \n\tIPA_STATUS_EXCEPTION_DEAGGR\t\t= 1,\n\tIPA_STATUS_EXCEPTION_IPTYPE\t\t= 4,\n\tIPA_STATUS_EXCEPTION_PACKET_LENGTH\t= 8,\n\tIPA_STATUS_EXCEPTION_FRAG_RULE_MISS\t= 16,\n\tIPA_STATUS_EXCEPTION_SW_FILTER\t\t= 32,\n\tIPA_STATUS_EXCEPTION_NAT\t\t= 64,\t\t \n\tIPA_STATUS_EXCEPTION_IPV6_CONN_TRACK\t= 64,\t\t \n\tIPA_STATUS_EXCEPTION_UC\t\t\t= 128,\n\tIPA_STATUS_EXCEPTION_INVALID_ENDPOINT\t= 129,\n\tIPA_STATUS_EXCEPTION_HEADER_INSERT\t= 136,\n\tIPA_STATUS_EXCEPTION_CHEKCSUM\t\t= 229,\n};\n\n \nenum ipa_status_mask {\n\tIPA_STATUS_MASK_FRAG_PROCESS\t\t= BIT(0),\n\tIPA_STATUS_MASK_FILT_PROCESS\t\t= BIT(1),\n\tIPA_STATUS_MASK_NAT_PROCESS\t\t= BIT(2),\n\tIPA_STATUS_MASK_ROUTE_PROCESS\t\t= BIT(3),\n\tIPA_STATUS_MASK_TAG_VALID\t\t= BIT(4),\n\tIPA_STATUS_MASK_FRAGMENT\t\t= BIT(5),\n\tIPA_STATUS_MASK_FIRST_FRAGMENT\t\t= BIT(6),\n\tIPA_STATUS_MASK_V4\t\t\t= BIT(7),\n\tIPA_STATUS_MASK_CKSUM_PROCESS\t\t= BIT(8),\n\tIPA_STATUS_MASK_AGGR_PROCESS\t\t= BIT(9),\n\tIPA_STATUS_MASK_DEST_EOT\t\t= BIT(10),\n\tIPA_STATUS_MASK_DEAGGR_PROCESS\t\t= BIT(11),\n\tIPA_STATUS_MASK_DEAGG_FIRST\t\t= BIT(12),\n\tIPA_STATUS_MASK_SRC_EOT\t\t\t= BIT(13),\n\tIPA_STATUS_MASK_PREV_EOT\t\t= BIT(14),\n\tIPA_STATUS_MASK_BYTE_LIMIT\t\t= BIT(15),\n};\n\n \n#define IPA_STATUS_RULE_MISS\t0x3ff\t \n\n \n\n \nenum ipa_status_field_id {\n\tSTATUS_OPCODE,\t\t\t \n\tSTATUS_EXCEPTION,\t\t \n\tSTATUS_MASK,\t\t\t \n\tSTATUS_LENGTH,\n\tSTATUS_SRC_ENDPOINT,\n\tSTATUS_DST_ENDPOINT,\n\tSTATUS_METADATA,\n\tSTATUS_FILTER_LOCAL,\t\t \n\tSTATUS_FILTER_HASH,\t\t \n\tSTATUS_FILTER_GLOBAL,\t\t \n\tSTATUS_FILTER_RETAIN,\t\t \n\tSTATUS_FILTER_RULE_INDEX,\n\tSTATUS_ROUTER_LOCAL,\t\t \n\tSTATUS_ROUTER_HASH,\t\t \n\tSTATUS_UCP,\t\t\t \n\tSTATUS_ROUTER_TABLE,\n\tSTATUS_ROUTER_RULE_INDEX,\n\tSTATUS_NAT_HIT,\t\t\t \n\tSTATUS_NAT_INDEX,\n\tSTATUS_NAT_TYPE,\t\t \n\tSTATUS_TAG_LOW32,\t\t \n\tSTATUS_TAG_HIGH16,\t\t \n\tSTATUS_SEQUENCE,\n\tSTATUS_TIME_OF_DAY,\n\tSTATUS_HEADER_LOCAL,\t\t \n\tSTATUS_HEADER_OFFSET,\n\tSTATUS_FRAG_HIT,\t\t \n\tSTATUS_FRAG_RULE_INDEX,\n};\n\n \n#define IPA_STATUS_SIZE\t\t\tsizeof(__le32[8])\n\n \nstatic u32 ipa_status_extract(struct ipa *ipa, const void *data,\n\t\t\t      enum ipa_status_field_id field)\n{\n\tenum ipa_version version = ipa->version;\n\tconst __le32 *word = data;\n\n\tswitch (field) {\n\tcase STATUS_OPCODE:\n\t\treturn le32_get_bits(word[0], GENMASK(7, 0));\n\tcase STATUS_EXCEPTION:\n\t\treturn le32_get_bits(word[0], GENMASK(15, 8));\n\tcase STATUS_MASK:\n\t\treturn le32_get_bits(word[0], GENMASK(31, 16));\n\tcase STATUS_LENGTH:\n\t\treturn le32_get_bits(word[1], GENMASK(15, 0));\n\tcase STATUS_SRC_ENDPOINT:\n\t\tif (version < IPA_VERSION_5_0)\n\t\t\treturn le32_get_bits(word[1], GENMASK(20, 16));\n\t\treturn le32_get_bits(word[1], GENMASK(23, 16));\n\t \n\t \n\tcase STATUS_DST_ENDPOINT:\n\t\tif (version < IPA_VERSION_5_0)\n\t\t\treturn le32_get_bits(word[1], GENMASK(28, 24));\n\t\treturn le32_get_bits(word[7], GENMASK(23, 16));\n\t \n\tcase STATUS_METADATA:\n\t\treturn le32_to_cpu(word[2]);\n\tcase STATUS_FILTER_LOCAL:\n\t\treturn le32_get_bits(word[3], GENMASK(0, 0));\n\tcase STATUS_FILTER_HASH:\n\t\treturn le32_get_bits(word[3], GENMASK(1, 1));\n\tcase STATUS_FILTER_GLOBAL:\n\t\treturn le32_get_bits(word[3], GENMASK(2, 2));\n\tcase STATUS_FILTER_RETAIN:\n\t\treturn le32_get_bits(word[3], GENMASK(3, 3));\n\tcase STATUS_FILTER_RULE_INDEX:\n\t\treturn le32_get_bits(word[3], GENMASK(13, 4));\n\t \n\tcase STATUS_ROUTER_LOCAL:\n\t\tif (version < IPA_VERSION_5_0)\n\t\t\treturn le32_get_bits(word[3], GENMASK(14, 14));\n\t\treturn le32_get_bits(word[1], GENMASK(27, 27));\n\tcase STATUS_ROUTER_HASH:\n\t\tif (version < IPA_VERSION_5_0)\n\t\t\treturn le32_get_bits(word[3], GENMASK(15, 15));\n\t\treturn le32_get_bits(word[1], GENMASK(28, 28));\n\tcase STATUS_UCP:\n\t\tif (version < IPA_VERSION_5_0)\n\t\t\treturn le32_get_bits(word[3], GENMASK(16, 16));\n\t\treturn le32_get_bits(word[7], GENMASK(31, 31));\n\tcase STATUS_ROUTER_TABLE:\n\t\tif (version < IPA_VERSION_5_0)\n\t\t\treturn le32_get_bits(word[3], GENMASK(21, 17));\n\t\treturn le32_get_bits(word[3], GENMASK(21, 14));\n\tcase STATUS_ROUTER_RULE_INDEX:\n\t\treturn le32_get_bits(word[3], GENMASK(31, 22));\n\tcase STATUS_NAT_HIT:\n\t\treturn le32_get_bits(word[4], GENMASK(0, 0));\n\tcase STATUS_NAT_INDEX:\n\t\treturn le32_get_bits(word[4], GENMASK(13, 1));\n\tcase STATUS_NAT_TYPE:\n\t\treturn le32_get_bits(word[4], GENMASK(15, 14));\n\tcase STATUS_TAG_LOW32:\n\t\treturn le32_get_bits(word[4], GENMASK(31, 16)) |\n\t\t\t(le32_get_bits(word[5], GENMASK(15, 0)) << 16);\n\tcase STATUS_TAG_HIGH16:\n\t\treturn le32_get_bits(word[5], GENMASK(31, 16));\n\tcase STATUS_SEQUENCE:\n\t\treturn le32_get_bits(word[6], GENMASK(7, 0));\n\tcase STATUS_TIME_OF_DAY:\n\t\treturn le32_get_bits(word[6], GENMASK(31, 8));\n\tcase STATUS_HEADER_LOCAL:\n\t\treturn le32_get_bits(word[7], GENMASK(0, 0));\n\tcase STATUS_HEADER_OFFSET:\n\t\treturn le32_get_bits(word[7], GENMASK(10, 1));\n\tcase STATUS_FRAG_HIT:\n\t\treturn le32_get_bits(word[7], GENMASK(11, 11));\n\tcase STATUS_FRAG_RULE_INDEX:\n\t\treturn le32_get_bits(word[7], GENMASK(15, 12));\n\t \n\t \n\tdefault:\n\t\tWARN(true, \"%s: bad field_id %u\\n\", __func__, field);\n\t\treturn 0;\n\t}\n}\n\n \nstatic u32 ipa_aggr_size_kb(u32 rx_buffer_size, bool aggr_hard_limit)\n{\n\t \n\tif (!aggr_hard_limit)\n\t\trx_buffer_size -= IPA_MTU + IPA_RX_BUFFER_OVERHEAD;\n\n\t \n\n\treturn rx_buffer_size / SZ_1K;\n}\n\nstatic bool ipa_endpoint_data_valid_one(struct ipa *ipa, u32 count,\n\t\t\t    const struct ipa_gsi_endpoint_data *all_data,\n\t\t\t    const struct ipa_gsi_endpoint_data *data)\n{\n\tconst struct ipa_gsi_endpoint_data *other_data;\n\tstruct device *dev = &ipa->pdev->dev;\n\tenum ipa_endpoint_name other_name;\n\n\tif (ipa_gsi_endpoint_data_empty(data))\n\t\treturn true;\n\n\tif (!data->toward_ipa) {\n\t\tconst struct ipa_endpoint_rx *rx_config;\n\t\tconst struct reg *reg;\n\t\tu32 buffer_size;\n\t\tu32 aggr_size;\n\t\tu32 limit;\n\n\t\tif (data->endpoint.filter_support) {\n\t\t\tdev_err(dev, \"filtering not supported for \"\n\t\t\t\t\t\"RX endpoint %u\\n\",\n\t\t\t\tdata->endpoint_id);\n\t\t\treturn false;\n\t\t}\n\n\t\t \n\t\tif (data->ee_id != GSI_EE_AP)\n\t\t\treturn true;\n\n\t\trx_config = &data->endpoint.config.rx;\n\n\t\t \n\t\tbuffer_size = rx_config->buffer_size;\n\t\tlimit = IPA_MTU + IPA_RX_BUFFER_OVERHEAD;\n\t\tif (buffer_size < limit) {\n\t\t\tdev_err(dev, \"RX buffer size too small for RX endpoint %u (%u < %u)\\n\",\n\t\t\t\tdata->endpoint_id, buffer_size, limit);\n\t\t\treturn false;\n\t\t}\n\n\t\tif (!data->endpoint.config.aggregation) {\n\t\t\tbool result = true;\n\n\t\t\t \n\t\t\tif (rx_config->aggr_time_limit) {\n\t\t\t\tdev_err(dev,\n\t\t\t\t\t\"time limit with no aggregation for RX endpoint %u\\n\",\n\t\t\t\t\tdata->endpoint_id);\n\t\t\t\tresult = false;\n\t\t\t}\n\n\t\t\tif (rx_config->aggr_hard_limit) {\n\t\t\t\tdev_err(dev, \"hard limit with no aggregation for RX endpoint %u\\n\",\n\t\t\t\t\tdata->endpoint_id);\n\t\t\t\tresult = false;\n\t\t\t}\n\n\t\t\tif (rx_config->aggr_close_eof) {\n\t\t\t\tdev_err(dev, \"close EOF with no aggregation for RX endpoint %u\\n\",\n\t\t\t\t\tdata->endpoint_id);\n\t\t\t\tresult = false;\n\t\t\t}\n\n\t\t\treturn result;\t \n\t\t}\n\n\t\t \n\t\taggr_size = ipa_aggr_size_kb(buffer_size - NET_SKB_PAD,\n\t\t\t\t\t     rx_config->aggr_hard_limit);\n\t\treg = ipa_reg(ipa, ENDP_INIT_AGGR);\n\n\t\tlimit = reg_field_max(reg, BYTE_LIMIT);\n\t\tif (aggr_size > limit) {\n\t\t\tdev_err(dev, \"aggregated size too large for RX endpoint %u (%u KB > %u KB)\\n\",\n\t\t\t\tdata->endpoint_id, aggr_size, limit);\n\n\t\t\treturn false;\n\t\t}\n\n\t\treturn true;\t \n\t}\n\n\t \n\tif (ipa->version >= IPA_VERSION_4_5) {\n\t\tif (data->endpoint.config.tx.seq_rep_type) {\n\t\t\tdev_err(dev, \"no-zero seq_rep_type TX endpoint %u\\n\",\n\t\t\t\tdata->endpoint_id);\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (data->endpoint.config.status_enable) {\n\t\tother_name = data->endpoint.config.tx.status_endpoint;\n\t\tif (other_name >= count) {\n\t\t\tdev_err(dev, \"status endpoint name %u out of range \"\n\t\t\t\t\t\"for endpoint %u\\n\",\n\t\t\t\tother_name, data->endpoint_id);\n\t\t\treturn false;\n\t\t}\n\n\t\t \n\t\tother_data = &all_data[other_name];\n\t\tif (ipa_gsi_endpoint_data_empty(other_data)) {\n\t\t\tdev_err(dev, \"DMA endpoint name %u undefined \"\n\t\t\t\t\t\"for endpoint %u\\n\",\n\t\t\t\tother_name, data->endpoint_id);\n\t\t\treturn false;\n\t\t}\n\n\t\t \n\t\tif (other_data->toward_ipa) {\n\t\t\tdev_err(dev,\n\t\t\t\t\"status endpoint for endpoint %u not RX\\n\",\n\t\t\t\tdata->endpoint_id);\n\t\t\treturn false;\n\t\t}\n\n\t\t \n\t\tif (other_data->ee_id == GSI_EE_AP) {\n\t\t\t \n\t\t\tif (!other_data->endpoint.config.status_enable) {\n\t\t\t\tdev_err(dev,\n\t\t\t\t\t\"status not enabled for endpoint %u\\n\",\n\t\t\t\t\tother_data->endpoint_id);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (data->endpoint.config.dma_mode) {\n\t\tother_name = data->endpoint.config.dma_endpoint;\n\t\tif (other_name >= count) {\n\t\t\tdev_err(dev, \"DMA endpoint name %u out of range \"\n\t\t\t\t\t\"for endpoint %u\\n\",\n\t\t\t\tother_name, data->endpoint_id);\n\t\t\treturn false;\n\t\t}\n\n\t\tother_data = &all_data[other_name];\n\t\tif (ipa_gsi_endpoint_data_empty(other_data)) {\n\t\t\tdev_err(dev, \"DMA endpoint name %u undefined \"\n\t\t\t\t\t\"for endpoint %u\\n\",\n\t\t\t\tother_name, data->endpoint_id);\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\n \nstatic u32 ipa_endpoint_max(struct ipa *ipa, u32 count,\n\t\t\t    const struct ipa_gsi_endpoint_data *data)\n{\n\tconst struct ipa_gsi_endpoint_data *dp = data;\n\tstruct device *dev = &ipa->pdev->dev;\n\tenum ipa_endpoint_name name;\n\tu32 max;\n\n\tif (count > IPA_ENDPOINT_COUNT) {\n\t\tdev_err(dev, \"too many endpoints specified (%u > %u)\\n\",\n\t\t\tcount, IPA_ENDPOINT_COUNT);\n\t\treturn 0;\n\t}\n\n\t \n\tif (ipa_gsi_endpoint_data_empty(&data[IPA_ENDPOINT_AP_COMMAND_TX])) {\n\t\tdev_err(dev, \"command TX endpoint not defined\\n\");\n\t\treturn 0;\n\t}\n\tif (ipa_gsi_endpoint_data_empty(&data[IPA_ENDPOINT_AP_LAN_RX])) {\n\t\tdev_err(dev, \"LAN RX endpoint not defined\\n\");\n\t\treturn 0;\n\t}\n\tif (ipa_gsi_endpoint_data_empty(&data[IPA_ENDPOINT_AP_MODEM_TX])) {\n\t\tdev_err(dev, \"AP->modem TX endpoint not defined\\n\");\n\t\treturn 0;\n\t}\n\tif (ipa_gsi_endpoint_data_empty(&data[IPA_ENDPOINT_AP_MODEM_RX])) {\n\t\tdev_err(dev, \"AP<-modem RX endpoint not defined\\n\");\n\t\treturn 0;\n\t}\n\n\tmax = 0;\n\tfor (name = 0; name < count; name++, dp++) {\n\t\tif (!ipa_endpoint_data_valid_one(ipa, count, data, dp))\n\t\t\treturn 0;\n\t\tmax = max_t(u32, max, dp->endpoint_id);\n\t}\n\n\treturn max;\n}\n\n \nstatic struct gsi_trans *ipa_endpoint_trans_alloc(struct ipa_endpoint *endpoint,\n\t\t\t\t\t\t  u32 tre_count)\n{\n\tstruct gsi *gsi = &endpoint->ipa->gsi;\n\tu32 channel_id = endpoint->channel_id;\n\tenum dma_data_direction direction;\n\n\tdirection = endpoint->toward_ipa ? DMA_TO_DEVICE : DMA_FROM_DEVICE;\n\n\treturn gsi_channel_trans_alloc(gsi, channel_id, tre_count, direction);\n}\n\n \nstatic bool\nipa_endpoint_init_ctrl(struct ipa_endpoint *endpoint, bool suspend_delay)\n{\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 field_id;\n\tu32 offset;\n\tbool state;\n\tu32 mask;\n\tu32 val;\n\n\tif (endpoint->toward_ipa)\n\t\tWARN_ON(ipa->version >= IPA_VERSION_4_2);\n\telse\n\t\tWARN_ON(ipa->version >= IPA_VERSION_4_0);\n\n\treg = ipa_reg(ipa, ENDP_INIT_CTRL);\n\toffset = reg_n_offset(reg, endpoint->endpoint_id);\n\tval = ioread32(ipa->reg_virt + offset);\n\n\tfield_id = endpoint->toward_ipa ? ENDP_DELAY : ENDP_SUSPEND;\n\tmask = reg_bit(reg, field_id);\n\n\tstate = !!(val & mask);\n\n\t \n\tif (suspend_delay != state) {\n\t\tval ^= mask;\n\t\tiowrite32(val, ipa->reg_virt + offset);\n\t}\n\n\treturn state;\n}\n\n \nstatic void\nipa_endpoint_program_delay(struct ipa_endpoint *endpoint, bool enable)\n{\n\t \n\tWARN_ON(endpoint->ipa->version >= IPA_VERSION_4_2);\n\tWARN_ON(!endpoint->toward_ipa);\n\n\t(void)ipa_endpoint_init_ctrl(endpoint, enable);\n}\n\nstatic bool ipa_endpoint_aggr_active(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tu32 unit = endpoint_id / 32;\n\tconst struct reg *reg;\n\tu32 val;\n\n\tWARN_ON(!test_bit(endpoint_id, ipa->available));\n\n\treg = ipa_reg(ipa, STATE_AGGR_ACTIVE);\n\tval = ioread32(ipa->reg_virt + reg_n_offset(reg, unit));\n\n\treturn !!(val & BIT(endpoint_id % 32));\n}\n\nstatic void ipa_endpoint_force_close(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tu32 mask = BIT(endpoint_id % 32);\n\tstruct ipa *ipa = endpoint->ipa;\n\tu32 unit = endpoint_id / 32;\n\tconst struct reg *reg;\n\n\tWARN_ON(!test_bit(endpoint_id, ipa->available));\n\n\treg = ipa_reg(ipa, AGGR_FORCE_CLOSE);\n\tiowrite32(mask, ipa->reg_virt + reg_n_offset(reg, unit));\n}\n\n \nstatic void ipa_endpoint_suspend_aggr(struct ipa_endpoint *endpoint)\n{\n\tstruct ipa *ipa = endpoint->ipa;\n\n\tif (!endpoint->config.aggregation)\n\t\treturn;\n\n\t \n\tif (!ipa_endpoint_aggr_active(endpoint))\n\t\treturn;\n\n\t \n\tipa_endpoint_force_close(endpoint);\n\n\tipa_interrupt_simulate_suspend(ipa->interrupt);\n}\n\n \nstatic bool\nipa_endpoint_program_suspend(struct ipa_endpoint *endpoint, bool enable)\n{\n\tbool suspended;\n\n\tif (endpoint->ipa->version >= IPA_VERSION_4_0)\n\t\treturn enable;\t \n\n\tWARN_ON(endpoint->toward_ipa);\n\n\tsuspended = ipa_endpoint_init_ctrl(endpoint, enable);\n\n\t \n\tif (enable && !suspended)\n\t\tipa_endpoint_suspend_aggr(endpoint);\n\n\treturn suspended;\n}\n\n \nvoid ipa_endpoint_modem_pause_all(struct ipa *ipa, bool enable)\n{\n\tu32 endpoint_id = 0;\n\n\twhile (endpoint_id < ipa->endpoint_count) {\n\t\tstruct ipa_endpoint *endpoint = &ipa->endpoint[endpoint_id++];\n\n\t\tif (endpoint->ee_id != GSI_EE_MODEM)\n\t\t\tcontinue;\n\n\t\tif (!endpoint->toward_ipa)\n\t\t\t(void)ipa_endpoint_program_suspend(endpoint, enable);\n\t\telse if (ipa->version < IPA_VERSION_4_2)\n\t\t\tipa_endpoint_program_delay(endpoint, enable);\n\t\telse\n\t\t\tgsi_modem_channel_flow_control(&ipa->gsi,\n\t\t\t\t\t\t       endpoint->channel_id,\n\t\t\t\t\t\t       enable);\n\t}\n}\n\n \nint ipa_endpoint_modem_exception_reset_all(struct ipa *ipa)\n{\n\tstruct gsi_trans *trans;\n\tu32 endpoint_id;\n\tu32 count;\n\n\t \n\tcount = ipa->modem_tx_count + ipa_cmd_pipeline_clear_count();\n\ttrans = ipa_cmd_trans_alloc(ipa, count);\n\tif (!trans) {\n\t\tdev_err(&ipa->pdev->dev,\n\t\t\t\"no transaction to reset modem exception endpoints\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tfor_each_set_bit(endpoint_id, ipa->defined, ipa->endpoint_count) {\n\t\tstruct ipa_endpoint *endpoint;\n\t\tconst struct reg *reg;\n\t\tu32 offset;\n\n\t\t \n\t\tendpoint = &ipa->endpoint[endpoint_id];\n\t\tif (!(endpoint->ee_id == GSI_EE_MODEM && endpoint->toward_ipa))\n\t\t\tcontinue;\n\n\t\treg = ipa_reg(ipa, ENDP_STATUS);\n\t\toffset = reg_n_offset(reg, endpoint_id);\n\n\t\t \n\t\tipa_cmd_register_write_add(trans, offset, 0, ~0, false);\n\t}\n\n\tipa_cmd_pipeline_clear_add(trans);\n\n\tgsi_trans_commit_wait(trans);\n\n\tipa_cmd_pipeline_clear_wait(ipa);\n\n\treturn 0;\n}\n\nstatic void ipa_endpoint_init_cfg(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tenum ipa_cs_offload_en enabled;\n\tconst struct reg *reg;\n\tu32 val = 0;\n\n\treg = ipa_reg(ipa, ENDP_INIT_CFG);\n\t \n\tif (endpoint->config.checksum) {\n\t\tenum ipa_version version = ipa->version;\n\n\t\tif (endpoint->toward_ipa) {\n\t\t\tu32 off;\n\n\t\t\t \n\t\t\toff = sizeof(struct rmnet_map_header) / sizeof(u32);\n\t\t\tval |= reg_encode(reg, CS_METADATA_HDR_OFFSET, off);\n\n\t\t\tenabled = version < IPA_VERSION_4_5\n\t\t\t\t\t? IPA_CS_OFFLOAD_UL\n\t\t\t\t\t: IPA_CS_OFFLOAD_INLINE;\n\t\t} else {\n\t\t\tenabled = version < IPA_VERSION_4_5\n\t\t\t\t\t? IPA_CS_OFFLOAD_DL\n\t\t\t\t\t: IPA_CS_OFFLOAD_INLINE;\n\t\t}\n\t} else {\n\t\tenabled = IPA_CS_OFFLOAD_NONE;\n\t}\n\tval |= reg_encode(reg, CS_OFFLOAD_EN, enabled);\n\t \n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\nstatic void ipa_endpoint_init_nat(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val;\n\n\tif (!endpoint->toward_ipa)\n\t\treturn;\n\n\treg = ipa_reg(ipa, ENDP_INIT_NAT);\n\tval = reg_encode(reg, NAT_EN, IPA_NAT_TYPE_BYPASS);\n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\nstatic u32\nipa_qmap_header_size(enum ipa_version version, struct ipa_endpoint *endpoint)\n{\n\tu32 header_size = sizeof(struct rmnet_map_header);\n\n\t \n\tif (!endpoint->config.checksum)\n\t\treturn header_size;\n\n\tif (version < IPA_VERSION_4_5) {\n\t\t \n\t\tif (endpoint->toward_ipa)\n\t\t\theader_size += sizeof(struct rmnet_map_ul_csum_header);\n\t} else {\n\t\t \n\t\theader_size += sizeof(struct rmnet_map_v5_csum_header);\n\t}\n\n\treturn header_size;\n}\n\n \nstatic u32 ipa_header_size_encode(enum ipa_version version,\n\t\t\t\t  const struct reg *reg, u32 header_size)\n{\n\tu32 field_max = reg_field_max(reg, HDR_LEN);\n\tu32 val;\n\n\t \n\tval = reg_encode(reg, HDR_LEN, header_size & field_max);\n\tif (version < IPA_VERSION_4_5) {\n\t\tWARN_ON(header_size > field_max);\n\t\treturn val;\n\t}\n\n\t \n\theader_size >>= hweight32(field_max);\n\tWARN_ON(header_size > reg_field_max(reg, HDR_LEN_MSB));\n\tval |= reg_encode(reg, HDR_LEN_MSB, header_size);\n\n\treturn val;\n}\n\n \nstatic u32 ipa_metadata_offset_encode(enum ipa_version version,\n\t\t\t\t      const struct reg *reg, u32 offset)\n{\n\tu32 field_max = reg_field_max(reg, HDR_OFST_METADATA);\n\tu32 val;\n\n\t \n\tval = reg_encode(reg, HDR_OFST_METADATA, offset);\n\tif (version < IPA_VERSION_4_5) {\n\t\tWARN_ON(offset > field_max);\n\t\treturn val;\n\t}\n\n\t \n\toffset >>= hweight32(field_max);\n\tWARN_ON(offset > reg_field_max(reg, HDR_OFST_METADATA_MSB));\n\tval |= reg_encode(reg, HDR_OFST_METADATA_MSB, offset);\n\n\treturn val;\n}\n\n \nstatic void ipa_endpoint_init_hdr(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val = 0;\n\n\treg = ipa_reg(ipa, ENDP_INIT_HDR);\n\tif (endpoint->config.qmap) {\n\t\tenum ipa_version version = ipa->version;\n\t\tsize_t header_size;\n\n\t\theader_size = ipa_qmap_header_size(version, endpoint);\n\t\tval = ipa_header_size_encode(version, reg, header_size);\n\n\t\t \n\t\tif (!endpoint->toward_ipa) {\n\t\t\tu32 off;      \n\n\t\t\t \n\t\t\toff = offsetof(struct rmnet_map_header, mux_id);\n\t\t\tval |= ipa_metadata_offset_encode(version, reg, off);\n\n\t\t\t \n\t\t\toff = offsetof(struct rmnet_map_header, pkt_len);\n\t\t\t \n\t\t\tif (version >= IPA_VERSION_4_5)\n\t\t\t\toff &= reg_field_max(reg, HDR_OFST_PKT_SIZE);\n\n\t\t\tval |= reg_bit(reg, HDR_OFST_PKT_SIZE_VALID);\n\t\t\tval |= reg_encode(reg, HDR_OFST_PKT_SIZE, off);\n\t\t}\n\t\t \n\t\tval |= reg_bit(reg, HDR_OFST_METADATA_VALID);\n\n\t\t \n\t\t \n\t\t \n\t\t \n\t}\n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\nstatic void ipa_endpoint_init_hdr_ext(struct ipa_endpoint *endpoint)\n{\n\tu32 pad_align = endpoint->config.rx.pad_align;\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val = 0;\n\n\treg = ipa_reg(ipa, ENDP_INIT_HDR_EXT);\n\tif (endpoint->config.qmap) {\n\t\t \n\t\tval |= reg_bit(reg, HDR_ENDIANNESS);\t \n\n\t\t \n\t\tif (!endpoint->toward_ipa) {\n\t\t\tval |= reg_bit(reg, HDR_TOTAL_LEN_OR_PAD_VALID);\n\t\t\t \n\t\t\tval |= reg_bit(reg, HDR_PAYLOAD_LEN_INC_PADDING);\n\t\t\t \n\t\t}\n\t}\n\n\t \n\tif (!endpoint->toward_ipa)\n\t\tval |= reg_encode(reg, HDR_PAD_TO_ALIGNMENT, pad_align);\n\n\t \n\tif (ipa->version >= IPA_VERSION_4_5) {\n\t\t \n\t\tif (endpoint->config.qmap && !endpoint->toward_ipa) {\n\t\t\tu32 mask = reg_field_max(reg, HDR_OFST_PKT_SIZE);\n\t\t\tu32 off;      \n\n\t\t\toff = offsetof(struct rmnet_map_header, pkt_len);\n\t\t\t \n\t\t\toff >>= hweight32(mask);\n\t\t\tval |= reg_encode(reg, HDR_OFST_PKT_SIZE_MSB, off);\n\t\t\t \n\t\t}\n\t}\n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\nstatic void ipa_endpoint_init_hdr_metadata_mask(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val = 0;\n\tu32 offset;\n\n\tif (endpoint->toward_ipa)\n\t\treturn;\t\t \n\n\treg = ipa_reg(ipa,  ENDP_INIT_HDR_METADATA_MASK);\n\toffset = reg_n_offset(reg, endpoint_id);\n\n\t \n\tif (endpoint->config.qmap)\n\t\tval = (__force u32)cpu_to_be32(IPA_ENDPOINT_QMAP_METADATA_MASK);\n\n\tiowrite32(val, ipa->reg_virt + offset);\n}\n\nstatic void ipa_endpoint_init_mode(struct ipa_endpoint *endpoint)\n{\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 offset;\n\tu32 val;\n\n\tif (!endpoint->toward_ipa)\n\t\treturn;\t\t \n\n\treg = ipa_reg(ipa, ENDP_INIT_MODE);\n\tif (endpoint->config.dma_mode) {\n\t\tenum ipa_endpoint_name name = endpoint->config.dma_endpoint;\n\t\tu32 dma_endpoint_id = ipa->name_map[name]->endpoint_id;\n\n\t\tval = reg_encode(reg, ENDP_MODE, IPA_DMA);\n\t\tval |= reg_encode(reg, DEST_PIPE_INDEX, dma_endpoint_id);\n\t} else {\n\t\tval = reg_encode(reg, ENDP_MODE, IPA_BASIC);\n\t}\n\t \n\n\toffset = reg_n_offset(reg, endpoint->endpoint_id);\n\tiowrite32(val, ipa->reg_virt + offset);\n}\n\n \nstatic u32\nipa_qtime_val(struct ipa *ipa, u32 microseconds, u32 max, u32 *select)\n{\n\tu32 which = 0;\n\tu32 ticks;\n\n\t \n\tticks = DIV_ROUND_CLOSEST(microseconds, 100);\n\tif (ticks <= max)\n\t\tgoto out;\n\n\t \n\twhich = 1;\n\tticks = DIV_ROUND_CLOSEST(microseconds, 1000);\n\tif (ticks <= max)\n\t\tgoto out;\n\n\tif (ipa->version >= IPA_VERSION_5_0) {\n\t\t \n\t\twhich = 2;\n\t\tticks = DIV_ROUND_CLOSEST(microseconds, 100);\n\t}\n\tWARN_ON(ticks > max);\nout:\n\t*select = which;\n\n\treturn ticks;\n}\n\n \nstatic u32 aggr_time_limit_encode(struct ipa *ipa, const struct reg *reg,\n\t\t\t\t  u32 microseconds)\n{\n\tu32 ticks;\n\tu32 max;\n\n\tif (!microseconds)\n\t\treturn 0;\t \n\n\tmax = reg_field_max(reg, TIME_LIMIT);\n\tif (ipa->version >= IPA_VERSION_4_5) {\n\t\tu32 select;\n\n\t\tticks = ipa_qtime_val(ipa, microseconds, max, &select);\n\n\t\treturn reg_encode(reg, AGGR_GRAN_SEL, select) |\n\t\t       reg_encode(reg, TIME_LIMIT, ticks);\n\t}\n\n\t \n\tticks = DIV_ROUND_CLOSEST(microseconds, IPA_AGGR_GRANULARITY);\n\tWARN(ticks > max, \"aggr_time_limit too large (%u > %u usec)\\n\",\n\t     microseconds, max * IPA_AGGR_GRANULARITY);\n\n\treturn reg_encode(reg, TIME_LIMIT, ticks);\n}\n\nstatic void ipa_endpoint_init_aggr(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val = 0;\n\n\treg = ipa_reg(ipa, ENDP_INIT_AGGR);\n\tif (endpoint->config.aggregation) {\n\t\tif (!endpoint->toward_ipa) {\n\t\t\tconst struct ipa_endpoint_rx *rx_config;\n\t\t\tu32 buffer_size;\n\t\t\tu32 limit;\n\n\t\t\trx_config = &endpoint->config.rx;\n\t\t\tval |= reg_encode(reg, AGGR_EN, IPA_ENABLE_AGGR);\n\t\t\tval |= reg_encode(reg, AGGR_TYPE, IPA_GENERIC);\n\n\t\t\tbuffer_size = rx_config->buffer_size;\n\t\t\tlimit = ipa_aggr_size_kb(buffer_size - NET_SKB_PAD,\n\t\t\t\t\t\t rx_config->aggr_hard_limit);\n\t\t\tval |= reg_encode(reg, BYTE_LIMIT, limit);\n\n\t\t\tlimit = rx_config->aggr_time_limit;\n\t\t\tval |= aggr_time_limit_encode(ipa, reg, limit);\n\n\t\t\t \n\n\t\t\tif (rx_config->aggr_close_eof)\n\t\t\t\tval |= reg_bit(reg, SW_EOF_ACTIVE);\n\t\t} else {\n\t\t\tval |= reg_encode(reg, AGGR_EN, IPA_ENABLE_DEAGGR);\n\t\t\tval |= reg_encode(reg, AGGR_TYPE, IPA_QCMAP);\n\t\t\t \n\t\t}\n\t\t \n\t\t \n\t} else {\n\t\tval |= reg_encode(reg, AGGR_EN, IPA_BYPASS_AGGR);\n\t\t \n\t}\n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\n \nstatic u32 hol_block_timer_encode(struct ipa *ipa, const struct reg *reg,\n\t\t\t\t  u32 microseconds)\n{\n\tu32 width;\n\tu32 scale;\n\tu64 ticks;\n\tu64 rate;\n\tu32 high;\n\tu32 val;\n\n\tif (!microseconds)\n\t\treturn 0;\t \n\n\tif (ipa->version >= IPA_VERSION_4_5) {\n\t\tu32 max = reg_field_max(reg, TIMER_LIMIT);\n\t\tu32 select;\n\t\tu32 ticks;\n\n\t\tticks = ipa_qtime_val(ipa, microseconds, max, &select);\n\n\t\treturn reg_encode(reg, TIMER_GRAN_SEL, 1) |\n\t\t       reg_encode(reg, TIMER_LIMIT, ticks);\n\t}\n\n\t \n\trate = ipa_core_clock_rate(ipa);\n\tticks = DIV_ROUND_CLOSEST(microseconds * rate, 128 * USEC_PER_SEC);\n\n\t \n\tWARN_ON(ticks > reg_field_max(reg, TIMER_BASE_VALUE));\n\n\t \n\tif (ipa->version < IPA_VERSION_4_2)\n\t\treturn reg_encode(reg, TIMER_BASE_VALUE, (u32)ticks);\n\n\t \n\thigh = fls(ticks);\t\t \n\twidth = hweight32(reg_fmask(reg, TIMER_BASE_VALUE));\n\tscale = high > width ? high - width : 0;\n\tif (scale) {\n\t\t \n\t\tticks += 1 << (scale - 1);\n\t\t \n\t\tif (fls(ticks) != high)\n\t\t\tscale++;\n\t}\n\n\tval = reg_encode(reg, TIMER_SCALE, scale);\n\tval |= reg_encode(reg, TIMER_BASE_VALUE, (u32)ticks >> scale);\n\n\treturn val;\n}\n\n \nstatic void ipa_endpoint_init_hol_block_timer(struct ipa_endpoint *endpoint,\n\t\t\t\t\t      u32 microseconds)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val;\n\n\t \n\treg = ipa_reg(ipa, ENDP_INIT_HOL_BLOCK_TIMER);\n\tval = hol_block_timer_encode(ipa, reg, microseconds);\n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\nstatic void\nipa_endpoint_init_hol_block_en(struct ipa_endpoint *endpoint, bool enable)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 offset;\n\tu32 val;\n\n\treg = ipa_reg(ipa, ENDP_INIT_HOL_BLOCK_EN);\n\toffset = reg_n_offset(reg, endpoint_id);\n\tval = enable ? reg_bit(reg, HOL_BLOCK_EN) : 0;\n\n\tiowrite32(val, ipa->reg_virt + offset);\n\n\t \n\tif (enable && ipa->version >= IPA_VERSION_4_5)\n\t\tiowrite32(val, ipa->reg_virt + offset);\n}\n\n \nstatic void ipa_endpoint_init_hol_block_enable(struct ipa_endpoint *endpoint,\n\t\t\t\t\t       u32 microseconds)\n{\n\tipa_endpoint_init_hol_block_timer(endpoint, microseconds);\n\tipa_endpoint_init_hol_block_en(endpoint, true);\n}\n\nstatic void ipa_endpoint_init_hol_block_disable(struct ipa_endpoint *endpoint)\n{\n\tipa_endpoint_init_hol_block_en(endpoint, false);\n}\n\nvoid ipa_endpoint_modem_hol_block_clear_all(struct ipa *ipa)\n{\n\tu32 endpoint_id = 0;\n\n\twhile (endpoint_id < ipa->endpoint_count) {\n\t\tstruct ipa_endpoint *endpoint = &ipa->endpoint[endpoint_id++];\n\n\t\tif (endpoint->toward_ipa || endpoint->ee_id != GSI_EE_MODEM)\n\t\t\tcontinue;\n\n\t\tipa_endpoint_init_hol_block_disable(endpoint);\n\t\tipa_endpoint_init_hol_block_enable(endpoint, 0);\n\t}\n}\n\nstatic void ipa_endpoint_init_deaggr(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val = 0;\n\n\tif (!endpoint->toward_ipa)\n\t\treturn;\t\t \n\n\treg = ipa_reg(ipa, ENDP_INIT_DEAGGR);\n\t \n\t \n\t \n\t \n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\nstatic void ipa_endpoint_init_rsrc_grp(struct ipa_endpoint *endpoint)\n{\n\tu32 resource_group = endpoint->config.resource_group;\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val;\n\n\treg = ipa_reg(ipa, ENDP_INIT_RSRC_GRP);\n\tval = reg_encode(reg, ENDP_RSRC_GRP, resource_group);\n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\nstatic void ipa_endpoint_init_seq(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val;\n\n\tif (!endpoint->toward_ipa)\n\t\treturn;\t\t \n\n\treg = ipa_reg(ipa, ENDP_INIT_SEQ);\n\n\t \n\tval = reg_encode(reg, SEQ_TYPE, endpoint->config.tx.seq_type);\n\n\t \n\tif (ipa->version < IPA_VERSION_4_5)\n\t\tval |= reg_encode(reg, SEQ_REP_TYPE,\n\t\t\t\t  endpoint->config.tx.seq_rep_type);\n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\n \nint ipa_endpoint_skb_tx(struct ipa_endpoint *endpoint, struct sk_buff *skb)\n{\n\tstruct gsi_trans *trans;\n\tu32 nr_frags;\n\tint ret;\n\n\t \n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\tif (nr_frags > endpoint->skb_frag_max) {\n\t\tif (skb_linearize(skb))\n\t\t\treturn -E2BIG;\n\t\tnr_frags = 0;\n\t}\n\n\ttrans = ipa_endpoint_trans_alloc(endpoint, 1 + nr_frags);\n\tif (!trans)\n\t\treturn -EBUSY;\n\n\tret = gsi_trans_skb_add(trans, skb);\n\tif (ret)\n\t\tgoto err_trans_free;\n\ttrans->data = skb;\t \n\n\tgsi_trans_commit(trans, !netdev_xmit_more());\n\n\treturn 0;\n\nerr_trans_free:\n\tgsi_trans_free(trans);\n\n\treturn -ENOMEM;\n}\n\nstatic void ipa_endpoint_status(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tconst struct reg *reg;\n\tu32 val = 0;\n\n\treg = ipa_reg(ipa, ENDP_STATUS);\n\tif (endpoint->config.status_enable) {\n\t\tval |= reg_bit(reg, STATUS_EN);\n\t\tif (endpoint->toward_ipa) {\n\t\t\tenum ipa_endpoint_name name;\n\t\t\tu32 status_endpoint_id;\n\n\t\t\tname = endpoint->config.tx.status_endpoint;\n\t\t\tstatus_endpoint_id = ipa->name_map[name]->endpoint_id;\n\n\t\t\tval |= reg_encode(reg, STATUS_ENDP, status_endpoint_id);\n\t\t}\n\t\t \n\t\t \n\t}\n\n\tiowrite32(val, ipa->reg_virt + reg_n_offset(reg, endpoint_id));\n}\n\nstatic int ipa_endpoint_replenish_one(struct ipa_endpoint *endpoint,\n\t\t\t\t      struct gsi_trans *trans)\n{\n\tstruct page *page;\n\tu32 buffer_size;\n\tu32 offset;\n\tu32 len;\n\tint ret;\n\n\tbuffer_size = endpoint->config.rx.buffer_size;\n\tpage = dev_alloc_pages(get_order(buffer_size));\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\t \n\toffset = NET_SKB_PAD;\n\tlen = buffer_size - offset;\n\n\tret = gsi_trans_page_add(trans, page, len, offset);\n\tif (ret)\n\t\tput_page(page);\n\telse\n\t\ttrans->data = page;\t \n\n\treturn ret;\n}\n\n \nstatic void ipa_endpoint_replenish(struct ipa_endpoint *endpoint)\n{\n\tstruct gsi_trans *trans;\n\n\tif (!test_bit(IPA_REPLENISH_ENABLED, endpoint->replenish_flags))\n\t\treturn;\n\n\t \n\tif (test_and_set_bit(IPA_REPLENISH_ACTIVE, endpoint->replenish_flags))\n\t\treturn;\n\n\twhile ((trans = ipa_endpoint_trans_alloc(endpoint, 1))) {\n\t\tbool doorbell;\n\n\t\tif (ipa_endpoint_replenish_one(endpoint, trans))\n\t\t\tgoto try_again_later;\n\n\n\t\t \n\t\tdoorbell = !(++endpoint->replenish_count % IPA_REPLENISH_BATCH);\n\t\tgsi_trans_commit(trans, doorbell);\n\t}\n\n\tclear_bit(IPA_REPLENISH_ACTIVE, endpoint->replenish_flags);\n\n\treturn;\n\ntry_again_later:\n\tgsi_trans_free(trans);\n\tclear_bit(IPA_REPLENISH_ACTIVE, endpoint->replenish_flags);\n\n\t \n\tif (gsi_channel_trans_idle(&endpoint->ipa->gsi, endpoint->channel_id))\n\t\tschedule_delayed_work(&endpoint->replenish_work,\n\t\t\t\t      msecs_to_jiffies(1));\n}\n\nstatic void ipa_endpoint_replenish_enable(struct ipa_endpoint *endpoint)\n{\n\tset_bit(IPA_REPLENISH_ENABLED, endpoint->replenish_flags);\n\n\t \n\tif (gsi_channel_trans_idle(&endpoint->ipa->gsi, endpoint->channel_id))\n\t\tipa_endpoint_replenish(endpoint);\n}\n\nstatic void ipa_endpoint_replenish_disable(struct ipa_endpoint *endpoint)\n{\n\tclear_bit(IPA_REPLENISH_ENABLED, endpoint->replenish_flags);\n}\n\nstatic void ipa_endpoint_replenish_work(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct ipa_endpoint *endpoint;\n\n\tendpoint = container_of(dwork, struct ipa_endpoint, replenish_work);\n\n\tipa_endpoint_replenish(endpoint);\n}\n\nstatic void ipa_endpoint_skb_copy(struct ipa_endpoint *endpoint,\n\t\t\t\t  void *data, u32 len, u32 extra)\n{\n\tstruct sk_buff *skb;\n\n\tif (!endpoint->netdev)\n\t\treturn;\n\n\tskb = __dev_alloc_skb(len, GFP_ATOMIC);\n\tif (skb) {\n\t\t \n\t\tskb_put(skb, len);\n\t\tmemcpy(skb->data, data, len);\n\t\tskb->truesize += extra;\n\t}\n\n\tipa_modem_skb_rx(endpoint->netdev, skb);\n}\n\nstatic bool ipa_endpoint_skb_build(struct ipa_endpoint *endpoint,\n\t\t\t\t   struct page *page, u32 len)\n{\n\tu32 buffer_size = endpoint->config.rx.buffer_size;\n\tstruct sk_buff *skb;\n\n\t \n\tif (!endpoint->netdev)\n\t\treturn false;\n\n\tWARN_ON(len > SKB_WITH_OVERHEAD(buffer_size - NET_SKB_PAD));\n\n\tskb = build_skb(page_address(page), buffer_size);\n\tif (skb) {\n\t\t \n\t\tskb_reserve(skb, NET_SKB_PAD);\n\t\tskb_put(skb, len);\n\t}\n\n\t \n\tipa_modem_skb_rx(endpoint->netdev, skb);\n\n\treturn skb != NULL;\n}\n\n  \nstatic bool ipa_status_format_packet(enum ipa_status_opcode opcode)\n{\n\tswitch (opcode) {\n\tcase IPA_STATUS_OPCODE_PACKET:\n\tcase IPA_STATUS_OPCODE_DROPPED_PACKET:\n\tcase IPA_STATUS_OPCODE_SUSPENDED_PACKET:\n\tcase IPA_STATUS_OPCODE_PACKET_2ND_PASS:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic bool\nipa_endpoint_status_skip(struct ipa_endpoint *endpoint, const void *data)\n{\n\tstruct ipa *ipa = endpoint->ipa;\n\tenum ipa_status_opcode opcode;\n\tu32 endpoint_id;\n\n\topcode = ipa_status_extract(ipa, data, STATUS_OPCODE);\n\tif (!ipa_status_format_packet(opcode))\n\t\treturn true;\n\n\tendpoint_id = ipa_status_extract(ipa, data, STATUS_DST_ENDPOINT);\n\tif (endpoint_id != endpoint->endpoint_id)\n\t\treturn true;\n\n\treturn false;\t \n}\n\nstatic bool\nipa_endpoint_status_tag_valid(struct ipa_endpoint *endpoint, const void *data)\n{\n\tstruct ipa_endpoint *command_endpoint;\n\tenum ipa_status_mask status_mask;\n\tstruct ipa *ipa = endpoint->ipa;\n\tu32 endpoint_id;\n\n\tstatus_mask = ipa_status_extract(ipa, data, STATUS_MASK);\n\tif (!status_mask)\n\t\treturn false;\t \n\n\t \n\tendpoint_id = ipa_status_extract(ipa, data, STATUS_SRC_ENDPOINT);\n\tcommand_endpoint = ipa->name_map[IPA_ENDPOINT_AP_COMMAND_TX];\n\tif (endpoint_id == command_endpoint->endpoint_id) {\n\t\tcomplete(&ipa->completion);\n\t} else {\n\t\tdev_err(&ipa->pdev->dev,\n\t\t\t\"unexpected tagged packet from endpoint %u\\n\",\n\t\t\tendpoint_id);\n\t}\n\n\treturn true;\n}\n\n \nstatic bool\nipa_endpoint_status_drop(struct ipa_endpoint *endpoint, const void *data)\n{\n\tenum ipa_status_exception exception;\n\tstruct ipa *ipa = endpoint->ipa;\n\tu32 rule;\n\n\t \n\tif (ipa_endpoint_status_tag_valid(endpoint, data))\n\t\treturn true;\n\n\t \n\texception = ipa_status_extract(ipa, data, STATUS_EXCEPTION);\n\tif (exception)\n\t\treturn exception == IPA_STATUS_EXCEPTION_DEAGGR;\n\n\t \n\trule = ipa_status_extract(ipa, data, STATUS_ROUTER_RULE_INDEX);\n\n\treturn rule == IPA_STATUS_RULE_MISS;\n}\n\nstatic void ipa_endpoint_status_parse(struct ipa_endpoint *endpoint,\n\t\t\t\t      struct page *page, u32 total_len)\n{\n\tu32 buffer_size = endpoint->config.rx.buffer_size;\n\tvoid *data = page_address(page) + NET_SKB_PAD;\n\tu32 unused = buffer_size - total_len;\n\tstruct ipa *ipa = endpoint->ipa;\n\tu32 resid = total_len;\n\n\twhile (resid) {\n\t\tu32 length;\n\t\tu32 align;\n\t\tu32 len;\n\n\t\tif (resid < IPA_STATUS_SIZE) {\n\t\t\tdev_err(&endpoint->ipa->pdev->dev,\n\t\t\t\t\"short message (%u bytes < %zu byte status)\\n\",\n\t\t\t\tresid, IPA_STATUS_SIZE);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tlength = ipa_status_extract(ipa, data, STATUS_LENGTH);\n\t\tif (!length || ipa_endpoint_status_skip(endpoint, data)) {\n\t\t\tdata += IPA_STATUS_SIZE;\n\t\t\tresid -= IPA_STATUS_SIZE;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\talign = endpoint->config.rx.pad_align ? : 1;\n\t\tlen = IPA_STATUS_SIZE + ALIGN(length, align);\n\t\tif (endpoint->config.checksum)\n\t\t\tlen += sizeof(struct rmnet_map_dl_csum_trailer);\n\n\t\tif (!ipa_endpoint_status_drop(endpoint, data)) {\n\t\t\tvoid *data2;\n\t\t\tu32 extra;\n\n\t\t\t \n\t\t\tdata2 = data + IPA_STATUS_SIZE;\n\n\t\t\t \n\t\t\textra = DIV_ROUND_CLOSEST(unused * len, total_len);\n\t\t\tipa_endpoint_skb_copy(endpoint, data2, length, extra);\n\t\t}\n\n\t\t \n\t\tdata += len;\n\t\tresid -= len;\n\t}\n}\n\nvoid ipa_endpoint_trans_complete(struct ipa_endpoint *endpoint,\n\t\t\t\t struct gsi_trans *trans)\n{\n\tstruct page *page;\n\n\tif (endpoint->toward_ipa)\n\t\treturn;\n\n\tif (trans->cancelled)\n\t\tgoto done;\n\n\t \n\tpage = trans->data;\n\tif (endpoint->config.status_enable)\n\t\tipa_endpoint_status_parse(endpoint, page, trans->len);\n\telse if (ipa_endpoint_skb_build(endpoint, page, trans->len))\n\t\ttrans->data = NULL;\t \ndone:\n\tipa_endpoint_replenish(endpoint);\n}\n\nvoid ipa_endpoint_trans_release(struct ipa_endpoint *endpoint,\n\t\t\t\tstruct gsi_trans *trans)\n{\n\tif (endpoint->toward_ipa) {\n\t\tstruct ipa *ipa = endpoint->ipa;\n\n\t\t \n\t\tif (endpoint != ipa->name_map[IPA_ENDPOINT_AP_COMMAND_TX]) {\n\t\t\tstruct sk_buff *skb = trans->data;\n\n\t\t\tif (skb)\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t}\n\t} else {\n\t\tstruct page *page = trans->data;\n\n\t\tif (page)\n\t\t\tput_page(page);\n\t}\n}\n\nvoid ipa_endpoint_default_route_set(struct ipa *ipa, u32 endpoint_id)\n{\n\tconst struct reg *reg;\n\tu32 val;\n\n\treg = ipa_reg(ipa, ROUTE);\n\t \n\tval = reg_encode(reg, ROUTE_DEF_PIPE, endpoint_id);\n\tval |= reg_bit(reg, ROUTE_DEF_HDR_TABLE);\n\t \n\tval |= reg_encode(reg, ROUTE_FRAG_DEF_PIPE, endpoint_id);\n\tval |= reg_bit(reg, ROUTE_DEF_RETAIN_HDR);\n\n\tiowrite32(val, ipa->reg_virt + reg_offset(reg));\n}\n\nvoid ipa_endpoint_default_route_clear(struct ipa *ipa)\n{\n\tipa_endpoint_default_route_set(ipa, 0);\n}\n\n \nstatic int ipa_endpoint_reset_rx_aggr(struct ipa_endpoint *endpoint)\n{\n\tstruct device *dev = &endpoint->ipa->pdev->dev;\n\tstruct ipa *ipa = endpoint->ipa;\n\tstruct gsi *gsi = &ipa->gsi;\n\tbool suspended = false;\n\tdma_addr_t addr;\n\tu32 retries;\n\tu32 len = 1;\n\tvoid *virt;\n\tint ret;\n\n\tvirt = kzalloc(len, GFP_KERNEL);\n\tif (!virt)\n\t\treturn -ENOMEM;\n\n\taddr = dma_map_single(dev, virt, len, DMA_FROM_DEVICE);\n\tif (dma_mapping_error(dev, addr)) {\n\t\tret = -ENOMEM;\n\t\tgoto out_kfree;\n\t}\n\n\t \n\tipa_endpoint_force_close(endpoint);\n\n\t \n\tgsi_channel_reset(gsi, endpoint->channel_id, false);\n\n\t \n\tsuspended = ipa_endpoint_program_suspend(endpoint, false);\n\n\t \n\tret = gsi_channel_start(gsi, endpoint->channel_id);\n\tif (ret)\n\t\tgoto out_suspend_again;\n\n\tret = gsi_trans_read_byte(gsi, endpoint->channel_id, addr);\n\tif (ret)\n\t\tgoto err_endpoint_stop;\n\n\t \n\tretries = IPA_ENDPOINT_RESET_AGGR_RETRY_MAX;\n\tdo {\n\t\tif (!ipa_endpoint_aggr_active(endpoint))\n\t\t\tbreak;\n\t\tusleep_range(USEC_PER_MSEC, 2 * USEC_PER_MSEC);\n\t} while (retries--);\n\n\t \n\tif (ipa_endpoint_aggr_active(endpoint))\n\t\tdev_err(dev, \"endpoint %u still active during reset\\n\",\n\t\t\tendpoint->endpoint_id);\n\n\tgsi_trans_read_byte_done(gsi, endpoint->channel_id);\n\n\tret = gsi_channel_stop(gsi, endpoint->channel_id);\n\tif (ret)\n\t\tgoto out_suspend_again;\n\n\t \n\tgsi_channel_reset(gsi, endpoint->channel_id, true);\n\n\tusleep_range(USEC_PER_MSEC, 2 * USEC_PER_MSEC);\n\n\tgoto out_suspend_again;\n\nerr_endpoint_stop:\n\t(void)gsi_channel_stop(gsi, endpoint->channel_id);\nout_suspend_again:\n\tif (suspended)\n\t\t(void)ipa_endpoint_program_suspend(endpoint, true);\n\tdma_unmap_single(dev, addr, len, DMA_FROM_DEVICE);\nout_kfree:\n\tkfree(virt);\n\n\treturn ret;\n}\n\nstatic void ipa_endpoint_reset(struct ipa_endpoint *endpoint)\n{\n\tu32 channel_id = endpoint->channel_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tbool special;\n\tint ret = 0;\n\n\t \n\tspecial = ipa->version < IPA_VERSION_4_0 && !endpoint->toward_ipa &&\n\t\t\tendpoint->config.aggregation;\n\tif (special && ipa_endpoint_aggr_active(endpoint))\n\t\tret = ipa_endpoint_reset_rx_aggr(endpoint);\n\telse\n\t\tgsi_channel_reset(&ipa->gsi, channel_id, true);\n\n\tif (ret)\n\t\tdev_err(&ipa->pdev->dev,\n\t\t\t\"error %d resetting channel %u for endpoint %u\\n\",\n\t\t\tret, endpoint->channel_id, endpoint->endpoint_id);\n}\n\nstatic void ipa_endpoint_program(struct ipa_endpoint *endpoint)\n{\n\tif (endpoint->toward_ipa) {\n\t\t \n\t\tif (endpoint->ipa->version < IPA_VERSION_4_2)\n\t\t\tipa_endpoint_program_delay(endpoint, false);\n\t} else {\n\t\t \n\t\t(void)ipa_endpoint_program_suspend(endpoint, false);\n\t}\n\tipa_endpoint_init_cfg(endpoint);\n\tipa_endpoint_init_nat(endpoint);\n\tipa_endpoint_init_hdr(endpoint);\n\tipa_endpoint_init_hdr_ext(endpoint);\n\tipa_endpoint_init_hdr_metadata_mask(endpoint);\n\tipa_endpoint_init_mode(endpoint);\n\tipa_endpoint_init_aggr(endpoint);\n\tif (!endpoint->toward_ipa) {\n\t\tif (endpoint->config.rx.holb_drop)\n\t\t\tipa_endpoint_init_hol_block_enable(endpoint, 0);\n\t\telse\n\t\t\tipa_endpoint_init_hol_block_disable(endpoint);\n\t}\n\tipa_endpoint_init_deaggr(endpoint);\n\tipa_endpoint_init_rsrc_grp(endpoint);\n\tipa_endpoint_init_seq(endpoint);\n\tipa_endpoint_status(endpoint);\n}\n\nint ipa_endpoint_enable_one(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tstruct gsi *gsi = &ipa->gsi;\n\tint ret;\n\n\tret = gsi_channel_start(gsi, endpoint->channel_id);\n\tif (ret) {\n\t\tdev_err(&ipa->pdev->dev,\n\t\t\t\"error %d starting %cX channel %u for endpoint %u\\n\",\n\t\t\tret, endpoint->toward_ipa ? 'T' : 'R',\n\t\t\tendpoint->channel_id, endpoint_id);\n\t\treturn ret;\n\t}\n\n\tif (!endpoint->toward_ipa) {\n\t\tipa_interrupt_suspend_enable(ipa->interrupt, endpoint_id);\n\t\tipa_endpoint_replenish_enable(endpoint);\n\t}\n\n\t__set_bit(endpoint_id, ipa->enabled);\n\n\treturn 0;\n}\n\nvoid ipa_endpoint_disable_one(struct ipa_endpoint *endpoint)\n{\n\tu32 endpoint_id = endpoint->endpoint_id;\n\tstruct ipa *ipa = endpoint->ipa;\n\tstruct gsi *gsi = &ipa->gsi;\n\tint ret;\n\n\tif (!test_bit(endpoint_id, ipa->enabled))\n\t\treturn;\n\n\t__clear_bit(endpoint_id, endpoint->ipa->enabled);\n\n\tif (!endpoint->toward_ipa) {\n\t\tipa_endpoint_replenish_disable(endpoint);\n\t\tipa_interrupt_suspend_disable(ipa->interrupt, endpoint_id);\n\t}\n\n\t \n\tret = gsi_channel_stop(gsi, endpoint->channel_id);\n\tif (ret)\n\t\tdev_err(&ipa->pdev->dev,\n\t\t\t\"error %d attempting to stop endpoint %u\\n\", ret,\n\t\t\tendpoint_id);\n}\n\nvoid ipa_endpoint_suspend_one(struct ipa_endpoint *endpoint)\n{\n\tstruct device *dev = &endpoint->ipa->pdev->dev;\n\tstruct gsi *gsi = &endpoint->ipa->gsi;\n\tint ret;\n\n\tif (!test_bit(endpoint->endpoint_id, endpoint->ipa->enabled))\n\t\treturn;\n\n\tif (!endpoint->toward_ipa) {\n\t\tipa_endpoint_replenish_disable(endpoint);\n\t\t(void)ipa_endpoint_program_suspend(endpoint, true);\n\t}\n\n\tret = gsi_channel_suspend(gsi, endpoint->channel_id);\n\tif (ret)\n\t\tdev_err(dev, \"error %d suspending channel %u\\n\", ret,\n\t\t\tendpoint->channel_id);\n}\n\nvoid ipa_endpoint_resume_one(struct ipa_endpoint *endpoint)\n{\n\tstruct device *dev = &endpoint->ipa->pdev->dev;\n\tstruct gsi *gsi = &endpoint->ipa->gsi;\n\tint ret;\n\n\tif (!test_bit(endpoint->endpoint_id, endpoint->ipa->enabled))\n\t\treturn;\n\n\tif (!endpoint->toward_ipa)\n\t\t(void)ipa_endpoint_program_suspend(endpoint, false);\n\n\tret = gsi_channel_resume(gsi, endpoint->channel_id);\n\tif (ret)\n\t\tdev_err(dev, \"error %d resuming channel %u\\n\", ret,\n\t\t\tendpoint->channel_id);\n\telse if (!endpoint->toward_ipa)\n\t\tipa_endpoint_replenish_enable(endpoint);\n}\n\nvoid ipa_endpoint_suspend(struct ipa *ipa)\n{\n\tif (!ipa->setup_complete)\n\t\treturn;\n\n\tif (ipa->modem_netdev)\n\t\tipa_modem_suspend(ipa->modem_netdev);\n\n\tipa_endpoint_suspend_one(ipa->name_map[IPA_ENDPOINT_AP_LAN_RX]);\n\tipa_endpoint_suspend_one(ipa->name_map[IPA_ENDPOINT_AP_COMMAND_TX]);\n}\n\nvoid ipa_endpoint_resume(struct ipa *ipa)\n{\n\tif (!ipa->setup_complete)\n\t\treturn;\n\n\tipa_endpoint_resume_one(ipa->name_map[IPA_ENDPOINT_AP_COMMAND_TX]);\n\tipa_endpoint_resume_one(ipa->name_map[IPA_ENDPOINT_AP_LAN_RX]);\n\n\tif (ipa->modem_netdev)\n\t\tipa_modem_resume(ipa->modem_netdev);\n}\n\nstatic void ipa_endpoint_setup_one(struct ipa_endpoint *endpoint)\n{\n\tstruct gsi *gsi = &endpoint->ipa->gsi;\n\tu32 channel_id = endpoint->channel_id;\n\n\t \n\tif (endpoint->ee_id != GSI_EE_AP)\n\t\treturn;\n\n\tendpoint->skb_frag_max = gsi->channel[channel_id].trans_tre_max - 1;\n\tif (!endpoint->toward_ipa) {\n\t\t \n\t\tclear_bit(IPA_REPLENISH_ENABLED, endpoint->replenish_flags);\n\t\tclear_bit(IPA_REPLENISH_ACTIVE, endpoint->replenish_flags);\n\t\tINIT_DELAYED_WORK(&endpoint->replenish_work,\n\t\t\t\t  ipa_endpoint_replenish_work);\n\t}\n\n\tipa_endpoint_program(endpoint);\n\n\t__set_bit(endpoint->endpoint_id, endpoint->ipa->set_up);\n}\n\nstatic void ipa_endpoint_teardown_one(struct ipa_endpoint *endpoint)\n{\n\t__clear_bit(endpoint->endpoint_id, endpoint->ipa->set_up);\n\n\tif (!endpoint->toward_ipa)\n\t\tcancel_delayed_work_sync(&endpoint->replenish_work);\n\n\tipa_endpoint_reset(endpoint);\n}\n\nvoid ipa_endpoint_setup(struct ipa *ipa)\n{\n\tu32 endpoint_id;\n\n\tfor_each_set_bit(endpoint_id, ipa->defined, ipa->endpoint_count)\n\t\tipa_endpoint_setup_one(&ipa->endpoint[endpoint_id]);\n}\n\nvoid ipa_endpoint_teardown(struct ipa *ipa)\n{\n\tu32 endpoint_id;\n\n\tfor_each_set_bit(endpoint_id, ipa->set_up, ipa->endpoint_count)\n\t\tipa_endpoint_teardown_one(&ipa->endpoint[endpoint_id]);\n}\n\nvoid ipa_endpoint_deconfig(struct ipa *ipa)\n{\n\tipa->available_count = 0;\n\tbitmap_free(ipa->available);\n\tipa->available = NULL;\n}\n\nint ipa_endpoint_config(struct ipa *ipa)\n{\n\tstruct device *dev = &ipa->pdev->dev;\n\tconst struct reg *reg;\n\tu32 endpoint_id;\n\tu32 hw_limit;\n\tu32 tx_count;\n\tu32 rx_count;\n\tu32 rx_base;\n\tu32 limit;\n\tu32 val;\n\n\t \n\tif (ipa->version < IPA_VERSION_3_5) {\n\t\tipa->available = bitmap_zalloc(IPA_ENDPOINT_MAX, GFP_KERNEL);\n\t\tif (!ipa->available)\n\t\t\treturn -ENOMEM;\n\t\tipa->available_count = IPA_ENDPOINT_MAX;\n\n\t\tbitmap_set(ipa->available, 0, IPA_ENDPOINT_MAX);\n\n\t\treturn 0;\n\t}\n\n\t \n\treg = ipa_reg(ipa, FLAVOR_0);\n\tval = ioread32(ipa->reg_virt + reg_offset(reg));\n\n\t \n\ttx_count = reg_decode(reg, MAX_CONS_PIPES, val);\n\trx_count = reg_decode(reg, MAX_PROD_PIPES, val);\n\trx_base = reg_decode(reg, PROD_LOWEST, val);\n\n\tlimit = rx_base + rx_count;\n\tif (limit > IPA_ENDPOINT_MAX) {\n\t\tdev_err(dev, \"too many endpoints, %u > %u\\n\",\n\t\t\tlimit, IPA_ENDPOINT_MAX);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\thw_limit = ipa->version < IPA_VERSION_5_0 ? 32 : U8_MAX + 1;\n\tif (limit > hw_limit) {\n\t\tdev_err(dev, \"unexpected endpoint count, %u > %u\\n\",\n\t\t\tlimit, hw_limit);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tipa->available = bitmap_zalloc(limit, GFP_KERNEL);\n\tif (!ipa->available)\n\t\treturn -ENOMEM;\n\tipa->available_count = limit;\n\n\t \n\tbitmap_set(ipa->available, 0, tx_count);\n\tbitmap_set(ipa->available, rx_base, rx_count);\n\n\tfor_each_set_bit(endpoint_id, ipa->defined, ipa->endpoint_count) {\n\t\tstruct ipa_endpoint *endpoint;\n\n\t\tif (endpoint_id >= limit) {\n\t\t\tdev_err(dev, \"invalid endpoint id, %u > %u\\n\",\n\t\t\t\tendpoint_id, limit - 1);\n\t\t\tgoto err_free_bitmap;\n\t\t}\n\n\t\tif (!test_bit(endpoint_id, ipa->available)) {\n\t\t\tdev_err(dev, \"unavailable endpoint id %u\\n\",\n\t\t\t\tendpoint_id);\n\t\t\tgoto err_free_bitmap;\n\t\t}\n\n\t\t \n\t\tendpoint = &ipa->endpoint[endpoint_id];\n\t\tif (endpoint->toward_ipa) {\n\t\t\tif (endpoint_id < tx_count)\n\t\t\t\tcontinue;\n\t\t} else if (endpoint_id >= rx_base) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_err(dev, \"endpoint id %u wrong direction\\n\", endpoint_id);\n\t\tgoto err_free_bitmap;\n\t}\n\n\treturn 0;\n\nerr_free_bitmap:\n\tipa_endpoint_deconfig(ipa);\n\n\treturn -EINVAL;\n}\n\nstatic void ipa_endpoint_init_one(struct ipa *ipa, enum ipa_endpoint_name name,\n\t\t\t\t  const struct ipa_gsi_endpoint_data *data)\n{\n\tstruct ipa_endpoint *endpoint;\n\n\tendpoint = &ipa->endpoint[data->endpoint_id];\n\n\tif (data->ee_id == GSI_EE_AP)\n\t\tipa->channel_map[data->channel_id] = endpoint;\n\tipa->name_map[name] = endpoint;\n\n\tendpoint->ipa = ipa;\n\tendpoint->ee_id = data->ee_id;\n\tendpoint->channel_id = data->channel_id;\n\tendpoint->endpoint_id = data->endpoint_id;\n\tendpoint->toward_ipa = data->toward_ipa;\n\tendpoint->config = data->endpoint.config;\n\n\t__set_bit(endpoint->endpoint_id, ipa->defined);\n}\n\nstatic void ipa_endpoint_exit_one(struct ipa_endpoint *endpoint)\n{\n\t__clear_bit(endpoint->endpoint_id, endpoint->ipa->defined);\n\n\tmemset(endpoint, 0, sizeof(*endpoint));\n}\n\nvoid ipa_endpoint_exit(struct ipa *ipa)\n{\n\tu32 endpoint_id;\n\n\tipa->filtered = 0;\n\n\tfor_each_set_bit(endpoint_id, ipa->defined, ipa->endpoint_count)\n\t\tipa_endpoint_exit_one(&ipa->endpoint[endpoint_id]);\n\n\tbitmap_free(ipa->enabled);\n\tipa->enabled = NULL;\n\tbitmap_free(ipa->set_up);\n\tipa->set_up = NULL;\n\tbitmap_free(ipa->defined);\n\tipa->defined = NULL;\n\n\tmemset(ipa->name_map, 0, sizeof(ipa->name_map));\n\tmemset(ipa->channel_map, 0, sizeof(ipa->channel_map));\n}\n\n \nint ipa_endpoint_init(struct ipa *ipa, u32 count,\n\t\t      const struct ipa_gsi_endpoint_data *data)\n{\n\tenum ipa_endpoint_name name;\n\tu32 filtered;\n\n\tBUILD_BUG_ON(!IPA_REPLENISH_BATCH);\n\n\t \n\tipa->endpoint_count = ipa_endpoint_max(ipa, count, data) + 1;\n\tif (!ipa->endpoint_count)\n\t\treturn -EINVAL;\n\n\t \n\tipa->defined = bitmap_zalloc(ipa->endpoint_count, GFP_KERNEL);\n\tif (!ipa->defined)\n\t\treturn -ENOMEM;\n\n\tipa->set_up = bitmap_zalloc(ipa->endpoint_count, GFP_KERNEL);\n\tif (!ipa->set_up)\n\t\tgoto err_free_defined;\n\n\tipa->enabled = bitmap_zalloc(ipa->endpoint_count, GFP_KERNEL);\n\tif (!ipa->enabled)\n\t\tgoto err_free_set_up;\n\n\tfiltered = 0;\n\tfor (name = 0; name < count; name++, data++) {\n\t\tif (ipa_gsi_endpoint_data_empty(data))\n\t\t\tcontinue;\t \n\n\t\tipa_endpoint_init_one(ipa, name, data);\n\n\t\tif (data->endpoint.filter_support)\n\t\t\tfiltered |= BIT(data->endpoint_id);\n\t\tif (data->ee_id == GSI_EE_MODEM && data->toward_ipa)\n\t\t\tipa->modem_tx_count++;\n\t}\n\n\t \n\tif (!ipa_filtered_valid(ipa, filtered)) {\n\t\tipa_endpoint_exit(ipa);\n\n\t\treturn -EINVAL;\n\t}\n\n\tipa->filtered = filtered;\n\n\treturn 0;\n\nerr_free_set_up:\n\tbitmap_free(ipa->set_up);\n\tipa->set_up = NULL;\nerr_free_defined:\n\tbitmap_free(ipa->defined);\n\tipa->defined = NULL;\n\n\treturn -ENOMEM;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}