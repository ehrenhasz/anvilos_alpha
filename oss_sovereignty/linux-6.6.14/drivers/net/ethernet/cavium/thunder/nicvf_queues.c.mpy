{
  "module_name": "nicvf_queues.c",
  "hash_id": "f5a1f72cc67f32267ac21c562d1438296c8be342d1dc644ad7f169ffa33b24b5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/cavium/thunder/nicvf_queues.c",
  "human_readable_source": "\n \n\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/ip.h>\n#include <linux/etherdevice.h>\n#include <linux/iommu.h>\n#include <net/ip.h>\n#include <net/tso.h>\n#include <uapi/linux/bpf.h>\n\n#include \"nic_reg.h\"\n#include \"nic.h\"\n#include \"q_struct.h\"\n#include \"nicvf_queues.h\"\n\nstatic inline void nicvf_sq_add_gather_subdesc(struct snd_queue *sq, int qentry,\n\t\t\t\t\t       int size, u64 data);\nstatic void nicvf_get_page(struct nicvf *nic)\n{\n\tif (!nic->rb_pageref || !nic->rb_page)\n\t\treturn;\n\n\tpage_ref_add(nic->rb_page, nic->rb_pageref);\n\tnic->rb_pageref = 0;\n}\n\n \nstatic int nicvf_poll_reg(struct nicvf *nic, int qidx,\n\t\t\t  u64 reg, int bit_pos, int bits, int val)\n{\n\tu64 bit_mask;\n\tu64 reg_val;\n\tint timeout = 10;\n\n\tbit_mask = (1ULL << bits) - 1;\n\tbit_mask = (bit_mask << bit_pos);\n\n\twhile (timeout) {\n\t\treg_val = nicvf_queue_reg_read(nic, reg, qidx);\n\t\tif (((reg_val & bit_mask) >> bit_pos) == val)\n\t\t\treturn 0;\n\t\tusleep_range(1000, 2000);\n\t\ttimeout--;\n\t}\n\tnetdev_err(nic->netdev, \"Poll on reg 0x%llx failed\\n\", reg);\n\treturn 1;\n}\n\n \nstatic int nicvf_alloc_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem,\n\t\t\t\t  int q_len, int desc_size, int align_bytes)\n{\n\tdmem->q_len = q_len;\n\tdmem->size = (desc_size * q_len) + align_bytes;\n\t \n\tdmem->unalign_base = dma_alloc_coherent(&nic->pdev->dev, dmem->size,\n\t\t\t\t\t\t&dmem->dma, GFP_KERNEL);\n\tif (!dmem->unalign_base)\n\t\treturn -ENOMEM;\n\n\t \n\tdmem->phys_base = NICVF_ALIGNED_ADDR((u64)dmem->dma, align_bytes);\n\tdmem->base = dmem->unalign_base + (dmem->phys_base - dmem->dma);\n\treturn 0;\n}\n\n \nstatic void nicvf_free_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem)\n{\n\tif (!dmem)\n\t\treturn;\n\n\tdma_free_coherent(&nic->pdev->dev, dmem->size,\n\t\t\t  dmem->unalign_base, dmem->dma);\n\tdmem->unalign_base = NULL;\n\tdmem->base = NULL;\n}\n\n#define XDP_PAGE_REFCNT_REFILL 256\n\n \nstatic inline struct pgcache *nicvf_alloc_page(struct nicvf *nic,\n\t\t\t\t\t       struct rbdr *rbdr, gfp_t gfp)\n{\n\tint ref_count;\n\tstruct page *page = NULL;\n\tstruct pgcache *pgcache, *next;\n\n\t \n\tpgcache = &rbdr->pgcache[rbdr->pgidx];\n\tpage = pgcache->page;\n\t \n\tif (page) {\n\t\tref_count = page_ref_count(page);\n\t\t \n\t\tif (rbdr->is_xdp) {\n\t\t\tif (ref_count == pgcache->ref_count)\n\t\t\t\tpgcache->ref_count--;\n\t\t\telse\n\t\t\t\tpage = NULL;\n\t\t} else if (ref_count != 1) {\n\t\t\tpage = NULL;\n\t\t}\n\t}\n\n\tif (!page) {\n\t\tpage = alloc_pages(gfp | __GFP_COMP | __GFP_NOWARN, 0);\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\tthis_cpu_inc(nic->pnicvf->drv_stats->page_alloc);\n\n\t\t \n\t\tif (rbdr->pgalloc >= rbdr->pgcnt) {\n\t\t\t \n\t\t\tnic->rb_page = page;\n\t\t\treturn NULL;\n\t\t}\n\n\t\t \n\t\tpgcache->page = page;\n\t\tpgcache->dma_addr = 0;\n\t\tpgcache->ref_count = 0;\n\t\trbdr->pgalloc++;\n\t}\n\n\t \n\tif (rbdr->is_xdp) {\n\t\t \n\t\tif (!pgcache->ref_count) {\n\t\t\tpgcache->ref_count = XDP_PAGE_REFCNT_REFILL;\n\t\t\tpage_ref_add(page, XDP_PAGE_REFCNT_REFILL);\n\t\t}\n\t} else {\n\t\t \n\t\tpage_ref_add(page, 1);\n\t}\n\n\trbdr->pgidx++;\n\trbdr->pgidx &= (rbdr->pgcnt - 1);\n\n\t \n\tnext = &rbdr->pgcache[rbdr->pgidx];\n\tpage = next->page;\n\tif (page)\n\t\tprefetch(&page->_refcount);\n\n\treturn pgcache;\n}\n\n \nstatic inline int nicvf_alloc_rcv_buffer(struct nicvf *nic, struct rbdr *rbdr,\n\t\t\t\t\t gfp_t gfp, u32 buf_len, u64 *rbuf)\n{\n\tstruct pgcache *pgcache = NULL;\n\n\t \n\tif (!rbdr->is_xdp && nic->rb_page &&\n\t    ((nic->rb_page_offset + buf_len) <= PAGE_SIZE)) {\n\t\tnic->rb_pageref++;\n\t\tgoto ret;\n\t}\n\n\tnicvf_get_page(nic);\n\tnic->rb_page = NULL;\n\n\t \n\tpgcache = nicvf_alloc_page(nic, rbdr, gfp);\n\tif (!pgcache && !nic->rb_page) {\n\t\tthis_cpu_inc(nic->pnicvf->drv_stats->rcv_buffer_alloc_failures);\n\t\treturn -ENOMEM;\n\t}\n\n\tnic->rb_page_offset = 0;\n\n\t \n\tif (rbdr->is_xdp)\n\t\tbuf_len += XDP_PACKET_HEADROOM;\n\n\t \n\tif (pgcache)\n\t\tnic->rb_page = pgcache->page;\nret:\n\tif (rbdr->is_xdp && pgcache && pgcache->dma_addr) {\n\t\t*rbuf = pgcache->dma_addr;\n\t} else {\n\t\t \n\t\t*rbuf = (u64)dma_map_page_attrs(&nic->pdev->dev, nic->rb_page,\n\t\t\t\t\t\tnic->rb_page_offset, buf_len,\n\t\t\t\t\t\tDMA_FROM_DEVICE,\n\t\t\t\t\t\tDMA_ATTR_SKIP_CPU_SYNC);\n\t\tif (dma_mapping_error(&nic->pdev->dev, (dma_addr_t)*rbuf)) {\n\t\t\tif (!nic->rb_page_offset)\n\t\t\t\t__free_pages(nic->rb_page, 0);\n\t\t\tnic->rb_page = NULL;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (pgcache)\n\t\t\tpgcache->dma_addr = *rbuf + XDP_PACKET_HEADROOM;\n\t\tnic->rb_page_offset += buf_len;\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct sk_buff *nicvf_rb_ptr_to_skb(struct nicvf *nic,\n\t\t\t\t\t   u64 rb_ptr, int len)\n{\n\tvoid *data;\n\tstruct sk_buff *skb;\n\n\tdata = phys_to_virt(rb_ptr);\n\n\t \n\tskb = build_skb(data, RCV_FRAG_LEN);\n\tif (!skb) {\n\t\tput_page(virt_to_page(data));\n\t\treturn NULL;\n\t}\n\n\tprefetch(skb->data);\n\treturn skb;\n}\n\n \nstatic int  nicvf_init_rbdr(struct nicvf *nic, struct rbdr *rbdr,\n\t\t\t    int ring_len, int buf_size)\n{\n\tint idx;\n\tu64 rbuf;\n\tstruct rbdr_entry_t *desc;\n\tint err;\n\n\terr = nicvf_alloc_q_desc_mem(nic, &rbdr->dmem, ring_len,\n\t\t\t\t     sizeof(struct rbdr_entry_t),\n\t\t\t\t     NICVF_RCV_BUF_ALIGN_BYTES);\n\tif (err)\n\t\treturn err;\n\n\trbdr->desc = rbdr->dmem.base;\n\t \n\trbdr->dma_size = buf_size;\n\trbdr->enable = true;\n\trbdr->thresh = RBDR_THRESH;\n\trbdr->head = 0;\n\trbdr->tail = 0;\n\n\t \n\tif (!nic->pnicvf->xdp_prog) {\n\t\trbdr->pgcnt = ring_len / (PAGE_SIZE / buf_size);\n\t\trbdr->is_xdp = false;\n\t} else {\n\t\trbdr->pgcnt = ring_len;\n\t\trbdr->is_xdp = true;\n\t}\n\trbdr->pgcnt = roundup_pow_of_two(rbdr->pgcnt);\n\trbdr->pgcache = kcalloc(rbdr->pgcnt, sizeof(*rbdr->pgcache),\n\t\t\t\tGFP_KERNEL);\n\tif (!rbdr->pgcache)\n\t\treturn -ENOMEM;\n\trbdr->pgidx = 0;\n\trbdr->pgalloc = 0;\n\n\tnic->rb_page = NULL;\n\tfor (idx = 0; idx < ring_len; idx++) {\n\t\terr = nicvf_alloc_rcv_buffer(nic, rbdr, GFP_KERNEL,\n\t\t\t\t\t     RCV_FRAG_LEN, &rbuf);\n\t\tif (err) {\n\t\t\t \n\t\t\trbdr->tail = idx - 1;\n\t\t\treturn err;\n\t\t}\n\n\t\tdesc = GET_RBDR_DESC(rbdr, idx);\n\t\tdesc->buf_addr = rbuf & ~(NICVF_RCV_BUF_ALIGN_BYTES - 1);\n\t}\n\n\tnicvf_get_page(nic);\n\n\treturn 0;\n}\n\n \nstatic void nicvf_free_rbdr(struct nicvf *nic, struct rbdr *rbdr)\n{\n\tint head, tail;\n\tu64 buf_addr, phys_addr;\n\tstruct pgcache *pgcache;\n\tstruct rbdr_entry_t *desc;\n\n\tif (!rbdr)\n\t\treturn;\n\n\trbdr->enable = false;\n\tif (!rbdr->dmem.base)\n\t\treturn;\n\n\thead = rbdr->head;\n\ttail = rbdr->tail;\n\n\t \n\twhile (head != tail) {\n\t\tdesc = GET_RBDR_DESC(rbdr, head);\n\t\tbuf_addr = desc->buf_addr;\n\t\tphys_addr = nicvf_iova_to_phys(nic, buf_addr);\n\t\tdma_unmap_page_attrs(&nic->pdev->dev, buf_addr, RCV_FRAG_LEN,\n\t\t\t\t     DMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\n\t\tif (phys_addr)\n\t\t\tput_page(virt_to_page(phys_to_virt(phys_addr)));\n\t\thead++;\n\t\thead &= (rbdr->dmem.q_len - 1);\n\t}\n\t \n\tdesc = GET_RBDR_DESC(rbdr, tail);\n\tbuf_addr = desc->buf_addr;\n\tphys_addr = nicvf_iova_to_phys(nic, buf_addr);\n\tdma_unmap_page_attrs(&nic->pdev->dev, buf_addr, RCV_FRAG_LEN,\n\t\t\t     DMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\n\tif (phys_addr)\n\t\tput_page(virt_to_page(phys_to_virt(phys_addr)));\n\n\t \n\tsmp_rmb();\n\n\t \n\thead = 0;\n\twhile (head < rbdr->pgcnt) {\n\t\tpgcache = &rbdr->pgcache[head];\n\t\tif (pgcache->page && page_ref_count(pgcache->page) != 0) {\n\t\t\tif (rbdr->is_xdp) {\n\t\t\t\tpage_ref_sub(pgcache->page,\n\t\t\t\t\t     pgcache->ref_count - 1);\n\t\t\t}\n\t\t\tput_page(pgcache->page);\n\t\t}\n\t\thead++;\n\t}\n\n\t \n\tnicvf_free_q_desc_mem(nic, &rbdr->dmem);\n}\n\n \nstatic void nicvf_refill_rbdr(struct nicvf *nic, gfp_t gfp)\n{\n\tstruct queue_set *qs = nic->qs;\n\tint rbdr_idx = qs->rbdr_cnt;\n\tint tail, qcount;\n\tint refill_rb_cnt;\n\tstruct rbdr *rbdr;\n\tstruct rbdr_entry_t *desc;\n\tu64 rbuf;\n\tint new_rb = 0;\n\nrefill:\n\tif (!rbdr_idx)\n\t\treturn;\n\trbdr_idx--;\n\trbdr = &qs->rbdr[rbdr_idx];\n\t \n\tif (!rbdr->enable)\n\t\tgoto next_rbdr;\n\n\t \n\tqcount = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0, rbdr_idx);\n\tqcount &= 0x7FFFF;\n\t \n\tif (qcount >= (qs->rbdr_len - 1))\n\t\tgoto next_rbdr;\n\telse\n\t\trefill_rb_cnt = qs->rbdr_len - qcount - 1;\n\n\t \n\tsmp_rmb();\n\n\t \n\ttail = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_TAIL, rbdr_idx) >> 3;\n\twhile (refill_rb_cnt) {\n\t\ttail++;\n\t\ttail &= (rbdr->dmem.q_len - 1);\n\n\t\tif (nicvf_alloc_rcv_buffer(nic, rbdr, gfp, RCV_FRAG_LEN, &rbuf))\n\t\t\tbreak;\n\n\t\tdesc = GET_RBDR_DESC(rbdr, tail);\n\t\tdesc->buf_addr = rbuf & ~(NICVF_RCV_BUF_ALIGN_BYTES - 1);\n\t\trefill_rb_cnt--;\n\t\tnew_rb++;\n\t}\n\n\tnicvf_get_page(nic);\n\n\t \n\tsmp_wmb();\n\n\t \n\tif (refill_rb_cnt)\n\t\tnic->rb_alloc_fail = true;\n\telse\n\t\tnic->rb_alloc_fail = false;\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,\n\t\t\t      rbdr_idx, new_rb);\nnext_rbdr:\n\t \n\tif (!nic->rb_alloc_fail && rbdr->enable &&\n\t    netif_running(nic->pnicvf->netdev))\n\t\tnicvf_enable_intr(nic, NICVF_INTR_RBDR, rbdr_idx);\n\n\tif (rbdr_idx)\n\t\tgoto refill;\n}\n\n \nvoid nicvf_rbdr_work(struct work_struct *work)\n{\n\tstruct nicvf *nic = container_of(work, struct nicvf, rbdr_work.work);\n\n\tnicvf_refill_rbdr(nic, GFP_KERNEL);\n\tif (nic->rb_alloc_fail)\n\t\tschedule_delayed_work(&nic->rbdr_work, msecs_to_jiffies(10));\n\telse\n\t\tnic->rb_work_scheduled = false;\n}\n\n \nvoid nicvf_rbdr_task(struct tasklet_struct *t)\n{\n\tstruct nicvf *nic = from_tasklet(nic, t, rbdr_task);\n\n\tnicvf_refill_rbdr(nic, GFP_ATOMIC);\n\tif (nic->rb_alloc_fail) {\n\t\tnic->rb_work_scheduled = true;\n\t\tschedule_delayed_work(&nic->rbdr_work, msecs_to_jiffies(10));\n\t}\n}\n\n \nstatic int nicvf_init_cmp_queue(struct nicvf *nic,\n\t\t\t\tstruct cmp_queue *cq, int q_len)\n{\n\tint err;\n\n\terr = nicvf_alloc_q_desc_mem(nic, &cq->dmem, q_len, CMP_QUEUE_DESC_SIZE,\n\t\t\t\t     NICVF_CQ_BASE_ALIGN_BYTES);\n\tif (err)\n\t\treturn err;\n\n\tcq->desc = cq->dmem.base;\n\tcq->thresh = pass1_silicon(nic->pdev) ? 0 : CMP_QUEUE_CQE_THRESH;\n\tnic->cq_coalesce_usecs = (CMP_QUEUE_TIMER_THRESH * 0.05) - 1;\n\n\treturn 0;\n}\n\nstatic void nicvf_free_cmp_queue(struct nicvf *nic, struct cmp_queue *cq)\n{\n\tif (!cq)\n\t\treturn;\n\tif (!cq->dmem.base)\n\t\treturn;\n\n\tnicvf_free_q_desc_mem(nic, &cq->dmem);\n}\n\n \nstatic int nicvf_init_snd_queue(struct nicvf *nic,\n\t\t\t\tstruct snd_queue *sq, int q_len, int qidx)\n{\n\tint err;\n\n\terr = nicvf_alloc_q_desc_mem(nic, &sq->dmem, q_len, SND_QUEUE_DESC_SIZE,\n\t\t\t\t     NICVF_SQ_BASE_ALIGN_BYTES);\n\tif (err)\n\t\treturn err;\n\n\tsq->desc = sq->dmem.base;\n\tsq->skbuff = kcalloc(q_len, sizeof(u64), GFP_KERNEL);\n\tif (!sq->skbuff)\n\t\treturn -ENOMEM;\n\n\tsq->head = 0;\n\tsq->tail = 0;\n\tsq->thresh = SND_QUEUE_THRESH;\n\n\t \n\tif (nic->sqs_mode)\n\t\tqidx += ((nic->sqs_id + 1) * MAX_SND_QUEUES_PER_QS);\n\tif (qidx < nic->pnicvf->xdp_tx_queues) {\n\t\t \n\t\tsq->xdp_page = kcalloc(q_len, sizeof(u64), GFP_KERNEL);\n\t\tif (!sq->xdp_page)\n\t\t\treturn -ENOMEM;\n\t\tsq->xdp_desc_cnt = 0;\n\t\tsq->xdp_free_cnt = q_len - 1;\n\t\tsq->is_xdp = true;\n\t} else {\n\t\tsq->xdp_page = NULL;\n\t\tsq->xdp_desc_cnt = 0;\n\t\tsq->xdp_free_cnt = 0;\n\t\tsq->is_xdp = false;\n\n\t\tatomic_set(&sq->free_cnt, q_len - 1);\n\n\t\t \n\t\tsq->tso_hdrs = dma_alloc_coherent(&nic->pdev->dev,\n\t\t\t\t\t\t  q_len * TSO_HEADER_SIZE,\n\t\t\t\t\t\t  &sq->tso_hdrs_phys,\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!sq->tso_hdrs)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nvoid nicvf_unmap_sndq_buffers(struct nicvf *nic, struct snd_queue *sq,\n\t\t\t      int hdr_sqe, u8 subdesc_cnt)\n{\n\tu8 idx;\n\tstruct sq_gather_subdesc *gather;\n\n\t \n\tfor (idx = 0; idx < subdesc_cnt; idx++) {\n\t\thdr_sqe++;\n\t\thdr_sqe &= (sq->dmem.q_len - 1);\n\t\tgather = (struct sq_gather_subdesc *)GET_SQ_DESC(sq, hdr_sqe);\n\t\t \n\t\tdma_unmap_page_attrs(&nic->pdev->dev, gather->addr,\n\t\t\t\t     gather->size, DMA_TO_DEVICE,\n\t\t\t\t     DMA_ATTR_SKIP_CPU_SYNC);\n\t}\n}\n\nstatic void nicvf_free_snd_queue(struct nicvf *nic, struct snd_queue *sq)\n{\n\tstruct sk_buff *skb;\n\tstruct page *page;\n\tstruct sq_hdr_subdesc *hdr;\n\tstruct sq_hdr_subdesc *tso_sqe;\n\n\tif (!sq)\n\t\treturn;\n\tif (!sq->dmem.base)\n\t\treturn;\n\n\tif (sq->tso_hdrs) {\n\t\tdma_free_coherent(&nic->pdev->dev,\n\t\t\t\t  sq->dmem.q_len * TSO_HEADER_SIZE,\n\t\t\t\t  sq->tso_hdrs, sq->tso_hdrs_phys);\n\t\tsq->tso_hdrs = NULL;\n\t}\n\n\t \n\tsmp_rmb();\n\twhile (sq->head != sq->tail) {\n\t\tskb = (struct sk_buff *)sq->skbuff[sq->head];\n\t\tif (!skb || !sq->xdp_page)\n\t\t\tgoto next;\n\n\t\tpage = (struct page *)sq->xdp_page[sq->head];\n\t\tif (!page)\n\t\t\tgoto next;\n\t\telse\n\t\t\tput_page(page);\n\n\t\thdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, sq->head);\n\t\t \n\t\tif (hdr->dont_send) {\n\t\t\t \n\t\t\ttso_sqe =\n\t\t\t (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, hdr->rsvd2);\n\t\t\tnicvf_unmap_sndq_buffers(nic, sq, hdr->rsvd2,\n\t\t\t\t\t\t tso_sqe->subdesc_cnt);\n\t\t} else {\n\t\t\tnicvf_unmap_sndq_buffers(nic, sq, sq->head,\n\t\t\t\t\t\t hdr->subdesc_cnt);\n\t\t}\n\t\tif (skb)\n\t\t\tdev_kfree_skb_any(skb);\nnext:\n\t\tsq->head++;\n\t\tsq->head &= (sq->dmem.q_len - 1);\n\t}\n\tkfree(sq->skbuff);\n\tkfree(sq->xdp_page);\n\tnicvf_free_q_desc_mem(nic, &sq->dmem);\n}\n\nstatic void nicvf_reclaim_snd_queue(struct nicvf *nic,\n\t\t\t\t    struct queue_set *qs, int qidx)\n{\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, 0);\n\t \n\tif (nicvf_poll_reg(nic, qidx, NIC_QSET_SQ_0_7_STATUS, 21, 1, 0x01))\n\t\treturn;\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, NICVF_SQ_RESET);\n}\n\nstatic void nicvf_reclaim_rcv_queue(struct nicvf *nic,\n\t\t\t\t    struct queue_set *qs, int qidx)\n{\n\tunion nic_mbx mbx = {};\n\n\t \n\tmbx.msg.msg = NIC_MBOX_MSG_RQ_SW_SYNC;\n\tnicvf_send_msg_to_pf(nic, &mbx);\n}\n\nstatic void nicvf_reclaim_cmp_queue(struct nicvf *nic,\n\t\t\t\t    struct queue_set *qs, int qidx)\n{\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2, qidx, 0);\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, 0);\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, NICVF_CQ_RESET);\n}\n\nstatic void nicvf_reclaim_rbdr(struct nicvf *nic,\n\t\t\t       struct rbdr *rbdr, int qidx)\n{\n\tu64 tmp, fifo_state;\n\tint timeout = 10;\n\n\t \n\trbdr->head = nicvf_queue_reg_read(nic,\n\t\t\t\t\t  NIC_QSET_RBDR_0_1_HEAD,\n\t\t\t\t\t  qidx) >> 3;\n\trbdr->tail = nicvf_queue_reg_read(nic,\n\t\t\t\t\t  NIC_QSET_RBDR_0_1_TAIL,\n\t\t\t\t\t  qidx) >> 3;\n\n\t \n\tfifo_state = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0, qidx);\n\tif (((fifo_state >> 62) & 0x03) == 0x3)\n\t\tnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,\n\t\t\t\t      qidx, NICVF_RBDR_RESET);\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0);\n\tif (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x00))\n\t\treturn;\n\twhile (1) {\n\t\ttmp = nicvf_queue_reg_read(nic,\n\t\t\t\t\t   NIC_QSET_RBDR_0_1_PREFETCH_STATUS,\n\t\t\t\t\t   qidx);\n\t\tif ((tmp & 0xFFFFFFFF) == ((tmp >> 32) & 0xFFFFFFFF))\n\t\t\tbreak;\n\t\tusleep_range(1000, 2000);\n\t\ttimeout--;\n\t\tif (!timeout) {\n\t\t\tnetdev_err(nic->netdev,\n\t\t\t\t   \"Failed polling on prefetch status\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\tnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,\n\t\t\t      qidx, NICVF_RBDR_RESET);\n\n\tif (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x02))\n\t\treturn;\n\tnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0x00);\n\tif (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x00))\n\t\treturn;\n}\n\nvoid nicvf_config_vlan_stripping(struct nicvf *nic, netdev_features_t features)\n{\n\tu64 rq_cfg;\n\tint sqs;\n\n\trq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_RQ_GEN_CFG, 0);\n\n\t \n\tif (features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\trq_cfg |= (1ULL << 25);\n\telse\n\t\trq_cfg &= ~(1ULL << 25);\n\tnicvf_queue_reg_write(nic, NIC_QSET_RQ_GEN_CFG, 0, rq_cfg);\n\n\t \n\tfor (sqs = 0; sqs < nic->sqs_count; sqs++)\n\t\tif (nic->snicvf[sqs])\n\t\t\tnicvf_queue_reg_write(nic->snicvf[sqs],\n\t\t\t\t\t      NIC_QSET_RQ_GEN_CFG, 0, rq_cfg);\n}\n\nstatic void nicvf_reset_rcv_queue_stats(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\n\t \n\tmbx.reset_stat.msg = NIC_MBOX_MSG_RESET_STAT_COUNTER;\n\tmbx.reset_stat.rx_stat_mask = 0x3FFF;\n\tmbx.reset_stat.tx_stat_mask = 0x1F;\n\tmbx.reset_stat.rq_stat_mask = 0xFFFF;\n\tmbx.reset_stat.sq_stat_mask = 0xFFFF;\n\tnicvf_send_msg_to_pf(nic, &mbx);\n}\n\n \nstatic void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,\n\t\t\t\t   int qidx, bool enable)\n{\n\tunion nic_mbx mbx = {};\n\tstruct rcv_queue *rq;\n\tstruct rq_cfg rq_cfg;\n\n\trq = &qs->rq[qidx];\n\trq->enable = enable;\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, 0);\n\n\tif (!rq->enable) {\n\t\tnicvf_reclaim_rcv_queue(nic, qs, qidx);\n\t\txdp_rxq_info_unreg(&rq->xdp_rxq);\n\t\treturn;\n\t}\n\n\trq->cq_qs = qs->vnic_id;\n\trq->cq_idx = qidx;\n\trq->start_rbdr_qs = qs->vnic_id;\n\trq->start_qs_rbdr_idx = qs->rbdr_cnt - 1;\n\trq->cont_rbdr_qs = qs->vnic_id;\n\trq->cont_qs_rbdr_idx = qs->rbdr_cnt - 1;\n\t \n\trq->caching = 1;\n\n\t \n\tWARN_ON(xdp_rxq_info_reg(&rq->xdp_rxq, nic->netdev, qidx, 0) < 0);\n\n\t \n\tmbx.rq.msg = NIC_MBOX_MSG_RQ_CFG;\n\tmbx.rq.qs_num = qs->vnic_id;\n\tmbx.rq.rq_num = qidx;\n\tmbx.rq.cfg = ((u64)rq->caching << 26) | (rq->cq_qs << 19) |\n\t\t\t  (rq->cq_idx << 16) | (rq->cont_rbdr_qs << 9) |\n\t\t\t  (rq->cont_qs_rbdr_idx << 8) |\n\t\t\t  (rq->start_rbdr_qs << 1) | (rq->start_qs_rbdr_idx);\n\tnicvf_send_msg_to_pf(nic, &mbx);\n\n\tmbx.rq.msg = NIC_MBOX_MSG_RQ_BP_CFG;\n\tmbx.rq.cfg = BIT_ULL(63) | BIT_ULL(62) |\n\t\t     (RQ_PASS_RBDR_LVL << 16) | (RQ_PASS_CQ_LVL << 8) |\n\t\t     (qs->vnic_id << 0);\n\tnicvf_send_msg_to_pf(nic, &mbx);\n\n\t \n\tmbx.rq.msg = NIC_MBOX_MSG_RQ_DROP_CFG;\n\tmbx.rq.cfg = BIT_ULL(63) | BIT_ULL(62) |\n\t\t     (RQ_PASS_RBDR_LVL << 40) | (RQ_DROP_RBDR_LVL << 32) |\n\t\t     (RQ_PASS_CQ_LVL << 16) | (RQ_DROP_CQ_LVL << 8);\n\tnicvf_send_msg_to_pf(nic, &mbx);\n\n\tif (!nic->sqs_mode && (qidx == 0)) {\n\t\t \n\t\tnicvf_queue_reg_write(nic, NIC_QSET_RQ_GEN_CFG, 0,\n\t\t\t\t      (BIT(24) | BIT(23) | BIT(21) | BIT(20)));\n\t\tnicvf_config_vlan_stripping(nic, nic->netdev->features);\n\t}\n\n\t \n\tmemset(&rq_cfg, 0, sizeof(struct rq_cfg));\n\trq_cfg.ena = 1;\n\trq_cfg.tcp_ena = 0;\n\tnicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, *(u64 *)&rq_cfg);\n}\n\n \nvoid nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,\n\t\t\t    int qidx, bool enable)\n{\n\tstruct cmp_queue *cq;\n\tstruct cq_cfg cq_cfg;\n\n\tcq = &qs->cq[qidx];\n\tcq->enable = enable;\n\n\tif (!cq->enable) {\n\t\tnicvf_reclaim_cmp_queue(nic, qs, qidx);\n\t\treturn;\n\t}\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, NICVF_CQ_RESET);\n\n\tif (!cq->enable)\n\t\treturn;\n\n\tspin_lock_init(&cq->lock);\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_BASE,\n\t\t\t      qidx, (u64)(cq->dmem.phys_base));\n\n\t \n\tmemset(&cq_cfg, 0, sizeof(struct cq_cfg));\n\tcq_cfg.ena = 1;\n\tcq_cfg.reset = 0;\n\tcq_cfg.caching = 0;\n\tcq_cfg.qsize = ilog2(qs->cq_len >> 10);\n\tcq_cfg.avg_con = 0;\n\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, *(u64 *)&cq_cfg);\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_THRESH, qidx, cq->thresh);\n\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2,\n\t\t\t      qidx, CMP_QUEUE_TIMER_THRESH);\n}\n\n \nstatic void nicvf_snd_queue_config(struct nicvf *nic, struct queue_set *qs,\n\t\t\t\t   int qidx, bool enable)\n{\n\tunion nic_mbx mbx = {};\n\tstruct snd_queue *sq;\n\tstruct sq_cfg sq_cfg;\n\n\tsq = &qs->sq[qidx];\n\tsq->enable = enable;\n\n\tif (!sq->enable) {\n\t\tnicvf_reclaim_snd_queue(nic, qs, qidx);\n\t\treturn;\n\t}\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, NICVF_SQ_RESET);\n\n\tsq->cq_qs = qs->vnic_id;\n\tsq->cq_idx = qidx;\n\n\t \n\tmbx.sq.msg = NIC_MBOX_MSG_SQ_CFG;\n\tmbx.sq.qs_num = qs->vnic_id;\n\tmbx.sq.sq_num = qidx;\n\tmbx.sq.sqs_mode = nic->sqs_mode;\n\tmbx.sq.cfg = (sq->cq_qs << 3) | sq->cq_idx;\n\tnicvf_send_msg_to_pf(nic, &mbx);\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_BASE,\n\t\t\t      qidx, (u64)(sq->dmem.phys_base));\n\n\t \n\tmemset(&sq_cfg, 0, sizeof(struct sq_cfg));\n\tsq_cfg.ena = 1;\n\tsq_cfg.reset = 0;\n\tsq_cfg.ldwb = 0;\n\tsq_cfg.qsize = ilog2(qs->sq_len >> 10);\n\tsq_cfg.tstmp_bgx_intf = 0;\n\t \n\tsq_cfg.cq_limit = (CMP_QUEUE_PIPELINE_RSVD * 256) / qs->cq_len;\n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, *(u64 *)&sq_cfg);\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_THRESH, qidx, sq->thresh);\n\n\t \n\tif (cpu_online(qidx)) {\n\t\tcpumask_set_cpu(qidx, &sq->affinity_mask);\n\t\tnetif_set_xps_queue(nic->netdev,\n\t\t\t\t    &sq->affinity_mask, qidx);\n\t}\n}\n\n \nstatic void nicvf_rbdr_config(struct nicvf *nic, struct queue_set *qs,\n\t\t\t      int qidx, bool enable)\n{\n\tstruct rbdr *rbdr;\n\tstruct rbdr_cfg rbdr_cfg;\n\n\trbdr = &qs->rbdr[qidx];\n\tnicvf_reclaim_rbdr(nic, rbdr, qidx);\n\tif (!enable)\n\t\treturn;\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_BASE,\n\t\t\t      qidx, (u64)(rbdr->dmem.phys_base));\n\n\t \n\t \n\tmemset(&rbdr_cfg, 0, sizeof(struct rbdr_cfg));\n\trbdr_cfg.ena = 1;\n\trbdr_cfg.reset = 0;\n\trbdr_cfg.ldwb = 0;\n\trbdr_cfg.qsize = RBDR_SIZE;\n\trbdr_cfg.avg_con = 0;\n\trbdr_cfg.lines = rbdr->dma_size / 128;\n\tnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,\n\t\t\t      qidx, *(u64 *)&rbdr_cfg);\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,\n\t\t\t      qidx, qs->rbdr_len - 1);\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_THRESH,\n\t\t\t      qidx, rbdr->thresh - 1);\n}\n\n \nvoid nicvf_qset_config(struct nicvf *nic, bool enable)\n{\n\tunion nic_mbx mbx = {};\n\tstruct queue_set *qs = nic->qs;\n\tstruct qs_cfg *qs_cfg;\n\n\tif (!qs) {\n\t\tnetdev_warn(nic->netdev,\n\t\t\t    \"Qset is still not allocated, don't init queues\\n\");\n\t\treturn;\n\t}\n\n\tqs->enable = enable;\n\tqs->vnic_id = nic->vf_id;\n\n\t \n\tmbx.qs.msg = NIC_MBOX_MSG_QS_CFG;\n\tmbx.qs.num = qs->vnic_id;\n\tmbx.qs.sqs_count = nic->sqs_count;\n\n\tmbx.qs.cfg = 0;\n\tqs_cfg = (struct qs_cfg *)&mbx.qs.cfg;\n\tif (qs->enable) {\n\t\tqs_cfg->ena = 1;\n#ifdef __BIG_ENDIAN\n\t\tqs_cfg->be = 1;\n#endif\n\t\tqs_cfg->vnic = qs->vnic_id;\n\t\t \n\t\tif (nic->ptp_clock)\n\t\t\tqs_cfg->send_tstmp_ena = 1;\n\t}\n\tnicvf_send_msg_to_pf(nic, &mbx);\n}\n\nstatic void nicvf_free_resources(struct nicvf *nic)\n{\n\tint qidx;\n\tstruct queue_set *qs = nic->qs;\n\n\t \n\tfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++)\n\t\tnicvf_free_rbdr(nic, &qs->rbdr[qidx]);\n\n\t \n\tfor (qidx = 0; qidx < qs->cq_cnt; qidx++)\n\t\tnicvf_free_cmp_queue(nic, &qs->cq[qidx]);\n\n\t \n\tfor (qidx = 0; qidx < qs->sq_cnt; qidx++)\n\t\tnicvf_free_snd_queue(nic, &qs->sq[qidx]);\n}\n\nstatic int nicvf_alloc_resources(struct nicvf *nic)\n{\n\tint qidx;\n\tstruct queue_set *qs = nic->qs;\n\n\t \n\tfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++) {\n\t\tif (nicvf_init_rbdr(nic, &qs->rbdr[qidx], qs->rbdr_len,\n\t\t\t\t    DMA_BUFFER_LEN))\n\t\t\tgoto alloc_fail;\n\t}\n\n\t \n\tfor (qidx = 0; qidx < qs->sq_cnt; qidx++) {\n\t\tif (nicvf_init_snd_queue(nic, &qs->sq[qidx], qs->sq_len, qidx))\n\t\t\tgoto alloc_fail;\n\t}\n\n\t \n\tfor (qidx = 0; qidx < qs->cq_cnt; qidx++) {\n\t\tif (nicvf_init_cmp_queue(nic, &qs->cq[qidx], qs->cq_len))\n\t\t\tgoto alloc_fail;\n\t}\n\n\treturn 0;\nalloc_fail:\n\tnicvf_free_resources(nic);\n\treturn -ENOMEM;\n}\n\nint nicvf_set_qset_resources(struct nicvf *nic)\n{\n\tstruct queue_set *qs;\n\n\tqs = devm_kzalloc(&nic->pdev->dev, sizeof(*qs), GFP_KERNEL);\n\tif (!qs)\n\t\treturn -ENOMEM;\n\tnic->qs = qs;\n\n\t \n\tqs->rbdr_cnt = DEFAULT_RBDR_CNT;\n\tqs->rq_cnt = min_t(u8, MAX_RCV_QUEUES_PER_QS, num_online_cpus());\n\tqs->sq_cnt = min_t(u8, MAX_SND_QUEUES_PER_QS, num_online_cpus());\n\tqs->cq_cnt = max_t(u8, qs->rq_cnt, qs->sq_cnt);\n\n\t \n\tqs->rbdr_len = RCV_BUF_COUNT;\n\tqs->sq_len = SND_QUEUE_LEN;\n\tqs->cq_len = CMP_QUEUE_LEN;\n\n\tnic->rx_queues = qs->rq_cnt;\n\tnic->tx_queues = qs->sq_cnt;\n\tnic->xdp_tx_queues = 0;\n\n\treturn 0;\n}\n\nint nicvf_config_data_transfer(struct nicvf *nic, bool enable)\n{\n\tbool disable = false;\n\tstruct queue_set *qs = nic->qs;\n\tstruct queue_set *pqs = nic->pnicvf->qs;\n\tint qidx;\n\n\tif (!qs)\n\t\treturn 0;\n\n\t \n\tif (nic->sqs_mode && pqs) {\n\t\tqs->cq_len = pqs->cq_len;\n\t\tqs->sq_len = pqs->sq_len;\n\t}\n\n\tif (enable) {\n\t\tif (nicvf_alloc_resources(nic))\n\t\t\treturn -ENOMEM;\n\n\t\tfor (qidx = 0; qidx < qs->sq_cnt; qidx++)\n\t\t\tnicvf_snd_queue_config(nic, qs, qidx, enable);\n\t\tfor (qidx = 0; qidx < qs->cq_cnt; qidx++)\n\t\t\tnicvf_cmp_queue_config(nic, qs, qidx, enable);\n\t\tfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++)\n\t\t\tnicvf_rbdr_config(nic, qs, qidx, enable);\n\t\tfor (qidx = 0; qidx < qs->rq_cnt; qidx++)\n\t\t\tnicvf_rcv_queue_config(nic, qs, qidx, enable);\n\t} else {\n\t\tfor (qidx = 0; qidx < qs->rq_cnt; qidx++)\n\t\t\tnicvf_rcv_queue_config(nic, qs, qidx, disable);\n\t\tfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++)\n\t\t\tnicvf_rbdr_config(nic, qs, qidx, disable);\n\t\tfor (qidx = 0; qidx < qs->sq_cnt; qidx++)\n\t\t\tnicvf_snd_queue_config(nic, qs, qidx, disable);\n\t\tfor (qidx = 0; qidx < qs->cq_cnt; qidx++)\n\t\t\tnicvf_cmp_queue_config(nic, qs, qidx, disable);\n\n\t\tnicvf_free_resources(nic);\n\t}\n\n\t \n\tnicvf_reset_rcv_queue_stats(nic);\n\n\treturn 0;\n}\n\n \nstatic inline int nicvf_get_sq_desc(struct snd_queue *sq, int desc_cnt)\n{\n\tint qentry;\n\n\tqentry = sq->tail;\n\tif (!sq->is_xdp)\n\t\tatomic_sub(desc_cnt, &sq->free_cnt);\n\telse\n\t\tsq->xdp_free_cnt -= desc_cnt;\n\tsq->tail += desc_cnt;\n\tsq->tail &= (sq->dmem.q_len - 1);\n\n\treturn qentry;\n}\n\n \nstatic inline void nicvf_rollback_sq_desc(struct snd_queue *sq,\n\t\t\t\t\t  int qentry, int desc_cnt)\n{\n\tsq->tail = qentry;\n\tatomic_add(desc_cnt, &sq->free_cnt);\n}\n\n \nvoid nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt)\n{\n\tif (!sq->is_xdp)\n\t\tatomic_add(desc_cnt, &sq->free_cnt);\n\telse\n\t\tsq->xdp_free_cnt += desc_cnt;\n\tsq->head += desc_cnt;\n\tsq->head &= (sq->dmem.q_len - 1);\n}\n\nstatic inline int nicvf_get_nxt_sqentry(struct snd_queue *sq, int qentry)\n{\n\tqentry++;\n\tqentry &= (sq->dmem.q_len - 1);\n\treturn qentry;\n}\n\nvoid nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx)\n{\n\tu64 sq_cfg;\n\n\tsq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);\n\tsq_cfg |= NICVF_SQ_EN;\n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg);\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR, qidx, 0);\n}\n\nvoid nicvf_sq_disable(struct nicvf *nic, int qidx)\n{\n\tu64 sq_cfg;\n\n\tsq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);\n\tsq_cfg &= ~NICVF_SQ_EN;\n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg);\n}\n\nvoid nicvf_sq_free_used_descs(struct net_device *netdev, struct snd_queue *sq,\n\t\t\t      int qidx)\n{\n\tu64 head;\n\tstruct sk_buff *skb;\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct sq_hdr_subdesc *hdr;\n\n\thead = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_HEAD, qidx) >> 4;\n\twhile (sq->head != head) {\n\t\thdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, sq->head);\n\t\tif (hdr->subdesc_type != SQ_DESC_TYPE_HEADER) {\n\t\t\tnicvf_put_sq_desc(sq, 1);\n\t\t\tcontinue;\n\t\t}\n\t\tskb = (struct sk_buff *)sq->skbuff[sq->head];\n\t\tif (skb)\n\t\t\tdev_kfree_skb_any(skb);\n\t\tatomic64_add(1, (atomic64_t *)&netdev->stats.tx_packets);\n\t\tatomic64_add(hdr->tot_len,\n\t\t\t     (atomic64_t *)&netdev->stats.tx_bytes);\n\t\tnicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);\n\t}\n}\n\n \nvoid nicvf_xdp_sq_doorbell(struct nicvf *nic,\n\t\t\t   struct snd_queue *sq, int sq_num)\n{\n\tif (!sq->xdp_desc_cnt)\n\t\treturn;\n\n\t \n\twmb();\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR,\n\t\t\t      sq_num, sq->xdp_desc_cnt);\n\tsq->xdp_desc_cnt = 0;\n}\n\nstatic inline void\nnicvf_xdp_sq_add_hdr_subdesc(struct snd_queue *sq, int qentry,\n\t\t\t     int subdesc_cnt, u64 data, int len)\n{\n\tstruct sq_hdr_subdesc *hdr;\n\n\thdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, qentry);\n\tmemset(hdr, 0, SND_QUEUE_DESC_SIZE);\n\thdr->subdesc_type = SQ_DESC_TYPE_HEADER;\n\thdr->subdesc_cnt = subdesc_cnt;\n\thdr->tot_len = len;\n\thdr->post_cqe = 1;\n\tsq->xdp_page[qentry] = (u64)virt_to_page((void *)data);\n}\n\nint nicvf_xdp_sq_append_pkt(struct nicvf *nic, struct snd_queue *sq,\n\t\t\t    u64 bufaddr, u64 dma_addr, u16 len)\n{\n\tint subdesc_cnt = MIN_SQ_DESC_PER_PKT_XMIT;\n\tint qentry;\n\n\tif (subdesc_cnt > sq->xdp_free_cnt)\n\t\treturn 0;\n\n\tqentry = nicvf_get_sq_desc(sq, subdesc_cnt);\n\n\tnicvf_xdp_sq_add_hdr_subdesc(sq, qentry, subdesc_cnt - 1, bufaddr, len);\n\n\tqentry = nicvf_get_nxt_sqentry(sq, qentry);\n\tnicvf_sq_add_gather_subdesc(sq, qentry, len, dma_addr);\n\n\tsq->xdp_desc_cnt += subdesc_cnt;\n\n\treturn 1;\n}\n\n \nstatic int nicvf_tso_count_subdescs(struct sk_buff *skb)\n{\n\tstruct skb_shared_info *sh = skb_shinfo(skb);\n\tunsigned int sh_len = skb_tcp_all_headers(skb);\n\tunsigned int data_len = skb->len - sh_len;\n\tunsigned int p_len = sh->gso_size;\n\tlong f_id = -1;     \n\tlong f_size = skb_headlen(skb) - sh_len;   \n\tlong f_used = 0;   \n\tlong n;             \n\tint num_edescs = 0;\n\tint segment;\n\n\tfor (segment = 0; segment < sh->gso_segs; segment++) {\n\t\tunsigned int p_used = 0;\n\n\t\t \n\t\tfor (num_edescs++; p_used < p_len; num_edescs++) {\n\t\t\t \n\t\t\twhile (f_used >= f_size) {\n\t\t\t\tf_id++;\n\t\t\t\tf_size = skb_frag_size(&sh->frags[f_id]);\n\t\t\t\tf_used = 0;\n\t\t\t}\n\n\t\t\t \n\t\t\tn = p_len - p_used;\n\t\t\tif (n > f_size - f_used)\n\t\t\t\tn = f_size - f_used;\n\t\t\tf_used += n;\n\t\t\tp_used += n;\n\t\t}\n\n\t\t \n\t\tdata_len -= p_len;\n\t\tif (data_len < p_len)\n\t\t\tp_len = data_len;\n\t}\n\n\t \n\treturn num_edescs + sh->gso_segs;\n}\n\n#define POST_CQE_DESC_COUNT 2\n\n \nstatic int nicvf_sq_subdesc_required(struct nicvf *nic, struct sk_buff *skb)\n{\n\tint subdesc_cnt = MIN_SQ_DESC_PER_PKT_XMIT;\n\n\tif (skb_shinfo(skb)->gso_size && !nic->hw_tso) {\n\t\tsubdesc_cnt = nicvf_tso_count_subdescs(skb);\n\t\treturn subdesc_cnt;\n\t}\n\n\t \n\tif (nic->t88 && nic->hw_tso && skb_shinfo(skb)->gso_size)\n\t\tsubdesc_cnt += POST_CQE_DESC_COUNT;\n\n\tif (skb_shinfo(skb)->nr_frags)\n\t\tsubdesc_cnt += skb_shinfo(skb)->nr_frags;\n\n\treturn subdesc_cnt;\n}\n\n \nstatic inline void\nnicvf_sq_add_hdr_subdesc(struct nicvf *nic, struct snd_queue *sq, int qentry,\n\t\t\t int subdesc_cnt, struct sk_buff *skb, int len)\n{\n\tint proto;\n\tstruct sq_hdr_subdesc *hdr;\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\n\tip.hdr = skb_network_header(skb);\n\thdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, qentry);\n\tmemset(hdr, 0, SND_QUEUE_DESC_SIZE);\n\thdr->subdesc_type = SQ_DESC_TYPE_HEADER;\n\n\tif (nic->t88 && nic->hw_tso && skb_shinfo(skb)->gso_size) {\n\t\t \n\t\thdr->subdesc_cnt = subdesc_cnt - POST_CQE_DESC_COUNT;\n\t} else {\n\t\tsq->skbuff[qentry] = (u64)skb;\n\t\t \n\t\thdr->post_cqe = 1;\n\t\t \n\t\thdr->subdesc_cnt = subdesc_cnt;\n\t}\n\thdr->tot_len = len;\n\n\t \n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tif (ip.v4->version == 4)\n\t\t\thdr->csum_l3 = 1;  \n\t\thdr->l3_offset = skb_network_offset(skb);\n\t\thdr->l4_offset = skb_transport_offset(skb);\n\n\t\tproto = (ip.v4->version == 4) ? ip.v4->protocol :\n\t\t\tip.v6->nexthdr;\n\n\t\tswitch (proto) {\n\t\tcase IPPROTO_TCP:\n\t\t\thdr->csum_l4 = SEND_L4_CSUM_TCP;\n\t\t\tbreak;\n\t\tcase IPPROTO_UDP:\n\t\t\thdr->csum_l4 = SEND_L4_CSUM_UDP;\n\t\t\tbreak;\n\t\tcase IPPROTO_SCTP:\n\t\t\thdr->csum_l4 = SEND_L4_CSUM_SCTP;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (nic->hw_tso && skb_shinfo(skb)->gso_size) {\n\t\thdr->tso = 1;\n\t\thdr->tso_start = skb_tcp_all_headers(skb);\n\t\thdr->tso_max_paysize = skb_shinfo(skb)->gso_size;\n\t\t \n\t\thdr->inner_l3_offset = skb_network_offset(skb) - 2;\n\t\tthis_cpu_inc(nic->pnicvf->drv_stats->tx_tso);\n\t}\n\n\t \n\tif (!(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {\n\t\tskb_tx_timestamp(skb);\n\t\treturn;\n\t}\n\n\t \n\tif (skb_shinfo(skb)->gso_size)\n\t\treturn;\n\n\t \n\tif (!atomic_add_unless(&nic->pnicvf->tx_ptp_skbs, 1, 1))\n\t\treturn;\n\n\t \n\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\n\t \n\thdr->tstmp = 1;\n}\n\n \nstatic inline void nicvf_sq_add_gather_subdesc(struct snd_queue *sq, int qentry,\n\t\t\t\t\t       int size, u64 data)\n{\n\tstruct sq_gather_subdesc *gather;\n\n\tqentry &= (sq->dmem.q_len - 1);\n\tgather = (struct sq_gather_subdesc *)GET_SQ_DESC(sq, qentry);\n\n\tmemset(gather, 0, SND_QUEUE_DESC_SIZE);\n\tgather->subdesc_type = SQ_DESC_TYPE_GATHER;\n\tgather->ld_type = NIC_SEND_LD_TYPE_E_LDD;\n\tgather->size = size;\n\tgather->addr = data;\n}\n\n \nstatic inline void nicvf_sq_add_cqe_subdesc(struct snd_queue *sq, int qentry,\n\t\t\t\t\t    int tso_sqe, struct sk_buff *skb)\n{\n\tstruct sq_imm_subdesc *imm;\n\tstruct sq_hdr_subdesc *hdr;\n\n\tsq->skbuff[qentry] = (u64)skb;\n\n\thdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, qentry);\n\tmemset(hdr, 0, SND_QUEUE_DESC_SIZE);\n\thdr->subdesc_type = SQ_DESC_TYPE_HEADER;\n\t \n\thdr->post_cqe = 1;\n\t \n\thdr->dont_send = 1;\n\thdr->subdesc_cnt = POST_CQE_DESC_COUNT - 1;\n\thdr->tot_len = 1;\n\t \n\thdr->rsvd2 = tso_sqe;\n\n\tqentry = nicvf_get_nxt_sqentry(sq, qentry);\n\timm = (struct sq_imm_subdesc *)GET_SQ_DESC(sq, qentry);\n\tmemset(imm, 0, SND_QUEUE_DESC_SIZE);\n\timm->subdesc_type = SQ_DESC_TYPE_IMMEDIATE;\n\timm->len = 1;\n}\n\nstatic inline void nicvf_sq_doorbell(struct nicvf *nic, struct sk_buff *skb,\n\t\t\t\t     int sq_num, int desc_cnt)\n{\n\tstruct netdev_queue *txq;\n\n\ttxq = netdev_get_tx_queue(nic->pnicvf->netdev,\n\t\t\t\t  skb_get_queue_mapping(skb));\n\n\tnetdev_tx_sent_queue(txq, skb->len);\n\n\t \n\tsmp_wmb();\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR,\n\t\t\t      sq_num, desc_cnt);\n}\n\n \nstatic int nicvf_sq_append_tso(struct nicvf *nic, struct snd_queue *sq,\n\t\t\t       int sq_num, int qentry, struct sk_buff *skb)\n{\n\tstruct tso_t tso;\n\tint seg_subdescs = 0, desc_cnt = 0;\n\tint seg_len, total_len, data_left;\n\tint hdr_qentry = qentry;\n\tint hdr_len;\n\n\thdr_len = tso_start(skb, &tso);\n\n\ttotal_len = skb->len - hdr_len;\n\twhile (total_len > 0) {\n\t\tchar *hdr;\n\n\t\t \n\t\thdr_qentry = qentry;\n\n\t\tdata_left = min_t(int, skb_shinfo(skb)->gso_size, total_len);\n\t\ttotal_len -= data_left;\n\n\t\t \n\t\tqentry = nicvf_get_nxt_sqentry(sq, qentry);\n\t\thdr = sq->tso_hdrs + qentry * TSO_HEADER_SIZE;\n\t\ttso_build_hdr(skb, hdr, &tso, data_left, total_len == 0);\n\t\tnicvf_sq_add_gather_subdesc(sq, qentry, hdr_len,\n\t\t\t\t\t    sq->tso_hdrs_phys +\n\t\t\t\t\t    qentry * TSO_HEADER_SIZE);\n\t\t \n\t\tseg_subdescs = 2;\n\t\tseg_len = hdr_len;\n\n\t\t \n\t\twhile (data_left > 0) {\n\t\t\tint size;\n\n\t\t\tsize = min_t(int, tso.size, data_left);\n\n\t\t\tqentry = nicvf_get_nxt_sqentry(sq, qentry);\n\t\t\tnicvf_sq_add_gather_subdesc(sq, qentry, size,\n\t\t\t\t\t\t    virt_to_phys(tso.data));\n\t\t\tseg_subdescs++;\n\t\t\tseg_len += size;\n\n\t\t\tdata_left -= size;\n\t\t\ttso_build_data(skb, &tso, size);\n\t\t}\n\t\tnicvf_sq_add_hdr_subdesc(nic, sq, hdr_qentry,\n\t\t\t\t\t seg_subdescs - 1, skb, seg_len);\n\t\tsq->skbuff[hdr_qentry] = (u64)NULL;\n\t\tqentry = nicvf_get_nxt_sqentry(sq, qentry);\n\n\t\tdesc_cnt += seg_subdescs;\n\t}\n\t \n\tsq->skbuff[hdr_qentry] = (u64)skb;\n\n\tnicvf_sq_doorbell(nic, skb, sq_num, desc_cnt);\n\n\tthis_cpu_inc(nic->pnicvf->drv_stats->tx_tso);\n\treturn 1;\n}\n\n \nint nicvf_sq_append_skb(struct nicvf *nic, struct snd_queue *sq,\n\t\t\tstruct sk_buff *skb, u8 sq_num)\n{\n\tint i, size;\n\tint subdesc_cnt, hdr_sqe = 0;\n\tint qentry;\n\tu64 dma_addr;\n\n\tsubdesc_cnt = nicvf_sq_subdesc_required(nic, skb);\n\tif (subdesc_cnt > atomic_read(&sq->free_cnt))\n\t\tgoto append_fail;\n\n\tqentry = nicvf_get_sq_desc(sq, subdesc_cnt);\n\n\t \n\tif (skb_shinfo(skb)->gso_size && !nic->hw_tso)\n\t\treturn nicvf_sq_append_tso(nic, sq, sq_num, qentry, skb);\n\n\t \n\tnicvf_sq_add_hdr_subdesc(nic, sq, qentry, subdesc_cnt - 1,\n\t\t\t\t skb, skb->len);\n\thdr_sqe = qentry;\n\n\t \n\tqentry = nicvf_get_nxt_sqentry(sq, qentry);\n\tsize = skb_is_nonlinear(skb) ? skb_headlen(skb) : skb->len;\n\t \n\tdma_addr = dma_map_page_attrs(&nic->pdev->dev, virt_to_page(skb->data),\n\t\t\t\t      offset_in_page(skb->data), size,\n\t\t\t\t      DMA_TO_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\n\tif (dma_mapping_error(&nic->pdev->dev, dma_addr)) {\n\t\tnicvf_rollback_sq_desc(sq, qentry, subdesc_cnt);\n\t\treturn 0;\n\t}\n\n\tnicvf_sq_add_gather_subdesc(sq, qentry, size, dma_addr);\n\n\t \n\tif (!skb_is_nonlinear(skb))\n\t\tgoto doorbell;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tqentry = nicvf_get_nxt_sqentry(sq, qentry);\n\t\tsize = skb_frag_size(frag);\n\t\tdma_addr = dma_map_page_attrs(&nic->pdev->dev,\n\t\t\t\t\t      skb_frag_page(frag),\n\t\t\t\t\t      skb_frag_off(frag), size,\n\t\t\t\t\t      DMA_TO_DEVICE,\n\t\t\t\t\t      DMA_ATTR_SKIP_CPU_SYNC);\n\t\tif (dma_mapping_error(&nic->pdev->dev, dma_addr)) {\n\t\t\t \n\t\t\tnicvf_unmap_sndq_buffers(nic, sq, hdr_sqe, i);\n\t\t\tnicvf_rollback_sq_desc(sq, qentry, subdesc_cnt);\n\t\t\treturn 0;\n\t\t}\n\t\tnicvf_sq_add_gather_subdesc(sq, qentry, size, dma_addr);\n\t}\n\ndoorbell:\n\tif (nic->t88 && skb_shinfo(skb)->gso_size) {\n\t\tqentry = nicvf_get_nxt_sqentry(sq, qentry);\n\t\tnicvf_sq_add_cqe_subdesc(sq, qentry, hdr_sqe, skb);\n\t}\n\n\tnicvf_sq_doorbell(nic, skb, sq_num, subdesc_cnt);\n\n\treturn 1;\n\nappend_fail:\n\t \n\tnic = nic->pnicvf;\n\tnetdev_dbg(nic->netdev, \"Not enough SQ descriptors to xmit pkt\\n\");\n\treturn 0;\n}\n\nstatic inline unsigned frag_num(unsigned i)\n{\n#ifdef __BIG_ENDIAN\n\treturn (i & ~3) + 3 - (i & 3);\n#else\n\treturn i;\n#endif\n}\n\nstatic void nicvf_unmap_rcv_buffer(struct nicvf *nic, u64 dma_addr,\n\t\t\t\t   u64 buf_addr, bool xdp)\n{\n\tstruct page *page = NULL;\n\tint len = RCV_FRAG_LEN;\n\n\tif (xdp) {\n\t\tpage = virt_to_page(phys_to_virt(buf_addr));\n\t\t \n\t\tif (page_ref_count(page) != 1)\n\t\t\treturn;\n\n\t\tlen += XDP_PACKET_HEADROOM;\n\t\t \n\t\tdma_addr &= PAGE_MASK;\n\t}\n\tdma_unmap_page_attrs(&nic->pdev->dev, dma_addr, len,\n\t\t\t     DMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\n}\n\n \nstruct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic,\n\t\t\t\t  struct cqe_rx_t *cqe_rx, bool xdp)\n{\n\tint frag;\n\tint payload_len = 0;\n\tstruct sk_buff *skb = NULL;\n\tstruct page *page;\n\tint offset;\n\tu16 *rb_lens = NULL;\n\tu64 *rb_ptrs = NULL;\n\tu64 phys_addr;\n\n\trb_lens = (void *)cqe_rx + (3 * sizeof(u64));\n\t \n\tif (!nic->hw_tso)\n\t\trb_ptrs = (void *)cqe_rx + (6 * sizeof(u64));\n\telse\n\t\trb_ptrs = (void *)cqe_rx + (7 * sizeof(u64));\n\n\tfor (frag = 0; frag < cqe_rx->rb_cnt; frag++) {\n\t\tpayload_len = rb_lens[frag_num(frag)];\n\t\tphys_addr = nicvf_iova_to_phys(nic, *rb_ptrs);\n\t\tif (!phys_addr) {\n\t\t\tif (skb)\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (!frag) {\n\t\t\t \n\t\t\tnicvf_unmap_rcv_buffer(nic,\n\t\t\t\t\t       *rb_ptrs - cqe_rx->align_pad,\n\t\t\t\t\t       phys_addr, xdp);\n\t\t\tskb = nicvf_rb_ptr_to_skb(nic,\n\t\t\t\t\t\t  phys_addr - cqe_rx->align_pad,\n\t\t\t\t\t\t  payload_len);\n\t\t\tif (!skb)\n\t\t\t\treturn NULL;\n\t\t\tskb_reserve(skb, cqe_rx->align_pad);\n\t\t\tskb_put(skb, payload_len);\n\t\t} else {\n\t\t\t \n\t\t\tnicvf_unmap_rcv_buffer(nic, *rb_ptrs, phys_addr, xdp);\n\t\t\tpage = virt_to_page(phys_to_virt(phys_addr));\n\t\t\toffset = phys_to_virt(phys_addr) - page_address(page);\n\t\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,\n\t\t\t\t\toffset, payload_len, RCV_FRAG_LEN);\n\t\t}\n\t\t \n\t\trb_ptrs++;\n\t}\n\treturn skb;\n}\n\nstatic u64 nicvf_int_type_to_mask(int int_type, int q_idx)\n{\n\tu64 reg_val;\n\n\tswitch (int_type) {\n\tcase NICVF_INTR_CQ:\n\t\treg_val = ((1ULL << q_idx) << NICVF_INTR_CQ_SHIFT);\n\t\tbreak;\n\tcase NICVF_INTR_SQ:\n\t\treg_val = ((1ULL << q_idx) << NICVF_INTR_SQ_SHIFT);\n\t\tbreak;\n\tcase NICVF_INTR_RBDR:\n\t\treg_val = ((1ULL << q_idx) << NICVF_INTR_RBDR_SHIFT);\n\t\tbreak;\n\tcase NICVF_INTR_PKT_DROP:\n\t\treg_val = (1ULL << NICVF_INTR_PKT_DROP_SHIFT);\n\t\tbreak;\n\tcase NICVF_INTR_TCP_TIMER:\n\t\treg_val = (1ULL << NICVF_INTR_TCP_TIMER_SHIFT);\n\t\tbreak;\n\tcase NICVF_INTR_MBOX:\n\t\treg_val = (1ULL << NICVF_INTR_MBOX_SHIFT);\n\t\tbreak;\n\tcase NICVF_INTR_QS_ERR:\n\t\treg_val = (1ULL << NICVF_INTR_QS_ERR_SHIFT);\n\t\tbreak;\n\tdefault:\n\t\treg_val = 0;\n\t}\n\n\treturn reg_val;\n}\n\n \nvoid nicvf_enable_intr(struct nicvf *nic, int int_type, int q_idx)\n{\n\tu64 mask = nicvf_int_type_to_mask(int_type, q_idx);\n\n\tif (!mask) {\n\t\tnetdev_dbg(nic->netdev,\n\t\t\t   \"Failed to enable interrupt: unknown type\\n\");\n\t\treturn;\n\t}\n\tnicvf_reg_write(nic, NIC_VF_ENA_W1S,\n\t\t\tnicvf_reg_read(nic, NIC_VF_ENA_W1S) | mask);\n}\n\n \nvoid nicvf_disable_intr(struct nicvf *nic, int int_type, int q_idx)\n{\n\tu64 mask = nicvf_int_type_to_mask(int_type, q_idx);\n\n\tif (!mask) {\n\t\tnetdev_dbg(nic->netdev,\n\t\t\t   \"Failed to disable interrupt: unknown type\\n\");\n\t\treturn;\n\t}\n\n\tnicvf_reg_write(nic, NIC_VF_ENA_W1C, mask);\n}\n\n \nvoid nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx)\n{\n\tu64 mask = nicvf_int_type_to_mask(int_type, q_idx);\n\n\tif (!mask) {\n\t\tnetdev_dbg(nic->netdev,\n\t\t\t   \"Failed to clear interrupt: unknown type\\n\");\n\t\treturn;\n\t}\n\n\tnicvf_reg_write(nic, NIC_VF_INT, mask);\n}\n\n \nint nicvf_is_intr_enabled(struct nicvf *nic, int int_type, int q_idx)\n{\n\tu64 mask = nicvf_int_type_to_mask(int_type, q_idx);\n\t \n\tif (!mask) {\n\t\tnetdev_dbg(nic->netdev,\n\t\t\t   \"Failed to check interrupt enable: unknown type\\n\");\n\t\treturn 0;\n\t}\n\n\treturn mask & nicvf_reg_read(nic, NIC_VF_ENA_W1S);\n}\n\nvoid nicvf_update_rq_stats(struct nicvf *nic, int rq_idx)\n{\n\tstruct rcv_queue *rq;\n\n#define GET_RQ_STATS(reg) \\\n\tnicvf_reg_read(nic, NIC_QSET_RQ_0_7_STAT_0_1 |\\\n\t\t\t    (rq_idx << NIC_Q_NUM_SHIFT) | (reg << 3))\n\n\trq = &nic->qs->rq[rq_idx];\n\trq->stats.bytes = GET_RQ_STATS(RQ_SQ_STATS_OCTS);\n\trq->stats.pkts = GET_RQ_STATS(RQ_SQ_STATS_PKTS);\n}\n\nvoid nicvf_update_sq_stats(struct nicvf *nic, int sq_idx)\n{\n\tstruct snd_queue *sq;\n\n#define GET_SQ_STATS(reg) \\\n\tnicvf_reg_read(nic, NIC_QSET_SQ_0_7_STAT_0_1 |\\\n\t\t\t    (sq_idx << NIC_Q_NUM_SHIFT) | (reg << 3))\n\n\tsq = &nic->qs->sq[sq_idx];\n\tsq->stats.bytes = GET_SQ_STATS(RQ_SQ_STATS_OCTS);\n\tsq->stats.pkts = GET_SQ_STATS(RQ_SQ_STATS_PKTS);\n}\n\n \nint nicvf_check_cqe_rx_errs(struct nicvf *nic, struct cqe_rx_t *cqe_rx)\n{\n\tnetif_err(nic, rx_err, nic->netdev,\n\t\t  \"RX error CQE err_level 0x%x err_opcode 0x%x\\n\",\n\t\t  cqe_rx->err_level, cqe_rx->err_opcode);\n\n\tswitch (cqe_rx->err_opcode) {\n\tcase CQ_RX_ERROP_RE_PARTIAL:\n\t\tthis_cpu_inc(nic->drv_stats->rx_bgx_truncated_pkts);\n\t\tbreak;\n\tcase CQ_RX_ERROP_RE_JABBER:\n\t\tthis_cpu_inc(nic->drv_stats->rx_jabber_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_RE_FCS:\n\t\tthis_cpu_inc(nic->drv_stats->rx_fcs_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_RE_RX_CTL:\n\t\tthis_cpu_inc(nic->drv_stats->rx_bgx_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_PREL2_ERR:\n\t\tthis_cpu_inc(nic->drv_stats->rx_prel2_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L2_MAL:\n\t\tthis_cpu_inc(nic->drv_stats->rx_l2_hdr_malformed);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L2_OVERSIZE:\n\t\tthis_cpu_inc(nic->drv_stats->rx_oversize);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L2_UNDERSIZE:\n\t\tthis_cpu_inc(nic->drv_stats->rx_undersize);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L2_LENMISM:\n\t\tthis_cpu_inc(nic->drv_stats->rx_l2_len_mismatch);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L2_PCLP:\n\t\tthis_cpu_inc(nic->drv_stats->rx_l2_pclp);\n\t\tbreak;\n\tcase CQ_RX_ERROP_IP_NOT:\n\t\tthis_cpu_inc(nic->drv_stats->rx_ip_ver_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_IP_CSUM_ERR:\n\t\tthis_cpu_inc(nic->drv_stats->rx_ip_csum_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_IP_MAL:\n\t\tthis_cpu_inc(nic->drv_stats->rx_ip_hdr_malformed);\n\t\tbreak;\n\tcase CQ_RX_ERROP_IP_MALD:\n\t\tthis_cpu_inc(nic->drv_stats->rx_ip_payload_malformed);\n\t\tbreak;\n\tcase CQ_RX_ERROP_IP_HOP:\n\t\tthis_cpu_inc(nic->drv_stats->rx_ip_ttl_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L3_PCLP:\n\t\tthis_cpu_inc(nic->drv_stats->rx_l3_pclp);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L4_MAL:\n\t\tthis_cpu_inc(nic->drv_stats->rx_l4_malformed);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L4_CHK:\n\t\tthis_cpu_inc(nic->drv_stats->rx_l4_csum_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_UDP_LEN:\n\t\tthis_cpu_inc(nic->drv_stats->rx_udp_len_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L4_PORT:\n\t\tthis_cpu_inc(nic->drv_stats->rx_l4_port_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_TCP_FLAG:\n\t\tthis_cpu_inc(nic->drv_stats->rx_tcp_flag_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_TCP_OFFSET:\n\t\tthis_cpu_inc(nic->drv_stats->rx_tcp_offset_errs);\n\t\tbreak;\n\tcase CQ_RX_ERROP_L4_PCLP:\n\t\tthis_cpu_inc(nic->drv_stats->rx_l4_pclp);\n\t\tbreak;\n\tcase CQ_RX_ERROP_RBDR_TRUNC:\n\t\tthis_cpu_inc(nic->drv_stats->rx_truncated_pkts);\n\t\tbreak;\n\t}\n\n\treturn 1;\n}\n\n \nint nicvf_check_cqe_tx_errs(struct nicvf *nic, struct cqe_send_t *cqe_tx)\n{\n\tswitch (cqe_tx->send_status) {\n\tcase CQ_TX_ERROP_DESC_FAULT:\n\t\tthis_cpu_inc(nic->drv_stats->tx_desc_fault);\n\t\tbreak;\n\tcase CQ_TX_ERROP_HDR_CONS_ERR:\n\t\tthis_cpu_inc(nic->drv_stats->tx_hdr_cons_err);\n\t\tbreak;\n\tcase CQ_TX_ERROP_SUBDC_ERR:\n\t\tthis_cpu_inc(nic->drv_stats->tx_subdesc_err);\n\t\tbreak;\n\tcase CQ_TX_ERROP_MAX_SIZE_VIOL:\n\t\tthis_cpu_inc(nic->drv_stats->tx_max_size_exceeded);\n\t\tbreak;\n\tcase CQ_TX_ERROP_IMM_SIZE_OFLOW:\n\t\tthis_cpu_inc(nic->drv_stats->tx_imm_size_oflow);\n\t\tbreak;\n\tcase CQ_TX_ERROP_DATA_SEQUENCE_ERR:\n\t\tthis_cpu_inc(nic->drv_stats->tx_data_seq_err);\n\t\tbreak;\n\tcase CQ_TX_ERROP_MEM_SEQUENCE_ERR:\n\t\tthis_cpu_inc(nic->drv_stats->tx_mem_seq_err);\n\t\tbreak;\n\tcase CQ_TX_ERROP_LOCK_VIOL:\n\t\tthis_cpu_inc(nic->drv_stats->tx_lock_viol);\n\t\tbreak;\n\tcase CQ_TX_ERROP_DATA_FAULT:\n\t\tthis_cpu_inc(nic->drv_stats->tx_data_fault);\n\t\tbreak;\n\tcase CQ_TX_ERROP_TSTMP_CONFLICT:\n\t\tthis_cpu_inc(nic->drv_stats->tx_tstmp_conflict);\n\t\tbreak;\n\tcase CQ_TX_ERROP_TSTMP_TIMEOUT:\n\t\tthis_cpu_inc(nic->drv_stats->tx_tstmp_timeout);\n\t\tbreak;\n\tcase CQ_TX_ERROP_MEM_FAULT:\n\t\tthis_cpu_inc(nic->drv_stats->tx_mem_fault);\n\t\tbreak;\n\tcase CQ_TX_ERROP_CK_OVERLAP:\n\t\tthis_cpu_inc(nic->drv_stats->tx_csum_overlap);\n\t\tbreak;\n\tcase CQ_TX_ERROP_CK_OFLOW:\n\t\tthis_cpu_inc(nic->drv_stats->tx_csum_overflow);\n\t\tbreak;\n\t}\n\n\treturn 1;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}