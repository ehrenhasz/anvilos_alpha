{
  "module_name": "nicvf_queues.h",
  "hash_id": "0e3b6d3240170a71edd0b6c2dff9e0ad89159a2244873f9f5708e1dce187e305",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/cavium/thunder/nicvf_queues.h",
  "human_readable_source": " \n \n\n#ifndef NICVF_QUEUES_H\n#define NICVF_QUEUES_H\n\n#include <linux/netdevice.h>\n#include <linux/iommu.h>\n#include <net/xdp.h>\n#include \"q_struct.h\"\n\n#define MAX_QUEUE_SET\t\t\t128\n#define MAX_RCV_QUEUES_PER_QS\t\t8\n#define MAX_RCV_BUF_DESC_RINGS_PER_QS\t2\n#define MAX_SND_QUEUES_PER_QS\t\t8\n#define MAX_CMP_QUEUES_PER_QS\t\t8\n\n \n#define\tNICVF_INTR_ID_CQ\t\t0\n#define\tNICVF_INTR_ID_SQ\t\t8\n#define\tNICVF_INTR_ID_RBDR\t\t16\n#define\tNICVF_INTR_ID_MISC\t\t18\n#define\tNICVF_INTR_ID_QS_ERR\t\t19\n\n#define\tfor_each_cq_irq(irq)\t\\\n\tfor (irq = NICVF_INTR_ID_CQ; irq < NICVF_INTR_ID_SQ; irq++)\n#define\tfor_each_sq_irq(irq)\t\\\n\tfor (irq = NICVF_INTR_ID_SQ; irq < NICVF_INTR_ID_RBDR; irq++)\n#define\tfor_each_rbdr_irq(irq)\t\\\n\tfor (irq = NICVF_INTR_ID_RBDR; irq < NICVF_INTR_ID_MISC; irq++)\n\n#define RBDR_SIZE0\t\t0ULL  \n#define RBDR_SIZE1\t\t1ULL  \n#define RBDR_SIZE2\t\t2ULL  \n#define RBDR_SIZE3\t\t3ULL  \n#define RBDR_SIZE4\t\t4ULL  \n#define RBDR_SIZE5\t\t5ULL  \n#define RBDR_SIZE6\t\t6ULL  \n\n#define SND_QUEUE_SIZE0\t\t0ULL  \n#define SND_QUEUE_SIZE1\t\t1ULL  \n#define SND_QUEUE_SIZE2\t\t2ULL  \n#define SND_QUEUE_SIZE3\t\t3ULL  \n#define SND_QUEUE_SIZE4\t\t4ULL  \n#define SND_QUEUE_SIZE5\t\t5ULL  \n#define SND_QUEUE_SIZE6\t\t6ULL  \n\n#define CMP_QUEUE_SIZE0\t\t0ULL  \n#define CMP_QUEUE_SIZE1\t\t1ULL  \n#define CMP_QUEUE_SIZE2\t\t2ULL  \n#define CMP_QUEUE_SIZE3\t\t3ULL  \n#define CMP_QUEUE_SIZE4\t\t4ULL  \n#define CMP_QUEUE_SIZE5\t\t5ULL  \n#define CMP_QUEUE_SIZE6\t\t6ULL  \n\n \n#define DEFAULT_RBDR_CNT\t1\n\n#define SND_QSIZE\t\tSND_QUEUE_SIZE0\n#define SND_QUEUE_LEN\t\t(1ULL << (SND_QSIZE + 10))\n#define MIN_SND_QUEUE_LEN\t(1ULL << (SND_QUEUE_SIZE0 + 10))\n#define MAX_SND_QUEUE_LEN\t(1ULL << (SND_QUEUE_SIZE6 + 10))\n#define SND_QUEUE_THRESH\t2ULL\n#define MIN_SQ_DESC_PER_PKT_XMIT\t2\n \n#define MAX_CQE_PER_PKT_XMIT\t\t1\n\n \n#define CMP_QSIZE\t\tCMP_QUEUE_SIZE0\n#define CMP_QUEUE_LEN\t\t(1ULL << (CMP_QSIZE + 10))\n#define MIN_CMP_QUEUE_LEN\t(1ULL << (CMP_QUEUE_SIZE0 + 10))\n#define MAX_CMP_QUEUE_LEN\t(1ULL << (CMP_QUEUE_SIZE6 + 10))\n#define CMP_QUEUE_CQE_THRESH\t(NAPI_POLL_WEIGHT / 2)\n#define CMP_QUEUE_TIMER_THRESH\t80  \n\n \n#define CMP_QUEUE_PIPELINE_RSVD 544\n\n#define RBDR_SIZE\t\tRBDR_SIZE0\n#define RCV_BUF_COUNT\t\t(1ULL << (RBDR_SIZE + 13))\n#define MAX_RCV_BUF_COUNT\t(1ULL << (RBDR_SIZE6 + 13))\n#define RBDR_THRESH\t\t(RCV_BUF_COUNT / 2)\n#define DMA_BUFFER_LEN\t\t1536  \n#define RCV_FRAG_LEN\t (SKB_DATA_ALIGN(DMA_BUFFER_LEN + NET_SKB_PAD) + \\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\n#define MAX_CQES_FOR_TX\t\t((SND_QUEUE_LEN / MIN_SQ_DESC_PER_PKT_XMIT) * \\\n\t\t\t\t MAX_CQE_PER_PKT_XMIT)\n\n \n#define RQ_PASS_CQ_LVL         224ULL\n#define RQ_DROP_CQ_LVL         216ULL\n\n \n#define RQ_PASS_RBDR_LVL\t8ULL\n#define RQ_DROP_RBDR_LVL\t0ULL\n\n \n#define SND_QUEUE_DESC_SIZE\t16\n#define CMP_QUEUE_DESC_SIZE\t512\n\n \n#define NICVF_RCV_BUF_ALIGN\t\t7\n#define NICVF_RCV_BUF_ALIGN_BYTES\t(1ULL << NICVF_RCV_BUF_ALIGN)\n#define NICVF_CQ_BASE_ALIGN_BYTES\t512   \n#define NICVF_SQ_BASE_ALIGN_BYTES\t128   \n\n#define NICVF_ALIGNED_ADDR(ADDR, ALIGN_BYTES)\tALIGN(ADDR, ALIGN_BYTES)\n\n \n#define NICVF_SQ_EN\t\tBIT_ULL(19)\n\n \n#define NICVF_CQ_RESET\t\tBIT_ULL(41)\n#define NICVF_SQ_RESET\t\tBIT_ULL(17)\n#define NICVF_RBDR_RESET\tBIT_ULL(43)\n\nenum CQ_RX_ERRLVL_E {\n\tCQ_ERRLVL_MAC,\n\tCQ_ERRLVL_L2,\n\tCQ_ERRLVL_L3,\n\tCQ_ERRLVL_L4,\n};\n\nenum CQ_RX_ERROP_E {\n\tCQ_RX_ERROP_RE_NONE = 0x0,\n\tCQ_RX_ERROP_RE_PARTIAL = 0x1,\n\tCQ_RX_ERROP_RE_JABBER = 0x2,\n\tCQ_RX_ERROP_RE_FCS = 0x7,\n\tCQ_RX_ERROP_RE_TERMINATE = 0x9,\n\tCQ_RX_ERROP_RE_RX_CTL = 0xb,\n\tCQ_RX_ERROP_PREL2_ERR = 0x1f,\n\tCQ_RX_ERROP_L2_FRAGMENT = 0x20,\n\tCQ_RX_ERROP_L2_OVERRUN = 0x21,\n\tCQ_RX_ERROP_L2_PFCS = 0x22,\n\tCQ_RX_ERROP_L2_PUNY = 0x23,\n\tCQ_RX_ERROP_L2_MAL = 0x24,\n\tCQ_RX_ERROP_L2_OVERSIZE = 0x25,\n\tCQ_RX_ERROP_L2_UNDERSIZE = 0x26,\n\tCQ_RX_ERROP_L2_LENMISM = 0x27,\n\tCQ_RX_ERROP_L2_PCLP = 0x28,\n\tCQ_RX_ERROP_IP_NOT = 0x41,\n\tCQ_RX_ERROP_IP_CSUM_ERR = 0x42,\n\tCQ_RX_ERROP_IP_MAL = 0x43,\n\tCQ_RX_ERROP_IP_MALD = 0x44,\n\tCQ_RX_ERROP_IP_HOP = 0x45,\n\tCQ_RX_ERROP_L3_ICRC = 0x46,\n\tCQ_RX_ERROP_L3_PCLP = 0x47,\n\tCQ_RX_ERROP_L4_MAL = 0x61,\n\tCQ_RX_ERROP_L4_CHK = 0x62,\n\tCQ_RX_ERROP_UDP_LEN = 0x63,\n\tCQ_RX_ERROP_L4_PORT = 0x64,\n\tCQ_RX_ERROP_TCP_FLAG = 0x65,\n\tCQ_RX_ERROP_TCP_OFFSET = 0x66,\n\tCQ_RX_ERROP_L4_PCLP = 0x67,\n\tCQ_RX_ERROP_RBDR_TRUNC = 0x70,\n};\n\nenum CQ_TX_ERROP_E {\n\tCQ_TX_ERROP_GOOD = 0x0,\n\tCQ_TX_ERROP_DESC_FAULT = 0x10,\n\tCQ_TX_ERROP_HDR_CONS_ERR = 0x11,\n\tCQ_TX_ERROP_SUBDC_ERR = 0x12,\n\tCQ_TX_ERROP_MAX_SIZE_VIOL = 0x13,\n\tCQ_TX_ERROP_IMM_SIZE_OFLOW = 0x80,\n\tCQ_TX_ERROP_DATA_SEQUENCE_ERR = 0x81,\n\tCQ_TX_ERROP_MEM_SEQUENCE_ERR = 0x82,\n\tCQ_TX_ERROP_LOCK_VIOL = 0x83,\n\tCQ_TX_ERROP_DATA_FAULT = 0x84,\n\tCQ_TX_ERROP_TSTMP_CONFLICT = 0x85,\n\tCQ_TX_ERROP_TSTMP_TIMEOUT = 0x86,\n\tCQ_TX_ERROP_MEM_FAULT = 0x87,\n\tCQ_TX_ERROP_CK_OVERLAP = 0x88,\n\tCQ_TX_ERROP_CK_OFLOW = 0x89,\n\tCQ_TX_ERROP_ENUM_LAST = 0x8a,\n};\n\nenum RQ_SQ_STATS {\n\tRQ_SQ_STATS_OCTS,\n\tRQ_SQ_STATS_PKTS,\n};\n\nstruct rx_tx_queue_stats {\n\tu64\tbytes;\n\tu64\tpkts;\n} ____cacheline_aligned_in_smp;\n\nstruct q_desc_mem {\n\tdma_addr_t\tdma;\n\tu64\t\tsize;\n\tu32\t\tq_len;\n\tdma_addr_t\tphys_base;\n\tvoid\t\t*base;\n\tvoid\t\t*unalign_base;\n};\n\nstruct pgcache {\n\tstruct page\t*page;\n\tint\t\tref_count;\n\tu64\t\tdma_addr;\n};\n\nstruct rbdr {\n\tbool\t\tenable;\n\tu32\t\tdma_size;\n\tu32\t\tfrag_len;\n\tu32\t\tthresh;\t\t \n\tvoid\t\t*desc;\n\tu32\t\thead;\n\tu32\t\ttail;\n\tstruct q_desc_mem   dmem;\n\tbool\t\tis_xdp;\n\n\t \n\tint\t\tpgidx;\n\tint\t\tpgcnt;\n\tint\t\tpgalloc;\n\tstruct pgcache\t*pgcache;\n} ____cacheline_aligned_in_smp;\n\nstruct rcv_queue {\n\tbool\t\tenable;\n\tstruct\trbdr\t*rbdr_start;\n\tstruct\trbdr\t*rbdr_cont;\n\tbool\t\ten_tcp_reassembly;\n\tu8\t\tcq_qs;   \n\tu8\t\tcq_idx;  \n\tu8\t\tcont_rbdr_qs;       \n\tu8\t\tcont_qs_rbdr_idx;   \n\tu8\t\tstart_rbdr_qs;      \n\tu8\t\tstart_qs_rbdr_idx;  \n\tu8\t\tcaching;\n\tstruct\t\trx_tx_queue_stats stats;\n\tstruct xdp_rxq_info xdp_rxq;\n} ____cacheline_aligned_in_smp;\n\nstruct cmp_queue {\n\tbool\t\tenable;\n\tu16\t\tthresh;\n\tspinlock_t\tlock;   \n\tvoid\t\t*desc;\n\tstruct q_desc_mem   dmem;\n\tint\t\tirq;\n} ____cacheline_aligned_in_smp;\n\nstruct snd_queue {\n\tbool\t\tenable;\n\tu8\t\tcq_qs;   \n\tu8\t\tcq_idx;  \n\tu16\t\tthresh;\n\tatomic_t\tfree_cnt;\n\tu32\t\thead;\n\tu32\t\ttail;\n\tu64\t\t*skbuff;\n\tvoid\t\t*desc;\n\tu64\t\t*xdp_page;\n\tu16\t\txdp_desc_cnt;\n\tu16\t\txdp_free_cnt;\n\tbool\t\tis_xdp;\n\n\t \n\tchar\t\t*tso_hdrs;\n\tdma_addr_t\ttso_hdrs_phys;\n\n\tcpumask_t\taffinity_mask;\n\tstruct q_desc_mem   dmem;\n\tstruct rx_tx_queue_stats stats;\n} ____cacheline_aligned_in_smp;\n\nstruct queue_set {\n\tbool\t\tenable;\n\tbool\t\tbe_en;\n\tu8\t\tvnic_id;\n\tu8\t\trq_cnt;\n\tu8\t\tcq_cnt;\n\tu64\t\tcq_len;\n\tu8\t\tsq_cnt;\n\tu64\t\tsq_len;\n\tu8\t\trbdr_cnt;\n\tu64\t\trbdr_len;\n\tstruct\trcv_queue\trq[MAX_RCV_QUEUES_PER_QS];\n\tstruct\tcmp_queue\tcq[MAX_CMP_QUEUES_PER_QS];\n\tstruct\tsnd_queue\tsq[MAX_SND_QUEUES_PER_QS];\n\tstruct\trbdr\t\trbdr[MAX_RCV_BUF_DESC_RINGS_PER_QS];\n} ____cacheline_aligned_in_smp;\n\n#define GET_RBDR_DESC(RING, idx)\\\n\t\t(&(((struct rbdr_entry_t *)((RING)->desc))[idx]))\n#define GET_SQ_DESC(RING, idx)\\\n\t\t(&(((struct sq_hdr_subdesc *)((RING)->desc))[idx]))\n#define GET_CQ_DESC(RING, idx)\\\n\t\t(&(((union cq_desc_t *)((RING)->desc))[idx]))\n\n \n#define\tCQ_WR_FULL\tBIT(26)\n#define\tCQ_WR_DISABLE\tBIT(25)\n#define\tCQ_WR_FAULT\tBIT(24)\n#define\tCQ_CQE_COUNT\t(0xFFFF << 0)\n\n#define\tCQ_ERR_MASK\t(CQ_WR_FULL | CQ_WR_DISABLE | CQ_WR_FAULT)\n\nstatic inline u64 nicvf_iova_to_phys(struct nicvf *nic, dma_addr_t dma_addr)\n{\n\t \n\tif (nic->iommu_domain)\n\t\treturn iommu_iova_to_phys(nic->iommu_domain, dma_addr);\n\treturn dma_addr;\n}\n\nvoid nicvf_unmap_sndq_buffers(struct nicvf *nic, struct snd_queue *sq,\n\t\t\t      int hdr_sqe, u8 subdesc_cnt);\nvoid nicvf_config_vlan_stripping(struct nicvf *nic,\n\t\t\t\t netdev_features_t features);\nint nicvf_set_qset_resources(struct nicvf *nic);\nint nicvf_config_data_transfer(struct nicvf *nic, bool enable);\nvoid nicvf_qset_config(struct nicvf *nic, bool enable);\nvoid nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,\n\t\t\t    int qidx, bool enable);\n\nvoid nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx);\nvoid nicvf_sq_disable(struct nicvf *nic, int qidx);\nvoid nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt);\nvoid nicvf_sq_free_used_descs(struct net_device *netdev,\n\t\t\t      struct snd_queue *sq, int qidx);\nint nicvf_sq_append_skb(struct nicvf *nic, struct snd_queue *sq,\n\t\t\tstruct sk_buff *skb, u8 sq_num);\nint nicvf_xdp_sq_append_pkt(struct nicvf *nic, struct snd_queue *sq,\n\t\t\t    u64 bufaddr, u64 dma_addr, u16 len);\nvoid nicvf_xdp_sq_doorbell(struct nicvf *nic, struct snd_queue *sq, int sq_num);\n\nstruct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic,\n\t\t\t\t  struct cqe_rx_t *cqe_rx, bool xdp);\nvoid nicvf_rbdr_task(struct tasklet_struct *t);\nvoid nicvf_rbdr_work(struct work_struct *work);\n\nvoid nicvf_enable_intr(struct nicvf *nic, int int_type, int q_idx);\nvoid nicvf_disable_intr(struct nicvf *nic, int int_type, int q_idx);\nvoid nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx);\nint nicvf_is_intr_enabled(struct nicvf *nic, int int_type, int q_idx);\n\n \nvoid nicvf_reg_write(struct nicvf *nic, u64 offset, u64 val);\nu64  nicvf_reg_read(struct nicvf *nic, u64 offset);\nvoid nicvf_qset_reg_write(struct nicvf *nic, u64 offset, u64 val);\nu64 nicvf_qset_reg_read(struct nicvf *nic, u64 offset);\nvoid nicvf_queue_reg_write(struct nicvf *nic, u64 offset,\n\t\t\t   u64 qidx, u64 val);\nu64  nicvf_queue_reg_read(struct nicvf *nic,\n\t\t\t  u64 offset, u64 qidx);\n\n \nvoid nicvf_update_rq_stats(struct nicvf *nic, int rq_idx);\nvoid nicvf_update_sq_stats(struct nicvf *nic, int sq_idx);\nint nicvf_check_cqe_rx_errs(struct nicvf *nic, struct cqe_rx_t *cqe_rx);\nint nicvf_check_cqe_tx_errs(struct nicvf *nic, struct cqe_send_t *cqe_tx);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}