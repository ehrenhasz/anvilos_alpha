{
  "module_name": "nicvf_main.c",
  "hash_id": "8e385ecfae4cd07d0162a3bbf3dd2ae853a80761d47f2dd41c7e3aa6dbec9e0a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/cavium/thunder/nicvf_main.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/log2.h>\n#include <linux/prefetch.h>\n#include <linux/irq.h>\n#include <linux/iommu.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <linux/filter.h>\n#include <linux/net_tstamp.h>\n#include <linux/workqueue.h>\n\n#include \"nic_reg.h\"\n#include \"nic.h\"\n#include \"nicvf_queues.h\"\n#include \"thunder_bgx.h\"\n#include \"../common/cavium_ptp.h\"\n\n#define DRV_NAME\t\"nicvf\"\n#define DRV_VERSION\t\"1.0\"\n\n \n#define MAX_XDP_MTU\t(1530 - ETH_HLEN - VLAN_HLEN * 2)\n\n \nstatic const struct pci_device_id nicvf_id_table[] = {\n\t{ PCI_DEVICE_SUB(PCI_VENDOR_ID_CAVIUM,\n\t\t\t PCI_DEVICE_ID_THUNDER_NIC_VF,\n\t\t\t PCI_VENDOR_ID_CAVIUM,\n\t\t\t PCI_SUBSYS_DEVID_88XX_NIC_VF) },\n\t{ PCI_DEVICE_SUB(PCI_VENDOR_ID_CAVIUM,\n\t\t\t PCI_DEVICE_ID_THUNDER_PASS1_NIC_VF,\n\t\t\t PCI_VENDOR_ID_CAVIUM,\n\t\t\t PCI_SUBSYS_DEVID_88XX_PASS1_NIC_VF) },\n\t{ PCI_DEVICE_SUB(PCI_VENDOR_ID_CAVIUM,\n\t\t\t PCI_DEVICE_ID_THUNDER_NIC_VF,\n\t\t\t PCI_VENDOR_ID_CAVIUM,\n\t\t\t PCI_SUBSYS_DEVID_81XX_NIC_VF) },\n\t{ PCI_DEVICE_SUB(PCI_VENDOR_ID_CAVIUM,\n\t\t\t PCI_DEVICE_ID_THUNDER_NIC_VF,\n\t\t\t PCI_VENDOR_ID_CAVIUM,\n\t\t\t PCI_SUBSYS_DEVID_83XX_NIC_VF) },\n\t{ 0, }   \n};\n\nMODULE_AUTHOR(\"Sunil Goutham\");\nMODULE_DESCRIPTION(\"Cavium Thunder NIC Virtual Function Driver\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_VERSION(DRV_VERSION);\nMODULE_DEVICE_TABLE(pci, nicvf_id_table);\n\nstatic int debug = 0x00;\nmodule_param(debug, int, 0644);\nMODULE_PARM_DESC(debug, \"Debug message level bitmap\");\n\nstatic int cpi_alg = CPI_ALG_NONE;\nmodule_param(cpi_alg, int, 0444);\nMODULE_PARM_DESC(cpi_alg,\n\t\t \"PFC algorithm (0=none, 1=VLAN, 2=VLAN16, 3=IP Diffserv)\");\n\nstatic inline u8 nicvf_netdev_qidx(struct nicvf *nic, u8 qidx)\n{\n\tif (nic->sqs_mode)\n\t\treturn qidx + ((nic->sqs_id + 1) * MAX_CMP_QUEUES_PER_QS);\n\telse\n\t\treturn qidx;\n}\n\n \n\n \nvoid nicvf_reg_write(struct nicvf *nic, u64 offset, u64 val)\n{\n\twriteq_relaxed(val, nic->reg_base + offset);\n}\n\nu64 nicvf_reg_read(struct nicvf *nic, u64 offset)\n{\n\treturn readq_relaxed(nic->reg_base + offset);\n}\n\nvoid nicvf_queue_reg_write(struct nicvf *nic, u64 offset,\n\t\t\t   u64 qidx, u64 val)\n{\n\tvoid __iomem *addr = nic->reg_base + offset;\n\n\twriteq_relaxed(val, addr + (qidx << NIC_Q_NUM_SHIFT));\n}\n\nu64 nicvf_queue_reg_read(struct nicvf *nic, u64 offset, u64 qidx)\n{\n\tvoid __iomem *addr = nic->reg_base + offset;\n\n\treturn readq_relaxed(addr + (qidx << NIC_Q_NUM_SHIFT));\n}\n\n \nstatic void nicvf_write_to_mbx(struct nicvf *nic, union nic_mbx *mbx)\n{\n\tu64 *msg = (u64 *)mbx;\n\n\tnicvf_reg_write(nic, NIC_VF_PF_MAILBOX_0_1 + 0, msg[0]);\n\tnicvf_reg_write(nic, NIC_VF_PF_MAILBOX_0_1 + 8, msg[1]);\n}\n\nint nicvf_send_msg_to_pf(struct nicvf *nic, union nic_mbx *mbx)\n{\n\tunsigned long timeout;\n\tint ret = 0;\n\n\tmutex_lock(&nic->rx_mode_mtx);\n\n\tnic->pf_acked = false;\n\tnic->pf_nacked = false;\n\n\tnicvf_write_to_mbx(nic, mbx);\n\n\ttimeout = jiffies + msecs_to_jiffies(NIC_MBOX_MSG_TIMEOUT);\n\t \n\twhile (!nic->pf_acked) {\n\t\tif (nic->pf_nacked) {\n\t\t\tnetdev_err(nic->netdev,\n\t\t\t\t   \"PF NACK to mbox msg 0x%02x from VF%d\\n\",\n\t\t\t\t   (mbx->msg.msg & 0xFF), nic->vf_id);\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tusleep_range(8000, 10000);\n\t\tif (nic->pf_acked)\n\t\t\tbreak;\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tnetdev_err(nic->netdev,\n\t\t\t\t   \"PF didn't ACK to mbox msg 0x%02x from VF%d\\n\",\n\t\t\t\t   (mbx->msg.msg & 0xFF), nic->vf_id);\n\t\t\tret = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&nic->rx_mode_mtx);\n\treturn ret;\n}\n\n \nstatic int nicvf_check_pf_ready(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.msg.msg = NIC_MBOX_MSG_READY;\n\tif (nicvf_send_msg_to_pf(nic, &mbx)) {\n\t\tnetdev_err(nic->netdev,\n\t\t\t   \"PF didn't respond to READY msg\\n\");\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic void nicvf_send_cfg_done(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.msg.msg = NIC_MBOX_MSG_CFG_DONE;\n\tif (nicvf_send_msg_to_pf(nic, &mbx)) {\n\t\tnetdev_err(nic->netdev,\n\t\t\t   \"PF didn't respond to CFG DONE msg\\n\");\n\t}\n}\n\nstatic void nicvf_read_bgx_stats(struct nicvf *nic, struct bgx_stats_msg *bgx)\n{\n\tif (bgx->rx)\n\t\tnic->bgx_stats.rx_stats[bgx->idx] = bgx->stats;\n\telse\n\t\tnic->bgx_stats.tx_stats[bgx->idx] = bgx->stats;\n}\n\nstatic void  nicvf_handle_mbx_intr(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\tu64 *mbx_data;\n\tu64 mbx_addr;\n\tint i;\n\n\tmbx_addr = NIC_VF_PF_MAILBOX_0_1;\n\tmbx_data = (u64 *)&mbx;\n\n\tfor (i = 0; i < NIC_PF_VF_MAILBOX_SIZE; i++) {\n\t\t*mbx_data = nicvf_reg_read(nic, mbx_addr);\n\t\tmbx_data++;\n\t\tmbx_addr += sizeof(u64);\n\t}\n\n\tnetdev_dbg(nic->netdev, \"Mbox message: msg: 0x%x\\n\", mbx.msg.msg);\n\tswitch (mbx.msg.msg) {\n\tcase NIC_MBOX_MSG_READY:\n\t\tnic->pf_acked = true;\n\t\tnic->vf_id = mbx.nic_cfg.vf_id & 0x7F;\n\t\tnic->tns_mode = mbx.nic_cfg.tns_mode & 0x7F;\n\t\tnic->node = mbx.nic_cfg.node_id;\n\t\tif (!nic->set_mac_pending)\n\t\t\teth_hw_addr_set(nic->netdev, mbx.nic_cfg.mac_addr);\n\t\tnic->sqs_mode = mbx.nic_cfg.sqs_mode;\n\t\tnic->loopback_supported = mbx.nic_cfg.loopback_supported;\n\t\tnic->link_up = false;\n\t\tnic->duplex = 0;\n\t\tnic->speed = 0;\n\t\tbreak;\n\tcase NIC_MBOX_MSG_ACK:\n\t\tnic->pf_acked = true;\n\t\tbreak;\n\tcase NIC_MBOX_MSG_NACK:\n\t\tnic->pf_nacked = true;\n\t\tbreak;\n\tcase NIC_MBOX_MSG_RSS_SIZE:\n\t\tnic->rss_info.rss_size = mbx.rss_size.ind_tbl_size;\n\t\tnic->pf_acked = true;\n\t\tbreak;\n\tcase NIC_MBOX_MSG_BGX_STATS:\n\t\tnicvf_read_bgx_stats(nic, &mbx.bgx_stats);\n\t\tnic->pf_acked = true;\n\t\tbreak;\n\tcase NIC_MBOX_MSG_BGX_LINK_CHANGE:\n\t\tnic->pf_acked = true;\n\t\tif (nic->link_up != mbx.link_status.link_up) {\n\t\t\tnic->link_up = mbx.link_status.link_up;\n\t\t\tnic->duplex = mbx.link_status.duplex;\n\t\t\tnic->speed = mbx.link_status.speed;\n\t\t\tnic->mac_type = mbx.link_status.mac_type;\n\t\t\tif (nic->link_up) {\n\t\t\t\tnetdev_info(nic->netdev,\n\t\t\t\t\t    \"Link is Up %d Mbps %s duplex\\n\",\n\t\t\t\t\t    nic->speed,\n\t\t\t\t\t    nic->duplex == DUPLEX_FULL ?\n\t\t\t\t\t    \"Full\" : \"Half\");\n\t\t\t\tnetif_carrier_on(nic->netdev);\n\t\t\t\tnetif_tx_start_all_queues(nic->netdev);\n\t\t\t} else {\n\t\t\t\tnetdev_info(nic->netdev, \"Link is Down\\n\");\n\t\t\t\tnetif_carrier_off(nic->netdev);\n\t\t\t\tnetif_tx_stop_all_queues(nic->netdev);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase NIC_MBOX_MSG_ALLOC_SQS:\n\t\tnic->sqs_count = mbx.sqs_alloc.qs_count;\n\t\tnic->pf_acked = true;\n\t\tbreak;\n\tcase NIC_MBOX_MSG_SNICVF_PTR:\n\t\t \n\t\tnic->snicvf[mbx.nicvf.sqs_id] =\n\t\t\t(struct nicvf *)mbx.nicvf.nicvf;\n\t\tnic->pf_acked = true;\n\t\tbreak;\n\tcase NIC_MBOX_MSG_PNICVF_PTR:\n\t\t \n\t\tnic->pnicvf = (struct nicvf *)mbx.nicvf.nicvf;\n\t\tnic->pf_acked = true;\n\t\tbreak;\n\tcase NIC_MBOX_MSG_PFC:\n\t\tnic->pfc.autoneg = mbx.pfc.autoneg;\n\t\tnic->pfc.fc_rx = mbx.pfc.fc_rx;\n\t\tnic->pfc.fc_tx = mbx.pfc.fc_tx;\n\t\tnic->pf_acked = true;\n\t\tbreak;\n\tdefault:\n\t\tnetdev_err(nic->netdev,\n\t\t\t   \"Invalid message from PF, msg 0x%x\\n\", mbx.msg.msg);\n\t\tbreak;\n\t}\n\tnicvf_clear_intr(nic, NICVF_INTR_MBOX, 0);\n}\n\nstatic int nicvf_hw_set_mac_addr(struct nicvf *nic, struct net_device *netdev)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.mac.msg = NIC_MBOX_MSG_SET_MAC;\n\tmbx.mac.vf_id = nic->vf_id;\n\tether_addr_copy(mbx.mac.mac_addr, netdev->dev_addr);\n\n\treturn nicvf_send_msg_to_pf(nic, &mbx);\n}\n\nstatic void nicvf_config_cpi(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.cpi_cfg.msg = NIC_MBOX_MSG_CPI_CFG;\n\tmbx.cpi_cfg.vf_id = nic->vf_id;\n\tmbx.cpi_cfg.cpi_alg = nic->cpi_alg;\n\tmbx.cpi_cfg.rq_cnt = nic->qs->rq_cnt;\n\n\tnicvf_send_msg_to_pf(nic, &mbx);\n}\n\nstatic void nicvf_get_rss_size(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.rss_size.msg = NIC_MBOX_MSG_RSS_SIZE;\n\tmbx.rss_size.vf_id = nic->vf_id;\n\tnicvf_send_msg_to_pf(nic, &mbx);\n}\n\nvoid nicvf_config_rss(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\tstruct nicvf_rss_info *rss = &nic->rss_info;\n\tint ind_tbl_len = rss->rss_size;\n\tint i, nextq = 0;\n\n\tmbx.rss_cfg.vf_id = nic->vf_id;\n\tmbx.rss_cfg.hash_bits = rss->hash_bits;\n\twhile (ind_tbl_len) {\n\t\tmbx.rss_cfg.tbl_offset = nextq;\n\t\tmbx.rss_cfg.tbl_len = min(ind_tbl_len,\n\t\t\t\t\t       RSS_IND_TBL_LEN_PER_MBX_MSG);\n\t\tmbx.rss_cfg.msg = mbx.rss_cfg.tbl_offset ?\n\t\t\t  NIC_MBOX_MSG_RSS_CFG_CONT : NIC_MBOX_MSG_RSS_CFG;\n\n\t\tfor (i = 0; i < mbx.rss_cfg.tbl_len; i++)\n\t\t\tmbx.rss_cfg.ind_tbl[i] = rss->ind_tbl[nextq++];\n\n\t\tnicvf_send_msg_to_pf(nic, &mbx);\n\n\t\tind_tbl_len -= mbx.rss_cfg.tbl_len;\n\t}\n}\n\nvoid nicvf_set_rss_key(struct nicvf *nic)\n{\n\tstruct nicvf_rss_info *rss = &nic->rss_info;\n\tu64 key_addr = NIC_VNIC_RSS_KEY_0_4;\n\tint idx;\n\n\tfor (idx = 0; idx < RSS_HASH_KEY_SIZE; idx++) {\n\t\tnicvf_reg_write(nic, key_addr, rss->key[idx]);\n\t\tkey_addr += sizeof(u64);\n\t}\n}\n\nstatic int nicvf_rss_init(struct nicvf *nic)\n{\n\tstruct nicvf_rss_info *rss = &nic->rss_info;\n\tint idx;\n\n\tnicvf_get_rss_size(nic);\n\n\tif (cpi_alg != CPI_ALG_NONE) {\n\t\trss->enable = false;\n\t\trss->hash_bits = 0;\n\t\treturn 0;\n\t}\n\n\trss->enable = true;\n\n\tnetdev_rss_key_fill(rss->key, RSS_HASH_KEY_SIZE * sizeof(u64));\n\tnicvf_set_rss_key(nic);\n\n\trss->cfg = RSS_IP_HASH_ENA | RSS_TCP_HASH_ENA | RSS_UDP_HASH_ENA;\n\tnicvf_reg_write(nic, NIC_VNIC_RSS_CFG, rss->cfg);\n\n\trss->hash_bits =  ilog2(rounddown_pow_of_two(rss->rss_size));\n\n\tfor (idx = 0; idx < rss->rss_size; idx++)\n\t\trss->ind_tbl[idx] = ethtool_rxfh_indir_default(idx,\n\t\t\t\t\t\t\t       nic->rx_queues);\n\tnicvf_config_rss(nic);\n\treturn 1;\n}\n\n \nstatic void nicvf_request_sqs(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\tint sqs;\n\tint sqs_count = nic->sqs_count;\n\tint rx_queues = 0, tx_queues = 0;\n\n\t \n\tif (nic->sqs_mode ||  !nic->sqs_count)\n\t\treturn;\n\n\tmbx.sqs_alloc.msg = NIC_MBOX_MSG_ALLOC_SQS;\n\tmbx.sqs_alloc.vf_id = nic->vf_id;\n\tmbx.sqs_alloc.qs_count = nic->sqs_count;\n\tif (nicvf_send_msg_to_pf(nic, &mbx)) {\n\t\t \n\t\tnic->sqs_count = 0;\n\t\treturn;\n\t}\n\n\t \n\tif (!nic->sqs_count)\n\t\treturn;\n\n\tif (nic->rx_queues > MAX_RCV_QUEUES_PER_QS)\n\t\trx_queues = nic->rx_queues - MAX_RCV_QUEUES_PER_QS;\n\n\ttx_queues = nic->tx_queues + nic->xdp_tx_queues;\n\tif (tx_queues > MAX_SND_QUEUES_PER_QS)\n\t\ttx_queues = tx_queues - MAX_SND_QUEUES_PER_QS;\n\n\t \n\tfor (sqs = 0; sqs < nic->sqs_count; sqs++) {\n\t\tmbx.nicvf.msg = NIC_MBOX_MSG_SNICVF_PTR;\n\t\tmbx.nicvf.vf_id = nic->vf_id;\n\t\tmbx.nicvf.sqs_id = sqs;\n\t\tnicvf_send_msg_to_pf(nic, &mbx);\n\n\t\tnic->snicvf[sqs]->sqs_id = sqs;\n\t\tif (rx_queues > MAX_RCV_QUEUES_PER_QS) {\n\t\t\tnic->snicvf[sqs]->qs->rq_cnt = MAX_RCV_QUEUES_PER_QS;\n\t\t\trx_queues -= MAX_RCV_QUEUES_PER_QS;\n\t\t} else {\n\t\t\tnic->snicvf[sqs]->qs->rq_cnt = rx_queues;\n\t\t\trx_queues = 0;\n\t\t}\n\n\t\tif (tx_queues > MAX_SND_QUEUES_PER_QS) {\n\t\t\tnic->snicvf[sqs]->qs->sq_cnt = MAX_SND_QUEUES_PER_QS;\n\t\t\ttx_queues -= MAX_SND_QUEUES_PER_QS;\n\t\t} else {\n\t\t\tnic->snicvf[sqs]->qs->sq_cnt = tx_queues;\n\t\t\ttx_queues = 0;\n\t\t}\n\n\t\tnic->snicvf[sqs]->qs->cq_cnt =\n\t\tmax(nic->snicvf[sqs]->qs->rq_cnt, nic->snicvf[sqs]->qs->sq_cnt);\n\n\t\t \n\t\tnicvf_open(nic->snicvf[sqs]->netdev);\n\t}\n\n\t \n\tif (sqs_count != nic->sqs_count)\n\t\tnicvf_set_real_num_queues(nic->netdev,\n\t\t\t\t\t  nic->tx_queues, nic->rx_queues);\n}\n\n \nstatic void nicvf_send_vf_struct(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.nicvf.msg = NIC_MBOX_MSG_NICVF_PTR;\n\tmbx.nicvf.sqs_mode = nic->sqs_mode;\n\tmbx.nicvf.nicvf = (u64)nic;\n\tnicvf_send_msg_to_pf(nic, &mbx);\n}\n\nstatic void nicvf_get_primary_vf_struct(struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.nicvf.msg = NIC_MBOX_MSG_PNICVF_PTR;\n\tnicvf_send_msg_to_pf(nic, &mbx);\n}\n\nint nicvf_set_real_num_queues(struct net_device *netdev,\n\t\t\t      int tx_queues, int rx_queues)\n{\n\tint err = 0;\n\n\terr = netif_set_real_num_tx_queues(netdev, tx_queues);\n\tif (err) {\n\t\tnetdev_err(netdev,\n\t\t\t   \"Failed to set no of Tx queues: %d\\n\", tx_queues);\n\t\treturn err;\n\t}\n\n\terr = netif_set_real_num_rx_queues(netdev, rx_queues);\n\tif (err)\n\t\tnetdev_err(netdev,\n\t\t\t   \"Failed to set no of Rx queues: %d\\n\", rx_queues);\n\treturn err;\n}\n\nstatic int nicvf_init_resources(struct nicvf *nic)\n{\n\tint err;\n\n\t \n\tnicvf_qset_config(nic, true);\n\n\t \n\terr = nicvf_config_data_transfer(nic, true);\n\tif (err) {\n\t\tnetdev_err(nic->netdev,\n\t\t\t   \"Failed to alloc/config VF's QSet resources\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic inline bool nicvf_xdp_rx(struct nicvf *nic, struct bpf_prog *prog,\n\t\t\t\tstruct cqe_rx_t *cqe_rx, struct snd_queue *sq,\n\t\t\t\tstruct rcv_queue *rq, struct sk_buff **skb)\n{\n\tunsigned char *hard_start, *data;\n\tstruct xdp_buff xdp;\n\tstruct page *page;\n\tu32 action;\n\tu16 len, offset = 0;\n\tu64 dma_addr, cpu_addr;\n\tvoid *orig_data;\n\n\t \n\tlen = *((u16 *)((void *)cqe_rx + (3 * sizeof(u64))));\n\tdma_addr = *((u64 *)((void *)cqe_rx + (7 * sizeof(u64))));\n\n\tcpu_addr = nicvf_iova_to_phys(nic, dma_addr);\n\tif (!cpu_addr)\n\t\treturn false;\n\tcpu_addr = (u64)phys_to_virt(cpu_addr);\n\tpage = virt_to_page((void *)cpu_addr);\n\n\txdp_init_buff(&xdp, RCV_FRAG_LEN + XDP_PACKET_HEADROOM,\n\t\t      &rq->xdp_rxq);\n\thard_start = page_address(page);\n\tdata = (unsigned char *)cpu_addr;\n\txdp_prepare_buff(&xdp, hard_start, data - hard_start, len, false);\n\torig_data = xdp.data;\n\n\taction = bpf_prog_run_xdp(prog, &xdp);\n\n\tlen = xdp.data_end - xdp.data;\n\t \n\tif (orig_data != xdp.data) {\n\t\toffset = orig_data - xdp.data;\n\t\tdma_addr -= offset;\n\t}\n\n\tswitch (action) {\n\tcase XDP_PASS:\n\t\t \n\t\tif (page_ref_count(page) == 1) {\n\t\t\tdma_addr &= PAGE_MASK;\n\t\t\tdma_unmap_page_attrs(&nic->pdev->dev, dma_addr,\n\t\t\t\t\t     RCV_FRAG_LEN + XDP_PACKET_HEADROOM,\n\t\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t\t     DMA_ATTR_SKIP_CPU_SYNC);\n\t\t}\n\n\t\t \n\t\t*skb = build_skb(xdp.data,\n\t\t\t\t RCV_FRAG_LEN - cqe_rx->align_pad + offset);\n\t\tif (!*skb)\n\t\t\tput_page(page);\n\t\telse\n\t\t\tskb_put(*skb, len);\n\t\treturn false;\n\tcase XDP_TX:\n\t\tnicvf_xdp_sq_append_pkt(nic, sq, (u64)xdp.data, dma_addr, len);\n\t\treturn true;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(nic->netdev, prog, action);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(nic->netdev, prog, action);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\t \n\t\tif (page_ref_count(page) == 1) {\n\t\t\tdma_addr &= PAGE_MASK;\n\t\t\tdma_unmap_page_attrs(&nic->pdev->dev, dma_addr,\n\t\t\t\t\t     RCV_FRAG_LEN + XDP_PACKET_HEADROOM,\n\t\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t\t     DMA_ATTR_SKIP_CPU_SYNC);\n\t\t}\n\t\tput_page(page);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void nicvf_snd_ptp_handler(struct net_device *netdev,\n\t\t\t\t  struct cqe_send_t *cqe_tx)\n{\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct skb_shared_hwtstamps ts;\n\tu64 ns;\n\n\tnic = nic->pnicvf;\n\n\t \n\tsmp_rmb();\n\n\t \n\tatomic_set(&nic->tx_ptp_skbs, 0);\n\n\t \n\tif (!nic->ptp_skb)\n\t\treturn;\n\n\t \n\tif (cqe_tx->send_status == CQ_TX_ERROP_TSTMP_TIMEOUT ||\n\t    cqe_tx->send_status == CQ_TX_ERROP_TSTMP_CONFLICT)\n\t\tgoto no_tstamp;\n\n\t \n\tmemset(&ts, 0, sizeof(ts));\n\tns = cavium_ptp_tstamp2time(nic->ptp_clock, cqe_tx->ptp_timestamp);\n\tts.hwtstamp = ns_to_ktime(ns);\n\tskb_tstamp_tx(nic->ptp_skb, &ts);\n\nno_tstamp:\n\t \n\tdev_kfree_skb_any(nic->ptp_skb);\n\tnic->ptp_skb = NULL;\n\t \n\tsmp_wmb();\n}\n\nstatic void nicvf_snd_pkt_handler(struct net_device *netdev,\n\t\t\t\t  struct cqe_send_t *cqe_tx,\n\t\t\t\t  int budget, int *subdesc_cnt,\n\t\t\t\t  unsigned int *tx_pkts, unsigned int *tx_bytes)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct page *page;\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct snd_queue *sq;\n\tstruct sq_hdr_subdesc *hdr;\n\tstruct sq_hdr_subdesc *tso_sqe;\n\n\tsq = &nic->qs->sq[cqe_tx->sq_idx];\n\n\thdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, cqe_tx->sqe_ptr);\n\tif (hdr->subdesc_type != SQ_DESC_TYPE_HEADER)\n\t\treturn;\n\n\t \n\tif (cqe_tx->send_status)\n\t\tnicvf_check_cqe_tx_errs(nic->pnicvf, cqe_tx);\n\n\t \n\tif (sq->is_xdp) {\n\t\tpage = (struct page *)sq->xdp_page[cqe_tx->sqe_ptr];\n\t\t \n\t\tif (page && (page_ref_count(page) == 1))\n\t\t\tnicvf_unmap_sndq_buffers(nic, sq, cqe_tx->sqe_ptr,\n\t\t\t\t\t\t hdr->subdesc_cnt);\n\n\t\t \n\t\tif (page)\n\t\t\tput_page(page);\n\t\tsq->xdp_page[cqe_tx->sqe_ptr] = (u64)NULL;\n\t\t*subdesc_cnt += hdr->subdesc_cnt + 1;\n\t\treturn;\n\t}\n\n\tskb = (struct sk_buff *)sq->skbuff[cqe_tx->sqe_ptr];\n\tif (skb) {\n\t\t \n\t\tif (hdr->dont_send) {\n\t\t\t \n\t\t\ttso_sqe =\n\t\t\t (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, hdr->rsvd2);\n\t\t\tnicvf_unmap_sndq_buffers(nic, sq, hdr->rsvd2,\n\t\t\t\t\t\t tso_sqe->subdesc_cnt);\n\t\t\t*subdesc_cnt += tso_sqe->subdesc_cnt + 1;\n\t\t} else {\n\t\t\tnicvf_unmap_sndq_buffers(nic, sq, cqe_tx->sqe_ptr,\n\t\t\t\t\t\t hdr->subdesc_cnt);\n\t\t}\n\t\t*subdesc_cnt += hdr->subdesc_cnt + 1;\n\t\tprefetch(skb);\n\t\t(*tx_pkts)++;\n\t\t*tx_bytes += skb->len;\n\t\t \n\t\tif (skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS &&\n\t\t    !nic->pnicvf->ptp_skb)\n\t\t\tnic->pnicvf->ptp_skb = skb;\n\t\telse\n\t\t\tnapi_consume_skb(skb, budget);\n\t\tsq->skbuff[cqe_tx->sqe_ptr] = (u64)NULL;\n\t} else {\n\t\t \n\t\tif (!nic->hw_tso)\n\t\t\t*subdesc_cnt += hdr->subdesc_cnt + 1;\n\t}\n}\n\nstatic inline void nicvf_set_rxhash(struct net_device *netdev,\n\t\t\t\t    struct cqe_rx_t *cqe_rx,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tu8 hash_type;\n\tu32 hash;\n\n\tif (!(netdev->features & NETIF_F_RXHASH))\n\t\treturn;\n\n\tswitch (cqe_rx->rss_alg) {\n\tcase RSS_ALG_TCP_IP:\n\tcase RSS_ALG_UDP_IP:\n\t\thash_type = PKT_HASH_TYPE_L4;\n\t\thash = cqe_rx->rss_tag;\n\t\tbreak;\n\tcase RSS_ALG_IP:\n\t\thash_type = PKT_HASH_TYPE_L3;\n\t\thash = cqe_rx->rss_tag;\n\t\tbreak;\n\tdefault:\n\t\thash_type = PKT_HASH_TYPE_NONE;\n\t\thash = 0;\n\t}\n\n\tskb_set_hash(skb, hash, hash_type);\n}\n\nstatic inline void nicvf_set_rxtstamp(struct nicvf *nic, struct sk_buff *skb)\n{\n\tu64 ns;\n\n\tif (!nic->ptp_clock || !nic->hw_rx_tstamp)\n\t\treturn;\n\n\t \n\tns = cavium_ptp_tstamp2time(nic->ptp_clock,\n\t\t\t\t    be64_to_cpu(*(__be64 *)skb->data));\n\tskb_hwtstamps(skb)->hwtstamp = ns_to_ktime(ns);\n\n\t__skb_pull(skb, 8);\n}\n\nstatic void nicvf_rcv_pkt_handler(struct net_device *netdev,\n\t\t\t\t  struct napi_struct *napi,\n\t\t\t\t  struct cqe_rx_t *cqe_rx,\n\t\t\t\t  struct snd_queue *sq, struct rcv_queue *rq)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct nicvf *snic = nic;\n\tint err = 0;\n\tint rq_idx;\n\n\trq_idx = nicvf_netdev_qidx(nic, cqe_rx->rq_idx);\n\n\tif (nic->sqs_mode) {\n\t\t \n\t\tnic = nic->pnicvf;\n\t\tnetdev = nic->netdev;\n\t}\n\n\t \n\tif (cqe_rx->err_level || cqe_rx->err_opcode) {\n\t\terr = nicvf_check_cqe_rx_errs(nic, cqe_rx);\n\t\tif (err && !cqe_rx->rb_cnt)\n\t\t\treturn;\n\t}\n\n\t \n\tif (nic->xdp_prog && (cqe_rx->rb_cnt == 1)) {\n\t\t \n\t\tif (nicvf_xdp_rx(snic, nic->xdp_prog, cqe_rx, sq, rq, &skb))\n\t\t\treturn;\n\t} else {\n\t\tskb = nicvf_get_rcv_skb(snic, cqe_rx,\n\t\t\t\t\tnic->xdp_prog ? true : false);\n\t}\n\n\tif (!skb)\n\t\treturn;\n\n\tif (netif_msg_pktdata(nic)) {\n\t\tnetdev_info(nic->netdev, \"skb 0x%p, len=%d\\n\", skb, skb->len);\n\t\tprint_hex_dump(KERN_INFO, \"\", DUMP_PREFIX_OFFSET, 16, 1,\n\t\t\t       skb->data, skb->len, true);\n\t}\n\n\t \n\tif (err) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn;\n\t}\n\n\tnicvf_set_rxtstamp(nic, skb);\n\tnicvf_set_rxhash(netdev, cqe_rx, skb);\n\n\tskb_record_rx_queue(skb, rq_idx);\n\tif (netdev->hw_features & NETIF_F_RXCSUM) {\n\t\t \n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t} else {\n\t\tskb_checksum_none_assert(skb);\n\t}\n\n\tskb->protocol = eth_type_trans(skb, netdev);\n\n\t \n\tif (cqe_rx->vlan_found && cqe_rx->vlan_stripped)\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t       ntohs((__force __be16)cqe_rx->vlan_tci));\n\n\tif (napi && (netdev->features & NETIF_F_GRO))\n\t\tnapi_gro_receive(napi, skb);\n\telse\n\t\tnetif_receive_skb(skb);\n}\n\nstatic int nicvf_cq_intr_handler(struct net_device *netdev, u8 cq_idx,\n\t\t\t\t struct napi_struct *napi, int budget)\n{\n\tint processed_cqe, work_done = 0, tx_done = 0;\n\tint cqe_count, cqe_head;\n\tint subdesc_cnt = 0;\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct queue_set *qs = nic->qs;\n\tstruct cmp_queue *cq = &qs->cq[cq_idx];\n\tstruct cqe_rx_t *cq_desc;\n\tstruct netdev_queue *txq;\n\tstruct snd_queue *sq = &qs->sq[cq_idx];\n\tstruct rcv_queue *rq = &qs->rq[cq_idx];\n\tunsigned int tx_pkts = 0, tx_bytes = 0, txq_idx;\n\n\tspin_lock_bh(&cq->lock);\nloop:\n\tprocessed_cqe = 0;\n\t \n\tcqe_count = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_STATUS, cq_idx);\n\tcqe_count &= CQ_CQE_COUNT;\n\tif (!cqe_count)\n\t\tgoto done;\n\n\t \n\tcqe_head = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_HEAD, cq_idx) >> 9;\n\tcqe_head &= 0xFFFF;\n\n\twhile (processed_cqe < cqe_count) {\n\t\t \n\t\tcq_desc = (struct cqe_rx_t *)GET_CQ_DESC(cq, cqe_head);\n\t\tcqe_head++;\n\t\tcqe_head &= (cq->dmem.q_len - 1);\n\t\t \n\t\tprefetch((struct cqe_rx_t *)GET_CQ_DESC(cq, cqe_head));\n\n\t\tif ((work_done >= budget) && napi &&\n\t\t    (cq_desc->cqe_type != CQE_TYPE_SEND)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (cq_desc->cqe_type) {\n\t\tcase CQE_TYPE_RX:\n\t\t\tnicvf_rcv_pkt_handler(netdev, napi, cq_desc, sq, rq);\n\t\t\twork_done++;\n\t\tbreak;\n\t\tcase CQE_TYPE_SEND:\n\t\t\tnicvf_snd_pkt_handler(netdev, (void *)cq_desc,\n\t\t\t\t\t      budget, &subdesc_cnt,\n\t\t\t\t\t      &tx_pkts, &tx_bytes);\n\t\t\ttx_done++;\n\t\tbreak;\n\t\tcase CQE_TYPE_SEND_PTP:\n\t\t\tnicvf_snd_ptp_handler(netdev, (void *)cq_desc);\n\t\tbreak;\n\t\tcase CQE_TYPE_INVALID:\n\t\tcase CQE_TYPE_RX_SPLIT:\n\t\tcase CQE_TYPE_RX_TCP:\n\t\t\t \n\t\tbreak;\n\t\t}\n\t\tprocessed_cqe++;\n\t}\n\n\t \n\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_DOOR,\n\t\t\t      cq_idx, processed_cqe);\n\n\tif ((work_done < budget) && napi)\n\t\tgoto loop;\n\ndone:\n\t \n\tif (subdesc_cnt)\n\t\tnicvf_put_sq_desc(sq, subdesc_cnt);\n\n\ttxq_idx = nicvf_netdev_qidx(nic, cq_idx);\n\t \n\tif (nic->pnicvf->xdp_prog) {\n\t\tif (txq_idx < nic->pnicvf->xdp_tx_queues) {\n\t\t\tnicvf_xdp_sq_doorbell(nic, sq, cq_idx);\n\t\t\tgoto out;\n\t\t}\n\t\tnic = nic->pnicvf;\n\t\ttxq_idx -= nic->pnicvf->xdp_tx_queues;\n\t}\n\n\t \n\tif (tx_done ||\n\t    (atomic_read(&sq->free_cnt) >= MIN_SQ_DESC_PER_PKT_XMIT)) {\n\t\tnetdev = nic->pnicvf->netdev;\n\t\ttxq = netdev_get_tx_queue(netdev, txq_idx);\n\t\tif (tx_pkts)\n\t\t\tnetdev_tx_completed_queue(txq, tx_pkts, tx_bytes);\n\n\t\t \n\t\tsmp_mb();\n\t\tif (netif_tx_queue_stopped(txq) && netif_carrier_ok(netdev)) {\n\t\t\tnetif_tx_wake_queue(txq);\n\t\t\tnic = nic->pnicvf;\n\t\t\tthis_cpu_inc(nic->drv_stats->txq_wake);\n\t\t\tnetif_warn(nic, tx_err, netdev,\n\t\t\t\t   \"Transmit queue wakeup SQ%d\\n\", txq_idx);\n\t\t}\n\t}\n\nout:\n\tspin_unlock_bh(&cq->lock);\n\treturn work_done;\n}\n\nstatic int nicvf_poll(struct napi_struct *napi, int budget)\n{\n\tu64  cq_head;\n\tint  work_done = 0;\n\tstruct net_device *netdev = napi->dev;\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct nicvf_cq_poll *cq;\n\n\tcq = container_of(napi, struct nicvf_cq_poll, napi);\n\twork_done = nicvf_cq_intr_handler(netdev, cq->cq_idx, napi, budget);\n\n\tif (work_done < budget) {\n\t\t \n\t\tnapi_complete_done(napi, work_done);\n\t\t \n\t\tcq_head = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_HEAD,\n\t\t\t\t\t       cq->cq_idx);\n\t\tnicvf_clear_intr(nic, NICVF_INTR_CQ, cq->cq_idx);\n\t\tnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_HEAD,\n\t\t\t\t      cq->cq_idx, cq_head);\n\t\tnicvf_enable_intr(nic, NICVF_INTR_CQ, cq->cq_idx);\n\t}\n\treturn work_done;\n}\n\n \nstatic void nicvf_handle_qs_err(struct tasklet_struct *t)\n{\n\tstruct nicvf *nic = from_tasklet(nic, t, qs_err_task);\n\tstruct queue_set *qs = nic->qs;\n\tint qidx;\n\tu64 status;\n\n\tnetif_tx_disable(nic->netdev);\n\n\t \n\tfor (qidx = 0; qidx < qs->cq_cnt; qidx++) {\n\t\tstatus = nicvf_queue_reg_read(nic, NIC_QSET_CQ_0_7_STATUS,\n\t\t\t\t\t      qidx);\n\t\tif (!(status & CQ_ERR_MASK))\n\t\t\tcontinue;\n\t\t \n\t\tnicvf_disable_intr(nic, NICVF_INTR_CQ, qidx);\n\t\tnicvf_sq_disable(nic, qidx);\n\t\tnicvf_cq_intr_handler(nic->netdev, qidx, NULL, 0);\n\t\tnicvf_cmp_queue_config(nic, qs, qidx, true);\n\t\tnicvf_sq_free_used_descs(nic->netdev, &qs->sq[qidx], qidx);\n\t\tnicvf_sq_enable(nic, &qs->sq[qidx], qidx);\n\n\t\tnicvf_enable_intr(nic, NICVF_INTR_CQ, qidx);\n\t}\n\n\tnetif_tx_start_all_queues(nic->netdev);\n\t \n\tnicvf_enable_intr(nic, NICVF_INTR_QS_ERR, 0);\n}\n\nstatic void nicvf_dump_intr_status(struct nicvf *nic)\n{\n\tnetif_info(nic, intr, nic->netdev, \"interrupt status 0x%llx\\n\",\n\t\t   nicvf_reg_read(nic, NIC_VF_INT));\n}\n\nstatic irqreturn_t nicvf_misc_intr_handler(int irq, void *nicvf_irq)\n{\n\tstruct nicvf *nic = (struct nicvf *)nicvf_irq;\n\tu64 intr;\n\n\tnicvf_dump_intr_status(nic);\n\n\tintr = nicvf_reg_read(nic, NIC_VF_INT);\n\t \n\tif (!(intr & NICVF_INTR_MBOX_MASK))\n\t\treturn IRQ_HANDLED;\n\n\tnicvf_handle_mbx_intr(nic);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t nicvf_intr_handler(int irq, void *cq_irq)\n{\n\tstruct nicvf_cq_poll *cq_poll = (struct nicvf_cq_poll *)cq_irq;\n\tstruct nicvf *nic = cq_poll->nicvf;\n\tint qidx = cq_poll->cq_idx;\n\n\tnicvf_dump_intr_status(nic);\n\n\t \n\tnicvf_disable_intr(nic, NICVF_INTR_CQ, qidx);\n\n\t \n\tnapi_schedule_irqoff(&cq_poll->napi);\n\n\t \n\tnicvf_clear_intr(nic, NICVF_INTR_CQ, qidx);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t nicvf_rbdr_intr_handler(int irq, void *nicvf_irq)\n{\n\tstruct nicvf *nic = (struct nicvf *)nicvf_irq;\n\tu8 qidx;\n\n\n\tnicvf_dump_intr_status(nic);\n\n\t \n\tfor (qidx = 0; qidx < nic->qs->rbdr_cnt; qidx++) {\n\t\tif (!nicvf_is_intr_enabled(nic, NICVF_INTR_RBDR, qidx))\n\t\t\tcontinue;\n\t\tnicvf_disable_intr(nic, NICVF_INTR_RBDR, qidx);\n\t\ttasklet_hi_schedule(&nic->rbdr_task);\n\t\t \n\t\tnicvf_clear_intr(nic, NICVF_INTR_RBDR, qidx);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t nicvf_qs_err_intr_handler(int irq, void *nicvf_irq)\n{\n\tstruct nicvf *nic = (struct nicvf *)nicvf_irq;\n\n\tnicvf_dump_intr_status(nic);\n\n\t \n\tnicvf_disable_intr(nic, NICVF_INTR_QS_ERR, 0);\n\ttasklet_hi_schedule(&nic->qs_err_task);\n\tnicvf_clear_intr(nic, NICVF_INTR_QS_ERR, 0);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void nicvf_set_irq_affinity(struct nicvf *nic)\n{\n\tint vec, cpu;\n\n\tfor (vec = 0; vec < nic->num_vec; vec++) {\n\t\tif (!nic->irq_allocated[vec])\n\t\t\tcontinue;\n\n\t\tif (!zalloc_cpumask_var(&nic->affinity_mask[vec], GFP_KERNEL))\n\t\t\treturn;\n\t\t  \n\t\tif (vec < NICVF_INTR_ID_SQ)\n\t\t\t \n\t\t\tcpu = nicvf_netdev_qidx(nic, vec) + 1;\n\t\telse\n\t\t\tcpu = 0;\n\n\t\tcpumask_set_cpu(cpumask_local_spread(cpu, nic->node),\n\t\t\t\tnic->affinity_mask[vec]);\n\t\tirq_set_affinity_hint(pci_irq_vector(nic->pdev, vec),\n\t\t\t\t      nic->affinity_mask[vec]);\n\t}\n}\n\nstatic int nicvf_register_interrupts(struct nicvf *nic)\n{\n\tint irq, ret = 0;\n\n\tfor_each_cq_irq(irq)\n\t\tsprintf(nic->irq_name[irq], \"%s-rxtx-%d\",\n\t\t\tnic->pnicvf->netdev->name,\n\t\t\tnicvf_netdev_qidx(nic, irq));\n\n\tfor_each_sq_irq(irq)\n\t\tsprintf(nic->irq_name[irq], \"%s-sq-%d\",\n\t\t\tnic->pnicvf->netdev->name,\n\t\t\tnicvf_netdev_qidx(nic, irq - NICVF_INTR_ID_SQ));\n\n\tfor_each_rbdr_irq(irq)\n\t\tsprintf(nic->irq_name[irq], \"%s-rbdr-%d\",\n\t\t\tnic->pnicvf->netdev->name,\n\t\t\tnic->sqs_mode ? (nic->sqs_id + 1) : 0);\n\n\t \n\tfor (irq = 0; irq < nic->qs->cq_cnt; irq++) {\n\t\tret = request_irq(pci_irq_vector(nic->pdev, irq),\n\t\t\t\t  nicvf_intr_handler,\n\t\t\t\t  0, nic->irq_name[irq], nic->napi[irq]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tnic->irq_allocated[irq] = true;\n\t}\n\n\t \n\tfor (irq = NICVF_INTR_ID_RBDR;\n\t     irq < (NICVF_INTR_ID_RBDR + nic->qs->rbdr_cnt); irq++) {\n\t\tret = request_irq(pci_irq_vector(nic->pdev, irq),\n\t\t\t\t  nicvf_rbdr_intr_handler,\n\t\t\t\t  0, nic->irq_name[irq], nic);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tnic->irq_allocated[irq] = true;\n\t}\n\n\t \n\tsprintf(nic->irq_name[NICVF_INTR_ID_QS_ERR], \"%s-qset-err-%d\",\n\t\tnic->pnicvf->netdev->name,\n\t\tnic->sqs_mode ? (nic->sqs_id + 1) : 0);\n\tirq = NICVF_INTR_ID_QS_ERR;\n\tret = request_irq(pci_irq_vector(nic->pdev, irq),\n\t\t\t  nicvf_qs_err_intr_handler,\n\t\t\t  0, nic->irq_name[irq], nic);\n\tif (ret)\n\t\tgoto err;\n\n\tnic->irq_allocated[irq] = true;\n\n\t \n\tnicvf_set_irq_affinity(nic);\n\nerr:\n\tif (ret)\n\t\tnetdev_err(nic->netdev, \"request_irq failed, vector %d\\n\", irq);\n\n\treturn ret;\n}\n\nstatic void nicvf_unregister_interrupts(struct nicvf *nic)\n{\n\tstruct pci_dev *pdev = nic->pdev;\n\tint irq;\n\n\t \n\tfor (irq = 0; irq < nic->num_vec; irq++) {\n\t\tif (!nic->irq_allocated[irq])\n\t\t\tcontinue;\n\n\t\tirq_set_affinity_hint(pci_irq_vector(pdev, irq), NULL);\n\t\tfree_cpumask_var(nic->affinity_mask[irq]);\n\n\t\tif (irq < NICVF_INTR_ID_SQ)\n\t\t\tfree_irq(pci_irq_vector(pdev, irq), nic->napi[irq]);\n\t\telse\n\t\t\tfree_irq(pci_irq_vector(pdev, irq), nic);\n\n\t\tnic->irq_allocated[irq] = false;\n\t}\n\n\t \n\tpci_free_irq_vectors(pdev);\n\tnic->num_vec = 0;\n}\n\n \nstatic int nicvf_register_misc_interrupt(struct nicvf *nic)\n{\n\tint ret = 0;\n\tint irq = NICVF_INTR_ID_MISC;\n\n\t \n\tif (nic->pdev->msix_enabled)\n\t\treturn 0;\n\n\t \n\tnic->num_vec = pci_msix_vec_count(nic->pdev);\n\tret = pci_alloc_irq_vectors(nic->pdev, nic->num_vec, nic->num_vec,\n\t\t\t\t    PCI_IRQ_MSIX);\n\tif (ret < 0) {\n\t\tnetdev_err(nic->netdev,\n\t\t\t   \"Req for #%d msix vectors failed\\n\", nic->num_vec);\n\t\treturn ret;\n\t}\n\n\tsprintf(nic->irq_name[irq], \"%s Mbox\", \"NICVF\");\n\t \n\tret = request_irq(pci_irq_vector(nic->pdev, irq),\n\t\t\t  nicvf_misc_intr_handler, 0, nic->irq_name[irq], nic);\n\n\tif (ret)\n\t\treturn ret;\n\tnic->irq_allocated[irq] = true;\n\n\t \n\tnicvf_enable_intr(nic, NICVF_INTR_MBOX, 0);\n\n\t \n\tif (!nicvf_check_pf_ready(nic)) {\n\t\tnicvf_disable_intr(nic, NICVF_INTR_MBOX, 0);\n\t\tnicvf_unregister_interrupts(nic);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic netdev_tx_t nicvf_xmit(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tint qid = skb_get_queue_mapping(skb);\n\tstruct netdev_queue *txq = netdev_get_tx_queue(netdev, qid);\n\tstruct nicvf *snic;\n\tstruct snd_queue *sq;\n\tint tmp;\n\n\t \n\tif (skb->len <= ETH_HLEN) {\n\t\tdev_kfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\t \n\tif (nic->xdp_prog)\n\t\tqid += nic->xdp_tx_queues;\n\n\tsnic = nic;\n\t \n\tif (qid >= MAX_SND_QUEUES_PER_QS) {\n\t\ttmp = qid / MAX_SND_QUEUES_PER_QS;\n\t\tsnic = (struct nicvf *)nic->snicvf[tmp - 1];\n\t\tif (!snic) {\n\t\t\tnetdev_warn(nic->netdev,\n\t\t\t\t    \"Secondary Qset#%d's ptr not initialized\\n\",\n\t\t\t\t    tmp - 1);\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tqid = qid % MAX_SND_QUEUES_PER_QS;\n\t}\n\n\tsq = &snic->qs->sq[qid];\n\tif (!netif_tx_queue_stopped(txq) &&\n\t    !nicvf_sq_append_skb(snic, sq, skb, qid)) {\n\t\tnetif_tx_stop_queue(txq);\n\n\t\t \n\t\tsmp_mb();\n\n\t\t \n\t\tif (atomic_read(&sq->free_cnt) > MIN_SQ_DESC_PER_PKT_XMIT) {\n\t\t\tnetif_tx_wake_queue(txq);\n\t\t} else {\n\t\t\tthis_cpu_inc(nic->drv_stats->txq_stop);\n\t\t\tnetif_warn(nic, tx_err, netdev,\n\t\t\t\t   \"Transmit ring full, stopping SQ%d\\n\", qid);\n\t\t}\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic inline void nicvf_free_cq_poll(struct nicvf *nic)\n{\n\tstruct nicvf_cq_poll *cq_poll;\n\tint qidx;\n\n\tfor (qidx = 0; qidx < nic->qs->cq_cnt; qidx++) {\n\t\tcq_poll = nic->napi[qidx];\n\t\tif (!cq_poll)\n\t\t\tcontinue;\n\t\tnic->napi[qidx] = NULL;\n\t\tkfree(cq_poll);\n\t}\n}\n\nint nicvf_stop(struct net_device *netdev)\n{\n\tint irq, qidx;\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct queue_set *qs = nic->qs;\n\tstruct nicvf_cq_poll *cq_poll = NULL;\n\tunion nic_mbx mbx = {};\n\n\t \n\tif (nic->nicvf_rx_mode_wq) {\n\t\tcancel_delayed_work_sync(&nic->link_change_work);\n\t\tdrain_workqueue(nic->nicvf_rx_mode_wq);\n\t}\n\n\tmbx.msg.msg = NIC_MBOX_MSG_SHUTDOWN;\n\tnicvf_send_msg_to_pf(nic, &mbx);\n\n\tnetif_carrier_off(netdev);\n\tnetif_tx_stop_all_queues(nic->netdev);\n\tnic->link_up = false;\n\n\t \n\tif (!nic->sqs_mode) {\n\t\tfor (qidx = 0; qidx < nic->sqs_count; qidx++) {\n\t\t\tif (!nic->snicvf[qidx])\n\t\t\t\tcontinue;\n\t\t\tnicvf_stop(nic->snicvf[qidx]->netdev);\n\t\t\tnic->snicvf[qidx] = NULL;\n\t\t}\n\t}\n\n\t \n\tfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++) {\n\t\tnicvf_disable_intr(nic, NICVF_INTR_RBDR, qidx);\n\t\tnicvf_clear_intr(nic, NICVF_INTR_RBDR, qidx);\n\t}\n\tnicvf_disable_intr(nic, NICVF_INTR_QS_ERR, 0);\n\tnicvf_clear_intr(nic, NICVF_INTR_QS_ERR, 0);\n\n\t \n\tfor (irq = 0; irq < nic->num_vec; irq++)\n\t\tsynchronize_irq(pci_irq_vector(nic->pdev, irq));\n\n\ttasklet_kill(&nic->rbdr_task);\n\ttasklet_kill(&nic->qs_err_task);\n\tif (nic->rb_work_scheduled)\n\t\tcancel_delayed_work_sync(&nic->rbdr_work);\n\n\tfor (qidx = 0; qidx < nic->qs->cq_cnt; qidx++) {\n\t\tcq_poll = nic->napi[qidx];\n\t\tif (!cq_poll)\n\t\t\tcontinue;\n\t\tnapi_synchronize(&cq_poll->napi);\n\t\t \n\t\tnicvf_disable_intr(nic, NICVF_INTR_CQ, qidx);\n\t\tnicvf_clear_intr(nic, NICVF_INTR_CQ, qidx);\n\t\tnapi_disable(&cq_poll->napi);\n\t\tnetif_napi_del(&cq_poll->napi);\n\t}\n\n\tnetif_tx_disable(netdev);\n\n\tfor (qidx = 0; qidx < netdev->num_tx_queues; qidx++)\n\t\tnetdev_tx_reset_queue(netdev_get_tx_queue(netdev, qidx));\n\n\t \n\tnicvf_config_data_transfer(nic, false);\n\n\t \n\tnicvf_qset_config(nic, false);\n\n\t \n\tnicvf_disable_intr(nic, NICVF_INTR_MBOX, 0);\n\n\tnicvf_unregister_interrupts(nic);\n\n\tnicvf_free_cq_poll(nic);\n\n\t \n\tif (nic->ptp_skb) {\n\t\tdev_kfree_skb_any(nic->ptp_skb);\n\t\tnic->ptp_skb = NULL;\n\t}\n\n\t \n\tnic->pnicvf = nic;\n\n\treturn 0;\n}\n\nstatic int nicvf_config_hw_rx_tstamp(struct nicvf *nic, bool enable)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.ptp.msg = NIC_MBOX_MSG_PTP_CFG;\n\tmbx.ptp.enable = enable;\n\n\treturn nicvf_send_msg_to_pf(nic, &mbx);\n}\n\nstatic int nicvf_update_hw_max_frs(struct nicvf *nic, int mtu)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.frs.msg = NIC_MBOX_MSG_SET_MAX_FRS;\n\tmbx.frs.max_frs = mtu;\n\tmbx.frs.vf_id = nic->vf_id;\n\n\treturn nicvf_send_msg_to_pf(nic, &mbx);\n}\n\nstatic void nicvf_link_status_check_task(struct work_struct *work_arg)\n{\n\tstruct nicvf *nic = container_of(work_arg,\n\t\t\t\t\t struct nicvf,\n\t\t\t\t\t link_change_work.work);\n\tunion nic_mbx mbx = {};\n\tmbx.msg.msg = NIC_MBOX_MSG_BGX_LINK_CHANGE;\n\tnicvf_send_msg_to_pf(nic, &mbx);\n\tqueue_delayed_work(nic->nicvf_rx_mode_wq,\n\t\t\t   &nic->link_change_work, 2 * HZ);\n}\n\nint nicvf_open(struct net_device *netdev)\n{\n\tint cpu, err, qidx;\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct queue_set *qs = nic->qs;\n\tstruct nicvf_cq_poll *cq_poll = NULL;\n\n\t \n\tif (nic->nicvf_rx_mode_wq)\n\t\tdrain_workqueue(nic->nicvf_rx_mode_wq);\n\n\tnetif_carrier_off(netdev);\n\n\terr = nicvf_register_misc_interrupt(nic);\n\tif (err)\n\t\treturn err;\n\n\t \n\tfor (qidx = 0; qidx < qs->cq_cnt; qidx++) {\n\t\tcq_poll = kzalloc(sizeof(*cq_poll), GFP_KERNEL);\n\t\tif (!cq_poll) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto napi_del;\n\t\t}\n\t\tcq_poll->cq_idx = qidx;\n\t\tcq_poll->nicvf = nic;\n\t\tnetif_napi_add(netdev, &cq_poll->napi, nicvf_poll);\n\t\tnapi_enable(&cq_poll->napi);\n\t\tnic->napi[qidx] = cq_poll;\n\t}\n\n\t \n\tif (!nic->sqs_mode && is_zero_ether_addr(netdev->dev_addr)) {\n\t\teth_hw_addr_random(netdev);\n\t\tnicvf_hw_set_mac_addr(nic, netdev);\n\t}\n\n\tif (nic->set_mac_pending) {\n\t\tnic->set_mac_pending = false;\n\t\tnicvf_hw_set_mac_addr(nic, netdev);\n\t}\n\n\t \n\ttasklet_setup(&nic->qs_err_task, nicvf_handle_qs_err);\n\n\t \n\ttasklet_setup(&nic->rbdr_task, nicvf_rbdr_task);\n\tINIT_DELAYED_WORK(&nic->rbdr_work, nicvf_rbdr_work);\n\n\t \n\tnic->cpi_alg = cpi_alg;\n\tif (!nic->sqs_mode)\n\t\tnicvf_config_cpi(nic);\n\n\tnicvf_request_sqs(nic);\n\tif (nic->sqs_mode)\n\t\tnicvf_get_primary_vf_struct(nic);\n\n\t \n\tif (nic->ptp_clock)\n\t\tnicvf_config_hw_rx_tstamp(nic, nic->hw_rx_tstamp);\n\tatomic_set(&nic->tx_ptp_skbs, 0);\n\tnic->ptp_skb = NULL;\n\n\t \n\tif (!nic->sqs_mode) {\n\t\tnicvf_rss_init(nic);\n\t\terr = nicvf_update_hw_max_frs(nic, netdev->mtu);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\t \n\t\tfor_each_possible_cpu(cpu)\n\t\t\tmemset(per_cpu_ptr(nic->drv_stats, cpu), 0,\n\t\t\t       sizeof(struct nicvf_drv_stats));\n\t}\n\n\terr = nicvf_register_interrupts(nic);\n\tif (err)\n\t\tgoto cleanup;\n\n\t \n\terr = nicvf_init_resources(nic);\n\tif (err)\n\t\tgoto cleanup;\n\n\t \n\twmb();\n\n\tnicvf_reg_write(nic, NIC_VF_INT, -1);\n\t \n\tnicvf_enable_intr(nic, NICVF_INTR_QS_ERR, 0);\n\n\t \n\tfor (qidx = 0; qidx < qs->cq_cnt; qidx++)\n\t\tnicvf_enable_intr(nic, NICVF_INTR_CQ, qidx);\n\n\t \n\tfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++)\n\t\tnicvf_enable_intr(nic, NICVF_INTR_RBDR, qidx);\n\n\t \n\tnicvf_send_cfg_done(nic);\n\n\tif (nic->nicvf_rx_mode_wq) {\n\t\tINIT_DELAYED_WORK(&nic->link_change_work,\n\t\t\t\t  nicvf_link_status_check_task);\n\t\tqueue_delayed_work(nic->nicvf_rx_mode_wq,\n\t\t\t\t   &nic->link_change_work, 0);\n\t}\n\n\treturn 0;\ncleanup:\n\tnicvf_disable_intr(nic, NICVF_INTR_MBOX, 0);\n\tnicvf_unregister_interrupts(nic);\n\ttasklet_kill(&nic->qs_err_task);\n\ttasklet_kill(&nic->rbdr_task);\nnapi_del:\n\tfor (qidx = 0; qidx < qs->cq_cnt; qidx++) {\n\t\tcq_poll = nic->napi[qidx];\n\t\tif (!cq_poll)\n\t\t\tcontinue;\n\t\tnapi_disable(&cq_poll->napi);\n\t\tnetif_napi_del(&cq_poll->napi);\n\t}\n\tnicvf_free_cq_poll(nic);\n\treturn err;\n}\n\nstatic int nicvf_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tint orig_mtu = netdev->mtu;\n\n\t \n\tif (nic->xdp_prog && new_mtu > MAX_XDP_MTU) {\n\t\tnetdev_warn(netdev, \"Jumbo frames not yet supported with XDP, current MTU %d.\\n\",\n\t\t\t    netdev->mtu);\n\t\treturn -EINVAL;\n\t}\n\n\tnetdev->mtu = new_mtu;\n\n\tif (!netif_running(netdev))\n\t\treturn 0;\n\n\tif (nicvf_update_hw_max_frs(nic, new_mtu)) {\n\t\tnetdev->mtu = orig_mtu;\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int nicvf_set_mac_address(struct net_device *netdev, void *p)\n{\n\tstruct sockaddr *addr = p;\n\tstruct nicvf *nic = netdev_priv(netdev);\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\teth_hw_addr_set(netdev, addr->sa_data);\n\n\tif (nic->pdev->msix_enabled) {\n\t\tif (nicvf_hw_set_mac_addr(nic, netdev))\n\t\t\treturn -EBUSY;\n\t} else {\n\t\tnic->set_mac_pending = true;\n\t}\n\n\treturn 0;\n}\n\nvoid nicvf_update_lmac_stats(struct nicvf *nic)\n{\n\tint stat = 0;\n\tunion nic_mbx mbx = {};\n\n\tif (!netif_running(nic->netdev))\n\t\treturn;\n\n\tmbx.bgx_stats.msg = NIC_MBOX_MSG_BGX_STATS;\n\tmbx.bgx_stats.vf_id = nic->vf_id;\n\t \n\tmbx.bgx_stats.rx = 1;\n\twhile (stat < BGX_RX_STATS_COUNT) {\n\t\tmbx.bgx_stats.idx = stat;\n\t\tif (nicvf_send_msg_to_pf(nic, &mbx))\n\t\t\treturn;\n\t\tstat++;\n\t}\n\n\tstat = 0;\n\n\t \n\tmbx.bgx_stats.rx = 0;\n\twhile (stat < BGX_TX_STATS_COUNT) {\n\t\tmbx.bgx_stats.idx = stat;\n\t\tif (nicvf_send_msg_to_pf(nic, &mbx))\n\t\t\treturn;\n\t\tstat++;\n\t}\n}\n\nvoid nicvf_update_stats(struct nicvf *nic)\n{\n\tint qidx, cpu;\n\tu64 tmp_stats = 0;\n\tstruct nicvf_hw_stats *stats = &nic->hw_stats;\n\tstruct nicvf_drv_stats *drv_stats;\n\tstruct queue_set *qs = nic->qs;\n\n#define GET_RX_STATS(reg) \\\n\tnicvf_reg_read(nic, NIC_VNIC_RX_STAT_0_13 | (reg << 3))\n#define GET_TX_STATS(reg) \\\n\tnicvf_reg_read(nic, NIC_VNIC_TX_STAT_0_4 | (reg << 3))\n\n\tstats->rx_bytes = GET_RX_STATS(RX_OCTS);\n\tstats->rx_ucast_frames = GET_RX_STATS(RX_UCAST);\n\tstats->rx_bcast_frames = GET_RX_STATS(RX_BCAST);\n\tstats->rx_mcast_frames = GET_RX_STATS(RX_MCAST);\n\tstats->rx_fcs_errors = GET_RX_STATS(RX_FCS);\n\tstats->rx_l2_errors = GET_RX_STATS(RX_L2ERR);\n\tstats->rx_drop_red = GET_RX_STATS(RX_RED);\n\tstats->rx_drop_red_bytes = GET_RX_STATS(RX_RED_OCTS);\n\tstats->rx_drop_overrun = GET_RX_STATS(RX_ORUN);\n\tstats->rx_drop_overrun_bytes = GET_RX_STATS(RX_ORUN_OCTS);\n\tstats->rx_drop_bcast = GET_RX_STATS(RX_DRP_BCAST);\n\tstats->rx_drop_mcast = GET_RX_STATS(RX_DRP_MCAST);\n\tstats->rx_drop_l3_bcast = GET_RX_STATS(RX_DRP_L3BCAST);\n\tstats->rx_drop_l3_mcast = GET_RX_STATS(RX_DRP_L3MCAST);\n\n\tstats->tx_bytes = GET_TX_STATS(TX_OCTS);\n\tstats->tx_ucast_frames = GET_TX_STATS(TX_UCAST);\n\tstats->tx_bcast_frames = GET_TX_STATS(TX_BCAST);\n\tstats->tx_mcast_frames = GET_TX_STATS(TX_MCAST);\n\tstats->tx_drops = GET_TX_STATS(TX_DROP);\n\n\t \n\tif (nic->t88 && nic->hw_tso) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tdrv_stats = per_cpu_ptr(nic->drv_stats, cpu);\n\t\t\ttmp_stats += drv_stats->tx_tso;\n\t\t}\n\t\tstats->tx_drops = tmp_stats - stats->tx_drops;\n\t}\n\tstats->tx_frames = stats->tx_ucast_frames +\n\t\t\t   stats->tx_bcast_frames +\n\t\t\t   stats->tx_mcast_frames;\n\tstats->rx_frames = stats->rx_ucast_frames +\n\t\t\t   stats->rx_bcast_frames +\n\t\t\t   stats->rx_mcast_frames;\n\tstats->rx_drops = stats->rx_drop_red +\n\t\t\t  stats->rx_drop_overrun;\n\n\t \n\tfor (qidx = 0; qidx < qs->rq_cnt; qidx++)\n\t\tnicvf_update_rq_stats(nic, qidx);\n\tfor (qidx = 0; qidx < qs->sq_cnt; qidx++)\n\t\tnicvf_update_sq_stats(nic, qidx);\n}\n\nstatic void nicvf_get_stats64(struct net_device *netdev,\n\t\t\t      struct rtnl_link_stats64 *stats)\n{\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct nicvf_hw_stats *hw_stats = &nic->hw_stats;\n\n\tnicvf_update_stats(nic);\n\n\tstats->rx_bytes = hw_stats->rx_bytes;\n\tstats->rx_packets = hw_stats->rx_frames;\n\tstats->rx_dropped = hw_stats->rx_drops;\n\tstats->multicast = hw_stats->rx_mcast_frames;\n\n\tstats->tx_bytes = hw_stats->tx_bytes;\n\tstats->tx_packets = hw_stats->tx_frames;\n\tstats->tx_dropped = hw_stats->tx_drops;\n\n}\n\nstatic void nicvf_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct nicvf *nic = netdev_priv(dev);\n\n\tnetif_warn(nic, tx_err, dev, \"Transmit timed out, resetting\\n\");\n\n\tthis_cpu_inc(nic->drv_stats->tx_timeout);\n\tschedule_work(&nic->reset_task);\n}\n\nstatic void nicvf_reset_task(struct work_struct *work)\n{\n\tstruct nicvf *nic;\n\n\tnic = container_of(work, struct nicvf, reset_task);\n\n\tif (!netif_running(nic->netdev))\n\t\treturn;\n\n\tnicvf_stop(nic->netdev);\n\tnicvf_open(nic->netdev);\n\tnetif_trans_update(nic->netdev);\n}\n\nstatic int nicvf_config_loopback(struct nicvf *nic,\n\t\t\t\t netdev_features_t features)\n{\n\tunion nic_mbx mbx = {};\n\n\tmbx.lbk.msg = NIC_MBOX_MSG_LOOPBACK;\n\tmbx.lbk.vf_id = nic->vf_id;\n\tmbx.lbk.enable = (features & NETIF_F_LOOPBACK) != 0;\n\n\treturn nicvf_send_msg_to_pf(nic, &mbx);\n}\n\nstatic netdev_features_t nicvf_fix_features(struct net_device *netdev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tstruct nicvf *nic = netdev_priv(netdev);\n\n\tif ((features & NETIF_F_LOOPBACK) &&\n\t    netif_running(netdev) && !nic->loopback_supported)\n\t\tfeatures &= ~NETIF_F_LOOPBACK;\n\n\treturn features;\n}\n\nstatic int nicvf_set_features(struct net_device *netdev,\n\t\t\t      netdev_features_t features)\n{\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tnetdev_features_t changed = features ^ netdev->features;\n\n\tif (changed & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tnicvf_config_vlan_stripping(nic, features);\n\n\tif ((changed & NETIF_F_LOOPBACK) && netif_running(netdev))\n\t\treturn nicvf_config_loopback(nic, features);\n\n\treturn 0;\n}\n\nstatic void nicvf_set_xdp_queues(struct nicvf *nic, bool bpf_attached)\n{\n\tu8 cq_count, txq_count;\n\n\t \n\tif (!bpf_attached)\n\t\tnic->xdp_tx_queues = 0;\n\telse\n\t\tnic->xdp_tx_queues = nic->rx_queues;\n\n\t \n\ttxq_count = nic->xdp_tx_queues + nic->tx_queues;\n\tcq_count = max(nic->rx_queues, txq_count);\n\tif (cq_count > MAX_CMP_QUEUES_PER_QS) {\n\t\tnic->sqs_count = roundup(cq_count, MAX_CMP_QUEUES_PER_QS);\n\t\tnic->sqs_count = (nic->sqs_count / MAX_CMP_QUEUES_PER_QS) - 1;\n\t} else {\n\t\tnic->sqs_count = 0;\n\t}\n\n\t \n\tnic->qs->rq_cnt = min_t(u8, nic->rx_queues, MAX_RCV_QUEUES_PER_QS);\n\tnic->qs->sq_cnt = min_t(u8, txq_count, MAX_SND_QUEUES_PER_QS);\n\tnic->qs->cq_cnt = max_t(u8, nic->qs->rq_cnt, nic->qs->sq_cnt);\n\n\t \n\tnicvf_set_real_num_queues(nic->netdev, nic->tx_queues, nic->rx_queues);\n}\n\nstatic int nicvf_xdp_setup(struct nicvf *nic, struct bpf_prog *prog)\n{\n\tstruct net_device *dev = nic->netdev;\n\tbool if_up = netif_running(nic->netdev);\n\tstruct bpf_prog *old_prog;\n\tbool bpf_attached = false;\n\tint ret = 0;\n\n\t \n\tif (prog && dev->mtu > MAX_XDP_MTU) {\n\t\tnetdev_warn(dev, \"Jumbo frames not yet supported with XDP, current MTU %d.\\n\",\n\t\t\t    dev->mtu);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif ((nic->rx_queues + nic->tx_queues) > nic->max_queues) {\n\t\tnetdev_warn(dev,\n\t\t\t    \"Failed to attach BPF prog, RXQs + TXQs > Max %d\\n\",\n\t\t\t    nic->max_queues);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (if_up)\n\t\tnicvf_stop(nic->netdev);\n\n\told_prog = xchg(&nic->xdp_prog, prog);\n\t \n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\tif (nic->xdp_prog) {\n\t\t \n\t\tbpf_prog_add(nic->xdp_prog, nic->rx_queues - 1);\n\t\tbpf_attached = true;\n\t}\n\n\t \n\tnicvf_set_xdp_queues(nic, bpf_attached);\n\n\tif (if_up) {\n\t\t \n\t\tnicvf_open(nic->netdev);\n\t\tnetif_trans_update(nic->netdev);\n\t}\n\n\treturn ret;\n}\n\nstatic int nicvf_xdp(struct net_device *netdev, struct netdev_bpf *xdp)\n{\n\tstruct nicvf *nic = netdev_priv(netdev);\n\n\t \n\tif (pass1_silicon(nic->pdev))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn nicvf_xdp_setup(nic, xdp->prog);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int nicvf_config_hwtstamp(struct net_device *netdev, struct ifreq *ifr)\n{\n\tstruct hwtstamp_config config;\n\tstruct nicvf *nic = netdev_priv(netdev);\n\n\tif (!nic->ptp_clock)\n\t\treturn -ENODEV;\n\n\tif (copy_from_user(&config, ifr->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\tswitch (config.tx_type) {\n\tcase HWTSTAMP_TX_OFF:\n\tcase HWTSTAMP_TX_ON:\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tswitch (config.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\tnic->hw_rx_tstamp = false;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_ALL:\n\tcase HWTSTAMP_FILTER_SOME:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\n\t\tnic->hw_rx_tstamp = true;\n\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tif (netif_running(netdev))\n\t\tnicvf_config_hw_rx_tstamp(nic, nic->hw_rx_tstamp);\n\n\tif (copy_to_user(ifr->ifr_data, &config, sizeof(config)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int nicvf_ioctl(struct net_device *netdev, struct ifreq *req, int cmd)\n{\n\tswitch (cmd) {\n\tcase SIOCSHWTSTAMP:\n\t\treturn nicvf_config_hwtstamp(netdev, req);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic void __nicvf_set_rx_mode_task(u8 mode, struct xcast_addr_list *mc_addrs,\n\t\t\t\t     struct nicvf *nic)\n{\n\tunion nic_mbx mbx = {};\n\tint idx;\n\n\t \n\n\t \n\tmbx.xcast.msg = NIC_MBOX_MSG_RESET_XCAST;\n\tif (nicvf_send_msg_to_pf(nic, &mbx) < 0)\n\t\tgoto free_mc;\n\n\tif (mode & BGX_XCAST_MCAST_FILTER) {\n\t\t \n\t\tmbx.xcast.msg = NIC_MBOX_MSG_ADD_MCAST;\n\t\tmbx.xcast.mac = 0;\n\t\tif (nicvf_send_msg_to_pf(nic, &mbx) < 0)\n\t\t\tgoto free_mc;\n\t}\n\n\t \n\tif (mc_addrs) {\n\t\t \n\t\tfor (idx = 0; idx < mc_addrs->count; idx++) {\n\t\t\tmbx.xcast.msg = NIC_MBOX_MSG_ADD_MCAST;\n\t\t\tmbx.xcast.mac = mc_addrs->mc[idx];\n\t\t\tif (nicvf_send_msg_to_pf(nic, &mbx) < 0)\n\t\t\t\tgoto free_mc;\n\t\t}\n\t}\n\n\t \n\tmbx.xcast.msg = NIC_MBOX_MSG_SET_XCAST;\n\tmbx.xcast.mode = mode;\n\n\tnicvf_send_msg_to_pf(nic, &mbx);\nfree_mc:\n\tkfree(mc_addrs);\n}\n\nstatic void nicvf_set_rx_mode_task(struct work_struct *work_arg)\n{\n\tstruct nicvf_work *vf_work = container_of(work_arg, struct nicvf_work,\n\t\t\t\t\t\t  work);\n\tstruct nicvf *nic = container_of(vf_work, struct nicvf, rx_mode_work);\n\tu8 mode;\n\tstruct xcast_addr_list *mc;\n\n\t \n\tspin_lock_bh(&nic->rx_mode_wq_lock);\n\tmode = vf_work->mode;\n\tmc = vf_work->mc;\n\tvf_work->mc = NULL;\n\tspin_unlock_bh(&nic->rx_mode_wq_lock);\n\n\t__nicvf_set_rx_mode_task(mode, mc, nic);\n}\n\nstatic void nicvf_set_rx_mode(struct net_device *netdev)\n{\n\tstruct nicvf *nic = netdev_priv(netdev);\n\tstruct netdev_hw_addr *ha;\n\tstruct xcast_addr_list *mc_list = NULL;\n\tu8 mode = 0;\n\n\tif (netdev->flags & IFF_PROMISC) {\n\t\tmode = BGX_XCAST_BCAST_ACCEPT | BGX_XCAST_MCAST_ACCEPT;\n\t} else {\n\t\tif (netdev->flags & IFF_BROADCAST)\n\t\t\tmode |= BGX_XCAST_BCAST_ACCEPT;\n\n\t\tif (netdev->flags & IFF_ALLMULTI) {\n\t\t\tmode |= BGX_XCAST_MCAST_ACCEPT;\n\t\t} else if (netdev->flags & IFF_MULTICAST) {\n\t\t\tmode |= BGX_XCAST_MCAST_FILTER;\n\t\t\t \n\t\t\tif (netdev_mc_count(netdev)) {\n\t\t\t\tmc_list = kmalloc(struct_size(mc_list, mc,\n\t\t\t\t\t\t\t      netdev_mc_count(netdev)),\n\t\t\t\t\t\t  GFP_ATOMIC);\n\t\t\t\tif (unlikely(!mc_list))\n\t\t\t\t\treturn;\n\t\t\t\tmc_list->count = 0;\n\t\t\t\tnetdev_hw_addr_list_for_each(ha, &netdev->mc) {\n\t\t\t\t\tmc_list->mc[mc_list->count] =\n\t\t\t\t\t\tether_addr_to_u64(ha->addr);\n\t\t\t\t\tmc_list->count++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tspin_lock(&nic->rx_mode_wq_lock);\n\tkfree(nic->rx_mode_work.mc);\n\tnic->rx_mode_work.mc = mc_list;\n\tnic->rx_mode_work.mode = mode;\n\tqueue_work(nic->nicvf_rx_mode_wq, &nic->rx_mode_work.work);\n\tspin_unlock(&nic->rx_mode_wq_lock);\n}\n\nstatic const struct net_device_ops nicvf_netdev_ops = {\n\t.ndo_open\t\t= nicvf_open,\n\t.ndo_stop\t\t= nicvf_stop,\n\t.ndo_start_xmit\t\t= nicvf_xmit,\n\t.ndo_change_mtu\t\t= nicvf_change_mtu,\n\t.ndo_set_mac_address\t= nicvf_set_mac_address,\n\t.ndo_get_stats64\t= nicvf_get_stats64,\n\t.ndo_tx_timeout         = nicvf_tx_timeout,\n\t.ndo_fix_features       = nicvf_fix_features,\n\t.ndo_set_features       = nicvf_set_features,\n\t.ndo_bpf\t\t= nicvf_xdp,\n\t.ndo_eth_ioctl           = nicvf_ioctl,\n\t.ndo_set_rx_mode        = nicvf_set_rx_mode,\n};\n\nstatic int nicvf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct net_device *netdev;\n\tstruct nicvf *nic;\n\tint    err, qcount;\n\tu16    sdevid;\n\tstruct cavium_ptp *ptp_clock;\n\n\tptp_clock = cavium_ptp_get();\n\tif (IS_ERR(ptp_clock)) {\n\t\tif (PTR_ERR(ptp_clock) == -ENODEV)\n\t\t\t \n\t\t\tptp_clock = NULL;\n\t\telse\n\t\t\treturn PTR_ERR(ptp_clock);\n\t}\n\n\terr = pci_enable_device(pdev);\n\tif (err)\n\t\treturn dev_err_probe(dev, err, \"Failed to enable PCI device\\n\");\n\n\terr = pci_request_regions(pdev, DRV_NAME);\n\tif (err) {\n\t\tdev_err(dev, \"PCI request regions failed 0x%x\\n\", err);\n\t\tgoto err_disable_device;\n\t}\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(48));\n\tif (err) {\n\t\tdev_err(dev, \"Unable to get usable DMA configuration\\n\");\n\t\tgoto err_release_regions;\n\t}\n\n\tqcount = netif_get_num_default_rss_queues();\n\n\t \n\tif (pdev->is_virtfn) {\n\t\t \n\t\tqcount = min_t(int, num_online_cpus(),\n\t\t\t       (MAX_SQS_PER_VF + 1) * MAX_CMP_QUEUES_PER_QS);\n\t}\n\n\tnetdev = alloc_etherdev_mqs(sizeof(struct nicvf), qcount, qcount);\n\tif (!netdev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_release_regions;\n\t}\n\n\tpci_set_drvdata(pdev, netdev);\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\n\tnic = netdev_priv(netdev);\n\tnic->netdev = netdev;\n\tnic->pdev = pdev;\n\tnic->pnicvf = nic;\n\tnic->max_queues = qcount;\n\t \n\tif (!nic->t88)\n\t\tnic->max_queues *= 2;\n\tnic->ptp_clock = ptp_clock;\n\n\t \n\tmutex_init(&nic->rx_mode_mtx);\n\n\t \n\tnic->reg_base = pcim_iomap(pdev, PCI_CFG_REG_BAR_NUM, 0);\n\tif (!nic->reg_base) {\n\t\tdev_err(dev, \"Cannot map config register space, aborting\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_free_netdev;\n\t}\n\n\tnic->drv_stats = netdev_alloc_pcpu_stats(struct nicvf_drv_stats);\n\tif (!nic->drv_stats) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_netdev;\n\t}\n\n\terr = nicvf_set_qset_resources(nic);\n\tif (err)\n\t\tgoto err_free_netdev;\n\n\t \n\terr = nicvf_register_misc_interrupt(nic);\n\tif (err)\n\t\tgoto err_free_netdev;\n\n\tnicvf_send_vf_struct(nic);\n\n\tif (!pass1_silicon(nic->pdev))\n\t\tnic->hw_tso = true;\n\n\t \n\tnic->iommu_domain = iommu_get_domain_for_dev(dev);\n\n\tpci_read_config_word(nic->pdev, PCI_SUBSYSTEM_ID, &sdevid);\n\tif (sdevid == 0xA134)\n\t\tnic->t88 = true;\n\n\t \n\tif (nic->sqs_mode)\n\t\treturn 0;\n\n\terr = nicvf_set_real_num_queues(netdev, nic->tx_queues, nic->rx_queues);\n\tif (err)\n\t\tgoto err_unregister_interrupts;\n\n\tnetdev->hw_features = (NETIF_F_RXCSUM | NETIF_F_SG |\n\t\t\t       NETIF_F_TSO | NETIF_F_GRO | NETIF_F_TSO6 |\n\t\t\t       NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\t       NETIF_F_HW_VLAN_CTAG_RX);\n\n\tnetdev->hw_features |= NETIF_F_RXHASH;\n\n\tnetdev->features |= netdev->hw_features;\n\tnetdev->hw_features |= NETIF_F_LOOPBACK;\n\n\tnetdev->vlan_features = NETIF_F_SG | NETIF_F_IP_CSUM |\n\t\t\t\tNETIF_F_IPV6_CSUM | NETIF_F_TSO | NETIF_F_TSO6;\n\n\tnetdev->netdev_ops = &nicvf_netdev_ops;\n\tnetdev->watchdog_timeo = NICVF_TX_TIMEOUT;\n\n\tif (!pass1_silicon(nic->pdev) &&\n\t    nic->rx_queues + nic->tx_queues <= nic->max_queues)\n\t\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC;\n\n\t \n\tnetdev->min_mtu = NIC_HW_MIN_FRS;\n\tnetdev->max_mtu = NIC_HW_MAX_FRS;\n\n\tINIT_WORK(&nic->reset_task, nicvf_reset_task);\n\n\tnic->nicvf_rx_mode_wq = alloc_ordered_workqueue(\"nicvf_rx_mode_wq_VF%d\",\n\t\t\t\t\t\t\tWQ_MEM_RECLAIM,\n\t\t\t\t\t\t\tnic->vf_id);\n\tif (!nic->nicvf_rx_mode_wq) {\n\t\terr = -ENOMEM;\n\t\tdev_err(dev, \"Failed to allocate work queue\\n\");\n\t\tgoto err_unregister_interrupts;\n\t}\n\n\tINIT_WORK(&nic->rx_mode_work.work, nicvf_set_rx_mode_task);\n\tspin_lock_init(&nic->rx_mode_wq_lock);\n\n\terr = register_netdev(netdev);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to register netdevice\\n\");\n\t\tgoto err_destroy_workqueue;\n\t}\n\n\tnic->msg_enable = debug;\n\n\tnicvf_set_ethtool_ops(netdev);\n\n\treturn 0;\n\nerr_destroy_workqueue:\n\tdestroy_workqueue(nic->nicvf_rx_mode_wq);\nerr_unregister_interrupts:\n\tnicvf_unregister_interrupts(nic);\nerr_free_netdev:\n\tpci_set_drvdata(pdev, NULL);\n\tif (nic->drv_stats)\n\t\tfree_percpu(nic->drv_stats);\n\tfree_netdev(netdev);\nerr_release_regions:\n\tpci_release_regions(pdev);\nerr_disable_device:\n\tpci_disable_device(pdev);\n\treturn err;\n}\n\nstatic void nicvf_remove(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct nicvf *nic;\n\tstruct net_device *pnetdev;\n\n\tif (!netdev)\n\t\treturn;\n\n\tnic = netdev_priv(netdev);\n\tpnetdev = nic->pnicvf->netdev;\n\n\t \n\tif (pnetdev && (pnetdev->reg_state == NETREG_REGISTERED))\n\t\tunregister_netdev(pnetdev);\n\tif (nic->nicvf_rx_mode_wq) {\n\t\tdestroy_workqueue(nic->nicvf_rx_mode_wq);\n\t\tnic->nicvf_rx_mode_wq = NULL;\n\t}\n\tnicvf_unregister_interrupts(nic);\n\tpci_set_drvdata(pdev, NULL);\n\tif (nic->drv_stats)\n\t\tfree_percpu(nic->drv_stats);\n\tcavium_ptp_put(nic->ptp_clock);\n\tfree_netdev(netdev);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n}\n\nstatic void nicvf_shutdown(struct pci_dev *pdev)\n{\n\tnicvf_remove(pdev);\n}\n\nstatic struct pci_driver nicvf_driver = {\n\t.name = DRV_NAME,\n\t.id_table = nicvf_id_table,\n\t.probe = nicvf_probe,\n\t.remove = nicvf_remove,\n\t.shutdown = nicvf_shutdown,\n};\n\nstatic int __init nicvf_init_module(void)\n{\n\tpr_info(\"%s, ver %s\\n\", DRV_NAME, DRV_VERSION);\n\treturn pci_register_driver(&nicvf_driver);\n}\n\nstatic void __exit nicvf_cleanup_module(void)\n{\n\tpci_unregister_driver(&nicvf_driver);\n}\n\nmodule_init(nicvf_init_module);\nmodule_exit(nicvf_cleanup_module);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}