{
  "module_name": "octeon_mgmt.c",
  "hash_id": "390a9eddc54425b43d467efd75c86c9180ede952e9b30abeb65d11c34a484a1a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/cavium/octeon/octeon_mgmt.c",
  "human_readable_source": " \n\n#include <linux/platform_device.h>\n#include <linux/dma-mapping.h>\n#include <linux/etherdevice.h>\n#include <linux/capability.h>\n#include <linux/net_tstamp.h>\n#include <linux/interrupt.h>\n#include <linux/netdevice.h>\n#include <linux/spinlock.h>\n#include <linux/if_vlan.h>\n#include <linux/of_mdio.h>\n#include <linux/module.h>\n#include <linux/of_net.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/phy.h>\n#include <linux/io.h>\n\n#include <asm/octeon/octeon.h>\n#include <asm/octeon/cvmx-mixx-defs.h>\n#include <asm/octeon/cvmx-agl-defs.h>\n\n#define DRV_NAME \"octeon_mgmt\"\n#define DRV_DESCRIPTION \\\n\t\"Cavium Networks Octeon MII (management) port Network Driver\"\n\n#define OCTEON_MGMT_NAPI_WEIGHT 16\n\n \n#define OCTEON_MGMT_RX_RING_SIZE 512\n#define OCTEON_MGMT_TX_RING_SIZE 128\n\n \n#define OCTEON_MGMT_RX_HEADROOM (ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN)\n\nunion mgmt_port_ring_entry {\n\tu64 d64;\n\tstruct {\n#define RING_ENTRY_CODE_DONE 0xf\n#define RING_ENTRY_CODE_MORE 0x10\n#ifdef __BIG_ENDIAN_BITFIELD\n\t\tu64 reserved_62_63:2;\n\t\t \n\t\tu64 len:14;\n\t\t \n\t\tu64 tstamp:1;\n\t\t \n\t\tu64 code:7;\n\t\t \n\t\tu64 addr:40;\n#else\n\t\tu64 addr:40;\n\t\tu64 code:7;\n\t\tu64 tstamp:1;\n\t\tu64 len:14;\n\t\tu64 reserved_62_63:2;\n#endif\n\t} s;\n};\n\n#define MIX_ORING1\t0x0\n#define MIX_ORING2\t0x8\n#define MIX_IRING1\t0x10\n#define MIX_IRING2\t0x18\n#define MIX_CTL\t\t0x20\n#define MIX_IRHWM\t0x28\n#define MIX_IRCNT\t0x30\n#define MIX_ORHWM\t0x38\n#define MIX_ORCNT\t0x40\n#define MIX_ISR\t\t0x48\n#define MIX_INTENA\t0x50\n#define MIX_REMCNT\t0x58\n#define MIX_BIST\t0x78\n\n#define AGL_GMX_PRT_CFG\t\t\t0x10\n#define AGL_GMX_RX_FRM_CTL\t\t0x18\n#define AGL_GMX_RX_FRM_MAX\t\t0x30\n#define AGL_GMX_RX_JABBER\t\t0x38\n#define AGL_GMX_RX_STATS_CTL\t\t0x50\n\n#define AGL_GMX_RX_STATS_PKTS_DRP\t0xb0\n#define AGL_GMX_RX_STATS_OCTS_DRP\t0xb8\n#define AGL_GMX_RX_STATS_PKTS_BAD\t0xc0\n\n#define AGL_GMX_RX_ADR_CTL\t\t0x100\n#define AGL_GMX_RX_ADR_CAM_EN\t\t0x108\n#define AGL_GMX_RX_ADR_CAM0\t\t0x180\n#define AGL_GMX_RX_ADR_CAM1\t\t0x188\n#define AGL_GMX_RX_ADR_CAM2\t\t0x190\n#define AGL_GMX_RX_ADR_CAM3\t\t0x198\n#define AGL_GMX_RX_ADR_CAM4\t\t0x1a0\n#define AGL_GMX_RX_ADR_CAM5\t\t0x1a8\n\n#define AGL_GMX_TX_CLK\t\t\t0x208\n#define AGL_GMX_TX_STATS_CTL\t\t0x268\n#define AGL_GMX_TX_CTL\t\t\t0x270\n#define AGL_GMX_TX_STAT0\t\t0x280\n#define AGL_GMX_TX_STAT1\t\t0x288\n#define AGL_GMX_TX_STAT2\t\t0x290\n#define AGL_GMX_TX_STAT3\t\t0x298\n#define AGL_GMX_TX_STAT4\t\t0x2a0\n#define AGL_GMX_TX_STAT5\t\t0x2a8\n#define AGL_GMX_TX_STAT6\t\t0x2b0\n#define AGL_GMX_TX_STAT7\t\t0x2b8\n#define AGL_GMX_TX_STAT8\t\t0x2c0\n#define AGL_GMX_TX_STAT9\t\t0x2c8\n\nstruct octeon_mgmt {\n\tstruct net_device *netdev;\n\tu64 mix;\n\tu64 agl;\n\tu64 agl_prt_ctl;\n\tint port;\n\tint irq;\n\tbool has_rx_tstamp;\n\tu64 *tx_ring;\n\tdma_addr_t tx_ring_handle;\n\tunsigned int tx_next;\n\tunsigned int tx_next_clean;\n\tunsigned int tx_current_fill;\n\t \n\tstruct sk_buff_head tx_list;\n\n\t \n\tu64 *rx_ring;\n\tdma_addr_t rx_ring_handle;\n\tunsigned int rx_next;\n\tunsigned int rx_next_fill;\n\tunsigned int rx_current_fill;\n\tstruct sk_buff_head rx_list;\n\n\tspinlock_t lock;\n\tunsigned int last_duplex;\n\tunsigned int last_link;\n\tunsigned int last_speed;\n\tstruct device *dev;\n\tstruct napi_struct napi;\n\tstruct tasklet_struct tx_clean_tasklet;\n\tstruct device_node *phy_np;\n\tresource_size_t mix_phys;\n\tresource_size_t mix_size;\n\tresource_size_t agl_phys;\n\tresource_size_t agl_size;\n\tresource_size_t agl_prt_ctl_phys;\n\tresource_size_t agl_prt_ctl_size;\n};\n\nstatic void octeon_mgmt_set_rx_irq(struct octeon_mgmt *p, int enable)\n{\n\tunion cvmx_mixx_intena mix_intena;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&p->lock, flags);\n\tmix_intena.u64 = cvmx_read_csr(p->mix + MIX_INTENA);\n\tmix_intena.s.ithena = enable ? 1 : 0;\n\tcvmx_write_csr(p->mix + MIX_INTENA, mix_intena.u64);\n\tspin_unlock_irqrestore(&p->lock, flags);\n}\n\nstatic void octeon_mgmt_set_tx_irq(struct octeon_mgmt *p, int enable)\n{\n\tunion cvmx_mixx_intena mix_intena;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&p->lock, flags);\n\tmix_intena.u64 = cvmx_read_csr(p->mix + MIX_INTENA);\n\tmix_intena.s.othena = enable ? 1 : 0;\n\tcvmx_write_csr(p->mix + MIX_INTENA, mix_intena.u64);\n\tspin_unlock_irqrestore(&p->lock, flags);\n}\n\nstatic void octeon_mgmt_enable_rx_irq(struct octeon_mgmt *p)\n{\n\tocteon_mgmt_set_rx_irq(p, 1);\n}\n\nstatic void octeon_mgmt_disable_rx_irq(struct octeon_mgmt *p)\n{\n\tocteon_mgmt_set_rx_irq(p, 0);\n}\n\nstatic void octeon_mgmt_enable_tx_irq(struct octeon_mgmt *p)\n{\n\tocteon_mgmt_set_tx_irq(p, 1);\n}\n\nstatic void octeon_mgmt_disable_tx_irq(struct octeon_mgmt *p)\n{\n\tocteon_mgmt_set_tx_irq(p, 0);\n}\n\nstatic unsigned int ring_max_fill(unsigned int ring_size)\n{\n\treturn ring_size - 8;\n}\n\nstatic unsigned int ring_size_to_bytes(unsigned int ring_size)\n{\n\treturn ring_size * sizeof(union mgmt_port_ring_entry);\n}\n\nstatic void octeon_mgmt_rx_fill_ring(struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\n\twhile (p->rx_current_fill < ring_max_fill(OCTEON_MGMT_RX_RING_SIZE)) {\n\t\tunsigned int size;\n\t\tunion mgmt_port_ring_entry re;\n\t\tstruct sk_buff *skb;\n\n\t\t \n\t\tsize = netdev->mtu + OCTEON_MGMT_RX_HEADROOM + 8 + NET_IP_ALIGN;\n\n\t\tskb = netdev_alloc_skb(netdev, size);\n\t\tif (!skb)\n\t\t\tbreak;\n\t\tskb_reserve(skb, NET_IP_ALIGN);\n\t\t__skb_queue_tail(&p->rx_list, skb);\n\n\t\tre.d64 = 0;\n\t\tre.s.len = size;\n\t\tre.s.addr = dma_map_single(p->dev, skb->data,\n\t\t\t\t\t   size,\n\t\t\t\t\t   DMA_FROM_DEVICE);\n\n\t\t \n\t\tp->rx_ring[p->rx_next_fill] = re.d64;\n\t\t \n\t\twmb();\n\n\t\tdma_sync_single_for_device(p->dev, p->rx_ring_handle,\n\t\t\t\t\t   ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\n\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tp->rx_next_fill =\n\t\t\t(p->rx_next_fill + 1) % OCTEON_MGMT_RX_RING_SIZE;\n\t\tp->rx_current_fill++;\n\t\t \n\t\tcvmx_write_csr(p->mix + MIX_IRING2, 1);\n\t}\n}\n\nstatic void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)\n{\n\tunion cvmx_mixx_orcnt mix_orcnt;\n\tunion mgmt_port_ring_entry re;\n\tstruct sk_buff *skb;\n\tint cleaned = 0;\n\tunsigned long flags;\n\n\tmix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);\n\twhile (mix_orcnt.s.orcnt) {\n\t\tspin_lock_irqsave(&p->tx_list.lock, flags);\n\n\t\tmix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);\n\n\t\tif (mix_orcnt.s.orcnt == 0) {\n\t\t\tspin_unlock_irqrestore(&p->tx_list.lock, flags);\n\t\t\tbreak;\n\t\t}\n\n\t\tdma_sync_single_for_cpu(p->dev, p->tx_ring_handle,\n\t\t\t\t\tring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\n\t\t\t\t\tDMA_BIDIRECTIONAL);\n\n\t\tre.d64 = p->tx_ring[p->tx_next_clean];\n\t\tp->tx_next_clean =\n\t\t\t(p->tx_next_clean + 1) % OCTEON_MGMT_TX_RING_SIZE;\n\t\tskb = __skb_dequeue(&p->tx_list);\n\n\t\tmix_orcnt.u64 = 0;\n\t\tmix_orcnt.s.orcnt = 1;\n\n\t\t \n\t\tcvmx_write_csr(p->mix + MIX_ORCNT, mix_orcnt.u64);\n\t\tp->tx_current_fill--;\n\n\t\tspin_unlock_irqrestore(&p->tx_list.lock, flags);\n\n\t\tdma_unmap_single(p->dev, re.s.addr, re.s.len,\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\tif (unlikely(re.s.tstamp)) {\n\t\t\tstruct skb_shared_hwtstamps ts;\n\t\t\tu64 ns;\n\n\t\t\tmemset(&ts, 0, sizeof(ts));\n\t\t\t \n\t\t\tns = cvmx_read_csr(CVMX_MIXX_TSTAMP(p->port));\n\t\t\t \n\t\t\tcvmx_write_csr(CVMX_MIXX_TSCTL(p->port), 0);\n\t\t\t \n\t\t\tts.hwtstamp = ns_to_ktime(ns);\n\t\t\tskb_tstamp_tx(skb, &ts);\n\t\t}\n\n\t\tdev_kfree_skb_any(skb);\n\t\tcleaned++;\n\n\t\tmix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);\n\t}\n\n\tif (cleaned && netif_queue_stopped(p->netdev))\n\t\tnetif_wake_queue(p->netdev);\n}\n\nstatic void octeon_mgmt_clean_tx_tasklet(struct tasklet_struct *t)\n{\n\tstruct octeon_mgmt *p = from_tasklet(p, t, tx_clean_tasklet);\n\tocteon_mgmt_clean_tx_buffers(p);\n\tocteon_mgmt_enable_tx_irq(p);\n}\n\nstatic void octeon_mgmt_update_rx_stats(struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tunsigned long flags;\n\tu64 drop, bad;\n\n\t \n\tdrop = cvmx_read_csr(p->agl + AGL_GMX_RX_STATS_PKTS_DRP);\n\tbad = cvmx_read_csr(p->agl + AGL_GMX_RX_STATS_PKTS_BAD);\n\n\tif (drop || bad) {\n\t\t \n\t\tspin_lock_irqsave(&p->lock, flags);\n\t\tnetdev->stats.rx_errors += bad;\n\t\tnetdev->stats.rx_dropped += drop;\n\t\tspin_unlock_irqrestore(&p->lock, flags);\n\t}\n}\n\nstatic void octeon_mgmt_update_tx_stats(struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tunsigned long flags;\n\n\tunion cvmx_agl_gmx_txx_stat0 s0;\n\tunion cvmx_agl_gmx_txx_stat1 s1;\n\n\t \n\ts0.u64 = cvmx_read_csr(p->agl + AGL_GMX_TX_STAT0);\n\ts1.u64 = cvmx_read_csr(p->agl + AGL_GMX_TX_STAT1);\n\n\tif (s0.s.xsdef || s0.s.xscol || s1.s.scol || s1.s.mcol) {\n\t\t \n\t\tspin_lock_irqsave(&p->lock, flags);\n\t\tnetdev->stats.tx_errors += s0.s.xsdef + s0.s.xscol;\n\t\tnetdev->stats.collisions += s1.s.scol + s1.s.mcol;\n\t\tspin_unlock_irqrestore(&p->lock, flags);\n\t}\n}\n\n \nstatic u64 octeon_mgmt_dequeue_rx_buffer(struct octeon_mgmt *p,\n\t\t\t\t\t struct sk_buff **pskb)\n{\n\tunion mgmt_port_ring_entry re;\n\n\tdma_sync_single_for_cpu(p->dev, p->rx_ring_handle,\n\t\t\t\tring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\n\t\t\t\tDMA_BIDIRECTIONAL);\n\n\tre.d64 = p->rx_ring[p->rx_next];\n\tp->rx_next = (p->rx_next + 1) % OCTEON_MGMT_RX_RING_SIZE;\n\tp->rx_current_fill--;\n\t*pskb = __skb_dequeue(&p->rx_list);\n\n\tdma_unmap_single(p->dev, re.s.addr,\n\t\t\t ETH_FRAME_LEN + OCTEON_MGMT_RX_HEADROOM,\n\t\t\t DMA_FROM_DEVICE);\n\n\treturn re.d64;\n}\n\n\nstatic int octeon_mgmt_receive_one(struct octeon_mgmt *p)\n{\n\tstruct net_device *netdev = p->netdev;\n\tunion cvmx_mixx_ircnt mix_ircnt;\n\tunion mgmt_port_ring_entry re;\n\tstruct sk_buff *skb;\n\tstruct sk_buff *skb2;\n\tstruct sk_buff *skb_new;\n\tunion mgmt_port_ring_entry re2;\n\tint rc = 1;\n\n\n\tre.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb);\n\tif (likely(re.s.code == RING_ENTRY_CODE_DONE)) {\n\t\t \n\t\tskb_put(skb, re.s.len);\ngood:\n\t\t \n\t\tif (p->has_rx_tstamp) {\n\t\t\t \n\t\t\tu64 ns = *(u64 *)skb->data;\n\t\t\tstruct skb_shared_hwtstamps *ts;\n\t\t\tts = skb_hwtstamps(skb);\n\t\t\tts->hwtstamp = ns_to_ktime(ns);\n\t\t\t__skb_pull(skb, 8);\n\t\t}\n\t\tskb->protocol = eth_type_trans(skb, netdev);\n\t\tnetdev->stats.rx_packets++;\n\t\tnetdev->stats.rx_bytes += skb->len;\n\t\tnetif_receive_skb(skb);\n\t\trc = 0;\n\t} else if (re.s.code == RING_ENTRY_CODE_MORE) {\n\t\t \n\t\tskb_put(skb, re.s.len);\n\t\tdo {\n\t\t\tre2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);\n\t\t\tif (re2.s.code != RING_ENTRY_CODE_MORE\n\t\t\t\t&& re2.s.code != RING_ENTRY_CODE_DONE)\n\t\t\t\tgoto split_error;\n\t\t\tskb_put(skb2,  re2.s.len);\n\t\t\tskb_new = skb_copy_expand(skb, 0, skb2->len,\n\t\t\t\t\t\t  GFP_ATOMIC);\n\t\t\tif (!skb_new)\n\t\t\t\tgoto split_error;\n\t\t\tif (skb_copy_bits(skb2, 0, skb_tail_pointer(skb_new),\n\t\t\t\t\t  skb2->len))\n\t\t\t\tgoto split_error;\n\t\t\tskb_put(skb_new, skb2->len);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tdev_kfree_skb_any(skb2);\n\t\t\tskb = skb_new;\n\t\t} while (re2.s.code == RING_ENTRY_CODE_MORE);\n\t\tgoto good;\n\t} else {\n\t\t \n\t\tdev_kfree_skb_any(skb);\n\t\t \n\t}\n\tgoto done;\nsplit_error:\n\t \n\tdev_kfree_skb_any(skb);\n\tdev_kfree_skb_any(skb2);\n\twhile (re2.s.code == RING_ENTRY_CODE_MORE) {\n\t\tre2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);\n\t\tdev_kfree_skb_any(skb2);\n\t}\n\tnetdev->stats.rx_errors++;\n\ndone:\n\t \n\tmix_ircnt.u64 = 0;\n\tmix_ircnt.s.ircnt = 1;\n\tcvmx_write_csr(p->mix + MIX_IRCNT, mix_ircnt.u64);\n\treturn rc;\n}\n\nstatic int octeon_mgmt_receive_packets(struct octeon_mgmt *p, int budget)\n{\n\tunsigned int work_done = 0;\n\tunion cvmx_mixx_ircnt mix_ircnt;\n\tint rc;\n\n\tmix_ircnt.u64 = cvmx_read_csr(p->mix + MIX_IRCNT);\n\twhile (work_done < budget && mix_ircnt.s.ircnt) {\n\n\t\trc = octeon_mgmt_receive_one(p);\n\t\tif (!rc)\n\t\t\twork_done++;\n\n\t\t \n\t\tmix_ircnt.u64 = cvmx_read_csr(p->mix + MIX_IRCNT);\n\t}\n\n\tocteon_mgmt_rx_fill_ring(p->netdev);\n\n\treturn work_done;\n}\n\nstatic int octeon_mgmt_napi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct octeon_mgmt *p = container_of(napi, struct octeon_mgmt, napi);\n\tstruct net_device *netdev = p->netdev;\n\tunsigned int work_done = 0;\n\n\twork_done = octeon_mgmt_receive_packets(p, budget);\n\n\tif (work_done < budget) {\n\t\t \n\t\tnapi_complete_done(napi, work_done);\n\t\tocteon_mgmt_enable_rx_irq(p);\n\t}\n\tocteon_mgmt_update_rx_stats(netdev);\n\n\treturn work_done;\n}\n\n \nstatic void octeon_mgmt_reset_hw(struct octeon_mgmt *p)\n{\n\tunion cvmx_mixx_ctl mix_ctl;\n\tunion cvmx_mixx_bist mix_bist;\n\tunion cvmx_agl_gmx_bist agl_gmx_bist;\n\n\tmix_ctl.u64 = 0;\n\tcvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);\n\tdo {\n\t\tmix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);\n\t} while (mix_ctl.s.busy);\n\tmix_ctl.s.reset = 1;\n\tcvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);\n\tcvmx_read_csr(p->mix + MIX_CTL);\n\tocteon_io_clk_delay(64);\n\n\tmix_bist.u64 = cvmx_read_csr(p->mix + MIX_BIST);\n\tif (mix_bist.u64)\n\t\tdev_warn(p->dev, \"MIX failed BIST (0x%016llx)\\n\",\n\t\t\t(unsigned long long)mix_bist.u64);\n\n\tagl_gmx_bist.u64 = cvmx_read_csr(CVMX_AGL_GMX_BIST);\n\tif (agl_gmx_bist.u64)\n\t\tdev_warn(p->dev, \"AGL failed BIST (0x%016llx)\\n\",\n\t\t\t (unsigned long long)agl_gmx_bist.u64);\n}\n\nstruct octeon_mgmt_cam_state {\n\tu64 cam[6];\n\tu64 cam_mask;\n\tint cam_index;\n};\n\nstatic void octeon_mgmt_cam_state_add(struct octeon_mgmt_cam_state *cs,\n\t\t\t\t      const unsigned char *addr)\n{\n\tint i;\n\n\tfor (i = 0; i < 6; i++)\n\t\tcs->cam[i] |= (u64)addr[i] << (8 * (cs->cam_index));\n\tcs->cam_mask |= (1ULL << cs->cam_index);\n\tcs->cam_index++;\n}\n\nstatic void octeon_mgmt_set_rx_filtering(struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tunion cvmx_agl_gmx_rxx_adr_ctl adr_ctl;\n\tunion cvmx_agl_gmx_prtx_cfg agl_gmx_prtx;\n\tunsigned long flags;\n\tunsigned int prev_packet_enable;\n\tunsigned int cam_mode = 1;  \n\tunsigned int multicast_mode = 1;  \n\tstruct octeon_mgmt_cam_state cam_state;\n\tstruct netdev_hw_addr *ha;\n\tint available_cam_entries;\n\n\tmemset(&cam_state, 0, sizeof(cam_state));\n\n\tif ((netdev->flags & IFF_PROMISC) || netdev->uc.count > 7) {\n\t\tcam_mode = 0;\n\t\tavailable_cam_entries = 8;\n\t} else {\n\t\t \n\t\tavailable_cam_entries = 7 - netdev->uc.count;\n\t}\n\n\tif (netdev->flags & IFF_MULTICAST) {\n\t\tif (cam_mode == 0 || (netdev->flags & IFF_ALLMULTI) ||\n\t\t    netdev_mc_count(netdev) > available_cam_entries)\n\t\t\tmulticast_mode = 2;  \n\t\telse\n\t\t\tmulticast_mode = 0;  \n\t}\n\n\tif (cam_mode == 1) {\n\t\t \n\t\tocteon_mgmt_cam_state_add(&cam_state, netdev->dev_addr);\n\t\tnetdev_for_each_uc_addr(ha, netdev)\n\t\t\tocteon_mgmt_cam_state_add(&cam_state, ha->addr);\n\t}\n\tif (multicast_mode == 0) {\n\t\tnetdev_for_each_mc_addr(ha, netdev)\n\t\t\tocteon_mgmt_cam_state_add(&cam_state, ha->addr);\n\t}\n\n\tspin_lock_irqsave(&p->lock, flags);\n\n\t \n\tagl_gmx_prtx.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\n\tprev_packet_enable = agl_gmx_prtx.s.en;\n\tagl_gmx_prtx.s.en = 0;\n\tcvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, agl_gmx_prtx.u64);\n\n\tadr_ctl.u64 = 0;\n\tadr_ctl.s.cam_mode = cam_mode;\n\tadr_ctl.s.mcst = multicast_mode;\n\tadr_ctl.s.bcst = 1;      \n\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CTL, adr_ctl.u64);\n\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM0, cam_state.cam[0]);\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM1, cam_state.cam[1]);\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM2, cam_state.cam[2]);\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM3, cam_state.cam[3]);\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM4, cam_state.cam[4]);\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM5, cam_state.cam[5]);\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM_EN, cam_state.cam_mask);\n\n\t \n\tagl_gmx_prtx.s.en = prev_packet_enable;\n\tcvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, agl_gmx_prtx.u64);\n\n\tspin_unlock_irqrestore(&p->lock, flags);\n}\n\nstatic int octeon_mgmt_set_mac_address(struct net_device *netdev, void *addr)\n{\n\tint r = eth_mac_addr(netdev, addr);\n\n\tif (r)\n\t\treturn r;\n\n\tocteon_mgmt_set_rx_filtering(netdev);\n\n\treturn 0;\n}\n\nstatic int octeon_mgmt_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tint max_packet = new_mtu + ETH_HLEN + ETH_FCS_LEN;\n\n\tnetdev->mtu = new_mtu;\n\n\t \n\tcvmx_write_csr(p->agl + AGL_GMX_RX_FRM_MAX, max_packet);\n\t \n\tcvmx_write_csr(p->agl + AGL_GMX_RX_JABBER,\n\t\t       (max_packet + 7 + VLAN_HLEN * 2) & 0xfff8);\n\n\treturn 0;\n}\n\nstatic irqreturn_t octeon_mgmt_interrupt(int cpl, void *dev_id)\n{\n\tstruct net_device *netdev = dev_id;\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tunion cvmx_mixx_isr mixx_isr;\n\n\tmixx_isr.u64 = cvmx_read_csr(p->mix + MIX_ISR);\n\n\t \n\tcvmx_write_csr(p->mix + MIX_ISR, mixx_isr.u64);\n\tcvmx_read_csr(p->mix + MIX_ISR);\n\n\tif (mixx_isr.s.irthresh) {\n\t\tocteon_mgmt_disable_rx_irq(p);\n\t\tnapi_schedule(&p->napi);\n\t}\n\tif (mixx_isr.s.orthresh) {\n\t\tocteon_mgmt_disable_tx_irq(p);\n\t\ttasklet_schedule(&p->tx_clean_tasklet);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int octeon_mgmt_ioctl_hwtstamp(struct net_device *netdev,\n\t\t\t\t      struct ifreq *rq, int cmd)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tstruct hwtstamp_config config;\n\tunion cvmx_mio_ptp_clock_cfg ptp;\n\tunion cvmx_agl_gmx_rxx_frm_ctl rxx_frm_ctl;\n\tbool have_hw_timestamps = false;\n\n\tif (copy_from_user(&config, rq->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\t \n\tif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\n\t\t \n\t\tptp.u64 = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_CFG);\n\t\tif (!ptp.s.ext_clk_en) {\n\t\t\t \n\t\t\tu64 clock_comp = (NSEC_PER_SEC << 32) /\tocteon_get_io_clock_rate();\n\t\t\tif (!ptp.s.ptp_en)\n\t\t\t\tcvmx_write_csr(CVMX_MIO_PTP_CLOCK_COMP, clock_comp);\n\t\t\tnetdev_info(netdev,\n\t\t\t\t    \"PTP Clock using sclk reference @ %lldHz\\n\",\n\t\t\t\t    (NSEC_PER_SEC << 32) / clock_comp);\n\t\t} else {\n\t\t\t \n\t\t\tu64 clock_comp = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_COMP);\n\t\t\tnetdev_info(netdev,\n\t\t\t\t    \"PTP Clock using GPIO%d @ %lld Hz\\n\",\n\t\t\t\t    ptp.s.ext_clk_in, (NSEC_PER_SEC << 32) / clock_comp);\n\t\t}\n\n\t\t \n\t\tif (!ptp.s.ptp_en) {\n\t\t\tptp.s.ptp_en = 1;\n\t\t\tcvmx_write_csr(CVMX_MIO_PTP_CLOCK_CFG, ptp.u64);\n\t\t}\n\t\thave_hw_timestamps = true;\n\t}\n\n\tif (!have_hw_timestamps)\n\t\treturn -EINVAL;\n\n\tswitch (config.tx_type) {\n\tcase HWTSTAMP_TX_OFF:\n\tcase HWTSTAMP_TX_ON:\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tswitch (config.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\tp->has_rx_tstamp = false;\n\t\trxx_frm_ctl.u64 = cvmx_read_csr(p->agl + AGL_GMX_RX_FRM_CTL);\n\t\trxx_frm_ctl.s.ptp_mode = 0;\n\t\tcvmx_write_csr(p->agl + AGL_GMX_RX_FRM_CTL, rxx_frm_ctl.u64);\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_ALL:\n\tcase HWTSTAMP_FILTER_SOME:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_NTP_ALL:\n\t\tp->has_rx_tstamp = have_hw_timestamps;\n\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t\tif (p->has_rx_tstamp) {\n\t\t\trxx_frm_ctl.u64 = cvmx_read_csr(p->agl + AGL_GMX_RX_FRM_CTL);\n\t\t\trxx_frm_ctl.s.ptp_mode = 1;\n\t\t\tcvmx_write_csr(p->agl + AGL_GMX_RX_FRM_CTL, rxx_frm_ctl.u64);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tif (copy_to_user(rq->ifr_data, &config, sizeof(config)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int octeon_mgmt_ioctl(struct net_device *netdev,\n\t\t\t     struct ifreq *rq, int cmd)\n{\n\tswitch (cmd) {\n\tcase SIOCSHWTSTAMP:\n\t\treturn octeon_mgmt_ioctl_hwtstamp(netdev, rq, cmd);\n\tdefault:\n\t\treturn phy_do_ioctl(netdev, rq, cmd);\n\t}\n}\n\nstatic void octeon_mgmt_disable_link(struct octeon_mgmt *p)\n{\n\tunion cvmx_agl_gmx_prtx_cfg prtx_cfg;\n\n\t \n\tprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\n\tprtx_cfg.s.en = 0;\n\tprtx_cfg.s.tx_en = 0;\n\tprtx_cfg.s.rx_en = 0;\n\tcvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, prtx_cfg.u64);\n\n\tif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\n\t\tint i;\n\t\tfor (i = 0; i < 10; i++) {\n\t\t\tprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\n\t\t\tif (prtx_cfg.s.tx_idle == 1 || prtx_cfg.s.rx_idle == 1)\n\t\t\t\tbreak;\n\t\t\tmdelay(1);\n\t\t\ti++;\n\t\t}\n\t}\n}\n\nstatic void octeon_mgmt_enable_link(struct octeon_mgmt *p)\n{\n\tunion cvmx_agl_gmx_prtx_cfg prtx_cfg;\n\n\t \n\tprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\n\tprtx_cfg.s.tx_en = 1;\n\tprtx_cfg.s.rx_en = 1;\n\tprtx_cfg.s.en = 1;\n\tcvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, prtx_cfg.u64);\n}\n\nstatic void octeon_mgmt_update_link(struct octeon_mgmt *p)\n{\n\tstruct net_device *ndev = p->netdev;\n\tstruct phy_device *phydev = ndev->phydev;\n\tunion cvmx_agl_gmx_prtx_cfg prtx_cfg;\n\n\tprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\n\n\tif (!phydev->link)\n\t\tprtx_cfg.s.duplex = 1;\n\telse\n\t\tprtx_cfg.s.duplex = phydev->duplex;\n\n\tswitch (phydev->speed) {\n\tcase 10:\n\t\tprtx_cfg.s.speed = 0;\n\t\tprtx_cfg.s.slottime = 0;\n\n\t\tif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\n\t\t\tprtx_cfg.s.burst = 1;\n\t\t\tprtx_cfg.s.speed_msb = 1;\n\t\t}\n\t\tbreak;\n\tcase 100:\n\t\tprtx_cfg.s.speed = 0;\n\t\tprtx_cfg.s.slottime = 0;\n\n\t\tif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\n\t\t\tprtx_cfg.s.burst = 1;\n\t\t\tprtx_cfg.s.speed_msb = 0;\n\t\t}\n\t\tbreak;\n\tcase 1000:\n\t\t \n\t\tif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\n\t\t\tprtx_cfg.s.speed = 1;\n\t\t\tprtx_cfg.s.speed_msb = 0;\n\t\t\t \n\t\t\tprtx_cfg.s.slottime = 1;\n\t\t\tprtx_cfg.s.burst = phydev->duplex;\n\t\t}\n\t\tbreak;\n\tcase 0:   \n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tcvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, prtx_cfg.u64);\n\n\t \n\tprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\n\n\tif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\n\t\tunion cvmx_agl_gmx_txx_clk agl_clk;\n\t\tunion cvmx_agl_prtx_ctl prtx_ctl;\n\n\t\tprtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\n\t\tagl_clk.u64 = cvmx_read_csr(p->agl + AGL_GMX_TX_CLK);\n\t\t \n\t\tagl_clk.s.clk_cnt = 1;\n\t\tif (prtx_ctl.s.mode == 0) {  \n\t\t\tif (phydev->speed == 10)\n\t\t\t\tagl_clk.s.clk_cnt = 50;\n\t\t\telse if (phydev->speed == 100)\n\t\t\t\tagl_clk.s.clk_cnt = 5;\n\t\t}\n\t\tcvmx_write_csr(p->agl + AGL_GMX_TX_CLK, agl_clk.u64);\n\t}\n}\n\nstatic void octeon_mgmt_adjust_link(struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tstruct phy_device *phydev = netdev->phydev;\n\tunsigned long flags;\n\tint link_changed = 0;\n\n\tif (!phydev)\n\t\treturn;\n\n\tspin_lock_irqsave(&p->lock, flags);\n\n\n\tif (!phydev->link && p->last_link)\n\t\tlink_changed = -1;\n\n\tif (phydev->link &&\n\t    (p->last_duplex != phydev->duplex ||\n\t     p->last_link != phydev->link ||\n\t     p->last_speed != phydev->speed)) {\n\t\tocteon_mgmt_disable_link(p);\n\t\tlink_changed = 1;\n\t\tocteon_mgmt_update_link(p);\n\t\tocteon_mgmt_enable_link(p);\n\t}\n\n\tp->last_link = phydev->link;\n\tp->last_speed = phydev->speed;\n\tp->last_duplex = phydev->duplex;\n\n\tspin_unlock_irqrestore(&p->lock, flags);\n\n\tif (link_changed != 0) {\n\t\tif (link_changed > 0)\n\t\t\tnetdev_info(netdev, \"Link is up - %d/%s\\n\",\n\t\t\t\t    phydev->speed, phydev->duplex == DUPLEX_FULL ? \"Full\" : \"Half\");\n\t\telse\n\t\t\tnetdev_info(netdev, \"Link is down\\n\");\n\t}\n}\n\nstatic int octeon_mgmt_init_phy(struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tstruct phy_device *phydev = NULL;\n\n\tif (octeon_is_simulation() || p->phy_np == NULL) {\n\t\t \n\t\tnetif_carrier_on(netdev);\n\t\treturn 0;\n\t}\n\n\tphydev = of_phy_connect(netdev, p->phy_np,\n\t\t\t\tocteon_mgmt_adjust_link, 0,\n\t\t\t\tPHY_INTERFACE_MODE_MII);\n\n\tif (!phydev)\n\t\treturn -EPROBE_DEFER;\n\n\treturn 0;\n}\n\nstatic int octeon_mgmt_open(struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tunion cvmx_mixx_ctl mix_ctl;\n\tunion cvmx_agl_gmx_inf_mode agl_gmx_inf_mode;\n\tunion cvmx_mixx_oring1 oring1;\n\tunion cvmx_mixx_iring1 iring1;\n\tunion cvmx_agl_gmx_rxx_frm_ctl rxx_frm_ctl;\n\tunion cvmx_mixx_irhwm mix_irhwm;\n\tunion cvmx_mixx_orhwm mix_orhwm;\n\tunion cvmx_mixx_intena mix_intena;\n\tstruct sockaddr sa;\n\n\t \n\tp->tx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\n\t\t\t     GFP_KERNEL);\n\tif (!p->tx_ring)\n\t\treturn -ENOMEM;\n\tp->tx_ring_handle =\n\t\tdma_map_single(p->dev, p->tx_ring,\n\t\t\t       ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\n\t\t\t       DMA_BIDIRECTIONAL);\n\tp->tx_next = 0;\n\tp->tx_next_clean = 0;\n\tp->tx_current_fill = 0;\n\n\n\tp->rx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\n\t\t\t     GFP_KERNEL);\n\tif (!p->rx_ring)\n\t\tgoto err_nomem;\n\tp->rx_ring_handle =\n\t\tdma_map_single(p->dev, p->rx_ring,\n\t\t\t       ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\n\t\t\t       DMA_BIDIRECTIONAL);\n\n\tp->rx_next = 0;\n\tp->rx_next_fill = 0;\n\tp->rx_current_fill = 0;\n\n\tocteon_mgmt_reset_hw(p);\n\n\tmix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);\n\n\t \n\tif (mix_ctl.s.reset) {\n\t\tmix_ctl.s.reset = 0;\n\t\tcvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);\n\t\tdo {\n\t\t\tmix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);\n\t\t} while (mix_ctl.s.reset);\n\t}\n\n\tif (OCTEON_IS_MODEL(OCTEON_CN5XXX)) {\n\t\tagl_gmx_inf_mode.u64 = 0;\n\t\tagl_gmx_inf_mode.s.en = 1;\n\t\tcvmx_write_csr(CVMX_AGL_GMX_INF_MODE, agl_gmx_inf_mode.u64);\n\t}\n\tif (OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X)\n\t\t|| OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X)) {\n\t\t \n\t\tunion cvmx_agl_gmx_drv_ctl drv_ctl;\n\n\t\tdrv_ctl.u64 = cvmx_read_csr(CVMX_AGL_GMX_DRV_CTL);\n\t\tif (p->port) {\n\t\t\tdrv_ctl.s.byp_en1 = 1;\n\t\t\tdrv_ctl.s.nctl1 = 6;\n\t\t\tdrv_ctl.s.pctl1 = 6;\n\t\t} else {\n\t\t\tdrv_ctl.s.byp_en = 1;\n\t\t\tdrv_ctl.s.nctl = 6;\n\t\t\tdrv_ctl.s.pctl = 6;\n\t\t}\n\t\tcvmx_write_csr(CVMX_AGL_GMX_DRV_CTL, drv_ctl.u64);\n\t}\n\n\toring1.u64 = 0;\n\toring1.s.obase = p->tx_ring_handle >> 3;\n\toring1.s.osize = OCTEON_MGMT_TX_RING_SIZE;\n\tcvmx_write_csr(p->mix + MIX_ORING1, oring1.u64);\n\n\tiring1.u64 = 0;\n\tiring1.s.ibase = p->rx_ring_handle >> 3;\n\tiring1.s.isize = OCTEON_MGMT_RX_RING_SIZE;\n\tcvmx_write_csr(p->mix + MIX_IRING1, iring1.u64);\n\n\tmemcpy(sa.sa_data, netdev->dev_addr, ETH_ALEN);\n\tocteon_mgmt_set_mac_address(netdev, &sa);\n\n\tocteon_mgmt_change_mtu(netdev, netdev->mtu);\n\n\t \n\tmix_ctl.u64 = 0;\n\tmix_ctl.s.crc_strip = 1;     \n\tmix_ctl.s.en = 1;            \n\tmix_ctl.s.nbtarb = 0;        \n\t \n\tmix_ctl.s.mrq_hwm = 1;\n#ifdef __LITTLE_ENDIAN\n\tmix_ctl.s.lendian = 1;\n#endif\n\tcvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);\n\n\t \n\tif (octeon_mgmt_init_phy(netdev)) {\n\t\tdev_err(p->dev, \"Cannot initialize PHY on MIX%d.\\n\", p->port);\n\t\tgoto err_noirq;\n\t}\n\n\t \n\tif (OCTEON_IS_MODEL(OCTEON_CN6XXX) && netdev->phydev) {\n\t\tunion cvmx_agl_prtx_ctl agl_prtx_ctl;\n\t\tint rgmii_mode =\n\t\t\t(linkmode_test_bit(ETHTOOL_LINK_MODE_1000baseT_Half_BIT,\n\t\t\t\t\t   netdev->phydev->supported) |\n\t\t\t linkmode_test_bit(ETHTOOL_LINK_MODE_1000baseT_Full_BIT,\n\t\t\t\t\t   netdev->phydev->supported)) != 0;\n\n\t\tagl_prtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\n\t\tagl_prtx_ctl.s.mode = rgmii_mode ? 0 : 1;\n\t\tcvmx_write_csr(p->agl_prt_ctl,\tagl_prtx_ctl.u64);\n\n\t\t \n#define NS_PER_PHY_CLK 8\n\n\t\t \n\t\tagl_prtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\n\t\tagl_prtx_ctl.s.clkrst = 0;\n\t\tif (rgmii_mode) {\n\t\t\tagl_prtx_ctl.s.dllrst = 0;\n\t\t\tagl_prtx_ctl.s.clktx_byp = 0;\n\t\t}\n\t\tcvmx_write_csr(p->agl_prt_ctl,\tagl_prtx_ctl.u64);\n\t\tcvmx_read_csr(p->agl_prt_ctl);  \n\n\t\t \n\t\tndelay(256 * NS_PER_PHY_CLK);\n\n\t\t \n\t\tagl_prtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\n\t\tagl_prtx_ctl.s.enable = 1;\n\t\tcvmx_write_csr(p->agl_prt_ctl, agl_prtx_ctl.u64);\n\n\t\t \n\t\tagl_prtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\n\n\t\t \n\t\tagl_prtx_ctl.s.comp = 1;\n\t\tagl_prtx_ctl.s.drv_byp = 0;\n\t\tcvmx_write_csr(p->agl_prt_ctl,\tagl_prtx_ctl.u64);\n\t\t \n\t\tcvmx_read_csr(p->agl_prt_ctl);\n\n\t\t \n\t\tndelay(1040 * NS_PER_PHY_CLK);\n\n\t\t \n\t\tcvmx_write_csr(CVMX_AGL_GMX_TX_IFG, 0xae);\n\t}\n\n\tocteon_mgmt_rx_fill_ring(netdev);\n\n\t \n\t \n\tcvmx_write_csr(p->agl + AGL_GMX_RX_STATS_CTL, 1);\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_STATS_PKTS_DRP, 0);\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_STATS_PKTS_BAD, 0);\n\n\tcvmx_write_csr(p->agl + AGL_GMX_TX_STATS_CTL, 1);\n\tcvmx_write_csr(p->agl + AGL_GMX_TX_STAT0, 0);\n\tcvmx_write_csr(p->agl + AGL_GMX_TX_STAT1, 0);\n\n\t \n\tcvmx_write_csr(p->mix + MIX_ISR, cvmx_read_csr(p->mix + MIX_ISR));\n\n\tif (request_irq(p->irq, octeon_mgmt_interrupt, 0, netdev->name,\n\t\t\tnetdev)) {\n\t\tdev_err(p->dev, \"request_irq(%d) failed.\\n\", p->irq);\n\t\tgoto err_noirq;\n\t}\n\n\t \n\tmix_irhwm.u64 = 0;\n\tmix_irhwm.s.irhwm = 0;\n\tcvmx_write_csr(p->mix + MIX_IRHWM, mix_irhwm.u64);\n\n\t \n\tmix_orhwm.u64 = 0;\n\tmix_orhwm.s.orhwm = 0;\n\tcvmx_write_csr(p->mix + MIX_ORHWM, mix_orhwm.u64);\n\n\t \n\tmix_intena.u64 = 0;\n\tmix_intena.s.ithena = 1;\n\tmix_intena.s.othena = 1;\n\tcvmx_write_csr(p->mix + MIX_INTENA, mix_intena.u64);\n\n\t \n\n\trxx_frm_ctl.u64 = 0;\n\trxx_frm_ctl.s.ptp_mode = p->has_rx_tstamp ? 1 : 0;\n\trxx_frm_ctl.s.pre_align = 1;\n\t \n\trxx_frm_ctl.s.pad_len = 1;\n\t \n\trxx_frm_ctl.s.vlan_len = 1;\n\t \n\trxx_frm_ctl.s.pre_free = 1;\n\t \n\trxx_frm_ctl.s.ctl_smac = 0;\n\t \n\trxx_frm_ctl.s.ctl_mcst = 1;\n\t \n\trxx_frm_ctl.s.ctl_bck = 1;\n\t \n\trxx_frm_ctl.s.ctl_drp = 1;\n\t \n\trxx_frm_ctl.s.pre_strp = 1;\n\t \n\trxx_frm_ctl.s.pre_chk = 1;\n\tcvmx_write_csr(p->agl + AGL_GMX_RX_FRM_CTL, rxx_frm_ctl.u64);\n\n\t \n\tocteon_mgmt_disable_link(p);\n\tif (netdev->phydev)\n\t\tocteon_mgmt_update_link(p);\n\tocteon_mgmt_enable_link(p);\n\n\tp->last_link = 0;\n\tp->last_speed = 0;\n\t \n\tif (netdev->phydev) {\n\t\tnetif_carrier_off(netdev);\n\t\tphy_start(netdev->phydev);\n\t}\n\n\tnetif_wake_queue(netdev);\n\tnapi_enable(&p->napi);\n\n\treturn 0;\nerr_noirq:\n\tocteon_mgmt_reset_hw(p);\n\tdma_unmap_single(p->dev, p->rx_ring_handle,\n\t\t\t ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\n\t\t\t DMA_BIDIRECTIONAL);\n\tkfree(p->rx_ring);\nerr_nomem:\n\tdma_unmap_single(p->dev, p->tx_ring_handle,\n\t\t\t ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\n\t\t\t DMA_BIDIRECTIONAL);\n\tkfree(p->tx_ring);\n\treturn -ENOMEM;\n}\n\nstatic int octeon_mgmt_stop(struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\n\tnapi_disable(&p->napi);\n\tnetif_stop_queue(netdev);\n\n\tif (netdev->phydev) {\n\t\tphy_stop(netdev->phydev);\n\t\tphy_disconnect(netdev->phydev);\n\t}\n\n\tnetif_carrier_off(netdev);\n\n\tocteon_mgmt_reset_hw(p);\n\n\tfree_irq(p->irq, netdev);\n\n\t \n\tskb_queue_purge(&p->tx_list);\n\tskb_queue_purge(&p->rx_list);\n\n\tdma_unmap_single(p->dev, p->rx_ring_handle,\n\t\t\t ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\n\t\t\t DMA_BIDIRECTIONAL);\n\tkfree(p->rx_ring);\n\n\tdma_unmap_single(p->dev, p->tx_ring_handle,\n\t\t\t ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\n\t\t\t DMA_BIDIRECTIONAL);\n\tkfree(p->tx_ring);\n\n\treturn 0;\n}\n\nstatic netdev_tx_t\nocteon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\tunion mgmt_port_ring_entry re;\n\tunsigned long flags;\n\tnetdev_tx_t rv = NETDEV_TX_BUSY;\n\n\tre.d64 = 0;\n\tre.s.tstamp = ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) != 0);\n\tre.s.len = skb->len;\n\tre.s.addr = dma_map_single(p->dev, skb->data,\n\t\t\t\t   skb->len,\n\t\t\t\t   DMA_TO_DEVICE);\n\n\tspin_lock_irqsave(&p->tx_list.lock, flags);\n\n\tif (unlikely(p->tx_current_fill >= ring_max_fill(OCTEON_MGMT_TX_RING_SIZE) - 1)) {\n\t\tspin_unlock_irqrestore(&p->tx_list.lock, flags);\n\t\tnetif_stop_queue(netdev);\n\t\tspin_lock_irqsave(&p->tx_list.lock, flags);\n\t}\n\n\tif (unlikely(p->tx_current_fill >=\n\t\t     ring_max_fill(OCTEON_MGMT_TX_RING_SIZE))) {\n\t\tspin_unlock_irqrestore(&p->tx_list.lock, flags);\n\t\tdma_unmap_single(p->dev, re.s.addr, re.s.len,\n\t\t\t\t DMA_TO_DEVICE);\n\t\tgoto out;\n\t}\n\n\t__skb_queue_tail(&p->tx_list, skb);\n\n\t \n\tp->tx_ring[p->tx_next] = re.d64;\n\tp->tx_next = (p->tx_next + 1) % OCTEON_MGMT_TX_RING_SIZE;\n\tp->tx_current_fill++;\n\n\tspin_unlock_irqrestore(&p->tx_list.lock, flags);\n\n\tdma_sync_single_for_device(p->dev, p->tx_ring_handle,\n\t\t\t\t   ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\n\tnetdev->stats.tx_packets++;\n\tnetdev->stats.tx_bytes += skb->len;\n\n\t \n\tcvmx_write_csr(p->mix + MIX_ORING2, 1);\n\n\tnetif_trans_update(netdev);\n\trv = NETDEV_TX_OK;\nout:\n\tocteon_mgmt_update_tx_stats(netdev);\n\treturn rv;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void octeon_mgmt_poll_controller(struct net_device *netdev)\n{\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\n\tocteon_mgmt_receive_packets(p, 16);\n\tocteon_mgmt_update_rx_stats(netdev);\n}\n#endif\n\nstatic void octeon_mgmt_get_drvinfo(struct net_device *netdev,\n\t\t\t\t    struct ethtool_drvinfo *info)\n{\n\tstrscpy(info->driver, DRV_NAME, sizeof(info->driver));\n}\n\nstatic int octeon_mgmt_nway_reset(struct net_device *dev)\n{\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif (dev->phydev)\n\t\treturn phy_start_aneg(dev->phydev);\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic const struct ethtool_ops octeon_mgmt_ethtool_ops = {\n\t.get_drvinfo = octeon_mgmt_get_drvinfo,\n\t.nway_reset = octeon_mgmt_nway_reset,\n\t.get_link = ethtool_op_get_link,\n\t.get_link_ksettings = phy_ethtool_get_link_ksettings,\n\t.set_link_ksettings = phy_ethtool_set_link_ksettings,\n};\n\nstatic const struct net_device_ops octeon_mgmt_ops = {\n\t.ndo_open =\t\t\tocteon_mgmt_open,\n\t.ndo_stop =\t\t\tocteon_mgmt_stop,\n\t.ndo_start_xmit =\t\tocteon_mgmt_xmit,\n\t.ndo_set_rx_mode =\t\tocteon_mgmt_set_rx_filtering,\n\t.ndo_set_mac_address =\t\tocteon_mgmt_set_mac_address,\n\t.ndo_eth_ioctl =\t\t\tocteon_mgmt_ioctl,\n\t.ndo_change_mtu =\t\tocteon_mgmt_change_mtu,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller =\t\tocteon_mgmt_poll_controller,\n#endif\n};\n\nstatic int octeon_mgmt_probe(struct platform_device *pdev)\n{\n\tstruct net_device *netdev;\n\tstruct octeon_mgmt *p;\n\tconst __be32 *data;\n\tstruct resource *res_mix;\n\tstruct resource *res_agl;\n\tstruct resource *res_agl_prt_ctl;\n\tint len;\n\tint result;\n\n\tnetdev = alloc_etherdev(sizeof(struct octeon_mgmt));\n\tif (netdev == NULL)\n\t\treturn -ENOMEM;\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\n\tplatform_set_drvdata(pdev, netdev);\n\tp = netdev_priv(netdev);\n\tnetif_napi_add_weight(netdev, &p->napi, octeon_mgmt_napi_poll,\n\t\t\t      OCTEON_MGMT_NAPI_WEIGHT);\n\n\tp->netdev = netdev;\n\tp->dev = &pdev->dev;\n\tp->has_rx_tstamp = false;\n\n\tdata = of_get_property(pdev->dev.of_node, \"cell-index\", &len);\n\tif (data && len == sizeof(*data)) {\n\t\tp->port = be32_to_cpup(data);\n\t} else {\n\t\tdev_err(&pdev->dev, \"no 'cell-index' property\\n\");\n\t\tresult = -ENXIO;\n\t\tgoto err;\n\t}\n\n\tsnprintf(netdev->name, IFNAMSIZ, \"mgmt%d\", p->port);\n\n\tresult = platform_get_irq(pdev, 0);\n\tif (result < 0)\n\t\tgoto err;\n\n\tp->irq = result;\n\n\tres_mix = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (res_mix == NULL) {\n\t\tdev_err(&pdev->dev, \"no 'reg' resource\\n\");\n\t\tresult = -ENXIO;\n\t\tgoto err;\n\t}\n\n\tres_agl = platform_get_resource(pdev, IORESOURCE_MEM, 1);\n\tif (res_agl == NULL) {\n\t\tdev_err(&pdev->dev, \"no 'reg' resource\\n\");\n\t\tresult = -ENXIO;\n\t\tgoto err;\n\t}\n\n\tres_agl_prt_ctl = platform_get_resource(pdev, IORESOURCE_MEM, 3);\n\tif (res_agl_prt_ctl == NULL) {\n\t\tdev_err(&pdev->dev, \"no 'reg' resource\\n\");\n\t\tresult = -ENXIO;\n\t\tgoto err;\n\t}\n\n\tp->mix_phys = res_mix->start;\n\tp->mix_size = resource_size(res_mix);\n\tp->agl_phys = res_agl->start;\n\tp->agl_size = resource_size(res_agl);\n\tp->agl_prt_ctl_phys = res_agl_prt_ctl->start;\n\tp->agl_prt_ctl_size = resource_size(res_agl_prt_ctl);\n\n\n\tif (!devm_request_mem_region(&pdev->dev, p->mix_phys, p->mix_size,\n\t\t\t\t     res_mix->name)) {\n\t\tdev_err(&pdev->dev, \"request_mem_region (%s) failed\\n\",\n\t\t\tres_mix->name);\n\t\tresult = -ENXIO;\n\t\tgoto err;\n\t}\n\n\tif (!devm_request_mem_region(&pdev->dev, p->agl_phys, p->agl_size,\n\t\t\t\t     res_agl->name)) {\n\t\tresult = -ENXIO;\n\t\tdev_err(&pdev->dev, \"request_mem_region (%s) failed\\n\",\n\t\t\tres_agl->name);\n\t\tgoto err;\n\t}\n\n\tif (!devm_request_mem_region(&pdev->dev, p->agl_prt_ctl_phys,\n\t\t\t\t     p->agl_prt_ctl_size, res_agl_prt_ctl->name)) {\n\t\tresult = -ENXIO;\n\t\tdev_err(&pdev->dev, \"request_mem_region (%s) failed\\n\",\n\t\t\tres_agl_prt_ctl->name);\n\t\tgoto err;\n\t}\n\n\tp->mix = (u64)devm_ioremap(&pdev->dev, p->mix_phys, p->mix_size);\n\tp->agl = (u64)devm_ioremap(&pdev->dev, p->agl_phys, p->agl_size);\n\tp->agl_prt_ctl = (u64)devm_ioremap(&pdev->dev, p->agl_prt_ctl_phys,\n\t\t\t\t\t   p->agl_prt_ctl_size);\n\tif (!p->mix || !p->agl || !p->agl_prt_ctl) {\n\t\tdev_err(&pdev->dev, \"failed to map I/O memory\\n\");\n\t\tresult = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tspin_lock_init(&p->lock);\n\n\tskb_queue_head_init(&p->tx_list);\n\tskb_queue_head_init(&p->rx_list);\n\ttasklet_setup(&p->tx_clean_tasklet,\n\t\t      octeon_mgmt_clean_tx_tasklet);\n\n\tnetdev->priv_flags |= IFF_UNICAST_FLT;\n\n\tnetdev->netdev_ops = &octeon_mgmt_ops;\n\tnetdev->ethtool_ops = &octeon_mgmt_ethtool_ops;\n\n\tnetdev->min_mtu = 64 - OCTEON_MGMT_RX_HEADROOM;\n\tnetdev->max_mtu = 16383 - OCTEON_MGMT_RX_HEADROOM - VLAN_HLEN;\n\n\tresult = of_get_ethdev_address(pdev->dev.of_node, netdev);\n\tif (result)\n\t\teth_hw_addr_random(netdev);\n\n\tp->phy_np = of_parse_phandle(pdev->dev.of_node, \"phy-handle\", 0);\n\n\tresult = dma_coerce_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (result)\n\t\tgoto err;\n\n\tnetif_carrier_off(netdev);\n\tresult = register_netdev(netdev);\n\tif (result)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tof_node_put(p->phy_np);\n\tfree_netdev(netdev);\n\treturn result;\n}\n\nstatic int octeon_mgmt_remove(struct platform_device *pdev)\n{\n\tstruct net_device *netdev = platform_get_drvdata(pdev);\n\tstruct octeon_mgmt *p = netdev_priv(netdev);\n\n\tunregister_netdev(netdev);\n\tof_node_put(p->phy_np);\n\tfree_netdev(netdev);\n\treturn 0;\n}\n\nstatic const struct of_device_id octeon_mgmt_match[] = {\n\t{\n\t\t.compatible = \"cavium,octeon-5750-mix\",\n\t},\n\t{},\n};\nMODULE_DEVICE_TABLE(of, octeon_mgmt_match);\n\nstatic struct platform_driver octeon_mgmt_driver = {\n\t.driver = {\n\t\t.name\t\t= \"octeon_mgmt\",\n\t\t.of_match_table = octeon_mgmt_match,\n\t},\n\t.probe\t\t= octeon_mgmt_probe,\n\t.remove\t\t= octeon_mgmt_remove,\n};\n\nmodule_platform_driver(octeon_mgmt_driver);\n\nMODULE_SOFTDEP(\"pre: mdio-cavium\");\nMODULE_DESCRIPTION(DRV_DESCRIPTION);\nMODULE_AUTHOR(\"David Daney\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}