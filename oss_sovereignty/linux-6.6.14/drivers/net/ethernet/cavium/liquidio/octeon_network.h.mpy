{
  "module_name": "octeon_network.h",
  "hash_id": "ccd616fa122d6e1eea68057dc57442481eea4987907a05f96e9a371f18858c93",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/cavium/liquidio/octeon_network.h",
  "human_readable_source": " \n\n \n\n#ifndef __OCTEON_NETWORK_H__\n#define __OCTEON_NETWORK_H__\n#include <linux/ptp_clock_kernel.h>\n\n#define LIO_MAX_MTU_SIZE (OCTNET_MAX_FRM_SIZE - OCTNET_FRM_HEADER_SIZE)\n#define LIO_MIN_MTU_SIZE ETH_MIN_MTU\n\n \n#define   LIO_IFSTATE_DROQ_OPS             0x01\n#define   LIO_IFSTATE_REGISTERED           0x02\n#define   LIO_IFSTATE_RUNNING              0x04\n#define   LIO_IFSTATE_RX_TIMESTAMP_ENABLED 0x08\n#define   LIO_IFSTATE_RESETTING\t\t   0x10\n\nstruct liquidio_if_cfg_resp {\n\tu64 rh;\n\tstruct liquidio_if_cfg_info cfg_info;\n\tu64 status;\n};\n\n#define LIO_IFCFG_WAIT_TIME    3000  \n#define LIQUIDIO_NDEV_STATS_POLL_TIME_MS 200\n\n \nstruct octnic_gather {\n\t \n\tstruct list_head list;\n\n\t \n\tint sg_size;\n\n\t \n\tint adjust;\n\n\t \n\tstruct octeon_sg_entry *sg;\n\n\tdma_addr_t sg_dma_ptr;\n};\n\nstruct oct_nic_stats_resp {\n\tu64     rh;\n\tstruct oct_link_stats stats;\n\tu64     status;\n};\n\nstruct oct_nic_vf_stats_resp {\n\tu64     rh;\n\tu64\tspoofmac_cnt;\n\tu64     status;\n};\n\nstruct oct_nic_stats_ctrl {\n\tstruct completion complete;\n\tstruct net_device *netdev;\n};\n\nstruct oct_nic_seapi_resp {\n\tu64 rh;\n\tunion {\n\t\tu32 fec_setting;\n\t\tu32 speed;\n\t};\n\tu64 status;\n};\n\n \nstruct lio {\n\t \n\tatomic_t ifstate;\n\n\t \n\tint ifidx;\n\n\t \n\tint txq;\n\n\t \n\tint rxq;\n\n\t \n\tspinlock_t *glist_lock;\n\n\t \n\tstruct list_head *glist;\n\tvoid **glists_virt_base;\n\tdma_addr_t *glists_dma_base;\n\tu32 glist_entry_size;\n\n\t \n\tstruct octdev_props *octprops;\n\n\t \n\tstruct octeon_device *oct_dev;\n\n\tstruct net_device *netdev;\n\n\t \n\tstruct oct_link_info linfo;\n\n\t \n\tu64 link_changes;\n\n\t \n\tu32 tx_qsize;\n\n\t \n\tu32 rx_qsize;\n\n\t \n\tu32 mtu;\n\n\t \n\tu32 msg_enable;\n\n\t \n\tu64 dev_capability;\n\n\t \n\tu64 enc_dev_capability;\n\n\t \n\tu32 phy_beacon_val;\n\n\t \n\tu32 led_ctrl_val;\n\n\t \n\tstruct ptp_clock_info ptp_info;\n\tstruct ptp_clock *ptp_clock;\n\ts64 ptp_adjust;\n\n\t \n\tspinlock_t ptp_lock;\n\n\t \n\tu32\tintf_open;\n\n\t \n\tstruct cavium_wq\ttxq_status_wq;\n\n\t \n\tstruct cavium_wq rxq_status_wq[MAX_POSSIBLE_OCTEON_OUTPUT_QUEUES];\n\n\t \n\tstruct cavium_wq\tlink_status_wq;\n\n\t \n\tstruct cavium_wq\tsync_octeon_time_wq;\n\n\tint netdev_uc_count;\n\tstruct cavium_wk stats_wk;\n};\n\n#define LIO_SIZE         (sizeof(struct lio))\n#define GET_LIO(netdev)  ((struct lio *)netdev_priv(netdev))\n\n#define LIO_MAX_CORES                16\n\n \nint liquidio_set_feature(struct net_device *netdev, int cmd, u16 param1);\n\nint setup_rx_oom_poll_fn(struct net_device *netdev);\n\nvoid cleanup_rx_oom_poll_fn(struct net_device *netdev);\n\n \nvoid liquidio_link_ctrl_cmd_completion(void *nctrl_ptr);\n\nint liquidio_setup_io_queues(struct octeon_device *octeon_dev, int ifidx,\n\t\t\t     u32 num_iqs, u32 num_oqs);\n\nirqreturn_t liquidio_msix_intr_handler(int irq __attribute__((unused)),\n\t\t\t\t       void *dev);\n\nint octeon_setup_interrupt(struct octeon_device *oct, u32 num_ioqs);\n\nvoid lio_fetch_stats(struct work_struct *work);\n\nint lio_wait_for_clean_oq(struct octeon_device *oct);\n \nvoid liquidio_set_ethtool_ops(struct net_device *netdev);\n\nvoid lio_delete_glists(struct lio *lio);\n\nint lio_setup_glists(struct octeon_device *oct, struct lio *lio, int num_qs);\n\nint liquidio_get_speed(struct lio *lio);\nint liquidio_set_speed(struct lio *lio, int speed);\nint liquidio_get_fec(struct lio *lio);\nint liquidio_set_fec(struct lio *lio, int on_off);\n\n \nint liquidio_change_mtu(struct net_device *netdev, int new_mtu);\n#define LIO_CHANGE_MTU_SUCCESS 1\n#define LIO_CHANGE_MTU_FAIL    2\n\n#define SKB_ADJ_MASK  0x3F\n#define SKB_ADJ       (SKB_ADJ_MASK + 1)\n\n#define MIN_SKB_SIZE       256  \n#define LIO_RXBUFFER_SZ    2048\n\nstatic inline void\n*recv_buffer_alloc(struct octeon_device *oct,\n\t\t   struct octeon_skb_page_info *pg_info)\n{\n\tstruct page *page;\n\tstruct sk_buff *skb;\n\tstruct octeon_skb_page_info *skb_pg_info;\n\n\tpage = alloc_page(GFP_ATOMIC);\n\tif (unlikely(!page))\n\t\treturn NULL;\n\n\tskb = dev_alloc_skb(MIN_SKB_SIZE + SKB_ADJ);\n\tif (unlikely(!skb)) {\n\t\t__free_page(page);\n\t\tpg_info->page = NULL;\n\t\treturn NULL;\n\t}\n\n\tif ((unsigned long)skb->data & SKB_ADJ_MASK) {\n\t\tu32 r = SKB_ADJ - ((unsigned long)skb->data & SKB_ADJ_MASK);\n\n\t\tskb_reserve(skb, r);\n\t}\n\n\tskb_pg_info = ((struct octeon_skb_page_info *)(skb->cb));\n\t \n\tpg_info->dma = dma_map_page(&oct->pci_dev->dev, page, 0,\n\t\t\t\t    PAGE_SIZE, DMA_FROM_DEVICE);\n\n\t \n\tif (dma_mapping_error(&oct->pci_dev->dev, pg_info->dma)) {\n\t\t__free_page(page);\n\t\tdev_kfree_skb_any((struct sk_buff *)skb);\n\t\tpg_info->page = NULL;\n\t\treturn NULL;\n\t}\n\n\tpg_info->page = page;\n\tpg_info->page_offset = 0;\n\tskb_pg_info->page = page;\n\tskb_pg_info->page_offset = 0;\n\tskb_pg_info->dma = pg_info->dma;\n\n\treturn (void *)skb;\n}\n\nstatic inline void\n*recv_buffer_fast_alloc(u32 size)\n{\n\tstruct sk_buff *skb;\n\tstruct octeon_skb_page_info *skb_pg_info;\n\n\tskb = dev_alloc_skb(size + SKB_ADJ);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tif ((unsigned long)skb->data & SKB_ADJ_MASK) {\n\t\tu32 r = SKB_ADJ - ((unsigned long)skb->data & SKB_ADJ_MASK);\n\n\t\tskb_reserve(skb, r);\n\t}\n\n\tskb_pg_info = ((struct octeon_skb_page_info *)(skb->cb));\n\tskb_pg_info->page = NULL;\n\tskb_pg_info->page_offset = 0;\n\tskb_pg_info->dma = 0;\n\n\treturn skb;\n}\n\nstatic inline int\nrecv_buffer_recycle(struct octeon_device *oct, void *buf)\n{\n\tstruct octeon_skb_page_info *pg_info = buf;\n\n\tif (!pg_info->page) {\n\t\tdev_err(&oct->pci_dev->dev, \"%s: pg_info->page NULL\\n\",\n\t\t\t__func__);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (unlikely(page_count(pg_info->page) != 1) ||\n\t    unlikely(page_to_nid(pg_info->page)\t!= numa_node_id())) {\n\t\tdma_unmap_page(&oct->pci_dev->dev,\n\t\t\t       pg_info->dma, (PAGE_SIZE << 0),\n\t\t\t       DMA_FROM_DEVICE);\n\t\tpg_info->dma = 0;\n\t\tpg_info->page = NULL;\n\t\tpg_info->page_offset = 0;\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (pg_info->page_offset == 0)\n\t\tpg_info->page_offset = LIO_RXBUFFER_SZ;\n\telse\n\t\tpg_info->page_offset = 0;\n\tpage_ref_inc(pg_info->page);\n\n\treturn 0;\n}\n\nstatic inline void\n*recv_buffer_reuse(struct octeon_device *oct, void *buf)\n{\n\tstruct octeon_skb_page_info *pg_info = buf, *skb_pg_info;\n\tstruct sk_buff *skb;\n\n\tskb = dev_alloc_skb(MIN_SKB_SIZE + SKB_ADJ);\n\tif (unlikely(!skb)) {\n\t\tdma_unmap_page(&oct->pci_dev->dev,\n\t\t\t       pg_info->dma, (PAGE_SIZE << 0),\n\t\t\t       DMA_FROM_DEVICE);\n\t\treturn NULL;\n\t}\n\n\tif ((unsigned long)skb->data & SKB_ADJ_MASK) {\n\t\tu32 r = SKB_ADJ - ((unsigned long)skb->data & SKB_ADJ_MASK);\n\n\t\tskb_reserve(skb, r);\n\t}\n\n\tskb_pg_info = ((struct octeon_skb_page_info *)(skb->cb));\n\tskb_pg_info->page = pg_info->page;\n\tskb_pg_info->page_offset = pg_info->page_offset;\n\tskb_pg_info->dma = pg_info->dma;\n\n\treturn skb;\n}\n\nstatic inline void\nrecv_buffer_destroy(void *buffer, struct octeon_skb_page_info *pg_info)\n{\n\tstruct sk_buff *skb = (struct sk_buff *)buffer;\n\n\tput_page(pg_info->page);\n\tpg_info->dma = 0;\n\tpg_info->page = NULL;\n\tpg_info->page_offset = 0;\n\n\tif (skb)\n\t\tdev_kfree_skb_any(skb);\n}\n\nstatic inline void recv_buffer_free(void *buffer)\n{\n\tstruct sk_buff *skb = (struct sk_buff *)buffer;\n\tstruct octeon_skb_page_info *pg_info;\n\n\tpg_info = ((struct octeon_skb_page_info *)(skb->cb));\n\n\tif (pg_info->page) {\n\t\tput_page(pg_info->page);\n\t\tpg_info->dma = 0;\n\t\tpg_info->page = NULL;\n\t\tpg_info->page_offset = 0;\n\t}\n\n\tdev_kfree_skb_any((struct sk_buff *)buffer);\n}\n\nstatic inline void\nrecv_buffer_fast_free(void *buffer)\n{\n\tdev_kfree_skb_any((struct sk_buff *)buffer);\n}\n\nstatic inline void tx_buffer_free(void *buffer)\n{\n\tdev_kfree_skb_any((struct sk_buff *)buffer);\n}\n\n#define lio_dma_alloc(oct, size, dma_addr) \\\n\tdma_alloc_coherent(&(oct)->pci_dev->dev, size, dma_addr, GFP_KERNEL)\n#define lio_dma_free(oct, size, virt_addr, dma_addr) \\\n\tdma_free_coherent(&(oct)->pci_dev->dev, size, virt_addr, dma_addr)\n\nstatic inline\nvoid *get_rbd(struct sk_buff *skb)\n{\n\tstruct octeon_skb_page_info *pg_info;\n\tunsigned char *va;\n\n\tpg_info = ((struct octeon_skb_page_info *)(skb->cb));\n\tva = page_address(pg_info->page) + pg_info->page_offset;\n\n\treturn va;\n}\n\nstatic inline u64\nlio_map_ring(void *buf)\n{\n\tdma_addr_t dma_addr;\n\n\tstruct sk_buff *skb = (struct sk_buff *)buf;\n\tstruct octeon_skb_page_info *pg_info;\n\n\tpg_info = ((struct octeon_skb_page_info *)(skb->cb));\n\tif (!pg_info->page) {\n\t\tpr_err(\"%s: pg_info->page NULL\\n\", __func__);\n\t\tWARN_ON(1);\n\t}\n\n\t \n\tdma_addr = pg_info->dma;\n\tif (!pg_info->dma) {\n\t\tpr_err(\"%s: ERROR it should be already available\\n\",\n\t\t       __func__);\n\t\tWARN_ON(1);\n\t}\n\tdma_addr += pg_info->page_offset;\n\n\treturn (u64)dma_addr;\n}\n\nstatic inline void\nlio_unmap_ring(struct pci_dev *pci_dev,\n\t       u64 buf_ptr)\n\n{\n\tdma_unmap_page(&pci_dev->dev,\n\t\t       buf_ptr, (PAGE_SIZE << 0),\n\t\t       DMA_FROM_DEVICE);\n}\n\nstatic inline void *octeon_fast_packet_alloc(u32 size)\n{\n\treturn recv_buffer_fast_alloc(size);\n}\n\nstatic inline void octeon_fast_packet_next(struct octeon_droq *droq,\n\t\t\t\t\t   struct sk_buff *nicbuf,\n\t\t\t\t\t   int copy_len,\n\t\t\t\t\t   int idx)\n{\n\tskb_put_data(nicbuf, get_rbd(droq->recv_buf_list[idx].buffer),\n\t\t     copy_len);\n}\n\n \nstatic inline int ifstate_check(struct lio *lio, int state_flag)\n{\n\treturn atomic_read(&lio->ifstate) & state_flag;\n}\n\n \nstatic inline void ifstate_set(struct lio *lio, int state_flag)\n{\n\tatomic_set(&lio->ifstate, (atomic_read(&lio->ifstate) | state_flag));\n}\n\n \nstatic inline void ifstate_reset(struct lio *lio, int state_flag)\n{\n\tatomic_set(&lio->ifstate, (atomic_read(&lio->ifstate) & ~(state_flag)));\n}\n\n \nstatic inline int wait_for_pending_requests(struct octeon_device *oct)\n{\n\tint i, pcount = 0;\n\n\tfor (i = 0; i < MAX_IO_PENDING_PKT_COUNT; i++) {\n\t\tpcount = atomic_read(\n\t\t    &oct->response_list[OCTEON_ORDERED_SC_LIST]\n\t\t\t .pending_req_count);\n\t\tif (pcount)\n\t\t\tschedule_timeout_uninterruptible(HZ / 10);\n\t\telse\n\t\t\tbreak;\n\t}\n\n\tif (pcount)\n\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic inline void stop_txqs(struct net_device *netdev)\n{\n\tint i;\n\n\tfor (i = 0; i < netdev->real_num_tx_queues; i++)\n\t\tnetif_stop_subqueue(netdev, i);\n}\n\n \nstatic inline void wake_txqs(struct net_device *netdev)\n{\n\tstruct lio *lio = GET_LIO(netdev);\n\tint i, qno;\n\n\tfor (i = 0; i < netdev->real_num_tx_queues; i++) {\n\t\tqno = lio->linfo.txpciq[i % lio->oct_dev->num_iqs].s.q_no;\n\n\t\tif (__netif_subqueue_stopped(netdev, i)) {\n\t\t\tINCR_INSTRQUEUE_PKT_COUNT(lio->oct_dev, qno,\n\t\t\t\t\t\t  tx_restart, 1);\n\t\t\tnetif_wake_subqueue(netdev, i);\n\t\t}\n\t}\n}\n\n \nstatic inline void start_txqs(struct net_device *netdev)\n{\n\tstruct lio *lio = GET_LIO(netdev);\n\tint i;\n\n\tif (lio->linfo.link.s.link_up) {\n\t\tfor (i = 0; i < netdev->real_num_tx_queues; i++)\n\t\t\tnetif_start_subqueue(netdev, i);\n\t}\n}\n\nstatic inline int skb_iq(struct octeon_device *oct, struct sk_buff *skb)\n{\n\treturn skb->queue_mapping % oct->num_iqs;\n}\n\n \nstatic inline struct list_head *lio_list_delete_head(struct list_head *root)\n{\n\tstruct list_head *node;\n\n\tif (list_empty_careful(root))\n\t\tnode = NULL;\n\telse\n\t\tnode = root->next;\n\n\tif (node)\n\t\tlist_del(node);\n\n\treturn node;\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}