{
  "module_name": "request_manager.c",
  "hash_id": "ad06f4aefc7ea7a06b2e53d002898caee03d8893e6ca0f648fbf6449191c0766",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/cavium/liquidio/request_manager.c",
  "human_readable_source": " \n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/vmalloc.h>\n#include \"liquidio_common.h\"\n#include \"octeon_droq.h\"\n#include \"octeon_iq.h\"\n#include \"response_manager.h\"\n#include \"octeon_device.h\"\n#include \"octeon_main.h\"\n#include \"octeon_network.h\"\n#include \"cn66xx_device.h\"\n#include \"cn23xx_pf_device.h\"\n#include \"cn23xx_vf_device.h\"\n\nstruct iq_post_status {\n\tint status;\n\tint index;\n};\n\nstatic void check_db_timeout(struct work_struct *work);\nstatic void  __check_db_timeout(struct octeon_device *oct, u64 iq_no);\n\nstatic void (*reqtype_free_fn[MAX_OCTEON_DEVICES][REQTYPE_LAST + 1]) (void *);\n\n \n \n\n \nint octeon_init_instr_queue(struct octeon_device *oct,\n\t\t\t    union oct_txpciq txpciq,\n\t\t\t    u32 num_descs)\n{\n\tstruct octeon_instr_queue *iq;\n\tstruct octeon_iq_config *conf = NULL;\n\tu32 iq_no = (u32)txpciq.s.q_no;\n\tu32 q_size;\n\tstruct cavium_wq *db_wq;\n\tint numa_node = dev_to_node(&oct->pci_dev->dev);\n\n\tif (OCTEON_CN6XXX(oct))\n\t\tconf = &(CFG_GET_IQ_CFG(CHIP_CONF(oct, cn6xxx)));\n\telse if (OCTEON_CN23XX_PF(oct))\n\t\tconf = &(CFG_GET_IQ_CFG(CHIP_CONF(oct, cn23xx_pf)));\n\telse if (OCTEON_CN23XX_VF(oct))\n\t\tconf = &(CFG_GET_IQ_CFG(CHIP_CONF(oct, cn23xx_vf)));\n\n\tif (!conf) {\n\t\tdev_err(&oct->pci_dev->dev, \"Unsupported Chip %x\\n\",\n\t\t\toct->chip_id);\n\t\treturn 1;\n\t}\n\n\tq_size = (u32)conf->instr_type * num_descs;\n\n\tiq = oct->instr_queue[iq_no];\n\n\tiq->oct_dev = oct;\n\n\tiq->base_addr = lio_dma_alloc(oct, q_size, &iq->base_addr_dma);\n\tif (!iq->base_addr) {\n\t\tdev_err(&oct->pci_dev->dev, \"Cannot allocate memory for instr queue %d\\n\",\n\t\t\tiq_no);\n\t\treturn 1;\n\t}\n\n\tiq->max_count = num_descs;\n\n\t \n\tiq->request_list = vzalloc_node(array_size(num_descs, sizeof(*iq->request_list)),\n\t\t\t\t\tnuma_node);\n\tif (!iq->request_list)\n\t\tiq->request_list = vzalloc(array_size(num_descs, sizeof(*iq->request_list)));\n\tif (!iq->request_list) {\n\t\tlio_dma_free(oct, q_size, iq->base_addr, iq->base_addr_dma);\n\t\tdev_err(&oct->pci_dev->dev, \"Alloc failed for IQ[%d] nr free list\\n\",\n\t\t\tiq_no);\n\t\treturn 1;\n\t}\n\n\tdev_dbg(&oct->pci_dev->dev, \"IQ[%d]: base: %p basedma: %pad count: %d\\n\",\n\t\tiq_no, iq->base_addr, &iq->base_addr_dma, iq->max_count);\n\n\tiq->txpciq.u64 = txpciq.u64;\n\tiq->fill_threshold = (u32)conf->db_min;\n\tiq->fill_cnt = 0;\n\tiq->host_write_index = 0;\n\tiq->octeon_read_index = 0;\n\tiq->flush_index = 0;\n\tiq->last_db_time = 0;\n\tiq->do_auto_flush = 1;\n\tiq->db_timeout = (u32)conf->db_timeout;\n\tatomic_set(&iq->instr_pending, 0);\n\tiq->pkts_processed = 0;\n\n\t \n\tspin_lock_init(&iq->lock);\n\tif (iq_no == 0) {\n\t\tiq->allow_soft_cmds = true;\n\t\tspin_lock_init(&iq->post_lock);\n\t} else {\n\t\tiq->allow_soft_cmds = false;\n\t}\n\n\tspin_lock_init(&iq->iq_flush_running_lock);\n\n\toct->io_qmask.iq |= BIT_ULL(iq_no);\n\n\t \n\toct->io_qmask.iq64B |= ((conf->instr_type == 64) << iq_no);\n\tiq->iqcmd_64B = (conf->instr_type == 64);\n\n\toct->fn_list.setup_iq_regs(oct, iq_no);\n\n\toct->check_db_wq[iq_no].wq = alloc_workqueue(\"check_iq_db\",\n\t\t\t\t\t\t     WQ_MEM_RECLAIM,\n\t\t\t\t\t\t     0);\n\tif (!oct->check_db_wq[iq_no].wq) {\n\t\tvfree(iq->request_list);\n\t\tiq->request_list = NULL;\n\t\tlio_dma_free(oct, q_size, iq->base_addr, iq->base_addr_dma);\n\t\tdev_err(&oct->pci_dev->dev, \"check db wq create failed for iq %d\\n\",\n\t\t\tiq_no);\n\t\treturn 1;\n\t}\n\n\tdb_wq = &oct->check_db_wq[iq_no];\n\n\tINIT_DELAYED_WORK(&db_wq->wk.work, check_db_timeout);\n\tdb_wq->wk.ctxptr = oct;\n\tdb_wq->wk.ctxul = iq_no;\n\tqueue_delayed_work(db_wq->wq, &db_wq->wk.work, msecs_to_jiffies(1));\n\n\treturn 0;\n}\n\nint octeon_delete_instr_queue(struct octeon_device *oct, u32 iq_no)\n{\n\tu64 desc_size = 0, q_size;\n\tstruct octeon_instr_queue *iq = oct->instr_queue[iq_no];\n\n\tcancel_delayed_work_sync(&oct->check_db_wq[iq_no].wk.work);\n\tdestroy_workqueue(oct->check_db_wq[iq_no].wq);\n\n\tif (OCTEON_CN6XXX(oct))\n\t\tdesc_size =\n\t\t    CFG_GET_IQ_INSTR_TYPE(CHIP_CONF(oct, cn6xxx));\n\telse if (OCTEON_CN23XX_PF(oct))\n\t\tdesc_size =\n\t\t    CFG_GET_IQ_INSTR_TYPE(CHIP_CONF(oct, cn23xx_pf));\n\telse if (OCTEON_CN23XX_VF(oct))\n\t\tdesc_size =\n\t\t    CFG_GET_IQ_INSTR_TYPE(CHIP_CONF(oct, cn23xx_vf));\n\n\tvfree(iq->request_list);\n\n\tif (iq->base_addr) {\n\t\tq_size = iq->max_count * desc_size;\n\t\tlio_dma_free(oct, (u32)q_size, iq->base_addr,\n\t\t\t     iq->base_addr_dma);\n\t\toct->io_qmask.iq &= ~(1ULL << iq_no);\n\t\tvfree(oct->instr_queue[iq_no]);\n\t\toct->instr_queue[iq_no] = NULL;\n\t\toct->num_iqs--;\n\t\treturn 0;\n\t}\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(octeon_delete_instr_queue);\n\n \nint octeon_setup_iq(struct octeon_device *oct,\n\t\t    int ifidx,\n\t\t    int q_index,\n\t\t    union oct_txpciq txpciq,\n\t\t    u32 num_descs,\n\t\t    void *app_ctx)\n{\n\tu32 iq_no = (u32)txpciq.s.q_no;\n\tint numa_node = dev_to_node(&oct->pci_dev->dev);\n\n\tif (oct->instr_queue[iq_no]) {\n\t\tdev_dbg(&oct->pci_dev->dev, \"IQ is in use. Cannot create the IQ: %d again\\n\",\n\t\t\tiq_no);\n\t\toct->instr_queue[iq_no]->txpciq.u64 = txpciq.u64;\n\t\toct->instr_queue[iq_no]->app_ctx = app_ctx;\n\t\treturn 0;\n\t}\n\toct->instr_queue[iq_no] =\n\t    vzalloc_node(sizeof(struct octeon_instr_queue), numa_node);\n\tif (!oct->instr_queue[iq_no])\n\t\toct->instr_queue[iq_no] =\n\t\t    vzalloc(sizeof(struct octeon_instr_queue));\n\tif (!oct->instr_queue[iq_no])\n\t\treturn 1;\n\n\n\toct->instr_queue[iq_no]->q_index = q_index;\n\toct->instr_queue[iq_no]->app_ctx = app_ctx;\n\toct->instr_queue[iq_no]->ifidx = ifidx;\n\n\tif (octeon_init_instr_queue(oct, txpciq, num_descs)) {\n\t\tvfree(oct->instr_queue[iq_no]);\n\t\toct->instr_queue[iq_no] = NULL;\n\t\treturn 1;\n\t}\n\n\toct->num_iqs++;\n\tif (oct->fn_list.enable_io_queues(oct)) {\n\t\tocteon_delete_instr_queue(oct, iq_no);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nint lio_wait_for_instr_fetch(struct octeon_device *oct)\n{\n\tint i, retry = 1000, pending, instr_cnt = 0;\n\n\tdo {\n\t\tinstr_cnt = 0;\n\n\t\tfor (i = 0; i < MAX_OCTEON_INSTR_QUEUES(oct); i++) {\n\t\t\tif (!(oct->io_qmask.iq & BIT_ULL(i)))\n\t\t\t\tcontinue;\n\t\t\tpending =\n\t\t\t    atomic_read(&oct->instr_queue[i]->instr_pending);\n\t\t\tif (pending)\n\t\t\t\t__check_db_timeout(oct, i);\n\t\t\tinstr_cnt += pending;\n\t\t}\n\n\t\tif (instr_cnt == 0)\n\t\t\tbreak;\n\n\t\tschedule_timeout_uninterruptible(1);\n\n\t} while (retry-- && instr_cnt);\n\n\treturn instr_cnt;\n}\nEXPORT_SYMBOL_GPL(lio_wait_for_instr_fetch);\n\nstatic inline void\nring_doorbell(struct octeon_device *oct, struct octeon_instr_queue *iq)\n{\n\tif (atomic_read(&oct->status) == OCT_DEV_RUNNING) {\n\t\twritel(iq->fill_cnt, iq->doorbell_reg);\n\t\t \n\t\tiq->fill_cnt = 0;\n\t\tiq->last_db_time = jiffies;\n\t\treturn;\n\t}\n}\n\nvoid\nocteon_ring_doorbell_locked(struct octeon_device *oct, u32 iq_no)\n{\n\tstruct octeon_instr_queue *iq;\n\n\tiq = oct->instr_queue[iq_no];\n\tspin_lock(&iq->post_lock);\n\tif (iq->fill_cnt)\n\t\tring_doorbell(oct, iq);\n\tspin_unlock(&iq->post_lock);\n}\nEXPORT_SYMBOL_GPL(octeon_ring_doorbell_locked);\n\nstatic inline void __copy_cmd_into_iq(struct octeon_instr_queue *iq,\n\t\t\t\t      u8 *cmd)\n{\n\tu8 *iqptr, cmdsize;\n\n\tcmdsize = ((iq->iqcmd_64B) ? 64 : 32);\n\tiqptr = iq->base_addr + (cmdsize * iq->host_write_index);\n\n\tmemcpy(iqptr, cmd, cmdsize);\n}\n\nstatic inline struct iq_post_status\n__post_command2(struct octeon_instr_queue *iq, u8 *cmd)\n{\n\tstruct iq_post_status st;\n\n\tst.status = IQ_SEND_OK;\n\n\t \n\tif (atomic_read(&iq->instr_pending) >= (s32)(iq->max_count - 1)) {\n\t\tst.status = IQ_SEND_FAILED;\n\t\tst.index = -1;\n\t\treturn st;\n\t}\n\n\tif (atomic_read(&iq->instr_pending) >= (s32)(iq->max_count - 2))\n\t\tst.status = IQ_SEND_STOP;\n\n\t__copy_cmd_into_iq(iq, cmd);\n\n\t \n\tst.index = iq->host_write_index;\n\tiq->host_write_index = incr_index(iq->host_write_index, 1,\n\t\t\t\t\t  iq->max_count);\n\tiq->fill_cnt++;\n\n\t \n\twmb();\n\n\tatomic_inc(&iq->instr_pending);\n\n\treturn st;\n}\n\nint\nocteon_register_reqtype_free_fn(struct octeon_device *oct, int reqtype,\n\t\t\t\tvoid (*fn)(void *))\n{\n\tif (reqtype > REQTYPE_LAST) {\n\t\tdev_err(&oct->pci_dev->dev, \"%s: Invalid reqtype: %d\\n\",\n\t\t\t__func__, reqtype);\n\t\treturn -EINVAL;\n\t}\n\n\treqtype_free_fn[oct->octeon_id][reqtype] = fn;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(octeon_register_reqtype_free_fn);\n\nstatic inline void\n__add_to_request_list(struct octeon_instr_queue *iq,\n\t\t      int idx, void *buf, int reqtype)\n{\n\tiq->request_list[idx].buf = buf;\n\tiq->request_list[idx].reqtype = reqtype;\n}\n\n \nint\nlio_process_iq_request_list(struct octeon_device *oct,\n\t\t\t    struct octeon_instr_queue *iq, u32 napi_budget)\n{\n\tstruct cavium_wq *cwq = &oct->dma_comp_wq;\n\tint reqtype;\n\tvoid *buf;\n\tu32 old = iq->flush_index;\n\tu32 inst_count = 0;\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\tstruct octeon_soft_command *sc;\n\tunsigned long flags;\n\n\twhile (old != iq->octeon_read_index) {\n\t\treqtype = iq->request_list[old].reqtype;\n\t\tbuf     = iq->request_list[old].buf;\n\n\t\tif (reqtype == REQTYPE_NONE)\n\t\t\tgoto skip_this;\n\n\t\tocteon_update_tx_completion_counters(buf, reqtype, &pkts_compl,\n\t\t\t\t\t\t     &bytes_compl);\n\n\t\tswitch (reqtype) {\n\t\tcase REQTYPE_NORESP_NET:\n\t\tcase REQTYPE_NORESP_NET_SG:\n\t\tcase REQTYPE_RESP_NET_SG:\n\t\t\treqtype_free_fn[oct->octeon_id][reqtype](buf);\n\t\t\tbreak;\n\t\tcase REQTYPE_RESP_NET:\n\t\tcase REQTYPE_SOFT_COMMAND:\n\t\t\tsc = buf;\n\t\t\t \n\t\t\tspin_lock_irqsave(&oct->response_list\n\t\t\t\t\t  [OCTEON_ORDERED_SC_LIST].lock, flags);\n\t\t\tatomic_inc(&oct->response_list\n\t\t\t\t   [OCTEON_ORDERED_SC_LIST].pending_req_count);\n\t\t\tlist_add_tail(&sc->node, &oct->response_list\n\t\t\t\t[OCTEON_ORDERED_SC_LIST].head);\n\t\t\tspin_unlock_irqrestore(&oct->response_list\n\t\t\t\t\t       [OCTEON_ORDERED_SC_LIST].lock,\n\t\t\t\t\t       flags);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(&oct->pci_dev->dev,\n\t\t\t\t\"%s Unknown reqtype: %d buf: %p at idx %d\\n\",\n\t\t\t\t__func__, reqtype, buf, old);\n\t\t}\n\n\t\tiq->request_list[old].buf = NULL;\n\t\tiq->request_list[old].reqtype = 0;\n\n skip_this:\n\t\tinst_count++;\n\t\told = incr_index(old, 1, iq->max_count);\n\n\t\tif ((napi_budget) && (inst_count >= napi_budget))\n\t\t\tbreak;\n\t}\n\tif (bytes_compl)\n\t\tocteon_report_tx_completion_to_bql(iq->app_ctx, pkts_compl,\n\t\t\t\t\t\t   bytes_compl);\n\tiq->flush_index = old;\n\n\tif (atomic_read(&oct->response_list\n\t\t\t[OCTEON_ORDERED_SC_LIST].pending_req_count))\n\t\tqueue_work(cwq->wq, &cwq->wk.work.work);\n\n\treturn inst_count;\n}\nEXPORT_SYMBOL_GPL(lio_process_iq_request_list);\n\n \nint\nocteon_flush_iq(struct octeon_device *oct, struct octeon_instr_queue *iq,\n\t\tu32 napi_budget)\n{\n\tu32 inst_processed = 0;\n\tu32 tot_inst_processed = 0;\n\tint tx_done = 1;\n\n\tif (!spin_trylock(&iq->iq_flush_running_lock))\n\t\treturn tx_done;\n\n\tspin_lock_bh(&iq->lock);\n\n\tiq->octeon_read_index = oct->fn_list.update_iq_read_idx(iq);\n\n\tdo {\n\t\t \n\t\tif (iq->flush_index == iq->octeon_read_index)\n\t\t\tbreak;\n\n\t\tif (napi_budget)\n\t\t\tinst_processed =\n\t\t\t\tlio_process_iq_request_list(oct, iq,\n\t\t\t\t\t\t\t    napi_budget -\n\t\t\t\t\t\t\t    tot_inst_processed);\n\t\telse\n\t\t\tinst_processed =\n\t\t\t\tlio_process_iq_request_list(oct, iq, 0);\n\n\t\tif (inst_processed) {\n\t\t\tiq->pkts_processed += inst_processed;\n\t\t\tatomic_sub(inst_processed, &iq->instr_pending);\n\t\t\tiq->stats.instr_processed += inst_processed;\n\t\t}\n\n\t\ttot_inst_processed += inst_processed;\n\t} while (tot_inst_processed < napi_budget);\n\n\tif (napi_budget && (tot_inst_processed >= napi_budget))\n\t\ttx_done = 0;\n\n\tiq->last_db_time = jiffies;\n\n\tspin_unlock_bh(&iq->lock);\n\n\tspin_unlock(&iq->iq_flush_running_lock);\n\n\treturn tx_done;\n}\n\n \nstatic void __check_db_timeout(struct octeon_device *oct, u64 iq_no)\n{\n\tstruct octeon_instr_queue *iq;\n\tu64 next_time;\n\n\tif (!oct)\n\t\treturn;\n\n\tiq = oct->instr_queue[iq_no];\n\tif (!iq)\n\t\treturn;\n\n\t \n\tif (!atomic_read(&iq->instr_pending))\n\t\treturn;\n\t \n\tnext_time = iq->last_db_time + iq->db_timeout;\n\tif (!time_after(jiffies, (unsigned long)next_time))\n\t\treturn;\n\tiq->last_db_time = jiffies;\n\n\t \n\tocteon_flush_iq(oct, iq, 0);\n\n\tlio_enable_irq(NULL, iq);\n}\n\n \nstatic void check_db_timeout(struct work_struct *work)\n{\n\tstruct cavium_wk *wk = (struct cavium_wk *)work;\n\tstruct octeon_device *oct = (struct octeon_device *)wk->ctxptr;\n\tu64 iq_no = wk->ctxul;\n\tstruct cavium_wq *db_wq = &oct->check_db_wq[iq_no];\n\tu32 delay = 10;\n\n\t__check_db_timeout(oct, iq_no);\n\tqueue_delayed_work(db_wq->wq, &db_wq->wk.work, msecs_to_jiffies(delay));\n}\n\nint\nocteon_send_command(struct octeon_device *oct, u32 iq_no,\n\t\t    u32 force_db, void *cmd, void *buf,\n\t\t    u32 datasize, u32 reqtype)\n{\n\tint xmit_stopped;\n\tstruct iq_post_status st;\n\tstruct octeon_instr_queue *iq = oct->instr_queue[iq_no];\n\n\t \n\tif (iq->allow_soft_cmds)\n\t\tspin_lock_bh(&iq->post_lock);\n\n\tst = __post_command2(iq, cmd);\n\n\tif (st.status != IQ_SEND_FAILED) {\n\t\txmit_stopped = octeon_report_sent_bytes_to_bql(buf, reqtype);\n\t\t__add_to_request_list(iq, st.index, buf, reqtype);\n\t\tINCR_INSTRQUEUE_PKT_COUNT(oct, iq_no, bytes_sent, datasize);\n\t\tINCR_INSTRQUEUE_PKT_COUNT(oct, iq_no, instr_posted, 1);\n\n\t\tif (iq->fill_cnt >= MAX_OCTEON_FILL_COUNT || force_db ||\n\t\t    xmit_stopped || st.status == IQ_SEND_STOP)\n\t\t\tring_doorbell(oct, iq);\n\t} else {\n\t\tINCR_INSTRQUEUE_PKT_COUNT(oct, iq_no, instr_dropped, 1);\n\t}\n\n\tif (iq->allow_soft_cmds)\n\t\tspin_unlock_bh(&iq->post_lock);\n\n\t \n\n\treturn st.status;\n}\nEXPORT_SYMBOL_GPL(octeon_send_command);\n\nvoid\nocteon_prepare_soft_command(struct octeon_device *oct,\n\t\t\t    struct octeon_soft_command *sc,\n\t\t\t    u8 opcode,\n\t\t\t    u8 subcode,\n\t\t\t    u32 irh_ossp,\n\t\t\t    u64 ossp0,\n\t\t\t    u64 ossp1)\n{\n\tstruct octeon_config *oct_cfg;\n\tstruct octeon_instr_ih2 *ih2;\n\tstruct octeon_instr_ih3 *ih3;\n\tstruct octeon_instr_pki_ih3 *pki_ih3;\n\tstruct octeon_instr_irh *irh;\n\tstruct octeon_instr_rdp *rdp;\n\n\tWARN_ON(opcode > 15);\n\tWARN_ON(subcode > 127);\n\n\toct_cfg = octeon_get_conf(oct);\n\n\tif (OCTEON_CN23XX_PF(oct) || OCTEON_CN23XX_VF(oct)) {\n\t\tih3 = (struct octeon_instr_ih3 *)&sc->cmd.cmd3.ih3;\n\n\t\tih3->pkind = oct->instr_queue[sc->iq_no]->txpciq.s.pkind;\n\n\t\tpki_ih3 = (struct octeon_instr_pki_ih3 *)&sc->cmd.cmd3.pki_ih3;\n\n\t\tpki_ih3->w           = 1;\n\t\tpki_ih3->raw         = 1;\n\t\tpki_ih3->utag        = 1;\n\t\tpki_ih3->uqpg        =\n\t\t\toct->instr_queue[sc->iq_no]->txpciq.s.use_qpg;\n\t\tpki_ih3->utt         = 1;\n\t\tpki_ih3->tag     = LIO_CONTROL;\n\t\tpki_ih3->tagtype = ATOMIC_TAG;\n\t\tpki_ih3->qpg         =\n\t\t\toct->instr_queue[sc->iq_no]->txpciq.s.ctrl_qpg;\n\n\t\tpki_ih3->pm          = 0x7;\n\t\tpki_ih3->sl          = 8;\n\n\t\tif (sc->datasize)\n\t\t\tih3->dlengsz = sc->datasize;\n\n\t\tirh            = (struct octeon_instr_irh *)&sc->cmd.cmd3.irh;\n\t\tirh->opcode    = opcode;\n\t\tirh->subcode   = subcode;\n\n\t\t \n\t\tirh->ossp       = irh_ossp;\n\t\tsc->cmd.cmd3.ossp[0] = ossp0;\n\t\tsc->cmd.cmd3.ossp[1] = ossp1;\n\n\t\tif (sc->rdatasize) {\n\t\t\trdp = (struct octeon_instr_rdp *)&sc->cmd.cmd3.rdp;\n\t\t\trdp->pcie_port = oct->pcie_port;\n\t\t\trdp->rlen      = sc->rdatasize;\n\n\t\t\tirh->rflag =  1;\n\t\t\t \n\t\t\t \n\t\t\tih3->fsz    = LIO_SOFTCMDRESP_IH3;\n\t\t} else {\n\t\t\tirh->rflag =  0;\n\t\t\t \n\t\t\t \n\t\t\tih3->fsz    = LIO_PCICMD_O3;\n\t\t}\n\n\t} else {\n\t\tih2          = (struct octeon_instr_ih2 *)&sc->cmd.cmd2.ih2;\n\t\tih2->tagtype = ATOMIC_TAG;\n\t\tih2->tag     = LIO_CONTROL;\n\t\tih2->raw     = 1;\n\t\tih2->grp     = CFG_GET_CTRL_Q_GRP(oct_cfg);\n\n\t\tif (sc->datasize) {\n\t\t\tih2->dlengsz = sc->datasize;\n\t\t\tih2->rs = 1;\n\t\t}\n\n\t\tirh            = (struct octeon_instr_irh *)&sc->cmd.cmd2.irh;\n\t\tirh->opcode    = opcode;\n\t\tirh->subcode   = subcode;\n\n\t\t \n\t\tirh->ossp       = irh_ossp;\n\t\tsc->cmd.cmd2.ossp[0] = ossp0;\n\t\tsc->cmd.cmd2.ossp[1] = ossp1;\n\n\t\tif (sc->rdatasize) {\n\t\t\trdp = (struct octeon_instr_rdp *)&sc->cmd.cmd2.rdp;\n\t\t\trdp->pcie_port = oct->pcie_port;\n\t\t\trdp->rlen      = sc->rdatasize;\n\n\t\t\tirh->rflag =  1;\n\t\t\t \n\t\t\tih2->fsz   = LIO_SOFTCMDRESP_IH2;\n\t\t} else {\n\t\t\tirh->rflag =  0;\n\t\t\t \n\t\t\tih2->fsz   = LIO_PCICMD_O2;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(octeon_prepare_soft_command);\n\nint octeon_send_soft_command(struct octeon_device *oct,\n\t\t\t     struct octeon_soft_command *sc)\n{\n\tstruct octeon_instr_queue *iq;\n\tstruct octeon_instr_ih2 *ih2;\n\tstruct octeon_instr_ih3 *ih3;\n\tstruct octeon_instr_irh *irh;\n\tu32 len;\n\n\tiq = oct->instr_queue[sc->iq_no];\n\tif (!iq->allow_soft_cmds) {\n\t\tdev_err(&oct->pci_dev->dev, \"Soft commands are not allowed on Queue %d\\n\",\n\t\t\tsc->iq_no);\n\t\tINCR_INSTRQUEUE_PKT_COUNT(oct, sc->iq_no, instr_dropped, 1);\n\t\treturn IQ_SEND_FAILED;\n\t}\n\n\tif (OCTEON_CN23XX_PF(oct) || OCTEON_CN23XX_VF(oct)) {\n\t\tih3 =  (struct octeon_instr_ih3 *)&sc->cmd.cmd3.ih3;\n\t\tif (ih3->dlengsz) {\n\t\t\tWARN_ON(!sc->dmadptr);\n\t\t\tsc->cmd.cmd3.dptr = sc->dmadptr;\n\t\t}\n\t\tirh = (struct octeon_instr_irh *)&sc->cmd.cmd3.irh;\n\t\tif (irh->rflag) {\n\t\t\tWARN_ON(!sc->dmarptr);\n\t\t\tWARN_ON(!sc->status_word);\n\t\t\t*sc->status_word = COMPLETION_WORD_INIT;\n\t\t\tsc->cmd.cmd3.rptr = sc->dmarptr;\n\t\t}\n\t\tlen = (u32)ih3->dlengsz;\n\t} else {\n\t\tih2 = (struct octeon_instr_ih2 *)&sc->cmd.cmd2.ih2;\n\t\tif (ih2->dlengsz) {\n\t\t\tWARN_ON(!sc->dmadptr);\n\t\t\tsc->cmd.cmd2.dptr = sc->dmadptr;\n\t\t}\n\t\tirh = (struct octeon_instr_irh *)&sc->cmd.cmd2.irh;\n\t\tif (irh->rflag) {\n\t\t\tWARN_ON(!sc->dmarptr);\n\t\t\tWARN_ON(!sc->status_word);\n\t\t\t*sc->status_word = COMPLETION_WORD_INIT;\n\t\t\tsc->cmd.cmd2.rptr = sc->dmarptr;\n\t\t}\n\t\tlen = (u32)ih2->dlengsz;\n\t}\n\n\tsc->expiry_time = jiffies + msecs_to_jiffies(LIO_SC_MAX_TMO_MS);\n\n\treturn (octeon_send_command(oct, sc->iq_no, 1, &sc->cmd, sc,\n\t\t\t\t    len, REQTYPE_SOFT_COMMAND));\n}\nEXPORT_SYMBOL_GPL(octeon_send_soft_command);\n\nint octeon_setup_sc_buffer_pool(struct octeon_device *oct)\n{\n\tint i;\n\tu64 dma_addr;\n\tstruct octeon_soft_command *sc;\n\n\tINIT_LIST_HEAD(&oct->sc_buf_pool.head);\n\tspin_lock_init(&oct->sc_buf_pool.lock);\n\tatomic_set(&oct->sc_buf_pool.alloc_buf_count, 0);\n\n\tfor (i = 0; i < MAX_SOFT_COMMAND_BUFFERS; i++) {\n\t\tsc = (struct octeon_soft_command *)\n\t\t\tlio_dma_alloc(oct,\n\t\t\t\t      SOFT_COMMAND_BUFFER_SIZE,\n\t\t\t\t\t  (dma_addr_t *)&dma_addr);\n\t\tif (!sc) {\n\t\t\tocteon_free_sc_buffer_pool(oct);\n\t\t\treturn 1;\n\t\t}\n\n\t\tsc->dma_addr = dma_addr;\n\t\tsc->size = SOFT_COMMAND_BUFFER_SIZE;\n\n\t\tlist_add_tail(&sc->node, &oct->sc_buf_pool.head);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(octeon_setup_sc_buffer_pool);\n\nint octeon_free_sc_done_list(struct octeon_device *oct)\n{\n\tstruct octeon_response_list *done_sc_list, *zombie_sc_list;\n\tstruct octeon_soft_command *sc;\n\tstruct list_head *tmp, *tmp2;\n\tspinlock_t *sc_lists_lock;  \n\n\tdone_sc_list = &oct->response_list[OCTEON_DONE_SC_LIST];\n\tzombie_sc_list = &oct->response_list[OCTEON_ZOMBIE_SC_LIST];\n\n\tif (!atomic_read(&done_sc_list->pending_req_count))\n\t\treturn 0;\n\n\tsc_lists_lock = &oct->response_list[OCTEON_ORDERED_SC_LIST].lock;\n\n\tspin_lock_bh(sc_lists_lock);\n\n\tlist_for_each_safe(tmp, tmp2, &done_sc_list->head) {\n\t\tsc = list_entry(tmp, struct octeon_soft_command, node);\n\n\t\tif (READ_ONCE(sc->caller_is_done)) {\n\t\t\tlist_del(&sc->node);\n\t\t\tatomic_dec(&done_sc_list->pending_req_count);\n\n\t\t\tif (*sc->status_word == COMPLETION_WORD_INIT) {\n\t\t\t\t \n\t\t\t\tlist_add_tail(&sc->node, &zombie_sc_list->head);\n\t\t\t\tatomic_inc(&zombie_sc_list->pending_req_count);\n\t\t\t} else {\n\t\t\t\tocteon_free_soft_command(oct, sc);\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_unlock_bh(sc_lists_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(octeon_free_sc_done_list);\n\nint octeon_free_sc_zombie_list(struct octeon_device *oct)\n{\n\tstruct octeon_response_list *zombie_sc_list;\n\tstruct octeon_soft_command *sc;\n\tstruct list_head *tmp, *tmp2;\n\tspinlock_t *sc_lists_lock;  \n\n\tzombie_sc_list = &oct->response_list[OCTEON_ZOMBIE_SC_LIST];\n\tsc_lists_lock = &oct->response_list[OCTEON_ORDERED_SC_LIST].lock;\n\n\tspin_lock_bh(sc_lists_lock);\n\n\tlist_for_each_safe(tmp, tmp2, &zombie_sc_list->head) {\n\t\tlist_del(tmp);\n\t\tatomic_dec(&zombie_sc_list->pending_req_count);\n\t\tsc = list_entry(tmp, struct octeon_soft_command, node);\n\t\tocteon_free_soft_command(oct, sc);\n\t}\n\n\tspin_unlock_bh(sc_lists_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(octeon_free_sc_zombie_list);\n\nint octeon_free_sc_buffer_pool(struct octeon_device *oct)\n{\n\tstruct list_head *tmp, *tmp2;\n\tstruct octeon_soft_command *sc;\n\n\tocteon_free_sc_zombie_list(oct);\n\n\tspin_lock_bh(&oct->sc_buf_pool.lock);\n\n\tlist_for_each_safe(tmp, tmp2, &oct->sc_buf_pool.head) {\n\t\tlist_del(tmp);\n\n\t\tsc = (struct octeon_soft_command *)tmp;\n\n\t\tlio_dma_free(oct, sc->size, sc, sc->dma_addr);\n\t}\n\n\tINIT_LIST_HEAD(&oct->sc_buf_pool.head);\n\n\tspin_unlock_bh(&oct->sc_buf_pool.lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(octeon_free_sc_buffer_pool);\n\nstruct octeon_soft_command *octeon_alloc_soft_command(struct octeon_device *oct,\n\t\t\t\t\t\t      u32 datasize,\n\t\t\t\t\t\t      u32 rdatasize,\n\t\t\t\t\t\t      u32 ctxsize)\n{\n\tu64 dma_addr;\n\tu32 size;\n\tu32 offset = sizeof(struct octeon_soft_command);\n\tstruct octeon_soft_command *sc = NULL;\n\tstruct list_head *tmp;\n\n\tif (!rdatasize)\n\t\trdatasize = 16;\n\n\tWARN_ON((offset + datasize + rdatasize + ctxsize) >\n\t       SOFT_COMMAND_BUFFER_SIZE);\n\n\tspin_lock_bh(&oct->sc_buf_pool.lock);\n\n\tif (list_empty(&oct->sc_buf_pool.head)) {\n\t\tspin_unlock_bh(&oct->sc_buf_pool.lock);\n\t\treturn NULL;\n\t}\n\n\tlist_for_each(tmp, &oct->sc_buf_pool.head)\n\t\tbreak;\n\n\tlist_del(tmp);\n\n\tatomic_inc(&oct->sc_buf_pool.alloc_buf_count);\n\n\tspin_unlock_bh(&oct->sc_buf_pool.lock);\n\n\tsc = (struct octeon_soft_command *)tmp;\n\n\tdma_addr = sc->dma_addr;\n\tsize = sc->size;\n\n\tmemset(sc, 0, sc->size);\n\n\tsc->dma_addr = dma_addr;\n\tsc->size = size;\n\n\tif (ctxsize) {\n\t\tsc->ctxptr = (u8 *)sc + offset;\n\t\tsc->ctxsize = ctxsize;\n\t}\n\n\t \n\toffset = (offset + ctxsize + 127) & 0xffffff80;\n\n\tif (datasize) {\n\t\tsc->virtdptr = (u8 *)sc + offset;\n\t\tsc->dmadptr = dma_addr + offset;\n\t\tsc->datasize = datasize;\n\t}\n\n\t \n\toffset = (offset + datasize + 127) & 0xffffff80;\n\n\tif (rdatasize) {\n\t\tWARN_ON(rdatasize < 16);\n\t\tsc->virtrptr = (u8 *)sc + offset;\n\t\tsc->dmarptr = dma_addr + offset;\n\t\tsc->rdatasize = rdatasize;\n\t\tsc->status_word = (u64 *)((u8 *)(sc->virtrptr) + rdatasize - 8);\n\t}\n\n\treturn sc;\n}\nEXPORT_SYMBOL_GPL(octeon_alloc_soft_command);\n\nvoid octeon_free_soft_command(struct octeon_device *oct,\n\t\t\t      struct octeon_soft_command *sc)\n{\n\tspin_lock_bh(&oct->sc_buf_pool.lock);\n\n\tlist_add_tail(&sc->node, &oct->sc_buf_pool.head);\n\n\tatomic_dec(&oct->sc_buf_pool.alloc_buf_count);\n\n\tspin_unlock_bh(&oct->sc_buf_pool.lock);\n}\nEXPORT_SYMBOL_GPL(octeon_free_soft_command);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}