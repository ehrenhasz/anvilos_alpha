{
  "module_name": "xgene_enet_main.c",
  "hash_id": "6b0e13a35750dbeff654457fc37a58b5e89e621cc0e0236e195e7c3a86de94d8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/apm/xgene/xgene_enet_main.c",
  "human_readable_source": "\n \n\n#include <linux/gpio.h>\n#include \"xgene_enet_main.h\"\n#include \"xgene_enet_hw.h\"\n#include \"xgene_enet_sgmac.h\"\n#include \"xgene_enet_xgmac.h\"\n\n#define RES_ENET_CSR\t0\n#define RES_RING_CSR\t1\n#define RES_RING_CMD\t2\n\nstatic void xgene_enet_init_bufpool(struct xgene_enet_desc_ring *buf_pool)\n{\n\tstruct xgene_enet_raw_desc16 *raw_desc;\n\tint i;\n\n\tif (!buf_pool)\n\t\treturn;\n\n\tfor (i = 0; i < buf_pool->slots; i++) {\n\t\traw_desc = &buf_pool->raw_desc16[i];\n\n\t\t \n\t\traw_desc->m0 = cpu_to_le64(i |\n\t\t\t\tSET_VAL(FPQNUM, buf_pool->dst_ring_num) |\n\t\t\t\tSET_VAL(STASH, 3));\n\t}\n}\n\nstatic u16 xgene_enet_get_data_len(u64 bufdatalen)\n{\n\tu16 hw_len, mask;\n\n\thw_len = GET_VAL(BUFDATALEN, bufdatalen);\n\n\tif (unlikely(hw_len == 0x7800)) {\n\t\treturn 0;\n\t} else if (!(hw_len & BIT(14))) {\n\t\tmask = GENMASK(13, 0);\n\t\treturn (hw_len & mask) ? (hw_len & mask) : SIZE_16K;\n\t} else if (!(hw_len & GENMASK(13, 12))) {\n\t\tmask = GENMASK(11, 0);\n\t\treturn (hw_len & mask) ? (hw_len & mask) : SIZE_4K;\n\t} else {\n\t\tmask = GENMASK(11, 0);\n\t\treturn (hw_len & mask) ? (hw_len & mask) : SIZE_2K;\n\t}\n}\n\nstatic u16 xgene_enet_set_data_len(u32 size)\n{\n\tu16 hw_len;\n\n\thw_len =  (size == SIZE_4K) ? BIT(14) : 0;\n\n\treturn hw_len;\n}\n\nstatic int xgene_enet_refill_pagepool(struct xgene_enet_desc_ring *buf_pool,\n\t\t\t\t      u32 nbuf)\n{\n\tstruct xgene_enet_raw_desc16 *raw_desc;\n\tstruct xgene_enet_pdata *pdata;\n\tstruct net_device *ndev;\n\tdma_addr_t dma_addr;\n\tstruct device *dev;\n\tstruct page *page;\n\tu32 slots, tail;\n\tu16 hw_len;\n\tint i;\n\n\tif (unlikely(!buf_pool))\n\t\treturn 0;\n\n\tndev = buf_pool->ndev;\n\tpdata = netdev_priv(ndev);\n\tdev = ndev_to_dev(ndev);\n\tslots = buf_pool->slots - 1;\n\ttail = buf_pool->tail;\n\n\tfor (i = 0; i < nbuf; i++) {\n\t\traw_desc = &buf_pool->raw_desc16[tail];\n\n\t\tpage = dev_alloc_page();\n\t\tif (unlikely(!page))\n\t\t\treturn -ENOMEM;\n\n\t\tdma_addr = dma_map_page(dev, page, 0,\n\t\t\t\t\tPAGE_SIZE, DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dev, dma_addr))) {\n\t\t\tput_page(page);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\thw_len = xgene_enet_set_data_len(PAGE_SIZE);\n\t\traw_desc->m1 = cpu_to_le64(SET_VAL(DATAADDR, dma_addr) |\n\t\t\t\t\t   SET_VAL(BUFDATALEN, hw_len) |\n\t\t\t\t\t   SET_BIT(COHERENT));\n\n\t\tbuf_pool->frag_page[tail] = page;\n\t\ttail = (tail + 1) & slots;\n\t}\n\n\tpdata->ring_ops->wr_cmd(buf_pool, nbuf);\n\tbuf_pool->tail = tail;\n\n\treturn 0;\n}\n\nstatic int xgene_enet_refill_bufpool(struct xgene_enet_desc_ring *buf_pool,\n\t\t\t\t     u32 nbuf)\n{\n\tstruct sk_buff *skb;\n\tstruct xgene_enet_raw_desc16 *raw_desc;\n\tstruct xgene_enet_pdata *pdata;\n\tstruct net_device *ndev;\n\tstruct device *dev;\n\tdma_addr_t dma_addr;\n\tu32 tail = buf_pool->tail;\n\tu32 slots = buf_pool->slots - 1;\n\tu16 bufdatalen, len;\n\tint i;\n\n\tndev = buf_pool->ndev;\n\tdev = ndev_to_dev(buf_pool->ndev);\n\tpdata = netdev_priv(ndev);\n\n\tbufdatalen = BUF_LEN_CODE_2K | (SKB_BUFFER_SIZE & GENMASK(11, 0));\n\tlen = XGENE_ENET_STD_MTU;\n\n\tfor (i = 0; i < nbuf; i++) {\n\t\traw_desc = &buf_pool->raw_desc16[tail];\n\n\t\tskb = netdev_alloc_skb_ip_align(ndev, len);\n\t\tif (unlikely(!skb))\n\t\t\treturn -ENOMEM;\n\n\t\tdma_addr = dma_map_single(dev, skb->data, len, DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\tnetdev_err(ndev, \"DMA mapping error\\n\");\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tbuf_pool->rx_skb[tail] = skb;\n\n\t\traw_desc->m1 = cpu_to_le64(SET_VAL(DATAADDR, dma_addr) |\n\t\t\t\t\t   SET_VAL(BUFDATALEN, bufdatalen) |\n\t\t\t\t\t   SET_BIT(COHERENT));\n\t\ttail = (tail + 1) & slots;\n\t}\n\n\tpdata->ring_ops->wr_cmd(buf_pool, nbuf);\n\tbuf_pool->tail = tail;\n\n\treturn 0;\n}\n\nstatic u8 xgene_enet_hdr_len(const void *data)\n{\n\tconst struct ethhdr *eth = data;\n\n\treturn (eth->h_proto == htons(ETH_P_8021Q)) ? VLAN_ETH_HLEN : ETH_HLEN;\n}\n\nstatic void xgene_enet_delete_bufpool(struct xgene_enet_desc_ring *buf_pool)\n{\n\tstruct device *dev = ndev_to_dev(buf_pool->ndev);\n\tstruct xgene_enet_raw_desc16 *raw_desc;\n\tdma_addr_t dma_addr;\n\tint i;\n\n\t \n\tfor (i = 0; i < buf_pool->slots; i++) {\n\t\tif (buf_pool->rx_skb[i]) {\n\t\t\tdev_kfree_skb_any(buf_pool->rx_skb[i]);\n\n\t\t\traw_desc = &buf_pool->raw_desc16[i];\n\t\t\tdma_addr = GET_VAL(DATAADDR, le64_to_cpu(raw_desc->m1));\n\t\t\tdma_unmap_single(dev, dma_addr, XGENE_ENET_MAX_MTU,\n\t\t\t\t\t DMA_FROM_DEVICE);\n\t\t}\n\t}\n}\n\nstatic void xgene_enet_delete_pagepool(struct xgene_enet_desc_ring *buf_pool)\n{\n\tstruct device *dev = ndev_to_dev(buf_pool->ndev);\n\tdma_addr_t dma_addr;\n\tstruct page *page;\n\tint i;\n\n\t \n\tfor (i = 0; i < buf_pool->slots; i++) {\n\t\tpage = buf_pool->frag_page[i];\n\t\tif (page) {\n\t\t\tdma_addr = buf_pool->frag_dma_addr[i];\n\t\t\tdma_unmap_page(dev, dma_addr, PAGE_SIZE,\n\t\t\t\t       DMA_FROM_DEVICE);\n\t\t\tput_page(page);\n\t\t}\n\t}\n}\n\nstatic irqreturn_t xgene_enet_rx_irq(const int irq, void *data)\n{\n\tstruct xgene_enet_desc_ring *rx_ring = data;\n\n\tif (napi_schedule_prep(&rx_ring->napi)) {\n\t\tdisable_irq_nosync(irq);\n\t\t__napi_schedule(&rx_ring->napi);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int xgene_enet_tx_completion(struct xgene_enet_desc_ring *cp_ring,\n\t\t\t\t    struct xgene_enet_raw_desc *raw_desc)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(cp_ring->ndev);\n\tstruct sk_buff *skb;\n\tstruct device *dev;\n\tskb_frag_t *frag;\n\tdma_addr_t *frag_dma_addr;\n\tu16 skb_index;\n\tu8 mss_index;\n\tu8 status;\n\tint i;\n\n\tskb_index = GET_VAL(USERINFO, le64_to_cpu(raw_desc->m0));\n\tskb = cp_ring->cp_skb[skb_index];\n\tfrag_dma_addr = &cp_ring->frag_dma_addr[skb_index * MAX_SKB_FRAGS];\n\n\tdev = ndev_to_dev(cp_ring->ndev);\n\tdma_unmap_single(dev, GET_VAL(DATAADDR, le64_to_cpu(raw_desc->m1)),\n\t\t\t skb_headlen(skb),\n\t\t\t DMA_TO_DEVICE);\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\tdma_unmap_page(dev, frag_dma_addr[i], skb_frag_size(frag),\n\t\t\t       DMA_TO_DEVICE);\n\t}\n\n\tif (GET_BIT(ET, le64_to_cpu(raw_desc->m3))) {\n\t\tmss_index = GET_VAL(MSS, le64_to_cpu(raw_desc->m3));\n\t\tspin_lock(&pdata->mss_lock);\n\t\tpdata->mss_refcnt[mss_index]--;\n\t\tspin_unlock(&pdata->mss_lock);\n\t}\n\n\t \n\tstatus = GET_VAL(LERR, le64_to_cpu(raw_desc->m0));\n\tif (unlikely(status > 2)) {\n\t\tcp_ring->tx_dropped++;\n\t\tcp_ring->tx_errors++;\n\t}\n\n\tif (likely(skb)) {\n\t\tdev_kfree_skb_any(skb);\n\t} else {\n\t\tnetdev_err(cp_ring->ndev, \"completion skb is NULL\\n\");\n\t}\n\n\treturn 0;\n}\n\nstatic int xgene_enet_setup_mss(struct net_device *ndev, u32 mss)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tint mss_index = -EBUSY;\n\tint i;\n\n\tspin_lock(&pdata->mss_lock);\n\n\t \n\tfor (i = 0; mss_index < 0 && i < NUM_MSS_REG; i++) {\n\t\tif (pdata->mss[i] == mss) {\n\t\t\tpdata->mss_refcnt[i]++;\n\t\t\tmss_index = i;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; mss_index < 0 && i < NUM_MSS_REG; i++) {\n\t\tif (!pdata->mss_refcnt[i]) {\n\t\t\tpdata->mss_refcnt[i]++;\n\t\t\tpdata->mac_ops->set_mss(pdata, mss, i);\n\t\t\tpdata->mss[i] = mss;\n\t\t\tmss_index = i;\n\t\t}\n\t}\n\n\tspin_unlock(&pdata->mss_lock);\n\n\treturn mss_index;\n}\n\nstatic int xgene_enet_work_msg(struct sk_buff *skb, u64 *hopinfo)\n{\n\tstruct net_device *ndev = skb->dev;\n\tstruct iphdr *iph;\n\tu8 l3hlen = 0, l4hlen = 0;\n\tu8 ethhdr, proto = 0, csum_enable = 0;\n\tu32 hdr_len, mss = 0;\n\tu32 i, len, nr_frags;\n\tint mss_index;\n\n\tethhdr = xgene_enet_hdr_len(skb->data);\n\n\tif (unlikely(skb->protocol != htons(ETH_P_IP)) &&\n\t    unlikely(skb->protocol != htons(ETH_P_8021Q)))\n\t\tgoto out;\n\n\tif (unlikely(!(skb->dev->features & NETIF_F_IP_CSUM)))\n\t\tgoto out;\n\n\tiph = ip_hdr(skb);\n\tif (unlikely(ip_is_fragment(iph)))\n\t\tgoto out;\n\n\tif (likely(iph->protocol == IPPROTO_TCP)) {\n\t\tl4hlen = tcp_hdrlen(skb) >> 2;\n\t\tcsum_enable = 1;\n\t\tproto = TSO_IPPROTO_TCP;\n\t\tif (ndev->features & NETIF_F_TSO) {\n\t\t\thdr_len = ethhdr + ip_hdrlen(skb) + tcp_hdrlen(skb);\n\t\t\tmss = skb_shinfo(skb)->gso_size;\n\n\t\t\tif (skb_is_nonlinear(skb)) {\n\t\t\t\tlen = skb_headlen(skb);\n\t\t\t\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\t\t\t\tfor (i = 0; i < 2 && i < nr_frags; i++)\n\t\t\t\t\tlen += skb_frag_size(\n\t\t\t\t\t\t&skb_shinfo(skb)->frags[i]);\n\n\t\t\t\t \n\t\t\t\tif (unlikely(hdr_len > len)) {\n\t\t\t\t\tif (skb_linearize(skb))\n\t\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (!mss || ((skb->len - hdr_len) <= mss))\n\t\t\t\tgoto out;\n\n\t\t\tmss_index = xgene_enet_setup_mss(ndev, mss);\n\t\t\tif (unlikely(mss_index < 0))\n\t\t\t\treturn -EBUSY;\n\n\t\t\t*hopinfo |= SET_BIT(ET) | SET_VAL(MSS, mss_index);\n\t\t}\n\t} else if (iph->protocol == IPPROTO_UDP) {\n\t\tl4hlen = UDP_HDR_SIZE;\n\t\tcsum_enable = 1;\n\t}\nout:\n\tl3hlen = ip_hdrlen(skb) >> 2;\n\t*hopinfo |= SET_VAL(TCPHDR, l4hlen) |\n\t\t    SET_VAL(IPHDR, l3hlen) |\n\t\t    SET_VAL(ETHHDR, ethhdr) |\n\t\t    SET_VAL(EC, csum_enable) |\n\t\t    SET_VAL(IS, proto) |\n\t\t    SET_BIT(IC) |\n\t\t    SET_BIT(TYPE_ETH_WORK_MESSAGE);\n\n\treturn 0;\n}\n\nstatic u16 xgene_enet_encode_len(u16 len)\n{\n\treturn (len == BUFLEN_16K) ? 0 : len;\n}\n\nstatic void xgene_set_addr_len(__le64 *desc, u32 idx, dma_addr_t addr, u32 len)\n{\n\tdesc[idx ^ 1] = cpu_to_le64(SET_VAL(DATAADDR, addr) |\n\t\t\t\t    SET_VAL(BUFDATALEN, len));\n}\n\nstatic __le64 *xgene_enet_get_exp_bufs(struct xgene_enet_desc_ring *ring)\n{\n\t__le64 *exp_bufs;\n\n\texp_bufs = &ring->exp_bufs[ring->exp_buf_tail * MAX_EXP_BUFFS];\n\tmemset(exp_bufs, 0, sizeof(__le64) * MAX_EXP_BUFFS);\n\tring->exp_buf_tail = (ring->exp_buf_tail + 1) & ((ring->slots / 2) - 1);\n\n\treturn exp_bufs;\n}\n\nstatic dma_addr_t *xgene_get_frag_dma_array(struct xgene_enet_desc_ring *ring)\n{\n\treturn &ring->cp_ring->frag_dma_addr[ring->tail * MAX_SKB_FRAGS];\n}\n\nstatic int xgene_enet_setup_tx_desc(struct xgene_enet_desc_ring *tx_ring,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tstruct device *dev = ndev_to_dev(tx_ring->ndev);\n\tstruct xgene_enet_pdata *pdata = netdev_priv(tx_ring->ndev);\n\tstruct xgene_enet_raw_desc *raw_desc;\n\t__le64 *exp_desc = NULL, *exp_bufs = NULL;\n\tdma_addr_t dma_addr, pbuf_addr, *frag_dma_addr;\n\tskb_frag_t *frag;\n\tu16 tail = tx_ring->tail;\n\tu64 hopinfo = 0;\n\tu32 len, hw_len;\n\tu8 ll = 0, nv = 0, idx = 0;\n\tbool split = false;\n\tu32 size, offset, ell_bytes = 0;\n\tu32 i, fidx, nr_frags, count = 1;\n\tint ret;\n\n\traw_desc = &tx_ring->raw_desc[tail];\n\ttail = (tail + 1) & (tx_ring->slots - 1);\n\tmemset(raw_desc, 0, sizeof(struct xgene_enet_raw_desc));\n\n\tret = xgene_enet_work_msg(skb, &hopinfo);\n\tif (ret)\n\t\treturn ret;\n\n\traw_desc->m3 = cpu_to_le64(SET_VAL(HENQNUM, tx_ring->dst_ring_num) |\n\t\t\t\t   hopinfo);\n\n\tlen = skb_headlen(skb);\n\thw_len = xgene_enet_encode_len(len);\n\n\tdma_addr = dma_map_single(dev, skb->data, len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma_addr)) {\n\t\tnetdev_err(tx_ring->ndev, \"DMA mapping error\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\traw_desc->m1 = cpu_to_le64(SET_VAL(DATAADDR, dma_addr) |\n\t\t\t\t   SET_VAL(BUFDATALEN, hw_len) |\n\t\t\t\t   SET_BIT(COHERENT));\n\n\tif (!skb_is_nonlinear(skb))\n\t\tgoto out;\n\n\t \n\tnv = 1;\n\texp_desc = (void *)&tx_ring->raw_desc[tail];\n\ttail = (tail + 1) & (tx_ring->slots - 1);\n\tmemset(exp_desc, 0, sizeof(struct xgene_enet_raw_desc));\n\n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\tfor (i = nr_frags; i < 4 ; i++)\n\t\texp_desc[i ^ 1] = cpu_to_le64(LAST_BUFFER);\n\n\tfrag_dma_addr = xgene_get_frag_dma_array(tx_ring);\n\n\tfor (i = 0, fidx = 0; split || (fidx < nr_frags); i++) {\n\t\tif (!split) {\n\t\t\tfrag = &skb_shinfo(skb)->frags[fidx];\n\t\t\tsize = skb_frag_size(frag);\n\t\t\toffset = 0;\n\n\t\t\tpbuf_addr = skb_frag_dma_map(dev, frag, 0, size,\n\t\t\t\t\t\t     DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(dev, pbuf_addr))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tfrag_dma_addr[fidx] = pbuf_addr;\n\t\t\tfidx++;\n\n\t\t\tif (size > BUFLEN_16K)\n\t\t\t\tsplit = true;\n\t\t}\n\n\t\tif (size > BUFLEN_16K) {\n\t\t\tlen = BUFLEN_16K;\n\t\t\tsize -= BUFLEN_16K;\n\t\t} else {\n\t\t\tlen = size;\n\t\t\tsplit = false;\n\t\t}\n\n\t\tdma_addr = pbuf_addr + offset;\n\t\thw_len = xgene_enet_encode_len(len);\n\n\t\tswitch (i) {\n\t\tcase 0:\n\t\tcase 1:\n\t\tcase 2:\n\t\t\txgene_set_addr_len(exp_desc, i, dma_addr, hw_len);\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tif (split || (fidx != nr_frags)) {\n\t\t\t\texp_bufs = xgene_enet_get_exp_bufs(tx_ring);\n\t\t\t\txgene_set_addr_len(exp_bufs, idx, dma_addr,\n\t\t\t\t\t\t   hw_len);\n\t\t\t\tidx++;\n\t\t\t\tell_bytes += len;\n\t\t\t} else {\n\t\t\t\txgene_set_addr_len(exp_desc, i, dma_addr,\n\t\t\t\t\t\t   hw_len);\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\txgene_set_addr_len(exp_bufs, idx, dma_addr, hw_len);\n\t\t\tidx++;\n\t\t\tell_bytes += len;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (split)\n\t\t\toffset += BUFLEN_16K;\n\t}\n\tcount++;\n\n\tif (idx) {\n\t\tll = 1;\n\t\tdma_addr = dma_map_single(dev, exp_bufs,\n\t\t\t\t\t  sizeof(u64) * MAX_EXP_BUFFS,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ti = ell_bytes >> LL_BYTES_LSB_LEN;\n\t\texp_desc[2] = cpu_to_le64(SET_VAL(DATAADDR, dma_addr) |\n\t\t\t\t\t  SET_VAL(LL_BYTES_MSB, i) |\n\t\t\t\t\t  SET_VAL(LL_LEN, idx));\n\t\traw_desc->m2 = cpu_to_le64(SET_VAL(LL_BYTES_LSB, ell_bytes));\n\t}\n\nout:\n\traw_desc->m0 = cpu_to_le64(SET_VAL(LL, ll) | SET_VAL(NV, nv) |\n\t\t\t\t   SET_VAL(USERINFO, tx_ring->tail));\n\ttx_ring->cp_ring->cp_skb[tx_ring->tail] = skb;\n\tpdata->tx_level[tx_ring->cp_ring->index] += count;\n\ttx_ring->tail = tail;\n\n\treturn count;\n}\n\nstatic netdev_tx_t xgene_enet_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t struct net_device *ndev)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tstruct xgene_enet_desc_ring *tx_ring;\n\tint index = skb->queue_mapping;\n\tu32 tx_level = pdata->tx_level[index];\n\tint count;\n\n\ttx_ring = pdata->tx_ring[index];\n\tif (tx_level < pdata->txc_level[index])\n\t\ttx_level += ((typeof(pdata->tx_level[index]))~0U);\n\n\tif ((tx_level - pdata->txc_level[index]) > pdata->tx_qcnt_hi) {\n\t\tnetif_stop_subqueue(ndev, index);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (skb_padto(skb, XGENE_MIN_ENET_FRAME_SIZE))\n\t\treturn NETDEV_TX_OK;\n\n\tcount = xgene_enet_setup_tx_desc(tx_ring, skb);\n\tif (count == -EBUSY)\n\t\treturn NETDEV_TX_BUSY;\n\n\tif (count <= 0) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tskb_tx_timestamp(skb);\n\n\ttx_ring->tx_packets++;\n\ttx_ring->tx_bytes += skb->len;\n\n\tpdata->ring_ops->wr_cmd(tx_ring, count);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void xgene_enet_rx_csum(struct sk_buff *skb)\n{\n\tstruct net_device *ndev = skb->dev;\n\tstruct iphdr *iph = ip_hdr(skb);\n\n\tif (!(ndev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\tif (skb->protocol != htons(ETH_P_IP))\n\t\treturn;\n\n\tif (ip_is_fragment(iph))\n\t\treturn;\n\n\tif (iph->protocol != IPPROTO_TCP && iph->protocol != IPPROTO_UDP)\n\t\treturn;\n\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n}\n\nstatic void xgene_enet_free_pagepool(struct xgene_enet_desc_ring *buf_pool,\n\t\t\t\t     struct xgene_enet_raw_desc *raw_desc,\n\t\t\t\t     struct xgene_enet_raw_desc *exp_desc)\n{\n\t__le64 *desc = (void *)exp_desc;\n\tdma_addr_t dma_addr;\n\tstruct device *dev;\n\tstruct page *page;\n\tu16 slots, head;\n\tu32 frag_size;\n\tint i;\n\n\tif (!buf_pool || !raw_desc || !exp_desc ||\n\t    (!GET_VAL(NV, le64_to_cpu(raw_desc->m0))))\n\t\treturn;\n\n\tdev = ndev_to_dev(buf_pool->ndev);\n\tslots = buf_pool->slots - 1;\n\thead = buf_pool->head;\n\n\tfor (i = 0; i < 4; i++) {\n\t\tfrag_size = xgene_enet_get_data_len(le64_to_cpu(desc[i ^ 1]));\n\t\tif (!frag_size)\n\t\t\tbreak;\n\n\t\tdma_addr = GET_VAL(DATAADDR, le64_to_cpu(desc[i ^ 1]));\n\t\tdma_unmap_page(dev, dma_addr, PAGE_SIZE, DMA_FROM_DEVICE);\n\n\t\tpage = buf_pool->frag_page[head];\n\t\tput_page(page);\n\n\t\tbuf_pool->frag_page[head] = NULL;\n\t\thead = (head + 1) & slots;\n\t}\n\tbuf_pool->head = head;\n}\n\n \nstatic bool xgene_enet_errata_10GE_10(struct sk_buff *skb, u32 len, u8 status)\n{\n\tif (status == INGRESS_CRC &&\n\t    len >= (ETHER_STD_PACKET + 1) &&\n\t    len <= (ETHER_STD_PACKET + 4) &&\n\t    skb->protocol == htons(ETH_P_8021Q))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool xgene_enet_errata_10GE_8(struct sk_buff *skb, u32 len, u8 status)\n{\n\tif (status == INGRESS_PKT_LEN && len == ETHER_MIN_PACKET) {\n\t\tif (ntohs(eth_hdr(skb)->h_proto) < 46)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int xgene_enet_rx_frame(struct xgene_enet_desc_ring *rx_ring,\n\t\t\t       struct xgene_enet_raw_desc *raw_desc,\n\t\t\t       struct xgene_enet_raw_desc *exp_desc)\n{\n\tstruct xgene_enet_desc_ring *buf_pool, *page_pool;\n\tu32 datalen, frag_size, skb_index;\n\tstruct xgene_enet_pdata *pdata;\n\tstruct net_device *ndev;\n\tdma_addr_t dma_addr;\n\tstruct sk_buff *skb;\n\tstruct device *dev;\n\tstruct page *page;\n\tu16 slots, head;\n\tint i, ret = 0;\n\t__le64 *desc;\n\tu8 status;\n\tbool nv;\n\n\tndev = rx_ring->ndev;\n\tpdata = netdev_priv(ndev);\n\tdev = ndev_to_dev(rx_ring->ndev);\n\tbuf_pool = rx_ring->buf_pool;\n\tpage_pool = rx_ring->page_pool;\n\n\tdma_unmap_single(dev, GET_VAL(DATAADDR, le64_to_cpu(raw_desc->m1)),\n\t\t\t XGENE_ENET_STD_MTU, DMA_FROM_DEVICE);\n\tskb_index = GET_VAL(USERINFO, le64_to_cpu(raw_desc->m0));\n\tskb = buf_pool->rx_skb[skb_index];\n\tbuf_pool->rx_skb[skb_index] = NULL;\n\n\tdatalen = xgene_enet_get_data_len(le64_to_cpu(raw_desc->m1));\n\n\t \n\tnv = GET_VAL(NV, le64_to_cpu(raw_desc->m0));\n\tif (!nv)\n\t\tdatalen -= 4;\n\n\tskb_put(skb, datalen);\n\tprefetch(skb->data - NET_IP_ALIGN);\n\tskb->protocol = eth_type_trans(skb, ndev);\n\n\t \n\tstatus = (GET_VAL(ELERR, le64_to_cpu(raw_desc->m0)) << LERR_LEN) |\n\t\t  GET_VAL(LERR, le64_to_cpu(raw_desc->m0));\n\tif (unlikely(status)) {\n\t\tif (xgene_enet_errata_10GE_8(skb, datalen, status)) {\n\t\t\tpdata->false_rflr++;\n\t\t} else if (xgene_enet_errata_10GE_10(skb, datalen, status)) {\n\t\t\tpdata->vlan_rjbr++;\n\t\t} else {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\txgene_enet_free_pagepool(page_pool, raw_desc, exp_desc);\n\t\t\txgene_enet_parse_error(rx_ring, status);\n\t\t\trx_ring->rx_dropped++;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!nv)\n\t\tgoto skip_jumbo;\n\n\tslots = page_pool->slots - 1;\n\thead = page_pool->head;\n\tdesc = (void *)exp_desc;\n\n\tfor (i = 0; i < 4; i++) {\n\t\tfrag_size = xgene_enet_get_data_len(le64_to_cpu(desc[i ^ 1]));\n\t\tif (!frag_size)\n\t\t\tbreak;\n\n\t\tdma_addr = GET_VAL(DATAADDR, le64_to_cpu(desc[i ^ 1]));\n\t\tdma_unmap_page(dev, dma_addr, PAGE_SIZE, DMA_FROM_DEVICE);\n\n\t\tpage = page_pool->frag_page[head];\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page, 0,\n\t\t\t\tfrag_size, PAGE_SIZE);\n\n\t\tdatalen += frag_size;\n\n\t\tpage_pool->frag_page[head] = NULL;\n\t\thead = (head + 1) & slots;\n\t}\n\n\tpage_pool->head = head;\n\trx_ring->npagepool -= skb_shinfo(skb)->nr_frags;\n\nskip_jumbo:\n\tskb_checksum_none_assert(skb);\n\txgene_enet_rx_csum(skb);\n\n\trx_ring->rx_packets++;\n\trx_ring->rx_bytes += datalen;\n\tnapi_gro_receive(&rx_ring->napi, skb);\n\nout:\n\tif (rx_ring->npagepool <= 0) {\n\t\tret = xgene_enet_refill_pagepool(page_pool, NUM_NXTBUFPOOL);\n\t\trx_ring->npagepool = NUM_NXTBUFPOOL;\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (--rx_ring->nbufpool == 0) {\n\t\tret = xgene_enet_refill_bufpool(buf_pool, NUM_BUFPOOL);\n\t\trx_ring->nbufpool = NUM_BUFPOOL;\n\t}\n\n\treturn ret;\n}\n\nstatic bool is_rx_desc(struct xgene_enet_raw_desc *raw_desc)\n{\n\treturn GET_VAL(FPQNUM, le64_to_cpu(raw_desc->m0)) ? true : false;\n}\n\nstatic int xgene_enet_process_ring(struct xgene_enet_desc_ring *ring,\n\t\t\t\t   int budget)\n{\n\tstruct net_device *ndev = ring->ndev;\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tstruct xgene_enet_raw_desc *raw_desc, *exp_desc;\n\tu16 head = ring->head;\n\tu16 slots = ring->slots - 1;\n\tint ret, desc_count, count = 0, processed = 0;\n\tbool is_completion;\n\n\tdo {\n\t\traw_desc = &ring->raw_desc[head];\n\t\tdesc_count = 0;\n\t\tis_completion = false;\n\t\texp_desc = NULL;\n\t\tif (unlikely(xgene_enet_is_desc_slot_empty(raw_desc)))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\t\tif (GET_BIT(NV, le64_to_cpu(raw_desc->m0))) {\n\t\t\thead = (head + 1) & slots;\n\t\t\texp_desc = &ring->raw_desc[head];\n\n\t\t\tif (unlikely(xgene_enet_is_desc_slot_empty(exp_desc))) {\n\t\t\t\thead = (head - 1) & slots;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdma_rmb();\n\t\t\tcount++;\n\t\t\tdesc_count++;\n\t\t}\n\t\tif (is_rx_desc(raw_desc)) {\n\t\t\tret = xgene_enet_rx_frame(ring, raw_desc, exp_desc);\n\t\t} else {\n\t\t\tret = xgene_enet_tx_completion(ring, raw_desc);\n\t\t\tis_completion = true;\n\t\t}\n\t\txgene_enet_mark_desc_slot_empty(raw_desc);\n\t\tif (exp_desc)\n\t\t\txgene_enet_mark_desc_slot_empty(exp_desc);\n\n\t\thead = (head + 1) & slots;\n\t\tcount++;\n\t\tdesc_count++;\n\t\tprocessed++;\n\t\tif (is_completion)\n\t\t\tpdata->txc_level[ring->index] += desc_count;\n\n\t\tif (ret)\n\t\t\tbreak;\n\t} while (--budget);\n\n\tif (likely(count)) {\n\t\tpdata->ring_ops->wr_cmd(ring, -count);\n\t\tring->head = head;\n\n\t\tif (__netif_subqueue_stopped(ndev, ring->index))\n\t\t\tnetif_start_subqueue(ndev, ring->index);\n\t}\n\n\treturn processed;\n}\n\nstatic int xgene_enet_napi(struct napi_struct *napi, const int budget)\n{\n\tstruct xgene_enet_desc_ring *ring;\n\tint processed;\n\n\tring = container_of(napi, struct xgene_enet_desc_ring, napi);\n\tprocessed = xgene_enet_process_ring(ring, budget);\n\n\tif (processed != budget) {\n\t\tnapi_complete_done(napi, processed);\n\t\tenable_irq(ring->irq);\n\t}\n\n\treturn processed;\n}\n\nstatic void xgene_enet_timeout(struct net_device *ndev, unsigned int txqueue)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tstruct netdev_queue *txq;\n\tint i;\n\n\tpdata->mac_ops->reset(pdata);\n\n\tfor (i = 0; i < pdata->txq_cnt; i++) {\n\t\ttxq = netdev_get_tx_queue(ndev, i);\n\t\ttxq_trans_cond_update(txq);\n\t\tnetif_tx_start_queue(txq);\n\t}\n}\n\nstatic void xgene_enet_set_irq_name(struct net_device *ndev)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tstruct xgene_enet_desc_ring *ring;\n\tint i;\n\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tring = pdata->rx_ring[i];\n\t\tif (!pdata->cq_cnt) {\n\t\t\tsnprintf(ring->irq_name, IRQ_ID_SIZE, \"%s-rx-txc\",\n\t\t\t\t ndev->name);\n\t\t} else {\n\t\t\tsnprintf(ring->irq_name, IRQ_ID_SIZE, \"%s-rx-%d\",\n\t\t\t\t ndev->name, i);\n\t\t}\n\t}\n\n\tfor (i = 0; i < pdata->cq_cnt; i++) {\n\t\tring = pdata->tx_ring[i]->cp_ring;\n\t\tsnprintf(ring->irq_name, IRQ_ID_SIZE, \"%s-txc-%d\",\n\t\t\t ndev->name, i);\n\t}\n}\n\nstatic int xgene_enet_register_irq(struct net_device *ndev)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tstruct device *dev = ndev_to_dev(ndev);\n\tstruct xgene_enet_desc_ring *ring;\n\tint ret = 0, i;\n\n\txgene_enet_set_irq_name(ndev);\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tring = pdata->rx_ring[i];\n\t\tirq_set_status_flags(ring->irq, IRQ_DISABLE_UNLAZY);\n\t\tret = devm_request_irq(dev, ring->irq, xgene_enet_rx_irq,\n\t\t\t\t       0, ring->irq_name, ring);\n\t\tif (ret) {\n\t\t\tnetdev_err(ndev, \"Failed to request irq %s\\n\",\n\t\t\t\t   ring->irq_name);\n\t\t}\n\t}\n\n\tfor (i = 0; i < pdata->cq_cnt; i++) {\n\t\tring = pdata->tx_ring[i]->cp_ring;\n\t\tirq_set_status_flags(ring->irq, IRQ_DISABLE_UNLAZY);\n\t\tret = devm_request_irq(dev, ring->irq, xgene_enet_rx_irq,\n\t\t\t\t       0, ring->irq_name, ring);\n\t\tif (ret) {\n\t\t\tnetdev_err(ndev, \"Failed to request irq %s\\n\",\n\t\t\t\t   ring->irq_name);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void xgene_enet_free_irq(struct net_device *ndev)\n{\n\tstruct xgene_enet_pdata *pdata;\n\tstruct xgene_enet_desc_ring *ring;\n\tstruct device *dev;\n\tint i;\n\n\tpdata = netdev_priv(ndev);\n\tdev = ndev_to_dev(ndev);\n\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tring = pdata->rx_ring[i];\n\t\tirq_clear_status_flags(ring->irq, IRQ_DISABLE_UNLAZY);\n\t\tdevm_free_irq(dev, ring->irq, ring);\n\t}\n\n\tfor (i = 0; i < pdata->cq_cnt; i++) {\n\t\tring = pdata->tx_ring[i]->cp_ring;\n\t\tirq_clear_status_flags(ring->irq, IRQ_DISABLE_UNLAZY);\n\t\tdevm_free_irq(dev, ring->irq, ring);\n\t}\n}\n\nstatic void xgene_enet_napi_enable(struct xgene_enet_pdata *pdata)\n{\n\tstruct napi_struct *napi;\n\tint i;\n\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tnapi = &pdata->rx_ring[i]->napi;\n\t\tnapi_enable(napi);\n\t}\n\n\tfor (i = 0; i < pdata->cq_cnt; i++) {\n\t\tnapi = &pdata->tx_ring[i]->cp_ring->napi;\n\t\tnapi_enable(napi);\n\t}\n}\n\nstatic void xgene_enet_napi_disable(struct xgene_enet_pdata *pdata)\n{\n\tstruct napi_struct *napi;\n\tint i;\n\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tnapi = &pdata->rx_ring[i]->napi;\n\t\tnapi_disable(napi);\n\t}\n\n\tfor (i = 0; i < pdata->cq_cnt; i++) {\n\t\tnapi = &pdata->tx_ring[i]->cp_ring->napi;\n\t\tnapi_disable(napi);\n\t}\n}\n\nstatic int xgene_enet_open(struct net_device *ndev)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tconst struct xgene_mac_ops *mac_ops = pdata->mac_ops;\n\tint ret;\n\n\tret = netif_set_real_num_tx_queues(ndev, pdata->txq_cnt);\n\tif (ret)\n\t\treturn ret;\n\n\tret = netif_set_real_num_rx_queues(ndev, pdata->rxq_cnt);\n\tif (ret)\n\t\treturn ret;\n\n\txgene_enet_napi_enable(pdata);\n\tret = xgene_enet_register_irq(ndev);\n\tif (ret) {\n\t\txgene_enet_napi_disable(pdata);\n\t\treturn ret;\n\t}\n\n\tif (ndev->phydev) {\n\t\tphy_start(ndev->phydev);\n\t} else {\n\t\tschedule_delayed_work(&pdata->link_work, PHY_POLL_LINK_OFF);\n\t\tnetif_carrier_off(ndev);\n\t}\n\n\tmac_ops->tx_enable(pdata);\n\tmac_ops->rx_enable(pdata);\n\tnetif_tx_start_all_queues(ndev);\n\n\treturn ret;\n}\n\nstatic int xgene_enet_close(struct net_device *ndev)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tconst struct xgene_mac_ops *mac_ops = pdata->mac_ops;\n\tint i;\n\n\tnetif_tx_stop_all_queues(ndev);\n\tmac_ops->tx_disable(pdata);\n\tmac_ops->rx_disable(pdata);\n\n\tif (ndev->phydev)\n\t\tphy_stop(ndev->phydev);\n\telse\n\t\tcancel_delayed_work_sync(&pdata->link_work);\n\n\txgene_enet_free_irq(ndev);\n\txgene_enet_napi_disable(pdata);\n\tfor (i = 0; i < pdata->rxq_cnt; i++)\n\t\txgene_enet_process_ring(pdata->rx_ring[i], -1);\n\n\treturn 0;\n}\nstatic void xgene_enet_delete_ring(struct xgene_enet_desc_ring *ring)\n{\n\tstruct xgene_enet_pdata *pdata;\n\tstruct device *dev;\n\n\tpdata = netdev_priv(ring->ndev);\n\tdev = ndev_to_dev(ring->ndev);\n\n\tpdata->ring_ops->clear(ring);\n\tdmam_free_coherent(dev, ring->size, ring->desc_addr, ring->dma);\n}\n\nstatic void xgene_enet_delete_desc_rings(struct xgene_enet_pdata *pdata)\n{\n\tstruct xgene_enet_desc_ring *buf_pool, *page_pool;\n\tstruct xgene_enet_desc_ring *ring;\n\tint i;\n\n\tfor (i = 0; i < pdata->txq_cnt; i++) {\n\t\tring = pdata->tx_ring[i];\n\t\tif (ring) {\n\t\t\txgene_enet_delete_ring(ring);\n\t\t\tpdata->port_ops->clear(pdata, ring);\n\t\t\tif (pdata->cq_cnt)\n\t\t\t\txgene_enet_delete_ring(ring->cp_ring);\n\t\t\tpdata->tx_ring[i] = NULL;\n\t\t}\n\n\t}\n\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tring = pdata->rx_ring[i];\n\t\tif (ring) {\n\t\t\tpage_pool = ring->page_pool;\n\t\t\tif (page_pool) {\n\t\t\t\txgene_enet_delete_pagepool(page_pool);\n\t\t\t\txgene_enet_delete_ring(page_pool);\n\t\t\t\tpdata->port_ops->clear(pdata, page_pool);\n\t\t\t}\n\n\t\t\tbuf_pool = ring->buf_pool;\n\t\t\txgene_enet_delete_bufpool(buf_pool);\n\t\t\txgene_enet_delete_ring(buf_pool);\n\t\t\tpdata->port_ops->clear(pdata, buf_pool);\n\n\t\t\txgene_enet_delete_ring(ring);\n\t\t\tpdata->rx_ring[i] = NULL;\n\t\t}\n\n\t}\n}\n\nstatic int xgene_enet_get_ring_size(struct device *dev,\n\t\t\t\t    enum xgene_enet_ring_cfgsize cfgsize)\n{\n\tint size = -EINVAL;\n\n\tswitch (cfgsize) {\n\tcase RING_CFGSIZE_512B:\n\t\tsize = 0x200;\n\t\tbreak;\n\tcase RING_CFGSIZE_2KB:\n\t\tsize = 0x800;\n\t\tbreak;\n\tcase RING_CFGSIZE_16KB:\n\t\tsize = 0x4000;\n\t\tbreak;\n\tcase RING_CFGSIZE_64KB:\n\t\tsize = 0x10000;\n\t\tbreak;\n\tcase RING_CFGSIZE_512KB:\n\t\tsize = 0x80000;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev, \"Unsupported cfg ring size %d\\n\", cfgsize);\n\t\tbreak;\n\t}\n\n\treturn size;\n}\n\nstatic void xgene_enet_free_desc_ring(struct xgene_enet_desc_ring *ring)\n{\n\tstruct xgene_enet_pdata *pdata;\n\tstruct device *dev;\n\n\tif (!ring)\n\t\treturn;\n\n\tdev = ndev_to_dev(ring->ndev);\n\tpdata = netdev_priv(ring->ndev);\n\n\tif (ring->desc_addr) {\n\t\tpdata->ring_ops->clear(ring);\n\t\tdmam_free_coherent(dev, ring->size, ring->desc_addr, ring->dma);\n\t}\n\tdevm_kfree(dev, ring);\n}\n\nstatic void xgene_enet_free_desc_rings(struct xgene_enet_pdata *pdata)\n{\n\tstruct xgene_enet_desc_ring *page_pool;\n\tstruct device *dev = &pdata->pdev->dev;\n\tstruct xgene_enet_desc_ring *ring;\n\tvoid *p;\n\tint i;\n\n\tfor (i = 0; i < pdata->txq_cnt; i++) {\n\t\tring = pdata->tx_ring[i];\n\t\tif (ring) {\n\t\t\tif (ring->cp_ring && ring->cp_ring->cp_skb)\n\t\t\t\tdevm_kfree(dev, ring->cp_ring->cp_skb);\n\n\t\t\tif (ring->cp_ring && pdata->cq_cnt)\n\t\t\t\txgene_enet_free_desc_ring(ring->cp_ring);\n\n\t\t\txgene_enet_free_desc_ring(ring);\n\t\t}\n\n\t}\n\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tring = pdata->rx_ring[i];\n\t\tif (ring) {\n\t\t\tif (ring->buf_pool) {\n\t\t\t\tif (ring->buf_pool->rx_skb)\n\t\t\t\t\tdevm_kfree(dev, ring->buf_pool->rx_skb);\n\n\t\t\t\txgene_enet_free_desc_ring(ring->buf_pool);\n\t\t\t}\n\n\t\t\tpage_pool = ring->page_pool;\n\t\t\tif (page_pool) {\n\t\t\t\tp = page_pool->frag_page;\n\t\t\t\tif (p)\n\t\t\t\t\tdevm_kfree(dev, p);\n\n\t\t\t\tp = page_pool->frag_dma_addr;\n\t\t\t\tif (p)\n\t\t\t\t\tdevm_kfree(dev, p);\n\t\t\t}\n\n\t\t\txgene_enet_free_desc_ring(ring);\n\t\t}\n\t}\n}\n\nstatic bool is_irq_mbox_required(struct xgene_enet_pdata *pdata,\n\t\t\t\t struct xgene_enet_desc_ring *ring)\n{\n\tif ((pdata->enet_id == XGENE_ENET2) &&\n\t    (xgene_enet_ring_owner(ring->id) == RING_OWNER_CPU)) {\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void __iomem *xgene_enet_ring_cmd_base(struct xgene_enet_pdata *pdata,\n\t\t\t\t\t      struct xgene_enet_desc_ring *ring)\n{\n\tu8 num_ring_id_shift = pdata->ring_ops->num_ring_id_shift;\n\n\treturn pdata->ring_cmd_addr + (ring->num << num_ring_id_shift);\n}\n\nstatic struct xgene_enet_desc_ring *xgene_enet_create_desc_ring(\n\t\t\tstruct net_device *ndev, u32 ring_num,\n\t\t\tenum xgene_enet_ring_cfgsize cfgsize, u32 ring_id)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tstruct device *dev = ndev_to_dev(ndev);\n\tstruct xgene_enet_desc_ring *ring;\n\tvoid *irq_mbox_addr;\n\tint size;\n\n\tsize = xgene_enet_get_ring_size(dev, cfgsize);\n\tif (size < 0)\n\t\treturn NULL;\n\n\tring = devm_kzalloc(dev, sizeof(struct xgene_enet_desc_ring),\n\t\t\t    GFP_KERNEL);\n\tif (!ring)\n\t\treturn NULL;\n\n\tring->ndev = ndev;\n\tring->num = ring_num;\n\tring->cfgsize = cfgsize;\n\tring->id = ring_id;\n\n\tring->desc_addr = dmam_alloc_coherent(dev, size, &ring->dma,\n\t\t\t\t\t      GFP_KERNEL | __GFP_ZERO);\n\tif (!ring->desc_addr) {\n\t\tdevm_kfree(dev, ring);\n\t\treturn NULL;\n\t}\n\tring->size = size;\n\n\tif (is_irq_mbox_required(pdata, ring)) {\n\t\tirq_mbox_addr = dmam_alloc_coherent(dev, INTR_MBOX_SIZE,\n\t\t\t\t\t\t    &ring->irq_mbox_dma,\n\t\t\t\t\t\t    GFP_KERNEL | __GFP_ZERO);\n\t\tif (!irq_mbox_addr) {\n\t\t\tdmam_free_coherent(dev, size, ring->desc_addr,\n\t\t\t\t\t   ring->dma);\n\t\t\tdevm_kfree(dev, ring);\n\t\t\treturn NULL;\n\t\t}\n\t\tring->irq_mbox_addr = irq_mbox_addr;\n\t}\n\n\tring->cmd_base = xgene_enet_ring_cmd_base(pdata, ring);\n\tring->cmd = ring->cmd_base + INC_DEC_CMD_ADDR;\n\tring = pdata->ring_ops->setup(ring);\n\tnetdev_dbg(ndev, \"ring info: num=%d  size=%d  id=%d  slots=%d\\n\",\n\t\t   ring->num, ring->size, ring->id, ring->slots);\n\n\treturn ring;\n}\n\nstatic u16 xgene_enet_get_ring_id(enum xgene_ring_owner owner, u8 bufnum)\n{\n\treturn (owner << 6) | (bufnum & GENMASK(5, 0));\n}\n\nstatic enum xgene_ring_owner xgene_derive_ring_owner(struct xgene_enet_pdata *p)\n{\n\tenum xgene_ring_owner owner;\n\n\tif (p->enet_id == XGENE_ENET1) {\n\t\tswitch (p->phy_mode) {\n\t\tcase PHY_INTERFACE_MODE_SGMII:\n\t\t\towner = RING_OWNER_ETH0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\towner = (!p->port_id) ? RING_OWNER_ETH0 :\n\t\t\t\t\t\tRING_OWNER_ETH1;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\towner = (!p->port_id) ? RING_OWNER_ETH0 : RING_OWNER_ETH1;\n\t}\n\n\treturn owner;\n}\n\nstatic u8 xgene_start_cpu_bufnum(struct xgene_enet_pdata *pdata)\n{\n\tstruct device *dev = &pdata->pdev->dev;\n\tu32 cpu_bufnum;\n\tint ret;\n\n\tret = device_property_read_u32(dev, \"channel\", &cpu_bufnum);\n\n\treturn (!ret) ? cpu_bufnum : pdata->cpu_bufnum;\n}\n\nstatic int xgene_enet_create_desc_rings(struct net_device *ndev)\n{\n\tstruct xgene_enet_desc_ring *rx_ring, *tx_ring, *cp_ring;\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tstruct xgene_enet_desc_ring *page_pool = NULL;\n\tstruct xgene_enet_desc_ring *buf_pool = NULL;\n\tstruct device *dev = ndev_to_dev(ndev);\n\tu8 eth_bufnum = pdata->eth_bufnum;\n\tu8 bp_bufnum = pdata->bp_bufnum;\n\tu16 ring_num = pdata->ring_num;\n\tenum xgene_ring_owner owner;\n\tdma_addr_t dma_exp_bufs;\n\tu16 ring_id, slots;\n\t__le64 *exp_bufs;\n\tint i, ret, size;\n\tu8 cpu_bufnum;\n\n\tcpu_bufnum = xgene_start_cpu_bufnum(pdata);\n\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\t \n\t\towner = xgene_derive_ring_owner(pdata);\n\t\tring_id = xgene_enet_get_ring_id(RING_OWNER_CPU, cpu_bufnum++);\n\t\trx_ring = xgene_enet_create_desc_ring(ndev, ring_num++,\n\t\t\t\t\t\t      RING_CFGSIZE_16KB,\n\t\t\t\t\t\t      ring_id);\n\t\tif (!rx_ring) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\t \n\t\towner = xgene_derive_ring_owner(pdata);\n\t\tring_id = xgene_enet_get_ring_id(owner, bp_bufnum++);\n\t\tbuf_pool = xgene_enet_create_desc_ring(ndev, ring_num++,\n\t\t\t\t\t\t       RING_CFGSIZE_16KB,\n\t\t\t\t\t\t       ring_id);\n\t\tif (!buf_pool) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\trx_ring->nbufpool = NUM_BUFPOOL;\n\t\trx_ring->npagepool = NUM_NXTBUFPOOL;\n\t\trx_ring->irq = pdata->irqs[i];\n\t\tbuf_pool->rx_skb = devm_kcalloc(dev, buf_pool->slots,\n\t\t\t\t\t\tsizeof(struct sk_buff *),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!buf_pool->rx_skb) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tbuf_pool->dst_ring_num = xgene_enet_dst_ring_num(buf_pool);\n\t\trx_ring->buf_pool = buf_pool;\n\t\tpdata->rx_ring[i] = rx_ring;\n\n\t\tif ((pdata->enet_id == XGENE_ENET1 &&  pdata->rxq_cnt > 4) ||\n\t\t    (pdata->enet_id == XGENE_ENET2 &&  pdata->rxq_cnt > 16)) {\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\towner = xgene_derive_ring_owner(pdata);\n\t\tring_id = xgene_enet_get_ring_id(owner, bp_bufnum++);\n\t\tpage_pool = xgene_enet_create_desc_ring(ndev, ring_num++,\n\t\t\t\t\t\t\tRING_CFGSIZE_16KB,\n\t\t\t\t\t\t\tring_id);\n\t\tif (!page_pool) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tslots = page_pool->slots;\n\t\tpage_pool->frag_page = devm_kcalloc(dev, slots,\n\t\t\t\t\t\t    sizeof(struct page *),\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!page_pool->frag_page) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tpage_pool->frag_dma_addr = devm_kcalloc(dev, slots,\n\t\t\t\t\t\t\tsizeof(dma_addr_t),\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!page_pool->frag_dma_addr) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tpage_pool->dst_ring_num = xgene_enet_dst_ring_num(page_pool);\n\t\trx_ring->page_pool = page_pool;\n\t}\n\n\tfor (i = 0; i < pdata->txq_cnt; i++) {\n\t\t \n\t\towner = xgene_derive_ring_owner(pdata);\n\t\tring_id = xgene_enet_get_ring_id(owner, eth_bufnum++);\n\t\ttx_ring = xgene_enet_create_desc_ring(ndev, ring_num++,\n\t\t\t\t\t\t      RING_CFGSIZE_16KB,\n\t\t\t\t\t\t      ring_id);\n\t\tif (!tx_ring) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tsize = (tx_ring->slots / 2) * sizeof(__le64) * MAX_EXP_BUFFS;\n\t\texp_bufs = dmam_alloc_coherent(dev, size, &dma_exp_bufs,\n\t\t\t\t\t       GFP_KERNEL | __GFP_ZERO);\n\t\tif (!exp_bufs) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\t\ttx_ring->exp_bufs = exp_bufs;\n\n\t\tpdata->tx_ring[i] = tx_ring;\n\n\t\tif (!pdata->cq_cnt) {\n\t\t\tcp_ring = pdata->rx_ring[i];\n\t\t} else {\n\t\t\t \n\t\t\tring_id = xgene_enet_get_ring_id(RING_OWNER_CPU,\n\t\t\t\t\t\t\t cpu_bufnum++);\n\t\t\tcp_ring = xgene_enet_create_desc_ring(ndev, ring_num++,\n\t\t\t\t\t\t\t      RING_CFGSIZE_16KB,\n\t\t\t\t\t\t\t      ring_id);\n\t\t\tif (!cp_ring) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tcp_ring->irq = pdata->irqs[pdata->rxq_cnt + i];\n\t\t\tcp_ring->index = i;\n\t\t}\n\n\t\tcp_ring->cp_skb = devm_kcalloc(dev, tx_ring->slots,\n\t\t\t\t\t       sizeof(struct sk_buff *),\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!cp_ring->cp_skb) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tsize = sizeof(dma_addr_t) * MAX_SKB_FRAGS;\n\t\tcp_ring->frag_dma_addr = devm_kcalloc(dev, tx_ring->slots,\n\t\t\t\t\t\t      size, GFP_KERNEL);\n\t\tif (!cp_ring->frag_dma_addr) {\n\t\t\tdevm_kfree(dev, cp_ring->cp_skb);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\ttx_ring->cp_ring = cp_ring;\n\t\ttx_ring->dst_ring_num = xgene_enet_dst_ring_num(cp_ring);\n\t}\n\n\tif (pdata->ring_ops->coalesce)\n\t\tpdata->ring_ops->coalesce(pdata->tx_ring[0]);\n\tpdata->tx_qcnt_hi = pdata->tx_ring[0]->slots - 128;\n\n\treturn 0;\n\nerr:\n\txgene_enet_free_desc_rings(pdata);\n\treturn ret;\n}\n\nstatic void xgene_enet_get_stats64(\n\t\t\tstruct net_device *ndev,\n\t\t\tstruct rtnl_link_stats64 *stats)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tstruct xgene_enet_desc_ring *ring;\n\tint i;\n\n\tfor (i = 0; i < pdata->txq_cnt; i++) {\n\t\tring = pdata->tx_ring[i];\n\t\tif (ring) {\n\t\t\tstats->tx_packets += ring->tx_packets;\n\t\t\tstats->tx_bytes += ring->tx_bytes;\n\t\t\tstats->tx_dropped += ring->tx_dropped;\n\t\t\tstats->tx_errors += ring->tx_errors;\n\t\t}\n\t}\n\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tring = pdata->rx_ring[i];\n\t\tif (ring) {\n\t\t\tstats->rx_packets += ring->rx_packets;\n\t\t\tstats->rx_bytes += ring->rx_bytes;\n\t\t\tstats->rx_dropped += ring->rx_dropped;\n\t\t\tstats->rx_errors += ring->rx_errors +\n\t\t\t\tring->rx_length_errors +\n\t\t\t\tring->rx_crc_errors +\n\t\t\t\tring->rx_frame_errors +\n\t\t\t\tring->rx_fifo_errors;\n\t\t\tstats->rx_length_errors += ring->rx_length_errors;\n\t\t\tstats->rx_crc_errors += ring->rx_crc_errors;\n\t\t\tstats->rx_frame_errors += ring->rx_frame_errors;\n\t\t\tstats->rx_fifo_errors += ring->rx_fifo_errors;\n\t\t}\n\t}\n}\n\nstatic int xgene_enet_set_mac_address(struct net_device *ndev, void *addr)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tint ret;\n\n\tret = eth_mac_addr(ndev, addr);\n\tif (ret)\n\t\treturn ret;\n\tpdata->mac_ops->set_mac_addr(pdata);\n\n\treturn ret;\n}\n\nstatic int xgene_change_mtu(struct net_device *ndev, int new_mtu)\n{\n\tstruct xgene_enet_pdata *pdata = netdev_priv(ndev);\n\tint frame_size;\n\n\tif (!netif_running(ndev))\n\t\treturn 0;\n\n\tframe_size = (new_mtu > ETH_DATA_LEN) ? (new_mtu + 18) : 0x600;\n\n\txgene_enet_close(ndev);\n\tndev->mtu = new_mtu;\n\tpdata->mac_ops->set_framesize(pdata, frame_size);\n\txgene_enet_open(ndev);\n\n\treturn 0;\n}\n\nstatic const struct net_device_ops xgene_ndev_ops = {\n\t.ndo_open = xgene_enet_open,\n\t.ndo_stop = xgene_enet_close,\n\t.ndo_start_xmit = xgene_enet_start_xmit,\n\t.ndo_tx_timeout = xgene_enet_timeout,\n\t.ndo_get_stats64 = xgene_enet_get_stats64,\n\t.ndo_change_mtu = xgene_change_mtu,\n\t.ndo_set_mac_address = xgene_enet_set_mac_address,\n};\n\n#ifdef CONFIG_ACPI\nstatic void xgene_get_port_id_acpi(struct device *dev,\n\t\t\t\t  struct xgene_enet_pdata *pdata)\n{\n\tacpi_status status;\n\tu64 temp;\n\n\tstatus = acpi_evaluate_integer(ACPI_HANDLE(dev), \"_SUN\", NULL, &temp);\n\tif (ACPI_FAILURE(status)) {\n\t\tpdata->port_id = 0;\n\t} else {\n\t\tpdata->port_id = temp;\n\t}\n\n\treturn;\n}\n#endif\n\nstatic void xgene_get_port_id_dt(struct device *dev, struct xgene_enet_pdata *pdata)\n{\n\tu32 id = 0;\n\n\tof_property_read_u32(dev->of_node, \"port-id\", &id);\n\n\tpdata->port_id = id & BIT(0);\n\n\treturn;\n}\n\nstatic int xgene_get_tx_delay(struct xgene_enet_pdata *pdata)\n{\n\tstruct device *dev = &pdata->pdev->dev;\n\tint delay, ret;\n\n\tret = device_property_read_u32(dev, \"tx-delay\", &delay);\n\tif (ret) {\n\t\tpdata->tx_delay = 4;\n\t\treturn 0;\n\t}\n\n\tif (delay < 0 || delay > 7) {\n\t\tdev_err(dev, \"Invalid tx-delay specified\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tpdata->tx_delay = delay;\n\n\treturn 0;\n}\n\nstatic int xgene_get_rx_delay(struct xgene_enet_pdata *pdata)\n{\n\tstruct device *dev = &pdata->pdev->dev;\n\tint delay, ret;\n\n\tret = device_property_read_u32(dev, \"rx-delay\", &delay);\n\tif (ret) {\n\t\tpdata->rx_delay = 2;\n\t\treturn 0;\n\t}\n\n\tif (delay < 0 || delay > 7) {\n\t\tdev_err(dev, \"Invalid rx-delay specified\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tpdata->rx_delay = delay;\n\n\treturn 0;\n}\n\nstatic int xgene_enet_get_irqs(struct xgene_enet_pdata *pdata)\n{\n\tstruct platform_device *pdev = pdata->pdev;\n\tint i, ret, max_irqs;\n\n\tif (phy_interface_mode_is_rgmii(pdata->phy_mode))\n\t\tmax_irqs = 1;\n\telse if (pdata->phy_mode == PHY_INTERFACE_MODE_SGMII)\n\t\tmax_irqs = 2;\n\telse\n\t\tmax_irqs = XGENE_MAX_ENET_IRQ;\n\n\tfor (i = 0; i < max_irqs; i++) {\n\t\tret = platform_get_irq(pdev, i);\n\t\tif (ret < 0) {\n\t\t\tif (pdata->phy_mode == PHY_INTERFACE_MODE_XGMII) {\n\t\t\t\tmax_irqs = i;\n\t\t\t\tpdata->rxq_cnt = max_irqs / 2;\n\t\t\t\tpdata->txq_cnt = max_irqs / 2;\n\t\t\t\tpdata->cq_cnt = max_irqs / 2;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn ret;\n\t\t}\n\t\tpdata->irqs[i] = ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void xgene_enet_check_phy_handle(struct xgene_enet_pdata *pdata)\n{\n\tint ret;\n\n\tif (pdata->phy_mode == PHY_INTERFACE_MODE_XGMII)\n\t\treturn;\n\n\tif (!IS_ENABLED(CONFIG_MDIO_XGENE))\n\t\treturn;\n\n\tret = xgene_enet_phy_connect(pdata->ndev);\n\tif (!ret)\n\t\tpdata->mdio_driver = true;\n}\n\nstatic void xgene_enet_gpiod_get(struct xgene_enet_pdata *pdata)\n{\n\tstruct device *dev = &pdata->pdev->dev;\n\n\tpdata->sfp_gpio_en = false;\n\tif (pdata->phy_mode != PHY_INTERFACE_MODE_XGMII ||\n\t    (!device_property_present(dev, \"sfp-gpios\") &&\n\t     !device_property_present(dev, \"rxlos-gpios\")))\n\t\treturn;\n\n\tpdata->sfp_gpio_en = true;\n\tpdata->sfp_rdy = gpiod_get(dev, \"rxlos\", GPIOD_IN);\n\tif (IS_ERR(pdata->sfp_rdy))\n\t\tpdata->sfp_rdy = gpiod_get(dev, \"sfp\", GPIOD_IN);\n}\n\nstatic int xgene_enet_get_resources(struct xgene_enet_pdata *pdata)\n{\n\tstruct platform_device *pdev;\n\tstruct net_device *ndev;\n\tstruct device *dev;\n\tstruct resource *res;\n\tvoid __iomem *base_addr;\n\tu32 offset;\n\tint ret = 0;\n\n\tpdev = pdata->pdev;\n\tdev = &pdev->dev;\n\tndev = pdata->ndev;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, RES_ENET_CSR);\n\tif (!res) {\n\t\tdev_err(dev, \"Resource enet_csr not defined\\n\");\n\t\treturn -ENODEV;\n\t}\n\tpdata->base_addr = devm_ioremap(dev, res->start, resource_size(res));\n\tif (!pdata->base_addr) {\n\t\tdev_err(dev, \"Unable to retrieve ENET Port CSR region\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, RES_RING_CSR);\n\tif (!res) {\n\t\tdev_err(dev, \"Resource ring_csr not defined\\n\");\n\t\treturn -ENODEV;\n\t}\n\tpdata->ring_csr_addr = devm_ioremap(dev, res->start,\n\t\t\t\t\t\t\tresource_size(res));\n\tif (!pdata->ring_csr_addr) {\n\t\tdev_err(dev, \"Unable to retrieve ENET Ring CSR region\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, RES_RING_CMD);\n\tif (!res) {\n\t\tdev_err(dev, \"Resource ring_cmd not defined\\n\");\n\t\treturn -ENODEV;\n\t}\n\tpdata->ring_cmd_addr = devm_ioremap(dev, res->start,\n\t\t\t\t\t\t\tresource_size(res));\n\tif (!pdata->ring_cmd_addr) {\n\t\tdev_err(dev, \"Unable to retrieve ENET Ring command region\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (dev->of_node)\n\t\txgene_get_port_id_dt(dev, pdata);\n#ifdef CONFIG_ACPI\n\telse\n\t\txgene_get_port_id_acpi(dev, pdata);\n#endif\n\n\tif (device_get_ethdev_address(dev, ndev))\n\t\teth_hw_addr_random(ndev);\n\n\tmemcpy(ndev->perm_addr, ndev->dev_addr, ndev->addr_len);\n\n\tpdata->phy_mode = device_get_phy_mode(dev);\n\tif (pdata->phy_mode < 0) {\n\t\tdev_err(dev, \"Unable to get phy-connection-type\\n\");\n\t\treturn pdata->phy_mode;\n\t}\n\tif (!phy_interface_mode_is_rgmii(pdata->phy_mode) &&\n\t    pdata->phy_mode != PHY_INTERFACE_MODE_SGMII &&\n\t    pdata->phy_mode != PHY_INTERFACE_MODE_XGMII) {\n\t\tdev_err(dev, \"Incorrect phy-connection-type specified\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tret = xgene_get_tx_delay(pdata);\n\tif (ret)\n\t\treturn ret;\n\n\tret = xgene_get_rx_delay(pdata);\n\tif (ret)\n\t\treturn ret;\n\n\tret = xgene_enet_get_irqs(pdata);\n\tif (ret)\n\t\treturn ret;\n\n\txgene_enet_gpiod_get(pdata);\n\n\tpdata->clk = devm_clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(pdata->clk)) {\n\t\tif (pdata->phy_mode != PHY_INTERFACE_MODE_SGMII) {\n\t\t\t \n\t\t\tif (PTR_ERR(pdata->clk) != -ENOENT || dev->of_node)\n\t\t\t\treturn PTR_ERR(pdata->clk);\n\t\t\t \n\t\t\tdev_info(dev, \"clocks have been setup already\\n\");\n\t\t}\n\t}\n\n\tif (pdata->phy_mode != PHY_INTERFACE_MODE_XGMII)\n\t\tbase_addr = pdata->base_addr - (pdata->port_id * MAC_OFFSET);\n\telse\n\t\tbase_addr = pdata->base_addr;\n\tpdata->eth_csr_addr = base_addr + BLOCK_ETH_CSR_OFFSET;\n\tpdata->cle.base = base_addr + BLOCK_ETH_CLE_CSR_OFFSET;\n\tpdata->eth_ring_if_addr = base_addr + BLOCK_ETH_RING_IF_OFFSET;\n\tpdata->eth_diag_csr_addr = base_addr + BLOCK_ETH_DIAG_CSR_OFFSET;\n\tif (phy_interface_mode_is_rgmii(pdata->phy_mode) ||\n\t    pdata->phy_mode == PHY_INTERFACE_MODE_SGMII) {\n\t\tpdata->mcx_mac_addr = pdata->base_addr + BLOCK_ETH_MAC_OFFSET;\n\t\tpdata->mcx_stats_addr =\n\t\t\tpdata->base_addr + BLOCK_ETH_STATS_OFFSET;\n\t\toffset = (pdata->enet_id == XGENE_ENET1) ?\n\t\t\t  BLOCK_ETH_MAC_CSR_OFFSET :\n\t\t\t  X2_BLOCK_ETH_MAC_CSR_OFFSET;\n\t\tpdata->mcx_mac_csr_addr = base_addr + offset;\n\t} else {\n\t\tpdata->mcx_mac_addr = base_addr + BLOCK_AXG_MAC_OFFSET;\n\t\tpdata->mcx_stats_addr = base_addr + BLOCK_AXG_STATS_OFFSET;\n\t\tpdata->mcx_mac_csr_addr = base_addr + BLOCK_AXG_MAC_CSR_OFFSET;\n\t\tpdata->pcs_addr = base_addr + BLOCK_PCS_OFFSET;\n\t}\n\tpdata->rx_buff_cnt = NUM_PKT_BUF;\n\n\treturn 0;\n}\n\nstatic int xgene_enet_init_hw(struct xgene_enet_pdata *pdata)\n{\n\tstruct xgene_enet_cle *enet_cle = &pdata->cle;\n\tstruct xgene_enet_desc_ring *page_pool;\n\tstruct net_device *ndev = pdata->ndev;\n\tstruct xgene_enet_desc_ring *buf_pool;\n\tu16 dst_ring_num, ring_id;\n\tint i, ret;\n\tu32 count;\n\n\tret = pdata->port_ops->reset(pdata);\n\tif (ret)\n\t\treturn ret;\n\n\tret = xgene_enet_create_desc_rings(ndev);\n\tif (ret) {\n\t\tnetdev_err(ndev, \"Error in ring configuration\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tbuf_pool = pdata->rx_ring[i]->buf_pool;\n\t\txgene_enet_init_bufpool(buf_pool);\n\t\tpage_pool = pdata->rx_ring[i]->page_pool;\n\t\txgene_enet_init_bufpool(page_pool);\n\n\t\tcount = pdata->rx_buff_cnt;\n\t\tret = xgene_enet_refill_bufpool(buf_pool, count);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tret = xgene_enet_refill_pagepool(page_pool, count);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t}\n\n\tdst_ring_num = xgene_enet_dst_ring_num(pdata->rx_ring[0]);\n\tbuf_pool = pdata->rx_ring[0]->buf_pool;\n\tif (pdata->phy_mode == PHY_INTERFACE_MODE_XGMII) {\n\t\t \n\t\tenet_cle->max_nodes = 512;\n\t\tenet_cle->max_dbptrs = 1024;\n\t\tenet_cle->parsers = 3;\n\t\tenet_cle->active_parser = PARSER_ALL;\n\t\tenet_cle->ptree.start_node = 0;\n\t\tenet_cle->ptree.start_dbptr = 0;\n\t\tenet_cle->jump_bytes = 8;\n\t\tret = pdata->cle_ops->cle_init(pdata);\n\t\tif (ret) {\n\t\t\tnetdev_err(ndev, \"Preclass Tree init error\\n\");\n\t\t\tgoto err;\n\t\t}\n\n\t} else {\n\t\tdst_ring_num = xgene_enet_dst_ring_num(pdata->rx_ring[0]);\n\t\tbuf_pool = pdata->rx_ring[0]->buf_pool;\n\t\tpage_pool = pdata->rx_ring[0]->page_pool;\n\t\tring_id = (page_pool) ? page_pool->id : 0;\n\t\tpdata->port_ops->cle_bypass(pdata, dst_ring_num,\n\t\t\t\t\t    buf_pool->id, ring_id);\n\t}\n\n\tndev->max_mtu = XGENE_ENET_MAX_MTU;\n\tpdata->phy_speed = SPEED_UNKNOWN;\n\tpdata->mac_ops->init(pdata);\n\n\treturn ret;\n\nerr:\n\txgene_enet_delete_desc_rings(pdata);\n\treturn ret;\n}\n\nstatic void xgene_enet_setup_ops(struct xgene_enet_pdata *pdata)\n{\n\tswitch (pdata->phy_mode) {\n\tcase PHY_INTERFACE_MODE_RGMII:\n\tcase PHY_INTERFACE_MODE_RGMII_ID:\n\tcase PHY_INTERFACE_MODE_RGMII_RXID:\n\tcase PHY_INTERFACE_MODE_RGMII_TXID:\n\t\tpdata->mac_ops = &xgene_gmac_ops;\n\t\tpdata->port_ops = &xgene_gport_ops;\n\t\tpdata->rm = RM3;\n\t\tpdata->rxq_cnt = 1;\n\t\tpdata->txq_cnt = 1;\n\t\tpdata->cq_cnt = 0;\n\t\tbreak;\n\tcase PHY_INTERFACE_MODE_SGMII:\n\t\tpdata->mac_ops = &xgene_sgmac_ops;\n\t\tpdata->port_ops = &xgene_sgport_ops;\n\t\tpdata->rm = RM1;\n\t\tpdata->rxq_cnt = 1;\n\t\tpdata->txq_cnt = 1;\n\t\tpdata->cq_cnt = 1;\n\t\tbreak;\n\tdefault:\n\t\tpdata->mac_ops = &xgene_xgmac_ops;\n\t\tpdata->port_ops = &xgene_xgport_ops;\n\t\tpdata->cle_ops = &xgene_cle3in_ops;\n\t\tpdata->rm = RM0;\n\t\tif (!pdata->rxq_cnt) {\n\t\t\tpdata->rxq_cnt = XGENE_NUM_RX_RING;\n\t\t\tpdata->txq_cnt = XGENE_NUM_TX_RING;\n\t\t\tpdata->cq_cnt = XGENE_NUM_TXC_RING;\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (pdata->enet_id == XGENE_ENET1) {\n\t\tswitch (pdata->port_id) {\n\t\tcase 0:\n\t\t\tif (pdata->phy_mode == PHY_INTERFACE_MODE_XGMII) {\n\t\t\t\tpdata->cpu_bufnum = X2_START_CPU_BUFNUM_0;\n\t\t\t\tpdata->eth_bufnum = X2_START_ETH_BUFNUM_0;\n\t\t\t\tpdata->bp_bufnum = X2_START_BP_BUFNUM_0;\n\t\t\t\tpdata->ring_num = START_RING_NUM_0;\n\t\t\t} else {\n\t\t\t\tpdata->cpu_bufnum = START_CPU_BUFNUM_0;\n\t\t\t\tpdata->eth_bufnum = START_ETH_BUFNUM_0;\n\t\t\t\tpdata->bp_bufnum = START_BP_BUFNUM_0;\n\t\t\t\tpdata->ring_num = START_RING_NUM_0;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tif (pdata->phy_mode == PHY_INTERFACE_MODE_XGMII) {\n\t\t\t\tpdata->cpu_bufnum = XG_START_CPU_BUFNUM_1;\n\t\t\t\tpdata->eth_bufnum = XG_START_ETH_BUFNUM_1;\n\t\t\t\tpdata->bp_bufnum = XG_START_BP_BUFNUM_1;\n\t\t\t\tpdata->ring_num = XG_START_RING_NUM_1;\n\t\t\t} else {\n\t\t\t\tpdata->cpu_bufnum = START_CPU_BUFNUM_1;\n\t\t\t\tpdata->eth_bufnum = START_ETH_BUFNUM_1;\n\t\t\t\tpdata->bp_bufnum = START_BP_BUFNUM_1;\n\t\t\t\tpdata->ring_num = START_RING_NUM_1;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tpdata->ring_ops = &xgene_ring1_ops;\n\t} else {\n\t\tswitch (pdata->port_id) {\n\t\tcase 0:\n\t\t\tpdata->cpu_bufnum = X2_START_CPU_BUFNUM_0;\n\t\t\tpdata->eth_bufnum = X2_START_ETH_BUFNUM_0;\n\t\t\tpdata->bp_bufnum = X2_START_BP_BUFNUM_0;\n\t\t\tpdata->ring_num = X2_START_RING_NUM_0;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tpdata->cpu_bufnum = X2_START_CPU_BUFNUM_1;\n\t\t\tpdata->eth_bufnum = X2_START_ETH_BUFNUM_1;\n\t\t\tpdata->bp_bufnum = X2_START_BP_BUFNUM_1;\n\t\t\tpdata->ring_num = X2_START_RING_NUM_1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tpdata->rm = RM0;\n\t\tpdata->ring_ops = &xgene_ring2_ops;\n\t}\n}\n\nstatic void xgene_enet_napi_add(struct xgene_enet_pdata *pdata)\n{\n\tstruct napi_struct *napi;\n\tint i;\n\n\tfor (i = 0; i < pdata->rxq_cnt; i++) {\n\t\tnapi = &pdata->rx_ring[i]->napi;\n\t\tnetif_napi_add(pdata->ndev, napi, xgene_enet_napi);\n\t}\n\n\tfor (i = 0; i < pdata->cq_cnt; i++) {\n\t\tnapi = &pdata->tx_ring[i]->cp_ring->napi;\n\t\tnetif_napi_add(pdata->ndev, napi, xgene_enet_napi);\n\t}\n}\n\n#ifdef CONFIG_ACPI\nstatic const struct acpi_device_id xgene_enet_acpi_match[] = {\n\t{ \"APMC0D05\", XGENE_ENET1},\n\t{ \"APMC0D30\", XGENE_ENET1},\n\t{ \"APMC0D31\", XGENE_ENET1},\n\t{ \"APMC0D3F\", XGENE_ENET1},\n\t{ \"APMC0D26\", XGENE_ENET2},\n\t{ \"APMC0D25\", XGENE_ENET2},\n\t{ }\n};\nMODULE_DEVICE_TABLE(acpi, xgene_enet_acpi_match);\n#endif\n\nstatic const struct of_device_id xgene_enet_of_match[] = {\n\t{.compatible = \"apm,xgene-enet\",    .data = (void *)XGENE_ENET1},\n\t{.compatible = \"apm,xgene1-sgenet\", .data = (void *)XGENE_ENET1},\n\t{.compatible = \"apm,xgene1-xgenet\", .data = (void *)XGENE_ENET1},\n\t{.compatible = \"apm,xgene2-sgenet\", .data = (void *)XGENE_ENET2},\n\t{.compatible = \"apm,xgene2-xgenet\", .data = (void *)XGENE_ENET2},\n\t{},\n};\n\nMODULE_DEVICE_TABLE(of, xgene_enet_of_match);\n\nstatic int xgene_enet_probe(struct platform_device *pdev)\n{\n\tstruct net_device *ndev;\n\tstruct xgene_enet_pdata *pdata;\n\tstruct device *dev = &pdev->dev;\n\tvoid (*link_state)(struct work_struct *);\n\tconst struct of_device_id *of_id;\n\tint ret;\n\n\tndev = alloc_etherdev_mqs(sizeof(struct xgene_enet_pdata),\n\t\t\t\t  XGENE_NUM_TX_RING, XGENE_NUM_RX_RING);\n\tif (!ndev)\n\t\treturn -ENOMEM;\n\n\tpdata = netdev_priv(ndev);\n\n\tpdata->pdev = pdev;\n\tpdata->ndev = ndev;\n\tSET_NETDEV_DEV(ndev, dev);\n\tplatform_set_drvdata(pdev, pdata);\n\tndev->netdev_ops = &xgene_ndev_ops;\n\txgene_enet_set_ethtool_ops(ndev);\n\tndev->features |= NETIF_F_IP_CSUM |\n\t\t\t  NETIF_F_GSO |\n\t\t\t  NETIF_F_GRO |\n\t\t\t  NETIF_F_SG;\n\n\tof_id = of_match_device(xgene_enet_of_match, &pdev->dev);\n\tif (of_id) {\n\t\tpdata->enet_id = (uintptr_t)of_id->data;\n\t}\n#ifdef CONFIG_ACPI\n\telse {\n\t\tconst struct acpi_device_id *acpi_id;\n\n\t\tacpi_id = acpi_match_device(xgene_enet_acpi_match, &pdev->dev);\n\t\tif (acpi_id)\n\t\t\tpdata->enet_id = (enum xgene_enet_id) acpi_id->driver_data;\n\t}\n#endif\n\tif (!pdata->enet_id) {\n\t\tret = -ENODEV;\n\t\tgoto err;\n\t}\n\n\tret = xgene_enet_get_resources(pdata);\n\tif (ret)\n\t\tgoto err;\n\n\txgene_enet_setup_ops(pdata);\n\tspin_lock_init(&pdata->mac_lock);\n\n\tif (pdata->phy_mode == PHY_INTERFACE_MODE_XGMII) {\n\t\tndev->features |= NETIF_F_TSO | NETIF_F_RXCSUM;\n\t\tspin_lock_init(&pdata->mss_lock);\n\t}\n\tndev->hw_features = ndev->features;\n\n\tret = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(64));\n\tif (ret) {\n\t\tnetdev_err(ndev, \"No usable DMA configuration\\n\");\n\t\tgoto err;\n\t}\n\n\txgene_enet_check_phy_handle(pdata);\n\n\tret = xgene_enet_init_hw(pdata);\n\tif (ret)\n\t\tgoto err2;\n\n\tlink_state = pdata->mac_ops->link_state;\n\tif (pdata->phy_mode == PHY_INTERFACE_MODE_XGMII) {\n\t\tINIT_DELAYED_WORK(&pdata->link_work, link_state);\n\t} else if (!pdata->mdio_driver) {\n\t\tif (phy_interface_mode_is_rgmii(pdata->phy_mode))\n\t\t\tret = xgene_enet_mdio_config(pdata);\n\t\telse\n\t\t\tINIT_DELAYED_WORK(&pdata->link_work, link_state);\n\n\t\tif (ret)\n\t\t\tgoto err1;\n\t}\n\n\tspin_lock_init(&pdata->stats_lock);\n\tret = xgene_extd_stats_init(pdata);\n\tif (ret)\n\t\tgoto err1;\n\n\txgene_enet_napi_add(pdata);\n\tret = register_netdev(ndev);\n\tif (ret) {\n\t\tnetdev_err(ndev, \"Failed to register netdev\\n\");\n\t\tgoto err1;\n\t}\n\n\treturn 0;\n\nerr1:\n\t \n\n\txgene_enet_delete_desc_rings(pdata);\n\nerr2:\n\tif (pdata->mdio_driver)\n\t\txgene_enet_phy_disconnect(pdata);\n\telse if (phy_interface_mode_is_rgmii(pdata->phy_mode))\n\t\txgene_enet_mdio_remove(pdata);\nerr:\n\tfree_netdev(ndev);\n\treturn ret;\n}\n\nstatic int xgene_enet_remove(struct platform_device *pdev)\n{\n\tstruct xgene_enet_pdata *pdata;\n\tstruct net_device *ndev;\n\n\tpdata = platform_get_drvdata(pdev);\n\tndev = pdata->ndev;\n\n\trtnl_lock();\n\tif (netif_running(ndev))\n\t\tdev_close(ndev);\n\trtnl_unlock();\n\n\tif (pdata->mdio_driver)\n\t\txgene_enet_phy_disconnect(pdata);\n\telse if (phy_interface_mode_is_rgmii(pdata->phy_mode))\n\t\txgene_enet_mdio_remove(pdata);\n\n\tunregister_netdev(ndev);\n\txgene_enet_delete_desc_rings(pdata);\n\tpdata->port_ops->shutdown(pdata);\n\tfree_netdev(ndev);\n\n\treturn 0;\n}\n\nstatic void xgene_enet_shutdown(struct platform_device *pdev)\n{\n\tstruct xgene_enet_pdata *pdata;\n\n\tpdata = platform_get_drvdata(pdev);\n\tif (!pdata)\n\t\treturn;\n\n\tif (!pdata->ndev)\n\t\treturn;\n\n\txgene_enet_remove(pdev);\n}\n\nstatic struct platform_driver xgene_enet_driver = {\n\t.driver = {\n\t\t   .name = \"xgene-enet\",\n\t\t   .of_match_table = xgene_enet_of_match,\n\t\t   .acpi_match_table = ACPI_PTR(xgene_enet_acpi_match),\n\t},\n\t.probe = xgene_enet_probe,\n\t.remove = xgene_enet_remove,\n\t.shutdown = xgene_enet_shutdown,\n};\n\nmodule_platform_driver(xgene_enet_driver);\n\nMODULE_DESCRIPTION(\"APM X-Gene SoC Ethernet driver\");\nMODULE_AUTHOR(\"Iyappan Subramanian <isubramanian@apm.com>\");\nMODULE_AUTHOR(\"Keyur Chudgar <kchudgar@apm.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}