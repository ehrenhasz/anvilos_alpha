{
  "module_name": "forcedeth.c",
  "hash_id": "7daaa590ca0f94b63110a810c9713dc034c9f79181390ea97c349f2d07335c12",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/nvidia/forcedeth.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#define FORCEDETH_VERSION\t\t\"0.64\"\n#define DRV_NAME\t\t\t\"forcedeth\"\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/pci.h>\n#include <linux/interrupt.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/delay.h>\n#include <linux/sched.h>\n#include <linux/spinlock.h>\n#include <linux/ethtool.h>\n#include <linux/timer.h>\n#include <linux/skbuff.h>\n#include <linux/mii.h>\n#include <linux/random.h>\n#include <linux/if_vlan.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/prefetch.h>\n#include <linux/u64_stats_sync.h>\n#include <linux/io.h>\n\n#include <asm/irq.h>\n\n#define TX_WORK_PER_LOOP  NAPI_POLL_WEIGHT\n#define RX_WORK_PER_LOOP  NAPI_POLL_WEIGHT\n\n \n\n#define DEV_NEED_TIMERIRQ          0x0000001   \n#define DEV_NEED_LINKTIMER         0x0000002   \n#define DEV_HAS_LARGEDESC          0x0000004   \n#define DEV_HAS_HIGH_DMA           0x0000008   \n#define DEV_HAS_CHECKSUM           0x0000010   \n#define DEV_HAS_VLAN               0x0000020   \n#define DEV_HAS_MSI                0x0000040   \n#define DEV_HAS_MSI_X              0x0000080   \n#define DEV_HAS_POWER_CNTRL        0x0000100   \n#define DEV_HAS_STATISTICS_V1      0x0000200   \n#define DEV_HAS_STATISTICS_V2      0x0000400   \n#define DEV_HAS_STATISTICS_V3      0x0000800   \n#define DEV_HAS_STATISTICS_V12     0x0000600   \n#define DEV_HAS_STATISTICS_V123    0x0000e00   \n#define DEV_HAS_TEST_EXTENDED      0x0001000   \n#define DEV_HAS_MGMT_UNIT          0x0002000   \n#define DEV_HAS_CORRECT_MACADDR    0x0004000   \n#define DEV_HAS_COLLISION_FIX      0x0008000   \n#define DEV_HAS_PAUSEFRAME_TX_V1   0x0010000   \n#define DEV_HAS_PAUSEFRAME_TX_V2   0x0020000   \n#define DEV_HAS_PAUSEFRAME_TX_V3   0x0040000   \n#define DEV_NEED_TX_LIMIT          0x0080000   \n#define DEV_NEED_TX_LIMIT2         0x0180000   \n#define DEV_HAS_GEAR_MODE          0x0200000   \n#define DEV_NEED_PHY_INIT_FIX      0x0400000   \n#define DEV_NEED_LOW_POWER_FIX     0x0800000   \n#define DEV_NEED_MSI_FIX           0x1000000   \n\nenum {\n\tNvRegIrqStatus = 0x000,\n#define NVREG_IRQSTAT_MIIEVENT\t0x040\n#define NVREG_IRQSTAT_MASK\t\t0x83ff\n\tNvRegIrqMask = 0x004,\n#define NVREG_IRQ_RX_ERROR\t\t0x0001\n#define NVREG_IRQ_RX\t\t\t0x0002\n#define NVREG_IRQ_RX_NOBUF\t\t0x0004\n#define NVREG_IRQ_TX_ERR\t\t0x0008\n#define NVREG_IRQ_TX_OK\t\t\t0x0010\n#define NVREG_IRQ_TIMER\t\t\t0x0020\n#define NVREG_IRQ_LINK\t\t\t0x0040\n#define NVREG_IRQ_RX_FORCED\t\t0x0080\n#define NVREG_IRQ_TX_FORCED\t\t0x0100\n#define NVREG_IRQ_RECOVER_ERROR\t\t0x8200\n#define NVREG_IRQMASK_THROUGHPUT\t0x00df\n#define NVREG_IRQMASK_CPU\t\t0x0060\n#define NVREG_IRQ_TX_ALL\t\t(NVREG_IRQ_TX_ERR|NVREG_IRQ_TX_OK|NVREG_IRQ_TX_FORCED)\n#define NVREG_IRQ_RX_ALL\t\t(NVREG_IRQ_RX_ERROR|NVREG_IRQ_RX|NVREG_IRQ_RX_NOBUF|NVREG_IRQ_RX_FORCED)\n#define NVREG_IRQ_OTHER\t\t\t(NVREG_IRQ_TIMER|NVREG_IRQ_LINK|NVREG_IRQ_RECOVER_ERROR)\n\n\tNvRegUnknownSetupReg6 = 0x008,\n#define NVREG_UNKSETUP6_VAL\t\t3\n\n \n\tNvRegPollingInterval = 0x00c,\n#define NVREG_POLL_DEFAULT_THROUGHPUT\t65535  \n#define NVREG_POLL_DEFAULT_CPU\t13\n\tNvRegMSIMap0 = 0x020,\n\tNvRegMSIMap1 = 0x024,\n\tNvRegMSIIrqMask = 0x030,\n#define NVREG_MSI_VECTOR_0_ENABLED 0x01\n\tNvRegMisc1 = 0x080,\n#define NVREG_MISC1_PAUSE_TX\t0x01\n#define NVREG_MISC1_HD\t\t0x02\n#define NVREG_MISC1_FORCE\t0x3b0f3c\n\n\tNvRegMacReset = 0x34,\n#define NVREG_MAC_RESET_ASSERT\t0x0F3\n\tNvRegTransmitterControl = 0x084,\n#define NVREG_XMITCTL_START\t0x01\n#define NVREG_XMITCTL_MGMT_ST\t0x40000000\n#define NVREG_XMITCTL_SYNC_MASK\t\t0x000f0000\n#define NVREG_XMITCTL_SYNC_NOT_READY\t0x0\n#define NVREG_XMITCTL_SYNC_PHY_INIT\t0x00040000\n#define NVREG_XMITCTL_MGMT_SEMA_MASK\t0x00000f00\n#define NVREG_XMITCTL_MGMT_SEMA_FREE\t0x0\n#define NVREG_XMITCTL_HOST_SEMA_MASK\t0x0000f000\n#define NVREG_XMITCTL_HOST_SEMA_ACQ\t0x0000f000\n#define NVREG_XMITCTL_HOST_LOADED\t0x00004000\n#define NVREG_XMITCTL_TX_PATH_EN\t0x01000000\n#define NVREG_XMITCTL_DATA_START\t0x00100000\n#define NVREG_XMITCTL_DATA_READY\t0x00010000\n#define NVREG_XMITCTL_DATA_ERROR\t0x00020000\n\tNvRegTransmitterStatus = 0x088,\n#define NVREG_XMITSTAT_BUSY\t0x01\n\n\tNvRegPacketFilterFlags = 0x8c,\n#define NVREG_PFF_PAUSE_RX\t0x08\n#define NVREG_PFF_ALWAYS\t0x7F0000\n#define NVREG_PFF_PROMISC\t0x80\n#define NVREG_PFF_MYADDR\t0x20\n#define NVREG_PFF_LOOPBACK\t0x10\n\n\tNvRegOffloadConfig = 0x90,\n#define NVREG_OFFLOAD_HOMEPHY\t0x601\n#define NVREG_OFFLOAD_NORMAL\tRX_NIC_BUFSIZE\n\tNvRegReceiverControl = 0x094,\n#define NVREG_RCVCTL_START\t0x01\n#define NVREG_RCVCTL_RX_PATH_EN\t0x01000000\n\tNvRegReceiverStatus = 0x98,\n#define NVREG_RCVSTAT_BUSY\t0x01\n\n\tNvRegSlotTime = 0x9c,\n#define NVREG_SLOTTIME_LEGBF_ENABLED\t0x80000000\n#define NVREG_SLOTTIME_10_100_FULL\t0x00007f00\n#define NVREG_SLOTTIME_1000_FULL\t0x0003ff00\n#define NVREG_SLOTTIME_HALF\t\t0x0000ff00\n#define NVREG_SLOTTIME_DEFAULT\t\t0x00007f00\n#define NVREG_SLOTTIME_MASK\t\t0x000000ff\n\n\tNvRegTxDeferral = 0xA0,\n#define NVREG_TX_DEFERRAL_DEFAULT\t\t0x15050f\n#define NVREG_TX_DEFERRAL_RGMII_10_100\t\t0x16070f\n#define NVREG_TX_DEFERRAL_RGMII_1000\t\t0x14050f\n#define NVREG_TX_DEFERRAL_RGMII_STRETCH_10\t0x16190f\n#define NVREG_TX_DEFERRAL_RGMII_STRETCH_100\t0x16300f\n#define NVREG_TX_DEFERRAL_MII_STRETCH\t\t0x152000\n\tNvRegRxDeferral = 0xA4,\n#define NVREG_RX_DEFERRAL_DEFAULT\t0x16\n\tNvRegMacAddrA = 0xA8,\n\tNvRegMacAddrB = 0xAC,\n\tNvRegMulticastAddrA = 0xB0,\n#define NVREG_MCASTADDRA_FORCE\t0x01\n\tNvRegMulticastAddrB = 0xB4,\n\tNvRegMulticastMaskA = 0xB8,\n#define NVREG_MCASTMASKA_NONE\t\t0xffffffff\n\tNvRegMulticastMaskB = 0xBC,\n#define NVREG_MCASTMASKB_NONE\t\t0xffff\n\n\tNvRegPhyInterface = 0xC0,\n#define PHY_RGMII\t\t0x10000000\n\tNvRegBackOffControl = 0xC4,\n#define NVREG_BKOFFCTRL_DEFAULT\t\t\t0x70000000\n#define NVREG_BKOFFCTRL_SEED_MASK\t\t0x000003ff\n#define NVREG_BKOFFCTRL_SELECT\t\t\t24\n#define NVREG_BKOFFCTRL_GEAR\t\t\t12\n\n\tNvRegTxRingPhysAddr = 0x100,\n\tNvRegRxRingPhysAddr = 0x104,\n\tNvRegRingSizes = 0x108,\n#define NVREG_RINGSZ_TXSHIFT 0\n#define NVREG_RINGSZ_RXSHIFT 16\n\tNvRegTransmitPoll = 0x10c,\n#define NVREG_TRANSMITPOLL_MAC_ADDR_REV\t0x00008000\n\tNvRegLinkSpeed = 0x110,\n#define NVREG_LINKSPEED_FORCE 0x10000\n#define NVREG_LINKSPEED_10\t1000\n#define NVREG_LINKSPEED_100\t100\n#define NVREG_LINKSPEED_1000\t50\n#define NVREG_LINKSPEED_MASK\t(0xFFF)\n\tNvRegUnknownSetupReg5 = 0x130,\n#define NVREG_UNKSETUP5_BIT31\t(1<<31)\n\tNvRegTxWatermark = 0x13c,\n#define NVREG_TX_WM_DESC1_DEFAULT\t0x0200010\n#define NVREG_TX_WM_DESC2_3_DEFAULT\t0x1e08000\n#define NVREG_TX_WM_DESC2_3_1000\t0xfe08000\n\tNvRegTxRxControl = 0x144,\n#define NVREG_TXRXCTL_KICK\t0x0001\n#define NVREG_TXRXCTL_BIT1\t0x0002\n#define NVREG_TXRXCTL_BIT2\t0x0004\n#define NVREG_TXRXCTL_IDLE\t0x0008\n#define NVREG_TXRXCTL_RESET\t0x0010\n#define NVREG_TXRXCTL_RXCHECK\t0x0400\n#define NVREG_TXRXCTL_DESC_1\t0\n#define NVREG_TXRXCTL_DESC_2\t0x002100\n#define NVREG_TXRXCTL_DESC_3\t0xc02200\n#define NVREG_TXRXCTL_VLANSTRIP 0x00040\n#define NVREG_TXRXCTL_VLANINS\t0x00080\n\tNvRegTxRingPhysAddrHigh = 0x148,\n\tNvRegRxRingPhysAddrHigh = 0x14C,\n\tNvRegTxPauseFrame = 0x170,\n#define NVREG_TX_PAUSEFRAME_DISABLE\t0x0fff0080\n#define NVREG_TX_PAUSEFRAME_ENABLE_V1\t0x01800010\n#define NVREG_TX_PAUSEFRAME_ENABLE_V2\t0x056003f0\n#define NVREG_TX_PAUSEFRAME_ENABLE_V3\t0x09f00880\n\tNvRegTxPauseFrameLimit = 0x174,\n#define NVREG_TX_PAUSEFRAMELIMIT_ENABLE\t0x00010000\n\tNvRegMIIStatus = 0x180,\n#define NVREG_MIISTAT_ERROR\t\t0x0001\n#define NVREG_MIISTAT_LINKCHANGE\t0x0008\n#define NVREG_MIISTAT_MASK_RW\t\t0x0007\n#define NVREG_MIISTAT_MASK_ALL\t\t0x000f\n\tNvRegMIIMask = 0x184,\n#define NVREG_MII_LINKCHANGE\t\t0x0008\n\n\tNvRegAdapterControl = 0x188,\n#define NVREG_ADAPTCTL_START\t0x02\n#define NVREG_ADAPTCTL_LINKUP\t0x04\n#define NVREG_ADAPTCTL_PHYVALID\t0x40000\n#define NVREG_ADAPTCTL_RUNNING\t0x100000\n#define NVREG_ADAPTCTL_PHYSHIFT\t24\n\tNvRegMIISpeed = 0x18c,\n#define NVREG_MIISPEED_BIT8\t(1<<8)\n#define NVREG_MIIDELAY\t5\n\tNvRegMIIControl = 0x190,\n#define NVREG_MIICTL_INUSE\t0x08000\n#define NVREG_MIICTL_WRITE\t0x00400\n#define NVREG_MIICTL_ADDRSHIFT\t5\n\tNvRegMIIData = 0x194,\n\tNvRegTxUnicast = 0x1a0,\n\tNvRegTxMulticast = 0x1a4,\n\tNvRegTxBroadcast = 0x1a8,\n\tNvRegWakeUpFlags = 0x200,\n#define NVREG_WAKEUPFLAGS_VAL\t\t0x7770\n#define NVREG_WAKEUPFLAGS_BUSYSHIFT\t24\n#define NVREG_WAKEUPFLAGS_ENABLESHIFT\t16\n#define NVREG_WAKEUPFLAGS_D3SHIFT\t12\n#define NVREG_WAKEUPFLAGS_D2SHIFT\t8\n#define NVREG_WAKEUPFLAGS_D1SHIFT\t4\n#define NVREG_WAKEUPFLAGS_D0SHIFT\t0\n#define NVREG_WAKEUPFLAGS_ACCEPT_MAGPAT\t\t0x01\n#define NVREG_WAKEUPFLAGS_ACCEPT_WAKEUPPAT\t0x02\n#define NVREG_WAKEUPFLAGS_ACCEPT_LINKCHANGE\t0x04\n#define NVREG_WAKEUPFLAGS_ENABLE\t0x1111\n\n\tNvRegMgmtUnitGetVersion = 0x204,\n#define NVREG_MGMTUNITGETVERSION\t0x01\n\tNvRegMgmtUnitVersion = 0x208,\n#define NVREG_MGMTUNITVERSION\t\t0x08\n\tNvRegPowerCap = 0x268,\n#define NVREG_POWERCAP_D3SUPP\t(1<<30)\n#define NVREG_POWERCAP_D2SUPP\t(1<<26)\n#define NVREG_POWERCAP_D1SUPP\t(1<<25)\n\tNvRegPowerState = 0x26c,\n#define NVREG_POWERSTATE_POWEREDUP\t0x8000\n#define NVREG_POWERSTATE_VALID\t\t0x0100\n#define NVREG_POWERSTATE_MASK\t\t0x0003\n#define NVREG_POWERSTATE_D0\t\t0x0000\n#define NVREG_POWERSTATE_D1\t\t0x0001\n#define NVREG_POWERSTATE_D2\t\t0x0002\n#define NVREG_POWERSTATE_D3\t\t0x0003\n\tNvRegMgmtUnitControl = 0x278,\n#define NVREG_MGMTUNITCONTROL_INUSE\t0x20000\n\tNvRegTxCnt = 0x280,\n\tNvRegTxZeroReXmt = 0x284,\n\tNvRegTxOneReXmt = 0x288,\n\tNvRegTxManyReXmt = 0x28c,\n\tNvRegTxLateCol = 0x290,\n\tNvRegTxUnderflow = 0x294,\n\tNvRegTxLossCarrier = 0x298,\n\tNvRegTxExcessDef = 0x29c,\n\tNvRegTxRetryErr = 0x2a0,\n\tNvRegRxFrameErr = 0x2a4,\n\tNvRegRxExtraByte = 0x2a8,\n\tNvRegRxLateCol = 0x2ac,\n\tNvRegRxRunt = 0x2b0,\n\tNvRegRxFrameTooLong = 0x2b4,\n\tNvRegRxOverflow = 0x2b8,\n\tNvRegRxFCSErr = 0x2bc,\n\tNvRegRxFrameAlignErr = 0x2c0,\n\tNvRegRxLenErr = 0x2c4,\n\tNvRegRxUnicast = 0x2c8,\n\tNvRegRxMulticast = 0x2cc,\n\tNvRegRxBroadcast = 0x2d0,\n\tNvRegTxDef = 0x2d4,\n\tNvRegTxFrame = 0x2d8,\n\tNvRegRxCnt = 0x2dc,\n\tNvRegTxPause = 0x2e0,\n\tNvRegRxPause = 0x2e4,\n\tNvRegRxDropFrame = 0x2e8,\n\tNvRegVlanControl = 0x300,\n#define NVREG_VLANCONTROL_ENABLE\t0x2000\n\tNvRegMSIXMap0 = 0x3e0,\n\tNvRegMSIXMap1 = 0x3e4,\n\tNvRegMSIXIrqStatus = 0x3f0,\n\n\tNvRegPowerState2 = 0x600,\n#define NVREG_POWERSTATE2_POWERUP_MASK\t\t0x0F15\n#define NVREG_POWERSTATE2_POWERUP_REV_A3\t0x0001\n#define NVREG_POWERSTATE2_PHY_RESET\t\t0x0004\n#define NVREG_POWERSTATE2_GATE_CLOCKS\t\t0x0F00\n};\n\n \nstruct ring_desc {\n\t__le32 buf;\n\t__le32 flaglen;\n};\n\nstruct ring_desc_ex {\n\t__le32 bufhigh;\n\t__le32 buflow;\n\t__le32 txvlan;\n\t__le32 flaglen;\n};\n\nunion ring_type {\n\tstruct ring_desc *orig;\n\tstruct ring_desc_ex *ex;\n};\n\n#define FLAG_MASK_V1 0xffff0000\n#define FLAG_MASK_V2 0xffffc000\n#define LEN_MASK_V1 (0xffffffff ^ FLAG_MASK_V1)\n#define LEN_MASK_V2 (0xffffffff ^ FLAG_MASK_V2)\n\n#define NV_TX_LASTPACKET\t(1<<16)\n#define NV_TX_RETRYERROR\t(1<<19)\n#define NV_TX_RETRYCOUNT_MASK\t(0xF<<20)\n#define NV_TX_FORCED_INTERRUPT\t(1<<24)\n#define NV_TX_DEFERRED\t\t(1<<26)\n#define NV_TX_CARRIERLOST\t(1<<27)\n#define NV_TX_LATECOLLISION\t(1<<28)\n#define NV_TX_UNDERFLOW\t\t(1<<29)\n#define NV_TX_ERROR\t\t(1<<30)\n#define NV_TX_VALID\t\t(1<<31)\n\n#define NV_TX2_LASTPACKET\t(1<<29)\n#define NV_TX2_RETRYERROR\t(1<<18)\n#define NV_TX2_RETRYCOUNT_MASK\t(0xF<<19)\n#define NV_TX2_FORCED_INTERRUPT\t(1<<30)\n#define NV_TX2_DEFERRED\t\t(1<<25)\n#define NV_TX2_CARRIERLOST\t(1<<26)\n#define NV_TX2_LATECOLLISION\t(1<<27)\n#define NV_TX2_UNDERFLOW\t(1<<28)\n \n#define NV_TX2_ERROR\t\t(1<<30)\n#define NV_TX2_VALID\t\t(1<<31)\n#define NV_TX2_TSO\t\t(1<<28)\n#define NV_TX2_TSO_SHIFT\t14\n#define NV_TX2_TSO_MAX_SHIFT\t14\n#define NV_TX2_TSO_MAX_SIZE\t(1<<NV_TX2_TSO_MAX_SHIFT)\n#define NV_TX2_CHECKSUM_L3\t(1<<27)\n#define NV_TX2_CHECKSUM_L4\t(1<<26)\n\n#define NV_TX3_VLAN_TAG_PRESENT (1<<18)\n\n#define NV_RX_DESCRIPTORVALID\t(1<<16)\n#define NV_RX_MISSEDFRAME\t(1<<17)\n#define NV_RX_SUBTRACT1\t\t(1<<18)\n#define NV_RX_ERROR1\t\t(1<<23)\n#define NV_RX_ERROR2\t\t(1<<24)\n#define NV_RX_ERROR3\t\t(1<<25)\n#define NV_RX_ERROR4\t\t(1<<26)\n#define NV_RX_CRCERR\t\t(1<<27)\n#define NV_RX_OVERFLOW\t\t(1<<28)\n#define NV_RX_FRAMINGERR\t(1<<29)\n#define NV_RX_ERROR\t\t(1<<30)\n#define NV_RX_AVAIL\t\t(1<<31)\n#define NV_RX_ERROR_MASK\t(NV_RX_ERROR1|NV_RX_ERROR2|NV_RX_ERROR3|NV_RX_ERROR4|NV_RX_CRCERR|NV_RX_OVERFLOW|NV_RX_FRAMINGERR)\n\n#define NV_RX2_CHECKSUMMASK\t(0x1C000000)\n#define NV_RX2_CHECKSUM_IP\t(0x10000000)\n#define NV_RX2_CHECKSUM_IP_TCP\t(0x14000000)\n#define NV_RX2_CHECKSUM_IP_UDP\t(0x18000000)\n#define NV_RX2_DESCRIPTORVALID\t(1<<29)\n#define NV_RX2_SUBTRACT1\t(1<<25)\n#define NV_RX2_ERROR1\t\t(1<<18)\n#define NV_RX2_ERROR2\t\t(1<<19)\n#define NV_RX2_ERROR3\t\t(1<<20)\n#define NV_RX2_ERROR4\t\t(1<<21)\n#define NV_RX2_CRCERR\t\t(1<<22)\n#define NV_RX2_OVERFLOW\t\t(1<<23)\n#define NV_RX2_FRAMINGERR\t(1<<24)\n \n#define NV_RX2_ERROR\t\t(1<<30)\n#define NV_RX2_AVAIL\t\t(1<<31)\n#define NV_RX2_ERROR_MASK\t(NV_RX2_ERROR1|NV_RX2_ERROR2|NV_RX2_ERROR3|NV_RX2_ERROR4|NV_RX2_CRCERR|NV_RX2_OVERFLOW|NV_RX2_FRAMINGERR)\n\n#define NV_RX3_VLAN_TAG_PRESENT (1<<16)\n#define NV_RX3_VLAN_TAG_MASK\t(0x0000FFFF)\n\n \n#define NV_PCI_REGSZ_VER1\t0x270\n#define NV_PCI_REGSZ_VER2\t0x2d4\n#define NV_PCI_REGSZ_VER3\t0x604\n#define NV_PCI_REGSZ_MAX\t0x604\n\n \n#define NV_TXRX_RESET_DELAY\t4\n#define NV_TXSTOP_DELAY1\t10\n#define NV_TXSTOP_DELAY1MAX\t500000\n#define NV_TXSTOP_DELAY2\t100\n#define NV_RXSTOP_DELAY1\t10\n#define NV_RXSTOP_DELAY1MAX\t500000\n#define NV_RXSTOP_DELAY2\t100\n#define NV_SETUP5_DELAY\t\t5\n#define NV_SETUP5_DELAYMAX\t50000\n#define NV_POWERUP_DELAY\t5\n#define NV_POWERUP_DELAYMAX\t5000\n#define NV_MIIBUSY_DELAY\t50\n#define NV_MIIPHY_DELAY\t10\n#define NV_MIIPHY_DELAYMAX\t10000\n#define NV_MAC_RESET_DELAY\t64\n\n#define NV_WAKEUPPATTERNS\t5\n#define NV_WAKEUPMASKENTRIES\t4\n\n \n#define NV_WATCHDOG_TIMEO\t(5*HZ)\n\n#define RX_RING_DEFAULT\t\t512\n#define TX_RING_DEFAULT\t\t256\n#define RX_RING_MIN\t\t128\n#define TX_RING_MIN\t\t64\n#define RING_MAX_DESC_VER_1\t1024\n#define RING_MAX_DESC_VER_2_3\t16384\n\n \n#define NV_RX_HEADERS\t\t(64)\n \n#define NV_RX_ALLOC_PAD\t\t(64)\n\n \n#define NV_PKTLIMIT_1\tETH_DATA_LEN\t \n#define NV_PKTLIMIT_2\t9100\t \n\n#define OOM_REFILL\t(1+HZ/20)\n#define POLL_WAIT\t(1+HZ/100)\n#define LINK_TIMEOUT\t(3*HZ)\n#define STATS_INTERVAL\t(10*HZ)\n\n \n#define DESC_VER_1\t1\n#define DESC_VER_2\t2\n#define DESC_VER_3\t3\n\n \n#define PHY_OUI_MARVELL\t\t0x5043\n#define PHY_OUI_CICADA\t\t0x03f1\n#define PHY_OUI_VITESSE\t\t0x01c1\n#define PHY_OUI_REALTEK\t\t0x0732\n#define PHY_OUI_REALTEK2\t0x0020\n#define PHYID1_OUI_MASK\t0x03ff\n#define PHYID1_OUI_SHFT\t6\n#define PHYID2_OUI_MASK\t0xfc00\n#define PHYID2_OUI_SHFT\t10\n#define PHYID2_MODEL_MASK\t\t0x03f0\n#define PHY_MODEL_REALTEK_8211\t\t0x0110\n#define PHY_REV_MASK\t\t\t0x0001\n#define PHY_REV_REALTEK_8211B\t\t0x0000\n#define PHY_REV_REALTEK_8211C\t\t0x0001\n#define PHY_MODEL_REALTEK_8201\t\t0x0200\n#define PHY_MODEL_MARVELL_E3016\t\t0x0220\n#define PHY_MARVELL_E3016_INITMASK\t0x0300\n#define PHY_CICADA_INIT1\t0x0f000\n#define PHY_CICADA_INIT2\t0x0e00\n#define PHY_CICADA_INIT3\t0x01000\n#define PHY_CICADA_INIT4\t0x0200\n#define PHY_CICADA_INIT5\t0x0004\n#define PHY_CICADA_INIT6\t0x02000\n#define PHY_VITESSE_INIT_REG1\t0x1f\n#define PHY_VITESSE_INIT_REG2\t0x10\n#define PHY_VITESSE_INIT_REG3\t0x11\n#define PHY_VITESSE_INIT_REG4\t0x12\n#define PHY_VITESSE_INIT_MSK1\t0xc\n#define PHY_VITESSE_INIT_MSK2\t0x0180\n#define PHY_VITESSE_INIT1\t0x52b5\n#define PHY_VITESSE_INIT2\t0xaf8a\n#define PHY_VITESSE_INIT3\t0x8\n#define PHY_VITESSE_INIT4\t0x8f8a\n#define PHY_VITESSE_INIT5\t0xaf86\n#define PHY_VITESSE_INIT6\t0x8f86\n#define PHY_VITESSE_INIT7\t0xaf82\n#define PHY_VITESSE_INIT8\t0x0100\n#define PHY_VITESSE_INIT9\t0x8f82\n#define PHY_VITESSE_INIT10\t0x0\n#define PHY_REALTEK_INIT_REG1\t0x1f\n#define PHY_REALTEK_INIT_REG2\t0x19\n#define PHY_REALTEK_INIT_REG3\t0x13\n#define PHY_REALTEK_INIT_REG4\t0x14\n#define PHY_REALTEK_INIT_REG5\t0x18\n#define PHY_REALTEK_INIT_REG6\t0x11\n#define PHY_REALTEK_INIT_REG7\t0x01\n#define PHY_REALTEK_INIT1\t0x0000\n#define PHY_REALTEK_INIT2\t0x8e00\n#define PHY_REALTEK_INIT3\t0x0001\n#define PHY_REALTEK_INIT4\t0xad17\n#define PHY_REALTEK_INIT5\t0xfb54\n#define PHY_REALTEK_INIT6\t0xf5c7\n#define PHY_REALTEK_INIT7\t0x1000\n#define PHY_REALTEK_INIT8\t0x0003\n#define PHY_REALTEK_INIT9\t0x0008\n#define PHY_REALTEK_INIT10\t0x0005\n#define PHY_REALTEK_INIT11\t0x0200\n#define PHY_REALTEK_INIT_MSK1\t0x0003\n\n#define PHY_GIGABIT\t0x0100\n\n#define PHY_TIMEOUT\t0x1\n#define PHY_ERROR\t0x2\n\n#define PHY_100\t0x1\n#define PHY_1000\t0x2\n#define PHY_HALF\t0x100\n\n#define NV_PAUSEFRAME_RX_CAPABLE 0x0001\n#define NV_PAUSEFRAME_TX_CAPABLE 0x0002\n#define NV_PAUSEFRAME_RX_ENABLE  0x0004\n#define NV_PAUSEFRAME_TX_ENABLE  0x0008\n#define NV_PAUSEFRAME_RX_REQ     0x0010\n#define NV_PAUSEFRAME_TX_REQ     0x0020\n#define NV_PAUSEFRAME_AUTONEG    0x0040\n\n \n#define NV_MSI_X_MAX_VECTORS  8\n#define NV_MSI_X_VECTORS_MASK 0x000f\n#define NV_MSI_CAPABLE        0x0010\n#define NV_MSI_X_CAPABLE      0x0020\n#define NV_MSI_ENABLED        0x0040\n#define NV_MSI_X_ENABLED      0x0080\n\n#define NV_MSI_X_VECTOR_ALL   0x0\n#define NV_MSI_X_VECTOR_RX    0x0\n#define NV_MSI_X_VECTOR_TX    0x1\n#define NV_MSI_X_VECTOR_OTHER 0x2\n\n#define NV_MSI_PRIV_OFFSET 0x68\n#define NV_MSI_PRIV_VALUE  0xffffffff\n\n#define NV_RESTART_TX         0x1\n#define NV_RESTART_RX         0x2\n\n#define NV_TX_LIMIT_COUNT     16\n\n#define NV_DYNAMIC_THRESHOLD        4\n#define NV_DYNAMIC_MAX_QUIET_COUNT  2048\n\n \nstruct nv_ethtool_str {\n\tchar name[ETH_GSTRING_LEN];\n};\n\nstatic const struct nv_ethtool_str nv_estats_str[] = {\n\t{ \"tx_bytes\" },  \n\t{ \"tx_zero_rexmt\" },\n\t{ \"tx_one_rexmt\" },\n\t{ \"tx_many_rexmt\" },\n\t{ \"tx_late_collision\" },\n\t{ \"tx_fifo_errors\" },\n\t{ \"tx_carrier_errors\" },\n\t{ \"tx_excess_deferral\" },\n\t{ \"tx_retry_error\" },\n\t{ \"rx_frame_error\" },\n\t{ \"rx_extra_byte\" },\n\t{ \"rx_late_collision\" },\n\t{ \"rx_runt\" },\n\t{ \"rx_frame_too_long\" },\n\t{ \"rx_over_errors\" },\n\t{ \"rx_crc_errors\" },\n\t{ \"rx_frame_align_error\" },\n\t{ \"rx_length_error\" },\n\t{ \"rx_unicast\" },\n\t{ \"rx_multicast\" },\n\t{ \"rx_broadcast\" },\n\t{ \"rx_packets\" },\n\t{ \"rx_errors_total\" },\n\t{ \"tx_errors_total\" },\n\n\t \n\t{ \"tx_deferral\" },\n\t{ \"tx_packets\" },\n\t{ \"rx_bytes\" },  \n\t{ \"tx_pause\" },\n\t{ \"rx_pause\" },\n\t{ \"rx_drop_frame\" },\n\n\t \n\t{ \"tx_unicast\" },\n\t{ \"tx_multicast\" },\n\t{ \"tx_broadcast\" }\n};\n\nstruct nv_ethtool_stats {\n\tu64 tx_bytes;  \n\tu64 tx_zero_rexmt;\n\tu64 tx_one_rexmt;\n\tu64 tx_many_rexmt;\n\tu64 tx_late_collision;\n\tu64 tx_fifo_errors;\n\tu64 tx_carrier_errors;\n\tu64 tx_excess_deferral;\n\tu64 tx_retry_error;\n\tu64 rx_frame_error;\n\tu64 rx_extra_byte;\n\tu64 rx_late_collision;\n\tu64 rx_runt;\n\tu64 rx_frame_too_long;\n\tu64 rx_over_errors;\n\tu64 rx_crc_errors;\n\tu64 rx_frame_align_error;\n\tu64 rx_length_error;\n\tu64 rx_unicast;\n\tu64 rx_multicast;\n\tu64 rx_broadcast;\n\tu64 rx_packets;  \n\tu64 rx_errors_total;\n\tu64 tx_errors_total;\n\n\t \n\tu64 tx_deferral;\n\tu64 tx_packets;  \n\tu64 rx_bytes;    \n\tu64 tx_pause;\n\tu64 rx_pause;\n\tu64 rx_drop_frame;\n\n\t \n\tu64 tx_unicast;\n\tu64 tx_multicast;\n\tu64 tx_broadcast;\n};\n\n#define NV_DEV_STATISTICS_V3_COUNT (sizeof(struct nv_ethtool_stats)/sizeof(u64))\n#define NV_DEV_STATISTICS_V2_COUNT (NV_DEV_STATISTICS_V3_COUNT - 3)\n#define NV_DEV_STATISTICS_V1_COUNT (NV_DEV_STATISTICS_V2_COUNT - 6)\n\n \n#define NV_TEST_COUNT_BASE 3\n#define NV_TEST_COUNT_EXTENDED 4\n\nstatic const struct nv_ethtool_str nv_etests_str[] = {\n\t{ \"link      (online/offline)\" },\n\t{ \"register  (offline)       \" },\n\t{ \"interrupt (offline)       \" },\n\t{ \"loopback  (offline)       \" }\n};\n\nstruct register_test {\n\t__u32 reg;\n\t__u32 mask;\n};\n\nstatic const struct register_test nv_registers_test[] = {\n\t{ NvRegUnknownSetupReg6, 0x01 },\n\t{ NvRegMisc1, 0x03c },\n\t{ NvRegOffloadConfig, 0x03ff },\n\t{ NvRegMulticastAddrA, 0xffffffff },\n\t{ NvRegTxWatermark, 0x0ff },\n\t{ NvRegWakeUpFlags, 0x07777 },\n\t{ 0, 0 }\n};\n\nstruct nv_skb_map {\n\tstruct sk_buff *skb;\n\tdma_addr_t dma;\n\tunsigned int dma_len:31;\n\tunsigned int dma_single:1;\n\tstruct ring_desc_ex *first_tx_desc;\n\tstruct nv_skb_map *next_tx_ctx;\n};\n\nstruct nv_txrx_stats {\n\tu64 stat_rx_packets;\n\tu64 stat_rx_bytes;  \n\tu64 stat_rx_missed_errors;\n\tu64 stat_rx_dropped;\n\tu64 stat_tx_packets;  \n\tu64 stat_tx_bytes;\n\tu64 stat_tx_dropped;\n};\n\n#define nv_txrx_stats_inc(member) \\\n\t\t__this_cpu_inc(np->txrx_stats->member)\n#define nv_txrx_stats_add(member, count) \\\n\t\t__this_cpu_add(np->txrx_stats->member, (count))\n\n \n\n \nstruct fe_priv {\n\tspinlock_t lock;\n\n\tstruct net_device *dev;\n\tstruct napi_struct napi;\n\n\t \n\tspinlock_t hwstats_lock;\n\tstruct nv_ethtool_stats estats;\n\n\tint in_shutdown;\n\tu32 linkspeed;\n\tint duplex;\n\tint autoneg;\n\tint fixed_mode;\n\tint phyaddr;\n\tint wolenabled;\n\tunsigned int phy_oui;\n\tunsigned int phy_model;\n\tunsigned int phy_rev;\n\tu16 gigabit;\n\tint intr_test;\n\tint recover_error;\n\tint quiet_count;\n\n\t \n\tdma_addr_t ring_addr;\n\tstruct pci_dev *pci_dev;\n\tu32 orig_mac[2];\n\tu32 events;\n\tu32 irqmask;\n\tu32 desc_ver;\n\tu32 txrxctl_bits;\n\tu32 vlanctl_bits;\n\tu32 driver_data;\n\tu32 device_id;\n\tu32 register_size;\n\tu32 mac_in_use;\n\tint mgmt_version;\n\tint mgmt_sema;\n\n\tvoid __iomem *base;\n\n\t \n\tunion ring_type get_rx, put_rx, last_rx;\n\tstruct nv_skb_map *get_rx_ctx, *put_rx_ctx;\n\tstruct nv_skb_map *last_rx_ctx;\n\tstruct nv_skb_map *rx_skb;\n\n\tunion ring_type rx_ring;\n\tunsigned int rx_buf_sz;\n\tunsigned int pkt_limit;\n\tstruct timer_list oom_kick;\n\tstruct timer_list nic_poll;\n\tstruct timer_list stats_poll;\n\tu32 nic_poll_irq;\n\tint rx_ring_size;\n\n\t \n\tstruct u64_stats_sync swstats_rx_syncp;\n\tstruct nv_txrx_stats __percpu *txrx_stats;\n\n\t \n\tint need_linktimer;\n\tunsigned long link_timeout;\n\t \n\tunion ring_type get_tx, put_tx, last_tx;\n\tstruct nv_skb_map *get_tx_ctx, *put_tx_ctx;\n\tstruct nv_skb_map *last_tx_ctx;\n\tstruct nv_skb_map *tx_skb;\n\n\tunion ring_type tx_ring;\n\tu32 tx_flags;\n\tint tx_ring_size;\n\tint tx_limit;\n\tu32 tx_pkts_in_progress;\n\tstruct nv_skb_map *tx_change_owner;\n\tstruct nv_skb_map *tx_end_flip;\n\tint tx_stop;\n\n\t \n\tstruct u64_stats_sync swstats_tx_syncp;\n\n\t \n\tu32 msi_flags;\n\tstruct msix_entry msi_x_entry[NV_MSI_X_MAX_VECTORS];\n\n\t \n\tu32 pause_flags;\n\n\t \n\tu32 saved_config_space[NV_PCI_REGSZ_MAX/4];\n\n\t \n\tchar name_rx[IFNAMSIZ + 3];        \n\tchar name_tx[IFNAMSIZ + 3];        \n\tchar name_other[IFNAMSIZ + 6];     \n};\n\n \nstatic int max_interrupt_work = 4;\n\n \nenum {\n\tNV_OPTIMIZATION_MODE_THROUGHPUT,\n\tNV_OPTIMIZATION_MODE_CPU,\n\tNV_OPTIMIZATION_MODE_DYNAMIC\n};\nstatic int optimization_mode = NV_OPTIMIZATION_MODE_DYNAMIC;\n\n \nstatic int poll_interval = -1;\n\n \nenum {\n\tNV_MSI_INT_DISABLED,\n\tNV_MSI_INT_ENABLED\n};\nstatic int msi = NV_MSI_INT_ENABLED;\n\n \nenum {\n\tNV_MSIX_INT_DISABLED,\n\tNV_MSIX_INT_ENABLED\n};\nstatic int msix = NV_MSIX_INT_ENABLED;\n\n \nenum {\n\tNV_DMA_64BIT_DISABLED,\n\tNV_DMA_64BIT_ENABLED\n};\nstatic int dma_64bit = NV_DMA_64BIT_ENABLED;\n\n \nstatic bool debug_tx_timeout = false;\n\n \nenum {\n\tNV_CROSSOVER_DETECTION_DISABLED,\n\tNV_CROSSOVER_DETECTION_ENABLED\n};\nstatic int phy_cross = NV_CROSSOVER_DETECTION_DISABLED;\n\n \nstatic int phy_power_down;\n\nstatic inline struct fe_priv *get_nvpriv(struct net_device *dev)\n{\n\treturn netdev_priv(dev);\n}\n\nstatic inline u8 __iomem *get_hwbase(struct net_device *dev)\n{\n\treturn ((struct fe_priv *)netdev_priv(dev))->base;\n}\n\nstatic inline void pci_push(u8 __iomem *base)\n{\n\t \n\treadl(base);\n}\n\nstatic inline u32 nv_descr_getlength(struct ring_desc *prd, u32 v)\n{\n\treturn le32_to_cpu(prd->flaglen)\n\t\t& ((v == DESC_VER_1) ? LEN_MASK_V1 : LEN_MASK_V2);\n}\n\nstatic inline u32 nv_descr_getlength_ex(struct ring_desc_ex *prd, u32 v)\n{\n\treturn le32_to_cpu(prd->flaglen) & LEN_MASK_V2;\n}\n\nstatic bool nv_optimized(struct fe_priv *np)\n{\n\tif (np->desc_ver == DESC_VER_1 || np->desc_ver == DESC_VER_2)\n\t\treturn false;\n\treturn true;\n}\n\nstatic int reg_delay(struct net_device *dev, int offset, u32 mask, u32 target,\n\t\t     int delay, int delaymax)\n{\n\tu8 __iomem *base = get_hwbase(dev);\n\n\tpci_push(base);\n\tdo {\n\t\tudelay(delay);\n\t\tdelaymax -= delay;\n\t\tif (delaymax < 0)\n\t\t\treturn 1;\n\t} while ((readl(base + offset) & mask) != target);\n\treturn 0;\n}\n\n#define NV_SETUP_RX_RING 0x01\n#define NV_SETUP_TX_RING 0x02\n\nstatic inline u32 dma_low(dma_addr_t addr)\n{\n\treturn addr;\n}\n\nstatic inline u32 dma_high(dma_addr_t addr)\n{\n\treturn addr>>31>>1;\t \n}\n\nstatic void setup_hw_rings(struct net_device *dev, int rxtx_flags)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\n\tif (!nv_optimized(np)) {\n\t\tif (rxtx_flags & NV_SETUP_RX_RING)\n\t\t\twritel(dma_low(np->ring_addr), base + NvRegRxRingPhysAddr);\n\t\tif (rxtx_flags & NV_SETUP_TX_RING)\n\t\t\twritel(dma_low(np->ring_addr + np->rx_ring_size*sizeof(struct ring_desc)), base + NvRegTxRingPhysAddr);\n\t} else {\n\t\tif (rxtx_flags & NV_SETUP_RX_RING) {\n\t\t\twritel(dma_low(np->ring_addr), base + NvRegRxRingPhysAddr);\n\t\t\twritel(dma_high(np->ring_addr), base + NvRegRxRingPhysAddrHigh);\n\t\t}\n\t\tif (rxtx_flags & NV_SETUP_TX_RING) {\n\t\t\twritel(dma_low(np->ring_addr + np->rx_ring_size*sizeof(struct ring_desc_ex)), base + NvRegTxRingPhysAddr);\n\t\t\twritel(dma_high(np->ring_addr + np->rx_ring_size*sizeof(struct ring_desc_ex)), base + NvRegTxRingPhysAddrHigh);\n\t\t}\n\t}\n}\n\nstatic void free_rings(struct net_device *dev)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\n\tif (!nv_optimized(np)) {\n\t\tif (np->rx_ring.orig)\n\t\t\tdma_free_coherent(&np->pci_dev->dev,\n\t\t\t\t\t  sizeof(struct ring_desc) *\n\t\t\t\t\t  (np->rx_ring_size +\n\t\t\t\t\t  np->tx_ring_size),\n\t\t\t\t\t  np->rx_ring.orig, np->ring_addr);\n\t} else {\n\t\tif (np->rx_ring.ex)\n\t\t\tdma_free_coherent(&np->pci_dev->dev,\n\t\t\t\t\t  sizeof(struct ring_desc_ex) *\n\t\t\t\t\t  (np->rx_ring_size +\n\t\t\t\t\t  np->tx_ring_size),\n\t\t\t\t\t  np->rx_ring.ex, np->ring_addr);\n\t}\n\tkfree(np->rx_skb);\n\tkfree(np->tx_skb);\n}\n\nstatic int using_multi_irqs(struct net_device *dev)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\n\tif (!(np->msi_flags & NV_MSI_X_ENABLED) ||\n\t    ((np->msi_flags & NV_MSI_X_VECTORS_MASK) == 0x1))\n\t\treturn 0;\n\telse\n\t\treturn 1;\n}\n\nstatic void nv_txrx_gate(struct net_device *dev, bool gate)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 powerstate;\n\n\tif (!np->mac_in_use &&\n\t    (np->driver_data & DEV_HAS_POWER_CNTRL)) {\n\t\tpowerstate = readl(base + NvRegPowerState2);\n\t\tif (gate)\n\t\t\tpowerstate |= NVREG_POWERSTATE2_GATE_CLOCKS;\n\t\telse\n\t\t\tpowerstate &= ~NVREG_POWERSTATE2_GATE_CLOCKS;\n\t\twritel(powerstate, base + NvRegPowerState2);\n\t}\n}\n\nstatic void nv_enable_irq(struct net_device *dev)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\n\tif (!using_multi_irqs(dev)) {\n\t\tif (np->msi_flags & NV_MSI_X_ENABLED)\n\t\t\tenable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_ALL].vector);\n\t\telse\n\t\t\tenable_irq(np->pci_dev->irq);\n\t} else {\n\t\tenable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector);\n\t\tenable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector);\n\t\tenable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_OTHER].vector);\n\t}\n}\n\nstatic void nv_disable_irq(struct net_device *dev)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\n\tif (!using_multi_irqs(dev)) {\n\t\tif (np->msi_flags & NV_MSI_X_ENABLED)\n\t\t\tdisable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_ALL].vector);\n\t\telse\n\t\t\tdisable_irq(np->pci_dev->irq);\n\t} else {\n\t\tdisable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector);\n\t\tdisable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector);\n\t\tdisable_irq(np->msi_x_entry[NV_MSI_X_VECTOR_OTHER].vector);\n\t}\n}\n\n \nstatic void nv_enable_hw_interrupts(struct net_device *dev, u32 mask)\n{\n\tu8 __iomem *base = get_hwbase(dev);\n\n\twritel(mask, base + NvRegIrqMask);\n}\n\nstatic void nv_disable_hw_interrupts(struct net_device *dev, u32 mask)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\n\tif (np->msi_flags & NV_MSI_X_ENABLED) {\n\t\twritel(mask, base + NvRegIrqMask);\n\t} else {\n\t\tif (np->msi_flags & NV_MSI_ENABLED)\n\t\t\twritel(0, base + NvRegMSIIrqMask);\n\t\twritel(0, base + NvRegIrqMask);\n\t}\n}\n\nstatic void nv_napi_enable(struct net_device *dev)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\n\tnapi_enable(&np->napi);\n}\n\nstatic void nv_napi_disable(struct net_device *dev)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\n\tnapi_disable(&np->napi);\n}\n\n#define MII_READ\t(-1)\n \nstatic int mii_rw(struct net_device *dev, int addr, int miireg, int value)\n{\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 reg;\n\tint retval;\n\n\twritel(NVREG_MIISTAT_MASK_RW, base + NvRegMIIStatus);\n\n\treg = readl(base + NvRegMIIControl);\n\tif (reg & NVREG_MIICTL_INUSE) {\n\t\twritel(NVREG_MIICTL_INUSE, base + NvRegMIIControl);\n\t\tudelay(NV_MIIBUSY_DELAY);\n\t}\n\n\treg = (addr << NVREG_MIICTL_ADDRSHIFT) | miireg;\n\tif (value != MII_READ) {\n\t\twritel(value, base + NvRegMIIData);\n\t\treg |= NVREG_MIICTL_WRITE;\n\t}\n\twritel(reg, base + NvRegMIIControl);\n\n\tif (reg_delay(dev, NvRegMIIControl, NVREG_MIICTL_INUSE, 0,\n\t\t\tNV_MIIPHY_DELAY, NV_MIIPHY_DELAYMAX)) {\n\t\tretval = -1;\n\t} else if (value != MII_READ) {\n\t\t \n\t\tretval = 0;\n\t} else if (readl(base + NvRegMIIStatus) & NVREG_MIISTAT_ERROR) {\n\t\tretval = -1;\n\t} else {\n\t\tretval = readl(base + NvRegMIIData);\n\t}\n\n\treturn retval;\n}\n\nstatic int phy_reset(struct net_device *dev, u32 bmcr_setup)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu32 miicontrol;\n\tunsigned int tries = 0;\n\n\tmiicontrol = BMCR_RESET | bmcr_setup;\n\tif (mii_rw(dev, np->phyaddr, MII_BMCR, miicontrol))\n\t\treturn -1;\n\n\t \n\tmsleep(500);\n\n\t \n\twhile (miicontrol & BMCR_RESET) {\n\t\tusleep_range(10000, 20000);\n\t\tmiicontrol = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\t\t \n\t\tif (tries++ > 100)\n\t\t\treturn -1;\n\t}\n\treturn 0;\n}\n\nstatic int init_realtek_8211b(struct net_device *dev, struct fe_priv *np)\n{\n\tstatic const struct {\n\t\tint reg;\n\t\tint init;\n\t} ri[] = {\n\t\t{ PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1 },\n\t\t{ PHY_REALTEK_INIT_REG2, PHY_REALTEK_INIT2 },\n\t\t{ PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT3 },\n\t\t{ PHY_REALTEK_INIT_REG3, PHY_REALTEK_INIT4 },\n\t\t{ PHY_REALTEK_INIT_REG4, PHY_REALTEK_INIT5 },\n\t\t{ PHY_REALTEK_INIT_REG5, PHY_REALTEK_INIT6 },\n\t\t{ PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1 },\n\t};\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(ri); i++) {\n\t\tif (mii_rw(dev, np->phyaddr, ri[i].reg, ri[i].init))\n\t\t\treturn PHY_ERROR;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_realtek_8211c(struct net_device *dev, struct fe_priv *np)\n{\n\tu32 reg;\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 powerstate = readl(base + NvRegPowerState2);\n\n\t \n\tpowerstate |= NVREG_POWERSTATE2_PHY_RESET;\n\twritel(powerstate, base + NvRegPowerState2);\n\tmsleep(25);\n\n\tpowerstate &= ~NVREG_POWERSTATE2_PHY_RESET;\n\twritel(powerstate, base + NvRegPowerState2);\n\tmsleep(25);\n\n\treg = mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG6, MII_READ);\n\treg |= PHY_REALTEK_INIT9;\n\tif (mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG6, reg))\n\t\treturn PHY_ERROR;\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT10))\n\t\treturn PHY_ERROR;\n\treg = mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG7, MII_READ);\n\tif (!(reg & PHY_REALTEK_INIT11)) {\n\t\treg |= PHY_REALTEK_INIT11;\n\t\tif (mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG7, reg))\n\t\t\treturn PHY_ERROR;\n\t}\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1))\n\t\treturn PHY_ERROR;\n\n\treturn 0;\n}\n\nstatic int init_realtek_8201(struct net_device *dev, struct fe_priv *np)\n{\n\tu32 phy_reserved;\n\n\tif (np->driver_data & DEV_NEED_PHY_INIT_FIX) {\n\t\tphy_reserved = mii_rw(dev, np->phyaddr,\n\t\t\t\t      PHY_REALTEK_INIT_REG6, MII_READ);\n\t\tphy_reserved |= PHY_REALTEK_INIT7;\n\t\tif (mii_rw(dev, np->phyaddr,\n\t\t\t   PHY_REALTEK_INIT_REG6, phy_reserved))\n\t\t\treturn PHY_ERROR;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_realtek_8201_cross(struct net_device *dev, struct fe_priv *np)\n{\n\tu32 phy_reserved;\n\n\tif (phy_cross == NV_CROSSOVER_DETECTION_DISABLED) {\n\t\tif (mii_rw(dev, np->phyaddr,\n\t\t\t   PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT3))\n\t\t\treturn PHY_ERROR;\n\t\tphy_reserved = mii_rw(dev, np->phyaddr,\n\t\t\t\t      PHY_REALTEK_INIT_REG2, MII_READ);\n\t\tphy_reserved &= ~PHY_REALTEK_INIT_MSK1;\n\t\tphy_reserved |= PHY_REALTEK_INIT3;\n\t\tif (mii_rw(dev, np->phyaddr,\n\t\t\t   PHY_REALTEK_INIT_REG2, phy_reserved))\n\t\t\treturn PHY_ERROR;\n\t\tif (mii_rw(dev, np->phyaddr,\n\t\t\t   PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1))\n\t\t\treturn PHY_ERROR;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_cicada(struct net_device *dev, struct fe_priv *np,\n\t\t       u32 phyinterface)\n{\n\tu32 phy_reserved;\n\n\tif (phyinterface & PHY_RGMII) {\n\t\tphy_reserved = mii_rw(dev, np->phyaddr, MII_RESV1, MII_READ);\n\t\tphy_reserved &= ~(PHY_CICADA_INIT1 | PHY_CICADA_INIT2);\n\t\tphy_reserved |= (PHY_CICADA_INIT3 | PHY_CICADA_INIT4);\n\t\tif (mii_rw(dev, np->phyaddr, MII_RESV1, phy_reserved))\n\t\t\treturn PHY_ERROR;\n\t\tphy_reserved = mii_rw(dev, np->phyaddr, MII_NCONFIG, MII_READ);\n\t\tphy_reserved |= PHY_CICADA_INIT5;\n\t\tif (mii_rw(dev, np->phyaddr, MII_NCONFIG, phy_reserved))\n\t\t\treturn PHY_ERROR;\n\t}\n\tphy_reserved = mii_rw(dev, np->phyaddr, MII_SREVISION, MII_READ);\n\tphy_reserved |= PHY_CICADA_INIT6;\n\tif (mii_rw(dev, np->phyaddr, MII_SREVISION, phy_reserved))\n\t\treturn PHY_ERROR;\n\n\treturn 0;\n}\n\nstatic int init_vitesse(struct net_device *dev, struct fe_priv *np)\n{\n\tu32 phy_reserved;\n\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_VITESSE_INIT_REG1, PHY_VITESSE_INIT1))\n\t\treturn PHY_ERROR;\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT2))\n\t\treturn PHY_ERROR;\n\tphy_reserved = mii_rw(dev, np->phyaddr,\n\t\t\t      PHY_VITESSE_INIT_REG4, MII_READ);\n\tif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG4, phy_reserved))\n\t\treturn PHY_ERROR;\n\tphy_reserved = mii_rw(dev, np->phyaddr,\n\t\t\t      PHY_VITESSE_INIT_REG3, MII_READ);\n\tphy_reserved &= ~PHY_VITESSE_INIT_MSK1;\n\tphy_reserved |= PHY_VITESSE_INIT3;\n\tif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG3, phy_reserved))\n\t\treturn PHY_ERROR;\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT4))\n\t\treturn PHY_ERROR;\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT5))\n\t\treturn PHY_ERROR;\n\tphy_reserved = mii_rw(dev, np->phyaddr,\n\t\t\t      PHY_VITESSE_INIT_REG4, MII_READ);\n\tphy_reserved &= ~PHY_VITESSE_INIT_MSK1;\n\tphy_reserved |= PHY_VITESSE_INIT3;\n\tif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG4, phy_reserved))\n\t\treturn PHY_ERROR;\n\tphy_reserved = mii_rw(dev, np->phyaddr,\n\t\t\t      PHY_VITESSE_INIT_REG3, MII_READ);\n\tif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG3, phy_reserved))\n\t\treturn PHY_ERROR;\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT6))\n\t\treturn PHY_ERROR;\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT7))\n\t\treturn PHY_ERROR;\n\tphy_reserved = mii_rw(dev, np->phyaddr,\n\t\t\t      PHY_VITESSE_INIT_REG4, MII_READ);\n\tif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG4, phy_reserved))\n\t\treturn PHY_ERROR;\n\tphy_reserved = mii_rw(dev, np->phyaddr,\n\t\t\t      PHY_VITESSE_INIT_REG3, MII_READ);\n\tphy_reserved &= ~PHY_VITESSE_INIT_MSK2;\n\tphy_reserved |= PHY_VITESSE_INIT8;\n\tif (mii_rw(dev, np->phyaddr, PHY_VITESSE_INIT_REG3, phy_reserved))\n\t\treturn PHY_ERROR;\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_VITESSE_INIT_REG2, PHY_VITESSE_INIT9))\n\t\treturn PHY_ERROR;\n\tif (mii_rw(dev, np->phyaddr,\n\t\t   PHY_VITESSE_INIT_REG1, PHY_VITESSE_INIT10))\n\t\treturn PHY_ERROR;\n\n\treturn 0;\n}\n\nstatic int phy_init(struct net_device *dev)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 phyinterface;\n\tu32 mii_status, mii_control, mii_control_1000, reg;\n\n\t \n\tif (np->phy_model == PHY_MODEL_MARVELL_E3016) {\n\t\treg = mii_rw(dev, np->phyaddr, MII_NCONFIG, MII_READ);\n\t\treg &= ~PHY_MARVELL_E3016_INITMASK;\n\t\tif (mii_rw(dev, np->phyaddr, MII_NCONFIG, reg)) {\n\t\t\tnetdev_info(dev, \"%s: phy write to errata reg failed\\n\",\n\t\t\t\t    pci_name(np->pci_dev));\n\t\t\treturn PHY_ERROR;\n\t\t}\n\t}\n\tif (np->phy_oui == PHY_OUI_REALTEK) {\n\t\tif (np->phy_model == PHY_MODEL_REALTEK_8211 &&\n\t\t    np->phy_rev == PHY_REV_REALTEK_8211B) {\n\t\t\tif (init_realtek_8211b(dev, np)) {\n\t\t\t\tnetdev_info(dev, \"%s: phy init failed\\n\",\n\t\t\t\t\t    pci_name(np->pci_dev));\n\t\t\t\treturn PHY_ERROR;\n\t\t\t}\n\t\t} else if (np->phy_model == PHY_MODEL_REALTEK_8211 &&\n\t\t\t   np->phy_rev == PHY_REV_REALTEK_8211C) {\n\t\t\tif (init_realtek_8211c(dev, np)) {\n\t\t\t\tnetdev_info(dev, \"%s: phy init failed\\n\",\n\t\t\t\t\t    pci_name(np->pci_dev));\n\t\t\t\treturn PHY_ERROR;\n\t\t\t}\n\t\t} else if (np->phy_model == PHY_MODEL_REALTEK_8201) {\n\t\t\tif (init_realtek_8201(dev, np)) {\n\t\t\t\tnetdev_info(dev, \"%s: phy init failed\\n\",\n\t\t\t\t\t    pci_name(np->pci_dev));\n\t\t\t\treturn PHY_ERROR;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\treg = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\n\treg |= (ADVERTISE_10HALF | ADVERTISE_10FULL |\n\t\tADVERTISE_100HALF | ADVERTISE_100FULL |\n\t\tADVERTISE_PAUSE_ASYM | ADVERTISE_PAUSE_CAP);\n\tif (mii_rw(dev, np->phyaddr, MII_ADVERTISE, reg)) {\n\t\tnetdev_info(dev, \"%s: phy write to advertise failed\\n\",\n\t\t\t    pci_name(np->pci_dev));\n\t\treturn PHY_ERROR;\n\t}\n\n\t \n\tphyinterface = readl(base + NvRegPhyInterface);\n\n\t \n\tmii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\n\tif (mii_status & PHY_GIGABIT) {\n\t\tnp->gigabit = PHY_GIGABIT;\n\t\tmii_control_1000 = mii_rw(dev, np->phyaddr,\n\t\t\t\t\t  MII_CTRL1000, MII_READ);\n\t\tmii_control_1000 &= ~ADVERTISE_1000HALF;\n\t\tif (phyinterface & PHY_RGMII)\n\t\t\tmii_control_1000 |= ADVERTISE_1000FULL;\n\t\telse\n\t\t\tmii_control_1000 &= ~ADVERTISE_1000FULL;\n\n\t\tif (mii_rw(dev, np->phyaddr, MII_CTRL1000, mii_control_1000)) {\n\t\t\tnetdev_info(dev, \"%s: phy init failed\\n\",\n\t\t\t\t    pci_name(np->pci_dev));\n\t\t\treturn PHY_ERROR;\n\t\t}\n\t} else\n\t\tnp->gigabit = 0;\n\n\tmii_control = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\tmii_control |= BMCR_ANENABLE;\n\n\tif (np->phy_oui == PHY_OUI_REALTEK &&\n\t    np->phy_model == PHY_MODEL_REALTEK_8211 &&\n\t    np->phy_rev == PHY_REV_REALTEK_8211C) {\n\t\t \n\t\tmii_control |= BMCR_ANRESTART;\n\t\tif (mii_rw(dev, np->phyaddr, MII_BMCR, mii_control)) {\n\t\t\tnetdev_info(dev, \"%s: phy init failed\\n\",\n\t\t\t\t    pci_name(np->pci_dev));\n\t\t\treturn PHY_ERROR;\n\t\t}\n\t} else {\n\t\t \n\t\tif (phy_reset(dev, mii_control)) {\n\t\t\tnetdev_info(dev, \"%s: phy reset failed\\n\",\n\t\t\t\t    pci_name(np->pci_dev));\n\t\t\treturn PHY_ERROR;\n\t\t}\n\t}\n\n\t \n\tif (np->phy_oui == PHY_OUI_CICADA) {\n\t\tif (init_cicada(dev, np, phyinterface)) {\n\t\t\tnetdev_info(dev, \"%s: phy init failed\\n\",\n\t\t\t\t    pci_name(np->pci_dev));\n\t\t\treturn PHY_ERROR;\n\t\t}\n\t} else if (np->phy_oui == PHY_OUI_VITESSE) {\n\t\tif (init_vitesse(dev, np)) {\n\t\t\tnetdev_info(dev, \"%s: phy init failed\\n\",\n\t\t\t\t    pci_name(np->pci_dev));\n\t\t\treturn PHY_ERROR;\n\t\t}\n\t} else if (np->phy_oui == PHY_OUI_REALTEK) {\n\t\tif (np->phy_model == PHY_MODEL_REALTEK_8211 &&\n\t\t    np->phy_rev == PHY_REV_REALTEK_8211B) {\n\t\t\t \n\t\t\tif (init_realtek_8211b(dev, np)) {\n\t\t\t\tnetdev_info(dev, \"%s: phy init failed\\n\",\n\t\t\t\t\t    pci_name(np->pci_dev));\n\t\t\t\treturn PHY_ERROR;\n\t\t\t}\n\t\t} else if (np->phy_model == PHY_MODEL_REALTEK_8201) {\n\t\t\tif (init_realtek_8201(dev, np) ||\n\t\t\t    init_realtek_8201_cross(dev, np)) {\n\t\t\t\tnetdev_info(dev, \"%s: phy init failed\\n\",\n\t\t\t\t\t    pci_name(np->pci_dev));\n\t\t\t\treturn PHY_ERROR;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tmii_rw(dev, np->phyaddr, MII_ADVERTISE, reg);\n\n\t \n\tmii_control = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\tmii_control |= (BMCR_ANRESTART | BMCR_ANENABLE);\n\tif (phy_power_down)\n\t\tmii_control |= BMCR_PDOWN;\n\tif (mii_rw(dev, np->phyaddr, MII_BMCR, mii_control))\n\t\treturn PHY_ERROR;\n\n\treturn 0;\n}\n\nstatic void nv_start_rx(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 rx_ctrl = readl(base + NvRegReceiverControl);\n\n\t \n\tif ((readl(base + NvRegReceiverControl) & NVREG_RCVCTL_START) && !np->mac_in_use) {\n\t\trx_ctrl &= ~NVREG_RCVCTL_START;\n\t\twritel(rx_ctrl, base + NvRegReceiverControl);\n\t\tpci_push(base);\n\t}\n\twritel(np->linkspeed, base + NvRegLinkSpeed);\n\tpci_push(base);\n\trx_ctrl |= NVREG_RCVCTL_START;\n\tif (np->mac_in_use)\n\t\trx_ctrl &= ~NVREG_RCVCTL_RX_PATH_EN;\n\twritel(rx_ctrl, base + NvRegReceiverControl);\n\tpci_push(base);\n}\n\nstatic void nv_stop_rx(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 rx_ctrl = readl(base + NvRegReceiverControl);\n\n\tif (!np->mac_in_use)\n\t\trx_ctrl &= ~NVREG_RCVCTL_START;\n\telse\n\t\trx_ctrl |= NVREG_RCVCTL_RX_PATH_EN;\n\twritel(rx_ctrl, base + NvRegReceiverControl);\n\tif (reg_delay(dev, NvRegReceiverStatus, NVREG_RCVSTAT_BUSY, 0,\n\t\t      NV_RXSTOP_DELAY1, NV_RXSTOP_DELAY1MAX))\n\t\tnetdev_info(dev, \"%s: ReceiverStatus remained busy\\n\",\n\t\t\t    __func__);\n\n\tudelay(NV_RXSTOP_DELAY2);\n\tif (!np->mac_in_use)\n\t\twritel(0, base + NvRegLinkSpeed);\n}\n\nstatic void nv_start_tx(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 tx_ctrl = readl(base + NvRegTransmitterControl);\n\n\ttx_ctrl |= NVREG_XMITCTL_START;\n\tif (np->mac_in_use)\n\t\ttx_ctrl &= ~NVREG_XMITCTL_TX_PATH_EN;\n\twritel(tx_ctrl, base + NvRegTransmitterControl);\n\tpci_push(base);\n}\n\nstatic void nv_stop_tx(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 tx_ctrl = readl(base + NvRegTransmitterControl);\n\n\tif (!np->mac_in_use)\n\t\ttx_ctrl &= ~NVREG_XMITCTL_START;\n\telse\n\t\ttx_ctrl |= NVREG_XMITCTL_TX_PATH_EN;\n\twritel(tx_ctrl, base + NvRegTransmitterControl);\n\tif (reg_delay(dev, NvRegTransmitterStatus, NVREG_XMITSTAT_BUSY, 0,\n\t\t      NV_TXSTOP_DELAY1, NV_TXSTOP_DELAY1MAX))\n\t\tnetdev_info(dev, \"%s: TransmitterStatus remained busy\\n\",\n\t\t\t    __func__);\n\n\tudelay(NV_TXSTOP_DELAY2);\n\tif (!np->mac_in_use)\n\t\twritel(readl(base + NvRegTransmitPoll) & NVREG_TRANSMITPOLL_MAC_ADDR_REV,\n\t\t       base + NvRegTransmitPoll);\n}\n\nstatic void nv_start_rxtx(struct net_device *dev)\n{\n\tnv_start_rx(dev);\n\tnv_start_tx(dev);\n}\n\nstatic void nv_stop_rxtx(struct net_device *dev)\n{\n\tnv_stop_rx(dev);\n\tnv_stop_tx(dev);\n}\n\nstatic void nv_txrx_reset(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\n\twritel(NVREG_TXRXCTL_BIT2 | NVREG_TXRXCTL_RESET | np->txrxctl_bits, base + NvRegTxRxControl);\n\tpci_push(base);\n\tudelay(NV_TXRX_RESET_DELAY);\n\twritel(NVREG_TXRXCTL_BIT2 | np->txrxctl_bits, base + NvRegTxRxControl);\n\tpci_push(base);\n}\n\nstatic void nv_mac_reset(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 temp1, temp2, temp3;\n\n\twritel(NVREG_TXRXCTL_BIT2 | NVREG_TXRXCTL_RESET | np->txrxctl_bits, base + NvRegTxRxControl);\n\tpci_push(base);\n\n\t \n\ttemp1 = readl(base + NvRegMacAddrA);\n\ttemp2 = readl(base + NvRegMacAddrB);\n\ttemp3 = readl(base + NvRegTransmitPoll);\n\n\twritel(NVREG_MAC_RESET_ASSERT, base + NvRegMacReset);\n\tpci_push(base);\n\tudelay(NV_MAC_RESET_DELAY);\n\twritel(0, base + NvRegMacReset);\n\tpci_push(base);\n\tudelay(NV_MAC_RESET_DELAY);\n\n\t \n\twritel(temp1, base + NvRegMacAddrA);\n\twritel(temp2, base + NvRegMacAddrB);\n\twritel(temp3, base + NvRegTransmitPoll);\n\n\twritel(NVREG_TXRXCTL_BIT2 | np->txrxctl_bits, base + NvRegTxRxControl);\n\tpci_push(base);\n}\n\n \nstatic void nv_update_stats(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\n\tlockdep_assert_held(&np->hwstats_lock);\n\n\t \n\tnp->estats.tx_bytes += readl(base + NvRegTxCnt);\n\tnp->estats.tx_zero_rexmt += readl(base + NvRegTxZeroReXmt);\n\tnp->estats.tx_one_rexmt += readl(base + NvRegTxOneReXmt);\n\tnp->estats.tx_many_rexmt += readl(base + NvRegTxManyReXmt);\n\tnp->estats.tx_late_collision += readl(base + NvRegTxLateCol);\n\tnp->estats.tx_fifo_errors += readl(base + NvRegTxUnderflow);\n\tnp->estats.tx_carrier_errors += readl(base + NvRegTxLossCarrier);\n\tnp->estats.tx_excess_deferral += readl(base + NvRegTxExcessDef);\n\tnp->estats.tx_retry_error += readl(base + NvRegTxRetryErr);\n\tnp->estats.rx_frame_error += readl(base + NvRegRxFrameErr);\n\tnp->estats.rx_extra_byte += readl(base + NvRegRxExtraByte);\n\tnp->estats.rx_late_collision += readl(base + NvRegRxLateCol);\n\tnp->estats.rx_runt += readl(base + NvRegRxRunt);\n\tnp->estats.rx_frame_too_long += readl(base + NvRegRxFrameTooLong);\n\tnp->estats.rx_over_errors += readl(base + NvRegRxOverflow);\n\tnp->estats.rx_crc_errors += readl(base + NvRegRxFCSErr);\n\tnp->estats.rx_frame_align_error += readl(base + NvRegRxFrameAlignErr);\n\tnp->estats.rx_length_error += readl(base + NvRegRxLenErr);\n\tnp->estats.rx_unicast += readl(base + NvRegRxUnicast);\n\tnp->estats.rx_multicast += readl(base + NvRegRxMulticast);\n\tnp->estats.rx_broadcast += readl(base + NvRegRxBroadcast);\n\tnp->estats.rx_packets =\n\t\tnp->estats.rx_unicast +\n\t\tnp->estats.rx_multicast +\n\t\tnp->estats.rx_broadcast;\n\tnp->estats.rx_errors_total =\n\t\tnp->estats.rx_crc_errors +\n\t\tnp->estats.rx_over_errors +\n\t\tnp->estats.rx_frame_error +\n\t\t(np->estats.rx_frame_align_error - np->estats.rx_extra_byte) +\n\t\tnp->estats.rx_late_collision +\n\t\tnp->estats.rx_runt +\n\t\tnp->estats.rx_frame_too_long;\n\tnp->estats.tx_errors_total =\n\t\tnp->estats.tx_late_collision +\n\t\tnp->estats.tx_fifo_errors +\n\t\tnp->estats.tx_carrier_errors +\n\t\tnp->estats.tx_excess_deferral +\n\t\tnp->estats.tx_retry_error;\n\n\tif (np->driver_data & DEV_HAS_STATISTICS_V2) {\n\t\tnp->estats.tx_deferral += readl(base + NvRegTxDef);\n\t\tnp->estats.tx_packets += readl(base + NvRegTxFrame);\n\t\tnp->estats.rx_bytes += readl(base + NvRegRxCnt);\n\t\tnp->estats.tx_pause += readl(base + NvRegTxPause);\n\t\tnp->estats.rx_pause += readl(base + NvRegRxPause);\n\t\tnp->estats.rx_drop_frame += readl(base + NvRegRxDropFrame);\n\t\tnp->estats.rx_errors_total += np->estats.rx_drop_frame;\n\t}\n\n\tif (np->driver_data & DEV_HAS_STATISTICS_V3) {\n\t\tnp->estats.tx_unicast += readl(base + NvRegTxUnicast);\n\t\tnp->estats.tx_multicast += readl(base + NvRegTxMulticast);\n\t\tnp->estats.tx_broadcast += readl(base + NvRegTxBroadcast);\n\t}\n}\n\nstatic void nv_get_stats(int cpu, struct fe_priv *np,\n\t\t\t struct rtnl_link_stats64 *storage)\n{\n\tstruct nv_txrx_stats *src = per_cpu_ptr(np->txrx_stats, cpu);\n\tunsigned int syncp_start;\n\tu64 rx_packets, rx_bytes, rx_dropped, rx_missed_errors;\n\tu64 tx_packets, tx_bytes, tx_dropped;\n\n\tdo {\n\t\tsyncp_start = u64_stats_fetch_begin(&np->swstats_rx_syncp);\n\t\trx_packets       = src->stat_rx_packets;\n\t\trx_bytes         = src->stat_rx_bytes;\n\t\trx_dropped       = src->stat_rx_dropped;\n\t\trx_missed_errors = src->stat_rx_missed_errors;\n\t} while (u64_stats_fetch_retry(&np->swstats_rx_syncp, syncp_start));\n\n\tstorage->rx_packets       += rx_packets;\n\tstorage->rx_bytes         += rx_bytes;\n\tstorage->rx_dropped       += rx_dropped;\n\tstorage->rx_missed_errors += rx_missed_errors;\n\n\tdo {\n\t\tsyncp_start = u64_stats_fetch_begin(&np->swstats_tx_syncp);\n\t\ttx_packets  = src->stat_tx_packets;\n\t\ttx_bytes    = src->stat_tx_bytes;\n\t\ttx_dropped  = src->stat_tx_dropped;\n\t} while (u64_stats_fetch_retry(&np->swstats_tx_syncp, syncp_start));\n\n\tstorage->tx_packets += tx_packets;\n\tstorage->tx_bytes   += tx_bytes;\n\tstorage->tx_dropped += tx_dropped;\n}\n\n \nstatic void\nnv_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *storage)\n\t__acquires(&netdev_priv(dev)->hwstats_lock)\n\t__releases(&netdev_priv(dev)->hwstats_lock)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tint cpu;\n\n\t \n\n\t \n\tfor_each_online_cpu(cpu)\n\t\tnv_get_stats(cpu, np, storage);\n\n\t \n\tif (np->driver_data & DEV_HAS_STATISTICS_V123) {\n\t\tspin_lock_bh(&np->hwstats_lock);\n\n\t\tnv_update_stats(dev);\n\n\t\t \n\t\tstorage->rx_errors = np->estats.rx_errors_total;\n\t\tstorage->tx_errors = np->estats.tx_errors_total;\n\n\t\t \n\t\tstorage->multicast = np->estats.rx_multicast;\n\n\t\t \n\t\tstorage->rx_length_errors = np->estats.rx_length_error;\n\t\tstorage->rx_over_errors   = np->estats.rx_over_errors;\n\t\tstorage->rx_crc_errors    = np->estats.rx_crc_errors;\n\t\tstorage->rx_frame_errors  = np->estats.rx_frame_align_error;\n\t\tstorage->rx_fifo_errors   = np->estats.rx_drop_frame;\n\n\t\t \n\t\tstorage->tx_carrier_errors = np->estats.tx_carrier_errors;\n\t\tstorage->tx_fifo_errors    = np->estats.tx_fifo_errors;\n\n\t\tspin_unlock_bh(&np->hwstats_lock);\n\t}\n}\n\n \nstatic int nv_alloc_rx(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tstruct ring_desc *less_rx;\n\n\tless_rx = np->get_rx.orig;\n\tif (less_rx-- == np->rx_ring.orig)\n\t\tless_rx = np->last_rx.orig;\n\n\twhile (np->put_rx.orig != less_rx) {\n\t\tstruct sk_buff *skb = netdev_alloc_skb(dev, np->rx_buf_sz + NV_RX_ALLOC_PAD);\n\t\tif (likely(skb)) {\n\t\t\tnp->put_rx_ctx->skb = skb;\n\t\t\tnp->put_rx_ctx->dma = dma_map_single(&np->pci_dev->dev,\n\t\t\t\t\t\t\t     skb->data,\n\t\t\t\t\t\t\t     skb_tailroom(skb),\n\t\t\t\t\t\t\t     DMA_FROM_DEVICE);\n\t\t\tif (unlikely(dma_mapping_error(&np->pci_dev->dev,\n\t\t\t\t\t\t       np->put_rx_ctx->dma))) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto packet_dropped;\n\t\t\t}\n\t\t\tnp->put_rx_ctx->dma_len = skb_tailroom(skb);\n\t\t\tnp->put_rx.orig->buf = cpu_to_le32(np->put_rx_ctx->dma);\n\t\t\twmb();\n\t\t\tnp->put_rx.orig->flaglen = cpu_to_le32(np->rx_buf_sz | NV_RX_AVAIL);\n\t\t\tif (unlikely(np->put_rx.orig++ == np->last_rx.orig))\n\t\t\t\tnp->put_rx.orig = np->rx_ring.orig;\n\t\t\tif (unlikely(np->put_rx_ctx++ == np->last_rx_ctx))\n\t\t\t\tnp->put_rx_ctx = np->rx_skb;\n\t\t} else {\npacket_dropped:\n\t\t\tu64_stats_update_begin(&np->swstats_rx_syncp);\n\t\t\tnv_txrx_stats_inc(stat_rx_dropped);\n\t\t\tu64_stats_update_end(&np->swstats_rx_syncp);\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int nv_alloc_rx_optimized(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tstruct ring_desc_ex *less_rx;\n\n\tless_rx = np->get_rx.ex;\n\tif (less_rx-- == np->rx_ring.ex)\n\t\tless_rx = np->last_rx.ex;\n\n\twhile (np->put_rx.ex != less_rx) {\n\t\tstruct sk_buff *skb = netdev_alloc_skb(dev, np->rx_buf_sz + NV_RX_ALLOC_PAD);\n\t\tif (likely(skb)) {\n\t\t\tnp->put_rx_ctx->skb = skb;\n\t\t\tnp->put_rx_ctx->dma = dma_map_single(&np->pci_dev->dev,\n\t\t\t\t\t\t\t     skb->data,\n\t\t\t\t\t\t\t     skb_tailroom(skb),\n\t\t\t\t\t\t\t     DMA_FROM_DEVICE);\n\t\t\tif (unlikely(dma_mapping_error(&np->pci_dev->dev,\n\t\t\t\t\t\t       np->put_rx_ctx->dma))) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto packet_dropped;\n\t\t\t}\n\t\t\tnp->put_rx_ctx->dma_len = skb_tailroom(skb);\n\t\t\tnp->put_rx.ex->bufhigh = cpu_to_le32(dma_high(np->put_rx_ctx->dma));\n\t\t\tnp->put_rx.ex->buflow = cpu_to_le32(dma_low(np->put_rx_ctx->dma));\n\t\t\twmb();\n\t\t\tnp->put_rx.ex->flaglen = cpu_to_le32(np->rx_buf_sz | NV_RX2_AVAIL);\n\t\t\tif (unlikely(np->put_rx.ex++ == np->last_rx.ex))\n\t\t\t\tnp->put_rx.ex = np->rx_ring.ex;\n\t\t\tif (unlikely(np->put_rx_ctx++ == np->last_rx_ctx))\n\t\t\t\tnp->put_rx_ctx = np->rx_skb;\n\t\t} else {\npacket_dropped:\n\t\t\tu64_stats_update_begin(&np->swstats_rx_syncp);\n\t\t\tnv_txrx_stats_inc(stat_rx_dropped);\n\t\t\tu64_stats_update_end(&np->swstats_rx_syncp);\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic void nv_do_rx_refill(struct timer_list *t)\n{\n\tstruct fe_priv *np = from_timer(np, t, oom_kick);\n\n\t \n\tnapi_schedule(&np->napi);\n}\n\nstatic void nv_init_rx(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tint i;\n\n\tnp->get_rx = np->rx_ring;\n\tnp->put_rx = np->rx_ring;\n\n\tif (!nv_optimized(np))\n\t\tnp->last_rx.orig = &np->rx_ring.orig[np->rx_ring_size-1];\n\telse\n\t\tnp->last_rx.ex = &np->rx_ring.ex[np->rx_ring_size-1];\n\tnp->get_rx_ctx = np->rx_skb;\n\tnp->put_rx_ctx = np->rx_skb;\n\tnp->last_rx_ctx = &np->rx_skb[np->rx_ring_size-1];\n\n\tfor (i = 0; i < np->rx_ring_size; i++) {\n\t\tif (!nv_optimized(np)) {\n\t\t\tnp->rx_ring.orig[i].flaglen = 0;\n\t\t\tnp->rx_ring.orig[i].buf = 0;\n\t\t} else {\n\t\t\tnp->rx_ring.ex[i].flaglen = 0;\n\t\t\tnp->rx_ring.ex[i].txvlan = 0;\n\t\t\tnp->rx_ring.ex[i].bufhigh = 0;\n\t\t\tnp->rx_ring.ex[i].buflow = 0;\n\t\t}\n\t\tnp->rx_skb[i].skb = NULL;\n\t\tnp->rx_skb[i].dma = 0;\n\t}\n}\n\nstatic void nv_init_tx(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tint i;\n\n\tnp->get_tx = np->tx_ring;\n\tnp->put_tx = np->tx_ring;\n\n\tif (!nv_optimized(np))\n\t\tnp->last_tx.orig = &np->tx_ring.orig[np->tx_ring_size-1];\n\telse\n\t\tnp->last_tx.ex = &np->tx_ring.ex[np->tx_ring_size-1];\n\tnp->get_tx_ctx = np->tx_skb;\n\tnp->put_tx_ctx = np->tx_skb;\n\tnp->last_tx_ctx = &np->tx_skb[np->tx_ring_size-1];\n\tnetdev_reset_queue(np->dev);\n\tnp->tx_pkts_in_progress = 0;\n\tnp->tx_change_owner = NULL;\n\tnp->tx_end_flip = NULL;\n\tnp->tx_stop = 0;\n\n\tfor (i = 0; i < np->tx_ring_size; i++) {\n\t\tif (!nv_optimized(np)) {\n\t\t\tnp->tx_ring.orig[i].flaglen = 0;\n\t\t\tnp->tx_ring.orig[i].buf = 0;\n\t\t} else {\n\t\t\tnp->tx_ring.ex[i].flaglen = 0;\n\t\t\tnp->tx_ring.ex[i].txvlan = 0;\n\t\t\tnp->tx_ring.ex[i].bufhigh = 0;\n\t\t\tnp->tx_ring.ex[i].buflow = 0;\n\t\t}\n\t\tnp->tx_skb[i].skb = NULL;\n\t\tnp->tx_skb[i].dma = 0;\n\t\tnp->tx_skb[i].dma_len = 0;\n\t\tnp->tx_skb[i].dma_single = 0;\n\t\tnp->tx_skb[i].first_tx_desc = NULL;\n\t\tnp->tx_skb[i].next_tx_ctx = NULL;\n\t}\n}\n\nstatic int nv_init_ring(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tnv_init_tx(dev);\n\tnv_init_rx(dev);\n\n\tif (!nv_optimized(np))\n\t\treturn nv_alloc_rx(dev);\n\telse\n\t\treturn nv_alloc_rx_optimized(dev);\n}\n\nstatic void nv_unmap_txskb(struct fe_priv *np, struct nv_skb_map *tx_skb)\n{\n\tif (tx_skb->dma) {\n\t\tif (tx_skb->dma_single)\n\t\t\tdma_unmap_single(&np->pci_dev->dev, tx_skb->dma,\n\t\t\t\t\t tx_skb->dma_len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_page(&np->pci_dev->dev, tx_skb->dma,\n\t\t\t\t       tx_skb->dma_len,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\ttx_skb->dma = 0;\n\t}\n}\n\nstatic int nv_release_txskb(struct fe_priv *np, struct nv_skb_map *tx_skb)\n{\n\tnv_unmap_txskb(np, tx_skb);\n\tif (tx_skb->skb) {\n\t\tdev_kfree_skb_any(tx_skb->skb);\n\t\ttx_skb->skb = NULL;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void nv_drain_tx(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tunsigned int i;\n\n\tfor (i = 0; i < np->tx_ring_size; i++) {\n\t\tif (!nv_optimized(np)) {\n\t\t\tnp->tx_ring.orig[i].flaglen = 0;\n\t\t\tnp->tx_ring.orig[i].buf = 0;\n\t\t} else {\n\t\t\tnp->tx_ring.ex[i].flaglen = 0;\n\t\t\tnp->tx_ring.ex[i].txvlan = 0;\n\t\t\tnp->tx_ring.ex[i].bufhigh = 0;\n\t\t\tnp->tx_ring.ex[i].buflow = 0;\n\t\t}\n\t\tif (nv_release_txskb(np, &np->tx_skb[i])) {\n\t\t\tu64_stats_update_begin(&np->swstats_tx_syncp);\n\t\t\tnv_txrx_stats_inc(stat_tx_dropped);\n\t\t\tu64_stats_update_end(&np->swstats_tx_syncp);\n\t\t}\n\t\tnp->tx_skb[i].dma = 0;\n\t\tnp->tx_skb[i].dma_len = 0;\n\t\tnp->tx_skb[i].dma_single = 0;\n\t\tnp->tx_skb[i].first_tx_desc = NULL;\n\t\tnp->tx_skb[i].next_tx_ctx = NULL;\n\t}\n\tnp->tx_pkts_in_progress = 0;\n\tnp->tx_change_owner = NULL;\n\tnp->tx_end_flip = NULL;\n}\n\nstatic void nv_drain_rx(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < np->rx_ring_size; i++) {\n\t\tif (!nv_optimized(np)) {\n\t\t\tnp->rx_ring.orig[i].flaglen = 0;\n\t\t\tnp->rx_ring.orig[i].buf = 0;\n\t\t} else {\n\t\t\tnp->rx_ring.ex[i].flaglen = 0;\n\t\t\tnp->rx_ring.ex[i].txvlan = 0;\n\t\t\tnp->rx_ring.ex[i].bufhigh = 0;\n\t\t\tnp->rx_ring.ex[i].buflow = 0;\n\t\t}\n\t\twmb();\n\t\tif (np->rx_skb[i].skb) {\n\t\t\tdma_unmap_single(&np->pci_dev->dev, np->rx_skb[i].dma,\n\t\t\t\t\t (skb_end_pointer(np->rx_skb[i].skb) -\n\t\t\t\t\t np->rx_skb[i].skb->data),\n\t\t\t\t\t DMA_FROM_DEVICE);\n\t\t\tdev_kfree_skb(np->rx_skb[i].skb);\n\t\t\tnp->rx_skb[i].skb = NULL;\n\t\t}\n\t}\n}\n\nstatic void nv_drain_rxtx(struct net_device *dev)\n{\n\tnv_drain_tx(dev);\n\tnv_drain_rx(dev);\n}\n\nstatic inline u32 nv_get_empty_tx_slots(struct fe_priv *np)\n{\n\treturn (u32)(np->tx_ring_size - ((np->tx_ring_size + (np->put_tx_ctx - np->get_tx_ctx)) % np->tx_ring_size));\n}\n\nstatic void nv_legacybackoff_reseed(struct net_device *dev)\n{\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 reg;\n\tu32 low;\n\tint tx_status = 0;\n\n\treg = readl(base + NvRegSlotTime) & ~NVREG_SLOTTIME_MASK;\n\tget_random_bytes(&low, sizeof(low));\n\treg |= low & NVREG_SLOTTIME_MASK;\n\n\t \n\ttx_status = readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_START;\n\tif (tx_status)\n\t\tnv_stop_tx(dev);\n\tnv_stop_rx(dev);\n\twritel(reg, base + NvRegSlotTime);\n\tif (tx_status)\n\t\tnv_start_tx(dev);\n\tnv_start_rx(dev);\n}\n\n \n#define BACKOFF_SEEDSET_ROWS\t8\n#define BACKOFF_SEEDSET_LFSRS\t15\n\n \nstatic const u32 main_seedset[BACKOFF_SEEDSET_ROWS][BACKOFF_SEEDSET_LFSRS] = {\n\t{145, 155, 165, 175, 185, 196, 235, 245, 255, 265, 275, 285, 660, 690, 874},\n\t{245, 255, 265, 575, 385, 298, 335, 345, 355, 366, 375, 385, 761, 790, 974},\n\t{145, 155, 165, 175, 185, 196, 235, 245, 255, 265, 275, 285, 660, 690, 874},\n\t{245, 255, 265, 575, 385, 298, 335, 345, 355, 366, 375, 386, 761, 790, 974},\n\t{266, 265, 276, 585, 397, 208, 345, 355, 365, 376, 385, 396, 771, 700, 984},\n\t{266, 265, 276, 586, 397, 208, 346, 355, 365, 376, 285, 396, 771, 700, 984},\n\t{366, 365, 376, 686, 497, 308, 447, 455, 466, 476, 485, 496, 871, 800,  84},\n\t{466, 465, 476, 786, 597, 408, 547, 555, 566, 576, 585, 597, 971, 900, 184} };\n\nstatic const u32 gear_seedset[BACKOFF_SEEDSET_ROWS][BACKOFF_SEEDSET_LFSRS] = {\n\t{251, 262, 273, 324, 319, 508, 375, 364, 341, 371, 398, 193, 375,  30, 295},\n\t{351, 375, 373, 469, 551, 639, 477, 464, 441, 472, 498, 293, 476, 130, 395},\n\t{351, 375, 373, 469, 551, 639, 477, 464, 441, 472, 498, 293, 476, 130, 397},\n\t{251, 262, 273, 324, 319, 508, 375, 364, 341, 371, 398, 193, 375,  30, 295},\n\t{251, 262, 273, 324, 319, 508, 375, 364, 341, 371, 398, 193, 375,  30, 295},\n\t{351, 375, 373, 469, 551, 639, 477, 464, 441, 472, 498, 293, 476, 130, 395},\n\t{351, 375, 373, 469, 551, 639, 477, 464, 441, 472, 498, 293, 476, 130, 395},\n\t{351, 375, 373, 469, 551, 639, 477, 464, 441, 472, 498, 293, 476, 130, 395} };\n\nstatic void nv_gear_backoff_reseed(struct net_device *dev)\n{\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 miniseed1, miniseed2, miniseed2_reversed, miniseed3, miniseed3_reversed;\n\tu32 temp, seedset, combinedSeed;\n\tint i;\n\n\t \n\t \n\tget_random_bytes(&miniseed1, sizeof(miniseed1));\n\tminiseed1 &= 0x0fff;\n\tif (miniseed1 == 0)\n\t\tminiseed1 = 0xabc;\n\n\tget_random_bytes(&miniseed2, sizeof(miniseed2));\n\tminiseed2 &= 0x0fff;\n\tif (miniseed2 == 0)\n\t\tminiseed2 = 0xabc;\n\tminiseed2_reversed =\n\t\t((miniseed2 & 0xF00) >> 8) |\n\t\t (miniseed2 & 0x0F0) |\n\t\t ((miniseed2 & 0x00F) << 8);\n\n\tget_random_bytes(&miniseed3, sizeof(miniseed3));\n\tminiseed3 &= 0x0fff;\n\tif (miniseed3 == 0)\n\t\tminiseed3 = 0xabc;\n\tminiseed3_reversed =\n\t\t((miniseed3 & 0xF00) >> 8) |\n\t\t (miniseed3 & 0x0F0) |\n\t\t ((miniseed3 & 0x00F) << 8);\n\n\tcombinedSeed = ((miniseed1 ^ miniseed2_reversed) << 12) |\n\t\t       (miniseed2 ^ miniseed3_reversed);\n\n\t \n\tif ((combinedSeed & NVREG_BKOFFCTRL_SEED_MASK) == 0)\n\t\tcombinedSeed |= 0x08;\n\tif ((combinedSeed & (NVREG_BKOFFCTRL_SEED_MASK << NVREG_BKOFFCTRL_GEAR)) == 0)\n\t\tcombinedSeed |= 0x8000;\n\n\t \n\ttemp = NVREG_BKOFFCTRL_DEFAULT | (0 << NVREG_BKOFFCTRL_SELECT);\n\ttemp |= combinedSeed & NVREG_BKOFFCTRL_SEED_MASK;\n\ttemp |= combinedSeed >> NVREG_BKOFFCTRL_GEAR;\n\twritel(temp, base + NvRegBackOffControl);\n\n\t \n\tget_random_bytes(&seedset, sizeof(seedset));\n\tseedset = seedset % BACKOFF_SEEDSET_ROWS;\n\tfor (i = 1; i <= BACKOFF_SEEDSET_LFSRS; i++) {\n\t\ttemp = NVREG_BKOFFCTRL_DEFAULT | (i << NVREG_BKOFFCTRL_SELECT);\n\t\ttemp |= main_seedset[seedset][i-1] & 0x3ff;\n\t\ttemp |= ((gear_seedset[seedset][i-1] & 0x3ff) << NVREG_BKOFFCTRL_GEAR);\n\t\twritel(temp, base + NvRegBackOffControl);\n\t}\n}\n\n \nstatic netdev_tx_t nv_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu32 tx_flags = 0;\n\tu32 tx_flags_extra = (np->desc_ver == DESC_VER_1 ? NV_TX_LASTPACKET : NV_TX2_LASTPACKET);\n\tunsigned int fragments = skb_shinfo(skb)->nr_frags;\n\tunsigned int i;\n\tu32 offset = 0;\n\tu32 bcnt;\n\tu32 size = skb_headlen(skb);\n\tu32 entries = (size >> NV_TX2_TSO_MAX_SHIFT) + ((size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);\n\tu32 empty_slots;\n\tstruct ring_desc *put_tx;\n\tstruct ring_desc *start_tx;\n\tstruct ring_desc *prev_tx;\n\tstruct nv_skb_map *prev_tx_ctx;\n\tstruct nv_skb_map *tmp_tx_ctx = NULL, *start_tx_ctx = NULL;\n\tunsigned long flags;\n\tnetdev_tx_t ret = NETDEV_TX_OK;\n\n\t \n\tfor (i = 0; i < fragments; i++) {\n\t\tu32 frag_size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tentries += (frag_size >> NV_TX2_TSO_MAX_SHIFT) +\n\t\t\t   ((frag_size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);\n\t}\n\n\tspin_lock_irqsave(&np->lock, flags);\n\tempty_slots = nv_get_empty_tx_slots(np);\n\tif (unlikely(empty_slots <= entries)) {\n\t\tnetif_stop_queue(dev);\n\t\tnp->tx_stop = 1;\n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\n\t\t \n\t\tret = NETDEV_TX_BUSY;\n\t\tgoto txkick;\n\t}\n\tspin_unlock_irqrestore(&np->lock, flags);\n\n\tstart_tx = put_tx = np->put_tx.orig;\n\n\t \n\tdo {\n\t\tbcnt = (size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : size;\n\t\tnp->put_tx_ctx->dma = dma_map_single(&np->pci_dev->dev,\n\t\t\t\t\t\t     skb->data + offset, bcnt,\n\t\t\t\t\t\t     DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(&np->pci_dev->dev,\n\t\t\t\t\t       np->put_tx_ctx->dma))) {\n\t\t\t \n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tu64_stats_update_begin(&np->swstats_tx_syncp);\n\t\t\tnv_txrx_stats_inc(stat_tx_dropped);\n\t\t\tu64_stats_update_end(&np->swstats_tx_syncp);\n\n\t\t\tret = NETDEV_TX_OK;\n\n\t\t\tgoto dma_error;\n\t\t}\n\t\tnp->put_tx_ctx->dma_len = bcnt;\n\t\tnp->put_tx_ctx->dma_single = 1;\n\t\tput_tx->buf = cpu_to_le32(np->put_tx_ctx->dma);\n\t\tput_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);\n\n\t\ttx_flags = np->tx_flags;\n\t\toffset += bcnt;\n\t\tsize -= bcnt;\n\t\tif (unlikely(put_tx++ == np->last_tx.orig))\n\t\t\tput_tx = np->tx_ring.orig;\n\t\tif (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))\n\t\t\tnp->put_tx_ctx = np->tx_skb;\n\t} while (size);\n\n\t \n\tfor (i = 0; i < fragments; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tu32 frag_size = skb_frag_size(frag);\n\t\toffset = 0;\n\n\t\tdo {\n\t\t\tif (!start_tx_ctx)\n\t\t\t\tstart_tx_ctx = tmp_tx_ctx = np->put_tx_ctx;\n\n\t\t\tbcnt = (frag_size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : frag_size;\n\t\t\tnp->put_tx_ctx->dma = skb_frag_dma_map(\n\t\t\t\t\t\t\t&np->pci_dev->dev,\n\t\t\t\t\t\t\tfrag, offset,\n\t\t\t\t\t\t\tbcnt,\n\t\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\tif (unlikely(dma_mapping_error(&np->pci_dev->dev,\n\t\t\t\t\t\t       np->put_tx_ctx->dma))) {\n\n\t\t\t\t \n\t\t\t\tdo {\n\t\t\t\t\tnv_unmap_txskb(np, start_tx_ctx);\n\t\t\t\t\tif (unlikely(tmp_tx_ctx++ == np->last_tx_ctx))\n\t\t\t\t\t\ttmp_tx_ctx = np->tx_skb;\n\t\t\t\t} while (tmp_tx_ctx != np->put_tx_ctx);\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\t\tnp->put_tx_ctx = start_tx_ctx;\n\t\t\t\tu64_stats_update_begin(&np->swstats_tx_syncp);\n\t\t\t\tnv_txrx_stats_inc(stat_tx_dropped);\n\t\t\t\tu64_stats_update_end(&np->swstats_tx_syncp);\n\n\t\t\t\tret = NETDEV_TX_OK;\n\n\t\t\t\tgoto dma_error;\n\t\t\t}\n\n\t\t\tnp->put_tx_ctx->dma_len = bcnt;\n\t\t\tnp->put_tx_ctx->dma_single = 0;\n\t\t\tput_tx->buf = cpu_to_le32(np->put_tx_ctx->dma);\n\t\t\tput_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);\n\n\t\t\toffset += bcnt;\n\t\t\tfrag_size -= bcnt;\n\t\t\tif (unlikely(put_tx++ == np->last_tx.orig))\n\t\t\t\tput_tx = np->tx_ring.orig;\n\t\t\tif (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))\n\t\t\t\tnp->put_tx_ctx = np->tx_skb;\n\t\t} while (frag_size);\n\t}\n\n\tif (unlikely(put_tx == np->tx_ring.orig))\n\t\tprev_tx = np->last_tx.orig;\n\telse\n\t\tprev_tx = put_tx - 1;\n\n\tif (unlikely(np->put_tx_ctx == np->tx_skb))\n\t\tprev_tx_ctx = np->last_tx_ctx;\n\telse\n\t\tprev_tx_ctx = np->put_tx_ctx - 1;\n\n\t \n\tprev_tx->flaglen |= cpu_to_le32(tx_flags_extra);\n\n\t \n\tprev_tx_ctx->skb = skb;\n\n\tif (skb_is_gso(skb))\n\t\ttx_flags_extra = NV_TX2_TSO | (skb_shinfo(skb)->gso_size << NV_TX2_TSO_SHIFT);\n\telse\n\t\ttx_flags_extra = skb->ip_summed == CHECKSUM_PARTIAL ?\n\t\t\t NV_TX2_CHECKSUM_L3 | NV_TX2_CHECKSUM_L4 : 0;\n\n\tspin_lock_irqsave(&np->lock, flags);\n\n\t \n\tstart_tx->flaglen |= cpu_to_le32(tx_flags | tx_flags_extra);\n\n\tnetdev_sent_queue(np->dev, skb->len);\n\n\tskb_tx_timestamp(skb);\n\n\tnp->put_tx.orig = put_tx;\n\n\tspin_unlock_irqrestore(&np->lock, flags);\n\ntxkick:\n\tif (netif_queue_stopped(dev) || !netdev_xmit_more()) {\n\t\tu32 txrxctl_kick;\ndma_error:\n\t\ttxrxctl_kick = NVREG_TXRXCTL_KICK | np->txrxctl_bits;\n\t\twritel(txrxctl_kick, get_hwbase(dev) + NvRegTxRxControl);\n\t}\n\n\treturn ret;\n}\n\nstatic netdev_tx_t nv_start_xmit_optimized(struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu32 tx_flags = 0;\n\tu32 tx_flags_extra;\n\tunsigned int fragments = skb_shinfo(skb)->nr_frags;\n\tunsigned int i;\n\tu32 offset = 0;\n\tu32 bcnt;\n\tu32 size = skb_headlen(skb);\n\tu32 entries = (size >> NV_TX2_TSO_MAX_SHIFT) + ((size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);\n\tu32 empty_slots;\n\tstruct ring_desc_ex *put_tx;\n\tstruct ring_desc_ex *start_tx;\n\tstruct ring_desc_ex *prev_tx;\n\tstruct nv_skb_map *prev_tx_ctx;\n\tstruct nv_skb_map *start_tx_ctx = NULL;\n\tstruct nv_skb_map *tmp_tx_ctx = NULL;\n\tunsigned long flags;\n\tnetdev_tx_t ret = NETDEV_TX_OK;\n\n\t \n\tfor (i = 0; i < fragments; i++) {\n\t\tu32 frag_size = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tentries += (frag_size >> NV_TX2_TSO_MAX_SHIFT) +\n\t\t\t   ((frag_size & (NV_TX2_TSO_MAX_SIZE-1)) ? 1 : 0);\n\t}\n\n\tspin_lock_irqsave(&np->lock, flags);\n\tempty_slots = nv_get_empty_tx_slots(np);\n\tif (unlikely(empty_slots <= entries)) {\n\t\tnetif_stop_queue(dev);\n\t\tnp->tx_stop = 1;\n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\n\t\t \n\t\tret = NETDEV_TX_BUSY;\n\n\t\tgoto txkick;\n\t}\n\tspin_unlock_irqrestore(&np->lock, flags);\n\n\tstart_tx = put_tx = np->put_tx.ex;\n\tstart_tx_ctx = np->put_tx_ctx;\n\n\t \n\tdo {\n\t\tbcnt = (size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : size;\n\t\tnp->put_tx_ctx->dma = dma_map_single(&np->pci_dev->dev,\n\t\t\t\t\t\t     skb->data + offset, bcnt,\n\t\t\t\t\t\t     DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(&np->pci_dev->dev,\n\t\t\t\t\t       np->put_tx_ctx->dma))) {\n\t\t\t \n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tu64_stats_update_begin(&np->swstats_tx_syncp);\n\t\t\tnv_txrx_stats_inc(stat_tx_dropped);\n\t\t\tu64_stats_update_end(&np->swstats_tx_syncp);\n\n\t\t\tret = NETDEV_TX_OK;\n\n\t\t\tgoto dma_error;\n\t\t}\n\t\tnp->put_tx_ctx->dma_len = bcnt;\n\t\tnp->put_tx_ctx->dma_single = 1;\n\t\tput_tx->bufhigh = cpu_to_le32(dma_high(np->put_tx_ctx->dma));\n\t\tput_tx->buflow = cpu_to_le32(dma_low(np->put_tx_ctx->dma));\n\t\tput_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);\n\n\t\ttx_flags = NV_TX2_VALID;\n\t\toffset += bcnt;\n\t\tsize -= bcnt;\n\t\tif (unlikely(put_tx++ == np->last_tx.ex))\n\t\t\tput_tx = np->tx_ring.ex;\n\t\tif (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))\n\t\t\tnp->put_tx_ctx = np->tx_skb;\n\t} while (size);\n\n\t \n\tfor (i = 0; i < fragments; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tu32 frag_size = skb_frag_size(frag);\n\t\toffset = 0;\n\n\t\tdo {\n\t\t\tbcnt = (frag_size > NV_TX2_TSO_MAX_SIZE) ? NV_TX2_TSO_MAX_SIZE : frag_size;\n\t\t\tif (!start_tx_ctx)\n\t\t\t\tstart_tx_ctx = tmp_tx_ctx = np->put_tx_ctx;\n\t\t\tnp->put_tx_ctx->dma = skb_frag_dma_map(\n\t\t\t\t\t\t\t&np->pci_dev->dev,\n\t\t\t\t\t\t\tfrag, offset,\n\t\t\t\t\t\t\tbcnt,\n\t\t\t\t\t\t\tDMA_TO_DEVICE);\n\n\t\t\tif (unlikely(dma_mapping_error(&np->pci_dev->dev,\n\t\t\t\t\t\t       np->put_tx_ctx->dma))) {\n\n\t\t\t\t \n\t\t\t\tdo {\n\t\t\t\t\tnv_unmap_txskb(np, start_tx_ctx);\n\t\t\t\t\tif (unlikely(tmp_tx_ctx++ == np->last_tx_ctx))\n\t\t\t\t\t\ttmp_tx_ctx = np->tx_skb;\n\t\t\t\t} while (tmp_tx_ctx != np->put_tx_ctx);\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\t\tnp->put_tx_ctx = start_tx_ctx;\n\t\t\t\tu64_stats_update_begin(&np->swstats_tx_syncp);\n\t\t\t\tnv_txrx_stats_inc(stat_tx_dropped);\n\t\t\t\tu64_stats_update_end(&np->swstats_tx_syncp);\n\n\t\t\t\tret = NETDEV_TX_OK;\n\n\t\t\t\tgoto dma_error;\n\t\t\t}\n\t\t\tnp->put_tx_ctx->dma_len = bcnt;\n\t\t\tnp->put_tx_ctx->dma_single = 0;\n\t\t\tput_tx->bufhigh = cpu_to_le32(dma_high(np->put_tx_ctx->dma));\n\t\t\tput_tx->buflow = cpu_to_le32(dma_low(np->put_tx_ctx->dma));\n\t\t\tput_tx->flaglen = cpu_to_le32((bcnt-1) | tx_flags);\n\n\t\t\toffset += bcnt;\n\t\t\tfrag_size -= bcnt;\n\t\t\tif (unlikely(put_tx++ == np->last_tx.ex))\n\t\t\t\tput_tx = np->tx_ring.ex;\n\t\t\tif (unlikely(np->put_tx_ctx++ == np->last_tx_ctx))\n\t\t\t\tnp->put_tx_ctx = np->tx_skb;\n\t\t} while (frag_size);\n\t}\n\n\tif (unlikely(put_tx == np->tx_ring.ex))\n\t\tprev_tx = np->last_tx.ex;\n\telse\n\t\tprev_tx = put_tx - 1;\n\n\tif (unlikely(np->put_tx_ctx == np->tx_skb))\n\t\tprev_tx_ctx = np->last_tx_ctx;\n\telse\n\t\tprev_tx_ctx = np->put_tx_ctx - 1;\n\n\t \n\tprev_tx->flaglen |= cpu_to_le32(NV_TX2_LASTPACKET);\n\n\t \n\tprev_tx_ctx->skb = skb;\n\n\tif (skb_is_gso(skb))\n\t\ttx_flags_extra = NV_TX2_TSO | (skb_shinfo(skb)->gso_size << NV_TX2_TSO_SHIFT);\n\telse\n\t\ttx_flags_extra = skb->ip_summed == CHECKSUM_PARTIAL ?\n\t\t\t NV_TX2_CHECKSUM_L3 | NV_TX2_CHECKSUM_L4 : 0;\n\n\t \n\tif (skb_vlan_tag_present(skb))\n\t\tstart_tx->txvlan = cpu_to_le32(NV_TX3_VLAN_TAG_PRESENT |\n\t\t\t\t\tskb_vlan_tag_get(skb));\n\telse\n\t\tstart_tx->txvlan = 0;\n\n\tspin_lock_irqsave(&np->lock, flags);\n\n\tif (np->tx_limit) {\n\t\t \n\n\t\tif (np->tx_pkts_in_progress == NV_TX_LIMIT_COUNT) {\n\t\t\tif (!np->tx_change_owner)\n\t\t\t\tnp->tx_change_owner = start_tx_ctx;\n\n\t\t\t \n\t\t\ttx_flags &= ~NV_TX2_VALID;\n\t\t\tstart_tx_ctx->first_tx_desc = start_tx;\n\t\t\tstart_tx_ctx->next_tx_ctx = np->put_tx_ctx;\n\t\t\tnp->tx_end_flip = np->put_tx_ctx;\n\t\t} else {\n\t\t\tnp->tx_pkts_in_progress++;\n\t\t}\n\t}\n\n\t \n\tstart_tx->flaglen |= cpu_to_le32(tx_flags | tx_flags_extra);\n\n\tnetdev_sent_queue(np->dev, skb->len);\n\n\tskb_tx_timestamp(skb);\n\n\tnp->put_tx.ex = put_tx;\n\n\tspin_unlock_irqrestore(&np->lock, flags);\n\ntxkick:\n\tif (netif_queue_stopped(dev) || !netdev_xmit_more()) {\n\t\tu32 txrxctl_kick;\ndma_error:\n\t\ttxrxctl_kick = NVREG_TXRXCTL_KICK | np->txrxctl_bits;\n\t\twritel(txrxctl_kick, get_hwbase(dev) + NvRegTxRxControl);\n\t}\n\n\treturn ret;\n}\n\nstatic inline void nv_tx_flip_ownership(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tnp->tx_pkts_in_progress--;\n\tif (np->tx_change_owner) {\n\t\tnp->tx_change_owner->first_tx_desc->flaglen |=\n\t\t\tcpu_to_le32(NV_TX2_VALID);\n\t\tnp->tx_pkts_in_progress++;\n\n\t\tnp->tx_change_owner = np->tx_change_owner->next_tx_ctx;\n\t\tif (np->tx_change_owner == np->tx_end_flip)\n\t\t\tnp->tx_change_owner = NULL;\n\n\t\twritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\n\t}\n}\n\n \nstatic int nv_tx_done(struct net_device *dev, int limit)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu32 flags;\n\tint tx_work = 0;\n\tstruct ring_desc *orig_get_tx = np->get_tx.orig;\n\tunsigned int bytes_compl = 0;\n\n\twhile ((np->get_tx.orig != np->put_tx.orig) &&\n\t       !((flags = le32_to_cpu(np->get_tx.orig->flaglen)) & NV_TX_VALID) &&\n\t       (tx_work < limit)) {\n\n\t\tnv_unmap_txskb(np, np->get_tx_ctx);\n\n\t\tif (np->desc_ver == DESC_VER_1) {\n\t\t\tif (flags & NV_TX_LASTPACKET) {\n\t\t\t\tif (unlikely(flags & NV_TX_ERROR)) {\n\t\t\t\t\tif ((flags & NV_TX_RETRYERROR)\n\t\t\t\t\t    && !(flags & NV_TX_RETRYCOUNT_MASK))\n\t\t\t\t\t\tnv_legacybackoff_reseed(dev);\n\t\t\t\t} else {\n\t\t\t\t\tunsigned int len;\n\n\t\t\t\t\tu64_stats_update_begin(&np->swstats_tx_syncp);\n\t\t\t\t\tnv_txrx_stats_inc(stat_tx_packets);\n\t\t\t\t\tlen = np->get_tx_ctx->skb->len;\n\t\t\t\t\tnv_txrx_stats_add(stat_tx_bytes, len);\n\t\t\t\t\tu64_stats_update_end(&np->swstats_tx_syncp);\n\t\t\t\t}\n\t\t\t\tbytes_compl += np->get_tx_ctx->skb->len;\n\t\t\t\tdev_kfree_skb_any(np->get_tx_ctx->skb);\n\t\t\t\tnp->get_tx_ctx->skb = NULL;\n\t\t\t\ttx_work++;\n\t\t\t}\n\t\t} else {\n\t\t\tif (flags & NV_TX2_LASTPACKET) {\n\t\t\t\tif (unlikely(flags & NV_TX2_ERROR)) {\n\t\t\t\t\tif ((flags & NV_TX2_RETRYERROR)\n\t\t\t\t\t    && !(flags & NV_TX2_RETRYCOUNT_MASK))\n\t\t\t\t\t\tnv_legacybackoff_reseed(dev);\n\t\t\t\t} else {\n\t\t\t\t\tunsigned int len;\n\n\t\t\t\t\tu64_stats_update_begin(&np->swstats_tx_syncp);\n\t\t\t\t\tnv_txrx_stats_inc(stat_tx_packets);\n\t\t\t\t\tlen = np->get_tx_ctx->skb->len;\n\t\t\t\t\tnv_txrx_stats_add(stat_tx_bytes, len);\n\t\t\t\t\tu64_stats_update_end(&np->swstats_tx_syncp);\n\t\t\t\t}\n\t\t\t\tbytes_compl += np->get_tx_ctx->skb->len;\n\t\t\t\tdev_kfree_skb_any(np->get_tx_ctx->skb);\n\t\t\t\tnp->get_tx_ctx->skb = NULL;\n\t\t\t\ttx_work++;\n\t\t\t}\n\t\t}\n\t\tif (unlikely(np->get_tx.orig++ == np->last_tx.orig))\n\t\t\tnp->get_tx.orig = np->tx_ring.orig;\n\t\tif (unlikely(np->get_tx_ctx++ == np->last_tx_ctx))\n\t\t\tnp->get_tx_ctx = np->tx_skb;\n\t}\n\n\tnetdev_completed_queue(np->dev, tx_work, bytes_compl);\n\n\tif (unlikely((np->tx_stop == 1) && (np->get_tx.orig != orig_get_tx))) {\n\t\tnp->tx_stop = 0;\n\t\tnetif_wake_queue(dev);\n\t}\n\treturn tx_work;\n}\n\nstatic int nv_tx_done_optimized(struct net_device *dev, int limit)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu32 flags;\n\tint tx_work = 0;\n\tstruct ring_desc_ex *orig_get_tx = np->get_tx.ex;\n\tunsigned long bytes_cleaned = 0;\n\n\twhile ((np->get_tx.ex != np->put_tx.ex) &&\n\t       !((flags = le32_to_cpu(np->get_tx.ex->flaglen)) & NV_TX2_VALID) &&\n\t       (tx_work < limit)) {\n\n\t\tnv_unmap_txskb(np, np->get_tx_ctx);\n\n\t\tif (flags & NV_TX2_LASTPACKET) {\n\t\t\tif (unlikely(flags & NV_TX2_ERROR)) {\n\t\t\t\tif ((flags & NV_TX2_RETRYERROR)\n\t\t\t\t    && !(flags & NV_TX2_RETRYCOUNT_MASK)) {\n\t\t\t\t\tif (np->driver_data & DEV_HAS_GEAR_MODE)\n\t\t\t\t\t\tnv_gear_backoff_reseed(dev);\n\t\t\t\t\telse\n\t\t\t\t\t\tnv_legacybackoff_reseed(dev);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tunsigned int len;\n\n\t\t\t\tu64_stats_update_begin(&np->swstats_tx_syncp);\n\t\t\t\tnv_txrx_stats_inc(stat_tx_packets);\n\t\t\t\tlen = np->get_tx_ctx->skb->len;\n\t\t\t\tnv_txrx_stats_add(stat_tx_bytes, len);\n\t\t\t\tu64_stats_update_end(&np->swstats_tx_syncp);\n\t\t\t}\n\n\t\t\tbytes_cleaned += np->get_tx_ctx->skb->len;\n\t\t\tdev_kfree_skb_any(np->get_tx_ctx->skb);\n\t\t\tnp->get_tx_ctx->skb = NULL;\n\t\t\ttx_work++;\n\n\t\t\tif (np->tx_limit)\n\t\t\t\tnv_tx_flip_ownership(dev);\n\t\t}\n\n\t\tif (unlikely(np->get_tx.ex++ == np->last_tx.ex))\n\t\t\tnp->get_tx.ex = np->tx_ring.ex;\n\t\tif (unlikely(np->get_tx_ctx++ == np->last_tx_ctx))\n\t\t\tnp->get_tx_ctx = np->tx_skb;\n\t}\n\n\tnetdev_completed_queue(np->dev, tx_work, bytes_cleaned);\n\n\tif (unlikely((np->tx_stop == 1) && (np->get_tx.ex != orig_get_tx))) {\n\t\tnp->tx_stop = 0;\n\t\tnetif_wake_queue(dev);\n\t}\n\treturn tx_work;\n}\n\n \nstatic void nv_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 status;\n\tunion ring_type put_tx;\n\tint saved_tx_limit;\n\n\tif (np->msi_flags & NV_MSI_X_ENABLED)\n\t\tstatus = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQSTAT_MASK;\n\telse\n\t\tstatus = readl(base + NvRegIrqStatus) & NVREG_IRQSTAT_MASK;\n\n\tnetdev_warn(dev, \"Got tx_timeout. irq status: %08x\\n\", status);\n\n\tif (unlikely(debug_tx_timeout)) {\n\t\tint i;\n\n\t\tnetdev_info(dev, \"Ring at %lx\\n\", (unsigned long)np->ring_addr);\n\t\tnetdev_info(dev, \"Dumping tx registers\\n\");\n\t\tfor (i = 0; i <= np->register_size; i += 32) {\n\t\t\tnetdev_info(dev,\n\t\t\t\t    \"%3x: %08x %08x %08x %08x \"\n\t\t\t\t    \"%08x %08x %08x %08x\\n\",\n\t\t\t\t    i,\n\t\t\t\t    readl(base + i + 0), readl(base + i + 4),\n\t\t\t\t    readl(base + i + 8), readl(base + i + 12),\n\t\t\t\t    readl(base + i + 16), readl(base + i + 20),\n\t\t\t\t    readl(base + i + 24), readl(base + i + 28));\n\t\t}\n\t\tnetdev_info(dev, \"Dumping tx ring\\n\");\n\t\tfor (i = 0; i < np->tx_ring_size; i += 4) {\n\t\t\tif (!nv_optimized(np)) {\n\t\t\t\tnetdev_info(dev,\n\t\t\t\t\t    \"%03x: %08x %08x // %08x %08x \"\n\t\t\t\t\t    \"// %08x %08x // %08x %08x\\n\",\n\t\t\t\t\t    i,\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.orig[i].buf),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.orig[i].flaglen),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.orig[i+1].buf),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.orig[i+1].flaglen),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.orig[i+2].buf),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.orig[i+2].flaglen),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.orig[i+3].buf),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.orig[i+3].flaglen));\n\t\t\t} else {\n\t\t\t\tnetdev_info(dev,\n\t\t\t\t\t    \"%03x: %08x %08x %08x \"\n\t\t\t\t\t    \"// %08x %08x %08x \"\n\t\t\t\t\t    \"// %08x %08x %08x \"\n\t\t\t\t\t    \"// %08x %08x %08x\\n\",\n\t\t\t\t\t    i,\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i].bufhigh),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i].buflow),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i].flaglen),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i+1].bufhigh),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i+1].buflow),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i+1].flaglen),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i+2].bufhigh),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i+2].buflow),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i+2].flaglen),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i+3].bufhigh),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i+3].buflow),\n\t\t\t\t\t    le32_to_cpu(np->tx_ring.ex[i+3].flaglen));\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_lock_irq(&np->lock);\n\n\t \n\tnv_stop_tx(dev);\n\n\t \n\tsaved_tx_limit = np->tx_limit;\n\tnp->tx_limit = 0;  \n\tnp->tx_stop = 0;   \n\tif (!nv_optimized(np))\n\t\tnv_tx_done(dev, np->tx_ring_size);\n\telse\n\t\tnv_tx_done_optimized(dev, np->tx_ring_size);\n\n\t \n\tif (np->tx_change_owner)\n\t\tput_tx.ex = np->tx_change_owner->first_tx_desc;\n\telse\n\t\tput_tx = np->put_tx;\n\n\t \n\tnv_drain_tx(dev);\n\tnv_init_tx(dev);\n\n\t \n\tnp->get_tx = np->put_tx = put_tx;\n\tnp->tx_limit = saved_tx_limit;\n\n\t \n\tnv_start_tx(dev);\n\tnetif_wake_queue(dev);\n\tspin_unlock_irq(&np->lock);\n}\n\n \nstatic int nv_getlen(struct net_device *dev, void *packet, int datalen)\n{\n\tint hdrlen;\t \n\tint protolen;\t \n\n\t \n\tif (((struct vlan_ethhdr *)packet)->h_vlan_proto == htons(ETH_P_8021Q)) {\n\t\tprotolen = ntohs(((struct vlan_ethhdr *)packet)->h_vlan_encapsulated_proto);\n\t\thdrlen = VLAN_HLEN;\n\t} else {\n\t\tprotolen = ntohs(((struct ethhdr *)packet)->h_proto);\n\t\thdrlen = ETH_HLEN;\n\t}\n\tif (protolen > ETH_DATA_LEN)\n\t\treturn datalen;  \n\n\tprotolen += hdrlen;\n\t \n\tif (datalen > ETH_ZLEN) {\n\t\tif (datalen >= protolen) {\n\t\t\t \n\t\t\treturn protolen;\n\t\t} else {\n\t\t\t \n\t\t\treturn -1;\n\t\t}\n\t} else {\n\t\t \n\t\tif (protolen > ETH_ZLEN) {\n\t\t\treturn -1;\n\t\t}\n\t\treturn datalen;\n\t}\n}\n\nstatic void rx_missing_handler(u32 flags, struct fe_priv *np)\n{\n\tif (flags & NV_RX_MISSEDFRAME) {\n\t\tu64_stats_update_begin(&np->swstats_rx_syncp);\n\t\tnv_txrx_stats_inc(stat_rx_missed_errors);\n\t\tu64_stats_update_end(&np->swstats_rx_syncp);\n\t}\n}\n\nstatic int nv_rx_process(struct net_device *dev, int limit)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu32 flags;\n\tint rx_work = 0;\n\tstruct sk_buff *skb;\n\tint len;\n\n\twhile ((np->get_rx.orig != np->put_rx.orig) &&\n\t      !((flags = le32_to_cpu(np->get_rx.orig->flaglen)) & NV_RX_AVAIL) &&\n\t\t(rx_work < limit)) {\n\n\t\t \n\t\tdma_unmap_single(&np->pci_dev->dev, np->get_rx_ctx->dma,\n\t\t\t\t np->get_rx_ctx->dma_len,\n\t\t\t\t DMA_FROM_DEVICE);\n\t\tskb = np->get_rx_ctx->skb;\n\t\tnp->get_rx_ctx->skb = NULL;\n\n\t\t \n\t\tif (np->desc_ver == DESC_VER_1) {\n\t\t\tif (likely(flags & NV_RX_DESCRIPTORVALID)) {\n\t\t\t\tlen = flags & LEN_MASK_V1;\n\t\t\t\tif (unlikely(flags & NV_RX_ERROR)) {\n\t\t\t\t\tif ((flags & NV_RX_ERROR_MASK) == NV_RX_ERROR4) {\n\t\t\t\t\t\tlen = nv_getlen(dev, skb->data, len);\n\t\t\t\t\t\tif (len < 0) {\n\t\t\t\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\t\t\t\tgoto next_pkt;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t \n\t\t\t\t\telse if ((flags & NV_RX_ERROR_MASK) == NV_RX_FRAMINGERR) {\n\t\t\t\t\t\tif (flags & NV_RX_SUBTRACT1)\n\t\t\t\t\t\t\tlen--;\n\t\t\t\t\t}\n\t\t\t\t\t \n\t\t\t\t\telse {\n\t\t\t\t\t\trx_missing_handler(flags, np);\n\t\t\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\t\t\tgoto next_pkt;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\tgoto next_pkt;\n\t\t\t}\n\t\t} else {\n\t\t\tif (likely(flags & NV_RX2_DESCRIPTORVALID)) {\n\t\t\t\tlen = flags & LEN_MASK_V2;\n\t\t\t\tif (unlikely(flags & NV_RX2_ERROR)) {\n\t\t\t\t\tif ((flags & NV_RX2_ERROR_MASK) == NV_RX2_ERROR4) {\n\t\t\t\t\t\tlen = nv_getlen(dev, skb->data, len);\n\t\t\t\t\t\tif (len < 0) {\n\t\t\t\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\t\t\t\tgoto next_pkt;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t \n\t\t\t\t\telse if ((flags & NV_RX2_ERROR_MASK) == NV_RX2_FRAMINGERR) {\n\t\t\t\t\t\tif (flags & NV_RX2_SUBTRACT1)\n\t\t\t\t\t\t\tlen--;\n\t\t\t\t\t}\n\t\t\t\t\t \n\t\t\t\t\telse {\n\t\t\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\t\t\tgoto next_pkt;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (((flags & NV_RX2_CHECKSUMMASK) == NV_RX2_CHECKSUM_IP_TCP) ||  \n\t\t\t\t    ((flags & NV_RX2_CHECKSUMMASK) == NV_RX2_CHECKSUM_IP_UDP))    \n\t\t\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\t} else {\n\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\tgoto next_pkt;\n\t\t\t}\n\t\t}\n\t\t \n\t\tskb_put(skb, len);\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tnapi_gro_receive(&np->napi, skb);\n\t\tu64_stats_update_begin(&np->swstats_rx_syncp);\n\t\tnv_txrx_stats_inc(stat_rx_packets);\n\t\tnv_txrx_stats_add(stat_rx_bytes, len);\n\t\tu64_stats_update_end(&np->swstats_rx_syncp);\nnext_pkt:\n\t\tif (unlikely(np->get_rx.orig++ == np->last_rx.orig))\n\t\t\tnp->get_rx.orig = np->rx_ring.orig;\n\t\tif (unlikely(np->get_rx_ctx++ == np->last_rx_ctx))\n\t\t\tnp->get_rx_ctx = np->rx_skb;\n\n\t\trx_work++;\n\t}\n\n\treturn rx_work;\n}\n\nstatic int nv_rx_process_optimized(struct net_device *dev, int limit)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu32 flags;\n\tu32 vlanflags = 0;\n\tint rx_work = 0;\n\tstruct sk_buff *skb;\n\tint len;\n\n\twhile ((np->get_rx.ex != np->put_rx.ex) &&\n\t      !((flags = le32_to_cpu(np->get_rx.ex->flaglen)) & NV_RX2_AVAIL) &&\n\t      (rx_work < limit)) {\n\n\t\t \n\t\tdma_unmap_single(&np->pci_dev->dev, np->get_rx_ctx->dma,\n\t\t\t\t np->get_rx_ctx->dma_len,\n\t\t\t\t DMA_FROM_DEVICE);\n\t\tskb = np->get_rx_ctx->skb;\n\t\tnp->get_rx_ctx->skb = NULL;\n\n\t\t \n\t\tif (likely(flags & NV_RX2_DESCRIPTORVALID)) {\n\t\t\tlen = flags & LEN_MASK_V2;\n\t\t\tif (unlikely(flags & NV_RX2_ERROR)) {\n\t\t\t\tif ((flags & NV_RX2_ERROR_MASK) == NV_RX2_ERROR4) {\n\t\t\t\t\tlen = nv_getlen(dev, skb->data, len);\n\t\t\t\t\tif (len < 0) {\n\t\t\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\t\t\tgoto next_pkt;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\telse if ((flags & NV_RX2_ERROR_MASK) == NV_RX2_FRAMINGERR) {\n\t\t\t\t\tif (flags & NV_RX2_SUBTRACT1)\n\t\t\t\t\t\tlen--;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\telse {\n\t\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\t\tgoto next_pkt;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (((flags & NV_RX2_CHECKSUMMASK) == NV_RX2_CHECKSUM_IP_TCP) ||  \n\t\t\t    ((flags & NV_RX2_CHECKSUMMASK) == NV_RX2_CHECKSUM_IP_UDP))    \n\t\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\t\t \n\t\t\tskb_put(skb, len);\n\t\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\t\tprefetch(skb->data);\n\n\t\t\tvlanflags = le32_to_cpu(np->get_rx.ex->buflow);\n\n\t\t\t \n\t\t\tif (dev->features & NETIF_F_HW_VLAN_CTAG_RX &&\n\t\t\t    vlanflags & NV_RX3_VLAN_TAG_PRESENT) {\n\t\t\t\tu16 vid = vlanflags & NV_RX3_VLAN_TAG_MASK;\n\n\t\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);\n\t\t\t}\n\t\t\tnapi_gro_receive(&np->napi, skb);\n\t\t\tu64_stats_update_begin(&np->swstats_rx_syncp);\n\t\t\tnv_txrx_stats_inc(stat_rx_packets);\n\t\t\tnv_txrx_stats_add(stat_rx_bytes, len);\n\t\t\tu64_stats_update_end(&np->swstats_rx_syncp);\n\t\t} else {\n\t\t\tdev_kfree_skb(skb);\n\t\t}\nnext_pkt:\n\t\tif (unlikely(np->get_rx.ex++ == np->last_rx.ex))\n\t\t\tnp->get_rx.ex = np->rx_ring.ex;\n\t\tif (unlikely(np->get_rx_ctx++ == np->last_rx_ctx))\n\t\t\tnp->get_rx_ctx = np->rx_skb;\n\n\t\trx_work++;\n\t}\n\n\treturn rx_work;\n}\n\nstatic void set_bufsize(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tif (dev->mtu <= ETH_DATA_LEN)\n\t\tnp->rx_buf_sz = ETH_DATA_LEN + NV_RX_HEADERS;\n\telse\n\t\tnp->rx_buf_sz = dev->mtu + NV_RX_HEADERS;\n}\n\n \nstatic int nv_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tint old_mtu;\n\n\told_mtu = dev->mtu;\n\tdev->mtu = new_mtu;\n\n\t \n\tif (old_mtu <= ETH_DATA_LEN && new_mtu <= ETH_DATA_LEN)\n\t\treturn 0;\n\n\t \n\tif (netif_running(dev)) {\n\t\tu8 __iomem *base = get_hwbase(dev);\n\t\t \n\t\tnv_disable_irq(dev);\n\t\tnv_napi_disable(dev);\n\t\tnetif_tx_lock_bh(dev);\n\t\tnetif_addr_lock(dev);\n\t\tspin_lock(&np->lock);\n\t\t \n\t\tnv_stop_rxtx(dev);\n\t\tnv_txrx_reset(dev);\n\t\t \n\t\tnv_drain_rxtx(dev);\n\t\t \n\t\tset_bufsize(dev);\n\t\tif (nv_init_ring(dev)) {\n\t\t\tif (!np->in_shutdown)\n\t\t\t\tmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\n\t\t}\n\t\t \n\t\twritel(np->rx_buf_sz, base + NvRegOffloadConfig);\n\t\tsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\n\t\twritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\n\t\t\tbase + NvRegRingSizes);\n\t\tpci_push(base);\n\t\twritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\n\t\tpci_push(base);\n\n\t\t \n\t\tnv_start_rxtx(dev);\n\t\tspin_unlock(&np->lock);\n\t\tnetif_addr_unlock(dev);\n\t\tnetif_tx_unlock_bh(dev);\n\t\tnv_napi_enable(dev);\n\t\tnv_enable_irq(dev);\n\t}\n\treturn 0;\n}\n\nstatic void nv_copy_mac_to_hw(struct net_device *dev)\n{\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 mac[2];\n\n\tmac[0] = (dev->dev_addr[0] << 0) + (dev->dev_addr[1] << 8) +\n\t\t\t(dev->dev_addr[2] << 16) + (dev->dev_addr[3] << 24);\n\tmac[1] = (dev->dev_addr[4] << 0) + (dev->dev_addr[5] << 8);\n\n\twritel(mac[0], base + NvRegMacAddrA);\n\twritel(mac[1], base + NvRegMacAddrB);\n}\n\n \nstatic int nv_set_mac_address(struct net_device *dev, void *addr)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tstruct sockaddr *macaddr = (struct sockaddr *)addr;\n\n\tif (!is_valid_ether_addr(macaddr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\t \n\teth_hw_addr_set(dev, macaddr->sa_data);\n\n\tif (netif_running(dev)) {\n\t\tnetif_tx_lock_bh(dev);\n\t\tnetif_addr_lock(dev);\n\t\tspin_lock_irq(&np->lock);\n\n\t\t \n\t\tnv_stop_rx(dev);\n\n\t\t \n\t\tnv_copy_mac_to_hw(dev);\n\n\t\t \n\t\tnv_start_rx(dev);\n\t\tspin_unlock_irq(&np->lock);\n\t\tnetif_addr_unlock(dev);\n\t\tnetif_tx_unlock_bh(dev);\n\t} else {\n\t\tnv_copy_mac_to_hw(dev);\n\t}\n\treturn 0;\n}\n\n \nstatic void nv_set_multicast(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 addr[2];\n\tu32 mask[2];\n\tu32 pff = readl(base + NvRegPacketFilterFlags) & NVREG_PFF_PAUSE_RX;\n\n\tmemset(addr, 0, sizeof(addr));\n\tmemset(mask, 0, sizeof(mask));\n\n\tif (dev->flags & IFF_PROMISC) {\n\t\tpff |= NVREG_PFF_PROMISC;\n\t} else {\n\t\tpff |= NVREG_PFF_MYADDR;\n\n\t\tif (dev->flags & IFF_ALLMULTI || !netdev_mc_empty(dev)) {\n\t\t\tu32 alwaysOff[2];\n\t\t\tu32 alwaysOn[2];\n\n\t\t\talwaysOn[0] = alwaysOn[1] = alwaysOff[0] = alwaysOff[1] = 0xffffffff;\n\t\t\tif (dev->flags & IFF_ALLMULTI) {\n\t\t\t\talwaysOn[0] = alwaysOn[1] = alwaysOff[0] = alwaysOff[1] = 0;\n\t\t\t} else {\n\t\t\t\tstruct netdev_hw_addr *ha;\n\n\t\t\t\tnetdev_for_each_mc_addr(ha, dev) {\n\t\t\t\t\tunsigned char *hw_addr = ha->addr;\n\t\t\t\t\tu32 a, b;\n\n\t\t\t\t\ta = le32_to_cpu(*(__le32 *) hw_addr);\n\t\t\t\t\tb = le16_to_cpu(*(__le16 *) (&hw_addr[4]));\n\t\t\t\t\talwaysOn[0] &= a;\n\t\t\t\t\talwaysOff[0] &= ~a;\n\t\t\t\t\talwaysOn[1] &= b;\n\t\t\t\t\talwaysOff[1] &= ~b;\n\t\t\t\t}\n\t\t\t}\n\t\t\taddr[0] = alwaysOn[0];\n\t\t\taddr[1] = alwaysOn[1];\n\t\t\tmask[0] = alwaysOn[0] | alwaysOff[0];\n\t\t\tmask[1] = alwaysOn[1] | alwaysOff[1];\n\t\t} else {\n\t\t\tmask[0] = NVREG_MCASTMASKA_NONE;\n\t\t\tmask[1] = NVREG_MCASTMASKB_NONE;\n\t\t}\n\t}\n\taddr[0] |= NVREG_MCASTADDRA_FORCE;\n\tpff |= NVREG_PFF_ALWAYS;\n\tspin_lock_irq(&np->lock);\n\tnv_stop_rx(dev);\n\twritel(addr[0], base + NvRegMulticastAddrA);\n\twritel(addr[1], base + NvRegMulticastAddrB);\n\twritel(mask[0], base + NvRegMulticastMaskA);\n\twritel(mask[1], base + NvRegMulticastMaskB);\n\twritel(pff, base + NvRegPacketFilterFlags);\n\tnv_start_rx(dev);\n\tspin_unlock_irq(&np->lock);\n}\n\nstatic void nv_update_pause(struct net_device *dev, u32 pause_flags)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\n\tnp->pause_flags &= ~(NV_PAUSEFRAME_TX_ENABLE | NV_PAUSEFRAME_RX_ENABLE);\n\n\tif (np->pause_flags & NV_PAUSEFRAME_RX_CAPABLE) {\n\t\tu32 pff = readl(base + NvRegPacketFilterFlags) & ~NVREG_PFF_PAUSE_RX;\n\t\tif (pause_flags & NV_PAUSEFRAME_RX_ENABLE) {\n\t\t\twritel(pff|NVREG_PFF_PAUSE_RX, base + NvRegPacketFilterFlags);\n\t\t\tnp->pause_flags |= NV_PAUSEFRAME_RX_ENABLE;\n\t\t} else {\n\t\t\twritel(pff, base + NvRegPacketFilterFlags);\n\t\t}\n\t}\n\tif (np->pause_flags & NV_PAUSEFRAME_TX_CAPABLE) {\n\t\tu32 regmisc = readl(base + NvRegMisc1) & ~NVREG_MISC1_PAUSE_TX;\n\t\tif (pause_flags & NV_PAUSEFRAME_TX_ENABLE) {\n\t\t\tu32 pause_enable = NVREG_TX_PAUSEFRAME_ENABLE_V1;\n\t\t\tif (np->driver_data & DEV_HAS_PAUSEFRAME_TX_V2)\n\t\t\t\tpause_enable = NVREG_TX_PAUSEFRAME_ENABLE_V2;\n\t\t\tif (np->driver_data & DEV_HAS_PAUSEFRAME_TX_V3) {\n\t\t\t\tpause_enable = NVREG_TX_PAUSEFRAME_ENABLE_V3;\n\t\t\t\t \n\t\t\t\twritel(readl(base + NvRegTxPauseFrameLimit)|NVREG_TX_PAUSEFRAMELIMIT_ENABLE, base + NvRegTxPauseFrameLimit);\n\t\t\t}\n\t\t\twritel(pause_enable,  base + NvRegTxPauseFrame);\n\t\t\twritel(regmisc|NVREG_MISC1_PAUSE_TX, base + NvRegMisc1);\n\t\t\tnp->pause_flags |= NV_PAUSEFRAME_TX_ENABLE;\n\t\t} else {\n\t\t\twritel(NVREG_TX_PAUSEFRAME_DISABLE,  base + NvRegTxPauseFrame);\n\t\t\twritel(regmisc, base + NvRegMisc1);\n\t\t}\n\t}\n}\n\nstatic void nv_force_linkspeed(struct net_device *dev, int speed, int duplex)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 phyreg, txreg;\n\tint mii_status;\n\n\tnp->linkspeed = NVREG_LINKSPEED_FORCE|speed;\n\tnp->duplex = duplex;\n\n\t \n\tmii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\n\tif (mii_status & PHY_GIGABIT) {\n\t\tnp->gigabit = PHY_GIGABIT;\n\t\tphyreg = readl(base + NvRegSlotTime);\n\t\tphyreg &= ~(0x3FF00);\n\t\tif ((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_10)\n\t\t\tphyreg |= NVREG_SLOTTIME_10_100_FULL;\n\t\telse if ((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_100)\n\t\t\tphyreg |= NVREG_SLOTTIME_10_100_FULL;\n\t\telse if ((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_1000)\n\t\t\tphyreg |= NVREG_SLOTTIME_1000_FULL;\n\t\twritel(phyreg, base + NvRegSlotTime);\n\t}\n\n\tphyreg = readl(base + NvRegPhyInterface);\n\tphyreg &= ~(PHY_HALF|PHY_100|PHY_1000);\n\tif (np->duplex == 0)\n\t\tphyreg |= PHY_HALF;\n\tif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_100)\n\t\tphyreg |= PHY_100;\n\telse if ((np->linkspeed & NVREG_LINKSPEED_MASK) ==\n\t\t\t\t\t\t\tNVREG_LINKSPEED_1000)\n\t\tphyreg |= PHY_1000;\n\twritel(phyreg, base + NvRegPhyInterface);\n\n\tif (phyreg & PHY_RGMII) {\n\t\tif ((np->linkspeed & NVREG_LINKSPEED_MASK) ==\n\t\t\t\t\t\t\tNVREG_LINKSPEED_1000)\n\t\t\ttxreg = NVREG_TX_DEFERRAL_RGMII_1000;\n\t\telse\n\t\t\ttxreg = NVREG_TX_DEFERRAL_RGMII_10_100;\n\t} else {\n\t\ttxreg = NVREG_TX_DEFERRAL_DEFAULT;\n\t}\n\twritel(txreg, base + NvRegTxDeferral);\n\n\tif (np->desc_ver == DESC_VER_1) {\n\t\ttxreg = NVREG_TX_WM_DESC1_DEFAULT;\n\t} else {\n\t\tif ((np->linkspeed & NVREG_LINKSPEED_MASK) ==\n\t\t\t\t\t NVREG_LINKSPEED_1000)\n\t\t\ttxreg = NVREG_TX_WM_DESC2_3_1000;\n\t\telse\n\t\t\ttxreg = NVREG_TX_WM_DESC2_3_DEFAULT;\n\t}\n\twritel(txreg, base + NvRegTxWatermark);\n\n\twritel(NVREG_MISC1_FORCE | (np->duplex ? 0 : NVREG_MISC1_HD),\n\t\t\tbase + NvRegMisc1);\n\tpci_push(base);\n\twritel(np->linkspeed, base + NvRegLinkSpeed);\n\tpci_push(base);\n}\n\n \nstatic int nv_update_linkspeed(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tint adv = 0;\n\tint lpa = 0;\n\tint adv_lpa, adv_pause, lpa_pause;\n\tint newls = np->linkspeed;\n\tint newdup = np->duplex;\n\tint mii_status;\n\tu32 bmcr;\n\tint retval = 0;\n\tu32 control_1000, status_1000, phyreg, pause_flags, txreg;\n\tu32 txrxFlags = 0;\n\tu32 phy_exp;\n\n\t \n\tbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\tif (bmcr & BMCR_LOOPBACK) {\n\t\tif (netif_running(dev)) {\n\t\t\tnv_force_linkspeed(dev, NVREG_LINKSPEED_1000, 1);\n\t\t\tif (!netif_carrier_ok(dev))\n\t\t\t\tnetif_carrier_on(dev);\n\t\t}\n\t\treturn 1;\n\t}\n\n\t \n\tmii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\n\tmii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\n\n\tif (!(mii_status & BMSR_LSTATUS)) {\n\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\n\t\tnewdup = 0;\n\t\tretval = 0;\n\t\tgoto set_speed;\n\t}\n\n\tif (np->autoneg == 0) {\n\t\tif (np->fixed_mode & LPA_100FULL) {\n\t\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_100;\n\t\t\tnewdup = 1;\n\t\t} else if (np->fixed_mode & LPA_100HALF) {\n\t\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_100;\n\t\t\tnewdup = 0;\n\t\t} else if (np->fixed_mode & LPA_10FULL) {\n\t\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\n\t\t\tnewdup = 1;\n\t\t} else {\n\t\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\n\t\t\tnewdup = 0;\n\t\t}\n\t\tretval = 1;\n\t\tgoto set_speed;\n\t}\n\t \n\tif (!(mii_status & BMSR_ANEGCOMPLETE)) {\n\t\t \n\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\n\t\tnewdup = 0;\n\t\tretval = 0;\n\t\tgoto set_speed;\n\t}\n\n\tadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\n\tlpa = mii_rw(dev, np->phyaddr, MII_LPA, MII_READ);\n\n\tretval = 1;\n\tif (np->gigabit == PHY_GIGABIT) {\n\t\tcontrol_1000 = mii_rw(dev, np->phyaddr, MII_CTRL1000, MII_READ);\n\t\tstatus_1000 = mii_rw(dev, np->phyaddr, MII_STAT1000, MII_READ);\n\n\t\tif ((control_1000 & ADVERTISE_1000FULL) &&\n\t\t\t(status_1000 & LPA_1000FULL)) {\n\t\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_1000;\n\t\t\tnewdup = 1;\n\t\t\tgoto set_speed;\n\t\t}\n\t}\n\n\t \n\tadv_lpa = lpa & adv;\n\tif (adv_lpa & LPA_100FULL) {\n\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_100;\n\t\tnewdup = 1;\n\t} else if (adv_lpa & LPA_100HALF) {\n\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_100;\n\t\tnewdup = 0;\n\t} else if (adv_lpa & LPA_10FULL) {\n\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\n\t\tnewdup = 1;\n\t} else if (adv_lpa & LPA_10HALF) {\n\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\n\t\tnewdup = 0;\n\t} else {\n\t\tnewls = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\n\t\tnewdup = 0;\n\t}\n\nset_speed:\n\tif (np->duplex == newdup && np->linkspeed == newls)\n\t\treturn retval;\n\n\tnp->duplex = newdup;\n\tnp->linkspeed = newls;\n\n\t \n\tif (readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_START) {\n\t\ttxrxFlags |= NV_RESTART_TX;\n\t\tnv_stop_tx(dev);\n\t}\n\tif (readl(base + NvRegReceiverControl) & NVREG_RCVCTL_START) {\n\t\ttxrxFlags |= NV_RESTART_RX;\n\t\tnv_stop_rx(dev);\n\t}\n\n\tif (np->gigabit == PHY_GIGABIT) {\n\t\tphyreg = readl(base + NvRegSlotTime);\n\t\tphyreg &= ~(0x3FF00);\n\t\tif (((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_10) ||\n\t\t    ((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_100))\n\t\t\tphyreg |= NVREG_SLOTTIME_10_100_FULL;\n\t\telse if ((np->linkspeed & 0xFFF) == NVREG_LINKSPEED_1000)\n\t\t\tphyreg |= NVREG_SLOTTIME_1000_FULL;\n\t\twritel(phyreg, base + NvRegSlotTime);\n\t}\n\n\tphyreg = readl(base + NvRegPhyInterface);\n\tphyreg &= ~(PHY_HALF|PHY_100|PHY_1000);\n\tif (np->duplex == 0)\n\t\tphyreg |= PHY_HALF;\n\tif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_100)\n\t\tphyreg |= PHY_100;\n\telse if ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_1000)\n\t\tphyreg |= PHY_1000;\n\twritel(phyreg, base + NvRegPhyInterface);\n\n\tphy_exp = mii_rw(dev, np->phyaddr, MII_EXPANSION, MII_READ) & EXPANSION_NWAY;  \n\tif (phyreg & PHY_RGMII) {\n\t\tif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_1000) {\n\t\t\ttxreg = NVREG_TX_DEFERRAL_RGMII_1000;\n\t\t} else {\n\t\t\tif (!phy_exp && !np->duplex && (np->driver_data & DEV_HAS_COLLISION_FIX)) {\n\t\t\t\tif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_10)\n\t\t\t\t\ttxreg = NVREG_TX_DEFERRAL_RGMII_STRETCH_10;\n\t\t\t\telse\n\t\t\t\t\ttxreg = NVREG_TX_DEFERRAL_RGMII_STRETCH_100;\n\t\t\t} else {\n\t\t\t\ttxreg = NVREG_TX_DEFERRAL_RGMII_10_100;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (!phy_exp && !np->duplex && (np->driver_data & DEV_HAS_COLLISION_FIX))\n\t\t\ttxreg = NVREG_TX_DEFERRAL_MII_STRETCH;\n\t\telse\n\t\t\ttxreg = NVREG_TX_DEFERRAL_DEFAULT;\n\t}\n\twritel(txreg, base + NvRegTxDeferral);\n\n\tif (np->desc_ver == DESC_VER_1) {\n\t\ttxreg = NVREG_TX_WM_DESC1_DEFAULT;\n\t} else {\n\t\tif ((np->linkspeed & NVREG_LINKSPEED_MASK) == NVREG_LINKSPEED_1000)\n\t\t\ttxreg = NVREG_TX_WM_DESC2_3_1000;\n\t\telse\n\t\t\ttxreg = NVREG_TX_WM_DESC2_3_DEFAULT;\n\t}\n\twritel(txreg, base + NvRegTxWatermark);\n\n\twritel(NVREG_MISC1_FORCE | (np->duplex ? 0 : NVREG_MISC1_HD),\n\t\tbase + NvRegMisc1);\n\tpci_push(base);\n\twritel(np->linkspeed, base + NvRegLinkSpeed);\n\tpci_push(base);\n\n\tpause_flags = 0;\n\t \n\tif (netif_running(dev) && (np->duplex != 0)) {\n\t\tif (np->autoneg && np->pause_flags & NV_PAUSEFRAME_AUTONEG) {\n\t\t\tadv_pause = adv & (ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM);\n\t\t\tlpa_pause = lpa & (LPA_PAUSE_CAP | LPA_PAUSE_ASYM);\n\n\t\t\tswitch (adv_pause) {\n\t\t\tcase ADVERTISE_PAUSE_CAP:\n\t\t\t\tif (lpa_pause & LPA_PAUSE_CAP) {\n\t\t\t\t\tpause_flags |= NV_PAUSEFRAME_RX_ENABLE;\n\t\t\t\t\tif (np->pause_flags & NV_PAUSEFRAME_TX_REQ)\n\t\t\t\t\t\tpause_flags |= NV_PAUSEFRAME_TX_ENABLE;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase ADVERTISE_PAUSE_ASYM:\n\t\t\t\tif (lpa_pause == (LPA_PAUSE_CAP | LPA_PAUSE_ASYM))\n\t\t\t\t\tpause_flags |= NV_PAUSEFRAME_TX_ENABLE;\n\t\t\t\tbreak;\n\t\t\tcase ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM:\n\t\t\t\tif (lpa_pause & LPA_PAUSE_CAP) {\n\t\t\t\t\tpause_flags |=  NV_PAUSEFRAME_RX_ENABLE;\n\t\t\t\t\tif (np->pause_flags & NV_PAUSEFRAME_TX_REQ)\n\t\t\t\t\t\tpause_flags |= NV_PAUSEFRAME_TX_ENABLE;\n\t\t\t\t}\n\t\t\t\tif (lpa_pause == LPA_PAUSE_ASYM)\n\t\t\t\t\tpause_flags |= NV_PAUSEFRAME_RX_ENABLE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tpause_flags = np->pause_flags;\n\t\t}\n\t}\n\tnv_update_pause(dev, pause_flags);\n\n\tif (txrxFlags & NV_RESTART_TX)\n\t\tnv_start_tx(dev);\n\tif (txrxFlags & NV_RESTART_RX)\n\t\tnv_start_rx(dev);\n\n\treturn retval;\n}\n\nstatic void nv_linkchange(struct net_device *dev)\n{\n\tif (nv_update_linkspeed(dev)) {\n\t\tif (!netif_carrier_ok(dev)) {\n\t\t\tnetif_carrier_on(dev);\n\t\t\tnetdev_info(dev, \"link up\\n\");\n\t\t\tnv_txrx_gate(dev, false);\n\t\t\tnv_start_rx(dev);\n\t\t}\n\t} else {\n\t\tif (netif_carrier_ok(dev)) {\n\t\t\tnetif_carrier_off(dev);\n\t\t\tnetdev_info(dev, \"link down\\n\");\n\t\t\tnv_txrx_gate(dev, true);\n\t\t\tnv_stop_rx(dev);\n\t\t}\n\t}\n}\n\nstatic void nv_link_irq(struct net_device *dev)\n{\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 miistat;\n\n\tmiistat = readl(base + NvRegMIIStatus);\n\twritel(NVREG_MIISTAT_LINKCHANGE, base + NvRegMIIStatus);\n\n\tif (miistat & (NVREG_MIISTAT_LINKCHANGE))\n\t\tnv_linkchange(dev);\n}\n\nstatic void nv_msi_workaround(struct fe_priv *np)\n{\n\n\t \n\tif (np->msi_flags & NV_MSI_ENABLED) {\n\t\tu8 __iomem *base = np->base;\n\n\t\twritel(0, base + NvRegMSIIrqMask);\n\t\twritel(NVREG_MSI_VECTOR_0_ENABLED, base + NvRegMSIIrqMask);\n\t}\n}\n\nstatic inline int nv_change_interrupt_mode(struct net_device *dev, int total_work)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tif (optimization_mode == NV_OPTIMIZATION_MODE_DYNAMIC) {\n\t\tif (total_work > NV_DYNAMIC_THRESHOLD) {\n\t\t\t \n\t\t\tnp->quiet_count = 0;\n\t\t\tif (np->irqmask != NVREG_IRQMASK_CPU) {\n\t\t\t\tnp->irqmask = NVREG_IRQMASK_CPU;\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t} else {\n\t\t\tif (np->quiet_count < NV_DYNAMIC_MAX_QUIET_COUNT) {\n\t\t\t\tnp->quiet_count++;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (np->irqmask != NVREG_IRQMASK_THROUGHPUT) {\n\t\t\t\t\tnp->irqmask = NVREG_IRQMASK_THROUGHPUT;\n\t\t\t\t\treturn 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic irqreturn_t nv_nic_irq(int foo, void *data)\n{\n\tstruct net_device *dev = (struct net_device *) data;\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\n\tif (!(np->msi_flags & NV_MSI_X_ENABLED)) {\n\t\tnp->events = readl(base + NvRegIrqStatus);\n\t\twritel(np->events, base + NvRegIrqStatus);\n\t} else {\n\t\tnp->events = readl(base + NvRegMSIXIrqStatus);\n\t\twritel(np->events, base + NvRegMSIXIrqStatus);\n\t}\n\tif (!(np->events & np->irqmask))\n\t\treturn IRQ_NONE;\n\n\tnv_msi_workaround(np);\n\n\tif (napi_schedule_prep(&np->napi)) {\n\t\t \n\t\twritel(0, base + NvRegIrqMask);\n\t\t__napi_schedule(&np->napi);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t nv_nic_irq_optimized(int foo, void *data)\n{\n\tstruct net_device *dev = (struct net_device *) data;\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\n\tif (!(np->msi_flags & NV_MSI_X_ENABLED)) {\n\t\tnp->events = readl(base + NvRegIrqStatus);\n\t\twritel(np->events, base + NvRegIrqStatus);\n\t} else {\n\t\tnp->events = readl(base + NvRegMSIXIrqStatus);\n\t\twritel(np->events, base + NvRegMSIXIrqStatus);\n\t}\n\tif (!(np->events & np->irqmask))\n\t\treturn IRQ_NONE;\n\n\tnv_msi_workaround(np);\n\n\tif (napi_schedule_prep(&np->napi)) {\n\t\t \n\t\twritel(0, base + NvRegIrqMask);\n\t\t__napi_schedule(&np->napi);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t nv_nic_irq_tx(int foo, void *data)\n{\n\tstruct net_device *dev = (struct net_device *) data;\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 events;\n\tint i;\n\tunsigned long flags;\n\n\tfor (i = 0;; i++) {\n\t\tevents = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQ_TX_ALL;\n\t\twritel(events, base + NvRegMSIXIrqStatus);\n\t\tnetdev_dbg(dev, \"tx irq events: %08x\\n\", events);\n\t\tif (!(events & np->irqmask))\n\t\t\tbreak;\n\n\t\tspin_lock_irqsave(&np->lock, flags);\n\t\tnv_tx_done_optimized(dev, TX_WORK_PER_LOOP);\n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\n\t\tif (unlikely(i > max_interrupt_work)) {\n\t\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t\t \n\t\t\twritel(NVREG_IRQ_TX_ALL, base + NvRegIrqMask);\n\t\t\tpci_push(base);\n\n\t\t\tif (!np->in_shutdown) {\n\t\t\t\tnp->nic_poll_irq |= NVREG_IRQ_TX_ALL;\n\t\t\t\tmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\tnetdev_dbg(dev, \"%s: too many iterations (%d)\\n\",\n\t\t\t\t   __func__, i);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\n\treturn IRQ_RETVAL(i);\n}\n\nstatic int nv_napi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct fe_priv *np = container_of(napi, struct fe_priv, napi);\n\tstruct net_device *dev = np->dev;\n\tu8 __iomem *base = get_hwbase(dev);\n\tunsigned long flags;\n\tint retcode;\n\tint rx_count, tx_work = 0, rx_work = 0;\n\n\tdo {\n\t\tif (!nv_optimized(np)) {\n\t\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t\ttx_work += nv_tx_done(dev, np->tx_ring_size);\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\n\t\t\trx_count = nv_rx_process(dev, budget - rx_work);\n\t\t\tretcode = nv_alloc_rx(dev);\n\t\t} else {\n\t\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t\ttx_work += nv_tx_done_optimized(dev, np->tx_ring_size);\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\n\t\t\trx_count = nv_rx_process_optimized(dev,\n\t\t\t    budget - rx_work);\n\t\t\tretcode = nv_alloc_rx_optimized(dev);\n\t\t}\n\t} while (retcode == 0 &&\n\t\t rx_count > 0 && (rx_work += rx_count) < budget);\n\n\tif (retcode) {\n\t\tspin_lock_irqsave(&np->lock, flags);\n\t\tif (!np->in_shutdown)\n\t\t\tmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t}\n\n\tnv_change_interrupt_mode(dev, tx_work + rx_work);\n\n\tif (unlikely(np->events & NVREG_IRQ_LINK)) {\n\t\tspin_lock_irqsave(&np->lock, flags);\n\t\tnv_link_irq(dev);\n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t}\n\tif (unlikely(np->need_linktimer && time_after(jiffies, np->link_timeout))) {\n\t\tspin_lock_irqsave(&np->lock, flags);\n\t\tnv_linkchange(dev);\n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\tnp->link_timeout = jiffies + LINK_TIMEOUT;\n\t}\n\tif (unlikely(np->events & NVREG_IRQ_RECOVER_ERROR)) {\n\t\tspin_lock_irqsave(&np->lock, flags);\n\t\tif (!np->in_shutdown) {\n\t\t\tnp->nic_poll_irq = np->irqmask;\n\t\t\tnp->recover_error = 1;\n\t\t\tmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\n\t\t}\n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\tnapi_complete(napi);\n\t\treturn rx_work;\n\t}\n\n\tif (rx_work < budget) {\n\t\t \n\t\tnapi_complete_done(napi, rx_work);\n\n\t\twritel(np->irqmask, base + NvRegIrqMask);\n\t}\n\treturn rx_work;\n}\n\nstatic irqreturn_t nv_nic_irq_rx(int foo, void *data)\n{\n\tstruct net_device *dev = (struct net_device *) data;\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 events;\n\tint i;\n\tunsigned long flags;\n\n\tfor (i = 0;; i++) {\n\t\tevents = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQ_RX_ALL;\n\t\twritel(events, base + NvRegMSIXIrqStatus);\n\t\tnetdev_dbg(dev, \"rx irq events: %08x\\n\", events);\n\t\tif (!(events & np->irqmask))\n\t\t\tbreak;\n\n\t\tif (nv_rx_process_optimized(dev, RX_WORK_PER_LOOP)) {\n\t\t\tif (unlikely(nv_alloc_rx_optimized(dev))) {\n\t\t\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t\t\tif (!np->in_shutdown)\n\t\t\t\t\tmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\n\t\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(i > max_interrupt_work)) {\n\t\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t\t \n\t\t\twritel(NVREG_IRQ_RX_ALL, base + NvRegIrqMask);\n\t\t\tpci_push(base);\n\n\t\t\tif (!np->in_shutdown) {\n\t\t\t\tnp->nic_poll_irq |= NVREG_IRQ_RX_ALL;\n\t\t\t\tmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\tnetdev_dbg(dev, \"%s: too many iterations (%d)\\n\",\n\t\t\t\t   __func__, i);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn IRQ_RETVAL(i);\n}\n\nstatic irqreturn_t nv_nic_irq_other(int foo, void *data)\n{\n\tstruct net_device *dev = (struct net_device *) data;\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 events;\n\tint i;\n\tunsigned long flags;\n\n\tfor (i = 0;; i++) {\n\t\tevents = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQ_OTHER;\n\t\twritel(events, base + NvRegMSIXIrqStatus);\n\t\tnetdev_dbg(dev, \"irq events: %08x\\n\", events);\n\t\tif (!(events & np->irqmask))\n\t\t\tbreak;\n\n\t\t \n\t\tspin_lock_irqsave(&np->lock, flags);\n\t\tnv_tx_done_optimized(dev, TX_WORK_PER_LOOP);\n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\n\t\tif (events & NVREG_IRQ_LINK) {\n\t\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t\tnv_link_irq(dev);\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t}\n\t\tif (np->need_linktimer && time_after(jiffies, np->link_timeout)) {\n\t\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t\tnv_linkchange(dev);\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\tnp->link_timeout = jiffies + LINK_TIMEOUT;\n\t\t}\n\t\tif (events & NVREG_IRQ_RECOVER_ERROR) {\n\t\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t\t \n\t\t\twritel(NVREG_IRQ_OTHER, base + NvRegIrqMask);\n\t\t\tpci_push(base);\n\n\t\t\tif (!np->in_shutdown) {\n\t\t\t\tnp->nic_poll_irq |= NVREG_IRQ_OTHER;\n\t\t\t\tnp->recover_error = 1;\n\t\t\t\tmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\tbreak;\n\t\t}\n\t\tif (unlikely(i > max_interrupt_work)) {\n\t\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t\t \n\t\t\twritel(NVREG_IRQ_OTHER, base + NvRegIrqMask);\n\t\t\tpci_push(base);\n\n\t\t\tif (!np->in_shutdown) {\n\t\t\t\tnp->nic_poll_irq |= NVREG_IRQ_OTHER;\n\t\t\t\tmod_timer(&np->nic_poll, jiffies + POLL_WAIT);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\tnetdev_dbg(dev, \"%s: too many iterations (%d)\\n\",\n\t\t\t\t   __func__, i);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\n\treturn IRQ_RETVAL(i);\n}\n\nstatic irqreturn_t nv_nic_irq_test(int foo, void *data)\n{\n\tstruct net_device *dev = (struct net_device *) data;\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 events;\n\n\tif (!(np->msi_flags & NV_MSI_X_ENABLED)) {\n\t\tevents = readl(base + NvRegIrqStatus) & NVREG_IRQSTAT_MASK;\n\t\twritel(events & NVREG_IRQ_TIMER, base + NvRegIrqStatus);\n\t} else {\n\t\tevents = readl(base + NvRegMSIXIrqStatus) & NVREG_IRQSTAT_MASK;\n\t\twritel(events & NVREG_IRQ_TIMER, base + NvRegMSIXIrqStatus);\n\t}\n\tpci_push(base);\n\tif (!(events & NVREG_IRQ_TIMER))\n\t\treturn IRQ_RETVAL(0);\n\n\tnv_msi_workaround(np);\n\n\tspin_lock(&np->lock);\n\tnp->intr_test = 1;\n\tspin_unlock(&np->lock);\n\n\treturn IRQ_RETVAL(1);\n}\n\nstatic void set_msix_vector_map(struct net_device *dev, u32 vector, u32 irqmask)\n{\n\tu8 __iomem *base = get_hwbase(dev);\n\tint i;\n\tu32 msixmap = 0;\n\n\t \n\tfor (i = 0; i < 8; i++) {\n\t\tif ((irqmask >> i) & 0x1)\n\t\t\tmsixmap |= vector << (i << 2);\n\t}\n\twritel(readl(base + NvRegMSIXMap0) | msixmap, base + NvRegMSIXMap0);\n\n\tmsixmap = 0;\n\tfor (i = 0; i < 8; i++) {\n\t\tif ((irqmask >> (i + 8)) & 0x1)\n\t\t\tmsixmap |= vector << (i << 2);\n\t}\n\twritel(readl(base + NvRegMSIXMap1) | msixmap, base + NvRegMSIXMap1);\n}\n\nstatic int nv_request_irq(struct net_device *dev, int intr_test)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tint ret;\n\tint i;\n\tirqreturn_t (*handler)(int foo, void *data);\n\n\tif (intr_test) {\n\t\thandler = nv_nic_irq_test;\n\t} else {\n\t\tif (nv_optimized(np))\n\t\t\thandler = nv_nic_irq_optimized;\n\t\telse\n\t\t\thandler = nv_nic_irq;\n\t}\n\n\tif (np->msi_flags & NV_MSI_X_CAPABLE) {\n\t\tfor (i = 0; i < (np->msi_flags & NV_MSI_X_VECTORS_MASK); i++)\n\t\t\tnp->msi_x_entry[i].entry = i;\n\t\tret = pci_enable_msix_range(np->pci_dev,\n\t\t\t\t\t    np->msi_x_entry,\n\t\t\t\t\t    np->msi_flags & NV_MSI_X_VECTORS_MASK,\n\t\t\t\t\t    np->msi_flags & NV_MSI_X_VECTORS_MASK);\n\t\tif (ret > 0) {\n\t\t\tnp->msi_flags |= NV_MSI_X_ENABLED;\n\t\t\tif (optimization_mode == NV_OPTIMIZATION_MODE_THROUGHPUT && !intr_test) {\n\t\t\t\t \n\t\t\t\tsprintf(np->name_rx, \"%s-rx\", dev->name);\n\t\t\t\tret = request_irq(np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector,\n\t\t\t\t\t\t  nv_nic_irq_rx, IRQF_SHARED, np->name_rx, dev);\n\t\t\t\tif (ret) {\n\t\t\t\t\tnetdev_info(dev,\n\t\t\t\t\t\t    \"request_irq failed for rx %d\\n\",\n\t\t\t\t\t\t    ret);\n\t\t\t\t\tpci_disable_msix(np->pci_dev);\n\t\t\t\t\tnp->msi_flags &= ~NV_MSI_X_ENABLED;\n\t\t\t\t\tgoto out_err;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tsprintf(np->name_tx, \"%s-tx\", dev->name);\n\t\t\t\tret = request_irq(np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector,\n\t\t\t\t\t\t  nv_nic_irq_tx, IRQF_SHARED, np->name_tx, dev);\n\t\t\t\tif (ret) {\n\t\t\t\t\tnetdev_info(dev,\n\t\t\t\t\t\t    \"request_irq failed for tx %d\\n\",\n\t\t\t\t\t\t    ret);\n\t\t\t\t\tpci_disable_msix(np->pci_dev);\n\t\t\t\t\tnp->msi_flags &= ~NV_MSI_X_ENABLED;\n\t\t\t\t\tgoto out_free_rx;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tsprintf(np->name_other, \"%s-other\", dev->name);\n\t\t\t\tret = request_irq(np->msi_x_entry[NV_MSI_X_VECTOR_OTHER].vector,\n\t\t\t\t\t\t  nv_nic_irq_other, IRQF_SHARED, np->name_other, dev);\n\t\t\t\tif (ret) {\n\t\t\t\t\tnetdev_info(dev,\n\t\t\t\t\t\t    \"request_irq failed for link %d\\n\",\n\t\t\t\t\t\t    ret);\n\t\t\t\t\tpci_disable_msix(np->pci_dev);\n\t\t\t\t\tnp->msi_flags &= ~NV_MSI_X_ENABLED;\n\t\t\t\t\tgoto out_free_tx;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\twritel(0, base + NvRegMSIXMap0);\n\t\t\t\twritel(0, base + NvRegMSIXMap1);\n\t\t\t\tset_msix_vector_map(dev, NV_MSI_X_VECTOR_RX, NVREG_IRQ_RX_ALL);\n\t\t\t\tset_msix_vector_map(dev, NV_MSI_X_VECTOR_TX, NVREG_IRQ_TX_ALL);\n\t\t\t\tset_msix_vector_map(dev, NV_MSI_X_VECTOR_OTHER, NVREG_IRQ_OTHER);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tret = request_irq(np->msi_x_entry[NV_MSI_X_VECTOR_ALL].vector,\n\t\t\t\t\t\t  handler, IRQF_SHARED, dev->name, dev);\n\t\t\t\tif (ret) {\n\t\t\t\t\tnetdev_info(dev,\n\t\t\t\t\t\t    \"request_irq failed %d\\n\",\n\t\t\t\t\t\t    ret);\n\t\t\t\t\tpci_disable_msix(np->pci_dev);\n\t\t\t\t\tnp->msi_flags &= ~NV_MSI_X_ENABLED;\n\t\t\t\t\tgoto out_err;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\twritel(0, base + NvRegMSIXMap0);\n\t\t\t\twritel(0, base + NvRegMSIXMap1);\n\t\t\t}\n\t\t\tnetdev_info(dev, \"MSI-X enabled\\n\");\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (np->msi_flags & NV_MSI_CAPABLE) {\n\t\tret = pci_enable_msi(np->pci_dev);\n\t\tif (ret == 0) {\n\t\t\tnp->msi_flags |= NV_MSI_ENABLED;\n\t\t\tret = request_irq(np->pci_dev->irq, handler, IRQF_SHARED, dev->name, dev);\n\t\t\tif (ret) {\n\t\t\t\tnetdev_info(dev, \"request_irq failed %d\\n\",\n\t\t\t\t\t    ret);\n\t\t\t\tpci_disable_msi(np->pci_dev);\n\t\t\t\tnp->msi_flags &= ~NV_MSI_ENABLED;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\n\t\t\t \n\t\t\twritel(0, base + NvRegMSIMap0);\n\t\t\twritel(0, base + NvRegMSIMap1);\n\t\t\t \n\t\t\twritel(NVREG_MSI_VECTOR_0_ENABLED, base + NvRegMSIIrqMask);\n\t\t\tnetdev_info(dev, \"MSI enabled\\n\");\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (request_irq(np->pci_dev->irq, handler, IRQF_SHARED, dev->name, dev) != 0)\n\t\tgoto out_err;\n\n\treturn 0;\nout_free_tx:\n\tfree_irq(np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector, dev);\nout_free_rx:\n\tfree_irq(np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector, dev);\nout_err:\n\treturn 1;\n}\n\nstatic void nv_free_irq(struct net_device *dev)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\tint i;\n\n\tif (np->msi_flags & NV_MSI_X_ENABLED) {\n\t\tfor (i = 0; i < (np->msi_flags & NV_MSI_X_VECTORS_MASK); i++)\n\t\t\tfree_irq(np->msi_x_entry[i].vector, dev);\n\t\tpci_disable_msix(np->pci_dev);\n\t\tnp->msi_flags &= ~NV_MSI_X_ENABLED;\n\t} else {\n\t\tfree_irq(np->pci_dev->irq, dev);\n\t\tif (np->msi_flags & NV_MSI_ENABLED) {\n\t\t\tpci_disable_msi(np->pci_dev);\n\t\t\tnp->msi_flags &= ~NV_MSI_ENABLED;\n\t\t}\n\t}\n}\n\nstatic void nv_do_nic_poll(struct timer_list *t)\n{\n\tstruct fe_priv *np = from_timer(np, t, nic_poll);\n\tstruct net_device *dev = np->dev;\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 mask = 0;\n\tunsigned long flags;\n\tunsigned int irq = 0;\n\n\t \n\n\tif (!using_multi_irqs(dev)) {\n\t\tif (np->msi_flags & NV_MSI_X_ENABLED)\n\t\t\tirq = np->msi_x_entry[NV_MSI_X_VECTOR_ALL].vector;\n\t\telse\n\t\t\tirq = np->pci_dev->irq;\n\t\tmask = np->irqmask;\n\t} else {\n\t\tif (np->nic_poll_irq & NVREG_IRQ_RX_ALL) {\n\t\t\tirq = np->msi_x_entry[NV_MSI_X_VECTOR_RX].vector;\n\t\t\tmask |= NVREG_IRQ_RX_ALL;\n\t\t}\n\t\tif (np->nic_poll_irq & NVREG_IRQ_TX_ALL) {\n\t\t\tirq = np->msi_x_entry[NV_MSI_X_VECTOR_TX].vector;\n\t\t\tmask |= NVREG_IRQ_TX_ALL;\n\t\t}\n\t\tif (np->nic_poll_irq & NVREG_IRQ_OTHER) {\n\t\t\tirq = np->msi_x_entry[NV_MSI_X_VECTOR_OTHER].vector;\n\t\t\tmask |= NVREG_IRQ_OTHER;\n\t\t}\n\t}\n\n\tdisable_irq_nosync_lockdep_irqsave(irq, &flags);\n\tsynchronize_irq(irq);\n\n\tif (np->recover_error) {\n\t\tnp->recover_error = 0;\n\t\tnetdev_info(dev, \"MAC in recoverable error state\\n\");\n\t\tif (netif_running(dev)) {\n\t\t\tnetif_tx_lock_bh(dev);\n\t\t\tnetif_addr_lock(dev);\n\t\t\tspin_lock(&np->lock);\n\t\t\t \n\t\t\tnv_stop_rxtx(dev);\n\t\t\tif (np->driver_data & DEV_HAS_POWER_CNTRL)\n\t\t\t\tnv_mac_reset(dev);\n\t\t\tnv_txrx_reset(dev);\n\t\t\t \n\t\t\tnv_drain_rxtx(dev);\n\t\t\t \n\t\t\tset_bufsize(dev);\n\t\t\tif (nv_init_ring(dev)) {\n\t\t\t\tif (!np->in_shutdown)\n\t\t\t\t\tmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\n\t\t\t}\n\t\t\t \n\t\t\twritel(np->rx_buf_sz, base + NvRegOffloadConfig);\n\t\t\tsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\n\t\t\twritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\n\t\t\t\tbase + NvRegRingSizes);\n\t\t\tpci_push(base);\n\t\t\twritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\n\t\t\tpci_push(base);\n\t\t\t \n\t\t\tif (!(np->msi_flags & NV_MSI_X_ENABLED))\n\t\t\t\twritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\n\t\t\telse\n\t\t\t\twritel(NVREG_IRQSTAT_MASK, base + NvRegMSIXIrqStatus);\n\n\t\t\t \n\t\t\tnv_start_rxtx(dev);\n\t\t\tspin_unlock(&np->lock);\n\t\t\tnetif_addr_unlock(dev);\n\t\t\tnetif_tx_unlock_bh(dev);\n\t\t}\n\t}\n\n\twritel(mask, base + NvRegIrqMask);\n\tpci_push(base);\n\n\tif (!using_multi_irqs(dev)) {\n\t\tnp->nic_poll_irq = 0;\n\t\tif (nv_optimized(np))\n\t\t\tnv_nic_irq_optimized(0, dev);\n\t\telse\n\t\t\tnv_nic_irq(0, dev);\n\t} else {\n\t\tif (np->nic_poll_irq & NVREG_IRQ_RX_ALL) {\n\t\t\tnp->nic_poll_irq &= ~NVREG_IRQ_RX_ALL;\n\t\t\tnv_nic_irq_rx(0, dev);\n\t\t}\n\t\tif (np->nic_poll_irq & NVREG_IRQ_TX_ALL) {\n\t\t\tnp->nic_poll_irq &= ~NVREG_IRQ_TX_ALL;\n\t\t\tnv_nic_irq_tx(0, dev);\n\t\t}\n\t\tif (np->nic_poll_irq & NVREG_IRQ_OTHER) {\n\t\t\tnp->nic_poll_irq &= ~NVREG_IRQ_OTHER;\n\t\t\tnv_nic_irq_other(0, dev);\n\t\t}\n\t}\n\n\tenable_irq_lockdep_irqrestore(irq, &flags);\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void nv_poll_controller(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tnv_do_nic_poll(&np->nic_poll);\n}\n#endif\n\nstatic void nv_do_stats_poll(struct timer_list *t)\n\t__acquires(&netdev_priv(dev)->hwstats_lock)\n\t__releases(&netdev_priv(dev)->hwstats_lock)\n{\n\tstruct fe_priv *np = from_timer(np, t, stats_poll);\n\tstruct net_device *dev = np->dev;\n\n\t \n\tif (spin_trylock(&np->hwstats_lock)) {\n\t\tnv_update_stats(dev);\n\t\tspin_unlock(&np->hwstats_lock);\n\t}\n\n\tif (!np->in_shutdown)\n\t\tmod_timer(&np->stats_poll,\n\t\t\tround_jiffies(jiffies + STATS_INTERVAL));\n}\n\nstatic void nv_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tstrscpy(info->driver, DRV_NAME, sizeof(info->driver));\n\tstrscpy(info->version, FORCEDETH_VERSION, sizeof(info->version));\n\tstrscpy(info->bus_info, pci_name(np->pci_dev), sizeof(info->bus_info));\n}\n\nstatic void nv_get_wol(struct net_device *dev, struct ethtool_wolinfo *wolinfo)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\twolinfo->supported = WAKE_MAGIC;\n\n\tspin_lock_irq(&np->lock);\n\tif (np->wolenabled)\n\t\twolinfo->wolopts = WAKE_MAGIC;\n\tspin_unlock_irq(&np->lock);\n}\n\nstatic int nv_set_wol(struct net_device *dev, struct ethtool_wolinfo *wolinfo)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 flags = 0;\n\n\tif (wolinfo->wolopts == 0) {\n\t\tnp->wolenabled = 0;\n\t} else if (wolinfo->wolopts & WAKE_MAGIC) {\n\t\tnp->wolenabled = 1;\n\t\tflags = NVREG_WAKEUPFLAGS_ENABLE;\n\t}\n\tif (netif_running(dev)) {\n\t\tspin_lock_irq(&np->lock);\n\t\twritel(flags, base + NvRegWakeUpFlags);\n\t\tspin_unlock_irq(&np->lock);\n\t}\n\tdevice_set_wakeup_enable(&np->pci_dev->dev, np->wolenabled);\n\treturn 0;\n}\n\nstatic int nv_get_link_ksettings(struct net_device *dev,\n\t\t\t\t struct ethtool_link_ksettings *cmd)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu32 speed, supported, advertising;\n\tint adv;\n\n\tspin_lock_irq(&np->lock);\n\tcmd->base.port = PORT_MII;\n\tif (!netif_running(dev)) {\n\t\t \n\t\tif (nv_update_linkspeed(dev)) {\n\t\t\tnetif_carrier_on(dev);\n\t\t} else {\n\t\t\tnetif_carrier_off(dev);\n\t\t}\n\t}\n\n\tif (netif_carrier_ok(dev)) {\n\t\tswitch (np->linkspeed & (NVREG_LINKSPEED_MASK)) {\n\t\tcase NVREG_LINKSPEED_10:\n\t\t\tspeed = SPEED_10;\n\t\t\tbreak;\n\t\tcase NVREG_LINKSPEED_100:\n\t\t\tspeed = SPEED_100;\n\t\t\tbreak;\n\t\tcase NVREG_LINKSPEED_1000:\n\t\t\tspeed = SPEED_1000;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tspeed = -1;\n\t\t\tbreak;\n\t\t}\n\t\tcmd->base.duplex = DUPLEX_HALF;\n\t\tif (np->duplex)\n\t\t\tcmd->base.duplex = DUPLEX_FULL;\n\t} else {\n\t\tspeed = SPEED_UNKNOWN;\n\t\tcmd->base.duplex = DUPLEX_UNKNOWN;\n\t}\n\tcmd->base.speed = speed;\n\tcmd->base.autoneg = np->autoneg;\n\n\tadvertising = ADVERTISED_MII;\n\tif (np->autoneg) {\n\t\tadvertising |= ADVERTISED_Autoneg;\n\t\tadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\n\t\tif (adv & ADVERTISE_10HALF)\n\t\t\tadvertising |= ADVERTISED_10baseT_Half;\n\t\tif (adv & ADVERTISE_10FULL)\n\t\t\tadvertising |= ADVERTISED_10baseT_Full;\n\t\tif (adv & ADVERTISE_100HALF)\n\t\t\tadvertising |= ADVERTISED_100baseT_Half;\n\t\tif (adv & ADVERTISE_100FULL)\n\t\t\tadvertising |= ADVERTISED_100baseT_Full;\n\t\tif (np->gigabit == PHY_GIGABIT) {\n\t\t\tadv = mii_rw(dev, np->phyaddr, MII_CTRL1000, MII_READ);\n\t\t\tif (adv & ADVERTISE_1000FULL)\n\t\t\t\tadvertising |= ADVERTISED_1000baseT_Full;\n\t\t}\n\t}\n\tsupported = (SUPPORTED_Autoneg |\n\t\tSUPPORTED_10baseT_Half | SUPPORTED_10baseT_Full |\n\t\tSUPPORTED_100baseT_Half | SUPPORTED_100baseT_Full |\n\t\tSUPPORTED_MII);\n\tif (np->gigabit == PHY_GIGABIT)\n\t\tsupported |= SUPPORTED_1000baseT_Full;\n\n\tcmd->base.phy_address = np->phyaddr;\n\n\tethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.supported,\n\t\t\t\t\t\tsupported);\n\tethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.advertising,\n\t\t\t\t\t\tadvertising);\n\n\t \n\tspin_unlock_irq(&np->lock);\n\treturn 0;\n}\n\nstatic int nv_set_link_ksettings(struct net_device *dev,\n\t\t\t\t const struct ethtool_link_ksettings *cmd)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu32 speed = cmd->base.speed;\n\tu32 advertising;\n\n\tethtool_convert_link_mode_to_legacy_u32(&advertising,\n\t\t\t\t\t\tcmd->link_modes.advertising);\n\n\tif (cmd->base.port != PORT_MII)\n\t\treturn -EINVAL;\n\tif (cmd->base.phy_address != np->phyaddr) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\tif (cmd->base.autoneg == AUTONEG_ENABLE) {\n\t\tu32 mask;\n\n\t\tmask = ADVERTISED_10baseT_Half | ADVERTISED_10baseT_Full |\n\t\t\t  ADVERTISED_100baseT_Half | ADVERTISED_100baseT_Full;\n\t\tif (np->gigabit == PHY_GIGABIT)\n\t\t\tmask |= ADVERTISED_1000baseT_Full;\n\n\t\tif ((advertising & mask) == 0)\n\t\t\treturn -EINVAL;\n\n\t} else if (cmd->base.autoneg == AUTONEG_DISABLE) {\n\t\t \n\n\t\tif (speed != SPEED_10 && speed != SPEED_100)\n\t\t\treturn -EINVAL;\n\t\tif (cmd->base.duplex != DUPLEX_HALF &&\n\t\t    cmd->base.duplex != DUPLEX_FULL)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tnetif_carrier_off(dev);\n\tif (netif_running(dev)) {\n\t\tunsigned long flags;\n\n\t\tnv_disable_irq(dev);\n\t\tnetif_tx_lock_bh(dev);\n\t\tnetif_addr_lock(dev);\n\t\t \n\t\tspin_lock_irqsave(&np->lock, flags);\n\t\t \n\t\t \n\t\tnv_stop_rxtx(dev);\n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\tnetif_addr_unlock(dev);\n\t\tnetif_tx_unlock_bh(dev);\n\t}\n\n\tif (cmd->base.autoneg == AUTONEG_ENABLE) {\n\t\tint adv, bmcr;\n\n\t\tnp->autoneg = 1;\n\n\t\t \n\t\tadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\n\t\tadv &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4 | ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM);\n\t\tif (advertising & ADVERTISED_10baseT_Half)\n\t\t\tadv |= ADVERTISE_10HALF;\n\t\tif (advertising & ADVERTISED_10baseT_Full)\n\t\t\tadv |= ADVERTISE_10FULL;\n\t\tif (advertising & ADVERTISED_100baseT_Half)\n\t\t\tadv |= ADVERTISE_100HALF;\n\t\tif (advertising & ADVERTISED_100baseT_Full)\n\t\t\tadv |= ADVERTISE_100FULL;\n\t\tif (np->pause_flags & NV_PAUSEFRAME_RX_REQ)   \n\t\t\tadv |=  ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM;\n\t\tif (np->pause_flags & NV_PAUSEFRAME_TX_REQ)\n\t\t\tadv |=  ADVERTISE_PAUSE_ASYM;\n\t\tmii_rw(dev, np->phyaddr, MII_ADVERTISE, adv);\n\n\t\tif (np->gigabit == PHY_GIGABIT) {\n\t\t\tadv = mii_rw(dev, np->phyaddr, MII_CTRL1000, MII_READ);\n\t\t\tadv &= ~ADVERTISE_1000FULL;\n\t\t\tif (advertising & ADVERTISED_1000baseT_Full)\n\t\t\t\tadv |= ADVERTISE_1000FULL;\n\t\t\tmii_rw(dev, np->phyaddr, MII_CTRL1000, adv);\n\t\t}\n\n\t\tif (netif_running(dev))\n\t\t\tnetdev_info(dev, \"link down\\n\");\n\t\tbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\t\tif (np->phy_model == PHY_MODEL_MARVELL_E3016) {\n\t\t\tbmcr |= BMCR_ANENABLE;\n\t\t\t \n\t\t\tif (phy_reset(dev, bmcr)) {\n\t\t\t\tnetdev_info(dev, \"phy reset failed\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tbmcr |= (BMCR_ANENABLE | BMCR_ANRESTART);\n\t\t\tmii_rw(dev, np->phyaddr, MII_BMCR, bmcr);\n\t\t}\n\t} else {\n\t\tint adv, bmcr;\n\n\t\tnp->autoneg = 0;\n\n\t\tadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\n\t\tadv &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4 | ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM);\n\t\tif (speed == SPEED_10 && cmd->base.duplex == DUPLEX_HALF)\n\t\t\tadv |= ADVERTISE_10HALF;\n\t\tif (speed == SPEED_10 && cmd->base.duplex == DUPLEX_FULL)\n\t\t\tadv |= ADVERTISE_10FULL;\n\t\tif (speed == SPEED_100 && cmd->base.duplex == DUPLEX_HALF)\n\t\t\tadv |= ADVERTISE_100HALF;\n\t\tif (speed == SPEED_100 && cmd->base.duplex == DUPLEX_FULL)\n\t\t\tadv |= ADVERTISE_100FULL;\n\t\tnp->pause_flags &= ~(NV_PAUSEFRAME_AUTONEG|NV_PAUSEFRAME_RX_ENABLE|NV_PAUSEFRAME_TX_ENABLE);\n\t\tif (np->pause_flags & NV_PAUSEFRAME_RX_REQ) { \n\t\t\tadv |=  ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM;\n\t\t\tnp->pause_flags |= NV_PAUSEFRAME_RX_ENABLE;\n\t\t}\n\t\tif (np->pause_flags & NV_PAUSEFRAME_TX_REQ) {\n\t\t\tadv |=  ADVERTISE_PAUSE_ASYM;\n\t\t\tnp->pause_flags |= NV_PAUSEFRAME_TX_ENABLE;\n\t\t}\n\t\tmii_rw(dev, np->phyaddr, MII_ADVERTISE, adv);\n\t\tnp->fixed_mode = adv;\n\n\t\tif (np->gigabit == PHY_GIGABIT) {\n\t\t\tadv = mii_rw(dev, np->phyaddr, MII_CTRL1000, MII_READ);\n\t\t\tadv &= ~ADVERTISE_1000FULL;\n\t\t\tmii_rw(dev, np->phyaddr, MII_CTRL1000, adv);\n\t\t}\n\n\t\tbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\t\tbmcr &= ~(BMCR_ANENABLE|BMCR_SPEED100|BMCR_SPEED1000|BMCR_FULLDPLX);\n\t\tif (np->fixed_mode & (ADVERTISE_10FULL|ADVERTISE_100FULL))\n\t\t\tbmcr |= BMCR_FULLDPLX;\n\t\tif (np->fixed_mode & (ADVERTISE_100HALF|ADVERTISE_100FULL))\n\t\t\tbmcr |= BMCR_SPEED100;\n\t\tif (np->phy_oui == PHY_OUI_MARVELL) {\n\t\t\t \n\t\t\tif (phy_reset(dev, bmcr)) {\n\t\t\t\tnetdev_info(dev, \"phy reset failed\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tmii_rw(dev, np->phyaddr, MII_BMCR, bmcr);\n\t\t\tif (netif_running(dev)) {\n\t\t\t\t \n\t\t\t\tudelay(10);\n\t\t\t\tnv_linkchange(dev);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (netif_running(dev)) {\n\t\tnv_start_rxtx(dev);\n\t\tnv_enable_irq(dev);\n\t}\n\n\treturn 0;\n}\n\n#define FORCEDETH_REGS_VER\t1\n\nstatic int nv_get_regs_len(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\treturn np->register_size;\n}\n\nstatic void nv_get_regs(struct net_device *dev, struct ethtool_regs *regs, void *buf)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 *rbuf = buf;\n\tint i;\n\n\tregs->version = FORCEDETH_REGS_VER;\n\tspin_lock_irq(&np->lock);\n\tfor (i = 0; i < np->register_size/sizeof(u32); i++)\n\t\trbuf[i] = readl(base + i*sizeof(u32));\n\tspin_unlock_irq(&np->lock);\n}\n\nstatic int nv_nway_reset(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tint ret;\n\n\tif (np->autoneg) {\n\t\tint bmcr;\n\n\t\tnetif_carrier_off(dev);\n\t\tif (netif_running(dev)) {\n\t\t\tnv_disable_irq(dev);\n\t\t\tnetif_tx_lock_bh(dev);\n\t\t\tnetif_addr_lock(dev);\n\t\t\tspin_lock(&np->lock);\n\t\t\t \n\t\t\tnv_stop_rxtx(dev);\n\t\t\tspin_unlock(&np->lock);\n\t\t\tnetif_addr_unlock(dev);\n\t\t\tnetif_tx_unlock_bh(dev);\n\t\t\tnetdev_info(dev, \"link down\\n\");\n\t\t}\n\n\t\tbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\t\tif (np->phy_model == PHY_MODEL_MARVELL_E3016) {\n\t\t\tbmcr |= BMCR_ANENABLE;\n\t\t\t \n\t\t\tif (phy_reset(dev, bmcr)) {\n\t\t\t\tnetdev_info(dev, \"phy reset failed\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tbmcr |= (BMCR_ANENABLE | BMCR_ANRESTART);\n\t\t\tmii_rw(dev, np->phyaddr, MII_BMCR, bmcr);\n\t\t}\n\n\t\tif (netif_running(dev)) {\n\t\t\tnv_start_rxtx(dev);\n\t\t\tnv_enable_irq(dev);\n\t\t}\n\t\tret = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic void nv_get_ringparam(struct net_device *dev,\n\t\t\t     struct ethtool_ringparam *ring,\n\t\t\t     struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tring->rx_max_pending = (np->desc_ver == DESC_VER_1) ? RING_MAX_DESC_VER_1 : RING_MAX_DESC_VER_2_3;\n\tring->tx_max_pending = (np->desc_ver == DESC_VER_1) ? RING_MAX_DESC_VER_1 : RING_MAX_DESC_VER_2_3;\n\n\tring->rx_pending = np->rx_ring_size;\n\tring->tx_pending = np->tx_ring_size;\n}\n\nstatic int nv_set_ringparam(struct net_device *dev,\n\t\t\t    struct ethtool_ringparam *ring,\n\t\t\t    struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu8 *rxtx_ring, *rx_skbuff, *tx_skbuff;\n\tdma_addr_t ring_addr;\n\n\tif (ring->rx_pending < RX_RING_MIN ||\n\t    ring->tx_pending < TX_RING_MIN ||\n\t    ring->rx_mini_pending != 0 ||\n\t    ring->rx_jumbo_pending != 0 ||\n\t    (np->desc_ver == DESC_VER_1 &&\n\t     (ring->rx_pending > RING_MAX_DESC_VER_1 ||\n\t      ring->tx_pending > RING_MAX_DESC_VER_1)) ||\n\t    (np->desc_ver != DESC_VER_1 &&\n\t     (ring->rx_pending > RING_MAX_DESC_VER_2_3 ||\n\t      ring->tx_pending > RING_MAX_DESC_VER_2_3))) {\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!nv_optimized(np)) {\n\t\trxtx_ring = dma_alloc_coherent(&np->pci_dev->dev,\n\t\t\t\t\t       sizeof(struct ring_desc) *\n\t\t\t\t\t       (ring->rx_pending +\n\t\t\t\t\t       ring->tx_pending),\n\t\t\t\t\t       &ring_addr, GFP_ATOMIC);\n\t} else {\n\t\trxtx_ring = dma_alloc_coherent(&np->pci_dev->dev,\n\t\t\t\t\t       sizeof(struct ring_desc_ex) *\n\t\t\t\t\t       (ring->rx_pending +\n\t\t\t\t\t       ring->tx_pending),\n\t\t\t\t\t       &ring_addr, GFP_ATOMIC);\n\t}\n\trx_skbuff = kmalloc_array(ring->rx_pending, sizeof(struct nv_skb_map),\n\t\t\t\t  GFP_KERNEL);\n\ttx_skbuff = kmalloc_array(ring->tx_pending, sizeof(struct nv_skb_map),\n\t\t\t\t  GFP_KERNEL);\n\tif (!rxtx_ring || !rx_skbuff || !tx_skbuff) {\n\t\t \n\t\tif (!nv_optimized(np)) {\n\t\t\tif (rxtx_ring)\n\t\t\t\tdma_free_coherent(&np->pci_dev->dev,\n\t\t\t\t\t\t  sizeof(struct ring_desc) *\n\t\t\t\t\t\t  (ring->rx_pending +\n\t\t\t\t\t\t  ring->tx_pending),\n\t\t\t\t\t\t  rxtx_ring, ring_addr);\n\t\t} else {\n\t\t\tif (rxtx_ring)\n\t\t\t\tdma_free_coherent(&np->pci_dev->dev,\n\t\t\t\t\t\t  sizeof(struct ring_desc_ex) *\n\t\t\t\t\t\t  (ring->rx_pending +\n\t\t\t\t\t\t  ring->tx_pending),\n\t\t\t\t\t\t  rxtx_ring, ring_addr);\n\t\t}\n\n\t\tkfree(rx_skbuff);\n\t\tkfree(tx_skbuff);\n\t\tgoto exit;\n\t}\n\n\tif (netif_running(dev)) {\n\t\tnv_disable_irq(dev);\n\t\tnv_napi_disable(dev);\n\t\tnetif_tx_lock_bh(dev);\n\t\tnetif_addr_lock(dev);\n\t\tspin_lock(&np->lock);\n\t\t \n\t\tnv_stop_rxtx(dev);\n\t\tnv_txrx_reset(dev);\n\t\t \n\t\tnv_drain_rxtx(dev);\n\t\t \n\t\tfree_rings(dev);\n\t}\n\n\t \n\tnp->rx_ring_size = ring->rx_pending;\n\tnp->tx_ring_size = ring->tx_pending;\n\n\tif (!nv_optimized(np)) {\n\t\tnp->rx_ring.orig = (struct ring_desc *)rxtx_ring;\n\t\tnp->tx_ring.orig = &np->rx_ring.orig[np->rx_ring_size];\n\t} else {\n\t\tnp->rx_ring.ex = (struct ring_desc_ex *)rxtx_ring;\n\t\tnp->tx_ring.ex = &np->rx_ring.ex[np->rx_ring_size];\n\t}\n\tnp->rx_skb = (struct nv_skb_map *)rx_skbuff;\n\tnp->tx_skb = (struct nv_skb_map *)tx_skbuff;\n\tnp->ring_addr = ring_addr;\n\n\tmemset(np->rx_skb, 0, sizeof(struct nv_skb_map) * np->rx_ring_size);\n\tmemset(np->tx_skb, 0, sizeof(struct nv_skb_map) * np->tx_ring_size);\n\n\tif (netif_running(dev)) {\n\t\t \n\t\tset_bufsize(dev);\n\t\tif (nv_init_ring(dev)) {\n\t\t\tif (!np->in_shutdown)\n\t\t\t\tmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\n\t\t}\n\n\t\t \n\t\twritel(np->rx_buf_sz, base + NvRegOffloadConfig);\n\t\tsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\n\t\twritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\n\t\t\tbase + NvRegRingSizes);\n\t\tpci_push(base);\n\t\twritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\n\t\tpci_push(base);\n\n\t\t \n\t\tnv_start_rxtx(dev);\n\t\tspin_unlock(&np->lock);\n\t\tnetif_addr_unlock(dev);\n\t\tnetif_tx_unlock_bh(dev);\n\t\tnv_napi_enable(dev);\n\t\tnv_enable_irq(dev);\n\t}\n\treturn 0;\nexit:\n\treturn -ENOMEM;\n}\n\nstatic void nv_get_pauseparam(struct net_device *dev, struct ethtool_pauseparam* pause)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tpause->autoneg = (np->pause_flags & NV_PAUSEFRAME_AUTONEG) != 0;\n\tpause->rx_pause = (np->pause_flags & NV_PAUSEFRAME_RX_ENABLE) != 0;\n\tpause->tx_pause = (np->pause_flags & NV_PAUSEFRAME_TX_ENABLE) != 0;\n}\n\nstatic int nv_set_pauseparam(struct net_device *dev, struct ethtool_pauseparam* pause)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tint adv, bmcr;\n\n\tif ((!np->autoneg && np->duplex == 0) ||\n\t    (np->autoneg && !pause->autoneg && np->duplex == 0)) {\n\t\tnetdev_info(dev, \"can not set pause settings when forced link is in half duplex\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (pause->tx_pause && !(np->pause_flags & NV_PAUSEFRAME_TX_CAPABLE)) {\n\t\tnetdev_info(dev, \"hardware does not support tx pause frames\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tnetif_carrier_off(dev);\n\tif (netif_running(dev)) {\n\t\tnv_disable_irq(dev);\n\t\tnetif_tx_lock_bh(dev);\n\t\tnetif_addr_lock(dev);\n\t\tspin_lock(&np->lock);\n\t\t \n\t\tnv_stop_rxtx(dev);\n\t\tspin_unlock(&np->lock);\n\t\tnetif_addr_unlock(dev);\n\t\tnetif_tx_unlock_bh(dev);\n\t}\n\n\tnp->pause_flags &= ~(NV_PAUSEFRAME_RX_REQ|NV_PAUSEFRAME_TX_REQ);\n\tif (pause->rx_pause)\n\t\tnp->pause_flags |= NV_PAUSEFRAME_RX_REQ;\n\tif (pause->tx_pause)\n\t\tnp->pause_flags |= NV_PAUSEFRAME_TX_REQ;\n\n\tif (np->autoneg && pause->autoneg) {\n\t\tnp->pause_flags |= NV_PAUSEFRAME_AUTONEG;\n\n\t\tadv = mii_rw(dev, np->phyaddr, MII_ADVERTISE, MII_READ);\n\t\tadv &= ~(ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM);\n\t\tif (np->pause_flags & NV_PAUSEFRAME_RX_REQ)  \n\t\t\tadv |=  ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM;\n\t\tif (np->pause_flags & NV_PAUSEFRAME_TX_REQ)\n\t\t\tadv |=  ADVERTISE_PAUSE_ASYM;\n\t\tmii_rw(dev, np->phyaddr, MII_ADVERTISE, adv);\n\n\t\tif (netif_running(dev))\n\t\t\tnetdev_info(dev, \"link down\\n\");\n\t\tbmcr = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\t\tbmcr |= (BMCR_ANENABLE | BMCR_ANRESTART);\n\t\tmii_rw(dev, np->phyaddr, MII_BMCR, bmcr);\n\t} else {\n\t\tnp->pause_flags &= ~(NV_PAUSEFRAME_AUTONEG|NV_PAUSEFRAME_RX_ENABLE|NV_PAUSEFRAME_TX_ENABLE);\n\t\tif (pause->rx_pause)\n\t\t\tnp->pause_flags |= NV_PAUSEFRAME_RX_ENABLE;\n\t\tif (pause->tx_pause)\n\t\t\tnp->pause_flags |= NV_PAUSEFRAME_TX_ENABLE;\n\n\t\tif (!netif_running(dev))\n\t\t\tnv_update_linkspeed(dev);\n\t\telse\n\t\t\tnv_update_pause(dev, np->pause_flags);\n\t}\n\n\tif (netif_running(dev)) {\n\t\tnv_start_rxtx(dev);\n\t\tnv_enable_irq(dev);\n\t}\n\treturn 0;\n}\n\nstatic int nv_set_loopback(struct net_device *dev, netdev_features_t features)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tunsigned long flags;\n\tu32 miicontrol;\n\tint err, retval = 0;\n\n\tspin_lock_irqsave(&np->lock, flags);\n\tmiicontrol = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\tif (features & NETIF_F_LOOPBACK) {\n\t\tif (miicontrol & BMCR_LOOPBACK) {\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\tnetdev_info(dev, \"Loopback already enabled\\n\");\n\t\t\treturn 0;\n\t\t}\n\t\tnv_disable_irq(dev);\n\t\t \n\t\tmiicontrol |= BMCR_LOOPBACK | BMCR_FULLDPLX | BMCR_SPEED1000;\n\t\terr = mii_rw(dev, np->phyaddr, MII_BMCR, miicontrol);\n\t\tif (err) {\n\t\t\tretval = PHY_ERROR;\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\tphy_init(dev);\n\t\t} else {\n\t\t\tif (netif_running(dev)) {\n\t\t\t\t \n\t\t\t\tnv_force_linkspeed(dev, NVREG_LINKSPEED_1000,\n\t\t\t\t\t\t\t\t\t 1);\n\t\t\t\t \n\t\t\t\tnetif_carrier_on(dev);\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\tnetdev_info(dev,\n\t\t\t\t\"Internal PHY loopback mode enabled.\\n\");\n\t\t}\n\t} else {\n\t\tif (!(miicontrol & BMCR_LOOPBACK)) {\n\t\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\t\tnetdev_info(dev, \"Loopback already disabled\\n\");\n\t\t\treturn 0;\n\t\t}\n\t\tnv_disable_irq(dev);\n\t\t \n\t\tspin_unlock_irqrestore(&np->lock, flags);\n\t\tnetdev_info(dev, \"Internal PHY loopback mode disabled.\\n\");\n\t\tphy_init(dev);\n\t}\n\tmsleep(500);\n\tspin_lock_irqsave(&np->lock, flags);\n\tnv_enable_irq(dev);\n\tspin_unlock_irqrestore(&np->lock, flags);\n\n\treturn retval;\n}\n\nstatic netdev_features_t nv_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t \n\tif (features & (NETIF_F_HW_VLAN_CTAG_TX|NETIF_F_HW_VLAN_CTAG_RX))\n\t\tfeatures |= NETIF_F_RXCSUM;\n\n\treturn features;\n}\n\nstatic void nv_vlan_mode(struct net_device *dev, netdev_features_t features)\n{\n\tstruct fe_priv *np = get_nvpriv(dev);\n\n\tspin_lock_irq(&np->lock);\n\n\tif (features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tnp->txrxctl_bits |= NVREG_TXRXCTL_VLANSTRIP;\n\telse\n\t\tnp->txrxctl_bits &= ~NVREG_TXRXCTL_VLANSTRIP;\n\n\tif (features & NETIF_F_HW_VLAN_CTAG_TX)\n\t\tnp->txrxctl_bits |= NVREG_TXRXCTL_VLANINS;\n\telse\n\t\tnp->txrxctl_bits &= ~NVREG_TXRXCTL_VLANINS;\n\n\twritel(np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\n\n\tspin_unlock_irq(&np->lock);\n}\n\nstatic int nv_set_features(struct net_device *dev, netdev_features_t features)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tnetdev_features_t changed = dev->features ^ features;\n\tint retval;\n\n\tif ((changed & NETIF_F_LOOPBACK) && netif_running(dev)) {\n\t\tretval = nv_set_loopback(dev, features);\n\t\tif (retval != 0)\n\t\t\treturn retval;\n\t}\n\n\tif (changed & NETIF_F_RXCSUM) {\n\t\tspin_lock_irq(&np->lock);\n\n\t\tif (features & NETIF_F_RXCSUM)\n\t\t\tnp->txrxctl_bits |= NVREG_TXRXCTL_RXCHECK;\n\t\telse\n\t\t\tnp->txrxctl_bits &= ~NVREG_TXRXCTL_RXCHECK;\n\n\t\tif (netif_running(dev))\n\t\t\twritel(np->txrxctl_bits, base + NvRegTxRxControl);\n\n\t\tspin_unlock_irq(&np->lock);\n\t}\n\n\tif (changed & (NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX))\n\t\tnv_vlan_mode(dev, features);\n\n\treturn 0;\n}\n\nstatic int nv_get_sset_count(struct net_device *dev, int sset)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tswitch (sset) {\n\tcase ETH_SS_TEST:\n\t\tif (np->driver_data & DEV_HAS_TEST_EXTENDED)\n\t\t\treturn NV_TEST_COUNT_EXTENDED;\n\t\telse\n\t\t\treturn NV_TEST_COUNT_BASE;\n\tcase ETH_SS_STATS:\n\t\tif (np->driver_data & DEV_HAS_STATISTICS_V3)\n\t\t\treturn NV_DEV_STATISTICS_V3_COUNT;\n\t\telse if (np->driver_data & DEV_HAS_STATISTICS_V2)\n\t\t\treturn NV_DEV_STATISTICS_V2_COUNT;\n\t\telse if (np->driver_data & DEV_HAS_STATISTICS_V1)\n\t\t\treturn NV_DEV_STATISTICS_V1_COUNT;\n\t\telse\n\t\t\treturn 0;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic void nv_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t struct ethtool_stats *estats, u64 *buffer)\n\t__acquires(&netdev_priv(dev)->hwstats_lock)\n\t__releases(&netdev_priv(dev)->hwstats_lock)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tspin_lock_bh(&np->hwstats_lock);\n\tnv_update_stats(dev);\n\tmemcpy(buffer, &np->estats,\n\t       nv_get_sset_count(dev, ETH_SS_STATS)*sizeof(u64));\n\tspin_unlock_bh(&np->hwstats_lock);\n}\n\nstatic int nv_link_test(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tint mii_status;\n\n\tmii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\n\tmii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\n\n\t \n\tif (!(mii_status & BMSR_LSTATUS))\n\t\treturn 0;\n\telse\n\t\treturn 1;\n}\n\nstatic int nv_register_test(struct net_device *dev)\n{\n\tu8 __iomem *base = get_hwbase(dev);\n\tint i = 0;\n\tu32 orig_read, new_read;\n\n\tdo {\n\t\torig_read = readl(base + nv_registers_test[i].reg);\n\n\t\t \n\t\torig_read ^= nv_registers_test[i].mask;\n\n\t\twritel(orig_read, base + nv_registers_test[i].reg);\n\n\t\tnew_read = readl(base + nv_registers_test[i].reg);\n\n\t\tif ((new_read & nv_registers_test[i].mask) != (orig_read & nv_registers_test[i].mask))\n\t\t\treturn 0;\n\n\t\t \n\t\torig_read ^= nv_registers_test[i].mask;\n\t\twritel(orig_read, base + nv_registers_test[i].reg);\n\n\t} while (nv_registers_test[++i].reg != 0);\n\n\treturn 1;\n}\n\nstatic int nv_interrupt_test(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tint ret = 1;\n\tint testcnt;\n\tu32 save_msi_flags, save_poll_interval = 0;\n\n\tif (netif_running(dev)) {\n\t\t \n\t\tnv_free_irq(dev);\n\t\tsave_poll_interval = readl(base+NvRegPollingInterval);\n\t}\n\n\t \n\tnp->intr_test = 0;\n\n\t \n\tsave_msi_flags = np->msi_flags;\n\tnp->msi_flags &= ~NV_MSI_X_VECTORS_MASK;\n\tnp->msi_flags |= 0x001;  \n\tif (nv_request_irq(dev, 1))\n\t\treturn 0;\n\n\t \n\twritel(NVREG_POLL_DEFAULT_CPU, base + NvRegPollingInterval);\n\twritel(NVREG_UNKSETUP6_VAL, base + NvRegUnknownSetupReg6);\n\n\tnv_enable_hw_interrupts(dev, NVREG_IRQ_TIMER);\n\n\t \n\tmsleep(100);\n\n\tspin_lock_irq(&np->lock);\n\n\t \n\ttestcnt = np->intr_test;\n\tif (!testcnt)\n\t\tret = 2;\n\n\tnv_disable_hw_interrupts(dev, NVREG_IRQ_TIMER);\n\tif (!(np->msi_flags & NV_MSI_X_ENABLED))\n\t\twritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\n\telse\n\t\twritel(NVREG_IRQSTAT_MASK, base + NvRegMSIXIrqStatus);\n\n\tspin_unlock_irq(&np->lock);\n\n\tnv_free_irq(dev);\n\n\tnp->msi_flags = save_msi_flags;\n\n\tif (netif_running(dev)) {\n\t\twritel(save_poll_interval, base + NvRegPollingInterval);\n\t\twritel(NVREG_UNKSETUP6_VAL, base + NvRegUnknownSetupReg6);\n\t\t \n\t\tif (nv_request_irq(dev, 0))\n\t\t\treturn 0;\n\t}\n\n\treturn ret;\n}\n\nstatic int nv_loopback_test(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tstruct sk_buff *tx_skb, *rx_skb;\n\tdma_addr_t test_dma_addr;\n\tu32 tx_flags_extra = (np->desc_ver == DESC_VER_1 ? NV_TX_LASTPACKET : NV_TX2_LASTPACKET);\n\tu32 flags;\n\tint len, i, pkt_len;\n\tu8 *pkt_data;\n\tu32 filter_flags = 0;\n\tu32 misc1_flags = 0;\n\tint ret = 1;\n\n\tif (netif_running(dev)) {\n\t\tnv_disable_irq(dev);\n\t\tfilter_flags = readl(base + NvRegPacketFilterFlags);\n\t\tmisc1_flags = readl(base + NvRegMisc1);\n\t} else {\n\t\tnv_txrx_reset(dev);\n\t}\n\n\t \n\tset_bufsize(dev);\n\tnv_init_ring(dev);\n\n\t \n\twritel(NVREG_MISC1_FORCE, base + NvRegMisc1);\n\twritel(NVREG_PFF_ALWAYS | NVREG_PFF_LOOPBACK, base + NvRegPacketFilterFlags);\n\n\t \n\twritel(np->rx_buf_sz, base + NvRegOffloadConfig);\n\tsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\n\twritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\n\t\tbase + NvRegRingSizes);\n\tpci_push(base);\n\n\t \n\tnv_start_rxtx(dev);\n\n\t \n\tpkt_len = ETH_DATA_LEN;\n\ttx_skb = netdev_alloc_skb(dev, pkt_len);\n\tif (!tx_skb) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\ttest_dma_addr = dma_map_single(&np->pci_dev->dev, tx_skb->data,\n\t\t\t\t       skb_tailroom(tx_skb),\n\t\t\t\t       DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(&np->pci_dev->dev,\n\t\t\t\t       test_dma_addr))) {\n\t\tdev_kfree_skb_any(tx_skb);\n\t\tgoto out;\n\t}\n\tpkt_data = skb_put(tx_skb, pkt_len);\n\tfor (i = 0; i < pkt_len; i++)\n\t\tpkt_data[i] = (u8)(i & 0xff);\n\n\tif (!nv_optimized(np)) {\n\t\tnp->tx_ring.orig[0].buf = cpu_to_le32(test_dma_addr);\n\t\tnp->tx_ring.orig[0].flaglen = cpu_to_le32((pkt_len-1) | np->tx_flags | tx_flags_extra);\n\t} else {\n\t\tnp->tx_ring.ex[0].bufhigh = cpu_to_le32(dma_high(test_dma_addr));\n\t\tnp->tx_ring.ex[0].buflow = cpu_to_le32(dma_low(test_dma_addr));\n\t\tnp->tx_ring.ex[0].flaglen = cpu_to_le32((pkt_len-1) | np->tx_flags | tx_flags_extra);\n\t}\n\twritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\n\tpci_push(get_hwbase(dev));\n\n\tmsleep(500);\n\n\t \n\tif (!nv_optimized(np)) {\n\t\tflags = le32_to_cpu(np->rx_ring.orig[0].flaglen);\n\t\tlen = nv_descr_getlength(&np->rx_ring.orig[0], np->desc_ver);\n\n\t} else {\n\t\tflags = le32_to_cpu(np->rx_ring.ex[0].flaglen);\n\t\tlen = nv_descr_getlength_ex(&np->rx_ring.ex[0], np->desc_ver);\n\t}\n\n\tif (flags & NV_RX_AVAIL) {\n\t\tret = 0;\n\t} else if (np->desc_ver == DESC_VER_1) {\n\t\tif (flags & NV_RX_ERROR)\n\t\t\tret = 0;\n\t} else {\n\t\tif (flags & NV_RX2_ERROR)\n\t\t\tret = 0;\n\t}\n\n\tif (ret) {\n\t\tif (len != pkt_len) {\n\t\t\tret = 0;\n\t\t} else {\n\t\t\trx_skb = np->rx_skb[0].skb;\n\t\t\tfor (i = 0; i < pkt_len; i++) {\n\t\t\t\tif (rx_skb->data[i] != (u8)(i & 0xff)) {\n\t\t\t\t\tret = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tdma_unmap_single(&np->pci_dev->dev, test_dma_addr,\n\t\t\t (skb_end_pointer(tx_skb) - tx_skb->data),\n\t\t\t DMA_TO_DEVICE);\n\tdev_kfree_skb_any(tx_skb);\n out:\n\t \n\tnv_stop_rxtx(dev);\n\tnv_txrx_reset(dev);\n\t \n\tnv_drain_rxtx(dev);\n\n\tif (netif_running(dev)) {\n\t\twritel(misc1_flags, base + NvRegMisc1);\n\t\twritel(filter_flags, base + NvRegPacketFilterFlags);\n\t\tnv_enable_irq(dev);\n\t}\n\n\treturn ret;\n}\n\nstatic void nv_self_test(struct net_device *dev, struct ethtool_test *test, u64 *buffer)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tint result, count;\n\n\tcount = nv_get_sset_count(dev, ETH_SS_TEST);\n\tmemset(buffer, 0, count * sizeof(u64));\n\n\tif (!nv_link_test(dev)) {\n\t\ttest->flags |= ETH_TEST_FL_FAILED;\n\t\tbuffer[0] = 1;\n\t}\n\n\tif (test->flags & ETH_TEST_FL_OFFLINE) {\n\t\tif (netif_running(dev)) {\n\t\t\tnetif_stop_queue(dev);\n\t\t\tnv_napi_disable(dev);\n\t\t\tnetif_tx_lock_bh(dev);\n\t\t\tnetif_addr_lock(dev);\n\t\t\tspin_lock_irq(&np->lock);\n\t\t\tnv_disable_hw_interrupts(dev, np->irqmask);\n\t\t\tif (!(np->msi_flags & NV_MSI_X_ENABLED))\n\t\t\t\twritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\n\t\t\telse\n\t\t\t\twritel(NVREG_IRQSTAT_MASK, base + NvRegMSIXIrqStatus);\n\t\t\t \n\t\t\tnv_stop_rxtx(dev);\n\t\t\tnv_txrx_reset(dev);\n\t\t\t \n\t\t\tnv_drain_rxtx(dev);\n\t\t\tspin_unlock_irq(&np->lock);\n\t\t\tnetif_addr_unlock(dev);\n\t\t\tnetif_tx_unlock_bh(dev);\n\t\t}\n\n\t\tif (!nv_register_test(dev)) {\n\t\t\ttest->flags |= ETH_TEST_FL_FAILED;\n\t\t\tbuffer[1] = 1;\n\t\t}\n\n\t\tresult = nv_interrupt_test(dev);\n\t\tif (result != 1) {\n\t\t\ttest->flags |= ETH_TEST_FL_FAILED;\n\t\t\tbuffer[2] = 1;\n\t\t}\n\t\tif (result == 0) {\n\t\t\t \n\t\t\treturn;\n\t\t}\n\n\t\tif (count > NV_TEST_COUNT_BASE && !nv_loopback_test(dev)) {\n\t\t\ttest->flags |= ETH_TEST_FL_FAILED;\n\t\t\tbuffer[3] = 1;\n\t\t}\n\n\t\tif (netif_running(dev)) {\n\t\t\t \n\t\t\tset_bufsize(dev);\n\t\t\tif (nv_init_ring(dev)) {\n\t\t\t\tif (!np->in_shutdown)\n\t\t\t\t\tmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\n\t\t\t}\n\t\t\t \n\t\t\twritel(np->rx_buf_sz, base + NvRegOffloadConfig);\n\t\t\tsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\n\t\t\twritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\n\t\t\t\tbase + NvRegRingSizes);\n\t\t\tpci_push(base);\n\t\t\twritel(NVREG_TXRXCTL_KICK|np->txrxctl_bits, get_hwbase(dev) + NvRegTxRxControl);\n\t\t\tpci_push(base);\n\t\t\t \n\t\t\tnv_start_rxtx(dev);\n\t\t\tnetif_start_queue(dev);\n\t\t\tnv_napi_enable(dev);\n\t\t\tnv_enable_hw_interrupts(dev, np->irqmask);\n\t\t}\n\t}\n}\n\nstatic void nv_get_strings(struct net_device *dev, u32 stringset, u8 *buffer)\n{\n\tswitch (stringset) {\n\tcase ETH_SS_STATS:\n\t\tmemcpy(buffer, &nv_estats_str, nv_get_sset_count(dev, ETH_SS_STATS)*sizeof(struct nv_ethtool_str));\n\t\tbreak;\n\tcase ETH_SS_TEST:\n\t\tmemcpy(buffer, &nv_etests_str, nv_get_sset_count(dev, ETH_SS_TEST)*sizeof(struct nv_ethtool_str));\n\t\tbreak;\n\t}\n}\n\nstatic const struct ethtool_ops ops = {\n\t.get_drvinfo = nv_get_drvinfo,\n\t.get_link = ethtool_op_get_link,\n\t.get_wol = nv_get_wol,\n\t.set_wol = nv_set_wol,\n\t.get_regs_len = nv_get_regs_len,\n\t.get_regs = nv_get_regs,\n\t.nway_reset = nv_nway_reset,\n\t.get_ringparam = nv_get_ringparam,\n\t.set_ringparam = nv_set_ringparam,\n\t.get_pauseparam = nv_get_pauseparam,\n\t.set_pauseparam = nv_set_pauseparam,\n\t.get_strings = nv_get_strings,\n\t.get_ethtool_stats = nv_get_ethtool_stats,\n\t.get_sset_count = nv_get_sset_count,\n\t.self_test = nv_self_test,\n\t.get_ts_info = ethtool_op_get_ts_info,\n\t.get_link_ksettings = nv_get_link_ksettings,\n\t.set_link_ksettings = nv_set_link_ksettings,\n};\n\n \nstatic int nv_mgmt_acquire_sema(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tint i;\n\tu32 tx_ctrl, mgmt_sema;\n\n\tfor (i = 0; i < 10; i++) {\n\t\tmgmt_sema = readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_MGMT_SEMA_MASK;\n\t\tif (mgmt_sema == NVREG_XMITCTL_MGMT_SEMA_FREE)\n\t\t\tbreak;\n\t\tmsleep(500);\n\t}\n\n\tif (mgmt_sema != NVREG_XMITCTL_MGMT_SEMA_FREE)\n\t\treturn 0;\n\n\tfor (i = 0; i < 2; i++) {\n\t\ttx_ctrl = readl(base + NvRegTransmitterControl);\n\t\ttx_ctrl |= NVREG_XMITCTL_HOST_SEMA_ACQ;\n\t\twritel(tx_ctrl, base + NvRegTransmitterControl);\n\n\t\t \n\t\ttx_ctrl = readl(base + NvRegTransmitterControl);\n\t\tif (((tx_ctrl & NVREG_XMITCTL_HOST_SEMA_MASK) == NVREG_XMITCTL_HOST_SEMA_ACQ) &&\n\t\t    ((tx_ctrl & NVREG_XMITCTL_MGMT_SEMA_MASK) == NVREG_XMITCTL_MGMT_SEMA_FREE)) {\n\t\t\tnp->mgmt_sema = 1;\n\t\t\treturn 1;\n\t\t} else\n\t\t\tudelay(50);\n\t}\n\n\treturn 0;\n}\n\nstatic void nv_mgmt_release_sema(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 tx_ctrl;\n\n\tif (np->driver_data & DEV_HAS_MGMT_UNIT) {\n\t\tif (np->mgmt_sema) {\n\t\t\ttx_ctrl = readl(base + NvRegTransmitterControl);\n\t\t\ttx_ctrl &= ~NVREG_XMITCTL_HOST_SEMA_ACQ;\n\t\t\twritel(tx_ctrl, base + NvRegTransmitterControl);\n\t\t}\n\t}\n}\n\n\nstatic int nv_mgmt_get_version(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tu32 data_ready = readl(base + NvRegTransmitterControl);\n\tu32 data_ready2 = 0;\n\tunsigned long start;\n\tint ready = 0;\n\n\twritel(NVREG_MGMTUNITGETVERSION, base + NvRegMgmtUnitGetVersion);\n\twritel(data_ready ^ NVREG_XMITCTL_DATA_START, base + NvRegTransmitterControl);\n\tstart = jiffies;\n\twhile (time_before(jiffies, start + 5*HZ)) {\n\t\tdata_ready2 = readl(base + NvRegTransmitterControl);\n\t\tif ((data_ready & NVREG_XMITCTL_DATA_READY) != (data_ready2 & NVREG_XMITCTL_DATA_READY)) {\n\t\t\tready = 1;\n\t\t\tbreak;\n\t\t}\n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\n\tif (!ready || (data_ready2 & NVREG_XMITCTL_DATA_ERROR))\n\t\treturn 0;\n\n\tnp->mgmt_version = readl(base + NvRegMgmtUnitVersion) & NVREG_MGMTUNITVERSION;\n\n\treturn 1;\n}\n\nstatic int nv_open(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tint ret = 1;\n\tint oom, i;\n\tu32 low;\n\n\t \n\tmii_rw(dev, np->phyaddr, MII_BMCR,\n\t       mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ) & ~BMCR_PDOWN);\n\n\tnv_txrx_gate(dev, false);\n\t \n\tif (np->driver_data & DEV_HAS_POWER_CNTRL)\n\t\tnv_mac_reset(dev);\n\twritel(NVREG_MCASTADDRA_FORCE, base + NvRegMulticastAddrA);\n\twritel(0, base + NvRegMulticastAddrB);\n\twritel(NVREG_MCASTMASKA_NONE, base + NvRegMulticastMaskA);\n\twritel(NVREG_MCASTMASKB_NONE, base + NvRegMulticastMaskB);\n\twritel(0, base + NvRegPacketFilterFlags);\n\n\twritel(0, base + NvRegTransmitterControl);\n\twritel(0, base + NvRegReceiverControl);\n\n\twritel(0, base + NvRegAdapterControl);\n\n\tif (np->pause_flags & NV_PAUSEFRAME_TX_CAPABLE)\n\t\twritel(NVREG_TX_PAUSEFRAME_DISABLE,  base + NvRegTxPauseFrame);\n\n\t \n\tset_bufsize(dev);\n\toom = nv_init_ring(dev);\n\n\twritel(0, base + NvRegLinkSpeed);\n\twritel(readl(base + NvRegTransmitPoll) & NVREG_TRANSMITPOLL_MAC_ADDR_REV, base + NvRegTransmitPoll);\n\tnv_txrx_reset(dev);\n\twritel(0, base + NvRegUnknownSetupReg6);\n\n\tnp->in_shutdown = 0;\n\n\t \n\tsetup_hw_rings(dev, NV_SETUP_RX_RING | NV_SETUP_TX_RING);\n\twritel(((np->rx_ring_size-1) << NVREG_RINGSZ_RXSHIFT) + ((np->tx_ring_size-1) << NVREG_RINGSZ_TXSHIFT),\n\t\tbase + NvRegRingSizes);\n\n\twritel(np->linkspeed, base + NvRegLinkSpeed);\n\tif (np->desc_ver == DESC_VER_1)\n\t\twritel(NVREG_TX_WM_DESC1_DEFAULT, base + NvRegTxWatermark);\n\telse\n\t\twritel(NVREG_TX_WM_DESC2_3_DEFAULT, base + NvRegTxWatermark);\n\twritel(np->txrxctl_bits, base + NvRegTxRxControl);\n\twritel(np->vlanctl_bits, base + NvRegVlanControl);\n\tpci_push(base);\n\twritel(NVREG_TXRXCTL_BIT1|np->txrxctl_bits, base + NvRegTxRxControl);\n\tif (reg_delay(dev, NvRegUnknownSetupReg5,\n\t\t      NVREG_UNKSETUP5_BIT31, NVREG_UNKSETUP5_BIT31,\n\t\t      NV_SETUP5_DELAY, NV_SETUP5_DELAYMAX))\n\t\tnetdev_info(dev,\n\t\t\t    \"%s: SetupReg5, Bit 31 remained off\\n\", __func__);\n\n\twritel(0, base + NvRegMIIMask);\n\twritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\n\twritel(NVREG_MIISTAT_MASK_ALL, base + NvRegMIIStatus);\n\n\twritel(NVREG_MISC1_FORCE | NVREG_MISC1_HD, base + NvRegMisc1);\n\twritel(readl(base + NvRegTransmitterStatus), base + NvRegTransmitterStatus);\n\twritel(NVREG_PFF_ALWAYS, base + NvRegPacketFilterFlags);\n\twritel(np->rx_buf_sz, base + NvRegOffloadConfig);\n\n\twritel(readl(base + NvRegReceiverStatus), base + NvRegReceiverStatus);\n\n\tget_random_bytes(&low, sizeof(low));\n\tlow &= NVREG_SLOTTIME_MASK;\n\tif (np->desc_ver == DESC_VER_1) {\n\t\twritel(low|NVREG_SLOTTIME_DEFAULT, base + NvRegSlotTime);\n\t} else {\n\t\tif (!(np->driver_data & DEV_HAS_GEAR_MODE)) {\n\t\t\t \n\t\t\twritel(NVREG_SLOTTIME_LEGBF_ENABLED|NVREG_SLOTTIME_10_100_FULL|low, base + NvRegSlotTime);\n\t\t} else {\n\t\t\twritel(NVREG_SLOTTIME_10_100_FULL, base + NvRegSlotTime);\n\t\t\tnv_gear_backoff_reseed(dev);\n\t\t}\n\t}\n\twritel(NVREG_TX_DEFERRAL_DEFAULT, base + NvRegTxDeferral);\n\twritel(NVREG_RX_DEFERRAL_DEFAULT, base + NvRegRxDeferral);\n\tif (poll_interval == -1) {\n\t\tif (optimization_mode == NV_OPTIMIZATION_MODE_THROUGHPUT)\n\t\t\twritel(NVREG_POLL_DEFAULT_THROUGHPUT, base + NvRegPollingInterval);\n\t\telse\n\t\t\twritel(NVREG_POLL_DEFAULT_CPU, base + NvRegPollingInterval);\n\t} else\n\t\twritel(poll_interval & 0xFFFF, base + NvRegPollingInterval);\n\twritel(NVREG_UNKSETUP6_VAL, base + NvRegUnknownSetupReg6);\n\twritel((np->phyaddr << NVREG_ADAPTCTL_PHYSHIFT)|NVREG_ADAPTCTL_PHYVALID|NVREG_ADAPTCTL_RUNNING,\n\t\t\tbase + NvRegAdapterControl);\n\twritel(NVREG_MIISPEED_BIT8|NVREG_MIIDELAY, base + NvRegMIISpeed);\n\twritel(NVREG_MII_LINKCHANGE, base + NvRegMIIMask);\n\tif (np->wolenabled)\n\t\twritel(NVREG_WAKEUPFLAGS_ENABLE , base + NvRegWakeUpFlags);\n\n\ti = readl(base + NvRegPowerState);\n\tif ((i & NVREG_POWERSTATE_POWEREDUP) == 0)\n\t\twritel(NVREG_POWERSTATE_POWEREDUP|i, base + NvRegPowerState);\n\n\tpci_push(base);\n\tudelay(10);\n\twritel(readl(base + NvRegPowerState) | NVREG_POWERSTATE_VALID, base + NvRegPowerState);\n\n\tnv_disable_hw_interrupts(dev, np->irqmask);\n\tpci_push(base);\n\twritel(NVREG_MIISTAT_MASK_ALL, base + NvRegMIIStatus);\n\twritel(NVREG_IRQSTAT_MASK, base + NvRegIrqStatus);\n\tpci_push(base);\n\n\tif (nv_request_irq(dev, 0))\n\t\tgoto out_drain;\n\n\t \n\tnv_enable_hw_interrupts(dev, np->irqmask);\n\n\tspin_lock_irq(&np->lock);\n\twritel(NVREG_MCASTADDRA_FORCE, base + NvRegMulticastAddrA);\n\twritel(0, base + NvRegMulticastAddrB);\n\twritel(NVREG_MCASTMASKA_NONE, base + NvRegMulticastMaskA);\n\twritel(NVREG_MCASTMASKB_NONE, base + NvRegMulticastMaskB);\n\twritel(NVREG_PFF_ALWAYS|NVREG_PFF_MYADDR, base + NvRegPacketFilterFlags);\n\t \n\treadl(base + NvRegMIIStatus);\n\twritel(NVREG_MIISTAT_MASK_ALL, base + NvRegMIIStatus);\n\n\t \n\tnp->linkspeed = 0;\n\tret = nv_update_linkspeed(dev);\n\tnv_start_rxtx(dev);\n\tnetif_start_queue(dev);\n\tnv_napi_enable(dev);\n\n\tif (ret) {\n\t\tnetif_carrier_on(dev);\n\t} else {\n\t\tnetdev_info(dev, \"no link during initialization\\n\");\n\t\tnetif_carrier_off(dev);\n\t}\n\tif (oom)\n\t\tmod_timer(&np->oom_kick, jiffies + OOM_REFILL);\n\n\t \n\tif (np->driver_data & (DEV_HAS_STATISTICS_V1|DEV_HAS_STATISTICS_V2|DEV_HAS_STATISTICS_V3))\n\t\tmod_timer(&np->stats_poll,\n\t\t\tround_jiffies(jiffies + STATS_INTERVAL));\n\n\tspin_unlock_irq(&np->lock);\n\n\t \n\tif (dev->features & NETIF_F_LOOPBACK)\n\t\tnv_set_loopback(dev, dev->features);\n\n\treturn 0;\nout_drain:\n\tnv_drain_rxtx(dev);\n\treturn ret;\n}\n\nstatic int nv_close(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base;\n\n\tspin_lock_irq(&np->lock);\n\tnp->in_shutdown = 1;\n\tspin_unlock_irq(&np->lock);\n\tnv_napi_disable(dev);\n\tsynchronize_irq(np->pci_dev->irq);\n\n\tdel_timer_sync(&np->oom_kick);\n\tdel_timer_sync(&np->nic_poll);\n\tdel_timer_sync(&np->stats_poll);\n\n\tnetif_stop_queue(dev);\n\tspin_lock_irq(&np->lock);\n\tnv_update_pause(dev, 0);  \n\tnv_stop_rxtx(dev);\n\tnv_txrx_reset(dev);\n\n\t \n\tbase = get_hwbase(dev);\n\tnv_disable_hw_interrupts(dev, np->irqmask);\n\tpci_push(base);\n\n\tspin_unlock_irq(&np->lock);\n\n\tnv_free_irq(dev);\n\n\tnv_drain_rxtx(dev);\n\n\tif (np->wolenabled || !phy_power_down) {\n\t\tnv_txrx_gate(dev, false);\n\t\twritel(NVREG_PFF_ALWAYS|NVREG_PFF_MYADDR, base + NvRegPacketFilterFlags);\n\t\tnv_start_rx(dev);\n\t} else {\n\t\t \n\t\tmii_rw(dev, np->phyaddr, MII_BMCR,\n\t\t       mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ)|BMCR_PDOWN);\n\t\tnv_txrx_gate(dev, true);\n\t}\n\n\t \n\n\treturn 0;\n}\n\nstatic const struct net_device_ops nv_netdev_ops = {\n\t.ndo_open\t\t= nv_open,\n\t.ndo_stop\t\t= nv_close,\n\t.ndo_get_stats64\t= nv_get_stats64,\n\t.ndo_start_xmit\t\t= nv_start_xmit,\n\t.ndo_tx_timeout\t\t= nv_tx_timeout,\n\t.ndo_change_mtu\t\t= nv_change_mtu,\n\t.ndo_fix_features\t= nv_fix_features,\n\t.ndo_set_features\t= nv_set_features,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_mac_address\t= nv_set_mac_address,\n\t.ndo_set_rx_mode\t= nv_set_multicast,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= nv_poll_controller,\n#endif\n};\n\nstatic const struct net_device_ops nv_netdev_ops_optimized = {\n\t.ndo_open\t\t= nv_open,\n\t.ndo_stop\t\t= nv_close,\n\t.ndo_get_stats64\t= nv_get_stats64,\n\t.ndo_start_xmit\t\t= nv_start_xmit_optimized,\n\t.ndo_tx_timeout\t\t= nv_tx_timeout,\n\t.ndo_change_mtu\t\t= nv_change_mtu,\n\t.ndo_fix_features\t= nv_fix_features,\n\t.ndo_set_features\t= nv_set_features,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_mac_address\t= nv_set_mac_address,\n\t.ndo_set_rx_mode\t= nv_set_multicast,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= nv_poll_controller,\n#endif\n};\n\nstatic int nv_probe(struct pci_dev *pci_dev, const struct pci_device_id *id)\n{\n\tstruct net_device *dev;\n\tstruct fe_priv *np;\n\tunsigned long addr;\n\tu8 __iomem *base;\n\tint err, i;\n\tu32 powerstate, txreg;\n\tu32 phystate_orig = 0, phystate;\n\tint phyinitialized = 0;\n\tstatic int printed_version;\n\tu8 mac[ETH_ALEN];\n\n\tif (!printed_version++)\n\t\tpr_info(\"Reverse Engineered nForce ethernet driver. Version %s.\\n\",\n\t\t\tFORCEDETH_VERSION);\n\n\tdev = alloc_etherdev(sizeof(struct fe_priv));\n\terr = -ENOMEM;\n\tif (!dev)\n\t\tgoto out;\n\n\tnp = netdev_priv(dev);\n\tnp->dev = dev;\n\tnp->pci_dev = pci_dev;\n\tspin_lock_init(&np->lock);\n\tspin_lock_init(&np->hwstats_lock);\n\tSET_NETDEV_DEV(dev, &pci_dev->dev);\n\tu64_stats_init(&np->swstats_rx_syncp);\n\tu64_stats_init(&np->swstats_tx_syncp);\n\tnp->txrx_stats = alloc_percpu(struct nv_txrx_stats);\n\tif (!np->txrx_stats) {\n\t\tpr_err(\"np->txrx_stats, alloc memory error.\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto out_alloc_percpu;\n\t}\n\n\ttimer_setup(&np->oom_kick, nv_do_rx_refill, 0);\n\ttimer_setup(&np->nic_poll, nv_do_nic_poll, 0);\n\ttimer_setup(&np->stats_poll, nv_do_stats_poll, TIMER_DEFERRABLE);\n\n\terr = pci_enable_device(pci_dev);\n\tif (err)\n\t\tgoto out_free;\n\n\tpci_set_master(pci_dev);\n\n\terr = pci_request_regions(pci_dev, DRV_NAME);\n\tif (err < 0)\n\t\tgoto out_disable;\n\n\tif (id->driver_data & (DEV_HAS_VLAN|DEV_HAS_MSI_X|DEV_HAS_POWER_CNTRL|DEV_HAS_STATISTICS_V2|DEV_HAS_STATISTICS_V3))\n\t\tnp->register_size = NV_PCI_REGSZ_VER3;\n\telse if (id->driver_data & DEV_HAS_STATISTICS_V1)\n\t\tnp->register_size = NV_PCI_REGSZ_VER2;\n\telse\n\t\tnp->register_size = NV_PCI_REGSZ_VER1;\n\n\terr = -EINVAL;\n\taddr = 0;\n\tfor (i = 0; i < DEVICE_COUNT_RESOURCE; i++) {\n\t\tif (pci_resource_flags(pci_dev, i) & IORESOURCE_MEM &&\n\t\t\t\tpci_resource_len(pci_dev, i) >= np->register_size) {\n\t\t\taddr = pci_resource_start(pci_dev, i);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (i == DEVICE_COUNT_RESOURCE) {\n\t\tdev_info(&pci_dev->dev, \"Couldn't find register window\\n\");\n\t\tgoto out_relreg;\n\t}\n\n\t \n\tnp->driver_data = id->driver_data;\n\t \n\tnp->device_id = id->device;\n\n\t \n\tif (id->driver_data & DEV_HAS_HIGH_DMA) {\n\t\t \n\t\tnp->desc_ver = DESC_VER_3;\n\t\tnp->txrxctl_bits = NVREG_TXRXCTL_DESC_3;\n\t\tif (dma_64bit) {\n\t\t\tif (dma_set_mask_and_coherent(&pci_dev->dev, DMA_BIT_MASK(39)))\n\t\t\t\tdev_info(&pci_dev->dev,\n\t\t\t\t\t \"64-bit DMA failed, using 32-bit addressing\\n\");\n\t\t\telse\n\t\t\t\tdev->features |= NETIF_F_HIGHDMA;\n\t\t}\n\t} else if (id->driver_data & DEV_HAS_LARGEDESC) {\n\t\t \n\t\tnp->desc_ver = DESC_VER_2;\n\t\tnp->txrxctl_bits = NVREG_TXRXCTL_DESC_2;\n\t} else {\n\t\t \n\t\tnp->desc_ver = DESC_VER_1;\n\t\tnp->txrxctl_bits = NVREG_TXRXCTL_DESC_1;\n\t}\n\n\tnp->pkt_limit = NV_PKTLIMIT_1;\n\tif (id->driver_data & DEV_HAS_LARGEDESC)\n\t\tnp->pkt_limit = NV_PKTLIMIT_2;\n\n\tif (id->driver_data & DEV_HAS_CHECKSUM) {\n\t\tnp->txrxctl_bits |= NVREG_TXRXCTL_RXCHECK;\n\t\tdev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_SG |\n\t\t\tNETIF_F_TSO | NETIF_F_RXCSUM;\n\t}\n\n\tnp->vlanctl_bits = 0;\n\tif (id->driver_data & DEV_HAS_VLAN) {\n\t\tnp->vlanctl_bits = NVREG_VLANCONTROL_ENABLE;\n\t\tdev->hw_features |= NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t\t    NETIF_F_HW_VLAN_CTAG_TX;\n\t}\n\n\tdev->features |= dev->hw_features;\n\n\t \n\tdev->hw_features |= NETIF_F_LOOPBACK;\n\n\t \n\tdev->min_mtu = ETH_ZLEN + ETH_FCS_LEN;\n\tdev->max_mtu = np->pkt_limit;\n\n\tnp->pause_flags = NV_PAUSEFRAME_RX_CAPABLE | NV_PAUSEFRAME_RX_REQ | NV_PAUSEFRAME_AUTONEG;\n\tif ((id->driver_data & DEV_HAS_PAUSEFRAME_TX_V1) ||\n\t    (id->driver_data & DEV_HAS_PAUSEFRAME_TX_V2) ||\n\t    (id->driver_data & DEV_HAS_PAUSEFRAME_TX_V3)) {\n\t\tnp->pause_flags |= NV_PAUSEFRAME_TX_CAPABLE | NV_PAUSEFRAME_TX_REQ;\n\t}\n\n\terr = -ENOMEM;\n\tnp->base = ioremap(addr, np->register_size);\n\tif (!np->base)\n\t\tgoto out_relreg;\n\n\tnp->rx_ring_size = RX_RING_DEFAULT;\n\tnp->tx_ring_size = TX_RING_DEFAULT;\n\n\tif (!nv_optimized(np)) {\n\t\tnp->rx_ring.orig = dma_alloc_coherent(&pci_dev->dev,\n\t\t\t\t\t\t      sizeof(struct ring_desc) *\n\t\t\t\t\t\t      (np->rx_ring_size +\n\t\t\t\t\t\t      np->tx_ring_size),\n\t\t\t\t\t\t      &np->ring_addr,\n\t\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!np->rx_ring.orig)\n\t\t\tgoto out_unmap;\n\t\tnp->tx_ring.orig = &np->rx_ring.orig[np->rx_ring_size];\n\t} else {\n\t\tnp->rx_ring.ex = dma_alloc_coherent(&pci_dev->dev,\n\t\t\t\t\t\t    sizeof(struct ring_desc_ex) *\n\t\t\t\t\t\t    (np->rx_ring_size +\n\t\t\t\t\t\t    np->tx_ring_size),\n\t\t\t\t\t\t    &np->ring_addr, GFP_KERNEL);\n\t\tif (!np->rx_ring.ex)\n\t\t\tgoto out_unmap;\n\t\tnp->tx_ring.ex = &np->rx_ring.ex[np->rx_ring_size];\n\t}\n\tnp->rx_skb = kcalloc(np->rx_ring_size, sizeof(struct nv_skb_map), GFP_KERNEL);\n\tnp->tx_skb = kcalloc(np->tx_ring_size, sizeof(struct nv_skb_map), GFP_KERNEL);\n\tif (!np->rx_skb || !np->tx_skb)\n\t\tgoto out_freering;\n\n\tif (!nv_optimized(np))\n\t\tdev->netdev_ops = &nv_netdev_ops;\n\telse\n\t\tdev->netdev_ops = &nv_netdev_ops_optimized;\n\n\tnetif_napi_add(dev, &np->napi, nv_napi_poll);\n\tdev->ethtool_ops = &ops;\n\tdev->watchdog_timeo = NV_WATCHDOG_TIMEO;\n\n\tpci_set_drvdata(pci_dev, dev);\n\n\t \n\tbase = get_hwbase(dev);\n\tnp->orig_mac[0] = readl(base + NvRegMacAddrA);\n\tnp->orig_mac[1] = readl(base + NvRegMacAddrB);\n\n\t \n\ttxreg = readl(base + NvRegTransmitPoll);\n\tif (id->driver_data & DEV_HAS_CORRECT_MACADDR) {\n\t\t \n\t\tmac[0] = (np->orig_mac[0] >>  0) & 0xff;\n\t\tmac[1] = (np->orig_mac[0] >>  8) & 0xff;\n\t\tmac[2] = (np->orig_mac[0] >> 16) & 0xff;\n\t\tmac[3] = (np->orig_mac[0] >> 24) & 0xff;\n\t\tmac[4] = (np->orig_mac[1] >>  0) & 0xff;\n\t\tmac[5] = (np->orig_mac[1] >>  8) & 0xff;\n\t} else if (txreg & NVREG_TRANSMITPOLL_MAC_ADDR_REV) {\n\t\t \n\t\tmac[0] = (np->orig_mac[0] >>  0) & 0xff;\n\t\tmac[1] = (np->orig_mac[0] >>  8) & 0xff;\n\t\tmac[2] = (np->orig_mac[0] >> 16) & 0xff;\n\t\tmac[3] = (np->orig_mac[0] >> 24) & 0xff;\n\t\tmac[4] = (np->orig_mac[1] >>  0) & 0xff;\n\t\tmac[5] = (np->orig_mac[1] >>  8) & 0xff;\n\t\t \n\t\tnp->orig_mac[0] = (mac[5] << 0) + (mac[4] << 8) +\n\t\t\t(mac[3] << 16) + (mac[2] << 24);\n\t\tnp->orig_mac[1] = (mac[1] << 0) + (mac[0] << 8);\n\t} else {\n\t\t \n\t\tmac[0] = (np->orig_mac[1] >>  8) & 0xff;\n\t\tmac[1] = (np->orig_mac[1] >>  0) & 0xff;\n\t\tmac[2] = (np->orig_mac[0] >> 24) & 0xff;\n\t\tmac[3] = (np->orig_mac[0] >> 16) & 0xff;\n\t\tmac[4] = (np->orig_mac[0] >>  8) & 0xff;\n\t\tmac[5] = (np->orig_mac[0] >>  0) & 0xff;\n\t\twritel(txreg|NVREG_TRANSMITPOLL_MAC_ADDR_REV, base + NvRegTransmitPoll);\n\t\tdev_dbg(&pci_dev->dev,\n\t\t\t\"%s: set workaround bit for reversed mac addr\\n\",\n\t\t\t__func__);\n\t}\n\n\tif (is_valid_ether_addr(mac)) {\n\t\teth_hw_addr_set(dev, mac);\n\t} else {\n\t\t \n\t\tdev_err(&pci_dev->dev,\n\t\t\t\"Invalid MAC address detected: %pM - Please complain to your hardware vendor.\\n\",\n\t\t\tmac);\n\t\teth_hw_addr_random(dev);\n\t\tdev_err(&pci_dev->dev,\n\t\t\t\"Using random MAC address: %pM\\n\", dev->dev_addr);\n\t}\n\n\t \n\tnv_copy_mac_to_hw(dev);\n\n\t \n\twritel(0, base + NvRegWakeUpFlags);\n\tnp->wolenabled = 0;\n\tdevice_set_wakeup_enable(&pci_dev->dev, false);\n\n\tif (id->driver_data & DEV_HAS_POWER_CNTRL) {\n\n\t\t \n\t\tpowerstate = readl(base + NvRegPowerState2);\n\t\tpowerstate &= ~NVREG_POWERSTATE2_POWERUP_MASK;\n\t\tif ((id->driver_data & DEV_NEED_LOW_POWER_FIX) &&\n\t\t    pci_dev->revision >= 0xA3)\n\t\t\tpowerstate |= NVREG_POWERSTATE2_POWERUP_REV_A3;\n\t\twritel(powerstate, base + NvRegPowerState2);\n\t}\n\n\tif (np->desc_ver == DESC_VER_1)\n\t\tnp->tx_flags = NV_TX_VALID;\n\telse\n\t\tnp->tx_flags = NV_TX2_VALID;\n\n\tnp->msi_flags = 0;\n\tif ((id->driver_data & DEV_HAS_MSI) && msi)\n\t\tnp->msi_flags |= NV_MSI_CAPABLE;\n\n\tif ((id->driver_data & DEV_HAS_MSI_X) && msix) {\n\t\t \n#if 0\n\t\tnp->msi_flags |= NV_MSI_X_CAPABLE;\n#endif\n\t}\n\n\tif (optimization_mode == NV_OPTIMIZATION_MODE_CPU) {\n\t\tnp->irqmask = NVREG_IRQMASK_CPU;\n\t\tif (np->msi_flags & NV_MSI_X_CAPABLE)  \n\t\t\tnp->msi_flags |= 0x0001;\n\t} else if (optimization_mode == NV_OPTIMIZATION_MODE_DYNAMIC &&\n\t\t   !(id->driver_data & DEV_NEED_TIMERIRQ)) {\n\t\t \n\t\tnp->irqmask = NVREG_IRQMASK_THROUGHPUT;\n\t\t \n\t\tnp->msi_flags &= ~NV_MSI_X_CAPABLE;\n\t} else {\n\t\toptimization_mode = NV_OPTIMIZATION_MODE_THROUGHPUT;\n\t\tnp->irqmask = NVREG_IRQMASK_THROUGHPUT;\n\t\tif (np->msi_flags & NV_MSI_X_CAPABLE)  \n\t\t\tnp->msi_flags |= 0x0003;\n\t}\n\n\tif (id->driver_data & DEV_NEED_TIMERIRQ)\n\t\tnp->irqmask |= NVREG_IRQ_TIMER;\n\tif (id->driver_data & DEV_NEED_LINKTIMER) {\n\t\tnp->need_linktimer = 1;\n\t\tnp->link_timeout = jiffies + LINK_TIMEOUT;\n\t} else {\n\t\tnp->need_linktimer = 0;\n\t}\n\n\t \n\tif (id->driver_data & DEV_NEED_TX_LIMIT) {\n\t\tnp->tx_limit = 1;\n\t\tif (((id->driver_data & DEV_NEED_TX_LIMIT2) == DEV_NEED_TX_LIMIT2) &&\n\t\t    pci_dev->revision >= 0xA2)\n\t\t\tnp->tx_limit = 0;\n\t}\n\n\t \n\twritel(0, base + NvRegMIIMask);\n\tphystate = readl(base + NvRegAdapterControl);\n\tif (phystate & NVREG_ADAPTCTL_RUNNING) {\n\t\tphystate_orig = 1;\n\t\tphystate &= ~NVREG_ADAPTCTL_RUNNING;\n\t\twritel(phystate, base + NvRegAdapterControl);\n\t}\n\twritel(NVREG_MIISTAT_MASK_ALL, base + NvRegMIIStatus);\n\n\tif (id->driver_data & DEV_HAS_MGMT_UNIT) {\n\t\t \n\t\tif ((readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_MGMT_ST) &&\n\t\t    (readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_SYNC_PHY_INIT) &&\n\t\t    nv_mgmt_acquire_sema(dev) &&\n\t\t    nv_mgmt_get_version(dev)) {\n\t\t\tnp->mac_in_use = 1;\n\t\t\tif (np->mgmt_version > 0)\n\t\t\t\tnp->mac_in_use = readl(base + NvRegMgmtUnitControl) & NVREG_MGMTUNITCONTROL_INUSE;\n\t\t\t \n\t\t\tif (np->mac_in_use &&\n\t\t\t    ((readl(base + NvRegTransmitterControl) & NVREG_XMITCTL_SYNC_MASK) ==\n\t\t\t     NVREG_XMITCTL_SYNC_PHY_INIT)) {\n\t\t\t\t \n\t\t\t\tphyinitialized = 1;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tfor (i = 1; i <= 32; i++) {\n\t\tint id1, id2;\n\t\tint phyaddr = i & 0x1F;\n\n\t\tspin_lock_irq(&np->lock);\n\t\tid1 = mii_rw(dev, phyaddr, MII_PHYSID1, MII_READ);\n\t\tspin_unlock_irq(&np->lock);\n\t\tif (id1 < 0 || id1 == 0xffff)\n\t\t\tcontinue;\n\t\tspin_lock_irq(&np->lock);\n\t\tid2 = mii_rw(dev, phyaddr, MII_PHYSID2, MII_READ);\n\t\tspin_unlock_irq(&np->lock);\n\t\tif (id2 < 0 || id2 == 0xffff)\n\t\t\tcontinue;\n\n\t\tnp->phy_model = id2 & PHYID2_MODEL_MASK;\n\t\tid1 = (id1 & PHYID1_OUI_MASK) << PHYID1_OUI_SHFT;\n\t\tid2 = (id2 & PHYID2_OUI_MASK) >> PHYID2_OUI_SHFT;\n\t\tnp->phyaddr = phyaddr;\n\t\tnp->phy_oui = id1 | id2;\n\n\t\t \n\t\tif (np->phy_oui == PHY_OUI_REALTEK2)\n\t\t\tnp->phy_oui = PHY_OUI_REALTEK;\n\t\t \n\t\tif (np->phy_oui == PHY_OUI_REALTEK && np->phy_model == PHY_MODEL_REALTEK_8211)\n\t\t\tnp->phy_rev = mii_rw(dev, phyaddr, MII_RESV1, MII_READ) & PHY_REV_MASK;\n\n\t\tbreak;\n\t}\n\tif (i == 33) {\n\t\tdev_info(&pci_dev->dev, \"open: Could not find a valid PHY\\n\");\n\t\tgoto out_error;\n\t}\n\n\tif (!phyinitialized) {\n\t\t \n\t\tphy_init(dev);\n\t} else {\n\t\t \n\t\tu32 mii_status = mii_rw(dev, np->phyaddr, MII_BMSR, MII_READ);\n\t\tif (mii_status & PHY_GIGABIT)\n\t\t\tnp->gigabit = PHY_GIGABIT;\n\t}\n\n\t \n\tnp->linkspeed = NVREG_LINKSPEED_FORCE|NVREG_LINKSPEED_10;\n\tnp->duplex = 0;\n\tnp->autoneg = 1;\n\n\terr = register_netdev(dev);\n\tif (err) {\n\t\tdev_info(&pci_dev->dev, \"unable to register netdev: %d\\n\", err);\n\t\tgoto out_error;\n\t}\n\n\tnetif_carrier_off(dev);\n\n\t \n\tnv_update_pause(dev, 0);\n\tnv_start_tx(dev);\n\tnv_stop_tx(dev);\n\n\tif (id->driver_data & DEV_HAS_VLAN)\n\t\tnv_vlan_mode(dev, dev->features);\n\n\tdev_info(&pci_dev->dev, \"ifname %s, PHY OUI 0x%x @ %d, addr %pM\\n\",\n\t\t dev->name, np->phy_oui, np->phyaddr, dev->dev_addr);\n\n\tdev_info(&pci_dev->dev, \"%s%s%s%s%s%s%s%s%s%s%sdesc-v%u\\n\",\n\t\t dev->features & NETIF_F_HIGHDMA ? \"highdma \" : \"\",\n\t\t dev->features & (NETIF_F_IP_CSUM | NETIF_F_SG) ?\n\t\t\t\"csum \" : \"\",\n\t\t dev->features & (NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t\t  NETIF_F_HW_VLAN_CTAG_TX) ?\n\t\t\t\"vlan \" : \"\",\n\t\t dev->features & (NETIF_F_LOOPBACK) ?\n\t\t\t\"loopback \" : \"\",\n\t\t id->driver_data & DEV_HAS_POWER_CNTRL ? \"pwrctl \" : \"\",\n\t\t id->driver_data & DEV_HAS_MGMT_UNIT ? \"mgmt \" : \"\",\n\t\t id->driver_data & DEV_NEED_TIMERIRQ ? \"timirq \" : \"\",\n\t\t np->gigabit == PHY_GIGABIT ? \"gbit \" : \"\",\n\t\t np->need_linktimer ? \"lnktim \" : \"\",\n\t\t np->msi_flags & NV_MSI_CAPABLE ? \"msi \" : \"\",\n\t\t np->msi_flags & NV_MSI_X_CAPABLE ? \"msi-x \" : \"\",\n\t\t np->desc_ver);\n\n\treturn 0;\n\nout_error:\n\tnv_mgmt_release_sema(dev);\n\tif (phystate_orig)\n\t\twritel(phystate|NVREG_ADAPTCTL_RUNNING, base + NvRegAdapterControl);\nout_freering:\n\tfree_rings(dev);\nout_unmap:\n\tiounmap(get_hwbase(dev));\nout_relreg:\n\tpci_release_regions(pci_dev);\nout_disable:\n\tpci_disable_device(pci_dev);\nout_free:\n\tfree_percpu(np->txrx_stats);\nout_alloc_percpu:\n\tfree_netdev(dev);\nout:\n\treturn err;\n}\n\nstatic void nv_restore_phy(struct net_device *dev)\n{\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu16 phy_reserved, mii_control;\n\n\tif (np->phy_oui == PHY_OUI_REALTEK &&\n\t    np->phy_model == PHY_MODEL_REALTEK_8201 &&\n\t    phy_cross == NV_CROSSOVER_DETECTION_DISABLED) {\n\t\tmii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT3);\n\t\tphy_reserved = mii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG2, MII_READ);\n\t\tphy_reserved &= ~PHY_REALTEK_INIT_MSK1;\n\t\tphy_reserved |= PHY_REALTEK_INIT8;\n\t\tmii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG2, phy_reserved);\n\t\tmii_rw(dev, np->phyaddr, PHY_REALTEK_INIT_REG1, PHY_REALTEK_INIT1);\n\n\t\t \n\t\tmii_control = mii_rw(dev, np->phyaddr, MII_BMCR, MII_READ);\n\t\tmii_control |= (BMCR_ANRESTART | BMCR_ANENABLE);\n\t\tmii_rw(dev, np->phyaddr, MII_BMCR, mii_control);\n\t}\n}\n\nstatic void nv_restore_mac_addr(struct pci_dev *pci_dev)\n{\n\tstruct net_device *dev = pci_get_drvdata(pci_dev);\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\n\t \n\twritel(np->orig_mac[0], base + NvRegMacAddrA);\n\twritel(np->orig_mac[1], base + NvRegMacAddrB);\n\twritel(readl(base + NvRegTransmitPoll) & ~NVREG_TRANSMITPOLL_MAC_ADDR_REV,\n\t       base + NvRegTransmitPoll);\n}\n\nstatic void nv_remove(struct pci_dev *pci_dev)\n{\n\tstruct net_device *dev = pci_get_drvdata(pci_dev);\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tfree_percpu(np->txrx_stats);\n\n\tunregister_netdev(dev);\n\n\tnv_restore_mac_addr(pci_dev);\n\n\t \n\tnv_restore_phy(dev);\n\n\tnv_mgmt_release_sema(dev);\n\n\t \n\tfree_rings(dev);\n\tiounmap(get_hwbase(dev));\n\tpci_release_regions(pci_dev);\n\tpci_disable_device(pci_dev);\n\tfree_netdev(dev);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int nv_suspend(struct device *device)\n{\n\tstruct net_device *dev = dev_get_drvdata(device);\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tint i;\n\n\tif (netif_running(dev)) {\n\t\t \n\t\tnv_close(dev);\n\t}\n\tnetif_device_detach(dev);\n\n\t \n\tfor (i = 0; i <= np->register_size/sizeof(u32); i++)\n\t\tnp->saved_config_space[i] = readl(base + i*sizeof(u32));\n\n\treturn 0;\n}\n\nstatic int nv_resume(struct device *device)\n{\n\tstruct pci_dev *pdev = to_pci_dev(device);\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct fe_priv *np = netdev_priv(dev);\n\tu8 __iomem *base = get_hwbase(dev);\n\tint i, rc = 0;\n\n\t \n\tfor (i = 0; i <= np->register_size/sizeof(u32); i++)\n\t\twritel(np->saved_config_space[i], base+i*sizeof(u32));\n\n\tif (np->driver_data & DEV_NEED_MSI_FIX)\n\t\tpci_write_config_dword(pdev, NV_MSI_PRIV_OFFSET, NV_MSI_PRIV_VALUE);\n\n\t \n\tphy_init(dev);\n\n\tnetif_device_attach(dev);\n\tif (netif_running(dev)) {\n\t\trc = nv_open(dev);\n\t\tnv_set_multicast(dev);\n\t}\n\treturn rc;\n}\n\nstatic SIMPLE_DEV_PM_OPS(nv_pm_ops, nv_suspend, nv_resume);\n#define NV_PM_OPS (&nv_pm_ops)\n\n#else\n#define NV_PM_OPS NULL\n#endif  \n\n#ifdef CONFIG_PM\nstatic void nv_shutdown(struct pci_dev *pdev)\n{\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct fe_priv *np = netdev_priv(dev);\n\n\tif (netif_running(dev))\n\t\tnv_close(dev);\n\n\t \n\tif (system_state != SYSTEM_POWER_OFF)\n\t\tnv_restore_mac_addr(pdev);\n\n\tpci_disable_device(pdev);\n\t \n\tif (system_state == SYSTEM_POWER_OFF) {\n\t\tpci_wake_from_d3(pdev, np->wolenabled);\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n\t}\n}\n#else\n#define nv_shutdown NULL\n#endif  \n\nstatic const struct pci_device_id pci_tbl[] = {\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x01C3),\n\t\t.driver_data = DEV_NEED_TIMERIRQ|DEV_NEED_LINKTIMER,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0066),\n\t\t.driver_data = DEV_NEED_TIMERIRQ|DEV_NEED_LINKTIMER,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x00D6),\n\t\t.driver_data = DEV_NEED_TIMERIRQ|DEV_NEED_LINKTIMER,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0086),\n\t\t.driver_data = DEV_NEED_TIMERIRQ|DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x008C),\n\t\t.driver_data = DEV_NEED_TIMERIRQ|DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x00E6),\n\t\t.driver_data = DEV_NEED_TIMERIRQ|DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x00DF),\n\t\t.driver_data = DEV_NEED_TIMERIRQ|DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0056),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_STATISTICS_V1|DEV_NEED_TX_LIMIT,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0057),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_STATISTICS_V1|DEV_NEED_TX_LIMIT,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0037),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_STATISTICS_V1|DEV_NEED_TX_LIMIT,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0038),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_STATISTICS_V1|DEV_NEED_TX_LIMIT,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0268),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_STATISTICS_V1|DEV_NEED_LOW_POWER_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0269),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_STATISTICS_V1|DEV_NEED_LOW_POWER_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0372),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_VLAN|DEV_HAS_MSI|DEV_HAS_MSI_X|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_NEED_TX_LIMIT|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0373),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_VLAN|DEV_HAS_MSI|DEV_HAS_MSI_X|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_NEED_TX_LIMIT|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x03E5),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x03E6),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x03EE),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x03EF),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0450),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_NEED_TX_LIMIT|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0451),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_NEED_TX_LIMIT|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0452),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_NEED_TX_LIMIT|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0453),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_NEED_TX_LIMIT|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x054C),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x054D),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x054E),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x054F),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x07DC),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x07DD),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x07DE),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x07DF),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_HIGH_DMA|DEV_HAS_POWER_CNTRL|DEV_HAS_MSI|DEV_HAS_PAUSEFRAME_TX_V1|DEV_HAS_STATISTICS_V12|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_HAS_GEAR_MODE|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0760),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_MSI|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V2|DEV_HAS_STATISTICS_V123|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_NEED_TX_LIMIT2|DEV_HAS_GEAR_MODE|DEV_NEED_PHY_INIT_FIX|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0761),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_MSI|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V2|DEV_HAS_STATISTICS_V123|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_NEED_TX_LIMIT2|DEV_HAS_GEAR_MODE|DEV_NEED_PHY_INIT_FIX|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0762),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_MSI|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V2|DEV_HAS_STATISTICS_V123|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_NEED_TX_LIMIT2|DEV_HAS_GEAR_MODE|DEV_NEED_PHY_INIT_FIX|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0763),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_MSI|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V2|DEV_HAS_STATISTICS_V123|DEV_HAS_TEST_EXTENDED|DEV_HAS_MGMT_UNIT|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_NEED_TX_LIMIT2|DEV_HAS_GEAR_MODE|DEV_NEED_PHY_INIT_FIX|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0AB0),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_MSI|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V3|DEV_HAS_STATISTICS_V123|DEV_HAS_TEST_EXTENDED|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_NEED_TX_LIMIT2|DEV_HAS_GEAR_MODE|DEV_NEED_PHY_INIT_FIX|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0AB1),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_MSI|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V3|DEV_HAS_STATISTICS_V123|DEV_HAS_TEST_EXTENDED|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_NEED_TX_LIMIT2|DEV_HAS_GEAR_MODE|DEV_NEED_PHY_INIT_FIX|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0AB2),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_MSI|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V3|DEV_HAS_STATISTICS_V123|DEV_HAS_TEST_EXTENDED|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_NEED_TX_LIMIT2|DEV_HAS_GEAR_MODE|DEV_NEED_PHY_INIT_FIX|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0AB3),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_MSI|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V3|DEV_HAS_STATISTICS_V123|DEV_HAS_TEST_EXTENDED|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_NEED_TX_LIMIT2|DEV_HAS_GEAR_MODE|DEV_NEED_PHY_INIT_FIX|DEV_NEED_MSI_FIX,\n\t},\n\t{\t \n\t\tPCI_DEVICE(0x10DE, 0x0D7D),\n\t\t.driver_data = DEV_NEED_LINKTIMER|DEV_HAS_LARGEDESC|DEV_HAS_CHECKSUM|DEV_HAS_HIGH_DMA|DEV_HAS_MSI|DEV_HAS_POWER_CNTRL|DEV_HAS_PAUSEFRAME_TX_V3|DEV_HAS_STATISTICS_V123|DEV_HAS_TEST_EXTENDED|DEV_HAS_CORRECT_MACADDR|DEV_HAS_COLLISION_FIX|DEV_HAS_GEAR_MODE|DEV_NEED_PHY_INIT_FIX,\n\t},\n\t{0,},\n};\n\nstatic struct pci_driver forcedeth_pci_driver = {\n\t.name\t\t= DRV_NAME,\n\t.id_table\t= pci_tbl,\n\t.probe\t\t= nv_probe,\n\t.remove\t\t= nv_remove,\n\t.shutdown\t= nv_shutdown,\n\t.driver.pm\t= NV_PM_OPS,\n};\n\nmodule_param(max_interrupt_work, int, 0);\nMODULE_PARM_DESC(max_interrupt_work, \"forcedeth maximum events handled per interrupt\");\nmodule_param(optimization_mode, int, 0);\nMODULE_PARM_DESC(optimization_mode, \"In throughput mode (0), every tx & rx packet will generate an interrupt. In CPU mode (1), interrupts are controlled by a timer. In dynamic mode (2), the mode toggles between throughput and CPU mode based on network load.\");\nmodule_param(poll_interval, int, 0);\nMODULE_PARM_DESC(poll_interval, \"Interval determines how frequent timer interrupt is generated by [(time_in_micro_secs * 100) / (2^10)]. Min is 0 and Max is 65535.\");\nmodule_param(msi, int, 0);\nMODULE_PARM_DESC(msi, \"MSI interrupts are enabled by setting to 1 and disabled by setting to 0.\");\nmodule_param(msix, int, 0);\nMODULE_PARM_DESC(msix, \"MSIX interrupts are enabled by setting to 1 and disabled by setting to 0.\");\nmodule_param(dma_64bit, int, 0);\nMODULE_PARM_DESC(dma_64bit, \"High DMA is enabled by setting to 1 and disabled by setting to 0.\");\nmodule_param(phy_cross, int, 0);\nMODULE_PARM_DESC(phy_cross, \"Phy crossover detection for Realtek 8201 phy is enabled by setting to 1 and disabled by setting to 0.\");\nmodule_param(phy_power_down, int, 0);\nMODULE_PARM_DESC(phy_power_down, \"Power down phy and disable link when interface is down (1), or leave phy powered up (0).\");\nmodule_param(debug_tx_timeout, bool, 0);\nMODULE_PARM_DESC(debug_tx_timeout,\n\t\t \"Dump tx related registers and ring when tx_timeout happens\");\n\nmodule_pci_driver(forcedeth_pci_driver);\nMODULE_AUTHOR(\"Manfred Spraul <manfred@colorfullife.com>\");\nMODULE_DESCRIPTION(\"Reverse Engineered nForce ethernet driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DEVICE_TABLE(pci, pci_tbl);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}