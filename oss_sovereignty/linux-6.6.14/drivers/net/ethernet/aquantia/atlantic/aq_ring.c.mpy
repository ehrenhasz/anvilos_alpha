{
  "module_name": "aq_ring.c",
  "hash_id": "268c919ba41c6f6224bc7215baf1ecc657d98131d1809af6e8c1b5008dcf1817",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/aquantia/atlantic/aq_ring.c",
  "human_readable_source": "\n \n\n \n\n#include \"aq_nic.h\"\n#include \"aq_hw.h\"\n#include \"aq_hw_utils.h\"\n#include \"aq_ptp.h\"\n#include \"aq_vec.h\"\n#include \"aq_main.h\"\n\n#include <net/xdp.h>\n#include <linux/filter.h>\n#include <linux/bpf_trace.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n\nstatic void aq_get_rxpages_xdp(struct aq_ring_buff_s *buff,\n\t\t\t       struct xdp_buff *xdp)\n{\n\tstruct skb_shared_info *sinfo;\n\tint i;\n\n\tif (xdp_buff_has_frags(xdp)) {\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\n\t\tfor (i = 0; i < sinfo->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &sinfo->frags[i];\n\n\t\t\tpage_ref_inc(skb_frag_page(frag));\n\t\t}\n\t}\n\tpage_ref_inc(buff->rxdata.page);\n}\n\nstatic inline void aq_free_rxpage(struct aq_rxpage *rxpage, struct device *dev)\n{\n\tunsigned int len = PAGE_SIZE << rxpage->order;\n\n\tdma_unmap_page(dev, rxpage->daddr, len, DMA_FROM_DEVICE);\n\n\t \n\t__free_pages(rxpage->page, rxpage->order);\n\trxpage->page = NULL;\n}\n\nstatic int aq_alloc_rxpages(struct aq_rxpage *rxpage, struct aq_ring_s *rx_ring)\n{\n\tstruct device *dev = aq_nic_get_dev(rx_ring->aq_nic);\n\tunsigned int order = rx_ring->page_order;\n\tstruct page *page;\n\tint ret = -ENOMEM;\n\tdma_addr_t daddr;\n\n\tpage = dev_alloc_pages(order);\n\tif (unlikely(!page))\n\t\tgoto err_exit;\n\n\tdaddr = dma_map_page(dev, page, 0, PAGE_SIZE << order,\n\t\t\t     DMA_FROM_DEVICE);\n\n\tif (unlikely(dma_mapping_error(dev, daddr)))\n\t\tgoto free_page;\n\n\trxpage->page = page;\n\trxpage->daddr = daddr;\n\trxpage->order = order;\n\trxpage->pg_off = rx_ring->page_offset;\n\n\treturn 0;\n\nfree_page:\n\t__free_pages(page, order);\n\nerr_exit:\n\treturn ret;\n}\n\nstatic int aq_get_rxpages(struct aq_ring_s *self, struct aq_ring_buff_s *rxbuf)\n{\n\tunsigned int order = self->page_order;\n\tu16 page_offset = self->page_offset;\n\tu16 frame_max = self->frame_max;\n\tu16 tail_size = self->tail_size;\n\tint ret;\n\n\tif (rxbuf->rxdata.page) {\n\t\t \n\t\tif (page_ref_count(rxbuf->rxdata.page) > 1) {\n\t\t\t \n\t\t\trxbuf->rxdata.pg_off += frame_max + page_offset +\n\t\t\t\t\t\ttail_size;\n\t\t\tif (rxbuf->rxdata.pg_off + frame_max + tail_size <=\n\t\t\t    (PAGE_SIZE << order)) {\n\t\t\t\tu64_stats_update_begin(&self->stats.rx.syncp);\n\t\t\t\tself->stats.rx.pg_flips++;\n\t\t\t\tu64_stats_update_end(&self->stats.rx.syncp);\n\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\taq_free_rxpage(&rxbuf->rxdata,\n\t\t\t\t\t       aq_nic_get_dev(self->aq_nic));\n\t\t\t\tu64_stats_update_begin(&self->stats.rx.syncp);\n\t\t\t\tself->stats.rx.pg_losts++;\n\t\t\t\tu64_stats_update_end(&self->stats.rx.syncp);\n\t\t\t}\n\t\t} else {\n\t\t\trxbuf->rxdata.pg_off = page_offset;\n\t\t\tu64_stats_update_begin(&self->stats.rx.syncp);\n\t\t\tself->stats.rx.pg_reuses++;\n\t\t\tu64_stats_update_end(&self->stats.rx.syncp);\n\t\t}\n\t}\n\n\tif (!rxbuf->rxdata.page) {\n\t\tret = aq_alloc_rxpages(&rxbuf->rxdata, self);\n\t\tif (ret) {\n\t\t\tu64_stats_update_begin(&self->stats.rx.syncp);\n\t\t\tself->stats.rx.alloc_fails++;\n\t\t\tu64_stats_update_end(&self->stats.rx.syncp);\n\t\t}\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic struct aq_ring_s *aq_ring_alloc(struct aq_ring_s *self,\n\t\t\t\t       struct aq_nic_s *aq_nic)\n{\n\tint err = 0;\n\n\tself->buff_ring =\n\t\tkcalloc(self->size, sizeof(struct aq_ring_buff_s), GFP_KERNEL);\n\n\tif (!self->buff_ring) {\n\t\terr = -ENOMEM;\n\t\tgoto err_exit;\n\t}\n\n\tself->dx_ring = dma_alloc_coherent(aq_nic_get_dev(aq_nic),\n\t\t\t\t\t   self->size * self->dx_size,\n\t\t\t\t\t   &self->dx_ring_pa, GFP_KERNEL);\n\tif (!self->dx_ring) {\n\t\terr = -ENOMEM;\n\t\tgoto err_exit;\n\t}\n\nerr_exit:\n\tif (err < 0) {\n\t\taq_ring_free(self);\n\t\tself = NULL;\n\t}\n\n\treturn self;\n}\n\nstruct aq_ring_s *aq_ring_tx_alloc(struct aq_ring_s *self,\n\t\t\t\t   struct aq_nic_s *aq_nic,\n\t\t\t\t   unsigned int idx,\n\t\t\t\t   struct aq_nic_cfg_s *aq_nic_cfg)\n{\n\tint err = 0;\n\n\tself->aq_nic = aq_nic;\n\tself->idx = idx;\n\tself->size = aq_nic_cfg->txds;\n\tself->dx_size = aq_nic_cfg->aq_hw_caps->txd_size;\n\n\tself = aq_ring_alloc(self, aq_nic);\n\tif (!self) {\n\t\terr = -ENOMEM;\n\t\tgoto err_exit;\n\t}\n\nerr_exit:\n\tif (err < 0) {\n\t\taq_ring_free(self);\n\t\tself = NULL;\n\t}\n\n\treturn self;\n}\n\nstruct aq_ring_s *aq_ring_rx_alloc(struct aq_ring_s *self,\n\t\t\t\t   struct aq_nic_s *aq_nic,\n\t\t\t\t   unsigned int idx,\n\t\t\t\t   struct aq_nic_cfg_s *aq_nic_cfg)\n{\n\tint err = 0;\n\n\tself->aq_nic = aq_nic;\n\tself->idx = idx;\n\tself->size = aq_nic_cfg->rxds;\n\tself->dx_size = aq_nic_cfg->aq_hw_caps->rxd_size;\n\tself->xdp_prog = aq_nic->xdp_prog;\n\tself->frame_max = AQ_CFG_RX_FRAME_MAX;\n\n\t \n\tif (READ_ONCE(self->xdp_prog)) {\n\t\tself->page_offset = AQ_XDP_HEADROOM;\n\t\tself->page_order = AQ_CFG_XDP_PAGEORDER;\n\t\tself->tail_size = AQ_XDP_TAILROOM;\n\t} else {\n\t\tself->page_offset = 0;\n\t\tself->page_order = fls(self->frame_max / PAGE_SIZE +\n\t\t\t\t       (self->frame_max % PAGE_SIZE ? 1 : 0)) - 1;\n\t\tif (aq_nic_cfg->rxpageorder > self->page_order)\n\t\t\tself->page_order = aq_nic_cfg->rxpageorder;\n\t\tself->tail_size = 0;\n\t}\n\n\tself = aq_ring_alloc(self, aq_nic);\n\tif (!self) {\n\t\terr = -ENOMEM;\n\t\tgoto err_exit;\n\t}\n\nerr_exit:\n\tif (err < 0) {\n\t\taq_ring_free(self);\n\t\tself = NULL;\n\t}\n\n\treturn self;\n}\n\nstruct aq_ring_s *\naq_ring_hwts_rx_alloc(struct aq_ring_s *self, struct aq_nic_s *aq_nic,\n\t\t      unsigned int idx, unsigned int size, unsigned int dx_size)\n{\n\tstruct device *dev = aq_nic_get_dev(aq_nic);\n\tsize_t sz = size * dx_size + AQ_CFG_RXDS_DEF;\n\n\tmemset(self, 0, sizeof(*self));\n\n\tself->aq_nic = aq_nic;\n\tself->idx = idx;\n\tself->size = size;\n\tself->dx_size = dx_size;\n\n\tself->dx_ring = dma_alloc_coherent(dev, sz, &self->dx_ring_pa,\n\t\t\t\t\t   GFP_KERNEL);\n\tif (!self->dx_ring) {\n\t\taq_ring_free(self);\n\t\treturn NULL;\n\t}\n\n\treturn self;\n}\n\nint aq_ring_init(struct aq_ring_s *self, const enum atl_ring_type ring_type)\n{\n\tself->hw_head = 0;\n\tself->sw_head = 0;\n\tself->sw_tail = 0;\n\tself->ring_type = ring_type;\n\n\tif (self->ring_type == ATL_RING_RX)\n\t\tu64_stats_init(&self->stats.rx.syncp);\n\telse\n\t\tu64_stats_init(&self->stats.tx.syncp);\n\n\treturn 0;\n}\n\nstatic inline bool aq_ring_dx_in_range(unsigned int h, unsigned int i,\n\t\t\t\t       unsigned int t)\n{\n\treturn (h < t) ? ((h < i) && (i < t)) : ((h < i) || (i < t));\n}\n\nvoid aq_ring_update_queue_state(struct aq_ring_s *ring)\n{\n\tif (aq_ring_avail_dx(ring) <= AQ_CFG_SKB_FRAGS_MAX)\n\t\taq_ring_queue_stop(ring);\n\telse if (aq_ring_avail_dx(ring) > AQ_CFG_RESTART_DESC_THRES)\n\t\taq_ring_queue_wake(ring);\n}\n\nvoid aq_ring_queue_wake(struct aq_ring_s *ring)\n{\n\tstruct net_device *ndev = aq_nic_get_ndev(ring->aq_nic);\n\n\tif (__netif_subqueue_stopped(ndev,\n\t\t\t\t     AQ_NIC_RING2QMAP(ring->aq_nic,\n\t\t\t\t\t\t      ring->idx))) {\n\t\tnetif_wake_subqueue(ndev,\n\t\t\t\t    AQ_NIC_RING2QMAP(ring->aq_nic, ring->idx));\n\t\tu64_stats_update_begin(&ring->stats.tx.syncp);\n\t\tring->stats.tx.queue_restarts++;\n\t\tu64_stats_update_end(&ring->stats.tx.syncp);\n\t}\n}\n\nvoid aq_ring_queue_stop(struct aq_ring_s *ring)\n{\n\tstruct net_device *ndev = aq_nic_get_ndev(ring->aq_nic);\n\n\tif (!__netif_subqueue_stopped(ndev,\n\t\t\t\t      AQ_NIC_RING2QMAP(ring->aq_nic,\n\t\t\t\t\t\t       ring->idx)))\n\t\tnetif_stop_subqueue(ndev,\n\t\t\t\t    AQ_NIC_RING2QMAP(ring->aq_nic, ring->idx));\n}\n\nbool aq_ring_tx_clean(struct aq_ring_s *self)\n{\n\tstruct device *dev = aq_nic_get_dev(self->aq_nic);\n\tunsigned int budget;\n\n\tfor (budget = AQ_CFG_TX_CLEAN_BUDGET;\n\t     budget && self->sw_head != self->hw_head; budget--) {\n\t\tstruct aq_ring_buff_s *buff = &self->buff_ring[self->sw_head];\n\n\t\tif (likely(buff->is_mapped)) {\n\t\t\tif (unlikely(buff->is_sop)) {\n\t\t\t\tif (!buff->is_eop &&\n\t\t\t\t    buff->eop_index != 0xffffU &&\n\t\t\t\t    (!aq_ring_dx_in_range(self->sw_head,\n\t\t\t\t\t\tbuff->eop_index,\n\t\t\t\t\t\tself->hw_head)))\n\t\t\t\t\tbreak;\n\n\t\t\t\tdma_unmap_single(dev, buff->pa, buff->len,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\t} else {\n\t\t\t\tdma_unmap_page(dev, buff->pa, buff->len,\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\t}\n\t\t}\n\n\t\tif (likely(!buff->is_eop))\n\t\t\tgoto out;\n\n\t\tif (buff->skb) {\n\t\t\tu64_stats_update_begin(&self->stats.tx.syncp);\n\t\t\t++self->stats.tx.packets;\n\t\t\tself->stats.tx.bytes += buff->skb->len;\n\t\t\tu64_stats_update_end(&self->stats.tx.syncp);\n\t\t\tdev_kfree_skb_any(buff->skb);\n\t\t} else if (buff->xdpf) {\n\t\t\tu64_stats_update_begin(&self->stats.tx.syncp);\n\t\t\t++self->stats.tx.packets;\n\t\t\tself->stats.tx.bytes += xdp_get_frame_len(buff->xdpf);\n\t\t\tu64_stats_update_end(&self->stats.tx.syncp);\n\t\t\txdp_return_frame_rx_napi(buff->xdpf);\n\t\t}\n\nout:\n\t\tbuff->skb = NULL;\n\t\tbuff->xdpf = NULL;\n\t\tbuff->pa = 0U;\n\t\tbuff->eop_index = 0xffffU;\n\t\tself->sw_head = aq_ring_next_dx(self, self->sw_head);\n\t}\n\n\treturn !!budget;\n}\n\nstatic void aq_rx_checksum(struct aq_ring_s *self,\n\t\t\t   struct aq_ring_buff_s *buff,\n\t\t\t   struct sk_buff *skb)\n{\n\tif (!(self->aq_nic->ndev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\tif (unlikely(buff->is_cso_err)) {\n\t\tu64_stats_update_begin(&self->stats.rx.syncp);\n\t\t++self->stats.rx.errors;\n\t\tu64_stats_update_end(&self->stats.rx.syncp);\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\treturn;\n\t}\n\tif (buff->is_ip_cso) {\n\t\t__skb_incr_checksum_unnecessary(skb);\n\t} else {\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t}\n\n\tif (buff->is_udp_cso || buff->is_tcp_cso)\n\t\t__skb_incr_checksum_unnecessary(skb);\n}\n\nint aq_xdp_xmit(struct net_device *dev, int num_frames,\n\t\tstruct xdp_frame **frames, u32 flags)\n{\n\tstruct aq_nic_s *aq_nic = netdev_priv(dev);\n\tunsigned int vec, i, drop = 0;\n\tint cpu = smp_processor_id();\n\tstruct aq_nic_cfg_s *aq_cfg;\n\tstruct aq_ring_s *ring;\n\n\taq_cfg = aq_nic_get_cfg(aq_nic);\n\tvec = cpu % aq_cfg->vecs;\n\tring = aq_nic->aq_ring_tx[AQ_NIC_CFG_TCVEC2RING(aq_cfg, 0, vec)];\n\n\tfor (i = 0; i < num_frames; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\n\t\tif (aq_nic_xmit_xdpf(aq_nic, ring, xdpf) == NETDEV_TX_BUSY)\n\t\t\tdrop++;\n\t}\n\n\treturn num_frames - drop;\n}\n\nstatic struct sk_buff *aq_xdp_build_skb(struct xdp_buff *xdp,\n\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\tstruct aq_ring_buff_s *buff)\n{\n\tstruct xdp_frame *xdpf;\n\tstruct sk_buff *skb;\n\n\txdpf = xdp_convert_buff_to_frame(xdp);\n\tif (unlikely(!xdpf))\n\t\treturn NULL;\n\n\tskb = xdp_build_skb_from_frame(xdpf, dev);\n\tif (!skb)\n\t\treturn NULL;\n\n\taq_get_rxpages_xdp(buff, xdp);\n\treturn skb;\n}\n\nstatic struct sk_buff *aq_xdp_run_prog(struct aq_nic_s *aq_nic,\n\t\t\t\t       struct xdp_buff *xdp,\n\t\t\t\t       struct aq_ring_s *rx_ring,\n\t\t\t\t       struct aq_ring_buff_s *buff)\n{\n\tint result = NETDEV_TX_BUSY;\n\tstruct aq_ring_s *tx_ring;\n\tstruct xdp_frame *xdpf;\n\tstruct bpf_prog *prog;\n\tu32 act = XDP_ABORTED;\n\tstruct sk_buff *skb;\n\n\tu64_stats_update_begin(&rx_ring->stats.rx.syncp);\n\t++rx_ring->stats.rx.packets;\n\trx_ring->stats.rx.bytes += xdp_get_buff_len(xdp);\n\tu64_stats_update_end(&rx_ring->stats.rx.syncp);\n\n\tprog = READ_ONCE(rx_ring->xdp_prog);\n\tif (!prog)\n\t\treturn aq_xdp_build_skb(xdp, aq_nic->ndev, buff);\n\n\tprefetchw(xdp->data_hard_start);  \n\n\t \n\tif (xdp_buff_has_frags(xdp) && !prog->aux->xdp_has_frags)\n\t\tgoto out_aborted;\n\n\tact = bpf_prog_run_xdp(prog, xdp);\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tskb = aq_xdp_build_skb(xdp, aq_nic->ndev, buff);\n\t\tif (!skb)\n\t\t\tgoto out_aborted;\n\t\tu64_stats_update_begin(&rx_ring->stats.rx.syncp);\n\t\t++rx_ring->stats.rx.xdp_pass;\n\t\tu64_stats_update_end(&rx_ring->stats.rx.syncp);\n\t\treturn skb;\n\tcase XDP_TX:\n\t\txdpf = xdp_convert_buff_to_frame(xdp);\n\t\tif (unlikely(!xdpf))\n\t\t\tgoto out_aborted;\n\t\ttx_ring = aq_nic->aq_ring_tx[rx_ring->idx];\n\t\tresult = aq_nic_xmit_xdpf(aq_nic, tx_ring, xdpf);\n\t\tif (result == NETDEV_TX_BUSY)\n\t\t\tgoto out_aborted;\n\t\tu64_stats_update_begin(&rx_ring->stats.rx.syncp);\n\t\t++rx_ring->stats.rx.xdp_tx;\n\t\tu64_stats_update_end(&rx_ring->stats.rx.syncp);\n\t\taq_get_rxpages_xdp(buff, xdp);\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\tif (xdp_do_redirect(aq_nic->ndev, xdp, prog) < 0)\n\t\t\tgoto out_aborted;\n\t\txdp_do_flush();\n\t\tu64_stats_update_begin(&rx_ring->stats.rx.syncp);\n\t\t++rx_ring->stats.rx.xdp_redirect;\n\t\tu64_stats_update_end(&rx_ring->stats.rx.syncp);\n\t\taq_get_rxpages_xdp(buff, xdp);\n\t\tbreak;\n\tdefault:\n\t\tfallthrough;\n\tcase XDP_ABORTED:\nout_aborted:\n\t\tu64_stats_update_begin(&rx_ring->stats.rx.syncp);\n\t\t++rx_ring->stats.rx.xdp_aborted;\n\t\tu64_stats_update_end(&rx_ring->stats.rx.syncp);\n\t\ttrace_xdp_exception(aq_nic->ndev, prog, act);\n\t\tbpf_warn_invalid_xdp_action(aq_nic->ndev, prog, act);\n\t\tbreak;\n\tcase XDP_DROP:\n\t\tu64_stats_update_begin(&rx_ring->stats.rx.syncp);\n\t\t++rx_ring->stats.rx.xdp_drop;\n\t\tu64_stats_update_end(&rx_ring->stats.rx.syncp);\n\t\tbreak;\n\t}\n\n\treturn ERR_PTR(-result);\n}\n\nstatic bool aq_add_rx_fragment(struct device *dev,\n\t\t\t       struct aq_ring_s *ring,\n\t\t\t       struct aq_ring_buff_s *buff,\n\t\t\t       struct xdp_buff *xdp)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\tstruct aq_ring_buff_s *buff_ = buff;\n\n\tmemset(sinfo, 0, sizeof(*sinfo));\n\tdo {\n\t\tskb_frag_t *frag;\n\n\t\tif (unlikely(sinfo->nr_frags >= MAX_SKB_FRAGS))\n\t\t\treturn true;\n\n\t\tfrag = &sinfo->frags[sinfo->nr_frags++];\n\t\tbuff_ = &ring->buff_ring[buff_->next];\n\t\tdma_sync_single_range_for_cpu(dev,\n\t\t\t\t\t      buff_->rxdata.daddr,\n\t\t\t\t\t      buff_->rxdata.pg_off,\n\t\t\t\t\t      buff_->len,\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\t\tsinfo->xdp_frags_size += buff_->len;\n\t\tskb_frag_fill_page_desc(frag, buff_->rxdata.page,\n\t\t\t\t\tbuff_->rxdata.pg_off,\n\t\t\t\t\tbuff_->len);\n\n\t\tbuff_->is_cleaned = 1;\n\n\t\tbuff->is_ip_cso &= buff_->is_ip_cso;\n\t\tbuff->is_udp_cso &= buff_->is_udp_cso;\n\t\tbuff->is_tcp_cso &= buff_->is_tcp_cso;\n\t\tbuff->is_cso_err |= buff_->is_cso_err;\n\n\t\tif (page_is_pfmemalloc(buff_->rxdata.page))\n\t\t\txdp_buff_set_frag_pfmemalloc(xdp);\n\n\t} while (!buff_->is_eop);\n\n\txdp_buff_set_frags_flag(xdp);\n\n\treturn false;\n}\n\nstatic int __aq_ring_rx_clean(struct aq_ring_s *self, struct napi_struct *napi,\n\t\t\t      int *work_done, int budget)\n{\n\tstruct net_device *ndev = aq_nic_get_ndev(self->aq_nic);\n\tint err = 0;\n\n\tfor (; (self->sw_head != self->hw_head) && budget;\n\t\tself->sw_head = aq_ring_next_dx(self, self->sw_head),\n\t\t--budget, ++(*work_done)) {\n\t\tstruct aq_ring_buff_s *buff = &self->buff_ring[self->sw_head];\n\t\tbool is_ptp_ring = aq_ptp_ring(self->aq_nic, self);\n\t\tstruct aq_ring_buff_s *buff_ = NULL;\n\t\tstruct sk_buff *skb = NULL;\n\t\tunsigned int next_ = 0U;\n\t\tunsigned int i = 0U;\n\t\tu16 hdr_len;\n\n\t\tif (buff->is_cleaned)\n\t\t\tcontinue;\n\n\t\tif (!buff->is_eop) {\n\t\t\tunsigned int frag_cnt = 0U;\n\t\t\tbuff_ = buff;\n\t\t\tdo {\n\t\t\t\tbool is_rsc_completed = true;\n\n\t\t\t\tif (buff_->next >= self->size) {\n\t\t\t\t\terr = -EIO;\n\t\t\t\t\tgoto err_exit;\n\t\t\t\t}\n\n\t\t\t\tfrag_cnt++;\n\t\t\t\tnext_ = buff_->next,\n\t\t\t\tbuff_ = &self->buff_ring[next_];\n\t\t\t\tis_rsc_completed =\n\t\t\t\t\taq_ring_dx_in_range(self->sw_head,\n\t\t\t\t\t\t\t    next_,\n\t\t\t\t\t\t\t    self->hw_head);\n\n\t\t\t\tif (unlikely(!is_rsc_completed) ||\n\t\t\t\t\t\tfrag_cnt > MAX_SKB_FRAGS) {\n\t\t\t\t\terr = 0;\n\t\t\t\t\tgoto err_exit;\n\t\t\t\t}\n\n\t\t\t\tbuff->is_error |= buff_->is_error;\n\t\t\t\tbuff->is_cso_err |= buff_->is_cso_err;\n\n\t\t\t} while (!buff_->is_eop);\n\n\t\t\tif (buff->is_error ||\n\t\t\t    (buff->is_lro && buff->is_cso_err)) {\n\t\t\t\tbuff_ = buff;\n\t\t\t\tdo {\n\t\t\t\t\tif (buff_->next >= self->size) {\n\t\t\t\t\t\terr = -EIO;\n\t\t\t\t\t\tgoto err_exit;\n\t\t\t\t\t}\n\t\t\t\t\tnext_ = buff_->next,\n\t\t\t\t\tbuff_ = &self->buff_ring[next_];\n\n\t\t\t\t\tbuff_->is_cleaned = true;\n\t\t\t\t} while (!buff_->is_eop);\n\n\t\t\t\tu64_stats_update_begin(&self->stats.rx.syncp);\n\t\t\t\t++self->stats.rx.errors;\n\t\t\t\tu64_stats_update_end(&self->stats.rx.syncp);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (buff->is_error) {\n\t\t\tu64_stats_update_begin(&self->stats.rx.syncp);\n\t\t\t++self->stats.rx.errors;\n\t\t\tu64_stats_update_end(&self->stats.rx.syncp);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdma_sync_single_range_for_cpu(aq_nic_get_dev(self->aq_nic),\n\t\t\t\t\t      buff->rxdata.daddr,\n\t\t\t\t\t      buff->rxdata.pg_off,\n\t\t\t\t\t      buff->len, DMA_FROM_DEVICE);\n\n\t\tskb = napi_alloc_skb(napi, AQ_CFG_RX_HDR_SIZE);\n\t\tif (unlikely(!skb)) {\n\t\t\tu64_stats_update_begin(&self->stats.rx.syncp);\n\t\t\tself->stats.rx.skb_alloc_fails++;\n\t\t\tu64_stats_update_end(&self->stats.rx.syncp);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\t\tif (is_ptp_ring)\n\t\t\tbuff->len -=\n\t\t\t\taq_ptp_extract_ts(self->aq_nic, skb_hwtstamps(skb),\n\t\t\t\t\t\t  aq_buf_vaddr(&buff->rxdata),\n\t\t\t\t\t\t  buff->len);\n\n\t\thdr_len = buff->len;\n\t\tif (hdr_len > AQ_CFG_RX_HDR_SIZE)\n\t\t\thdr_len = eth_get_headlen(skb->dev,\n\t\t\t\t\t\t  aq_buf_vaddr(&buff->rxdata),\n\t\t\t\t\t\t  AQ_CFG_RX_HDR_SIZE);\n\n\t\tmemcpy(__skb_put(skb, hdr_len), aq_buf_vaddr(&buff->rxdata),\n\t\t       ALIGN(hdr_len, sizeof(long)));\n\n\t\tif (buff->len - hdr_len > 0) {\n\t\t\tskb_add_rx_frag(skb, i++, buff->rxdata.page,\n\t\t\t\t\tbuff->rxdata.pg_off + hdr_len,\n\t\t\t\t\tbuff->len - hdr_len,\n\t\t\t\t\tself->frame_max);\n\t\t\tpage_ref_inc(buff->rxdata.page);\n\t\t}\n\n\t\tif (!buff->is_eop) {\n\t\t\tbuff_ = buff;\n\t\t\tdo {\n\t\t\t\tnext_ = buff_->next;\n\t\t\t\tbuff_ = &self->buff_ring[next_];\n\n\t\t\t\tdma_sync_single_range_for_cpu(aq_nic_get_dev(self->aq_nic),\n\t\t\t\t\t\t\t      buff_->rxdata.daddr,\n\t\t\t\t\t\t\t      buff_->rxdata.pg_off,\n\t\t\t\t\t\t\t      buff_->len,\n\t\t\t\t\t\t\t      DMA_FROM_DEVICE);\n\t\t\t\tskb_add_rx_frag(skb, i++,\n\t\t\t\t\t\tbuff_->rxdata.page,\n\t\t\t\t\t\tbuff_->rxdata.pg_off,\n\t\t\t\t\t\tbuff_->len,\n\t\t\t\t\t\tself->frame_max);\n\t\t\t\tpage_ref_inc(buff_->rxdata.page);\n\t\t\t\tbuff_->is_cleaned = 1;\n\n\t\t\t\tbuff->is_ip_cso &= buff_->is_ip_cso;\n\t\t\t\tbuff->is_udp_cso &= buff_->is_udp_cso;\n\t\t\t\tbuff->is_tcp_cso &= buff_->is_tcp_cso;\n\t\t\t\tbuff->is_cso_err |= buff_->is_cso_err;\n\n\t\t\t} while (!buff_->is_eop);\n\t\t}\n\n\t\tif (buff->is_vlan)\n\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t\t       buff->vlan_rx_tag);\n\n\t\tskb->protocol = eth_type_trans(skb, ndev);\n\n\t\taq_rx_checksum(self, buff, skb);\n\n\t\tskb_set_hash(skb, buff->rss_hash,\n\t\t\t     buff->is_hash_l4 ? PKT_HASH_TYPE_L4 :\n\t\t\t     PKT_HASH_TYPE_NONE);\n\t\t \n\t\tskb_record_rx_queue(skb,\n\t\t\t\t    is_ptp_ring ? 0\n\t\t\t\t\t\t: AQ_NIC_RING2QMAP(self->aq_nic,\n\t\t\t\t\t\t\t\t   self->idx));\n\n\t\tu64_stats_update_begin(&self->stats.rx.syncp);\n\t\t++self->stats.rx.packets;\n\t\tself->stats.rx.bytes += skb->len;\n\t\tu64_stats_update_end(&self->stats.rx.syncp);\n\n\t\tnapi_gro_receive(napi, skb);\n\t}\n\nerr_exit:\n\treturn err;\n}\n\nstatic int __aq_ring_xdp_clean(struct aq_ring_s *rx_ring,\n\t\t\t       struct napi_struct *napi, int *work_done,\n\t\t\t       int budget)\n{\n\tint frame_sz = rx_ring->page_offset + rx_ring->frame_max +\n\t\t       rx_ring->tail_size;\n\tstruct aq_nic_s *aq_nic = rx_ring->aq_nic;\n\tbool is_rsc_completed = true;\n\tstruct device *dev;\n\tint err = 0;\n\n\tdev = aq_nic_get_dev(aq_nic);\n\tfor (; (rx_ring->sw_head != rx_ring->hw_head) && budget;\n\t\trx_ring->sw_head = aq_ring_next_dx(rx_ring, rx_ring->sw_head),\n\t\t--budget, ++(*work_done)) {\n\t\tstruct aq_ring_buff_s *buff = &rx_ring->buff_ring[rx_ring->sw_head];\n\t\tbool is_ptp_ring = aq_ptp_ring(rx_ring->aq_nic, rx_ring);\n\t\tstruct aq_ring_buff_s *buff_ = NULL;\n\t\tu16 ptp_hwtstamp_len = 0;\n\t\tstruct skb_shared_hwtstamps shhwtstamps;\n\t\tstruct sk_buff *skb = NULL;\n\t\tunsigned int next_ = 0U;\n\t\tstruct xdp_buff xdp;\n\t\tvoid *hard_start;\n\n\t\tif (buff->is_cleaned)\n\t\t\tcontinue;\n\n\t\tif (!buff->is_eop) {\n\t\t\tbuff_ = buff;\n\t\t\tdo {\n\t\t\t\tif (buff_->next >= rx_ring->size) {\n\t\t\t\t\terr = -EIO;\n\t\t\t\t\tgoto err_exit;\n\t\t\t\t}\n\t\t\t\tnext_ = buff_->next;\n\t\t\t\tbuff_ = &rx_ring->buff_ring[next_];\n\t\t\t\tis_rsc_completed =\n\t\t\t\t\taq_ring_dx_in_range(rx_ring->sw_head,\n\t\t\t\t\t\t\t    next_,\n\t\t\t\t\t\t\t    rx_ring->hw_head);\n\n\t\t\t\tif (unlikely(!is_rsc_completed))\n\t\t\t\t\tbreak;\n\n\t\t\t\tbuff->is_error |= buff_->is_error;\n\t\t\t\tbuff->is_cso_err |= buff_->is_cso_err;\n\t\t\t} while (!buff_->is_eop);\n\n\t\t\tif (!is_rsc_completed) {\n\t\t\t\terr = 0;\n\t\t\t\tgoto err_exit;\n\t\t\t}\n\t\t\tif (buff->is_error ||\n\t\t\t    (buff->is_lro && buff->is_cso_err)) {\n\t\t\t\tbuff_ = buff;\n\t\t\t\tdo {\n\t\t\t\t\tif (buff_->next >= rx_ring->size) {\n\t\t\t\t\t\terr = -EIO;\n\t\t\t\t\t\tgoto err_exit;\n\t\t\t\t\t}\n\t\t\t\t\tnext_ = buff_->next;\n\t\t\t\t\tbuff_ = &rx_ring->buff_ring[next_];\n\n\t\t\t\t\tbuff_->is_cleaned = true;\n\t\t\t\t} while (!buff_->is_eop);\n\n\t\t\t\tu64_stats_update_begin(&rx_ring->stats.rx.syncp);\n\t\t\t\t++rx_ring->stats.rx.errors;\n\t\t\t\tu64_stats_update_end(&rx_ring->stats.rx.syncp);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (buff->is_error) {\n\t\t\tu64_stats_update_begin(&rx_ring->stats.rx.syncp);\n\t\t\t++rx_ring->stats.rx.errors;\n\t\t\tu64_stats_update_end(&rx_ring->stats.rx.syncp);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdma_sync_single_range_for_cpu(dev,\n\t\t\t\t\t      buff->rxdata.daddr,\n\t\t\t\t\t      buff->rxdata.pg_off,\n\t\t\t\t\t      buff->len, DMA_FROM_DEVICE);\n\t\thard_start = page_address(buff->rxdata.page) +\n\t\t\t     buff->rxdata.pg_off - rx_ring->page_offset;\n\n\t\tif (is_ptp_ring) {\n\t\t\tptp_hwtstamp_len = aq_ptp_extract_ts(rx_ring->aq_nic, &shhwtstamps,\n\t\t\t\t\t\t\t     aq_buf_vaddr(&buff->rxdata),\n\t\t\t\t\t\t\t     buff->len);\n\t\t\tbuff->len -= ptp_hwtstamp_len;\n\t\t}\n\n\t\txdp_init_buff(&xdp, frame_sz, &rx_ring->xdp_rxq);\n\t\txdp_prepare_buff(&xdp, hard_start, rx_ring->page_offset,\n\t\t\t\t buff->len, false);\n\t\tif (!buff->is_eop) {\n\t\t\tif (aq_add_rx_fragment(dev, rx_ring, buff, &xdp)) {\n\t\t\t\tu64_stats_update_begin(&rx_ring->stats.rx.syncp);\n\t\t\t\t++rx_ring->stats.rx.packets;\n\t\t\t\trx_ring->stats.rx.bytes += xdp_get_buff_len(&xdp);\n\t\t\t\t++rx_ring->stats.rx.xdp_aborted;\n\t\t\t\tu64_stats_update_end(&rx_ring->stats.rx.syncp);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tskb = aq_xdp_run_prog(aq_nic, &xdp, rx_ring, buff);\n\t\tif (IS_ERR(skb) || !skb)\n\t\t\tcontinue;\n\n\t\tif (ptp_hwtstamp_len > 0)\n\t\t\t*skb_hwtstamps(skb) = shhwtstamps;\n\n\t\tif (buff->is_vlan)\n\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t\t       buff->vlan_rx_tag);\n\n\t\taq_rx_checksum(rx_ring, buff, skb);\n\n\t\tskb_set_hash(skb, buff->rss_hash,\n\t\t\t     buff->is_hash_l4 ? PKT_HASH_TYPE_L4 :\n\t\t\t     PKT_HASH_TYPE_NONE);\n\t\t \n\t\tskb_record_rx_queue(skb,\n\t\t\t\t    is_ptp_ring ? 0\n\t\t\t\t\t\t: AQ_NIC_RING2QMAP(rx_ring->aq_nic,\n\t\t\t\t\t\t\t\t   rx_ring->idx));\n\n\t\tnapi_gro_receive(napi, skb);\n\t}\n\nerr_exit:\n\treturn err;\n}\n\nint aq_ring_rx_clean(struct aq_ring_s *self,\n\t\t     struct napi_struct *napi,\n\t\t     int *work_done,\n\t\t     int budget)\n{\n\tif (static_branch_unlikely(&aq_xdp_locking_key))\n\t\treturn __aq_ring_xdp_clean(self, napi, work_done, budget);\n\telse\n\t\treturn __aq_ring_rx_clean(self, napi, work_done, budget);\n}\n\nvoid aq_ring_hwts_rx_clean(struct aq_ring_s *self, struct aq_nic_s *aq_nic)\n{\n#if IS_REACHABLE(CONFIG_PTP_1588_CLOCK)\n\twhile (self->sw_head != self->hw_head) {\n\t\tu64 ns;\n\n\t\taq_nic->aq_hw_ops->extract_hwts(aq_nic->aq_hw,\n\t\t\t\t\t\tself->dx_ring +\n\t\t\t\t\t\t(self->sw_head * self->dx_size),\n\t\t\t\t\t\tself->dx_size, &ns);\n\t\taq_ptp_tx_hwtstamp(aq_nic, ns);\n\n\t\tself->sw_head = aq_ring_next_dx(self, self->sw_head);\n\t}\n#endif\n}\n\nint aq_ring_rx_fill(struct aq_ring_s *self)\n{\n\tstruct aq_ring_buff_s *buff = NULL;\n\tint err = 0;\n\tint i = 0;\n\n\tif (aq_ring_avail_dx(self) < min_t(unsigned int, AQ_CFG_RX_REFILL_THRES,\n\t\t\t\t\t   self->size / 2))\n\t\treturn err;\n\n\tfor (i = aq_ring_avail_dx(self); i--;\n\t\tself->sw_tail = aq_ring_next_dx(self, self->sw_tail)) {\n\t\tbuff = &self->buff_ring[self->sw_tail];\n\n\t\tbuff->flags = 0U;\n\t\tbuff->len = self->frame_max;\n\n\t\terr = aq_get_rxpages(self, buff);\n\t\tif (err)\n\t\t\tgoto err_exit;\n\n\t\tbuff->pa = aq_buf_daddr(&buff->rxdata);\n\t\tbuff = NULL;\n\t}\n\nerr_exit:\n\treturn err;\n}\n\nvoid aq_ring_rx_deinit(struct aq_ring_s *self)\n{\n\tif (!self)\n\t\treturn;\n\n\tfor (; self->sw_head != self->sw_tail;\n\t\tself->sw_head = aq_ring_next_dx(self, self->sw_head)) {\n\t\tstruct aq_ring_buff_s *buff = &self->buff_ring[self->sw_head];\n\n\t\taq_free_rxpage(&buff->rxdata, aq_nic_get_dev(self->aq_nic));\n\t}\n}\n\nvoid aq_ring_free(struct aq_ring_s *self)\n{\n\tif (!self)\n\t\treturn;\n\n\tkfree(self->buff_ring);\n\tself->buff_ring = NULL;\n\n\tif (self->dx_ring) {\n\t\tdma_free_coherent(aq_nic_get_dev(self->aq_nic),\n\t\t\t\t  self->size * self->dx_size, self->dx_ring,\n\t\t\t\t  self->dx_ring_pa);\n\t\tself->dx_ring = NULL;\n\t}\n}\n\nunsigned int aq_ring_fill_stats_data(struct aq_ring_s *self, u64 *data)\n{\n\tunsigned int count;\n\tunsigned int start;\n\n\tif (self->ring_type == ATL_RING_RX) {\n\t\t \n\t\tdo {\n\t\t\tcount = 0;\n\t\t\tstart = u64_stats_fetch_begin(&self->stats.rx.syncp);\n\t\t\tdata[count] = self->stats.rx.packets;\n\t\t\tdata[++count] = self->stats.rx.jumbo_packets;\n\t\t\tdata[++count] = self->stats.rx.lro_packets;\n\t\t\tdata[++count] = self->stats.rx.errors;\n\t\t\tdata[++count] = self->stats.rx.alloc_fails;\n\t\t\tdata[++count] = self->stats.rx.skb_alloc_fails;\n\t\t\tdata[++count] = self->stats.rx.polls;\n\t\t\tdata[++count] = self->stats.rx.pg_flips;\n\t\t\tdata[++count] = self->stats.rx.pg_reuses;\n\t\t\tdata[++count] = self->stats.rx.pg_losts;\n\t\t\tdata[++count] = self->stats.rx.xdp_aborted;\n\t\t\tdata[++count] = self->stats.rx.xdp_drop;\n\t\t\tdata[++count] = self->stats.rx.xdp_pass;\n\t\t\tdata[++count] = self->stats.rx.xdp_tx;\n\t\t\tdata[++count] = self->stats.rx.xdp_invalid;\n\t\t\tdata[++count] = self->stats.rx.xdp_redirect;\n\t\t} while (u64_stats_fetch_retry(&self->stats.rx.syncp, start));\n\t} else {\n\t\t \n\t\tdo {\n\t\t\tcount = 0;\n\t\t\tstart = u64_stats_fetch_begin(&self->stats.tx.syncp);\n\t\t\tdata[count] = self->stats.tx.packets;\n\t\t\tdata[++count] = self->stats.tx.queue_restarts;\n\t\t} while (u64_stats_fetch_retry(&self->stats.tx.syncp, start));\n\t}\n\n\treturn ++count;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}