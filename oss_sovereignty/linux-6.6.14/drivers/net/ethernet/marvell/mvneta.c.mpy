{
  "module_name": "mvneta.c",
  "hash_id": "fd728fbb6e1fe1a88b31afad7a8446570d3dac474d102b73f9f910c338c39b9f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/marvell/mvneta.c",
  "human_readable_source": " \n\n#include <linux/clk.h>\n#include <linux/cpu.h>\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/inetdevice.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/kernel.h>\n#include <linux/mbus.h>\n#include <linux/module.h>\n#include <linux/netdevice.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/of_irq.h>\n#include <linux/of_mdio.h>\n#include <linux/of_net.h>\n#include <linux/phy/phy.h>\n#include <linux/phy.h>\n#include <linux/phylink.h>\n#include <linux/platform_device.h>\n#include <linux/skbuff.h>\n#include <net/hwbm.h>\n#include \"mvneta_bm.h\"\n#include <net/ip.h>\n#include <net/ipv6.h>\n#include <net/tso.h>\n#include <net/page_pool/helpers.h>\n#include <net/pkt_sched.h>\n#include <linux/bpf_trace.h>\n\n \n#define MVNETA_RXQ_CONFIG_REG(q)                (0x1400 + ((q) << 2))\n#define      MVNETA_RXQ_HW_BUF_ALLOC            BIT(0)\n#define      MVNETA_RXQ_SHORT_POOL_ID_SHIFT\t4\n#define      MVNETA_RXQ_SHORT_POOL_ID_MASK\t0x30\n#define      MVNETA_RXQ_LONG_POOL_ID_SHIFT\t6\n#define      MVNETA_RXQ_LONG_POOL_ID_MASK\t0xc0\n#define      MVNETA_RXQ_PKT_OFFSET_ALL_MASK     (0xf    << 8)\n#define      MVNETA_RXQ_PKT_OFFSET_MASK(offs)   ((offs) << 8)\n#define MVNETA_RXQ_THRESHOLD_REG(q)             (0x14c0 + ((q) << 2))\n#define      MVNETA_RXQ_NON_OCCUPIED(v)         ((v) << 16)\n#define MVNETA_RXQ_BASE_ADDR_REG(q)             (0x1480 + ((q) << 2))\n#define MVNETA_RXQ_SIZE_REG(q)                  (0x14a0 + ((q) << 2))\n#define      MVNETA_RXQ_BUF_SIZE_SHIFT          19\n#define      MVNETA_RXQ_BUF_SIZE_MASK           (0x1fff << 19)\n#define MVNETA_RXQ_STATUS_REG(q)                (0x14e0 + ((q) << 2))\n#define      MVNETA_RXQ_OCCUPIED_ALL_MASK       0x3fff\n#define MVNETA_RXQ_STATUS_UPDATE_REG(q)         (0x1500 + ((q) << 2))\n#define      MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT  16\n#define      MVNETA_RXQ_ADD_NON_OCCUPIED_MAX    255\n#define MVNETA_PORT_POOL_BUFFER_SZ_REG(pool)\t(0x1700 + ((pool) << 2))\n#define      MVNETA_PORT_POOL_BUFFER_SZ_SHIFT\t3\n#define      MVNETA_PORT_POOL_BUFFER_SZ_MASK\t0xfff8\n#define MVNETA_PORT_RX_RESET                    0x1cc0\n#define      MVNETA_PORT_RX_DMA_RESET           BIT(0)\n#define MVNETA_PHY_ADDR                         0x2000\n#define      MVNETA_PHY_ADDR_MASK               0x1f\n#define MVNETA_MBUS_RETRY                       0x2010\n#define MVNETA_UNIT_INTR_CAUSE                  0x2080\n#define MVNETA_UNIT_CONTROL                     0x20B0\n#define      MVNETA_PHY_POLLING_ENABLE          BIT(1)\n#define MVNETA_WIN_BASE(w)                      (0x2200 + ((w) << 3))\n#define MVNETA_WIN_SIZE(w)                      (0x2204 + ((w) << 3))\n#define MVNETA_WIN_REMAP(w)                     (0x2280 + ((w) << 2))\n#define MVNETA_BASE_ADDR_ENABLE                 0x2290\n#define      MVNETA_AC5_CNM_DDR_TARGET\t\t0x2\n#define      MVNETA_AC5_CNM_DDR_ATTR\t\t0xb\n#define MVNETA_ACCESS_PROTECT_ENABLE            0x2294\n#define MVNETA_PORT_CONFIG                      0x2400\n#define      MVNETA_UNI_PROMISC_MODE            BIT(0)\n#define      MVNETA_DEF_RXQ(q)                  ((q) << 1)\n#define      MVNETA_DEF_RXQ_ARP(q)              ((q) << 4)\n#define      MVNETA_TX_UNSET_ERR_SUM            BIT(12)\n#define      MVNETA_DEF_RXQ_TCP(q)              ((q) << 16)\n#define      MVNETA_DEF_RXQ_UDP(q)              ((q) << 19)\n#define      MVNETA_DEF_RXQ_BPDU(q)             ((q) << 22)\n#define      MVNETA_RX_CSUM_WITH_PSEUDO_HDR     BIT(25)\n#define      MVNETA_PORT_CONFIG_DEFL_VALUE(q)   (MVNETA_DEF_RXQ(q)       | \\\n\t\t\t\t\t\t MVNETA_DEF_RXQ_ARP(q)\t | \\\n\t\t\t\t\t\t MVNETA_DEF_RXQ_TCP(q)\t | \\\n\t\t\t\t\t\t MVNETA_DEF_RXQ_UDP(q)\t | \\\n\t\t\t\t\t\t MVNETA_DEF_RXQ_BPDU(q)\t | \\\n\t\t\t\t\t\t MVNETA_TX_UNSET_ERR_SUM | \\\n\t\t\t\t\t\t MVNETA_RX_CSUM_WITH_PSEUDO_HDR)\n#define MVNETA_PORT_CONFIG_EXTEND                0x2404\n#define MVNETA_MAC_ADDR_LOW                      0x2414\n#define MVNETA_MAC_ADDR_HIGH                     0x2418\n#define MVNETA_SDMA_CONFIG                       0x241c\n#define      MVNETA_SDMA_BRST_SIZE_16            4\n#define      MVNETA_RX_BRST_SZ_MASK(burst)       ((burst) << 1)\n#define      MVNETA_RX_NO_DATA_SWAP              BIT(4)\n#define      MVNETA_TX_NO_DATA_SWAP              BIT(5)\n#define      MVNETA_DESC_SWAP                    BIT(6)\n#define      MVNETA_TX_BRST_SZ_MASK(burst)       ((burst) << 22)\n#define\tMVNETA_VLAN_PRIO_TO_RXQ\t\t\t 0x2440\n#define      MVNETA_VLAN_PRIO_RXQ_MAP(prio, rxq) ((rxq) << ((prio) * 3))\n#define MVNETA_PORT_STATUS                       0x2444\n#define      MVNETA_TX_IN_PRGRS                  BIT(0)\n#define      MVNETA_TX_FIFO_EMPTY                BIT(8)\n#define MVNETA_RX_MIN_FRAME_SIZE                 0x247c\n \n#define MVNETA_SERDES_CFG\t\t\t 0x24A0\n#define      MVNETA_SGMII_SERDES_PROTO\t\t 0x0cc7\n#define      MVNETA_QSGMII_SERDES_PROTO\t\t 0x0667\n#define      MVNETA_HSGMII_SERDES_PROTO\t\t 0x1107\n#define MVNETA_TYPE_PRIO                         0x24bc\n#define      MVNETA_FORCE_UNI                    BIT(21)\n#define MVNETA_TXQ_CMD_1                         0x24e4\n#define MVNETA_TXQ_CMD                           0x2448\n#define      MVNETA_TXQ_DISABLE_SHIFT            8\n#define      MVNETA_TXQ_ENABLE_MASK              0x000000ff\n#define MVNETA_RX_DISCARD_FRAME_COUNT\t\t 0x2484\n#define MVNETA_OVERRUN_FRAME_COUNT\t\t 0x2488\n#define MVNETA_GMAC_CLOCK_DIVIDER                0x24f4\n#define      MVNETA_GMAC_1MS_CLOCK_ENABLE        BIT(31)\n#define MVNETA_ACC_MODE                          0x2500\n#define MVNETA_BM_ADDRESS                        0x2504\n#define MVNETA_CPU_MAP(cpu)                      (0x2540 + ((cpu) << 2))\n#define      MVNETA_CPU_RXQ_ACCESS_ALL_MASK      0x000000ff\n#define      MVNETA_CPU_TXQ_ACCESS_ALL_MASK      0x0000ff00\n#define      MVNETA_CPU_RXQ_ACCESS(rxq)\t\t BIT(rxq)\n#define      MVNETA_CPU_TXQ_ACCESS(txq)\t\t BIT(txq + 8)\n#define MVNETA_RXQ_TIME_COAL_REG(q)              (0x2580 + ((q) << 2))\n\n \n\n#define MVNETA_INTR_NEW_CAUSE                    0x25a0\n#define MVNETA_INTR_NEW_MASK                     0x25a4\n\n \n#define      MVNETA_TX_INTR_MASK(nr_txqs)        (((1 << nr_txqs) - 1) << 0)\n#define      MVNETA_TX_INTR_MASK_ALL             (0xff << 0)\n#define      MVNETA_RX_INTR_MASK(nr_rxqs)        (((1 << nr_rxqs) - 1) << 8)\n#define      MVNETA_RX_INTR_MASK_ALL             (0xff << 8)\n#define      MVNETA_MISCINTR_INTR_MASK           BIT(31)\n\n#define MVNETA_INTR_OLD_CAUSE                    0x25a8\n#define MVNETA_INTR_OLD_MASK                     0x25ac\n\n \n#define MVNETA_INTR_MISC_CAUSE                   0x25b0\n#define MVNETA_INTR_MISC_MASK                    0x25b4\n\n#define      MVNETA_CAUSE_PHY_STATUS_CHANGE      BIT(0)\n#define      MVNETA_CAUSE_LINK_CHANGE            BIT(1)\n#define      MVNETA_CAUSE_PTP                    BIT(4)\n\n#define      MVNETA_CAUSE_INTERNAL_ADDR_ERR      BIT(7)\n#define      MVNETA_CAUSE_RX_OVERRUN             BIT(8)\n#define      MVNETA_CAUSE_RX_CRC_ERROR           BIT(9)\n#define      MVNETA_CAUSE_RX_LARGE_PKT           BIT(10)\n#define      MVNETA_CAUSE_TX_UNDERUN             BIT(11)\n#define      MVNETA_CAUSE_PRBS_ERR               BIT(12)\n#define      MVNETA_CAUSE_PSC_SYNC_CHANGE        BIT(13)\n#define      MVNETA_CAUSE_SERDES_SYNC_ERR        BIT(14)\n\n#define      MVNETA_CAUSE_BMU_ALLOC_ERR_SHIFT    16\n#define      MVNETA_CAUSE_BMU_ALLOC_ERR_ALL_MASK   (0xF << MVNETA_CAUSE_BMU_ALLOC_ERR_SHIFT)\n#define      MVNETA_CAUSE_BMU_ALLOC_ERR_MASK(pool) (1 << (MVNETA_CAUSE_BMU_ALLOC_ERR_SHIFT + (pool)))\n\n#define      MVNETA_CAUSE_TXQ_ERROR_SHIFT        24\n#define      MVNETA_CAUSE_TXQ_ERROR_ALL_MASK     (0xFF << MVNETA_CAUSE_TXQ_ERROR_SHIFT)\n#define      MVNETA_CAUSE_TXQ_ERROR_MASK(q)      (1 << (MVNETA_CAUSE_TXQ_ERROR_SHIFT + (q)))\n\n#define MVNETA_INTR_ENABLE                       0x25b8\n#define      MVNETA_TXQ_INTR_ENABLE_ALL_MASK     0x0000ff00\n#define      MVNETA_RXQ_INTR_ENABLE_ALL_MASK     0x000000ff\n\n#define MVNETA_RXQ_CMD                           0x2680\n#define      MVNETA_RXQ_DISABLE_SHIFT            8\n#define      MVNETA_RXQ_ENABLE_MASK              0x000000ff\n#define MVETH_TXQ_TOKEN_COUNT_REG(q)             (0x2700 + ((q) << 4))\n#define MVETH_TXQ_TOKEN_CFG_REG(q)               (0x2704 + ((q) << 4))\n#define MVNETA_GMAC_CTRL_0                       0x2c00\n#define      MVNETA_GMAC_MAX_RX_SIZE_SHIFT       2\n#define      MVNETA_GMAC_MAX_RX_SIZE_MASK        0x7ffc\n#define      MVNETA_GMAC0_PORT_1000BASE_X        BIT(1)\n#define      MVNETA_GMAC0_PORT_ENABLE            BIT(0)\n#define MVNETA_GMAC_CTRL_2                       0x2c08\n#define      MVNETA_GMAC2_INBAND_AN_ENABLE       BIT(0)\n#define      MVNETA_GMAC2_PCS_ENABLE             BIT(3)\n#define      MVNETA_GMAC2_PORT_RGMII             BIT(4)\n#define      MVNETA_GMAC2_PORT_RESET             BIT(6)\n#define MVNETA_GMAC_STATUS                       0x2c10\n#define      MVNETA_GMAC_LINK_UP                 BIT(0)\n#define      MVNETA_GMAC_SPEED_1000              BIT(1)\n#define      MVNETA_GMAC_SPEED_100               BIT(2)\n#define      MVNETA_GMAC_FULL_DUPLEX             BIT(3)\n#define      MVNETA_GMAC_RX_FLOW_CTRL_ENABLE     BIT(4)\n#define      MVNETA_GMAC_TX_FLOW_CTRL_ENABLE     BIT(5)\n#define      MVNETA_GMAC_RX_FLOW_CTRL_ACTIVE     BIT(6)\n#define      MVNETA_GMAC_TX_FLOW_CTRL_ACTIVE     BIT(7)\n#define      MVNETA_GMAC_AN_COMPLETE             BIT(11)\n#define      MVNETA_GMAC_SYNC_OK                 BIT(14)\n#define MVNETA_GMAC_AUTONEG_CONFIG               0x2c0c\n#define      MVNETA_GMAC_FORCE_LINK_DOWN         BIT(0)\n#define      MVNETA_GMAC_FORCE_LINK_PASS         BIT(1)\n#define      MVNETA_GMAC_INBAND_AN_ENABLE        BIT(2)\n#define      MVNETA_GMAC_AN_BYPASS_ENABLE        BIT(3)\n#define      MVNETA_GMAC_INBAND_RESTART_AN       BIT(4)\n#define      MVNETA_GMAC_CONFIG_MII_SPEED        BIT(5)\n#define      MVNETA_GMAC_CONFIG_GMII_SPEED       BIT(6)\n#define      MVNETA_GMAC_AN_SPEED_EN             BIT(7)\n#define      MVNETA_GMAC_CONFIG_FLOW_CTRL        BIT(8)\n#define      MVNETA_GMAC_ADVERT_SYM_FLOW_CTRL    BIT(9)\n#define      MVNETA_GMAC_AN_FLOW_CTRL_EN         BIT(11)\n#define      MVNETA_GMAC_CONFIG_FULL_DUPLEX      BIT(12)\n#define      MVNETA_GMAC_AN_DUPLEX_EN            BIT(13)\n#define MVNETA_GMAC_CTRL_4                       0x2c90\n#define      MVNETA_GMAC4_SHORT_PREAMBLE_ENABLE  BIT(1)\n#define MVNETA_MIB_COUNTERS_BASE                 0x3000\n#define      MVNETA_MIB_LATE_COLLISION           0x7c\n#define MVNETA_DA_FILT_SPEC_MCAST                0x3400\n#define MVNETA_DA_FILT_OTH_MCAST                 0x3500\n#define MVNETA_DA_FILT_UCAST_BASE                0x3600\n#define MVNETA_TXQ_BASE_ADDR_REG(q)              (0x3c00 + ((q) << 2))\n#define MVNETA_TXQ_SIZE_REG(q)                   (0x3c20 + ((q) << 2))\n#define      MVNETA_TXQ_SENT_THRESH_ALL_MASK     0x3fff0000\n#define      MVNETA_TXQ_SENT_THRESH_MASK(coal)   ((coal) << 16)\n#define MVNETA_TXQ_UPDATE_REG(q)                 (0x3c60 + ((q) << 2))\n#define      MVNETA_TXQ_DEC_SENT_SHIFT           16\n#define      MVNETA_TXQ_DEC_SENT_MASK            0xff\n#define MVNETA_TXQ_STATUS_REG(q)                 (0x3c40 + ((q) << 2))\n#define      MVNETA_TXQ_SENT_DESC_SHIFT          16\n#define      MVNETA_TXQ_SENT_DESC_MASK           0x3fff0000\n#define MVNETA_PORT_TX_RESET                     0x3cf0\n#define      MVNETA_PORT_TX_DMA_RESET            BIT(0)\n#define MVNETA_TXQ_CMD1_REG\t\t\t 0x3e00\n#define      MVNETA_TXQ_CMD1_BW_LIM_SEL_V1\t BIT(3)\n#define      MVNETA_TXQ_CMD1_BW_LIM_EN\t\t BIT(0)\n#define MVNETA_REFILL_NUM_CLK_REG\t\t 0x3e08\n#define      MVNETA_REFILL_MAX_NUM_CLK\t\t 0x0000ffff\n#define MVNETA_TX_MTU                            0x3e0c\n#define MVNETA_TX_TOKEN_SIZE                     0x3e14\n#define      MVNETA_TX_TOKEN_SIZE_MAX            0xffffffff\n#define MVNETA_TXQ_BUCKET_REFILL_REG(q)\t\t (0x3e20 + ((q) << 2))\n#define      MVNETA_TXQ_BUCKET_REFILL_PERIOD_MASK\t0x3ff00000\n#define      MVNETA_TXQ_BUCKET_REFILL_PERIOD_SHIFT\t20\n#define      MVNETA_TXQ_BUCKET_REFILL_VALUE_MAX\t 0x0007ffff\n#define MVNETA_TXQ_TOKEN_SIZE_REG(q)             (0x3e40 + ((q) << 2))\n#define      MVNETA_TXQ_TOKEN_SIZE_MAX           0x7fffffff\n\n \n\n \n#define MVNETA_TXQ_BUCKET_REFILL_BASE_PERIOD_NS\t100\n\n \n#define MVNETA_TXQ_BUCKET_REFILL_PERIOD\t1000\n\n \n#define MVNETA_TXQ_RATE_LIMIT_RESOLUTION (NSEC_PER_SEC / \\\n\t\t\t\t\t (MVNETA_TXQ_BUCKET_REFILL_BASE_PERIOD_NS * \\\n\t\t\t\t\t  MVNETA_TXQ_BUCKET_REFILL_PERIOD))\n\n#define MVNETA_LPI_CTRL_0                        0x2cc0\n#define MVNETA_LPI_CTRL_1                        0x2cc4\n#define      MVNETA_LPI_REQUEST_ENABLE           BIT(0)\n#define MVNETA_LPI_CTRL_2                        0x2cc8\n#define MVNETA_LPI_STATUS                        0x2ccc\n\n#define MVNETA_CAUSE_TXQ_SENT_DESC_ALL_MASK\t 0xff\n\n \n#define MVNETA_QUEUE_NEXT_DESC(q, index)\t\\\n\t(((index) < (q)->last_desc) ? ((index) + 1) : 0)\n\n \n\n \n#define MVNETA_TXDONE_COAL_PKTS\t\t0\t \n#define MVNETA_RX_COAL_PKTS\t\t32\n#define MVNETA_RX_COAL_USEC\t\t100\n\n \n#define MVNETA_MH_SIZE\t\t\t2\n\n#define MVNETA_VLAN_TAG_LEN             4\n\n#define MVNETA_TX_CSUM_DEF_SIZE\t\t1600\n#define MVNETA_TX_CSUM_MAX_SIZE\t\t9800\n#define MVNETA_ACC_MODE_EXT1\t\t1\n#define MVNETA_ACC_MODE_EXT2\t\t2\n\n#define MVNETA_MAX_DECODE_WIN\t\t6\n\n \n#define MVNETA_TX_DISABLE_TIMEOUT_MSEC\t1000\n#define MVNETA_RX_DISABLE_TIMEOUT_MSEC\t1000\n#define MVNETA_TX_FIFO_EMPTY_TIMEOUT\t10000\n\n#define MVNETA_TX_MTU_MAX\t\t0x3ffff\n\n \n#define MVNETA_RSS_LU_TABLE_SIZE\t1\n\n \n#define MVNETA_MAX_RXD 512\n\n \n#define MVNETA_MAX_TXD 1024\n\n \n#define MVNETA_MAX_TSO_SEGS 100\n\n#define MVNETA_MAX_SKB_DESCS (MVNETA_MAX_TSO_SEGS * 2 + MAX_SKB_FRAGS)\n\n \n#define MVNETA_TSO_PAGE_SIZE (2 * PAGE_SIZE)\n\n \n#define MVNETA_TSO_PER_PAGE (MVNETA_TSO_PAGE_SIZE / TSO_HEADER_SIZE)\n\n \n#define MVNETA_MAX_TSO_PAGES (MVNETA_MAX_TXD / MVNETA_TSO_PER_PAGE)\n\n \n#define MVNETA_DESC_ALIGNED_SIZE\t32\n\n \n#define MVNETA_RX_PKT_OFFSET_CORRECTION\t\t64\n\n#define MVNETA_RX_PKT_SIZE(mtu) \\\n\tALIGN((mtu) + MVNETA_MH_SIZE + MVNETA_VLAN_TAG_LEN + \\\n\t      ETH_HLEN + ETH_FCS_LEN,\t\t\t     \\\n\t      cache_line_size())\n\n \n#define MVNETA_SKB_HEADROOM\tALIGN(max(NET_SKB_PAD, XDP_PACKET_HEADROOM), 8)\n#define MVNETA_SKB_PAD\t(SKB_DATA_ALIGN(sizeof(struct skb_shared_info) + \\\n\t\t\t MVNETA_SKB_HEADROOM))\n#define MVNETA_MAX_RX_BUF_SIZE\t(PAGE_SIZE - MVNETA_SKB_PAD)\n\n#define MVNETA_RX_GET_BM_POOL_ID(rxd) \\\n\t(((rxd)->status & MVNETA_RXD_BM_POOL_MASK) >> MVNETA_RXD_BM_POOL_SHIFT)\n\nenum {\n\tETHTOOL_STAT_EEE_WAKEUP,\n\tETHTOOL_STAT_SKB_ALLOC_ERR,\n\tETHTOOL_STAT_REFILL_ERR,\n\tETHTOOL_XDP_REDIRECT,\n\tETHTOOL_XDP_PASS,\n\tETHTOOL_XDP_DROP,\n\tETHTOOL_XDP_TX,\n\tETHTOOL_XDP_TX_ERR,\n\tETHTOOL_XDP_XMIT,\n\tETHTOOL_XDP_XMIT_ERR,\n\tETHTOOL_MAX_STATS,\n};\n\nstruct mvneta_statistic {\n\tunsigned short offset;\n\tunsigned short type;\n\tconst char name[ETH_GSTRING_LEN];\n};\n\n#define T_REG_32\t32\n#define T_REG_64\t64\n#define T_SW\t\t1\n\n#define MVNETA_XDP_PASS\t\t0\n#define MVNETA_XDP_DROPPED\tBIT(0)\n#define MVNETA_XDP_TX\t\tBIT(1)\n#define MVNETA_XDP_REDIR\tBIT(2)\n\nstatic const struct mvneta_statistic mvneta_statistics[] = {\n\t{ 0x3000, T_REG_64, \"good_octets_received\", },\n\t{ 0x3010, T_REG_32, \"good_frames_received\", },\n\t{ 0x3008, T_REG_32, \"bad_octets_received\", },\n\t{ 0x3014, T_REG_32, \"bad_frames_received\", },\n\t{ 0x3018, T_REG_32, \"broadcast_frames_received\", },\n\t{ 0x301c, T_REG_32, \"multicast_frames_received\", },\n\t{ 0x3050, T_REG_32, \"unrec_mac_control_received\", },\n\t{ 0x3058, T_REG_32, \"good_fc_received\", },\n\t{ 0x305c, T_REG_32, \"bad_fc_received\", },\n\t{ 0x3060, T_REG_32, \"undersize_received\", },\n\t{ 0x3064, T_REG_32, \"fragments_received\", },\n\t{ 0x3068, T_REG_32, \"oversize_received\", },\n\t{ 0x306c, T_REG_32, \"jabber_received\", },\n\t{ 0x3070, T_REG_32, \"mac_receive_error\", },\n\t{ 0x3074, T_REG_32, \"bad_crc_event\", },\n\t{ 0x3078, T_REG_32, \"collision\", },\n\t{ 0x307c, T_REG_32, \"late_collision\", },\n\t{ 0x2484, T_REG_32, \"rx_discard\", },\n\t{ 0x2488, T_REG_32, \"rx_overrun\", },\n\t{ 0x3020, T_REG_32, \"frames_64_octets\", },\n\t{ 0x3024, T_REG_32, \"frames_65_to_127_octets\", },\n\t{ 0x3028, T_REG_32, \"frames_128_to_255_octets\", },\n\t{ 0x302c, T_REG_32, \"frames_256_to_511_octets\", },\n\t{ 0x3030, T_REG_32, \"frames_512_to_1023_octets\", },\n\t{ 0x3034, T_REG_32, \"frames_1024_to_max_octets\", },\n\t{ 0x3038, T_REG_64, \"good_octets_sent\", },\n\t{ 0x3040, T_REG_32, \"good_frames_sent\", },\n\t{ 0x3044, T_REG_32, \"excessive_collision\", },\n\t{ 0x3048, T_REG_32, \"multicast_frames_sent\", },\n\t{ 0x304c, T_REG_32, \"broadcast_frames_sent\", },\n\t{ 0x3054, T_REG_32, \"fc_sent\", },\n\t{ 0x300c, T_REG_32, \"internal_mac_transmit_err\", },\n\t{ ETHTOOL_STAT_EEE_WAKEUP, T_SW, \"eee_wakeup_errors\", },\n\t{ ETHTOOL_STAT_SKB_ALLOC_ERR, T_SW, \"skb_alloc_errors\", },\n\t{ ETHTOOL_STAT_REFILL_ERR, T_SW, \"refill_errors\", },\n\t{ ETHTOOL_XDP_REDIRECT, T_SW, \"rx_xdp_redirect\", },\n\t{ ETHTOOL_XDP_PASS, T_SW, \"rx_xdp_pass\", },\n\t{ ETHTOOL_XDP_DROP, T_SW, \"rx_xdp_drop\", },\n\t{ ETHTOOL_XDP_TX, T_SW, \"rx_xdp_tx\", },\n\t{ ETHTOOL_XDP_TX_ERR, T_SW, \"rx_xdp_tx_errors\", },\n\t{ ETHTOOL_XDP_XMIT, T_SW, \"tx_xdp_xmit\", },\n\t{ ETHTOOL_XDP_XMIT_ERR, T_SW, \"tx_xdp_xmit_errors\", },\n};\n\nstruct mvneta_stats {\n\tu64\trx_packets;\n\tu64\trx_bytes;\n\tu64\ttx_packets;\n\tu64\ttx_bytes;\n\t \n\tu64\txdp_redirect;\n\tu64\txdp_pass;\n\tu64\txdp_drop;\n\tu64\txdp_xmit;\n\tu64\txdp_xmit_err;\n\tu64\txdp_tx;\n\tu64\txdp_tx_err;\n};\n\nstruct mvneta_ethtool_stats {\n\tstruct mvneta_stats ps;\n\tu64\tskb_alloc_error;\n\tu64\trefill_error;\n};\n\nstruct mvneta_pcpu_stats {\n\tstruct u64_stats_sync syncp;\n\n\tstruct mvneta_ethtool_stats es;\n\tu64\trx_dropped;\n\tu64\trx_errors;\n};\n\nstruct mvneta_pcpu_port {\n\t \n\tstruct mvneta_port\t*pp;\n\n\t \n\tstruct napi_struct\tnapi;\n\n\t \n\tu32\t\t\tcause_rx_tx;\n};\n\nenum {\n\t__MVNETA_DOWN,\n};\n\nstruct mvneta_port {\n\tu8 id;\n\tstruct mvneta_pcpu_port __percpu\t*ports;\n\tstruct mvneta_pcpu_stats __percpu\t*stats;\n\n\tunsigned long state;\n\n\tint pkt_size;\n\tvoid __iomem *base;\n\tstruct mvneta_rx_queue *rxqs;\n\tstruct mvneta_tx_queue *txqs;\n\tstruct net_device *dev;\n\tstruct hlist_node node_online;\n\tstruct hlist_node node_dead;\n\tint rxq_def;\n\t \n\tspinlock_t lock;\n\tbool is_stopped;\n\n\tu32 cause_rx_tx;\n\tstruct napi_struct napi;\n\n\tstruct bpf_prog *xdp_prog;\n\n\t \n\tstruct clk *clk;\n\t \n\tstruct clk *clk_bus;\n\tu8 mcast_count[256];\n\tu16 tx_ring_size;\n\tu16 rx_ring_size;\n\n\tphy_interface_t phy_interface;\n\tstruct device_node *dn;\n\tunsigned int tx_csum_limit;\n\tstruct phylink *phylink;\n\tstruct phylink_config phylink_config;\n\tstruct phylink_pcs phylink_pcs;\n\tstruct phy *comphy;\n\n\tstruct mvneta_bm *bm_priv;\n\tstruct mvneta_bm_pool *pool_long;\n\tstruct mvneta_bm_pool *pool_short;\n\tint bm_win_id;\n\n\tbool eee_enabled;\n\tbool eee_active;\n\tbool tx_lpi_enabled;\n\n\tu64 ethtool_stats[ARRAY_SIZE(mvneta_statistics)];\n\n\tu32 indir[MVNETA_RSS_LU_TABLE_SIZE];\n\n\t \n\tbool neta_armada3700;\n\tbool neta_ac5;\n\tu16 rx_offset_correction;\n\tconst struct mbus_dram_target_info *dram_target_info;\n};\n\n \n\n#define MVNETA_TX_L3_OFF_SHIFT\t0\n#define MVNETA_TX_IP_HLEN_SHIFT\t8\n#define MVNETA_TX_L4_UDP\tBIT(16)\n#define MVNETA_TX_L3_IP6\tBIT(17)\n#define MVNETA_TXD_IP_CSUM\tBIT(18)\n#define MVNETA_TXD_Z_PAD\tBIT(19)\n#define MVNETA_TXD_L_DESC\tBIT(20)\n#define MVNETA_TXD_F_DESC\tBIT(21)\n#define MVNETA_TXD_FLZ_DESC\t(MVNETA_TXD_Z_PAD  | \\\n\t\t\t\t MVNETA_TXD_L_DESC | \\\n\t\t\t\t MVNETA_TXD_F_DESC)\n#define MVNETA_TX_L4_CSUM_FULL\tBIT(30)\n#define MVNETA_TX_L4_CSUM_NOT\tBIT(31)\n\n#define MVNETA_RXD_ERR_CRC\t\t0x0\n#define MVNETA_RXD_BM_POOL_SHIFT\t13\n#define MVNETA_RXD_BM_POOL_MASK\t\t(BIT(13) | BIT(14))\n#define MVNETA_RXD_ERR_SUMMARY\t\tBIT(16)\n#define MVNETA_RXD_ERR_OVERRUN\t\tBIT(17)\n#define MVNETA_RXD_ERR_LEN\t\tBIT(18)\n#define MVNETA_RXD_ERR_RESOURCE\t\t(BIT(17) | BIT(18))\n#define MVNETA_RXD_ERR_CODE_MASK\t(BIT(17) | BIT(18))\n#define MVNETA_RXD_L3_IP4\t\tBIT(25)\n#define MVNETA_RXD_LAST_DESC\t\tBIT(26)\n#define MVNETA_RXD_FIRST_DESC\t\tBIT(27)\n#define MVNETA_RXD_FIRST_LAST_DESC\t(MVNETA_RXD_FIRST_DESC | \\\n\t\t\t\t\t MVNETA_RXD_LAST_DESC)\n#define MVNETA_RXD_L4_CSUM_OK\t\tBIT(30)\n\n#if defined(__LITTLE_ENDIAN)\nstruct mvneta_tx_desc {\n\tu32  command;\t\t \n\tu16  reserved1;\t\t \n\tu16  data_size;\t\t \n\tu32  buf_phys_addr;\t \n\tu32  reserved2;\t\t \n\tu32  reserved3[4];\t \n};\n\nstruct mvneta_rx_desc {\n\tu32  status;\t\t \n\tu16  reserved1;\t\t \n\tu16  data_size;\t\t \n\n\tu32  buf_phys_addr;\t \n\tu32  reserved2;\t\t \n\n\tu32  buf_cookie;\t \n\tu16  reserved3;\t\t \n\tu16  reserved4;\t\t \n\n\tu32  reserved5;\t\t \n\tu32  reserved6;\t\t \n};\n#else\nstruct mvneta_tx_desc {\n\tu16  data_size;\t\t \n\tu16  reserved1;\t\t \n\tu32  command;\t\t \n\tu32  reserved2;\t\t \n\tu32  buf_phys_addr;\t \n\tu32  reserved3[4];\t \n};\n\nstruct mvneta_rx_desc {\n\tu16  data_size;\t\t \n\tu16  reserved1;\t\t \n\tu32  status;\t\t \n\n\tu32  reserved2;\t\t \n\tu32  buf_phys_addr;\t \n\n\tu16  reserved4;\t\t \n\tu16  reserved3;\t\t \n\tu32  buf_cookie;\t \n\n\tu32  reserved5;\t\t \n\tu32  reserved6;\t\t \n};\n#endif\n\nenum mvneta_tx_buf_type {\n\tMVNETA_TYPE_TSO,\n\tMVNETA_TYPE_SKB,\n\tMVNETA_TYPE_XDP_TX,\n\tMVNETA_TYPE_XDP_NDO,\n};\n\nstruct mvneta_tx_buf {\n\tenum mvneta_tx_buf_type type;\n\tunion {\n\t\tstruct xdp_frame *xdpf;\n\t\tstruct sk_buff *skb;\n\t};\n};\n\nstruct mvneta_tx_queue {\n\t \n\tu8 id;\n\n\t \n\tint size;\n\n\t \n\tint count;\n\tint pending;\n\tint tx_stop_threshold;\n\tint tx_wake_threshold;\n\n\t \n\tstruct mvneta_tx_buf *buf;\n\n\t \n\tint txq_put_index;\n\n\t \n\tint txq_get_index;\n\n\tu32 done_pkts_coal;\n\n\t \n\tstruct mvneta_tx_desc *descs;\n\n\t \n\tdma_addr_t descs_phys;\n\n\t \n\tint last_desc;\n\n\t \n\tint next_desc_to_proc;\n\n\t \n\tchar *tso_hdrs[MVNETA_MAX_TSO_PAGES];\n\n\t \n\tdma_addr_t tso_hdrs_phys[MVNETA_MAX_TSO_PAGES];\n\n\t \n\tcpumask_t affinity_mask;\n};\n\nstruct mvneta_rx_queue {\n\t \n\tu8 id;\n\n\t \n\tint size;\n\n\tu32 pkts_coal;\n\tu32 time_coal;\n\n\t \n\tstruct page_pool *page_pool;\n\tstruct xdp_rxq_info xdp_rxq;\n\n\t \n\tvoid  **buf_virt_addr;\n\n\t \n\tstruct mvneta_rx_desc *descs;\n\n\t \n\tdma_addr_t descs_phys;\n\n\t \n\tint last_desc;\n\n\t \n\tint next_desc_to_proc;\n\n\t \n\tint first_to_refill;\n\tu32 refill_num;\n};\n\nstatic enum cpuhp_state online_hpstate;\n \nstatic int rxq_number = 8;\nstatic int txq_number = 8;\n\nstatic int rxq_def;\n\nstatic int rx_copybreak __read_mostly = 256;\n\n \nstatic int global_port_id;\n\n#define MVNETA_DRIVER_NAME \"mvneta\"\n#define MVNETA_DRIVER_VERSION \"1.0\"\n\n \n\n \nstatic void mvreg_write(struct mvneta_port *pp, u32 offset, u32 data)\n{\n\twritel(data, pp->base + offset);\n}\n\n \nstatic u32 mvreg_read(struct mvneta_port *pp, u32 offset)\n{\n\treturn readl(pp->base + offset);\n}\n\n \nstatic void mvneta_txq_inc_get(struct mvneta_tx_queue *txq)\n{\n\ttxq->txq_get_index++;\n\tif (txq->txq_get_index == txq->size)\n\t\ttxq->txq_get_index = 0;\n}\n\n \nstatic void mvneta_txq_inc_put(struct mvneta_tx_queue *txq)\n{\n\ttxq->txq_put_index++;\n\tif (txq->txq_put_index == txq->size)\n\t\ttxq->txq_put_index = 0;\n}\n\n\n \nstatic void mvneta_mib_counters_clear(struct mvneta_port *pp)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < MVNETA_MIB_LATE_COLLISION; i += 4)\n\t\tmvreg_read(pp, (MVNETA_MIB_COUNTERS_BASE + i));\n\tmvreg_read(pp, MVNETA_RX_DISCARD_FRAME_COUNT);\n\tmvreg_read(pp, MVNETA_OVERRUN_FRAME_COUNT);\n}\n\n \nstatic void\nmvneta_get_stats64(struct net_device *dev,\n\t\t   struct rtnl_link_stats64 *stats)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tunsigned int start;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct mvneta_pcpu_stats *cpu_stats;\n\t\tu64 rx_packets;\n\t\tu64 rx_bytes;\n\t\tu64 rx_dropped;\n\t\tu64 rx_errors;\n\t\tu64 tx_packets;\n\t\tu64 tx_bytes;\n\n\t\tcpu_stats = per_cpu_ptr(pp->stats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&cpu_stats->syncp);\n\t\t\trx_packets = cpu_stats->es.ps.rx_packets;\n\t\t\trx_bytes   = cpu_stats->es.ps.rx_bytes;\n\t\t\trx_dropped = cpu_stats->rx_dropped;\n\t\t\trx_errors  = cpu_stats->rx_errors;\n\t\t\ttx_packets = cpu_stats->es.ps.tx_packets;\n\t\t\ttx_bytes   = cpu_stats->es.ps.tx_bytes;\n\t\t} while (u64_stats_fetch_retry(&cpu_stats->syncp, start));\n\n\t\tstats->rx_packets += rx_packets;\n\t\tstats->rx_bytes   += rx_bytes;\n\t\tstats->rx_dropped += rx_dropped;\n\t\tstats->rx_errors  += rx_errors;\n\t\tstats->tx_packets += tx_packets;\n\t\tstats->tx_bytes   += tx_bytes;\n\t}\n\n\tstats->tx_dropped\t= dev->stats.tx_dropped;\n}\n\n \n\n \nstatic int mvneta_rxq_desc_is_first_last(u32 status)\n{\n\treturn (status & MVNETA_RXD_FIRST_LAST_DESC) ==\n\t\tMVNETA_RXD_FIRST_LAST_DESC;\n}\n\n \nstatic void mvneta_rxq_non_occup_desc_add(struct mvneta_port *pp,\n\t\t\t\t\t  struct mvneta_rx_queue *rxq,\n\t\t\t\t\t  int ndescs)\n{\n\t \n\twhile (ndescs > MVNETA_RXQ_ADD_NON_OCCUPIED_MAX) {\n\t\tmvreg_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id),\n\t\t\t    (MVNETA_RXQ_ADD_NON_OCCUPIED_MAX <<\n\t\t\t     MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT));\n\t\tndescs -= MVNETA_RXQ_ADD_NON_OCCUPIED_MAX;\n\t}\n\n\tmvreg_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id),\n\t\t    (ndescs << MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT));\n}\n\n \nstatic int mvneta_rxq_busy_desc_num_get(struct mvneta_port *pp,\n\t\t\t\t\tstruct mvneta_rx_queue *rxq)\n{\n\tu32 val;\n\n\tval = mvreg_read(pp, MVNETA_RXQ_STATUS_REG(rxq->id));\n\treturn val & MVNETA_RXQ_OCCUPIED_ALL_MASK;\n}\n\n \nstatic void mvneta_rxq_desc_num_update(struct mvneta_port *pp,\n\t\t\t\t       struct mvneta_rx_queue *rxq,\n\t\t\t\t       int rx_done, int rx_filled)\n{\n\tu32 val;\n\n\tif ((rx_done <= 0xff) && (rx_filled <= 0xff)) {\n\t\tval = rx_done |\n\t\t  (rx_filled << MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT);\n\t\tmvreg_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id), val);\n\t\treturn;\n\t}\n\n\t \n\twhile ((rx_done > 0) || (rx_filled > 0)) {\n\t\tif (rx_done <= 0xff) {\n\t\t\tval = rx_done;\n\t\t\trx_done = 0;\n\t\t} else {\n\t\t\tval = 0xff;\n\t\t\trx_done -= 0xff;\n\t\t}\n\t\tif (rx_filled <= 0xff) {\n\t\t\tval |= rx_filled << MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT;\n\t\t\trx_filled = 0;\n\t\t} else {\n\t\t\tval |= 0xff << MVNETA_RXQ_ADD_NON_OCCUPIED_SHIFT;\n\t\t\trx_filled -= 0xff;\n\t\t}\n\t\tmvreg_write(pp, MVNETA_RXQ_STATUS_UPDATE_REG(rxq->id), val);\n\t}\n}\n\n \nstatic struct mvneta_rx_desc *\nmvneta_rxq_next_desc_get(struct mvneta_rx_queue *rxq)\n{\n\tint rx_desc = rxq->next_desc_to_proc;\n\n\trxq->next_desc_to_proc = MVNETA_QUEUE_NEXT_DESC(rxq, rx_desc);\n\tprefetch(rxq->descs + rxq->next_desc_to_proc);\n\treturn rxq->descs + rx_desc;\n}\n\n \nstatic void mvneta_max_rx_size_set(struct mvneta_port *pp, int max_rx_size)\n{\n\tu32 val;\n\n\tval =  mvreg_read(pp, MVNETA_GMAC_CTRL_0);\n\tval &= ~MVNETA_GMAC_MAX_RX_SIZE_MASK;\n\tval |= ((max_rx_size - MVNETA_MH_SIZE) / 2) <<\n\t\tMVNETA_GMAC_MAX_RX_SIZE_SHIFT;\n\tmvreg_write(pp, MVNETA_GMAC_CTRL_0, val);\n}\n\n\n \nstatic void mvneta_rxq_offset_set(struct mvneta_port *pp,\n\t\t\t\t  struct mvneta_rx_queue *rxq,\n\t\t\t\t  int offset)\n{\n\tu32 val;\n\n\tval = mvreg_read(pp, MVNETA_RXQ_CONFIG_REG(rxq->id));\n\tval &= ~MVNETA_RXQ_PKT_OFFSET_ALL_MASK;\n\n\t \n\tval |= MVNETA_RXQ_PKT_OFFSET_MASK(offset >> 3);\n\tmvreg_write(pp, MVNETA_RXQ_CONFIG_REG(rxq->id), val);\n}\n\n\n \n\n \nstatic void mvneta_txq_pend_desc_add(struct mvneta_port *pp,\n\t\t\t\t     struct mvneta_tx_queue *txq,\n\t\t\t\t     int pend_desc)\n{\n\tu32 val;\n\n\tpend_desc += txq->pending;\n\n\t \n\tdo {\n\t\tval = min(pend_desc, 255);\n\t\tmvreg_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);\n\t\tpend_desc -= val;\n\t} while (pend_desc > 0);\n\ttxq->pending = 0;\n}\n\n \nstatic struct mvneta_tx_desc *\nmvneta_txq_next_desc_get(struct mvneta_tx_queue *txq)\n{\n\tint tx_desc = txq->next_desc_to_proc;\n\n\ttxq->next_desc_to_proc = MVNETA_QUEUE_NEXT_DESC(txq, tx_desc);\n\treturn txq->descs + tx_desc;\n}\n\n \nstatic void mvneta_txq_desc_put(struct mvneta_tx_queue *txq)\n{\n\tif (txq->next_desc_to_proc == 0)\n\t\ttxq->next_desc_to_proc = txq->last_desc - 1;\n\telse\n\t\ttxq->next_desc_to_proc--;\n}\n\n \nstatic void mvneta_rxq_buf_size_set(struct mvneta_port *pp,\n\t\t\t\t    struct mvneta_rx_queue *rxq,\n\t\t\t\t    int buf_size)\n{\n\tu32 val;\n\n\tval = mvreg_read(pp, MVNETA_RXQ_SIZE_REG(rxq->id));\n\n\tval &= ~MVNETA_RXQ_BUF_SIZE_MASK;\n\tval |= ((buf_size >> 3) << MVNETA_RXQ_BUF_SIZE_SHIFT);\n\n\tmvreg_write(pp, MVNETA_RXQ_SIZE_REG(rxq->id), val);\n}\n\n \nstatic void mvneta_rxq_bm_disable(struct mvneta_port *pp,\n\t\t\t\t  struct mvneta_rx_queue *rxq)\n{\n\tu32 val;\n\n\tval = mvreg_read(pp, MVNETA_RXQ_CONFIG_REG(rxq->id));\n\tval &= ~MVNETA_RXQ_HW_BUF_ALLOC;\n\tmvreg_write(pp, MVNETA_RXQ_CONFIG_REG(rxq->id), val);\n}\n\n \nstatic void mvneta_rxq_bm_enable(struct mvneta_port *pp,\n\t\t\t\t struct mvneta_rx_queue *rxq)\n{\n\tu32 val;\n\n\tval = mvreg_read(pp, MVNETA_RXQ_CONFIG_REG(rxq->id));\n\tval |= MVNETA_RXQ_HW_BUF_ALLOC;\n\tmvreg_write(pp, MVNETA_RXQ_CONFIG_REG(rxq->id), val);\n}\n\n \nstatic void mvneta_rxq_long_pool_set(struct mvneta_port *pp,\n\t\t\t\t     struct mvneta_rx_queue *rxq)\n{\n\tu32 val;\n\n\tval = mvreg_read(pp, MVNETA_RXQ_CONFIG_REG(rxq->id));\n\tval &= ~MVNETA_RXQ_LONG_POOL_ID_MASK;\n\tval |= (pp->pool_long->id << MVNETA_RXQ_LONG_POOL_ID_SHIFT);\n\n\tmvreg_write(pp, MVNETA_RXQ_CONFIG_REG(rxq->id), val);\n}\n\n \nstatic void mvneta_rxq_short_pool_set(struct mvneta_port *pp,\n\t\t\t\t      struct mvneta_rx_queue *rxq)\n{\n\tu32 val;\n\n\tval = mvreg_read(pp, MVNETA_RXQ_CONFIG_REG(rxq->id));\n\tval &= ~MVNETA_RXQ_SHORT_POOL_ID_MASK;\n\tval |= (pp->pool_short->id << MVNETA_RXQ_SHORT_POOL_ID_SHIFT);\n\n\tmvreg_write(pp, MVNETA_RXQ_CONFIG_REG(rxq->id), val);\n}\n\n \nstatic inline void mvneta_bm_pool_bufsize_set(struct mvneta_port *pp,\n\t\t\t\t\t      int buf_size,\n\t\t\t\t\t      u8 pool_id)\n{\n\tu32 val;\n\n\tif (!IS_ALIGNED(buf_size, 8)) {\n\t\tdev_warn(pp->dev->dev.parent,\n\t\t\t \"illegal buf_size value %d, round to %d\\n\",\n\t\t\t buf_size, ALIGN(buf_size, 8));\n\t\tbuf_size = ALIGN(buf_size, 8);\n\t}\n\n\tval = mvreg_read(pp, MVNETA_PORT_POOL_BUFFER_SZ_REG(pool_id));\n\tval |= buf_size & MVNETA_PORT_POOL_BUFFER_SZ_MASK;\n\tmvreg_write(pp, MVNETA_PORT_POOL_BUFFER_SZ_REG(pool_id), val);\n}\n\n \nstatic int mvneta_mbus_io_win_set(struct mvneta_port *pp, u32 base, u32 wsize,\n\t\t\t\t  u8 target, u8 attr)\n{\n\tu32 win_enable, win_protect;\n\tint i;\n\n\twin_enable = mvreg_read(pp, MVNETA_BASE_ADDR_ENABLE);\n\n\tif (pp->bm_win_id < 0) {\n\t\t \n\t\tfor (i = 0; i < MVNETA_MAX_DECODE_WIN; i++) {\n\t\t\tif (win_enable & (1 << i)) {\n\t\t\t\tpp->bm_win_id = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == MVNETA_MAX_DECODE_WIN)\n\t\t\treturn -ENOMEM;\n\t} else {\n\t\ti = pp->bm_win_id;\n\t}\n\n\tmvreg_write(pp, MVNETA_WIN_BASE(i), 0);\n\tmvreg_write(pp, MVNETA_WIN_SIZE(i), 0);\n\n\tif (i < 4)\n\t\tmvreg_write(pp, MVNETA_WIN_REMAP(i), 0);\n\n\tmvreg_write(pp, MVNETA_WIN_BASE(i), (base & 0xffff0000) |\n\t\t    (attr << 8) | target);\n\n\tmvreg_write(pp, MVNETA_WIN_SIZE(i), (wsize - 1) & 0xffff0000);\n\n\twin_protect = mvreg_read(pp, MVNETA_ACCESS_PROTECT_ENABLE);\n\twin_protect |= 3 << (2 * i);\n\tmvreg_write(pp, MVNETA_ACCESS_PROTECT_ENABLE, win_protect);\n\n\twin_enable &= ~(1 << i);\n\tmvreg_write(pp, MVNETA_BASE_ADDR_ENABLE, win_enable);\n\n\treturn 0;\n}\n\nstatic int mvneta_bm_port_mbus_init(struct mvneta_port *pp)\n{\n\tu32 wsize;\n\tu8 target, attr;\n\tint err;\n\n\t \n\terr = mvebu_mbus_get_io_win_info(pp->bm_priv->bppi_phys_addr, &wsize,\n\t\t\t\t\t &target, &attr);\n\tif (err < 0)\n\t\treturn err;\n\n\tpp->bm_win_id = -1;\n\n\t \n\terr = mvneta_mbus_io_win_set(pp, pp->bm_priv->bppi_phys_addr, wsize,\n\t\t\t\t     target, attr);\n\tif (err < 0) {\n\t\tnetdev_info(pp->dev, \"fail to configure mbus window to BM\\n\");\n\t\treturn err;\n\t}\n\treturn 0;\n}\n\n \nstatic int mvneta_bm_port_init(struct platform_device *pdev,\n\t\t\t       struct mvneta_port *pp)\n{\n\tstruct device_node *dn = pdev->dev.of_node;\n\tu32 long_pool_id, short_pool_id;\n\n\tif (!pp->neta_armada3700) {\n\t\tint ret;\n\n\t\tret = mvneta_bm_port_mbus_init(pp);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (of_property_read_u32(dn, \"bm,pool-long\", &long_pool_id)) {\n\t\tnetdev_info(pp->dev, \"missing long pool id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tpp->pool_long = mvneta_bm_pool_use(pp->bm_priv, long_pool_id,\n\t\t\t\t\t   MVNETA_BM_LONG, pp->id,\n\t\t\t\t\t   MVNETA_RX_PKT_SIZE(pp->dev->mtu));\n\tif (!pp->pool_long) {\n\t\tnetdev_info(pp->dev, \"fail to obtain long pool for port\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpp->pool_long->port_map |= 1 << pp->id;\n\n\tmvneta_bm_pool_bufsize_set(pp, pp->pool_long->buf_size,\n\t\t\t\t   pp->pool_long->id);\n\n\t \n\tif (of_property_read_u32(dn, \"bm,pool-short\", &short_pool_id))\n\t\tshort_pool_id = long_pool_id;\n\n\t \n\tpp->pool_short = mvneta_bm_pool_use(pp->bm_priv, short_pool_id,\n\t\t\t\t\t    MVNETA_BM_SHORT, pp->id,\n\t\t\t\t\t    MVNETA_BM_SHORT_PKT_SIZE);\n\tif (!pp->pool_short) {\n\t\tnetdev_info(pp->dev, \"fail to obtain short pool for port\\n\");\n\t\tmvneta_bm_pool_destroy(pp->bm_priv, pp->pool_long, 1 << pp->id);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (short_pool_id != long_pool_id) {\n\t\tpp->pool_short->port_map |= 1 << pp->id;\n\t\tmvneta_bm_pool_bufsize_set(pp, pp->pool_short->buf_size,\n\t\t\t\t\t   pp->pool_short->id);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void mvneta_bm_update_mtu(struct mvneta_port *pp, int mtu)\n{\n\tstruct mvneta_bm_pool *bm_pool = pp->pool_long;\n\tstruct hwbm_pool *hwbm_pool = &bm_pool->hwbm_pool;\n\tint num;\n\n\t \n\tmvneta_bm_bufs_free(pp->bm_priv, bm_pool, 1 << pp->id);\n\tif (hwbm_pool->buf_num) {\n\t\tWARN(1, \"cannot free all buffers in pool %d\\n\",\n\t\t     bm_pool->id);\n\t\tgoto bm_mtu_err;\n\t}\n\n\tbm_pool->pkt_size = MVNETA_RX_PKT_SIZE(mtu);\n\tbm_pool->buf_size = MVNETA_RX_BUF_SIZE(bm_pool->pkt_size);\n\thwbm_pool->frag_size = SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) +\n\t\t\tSKB_DATA_ALIGN(MVNETA_RX_BUF_SIZE(bm_pool->pkt_size));\n\n\t \n\tnum = hwbm_pool_add(hwbm_pool, hwbm_pool->size);\n\tif (num != hwbm_pool->size) {\n\t\tWARN(1, \"pool %d: %d of %d allocated\\n\",\n\t\t     bm_pool->id, num, hwbm_pool->size);\n\t\tgoto bm_mtu_err;\n\t}\n\tmvneta_bm_pool_bufsize_set(pp, bm_pool->buf_size, bm_pool->id);\n\n\treturn;\n\nbm_mtu_err:\n\tmvneta_bm_pool_destroy(pp->bm_priv, pp->pool_long, 1 << pp->id);\n\tmvneta_bm_pool_destroy(pp->bm_priv, pp->pool_short, 1 << pp->id);\n\n\tpp->bm_priv = NULL;\n\tpp->rx_offset_correction = MVNETA_SKB_HEADROOM;\n\tmvreg_write(pp, MVNETA_ACC_MODE, MVNETA_ACC_MODE_EXT1);\n\tnetdev_info(pp->dev, \"fail to update MTU, fall back to software BM\\n\");\n}\n\n \nstatic void mvneta_port_up(struct mvneta_port *pp)\n{\n\tint queue;\n\tu32 q_map;\n\n\t \n\tq_map = 0;\n\tfor (queue = 0; queue < txq_number; queue++) {\n\t\tstruct mvneta_tx_queue *txq = &pp->txqs[queue];\n\t\tif (txq->descs)\n\t\t\tq_map |= (1 << queue);\n\t}\n\tmvreg_write(pp, MVNETA_TXQ_CMD, q_map);\n\n\tq_map = 0;\n\t \n\tfor (queue = 0; queue < rxq_number; queue++) {\n\t\tstruct mvneta_rx_queue *rxq = &pp->rxqs[queue];\n\n\t\tif (rxq->descs)\n\t\t\tq_map |= (1 << queue);\n\t}\n\tmvreg_write(pp, MVNETA_RXQ_CMD, q_map);\n}\n\n \nstatic void mvneta_port_down(struct mvneta_port *pp)\n{\n\tu32 val;\n\tint count;\n\n\t \n\tval = mvreg_read(pp, MVNETA_RXQ_CMD) & MVNETA_RXQ_ENABLE_MASK;\n\n\t \n\tif (val != 0)\n\t\tmvreg_write(pp, MVNETA_RXQ_CMD,\n\t\t\t    val << MVNETA_RXQ_DISABLE_SHIFT);\n\n\t \n\tcount = 0;\n\tdo {\n\t\tif (count++ >= MVNETA_RX_DISABLE_TIMEOUT_MSEC) {\n\t\t\tnetdev_warn(pp->dev,\n\t\t\t\t    \"TIMEOUT for RX stopped ! rx_queue_cmd: 0x%08x\\n\",\n\t\t\t\t    val);\n\t\t\tbreak;\n\t\t}\n\t\tmdelay(1);\n\n\t\tval = mvreg_read(pp, MVNETA_RXQ_CMD);\n\t} while (val & MVNETA_RXQ_ENABLE_MASK);\n\n\t \n\tval = (mvreg_read(pp, MVNETA_TXQ_CMD)) & MVNETA_TXQ_ENABLE_MASK;\n\n\tif (val != 0)\n\t\tmvreg_write(pp, MVNETA_TXQ_CMD,\n\t\t\t    (val << MVNETA_TXQ_DISABLE_SHIFT));\n\n\t \n\tcount = 0;\n\tdo {\n\t\tif (count++ >= MVNETA_TX_DISABLE_TIMEOUT_MSEC) {\n\t\t\tnetdev_warn(pp->dev,\n\t\t\t\t    \"TIMEOUT for TX stopped status=0x%08x\\n\",\n\t\t\t\t    val);\n\t\t\tbreak;\n\t\t}\n\t\tmdelay(1);\n\n\t\t \n\t\tval = mvreg_read(pp, MVNETA_TXQ_CMD);\n\n\t} while (val & MVNETA_TXQ_ENABLE_MASK);\n\n\t \n\tcount = 0;\n\tdo {\n\t\tif (count++ >= MVNETA_TX_FIFO_EMPTY_TIMEOUT) {\n\t\t\tnetdev_warn(pp->dev,\n\t\t\t\t    \"TX FIFO empty timeout status=0x%08x\\n\",\n\t\t\t\t    val);\n\t\t\tbreak;\n\t\t}\n\t\tmdelay(1);\n\n\t\tval = mvreg_read(pp, MVNETA_PORT_STATUS);\n\t} while (!(val & MVNETA_TX_FIFO_EMPTY) &&\n\t\t (val & MVNETA_TX_IN_PRGRS));\n\n\tudelay(200);\n}\n\n \nstatic void mvneta_port_enable(struct mvneta_port *pp)\n{\n\tu32 val;\n\n\t \n\tval = mvreg_read(pp, MVNETA_GMAC_CTRL_0);\n\tval |= MVNETA_GMAC0_PORT_ENABLE;\n\tmvreg_write(pp, MVNETA_GMAC_CTRL_0, val);\n}\n\n \nstatic void mvneta_port_disable(struct mvneta_port *pp)\n{\n\tu32 val;\n\n\t \n\tval = mvreg_read(pp, MVNETA_GMAC_CTRL_0);\n\tval &= ~MVNETA_GMAC0_PORT_ENABLE;\n\tmvreg_write(pp, MVNETA_GMAC_CTRL_0, val);\n\n\tudelay(200);\n}\n\n \n\n \nstatic void mvneta_set_ucast_table(struct mvneta_port *pp, int queue)\n{\n\tint offset;\n\tu32 val;\n\n\tif (queue == -1) {\n\t\tval = 0;\n\t} else {\n\t\tval = 0x1 | (queue << 1);\n\t\tval |= (val << 24) | (val << 16) | (val << 8);\n\t}\n\n\tfor (offset = 0; offset <= 0xc; offset += 4)\n\t\tmvreg_write(pp, MVNETA_DA_FILT_UCAST_BASE + offset, val);\n}\n\n \nstatic void mvneta_set_special_mcast_table(struct mvneta_port *pp, int queue)\n{\n\tint offset;\n\tu32 val;\n\n\tif (queue == -1) {\n\t\tval = 0;\n\t} else {\n\t\tval = 0x1 | (queue << 1);\n\t\tval |= (val << 24) | (val << 16) | (val << 8);\n\t}\n\n\tfor (offset = 0; offset <= 0xfc; offset += 4)\n\t\tmvreg_write(pp, MVNETA_DA_FILT_SPEC_MCAST + offset, val);\n\n}\n\n \nstatic void mvneta_set_other_mcast_table(struct mvneta_port *pp, int queue)\n{\n\tint offset;\n\tu32 val;\n\n\tif (queue == -1) {\n\t\tmemset(pp->mcast_count, 0, sizeof(pp->mcast_count));\n\t\tval = 0;\n\t} else {\n\t\tmemset(pp->mcast_count, 1, sizeof(pp->mcast_count));\n\t\tval = 0x1 | (queue << 1);\n\t\tval |= (val << 24) | (val << 16) | (val << 8);\n\t}\n\n\tfor (offset = 0; offset <= 0xfc; offset += 4)\n\t\tmvreg_write(pp, MVNETA_DA_FILT_OTH_MCAST + offset, val);\n}\n\nstatic void mvneta_percpu_unmask_interrupt(void *arg)\n{\n\tstruct mvneta_port *pp = arg;\n\n\t \n\tmvreg_write(pp, MVNETA_INTR_NEW_MASK,\n\t\t    MVNETA_RX_INTR_MASK_ALL |\n\t\t    MVNETA_TX_INTR_MASK_ALL |\n\t\t    MVNETA_MISCINTR_INTR_MASK);\n}\n\nstatic void mvneta_percpu_mask_interrupt(void *arg)\n{\n\tstruct mvneta_port *pp = arg;\n\n\t \n\tmvreg_write(pp, MVNETA_INTR_NEW_MASK, 0);\n\tmvreg_write(pp, MVNETA_INTR_OLD_MASK, 0);\n\tmvreg_write(pp, MVNETA_INTR_MISC_MASK, 0);\n}\n\nstatic void mvneta_percpu_clear_intr_cause(void *arg)\n{\n\tstruct mvneta_port *pp = arg;\n\n\t \n\tmvreg_write(pp, MVNETA_INTR_NEW_CAUSE, 0);\n\tmvreg_write(pp, MVNETA_INTR_MISC_CAUSE, 0);\n\tmvreg_write(pp, MVNETA_INTR_OLD_CAUSE, 0);\n}\n\n \nstatic void mvneta_defaults_set(struct mvneta_port *pp)\n{\n\tint cpu;\n\tint queue;\n\tu32 val;\n\tint max_cpu = num_present_cpus();\n\n\t \n\ton_each_cpu(mvneta_percpu_clear_intr_cause, pp, true);\n\n\t \n\ton_each_cpu(mvneta_percpu_mask_interrupt, pp, true);\n\tmvreg_write(pp, MVNETA_INTR_ENABLE, 0);\n\n\t \n\tmvreg_write(pp, MVNETA_MBUS_RETRY, 0x20);\n\n\t \n\tfor_each_present_cpu(cpu) {\n\t\tint rxq_map = 0, txq_map = 0;\n\t\tint rxq, txq;\n\t\tif (!pp->neta_armada3700) {\n\t\t\tfor (rxq = 0; rxq < rxq_number; rxq++)\n\t\t\t\tif ((rxq % max_cpu) == cpu)\n\t\t\t\t\trxq_map |= MVNETA_CPU_RXQ_ACCESS(rxq);\n\n\t\t\tfor (txq = 0; txq < txq_number; txq++)\n\t\t\t\tif ((txq % max_cpu) == cpu)\n\t\t\t\t\ttxq_map |= MVNETA_CPU_TXQ_ACCESS(txq);\n\n\t\t\t \n\t\t\tif (txq_number == 1)\n\t\t\t\ttxq_map = (cpu == pp->rxq_def) ?\n\t\t\t\t\tMVNETA_CPU_TXQ_ACCESS(0) : 0;\n\n\t\t} else {\n\t\t\ttxq_map = MVNETA_CPU_TXQ_ACCESS_ALL_MASK;\n\t\t\trxq_map = MVNETA_CPU_RXQ_ACCESS_ALL_MASK;\n\t\t}\n\n\t\tmvreg_write(pp, MVNETA_CPU_MAP(cpu), rxq_map | txq_map);\n\t}\n\n\t \n\tmvreg_write(pp, MVNETA_PORT_RX_RESET, MVNETA_PORT_RX_DMA_RESET);\n\tmvreg_write(pp, MVNETA_PORT_TX_RESET, MVNETA_PORT_TX_DMA_RESET);\n\n\t \n\tmvreg_write(pp, MVNETA_TXQ_CMD_1, 0);\n\tfor (queue = 0; queue < txq_number; queue++) {\n\t\tmvreg_write(pp, MVETH_TXQ_TOKEN_COUNT_REG(queue), 0);\n\t\tmvreg_write(pp, MVETH_TXQ_TOKEN_CFG_REG(queue), 0);\n\t}\n\n\tmvreg_write(pp, MVNETA_PORT_TX_RESET, 0);\n\tmvreg_write(pp, MVNETA_PORT_RX_RESET, 0);\n\n\t \n\tif (pp->bm_priv)\n\t\t \n\t\tval = MVNETA_ACC_MODE_EXT2;\n\telse\n\t\t \n\t\tval = MVNETA_ACC_MODE_EXT1;\n\tmvreg_write(pp, MVNETA_ACC_MODE, val);\n\n\tif (pp->bm_priv)\n\t\tmvreg_write(pp, MVNETA_BM_ADDRESS, pp->bm_priv->bppi_phys_addr);\n\n\t \n\tval = MVNETA_PORT_CONFIG_DEFL_VALUE(pp->rxq_def);\n\tmvreg_write(pp, MVNETA_PORT_CONFIG, val);\n\n\tval = 0;\n\tmvreg_write(pp, MVNETA_PORT_CONFIG_EXTEND, val);\n\tmvreg_write(pp, MVNETA_RX_MIN_FRAME_SIZE, 64);\n\n\t \n\tval = 0;\n\n\t \n\tval |= MVNETA_TX_BRST_SZ_MASK(MVNETA_SDMA_BRST_SIZE_16);\n\tval |= MVNETA_RX_BRST_SZ_MASK(MVNETA_SDMA_BRST_SIZE_16);\n\tval |= MVNETA_RX_NO_DATA_SWAP | MVNETA_TX_NO_DATA_SWAP;\n\n#if defined(__BIG_ENDIAN)\n\tval |= MVNETA_DESC_SWAP;\n#endif\n\n\t \n\tmvreg_write(pp, MVNETA_SDMA_CONFIG, val);\n\n\t \n\tval = mvreg_read(pp, MVNETA_UNIT_CONTROL);\n\tval &= ~MVNETA_PHY_POLLING_ENABLE;\n\tmvreg_write(pp, MVNETA_UNIT_CONTROL, val);\n\n\tmvneta_set_ucast_table(pp, -1);\n\tmvneta_set_special_mcast_table(pp, -1);\n\tmvneta_set_other_mcast_table(pp, -1);\n\n\t \n\tmvreg_write(pp, MVNETA_INTR_ENABLE,\n\t\t    (MVNETA_RXQ_INTR_ENABLE_ALL_MASK\n\t\t     | MVNETA_TXQ_INTR_ENABLE_ALL_MASK));\n\n\tmvneta_mib_counters_clear(pp);\n}\n\n \nstatic void mvneta_txq_max_tx_size_set(struct mvneta_port *pp, int max_tx_size)\n\n{\n\tu32 val, size, mtu;\n\tint queue;\n\n\tmtu = max_tx_size * 8;\n\tif (mtu > MVNETA_TX_MTU_MAX)\n\t\tmtu = MVNETA_TX_MTU_MAX;\n\n\t \n\tval = mvreg_read(pp, MVNETA_TX_MTU);\n\tval &= ~MVNETA_TX_MTU_MAX;\n\tval |= mtu;\n\tmvreg_write(pp, MVNETA_TX_MTU, val);\n\n\t \n\tval = mvreg_read(pp, MVNETA_TX_TOKEN_SIZE);\n\n\tsize = val & MVNETA_TX_TOKEN_SIZE_MAX;\n\tif (size < mtu) {\n\t\tsize = mtu;\n\t\tval &= ~MVNETA_TX_TOKEN_SIZE_MAX;\n\t\tval |= size;\n\t\tmvreg_write(pp, MVNETA_TX_TOKEN_SIZE, val);\n\t}\n\tfor (queue = 0; queue < txq_number; queue++) {\n\t\tval = mvreg_read(pp, MVNETA_TXQ_TOKEN_SIZE_REG(queue));\n\n\t\tsize = val & MVNETA_TXQ_TOKEN_SIZE_MAX;\n\t\tif (size < mtu) {\n\t\t\tsize = mtu;\n\t\t\tval &= ~MVNETA_TXQ_TOKEN_SIZE_MAX;\n\t\t\tval |= size;\n\t\t\tmvreg_write(pp, MVNETA_TXQ_TOKEN_SIZE_REG(queue), val);\n\t\t}\n\t}\n}\n\n \nstatic void mvneta_set_ucast_addr(struct mvneta_port *pp, u8 last_nibble,\n\t\t\t\t  int queue)\n{\n\tunsigned int unicast_reg;\n\tunsigned int tbl_offset;\n\tunsigned int reg_offset;\n\n\t \n\tlast_nibble = (0xf & last_nibble);\n\n\t \n\ttbl_offset = (last_nibble / 4) * 4;\n\n\t \n\treg_offset = last_nibble % 4;\n\n\tunicast_reg = mvreg_read(pp, (MVNETA_DA_FILT_UCAST_BASE + tbl_offset));\n\n\tif (queue == -1) {\n\t\t \n\t\tunicast_reg &= ~(0xff << (8 * reg_offset));\n\t} else {\n\t\tunicast_reg &= ~(0xff << (8 * reg_offset));\n\t\tunicast_reg |= ((0x01 | (queue << 1)) << (8 * reg_offset));\n\t}\n\n\tmvreg_write(pp, (MVNETA_DA_FILT_UCAST_BASE + tbl_offset), unicast_reg);\n}\n\n \nstatic void mvneta_mac_addr_set(struct mvneta_port *pp,\n\t\t\t\tconst unsigned char *addr, int queue)\n{\n\tunsigned int mac_h;\n\tunsigned int mac_l;\n\n\tif (queue != -1) {\n\t\tmac_l = (addr[4] << 8) | (addr[5]);\n\t\tmac_h = (addr[0] << 24) | (addr[1] << 16) |\n\t\t\t(addr[2] << 8) | (addr[3] << 0);\n\n\t\tmvreg_write(pp, MVNETA_MAC_ADDR_LOW, mac_l);\n\t\tmvreg_write(pp, MVNETA_MAC_ADDR_HIGH, mac_h);\n\t}\n\n\t \n\tmvneta_set_ucast_addr(pp, addr[5], queue);\n}\n\n \nstatic void mvneta_rx_pkts_coal_set(struct mvneta_port *pp,\n\t\t\t\t    struct mvneta_rx_queue *rxq, u32 value)\n{\n\tmvreg_write(pp, MVNETA_RXQ_THRESHOLD_REG(rxq->id),\n\t\t    value | MVNETA_RXQ_NON_OCCUPIED(0));\n}\n\n \nstatic void mvneta_rx_time_coal_set(struct mvneta_port *pp,\n\t\t\t\t    struct mvneta_rx_queue *rxq, u32 value)\n{\n\tu32 val;\n\tunsigned long clk_rate;\n\n\tclk_rate = clk_get_rate(pp->clk);\n\tval = (clk_rate / 1000000) * value;\n\n\tmvreg_write(pp, MVNETA_RXQ_TIME_COAL_REG(rxq->id), val);\n}\n\n \nstatic void mvneta_tx_done_pkts_coal_set(struct mvneta_port *pp,\n\t\t\t\t\t struct mvneta_tx_queue *txq, u32 value)\n{\n\tu32 val;\n\n\tval = mvreg_read(pp, MVNETA_TXQ_SIZE_REG(txq->id));\n\n\tval &= ~MVNETA_TXQ_SENT_THRESH_ALL_MASK;\n\tval |= MVNETA_TXQ_SENT_THRESH_MASK(value);\n\n\tmvreg_write(pp, MVNETA_TXQ_SIZE_REG(txq->id), val);\n}\n\n \nstatic void mvneta_rx_desc_fill(struct mvneta_rx_desc *rx_desc,\n\t\t\t\tu32 phys_addr, void *virt_addr,\n\t\t\t\tstruct mvneta_rx_queue *rxq)\n{\n\tint i;\n\n\trx_desc->buf_phys_addr = phys_addr;\n\ti = rx_desc - rxq->descs;\n\trxq->buf_virt_addr[i] = virt_addr;\n}\n\n \nstatic void mvneta_txq_sent_desc_dec(struct mvneta_port *pp,\n\t\t\t\t     struct mvneta_tx_queue *txq,\n\t\t\t\t     int sent_desc)\n{\n\tu32 val;\n\n\t \n\twhile (sent_desc > 0xff) {\n\t\tval = 0xff << MVNETA_TXQ_DEC_SENT_SHIFT;\n\t\tmvreg_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);\n\t\tsent_desc = sent_desc - 0xff;\n\t}\n\n\tval = sent_desc << MVNETA_TXQ_DEC_SENT_SHIFT;\n\tmvreg_write(pp, MVNETA_TXQ_UPDATE_REG(txq->id), val);\n}\n\n \nstatic int mvneta_txq_sent_desc_num_get(struct mvneta_port *pp,\n\t\t\t\t\tstruct mvneta_tx_queue *txq)\n{\n\tu32 val;\n\tint sent_desc;\n\n\tval = mvreg_read(pp, MVNETA_TXQ_STATUS_REG(txq->id));\n\tsent_desc = (val & MVNETA_TXQ_SENT_DESC_MASK) >>\n\t\tMVNETA_TXQ_SENT_DESC_SHIFT;\n\n\treturn sent_desc;\n}\n\n \nstatic int mvneta_txq_sent_desc_proc(struct mvneta_port *pp,\n\t\t\t\t     struct mvneta_tx_queue *txq)\n{\n\tint sent_desc;\n\n\t \n\tsent_desc = mvneta_txq_sent_desc_num_get(pp, txq);\n\n\t \n\tif (sent_desc)\n\t\tmvneta_txq_sent_desc_dec(pp, txq, sent_desc);\n\n\treturn sent_desc;\n}\n\n \nstatic u32 mvneta_txq_desc_csum(int l3_offs, int l3_proto,\n\t\t\t\tint ip_hdr_len, int l4_proto)\n{\n\tu32 command;\n\n\t \n\tcommand =  l3_offs    << MVNETA_TX_L3_OFF_SHIFT;\n\tcommand |= ip_hdr_len << MVNETA_TX_IP_HLEN_SHIFT;\n\n\tif (l3_proto == htons(ETH_P_IP))\n\t\tcommand |= MVNETA_TXD_IP_CSUM;\n\telse\n\t\tcommand |= MVNETA_TX_L3_IP6;\n\n\tif (l4_proto == IPPROTO_TCP)\n\t\tcommand |=  MVNETA_TX_L4_CSUM_FULL;\n\telse if (l4_proto == IPPROTO_UDP)\n\t\tcommand |= MVNETA_TX_L4_UDP | MVNETA_TX_L4_CSUM_FULL;\n\telse\n\t\tcommand |= MVNETA_TX_L4_CSUM_NOT;\n\n\treturn command;\n}\n\n\n \nstatic void mvneta_rx_error(struct mvneta_port *pp,\n\t\t\t    struct mvneta_rx_desc *rx_desc)\n{\n\tstruct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);\n\tu32 status = rx_desc->status;\n\n\t \n\tu64_stats_update_begin(&stats->syncp);\n\tstats->rx_errors++;\n\tu64_stats_update_end(&stats->syncp);\n\n\tswitch (status & MVNETA_RXD_ERR_CODE_MASK) {\n\tcase MVNETA_RXD_ERR_CRC:\n\t\tnetdev_err(pp->dev, \"bad rx status %08x (crc error), size=%d\\n\",\n\t\t\t   status, rx_desc->data_size);\n\t\tbreak;\n\tcase MVNETA_RXD_ERR_OVERRUN:\n\t\tnetdev_err(pp->dev, \"bad rx status %08x (overrun error), size=%d\\n\",\n\t\t\t   status, rx_desc->data_size);\n\t\tbreak;\n\tcase MVNETA_RXD_ERR_LEN:\n\t\tnetdev_err(pp->dev, \"bad rx status %08x (max frame length error), size=%d\\n\",\n\t\t\t   status, rx_desc->data_size);\n\t\tbreak;\n\tcase MVNETA_RXD_ERR_RESOURCE:\n\t\tnetdev_err(pp->dev, \"bad rx status %08x (resource error), size=%d\\n\",\n\t\t\t   status, rx_desc->data_size);\n\t\tbreak;\n\t}\n}\n\n \nstatic int mvneta_rx_csum(struct mvneta_port *pp, u32 status)\n{\n\tif ((pp->dev->features & NETIF_F_RXCSUM) &&\n\t    (status & MVNETA_RXD_L3_IP4) &&\n\t    (status & MVNETA_RXD_L4_CSUM_OK))\n\t\treturn CHECKSUM_UNNECESSARY;\n\n\treturn CHECKSUM_NONE;\n}\n\n \nstatic struct mvneta_tx_queue *mvneta_tx_done_policy(struct mvneta_port *pp,\n\t\t\t\t\t\t     u32 cause)\n{\n\tint queue = fls(cause) - 1;\n\n\treturn &pp->txqs[queue];\n}\n\n \nstatic void mvneta_txq_bufs_free(struct mvneta_port *pp,\n\t\t\t\t struct mvneta_tx_queue *txq, int num,\n\t\t\t\t struct netdev_queue *nq, bool napi)\n{\n\tunsigned int bytes_compl = 0, pkts_compl = 0;\n\tstruct xdp_frame_bulk bq;\n\tint i;\n\n\txdp_frame_bulk_init(&bq);\n\n\trcu_read_lock();  \n\n\tfor (i = 0; i < num; i++) {\n\t\tstruct mvneta_tx_buf *buf = &txq->buf[txq->txq_get_index];\n\t\tstruct mvneta_tx_desc *tx_desc = txq->descs +\n\t\t\ttxq->txq_get_index;\n\n\t\tmvneta_txq_inc_get(txq);\n\n\t\tif (buf->type == MVNETA_TYPE_XDP_NDO ||\n\t\t    buf->type == MVNETA_TYPE_SKB)\n\t\t\tdma_unmap_single(pp->dev->dev.parent,\n\t\t\t\t\t tx_desc->buf_phys_addr,\n\t\t\t\t\t tx_desc->data_size, DMA_TO_DEVICE);\n\t\tif ((buf->type == MVNETA_TYPE_TSO ||\n\t\t     buf->type == MVNETA_TYPE_SKB) && buf->skb) {\n\t\t\tbytes_compl += buf->skb->len;\n\t\t\tpkts_compl++;\n\t\t\tdev_kfree_skb_any(buf->skb);\n\t\t} else if ((buf->type == MVNETA_TYPE_XDP_TX ||\n\t\t\t    buf->type == MVNETA_TYPE_XDP_NDO) && buf->xdpf) {\n\t\t\tif (napi && buf->type == MVNETA_TYPE_XDP_TX)\n\t\t\t\txdp_return_frame_rx_napi(buf->xdpf);\n\t\t\telse\n\t\t\t\txdp_return_frame_bulk(buf->xdpf, &bq);\n\t\t}\n\t}\n\txdp_flush_frame_bulk(&bq);\n\n\trcu_read_unlock();\n\n\tnetdev_tx_completed_queue(nq, pkts_compl, bytes_compl);\n}\n\n \nstatic void mvneta_txq_done(struct mvneta_port *pp,\n\t\t\t   struct mvneta_tx_queue *txq)\n{\n\tstruct netdev_queue *nq = netdev_get_tx_queue(pp->dev, txq->id);\n\tint tx_done;\n\n\ttx_done = mvneta_txq_sent_desc_proc(pp, txq);\n\tif (!tx_done)\n\t\treturn;\n\n\tmvneta_txq_bufs_free(pp, txq, tx_done, nq, true);\n\n\ttxq->count -= tx_done;\n\n\tif (netif_tx_queue_stopped(nq)) {\n\t\tif (txq->count <= txq->tx_wake_threshold)\n\t\t\tnetif_tx_wake_queue(nq);\n\t}\n}\n\n \n \nstatic int mvneta_rx_refill(struct mvneta_port *pp,\n\t\t\t    struct mvneta_rx_desc *rx_desc,\n\t\t\t    struct mvneta_rx_queue *rxq,\n\t\t\t    gfp_t gfp_mask)\n{\n\tdma_addr_t phys_addr;\n\tstruct page *page;\n\n\tpage = page_pool_alloc_pages(rxq->page_pool,\n\t\t\t\t     gfp_mask | __GFP_NOWARN);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tphys_addr = page_pool_get_dma_addr(page) + pp->rx_offset_correction;\n\tmvneta_rx_desc_fill(rx_desc, phys_addr, page, rxq);\n\n\treturn 0;\n}\n\n \nstatic u32 mvneta_skb_tx_csum(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tint ip_hdr_len = 0;\n\t\t__be16 l3_proto = vlan_get_protocol(skb);\n\t\tu8 l4_proto;\n\n\t\tif (l3_proto == htons(ETH_P_IP)) {\n\t\t\tstruct iphdr *ip4h = ip_hdr(skb);\n\n\t\t\t \n\t\t\tip_hdr_len = ip4h->ihl;\n\t\t\tl4_proto = ip4h->protocol;\n\t\t} else if (l3_proto == htons(ETH_P_IPV6)) {\n\t\t\tstruct ipv6hdr *ip6h = ipv6_hdr(skb);\n\n\t\t\t \n\t\t\tif (skb_network_header_len(skb) > 0)\n\t\t\t\tip_hdr_len = (skb_network_header_len(skb) >> 2);\n\t\t\tl4_proto = ip6h->nexthdr;\n\t\t} else\n\t\t\treturn MVNETA_TX_L4_CSUM_NOT;\n\n\t\treturn mvneta_txq_desc_csum(skb_network_offset(skb),\n\t\t\t\t\t    l3_proto, ip_hdr_len, l4_proto);\n\t}\n\n\treturn MVNETA_TX_L4_CSUM_NOT;\n}\n\n \nstatic void mvneta_rxq_drop_pkts(struct mvneta_port *pp,\n\t\t\t\t struct mvneta_rx_queue *rxq)\n{\n\tint rx_done, i;\n\n\trx_done = mvneta_rxq_busy_desc_num_get(pp, rxq);\n\tif (rx_done)\n\t\tmvneta_rxq_desc_num_update(pp, rxq, rx_done, rx_done);\n\n\tif (pp->bm_priv) {\n\t\tfor (i = 0; i < rx_done; i++) {\n\t\t\tstruct mvneta_rx_desc *rx_desc =\n\t\t\t\t\t\t  mvneta_rxq_next_desc_get(rxq);\n\t\t\tu8 pool_id = MVNETA_RX_GET_BM_POOL_ID(rx_desc);\n\t\t\tstruct mvneta_bm_pool *bm_pool;\n\n\t\t\tbm_pool = &pp->bm_priv->bm_pools[pool_id];\n\t\t\t \n\t\t\tmvneta_bm_pool_put_bp(pp->bm_priv, bm_pool,\n\t\t\t\t\t      rx_desc->buf_phys_addr);\n\t\t}\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < rxq->size; i++) {\n\t\tstruct mvneta_rx_desc *rx_desc = rxq->descs + i;\n\t\tvoid *data = rxq->buf_virt_addr[i];\n\t\tif (!data || !(rx_desc->buf_phys_addr))\n\t\t\tcontinue;\n\n\t\tpage_pool_put_full_page(rxq->page_pool, data, false);\n\t}\n\tif (xdp_rxq_info_is_reg(&rxq->xdp_rxq))\n\t\txdp_rxq_info_unreg(&rxq->xdp_rxq);\n\tpage_pool_destroy(rxq->page_pool);\n\trxq->page_pool = NULL;\n}\n\nstatic void\nmvneta_update_stats(struct mvneta_port *pp,\n\t\t    struct mvneta_stats *ps)\n{\n\tstruct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);\n\n\tu64_stats_update_begin(&stats->syncp);\n\tstats->es.ps.rx_packets += ps->rx_packets;\n\tstats->es.ps.rx_bytes += ps->rx_bytes;\n\t \n\tstats->es.ps.xdp_redirect += ps->xdp_redirect;\n\tstats->es.ps.xdp_pass += ps->xdp_pass;\n\tstats->es.ps.xdp_drop += ps->xdp_drop;\n\tu64_stats_update_end(&stats->syncp);\n}\n\nstatic inline\nint mvneta_rx_refill_queue(struct mvneta_port *pp, struct mvneta_rx_queue *rxq)\n{\n\tstruct mvneta_rx_desc *rx_desc;\n\tint curr_desc = rxq->first_to_refill;\n\tint i;\n\n\tfor (i = 0; (i < rxq->refill_num) && (i < 64); i++) {\n\t\trx_desc = rxq->descs + curr_desc;\n\t\tif (!(rx_desc->buf_phys_addr)) {\n\t\t\tif (mvneta_rx_refill(pp, rx_desc, rxq, GFP_ATOMIC)) {\n\t\t\t\tstruct mvneta_pcpu_stats *stats;\n\n\t\t\t\tpr_err(\"Can't refill queue %d. Done %d from %d\\n\",\n\t\t\t\t       rxq->id, i, rxq->refill_num);\n\n\t\t\t\tstats = this_cpu_ptr(pp->stats);\n\t\t\t\tu64_stats_update_begin(&stats->syncp);\n\t\t\t\tstats->es.refill_error++;\n\t\t\t\tu64_stats_update_end(&stats->syncp);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tcurr_desc = MVNETA_QUEUE_NEXT_DESC(rxq, curr_desc);\n\t}\n\trxq->refill_num -= i;\n\trxq->first_to_refill = curr_desc;\n\n\treturn i;\n}\n\nstatic void\nmvneta_xdp_put_buff(struct mvneta_port *pp, struct mvneta_rx_queue *rxq,\n\t\t    struct xdp_buff *xdp, int sync_len)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\tint i;\n\n\tif (likely(!xdp_buff_has_frags(xdp)))\n\t\tgoto out;\n\n\tfor (i = 0; i < sinfo->nr_frags; i++)\n\t\tpage_pool_put_full_page(rxq->page_pool,\n\t\t\t\t\tskb_frag_page(&sinfo->frags[i]), true);\n\nout:\n\tpage_pool_put_page(rxq->page_pool, virt_to_head_page(xdp->data),\n\t\t\t   sync_len, true);\n}\n\nstatic int\nmvneta_xdp_submit_frame(struct mvneta_port *pp, struct mvneta_tx_queue *txq,\n\t\t\tstruct xdp_frame *xdpf, int *nxmit_byte, bool dma_map)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_frame(xdpf);\n\tstruct device *dev = pp->dev->dev.parent;\n\tstruct mvneta_tx_desc *tx_desc;\n\tint i, num_frames = 1;\n\tstruct page *page;\n\n\tif (unlikely(xdp_frame_has_frags(xdpf)))\n\t\tnum_frames += sinfo->nr_frags;\n\n\tif (txq->count + num_frames >= txq->size)\n\t\treturn MVNETA_XDP_DROPPED;\n\n\tfor (i = 0; i < num_frames; i++) {\n\t\tstruct mvneta_tx_buf *buf = &txq->buf[txq->txq_put_index];\n\t\tskb_frag_t *frag = NULL;\n\t\tint len = xdpf->len;\n\t\tdma_addr_t dma_addr;\n\n\t\tif (unlikely(i)) {  \n\t\t\tfrag = &sinfo->frags[i - 1];\n\t\t\tlen = skb_frag_size(frag);\n\t\t}\n\n\t\ttx_desc = mvneta_txq_next_desc_get(txq);\n\t\tif (dma_map) {\n\t\t\t \n\t\t\tvoid *data;\n\n\t\t\tdata = unlikely(frag) ? skb_frag_address(frag)\n\t\t\t\t\t      : xdpf->data;\n\t\t\tdma_addr = dma_map_single(dev, data, len,\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\t\tmvneta_txq_desc_put(txq);\n\t\t\t\tgoto unmap;\n\t\t\t}\n\n\t\t\tbuf->type = MVNETA_TYPE_XDP_NDO;\n\t\t} else {\n\t\t\tpage = unlikely(frag) ? skb_frag_page(frag)\n\t\t\t\t\t      : virt_to_page(xdpf->data);\n\t\t\tdma_addr = page_pool_get_dma_addr(page);\n\t\t\tif (unlikely(frag))\n\t\t\t\tdma_addr += skb_frag_off(frag);\n\t\t\telse\n\t\t\t\tdma_addr += sizeof(*xdpf) + xdpf->headroom;\n\t\t\tdma_sync_single_for_device(dev, dma_addr, len,\n\t\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\t\tbuf->type = MVNETA_TYPE_XDP_TX;\n\t\t}\n\t\tbuf->xdpf = unlikely(i) ? NULL : xdpf;\n\n\t\ttx_desc->command = unlikely(i) ? 0 : MVNETA_TXD_F_DESC;\n\t\ttx_desc->buf_phys_addr = dma_addr;\n\t\ttx_desc->data_size = len;\n\t\t*nxmit_byte += len;\n\n\t\tmvneta_txq_inc_put(txq);\n\t}\n\t \n\ttx_desc->command |= MVNETA_TXD_L_DESC | MVNETA_TXD_Z_PAD;\n\n\ttxq->pending += num_frames;\n\ttxq->count += num_frames;\n\n\treturn MVNETA_XDP_TX;\n\nunmap:\n\tfor (i--; i >= 0; i--) {\n\t\tmvneta_txq_desc_put(txq);\n\t\ttx_desc = txq->descs + txq->next_desc_to_proc;\n\t\tdma_unmap_single(dev, tx_desc->buf_phys_addr,\n\t\t\t\t tx_desc->data_size,\n\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\treturn MVNETA_XDP_DROPPED;\n}\n\nstatic int\nmvneta_xdp_xmit_back(struct mvneta_port *pp, struct xdp_buff *xdp)\n{\n\tstruct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);\n\tstruct mvneta_tx_queue *txq;\n\tstruct netdev_queue *nq;\n\tint cpu, nxmit_byte = 0;\n\tstruct xdp_frame *xdpf;\n\tu32 ret;\n\n\txdpf = xdp_convert_buff_to_frame(xdp);\n\tif (unlikely(!xdpf))\n\t\treturn MVNETA_XDP_DROPPED;\n\n\tcpu = smp_processor_id();\n\ttxq = &pp->txqs[cpu % txq_number];\n\tnq = netdev_get_tx_queue(pp->dev, txq->id);\n\n\t__netif_tx_lock(nq, cpu);\n\tret = mvneta_xdp_submit_frame(pp, txq, xdpf, &nxmit_byte, false);\n\tif (ret == MVNETA_XDP_TX) {\n\t\tu64_stats_update_begin(&stats->syncp);\n\t\tstats->es.ps.tx_bytes += nxmit_byte;\n\t\tstats->es.ps.tx_packets++;\n\t\tstats->es.ps.xdp_tx++;\n\t\tu64_stats_update_end(&stats->syncp);\n\n\t\tmvneta_txq_pend_desc_add(pp, txq, 0);\n\t} else {\n\t\tu64_stats_update_begin(&stats->syncp);\n\t\tstats->es.ps.xdp_tx_err++;\n\t\tu64_stats_update_end(&stats->syncp);\n\t}\n\t__netif_tx_unlock(nq);\n\n\treturn ret;\n}\n\nstatic int\nmvneta_xdp_xmit(struct net_device *dev, int num_frame,\n\t\tstruct xdp_frame **frames, u32 flags)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tstruct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);\n\tint i, nxmit_byte = 0, nxmit = 0;\n\tint cpu = smp_processor_id();\n\tstruct mvneta_tx_queue *txq;\n\tstruct netdev_queue *nq;\n\tu32 ret;\n\n\tif (unlikely(test_bit(__MVNETA_DOWN, &pp->state)))\n\t\treturn -ENETDOWN;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\ttxq = &pp->txqs[cpu % txq_number];\n\tnq = netdev_get_tx_queue(pp->dev, txq->id);\n\n\t__netif_tx_lock(nq, cpu);\n\tfor (i = 0; i < num_frame; i++) {\n\t\tret = mvneta_xdp_submit_frame(pp, txq, frames[i], &nxmit_byte,\n\t\t\t\t\t      true);\n\t\tif (ret != MVNETA_XDP_TX)\n\t\t\tbreak;\n\n\t\tnxmit++;\n\t}\n\n\tif (unlikely(flags & XDP_XMIT_FLUSH))\n\t\tmvneta_txq_pend_desc_add(pp, txq, 0);\n\t__netif_tx_unlock(nq);\n\n\tu64_stats_update_begin(&stats->syncp);\n\tstats->es.ps.tx_bytes += nxmit_byte;\n\tstats->es.ps.tx_packets += nxmit;\n\tstats->es.ps.xdp_xmit += nxmit;\n\tstats->es.ps.xdp_xmit_err += num_frame - nxmit;\n\tu64_stats_update_end(&stats->syncp);\n\n\treturn nxmit;\n}\n\nstatic int\nmvneta_run_xdp(struct mvneta_port *pp, struct mvneta_rx_queue *rxq,\n\t       struct bpf_prog *prog, struct xdp_buff *xdp,\n\t       u32 frame_sz, struct mvneta_stats *stats)\n{\n\tunsigned int len, data_len, sync;\n\tu32 ret, act;\n\n\tlen = xdp->data_end - xdp->data_hard_start - pp->rx_offset_correction;\n\tdata_len = xdp->data_end - xdp->data;\n\tact = bpf_prog_run_xdp(prog, xdp);\n\n\t \n\tsync = xdp->data_end - xdp->data_hard_start - pp->rx_offset_correction;\n\tsync = max(sync, len);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tstats->xdp_pass++;\n\t\treturn MVNETA_XDP_PASS;\n\tcase XDP_REDIRECT: {\n\t\tint err;\n\n\t\terr = xdp_do_redirect(pp->dev, xdp, prog);\n\t\tif (unlikely(err)) {\n\t\t\tmvneta_xdp_put_buff(pp, rxq, xdp, sync);\n\t\t\tret = MVNETA_XDP_DROPPED;\n\t\t} else {\n\t\t\tret = MVNETA_XDP_REDIR;\n\t\t\tstats->xdp_redirect++;\n\t\t}\n\t\tbreak;\n\t}\n\tcase XDP_TX:\n\t\tret = mvneta_xdp_xmit_back(pp, xdp);\n\t\tif (ret != MVNETA_XDP_TX)\n\t\t\tmvneta_xdp_put_buff(pp, rxq, xdp, sync);\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(pp->dev, prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(pp->dev, prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tmvneta_xdp_put_buff(pp, rxq, xdp, sync);\n\t\tret = MVNETA_XDP_DROPPED;\n\t\tstats->xdp_drop++;\n\t\tbreak;\n\t}\n\n\tstats->rx_bytes += frame_sz + xdp->data_end - xdp->data - data_len;\n\tstats->rx_packets++;\n\n\treturn ret;\n}\n\nstatic void\nmvneta_swbm_rx_frame(struct mvneta_port *pp,\n\t\t     struct mvneta_rx_desc *rx_desc,\n\t\t     struct mvneta_rx_queue *rxq,\n\t\t     struct xdp_buff *xdp, int *size,\n\t\t     struct page *page)\n{\n\tunsigned char *data = page_address(page);\n\tint data_len = -MVNETA_MH_SIZE, len;\n\tstruct net_device *dev = pp->dev;\n\tenum dma_data_direction dma_dir;\n\n\tif (*size > MVNETA_MAX_RX_BUF_SIZE) {\n\t\tlen = MVNETA_MAX_RX_BUF_SIZE;\n\t\tdata_len += len;\n\t} else {\n\t\tlen = *size;\n\t\tdata_len += len - ETH_FCS_LEN;\n\t}\n\t*size = *size - len;\n\n\tdma_dir = page_pool_get_dma_dir(rxq->page_pool);\n\tdma_sync_single_for_cpu(dev->dev.parent,\n\t\t\t\trx_desc->buf_phys_addr,\n\t\t\t\tlen, dma_dir);\n\n\trx_desc->buf_phys_addr = 0;\n\n\t \n\tprefetch(data);\n\txdp_buff_clear_frags_flag(xdp);\n\txdp_prepare_buff(xdp, data, pp->rx_offset_correction + MVNETA_MH_SIZE,\n\t\t\t data_len, false);\n}\n\nstatic void\nmvneta_swbm_add_rx_fragment(struct mvneta_port *pp,\n\t\t\t    struct mvneta_rx_desc *rx_desc,\n\t\t\t    struct mvneta_rx_queue *rxq,\n\t\t\t    struct xdp_buff *xdp, int *size,\n\t\t\t    struct page *page)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\tstruct net_device *dev = pp->dev;\n\tenum dma_data_direction dma_dir;\n\tint data_len, len;\n\n\tif (*size > MVNETA_MAX_RX_BUF_SIZE) {\n\t\tlen = MVNETA_MAX_RX_BUF_SIZE;\n\t\tdata_len = len;\n\t} else {\n\t\tlen = *size;\n\t\tdata_len = len - ETH_FCS_LEN;\n\t}\n\tdma_dir = page_pool_get_dma_dir(rxq->page_pool);\n\tdma_sync_single_for_cpu(dev->dev.parent,\n\t\t\t\trx_desc->buf_phys_addr,\n\t\t\t\tlen, dma_dir);\n\trx_desc->buf_phys_addr = 0;\n\n\tif (!xdp_buff_has_frags(xdp))\n\t\tsinfo->nr_frags = 0;\n\n\tif (data_len > 0 && sinfo->nr_frags < MAX_SKB_FRAGS) {\n\t\tskb_frag_t *frag = &sinfo->frags[sinfo->nr_frags++];\n\n\t\tskb_frag_fill_page_desc(frag, page,\n\t\t\t\t\tpp->rx_offset_correction, data_len);\n\n\t\tif (!xdp_buff_has_frags(xdp)) {\n\t\t\tsinfo->xdp_frags_size = *size;\n\t\t\txdp_buff_set_frags_flag(xdp);\n\t\t}\n\t\tif (page_is_pfmemalloc(page))\n\t\t\txdp_buff_set_frag_pfmemalloc(xdp);\n\t} else {\n\t\tpage_pool_put_full_page(rxq->page_pool, page, true);\n\t}\n\t*size -= len;\n}\n\nstatic struct sk_buff *\nmvneta_swbm_build_skb(struct mvneta_port *pp, struct page_pool *pool,\n\t\t      struct xdp_buff *xdp, u32 desc_status)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\tstruct sk_buff *skb;\n\tu8 num_frags;\n\n\tif (unlikely(xdp_buff_has_frags(xdp)))\n\t\tnum_frags = sinfo->nr_frags;\n\n\tskb = build_skb(xdp->data_hard_start, PAGE_SIZE);\n\tif (!skb)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tskb_mark_for_recycle(skb);\n\n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\tskb_put(skb, xdp->data_end - xdp->data);\n\tskb->ip_summed = mvneta_rx_csum(pp, desc_status);\n\n\tif (unlikely(xdp_buff_has_frags(xdp)))\n\t\txdp_update_skb_shared_info(skb, num_frags,\n\t\t\t\t\t   sinfo->xdp_frags_size,\n\t\t\t\t\t   num_frags * xdp->frame_sz,\n\t\t\t\t\t   xdp_buff_is_frag_pfmemalloc(xdp));\n\n\treturn skb;\n}\n\n \nstatic int mvneta_rx_swbm(struct napi_struct *napi,\n\t\t\t  struct mvneta_port *pp, int budget,\n\t\t\t  struct mvneta_rx_queue *rxq)\n{\n\tint rx_proc = 0, rx_todo, refill, size = 0;\n\tstruct net_device *dev = pp->dev;\n\tstruct mvneta_stats ps = {};\n\tstruct bpf_prog *xdp_prog;\n\tu32 desc_status, frame_sz;\n\tstruct xdp_buff xdp_buf;\n\n\txdp_init_buff(&xdp_buf, PAGE_SIZE, &rxq->xdp_rxq);\n\txdp_buf.data_hard_start = NULL;\n\n\t \n\trx_todo = mvneta_rxq_busy_desc_num_get(pp, rxq);\n\n\txdp_prog = READ_ONCE(pp->xdp_prog);\n\n\t \n\twhile (rx_proc < budget && rx_proc < rx_todo) {\n\t\tstruct mvneta_rx_desc *rx_desc = mvneta_rxq_next_desc_get(rxq);\n\t\tu32 rx_status, index;\n\t\tstruct sk_buff *skb;\n\t\tstruct page *page;\n\n\t\tindex = rx_desc - rxq->descs;\n\t\tpage = (struct page *)rxq->buf_virt_addr[index];\n\n\t\trx_status = rx_desc->status;\n\t\trx_proc++;\n\t\trxq->refill_num++;\n\n\t\tif (rx_status & MVNETA_RXD_FIRST_DESC) {\n\t\t\t \n\t\t\tif (rx_status & MVNETA_RXD_ERR_SUMMARY) {\n\t\t\t\tmvneta_rx_error(pp, rx_desc);\n\t\t\t\tgoto next;\n\t\t\t}\n\n\t\t\tsize = rx_desc->data_size;\n\t\t\tframe_sz = size - ETH_FCS_LEN;\n\t\t\tdesc_status = rx_status;\n\n\t\t\tmvneta_swbm_rx_frame(pp, rx_desc, rxq, &xdp_buf,\n\t\t\t\t\t     &size, page);\n\t\t} else {\n\t\t\tif (unlikely(!xdp_buf.data_hard_start)) {\n\t\t\t\trx_desc->buf_phys_addr = 0;\n\t\t\t\tpage_pool_put_full_page(rxq->page_pool, page,\n\t\t\t\t\t\t\ttrue);\n\t\t\t\tgoto next;\n\t\t\t}\n\n\t\t\tmvneta_swbm_add_rx_fragment(pp, rx_desc, rxq, &xdp_buf,\n\t\t\t\t\t\t    &size, page);\n\t\t}  \n\n\t\tif (!(rx_status & MVNETA_RXD_LAST_DESC))\n\t\t\t \n\t\t\tcontinue;\n\n\t\tif (size) {\n\t\t\tmvneta_xdp_put_buff(pp, rxq, &xdp_buf, -1);\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (xdp_prog &&\n\t\t    mvneta_run_xdp(pp, rxq, xdp_prog, &xdp_buf, frame_sz, &ps))\n\t\t\tgoto next;\n\n\t\tskb = mvneta_swbm_build_skb(pp, rxq->page_pool, &xdp_buf, desc_status);\n\t\tif (IS_ERR(skb)) {\n\t\t\tstruct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);\n\n\t\t\tmvneta_xdp_put_buff(pp, rxq, &xdp_buf, -1);\n\n\t\t\tu64_stats_update_begin(&stats->syncp);\n\t\t\tstats->es.skb_alloc_error++;\n\t\t\tstats->rx_dropped++;\n\t\t\tu64_stats_update_end(&stats->syncp);\n\n\t\t\tgoto next;\n\t\t}\n\n\t\tps.rx_bytes += skb->len;\n\t\tps.rx_packets++;\n\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tnapi_gro_receive(napi, skb);\nnext:\n\t\txdp_buf.data_hard_start = NULL;\n\t}\n\n\tif (xdp_buf.data_hard_start)\n\t\tmvneta_xdp_put_buff(pp, rxq, &xdp_buf, -1);\n\n\tif (ps.xdp_redirect)\n\t\txdp_do_flush_map();\n\n\tif (ps.rx_packets)\n\t\tmvneta_update_stats(pp, &ps);\n\n\t \n\trefill = mvneta_rx_refill_queue(pp, rxq);\n\n\t \n\tmvneta_rxq_desc_num_update(pp, rxq, rx_proc, refill);\n\n\treturn ps.rx_packets;\n}\n\n \nstatic int mvneta_rx_hwbm(struct napi_struct *napi,\n\t\t\t  struct mvneta_port *pp, int rx_todo,\n\t\t\t  struct mvneta_rx_queue *rxq)\n{\n\tstruct net_device *dev = pp->dev;\n\tint rx_done;\n\tu32 rcvd_pkts = 0;\n\tu32 rcvd_bytes = 0;\n\n\t \n\trx_done = mvneta_rxq_busy_desc_num_get(pp, rxq);\n\n\tif (rx_todo > rx_done)\n\t\trx_todo = rx_done;\n\n\trx_done = 0;\n\n\t \n\twhile (rx_done < rx_todo) {\n\t\tstruct mvneta_rx_desc *rx_desc = mvneta_rxq_next_desc_get(rxq);\n\t\tstruct mvneta_bm_pool *bm_pool = NULL;\n\t\tstruct sk_buff *skb;\n\t\tunsigned char *data;\n\t\tdma_addr_t phys_addr;\n\t\tu32 rx_status, frag_size;\n\t\tint rx_bytes, err;\n\t\tu8 pool_id;\n\n\t\trx_done++;\n\t\trx_status = rx_desc->status;\n\t\trx_bytes = rx_desc->data_size - (ETH_FCS_LEN + MVNETA_MH_SIZE);\n\t\tdata = (u8 *)(uintptr_t)rx_desc->buf_cookie;\n\t\tphys_addr = rx_desc->buf_phys_addr;\n\t\tpool_id = MVNETA_RX_GET_BM_POOL_ID(rx_desc);\n\t\tbm_pool = &pp->bm_priv->bm_pools[pool_id];\n\n\t\tif (!mvneta_rxq_desc_is_first_last(rx_status) ||\n\t\t    (rx_status & MVNETA_RXD_ERR_SUMMARY)) {\nerr_drop_frame_ret_pool:\n\t\t\t \n\t\t\tmvneta_bm_pool_put_bp(pp->bm_priv, bm_pool,\n\t\t\t\t\t      rx_desc->buf_phys_addr);\nerr_drop_frame:\n\t\t\tmvneta_rx_error(pp, rx_desc);\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\tif (rx_bytes <= rx_copybreak) {\n\t\t\t \n\t\t\tskb = netdev_alloc_skb_ip_align(dev, rx_bytes);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto err_drop_frame_ret_pool;\n\n\t\t\tdma_sync_single_range_for_cpu(&pp->bm_priv->pdev->dev,\n\t\t\t                              rx_desc->buf_phys_addr,\n\t\t\t                              MVNETA_MH_SIZE + NET_SKB_PAD,\n\t\t\t                              rx_bytes,\n\t\t\t                              DMA_FROM_DEVICE);\n\t\t\tskb_put_data(skb, data + MVNETA_MH_SIZE + NET_SKB_PAD,\n\t\t\t\t     rx_bytes);\n\n\t\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\t\tskb->ip_summed = mvneta_rx_csum(pp, rx_status);\n\t\t\tnapi_gro_receive(napi, skb);\n\n\t\t\trcvd_pkts++;\n\t\t\trcvd_bytes += rx_bytes;\n\n\t\t\t \n\t\t\tmvneta_bm_pool_put_bp(pp->bm_priv, bm_pool,\n\t\t\t\t\t      rx_desc->buf_phys_addr);\n\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\terr = hwbm_pool_refill(&bm_pool->hwbm_pool, GFP_ATOMIC);\n\t\tif (err) {\n\t\t\tstruct mvneta_pcpu_stats *stats;\n\n\t\t\tnetdev_err(dev, \"Linux processing - Can't refill\\n\");\n\n\t\t\tstats = this_cpu_ptr(pp->stats);\n\t\t\tu64_stats_update_begin(&stats->syncp);\n\t\t\tstats->es.refill_error++;\n\t\t\tu64_stats_update_end(&stats->syncp);\n\n\t\t\tgoto err_drop_frame_ret_pool;\n\t\t}\n\n\t\tfrag_size = bm_pool->hwbm_pool.frag_size;\n\n\t\tskb = build_skb(data, frag_size > PAGE_SIZE ? 0 : frag_size);\n\n\t\t \n\t\tdma_unmap_single(&pp->bm_priv->pdev->dev, phys_addr,\n\t\t\t\t bm_pool->buf_size, DMA_FROM_DEVICE);\n\t\tif (!skb)\n\t\t\tgoto err_drop_frame;\n\n\t\trcvd_pkts++;\n\t\trcvd_bytes += rx_bytes;\n\n\t\t \n\t\tskb_reserve(skb, MVNETA_MH_SIZE + NET_SKB_PAD);\n\t\tskb_put(skb, rx_bytes);\n\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tskb->ip_summed = mvneta_rx_csum(pp, rx_status);\n\n\t\tnapi_gro_receive(napi, skb);\n\t}\n\n\tif (rcvd_pkts) {\n\t\tstruct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);\n\n\t\tu64_stats_update_begin(&stats->syncp);\n\t\tstats->es.ps.rx_packets += rcvd_pkts;\n\t\tstats->es.ps.rx_bytes += rcvd_bytes;\n\t\tu64_stats_update_end(&stats->syncp);\n\t}\n\n\t \n\tmvneta_rxq_desc_num_update(pp, rxq, rx_done, rx_done);\n\n\treturn rx_done;\n}\n\nstatic void mvneta_free_tso_hdrs(struct mvneta_port *pp,\n\t\t\t\t struct mvneta_tx_queue *txq)\n{\n\tstruct device *dev = pp->dev->dev.parent;\n\tint i;\n\n\tfor (i = 0; i < MVNETA_MAX_TSO_PAGES; i++) {\n\t\tif (txq->tso_hdrs[i]) {\n\t\t\tdma_free_coherent(dev, MVNETA_TSO_PAGE_SIZE,\n\t\t\t\t\t  txq->tso_hdrs[i],\n\t\t\t\t\t  txq->tso_hdrs_phys[i]);\n\t\t\ttxq->tso_hdrs[i] = NULL;\n\t\t}\n\t}\n}\n\nstatic int mvneta_alloc_tso_hdrs(struct mvneta_port *pp,\n\t\t\t\t struct mvneta_tx_queue *txq)\n{\n\tstruct device *dev = pp->dev->dev.parent;\n\tint i, num;\n\n\tnum = DIV_ROUND_UP(txq->size, MVNETA_TSO_PER_PAGE);\n\tfor (i = 0; i < num; i++) {\n\t\ttxq->tso_hdrs[i] = dma_alloc_coherent(dev, MVNETA_TSO_PAGE_SIZE,\n\t\t\t\t\t\t      &txq->tso_hdrs_phys[i],\n\t\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!txq->tso_hdrs[i]) {\n\t\t\tmvneta_free_tso_hdrs(pp, txq);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic char *mvneta_get_tso_hdr(struct mvneta_tx_queue *txq, dma_addr_t *dma)\n{\n\tint index, offset;\n\n\tindex = txq->txq_put_index / MVNETA_TSO_PER_PAGE;\n\toffset = (txq->txq_put_index % MVNETA_TSO_PER_PAGE) * TSO_HEADER_SIZE;\n\n\t*dma = txq->tso_hdrs_phys[index] + offset;\n\n\treturn txq->tso_hdrs[index] + offset;\n}\n\nstatic void mvneta_tso_put_hdr(struct sk_buff *skb, struct mvneta_tx_queue *txq,\n\t\t\t       struct tso_t *tso, int size, bool is_last)\n{\n\tstruct mvneta_tx_buf *buf = &txq->buf[txq->txq_put_index];\n\tint hdr_len = skb_tcp_all_headers(skb);\n\tstruct mvneta_tx_desc *tx_desc;\n\tdma_addr_t hdr_phys;\n\tchar *hdr;\n\n\thdr = mvneta_get_tso_hdr(txq, &hdr_phys);\n\ttso_build_hdr(skb, hdr, tso, size, is_last);\n\n\ttx_desc = mvneta_txq_next_desc_get(txq);\n\ttx_desc->data_size = hdr_len;\n\ttx_desc->command = mvneta_skb_tx_csum(skb);\n\ttx_desc->command |= MVNETA_TXD_F_DESC;\n\ttx_desc->buf_phys_addr = hdr_phys;\n\tbuf->type = MVNETA_TYPE_TSO;\n\tbuf->skb = NULL;\n\n\tmvneta_txq_inc_put(txq);\n}\n\nstatic inline int\nmvneta_tso_put_data(struct net_device *dev, struct mvneta_tx_queue *txq,\n\t\t    struct sk_buff *skb, char *data, int size,\n\t\t    bool last_tcp, bool is_last)\n{\n\tstruct mvneta_tx_buf *buf = &txq->buf[txq->txq_put_index];\n\tstruct mvneta_tx_desc *tx_desc;\n\n\ttx_desc = mvneta_txq_next_desc_get(txq);\n\ttx_desc->data_size = size;\n\ttx_desc->buf_phys_addr = dma_map_single(dev->dev.parent, data,\n\t\t\t\t\t\tsize, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dev.parent,\n\t\t     tx_desc->buf_phys_addr))) {\n\t\tmvneta_txq_desc_put(txq);\n\t\treturn -ENOMEM;\n\t}\n\n\ttx_desc->command = 0;\n\tbuf->type = MVNETA_TYPE_SKB;\n\tbuf->skb = NULL;\n\n\tif (last_tcp) {\n\t\t \n\t\ttx_desc->command = MVNETA_TXD_L_DESC;\n\n\t\t \n\t\tif (is_last)\n\t\t\tbuf->skb = skb;\n\t}\n\tmvneta_txq_inc_put(txq);\n\treturn 0;\n}\n\nstatic void mvneta_release_descs(struct mvneta_port *pp,\n\t\t\t\t struct mvneta_tx_queue *txq,\n\t\t\t\t int first, int num)\n{\n\tint desc_idx, i;\n\n\tdesc_idx = first + num;\n\tif (desc_idx >= txq->size)\n\t\tdesc_idx -= txq->size;\n\n\tfor (i = num; i >= 0; i--) {\n\t\tstruct mvneta_tx_desc *tx_desc = txq->descs + desc_idx;\n\t\tstruct mvneta_tx_buf *buf = &txq->buf[desc_idx];\n\n\t\tif (buf->type == MVNETA_TYPE_SKB)\n\t\t\tdma_unmap_single(pp->dev->dev.parent,\n\t\t\t\t\t tx_desc->buf_phys_addr,\n\t\t\t\t\t tx_desc->data_size,\n\t\t\t\t\t DMA_TO_DEVICE);\n\n\t\tmvneta_txq_desc_put(txq);\n\n\t\tif (desc_idx == 0)\n\t\t\tdesc_idx = txq->size;\n\t\tdesc_idx -= 1;\n\t}\n}\n\nstatic int mvneta_tx_tso(struct sk_buff *skb, struct net_device *dev,\n\t\t\t struct mvneta_tx_queue *txq)\n{\n\tint hdr_len, total_len, data_left;\n\tint first_desc, desc_count = 0;\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tstruct tso_t tso;\n\n\t \n\tif ((txq->count + tso_count_descs(skb)) >= txq->size)\n\t\treturn 0;\n\n\tif (skb_headlen(skb) < skb_tcp_all_headers(skb)) {\n\t\tpr_info(\"*** Is this even possible?\\n\");\n\t\treturn 0;\n\t}\n\n\tfirst_desc = txq->txq_put_index;\n\n\t \n\thdr_len = tso_start(skb, &tso);\n\n\ttotal_len = skb->len - hdr_len;\n\twhile (total_len > 0) {\n\t\tdata_left = min_t(int, skb_shinfo(skb)->gso_size, total_len);\n\t\ttotal_len -= data_left;\n\t\tdesc_count++;\n\n\t\t \n\t\tmvneta_tso_put_hdr(skb, txq, &tso, data_left, total_len == 0);\n\n\t\twhile (data_left > 0) {\n\t\t\tint size;\n\t\t\tdesc_count++;\n\n\t\t\tsize = min_t(int, tso.size, data_left);\n\n\t\t\tif (mvneta_tso_put_data(dev, txq, skb,\n\t\t\t\t\t\t tso.data, size,\n\t\t\t\t\t\t size == data_left,\n\t\t\t\t\t\t total_len == 0))\n\t\t\t\tgoto err_release;\n\t\t\tdata_left -= size;\n\n\t\t\ttso_build_data(skb, &tso, size);\n\t\t}\n\t}\n\n\treturn desc_count;\n\nerr_release:\n\t \n\tmvneta_release_descs(pp, txq, first_desc, desc_count - 1);\n\treturn 0;\n}\n\n \nstatic int mvneta_tx_frag_process(struct mvneta_port *pp, struct sk_buff *skb,\n\t\t\t\t  struct mvneta_tx_queue *txq)\n{\n\tstruct mvneta_tx_desc *tx_desc;\n\tint i, nr_frags = skb_shinfo(skb)->nr_frags;\n\tint first_desc = txq->txq_put_index;\n\n\tfor (i = 0; i < nr_frags; i++) {\n\t\tstruct mvneta_tx_buf *buf = &txq->buf[txq->txq_put_index];\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tvoid *addr = skb_frag_address(frag);\n\n\t\ttx_desc = mvneta_txq_next_desc_get(txq);\n\t\ttx_desc->data_size = skb_frag_size(frag);\n\n\t\ttx_desc->buf_phys_addr =\n\t\t\tdma_map_single(pp->dev->dev.parent, addr,\n\t\t\t\t       tx_desc->data_size, DMA_TO_DEVICE);\n\n\t\tif (dma_mapping_error(pp->dev->dev.parent,\n\t\t\t\t      tx_desc->buf_phys_addr)) {\n\t\t\tmvneta_txq_desc_put(txq);\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (i == nr_frags - 1) {\n\t\t\t \n\t\t\ttx_desc->command = MVNETA_TXD_L_DESC | MVNETA_TXD_Z_PAD;\n\t\t\tbuf->skb = skb;\n\t\t} else {\n\t\t\t \n\t\t\ttx_desc->command = 0;\n\t\t\tbuf->skb = NULL;\n\t\t}\n\t\tbuf->type = MVNETA_TYPE_SKB;\n\t\tmvneta_txq_inc_put(txq);\n\t}\n\n\treturn 0;\n\nerror:\n\t \n\tmvneta_release_descs(pp, txq, first_desc, i - 1);\n\treturn -ENOMEM;\n}\n\n \nstatic netdev_tx_t mvneta_tx(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tu16 txq_id = skb_get_queue_mapping(skb);\n\tstruct mvneta_tx_queue *txq = &pp->txqs[txq_id];\n\tstruct mvneta_tx_buf *buf = &txq->buf[txq->txq_put_index];\n\tstruct mvneta_tx_desc *tx_desc;\n\tint len = skb->len;\n\tint frags = 0;\n\tu32 tx_cmd;\n\n\tif (!netif_running(dev))\n\t\tgoto out;\n\n\tif (skb_is_gso(skb)) {\n\t\tfrags = mvneta_tx_tso(skb, dev, txq);\n\t\tgoto out;\n\t}\n\n\tfrags = skb_shinfo(skb)->nr_frags + 1;\n\n\t \n\ttx_desc = mvneta_txq_next_desc_get(txq);\n\n\ttx_cmd = mvneta_skb_tx_csum(skb);\n\n\ttx_desc->data_size = skb_headlen(skb);\n\n\ttx_desc->buf_phys_addr = dma_map_single(dev->dev.parent, skb->data,\n\t\t\t\t\t\ttx_desc->data_size,\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dev.parent,\n\t\t\t\t       tx_desc->buf_phys_addr))) {\n\t\tmvneta_txq_desc_put(txq);\n\t\tfrags = 0;\n\t\tgoto out;\n\t}\n\n\tbuf->type = MVNETA_TYPE_SKB;\n\tif (frags == 1) {\n\t\t \n\t\ttx_cmd |= MVNETA_TXD_FLZ_DESC;\n\t\ttx_desc->command = tx_cmd;\n\t\tbuf->skb = skb;\n\t\tmvneta_txq_inc_put(txq);\n\t} else {\n\t\t \n\t\ttx_cmd |= MVNETA_TXD_F_DESC;\n\t\tbuf->skb = NULL;\n\t\tmvneta_txq_inc_put(txq);\n\t\ttx_desc->command = tx_cmd;\n\t\t \n\t\tif (mvneta_tx_frag_process(pp, skb, txq)) {\n\t\t\tdma_unmap_single(dev->dev.parent,\n\t\t\t\t\t tx_desc->buf_phys_addr,\n\t\t\t\t\t tx_desc->data_size,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\tmvneta_txq_desc_put(txq);\n\t\t\tfrags = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tif (frags > 0) {\n\t\tstruct netdev_queue *nq = netdev_get_tx_queue(dev, txq_id);\n\t\tstruct mvneta_pcpu_stats *stats = this_cpu_ptr(pp->stats);\n\n\t\tnetdev_tx_sent_queue(nq, len);\n\n\t\ttxq->count += frags;\n\t\tif (txq->count >= txq->tx_stop_threshold)\n\t\t\tnetif_tx_stop_queue(nq);\n\n\t\tif (!netdev_xmit_more() || netif_xmit_stopped(nq) ||\n\t\t    txq->pending + frags > MVNETA_TXQ_DEC_SENT_MASK)\n\t\t\tmvneta_txq_pend_desc_add(pp, txq, frags);\n\t\telse\n\t\t\ttxq->pending += frags;\n\n\t\tu64_stats_update_begin(&stats->syncp);\n\t\tstats->es.ps.tx_bytes += len;\n\t\tstats->es.ps.tx_packets++;\n\t\tu64_stats_update_end(&stats->syncp);\n\t} else {\n\t\tdev->stats.tx_dropped++;\n\t\tdev_kfree_skb_any(skb);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\n\n \nstatic void mvneta_txq_done_force(struct mvneta_port *pp,\n\t\t\t\t  struct mvneta_tx_queue *txq)\n\n{\n\tstruct netdev_queue *nq = netdev_get_tx_queue(pp->dev, txq->id);\n\tint tx_done = txq->count;\n\n\tmvneta_txq_bufs_free(pp, txq, tx_done, nq, false);\n\n\t \n\ttxq->count = 0;\n\ttxq->txq_put_index = 0;\n\ttxq->txq_get_index = 0;\n}\n\n \nstatic void mvneta_tx_done_gbe(struct mvneta_port *pp, u32 cause_tx_done)\n{\n\tstruct mvneta_tx_queue *txq;\n\tstruct netdev_queue *nq;\n\tint cpu = smp_processor_id();\n\n\twhile (cause_tx_done) {\n\t\ttxq = mvneta_tx_done_policy(pp, cause_tx_done);\n\n\t\tnq = netdev_get_tx_queue(pp->dev, txq->id);\n\t\t__netif_tx_lock(nq, cpu);\n\n\t\tif (txq->count)\n\t\t\tmvneta_txq_done(pp, txq);\n\n\t\t__netif_tx_unlock(nq);\n\t\tcause_tx_done &= ~((1 << txq->id));\n\t}\n}\n\n \nstatic int mvneta_addr_crc(unsigned char *addr)\n{\n\tint crc = 0;\n\tint i;\n\n\tfor (i = 0; i < ETH_ALEN; i++) {\n\t\tint j;\n\n\t\tcrc = (crc ^ addr[i]) << 8;\n\t\tfor (j = 7; j >= 0; j--) {\n\t\t\tif (crc & (0x100 << j))\n\t\t\t\tcrc ^= 0x107 << j;\n\t\t}\n\t}\n\n\treturn crc;\n}\n\n \nstatic void mvneta_set_special_mcast_addr(struct mvneta_port *pp,\n\t\t\t\t\t  unsigned char last_byte,\n\t\t\t\t\t  int queue)\n{\n\tunsigned int smc_table_reg;\n\tunsigned int tbl_offset;\n\tunsigned int reg_offset;\n\n\t \n\ttbl_offset = (last_byte / 4);\n\t \n\treg_offset = last_byte % 4;\n\n\tsmc_table_reg = mvreg_read(pp, (MVNETA_DA_FILT_SPEC_MCAST\n\t\t\t\t\t+ tbl_offset * 4));\n\n\tif (queue == -1)\n\t\tsmc_table_reg &= ~(0xff << (8 * reg_offset));\n\telse {\n\t\tsmc_table_reg &= ~(0xff << (8 * reg_offset));\n\t\tsmc_table_reg |= ((0x01 | (queue << 1)) << (8 * reg_offset));\n\t}\n\n\tmvreg_write(pp, MVNETA_DA_FILT_SPEC_MCAST + tbl_offset * 4,\n\t\t    smc_table_reg);\n}\n\n \nstatic void mvneta_set_other_mcast_addr(struct mvneta_port *pp,\n\t\t\t\t\tunsigned char crc8,\n\t\t\t\t\tint queue)\n{\n\tunsigned int omc_table_reg;\n\tunsigned int tbl_offset;\n\tunsigned int reg_offset;\n\n\ttbl_offset = (crc8 / 4) * 4;  \n\treg_offset = crc8 % 4;\t      \n\n\tomc_table_reg = mvreg_read(pp, MVNETA_DA_FILT_OTH_MCAST + tbl_offset);\n\n\tif (queue == -1) {\n\t\t \n\t\tomc_table_reg &= ~(0xff << (8 * reg_offset));\n\t} else {\n\t\tomc_table_reg &= ~(0xff << (8 * reg_offset));\n\t\tomc_table_reg |= ((0x01 | (queue << 1)) << (8 * reg_offset));\n\t}\n\n\tmvreg_write(pp, MVNETA_DA_FILT_OTH_MCAST + tbl_offset, omc_table_reg);\n}\n\n \nstatic int mvneta_mcast_addr_set(struct mvneta_port *pp, unsigned char *p_addr,\n\t\t\t\t int queue)\n{\n\tunsigned char crc_result = 0;\n\n\tif (memcmp(p_addr, \"\\x01\\x00\\x5e\\x00\\x00\", 5) == 0) {\n\t\tmvneta_set_special_mcast_addr(pp, p_addr[5], queue);\n\t\treturn 0;\n\t}\n\n\tcrc_result = mvneta_addr_crc(p_addr);\n\tif (queue == -1) {\n\t\tif (pp->mcast_count[crc_result] == 0) {\n\t\t\tnetdev_info(pp->dev, \"No valid Mcast for crc8=0x%02x\\n\",\n\t\t\t\t    crc_result);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tpp->mcast_count[crc_result]--;\n\t\tif (pp->mcast_count[crc_result] != 0) {\n\t\t\tnetdev_info(pp->dev,\n\t\t\t\t    \"After delete there are %d valid Mcast for crc8=0x%02x\\n\",\n\t\t\t\t    pp->mcast_count[crc_result], crc_result);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else\n\t\tpp->mcast_count[crc_result]++;\n\n\tmvneta_set_other_mcast_addr(pp, crc_result, queue);\n\n\treturn 0;\n}\n\n \nstatic void mvneta_rx_unicast_promisc_set(struct mvneta_port *pp,\n\t\t\t\t\t  int is_promisc)\n{\n\tu32 port_cfg_reg, val;\n\n\tport_cfg_reg = mvreg_read(pp, MVNETA_PORT_CONFIG);\n\n\tval = mvreg_read(pp, MVNETA_TYPE_PRIO);\n\n\t \n\tif (is_promisc) {\n\t\t \n\t\tport_cfg_reg |= MVNETA_UNI_PROMISC_MODE;\n\t\tval |= MVNETA_FORCE_UNI;\n\t\tmvreg_write(pp, MVNETA_MAC_ADDR_LOW, 0xffff);\n\t\tmvreg_write(pp, MVNETA_MAC_ADDR_HIGH, 0xffffffff);\n\t} else {\n\t\t \n\t\tport_cfg_reg &= ~MVNETA_UNI_PROMISC_MODE;\n\t\tval &= ~MVNETA_FORCE_UNI;\n\t}\n\n\tmvreg_write(pp, MVNETA_PORT_CONFIG, port_cfg_reg);\n\tmvreg_write(pp, MVNETA_TYPE_PRIO, val);\n}\n\n \nstatic void mvneta_set_rx_mode(struct net_device *dev)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tstruct netdev_hw_addr *ha;\n\n\tif (dev->flags & IFF_PROMISC) {\n\t\t \n\t\tmvneta_rx_unicast_promisc_set(pp, 1);\n\t\tmvneta_set_ucast_table(pp, pp->rxq_def);\n\t\tmvneta_set_special_mcast_table(pp, pp->rxq_def);\n\t\tmvneta_set_other_mcast_table(pp, pp->rxq_def);\n\t} else {\n\t\t \n\t\tmvneta_rx_unicast_promisc_set(pp, 0);\n\t\tmvneta_set_ucast_table(pp, -1);\n\t\tmvneta_mac_addr_set(pp, dev->dev_addr, pp->rxq_def);\n\n\t\tif (dev->flags & IFF_ALLMULTI) {\n\t\t\t \n\t\t\tmvneta_set_special_mcast_table(pp, pp->rxq_def);\n\t\t\tmvneta_set_other_mcast_table(pp, pp->rxq_def);\n\t\t} else {\n\t\t\t \n\t\t\tmvneta_set_special_mcast_table(pp, -1);\n\t\t\tmvneta_set_other_mcast_table(pp, -1);\n\n\t\t\tif (!netdev_mc_empty(dev)) {\n\t\t\t\tnetdev_for_each_mc_addr(ha, dev) {\n\t\t\t\t\tmvneta_mcast_addr_set(pp, ha->addr,\n\t\t\t\t\t\t\t      pp->rxq_def);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nstatic irqreturn_t mvneta_isr(int irq, void *dev_id)\n{\n\tstruct mvneta_port *pp = (struct mvneta_port *)dev_id;\n\n\tmvreg_write(pp, MVNETA_INTR_NEW_MASK, 0);\n\tnapi_schedule(&pp->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t mvneta_percpu_isr(int irq, void *dev_id)\n{\n\tstruct mvneta_pcpu_port *port = (struct mvneta_pcpu_port *)dev_id;\n\n\tdisable_percpu_irq(port->pp->dev->irq);\n\tnapi_schedule(&port->napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void mvneta_link_change(struct mvneta_port *pp)\n{\n\tu32 gmac_stat = mvreg_read(pp, MVNETA_GMAC_STATUS);\n\n\tphylink_mac_change(pp->phylink, !!(gmac_stat & MVNETA_GMAC_LINK_UP));\n}\n\n \nstatic int mvneta_poll(struct napi_struct *napi, int budget)\n{\n\tint rx_done = 0;\n\tu32 cause_rx_tx;\n\tint rx_queue;\n\tstruct mvneta_port *pp = netdev_priv(napi->dev);\n\tstruct mvneta_pcpu_port *port = this_cpu_ptr(pp->ports);\n\n\tif (!netif_running(pp->dev)) {\n\t\tnapi_complete(napi);\n\t\treturn rx_done;\n\t}\n\n\t \n\tcause_rx_tx = mvreg_read(pp, MVNETA_INTR_NEW_CAUSE);\n\tif (cause_rx_tx & MVNETA_MISCINTR_INTR_MASK) {\n\t\tu32 cause_misc = mvreg_read(pp, MVNETA_INTR_MISC_CAUSE);\n\n\t\tmvreg_write(pp, MVNETA_INTR_MISC_CAUSE, 0);\n\n\t\tif (cause_misc & (MVNETA_CAUSE_PHY_STATUS_CHANGE |\n\t\t\t\t  MVNETA_CAUSE_LINK_CHANGE))\n\t\t\tmvneta_link_change(pp);\n\t}\n\n\t \n\tif (cause_rx_tx & MVNETA_TX_INTR_MASK_ALL) {\n\t\tmvneta_tx_done_gbe(pp, (cause_rx_tx & MVNETA_TX_INTR_MASK_ALL));\n\t\tcause_rx_tx &= ~MVNETA_TX_INTR_MASK_ALL;\n\t}\n\n\t \n\tcause_rx_tx |= pp->neta_armada3700 ? pp->cause_rx_tx :\n\t\tport->cause_rx_tx;\n\n\trx_queue = fls(((cause_rx_tx >> 8) & 0xff));\n\tif (rx_queue) {\n\t\trx_queue = rx_queue - 1;\n\t\tif (pp->bm_priv)\n\t\t\trx_done = mvneta_rx_hwbm(napi, pp, budget,\n\t\t\t\t\t\t &pp->rxqs[rx_queue]);\n\t\telse\n\t\t\trx_done = mvneta_rx_swbm(napi, pp, budget,\n\t\t\t\t\t\t &pp->rxqs[rx_queue]);\n\t}\n\n\tif (rx_done < budget) {\n\t\tcause_rx_tx = 0;\n\t\tnapi_complete_done(napi, rx_done);\n\n\t\tif (pp->neta_armada3700) {\n\t\t\tunsigned long flags;\n\n\t\t\tlocal_irq_save(flags);\n\t\t\tmvreg_write(pp, MVNETA_INTR_NEW_MASK,\n\t\t\t\t    MVNETA_RX_INTR_MASK(rxq_number) |\n\t\t\t\t    MVNETA_TX_INTR_MASK(txq_number) |\n\t\t\t\t    MVNETA_MISCINTR_INTR_MASK);\n\t\t\tlocal_irq_restore(flags);\n\t\t} else {\n\t\t\tenable_percpu_irq(pp->dev->irq, 0);\n\t\t}\n\t}\n\n\tif (pp->neta_armada3700)\n\t\tpp->cause_rx_tx = cause_rx_tx;\n\telse\n\t\tport->cause_rx_tx = cause_rx_tx;\n\n\treturn rx_done;\n}\n\nstatic int mvneta_create_page_pool(struct mvneta_port *pp,\n\t\t\t\t   struct mvneta_rx_queue *rxq, int size)\n{\n\tstruct bpf_prog *xdp_prog = READ_ONCE(pp->xdp_prog);\n\tstruct page_pool_params pp_params = {\n\t\t.order = 0,\n\t\t.flags = PP_FLAG_DMA_MAP | PP_FLAG_DMA_SYNC_DEV,\n\t\t.pool_size = size,\n\t\t.nid = NUMA_NO_NODE,\n\t\t.dev = pp->dev->dev.parent,\n\t\t.dma_dir = xdp_prog ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE,\n\t\t.offset = pp->rx_offset_correction,\n\t\t.max_len = MVNETA_MAX_RX_BUF_SIZE,\n\t};\n\tint err;\n\n\trxq->page_pool = page_pool_create(&pp_params);\n\tif (IS_ERR(rxq->page_pool)) {\n\t\terr = PTR_ERR(rxq->page_pool);\n\t\trxq->page_pool = NULL;\n\t\treturn err;\n\t}\n\n\terr = __xdp_rxq_info_reg(&rxq->xdp_rxq, pp->dev, rxq->id, 0,\n\t\t\t\t PAGE_SIZE);\n\tif (err < 0)\n\t\tgoto err_free_pp;\n\n\terr = xdp_rxq_info_reg_mem_model(&rxq->xdp_rxq, MEM_TYPE_PAGE_POOL,\n\t\t\t\t\t rxq->page_pool);\n\tif (err)\n\t\tgoto err_unregister_rxq;\n\n\treturn 0;\n\nerr_unregister_rxq:\n\txdp_rxq_info_unreg(&rxq->xdp_rxq);\nerr_free_pp:\n\tpage_pool_destroy(rxq->page_pool);\n\trxq->page_pool = NULL;\n\treturn err;\n}\n\n \nstatic int mvneta_rxq_fill(struct mvneta_port *pp, struct mvneta_rx_queue *rxq,\n\t\t\t   int num)\n{\n\tint i, err;\n\n\terr = mvneta_create_page_pool(pp, rxq, num);\n\tif (err < 0)\n\t\treturn err;\n\n\tfor (i = 0; i < num; i++) {\n\t\tmemset(rxq->descs + i, 0, sizeof(struct mvneta_rx_desc));\n\t\tif (mvneta_rx_refill(pp, rxq->descs + i, rxq,\n\t\t\t\t     GFP_KERNEL) != 0) {\n\t\t\tnetdev_err(pp->dev,\n\t\t\t\t   \"%s:rxq %d, %d of %d buffs  filled\\n\",\n\t\t\t\t   __func__, rxq->id, i, num);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tmvneta_rxq_non_occup_desc_add(pp, rxq, i);\n\n\treturn i;\n}\n\n \nstatic void mvneta_tx_reset(struct mvneta_port *pp)\n{\n\tint queue;\n\n\t \n\tfor (queue = 0; queue < txq_number; queue++)\n\t\tmvneta_txq_done_force(pp, &pp->txqs[queue]);\n\n\tmvreg_write(pp, MVNETA_PORT_TX_RESET, MVNETA_PORT_TX_DMA_RESET);\n\tmvreg_write(pp, MVNETA_PORT_TX_RESET, 0);\n}\n\nstatic void mvneta_rx_reset(struct mvneta_port *pp)\n{\n\tmvreg_write(pp, MVNETA_PORT_RX_RESET, MVNETA_PORT_RX_DMA_RESET);\n\tmvreg_write(pp, MVNETA_PORT_RX_RESET, 0);\n}\n\n \n\nstatic int mvneta_rxq_sw_init(struct mvneta_port *pp,\n\t\t\t      struct mvneta_rx_queue *rxq)\n{\n\trxq->size = pp->rx_ring_size;\n\n\t \n\trxq->descs = dma_alloc_coherent(pp->dev->dev.parent,\n\t\t\t\t\trxq->size * MVNETA_DESC_ALIGNED_SIZE,\n\t\t\t\t\t&rxq->descs_phys, GFP_KERNEL);\n\tif (!rxq->descs)\n\t\treturn -ENOMEM;\n\n\trxq->last_desc = rxq->size - 1;\n\n\treturn 0;\n}\n\nstatic void mvneta_rxq_hw_init(struct mvneta_port *pp,\n\t\t\t       struct mvneta_rx_queue *rxq)\n{\n\t \n\tmvreg_write(pp, MVNETA_RXQ_BASE_ADDR_REG(rxq->id), rxq->descs_phys);\n\tmvreg_write(pp, MVNETA_RXQ_SIZE_REG(rxq->id), rxq->size);\n\n\t \n\tmvneta_rx_pkts_coal_set(pp, rxq, rxq->pkts_coal);\n\tmvneta_rx_time_coal_set(pp, rxq, rxq->time_coal);\n\n\tif (!pp->bm_priv) {\n\t\t \n\t\tmvneta_rxq_offset_set(pp, rxq, 0);\n\t\tmvneta_rxq_buf_size_set(pp, rxq, PAGE_SIZE < SZ_64K ?\n\t\t\t\t\tMVNETA_MAX_RX_BUF_SIZE :\n\t\t\t\t\tMVNETA_RX_BUF_SIZE(pp->pkt_size));\n\t\tmvneta_rxq_bm_disable(pp, rxq);\n\t\tmvneta_rxq_fill(pp, rxq, rxq->size);\n\t} else {\n\t\t \n\t\tmvneta_rxq_offset_set(pp, rxq,\n\t\t\t\t      NET_SKB_PAD - pp->rx_offset_correction);\n\n\t\tmvneta_rxq_bm_enable(pp, rxq);\n\t\t \n\t\tmvneta_rxq_long_pool_set(pp, rxq);\n\t\tmvneta_rxq_short_pool_set(pp, rxq);\n\t\tmvneta_rxq_non_occup_desc_add(pp, rxq, rxq->size);\n\t}\n}\n\n \nstatic int mvneta_rxq_init(struct mvneta_port *pp,\n\t\t\t   struct mvneta_rx_queue *rxq)\n\n{\n\tint ret;\n\n\tret = mvneta_rxq_sw_init(pp, rxq);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tmvneta_rxq_hw_init(pp, rxq);\n\n\treturn 0;\n}\n\n \nstatic void mvneta_rxq_deinit(struct mvneta_port *pp,\n\t\t\t      struct mvneta_rx_queue *rxq)\n{\n\tmvneta_rxq_drop_pkts(pp, rxq);\n\n\tif (rxq->descs)\n\t\tdma_free_coherent(pp->dev->dev.parent,\n\t\t\t\t  rxq->size * MVNETA_DESC_ALIGNED_SIZE,\n\t\t\t\t  rxq->descs,\n\t\t\t\t  rxq->descs_phys);\n\n\trxq->descs             = NULL;\n\trxq->last_desc         = 0;\n\trxq->next_desc_to_proc = 0;\n\trxq->descs_phys        = 0;\n\trxq->first_to_refill   = 0;\n\trxq->refill_num        = 0;\n}\n\nstatic int mvneta_txq_sw_init(struct mvneta_port *pp,\n\t\t\t      struct mvneta_tx_queue *txq)\n{\n\tint cpu, err;\n\n\ttxq->size = pp->tx_ring_size;\n\n\t \n\ttxq->tx_stop_threshold = txq->size - MVNETA_MAX_SKB_DESCS;\n\ttxq->tx_wake_threshold = txq->tx_stop_threshold / 2;\n\n\t \n\ttxq->descs = dma_alloc_coherent(pp->dev->dev.parent,\n\t\t\t\t\ttxq->size * MVNETA_DESC_ALIGNED_SIZE,\n\t\t\t\t\t&txq->descs_phys, GFP_KERNEL);\n\tif (!txq->descs)\n\t\treturn -ENOMEM;\n\n\ttxq->last_desc = txq->size - 1;\n\n\ttxq->buf = kmalloc_array(txq->size, sizeof(*txq->buf), GFP_KERNEL);\n\tif (!txq->buf)\n\t\treturn -ENOMEM;\n\n\t \n\terr = mvneta_alloc_tso_hdrs(pp, txq);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (pp->neta_armada3700)\n\t\tcpu = 0;\n\telse if (txq_number > 1)\n\t\tcpu = txq->id % num_present_cpus();\n\telse\n\t\tcpu = pp->rxq_def % num_present_cpus();\n\tcpumask_set_cpu(cpu, &txq->affinity_mask);\n\tnetif_set_xps_queue(pp->dev, &txq->affinity_mask, txq->id);\n\n\treturn 0;\n}\n\nstatic void mvneta_txq_hw_init(struct mvneta_port *pp,\n\t\t\t       struct mvneta_tx_queue *txq)\n{\n\t \n\tmvreg_write(pp, MVETH_TXQ_TOKEN_CFG_REG(txq->id), 0x03ffffff);\n\tmvreg_write(pp, MVETH_TXQ_TOKEN_COUNT_REG(txq->id), 0x3fffffff);\n\n\t \n\tmvreg_write(pp, MVNETA_TXQ_BASE_ADDR_REG(txq->id), txq->descs_phys);\n\tmvreg_write(pp, MVNETA_TXQ_SIZE_REG(txq->id), txq->size);\n\n\tmvneta_tx_done_pkts_coal_set(pp, txq, txq->done_pkts_coal);\n}\n\n \nstatic int mvneta_txq_init(struct mvneta_port *pp,\n\t\t\t   struct mvneta_tx_queue *txq)\n{\n\tint ret;\n\n\tret = mvneta_txq_sw_init(pp, txq);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tmvneta_txq_hw_init(pp, txq);\n\n\treturn 0;\n}\n\n \nstatic void mvneta_txq_sw_deinit(struct mvneta_port *pp,\n\t\t\t\t struct mvneta_tx_queue *txq)\n{\n\tstruct netdev_queue *nq = netdev_get_tx_queue(pp->dev, txq->id);\n\n\tkfree(txq->buf);\n\n\tmvneta_free_tso_hdrs(pp, txq);\n\tif (txq->descs)\n\t\tdma_free_coherent(pp->dev->dev.parent,\n\t\t\t\t  txq->size * MVNETA_DESC_ALIGNED_SIZE,\n\t\t\t\t  txq->descs, txq->descs_phys);\n\n\tnetdev_tx_reset_queue(nq);\n\n\ttxq->buf               = NULL;\n\ttxq->descs             = NULL;\n\ttxq->last_desc         = 0;\n\ttxq->next_desc_to_proc = 0;\n\ttxq->descs_phys        = 0;\n}\n\nstatic void mvneta_txq_hw_deinit(struct mvneta_port *pp,\n\t\t\t\t struct mvneta_tx_queue *txq)\n{\n\t \n\tmvreg_write(pp, MVETH_TXQ_TOKEN_CFG_REG(txq->id), 0);\n\tmvreg_write(pp, MVETH_TXQ_TOKEN_COUNT_REG(txq->id), 0);\n\n\t \n\tmvreg_write(pp, MVNETA_TXQ_BASE_ADDR_REG(txq->id), 0);\n\tmvreg_write(pp, MVNETA_TXQ_SIZE_REG(txq->id), 0);\n}\n\nstatic void mvneta_txq_deinit(struct mvneta_port *pp,\n\t\t\t      struct mvneta_tx_queue *txq)\n{\n\tmvneta_txq_sw_deinit(pp, txq);\n\tmvneta_txq_hw_deinit(pp, txq);\n}\n\n \nstatic void mvneta_cleanup_txqs(struct mvneta_port *pp)\n{\n\tint queue;\n\n\tfor (queue = 0; queue < txq_number; queue++)\n\t\tmvneta_txq_deinit(pp, &pp->txqs[queue]);\n}\n\n \nstatic void mvneta_cleanup_rxqs(struct mvneta_port *pp)\n{\n\tint queue;\n\n\tfor (queue = 0; queue < rxq_number; queue++)\n\t\tmvneta_rxq_deinit(pp, &pp->rxqs[queue]);\n}\n\n\n \nstatic int mvneta_setup_rxqs(struct mvneta_port *pp)\n{\n\tint queue;\n\n\tfor (queue = 0; queue < rxq_number; queue++) {\n\t\tint err = mvneta_rxq_init(pp, &pp->rxqs[queue]);\n\n\t\tif (err) {\n\t\t\tnetdev_err(pp->dev, \"%s: can't create rxq=%d\\n\",\n\t\t\t\t   __func__, queue);\n\t\t\tmvneta_cleanup_rxqs(pp);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int mvneta_setup_txqs(struct mvneta_port *pp)\n{\n\tint queue;\n\n\tfor (queue = 0; queue < txq_number; queue++) {\n\t\tint err = mvneta_txq_init(pp, &pp->txqs[queue]);\n\t\tif (err) {\n\t\t\tnetdev_err(pp->dev, \"%s: can't create txq=%d\\n\",\n\t\t\t\t   __func__, queue);\n\t\t\tmvneta_cleanup_txqs(pp);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int mvneta_comphy_init(struct mvneta_port *pp, phy_interface_t interface)\n{\n\tint ret;\n\n\tret = phy_set_mode_ext(pp->comphy, PHY_MODE_ETHERNET, interface);\n\tif (ret)\n\t\treturn ret;\n\n\treturn phy_power_on(pp->comphy);\n}\n\nstatic int mvneta_config_interface(struct mvneta_port *pp,\n\t\t\t\t   phy_interface_t interface)\n{\n\tint ret = 0;\n\n\tif (pp->comphy) {\n\t\tif (interface == PHY_INTERFACE_MODE_SGMII ||\n\t\t    interface == PHY_INTERFACE_MODE_1000BASEX ||\n\t\t    interface == PHY_INTERFACE_MODE_2500BASEX) {\n\t\t\tret = mvneta_comphy_init(pp, interface);\n\t\t}\n\t} else {\n\t\tswitch (interface) {\n\t\tcase PHY_INTERFACE_MODE_QSGMII:\n\t\t\tmvreg_write(pp, MVNETA_SERDES_CFG,\n\t\t\t\t    MVNETA_QSGMII_SERDES_PROTO);\n\t\t\tbreak;\n\n\t\tcase PHY_INTERFACE_MODE_SGMII:\n\t\tcase PHY_INTERFACE_MODE_1000BASEX:\n\t\t\tmvreg_write(pp, MVNETA_SERDES_CFG,\n\t\t\t\t    MVNETA_SGMII_SERDES_PROTO);\n\t\t\tbreak;\n\n\t\tcase PHY_INTERFACE_MODE_2500BASEX:\n\t\t\tmvreg_write(pp, MVNETA_SERDES_CFG,\n\t\t\t\t    MVNETA_HSGMII_SERDES_PROTO);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tpp->phy_interface = interface;\n\n\treturn ret;\n}\n\nstatic void mvneta_start_dev(struct mvneta_port *pp)\n{\n\tint cpu;\n\n\tWARN_ON(mvneta_config_interface(pp, pp->phy_interface));\n\n\tmvneta_max_rx_size_set(pp, pp->pkt_size);\n\tmvneta_txq_max_tx_size_set(pp, pp->pkt_size);\n\n\t \n\tmvneta_port_enable(pp);\n\n\tif (!pp->neta_armada3700) {\n\t\t \n\t\tfor_each_online_cpu(cpu) {\n\t\t\tstruct mvneta_pcpu_port *port =\n\t\t\t\tper_cpu_ptr(pp->ports, cpu);\n\n\t\t\tnapi_enable(&port->napi);\n\t\t}\n\t} else {\n\t\tnapi_enable(&pp->napi);\n\t}\n\n\t \n\ton_each_cpu(mvneta_percpu_unmask_interrupt, pp, true);\n\n\tmvreg_write(pp, MVNETA_INTR_MISC_MASK,\n\t\t    MVNETA_CAUSE_PHY_STATUS_CHANGE |\n\t\t    MVNETA_CAUSE_LINK_CHANGE);\n\n\tphylink_start(pp->phylink);\n\n\t \n\tphylink_speed_up(pp->phylink);\n\n\tnetif_tx_start_all_queues(pp->dev);\n\n\tclear_bit(__MVNETA_DOWN, &pp->state);\n}\n\nstatic void mvneta_stop_dev(struct mvneta_port *pp)\n{\n\tunsigned int cpu;\n\n\tset_bit(__MVNETA_DOWN, &pp->state);\n\n\tif (device_may_wakeup(&pp->dev->dev))\n\t\tphylink_speed_down(pp->phylink, false);\n\n\tphylink_stop(pp->phylink);\n\n\tif (!pp->neta_armada3700) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tstruct mvneta_pcpu_port *port =\n\t\t\t\tper_cpu_ptr(pp->ports, cpu);\n\n\t\t\tnapi_disable(&port->napi);\n\t\t}\n\t} else {\n\t\tnapi_disable(&pp->napi);\n\t}\n\n\tnetif_carrier_off(pp->dev);\n\n\tmvneta_port_down(pp);\n\tnetif_tx_stop_all_queues(pp->dev);\n\n\t \n\tmvneta_port_disable(pp);\n\n\t \n\ton_each_cpu(mvneta_percpu_clear_intr_cause, pp, true);\n\n\t \n\ton_each_cpu(mvneta_percpu_mask_interrupt, pp, true);\n\n\tmvneta_tx_reset(pp);\n\tmvneta_rx_reset(pp);\n\n\tWARN_ON(phy_power_off(pp->comphy));\n}\n\nstatic void mvneta_percpu_enable(void *arg)\n{\n\tstruct mvneta_port *pp = arg;\n\n\tenable_percpu_irq(pp->dev->irq, IRQ_TYPE_NONE);\n}\n\nstatic void mvneta_percpu_disable(void *arg)\n{\n\tstruct mvneta_port *pp = arg;\n\n\tdisable_percpu_irq(pp->dev->irq);\n}\n\n \nstatic int mvneta_change_mtu(struct net_device *dev, int mtu)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tstruct bpf_prog *prog = pp->xdp_prog;\n\tint ret;\n\n\tif (!IS_ALIGNED(MVNETA_RX_PKT_SIZE(mtu), 8)) {\n\t\tnetdev_info(dev, \"Illegal MTU value %d, rounding to %d\\n\",\n\t\t\t    mtu, ALIGN(MVNETA_RX_PKT_SIZE(mtu), 8));\n\t\tmtu = ALIGN(MVNETA_RX_PKT_SIZE(mtu), 8);\n\t}\n\n\tif (prog && !prog->aux->xdp_has_frags &&\n\t    mtu > MVNETA_MAX_RX_BUF_SIZE) {\n\t\tnetdev_info(dev, \"Illegal MTU %d for XDP prog without frags\\n\",\n\t\t\t    mtu);\n\n\t\treturn -EINVAL;\n\t}\n\n\tdev->mtu = mtu;\n\n\tif (!netif_running(dev)) {\n\t\tif (pp->bm_priv)\n\t\t\tmvneta_bm_update_mtu(pp, mtu);\n\n\t\tnetdev_update_features(dev);\n\t\treturn 0;\n\t}\n\n\t \n\tmvneta_stop_dev(pp);\n\ton_each_cpu(mvneta_percpu_disable, pp, true);\n\n\tmvneta_cleanup_txqs(pp);\n\tmvneta_cleanup_rxqs(pp);\n\n\tif (pp->bm_priv)\n\t\tmvneta_bm_update_mtu(pp, mtu);\n\n\tpp->pkt_size = MVNETA_RX_PKT_SIZE(dev->mtu);\n\n\tret = mvneta_setup_rxqs(pp);\n\tif (ret) {\n\t\tnetdev_err(dev, \"unable to setup rxqs after MTU change\\n\");\n\t\treturn ret;\n\t}\n\n\tret = mvneta_setup_txqs(pp);\n\tif (ret) {\n\t\tnetdev_err(dev, \"unable to setup txqs after MTU change\\n\");\n\t\treturn ret;\n\t}\n\n\ton_each_cpu(mvneta_percpu_enable, pp, true);\n\tmvneta_start_dev(pp);\n\n\tnetdev_update_features(dev);\n\n\treturn 0;\n}\n\nstatic netdev_features_t mvneta_fix_features(struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\tif (pp->tx_csum_limit && dev->mtu > pp->tx_csum_limit) {\n\t\tfeatures &= ~(NETIF_F_IP_CSUM | NETIF_F_TSO);\n\t\tnetdev_info(dev,\n\t\t\t    \"Disable IP checksum for MTU greater than %dB\\n\",\n\t\t\t    pp->tx_csum_limit);\n\t}\n\n\treturn features;\n}\n\n \nstatic void mvneta_get_mac_addr(struct mvneta_port *pp, unsigned char *addr)\n{\n\tu32 mac_addr_l, mac_addr_h;\n\n\tmac_addr_l = mvreg_read(pp, MVNETA_MAC_ADDR_LOW);\n\tmac_addr_h = mvreg_read(pp, MVNETA_MAC_ADDR_HIGH);\n\taddr[0] = (mac_addr_h >> 24) & 0xFF;\n\taddr[1] = (mac_addr_h >> 16) & 0xFF;\n\taddr[2] = (mac_addr_h >> 8) & 0xFF;\n\taddr[3] = mac_addr_h & 0xFF;\n\taddr[4] = (mac_addr_l >> 8) & 0xFF;\n\taddr[5] = mac_addr_l & 0xFF;\n}\n\n \nstatic int mvneta_set_mac_addr(struct net_device *dev, void *addr)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tstruct sockaddr *sockaddr = addr;\n\tint ret;\n\n\tret = eth_prepare_mac_addr_change(dev, addr);\n\tif (ret < 0)\n\t\treturn ret;\n\t \n\tmvneta_mac_addr_set(pp, dev->dev_addr, -1);\n\n\t \n\tmvneta_mac_addr_set(pp, sockaddr->sa_data, pp->rxq_def);\n\n\teth_commit_mac_addr_change(dev, addr);\n\treturn 0;\n}\n\nstatic struct mvneta_port *mvneta_pcs_to_port(struct phylink_pcs *pcs)\n{\n\treturn container_of(pcs, struct mvneta_port, phylink_pcs);\n}\n\nstatic int mvneta_pcs_validate(struct phylink_pcs *pcs,\n\t\t\t       unsigned long *supported,\n\t\t\t       const struct phylink_link_state *state)\n{\n\t \n\tif (phy_interface_mode_is_8023z(state->interface) &&\n\t    !phylink_test(state->advertising, Autoneg))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void mvneta_pcs_get_state(struct phylink_pcs *pcs,\n\t\t\t\t struct phylink_link_state *state)\n{\n\tstruct mvneta_port *pp = mvneta_pcs_to_port(pcs);\n\tu32 gmac_stat;\n\n\tgmac_stat = mvreg_read(pp, MVNETA_GMAC_STATUS);\n\n\tif (gmac_stat & MVNETA_GMAC_SPEED_1000)\n\t\tstate->speed =\n\t\t\tstate->interface == PHY_INTERFACE_MODE_2500BASEX ?\n\t\t\tSPEED_2500 : SPEED_1000;\n\telse if (gmac_stat & MVNETA_GMAC_SPEED_100)\n\t\tstate->speed = SPEED_100;\n\telse\n\t\tstate->speed = SPEED_10;\n\n\tstate->an_complete = !!(gmac_stat & MVNETA_GMAC_AN_COMPLETE);\n\tstate->link = !!(gmac_stat & MVNETA_GMAC_LINK_UP);\n\tstate->duplex = !!(gmac_stat & MVNETA_GMAC_FULL_DUPLEX);\n\n\tif (gmac_stat & MVNETA_GMAC_RX_FLOW_CTRL_ENABLE)\n\t\tstate->pause |= MLO_PAUSE_RX;\n\tif (gmac_stat & MVNETA_GMAC_TX_FLOW_CTRL_ENABLE)\n\t\tstate->pause |= MLO_PAUSE_TX;\n}\n\nstatic int mvneta_pcs_config(struct phylink_pcs *pcs, unsigned int neg_mode,\n\t\t\t     phy_interface_t interface,\n\t\t\t     const unsigned long *advertising,\n\t\t\t     bool permit_pause_to_mac)\n{\n\tstruct mvneta_port *pp = mvneta_pcs_to_port(pcs);\n\tu32 mask, val, an, old_an, changed;\n\n\tmask = MVNETA_GMAC_INBAND_AN_ENABLE |\n\t       MVNETA_GMAC_INBAND_RESTART_AN |\n\t       MVNETA_GMAC_AN_SPEED_EN |\n\t       MVNETA_GMAC_AN_FLOW_CTRL_EN |\n\t       MVNETA_GMAC_AN_DUPLEX_EN;\n\n\tif (neg_mode == PHYLINK_PCS_NEG_INBAND_ENABLED) {\n\t\tmask |= MVNETA_GMAC_CONFIG_MII_SPEED |\n\t\t\tMVNETA_GMAC_CONFIG_GMII_SPEED |\n\t\t\tMVNETA_GMAC_CONFIG_FULL_DUPLEX;\n\t\tval = MVNETA_GMAC_INBAND_AN_ENABLE;\n\n\t\tif (interface == PHY_INTERFACE_MODE_SGMII) {\n\t\t\t \n\t\t\tval |= MVNETA_GMAC_AN_SPEED_EN |\n\t\t\t       MVNETA_GMAC_AN_DUPLEX_EN;\n\t\t} else {\n\t\t\t \n\t\t\tval |= MVNETA_GMAC_CONFIG_GMII_SPEED |\n\t\t\t       MVNETA_GMAC_CONFIG_FULL_DUPLEX;\n\n\t\t\t \n\t\t\tif (permit_pause_to_mac)\n\t\t\t\tval |= MVNETA_GMAC_AN_FLOW_CTRL_EN;\n\n\t\t\t \n\t\t\tmask |= MVNETA_GMAC_ADVERT_SYM_FLOW_CTRL;\n\t\t\tif (phylink_test(advertising, Pause))\n\t\t\t\tval |= MVNETA_GMAC_ADVERT_SYM_FLOW_CTRL;\n\t\t}\n\t} else {\n\t\t \n\t\tval = 0;\n\t}\n\n\told_an = an = mvreg_read(pp, MVNETA_GMAC_AUTONEG_CONFIG);\n\tan = (an & ~mask) | val;\n\tchanged = old_an ^ an;\n\tif (changed)\n\t\tmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG, an);\n\n\t \n\treturn !!(changed & MVNETA_GMAC_ADVERT_SYM_FLOW_CTRL);\n}\n\nstatic void mvneta_pcs_an_restart(struct phylink_pcs *pcs)\n{\n\tstruct mvneta_port *pp = mvneta_pcs_to_port(pcs);\n\tu32 gmac_an = mvreg_read(pp, MVNETA_GMAC_AUTONEG_CONFIG);\n\n\tmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG,\n\t\t    gmac_an | MVNETA_GMAC_INBAND_RESTART_AN);\n\tmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG,\n\t\t    gmac_an & ~MVNETA_GMAC_INBAND_RESTART_AN);\n}\n\nstatic const struct phylink_pcs_ops mvneta_phylink_pcs_ops = {\n\t.pcs_validate = mvneta_pcs_validate,\n\t.pcs_get_state = mvneta_pcs_get_state,\n\t.pcs_config = mvneta_pcs_config,\n\t.pcs_an_restart = mvneta_pcs_an_restart,\n};\n\nstatic struct phylink_pcs *mvneta_mac_select_pcs(struct phylink_config *config,\n\t\t\t\t\t\t phy_interface_t interface)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct mvneta_port *pp = netdev_priv(ndev);\n\n\treturn &pp->phylink_pcs;\n}\n\nstatic int mvneta_mac_prepare(struct phylink_config *config, unsigned int mode,\n\t\t\t      phy_interface_t interface)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct mvneta_port *pp = netdev_priv(ndev);\n\tu32 val;\n\n\tif (pp->phy_interface != interface ||\n\t    phylink_autoneg_inband(mode)) {\n\t\t \n\t\tval = mvreg_read(pp, MVNETA_GMAC_AUTONEG_CONFIG);\n\t\tval &= ~MVNETA_GMAC_FORCE_LINK_PASS;\n\t\tval |= MVNETA_GMAC_FORCE_LINK_DOWN;\n\t\tmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG, val);\n\t}\n\n\tif (pp->phy_interface != interface)\n\t\tWARN_ON(phy_power_off(pp->comphy));\n\n\t \n\tif (phylink_autoneg_inband(mode)) {\n\t\tunsigned long rate = clk_get_rate(pp->clk);\n\n\t\tmvreg_write(pp, MVNETA_GMAC_CLOCK_DIVIDER,\n\t\t\t    MVNETA_GMAC_1MS_CLOCK_ENABLE | (rate / 1000));\n\t}\n\n\treturn 0;\n}\n\nstatic void mvneta_mac_config(struct phylink_config *config, unsigned int mode,\n\t\t\t      const struct phylink_link_state *state)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct mvneta_port *pp = netdev_priv(ndev);\n\tu32 new_ctrl0, gmac_ctrl0 = mvreg_read(pp, MVNETA_GMAC_CTRL_0);\n\tu32 new_ctrl2, gmac_ctrl2 = mvreg_read(pp, MVNETA_GMAC_CTRL_2);\n\tu32 new_ctrl4, gmac_ctrl4 = mvreg_read(pp, MVNETA_GMAC_CTRL_4);\n\n\tnew_ctrl0 = gmac_ctrl0 & ~MVNETA_GMAC0_PORT_1000BASE_X;\n\tnew_ctrl2 = gmac_ctrl2 & ~(MVNETA_GMAC2_INBAND_AN_ENABLE |\n\t\t\t\t   MVNETA_GMAC2_PORT_RESET);\n\tnew_ctrl4 = gmac_ctrl4 & ~(MVNETA_GMAC4_SHORT_PREAMBLE_ENABLE);\n\n\t \n\tnew_ctrl2 |= MVNETA_GMAC2_PORT_RGMII;\n\n\tif (state->interface == PHY_INTERFACE_MODE_QSGMII ||\n\t    state->interface == PHY_INTERFACE_MODE_SGMII ||\n\t    phy_interface_mode_is_8023z(state->interface))\n\t\tnew_ctrl2 |= MVNETA_GMAC2_PCS_ENABLE;\n\n\tif (!phylink_autoneg_inband(mode)) {\n\t\t \n\t} else if (state->interface == PHY_INTERFACE_MODE_SGMII) {\n\t\t \n\t\tnew_ctrl2 |= MVNETA_GMAC2_INBAND_AN_ENABLE;\n\t} else {\n\t\t \n\t\tnew_ctrl0 |= MVNETA_GMAC0_PORT_1000BASE_X;\n\t}\n\n\t \n\tif (state->interface == PHY_INTERFACE_MODE_2500BASEX)\n\t\tnew_ctrl4 |= MVNETA_GMAC4_SHORT_PREAMBLE_ENABLE;\n\n\tif (new_ctrl0 != gmac_ctrl0)\n\t\tmvreg_write(pp, MVNETA_GMAC_CTRL_0, new_ctrl0);\n\tif (new_ctrl2 != gmac_ctrl2)\n\t\tmvreg_write(pp, MVNETA_GMAC_CTRL_2, new_ctrl2);\n\tif (new_ctrl4 != gmac_ctrl4)\n\t\tmvreg_write(pp, MVNETA_GMAC_CTRL_4, new_ctrl4);\n\n\tif (gmac_ctrl2 & MVNETA_GMAC2_PORT_RESET) {\n\t\twhile ((mvreg_read(pp, MVNETA_GMAC_CTRL_2) &\n\t\t\tMVNETA_GMAC2_PORT_RESET) != 0)\n\t\t\tcontinue;\n\t}\n}\n\nstatic int mvneta_mac_finish(struct phylink_config *config, unsigned int mode,\n\t\t\t     phy_interface_t interface)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct mvneta_port *pp = netdev_priv(ndev);\n\tu32 val, clk;\n\n\t \n\tif (!phylink_autoneg_inband(mode)) {\n\t\tclk = mvreg_read(pp, MVNETA_GMAC_CLOCK_DIVIDER);\n\t\tclk &= ~MVNETA_GMAC_1MS_CLOCK_ENABLE;\n\t\tmvreg_write(pp, MVNETA_GMAC_CLOCK_DIVIDER, clk);\n\t}\n\n\tif (pp->phy_interface != interface)\n\t\t \n\t\tWARN_ON(mvneta_config_interface(pp, interface));\n\n\t \n\tif (phylink_autoneg_inband(mode)) {\n\t\tval = mvreg_read(pp, MVNETA_GMAC_AUTONEG_CONFIG);\n\t\tval &= ~MVNETA_GMAC_FORCE_LINK_DOWN;\n\t\tmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG, val);\n\t}\n\n\treturn 0;\n}\n\nstatic void mvneta_set_eee(struct mvneta_port *pp, bool enable)\n{\n\tu32 lpi_ctl1;\n\n\tlpi_ctl1 = mvreg_read(pp, MVNETA_LPI_CTRL_1);\n\tif (enable)\n\t\tlpi_ctl1 |= MVNETA_LPI_REQUEST_ENABLE;\n\telse\n\t\tlpi_ctl1 &= ~MVNETA_LPI_REQUEST_ENABLE;\n\tmvreg_write(pp, MVNETA_LPI_CTRL_1, lpi_ctl1);\n}\n\nstatic void mvneta_mac_link_down(struct phylink_config *config,\n\t\t\t\t unsigned int mode, phy_interface_t interface)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct mvneta_port *pp = netdev_priv(ndev);\n\tu32 val;\n\n\tmvneta_port_down(pp);\n\n\tif (!phylink_autoneg_inband(mode)) {\n\t\tval = mvreg_read(pp, MVNETA_GMAC_AUTONEG_CONFIG);\n\t\tval &= ~MVNETA_GMAC_FORCE_LINK_PASS;\n\t\tval |= MVNETA_GMAC_FORCE_LINK_DOWN;\n\t\tmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG, val);\n\t}\n\n\tpp->eee_active = false;\n\tmvneta_set_eee(pp, false);\n}\n\nstatic void mvneta_mac_link_up(struct phylink_config *config,\n\t\t\t       struct phy_device *phy,\n\t\t\t       unsigned int mode, phy_interface_t interface,\n\t\t\t       int speed, int duplex,\n\t\t\t       bool tx_pause, bool rx_pause)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct mvneta_port *pp = netdev_priv(ndev);\n\tu32 val;\n\n\tif (!phylink_autoneg_inband(mode)) {\n\t\tval = mvreg_read(pp, MVNETA_GMAC_AUTONEG_CONFIG);\n\t\tval &= ~(MVNETA_GMAC_FORCE_LINK_DOWN |\n\t\t\t MVNETA_GMAC_CONFIG_MII_SPEED |\n\t\t\t MVNETA_GMAC_CONFIG_GMII_SPEED |\n\t\t\t MVNETA_GMAC_CONFIG_FLOW_CTRL |\n\t\t\t MVNETA_GMAC_CONFIG_FULL_DUPLEX);\n\t\tval |= MVNETA_GMAC_FORCE_LINK_PASS;\n\n\t\tif (speed == SPEED_1000 || speed == SPEED_2500)\n\t\t\tval |= MVNETA_GMAC_CONFIG_GMII_SPEED;\n\t\telse if (speed == SPEED_100)\n\t\t\tval |= MVNETA_GMAC_CONFIG_MII_SPEED;\n\n\t\tif (duplex == DUPLEX_FULL)\n\t\t\tval |= MVNETA_GMAC_CONFIG_FULL_DUPLEX;\n\n\t\tif (tx_pause || rx_pause)\n\t\t\tval |= MVNETA_GMAC_CONFIG_FLOW_CTRL;\n\n\t\tmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG, val);\n\t} else {\n\t\t \n\t\tval = mvreg_read(pp, MVNETA_GMAC_AUTONEG_CONFIG);\n\t\tval &= ~MVNETA_GMAC_CONFIG_FLOW_CTRL;\n\n\t\tif (tx_pause || rx_pause)\n\t\t\tval |= MVNETA_GMAC_CONFIG_FLOW_CTRL;\n\n\t\tmvreg_write(pp, MVNETA_GMAC_AUTONEG_CONFIG, val);\n\t}\n\n\tmvneta_port_up(pp);\n\n\tif (phy && pp->eee_enabled) {\n\t\tpp->eee_active = phy_init_eee(phy, false) >= 0;\n\t\tmvneta_set_eee(pp, pp->eee_active && pp->tx_lpi_enabled);\n\t}\n}\n\nstatic const struct phylink_mac_ops mvneta_phylink_ops = {\n\t.mac_select_pcs = mvneta_mac_select_pcs,\n\t.mac_prepare = mvneta_mac_prepare,\n\t.mac_config = mvneta_mac_config,\n\t.mac_finish = mvneta_mac_finish,\n\t.mac_link_down = mvneta_mac_link_down,\n\t.mac_link_up = mvneta_mac_link_up,\n};\n\nstatic int mvneta_mdio_probe(struct mvneta_port *pp)\n{\n\tstruct ethtool_wolinfo wol = { .cmd = ETHTOOL_GWOL };\n\tint err = phylink_of_phy_connect(pp->phylink, pp->dn, 0);\n\n\tif (err)\n\t\tnetdev_err(pp->dev, \"could not attach PHY: %d\\n\", err);\n\n\tphylink_ethtool_get_wol(pp->phylink, &wol);\n\tdevice_set_wakeup_capable(&pp->dev->dev, !!wol.supported);\n\n\t \n\tif (wol.supported)\n\t\tdevice_set_wakeup_enable(&pp->dev->dev, !!wol.wolopts);\n\n\treturn err;\n}\n\nstatic void mvneta_mdio_remove(struct mvneta_port *pp)\n{\n\tphylink_disconnect_phy(pp->phylink);\n}\n\n \nstatic void mvneta_percpu_elect(struct mvneta_port *pp)\n{\n\tint elected_cpu = 0, max_cpu, cpu;\n\n\t \n\tif (pp->rxq_def < nr_cpu_ids && cpu_online(pp->rxq_def))\n\t\telected_cpu = pp->rxq_def;\n\n\tmax_cpu = num_present_cpus();\n\n\tfor_each_online_cpu(cpu) {\n\t\tint rxq_map = 0, txq_map = 0;\n\t\tint rxq;\n\n\t\tfor (rxq = 0; rxq < rxq_number; rxq++)\n\t\t\tif ((rxq % max_cpu) == cpu)\n\t\t\t\trxq_map |= MVNETA_CPU_RXQ_ACCESS(rxq);\n\n\t\tif (cpu == elected_cpu)\n\t\t\t \n\t\t\trxq_map |= MVNETA_CPU_RXQ_ACCESS(pp->rxq_def);\n\n\t\t \n\t\tif (txq_number == 1)\n\t\t\ttxq_map = (cpu == elected_cpu) ?\n\t\t\t\tMVNETA_CPU_TXQ_ACCESS(0) : 0;\n\t\telse\n\t\t\ttxq_map = mvreg_read(pp, MVNETA_CPU_MAP(cpu)) &\n\t\t\t\tMVNETA_CPU_TXQ_ACCESS_ALL_MASK;\n\n\t\tmvreg_write(pp, MVNETA_CPU_MAP(cpu), rxq_map | txq_map);\n\n\t\t \n\t\tsmp_call_function_single(cpu, mvneta_percpu_unmask_interrupt,\n\t\t\t\t\t pp, true);\n\t}\n};\n\nstatic int mvneta_cpu_online(unsigned int cpu, struct hlist_node *node)\n{\n\tint other_cpu;\n\tstruct mvneta_port *pp = hlist_entry_safe(node, struct mvneta_port,\n\t\t\t\t\t\t  node_online);\n\tstruct mvneta_pcpu_port *port = per_cpu_ptr(pp->ports, cpu);\n\n\t \n\tif (pp->neta_armada3700)\n\t\treturn 0;\n\n\tspin_lock(&pp->lock);\n\t \n\tif (pp->is_stopped) {\n\t\tspin_unlock(&pp->lock);\n\t\treturn 0;\n\t}\n\tnetif_tx_stop_all_queues(pp->dev);\n\n\t \n\tfor_each_online_cpu(other_cpu) {\n\t\tif (other_cpu != cpu) {\n\t\t\tstruct mvneta_pcpu_port *other_port =\n\t\t\t\tper_cpu_ptr(pp->ports, other_cpu);\n\n\t\t\tnapi_synchronize(&other_port->napi);\n\t\t}\n\t}\n\n\t \n\ton_each_cpu(mvneta_percpu_mask_interrupt, pp, true);\n\tnapi_enable(&port->napi);\n\n\t \n\tmvneta_percpu_enable(pp);\n\n\t \n\tmvneta_percpu_elect(pp);\n\n\t \n\ton_each_cpu(mvneta_percpu_unmask_interrupt, pp, true);\n\tmvreg_write(pp, MVNETA_INTR_MISC_MASK,\n\t\t    MVNETA_CAUSE_PHY_STATUS_CHANGE |\n\t\t    MVNETA_CAUSE_LINK_CHANGE);\n\tnetif_tx_start_all_queues(pp->dev);\n\tspin_unlock(&pp->lock);\n\treturn 0;\n}\n\nstatic int mvneta_cpu_down_prepare(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct mvneta_port *pp = hlist_entry_safe(node, struct mvneta_port,\n\t\t\t\t\t\t  node_online);\n\tstruct mvneta_pcpu_port *port = per_cpu_ptr(pp->ports, cpu);\n\n\t \n\tspin_lock(&pp->lock);\n\t \n\ton_each_cpu(mvneta_percpu_mask_interrupt, pp, true);\n\tspin_unlock(&pp->lock);\n\n\tnapi_synchronize(&port->napi);\n\tnapi_disable(&port->napi);\n\t \n\tmvneta_percpu_disable(pp);\n\treturn 0;\n}\n\nstatic int mvneta_cpu_dead(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct mvneta_port *pp = hlist_entry_safe(node, struct mvneta_port,\n\t\t\t\t\t\t  node_dead);\n\n\t \n\tspin_lock(&pp->lock);\n\tmvneta_percpu_elect(pp);\n\tspin_unlock(&pp->lock);\n\t \n\ton_each_cpu(mvneta_percpu_unmask_interrupt, pp, true);\n\tmvreg_write(pp, MVNETA_INTR_MISC_MASK,\n\t\t    MVNETA_CAUSE_PHY_STATUS_CHANGE |\n\t\t    MVNETA_CAUSE_LINK_CHANGE);\n\tnetif_tx_start_all_queues(pp->dev);\n\treturn 0;\n}\n\nstatic int mvneta_open(struct net_device *dev)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tint ret;\n\n\tpp->pkt_size = MVNETA_RX_PKT_SIZE(pp->dev->mtu);\n\n\tret = mvneta_setup_rxqs(pp);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mvneta_setup_txqs(pp);\n\tif (ret)\n\t\tgoto err_cleanup_rxqs;\n\n\t \n\tif (pp->neta_armada3700)\n\t\tret = request_irq(pp->dev->irq, mvneta_isr, 0,\n\t\t\t\t  dev->name, pp);\n\telse\n\t\tret = request_percpu_irq(pp->dev->irq, mvneta_percpu_isr,\n\t\t\t\t\t dev->name, pp->ports);\n\tif (ret) {\n\t\tnetdev_err(pp->dev, \"cannot request irq %d\\n\", pp->dev->irq);\n\t\tgoto err_cleanup_txqs;\n\t}\n\n\tif (!pp->neta_armada3700) {\n\t\t \n\t\ton_each_cpu(mvneta_percpu_enable, pp, true);\n\n\t\tpp->is_stopped = false;\n\t\t \n\t\tret = cpuhp_state_add_instance_nocalls(online_hpstate,\n\t\t\t\t\t\t       &pp->node_online);\n\t\tif (ret)\n\t\t\tgoto err_free_irq;\n\n\t\tret = cpuhp_state_add_instance_nocalls(CPUHP_NET_MVNETA_DEAD,\n\t\t\t\t\t\t       &pp->node_dead);\n\t\tif (ret)\n\t\t\tgoto err_free_online_hp;\n\t}\n\n\tret = mvneta_mdio_probe(pp);\n\tif (ret < 0) {\n\t\tnetdev_err(dev, \"cannot probe MDIO bus\\n\");\n\t\tgoto err_free_dead_hp;\n\t}\n\n\tmvneta_start_dev(pp);\n\n\treturn 0;\n\nerr_free_dead_hp:\n\tif (!pp->neta_armada3700)\n\t\tcpuhp_state_remove_instance_nocalls(CPUHP_NET_MVNETA_DEAD,\n\t\t\t\t\t\t    &pp->node_dead);\nerr_free_online_hp:\n\tif (!pp->neta_armada3700)\n\t\tcpuhp_state_remove_instance_nocalls(online_hpstate,\n\t\t\t\t\t\t    &pp->node_online);\nerr_free_irq:\n\tif (pp->neta_armada3700) {\n\t\tfree_irq(pp->dev->irq, pp);\n\t} else {\n\t\ton_each_cpu(mvneta_percpu_disable, pp, true);\n\t\tfree_percpu_irq(pp->dev->irq, pp->ports);\n\t}\nerr_cleanup_txqs:\n\tmvneta_cleanup_txqs(pp);\nerr_cleanup_rxqs:\n\tmvneta_cleanup_rxqs(pp);\n\treturn ret;\n}\n\n \nstatic int mvneta_stop(struct net_device *dev)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\tif (!pp->neta_armada3700) {\n\t\t \n\t\tspin_lock(&pp->lock);\n\t\tpp->is_stopped = true;\n\t\tspin_unlock(&pp->lock);\n\n\t\tmvneta_stop_dev(pp);\n\t\tmvneta_mdio_remove(pp);\n\n\t\tcpuhp_state_remove_instance_nocalls(online_hpstate,\n\t\t\t\t\t\t    &pp->node_online);\n\t\tcpuhp_state_remove_instance_nocalls(CPUHP_NET_MVNETA_DEAD,\n\t\t\t\t\t\t    &pp->node_dead);\n\t\ton_each_cpu(mvneta_percpu_disable, pp, true);\n\t\tfree_percpu_irq(dev->irq, pp->ports);\n\t} else {\n\t\tmvneta_stop_dev(pp);\n\t\tmvneta_mdio_remove(pp);\n\t\tfree_irq(dev->irq, pp);\n\t}\n\n\tmvneta_cleanup_rxqs(pp);\n\tmvneta_cleanup_txqs(pp);\n\n\treturn 0;\n}\n\nstatic int mvneta_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\treturn phylink_mii_ioctl(pp->phylink, ifr, cmd);\n}\n\nstatic int mvneta_xdp_setup(struct net_device *dev, struct bpf_prog *prog,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tbool need_update, running = netif_running(dev);\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tstruct bpf_prog *old_prog;\n\n\tif (prog && !prog->aux->xdp_has_frags &&\n\t    dev->mtu > MVNETA_MAX_RX_BUF_SIZE) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"prog does not support XDP frags\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (pp->bm_priv) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Hardware Buffer Management not supported on XDP\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tneed_update = !!pp->xdp_prog != !!prog;\n\tif (running && need_update)\n\t\tmvneta_stop(dev);\n\n\told_prog = xchg(&pp->xdp_prog, prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\tif (running && need_update)\n\t\treturn mvneta_open(dev);\n\n\treturn 0;\n}\n\nstatic int mvneta_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn mvneta_xdp_setup(dev, xdp->prog, xdp->extack);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n \n\n \nstatic int\nmvneta_ethtool_set_link_ksettings(struct net_device *ndev,\n\t\t\t\t  const struct ethtool_link_ksettings *cmd)\n{\n\tstruct mvneta_port *pp = netdev_priv(ndev);\n\n\treturn phylink_ethtool_ksettings_set(pp->phylink, cmd);\n}\n\n \nstatic int\nmvneta_ethtool_get_link_ksettings(struct net_device *ndev,\n\t\t\t\t  struct ethtool_link_ksettings *cmd)\n{\n\tstruct mvneta_port *pp = netdev_priv(ndev);\n\n\treturn phylink_ethtool_ksettings_get(pp->phylink, cmd);\n}\n\nstatic int mvneta_ethtool_nway_reset(struct net_device *dev)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\treturn phylink_ethtool_nway_reset(pp->phylink);\n}\n\n \nstatic int\nmvneta_ethtool_set_coalesce(struct net_device *dev,\n\t\t\t    struct ethtool_coalesce *c,\n\t\t\t    struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tint queue;\n\n\tfor (queue = 0; queue < rxq_number; queue++) {\n\t\tstruct mvneta_rx_queue *rxq = &pp->rxqs[queue];\n\t\trxq->time_coal = c->rx_coalesce_usecs;\n\t\trxq->pkts_coal = c->rx_max_coalesced_frames;\n\t\tmvneta_rx_pkts_coal_set(pp, rxq, rxq->pkts_coal);\n\t\tmvneta_rx_time_coal_set(pp, rxq, rxq->time_coal);\n\t}\n\n\tfor (queue = 0; queue < txq_number; queue++) {\n\t\tstruct mvneta_tx_queue *txq = &pp->txqs[queue];\n\t\ttxq->done_pkts_coal = c->tx_max_coalesced_frames;\n\t\tmvneta_tx_done_pkts_coal_set(pp, txq, txq->done_pkts_coal);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\nmvneta_ethtool_get_coalesce(struct net_device *dev,\n\t\t\t    struct ethtool_coalesce *c,\n\t\t\t    struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\tc->rx_coalesce_usecs        = pp->rxqs[0].time_coal;\n\tc->rx_max_coalesced_frames  = pp->rxqs[0].pkts_coal;\n\n\tc->tx_max_coalesced_frames =  pp->txqs[0].done_pkts_coal;\n\treturn 0;\n}\n\n\nstatic void mvneta_ethtool_get_drvinfo(struct net_device *dev,\n\t\t\t\t    struct ethtool_drvinfo *drvinfo)\n{\n\tstrscpy(drvinfo->driver, MVNETA_DRIVER_NAME,\n\t\tsizeof(drvinfo->driver));\n\tstrscpy(drvinfo->version, MVNETA_DRIVER_VERSION,\n\t\tsizeof(drvinfo->version));\n\tstrscpy(drvinfo->bus_info, dev_name(&dev->dev),\n\t\tsizeof(drvinfo->bus_info));\n}\n\n\nstatic void\nmvneta_ethtool_get_ringparam(struct net_device *netdev,\n\t\t\t     struct ethtool_ringparam *ring,\n\t\t\t     struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct mvneta_port *pp = netdev_priv(netdev);\n\n\tring->rx_max_pending = MVNETA_MAX_RXD;\n\tring->tx_max_pending = MVNETA_MAX_TXD;\n\tring->rx_pending = pp->rx_ring_size;\n\tring->tx_pending = pp->tx_ring_size;\n}\n\nstatic int\nmvneta_ethtool_set_ringparam(struct net_device *dev,\n\t\t\t     struct ethtool_ringparam *ring,\n\t\t\t     struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\tif ((ring->rx_pending == 0) || (ring->tx_pending == 0))\n\t\treturn -EINVAL;\n\tpp->rx_ring_size = ring->rx_pending < MVNETA_MAX_RXD ?\n\t\tring->rx_pending : MVNETA_MAX_RXD;\n\n\tpp->tx_ring_size = clamp_t(u16, ring->tx_pending,\n\t\t\t\t   MVNETA_MAX_SKB_DESCS * 2, MVNETA_MAX_TXD);\n\tif (pp->tx_ring_size != ring->tx_pending)\n\t\tnetdev_warn(dev, \"TX queue size set to %u (requested %u)\\n\",\n\t\t\t    pp->tx_ring_size, ring->tx_pending);\n\n\tif (netif_running(dev)) {\n\t\tmvneta_stop(dev);\n\t\tif (mvneta_open(dev)) {\n\t\t\tnetdev_err(dev,\n\t\t\t\t   \"error on opening device after ring param change\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void mvneta_ethtool_get_pauseparam(struct net_device *dev,\n\t\t\t\t\t  struct ethtool_pauseparam *pause)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\tphylink_ethtool_get_pauseparam(pp->phylink, pause);\n}\n\nstatic int mvneta_ethtool_set_pauseparam(struct net_device *dev,\n\t\t\t\t\t struct ethtool_pauseparam *pause)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\treturn phylink_ethtool_set_pauseparam(pp->phylink, pause);\n}\n\nstatic void mvneta_ethtool_get_strings(struct net_device *netdev, u32 sset,\n\t\t\t\t       u8 *data)\n{\n\tif (sset == ETH_SS_STATS) {\n\t\tstruct mvneta_port *pp = netdev_priv(netdev);\n\t\tint i;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(mvneta_statistics); i++)\n\t\t\tmemcpy(data + i * ETH_GSTRING_LEN,\n\t\t\t       mvneta_statistics[i].name, ETH_GSTRING_LEN);\n\n\t\tif (!pp->bm_priv) {\n\t\t\tdata += ETH_GSTRING_LEN * ARRAY_SIZE(mvneta_statistics);\n\t\t\tpage_pool_ethtool_stats_get_strings(data);\n\t\t}\n\t}\n}\n\nstatic void\nmvneta_ethtool_update_pcpu_stats(struct mvneta_port *pp,\n\t\t\t\t struct mvneta_ethtool_stats *es)\n{\n\tunsigned int start;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct mvneta_pcpu_stats *stats;\n\t\tu64 skb_alloc_error;\n\t\tu64 refill_error;\n\t\tu64 xdp_redirect;\n\t\tu64 xdp_xmit_err;\n\t\tu64 xdp_tx_err;\n\t\tu64 xdp_pass;\n\t\tu64 xdp_drop;\n\t\tu64 xdp_xmit;\n\t\tu64 xdp_tx;\n\n\t\tstats = per_cpu_ptr(pp->stats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&stats->syncp);\n\t\t\tskb_alloc_error = stats->es.skb_alloc_error;\n\t\t\trefill_error = stats->es.refill_error;\n\t\t\txdp_redirect = stats->es.ps.xdp_redirect;\n\t\t\txdp_pass = stats->es.ps.xdp_pass;\n\t\t\txdp_drop = stats->es.ps.xdp_drop;\n\t\t\txdp_xmit = stats->es.ps.xdp_xmit;\n\t\t\txdp_xmit_err = stats->es.ps.xdp_xmit_err;\n\t\t\txdp_tx = stats->es.ps.xdp_tx;\n\t\t\txdp_tx_err = stats->es.ps.xdp_tx_err;\n\t\t} while (u64_stats_fetch_retry(&stats->syncp, start));\n\n\t\tes->skb_alloc_error += skb_alloc_error;\n\t\tes->refill_error += refill_error;\n\t\tes->ps.xdp_redirect += xdp_redirect;\n\t\tes->ps.xdp_pass += xdp_pass;\n\t\tes->ps.xdp_drop += xdp_drop;\n\t\tes->ps.xdp_xmit += xdp_xmit;\n\t\tes->ps.xdp_xmit_err += xdp_xmit_err;\n\t\tes->ps.xdp_tx += xdp_tx;\n\t\tes->ps.xdp_tx_err += xdp_tx_err;\n\t}\n}\n\nstatic void mvneta_ethtool_update_stats(struct mvneta_port *pp)\n{\n\tstruct mvneta_ethtool_stats stats = {};\n\tconst struct mvneta_statistic *s;\n\tvoid __iomem *base = pp->base;\n\tu32 high, low;\n\tu64 val;\n\tint i;\n\n\tmvneta_ethtool_update_pcpu_stats(pp, &stats);\n\tfor (i = 0, s = mvneta_statistics;\n\t     s < mvneta_statistics + ARRAY_SIZE(mvneta_statistics);\n\t     s++, i++) {\n\t\tswitch (s->type) {\n\t\tcase T_REG_32:\n\t\t\tval = readl_relaxed(base + s->offset);\n\t\t\tpp->ethtool_stats[i] += val;\n\t\t\tbreak;\n\t\tcase T_REG_64:\n\t\t\t \n\t\t\tlow = readl_relaxed(base + s->offset);\n\t\t\thigh = readl_relaxed(base + s->offset + 4);\n\t\t\tval = (u64)high << 32 | low;\n\t\t\tpp->ethtool_stats[i] += val;\n\t\t\tbreak;\n\t\tcase T_SW:\n\t\t\tswitch (s->offset) {\n\t\t\tcase ETHTOOL_STAT_EEE_WAKEUP:\n\t\t\t\tval = phylink_get_eee_err(pp->phylink);\n\t\t\t\tpp->ethtool_stats[i] += val;\n\t\t\t\tbreak;\n\t\t\tcase ETHTOOL_STAT_SKB_ALLOC_ERR:\n\t\t\t\tpp->ethtool_stats[i] = stats.skb_alloc_error;\n\t\t\t\tbreak;\n\t\t\tcase ETHTOOL_STAT_REFILL_ERR:\n\t\t\t\tpp->ethtool_stats[i] = stats.refill_error;\n\t\t\t\tbreak;\n\t\t\tcase ETHTOOL_XDP_REDIRECT:\n\t\t\t\tpp->ethtool_stats[i] = stats.ps.xdp_redirect;\n\t\t\t\tbreak;\n\t\t\tcase ETHTOOL_XDP_PASS:\n\t\t\t\tpp->ethtool_stats[i] = stats.ps.xdp_pass;\n\t\t\t\tbreak;\n\t\t\tcase ETHTOOL_XDP_DROP:\n\t\t\t\tpp->ethtool_stats[i] = stats.ps.xdp_drop;\n\t\t\t\tbreak;\n\t\t\tcase ETHTOOL_XDP_TX:\n\t\t\t\tpp->ethtool_stats[i] = stats.ps.xdp_tx;\n\t\t\t\tbreak;\n\t\t\tcase ETHTOOL_XDP_TX_ERR:\n\t\t\t\tpp->ethtool_stats[i] = stats.ps.xdp_tx_err;\n\t\t\t\tbreak;\n\t\t\tcase ETHTOOL_XDP_XMIT:\n\t\t\t\tpp->ethtool_stats[i] = stats.ps.xdp_xmit;\n\t\t\t\tbreak;\n\t\t\tcase ETHTOOL_XDP_XMIT_ERR:\n\t\t\t\tpp->ethtool_stats[i] = stats.ps.xdp_xmit_err;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void mvneta_ethtool_pp_stats(struct mvneta_port *pp, u64 *data)\n{\n\tstruct page_pool_stats stats = {};\n\tint i;\n\n\tfor (i = 0; i < rxq_number; i++) {\n\t\tif (pp->rxqs[i].page_pool)\n\t\t\tpage_pool_get_stats(pp->rxqs[i].page_pool, &stats);\n\t}\n\n\tpage_pool_ethtool_stats_get(data, &stats);\n}\n\nstatic void mvneta_ethtool_get_stats(struct net_device *dev,\n\t\t\t\t     struct ethtool_stats *stats, u64 *data)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tint i;\n\n\tmvneta_ethtool_update_stats(pp);\n\n\tfor (i = 0; i < ARRAY_SIZE(mvneta_statistics); i++)\n\t\t*data++ = pp->ethtool_stats[i];\n\n\tif (!pp->bm_priv)\n\t\tmvneta_ethtool_pp_stats(pp, data);\n}\n\nstatic int mvneta_ethtool_get_sset_count(struct net_device *dev, int sset)\n{\n\tif (sset == ETH_SS_STATS) {\n\t\tint count = ARRAY_SIZE(mvneta_statistics);\n\t\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\t\tif (!pp->bm_priv)\n\t\t\tcount += page_pool_ethtool_stats_get_count();\n\n\t\treturn count;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic u32 mvneta_ethtool_get_rxfh_indir_size(struct net_device *dev)\n{\n\treturn MVNETA_RSS_LU_TABLE_SIZE;\n}\n\nstatic int mvneta_ethtool_get_rxnfc(struct net_device *dev,\n\t\t\t\t    struct ethtool_rxnfc *info,\n\t\t\t\t    u32 *rules __always_unused)\n{\n\tswitch (info->cmd) {\n\tcase ETHTOOL_GRXRINGS:\n\t\tinfo->data =  rxq_number;\n\t\treturn 0;\n\tcase ETHTOOL_GRXFH:\n\t\treturn -EOPNOTSUPP;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int  mvneta_config_rss(struct mvneta_port *pp)\n{\n\tint cpu;\n\tu32 val;\n\n\tnetif_tx_stop_all_queues(pp->dev);\n\n\ton_each_cpu(mvneta_percpu_mask_interrupt, pp, true);\n\n\tif (!pp->neta_armada3700) {\n\t\t \n\t\tfor_each_online_cpu(cpu) {\n\t\t\tstruct mvneta_pcpu_port *pcpu_port =\n\t\t\t\tper_cpu_ptr(pp->ports, cpu);\n\n\t\t\tnapi_synchronize(&pcpu_port->napi);\n\t\t\tnapi_disable(&pcpu_port->napi);\n\t\t}\n\t} else {\n\t\tnapi_synchronize(&pp->napi);\n\t\tnapi_disable(&pp->napi);\n\t}\n\n\tpp->rxq_def = pp->indir[0];\n\n\t \n\tmvneta_set_rx_mode(pp->dev);\n\n\t \n\tval = MVNETA_PORT_CONFIG_DEFL_VALUE(pp->rxq_def);\n\tmvreg_write(pp, MVNETA_PORT_CONFIG, val);\n\n\t \n\tspin_lock(&pp->lock);\n\tmvneta_percpu_elect(pp);\n\tspin_unlock(&pp->lock);\n\n\tif (!pp->neta_armada3700) {\n\t\t \n\t\tfor_each_online_cpu(cpu) {\n\t\t\tstruct mvneta_pcpu_port *pcpu_port =\n\t\t\t\tper_cpu_ptr(pp->ports, cpu);\n\n\t\t\tnapi_enable(&pcpu_port->napi);\n\t\t}\n\t} else {\n\t\tnapi_enable(&pp->napi);\n\t}\n\n\tnetif_tx_start_all_queues(pp->dev);\n\n\treturn 0;\n}\n\nstatic int mvneta_ethtool_set_rxfh(struct net_device *dev, const u32 *indir,\n\t\t\t\t   const u8 *key, const u8 hfunc)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\t \n\tif (pp->neta_armada3700)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (key ||\n\t    (hfunc != ETH_RSS_HASH_NO_CHANGE && hfunc != ETH_RSS_HASH_TOP))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!indir)\n\t\treturn 0;\n\n\tmemcpy(pp->indir, indir, MVNETA_RSS_LU_TABLE_SIZE);\n\n\treturn mvneta_config_rss(pp);\n}\n\nstatic int mvneta_ethtool_get_rxfh(struct net_device *dev, u32 *indir, u8 *key,\n\t\t\t\t   u8 *hfunc)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\t \n\tif (pp->neta_armada3700)\n\t\treturn -EOPNOTSUPP;\n\n\tif (hfunc)\n\t\t*hfunc = ETH_RSS_HASH_TOP;\n\n\tif (!indir)\n\t\treturn 0;\n\n\tmemcpy(indir, pp->indir, MVNETA_RSS_LU_TABLE_SIZE);\n\n\treturn 0;\n}\n\nstatic void mvneta_ethtool_get_wol(struct net_device *dev,\n\t\t\t\t   struct ethtool_wolinfo *wol)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\tphylink_ethtool_get_wol(pp->phylink, wol);\n}\n\nstatic int mvneta_ethtool_set_wol(struct net_device *dev,\n\t\t\t\t  struct ethtool_wolinfo *wol)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tint ret;\n\n\tret = phylink_ethtool_set_wol(pp->phylink, wol);\n\tif (!ret)\n\t\tdevice_set_wakeup_enable(&dev->dev, !!wol->wolopts);\n\n\treturn ret;\n}\n\nstatic int mvneta_ethtool_get_eee(struct net_device *dev,\n\t\t\t\t  struct ethtool_eee *eee)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tu32 lpi_ctl0;\n\n\tlpi_ctl0 = mvreg_read(pp, MVNETA_LPI_CTRL_0);\n\n\teee->eee_enabled = pp->eee_enabled;\n\teee->eee_active = pp->eee_active;\n\teee->tx_lpi_enabled = pp->tx_lpi_enabled;\n\teee->tx_lpi_timer = (lpi_ctl0) >> 8;  \n\n\treturn phylink_ethtool_get_eee(pp->phylink, eee);\n}\n\nstatic int mvneta_ethtool_set_eee(struct net_device *dev,\n\t\t\t\t  struct ethtool_eee *eee)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tu32 lpi_ctl0;\n\n\t \n\tif (eee->tx_lpi_enabled && eee->tx_lpi_timer > 255)\n\t\treturn -EINVAL;\n\n\tlpi_ctl0 = mvreg_read(pp, MVNETA_LPI_CTRL_0);\n\tlpi_ctl0 &= ~(0xff << 8);\n\tlpi_ctl0 |= eee->tx_lpi_timer << 8;\n\tmvreg_write(pp, MVNETA_LPI_CTRL_0, lpi_ctl0);\n\n\tpp->eee_enabled = eee->eee_enabled;\n\tpp->tx_lpi_enabled = eee->tx_lpi_enabled;\n\n\tmvneta_set_eee(pp, eee->tx_lpi_enabled && eee->eee_enabled);\n\n\treturn phylink_ethtool_set_eee(pp->phylink, eee);\n}\n\nstatic void mvneta_clear_rx_prio_map(struct mvneta_port *pp)\n{\n\tmvreg_write(pp, MVNETA_VLAN_PRIO_TO_RXQ, 0);\n}\n\nstatic void mvneta_map_vlan_prio_to_rxq(struct mvneta_port *pp, u8 pri, u8 rxq)\n{\n\tu32 val = mvreg_read(pp, MVNETA_VLAN_PRIO_TO_RXQ);\n\n\tval &= ~MVNETA_VLAN_PRIO_RXQ_MAP(pri, 0x7);\n\tval |= MVNETA_VLAN_PRIO_RXQ_MAP(pri, rxq);\n\n\tmvreg_write(pp, MVNETA_VLAN_PRIO_TO_RXQ, val);\n}\n\nstatic int mvneta_enable_per_queue_rate_limit(struct mvneta_port *pp)\n{\n\tunsigned long core_clk_rate;\n\tu32 refill_cycles;\n\tu32 val;\n\n\tcore_clk_rate = clk_get_rate(pp->clk);\n\tif (!core_clk_rate)\n\t\treturn -EINVAL;\n\n\trefill_cycles = MVNETA_TXQ_BUCKET_REFILL_BASE_PERIOD_NS /\n\t\t\t(NSEC_PER_SEC / core_clk_rate);\n\n\tif (refill_cycles > MVNETA_REFILL_MAX_NUM_CLK)\n\t\treturn -EINVAL;\n\n\t \n\tval = mvreg_read(pp, MVNETA_TXQ_CMD1_REG);\n\tval &= ~(MVNETA_TXQ_CMD1_BW_LIM_SEL_V1 | MVNETA_TXQ_CMD1_BW_LIM_EN);\n\tmvreg_write(pp, MVNETA_TXQ_CMD1_REG, val);\n\n\t \n\tmvreg_write(pp, MVNETA_REFILL_NUM_CLK_REG, refill_cycles);\n\n\treturn 0;\n}\n\nstatic void mvneta_disable_per_queue_rate_limit(struct mvneta_port *pp)\n{\n\tu32 val = mvreg_read(pp, MVNETA_TXQ_CMD1_REG);\n\n\tval |= (MVNETA_TXQ_CMD1_BW_LIM_SEL_V1 | MVNETA_TXQ_CMD1_BW_LIM_EN);\n\tmvreg_write(pp, MVNETA_TXQ_CMD1_REG, val);\n}\n\nstatic int mvneta_setup_queue_rates(struct mvneta_port *pp, int queue,\n\t\t\t\t    u64 min_rate, u64 max_rate)\n{\n\tu32 refill_val, rem;\n\tu32 val = 0;\n\n\t \n\tmax_rate *= 8;\n\n\tif (min_rate)\n\t\treturn -EINVAL;\n\n\trefill_val = div_u64_rem(max_rate, MVNETA_TXQ_RATE_LIMIT_RESOLUTION,\n\t\t\t\t &rem);\n\n\tif (rem || !refill_val ||\n\t    refill_val > MVNETA_TXQ_BUCKET_REFILL_VALUE_MAX)\n\t\treturn -EINVAL;\n\n\tval = refill_val;\n\tval |= (MVNETA_TXQ_BUCKET_REFILL_PERIOD <<\n\t\tMVNETA_TXQ_BUCKET_REFILL_PERIOD_SHIFT);\n\n\tmvreg_write(pp, MVNETA_TXQ_BUCKET_REFILL_REG(queue), val);\n\n\treturn 0;\n}\n\nstatic int mvneta_setup_mqprio(struct net_device *dev,\n\t\t\t       struct tc_mqprio_qopt_offload *mqprio)\n{\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tint rxq, txq, tc, ret;\n\tu8 num_tc;\n\n\tif (mqprio->qopt.hw != TC_MQPRIO_HW_OFFLOAD_TCS)\n\t\treturn 0;\n\n\tnum_tc = mqprio->qopt.num_tc;\n\n\tif (num_tc > rxq_number)\n\t\treturn -EINVAL;\n\n\tmvneta_clear_rx_prio_map(pp);\n\n\tif (!num_tc) {\n\t\tmvneta_disable_per_queue_rate_limit(pp);\n\t\tnetdev_reset_tc(dev);\n\t\treturn 0;\n\t}\n\n\tnetdev_set_num_tc(dev, mqprio->qopt.num_tc);\n\n\tfor (tc = 0; tc < mqprio->qopt.num_tc; tc++) {\n\t\tnetdev_set_tc_queue(dev, tc, mqprio->qopt.count[tc],\n\t\t\t\t    mqprio->qopt.offset[tc]);\n\n\t\tfor (rxq = mqprio->qopt.offset[tc];\n\t\t     rxq < mqprio->qopt.count[tc] + mqprio->qopt.offset[tc];\n\t\t     rxq++) {\n\t\t\tif (rxq >= rxq_number)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tmvneta_map_vlan_prio_to_rxq(pp, tc, rxq);\n\t\t}\n\t}\n\n\tif (mqprio->shaper != TC_MQPRIO_SHAPER_BW_RATE) {\n\t\tmvneta_disable_per_queue_rate_limit(pp);\n\t\treturn 0;\n\t}\n\n\tif (mqprio->qopt.num_tc > txq_number)\n\t\treturn -EINVAL;\n\n\tret = mvneta_enable_per_queue_rate_limit(pp);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (tc = 0; tc < mqprio->qopt.num_tc; tc++) {\n\t\tfor (txq = mqprio->qopt.offset[tc];\n\t\t     txq < mqprio->qopt.count[tc] + mqprio->qopt.offset[tc];\n\t\t     txq++) {\n\t\t\tif (txq >= txq_number)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tret = mvneta_setup_queue_rates(pp, txq,\n\t\t\t\t\t\t       mqprio->min_rate[tc],\n\t\t\t\t\t\t       mqprio->max_rate[tc]);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int mvneta_setup_tc(struct net_device *dev, enum tc_setup_type type,\n\t\t\t   void *type_data)\n{\n\tswitch (type) {\n\tcase TC_SETUP_QDISC_MQPRIO:\n\t\treturn mvneta_setup_mqprio(dev, type_data);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic const struct net_device_ops mvneta_netdev_ops = {\n\t.ndo_open            = mvneta_open,\n\t.ndo_stop            = mvneta_stop,\n\t.ndo_start_xmit      = mvneta_tx,\n\t.ndo_set_rx_mode     = mvneta_set_rx_mode,\n\t.ndo_set_mac_address = mvneta_set_mac_addr,\n\t.ndo_change_mtu      = mvneta_change_mtu,\n\t.ndo_fix_features    = mvneta_fix_features,\n\t.ndo_get_stats64     = mvneta_get_stats64,\n\t.ndo_eth_ioctl        = mvneta_ioctl,\n\t.ndo_bpf\t     = mvneta_xdp,\n\t.ndo_xdp_xmit        = mvneta_xdp_xmit,\n\t.ndo_setup_tc\t     = mvneta_setup_tc,\n};\n\nstatic const struct ethtool_ops mvneta_eth_tool_ops = {\n\t.supported_coalesce_params = ETHTOOL_COALESCE_RX_USECS |\n\t\t\t\t     ETHTOOL_COALESCE_MAX_FRAMES,\n\t.nway_reset\t= mvneta_ethtool_nway_reset,\n\t.get_link       = ethtool_op_get_link,\n\t.set_coalesce   = mvneta_ethtool_set_coalesce,\n\t.get_coalesce   = mvneta_ethtool_get_coalesce,\n\t.get_drvinfo    = mvneta_ethtool_get_drvinfo,\n\t.get_ringparam  = mvneta_ethtool_get_ringparam,\n\t.set_ringparam\t= mvneta_ethtool_set_ringparam,\n\t.get_pauseparam\t= mvneta_ethtool_get_pauseparam,\n\t.set_pauseparam\t= mvneta_ethtool_set_pauseparam,\n\t.get_strings\t= mvneta_ethtool_get_strings,\n\t.get_ethtool_stats = mvneta_ethtool_get_stats,\n\t.get_sset_count\t= mvneta_ethtool_get_sset_count,\n\t.get_rxfh_indir_size = mvneta_ethtool_get_rxfh_indir_size,\n\t.get_rxnfc\t= mvneta_ethtool_get_rxnfc,\n\t.get_rxfh\t= mvneta_ethtool_get_rxfh,\n\t.set_rxfh\t= mvneta_ethtool_set_rxfh,\n\t.get_link_ksettings = mvneta_ethtool_get_link_ksettings,\n\t.set_link_ksettings = mvneta_ethtool_set_link_ksettings,\n\t.get_wol        = mvneta_ethtool_get_wol,\n\t.set_wol        = mvneta_ethtool_set_wol,\n\t.get_eee\t= mvneta_ethtool_get_eee,\n\t.set_eee\t= mvneta_ethtool_set_eee,\n};\n\n \nstatic int mvneta_init(struct device *dev, struct mvneta_port *pp)\n{\n\tint queue;\n\n\t \n\tmvneta_port_disable(pp);\n\n\t \n\tmvneta_defaults_set(pp);\n\n\tpp->txqs = devm_kcalloc(dev, txq_number, sizeof(*pp->txqs), GFP_KERNEL);\n\tif (!pp->txqs)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (queue = 0; queue < txq_number; queue++) {\n\t\tstruct mvneta_tx_queue *txq = &pp->txqs[queue];\n\t\ttxq->id = queue;\n\t\ttxq->size = pp->tx_ring_size;\n\t\ttxq->done_pkts_coal = MVNETA_TXDONE_COAL_PKTS;\n\t}\n\n\tpp->rxqs = devm_kcalloc(dev, rxq_number, sizeof(*pp->rxqs), GFP_KERNEL);\n\tif (!pp->rxqs)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (queue = 0; queue < rxq_number; queue++) {\n\t\tstruct mvneta_rx_queue *rxq = &pp->rxqs[queue];\n\t\trxq->id = queue;\n\t\trxq->size = pp->rx_ring_size;\n\t\trxq->pkts_coal = MVNETA_RX_COAL_PKTS;\n\t\trxq->time_coal = MVNETA_RX_COAL_USEC;\n\t\trxq->buf_virt_addr\n\t\t\t= devm_kmalloc_array(pp->dev->dev.parent,\n\t\t\t\t\t     rxq->size,\n\t\t\t\t\t     sizeof(*rxq->buf_virt_addr),\n\t\t\t\t\t     GFP_KERNEL);\n\t\tif (!rxq->buf_virt_addr)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void mvneta_conf_mbus_windows(struct mvneta_port *pp,\n\t\t\t\t     const struct mbus_dram_target_info *dram)\n{\n\tu32 win_enable;\n\tu32 win_protect;\n\tint i;\n\n\tfor (i = 0; i < 6; i++) {\n\t\tmvreg_write(pp, MVNETA_WIN_BASE(i), 0);\n\t\tmvreg_write(pp, MVNETA_WIN_SIZE(i), 0);\n\n\t\tif (i < 4)\n\t\t\tmvreg_write(pp, MVNETA_WIN_REMAP(i), 0);\n\t}\n\n\twin_enable = 0x3f;\n\twin_protect = 0;\n\n\tif (dram) {\n\t\tfor (i = 0; i < dram->num_cs; i++) {\n\t\t\tconst struct mbus_dram_window *cs = dram->cs + i;\n\n\t\t\tmvreg_write(pp, MVNETA_WIN_BASE(i),\n\t\t\t\t    (cs->base & 0xffff0000) |\n\t\t\t\t    (cs->mbus_attr << 8) |\n\t\t\t\t    dram->mbus_dram_target_id);\n\n\t\t\tmvreg_write(pp, MVNETA_WIN_SIZE(i),\n\t\t\t\t    (cs->size - 1) & 0xffff0000);\n\n\t\t\twin_enable &= ~(1 << i);\n\t\t\twin_protect |= 3 << (2 * i);\n\t\t}\n\t} else {\n\t\tif (pp->neta_ac5)\n\t\t\tmvreg_write(pp, MVNETA_WIN_BASE(0),\n\t\t\t\t    (MVNETA_AC5_CNM_DDR_ATTR << 8) |\n\t\t\t\t    MVNETA_AC5_CNM_DDR_TARGET);\n\t\t \n\t\tmvreg_write(pp, MVNETA_WIN_SIZE(0), 0xffff0000);\n\t\twin_enable &= ~BIT(0);\n\t\twin_protect = 3;\n\t}\n\n\tmvreg_write(pp, MVNETA_BASE_ADDR_ENABLE, win_enable);\n\tmvreg_write(pp, MVNETA_ACCESS_PROTECT_ENABLE, win_protect);\n}\n\n \nstatic int mvneta_port_power_up(struct mvneta_port *pp, int phy_mode)\n{\n\t \n\tmvreg_write(pp, MVNETA_UNIT_INTR_CAUSE, 0);\n\n\tif (phy_mode != PHY_INTERFACE_MODE_QSGMII &&\n\t    phy_mode != PHY_INTERFACE_MODE_SGMII &&\n\t    !phy_interface_mode_is_8023z(phy_mode) &&\n\t    !phy_interface_mode_is_rgmii(phy_mode))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n \nstatic int mvneta_probe(struct platform_device *pdev)\n{\n\tstruct device_node *dn = pdev->dev.of_node;\n\tstruct device_node *bm_node;\n\tstruct mvneta_port *pp;\n\tstruct net_device *dev;\n\tstruct phylink *phylink;\n\tstruct phy *comphy;\n\tchar hw_mac_addr[ETH_ALEN];\n\tphy_interface_t phy_mode;\n\tconst char *mac_from;\n\tint tx_csum_limit;\n\tint err;\n\tint cpu;\n\n\tdev = devm_alloc_etherdev_mqs(&pdev->dev, sizeof(struct mvneta_port),\n\t\t\t\t      txq_number, rxq_number);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tdev->tx_queue_len = MVNETA_MAX_TXD;\n\tdev->watchdog_timeo = 5 * HZ;\n\tdev->netdev_ops = &mvneta_netdev_ops;\n\tdev->ethtool_ops = &mvneta_eth_tool_ops;\n\n\tpp = netdev_priv(dev);\n\tspin_lock_init(&pp->lock);\n\tpp->dn = dn;\n\n\tpp->rxq_def = rxq_def;\n\tpp->indir[0] = rxq_def;\n\n\terr = of_get_phy_mode(dn, &phy_mode);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"incorrect phy-mode\\n\");\n\t\treturn err;\n\t}\n\n\tpp->phy_interface = phy_mode;\n\n\tcomphy = devm_of_phy_get(&pdev->dev, dn, NULL);\n\tif (comphy == ERR_PTR(-EPROBE_DEFER))\n\t\treturn -EPROBE_DEFER;\n\n\tif (IS_ERR(comphy))\n\t\tcomphy = NULL;\n\n\tpp->comphy = comphy;\n\n\tpp->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(pp->base))\n\t\treturn PTR_ERR(pp->base);\n\n\t \n\tif (of_device_is_compatible(dn, \"marvell,armada-3700-neta\"))\n\t\tpp->neta_armada3700 = true;\n\tif (of_device_is_compatible(dn, \"marvell,armada-ac5-neta\")) {\n\t\tpp->neta_armada3700 = true;\n\t\tpp->neta_ac5 = true;\n\t}\n\n\tdev->irq = irq_of_parse_and_map(dn, 0);\n\tif (dev->irq == 0)\n\t\treturn -EINVAL;\n\n\tpp->clk = devm_clk_get(&pdev->dev, \"core\");\n\tif (IS_ERR(pp->clk))\n\t\tpp->clk = devm_clk_get(&pdev->dev, NULL);\n\tif (IS_ERR(pp->clk)) {\n\t\terr = PTR_ERR(pp->clk);\n\t\tgoto err_free_irq;\n\t}\n\n\tclk_prepare_enable(pp->clk);\n\n\tpp->clk_bus = devm_clk_get(&pdev->dev, \"bus\");\n\tif (!IS_ERR(pp->clk_bus))\n\t\tclk_prepare_enable(pp->clk_bus);\n\n\tpp->phylink_pcs.ops = &mvneta_phylink_pcs_ops;\n\tpp->phylink_pcs.neg_mode = true;\n\n\tpp->phylink_config.dev = &dev->dev;\n\tpp->phylink_config.type = PHYLINK_NETDEV;\n\tpp->phylink_config.mac_capabilities = MAC_SYM_PAUSE | MAC_10 |\n\t\tMAC_100 | MAC_1000FD | MAC_2500FD;\n\n\tphy_interface_set_rgmii(pp->phylink_config.supported_interfaces);\n\t__set_bit(PHY_INTERFACE_MODE_QSGMII,\n\t\t  pp->phylink_config.supported_interfaces);\n\tif (comphy) {\n\t\t \n\t\t__set_bit(PHY_INTERFACE_MODE_SGMII,\n\t\t\t  pp->phylink_config.supported_interfaces);\n\t\t__set_bit(PHY_INTERFACE_MODE_1000BASEX,\n\t\t\t  pp->phylink_config.supported_interfaces);\n\t\t__set_bit(PHY_INTERFACE_MODE_2500BASEX,\n\t\t\t  pp->phylink_config.supported_interfaces);\n\t} else if (phy_mode == PHY_INTERFACE_MODE_2500BASEX) {\n\t\t \n\t\t__set_bit(PHY_INTERFACE_MODE_2500BASEX,\n\t\t\t  pp->phylink_config.supported_interfaces);\n\t} else if (phy_mode == PHY_INTERFACE_MODE_1000BASEX ||\n\t\t   phy_mode == PHY_INTERFACE_MODE_SGMII) {\n\t\t \n\t\t__set_bit(PHY_INTERFACE_MODE_1000BASEX,\n\t\t\t  pp->phylink_config.supported_interfaces);\n\t\t__set_bit(PHY_INTERFACE_MODE_SGMII,\n\t\t\t  pp->phylink_config.supported_interfaces);\n\t}\n\n\tphylink = phylink_create(&pp->phylink_config, pdev->dev.fwnode,\n\t\t\t\t phy_mode, &mvneta_phylink_ops);\n\tif (IS_ERR(phylink)) {\n\t\terr = PTR_ERR(phylink);\n\t\tgoto err_clk;\n\t}\n\n\tpp->phylink = phylink;\n\n\t \n\tpp->ports = alloc_percpu(struct mvneta_pcpu_port);\n\tif (!pp->ports) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_phylink;\n\t}\n\n\t \n\tpp->stats = netdev_alloc_pcpu_stats(struct mvneta_pcpu_stats);\n\tif (!pp->stats) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_ports;\n\t}\n\n\terr = of_get_ethdev_address(dn, dev);\n\tif (!err) {\n\t\tmac_from = \"device tree\";\n\t} else {\n\t\tmvneta_get_mac_addr(pp, hw_mac_addr);\n\t\tif (is_valid_ether_addr(hw_mac_addr)) {\n\t\t\tmac_from = \"hardware\";\n\t\t\teth_hw_addr_set(dev, hw_mac_addr);\n\t\t} else {\n\t\t\tmac_from = \"random\";\n\t\t\teth_hw_addr_random(dev);\n\t\t}\n\t}\n\n\tif (!of_property_read_u32(dn, \"tx-csum-limit\", &tx_csum_limit)) {\n\t\tif (tx_csum_limit < 0 ||\n\t\t    tx_csum_limit > MVNETA_TX_CSUM_MAX_SIZE) {\n\t\t\ttx_csum_limit = MVNETA_TX_CSUM_DEF_SIZE;\n\t\t\tdev_info(&pdev->dev,\n\t\t\t\t \"Wrong TX csum limit in DT, set to %dB\\n\",\n\t\t\t\t MVNETA_TX_CSUM_DEF_SIZE);\n\t\t}\n\t} else if (of_device_is_compatible(dn, \"marvell,armada-370-neta\")) {\n\t\ttx_csum_limit = MVNETA_TX_CSUM_DEF_SIZE;\n\t} else {\n\t\ttx_csum_limit = MVNETA_TX_CSUM_MAX_SIZE;\n\t}\n\n\tpp->tx_csum_limit = tx_csum_limit;\n\n\tpp->dram_target_info = mv_mbus_dram_info();\n\t \n\tif (pp->dram_target_info || pp->neta_armada3700)\n\t\tmvneta_conf_mbus_windows(pp, pp->dram_target_info);\n\n\tpp->tx_ring_size = MVNETA_MAX_TXD;\n\tpp->rx_ring_size = MVNETA_MAX_RXD;\n\n\tpp->dev = dev;\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\n\tpp->id = global_port_id++;\n\n\t \n\tbm_node = of_parse_phandle(dn, \"buffer-manager\", 0);\n\tif (bm_node) {\n\t\tpp->bm_priv = mvneta_bm_get(bm_node);\n\t\tif (pp->bm_priv) {\n\t\t\terr = mvneta_bm_port_init(pdev, pp);\n\t\t\tif (err < 0) {\n\t\t\t\tdev_info(&pdev->dev,\n\t\t\t\t\t \"use SW buffer management\\n\");\n\t\t\t\tmvneta_bm_put(pp->bm_priv);\n\t\t\t\tpp->bm_priv = NULL;\n\t\t\t}\n\t\t}\n\t\t \n\t\tpp->rx_offset_correction = max(0,\n\t\t\t\t\t       NET_SKB_PAD -\n\t\t\t\t\t       MVNETA_RX_PKT_OFFSET_CORRECTION);\n\t}\n\tof_node_put(bm_node);\n\n\t \n\tif (!pp->bm_priv)\n\t\tpp->rx_offset_correction = MVNETA_SKB_HEADROOM;\n\n\terr = mvneta_init(&pdev->dev, pp);\n\tif (err < 0)\n\t\tgoto err_netdev;\n\n\terr = mvneta_port_power_up(pp, pp->phy_interface);\n\tif (err < 0) {\n\t\tdev_err(&pdev->dev, \"can't power up port\\n\");\n\t\tgoto err_netdev;\n\t}\n\n\t \n\tif (pp->neta_armada3700) {\n\t\tnetif_napi_add(dev, &pp->napi, mvneta_poll);\n\t} else {\n\t\tfor_each_present_cpu(cpu) {\n\t\t\tstruct mvneta_pcpu_port *port =\n\t\t\t\tper_cpu_ptr(pp->ports, cpu);\n\n\t\t\tnetif_napi_add(dev, &port->napi, mvneta_poll);\n\t\t\tport->pp = pp;\n\t\t}\n\t}\n\n\tdev->features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\tNETIF_F_TSO | NETIF_F_RXCSUM;\n\tdev->hw_features |= dev->features;\n\tdev->vlan_features |= dev->features;\n\tif (!pp->bm_priv)\n\t\tdev->xdp_features = NETDEV_XDP_ACT_BASIC |\n\t\t\t\t    NETDEV_XDP_ACT_REDIRECT |\n\t\t\t\t    NETDEV_XDP_ACT_NDO_XMIT |\n\t\t\t\t    NETDEV_XDP_ACT_RX_SG |\n\t\t\t\t    NETDEV_XDP_ACT_NDO_XMIT_SG;\n\tdev->priv_flags |= IFF_LIVE_ADDR_CHANGE;\n\tnetif_set_tso_max_segs(dev, MVNETA_MAX_TSO_SEGS);\n\n\t \n\tdev->min_mtu = ETH_MIN_MTU;\n\t \n\tdev->max_mtu = 9676;\n\n\terr = register_netdev(dev);\n\tif (err < 0) {\n\t\tdev_err(&pdev->dev, \"failed to register\\n\");\n\t\tgoto err_netdev;\n\t}\n\n\tnetdev_info(dev, \"Using %s mac address %pM\\n\", mac_from,\n\t\t    dev->dev_addr);\n\n\tplatform_set_drvdata(pdev, pp->dev);\n\n\treturn 0;\n\nerr_netdev:\n\tif (pp->bm_priv) {\n\t\tmvneta_bm_pool_destroy(pp->bm_priv, pp->pool_long, 1 << pp->id);\n\t\tmvneta_bm_pool_destroy(pp->bm_priv, pp->pool_short,\n\t\t\t\t       1 << pp->id);\n\t\tmvneta_bm_put(pp->bm_priv);\n\t}\n\tfree_percpu(pp->stats);\nerr_free_ports:\n\tfree_percpu(pp->ports);\nerr_free_phylink:\n\tif (pp->phylink)\n\t\tphylink_destroy(pp->phylink);\nerr_clk:\n\tclk_disable_unprepare(pp->clk_bus);\n\tclk_disable_unprepare(pp->clk);\nerr_free_irq:\n\tirq_dispose_mapping(dev->irq);\n\treturn err;\n}\n\n \nstatic int mvneta_remove(struct platform_device *pdev)\n{\n\tstruct net_device  *dev = platform_get_drvdata(pdev);\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\tunregister_netdev(dev);\n\tclk_disable_unprepare(pp->clk_bus);\n\tclk_disable_unprepare(pp->clk);\n\tfree_percpu(pp->ports);\n\tfree_percpu(pp->stats);\n\tirq_dispose_mapping(dev->irq);\n\tphylink_destroy(pp->phylink);\n\n\tif (pp->bm_priv) {\n\t\tmvneta_bm_pool_destroy(pp->bm_priv, pp->pool_long, 1 << pp->id);\n\t\tmvneta_bm_pool_destroy(pp->bm_priv, pp->pool_short,\n\t\t\t\t       1 << pp->id);\n\t\tmvneta_bm_put(pp->bm_priv);\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int mvneta_suspend(struct device *device)\n{\n\tint queue;\n\tstruct net_device *dev = dev_get_drvdata(device);\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\n\tif (!netif_running(dev))\n\t\tgoto clean_exit;\n\n\tif (!pp->neta_armada3700) {\n\t\tspin_lock(&pp->lock);\n\t\tpp->is_stopped = true;\n\t\tspin_unlock(&pp->lock);\n\n\t\tcpuhp_state_remove_instance_nocalls(online_hpstate,\n\t\t\t\t\t\t    &pp->node_online);\n\t\tcpuhp_state_remove_instance_nocalls(CPUHP_NET_MVNETA_DEAD,\n\t\t\t\t\t\t    &pp->node_dead);\n\t}\n\n\trtnl_lock();\n\tmvneta_stop_dev(pp);\n\trtnl_unlock();\n\n\tfor (queue = 0; queue < rxq_number; queue++) {\n\t\tstruct mvneta_rx_queue *rxq = &pp->rxqs[queue];\n\n\t\tmvneta_rxq_drop_pkts(pp, rxq);\n\t}\n\n\tfor (queue = 0; queue < txq_number; queue++) {\n\t\tstruct mvneta_tx_queue *txq = &pp->txqs[queue];\n\n\t\tmvneta_txq_hw_deinit(pp, txq);\n\t}\n\nclean_exit:\n\tnetif_device_detach(dev);\n\tclk_disable_unprepare(pp->clk_bus);\n\tclk_disable_unprepare(pp->clk);\n\n\treturn 0;\n}\n\nstatic int mvneta_resume(struct device *device)\n{\n\tstruct platform_device *pdev = to_platform_device(device);\n\tstruct net_device *dev = dev_get_drvdata(device);\n\tstruct mvneta_port *pp = netdev_priv(dev);\n\tint err, queue;\n\n\tclk_prepare_enable(pp->clk);\n\tif (!IS_ERR(pp->clk_bus))\n\t\tclk_prepare_enable(pp->clk_bus);\n\tif (pp->dram_target_info || pp->neta_armada3700)\n\t\tmvneta_conf_mbus_windows(pp, pp->dram_target_info);\n\tif (pp->bm_priv) {\n\t\terr = mvneta_bm_port_init(pdev, pp);\n\t\tif (err < 0) {\n\t\t\tdev_info(&pdev->dev, \"use SW buffer management\\n\");\n\t\t\tpp->rx_offset_correction = MVNETA_SKB_HEADROOM;\n\t\t\tpp->bm_priv = NULL;\n\t\t}\n\t}\n\tmvneta_defaults_set(pp);\n\terr = mvneta_port_power_up(pp, pp->phy_interface);\n\tif (err < 0) {\n\t\tdev_err(device, \"can't power up port\\n\");\n\t\treturn err;\n\t}\n\n\tnetif_device_attach(dev);\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tfor (queue = 0; queue < rxq_number; queue++) {\n\t\tstruct mvneta_rx_queue *rxq = &pp->rxqs[queue];\n\n\t\trxq->next_desc_to_proc = 0;\n\t\tmvneta_rxq_hw_init(pp, rxq);\n\t}\n\n\tfor (queue = 0; queue < txq_number; queue++) {\n\t\tstruct mvneta_tx_queue *txq = &pp->txqs[queue];\n\n\t\ttxq->next_desc_to_proc = 0;\n\t\tmvneta_txq_hw_init(pp, txq);\n\t}\n\n\tif (!pp->neta_armada3700) {\n\t\tspin_lock(&pp->lock);\n\t\tpp->is_stopped = false;\n\t\tspin_unlock(&pp->lock);\n\t\tcpuhp_state_add_instance_nocalls(online_hpstate,\n\t\t\t\t\t\t &pp->node_online);\n\t\tcpuhp_state_add_instance_nocalls(CPUHP_NET_MVNETA_DEAD,\n\t\t\t\t\t\t &pp->node_dead);\n\t}\n\n\trtnl_lock();\n\tmvneta_start_dev(pp);\n\trtnl_unlock();\n\tmvneta_set_rx_mode(dev);\n\n\treturn 0;\n}\n#endif\n\nstatic SIMPLE_DEV_PM_OPS(mvneta_pm_ops, mvneta_suspend, mvneta_resume);\n\nstatic const struct of_device_id mvneta_match[] = {\n\t{ .compatible = \"marvell,armada-370-neta\" },\n\t{ .compatible = \"marvell,armada-xp-neta\" },\n\t{ .compatible = \"marvell,armada-3700-neta\" },\n\t{ .compatible = \"marvell,armada-ac5-neta\" },\n\t{ }\n};\nMODULE_DEVICE_TABLE(of, mvneta_match);\n\nstatic struct platform_driver mvneta_driver = {\n\t.probe = mvneta_probe,\n\t.remove = mvneta_remove,\n\t.driver = {\n\t\t.name = MVNETA_DRIVER_NAME,\n\t\t.of_match_table = mvneta_match,\n\t\t.pm = &mvneta_pm_ops,\n\t},\n};\n\nstatic int __init mvneta_driver_init(void)\n{\n\tint ret;\n\n\tBUILD_BUG_ON_NOT_POWER_OF_2(MVNETA_TSO_PER_PAGE);\n\n\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN, \"net/mvneta:online\",\n\t\t\t\t      mvneta_cpu_online,\n\t\t\t\t      mvneta_cpu_down_prepare);\n\tif (ret < 0)\n\t\tgoto out;\n\tonline_hpstate = ret;\n\tret = cpuhp_setup_state_multi(CPUHP_NET_MVNETA_DEAD, \"net/mvneta:dead\",\n\t\t\t\t      NULL, mvneta_cpu_dead);\n\tif (ret)\n\t\tgoto err_dead;\n\n\tret = platform_driver_register(&mvneta_driver);\n\tif (ret)\n\t\tgoto err;\n\treturn 0;\n\nerr:\n\tcpuhp_remove_multi_state(CPUHP_NET_MVNETA_DEAD);\nerr_dead:\n\tcpuhp_remove_multi_state(online_hpstate);\nout:\n\treturn ret;\n}\nmodule_init(mvneta_driver_init);\n\nstatic void __exit mvneta_driver_exit(void)\n{\n\tplatform_driver_unregister(&mvneta_driver);\n\tcpuhp_remove_multi_state(CPUHP_NET_MVNETA_DEAD);\n\tcpuhp_remove_multi_state(online_hpstate);\n}\nmodule_exit(mvneta_driver_exit);\n\nMODULE_DESCRIPTION(\"Marvell NETA Ethernet Driver - www.marvell.com\");\nMODULE_AUTHOR(\"Rami Rosen <rosenr@marvell.com>, Thomas Petazzoni <thomas.petazzoni@free-electrons.com>\");\nMODULE_LICENSE(\"GPL\");\n\nmodule_param(rxq_number, int, 0444);\nmodule_param(txq_number, int, 0444);\n\nmodule_param(rxq_def, int, 0444);\nmodule_param(rx_copybreak, int, 0644);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}