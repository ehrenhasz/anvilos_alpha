{
  "module_name": "otx2_txrx.c",
  "hash_id": "70e7e118d743350289e87a0ba4982e8a4071f1398ebfe6ac079f98b6c4a84627",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/marvell/octeontx2/nic/otx2_txrx.c",
  "human_readable_source": "\n \n\n#include <linux/etherdevice.h>\n#include <net/ip.h>\n#include <net/tso.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/ip6_checksum.h>\n\n#include \"otx2_reg.h\"\n#include \"otx2_common.h\"\n#include \"otx2_struct.h\"\n#include \"otx2_txrx.h\"\n#include \"otx2_ptp.h\"\n#include \"cn10k.h\"\n\n#define CQE_ADDR(CQ, idx) ((CQ)->cqe_base + ((CQ)->cqe_size * (idx)))\n#define PTP_PORT\t        0x13F\n \n#define PTP_SYNC_SEC_OFFSET\t34\n\nstatic bool otx2_xdp_rcv_pkt_handler(struct otx2_nic *pfvf,\n\t\t\t\t     struct bpf_prog *prog,\n\t\t\t\t     struct nix_cqe_rx_s *cqe,\n\t\t\t\t     struct otx2_cq_queue *cq,\n\t\t\t\t     bool *need_xdp_flush);\n\nstatic int otx2_nix_cq_op_status(struct otx2_nic *pfvf,\n\t\t\t\t struct otx2_cq_queue *cq)\n{\n\tu64 incr = (u64)(cq->cq_idx) << 32;\n\tu64 status;\n\n\tstatus = otx2_atomic64_fetch_add(incr, pfvf->cq_op_addr);\n\n\tif (unlikely(status & BIT_ULL(CQ_OP_STAT_OP_ERR) ||\n\t\t     status & BIT_ULL(CQ_OP_STAT_CQ_ERR))) {\n\t\tdev_err(pfvf->dev, \"CQ stopped due to error\");\n\t\treturn -EINVAL;\n\t}\n\n\tcq->cq_tail = status & 0xFFFFF;\n\tcq->cq_head = (status >> 20) & 0xFFFFF;\n\tif (cq->cq_tail < cq->cq_head)\n\t\tcq->pend_cqe = (cq->cqe_cnt - cq->cq_head) +\n\t\t\t\tcq->cq_tail;\n\telse\n\t\tcq->pend_cqe = cq->cq_tail - cq->cq_head;\n\n\treturn 0;\n}\n\nstatic struct nix_cqe_hdr_s *otx2_get_next_cqe(struct otx2_cq_queue *cq)\n{\n\tstruct nix_cqe_hdr_s *cqe_hdr;\n\n\tcqe_hdr = (struct nix_cqe_hdr_s *)CQE_ADDR(cq, cq->cq_head);\n\tif (cqe_hdr->cqe_type == NIX_XQE_TYPE_INVALID)\n\t\treturn NULL;\n\n\tcq->cq_head++;\n\tcq->cq_head &= (cq->cqe_cnt - 1);\n\n\treturn cqe_hdr;\n}\n\nstatic unsigned int frag_num(unsigned int i)\n{\n#ifdef __BIG_ENDIAN\n\treturn (i & ~3) + 3 - (i & 3);\n#else\n\treturn i;\n#endif\n}\n\nstatic dma_addr_t otx2_dma_map_skb_frag(struct otx2_nic *pfvf,\n\t\t\t\t\tstruct sk_buff *skb, int seg, int *len)\n{\n\tconst skb_frag_t *frag;\n\tstruct page *page;\n\tint offset;\n\n\t \n\tif (!seg) {\n\t\tpage = virt_to_page(skb->data);\n\t\toffset = offset_in_page(skb->data);\n\t\t*len = skb_headlen(skb);\n\t} else {\n\t\tfrag = &skb_shinfo(skb)->frags[seg - 1];\n\t\tpage = skb_frag_page(frag);\n\t\toffset = skb_frag_off(frag);\n\t\t*len = skb_frag_size(frag);\n\t}\n\treturn otx2_dma_map_page(pfvf, page, offset, *len, DMA_TO_DEVICE);\n}\n\nstatic void otx2_dma_unmap_skb_frags(struct otx2_nic *pfvf, struct sg_list *sg)\n{\n\tint seg;\n\n\tfor (seg = 0; seg < sg->num_segs; seg++) {\n\t\totx2_dma_unmap_page(pfvf, sg->dma_addr[seg],\n\t\t\t\t    sg->size[seg], DMA_TO_DEVICE);\n\t}\n\tsg->num_segs = 0;\n}\n\nstatic void otx2_xdp_snd_pkt_handler(struct otx2_nic *pfvf,\n\t\t\t\t     struct otx2_snd_queue *sq,\n\t\t\t\t struct nix_cqe_tx_s *cqe)\n{\n\tstruct nix_send_comp_s *snd_comp = &cqe->comp;\n\tstruct sg_list *sg;\n\tstruct page *page;\n\tu64 pa;\n\n\tsg = &sq->sg[snd_comp->sqe_id];\n\n\tpa = otx2_iova_to_phys(pfvf->iommu_domain, sg->dma_addr[0]);\n\totx2_dma_unmap_page(pfvf, sg->dma_addr[0],\n\t\t\t    sg->size[0], DMA_TO_DEVICE);\n\tpage = virt_to_page(phys_to_virt(pa));\n\tput_page(page);\n}\n\nstatic void otx2_snd_pkt_handler(struct otx2_nic *pfvf,\n\t\t\t\t struct otx2_cq_queue *cq,\n\t\t\t\t struct otx2_snd_queue *sq,\n\t\t\t\t struct nix_cqe_tx_s *cqe,\n\t\t\t\t int budget, int *tx_pkts, int *tx_bytes)\n{\n\tstruct nix_send_comp_s *snd_comp = &cqe->comp;\n\tstruct skb_shared_hwtstamps ts;\n\tstruct sk_buff *skb = NULL;\n\tu64 timestamp, tsns;\n\tstruct sg_list *sg;\n\tint err;\n\n\tif (unlikely(snd_comp->status) && netif_msg_tx_err(pfvf))\n\t\tnet_err_ratelimited(\"%s: TX%d: Error in send CQ status:%x\\n\",\n\t\t\t\t    pfvf->netdev->name, cq->cint_idx,\n\t\t\t\t    snd_comp->status);\n\n\tsg = &sq->sg[snd_comp->sqe_id];\n\tskb = (struct sk_buff *)sg->skb;\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS) {\n\t\ttimestamp = ((u64 *)sq->timestamps->base)[snd_comp->sqe_id];\n\t\tif (timestamp != 1) {\n\t\t\ttimestamp = pfvf->ptp->convert_tx_ptp_tstmp(timestamp);\n\t\t\terr = otx2_ptp_tstamp2time(pfvf, timestamp, &tsns);\n\t\t\tif (!err) {\n\t\t\t\tmemset(&ts, 0, sizeof(ts));\n\t\t\t\tts.hwtstamp = ns_to_ktime(tsns);\n\t\t\t\tskb_tstamp_tx(skb, &ts);\n\t\t\t}\n\t\t}\n\t}\n\n\t*tx_bytes += skb->len;\n\t(*tx_pkts)++;\n\totx2_dma_unmap_skb_frags(pfvf, sg);\n\tnapi_consume_skb(skb, budget);\n\tsg->skb = (u64)NULL;\n}\n\nstatic void otx2_set_rxtstamp(struct otx2_nic *pfvf,\n\t\t\t      struct sk_buff *skb, void *data)\n{\n\tu64 timestamp, tsns;\n\tint err;\n\n\tif (!(pfvf->flags & OTX2_FLAG_RX_TSTAMP_ENABLED))\n\t\treturn;\n\n\ttimestamp = pfvf->ptp->convert_rx_ptp_tstmp(*(u64 *)data);\n\t \n\terr = otx2_ptp_tstamp2time(pfvf, timestamp, &tsns);\n\tif (err)\n\t\treturn;\n\n\tskb_hwtstamps(skb)->hwtstamp = ns_to_ktime(tsns);\n}\n\nstatic bool otx2_skb_add_frag(struct otx2_nic *pfvf, struct sk_buff *skb,\n\t\t\t      u64 iova, int len, struct nix_rx_parse_s *parse,\n\t\t\t      int qidx)\n{\n\tstruct page *page;\n\tint off = 0;\n\tvoid *va;\n\n\tva = phys_to_virt(otx2_iova_to_phys(pfvf->iommu_domain, iova));\n\n\tif (likely(!skb_shinfo(skb)->nr_frags)) {\n\t\t \n\t\tif (parse->laptr) {\n\t\t\totx2_set_rxtstamp(pfvf, skb, va);\n\t\t\toff = OTX2_HW_TIMESTAMP_LEN;\n\t\t}\n\t}\n\n\tpage = virt_to_page(va);\n\tif (likely(skb_shinfo(skb)->nr_frags < MAX_SKB_FRAGS)) {\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,\n\t\t\t\tva - page_address(page) + off,\n\t\t\t\tlen - off, pfvf->rbsize);\n\t\treturn true;\n\t}\n\n\t \n\tpfvf->hw_ops->aura_freeptr(pfvf, qidx, iova & ~0x07ULL);\n\n\treturn false;\n}\n\nstatic void otx2_set_rxhash(struct otx2_nic *pfvf,\n\t\t\t    struct nix_cqe_rx_s *cqe, struct sk_buff *skb)\n{\n\tenum pkt_hash_types hash_type = PKT_HASH_TYPE_NONE;\n\tstruct otx2_rss_info *rss;\n\tu32 hash = 0;\n\n\tif (!(pfvf->netdev->features & NETIF_F_RXHASH))\n\t\treturn;\n\n\trss = &pfvf->hw.rss_info;\n\tif (rss->flowkey_cfg) {\n\t\tif (rss->flowkey_cfg &\n\t\t    ~(NIX_FLOW_KEY_TYPE_IPV4 | NIX_FLOW_KEY_TYPE_IPV6))\n\t\t\thash_type = PKT_HASH_TYPE_L4;\n\t\telse\n\t\t\thash_type = PKT_HASH_TYPE_L3;\n\t\thash = cqe->hdr.flow_tag;\n\t}\n\tskb_set_hash(skb, hash, hash_type);\n}\n\nstatic void otx2_free_rcv_seg(struct otx2_nic *pfvf, struct nix_cqe_rx_s *cqe,\n\t\t\t      int qidx)\n{\n\tstruct nix_rx_sg_s *sg = &cqe->sg;\n\tvoid *end, *start;\n\tu64 *seg_addr;\n\tint seg;\n\n\tstart = (void *)sg;\n\tend = start + ((cqe->parse.desc_sizem1 + 1) * 16);\n\twhile (start < end) {\n\t\tsg = (struct nix_rx_sg_s *)start;\n\t\tseg_addr = &sg->seg_addr;\n\t\tfor (seg = 0; seg < sg->segs; seg++, seg_addr++)\n\t\t\tpfvf->hw_ops->aura_freeptr(pfvf, qidx,\n\t\t\t\t\t\t   *seg_addr & ~0x07ULL);\n\t\tstart += sizeof(*sg);\n\t}\n}\n\nstatic bool otx2_check_rcv_errors(struct otx2_nic *pfvf,\n\t\t\t\t  struct nix_cqe_rx_s *cqe, int qidx)\n{\n\tstruct otx2_drv_stats *stats = &pfvf->hw.drv_stats;\n\tstruct nix_rx_parse_s *parse = &cqe->parse;\n\n\tif (netif_msg_rx_err(pfvf))\n\t\tnetdev_err(pfvf->netdev,\n\t\t\t   \"RQ%d: Error pkt with errlev:0x%x errcode:0x%x\\n\",\n\t\t\t   qidx, parse->errlev, parse->errcode);\n\n\tif (parse->errlev == NPC_ERRLVL_RE) {\n\t\tswitch (parse->errcode) {\n\t\tcase ERRCODE_FCS:\n\t\tcase ERRCODE_FCS_RCV:\n\t\t\tatomic_inc(&stats->rx_fcs_errs);\n\t\t\tbreak;\n\t\tcase ERRCODE_UNDERSIZE:\n\t\t\tatomic_inc(&stats->rx_undersize_errs);\n\t\t\tbreak;\n\t\tcase ERRCODE_OVERSIZE:\n\t\t\tatomic_inc(&stats->rx_oversize_errs);\n\t\t\tbreak;\n\t\tcase ERRCODE_OL2_LEN_MISMATCH:\n\t\t\tatomic_inc(&stats->rx_len_errs);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tatomic_inc(&stats->rx_other_errs);\n\t\t\tbreak;\n\t\t}\n\t} else if (parse->errlev == NPC_ERRLVL_NIX) {\n\t\tswitch (parse->errcode) {\n\t\tcase ERRCODE_OL3_LEN:\n\t\tcase ERRCODE_OL4_LEN:\n\t\tcase ERRCODE_IL3_LEN:\n\t\tcase ERRCODE_IL4_LEN:\n\t\t\tatomic_inc(&stats->rx_len_errs);\n\t\t\tbreak;\n\t\tcase ERRCODE_OL4_CSUM:\n\t\tcase ERRCODE_IL4_CSUM:\n\t\t\tatomic_inc(&stats->rx_csum_errs);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tatomic_inc(&stats->rx_other_errs);\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tatomic_inc(&stats->rx_other_errs);\n\t\t \n\t\treturn false;\n\t}\n\n\t \n\tif (pfvf->netdev->features & NETIF_F_RXALL)\n\t\treturn false;\n\n\t \n\tif (cqe->sg.segs)\n\t\totx2_free_rcv_seg(pfvf, cqe, qidx);\n\treturn true;\n}\n\nstatic void otx2_rcv_pkt_handler(struct otx2_nic *pfvf,\n\t\t\t\t struct napi_struct *napi,\n\t\t\t\t struct otx2_cq_queue *cq,\n\t\t\t\t struct nix_cqe_rx_s *cqe, bool *need_xdp_flush)\n{\n\tstruct nix_rx_parse_s *parse = &cqe->parse;\n\tstruct nix_rx_sg_s *sg = &cqe->sg;\n\tstruct sk_buff *skb = NULL;\n\tvoid *end, *start;\n\tu64 *seg_addr;\n\tu16 *seg_size;\n\tint seg;\n\n\tif (unlikely(parse->errlev || parse->errcode)) {\n\t\tif (otx2_check_rcv_errors(pfvf, cqe, cq->cq_idx))\n\t\t\treturn;\n\t}\n\n\tif (pfvf->xdp_prog)\n\t\tif (otx2_xdp_rcv_pkt_handler(pfvf, pfvf->xdp_prog, cqe, cq, need_xdp_flush))\n\t\t\treturn;\n\n\tskb = napi_get_frags(napi);\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tstart = (void *)sg;\n\tend = start + ((cqe->parse.desc_sizem1 + 1) * 16);\n\twhile (start < end) {\n\t\tsg = (struct nix_rx_sg_s *)start;\n\t\tseg_addr = &sg->seg_addr;\n\t\tseg_size = (void *)sg;\n\t\tfor (seg = 0; seg < sg->segs; seg++, seg_addr++) {\n\t\t\tif (otx2_skb_add_frag(pfvf, skb, *seg_addr,\n\t\t\t\t\t      seg_size[seg], parse, cq->cq_idx))\n\t\t\t\tcq->pool_ptrs++;\n\t\t}\n\t\tstart += sizeof(*sg);\n\t}\n\totx2_set_rxhash(pfvf, cqe, skb);\n\n\tskb_record_rx_queue(skb, cq->cq_idx);\n\tif (pfvf->netdev->features & NETIF_F_RXCSUM)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tskb_mark_for_recycle(skb);\n\n\tnapi_gro_frags(napi);\n}\n\nstatic int otx2_rx_napi_handler(struct otx2_nic *pfvf,\n\t\t\t\tstruct napi_struct *napi,\n\t\t\t\tstruct otx2_cq_queue *cq, int budget)\n{\n\tbool need_xdp_flush = false;\n\tstruct nix_cqe_rx_s *cqe;\n\tint processed_cqe = 0;\n\n\tif (cq->pend_cqe >= budget)\n\t\tgoto process_cqe;\n\n\tif (otx2_nix_cq_op_status(pfvf, cq) || !cq->pend_cqe)\n\t\treturn 0;\n\nprocess_cqe:\n\twhile (likely(processed_cqe < budget) && cq->pend_cqe) {\n\t\tcqe = (struct nix_cqe_rx_s *)CQE_ADDR(cq, cq->cq_head);\n\t\tif (cqe->hdr.cqe_type == NIX_XQE_TYPE_INVALID ||\n\t\t    !cqe->sg.seg_addr) {\n\t\t\tif (!processed_cqe)\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\t}\n\t\tcq->cq_head++;\n\t\tcq->cq_head &= (cq->cqe_cnt - 1);\n\n\t\totx2_rcv_pkt_handler(pfvf, napi, cq, cqe, &need_xdp_flush);\n\n\t\tcqe->hdr.cqe_type = NIX_XQE_TYPE_INVALID;\n\t\tcqe->sg.seg_addr = 0x00;\n\t\tprocessed_cqe++;\n\t\tcq->pend_cqe--;\n\t}\n\tif (need_xdp_flush)\n\t\txdp_do_flush();\n\n\t \n\totx2_write64(pfvf, NIX_LF_CQ_OP_DOOR,\n\t\t     ((u64)cq->cq_idx << 32) | processed_cqe);\n\n\treturn processed_cqe;\n}\n\nint otx2_refill_pool_ptrs(void *dev, struct otx2_cq_queue *cq)\n{\n\tstruct otx2_nic *pfvf = dev;\n\tint cnt = cq->pool_ptrs;\n\tdma_addr_t bufptr;\n\n\twhile (cq->pool_ptrs) {\n\t\tif (otx2_alloc_buffer(pfvf, cq, &bufptr))\n\t\t\tbreak;\n\t\totx2_aura_freeptr(pfvf, cq->cq_idx, bufptr + OTX2_HEAD_ROOM);\n\t\tcq->pool_ptrs--;\n\t}\n\n\treturn cnt - cq->pool_ptrs;\n}\n\nstatic int otx2_tx_napi_handler(struct otx2_nic *pfvf,\n\t\t\t\tstruct otx2_cq_queue *cq, int budget)\n{\n\tint tx_pkts = 0, tx_bytes = 0, qidx;\n\tstruct otx2_snd_queue *sq;\n\tstruct nix_cqe_tx_s *cqe;\n\tint processed_cqe = 0;\n\n\tif (cq->pend_cqe >= budget)\n\t\tgoto process_cqe;\n\n\tif (otx2_nix_cq_op_status(pfvf, cq) || !cq->pend_cqe)\n\t\treturn 0;\n\nprocess_cqe:\n\tqidx = cq->cq_idx - pfvf->hw.rx_queues;\n\tsq = &pfvf->qset.sq[qidx];\n\n\twhile (likely(processed_cqe < budget) && cq->pend_cqe) {\n\t\tcqe = (struct nix_cqe_tx_s *)otx2_get_next_cqe(cq);\n\t\tif (unlikely(!cqe)) {\n\t\t\tif (!processed_cqe)\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tqidx = cq->cq_idx - pfvf->hw.rx_queues;\n\n\t\tif (cq->cq_type == CQ_XDP)\n\t\t\totx2_xdp_snd_pkt_handler(pfvf, sq, cqe);\n\t\telse\n\t\t\totx2_snd_pkt_handler(pfvf, cq, &pfvf->qset.sq[qidx],\n\t\t\t\t\t     cqe, budget, &tx_pkts, &tx_bytes);\n\n\t\tcqe->hdr.cqe_type = NIX_XQE_TYPE_INVALID;\n\t\tprocessed_cqe++;\n\t\tcq->pend_cqe--;\n\n\t\tsq->cons_head++;\n\t\tsq->cons_head &= (sq->sqe_cnt - 1);\n\t}\n\n\t \n\totx2_write64(pfvf, NIX_LF_CQ_OP_DOOR,\n\t\t     ((u64)cq->cq_idx << 32) | processed_cqe);\n\n\tif (likely(tx_pkts)) {\n\t\tstruct netdev_queue *txq;\n\n\t\tqidx = cq->cq_idx - pfvf->hw.rx_queues;\n\n\t\tif (qidx >= pfvf->hw.tx_queues)\n\t\t\tqidx -= pfvf->hw.xdp_queues;\n\t\ttxq = netdev_get_tx_queue(pfvf->netdev, qidx);\n\t\tnetdev_tx_completed_queue(txq, tx_pkts, tx_bytes);\n\t\t \n\t\tsmp_mb();\n\t\tif (netif_tx_queue_stopped(txq) &&\n\t\t    netif_carrier_ok(pfvf->netdev))\n\t\t\tnetif_tx_wake_queue(txq);\n\t}\n\treturn 0;\n}\n\nstatic void otx2_adjust_adaptive_coalese(struct otx2_nic *pfvf, struct otx2_cq_poll *cq_poll)\n{\n\tstruct dim_sample dim_sample;\n\tu64 rx_frames, rx_bytes;\n\tu64 tx_frames, tx_bytes;\n\n\trx_frames = OTX2_GET_RX_STATS(RX_BCAST) + OTX2_GET_RX_STATS(RX_MCAST) +\n\t\tOTX2_GET_RX_STATS(RX_UCAST);\n\trx_bytes = OTX2_GET_RX_STATS(RX_OCTS);\n\ttx_bytes = OTX2_GET_TX_STATS(TX_OCTS);\n\ttx_frames = OTX2_GET_TX_STATS(TX_UCAST);\n\n\tdim_update_sample(pfvf->napi_events,\n\t\t\t  rx_frames + tx_frames,\n\t\t\t  rx_bytes + tx_bytes,\n\t\t\t  &dim_sample);\n\tnet_dim(&cq_poll->dim, dim_sample);\n}\n\nint otx2_napi_handler(struct napi_struct *napi, int budget)\n{\n\tstruct otx2_cq_queue *rx_cq = NULL;\n\tstruct otx2_cq_poll *cq_poll;\n\tint workdone = 0, cq_idx, i;\n\tstruct otx2_cq_queue *cq;\n\tstruct otx2_qset *qset;\n\tstruct otx2_nic *pfvf;\n\tint filled_cnt = -1;\n\n\tcq_poll = container_of(napi, struct otx2_cq_poll, napi);\n\tpfvf = (struct otx2_nic *)cq_poll->dev;\n\tqset = &pfvf->qset;\n\n\tfor (i = 0; i < CQS_PER_CINT; i++) {\n\t\tcq_idx = cq_poll->cq_ids[i];\n\t\tif (unlikely(cq_idx == CINT_INVALID_CQ))\n\t\t\tcontinue;\n\t\tcq = &qset->cq[cq_idx];\n\t\tif (cq->cq_type == CQ_RX) {\n\t\t\trx_cq = cq;\n\t\t\tworkdone += otx2_rx_napi_handler(pfvf, napi,\n\t\t\t\t\t\t\t cq, budget);\n\t\t} else {\n\t\t\tworkdone += otx2_tx_napi_handler(pfvf, cq, budget);\n\t\t}\n\t}\n\n\tif (rx_cq && rx_cq->pool_ptrs)\n\t\tfilled_cnt = pfvf->hw_ops->refill_pool_ptrs(pfvf, rx_cq);\n\t \n\totx2_write64(pfvf, NIX_LF_CINTX_INT(cq_poll->cint_idx), BIT_ULL(0));\n\n\tif (workdone < budget && napi_complete_done(napi, workdone)) {\n\t\t \n\t\tif (pfvf->flags & OTX2_FLAG_INTF_DOWN)\n\t\t\treturn workdone;\n\n\t\t \n\t\tif (pfvf->flags & OTX2_FLAG_ADPTV_INT_COAL_ENABLED)\n\t\t\totx2_adjust_adaptive_coalese(pfvf, cq_poll);\n\n\t\tif (unlikely(!filled_cnt)) {\n\t\t\tstruct refill_work *work;\n\t\t\tstruct delayed_work *dwork;\n\n\t\t\twork = &pfvf->refill_wrk[cq->cq_idx];\n\t\t\tdwork = &work->pool_refill_work;\n\t\t\t \n\t\t\tif (!cq->refill_task_sched) {\n\t\t\t\twork->napi = napi;\n\t\t\t\tcq->refill_task_sched = true;\n\t\t\t\tschedule_delayed_work(dwork,\n\t\t\t\t\t\t      msecs_to_jiffies(100));\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\totx2_write64(pfvf,\n\t\t\t\t     NIX_LF_CINTX_ENA_W1S(cq_poll->cint_idx),\n\t\t\t\t     BIT_ULL(0));\n\t\t}\n\t}\n\treturn workdone;\n}\n\nvoid otx2_sqe_flush(void *dev, struct otx2_snd_queue *sq,\n\t\t    int size, int qidx)\n{\n\tu64 status;\n\n\t \n\tdma_wmb();\n\n\tdo {\n\t\tmemcpy(sq->lmt_addr, sq->sqe_base, size);\n\t\tstatus = otx2_lmt_flush(sq->io_addr);\n\t} while (status == 0);\n\n\tsq->head++;\n\tsq->head &= (sq->sqe_cnt - 1);\n}\n\n#define MAX_SEGS_PER_SG\t3\n \nstatic bool otx2_sqe_add_sg(struct otx2_nic *pfvf, struct otx2_snd_queue *sq,\n\t\t\t    struct sk_buff *skb, int num_segs, int *offset)\n{\n\tstruct nix_sqe_sg_s *sg = NULL;\n\tu64 dma_addr, *iova = NULL;\n\tu16 *sg_lens = NULL;\n\tint seg, len;\n\n\tsq->sg[sq->head].num_segs = 0;\n\n\tfor (seg = 0; seg < num_segs; seg++) {\n\t\tif ((seg % MAX_SEGS_PER_SG) == 0) {\n\t\t\tsg = (struct nix_sqe_sg_s *)(sq->sqe_base + *offset);\n\t\t\tsg->ld_type = NIX_SEND_LDTYPE_LDD;\n\t\t\tsg->subdc = NIX_SUBDC_SG;\n\t\t\tsg->segs = 0;\n\t\t\tsg_lens = (void *)sg;\n\t\t\tiova = (void *)sg + sizeof(*sg);\n\t\t\t \n\t\t\tif ((num_segs - seg) >= (MAX_SEGS_PER_SG - 1))\n\t\t\t\t*offset += sizeof(*sg) + (3 * sizeof(u64));\n\t\t\telse\n\t\t\t\t*offset += sizeof(*sg) + sizeof(u64);\n\t\t}\n\t\tdma_addr = otx2_dma_map_skb_frag(pfvf, skb, seg, &len);\n\t\tif (dma_mapping_error(pfvf->dev, dma_addr))\n\t\t\treturn false;\n\n\t\tsg_lens[frag_num(seg % MAX_SEGS_PER_SG)] = len;\n\t\tsg->segs++;\n\t\t*iova++ = dma_addr;\n\n\t\t \n\t\tsq->sg[sq->head].dma_addr[seg] = dma_addr;\n\t\tsq->sg[sq->head].size[seg] = len;\n\t\tsq->sg[sq->head].num_segs++;\n\t}\n\n\tsq->sg[sq->head].skb = (u64)skb;\n\treturn true;\n}\n\n \nstatic void otx2_sqe_add_ext(struct otx2_nic *pfvf, struct otx2_snd_queue *sq,\n\t\t\t     struct sk_buff *skb, int *offset)\n{\n\tstruct nix_sqe_ext_s *ext;\n\n\text = (struct nix_sqe_ext_s *)(sq->sqe_base + *offset);\n\text->subdc = NIX_SUBDC_EXT;\n\tif (skb_shinfo(skb)->gso_size) {\n\t\text->lso = 1;\n\t\text->lso_sb = skb_tcp_all_headers(skb);\n\t\text->lso_mps = skb_shinfo(skb)->gso_size;\n\n\t\t \n\t\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {\n\t\t\text->lso_format = pfvf->hw.lso_tsov4_idx;\n\n\t\t\t \n\t\t\tip_hdr(skb)->tot_len =\n\t\t\t\thtons(ext->lso_sb - skb_network_offset(skb));\n\t\t} else if (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6) {\n\t\t\text->lso_format = pfvf->hw.lso_tsov6_idx;\n\t\t\tipv6_hdr(skb)->payload_len = htons(tcp_hdrlen(skb));\n\t\t} else if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {\n\t\t\t__be16 l3_proto = vlan_get_protocol(skb);\n\t\t\tstruct udphdr *udph = udp_hdr(skb);\n\t\t\tu16 iplen;\n\n\t\t\text->lso_sb = skb_transport_offset(skb) +\n\t\t\t\t\tsizeof(struct udphdr);\n\n\t\t\t \n\t\t\tiplen = htons(ext->lso_sb - skb_network_offset(skb));\n\t\t\tif (l3_proto == htons(ETH_P_IP)) {\n\t\t\t\tip_hdr(skb)->tot_len = iplen;\n\t\t\t\text->lso_format = pfvf->hw.lso_udpv4_idx;\n\t\t\t} else {\n\t\t\t\tipv6_hdr(skb)->payload_len = iplen;\n\t\t\t\text->lso_format = pfvf->hw.lso_udpv6_idx;\n\t\t\t}\n\n\t\t\tudph->len = htons(sizeof(struct udphdr));\n\t\t}\n\t} else if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {\n\t\text->tstmp = 1;\n\t}\n\n#define OTX2_VLAN_PTR_OFFSET     (ETH_HLEN - ETH_TLEN)\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (skb->vlan_proto == htons(ETH_P_8021Q)) {\n\t\t\text->vlan1_ins_ena = 1;\n\t\t\text->vlan1_ins_ptr = OTX2_VLAN_PTR_OFFSET;\n\t\t\text->vlan1_ins_tci = skb_vlan_tag_get(skb);\n\t\t} else if (skb->vlan_proto == htons(ETH_P_8021AD)) {\n\t\t\text->vlan0_ins_ena = 1;\n\t\t\text->vlan0_ins_ptr = OTX2_VLAN_PTR_OFFSET;\n\t\t\text->vlan0_ins_tci = skb_vlan_tag_get(skb);\n\t\t}\n\t}\n\n\t*offset += sizeof(*ext);\n}\n\nstatic void otx2_sqe_add_mem(struct otx2_snd_queue *sq, int *offset,\n\t\t\t     int alg, u64 iova, int ptp_offset,\n\t\t\t     u64 base_ns, bool udp_csum_crt)\n{\n\tstruct nix_sqe_mem_s *mem;\n\n\tmem = (struct nix_sqe_mem_s *)(sq->sqe_base + *offset);\n\tmem->subdc = NIX_SUBDC_MEM;\n\tmem->alg = alg;\n\tmem->wmem = 1;  \n\tmem->addr = iova;\n\n\tif (ptp_offset) {\n\t\tmem->start_offset = ptp_offset;\n\t\tmem->udp_csum_crt = !!udp_csum_crt;\n\t\tmem->base_ns = base_ns;\n\t\tmem->step_type = 1;\n\t}\n\n\t*offset += sizeof(*mem);\n}\n\n \nstatic void otx2_sqe_add_hdr(struct otx2_nic *pfvf, struct otx2_snd_queue *sq,\n\t\t\t     struct nix_sqe_hdr_s *sqe_hdr,\n\t\t\t     struct sk_buff *skb, u16 qidx)\n{\n\tint proto = 0;\n\n\t \n\tif (!sqe_hdr->total) {\n\t\t \n\t\tsqe_hdr->df = 1;\n\t\tsqe_hdr->aura = sq->aura_id;\n\t\t \n\t\tsqe_hdr->pnc = 1;\n\t\tsqe_hdr->sq = (qidx >=  pfvf->hw.tx_queues) ?\n\t\t\t       qidx + pfvf->hw.xdp_queues : qidx;\n\t}\n\tsqe_hdr->total = skb->len;\n\t \n\tsqe_hdr->sqe_id = sq->head;\n\n\t \n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tsqe_hdr->ol3ptr = skb_network_offset(skb);\n\t\tsqe_hdr->ol4ptr = skb_transport_offset(skb);\n\t\t \n\t\tif (eth_type_vlan(skb->protocol))\n\t\t\tskb->protocol = vlan_get_protocol(skb);\n\n\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\tproto = ip_hdr(skb)->protocol;\n\t\t\t \n\t\t\tsqe_hdr->ol3type = NIX_SENDL3TYPE_IP4_CKSUM;\n\t\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\tproto = ipv6_hdr(skb)->nexthdr;\n\t\t\tsqe_hdr->ol3type = NIX_SENDL3TYPE_IP6;\n\t\t}\n\n\t\tif (proto == IPPROTO_TCP)\n\t\t\tsqe_hdr->ol4type = NIX_SENDL4TYPE_TCP_CKSUM;\n\t\telse if (proto == IPPROTO_UDP)\n\t\t\tsqe_hdr->ol4type = NIX_SENDL4TYPE_UDP_CKSUM;\n\t}\n}\n\nstatic int otx2_dma_map_tso_skb(struct otx2_nic *pfvf,\n\t\t\t\tstruct otx2_snd_queue *sq,\n\t\t\t\tstruct sk_buff *skb, int sqe, int hdr_len)\n{\n\tint num_segs = skb_shinfo(skb)->nr_frags + 1;\n\tstruct sg_list *sg = &sq->sg[sqe];\n\tu64 dma_addr;\n\tint seg, len;\n\n\tsg->num_segs = 0;\n\n\t \n\tlen = skb_headlen(skb) - hdr_len;\n\n\tfor (seg = 0; seg < num_segs; seg++) {\n\t\t \n\t\tif (!seg && !len)\n\t\t\tcontinue;\n\t\tdma_addr = otx2_dma_map_skb_frag(pfvf, skb, seg, &len);\n\t\tif (dma_mapping_error(pfvf->dev, dma_addr))\n\t\t\tgoto unmap;\n\n\t\t \n\t\tsg->dma_addr[sg->num_segs] = dma_addr;\n\t\tsg->size[sg->num_segs] = len;\n\t\tsg->num_segs++;\n\t}\n\treturn 0;\nunmap:\n\totx2_dma_unmap_skb_frags(pfvf, sg);\n\treturn -EINVAL;\n}\n\nstatic u64 otx2_tso_frag_dma_addr(struct otx2_snd_queue *sq,\n\t\t\t\t  struct sk_buff *skb, int seg,\n\t\t\t\t  u64 seg_addr, int hdr_len, int sqe)\n{\n\tstruct sg_list *sg = &sq->sg[sqe];\n\tconst skb_frag_t *frag;\n\tint offset;\n\n\tif (seg < 0)\n\t\treturn sg->dma_addr[0] + (seg_addr - (u64)skb->data);\n\n\tfrag = &skb_shinfo(skb)->frags[seg];\n\toffset = seg_addr - (u64)skb_frag_address(frag);\n\tif (skb_headlen(skb) - hdr_len)\n\t\tseg++;\n\treturn sg->dma_addr[seg] + offset;\n}\n\nstatic void otx2_sqe_tso_add_sg(struct otx2_snd_queue *sq,\n\t\t\t\tstruct sg_list *list, int *offset)\n{\n\tstruct nix_sqe_sg_s *sg = NULL;\n\tu16 *sg_lens = NULL;\n\tu64 *iova = NULL;\n\tint seg;\n\n\t \n\tfor (seg = 0; seg < list->num_segs; seg++) {\n\t\tif ((seg % MAX_SEGS_PER_SG) == 0) {\n\t\t\tsg = (struct nix_sqe_sg_s *)(sq->sqe_base + *offset);\n\t\t\tsg->ld_type = NIX_SEND_LDTYPE_LDD;\n\t\t\tsg->subdc = NIX_SUBDC_SG;\n\t\t\tsg->segs = 0;\n\t\t\tsg_lens = (void *)sg;\n\t\t\tiova = (void *)sg + sizeof(*sg);\n\t\t\t \n\t\t\tif ((list->num_segs - seg) >= (MAX_SEGS_PER_SG - 1))\n\t\t\t\t*offset += sizeof(*sg) + (3 * sizeof(u64));\n\t\t\telse\n\t\t\t\t*offset += sizeof(*sg) + sizeof(u64);\n\t\t}\n\t\tsg_lens[frag_num(seg % MAX_SEGS_PER_SG)] = list->size[seg];\n\t\t*iova++ = list->dma_addr[seg];\n\t\tsg->segs++;\n\t}\n}\n\nstatic void otx2_sq_append_tso(struct otx2_nic *pfvf, struct otx2_snd_queue *sq,\n\t\t\t       struct sk_buff *skb, u16 qidx)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(pfvf->netdev, qidx);\n\tint hdr_len, tcp_data, seg_len, pkt_len, offset;\n\tstruct nix_sqe_hdr_s *sqe_hdr;\n\tint first_sqe = sq->head;\n\tstruct sg_list list;\n\tstruct tso_t tso;\n\n\thdr_len = tso_start(skb, &tso);\n\n\t \n\tif (otx2_dma_map_tso_skb(pfvf, sq, skb, first_sqe, hdr_len)) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn;\n\t}\n\n\tnetdev_tx_sent_queue(txq, skb->len);\n\n\ttcp_data = skb->len - hdr_len;\n\twhile (tcp_data > 0) {\n\t\tchar *hdr;\n\n\t\tseg_len = min_t(int, skb_shinfo(skb)->gso_size, tcp_data);\n\t\ttcp_data -= seg_len;\n\n\t\t \n\t\tmemset(sq->sqe_base, 0, sq->sqe_size);\n\t\tsqe_hdr = (struct nix_sqe_hdr_s *)(sq->sqe_base);\n\t\totx2_sqe_add_hdr(pfvf, sq, sqe_hdr, skb, qidx);\n\t\toffset = sizeof(*sqe_hdr);\n\n\t\t \n\t\thdr = sq->tso_hdrs->base + (sq->head * TSO_HEADER_SIZE);\n\t\ttso_build_hdr(skb, hdr, &tso, seg_len, tcp_data == 0);\n\t\tlist.dma_addr[0] =\n\t\t\tsq->tso_hdrs->iova + (sq->head * TSO_HEADER_SIZE);\n\t\tlist.size[0] = hdr_len;\n\t\tlist.num_segs = 1;\n\n\t\t \n\t\tpkt_len = hdr_len;\n\t\twhile (seg_len > 0) {\n\t\t\tint size;\n\n\t\t\tsize = min_t(int, tso.size, seg_len);\n\n\t\t\tlist.size[list.num_segs] = size;\n\t\t\tlist.dma_addr[list.num_segs] =\n\t\t\t\totx2_tso_frag_dma_addr(sq, skb,\n\t\t\t\t\t\t       tso.next_frag_idx - 1,\n\t\t\t\t\t\t       (u64)tso.data, hdr_len,\n\t\t\t\t\t\t       first_sqe);\n\t\t\tlist.num_segs++;\n\t\t\tpkt_len += size;\n\t\t\tseg_len -= size;\n\t\t\ttso_build_data(skb, &tso, size);\n\t\t}\n\t\tsqe_hdr->total = pkt_len;\n\t\totx2_sqe_tso_add_sg(sq, &list, &offset);\n\n\t\t \n\t\tif (!tcp_data) {\n\t\t\tsqe_hdr->pnc = 1;\n\t\t\tsqe_hdr->sqe_id = first_sqe;\n\t\t\tsq->sg[first_sqe].skb = (u64)skb;\n\t\t} else {\n\t\t\tsqe_hdr->pnc = 0;\n\t\t}\n\n\t\tsqe_hdr->sizem1 = (offset / 16) - 1;\n\n\t\t \n\t\tpfvf->hw_ops->sqe_flush(pfvf, sq, offset, qidx);\n\t}\n}\n\nstatic bool is_hw_tso_supported(struct otx2_nic *pfvf,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tint payload_len, last_seg_size;\n\n\tif (test_bit(HW_TSO, &pfvf->hw.cap_flag))\n\t\treturn true;\n\n\t \n\tif (!is_96xx_B0(pfvf->pdev))\n\t\treturn false;\n\n\t \n\n\tpayload_len = skb->len - skb_tcp_all_headers(skb);\n\tlast_seg_size = payload_len % skb_shinfo(skb)->gso_size;\n\tif (last_seg_size && last_seg_size < 16)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int otx2_get_sqe_count(struct otx2_nic *pfvf, struct sk_buff *skb)\n{\n\tif (!skb_shinfo(skb)->gso_size)\n\t\treturn 1;\n\n\t \n\tif (is_hw_tso_supported(pfvf, skb))\n\t\treturn 1;\n\n\t \n\treturn skb_shinfo(skb)->gso_segs;\n}\n\nstatic bool otx2_validate_network_transport(struct sk_buff *skb)\n{\n\tif ((ip_hdr(skb)->protocol == IPPROTO_UDP) ||\n\t    (ipv6_hdr(skb)->nexthdr == IPPROTO_UDP)) {\n\t\tstruct udphdr *udph = udp_hdr(skb);\n\n\t\tif (udph->source == htons(PTP_PORT) &&\n\t\t    udph->dest == htons(PTP_PORT))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool otx2_ptp_is_sync(struct sk_buff *skb, int *offset, bool *udp_csum_crt)\n{\n\tstruct ethhdr *eth = (struct ethhdr *)(skb->data);\n\tu16 nix_offload_hlen = 0, inner_vhlen = 0;\n\tbool udp_hdr_present = false, is_sync;\n\tu8 *data = skb->data, *msgtype;\n\t__be16 proto = eth->h_proto;\n\tint network_depth = 0;\n\n\t \n\tif (skb->dev->features & NETIF_F_HW_VLAN_CTAG_TX &&\n\t    skb->dev->features & NETIF_F_HW_VLAN_STAG_TX) {\n\t\tif (skb->vlan_proto == htons(ETH_P_8021AD)) {\n\t\t\t \n\t\t\tproto = __vlan_get_protocol(skb, eth->h_proto, NULL);\n\t\t\t \n\t\t\tnix_offload_hlen = VLAN_HLEN;\n\t\t\tinner_vhlen = VLAN_HLEN;\n\t\t} else if (skb->vlan_proto == htons(ETH_P_8021Q)) {\n\t\t\tnix_offload_hlen = VLAN_HLEN;\n\t\t}\n\t} else if (eth_type_vlan(eth->h_proto)) {\n\t\tproto = __vlan_get_protocol(skb, eth->h_proto, &network_depth);\n\t}\n\n\tswitch (ntohs(proto)) {\n\tcase ETH_P_1588:\n\t\tif (network_depth)\n\t\t\t*offset = network_depth;\n\t\telse\n\t\t\t*offset = ETH_HLEN + nix_offload_hlen +\n\t\t\t\t  inner_vhlen;\n\t\tbreak;\n\tcase ETH_P_IP:\n\tcase ETH_P_IPV6:\n\t\tif (!otx2_validate_network_transport(skb))\n\t\t\treturn false;\n\n\t\t*offset = nix_offload_hlen + skb_transport_offset(skb) +\n\t\t\t  sizeof(struct udphdr);\n\t\tudp_hdr_present = true;\n\n\t}\n\n\tmsgtype = data + *offset;\n\t \n\tis_sync = !(*msgtype & 0xf);\n\tif (is_sync)\n\t\t*udp_csum_crt = udp_hdr_present;\n\telse\n\t\t*offset = 0;\n\n\treturn is_sync;\n}\n\nstatic void otx2_set_txtstamp(struct otx2_nic *pfvf, struct sk_buff *skb,\n\t\t\t      struct otx2_snd_queue *sq, int *offset)\n{\n\tstruct ethhdr\t*eth = (struct ethhdr *)(skb->data);\n\tstruct ptpv2_tstamp *origin_tstamp;\n\tbool udp_csum_crt = false;\n\tunsigned int udphoff;\n\tstruct timespec64 ts;\n\tint ptp_offset = 0;\n\t__wsum skb_csum;\n\tu64 iova;\n\n\tif (unlikely(!skb_shinfo(skb)->gso_size &&\n\t\t     (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))) {\n\t\tif (unlikely(pfvf->flags & OTX2_FLAG_PTP_ONESTEP_SYNC &&\n\t\t\t     otx2_ptp_is_sync(skb, &ptp_offset, &udp_csum_crt))) {\n\t\t\torigin_tstamp = (struct ptpv2_tstamp *)\n\t\t\t\t\t((u8 *)skb->data + ptp_offset +\n\t\t\t\t\t PTP_SYNC_SEC_OFFSET);\n\t\t\tts = ns_to_timespec64(pfvf->ptp->tstamp);\n\t\t\torigin_tstamp->seconds_msb = htons((ts.tv_sec >> 32) & 0xffff);\n\t\t\torigin_tstamp->seconds_lsb = htonl(ts.tv_sec & 0xffffffff);\n\t\t\torigin_tstamp->nanoseconds = htonl(ts.tv_nsec);\n\t\t\t \n\t\t\tptp_offset += 8;\n\n\t\t\t \n\t\t\tif (udp_csum_crt) {\n\t\t\t\tstruct udphdr *uh = udp_hdr(skb);\n\n\t\t\t\tif (skb->ip_summed != CHECKSUM_PARTIAL && uh->check != 0) {\n\t\t\t\t\tudphoff = skb_transport_offset(skb);\n\t\t\t\t\tuh->check = 0;\n\t\t\t\t\tskb_csum = skb_checksum(skb, udphoff, skb->len - udphoff,\n\t\t\t\t\t\t\t\t0);\n\t\t\t\t\tif (ntohs(eth->h_proto) == ETH_P_IPV6)\n\t\t\t\t\t\tuh->check = csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t\t\t\t    &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t\t\t\t    skb->len - udphoff,\n\t\t\t\t\t\t\t\t\t    ipv6_hdr(skb)->nexthdr,\n\t\t\t\t\t\t\t\t\t    skb_csum);\n\t\t\t\t\telse\n\t\t\t\t\t\tuh->check = csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t\t\t\t\t      ip_hdr(skb)->daddr,\n\t\t\t\t\t\t\t\t\t      skb->len - udphoff,\n\t\t\t\t\t\t\t\t\t      IPPROTO_UDP,\n\t\t\t\t\t\t\t\t\t      skb_csum);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t}\n\t\tiova = sq->timestamps->iova + (sq->head * sizeof(u64));\n\t\totx2_sqe_add_mem(sq, offset, NIX_SENDMEMALG_E_SETTSTMP, iova,\n\t\t\t\t ptp_offset, pfvf->ptp->base_ns, udp_csum_crt);\n\t} else {\n\t\tskb_tx_timestamp(skb);\n\t}\n}\n\nbool otx2_sq_append_skb(struct net_device *netdev, struct otx2_snd_queue *sq,\n\t\t\tstruct sk_buff *skb, u16 qidx)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(netdev, qidx);\n\tstruct otx2_nic *pfvf = netdev_priv(netdev);\n\tint offset, num_segs, free_desc;\n\tstruct nix_sqe_hdr_s *sqe_hdr;\n\n\t \n\tfree_desc = (sq->cons_head - sq->head - 1 + sq->sqe_cnt) & (sq->sqe_cnt - 1);\n\tif (free_desc < sq->sqe_thresh)\n\t\treturn false;\n\n\tif (free_desc < otx2_get_sqe_count(pfvf, skb))\n\t\treturn false;\n\n\tnum_segs = skb_shinfo(skb)->nr_frags + 1;\n\n\t \n\tif (unlikely(num_segs > OTX2_MAX_FRAGS_IN_SQE)) {\n\t\tif (__skb_linearize(skb)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn true;\n\t\t}\n\t\tnum_segs = skb_shinfo(skb)->nr_frags + 1;\n\t}\n\n\tif (skb_shinfo(skb)->gso_size && !is_hw_tso_supported(pfvf, skb)) {\n\t\t \n\t\tif (skb_vlan_tag_present(skb))\n\t\t\tskb = __vlan_hwaccel_push_inside(skb);\n\t\totx2_sq_append_tso(pfvf, sq, skb, qidx);\n\t\treturn true;\n\t}\n\n\t \n\tmemset(sq->sqe_base + 8, 0, sq->sqe_size - 8);\n\tsqe_hdr = (struct nix_sqe_hdr_s *)(sq->sqe_base);\n\totx2_sqe_add_hdr(pfvf, sq, sqe_hdr, skb, qidx);\n\toffset = sizeof(*sqe_hdr);\n\n\t \n\totx2_sqe_add_ext(pfvf, sq, skb, &offset);\n\n\t \n\tif (!otx2_sqe_add_sg(pfvf, sq, skb, num_segs, &offset)) {\n\t\totx2_dma_unmap_skb_frags(pfvf, &sq->sg[sq->head]);\n\t\treturn false;\n\t}\n\n\totx2_set_txtstamp(pfvf, skb, sq, &offset);\n\n\tsqe_hdr->sizem1 = (offset / 16) - 1;\n\n\tnetdev_tx_sent_queue(txq, skb->len);\n\n\t \n\tpfvf->hw_ops->sqe_flush(pfvf, sq, offset, qidx);\n\n\treturn true;\n}\nEXPORT_SYMBOL(otx2_sq_append_skb);\n\nvoid otx2_cleanup_rx_cqes(struct otx2_nic *pfvf, struct otx2_cq_queue *cq, int qidx)\n{\n\tstruct nix_cqe_rx_s *cqe;\n\tstruct otx2_pool *pool;\n\tint processed_cqe = 0;\n\tu16 pool_id;\n\tu64 iova;\n\n\tif (pfvf->xdp_prog)\n\t\txdp_rxq_info_unreg(&cq->xdp_rxq);\n\n\tif (otx2_nix_cq_op_status(pfvf, cq) || !cq->pend_cqe)\n\t\treturn;\n\n\tpool_id = otx2_get_pool_idx(pfvf, AURA_NIX_RQ, qidx);\n\tpool = &pfvf->qset.pool[pool_id];\n\n\twhile (cq->pend_cqe) {\n\t\tcqe = (struct nix_cqe_rx_s *)otx2_get_next_cqe(cq);\n\t\tprocessed_cqe++;\n\t\tcq->pend_cqe--;\n\n\t\tif (!cqe)\n\t\t\tcontinue;\n\t\tif (cqe->sg.segs > 1) {\n\t\t\totx2_free_rcv_seg(pfvf, cqe, cq->cq_idx);\n\t\t\tcontinue;\n\t\t}\n\t\tiova = cqe->sg.seg_addr - OTX2_HEAD_ROOM;\n\n\t\totx2_free_bufs(pfvf, pool, iova, pfvf->rbsize);\n\t}\n\n\t \n\totx2_write64(pfvf, NIX_LF_CQ_OP_DOOR,\n\t\t     ((u64)cq->cq_idx << 32) | processed_cqe);\n}\n\nvoid otx2_cleanup_tx_cqes(struct otx2_nic *pfvf, struct otx2_cq_queue *cq)\n{\n\tint tx_pkts = 0, tx_bytes = 0;\n\tstruct sk_buff *skb = NULL;\n\tstruct otx2_snd_queue *sq;\n\tstruct nix_cqe_tx_s *cqe;\n\tstruct netdev_queue *txq;\n\tint processed_cqe = 0;\n\tstruct sg_list *sg;\n\tint qidx;\n\n\tqidx = cq->cq_idx - pfvf->hw.rx_queues;\n\tsq = &pfvf->qset.sq[qidx];\n\n\tif (otx2_nix_cq_op_status(pfvf, cq) || !cq->pend_cqe)\n\t\treturn;\n\n\twhile (cq->pend_cqe) {\n\t\tcqe = (struct nix_cqe_tx_s *)otx2_get_next_cqe(cq);\n\t\tprocessed_cqe++;\n\t\tcq->pend_cqe--;\n\n\t\tif (!cqe)\n\t\t\tcontinue;\n\t\tsg = &sq->sg[cqe->comp.sqe_id];\n\t\tskb = (struct sk_buff *)sg->skb;\n\t\tif (skb) {\n\t\t\ttx_bytes += skb->len;\n\t\t\ttx_pkts++;\n\t\t\totx2_dma_unmap_skb_frags(pfvf, sg);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tsg->skb = (u64)NULL;\n\t\t}\n\t}\n\n\tif (likely(tx_pkts)) {\n\t\tif (qidx >= pfvf->hw.tx_queues)\n\t\t\tqidx -= pfvf->hw.xdp_queues;\n\t\ttxq = netdev_get_tx_queue(pfvf->netdev, qidx);\n\t\tnetdev_tx_completed_queue(txq, tx_pkts, tx_bytes);\n\t}\n\t \n\totx2_write64(pfvf, NIX_LF_CQ_OP_DOOR,\n\t\t     ((u64)cq->cq_idx << 32) | processed_cqe);\n}\n\nint otx2_rxtx_enable(struct otx2_nic *pfvf, bool enable)\n{\n\tstruct msg_req *msg;\n\tint err;\n\n\tmutex_lock(&pfvf->mbox.lock);\n\tif (enable)\n\t\tmsg = otx2_mbox_alloc_msg_nix_lf_start_rx(&pfvf->mbox);\n\telse\n\t\tmsg = otx2_mbox_alloc_msg_nix_lf_stop_rx(&pfvf->mbox);\n\n\tif (!msg) {\n\t\tmutex_unlock(&pfvf->mbox.lock);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = otx2_sync_mbox_msg(&pfvf->mbox);\n\tmutex_unlock(&pfvf->mbox.lock);\n\treturn err;\n}\n\nvoid otx2_free_pending_sqe(struct otx2_nic *pfvf)\n{\n\tint tx_pkts = 0, tx_bytes = 0;\n\tstruct sk_buff *skb = NULL;\n\tstruct otx2_snd_queue *sq;\n\tstruct netdev_queue *txq;\n\tstruct sg_list *sg;\n\tint sq_idx, sqe;\n\n\tfor (sq_idx = 0; sq_idx < pfvf->hw.tx_queues; sq_idx++) {\n\t\tsq = &pfvf->qset.sq[sq_idx];\n\t\tfor (sqe = 0; sqe < sq->sqe_cnt; sqe++) {\n\t\t\tsg = &sq->sg[sqe];\n\t\t\tskb = (struct sk_buff *)sg->skb;\n\t\t\tif (skb) {\n\t\t\t\ttx_bytes += skb->len;\n\t\t\t\ttx_pkts++;\n\t\t\t\totx2_dma_unmap_skb_frags(pfvf, sg);\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\t\tsg->skb = (u64)NULL;\n\t\t\t}\n\t\t}\n\n\t\tif (!tx_pkts)\n\t\t\tcontinue;\n\t\ttxq = netdev_get_tx_queue(pfvf->netdev, sq_idx);\n\t\tnetdev_tx_completed_queue(txq, tx_pkts, tx_bytes);\n\t\ttx_pkts = 0;\n\t\ttx_bytes = 0;\n\t}\n}\n\nstatic void otx2_xdp_sqe_add_sg(struct otx2_snd_queue *sq, u64 dma_addr,\n\t\t\t\tint len, int *offset)\n{\n\tstruct nix_sqe_sg_s *sg = NULL;\n\tu64 *iova = NULL;\n\n\tsg = (struct nix_sqe_sg_s *)(sq->sqe_base + *offset);\n\tsg->ld_type = NIX_SEND_LDTYPE_LDD;\n\tsg->subdc = NIX_SUBDC_SG;\n\tsg->segs = 1;\n\tsg->seg1_size = len;\n\tiova = (void *)sg + sizeof(*sg);\n\t*iova = dma_addr;\n\t*offset += sizeof(*sg) + sizeof(u64);\n\n\tsq->sg[sq->head].dma_addr[0] = dma_addr;\n\tsq->sg[sq->head].size[0] = len;\n\tsq->sg[sq->head].num_segs = 1;\n}\n\nbool otx2_xdp_sq_append_pkt(struct otx2_nic *pfvf, u64 iova, int len, u16 qidx)\n{\n\tstruct nix_sqe_hdr_s *sqe_hdr;\n\tstruct otx2_snd_queue *sq;\n\tint offset, free_sqe;\n\n\tsq = &pfvf->qset.sq[qidx];\n\tfree_sqe = (sq->num_sqbs - *sq->aura_fc_addr) * sq->sqe_per_sqb;\n\tif (free_sqe < sq->sqe_thresh)\n\t\treturn false;\n\n\tmemset(sq->sqe_base + 8, 0, sq->sqe_size - 8);\n\n\tsqe_hdr = (struct nix_sqe_hdr_s *)(sq->sqe_base);\n\n\tif (!sqe_hdr->total) {\n\t\tsqe_hdr->aura = sq->aura_id;\n\t\tsqe_hdr->df = 1;\n\t\tsqe_hdr->sq = qidx;\n\t\tsqe_hdr->pnc = 1;\n\t}\n\tsqe_hdr->total = len;\n\tsqe_hdr->sqe_id = sq->head;\n\n\toffset = sizeof(*sqe_hdr);\n\n\totx2_xdp_sqe_add_sg(sq, iova, len, &offset);\n\tsqe_hdr->sizem1 = (offset / 16) - 1;\n\tpfvf->hw_ops->sqe_flush(pfvf, sq, offset, qidx);\n\n\treturn true;\n}\n\nstatic bool otx2_xdp_rcv_pkt_handler(struct otx2_nic *pfvf,\n\t\t\t\t     struct bpf_prog *prog,\n\t\t\t\t     struct nix_cqe_rx_s *cqe,\n\t\t\t\t     struct otx2_cq_queue *cq,\n\t\t\t\t     bool *need_xdp_flush)\n{\n\tunsigned char *hard_start, *data;\n\tint qidx = cq->cq_idx;\n\tstruct xdp_buff xdp;\n\tstruct page *page;\n\tu64 iova, pa;\n\tu32 act;\n\tint err;\n\n\tiova = cqe->sg.seg_addr - OTX2_HEAD_ROOM;\n\tpa = otx2_iova_to_phys(pfvf->iommu_domain, iova);\n\tpage = virt_to_page(phys_to_virt(pa));\n\n\txdp_init_buff(&xdp, pfvf->rbsize, &cq->xdp_rxq);\n\n\tdata = (unsigned char *)phys_to_virt(pa);\n\thard_start = page_address(page);\n\txdp_prepare_buff(&xdp, hard_start, data - hard_start,\n\t\t\t cqe->sg.seg_size, false);\n\n\tact = bpf_prog_run_xdp(prog, &xdp);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\tqidx += pfvf->hw.tx_queues;\n\t\tcq->pool_ptrs++;\n\t\treturn otx2_xdp_sq_append_pkt(pfvf, iova,\n\t\t\t\t\t      cqe->sg.seg_size, qidx);\n\tcase XDP_REDIRECT:\n\t\tcq->pool_ptrs++;\n\t\terr = xdp_do_redirect(pfvf->netdev, &xdp, prog);\n\n\t\totx2_dma_unmap_page(pfvf, iova, pfvf->rbsize,\n\t\t\t\t    DMA_FROM_DEVICE);\n\t\tif (!err) {\n\t\t\t*need_xdp_flush = true;\n\t\t\treturn true;\n\t\t}\n\t\tput_page(page);\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(pfvf->netdev, prog, act);\n\t\tbreak;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(pfvf->netdev, prog, act);\n\t\tbreak;\n\tcase XDP_DROP:\n\t\totx2_dma_unmap_page(pfvf, iova, pfvf->rbsize,\n\t\t\t\t    DMA_FROM_DEVICE);\n\t\tput_page(page);\n\t\tcq->pool_ptrs++;\n\t\treturn true;\n\t}\n\treturn false;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}