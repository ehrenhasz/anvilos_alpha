{
  "module_name": "otx2_pf.c",
  "hash_id": "574439be2a517b0e09b8be6bfb42bcb2ba566d91ac6d2708ce6ea3281280f6ae",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/marvell/octeontx2/nic/otx2_pf.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/interrupt.h>\n#include <linux/pci.h>\n#include <linux/etherdevice.h>\n#include <linux/of.h>\n#include <linux/if_vlan.h>\n#include <linux/iommu.h>\n#include <net/ip.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <linux/bitfield.h>\n#include <net/page_pool/types.h>\n\n#include \"otx2_reg.h\"\n#include \"otx2_common.h\"\n#include \"otx2_txrx.h\"\n#include \"otx2_struct.h\"\n#include \"otx2_ptp.h\"\n#include \"cn10k.h\"\n#include \"qos.h\"\n#include <rvu_trace.h>\n\n#define DRV_NAME\t\"rvu_nicpf\"\n#define DRV_STRING\t\"Marvell RVU NIC Physical Function Driver\"\n\n \nstatic const struct pci_device_id otx2_pf_id_table[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, PCI_DEVID_OCTEONTX2_RVU_PF) },\n\t{ 0, }   \n};\n\nMODULE_AUTHOR(\"Sunil Goutham <sgoutham@marvell.com>\");\nMODULE_DESCRIPTION(DRV_STRING);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DEVICE_TABLE(pci, otx2_pf_id_table);\n\nstatic void otx2_vf_link_event_task(struct work_struct *work);\n\nenum {\n\tTYPE_PFAF,\n\tTYPE_PFVF,\n};\n\nstatic int otx2_config_hw_tx_tstamp(struct otx2_nic *pfvf, bool enable);\nstatic int otx2_config_hw_rx_tstamp(struct otx2_nic *pfvf, bool enable);\n\nstatic int otx2_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tbool if_up = netif_running(netdev);\n\tint err = 0;\n\n\tif (pf->xdp_prog && new_mtu > MAX_XDP_MTU) {\n\t\tnetdev_warn(netdev, \"Jumbo frames not yet supported with XDP, current MTU %d.\\n\",\n\t\t\t    netdev->mtu);\n\t\treturn -EINVAL;\n\t}\n\tif (if_up)\n\t\totx2_stop(netdev);\n\n\tnetdev_info(netdev, \"Changing MTU from %d to %d\\n\",\n\t\t    netdev->mtu, new_mtu);\n\tnetdev->mtu = new_mtu;\n\n\tif (if_up)\n\t\terr = otx2_open(netdev);\n\n\treturn err;\n}\n\nstatic void otx2_disable_flr_me_intr(struct otx2_nic *pf)\n{\n\tint irq, vfs = pf->total_vfs;\n\n\t \n\totx2_write64(pf, RVU_PF_VFME_INT_ENA_W1CX(0), INTR_MASK(vfs));\n\tirq = pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFME0);\n\tfree_irq(irq, pf);\n\n\t \n\totx2_write64(pf, RVU_PF_VFFLR_INT_ENA_W1CX(0), INTR_MASK(vfs));\n\tirq = pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFFLR0);\n\tfree_irq(irq, pf);\n\n\tif (vfs <= 64)\n\t\treturn;\n\n\totx2_write64(pf, RVU_PF_VFME_INT_ENA_W1CX(1), INTR_MASK(vfs - 64));\n\tirq = pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFME1);\n\tfree_irq(irq, pf);\n\n\totx2_write64(pf, RVU_PF_VFFLR_INT_ENA_W1CX(1), INTR_MASK(vfs - 64));\n\tirq = pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFFLR1);\n\tfree_irq(irq, pf);\n}\n\nstatic void otx2_flr_wq_destroy(struct otx2_nic *pf)\n{\n\tif (!pf->flr_wq)\n\t\treturn;\n\tdestroy_workqueue(pf->flr_wq);\n\tpf->flr_wq = NULL;\n\tdevm_kfree(pf->dev, pf->flr_wrk);\n}\n\nstatic void otx2_flr_handler(struct work_struct *work)\n{\n\tstruct flr_work *flrwork = container_of(work, struct flr_work, work);\n\tstruct otx2_nic *pf = flrwork->pf;\n\tstruct mbox *mbox = &pf->mbox;\n\tstruct msg_req *req;\n\tint vf, reg = 0;\n\n\tvf = flrwork - pf->flr_wrk;\n\n\tmutex_lock(&mbox->lock);\n\treq = otx2_mbox_alloc_msg_vf_flr(mbox);\n\tif (!req) {\n\t\tmutex_unlock(&mbox->lock);\n\t\treturn;\n\t}\n\treq->hdr.pcifunc &= RVU_PFVF_FUNC_MASK;\n\treq->hdr.pcifunc |= (vf + 1) & RVU_PFVF_FUNC_MASK;\n\n\tif (!otx2_sync_mbox_msg(&pf->mbox)) {\n\t\tif (vf >= 64) {\n\t\t\treg = 1;\n\t\t\tvf = vf - 64;\n\t\t}\n\t\t \n\t\totx2_write64(pf, RVU_PF_VFTRPENDX(reg), BIT_ULL(vf));\n\t\totx2_write64(pf, RVU_PF_VFFLR_INT_ENA_W1SX(reg), BIT_ULL(vf));\n\t}\n\n\tmutex_unlock(&mbox->lock);\n}\n\nstatic irqreturn_t otx2_pf_flr_intr_handler(int irq, void *pf_irq)\n{\n\tstruct otx2_nic *pf = (struct otx2_nic *)pf_irq;\n\tint reg, dev, vf, start_vf, num_reg = 1;\n\tu64 intr;\n\n\tif (pf->total_vfs > 64)\n\t\tnum_reg = 2;\n\n\tfor (reg = 0; reg < num_reg; reg++) {\n\t\tintr = otx2_read64(pf, RVU_PF_VFFLR_INTX(reg));\n\t\tif (!intr)\n\t\t\tcontinue;\n\t\tstart_vf = 64 * reg;\n\t\tfor (vf = 0; vf < 64; vf++) {\n\t\t\tif (!(intr & BIT_ULL(vf)))\n\t\t\t\tcontinue;\n\t\t\tdev = vf + start_vf;\n\t\t\tqueue_work(pf->flr_wq, &pf->flr_wrk[dev].work);\n\t\t\t \n\t\t\totx2_write64(pf, RVU_PF_VFFLR_INTX(reg), BIT_ULL(vf));\n\t\t\t \n\t\t\totx2_write64(pf, RVU_PF_VFFLR_INT_ENA_W1CX(reg),\n\t\t\t\t     BIT_ULL(vf));\n\t\t}\n\t}\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t otx2_pf_me_intr_handler(int irq, void *pf_irq)\n{\n\tstruct otx2_nic *pf = (struct otx2_nic *)pf_irq;\n\tint vf, reg, num_reg = 1;\n\tu64 intr;\n\n\tif (pf->total_vfs > 64)\n\t\tnum_reg = 2;\n\n\tfor (reg = 0; reg < num_reg; reg++) {\n\t\tintr = otx2_read64(pf, RVU_PF_VFME_INTX(reg));\n\t\tif (!intr)\n\t\t\tcontinue;\n\t\tfor (vf = 0; vf < 64; vf++) {\n\t\t\tif (!(intr & BIT_ULL(vf)))\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\totx2_write64(pf, RVU_PF_VFTRPENDX(reg), BIT_ULL(vf));\n\t\t\t \n\t\t\totx2_write64(pf, RVU_PF_VFME_INTX(reg), BIT_ULL(vf));\n\t\t}\n\t}\n\treturn IRQ_HANDLED;\n}\n\nstatic int otx2_register_flr_me_intr(struct otx2_nic *pf, int numvfs)\n{\n\tstruct otx2_hw *hw = &pf->hw;\n\tchar *irq_name;\n\tint ret;\n\n\t \n\tirq_name = &hw->irq_name[RVU_PF_INT_VEC_VFME0 * NAME_SIZE];\n\tsnprintf(irq_name, NAME_SIZE, \"RVUPF%d_ME0\", rvu_get_pf(pf->pcifunc));\n\tret = request_irq(pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFME0),\n\t\t\t  otx2_pf_me_intr_handler, 0, irq_name, pf);\n\tif (ret) {\n\t\tdev_err(pf->dev,\n\t\t\t\"RVUPF: IRQ registration failed for ME0\\n\");\n\t}\n\n\t \n\tirq_name = &hw->irq_name[RVU_PF_INT_VEC_VFFLR0 * NAME_SIZE];\n\tsnprintf(irq_name, NAME_SIZE, \"RVUPF%d_FLR0\", rvu_get_pf(pf->pcifunc));\n\tret = request_irq(pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFFLR0),\n\t\t\t  otx2_pf_flr_intr_handler, 0, irq_name, pf);\n\tif (ret) {\n\t\tdev_err(pf->dev,\n\t\t\t\"RVUPF: IRQ registration failed for FLR0\\n\");\n\t\treturn ret;\n\t}\n\n\tif (numvfs > 64) {\n\t\tirq_name = &hw->irq_name[RVU_PF_INT_VEC_VFME1 * NAME_SIZE];\n\t\tsnprintf(irq_name, NAME_SIZE, \"RVUPF%d_ME1\",\n\t\t\t rvu_get_pf(pf->pcifunc));\n\t\tret = request_irq(pci_irq_vector\n\t\t\t\t  (pf->pdev, RVU_PF_INT_VEC_VFME1),\n\t\t\t\t  otx2_pf_me_intr_handler, 0, irq_name, pf);\n\t\tif (ret) {\n\t\t\tdev_err(pf->dev,\n\t\t\t\t\"RVUPF: IRQ registration failed for ME1\\n\");\n\t\t}\n\t\tirq_name = &hw->irq_name[RVU_PF_INT_VEC_VFFLR1 * NAME_SIZE];\n\t\tsnprintf(irq_name, NAME_SIZE, \"RVUPF%d_FLR1\",\n\t\t\t rvu_get_pf(pf->pcifunc));\n\t\tret = request_irq(pci_irq_vector\n\t\t\t\t  (pf->pdev, RVU_PF_INT_VEC_VFFLR1),\n\t\t\t\t  otx2_pf_flr_intr_handler, 0, irq_name, pf);\n\t\tif (ret) {\n\t\t\tdev_err(pf->dev,\n\t\t\t\t\"RVUPF: IRQ registration failed for FLR1\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\totx2_write64(pf, RVU_PF_VFME_INTX(0), INTR_MASK(numvfs));\n\totx2_write64(pf, RVU_PF_VFME_INT_ENA_W1SX(0), INTR_MASK(numvfs));\n\n\t \n\totx2_write64(pf, RVU_PF_VFFLR_INTX(0), INTR_MASK(numvfs));\n\totx2_write64(pf, RVU_PF_VFFLR_INT_ENA_W1SX(0), INTR_MASK(numvfs));\n\n\tif (numvfs > 64) {\n\t\tnumvfs -= 64;\n\n\t\totx2_write64(pf, RVU_PF_VFME_INTX(1), INTR_MASK(numvfs));\n\t\totx2_write64(pf, RVU_PF_VFME_INT_ENA_W1SX(1),\n\t\t\t     INTR_MASK(numvfs));\n\n\t\totx2_write64(pf, RVU_PF_VFFLR_INTX(1), INTR_MASK(numvfs));\n\t\totx2_write64(pf, RVU_PF_VFFLR_INT_ENA_W1SX(1),\n\t\t\t     INTR_MASK(numvfs));\n\t}\n\treturn 0;\n}\n\nstatic int otx2_pf_flr_init(struct otx2_nic *pf, int num_vfs)\n{\n\tint vf;\n\n\tpf->flr_wq = alloc_ordered_workqueue(\"otx2_pf_flr_wq\", WQ_HIGHPRI);\n\tif (!pf->flr_wq)\n\t\treturn -ENOMEM;\n\n\tpf->flr_wrk = devm_kcalloc(pf->dev, num_vfs,\n\t\t\t\t   sizeof(struct flr_work), GFP_KERNEL);\n\tif (!pf->flr_wrk) {\n\t\tdestroy_workqueue(pf->flr_wq);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (vf = 0; vf < num_vfs; vf++) {\n\t\tpf->flr_wrk[vf].pf = pf;\n\t\tINIT_WORK(&pf->flr_wrk[vf].work, otx2_flr_handler);\n\t}\n\n\treturn 0;\n}\n\nstatic void otx2_queue_work(struct mbox *mw, struct workqueue_struct *mbox_wq,\n\t\t\t    int first, int mdevs, u64 intr, int type)\n{\n\tstruct otx2_mbox_dev *mdev;\n\tstruct otx2_mbox *mbox;\n\tstruct mbox_hdr *hdr;\n\tint i;\n\n\tfor (i = first; i < mdevs; i++) {\n\t\t \n\t\tif (!(intr & BIT_ULL(i - first)))\n\t\t\tcontinue;\n\n\t\tmbox = &mw->mbox;\n\t\tmdev = &mbox->dev[i];\n\t\tif (type == TYPE_PFAF)\n\t\t\totx2_sync_mbox_bbuf(mbox, i);\n\t\thdr = mdev->mbase + mbox->rx_start;\n\t\t \n\t\tif (hdr->num_msgs) {\n\t\t\tmw[i].num_msgs = hdr->num_msgs;\n\t\t\thdr->num_msgs = 0;\n\t\t\tif (type == TYPE_PFAF)\n\t\t\t\tmemset(mbox->hwbase + mbox->rx_start, 0,\n\t\t\t\t       ALIGN(sizeof(struct mbox_hdr),\n\t\t\t\t\t     sizeof(u64)));\n\n\t\t\tqueue_work(mbox_wq, &mw[i].mbox_wrk);\n\t\t}\n\n\t\tmbox = &mw->mbox_up;\n\t\tmdev = &mbox->dev[i];\n\t\tif (type == TYPE_PFAF)\n\t\t\totx2_sync_mbox_bbuf(mbox, i);\n\t\thdr = mdev->mbase + mbox->rx_start;\n\t\tif (hdr->num_msgs) {\n\t\t\tmw[i].up_num_msgs = hdr->num_msgs;\n\t\t\thdr->num_msgs = 0;\n\t\t\tif (type == TYPE_PFAF)\n\t\t\t\tmemset(mbox->hwbase + mbox->rx_start, 0,\n\t\t\t\t       ALIGN(sizeof(struct mbox_hdr),\n\t\t\t\t\t     sizeof(u64)));\n\n\t\t\tqueue_work(mbox_wq, &mw[i].mbox_up_wrk);\n\t\t}\n\t}\n}\n\nstatic void otx2_forward_msg_pfvf(struct otx2_mbox_dev *mdev,\n\t\t\t\t  struct otx2_mbox *pfvf_mbox, void *bbuf_base,\n\t\t\t\t  int devid)\n{\n\tstruct otx2_mbox_dev *src_mdev = mdev;\n\tint offset;\n\n\t \n\tsmp_wmb();\n\n\toffset = pfvf_mbox->trigger | (devid << pfvf_mbox->tr_shift);\n\twriteq(1, (void __iomem *)pfvf_mbox->reg_base + offset);\n\n\t \n\tsrc_mdev->mbase = bbuf_base;\n}\n\nstatic int otx2_forward_vf_mbox_msgs(struct otx2_nic *pf,\n\t\t\t\t     struct otx2_mbox *src_mbox,\n\t\t\t\t     int dir, int vf, int num_msgs)\n{\n\tstruct otx2_mbox_dev *src_mdev, *dst_mdev;\n\tstruct mbox_hdr *mbox_hdr;\n\tstruct mbox_hdr *req_hdr;\n\tstruct mbox *dst_mbox;\n\tint dst_size, err;\n\n\tif (dir == MBOX_DIR_PFAF) {\n\t\t \n\t\tsrc_mdev = &src_mbox->dev[vf];\n\t\tmbox_hdr = src_mbox->hwbase +\n\t\t\t\tsrc_mbox->rx_start + (vf * MBOX_SIZE);\n\n\t\tdst_mbox = &pf->mbox;\n\t\tdst_size = dst_mbox->mbox.tx_size -\n\t\t\t\tALIGN(sizeof(*mbox_hdr), MBOX_MSG_ALIGN);\n\t\t \n\t\tif (mbox_hdr->msg_size > dst_size || !mbox_hdr->msg_size)\n\t\t\treturn -EINVAL;\n\n\t\tdst_mdev = &dst_mbox->mbox.dev[0];\n\n\t\tmutex_lock(&pf->mbox.lock);\n\t\tdst_mdev->mbase = src_mdev->mbase;\n\t\tdst_mdev->msg_size = mbox_hdr->msg_size;\n\t\tdst_mdev->num_msgs = num_msgs;\n\t\terr = otx2_sync_mbox_msg(dst_mbox);\n\t\t \n\t\tif (err == -EIO) {\n\t\t\tdev_warn(pf->dev,\n\t\t\t\t \"AF not responding to VF%d messages\\n\", vf);\n\t\t\t \n\t\t\tdst_mdev->mbase = pf->mbox.bbuf_base;\n\t\t\tmutex_unlock(&pf->mbox.lock);\n\t\t\treturn err;\n\t\t}\n\t\t \n\t\treq_hdr = (struct mbox_hdr *)(dst_mdev->mbase +\n\t\t\t\t\t      dst_mbox->mbox.rx_start);\n\t\treq_hdr->num_msgs = num_msgs;\n\n\t\totx2_forward_msg_pfvf(dst_mdev, &pf->mbox_pfvf[0].mbox,\n\t\t\t\t      pf->mbox.bbuf_base, vf);\n\t\tmutex_unlock(&pf->mbox.lock);\n\t} else if (dir == MBOX_DIR_PFVF_UP) {\n\t\tsrc_mdev = &src_mbox->dev[0];\n\t\tmbox_hdr = src_mbox->hwbase + src_mbox->rx_start;\n\t\treq_hdr = (struct mbox_hdr *)(src_mdev->mbase +\n\t\t\t\t\t      src_mbox->rx_start);\n\t\treq_hdr->num_msgs = num_msgs;\n\n\t\tdst_mbox = &pf->mbox_pfvf[0];\n\t\tdst_size = dst_mbox->mbox_up.tx_size -\n\t\t\t\tALIGN(sizeof(*mbox_hdr), MBOX_MSG_ALIGN);\n\t\t \n\t\tif (mbox_hdr->msg_size > dst_size)\n\t\t\treturn -EINVAL;\n\n\t\tdst_mdev = &dst_mbox->mbox_up.dev[vf];\n\t\tdst_mdev->mbase = src_mdev->mbase;\n\t\tdst_mdev->msg_size = mbox_hdr->msg_size;\n\t\tdst_mdev->num_msgs = mbox_hdr->num_msgs;\n\t\terr = otx2_sync_mbox_up_msg(dst_mbox, vf);\n\t\tif (err) {\n\t\t\tdev_warn(pf->dev,\n\t\t\t\t \"VF%d is not responding to mailbox\\n\", vf);\n\t\t\treturn err;\n\t\t}\n\t} else if (dir == MBOX_DIR_VFPF_UP) {\n\t\treq_hdr = (struct mbox_hdr *)(src_mbox->dev[0].mbase +\n\t\t\t\t\t      src_mbox->rx_start);\n\t\treq_hdr->num_msgs = num_msgs;\n\t\totx2_forward_msg_pfvf(&pf->mbox_pfvf->mbox_up.dev[vf],\n\t\t\t\t      &pf->mbox.mbox_up,\n\t\t\t\t      pf->mbox_pfvf[vf].bbuf_base,\n\t\t\t\t      0);\n\t}\n\n\treturn 0;\n}\n\nstatic void otx2_pfvf_mbox_handler(struct work_struct *work)\n{\n\tstruct mbox_msghdr *msg = NULL;\n\tint offset, vf_idx, id, err;\n\tstruct otx2_mbox_dev *mdev;\n\tstruct mbox_hdr *req_hdr;\n\tstruct otx2_mbox *mbox;\n\tstruct mbox *vf_mbox;\n\tstruct otx2_nic *pf;\n\n\tvf_mbox = container_of(work, struct mbox, mbox_wrk);\n\tpf = vf_mbox->pfvf;\n\tvf_idx = vf_mbox - pf->mbox_pfvf;\n\n\tmbox = &pf->mbox_pfvf[0].mbox;\n\tmdev = &mbox->dev[vf_idx];\n\treq_hdr = (struct mbox_hdr *)(mdev->mbase + mbox->rx_start);\n\n\toffset = ALIGN(sizeof(*req_hdr), MBOX_MSG_ALIGN);\n\n\tfor (id = 0; id < vf_mbox->num_msgs; id++) {\n\t\tmsg = (struct mbox_msghdr *)(mdev->mbase + mbox->rx_start +\n\t\t\t\t\t     offset);\n\n\t\tif (msg->sig != OTX2_MBOX_REQ_SIG)\n\t\t\tgoto inval_msg;\n\n\t\t \n\t\tmsg->pcifunc &= RVU_PFVF_FUNC_MASK;\n\t\tmsg->pcifunc |= (vf_idx + 1) & RVU_PFVF_FUNC_MASK;\n\t\toffset = msg->next_msgoff;\n\t}\n\terr = otx2_forward_vf_mbox_msgs(pf, mbox, MBOX_DIR_PFAF, vf_idx,\n\t\t\t\t\tvf_mbox->num_msgs);\n\tif (err)\n\t\tgoto inval_msg;\n\treturn;\n\ninval_msg:\n\totx2_reply_invalid_msg(mbox, vf_idx, 0, msg->id);\n\totx2_mbox_msg_send(mbox, vf_idx);\n}\n\nstatic void otx2_pfvf_mbox_up_handler(struct work_struct *work)\n{\n\tstruct mbox *vf_mbox = container_of(work, struct mbox, mbox_up_wrk);\n\tstruct otx2_nic *pf = vf_mbox->pfvf;\n\tstruct otx2_mbox_dev *mdev;\n\tint offset, id, vf_idx = 0;\n\tstruct mbox_hdr *rsp_hdr;\n\tstruct mbox_msghdr *msg;\n\tstruct otx2_mbox *mbox;\n\n\tvf_idx = vf_mbox - pf->mbox_pfvf;\n\tmbox = &pf->mbox_pfvf[0].mbox_up;\n\tmdev = &mbox->dev[vf_idx];\n\n\trsp_hdr = (struct mbox_hdr *)(mdev->mbase + mbox->rx_start);\n\toffset = mbox->rx_start + ALIGN(sizeof(*rsp_hdr), MBOX_MSG_ALIGN);\n\n\tfor (id = 0; id < vf_mbox->up_num_msgs; id++) {\n\t\tmsg = mdev->mbase + offset;\n\n\t\tif (msg->id >= MBOX_MSG_MAX) {\n\t\t\tdev_err(pf->dev,\n\t\t\t\t\"Mbox msg with unknown ID 0x%x\\n\", msg->id);\n\t\t\tgoto end;\n\t\t}\n\n\t\tif (msg->sig != OTX2_MBOX_RSP_SIG) {\n\t\t\tdev_err(pf->dev,\n\t\t\t\t\"Mbox msg with wrong signature %x, ID 0x%x\\n\",\n\t\t\t\tmsg->sig, msg->id);\n\t\t\tgoto end;\n\t\t}\n\n\t\tswitch (msg->id) {\n\t\tcase MBOX_MSG_CGX_LINK_EVENT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (msg->rc)\n\t\t\t\tdev_err(pf->dev,\n\t\t\t\t\t\"Mbox msg response has err %d, ID 0x%x\\n\",\n\t\t\t\t\tmsg->rc, msg->id);\n\t\t\tbreak;\n\t\t}\n\nend:\n\t\toffset = mbox->rx_start + msg->next_msgoff;\n\t\tif (mdev->msgs_acked == (vf_mbox->up_num_msgs - 1))\n\t\t\t__otx2_mbox_reset(mbox, 0);\n\t\tmdev->msgs_acked++;\n\t}\n}\n\nstatic irqreturn_t otx2_pfvf_mbox_intr_handler(int irq, void *pf_irq)\n{\n\tstruct otx2_nic *pf = (struct otx2_nic *)(pf_irq);\n\tint vfs = pf->total_vfs;\n\tstruct mbox *mbox;\n\tu64 intr;\n\n\tmbox = pf->mbox_pfvf;\n\t \n\tif (vfs > 64) {\n\t\tintr = otx2_read64(pf, RVU_PF_VFPF_MBOX_INTX(1));\n\t\totx2_write64(pf, RVU_PF_VFPF_MBOX_INTX(1), intr);\n\t\totx2_queue_work(mbox, pf->mbox_pfvf_wq, 64, vfs, intr,\n\t\t\t\tTYPE_PFVF);\n\t\tif (intr)\n\t\t\ttrace_otx2_msg_interrupt(mbox->mbox.pdev, \"VF(s) to PF\", intr);\n\t\tvfs = 64;\n\t}\n\n\tintr = otx2_read64(pf, RVU_PF_VFPF_MBOX_INTX(0));\n\totx2_write64(pf, RVU_PF_VFPF_MBOX_INTX(0), intr);\n\n\totx2_queue_work(mbox, pf->mbox_pfvf_wq, 0, vfs, intr, TYPE_PFVF);\n\n\tif (intr)\n\t\ttrace_otx2_msg_interrupt(mbox->mbox.pdev, \"VF(s) to PF\", intr);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int otx2_pfvf_mbox_init(struct otx2_nic *pf, int numvfs)\n{\n\tvoid __iomem *hwbase;\n\tstruct mbox *mbox;\n\tint err, vf;\n\tu64 base;\n\n\tif (!numvfs)\n\t\treturn -EINVAL;\n\n\tpf->mbox_pfvf = devm_kcalloc(&pf->pdev->dev, numvfs,\n\t\t\t\t     sizeof(struct mbox), GFP_KERNEL);\n\tif (!pf->mbox_pfvf)\n\t\treturn -ENOMEM;\n\n\tpf->mbox_pfvf_wq = alloc_ordered_workqueue(\"otx2_pfvf_mailbox\",\n\t\t\t\t\t\t   WQ_HIGHPRI | WQ_MEM_RECLAIM);\n\tif (!pf->mbox_pfvf_wq)\n\t\treturn -ENOMEM;\n\n\t \n\tif (test_bit(CN10K_MBOX, &pf->hw.cap_flag))\n\t\tbase = pci_resource_start(pf->pdev, PCI_MBOX_BAR_NUM) +\n\t\t       MBOX_SIZE;\n\telse\n\t\tbase = readq((void __iomem *)((u64)pf->reg_base +\n\t\t\t\t\t      RVU_PF_VF_BAR4_ADDR));\n\n\thwbase = ioremap_wc(base, MBOX_SIZE * pf->total_vfs);\n\tif (!hwbase) {\n\t\terr = -ENOMEM;\n\t\tgoto free_wq;\n\t}\n\n\tmbox = &pf->mbox_pfvf[0];\n\terr = otx2_mbox_init(&mbox->mbox, hwbase, pf->pdev, pf->reg_base,\n\t\t\t     MBOX_DIR_PFVF, numvfs);\n\tif (err)\n\t\tgoto free_iomem;\n\n\terr = otx2_mbox_init(&mbox->mbox_up, hwbase, pf->pdev, pf->reg_base,\n\t\t\t     MBOX_DIR_PFVF_UP, numvfs);\n\tif (err)\n\t\tgoto free_iomem;\n\n\tfor (vf = 0; vf < numvfs; vf++) {\n\t\tmbox->pfvf = pf;\n\t\tINIT_WORK(&mbox->mbox_wrk, otx2_pfvf_mbox_handler);\n\t\tINIT_WORK(&mbox->mbox_up_wrk, otx2_pfvf_mbox_up_handler);\n\t\tmbox++;\n\t}\n\n\treturn 0;\n\nfree_iomem:\n\tif (hwbase)\n\t\tiounmap(hwbase);\nfree_wq:\n\tdestroy_workqueue(pf->mbox_pfvf_wq);\n\treturn err;\n}\n\nstatic void otx2_pfvf_mbox_destroy(struct otx2_nic *pf)\n{\n\tstruct mbox *mbox = &pf->mbox_pfvf[0];\n\n\tif (!mbox)\n\t\treturn;\n\n\tif (pf->mbox_pfvf_wq) {\n\t\tdestroy_workqueue(pf->mbox_pfvf_wq);\n\t\tpf->mbox_pfvf_wq = NULL;\n\t}\n\n\tif (mbox->mbox.hwbase)\n\t\tiounmap(mbox->mbox.hwbase);\n\n\totx2_mbox_destroy(&mbox->mbox);\n}\n\nstatic void otx2_enable_pfvf_mbox_intr(struct otx2_nic *pf, int numvfs)\n{\n\t \n\totx2_write64(pf, RVU_PF_VFPF_MBOX_INTX(0), ~0ull);\n\totx2_write64(pf, RVU_PF_VFPF_MBOX_INTX(1), ~0ull);\n\n\t \n\totx2_write64(pf, RVU_PF_VFPF_MBOX_INT_ENA_W1SX(0), INTR_MASK(numvfs));\n\tif (numvfs > 64) {\n\t\tnumvfs -= 64;\n\t\totx2_write64(pf, RVU_PF_VFPF_MBOX_INT_ENA_W1SX(1),\n\t\t\t     INTR_MASK(numvfs));\n\t}\n}\n\nstatic void otx2_disable_pfvf_mbox_intr(struct otx2_nic *pf, int numvfs)\n{\n\tint vector;\n\n\t \n\totx2_write64(pf, RVU_PF_VFPF_MBOX_INT_ENA_W1CX(0), ~0ull);\n\totx2_write64(pf, RVU_PF_VFPF_MBOX_INT_ENA_W1CX(1), ~0ull);\n\n\totx2_write64(pf, RVU_PF_VFPF_MBOX_INTX(0), ~0ull);\n\tvector = pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFPF_MBOX0);\n\tfree_irq(vector, pf);\n\n\tif (numvfs > 64) {\n\t\totx2_write64(pf, RVU_PF_VFPF_MBOX_INTX(1), ~0ull);\n\t\tvector = pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFPF_MBOX1);\n\t\tfree_irq(vector, pf);\n\t}\n}\n\nstatic int otx2_register_pfvf_mbox_intr(struct otx2_nic *pf, int numvfs)\n{\n\tstruct otx2_hw *hw = &pf->hw;\n\tchar *irq_name;\n\tint err;\n\n\t \n\tirq_name = &hw->irq_name[RVU_PF_INT_VEC_VFPF_MBOX0 * NAME_SIZE];\n\tif (pf->pcifunc)\n\t\tsnprintf(irq_name, NAME_SIZE,\n\t\t\t \"RVUPF%d_VF Mbox0\", rvu_get_pf(pf->pcifunc));\n\telse\n\t\tsnprintf(irq_name, NAME_SIZE, \"RVUPF_VF Mbox0\");\n\terr = request_irq(pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_VFPF_MBOX0),\n\t\t\t  otx2_pfvf_mbox_intr_handler, 0, irq_name, pf);\n\tif (err) {\n\t\tdev_err(pf->dev,\n\t\t\t\"RVUPF: IRQ registration failed for PFVF mbox0 irq\\n\");\n\t\treturn err;\n\t}\n\n\tif (numvfs > 64) {\n\t\t \n\t\tirq_name = &hw->irq_name[RVU_PF_INT_VEC_VFPF_MBOX1 * NAME_SIZE];\n\t\tif (pf->pcifunc)\n\t\t\tsnprintf(irq_name, NAME_SIZE,\n\t\t\t\t \"RVUPF%d_VF Mbox1\", rvu_get_pf(pf->pcifunc));\n\t\telse\n\t\t\tsnprintf(irq_name, NAME_SIZE, \"RVUPF_VF Mbox1\");\n\t\terr = request_irq(pci_irq_vector(pf->pdev,\n\t\t\t\t\t\t RVU_PF_INT_VEC_VFPF_MBOX1),\n\t\t\t\t\t\t otx2_pfvf_mbox_intr_handler,\n\t\t\t\t\t\t 0, irq_name, pf);\n\t\tif (err) {\n\t\t\tdev_err(pf->dev,\n\t\t\t\t\"RVUPF: IRQ registration failed for PFVF mbox1 irq\\n\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\totx2_enable_pfvf_mbox_intr(pf, numvfs);\n\n\treturn 0;\n}\n\nstatic void otx2_process_pfaf_mbox_msg(struct otx2_nic *pf,\n\t\t\t\t       struct mbox_msghdr *msg)\n{\n\tint devid;\n\n\tif (msg->id >= MBOX_MSG_MAX) {\n\t\tdev_err(pf->dev,\n\t\t\t\"Mbox msg with unknown ID 0x%x\\n\", msg->id);\n\t\treturn;\n\t}\n\n\tif (msg->sig != OTX2_MBOX_RSP_SIG) {\n\t\tdev_err(pf->dev,\n\t\t\t\"Mbox msg with wrong signature %x, ID 0x%x\\n\",\n\t\t\t msg->sig, msg->id);\n\t\treturn;\n\t}\n\n\t \n\tdevid = msg->pcifunc & RVU_PFVF_FUNC_MASK;\n\tif (devid) {\n\t\tstruct otx2_vf_config *config = &pf->vf_configs[devid - 1];\n\t\tstruct delayed_work *dwork;\n\n\t\tswitch (msg->id) {\n\t\tcase MBOX_MSG_NIX_LF_START_RX:\n\t\t\tconfig->intf_down = false;\n\t\t\tdwork = &config->link_event_work;\n\t\t\tschedule_delayed_work(dwork, msecs_to_jiffies(100));\n\t\t\tbreak;\n\t\tcase MBOX_MSG_NIX_LF_STOP_RX:\n\t\t\tconfig->intf_down = true;\n\t\t\tbreak;\n\t\t}\n\n\t\treturn;\n\t}\n\n\tswitch (msg->id) {\n\tcase MBOX_MSG_READY:\n\t\tpf->pcifunc = msg->pcifunc;\n\t\tbreak;\n\tcase MBOX_MSG_MSIX_OFFSET:\n\t\tmbox_handler_msix_offset(pf, (struct msix_offset_rsp *)msg);\n\t\tbreak;\n\tcase MBOX_MSG_NPA_LF_ALLOC:\n\t\tmbox_handler_npa_lf_alloc(pf, (struct npa_lf_alloc_rsp *)msg);\n\t\tbreak;\n\tcase MBOX_MSG_NIX_LF_ALLOC:\n\t\tmbox_handler_nix_lf_alloc(pf, (struct nix_lf_alloc_rsp *)msg);\n\t\tbreak;\n\tcase MBOX_MSG_NIX_BP_ENABLE:\n\t\tmbox_handler_nix_bp_enable(pf, (struct nix_bp_cfg_rsp *)msg);\n\t\tbreak;\n\tcase MBOX_MSG_CGX_STATS:\n\t\tmbox_handler_cgx_stats(pf, (struct cgx_stats_rsp *)msg);\n\t\tbreak;\n\tcase MBOX_MSG_CGX_FEC_STATS:\n\t\tmbox_handler_cgx_fec_stats(pf, (struct cgx_fec_stats_rsp *)msg);\n\t\tbreak;\n\tdefault:\n\t\tif (msg->rc)\n\t\t\tdev_err(pf->dev,\n\t\t\t\t\"Mbox msg response has err %d, ID 0x%x\\n\",\n\t\t\t\tmsg->rc, msg->id);\n\t\tbreak;\n\t}\n}\n\nstatic void otx2_pfaf_mbox_handler(struct work_struct *work)\n{\n\tstruct otx2_mbox_dev *mdev;\n\tstruct mbox_hdr *rsp_hdr;\n\tstruct mbox_msghdr *msg;\n\tstruct otx2_mbox *mbox;\n\tstruct mbox *af_mbox;\n\tstruct otx2_nic *pf;\n\tint offset, id;\n\n\taf_mbox = container_of(work, struct mbox, mbox_wrk);\n\tmbox = &af_mbox->mbox;\n\tmdev = &mbox->dev[0];\n\trsp_hdr = (struct mbox_hdr *)(mdev->mbase + mbox->rx_start);\n\n\toffset = mbox->rx_start + ALIGN(sizeof(*rsp_hdr), MBOX_MSG_ALIGN);\n\tpf = af_mbox->pfvf;\n\n\tfor (id = 0; id < af_mbox->num_msgs; id++) {\n\t\tmsg = (struct mbox_msghdr *)(mdev->mbase + offset);\n\t\totx2_process_pfaf_mbox_msg(pf, msg);\n\t\toffset = mbox->rx_start + msg->next_msgoff;\n\t\tif (mdev->msgs_acked == (af_mbox->num_msgs - 1))\n\t\t\t__otx2_mbox_reset(mbox, 0);\n\t\tmdev->msgs_acked++;\n\t}\n\n}\n\nstatic void otx2_handle_link_event(struct otx2_nic *pf)\n{\n\tstruct cgx_link_user_info *linfo = &pf->linfo;\n\tstruct net_device *netdev = pf->netdev;\n\n\tpr_info(\"%s NIC Link is %s %d Mbps %s duplex\\n\", netdev->name,\n\t\tlinfo->link_up ? \"UP\" : \"DOWN\", linfo->speed,\n\t\tlinfo->full_duplex ? \"Full\" : \"Half\");\n\tif (linfo->link_up) {\n\t\tnetif_carrier_on(netdev);\n\t\tnetif_tx_start_all_queues(netdev);\n\t} else {\n\t\tnetif_tx_stop_all_queues(netdev);\n\t\tnetif_carrier_off(netdev);\n\t}\n}\n\nint otx2_mbox_up_handler_mcs_intr_notify(struct otx2_nic *pf,\n\t\t\t\t\t struct mcs_intr_info *event,\n\t\t\t\t\t struct msg_rsp *rsp)\n{\n\tcn10k_handle_mcs_event(pf, event);\n\n\treturn 0;\n}\n\nint otx2_mbox_up_handler_cgx_link_event(struct otx2_nic *pf,\n\t\t\t\t\tstruct cgx_link_info_msg *msg,\n\t\t\t\t\tstruct msg_rsp *rsp)\n{\n\tint i;\n\n\t \n\tpf->linfo = msg->link_info;\n\n\t \n\tfor (i = 0; i < pci_num_vf(pf->pdev); i++) {\n\t\tstruct otx2_vf_config *config = &pf->vf_configs[i];\n\t\tstruct delayed_work *dwork = &config->link_event_work;\n\n\t\tif (config->intf_down)\n\t\t\tcontinue;\n\n\t\tschedule_delayed_work(dwork, msecs_to_jiffies(100));\n\t}\n\n\t \n\tif (pf->flags & OTX2_FLAG_INTF_DOWN)\n\t\treturn 0;\n\n\totx2_handle_link_event(pf);\n\treturn 0;\n}\n\nstatic int otx2_process_mbox_msg_up(struct otx2_nic *pf,\n\t\t\t\t    struct mbox_msghdr *req)\n{\n\t \n\tif (req->sig != OTX2_MBOX_REQ_SIG) {\n\t\totx2_reply_invalid_msg(&pf->mbox.mbox_up, 0, 0, req->id);\n\t\treturn -ENODEV;\n\t}\n\n\tswitch (req->id) {\n#define M(_name, _id, _fn_name, _req_type, _rsp_type)\t\t\t\\\n\tcase _id: {\t\t\t\t\t\t\t\\\n\t\tstruct _rsp_type *rsp;\t\t\t\t\t\\\n\t\tint err;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\trsp = (struct _rsp_type *)otx2_mbox_alloc_msg(\t\t\\\n\t\t\t&pf->mbox.mbox_up, 0,\t\t\t\t\\\n\t\t\tsizeof(struct _rsp_type));\t\t\t\\\n\t\tif (!rsp)\t\t\t\t\t\t\\\n\t\t\treturn -ENOMEM;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\trsp->hdr.id = _id;\t\t\t\t\t\\\n\t\trsp->hdr.sig = OTX2_MBOX_RSP_SIG;\t\t\t\\\n\t\trsp->hdr.pcifunc = 0;\t\t\t\t\t\\\n\t\trsp->hdr.rc = 0;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\terr = otx2_mbox_up_handler_ ## _fn_name(\t\t\\\n\t\t\tpf, (struct _req_type *)req, rsp);\t\t\\\n\t\treturn err;\t\t\t\t\t\t\\\n\t}\nMBOX_UP_CGX_MESSAGES\nMBOX_UP_MCS_MESSAGES\n#undef M\n\t\tbreak;\n\tdefault:\n\t\totx2_reply_invalid_msg(&pf->mbox.mbox_up, 0, 0, req->id);\n\t\treturn -ENODEV;\n\t}\n\treturn 0;\n}\n\nstatic void otx2_pfaf_mbox_up_handler(struct work_struct *work)\n{\n\tstruct mbox *af_mbox = container_of(work, struct mbox, mbox_up_wrk);\n\tstruct otx2_mbox *mbox = &af_mbox->mbox_up;\n\tstruct otx2_mbox_dev *mdev = &mbox->dev[0];\n\tstruct otx2_nic *pf = af_mbox->pfvf;\n\tint offset, id, devid = 0;\n\tstruct mbox_hdr *rsp_hdr;\n\tstruct mbox_msghdr *msg;\n\n\trsp_hdr = (struct mbox_hdr *)(mdev->mbase + mbox->rx_start);\n\n\toffset = mbox->rx_start + ALIGN(sizeof(*rsp_hdr), MBOX_MSG_ALIGN);\n\n\tfor (id = 0; id < af_mbox->up_num_msgs; id++) {\n\t\tmsg = (struct mbox_msghdr *)(mdev->mbase + offset);\n\n\t\tdevid = msg->pcifunc & RVU_PFVF_FUNC_MASK;\n\t\t \n\t\tif (!devid)\n\t\t\totx2_process_mbox_msg_up(pf, msg);\n\t\toffset = mbox->rx_start + msg->next_msgoff;\n\t}\n\tif (devid) {\n\t\totx2_forward_vf_mbox_msgs(pf, &pf->mbox.mbox_up,\n\t\t\t\t\t  MBOX_DIR_PFVF_UP, devid - 1,\n\t\t\t\t\t  af_mbox->up_num_msgs);\n\t\treturn;\n\t}\n\n\totx2_mbox_msg_send(mbox, 0);\n}\n\nstatic irqreturn_t otx2_pfaf_mbox_intr_handler(int irq, void *pf_irq)\n{\n\tstruct otx2_nic *pf = (struct otx2_nic *)pf_irq;\n\tstruct mbox *mbox;\n\n\t \n\totx2_write64(pf, RVU_PF_INT, BIT_ULL(0));\n\n\tmbox = &pf->mbox;\n\n\ttrace_otx2_msg_interrupt(mbox->mbox.pdev, \"AF to PF\", BIT_ULL(0));\n\n\totx2_queue_work(mbox, pf->mbox_wq, 0, 1, 1, TYPE_PFAF);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void otx2_disable_mbox_intr(struct otx2_nic *pf)\n{\n\tint vector = pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_AFPF_MBOX);\n\n\t \n\totx2_write64(pf, RVU_PF_INT_ENA_W1C, BIT_ULL(0));\n\tfree_irq(vector, pf);\n}\n\nstatic int otx2_register_mbox_intr(struct otx2_nic *pf, bool probe_af)\n{\n\tstruct otx2_hw *hw = &pf->hw;\n\tstruct msg_req *req;\n\tchar *irq_name;\n\tint err;\n\n\t \n\tirq_name = &hw->irq_name[RVU_PF_INT_VEC_AFPF_MBOX * NAME_SIZE];\n\tsnprintf(irq_name, NAME_SIZE, \"RVUPFAF Mbox\");\n\terr = request_irq(pci_irq_vector(pf->pdev, RVU_PF_INT_VEC_AFPF_MBOX),\n\t\t\t  otx2_pfaf_mbox_intr_handler, 0, irq_name, pf);\n\tif (err) {\n\t\tdev_err(pf->dev,\n\t\t\t\"RVUPF: IRQ registration failed for PFAF mbox irq\\n\");\n\t\treturn err;\n\t}\n\n\t \n\totx2_write64(pf, RVU_PF_INT, BIT_ULL(0));\n\totx2_write64(pf, RVU_PF_INT_ENA_W1S, BIT_ULL(0));\n\n\tif (!probe_af)\n\t\treturn 0;\n\n\t \n\treq = otx2_mbox_alloc_msg_ready(&pf->mbox);\n\tif (!req) {\n\t\totx2_disable_mbox_intr(pf);\n\t\treturn -ENOMEM;\n\t}\n\terr = otx2_sync_mbox_msg(&pf->mbox);\n\tif (err) {\n\t\tdev_warn(pf->dev,\n\t\t\t \"AF not responding to mailbox, deferring probe\\n\");\n\t\totx2_disable_mbox_intr(pf);\n\t\treturn -EPROBE_DEFER;\n\t}\n\n\treturn 0;\n}\n\nstatic void otx2_pfaf_mbox_destroy(struct otx2_nic *pf)\n{\n\tstruct mbox *mbox = &pf->mbox;\n\n\tif (pf->mbox_wq) {\n\t\tdestroy_workqueue(pf->mbox_wq);\n\t\tpf->mbox_wq = NULL;\n\t}\n\n\tif (mbox->mbox.hwbase)\n\t\tiounmap((void __iomem *)mbox->mbox.hwbase);\n\n\totx2_mbox_destroy(&mbox->mbox);\n\totx2_mbox_destroy(&mbox->mbox_up);\n}\n\nstatic int otx2_pfaf_mbox_init(struct otx2_nic *pf)\n{\n\tstruct mbox *mbox = &pf->mbox;\n\tvoid __iomem *hwbase;\n\tint err;\n\n\tmbox->pfvf = pf;\n\tpf->mbox_wq = alloc_ordered_workqueue(\"otx2_pfaf_mailbox\",\n\t\t\t\t\t      WQ_HIGHPRI | WQ_MEM_RECLAIM);\n\tif (!pf->mbox_wq)\n\t\treturn -ENOMEM;\n\n\t \n\thwbase = ioremap_wc(pci_resource_start(pf->pdev, PCI_MBOX_BAR_NUM),\n\t\t\t    MBOX_SIZE);\n\tif (!hwbase) {\n\t\tdev_err(pf->dev, \"Unable to map PFAF mailbox region\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\terr = otx2_mbox_init(&mbox->mbox, hwbase, pf->pdev, pf->reg_base,\n\t\t\t     MBOX_DIR_PFAF, 1);\n\tif (err)\n\t\tgoto exit;\n\n\terr = otx2_mbox_init(&mbox->mbox_up, hwbase, pf->pdev, pf->reg_base,\n\t\t\t     MBOX_DIR_PFAF_UP, 1);\n\tif (err)\n\t\tgoto exit;\n\n\terr = otx2_mbox_bbuf_init(mbox, pf->pdev);\n\tif (err)\n\t\tgoto exit;\n\n\tINIT_WORK(&mbox->mbox_wrk, otx2_pfaf_mbox_handler);\n\tINIT_WORK(&mbox->mbox_up_wrk, otx2_pfaf_mbox_up_handler);\n\tmutex_init(&mbox->lock);\n\n\treturn 0;\nexit:\n\totx2_pfaf_mbox_destroy(pf);\n\treturn err;\n}\n\nstatic int otx2_cgx_config_linkevents(struct otx2_nic *pf, bool enable)\n{\n\tstruct msg_req *msg;\n\tint err;\n\n\tmutex_lock(&pf->mbox.lock);\n\tif (enable)\n\t\tmsg = otx2_mbox_alloc_msg_cgx_start_linkevents(&pf->mbox);\n\telse\n\t\tmsg = otx2_mbox_alloc_msg_cgx_stop_linkevents(&pf->mbox);\n\n\tif (!msg) {\n\t\tmutex_unlock(&pf->mbox.lock);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = otx2_sync_mbox_msg(&pf->mbox);\n\tmutex_unlock(&pf->mbox.lock);\n\treturn err;\n}\n\nstatic int otx2_cgx_config_loopback(struct otx2_nic *pf, bool enable)\n{\n\tstruct msg_req *msg;\n\tint err;\n\n\tif (enable && !bitmap_empty(pf->flow_cfg->dmacflt_bmap,\n\t\t\t\t    pf->flow_cfg->dmacflt_max_flows))\n\t\tnetdev_warn(pf->netdev,\n\t\t\t    \"CGX/RPM internal loopback might not work as DMAC filters are active\\n\");\n\n\tmutex_lock(&pf->mbox.lock);\n\tif (enable)\n\t\tmsg = otx2_mbox_alloc_msg_cgx_intlbk_enable(&pf->mbox);\n\telse\n\t\tmsg = otx2_mbox_alloc_msg_cgx_intlbk_disable(&pf->mbox);\n\n\tif (!msg) {\n\t\tmutex_unlock(&pf->mbox.lock);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = otx2_sync_mbox_msg(&pf->mbox);\n\tmutex_unlock(&pf->mbox.lock);\n\treturn err;\n}\n\nint otx2_set_real_num_queues(struct net_device *netdev,\n\t\t\t     int tx_queues, int rx_queues)\n{\n\tint err;\n\n\terr = netif_set_real_num_tx_queues(netdev, tx_queues);\n\tif (err) {\n\t\tnetdev_err(netdev,\n\t\t\t   \"Failed to set no of Tx queues: %d\\n\", tx_queues);\n\t\treturn err;\n\t}\n\n\terr = netif_set_real_num_rx_queues(netdev, rx_queues);\n\tif (err)\n\t\tnetdev_err(netdev,\n\t\t\t   \"Failed to set no of Rx queues: %d\\n\", rx_queues);\n\treturn err;\n}\nEXPORT_SYMBOL(otx2_set_real_num_queues);\n\nstatic char *nix_sqoperr_e_str[NIX_SQOPERR_MAX] = {\n\t\"NIX_SQOPERR_OOR\",\n\t\"NIX_SQOPERR_CTX_FAULT\",\n\t\"NIX_SQOPERR_CTX_POISON\",\n\t\"NIX_SQOPERR_DISABLED\",\n\t\"NIX_SQOPERR_SIZE_ERR\",\n\t\"NIX_SQOPERR_OFLOW\",\n\t\"NIX_SQOPERR_SQB_NULL\",\n\t\"NIX_SQOPERR_SQB_FAULT\",\n\t\"NIX_SQOPERR_SQE_SZ_ZERO\",\n};\n\nstatic char *nix_mnqerr_e_str[NIX_MNQERR_MAX] = {\n\t\"NIX_MNQERR_SQ_CTX_FAULT\",\n\t\"NIX_MNQERR_SQ_CTX_POISON\",\n\t\"NIX_MNQERR_SQB_FAULT\",\n\t\"NIX_MNQERR_SQB_POISON\",\n\t\"NIX_MNQERR_TOTAL_ERR\",\n\t\"NIX_MNQERR_LSO_ERR\",\n\t\"NIX_MNQERR_CQ_QUERY_ERR\",\n\t\"NIX_MNQERR_MAX_SQE_SIZE_ERR\",\n\t\"NIX_MNQERR_MAXLEN_ERR\",\n\t\"NIX_MNQERR_SQE_SIZEM1_ZERO\",\n};\n\nstatic char *nix_snd_status_e_str[NIX_SND_STATUS_MAX] =  {\n\t[NIX_SND_STATUS_GOOD] = \"NIX_SND_STATUS_GOOD\",\n\t[NIX_SND_STATUS_SQ_CTX_FAULT] = \"NIX_SND_STATUS_SQ_CTX_FAULT\",\n\t[NIX_SND_STATUS_SQ_CTX_POISON] = \"NIX_SND_STATUS_SQ_CTX_POISON\",\n\t[NIX_SND_STATUS_SQB_FAULT] = \"NIX_SND_STATUS_SQB_FAULT\",\n\t[NIX_SND_STATUS_SQB_POISON] = \"NIX_SND_STATUS_SQB_POISON\",\n\t[NIX_SND_STATUS_HDR_ERR] = \"NIX_SND_STATUS_HDR_ERR\",\n\t[NIX_SND_STATUS_EXT_ERR] = \"NIX_SND_STATUS_EXT_ERR\",\n\t[NIX_SND_STATUS_JUMP_FAULT] = \"NIX_SND_STATUS_JUMP_FAULT\",\n\t[NIX_SND_STATUS_JUMP_POISON] = \"NIX_SND_STATUS_JUMP_POISON\",\n\t[NIX_SND_STATUS_CRC_ERR] = \"NIX_SND_STATUS_CRC_ERR\",\n\t[NIX_SND_STATUS_IMM_ERR] = \"NIX_SND_STATUS_IMM_ERR\",\n\t[NIX_SND_STATUS_SG_ERR] = \"NIX_SND_STATUS_SG_ERR\",\n\t[NIX_SND_STATUS_MEM_ERR] = \"NIX_SND_STATUS_MEM_ERR\",\n\t[NIX_SND_STATUS_INVALID_SUBDC] = \"NIX_SND_STATUS_INVALID_SUBDC\",\n\t[NIX_SND_STATUS_SUBDC_ORDER_ERR] = \"NIX_SND_STATUS_SUBDC_ORDER_ERR\",\n\t[NIX_SND_STATUS_DATA_FAULT] = \"NIX_SND_STATUS_DATA_FAULT\",\n\t[NIX_SND_STATUS_DATA_POISON] = \"NIX_SND_STATUS_DATA_POISON\",\n\t[NIX_SND_STATUS_NPC_DROP_ACTION] = \"NIX_SND_STATUS_NPC_DROP_ACTION\",\n\t[NIX_SND_STATUS_LOCK_VIOL] = \"NIX_SND_STATUS_LOCK_VIOL\",\n\t[NIX_SND_STATUS_NPC_UCAST_CHAN_ERR] = \"NIX_SND_STAT_NPC_UCAST_CHAN_ERR\",\n\t[NIX_SND_STATUS_NPC_MCAST_CHAN_ERR] = \"NIX_SND_STAT_NPC_MCAST_CHAN_ERR\",\n\t[NIX_SND_STATUS_NPC_MCAST_ABORT] = \"NIX_SND_STATUS_NPC_MCAST_ABORT\",\n\t[NIX_SND_STATUS_NPC_VTAG_PTR_ERR] = \"NIX_SND_STATUS_NPC_VTAG_PTR_ERR\",\n\t[NIX_SND_STATUS_NPC_VTAG_SIZE_ERR] = \"NIX_SND_STATUS_NPC_VTAG_SIZE_ERR\",\n\t[NIX_SND_STATUS_SEND_MEM_FAULT] = \"NIX_SND_STATUS_SEND_MEM_FAULT\",\n\t[NIX_SND_STATUS_SEND_STATS_ERR] = \"NIX_SND_STATUS_SEND_STATS_ERR\",\n};\n\nstatic irqreturn_t otx2_q_intr_handler(int irq, void *data)\n{\n\tstruct otx2_nic *pf = data;\n\tstruct otx2_snd_queue *sq;\n\tu64 val, *ptr;\n\tu64 qidx = 0;\n\n\t \n\tfor (qidx = 0; qidx < pf->qset.cq_cnt; qidx++) {\n\t\tptr = otx2_get_regaddr(pf, NIX_LF_CQ_OP_INT);\n\t\tval = otx2_atomic64_add((qidx << 44), ptr);\n\n\t\totx2_write64(pf, NIX_LF_CQ_OP_INT, (qidx << 44) |\n\t\t\t     (val & NIX_CQERRINT_BITS));\n\t\tif (!(val & (NIX_CQERRINT_BITS | BIT_ULL(42))))\n\t\t\tcontinue;\n\n\t\tif (val & BIT_ULL(42)) {\n\t\t\tnetdev_err(pf->netdev,\n\t\t\t\t   \"CQ%lld: error reading NIX_LF_CQ_OP_INT, NIX_LF_ERR_INT 0x%llx\\n\",\n\t\t\t\t   qidx, otx2_read64(pf, NIX_LF_ERR_INT));\n\t\t} else {\n\t\t\tif (val & BIT_ULL(NIX_CQERRINT_DOOR_ERR))\n\t\t\t\tnetdev_err(pf->netdev, \"CQ%lld: Doorbell error\",\n\t\t\t\t\t   qidx);\n\t\t\tif (val & BIT_ULL(NIX_CQERRINT_CQE_FAULT))\n\t\t\t\tnetdev_err(pf->netdev,\n\t\t\t\t\t   \"CQ%lld: Memory fault on CQE write to LLC/DRAM\",\n\t\t\t\t\t   qidx);\n\t\t}\n\n\t\tschedule_work(&pf->reset_task);\n\t}\n\n\t \n\tfor (qidx = 0; qidx < otx2_get_total_tx_queues(pf); qidx++) {\n\t\tu64 sq_op_err_dbg, mnq_err_dbg, snd_err_dbg;\n\t\tu8 sq_op_err_code, mnq_err_code, snd_err_code;\n\n\t\tsq = &pf->qset.sq[qidx];\n\t\tif (!sq->sqb_ptrs)\n\t\t\tcontinue;\n\n\t\t \n\n\t\tptr = otx2_get_regaddr(pf, NIX_LF_SQ_OP_INT);\n\t\tval = otx2_atomic64_add((qidx << 44), ptr);\n\t\totx2_write64(pf, NIX_LF_SQ_OP_INT, (qidx << 44) |\n\t\t\t     (val & NIX_SQINT_BITS));\n\n\t\tif (val & BIT_ULL(42)) {\n\t\t\tnetdev_err(pf->netdev,\n\t\t\t\t   \"SQ%lld: error reading NIX_LF_SQ_OP_INT, NIX_LF_ERR_INT 0x%llx\\n\",\n\t\t\t\t   qidx, otx2_read64(pf, NIX_LF_ERR_INT));\n\t\t\tgoto done;\n\t\t}\n\n\t\tsq_op_err_dbg = otx2_read64(pf, NIX_LF_SQ_OP_ERR_DBG);\n\t\tif (!(sq_op_err_dbg & BIT(44)))\n\t\t\tgoto chk_mnq_err_dbg;\n\n\t\tsq_op_err_code = FIELD_GET(GENMASK(7, 0), sq_op_err_dbg);\n\t\tnetdev_err(pf->netdev,\n\t\t\t   \"SQ%lld: NIX_LF_SQ_OP_ERR_DBG(0x%llx)  err=%s(%#x)\\n\",\n\t\t\t   qidx, sq_op_err_dbg,\n\t\t\t   nix_sqoperr_e_str[sq_op_err_code],\n\t\t\t   sq_op_err_code);\n\n\t\totx2_write64(pf, NIX_LF_SQ_OP_ERR_DBG, BIT_ULL(44));\n\n\t\tif (sq_op_err_code == NIX_SQOPERR_SQB_NULL)\n\t\t\tgoto chk_mnq_err_dbg;\n\n\t\t \n\nchk_mnq_err_dbg:\n\t\tmnq_err_dbg = otx2_read64(pf, NIX_LF_MNQ_ERR_DBG);\n\t\tif (!(mnq_err_dbg & BIT(44)))\n\t\t\tgoto chk_snd_err_dbg;\n\n\t\tmnq_err_code = FIELD_GET(GENMASK(7, 0), mnq_err_dbg);\n\t\tnetdev_err(pf->netdev,\n\t\t\t   \"SQ%lld: NIX_LF_MNQ_ERR_DBG(0x%llx)  err=%s(%#x)\\n\",\n\t\t\t   qidx, mnq_err_dbg,  nix_mnqerr_e_str[mnq_err_code],\n\t\t\t   mnq_err_code);\n\t\totx2_write64(pf, NIX_LF_MNQ_ERR_DBG, BIT_ULL(44));\n\nchk_snd_err_dbg:\n\t\tsnd_err_dbg = otx2_read64(pf, NIX_LF_SEND_ERR_DBG);\n\t\tif (snd_err_dbg & BIT(44)) {\n\t\t\tsnd_err_code = FIELD_GET(GENMASK(7, 0), snd_err_dbg);\n\t\t\tnetdev_err(pf->netdev,\n\t\t\t\t   \"SQ%lld: NIX_LF_SND_ERR_DBG:0x%llx err=%s(%#x)\\n\",\n\t\t\t\t   qidx, snd_err_dbg,\n\t\t\t\t   nix_snd_status_e_str[snd_err_code],\n\t\t\t\t   snd_err_code);\n\t\t\totx2_write64(pf, NIX_LF_SEND_ERR_DBG, BIT_ULL(44));\n\t\t}\n\ndone:\n\t\t \n\t\tif (val & BIT_ULL(NIX_SQINT_SQB_ALLOC_FAIL))\n\t\t\tnetdev_err(pf->netdev, \"SQ%lld: SQB allocation failed\",\n\t\t\t\t   qidx);\n\n\t\tschedule_work(&pf->reset_task);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t otx2_cq_intr_handler(int irq, void *cq_irq)\n{\n\tstruct otx2_cq_poll *cq_poll = (struct otx2_cq_poll *)cq_irq;\n\tstruct otx2_nic *pf = (struct otx2_nic *)cq_poll->dev;\n\tint qidx = cq_poll->cint_idx;\n\n\t \n\totx2_write64(pf, NIX_LF_CINTX_ENA_W1C(qidx), BIT_ULL(0));\n\n\t \n\tpf->napi_events++;\n\tnapi_schedule_irqoff(&cq_poll->napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void otx2_disable_napi(struct otx2_nic *pf)\n{\n\tstruct otx2_qset *qset = &pf->qset;\n\tstruct otx2_cq_poll *cq_poll;\n\tint qidx;\n\n\tfor (qidx = 0; qidx < pf->hw.cint_cnt; qidx++) {\n\t\tcq_poll = &qset->napi[qidx];\n\t\tcancel_work_sync(&cq_poll->dim.work);\n\t\tnapi_disable(&cq_poll->napi);\n\t\tnetif_napi_del(&cq_poll->napi);\n\t}\n}\n\nstatic void otx2_free_cq_res(struct otx2_nic *pf)\n{\n\tstruct otx2_qset *qset = &pf->qset;\n\tstruct otx2_cq_queue *cq;\n\tint qidx;\n\n\t \n\totx2_ctx_disable(&pf->mbox, NIX_AQ_CTYPE_CQ, false);\n\tfor (qidx = 0; qidx < qset->cq_cnt; qidx++) {\n\t\tcq = &qset->cq[qidx];\n\t\tqmem_free(pf->dev, cq->cqe);\n\t}\n}\n\nstatic void otx2_free_sq_res(struct otx2_nic *pf)\n{\n\tstruct otx2_qset *qset = &pf->qset;\n\tstruct otx2_snd_queue *sq;\n\tint qidx;\n\n\t \n\totx2_ctx_disable(&pf->mbox, NIX_AQ_CTYPE_SQ, false);\n\t \n\totx2_sq_free_sqbs(pf);\n\tfor (qidx = 0; qidx < otx2_get_total_tx_queues(pf); qidx++) {\n\t\tsq = &qset->sq[qidx];\n\t\t \n\t\tif (!sq->sqe)\n\t\t\tcontinue;\n\t\tqmem_free(pf->dev, sq->sqe);\n\t\tqmem_free(pf->dev, sq->tso_hdrs);\n\t\tkfree(sq->sg);\n\t\tkfree(sq->sqb_ptrs);\n\t}\n}\n\nstatic int otx2_get_rbuf_size(struct otx2_nic *pf, int mtu)\n{\n\tint frame_size;\n\tint total_size;\n\tint rbuf_size;\n\n\tif (pf->hw.rbuf_len)\n\t\treturn ALIGN(pf->hw.rbuf_len, OTX2_ALIGN) + OTX2_HEAD_ROOM;\n\n\t \n\tframe_size = mtu + OTX2_ETH_HLEN + OTX2_HW_TIMESTAMP_LEN;\n\ttotal_size = frame_size + OTX2_HEAD_ROOM * 6;\n\trbuf_size = total_size / 6;\n\n\treturn ALIGN(rbuf_size, 2048);\n}\n\nstatic int otx2_init_hw_resources(struct otx2_nic *pf)\n{\n\tstruct nix_lf_free_req *free_req;\n\tstruct mbox *mbox = &pf->mbox;\n\tstruct otx2_hw *hw = &pf->hw;\n\tstruct msg_req *req;\n\tint err = 0, lvl;\n\n\t \n\thw->rqpool_cnt = hw->rx_queues;\n\thw->sqpool_cnt = otx2_get_total_tx_queues(pf);\n\thw->pool_cnt = hw->rqpool_cnt + hw->sqpool_cnt;\n\n\t \n\tpf->tx_max_pktlen = pf->netdev->max_mtu + OTX2_ETH_HLEN;\n\n\tpf->rbsize = otx2_get_rbuf_size(pf, pf->netdev->mtu);\n\n\tmutex_lock(&mbox->lock);\n\t \n\terr = otx2_config_npa(pf);\n\tif (err)\n\t\tgoto exit;\n\n\t \n\terr = otx2_config_nix(pf);\n\tif (err)\n\t\tgoto err_free_npa_lf;\n\n\t \n\tif (!is_otx2_lbkvf(pf->pdev))\n\t\totx2_nix_config_bp(pf, true);\n\n\t \n\terr = otx2_rq_aura_pool_init(pf);\n\tif (err) {\n\t\tmutex_unlock(&mbox->lock);\n\t\tgoto err_free_nix_lf;\n\t}\n\t \n\terr = otx2_sq_aura_pool_init(pf);\n\tif (err) {\n\t\tmutex_unlock(&mbox->lock);\n\t\tgoto err_free_rq_ptrs;\n\t}\n\n\terr = otx2_txsch_alloc(pf);\n\tif (err) {\n\t\tmutex_unlock(&mbox->lock);\n\t\tgoto err_free_sq_ptrs;\n\t}\n\n#ifdef CONFIG_DCB\n\tif (pf->pfc_en) {\n\t\terr = otx2_pfc_txschq_alloc(pf);\n\t\tif (err) {\n\t\t\tmutex_unlock(&mbox->lock);\n\t\t\tgoto err_free_sq_ptrs;\n\t\t}\n\t}\n#endif\n\n\terr = otx2_config_nix_queues(pf);\n\tif (err) {\n\t\tmutex_unlock(&mbox->lock);\n\t\tgoto err_free_txsch;\n\t}\n\n\tfor (lvl = 0; lvl < NIX_TXSCH_LVL_CNT; lvl++) {\n\t\terr = otx2_txschq_config(pf, lvl, 0, false);\n\t\tif (err) {\n\t\t\tmutex_unlock(&mbox->lock);\n\t\t\tgoto err_free_nix_queues;\n\t\t}\n\t}\n\n#ifdef CONFIG_DCB\n\tif (pf->pfc_en) {\n\t\terr = otx2_pfc_txschq_config(pf);\n\t\tif (err) {\n\t\t\tmutex_unlock(&mbox->lock);\n\t\t\tgoto err_free_nix_queues;\n\t\t}\n\t}\n#endif\n\n\tmutex_unlock(&mbox->lock);\n\treturn err;\n\nerr_free_nix_queues:\n\totx2_free_sq_res(pf);\n\totx2_free_cq_res(pf);\n\totx2_ctx_disable(mbox, NIX_AQ_CTYPE_RQ, false);\nerr_free_txsch:\n\totx2_txschq_stop(pf);\nerr_free_sq_ptrs:\n\totx2_sq_free_sqbs(pf);\nerr_free_rq_ptrs:\n\totx2_free_aura_ptr(pf, AURA_NIX_RQ);\n\totx2_ctx_disable(mbox, NPA_AQ_CTYPE_POOL, true);\n\totx2_ctx_disable(mbox, NPA_AQ_CTYPE_AURA, true);\n\totx2_aura_pool_free(pf);\nerr_free_nix_lf:\n\tmutex_lock(&mbox->lock);\n\tfree_req = otx2_mbox_alloc_msg_nix_lf_free(mbox);\n\tif (free_req) {\n\t\tfree_req->flags = NIX_LF_DISABLE_FLOWS;\n\t\tif (otx2_sync_mbox_msg(mbox))\n\t\t\tdev_err(pf->dev, \"%s failed to free nixlf\\n\", __func__);\n\t}\nerr_free_npa_lf:\n\t \n\treq = otx2_mbox_alloc_msg_npa_lf_free(mbox);\n\tif (req) {\n\t\tif (otx2_sync_mbox_msg(mbox))\n\t\t\tdev_err(pf->dev, \"%s failed to free npalf\\n\", __func__);\n\t}\nexit:\n\tmutex_unlock(&mbox->lock);\n\treturn err;\n}\n\nstatic void otx2_free_hw_resources(struct otx2_nic *pf)\n{\n\tstruct otx2_qset *qset = &pf->qset;\n\tstruct nix_lf_free_req *free_req;\n\tstruct mbox *mbox = &pf->mbox;\n\tstruct otx2_cq_queue *cq;\n\tstruct otx2_pool *pool;\n\tstruct msg_req *req;\n\tint pool_id;\n\tint qidx;\n\n\t \n\totx2_sqb_flush(pf);\n\n\t \n\totx2_txschq_stop(pf);\n\n#ifdef CONFIG_DCB\n\tif (pf->pfc_en)\n\t\totx2_pfc_txschq_stop(pf);\n#endif\n\n\totx2_clean_qos_queues(pf);\n\n\tmutex_lock(&mbox->lock);\n\t \n\tif (!(pf->pcifunc & RVU_PFVF_FUNC_MASK))\n\t\totx2_nix_config_bp(pf, false);\n\tmutex_unlock(&mbox->lock);\n\n\t \n\totx2_ctx_disable(mbox, NIX_AQ_CTYPE_RQ, false);\n\n\t \n\tfor (qidx = 0; qidx < qset->cq_cnt; qidx++) {\n\t\tcq = &qset->cq[qidx];\n\t\tif (cq->cq_type == CQ_RX)\n\t\t\totx2_cleanup_rx_cqes(pf, cq, qidx);\n\t\telse\n\t\t\totx2_cleanup_tx_cqes(pf, cq);\n\t}\n\totx2_free_pending_sqe(pf);\n\n\totx2_free_sq_res(pf);\n\n\t \n\totx2_free_aura_ptr(pf, AURA_NIX_RQ);\n\n\tfor (qidx = 0; qidx < pf->hw.rx_queues; qidx++) {\n\t\tpool_id = otx2_get_pool_idx(pf, AURA_NIX_RQ, qidx);\n\t\tpool = &pf->qset.pool[pool_id];\n\t\tpage_pool_destroy(pool->page_pool);\n\t\tpool->page_pool = NULL;\n\t}\n\n\totx2_free_cq_res(pf);\n\n\t \n\tcn10k_free_all_ipolicers(pf);\n\n\tmutex_lock(&mbox->lock);\n\t \n\tfree_req = otx2_mbox_alloc_msg_nix_lf_free(mbox);\n\tif (free_req) {\n\t\tfree_req->flags = NIX_LF_DISABLE_FLOWS;\n\t\tif (!(pf->flags & OTX2_FLAG_PF_SHUTDOWN))\n\t\t\tfree_req->flags |= NIX_LF_DONT_FREE_TX_VTAG;\n\t\tif (otx2_sync_mbox_msg(mbox))\n\t\t\tdev_err(pf->dev, \"%s failed to free nixlf\\n\", __func__);\n\t}\n\tmutex_unlock(&mbox->lock);\n\n\t \n\totx2_ctx_disable(mbox, NPA_AQ_CTYPE_POOL, true);\n\totx2_ctx_disable(mbox, NPA_AQ_CTYPE_AURA, true);\n\totx2_aura_pool_free(pf);\n\n\tmutex_lock(&mbox->lock);\n\t \n\treq = otx2_mbox_alloc_msg_npa_lf_free(mbox);\n\tif (req) {\n\t\tif (otx2_sync_mbox_msg(mbox))\n\t\t\tdev_err(pf->dev, \"%s failed to free npalf\\n\", __func__);\n\t}\n\tmutex_unlock(&mbox->lock);\n}\n\nstatic bool otx2_promisc_use_mce_list(struct otx2_nic *pfvf)\n{\n\tint vf;\n\n\t \n\tif (is_otx2_vf(pfvf->pcifunc))\n\t\treturn true;\n\n\t \n\tfor (vf = 0; vf < pci_num_vf(pfvf->pdev); vf++)\n\t\tif (pfvf->vf_configs[vf].trusted)\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic void otx2_do_set_rx_mode(struct otx2_nic *pf)\n{\n\tstruct net_device *netdev = pf->netdev;\n\tstruct nix_rx_mode *req;\n\tbool promisc = false;\n\n\tif (!(netdev->flags & IFF_UP))\n\t\treturn;\n\n\tif ((netdev->flags & IFF_PROMISC) ||\n\t    (netdev_uc_count(netdev) > OTX2_MAX_UNICAST_FLOWS)) {\n\t\tpromisc = true;\n\t}\n\n\t \n\tif (!promisc && netdev->priv_flags & IFF_UNICAST_FLT)\n\t\t__dev_uc_sync(netdev, otx2_add_macfilter, otx2_del_macfilter);\n\n\tmutex_lock(&pf->mbox.lock);\n\treq = otx2_mbox_alloc_msg_nix_set_rx_mode(&pf->mbox);\n\tif (!req) {\n\t\tmutex_unlock(&pf->mbox.lock);\n\t\treturn;\n\t}\n\n\treq->mode = NIX_RX_MODE_UCAST;\n\n\tif (promisc)\n\t\treq->mode |= NIX_RX_MODE_PROMISC;\n\tif (netdev->flags & (IFF_ALLMULTI | IFF_MULTICAST))\n\t\treq->mode |= NIX_RX_MODE_ALLMULTI;\n\n\tif (otx2_promisc_use_mce_list(pf))\n\t\treq->mode |= NIX_RX_MODE_USE_MCE;\n\n\totx2_sync_mbox_msg(&pf->mbox);\n\tmutex_unlock(&pf->mbox.lock);\n}\n\nstatic void otx2_set_irq_coalesce(struct otx2_nic *pfvf)\n{\n\tint cint;\n\n\tfor (cint = 0; cint < pfvf->hw.cint_cnt; cint++)\n\t\totx2_config_irq_coalescing(pfvf, cint);\n}\n\nstatic void otx2_dim_work(struct work_struct *w)\n{\n\tstruct dim_cq_moder cur_moder;\n\tstruct otx2_cq_poll *cq_poll;\n\tstruct otx2_nic *pfvf;\n\tstruct dim *dim;\n\n\tdim = container_of(w, struct dim, work);\n\tcur_moder = net_dim_get_rx_moderation(dim->mode, dim->profile_ix);\n\tcq_poll = container_of(dim, struct otx2_cq_poll, dim);\n\tpfvf = (struct otx2_nic *)cq_poll->dev;\n\tpfvf->hw.cq_time_wait = (cur_moder.usec > CQ_TIMER_THRESH_MAX) ?\n\t\tCQ_TIMER_THRESH_MAX : cur_moder.usec;\n\tpfvf->hw.cq_ecount_wait = (cur_moder.pkts > NAPI_POLL_WEIGHT) ?\n\t\tNAPI_POLL_WEIGHT : cur_moder.pkts;\n\totx2_set_irq_coalesce(pfvf);\n\tdim->state = DIM_START_MEASURE;\n}\n\nint otx2_open(struct net_device *netdev)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tstruct otx2_cq_poll *cq_poll = NULL;\n\tstruct otx2_qset *qset = &pf->qset;\n\tint err = 0, qidx, vec;\n\tchar *irq_name;\n\n\tnetif_carrier_off(netdev);\n\n\t \n\tpf->hw.cint_cnt = max3(pf->hw.rx_queues, pf->hw.tx_queues,\n\t\t\t       pf->hw.tc_tx_queues);\n\n\tpf->qset.cq_cnt = pf->hw.rx_queues + otx2_get_total_tx_queues(pf);\n\n\tqset->napi = kcalloc(pf->hw.cint_cnt, sizeof(*cq_poll), GFP_KERNEL);\n\tif (!qset->napi)\n\t\treturn -ENOMEM;\n\n\t \n\tqset->rqe_cnt = qset->rqe_cnt ? qset->rqe_cnt : Q_COUNT(Q_SIZE_256);\n\t \n\tqset->sqe_cnt = qset->sqe_cnt ? qset->sqe_cnt : Q_COUNT(Q_SIZE_4K);\n\n\terr = -ENOMEM;\n\tqset->cq = kcalloc(pf->qset.cq_cnt,\n\t\t\t   sizeof(struct otx2_cq_queue), GFP_KERNEL);\n\tif (!qset->cq)\n\t\tgoto err_free_mem;\n\n\tqset->sq = kcalloc(otx2_get_total_tx_queues(pf),\n\t\t\t   sizeof(struct otx2_snd_queue), GFP_KERNEL);\n\tif (!qset->sq)\n\t\tgoto err_free_mem;\n\n\tqset->rq = kcalloc(pf->hw.rx_queues,\n\t\t\t   sizeof(struct otx2_rcv_queue), GFP_KERNEL);\n\tif (!qset->rq)\n\t\tgoto err_free_mem;\n\n\terr = otx2_init_hw_resources(pf);\n\tif (err)\n\t\tgoto err_free_mem;\n\n\t \n\tfor (qidx = 0; qidx < pf->hw.cint_cnt; qidx++) {\n\t\tcq_poll = &qset->napi[qidx];\n\t\tcq_poll->cint_idx = qidx;\n\t\t \n\t\tcq_poll->cq_ids[CQ_RX] =\n\t\t\t(qidx <  pf->hw.rx_queues) ? qidx : CINT_INVALID_CQ;\n\t\tcq_poll->cq_ids[CQ_TX] = (qidx < pf->hw.tx_queues) ?\n\t\t\t\t      qidx + pf->hw.rx_queues : CINT_INVALID_CQ;\n\t\tif (pf->xdp_prog)\n\t\t\tcq_poll->cq_ids[CQ_XDP] = (qidx < pf->hw.xdp_queues) ?\n\t\t\t\t\t\t  (qidx + pf->hw.rx_queues +\n\t\t\t\t\t\t  pf->hw.tx_queues) :\n\t\t\t\t\t\t  CINT_INVALID_CQ;\n\t\telse\n\t\t\tcq_poll->cq_ids[CQ_XDP] = CINT_INVALID_CQ;\n\n\t\tcq_poll->cq_ids[CQ_QOS] = (qidx < pf->hw.tc_tx_queues) ?\n\t\t\t\t\t  (qidx + pf->hw.rx_queues +\n\t\t\t\t\t   pf->hw.non_qos_queues) :\n\t\t\t\t\t  CINT_INVALID_CQ;\n\n\t\tcq_poll->dev = (void *)pf;\n\t\tcq_poll->dim.mode = DIM_CQ_PERIOD_MODE_START_FROM_CQE;\n\t\tINIT_WORK(&cq_poll->dim.work, otx2_dim_work);\n\t\tnetif_napi_add(netdev, &cq_poll->napi, otx2_napi_handler);\n\t\tnapi_enable(&cq_poll->napi);\n\t}\n\n\t \n\terr = otx2_hw_set_mtu(pf, netdev->mtu);\n\tif (err)\n\t\tgoto err_disable_napi;\n\n\t \n\totx2_setup_segmentation(pf);\n\n\t \n\terr = otx2_rss_init(pf);\n\tif (err)\n\t\tgoto err_disable_napi;\n\n\t \n\tvec = pf->hw.nix_msixoff + NIX_LF_QINT_VEC_START;\n\tirq_name = &pf->hw.irq_name[vec * NAME_SIZE];\n\n\tsnprintf(irq_name, NAME_SIZE, \"%s-qerr\", pf->netdev->name);\n\n\terr = request_irq(pci_irq_vector(pf->pdev, vec),\n\t\t\t  otx2_q_intr_handler, 0, irq_name, pf);\n\tif (err) {\n\t\tdev_err(pf->dev,\n\t\t\t\"RVUPF%d: IRQ registration failed for QERR\\n\",\n\t\t\trvu_get_pf(pf->pcifunc));\n\t\tgoto err_disable_napi;\n\t}\n\n\t \n\totx2_write64(pf, NIX_LF_QINTX_ENA_W1S(0), BIT_ULL(0));\n\n\t \n\tvec = pf->hw.nix_msixoff + NIX_LF_CINT_VEC_START;\n\tfor (qidx = 0; qidx < pf->hw.cint_cnt; qidx++) {\n\t\tirq_name = &pf->hw.irq_name[vec * NAME_SIZE];\n\n\t\tsnprintf(irq_name, NAME_SIZE, \"%s-rxtx-%d\", pf->netdev->name,\n\t\t\t qidx);\n\n\t\terr = request_irq(pci_irq_vector(pf->pdev, vec),\n\t\t\t\t  otx2_cq_intr_handler, 0, irq_name,\n\t\t\t\t  &qset->napi[qidx]);\n\t\tif (err) {\n\t\t\tdev_err(pf->dev,\n\t\t\t\t\"RVUPF%d: IRQ registration failed for CQ%d\\n\",\n\t\t\t\trvu_get_pf(pf->pcifunc), qidx);\n\t\t\tgoto err_free_cints;\n\t\t}\n\t\tvec++;\n\n\t\totx2_config_irq_coalescing(pf, qidx);\n\n\t\t \n\t\totx2_write64(pf, NIX_LF_CINTX_INT(qidx), BIT_ULL(0));\n\t\totx2_write64(pf, NIX_LF_CINTX_ENA_W1S(qidx), BIT_ULL(0));\n\t}\n\n\totx2_set_cints_affinity(pf);\n\n\tif (pf->flags & OTX2_FLAG_RX_VLAN_SUPPORT)\n\t\totx2_enable_rxvlan(pf, true);\n\n\t \n\tif (pf->flags & OTX2_FLAG_TX_TSTAMP_ENABLED) {\n\t\tpf->flags &= ~OTX2_FLAG_TX_TSTAMP_ENABLED;\n\t\totx2_config_hw_tx_tstamp(pf, true);\n\t}\n\tif (pf->flags & OTX2_FLAG_RX_TSTAMP_ENABLED) {\n\t\tpf->flags &= ~OTX2_FLAG_RX_TSTAMP_ENABLED;\n\t\totx2_config_hw_rx_tstamp(pf, true);\n\t}\n\n\tpf->flags &= ~OTX2_FLAG_INTF_DOWN;\n\t \n\tsmp_wmb();\n\n\t \n\totx2_qos_config_txschq(pf);\n\n\t \n\tif (pf->linfo.link_up && !(pf->pcifunc & RVU_PFVF_FUNC_MASK))\n\t\totx2_handle_link_event(pf);\n\n\t \n\tif (pf->flags & OTX2_FLAG_DMACFLTR_SUPPORT)\n\t\totx2_dmacflt_reinstall_flows(pf);\n\n\totx2_tc_apply_ingress_police_rules(pf);\n\n\terr = otx2_rxtx_enable(pf, true);\n\t \n\tif (err == EIO)\n\t\tgoto err_disable_rxtx;\n\telse if (err)\n\t\tgoto err_tx_stop_queues;\n\n\totx2_do_set_rx_mode(pf);\n\n\treturn 0;\n\nerr_disable_rxtx:\n\totx2_rxtx_enable(pf, false);\nerr_tx_stop_queues:\n\tnetif_tx_stop_all_queues(netdev);\n\tnetif_carrier_off(netdev);\n\tpf->flags |= OTX2_FLAG_INTF_DOWN;\nerr_free_cints:\n\totx2_free_cints(pf, qidx);\n\tvec = pci_irq_vector(pf->pdev,\n\t\t\t     pf->hw.nix_msixoff + NIX_LF_QINT_VEC_START);\n\totx2_write64(pf, NIX_LF_QINTX_ENA_W1C(0), BIT_ULL(0));\n\tfree_irq(vec, pf);\nerr_disable_napi:\n\totx2_disable_napi(pf);\n\totx2_free_hw_resources(pf);\nerr_free_mem:\n\tkfree(qset->sq);\n\tkfree(qset->cq);\n\tkfree(qset->rq);\n\tkfree(qset->napi);\n\treturn err;\n}\nEXPORT_SYMBOL(otx2_open);\n\nint otx2_stop(struct net_device *netdev)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tstruct otx2_cq_poll *cq_poll = NULL;\n\tstruct otx2_qset *qset = &pf->qset;\n\tstruct otx2_rss_info *rss;\n\tint qidx, vec, wrk;\n\n\t \n\tif (pf->flags & OTX2_FLAG_INTF_DOWN)\n\t\treturn 0;\n\n\tnetif_carrier_off(netdev);\n\tnetif_tx_stop_all_queues(netdev);\n\n\tpf->flags |= OTX2_FLAG_INTF_DOWN;\n\t \n\tsmp_wmb();\n\n\t \n\totx2_rxtx_enable(pf, false);\n\n\t \n\trss = &pf->hw.rss_info;\n\trss->enable = false;\n\tif (!netif_is_rxfh_configured(netdev))\n\t\tkfree(rss->rss_ctx[DEFAULT_RSS_CONTEXT_GROUP]);\n\n\t \n\tvec = pci_irq_vector(pf->pdev,\n\t\t\t     pf->hw.nix_msixoff + NIX_LF_QINT_VEC_START);\n\totx2_write64(pf, NIX_LF_QINTX_ENA_W1C(0), BIT_ULL(0));\n\tfree_irq(vec, pf);\n\n\t \n\tvec = pf->hw.nix_msixoff + NIX_LF_CINT_VEC_START;\n\tfor (qidx = 0; qidx < pf->hw.cint_cnt; qidx++) {\n\t\t \n\t\totx2_write64(pf, NIX_LF_CINTX_ENA_W1C(qidx), BIT_ULL(0));\n\n\t\tsynchronize_irq(pci_irq_vector(pf->pdev, vec));\n\n\t\tcq_poll = &qset->napi[qidx];\n\t\tnapi_synchronize(&cq_poll->napi);\n\t\tvec++;\n\t}\n\n\tnetif_tx_disable(netdev);\n\n\tfor (wrk = 0; wrk < pf->qset.cq_cnt; wrk++)\n\t\tcancel_delayed_work_sync(&pf->refill_wrk[wrk].pool_refill_work);\n\tdevm_kfree(pf->dev, pf->refill_wrk);\n\n\totx2_free_hw_resources(pf);\n\totx2_free_cints(pf, pf->hw.cint_cnt);\n\totx2_disable_napi(pf);\n\n\tfor (qidx = 0; qidx < netdev->num_tx_queues; qidx++)\n\t\tnetdev_tx_reset_queue(netdev_get_tx_queue(netdev, qidx));\n\n\n\tkfree(qset->sq);\n\tkfree(qset->cq);\n\tkfree(qset->rq);\n\tkfree(qset->napi);\n\t \n\tmemset_startat(qset, 0, sqe_cnt);\n\treturn 0;\n}\nEXPORT_SYMBOL(otx2_stop);\n\nstatic netdev_tx_t otx2_xmit(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tint qidx = skb_get_queue_mapping(skb);\n\tstruct otx2_snd_queue *sq;\n\tstruct netdev_queue *txq;\n\tint sq_idx;\n\n\t \n\tsq_idx = (qidx >= pf->hw.tx_queues) ? (qidx + pf->hw.xdp_queues) : qidx;\n\n\t \n\tif (skb->len <= ETH_HLEN ||\n\t    (!skb_shinfo(skb)->gso_size && skb->len > pf->tx_max_pktlen)) {\n\t\tdev_kfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tsq = &pf->qset.sq[sq_idx];\n\ttxq = netdev_get_tx_queue(netdev, qidx);\n\n\tif (!otx2_sq_append_skb(netdev, sq, skb, qidx)) {\n\t\tnetif_tx_stop_queue(txq);\n\n\t\t \n\t\tsmp_mb();\n\t\tif (((sq->num_sqbs - *sq->aura_fc_addr) * sq->sqe_per_sqb)\n\t\t\t\t\t\t\t> sq->sqe_thresh)\n\t\t\tnetif_tx_wake_queue(txq);\n\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic int otx2_qos_select_htb_queue(struct otx2_nic *pf, struct sk_buff *skb,\n\t\t\t\t     u16 htb_maj_id)\n{\n\tu16 classid;\n\n\tif ((TC_H_MAJ(skb->priority) >> 16) == htb_maj_id)\n\t\tclassid = TC_H_MIN(skb->priority);\n\telse\n\t\tclassid = READ_ONCE(pf->qos.defcls);\n\n\tif (!classid)\n\t\treturn 0;\n\n\treturn otx2_get_txq_by_classid(pf, classid);\n}\n\nu16 otx2_select_queue(struct net_device *netdev, struct sk_buff *skb,\n\t\t      struct net_device *sb_dev)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tbool qos_enabled;\n#ifdef CONFIG_DCB\n\tu8 vlan_prio;\n#endif\n\tint txq;\n\n\tqos_enabled = netdev->real_num_tx_queues > pf->hw.tx_queues;\n\tif (unlikely(qos_enabled)) {\n\t\t \n\t\tu16 htb_maj_id = smp_load_acquire(&pf->qos.maj_id);\n\n\t\tif (unlikely(htb_maj_id)) {\n\t\t\ttxq = otx2_qos_select_htb_queue(pf, skb, htb_maj_id);\n\t\t\tif (txq > 0)\n\t\t\t\treturn txq;\n\t\t\tgoto process_pfc;\n\t\t}\n\t}\n\nprocess_pfc:\n#ifdef CONFIG_DCB\n\tif (!skb_vlan_tag_present(skb))\n\t\tgoto pick_tx;\n\n\tvlan_prio = skb->vlan_tci >> 13;\n\tif ((vlan_prio > pf->hw.tx_queues - 1) ||\n\t    !pf->pfc_alloc_status[vlan_prio])\n\t\tgoto pick_tx;\n\n\treturn vlan_prio;\n\npick_tx:\n#endif\n\ttxq = netdev_pick_tx(netdev, skb, NULL);\n\tif (unlikely(qos_enabled))\n\t\treturn txq % pf->hw.tx_queues;\n\n\treturn txq;\n}\nEXPORT_SYMBOL(otx2_select_queue);\n\nstatic netdev_features_t otx2_fix_features(struct net_device *dev,\n\t\t\t\t\t   netdev_features_t features)\n{\n\tif (features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tfeatures |= NETIF_F_HW_VLAN_STAG_RX;\n\telse\n\t\tfeatures &= ~NETIF_F_HW_VLAN_STAG_RX;\n\n\treturn features;\n}\n\nstatic void otx2_set_rx_mode(struct net_device *netdev)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\n\tqueue_work(pf->otx2_wq, &pf->rx_mode_work);\n}\n\nstatic void otx2_rx_mode_wrk_handler(struct work_struct *work)\n{\n\tstruct otx2_nic *pf = container_of(work, struct otx2_nic, rx_mode_work);\n\n\totx2_do_set_rx_mode(pf);\n}\n\nstatic int otx2_set_features(struct net_device *netdev,\n\t\t\t     netdev_features_t features)\n{\n\tnetdev_features_t changed = features ^ netdev->features;\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\n\tif ((changed & NETIF_F_LOOPBACK) && netif_running(netdev))\n\t\treturn otx2_cgx_config_loopback(pf,\n\t\t\t\t\t\tfeatures & NETIF_F_LOOPBACK);\n\n\tif ((changed & NETIF_F_HW_VLAN_CTAG_RX) && netif_running(netdev))\n\t\treturn otx2_enable_rxvlan(pf,\n\t\t\t\t\t  features & NETIF_F_HW_VLAN_CTAG_RX);\n\n\treturn otx2_handle_ntuple_tc_features(netdev, features);\n}\n\nstatic void otx2_reset_task(struct work_struct *work)\n{\n\tstruct otx2_nic *pf = container_of(work, struct otx2_nic, reset_task);\n\n\tif (!netif_running(pf->netdev))\n\t\treturn;\n\n\trtnl_lock();\n\totx2_stop(pf->netdev);\n\tpf->reset_count++;\n\totx2_open(pf->netdev);\n\tnetif_trans_update(pf->netdev);\n\trtnl_unlock();\n}\n\nstatic int otx2_config_hw_rx_tstamp(struct otx2_nic *pfvf, bool enable)\n{\n\tstruct msg_req *req;\n\tint err;\n\n\tif (pfvf->flags & OTX2_FLAG_RX_TSTAMP_ENABLED && enable)\n\t\treturn 0;\n\n\tmutex_lock(&pfvf->mbox.lock);\n\tif (enable)\n\t\treq = otx2_mbox_alloc_msg_cgx_ptp_rx_enable(&pfvf->mbox);\n\telse\n\t\treq = otx2_mbox_alloc_msg_cgx_ptp_rx_disable(&pfvf->mbox);\n\tif (!req) {\n\t\tmutex_unlock(&pfvf->mbox.lock);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = otx2_sync_mbox_msg(&pfvf->mbox);\n\tif (err) {\n\t\tmutex_unlock(&pfvf->mbox.lock);\n\t\treturn err;\n\t}\n\n\tmutex_unlock(&pfvf->mbox.lock);\n\tif (enable)\n\t\tpfvf->flags |= OTX2_FLAG_RX_TSTAMP_ENABLED;\n\telse\n\t\tpfvf->flags &= ~OTX2_FLAG_RX_TSTAMP_ENABLED;\n\treturn 0;\n}\n\nstatic int otx2_config_hw_tx_tstamp(struct otx2_nic *pfvf, bool enable)\n{\n\tstruct msg_req *req;\n\tint err;\n\n\tif (pfvf->flags & OTX2_FLAG_TX_TSTAMP_ENABLED && enable)\n\t\treturn 0;\n\n\tmutex_lock(&pfvf->mbox.lock);\n\tif (enable)\n\t\treq = otx2_mbox_alloc_msg_nix_lf_ptp_tx_enable(&pfvf->mbox);\n\telse\n\t\treq = otx2_mbox_alloc_msg_nix_lf_ptp_tx_disable(&pfvf->mbox);\n\tif (!req) {\n\t\tmutex_unlock(&pfvf->mbox.lock);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = otx2_sync_mbox_msg(&pfvf->mbox);\n\tif (err) {\n\t\tmutex_unlock(&pfvf->mbox.lock);\n\t\treturn err;\n\t}\n\n\tmutex_unlock(&pfvf->mbox.lock);\n\tif (enable)\n\t\tpfvf->flags |= OTX2_FLAG_TX_TSTAMP_ENABLED;\n\telse\n\t\tpfvf->flags &= ~OTX2_FLAG_TX_TSTAMP_ENABLED;\n\treturn 0;\n}\n\nint otx2_config_hwtstamp(struct net_device *netdev, struct ifreq *ifr)\n{\n\tstruct otx2_nic *pfvf = netdev_priv(netdev);\n\tstruct hwtstamp_config config;\n\n\tif (!pfvf->ptp)\n\t\treturn -ENODEV;\n\n\tif (copy_from_user(&config, ifr->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\tswitch (config.tx_type) {\n\tcase HWTSTAMP_TX_OFF:\n\t\tif (pfvf->flags & OTX2_FLAG_PTP_ONESTEP_SYNC)\n\t\t\tpfvf->flags &= ~OTX2_FLAG_PTP_ONESTEP_SYNC;\n\n\t\tcancel_delayed_work(&pfvf->ptp->synctstamp_work);\n\t\totx2_config_hw_tx_tstamp(pfvf, false);\n\t\tbreak;\n\tcase HWTSTAMP_TX_ONESTEP_SYNC:\n\t\tif (!test_bit(CN10K_PTP_ONESTEP, &pfvf->hw.cap_flag))\n\t\t\treturn -ERANGE;\n\t\tpfvf->flags |= OTX2_FLAG_PTP_ONESTEP_SYNC;\n\t\tschedule_delayed_work(&pfvf->ptp->synctstamp_work,\n\t\t\t\t      msecs_to_jiffies(500));\n\t\tfallthrough;\n\tcase HWTSTAMP_TX_ON:\n\t\totx2_config_hw_tx_tstamp(pfvf, true);\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tswitch (config.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\totx2_config_hw_rx_tstamp(pfvf, false);\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_ALL:\n\tcase HWTSTAMP_FILTER_SOME:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\n\t\totx2_config_hw_rx_tstamp(pfvf, true);\n\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tmemcpy(&pfvf->tstamp, &config, sizeof(config));\n\n\treturn copy_to_user(ifr->ifr_data, &config,\n\t\t\t    sizeof(config)) ? -EFAULT : 0;\n}\nEXPORT_SYMBOL(otx2_config_hwtstamp);\n\nint otx2_ioctl(struct net_device *netdev, struct ifreq *req, int cmd)\n{\n\tstruct otx2_nic *pfvf = netdev_priv(netdev);\n\tstruct hwtstamp_config *cfg = &pfvf->tstamp;\n\n\tswitch (cmd) {\n\tcase SIOCSHWTSTAMP:\n\t\treturn otx2_config_hwtstamp(netdev, req);\n\tcase SIOCGHWTSTAMP:\n\t\treturn copy_to_user(req->ifr_data, cfg,\n\t\t\t\t    sizeof(*cfg)) ? -EFAULT : 0;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\nEXPORT_SYMBOL(otx2_ioctl);\n\nstatic int otx2_do_set_vf_mac(struct otx2_nic *pf, int vf, const u8 *mac)\n{\n\tstruct npc_install_flow_req *req;\n\tint err;\n\n\tmutex_lock(&pf->mbox.lock);\n\treq = otx2_mbox_alloc_msg_npc_install_flow(&pf->mbox);\n\tif (!req) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tether_addr_copy(req->packet.dmac, mac);\n\teth_broadcast_addr((u8 *)&req->mask.dmac);\n\treq->features = BIT_ULL(NPC_DMAC);\n\treq->channel = pf->hw.rx_chan_base;\n\treq->intf = NIX_INTF_RX;\n\treq->default_rule = 1;\n\treq->append = 1;\n\treq->vf = vf + 1;\n\treq->op = NIX_RX_ACTION_DEFAULT;\n\n\terr = otx2_sync_mbox_msg(&pf->mbox);\nout:\n\tmutex_unlock(&pf->mbox.lock);\n\treturn err;\n}\n\nstatic int otx2_set_vf_mac(struct net_device *netdev, int vf, u8 *mac)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tstruct pci_dev *pdev = pf->pdev;\n\tstruct otx2_vf_config *config;\n\tint ret;\n\n\tif (!netif_running(netdev))\n\t\treturn -EAGAIN;\n\n\tif (vf >= pf->total_vfs)\n\t\treturn -EINVAL;\n\n\tif (!is_valid_ether_addr(mac))\n\t\treturn -EINVAL;\n\n\tconfig = &pf->vf_configs[vf];\n\tether_addr_copy(config->mac, mac);\n\n\tret = otx2_do_set_vf_mac(pf, vf, mac);\n\tif (ret == 0)\n\t\tdev_info(&pdev->dev,\n\t\t\t \"Load/Reload VF driver\\n\");\n\n\treturn ret;\n}\n\nstatic int otx2_do_set_vf_vlan(struct otx2_nic *pf, int vf, u16 vlan, u8 qos,\n\t\t\t       __be16 proto)\n{\n\tstruct otx2_flow_config *flow_cfg = pf->flow_cfg;\n\tstruct nix_vtag_config_rsp *vtag_rsp;\n\tstruct npc_delete_flow_req *del_req;\n\tstruct nix_vtag_config *vtag_req;\n\tstruct npc_install_flow_req *req;\n\tstruct otx2_vf_config *config;\n\tint err = 0;\n\tu32 idx;\n\n\tconfig = &pf->vf_configs[vf];\n\n\tif (!vlan && !config->vlan)\n\t\tgoto out;\n\n\tmutex_lock(&pf->mbox.lock);\n\n\t \n\tif (config->vlan) {\n\t\tvtag_req = otx2_mbox_alloc_msg_nix_vtag_cfg(&pf->mbox);\n\t\tif (!vtag_req) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tvtag_req->cfg_type = 0;\n\t\tvtag_req->tx.free_vtag0 = 1;\n\t\tvtag_req->tx.vtag0_idx = config->tx_vtag_idx;\n\n\t\terr = otx2_sync_mbox_msg(&pf->mbox);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (!vlan && config->vlan) {\n\t\t \n\t\tdel_req = otx2_mbox_alloc_msg_npc_delete_flow(&pf->mbox);\n\t\tif (!del_req) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tidx = ((vf * OTX2_PER_VF_VLAN_FLOWS) + OTX2_VF_VLAN_RX_INDEX);\n\t\tdel_req->entry =\n\t\t\tflow_cfg->def_ent[flow_cfg->vf_vlan_offset + idx];\n\t\terr = otx2_sync_mbox_msg(&pf->mbox);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t \n\t\tdel_req = otx2_mbox_alloc_msg_npc_delete_flow(&pf->mbox);\n\t\tif (!del_req) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tidx = ((vf * OTX2_PER_VF_VLAN_FLOWS) + OTX2_VF_VLAN_TX_INDEX);\n\t\tdel_req->entry =\n\t\t\tflow_cfg->def_ent[flow_cfg->vf_vlan_offset + idx];\n\t\terr = otx2_sync_mbox_msg(&pf->mbox);\n\n\t\tgoto out;\n\t}\n\n\t \n\treq = otx2_mbox_alloc_msg_npc_install_flow(&pf->mbox);\n\tif (!req) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tidx = ((vf * OTX2_PER_VF_VLAN_FLOWS) + OTX2_VF_VLAN_RX_INDEX);\n\treq->entry = flow_cfg->def_ent[flow_cfg->vf_vlan_offset + idx];\n\treq->packet.vlan_tci = htons(vlan);\n\treq->mask.vlan_tci = htons(VLAN_VID_MASK);\n\t \n\teth_broadcast_addr((u8 *)&req->mask.dmac);\n\treq->features = BIT_ULL(NPC_OUTER_VID) | BIT_ULL(NPC_DMAC);\n\treq->channel = pf->hw.rx_chan_base;\n\treq->intf = NIX_INTF_RX;\n\treq->vf = vf + 1;\n\treq->op = NIX_RX_ACTION_DEFAULT;\n\treq->vtag0_valid = true;\n\treq->vtag0_type = NIX_AF_LFX_RX_VTAG_TYPE7;\n\treq->set_cntr = 1;\n\n\terr = otx2_sync_mbox_msg(&pf->mbox);\n\tif (err)\n\t\tgoto out;\n\n\t \n\tvtag_req = otx2_mbox_alloc_msg_nix_vtag_cfg(&pf->mbox);\n\tif (!vtag_req) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tvtag_req->vtag_size = VTAGSIZE_T4;\n\tvtag_req->cfg_type = 0;  \n\tvtag_req->tx.cfg_vtag0 = 1;\n\tvtag_req->tx.vtag0 = ((u64)ntohs(proto) << 16) | vlan;\n\n\terr = otx2_sync_mbox_msg(&pf->mbox);\n\tif (err)\n\t\tgoto out;\n\n\tvtag_rsp = (struct nix_vtag_config_rsp *)otx2_mbox_get_rsp\n\t\t\t(&pf->mbox.mbox, 0, &vtag_req->hdr);\n\tif (IS_ERR(vtag_rsp)) {\n\t\terr = PTR_ERR(vtag_rsp);\n\t\tgoto out;\n\t}\n\tconfig->tx_vtag_idx = vtag_rsp->vtag0_idx;\n\n\treq = otx2_mbox_alloc_msg_npc_install_flow(&pf->mbox);\n\tif (!req) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\teth_zero_addr((u8 *)&req->mask.dmac);\n\tidx = ((vf * OTX2_PER_VF_VLAN_FLOWS) + OTX2_VF_VLAN_TX_INDEX);\n\treq->entry = flow_cfg->def_ent[flow_cfg->vf_vlan_offset + idx];\n\treq->features = BIT_ULL(NPC_DMAC);\n\treq->channel = pf->hw.tx_chan_base;\n\treq->intf = NIX_INTF_TX;\n\treq->vf = vf + 1;\n\treq->op = NIX_TX_ACTIONOP_UCAST_DEFAULT;\n\treq->vtag0_def = vtag_rsp->vtag0_idx;\n\treq->vtag0_op = VTAG_INSERT;\n\treq->set_cntr = 1;\n\n\terr = otx2_sync_mbox_msg(&pf->mbox);\nout:\n\tconfig->vlan = vlan;\n\tmutex_unlock(&pf->mbox.lock);\n\treturn err;\n}\n\nstatic int otx2_set_vf_vlan(struct net_device *netdev, int vf, u16 vlan, u8 qos,\n\t\t\t    __be16 proto)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tstruct pci_dev *pdev = pf->pdev;\n\n\tif (!netif_running(netdev))\n\t\treturn -EAGAIN;\n\n\tif (vf >= pci_num_vf(pdev))\n\t\treturn -EINVAL;\n\n\t \n\tif (vlan >= VLAN_N_VID || qos)\n\t\treturn -EINVAL;\n\n\tif (proto != htons(ETH_P_8021Q))\n\t\treturn -EPROTONOSUPPORT;\n\n\tif (!(pf->flags & OTX2_FLAG_VF_VLAN_SUPPORT))\n\t\treturn -EOPNOTSUPP;\n\n\treturn otx2_do_set_vf_vlan(pf, vf, vlan, qos, proto);\n}\n\nstatic int otx2_get_vf_config(struct net_device *netdev, int vf,\n\t\t\t      struct ifla_vf_info *ivi)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tstruct pci_dev *pdev = pf->pdev;\n\tstruct otx2_vf_config *config;\n\n\tif (!netif_running(netdev))\n\t\treturn -EAGAIN;\n\n\tif (vf >= pci_num_vf(pdev))\n\t\treturn -EINVAL;\n\n\tconfig = &pf->vf_configs[vf];\n\tivi->vf = vf;\n\tether_addr_copy(ivi->mac, config->mac);\n\tivi->vlan = config->vlan;\n\tivi->trusted = config->trusted;\n\n\treturn 0;\n}\n\nstatic int otx2_xdp_xmit_tx(struct otx2_nic *pf, struct xdp_frame *xdpf,\n\t\t\t    int qidx)\n{\n\tstruct page *page;\n\tu64 dma_addr;\n\tint err = 0;\n\n\tdma_addr = otx2_dma_map_page(pf, virt_to_page(xdpf->data),\n\t\t\t\t     offset_in_page(xdpf->data), xdpf->len,\n\t\t\t\t     DMA_TO_DEVICE);\n\tif (dma_mapping_error(pf->dev, dma_addr))\n\t\treturn -ENOMEM;\n\n\terr = otx2_xdp_sq_append_pkt(pf, dma_addr, xdpf->len, qidx);\n\tif (!err) {\n\t\totx2_dma_unmap_page(pf, dma_addr, xdpf->len, DMA_TO_DEVICE);\n\t\tpage = virt_to_page(xdpf->data);\n\t\tput_page(page);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic int otx2_xdp_xmit(struct net_device *netdev, int n,\n\t\t\t struct xdp_frame **frames, u32 flags)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tint qidx = smp_processor_id();\n\tstruct otx2_snd_queue *sq;\n\tint drops = 0, i;\n\n\tif (!netif_running(netdev))\n\t\treturn -ENETDOWN;\n\n\tqidx += pf->hw.tx_queues;\n\tsq = pf->xdp_prog ? &pf->qset.sq[qidx] : NULL;\n\n\t \n\tif (unlikely(!sq))\n\t\treturn -ENXIO;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < n; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\t\tint err;\n\n\t\terr = otx2_xdp_xmit_tx(pf, xdpf, qidx);\n\t\tif (err)\n\t\t\tdrops++;\n\t}\n\treturn n - drops;\n}\n\nstatic int otx2_xdp_setup(struct otx2_nic *pf, struct bpf_prog *prog)\n{\n\tstruct net_device *dev = pf->netdev;\n\tbool if_up = netif_running(pf->netdev);\n\tstruct bpf_prog *old_prog;\n\n\tif (prog && dev->mtu > MAX_XDP_MTU) {\n\t\tnetdev_warn(dev, \"Jumbo frames not yet supported with XDP\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (if_up)\n\t\totx2_stop(pf->netdev);\n\n\told_prog = xchg(&pf->xdp_prog, prog);\n\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\tif (pf->xdp_prog)\n\t\tbpf_prog_add(pf->xdp_prog, pf->hw.rx_queues - 1);\n\n\t \n\tif (pf->xdp_prog) {\n\t\tpf->hw.xdp_queues = pf->hw.rx_queues;\n\t\txdp_features_set_redirect_target(dev, false);\n\t} else {\n\t\tpf->hw.xdp_queues = 0;\n\t\txdp_features_clear_redirect_target(dev);\n\t}\n\n\tpf->hw.non_qos_queues += pf->hw.xdp_queues;\n\n\tif (if_up)\n\t\totx2_open(pf->netdev);\n\n\treturn 0;\n}\n\nstatic int otx2_xdp(struct net_device *netdev, struct netdev_bpf *xdp)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn otx2_xdp_setup(pf, xdp->prog);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int otx2_set_vf_permissions(struct otx2_nic *pf, int vf,\n\t\t\t\t   int req_perm)\n{\n\tstruct set_vf_perm *req;\n\tint rc;\n\n\tmutex_lock(&pf->mbox.lock);\n\treq = otx2_mbox_alloc_msg_set_vf_perm(&pf->mbox);\n\tif (!req) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (req_perm == OTX2_RESET_VF_PERM) {\n\t\treq->flags |= RESET_VF_PERM;\n\t} else if (req_perm == OTX2_TRUSTED_VF) {\n\t\tif (pf->vf_configs[vf].trusted)\n\t\t\treq->flags |= VF_TRUSTED;\n\t}\n\n\treq->vf = vf;\n\trc = otx2_sync_mbox_msg(&pf->mbox);\nout:\n\tmutex_unlock(&pf->mbox.lock);\n\treturn rc;\n}\n\nstatic int otx2_ndo_set_vf_trust(struct net_device *netdev, int vf,\n\t\t\t\t bool enable)\n{\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tstruct pci_dev *pdev = pf->pdev;\n\tint rc;\n\n\tif (vf >= pci_num_vf(pdev))\n\t\treturn -EINVAL;\n\n\tif (pf->vf_configs[vf].trusted == enable)\n\t\treturn 0;\n\n\tpf->vf_configs[vf].trusted = enable;\n\trc = otx2_set_vf_permissions(pf, vf, OTX2_TRUSTED_VF);\n\n\tif (rc) {\n\t\tpf->vf_configs[vf].trusted = !enable;\n\t} else {\n\t\tnetdev_info(pf->netdev, \"VF %d is %strusted\\n\",\n\t\t\t    vf, enable ? \"\" : \"not \");\n\t\totx2_set_rx_mode(netdev);\n\t}\n\n\treturn rc;\n}\n\nstatic const struct net_device_ops otx2_netdev_ops = {\n\t.ndo_open\t\t= otx2_open,\n\t.ndo_stop\t\t= otx2_stop,\n\t.ndo_start_xmit\t\t= otx2_xmit,\n\t.ndo_select_queue\t= otx2_select_queue,\n\t.ndo_fix_features\t= otx2_fix_features,\n\t.ndo_set_mac_address    = otx2_set_mac_address,\n\t.ndo_change_mtu\t\t= otx2_change_mtu,\n\t.ndo_set_rx_mode\t= otx2_set_rx_mode,\n\t.ndo_set_features\t= otx2_set_features,\n\t.ndo_tx_timeout\t\t= otx2_tx_timeout,\n\t.ndo_get_stats64\t= otx2_get_stats64,\n\t.ndo_eth_ioctl\t\t= otx2_ioctl,\n\t.ndo_set_vf_mac\t\t= otx2_set_vf_mac,\n\t.ndo_set_vf_vlan\t= otx2_set_vf_vlan,\n\t.ndo_get_vf_config\t= otx2_get_vf_config,\n\t.ndo_bpf\t\t= otx2_xdp,\n\t.ndo_xdp_xmit           = otx2_xdp_xmit,\n\t.ndo_setup_tc\t\t= otx2_setup_tc,\n\t.ndo_set_vf_trust\t= otx2_ndo_set_vf_trust,\n};\n\nstatic int otx2_wq_init(struct otx2_nic *pf)\n{\n\tpf->otx2_wq = create_singlethread_workqueue(\"otx2_wq\");\n\tif (!pf->otx2_wq)\n\t\treturn -ENOMEM;\n\n\tINIT_WORK(&pf->rx_mode_work, otx2_rx_mode_wrk_handler);\n\tINIT_WORK(&pf->reset_task, otx2_reset_task);\n\treturn 0;\n}\n\nstatic int otx2_check_pf_usable(struct otx2_nic *nic)\n{\n\tu64 rev;\n\n\trev = otx2_read64(nic, RVU_PF_BLOCK_ADDRX_DISC(BLKADDR_RVUM));\n\trev = (rev >> 12) & 0xFF;\n\t \n\tif (!rev) {\n\t\tdev_warn(nic->dev,\n\t\t\t \"AF is not initialized, deferring probe\\n\");\n\t\treturn -EPROBE_DEFER;\n\t}\n\treturn 0;\n}\n\nstatic int otx2_realloc_msix_vectors(struct otx2_nic *pf)\n{\n\tstruct otx2_hw *hw = &pf->hw;\n\tint num_vec, err;\n\n\t \n\tnum_vec = hw->nix_msixoff;\n\tnum_vec += NIX_LF_CINT_VEC_START + hw->max_queues;\n\n\totx2_disable_mbox_intr(pf);\n\tpci_free_irq_vectors(hw->pdev);\n\terr = pci_alloc_irq_vectors(hw->pdev, num_vec, num_vec, PCI_IRQ_MSIX);\n\tif (err < 0) {\n\t\tdev_err(pf->dev, \"%s: Failed to realloc %d IRQ vectors\\n\",\n\t\t\t__func__, num_vec);\n\t\treturn err;\n\t}\n\n\treturn otx2_register_mbox_intr(pf, false);\n}\n\nstatic int otx2_sriov_vfcfg_init(struct otx2_nic *pf)\n{\n\tint i;\n\n\tpf->vf_configs = devm_kcalloc(pf->dev, pf->total_vfs,\n\t\t\t\t      sizeof(struct otx2_vf_config),\n\t\t\t\t      GFP_KERNEL);\n\tif (!pf->vf_configs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < pf->total_vfs; i++) {\n\t\tpf->vf_configs[i].pf = pf;\n\t\tpf->vf_configs[i].intf_down = true;\n\t\tpf->vf_configs[i].trusted = false;\n\t\tINIT_DELAYED_WORK(&pf->vf_configs[i].link_event_work,\n\t\t\t\t  otx2_vf_link_event_task);\n\t}\n\n\treturn 0;\n}\n\nstatic void otx2_sriov_vfcfg_cleanup(struct otx2_nic *pf)\n{\n\tint i;\n\n\tif (!pf->vf_configs)\n\t\treturn;\n\n\tfor (i = 0; i < pf->total_vfs; i++) {\n\t\tcancel_delayed_work_sync(&pf->vf_configs[i].link_event_work);\n\t\totx2_set_vf_permissions(pf, i, OTX2_RESET_VF_PERM);\n\t}\n}\n\nstatic int otx2_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct device *dev = &pdev->dev;\n\tint err, qcount, qos_txqs;\n\tstruct net_device *netdev;\n\tstruct otx2_nic *pf;\n\tstruct otx2_hw *hw;\n\tint num_vec;\n\n\terr = pcim_enable_device(pdev);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to enable PCI device\\n\");\n\t\treturn err;\n\t}\n\n\terr = pci_request_regions(pdev, DRV_NAME);\n\tif (err) {\n\t\tdev_err(dev, \"PCI request regions failed 0x%x\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(48));\n\tif (err) {\n\t\tdev_err(dev, \"DMA mask config failed, abort\\n\");\n\t\tgoto err_release_regions;\n\t}\n\n\tpci_set_master(pdev);\n\n\t \n\tqcount = min_t(int, num_online_cpus(), OTX2_MAX_CQ_CNT);\n\tqos_txqs = min_t(int, qcount, OTX2_QOS_MAX_LEAF_NODES);\n\n\tnetdev = alloc_etherdev_mqs(sizeof(*pf), qcount + qos_txqs, qcount);\n\tif (!netdev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_release_regions;\n\t}\n\n\tpci_set_drvdata(pdev, netdev);\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\tpf = netdev_priv(netdev);\n\tpf->netdev = netdev;\n\tpf->pdev = pdev;\n\tpf->dev = dev;\n\tpf->total_vfs = pci_sriov_get_totalvfs(pdev);\n\tpf->flags |= OTX2_FLAG_INTF_DOWN;\n\n\thw = &pf->hw;\n\thw->pdev = pdev;\n\thw->rx_queues = qcount;\n\thw->tx_queues = qcount;\n\thw->non_qos_queues = qcount;\n\thw->max_queues = qcount;\n\thw->rbuf_len = OTX2_DEFAULT_RBUF_LEN;\n\t \n\thw->xqe_size = 128;\n\n\tnum_vec = pci_msix_vec_count(pdev);\n\thw->irq_name = devm_kmalloc_array(&hw->pdev->dev, num_vec, NAME_SIZE,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!hw->irq_name) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_netdev;\n\t}\n\n\thw->affinity_mask = devm_kcalloc(&hw->pdev->dev, num_vec,\n\t\t\t\t\t sizeof(cpumask_var_t), GFP_KERNEL);\n\tif (!hw->affinity_mask) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_netdev;\n\t}\n\n\t \n\tpf->reg_base = pcim_iomap(pdev, PCI_CFG_REG_BAR_NUM, 0);\n\tif (!pf->reg_base) {\n\t\tdev_err(dev, \"Unable to map physical function CSRs, aborting\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_free_netdev;\n\t}\n\n\terr = otx2_check_pf_usable(pf);\n\tif (err)\n\t\tgoto err_free_netdev;\n\n\terr = pci_alloc_irq_vectors(hw->pdev, RVU_PF_INT_VEC_CNT,\n\t\t\t\t    RVU_PF_INT_VEC_CNT, PCI_IRQ_MSIX);\n\tif (err < 0) {\n\t\tdev_err(dev, \"%s: Failed to alloc %d IRQ vectors\\n\",\n\t\t\t__func__, num_vec);\n\t\tgoto err_free_netdev;\n\t}\n\n\totx2_setup_dev_hw_settings(pf);\n\n\t \n\terr = otx2_pfaf_mbox_init(pf);\n\tif (err)\n\t\tgoto err_free_irq_vectors;\n\n\t \n\terr = otx2_register_mbox_intr(pf, true);\n\tif (err)\n\t\tgoto err_mbox_destroy;\n\n\t \n\terr = otx2_attach_npa_nix(pf);\n\tif (err)\n\t\tgoto err_disable_mbox_intr;\n\n\terr = otx2_realloc_msix_vectors(pf);\n\tif (err)\n\t\tgoto err_detach_rsrc;\n\n\terr = otx2_set_real_num_queues(netdev, hw->tx_queues, hw->rx_queues);\n\tif (err)\n\t\tgoto err_detach_rsrc;\n\n\terr = cn10k_lmtst_init(pf);\n\tif (err)\n\t\tgoto err_detach_rsrc;\n\n\t \n\totx2_get_mac_from_af(netdev);\n\n\t \n\totx2_ptp_init(pf);\n\n\t \n\tpf->iommu_domain = iommu_get_domain_for_dev(dev);\n\n\tnetdev->hw_features = (NETIF_F_RXCSUM | NETIF_F_IP_CSUM |\n\t\t\t       NETIF_F_IPV6_CSUM | NETIF_F_RXHASH |\n\t\t\t       NETIF_F_SG | NETIF_F_TSO | NETIF_F_TSO6 |\n\t\t\t       NETIF_F_GSO_UDP_L4);\n\tnetdev->features |= netdev->hw_features;\n\n\terr = otx2_mcam_flow_init(pf);\n\tif (err)\n\t\tgoto err_ptp_destroy;\n\n\terr = cn10k_mcs_init(pf);\n\tif (err)\n\t\tgoto err_del_mcam_entries;\n\n\tif (pf->flags & OTX2_FLAG_NTUPLE_SUPPORT)\n\t\tnetdev->hw_features |= NETIF_F_NTUPLE;\n\n\tif (pf->flags & OTX2_FLAG_UCAST_FLTR_SUPPORT)\n\t\tnetdev->priv_flags |= IFF_UNICAST_FLT;\n\n\t \n\tnetdev->vlan_features |= netdev->features;\n\tnetdev->hw_features  |= NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\tNETIF_F_HW_VLAN_STAG_TX;\n\tif (pf->flags & OTX2_FLAG_RX_VLAN_SUPPORT)\n\t\tnetdev->hw_features |= NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t\t       NETIF_F_HW_VLAN_STAG_RX;\n\tnetdev->features |= netdev->hw_features;\n\n\t \n\tif (pf->flags & OTX2_FLAG_TC_FLOWER_SUPPORT)\n\t\tnetdev->hw_features |= NETIF_F_HW_TC;\n\n\tnetdev->hw_features |= NETIF_F_LOOPBACK | NETIF_F_RXALL;\n\n\tnetif_set_tso_max_segs(netdev, OTX2_MAX_GSO_SEGS);\n\tnetdev->watchdog_timeo = OTX2_TX_TIMEOUT;\n\n\tnetdev->netdev_ops = &otx2_netdev_ops;\n\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT;\n\n\tnetdev->min_mtu = OTX2_MIN_MTU;\n\tnetdev->max_mtu = otx2_get_max_mtu(pf);\n\n\terr = register_netdev(netdev);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to register netdevice\\n\");\n\t\tgoto err_mcs_free;\n\t}\n\n\terr = otx2_wq_init(pf);\n\tif (err)\n\t\tgoto err_unreg_netdev;\n\n\totx2_set_ethtool_ops(netdev);\n\n\terr = otx2_init_tc(pf);\n\tif (err)\n\t\tgoto err_mcam_flow_del;\n\n\terr = otx2_register_dl(pf);\n\tif (err)\n\t\tgoto err_mcam_flow_del;\n\n\t \n\terr = otx2_sriov_vfcfg_init(pf);\n\tif (err)\n\t\tgoto err_pf_sriov_init;\n\n\t \n\totx2_cgx_config_linkevents(pf, true);\n\n#ifdef CONFIG_DCB\n\terr = otx2_dcbnl_set_ops(netdev);\n\tif (err)\n\t\tgoto err_pf_sriov_init;\n#endif\n\n\totx2_qos_init(pf, qos_txqs);\n\n\treturn 0;\n\nerr_pf_sriov_init:\n\totx2_shutdown_tc(pf);\nerr_mcam_flow_del:\n\totx2_mcam_flow_del(pf);\nerr_unreg_netdev:\n\tunregister_netdev(netdev);\nerr_mcs_free:\n\tcn10k_mcs_free(pf);\nerr_del_mcam_entries:\n\totx2_mcam_flow_del(pf);\nerr_ptp_destroy:\n\totx2_ptp_destroy(pf);\nerr_detach_rsrc:\n\tif (pf->hw.lmt_info)\n\t\tfree_percpu(pf->hw.lmt_info);\n\tif (test_bit(CN10K_LMTST, &pf->hw.cap_flag))\n\t\tqmem_free(pf->dev, pf->dync_lmt);\n\totx2_detach_resources(&pf->mbox);\nerr_disable_mbox_intr:\n\totx2_disable_mbox_intr(pf);\nerr_mbox_destroy:\n\totx2_pfaf_mbox_destroy(pf);\nerr_free_irq_vectors:\n\tpci_free_irq_vectors(hw->pdev);\nerr_free_netdev:\n\tpci_set_drvdata(pdev, NULL);\n\tfree_netdev(netdev);\nerr_release_regions:\n\tpci_release_regions(pdev);\n\treturn err;\n}\n\nstatic void otx2_vf_link_event_task(struct work_struct *work)\n{\n\tstruct otx2_vf_config *config;\n\tstruct cgx_link_info_msg *req;\n\tstruct mbox_msghdr *msghdr;\n\tstruct otx2_nic *pf;\n\tint vf_idx;\n\n\tconfig = container_of(work, struct otx2_vf_config,\n\t\t\t      link_event_work.work);\n\tvf_idx = config - config->pf->vf_configs;\n\tpf = config->pf;\n\n\tmsghdr = otx2_mbox_alloc_msg_rsp(&pf->mbox_pfvf[0].mbox_up, vf_idx,\n\t\t\t\t\t sizeof(*req), sizeof(struct msg_rsp));\n\tif (!msghdr) {\n\t\tdev_err(pf->dev, \"Failed to create VF%d link event\\n\", vf_idx);\n\t\treturn;\n\t}\n\n\treq = (struct cgx_link_info_msg *)msghdr;\n\treq->hdr.id = MBOX_MSG_CGX_LINK_EVENT;\n\treq->hdr.sig = OTX2_MBOX_REQ_SIG;\n\tmemcpy(&req->link_info, &pf->linfo, sizeof(req->link_info));\n\n\totx2_sync_mbox_up_msg(&pf->mbox_pfvf[0], vf_idx);\n}\n\nstatic int otx2_sriov_enable(struct pci_dev *pdev, int numvfs)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tint ret;\n\n\t \n\tret = otx2_pfvf_mbox_init(pf, numvfs);\n\tif (ret)\n\t\treturn ret;\n\n\tret = otx2_register_pfvf_mbox_intr(pf, numvfs);\n\tif (ret)\n\t\tgoto free_mbox;\n\n\tret = otx2_pf_flr_init(pf, numvfs);\n\tif (ret)\n\t\tgoto free_intr;\n\n\tret = otx2_register_flr_me_intr(pf, numvfs);\n\tif (ret)\n\t\tgoto free_flr;\n\n\tret = pci_enable_sriov(pdev, numvfs);\n\tif (ret)\n\t\tgoto free_flr_intr;\n\n\treturn numvfs;\nfree_flr_intr:\n\totx2_disable_flr_me_intr(pf);\nfree_flr:\n\totx2_flr_wq_destroy(pf);\nfree_intr:\n\totx2_disable_pfvf_mbox_intr(pf, numvfs);\nfree_mbox:\n\totx2_pfvf_mbox_destroy(pf);\n\treturn ret;\n}\n\nstatic int otx2_sriov_disable(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct otx2_nic *pf = netdev_priv(netdev);\n\tint numvfs = pci_num_vf(pdev);\n\n\tif (!numvfs)\n\t\treturn 0;\n\n\tpci_disable_sriov(pdev);\n\n\totx2_disable_flr_me_intr(pf);\n\totx2_flr_wq_destroy(pf);\n\totx2_disable_pfvf_mbox_intr(pf, numvfs);\n\totx2_pfvf_mbox_destroy(pf);\n\n\treturn 0;\n}\n\nstatic int otx2_sriov_configure(struct pci_dev *pdev, int numvfs)\n{\n\tif (numvfs == 0)\n\t\treturn otx2_sriov_disable(pdev);\n\telse\n\t\treturn otx2_sriov_enable(pdev, numvfs);\n}\n\nstatic void otx2_remove(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct otx2_nic *pf;\n\n\tif (!netdev)\n\t\treturn;\n\n\tpf = netdev_priv(netdev);\n\n\tpf->flags |= OTX2_FLAG_PF_SHUTDOWN;\n\n\tif (pf->flags & OTX2_FLAG_TX_TSTAMP_ENABLED)\n\t\totx2_config_hw_tx_tstamp(pf, false);\n\tif (pf->flags & OTX2_FLAG_RX_TSTAMP_ENABLED)\n\t\totx2_config_hw_rx_tstamp(pf, false);\n\n\t \n\tif (pf->flags & OTX2_FLAG_RX_PAUSE_ENABLED ||\n\t    (pf->flags & OTX2_FLAG_TX_PAUSE_ENABLED)) {\n\t\tpf->flags &= ~OTX2_FLAG_RX_PAUSE_ENABLED;\n\t\tpf->flags &= ~OTX2_FLAG_TX_PAUSE_ENABLED;\n\t\totx2_config_pause_frm(pf);\n\t}\n\n#ifdef CONFIG_DCB\n\t \n\tif (pf->pfc_en) {\n\t\tpf->pfc_en = 0;\n\t\totx2_config_priority_flow_ctrl(pf);\n\t}\n#endif\n\tcancel_work_sync(&pf->reset_task);\n\t \n\totx2_cgx_config_linkevents(pf, false);\n\n\totx2_unregister_dl(pf);\n\tunregister_netdev(netdev);\n\tcn10k_mcs_free(pf);\n\totx2_sriov_disable(pf->pdev);\n\totx2_sriov_vfcfg_cleanup(pf);\n\tif (pf->otx2_wq)\n\t\tdestroy_workqueue(pf->otx2_wq);\n\n\totx2_ptp_destroy(pf);\n\totx2_mcam_flow_del(pf);\n\totx2_shutdown_tc(pf);\n\totx2_shutdown_qos(pf);\n\totx2_detach_resources(&pf->mbox);\n\tif (pf->hw.lmt_info)\n\t\tfree_percpu(pf->hw.lmt_info);\n\tif (test_bit(CN10K_LMTST, &pf->hw.cap_flag))\n\t\tqmem_free(pf->dev, pf->dync_lmt);\n\totx2_disable_mbox_intr(pf);\n\totx2_pfaf_mbox_destroy(pf);\n\tpci_free_irq_vectors(pf->pdev);\n\tpci_set_drvdata(pdev, NULL);\n\tfree_netdev(netdev);\n\n\tpci_release_regions(pdev);\n}\n\nstatic struct pci_driver otx2_pf_driver = {\n\t.name = DRV_NAME,\n\t.id_table = otx2_pf_id_table,\n\t.probe = otx2_probe,\n\t.shutdown = otx2_remove,\n\t.remove = otx2_remove,\n\t.sriov_configure = otx2_sriov_configure\n};\n\nstatic int __init otx2_rvupf_init_module(void)\n{\n\tpr_info(\"%s: %s\\n\", DRV_NAME, DRV_STRING);\n\n\treturn pci_register_driver(&otx2_pf_driver);\n}\n\nstatic void __exit otx2_rvupf_cleanup_module(void)\n{\n\tpci_unregister_driver(&otx2_pf_driver);\n}\n\nmodule_init(otx2_rvupf_init_module);\nmodule_exit(otx2_rvupf_cleanup_module);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}