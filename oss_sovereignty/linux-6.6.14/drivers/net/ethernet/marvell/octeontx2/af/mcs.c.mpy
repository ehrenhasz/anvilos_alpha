{
  "module_name": "mcs.c",
  "hash_id": "7c64e354b90a93d1f93fd5311edfa92f60246cc71fd600b049f5ed9ee71ecf9a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/marvell/octeontx2/af/mcs.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/delay.h>\n#include <linux/device.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include \"mcs.h\"\n#include \"mcs_reg.h\"\n\n#define DRV_NAME\t\"Marvell MCS Driver\"\n\n#define PCI_CFG_REG_BAR_NUM\t0\n\nstatic const struct pci_device_id mcs_id_table[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_CAVIUM, PCI_DEVID_CN10K_MCS) },\n\t{ 0, }   \n};\n\nstatic LIST_HEAD(mcs_list);\n\nvoid mcs_get_tx_secy_stats(struct mcs *mcs, struct mcs_secy_stats *stats, int id)\n{\n\tu64 reg;\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_IFOUTCTLBCPKTSX(id);\n\tstats->ctl_pkt_bcast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_IFOUTCTLMCPKTSX(id);\n\tstats->ctl_pkt_mcast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_IFOUTCTLOCTETSX(id);\n\tstats->ctl_octet_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_IFOUTCTLUCPKTSX(id);\n\tstats->ctl_pkt_ucast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_IFOUTUNCTLBCPKTSX(id);\n\tstats->unctl_pkt_bcast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_IFOUTUNCTLMCPKTSX(id);\n\tstats->unctl_pkt_mcast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_IFOUTUNCTLOCTETSX(id);\n\tstats->unctl_octet_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_IFOUTUNCTLUCPKTSX(id);\n\tstats->unctl_pkt_ucast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_OUTOCTETSSECYENCRYPTEDX(id);\n\tstats->octet_encrypted_cnt =  mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_OUTOCTETSSECYPROTECTEDX(id);\n\tstats->octet_protected_cnt =  mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSSECYNOACTIVESAX(id);\n\tstats->pkt_noactivesa_cnt =  mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSSECYTOOLONGX(id);\n\tstats->pkt_toolong_cnt =  mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSSECYUNTAGGEDX(id);\n\tstats->pkt_untagged_cnt =  mcs_reg_read(mcs, reg);\n}\n\nvoid mcs_get_rx_secy_stats(struct mcs *mcs, struct mcs_secy_stats *stats, int id)\n{\n\tu64 reg;\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_IFINCTLBCPKTSX(id);\n\tstats->ctl_pkt_bcast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_IFINCTLMCPKTSX(id);\n\tstats->ctl_pkt_mcast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_IFINCTLOCTETSX(id);\n\tstats->ctl_octet_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_IFINCTLUCPKTSX(id);\n\tstats->ctl_pkt_ucast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_IFINUNCTLBCPKTSX(id);\n\tstats->unctl_pkt_bcast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_IFINUNCTLMCPKTSX(id);\n\tstats->unctl_pkt_mcast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_IFINUNCTLOCTETSX(id);\n\tstats->unctl_octet_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_IFINUNCTLUCPKTSX(id);\n\tstats->unctl_pkt_ucast_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_INOCTETSSECYDECRYPTEDX(id);\n\tstats->octet_decrypted_cnt =  mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_INOCTETSSECYVALIDATEX(id);\n\tstats->octet_validated_cnt =  mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSCTRLPORTDISABLEDX(id);\n\tstats->pkt_port_disabled_cnt =  mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSECYBADTAGX(id);\n\tstats->pkt_badtag_cnt =  mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSECYNOSAX(id);\n\tstats->pkt_nosa_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSECYNOSAERRORX(id);\n\tstats->pkt_nosaerror_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSECYTAGGEDCTLX(id);\n\tstats->pkt_tagged_ctl_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSECYUNTAGGEDX(id);\n\tstats->pkt_untaged_cnt = mcs_reg_read(mcs, reg);\n\n\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSECYCTLX(id);\n\tstats->pkt_ctl_cnt = mcs_reg_read(mcs, reg);\n\n\tif (mcs->hw->mcs_blks > 1) {\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSECYNOTAGX(id);\n\t\tstats->pkt_notag_cnt = mcs_reg_read(mcs, reg);\n\t}\n}\n\nvoid mcs_get_flowid_stats(struct mcs *mcs, struct mcs_flowid_stats *stats,\n\t\t\t  int id, int dir)\n{\n\tu64 reg;\n\n\tif (dir == MCS_RX)\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSFLOWIDTCAMHITX(id);\n\telse\n\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSFLOWIDTCAMHITX(id);\n\n\tstats->tcam_hit_cnt = mcs_reg_read(mcs, reg);\n}\n\nvoid mcs_get_port_stats(struct mcs *mcs, struct mcs_port_stats *stats,\n\t\t\tint id, int dir)\n{\n\tu64 reg;\n\n\tif (dir == MCS_RX) {\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSFLOWIDTCAMMISSX(id);\n\t\tstats->tcam_miss_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSPARSEERRX(id);\n\t\tstats->parser_err_cnt = mcs_reg_read(mcs, reg);\n\t\tif (mcs->hw->mcs_blks > 1) {\n\t\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSEARLYPREEMPTERRX(id);\n\t\t\tstats->preempt_err_cnt = mcs_reg_read(mcs, reg);\n\t\t}\n\t} else {\n\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSFLOWIDTCAMMISSX(id);\n\t\tstats->tcam_miss_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSPARSEERRX(id);\n\t\tstats->parser_err_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSSECTAGINSERTIONERRX(id);\n\t\tstats->sectag_insert_err_cnt = mcs_reg_read(mcs, reg);\n\t}\n}\n\nvoid mcs_get_sa_stats(struct mcs *mcs, struct mcs_sa_stats *stats, int id, int dir)\n{\n\tu64 reg;\n\n\tif (dir == MCS_RX) {\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSAINVALIDX(id);\n\t\tstats->pkt_invalid_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSANOTUSINGSAERRORX(id);\n\t\tstats->pkt_nosaerror_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSANOTVALIDX(id);\n\t\tstats->pkt_notvalid_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSAOKX(id);\n\t\tstats->pkt_ok_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSAUNUSEDSAX(id);\n\t\tstats->pkt_nosa_cnt = mcs_reg_read(mcs, reg);\n\t} else {\n\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSSAENCRYPTEDX(id);\n\t\tstats->pkt_encrypt_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSSAPROTECTEDX(id);\n\t\tstats->pkt_protected_cnt = mcs_reg_read(mcs, reg);\n\t}\n}\n\nvoid mcs_get_sc_stats(struct mcs *mcs, struct mcs_sc_stats *stats,\n\t\t      int id, int dir)\n{\n\tu64 reg;\n\n\tif (dir == MCS_RX) {\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSCCAMHITX(id);\n\t\tstats->hit_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSCINVALIDX(id);\n\t\tstats->pkt_invalid_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSCLATEORDELAYEDX(id);\n\t\tstats->pkt_late_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSCNOTVALIDX(id);\n\t\tstats->pkt_notvalid_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSCUNCHECKEDX(id);\n\t\tstats->pkt_unchecked_cnt = mcs_reg_read(mcs, reg);\n\n\t\tif (mcs->hw->mcs_blks > 1) {\n\t\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSCDELAYEDX(id);\n\t\t\tstats->pkt_delay_cnt = mcs_reg_read(mcs, reg);\n\n\t\t\treg = MCSX_CSE_RX_MEM_SLAVE_INPKTSSCOKX(id);\n\t\t\tstats->pkt_ok_cnt = mcs_reg_read(mcs, reg);\n\t\t}\n\t\tif (mcs->hw->mcs_blks == 1) {\n\t\t\treg = MCSX_CSE_RX_MEM_SLAVE_INOCTETSSCDECRYPTEDX(id);\n\t\t\tstats->octet_decrypt_cnt = mcs_reg_read(mcs, reg);\n\n\t\t\treg = MCSX_CSE_RX_MEM_SLAVE_INOCTETSSCVALIDATEX(id);\n\t\t\tstats->octet_validate_cnt = mcs_reg_read(mcs, reg);\n\t\t}\n\t} else {\n\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSSCENCRYPTEDX(id);\n\t\tstats->pkt_encrypt_cnt = mcs_reg_read(mcs, reg);\n\n\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTPKTSSCPROTECTEDX(id);\n\t\tstats->pkt_protected_cnt = mcs_reg_read(mcs, reg);\n\n\t\tif (mcs->hw->mcs_blks == 1) {\n\t\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTOCTETSSCENCRYPTEDX(id);\n\t\t\tstats->octet_encrypt_cnt = mcs_reg_read(mcs, reg);\n\n\t\t\treg = MCSX_CSE_TX_MEM_SLAVE_OUTOCTETSSCPROTECTEDX(id);\n\t\t\tstats->octet_protected_cnt = mcs_reg_read(mcs, reg);\n\t\t}\n\t}\n}\n\nvoid mcs_clear_stats(struct mcs *mcs, u8 type, u8 id, int dir)\n{\n\tstruct mcs_flowid_stats flowid_st;\n\tstruct mcs_port_stats port_st;\n\tstruct mcs_secy_stats secy_st;\n\tstruct mcs_sc_stats sc_st;\n\tstruct mcs_sa_stats sa_st;\n\tu64 reg;\n\n\tif (dir == MCS_RX)\n\t\treg = MCSX_CSE_RX_SLAVE_CTRL;\n\telse\n\t\treg = MCSX_CSE_TX_SLAVE_CTRL;\n\n\tmcs_reg_write(mcs, reg, BIT_ULL(0));\n\n\tswitch (type) {\n\tcase MCS_FLOWID_STATS:\n\t\tmcs_get_flowid_stats(mcs, &flowid_st, id, dir);\n\t\tbreak;\n\tcase MCS_SECY_STATS:\n\t\tif (dir == MCS_RX)\n\t\t\tmcs_get_rx_secy_stats(mcs, &secy_st, id);\n\t\telse\n\t\t\tmcs_get_tx_secy_stats(mcs, &secy_st, id);\n\t\tbreak;\n\tcase MCS_SC_STATS:\n\t\tmcs_get_sc_stats(mcs, &sc_st, id, dir);\n\t\tbreak;\n\tcase MCS_SA_STATS:\n\t\tmcs_get_sa_stats(mcs, &sa_st, id, dir);\n\t\tbreak;\n\tcase MCS_PORT_STATS:\n\t\tmcs_get_port_stats(mcs, &port_st, id, dir);\n\t\tbreak;\n\t}\n\n\tmcs_reg_write(mcs, reg, 0x0);\n}\n\nint mcs_clear_all_stats(struct mcs *mcs, u16 pcifunc, int dir)\n{\n\tstruct mcs_rsrc_map *map;\n\tint id;\n\n\tif (dir == MCS_RX)\n\t\tmap = &mcs->rx;\n\telse\n\t\tmap = &mcs->tx;\n\n\t \n\tfor (id = 0; id < map->flow_ids.max; id++) {\n\t\tif (map->flowid2pf_map[id] != pcifunc)\n\t\t\tcontinue;\n\t\tmcs_clear_stats(mcs, MCS_FLOWID_STATS, id, dir);\n\t}\n\n\t \n\tfor (id = 0; id < map->secy.max; id++) {\n\t\tif (map->secy2pf_map[id] != pcifunc)\n\t\t\tcontinue;\n\t\tmcs_clear_stats(mcs, MCS_SECY_STATS, id, dir);\n\t}\n\n\t \n\tfor (id = 0; id < map->secy.max; id++) {\n\t\tif (map->sc2pf_map[id] != pcifunc)\n\t\t\tcontinue;\n\t\tmcs_clear_stats(mcs, MCS_SC_STATS, id, dir);\n\t}\n\n\t \n\tfor (id = 0; id < map->sa.max; id++) {\n\t\tif (map->sa2pf_map[id] != pcifunc)\n\t\t\tcontinue;\n\t\tmcs_clear_stats(mcs, MCS_SA_STATS, id, dir);\n\t}\n\treturn 0;\n}\n\nvoid mcs_pn_table_write(struct mcs *mcs, u8 pn_id, u64 next_pn, u8 dir)\n{\n\tu64 reg;\n\n\tif (dir == MCS_RX)\n\t\treg = MCSX_CPM_RX_SLAVE_SA_PN_TABLE_MEMX(pn_id);\n\telse\n\t\treg = MCSX_CPM_TX_SLAVE_SA_PN_TABLE_MEMX(pn_id);\n\tmcs_reg_write(mcs, reg, next_pn);\n}\n\nvoid cn10kb_mcs_tx_sa_mem_map_write(struct mcs *mcs, struct mcs_tx_sc_sa_map *map)\n{\n\tu64 reg, val;\n\n\tval = (map->sa_index0 & 0xFF) |\n\t      (map->sa_index1 & 0xFF) << 9 |\n\t      (map->rekey_ena & 0x1) << 18 |\n\t      (map->sa_index0_vld & 0x1) << 19 |\n\t      (map->sa_index1_vld & 0x1) << 20 |\n\t      (map->tx_sa_active & 0x1) << 21 |\n\t      map->sectag_sci << 22;\n\treg = MCSX_CPM_TX_SLAVE_SA_MAP_MEM_0X(map->sc_id);\n\tmcs_reg_write(mcs, reg, val);\n\n\tval = map->sectag_sci >> 42;\n\treg = MCSX_CPM_TX_SLAVE_SA_MAP_MEM_1X(map->sc_id);\n\tmcs_reg_write(mcs, reg, val);\n}\n\nvoid cn10kb_mcs_rx_sa_mem_map_write(struct mcs *mcs, struct mcs_rx_sc_sa_map *map)\n{\n\tu64 val, reg;\n\n\tval = (map->sa_index & 0xFF) | map->sa_in_use << 9;\n\n\treg = MCSX_CPM_RX_SLAVE_SA_MAP_MEMX((4 * map->sc_id) + map->an);\n\tmcs_reg_write(mcs, reg, val);\n}\n\nvoid mcs_sa_plcy_write(struct mcs *mcs, u64 *plcy, int sa_id, int dir)\n{\n\tint reg_id;\n\tu64 reg;\n\n\tif (dir == MCS_RX) {\n\t\tfor (reg_id = 0; reg_id < 8; reg_id++) {\n\t\t\treg =  MCSX_CPM_RX_SLAVE_SA_PLCY_MEMX(reg_id, sa_id);\n\t\t\tmcs_reg_write(mcs, reg, plcy[reg_id]);\n\t\t}\n\t} else {\n\t\tfor (reg_id = 0; reg_id < 9; reg_id++) {\n\t\t\treg =  MCSX_CPM_TX_SLAVE_SA_PLCY_MEMX(reg_id, sa_id);\n\t\t\tmcs_reg_write(mcs, reg, plcy[reg_id]);\n\t\t}\n\t}\n}\n\nvoid mcs_ena_dis_sc_cam_entry(struct mcs *mcs, int sc_id, int ena)\n{\n\tu64 reg, val;\n\n\treg = MCSX_CPM_RX_SLAVE_SC_CAM_ENA(0);\n\tif (sc_id > 63)\n\t\treg = MCSX_CPM_RX_SLAVE_SC_CAM_ENA(1);\n\n\tif (ena)\n\t\tval = mcs_reg_read(mcs, reg) | BIT_ULL(sc_id);\n\telse\n\t\tval = mcs_reg_read(mcs, reg) & ~BIT_ULL(sc_id);\n\n\tmcs_reg_write(mcs, reg, val);\n}\n\nvoid mcs_rx_sc_cam_write(struct mcs *mcs, u64 sci, u64 secy, int sc_id)\n{\n\tmcs_reg_write(mcs, MCSX_CPM_RX_SLAVE_SC_CAMX(0, sc_id), sci);\n\tmcs_reg_write(mcs, MCSX_CPM_RX_SLAVE_SC_CAMX(1, sc_id), secy);\n\t \n\tmcs_ena_dis_sc_cam_entry(mcs, sc_id, true);\n}\n\nvoid mcs_secy_plcy_write(struct mcs *mcs, u64 plcy, int secy_id, int dir)\n{\n\tu64 reg;\n\n\tif (dir == MCS_RX)\n\t\treg = MCSX_CPM_RX_SLAVE_SECY_PLCY_MEM_0X(secy_id);\n\telse\n\t\treg = MCSX_CPM_TX_SLAVE_SECY_PLCY_MEMX(secy_id);\n\n\tmcs_reg_write(mcs, reg, plcy);\n\n\tif (mcs->hw->mcs_blks == 1 && dir == MCS_RX)\n\t\tmcs_reg_write(mcs, MCSX_CPM_RX_SLAVE_SECY_PLCY_MEM_1X(secy_id), 0x0ull);\n}\n\nvoid cn10kb_mcs_flowid_secy_map(struct mcs *mcs, struct secy_mem_map *map, int dir)\n{\n\tu64 reg, val;\n\n\tval = (map->secy & 0x7F) | (map->ctrl_pkt & 0x1) << 8;\n\tif (dir == MCS_RX) {\n\t\treg = MCSX_CPM_RX_SLAVE_SECY_MAP_MEMX(map->flow_id);\n\t} else {\n\t\tval |= (map->sc & 0x7F) << 9;\n\t\treg = MCSX_CPM_TX_SLAVE_SECY_MAP_MEM_0X(map->flow_id);\n\t}\n\n\tmcs_reg_write(mcs, reg, val);\n}\n\nvoid mcs_ena_dis_flowid_entry(struct mcs *mcs, int flow_id, int dir, int ena)\n{\n\tu64 reg, val;\n\n\tif (dir == MCS_RX) {\n\t\treg = MCSX_CPM_RX_SLAVE_FLOWID_TCAM_ENA_0;\n\t\tif (flow_id > 63)\n\t\t\treg = MCSX_CPM_RX_SLAVE_FLOWID_TCAM_ENA_1;\n\t} else {\n\t\treg = MCSX_CPM_TX_SLAVE_FLOWID_TCAM_ENA_0;\n\t\tif (flow_id > 63)\n\t\t\treg = MCSX_CPM_TX_SLAVE_FLOWID_TCAM_ENA_1;\n\t}\n\n\t \n\tif (ena)\n\t\tval = mcs_reg_read(mcs, reg) | BIT_ULL(flow_id);\n\telse\n\t\tval = mcs_reg_read(mcs, reg) & ~BIT_ULL(flow_id);\n\n\tmcs_reg_write(mcs, reg, val);\n}\n\nvoid mcs_flowid_entry_write(struct mcs *mcs, u64 *data, u64 *mask, int flow_id, int dir)\n{\n\tint reg_id;\n\tu64 reg;\n\n\tif (dir == MCS_RX) {\n\t\tfor (reg_id = 0; reg_id < 4; reg_id++) {\n\t\t\treg = MCSX_CPM_RX_SLAVE_FLOWID_TCAM_DATAX(reg_id, flow_id);\n\t\t\tmcs_reg_write(mcs, reg, data[reg_id]);\n\t\t}\n\t\tfor (reg_id = 0; reg_id < 4; reg_id++) {\n\t\t\treg = MCSX_CPM_RX_SLAVE_FLOWID_TCAM_MASKX(reg_id, flow_id);\n\t\t\tmcs_reg_write(mcs, reg, mask[reg_id]);\n\t\t}\n\t} else {\n\t\tfor (reg_id = 0; reg_id < 4; reg_id++) {\n\t\t\treg = MCSX_CPM_TX_SLAVE_FLOWID_TCAM_DATAX(reg_id, flow_id);\n\t\t\tmcs_reg_write(mcs, reg, data[reg_id]);\n\t\t}\n\t\tfor (reg_id = 0; reg_id < 4; reg_id++) {\n\t\t\treg = MCSX_CPM_TX_SLAVE_FLOWID_TCAM_MASKX(reg_id, flow_id);\n\t\t\tmcs_reg_write(mcs, reg, mask[reg_id]);\n\t\t}\n\t}\n}\n\nint mcs_install_flowid_bypass_entry(struct mcs *mcs)\n{\n\tint flow_id, secy_id, reg_id;\n\tstruct secy_mem_map map;\n\tu64 reg, plcy = 0;\n\n\t \n\tflow_id = mcs->hw->tcam_entries - MCS_RSRC_RSVD_CNT;\n\t__set_bit(flow_id, mcs->rx.flow_ids.bmap);\n\t__set_bit(flow_id, mcs->tx.flow_ids.bmap);\n\n\tfor (reg_id = 0; reg_id < 4; reg_id++) {\n\t\treg = MCSX_CPM_RX_SLAVE_FLOWID_TCAM_MASKX(reg_id, flow_id);\n\t\tmcs_reg_write(mcs, reg, GENMASK_ULL(63, 0));\n\t}\n\tfor (reg_id = 0; reg_id < 4; reg_id++) {\n\t\treg = MCSX_CPM_TX_SLAVE_FLOWID_TCAM_MASKX(reg_id, flow_id);\n\t\tmcs_reg_write(mcs, reg, GENMASK_ULL(63, 0));\n\t}\n\t \n\tsecy_id = mcs->hw->secy_entries - MCS_RSRC_RSVD_CNT;\n\t__set_bit(secy_id, mcs->rx.secy.bmap);\n\t__set_bit(secy_id, mcs->tx.secy.bmap);\n\n\t \n\tplcy = 0x7ull;\n\tif (mcs->hw->mcs_blks > 1)\n\t\tplcy = BIT_ULL(0) | 0x3ull << 4;\n\tmcs_secy_plcy_write(mcs, plcy, secy_id, MCS_RX);\n\n\t \n\tplcy = BIT_ULL(0) | GENMASK_ULL(43, 28);\n\tif (mcs->hw->mcs_blks > 1)\n\t\tplcy = BIT_ULL(0) | GENMASK_ULL(63, 48);\n\tmcs_secy_plcy_write(mcs, plcy, secy_id, MCS_TX);\n\n\t \n\tmap.secy = secy_id;\n\tmap.ctrl_pkt = 0;\n\tmap.flow_id = flow_id;\n\tmcs->mcs_ops->mcs_flowid_secy_map(mcs, &map, MCS_RX);\n\tmap.sc = secy_id;\n\tmcs->mcs_ops->mcs_flowid_secy_map(mcs, &map, MCS_TX);\n\n\t \n\tmcs_ena_dis_flowid_entry(mcs, flow_id, MCS_RX, true);\n\tmcs_ena_dis_flowid_entry(mcs, flow_id, MCS_TX, true);\n\n\treturn 0;\n}\n\nvoid mcs_clear_secy_plcy(struct mcs *mcs, int secy_id, int dir)\n{\n\tstruct mcs_rsrc_map *map;\n\tint flow_id;\n\n\tif (dir == MCS_RX)\n\t\tmap = &mcs->rx;\n\telse\n\t\tmap = &mcs->tx;\n\n\t \n\tmcs_secy_plcy_write(mcs, 0, secy_id, dir);\n\n\t \n\tfor (flow_id = 0; flow_id < map->flow_ids.max; flow_id++) {\n\t\tif (map->flowid2secy_map[flow_id] != secy_id)\n\t\t\tcontinue;\n\t\tmcs_ena_dis_flowid_entry(mcs, flow_id, dir, false);\n\t}\n}\n\nint mcs_alloc_ctrlpktrule(struct rsrc_bmap *rsrc, u16 *pf_map, u16 offset, u16 pcifunc)\n{\n\tint rsrc_id;\n\n\tif (!rsrc->bmap)\n\t\treturn -EINVAL;\n\n\trsrc_id = bitmap_find_next_zero_area(rsrc->bmap, rsrc->max, offset, 1, 0);\n\tif (rsrc_id >= rsrc->max)\n\t\treturn -ENOSPC;\n\n\tbitmap_set(rsrc->bmap, rsrc_id, 1);\n\tpf_map[rsrc_id] = pcifunc;\n\n\treturn rsrc_id;\n}\n\nint mcs_free_ctrlpktrule(struct mcs *mcs, struct mcs_free_ctrl_pkt_rule_req *req)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct mcs_rsrc_map *map;\n\tu64 dis, reg;\n\tint id, rc;\n\n\treg = (req->dir == MCS_RX) ? MCSX_PEX_RX_SLAVE_RULE_ENABLE : MCSX_PEX_TX_SLAVE_RULE_ENABLE;\n\tmap = (req->dir == MCS_RX) ? &mcs->rx : &mcs->tx;\n\n\tif (req->all) {\n\t\tfor (id = 0; id < map->ctrlpktrule.max; id++) {\n\t\t\tif (map->ctrlpktrule2pf_map[id] != pcifunc)\n\t\t\t\tcontinue;\n\t\t\tmcs_free_rsrc(&map->ctrlpktrule, map->ctrlpktrule2pf_map, id, pcifunc);\n\t\t\tdis = mcs_reg_read(mcs, reg);\n\t\t\tdis &= ~BIT_ULL(id);\n\t\t\tmcs_reg_write(mcs, reg, dis);\n\t\t}\n\t\treturn 0;\n\t}\n\n\trc = mcs_free_rsrc(&map->ctrlpktrule, map->ctrlpktrule2pf_map, req->rule_idx, pcifunc);\n\tdis = mcs_reg_read(mcs, reg);\n\tdis &= ~BIT_ULL(req->rule_idx);\n\tmcs_reg_write(mcs, reg, dis);\n\n\treturn rc;\n}\n\nint mcs_ctrlpktrule_write(struct mcs *mcs, struct mcs_ctrl_pkt_rule_write_req *req)\n{\n\tu64 reg, enb;\n\tu64 idx;\n\n\tswitch (req->rule_type) {\n\tcase MCS_CTRL_PKT_RULE_TYPE_ETH:\n\t\treq->data0 &= GENMASK(15, 0);\n\t\tif (req->data0 != ETH_P_PAE)\n\t\t\treturn -EINVAL;\n\n\t\tidx = req->rule_idx - MCS_CTRLPKT_ETYPE_RULE_OFFSET;\n\t\treg = (req->dir == MCS_RX) ? MCSX_PEX_RX_SLAVE_RULE_ETYPE_CFGX(idx) :\n\t\t      MCSX_PEX_TX_SLAVE_RULE_ETYPE_CFGX(idx);\n\n\t\tmcs_reg_write(mcs, reg, req->data0);\n\t\tbreak;\n\tcase MCS_CTRL_PKT_RULE_TYPE_DA:\n\t\tif (!(req->data0 & BIT_ULL(40)))\n\t\t\treturn -EINVAL;\n\n\t\tidx = req->rule_idx - MCS_CTRLPKT_DA_RULE_OFFSET;\n\t\treg = (req->dir == MCS_RX) ? MCSX_PEX_RX_SLAVE_RULE_DAX(idx) :\n\t\t      MCSX_PEX_TX_SLAVE_RULE_DAX(idx);\n\n\t\tmcs_reg_write(mcs, reg, req->data0 & GENMASK_ULL(47, 0));\n\t\tbreak;\n\tcase MCS_CTRL_PKT_RULE_TYPE_RANGE:\n\t\tif (!(req->data0 & BIT_ULL(40)) || !(req->data1 & BIT_ULL(40)))\n\t\t\treturn -EINVAL;\n\n\t\tidx = req->rule_idx - MCS_CTRLPKT_DA_RANGE_RULE_OFFSET;\n\t\tif (req->dir == MCS_RX) {\n\t\t\treg = MCSX_PEX_RX_SLAVE_RULE_DA_RANGE_MINX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data0 & GENMASK_ULL(47, 0));\n\t\t\treg = MCSX_PEX_RX_SLAVE_RULE_DA_RANGE_MAXX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data1 & GENMASK_ULL(47, 0));\n\t\t} else {\n\t\t\treg = MCSX_PEX_TX_SLAVE_RULE_DA_RANGE_MINX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data0 & GENMASK_ULL(47, 0));\n\t\t\treg = MCSX_PEX_TX_SLAVE_RULE_DA_RANGE_MAXX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data1 & GENMASK_ULL(47, 0));\n\t\t}\n\t\tbreak;\n\tcase MCS_CTRL_PKT_RULE_TYPE_COMBO:\n\t\treq->data2 &= GENMASK(15, 0);\n\t\tif (req->data2 != ETH_P_PAE || !(req->data0 & BIT_ULL(40)) ||\n\t\t    !(req->data1 & BIT_ULL(40)))\n\t\t\treturn -EINVAL;\n\n\t\tidx = req->rule_idx - MCS_CTRLPKT_COMBO_RULE_OFFSET;\n\t\tif (req->dir == MCS_RX) {\n\t\t\treg = MCSX_PEX_RX_SLAVE_RULE_COMBO_MINX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data0 & GENMASK_ULL(47, 0));\n\t\t\treg = MCSX_PEX_RX_SLAVE_RULE_COMBO_MAXX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data1 & GENMASK_ULL(47, 0));\n\t\t\treg = MCSX_PEX_RX_SLAVE_RULE_COMBO_ETX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data2);\n\t\t} else {\n\t\t\treg = MCSX_PEX_TX_SLAVE_RULE_COMBO_MINX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data0 & GENMASK_ULL(47, 0));\n\t\t\treg = MCSX_PEX_TX_SLAVE_RULE_COMBO_MAXX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data1 & GENMASK_ULL(47, 0));\n\t\t\treg = MCSX_PEX_TX_SLAVE_RULE_COMBO_ETX(idx);\n\t\t\tmcs_reg_write(mcs, reg, req->data2);\n\t\t}\n\t\tbreak;\n\tcase MCS_CTRL_PKT_RULE_TYPE_MAC:\n\t\tif (!(req->data0 & BIT_ULL(40)))\n\t\t\treturn -EINVAL;\n\n\t\tidx = req->rule_idx - MCS_CTRLPKT_MAC_EN_RULE_OFFSET;\n\t\treg = (req->dir == MCS_RX) ? MCSX_PEX_RX_SLAVE_RULE_MAC :\n\t\t      MCSX_PEX_TX_SLAVE_RULE_MAC;\n\n\t\tmcs_reg_write(mcs, reg, req->data0 & GENMASK_ULL(47, 0));\n\t\tbreak;\n\t}\n\n\treg = (req->dir == MCS_RX) ? MCSX_PEX_RX_SLAVE_RULE_ENABLE : MCSX_PEX_TX_SLAVE_RULE_ENABLE;\n\n\tenb = mcs_reg_read(mcs, reg);\n\tenb |= BIT_ULL(req->rule_idx);\n\tmcs_reg_write(mcs, reg, enb);\n\n\treturn 0;\n}\n\nint mcs_free_rsrc(struct rsrc_bmap *rsrc, u16 *pf_map, int rsrc_id, u16 pcifunc)\n{\n\t \n\tif (pf_map[rsrc_id] != pcifunc)\n\t\treturn -EINVAL;\n\n\trvu_free_rsrc(rsrc, rsrc_id);\n\tpf_map[rsrc_id] = 0;\n\treturn 0;\n}\n\n \nint mcs_free_all_rsrc(struct mcs *mcs, int dir, u16 pcifunc)\n{\n\tstruct mcs_rsrc_map *map;\n\tint id;\n\n\tif (dir == MCS_RX)\n\t\tmap = &mcs->rx;\n\telse\n\t\tmap = &mcs->tx;\n\n\t \n\tfor (id = 0; id < map->flow_ids.max; id++) {\n\t\tif (map->flowid2pf_map[id] != pcifunc)\n\t\t\tcontinue;\n\t\tmcs_free_rsrc(&map->flow_ids, map->flowid2pf_map,\n\t\t\t      id, pcifunc);\n\t\tmcs_ena_dis_flowid_entry(mcs, id, dir, false);\n\t}\n\n\t \n\tfor (id = 0; id < map->secy.max; id++) {\n\t\tif (map->secy2pf_map[id] != pcifunc)\n\t\t\tcontinue;\n\t\tmcs_free_rsrc(&map->secy, map->secy2pf_map,\n\t\t\t      id, pcifunc);\n\t\tmcs_clear_secy_plcy(mcs, id, dir);\n\t}\n\n\t \n\tfor (id = 0; id < map->secy.max; id++) {\n\t\tif (map->sc2pf_map[id] != pcifunc)\n\t\t\tcontinue;\n\t\tmcs_free_rsrc(&map->sc, map->sc2pf_map, id, pcifunc);\n\n\t\t \n\t\tif (dir == MCS_RX)\n\t\t\tmcs_ena_dis_sc_cam_entry(mcs, id, false);\n\t}\n\n\t \n\tfor (id = 0; id < map->sa.max; id++) {\n\t\tif (map->sa2pf_map[id] != pcifunc)\n\t\t\tcontinue;\n\t\tmcs_free_rsrc(&map->sa, map->sa2pf_map, id, pcifunc);\n\t}\n\treturn 0;\n}\n\nint mcs_alloc_rsrc(struct rsrc_bmap *rsrc, u16 *pf_map, u16 pcifunc)\n{\n\tint rsrc_id;\n\n\trsrc_id = rvu_alloc_rsrc(rsrc);\n\tif (rsrc_id < 0)\n\t\treturn -ENOMEM;\n\tpf_map[rsrc_id] = pcifunc;\n\treturn rsrc_id;\n}\n\nint mcs_alloc_all_rsrc(struct mcs *mcs, u8 *flow_id, u8 *secy_id,\n\t\t       u8 *sc_id, u8 *sa1_id, u8 *sa2_id, u16 pcifunc, int dir)\n{\n\tstruct mcs_rsrc_map *map;\n\tint id;\n\n\tif (dir == MCS_RX)\n\t\tmap = &mcs->rx;\n\telse\n\t\tmap = &mcs->tx;\n\n\tid = mcs_alloc_rsrc(&map->flow_ids, map->flowid2pf_map, pcifunc);\n\tif (id < 0)\n\t\treturn -ENOMEM;\n\t*flow_id = id;\n\n\tid = mcs_alloc_rsrc(&map->secy, map->secy2pf_map, pcifunc);\n\tif (id < 0)\n\t\treturn -ENOMEM;\n\t*secy_id = id;\n\n\tid = mcs_alloc_rsrc(&map->sc, map->sc2pf_map, pcifunc);\n\tif (id < 0)\n\t\treturn -ENOMEM;\n\t*sc_id = id;\n\n\tid =  mcs_alloc_rsrc(&map->sa, map->sa2pf_map, pcifunc);\n\tif (id < 0)\n\t\treturn -ENOMEM;\n\t*sa1_id = id;\n\n\tid =  mcs_alloc_rsrc(&map->sa, map->sa2pf_map, pcifunc);\n\tif (id < 0)\n\t\treturn -ENOMEM;\n\t*sa2_id = id;\n\n\treturn 0;\n}\n\nstatic void cn10kb_mcs_tx_pn_wrapped_handler(struct mcs *mcs)\n{\n\tstruct mcs_intr_event event = { 0 };\n\tstruct rsrc_bmap *sc_bmap;\n\tu64 val;\n\tint sc;\n\n\tsc_bmap = &mcs->tx.sc;\n\n\tevent.mcs_id = mcs->mcs_id;\n\tevent.intr_mask = MCS_CPM_TX_PACKET_XPN_EQ0_INT;\n\n\tfor_each_set_bit(sc, sc_bmap->bmap, mcs->hw->sc_entries) {\n\t\tval = mcs_reg_read(mcs, MCSX_CPM_TX_SLAVE_SA_MAP_MEM_0X(sc));\n\n\t\tif (mcs->tx_sa_active[sc])\n\t\t\t \n\t\t\tevent.sa_id = (val >> 9) & 0xFF;\n\t\telse\n\t\t\t \n\t\t\tevent.sa_id = val & 0xFF;\n\n\t\tevent.pcifunc = mcs->tx.sa2pf_map[event.sa_id];\n\t\tmcs_add_intr_wq_entry(mcs, &event);\n\t}\n}\n\nstatic void cn10kb_mcs_tx_pn_thresh_reached_handler(struct mcs *mcs)\n{\n\tstruct mcs_intr_event event = { 0 };\n\tstruct rsrc_bmap *sc_bmap;\n\tu64 val, status;\n\tint sc;\n\n\tsc_bmap = &mcs->tx.sc;\n\n\tevent.mcs_id = mcs->mcs_id;\n\tevent.intr_mask = MCS_CPM_TX_PN_THRESH_REACHED_INT;\n\n\t \n\tfor_each_set_bit(sc, sc_bmap->bmap, mcs->hw->sc_entries) {\n\t\tval = mcs_reg_read(mcs, MCSX_CPM_TX_SLAVE_SA_MAP_MEM_0X(sc));\n\t\t \n\t\tif (!((val >> 18) & 0x1))\n\t\t\tcontinue;\n\n\t\tstatus = (val >> 21) & 0x1;\n\n\t\t \n\t\tif (status == mcs->tx_sa_active[sc])\n\t\t\tcontinue;\n\t\t \n\t\tif (status)\n\t\t\tevent.sa_id = val & 0xFF;\n\t\telse\n\t\t\tevent.sa_id = (val >> 9) & 0xFF;\n\n\t\tevent.pcifunc = mcs->tx.sa2pf_map[event.sa_id];\n\t\tmcs_add_intr_wq_entry(mcs, &event);\n\t}\n}\n\nstatic void mcs_rx_pn_thresh_reached_handler(struct mcs *mcs)\n{\n\tstruct mcs_intr_event event = { 0 };\n\tint sa, reg;\n\tu64 intr;\n\n\t \n\tfor (reg = 0; reg < (mcs->hw->sa_entries / 64); reg++) {\n\t\t \n\t\tintr = mcs_reg_read(mcs, MCSX_CPM_RX_SLAVE_PN_THRESH_REACHEDX(reg));\n\t\tfor (sa = 0; sa < 64; sa++) {\n\t\t\tif (!(intr & BIT_ULL(sa)))\n\t\t\t\tcontinue;\n\n\t\t\tevent.mcs_id = mcs->mcs_id;\n\t\t\tevent.intr_mask = MCS_CPM_RX_PN_THRESH_REACHED_INT;\n\t\t\tevent.sa_id = sa + (reg * 64);\n\t\t\tevent.pcifunc = mcs->rx.sa2pf_map[event.sa_id];\n\t\t\tmcs_add_intr_wq_entry(mcs, &event);\n\t\t}\n\t}\n}\n\nstatic void mcs_rx_misc_intr_handler(struct mcs *mcs, u64 intr)\n{\n\tstruct mcs_intr_event event = { 0 };\n\n\tevent.mcs_id = mcs->mcs_id;\n\tevent.pcifunc = mcs->pf_map[0];\n\n\tif (intr & MCS_CPM_RX_INT_SECTAG_V_EQ1)\n\t\tevent.intr_mask = MCS_CPM_RX_SECTAG_V_EQ1_INT;\n\tif (intr & MCS_CPM_RX_INT_SECTAG_E_EQ0_C_EQ1)\n\t\tevent.intr_mask |= MCS_CPM_RX_SECTAG_E_EQ0_C_EQ1_INT;\n\tif (intr & MCS_CPM_RX_INT_SL_GTE48)\n\t\tevent.intr_mask |= MCS_CPM_RX_SECTAG_SL_GTE48_INT;\n\tif (intr & MCS_CPM_RX_INT_ES_EQ1_SC_EQ1)\n\t\tevent.intr_mask |= MCS_CPM_RX_SECTAG_ES_EQ1_SC_EQ1_INT;\n\tif (intr & MCS_CPM_RX_INT_SC_EQ1_SCB_EQ1)\n\t\tevent.intr_mask |= MCS_CPM_RX_SECTAG_SC_EQ1_SCB_EQ1_INT;\n\tif (intr & MCS_CPM_RX_INT_PACKET_XPN_EQ0)\n\t\tevent.intr_mask |= MCS_CPM_RX_PACKET_XPN_EQ0_INT;\n\n\tmcs_add_intr_wq_entry(mcs, &event);\n}\n\nstatic void mcs_tx_misc_intr_handler(struct mcs *mcs, u64 intr)\n{\n\tstruct mcs_intr_event event = { 0 };\n\n\tif (!(intr & MCS_CPM_TX_INT_SA_NOT_VALID))\n\t\treturn;\n\n\tevent.mcs_id = mcs->mcs_id;\n\tevent.pcifunc = mcs->pf_map[0];\n\n\tevent.intr_mask = MCS_CPM_TX_SA_NOT_VALID_INT;\n\n\tmcs_add_intr_wq_entry(mcs, &event);\n}\n\nvoid cn10kb_mcs_bbe_intr_handler(struct mcs *mcs, u64 intr,\n\t\t\t\t enum mcs_direction dir)\n{\n\tu64 val, reg;\n\tint lmac;\n\n\tif (!(intr & 0x6ULL))\n\t\treturn;\n\n\tif (intr & BIT_ULL(1))\n\t\treg = (dir == MCS_RX) ? MCSX_BBE_RX_SLAVE_DFIFO_OVERFLOW_0 :\n\t\t\t\t\tMCSX_BBE_TX_SLAVE_DFIFO_OVERFLOW_0;\n\telse\n\t\treg = (dir == MCS_RX) ? MCSX_BBE_RX_SLAVE_PLFIFO_OVERFLOW_0 :\n\t\t\t\t\tMCSX_BBE_TX_SLAVE_PLFIFO_OVERFLOW_0;\n\tval = mcs_reg_read(mcs, reg);\n\n\t \n\tfor (lmac = 0; lmac < mcs->hw->lmac_cnt; lmac++) {\n\t\tif (!(val & BIT_ULL(lmac)))\n\t\t\tcontinue;\n\t\tdev_warn(mcs->dev, \"BEE:Policy or data overflow occurred on lmac:%d\\n\", lmac);\n\t}\n}\n\nvoid cn10kb_mcs_pab_intr_handler(struct mcs *mcs, u64 intr,\n\t\t\t\t enum mcs_direction dir)\n{\n\tint lmac;\n\n\tif (!(intr & 0xFFFFFULL))\n\t\treturn;\n\n\tfor (lmac = 0; lmac < mcs->hw->lmac_cnt; lmac++) {\n\t\tif (intr & BIT_ULL(lmac))\n\t\t\tdev_warn(mcs->dev, \"PAB: overflow occurred on lmac:%d\\n\", lmac);\n\t}\n}\n\nstatic irqreturn_t mcs_ip_intr_handler(int irq, void *mcs_irq)\n{\n\tstruct mcs *mcs = (struct mcs *)mcs_irq;\n\tu64 intr, cpm_intr, bbe_intr, pab_intr;\n\n\t \n\tmcs_reg_write(mcs, MCSX_IP_INT_ENA_W1C, BIT_ULL(0));\n\n\t \n\tintr = mcs_reg_read(mcs, MCSX_TOP_SLAVE_INT_SUM);\n\n\t \n\tif (intr & MCS_CPM_RX_INT_ENA) {\n\t\t \n\t\tcpm_intr = mcs_reg_read(mcs, MCSX_CPM_RX_SLAVE_RX_INT);\n\n\t\tif (cpm_intr & MCS_CPM_RX_INT_PN_THRESH_REACHED)\n\t\t\tmcs_rx_pn_thresh_reached_handler(mcs);\n\n\t\tif (cpm_intr & MCS_CPM_RX_INT_ALL)\n\t\t\tmcs_rx_misc_intr_handler(mcs, cpm_intr);\n\n\t\t \n\t\tmcs_reg_write(mcs, MCSX_CPM_RX_SLAVE_RX_INT, cpm_intr);\n\t}\n\n\t \n\tif (intr & MCS_CPM_TX_INT_ENA) {\n\t\tcpm_intr = mcs_reg_read(mcs, MCSX_CPM_TX_SLAVE_TX_INT);\n\n\t\tif (cpm_intr & MCS_CPM_TX_INT_PN_THRESH_REACHED) {\n\t\t\tif (mcs->hw->mcs_blks > 1)\n\t\t\t\tcnf10kb_mcs_tx_pn_thresh_reached_handler(mcs);\n\t\t\telse\n\t\t\t\tcn10kb_mcs_tx_pn_thresh_reached_handler(mcs);\n\t\t}\n\n\t\tif (cpm_intr & MCS_CPM_TX_INT_SA_NOT_VALID)\n\t\t\tmcs_tx_misc_intr_handler(mcs, cpm_intr);\n\n\t\tif (cpm_intr & MCS_CPM_TX_INT_PACKET_XPN_EQ0) {\n\t\t\tif (mcs->hw->mcs_blks > 1)\n\t\t\t\tcnf10kb_mcs_tx_pn_wrapped_handler(mcs);\n\t\t\telse\n\t\t\t\tcn10kb_mcs_tx_pn_wrapped_handler(mcs);\n\t\t}\n\t\t \n\t\tmcs_reg_write(mcs, MCSX_CPM_TX_SLAVE_TX_INT, cpm_intr);\n\t}\n\n\t \n\tif (intr & MCS_BBE_RX_INT_ENA) {\n\t\tbbe_intr = mcs_reg_read(mcs, MCSX_BBE_RX_SLAVE_BBE_INT);\n\t\tmcs->mcs_ops->mcs_bbe_intr_handler(mcs, bbe_intr, MCS_RX);\n\n\t\t \n\t\tmcs_reg_write(mcs, MCSX_BBE_RX_SLAVE_BBE_INT_INTR_RW, 0);\n\t\tmcs_reg_write(mcs, MCSX_BBE_RX_SLAVE_BBE_INT, bbe_intr);\n\t}\n\n\t \n\tif (intr & MCS_BBE_TX_INT_ENA) {\n\t\tbbe_intr = mcs_reg_read(mcs, MCSX_BBE_TX_SLAVE_BBE_INT);\n\t\tmcs->mcs_ops->mcs_bbe_intr_handler(mcs, bbe_intr, MCS_TX);\n\n\t\t \n\t\tmcs_reg_write(mcs, MCSX_BBE_TX_SLAVE_BBE_INT_INTR_RW, 0);\n\t\tmcs_reg_write(mcs, MCSX_BBE_TX_SLAVE_BBE_INT, bbe_intr);\n\t}\n\n\t \n\tif (intr & MCS_PAB_RX_INT_ENA) {\n\t\tpab_intr = mcs_reg_read(mcs, MCSX_PAB_RX_SLAVE_PAB_INT);\n\t\tmcs->mcs_ops->mcs_pab_intr_handler(mcs, pab_intr, MCS_RX);\n\n\t\t \n\t\tmcs_reg_write(mcs, MCSX_PAB_RX_SLAVE_PAB_INT_INTR_RW, 0);\n\t\tmcs_reg_write(mcs, MCSX_PAB_RX_SLAVE_PAB_INT, pab_intr);\n\t}\n\n\t \n\tif (intr & MCS_PAB_TX_INT_ENA) {\n\t\tpab_intr = mcs_reg_read(mcs, MCSX_PAB_TX_SLAVE_PAB_INT);\n\t\tmcs->mcs_ops->mcs_pab_intr_handler(mcs, pab_intr, MCS_TX);\n\n\t\t \n\t\tmcs_reg_write(mcs, MCSX_PAB_TX_SLAVE_PAB_INT_INTR_RW, 0);\n\t\tmcs_reg_write(mcs, MCSX_PAB_TX_SLAVE_PAB_INT, pab_intr);\n\t}\n\n\t \n\tmcs_reg_write(mcs, MCSX_IP_INT, BIT_ULL(0));\n\tmcs_reg_write(mcs, MCSX_IP_INT_ENA_W1S, BIT_ULL(0));\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void *alloc_mem(struct mcs *mcs, int n)\n{\n\treturn devm_kcalloc(mcs->dev, n, sizeof(u16), GFP_KERNEL);\n}\n\nstatic int mcs_alloc_struct_mem(struct mcs *mcs, struct mcs_rsrc_map *res)\n{\n\tstruct hwinfo *hw = mcs->hw;\n\tint err;\n\n\tres->flowid2pf_map = alloc_mem(mcs, hw->tcam_entries);\n\tif (!res->flowid2pf_map)\n\t\treturn -ENOMEM;\n\n\tres->secy2pf_map = alloc_mem(mcs, hw->secy_entries);\n\tif (!res->secy2pf_map)\n\t\treturn -ENOMEM;\n\n\tres->sc2pf_map = alloc_mem(mcs, hw->sc_entries);\n\tif (!res->sc2pf_map)\n\t\treturn -ENOMEM;\n\n\tres->sa2pf_map = alloc_mem(mcs, hw->sa_entries);\n\tif (!res->sa2pf_map)\n\t\treturn -ENOMEM;\n\n\tres->flowid2secy_map = alloc_mem(mcs, hw->tcam_entries);\n\tif (!res->flowid2secy_map)\n\t\treturn -ENOMEM;\n\n\tres->ctrlpktrule2pf_map = alloc_mem(mcs, MCS_MAX_CTRLPKT_RULES);\n\tif (!res->ctrlpktrule2pf_map)\n\t\treturn -ENOMEM;\n\n\tres->flow_ids.max = hw->tcam_entries - MCS_RSRC_RSVD_CNT;\n\terr = rvu_alloc_bitmap(&res->flow_ids);\n\tif (err)\n\t\treturn err;\n\n\tres->secy.max = hw->secy_entries - MCS_RSRC_RSVD_CNT;\n\terr = rvu_alloc_bitmap(&res->secy);\n\tif (err)\n\t\treturn err;\n\n\tres->sc.max = hw->sc_entries;\n\terr = rvu_alloc_bitmap(&res->sc);\n\tif (err)\n\t\treturn err;\n\n\tres->sa.max = hw->sa_entries;\n\terr = rvu_alloc_bitmap(&res->sa);\n\tif (err)\n\t\treturn err;\n\n\tres->ctrlpktrule.max = MCS_MAX_CTRLPKT_RULES;\n\terr = rvu_alloc_bitmap(&res->ctrlpktrule);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic int mcs_register_interrupts(struct mcs *mcs)\n{\n\tint ret = 0;\n\n\tmcs->num_vec = pci_msix_vec_count(mcs->pdev);\n\n\tret = pci_alloc_irq_vectors(mcs->pdev, mcs->num_vec,\n\t\t\t\t    mcs->num_vec, PCI_IRQ_MSIX);\n\tif (ret < 0) {\n\t\tdev_err(mcs->dev, \"MCS Request for %d msix vector failed err:%d\\n\",\n\t\t\tmcs->num_vec, ret);\n\t\treturn ret;\n\t}\n\n\tret = request_irq(pci_irq_vector(mcs->pdev, mcs->hw->ip_vec),\n\t\t\t  mcs_ip_intr_handler, 0, \"MCS_IP\", mcs);\n\tif (ret) {\n\t\tdev_err(mcs->dev, \"MCS IP irq registration failed\\n\");\n\t\tgoto exit;\n\t}\n\n\t \n\tmcs_reg_write(mcs, MCSX_IP_INT_ENA_W1S, BIT_ULL(0));\n\n\t \n\tmcs_reg_write(mcs, MCSX_TOP_SLAVE_INT_SUM_ENB,\n\t\t      MCS_CPM_RX_INT_ENA | MCS_CPM_TX_INT_ENA |\n\t\t      MCS_BBE_RX_INT_ENA | MCS_BBE_TX_INT_ENA |\n\t\t      MCS_PAB_RX_INT_ENA | MCS_PAB_TX_INT_ENA);\n\n\tmcs_reg_write(mcs, MCSX_CPM_TX_SLAVE_TX_INT_ENB, 0x7ULL);\n\tmcs_reg_write(mcs, MCSX_CPM_RX_SLAVE_RX_INT_ENB, 0x7FULL);\n\n\tmcs_reg_write(mcs, MCSX_BBE_RX_SLAVE_BBE_INT_ENB, 0xFFULL);\n\tmcs_reg_write(mcs, MCSX_BBE_TX_SLAVE_BBE_INT_ENB, 0xFFULL);\n\n\tmcs_reg_write(mcs, MCSX_PAB_RX_SLAVE_PAB_INT_ENB, 0xFFFFFULL);\n\tmcs_reg_write(mcs, MCSX_PAB_TX_SLAVE_PAB_INT_ENB, 0xFFFFFULL);\n\n\tmcs->tx_sa_active = alloc_mem(mcs, mcs->hw->sc_entries);\n\tif (!mcs->tx_sa_active) {\n\t\tret = -ENOMEM;\n\t\tgoto free_irq;\n\t}\n\n\treturn ret;\n\nfree_irq:\n\tfree_irq(pci_irq_vector(mcs->pdev, mcs->hw->ip_vec), mcs);\nexit:\n\tpci_free_irq_vectors(mcs->pdev);\n\tmcs->num_vec = 0;\n\treturn ret;\n}\n\nint mcs_get_blkcnt(void)\n{\n\tstruct mcs *mcs;\n\tint idmax = -ENODEV;\n\n\t \n\tif (!pci_dev_present(mcs_id_table))\n\t\treturn 0;\n\n\tlist_for_each_entry(mcs, &mcs_list, mcs_list)\n\t\tif (mcs->mcs_id > idmax)\n\t\t\tidmax = mcs->mcs_id;\n\n\tif (idmax < 0)\n\t\treturn 0;\n\n\treturn idmax + 1;\n}\n\nstruct mcs *mcs_get_pdata(int mcs_id)\n{\n\tstruct mcs *mcs_dev;\n\n\tlist_for_each_entry(mcs_dev, &mcs_list, mcs_list) {\n\t\tif (mcs_dev->mcs_id == mcs_id)\n\t\t\treturn mcs_dev;\n\t}\n\treturn NULL;\n}\n\nbool is_mcs_bypass(int mcs_id)\n{\n\tstruct mcs *mcs_dev;\n\n\tlist_for_each_entry(mcs_dev, &mcs_list, mcs_list) {\n\t\tif (mcs_dev->mcs_id == mcs_id)\n\t\t\treturn mcs_dev->bypass;\n\t}\n\treturn true;\n}\n\nvoid mcs_set_port_cfg(struct mcs *mcs, struct mcs_port_cfg_set_req *req)\n{\n\tu64 val = 0;\n\n\tmcs_reg_write(mcs, MCSX_PAB_RX_SLAVE_PORT_CFGX(req->port_id),\n\t\t      req->port_mode & MCS_PORT_MODE_MASK);\n\n\treq->cstm_tag_rel_mode_sel &= 0x3;\n\n\tif (mcs->hw->mcs_blks > 1) {\n\t\treq->fifo_skid &= MCS_PORT_FIFO_SKID_MASK;\n\t\tval = (u32)req->fifo_skid << 0x10;\n\t\tval |= req->fifo_skid;\n\t\tmcs_reg_write(mcs, MCSX_PAB_RX_SLAVE_FIFO_SKID_CFGX(req->port_id), val);\n\t\tmcs_reg_write(mcs, MCSX_PEX_TX_SLAVE_CUSTOM_TAG_REL_MODE_SEL(req->port_id),\n\t\t\t      req->cstm_tag_rel_mode_sel);\n\t\tval = mcs_reg_read(mcs, MCSX_PEX_RX_SLAVE_PEX_CONFIGURATION);\n\n\t\tif (req->custom_hdr_enb)\n\t\t\tval |= BIT_ULL(req->port_id);\n\t\telse\n\t\t\tval &= ~BIT_ULL(req->port_id);\n\n\t\tmcs_reg_write(mcs, MCSX_PEX_RX_SLAVE_PEX_CONFIGURATION, val);\n\t} else {\n\t\tval = mcs_reg_read(mcs, MCSX_PEX_TX_SLAVE_PORT_CONFIG(req->port_id));\n\t\tval |= (req->cstm_tag_rel_mode_sel << 2);\n\t\tmcs_reg_write(mcs, MCSX_PEX_TX_SLAVE_PORT_CONFIG(req->port_id), val);\n\t}\n}\n\nvoid mcs_get_port_cfg(struct mcs *mcs, struct mcs_port_cfg_get_req *req,\n\t\t      struct mcs_port_cfg_get_rsp *rsp)\n{\n\tu64 reg = 0;\n\n\trsp->port_mode = mcs_reg_read(mcs, MCSX_PAB_RX_SLAVE_PORT_CFGX(req->port_id)) &\n\t\t\t MCS_PORT_MODE_MASK;\n\n\tif (mcs->hw->mcs_blks > 1) {\n\t\treg = MCSX_PAB_RX_SLAVE_FIFO_SKID_CFGX(req->port_id);\n\t\trsp->fifo_skid = mcs_reg_read(mcs, reg) & MCS_PORT_FIFO_SKID_MASK;\n\t\treg = MCSX_PEX_TX_SLAVE_CUSTOM_TAG_REL_MODE_SEL(req->port_id);\n\t\trsp->cstm_tag_rel_mode_sel = mcs_reg_read(mcs, reg) & 0x3;\n\t\tif (mcs_reg_read(mcs, MCSX_PEX_RX_SLAVE_PEX_CONFIGURATION) & BIT_ULL(req->port_id))\n\t\t\trsp->custom_hdr_enb = 1;\n\t} else {\n\t\treg = MCSX_PEX_TX_SLAVE_PORT_CONFIG(req->port_id);\n\t\trsp->cstm_tag_rel_mode_sel = mcs_reg_read(mcs, reg) >> 2;\n\t}\n\n\trsp->port_id = req->port_id;\n\trsp->mcs_id = req->mcs_id;\n}\n\nvoid mcs_get_custom_tag_cfg(struct mcs *mcs, struct mcs_custom_tag_cfg_get_req *req,\n\t\t\t    struct mcs_custom_tag_cfg_get_rsp *rsp)\n{\n\tu64 reg = 0, val = 0;\n\tu8 idx;\n\n\tfor (idx = 0; idx < MCS_MAX_CUSTOM_TAGS; idx++) {\n\t\tif (mcs->hw->mcs_blks > 1)\n\t\t\treg  = (req->dir == MCS_RX) ? MCSX_PEX_RX_SLAVE_CUSTOM_TAGX(idx) :\n\t\t\t\tMCSX_PEX_TX_SLAVE_CUSTOM_TAGX(idx);\n\t\telse\n\t\t\treg = (req->dir == MCS_RX) ? MCSX_PEX_RX_SLAVE_VLAN_CFGX(idx) :\n\t\t\t\tMCSX_PEX_TX_SLAVE_VLAN_CFGX(idx);\n\n\t\tval = mcs_reg_read(mcs, reg);\n\t\tif (mcs->hw->mcs_blks > 1) {\n\t\t\trsp->cstm_etype[idx] = val & GENMASK(15, 0);\n\t\t\trsp->cstm_indx[idx] = (val >> 0x16) & 0x3;\n\t\t\treg = (req->dir == MCS_RX) ? MCSX_PEX_RX_SLAVE_ETYPE_ENABLE :\n\t\t\t\tMCSX_PEX_TX_SLAVE_ETYPE_ENABLE;\n\t\t\trsp->cstm_etype_en = mcs_reg_read(mcs, reg) & 0xFF;\n\t\t} else {\n\t\t\trsp->cstm_etype[idx] = (val >> 0x1) & GENMASK(15, 0);\n\t\t\trsp->cstm_indx[idx] = (val >> 0x11) & 0x3;\n\t\t\trsp->cstm_etype_en |= (val & 0x1) << idx;\n\t\t}\n\t}\n\n\trsp->mcs_id = req->mcs_id;\n\trsp->dir = req->dir;\n}\n\nvoid mcs_reset_port(struct mcs *mcs, u8 port_id, u8 reset)\n{\n\tu64 reg = MCSX_MCS_TOP_SLAVE_PORT_RESET(port_id);\n\n\tmcs_reg_write(mcs, reg, reset & 0x1);\n}\n\n \nvoid mcs_set_lmac_mode(struct mcs *mcs, int lmac_id, u8 mode)\n{\n\tu64 reg;\n\tint id = lmac_id * 2;\n\n\treg = MCSX_MCS_TOP_SLAVE_CHANNEL_CFG(id);\n\tmcs_reg_write(mcs, reg, (u64)mode);\n\treg = MCSX_MCS_TOP_SLAVE_CHANNEL_CFG((id + 1));\n\tmcs_reg_write(mcs, reg, (u64)mode);\n}\n\nvoid mcs_pn_threshold_set(struct mcs *mcs, struct mcs_set_pn_threshold *pn)\n{\n\tu64 reg;\n\n\tif (pn->dir == MCS_RX)\n\t\treg = pn->xpn ? MCSX_CPM_RX_SLAVE_XPN_THRESHOLD : MCSX_CPM_RX_SLAVE_PN_THRESHOLD;\n\telse\n\t\treg = pn->xpn ? MCSX_CPM_TX_SLAVE_XPN_THRESHOLD : MCSX_CPM_TX_SLAVE_PN_THRESHOLD;\n\n\tmcs_reg_write(mcs, reg, pn->threshold);\n}\n\nvoid cn10kb_mcs_parser_cfg(struct mcs *mcs)\n{\n\tu64 reg, val;\n\n\t \n\tval = BIT_ULL(0) | (0x8100ull & 0xFFFF) << 1 | BIT_ULL(17);\n\t \n\treg = MCSX_PEX_RX_SLAVE_VLAN_CFGX(0);\n\tmcs_reg_write(mcs, reg, val);\n\n\t \n\treg = MCSX_PEX_TX_SLAVE_VLAN_CFGX(0);\n\tmcs_reg_write(mcs, reg, val);\n\n\t \n\tval = BIT_ULL(0) | (0x88a8ull & 0xFFFF) << 1 | BIT_ULL(18);\n\t \n\treg = MCSX_PEX_RX_SLAVE_VLAN_CFGX(1);\n\tmcs_reg_write(mcs, reg, val);\n\n\t \n\treg = MCSX_PEX_TX_SLAVE_VLAN_CFGX(1);\n\tmcs_reg_write(mcs, reg, val);\n}\n\nstatic void mcs_lmac_init(struct mcs *mcs, int lmac_id)\n{\n\tu64 reg;\n\n\t \n\treg = MCSX_PAB_RX_SLAVE_PORT_CFGX(lmac_id);\n\tmcs_reg_write(mcs, reg, 0);\n\n\tif (mcs->hw->mcs_blks > 1) {\n\t\treg = MCSX_PAB_RX_SLAVE_FIFO_SKID_CFGX(lmac_id);\n\t\tmcs_reg_write(mcs, reg, 0xe000e);\n\t\treturn;\n\t}\n\n\treg = MCSX_PAB_TX_SLAVE_PORT_CFGX(lmac_id);\n\tmcs_reg_write(mcs, reg, 0);\n}\n\nint mcs_set_lmac_channels(int mcs_id, u16 base)\n{\n\tstruct mcs *mcs;\n\tint lmac;\n\tu64 cfg;\n\n\tmcs = mcs_get_pdata(mcs_id);\n\tif (!mcs)\n\t\treturn -ENODEV;\n\tfor (lmac = 0; lmac < mcs->hw->lmac_cnt; lmac++) {\n\t\tcfg = mcs_reg_read(mcs, MCSX_LINK_LMACX_CFG(lmac));\n\t\tcfg &= ~(MCSX_LINK_LMAC_BASE_MASK | MCSX_LINK_LMAC_RANGE_MASK);\n\t\tcfg |=\tFIELD_PREP(MCSX_LINK_LMAC_RANGE_MASK, ilog2(16));\n\t\tcfg |=\tFIELD_PREP(MCSX_LINK_LMAC_BASE_MASK, base);\n\t\tmcs_reg_write(mcs, MCSX_LINK_LMACX_CFG(lmac), cfg);\n\t\tbase += 16;\n\t}\n\treturn 0;\n}\n\nstatic int mcs_x2p_calibration(struct mcs *mcs)\n{\n\tunsigned long timeout = jiffies + usecs_to_jiffies(20000);\n\tint i, err = 0;\n\tu64 val;\n\n\t \n\tval = mcs_reg_read(mcs, MCSX_MIL_GLOBAL);\n\tval |= BIT_ULL(5);\n\tmcs_reg_write(mcs, MCSX_MIL_GLOBAL, val);\n\n\t \n\twhile (!(mcs_reg_read(mcs, MCSX_MIL_RX_GBL_STATUS) & BIT_ULL(0))) {\n\t\tif (time_before(jiffies, timeout)) {\n\t\t\tusleep_range(80, 100);\n\t\t\tcontinue;\n\t\t} else {\n\t\t\terr = -EBUSY;\n\t\t\tdev_err(mcs->dev, \"MCS X2P calibration failed..ignoring\\n\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tval = mcs_reg_read(mcs, MCSX_MIL_RX_GBL_STATUS);\n\tfor (i = 0; i < mcs->hw->mcs_x2p_intf; i++) {\n\t\tif (val & BIT_ULL(1 + i))\n\t\t\tcontinue;\n\t\terr = -EBUSY;\n\t\tdev_err(mcs->dev, \"MCS:%d didn't respond to X2P calibration\\n\", i);\n\t}\n\t \n\tmcs_reg_write(mcs, MCSX_MIL_GLOBAL, mcs_reg_read(mcs, MCSX_MIL_GLOBAL) & ~BIT_ULL(5));\n\n\treturn err;\n}\n\nstatic void mcs_set_external_bypass(struct mcs *mcs, bool bypass)\n{\n\tu64 val;\n\n\t \n\tval = mcs_reg_read(mcs, MCSX_MIL_GLOBAL);\n\tif (bypass)\n\t\tval |= BIT_ULL(6);\n\telse\n\t\tval &= ~BIT_ULL(6);\n\tmcs_reg_write(mcs, MCSX_MIL_GLOBAL, val);\n\tmcs->bypass = bypass;\n}\n\nstatic void mcs_global_cfg(struct mcs *mcs)\n{\n\t \n\tmcs_set_external_bypass(mcs, false);\n\n\t \n\tmcs_reg_write(mcs, MCSX_CSE_RX_SLAVE_STATS_CLEAR, 0x1F);\n\tmcs_reg_write(mcs, MCSX_CSE_TX_SLAVE_STATS_CLEAR, 0x1F);\n\n\t \n\tif (mcs->hw->mcs_blks == 1) {\n\t\tmcs_reg_write(mcs, MCSX_IP_MODE, BIT_ULL(3));\n\t\treturn;\n\t}\n\n\tmcs_reg_write(mcs, MCSX_BBE_RX_SLAVE_CAL_ENTRY, 0xe4);\n\tmcs_reg_write(mcs, MCSX_BBE_RX_SLAVE_CAL_LEN, 4);\n}\n\nvoid cn10kb_mcs_set_hw_capabilities(struct mcs *mcs)\n{\n\tstruct hwinfo *hw = mcs->hw;\n\n\thw->tcam_entries = 128;\t\t \n\thw->secy_entries  = 128;\t \n\thw->sc_entries = 128;\t\t \n\thw->sa_entries = 256;\t\t \n\thw->lmac_cnt = 20;\t\t \n\thw->mcs_x2p_intf = 5;\t\t \n\thw->mcs_blks = 1;\t\t \n\thw->ip_vec = MCS_CN10KB_INT_VEC_IP;  \n}\n\nstatic struct mcs_ops cn10kb_mcs_ops = {\n\t.mcs_set_hw_capabilities\t= cn10kb_mcs_set_hw_capabilities,\n\t.mcs_parser_cfg\t\t\t= cn10kb_mcs_parser_cfg,\n\t.mcs_tx_sa_mem_map_write\t= cn10kb_mcs_tx_sa_mem_map_write,\n\t.mcs_rx_sa_mem_map_write\t= cn10kb_mcs_rx_sa_mem_map_write,\n\t.mcs_flowid_secy_map\t\t= cn10kb_mcs_flowid_secy_map,\n\t.mcs_bbe_intr_handler\t\t= cn10kb_mcs_bbe_intr_handler,\n\t.mcs_pab_intr_handler\t\t= cn10kb_mcs_pab_intr_handler,\n};\n\nstatic int mcs_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct device *dev = &pdev->dev;\n\tint lmac, err = 0;\n\tstruct mcs *mcs;\n\n\tmcs = devm_kzalloc(dev, sizeof(*mcs), GFP_KERNEL);\n\tif (!mcs)\n\t\treturn -ENOMEM;\n\n\tmcs->hw = devm_kzalloc(dev, sizeof(struct hwinfo), GFP_KERNEL);\n\tif (!mcs->hw)\n\t\treturn -ENOMEM;\n\n\terr = pci_enable_device(pdev);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to enable PCI device\\n\");\n\t\tpci_set_drvdata(pdev, NULL);\n\t\treturn err;\n\t}\n\n\terr = pci_request_regions(pdev, DRV_NAME);\n\tif (err) {\n\t\tdev_err(dev, \"PCI request regions failed 0x%x\\n\", err);\n\t\tgoto exit;\n\t}\n\n\tmcs->reg_base = pcim_iomap(pdev, PCI_CFG_REG_BAR_NUM, 0);\n\tif (!mcs->reg_base) {\n\t\tdev_err(dev, \"mcs: Cannot map CSR memory space, aborting\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\tpci_set_drvdata(pdev, mcs);\n\tmcs->pdev = pdev;\n\tmcs->dev = &pdev->dev;\n\n\tif (pdev->subsystem_device == PCI_SUBSYS_DEVID_CN10K_B)\n\t\tmcs->mcs_ops = &cn10kb_mcs_ops;\n\telse\n\t\tmcs->mcs_ops = cnf10kb_get_mac_ops();\n\n\t \n\tmcs->mcs_ops->mcs_set_hw_capabilities(mcs);\n\n\tmcs_global_cfg(mcs);\n\n\t \n\terr = mcs_x2p_calibration(mcs);\n\tif (err)\n\t\tgoto err_x2p;\n\n\tmcs->mcs_id = (pci_resource_start(pdev, PCI_CFG_REG_BAR_NUM) >> 24)\n\t\t\t& MCS_ID_MASK;\n\n\t \n\terr = mcs_alloc_struct_mem(mcs, &mcs->tx);\n\tif (err)\n\t\tgoto err_x2p;\n\n\t \n\terr = mcs_alloc_struct_mem(mcs, &mcs->rx);\n\tif (err)\n\t\tgoto err_x2p;\n\n\t \n\tfor (lmac = 0; lmac < mcs->hw->lmac_cnt; lmac++)\n\t\tmcs_lmac_init(mcs, lmac);\n\n\t \n\tmcs->mcs_ops->mcs_parser_cfg(mcs);\n\n\terr = mcs_register_interrupts(mcs);\n\tif (err)\n\t\tgoto exit;\n\n\tlist_add(&mcs->mcs_list, &mcs_list);\n\tmutex_init(&mcs->stats_lock);\n\n\treturn 0;\n\nerr_x2p:\n\t \n\tmcs_set_external_bypass(mcs, true);\nexit:\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tpci_set_drvdata(pdev, NULL);\n\treturn err;\n}\n\nstatic void mcs_remove(struct pci_dev *pdev)\n{\n\tstruct mcs *mcs = pci_get_drvdata(pdev);\n\n\t \n\tmcs_set_external_bypass(mcs, true);\n\tfree_irq(pci_irq_vector(pdev, mcs->hw->ip_vec), mcs);\n\tpci_free_irq_vectors(pdev);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n\tpci_set_drvdata(pdev, NULL);\n}\n\nstruct pci_driver mcs_driver = {\n\t.name = DRV_NAME,\n\t.id_table = mcs_id_table,\n\t.probe = mcs_probe,\n\t.remove = mcs_remove,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}