{
  "module_name": "rvu_nix.c",
  "hash_id": "391de7308c84af7eac38edb22c3c0b1dcecd70b90a9ee5a135deaccd7ae1a177",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/marvell/octeontx2/af/rvu_nix.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include \"rvu_struct.h\"\n#include \"rvu_reg.h\"\n#include \"rvu.h\"\n#include \"npc.h\"\n#include \"mcs.h\"\n#include \"cgx.h\"\n#include \"lmac_common.h\"\n#include \"rvu_npc_hash.h\"\n\nstatic void nix_free_tx_vtag_entries(struct rvu *rvu, u16 pcifunc);\nstatic int rvu_nix_get_bpid(struct rvu *rvu, struct nix_bp_cfg_req *req,\n\t\t\t    int type, int chan_id);\nstatic int nix_update_mce_rule(struct rvu *rvu, u16 pcifunc,\n\t\t\t       int type, bool add);\nstatic int nix_setup_ipolicers(struct rvu *rvu,\n\t\t\t       struct nix_hw *nix_hw, int blkaddr);\nstatic void nix_ipolicer_freemem(struct rvu *rvu, struct nix_hw *nix_hw);\nstatic int nix_verify_bandprof(struct nix_cn10k_aq_enq_req *req,\n\t\t\t       struct nix_hw *nix_hw, u16 pcifunc);\nstatic int nix_free_all_bandprof(struct rvu *rvu, u16 pcifunc);\nstatic void nix_clear_ratelimit_aggr(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t\t     u32 leaf_prof);\nstatic const char *nix_get_ctx_name(int ctype);\n\nenum mc_tbl_sz {\n\tMC_TBL_SZ_256,\n\tMC_TBL_SZ_512,\n\tMC_TBL_SZ_1K,\n\tMC_TBL_SZ_2K,\n\tMC_TBL_SZ_4K,\n\tMC_TBL_SZ_8K,\n\tMC_TBL_SZ_16K,\n\tMC_TBL_SZ_32K,\n\tMC_TBL_SZ_64K,\n};\n\nenum mc_buf_cnt {\n\tMC_BUF_CNT_8,\n\tMC_BUF_CNT_16,\n\tMC_BUF_CNT_32,\n\tMC_BUF_CNT_64,\n\tMC_BUF_CNT_128,\n\tMC_BUF_CNT_256,\n\tMC_BUF_CNT_512,\n\tMC_BUF_CNT_1024,\n\tMC_BUF_CNT_2048,\n};\n\nenum nix_makr_fmt_indexes {\n\tNIX_MARK_CFG_IP_DSCP_RED,\n\tNIX_MARK_CFG_IP_DSCP_YELLOW,\n\tNIX_MARK_CFG_IP_DSCP_YELLOW_RED,\n\tNIX_MARK_CFG_IP_ECN_RED,\n\tNIX_MARK_CFG_IP_ECN_YELLOW,\n\tNIX_MARK_CFG_IP_ECN_YELLOW_RED,\n\tNIX_MARK_CFG_VLAN_DEI_RED,\n\tNIX_MARK_CFG_VLAN_DEI_YELLOW,\n\tNIX_MARK_CFG_VLAN_DEI_YELLOW_RED,\n\tNIX_MARK_CFG_MAX,\n};\n\n \n#define MC_TBL_SIZE\tMC_TBL_SZ_512\n#define MC_BUF_CNT\tMC_BUF_CNT_128\n\nstruct mce {\n\tstruct hlist_node\tnode;\n\tu16\t\t\tpcifunc;\n};\n\nint rvu_get_next_nix_blkaddr(struct rvu *rvu, int blkaddr)\n{\n\tint i = 0;\n\n\t \n\tif (blkaddr == 0)\n\t\treturn rvu->nix_blkaddr[blkaddr];\n\n\twhile (i + 1 < MAX_NIX_BLKS) {\n\t\tif (rvu->nix_blkaddr[i] == blkaddr)\n\t\t\treturn rvu->nix_blkaddr[i + 1];\n\t\ti++;\n\t}\n\n\treturn 0;\n}\n\nbool is_nixlf_attached(struct rvu *rvu, u16 pcifunc)\n{\n\tstruct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, pcifunc);\n\tint blkaddr;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (!pfvf->nixlf || blkaddr < 0)\n\t\treturn false;\n\treturn true;\n}\n\nint rvu_get_nixlf_count(struct rvu *rvu)\n{\n\tint blkaddr = 0, max = 0;\n\tstruct rvu_block *block;\n\n\tblkaddr = rvu_get_next_nix_blkaddr(rvu, blkaddr);\n\twhile (blkaddr) {\n\t\tblock = &rvu->hw->block[blkaddr];\n\t\tmax += block->lf.max;\n\t\tblkaddr = rvu_get_next_nix_blkaddr(rvu, blkaddr);\n\t}\n\treturn max;\n}\n\nint nix_get_nixlf(struct rvu *rvu, u16 pcifunc, int *nixlf, int *nix_blkaddr)\n{\n\tstruct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, pcifunc);\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint blkaddr;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (!pfvf->nixlf || blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\t*nixlf = rvu_get_lf(rvu, &hw->block[blkaddr], pcifunc, 0);\n\tif (*nixlf < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tif (nix_blkaddr)\n\t\t*nix_blkaddr = blkaddr;\n\n\treturn 0;\n}\n\nint nix_get_struct_ptrs(struct rvu *rvu, u16 pcifunc,\n\t\t\tstruct nix_hw **nix_hw, int *blkaddr)\n{\n\tstruct rvu_pfvf *pfvf;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\t*blkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (!pfvf->nixlf || *blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\t*nix_hw = get_nix_hw(rvu->hw, *blkaddr);\n\tif (!*nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\treturn 0;\n}\n\nstatic void nix_mce_list_init(struct nix_mce_list *list, int max)\n{\n\tINIT_HLIST_HEAD(&list->head);\n\tlist->count = 0;\n\tlist->max = max;\n}\n\nstatic u16 nix_alloc_mce_list(struct nix_mcast *mcast, int count)\n{\n\tint idx;\n\n\tif (!mcast)\n\t\treturn 0;\n\n\tidx = mcast->next_free_mce;\n\tmcast->next_free_mce += count;\n\treturn idx;\n}\n\nstruct nix_hw *get_nix_hw(struct rvu_hwinfo *hw, int blkaddr)\n{\n\tint nix_blkaddr = 0, i = 0;\n\tstruct rvu *rvu = hw->rvu;\n\n\tnix_blkaddr = rvu_get_next_nix_blkaddr(rvu, nix_blkaddr);\n\twhile (nix_blkaddr) {\n\t\tif (blkaddr == nix_blkaddr && hw->nix)\n\t\t\treturn &hw->nix[i];\n\t\tnix_blkaddr = rvu_get_next_nix_blkaddr(rvu, nix_blkaddr);\n\t\ti++;\n\t}\n\treturn NULL;\n}\n\nint nix_get_dwrr_mtu_reg(struct rvu_hwinfo *hw, int smq_link_type)\n{\n\tif (hw->cap.nix_multiple_dwrr_mtu)\n\t\treturn NIX_AF_DWRR_MTUX(smq_link_type);\n\n\tif (smq_link_type == SMQ_LINK_TYPE_SDP)\n\t\treturn NIX_AF_DWRR_SDP_MTU;\n\n\t \n\treturn NIX_AF_DWRR_RPM_MTU;\n}\n\nu32 convert_dwrr_mtu_to_bytes(u8 dwrr_mtu)\n{\n\tdwrr_mtu &= 0x1FULL;\n\n\t \n\tswitch (dwrr_mtu) {\n\tcase 4:\n\t\treturn 9728;\n\tcase 5:\n\t\treturn 10240;\n\tdefault:\n\t\treturn BIT_ULL(dwrr_mtu);\n\t}\n\n\treturn 0;\n}\n\nu32 convert_bytes_to_dwrr_mtu(u32 bytes)\n{\n\t \n\tif (bytes > BIT_ULL(16))\n\t\treturn 0;\n\n\tswitch (bytes) {\n\tcase 9728:\n\t\treturn 4;\n\tcase 10240:\n\t\treturn 5;\n\tdefault:\n\t\treturn ilog2(bytes);\n\t}\n\n\treturn 0;\n}\n\nstatic void nix_rx_sync(struct rvu *rvu, int blkaddr)\n{\n\tint err;\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_RX_SW_SYNC, BIT_ULL(0));\n\terr = rvu_poll_reg(rvu, blkaddr, NIX_AF_RX_SW_SYNC, BIT_ULL(0), true);\n\tif (err)\n\t\tdev_err(rvu->dev, \"SYNC1: NIX RX software sync failed\\n\");\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_RX_SW_SYNC, BIT_ULL(0));\n\terr = rvu_poll_reg(rvu, blkaddr, NIX_AF_RX_SW_SYNC, BIT_ULL(0), true);\n\tif (err)\n\t\tdev_err(rvu->dev, \"SYNC2: NIX RX software sync failed\\n\");\n}\n\nstatic bool is_valid_txschq(struct rvu *rvu, int blkaddr,\n\t\t\t    int lvl, u16 pcifunc, u16 schq)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct nix_txsch *txsch;\n\tstruct nix_hw *nix_hw;\n\tu16 map_func;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn false;\n\n\ttxsch = &nix_hw->txsch[lvl];\n\t \n\tif (schq >= txsch->schq.max)\n\t\treturn false;\n\n\tmutex_lock(&rvu->rsrc_lock);\n\tmap_func = TXSCH_MAP_FUNC(txsch->pfvf_map[schq]);\n\tmutex_unlock(&rvu->rsrc_lock);\n\n\t \n\tif (lvl >= hw->cap.nix_tx_aggr_lvl) {\n\t\tif (rvu_get_pf(map_func) != rvu_get_pf(pcifunc))\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\t}\n\n\tif (map_func != pcifunc)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int nix_interface_init(struct rvu *rvu, u16 pcifunc, int type, int nixlf,\n\t\t\t      struct nix_lf_alloc_rsp *rsp, bool loop)\n{\n\tstruct rvu_pfvf *parent_pf, *pfvf = rvu_get_pfvf(rvu, pcifunc);\n\tu16 req_chan_base, req_chan_end, req_chan_cnt;\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct sdp_node_info *sdp_info;\n\tint pkind, pf, vf, lbkid, vfid;\n\tu8 cgx_id, lmac_id;\n\tbool from_vf;\n\tint err;\n\n\tpf = rvu_get_pf(pcifunc);\n\tif (!is_pf_cgxmapped(rvu, pf) && type != NIX_INTF_TYPE_LBK &&\n\t    type != NIX_INTF_TYPE_SDP)\n\t\treturn 0;\n\n\tswitch (type) {\n\tcase NIX_INTF_TYPE_CGX:\n\t\tpfvf->cgx_lmac = rvu->pf2cgxlmac_map[pf];\n\t\trvu_get_cgx_lmac_id(pfvf->cgx_lmac, &cgx_id, &lmac_id);\n\n\t\tpkind = rvu_npc_get_pkind(rvu, pf);\n\t\tif (pkind < 0) {\n\t\t\tdev_err(rvu->dev,\n\t\t\t\t\"PF_Func 0x%x: Invalid pkind\\n\", pcifunc);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tpfvf->rx_chan_base = rvu_nix_chan_cgx(rvu, cgx_id, lmac_id, 0);\n\t\tpfvf->tx_chan_base = pfvf->rx_chan_base;\n\t\tpfvf->rx_chan_cnt = 1;\n\t\tpfvf->tx_chan_cnt = 1;\n\t\trsp->tx_link = cgx_id * hw->lmac_per_cgx + lmac_id;\n\n\t\tcgx_set_pkind(rvu_cgx_pdata(cgx_id, rvu), lmac_id, pkind);\n\t\trvu_npc_set_pkind(rvu, pkind, pfvf);\n\n\t\tbreak;\n\tcase NIX_INTF_TYPE_LBK:\n\t\tvf = (pcifunc & RVU_PFVF_FUNC_MASK) - 1;\n\n\t\t \n\t\tlbkid = 0;\n\t\tif (rvu->hw->lbk_links > 1)\n\t\t\tlbkid = vf & 0x1 ? 0 : 1;\n\n\t\t \n\t\tif (loop)\n\t\t\tlbkid = !lbkid;\n\n\t\t \n\t\tpfvf->rx_chan_base = rvu_nix_chan_lbk(rvu, lbkid, vf);\n\t\tpfvf->tx_chan_base = vf & 0x1 ?\n\t\t\t\t\trvu_nix_chan_lbk(rvu, lbkid, vf - 1) :\n\t\t\t\t\trvu_nix_chan_lbk(rvu, lbkid, vf + 1);\n\t\tpfvf->rx_chan_cnt = 1;\n\t\tpfvf->tx_chan_cnt = 1;\n\t\trsp->tx_link = hw->cgx_links + lbkid;\n\t\tpfvf->lbkid = lbkid;\n\t\trvu_npc_set_pkind(rvu, NPC_RX_LBK_PKIND, pfvf);\n\t\trvu_npc_install_promisc_entry(rvu, pcifunc, nixlf,\n\t\t\t\t\t      pfvf->rx_chan_base,\n\t\t\t\t\t      pfvf->rx_chan_cnt);\n\n\t\tbreak;\n\tcase NIX_INTF_TYPE_SDP:\n\t\tfrom_vf = !!(pcifunc & RVU_PFVF_FUNC_MASK);\n\t\tparent_pf = &rvu->pf[rvu_get_pf(pcifunc)];\n\t\tsdp_info = parent_pf->sdp_info;\n\t\tif (!sdp_info) {\n\t\t\tdev_err(rvu->dev, \"Invalid sdp_info pointer\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (from_vf) {\n\t\t\treq_chan_base = rvu_nix_chan_sdp(rvu, 0) + sdp_info->pf_srn +\n\t\t\t\tsdp_info->num_pf_rings;\n\t\t\tvf = (pcifunc & RVU_PFVF_FUNC_MASK) - 1;\n\t\t\tfor (vfid = 0; vfid < vf; vfid++)\n\t\t\t\treq_chan_base += sdp_info->vf_rings[vfid];\n\t\t\treq_chan_cnt = sdp_info->vf_rings[vf];\n\t\t\treq_chan_end = req_chan_base + req_chan_cnt - 1;\n\t\t\tif (req_chan_base < rvu_nix_chan_sdp(rvu, 0) ||\n\t\t\t    req_chan_end > rvu_nix_chan_sdp(rvu, 255)) {\n\t\t\t\tdev_err(rvu->dev,\n\t\t\t\t\t\"PF_Func 0x%x: Invalid channel base and count\\n\",\n\t\t\t\t\tpcifunc);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\treq_chan_base = rvu_nix_chan_sdp(rvu, 0) + sdp_info->pf_srn;\n\t\t\treq_chan_cnt = sdp_info->num_pf_rings;\n\t\t}\n\n\t\tpfvf->rx_chan_base = req_chan_base;\n\t\tpfvf->rx_chan_cnt = req_chan_cnt;\n\t\tpfvf->tx_chan_base = pfvf->rx_chan_base;\n\t\tpfvf->tx_chan_cnt = pfvf->rx_chan_cnt;\n\n\t\trsp->tx_link = hw->cgx_links + hw->lbk_links;\n\t\trvu_npc_install_promisc_entry(rvu, pcifunc, nixlf,\n\t\t\t\t\t      pfvf->rx_chan_base,\n\t\t\t\t\t      pfvf->rx_chan_cnt);\n\t\tbreak;\n\t}\n\n\t \n\trvu_npc_install_ucast_entry(rvu, pcifunc, nixlf,\n\t\t\t\t    pfvf->rx_chan_base, pfvf->mac_addr);\n\n\t \n\terr = nix_update_mce_rule(rvu, pcifunc, NIXLF_BCAST_ENTRY, true);\n\tif (err) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"Bcast list, failed to enable PF_FUNC 0x%x\\n\",\n\t\t\tpcifunc);\n\t\treturn err;\n\t}\n\t \n\trvu_npc_install_bcast_match_entry(rvu, pcifunc,\n\t\t\t\t\t  nixlf, pfvf->rx_chan_base);\n\n\tpfvf->maxlen = NIC_HW_MIN_FRS;\n\tpfvf->minlen = NIC_HW_MIN_FRS;\n\n\treturn 0;\n}\n\nstatic void nix_interface_deinit(struct rvu *rvu, u16 pcifunc, u8 nixlf)\n{\n\tstruct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, pcifunc);\n\tint err;\n\n\tpfvf->maxlen = 0;\n\tpfvf->minlen = 0;\n\n\t \n\terr = nix_update_mce_rule(rvu, pcifunc, NIXLF_BCAST_ENTRY, false);\n\tif (err) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"Bcast list, failed to disable PF_FUNC 0x%x\\n\",\n\t\t\tpcifunc);\n\t}\n\n\t \n\trvu_npc_disable_mcam_entries(rvu, pcifunc, nixlf);\n\n\t \n\trvu_cgx_disable_dmac_entries(rvu, pcifunc);\n}\n\nint rvu_mbox_handler_nix_bp_disable(struct rvu *rvu,\n\t\t\t\t    struct nix_bp_cfg_req *req,\n\t\t\t\t    struct msg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct rvu_pfvf *pfvf;\n\tint blkaddr, pf, type;\n\tu16 chan_base, chan;\n\tu64 cfg;\n\n\tpf = rvu_get_pf(pcifunc);\n\ttype = is_afvf(pcifunc) ? NIX_INTF_TYPE_LBK : NIX_INTF_TYPE_CGX;\n\tif (!is_pf_cgxmapped(rvu, pf) && type != NIX_INTF_TYPE_LBK)\n\t\treturn 0;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\n\tchan_base = pfvf->rx_chan_base + req->chan_base;\n\tfor (chan = chan_base; chan < (chan_base + req->chan_cnt); chan++) {\n\t\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_RX_CHANX_CFG(chan));\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_CHANX_CFG(chan),\n\t\t\t    cfg & ~BIT_ULL(16));\n\t}\n\treturn 0;\n}\n\nstatic int rvu_nix_get_bpid(struct rvu *rvu, struct nix_bp_cfg_req *req,\n\t\t\t    int type, int chan_id)\n{\n\tint bpid, blkaddr, lmac_chan_cnt, sdp_chan_cnt;\n\tu16 cgx_bpid_cnt, lbk_bpid_cnt, sdp_bpid_cnt;\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct rvu_pfvf *pfvf;\n\tu8 cgx_id, lmac_id;\n\tu64 cfg;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, req->hdr.pcifunc);\n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_CONST);\n\tlmac_chan_cnt = cfg & 0xFF;\n\n\tcgx_bpid_cnt = hw->cgx_links * lmac_chan_cnt;\n\tlbk_bpid_cnt = hw->lbk_links * ((cfg >> 16) & 0xFF);\n\n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_CONST1);\n\tsdp_chan_cnt = cfg & 0xFFF;\n\tsdp_bpid_cnt = hw->sdp_links * sdp_chan_cnt;\n\n\tpfvf = rvu_get_pfvf(rvu, req->hdr.pcifunc);\n\n\t \n\tswitch (type) {\n\tcase NIX_INTF_TYPE_CGX:\n\t\tif ((req->chan_base + req->chan_cnt) > 16)\n\t\t\treturn -EINVAL;\n\t\trvu_get_cgx_lmac_id(pfvf->cgx_lmac, &cgx_id, &lmac_id);\n\t\t \n\t\tbpid = (cgx_id * hw->lmac_per_cgx * lmac_chan_cnt) +\n\t\t\t(lmac_id * lmac_chan_cnt) + req->chan_base;\n\n\t\tif (req->bpid_per_chan)\n\t\t\tbpid += chan_id;\n\t\tif (bpid > cgx_bpid_cnt)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tcase NIX_INTF_TYPE_LBK:\n\t\tif ((req->chan_base + req->chan_cnt) > 63)\n\t\t\treturn -EINVAL;\n\t\tbpid = cgx_bpid_cnt + req->chan_base;\n\t\tif (req->bpid_per_chan)\n\t\t\tbpid += chan_id;\n\t\tif (bpid > (cgx_bpid_cnt + lbk_bpid_cnt))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase NIX_INTF_TYPE_SDP:\n\t\tif ((req->chan_base + req->chan_cnt) > 255)\n\t\t\treturn -EINVAL;\n\n\t\tbpid = sdp_bpid_cnt + req->chan_base;\n\t\tif (req->bpid_per_chan)\n\t\t\tbpid += chan_id;\n\n\t\tif (bpid > (cgx_bpid_cnt + lbk_bpid_cnt + sdp_bpid_cnt))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn bpid;\n}\n\nint rvu_mbox_handler_nix_bp_enable(struct rvu *rvu,\n\t\t\t\t   struct nix_bp_cfg_req *req,\n\t\t\t\t   struct nix_bp_cfg_rsp *rsp)\n{\n\tint blkaddr, pf, type, chan_id = 0;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct rvu_pfvf *pfvf;\n\tu16 chan_base, chan;\n\ts16 bpid, bpid_base;\n\tu64 cfg;\n\n\tpf = rvu_get_pf(pcifunc);\n\ttype = is_afvf(pcifunc) ? NIX_INTF_TYPE_LBK : NIX_INTF_TYPE_CGX;\n\tif (is_sdp_pfvf(pcifunc))\n\t\ttype = NIX_INTF_TYPE_SDP;\n\n\t \n\tif (!is_pf_cgxmapped(rvu, pf) && type != NIX_INTF_TYPE_LBK &&\n\t    type != NIX_INTF_TYPE_SDP)\n\t\treturn 0;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\n\tbpid_base = rvu_nix_get_bpid(rvu, req, type, chan_id);\n\tchan_base = pfvf->rx_chan_base + req->chan_base;\n\tbpid = bpid_base;\n\n\tfor (chan = chan_base; chan < (chan_base + req->chan_cnt); chan++) {\n\t\tif (bpid < 0) {\n\t\t\tdev_warn(rvu->dev, \"Fail to enable backpressure\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_RX_CHANX_CFG(chan));\n\t\tcfg &= ~GENMASK_ULL(8, 0);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_CHANX_CFG(chan),\n\t\t\t    cfg | (bpid & GENMASK_ULL(8, 0)) | BIT_ULL(16));\n\t\tchan_id++;\n\t\tbpid = rvu_nix_get_bpid(rvu, req, type, chan_id);\n\t}\n\n\tfor (chan = 0; chan < req->chan_cnt; chan++) {\n\t\t \n\t\trsp->chan_bpid[chan] = ((req->chan_base + chan) & 0x7F) << 10 |\n\t\t\t\t\t(bpid_base & 0x3FF);\n\t\tif (req->bpid_per_chan)\n\t\t\tbpid_base++;\n\t}\n\trsp->chan_cnt = req->chan_cnt;\n\n\treturn 0;\n}\n\nstatic void nix_setup_lso_tso_l3(struct rvu *rvu, int blkaddr,\n\t\t\t\t u64 format, bool v4, u64 *fidx)\n{\n\tstruct nix_lso_format field = {0};\n\n\t \n\tfield.layer = NIX_TXLAYER_OL3;\n\t \n\tfield.offset = v4 ? 2 : 4;\n\tfield.sizem1 = 1;  \n\tfield.alg = NIX_LSOALG_ADD_PAYLEN;\n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_LSO_FORMATX_FIELDX(format, (*fidx)++),\n\t\t    *(u64 *)&field);\n\n\t \n\tif (!v4)\n\t\treturn;\n\n\t \n\tfield.layer = NIX_TXLAYER_OL3;\n\tfield.offset = 4;\n\tfield.sizem1 = 1;  \n\tfield.alg = NIX_LSOALG_ADD_SEGNUM;\n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_LSO_FORMATX_FIELDX(format, (*fidx)++),\n\t\t    *(u64 *)&field);\n}\n\nstatic void nix_setup_lso_tso_l4(struct rvu *rvu, int blkaddr,\n\t\t\t\t u64 format, u64 *fidx)\n{\n\tstruct nix_lso_format field = {0};\n\n\t \n\tfield.layer = NIX_TXLAYER_OL4;\n\tfield.offset = 4;\n\tfield.sizem1 = 3;  \n\tfield.alg = NIX_LSOALG_ADD_OFFSET;\n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_LSO_FORMATX_FIELDX(format, (*fidx)++),\n\t\t    *(u64 *)&field);\n\n\t \n\tfield.layer = NIX_TXLAYER_OL4;\n\tfield.offset = 12;\n\tfield.sizem1 = 1;  \n\tfield.alg = NIX_LSOALG_TCP_FLAGS;\n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_LSO_FORMATX_FIELDX(format, (*fidx)++),\n\t\t    *(u64 *)&field);\n}\n\nstatic void nix_setup_lso(struct rvu *rvu, struct nix_hw *nix_hw, int blkaddr)\n{\n\tu64 cfg, idx, fidx = 0;\n\n\t \n\tcfg = (rvu_read64(rvu, blkaddr, NIX_AF_CONST1) >> 48) & 0xFF;\n\tnix_hw->lso.total = cfg;\n\n\t \n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_LSO_CFG);\n\t \n\tcfg &= ~((0xFFFFULL << 32) | (0xFFFFULL << 16));\n\tcfg |= (0xFFF2ULL << 32) | (0xFFF2ULL << 16);\n\trvu_write64(rvu, blkaddr, NIX_AF_LSO_CFG, cfg | BIT_ULL(63));\n\n\t \n\tidx = NIX_LSO_FORMAT_IDX_TSOV4;\n\tnix_setup_lso_tso_l3(rvu, blkaddr, idx, true, &fidx);\n\tnix_setup_lso_tso_l4(rvu, blkaddr, idx, &fidx);\n\n\t \n\tfor (; fidx < 8; fidx++) {\n\t\trvu_write64(rvu, blkaddr,\n\t\t\t    NIX_AF_LSO_FORMATX_FIELDX(idx, fidx), 0x0ULL);\n\t}\n\tnix_hw->lso.in_use++;\n\n\t \n\tidx = NIX_LSO_FORMAT_IDX_TSOV6;\n\tfidx = 0;\n\tnix_setup_lso_tso_l3(rvu, blkaddr, idx, false, &fidx);\n\tnix_setup_lso_tso_l4(rvu, blkaddr, idx, &fidx);\n\n\t \n\tfor (; fidx < 8; fidx++) {\n\t\trvu_write64(rvu, blkaddr,\n\t\t\t    NIX_AF_LSO_FORMATX_FIELDX(idx, fidx), 0x0ULL);\n\t}\n\tnix_hw->lso.in_use++;\n}\n\nstatic void nix_ctx_free(struct rvu *rvu, struct rvu_pfvf *pfvf)\n{\n\tkfree(pfvf->rq_bmap);\n\tkfree(pfvf->sq_bmap);\n\tkfree(pfvf->cq_bmap);\n\tif (pfvf->rq_ctx)\n\t\tqmem_free(rvu->dev, pfvf->rq_ctx);\n\tif (pfvf->sq_ctx)\n\t\tqmem_free(rvu->dev, pfvf->sq_ctx);\n\tif (pfvf->cq_ctx)\n\t\tqmem_free(rvu->dev, pfvf->cq_ctx);\n\tif (pfvf->rss_ctx)\n\t\tqmem_free(rvu->dev, pfvf->rss_ctx);\n\tif (pfvf->nix_qints_ctx)\n\t\tqmem_free(rvu->dev, pfvf->nix_qints_ctx);\n\tif (pfvf->cq_ints_ctx)\n\t\tqmem_free(rvu->dev, pfvf->cq_ints_ctx);\n\n\tpfvf->rq_bmap = NULL;\n\tpfvf->cq_bmap = NULL;\n\tpfvf->sq_bmap = NULL;\n\tpfvf->rq_ctx = NULL;\n\tpfvf->sq_ctx = NULL;\n\tpfvf->cq_ctx = NULL;\n\tpfvf->rss_ctx = NULL;\n\tpfvf->nix_qints_ctx = NULL;\n\tpfvf->cq_ints_ctx = NULL;\n}\n\nstatic int nixlf_rss_ctx_init(struct rvu *rvu, int blkaddr,\n\t\t\t      struct rvu_pfvf *pfvf, int nixlf,\n\t\t\t      int rss_sz, int rss_grps, int hwctx_size,\n\t\t\t      u64 way_mask, bool tag_lsb_as_adder)\n{\n\tint err, grp, num_indices;\n\tu64 val;\n\n\t \n\tif (!rss_sz)\n\t\treturn 0;\n\tnum_indices = rss_sz * rss_grps;\n\n\t \n\terr = qmem_alloc(rvu->dev, &pfvf->rss_ctx, num_indices, hwctx_size);\n\tif (err)\n\t\treturn err;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RSS_BASE(nixlf),\n\t\t    (u64)pfvf->rss_ctx->iova);\n\n\t \n\tval = BIT_ULL(36) | BIT_ULL(4) | way_mask << 20 |\n\t\t\tilog2(num_indices / MAX_RSS_INDIR_TBL_SIZE);\n\n\tif (tag_lsb_as_adder)\n\t\tval |= BIT_ULL(5);\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RSS_CFG(nixlf), val);\n\t \n\tfor (grp = 0; grp < rss_grps; grp++)\n\t\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RSS_GRPX(nixlf, grp),\n\t\t\t    ((ilog2(rss_sz) - 1) << 16) | (rss_sz * grp));\n\treturn 0;\n}\n\nstatic int nix_aq_enqueue_wait(struct rvu *rvu, struct rvu_block *block,\n\t\t\t       struct nix_aq_inst_s *inst)\n{\n\tstruct admin_queue *aq = block->aq;\n\tstruct nix_aq_res_s *result;\n\tint timeout = 1000;\n\tu64 reg, head;\n\tint ret;\n\n\tresult = (struct nix_aq_res_s *)aq->res->base;\n\n\t \n\treg = rvu_read64(rvu, block->addr, NIX_AF_AQ_STATUS);\n\thead = (reg >> 4) & AQ_PTR_MASK;\n\n\tmemcpy((void *)(aq->inst->base + (head * aq->inst->entry_sz)),\n\t       (void *)inst, aq->inst->entry_sz);\n\tmemset(result, 0, sizeof(*result));\n\t \n\twmb();\n\n\t \n\trvu_write64(rvu, block->addr, NIX_AF_AQ_DOOR, 1);\n\twhile (result->compcode == NIX_AQ_COMP_NOTDONE) {\n\t\tcpu_relax();\n\t\tudelay(1);\n\t\ttimeout--;\n\t\tif (!timeout)\n\t\t\treturn -EBUSY;\n\t}\n\n\tif (result->compcode != NIX_AQ_COMP_GOOD) {\n\t\t \n\t\tif (result->compcode == NIX_AQ_COMP_CTX_FAULT ||\n\t\t    result->compcode == NIX_AQ_COMP_LOCKERR ||\n\t\t    result->compcode == NIX_AQ_COMP_CTX_POISON) {\n\t\t\tret = rvu_ndc_fix_locked_cacheline(rvu, BLKADDR_NDC_NIX0_RX);\n\t\t\tret |= rvu_ndc_fix_locked_cacheline(rvu, BLKADDR_NDC_NIX0_TX);\n\t\t\tret |= rvu_ndc_fix_locked_cacheline(rvu, BLKADDR_NDC_NIX1_RX);\n\t\t\tret |= rvu_ndc_fix_locked_cacheline(rvu, BLKADDR_NDC_NIX1_TX);\n\t\t\tif (ret)\n\t\t\t\tdev_err(rvu->dev,\n\t\t\t\t\t\"%s: Not able to unlock cachelines\\n\", __func__);\n\t\t}\n\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic void nix_get_aq_req_smq(struct rvu *rvu, struct nix_aq_enq_req *req,\n\t\t\t       u16 *smq, u16 *smq_mask)\n{\n\tstruct nix_cn10k_aq_enq_req *aq_req;\n\n\tif (!is_rvu_otx2(rvu)) {\n\t\taq_req = (struct nix_cn10k_aq_enq_req *)req;\n\t\t*smq = aq_req->sq.smq;\n\t\t*smq_mask = aq_req->sq_mask.smq;\n\t} else {\n\t\t*smq = req->sq.smq;\n\t\t*smq_mask = req->sq_mask.smq;\n\t}\n}\n\nstatic int rvu_nix_blk_aq_enq_inst(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t\t   struct nix_aq_enq_req *req,\n\t\t\t\t   struct nix_aq_enq_rsp *rsp)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint nixlf, blkaddr, rc = 0;\n\tstruct nix_aq_inst_s inst;\n\tstruct rvu_block *block;\n\tstruct admin_queue *aq;\n\tstruct rvu_pfvf *pfvf;\n\tu16 smq, smq_mask;\n\tvoid *ctx, *mask;\n\tbool ena;\n\tu64 cfg;\n\n\tblkaddr = nix_hw->blkaddr;\n\tblock = &hw->block[blkaddr];\n\taq = block->aq;\n\tif (!aq) {\n\t\tdev_warn(rvu->dev, \"%s: NIX AQ not initialized\\n\", __func__);\n\t\treturn NIX_AF_ERR_AQ_ENQUEUE;\n\t}\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tnixlf = rvu_get_lf(rvu, block, pcifunc, 0);\n\n\t \n\tif (!((!rsp && req->ctype == NIX_AQ_CTYPE_MCE) ||\n\t      (req->ctype == NIX_AQ_CTYPE_BANDPROF && !pcifunc))) {\n\t\tif (!pfvf->nixlf || nixlf < 0)\n\t\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\t}\n\n\tswitch (req->ctype) {\n\tcase NIX_AQ_CTYPE_RQ:\n\t\t \n\t\tif (!pfvf->rq_ctx || req->qidx >= pfvf->rq_ctx->qsize)\n\t\t\trc = NIX_AF_ERR_AQ_ENQUEUE;\n\t\tbreak;\n\tcase NIX_AQ_CTYPE_SQ:\n\t\tif (!pfvf->sq_ctx || req->qidx >= pfvf->sq_ctx->qsize)\n\t\t\trc = NIX_AF_ERR_AQ_ENQUEUE;\n\t\tbreak;\n\tcase NIX_AQ_CTYPE_CQ:\n\t\tif (!pfvf->cq_ctx || req->qidx >= pfvf->cq_ctx->qsize)\n\t\t\trc = NIX_AF_ERR_AQ_ENQUEUE;\n\t\tbreak;\n\tcase NIX_AQ_CTYPE_RSS:\n\t\t \n\t\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_LFX_RSS_CFG(nixlf));\n\t\tif (!(cfg & BIT_ULL(4)) || !pfvf->rss_ctx ||\n\t\t    (req->qidx >= (256UL << (cfg & 0xF))))\n\t\t\trc = NIX_AF_ERR_AQ_ENQUEUE;\n\t\tbreak;\n\tcase NIX_AQ_CTYPE_MCE:\n\t\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_RX_MCAST_CFG);\n\n\t\t \n\t\tif (!nix_hw->mcast.mce_ctx ||\n\t\t    (req->qidx >= (256UL << (cfg & 0xF))))\n\t\t\trc = NIX_AF_ERR_AQ_ENQUEUE;\n\n\t\t \n\t\tif (rsp)\n\t\t\trc = NIX_AF_ERR_AQ_ENQUEUE;\n\t\tbreak;\n\tcase NIX_AQ_CTYPE_BANDPROF:\n\t\tif (nix_verify_bandprof((struct nix_cn10k_aq_enq_req *)req,\n\t\t\t\t\tnix_hw, pcifunc))\n\t\t\trc = NIX_AF_ERR_INVALID_BANDPROF;\n\t\tbreak;\n\tdefault:\n\t\trc = NIX_AF_ERR_AQ_ENQUEUE;\n\t}\n\n\tif (rc)\n\t\treturn rc;\n\n\tnix_get_aq_req_smq(rvu, req, &smq, &smq_mask);\n\t \n\tif (req->ctype == NIX_AQ_CTYPE_SQ &&\n\t    ((req->op == NIX_AQ_INSTOP_INIT && req->sq.ena) ||\n\t     (req->op == NIX_AQ_INSTOP_WRITE &&\n\t      req->sq_mask.ena && req->sq.ena && smq_mask))) {\n\t\tif (!is_valid_txschq(rvu, blkaddr, NIX_TXSCH_LVL_SMQ,\n\t\t\t\t     pcifunc, smq))\n\t\t\treturn NIX_AF_ERR_AQ_ENQUEUE;\n\t}\n\n\tmemset(&inst, 0, sizeof(struct nix_aq_inst_s));\n\tinst.lf = nixlf;\n\tinst.cindex = req->qidx;\n\tinst.ctype = req->ctype;\n\tinst.op = req->op;\n\t \n\tinst.res_addr = (u64)aq->res->iova;\n\n\t \n\tspin_lock(&aq->lock);\n\n\t \n\tmemset(aq->res->base, 0, aq->res->entry_sz);\n\t \n\tctx = aq->res->base + 128;\n\t \n\tmask = aq->res->base + 256;\n\n\tswitch (req->op) {\n\tcase NIX_AQ_INSTOP_WRITE:\n\t\tif (req->ctype == NIX_AQ_CTYPE_RQ)\n\t\t\tmemcpy(mask, &req->rq_mask,\n\t\t\t       sizeof(struct nix_rq_ctx_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_SQ)\n\t\t\tmemcpy(mask, &req->sq_mask,\n\t\t\t       sizeof(struct nix_sq_ctx_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_CQ)\n\t\t\tmemcpy(mask, &req->cq_mask,\n\t\t\t       sizeof(struct nix_cq_ctx_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_RSS)\n\t\t\tmemcpy(mask, &req->rss_mask,\n\t\t\t       sizeof(struct nix_rsse_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_MCE)\n\t\t\tmemcpy(mask, &req->mce_mask,\n\t\t\t       sizeof(struct nix_rx_mce_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_BANDPROF)\n\t\t\tmemcpy(mask, &req->prof_mask,\n\t\t\t       sizeof(struct nix_bandprof_s));\n\t\tfallthrough;\n\tcase NIX_AQ_INSTOP_INIT:\n\t\tif (req->ctype == NIX_AQ_CTYPE_RQ)\n\t\t\tmemcpy(ctx, &req->rq, sizeof(struct nix_rq_ctx_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_SQ)\n\t\t\tmemcpy(ctx, &req->sq, sizeof(struct nix_sq_ctx_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_CQ)\n\t\t\tmemcpy(ctx, &req->cq, sizeof(struct nix_cq_ctx_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_RSS)\n\t\t\tmemcpy(ctx, &req->rss, sizeof(struct nix_rsse_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_MCE)\n\t\t\tmemcpy(ctx, &req->mce, sizeof(struct nix_rx_mce_s));\n\t\telse if (req->ctype == NIX_AQ_CTYPE_BANDPROF)\n\t\t\tmemcpy(ctx, &req->prof, sizeof(struct nix_bandprof_s));\n\t\tbreak;\n\tcase NIX_AQ_INSTOP_NOP:\n\tcase NIX_AQ_INSTOP_READ:\n\tcase NIX_AQ_INSTOP_LOCK:\n\tcase NIX_AQ_INSTOP_UNLOCK:\n\t\tbreak;\n\tdefault:\n\t\trc = NIX_AF_ERR_AQ_ENQUEUE;\n\t\tspin_unlock(&aq->lock);\n\t\treturn rc;\n\t}\n\n\t \n\trc = nix_aq_enqueue_wait(rvu, block, &inst);\n\tif (rc) {\n\t\tspin_unlock(&aq->lock);\n\t\treturn rc;\n\t}\n\n\t \n\tif (req->op == NIX_AQ_INSTOP_INIT) {\n\t\tif (req->ctype == NIX_AQ_CTYPE_RQ && req->rq.ena)\n\t\t\t__set_bit(req->qidx, pfvf->rq_bmap);\n\t\tif (req->ctype == NIX_AQ_CTYPE_SQ && req->sq.ena)\n\t\t\t__set_bit(req->qidx, pfvf->sq_bmap);\n\t\tif (req->ctype == NIX_AQ_CTYPE_CQ && req->cq.ena)\n\t\t\t__set_bit(req->qidx, pfvf->cq_bmap);\n\t}\n\n\tif (req->op == NIX_AQ_INSTOP_WRITE) {\n\t\tif (req->ctype == NIX_AQ_CTYPE_RQ) {\n\t\t\tena = (req->rq.ena & req->rq_mask.ena) |\n\t\t\t\t(test_bit(req->qidx, pfvf->rq_bmap) &\n\t\t\t\t~req->rq_mask.ena);\n\t\t\tif (ena)\n\t\t\t\t__set_bit(req->qidx, pfvf->rq_bmap);\n\t\t\telse\n\t\t\t\t__clear_bit(req->qidx, pfvf->rq_bmap);\n\t\t}\n\t\tif (req->ctype == NIX_AQ_CTYPE_SQ) {\n\t\t\tena = (req->rq.ena & req->sq_mask.ena) |\n\t\t\t\t(test_bit(req->qidx, pfvf->sq_bmap) &\n\t\t\t\t~req->sq_mask.ena);\n\t\t\tif (ena)\n\t\t\t\t__set_bit(req->qidx, pfvf->sq_bmap);\n\t\t\telse\n\t\t\t\t__clear_bit(req->qidx, pfvf->sq_bmap);\n\t\t}\n\t\tif (req->ctype == NIX_AQ_CTYPE_CQ) {\n\t\t\tena = (req->rq.ena & req->cq_mask.ena) |\n\t\t\t\t(test_bit(req->qidx, pfvf->cq_bmap) &\n\t\t\t\t~req->cq_mask.ena);\n\t\t\tif (ena)\n\t\t\t\t__set_bit(req->qidx, pfvf->cq_bmap);\n\t\t\telse\n\t\t\t\t__clear_bit(req->qidx, pfvf->cq_bmap);\n\t\t}\n\t}\n\n\tif (rsp) {\n\t\t \n\t\tif (req->op == NIX_AQ_INSTOP_READ) {\n\t\t\tif (req->ctype == NIX_AQ_CTYPE_RQ)\n\t\t\t\tmemcpy(&rsp->rq, ctx,\n\t\t\t\t       sizeof(struct nix_rq_ctx_s));\n\t\t\telse if (req->ctype == NIX_AQ_CTYPE_SQ)\n\t\t\t\tmemcpy(&rsp->sq, ctx,\n\t\t\t\t       sizeof(struct nix_sq_ctx_s));\n\t\t\telse if (req->ctype == NIX_AQ_CTYPE_CQ)\n\t\t\t\tmemcpy(&rsp->cq, ctx,\n\t\t\t\t       sizeof(struct nix_cq_ctx_s));\n\t\t\telse if (req->ctype == NIX_AQ_CTYPE_RSS)\n\t\t\t\tmemcpy(&rsp->rss, ctx,\n\t\t\t\t       sizeof(struct nix_rsse_s));\n\t\t\telse if (req->ctype == NIX_AQ_CTYPE_MCE)\n\t\t\t\tmemcpy(&rsp->mce, ctx,\n\t\t\t\t       sizeof(struct nix_rx_mce_s));\n\t\t\telse if (req->ctype == NIX_AQ_CTYPE_BANDPROF)\n\t\t\t\tmemcpy(&rsp->prof, ctx,\n\t\t\t\t       sizeof(struct nix_bandprof_s));\n\t\t}\n\t}\n\n\tspin_unlock(&aq->lock);\n\treturn 0;\n}\n\nstatic int rvu_nix_verify_aq_ctx(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t\t struct nix_aq_enq_req *req, u8 ctype)\n{\n\tstruct nix_cn10k_aq_enq_req aq_req;\n\tstruct nix_cn10k_aq_enq_rsp aq_rsp;\n\tint rc, word;\n\n\tif (req->ctype != NIX_AQ_CTYPE_CQ)\n\t\treturn 0;\n\n\trc = nix_aq_context_read(rvu, nix_hw, &aq_req, &aq_rsp,\n\t\t\t\t req->hdr.pcifunc, ctype, req->qidx);\n\tif (rc) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Failed to fetch %s%d context of PFFUNC 0x%x\\n\",\n\t\t\t__func__, nix_get_ctx_name(ctype), req->qidx,\n\t\t\treq->hdr.pcifunc);\n\t\treturn rc;\n\t}\n\n\t \n\tmemcpy(&aq_req.cq_mask, &req->cq_mask, sizeof(struct nix_cq_ctx_s));\n\tmemcpy(&aq_req.cq, &req->cq, sizeof(struct nix_cq_ctx_s));\n\n\t \n\taq_req.cq_mask.cq_err       = 0;\n\taq_req.cq_mask.wrptr        = 0;\n\taq_req.cq_mask.tail         = 0;\n\taq_req.cq_mask.head\t    = 0;\n\taq_req.cq_mask.avg_level    = 0;\n\taq_req.cq_mask.update_time  = 0;\n\taq_req.cq_mask.substream    = 0;\n\n\t \n\tfor (word = 0; word < sizeof(struct nix_cq_ctx_s) / sizeof(u64);\n\t     word++) {\n\t\t*(u64 *)((u8 *)&aq_rsp.cq + word * 8) &=\n\t\t\t(*(u64 *)((u8 *)&aq_req.cq_mask + word * 8));\n\t\t*(u64 *)((u8 *)&aq_req.cq + word * 8) &=\n\t\t\t(*(u64 *)((u8 *)&aq_req.cq_mask + word * 8));\n\t}\n\n\tif (memcmp(&aq_req.cq, &aq_rsp.cq, sizeof(struct nix_cq_ctx_s)))\n\t\treturn NIX_AF_ERR_AQ_CTX_RETRY_WRITE;\n\n\treturn 0;\n}\n\nstatic int rvu_nix_aq_enq_inst(struct rvu *rvu, struct nix_aq_enq_req *req,\n\t\t\t       struct nix_aq_enq_rsp *rsp)\n{\n\tstruct nix_hw *nix_hw;\n\tint err, retries = 5;\n\tint blkaddr;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, req->hdr.pcifunc);\n\tif (blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tnix_hw =  get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\nretry:\n\terr = rvu_nix_blk_aq_enq_inst(rvu, nix_hw, req, rsp);\n\n\t \n\tif (!err && req->op == NIX_AQ_INSTOP_WRITE) {\n\t\terr = rvu_nix_verify_aq_ctx(rvu, nix_hw, req, NIX_AQ_CTYPE_CQ);\n\t\tif (err == NIX_AF_ERR_AQ_CTX_RETRY_WRITE) {\n\t\t\tif (retries--)\n\t\t\t\tgoto retry;\n\t\t\telse\n\t\t\t\treturn NIX_AF_ERR_CQ_CTX_WRITE_ERR;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic const char *nix_get_ctx_name(int ctype)\n{\n\tswitch (ctype) {\n\tcase NIX_AQ_CTYPE_CQ:\n\t\treturn \"CQ\";\n\tcase NIX_AQ_CTYPE_SQ:\n\t\treturn \"SQ\";\n\tcase NIX_AQ_CTYPE_RQ:\n\t\treturn \"RQ\";\n\tcase NIX_AQ_CTYPE_RSS:\n\t\treturn \"RSS\";\n\t}\n\treturn \"\";\n}\n\nstatic int nix_lf_hwctx_disable(struct rvu *rvu, struct hwctx_disable_req *req)\n{\n\tstruct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, req->hdr.pcifunc);\n\tstruct nix_aq_enq_req aq_req;\n\tunsigned long *bmap;\n\tint qidx, q_cnt = 0;\n\tint err = 0, rc;\n\n\tif (!pfvf->cq_ctx || !pfvf->sq_ctx || !pfvf->rq_ctx)\n\t\treturn NIX_AF_ERR_AQ_ENQUEUE;\n\n\tmemset(&aq_req, 0, sizeof(struct nix_aq_enq_req));\n\taq_req.hdr.pcifunc = req->hdr.pcifunc;\n\n\tif (req->ctype == NIX_AQ_CTYPE_CQ) {\n\t\taq_req.cq.ena = 0;\n\t\taq_req.cq_mask.ena = 1;\n\t\taq_req.cq.bp_ena = 0;\n\t\taq_req.cq_mask.bp_ena = 1;\n\t\tq_cnt = pfvf->cq_ctx->qsize;\n\t\tbmap = pfvf->cq_bmap;\n\t}\n\tif (req->ctype == NIX_AQ_CTYPE_SQ) {\n\t\taq_req.sq.ena = 0;\n\t\taq_req.sq_mask.ena = 1;\n\t\tq_cnt = pfvf->sq_ctx->qsize;\n\t\tbmap = pfvf->sq_bmap;\n\t}\n\tif (req->ctype == NIX_AQ_CTYPE_RQ) {\n\t\taq_req.rq.ena = 0;\n\t\taq_req.rq_mask.ena = 1;\n\t\tq_cnt = pfvf->rq_ctx->qsize;\n\t\tbmap = pfvf->rq_bmap;\n\t}\n\n\taq_req.ctype = req->ctype;\n\taq_req.op = NIX_AQ_INSTOP_WRITE;\n\n\tfor (qidx = 0; qidx < q_cnt; qidx++) {\n\t\tif (!test_bit(qidx, bmap))\n\t\t\tcontinue;\n\t\taq_req.qidx = qidx;\n\t\trc = rvu_nix_aq_enq_inst(rvu, &aq_req, NULL);\n\t\tif (rc) {\n\t\t\terr = rc;\n\t\t\tdev_err(rvu->dev, \"Failed to disable %s:%d context\\n\",\n\t\t\t\tnix_get_ctx_name(req->ctype), qidx);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n#ifdef CONFIG_NDC_DIS_DYNAMIC_CACHING\nstatic int nix_lf_hwctx_lockdown(struct rvu *rvu, struct nix_aq_enq_req *req)\n{\n\tstruct nix_aq_enq_req lock_ctx_req;\n\tint err;\n\n\tif (req->op != NIX_AQ_INSTOP_INIT)\n\t\treturn 0;\n\n\tif (req->ctype == NIX_AQ_CTYPE_MCE ||\n\t    req->ctype == NIX_AQ_CTYPE_DYNO)\n\t\treturn 0;\n\n\tmemset(&lock_ctx_req, 0, sizeof(struct nix_aq_enq_req));\n\tlock_ctx_req.hdr.pcifunc = req->hdr.pcifunc;\n\tlock_ctx_req.ctype = req->ctype;\n\tlock_ctx_req.op = NIX_AQ_INSTOP_LOCK;\n\tlock_ctx_req.qidx = req->qidx;\n\terr = rvu_nix_aq_enq_inst(rvu, &lock_ctx_req, NULL);\n\tif (err)\n\t\tdev_err(rvu->dev,\n\t\t\t\"PFUNC 0x%x: Failed to lock NIX %s:%d context\\n\",\n\t\t\treq->hdr.pcifunc,\n\t\t\tnix_get_ctx_name(req->ctype), req->qidx);\n\treturn err;\n}\n\nint rvu_mbox_handler_nix_aq_enq(struct rvu *rvu,\n\t\t\t\tstruct nix_aq_enq_req *req,\n\t\t\t\tstruct nix_aq_enq_rsp *rsp)\n{\n\tint err;\n\n\terr = rvu_nix_aq_enq_inst(rvu, req, rsp);\n\tif (!err)\n\t\terr = nix_lf_hwctx_lockdown(rvu, req);\n\treturn err;\n}\n#else\n\nint rvu_mbox_handler_nix_aq_enq(struct rvu *rvu,\n\t\t\t\tstruct nix_aq_enq_req *req,\n\t\t\t\tstruct nix_aq_enq_rsp *rsp)\n{\n\treturn rvu_nix_aq_enq_inst(rvu, req, rsp);\n}\n#endif\n \nint rvu_mbox_handler_nix_cn10k_aq_enq(struct rvu *rvu,\n\t\t\t\t      struct nix_cn10k_aq_enq_req *req,\n\t\t\t\t      struct nix_cn10k_aq_enq_rsp *rsp)\n{\n\treturn rvu_nix_aq_enq_inst(rvu, (struct nix_aq_enq_req *)req,\n\t\t\t\t  (struct nix_aq_enq_rsp *)rsp);\n}\n\nint rvu_mbox_handler_nix_hwctx_disable(struct rvu *rvu,\n\t\t\t\t       struct hwctx_disable_req *req,\n\t\t\t\t       struct msg_rsp *rsp)\n{\n\treturn nix_lf_hwctx_disable(rvu, req);\n}\n\nint rvu_mbox_handler_nix_lf_alloc(struct rvu *rvu,\n\t\t\t\t  struct nix_lf_alloc_req *req,\n\t\t\t\t  struct nix_lf_alloc_rsp *rsp)\n{\n\tint nixlf, qints, hwctx_size, intf, err, rc = 0;\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct rvu_block *block;\n\tstruct rvu_pfvf *pfvf;\n\tu64 cfg, ctx_cfg;\n\tint blkaddr;\n\n\tif (!req->rq_cnt || !req->sq_cnt || !req->cq_cnt)\n\t\treturn NIX_AF_ERR_PARAM;\n\n\tif (req->way_mask)\n\t\treq->way_mask &= 0xFFFF;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (!pfvf->nixlf || blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tblock = &hw->block[blkaddr];\n\tnixlf = rvu_get_lf(rvu, block, pcifunc, 0);\n\tif (nixlf < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\t \n\tif (req->npa_func) {\n\t\t \n\t\tif (req->npa_func == RVU_DEFAULT_PF_FUNC)\n\t\t\treq->npa_func = pcifunc;\n\t\tif (!is_pffunc_map_valid(rvu, req->npa_func, BLKTYPE_NPA))\n\t\t\treturn NIX_AF_INVAL_NPA_PF_FUNC;\n\t}\n\n\t \n\tif (req->sso_func) {\n\t\t \n\t\tif (req->sso_func == RVU_DEFAULT_PF_FUNC)\n\t\t\treq->sso_func = pcifunc;\n\t\tif (!is_pffunc_map_valid(rvu, req->sso_func, BLKTYPE_SSO))\n\t\t\treturn NIX_AF_INVAL_SSO_PF_FUNC;\n\t}\n\n\t \n\tif (req->rss_sz && (req->rss_sz > MAX_RSS_INDIR_TBL_SIZE ||\n\t\t\t    !is_power_of_2(req->rss_sz)))\n\t\treturn NIX_AF_ERR_RSS_SIZE_INVALID;\n\n\tif (req->rss_sz &&\n\t    (!req->rss_grps || req->rss_grps > MAX_RSS_GROUPS))\n\t\treturn NIX_AF_ERR_RSS_GRPS_INVALID;\n\n\t \n\terr = rvu_lf_reset(rvu, block, nixlf);\n\tif (err) {\n\t\tdev_err(rvu->dev, \"Failed to reset NIX%d LF%d\\n\",\n\t\t\tblock->addr - BLKADDR_NIX0, nixlf);\n\t\treturn NIX_AF_ERR_LF_RESET;\n\t}\n\n\tctx_cfg = rvu_read64(rvu, blkaddr, NIX_AF_CONST3);\n\n\t \n\thwctx_size = 1UL << ((ctx_cfg >> 4) & 0xF);\n\terr = qmem_alloc(rvu->dev, &pfvf->rq_ctx, req->rq_cnt, hwctx_size);\n\tif (err)\n\t\tgoto free_mem;\n\n\tpfvf->rq_bmap = kcalloc(req->rq_cnt, sizeof(long), GFP_KERNEL);\n\tif (!pfvf->rq_bmap)\n\t\tgoto free_mem;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RQS_BASE(nixlf),\n\t\t    (u64)pfvf->rq_ctx->iova);\n\n\t \n\tcfg = BIT_ULL(36) | (req->rq_cnt - 1) | req->way_mask << 20;\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RQS_CFG(nixlf), cfg);\n\n\t \n\thwctx_size = 1UL << (ctx_cfg & 0xF);\n\terr = qmem_alloc(rvu->dev, &pfvf->sq_ctx, req->sq_cnt, hwctx_size);\n\tif (err)\n\t\tgoto free_mem;\n\n\tpfvf->sq_bmap = kcalloc(req->sq_cnt, sizeof(long), GFP_KERNEL);\n\tif (!pfvf->sq_bmap)\n\t\tgoto free_mem;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_SQS_BASE(nixlf),\n\t\t    (u64)pfvf->sq_ctx->iova);\n\n\tcfg = BIT_ULL(36) | (req->sq_cnt - 1) | req->way_mask << 20;\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_SQS_CFG(nixlf), cfg);\n\n\t \n\thwctx_size = 1UL << ((ctx_cfg >> 8) & 0xF);\n\terr = qmem_alloc(rvu->dev, &pfvf->cq_ctx, req->cq_cnt, hwctx_size);\n\tif (err)\n\t\tgoto free_mem;\n\n\tpfvf->cq_bmap = kcalloc(req->cq_cnt, sizeof(long), GFP_KERNEL);\n\tif (!pfvf->cq_bmap)\n\t\tgoto free_mem;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_CQS_BASE(nixlf),\n\t\t    (u64)pfvf->cq_ctx->iova);\n\n\tcfg = BIT_ULL(36) | (req->cq_cnt - 1) | req->way_mask << 20;\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_CQS_CFG(nixlf), cfg);\n\n\t \n\thwctx_size = 1UL << ((ctx_cfg >> 12) & 0xF);\n\terr = nixlf_rss_ctx_init(rvu, blkaddr, pfvf, nixlf, req->rss_sz,\n\t\t\t\t req->rss_grps, hwctx_size, req->way_mask,\n\t\t\t\t !!(req->flags & NIX_LF_RSS_TAG_LSB_AS_ADDER));\n\tif (err)\n\t\tgoto free_mem;\n\n\t \n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_CONST2);\n\tqints = (cfg >> 24) & 0xFFF;\n\thwctx_size = 1UL << ((ctx_cfg >> 24) & 0xF);\n\terr = qmem_alloc(rvu->dev, &pfvf->cq_ints_ctx, qints, hwctx_size);\n\tif (err)\n\t\tgoto free_mem;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_CINTS_BASE(nixlf),\n\t\t    (u64)pfvf->cq_ints_ctx->iova);\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_CINTS_CFG(nixlf),\n\t\t    BIT_ULL(36) | req->way_mask << 20);\n\n\t \n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_CONST2);\n\tqints = (cfg >> 12) & 0xFFF;\n\thwctx_size = 1UL << ((ctx_cfg >> 20) & 0xF);\n\terr = qmem_alloc(rvu->dev, &pfvf->nix_qints_ctx, qints, hwctx_size);\n\tif (err)\n\t\tgoto free_mem;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_QINTS_BASE(nixlf),\n\t\t    (u64)pfvf->nix_qints_ctx->iova);\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_QINTS_CFG(nixlf),\n\t\t    BIT_ULL(36) | req->way_mask << 20);\n\n\t \n\tcfg = (0x8100ULL << 16) | 0x88A8ULL;\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_TX_CFG(nixlf), cfg);\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_TX_CFG2(nixlf), BIT_ULL(0));\n\n\t \n\tif (req->npa_func)\n\t\tcfg = req->npa_func;\n\tif (req->sso_func)\n\t\tcfg |= (u64)req->sso_func << 16;\n\n\tcfg |= (u64)req->xqe_sz << 33;\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_CFG(nixlf), cfg);\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RX_CFG(nixlf), req->rx_cfg);\n\n\t \n\tcfg = NPC_TX_DEF_PKIND;\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_TX_PARSE_CFG(nixlf), cfg);\n\n\tintf = is_afvf(pcifunc) ? NIX_INTF_TYPE_LBK : NIX_INTF_TYPE_CGX;\n\tif (is_sdp_pfvf(pcifunc))\n\t\tintf = NIX_INTF_TYPE_SDP;\n\n\terr = nix_interface_init(rvu, pcifunc, intf, nixlf, rsp,\n\t\t\t\t !!(req->flags & NIX_LF_LBK_BLK_SEL));\n\tif (err)\n\t\tgoto free_mem;\n\n\t \n\trvu_npc_disable_default_entries(rvu, pcifunc, nixlf);\n\n\t \n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_LFX_RX_VTAG_TYPEX(nixlf, NIX_AF_LFX_RX_VTAG_TYPE7),\n\t\t    VTAGSIZE_T4 | VTAG_STRIP);\n\n\tgoto exit;\n\nfree_mem:\n\tnix_ctx_free(rvu, pfvf);\n\trc = -ENOMEM;\n\nexit:\n\t \n\tether_addr_copy(rsp->mac_addr, pfvf->mac_addr);\n\n\t \n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_SQ_CONST);\n\trsp->sqb_size = (cfg >> 34) & 0xFFFF;\n\trsp->rx_chan_base = pfvf->rx_chan_base;\n\trsp->tx_chan_base = pfvf->tx_chan_base;\n\trsp->rx_chan_cnt = pfvf->rx_chan_cnt;\n\trsp->tx_chan_cnt = pfvf->tx_chan_cnt;\n\trsp->lso_tsov4_idx = NIX_LSO_FORMAT_IDX_TSOV4;\n\trsp->lso_tsov6_idx = NIX_LSO_FORMAT_IDX_TSOV6;\n\t \n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_CONST1);\n\trsp->lf_rx_stats = ((cfg >> 32) & 0xFF);\n\trsp->lf_tx_stats = ((cfg >> 24) & 0xFF);\n\t \n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_CONST2);\n\trsp->qints = ((cfg >> 12) & 0xFFF);\n\trsp->cints = ((cfg >> 24) & 0xFFF);\n\trsp->cgx_links = hw->cgx_links;\n\trsp->lbk_links = hw->lbk_links;\n\trsp->sdp_links = hw->sdp_links;\n\n\treturn rc;\n}\n\nint rvu_mbox_handler_nix_lf_free(struct rvu *rvu, struct nix_lf_free_req *req,\n\t\t\t\t struct msg_rsp *rsp)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct rvu_block *block;\n\tint blkaddr, nixlf, err;\n\tstruct rvu_pfvf *pfvf;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (!pfvf->nixlf || blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tblock = &hw->block[blkaddr];\n\tnixlf = rvu_get_lf(rvu, block, pcifunc, 0);\n\tif (nixlf < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tif (req->flags & NIX_LF_DISABLE_FLOWS)\n\t\trvu_npc_disable_mcam_entries(rvu, pcifunc, nixlf);\n\telse\n\t\trvu_npc_free_mcam_entries(rvu, pcifunc, nixlf);\n\n\t \n\tif (!(req->flags & NIX_LF_DONT_FREE_TX_VTAG))\n\t\tnix_free_tx_vtag_entries(rvu, pcifunc);\n\n\tnix_interface_deinit(rvu, pcifunc, nixlf);\n\n\t \n\terr = rvu_lf_reset(rvu, block, nixlf);\n\tif (err) {\n\t\tdev_err(rvu->dev, \"Failed to reset NIX%d LF%d\\n\",\n\t\t\tblock->addr - BLKADDR_NIX0, nixlf);\n\t\treturn NIX_AF_ERR_LF_RESET;\n\t}\n\n\tnix_ctx_free(rvu, pfvf);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_mark_format_cfg(struct rvu *rvu,\n\t\t\t\t\t struct nix_mark_format_cfg  *req,\n\t\t\t\t\t struct nix_mark_format_cfg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct nix_hw *nix_hw;\n\tstruct rvu_pfvf *pfvf;\n\tint blkaddr, rc;\n\tu32 cfg;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (!pfvf->nixlf || blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tcfg = (((u32)req->offset & 0x7) << 16) |\n\t      (((u32)req->y_mask & 0xF) << 12) |\n\t      (((u32)req->y_val & 0xF) << 8) |\n\t      (((u32)req->r_mask & 0xF) << 4) | ((u32)req->r_val & 0xF);\n\n\trc = rvu_nix_reserve_mark_format(rvu, nix_hw, blkaddr, cfg);\n\tif (rc < 0) {\n\t\tdev_err(rvu->dev, \"No mark_format_ctl for (pf:%d, vf:%d)\",\n\t\t\trvu_get_pf(pcifunc), pcifunc & RVU_PFVF_FUNC_MASK);\n\t\treturn NIX_AF_ERR_MARK_CFG_FAIL;\n\t}\n\n\trsp->mark_format_idx = rc;\n\treturn 0;\n}\n\n \nstatic bool\nhandle_txschq_shaper_update(struct rvu *rvu, int blkaddr, int nixlf,\n\t\t\t    int lvl, u64 reg, u64 regval)\n{\n\tu64 regbase, oldval, sw_xoff = 0;\n\tu64 dbgval, md_debug0 = 0;\n\tunsigned long poll_tmo;\n\tbool rate_reg = 0;\n\tu32 schq;\n\n\tregbase = reg & 0xFFFF;\n\tschq = TXSCHQ_IDX(reg, TXSCHQ_IDX_SHIFT);\n\n\t \n\tswitch (lvl) {\n\tcase NIX_TXSCH_LVL_TL1:\n\t\tmd_debug0 = NIX_AF_TL1X_MD_DEBUG0(schq);\n\t\tsw_xoff = NIX_AF_TL1X_SW_XOFF(schq);\n\n\t\trate_reg = !!(regbase == NIX_AF_TL1X_CIR(0));\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL2:\n\t\tmd_debug0 = NIX_AF_TL2X_MD_DEBUG0(schq);\n\t\tsw_xoff = NIX_AF_TL2X_SW_XOFF(schq);\n\n\t\trate_reg = (regbase == NIX_AF_TL2X_CIR(0) ||\n\t\t\t    regbase == NIX_AF_TL2X_PIR(0));\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL3:\n\t\tmd_debug0 = NIX_AF_TL3X_MD_DEBUG0(schq);\n\t\tsw_xoff = NIX_AF_TL3X_SW_XOFF(schq);\n\n\t\trate_reg = (regbase == NIX_AF_TL3X_CIR(0) ||\n\t\t\t    regbase == NIX_AF_TL3X_PIR(0));\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL4:\n\t\tmd_debug0 = NIX_AF_TL4X_MD_DEBUG0(schq);\n\t\tsw_xoff = NIX_AF_TL4X_SW_XOFF(schq);\n\n\t\trate_reg = (regbase == NIX_AF_TL4X_CIR(0) ||\n\t\t\t    regbase == NIX_AF_TL4X_PIR(0));\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_MDQ:\n\t\tsw_xoff = NIX_AF_MDQX_SW_XOFF(schq);\n\t\trate_reg = (regbase == NIX_AF_MDQX_CIR(0) ||\n\t\t\t    regbase == NIX_AF_MDQX_PIR(0));\n\t\tbreak;\n\t}\n\n\tif (!rate_reg)\n\t\treturn false;\n\n\t \n\toldval = rvu_read64(rvu, blkaddr, reg);\n\tif ((oldval & 0x1) == (regval & 0x1)) {\n\t\trvu_write64(rvu, blkaddr, reg, regval);\n\t\treturn true;\n\t}\n\n\t \n\tif (!(regval & 0x1)) {\n\t\trvu_write64(rvu, blkaddr, sw_xoff, 1);\n\t\trvu_write64(rvu, blkaddr, reg, 0);\n\t\tudelay(4);\n\t\trvu_write64(rvu, blkaddr, sw_xoff, 0);\n\t\treturn true;\n\t}\n\n\t \n\trvu_write64(rvu, blkaddr, sw_xoff, 1);\n\tif (md_debug0) {\n\t\tpoll_tmo = jiffies + usecs_to_jiffies(10000);\n\t\t \n\t\tdo {\n\t\t\tif (time_after(jiffies, poll_tmo)) {\n\t\t\t\tdev_err(rvu->dev,\n\t\t\t\t\t\"NIXLF%d: TLX%u(lvl %u) CIR/PIR enable failed\\n\",\n\t\t\t\t\tnixlf, schq, lvl);\n\t\t\t\tgoto exit;\n\t\t\t}\n\t\t\tusleep_range(1, 5);\n\t\t\tdbgval = rvu_read64(rvu, blkaddr, md_debug0);\n\t\t} while (!(dbgval & BIT_ULL(32)) && (dbgval & BIT_ULL(48)));\n\t}\n\trvu_write64(rvu, blkaddr, reg, regval);\nexit:\n\trvu_write64(rvu, blkaddr, sw_xoff, 0);\n\treturn true;\n}\n\nstatic void nix_reset_tx_schedule(struct rvu *rvu, int blkaddr,\n\t\t\t\t  int lvl, int schq)\n{\n\tu64 tlx_parent = 0, tlx_schedule = 0;\n\n\tswitch (lvl) {\n\tcase NIX_TXSCH_LVL_TL2:\n\t\ttlx_parent   = NIX_AF_TL2X_PARENT(schq);\n\t\ttlx_schedule = NIX_AF_TL2X_SCHEDULE(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL3:\n\t\ttlx_parent   = NIX_AF_TL3X_PARENT(schq);\n\t\ttlx_schedule = NIX_AF_TL3X_SCHEDULE(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL4:\n\t\ttlx_parent   = NIX_AF_TL4X_PARENT(schq);\n\t\ttlx_schedule = NIX_AF_TL4X_SCHEDULE(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_MDQ:\n\t\t \n\t\ttlx_parent   = NIX_AF_MDQX_PARENT(schq);\n\t\ttlx_schedule = NIX_AF_MDQX_SCHEDULE(schq);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (tlx_parent)\n\t\trvu_write64(rvu, blkaddr, tlx_parent, 0x0);\n\n\tif (tlx_schedule)\n\t\trvu_write64(rvu, blkaddr, tlx_schedule, 0x0);\n}\n\n \nstatic void nix_reset_tx_shaping(struct rvu *rvu, int blkaddr,\n\t\t\t\t int nixlf, int lvl, int schq)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu64  cir_reg = 0, pir_reg = 0;\n\tu64  cfg;\n\n\tswitch (lvl) {\n\tcase NIX_TXSCH_LVL_TL1:\n\t\tcir_reg = NIX_AF_TL1X_CIR(schq);\n\t\tpir_reg = 0;  \n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL2:\n\t\tcir_reg = NIX_AF_TL2X_CIR(schq);\n\t\tpir_reg = NIX_AF_TL2X_PIR(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL3:\n\t\tcir_reg = NIX_AF_TL3X_CIR(schq);\n\t\tpir_reg = NIX_AF_TL3X_PIR(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL4:\n\t\tcir_reg = NIX_AF_TL4X_CIR(schq);\n\t\tpir_reg = NIX_AF_TL4X_PIR(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_MDQ:\n\t\tcir_reg = NIX_AF_MDQX_CIR(schq);\n\t\tpir_reg = NIX_AF_MDQX_PIR(schq);\n\t\tbreak;\n\t}\n\n\t \n\tif (hw->cap.nix_shaper_toggle_wait) {\n\t\tif (cir_reg)\n\t\t\thandle_txschq_shaper_update(rvu, blkaddr, nixlf,\n\t\t\t\t\t\t    lvl, cir_reg, 0);\n\t\tif (pir_reg)\n\t\t\thandle_txschq_shaper_update(rvu, blkaddr, nixlf,\n\t\t\t\t\t\t    lvl, pir_reg, 0);\n\t\treturn;\n\t}\n\n\tif (!cir_reg)\n\t\treturn;\n\tcfg = rvu_read64(rvu, blkaddr, cir_reg);\n\trvu_write64(rvu, blkaddr, cir_reg, cfg & ~BIT_ULL(0));\n\n\tif (!pir_reg)\n\t\treturn;\n\tcfg = rvu_read64(rvu, blkaddr, pir_reg);\n\trvu_write64(rvu, blkaddr, pir_reg, cfg & ~BIT_ULL(0));\n}\n\nstatic void nix_reset_tx_linkcfg(struct rvu *rvu, int blkaddr,\n\t\t\t\t int lvl, int schq)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint link_level;\n\tint link;\n\n\tif (lvl >= hw->cap.nix_tx_aggr_lvl)\n\t\treturn;\n\n\t \n\tif (lvl == NIX_TXSCH_LVL_TL4)\n\t\trvu_write64(rvu, blkaddr, NIX_AF_TL4X_SDP_LINK_CFG(schq), 0x00);\n\n\tlink_level = rvu_read64(rvu, blkaddr, NIX_AF_PSE_CHANNEL_LEVEL) & 0x01 ?\n\t\t\tNIX_TXSCH_LVL_TL3 : NIX_TXSCH_LVL_TL2;\n\tif (lvl != link_level)\n\t\treturn;\n\n\t \n\tfor (link = 0; link < (hw->cgx_links + hw->lbk_links); link++)\n\t\trvu_write64(rvu, blkaddr,\n\t\t\t    NIX_AF_TL3_TL2X_LINKX_CFG(schq, link), 0x00);\n}\n\nstatic void nix_clear_tx_xoff(struct rvu *rvu, int blkaddr,\n\t\t\t      int lvl, int schq)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu64 reg;\n\n\t \n\tif (!hw->cap.nix_shaping)\n\t\treturn;\n\n\t \n\tswitch (lvl) {\n\tcase NIX_TXSCH_LVL_TL1:\n\t\treg = NIX_AF_TL1X_SW_XOFF(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL2:\n\t\treg = NIX_AF_TL2X_SW_XOFF(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL3:\n\t\treg = NIX_AF_TL3X_SW_XOFF(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL4:\n\t\treg = NIX_AF_TL4X_SW_XOFF(schq);\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_MDQ:\n\t\treg = NIX_AF_MDQX_SW_XOFF(schq);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\trvu_write64(rvu, blkaddr, reg, 0x0);\n}\n\nstatic int nix_get_tx_link(struct rvu *rvu, u16 pcifunc)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint pf = rvu_get_pf(pcifunc);\n\tu8 cgx_id = 0, lmac_id = 0;\n\n\tif (is_afvf(pcifunc)) { \n\t\treturn hw->cgx_links;\n\t} else if (is_pf_cgxmapped(rvu, pf)) {\n\t\trvu_get_cgx_lmac_id(rvu->pf2cgxlmac_map[pf], &cgx_id, &lmac_id);\n\t\treturn (cgx_id * hw->lmac_per_cgx) + lmac_id;\n\t}\n\n\t \n\treturn hw->cgx_links + hw->lbk_links;\n}\n\nstatic void nix_get_txschq_range(struct rvu *rvu, u16 pcifunc,\n\t\t\t\t int link, int *start, int *end)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint pf = rvu_get_pf(pcifunc);\n\n\tif (is_afvf(pcifunc)) {  \n\t\t*start = hw->cap.nix_txsch_per_cgx_lmac * link;\n\t\t*end = *start + hw->cap.nix_txsch_per_lbk_lmac;\n\t} else if (is_pf_cgxmapped(rvu, pf)) {  \n\t\t*start = hw->cap.nix_txsch_per_cgx_lmac * link;\n\t\t*end = *start + hw->cap.nix_txsch_per_cgx_lmac;\n\t} else {  \n\t\t*start = (hw->cap.nix_txsch_per_cgx_lmac * hw->cgx_links) +\n\t\t\t(hw->cap.nix_txsch_per_lbk_lmac * hw->lbk_links);\n\t\t*end = *start + hw->cap.nix_txsch_per_sdp_lmac;\n\t}\n}\n\nstatic int nix_check_txschq_alloc_req(struct rvu *rvu, int lvl, u16 pcifunc,\n\t\t\t\t      struct nix_hw *nix_hw,\n\t\t\t\t      struct nix_txsch_alloc_req *req)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint schq, req_schq, free_cnt;\n\tstruct nix_txsch *txsch;\n\tint link, start, end;\n\n\ttxsch = &nix_hw->txsch[lvl];\n\treq_schq = req->schq_contig[lvl] + req->schq[lvl];\n\n\tif (!req_schq)\n\t\treturn 0;\n\n\tlink = nix_get_tx_link(rvu, pcifunc);\n\n\t \n\tif (lvl >= hw->cap.nix_tx_aggr_lvl) {\n\t\tif (req_schq != 1)\n\t\t\treturn NIX_AF_ERR_TLX_ALLOC_FAIL;\n\t\treturn 0;\n\t}\n\n\t \n\tif (hw->cap.nix_fixed_txschq_mapping) {\n\t\tnix_get_txschq_range(rvu, pcifunc, link, &start, &end);\n\t\tschq = start + (pcifunc & RVU_PFVF_FUNC_MASK);\n\t\tif (end <= txsch->schq.max && schq < end &&\n\t\t    !test_bit(schq, txsch->schq.bmap))\n\t\t\tfree_cnt = 1;\n\t\telse\n\t\t\tfree_cnt = 0;\n\t} else {\n\t\tfree_cnt = rvu_rsrc_free_count(&txsch->schq);\n\t}\n\n\tif (free_cnt < req_schq || req->schq[lvl] > MAX_TXSCHQ_PER_FUNC ||\n\t    req->schq_contig[lvl] > MAX_TXSCHQ_PER_FUNC)\n\t\treturn NIX_AF_ERR_TLX_ALLOC_FAIL;\n\n\t \n\tif (!hw->cap.nix_fixed_txschq_mapping && req->schq_contig[lvl] &&\n\t    !rvu_rsrc_check_contig(&txsch->schq, req->schq_contig[lvl]))\n\t\treturn NIX_AF_ERR_TLX_ALLOC_FAIL;\n\n\treturn 0;\n}\n\nstatic void nix_txsch_alloc(struct rvu *rvu, struct nix_txsch *txsch,\n\t\t\t    struct nix_txsch_alloc_rsp *rsp,\n\t\t\t    int lvl, int start, int end)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu16 pcifunc = rsp->hdr.pcifunc;\n\tint idx, schq;\n\n\t \n\tif (lvl >= hw->cap.nix_tx_aggr_lvl) {\n\t\t \n\t\tif (rsp->schq_contig[lvl]) {\n\t\t\trsp->schq_contig[lvl] = 1;\n\t\t\trsp->schq_contig_list[lvl][0] = start;\n\t\t}\n\n\t\t \n\t\tif (rsp->schq_contig[lvl])\n\t\t\trsp->schq[lvl] = 0;\n\n\t\tif (rsp->schq[lvl]) {\n\t\t\trsp->schq[lvl] = 1;\n\t\t\trsp->schq_list[lvl][0] = start;\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tif (hw->cap.nix_fixed_txschq_mapping) {\n\t\tidx = pcifunc & RVU_PFVF_FUNC_MASK;\n\t\tschq = start + idx;\n\t\tif (idx >= (end - start) || test_bit(schq, txsch->schq.bmap)) {\n\t\t\trsp->schq_contig[lvl] = 0;\n\t\t\trsp->schq[lvl] = 0;\n\t\t\treturn;\n\t\t}\n\n\t\tif (rsp->schq_contig[lvl]) {\n\t\t\trsp->schq_contig[lvl] = 1;\n\t\t\tset_bit(schq, txsch->schq.bmap);\n\t\t\trsp->schq_contig_list[lvl][0] = schq;\n\t\t\trsp->schq[lvl] = 0;\n\t\t} else if (rsp->schq[lvl]) {\n\t\t\trsp->schq[lvl] = 1;\n\t\t\tset_bit(schq, txsch->schq.bmap);\n\t\t\trsp->schq_list[lvl][0] = schq;\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tif (rsp->schq_contig[lvl]) {\n\t\tschq = bitmap_find_next_zero_area(txsch->schq.bmap,\n\t\t\t\t\t\t  txsch->schq.max, start,\n\t\t\t\t\t\t  rsp->schq_contig[lvl], 0);\n\t\tif (schq >= end)\n\t\t\trsp->schq_contig[lvl] = 0;\n\t\tfor (idx = 0; idx < rsp->schq_contig[lvl]; idx++) {\n\t\t\tset_bit(schq, txsch->schq.bmap);\n\t\t\trsp->schq_contig_list[lvl][idx] = schq;\n\t\t\tschq++;\n\t\t}\n\t}\n\n\t \n\tif (rsp->schq[lvl]) {\n\t\tidx = 0;\n\t\tfor (schq = start; schq < end; schq++) {\n\t\t\tif (!test_bit(schq, txsch->schq.bmap)) {\n\t\t\t\tset_bit(schq, txsch->schq.bmap);\n\t\t\t\trsp->schq_list[lvl][idx++] = schq;\n\t\t\t}\n\t\t\tif (idx == rsp->schq[lvl])\n\t\t\t\tbreak;\n\t\t}\n\t\t \n\t\trsp->schq[lvl] = idx;\n\t}\n}\n\nint rvu_mbox_handler_nix_txsch_alloc(struct rvu *rvu,\n\t\t\t\t     struct nix_txsch_alloc_req *req,\n\t\t\t\t     struct nix_txsch_alloc_rsp *rsp)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint link, blkaddr, rc = 0;\n\tint lvl, idx, start, end;\n\tstruct nix_txsch *txsch;\n\tstruct nix_hw *nix_hw;\n\tu32 *pfvf_map;\n\tint nixlf;\n\tu16 schq;\n\n\trc = nix_get_nixlf(rvu, pcifunc, &nixlf, &blkaddr);\n\tif (rc)\n\t\treturn rc;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tmutex_lock(&rvu->rsrc_lock);\n\n\t \n\tfor (lvl = 0; lvl < NIX_TXSCH_LVL_CNT; lvl++) {\n\t\trc = nix_check_txschq_alloc_req(rvu, lvl, pcifunc, nix_hw, req);\n\t\tif (rc)\n\t\t\tgoto err;\n\t}\n\n\t \n\tfor (lvl = 0; lvl < NIX_TXSCH_LVL_CNT; lvl++) {\n\t\ttxsch = &nix_hw->txsch[lvl];\n\t\tpfvf_map = txsch->pfvf_map;\n\n\t\tif (!req->schq[lvl] && !req->schq_contig[lvl])\n\t\t\tcontinue;\n\n\t\trsp->schq[lvl] = req->schq[lvl];\n\t\trsp->schq_contig[lvl] = req->schq_contig[lvl];\n\n\t\tlink = nix_get_tx_link(rvu, pcifunc);\n\n\t\tif (lvl >= hw->cap.nix_tx_aggr_lvl) {\n\t\t\tstart = link;\n\t\t\tend = link;\n\t\t} else if (hw->cap.nix_fixed_txschq_mapping) {\n\t\t\tnix_get_txschq_range(rvu, pcifunc, link, &start, &end);\n\t\t} else {\n\t\t\tstart = 0;\n\t\t\tend = txsch->schq.max;\n\t\t}\n\n\t\tnix_txsch_alloc(rvu, txsch, rsp, lvl, start, end);\n\n\t\t \n\t\tfor (idx = 0; idx < req->schq_contig[lvl]; idx++) {\n\t\t\tschq = rsp->schq_contig_list[lvl][idx];\n\t\t\tif (!(TXSCH_MAP_FLAGS(pfvf_map[schq]) &\n\t\t\t    NIX_TXSCHQ_CFG_DONE))\n\t\t\t\tpfvf_map[schq] = TXSCH_MAP(pcifunc, 0);\n\t\t\tnix_reset_tx_linkcfg(rvu, blkaddr, lvl, schq);\n\t\t\tnix_reset_tx_shaping(rvu, blkaddr, nixlf, lvl, schq);\n\t\t\tnix_reset_tx_schedule(rvu, blkaddr, lvl, schq);\n\t\t}\n\n\t\tfor (idx = 0; idx < req->schq[lvl]; idx++) {\n\t\t\tschq = rsp->schq_list[lvl][idx];\n\t\t\tif (!(TXSCH_MAP_FLAGS(pfvf_map[schq]) &\n\t\t\t    NIX_TXSCHQ_CFG_DONE))\n\t\t\t\tpfvf_map[schq] = TXSCH_MAP(pcifunc, 0);\n\t\t\tnix_reset_tx_linkcfg(rvu, blkaddr, lvl, schq);\n\t\t\tnix_reset_tx_shaping(rvu, blkaddr, nixlf, lvl, schq);\n\t\t\tnix_reset_tx_schedule(rvu, blkaddr, lvl, schq);\n\t\t}\n\t}\n\n\trsp->aggr_level = hw->cap.nix_tx_aggr_lvl;\n\trsp->aggr_lvl_rr_prio = TXSCH_TL1_DFLT_RR_PRIO;\n\trsp->link_cfg_lvl = rvu_read64(rvu, blkaddr,\n\t\t\t\t       NIX_AF_PSE_CHANNEL_LEVEL) & 0x01 ?\n\t\t\t\t       NIX_TXSCH_LVL_TL3 : NIX_TXSCH_LVL_TL2;\n\tgoto exit;\nerr:\n\trc = NIX_AF_ERR_TLX_ALLOC_FAIL;\nexit:\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn rc;\n}\n\nstatic void nix_smq_flush_fill_ctx(struct rvu *rvu, int blkaddr, int smq,\n\t\t\t\t   struct nix_smq_flush_ctx *smq_flush_ctx)\n{\n\tstruct nix_smq_tree_ctx *smq_tree_ctx;\n\tu64 parent_off, regval;\n\tu16 schq;\n\tint lvl;\n\n\tsmq_flush_ctx->smq = smq;\n\n\tschq = smq;\n\tfor (lvl = NIX_TXSCH_LVL_SMQ; lvl <= NIX_TXSCH_LVL_TL1; lvl++) {\n\t\tsmq_tree_ctx = &smq_flush_ctx->smq_tree_ctx[lvl];\n\t\tif (lvl == NIX_TXSCH_LVL_TL1) {\n\t\t\tsmq_flush_ctx->tl1_schq = schq;\n\t\t\tsmq_tree_ctx->cir_off = NIX_AF_TL1X_CIR(schq);\n\t\t\tsmq_tree_ctx->pir_off = 0;\n\t\t\tsmq_tree_ctx->pir_val = 0;\n\t\t\tparent_off = 0;\n\t\t} else if (lvl == NIX_TXSCH_LVL_TL2) {\n\t\t\tsmq_flush_ctx->tl2_schq = schq;\n\t\t\tsmq_tree_ctx->cir_off = NIX_AF_TL2X_CIR(schq);\n\t\t\tsmq_tree_ctx->pir_off = NIX_AF_TL2X_PIR(schq);\n\t\t\tparent_off = NIX_AF_TL2X_PARENT(schq);\n\t\t} else if (lvl == NIX_TXSCH_LVL_TL3) {\n\t\t\tsmq_tree_ctx->cir_off = NIX_AF_TL3X_CIR(schq);\n\t\t\tsmq_tree_ctx->pir_off = NIX_AF_TL3X_PIR(schq);\n\t\t\tparent_off = NIX_AF_TL3X_PARENT(schq);\n\t\t} else if (lvl == NIX_TXSCH_LVL_TL4) {\n\t\t\tsmq_tree_ctx->cir_off = NIX_AF_TL4X_CIR(schq);\n\t\t\tsmq_tree_ctx->pir_off = NIX_AF_TL4X_PIR(schq);\n\t\t\tparent_off = NIX_AF_TL4X_PARENT(schq);\n\t\t} else if (lvl == NIX_TXSCH_LVL_MDQ) {\n\t\t\tsmq_tree_ctx->cir_off = NIX_AF_MDQX_CIR(schq);\n\t\t\tsmq_tree_ctx->pir_off = NIX_AF_MDQX_PIR(schq);\n\t\t\tparent_off = NIX_AF_MDQX_PARENT(schq);\n\t\t}\n\t\t \n\t\tsmq_tree_ctx->cir_val = rvu_read64(rvu, blkaddr, smq_tree_ctx->cir_off);\n\t\tif (smq_tree_ctx->pir_off)\n\t\t\tsmq_tree_ctx->pir_val = rvu_read64(rvu, blkaddr, smq_tree_ctx->pir_off);\n\n\t\t \n\t\tif (parent_off) {\n\t\t\tregval = rvu_read64(rvu, blkaddr, parent_off);\n\t\t\tschq = (regval >> 16) & 0x1FF;\n\t\t}\n\t}\n}\n\nstatic void nix_smq_flush_enadis_xoff(struct rvu *rvu, int blkaddr,\n\t\t\t\t      struct nix_smq_flush_ctx *smq_flush_ctx, bool enable)\n{\n\tstruct nix_txsch *txsch;\n\tstruct nix_hw *nix_hw;\n\tu64 regoff;\n\tint tl2;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn;\n\n\t \n\ttxsch = &nix_hw->txsch[NIX_TXSCH_LVL_TL2];\n\tfor (tl2 = 0; tl2 < txsch->schq.max; tl2++) {\n\t\t \n\t\tif (tl2 == smq_flush_ctx->tl2_schq)\n\t\t\tcontinue;\n\t\t \n\t\tif (TXSCH_MAP_FLAGS(txsch->pfvf_map[tl2]) & NIX_TXSCHQ_FREE)\n\t\t\tcontinue;\n\t\t \n\t\tif ((TXSCH_MAP_FUNC(txsch->pfvf_map[tl2]) & ~RVU_PFVF_FUNC_MASK) !=\n\t\t    (TXSCH_MAP_FUNC(txsch->pfvf_map[smq_flush_ctx->tl2_schq] &\n\t\t\t\t    ~RVU_PFVF_FUNC_MASK)))\n\t\t\tcontinue;\n\t\t \n\t\tregoff = NIX_AF_TL2X_SW_XOFF(tl2);\n\t\tif (enable)\n\t\t\trvu_write64(rvu, blkaddr, regoff, 0x1);\n\t\telse\n\t\t\trvu_write64(rvu, blkaddr, regoff, 0x0);\n\t}\n}\n\nstatic void nix_smq_flush_enadis_rate(struct rvu *rvu, int blkaddr,\n\t\t\t\t      struct nix_smq_flush_ctx *smq_flush_ctx, bool enable)\n{\n\tu64 cir_off, pir_off, cir_val, pir_val;\n\tstruct nix_smq_tree_ctx *smq_tree_ctx;\n\tint lvl;\n\n\tfor (lvl = NIX_TXSCH_LVL_SMQ; lvl <= NIX_TXSCH_LVL_TL1; lvl++) {\n\t\tsmq_tree_ctx = &smq_flush_ctx->smq_tree_ctx[lvl];\n\t\tcir_off = smq_tree_ctx->cir_off;\n\t\tcir_val = smq_tree_ctx->cir_val;\n\t\tpir_off = smq_tree_ctx->pir_off;\n\t\tpir_val = smq_tree_ctx->pir_val;\n\n\t\tif (enable) {\n\t\t\trvu_write64(rvu, blkaddr, cir_off, cir_val);\n\t\t\tif (lvl != NIX_TXSCH_LVL_TL1)\n\t\t\t\trvu_write64(rvu, blkaddr, pir_off, pir_val);\n\t\t} else {\n\t\t\trvu_write64(rvu, blkaddr, cir_off, 0x0);\n\t\t\tif (lvl != NIX_TXSCH_LVL_TL1)\n\t\t\t\trvu_write64(rvu, blkaddr, pir_off, 0x0);\n\t\t}\n\t}\n}\n\nstatic int nix_smq_flush(struct rvu *rvu, int blkaddr,\n\t\t\t int smq, u16 pcifunc, int nixlf)\n{\n\tstruct nix_smq_flush_ctx *smq_flush_ctx;\n\tint pf = rvu_get_pf(pcifunc);\n\tu8 cgx_id = 0, lmac_id = 0;\n\tint err, restore_tx_en = 0;\n\tu64 cfg;\n\n\tif (!is_rvu_otx2(rvu)) {\n\t\t \n\t\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_MDQX_IN_MD_COUNT(smq));\n\t\tif (!cfg)\n\t\t\treturn 0;\n\t}\n\n\t \n\tif (is_pf_cgxmapped(rvu, pf)) {\n\t\trvu_get_cgx_lmac_id(rvu->pf2cgxlmac_map[pf], &cgx_id, &lmac_id);\n\t\trestore_tx_en = !rvu_cgx_config_tx(rvu_cgx_pdata(cgx_id, rvu),\n\t\t\t\t\t\t   lmac_id, true);\n\t}\n\n\t \n\tsmq_flush_ctx = kzalloc(sizeof(*smq_flush_ctx), GFP_KERNEL);\n\tif (!smq_flush_ctx)\n\t\treturn -ENOMEM;\n\tnix_smq_flush_fill_ctx(rvu, blkaddr, smq, smq_flush_ctx);\n\tnix_smq_flush_enadis_xoff(rvu, blkaddr, smq_flush_ctx, true);\n\tnix_smq_flush_enadis_rate(rvu, blkaddr, smq_flush_ctx, false);\n\n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_SMQX_CFG(smq));\n\t \n\tcfg |= BIT_ULL(50) | BIT_ULL(49);\n\trvu_write64(rvu, blkaddr, NIX_AF_SMQX_CFG(smq), cfg);\n\n\t \n\trvu_cgx_enadis_rx_bp(rvu, pf, false);\n\n\t \n\terr = rvu_poll_reg(rvu, blkaddr,\n\t\t\t   NIX_AF_SMQX_CFG(smq), BIT_ULL(49), true);\n\tif (err)\n\t\tdev_info(rvu->dev,\n\t\t\t \"NIXLF%d: SMQ%d flush failed, txlink might be busy\\n\",\n\t\t\t nixlf, smq);\n\n\t \n\tnix_smq_flush_enadis_rate(rvu, blkaddr, smq_flush_ctx, true);\n\tnix_smq_flush_enadis_xoff(rvu, blkaddr, smq_flush_ctx, false);\n\tkfree(smq_flush_ctx);\n\n\trvu_cgx_enadis_rx_bp(rvu, pf, true);\n\t \n\tif (restore_tx_en)\n\t\trvu_cgx_config_tx(rvu_cgx_pdata(cgx_id, rvu), lmac_id, false);\n\treturn err;\n}\n\nstatic int nix_txschq_free(struct rvu *rvu, u16 pcifunc)\n{\n\tint blkaddr, nixlf, lvl, schq, err;\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct nix_txsch *txsch;\n\tstruct nix_hw *nix_hw;\n\tu16 map_func;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tnixlf = rvu_get_lf(rvu, &hw->block[blkaddr], pcifunc, 0);\n\tif (nixlf < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\t \n\tmutex_lock(&rvu->rsrc_lock);\n\tfor (lvl = NIX_TXSCH_LVL_MDQ; lvl < NIX_TXSCH_LVL_CNT; lvl++) {\n\t\ttxsch = &nix_hw->txsch[lvl];\n\n\t\tif (lvl >= hw->cap.nix_tx_aggr_lvl)\n\t\t\tcontinue;\n\n\t\tfor (schq = 0; schq < txsch->schq.max; schq++) {\n\t\t\tif (TXSCH_MAP_FUNC(txsch->pfvf_map[schq]) != pcifunc)\n\t\t\t\tcontinue;\n\t\t\tnix_reset_tx_linkcfg(rvu, blkaddr, lvl, schq);\n\t\t\tnix_clear_tx_xoff(rvu, blkaddr, lvl, schq);\n\t\t\tnix_reset_tx_shaping(rvu, blkaddr, nixlf, lvl, schq);\n\t\t}\n\t}\n\tnix_clear_tx_xoff(rvu, blkaddr, NIX_TXSCH_LVL_TL1,\n\t\t\t  nix_get_tx_link(rvu, pcifunc));\n\n\t \n\tif (!(pcifunc & RVU_PFVF_FUNC_MASK)) {\n\t\ttxsch = &nix_hw->txsch[NIX_TXSCH_LVL_TL1];\n\t\tschq = nix_get_tx_link(rvu, pcifunc);\n\t\t \n\t\tmap_func = TXSCH_MAP_FUNC(txsch->pfvf_map[schq]);\n\t\ttxsch->pfvf_map[schq] = TXSCH_SET_FLAG(map_func, 0x0);\n\t}\n\n\t \n\ttxsch = &nix_hw->txsch[NIX_TXSCH_LVL_SMQ];\n\tfor (schq = 0; schq < txsch->schq.max; schq++) {\n\t\tif (TXSCH_MAP_FUNC(txsch->pfvf_map[schq]) != pcifunc)\n\t\t\tcontinue;\n\t\tnix_smq_flush(rvu, blkaddr, schq, pcifunc, nixlf);\n\t}\n\n\t \n\tfor (lvl = 0; lvl < NIX_TXSCH_LVL_CNT; lvl++) {\n\t\t  \n\t\tif (lvl >= hw->cap.nix_tx_aggr_lvl)\n\t\t\tcontinue;\n\n\t\ttxsch = &nix_hw->txsch[lvl];\n\t\tfor (schq = 0; schq < txsch->schq.max; schq++) {\n\t\t\tif (TXSCH_MAP_FUNC(txsch->pfvf_map[schq]) != pcifunc)\n\t\t\t\tcontinue;\n\t\t\tnix_reset_tx_schedule(rvu, blkaddr, lvl, schq);\n\t\t\trvu_free_rsrc(&txsch->schq, schq);\n\t\t\ttxsch->pfvf_map[schq] = TXSCH_MAP(0, NIX_TXSCHQ_FREE);\n\t\t}\n\t}\n\tmutex_unlock(&rvu->rsrc_lock);\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_NDC_TX_SYNC, BIT_ULL(12) | nixlf);\n\terr = rvu_poll_reg(rvu, blkaddr, NIX_AF_NDC_TX_SYNC, BIT_ULL(12), true);\n\tif (err)\n\t\tdev_err(rvu->dev, \"NDC-TX sync failed for NIXLF %d\\n\", nixlf);\n\n\treturn 0;\n}\n\nstatic int nix_txschq_free_one(struct rvu *rvu,\n\t\t\t       struct nix_txsch_free_req *req)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint lvl, schq, nixlf, blkaddr;\n\tstruct nix_txsch *txsch;\n\tstruct nix_hw *nix_hw;\n\tu32 *pfvf_map;\n\tint rc;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tnixlf = rvu_get_lf(rvu, &hw->block[blkaddr], pcifunc, 0);\n\tif (nixlf < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tlvl = req->schq_lvl;\n\tschq = req->schq;\n\ttxsch = &nix_hw->txsch[lvl];\n\n\tif (lvl >= hw->cap.nix_tx_aggr_lvl || schq >= txsch->schq.max)\n\t\treturn 0;\n\n\tpfvf_map = txsch->pfvf_map;\n\tmutex_lock(&rvu->rsrc_lock);\n\n\tif (TXSCH_MAP_FUNC(pfvf_map[schq]) != pcifunc) {\n\t\trc = NIX_AF_ERR_TLX_INVALID;\n\t\tgoto err;\n\t}\n\n\t \n\tnix_clear_tx_xoff(rvu, blkaddr, lvl, schq);\n\n\tnix_reset_tx_linkcfg(rvu, blkaddr, lvl, schq);\n\tnix_reset_tx_shaping(rvu, blkaddr, nixlf, lvl, schq);\n\n\t \n\tif (lvl == NIX_TXSCH_LVL_SMQ &&\n\t    nix_smq_flush(rvu, blkaddr, schq, pcifunc, nixlf)) {\n\t\trc = NIX_AF_SMQ_FLUSH_FAILED;\n\t\tgoto err;\n\t}\n\n\tnix_reset_tx_schedule(rvu, blkaddr, lvl, schq);\n\n\t \n\trvu_free_rsrc(&txsch->schq, schq);\n\ttxsch->pfvf_map[schq] = TXSCH_MAP(0, NIX_TXSCHQ_FREE);\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn 0;\nerr:\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn rc;\n}\n\nint rvu_mbox_handler_nix_txsch_free(struct rvu *rvu,\n\t\t\t\t    struct nix_txsch_free_req *req,\n\t\t\t\t    struct msg_rsp *rsp)\n{\n\tif (req->flags & TXSCHQ_FREE_ALL)\n\t\treturn nix_txschq_free(rvu, req->hdr.pcifunc);\n\telse\n\t\treturn nix_txschq_free_one(rvu, req);\n}\n\nstatic bool is_txschq_hierarchy_valid(struct rvu *rvu, u16 pcifunc, int blkaddr,\n\t\t\t\t      int lvl, u64 reg, u64 regval)\n{\n\tu64 regbase = reg & 0xFFFF;\n\tu16 schq, parent;\n\n\tif (!rvu_check_valid_reg(TXSCHQ_HWREGMAP, lvl, reg))\n\t\treturn false;\n\n\tschq = TXSCHQ_IDX(reg, TXSCHQ_IDX_SHIFT);\n\t \n\tif (!is_valid_txschq(rvu, blkaddr, lvl, pcifunc, schq))\n\t\treturn false;\n\n\tparent = (regval >> 16) & 0x1FF;\n\t \n\tif (regbase == NIX_AF_MDQX_PARENT(0) &&\n\t    !is_valid_txschq(rvu, blkaddr, NIX_TXSCH_LVL_TL4, pcifunc, parent))\n\t\treturn false;\n\n\t \n\tif (regbase == NIX_AF_TL4X_PARENT(0) &&\n\t    !is_valid_txschq(rvu, blkaddr, NIX_TXSCH_LVL_TL3, pcifunc, parent))\n\t\treturn false;\n\n\t \n\tif (regbase == NIX_AF_TL3X_PARENT(0) &&\n\t    !is_valid_txschq(rvu, blkaddr, NIX_TXSCH_LVL_TL2, pcifunc, parent))\n\t\treturn false;\n\n\t \n\tif (regbase == NIX_AF_TL2X_PARENT(0) &&\n\t    !is_valid_txschq(rvu, blkaddr, NIX_TXSCH_LVL_TL1, pcifunc, parent))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool is_txschq_shaping_valid(struct rvu_hwinfo *hw, int lvl, u64 reg)\n{\n\tu64 regbase;\n\n\tif (hw->cap.nix_shaping)\n\t\treturn true;\n\n\t \n\tregbase = reg & 0xFFFF;\n\n\tswitch (lvl) {\n\tcase NIX_TXSCH_LVL_TL1:\n\t\tif (regbase == NIX_AF_TL1X_CIR(0))\n\t\t\treturn false;\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL2:\n\t\tif (regbase == NIX_AF_TL2X_CIR(0) ||\n\t\t    regbase == NIX_AF_TL2X_PIR(0))\n\t\t\treturn false;\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL3:\n\t\tif (regbase == NIX_AF_TL3X_CIR(0) ||\n\t\t    regbase == NIX_AF_TL3X_PIR(0))\n\t\t\treturn false;\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_TL4:\n\t\tif (regbase == NIX_AF_TL4X_CIR(0) ||\n\t\t    regbase == NIX_AF_TL4X_PIR(0))\n\t\t\treturn false;\n\t\tbreak;\n\tcase NIX_TXSCH_LVL_MDQ:\n\t\tif (regbase == NIX_AF_MDQX_CIR(0) ||\n\t\t    regbase == NIX_AF_MDQX_PIR(0))\n\t\t\treturn false;\n\t\tbreak;\n\t}\n\treturn true;\n}\n\nstatic void nix_tl1_default_cfg(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t\tu16 pcifunc, int blkaddr)\n{\n\tu32 *pfvf_map;\n\tint schq;\n\n\tschq = nix_get_tx_link(rvu, pcifunc);\n\tpfvf_map = nix_hw->txsch[NIX_TXSCH_LVL_TL1].pfvf_map;\n\t \n\tif (TXSCH_MAP_FLAGS(pfvf_map[schq]) & NIX_TXSCHQ_CFG_DONE)\n\t\treturn;\n\trvu_write64(rvu, blkaddr, NIX_AF_TL1X_TOPOLOGY(schq),\n\t\t    (TXSCH_TL1_DFLT_RR_PRIO << 1));\n\n\t \n\tif (!rvu->hw->cap.nix_common_dwrr_mtu)\n\t\trvu_write64(rvu, blkaddr, NIX_AF_TL1X_SCHEDULE(schq),\n\t\t\t    TXSCH_TL1_DFLT_RR_QTM);\n\telse\n\t\trvu_write64(rvu, blkaddr, NIX_AF_TL1X_SCHEDULE(schq),\n\t\t\t    CN10K_MAX_DWRR_WEIGHT);\n\n\trvu_write64(rvu, blkaddr, NIX_AF_TL1X_CIR(schq), 0x00);\n\tpfvf_map[schq] = TXSCH_SET_FLAG(pfvf_map[schq], NIX_TXSCHQ_CFG_DONE);\n}\n\n \n#define NIX_TX_SCHQ_MASK\tGENMASK_ULL(25, 0)\n\nstatic int nix_txschq_cfg_read(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t       int blkaddr, struct nix_txschq_config *req,\n\t\t\t       struct nix_txschq_config *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint idx, schq;\n\tu64 reg;\n\n\tfor (idx = 0; idx < req->num_regs; idx++) {\n\t\treg = req->reg[idx];\n\t\treg &= NIX_TX_SCHQ_MASK;\n\t\tschq = TXSCHQ_IDX(reg, TXSCHQ_IDX_SHIFT);\n\t\tif (!rvu_check_valid_reg(TXSCHQ_HWREGMAP, req->lvl, reg) ||\n\t\t    !is_valid_txschq(rvu, blkaddr, req->lvl, pcifunc, schq))\n\t\t\treturn NIX_AF_INVAL_TXSCHQ_CFG;\n\t\trsp->regval[idx] = rvu_read64(rvu, blkaddr, reg);\n\t}\n\trsp->lvl = req->lvl;\n\trsp->num_regs = req->num_regs;\n\treturn 0;\n}\n\nvoid rvu_nix_tx_tl2_cfg(struct rvu *rvu, int blkaddr, u16 pcifunc,\n\t\t\tstruct nix_txsch *txsch, bool enable)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint lbk_link_start, lbk_links;\n\tu8 pf = rvu_get_pf(pcifunc);\n\tint schq;\n\tu64 cfg;\n\n\tif (!is_pf_cgxmapped(rvu, pf))\n\t\treturn;\n\n\tcfg = enable ? (BIT_ULL(12) | RVU_SWITCH_LBK_CHAN) : 0;\n\tlbk_link_start = hw->cgx_links;\n\n\tfor (schq = 0; schq < txsch->schq.max; schq++) {\n\t\tif (TXSCH_MAP_FUNC(txsch->pfvf_map[schq]) != pcifunc)\n\t\t\tcontinue;\n\t\t \n\t\tlbk_links = hw->lbk_links;\n\t\twhile (lbk_links--)\n\t\t\trvu_write64(rvu, blkaddr,\n\t\t\t\t    NIX_AF_TL3_TL2X_LINKX_CFG(schq,\n\t\t\t\t\t\t\t      lbk_link_start +\n\t\t\t\t\t\t\t      lbk_links), cfg);\n\t}\n}\n\nint rvu_mbox_handler_nix_txschq_cfg(struct rvu *rvu,\n\t\t\t\t    struct nix_txschq_config *req,\n\t\t\t\t    struct nix_txschq_config *rsp)\n{\n\tu64 reg, val, regval, schq_regbase, val_mask;\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct nix_txsch *txsch;\n\tstruct nix_hw *nix_hw;\n\tint blkaddr, idx, err;\n\tint nixlf, schq;\n\tu32 *pfvf_map;\n\n\tif (req->lvl >= NIX_TXSCH_LVL_CNT ||\n\t    req->num_regs > MAX_REGS_PER_MBOX_MSG)\n\t\treturn NIX_AF_INVAL_TXSCHQ_CFG;\n\n\terr = nix_get_nixlf(rvu, pcifunc, &nixlf, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tif (req->read)\n\t\treturn nix_txschq_cfg_read(rvu, nix_hw, blkaddr, req, rsp);\n\n\ttxsch = &nix_hw->txsch[req->lvl];\n\tpfvf_map = txsch->pfvf_map;\n\n\tif (req->lvl >= hw->cap.nix_tx_aggr_lvl &&\n\t    pcifunc & RVU_PFVF_FUNC_MASK) {\n\t\tmutex_lock(&rvu->rsrc_lock);\n\t\tif (req->lvl == NIX_TXSCH_LVL_TL1)\n\t\t\tnix_tl1_default_cfg(rvu, nix_hw, pcifunc, blkaddr);\n\t\tmutex_unlock(&rvu->rsrc_lock);\n\t\treturn 0;\n\t}\n\n\tfor (idx = 0; idx < req->num_regs; idx++) {\n\t\treg = req->reg[idx];\n\t\treg &= NIX_TX_SCHQ_MASK;\n\t\tregval = req->regval[idx];\n\t\tschq_regbase = reg & 0xFFFF;\n\t\tval_mask = req->regval_mask[idx];\n\n\t\tif (!is_txschq_hierarchy_valid(rvu, pcifunc, blkaddr,\n\t\t\t\t\t       txsch->lvl, reg, regval))\n\t\t\treturn NIX_AF_INVAL_TXSCHQ_CFG;\n\n\t\t \n\t\tif (!is_txschq_shaping_valid(hw, req->lvl, reg))\n\t\t\tcontinue;\n\n\t\tval = rvu_read64(rvu, blkaddr, reg);\n\t\tregval = (val & val_mask) | (regval & ~val_mask);\n\n\t\t \n\t\tif (hw->cap.nix_shaper_toggle_wait &&\n\t\t    handle_txschq_shaper_update(rvu, blkaddr, nixlf,\n\t\t\t\t\t\treq->lvl, reg, regval))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (schq_regbase == NIX_AF_SMQX_CFG(0)) {\n\t\t\tnixlf = rvu_get_lf(rvu, &hw->block[blkaddr],\n\t\t\t\t\t   pcifunc, 0);\n\t\t\tregval &= ~(0x7FULL << 24);\n\t\t\tregval |= ((u64)nixlf << 24);\n\t\t}\n\n\t\t \n\t\tif (!hw->cap.nix_tx_link_bp) {\n\t\t\tif (schq_regbase == NIX_AF_TL4X_SDP_LINK_CFG(0) ||\n\t\t\t    (schq_regbase & 0xFF00) ==\n\t\t\t    NIX_AF_TL3_TL2X_LINKX_CFG(0, 0))\n\t\t\t\tregval &= ~BIT_ULL(13);\n\t\t}\n\n\t\t \n\t\tif (schq_regbase >= NIX_AF_TL1X_SCHEDULE(0) &&\n\t\t    schq_regbase <= NIX_AF_TL1X_GREEN_BYTES(0)) {\n\t\t\tschq = TXSCHQ_IDX(reg, TXSCHQ_IDX_SHIFT);\n\t\t\tmutex_lock(&rvu->rsrc_lock);\n\t\t\tpfvf_map[schq] = TXSCH_SET_FLAG(pfvf_map[schq],\n\t\t\t\t\t\t\tNIX_TXSCHQ_CFG_DONE);\n\t\t\tmutex_unlock(&rvu->rsrc_lock);\n\t\t}\n\n\t\t \n\t\tif (schq_regbase == NIX_AF_SMQX_CFG(0) &&\n\t\t    (regval & BIT_ULL(49))) {\n\t\t\tschq = TXSCHQ_IDX(reg, TXSCHQ_IDX_SHIFT);\n\t\t\tnix_smq_flush(rvu, blkaddr, schq, pcifunc, nixlf);\n\t\t\tregval &= ~BIT_ULL(49);\n\t\t}\n\t\trvu_write64(rvu, blkaddr, reg, regval);\n\t}\n\n\treturn 0;\n}\n\nstatic int nix_rx_vtag_cfg(struct rvu *rvu, int nixlf, int blkaddr,\n\t\t\t   struct nix_vtag_config *req)\n{\n\tu64 regval = req->vtag_size;\n\n\tif (req->rx.vtag_type > NIX_AF_LFX_RX_VTAG_TYPE7 ||\n\t    req->vtag_size > VTAGSIZE_T8)\n\t\treturn -EINVAL;\n\n\t \n\tif (req->rx.vtag_type == NIX_AF_LFX_RX_VTAG_TYPE7)\n\t\treturn NIX_AF_ERR_RX_VTAG_INUSE;\n\n\tif (req->rx.capture_vtag)\n\t\tregval |= BIT_ULL(5);\n\tif (req->rx.strip_vtag)\n\t\tregval |= BIT_ULL(4);\n\n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_LFX_RX_VTAG_TYPEX(nixlf, req->rx.vtag_type), regval);\n\treturn 0;\n}\n\nstatic int nix_tx_vtag_free(struct rvu *rvu, int blkaddr,\n\t\t\t    u16 pcifunc, int index)\n{\n\tstruct nix_hw *nix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tstruct nix_txvlan *vlan;\n\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tvlan = &nix_hw->txvlan;\n\tif (vlan->entry2pfvf_map[index] != pcifunc)\n\t\treturn NIX_AF_ERR_PARAM;\n\n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_TX_VTAG_DEFX_DATA(index), 0x0ull);\n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_TX_VTAG_DEFX_CTL(index), 0x0ull);\n\n\tvlan->entry2pfvf_map[index] = 0;\n\trvu_free_rsrc(&vlan->rsrc, index);\n\n\treturn 0;\n}\n\nstatic void nix_free_tx_vtag_entries(struct rvu *rvu, u16 pcifunc)\n{\n\tstruct nix_txvlan *vlan;\n\tstruct nix_hw *nix_hw;\n\tint index, blkaddr;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (blkaddr < 0)\n\t\treturn;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn;\n\n\tvlan = &nix_hw->txvlan;\n\n\tmutex_lock(&vlan->rsrc_lock);\n\t \n\tfor (index = 0; index < vlan->rsrc.max; index++) {\n\t\tif (vlan->entry2pfvf_map[index] == pcifunc)\n\t\t\tnix_tx_vtag_free(rvu, blkaddr, pcifunc, index);\n\t}\n\tmutex_unlock(&vlan->rsrc_lock);\n}\n\nstatic int nix_tx_vtag_alloc(struct rvu *rvu, int blkaddr,\n\t\t\t     u64 vtag, u8 size)\n{\n\tstruct nix_hw *nix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tstruct nix_txvlan *vlan;\n\tu64 regval;\n\tint index;\n\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tvlan = &nix_hw->txvlan;\n\n\tmutex_lock(&vlan->rsrc_lock);\n\n\tindex = rvu_alloc_rsrc(&vlan->rsrc);\n\tif (index < 0) {\n\t\tmutex_unlock(&vlan->rsrc_lock);\n\t\treturn index;\n\t}\n\n\tmutex_unlock(&vlan->rsrc_lock);\n\n\tregval = size ? vtag : vtag << 32;\n\n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_TX_VTAG_DEFX_DATA(index), regval);\n\trvu_write64(rvu, blkaddr,\n\t\t    NIX_AF_TX_VTAG_DEFX_CTL(index), size);\n\n\treturn index;\n}\n\nstatic int nix_tx_vtag_decfg(struct rvu *rvu, int blkaddr,\n\t\t\t     struct nix_vtag_config *req)\n{\n\tstruct nix_hw *nix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint idx0 = req->tx.vtag0_idx;\n\tint idx1 = req->tx.vtag1_idx;\n\tstruct nix_txvlan *vlan;\n\tint err = 0;\n\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tvlan = &nix_hw->txvlan;\n\tif (req->tx.free_vtag0 && req->tx.free_vtag1)\n\t\tif (vlan->entry2pfvf_map[idx0] != pcifunc ||\n\t\t    vlan->entry2pfvf_map[idx1] != pcifunc)\n\t\t\treturn NIX_AF_ERR_PARAM;\n\n\tmutex_lock(&vlan->rsrc_lock);\n\n\tif (req->tx.free_vtag0) {\n\t\terr = nix_tx_vtag_free(rvu, blkaddr, pcifunc, idx0);\n\t\tif (err)\n\t\t\tgoto exit;\n\t}\n\n\tif (req->tx.free_vtag1)\n\t\terr = nix_tx_vtag_free(rvu, blkaddr, pcifunc, idx1);\n\nexit:\n\tmutex_unlock(&vlan->rsrc_lock);\n\treturn err;\n}\n\nstatic int nix_tx_vtag_cfg(struct rvu *rvu, int blkaddr,\n\t\t\t   struct nix_vtag_config *req,\n\t\t\t   struct nix_vtag_config_rsp *rsp)\n{\n\tstruct nix_hw *nix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tstruct nix_txvlan *vlan;\n\tu16 pcifunc = req->hdr.pcifunc;\n\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tvlan = &nix_hw->txvlan;\n\tif (req->tx.cfg_vtag0) {\n\t\trsp->vtag0_idx =\n\t\t\tnix_tx_vtag_alloc(rvu, blkaddr,\n\t\t\t\t\t  req->tx.vtag0, req->vtag_size);\n\n\t\tif (rsp->vtag0_idx < 0)\n\t\t\treturn NIX_AF_ERR_TX_VTAG_NOSPC;\n\n\t\tvlan->entry2pfvf_map[rsp->vtag0_idx] = pcifunc;\n\t}\n\n\tif (req->tx.cfg_vtag1) {\n\t\trsp->vtag1_idx =\n\t\t\tnix_tx_vtag_alloc(rvu, blkaddr,\n\t\t\t\t\t  req->tx.vtag1, req->vtag_size);\n\n\t\tif (rsp->vtag1_idx < 0)\n\t\t\tgoto err_free;\n\n\t\tvlan->entry2pfvf_map[rsp->vtag1_idx] = pcifunc;\n\t}\n\n\treturn 0;\n\nerr_free:\n\tif (req->tx.cfg_vtag0)\n\t\tnix_tx_vtag_free(rvu, blkaddr, pcifunc, rsp->vtag0_idx);\n\n\treturn NIX_AF_ERR_TX_VTAG_NOSPC;\n}\n\nint rvu_mbox_handler_nix_vtag_cfg(struct rvu *rvu,\n\t\t\t\t  struct nix_vtag_config *req,\n\t\t\t\t  struct nix_vtag_config_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint blkaddr, nixlf, err;\n\n\terr = nix_get_nixlf(rvu, pcifunc, &nixlf, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tif (req->cfg_type) {\n\t\t \n\t\terr = nix_rx_vtag_cfg(rvu, nixlf, blkaddr, req);\n\t\tif (err)\n\t\t\treturn NIX_AF_ERR_PARAM;\n\t} else {\n\t\t \n\t\tif ((req->tx.cfg_vtag0 || req->tx.cfg_vtag1) &&\n\t\t    (req->tx.free_vtag0 || req->tx.free_vtag1))\n\t\t\treturn NIX_AF_ERR_PARAM;\n\n\t\tif (req->tx.cfg_vtag0 || req->tx.cfg_vtag1)\n\t\t\treturn nix_tx_vtag_cfg(rvu, blkaddr, req, rsp);\n\n\t\tif (req->tx.free_vtag0 || req->tx.free_vtag1)\n\t\t\treturn nix_tx_vtag_decfg(rvu, blkaddr, req);\n\t}\n\n\treturn 0;\n}\n\nstatic int nix_blk_setup_mce(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t     int mce, u8 op, u16 pcifunc, int next, bool eol)\n{\n\tstruct nix_aq_enq_req aq_req;\n\tint err;\n\n\taq_req.hdr.pcifunc = 0;\n\taq_req.ctype = NIX_AQ_CTYPE_MCE;\n\taq_req.op = op;\n\taq_req.qidx = mce;\n\n\t \n\taq_req.mce.op = 1;\n\taq_req.mce.index = 0;\n\taq_req.mce.eol = eol;\n\taq_req.mce.pf_func = pcifunc;\n\taq_req.mce.next = next;\n\n\t \n\t*(u64 *)(&aq_req.mce_mask) = ~0ULL;\n\n\terr = rvu_nix_blk_aq_enq_inst(rvu, nix_hw, &aq_req, NULL);\n\tif (err) {\n\t\tdev_err(rvu->dev, \"Failed to setup Bcast MCE for PF%d:VF%d\\n\",\n\t\t\trvu_get_pf(pcifunc), pcifunc & RVU_PFVF_FUNC_MASK);\n\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic int nix_update_mce_list_entry(struct nix_mce_list *mce_list,\n\t\t\t\t     u16 pcifunc, bool add)\n{\n\tstruct mce *mce, *tail = NULL;\n\tbool delete = false;\n\n\t \n\thlist_for_each_entry(mce, &mce_list->head, node) {\n\t\t \n\t\tif (mce->pcifunc == pcifunc && !add) {\n\t\t\tdelete = true;\n\t\t\tbreak;\n\t\t} else if (mce->pcifunc == pcifunc && add) {\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\t\ttail = mce;\n\t}\n\n\tif (delete) {\n\t\thlist_del(&mce->node);\n\t\tkfree(mce);\n\t\tmce_list->count--;\n\t\treturn 0;\n\t}\n\n\tif (!add)\n\t\treturn 0;\n\n\t \n\tmce = kzalloc(sizeof(*mce), GFP_KERNEL);\n\tif (!mce)\n\t\treturn -ENOMEM;\n\tmce->pcifunc = pcifunc;\n\tif (!tail)\n\t\thlist_add_head(&mce->node, &mce_list->head);\n\telse\n\t\thlist_add_behind(&mce->node, &tail->node);\n\tmce_list->count++;\n\treturn 0;\n}\n\nint nix_update_mce_list(struct rvu *rvu, u16 pcifunc,\n\t\t\tstruct nix_mce_list *mce_list,\n\t\t\tint mce_idx, int mcam_index, bool add)\n{\n\tint err = 0, idx, next_idx, last_idx, blkaddr, npc_blkaddr;\n\tstruct npc_mcam *mcam = &rvu->hw->mcam;\n\tstruct nix_mcast *mcast;\n\tstruct nix_hw *nix_hw;\n\tstruct mce *mce;\n\n\tif (!mce_list)\n\t\treturn -EINVAL;\n\n\t \n\tidx = mce_idx + (pcifunc & RVU_PFVF_FUNC_MASK);\n\n\tif (idx > (mce_idx + mce_list->max)) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Idx %d > max MCE idx %d, for PF%d bcast list\\n\",\n\t\t\t__func__, idx, mce_list->max,\n\t\t\tpcifunc >> RVU_PFVF_PF_SHIFT);\n\t\treturn -EINVAL;\n\t}\n\n\terr = nix_get_struct_ptrs(rvu, pcifunc, &nix_hw, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tmcast = &nix_hw->mcast;\n\tmutex_lock(&mcast->mce_lock);\n\n\terr = nix_update_mce_list_entry(mce_list, pcifunc, add);\n\tif (err)\n\t\tgoto end;\n\n\t \n\tif (!mce_list->count) {\n\t\tnpc_blkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NPC, 0);\n\t\tnpc_enable_mcam_entry(rvu, mcam, npc_blkaddr, mcam_index, false);\n\t\tgoto end;\n\t}\n\n\t \n\tidx = mce_idx;\n\tlast_idx = idx + mce_list->count - 1;\n\thlist_for_each_entry(mce, &mce_list->head, node) {\n\t\tif (idx > last_idx)\n\t\t\tbreak;\n\n\t\tnext_idx = idx + 1;\n\t\t \n\t\terr = nix_blk_setup_mce(rvu, nix_hw, idx, NIX_AQ_INSTOP_WRITE,\n\t\t\t\t\tmce->pcifunc, next_idx,\n\t\t\t\t\t(next_idx > last_idx) ? true : false);\n\t\tif (err)\n\t\t\tgoto end;\n\t\tidx++;\n\t}\n\nend:\n\tmutex_unlock(&mcast->mce_lock);\n\treturn err;\n}\n\nvoid nix_get_mce_list(struct rvu *rvu, u16 pcifunc, int type,\n\t\t      struct nix_mce_list **mce_list, int *mce_idx)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct rvu_pfvf *pfvf;\n\n\tif (!hw->cap.nix_rx_multicast ||\n\t    !is_pf_cgxmapped(rvu, rvu_get_pf(pcifunc & ~RVU_PFVF_FUNC_MASK))) {\n\t\t*mce_list = NULL;\n\t\t*mce_idx = 0;\n\t\treturn;\n\t}\n\n\t \n\tpfvf = rvu_get_pfvf(rvu, pcifunc & ~RVU_PFVF_FUNC_MASK);\n\n\tif (type == NIXLF_BCAST_ENTRY) {\n\t\t*mce_list = &pfvf->bcast_mce_list;\n\t\t*mce_idx = pfvf->bcast_mce_idx;\n\t} else if (type == NIXLF_ALLMULTI_ENTRY) {\n\t\t*mce_list = &pfvf->mcast_mce_list;\n\t\t*mce_idx = pfvf->mcast_mce_idx;\n\t} else if (type == NIXLF_PROMISC_ENTRY) {\n\t\t*mce_list = &pfvf->promisc_mce_list;\n\t\t*mce_idx = pfvf->promisc_mce_idx;\n\t}  else {\n\t\t*mce_list = NULL;\n\t\t*mce_idx = 0;\n\t}\n}\n\nstatic int nix_update_mce_rule(struct rvu *rvu, u16 pcifunc,\n\t\t\t       int type, bool add)\n{\n\tint err = 0, nixlf, blkaddr, mcam_index, mce_idx;\n\tstruct npc_mcam *mcam = &rvu->hw->mcam;\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct nix_mce_list *mce_list;\n\tint pf;\n\n\t \n\tif (is_afvf(pcifunc) || is_sdp_pfvf(pcifunc))\n\t\treturn 0;\n\n\tif (!hw->cap.nix_rx_multicast)\n\t\treturn 0;\n\n\tpf = rvu_get_pf(pcifunc);\n\tif (!is_pf_cgxmapped(rvu, pf))\n\t\treturn 0;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (blkaddr < 0)\n\t\treturn -EINVAL;\n\n\tnixlf = rvu_get_lf(rvu, &hw->block[blkaddr], pcifunc, 0);\n\tif (nixlf < 0)\n\t\treturn -EINVAL;\n\n\tnix_get_mce_list(rvu, pcifunc, type, &mce_list, &mce_idx);\n\n\tmcam_index = npc_get_nixlf_mcam_index(mcam,\n\t\t\t\t\t      pcifunc & ~RVU_PFVF_FUNC_MASK,\n\t\t\t\t\t      nixlf, type);\n\terr = nix_update_mce_list(rvu, pcifunc, mce_list,\n\t\t\t\t  mce_idx, mcam_index, add);\n\treturn err;\n}\n\nstatic int nix_setup_mce_tables(struct rvu *rvu, struct nix_hw *nix_hw)\n{\n\tstruct nix_mcast *mcast = &nix_hw->mcast;\n\tint err, pf, numvfs, idx;\n\tstruct rvu_pfvf *pfvf;\n\tu16 pcifunc;\n\tu64 cfg;\n\n\t \n\tfor (pf = 1; pf < (rvu->cgx_mapped_pfs + 1); pf++) {\n\t\tcfg = rvu_read64(rvu, BLKADDR_RVUM, RVU_PRIV_PFX_CFG(pf));\n\t\t \n\t\tif (!((cfg >> 20) & 0x01))\n\t\t\tcontinue;\n\t\t \n\t\tnumvfs = (cfg >> 12) & 0xFF;\n\n\t\tpfvf = &rvu->pf[pf];\n\n\t\t \n\t\tif (pfvf->nix_blkaddr != nix_hw->blkaddr)\n\t\t\tcontinue;\n\n\t\t \n\t\tpfvf->bcast_mce_idx = nix_alloc_mce_list(mcast, numvfs + 1);\n\t\tnix_mce_list_init(&pfvf->bcast_mce_list, numvfs + 1);\n\n\t\t \n\t\tpfvf->mcast_mce_idx = nix_alloc_mce_list(mcast, numvfs + 1);\n\t\tnix_mce_list_init(&pfvf->mcast_mce_list, numvfs + 1);\n\n\t\t \n\t\tpfvf->promisc_mce_idx = nix_alloc_mce_list(mcast, numvfs + 1);\n\t\tnix_mce_list_init(&pfvf->promisc_mce_list, numvfs + 1);\n\n\t\tfor (idx = 0; idx < (numvfs + 1); idx++) {\n\t\t\t \n\t\t\tpcifunc = (pf << RVU_PFVF_PF_SHIFT);\n\t\t\tpcifunc |= idx;\n\t\t\t \n\t\t\terr = nix_blk_setup_mce(rvu, nix_hw,\n\t\t\t\t\t\tpfvf->bcast_mce_idx + idx,\n\t\t\t\t\t\tNIX_AQ_INSTOP_INIT,\n\t\t\t\t\t\tpcifunc, 0, true);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t \n\t\t\terr = nix_blk_setup_mce(rvu, nix_hw,\n\t\t\t\t\t\tpfvf->mcast_mce_idx + idx,\n\t\t\t\t\t\tNIX_AQ_INSTOP_INIT,\n\t\t\t\t\t\tpcifunc, 0, true);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t \n\t\t\terr = nix_blk_setup_mce(rvu, nix_hw,\n\t\t\t\t\t\tpfvf->promisc_mce_idx + idx,\n\t\t\t\t\t\tNIX_AQ_INSTOP_INIT,\n\t\t\t\t\t\tpcifunc, 0, true);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int nix_setup_mcast(struct rvu *rvu, struct nix_hw *nix_hw, int blkaddr)\n{\n\tstruct nix_mcast *mcast = &nix_hw->mcast;\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint err, size;\n\n\tsize = (rvu_read64(rvu, blkaddr, NIX_AF_CONST3) >> 16) & 0x0F;\n\tsize = (1ULL << size);\n\n\t \n\terr = qmem_alloc(rvu->dev, &mcast->mce_ctx,\n\t\t\t (256UL << MC_TBL_SIZE), size);\n\tif (err)\n\t\treturn -ENOMEM;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_RX_MCAST_BASE,\n\t\t    (u64)mcast->mce_ctx->iova);\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_RX_MCAST_CFG,\n\t\t    BIT_ULL(36) | (hw->max_vfs_per_pf << 4) | MC_TBL_SIZE);\n\n\t \n\tsize = rvu_read64(rvu, blkaddr, NIX_AF_MC_MIRROR_CONST) & 0xFFFF;\n\terr = qmem_alloc(rvu->dev, &mcast->mcast_buf,\n\t\t\t (8UL << MC_BUF_CNT), size);\n\tif (err)\n\t\treturn -ENOMEM;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_RX_MCAST_BUF_BASE,\n\t\t    (u64)mcast->mcast_buf->iova);\n\n\t \n\tmcast->replay_pkind = rvu_alloc_rsrc(&hw->pkind.rsrc);\n\n\trvu_write64(rvu, blkaddr, NIX_AF_RX_MCAST_BUF_CFG,\n\t\t    BIT_ULL(63) | (mcast->replay_pkind << 24) |\n\t\t    BIT_ULL(20) | MC_BUF_CNT);\n\n\tmutex_init(&mcast->mce_lock);\n\n\treturn nix_setup_mce_tables(rvu, nix_hw);\n}\n\nstatic int nix_setup_txvlan(struct rvu *rvu, struct nix_hw *nix_hw)\n{\n\tstruct nix_txvlan *vlan = &nix_hw->txvlan;\n\tint err;\n\n\t \n\tvlan->rsrc.max = NIX_TX_VTAG_DEF_MAX;\n\terr = rvu_alloc_bitmap(&vlan->rsrc);\n\tif (err)\n\t\treturn -ENOMEM;\n\n\t \n\tvlan->entry2pfvf_map = devm_kcalloc(rvu->dev, vlan->rsrc.max,\n\t\t\t\t\t    sizeof(u16), GFP_KERNEL);\n\tif (!vlan->entry2pfvf_map)\n\t\tgoto free_mem;\n\n\tmutex_init(&vlan->rsrc_lock);\n\treturn 0;\n\nfree_mem:\n\tkfree(vlan->rsrc.bmap);\n\treturn -ENOMEM;\n}\n\nstatic int nix_setup_txschq(struct rvu *rvu, struct nix_hw *nix_hw, int blkaddr)\n{\n\tstruct nix_txsch *txsch;\n\tint err, lvl, schq;\n\tu64 cfg, reg;\n\n\t \n\tfor (lvl = 0; lvl < NIX_TXSCH_LVL_CNT; lvl++) {\n\t\ttxsch = &nix_hw->txsch[lvl];\n\t\ttxsch->lvl = lvl;\n\t\tswitch (lvl) {\n\t\tcase NIX_TXSCH_LVL_SMQ:\n\t\t\treg = NIX_AF_MDQ_CONST;\n\t\t\tbreak;\n\t\tcase NIX_TXSCH_LVL_TL4:\n\t\t\treg = NIX_AF_TL4_CONST;\n\t\t\tbreak;\n\t\tcase NIX_TXSCH_LVL_TL3:\n\t\t\treg = NIX_AF_TL3_CONST;\n\t\t\tbreak;\n\t\tcase NIX_TXSCH_LVL_TL2:\n\t\t\treg = NIX_AF_TL2_CONST;\n\t\t\tbreak;\n\t\tcase NIX_TXSCH_LVL_TL1:\n\t\t\treg = NIX_AF_TL1_CONST;\n\t\t\tbreak;\n\t\t}\n\t\tcfg = rvu_read64(rvu, blkaddr, reg);\n\t\ttxsch->schq.max = cfg & 0xFFFF;\n\t\terr = rvu_alloc_bitmap(&txsch->schq);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t \n\t\ttxsch->pfvf_map = devm_kcalloc(rvu->dev, txsch->schq.max,\n\t\t\t\t\t       sizeof(u32), GFP_KERNEL);\n\t\tif (!txsch->pfvf_map)\n\t\t\treturn -ENOMEM;\n\t\tfor (schq = 0; schq < txsch->schq.max; schq++)\n\t\t\ttxsch->pfvf_map[schq] = TXSCH_MAP(0, NIX_TXSCHQ_FREE);\n\t}\n\n\t \n\tif (rvu->hw->cap.nix_common_dwrr_mtu ||\n\t    rvu->hw->cap.nix_multiple_dwrr_mtu) {\n\t\trvu_write64(rvu, blkaddr,\n\t\t\t    nix_get_dwrr_mtu_reg(rvu->hw, SMQ_LINK_TYPE_RPM),\n\t\t\t    convert_bytes_to_dwrr_mtu(8192));\n\t\trvu_write64(rvu, blkaddr,\n\t\t\t    nix_get_dwrr_mtu_reg(rvu->hw, SMQ_LINK_TYPE_LBK),\n\t\t\t    convert_bytes_to_dwrr_mtu(8192));\n\t\trvu_write64(rvu, blkaddr,\n\t\t\t    nix_get_dwrr_mtu_reg(rvu->hw, SMQ_LINK_TYPE_SDP),\n\t\t\t    convert_bytes_to_dwrr_mtu(8192));\n\t}\n\n\treturn 0;\n}\n\nint rvu_nix_reserve_mark_format(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t\tint blkaddr, u32 cfg)\n{\n\tint fmt_idx;\n\n\tfor (fmt_idx = 0; fmt_idx < nix_hw->mark_format.in_use; fmt_idx++) {\n\t\tif (nix_hw->mark_format.cfg[fmt_idx] == cfg)\n\t\t\treturn fmt_idx;\n\t}\n\tif (fmt_idx >= nix_hw->mark_format.total)\n\t\treturn -ERANGE;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_MARK_FORMATX_CTL(fmt_idx), cfg);\n\tnix_hw->mark_format.cfg[fmt_idx] = cfg;\n\tnix_hw->mark_format.in_use++;\n\treturn fmt_idx;\n}\n\nstatic int nix_af_mark_format_setup(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t\t    int blkaddr)\n{\n\tu64 cfgs[] = {\n\t\t[NIX_MARK_CFG_IP_DSCP_RED]         = 0x10003,\n\t\t[NIX_MARK_CFG_IP_DSCP_YELLOW]      = 0x11200,\n\t\t[NIX_MARK_CFG_IP_DSCP_YELLOW_RED]  = 0x11203,\n\t\t[NIX_MARK_CFG_IP_ECN_RED]          = 0x6000c,\n\t\t[NIX_MARK_CFG_IP_ECN_YELLOW]       = 0x60c00,\n\t\t[NIX_MARK_CFG_IP_ECN_YELLOW_RED]   = 0x60c0c,\n\t\t[NIX_MARK_CFG_VLAN_DEI_RED]        = 0x30008,\n\t\t[NIX_MARK_CFG_VLAN_DEI_YELLOW]     = 0x30800,\n\t\t[NIX_MARK_CFG_VLAN_DEI_YELLOW_RED] = 0x30808,\n\t};\n\tint i, rc;\n\tu64 total;\n\n\ttotal = (rvu_read64(rvu, blkaddr, NIX_AF_PSE_CONST) & 0xFF00) >> 8;\n\tnix_hw->mark_format.total = (u8)total;\n\tnix_hw->mark_format.cfg = devm_kcalloc(rvu->dev, total, sizeof(u32),\n\t\t\t\t\t       GFP_KERNEL);\n\tif (!nix_hw->mark_format.cfg)\n\t\treturn -ENOMEM;\n\tfor (i = 0; i < NIX_MARK_CFG_MAX; i++) {\n\t\trc = rvu_nix_reserve_mark_format(rvu, nix_hw, blkaddr, cfgs[i]);\n\t\tif (rc < 0)\n\t\t\tdev_err(rvu->dev, \"Err %d in setup mark format %d\\n\",\n\t\t\t\ti, rc);\n\t}\n\n\treturn 0;\n}\n\nstatic void rvu_get_lbk_link_max_frs(struct rvu *rvu,  u16 *max_mtu)\n{\n\t \n\tif (rvu->hw->lbk_bufsize == 0x12000)\n\t\t*max_mtu = CN10K_LBK_LINK_MAX_FRS;\n\telse\n\t\t*max_mtu = NIC_HW_MAX_FRS;\n}\n\nstatic void rvu_get_lmac_link_max_frs(struct rvu *rvu, u16 *max_mtu)\n{\n\tint fifo_size = rvu_cgx_get_fifolen(rvu);\n\n\t \n\tif (fifo_size == 0x20000 || fifo_size == 0x40000)\n\t\t*max_mtu = CN10K_LMAC_LINK_MAX_FRS;\n\telse\n\t\t*max_mtu = NIC_HW_MAX_FRS;\n}\n\nint rvu_mbox_handler_nix_get_hw_info(struct rvu *rvu, struct msg_req *req,\n\t\t\t\t     struct nix_hw_info *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tu64 dwrr_mtu;\n\tint blkaddr;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tif (is_afvf(pcifunc))\n\t\trvu_get_lbk_link_max_frs(rvu, &rsp->max_mtu);\n\telse\n\t\trvu_get_lmac_link_max_frs(rvu, &rsp->max_mtu);\n\n\trsp->min_mtu = NIC_HW_MIN_FRS;\n\n\tif (!rvu->hw->cap.nix_common_dwrr_mtu &&\n\t    !rvu->hw->cap.nix_multiple_dwrr_mtu) {\n\t\t \n\t\trsp->rpm_dwrr_mtu = 1;\n\t\trsp->sdp_dwrr_mtu = 1;\n\t\trsp->lbk_dwrr_mtu = 1;\n\t\treturn 0;\n\t}\n\n\t \n\tdwrr_mtu = rvu_read64(rvu, blkaddr,\n\t\t\t      nix_get_dwrr_mtu_reg(rvu->hw, SMQ_LINK_TYPE_RPM));\n\trsp->rpm_dwrr_mtu = convert_dwrr_mtu_to_bytes(dwrr_mtu);\n\n\tdwrr_mtu = rvu_read64(rvu, blkaddr,\n\t\t\t      nix_get_dwrr_mtu_reg(rvu->hw, SMQ_LINK_TYPE_SDP));\n\trsp->sdp_dwrr_mtu = convert_dwrr_mtu_to_bytes(dwrr_mtu);\n\n\tdwrr_mtu = rvu_read64(rvu, blkaddr,\n\t\t\t      nix_get_dwrr_mtu_reg(rvu->hw, SMQ_LINK_TYPE_LBK));\n\trsp->lbk_dwrr_mtu = convert_dwrr_mtu_to_bytes(dwrr_mtu);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_stats_rst(struct rvu *rvu, struct msg_req *req,\n\t\t\t\t   struct msg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint i, nixlf, blkaddr, err;\n\tu64 stats;\n\n\terr = nix_get_nixlf(rvu, pcifunc, &nixlf, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\t \n\tstats = rvu_read64(rvu, blkaddr, NIX_AF_CONST1);\n\n\t \n\tfor (i = 0; i < ((stats >> 24) & 0xFF); i++)\n\t\trvu_write64(rvu, blkaddr, NIX_AF_LFX_TX_STATX(nixlf, i), 0);\n\n\t \n\tfor (i = 0; i < ((stats >> 32) & 0xFF); i++)\n\t\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RX_STATX(nixlf, i), 0);\n\n\treturn 0;\n}\n\n \nstatic int get_flowkey_alg_idx(struct nix_hw *nix_hw, u32 flow_cfg)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < nix_hw->flowkey.in_use; i++)\n\t\tif (nix_hw->flowkey.flowkey[i] == flow_cfg)\n\t\t\treturn i;\n\n\treturn -ERANGE;\n}\n\nstatic int set_flowkey_fields(struct nix_rx_flowkey_alg *alg, u32 flow_cfg)\n{\n\tint idx, nr_field, key_off, field_marker, keyoff_marker;\n\tint max_key_off, max_bit_pos, group_member;\n\tstruct nix_rx_flowkey_alg *field;\n\tstruct nix_rx_flowkey_alg tmp;\n\tu32 key_type, valid_key;\n\tu32 l3_l4_src_dst;\n\tint l4_key_offset = 0;\n\n\tif (!alg)\n\t\treturn -EINVAL;\n\n#define FIELDS_PER_ALG  5\n#define MAX_KEY_OFF\t40\n\t \n\tmemset(alg, 0, sizeof(uint64_t) * FIELDS_PER_ALG);\n\n\t \n\n\t \n\tl3_l4_src_dst = flow_cfg;\n\t \n\tflow_cfg &= NIX_FLOW_KEY_TYPE_L3_L4_MASK;\n\n\tkeyoff_marker = 0; max_key_off = 0; group_member = 0;\n\tnr_field = 0; key_off = 0; field_marker = 1;\n\tfield = &tmp; max_bit_pos = fls(flow_cfg);\n\tfor (idx = 0;\n\t     idx < max_bit_pos && nr_field < FIELDS_PER_ALG &&\n\t     key_off < MAX_KEY_OFF; idx++) {\n\t\tkey_type = BIT(idx);\n\t\tvalid_key = flow_cfg & key_type;\n\t\t \n\t\tif (field_marker)\n\t\t\tmemset(&tmp, 0, sizeof(tmp));\n\n\t\tfield_marker = true;\n\t\tkeyoff_marker = true;\n\t\tswitch (key_type) {\n\t\tcase NIX_FLOW_KEY_TYPE_PORT:\n\t\t\tfield->sel_chan = true;\n\t\t\t \n\t\t\tfield->bytesm1 = 1;\n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_IPV4_PROTO:\n\t\t\tfield->lid = NPC_LID_LC;\n\t\t\tfield->hdr_offset = 9;  \n\t\t\tfield->bytesm1 = 0;  \n\t\t\tfield->ltype_match = NPC_LT_LC_IP;\n\t\t\tfield->ltype_mask = 0xF;\n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_IPV4:\n\t\tcase NIX_FLOW_KEY_TYPE_INNR_IPV4:\n\t\t\tfield->lid = NPC_LID_LC;\n\t\t\tfield->ltype_match = NPC_LT_LC_IP;\n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_INNR_IPV4) {\n\t\t\t\tfield->lid = NPC_LID_LG;\n\t\t\t\tfield->ltype_match = NPC_LT_LG_TU_IP;\n\t\t\t}\n\t\t\tfield->hdr_offset = 12;  \n\t\t\tfield->bytesm1 = 7;  \n\n\t\t\t \n\t\t\tif (l3_l4_src_dst & NIX_FLOW_KEY_TYPE_L3_SRC_ONLY)\n\t\t\t\tfield->bytesm1 = 3;  \n\n\t\t\tif (l3_l4_src_dst & NIX_FLOW_KEY_TYPE_L3_DST_ONLY) {\n\t\t\t\t \n\t\t\t\tif (field->bytesm1 == 3) {\n\t\t\t\t\tfield->bytesm1 = 7;  \n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tfield->hdr_offset = 16;  \n\t\t\t\t\tfield->bytesm1 = 3;  \n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfield->ltype_mask = 0xF;  \n\t\t\tkeyoff_marker = false;\n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_IPV6:\n\t\tcase NIX_FLOW_KEY_TYPE_INNR_IPV6:\n\t\t\tfield->lid = NPC_LID_LC;\n\t\t\tfield->ltype_match = NPC_LT_LC_IP6;\n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_INNR_IPV6) {\n\t\t\t\tfield->lid = NPC_LID_LG;\n\t\t\t\tfield->ltype_match = NPC_LT_LG_TU_IP6;\n\t\t\t}\n\t\t\tfield->hdr_offset = 8;  \n\t\t\tfield->bytesm1 = 31;  \n\n\t\t\t \n\t\t\tif (l3_l4_src_dst & NIX_FLOW_KEY_TYPE_L3_SRC_ONLY)\n\t\t\t\tfield->bytesm1 = 15;  \n\n\t\t\tif (l3_l4_src_dst & NIX_FLOW_KEY_TYPE_L3_DST_ONLY) {\n\t\t\t\t \n\t\t\t\tif (field->bytesm1 == 15) {\n\t\t\t\t\t \n\t\t\t\t\tfield->bytesm1 = 31;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tfield->hdr_offset = 24;  \n\t\t\t\t\tfield->bytesm1 = 15;  \n\t\t\t\t}\n\t\t\t}\n\t\t\tfield->ltype_mask = 0xF;  \n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_TCP:\n\t\tcase NIX_FLOW_KEY_TYPE_UDP:\n\t\tcase NIX_FLOW_KEY_TYPE_SCTP:\n\t\tcase NIX_FLOW_KEY_TYPE_INNR_TCP:\n\t\tcase NIX_FLOW_KEY_TYPE_INNR_UDP:\n\t\tcase NIX_FLOW_KEY_TYPE_INNR_SCTP:\n\t\t\tfield->lid = NPC_LID_LD;\n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_INNR_TCP ||\n\t\t\t    key_type == NIX_FLOW_KEY_TYPE_INNR_UDP ||\n\t\t\t    key_type == NIX_FLOW_KEY_TYPE_INNR_SCTP)\n\t\t\t\tfield->lid = NPC_LID_LH;\n\t\t\tfield->bytesm1 = 3;  \n\n\t\t\tif (l3_l4_src_dst & NIX_FLOW_KEY_TYPE_L4_SRC_ONLY)\n\t\t\t\tfield->bytesm1 = 1;  \n\n\t\t\tif (l3_l4_src_dst & NIX_FLOW_KEY_TYPE_L4_DST_ONLY) {\n\t\t\t\t \n\t\t\t\tif (field->bytesm1 == 1) {\n\t\t\t\t\t \n\t\t\t\t\tfield->bytesm1 = 3;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tfield->hdr_offset = 2;  \n\t\t\t\t\tfield->bytesm1 = 1;  \n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\tBUILD_BUG_ON((int)NPC_LT_LD_TCP !=\n\t\t\t\t     (int)NPC_LT_LH_TU_TCP);\n\t\t\tBUILD_BUG_ON((int)NPC_LT_LD_UDP !=\n\t\t\t\t     (int)NPC_LT_LH_TU_UDP);\n\t\t\tBUILD_BUG_ON((int)NPC_LT_LD_SCTP !=\n\t\t\t\t     (int)NPC_LT_LH_TU_SCTP);\n\n\t\t\tif ((key_type == NIX_FLOW_KEY_TYPE_TCP ||\n\t\t\t     key_type == NIX_FLOW_KEY_TYPE_INNR_TCP) &&\n\t\t\t    valid_key) {\n\t\t\t\tfield->ltype_match |= NPC_LT_LD_TCP;\n\t\t\t\tgroup_member = true;\n\t\t\t} else if ((key_type == NIX_FLOW_KEY_TYPE_UDP ||\n\t\t\t\t    key_type == NIX_FLOW_KEY_TYPE_INNR_UDP) &&\n\t\t\t\t   valid_key) {\n\t\t\t\tfield->ltype_match |= NPC_LT_LD_UDP;\n\t\t\t\tgroup_member = true;\n\t\t\t} else if ((key_type == NIX_FLOW_KEY_TYPE_SCTP ||\n\t\t\t\t    key_type == NIX_FLOW_KEY_TYPE_INNR_SCTP) &&\n\t\t\t\t   valid_key) {\n\t\t\t\tfield->ltype_match |= NPC_LT_LD_SCTP;\n\t\t\t\tgroup_member = true;\n\t\t\t}\n\t\t\tfield->ltype_mask = ~field->ltype_match;\n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_SCTP ||\n\t\t\t    key_type == NIX_FLOW_KEY_TYPE_INNR_SCTP) {\n\t\t\t\t \n\t\t\t\tif (group_member) {\n\t\t\t\t\tvalid_key = true;\n\t\t\t\t\tgroup_member = false;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfield_marker = false;\n\t\t\t\tkeyoff_marker = false;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_TCP)\n\t\t\t\tl4_key_offset = key_off;\n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_NVGRE:\n\t\t\tfield->lid = NPC_LID_LD;\n\t\t\tfield->hdr_offset = 4;  \n\t\t\tfield->bytesm1 = 2;\n\t\t\tfield->ltype_match = NPC_LT_LD_NVGRE;\n\t\t\tfield->ltype_mask = 0xF;\n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_VXLAN:\n\t\tcase NIX_FLOW_KEY_TYPE_GENEVE:\n\t\t\tfield->lid = NPC_LID_LE;\n\t\t\tfield->bytesm1 = 2;\n\t\t\tfield->hdr_offset = 4;\n\t\t\tfield->ltype_mask = 0xF;\n\t\t\tfield_marker = false;\n\t\t\tkeyoff_marker = false;\n\n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_VXLAN && valid_key) {\n\t\t\t\tfield->ltype_match |= NPC_LT_LE_VXLAN;\n\t\t\t\tgroup_member = true;\n\t\t\t}\n\n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_GENEVE && valid_key) {\n\t\t\t\tfield->ltype_match |= NPC_LT_LE_GENEVE;\n\t\t\t\tgroup_member = true;\n\t\t\t}\n\n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_GENEVE) {\n\t\t\t\tif (group_member) {\n\t\t\t\t\tfield->ltype_mask = ~field->ltype_match;\n\t\t\t\t\tfield_marker = true;\n\t\t\t\t\tkeyoff_marker = true;\n\t\t\t\t\tvalid_key = true;\n\t\t\t\t\tgroup_member = false;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_ETH_DMAC:\n\t\tcase NIX_FLOW_KEY_TYPE_INNR_ETH_DMAC:\n\t\t\tfield->lid = NPC_LID_LA;\n\t\t\tfield->ltype_match = NPC_LT_LA_ETHER;\n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_INNR_ETH_DMAC) {\n\t\t\t\tfield->lid = NPC_LID_LF;\n\t\t\t\tfield->ltype_match = NPC_LT_LF_TU_ETHER;\n\t\t\t}\n\t\t\tfield->hdr_offset = 0;\n\t\t\tfield->bytesm1 = 5;  \n\t\t\tfield->ltype_mask = 0xF;\n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_IPV6_EXT:\n\t\t\tfield->lid = NPC_LID_LC;\n\t\t\tfield->hdr_offset = 40;  \n\t\t\tfield->bytesm1 = 0;  \n\t\t\tfield->ltype_match = NPC_LT_LC_IP6_EXT;\n\t\t\tfield->ltype_mask = 0xF;\n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_GTPU:\n\t\t\tfield->lid = NPC_LID_LE;\n\t\t\tfield->hdr_offset = 4;\n\t\t\tfield->bytesm1 = 3;  \n\t\t\tfield->ltype_match = NPC_LT_LE_GTPU;\n\t\t\tfield->ltype_mask = 0xF;\n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_VLAN:\n\t\t\tfield->lid = NPC_LID_LB;\n\t\t\tfield->hdr_offset = 2;  \n\t\t\tfield->bytesm1 = 1;  \n\t\t\tfield->ltype_match = NPC_LT_LB_CTAG;\n\t\t\tfield->ltype_mask = 0xF;\n\t\t\tfield->fn_mask = 1;  \n\t\t\tbreak;\n\t\tcase NIX_FLOW_KEY_TYPE_AH:\n\t\tcase NIX_FLOW_KEY_TYPE_ESP:\n\t\t\tfield->hdr_offset = 0;\n\t\t\tfield->bytesm1 = 7;  \n\t\t\tfield->ltype_mask = 0xF;\n\t\t\tfield->lid = NPC_LID_LE;\n\t\t\tfield->ltype_match = NPC_LT_LE_ESP;\n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_AH) {\n\t\t\t\tfield->lid = NPC_LID_LD;\n\t\t\t\tfield->ltype_match = NPC_LT_LD_AH;\n\t\t\t\tfield->hdr_offset = 4;\n\t\t\t\tkeyoff_marker = false;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tfield->ena = 1;\n\n\t\t \n\t\tif (valid_key) {\n\t\t\t \n\t\t\tif (key_type == NIX_FLOW_KEY_TYPE_ESP ||\n\t\t\t    key_type == NIX_FLOW_KEY_TYPE_AH)\n\t\t\t\tkey_off = l4_key_offset;\n\t\t\tfield->key_offset = key_off;\n\t\t\tmemcpy(&alg[nr_field], field, sizeof(*field));\n\t\t\tmax_key_off = max(max_key_off, field->bytesm1 + 1);\n\n\t\t\t \n\t\t\tif (field_marker)\n\t\t\t\tnr_field++;\n\t\t}\n\n\t\t \n\t\tif (keyoff_marker) {\n\t\t\tkey_off += max_key_off;\n\t\t\tmax_key_off = 0;\n\t\t}\n\t}\n\t \n\tif (idx == max_bit_pos && key_off <= MAX_KEY_OFF)\n\t\treturn 0;\n\telse\n\t\treturn NIX_AF_ERR_RSS_NOSPC_FIELD;\n}\n\nstatic int reserve_flowkey_alg_idx(struct rvu *rvu, int blkaddr, u32 flow_cfg)\n{\n\tu64 field[FIELDS_PER_ALG];\n\tstruct nix_hw *hw;\n\tint fid, rc;\n\n\thw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\t \n\tif (hw->flowkey.in_use >= NIX_FLOW_KEY_ALG_MAX)\n\t\treturn NIX_AF_ERR_RSS_NOSPC_ALGO;\n\n\t \n\trc = set_flowkey_fields((struct nix_rx_flowkey_alg *)field, flow_cfg);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tfor (fid = 0; fid < FIELDS_PER_ALG; fid++)\n\t\trvu_write64(rvu, blkaddr,\n\t\t\t    NIX_AF_RX_FLOW_KEY_ALGX_FIELDX(hw->flowkey.in_use,\n\t\t\t\t\t\t\t   fid), field[fid]);\n\n\t \n\trc = hw->flowkey.in_use;\n\thw->flowkey.flowkey[rc] = flow_cfg;\n\thw->flowkey.in_use++;\n\n\treturn rc;\n}\n\nint rvu_mbox_handler_nix_rss_flowkey_cfg(struct rvu *rvu,\n\t\t\t\t\t struct nix_rss_flowkey_cfg *req,\n\t\t\t\t\t struct nix_rss_flowkey_cfg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint alg_idx, nixlf, blkaddr;\n\tstruct nix_hw *nix_hw;\n\tint err;\n\n\terr = nix_get_nixlf(rvu, pcifunc, &nixlf, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\talg_idx = get_flowkey_alg_idx(nix_hw, req->flowkey_cfg);\n\t \n\tif (alg_idx < 0) {\n\t\talg_idx = reserve_flowkey_alg_idx(rvu, blkaddr,\n\t\t\t\t\t\t  req->flowkey_cfg);\n\t\tif (alg_idx < 0)\n\t\t\treturn alg_idx;\n\t}\n\trsp->alg_idx = alg_idx;\n\trvu_npc_update_flowkey_alg_idx(rvu, pcifunc, nixlf, req->group,\n\t\t\t\t       alg_idx, req->mcam_index);\n\treturn 0;\n}\n\nstatic int nix_rx_flowkey_alg_cfg(struct rvu *rvu, int blkaddr)\n{\n\tu32 flowkey_cfg, minkey_cfg;\n\tint alg, fid, rc;\n\n\t \n\tfor (alg = 0; alg < NIX_FLOW_KEY_ALG_MAX; alg++) {\n\t\tfor (fid = 0; fid < FIELDS_PER_ALG; fid++)\n\t\t\trvu_write64(rvu, blkaddr,\n\t\t\t\t    NIX_AF_RX_FLOW_KEY_ALGX_FIELDX(alg, fid),\n\t\t\t\t    0);\n\t}\n\n\t \n\tflowkey_cfg = NIX_FLOW_KEY_TYPE_IPV4 | NIX_FLOW_KEY_TYPE_IPV6;\n\trc = reserve_flowkey_alg_idx(rvu, blkaddr, flowkey_cfg);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tminkey_cfg = flowkey_cfg;\n\tflowkey_cfg = minkey_cfg | NIX_FLOW_KEY_TYPE_TCP;\n\trc = reserve_flowkey_alg_idx(rvu, blkaddr, flowkey_cfg);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tflowkey_cfg = minkey_cfg | NIX_FLOW_KEY_TYPE_UDP;\n\trc = reserve_flowkey_alg_idx(rvu, blkaddr, flowkey_cfg);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tflowkey_cfg = minkey_cfg | NIX_FLOW_KEY_TYPE_SCTP;\n\trc = reserve_flowkey_alg_idx(rvu, blkaddr, flowkey_cfg);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tflowkey_cfg = minkey_cfg | NIX_FLOW_KEY_TYPE_TCP |\n\t\t\tNIX_FLOW_KEY_TYPE_UDP;\n\trc = reserve_flowkey_alg_idx(rvu, blkaddr, flowkey_cfg);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tflowkey_cfg = minkey_cfg | NIX_FLOW_KEY_TYPE_TCP |\n\t\t\tNIX_FLOW_KEY_TYPE_SCTP;\n\trc = reserve_flowkey_alg_idx(rvu, blkaddr, flowkey_cfg);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tflowkey_cfg = minkey_cfg | NIX_FLOW_KEY_TYPE_UDP |\n\t\t\tNIX_FLOW_KEY_TYPE_SCTP;\n\trc = reserve_flowkey_alg_idx(rvu, blkaddr, flowkey_cfg);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tflowkey_cfg = minkey_cfg | NIX_FLOW_KEY_TYPE_TCP |\n\t\t      NIX_FLOW_KEY_TYPE_UDP | NIX_FLOW_KEY_TYPE_SCTP;\n\trc = reserve_flowkey_alg_idx(rvu, blkaddr, flowkey_cfg);\n\tif (rc < 0)\n\t\treturn rc;\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_set_mac_addr(struct rvu *rvu,\n\t\t\t\t      struct nix_set_mac_addr *req,\n\t\t\t\t      struct msg_rsp *rsp)\n{\n\tbool from_vf = req->hdr.pcifunc & RVU_PFVF_FUNC_MASK;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint blkaddr, nixlf, err;\n\tstruct rvu_pfvf *pfvf;\n\n\terr = nix_get_nixlf(rvu, pcifunc, &nixlf, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\n\t \n\tif (!test_bit(PF_SET_VF_TRUSTED, &pfvf->flags) &&\n\t    (from_vf && test_bit(PF_SET_VF_MAC, &pfvf->flags))) {\n\t\tdev_warn(rvu->dev,\n\t\t\t \"MAC address set by admin(PF) cannot be overwritten by untrusted VF\");\n\t\treturn -EPERM;\n\t}\n\n\tether_addr_copy(pfvf->mac_addr, req->mac_addr);\n\n\trvu_npc_install_ucast_entry(rvu, pcifunc, nixlf,\n\t\t\t\t    pfvf->rx_chan_base, req->mac_addr);\n\n\tif (test_bit(PF_SET_VF_TRUSTED, &pfvf->flags) && from_vf)\n\t\tether_addr_copy(pfvf->default_mac, req->mac_addr);\n\n\trvu_switch_update_rules(rvu, pcifunc);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_get_mac_addr(struct rvu *rvu,\n\t\t\t\t      struct msg_req *req,\n\t\t\t\t      struct nix_get_mac_addr_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct rvu_pfvf *pfvf;\n\n\tif (!is_nixlf_attached(rvu, pcifunc))\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\n\tether_addr_copy(rsp->mac_addr, pfvf->mac_addr);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_set_rx_mode(struct rvu *rvu, struct nix_rx_mode *req,\n\t\t\t\t     struct msg_rsp *rsp)\n{\n\tbool allmulti, promisc, nix_rx_multicast;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct rvu_pfvf *pfvf;\n\tint nixlf, err;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tpromisc = req->mode & NIX_RX_MODE_PROMISC ? true : false;\n\tallmulti = req->mode & NIX_RX_MODE_ALLMULTI ? true : false;\n\tpfvf->use_mce_list = req->mode & NIX_RX_MODE_USE_MCE ? true : false;\n\n\tnix_rx_multicast = rvu->hw->cap.nix_rx_multicast & pfvf->use_mce_list;\n\n\tif (is_vf(pcifunc) && !nix_rx_multicast &&\n\t    (promisc || allmulti)) {\n\t\tdev_warn_ratelimited(rvu->dev,\n\t\t\t\t     \"VF promisc/multicast not supported\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tif (is_vf(pcifunc) && !test_bit(PF_SET_VF_TRUSTED, &pfvf->flags) &&\n\t    (promisc || allmulti))\n\t\treturn 0;\n\n\terr = nix_get_nixlf(rvu, pcifunc, &nixlf, NULL);\n\tif (err)\n\t\treturn err;\n\n\tif (nix_rx_multicast) {\n\t\t \n\t\terr = nix_update_mce_rule(rvu, pcifunc, NIXLF_ALLMULTI_ENTRY,\n\t\t\t\t\t  allmulti);\n\t\tif (err) {\n\t\t\tdev_err(rvu->dev,\n\t\t\t\t\"Failed to update pcifunc 0x%x to multicast list\\n\",\n\t\t\t\tpcifunc);\n\t\t\treturn err;\n\t\t}\n\n\t\t \n\t\terr = nix_update_mce_rule(rvu, pcifunc, NIXLF_PROMISC_ENTRY,\n\t\t\t\t\t  promisc);\n\t\tif (err) {\n\t\t\tdev_err(rvu->dev,\n\t\t\t\t\"Failed to update pcifunc 0x%x to promisc list\\n\",\n\t\t\t\tpcifunc);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\t \n\tif (allmulti) {\n\t\trvu_npc_install_allmulti_entry(rvu, pcifunc, nixlf,\n\t\t\t\t\t       pfvf->rx_chan_base);\n\t} else {\n\t\tif (!nix_rx_multicast)\n\t\t\trvu_npc_enable_allmulti_entry(rvu, pcifunc, nixlf, false);\n\t}\n\n\t \n\tif (promisc)\n\t\trvu_npc_install_promisc_entry(rvu, pcifunc, nixlf,\n\t\t\t\t\t      pfvf->rx_chan_base,\n\t\t\t\t\t      pfvf->rx_chan_cnt);\n\telse\n\t\tif (!nix_rx_multicast)\n\t\t\trvu_npc_enable_promisc_entry(rvu, pcifunc, nixlf, false);\n\n\treturn 0;\n}\n\nstatic void nix_find_link_frs(struct rvu *rvu,\n\t\t\t      struct nix_frs_cfg *req, u16 pcifunc)\n{\n\tint pf = rvu_get_pf(pcifunc);\n\tstruct rvu_pfvf *pfvf;\n\tint maxlen, minlen;\n\tint numvfs, hwvf;\n\tint vf;\n\n\t \n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tpfvf->maxlen = req->maxlen;\n\tif (req->update_minlen)\n\t\tpfvf->minlen = req->minlen;\n\n\tmaxlen = req->maxlen;\n\tminlen = req->update_minlen ? req->minlen : 0;\n\n\t \n\trvu_get_pf_numvfs(rvu, pf, &numvfs, &hwvf);\n\n\t \n\tfor (vf = 0; vf < numvfs; vf++) {\n\t\tpfvf =  &rvu->hwvf[hwvf + vf];\n\t\tif (pfvf->maxlen > maxlen)\n\t\t\tmaxlen = pfvf->maxlen;\n\t\tif (req->update_minlen &&\n\t\t    pfvf->minlen && pfvf->minlen < minlen)\n\t\t\tminlen = pfvf->minlen;\n\t}\n\n\t \n\tpfvf = &rvu->pf[pf];\n\tif (pfvf->maxlen > maxlen)\n\t\tmaxlen = pfvf->maxlen;\n\tif (req->update_minlen &&\n\t    pfvf->minlen && pfvf->minlen < minlen)\n\t\tminlen = pfvf->minlen;\n\n\t \n\treq->maxlen = maxlen;\n\tif (req->update_minlen)\n\t\treq->minlen = minlen;\n}\n\nint rvu_mbox_handler_nix_set_hw_frs(struct rvu *rvu, struct nix_frs_cfg *req,\n\t\t\t\t    struct msg_rsp *rsp)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tint pf = rvu_get_pf(pcifunc);\n\tint blkaddr, link = -1;\n\tstruct nix_hw *nix_hw;\n\tstruct rvu_pfvf *pfvf;\n\tu8 cgx = 0, lmac = 0;\n\tu16 max_mtu;\n\tu64 cfg;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\tif (is_afvf(pcifunc))\n\t\trvu_get_lbk_link_max_frs(rvu, &max_mtu);\n\telse\n\t\trvu_get_lmac_link_max_frs(rvu, &max_mtu);\n\n\tif (!req->sdp_link && req->maxlen > max_mtu)\n\t\treturn NIX_AF_ERR_FRS_INVALID;\n\n\tif (req->update_minlen && req->minlen < NIC_HW_MIN_FRS)\n\t\treturn NIX_AF_ERR_FRS_INVALID;\n\n\t \n\tif (req->sdp_link) {\n\t\tif (!hw->sdp_links)\n\t\t\treturn NIX_AF_ERR_RX_LINK_INVALID;\n\t\tlink = hw->cgx_links + hw->lbk_links;\n\t\tgoto linkcfg;\n\t}\n\n\t \n\tif (is_pf_cgxmapped(rvu, pf)) {\n\t\t \n\t\trvu_get_cgx_lmac_id(rvu->pf2cgxlmac_map[pf], &cgx, &lmac);\n\t\tlink = (cgx * hw->lmac_per_cgx) + lmac;\n\t} else if (pf == 0) {\n\t\t \n\t\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\t\tlink = hw->cgx_links + pfvf->lbkid;\n\t}\n\n\tif (link < 0)\n\t\treturn NIX_AF_ERR_RX_LINK_INVALID;\n\nlinkcfg:\n\tnix_find_link_frs(rvu, req, pcifunc);\n\n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_RX_LINKX_CFG(link));\n\tcfg = (cfg & ~(0xFFFFULL << 16)) | ((u64)req->maxlen << 16);\n\tif (req->update_minlen)\n\t\tcfg = (cfg & ~0xFFFFULL) | req->minlen;\n\trvu_write64(rvu, blkaddr, NIX_AF_RX_LINKX_CFG(link), cfg);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_set_rx_cfg(struct rvu *rvu, struct nix_rx_cfg *req,\n\t\t\t\t    struct msg_rsp *rsp)\n{\n\tint nixlf, blkaddr, err;\n\tu64 cfg;\n\n\terr = nix_get_nixlf(rvu, req->hdr.pcifunc, &nixlf, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_LFX_RX_CFG(nixlf));\n\t \n\tif (req->len_verify & BIT(0))\n\t\tcfg |= BIT_ULL(41);\n\telse\n\t\tcfg &= ~BIT_ULL(41);\n\n\tif (req->len_verify & BIT(1))\n\t\tcfg |= BIT_ULL(40);\n\telse\n\t\tcfg &= ~BIT_ULL(40);\n\n\tif (req->len_verify & NIX_RX_DROP_RE)\n\t\tcfg |= BIT_ULL(32);\n\telse\n\t\tcfg &= ~BIT_ULL(32);\n\n\tif (req->csum_verify & BIT(0))\n\t\tcfg |= BIT_ULL(37);\n\telse\n\t\tcfg &= ~BIT_ULL(37);\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RX_CFG(nixlf), cfg);\n\n\treturn 0;\n}\n\nstatic u64 rvu_get_lbk_link_credits(struct rvu *rvu, u16 lbk_max_frs)\n{\n\treturn 1600;  \n}\n\nstatic void nix_link_config(struct rvu *rvu, int blkaddr,\n\t\t\t    struct nix_hw *nix_hw)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint cgx, lmac_cnt, slink, link;\n\tu16 lbk_max_frs, lmac_max_frs;\n\tunsigned long lmac_bmap;\n\tu64 tx_credits, cfg;\n\tu64 lmac_fifo_len;\n\tint iter;\n\n\trvu_get_lbk_link_max_frs(rvu, &lbk_max_frs);\n\trvu_get_lmac_link_max_frs(rvu, &lmac_max_frs);\n\n\t \n\tfor (link = 0; link < hw->cgx_links; link++) {\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_LINKX_CFG(link),\n\t\t\t\t((u64)lmac_max_frs << 16) | NIC_HW_MIN_FRS);\n\t}\n\n\tfor (link = hw->cgx_links; link < hw->lbk_links; link++) {\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_LINKX_CFG(link),\n\t\t\t    ((u64)lbk_max_frs << 16) | NIC_HW_MIN_FRS);\n\t}\n\tif (hw->sdp_links) {\n\t\tlink = hw->cgx_links + hw->lbk_links;\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_LINKX_CFG(link),\n\t\t\t    SDP_HW_MAX_FRS << 16 | NIC_HW_MIN_FRS);\n\t}\n\n\t \n\tif (mcs_get_blkcnt() == 1) {\n\t\t \n\t\tnix_hw->cc_mcs_cnt = is_mcs_bypass(0) ? 0 : 2;\n\t}\n\n\t \n\tfor (cgx = 0; cgx < hw->cgx; cgx++) {\n\t\tlmac_cnt = cgx_get_lmac_cnt(rvu_cgx_pdata(cgx, rvu));\n\t\t \n\t\tif (lmac_cnt <= 0)\n\t\t\tcontinue;\n\t\tslink = cgx * hw->lmac_per_cgx;\n\n\t\t \n\t\tlmac_bmap = cgx_get_lmac_bmap(rvu_cgx_pdata(cgx, rvu));\n\t\tfor_each_set_bit(iter, &lmac_bmap, rvu->hw->lmac_per_cgx) {\n\t\t\tlmac_fifo_len = rvu_cgx_get_lmac_fifolen(rvu, cgx, iter);\n\t\t\tif (!lmac_fifo_len) {\n\t\t\t\tdev_err(rvu->dev,\n\t\t\t\t\t\"%s: Failed to get CGX/RPM%d:LMAC%d FIFO size\\n\",\n\t\t\t\t\t__func__, cgx, iter);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttx_credits = (lmac_fifo_len - lmac_max_frs) / 16;\n\t\t\t \n\t\t\tcfg =  (tx_credits << 12) | (0x1FF << 2) | BIT_ULL(1);\n\t\t\tcfg |= FIELD_PREP(NIX_AF_LINKX_MCS_CNT_MASK, nix_hw->cc_mcs_cnt);\n\n\t\t\tlink = iter + slink;\n\t\t\tnix_hw->tx_credits[link] = tx_credits;\n\t\t\trvu_write64(rvu, blkaddr,\n\t\t\t\t    NIX_AF_TX_LINKX_NORM_CREDIT(link), cfg);\n\t\t}\n\t}\n\n\t \n\tslink = hw->cgx_links;\n\tfor (link = slink; link < (slink + hw->lbk_links); link++) {\n\t\ttx_credits = rvu_get_lbk_link_credits(rvu, lbk_max_frs);\n\t\tnix_hw->tx_credits[link] = tx_credits;\n\t\t \n\t\ttx_credits =  (tx_credits << 12) | (0x1FF << 2) | BIT_ULL(1);\n\t\trvu_write64(rvu, blkaddr,\n\t\t\t    NIX_AF_TX_LINKX_NORM_CREDIT(link), tx_credits);\n\t}\n}\n\nstatic int nix_calibrate_x2p(struct rvu *rvu, int blkaddr)\n{\n\tint idx, err;\n\tu64 status;\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_CFG,\n\t\t    rvu_read64(rvu, blkaddr, NIX_AF_CFG) | BIT_ULL(9));\n\t \n\terr = rvu_poll_reg(rvu, blkaddr,\n\t\t\t   NIX_AF_STATUS, BIT_ULL(10), false);\n\tif (err) {\n\t\tdev_err(rvu->dev, \"NIX X2P bus calibration failed\\n\");\n\t\treturn err;\n\t}\n\n\tstatus = rvu_read64(rvu, blkaddr, NIX_AF_STATUS);\n\t \n\tfor (idx = 0; idx < rvu->cgx_cnt_max; idx++) {\n\t\t \n\t\tif (!rvu_cgx_pdata(idx, rvu) ||\n\t\t    (status & (BIT_ULL(16 + idx))))\n\t\t\tcontinue;\n\t\tdev_err(rvu->dev,\n\t\t\t\"CGX%d didn't respond to NIX X2P calibration\\n\", idx);\n\t\terr = -EBUSY;\n\t}\n\n\t \n\tif (!(status & BIT_ULL(19))) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"LBK didn't respond to NIX X2P calibration\\n\");\n\t\terr = -EBUSY;\n\t}\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_CFG,\n\t\t    rvu_read64(rvu, blkaddr, NIX_AF_CFG) & ~BIT_ULL(9));\n\tif (err || (status & 0x3FFULL))\n\t\tdev_err(rvu->dev,\n\t\t\t\"NIX X2P calibration failed, status 0x%llx\\n\", status);\n\tif (err)\n\t\treturn err;\n\treturn 0;\n}\n\nstatic int nix_aq_init(struct rvu *rvu, struct rvu_block *block)\n{\n\tu64 cfg;\n\tint err;\n\n\t \n\tcfg = rvu_read64(rvu, block->addr, NIX_AF_CFG);\n#ifdef __BIG_ENDIAN\n\tcfg |= BIT_ULL(8);\n\trvu_write64(rvu, block->addr, NIX_AF_CFG, cfg);\n#else\n\tcfg &= ~BIT_ULL(8);\n\trvu_write64(rvu, block->addr, NIX_AF_CFG, cfg);\n#endif\n\n\t \n\tcfg = rvu_read64(rvu, block->addr, NIX_AF_NDC_CFG);\n\tcfg &= ~0x3FFEULL;\n#ifdef CONFIG_NDC_DIS_DYNAMIC_CACHING\n\t \n\tcfg |= 0x04ULL;\n#endif\n\trvu_write64(rvu, block->addr, NIX_AF_NDC_CFG, cfg);\n\n\t \n\terr = rvu_aq_alloc(rvu, &block->aq,\n\t\t\t   Q_COUNT(AQ_SIZE), sizeof(struct nix_aq_inst_s),\n\t\t\t   ALIGN(sizeof(struct nix_aq_res_s), 128) + 256);\n\tif (err)\n\t\treturn err;\n\n\trvu_write64(rvu, block->addr, NIX_AF_AQ_CFG, AQ_SIZE);\n\trvu_write64(rvu, block->addr,\n\t\t    NIX_AF_AQ_BASE, (u64)block->aq->inst->iova);\n\treturn 0;\n}\n\nstatic void rvu_nix_setup_capabilities(struct rvu *rvu, int blkaddr)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tu64 hw_const;\n\n\thw_const = rvu_read64(rvu, blkaddr, NIX_AF_CONST1);\n\n\t \n\tif ((((hw_const >> 56) & 0x10) == 0x10) && !(hw_const & BIT_ULL(61)))\n\t\thw->cap.nix_common_dwrr_mtu = true;\n\n\tif (hw_const & BIT_ULL(61))\n\t\thw->cap.nix_multiple_dwrr_mtu = true;\n}\n\nstatic int rvu_nix_block_init(struct rvu *rvu, struct nix_hw *nix_hw)\n{\n\tconst struct npc_lt_def_cfg *ltdefs;\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint blkaddr = nix_hw->blkaddr;\n\tstruct rvu_block *block;\n\tint err;\n\tu64 cfg;\n\n\tblock = &hw->block[blkaddr];\n\n\tif (is_rvu_96xx_B0(rvu)) {\n\t\t \n\t\trvu_write64(rvu, blkaddr, NIX_AF_CFG,\n\t\t\t    rvu_read64(rvu, blkaddr, NIX_AF_CFG) | 0x40ULL);\n\n\t\t \n\t\trvu_write64(rvu, blkaddr, NIX_AF_PSE_CHANNEL_LEVEL, 0x01);\n\n\t\t \n\t\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_SQM_DBG_CTL_STATUS);\n\t\tcfg &= ~BIT_ULL(15);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_SQM_DBG_CTL_STATUS, cfg);\n\t}\n\n\tltdefs = rvu->kpu.lt_def;\n\t \n\terr = nix_calibrate_x2p(rvu, blkaddr);\n\tif (err)\n\t\treturn err;\n\n\t \n\trvu_nix_setup_capabilities(rvu, blkaddr);\n\n\t \n\terr = nix_aq_init(rvu, block);\n\tif (err)\n\t\treturn err;\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_CINT_DELAY, 0x0ULL);\n\n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_SEB_CFG);\n\n\t \n\tcfg |= 1ULL;\n\tif (!is_rvu_otx2(rvu))\n\t\tcfg |= NIX_PTP_1STEP_EN;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_SEB_CFG, cfg);\n\n\tif (!is_rvu_otx2(rvu))\n\t\trvu_nix_block_cn10k_init(rvu, nix_hw);\n\n\tif (is_block_implemented(hw, blkaddr)) {\n\t\terr = nix_setup_txschq(rvu, nix_hw, blkaddr);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = nix_setup_ipolicers(rvu, nix_hw, blkaddr);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = nix_af_mark_format_setup(rvu, nix_hw, blkaddr);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = nix_setup_mcast(rvu, nix_hw, blkaddr);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = nix_setup_txvlan(rvu, nix_hw);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t \n\t\tnix_setup_lso(rvu, nix_hw, blkaddr);\n\n\t\t \n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_OL2,\n\t\t\t    (ltdefs->rx_ol2.lid << 8) | (ltdefs->rx_ol2.ltype_match << 4) |\n\t\t\t    ltdefs->rx_ol2.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_OIP4,\n\t\t\t    (ltdefs->rx_oip4.lid << 8) | (ltdefs->rx_oip4.ltype_match << 4) |\n\t\t\t    ltdefs->rx_oip4.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_IIP4,\n\t\t\t    (ltdefs->rx_iip4.lid << 8) | (ltdefs->rx_iip4.ltype_match << 4) |\n\t\t\t    ltdefs->rx_iip4.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_OIP6,\n\t\t\t    (ltdefs->rx_oip6.lid << 8) | (ltdefs->rx_oip6.ltype_match << 4) |\n\t\t\t    ltdefs->rx_oip6.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_IIP6,\n\t\t\t    (ltdefs->rx_iip6.lid << 8) | (ltdefs->rx_iip6.ltype_match << 4) |\n\t\t\t    ltdefs->rx_iip6.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_OTCP,\n\t\t\t    (ltdefs->rx_otcp.lid << 8) | (ltdefs->rx_otcp.ltype_match << 4) |\n\t\t\t    ltdefs->rx_otcp.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_ITCP,\n\t\t\t    (ltdefs->rx_itcp.lid << 8) | (ltdefs->rx_itcp.ltype_match << 4) |\n\t\t\t    ltdefs->rx_itcp.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_OUDP,\n\t\t\t    (ltdefs->rx_oudp.lid << 8) | (ltdefs->rx_oudp.ltype_match << 4) |\n\t\t\t    ltdefs->rx_oudp.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_IUDP,\n\t\t\t    (ltdefs->rx_iudp.lid << 8) | (ltdefs->rx_iudp.ltype_match << 4) |\n\t\t\t    ltdefs->rx_iudp.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_OSCTP,\n\t\t\t    (ltdefs->rx_osctp.lid << 8) | (ltdefs->rx_osctp.ltype_match << 4) |\n\t\t\t    ltdefs->rx_osctp.ltype_mask);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_ISCTP,\n\t\t\t    (ltdefs->rx_isctp.lid << 8) | (ltdefs->rx_isctp.ltype_match << 4) |\n\t\t\t    ltdefs->rx_isctp.ltype_mask);\n\n\t\tif (!is_rvu_otx2(rvu)) {\n\t\t\t \n\t\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_CST_APAD0,\n\t\t\t\t    (ltdefs->rx_apad0.valid << 11) |\n\t\t\t\t    (ltdefs->rx_apad0.lid << 8) |\n\t\t\t\t    (ltdefs->rx_apad0.ltype_match << 4) |\n\t\t\t\t    ltdefs->rx_apad0.ltype_mask);\n\t\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_CST_APAD1,\n\t\t\t\t    (ltdefs->rx_apad1.valid << 11) |\n\t\t\t\t    (ltdefs->rx_apad1.lid << 8) |\n\t\t\t\t    (ltdefs->rx_apad1.ltype_match << 4) |\n\t\t\t\t    ltdefs->rx_apad1.ltype_mask);\n\n\t\t\t \n\t\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_ET(0),\n\t\t\t\t    (ltdefs->rx_et[0].offset << 12) |\n\t\t\t\t    (ltdefs->rx_et[0].valid << 11) |\n\t\t\t\t    (ltdefs->rx_et[0].lid << 8) |\n\t\t\t\t    (ltdefs->rx_et[0].ltype_match << 4) |\n\t\t\t\t    ltdefs->rx_et[0].ltype_mask);\n\t\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_ET(1),\n\t\t\t\t    (ltdefs->rx_et[1].offset << 12) |\n\t\t\t\t    (ltdefs->rx_et[1].valid << 11) |\n\t\t\t\t    (ltdefs->rx_et[1].lid << 8) |\n\t\t\t\t    (ltdefs->rx_et[1].ltype_match << 4) |\n\t\t\t\t    ltdefs->rx_et[1].ltype_mask);\n\t\t}\n\n\t\terr = nix_rx_flowkey_alg_cfg(rvu, blkaddr);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tnix_hw->tx_credits = kcalloc(hw->cgx_links + hw->lbk_links,\n\t\t\t\t\t     sizeof(u64), GFP_KERNEL);\n\t\tif (!nix_hw->tx_credits)\n\t\t\treturn -ENOMEM;\n\n\t\t \n\t\tnix_link_config(rvu, blkaddr, nix_hw);\n\n\t\t \n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_CFG, BIT_ULL(0));\n\t}\n\treturn 0;\n}\n\nint rvu_nix_init(struct rvu *rvu)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct nix_hw *nix_hw;\n\tint blkaddr = 0, err;\n\tint i = 0;\n\n\thw->nix = devm_kcalloc(rvu->dev, MAX_NIX_BLKS, sizeof(struct nix_hw),\n\t\t\t       GFP_KERNEL);\n\tif (!hw->nix)\n\t\treturn -ENOMEM;\n\n\tblkaddr = rvu_get_next_nix_blkaddr(rvu, blkaddr);\n\twhile (blkaddr) {\n\t\tnix_hw = &hw->nix[i];\n\t\tnix_hw->rvu = rvu;\n\t\tnix_hw->blkaddr = blkaddr;\n\t\terr = rvu_nix_block_init(rvu, nix_hw);\n\t\tif (err)\n\t\t\treturn err;\n\t\tblkaddr = rvu_get_next_nix_blkaddr(rvu, blkaddr);\n\t\ti++;\n\t}\n\n\treturn 0;\n}\n\nstatic void rvu_nix_block_freemem(struct rvu *rvu, int blkaddr,\n\t\t\t\t  struct rvu_block *block)\n{\n\tstruct nix_txsch *txsch;\n\tstruct nix_mcast *mcast;\n\tstruct nix_txvlan *vlan;\n\tstruct nix_hw *nix_hw;\n\tint lvl;\n\n\trvu_aq_free(rvu, block->aq);\n\n\tif (is_block_implemented(rvu->hw, blkaddr)) {\n\t\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\t\tif (!nix_hw)\n\t\t\treturn;\n\n\t\tfor (lvl = 0; lvl < NIX_TXSCH_LVL_CNT; lvl++) {\n\t\t\ttxsch = &nix_hw->txsch[lvl];\n\t\t\tkfree(txsch->schq.bmap);\n\t\t}\n\n\t\tkfree(nix_hw->tx_credits);\n\n\t\tnix_ipolicer_freemem(rvu, nix_hw);\n\n\t\tvlan = &nix_hw->txvlan;\n\t\tkfree(vlan->rsrc.bmap);\n\t\tmutex_destroy(&vlan->rsrc_lock);\n\n\t\tmcast = &nix_hw->mcast;\n\t\tqmem_free(rvu->dev, mcast->mce_ctx);\n\t\tqmem_free(rvu->dev, mcast->mcast_buf);\n\t\tmutex_destroy(&mcast->mce_lock);\n\t}\n}\n\nvoid rvu_nix_freemem(struct rvu *rvu)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct rvu_block *block;\n\tint blkaddr = 0;\n\n\tblkaddr = rvu_get_next_nix_blkaddr(rvu, blkaddr);\n\twhile (blkaddr) {\n\t\tblock = &hw->block[blkaddr];\n\t\trvu_nix_block_freemem(rvu, blkaddr, block);\n\t\tblkaddr = rvu_get_next_nix_blkaddr(rvu, blkaddr);\n\t}\n}\n\nint rvu_mbox_handler_nix_lf_start_rx(struct rvu *rvu, struct msg_req *req,\n\t\t\t\t     struct msg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct rvu_pfvf *pfvf;\n\tint nixlf, err;\n\n\terr = nix_get_nixlf(rvu, pcifunc, &nixlf, NULL);\n\tif (err)\n\t\treturn err;\n\n\trvu_npc_enable_default_entries(rvu, pcifunc, nixlf);\n\n\tnpc_mcam_enable_flows(rvu, pcifunc);\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tset_bit(NIXLF_INITIALIZED, &pfvf->flags);\n\n\trvu_switch_update_rules(rvu, pcifunc);\n\n\treturn rvu_cgx_start_stop_io(rvu, pcifunc, true);\n}\n\nint rvu_mbox_handler_nix_lf_stop_rx(struct rvu *rvu, struct msg_req *req,\n\t\t\t\t    struct msg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct rvu_pfvf *pfvf;\n\tint nixlf, err;\n\n\terr = nix_get_nixlf(rvu, pcifunc, &nixlf, NULL);\n\tif (err)\n\t\treturn err;\n\n\trvu_npc_disable_mcam_entries(rvu, pcifunc, nixlf);\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tclear_bit(NIXLF_INITIALIZED, &pfvf->flags);\n\n\terr = rvu_cgx_start_stop_io(rvu, pcifunc, false);\n\tif (err)\n\t\treturn err;\n\n\trvu_cgx_tx_enable(rvu, pcifunc, true);\n\n\treturn 0;\n}\n\n#define RX_SA_BASE  GENMASK_ULL(52, 7)\n\nvoid rvu_nix_lf_teardown(struct rvu *rvu, u16 pcifunc, int blkaddr, int nixlf)\n{\n\tstruct rvu_pfvf *pfvf = rvu_get_pfvf(rvu, pcifunc);\n\tstruct hwctx_disable_req ctx_req;\n\tint pf = rvu_get_pf(pcifunc);\n\tstruct mac_ops *mac_ops;\n\tu8 cgx_id, lmac_id;\n\tu64 sa_base;\n\tvoid *cgxd;\n\tint err;\n\n\tctx_req.hdr.pcifunc = pcifunc;\n\n\t \n\trvu_npc_disable_mcam_entries(rvu, pcifunc, nixlf);\n\trvu_npc_free_mcam_entries(rvu, pcifunc, nixlf);\n\tnix_interface_deinit(rvu, pcifunc, nixlf);\n\tnix_rx_sync(rvu, blkaddr);\n\tnix_txschq_free(rvu, pcifunc);\n\n\tclear_bit(NIXLF_INITIALIZED, &pfvf->flags);\n\n\trvu_cgx_start_stop_io(rvu, pcifunc, false);\n\n\tif (pfvf->sq_ctx) {\n\t\tctx_req.ctype = NIX_AQ_CTYPE_SQ;\n\t\terr = nix_lf_hwctx_disable(rvu, &ctx_req);\n\t\tif (err)\n\t\t\tdev_err(rvu->dev, \"SQ ctx disable failed\\n\");\n\t}\n\n\tif (pfvf->rq_ctx) {\n\t\tctx_req.ctype = NIX_AQ_CTYPE_RQ;\n\t\terr = nix_lf_hwctx_disable(rvu, &ctx_req);\n\t\tif (err)\n\t\t\tdev_err(rvu->dev, \"RQ ctx disable failed\\n\");\n\t}\n\n\tif (pfvf->cq_ctx) {\n\t\tctx_req.ctype = NIX_AQ_CTYPE_CQ;\n\t\terr = nix_lf_hwctx_disable(rvu, &ctx_req);\n\t\tif (err)\n\t\t\tdev_err(rvu->dev, \"CQ ctx disable failed\\n\");\n\t}\n\n\t \n\trvu_npc_set_parse_mode(rvu, pcifunc, OTX2_PRIV_FLAGS_DEFAULT,\n\t\t\t       (PKIND_TX | PKIND_RX), 0, 0, 0, 0);\n\n\t \n\tif (pfvf->hw_rx_tstamp_en) {\n\t\trvu_get_cgx_lmac_id(rvu->pf2cgxlmac_map[pf], &cgx_id, &lmac_id);\n\t\tcgxd = rvu_cgx_pdata(cgx_id, rvu);\n\t\tmac_ops = get_mac_ops(cgxd);\n\t\tmac_ops->mac_enadis_ptp_config(cgxd, lmac_id, false);\n\t\t \n\t\tif (npc_config_ts_kpuaction(rvu, pf, pcifunc, false))\n\t\t\tdev_err(rvu->dev, \"NPC config for PTP failed\\n\");\n\t\tpfvf->hw_rx_tstamp_en = false;\n\t}\n\n\t \n\trvu_cgx_prio_flow_ctrl_cfg(rvu, pcifunc, 0, 0, 0);\n\n\t \n\trvu_cgx_cfg_pause_frm(rvu, pcifunc, 0, 0);\n\n\tnix_ctx_free(rvu, pfvf);\n\n\tnix_free_all_bandprof(rvu, pcifunc);\n\n\tsa_base = rvu_read64(rvu, blkaddr, NIX_AF_LFX_RX_IPSEC_SA_BASE(nixlf));\n\tif (FIELD_GET(RX_SA_BASE, sa_base)) {\n\t\terr = rvu_cpt_ctx_flush(rvu, pcifunc);\n\t\tif (err)\n\t\t\tdev_err(rvu->dev,\n\t\t\t\t\"CPT ctx flush failed with error: %d\\n\", err);\n\t}\n}\n\n#define NIX_AF_LFX_TX_CFG_PTP_EN\tBIT_ULL(32)\n\nstatic int rvu_nix_lf_ptp_tx_cfg(struct rvu *rvu, u16 pcifunc, bool enable)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct rvu_block *block;\n\tint blkaddr, pf;\n\tint nixlf;\n\tu64 cfg;\n\n\tpf = rvu_get_pf(pcifunc);\n\tif (!is_mac_feature_supported(rvu, pf, RVU_LMAC_FEAT_PTP))\n\t\treturn 0;\n\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tblock = &hw->block[blkaddr];\n\tnixlf = rvu_get_lf(rvu, block, pcifunc, 0);\n\tif (nixlf < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_LFX_TX_CFG(nixlf));\n\n\tif (enable)\n\t\tcfg |= NIX_AF_LFX_TX_CFG_PTP_EN;\n\telse\n\t\tcfg &= ~NIX_AF_LFX_TX_CFG_PTP_EN;\n\n\trvu_write64(rvu, blkaddr, NIX_AF_LFX_TX_CFG(nixlf), cfg);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_lf_ptp_tx_enable(struct rvu *rvu, struct msg_req *req,\n\t\t\t\t\t  struct msg_rsp *rsp)\n{\n\treturn rvu_nix_lf_ptp_tx_cfg(rvu, req->hdr.pcifunc, true);\n}\n\nint rvu_mbox_handler_nix_lf_ptp_tx_disable(struct rvu *rvu, struct msg_req *req,\n\t\t\t\t\t   struct msg_rsp *rsp)\n{\n\treturn rvu_nix_lf_ptp_tx_cfg(rvu, req->hdr.pcifunc, false);\n}\n\nint rvu_mbox_handler_nix_lso_format_cfg(struct rvu *rvu,\n\t\t\t\t\tstruct nix_lso_format_cfg *req,\n\t\t\t\t\tstruct nix_lso_format_cfg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct nix_hw *nix_hw;\n\tstruct rvu_pfvf *pfvf;\n\tint blkaddr, idx, f;\n\tu64 reg;\n\n\tpfvf = rvu_get_pfvf(rvu, pcifunc);\n\tblkaddr = rvu_get_blkaddr(rvu, BLKTYPE_NIX, pcifunc);\n\tif (!pfvf->nixlf || blkaddr < 0)\n\t\treturn NIX_AF_ERR_AF_LF_INVALID;\n\n\tnix_hw = get_nix_hw(rvu->hw, blkaddr);\n\tif (!nix_hw)\n\t\treturn NIX_AF_ERR_INVALID_NIXBLK;\n\n\t \n\tfor (idx = 0; idx < nix_hw->lso.in_use; idx++) {\n\t\tfor (f = 0; f < NIX_LSO_FIELD_MAX; f++) {\n\t\t\treg = rvu_read64(rvu, blkaddr,\n\t\t\t\t\t NIX_AF_LSO_FORMATX_FIELDX(idx, f));\n\t\t\tif (req->fields[f] != (reg & req->field_mask))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (f == NIX_LSO_FIELD_MAX)\n\t\t\tbreak;\n\t}\n\n\tif (idx < nix_hw->lso.in_use) {\n\t\t \n\t\trsp->lso_format_idx = idx;\n\t\treturn 0;\n\t}\n\n\tif (nix_hw->lso.in_use == nix_hw->lso.total)\n\t\treturn NIX_AF_ERR_LSO_CFG_FAIL;\n\n\trsp->lso_format_idx = nix_hw->lso.in_use++;\n\n\tfor (f = 0; f < NIX_LSO_FIELD_MAX; f++)\n\t\trvu_write64(rvu, blkaddr,\n\t\t\t    NIX_AF_LSO_FORMATX_FIELDX(rsp->lso_format_idx, f),\n\t\t\t    req->fields[f]);\n\n\treturn 0;\n}\n\n#define IPSEC_GEN_CFG_EGRP    GENMASK_ULL(50, 48)\n#define IPSEC_GEN_CFG_OPCODE  GENMASK_ULL(47, 32)\n#define IPSEC_GEN_CFG_PARAM1  GENMASK_ULL(31, 16)\n#define IPSEC_GEN_CFG_PARAM2  GENMASK_ULL(15, 0)\n\n#define CPT_INST_QSEL_BLOCK   GENMASK_ULL(28, 24)\n#define CPT_INST_QSEL_PF_FUNC GENMASK_ULL(23, 8)\n#define CPT_INST_QSEL_SLOT    GENMASK_ULL(7, 0)\n\n#define CPT_INST_CREDIT_TH    GENMASK_ULL(53, 32)\n#define CPT_INST_CREDIT_BPID  GENMASK_ULL(30, 22)\n#define CPT_INST_CREDIT_CNT   GENMASK_ULL(21, 0)\n\nstatic void nix_inline_ipsec_cfg(struct rvu *rvu, struct nix_inline_ipsec_cfg *req,\n\t\t\t\t int blkaddr)\n{\n\tu8 cpt_idx, cpt_blkaddr;\n\tu64 val;\n\n\tcpt_idx = (blkaddr == BLKADDR_NIX0) ? 0 : 1;\n\tif (req->enable) {\n\t\tval = 0;\n\t\t \n\t\tif (!is_rvu_otx2(rvu))\n\t\t\tval |= BIT_ULL(51);\n\n\t\t \n\t\tval |= FIELD_PREP(IPSEC_GEN_CFG_EGRP, req->gen_cfg.egrp);\n\t\tval |= FIELD_PREP(IPSEC_GEN_CFG_OPCODE, req->gen_cfg.opcode);\n\t\tval |= FIELD_PREP(IPSEC_GEN_CFG_PARAM1, req->gen_cfg.param1);\n\t\tval |= FIELD_PREP(IPSEC_GEN_CFG_PARAM2, req->gen_cfg.param2);\n\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_IPSEC_GEN_CFG, val);\n\n\t\t \n\t\tval = FIELD_PREP(CPT_INST_QSEL_SLOT, req->inst_qsel.cpt_slot);\n\t\tval |= FIELD_PREP(CPT_INST_QSEL_PF_FUNC,\n\t\t\t\t  req->inst_qsel.cpt_pf_func);\n\n\t\tif (!is_rvu_otx2(rvu)) {\n\t\t\tcpt_blkaddr = (cpt_idx == 0) ? BLKADDR_CPT0 :\n\t\t\t\t\t\t       BLKADDR_CPT1;\n\t\t\tval |= FIELD_PREP(CPT_INST_QSEL_BLOCK, cpt_blkaddr);\n\t\t}\n\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_CPTX_INST_QSEL(cpt_idx),\n\t\t\t    val);\n\n\t\t \n\t\tval = rvu_read64(rvu, blkaddr, NIX_AF_RX_CPTX_CREDIT(cpt_idx));\n\t\tif ((val & 0x3FFFFF) != 0x3FFFFF)\n\t\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_CPTX_CREDIT(cpt_idx),\n\t\t\t\t    0x3FFFFF - val);\n\n\t\tval = FIELD_PREP(CPT_INST_CREDIT_CNT, req->cpt_credit);\n\t\tval |= FIELD_PREP(CPT_INST_CREDIT_BPID, req->bpid);\n\t\tval |= FIELD_PREP(CPT_INST_CREDIT_TH, req->credit_th);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_CPTX_CREDIT(cpt_idx), val);\n\t} else {\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_IPSEC_GEN_CFG, 0x0);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_CPTX_INST_QSEL(cpt_idx),\n\t\t\t    0x0);\n\t\tval = rvu_read64(rvu, blkaddr, NIX_AF_RX_CPTX_CREDIT(cpt_idx));\n\t\tif ((val & 0x3FFFFF) != 0x3FFFFF)\n\t\t\trvu_write64(rvu, blkaddr, NIX_AF_RX_CPTX_CREDIT(cpt_idx),\n\t\t\t\t    0x3FFFFF - val);\n\t}\n}\n\nint rvu_mbox_handler_nix_inline_ipsec_cfg(struct rvu *rvu,\n\t\t\t\t\t  struct nix_inline_ipsec_cfg *req,\n\t\t\t\t\t  struct msg_rsp *rsp)\n{\n\tif (!is_block_implemented(rvu->hw, BLKADDR_CPT0))\n\t\treturn 0;\n\n\tnix_inline_ipsec_cfg(rvu, req, BLKADDR_NIX0);\n\tif (is_block_implemented(rvu->hw, BLKADDR_CPT1))\n\t\tnix_inline_ipsec_cfg(rvu, req, BLKADDR_NIX1);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_read_inline_ipsec_cfg(struct rvu *rvu,\n\t\t\t\t\t       struct msg_req *req,\n\t\t\t\t\t       struct nix_inline_ipsec_cfg *rsp)\n\n{\n\tu64 val;\n\n\tif (!is_block_implemented(rvu->hw, BLKADDR_CPT0))\n\t\treturn 0;\n\n\tval = rvu_read64(rvu, BLKADDR_NIX0, NIX_AF_RX_IPSEC_GEN_CFG);\n\trsp->gen_cfg.egrp = FIELD_GET(IPSEC_GEN_CFG_EGRP, val);\n\trsp->gen_cfg.opcode = FIELD_GET(IPSEC_GEN_CFG_OPCODE, val);\n\trsp->gen_cfg.param1 = FIELD_GET(IPSEC_GEN_CFG_PARAM1, val);\n\trsp->gen_cfg.param2 = FIELD_GET(IPSEC_GEN_CFG_PARAM2, val);\n\n\tval = rvu_read64(rvu, BLKADDR_NIX0, NIX_AF_RX_CPTX_CREDIT(0));\n\trsp->cpt_credit = FIELD_GET(CPT_INST_CREDIT_CNT, val);\n\trsp->credit_th = FIELD_GET(CPT_INST_CREDIT_TH, val);\n\trsp->bpid = FIELD_GET(CPT_INST_CREDIT_BPID, val);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_inline_ipsec_lf_cfg(struct rvu *rvu,\n\t\t\t\t\t     struct nix_inline_ipsec_lf_cfg *req,\n\t\t\t\t\t     struct msg_rsp *rsp)\n{\n\tint lf, blkaddr, err;\n\tu64 val;\n\n\tif (!is_block_implemented(rvu->hw, BLKADDR_CPT0))\n\t\treturn 0;\n\n\terr = nix_get_nixlf(rvu, req->hdr.pcifunc, &lf, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tif (req->enable) {\n\t\t \n\t\tval = (u64)req->ipsec_cfg0.tt << 44 |\n\t\t      (u64)req->ipsec_cfg0.tag_const << 20 |\n\t\t      (u64)req->ipsec_cfg0.sa_pow2_size << 16 |\n\t\t      req->ipsec_cfg0.lenm1_max;\n\n\t\tif (blkaddr == BLKADDR_NIX1)\n\t\t\tval |= BIT_ULL(46);\n\n\t\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RX_IPSEC_CFG0(lf), val);\n\n\t\t \n\t\tval = (u64)req->ipsec_cfg1.sa_idx_w << 32 |\n\t\t      req->ipsec_cfg1.sa_idx_max;\n\t\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RX_IPSEC_CFG1(lf), val);\n\n\t\t \n\t\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RX_IPSEC_SA_BASE(lf),\n\t\t\t    req->sa_base_addr);\n\t} else {\n\t\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RX_IPSEC_CFG0(lf), 0x0);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RX_IPSEC_CFG1(lf), 0x0);\n\t\trvu_write64(rvu, blkaddr, NIX_AF_LFX_RX_IPSEC_SA_BASE(lf),\n\t\t\t    0x0);\n\t}\n\n\treturn 0;\n}\n\nvoid rvu_nix_reset_mac(struct rvu_pfvf *pfvf, int pcifunc)\n{\n\tbool from_vf = !!(pcifunc & RVU_PFVF_FUNC_MASK);\n\n\t \n\tif (from_vf)\n\t\tether_addr_copy(pfvf->mac_addr, pfvf->default_mac);\n}\n\n \nstatic void nix_config_rx_pkt_policer_precolor(struct rvu *rvu, int blkaddr)\n{\n\tstruct npc_lt_def_cfg defs, *ltdefs;\n\n\tltdefs = &defs;\n\tmemcpy(ltdefs, rvu->kpu.lt_def, sizeof(struct npc_lt_def_cfg));\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_VLAN0_PCP_DEI,\n\t\t    (2UL << 12) | (ltdefs->ovlan.lid << 8) |\n\t\t    (ltdefs->ovlan.ltype_match << 4) |\n\t\t    ltdefs->ovlan.ltype_mask);\n\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_VLAN1_PCP_DEI,\n\t\t    (2UL << 12) | (ltdefs->ivlan.lid << 8) |\n\t\t    (ltdefs->ivlan.ltype_match << 4) |\n\t\t    ltdefs->ivlan.ltype_mask);\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_OIP4_DSCP,\n\t\t    (1UL << 12) | (ltdefs->rx_oip4.lid << 8) |\n\t\t    (ltdefs->rx_oip4.ltype_match << 4) |\n\t\t    ltdefs->rx_oip4.ltype_mask);\n\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_IIP4_DSCP,\n\t\t    (1UL << 12) | (ltdefs->rx_iip4.lid << 8) |\n\t\t    (ltdefs->rx_iip4.ltype_match << 4) |\n\t\t    ltdefs->rx_iip4.ltype_mask);\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_OIP6_DSCP,\n\t\t    (1UL << 11) | (ltdefs->rx_oip6.lid << 8) |\n\t\t    (ltdefs->rx_oip6.ltype_match << 4) |\n\t\t    ltdefs->rx_oip6.ltype_mask);\n\trvu_write64(rvu, blkaddr, NIX_AF_RX_DEF_IIP6_DSCP,\n\t\t    (1UL << 11) | (ltdefs->rx_iip6.lid << 8) |\n\t\t    (ltdefs->rx_iip6.ltype_match << 4) |\n\t\t    ltdefs->rx_iip6.ltype_mask);\n}\n\nstatic int nix_init_policer_context(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t\t    int layer, int prof_idx)\n{\n\tstruct nix_cn10k_aq_enq_req aq_req;\n\tint rc;\n\n\tmemset(&aq_req, 0, sizeof(struct nix_cn10k_aq_enq_req));\n\n\taq_req.qidx = (prof_idx & 0x3FFF) | (layer << 14);\n\taq_req.ctype = NIX_AQ_CTYPE_BANDPROF;\n\taq_req.op = NIX_AQ_INSTOP_INIT;\n\n\t \n\trc = rvu_nix_blk_aq_enq_inst(rvu, nix_hw,\n\t\t\t\t     (struct nix_aq_enq_req *)&aq_req, NULL);\n\tif (rc)\n\t\tdev_err(rvu->dev, \"Failed to INIT bandwidth profile layer %d profile %d\\n\",\n\t\t\tlayer, prof_idx);\n\treturn rc;\n}\n\nstatic int nix_setup_ipolicers(struct rvu *rvu,\n\t\t\t       struct nix_hw *nix_hw, int blkaddr)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tstruct nix_ipolicer *ipolicer;\n\tint err, layer, prof_idx;\n\tu64 cfg;\n\n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_CONST);\n\tif (!(cfg & BIT_ULL(61))) {\n\t\thw->cap.ipolicer = false;\n\t\treturn 0;\n\t}\n\n\thw->cap.ipolicer = true;\n\tnix_hw->ipolicer = devm_kcalloc(rvu->dev, BAND_PROF_NUM_LAYERS,\n\t\t\t\t\tsizeof(*ipolicer), GFP_KERNEL);\n\tif (!nix_hw->ipolicer)\n\t\treturn -ENOMEM;\n\n\tcfg = rvu_read64(rvu, blkaddr, NIX_AF_PL_CONST);\n\n\tfor (layer = 0; layer < BAND_PROF_NUM_LAYERS; layer++) {\n\t\tipolicer = &nix_hw->ipolicer[layer];\n\t\tswitch (layer) {\n\t\tcase BAND_PROF_LEAF_LAYER:\n\t\t\tipolicer->band_prof.max = cfg & 0XFFFF;\n\t\t\tbreak;\n\t\tcase BAND_PROF_MID_LAYER:\n\t\t\tipolicer->band_prof.max = (cfg >> 16) & 0XFFFF;\n\t\t\tbreak;\n\t\tcase BAND_PROF_TOP_LAYER:\n\t\t\tipolicer->band_prof.max = (cfg >> 32) & 0XFFFF;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!ipolicer->band_prof.max)\n\t\t\tcontinue;\n\n\t\terr = rvu_alloc_bitmap(&ipolicer->band_prof);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tipolicer->pfvf_map = devm_kcalloc(rvu->dev,\n\t\t\t\t\t\t  ipolicer->band_prof.max,\n\t\t\t\t\t\t  sizeof(u16), GFP_KERNEL);\n\t\tif (!ipolicer->pfvf_map)\n\t\t\treturn -ENOMEM;\n\n\t\tipolicer->match_id = devm_kcalloc(rvu->dev,\n\t\t\t\t\t\t  ipolicer->band_prof.max,\n\t\t\t\t\t\t  sizeof(u16), GFP_KERNEL);\n\t\tif (!ipolicer->match_id)\n\t\t\treturn -ENOMEM;\n\n\t\tfor (prof_idx = 0;\n\t\t     prof_idx < ipolicer->band_prof.max; prof_idx++) {\n\t\t\t \n\t\t\tipolicer->pfvf_map[prof_idx] = 0x00;\n\n\t\t\t \n\t\t\terr = nix_init_policer_context(rvu, nix_hw,\n\t\t\t\t\t\t       layer, prof_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\t \n\t\tif (layer != BAND_PROF_MID_LAYER)\n\t\t\tcontinue;\n\n\t\tipolicer->ref_count = devm_kcalloc(rvu->dev,\n\t\t\t\t\t\t   ipolicer->band_prof.max,\n\t\t\t\t\t\t   sizeof(u16), GFP_KERNEL);\n\t\tif (!ipolicer->ref_count)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\trvu_write64(rvu, blkaddr, NIX_AF_PL_TS, 19);\n\n\tnix_config_rx_pkt_policer_precolor(rvu, blkaddr);\n\n\treturn 0;\n}\n\nstatic void nix_ipolicer_freemem(struct rvu *rvu, struct nix_hw *nix_hw)\n{\n\tstruct nix_ipolicer *ipolicer;\n\tint layer;\n\n\tif (!rvu->hw->cap.ipolicer)\n\t\treturn;\n\n\tfor (layer = 0; layer < BAND_PROF_NUM_LAYERS; layer++) {\n\t\tipolicer = &nix_hw->ipolicer[layer];\n\n\t\tif (!ipolicer->band_prof.max)\n\t\t\tcontinue;\n\n\t\tkfree(ipolicer->band_prof.bmap);\n\t}\n}\n\nstatic int nix_verify_bandprof(struct nix_cn10k_aq_enq_req *req,\n\t\t\t       struct nix_hw *nix_hw, u16 pcifunc)\n{\n\tstruct nix_ipolicer *ipolicer;\n\tint layer, hi_layer, prof_idx;\n\n\t \n\tlayer = (req->qidx >> 14) & 0x03;\n\tprof_idx = req->qidx & 0x3FFF;\n\n\tipolicer = &nix_hw->ipolicer[layer];\n\tif (prof_idx >= ipolicer->band_prof.max)\n\t\treturn -EINVAL;\n\n\t \n\tif (pcifunc && ipolicer->pfvf_map[prof_idx] != pcifunc)\n\t\treturn -EINVAL;\n\n\t \n\tif (!req->prof.hl_en)\n\t\treturn 0;\n\n\t \n\tif (layer == BAND_PROF_LEAF_LAYER)\n\t\thi_layer = BAND_PROF_MID_LAYER;\n\telse if (layer == BAND_PROF_MID_LAYER)\n\t\thi_layer = BAND_PROF_TOP_LAYER;\n\telse\n\t\treturn -EINVAL;\n\n\tipolicer = &nix_hw->ipolicer[hi_layer];\n\tprof_idx = req->prof.band_prof_id;\n\tif (prof_idx >= ipolicer->band_prof.max ||\n\t    ipolicer->pfvf_map[prof_idx] != pcifunc)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_bandprof_alloc(struct rvu *rvu,\n\t\t\t\t\tstruct nix_bandprof_alloc_req *req,\n\t\t\t\t\tstruct nix_bandprof_alloc_rsp *rsp)\n{\n\tint blkaddr, layer, prof, idx, err;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct nix_ipolicer *ipolicer;\n\tstruct nix_hw *nix_hw;\n\n\tif (!rvu->hw->cap.ipolicer)\n\t\treturn NIX_AF_ERR_IPOLICER_NOTSUPP;\n\n\terr = nix_get_struct_ptrs(rvu, pcifunc, &nix_hw, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tmutex_lock(&rvu->rsrc_lock);\n\tfor (layer = 0; layer < BAND_PROF_NUM_LAYERS; layer++) {\n\t\tif (layer == BAND_PROF_INVAL_LAYER)\n\t\t\tcontinue;\n\t\tif (!req->prof_count[layer])\n\t\t\tcontinue;\n\n\t\tipolicer = &nix_hw->ipolicer[layer];\n\t\tfor (idx = 0; idx < req->prof_count[layer]; idx++) {\n\t\t\t \n\t\t\tif (idx == MAX_BANDPROF_PER_PFFUNC)\n\t\t\t\tbreak;\n\n\t\t\tprof = rvu_alloc_rsrc(&ipolicer->band_prof);\n\t\t\tif (prof < 0)\n\t\t\t\tbreak;\n\t\t\trsp->prof_count[layer]++;\n\t\t\trsp->prof_idx[layer][idx] = prof;\n\t\t\tipolicer->pfvf_map[prof] = pcifunc;\n\t\t}\n\t}\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn 0;\n}\n\nstatic int nix_free_all_bandprof(struct rvu *rvu, u16 pcifunc)\n{\n\tint blkaddr, layer, prof_idx, err;\n\tstruct nix_ipolicer *ipolicer;\n\tstruct nix_hw *nix_hw;\n\n\tif (!rvu->hw->cap.ipolicer)\n\t\treturn NIX_AF_ERR_IPOLICER_NOTSUPP;\n\n\terr = nix_get_struct_ptrs(rvu, pcifunc, &nix_hw, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tmutex_lock(&rvu->rsrc_lock);\n\t \n\tfor (layer = 0; layer < BAND_PROF_NUM_LAYERS; layer++) {\n\t\tif (layer == BAND_PROF_INVAL_LAYER)\n\t\t\tcontinue;\n\t\tipolicer = &nix_hw->ipolicer[layer];\n\n\t\tfor (prof_idx = 0; prof_idx < ipolicer->band_prof.max; prof_idx++) {\n\t\t\tif (ipolicer->pfvf_map[prof_idx] != pcifunc)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (layer == BAND_PROF_LEAF_LAYER &&\n\t\t\t    ipolicer->match_id[prof_idx])\n\t\t\t\tnix_clear_ratelimit_aggr(rvu, nix_hw, prof_idx);\n\n\t\t\tipolicer->pfvf_map[prof_idx] = 0x00;\n\t\t\tipolicer->match_id[prof_idx] = 0;\n\t\t\trvu_free_rsrc(&ipolicer->band_prof, prof_idx);\n\t\t}\n\t}\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn 0;\n}\n\nint rvu_mbox_handler_nix_bandprof_free(struct rvu *rvu,\n\t\t\t\t       struct nix_bandprof_free_req *req,\n\t\t\t\t       struct msg_rsp *rsp)\n{\n\tint blkaddr, layer, prof_idx, idx, err;\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct nix_ipolicer *ipolicer;\n\tstruct nix_hw *nix_hw;\n\n\tif (req->free_all)\n\t\treturn nix_free_all_bandprof(rvu, pcifunc);\n\n\tif (!rvu->hw->cap.ipolicer)\n\t\treturn NIX_AF_ERR_IPOLICER_NOTSUPP;\n\n\terr = nix_get_struct_ptrs(rvu, pcifunc, &nix_hw, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\tmutex_lock(&rvu->rsrc_lock);\n\t \n\tfor (layer = 0; layer < BAND_PROF_NUM_LAYERS; layer++) {\n\t\tif (layer == BAND_PROF_INVAL_LAYER)\n\t\t\tcontinue;\n\t\tif (!req->prof_count[layer])\n\t\t\tcontinue;\n\n\t\tipolicer = &nix_hw->ipolicer[layer];\n\t\tfor (idx = 0; idx < req->prof_count[layer]; idx++) {\n\t\t\tif (idx == MAX_BANDPROF_PER_PFFUNC)\n\t\t\t\tbreak;\n\t\t\tprof_idx = req->prof_idx[layer][idx];\n\t\t\tif (prof_idx >= ipolicer->band_prof.max ||\n\t\t\t    ipolicer->pfvf_map[prof_idx] != pcifunc)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (layer == BAND_PROF_LEAF_LAYER &&\n\t\t\t    ipolicer->match_id[prof_idx])\n\t\t\t\tnix_clear_ratelimit_aggr(rvu, nix_hw, prof_idx);\n\n\t\t\tipolicer->pfvf_map[prof_idx] = 0x00;\n\t\t\tipolicer->match_id[prof_idx] = 0;\n\t\t\trvu_free_rsrc(&ipolicer->band_prof, prof_idx);\n\t\t}\n\t}\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn 0;\n}\n\nint nix_aq_context_read(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\tstruct nix_cn10k_aq_enq_req *aq_req,\n\t\t\tstruct nix_cn10k_aq_enq_rsp *aq_rsp,\n\t\t\tu16 pcifunc, u8 ctype, u32 qidx)\n{\n\tmemset(aq_req, 0, sizeof(struct nix_cn10k_aq_enq_req));\n\taq_req->hdr.pcifunc = pcifunc;\n\taq_req->ctype = ctype;\n\taq_req->op = NIX_AQ_INSTOP_READ;\n\taq_req->qidx = qidx;\n\n\treturn rvu_nix_blk_aq_enq_inst(rvu, nix_hw,\n\t\t\t\t       (struct nix_aq_enq_req *)aq_req,\n\t\t\t\t       (struct nix_aq_enq_rsp *)aq_rsp);\n}\n\nstatic int nix_ipolicer_map_leaf_midprofs(struct rvu *rvu,\n\t\t\t\t\t  struct nix_hw *nix_hw,\n\t\t\t\t\t  struct nix_cn10k_aq_enq_req *aq_req,\n\t\t\t\t\t  struct nix_cn10k_aq_enq_rsp *aq_rsp,\n\t\t\t\t\t  u32 leaf_prof, u16 mid_prof)\n{\n\tmemset(aq_req, 0, sizeof(struct nix_cn10k_aq_enq_req));\n\taq_req->hdr.pcifunc = 0x00;\n\taq_req->ctype = NIX_AQ_CTYPE_BANDPROF;\n\taq_req->op = NIX_AQ_INSTOP_WRITE;\n\taq_req->qidx = leaf_prof;\n\n\taq_req->prof.band_prof_id = mid_prof;\n\taq_req->prof_mask.band_prof_id = GENMASK(6, 0);\n\taq_req->prof.hl_en = 1;\n\taq_req->prof_mask.hl_en = 1;\n\n\treturn rvu_nix_blk_aq_enq_inst(rvu, nix_hw,\n\t\t\t\t       (struct nix_aq_enq_req *)aq_req,\n\t\t\t\t       (struct nix_aq_enq_rsp *)aq_rsp);\n}\n\nint rvu_nix_setup_ratelimit_aggr(struct rvu *rvu, u16 pcifunc,\n\t\t\t\t u16 rq_idx, u16 match_id)\n{\n\tint leaf_prof, mid_prof, leaf_match;\n\tstruct nix_cn10k_aq_enq_req aq_req;\n\tstruct nix_cn10k_aq_enq_rsp aq_rsp;\n\tstruct nix_ipolicer *ipolicer;\n\tstruct nix_hw *nix_hw;\n\tint blkaddr, idx, rc;\n\n\tif (!rvu->hw->cap.ipolicer)\n\t\treturn 0;\n\n\trc = nix_get_struct_ptrs(rvu, pcifunc, &nix_hw, &blkaddr);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\trc = nix_aq_context_read(rvu, nix_hw, &aq_req, &aq_rsp, pcifunc,\n\t\t\t\t NIX_AQ_CTYPE_RQ, rq_idx);\n\tif (rc) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Failed to fetch RQ%d context of PFFUNC 0x%x\\n\",\n\t\t\t__func__, rq_idx, pcifunc);\n\t\treturn rc;\n\t}\n\n\tif (!aq_rsp.rq.policer_ena)\n\t\treturn 0;\n\n\t \n\tleaf_prof = aq_rsp.rq.band_prof_id;\n\n\tipolicer = &nix_hw->ipolicer[BAND_PROF_LEAF_LAYER];\n\tipolicer->match_id[leaf_prof] = match_id;\n\n\t \n\tfor (idx = 0; idx < ipolicer->band_prof.max; idx++) {\n\t\tif (idx == leaf_prof)\n\t\t\tcontinue;\n\t\tif (ipolicer->match_id[idx] != match_id)\n\t\t\tcontinue;\n\n\t\tleaf_match = idx;\n\t\tbreak;\n\t}\n\n\tif (idx == ipolicer->band_prof.max)\n\t\treturn 0;\n\n\t \n\trc = nix_aq_context_read(rvu, nix_hw, &aq_req, &aq_rsp, 0x00,\n\t\t\t\t NIX_AQ_CTYPE_BANDPROF, leaf_match);\n\tif (rc) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Failed to fetch context of leaf profile %d\\n\",\n\t\t\t__func__, leaf_match);\n\t\treturn rc;\n\t}\n\n\tipolicer = &nix_hw->ipolicer[BAND_PROF_MID_LAYER];\n\tif (aq_rsp.prof.hl_en) {\n\t\t \n\t\tmid_prof = aq_rsp.prof.band_prof_id;\n\t\trc = nix_ipolicer_map_leaf_midprofs(rvu, nix_hw,\n\t\t\t\t\t\t    &aq_req, &aq_rsp,\n\t\t\t\t\t\t    leaf_prof, mid_prof);\n\t\tif (rc) {\n\t\t\tdev_err(rvu->dev,\n\t\t\t\t\"%s: Failed to map leaf(%d) and mid(%d) profiles\\n\",\n\t\t\t\t__func__, leaf_prof, mid_prof);\n\t\t\tgoto exit;\n\t\t}\n\n\t\tmutex_lock(&rvu->rsrc_lock);\n\t\tipolicer->ref_count[mid_prof]++;\n\t\tmutex_unlock(&rvu->rsrc_lock);\n\t\tgoto exit;\n\t}\n\n\t \n\tmutex_lock(&rvu->rsrc_lock);\n\tmid_prof = rvu_alloc_rsrc(&ipolicer->band_prof);\n\tif (mid_prof < 0) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Unable to allocate mid layer profile\\n\", __func__);\n\t\tmutex_unlock(&rvu->rsrc_lock);\n\t\tgoto exit;\n\t}\n\tmutex_unlock(&rvu->rsrc_lock);\n\tipolicer->pfvf_map[mid_prof] = 0x00;\n\tipolicer->ref_count[mid_prof] = 0;\n\n\t \n\trc = nix_aq_context_read(rvu, nix_hw, &aq_req, &aq_rsp, 0x00,\n\t\t\t\t NIX_AQ_CTYPE_BANDPROF, leaf_prof);\n\tif (rc) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Failed to fetch context of leaf profile %d\\n\",\n\t\t\t__func__, leaf_prof);\n\t\tgoto exit;\n\t}\n\n\tmemset(&aq_req, 0, sizeof(struct nix_cn10k_aq_enq_req));\n\taq_req.hdr.pcifunc = 0x00;\n\taq_req.qidx = (mid_prof & 0x3FFF) | (BAND_PROF_MID_LAYER << 14);\n\taq_req.ctype = NIX_AQ_CTYPE_BANDPROF;\n\taq_req.op = NIX_AQ_INSTOP_WRITE;\n\tmemcpy(&aq_req.prof, &aq_rsp.prof, sizeof(struct nix_bandprof_s));\n\tmemset((char *)&aq_req.prof_mask, 0xff, sizeof(struct nix_bandprof_s));\n\t \n\taq_req.prof.hl_en = 0;\n\taq_req.prof_mask.hl_en = 1;\n\n\trc = rvu_nix_blk_aq_enq_inst(rvu, nix_hw,\n\t\t\t\t     (struct nix_aq_enq_req *)&aq_req, NULL);\n\tif (rc) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Failed to INIT context of mid layer profile %d\\n\",\n\t\t\t__func__, mid_prof);\n\t\tgoto exit;\n\t}\n\n\t \n\trc = nix_ipolicer_map_leaf_midprofs(rvu, nix_hw,\n\t\t\t\t\t    &aq_req, &aq_rsp,\n\t\t\t\t\t    leaf_prof, mid_prof);\n\tif (rc) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Failed to map leaf(%d) and mid(%d) profiles\\n\",\n\t\t\t__func__, leaf_prof, mid_prof);\n\t\tgoto exit;\n\t}\n\n\tmutex_lock(&rvu->rsrc_lock);\n\tipolicer->ref_count[mid_prof]++;\n\tmutex_unlock(&rvu->rsrc_lock);\n\n\trc = nix_ipolicer_map_leaf_midprofs(rvu, nix_hw,\n\t\t\t\t\t    &aq_req, &aq_rsp,\n\t\t\t\t\t    leaf_match, mid_prof);\n\tif (rc) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Failed to map leaf(%d) and mid(%d) profiles\\n\",\n\t\t\t__func__, leaf_match, mid_prof);\n\t\tipolicer->ref_count[mid_prof]--;\n\t\tgoto exit;\n\t}\n\n\tmutex_lock(&rvu->rsrc_lock);\n\tipolicer->ref_count[mid_prof]++;\n\tmutex_unlock(&rvu->rsrc_lock);\n\nexit:\n\treturn rc;\n}\n\n \nstatic void nix_clear_ratelimit_aggr(struct rvu *rvu, struct nix_hw *nix_hw,\n\t\t\t\t     u32 leaf_prof)\n{\n\tstruct nix_cn10k_aq_enq_req aq_req;\n\tstruct nix_cn10k_aq_enq_rsp aq_rsp;\n\tstruct nix_ipolicer *ipolicer;\n\tu16 mid_prof;\n\tint rc;\n\n\tmutex_unlock(&rvu->rsrc_lock);\n\n\trc = nix_aq_context_read(rvu, nix_hw, &aq_req, &aq_rsp, 0x00,\n\t\t\t\t NIX_AQ_CTYPE_BANDPROF, leaf_prof);\n\n\tmutex_lock(&rvu->rsrc_lock);\n\tif (rc) {\n\t\tdev_err(rvu->dev,\n\t\t\t\"%s: Failed to fetch context of leaf profile %d\\n\",\n\t\t\t__func__, leaf_prof);\n\t\treturn;\n\t}\n\n\tif (!aq_rsp.prof.hl_en)\n\t\treturn;\n\n\tmid_prof = aq_rsp.prof.band_prof_id;\n\tipolicer = &nix_hw->ipolicer[BAND_PROF_MID_LAYER];\n\tipolicer->ref_count[mid_prof]--;\n\t \n\tif (!ipolicer->ref_count[mid_prof]) {\n\t\tipolicer->pfvf_map[mid_prof] = 0x00;\n\t\trvu_free_rsrc(&ipolicer->band_prof, mid_prof);\n\t}\n}\n\nint rvu_mbox_handler_nix_bandprof_get_hwinfo(struct rvu *rvu, struct msg_req *req,\n\t\t\t\t\t     struct nix_bandprof_get_hwinfo_rsp *rsp)\n{\n\tstruct nix_ipolicer *ipolicer;\n\tint blkaddr, layer, err;\n\tstruct nix_hw *nix_hw;\n\tu64 tu;\n\n\tif (!rvu->hw->cap.ipolicer)\n\t\treturn NIX_AF_ERR_IPOLICER_NOTSUPP;\n\n\terr = nix_get_struct_ptrs(rvu, req->hdr.pcifunc, &nix_hw, &blkaddr);\n\tif (err)\n\t\treturn err;\n\n\t \n\tmutex_lock(&rvu->rsrc_lock);\n\tfor (layer = 0; layer < BAND_PROF_NUM_LAYERS; layer++) {\n\t\tif (layer == BAND_PROF_INVAL_LAYER)\n\t\t\tcontinue;\n\n\t\tipolicer = &nix_hw->ipolicer[layer];\n\t\trsp->prof_count[layer] = rvu_rsrc_free_count(&ipolicer->band_prof);\n\t}\n\tmutex_unlock(&rvu->rsrc_lock);\n\n\t \n\ttu = rvu_read64(rvu, blkaddr, NIX_AF_PL_TS) & GENMASK_ULL(9, 0);\n\trsp->policer_timeunit = (tu + 1) * 100;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}