{
  "module_name": "mcs_rvu_if.c",
  "hash_id": "6bc4fe076daa97e270fa6558401ebab317e46433435d7eddd6db91d53ed36fef",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/marvell/octeontx2/af/mcs_rvu_if.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/device.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include \"mcs.h\"\n#include \"rvu.h\"\n#include \"mcs_reg.h\"\n#include \"lmac_common.h\"\n\n#define M(_name, _id, _fn_name, _req_type, _rsp_type)\t\t\t\\\nstatic struct _req_type __maybe_unused\t\t\t\t\t\\\n*otx2_mbox_alloc_msg_ ## _fn_name(struct rvu *rvu, int devid)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct _req_type *req;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treq = (struct _req_type *)otx2_mbox_alloc_msg_rsp(\t\t\\\n\t\t&rvu->afpf_wq_info.mbox_up, devid, sizeof(struct _req_type), \\\n\t\tsizeof(struct _rsp_type));\t\t\t\t\\\n\tif (!req)\t\t\t\t\t\t\t\\\n\t\treturn NULL;\t\t\t\t\t\t\\\n\treq->hdr.sig = OTX2_MBOX_REQ_SIG;\t\t\t\t\\\n\treq->hdr.id = _id;\t\t\t\t\t\t\\\n\treturn req;\t\t\t\t\t\t\t\\\n}\n\nMBOX_UP_MCS_MESSAGES\n#undef M\n\nvoid rvu_mcs_ptp_cfg(struct rvu *rvu, u8 rpm_id, u8 lmac_id, bool ena)\n{\n\tstruct mcs *mcs;\n\tu64 cfg;\n\tu8 port;\n\n\tif (!rvu->mcs_blk_cnt)\n\t\treturn;\n\n\t \n\n\t \n\tif (rvu->mcs_blk_cnt > 1) {\n\t\tmcs = mcs_get_pdata(rpm_id);\n\t\tcfg = mcs_reg_read(mcs, MCSX_PEX_RX_SLAVE_PEX_CONFIGURATION);\n\t\tif (ena)\n\t\t\tcfg |= BIT_ULL(lmac_id);\n\t\telse\n\t\t\tcfg &= ~BIT_ULL(lmac_id);\n\t\tmcs_reg_write(mcs, MCSX_PEX_RX_SLAVE_PEX_CONFIGURATION, cfg);\n\t\treturn;\n\t}\n\t \n\tmcs = mcs_get_pdata(0);\n\tport = (rpm_id * rvu->hw->lmac_per_cgx) + lmac_id;\n\tcfg = mcs_reg_read(mcs, MCSX_PEX_RX_SLAVE_PORT_CFGX(port));\n\tif (ena)\n\t\tcfg |= BIT_ULL(0);\n\telse\n\t\tcfg &= ~BIT_ULL(0);\n\tmcs_reg_write(mcs, MCSX_PEX_RX_SLAVE_PORT_CFGX(port), cfg);\n}\n\nint rvu_mbox_handler_mcs_set_lmac_mode(struct rvu *rvu,\n\t\t\t\t       struct mcs_set_lmac_mode *req,\n\t\t\t\t       struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tif (BIT_ULL(req->lmac_id) & mcs->hw->lmac_bmap)\n\t\tmcs_set_lmac_mode(mcs, req->lmac_id, req->mode);\n\n\treturn 0;\n}\n\nint mcs_add_intr_wq_entry(struct mcs *mcs, struct mcs_intr_event *event)\n{\n\tstruct mcs_intrq_entry *qentry;\n\tu16 pcifunc = event->pcifunc;\n\tstruct rvu *rvu = mcs->rvu;\n\tstruct mcs_pfvf *pfvf;\n\n\t \n\tif (pcifunc & RVU_PFVF_FUNC_MASK)\n\t\tpfvf = &mcs->vf[rvu_get_hwvf(rvu, pcifunc)];\n\telse\n\t\tpfvf = &mcs->pf[rvu_get_pf(pcifunc)];\n\n\tevent->intr_mask &= pfvf->intr_mask;\n\n\t \n\tif (!(pfvf->intr_mask && event->intr_mask))\n\t\treturn 0;\n\n\tqentry = kmalloc(sizeof(*qentry), GFP_ATOMIC);\n\tif (!qentry)\n\t\treturn -ENOMEM;\n\n\tqentry->intr_event = *event;\n\tspin_lock(&rvu->mcs_intrq_lock);\n\tlist_add_tail(&qentry->node, &rvu->mcs_intrq_head);\n\tspin_unlock(&rvu->mcs_intrq_lock);\n\tqueue_work(rvu->mcs_intr_wq, &rvu->mcs_intr_work);\n\n\treturn 0;\n}\n\nstatic int mcs_notify_pfvf(struct mcs_intr_event *event, struct rvu *rvu)\n{\n\tstruct mcs_intr_info *req;\n\tint err, pf;\n\n\tpf = rvu_get_pf(event->pcifunc);\n\n\treq = otx2_mbox_alloc_msg_mcs_intr_notify(rvu, pf);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\treq->mcs_id = event->mcs_id;\n\treq->intr_mask = event->intr_mask;\n\treq->sa_id = event->sa_id;\n\treq->hdr.pcifunc = event->pcifunc;\n\treq->lmac_id = event->lmac_id;\n\n\totx2_mbox_msg_send(&rvu->afpf_wq_info.mbox_up, pf);\n\terr = otx2_mbox_wait_for_rsp(&rvu->afpf_wq_info.mbox_up, pf);\n\tif (err)\n\t\tdev_warn(rvu->dev, \"MCS notification to pf %d failed\\n\", pf);\n\n\treturn 0;\n}\n\nstatic void mcs_intr_handler_task(struct work_struct *work)\n{\n\tstruct rvu *rvu = container_of(work, struct rvu, mcs_intr_work);\n\tstruct mcs_intrq_entry *qentry;\n\tstruct mcs_intr_event *event;\n\tunsigned long flags;\n\n\tdo {\n\t\tspin_lock_irqsave(&rvu->mcs_intrq_lock, flags);\n\t\tqentry = list_first_entry_or_null(&rvu->mcs_intrq_head,\n\t\t\t\t\t\t  struct mcs_intrq_entry,\n\t\t\t\t\t\t  node);\n\t\tif (qentry)\n\t\t\tlist_del(&qentry->node);\n\n\t\tspin_unlock_irqrestore(&rvu->mcs_intrq_lock, flags);\n\t\tif (!qentry)\n\t\t\tbreak;  \n\n\t\tevent = &qentry->intr_event;\n\n\t\tmcs_notify_pfvf(event, rvu);\n\t\tkfree(qentry);\n\t} while (1);\n}\n\nint rvu_mbox_handler_mcs_intr_cfg(struct rvu *rvu,\n\t\t\t\t  struct mcs_intr_cfg *req,\n\t\t\t\t  struct msg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct mcs_pfvf *pfvf;\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\t \n\tif (pcifunc & RVU_PFVF_FUNC_MASK)\n\t\tpfvf = &mcs->vf[rvu_get_hwvf(rvu, pcifunc)];\n\telse\n\t\tpfvf = &mcs->pf[rvu_get_pf(pcifunc)];\n\n\tmcs->pf_map[0] = pcifunc;\n\tpfvf->intr_mask = req->intr_mask;\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_get_hw_info(struct rvu *rvu,\n\t\t\t\t     struct msg_req *req,\n\t\t\t\t     struct mcs_hw_info *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (!rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_NOT_MAPPED;\n\n\t \n\tmcs = mcs_get_pdata(0);\n\trsp->num_mcs_blks = rvu->mcs_blk_cnt;\n\trsp->tcam_entries = mcs->hw->tcam_entries;\n\trsp->secy_entries = mcs->hw->secy_entries;\n\trsp->sc_entries = mcs->hw->sc_entries;\n\trsp->sa_entries = mcs->hw->sa_entries;\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_port_reset(struct rvu *rvu, struct mcs_port_reset_req *req,\n\t\t\t\t    struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tmcs_reset_port(mcs, req->port_id, req->reset);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_clear_stats(struct rvu *rvu,\n\t\t\t\t     struct mcs_clear_stats *req,\n\t\t\t\t     struct msg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tmutex_lock(&mcs->stats_lock);\n\tif (req->all)\n\t\tmcs_clear_all_stats(mcs, pcifunc, req->dir);\n\telse\n\t\tmcs_clear_stats(mcs, req->type, req->id, req->dir);\n\n\tmutex_unlock(&mcs->stats_lock);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_get_flowid_stats(struct rvu *rvu,\n\t\t\t\t\t  struct mcs_stats_req *req,\n\t\t\t\t\t  struct mcs_flowid_stats *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\t \n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, true);\n\n\tmutex_lock(&mcs->stats_lock);\n\tmcs_get_flowid_stats(mcs, rsp, req->id, req->dir);\n\tmutex_unlock(&mcs->stats_lock);\n\n\t \n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, false);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_get_secy_stats(struct rvu *rvu,\n\t\t\t\t\tstruct mcs_stats_req *req,\n\t\t\t\t\tstruct mcs_secy_stats *rsp)\n{\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, true);\n\n\tmutex_lock(&mcs->stats_lock);\n\n\tif (req->dir == MCS_RX)\n\t\tmcs_get_rx_secy_stats(mcs, rsp, req->id);\n\telse\n\t\tmcs_get_tx_secy_stats(mcs, rsp, req->id);\n\n\tmutex_unlock(&mcs->stats_lock);\n\n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, false);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_get_sc_stats(struct rvu *rvu,\n\t\t\t\t      struct mcs_stats_req *req,\n\t\t\t\t      struct mcs_sc_stats *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, true);\n\n\tmutex_lock(&mcs->stats_lock);\n\tmcs_get_sc_stats(mcs, rsp, req->id, req->dir);\n\tmutex_unlock(&mcs->stats_lock);\n\n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, false);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_get_sa_stats(struct rvu *rvu,\n\t\t\t\t      struct mcs_stats_req *req,\n\t\t\t\t      struct mcs_sa_stats *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, true);\n\n\tmutex_lock(&mcs->stats_lock);\n\tmcs_get_sa_stats(mcs, rsp, req->id, req->dir);\n\tmutex_unlock(&mcs->stats_lock);\n\n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, false);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_get_port_stats(struct rvu *rvu,\n\t\t\t\t\tstruct mcs_stats_req *req,\n\t\t\t\t\tstruct mcs_port_stats *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, true);\n\n\tmutex_lock(&mcs->stats_lock);\n\tmcs_get_port_stats(mcs, rsp, req->id, req->dir);\n\tmutex_unlock(&mcs->stats_lock);\n\n\tif (mcs->hw->mcs_blks > 1)\n\t\tmcs_set_force_clk_en(mcs, false);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_set_active_lmac(struct rvu *rvu,\n\t\t\t\t\t struct mcs_set_active_lmac *req,\n\t\t\t\t\t struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\tif (!mcs)\n\t\treturn MCS_AF_ERR_NOT_MAPPED;\n\n\tmcs->hw->lmac_bmap = req->lmac_bmap;\n\tmcs_set_lmac_channels(req->mcs_id, req->chan_base);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_port_cfg_set(struct rvu *rvu, struct mcs_port_cfg_set_req *req,\n\t\t\t\t      struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tif (mcs->hw->lmac_cnt <= req->port_id || !(mcs->hw->lmac_bmap & BIT_ULL(req->port_id)))\n\t\treturn -EINVAL;\n\n\tmcs_set_port_cfg(mcs, req);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_port_cfg_get(struct rvu *rvu, struct mcs_port_cfg_get_req *req,\n\t\t\t\t      struct mcs_port_cfg_get_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tif (mcs->hw->lmac_cnt <= req->port_id || !(mcs->hw->lmac_bmap & BIT_ULL(req->port_id)))\n\t\treturn -EINVAL;\n\n\tmcs_get_port_cfg(mcs, req, rsp);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_custom_tag_cfg_get(struct rvu *rvu, struct mcs_custom_tag_cfg_get_req *req,\n\t\t\t\t\t    struct mcs_custom_tag_cfg_get_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tmcs_get_custom_tag_cfg(mcs, req, rsp);\n\n\treturn 0;\n}\n\nint rvu_mcs_flr_handler(struct rvu *rvu, u16 pcifunc)\n{\n\tstruct mcs *mcs;\n\tint mcs_id;\n\n\t \n\tif (rvu->mcs_blk_cnt > 1) {\n\t\tfor (mcs_id = 0; mcs_id < rvu->mcs_blk_cnt; mcs_id++) {\n\t\t\tmcs = mcs_get_pdata(mcs_id);\n\t\t\tmcs_free_all_rsrc(mcs, MCS_RX, pcifunc);\n\t\t\tmcs_free_all_rsrc(mcs, MCS_TX, pcifunc);\n\t\t}\n\t} else {\n\t\t \n\t\tmcs = mcs_get_pdata(0);\n\t\tmcs_free_all_rsrc(mcs, MCS_RX, pcifunc);\n\t\tmcs_free_all_rsrc(mcs, MCS_TX, pcifunc);\n\t}\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_flowid_ena_entry(struct rvu *rvu,\n\t\t\t\t\t  struct mcs_flowid_ena_dis_entry *req,\n\t\t\t\t\t  struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\tmcs_ena_dis_flowid_entry(mcs, req->flow_id, req->dir, req->ena);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_pn_table_write(struct rvu *rvu,\n\t\t\t\t\tstruct mcs_pn_table_write_req *req,\n\t\t\t\t\tstruct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\tmcs_pn_table_write(mcs, req->pn_id, req->next_pn, req->dir);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_set_pn_threshold(struct rvu *rvu,\n\t\t\t\t\t  struct mcs_set_pn_threshold *req,\n\t\t\t\t\t  struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tmcs_pn_threshold_set(mcs, req);\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_rx_sc_sa_map_write(struct rvu *rvu,\n\t\t\t\t\t    struct mcs_rx_sc_sa_map *req,\n\t\t\t\t\t    struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\tmcs->mcs_ops->mcs_rx_sa_mem_map_write(mcs, req);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_tx_sc_sa_map_write(struct rvu *rvu,\n\t\t\t\t\t    struct mcs_tx_sc_sa_map *req,\n\t\t\t\t\t    struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\tmcs->mcs_ops->mcs_tx_sa_mem_map_write(mcs, req);\n\tmcs->tx_sa_active[req->sc_id] = req->tx_sa_active;\n\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_sa_plcy_write(struct rvu *rvu,\n\t\t\t\t       struct mcs_sa_plcy_write_req *req,\n\t\t\t\t       struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\tint i;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tfor (i = 0; i < req->sa_cnt; i++)\n\t\tmcs_sa_plcy_write(mcs, &req->plcy[i][0],\n\t\t\t\t  req->sa_index[i], req->dir);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_rx_sc_cam_write(struct rvu *rvu,\n\t\t\t\t\t struct mcs_rx_sc_cam_write_req *req,\n\t\t\t\t\t struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\tmcs_rx_sc_cam_write(mcs, req->sci, req->secy_id, req->sc_id);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_secy_plcy_write(struct rvu *rvu,\n\t\t\t\t\t struct mcs_secy_plcy_write_req *req,\n\t\t\t\t\t struct msg_rsp *rsp)\n{\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tmcs_secy_plcy_write(mcs, req->plcy,\n\t\t\t    req->secy_id, req->dir);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_flowid_entry_write(struct rvu *rvu,\n\t\t\t\t\t    struct mcs_flowid_entry_write_req *req,\n\t\t\t\t\t    struct msg_rsp *rsp)\n{\n\tstruct secy_mem_map map;\n\tstruct mcs *mcs;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\t \n\tmcs_flowid_entry_write(mcs, req->data, req->mask,\n\t\t\t       req->flow_id, req->dir);\n\tmap.secy = req->secy_id;\n\tmap.sc = req->sc_id;\n\tmap.ctrl_pkt = req->ctrl_pkt;\n\tmap.flow_id = req->flow_id;\n\tmap.sci = req->sci;\n\tmcs->mcs_ops->mcs_flowid_secy_map(mcs, &map, req->dir);\n\tif (req->ena)\n\t\tmcs_ena_dis_flowid_entry(mcs, req->flow_id,\n\t\t\t\t\t req->dir, true);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_free_resources(struct rvu *rvu,\n\t\t\t\t\tstruct mcs_free_rsrc_req *req,\n\t\t\t\t\tstruct msg_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct mcs_rsrc_map *map;\n\tstruct mcs *mcs;\n\tint rc = 0;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tif (req->dir == MCS_RX)\n\t\tmap = &mcs->rx;\n\telse\n\t\tmap = &mcs->tx;\n\n\tmutex_lock(&rvu->rsrc_lock);\n\t \n\tif (req->all) {\n\t\trc = mcs_free_all_rsrc(mcs, req->dir, pcifunc);\n\t\tgoto exit;\n\t}\n\n\tswitch (req->rsrc_type) {\n\tcase MCS_RSRC_TYPE_FLOWID:\n\t\trc = mcs_free_rsrc(&map->flow_ids, map->flowid2pf_map, req->rsrc_id, pcifunc);\n\t\tmcs_ena_dis_flowid_entry(mcs, req->rsrc_id, req->dir, false);\n\t\tbreak;\n\tcase MCS_RSRC_TYPE_SECY:\n\t\trc =  mcs_free_rsrc(&map->secy, map->secy2pf_map, req->rsrc_id, pcifunc);\n\t\tmcs_clear_secy_plcy(mcs, req->rsrc_id, req->dir);\n\t\tbreak;\n\tcase MCS_RSRC_TYPE_SC:\n\t\trc = mcs_free_rsrc(&map->sc, map->sc2pf_map, req->rsrc_id, pcifunc);\n\t\t \n\t\tif (req->dir == MCS_RX)\n\t\t\tmcs_ena_dis_sc_cam_entry(mcs, req->rsrc_id, false);\n\t\tbreak;\n\tcase MCS_RSRC_TYPE_SA:\n\t\trc = mcs_free_rsrc(&map->sa, map->sa2pf_map, req->rsrc_id, pcifunc);\n\t\tbreak;\n\t}\nexit:\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn rc;\n}\n\nint rvu_mbox_handler_mcs_alloc_resources(struct rvu *rvu,\n\t\t\t\t\t struct mcs_alloc_rsrc_req *req,\n\t\t\t\t\t struct mcs_alloc_rsrc_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct mcs_rsrc_map *map;\n\tstruct mcs *mcs;\n\tint rsrc_id, i;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tif (req->dir == MCS_RX)\n\t\tmap = &mcs->rx;\n\telse\n\t\tmap = &mcs->tx;\n\n\tmutex_lock(&rvu->rsrc_lock);\n\n\tif (req->all) {\n\t\trsrc_id = mcs_alloc_all_rsrc(mcs, &rsp->flow_ids[0],\n\t\t\t\t\t     &rsp->secy_ids[0],\n\t\t\t\t\t     &rsp->sc_ids[0],\n\t\t\t\t\t     &rsp->sa_ids[0],\n\t\t\t\t\t     &rsp->sa_ids[1],\n\t\t\t\t\t     pcifunc, req->dir);\n\t\tgoto exit;\n\t}\n\n\tswitch (req->rsrc_type) {\n\tcase MCS_RSRC_TYPE_FLOWID:\n\t\tfor (i = 0; i < req->rsrc_cnt; i++) {\n\t\t\trsrc_id = mcs_alloc_rsrc(&map->flow_ids, map->flowid2pf_map, pcifunc);\n\t\t\tif (rsrc_id < 0)\n\t\t\t\tgoto exit;\n\t\t\trsp->flow_ids[i] = rsrc_id;\n\t\t\trsp->rsrc_cnt++;\n\t\t}\n\t\tbreak;\n\tcase MCS_RSRC_TYPE_SECY:\n\t\tfor (i = 0; i < req->rsrc_cnt; i++) {\n\t\t\trsrc_id = mcs_alloc_rsrc(&map->secy, map->secy2pf_map, pcifunc);\n\t\t\tif (rsrc_id < 0)\n\t\t\t\tgoto exit;\n\t\t\trsp->secy_ids[i] = rsrc_id;\n\t\t\trsp->rsrc_cnt++;\n\t\t}\n\t\tbreak;\n\tcase MCS_RSRC_TYPE_SC:\n\t\tfor (i = 0; i < req->rsrc_cnt; i++) {\n\t\t\trsrc_id = mcs_alloc_rsrc(&map->sc, map->sc2pf_map, pcifunc);\n\t\t\tif (rsrc_id < 0)\n\t\t\t\tgoto exit;\n\t\t\trsp->sc_ids[i] = rsrc_id;\n\t\t\trsp->rsrc_cnt++;\n\t\t}\n\t\tbreak;\n\tcase MCS_RSRC_TYPE_SA:\n\t\tfor (i = 0; i < req->rsrc_cnt; i++) {\n\t\t\trsrc_id = mcs_alloc_rsrc(&map->sa, map->sa2pf_map, pcifunc);\n\t\t\tif (rsrc_id < 0)\n\t\t\t\tgoto exit;\n\t\t\trsp->sa_ids[i] = rsrc_id;\n\t\t\trsp->rsrc_cnt++;\n\t\t}\n\t\tbreak;\n\t}\n\n\trsp->rsrc_type = req->rsrc_type;\n\trsp->dir = req->dir;\n\trsp->mcs_id = req->mcs_id;\n\trsp->all = req->all;\n\nexit:\n\tif (rsrc_id < 0)\n\t\tdev_err(rvu->dev, \"Failed to allocate the mcs resources for PCIFUNC:%d\\n\", pcifunc);\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn 0;\n}\n\nint rvu_mbox_handler_mcs_alloc_ctrl_pkt_rule(struct rvu *rvu,\n\t\t\t\t\t     struct mcs_alloc_ctrl_pkt_rule_req *req,\n\t\t\t\t\t     struct mcs_alloc_ctrl_pkt_rule_rsp *rsp)\n{\n\tu16 pcifunc = req->hdr.pcifunc;\n\tstruct mcs_rsrc_map *map;\n\tstruct mcs *mcs;\n\tint rsrc_id;\n\tu16 offset;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tmap = (req->dir == MCS_RX) ? &mcs->rx : &mcs->tx;\n\n\tmutex_lock(&rvu->rsrc_lock);\n\n\tswitch (req->rule_type) {\n\tcase MCS_CTRL_PKT_RULE_TYPE_ETH:\n\t\toffset = MCS_CTRLPKT_ETYPE_RULE_OFFSET;\n\t\tbreak;\n\tcase MCS_CTRL_PKT_RULE_TYPE_DA:\n\t\toffset = MCS_CTRLPKT_DA_RULE_OFFSET;\n\t\tbreak;\n\tcase MCS_CTRL_PKT_RULE_TYPE_RANGE:\n\t\toffset = MCS_CTRLPKT_DA_RANGE_RULE_OFFSET;\n\t\tbreak;\n\tcase MCS_CTRL_PKT_RULE_TYPE_COMBO:\n\t\toffset = MCS_CTRLPKT_COMBO_RULE_OFFSET;\n\t\tbreak;\n\tcase MCS_CTRL_PKT_RULE_TYPE_MAC:\n\t\toffset = MCS_CTRLPKT_MAC_EN_RULE_OFFSET;\n\t\tbreak;\n\t}\n\n\trsrc_id = mcs_alloc_ctrlpktrule(&map->ctrlpktrule, map->ctrlpktrule2pf_map, offset,\n\t\t\t\t\tpcifunc);\n\tif (rsrc_id < 0)\n\t\tgoto exit;\n\n\trsp->rule_idx = rsrc_id;\n\trsp->rule_type = req->rule_type;\n\trsp->dir = req->dir;\n\trsp->mcs_id = req->mcs_id;\n\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn 0;\nexit:\n\tif (rsrc_id < 0)\n\t\tdev_err(rvu->dev, \"Failed to allocate the mcs ctrl pkt rule for PCIFUNC:%d\\n\",\n\t\t\tpcifunc);\n\tmutex_unlock(&rvu->rsrc_lock);\n\treturn rsrc_id;\n}\n\nint rvu_mbox_handler_mcs_free_ctrl_pkt_rule(struct rvu *rvu,\n\t\t\t\t\t    struct mcs_free_ctrl_pkt_rule_req *req,\n\t\t\t\t\t    struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\tint rc;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\tmutex_lock(&rvu->rsrc_lock);\n\n\trc = mcs_free_ctrlpktrule(mcs, req);\n\n\tmutex_unlock(&rvu->rsrc_lock);\n\n\treturn rc;\n}\n\nint rvu_mbox_handler_mcs_ctrl_pkt_rule_write(struct rvu *rvu,\n\t\t\t\t\t     struct mcs_ctrl_pkt_rule_write_req *req,\n\t\t\t\t\t     struct msg_rsp *rsp)\n{\n\tstruct mcs *mcs;\n\tint rc;\n\n\tif (req->mcs_id >= rvu->mcs_blk_cnt)\n\t\treturn MCS_AF_ERR_INVALID_MCSID;\n\n\tmcs = mcs_get_pdata(req->mcs_id);\n\n\trc = mcs_ctrlpktrule_write(mcs, req);\n\n\treturn rc;\n}\n\nstatic void rvu_mcs_set_lmac_bmap(struct rvu *rvu)\n{\n\tstruct mcs *mcs = mcs_get_pdata(0);\n\tunsigned long lmac_bmap;\n\tint cgx, lmac, port;\n\n\tfor (port = 0; port < mcs->hw->lmac_cnt; port++) {\n\t\tcgx = port / rvu->hw->lmac_per_cgx;\n\t\tlmac = port % rvu->hw->lmac_per_cgx;\n\t\tif (!is_lmac_valid(rvu_cgx_pdata(cgx, rvu), lmac))\n\t\t\tcontinue;\n\t\tset_bit(port, &lmac_bmap);\n\t}\n\tmcs->hw->lmac_bmap = lmac_bmap;\n}\n\nint rvu_mcs_init(struct rvu *rvu)\n{\n\tstruct rvu_hwinfo *hw = rvu->hw;\n\tint lmac, err = 0, mcs_id;\n\tstruct mcs *mcs;\n\n\trvu->mcs_blk_cnt = mcs_get_blkcnt();\n\n\tif (!rvu->mcs_blk_cnt)\n\t\treturn 0;\n\n\t \n\tif (rvu->mcs_blk_cnt == 1) {\n\t\terr = mcs_set_lmac_channels(0, hw->cgx_chan_base);\n\t\tif (err)\n\t\t\treturn err;\n\t\t \n\t\trvu_mcs_set_lmac_bmap(rvu);\n\t}\n\n\t \n\tfor (mcs_id = 0; mcs_id < rvu->mcs_blk_cnt; mcs_id++) {\n\t\tmcs = mcs_get_pdata(mcs_id);\n\t\tmcs_install_flowid_bypass_entry(mcs);\n\t\tfor (lmac = 0; lmac < mcs->hw->lmac_cnt; lmac++)\n\t\t\tmcs_set_lmac_mode(mcs, lmac, 0);\n\n\t\tmcs->rvu = rvu;\n\n\t\t \n\t\tmcs->pf = devm_kcalloc(mcs->dev, hw->total_pfs,\n\t\t\t\t       sizeof(struct mcs_pfvf), GFP_KERNEL);\n\t\tif (!mcs->pf)\n\t\t\treturn -ENOMEM;\n\n\t\tmcs->vf = devm_kcalloc(mcs->dev, hw->total_vfs,\n\t\t\t\t       sizeof(struct mcs_pfvf), GFP_KERNEL);\n\t\tif (!mcs->vf)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tINIT_LIST_HEAD(&rvu->mcs_intrq_head);\n\tINIT_WORK(&rvu->mcs_intr_work, mcs_intr_handler_task);\n\trvu->mcs_intr_wq = alloc_workqueue(\"mcs_intr_wq\", 0, 0);\n\tif (!rvu->mcs_intr_wq) {\n\t\tdev_err(rvu->dev, \"mcs alloc workqueue failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn err;\n}\n\nvoid rvu_mcs_exit(struct rvu *rvu)\n{\n\tif (!rvu->mcs_intr_wq)\n\t\treturn;\n\n\tflush_workqueue(rvu->mcs_intr_wq);\n\tdestroy_workqueue(rvu->mcs_intr_wq);\n\trvu->mcs_intr_wq = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}