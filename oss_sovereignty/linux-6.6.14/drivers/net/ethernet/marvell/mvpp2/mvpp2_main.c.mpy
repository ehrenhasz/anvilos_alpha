{
  "module_name": "mvpp2_main.c",
  "hash_id": "3f41d4f288f3e660646fce996820732bf076822564e6b4ef6aac8a920310c207",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/marvell/mvpp2/mvpp2_main.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/kernel.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/platform_device.h>\n#include <linux/skbuff.h>\n#include <linux/inetdevice.h>\n#include <linux/mbus.h>\n#include <linux/module.h>\n#include <linux/mfd/syscon.h>\n#include <linux/interrupt.h>\n#include <linux/cpumask.h>\n#include <linux/of.h>\n#include <linux/of_irq.h>\n#include <linux/of_mdio.h>\n#include <linux/of_net.h>\n#include <linux/of_address.h>\n#include <linux/phy.h>\n#include <linux/phylink.h>\n#include <linux/phy/phy.h>\n#include <linux/ptp_classify.h>\n#include <linux/clk.h>\n#include <linux/hrtimer.h>\n#include <linux/ktime.h>\n#include <linux/regmap.h>\n#include <uapi/linux/ppp_defs.h>\n#include <net/ip.h>\n#include <net/ipv6.h>\n#include <net/page_pool/helpers.h>\n#include <net/tso.h>\n#include <linux/bpf_trace.h>\n\n#include \"mvpp2.h\"\n#include \"mvpp2_prs.h\"\n#include \"mvpp2_cls.h\"\n\nenum mvpp2_bm_pool_log_num {\n\tMVPP2_BM_SHORT,\n\tMVPP2_BM_LONG,\n\tMVPP2_BM_JUMBO,\n\tMVPP2_BM_POOLS_NUM\n};\n\nstatic struct {\n\tint pkt_size;\n\tint buf_num;\n} mvpp2_pools[MVPP2_BM_POOLS_NUM];\n\n \nstatic void mvpp2_acpi_start(struct mvpp2_port *port);\n\n \n#define MVPP2_QDIST_SINGLE_MODE\t0\n#define MVPP2_QDIST_MULTI_MODE\t1\n\nstatic int queue_mode = MVPP2_QDIST_MULTI_MODE;\n\nmodule_param(queue_mode, int, 0444);\nMODULE_PARM_DESC(queue_mode, \"Set queue_mode (single=0, multi=1)\");\n\n \n\nvoid mvpp2_write(struct mvpp2 *priv, u32 offset, u32 data)\n{\n\twritel(data, priv->swth_base[0] + offset);\n}\n\nu32 mvpp2_read(struct mvpp2 *priv, u32 offset)\n{\n\treturn readl(priv->swth_base[0] + offset);\n}\n\nstatic u32 mvpp2_read_relaxed(struct mvpp2 *priv, u32 offset)\n{\n\treturn readl_relaxed(priv->swth_base[0] + offset);\n}\n\nstatic inline u32 mvpp2_cpu_to_thread(struct mvpp2 *priv, int cpu)\n{\n\treturn cpu % priv->nthreads;\n}\n\nstatic void mvpp2_cm3_write(struct mvpp2 *priv, u32 offset, u32 data)\n{\n\twritel(data, priv->cm3_base + offset);\n}\n\nstatic u32 mvpp2_cm3_read(struct mvpp2 *priv, u32 offset)\n{\n\treturn readl(priv->cm3_base + offset);\n}\n\nstatic struct page_pool *\nmvpp2_create_page_pool(struct device *dev, int num, int len,\n\t\t       enum dma_data_direction dma_dir)\n{\n\tstruct page_pool_params pp_params = {\n\t\t \n\t\t.flags = PP_FLAG_DMA_MAP | PP_FLAG_DMA_SYNC_DEV,\n\t\t.pool_size = num,\n\t\t.nid = NUMA_NO_NODE,\n\t\t.dev = dev,\n\t\t.dma_dir = dma_dir,\n\t\t.offset = MVPP2_SKB_HEADROOM,\n\t\t.max_len = len,\n\t};\n\n\treturn page_pool_create(&pp_params);\n}\n\n \nstatic void mvpp2_thread_write(struct mvpp2 *priv, unsigned int thread,\n\t\t\t       u32 offset, u32 data)\n{\n\twritel(data, priv->swth_base[thread] + offset);\n}\n\nstatic u32 mvpp2_thread_read(struct mvpp2 *priv, unsigned int thread,\n\t\t\t     u32 offset)\n{\n\treturn readl(priv->swth_base[thread] + offset);\n}\n\nstatic void mvpp2_thread_write_relaxed(struct mvpp2 *priv, unsigned int thread,\n\t\t\t\t       u32 offset, u32 data)\n{\n\twritel_relaxed(data, priv->swth_base[thread] + offset);\n}\n\nstatic u32 mvpp2_thread_read_relaxed(struct mvpp2 *priv, unsigned int thread,\n\t\t\t\t     u32 offset)\n{\n\treturn readl_relaxed(priv->swth_base[thread] + offset);\n}\n\nstatic dma_addr_t mvpp2_txdesc_dma_addr_get(struct mvpp2_port *port,\n\t\t\t\t\t    struct mvpp2_tx_desc *tx_desc)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\treturn le32_to_cpu(tx_desc->pp21.buf_dma_addr);\n\telse\n\t\treturn le64_to_cpu(tx_desc->pp22.buf_dma_addr_ptp) &\n\t\t       MVPP2_DESC_DMA_MASK;\n}\n\nstatic void mvpp2_txdesc_dma_addr_set(struct mvpp2_port *port,\n\t\t\t\t      struct mvpp2_tx_desc *tx_desc,\n\t\t\t\t      dma_addr_t dma_addr)\n{\n\tdma_addr_t addr, offset;\n\n\taddr = dma_addr & ~MVPP2_TX_DESC_ALIGN;\n\toffset = dma_addr & MVPP2_TX_DESC_ALIGN;\n\n\tif (port->priv->hw_version == MVPP21) {\n\t\ttx_desc->pp21.buf_dma_addr = cpu_to_le32(addr);\n\t\ttx_desc->pp21.packet_offset = offset;\n\t} else {\n\t\t__le64 val = cpu_to_le64(addr);\n\n\t\ttx_desc->pp22.buf_dma_addr_ptp &= ~cpu_to_le64(MVPP2_DESC_DMA_MASK);\n\t\ttx_desc->pp22.buf_dma_addr_ptp |= val;\n\t\ttx_desc->pp22.packet_offset = offset;\n\t}\n}\n\nstatic size_t mvpp2_txdesc_size_get(struct mvpp2_port *port,\n\t\t\t\t    struct mvpp2_tx_desc *tx_desc)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\treturn le16_to_cpu(tx_desc->pp21.data_size);\n\telse\n\t\treturn le16_to_cpu(tx_desc->pp22.data_size);\n}\n\nstatic void mvpp2_txdesc_size_set(struct mvpp2_port *port,\n\t\t\t\t  struct mvpp2_tx_desc *tx_desc,\n\t\t\t\t  size_t size)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\ttx_desc->pp21.data_size = cpu_to_le16(size);\n\telse\n\t\ttx_desc->pp22.data_size = cpu_to_le16(size);\n}\n\nstatic void mvpp2_txdesc_txq_set(struct mvpp2_port *port,\n\t\t\t\t struct mvpp2_tx_desc *tx_desc,\n\t\t\t\t unsigned int txq)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\ttx_desc->pp21.phys_txq = txq;\n\telse\n\t\ttx_desc->pp22.phys_txq = txq;\n}\n\nstatic void mvpp2_txdesc_cmd_set(struct mvpp2_port *port,\n\t\t\t\t struct mvpp2_tx_desc *tx_desc,\n\t\t\t\t unsigned int command)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\ttx_desc->pp21.command = cpu_to_le32(command);\n\telse\n\t\ttx_desc->pp22.command = cpu_to_le32(command);\n}\n\nstatic unsigned int mvpp2_txdesc_offset_get(struct mvpp2_port *port,\n\t\t\t\t\t    struct mvpp2_tx_desc *tx_desc)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\treturn tx_desc->pp21.packet_offset;\n\telse\n\t\treturn tx_desc->pp22.packet_offset;\n}\n\nstatic dma_addr_t mvpp2_rxdesc_dma_addr_get(struct mvpp2_port *port,\n\t\t\t\t\t    struct mvpp2_rx_desc *rx_desc)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\treturn le32_to_cpu(rx_desc->pp21.buf_dma_addr);\n\telse\n\t\treturn le64_to_cpu(rx_desc->pp22.buf_dma_addr_key_hash) &\n\t\t       MVPP2_DESC_DMA_MASK;\n}\n\nstatic unsigned long mvpp2_rxdesc_cookie_get(struct mvpp2_port *port,\n\t\t\t\t\t     struct mvpp2_rx_desc *rx_desc)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\treturn le32_to_cpu(rx_desc->pp21.buf_cookie);\n\telse\n\t\treturn le64_to_cpu(rx_desc->pp22.buf_cookie_misc) &\n\t\t       MVPP2_DESC_DMA_MASK;\n}\n\nstatic size_t mvpp2_rxdesc_size_get(struct mvpp2_port *port,\n\t\t\t\t    struct mvpp2_rx_desc *rx_desc)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\treturn le16_to_cpu(rx_desc->pp21.data_size);\n\telse\n\t\treturn le16_to_cpu(rx_desc->pp22.data_size);\n}\n\nstatic u32 mvpp2_rxdesc_status_get(struct mvpp2_port *port,\n\t\t\t\t   struct mvpp2_rx_desc *rx_desc)\n{\n\tif (port->priv->hw_version == MVPP21)\n\t\treturn le32_to_cpu(rx_desc->pp21.status);\n\telse\n\t\treturn le32_to_cpu(rx_desc->pp22.status);\n}\n\nstatic void mvpp2_txq_inc_get(struct mvpp2_txq_pcpu *txq_pcpu)\n{\n\ttxq_pcpu->txq_get_index++;\n\tif (txq_pcpu->txq_get_index == txq_pcpu->size)\n\t\ttxq_pcpu->txq_get_index = 0;\n}\n\nstatic void mvpp2_txq_inc_put(struct mvpp2_port *port,\n\t\t\t      struct mvpp2_txq_pcpu *txq_pcpu,\n\t\t\t      void *data,\n\t\t\t      struct mvpp2_tx_desc *tx_desc,\n\t\t\t      enum mvpp2_tx_buf_type buf_type)\n{\n\tstruct mvpp2_txq_pcpu_buf *tx_buf =\n\t\ttxq_pcpu->buffs + txq_pcpu->txq_put_index;\n\ttx_buf->type = buf_type;\n\tif (buf_type == MVPP2_TYPE_SKB)\n\t\ttx_buf->skb = data;\n\telse\n\t\ttx_buf->xdpf = data;\n\ttx_buf->size = mvpp2_txdesc_size_get(port, tx_desc);\n\ttx_buf->dma = mvpp2_txdesc_dma_addr_get(port, tx_desc) +\n\t\tmvpp2_txdesc_offset_get(port, tx_desc);\n\ttxq_pcpu->txq_put_index++;\n\tif (txq_pcpu->txq_put_index == txq_pcpu->size)\n\t\ttxq_pcpu->txq_put_index = 0;\n}\n\n \nstatic int mvpp2_get_nrxqs(struct mvpp2 *priv)\n{\n\tunsigned int nrxqs;\n\n\tif (priv->hw_version >= MVPP22 && queue_mode == MVPP2_QDIST_SINGLE_MODE)\n\t\treturn 1;\n\n\t \n\tnrxqs = (num_possible_cpus() + 3) & ~0x3;\n\tif (nrxqs > MVPP2_PORT_MAX_RXQ)\n\t\tnrxqs = MVPP2_PORT_MAX_RXQ;\n\n\treturn nrxqs;\n}\n\n \nstatic inline int mvpp2_egress_port(struct mvpp2_port *port)\n{\n\treturn MVPP2_MAX_TCONT + port->id;\n}\n\n \nstatic inline int mvpp2_txq_phys(int port, int txq)\n{\n\treturn (MVPP2_MAX_TCONT + port) * MVPP2_MAX_TXQ + txq;\n}\n\n \nstatic void *mvpp2_frag_alloc(const struct mvpp2_bm_pool *pool,\n\t\t\t      struct page_pool *page_pool)\n{\n\tif (page_pool)\n\t\treturn page_pool_dev_alloc_pages(page_pool);\n\n\tif (likely(pool->frag_size <= PAGE_SIZE))\n\t\treturn netdev_alloc_frag(pool->frag_size);\n\n\treturn kmalloc(pool->frag_size, GFP_ATOMIC);\n}\n\nstatic void mvpp2_frag_free(const struct mvpp2_bm_pool *pool,\n\t\t\t    struct page_pool *page_pool, void *data)\n{\n\tif (page_pool)\n\t\tpage_pool_put_full_page(page_pool, virt_to_head_page(data), false);\n\telse if (likely(pool->frag_size <= PAGE_SIZE))\n\t\tskb_free_frag(data);\n\telse\n\t\tkfree(data);\n}\n\n \n\n \nstatic int mvpp2_bm_pool_create(struct device *dev, struct mvpp2 *priv,\n\t\t\t\tstruct mvpp2_bm_pool *bm_pool, int size)\n{\n\tu32 val;\n\n\t \n\tif (!IS_ALIGNED(size, 16))\n\t\treturn -EINVAL;\n\n\t \n\tif (priv->hw_version == MVPP21)\n\t\tbm_pool->size_bytes = 2 * sizeof(u32) * size;\n\telse\n\t\tbm_pool->size_bytes = 2 * sizeof(u64) * size;\n\n\tbm_pool->virt_addr = dma_alloc_coherent(dev, bm_pool->size_bytes,\n\t\t\t\t\t\t&bm_pool->dma_addr,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!bm_pool->virt_addr)\n\t\treturn -ENOMEM;\n\n\tif (!IS_ALIGNED((unsigned long)bm_pool->virt_addr,\n\t\t\tMVPP2_BM_POOL_PTR_ALIGN)) {\n\t\tdma_free_coherent(dev, bm_pool->size_bytes,\n\t\t\t\t  bm_pool->virt_addr, bm_pool->dma_addr);\n\t\tdev_err(dev, \"BM pool %d is not %d bytes aligned\\n\",\n\t\t\tbm_pool->id, MVPP2_BM_POOL_PTR_ALIGN);\n\t\treturn -ENOMEM;\n\t}\n\n\tmvpp2_write(priv, MVPP2_BM_POOL_BASE_REG(bm_pool->id),\n\t\t    lower_32_bits(bm_pool->dma_addr));\n\tmvpp2_write(priv, MVPP2_BM_POOL_SIZE_REG(bm_pool->id), size);\n\n\tval = mvpp2_read(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id));\n\tval |= MVPP2_BM_START_MASK;\n\n\tval &= ~MVPP2_BM_LOW_THRESH_MASK;\n\tval &= ~MVPP2_BM_HIGH_THRESH_MASK;\n\n\t \n\tif (priv->hw_version == MVPP23) {\n\t\tval |= MVPP2_BM_LOW_THRESH_VALUE(MVPP23_BM_BPPI_LOW_THRESH);\n\t\tval |= MVPP2_BM_HIGH_THRESH_VALUE(MVPP23_BM_BPPI_HIGH_THRESH);\n\t} else {\n\t\tval |= MVPP2_BM_LOW_THRESH_VALUE(MVPP2_BM_BPPI_LOW_THRESH);\n\t\tval |= MVPP2_BM_HIGH_THRESH_VALUE(MVPP2_BM_BPPI_HIGH_THRESH);\n\t}\n\n\tmvpp2_write(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id), val);\n\n\tbm_pool->size = size;\n\tbm_pool->pkt_size = 0;\n\tbm_pool->buf_num = 0;\n\n\treturn 0;\n}\n\n \nstatic void mvpp2_bm_pool_bufsize_set(struct mvpp2 *priv,\n\t\t\t\t      struct mvpp2_bm_pool *bm_pool,\n\t\t\t\t      int buf_size)\n{\n\tu32 val;\n\n\tbm_pool->buf_size = buf_size;\n\n\tval = ALIGN(buf_size, 1 << MVPP2_POOL_BUF_SIZE_OFFSET);\n\tmvpp2_write(priv, MVPP2_POOL_BUF_SIZE_REG(bm_pool->id), val);\n}\n\nstatic void mvpp2_bm_bufs_get_addrs(struct device *dev, struct mvpp2 *priv,\n\t\t\t\t    struct mvpp2_bm_pool *bm_pool,\n\t\t\t\t    dma_addr_t *dma_addr,\n\t\t\t\t    phys_addr_t *phys_addr)\n{\n\tunsigned int thread = mvpp2_cpu_to_thread(priv, get_cpu());\n\n\t*dma_addr = mvpp2_thread_read(priv, thread,\n\t\t\t\t      MVPP2_BM_PHY_ALLOC_REG(bm_pool->id));\n\t*phys_addr = mvpp2_thread_read(priv, thread, MVPP2_BM_VIRT_ALLOC_REG);\n\n\tif (priv->hw_version >= MVPP22) {\n\t\tu32 val;\n\t\tu32 dma_addr_highbits, phys_addr_highbits;\n\n\t\tval = mvpp2_thread_read(priv, thread, MVPP22_BM_ADDR_HIGH_ALLOC);\n\t\tdma_addr_highbits = (val & MVPP22_BM_ADDR_HIGH_PHYS_MASK);\n\t\tphys_addr_highbits = (val & MVPP22_BM_ADDR_HIGH_VIRT_MASK) >>\n\t\t\tMVPP22_BM_ADDR_HIGH_VIRT_SHIFT;\n\n\t\tif (sizeof(dma_addr_t) == 8)\n\t\t\t*dma_addr |= (u64)dma_addr_highbits << 32;\n\n\t\tif (sizeof(phys_addr_t) == 8)\n\t\t\t*phys_addr |= (u64)phys_addr_highbits << 32;\n\t}\n\n\tput_cpu();\n}\n\n \nstatic void mvpp2_bm_bufs_free(struct device *dev, struct mvpp2 *priv,\n\t\t\t       struct mvpp2_bm_pool *bm_pool, int buf_num)\n{\n\tstruct page_pool *pp = NULL;\n\tint i;\n\n\tif (buf_num > bm_pool->buf_num) {\n\t\tWARN(1, \"Pool does not have so many bufs pool(%d) bufs(%d)\\n\",\n\t\t     bm_pool->id, buf_num);\n\t\tbuf_num = bm_pool->buf_num;\n\t}\n\n\tif (priv->percpu_pools)\n\t\tpp = priv->page_pool[bm_pool->id];\n\n\tfor (i = 0; i < buf_num; i++) {\n\t\tdma_addr_t buf_dma_addr;\n\t\tphys_addr_t buf_phys_addr;\n\t\tvoid *data;\n\n\t\tmvpp2_bm_bufs_get_addrs(dev, priv, bm_pool,\n\t\t\t\t\t&buf_dma_addr, &buf_phys_addr);\n\n\t\tif (!pp)\n\t\t\tdma_unmap_single(dev, buf_dma_addr,\n\t\t\t\t\t bm_pool->buf_size, DMA_FROM_DEVICE);\n\n\t\tdata = (void *)phys_to_virt(buf_phys_addr);\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\tmvpp2_frag_free(bm_pool, pp, data);\n\t}\n\n\t \n\tbm_pool->buf_num -= i;\n}\n\n \nstatic int mvpp2_check_hw_buf_num(struct mvpp2 *priv, struct mvpp2_bm_pool *bm_pool)\n{\n\tint buf_num = 0;\n\n\tbuf_num += mvpp2_read(priv, MVPP2_BM_POOL_PTRS_NUM_REG(bm_pool->id)) &\n\t\t\t\t    MVPP22_BM_POOL_PTRS_NUM_MASK;\n\tbuf_num += mvpp2_read(priv, MVPP2_BM_BPPI_PTRS_NUM_REG(bm_pool->id)) &\n\t\t\t\t    MVPP2_BM_BPPI_PTR_NUM_MASK;\n\n\t \n\tif (buf_num)\n\t\tbuf_num += 1;\n\n\treturn buf_num;\n}\n\n \nstatic int mvpp2_bm_pool_destroy(struct device *dev, struct mvpp2 *priv,\n\t\t\t\t struct mvpp2_bm_pool *bm_pool)\n{\n\tint buf_num;\n\tu32 val;\n\n\tbuf_num = mvpp2_check_hw_buf_num(priv, bm_pool);\n\tmvpp2_bm_bufs_free(dev, priv, bm_pool, buf_num);\n\n\t \n\tbuf_num = mvpp2_check_hw_buf_num(priv, bm_pool);\n\tif (buf_num) {\n\t\tWARN(1, \"cannot free all buffers in pool %d, buf_num left %d\\n\",\n\t\t     bm_pool->id, bm_pool->buf_num);\n\t\treturn 0;\n\t}\n\n\tval = mvpp2_read(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id));\n\tval |= MVPP2_BM_STOP_MASK;\n\tmvpp2_write(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id), val);\n\n\tif (priv->percpu_pools) {\n\t\tpage_pool_destroy(priv->page_pool[bm_pool->id]);\n\t\tpriv->page_pool[bm_pool->id] = NULL;\n\t}\n\n\tdma_free_coherent(dev, bm_pool->size_bytes,\n\t\t\t  bm_pool->virt_addr,\n\t\t\t  bm_pool->dma_addr);\n\treturn 0;\n}\n\nstatic int mvpp2_bm_pools_init(struct device *dev, struct mvpp2 *priv)\n{\n\tint i, err, size, poolnum = MVPP2_BM_POOLS_NUM;\n\tstruct mvpp2_bm_pool *bm_pool;\n\n\tif (priv->percpu_pools)\n\t\tpoolnum = mvpp2_get_nrxqs(priv) * 2;\n\n\t \n\tsize = MVPP2_BM_POOL_SIZE_MAX;\n\tfor (i = 0; i < poolnum; i++) {\n\t\tbm_pool = &priv->bm_pools[i];\n\t\tbm_pool->id = i;\n\t\terr = mvpp2_bm_pool_create(dev, priv, bm_pool, size);\n\t\tif (err)\n\t\t\tgoto err_unroll_pools;\n\t\tmvpp2_bm_pool_bufsize_set(priv, bm_pool, 0);\n\t}\n\treturn 0;\n\nerr_unroll_pools:\n\tdev_err(dev, \"failed to create BM pool %d, size %d\\n\", i, size);\n\tfor (i = i - 1; i >= 0; i--)\n\t\tmvpp2_bm_pool_destroy(dev, priv, &priv->bm_pools[i]);\n\treturn err;\n}\n\n \nstatic void mvpp23_bm_set_8pool_mode(struct mvpp2 *priv)\n{\n\tint val;\n\n\tval = mvpp2_read(priv, MVPP22_BM_POOL_BASE_ADDR_HIGH_REG);\n\tval |= MVPP23_BM_8POOL_MODE;\n\tmvpp2_write(priv, MVPP22_BM_POOL_BASE_ADDR_HIGH_REG, val);\n}\n\nstatic int mvpp2_bm_init(struct device *dev, struct mvpp2 *priv)\n{\n\tenum dma_data_direction dma_dir = DMA_FROM_DEVICE;\n\tint i, err, poolnum = MVPP2_BM_POOLS_NUM;\n\tstruct mvpp2_port *port;\n\n\tif (priv->percpu_pools) {\n\t\tfor (i = 0; i < priv->port_count; i++) {\n\t\t\tport = priv->port_list[i];\n\t\t\tif (port->xdp_prog) {\n\t\t\t\tdma_dir = DMA_BIDIRECTIONAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tpoolnum = mvpp2_get_nrxqs(priv) * 2;\n\t\tfor (i = 0; i < poolnum; i++) {\n\t\t\t \n\t\t\tint pn = i / (poolnum / 2);\n\n\t\t\tpriv->page_pool[i] =\n\t\t\t\tmvpp2_create_page_pool(dev,\n\t\t\t\t\t\t       mvpp2_pools[pn].buf_num,\n\t\t\t\t\t\t       mvpp2_pools[pn].pkt_size,\n\t\t\t\t\t\t       dma_dir);\n\t\t\tif (IS_ERR(priv->page_pool[i])) {\n\t\t\t\tint j;\n\n\t\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\t\tpage_pool_destroy(priv->page_pool[j]);\n\t\t\t\t\tpriv->page_pool[j] = NULL;\n\t\t\t\t}\n\t\t\t\treturn PTR_ERR(priv->page_pool[i]);\n\t\t\t}\n\t\t}\n\t}\n\n\tdev_info(dev, \"using %d %s buffers\\n\", poolnum,\n\t\t priv->percpu_pools ? \"per-cpu\" : \"shared\");\n\n\tfor (i = 0; i < poolnum; i++) {\n\t\t \n\t\tmvpp2_write(priv, MVPP2_BM_INTR_MASK_REG(i), 0);\n\t\t \n\t\tmvpp2_write(priv, MVPP2_BM_INTR_CAUSE_REG(i), 0);\n\t}\n\n\t \n\tpriv->bm_pools = devm_kcalloc(dev, poolnum,\n\t\t\t\t      sizeof(*priv->bm_pools), GFP_KERNEL);\n\tif (!priv->bm_pools)\n\t\treturn -ENOMEM;\n\n\tif (priv->hw_version == MVPP23)\n\t\tmvpp23_bm_set_8pool_mode(priv);\n\n\terr = mvpp2_bm_pools_init(dev, priv);\n\tif (err < 0)\n\t\treturn err;\n\treturn 0;\n}\n\nstatic void mvpp2_setup_bm_pool(void)\n{\n\t \n\tmvpp2_pools[MVPP2_BM_SHORT].buf_num  = MVPP2_BM_SHORT_BUF_NUM;\n\tmvpp2_pools[MVPP2_BM_SHORT].pkt_size = MVPP2_BM_SHORT_PKT_SIZE;\n\n\t \n\tmvpp2_pools[MVPP2_BM_LONG].buf_num  = MVPP2_BM_LONG_BUF_NUM;\n\tmvpp2_pools[MVPP2_BM_LONG].pkt_size = MVPP2_BM_LONG_PKT_SIZE;\n\n\t \n\tmvpp2_pools[MVPP2_BM_JUMBO].buf_num  = MVPP2_BM_JUMBO_BUF_NUM;\n\tmvpp2_pools[MVPP2_BM_JUMBO].pkt_size = MVPP2_BM_JUMBO_PKT_SIZE;\n}\n\n \nstatic void mvpp2_rxq_long_pool_set(struct mvpp2_port *port,\n\t\t\t\t    int lrxq, int long_pool)\n{\n\tu32 val, mask;\n\tint prxq;\n\n\t \n\tprxq = port->rxqs[lrxq]->id;\n\n\tif (port->priv->hw_version == MVPP21)\n\t\tmask = MVPP21_RXQ_POOL_LONG_MASK;\n\telse\n\t\tmask = MVPP22_RXQ_POOL_LONG_MASK;\n\n\tval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(prxq));\n\tval &= ~mask;\n\tval |= (long_pool << MVPP2_RXQ_POOL_LONG_OFFS) & mask;\n\tmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(prxq), val);\n}\n\n \nstatic void mvpp2_rxq_short_pool_set(struct mvpp2_port *port,\n\t\t\t\t     int lrxq, int short_pool)\n{\n\tu32 val, mask;\n\tint prxq;\n\n\t \n\tprxq = port->rxqs[lrxq]->id;\n\n\tif (port->priv->hw_version == MVPP21)\n\t\tmask = MVPP21_RXQ_POOL_SHORT_MASK;\n\telse\n\t\tmask = MVPP22_RXQ_POOL_SHORT_MASK;\n\n\tval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(prxq));\n\tval &= ~mask;\n\tval |= (short_pool << MVPP2_RXQ_POOL_SHORT_OFFS) & mask;\n\tmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(prxq), val);\n}\n\nstatic void *mvpp2_buf_alloc(struct mvpp2_port *port,\n\t\t\t     struct mvpp2_bm_pool *bm_pool,\n\t\t\t     struct page_pool *page_pool,\n\t\t\t     dma_addr_t *buf_dma_addr,\n\t\t\t     phys_addr_t *buf_phys_addr,\n\t\t\t     gfp_t gfp_mask)\n{\n\tdma_addr_t dma_addr;\n\tstruct page *page;\n\tvoid *data;\n\n\tdata = mvpp2_frag_alloc(bm_pool, page_pool);\n\tif (!data)\n\t\treturn NULL;\n\n\tif (page_pool) {\n\t\tpage = (struct page *)data;\n\t\tdma_addr = page_pool_get_dma_addr(page);\n\t\tdata = page_to_virt(page);\n\t} else {\n\t\tdma_addr = dma_map_single(port->dev->dev.parent, data,\n\t\t\t\t\t  MVPP2_RX_BUF_SIZE(bm_pool->pkt_size),\n\t\t\t\t\t  DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(port->dev->dev.parent, dma_addr))) {\n\t\t\tmvpp2_frag_free(bm_pool, NULL, data);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\t*buf_dma_addr = dma_addr;\n\t*buf_phys_addr = virt_to_phys(data);\n\n\treturn data;\n}\n\n \nstatic void mvpp2_rxq_enable_fc(struct mvpp2_port *port)\n{\n\tint val, cm3_state, host_id, q;\n\tint fq = port->first_rxq;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&port->priv->mss_spinlock, flags);\n\n\t \n\tval = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);\n\tcm3_state = (val & FLOW_CONTROL_ENABLE_BIT);\n\tval &= ~FLOW_CONTROL_ENABLE_BIT;\n\tmvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);\n\n\t \n\tfor (q = 0; q < port->nrxqs; q++) {\n\t\t \n\t\tval = MSS_THRESHOLD_START;\n\t\tval |= (MSS_THRESHOLD_STOP << MSS_RXQ_TRESH_STOP_OFFS);\n\t\tmvpp2_cm3_write(port->priv, MSS_RXQ_TRESH_REG(q, fq), val);\n\n\t\tval = mvpp2_cm3_read(port->priv, MSS_RXQ_ASS_REG(q, fq));\n\t\t \n\t\tval &= ~(MSS_RXQ_ASS_PORTID_MASK << MSS_RXQ_ASS_Q_BASE(q, fq));\n\t\tval |= (port->id << MSS_RXQ_ASS_Q_BASE(q, fq));\n\t\tval &= ~(MSS_RXQ_ASS_HOSTID_MASK << (MSS_RXQ_ASS_Q_BASE(q, fq)\n\t\t\t+ MSS_RXQ_ASS_HOSTID_OFFS));\n\n\t\t \n\t\tif (queue_mode == MVPP2_QDIST_SINGLE_MODE)\n\t\t\thost_id = port->nqvecs;\n\t\telse if (queue_mode == MVPP2_QDIST_MULTI_MODE)\n\t\t\thost_id = q;\n\t\telse\n\t\t\thost_id = 0;\n\n\t\t \n\t\tval |= (host_id << (MSS_RXQ_ASS_Q_BASE(q, fq)\n\t\t\t+ MSS_RXQ_ASS_HOSTID_OFFS));\n\n\t\tmvpp2_cm3_write(port->priv, MSS_RXQ_ASS_REG(q, fq), val);\n\t}\n\n\t \n\tval = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);\n\tval |= FLOW_CONTROL_UPDATE_COMMAND_BIT;\n\tval |= cm3_state;\n\tmvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);\n\n\tspin_unlock_irqrestore(&port->priv->mss_spinlock, flags);\n}\n\n \nstatic void mvpp2_rxq_disable_fc(struct mvpp2_port *port)\n{\n\tint val, cm3_state, q;\n\tunsigned long flags;\n\tint fq = port->first_rxq;\n\n\tspin_lock_irqsave(&port->priv->mss_spinlock, flags);\n\n\t \n\tval = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);\n\tcm3_state = (val & FLOW_CONTROL_ENABLE_BIT);\n\tval &= ~FLOW_CONTROL_ENABLE_BIT;\n\tmvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);\n\n\t \n\tfor (q = 0; q < port->nrxqs; q++) {\n\t\t \n\t\tval = 0;\n\t\tval |= (0 << MSS_RXQ_TRESH_STOP_OFFS);\n\t\tmvpp2_cm3_write(port->priv, MSS_RXQ_TRESH_REG(q, fq), val);\n\n\t\tval = mvpp2_cm3_read(port->priv, MSS_RXQ_ASS_REG(q, fq));\n\n\t\tval &= ~(MSS_RXQ_ASS_PORTID_MASK << MSS_RXQ_ASS_Q_BASE(q, fq));\n\n\t\tval &= ~(MSS_RXQ_ASS_HOSTID_MASK << (MSS_RXQ_ASS_Q_BASE(q, fq)\n\t\t\t+ MSS_RXQ_ASS_HOSTID_OFFS));\n\n\t\tmvpp2_cm3_write(port->priv, MSS_RXQ_ASS_REG(q, fq), val);\n\t}\n\n\t \n\tval = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);\n\tval |= FLOW_CONTROL_UPDATE_COMMAND_BIT;\n\tval |= cm3_state;\n\tmvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);\n\n\tspin_unlock_irqrestore(&port->priv->mss_spinlock, flags);\n}\n\n \nstatic void mvpp2_bm_pool_update_fc(struct mvpp2_port *port,\n\t\t\t\t    struct mvpp2_bm_pool *pool,\n\t\t\t\t    bool en)\n{\n\tint val, cm3_state;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&port->priv->mss_spinlock, flags);\n\n\t \n\tval = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);\n\tcm3_state = (val & FLOW_CONTROL_ENABLE_BIT);\n\tval &= ~FLOW_CONTROL_ENABLE_BIT;\n\tmvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);\n\n\t \n\tif (en) {\n\t\t \n\t\tval = mvpp2_cm3_read(port->priv, MSS_BUF_POOL_REG(pool->id));\n\t\tval |= MSS_BUF_POOL_PORT_OFFS(port->id);\n\t\tval &= ~MSS_BUF_POOL_START_MASK;\n\t\tval |= (MSS_THRESHOLD_START << MSS_BUF_POOL_START_OFFS);\n\t\tval &= ~MSS_BUF_POOL_STOP_MASK;\n\t\tval |= MSS_THRESHOLD_STOP;\n\t\tmvpp2_cm3_write(port->priv, MSS_BUF_POOL_REG(pool->id), val);\n\t} else {\n\t\t \n\t\tval = mvpp2_cm3_read(port->priv, MSS_BUF_POOL_REG(pool->id));\n\t\tval &= ~MSS_BUF_POOL_PORT_OFFS(port->id);\n\n\t\t \n\t\tif (!pool->buf_num) {\n\t\t\tval &= ~MSS_BUF_POOL_START_MASK;\n\t\t\tval &= ~MSS_BUF_POOL_STOP_MASK;\n\t\t}\n\n\t\tmvpp2_cm3_write(port->priv, MSS_BUF_POOL_REG(pool->id), val);\n\t}\n\n\t \n\tval = mvpp2_cm3_read(port->priv, MSS_FC_COM_REG);\n\tval |= FLOW_CONTROL_UPDATE_COMMAND_BIT;\n\tval |= cm3_state;\n\tmvpp2_cm3_write(port->priv, MSS_FC_COM_REG, val);\n\n\tspin_unlock_irqrestore(&port->priv->mss_spinlock, flags);\n}\n\n \nstatic void mvpp2_bm_pool_update_priv_fc(struct mvpp2 *priv, bool en)\n{\n\tstruct mvpp2_port *port;\n\tint i;\n\n\tfor (i = 0; i < priv->port_count; i++) {\n\t\tport = priv->port_list[i];\n\t\tif (port->priv->percpu_pools) {\n\t\t\tfor (i = 0; i < port->nrxqs; i++)\n\t\t\t\tmvpp2_bm_pool_update_fc(port, &port->priv->bm_pools[i],\n\t\t\t\t\t\t\tport->tx_fc & en);\n\t\t} else {\n\t\t\tmvpp2_bm_pool_update_fc(port, port->pool_long, port->tx_fc & en);\n\t\t\tmvpp2_bm_pool_update_fc(port, port->pool_short, port->tx_fc & en);\n\t\t}\n\t}\n}\n\nstatic int mvpp2_enable_global_fc(struct mvpp2 *priv)\n{\n\tint val, timeout = 0;\n\n\t \n\tval = mvpp2_cm3_read(priv, MSS_FC_COM_REG);\n\tval |= FLOW_CONTROL_ENABLE_BIT;\n\tmvpp2_cm3_write(priv, MSS_FC_COM_REG, val);\n\n\t \n\tval |= FLOW_CONTROL_UPDATE_COMMAND_BIT;\n\tmvpp2_cm3_write(priv, MSS_FC_COM_REG, val);\n\n\twhile (timeout < MSS_FC_MAX_TIMEOUT) {\n\t\tval = mvpp2_cm3_read(priv, MSS_FC_COM_REG);\n\n\t\tif (!(val & FLOW_CONTROL_UPDATE_COMMAND_BIT))\n\t\t\treturn 0;\n\t\tusleep_range(10, 20);\n\t\ttimeout++;\n\t}\n\n\tpriv->global_tx_fc = false;\n\treturn -EOPNOTSUPP;\n}\n\n \nstatic inline void mvpp2_bm_pool_put(struct mvpp2_port *port, int pool,\n\t\t\t\t     dma_addr_t buf_dma_addr,\n\t\t\t\t     phys_addr_t buf_phys_addr)\n{\n\tunsigned int thread = mvpp2_cpu_to_thread(port->priv, get_cpu());\n\tunsigned long flags = 0;\n\n\tif (test_bit(thread, &port->priv->lock_map))\n\t\tspin_lock_irqsave(&port->bm_lock[thread], flags);\n\n\tif (port->priv->hw_version >= MVPP22) {\n\t\tu32 val = 0;\n\n\t\tif (sizeof(dma_addr_t) == 8)\n\t\t\tval |= upper_32_bits(buf_dma_addr) &\n\t\t\t\tMVPP22_BM_ADDR_HIGH_PHYS_RLS_MASK;\n\n\t\tif (sizeof(phys_addr_t) == 8)\n\t\t\tval |= (upper_32_bits(buf_phys_addr)\n\t\t\t\t<< MVPP22_BM_ADDR_HIGH_VIRT_RLS_SHIFT) &\n\t\t\t\tMVPP22_BM_ADDR_HIGH_VIRT_RLS_MASK;\n\n\t\tmvpp2_thread_write_relaxed(port->priv, thread,\n\t\t\t\t\t   MVPP22_BM_ADDR_HIGH_RLS_REG, val);\n\t}\n\n\t \n\tmvpp2_thread_write_relaxed(port->priv, thread,\n\t\t\t\t   MVPP2_BM_VIRT_RLS_REG, buf_phys_addr);\n\tmvpp2_thread_write_relaxed(port->priv, thread,\n\t\t\t\t   MVPP2_BM_PHY_RLS_REG(pool), buf_dma_addr);\n\n\tif (test_bit(thread, &port->priv->lock_map))\n\t\tspin_unlock_irqrestore(&port->bm_lock[thread], flags);\n\n\tput_cpu();\n}\n\n \nstatic int mvpp2_bm_bufs_add(struct mvpp2_port *port,\n\t\t\t     struct mvpp2_bm_pool *bm_pool, int buf_num)\n{\n\tint i, buf_size, total_size;\n\tdma_addr_t dma_addr;\n\tphys_addr_t phys_addr;\n\tstruct page_pool *pp = NULL;\n\tvoid *buf;\n\n\tif (port->priv->percpu_pools &&\n\t    bm_pool->pkt_size > MVPP2_BM_LONG_PKT_SIZE) {\n\t\tnetdev_err(port->dev,\n\t\t\t   \"attempted to use jumbo frames with per-cpu pools\");\n\t\treturn 0;\n\t}\n\n\tbuf_size = MVPP2_RX_BUF_SIZE(bm_pool->pkt_size);\n\ttotal_size = MVPP2_RX_TOTAL_SIZE(buf_size);\n\n\tif (buf_num < 0 ||\n\t    (buf_num + bm_pool->buf_num > bm_pool->size)) {\n\t\tnetdev_err(port->dev,\n\t\t\t   \"cannot allocate %d buffers for pool %d\\n\",\n\t\t\t   buf_num, bm_pool->id);\n\t\treturn 0;\n\t}\n\n\tif (port->priv->percpu_pools)\n\t\tpp = port->priv->page_pool[bm_pool->id];\n\tfor (i = 0; i < buf_num; i++) {\n\t\tbuf = mvpp2_buf_alloc(port, bm_pool, pp, &dma_addr,\n\t\t\t\t      &phys_addr, GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tmvpp2_bm_pool_put(port, bm_pool->id, dma_addr,\n\t\t\t\t  phys_addr);\n\t}\n\n\t \n\tbm_pool->buf_num += i;\n\n\tnetdev_dbg(port->dev,\n\t\t   \"pool %d: pkt_size=%4d, buf_size=%4d, total_size=%4d\\n\",\n\t\t   bm_pool->id, bm_pool->pkt_size, buf_size, total_size);\n\n\tnetdev_dbg(port->dev,\n\t\t   \"pool %d: %d of %d buffers added\\n\",\n\t\t   bm_pool->id, i, buf_num);\n\treturn i;\n}\n\n \nstatic struct mvpp2_bm_pool *\nmvpp2_bm_pool_use(struct mvpp2_port *port, unsigned pool, int pkt_size)\n{\n\tstruct mvpp2_bm_pool *new_pool = &port->priv->bm_pools[pool];\n\tint num;\n\n\tif ((port->priv->percpu_pools && pool > mvpp2_get_nrxqs(port->priv) * 2) ||\n\t    (!port->priv->percpu_pools && pool >= MVPP2_BM_POOLS_NUM)) {\n\t\tnetdev_err(port->dev, \"Invalid pool %d\\n\", pool);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (new_pool->pkt_size == 0) {\n\t\tint pkts_num;\n\n\t\t \n\t\tpkts_num = new_pool->buf_num;\n\t\tif (pkts_num == 0) {\n\t\t\tif (port->priv->percpu_pools) {\n\t\t\t\tif (pool < port->nrxqs)\n\t\t\t\t\tpkts_num = mvpp2_pools[MVPP2_BM_SHORT].buf_num;\n\t\t\t\telse\n\t\t\t\t\tpkts_num = mvpp2_pools[MVPP2_BM_LONG].buf_num;\n\t\t\t} else {\n\t\t\t\tpkts_num = mvpp2_pools[pool].buf_num;\n\t\t\t}\n\t\t} else {\n\t\t\tmvpp2_bm_bufs_free(port->dev->dev.parent,\n\t\t\t\t\t   port->priv, new_pool, pkts_num);\n\t\t}\n\n\t\tnew_pool->pkt_size = pkt_size;\n\t\tnew_pool->frag_size =\n\t\t\tSKB_DATA_ALIGN(MVPP2_RX_BUF_SIZE(pkt_size)) +\n\t\t\tMVPP2_SKB_SHINFO_SIZE;\n\n\t\t \n\t\tnum = mvpp2_bm_bufs_add(port, new_pool, pkts_num);\n\t\tif (num != pkts_num) {\n\t\t\tWARN(1, \"pool %d: %d of %d allocated\\n\",\n\t\t\t     new_pool->id, num, pkts_num);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tmvpp2_bm_pool_bufsize_set(port->priv, new_pool,\n\t\t\t\t  MVPP2_RX_BUF_SIZE(new_pool->pkt_size));\n\n\treturn new_pool;\n}\n\nstatic struct mvpp2_bm_pool *\nmvpp2_bm_pool_use_percpu(struct mvpp2_port *port, int type,\n\t\t\t unsigned int pool, int pkt_size)\n{\n\tstruct mvpp2_bm_pool *new_pool = &port->priv->bm_pools[pool];\n\tint num;\n\n\tif (pool > port->nrxqs * 2) {\n\t\tnetdev_err(port->dev, \"Invalid pool %d\\n\", pool);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (new_pool->pkt_size == 0) {\n\t\tint pkts_num;\n\n\t\t \n\t\tpkts_num = new_pool->buf_num;\n\t\tif (pkts_num == 0)\n\t\t\tpkts_num = mvpp2_pools[type].buf_num;\n\t\telse\n\t\t\tmvpp2_bm_bufs_free(port->dev->dev.parent,\n\t\t\t\t\t   port->priv, new_pool, pkts_num);\n\n\t\tnew_pool->pkt_size = pkt_size;\n\t\tnew_pool->frag_size =\n\t\t\tSKB_DATA_ALIGN(MVPP2_RX_BUF_SIZE(pkt_size)) +\n\t\t\tMVPP2_SKB_SHINFO_SIZE;\n\n\t\t \n\t\tnum = mvpp2_bm_bufs_add(port, new_pool, pkts_num);\n\t\tif (num != pkts_num) {\n\t\t\tWARN(1, \"pool %d: %d of %d allocated\\n\",\n\t\t\t     new_pool->id, num, pkts_num);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tmvpp2_bm_pool_bufsize_set(port->priv, new_pool,\n\t\t\t\t  MVPP2_RX_BUF_SIZE(new_pool->pkt_size));\n\n\treturn new_pool;\n}\n\n \nstatic int mvpp2_swf_bm_pool_init_shared(struct mvpp2_port *port)\n{\n\tenum mvpp2_bm_pool_log_num long_log_pool, short_log_pool;\n\tint rxq;\n\n\t \n\tif (port->pkt_size > MVPP2_BM_LONG_PKT_SIZE) {\n\t\tlong_log_pool = MVPP2_BM_JUMBO;\n\t\tshort_log_pool = MVPP2_BM_LONG;\n\t} else {\n\t\tlong_log_pool = MVPP2_BM_LONG;\n\t\tshort_log_pool = MVPP2_BM_SHORT;\n\t}\n\n\tif (!port->pool_long) {\n\t\tport->pool_long =\n\t\t\tmvpp2_bm_pool_use(port, long_log_pool,\n\t\t\t\t\t  mvpp2_pools[long_log_pool].pkt_size);\n\t\tif (!port->pool_long)\n\t\t\treturn -ENOMEM;\n\n\t\tport->pool_long->port_map |= BIT(port->id);\n\n\t\tfor (rxq = 0; rxq < port->nrxqs; rxq++)\n\t\t\tmvpp2_rxq_long_pool_set(port, rxq, port->pool_long->id);\n\t}\n\n\tif (!port->pool_short) {\n\t\tport->pool_short =\n\t\t\tmvpp2_bm_pool_use(port, short_log_pool,\n\t\t\t\t\t  mvpp2_pools[short_log_pool].pkt_size);\n\t\tif (!port->pool_short)\n\t\t\treturn -ENOMEM;\n\n\t\tport->pool_short->port_map |= BIT(port->id);\n\n\t\tfor (rxq = 0; rxq < port->nrxqs; rxq++)\n\t\t\tmvpp2_rxq_short_pool_set(port, rxq,\n\t\t\t\t\t\t port->pool_short->id);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int mvpp2_swf_bm_pool_init_percpu(struct mvpp2_port *port)\n{\n\tstruct mvpp2_bm_pool *bm_pool;\n\tint i;\n\n\tfor (i = 0; i < port->nrxqs; i++) {\n\t\tbm_pool = mvpp2_bm_pool_use_percpu(port, MVPP2_BM_SHORT, i,\n\t\t\t\t\t\t   mvpp2_pools[MVPP2_BM_SHORT].pkt_size);\n\t\tif (!bm_pool)\n\t\t\treturn -ENOMEM;\n\n\t\tbm_pool->port_map |= BIT(port->id);\n\t\tmvpp2_rxq_short_pool_set(port, i, bm_pool->id);\n\t}\n\n\tfor (i = 0; i < port->nrxqs; i++) {\n\t\tbm_pool = mvpp2_bm_pool_use_percpu(port, MVPP2_BM_LONG, i + port->nrxqs,\n\t\t\t\t\t\t   mvpp2_pools[MVPP2_BM_LONG].pkt_size);\n\t\tif (!bm_pool)\n\t\t\treturn -ENOMEM;\n\n\t\tbm_pool->port_map |= BIT(port->id);\n\t\tmvpp2_rxq_long_pool_set(port, i, bm_pool->id);\n\t}\n\n\tport->pool_long = NULL;\n\tport->pool_short = NULL;\n\n\treturn 0;\n}\n\nstatic int mvpp2_swf_bm_pool_init(struct mvpp2_port *port)\n{\n\tif (port->priv->percpu_pools)\n\t\treturn mvpp2_swf_bm_pool_init_percpu(port);\n\telse\n\t\treturn mvpp2_swf_bm_pool_init_shared(port);\n}\n\nstatic void mvpp2_set_hw_csum(struct mvpp2_port *port,\n\t\t\t      enum mvpp2_bm_pool_log_num new_long_pool)\n{\n\tconst netdev_features_t csums = NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\n\n\t \n\tif (new_long_pool == MVPP2_BM_JUMBO && port->id != 0) {\n\t\tport->dev->features &= ~csums;\n\t\tport->dev->hw_features &= ~csums;\n\t} else {\n\t\tport->dev->features |= csums;\n\t\tport->dev->hw_features |= csums;\n\t}\n}\n\nstatic int mvpp2_bm_update_mtu(struct net_device *dev, int mtu)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tenum mvpp2_bm_pool_log_num new_long_pool;\n\tint pkt_size = MVPP2_RX_PKT_SIZE(mtu);\n\n\tif (port->priv->percpu_pools)\n\t\tgoto out_set;\n\n\t \n\tif (pkt_size > MVPP2_BM_LONG_PKT_SIZE)\n\t\tnew_long_pool = MVPP2_BM_JUMBO;\n\telse\n\t\tnew_long_pool = MVPP2_BM_LONG;\n\n\tif (new_long_pool != port->pool_long->id) {\n\t\tif (port->tx_fc) {\n\t\t\tif (pkt_size > MVPP2_BM_LONG_PKT_SIZE)\n\t\t\t\tmvpp2_bm_pool_update_fc(port,\n\t\t\t\t\t\t\tport->pool_short,\n\t\t\t\t\t\t\tfalse);\n\t\t\telse\n\t\t\t\tmvpp2_bm_pool_update_fc(port, port->pool_long,\n\t\t\t\t\t\t\tfalse);\n\t\t}\n\n\t\t \n\t\tport->pool_long = mvpp2_bm_pool_use(port, port->pool_long->id,\n\t\t\t\t\t\t    port->pool_long->pkt_size);\n\t\tport->pool_long->port_map &= ~BIT(port->id);\n\t\tport->pool_long = NULL;\n\n\t\tport->pool_short = mvpp2_bm_pool_use(port, port->pool_short->id,\n\t\t\t\t\t\t     port->pool_short->pkt_size);\n\t\tport->pool_short->port_map &= ~BIT(port->id);\n\t\tport->pool_short = NULL;\n\n\t\tport->pkt_size =  pkt_size;\n\n\t\t \n\t\tmvpp2_swf_bm_pool_init(port);\n\n\t\tmvpp2_set_hw_csum(port, new_long_pool);\n\n\t\tif (port->tx_fc) {\n\t\t\tif (pkt_size > MVPP2_BM_LONG_PKT_SIZE)\n\t\t\t\tmvpp2_bm_pool_update_fc(port, port->pool_long,\n\t\t\t\t\t\t\ttrue);\n\t\t\telse\n\t\t\t\tmvpp2_bm_pool_update_fc(port, port->pool_short,\n\t\t\t\t\t\t\ttrue);\n\t\t}\n\n\t\t \n\t\tif (new_long_pool == MVPP2_BM_JUMBO && port->id != 0) {\n\t\t\tdev->features &= ~(NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM);\n\t\t\tdev->hw_features &= ~(NETIF_F_IP_CSUM |\n\t\t\t\t\t      NETIF_F_IPV6_CSUM);\n\t\t} else {\n\t\t\tdev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\n\t\t\tdev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\n\t\t}\n\t}\n\nout_set:\n\tdev->mtu = mtu;\n\tdev->wanted_features = dev->features;\n\n\tnetdev_update_features(dev);\n\treturn 0;\n}\n\nstatic inline void mvpp2_interrupts_enable(struct mvpp2_port *port)\n{\n\tint i, sw_thread_mask = 0;\n\n\tfor (i = 0; i < port->nqvecs; i++)\n\t\tsw_thread_mask |= port->qvecs[i].sw_thread_mask;\n\n\tmvpp2_write(port->priv, MVPP2_ISR_ENABLE_REG(port->id),\n\t\t    MVPP2_ISR_ENABLE_INTERRUPT(sw_thread_mask));\n}\n\nstatic inline void mvpp2_interrupts_disable(struct mvpp2_port *port)\n{\n\tint i, sw_thread_mask = 0;\n\n\tfor (i = 0; i < port->nqvecs; i++)\n\t\tsw_thread_mask |= port->qvecs[i].sw_thread_mask;\n\n\tmvpp2_write(port->priv, MVPP2_ISR_ENABLE_REG(port->id),\n\t\t    MVPP2_ISR_DISABLE_INTERRUPT(sw_thread_mask));\n}\n\nstatic inline void mvpp2_qvec_interrupt_enable(struct mvpp2_queue_vector *qvec)\n{\n\tstruct mvpp2_port *port = qvec->port;\n\n\tmvpp2_write(port->priv, MVPP2_ISR_ENABLE_REG(port->id),\n\t\t    MVPP2_ISR_ENABLE_INTERRUPT(qvec->sw_thread_mask));\n}\n\nstatic inline void mvpp2_qvec_interrupt_disable(struct mvpp2_queue_vector *qvec)\n{\n\tstruct mvpp2_port *port = qvec->port;\n\n\tmvpp2_write(port->priv, MVPP2_ISR_ENABLE_REG(port->id),\n\t\t    MVPP2_ISR_DISABLE_INTERRUPT(qvec->sw_thread_mask));\n}\n\n \nstatic void mvpp2_interrupts_mask(void *arg)\n{\n\tstruct mvpp2_port *port = arg;\n\tint cpu = smp_processor_id();\n\tu32 thread;\n\n\t \n\tif (cpu > port->priv->nthreads)\n\t\treturn;\n\n\tthread = mvpp2_cpu_to_thread(port->priv, cpu);\n\n\tmvpp2_thread_write(port->priv, thread,\n\t\t\t   MVPP2_ISR_RX_TX_MASK_REG(port->id), 0);\n\tmvpp2_thread_write(port->priv, thread,\n\t\t\t   MVPP2_ISR_RX_ERR_CAUSE_REG(port->id), 0);\n}\n\n \nstatic void mvpp2_interrupts_unmask(void *arg)\n{\n\tstruct mvpp2_port *port = arg;\n\tint cpu = smp_processor_id();\n\tu32 val, thread;\n\n\t \n\tif (cpu >= port->priv->nthreads)\n\t\treturn;\n\n\tthread = mvpp2_cpu_to_thread(port->priv, cpu);\n\n\tval = MVPP2_CAUSE_MISC_SUM_MASK |\n\t\tMVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(port->priv->hw_version);\n\tif (port->has_tx_irqs)\n\t\tval |= MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK;\n\n\tmvpp2_thread_write(port->priv, thread,\n\t\t\t   MVPP2_ISR_RX_TX_MASK_REG(port->id), val);\n\tmvpp2_thread_write(port->priv, thread,\n\t\t\t   MVPP2_ISR_RX_ERR_CAUSE_REG(port->id),\n\t\t\t   MVPP2_ISR_RX_ERR_CAUSE_NONOCC_MASK);\n}\n\nstatic void\nmvpp2_shared_interrupt_mask_unmask(struct mvpp2_port *port, bool mask)\n{\n\tu32 val;\n\tint i;\n\n\tif (port->priv->hw_version == MVPP21)\n\t\treturn;\n\n\tif (mask)\n\t\tval = 0;\n\telse\n\t\tval = MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(MVPP22);\n\n\tfor (i = 0; i < port->nqvecs; i++) {\n\t\tstruct mvpp2_queue_vector *v = port->qvecs + i;\n\n\t\tif (v->type != MVPP2_QUEUE_VECTOR_SHARED)\n\t\t\tcontinue;\n\n\t\tmvpp2_thread_write(port->priv, v->sw_thread_id,\n\t\t\t\t   MVPP2_ISR_RX_TX_MASK_REG(port->id), val);\n\t\tmvpp2_thread_write(port->priv, v->sw_thread_id,\n\t\t\t\t   MVPP2_ISR_RX_ERR_CAUSE_REG(port->id),\n\t\t\t\t   MVPP2_ISR_RX_ERR_CAUSE_NONOCC_MASK);\n\t}\n}\n\n \nstatic bool mvpp2_port_supports_xlg(struct mvpp2_port *port)\n{\n\treturn port->gop_id == 0;\n}\n\nstatic bool mvpp2_port_supports_rgmii(struct mvpp2_port *port)\n{\n\treturn !(port->priv->hw_version >= MVPP22 && port->gop_id == 0);\n}\n\n \nstatic bool mvpp2_is_xlg(phy_interface_t interface)\n{\n\treturn interface == PHY_INTERFACE_MODE_10GBASER ||\n\t       interface == PHY_INTERFACE_MODE_5GBASER ||\n\t       interface == PHY_INTERFACE_MODE_XAUI;\n}\n\nstatic void mvpp2_modify(void __iomem *ptr, u32 mask, u32 set)\n{\n\tu32 old, val;\n\n\told = val = readl(ptr);\n\tval &= ~mask;\n\tval |= set;\n\tif (old != val)\n\t\twritel(val, ptr);\n}\n\nstatic void mvpp22_gop_init_rgmii(struct mvpp2_port *port)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tu32 val;\n\n\tregmap_read(priv->sysctrl_base, GENCONF_PORT_CTRL0, &val);\n\tval |= GENCONF_PORT_CTRL0_BUS_WIDTH_SELECT;\n\tregmap_write(priv->sysctrl_base, GENCONF_PORT_CTRL0, val);\n\n\tregmap_read(priv->sysctrl_base, GENCONF_CTRL0, &val);\n\tif (port->gop_id == 2)\n\t\tval |= GENCONF_CTRL0_PORT2_RGMII;\n\telse if (port->gop_id == 3)\n\t\tval |= GENCONF_CTRL0_PORT3_RGMII_MII;\n\tregmap_write(priv->sysctrl_base, GENCONF_CTRL0, val);\n}\n\nstatic void mvpp22_gop_init_sgmii(struct mvpp2_port *port)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tu32 val;\n\n\tregmap_read(priv->sysctrl_base, GENCONF_PORT_CTRL0, &val);\n\tval |= GENCONF_PORT_CTRL0_BUS_WIDTH_SELECT |\n\t       GENCONF_PORT_CTRL0_RX_DATA_SAMPLE;\n\tregmap_write(priv->sysctrl_base, GENCONF_PORT_CTRL0, val);\n\n\tif (port->gop_id > 1) {\n\t\tregmap_read(priv->sysctrl_base, GENCONF_CTRL0, &val);\n\t\tif (port->gop_id == 2)\n\t\t\tval &= ~GENCONF_CTRL0_PORT2_RGMII;\n\t\telse if (port->gop_id == 3)\n\t\t\tval &= ~GENCONF_CTRL0_PORT3_RGMII_MII;\n\t\tregmap_write(priv->sysctrl_base, GENCONF_CTRL0, val);\n\t}\n}\n\nstatic void mvpp22_gop_init_10gkr(struct mvpp2_port *port)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tvoid __iomem *mpcs = priv->iface_base + MVPP22_MPCS_BASE(port->gop_id);\n\tvoid __iomem *xpcs = priv->iface_base + MVPP22_XPCS_BASE(port->gop_id);\n\tu32 val;\n\n\tval = readl(xpcs + MVPP22_XPCS_CFG0);\n\tval &= ~(MVPP22_XPCS_CFG0_PCS_MODE(0x3) |\n\t\t MVPP22_XPCS_CFG0_ACTIVE_LANE(0x3));\n\tval |= MVPP22_XPCS_CFG0_ACTIVE_LANE(2);\n\twritel(val, xpcs + MVPP22_XPCS_CFG0);\n\n\tval = readl(mpcs + MVPP22_MPCS_CTRL);\n\tval &= ~MVPP22_MPCS_CTRL_FWD_ERR_CONN;\n\twritel(val, mpcs + MVPP22_MPCS_CTRL);\n\n\tval = readl(mpcs + MVPP22_MPCS_CLK_RESET);\n\tval &= ~MVPP22_MPCS_CLK_RESET_DIV_RATIO(0x7);\n\tval |= MVPP22_MPCS_CLK_RESET_DIV_RATIO(1);\n\twritel(val, mpcs + MVPP22_MPCS_CLK_RESET);\n}\n\nstatic void mvpp22_gop_fca_enable_periodic(struct mvpp2_port *port, bool en)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tvoid __iomem *fca = priv->iface_base + MVPP22_FCA_BASE(port->gop_id);\n\tu32 val;\n\n\tval = readl(fca + MVPP22_FCA_CONTROL_REG);\n\tval &= ~MVPP22_FCA_ENABLE_PERIODIC;\n\tif (en)\n\t\tval |= MVPP22_FCA_ENABLE_PERIODIC;\n\twritel(val, fca + MVPP22_FCA_CONTROL_REG);\n}\n\nstatic void mvpp22_gop_fca_set_timer(struct mvpp2_port *port, u32 timer)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tvoid __iomem *fca = priv->iface_base + MVPP22_FCA_BASE(port->gop_id);\n\tu32 lsb, msb;\n\n\tlsb = timer & MVPP22_FCA_REG_MASK;\n\tmsb = timer >> MVPP22_FCA_REG_SIZE;\n\n\twritel(lsb, fca + MVPP22_PERIODIC_COUNTER_LSB_REG);\n\twritel(msb, fca + MVPP22_PERIODIC_COUNTER_MSB_REG);\n}\n\n \nstatic void mvpp22_gop_fca_set_periodic_timer(struct mvpp2_port *port)\n{\n\tu32 timer;\n\n\ttimer = (port->priv->tclk / (USEC_PER_SEC * FC_CLK_DIVIDER))\n\t\t* FC_QUANTA;\n\n\tmvpp22_gop_fca_enable_periodic(port, false);\n\n\tmvpp22_gop_fca_set_timer(port, timer);\n\n\tmvpp22_gop_fca_enable_periodic(port, true);\n}\n\nstatic int mvpp22_gop_init(struct mvpp2_port *port, phy_interface_t interface)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tu32 val;\n\n\tif (!priv->sysctrl_base)\n\t\treturn 0;\n\n\tswitch (interface) {\n\tcase PHY_INTERFACE_MODE_RGMII:\n\tcase PHY_INTERFACE_MODE_RGMII_ID:\n\tcase PHY_INTERFACE_MODE_RGMII_RXID:\n\tcase PHY_INTERFACE_MODE_RGMII_TXID:\n\t\tif (!mvpp2_port_supports_rgmii(port))\n\t\t\tgoto invalid_conf;\n\t\tmvpp22_gop_init_rgmii(port);\n\t\tbreak;\n\tcase PHY_INTERFACE_MODE_SGMII:\n\tcase PHY_INTERFACE_MODE_1000BASEX:\n\tcase PHY_INTERFACE_MODE_2500BASEX:\n\t\tmvpp22_gop_init_sgmii(port);\n\t\tbreak;\n\tcase PHY_INTERFACE_MODE_5GBASER:\n\tcase PHY_INTERFACE_MODE_10GBASER:\n\t\tif (!mvpp2_port_supports_xlg(port))\n\t\t\tgoto invalid_conf;\n\t\tmvpp22_gop_init_10gkr(port);\n\t\tbreak;\n\tdefault:\n\t\tgoto unsupported_conf;\n\t}\n\n\tregmap_read(priv->sysctrl_base, GENCONF_PORT_CTRL1, &val);\n\tval |= GENCONF_PORT_CTRL1_RESET(port->gop_id) |\n\t       GENCONF_PORT_CTRL1_EN(port->gop_id);\n\tregmap_write(priv->sysctrl_base, GENCONF_PORT_CTRL1, val);\n\n\tregmap_read(priv->sysctrl_base, GENCONF_PORT_CTRL0, &val);\n\tval |= GENCONF_PORT_CTRL0_CLK_DIV_PHASE_CLR;\n\tregmap_write(priv->sysctrl_base, GENCONF_PORT_CTRL0, val);\n\n\tregmap_read(priv->sysctrl_base, GENCONF_SOFT_RESET1, &val);\n\tval |= GENCONF_SOFT_RESET1_GOP;\n\tregmap_write(priv->sysctrl_base, GENCONF_SOFT_RESET1, val);\n\n\tmvpp22_gop_fca_set_periodic_timer(port);\n\nunsupported_conf:\n\treturn 0;\n\ninvalid_conf:\n\tnetdev_err(port->dev, \"Invalid port configuration\\n\");\n\treturn -EINVAL;\n}\n\nstatic void mvpp22_gop_unmask_irq(struct mvpp2_port *port)\n{\n\tu32 val;\n\n\tif (phy_interface_mode_is_rgmii(port->phy_interface) ||\n\t    phy_interface_mode_is_8023z(port->phy_interface) ||\n\t    port->phy_interface == PHY_INTERFACE_MODE_SGMII) {\n\t\t \n\t\tval = readl(port->base + MVPP22_GMAC_INT_SUM_MASK);\n\t\tval |= MVPP22_GMAC_INT_SUM_MASK_LINK_STAT;\n\t\twritel(val, port->base + MVPP22_GMAC_INT_SUM_MASK);\n\t}\n\n\tif (mvpp2_port_supports_xlg(port)) {\n\t\t \n\t\tval = readl(port->base + MVPP22_XLG_EXT_INT_MASK);\n\t\tif (mvpp2_is_xlg(port->phy_interface))\n\t\t\tval |= MVPP22_XLG_EXT_INT_MASK_XLG;\n\t\telse\n\t\t\tval |= MVPP22_XLG_EXT_INT_MASK_GIG;\n\t\twritel(val, port->base + MVPP22_XLG_EXT_INT_MASK);\n\t}\n}\n\nstatic void mvpp22_gop_mask_irq(struct mvpp2_port *port)\n{\n\tu32 val;\n\n\tif (mvpp2_port_supports_xlg(port)) {\n\t\tval = readl(port->base + MVPP22_XLG_EXT_INT_MASK);\n\t\tval &= ~(MVPP22_XLG_EXT_INT_MASK_XLG |\n\t\t\t MVPP22_XLG_EXT_INT_MASK_GIG);\n\t\twritel(val, port->base + MVPP22_XLG_EXT_INT_MASK);\n\t}\n\n\tif (phy_interface_mode_is_rgmii(port->phy_interface) ||\n\t    phy_interface_mode_is_8023z(port->phy_interface) ||\n\t    port->phy_interface == PHY_INTERFACE_MODE_SGMII) {\n\t\tval = readl(port->base + MVPP22_GMAC_INT_SUM_MASK);\n\t\tval &= ~MVPP22_GMAC_INT_SUM_MASK_LINK_STAT;\n\t\twritel(val, port->base + MVPP22_GMAC_INT_SUM_MASK);\n\t}\n}\n\nstatic void mvpp22_gop_setup_irq(struct mvpp2_port *port)\n{\n\tu32 val;\n\n\tmvpp2_modify(port->base + MVPP22_GMAC_INT_SUM_MASK,\n\t\t     MVPP22_GMAC_INT_SUM_MASK_PTP,\n\t\t     MVPP22_GMAC_INT_SUM_MASK_PTP);\n\n\tif (port->phylink ||\n\t    phy_interface_mode_is_rgmii(port->phy_interface) ||\n\t    phy_interface_mode_is_8023z(port->phy_interface) ||\n\t    port->phy_interface == PHY_INTERFACE_MODE_SGMII) {\n\t\tval = readl(port->base + MVPP22_GMAC_INT_MASK);\n\t\tval |= MVPP22_GMAC_INT_MASK_LINK_STAT;\n\t\twritel(val, port->base + MVPP22_GMAC_INT_MASK);\n\t}\n\n\tif (mvpp2_port_supports_xlg(port)) {\n\t\tval = readl(port->base + MVPP22_XLG_INT_MASK);\n\t\tval |= MVPP22_XLG_INT_MASK_LINK;\n\t\twritel(val, port->base + MVPP22_XLG_INT_MASK);\n\n\t\tmvpp2_modify(port->base + MVPP22_XLG_EXT_INT_MASK,\n\t\t\t     MVPP22_XLG_EXT_INT_MASK_PTP,\n\t\t\t     MVPP22_XLG_EXT_INT_MASK_PTP);\n\t}\n\n\tmvpp22_gop_unmask_irq(port);\n}\n\n \nstatic int mvpp22_comphy_init(struct mvpp2_port *port,\n\t\t\t      phy_interface_t interface)\n{\n\tint ret;\n\n\tif (!port->comphy)\n\t\treturn 0;\n\n\tret = phy_set_mode_ext(port->comphy, PHY_MODE_ETHERNET, interface);\n\tif (ret)\n\t\treturn ret;\n\n\treturn phy_power_on(port->comphy);\n}\n\nstatic void mvpp2_port_enable(struct mvpp2_port *port)\n{\n\tu32 val;\n\n\tif (mvpp2_port_supports_xlg(port) &&\n\t    mvpp2_is_xlg(port->phy_interface)) {\n\t\tval = readl(port->base + MVPP22_XLG_CTRL0_REG);\n\t\tval |= MVPP22_XLG_CTRL0_PORT_EN;\n\t\tval &= ~MVPP22_XLG_CTRL0_MIB_CNT_DIS;\n\t\twritel(val, port->base + MVPP22_XLG_CTRL0_REG);\n\t} else {\n\t\tval = readl(port->base + MVPP2_GMAC_CTRL_0_REG);\n\t\tval |= MVPP2_GMAC_PORT_EN_MASK;\n\t\tval |= MVPP2_GMAC_MIB_CNTR_EN_MASK;\n\t\twritel(val, port->base + MVPP2_GMAC_CTRL_0_REG);\n\t}\n}\n\nstatic void mvpp2_port_disable(struct mvpp2_port *port)\n{\n\tu32 val;\n\n\tif (mvpp2_port_supports_xlg(port) &&\n\t    mvpp2_is_xlg(port->phy_interface)) {\n\t\tval = readl(port->base + MVPP22_XLG_CTRL0_REG);\n\t\tval &= ~MVPP22_XLG_CTRL0_PORT_EN;\n\t\twritel(val, port->base + MVPP22_XLG_CTRL0_REG);\n\t}\n\n\tval = readl(port->base + MVPP2_GMAC_CTRL_0_REG);\n\tval &= ~(MVPP2_GMAC_PORT_EN_MASK);\n\twritel(val, port->base + MVPP2_GMAC_CTRL_0_REG);\n}\n\n \nstatic void mvpp2_port_periodic_xon_disable(struct mvpp2_port *port)\n{\n\tu32 val;\n\n\tval = readl(port->base + MVPP2_GMAC_CTRL_1_REG) &\n\t\t    ~MVPP2_GMAC_PERIODIC_XON_EN_MASK;\n\twritel(val, port->base + MVPP2_GMAC_CTRL_1_REG);\n}\n\n \nstatic void mvpp2_port_loopback_set(struct mvpp2_port *port,\n\t\t\t\t    const struct phylink_link_state *state)\n{\n\tu32 val;\n\n\tval = readl(port->base + MVPP2_GMAC_CTRL_1_REG);\n\n\tif (state->speed == 1000)\n\t\tval |= MVPP2_GMAC_GMII_LB_EN_MASK;\n\telse\n\t\tval &= ~MVPP2_GMAC_GMII_LB_EN_MASK;\n\n\tif (phy_interface_mode_is_8023z(state->interface) ||\n\t    state->interface == PHY_INTERFACE_MODE_SGMII)\n\t\tval |= MVPP2_GMAC_PCS_LB_EN_MASK;\n\telse\n\t\tval &= ~MVPP2_GMAC_PCS_LB_EN_MASK;\n\n\twritel(val, port->base + MVPP2_GMAC_CTRL_1_REG);\n}\n\nenum {\n\tETHTOOL_XDP_REDIRECT,\n\tETHTOOL_XDP_PASS,\n\tETHTOOL_XDP_DROP,\n\tETHTOOL_XDP_TX,\n\tETHTOOL_XDP_TX_ERR,\n\tETHTOOL_XDP_XMIT,\n\tETHTOOL_XDP_XMIT_ERR,\n};\n\nstruct mvpp2_ethtool_counter {\n\tunsigned int offset;\n\tconst char string[ETH_GSTRING_LEN];\n\tbool reg_is_64b;\n};\n\nstatic u64 mvpp2_read_count(struct mvpp2_port *port,\n\t\t\t    const struct mvpp2_ethtool_counter *counter)\n{\n\tu64 val;\n\n\tval = readl(port->stats_base + counter->offset);\n\tif (counter->reg_is_64b)\n\t\tval += (u64)readl(port->stats_base + counter->offset + 4) << 32;\n\n\treturn val;\n}\n\n \nstatic u32 mvpp2_read_index(struct mvpp2 *priv, u32 index, u32 reg)\n{\n\tmvpp2_write(priv, MVPP2_CTRS_IDX, index);\n\treturn mvpp2_read(priv, reg);\n}\n\n \nstatic const struct mvpp2_ethtool_counter mvpp2_ethtool_mib_regs[] = {\n\t{ MVPP2_MIB_GOOD_OCTETS_RCVD, \"good_octets_received\", true },\n\t{ MVPP2_MIB_BAD_OCTETS_RCVD, \"bad_octets_received\" },\n\t{ MVPP2_MIB_CRC_ERRORS_SENT, \"crc_errors_sent\" },\n\t{ MVPP2_MIB_UNICAST_FRAMES_RCVD, \"unicast_frames_received\" },\n\t{ MVPP2_MIB_BROADCAST_FRAMES_RCVD, \"broadcast_frames_received\" },\n\t{ MVPP2_MIB_MULTICAST_FRAMES_RCVD, \"multicast_frames_received\" },\n\t{ MVPP2_MIB_FRAMES_64_OCTETS, \"frames_64_octets\" },\n\t{ MVPP2_MIB_FRAMES_65_TO_127_OCTETS, \"frames_65_to_127_octet\" },\n\t{ MVPP2_MIB_FRAMES_128_TO_255_OCTETS, \"frames_128_to_255_octet\" },\n\t{ MVPP2_MIB_FRAMES_256_TO_511_OCTETS, \"frames_256_to_511_octet\" },\n\t{ MVPP2_MIB_FRAMES_512_TO_1023_OCTETS, \"frames_512_to_1023_octet\" },\n\t{ MVPP2_MIB_FRAMES_1024_TO_MAX_OCTETS, \"frames_1024_to_max_octet\" },\n\t{ MVPP2_MIB_GOOD_OCTETS_SENT, \"good_octets_sent\", true },\n\t{ MVPP2_MIB_UNICAST_FRAMES_SENT, \"unicast_frames_sent\" },\n\t{ MVPP2_MIB_MULTICAST_FRAMES_SENT, \"multicast_frames_sent\" },\n\t{ MVPP2_MIB_BROADCAST_FRAMES_SENT, \"broadcast_frames_sent\" },\n\t{ MVPP2_MIB_FC_SENT, \"fc_sent\" },\n\t{ MVPP2_MIB_FC_RCVD, \"fc_received\" },\n\t{ MVPP2_MIB_RX_FIFO_OVERRUN, \"rx_fifo_overrun\" },\n\t{ MVPP2_MIB_UNDERSIZE_RCVD, \"undersize_received\" },\n\t{ MVPP2_MIB_FRAGMENTS_RCVD, \"fragments_received\" },\n\t{ MVPP2_MIB_OVERSIZE_RCVD, \"oversize_received\" },\n\t{ MVPP2_MIB_JABBER_RCVD, \"jabber_received\" },\n\t{ MVPP2_MIB_MAC_RCV_ERROR, \"mac_receive_error\" },\n\t{ MVPP2_MIB_BAD_CRC_EVENT, \"bad_crc_event\" },\n\t{ MVPP2_MIB_COLLISION, \"collision\" },\n\t{ MVPP2_MIB_LATE_COLLISION, \"late_collision\" },\n};\n\nstatic const struct mvpp2_ethtool_counter mvpp2_ethtool_port_regs[] = {\n\t{ MVPP2_OVERRUN_ETH_DROP, \"rx_fifo_or_parser_overrun_drops\" },\n\t{ MVPP2_CLS_ETH_DROP, \"rx_classifier_drops\" },\n};\n\nstatic const struct mvpp2_ethtool_counter mvpp2_ethtool_txq_regs[] = {\n\t{ MVPP2_TX_DESC_ENQ_CTR, \"txq_%d_desc_enqueue\" },\n\t{ MVPP2_TX_DESC_ENQ_TO_DDR_CTR, \"txq_%d_desc_enqueue_to_ddr\" },\n\t{ MVPP2_TX_BUFF_ENQ_TO_DDR_CTR, \"txq_%d_buff_euqueue_to_ddr\" },\n\t{ MVPP2_TX_DESC_ENQ_HW_FWD_CTR, \"txq_%d_desc_hardware_forwarded\" },\n\t{ MVPP2_TX_PKTS_DEQ_CTR, \"txq_%d_packets_dequeued\" },\n\t{ MVPP2_TX_PKTS_FULL_QUEUE_DROP_CTR, \"txq_%d_queue_full_drops\" },\n\t{ MVPP2_TX_PKTS_EARLY_DROP_CTR, \"txq_%d_packets_early_drops\" },\n\t{ MVPP2_TX_PKTS_BM_DROP_CTR, \"txq_%d_packets_bm_drops\" },\n\t{ MVPP2_TX_PKTS_BM_MC_DROP_CTR, \"txq_%d_packets_rep_bm_drops\" },\n};\n\nstatic const struct mvpp2_ethtool_counter mvpp2_ethtool_rxq_regs[] = {\n\t{ MVPP2_RX_DESC_ENQ_CTR, \"rxq_%d_desc_enqueue\" },\n\t{ MVPP2_RX_PKTS_FULL_QUEUE_DROP_CTR, \"rxq_%d_queue_full_drops\" },\n\t{ MVPP2_RX_PKTS_EARLY_DROP_CTR, \"rxq_%d_packets_early_drops\" },\n\t{ MVPP2_RX_PKTS_BM_DROP_CTR, \"rxq_%d_packets_bm_drops\" },\n};\n\nstatic const struct mvpp2_ethtool_counter mvpp2_ethtool_xdp[] = {\n\t{ ETHTOOL_XDP_REDIRECT, \"rx_xdp_redirect\", },\n\t{ ETHTOOL_XDP_PASS, \"rx_xdp_pass\", },\n\t{ ETHTOOL_XDP_DROP, \"rx_xdp_drop\", },\n\t{ ETHTOOL_XDP_TX, \"rx_xdp_tx\", },\n\t{ ETHTOOL_XDP_TX_ERR, \"rx_xdp_tx_errors\", },\n\t{ ETHTOOL_XDP_XMIT, \"tx_xdp_xmit\", },\n\t{ ETHTOOL_XDP_XMIT_ERR, \"tx_xdp_xmit_errors\", },\n};\n\n#define MVPP2_N_ETHTOOL_STATS(ntxqs, nrxqs)\t(ARRAY_SIZE(mvpp2_ethtool_mib_regs) + \\\n\t\t\t\t\t\t ARRAY_SIZE(mvpp2_ethtool_port_regs) + \\\n\t\t\t\t\t\t (ARRAY_SIZE(mvpp2_ethtool_txq_regs) * (ntxqs)) + \\\n\t\t\t\t\t\t (ARRAY_SIZE(mvpp2_ethtool_rxq_regs) * (nrxqs)) + \\\n\t\t\t\t\t\t ARRAY_SIZE(mvpp2_ethtool_xdp))\n\nstatic void mvpp2_ethtool_get_strings(struct net_device *netdev, u32 sset,\n\t\t\t\t      u8 *data)\n{\n\tstruct mvpp2_port *port = netdev_priv(netdev);\n\tint i, q;\n\n\tif (sset != ETH_SS_STATS)\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_mib_regs); i++) {\n\t\tstrscpy(data, mvpp2_ethtool_mib_regs[i].string,\n\t\t\tETH_GSTRING_LEN);\n\t\tdata += ETH_GSTRING_LEN;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_port_regs); i++) {\n\t\tstrscpy(data, mvpp2_ethtool_port_regs[i].string,\n\t\t\tETH_GSTRING_LEN);\n\t\tdata += ETH_GSTRING_LEN;\n\t}\n\n\tfor (q = 0; q < port->ntxqs; q++) {\n\t\tfor (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_txq_regs); i++) {\n\t\t\tsnprintf(data, ETH_GSTRING_LEN,\n\t\t\t\t mvpp2_ethtool_txq_regs[i].string, q);\n\t\t\tdata += ETH_GSTRING_LEN;\n\t\t}\n\t}\n\n\tfor (q = 0; q < port->nrxqs; q++) {\n\t\tfor (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_rxq_regs); i++) {\n\t\t\tsnprintf(data, ETH_GSTRING_LEN,\n\t\t\t\t mvpp2_ethtool_rxq_regs[i].string,\n\t\t\t\t q);\n\t\t\tdata += ETH_GSTRING_LEN;\n\t\t}\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_xdp); i++) {\n\t\tstrscpy(data, mvpp2_ethtool_xdp[i].string,\n\t\t\tETH_GSTRING_LEN);\n\t\tdata += ETH_GSTRING_LEN;\n\t}\n}\n\nstatic void\nmvpp2_get_xdp_stats(struct mvpp2_port *port, struct mvpp2_pcpu_stats *xdp_stats)\n{\n\tunsigned int start;\n\tunsigned int cpu;\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct mvpp2_pcpu_stats *cpu_stats;\n\t\tu64\txdp_redirect;\n\t\tu64\txdp_pass;\n\t\tu64\txdp_drop;\n\t\tu64\txdp_xmit;\n\t\tu64\txdp_xmit_err;\n\t\tu64\txdp_tx;\n\t\tu64\txdp_tx_err;\n\n\t\tcpu_stats = per_cpu_ptr(port->stats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&cpu_stats->syncp);\n\t\t\txdp_redirect = cpu_stats->xdp_redirect;\n\t\t\txdp_pass   = cpu_stats->xdp_pass;\n\t\t\txdp_drop = cpu_stats->xdp_drop;\n\t\t\txdp_xmit   = cpu_stats->xdp_xmit;\n\t\t\txdp_xmit_err   = cpu_stats->xdp_xmit_err;\n\t\t\txdp_tx   = cpu_stats->xdp_tx;\n\t\t\txdp_tx_err   = cpu_stats->xdp_tx_err;\n\t\t} while (u64_stats_fetch_retry(&cpu_stats->syncp, start));\n\n\t\txdp_stats->xdp_redirect += xdp_redirect;\n\t\txdp_stats->xdp_pass   += xdp_pass;\n\t\txdp_stats->xdp_drop += xdp_drop;\n\t\txdp_stats->xdp_xmit   += xdp_xmit;\n\t\txdp_stats->xdp_xmit_err   += xdp_xmit_err;\n\t\txdp_stats->xdp_tx   += xdp_tx;\n\t\txdp_stats->xdp_tx_err   += xdp_tx_err;\n\t}\n}\n\nstatic void mvpp2_read_stats(struct mvpp2_port *port)\n{\n\tstruct mvpp2_pcpu_stats xdp_stats = {};\n\tconst struct mvpp2_ethtool_counter *s;\n\tu64 *pstats;\n\tint i, q;\n\n\tpstats = port->ethtool_stats;\n\n\tfor (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_mib_regs); i++)\n\t\t*pstats++ += mvpp2_read_count(port, &mvpp2_ethtool_mib_regs[i]);\n\n\tfor (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_port_regs); i++)\n\t\t*pstats++ += mvpp2_read(port->priv,\n\t\t\t\t\tmvpp2_ethtool_port_regs[i].offset +\n\t\t\t\t\t4 * port->id);\n\n\tfor (q = 0; q < port->ntxqs; q++)\n\t\tfor (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_txq_regs); i++)\n\t\t\t*pstats++ += mvpp2_read_index(port->priv,\n\t\t\t\t\t\t      MVPP22_CTRS_TX_CTR(port->id, q),\n\t\t\t\t\t\t      mvpp2_ethtool_txq_regs[i].offset);\n\n\t \n\tfor (q = 0; q < port->nrxqs; q++)\n\t\tfor (i = 0; i < ARRAY_SIZE(mvpp2_ethtool_rxq_regs); i++)\n\t\t\t*pstats++ += mvpp2_read_index(port->priv,\n\t\t\t\t\t\t      port->first_rxq + q,\n\t\t\t\t\t\t      mvpp2_ethtool_rxq_regs[i].offset);\n\n\t \n\tmvpp2_get_xdp_stats(port, &xdp_stats);\n\n\tfor (i = 0, s = mvpp2_ethtool_xdp;\n\t\t s < mvpp2_ethtool_xdp + ARRAY_SIZE(mvpp2_ethtool_xdp);\n\t     s++, i++) {\n\t\tswitch (s->offset) {\n\t\tcase ETHTOOL_XDP_REDIRECT:\n\t\t\t*pstats++ = xdp_stats.xdp_redirect;\n\t\t\tbreak;\n\t\tcase ETHTOOL_XDP_PASS:\n\t\t\t*pstats++ = xdp_stats.xdp_pass;\n\t\t\tbreak;\n\t\tcase ETHTOOL_XDP_DROP:\n\t\t\t*pstats++ = xdp_stats.xdp_drop;\n\t\t\tbreak;\n\t\tcase ETHTOOL_XDP_TX:\n\t\t\t*pstats++ = xdp_stats.xdp_tx;\n\t\t\tbreak;\n\t\tcase ETHTOOL_XDP_TX_ERR:\n\t\t\t*pstats++ = xdp_stats.xdp_tx_err;\n\t\t\tbreak;\n\t\tcase ETHTOOL_XDP_XMIT:\n\t\t\t*pstats++ = xdp_stats.xdp_xmit;\n\t\t\tbreak;\n\t\tcase ETHTOOL_XDP_XMIT_ERR:\n\t\t\t*pstats++ = xdp_stats.xdp_xmit_err;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void mvpp2_gather_hw_statistics(struct work_struct *work)\n{\n\tstruct delayed_work *del_work = to_delayed_work(work);\n\tstruct mvpp2_port *port = container_of(del_work, struct mvpp2_port,\n\t\t\t\t\t       stats_work);\n\n\tmutex_lock(&port->gather_stats_lock);\n\n\tmvpp2_read_stats(port);\n\n\t \n\tcancel_delayed_work(&port->stats_work);\n\tqueue_delayed_work(port->priv->stats_queue, &port->stats_work,\n\t\t\t   MVPP2_MIB_COUNTERS_STATS_DELAY);\n\n\tmutex_unlock(&port->gather_stats_lock);\n}\n\nstatic void mvpp2_ethtool_get_stats(struct net_device *dev,\n\t\t\t\t    struct ethtool_stats *stats, u64 *data)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\t \n\tmvpp2_gather_hw_statistics(&port->stats_work.work);\n\n\tmutex_lock(&port->gather_stats_lock);\n\tmemcpy(data, port->ethtool_stats,\n\t       sizeof(u64) * MVPP2_N_ETHTOOL_STATS(port->ntxqs, port->nrxqs));\n\tmutex_unlock(&port->gather_stats_lock);\n}\n\nstatic int mvpp2_ethtool_get_sset_count(struct net_device *dev, int sset)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tif (sset == ETH_SS_STATS)\n\t\treturn MVPP2_N_ETHTOOL_STATS(port->ntxqs, port->nrxqs);\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic void mvpp2_mac_reset_assert(struct mvpp2_port *port)\n{\n\tu32 val;\n\n\tval = readl(port->base + MVPP2_GMAC_CTRL_2_REG) |\n\t      MVPP2_GMAC_PORT_RESET_MASK;\n\twritel(val, port->base + MVPP2_GMAC_CTRL_2_REG);\n\n\tif (port->priv->hw_version >= MVPP22 && port->gop_id == 0) {\n\t\tval = readl(port->base + MVPP22_XLG_CTRL0_REG) &\n\t\t      ~MVPP22_XLG_CTRL0_MAC_RESET_DIS;\n\t\twritel(val, port->base + MVPP22_XLG_CTRL0_REG);\n\t}\n}\n\nstatic void mvpp22_pcs_reset_assert(struct mvpp2_port *port)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tvoid __iomem *mpcs, *xpcs;\n\tu32 val;\n\n\tif (port->priv->hw_version == MVPP21 || port->gop_id != 0)\n\t\treturn;\n\n\tmpcs = priv->iface_base + MVPP22_MPCS_BASE(port->gop_id);\n\txpcs = priv->iface_base + MVPP22_XPCS_BASE(port->gop_id);\n\n\tval = readl(mpcs + MVPP22_MPCS_CLK_RESET);\n\tval &= ~(MAC_CLK_RESET_MAC | MAC_CLK_RESET_SD_RX | MAC_CLK_RESET_SD_TX);\n\tval |= MVPP22_MPCS_CLK_RESET_DIV_SET;\n\twritel(val, mpcs + MVPP22_MPCS_CLK_RESET);\n\n\tval = readl(xpcs + MVPP22_XPCS_CFG0);\n\twritel(val & ~MVPP22_XPCS_CFG0_RESET_DIS, xpcs + MVPP22_XPCS_CFG0);\n}\n\nstatic void mvpp22_pcs_reset_deassert(struct mvpp2_port *port,\n\t\t\t\t      phy_interface_t interface)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tvoid __iomem *mpcs, *xpcs;\n\tu32 val;\n\n\tif (port->priv->hw_version == MVPP21 || port->gop_id != 0)\n\t\treturn;\n\n\tmpcs = priv->iface_base + MVPP22_MPCS_BASE(port->gop_id);\n\txpcs = priv->iface_base + MVPP22_XPCS_BASE(port->gop_id);\n\n\tswitch (interface) {\n\tcase PHY_INTERFACE_MODE_5GBASER:\n\tcase PHY_INTERFACE_MODE_10GBASER:\n\t\tval = readl(mpcs + MVPP22_MPCS_CLK_RESET);\n\t\tval |= MAC_CLK_RESET_MAC | MAC_CLK_RESET_SD_RX |\n\t\t       MAC_CLK_RESET_SD_TX;\n\t\tval &= ~MVPP22_MPCS_CLK_RESET_DIV_SET;\n\t\twritel(val, mpcs + MVPP22_MPCS_CLK_RESET);\n\t\tbreak;\n\tcase PHY_INTERFACE_MODE_XAUI:\n\tcase PHY_INTERFACE_MODE_RXAUI:\n\t\tval = readl(xpcs + MVPP22_XPCS_CFG0);\n\t\twritel(val | MVPP22_XPCS_CFG0_RESET_DIS, xpcs + MVPP22_XPCS_CFG0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n \nstatic inline void mvpp2_gmac_max_rx_size_set(struct mvpp2_port *port)\n{\n\tu32 val;\n\n\tval = readl(port->base + MVPP2_GMAC_CTRL_0_REG);\n\tval &= ~MVPP2_GMAC_MAX_RX_SIZE_MASK;\n\tval |= (((port->pkt_size - MVPP2_MH_SIZE) / 2) <<\n\t\t    MVPP2_GMAC_MAX_RX_SIZE_OFFS);\n\twritel(val, port->base + MVPP2_GMAC_CTRL_0_REG);\n}\n\n \nstatic inline void mvpp2_xlg_max_rx_size_set(struct mvpp2_port *port)\n{\n\tu32 val;\n\n\tval =  readl(port->base + MVPP22_XLG_CTRL1_REG);\n\tval &= ~MVPP22_XLG_CTRL1_FRAMESIZELIMIT_MASK;\n\tval |= ((port->pkt_size - MVPP2_MH_SIZE) / 2) <<\n\t       MVPP22_XLG_CTRL1_FRAMESIZELIMIT_OFFS;\n\twritel(val, port->base + MVPP22_XLG_CTRL1_REG);\n}\n\n \nstatic void mvpp2_defaults_set(struct mvpp2_port *port)\n{\n\tint tx_port_num, val, queue, lrxq;\n\n\tif (port->priv->hw_version == MVPP21) {\n\t\t \n\t\tval = readl(port->base + MVPP2_GMAC_PORT_FIFO_CFG_1_REG);\n\t\tval &= ~MVPP2_GMAC_TX_FIFO_MIN_TH_ALL_MASK;\n\t\t \n\t\tval |= MVPP2_GMAC_TX_FIFO_MIN_TH_MASK(64 - 4 - 2);\n\t\twritel(val, port->base + MVPP2_GMAC_PORT_FIFO_CFG_1_REG);\n\t}\n\n\t \n\ttx_port_num = mvpp2_egress_port(port);\n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG,\n\t\t    tx_port_num);\n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_CMD_1_REG, 0);\n\n\t \n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_FIXED_PRIO_REG, 0);\n\n\t \n\tfor (queue = 0; queue < MVPP2_MAX_TXQ; queue++)\n\t\tmvpp2_write(port->priv,\n\t\t\t    MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(queue), 0);\n\n\t \n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_PERIOD_REG,\n\t\t    port->priv->tclk / USEC_PER_SEC);\n\tval = mvpp2_read(port->priv, MVPP2_TXP_SCHED_REFILL_REG);\n\tval &= ~MVPP2_TXP_REFILL_PERIOD_ALL_MASK;\n\tval |= MVPP2_TXP_REFILL_PERIOD_MASK(1);\n\tval |= MVPP2_TXP_REFILL_TOKENS_ALL_MASK;\n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_REFILL_REG, val);\n\tval = MVPP2_TXP_TOKEN_SIZE_MAX;\n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_TOKEN_SIZE_REG, val);\n\n\t \n\tmvpp2_write(port->priv, MVPP2_RX_CTRL_REG(port->id),\n\t\t    MVPP2_RX_USE_PSEUDO_FOR_CSUM_MASK |\n\t\t    MVPP2_RX_LOW_LATENCY_PKT_SIZE(256));\n\n\t \n\tfor (lrxq = 0; lrxq < port->nrxqs; lrxq++) {\n\t\tqueue = port->rxqs[lrxq]->id;\n\t\tval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(queue));\n\t\tval |= MVPP2_SNOOP_PKT_SIZE_MASK |\n\t\t\t   MVPP2_SNOOP_BUF_HDR_MASK;\n\t\tmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(queue), val);\n\t}\n\n\t \n\tmvpp2_interrupts_disable(port);\n}\n\n \nstatic void mvpp2_ingress_enable(struct mvpp2_port *port)\n{\n\tu32 val;\n\tint lrxq, queue;\n\n\tfor (lrxq = 0; lrxq < port->nrxqs; lrxq++) {\n\t\tqueue = port->rxqs[lrxq]->id;\n\t\tval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(queue));\n\t\tval &= ~MVPP2_RXQ_DISABLE_MASK;\n\t\tmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(queue), val);\n\t}\n}\n\nstatic void mvpp2_ingress_disable(struct mvpp2_port *port)\n{\n\tu32 val;\n\tint lrxq, queue;\n\n\tfor (lrxq = 0; lrxq < port->nrxqs; lrxq++) {\n\t\tqueue = port->rxqs[lrxq]->id;\n\t\tval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(queue));\n\t\tval |= MVPP2_RXQ_DISABLE_MASK;\n\t\tmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(queue), val);\n\t}\n}\n\n \nstatic void mvpp2_egress_enable(struct mvpp2_port *port)\n{\n\tu32 qmap;\n\tint queue;\n\tint tx_port_num = mvpp2_egress_port(port);\n\n\t \n\tqmap = 0;\n\tfor (queue = 0; queue < port->ntxqs; queue++) {\n\t\tstruct mvpp2_tx_queue *txq = port->txqs[queue];\n\n\t\tif (txq->descs)\n\t\t\tqmap |= (1 << queue);\n\t}\n\n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);\n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_Q_CMD_REG, qmap);\n}\n\n \nstatic void mvpp2_egress_disable(struct mvpp2_port *port)\n{\n\tu32 reg_data;\n\tint delay;\n\tint tx_port_num = mvpp2_egress_port(port);\n\n\t \n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);\n\treg_data = (mvpp2_read(port->priv, MVPP2_TXP_SCHED_Q_CMD_REG)) &\n\t\t    MVPP2_TXP_SCHED_ENQ_MASK;\n\tif (reg_data != 0)\n\t\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_Q_CMD_REG,\n\t\t\t    (reg_data << MVPP2_TXP_SCHED_DISQ_OFFSET));\n\n\t \n\tdelay = 0;\n\tdo {\n\t\tif (delay >= MVPP2_TX_DISABLE_TIMEOUT_MSEC) {\n\t\t\tnetdev_warn(port->dev,\n\t\t\t\t    \"Tx stop timed out, status=0x%08x\\n\",\n\t\t\t\t    reg_data);\n\t\t\tbreak;\n\t\t}\n\t\tmdelay(1);\n\t\tdelay++;\n\n\t\t \n\t\treg_data = mvpp2_read(port->priv, MVPP2_TXP_SCHED_Q_CMD_REG);\n\t} while (reg_data & MVPP2_TXP_SCHED_ENQ_MASK);\n}\n\n \n\n \nstatic inline int\nmvpp2_rxq_received(struct mvpp2_port *port, int rxq_id)\n{\n\tu32 val = mvpp2_read(port->priv, MVPP2_RXQ_STATUS_REG(rxq_id));\n\n\treturn val & MVPP2_RXQ_OCCUPIED_MASK;\n}\n\n \nstatic inline void\nmvpp2_rxq_status_update(struct mvpp2_port *port, int rxq_id,\n\t\t\tint used_count, int free_count)\n{\n\t \n\tu32 val = used_count | (free_count << MVPP2_RXQ_NUM_NEW_OFFSET);\n\n\tmvpp2_write(port->priv, MVPP2_RXQ_STATUS_UPDATE_REG(rxq_id), val);\n}\n\n \nstatic inline struct mvpp2_rx_desc *\nmvpp2_rxq_next_desc_get(struct mvpp2_rx_queue *rxq)\n{\n\tint rx_desc = rxq->next_desc_to_proc;\n\n\trxq->next_desc_to_proc = MVPP2_QUEUE_NEXT_DESC(rxq, rx_desc);\n\tprefetch(rxq->descs + rxq->next_desc_to_proc);\n\treturn rxq->descs + rx_desc;\n}\n\n \nstatic void mvpp2_rxq_offset_set(struct mvpp2_port *port,\n\t\t\t\t int prxq, int offset)\n{\n\tu32 val;\n\n\t \n\toffset = offset >> 5;\n\n\tval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(prxq));\n\tval &= ~MVPP2_RXQ_PACKET_OFFSET_MASK;\n\n\t \n\tval |= ((offset << MVPP2_RXQ_PACKET_OFFSET_OFFS) &\n\t\t    MVPP2_RXQ_PACKET_OFFSET_MASK);\n\n\tmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(prxq), val);\n}\n\n \n\n \nstatic struct mvpp2_tx_desc *\nmvpp2_txq_next_desc_get(struct mvpp2_tx_queue *txq)\n{\n\tint tx_desc = txq->next_desc_to_proc;\n\n\ttxq->next_desc_to_proc = MVPP2_QUEUE_NEXT_DESC(txq, tx_desc);\n\treturn txq->descs + tx_desc;\n}\n\n \nstatic void mvpp2_aggr_txq_pend_desc_add(struct mvpp2_port *port, int pending)\n{\n\t \n\tmvpp2_thread_write(port->priv,\n\t\t\t   mvpp2_cpu_to_thread(port->priv, smp_processor_id()),\n\t\t\t   MVPP2_AGGR_TXQ_UPDATE_REG, pending);\n}\n\n \nstatic int mvpp2_aggr_desc_num_check(struct mvpp2_port *port,\n\t\t\t\t     struct mvpp2_tx_queue *aggr_txq, int num)\n{\n\tif ((aggr_txq->count + num) > MVPP2_AGGR_TXQ_SIZE) {\n\t\t \n\t\tunsigned int thread =\n\t\t\tmvpp2_cpu_to_thread(port->priv, smp_processor_id());\n\t\tu32 val = mvpp2_read_relaxed(port->priv,\n\t\t\t\t\t     MVPP2_AGGR_TXQ_STATUS_REG(thread));\n\n\t\taggr_txq->count = val & MVPP2_AGGR_TXQ_PENDING_MASK;\n\n\t\tif ((aggr_txq->count + num) > MVPP2_AGGR_TXQ_SIZE)\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\n \nstatic int mvpp2_txq_alloc_reserved_desc(struct mvpp2_port *port,\n\t\t\t\t\t struct mvpp2_tx_queue *txq, int num)\n{\n\tunsigned int thread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());\n\tstruct mvpp2 *priv = port->priv;\n\tu32 val;\n\n\tval = (txq->id << MVPP2_TXQ_RSVD_REQ_Q_OFFSET) | num;\n\tmvpp2_thread_write_relaxed(priv, thread, MVPP2_TXQ_RSVD_REQ_REG, val);\n\n\tval = mvpp2_thread_read_relaxed(priv, thread, MVPP2_TXQ_RSVD_RSLT_REG);\n\n\treturn val & MVPP2_TXQ_RSVD_RSLT_MASK;\n}\n\n \nstatic int mvpp2_txq_reserved_desc_num_proc(struct mvpp2_port *port,\n\t\t\t\t\t    struct mvpp2_tx_queue *txq,\n\t\t\t\t\t    struct mvpp2_txq_pcpu *txq_pcpu,\n\t\t\t\t\t    int num)\n{\n\tint req, desc_count;\n\tunsigned int thread;\n\n\tif (txq_pcpu->reserved_num >= num)\n\t\treturn 0;\n\n\t \n\n\tdesc_count = 0;\n\t \n\tfor (thread = 0; thread < port->priv->nthreads; thread++) {\n\t\tstruct mvpp2_txq_pcpu *txq_pcpu_aux;\n\n\t\ttxq_pcpu_aux = per_cpu_ptr(txq->pcpu, thread);\n\t\tdesc_count += txq_pcpu_aux->count;\n\t\tdesc_count += txq_pcpu_aux->reserved_num;\n\t}\n\n\treq = max(MVPP2_CPU_DESC_CHUNK, num - txq_pcpu->reserved_num);\n\tdesc_count += req;\n\n\tif (desc_count >\n\t   (txq->size - (MVPP2_MAX_THREADS * MVPP2_CPU_DESC_CHUNK)))\n\t\treturn -ENOMEM;\n\n\ttxq_pcpu->reserved_num += mvpp2_txq_alloc_reserved_desc(port, txq, req);\n\n\t \n\tif (txq_pcpu->reserved_num < num)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\n \nstatic void mvpp2_txq_desc_put(struct mvpp2_tx_queue *txq)\n{\n\tif (txq->next_desc_to_proc == 0)\n\t\ttxq->next_desc_to_proc = txq->last_desc - 1;\n\telse\n\t\ttxq->next_desc_to_proc--;\n}\n\n \nstatic u32 mvpp2_txq_desc_csum(int l3_offs, __be16 l3_proto,\n\t\t\t       int ip_hdr_len, int l4_proto)\n{\n\tu32 command;\n\n\t \n\tcommand = (l3_offs << MVPP2_TXD_L3_OFF_SHIFT);\n\tcommand |= (ip_hdr_len << MVPP2_TXD_IP_HLEN_SHIFT);\n\tcommand |= MVPP2_TXD_IP_CSUM_DISABLE;\n\n\tif (l3_proto == htons(ETH_P_IP)) {\n\t\tcommand &= ~MVPP2_TXD_IP_CSUM_DISABLE;\t \n\t\tcommand &= ~MVPP2_TXD_L3_IP6;\t\t \n\t} else {\n\t\tcommand |= MVPP2_TXD_L3_IP6;\t\t \n\t}\n\n\tif (l4_proto == IPPROTO_TCP) {\n\t\tcommand &= ~MVPP2_TXD_L4_UDP;\t\t \n\t\tcommand &= ~MVPP2_TXD_L4_CSUM_FRAG;\t \n\t} else if (l4_proto == IPPROTO_UDP) {\n\t\tcommand |= MVPP2_TXD_L4_UDP;\t\t \n\t\tcommand &= ~MVPP2_TXD_L4_CSUM_FRAG;\t \n\t} else {\n\t\tcommand |= MVPP2_TXD_L4_CSUM_NOT;\n\t}\n\n\treturn command;\n}\n\n \nstatic inline int mvpp2_txq_sent_desc_proc(struct mvpp2_port *port,\n\t\t\t\t\t   struct mvpp2_tx_queue *txq)\n{\n\tu32 val;\n\n\t \n\tval = mvpp2_thread_read_relaxed(port->priv,\n\t\t\t\t\tmvpp2_cpu_to_thread(port->priv, smp_processor_id()),\n\t\t\t\t\tMVPP2_TXQ_SENT_REG(txq->id));\n\n\treturn (val & MVPP2_TRANSMITTED_COUNT_MASK) >>\n\t\tMVPP2_TRANSMITTED_COUNT_OFFSET;\n}\n\n \nstatic void mvpp2_txq_sent_counter_clear(void *arg)\n{\n\tstruct mvpp2_port *port = arg;\n\tint queue;\n\n\t \n\tif (smp_processor_id() >= port->priv->nthreads)\n\t\treturn;\n\n\tfor (queue = 0; queue < port->ntxqs; queue++) {\n\t\tint id = port->txqs[queue]->id;\n\n\t\tmvpp2_thread_read(port->priv,\n\t\t\t\t  mvpp2_cpu_to_thread(port->priv, smp_processor_id()),\n\t\t\t\t  MVPP2_TXQ_SENT_REG(id));\n\t}\n}\n\n \nstatic void mvpp2_txp_max_tx_size_set(struct mvpp2_port *port)\n{\n\tu32\tval, size, mtu;\n\tint\ttxq, tx_port_num;\n\n\tmtu = port->pkt_size * 8;\n\tif (mtu > MVPP2_TXP_MTU_MAX)\n\t\tmtu = MVPP2_TXP_MTU_MAX;\n\n\t \n\tmtu = 3 * mtu;\n\n\t \n\ttx_port_num = mvpp2_egress_port(port);\n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);\n\n\t \n\tval = mvpp2_read(port->priv, MVPP2_TXP_SCHED_MTU_REG);\n\tval &= ~MVPP2_TXP_MTU_MAX;\n\tval |= mtu;\n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_MTU_REG, val);\n\n\t \n\tval = mvpp2_read(port->priv, MVPP2_TXP_SCHED_TOKEN_SIZE_REG);\n\tsize = val & MVPP2_TXP_TOKEN_SIZE_MAX;\n\tif (size < mtu) {\n\t\tsize = mtu;\n\t\tval &= ~MVPP2_TXP_TOKEN_SIZE_MAX;\n\t\tval |= size;\n\t\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_TOKEN_SIZE_REG, val);\n\t}\n\n\tfor (txq = 0; txq < port->ntxqs; txq++) {\n\t\tval = mvpp2_read(port->priv,\n\t\t\t\t MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq));\n\t\tsize = val & MVPP2_TXQ_TOKEN_SIZE_MAX;\n\n\t\tif (size < mtu) {\n\t\t\tsize = mtu;\n\t\t\tval &= ~MVPP2_TXQ_TOKEN_SIZE_MAX;\n\t\t\tval |= size;\n\t\t\tmvpp2_write(port->priv,\n\t\t\t\t    MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq),\n\t\t\t\t    val);\n\t\t}\n\t}\n}\n\n \nstatic void mvpp2_set_rxq_free_tresh(struct mvpp2_port *port,\n\t\t\t\t     struct mvpp2_rx_queue *rxq)\n{\n\tu32 val;\n\n\tmvpp2_write(port->priv, MVPP2_RXQ_NUM_REG, rxq->id);\n\n\tval = mvpp2_read(port->priv, MVPP2_RXQ_THRESH_REG);\n\tval &= ~MVPP2_RXQ_NON_OCCUPIED_MASK;\n\tval |= MSS_THRESHOLD_STOP << MVPP2_RXQ_NON_OCCUPIED_OFFSET;\n\tmvpp2_write(port->priv, MVPP2_RXQ_THRESH_REG, val);\n}\n\n \nstatic void mvpp2_rx_pkts_coal_set(struct mvpp2_port *port,\n\t\t\t\t   struct mvpp2_rx_queue *rxq)\n{\n\tunsigned int thread = mvpp2_cpu_to_thread(port->priv, get_cpu());\n\n\tif (rxq->pkts_coal > MVPP2_OCCUPIED_THRESH_MASK)\n\t\trxq->pkts_coal = MVPP2_OCCUPIED_THRESH_MASK;\n\n\tmvpp2_thread_write(port->priv, thread, MVPP2_RXQ_NUM_REG, rxq->id);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_RXQ_THRESH_REG,\n\t\t\t   rxq->pkts_coal);\n\n\tput_cpu();\n}\n\n \nstatic void mvpp2_tx_pkts_coal_set(struct mvpp2_port *port,\n\t\t\t\t   struct mvpp2_tx_queue *txq)\n{\n\tunsigned int thread;\n\tu32 val;\n\n\tif (txq->done_pkts_coal > MVPP2_TXQ_THRESH_MASK)\n\t\ttxq->done_pkts_coal = MVPP2_TXQ_THRESH_MASK;\n\n\tval = (txq->done_pkts_coal << MVPP2_TXQ_THRESH_OFFSET);\n\t \n\tfor (thread = 0; thread < MVPP2_MAX_THREADS; thread++) {\n\t\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_NUM_REG, txq->id);\n\t\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_THRESH_REG, val);\n\t}\n}\n\nstatic u32 mvpp2_usec_to_cycles(u32 usec, unsigned long clk_hz)\n{\n\tu64 tmp = (u64)clk_hz * usec;\n\n\tdo_div(tmp, USEC_PER_SEC);\n\n\treturn tmp > U32_MAX ? U32_MAX : tmp;\n}\n\nstatic u32 mvpp2_cycles_to_usec(u32 cycles, unsigned long clk_hz)\n{\n\tu64 tmp = (u64)cycles * USEC_PER_SEC;\n\n\tdo_div(tmp, clk_hz);\n\n\treturn tmp > U32_MAX ? U32_MAX : tmp;\n}\n\n \nstatic void mvpp2_rx_time_coal_set(struct mvpp2_port *port,\n\t\t\t\t   struct mvpp2_rx_queue *rxq)\n{\n\tunsigned long freq = port->priv->tclk;\n\tu32 val = mvpp2_usec_to_cycles(rxq->time_coal, freq);\n\n\tif (val > MVPP2_MAX_ISR_RX_THRESHOLD) {\n\t\trxq->time_coal =\n\t\t\tmvpp2_cycles_to_usec(MVPP2_MAX_ISR_RX_THRESHOLD, freq);\n\n\t\t \n\t\tval = mvpp2_usec_to_cycles(rxq->time_coal, freq);\n\t}\n\n\tmvpp2_write(port->priv, MVPP2_ISR_RX_THRESHOLD_REG(rxq->id), val);\n}\n\nstatic void mvpp2_tx_time_coal_set(struct mvpp2_port *port)\n{\n\tunsigned long freq = port->priv->tclk;\n\tu32 val = mvpp2_usec_to_cycles(port->tx_time_coal, freq);\n\n\tif (val > MVPP2_MAX_ISR_TX_THRESHOLD) {\n\t\tport->tx_time_coal =\n\t\t\tmvpp2_cycles_to_usec(MVPP2_MAX_ISR_TX_THRESHOLD, freq);\n\n\t\t \n\t\tval = mvpp2_usec_to_cycles(port->tx_time_coal, freq);\n\t}\n\n\tmvpp2_write(port->priv, MVPP2_ISR_TX_THRESHOLD_REG(port->id), val);\n}\n\n \nstatic void mvpp2_txq_bufs_free(struct mvpp2_port *port,\n\t\t\t\tstruct mvpp2_tx_queue *txq,\n\t\t\t\tstruct mvpp2_txq_pcpu *txq_pcpu, int num)\n{\n\tstruct xdp_frame_bulk bq;\n\tint i;\n\n\txdp_frame_bulk_init(&bq);\n\n\trcu_read_lock();  \n\n\tfor (i = 0; i < num; i++) {\n\t\tstruct mvpp2_txq_pcpu_buf *tx_buf =\n\t\t\ttxq_pcpu->buffs + txq_pcpu->txq_get_index;\n\n\t\tif (!IS_TSO_HEADER(txq_pcpu, tx_buf->dma) &&\n\t\t    tx_buf->type != MVPP2_TYPE_XDP_TX)\n\t\t\tdma_unmap_single(port->dev->dev.parent, tx_buf->dma,\n\t\t\t\t\t tx_buf->size, DMA_TO_DEVICE);\n\t\tif (tx_buf->type == MVPP2_TYPE_SKB && tx_buf->skb)\n\t\t\tdev_kfree_skb_any(tx_buf->skb);\n\t\telse if (tx_buf->type == MVPP2_TYPE_XDP_TX ||\n\t\t\t tx_buf->type == MVPP2_TYPE_XDP_NDO)\n\t\t\txdp_return_frame_bulk(tx_buf->xdpf, &bq);\n\n\t\tmvpp2_txq_inc_get(txq_pcpu);\n\t}\n\txdp_flush_frame_bulk(&bq);\n\n\trcu_read_unlock();\n}\n\nstatic inline struct mvpp2_rx_queue *mvpp2_get_rx_queue(struct mvpp2_port *port,\n\t\t\t\t\t\t\tu32 cause)\n{\n\tint queue = fls(cause) - 1;\n\n\treturn port->rxqs[queue];\n}\n\nstatic inline struct mvpp2_tx_queue *mvpp2_get_tx_queue(struct mvpp2_port *port,\n\t\t\t\t\t\t\tu32 cause)\n{\n\tint queue = fls(cause) - 1;\n\n\treturn port->txqs[queue];\n}\n\n \nstatic void mvpp2_txq_done(struct mvpp2_port *port, struct mvpp2_tx_queue *txq,\n\t\t\t   struct mvpp2_txq_pcpu *txq_pcpu)\n{\n\tstruct netdev_queue *nq = netdev_get_tx_queue(port->dev, txq->log_id);\n\tint tx_done;\n\n\tif (txq_pcpu->thread != mvpp2_cpu_to_thread(port->priv, smp_processor_id()))\n\t\tnetdev_err(port->dev, \"wrong cpu on the end of Tx processing\\n\");\n\n\ttx_done = mvpp2_txq_sent_desc_proc(port, txq);\n\tif (!tx_done)\n\t\treturn;\n\tmvpp2_txq_bufs_free(port, txq, txq_pcpu, tx_done);\n\n\ttxq_pcpu->count -= tx_done;\n\n\tif (netif_tx_queue_stopped(nq))\n\t\tif (txq_pcpu->count <= txq_pcpu->wake_threshold)\n\t\t\tnetif_tx_wake_queue(nq);\n}\n\nstatic unsigned int mvpp2_tx_done(struct mvpp2_port *port, u32 cause,\n\t\t\t\t  unsigned int thread)\n{\n\tstruct mvpp2_tx_queue *txq;\n\tstruct mvpp2_txq_pcpu *txq_pcpu;\n\tunsigned int tx_todo = 0;\n\n\twhile (cause) {\n\t\ttxq = mvpp2_get_tx_queue(port, cause);\n\t\tif (!txq)\n\t\t\tbreak;\n\n\t\ttxq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\n\t\tif (txq_pcpu->count) {\n\t\t\tmvpp2_txq_done(port, txq, txq_pcpu);\n\t\t\ttx_todo += txq_pcpu->count;\n\t\t}\n\n\t\tcause &= ~(1 << txq->log_id);\n\t}\n\treturn tx_todo;\n}\n\n \n\n \nstatic int mvpp2_aggr_txq_init(struct platform_device *pdev,\n\t\t\t       struct mvpp2_tx_queue *aggr_txq,\n\t\t\t       unsigned int thread, struct mvpp2 *priv)\n{\n\tu32 txq_dma;\n\n\t \n\taggr_txq->descs = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\t     MVPP2_AGGR_TXQ_SIZE * MVPP2_DESC_ALIGNED_SIZE,\n\t\t\t\t\t     &aggr_txq->descs_dma, GFP_KERNEL);\n\tif (!aggr_txq->descs)\n\t\treturn -ENOMEM;\n\n\taggr_txq->last_desc = MVPP2_AGGR_TXQ_SIZE - 1;\n\n\t \n\taggr_txq->next_desc_to_proc = mvpp2_read(priv,\n\t\t\t\t\t\t MVPP2_AGGR_TXQ_INDEX_REG(thread));\n\n\t \n\tif (priv->hw_version == MVPP21)\n\t\ttxq_dma = aggr_txq->descs_dma;\n\telse\n\t\ttxq_dma = aggr_txq->descs_dma >>\n\t\t\tMVPP22_AGGR_TXQ_DESC_ADDR_OFFS;\n\n\tmvpp2_write(priv, MVPP2_AGGR_TXQ_DESC_ADDR_REG(thread), txq_dma);\n\tmvpp2_write(priv, MVPP2_AGGR_TXQ_DESC_SIZE_REG(thread),\n\t\t    MVPP2_AGGR_TXQ_SIZE);\n\n\treturn 0;\n}\n\n \nstatic int mvpp2_rxq_init(struct mvpp2_port *port,\n\t\t\t  struct mvpp2_rx_queue *rxq)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tunsigned int thread;\n\tu32 rxq_dma;\n\tint err;\n\n\trxq->size = port->rx_ring_size;\n\n\t \n\trxq->descs = dma_alloc_coherent(port->dev->dev.parent,\n\t\t\t\t\trxq->size * MVPP2_DESC_ALIGNED_SIZE,\n\t\t\t\t\t&rxq->descs_dma, GFP_KERNEL);\n\tif (!rxq->descs)\n\t\treturn -ENOMEM;\n\n\trxq->last_desc = rxq->size - 1;\n\n\t \n\tmvpp2_write(port->priv, MVPP2_RXQ_STATUS_REG(rxq->id), 0);\n\n\t \n\tthread = mvpp2_cpu_to_thread(port->priv, get_cpu());\n\tmvpp2_thread_write(port->priv, thread, MVPP2_RXQ_NUM_REG, rxq->id);\n\tif (port->priv->hw_version == MVPP21)\n\t\trxq_dma = rxq->descs_dma;\n\telse\n\t\trxq_dma = rxq->descs_dma >> MVPP22_DESC_ADDR_OFFS;\n\tmvpp2_thread_write(port->priv, thread, MVPP2_RXQ_DESC_ADDR_REG, rxq_dma);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_RXQ_DESC_SIZE_REG, rxq->size);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_RXQ_INDEX_REG, 0);\n\tput_cpu();\n\n\t \n\tmvpp2_rxq_offset_set(port, rxq->id, MVPP2_SKB_HEADROOM);\n\n\t \n\tmvpp2_rx_pkts_coal_set(port, rxq);\n\tmvpp2_rx_time_coal_set(port, rxq);\n\n\t \n\tmvpp2_set_rxq_free_tresh(port, rxq);\n\n\t \n\tmvpp2_rxq_status_update(port, rxq->id, 0, rxq->size);\n\n\tif (priv->percpu_pools) {\n\t\terr = xdp_rxq_info_reg(&rxq->xdp_rxq_short, port->dev, rxq->logic_rxq, 0);\n\t\tif (err < 0)\n\t\t\tgoto err_free_dma;\n\n\t\terr = xdp_rxq_info_reg(&rxq->xdp_rxq_long, port->dev, rxq->logic_rxq, 0);\n\t\tif (err < 0)\n\t\t\tgoto err_unregister_rxq_short;\n\n\t\t \n\t\terr = xdp_rxq_info_reg_mem_model(&rxq->xdp_rxq_short,\n\t\t\t\t\t\t MEM_TYPE_PAGE_POOL,\n\t\t\t\t\t\t priv->page_pool[rxq->logic_rxq]);\n\t\tif (err < 0)\n\t\t\tgoto err_unregister_rxq_long;\n\n\t\terr = xdp_rxq_info_reg_mem_model(&rxq->xdp_rxq_long,\n\t\t\t\t\t\t MEM_TYPE_PAGE_POOL,\n\t\t\t\t\t\t priv->page_pool[rxq->logic_rxq +\n\t\t\t\t\t\t\t\t port->nrxqs]);\n\t\tif (err < 0)\n\t\t\tgoto err_unregister_mem_rxq_short;\n\t}\n\n\treturn 0;\n\nerr_unregister_mem_rxq_short:\n\txdp_rxq_info_unreg_mem_model(&rxq->xdp_rxq_short);\nerr_unregister_rxq_long:\n\txdp_rxq_info_unreg(&rxq->xdp_rxq_long);\nerr_unregister_rxq_short:\n\txdp_rxq_info_unreg(&rxq->xdp_rxq_short);\nerr_free_dma:\n\tdma_free_coherent(port->dev->dev.parent,\n\t\t\t  rxq->size * MVPP2_DESC_ALIGNED_SIZE,\n\t\t\t  rxq->descs, rxq->descs_dma);\n\treturn err;\n}\n\n \nstatic void mvpp2_rxq_drop_pkts(struct mvpp2_port *port,\n\t\t\t\tstruct mvpp2_rx_queue *rxq)\n{\n\tint rx_received, i;\n\n\trx_received = mvpp2_rxq_received(port, rxq->id);\n\tif (!rx_received)\n\t\treturn;\n\n\tfor (i = 0; i < rx_received; i++) {\n\t\tstruct mvpp2_rx_desc *rx_desc = mvpp2_rxq_next_desc_get(rxq);\n\t\tu32 status = mvpp2_rxdesc_status_get(port, rx_desc);\n\t\tint pool;\n\n\t\tpool = (status & MVPP2_RXD_BM_POOL_ID_MASK) >>\n\t\t\tMVPP2_RXD_BM_POOL_ID_OFFS;\n\n\t\tmvpp2_bm_pool_put(port, pool,\n\t\t\t\t  mvpp2_rxdesc_dma_addr_get(port, rx_desc),\n\t\t\t\t  mvpp2_rxdesc_cookie_get(port, rx_desc));\n\t}\n\tmvpp2_rxq_status_update(port, rxq->id, rx_received, rx_received);\n}\n\n \nstatic void mvpp2_rxq_deinit(struct mvpp2_port *port,\n\t\t\t     struct mvpp2_rx_queue *rxq)\n{\n\tunsigned int thread;\n\n\tif (xdp_rxq_info_is_reg(&rxq->xdp_rxq_short))\n\t\txdp_rxq_info_unreg(&rxq->xdp_rxq_short);\n\n\tif (xdp_rxq_info_is_reg(&rxq->xdp_rxq_long))\n\t\txdp_rxq_info_unreg(&rxq->xdp_rxq_long);\n\n\tmvpp2_rxq_drop_pkts(port, rxq);\n\n\tif (rxq->descs)\n\t\tdma_free_coherent(port->dev->dev.parent,\n\t\t\t\t  rxq->size * MVPP2_DESC_ALIGNED_SIZE,\n\t\t\t\t  rxq->descs,\n\t\t\t\t  rxq->descs_dma);\n\n\trxq->descs             = NULL;\n\trxq->last_desc         = 0;\n\trxq->next_desc_to_proc = 0;\n\trxq->descs_dma         = 0;\n\n\t \n\tmvpp2_write(port->priv, MVPP2_RXQ_STATUS_REG(rxq->id), 0);\n\tthread = mvpp2_cpu_to_thread(port->priv, get_cpu());\n\tmvpp2_thread_write(port->priv, thread, MVPP2_RXQ_NUM_REG, rxq->id);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_RXQ_DESC_ADDR_REG, 0);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_RXQ_DESC_SIZE_REG, 0);\n\tput_cpu();\n}\n\n \nstatic int mvpp2_txq_init(struct mvpp2_port *port,\n\t\t\t  struct mvpp2_tx_queue *txq)\n{\n\tu32 val;\n\tunsigned int thread;\n\tint desc, desc_per_txq, tx_port_num;\n\tstruct mvpp2_txq_pcpu *txq_pcpu;\n\n\ttxq->size = port->tx_ring_size;\n\n\t \n\ttxq->descs = dma_alloc_coherent(port->dev->dev.parent,\n\t\t\t\ttxq->size * MVPP2_DESC_ALIGNED_SIZE,\n\t\t\t\t&txq->descs_dma, GFP_KERNEL);\n\tif (!txq->descs)\n\t\treturn -ENOMEM;\n\n\ttxq->last_desc = txq->size - 1;\n\n\t \n\tthread = mvpp2_cpu_to_thread(port->priv, get_cpu());\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_NUM_REG, txq->id);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_DESC_ADDR_REG,\n\t\t\t   txq->descs_dma);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_DESC_SIZE_REG,\n\t\t\t   txq->size & MVPP2_TXQ_DESC_SIZE_MASK);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_INDEX_REG, 0);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_RSVD_CLR_REG,\n\t\t\t   txq->id << MVPP2_TXQ_RSVD_CLR_OFFSET);\n\tval = mvpp2_thread_read(port->priv, thread, MVPP2_TXQ_PENDING_REG);\n\tval &= ~MVPP2_TXQ_PENDING_MASK;\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_PENDING_REG, val);\n\n\t \n\tdesc_per_txq = 16;\n\tdesc = (port->id * MVPP2_MAX_TXQ * desc_per_txq) +\n\t       (txq->log_id * desc_per_txq);\n\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_PREF_BUF_REG,\n\t\t\t   MVPP2_PREF_BUF_PTR(desc) | MVPP2_PREF_BUF_SIZE_16 |\n\t\t\t   MVPP2_PREF_BUF_THRESH(desc_per_txq / 2));\n\tput_cpu();\n\n\t \n\ttx_port_num = mvpp2_egress_port(port);\n\tmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);\n\n\tval = mvpp2_read(port->priv, MVPP2_TXQ_SCHED_REFILL_REG(txq->log_id));\n\tval &= ~MVPP2_TXQ_REFILL_PERIOD_ALL_MASK;\n\tval |= MVPP2_TXQ_REFILL_PERIOD_MASK(1);\n\tval |= MVPP2_TXQ_REFILL_TOKENS_ALL_MASK;\n\tmvpp2_write(port->priv, MVPP2_TXQ_SCHED_REFILL_REG(txq->log_id), val);\n\n\tval = MVPP2_TXQ_TOKEN_SIZE_MAX;\n\tmvpp2_write(port->priv, MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq->log_id),\n\t\t    val);\n\n\tfor (thread = 0; thread < port->priv->nthreads; thread++) {\n\t\ttxq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\t\ttxq_pcpu->size = txq->size;\n\t\ttxq_pcpu->buffs = kmalloc_array(txq_pcpu->size,\n\t\t\t\t\t\tsizeof(*txq_pcpu->buffs),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!txq_pcpu->buffs)\n\t\t\treturn -ENOMEM;\n\n\t\ttxq_pcpu->count = 0;\n\t\ttxq_pcpu->reserved_num = 0;\n\t\ttxq_pcpu->txq_put_index = 0;\n\t\ttxq_pcpu->txq_get_index = 0;\n\t\ttxq_pcpu->tso_headers = NULL;\n\n\t\ttxq_pcpu->stop_threshold = txq->size - MVPP2_MAX_SKB_DESCS;\n\t\ttxq_pcpu->wake_threshold = txq_pcpu->stop_threshold / 2;\n\n\t\ttxq_pcpu->tso_headers =\n\t\t\tdma_alloc_coherent(port->dev->dev.parent,\n\t\t\t\t\t   txq_pcpu->size * TSO_HEADER_SIZE,\n\t\t\t\t\t   &txq_pcpu->tso_headers_dma,\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!txq_pcpu->tso_headers)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void mvpp2_txq_deinit(struct mvpp2_port *port,\n\t\t\t     struct mvpp2_tx_queue *txq)\n{\n\tstruct mvpp2_txq_pcpu *txq_pcpu;\n\tunsigned int thread;\n\n\tfor (thread = 0; thread < port->priv->nthreads; thread++) {\n\t\ttxq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\t\tkfree(txq_pcpu->buffs);\n\n\t\tif (txq_pcpu->tso_headers)\n\t\t\tdma_free_coherent(port->dev->dev.parent,\n\t\t\t\t\t  txq_pcpu->size * TSO_HEADER_SIZE,\n\t\t\t\t\t  txq_pcpu->tso_headers,\n\t\t\t\t\t  txq_pcpu->tso_headers_dma);\n\n\t\ttxq_pcpu->tso_headers = NULL;\n\t}\n\n\tif (txq->descs)\n\t\tdma_free_coherent(port->dev->dev.parent,\n\t\t\t\t  txq->size * MVPP2_DESC_ALIGNED_SIZE,\n\t\t\t\t  txq->descs, txq->descs_dma);\n\n\ttxq->descs             = NULL;\n\ttxq->last_desc         = 0;\n\ttxq->next_desc_to_proc = 0;\n\ttxq->descs_dma         = 0;\n\n\t \n\tmvpp2_write(port->priv, MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(txq->log_id), 0);\n\n\t \n\tthread = mvpp2_cpu_to_thread(port->priv, get_cpu());\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_NUM_REG, txq->id);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_DESC_ADDR_REG, 0);\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_DESC_SIZE_REG, 0);\n\tput_cpu();\n}\n\n \nstatic void mvpp2_txq_clean(struct mvpp2_port *port, struct mvpp2_tx_queue *txq)\n{\n\tstruct mvpp2_txq_pcpu *txq_pcpu;\n\tint delay, pending;\n\tunsigned int thread = mvpp2_cpu_to_thread(port->priv, get_cpu());\n\tu32 val;\n\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_NUM_REG, txq->id);\n\tval = mvpp2_thread_read(port->priv, thread, MVPP2_TXQ_PREF_BUF_REG);\n\tval |= MVPP2_TXQ_DRAIN_EN_MASK;\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_PREF_BUF_REG, val);\n\n\t \n\tdelay = 0;\n\tdo {\n\t\tif (delay >= MVPP2_TX_PENDING_TIMEOUT_MSEC) {\n\t\t\tnetdev_warn(port->dev,\n\t\t\t\t    \"port %d: cleaning queue %d timed out\\n\",\n\t\t\t\t    port->id, txq->log_id);\n\t\t\tbreak;\n\t\t}\n\t\tmdelay(1);\n\t\tdelay++;\n\n\t\tpending = mvpp2_thread_read(port->priv, thread,\n\t\t\t\t\t    MVPP2_TXQ_PENDING_REG);\n\t\tpending &= MVPP2_TXQ_PENDING_MASK;\n\t} while (pending);\n\n\tval &= ~MVPP2_TXQ_DRAIN_EN_MASK;\n\tmvpp2_thread_write(port->priv, thread, MVPP2_TXQ_PREF_BUF_REG, val);\n\tput_cpu();\n\n\tfor (thread = 0; thread < port->priv->nthreads; thread++) {\n\t\ttxq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\n\t\t \n\t\tmvpp2_txq_bufs_free(port, txq, txq_pcpu, txq_pcpu->count);\n\n\t\t \n\t\ttxq_pcpu->count = 0;\n\t\ttxq_pcpu->txq_put_index = 0;\n\t\ttxq_pcpu->txq_get_index = 0;\n\t}\n}\n\n \nstatic void mvpp2_cleanup_txqs(struct mvpp2_port *port)\n{\n\tstruct mvpp2_tx_queue *txq;\n\tint queue;\n\tu32 val;\n\n\tval = mvpp2_read(port->priv, MVPP2_TX_PORT_FLUSH_REG);\n\n\t \n\tval |= MVPP2_TX_PORT_FLUSH_MASK(port->id);\n\tmvpp2_write(port->priv, MVPP2_TX_PORT_FLUSH_REG, val);\n\n\tfor (queue = 0; queue < port->ntxqs; queue++) {\n\t\ttxq = port->txqs[queue];\n\t\tmvpp2_txq_clean(port, txq);\n\t\tmvpp2_txq_deinit(port, txq);\n\t}\n\n\ton_each_cpu(mvpp2_txq_sent_counter_clear, port, 1);\n\n\tval &= ~MVPP2_TX_PORT_FLUSH_MASK(port->id);\n\tmvpp2_write(port->priv, MVPP2_TX_PORT_FLUSH_REG, val);\n}\n\n \nstatic void mvpp2_cleanup_rxqs(struct mvpp2_port *port)\n{\n\tint queue;\n\n\tfor (queue = 0; queue < port->nrxqs; queue++)\n\t\tmvpp2_rxq_deinit(port, port->rxqs[queue]);\n\n\tif (port->tx_fc)\n\t\tmvpp2_rxq_disable_fc(port);\n}\n\n \nstatic int mvpp2_setup_rxqs(struct mvpp2_port *port)\n{\n\tint queue, err;\n\n\tfor (queue = 0; queue < port->nrxqs; queue++) {\n\t\terr = mvpp2_rxq_init(port, port->rxqs[queue]);\n\t\tif (err)\n\t\t\tgoto err_cleanup;\n\t}\n\n\tif (port->tx_fc)\n\t\tmvpp2_rxq_enable_fc(port);\n\n\treturn 0;\n\nerr_cleanup:\n\tmvpp2_cleanup_rxqs(port);\n\treturn err;\n}\n\n \nstatic int mvpp2_setup_txqs(struct mvpp2_port *port)\n{\n\tstruct mvpp2_tx_queue *txq;\n\tint queue, err;\n\n\tfor (queue = 0; queue < port->ntxqs; queue++) {\n\t\ttxq = port->txqs[queue];\n\t\terr = mvpp2_txq_init(port, txq);\n\t\tif (err)\n\t\t\tgoto err_cleanup;\n\n\t\t \n\t\tif (queue < num_possible_cpus())\n\t\t\tnetif_set_xps_queue(port->dev, cpumask_of(queue), queue);\n\t}\n\n\tif (port->has_tx_irqs) {\n\t\tmvpp2_tx_time_coal_set(port);\n\t\tfor (queue = 0; queue < port->ntxqs; queue++) {\n\t\t\ttxq = port->txqs[queue];\n\t\t\tmvpp2_tx_pkts_coal_set(port, txq);\n\t\t}\n\t}\n\n\ton_each_cpu(mvpp2_txq_sent_counter_clear, port, 1);\n\treturn 0;\n\nerr_cleanup:\n\tmvpp2_cleanup_txqs(port);\n\treturn err;\n}\n\n \nstatic irqreturn_t mvpp2_isr(int irq, void *dev_id)\n{\n\tstruct mvpp2_queue_vector *qv = dev_id;\n\n\tmvpp2_qvec_interrupt_disable(qv);\n\n\tnapi_schedule(&qv->napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void mvpp2_isr_handle_ptp_queue(struct mvpp2_port *port, int nq)\n{\n\tstruct skb_shared_hwtstamps shhwtstamps;\n\tstruct mvpp2_hwtstamp_queue *queue;\n\tstruct sk_buff *skb;\n\tvoid __iomem *ptp_q;\n\tunsigned int id;\n\tu32 r0, r1, r2;\n\n\tptp_q = port->priv->iface_base + MVPP22_PTP_BASE(port->gop_id);\n\tif (nq)\n\t\tptp_q += MVPP22_PTP_TX_Q1_R0 - MVPP22_PTP_TX_Q0_R0;\n\n\tqueue = &port->tx_hwtstamp_queue[nq];\n\n\twhile (1) {\n\t\tr0 = readl_relaxed(ptp_q + MVPP22_PTP_TX_Q0_R0) & 0xffff;\n\t\tif (!r0)\n\t\t\tbreak;\n\n\t\tr1 = readl_relaxed(ptp_q + MVPP22_PTP_TX_Q0_R1) & 0xffff;\n\t\tr2 = readl_relaxed(ptp_q + MVPP22_PTP_TX_Q0_R2) & 0xffff;\n\n\t\tid = (r0 >> 1) & 31;\n\n\t\tskb = queue->skb[id];\n\t\tqueue->skb[id] = NULL;\n\t\tif (skb) {\n\t\t\tu32 ts = r2 << 19 | r1 << 3 | r0 >> 13;\n\n\t\t\tmvpp22_tai_tstamp(port->priv->tai, ts, &shhwtstamps);\n\t\t\tskb_tstamp_tx(skb, &shhwtstamps);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t}\n\t}\n}\n\nstatic void mvpp2_isr_handle_ptp(struct mvpp2_port *port)\n{\n\tvoid __iomem *ptp;\n\tu32 val;\n\n\tptp = port->priv->iface_base + MVPP22_PTP_BASE(port->gop_id);\n\tval = readl(ptp + MVPP22_PTP_INT_CAUSE);\n\tif (val & MVPP22_PTP_INT_CAUSE_QUEUE0)\n\t\tmvpp2_isr_handle_ptp_queue(port, 0);\n\tif (val & MVPP22_PTP_INT_CAUSE_QUEUE1)\n\t\tmvpp2_isr_handle_ptp_queue(port, 1);\n}\n\nstatic void mvpp2_isr_handle_link(struct mvpp2_port *port, bool link)\n{\n\tstruct net_device *dev = port->dev;\n\n\tif (port->phylink) {\n\t\tphylink_mac_change(port->phylink, link);\n\t\treturn;\n\t}\n\n\tif (!netif_running(dev))\n\t\treturn;\n\n\tif (link) {\n\t\tmvpp2_interrupts_enable(port);\n\n\t\tmvpp2_egress_enable(port);\n\t\tmvpp2_ingress_enable(port);\n\t\tnetif_carrier_on(dev);\n\t\tnetif_tx_wake_all_queues(dev);\n\t} else {\n\t\tnetif_tx_stop_all_queues(dev);\n\t\tnetif_carrier_off(dev);\n\t\tmvpp2_ingress_disable(port);\n\t\tmvpp2_egress_disable(port);\n\n\t\tmvpp2_interrupts_disable(port);\n\t}\n}\n\nstatic void mvpp2_isr_handle_xlg(struct mvpp2_port *port)\n{\n\tbool link;\n\tu32 val;\n\n\tval = readl(port->base + MVPP22_XLG_INT_STAT);\n\tif (val & MVPP22_XLG_INT_STAT_LINK) {\n\t\tval = readl(port->base + MVPP22_XLG_STATUS);\n\t\tlink = (val & MVPP22_XLG_STATUS_LINK_UP);\n\t\tmvpp2_isr_handle_link(port, link);\n\t}\n}\n\nstatic void mvpp2_isr_handle_gmac_internal(struct mvpp2_port *port)\n{\n\tbool link;\n\tu32 val;\n\n\tif (phy_interface_mode_is_rgmii(port->phy_interface) ||\n\t    phy_interface_mode_is_8023z(port->phy_interface) ||\n\t    port->phy_interface == PHY_INTERFACE_MODE_SGMII) {\n\t\tval = readl(port->base + MVPP22_GMAC_INT_STAT);\n\t\tif (val & MVPP22_GMAC_INT_STAT_LINK) {\n\t\t\tval = readl(port->base + MVPP2_GMAC_STATUS0);\n\t\t\tlink = (val & MVPP2_GMAC_STATUS0_LINK_UP);\n\t\t\tmvpp2_isr_handle_link(port, link);\n\t\t}\n\t}\n}\n\n \nstatic irqreturn_t mvpp2_port_isr(int irq, void *dev_id)\n{\n\tstruct mvpp2_port *port = (struct mvpp2_port *)dev_id;\n\tu32 val;\n\n\tmvpp22_gop_mask_irq(port);\n\n\tif (mvpp2_port_supports_xlg(port) &&\n\t    mvpp2_is_xlg(port->phy_interface)) {\n\t\t \n\t\tval = readl(port->base + MVPP22_XLG_EXT_INT_STAT);\n\t\tif (val & MVPP22_XLG_EXT_INT_STAT_XLG)\n\t\t\tmvpp2_isr_handle_xlg(port);\n\t\tif (val & MVPP22_XLG_EXT_INT_STAT_PTP)\n\t\t\tmvpp2_isr_handle_ptp(port);\n\t} else {\n\t\t \n\t\tval = readl(port->base + MVPP22_GMAC_INT_SUM_STAT);\n\t\tif (val & MVPP22_GMAC_INT_SUM_STAT_INTERNAL)\n\t\t\tmvpp2_isr_handle_gmac_internal(port);\n\t\tif (val & MVPP22_GMAC_INT_SUM_STAT_PTP)\n\t\t\tmvpp2_isr_handle_ptp(port);\n\t}\n\n\tmvpp22_gop_unmask_irq(port);\n\treturn IRQ_HANDLED;\n}\n\nstatic enum hrtimer_restart mvpp2_hr_timer_cb(struct hrtimer *timer)\n{\n\tstruct net_device *dev;\n\tstruct mvpp2_port *port;\n\tstruct mvpp2_port_pcpu *port_pcpu;\n\tunsigned int tx_todo, cause;\n\n\tport_pcpu = container_of(timer, struct mvpp2_port_pcpu, tx_done_timer);\n\tdev = port_pcpu->dev;\n\n\tif (!netif_running(dev))\n\t\treturn HRTIMER_NORESTART;\n\n\tport_pcpu->timer_scheduled = false;\n\tport = netdev_priv(dev);\n\n\t \n\tcause = (1 << port->ntxqs) - 1;\n\ttx_todo = mvpp2_tx_done(port, cause,\n\t\t\t\tmvpp2_cpu_to_thread(port->priv, smp_processor_id()));\n\n\t \n\tif (tx_todo && !port_pcpu->timer_scheduled) {\n\t\tport_pcpu->timer_scheduled = true;\n\t\thrtimer_forward_now(&port_pcpu->tx_done_timer,\n\t\t\t\t    MVPP2_TXDONE_HRTIMER_PERIOD_NS);\n\n\t\treturn HRTIMER_RESTART;\n\t}\n\treturn HRTIMER_NORESTART;\n}\n\n \n\n \nstatic void mvpp2_rx_error(struct mvpp2_port *port,\n\t\t\t   struct mvpp2_rx_desc *rx_desc)\n{\n\tu32 status = mvpp2_rxdesc_status_get(port, rx_desc);\n\tsize_t sz = mvpp2_rxdesc_size_get(port, rx_desc);\n\tchar *err_str = NULL;\n\n\tswitch (status & MVPP2_RXD_ERR_CODE_MASK) {\n\tcase MVPP2_RXD_ERR_CRC:\n\t\terr_str = \"crc\";\n\t\tbreak;\n\tcase MVPP2_RXD_ERR_OVERRUN:\n\t\terr_str = \"overrun\";\n\t\tbreak;\n\tcase MVPP2_RXD_ERR_RESOURCE:\n\t\terr_str = \"resource\";\n\t\tbreak;\n\t}\n\tif (err_str && net_ratelimit())\n\t\tnetdev_err(port->dev,\n\t\t\t   \"bad rx status %08x (%s error), size=%zu\\n\",\n\t\t\t   status, err_str, sz);\n}\n\n \nstatic int mvpp2_rx_csum(struct mvpp2_port *port, u32 status)\n{\n\tif (((status & MVPP2_RXD_L3_IP4) &&\n\t     !(status & MVPP2_RXD_IP4_HEADER_ERR)) ||\n\t    (status & MVPP2_RXD_L3_IP6))\n\t\tif (((status & MVPP2_RXD_L4_UDP) ||\n\t\t     (status & MVPP2_RXD_L4_TCP)) &&\n\t\t     (status & MVPP2_RXD_L4_CSUM_OK))\n\t\t\treturn CHECKSUM_UNNECESSARY;\n\n\treturn CHECKSUM_NONE;\n}\n\n \nstatic int mvpp2_rx_refill(struct mvpp2_port *port,\n\t\t\t   struct mvpp2_bm_pool *bm_pool,\n\t\t\t   struct page_pool *page_pool, int pool)\n{\n\tdma_addr_t dma_addr;\n\tphys_addr_t phys_addr;\n\tvoid *buf;\n\n\tbuf = mvpp2_buf_alloc(port, bm_pool, page_pool,\n\t\t\t      &dma_addr, &phys_addr, GFP_ATOMIC);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tmvpp2_bm_pool_put(port, pool, dma_addr, phys_addr);\n\n\treturn 0;\n}\n\n \nstatic u32 mvpp2_skb_tx_csum(struct mvpp2_port *port, struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tint ip_hdr_len = 0;\n\t\tu8 l4_proto;\n\t\t__be16 l3_proto = vlan_get_protocol(skb);\n\n\t\tif (l3_proto == htons(ETH_P_IP)) {\n\t\t\tstruct iphdr *ip4h = ip_hdr(skb);\n\n\t\t\t \n\t\t\tip_hdr_len = ip4h->ihl;\n\t\t\tl4_proto = ip4h->protocol;\n\t\t} else if (l3_proto == htons(ETH_P_IPV6)) {\n\t\t\tstruct ipv6hdr *ip6h = ipv6_hdr(skb);\n\n\t\t\t \n\t\t\tif (skb_network_header_len(skb) > 0)\n\t\t\t\tip_hdr_len = (skb_network_header_len(skb) >> 2);\n\t\t\tl4_proto = ip6h->nexthdr;\n\t\t} else {\n\t\t\treturn MVPP2_TXD_L4_CSUM_NOT;\n\t\t}\n\n\t\treturn mvpp2_txq_desc_csum(skb_network_offset(skb),\n\t\t\t\t\t   l3_proto, ip_hdr_len, l4_proto);\n\t}\n\n\treturn MVPP2_TXD_L4_CSUM_NOT | MVPP2_TXD_IP_CSUM_DISABLE;\n}\n\nstatic void mvpp2_xdp_finish_tx(struct mvpp2_port *port, u16 txq_id, int nxmit, int nxmit_byte)\n{\n\tunsigned int thread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());\n\tstruct mvpp2_tx_queue *aggr_txq;\n\tstruct mvpp2_txq_pcpu *txq_pcpu;\n\tstruct mvpp2_tx_queue *txq;\n\tstruct netdev_queue *nq;\n\n\ttxq = port->txqs[txq_id];\n\ttxq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\tnq = netdev_get_tx_queue(port->dev, txq_id);\n\taggr_txq = &port->priv->aggr_txqs[thread];\n\n\ttxq_pcpu->reserved_num -= nxmit;\n\ttxq_pcpu->count += nxmit;\n\taggr_txq->count += nxmit;\n\n\t \n\twmb();\n\tmvpp2_aggr_txq_pend_desc_add(port, nxmit);\n\n\tif (txq_pcpu->count >= txq_pcpu->stop_threshold)\n\t\tnetif_tx_stop_queue(nq);\n\n\t \n\tif (!port->has_tx_irqs && txq_pcpu->count >= txq->done_pkts_coal)\n\t\tmvpp2_txq_done(port, txq, txq_pcpu);\n}\n\nstatic int\nmvpp2_xdp_submit_frame(struct mvpp2_port *port, u16 txq_id,\n\t\t       struct xdp_frame *xdpf, bool dma_map)\n{\n\tunsigned int thread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());\n\tu32 tx_cmd = MVPP2_TXD_L4_CSUM_NOT | MVPP2_TXD_IP_CSUM_DISABLE |\n\t\t     MVPP2_TXD_F_DESC | MVPP2_TXD_L_DESC;\n\tenum mvpp2_tx_buf_type buf_type;\n\tstruct mvpp2_txq_pcpu *txq_pcpu;\n\tstruct mvpp2_tx_queue *aggr_txq;\n\tstruct mvpp2_tx_desc *tx_desc;\n\tstruct mvpp2_tx_queue *txq;\n\tint ret = MVPP2_XDP_TX;\n\tdma_addr_t dma_addr;\n\n\ttxq = port->txqs[txq_id];\n\ttxq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\taggr_txq = &port->priv->aggr_txqs[thread];\n\n\t \n\tif (mvpp2_aggr_desc_num_check(port, aggr_txq, 1) ||\n\t    mvpp2_txq_reserved_desc_num_proc(port, txq, txq_pcpu, 1)) {\n\t\tret = MVPP2_XDP_DROPPED;\n\t\tgoto out;\n\t}\n\n\t \n\ttx_desc = mvpp2_txq_next_desc_get(aggr_txq);\n\tmvpp2_txdesc_txq_set(port, tx_desc, txq->id);\n\tmvpp2_txdesc_size_set(port, tx_desc, xdpf->len);\n\n\tif (dma_map) {\n\t\t \n\t\tdma_addr = dma_map_single(port->dev->dev.parent, xdpf->data,\n\t\t\t\t\t  xdpf->len, DMA_TO_DEVICE);\n\n\t\tif (unlikely(dma_mapping_error(port->dev->dev.parent, dma_addr))) {\n\t\t\tmvpp2_txq_desc_put(txq);\n\t\t\tret = MVPP2_XDP_DROPPED;\n\t\t\tgoto out;\n\t\t}\n\n\t\tbuf_type = MVPP2_TYPE_XDP_NDO;\n\t} else {\n\t\t \n\t\tstruct page *page = virt_to_page(xdpf->data);\n\n\t\tdma_addr = page_pool_get_dma_addr(page) +\n\t\t\t   sizeof(*xdpf) + xdpf->headroom;\n\t\tdma_sync_single_for_device(port->dev->dev.parent, dma_addr,\n\t\t\t\t\t   xdpf->len, DMA_BIDIRECTIONAL);\n\n\t\tbuf_type = MVPP2_TYPE_XDP_TX;\n\t}\n\n\tmvpp2_txdesc_dma_addr_set(port, tx_desc, dma_addr);\n\n\tmvpp2_txdesc_cmd_set(port, tx_desc, tx_cmd);\n\tmvpp2_txq_inc_put(port, txq_pcpu, xdpf, tx_desc, buf_type);\n\nout:\n\treturn ret;\n}\n\nstatic int\nmvpp2_xdp_xmit_back(struct mvpp2_port *port, struct xdp_buff *xdp)\n{\n\tstruct mvpp2_pcpu_stats *stats = this_cpu_ptr(port->stats);\n\tstruct xdp_frame *xdpf;\n\tu16 txq_id;\n\tint ret;\n\n\txdpf = xdp_convert_buff_to_frame(xdp);\n\tif (unlikely(!xdpf))\n\t\treturn MVPP2_XDP_DROPPED;\n\n\t \n\ttxq_id = mvpp2_cpu_to_thread(port->priv, smp_processor_id()) + (port->ntxqs / 2);\n\n\tret = mvpp2_xdp_submit_frame(port, txq_id, xdpf, false);\n\tif (ret == MVPP2_XDP_TX) {\n\t\tu64_stats_update_begin(&stats->syncp);\n\t\tstats->tx_bytes += xdpf->len;\n\t\tstats->tx_packets++;\n\t\tstats->xdp_tx++;\n\t\tu64_stats_update_end(&stats->syncp);\n\n\t\tmvpp2_xdp_finish_tx(port, txq_id, 1, xdpf->len);\n\t} else {\n\t\tu64_stats_update_begin(&stats->syncp);\n\t\tstats->xdp_tx_err++;\n\t\tu64_stats_update_end(&stats->syncp);\n\t}\n\n\treturn ret;\n}\n\nstatic int\nmvpp2_xdp_xmit(struct net_device *dev, int num_frame,\n\t       struct xdp_frame **frames, u32 flags)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint i, nxmit_byte = 0, nxmit = 0;\n\tstruct mvpp2_pcpu_stats *stats;\n\tu16 txq_id;\n\tu32 ret;\n\n\tif (unlikely(test_bit(0, &port->state)))\n\t\treturn -ENETDOWN;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\t \n\ttxq_id = mvpp2_cpu_to_thread(port->priv, smp_processor_id()) + (port->ntxqs / 2);\n\n\tfor (i = 0; i < num_frame; i++) {\n\t\tret = mvpp2_xdp_submit_frame(port, txq_id, frames[i], true);\n\t\tif (ret != MVPP2_XDP_TX)\n\t\t\tbreak;\n\n\t\tnxmit_byte += frames[i]->len;\n\t\tnxmit++;\n\t}\n\n\tif (likely(nxmit > 0))\n\t\tmvpp2_xdp_finish_tx(port, txq_id, nxmit, nxmit_byte);\n\n\tstats = this_cpu_ptr(port->stats);\n\tu64_stats_update_begin(&stats->syncp);\n\tstats->tx_bytes += nxmit_byte;\n\tstats->tx_packets += nxmit;\n\tstats->xdp_xmit += nxmit;\n\tstats->xdp_xmit_err += num_frame - nxmit;\n\tu64_stats_update_end(&stats->syncp);\n\n\treturn nxmit;\n}\n\nstatic int\nmvpp2_run_xdp(struct mvpp2_port *port, struct bpf_prog *prog,\n\t      struct xdp_buff *xdp, struct page_pool *pp,\n\t      struct mvpp2_pcpu_stats *stats)\n{\n\tunsigned int len, sync, err;\n\tstruct page *page;\n\tu32 ret, act;\n\n\tlen = xdp->data_end - xdp->data_hard_start - MVPP2_SKB_HEADROOM;\n\tact = bpf_prog_run_xdp(prog, xdp);\n\n\t \n\tsync = xdp->data_end - xdp->data_hard_start - MVPP2_SKB_HEADROOM;\n\tsync = max(sync, len);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tstats->xdp_pass++;\n\t\tret = MVPP2_XDP_PASS;\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\terr = xdp_do_redirect(port->dev, xdp, prog);\n\t\tif (unlikely(err)) {\n\t\t\tret = MVPP2_XDP_DROPPED;\n\t\t\tpage = virt_to_head_page(xdp->data);\n\t\t\tpage_pool_put_page(pp, page, sync, true);\n\t\t} else {\n\t\t\tret = MVPP2_XDP_REDIR;\n\t\t\tstats->xdp_redirect++;\n\t\t}\n\t\tbreak;\n\tcase XDP_TX:\n\t\tret = mvpp2_xdp_xmit_back(port, xdp);\n\t\tif (ret != MVPP2_XDP_TX) {\n\t\t\tpage = virt_to_head_page(xdp->data);\n\t\t\tpage_pool_put_page(pp, page, sync, true);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(port->dev, prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(port->dev, prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tpage = virt_to_head_page(xdp->data);\n\t\tpage_pool_put_page(pp, page, sync, true);\n\t\tret = MVPP2_XDP_DROPPED;\n\t\tstats->xdp_drop++;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void mvpp2_buff_hdr_pool_put(struct mvpp2_port *port, struct mvpp2_rx_desc *rx_desc,\n\t\t\t\t    int pool, u32 rx_status)\n{\n\tphys_addr_t phys_addr, phys_addr_next;\n\tdma_addr_t dma_addr, dma_addr_next;\n\tstruct mvpp2_buff_hdr *buff_hdr;\n\n\tphys_addr = mvpp2_rxdesc_dma_addr_get(port, rx_desc);\n\tdma_addr = mvpp2_rxdesc_cookie_get(port, rx_desc);\n\n\tdo {\n\t\tbuff_hdr = (struct mvpp2_buff_hdr *)phys_to_virt(phys_addr);\n\n\t\tphys_addr_next = le32_to_cpu(buff_hdr->next_phys_addr);\n\t\tdma_addr_next = le32_to_cpu(buff_hdr->next_dma_addr);\n\n\t\tif (port->priv->hw_version >= MVPP22) {\n\t\t\tphys_addr_next |= ((u64)buff_hdr->next_phys_addr_high << 32);\n\t\t\tdma_addr_next |= ((u64)buff_hdr->next_dma_addr_high << 32);\n\t\t}\n\n\t\tmvpp2_bm_pool_put(port, pool, dma_addr, phys_addr);\n\n\t\tphys_addr = phys_addr_next;\n\t\tdma_addr = dma_addr_next;\n\n\t} while (!MVPP2_B_HDR_INFO_IS_LAST(le16_to_cpu(buff_hdr->info)));\n}\n\n \nstatic int mvpp2_rx(struct mvpp2_port *port, struct napi_struct *napi,\n\t\t    int rx_todo, struct mvpp2_rx_queue *rxq)\n{\n\tstruct net_device *dev = port->dev;\n\tstruct mvpp2_pcpu_stats ps = {};\n\tenum dma_data_direction dma_dir;\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_buff xdp;\n\tint rx_received;\n\tint rx_done = 0;\n\tu32 xdp_ret = 0;\n\n\txdp_prog = READ_ONCE(port->xdp_prog);\n\n\t \n\trx_received = mvpp2_rxq_received(port, rxq->id);\n\tif (rx_todo > rx_received)\n\t\trx_todo = rx_received;\n\n\twhile (rx_done < rx_todo) {\n\t\tstruct mvpp2_rx_desc *rx_desc = mvpp2_rxq_next_desc_get(rxq);\n\t\tstruct mvpp2_bm_pool *bm_pool;\n\t\tstruct page_pool *pp = NULL;\n\t\tstruct sk_buff *skb;\n\t\tunsigned int frag_size;\n\t\tdma_addr_t dma_addr;\n\t\tphys_addr_t phys_addr;\n\t\tu32 rx_status, timestamp;\n\t\tint pool, rx_bytes, err, ret;\n\t\tstruct page *page;\n\t\tvoid *data;\n\n\t\tphys_addr = mvpp2_rxdesc_cookie_get(port, rx_desc);\n\t\tdata = (void *)phys_to_virt(phys_addr);\n\t\tpage = virt_to_page(data);\n\t\tprefetch(page);\n\n\t\trx_done++;\n\t\trx_status = mvpp2_rxdesc_status_get(port, rx_desc);\n\t\trx_bytes = mvpp2_rxdesc_size_get(port, rx_desc);\n\t\trx_bytes -= MVPP2_MH_SIZE;\n\t\tdma_addr = mvpp2_rxdesc_dma_addr_get(port, rx_desc);\n\n\t\tpool = (rx_status & MVPP2_RXD_BM_POOL_ID_MASK) >>\n\t\t\tMVPP2_RXD_BM_POOL_ID_OFFS;\n\t\tbm_pool = &port->priv->bm_pools[pool];\n\n\t\tif (port->priv->percpu_pools) {\n\t\t\tpp = port->priv->page_pool[pool];\n\t\t\tdma_dir = page_pool_get_dma_dir(pp);\n\t\t} else {\n\t\t\tdma_dir = DMA_FROM_DEVICE;\n\t\t}\n\n\t\tdma_sync_single_for_cpu(dev->dev.parent, dma_addr,\n\t\t\t\t\trx_bytes + MVPP2_MH_SIZE,\n\t\t\t\t\tdma_dir);\n\n\t\t \n\t\tif (rx_status & MVPP2_RXD_BUF_HDR)\n\t\t\tgoto err_drop_frame;\n\n\t\t \n\t\tif (rx_status & MVPP2_RXD_ERR_SUMMARY)\n\t\t\tgoto err_drop_frame;\n\n\t\t \n\t\tprefetch(data + MVPP2_MH_SIZE + MVPP2_SKB_HEADROOM);\n\n\t\tif (bm_pool->frag_size > PAGE_SIZE)\n\t\t\tfrag_size = 0;\n\t\telse\n\t\t\tfrag_size = bm_pool->frag_size;\n\n\t\tif (xdp_prog) {\n\t\t\tstruct xdp_rxq_info *xdp_rxq;\n\n\t\t\tif (bm_pool->pkt_size == MVPP2_BM_SHORT_PKT_SIZE)\n\t\t\t\txdp_rxq = &rxq->xdp_rxq_short;\n\t\t\telse\n\t\t\t\txdp_rxq = &rxq->xdp_rxq_long;\n\n\t\t\txdp_init_buff(&xdp, PAGE_SIZE, xdp_rxq);\n\t\t\txdp_prepare_buff(&xdp, data,\n\t\t\t\t\t MVPP2_MH_SIZE + MVPP2_SKB_HEADROOM,\n\t\t\t\t\t rx_bytes, false);\n\n\t\t\tret = mvpp2_run_xdp(port, xdp_prog, &xdp, pp, &ps);\n\n\t\t\tif (ret) {\n\t\t\t\txdp_ret |= ret;\n\t\t\t\terr = mvpp2_rx_refill(port, bm_pool, pp, pool);\n\t\t\t\tif (err) {\n\t\t\t\t\tnetdev_err(port->dev, \"failed to refill BM pools\\n\");\n\t\t\t\t\tgoto err_drop_frame;\n\t\t\t\t}\n\n\t\t\t\tps.rx_packets++;\n\t\t\t\tps.rx_bytes += rx_bytes;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tskb = build_skb(data, frag_size);\n\t\tif (!skb) {\n\t\t\tnetdev_warn(port->dev, \"skb build failed\\n\");\n\t\t\tgoto err_drop_frame;\n\t\t}\n\n\t\t \n\t\tif (mvpp22_rx_hwtstamping(port)) {\n\t\t\ttimestamp = le32_to_cpu(rx_desc->pp22.timestamp);\n\t\t\tmvpp22_tai_tstamp(port->priv->tai, timestamp,\n\t\t\t\t\t skb_hwtstamps(skb));\n\t\t}\n\n\t\terr = mvpp2_rx_refill(port, bm_pool, pp, pool);\n\t\tif (err) {\n\t\t\tnetdev_err(port->dev, \"failed to refill BM pools\\n\");\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto err_drop_frame;\n\t\t}\n\n\t\tif (pp)\n\t\t\tskb_mark_for_recycle(skb);\n\t\telse\n\t\t\tdma_unmap_single_attrs(dev->dev.parent, dma_addr,\n\t\t\t\t\t       bm_pool->buf_size, DMA_FROM_DEVICE,\n\t\t\t\t\t       DMA_ATTR_SKIP_CPU_SYNC);\n\n\t\tps.rx_packets++;\n\t\tps.rx_bytes += rx_bytes;\n\n\t\tskb_reserve(skb, MVPP2_MH_SIZE + MVPP2_SKB_HEADROOM);\n\t\tskb_put(skb, rx_bytes);\n\t\tskb->ip_summed = mvpp2_rx_csum(port, rx_status);\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\n\t\tnapi_gro_receive(napi, skb);\n\t\tcontinue;\n\nerr_drop_frame:\n\t\tdev->stats.rx_errors++;\n\t\tmvpp2_rx_error(port, rx_desc);\n\t\t \n\t\tif (rx_status & MVPP2_RXD_BUF_HDR)\n\t\t\tmvpp2_buff_hdr_pool_put(port, rx_desc, pool, rx_status);\n\t\telse\n\t\t\tmvpp2_bm_pool_put(port, pool, dma_addr, phys_addr);\n\t}\n\n\tif (xdp_ret & MVPP2_XDP_REDIR)\n\t\txdp_do_flush_map();\n\n\tif (ps.rx_packets) {\n\t\tstruct mvpp2_pcpu_stats *stats = this_cpu_ptr(port->stats);\n\n\t\tu64_stats_update_begin(&stats->syncp);\n\t\tstats->rx_packets += ps.rx_packets;\n\t\tstats->rx_bytes   += ps.rx_bytes;\n\t\t \n\t\tstats->xdp_redirect += ps.xdp_redirect;\n\t\tstats->xdp_pass += ps.xdp_pass;\n\t\tstats->xdp_drop += ps.xdp_drop;\n\t\tu64_stats_update_end(&stats->syncp);\n\t}\n\n\t \n\twmb();\n\tmvpp2_rxq_status_update(port, rxq->id, rx_done, rx_done);\n\n\treturn rx_todo;\n}\n\nstatic inline void\ntx_desc_unmap_put(struct mvpp2_port *port, struct mvpp2_tx_queue *txq,\n\t\t  struct mvpp2_tx_desc *desc)\n{\n\tunsigned int thread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());\n\tstruct mvpp2_txq_pcpu *txq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\n\tdma_addr_t buf_dma_addr =\n\t\tmvpp2_txdesc_dma_addr_get(port, desc);\n\tsize_t buf_sz =\n\t\tmvpp2_txdesc_size_get(port, desc);\n\tif (!IS_TSO_HEADER(txq_pcpu, buf_dma_addr))\n\t\tdma_unmap_single(port->dev->dev.parent, buf_dma_addr,\n\t\t\t\t buf_sz, DMA_TO_DEVICE);\n\tmvpp2_txq_desc_put(txq);\n}\n\nstatic void mvpp2_txdesc_clear_ptp(struct mvpp2_port *port,\n\t\t\t\t   struct mvpp2_tx_desc *desc)\n{\n\t \n\tif (port->priv->hw_version >= MVPP22)\n\t\tdesc->pp22.ptp_descriptor &=\n\t\t\tcpu_to_le32(~MVPP22_PTP_DESC_MASK_LOW);\n}\n\nstatic bool mvpp2_tx_hw_tstamp(struct mvpp2_port *port,\n\t\t\t       struct mvpp2_tx_desc *tx_desc,\n\t\t\t       struct sk_buff *skb)\n{\n\tstruct mvpp2_hwtstamp_queue *queue;\n\tunsigned int mtype, type, i;\n\tstruct ptp_header *hdr;\n\tu64 ptpdesc;\n\n\tif (port->priv->hw_version == MVPP21 ||\n\t    port->tx_hwtstamp_type == HWTSTAMP_TX_OFF)\n\t\treturn false;\n\n\ttype = ptp_classify_raw(skb);\n\tif (!type)\n\t\treturn false;\n\n\thdr = ptp_parse_header(skb, type);\n\tif (!hdr)\n\t\treturn false;\n\n\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\n\tptpdesc = MVPP22_PTP_MACTIMESTAMPINGEN |\n\t\t  MVPP22_PTP_ACTION_CAPTURE;\n\tqueue = &port->tx_hwtstamp_queue[0];\n\n\tswitch (type & PTP_CLASS_VMASK) {\n\tcase PTP_CLASS_V1:\n\t\tptpdesc |= MVPP22_PTP_PACKETFORMAT(MVPP22_PTP_PKT_FMT_PTPV1);\n\t\tbreak;\n\n\tcase PTP_CLASS_V2:\n\t\tptpdesc |= MVPP22_PTP_PACKETFORMAT(MVPP22_PTP_PKT_FMT_PTPV2);\n\t\tmtype = hdr->tsmt & 15;\n\t\t \n\t\tif (mtype == 0) {\n\t\t\tptpdesc |= MVPP22_PTP_TIMESTAMPQUEUESELECT;\n\t\t\tqueue = &port->tx_hwtstamp_queue[1];\n\t\t}\n\t\tbreak;\n\t}\n\n\t \n\ti = queue->next;\n\tqueue->next = (i + 1) & 31;\n\tif (queue->skb[i])\n\t\tdev_kfree_skb_any(queue->skb[i]);\n\tqueue->skb[i] = skb_get(skb);\n\n\tptpdesc |= MVPP22_PTP_TIMESTAMPENTRYID(i);\n\n\t \n\ttx_desc->pp22.ptp_descriptor &=\n\t\tcpu_to_le32(~MVPP22_PTP_DESC_MASK_LOW);\n\ttx_desc->pp22.ptp_descriptor |=\n\t\tcpu_to_le32(ptpdesc & MVPP22_PTP_DESC_MASK_LOW);\n\ttx_desc->pp22.buf_dma_addr_ptp &= cpu_to_le64(~0xffffff0000000000ULL);\n\ttx_desc->pp22.buf_dma_addr_ptp |= cpu_to_le64((ptpdesc >> 12) << 40);\n\n\treturn true;\n}\n\n \nstatic int mvpp2_tx_frag_process(struct mvpp2_port *port, struct sk_buff *skb,\n\t\t\t\t struct mvpp2_tx_queue *aggr_txq,\n\t\t\t\t struct mvpp2_tx_queue *txq)\n{\n\tunsigned int thread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());\n\tstruct mvpp2_txq_pcpu *txq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\tstruct mvpp2_tx_desc *tx_desc;\n\tint i;\n\tdma_addr_t buf_dma_addr;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tvoid *addr = skb_frag_address(frag);\n\n\t\ttx_desc = mvpp2_txq_next_desc_get(aggr_txq);\n\t\tmvpp2_txdesc_clear_ptp(port, tx_desc);\n\t\tmvpp2_txdesc_txq_set(port, tx_desc, txq->id);\n\t\tmvpp2_txdesc_size_set(port, tx_desc, skb_frag_size(frag));\n\n\t\tbuf_dma_addr = dma_map_single(port->dev->dev.parent, addr,\n\t\t\t\t\t      skb_frag_size(frag),\n\t\t\t\t\t      DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(port->dev->dev.parent, buf_dma_addr)) {\n\t\t\tmvpp2_txq_desc_put(txq);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tmvpp2_txdesc_dma_addr_set(port, tx_desc, buf_dma_addr);\n\n\t\tif (i == (skb_shinfo(skb)->nr_frags - 1)) {\n\t\t\t \n\t\t\tmvpp2_txdesc_cmd_set(port, tx_desc,\n\t\t\t\t\t     MVPP2_TXD_L_DESC);\n\t\t\tmvpp2_txq_inc_put(port, txq_pcpu, skb, tx_desc, MVPP2_TYPE_SKB);\n\t\t} else {\n\t\t\t \n\t\t\tmvpp2_txdesc_cmd_set(port, tx_desc, 0);\n\t\t\tmvpp2_txq_inc_put(port, txq_pcpu, NULL, tx_desc, MVPP2_TYPE_SKB);\n\t\t}\n\t}\n\n\treturn 0;\ncleanup:\n\t \n\tfor (i = i - 1; i >= 0; i--) {\n\t\ttx_desc = txq->descs + i;\n\t\ttx_desc_unmap_put(port, txq, tx_desc);\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic inline void mvpp2_tso_put_hdr(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev,\n\t\t\t\t     struct mvpp2_tx_queue *txq,\n\t\t\t\t     struct mvpp2_tx_queue *aggr_txq,\n\t\t\t\t     struct mvpp2_txq_pcpu *txq_pcpu,\n\t\t\t\t     int hdr_sz)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tstruct mvpp2_tx_desc *tx_desc = mvpp2_txq_next_desc_get(aggr_txq);\n\tdma_addr_t addr;\n\n\tmvpp2_txdesc_clear_ptp(port, tx_desc);\n\tmvpp2_txdesc_txq_set(port, tx_desc, txq->id);\n\tmvpp2_txdesc_size_set(port, tx_desc, hdr_sz);\n\n\taddr = txq_pcpu->tso_headers_dma +\n\t       txq_pcpu->txq_put_index * TSO_HEADER_SIZE;\n\tmvpp2_txdesc_dma_addr_set(port, tx_desc, addr);\n\n\tmvpp2_txdesc_cmd_set(port, tx_desc, mvpp2_skb_tx_csum(port, skb) |\n\t\t\t\t\t    MVPP2_TXD_F_DESC |\n\t\t\t\t\t    MVPP2_TXD_PADDING_DISABLE);\n\tmvpp2_txq_inc_put(port, txq_pcpu, NULL, tx_desc, MVPP2_TYPE_SKB);\n}\n\nstatic inline int mvpp2_tso_put_data(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev, struct tso_t *tso,\n\t\t\t\t     struct mvpp2_tx_queue *txq,\n\t\t\t\t     struct mvpp2_tx_queue *aggr_txq,\n\t\t\t\t     struct mvpp2_txq_pcpu *txq_pcpu,\n\t\t\t\t     int sz, bool left, bool last)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tstruct mvpp2_tx_desc *tx_desc = mvpp2_txq_next_desc_get(aggr_txq);\n\tdma_addr_t buf_dma_addr;\n\n\tmvpp2_txdesc_clear_ptp(port, tx_desc);\n\tmvpp2_txdesc_txq_set(port, tx_desc, txq->id);\n\tmvpp2_txdesc_size_set(port, tx_desc, sz);\n\n\tbuf_dma_addr = dma_map_single(dev->dev.parent, tso->data, sz,\n\t\t\t\t      DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dev.parent, buf_dma_addr))) {\n\t\tmvpp2_txq_desc_put(txq);\n\t\treturn -ENOMEM;\n\t}\n\n\tmvpp2_txdesc_dma_addr_set(port, tx_desc, buf_dma_addr);\n\n\tif (!left) {\n\t\tmvpp2_txdesc_cmd_set(port, tx_desc, MVPP2_TXD_L_DESC);\n\t\tif (last) {\n\t\t\tmvpp2_txq_inc_put(port, txq_pcpu, skb, tx_desc, MVPP2_TYPE_SKB);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tmvpp2_txdesc_cmd_set(port, tx_desc, 0);\n\t}\n\n\tmvpp2_txq_inc_put(port, txq_pcpu, NULL, tx_desc, MVPP2_TYPE_SKB);\n\treturn 0;\n}\n\nstatic int mvpp2_tx_tso(struct sk_buff *skb, struct net_device *dev,\n\t\t\tstruct mvpp2_tx_queue *txq,\n\t\t\tstruct mvpp2_tx_queue *aggr_txq,\n\t\t\tstruct mvpp2_txq_pcpu *txq_pcpu)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint hdr_sz, i, len, descs = 0;\n\tstruct tso_t tso;\n\n\t \n\tif (mvpp2_aggr_desc_num_check(port, aggr_txq, tso_count_descs(skb)) ||\n\t    mvpp2_txq_reserved_desc_num_proc(port, txq, txq_pcpu,\n\t\t\t\t\t     tso_count_descs(skb)))\n\t\treturn 0;\n\n\thdr_sz = tso_start(skb, &tso);\n\n\tlen = skb->len - hdr_sz;\n\twhile (len > 0) {\n\t\tint left = min_t(int, skb_shinfo(skb)->gso_size, len);\n\t\tchar *hdr = txq_pcpu->tso_headers +\n\t\t\t    txq_pcpu->txq_put_index * TSO_HEADER_SIZE;\n\n\t\tlen -= left;\n\t\tdescs++;\n\n\t\ttso_build_hdr(skb, hdr, &tso, left, len == 0);\n\t\tmvpp2_tso_put_hdr(skb, dev, txq, aggr_txq, txq_pcpu, hdr_sz);\n\n\t\twhile (left > 0) {\n\t\t\tint sz = min_t(int, tso.size, left);\n\t\t\tleft -= sz;\n\t\t\tdescs++;\n\n\t\t\tif (mvpp2_tso_put_data(skb, dev, &tso, txq, aggr_txq,\n\t\t\t\t\t       txq_pcpu, sz, left, len == 0))\n\t\t\t\tgoto release;\n\t\t\ttso_build_data(skb, &tso, sz);\n\t\t}\n\t}\n\n\treturn descs;\n\nrelease:\n\tfor (i = descs - 1; i >= 0; i--) {\n\t\tstruct mvpp2_tx_desc *tx_desc = txq->descs + i;\n\t\ttx_desc_unmap_put(port, txq, tx_desc);\n\t}\n\treturn 0;\n}\n\n \nstatic netdev_tx_t mvpp2_tx(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tstruct mvpp2_tx_queue *txq, *aggr_txq;\n\tstruct mvpp2_txq_pcpu *txq_pcpu;\n\tstruct mvpp2_tx_desc *tx_desc;\n\tdma_addr_t buf_dma_addr;\n\tunsigned long flags = 0;\n\tunsigned int thread;\n\tint frags = 0;\n\tu16 txq_id;\n\tu32 tx_cmd;\n\n\tthread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());\n\n\ttxq_id = skb_get_queue_mapping(skb);\n\ttxq = port->txqs[txq_id];\n\ttxq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\taggr_txq = &port->priv->aggr_txqs[thread];\n\n\tif (test_bit(thread, &port->priv->lock_map))\n\t\tspin_lock_irqsave(&port->tx_lock[thread], flags);\n\n\tif (skb_is_gso(skb)) {\n\t\tfrags = mvpp2_tx_tso(skb, dev, txq, aggr_txq, txq_pcpu);\n\t\tgoto out;\n\t}\n\tfrags = skb_shinfo(skb)->nr_frags + 1;\n\n\t \n\tif (mvpp2_aggr_desc_num_check(port, aggr_txq, frags) ||\n\t    mvpp2_txq_reserved_desc_num_proc(port, txq, txq_pcpu, frags)) {\n\t\tfrags = 0;\n\t\tgoto out;\n\t}\n\n\t \n\ttx_desc = mvpp2_txq_next_desc_get(aggr_txq);\n\tif (!(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) ||\n\t    !mvpp2_tx_hw_tstamp(port, tx_desc, skb))\n\t\tmvpp2_txdesc_clear_ptp(port, tx_desc);\n\tmvpp2_txdesc_txq_set(port, tx_desc, txq->id);\n\tmvpp2_txdesc_size_set(port, tx_desc, skb_headlen(skb));\n\n\tbuf_dma_addr = dma_map_single(dev->dev.parent, skb->data,\n\t\t\t\t      skb_headlen(skb), DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dev.parent, buf_dma_addr))) {\n\t\tmvpp2_txq_desc_put(txq);\n\t\tfrags = 0;\n\t\tgoto out;\n\t}\n\n\tmvpp2_txdesc_dma_addr_set(port, tx_desc, buf_dma_addr);\n\n\ttx_cmd = mvpp2_skb_tx_csum(port, skb);\n\n\tif (frags == 1) {\n\t\t \n\t\ttx_cmd |= MVPP2_TXD_F_DESC | MVPP2_TXD_L_DESC;\n\t\tmvpp2_txdesc_cmd_set(port, tx_desc, tx_cmd);\n\t\tmvpp2_txq_inc_put(port, txq_pcpu, skb, tx_desc, MVPP2_TYPE_SKB);\n\t} else {\n\t\t \n\t\ttx_cmd |= MVPP2_TXD_F_DESC | MVPP2_TXD_PADDING_DISABLE;\n\t\tmvpp2_txdesc_cmd_set(port, tx_desc, tx_cmd);\n\t\tmvpp2_txq_inc_put(port, txq_pcpu, NULL, tx_desc, MVPP2_TYPE_SKB);\n\n\t\t \n\t\tif (mvpp2_tx_frag_process(port, skb, aggr_txq, txq)) {\n\t\t\ttx_desc_unmap_put(port, txq, tx_desc);\n\t\t\tfrags = 0;\n\t\t}\n\t}\n\nout:\n\tif (frags > 0) {\n\t\tstruct mvpp2_pcpu_stats *stats = per_cpu_ptr(port->stats, thread);\n\t\tstruct netdev_queue *nq = netdev_get_tx_queue(dev, txq_id);\n\n\t\ttxq_pcpu->reserved_num -= frags;\n\t\ttxq_pcpu->count += frags;\n\t\taggr_txq->count += frags;\n\n\t\t \n\t\twmb();\n\t\tmvpp2_aggr_txq_pend_desc_add(port, frags);\n\n\t\tif (txq_pcpu->count >= txq_pcpu->stop_threshold)\n\t\t\tnetif_tx_stop_queue(nq);\n\n\t\tu64_stats_update_begin(&stats->syncp);\n\t\tstats->tx_packets++;\n\t\tstats->tx_bytes += skb->len;\n\t\tu64_stats_update_end(&stats->syncp);\n\t} else {\n\t\tdev->stats.tx_dropped++;\n\t\tdev_kfree_skb_any(skb);\n\t}\n\n\t \n\tif (!port->has_tx_irqs && txq_pcpu->count >= txq->done_pkts_coal)\n\t\tmvpp2_txq_done(port, txq, txq_pcpu);\n\n\t \n\tif (!port->has_tx_irqs && txq_pcpu->count <= frags &&\n\t    txq_pcpu->count > 0) {\n\t\tstruct mvpp2_port_pcpu *port_pcpu = per_cpu_ptr(port->pcpu, thread);\n\n\t\tif (!port_pcpu->timer_scheduled) {\n\t\t\tport_pcpu->timer_scheduled = true;\n\t\t\thrtimer_start(&port_pcpu->tx_done_timer,\n\t\t\t\t      MVPP2_TXDONE_HRTIMER_PERIOD_NS,\n\t\t\t\t      HRTIMER_MODE_REL_PINNED_SOFT);\n\t\t}\n\t}\n\n\tif (test_bit(thread, &port->priv->lock_map))\n\t\tspin_unlock_irqrestore(&port->tx_lock[thread], flags);\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic inline void mvpp2_cause_error(struct net_device *dev, int cause)\n{\n\tif (cause & MVPP2_CAUSE_FCS_ERR_MASK)\n\t\tnetdev_err(dev, \"FCS error\\n\");\n\tif (cause & MVPP2_CAUSE_RX_FIFO_OVERRUN_MASK)\n\t\tnetdev_err(dev, \"rx fifo overrun error\\n\");\n\tif (cause & MVPP2_CAUSE_TX_FIFO_UNDERRUN_MASK)\n\t\tnetdev_err(dev, \"tx fifo underrun error\\n\");\n}\n\nstatic int mvpp2_poll(struct napi_struct *napi, int budget)\n{\n\tu32 cause_rx_tx, cause_rx, cause_tx, cause_misc;\n\tint rx_done = 0;\n\tstruct mvpp2_port *port = netdev_priv(napi->dev);\n\tstruct mvpp2_queue_vector *qv;\n\tunsigned int thread = mvpp2_cpu_to_thread(port->priv, smp_processor_id());\n\n\tqv = container_of(napi, struct mvpp2_queue_vector, napi);\n\n\t \n\tcause_rx_tx = mvpp2_thread_read_relaxed(port->priv, qv->sw_thread_id,\n\t\t\t\t\t\tMVPP2_ISR_RX_TX_CAUSE_REG(port->id));\n\n\tcause_misc = cause_rx_tx & MVPP2_CAUSE_MISC_SUM_MASK;\n\tif (cause_misc) {\n\t\tmvpp2_cause_error(port->dev, cause_misc);\n\n\t\t \n\t\tmvpp2_write(port->priv, MVPP2_ISR_MISC_CAUSE_REG, 0);\n\t\tmvpp2_thread_write(port->priv, thread,\n\t\t\t\t   MVPP2_ISR_RX_TX_CAUSE_REG(port->id),\n\t\t\t\t   cause_rx_tx & ~MVPP2_CAUSE_MISC_SUM_MASK);\n\t}\n\n\tif (port->has_tx_irqs) {\n\t\tcause_tx = cause_rx_tx & MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK;\n\t\tif (cause_tx) {\n\t\t\tcause_tx >>= MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_OFFSET;\n\t\t\tmvpp2_tx_done(port, cause_tx, qv->sw_thread_id);\n\t\t}\n\t}\n\n\t \n\tcause_rx = cause_rx_tx &\n\t\t   MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK(port->priv->hw_version);\n\tcause_rx <<= qv->first_rxq;\n\tcause_rx |= qv->pending_cause_rx;\n\twhile (cause_rx && budget > 0) {\n\t\tint count;\n\t\tstruct mvpp2_rx_queue *rxq;\n\n\t\trxq = mvpp2_get_rx_queue(port, cause_rx);\n\t\tif (!rxq)\n\t\t\tbreak;\n\n\t\tcount = mvpp2_rx(port, napi, budget, rxq);\n\t\trx_done += count;\n\t\tbudget -= count;\n\t\tif (budget > 0) {\n\t\t\t \n\t\t\tcause_rx &= ~(1 << rxq->logic_rxq);\n\t\t}\n\t}\n\n\tif (budget > 0) {\n\t\tcause_rx = 0;\n\t\tnapi_complete_done(napi, rx_done);\n\n\t\tmvpp2_qvec_interrupt_enable(qv);\n\t}\n\tqv->pending_cause_rx = cause_rx;\n\treturn rx_done;\n}\n\nstatic void mvpp22_mode_reconfigure(struct mvpp2_port *port,\n\t\t\t\t    phy_interface_t interface)\n{\n\tu32 ctrl3;\n\n\t \n\tmvpp2_mac_reset_assert(port);\n\n\t \n\tmvpp22_pcs_reset_assert(port);\n\n\t \n\tmvpp22_comphy_init(port, interface);\n\n\t \n\tmvpp22_gop_init(port, interface);\n\n\tmvpp22_pcs_reset_deassert(port, interface);\n\n\tif (mvpp2_port_supports_xlg(port)) {\n\t\tctrl3 = readl(port->base + MVPP22_XLG_CTRL3_REG);\n\t\tctrl3 &= ~MVPP22_XLG_CTRL3_MACMODESELECT_MASK;\n\n\t\tif (mvpp2_is_xlg(interface))\n\t\t\tctrl3 |= MVPP22_XLG_CTRL3_MACMODESELECT_10G;\n\t\telse\n\t\t\tctrl3 |= MVPP22_XLG_CTRL3_MACMODESELECT_GMAC;\n\n\t\twritel(ctrl3, port->base + MVPP22_XLG_CTRL3_REG);\n\t}\n\n\tif (mvpp2_port_supports_xlg(port) && mvpp2_is_xlg(interface))\n\t\tmvpp2_xlg_max_rx_size_set(port);\n\telse\n\t\tmvpp2_gmac_max_rx_size_set(port);\n}\n\n \nstatic void mvpp2_start_dev(struct mvpp2_port *port)\n{\n\tint i;\n\n\tmvpp2_txp_max_tx_size_set(port);\n\n\tfor (i = 0; i < port->nqvecs; i++)\n\t\tnapi_enable(&port->qvecs[i].napi);\n\n\t \n\tmvpp2_interrupts_enable(port);\n\n\tif (port->priv->hw_version >= MVPP22)\n\t\tmvpp22_mode_reconfigure(port, port->phy_interface);\n\n\tif (port->phylink) {\n\t\tphylink_start(port->phylink);\n\t} else {\n\t\tmvpp2_acpi_start(port);\n\t}\n\n\tnetif_tx_start_all_queues(port->dev);\n\n\tclear_bit(0, &port->state);\n}\n\n \nstatic void mvpp2_stop_dev(struct mvpp2_port *port)\n{\n\tint i;\n\n\tset_bit(0, &port->state);\n\n\t \n\tmvpp2_interrupts_disable(port);\n\n\tfor (i = 0; i < port->nqvecs; i++)\n\t\tnapi_disable(&port->qvecs[i].napi);\n\n\tif (port->phylink)\n\t\tphylink_stop(port->phylink);\n\tphy_power_off(port->comphy);\n}\n\nstatic int mvpp2_check_ringparam_valid(struct net_device *dev,\n\t\t\t\t       struct ethtool_ringparam *ring)\n{\n\tu16 new_rx_pending = ring->rx_pending;\n\tu16 new_tx_pending = ring->tx_pending;\n\n\tif (ring->rx_pending == 0 || ring->tx_pending == 0)\n\t\treturn -EINVAL;\n\n\tif (ring->rx_pending > MVPP2_MAX_RXD_MAX)\n\t\tnew_rx_pending = MVPP2_MAX_RXD_MAX;\n\telse if (ring->rx_pending < MSS_THRESHOLD_START)\n\t\tnew_rx_pending = MSS_THRESHOLD_START;\n\telse if (!IS_ALIGNED(ring->rx_pending, 16))\n\t\tnew_rx_pending = ALIGN(ring->rx_pending, 16);\n\n\tif (ring->tx_pending > MVPP2_MAX_TXD_MAX)\n\t\tnew_tx_pending = MVPP2_MAX_TXD_MAX;\n\telse if (!IS_ALIGNED(ring->tx_pending, 32))\n\t\tnew_tx_pending = ALIGN(ring->tx_pending, 32);\n\n\t \n\tif (new_tx_pending < MVPP2_MAX_SKB_DESCS)\n\t\tnew_tx_pending = ALIGN(MVPP2_MAX_SKB_DESCS, 32);\n\n\tif (ring->rx_pending != new_rx_pending) {\n\t\tnetdev_info(dev, \"illegal Rx ring size value %d, round to %d\\n\",\n\t\t\t    ring->rx_pending, new_rx_pending);\n\t\tring->rx_pending = new_rx_pending;\n\t}\n\n\tif (ring->tx_pending != new_tx_pending) {\n\t\tnetdev_info(dev, \"illegal Tx ring size value %d, round to %d\\n\",\n\t\t\t    ring->tx_pending, new_tx_pending);\n\t\tring->tx_pending = new_tx_pending;\n\t}\n\n\treturn 0;\n}\n\nstatic void mvpp21_get_mac_address(struct mvpp2_port *port, unsigned char *addr)\n{\n\tu32 mac_addr_l, mac_addr_m, mac_addr_h;\n\n\tmac_addr_l = readl(port->base + MVPP2_GMAC_CTRL_1_REG);\n\tmac_addr_m = readl(port->priv->lms_base + MVPP2_SRC_ADDR_MIDDLE);\n\tmac_addr_h = readl(port->priv->lms_base + MVPP2_SRC_ADDR_HIGH);\n\taddr[0] = (mac_addr_h >> 24) & 0xFF;\n\taddr[1] = (mac_addr_h >> 16) & 0xFF;\n\taddr[2] = (mac_addr_h >> 8) & 0xFF;\n\taddr[3] = mac_addr_h & 0xFF;\n\taddr[4] = mac_addr_m & 0xFF;\n\taddr[5] = (mac_addr_l >> MVPP2_GMAC_SA_LOW_OFFS) & 0xFF;\n}\n\nstatic int mvpp2_irqs_init(struct mvpp2_port *port)\n{\n\tint err, i;\n\n\tfor (i = 0; i < port->nqvecs; i++) {\n\t\tstruct mvpp2_queue_vector *qv = port->qvecs + i;\n\n\t\tif (qv->type == MVPP2_QUEUE_VECTOR_PRIVATE) {\n\t\t\tqv->mask = kzalloc(cpumask_size(), GFP_KERNEL);\n\t\t\tif (!qv->mask) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tirq_set_status_flags(qv->irq, IRQ_NO_BALANCING);\n\t\t}\n\n\t\terr = request_irq(qv->irq, mvpp2_isr, 0, port->dev->name, qv);\n\t\tif (err)\n\t\t\tgoto err;\n\n\t\tif (qv->type == MVPP2_QUEUE_VECTOR_PRIVATE) {\n\t\t\tunsigned int cpu;\n\n\t\t\tfor_each_present_cpu(cpu) {\n\t\t\t\tif (mvpp2_cpu_to_thread(port->priv, cpu) ==\n\t\t\t\t    qv->sw_thread_id)\n\t\t\t\t\tcpumask_set_cpu(cpu, qv->mask);\n\t\t\t}\n\n\t\t\tirq_set_affinity_hint(qv->irq, qv->mask);\n\t\t}\n\t}\n\n\treturn 0;\nerr:\n\tfor (i = 0; i < port->nqvecs; i++) {\n\t\tstruct mvpp2_queue_vector *qv = port->qvecs + i;\n\n\t\tirq_set_affinity_hint(qv->irq, NULL);\n\t\tkfree(qv->mask);\n\t\tqv->mask = NULL;\n\t\tfree_irq(qv->irq, qv);\n\t}\n\n\treturn err;\n}\n\nstatic void mvpp2_irqs_deinit(struct mvpp2_port *port)\n{\n\tint i;\n\n\tfor (i = 0; i < port->nqvecs; i++) {\n\t\tstruct mvpp2_queue_vector *qv = port->qvecs + i;\n\n\t\tirq_set_affinity_hint(qv->irq, NULL);\n\t\tkfree(qv->mask);\n\t\tqv->mask = NULL;\n\t\tirq_clear_status_flags(qv->irq, IRQ_NO_BALANCING);\n\t\tfree_irq(qv->irq, qv);\n\t}\n}\n\nstatic bool mvpp22_rss_is_supported(struct mvpp2_port *port)\n{\n\treturn (queue_mode == MVPP2_QDIST_MULTI_MODE) &&\n\t\t!(port->flags & MVPP2_F_LOOPBACK);\n}\n\nstatic int mvpp2_open(struct net_device *dev)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tstruct mvpp2 *priv = port->priv;\n\tunsigned char mac_bcast[ETH_ALEN] = {\n\t\t\t0xff, 0xff, 0xff, 0xff, 0xff, 0xff };\n\tbool valid = false;\n\tint err;\n\n\terr = mvpp2_prs_mac_da_accept(port, mac_bcast, true);\n\tif (err) {\n\t\tnetdev_err(dev, \"mvpp2_prs_mac_da_accept BC failed\\n\");\n\t\treturn err;\n\t}\n\terr = mvpp2_prs_mac_da_accept(port, dev->dev_addr, true);\n\tif (err) {\n\t\tnetdev_err(dev, \"mvpp2_prs_mac_da_accept own addr failed\\n\");\n\t\treturn err;\n\t}\n\terr = mvpp2_prs_tag_mode_set(port->priv, port->id, MVPP2_TAG_TYPE_MH);\n\tif (err) {\n\t\tnetdev_err(dev, \"mvpp2_prs_tag_mode_set failed\\n\");\n\t\treturn err;\n\t}\n\terr = mvpp2_prs_def_flow(port);\n\tif (err) {\n\t\tnetdev_err(dev, \"mvpp2_prs_def_flow failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\terr = mvpp2_setup_rxqs(port);\n\tif (err) {\n\t\tnetdev_err(port->dev, \"cannot allocate Rx queues\\n\");\n\t\treturn err;\n\t}\n\n\terr = mvpp2_setup_txqs(port);\n\tif (err) {\n\t\tnetdev_err(port->dev, \"cannot allocate Tx queues\\n\");\n\t\tgoto err_cleanup_rxqs;\n\t}\n\n\terr = mvpp2_irqs_init(port);\n\tif (err) {\n\t\tnetdev_err(port->dev, \"cannot init IRQs\\n\");\n\t\tgoto err_cleanup_txqs;\n\t}\n\n\tif (port->phylink) {\n\t\terr = phylink_fwnode_phy_connect(port->phylink, port->fwnode, 0);\n\t\tif (err) {\n\t\t\tnetdev_err(port->dev, \"could not attach PHY (%d)\\n\",\n\t\t\t\t   err);\n\t\t\tgoto err_free_irq;\n\t\t}\n\n\t\tvalid = true;\n\t}\n\n\tif (priv->hw_version >= MVPP22 && port->port_irq) {\n\t\terr = request_irq(port->port_irq, mvpp2_port_isr, 0,\n\t\t\t\t  dev->name, port);\n\t\tif (err) {\n\t\t\tnetdev_err(port->dev,\n\t\t\t\t   \"cannot request port link/ptp IRQ %d\\n\",\n\t\t\t\t   port->port_irq);\n\t\t\tgoto err_free_irq;\n\t\t}\n\n\t\tmvpp22_gop_setup_irq(port);\n\n\t\t \n\t\tnetif_carrier_off(port->dev);\n\n\t\tvalid = true;\n\t} else {\n\t\tport->port_irq = 0;\n\t}\n\n\tif (!valid) {\n\t\tnetdev_err(port->dev,\n\t\t\t   \"invalid configuration: no dt or link IRQ\");\n\t\terr = -ENOENT;\n\t\tgoto err_free_irq;\n\t}\n\n\t \n\ton_each_cpu(mvpp2_interrupts_unmask, port, 1);\n\tmvpp2_shared_interrupt_mask_unmask(port, false);\n\n\tmvpp2_start_dev(port);\n\n\t \n\tqueue_delayed_work(priv->stats_queue, &port->stats_work,\n\t\t\t   MVPP2_MIB_COUNTERS_STATS_DELAY);\n\n\treturn 0;\n\nerr_free_irq:\n\tmvpp2_irqs_deinit(port);\nerr_cleanup_txqs:\n\tmvpp2_cleanup_txqs(port);\nerr_cleanup_rxqs:\n\tmvpp2_cleanup_rxqs(port);\n\treturn err;\n}\n\nstatic int mvpp2_stop(struct net_device *dev)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tstruct mvpp2_port_pcpu *port_pcpu;\n\tunsigned int thread;\n\n\tmvpp2_stop_dev(port);\n\n\t \n\ton_each_cpu(mvpp2_interrupts_mask, port, 1);\n\tmvpp2_shared_interrupt_mask_unmask(port, true);\n\n\tif (port->phylink)\n\t\tphylink_disconnect_phy(port->phylink);\n\tif (port->port_irq)\n\t\tfree_irq(port->port_irq, port);\n\n\tmvpp2_irqs_deinit(port);\n\tif (!port->has_tx_irqs) {\n\t\tfor (thread = 0; thread < port->priv->nthreads; thread++) {\n\t\t\tport_pcpu = per_cpu_ptr(port->pcpu, thread);\n\n\t\t\thrtimer_cancel(&port_pcpu->tx_done_timer);\n\t\t\tport_pcpu->timer_scheduled = false;\n\t\t}\n\t}\n\tmvpp2_cleanup_rxqs(port);\n\tmvpp2_cleanup_txqs(port);\n\n\tcancel_delayed_work_sync(&port->stats_work);\n\n\tmvpp2_mac_reset_assert(port);\n\tmvpp22_pcs_reset_assert(port);\n\n\treturn 0;\n}\n\nstatic int mvpp2_prs_mac_da_accept_list(struct mvpp2_port *port,\n\t\t\t\t\tstruct netdev_hw_addr_list *list)\n{\n\tstruct netdev_hw_addr *ha;\n\tint ret;\n\n\tnetdev_hw_addr_list_for_each(ha, list) {\n\t\tret = mvpp2_prs_mac_da_accept(port, ha->addr, true);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void mvpp2_set_rx_promisc(struct mvpp2_port *port, bool enable)\n{\n\tif (!enable && (port->dev->features & NETIF_F_HW_VLAN_CTAG_FILTER))\n\t\tmvpp2_prs_vid_enable_filtering(port);\n\telse\n\t\tmvpp2_prs_vid_disable_filtering(port);\n\n\tmvpp2_prs_mac_promisc_set(port->priv, port->id,\n\t\t\t\t  MVPP2_PRS_L2_UNI_CAST, enable);\n\n\tmvpp2_prs_mac_promisc_set(port->priv, port->id,\n\t\t\t\t  MVPP2_PRS_L2_MULTI_CAST, enable);\n}\n\nstatic void mvpp2_set_rx_mode(struct net_device *dev)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\t \n\tmvpp2_prs_mac_del_all(port);\n\n\tif (dev->flags & IFF_PROMISC) {\n\t\tmvpp2_set_rx_promisc(port, true);\n\t\treturn;\n\t}\n\n\tmvpp2_set_rx_promisc(port, false);\n\n\tif (netdev_uc_count(dev) > MVPP2_PRS_MAC_UC_FILT_MAX ||\n\t    mvpp2_prs_mac_da_accept_list(port, &dev->uc))\n\t\tmvpp2_prs_mac_promisc_set(port->priv, port->id,\n\t\t\t\t\t  MVPP2_PRS_L2_UNI_CAST, true);\n\n\tif (dev->flags & IFF_ALLMULTI) {\n\t\tmvpp2_prs_mac_promisc_set(port->priv, port->id,\n\t\t\t\t\t  MVPP2_PRS_L2_MULTI_CAST, true);\n\t\treturn;\n\t}\n\n\tif (netdev_mc_count(dev) > MVPP2_PRS_MAC_MC_FILT_MAX ||\n\t    mvpp2_prs_mac_da_accept_list(port, &dev->mc))\n\t\tmvpp2_prs_mac_promisc_set(port->priv, port->id,\n\t\t\t\t\t  MVPP2_PRS_L2_MULTI_CAST, true);\n}\n\nstatic int mvpp2_set_mac_address(struct net_device *dev, void *p)\n{\n\tconst struct sockaddr *addr = p;\n\tint err;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\terr = mvpp2_prs_update_mac_da(dev, addr->sa_data);\n\tif (err) {\n\t\t \n\t\tmvpp2_prs_update_mac_da(dev, dev->dev_addr);\n\t\tnetdev_err(dev, \"failed to change MAC address\\n\");\n\t}\n\treturn err;\n}\n\n \nstatic int mvpp2_bm_switch_buffers(struct mvpp2 *priv, bool percpu)\n{\n\tbool change_percpu = (percpu != priv->percpu_pools);\n\tint numbufs = MVPP2_BM_POOLS_NUM, i;\n\tstruct mvpp2_port *port = NULL;\n\tbool status[MVPP2_MAX_PORTS];\n\n\tfor (i = 0; i < priv->port_count; i++) {\n\t\tport = priv->port_list[i];\n\t\tstatus[i] = netif_running(port->dev);\n\t\tif (status[i])\n\t\t\tmvpp2_stop(port->dev);\n\t}\n\n\t \n\tif (priv->percpu_pools)\n\t\tnumbufs = port->nrxqs * 2;\n\n\tif (change_percpu)\n\t\tmvpp2_bm_pool_update_priv_fc(priv, false);\n\n\tfor (i = 0; i < numbufs; i++)\n\t\tmvpp2_bm_pool_destroy(port->dev->dev.parent, priv, &priv->bm_pools[i]);\n\n\tdevm_kfree(port->dev->dev.parent, priv->bm_pools);\n\tpriv->percpu_pools = percpu;\n\tmvpp2_bm_init(port->dev->dev.parent, priv);\n\n\tfor (i = 0; i < priv->port_count; i++) {\n\t\tport = priv->port_list[i];\n\t\tif (percpu && port->ntxqs >= num_possible_cpus() * 2)\n\t\t\txdp_set_features_flag(port->dev,\n\t\t\t\t\t      NETDEV_XDP_ACT_BASIC |\n\t\t\t\t\t      NETDEV_XDP_ACT_REDIRECT |\n\t\t\t\t\t      NETDEV_XDP_ACT_NDO_XMIT);\n\t\telse\n\t\t\txdp_clear_features_flag(port->dev);\n\n\t\tmvpp2_swf_bm_pool_init(port);\n\t\tif (status[i])\n\t\t\tmvpp2_open(port->dev);\n\t}\n\n\tif (change_percpu)\n\t\tmvpp2_bm_pool_update_priv_fc(priv, true);\n\n\treturn 0;\n}\n\nstatic int mvpp2_change_mtu(struct net_device *dev, int mtu)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tbool running = netif_running(dev);\n\tstruct mvpp2 *priv = port->priv;\n\tint err;\n\n\tif (!IS_ALIGNED(MVPP2_RX_PKT_SIZE(mtu), 8)) {\n\t\tnetdev_info(dev, \"illegal MTU value %d, round to %d\\n\", mtu,\n\t\t\t    ALIGN(MVPP2_RX_PKT_SIZE(mtu), 8));\n\t\tmtu = ALIGN(MVPP2_RX_PKT_SIZE(mtu), 8);\n\t}\n\n\tif (port->xdp_prog && mtu > MVPP2_MAX_RX_BUF_SIZE) {\n\t\tnetdev_err(dev, \"Illegal MTU value %d (> %d) for XDP mode\\n\",\n\t\t\t   mtu, (int)MVPP2_MAX_RX_BUF_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\tif (MVPP2_RX_PKT_SIZE(mtu) > MVPP2_BM_LONG_PKT_SIZE) {\n\t\tif (priv->percpu_pools) {\n\t\t\tnetdev_warn(dev, \"mtu %d too high, switching to shared buffers\", mtu);\n\t\t\tmvpp2_bm_switch_buffers(priv, false);\n\t\t}\n\t} else {\n\t\tbool jumbo = false;\n\t\tint i;\n\n\t\tfor (i = 0; i < priv->port_count; i++)\n\t\t\tif (priv->port_list[i] != port &&\n\t\t\t    MVPP2_RX_PKT_SIZE(priv->port_list[i]->dev->mtu) >\n\t\t\t    MVPP2_BM_LONG_PKT_SIZE) {\n\t\t\t\tjumbo = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t \n\t\tif (!jumbo) {\n\t\t\tdev_info(port->dev->dev.parent,\n\t\t\t\t \"all ports have a low MTU, switching to per-cpu buffers\");\n\t\t\tmvpp2_bm_switch_buffers(priv, true);\n\t\t}\n\t}\n\n\tif (running)\n\t\tmvpp2_stop_dev(port);\n\n\terr = mvpp2_bm_update_mtu(dev, mtu);\n\tif (err) {\n\t\tnetdev_err(dev, \"failed to change MTU\\n\");\n\t\t \n\t\tmvpp2_bm_update_mtu(dev, dev->mtu);\n\t} else {\n\t\tport->pkt_size =  MVPP2_RX_PKT_SIZE(mtu);\n\t}\n\n\tif (running) {\n\t\tmvpp2_start_dev(port);\n\t\tmvpp2_egress_enable(port);\n\t\tmvpp2_ingress_enable(port);\n\t}\n\n\treturn err;\n}\n\nstatic int mvpp2_check_pagepool_dma(struct mvpp2_port *port)\n{\n\tenum dma_data_direction dma_dir = DMA_FROM_DEVICE;\n\tstruct mvpp2 *priv = port->priv;\n\tint err = -1, i;\n\n\tif (!priv->percpu_pools)\n\t\treturn err;\n\n\tif (!priv->page_pool[0])\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < priv->port_count; i++) {\n\t\tport = priv->port_list[i];\n\t\tif (port->xdp_prog) {\n\t\t\tdma_dir = DMA_BIDIRECTIONAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (priv->page_pool[0]->p.dma_dir != dma_dir)\n\t\terr = mvpp2_bm_switch_buffers(priv, true);\n\n\treturn err;\n}\n\nstatic void\nmvpp2_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tunsigned int start;\n\tunsigned int cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct mvpp2_pcpu_stats *cpu_stats;\n\t\tu64 rx_packets;\n\t\tu64 rx_bytes;\n\t\tu64 tx_packets;\n\t\tu64 tx_bytes;\n\n\t\tcpu_stats = per_cpu_ptr(port->stats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&cpu_stats->syncp);\n\t\t\trx_packets = cpu_stats->rx_packets;\n\t\t\trx_bytes   = cpu_stats->rx_bytes;\n\t\t\ttx_packets = cpu_stats->tx_packets;\n\t\t\ttx_bytes   = cpu_stats->tx_bytes;\n\t\t} while (u64_stats_fetch_retry(&cpu_stats->syncp, start));\n\n\t\tstats->rx_packets += rx_packets;\n\t\tstats->rx_bytes   += rx_bytes;\n\t\tstats->tx_packets += tx_packets;\n\t\tstats->tx_bytes   += tx_bytes;\n\t}\n\n\tstats->rx_errors\t= dev->stats.rx_errors;\n\tstats->rx_dropped\t= dev->stats.rx_dropped;\n\tstats->tx_dropped\t= dev->stats.tx_dropped;\n}\n\nstatic int mvpp2_set_ts_config(struct mvpp2_port *port, struct ifreq *ifr)\n{\n\tstruct hwtstamp_config config;\n\tvoid __iomem *ptp;\n\tu32 gcr, int_mask;\n\n\tif (copy_from_user(&config, ifr->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\tif (config.tx_type != HWTSTAMP_TX_OFF &&\n\t    config.tx_type != HWTSTAMP_TX_ON)\n\t\treturn -ERANGE;\n\n\tptp = port->priv->iface_base + MVPP22_PTP_BASE(port->gop_id);\n\n\tint_mask = gcr = 0;\n\tif (config.tx_type != HWTSTAMP_TX_OFF) {\n\t\tgcr |= MVPP22_PTP_GCR_TSU_ENABLE | MVPP22_PTP_GCR_TX_RESET;\n\t\tint_mask |= MVPP22_PTP_INT_MASK_QUEUE1 |\n\t\t\t    MVPP22_PTP_INT_MASK_QUEUE0;\n\t}\n\n\t \n\tif (config.rx_filter != HWTSTAMP_FILTER_NONE)\n\t\tgcr |= MVPP22_PTP_GCR_TSU_ENABLE | MVPP22_PTP_GCR_RX_RESET |\n\t\t       MVPP22_PTP_GCR_TX_RESET;\n\n\tif (gcr & MVPP22_PTP_GCR_TSU_ENABLE)\n\t\tmvpp22_tai_start(port->priv->tai);\n\n\tif (config.rx_filter != HWTSTAMP_FILTER_NONE) {\n\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t\tmvpp2_modify(ptp + MVPP22_PTP_GCR,\n\t\t\t     MVPP22_PTP_GCR_RX_RESET |\n\t\t\t     MVPP22_PTP_GCR_TX_RESET |\n\t\t\t     MVPP22_PTP_GCR_TSU_ENABLE, gcr);\n\t\tport->rx_hwtstamp = true;\n\t} else {\n\t\tport->rx_hwtstamp = false;\n\t\tmvpp2_modify(ptp + MVPP22_PTP_GCR,\n\t\t\t     MVPP22_PTP_GCR_RX_RESET |\n\t\t\t     MVPP22_PTP_GCR_TX_RESET |\n\t\t\t     MVPP22_PTP_GCR_TSU_ENABLE, gcr);\n\t}\n\n\tmvpp2_modify(ptp + MVPP22_PTP_INT_MASK,\n\t\t     MVPP22_PTP_INT_MASK_QUEUE1 |\n\t\t     MVPP22_PTP_INT_MASK_QUEUE0, int_mask);\n\n\tif (!(gcr & MVPP22_PTP_GCR_TSU_ENABLE))\n\t\tmvpp22_tai_stop(port->priv->tai);\n\n\tport->tx_hwtstamp_type = config.tx_type;\n\n\tif (copy_to_user(ifr->ifr_data, &config, sizeof(config)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int mvpp2_get_ts_config(struct mvpp2_port *port, struct ifreq *ifr)\n{\n\tstruct hwtstamp_config config;\n\n\tmemset(&config, 0, sizeof(config));\n\n\tconfig.tx_type = port->tx_hwtstamp_type;\n\tconfig.rx_filter = port->rx_hwtstamp ?\n\t\tHWTSTAMP_FILTER_ALL : HWTSTAMP_FILTER_NONE;\n\n\tif (copy_to_user(ifr->ifr_data, &config, sizeof(config)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int mvpp2_ethtool_get_ts_info(struct net_device *dev,\n\t\t\t\t     struct ethtool_ts_info *info)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tif (!port->hwtstamp)\n\t\treturn -EOPNOTSUPP;\n\n\tinfo->phc_index = mvpp22_tai_ptp_clock_index(port->priv->tai);\n\tinfo->so_timestamping = SOF_TIMESTAMPING_TX_SOFTWARE |\n\t\t\t\tSOF_TIMESTAMPING_RX_SOFTWARE |\n\t\t\t\tSOF_TIMESTAMPING_SOFTWARE |\n\t\t\t\tSOF_TIMESTAMPING_TX_HARDWARE |\n\t\t\t\tSOF_TIMESTAMPING_RX_HARDWARE |\n\t\t\t\tSOF_TIMESTAMPING_RAW_HARDWARE;\n\tinfo->tx_types = BIT(HWTSTAMP_TX_OFF) |\n\t\t\t BIT(HWTSTAMP_TX_ON);\n\tinfo->rx_filters = BIT(HWTSTAMP_FILTER_NONE) |\n\t\t\t   BIT(HWTSTAMP_FILTER_ALL);\n\n\treturn 0;\n}\n\nstatic int mvpp2_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tswitch (cmd) {\n\tcase SIOCSHWTSTAMP:\n\t\tif (port->hwtstamp)\n\t\t\treturn mvpp2_set_ts_config(port, ifr);\n\t\tbreak;\n\n\tcase SIOCGHWTSTAMP:\n\t\tif (port->hwtstamp)\n\t\t\treturn mvpp2_get_ts_config(port, ifr);\n\t\tbreak;\n\t}\n\n\tif (!port->phylink)\n\t\treturn -ENOTSUPP;\n\n\treturn phylink_mii_ioctl(port->phylink, ifr, cmd);\n}\n\nstatic int mvpp2_vlan_rx_add_vid(struct net_device *dev, __be16 proto, u16 vid)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint ret;\n\n\tret = mvpp2_prs_vid_entry_add(port, vid);\n\tif (ret)\n\t\tnetdev_err(dev, \"rx-vlan-filter offloading cannot accept more than %d VIDs per port\\n\",\n\t\t\t   MVPP2_PRS_VLAN_FILT_MAX - 1);\n\treturn ret;\n}\n\nstatic int mvpp2_vlan_rx_kill_vid(struct net_device *dev, __be16 proto, u16 vid)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tmvpp2_prs_vid_entry_remove(port, vid);\n\treturn 0;\n}\n\nstatic int mvpp2_set_features(struct net_device *dev,\n\t\t\t      netdev_features_t features)\n{\n\tnetdev_features_t changed = dev->features ^ features;\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tif (changed & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\tif (features & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t\tmvpp2_prs_vid_enable_filtering(port);\n\t\t} else {\n\t\t\t \n\t\t\tmvpp2_prs_vid_remove_all(port);\n\n\t\t\tmvpp2_prs_vid_disable_filtering(port);\n\t\t}\n\t}\n\n\tif (changed & NETIF_F_RXHASH) {\n\t\tif (features & NETIF_F_RXHASH)\n\t\t\tmvpp22_port_rss_enable(port);\n\t\telse\n\t\t\tmvpp22_port_rss_disable(port);\n\t}\n\n\treturn 0;\n}\n\nstatic int mvpp2_xdp_setup(struct mvpp2_port *port, struct netdev_bpf *bpf)\n{\n\tstruct bpf_prog *prog = bpf->prog, *old_prog;\n\tbool running = netif_running(port->dev);\n\tbool reset = !prog != !port->xdp_prog;\n\n\tif (port->dev->mtu > MVPP2_MAX_RX_BUF_SIZE) {\n\t\tNL_SET_ERR_MSG_MOD(bpf->extack, \"MTU too large for XDP\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!port->priv->percpu_pools) {\n\t\tNL_SET_ERR_MSG_MOD(bpf->extack, \"Per CPU Pools required for XDP\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (port->ntxqs < num_possible_cpus() * 2) {\n\t\tNL_SET_ERR_MSG_MOD(bpf->extack, \"XDP_TX needs two TX queues per CPU\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif (running && reset)\n\t\tmvpp2_stop(port->dev);\n\n\told_prog = xchg(&port->xdp_prog, prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\t \n\tif (!reset)\n\t\treturn 0;\n\n\t \n\tif (running)\n\t\tmvpp2_open(port->dev);\n\n\t \n\tmvpp2_check_pagepool_dma(port);\n\n\treturn 0;\n}\n\nstatic int mvpp2_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn mvpp2_xdp_setup(port, xdp);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n \n\nstatic int mvpp2_ethtool_nway_reset(struct net_device *dev)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tif (!port->phylink)\n\t\treturn -ENOTSUPP;\n\n\treturn phylink_ethtool_nway_reset(port->phylink);\n}\n\n \nstatic int\nmvpp2_ethtool_set_coalesce(struct net_device *dev,\n\t\t\t   struct ethtool_coalesce *c,\n\t\t\t   struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint queue;\n\n\tfor (queue = 0; queue < port->nrxqs; queue++) {\n\t\tstruct mvpp2_rx_queue *rxq = port->rxqs[queue];\n\n\t\trxq->time_coal = c->rx_coalesce_usecs;\n\t\trxq->pkts_coal = c->rx_max_coalesced_frames;\n\t\tmvpp2_rx_pkts_coal_set(port, rxq);\n\t\tmvpp2_rx_time_coal_set(port, rxq);\n\t}\n\n\tif (port->has_tx_irqs) {\n\t\tport->tx_time_coal = c->tx_coalesce_usecs;\n\t\tmvpp2_tx_time_coal_set(port);\n\t}\n\n\tfor (queue = 0; queue < port->ntxqs; queue++) {\n\t\tstruct mvpp2_tx_queue *txq = port->txqs[queue];\n\n\t\ttxq->done_pkts_coal = c->tx_max_coalesced_frames;\n\n\t\tif (port->has_tx_irqs)\n\t\t\tmvpp2_tx_pkts_coal_set(port, txq);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\nmvpp2_ethtool_get_coalesce(struct net_device *dev,\n\t\t\t   struct ethtool_coalesce *c,\n\t\t\t   struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tc->rx_coalesce_usecs       = port->rxqs[0]->time_coal;\n\tc->rx_max_coalesced_frames = port->rxqs[0]->pkts_coal;\n\tc->tx_max_coalesced_frames = port->txqs[0]->done_pkts_coal;\n\tc->tx_coalesce_usecs       = port->tx_time_coal;\n\treturn 0;\n}\n\nstatic void mvpp2_ethtool_get_drvinfo(struct net_device *dev,\n\t\t\t\t      struct ethtool_drvinfo *drvinfo)\n{\n\tstrscpy(drvinfo->driver, MVPP2_DRIVER_NAME,\n\t\tsizeof(drvinfo->driver));\n\tstrscpy(drvinfo->version, MVPP2_DRIVER_VERSION,\n\t\tsizeof(drvinfo->version));\n\tstrscpy(drvinfo->bus_info, dev_name(&dev->dev),\n\t\tsizeof(drvinfo->bus_info));\n}\n\nstatic void\nmvpp2_ethtool_get_ringparam(struct net_device *dev,\n\t\t\t    struct ethtool_ringparam *ring,\n\t\t\t    struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tring->rx_max_pending = MVPP2_MAX_RXD_MAX;\n\tring->tx_max_pending = MVPP2_MAX_TXD_MAX;\n\tring->rx_pending = port->rx_ring_size;\n\tring->tx_pending = port->tx_ring_size;\n}\n\nstatic int\nmvpp2_ethtool_set_ringparam(struct net_device *dev,\n\t\t\t    struct ethtool_ringparam *ring,\n\t\t\t    struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tu16 prev_rx_ring_size = port->rx_ring_size;\n\tu16 prev_tx_ring_size = port->tx_ring_size;\n\tint err;\n\n\terr = mvpp2_check_ringparam_valid(dev, ring);\n\tif (err)\n\t\treturn err;\n\n\tif (!netif_running(dev)) {\n\t\tport->rx_ring_size = ring->rx_pending;\n\t\tport->tx_ring_size = ring->tx_pending;\n\t\treturn 0;\n\t}\n\n\t \n\tmvpp2_stop_dev(port);\n\tmvpp2_cleanup_rxqs(port);\n\tmvpp2_cleanup_txqs(port);\n\n\tport->rx_ring_size = ring->rx_pending;\n\tport->tx_ring_size = ring->tx_pending;\n\n\terr = mvpp2_setup_rxqs(port);\n\tif (err) {\n\t\t \n\t\tport->rx_ring_size = prev_rx_ring_size;\n\t\tring->rx_pending = prev_rx_ring_size;\n\t\terr = mvpp2_setup_rxqs(port);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\terr = mvpp2_setup_txqs(port);\n\tif (err) {\n\t\t \n\t\tport->tx_ring_size = prev_tx_ring_size;\n\t\tring->tx_pending = prev_tx_ring_size;\n\t\terr = mvpp2_setup_txqs(port);\n\t\tif (err)\n\t\t\tgoto err_clean_rxqs;\n\t}\n\n\tmvpp2_start_dev(port);\n\tmvpp2_egress_enable(port);\n\tmvpp2_ingress_enable(port);\n\n\treturn 0;\n\nerr_clean_rxqs:\n\tmvpp2_cleanup_rxqs(port);\nerr_out:\n\tnetdev_err(dev, \"failed to change ring parameters\");\n\treturn err;\n}\n\nstatic void mvpp2_ethtool_get_pause_param(struct net_device *dev,\n\t\t\t\t\t  struct ethtool_pauseparam *pause)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tif (!port->phylink)\n\t\treturn;\n\n\tphylink_ethtool_get_pauseparam(port->phylink, pause);\n}\n\nstatic int mvpp2_ethtool_set_pause_param(struct net_device *dev,\n\t\t\t\t\t struct ethtool_pauseparam *pause)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tif (!port->phylink)\n\t\treturn -ENOTSUPP;\n\n\treturn phylink_ethtool_set_pauseparam(port->phylink, pause);\n}\n\nstatic int mvpp2_ethtool_get_link_ksettings(struct net_device *dev,\n\t\t\t\t\t    struct ethtool_link_ksettings *cmd)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tif (!port->phylink)\n\t\treturn -ENOTSUPP;\n\n\treturn phylink_ethtool_ksettings_get(port->phylink, cmd);\n}\n\nstatic int mvpp2_ethtool_set_link_ksettings(struct net_device *dev,\n\t\t\t\t\t    const struct ethtool_link_ksettings *cmd)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\tif (!port->phylink)\n\t\treturn -ENOTSUPP;\n\n\treturn phylink_ethtool_ksettings_set(port->phylink, cmd);\n}\n\nstatic int mvpp2_ethtool_get_rxnfc(struct net_device *dev,\n\t\t\t\t   struct ethtool_rxnfc *info, u32 *rules)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint ret = 0, i, loc = 0;\n\n\tif (!mvpp22_rss_is_supported(port))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (info->cmd) {\n\tcase ETHTOOL_GRXFH:\n\t\tret = mvpp2_ethtool_rxfh_get(port, info);\n\t\tbreak;\n\tcase ETHTOOL_GRXRINGS:\n\t\tinfo->data = port->nrxqs;\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRLCNT:\n\t\tinfo->rule_cnt = port->n_rfs_rules;\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRULE:\n\t\tret = mvpp2_ethtool_cls_rule_get(port, info);\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRLALL:\n\t\tfor (i = 0; i < MVPP2_N_RFS_ENTRIES_PER_FLOW; i++) {\n\t\t\tif (loc == info->rule_cnt) {\n\t\t\t\tret = -EMSGSIZE;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (port->rfs_rules[i])\n\t\t\t\trules[loc++] = i;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOTSUPP;\n\t}\n\n\treturn ret;\n}\n\nstatic int mvpp2_ethtool_set_rxnfc(struct net_device *dev,\n\t\t\t\t   struct ethtool_rxnfc *info)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint ret = 0;\n\n\tif (!mvpp22_rss_is_supported(port))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (info->cmd) {\n\tcase ETHTOOL_SRXFH:\n\t\tret = mvpp2_ethtool_rxfh_set(port, info);\n\t\tbreak;\n\tcase ETHTOOL_SRXCLSRLINS:\n\t\tret = mvpp2_ethtool_cls_rule_ins(port, info);\n\t\tbreak;\n\tcase ETHTOOL_SRXCLSRLDEL:\n\t\tret = mvpp2_ethtool_cls_rule_del(port, info);\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn ret;\n}\n\nstatic u32 mvpp2_ethtool_get_rxfh_indir_size(struct net_device *dev)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\n\treturn mvpp22_rss_is_supported(port) ? MVPP22_RSS_TABLE_ENTRIES : 0;\n}\n\nstatic int mvpp2_ethtool_get_rxfh(struct net_device *dev, u32 *indir, u8 *key,\n\t\t\t\t  u8 *hfunc)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint ret = 0;\n\n\tif (!mvpp22_rss_is_supported(port))\n\t\treturn -EOPNOTSUPP;\n\n\tif (indir)\n\t\tret = mvpp22_port_rss_ctx_indir_get(port, 0, indir);\n\n\tif (hfunc)\n\t\t*hfunc = ETH_RSS_HASH_CRC32;\n\n\treturn ret;\n}\n\nstatic int mvpp2_ethtool_set_rxfh(struct net_device *dev, const u32 *indir,\n\t\t\t\t  const u8 *key, const u8 hfunc)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint ret = 0;\n\n\tif (!mvpp22_rss_is_supported(port))\n\t\treturn -EOPNOTSUPP;\n\n\tif (hfunc != ETH_RSS_HASH_NO_CHANGE && hfunc != ETH_RSS_HASH_CRC32)\n\t\treturn -EOPNOTSUPP;\n\n\tif (key)\n\t\treturn -EOPNOTSUPP;\n\n\tif (indir)\n\t\tret = mvpp22_port_rss_ctx_indir_set(port, 0, indir);\n\n\treturn ret;\n}\n\nstatic int mvpp2_ethtool_get_rxfh_context(struct net_device *dev, u32 *indir,\n\t\t\t\t\t  u8 *key, u8 *hfunc, u32 rss_context)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint ret = 0;\n\n\tif (!mvpp22_rss_is_supported(port))\n\t\treturn -EOPNOTSUPP;\n\tif (rss_context >= MVPP22_N_RSS_TABLES)\n\t\treturn -EINVAL;\n\n\tif (hfunc)\n\t\t*hfunc = ETH_RSS_HASH_CRC32;\n\n\tif (indir)\n\t\tret = mvpp22_port_rss_ctx_indir_get(port, rss_context, indir);\n\n\treturn ret;\n}\n\nstatic int mvpp2_ethtool_set_rxfh_context(struct net_device *dev,\n\t\t\t\t\t  const u32 *indir, const u8 *key,\n\t\t\t\t\t  const u8 hfunc, u32 *rss_context,\n\t\t\t\t\t  bool delete)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tint ret;\n\n\tif (!mvpp22_rss_is_supported(port))\n\t\treturn -EOPNOTSUPP;\n\n\tif (hfunc != ETH_RSS_HASH_NO_CHANGE && hfunc != ETH_RSS_HASH_CRC32)\n\t\treturn -EOPNOTSUPP;\n\n\tif (key)\n\t\treturn -EOPNOTSUPP;\n\n\tif (delete)\n\t\treturn mvpp22_port_rss_ctx_delete(port, *rss_context);\n\n\tif (*rss_context == ETH_RXFH_CONTEXT_ALLOC) {\n\t\tret = mvpp22_port_rss_ctx_create(port, rss_context);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn mvpp22_port_rss_ctx_indir_set(port, *rss_context, indir);\n}\n \n\nstatic const struct net_device_ops mvpp2_netdev_ops = {\n\t.ndo_open\t\t= mvpp2_open,\n\t.ndo_stop\t\t= mvpp2_stop,\n\t.ndo_start_xmit\t\t= mvpp2_tx,\n\t.ndo_set_rx_mode\t= mvpp2_set_rx_mode,\n\t.ndo_set_mac_address\t= mvpp2_set_mac_address,\n\t.ndo_change_mtu\t\t= mvpp2_change_mtu,\n\t.ndo_get_stats64\t= mvpp2_get_stats64,\n\t.ndo_eth_ioctl\t\t= mvpp2_ioctl,\n\t.ndo_vlan_rx_add_vid\t= mvpp2_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= mvpp2_vlan_rx_kill_vid,\n\t.ndo_set_features\t= mvpp2_set_features,\n\t.ndo_bpf\t\t= mvpp2_xdp,\n\t.ndo_xdp_xmit\t\t= mvpp2_xdp_xmit,\n};\n\nstatic const struct ethtool_ops mvpp2_eth_tool_ops = {\n\t.supported_coalesce_params = ETHTOOL_COALESCE_USECS |\n\t\t\t\t     ETHTOOL_COALESCE_MAX_FRAMES,\n\t.nway_reset\t\t= mvpp2_ethtool_nway_reset,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_ts_info\t\t= mvpp2_ethtool_get_ts_info,\n\t.set_coalesce\t\t= mvpp2_ethtool_set_coalesce,\n\t.get_coalesce\t\t= mvpp2_ethtool_get_coalesce,\n\t.get_drvinfo\t\t= mvpp2_ethtool_get_drvinfo,\n\t.get_ringparam\t\t= mvpp2_ethtool_get_ringparam,\n\t.set_ringparam\t\t= mvpp2_ethtool_set_ringparam,\n\t.get_strings\t\t= mvpp2_ethtool_get_strings,\n\t.get_ethtool_stats\t= mvpp2_ethtool_get_stats,\n\t.get_sset_count\t\t= mvpp2_ethtool_get_sset_count,\n\t.get_pauseparam\t\t= mvpp2_ethtool_get_pause_param,\n\t.set_pauseparam\t\t= mvpp2_ethtool_set_pause_param,\n\t.get_link_ksettings\t= mvpp2_ethtool_get_link_ksettings,\n\t.set_link_ksettings\t= mvpp2_ethtool_set_link_ksettings,\n\t.get_rxnfc\t\t= mvpp2_ethtool_get_rxnfc,\n\t.set_rxnfc\t\t= mvpp2_ethtool_set_rxnfc,\n\t.get_rxfh_indir_size\t= mvpp2_ethtool_get_rxfh_indir_size,\n\t.get_rxfh\t\t= mvpp2_ethtool_get_rxfh,\n\t.set_rxfh\t\t= mvpp2_ethtool_set_rxfh,\n\t.get_rxfh_context\t= mvpp2_ethtool_get_rxfh_context,\n\t.set_rxfh_context\t= mvpp2_ethtool_set_rxfh_context,\n};\n\n \nstatic int mvpp2_simple_queue_vectors_init(struct mvpp2_port *port,\n\t\t\t\t\t   struct device_node *port_node)\n{\n\tstruct mvpp2_queue_vector *v = &port->qvecs[0];\n\n\tv->first_rxq = 0;\n\tv->nrxqs = port->nrxqs;\n\tv->type = MVPP2_QUEUE_VECTOR_SHARED;\n\tv->sw_thread_id = 0;\n\tv->sw_thread_mask = *cpumask_bits(cpu_online_mask);\n\tv->port = port;\n\tv->irq = irq_of_parse_and_map(port_node, 0);\n\tif (v->irq <= 0)\n\t\treturn -EINVAL;\n\tnetif_napi_add(port->dev, &v->napi, mvpp2_poll);\n\n\tport->nqvecs = 1;\n\n\treturn 0;\n}\n\nstatic int mvpp2_multi_queue_vectors_init(struct mvpp2_port *port,\n\t\t\t\t\t  struct device_node *port_node)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tstruct mvpp2_queue_vector *v;\n\tint i, ret;\n\n\tswitch (queue_mode) {\n\tcase MVPP2_QDIST_SINGLE_MODE:\n\t\tport->nqvecs = priv->nthreads + 1;\n\t\tbreak;\n\tcase MVPP2_QDIST_MULTI_MODE:\n\t\tport->nqvecs = priv->nthreads;\n\t\tbreak;\n\t}\n\n\tfor (i = 0; i < port->nqvecs; i++) {\n\t\tchar irqname[16];\n\n\t\tv = port->qvecs + i;\n\n\t\tv->port = port;\n\t\tv->type = MVPP2_QUEUE_VECTOR_PRIVATE;\n\t\tv->sw_thread_id = i;\n\t\tv->sw_thread_mask = BIT(i);\n\n\t\tif (port->flags & MVPP2_F_DT_COMPAT)\n\t\t\tsnprintf(irqname, sizeof(irqname), \"tx-cpu%d\", i);\n\t\telse\n\t\t\tsnprintf(irqname, sizeof(irqname), \"hif%d\", i);\n\n\t\tif (queue_mode == MVPP2_QDIST_MULTI_MODE) {\n\t\t\tv->first_rxq = i;\n\t\t\tv->nrxqs = 1;\n\t\t} else if (queue_mode == MVPP2_QDIST_SINGLE_MODE &&\n\t\t\t   i == (port->nqvecs - 1)) {\n\t\t\tv->first_rxq = 0;\n\t\t\tv->nrxqs = port->nrxqs;\n\t\t\tv->type = MVPP2_QUEUE_VECTOR_SHARED;\n\n\t\t\tif (port->flags & MVPP2_F_DT_COMPAT)\n\t\t\t\tstrncpy(irqname, \"rx-shared\", sizeof(irqname));\n\t\t}\n\n\t\tif (port_node)\n\t\t\tv->irq = of_irq_get_byname(port_node, irqname);\n\t\telse\n\t\t\tv->irq = fwnode_irq_get(port->fwnode, i);\n\t\tif (v->irq <= 0) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tnetif_napi_add(port->dev, &v->napi, mvpp2_poll);\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (i = 0; i < port->nqvecs; i++)\n\t\tirq_dispose_mapping(port->qvecs[i].irq);\n\treturn ret;\n}\n\nstatic int mvpp2_queue_vectors_init(struct mvpp2_port *port,\n\t\t\t\t    struct device_node *port_node)\n{\n\tif (port->has_tx_irqs)\n\t\treturn mvpp2_multi_queue_vectors_init(port, port_node);\n\telse\n\t\treturn mvpp2_simple_queue_vectors_init(port, port_node);\n}\n\nstatic void mvpp2_queue_vectors_deinit(struct mvpp2_port *port)\n{\n\tint i;\n\n\tfor (i = 0; i < port->nqvecs; i++)\n\t\tirq_dispose_mapping(port->qvecs[i].irq);\n}\n\n \nstatic void mvpp2_rx_irqs_setup(struct mvpp2_port *port)\n{\n\tstruct mvpp2 *priv = port->priv;\n\tu32 val;\n\tint i;\n\n\tif (priv->hw_version == MVPP21) {\n\t\tmvpp2_write(priv, MVPP21_ISR_RXQ_GROUP_REG(port->id),\n\t\t\t    port->nrxqs);\n\t\treturn;\n\t}\n\n\t \n\tfor (i = 0; i < port->nqvecs; i++) {\n\t\tstruct mvpp2_queue_vector *qv = port->qvecs + i;\n\n\t\tif (!qv->nrxqs)\n\t\t\tcontinue;\n\n\t\tval = qv->sw_thread_id;\n\t\tval |= port->id << MVPP22_ISR_RXQ_GROUP_INDEX_GROUP_OFFSET;\n\t\tmvpp2_write(priv, MVPP22_ISR_RXQ_GROUP_INDEX_REG, val);\n\n\t\tval = qv->first_rxq;\n\t\tval |= qv->nrxqs << MVPP22_ISR_RXQ_SUB_GROUP_SIZE_OFFSET;\n\t\tmvpp2_write(priv, MVPP22_ISR_RXQ_SUB_GROUP_CONFIG_REG, val);\n\t}\n}\n\n \nstatic int mvpp2_port_init(struct mvpp2_port *port)\n{\n\tstruct device *dev = port->dev->dev.parent;\n\tstruct mvpp2 *priv = port->priv;\n\tstruct mvpp2_txq_pcpu *txq_pcpu;\n\tunsigned int thread;\n\tint queue, err, val;\n\n\t \n\tif (port->first_rxq + port->nrxqs >\n\t    MVPP2_MAX_PORTS * priv->max_port_rxqs)\n\t\treturn -EINVAL;\n\n\tif (port->nrxqs > priv->max_port_rxqs || port->ntxqs > MVPP2_MAX_TXQ)\n\t\treturn -EINVAL;\n\n\t \n\tmvpp2_egress_disable(port);\n\tmvpp2_port_disable(port);\n\n\tif (mvpp2_is_xlg(port->phy_interface)) {\n\t\tval = readl(port->base + MVPP22_XLG_CTRL0_REG);\n\t\tval &= ~MVPP22_XLG_CTRL0_FORCE_LINK_PASS;\n\t\tval |= MVPP22_XLG_CTRL0_FORCE_LINK_DOWN;\n\t\twritel(val, port->base + MVPP22_XLG_CTRL0_REG);\n\t} else {\n\t\tval = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);\n\t\tval &= ~MVPP2_GMAC_FORCE_LINK_PASS;\n\t\tval |= MVPP2_GMAC_FORCE_LINK_DOWN;\n\t\twritel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);\n\t}\n\n\tport->tx_time_coal = MVPP2_TXDONE_COAL_USEC;\n\n\tport->txqs = devm_kcalloc(dev, port->ntxqs, sizeof(*port->txqs),\n\t\t\t\t  GFP_KERNEL);\n\tif (!port->txqs)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (queue = 0; queue < port->ntxqs; queue++) {\n\t\tint queue_phy_id = mvpp2_txq_phys(port->id, queue);\n\t\tstruct mvpp2_tx_queue *txq;\n\n\t\ttxq = devm_kzalloc(dev, sizeof(*txq), GFP_KERNEL);\n\t\tif (!txq) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_percpu;\n\t\t}\n\n\t\ttxq->pcpu = alloc_percpu(struct mvpp2_txq_pcpu);\n\t\tif (!txq->pcpu) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_percpu;\n\t\t}\n\n\t\ttxq->id = queue_phy_id;\n\t\ttxq->log_id = queue;\n\t\ttxq->done_pkts_coal = MVPP2_TXDONE_COAL_PKTS_THRESH;\n\t\tfor (thread = 0; thread < priv->nthreads; thread++) {\n\t\t\ttxq_pcpu = per_cpu_ptr(txq->pcpu, thread);\n\t\t\ttxq_pcpu->thread = thread;\n\t\t}\n\n\t\tport->txqs[queue] = txq;\n\t}\n\n\tport->rxqs = devm_kcalloc(dev, port->nrxqs, sizeof(*port->rxqs),\n\t\t\t\t  GFP_KERNEL);\n\tif (!port->rxqs) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_percpu;\n\t}\n\n\t \n\tfor (queue = 0; queue < port->nrxqs; queue++) {\n\t\tstruct mvpp2_rx_queue *rxq;\n\n\t\t \n\t\trxq = devm_kzalloc(dev, sizeof(*rxq), GFP_KERNEL);\n\t\tif (!rxq) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_percpu;\n\t\t}\n\t\t \n\t\trxq->id = port->first_rxq + queue;\n\t\trxq->port = port->id;\n\t\trxq->logic_rxq = queue;\n\n\t\tport->rxqs[queue] = rxq;\n\t}\n\n\tmvpp2_rx_irqs_setup(port);\n\n\t \n\tfor (queue = 0; queue < port->nrxqs; queue++) {\n\t\tstruct mvpp2_rx_queue *rxq = port->rxqs[queue];\n\n\t\trxq->size = port->rx_ring_size;\n\t\trxq->pkts_coal = MVPP2_RX_COAL_PKTS;\n\t\trxq->time_coal = MVPP2_RX_COAL_USEC;\n\t}\n\n\tmvpp2_ingress_disable(port);\n\n\t \n\tmvpp2_defaults_set(port);\n\n\t \n\tmvpp2_cls_oversize_rxq_set(port);\n\tmvpp2_cls_port_config(port);\n\n\tif (mvpp22_rss_is_supported(port))\n\t\tmvpp22_port_rss_init(port);\n\n\t \n\tport->pkt_size = MVPP2_RX_PKT_SIZE(port->dev->mtu);\n\n\t \n\terr = mvpp2_swf_bm_pool_init(port);\n\tif (err)\n\t\tgoto err_free_percpu;\n\n\t \n\tmvpp2_read_stats(port);\n\tmemset(port->ethtool_stats, 0,\n\t       MVPP2_N_ETHTOOL_STATS(port->ntxqs, port->nrxqs) * sizeof(u64));\n\n\treturn 0;\n\nerr_free_percpu:\n\tfor (queue = 0; queue < port->ntxqs; queue++) {\n\t\tif (!port->txqs[queue])\n\t\t\tcontinue;\n\t\tfree_percpu(port->txqs[queue]->pcpu);\n\t}\n\treturn err;\n}\n\nstatic bool mvpp22_port_has_legacy_tx_irqs(struct device_node *port_node,\n\t\t\t\t\t   unsigned long *flags)\n{\n\tchar *irqs[5] = { \"rx-shared\", \"tx-cpu0\", \"tx-cpu1\", \"tx-cpu2\",\n\t\t\t  \"tx-cpu3\" };\n\tint i;\n\n\tfor (i = 0; i < 5; i++)\n\t\tif (of_property_match_string(port_node, \"interrupt-names\",\n\t\t\t\t\t     irqs[i]) < 0)\n\t\t\treturn false;\n\n\t*flags |= MVPP2_F_DT_COMPAT;\n\treturn true;\n}\n\n \nstatic bool mvpp2_port_has_irqs(struct mvpp2 *priv,\n\t\t\t\tstruct device_node *port_node,\n\t\t\t\tunsigned long *flags)\n{\n\tchar name[5];\n\tint i;\n\n\t \n\tif (!port_node)\n\t\treturn true;\n\n\tif (priv->hw_version == MVPP21)\n\t\treturn false;\n\n\tif (mvpp22_port_has_legacy_tx_irqs(port_node, flags))\n\t\treturn true;\n\n\tfor (i = 0; i < MVPP2_MAX_THREADS; i++) {\n\t\tsnprintf(name, 5, \"hif%d\", i);\n\t\tif (of_property_match_string(port_node, \"interrupt-names\",\n\t\t\t\t\t     name) < 0)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int mvpp2_port_copy_mac_addr(struct net_device *dev, struct mvpp2 *priv,\n\t\t\t\t    struct fwnode_handle *fwnode,\n\t\t\t\t    char **mac_from)\n{\n\tstruct mvpp2_port *port = netdev_priv(dev);\n\tchar hw_mac_addr[ETH_ALEN] = {0};\n\tchar fw_mac_addr[ETH_ALEN];\n\tint ret;\n\n\tif (!fwnode_get_mac_address(fwnode, fw_mac_addr)) {\n\t\t*mac_from = \"firmware node\";\n\t\teth_hw_addr_set(dev, fw_mac_addr);\n\t\treturn 0;\n\t}\n\n\tif (priv->hw_version == MVPP21) {\n\t\tmvpp21_get_mac_address(port, hw_mac_addr);\n\t\tif (is_valid_ether_addr(hw_mac_addr)) {\n\t\t\t*mac_from = \"hardware\";\n\t\t\teth_hw_addr_set(dev, hw_mac_addr);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tret = of_get_mac_address_nvmem(to_of_node(fwnode), fw_mac_addr);\n\tif (ret == -EPROBE_DEFER)\n\t\treturn ret;\n\tif (!ret) {\n\t\t*mac_from = \"nvmem cell\";\n\t\teth_hw_addr_set(dev, fw_mac_addr);\n\t\treturn 0;\n\t}\n\n\t*mac_from = \"random\";\n\teth_hw_addr_random(dev);\n\n\treturn 0;\n}\n\nstatic struct mvpp2_port *mvpp2_phylink_to_port(struct phylink_config *config)\n{\n\treturn container_of(config, struct mvpp2_port, phylink_config);\n}\n\nstatic struct mvpp2_port *mvpp2_pcs_xlg_to_port(struct phylink_pcs *pcs)\n{\n\treturn container_of(pcs, struct mvpp2_port, pcs_xlg);\n}\n\nstatic struct mvpp2_port *mvpp2_pcs_gmac_to_port(struct phylink_pcs *pcs)\n{\n\treturn container_of(pcs, struct mvpp2_port, pcs_gmac);\n}\n\nstatic void mvpp2_xlg_pcs_get_state(struct phylink_pcs *pcs,\n\t\t\t\t    struct phylink_link_state *state)\n{\n\tstruct mvpp2_port *port = mvpp2_pcs_xlg_to_port(pcs);\n\tu32 val;\n\n\tif (port->phy_interface == PHY_INTERFACE_MODE_5GBASER)\n\t\tstate->speed = SPEED_5000;\n\telse\n\t\tstate->speed = SPEED_10000;\n\tstate->duplex = 1;\n\tstate->an_complete = 1;\n\n\tval = readl(port->base + MVPP22_XLG_STATUS);\n\tstate->link = !!(val & MVPP22_XLG_STATUS_LINK_UP);\n\n\tstate->pause = 0;\n\tval = readl(port->base + MVPP22_XLG_CTRL0_REG);\n\tif (val & MVPP22_XLG_CTRL0_TX_FLOW_CTRL_EN)\n\t\tstate->pause |= MLO_PAUSE_TX;\n\tif (val & MVPP22_XLG_CTRL0_RX_FLOW_CTRL_EN)\n\t\tstate->pause |= MLO_PAUSE_RX;\n}\n\nstatic int mvpp2_xlg_pcs_config(struct phylink_pcs *pcs, unsigned int neg_mode,\n\t\t\t\tphy_interface_t interface,\n\t\t\t\tconst unsigned long *advertising,\n\t\t\t\tbool permit_pause_to_mac)\n{\n\treturn 0;\n}\n\nstatic const struct phylink_pcs_ops mvpp2_phylink_xlg_pcs_ops = {\n\t.pcs_get_state = mvpp2_xlg_pcs_get_state,\n\t.pcs_config = mvpp2_xlg_pcs_config,\n};\n\nstatic int mvpp2_gmac_pcs_validate(struct phylink_pcs *pcs,\n\t\t\t\t   unsigned long *supported,\n\t\t\t\t   const struct phylink_link_state *state)\n{\n\t \n\tif (phy_interface_mode_is_8023z(state->interface) &&\n\t    !phylink_test(state->advertising, Autoneg))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void mvpp2_gmac_pcs_get_state(struct phylink_pcs *pcs,\n\t\t\t\t     struct phylink_link_state *state)\n{\n\tstruct mvpp2_port *port = mvpp2_pcs_gmac_to_port(pcs);\n\tu32 val;\n\n\tval = readl(port->base + MVPP2_GMAC_STATUS0);\n\n\tstate->an_complete = !!(val & MVPP2_GMAC_STATUS0_AN_COMPLETE);\n\tstate->link = !!(val & MVPP2_GMAC_STATUS0_LINK_UP);\n\tstate->duplex = !!(val & MVPP2_GMAC_STATUS0_FULL_DUPLEX);\n\n\tswitch (port->phy_interface) {\n\tcase PHY_INTERFACE_MODE_1000BASEX:\n\t\tstate->speed = SPEED_1000;\n\t\tbreak;\n\tcase PHY_INTERFACE_MODE_2500BASEX:\n\t\tstate->speed = SPEED_2500;\n\t\tbreak;\n\tdefault:\n\t\tif (val & MVPP2_GMAC_STATUS0_GMII_SPEED)\n\t\t\tstate->speed = SPEED_1000;\n\t\telse if (val & MVPP2_GMAC_STATUS0_MII_SPEED)\n\t\t\tstate->speed = SPEED_100;\n\t\telse\n\t\t\tstate->speed = SPEED_10;\n\t}\n\n\tstate->pause = 0;\n\tif (val & MVPP2_GMAC_STATUS0_RX_PAUSE)\n\t\tstate->pause |= MLO_PAUSE_RX;\n\tif (val & MVPP2_GMAC_STATUS0_TX_PAUSE)\n\t\tstate->pause |= MLO_PAUSE_TX;\n}\n\nstatic int mvpp2_gmac_pcs_config(struct phylink_pcs *pcs, unsigned int neg_mode,\n\t\t\t\t phy_interface_t interface,\n\t\t\t\t const unsigned long *advertising,\n\t\t\t\t bool permit_pause_to_mac)\n{\n\tstruct mvpp2_port *port = mvpp2_pcs_gmac_to_port(pcs);\n\tu32 mask, val, an, old_an, changed;\n\n\tmask = MVPP2_GMAC_IN_BAND_AUTONEG_BYPASS |\n\t       MVPP2_GMAC_IN_BAND_AUTONEG |\n\t       MVPP2_GMAC_AN_SPEED_EN |\n\t       MVPP2_GMAC_FLOW_CTRL_AUTONEG |\n\t       MVPP2_GMAC_AN_DUPLEX_EN;\n\n\tif (neg_mode == PHYLINK_PCS_NEG_INBAND_ENABLED) {\n\t\tmask |= MVPP2_GMAC_CONFIG_MII_SPEED |\n\t\t\tMVPP2_GMAC_CONFIG_GMII_SPEED |\n\t\t\tMVPP2_GMAC_CONFIG_FULL_DUPLEX;\n\t\tval = MVPP2_GMAC_IN_BAND_AUTONEG;\n\n\t\tif (interface == PHY_INTERFACE_MODE_SGMII) {\n\t\t\t \n\t\t\tval |= MVPP2_GMAC_AN_SPEED_EN |\n\t\t\t       MVPP2_GMAC_AN_DUPLEX_EN;\n\t\t} else {\n\t\t\t \n\t\t\tval |= MVPP2_GMAC_CONFIG_GMII_SPEED |\n\t\t\t       MVPP2_GMAC_CONFIG_FULL_DUPLEX;\n\n\t\t\t \n\t\t\tif (permit_pause_to_mac)\n\t\t\t\tval |= MVPP2_GMAC_FLOW_CTRL_AUTONEG;\n\n\t\t\t \n\t\t\tmask |= MVPP2_GMAC_FC_ADV_EN | MVPP2_GMAC_FC_ADV_ASM_EN;\n\t\t\tif (phylink_test(advertising, Pause))\n\t\t\t\tval |= MVPP2_GMAC_FC_ADV_EN;\n\t\t\tif (phylink_test(advertising, Asym_Pause))\n\t\t\t\tval |= MVPP2_GMAC_FC_ADV_ASM_EN;\n\t\t}\n\t} else {\n\t\tval = 0;\n\t}\n\n\told_an = an = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);\n\tan = (an & ~mask) | val;\n\tchanged = an ^ old_an;\n\tif (changed)\n\t\twritel(an, port->base + MVPP2_GMAC_AUTONEG_CONFIG);\n\n\t \n\treturn changed & (MVPP2_GMAC_FC_ADV_EN | MVPP2_GMAC_FC_ADV_ASM_EN);\n}\n\nstatic void mvpp2_gmac_pcs_an_restart(struct phylink_pcs *pcs)\n{\n\tstruct mvpp2_port *port = mvpp2_pcs_gmac_to_port(pcs);\n\tu32 val = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);\n\n\twritel(val | MVPP2_GMAC_IN_BAND_RESTART_AN,\n\t       port->base + MVPP2_GMAC_AUTONEG_CONFIG);\n\twritel(val & ~MVPP2_GMAC_IN_BAND_RESTART_AN,\n\t       port->base + MVPP2_GMAC_AUTONEG_CONFIG);\n}\n\nstatic const struct phylink_pcs_ops mvpp2_phylink_gmac_pcs_ops = {\n\t.pcs_validate = mvpp2_gmac_pcs_validate,\n\t.pcs_get_state = mvpp2_gmac_pcs_get_state,\n\t.pcs_config = mvpp2_gmac_pcs_config,\n\t.pcs_an_restart = mvpp2_gmac_pcs_an_restart,\n};\n\nstatic void mvpp2_xlg_config(struct mvpp2_port *port, unsigned int mode,\n\t\t\t     const struct phylink_link_state *state)\n{\n\tu32 val;\n\n\tmvpp2_modify(port->base + MVPP22_XLG_CTRL0_REG,\n\t\t     MVPP22_XLG_CTRL0_MAC_RESET_DIS,\n\t\t     MVPP22_XLG_CTRL0_MAC_RESET_DIS);\n\tmvpp2_modify(port->base + MVPP22_XLG_CTRL4_REG,\n\t\t     MVPP22_XLG_CTRL4_MACMODSELECT_GMAC |\n\t\t     MVPP22_XLG_CTRL4_EN_IDLE_CHECK |\n\t\t     MVPP22_XLG_CTRL4_FWD_FC | MVPP22_XLG_CTRL4_FWD_PFC,\n\t\t     MVPP22_XLG_CTRL4_FWD_FC | MVPP22_XLG_CTRL4_FWD_PFC);\n\n\t \n\tdo {\n\t\tval = readl(port->base + MVPP22_XLG_CTRL0_REG);\n\t} while (!(val & MVPP22_XLG_CTRL0_MAC_RESET_DIS));\n}\n\nstatic void mvpp2_gmac_config(struct mvpp2_port *port, unsigned int mode,\n\t\t\t      const struct phylink_link_state *state)\n{\n\tu32 old_ctrl0, ctrl0;\n\tu32 old_ctrl2, ctrl2;\n\tu32 old_ctrl4, ctrl4;\n\n\told_ctrl0 = ctrl0 = readl(port->base + MVPP2_GMAC_CTRL_0_REG);\n\told_ctrl2 = ctrl2 = readl(port->base + MVPP2_GMAC_CTRL_2_REG);\n\told_ctrl4 = ctrl4 = readl(port->base + MVPP22_GMAC_CTRL_4_REG);\n\n\tctrl0 &= ~MVPP2_GMAC_PORT_TYPE_MASK;\n\tctrl2 &= ~(MVPP2_GMAC_INBAND_AN_MASK | MVPP2_GMAC_PCS_ENABLE_MASK | MVPP2_GMAC_FLOW_CTRL_MASK);\n\n\t \n\tif (phy_interface_mode_is_8023z(state->interface)) {\n\t\tctrl2 |= MVPP2_GMAC_PCS_ENABLE_MASK;\n\t\tctrl4 &= ~MVPP22_CTRL4_EXT_PIN_GMII_SEL;\n\t\tctrl4 |= MVPP22_CTRL4_SYNC_BYPASS_DIS |\n\t\t\t MVPP22_CTRL4_DP_CLK_SEL |\n\t\t\t MVPP22_CTRL4_QSGMII_BYPASS_ACTIVE;\n\t} else if (state->interface == PHY_INTERFACE_MODE_SGMII) {\n\t\tctrl2 |= MVPP2_GMAC_PCS_ENABLE_MASK | MVPP2_GMAC_INBAND_AN_MASK;\n\t\tctrl4 &= ~MVPP22_CTRL4_EXT_PIN_GMII_SEL;\n\t\tctrl4 |= MVPP22_CTRL4_SYNC_BYPASS_DIS |\n\t\t\t MVPP22_CTRL4_DP_CLK_SEL |\n\t\t\t MVPP22_CTRL4_QSGMII_BYPASS_ACTIVE;\n\t} else if (phy_interface_mode_is_rgmii(state->interface)) {\n\t\tctrl4 &= ~MVPP22_CTRL4_DP_CLK_SEL;\n\t\tctrl4 |= MVPP22_CTRL4_EXT_PIN_GMII_SEL |\n\t\t\t MVPP22_CTRL4_SYNC_BYPASS_DIS |\n\t\t\t MVPP22_CTRL4_QSGMII_BYPASS_ACTIVE;\n\t}\n\n\t \n\tif (!phylink_autoneg_inband(mode)) {\n\t\t \n\t} else if (state->interface == PHY_INTERFACE_MODE_SGMII) {\n\t\t \n\t} else if (phy_interface_mode_is_8023z(state->interface)) {\n\t\t \n\t\tctrl0 |= MVPP2_GMAC_PORT_TYPE_MASK;\n\t}\n\n\tif (old_ctrl0 != ctrl0)\n\t\twritel(ctrl0, port->base + MVPP2_GMAC_CTRL_0_REG);\n\tif (old_ctrl2 != ctrl2)\n\t\twritel(ctrl2, port->base + MVPP2_GMAC_CTRL_2_REG);\n\tif (old_ctrl4 != ctrl4)\n\t\twritel(ctrl4, port->base + MVPP22_GMAC_CTRL_4_REG);\n}\n\nstatic struct phylink_pcs *mvpp2_select_pcs(struct phylink_config *config,\n\t\t\t\t\t    phy_interface_t interface)\n{\n\tstruct mvpp2_port *port = mvpp2_phylink_to_port(config);\n\n\t \n\tif (mvpp2_is_xlg(interface))\n\t\treturn &port->pcs_xlg;\n\telse\n\t\treturn &port->pcs_gmac;\n}\n\nstatic int mvpp2_mac_prepare(struct phylink_config *config, unsigned int mode,\n\t\t\t     phy_interface_t interface)\n{\n\tstruct mvpp2_port *port = mvpp2_phylink_to_port(config);\n\n\t \n\tif (mvpp2_is_xlg(interface) && port->gop_id != 0) {\n\t\tnetdev_err(port->dev, \"Invalid mode on %s\\n\", port->dev->name);\n\t\treturn -EINVAL;\n\t}\n\n\tif (port->phy_interface != interface ||\n\t    phylink_autoneg_inband(mode)) {\n\t\t \n\t\tmvpp2_modify(port->base + MVPP2_GMAC_AUTONEG_CONFIG,\n\t\t\t     MVPP2_GMAC_FORCE_LINK_PASS |\n\t\t\t     MVPP2_GMAC_FORCE_LINK_DOWN,\n\t\t\t     MVPP2_GMAC_FORCE_LINK_DOWN);\n\n\t\tif (mvpp2_port_supports_xlg(port))\n\t\t\tmvpp2_modify(port->base + MVPP22_XLG_CTRL0_REG,\n\t\t\t\t     MVPP22_XLG_CTRL0_FORCE_LINK_PASS |\n\t\t\t\t     MVPP22_XLG_CTRL0_FORCE_LINK_DOWN,\n\t\t\t\t     MVPP22_XLG_CTRL0_FORCE_LINK_DOWN);\n\t}\n\n\t \n\tmvpp2_port_disable(port);\n\n\tif (port->phy_interface != interface) {\n\t\t \n\t\tmvpp2_modify(port->base + MVPP2_GMAC_CTRL_2_REG,\n\t\t\t     MVPP2_GMAC_PORT_RESET_MASK,\n\t\t\t     MVPP2_GMAC_PORT_RESET_MASK);\n\n\t\tif (port->priv->hw_version >= MVPP22) {\n\t\t\tmvpp22_gop_mask_irq(port);\n\n\t\t\tphy_power_off(port->comphy);\n\n\t\t\t \n\t\t\tmvpp22_mode_reconfigure(port, interface);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void mvpp2_mac_config(struct phylink_config *config, unsigned int mode,\n\t\t\t     const struct phylink_link_state *state)\n{\n\tstruct mvpp2_port *port = mvpp2_phylink_to_port(config);\n\n\t \n\tif (mvpp2_is_xlg(state->interface))\n\t\tmvpp2_xlg_config(port, mode, state);\n\telse if (phy_interface_mode_is_rgmii(state->interface) ||\n\t\t phy_interface_mode_is_8023z(state->interface) ||\n\t\t state->interface == PHY_INTERFACE_MODE_SGMII)\n\t\tmvpp2_gmac_config(port, mode, state);\n\n\tif (port->priv->hw_version == MVPP21 && port->flags & MVPP2_F_LOOPBACK)\n\t\tmvpp2_port_loopback_set(port, state);\n}\n\nstatic int mvpp2_mac_finish(struct phylink_config *config, unsigned int mode,\n\t\t\t    phy_interface_t interface)\n{\n\tstruct mvpp2_port *port = mvpp2_phylink_to_port(config);\n\n\tif (port->priv->hw_version >= MVPP22 &&\n\t    port->phy_interface != interface) {\n\t\tport->phy_interface = interface;\n\n\t\t \n\t\tmvpp22_gop_unmask_irq(port);\n\t}\n\n\tif (!mvpp2_is_xlg(interface)) {\n\t\t \n\t\tmvpp2_modify(port->base + MVPP2_GMAC_CTRL_2_REG,\n\t\t\t     MVPP2_GMAC_PORT_RESET_MASK, 0);\n\n\t\twhile (readl(port->base + MVPP2_GMAC_CTRL_2_REG) &\n\t\t       MVPP2_GMAC_PORT_RESET_MASK)\n\t\t\tcontinue;\n\t}\n\n\tmvpp2_port_enable(port);\n\n\t \n\tif (phylink_autoneg_inband(mode)) {\n\t\tif (mvpp2_is_xlg(interface))\n\t\t\tmvpp2_modify(port->base + MVPP22_XLG_CTRL0_REG,\n\t\t\t\t     MVPP22_XLG_CTRL0_FORCE_LINK_PASS |\n\t\t\t\t     MVPP22_XLG_CTRL0_FORCE_LINK_DOWN, 0);\n\t\telse\n\t\t\tmvpp2_modify(port->base + MVPP2_GMAC_AUTONEG_CONFIG,\n\t\t\t\t     MVPP2_GMAC_FORCE_LINK_PASS |\n\t\t\t\t     MVPP2_GMAC_FORCE_LINK_DOWN, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic void mvpp2_mac_link_up(struct phylink_config *config,\n\t\t\t      struct phy_device *phy,\n\t\t\t      unsigned int mode, phy_interface_t interface,\n\t\t\t      int speed, int duplex,\n\t\t\t      bool tx_pause, bool rx_pause)\n{\n\tstruct mvpp2_port *port = mvpp2_phylink_to_port(config);\n\tu32 val;\n\tint i;\n\n\tif (mvpp2_is_xlg(interface)) {\n\t\tif (!phylink_autoneg_inband(mode)) {\n\t\t\tval = MVPP22_XLG_CTRL0_FORCE_LINK_PASS;\n\t\t\tif (tx_pause)\n\t\t\t\tval |= MVPP22_XLG_CTRL0_TX_FLOW_CTRL_EN;\n\t\t\tif (rx_pause)\n\t\t\t\tval |= MVPP22_XLG_CTRL0_RX_FLOW_CTRL_EN;\n\n\t\t\tmvpp2_modify(port->base + MVPP22_XLG_CTRL0_REG,\n\t\t\t\t     MVPP22_XLG_CTRL0_FORCE_LINK_DOWN |\n\t\t\t\t     MVPP22_XLG_CTRL0_FORCE_LINK_PASS |\n\t\t\t\t     MVPP22_XLG_CTRL0_TX_FLOW_CTRL_EN |\n\t\t\t\t     MVPP22_XLG_CTRL0_RX_FLOW_CTRL_EN, val);\n\t\t}\n\t} else {\n\t\tif (!phylink_autoneg_inband(mode)) {\n\t\t\tval = MVPP2_GMAC_FORCE_LINK_PASS;\n\n\t\t\tif (speed == SPEED_1000 || speed == SPEED_2500)\n\t\t\t\tval |= MVPP2_GMAC_CONFIG_GMII_SPEED;\n\t\t\telse if (speed == SPEED_100)\n\t\t\t\tval |= MVPP2_GMAC_CONFIG_MII_SPEED;\n\n\t\t\tif (duplex == DUPLEX_FULL)\n\t\t\t\tval |= MVPP2_GMAC_CONFIG_FULL_DUPLEX;\n\n\t\t\tmvpp2_modify(port->base + MVPP2_GMAC_AUTONEG_CONFIG,\n\t\t\t\t     MVPP2_GMAC_FORCE_LINK_DOWN |\n\t\t\t\t     MVPP2_GMAC_FORCE_LINK_PASS |\n\t\t\t\t     MVPP2_GMAC_CONFIG_MII_SPEED |\n\t\t\t\t     MVPP2_GMAC_CONFIG_GMII_SPEED |\n\t\t\t\t     MVPP2_GMAC_CONFIG_FULL_DUPLEX, val);\n\t\t}\n\n\t\t \n\t\tval = 0;\n\t\tif (tx_pause)\n\t\t\tval |= MVPP22_CTRL4_TX_FC_EN;\n\t\tif (rx_pause)\n\t\t\tval |= MVPP22_CTRL4_RX_FC_EN;\n\n\t\tmvpp2_modify(port->base + MVPP22_GMAC_CTRL_4_REG,\n\t\t\t     MVPP22_CTRL4_RX_FC_EN | MVPP22_CTRL4_TX_FC_EN,\n\t\t\t     val);\n\t}\n\n\tif (port->priv->global_tx_fc) {\n\t\tport->tx_fc = tx_pause;\n\t\tif (tx_pause)\n\t\t\tmvpp2_rxq_enable_fc(port);\n\t\telse\n\t\t\tmvpp2_rxq_disable_fc(port);\n\t\tif (port->priv->percpu_pools) {\n\t\t\tfor (i = 0; i < port->nrxqs; i++)\n\t\t\t\tmvpp2_bm_pool_update_fc(port, &port->priv->bm_pools[i], tx_pause);\n\t\t} else {\n\t\t\tmvpp2_bm_pool_update_fc(port, port->pool_long, tx_pause);\n\t\t\tmvpp2_bm_pool_update_fc(port, port->pool_short, tx_pause);\n\t\t}\n\t\tif (port->priv->hw_version == MVPP23)\n\t\t\tmvpp23_rx_fifo_fc_en(port->priv, port->id, tx_pause);\n\t}\n\n\tmvpp2_port_enable(port);\n\n\tmvpp2_egress_enable(port);\n\tmvpp2_ingress_enable(port);\n\tnetif_tx_wake_all_queues(port->dev);\n}\n\nstatic void mvpp2_mac_link_down(struct phylink_config *config,\n\t\t\t\tunsigned int mode, phy_interface_t interface)\n{\n\tstruct mvpp2_port *port = mvpp2_phylink_to_port(config);\n\tu32 val;\n\n\tif (!phylink_autoneg_inband(mode)) {\n\t\tif (mvpp2_is_xlg(interface)) {\n\t\t\tval = readl(port->base + MVPP22_XLG_CTRL0_REG);\n\t\t\tval &= ~MVPP22_XLG_CTRL0_FORCE_LINK_PASS;\n\t\t\tval |= MVPP22_XLG_CTRL0_FORCE_LINK_DOWN;\n\t\t\twritel(val, port->base + MVPP22_XLG_CTRL0_REG);\n\t\t} else {\n\t\t\tval = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);\n\t\t\tval &= ~MVPP2_GMAC_FORCE_LINK_PASS;\n\t\t\tval |= MVPP2_GMAC_FORCE_LINK_DOWN;\n\t\t\twritel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);\n\t\t}\n\t}\n\n\tnetif_tx_stop_all_queues(port->dev);\n\tmvpp2_egress_disable(port);\n\tmvpp2_ingress_disable(port);\n\n\tmvpp2_port_disable(port);\n}\n\nstatic const struct phylink_mac_ops mvpp2_phylink_ops = {\n\t.mac_select_pcs = mvpp2_select_pcs,\n\t.mac_prepare = mvpp2_mac_prepare,\n\t.mac_config = mvpp2_mac_config,\n\t.mac_finish = mvpp2_mac_finish,\n\t.mac_link_up = mvpp2_mac_link_up,\n\t.mac_link_down = mvpp2_mac_link_down,\n};\n\n \nstatic void mvpp2_acpi_start(struct mvpp2_port *port)\n{\n\t \n\tstruct phylink_link_state state = {\n\t\t.interface = port->phy_interface,\n\t};\n\tstruct phylink_pcs *pcs;\n\n\tpcs = mvpp2_select_pcs(&port->phylink_config, port->phy_interface);\n\n\tmvpp2_mac_prepare(&port->phylink_config, MLO_AN_INBAND,\n\t\t\t  port->phy_interface);\n\tmvpp2_mac_config(&port->phylink_config, MLO_AN_INBAND, &state);\n\tpcs->ops->pcs_config(pcs, PHYLINK_PCS_NEG_INBAND_ENABLED,\n\t\t\t     port->phy_interface, state.advertising,\n\t\t\t     false);\n\tmvpp2_mac_finish(&port->phylink_config, MLO_AN_INBAND,\n\t\t\t port->phy_interface);\n\tmvpp2_mac_link_up(&port->phylink_config, NULL,\n\t\t\t  MLO_AN_INBAND, port->phy_interface,\n\t\t\t  SPEED_UNKNOWN, DUPLEX_UNKNOWN, false, false);\n}\n\n \nstatic bool mvpp2_use_acpi_compat_mode(struct fwnode_handle *port_fwnode)\n{\n\tif (!is_acpi_node(port_fwnode))\n\t\treturn false;\n\n\treturn (!fwnode_property_present(port_fwnode, \"phy-handle\") &&\n\t\t!fwnode_property_present(port_fwnode, \"managed\") &&\n\t\t!fwnode_get_named_child_node(port_fwnode, \"fixed-link\"));\n}\n\n \nstatic int mvpp2_port_probe(struct platform_device *pdev,\n\t\t\t    struct fwnode_handle *port_fwnode,\n\t\t\t    struct mvpp2 *priv)\n{\n\tstruct phy *comphy = NULL;\n\tstruct mvpp2_port *port;\n\tstruct mvpp2_port_pcpu *port_pcpu;\n\tstruct device_node *port_node = to_of_node(port_fwnode);\n\tnetdev_features_t features;\n\tstruct net_device *dev;\n\tstruct phylink *phylink;\n\tchar *mac_from = \"\";\n\tunsigned int ntxqs, nrxqs, thread;\n\tunsigned long flags = 0;\n\tbool has_tx_irqs;\n\tu32 id;\n\tint phy_mode;\n\tint err, i;\n\n\thas_tx_irqs = mvpp2_port_has_irqs(priv, port_node, &flags);\n\tif (!has_tx_irqs && queue_mode == MVPP2_QDIST_MULTI_MODE) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"not enough IRQs to support multi queue mode\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tntxqs = MVPP2_MAX_TXQ;\n\tnrxqs = mvpp2_get_nrxqs(priv);\n\n\tdev = alloc_etherdev_mqs(sizeof(*port), ntxqs, nrxqs);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tphy_mode = fwnode_get_phy_mode(port_fwnode);\n\tif (phy_mode < 0) {\n\t\tdev_err(&pdev->dev, \"incorrect phy mode\\n\");\n\t\terr = phy_mode;\n\t\tgoto err_free_netdev;\n\t}\n\n\t \n\tif (phy_mode == PHY_INTERFACE_MODE_10GKR)\n\t\tphy_mode = PHY_INTERFACE_MODE_10GBASER;\n\n\tif (port_node) {\n\t\tcomphy = devm_of_phy_get(&pdev->dev, port_node, NULL);\n\t\tif (IS_ERR(comphy)) {\n\t\t\tif (PTR_ERR(comphy) == -EPROBE_DEFER) {\n\t\t\t\terr = -EPROBE_DEFER;\n\t\t\t\tgoto err_free_netdev;\n\t\t\t}\n\t\t\tcomphy = NULL;\n\t\t}\n\t}\n\n\tif (fwnode_property_read_u32(port_fwnode, \"port-id\", &id)) {\n\t\terr = -EINVAL;\n\t\tdev_err(&pdev->dev, \"missing port-id value\\n\");\n\t\tgoto err_free_netdev;\n\t}\n\n\tdev->tx_queue_len = MVPP2_MAX_TXD_MAX;\n\tdev->watchdog_timeo = 5 * HZ;\n\tdev->netdev_ops = &mvpp2_netdev_ops;\n\tdev->ethtool_ops = &mvpp2_eth_tool_ops;\n\n\tport = netdev_priv(dev);\n\tport->dev = dev;\n\tport->fwnode = port_fwnode;\n\tport->ntxqs = ntxqs;\n\tport->nrxqs = nrxqs;\n\tport->priv = priv;\n\tport->has_tx_irqs = has_tx_irqs;\n\tport->flags = flags;\n\n\terr = mvpp2_queue_vectors_init(port, port_node);\n\tif (err)\n\t\tgoto err_free_netdev;\n\n\tif (port_node)\n\t\tport->port_irq = of_irq_get_byname(port_node, \"link\");\n\telse\n\t\tport->port_irq = fwnode_irq_get(port_fwnode, port->nqvecs + 1);\n\tif (port->port_irq == -EPROBE_DEFER) {\n\t\terr = -EPROBE_DEFER;\n\t\tgoto err_deinit_qvecs;\n\t}\n\tif (port->port_irq <= 0)\n\t\t \n\t\tport->port_irq = 0;\n\n\tif (fwnode_property_read_bool(port_fwnode, \"marvell,loopback\"))\n\t\tport->flags |= MVPP2_F_LOOPBACK;\n\n\tport->id = id;\n\tif (priv->hw_version == MVPP21)\n\t\tport->first_rxq = port->id * port->nrxqs;\n\telse\n\t\tport->first_rxq = port->id * priv->max_port_rxqs;\n\n\tport->of_node = port_node;\n\tport->phy_interface = phy_mode;\n\tport->comphy = comphy;\n\n\tif (priv->hw_version == MVPP21) {\n\t\tport->base = devm_platform_ioremap_resource(pdev, 2 + id);\n\t\tif (IS_ERR(port->base)) {\n\t\t\terr = PTR_ERR(port->base);\n\t\t\tgoto err_free_irq;\n\t\t}\n\n\t\tport->stats_base = port->priv->lms_base +\n\t\t\t\t   MVPP21_MIB_COUNTERS_OFFSET +\n\t\t\t\t   port->gop_id * MVPP21_MIB_COUNTERS_PORT_SZ;\n\t} else {\n\t\tif (fwnode_property_read_u32(port_fwnode, \"gop-port-id\",\n\t\t\t\t\t     &port->gop_id)) {\n\t\t\terr = -EINVAL;\n\t\t\tdev_err(&pdev->dev, \"missing gop-port-id value\\n\");\n\t\t\tgoto err_deinit_qvecs;\n\t\t}\n\n\t\tport->base = priv->iface_base + MVPP22_GMAC_BASE(port->gop_id);\n\t\tport->stats_base = port->priv->iface_base +\n\t\t\t\t   MVPP22_MIB_COUNTERS_OFFSET +\n\t\t\t\t   port->gop_id * MVPP22_MIB_COUNTERS_PORT_SZ;\n\n\t\t \n\t\tif (priv->tai)\n\t\t\tport->hwtstamp = true;\n\t}\n\n\t \n\tport->stats = netdev_alloc_pcpu_stats(struct mvpp2_pcpu_stats);\n\tif (!port->stats) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_irq;\n\t}\n\n\tport->ethtool_stats = devm_kcalloc(&pdev->dev,\n\t\t\t\t\t   MVPP2_N_ETHTOOL_STATS(ntxqs, nrxqs),\n\t\t\t\t\t   sizeof(u64), GFP_KERNEL);\n\tif (!port->ethtool_stats) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_stats;\n\t}\n\n\tmutex_init(&port->gather_stats_lock);\n\tINIT_DELAYED_WORK(&port->stats_work, mvpp2_gather_hw_statistics);\n\n\terr = mvpp2_port_copy_mac_addr(dev, priv, port_fwnode, &mac_from);\n\tif (err < 0)\n\t\tgoto err_free_stats;\n\n\tport->tx_ring_size = MVPP2_MAX_TXD_DFLT;\n\tport->rx_ring_size = MVPP2_MAX_RXD_DFLT;\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\n\terr = mvpp2_port_init(port);\n\tif (err < 0) {\n\t\tdev_err(&pdev->dev, \"failed to init port %d\\n\", id);\n\t\tgoto err_free_stats;\n\t}\n\n\tmvpp2_port_periodic_xon_disable(port);\n\n\tmvpp2_mac_reset_assert(port);\n\tmvpp22_pcs_reset_assert(port);\n\n\tport->pcpu = alloc_percpu(struct mvpp2_port_pcpu);\n\tif (!port->pcpu) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_txq_pcpu;\n\t}\n\n\tif (!port->has_tx_irqs) {\n\t\tfor (thread = 0; thread < priv->nthreads; thread++) {\n\t\t\tport_pcpu = per_cpu_ptr(port->pcpu, thread);\n\n\t\t\thrtimer_init(&port_pcpu->tx_done_timer, CLOCK_MONOTONIC,\n\t\t\t\t     HRTIMER_MODE_REL_PINNED_SOFT);\n\t\t\tport_pcpu->tx_done_timer.function = mvpp2_hr_timer_cb;\n\t\t\tport_pcpu->timer_scheduled = false;\n\t\t\tport_pcpu->dev = dev;\n\t\t}\n\t}\n\n\tfeatures = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t   NETIF_F_TSO;\n\tdev->features = features | NETIF_F_RXCSUM;\n\tdev->hw_features |= features | NETIF_F_RXCSUM | NETIF_F_GRO |\n\t\t\t    NETIF_F_HW_VLAN_CTAG_FILTER;\n\n\tif (mvpp22_rss_is_supported(port)) {\n\t\tdev->hw_features |= NETIF_F_RXHASH;\n\t\tdev->features |= NETIF_F_NTUPLE;\n\t}\n\n\tif (!port->priv->percpu_pools)\n\t\tmvpp2_set_hw_csum(port, port->pool_long->id);\n\telse if (port->ntxqs >= num_possible_cpus() * 2)\n\t\tdev->xdp_features = NETDEV_XDP_ACT_BASIC |\n\t\t\t\t    NETDEV_XDP_ACT_REDIRECT |\n\t\t\t\t    NETDEV_XDP_ACT_NDO_XMIT;\n\n\tdev->vlan_features |= features;\n\tnetif_set_tso_max_segs(dev, MVPP2_MAX_TSO_SEGS);\n\n\tdev->priv_flags |= IFF_UNICAST_FLT;\n\n\t \n\tdev->min_mtu = ETH_MIN_MTU;\n\t \n\tdev->max_mtu = MVPP2_BM_JUMBO_PKT_SIZE;\n\tdev->dev.of_node = port_node;\n\n\tport->pcs_gmac.ops = &mvpp2_phylink_gmac_pcs_ops;\n\tport->pcs_gmac.neg_mode = true;\n\tport->pcs_xlg.ops = &mvpp2_phylink_xlg_pcs_ops;\n\tport->pcs_xlg.neg_mode = true;\n\n\tif (!mvpp2_use_acpi_compat_mode(port_fwnode)) {\n\t\tport->phylink_config.dev = &dev->dev;\n\t\tport->phylink_config.type = PHYLINK_NETDEV;\n\t\tport->phylink_config.mac_capabilities =\n\t\t\tMAC_2500FD | MAC_1000FD | MAC_100 | MAC_10;\n\n\t\tif (port->priv->global_tx_fc)\n\t\t\tport->phylink_config.mac_capabilities |=\n\t\t\t\tMAC_SYM_PAUSE | MAC_ASYM_PAUSE;\n\n\t\tif (mvpp2_port_supports_xlg(port)) {\n\t\t\t \n\t\t\tif (comphy) {\n\t\t\t\t__set_bit(PHY_INTERFACE_MODE_5GBASER,\n\t\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t\t\t__set_bit(PHY_INTERFACE_MODE_10GBASER,\n\t\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t\t\t__set_bit(PHY_INTERFACE_MODE_XAUI,\n\t\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t\t} else if (phy_mode == PHY_INTERFACE_MODE_5GBASER) {\n\t\t\t\t__set_bit(PHY_INTERFACE_MODE_5GBASER,\n\t\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t\t} else if (phy_mode == PHY_INTERFACE_MODE_10GBASER) {\n\t\t\t\t__set_bit(PHY_INTERFACE_MODE_10GBASER,\n\t\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t\t} else if (phy_mode == PHY_INTERFACE_MODE_XAUI) {\n\t\t\t\t__set_bit(PHY_INTERFACE_MODE_XAUI,\n\t\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t\t}\n\n\t\t\tif (comphy)\n\t\t\t\tport->phylink_config.mac_capabilities |=\n\t\t\t\t\tMAC_10000FD | MAC_5000FD;\n\t\t\telse if (phy_mode == PHY_INTERFACE_MODE_5GBASER)\n\t\t\t\tport->phylink_config.mac_capabilities |=\n\t\t\t\t\tMAC_5000FD;\n\t\t\telse\n\t\t\t\tport->phylink_config.mac_capabilities |=\n\t\t\t\t\tMAC_10000FD;\n\t\t}\n\n\t\tif (mvpp2_port_supports_rgmii(port))\n\t\t\tphy_interface_set_rgmii(port->phylink_config.supported_interfaces);\n\n\t\tif (comphy) {\n\t\t\t \n\t\t\t__set_bit(PHY_INTERFACE_MODE_SGMII,\n\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t\t__set_bit(PHY_INTERFACE_MODE_1000BASEX,\n\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t\t__set_bit(PHY_INTERFACE_MODE_2500BASEX,\n\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t} else if (phy_mode == PHY_INTERFACE_MODE_2500BASEX) {\n\t\t\t \n\t\t\t__set_bit(PHY_INTERFACE_MODE_2500BASEX,\n\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t} else if (phy_mode == PHY_INTERFACE_MODE_1000BASEX ||\n\t\t\t   phy_mode == PHY_INTERFACE_MODE_SGMII) {\n\t\t\t \n\t\t\t__set_bit(PHY_INTERFACE_MODE_1000BASEX,\n\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t\t__set_bit(PHY_INTERFACE_MODE_SGMII,\n\t\t\t\t  port->phylink_config.supported_interfaces);\n\t\t}\n\n\t\tphylink = phylink_create(&port->phylink_config, port_fwnode,\n\t\t\t\t\t phy_mode, &mvpp2_phylink_ops);\n\t\tif (IS_ERR(phylink)) {\n\t\t\terr = PTR_ERR(phylink);\n\t\t\tgoto err_free_port_pcpu;\n\t\t}\n\t\tport->phylink = phylink;\n\t} else {\n\t\tdev_warn(&pdev->dev, \"Use link irqs for port#%d. FW update required\\n\", port->id);\n\t\tport->phylink = NULL;\n\t}\n\n\t \n\tif (port->comphy) {\n\t\terr = mvpp22_comphy_init(port, port->phy_interface);\n\t\tif (err == 0)\n\t\t\tphy_power_off(port->comphy);\n\t}\n\n\terr = register_netdev(dev);\n\tif (err < 0) {\n\t\tdev_err(&pdev->dev, \"failed to register netdev\\n\");\n\t\tgoto err_phylink;\n\t}\n\tnetdev_info(dev, \"Using %s mac address %pM\\n\", mac_from, dev->dev_addr);\n\n\tpriv->port_list[priv->port_count++] = port;\n\n\treturn 0;\n\nerr_phylink:\n\tif (port->phylink)\n\t\tphylink_destroy(port->phylink);\nerr_free_port_pcpu:\n\tfree_percpu(port->pcpu);\nerr_free_txq_pcpu:\n\tfor (i = 0; i < port->ntxqs; i++)\n\t\tfree_percpu(port->txqs[i]->pcpu);\nerr_free_stats:\n\tfree_percpu(port->stats);\nerr_free_irq:\n\tif (port->port_irq)\n\t\tirq_dispose_mapping(port->port_irq);\nerr_deinit_qvecs:\n\tmvpp2_queue_vectors_deinit(port);\nerr_free_netdev:\n\tfree_netdev(dev);\n\treturn err;\n}\n\n \nstatic void mvpp2_port_remove(struct mvpp2_port *port)\n{\n\tint i;\n\n\tunregister_netdev(port->dev);\n\tif (port->phylink)\n\t\tphylink_destroy(port->phylink);\n\tfree_percpu(port->pcpu);\n\tfree_percpu(port->stats);\n\tfor (i = 0; i < port->ntxqs; i++)\n\t\tfree_percpu(port->txqs[i]->pcpu);\n\tmvpp2_queue_vectors_deinit(port);\n\tif (port->port_irq)\n\t\tirq_dispose_mapping(port->port_irq);\n\tfree_netdev(port->dev);\n}\n\n \nstatic void mvpp2_conf_mbus_windows(const struct mbus_dram_target_info *dram,\n\t\t\t\t    struct mvpp2 *priv)\n{\n\tu32 win_enable;\n\tint i;\n\n\tfor (i = 0; i < 6; i++) {\n\t\tmvpp2_write(priv, MVPP2_WIN_BASE(i), 0);\n\t\tmvpp2_write(priv, MVPP2_WIN_SIZE(i), 0);\n\n\t\tif (i < 4)\n\t\t\tmvpp2_write(priv, MVPP2_WIN_REMAP(i), 0);\n\t}\n\n\twin_enable = 0;\n\n\tfor (i = 0; i < dram->num_cs; i++) {\n\t\tconst struct mbus_dram_window *cs = dram->cs + i;\n\n\t\tmvpp2_write(priv, MVPP2_WIN_BASE(i),\n\t\t\t    (cs->base & 0xffff0000) | (cs->mbus_attr << 8) |\n\t\t\t    dram->mbus_dram_target_id);\n\n\t\tmvpp2_write(priv, MVPP2_WIN_SIZE(i),\n\t\t\t    (cs->size - 1) & 0xffff0000);\n\n\t\twin_enable |= (1 << i);\n\t}\n\n\tmvpp2_write(priv, MVPP2_BASE_ADDR_ENABLE, win_enable);\n}\n\n \nstatic void mvpp2_rx_fifo_init(struct mvpp2 *priv)\n{\n\tint port;\n\n\tfor (port = 0; port < MVPP2_MAX_PORTS; port++) {\n\t\tmvpp2_write(priv, MVPP2_RX_DATA_FIFO_SIZE_REG(port),\n\t\t\t    MVPP2_RX_FIFO_PORT_DATA_SIZE_4KB);\n\t\tmvpp2_write(priv, MVPP2_RX_ATTR_FIFO_SIZE_REG(port),\n\t\t\t    MVPP2_RX_FIFO_PORT_ATTR_SIZE_4KB);\n\t}\n\n\tmvpp2_write(priv, MVPP2_RX_MIN_PKT_SIZE_REG,\n\t\t    MVPP2_RX_FIFO_PORT_MIN_PKT);\n\tmvpp2_write(priv, MVPP2_RX_FIFO_INIT_REG, 0x1);\n}\n\nstatic void mvpp22_rx_fifo_set_hw(struct mvpp2 *priv, int port, int data_size)\n{\n\tint attr_size = MVPP2_RX_FIFO_PORT_ATTR_SIZE(data_size);\n\n\tmvpp2_write(priv, MVPP2_RX_DATA_FIFO_SIZE_REG(port), data_size);\n\tmvpp2_write(priv, MVPP2_RX_ATTR_FIFO_SIZE_REG(port), attr_size);\n}\n\n \nstatic void mvpp22_rx_fifo_init(struct mvpp2 *priv)\n{\n\tint remaining_ports_count;\n\tunsigned long port_map;\n\tint size_remainder;\n\tint port, size;\n\n\t \n\tmvpp22_rx_fifo_set_hw(priv, MVPP2_LOOPBACK_PORT_INDEX,\n\t\t\t      MVPP2_RX_FIFO_PORT_DATA_SIZE_4KB);\n\tport_map = priv->port_map & ~BIT(MVPP2_LOOPBACK_PORT_INDEX);\n\n\t \n\tfor_each_clear_bit(port, &port_map, MVPP2_LOOPBACK_PORT_INDEX)\n\t\tmvpp22_rx_fifo_set_hw(priv, port, 0);\n\n\t \n\tsize_remainder = MVPP2_RX_FIFO_PORT_DATA_SIZE_44KB;\n\tremaining_ports_count = hweight_long(port_map);\n\n\tfor_each_set_bit(port, &port_map, MVPP2_LOOPBACK_PORT_INDEX) {\n\t\tif (remaining_ports_count == 1)\n\t\t\tsize = size_remainder;\n\t\telse if (port == 0)\n\t\t\tsize = max(size_remainder / remaining_ports_count,\n\t\t\t\t   MVPP2_RX_FIFO_PORT_DATA_SIZE_32KB);\n\t\telse if (port == 1)\n\t\t\tsize = max(size_remainder / remaining_ports_count,\n\t\t\t\t   MVPP2_RX_FIFO_PORT_DATA_SIZE_8KB);\n\t\telse\n\t\t\tsize = size_remainder / remaining_ports_count;\n\n\t\tsize_remainder -= size;\n\t\tremaining_ports_count--;\n\n\t\tmvpp22_rx_fifo_set_hw(priv, port, size);\n\t}\n\n\tmvpp2_write(priv, MVPP2_RX_MIN_PKT_SIZE_REG,\n\t\t    MVPP2_RX_FIFO_PORT_MIN_PKT);\n\tmvpp2_write(priv, MVPP2_RX_FIFO_INIT_REG, 0x1);\n}\n\n \nstatic void mvpp23_rx_fifo_fc_set_tresh(struct mvpp2 *priv)\n{\n\tint port, val;\n\n\t \n\n\t \n\tfor (port = 0; port < (MVPP2_MAX_PORTS - 1); port++) {\n\t\tif (port == 0) {\n\t\t\tval = (MVPP23_PORT0_FIFO_TRSH / MVPP2_RX_FC_TRSH_UNIT)\n\t\t\t\t<< MVPP2_RX_FC_TRSH_OFFS;\n\t\t\tval &= MVPP2_RX_FC_TRSH_MASK;\n\t\t\tmvpp2_write(priv, MVPP2_RX_FC_REG(port), val);\n\t\t} else if (port == 1) {\n\t\t\tval = (MVPP23_PORT1_FIFO_TRSH / MVPP2_RX_FC_TRSH_UNIT)\n\t\t\t\t<< MVPP2_RX_FC_TRSH_OFFS;\n\t\t\tval &= MVPP2_RX_FC_TRSH_MASK;\n\t\t\tmvpp2_write(priv, MVPP2_RX_FC_REG(port), val);\n\t\t} else {\n\t\t\tval = (MVPP23_PORT2_FIFO_TRSH / MVPP2_RX_FC_TRSH_UNIT)\n\t\t\t\t<< MVPP2_RX_FC_TRSH_OFFS;\n\t\t\tval &= MVPP2_RX_FC_TRSH_MASK;\n\t\t\tmvpp2_write(priv, MVPP2_RX_FC_REG(port), val);\n\t\t}\n\t}\n}\n\n \nvoid mvpp23_rx_fifo_fc_en(struct mvpp2 *priv, int port, bool en)\n{\n\tint val;\n\n\tval = mvpp2_read(priv, MVPP2_RX_FC_REG(port));\n\n\tif (en)\n\t\tval |= MVPP2_RX_FC_EN;\n\telse\n\t\tval &= ~MVPP2_RX_FC_EN;\n\n\tmvpp2_write(priv, MVPP2_RX_FC_REG(port), val);\n}\n\nstatic void mvpp22_tx_fifo_set_hw(struct mvpp2 *priv, int port, int size)\n{\n\tint threshold = MVPP2_TX_FIFO_THRESHOLD(size);\n\n\tmvpp2_write(priv, MVPP22_TX_FIFO_SIZE_REG(port), size);\n\tmvpp2_write(priv, MVPP22_TX_FIFO_THRESH_REG(port), threshold);\n}\n\n \nstatic void mvpp22_tx_fifo_init(struct mvpp2 *priv)\n{\n\tint remaining_ports_count;\n\tunsigned long port_map;\n\tint size_remainder;\n\tint port, size;\n\n\t \n\tmvpp22_tx_fifo_set_hw(priv, MVPP2_LOOPBACK_PORT_INDEX,\n\t\t\t      MVPP22_TX_FIFO_DATA_SIZE_1KB);\n\tport_map = priv->port_map & ~BIT(MVPP2_LOOPBACK_PORT_INDEX);\n\n\t \n\tfor_each_clear_bit(port, &port_map, MVPP2_LOOPBACK_PORT_INDEX)\n\t\tmvpp22_tx_fifo_set_hw(priv, port, 0);\n\n\t \n\tsize_remainder = MVPP22_TX_FIFO_DATA_SIZE_18KB;\n\tremaining_ports_count = hweight_long(port_map);\n\n\tfor_each_set_bit(port, &port_map, MVPP2_LOOPBACK_PORT_INDEX) {\n\t\tif (remaining_ports_count == 1)\n\t\t\tsize = min(size_remainder,\n\t\t\t\t   MVPP22_TX_FIFO_DATA_SIZE_10KB);\n\t\telse if (port == 0)\n\t\t\tsize = MVPP22_TX_FIFO_DATA_SIZE_10KB;\n\t\telse\n\t\t\tsize = size_remainder / remaining_ports_count;\n\n\t\tsize_remainder -= size;\n\t\tremaining_ports_count--;\n\n\t\tmvpp22_tx_fifo_set_hw(priv, port, size);\n\t}\n}\n\nstatic void mvpp2_axi_init(struct mvpp2 *priv)\n{\n\tu32 val, rdval, wrval;\n\n\tmvpp2_write(priv, MVPP22_BM_ADDR_HIGH_RLS_REG, 0x0);\n\n\t \n\n\trdval = MVPP22_AXI_CODE_CACHE_RD_CACHE\n\t\t<< MVPP22_AXI_ATTR_CACHE_OFFS;\n\trdval |= MVPP22_AXI_CODE_DOMAIN_OUTER_DOM\n\t\t<< MVPP22_AXI_ATTR_DOMAIN_OFFS;\n\n\twrval = MVPP22_AXI_CODE_CACHE_WR_CACHE\n\t\t<< MVPP22_AXI_ATTR_CACHE_OFFS;\n\twrval |= MVPP22_AXI_CODE_DOMAIN_OUTER_DOM\n\t\t<< MVPP22_AXI_ATTR_DOMAIN_OFFS;\n\n\t \n\tmvpp2_write(priv, MVPP22_AXI_BM_WR_ATTR_REG, wrval);\n\tmvpp2_write(priv, MVPP22_AXI_BM_RD_ATTR_REG, rdval);\n\n\t \n\tmvpp2_write(priv, MVPP22_AXI_AGGRQ_DESCR_RD_ATTR_REG, rdval);\n\tmvpp2_write(priv, MVPP22_AXI_TXQ_DESCR_WR_ATTR_REG, wrval);\n\tmvpp2_write(priv, MVPP22_AXI_TXQ_DESCR_RD_ATTR_REG, rdval);\n\tmvpp2_write(priv, MVPP22_AXI_RXQ_DESCR_WR_ATTR_REG, wrval);\n\n\t \n\tmvpp2_write(priv, MVPP22_AXI_TX_DATA_RD_ATTR_REG, rdval);\n\tmvpp2_write(priv, MVPP22_AXI_RX_DATA_WR_ATTR_REG, wrval);\n\n\tval = MVPP22_AXI_CODE_CACHE_NON_CACHE\n\t\t<< MVPP22_AXI_CODE_CACHE_OFFS;\n\tval |= MVPP22_AXI_CODE_DOMAIN_SYSTEM\n\t\t<< MVPP22_AXI_CODE_DOMAIN_OFFS;\n\tmvpp2_write(priv, MVPP22_AXI_RD_NORMAL_CODE_REG, val);\n\tmvpp2_write(priv, MVPP22_AXI_WR_NORMAL_CODE_REG, val);\n\n\tval = MVPP22_AXI_CODE_CACHE_RD_CACHE\n\t\t<< MVPP22_AXI_CODE_CACHE_OFFS;\n\tval |= MVPP22_AXI_CODE_DOMAIN_OUTER_DOM\n\t\t<< MVPP22_AXI_CODE_DOMAIN_OFFS;\n\n\tmvpp2_write(priv, MVPP22_AXI_RD_SNOOP_CODE_REG, val);\n\n\tval = MVPP22_AXI_CODE_CACHE_WR_CACHE\n\t\t<< MVPP22_AXI_CODE_CACHE_OFFS;\n\tval |= MVPP22_AXI_CODE_DOMAIN_OUTER_DOM\n\t\t<< MVPP22_AXI_CODE_DOMAIN_OFFS;\n\n\tmvpp2_write(priv, MVPP22_AXI_WR_SNOOP_CODE_REG, val);\n}\n\n \nstatic int mvpp2_init(struct platform_device *pdev, struct mvpp2 *priv)\n{\n\tconst struct mbus_dram_target_info *dram_target_info;\n\tint err, i;\n\tu32 val;\n\n\t \n\tdram_target_info = mv_mbus_dram_info();\n\tif (dram_target_info)\n\t\tmvpp2_conf_mbus_windows(dram_target_info, priv);\n\n\tif (priv->hw_version >= MVPP22)\n\t\tmvpp2_axi_init(priv);\n\n\t \n\tif (priv->hw_version == MVPP21) {\n\t\tval = readl(priv->lms_base + MVPP2_PHY_AN_CFG0_REG);\n\t\tval |= MVPP2_PHY_AN_STOP_SMI0_MASK;\n\t\twritel(val, priv->lms_base + MVPP2_PHY_AN_CFG0_REG);\n\t} else {\n\t\tval = readl(priv->iface_base + MVPP22_SMI_MISC_CFG_REG);\n\t\tval &= ~MVPP22_SMI_POLLING_EN;\n\t\twritel(val, priv->iface_base + MVPP22_SMI_MISC_CFG_REG);\n\t}\n\n\t \n\tpriv->aggr_txqs = devm_kcalloc(&pdev->dev, MVPP2_MAX_THREADS,\n\t\t\t\t       sizeof(*priv->aggr_txqs),\n\t\t\t\t       GFP_KERNEL);\n\tif (!priv->aggr_txqs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < MVPP2_MAX_THREADS; i++) {\n\t\tpriv->aggr_txqs[i].id = i;\n\t\tpriv->aggr_txqs[i].size = MVPP2_AGGR_TXQ_SIZE;\n\t\terr = mvpp2_aggr_txq_init(pdev, &priv->aggr_txqs[i], i, priv);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\t \n\tif (priv->hw_version == MVPP21) {\n\t\tmvpp2_rx_fifo_init(priv);\n\t} else {\n\t\tmvpp22_rx_fifo_init(priv);\n\t\tmvpp22_tx_fifo_init(priv);\n\t\tif (priv->hw_version == MVPP23)\n\t\t\tmvpp23_rx_fifo_fc_set_tresh(priv);\n\t}\n\n\tif (priv->hw_version == MVPP21)\n\t\twritel(MVPP2_EXT_GLOBAL_CTRL_DEFAULT,\n\t\t       priv->lms_base + MVPP2_MNG_EXTENDED_GLOBAL_CTRL_REG);\n\n\t \n\tmvpp2_write(priv, MVPP2_TX_SNOOP_REG, 0x1);\n\n\t \n\terr = mvpp2_bm_init(&pdev->dev, priv);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\terr = mvpp2_prs_default_init(pdev, priv);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tmvpp2_cls_init(priv);\n\n\treturn 0;\n}\n\nstatic int mvpp2_get_sram(struct platform_device *pdev,\n\t\t\t  struct mvpp2 *priv)\n{\n\tstruct resource *res;\n\tvoid __iomem *base;\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 2);\n\tif (!res) {\n\t\tif (has_acpi_companion(&pdev->dev))\n\t\t\tdev_warn(&pdev->dev, \"ACPI is too old, Flow control not supported\\n\");\n\t\telse\n\t\t\tdev_warn(&pdev->dev, \"DT is too old, Flow control not supported\\n\");\n\t\treturn 0;\n\t}\n\n\tbase = devm_ioremap_resource(&pdev->dev, res);\n\tif (IS_ERR(base))\n\t\treturn PTR_ERR(base);\n\n\tpriv->cm3_base = base;\n\treturn 0;\n}\n\nstatic int mvpp2_probe(struct platform_device *pdev)\n{\n\tstruct fwnode_handle *fwnode = pdev->dev.fwnode;\n\tstruct fwnode_handle *port_fwnode;\n\tstruct mvpp2 *priv;\n\tstruct resource *res;\n\tvoid __iomem *base;\n\tint i, shared;\n\tint err;\n\n\tpriv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tpriv->hw_version = (unsigned long)device_get_match_data(&pdev->dev);\n\n\t \n\tif (priv->hw_version == MVPP21)\n\t\tqueue_mode = MVPP2_QDIST_SINGLE_MODE;\n\n\tbase = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(base))\n\t\treturn PTR_ERR(base);\n\n\tif (priv->hw_version == MVPP21) {\n\t\tpriv->lms_base = devm_platform_ioremap_resource(pdev, 1);\n\t\tif (IS_ERR(priv->lms_base))\n\t\t\treturn PTR_ERR(priv->lms_base);\n\t} else {\n\t\tres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\n\t\tif (!res) {\n\t\t\tdev_err(&pdev->dev, \"Invalid resource\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (has_acpi_companion(&pdev->dev)) {\n\t\t\t \n\t\t\trelease_resource(res);\n\t\t}\n\t\tpriv->iface_base = devm_ioremap_resource(&pdev->dev, res);\n\t\tif (IS_ERR(priv->iface_base))\n\t\t\treturn PTR_ERR(priv->iface_base);\n\n\t\t \n\t\terr = mvpp2_get_sram(pdev, priv);\n\t\tif (err)\n\t\t\tdev_warn(&pdev->dev, \"Fail to alloc CM3 SRAM\\n\");\n\n\t\t \n\t\tif (priv->cm3_base)\n\t\t\tpriv->global_tx_fc = true;\n\t}\n\n\tif (priv->hw_version >= MVPP22 && dev_of_node(&pdev->dev)) {\n\t\tpriv->sysctrl_base =\n\t\t\tsyscon_regmap_lookup_by_phandle(pdev->dev.of_node,\n\t\t\t\t\t\t\t\"marvell,system-controller\");\n\t\tif (IS_ERR(priv->sysctrl_base))\n\t\t\t \n\t\t\tpriv->sysctrl_base = NULL;\n\t}\n\n\tif (priv->hw_version >= MVPP22 &&\n\t    mvpp2_get_nrxqs(priv) * 2 <= MVPP2_BM_MAX_POOLS)\n\t\tpriv->percpu_pools = 1;\n\n\tmvpp2_setup_bm_pool();\n\n\n\tpriv->nthreads = min_t(unsigned int, num_present_cpus(),\n\t\t\t       MVPP2_MAX_THREADS);\n\n\tshared = num_present_cpus() - priv->nthreads;\n\tif (shared > 0)\n\t\tbitmap_set(&priv->lock_map, 0,\n\t\t\t    min_t(int, shared, MVPP2_MAX_THREADS));\n\n\tfor (i = 0; i < MVPP2_MAX_THREADS; i++) {\n\t\tu32 addr_space_sz;\n\n\t\taddr_space_sz = (priv->hw_version == MVPP21 ?\n\t\t\t\t MVPP21_ADDR_SPACE_SZ : MVPP22_ADDR_SPACE_SZ);\n\t\tpriv->swth_base[i] = base + i * addr_space_sz;\n\t}\n\n\tif (priv->hw_version == MVPP21)\n\t\tpriv->max_port_rxqs = 8;\n\telse\n\t\tpriv->max_port_rxqs = 32;\n\n\tif (dev_of_node(&pdev->dev)) {\n\t\tpriv->pp_clk = devm_clk_get(&pdev->dev, \"pp_clk\");\n\t\tif (IS_ERR(priv->pp_clk))\n\t\t\treturn PTR_ERR(priv->pp_clk);\n\t\terr = clk_prepare_enable(priv->pp_clk);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tpriv->gop_clk = devm_clk_get(&pdev->dev, \"gop_clk\");\n\t\tif (IS_ERR(priv->gop_clk)) {\n\t\t\terr = PTR_ERR(priv->gop_clk);\n\t\t\tgoto err_pp_clk;\n\t\t}\n\t\terr = clk_prepare_enable(priv->gop_clk);\n\t\tif (err < 0)\n\t\t\tgoto err_pp_clk;\n\n\t\tif (priv->hw_version >= MVPP22) {\n\t\t\tpriv->mg_clk = devm_clk_get(&pdev->dev, \"mg_clk\");\n\t\t\tif (IS_ERR(priv->mg_clk)) {\n\t\t\t\terr = PTR_ERR(priv->mg_clk);\n\t\t\t\tgoto err_gop_clk;\n\t\t\t}\n\n\t\t\terr = clk_prepare_enable(priv->mg_clk);\n\t\t\tif (err < 0)\n\t\t\t\tgoto err_gop_clk;\n\n\t\t\tpriv->mg_core_clk = devm_clk_get_optional(&pdev->dev, \"mg_core_clk\");\n\t\t\tif (IS_ERR(priv->mg_core_clk)) {\n\t\t\t\terr = PTR_ERR(priv->mg_core_clk);\n\t\t\t\tgoto err_mg_clk;\n\t\t\t}\n\n\t\t\terr = clk_prepare_enable(priv->mg_core_clk);\n\t\t\tif (err < 0)\n\t\t\t\tgoto err_mg_clk;\n\t\t}\n\n\t\tpriv->axi_clk = devm_clk_get_optional(&pdev->dev, \"axi_clk\");\n\t\tif (IS_ERR(priv->axi_clk)) {\n\t\t\terr = PTR_ERR(priv->axi_clk);\n\t\t\tgoto err_mg_core_clk;\n\t\t}\n\n\t\terr = clk_prepare_enable(priv->axi_clk);\n\t\tif (err < 0)\n\t\t\tgoto err_mg_core_clk;\n\n\t\t \n\t\tpriv->tclk = clk_get_rate(priv->pp_clk);\n\t} else {\n\t\terr = device_property_read_u32(&pdev->dev, \"clock-frequency\", &priv->tclk);\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev, \"missing clock-frequency value\\n\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (priv->hw_version >= MVPP22) {\n\t\terr = dma_set_mask(&pdev->dev, MVPP2_DESC_DMA_MASK);\n\t\tif (err)\n\t\t\tgoto err_axi_clk;\n\t\t \n\t\terr = dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (err)\n\t\t\tgoto err_axi_clk;\n\t}\n\n\t \n\tfwnode_for_each_available_child_node(fwnode, port_fwnode) {\n\t\tif (!fwnode_property_read_u32(port_fwnode, \"port-id\", &i))\n\t\t\tpriv->port_map |= BIT(i);\n\t}\n\n\tif (mvpp2_read(priv, MVPP2_VER_ID_REG) == MVPP2_VER_PP23)\n\t\tpriv->hw_version = MVPP23;\n\n\t \n\tspin_lock_init(&priv->mss_spinlock);\n\n\t \n\terr = mvpp2_init(pdev, priv);\n\tif (err < 0) {\n\t\tdev_err(&pdev->dev, \"failed to initialize controller\\n\");\n\t\tgoto err_axi_clk;\n\t}\n\n\terr = mvpp22_tai_probe(&pdev->dev, priv);\n\tif (err < 0)\n\t\tgoto err_axi_clk;\n\n\t \n\tfwnode_for_each_available_child_node(fwnode, port_fwnode) {\n\t\terr = mvpp2_port_probe(pdev, port_fwnode, priv);\n\t\tif (err < 0)\n\t\t\tgoto err_port_probe;\n\t}\n\n\tif (priv->port_count == 0) {\n\t\tdev_err(&pdev->dev, \"no ports enabled\\n\");\n\t\terr = -ENODEV;\n\t\tgoto err_axi_clk;\n\t}\n\n\t \n\tsnprintf(priv->queue_name, sizeof(priv->queue_name),\n\t\t \"stats-wq-%s%s\", netdev_name(priv->port_list[0]->dev),\n\t\t priv->port_count > 1 ? \"+\" : \"\");\n\tpriv->stats_queue = create_singlethread_workqueue(priv->queue_name);\n\tif (!priv->stats_queue) {\n\t\terr = -ENOMEM;\n\t\tgoto err_port_probe;\n\t}\n\n\tif (priv->global_tx_fc && priv->hw_version >= MVPP22) {\n\t\terr = mvpp2_enable_global_fc(priv);\n\t\tif (err)\n\t\t\tdev_warn(&pdev->dev, \"Minimum of CM3 firmware 18.09 and chip revision B0 required for flow control\\n\");\n\t}\n\n\tmvpp2_dbgfs_init(priv, pdev->name);\n\n\tplatform_set_drvdata(pdev, priv);\n\treturn 0;\n\nerr_port_probe:\n\tfwnode_handle_put(port_fwnode);\n\n\ti = 0;\n\tfwnode_for_each_available_child_node(fwnode, port_fwnode) {\n\t\tif (priv->port_list[i])\n\t\t\tmvpp2_port_remove(priv->port_list[i]);\n\t\ti++;\n\t}\nerr_axi_clk:\n\tclk_disable_unprepare(priv->axi_clk);\nerr_mg_core_clk:\n\tclk_disable_unprepare(priv->mg_core_clk);\nerr_mg_clk:\n\tclk_disable_unprepare(priv->mg_clk);\nerr_gop_clk:\n\tclk_disable_unprepare(priv->gop_clk);\nerr_pp_clk:\n\tclk_disable_unprepare(priv->pp_clk);\n\treturn err;\n}\n\nstatic int mvpp2_remove(struct platform_device *pdev)\n{\n\tstruct mvpp2 *priv = platform_get_drvdata(pdev);\n\tstruct fwnode_handle *fwnode = pdev->dev.fwnode;\n\tint i = 0, poolnum = MVPP2_BM_POOLS_NUM;\n\tstruct fwnode_handle *port_fwnode;\n\n\tmvpp2_dbgfs_cleanup(priv);\n\n\tfwnode_for_each_available_child_node(fwnode, port_fwnode) {\n\t\tif (priv->port_list[i]) {\n\t\t\tmutex_destroy(&priv->port_list[i]->gather_stats_lock);\n\t\t\tmvpp2_port_remove(priv->port_list[i]);\n\t\t}\n\t\ti++;\n\t}\n\n\tdestroy_workqueue(priv->stats_queue);\n\n\tif (priv->percpu_pools)\n\t\tpoolnum = mvpp2_get_nrxqs(priv) * 2;\n\n\tfor (i = 0; i < poolnum; i++) {\n\t\tstruct mvpp2_bm_pool *bm_pool = &priv->bm_pools[i];\n\n\t\tmvpp2_bm_pool_destroy(&pdev->dev, priv, bm_pool);\n\t}\n\n\tfor (i = 0; i < MVPP2_MAX_THREADS; i++) {\n\t\tstruct mvpp2_tx_queue *aggr_txq = &priv->aggr_txqs[i];\n\n\t\tdma_free_coherent(&pdev->dev,\n\t\t\t\t  MVPP2_AGGR_TXQ_SIZE * MVPP2_DESC_ALIGNED_SIZE,\n\t\t\t\t  aggr_txq->descs,\n\t\t\t\t  aggr_txq->descs_dma);\n\t}\n\n\tif (is_acpi_node(port_fwnode))\n\t\treturn 0;\n\n\tclk_disable_unprepare(priv->axi_clk);\n\tclk_disable_unprepare(priv->mg_core_clk);\n\tclk_disable_unprepare(priv->mg_clk);\n\tclk_disable_unprepare(priv->pp_clk);\n\tclk_disable_unprepare(priv->gop_clk);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id mvpp2_match[] = {\n\t{\n\t\t.compatible = \"marvell,armada-375-pp2\",\n\t\t.data = (void *)MVPP21,\n\t},\n\t{\n\t\t.compatible = \"marvell,armada-7k-pp22\",\n\t\t.data = (void *)MVPP22,\n\t},\n\t{ }\n};\nMODULE_DEVICE_TABLE(of, mvpp2_match);\n\n#ifdef CONFIG_ACPI\nstatic const struct acpi_device_id mvpp2_acpi_match[] = {\n\t{ \"MRVL0110\", MVPP22 },\n\t{ },\n};\nMODULE_DEVICE_TABLE(acpi, mvpp2_acpi_match);\n#endif\n\nstatic struct platform_driver mvpp2_driver = {\n\t.probe = mvpp2_probe,\n\t.remove = mvpp2_remove,\n\t.driver = {\n\t\t.name = MVPP2_DRIVER_NAME,\n\t\t.of_match_table = mvpp2_match,\n\t\t.acpi_match_table = ACPI_PTR(mvpp2_acpi_match),\n\t},\n};\n\nstatic int __init mvpp2_driver_init(void)\n{\n\treturn platform_driver_register(&mvpp2_driver);\n}\nmodule_init(mvpp2_driver_init);\n\nstatic void __exit mvpp2_driver_exit(void)\n{\n\tplatform_driver_unregister(&mvpp2_driver);\n\tmvpp2_dbgfs_exit();\n}\nmodule_exit(mvpp2_driver_exit);\n\nMODULE_DESCRIPTION(\"Marvell PPv2 Ethernet Driver - www.marvell.com\");\nMODULE_AUTHOR(\"Marcin Wojtas <mw@semihalf.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}