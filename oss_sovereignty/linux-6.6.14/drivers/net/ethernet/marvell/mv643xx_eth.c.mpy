{
  "module_name": "mv643xx_eth.c",
  "hash_id": "4a30ccc3bca38100b0b124bdbc16ac0b98c43d33a96020e7d6701ee5048efd61",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/marvell/mv643xx_eth.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/init.h>\n#include <linux/dma-mapping.h>\n#include <linux/in.h>\n#include <linux/ip.h>\n#include <net/tso.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/etherdevice.h>\n#include <linux/delay.h>\n#include <linux/ethtool.h>\n#include <linux/platform_device.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/spinlock.h>\n#include <linux/workqueue.h>\n#include <linux/phy.h>\n#include <linux/mv643xx_eth.h>\n#include <linux/io.h>\n#include <linux/interrupt.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/clk.h>\n#include <linux/of.h>\n#include <linux/of_irq.h>\n#include <linux/of_net.h>\n#include <linux/of_mdio.h>\n\nstatic char mv643xx_eth_driver_name[] = \"mv643xx_eth\";\nstatic char mv643xx_eth_driver_version[] = \"1.4\";\n\n\n \n#define PHY_ADDR\t\t\t0x0000\n#define WINDOW_BASE(w)\t\t\t(0x0200 + ((w) << 3))\n#define WINDOW_SIZE(w)\t\t\t(0x0204 + ((w) << 3))\n#define WINDOW_REMAP_HIGH(w)\t\t(0x0280 + ((w) << 2))\n#define WINDOW_BAR_ENABLE\t\t0x0290\n#define WINDOW_PROTECT(w)\t\t(0x0294 + ((w) << 4))\n\n \n#define PORT_CONFIG\t\t\t0x0000\n#define  UNICAST_PROMISCUOUS_MODE\t0x00000001\n#define PORT_CONFIG_EXT\t\t\t0x0004\n#define MAC_ADDR_LOW\t\t\t0x0014\n#define MAC_ADDR_HIGH\t\t\t0x0018\n#define SDMA_CONFIG\t\t\t0x001c\n#define  TX_BURST_SIZE_16_64BIT\t\t0x01000000\n#define  TX_BURST_SIZE_4_64BIT\t\t0x00800000\n#define  BLM_TX_NO_SWAP\t\t\t0x00000020\n#define  BLM_RX_NO_SWAP\t\t\t0x00000010\n#define  RX_BURST_SIZE_16_64BIT\t\t0x00000008\n#define  RX_BURST_SIZE_4_64BIT\t\t0x00000004\n#define PORT_SERIAL_CONTROL\t\t0x003c\n#define  SET_MII_SPEED_TO_100\t\t0x01000000\n#define  SET_GMII_SPEED_TO_1000\t\t0x00800000\n#define  SET_FULL_DUPLEX_MODE\t\t0x00200000\n#define  MAX_RX_PACKET_9700BYTE\t\t0x000a0000\n#define  DISABLE_AUTO_NEG_SPEED_GMII\t0x00002000\n#define  DO_NOT_FORCE_LINK_FAIL\t\t0x00000400\n#define  SERIAL_PORT_CONTROL_RESERVED\t0x00000200\n#define  DISABLE_AUTO_NEG_FOR_FLOW_CTRL\t0x00000008\n#define  DISABLE_AUTO_NEG_FOR_DUPLEX\t0x00000004\n#define  FORCE_LINK_PASS\t\t0x00000002\n#define  SERIAL_PORT_ENABLE\t\t0x00000001\n#define PORT_STATUS\t\t\t0x0044\n#define  TX_FIFO_EMPTY\t\t\t0x00000400\n#define  TX_IN_PROGRESS\t\t\t0x00000080\n#define  PORT_SPEED_MASK\t\t0x00000030\n#define  PORT_SPEED_1000\t\t0x00000010\n#define  PORT_SPEED_100\t\t\t0x00000020\n#define  PORT_SPEED_10\t\t\t0x00000000\n#define  FLOW_CONTROL_ENABLED\t\t0x00000008\n#define  FULL_DUPLEX\t\t\t0x00000004\n#define  LINK_UP\t\t\t0x00000002\n#define TXQ_COMMAND\t\t\t0x0048\n#define TXQ_FIX_PRIO_CONF\t\t0x004c\n#define PORT_SERIAL_CONTROL1\t\t0x004c\n#define  RGMII_EN\t\t\t0x00000008\n#define  CLK125_BYPASS_EN\t\t0x00000010\n#define TX_BW_RATE\t\t\t0x0050\n#define TX_BW_MTU\t\t\t0x0058\n#define TX_BW_BURST\t\t\t0x005c\n#define INT_CAUSE\t\t\t0x0060\n#define  INT_TX_END\t\t\t0x07f80000\n#define  INT_TX_END_0\t\t\t0x00080000\n#define  INT_RX\t\t\t\t0x000003fc\n#define  INT_RX_0\t\t\t0x00000004\n#define  INT_EXT\t\t\t0x00000002\n#define INT_CAUSE_EXT\t\t\t0x0064\n#define  INT_EXT_LINK_PHY\t\t0x00110000\n#define  INT_EXT_TX\t\t\t0x000000ff\n#define INT_MASK\t\t\t0x0068\n#define INT_MASK_EXT\t\t\t0x006c\n#define TX_FIFO_URGENT_THRESHOLD\t0x0074\n#define RX_DISCARD_FRAME_CNT\t\t0x0084\n#define RX_OVERRUN_FRAME_CNT\t\t0x0088\n#define TXQ_FIX_PRIO_CONF_MOVED\t\t0x00dc\n#define TX_BW_RATE_MOVED\t\t0x00e0\n#define TX_BW_MTU_MOVED\t\t\t0x00e8\n#define TX_BW_BURST_MOVED\t\t0x00ec\n#define RXQ_CURRENT_DESC_PTR(q)\t\t(0x020c + ((q) << 4))\n#define RXQ_COMMAND\t\t\t0x0280\n#define TXQ_CURRENT_DESC_PTR(q)\t\t(0x02c0 + ((q) << 2))\n#define TXQ_BW_TOKENS(q)\t\t(0x0300 + ((q) << 4))\n#define TXQ_BW_CONF(q)\t\t\t(0x0304 + ((q) << 4))\n#define TXQ_BW_WRR_CONF(q)\t\t(0x0308 + ((q) << 4))\n\n \n#define MIB_COUNTERS(p)\t\t\t(0x1000 + ((p) << 7))\n#define SPECIAL_MCAST_TABLE(p)\t\t(0x1400 + ((p) << 10))\n#define OTHER_MCAST_TABLE(p)\t\t(0x1500 + ((p) << 10))\n#define UNICAST_TABLE(p)\t\t(0x1600 + ((p) << 10))\n\n\n \n#if defined(__BIG_ENDIAN)\n#define PORT_SDMA_CONFIG_DEFAULT_VALUE\t\t\\\n\t\t(RX_BURST_SIZE_4_64BIT\t|\t\\\n\t\t TX_BURST_SIZE_4_64BIT)\n#elif defined(__LITTLE_ENDIAN)\n#define PORT_SDMA_CONFIG_DEFAULT_VALUE\t\t\\\n\t\t(RX_BURST_SIZE_4_64BIT\t|\t\\\n\t\t BLM_RX_NO_SWAP\t\t|\t\\\n\t\t BLM_TX_NO_SWAP\t\t|\t\\\n\t\t TX_BURST_SIZE_4_64BIT)\n#else\n#error One of __BIG_ENDIAN or __LITTLE_ENDIAN must be defined\n#endif\n\n\n \n#define DEFAULT_RX_QUEUE_SIZE\t128\n#define DEFAULT_TX_QUEUE_SIZE\t512\n#define SKB_DMA_REALIGN\t\t((PAGE_SIZE - NET_SKB_PAD) % SMP_CACHE_BYTES)\n\n \n#define MV643XX_MAX_TSO_SEGS 100\n#define MV643XX_MAX_SKB_DESCS (MV643XX_MAX_TSO_SEGS * 2 + MAX_SKB_FRAGS)\n\n#define IS_TSO_HEADER(txq, addr) \\\n\t((addr >= txq->tso_hdrs_dma) && \\\n\t (addr < txq->tso_hdrs_dma + txq->tx_ring_size * TSO_HEADER_SIZE))\n\n#define DESC_DMA_MAP_SINGLE 0\n#define DESC_DMA_MAP_PAGE 1\n\n \n#if defined(__BIG_ENDIAN)\nstruct rx_desc {\n\tu16 byte_cnt;\t\t \n\tu16 buf_size;\t\t \n\tu32 cmd_sts;\t\t \n\tu32 next_desc_ptr;\t \n\tu32 buf_ptr;\t\t \n};\n\nstruct tx_desc {\n\tu16 byte_cnt;\t\t \n\tu16 l4i_chk;\t\t \n\tu32 cmd_sts;\t\t \n\tu32 next_desc_ptr;\t \n\tu32 buf_ptr;\t\t \n};\n#elif defined(__LITTLE_ENDIAN)\nstruct rx_desc {\n\tu32 cmd_sts;\t\t \n\tu16 buf_size;\t\t \n\tu16 byte_cnt;\t\t \n\tu32 buf_ptr;\t\t \n\tu32 next_desc_ptr;\t \n};\n\nstruct tx_desc {\n\tu32 cmd_sts;\t\t \n\tu16 l4i_chk;\t\t \n\tu16 byte_cnt;\t\t \n\tu32 buf_ptr;\t\t \n\tu32 next_desc_ptr;\t \n};\n#else\n#error One of __BIG_ENDIAN or __LITTLE_ENDIAN must be defined\n#endif\n\n \n#define BUFFER_OWNED_BY_DMA\t\t0x80000000\n\n \n#define ERROR_SUMMARY\t\t\t0x00000001\n\n \n#define LAYER_4_CHECKSUM_OK\t\t0x40000000\n#define RX_ENABLE_INTERRUPT\t\t0x20000000\n#define RX_FIRST_DESC\t\t\t0x08000000\n#define RX_LAST_DESC\t\t\t0x04000000\n#define RX_IP_HDR_OK\t\t\t0x02000000\n#define RX_PKT_IS_IPV4\t\t\t0x01000000\n#define RX_PKT_IS_ETHERNETV2\t\t0x00800000\n#define RX_PKT_LAYER4_TYPE_MASK\t\t0x00600000\n#define RX_PKT_LAYER4_TYPE_TCP_IPV4\t0x00000000\n#define RX_PKT_IS_VLAN_TAGGED\t\t0x00080000\n\n \n#define TX_ENABLE_INTERRUPT\t\t0x00800000\n#define GEN_CRC\t\t\t\t0x00400000\n#define TX_FIRST_DESC\t\t\t0x00200000\n#define TX_LAST_DESC\t\t\t0x00100000\n#define ZERO_PADDING\t\t\t0x00080000\n#define GEN_IP_V4_CHECKSUM\t\t0x00040000\n#define GEN_TCP_UDP_CHECKSUM\t\t0x00020000\n#define UDP_FRAME\t\t\t0x00010000\n#define MAC_HDR_EXTRA_4_BYTES\t\t0x00008000\n#define GEN_TCP_UDP_CHK_FULL\t\t0x00000400\n#define MAC_HDR_EXTRA_8_BYTES\t\t0x00000200\n\n#define TX_IHL_SHIFT\t\t\t11\n\n\n \nstruct mv643xx_eth_shared_private {\n\t \n\tvoid __iomem *base;\n\n\t \n\tu32 win_protect;\n\n\t \n\tint extended_rx_coal_limit;\n\tint tx_bw_control;\n\tint tx_csum_limit;\n\tstruct clk *clk;\n};\n\n#define TX_BW_CONTROL_ABSENT\t\t0\n#define TX_BW_CONTROL_OLD_LAYOUT\t1\n#define TX_BW_CONTROL_NEW_LAYOUT\t2\n\nstatic int mv643xx_eth_open(struct net_device *dev);\nstatic int mv643xx_eth_stop(struct net_device *dev);\n\n\n \nstruct mib_counters {\n\tu64 good_octets_received;\n\tu32 bad_octets_received;\n\tu32 internal_mac_transmit_err;\n\tu32 good_frames_received;\n\tu32 bad_frames_received;\n\tu32 broadcast_frames_received;\n\tu32 multicast_frames_received;\n\tu32 frames_64_octets;\n\tu32 frames_65_to_127_octets;\n\tu32 frames_128_to_255_octets;\n\tu32 frames_256_to_511_octets;\n\tu32 frames_512_to_1023_octets;\n\tu32 frames_1024_to_max_octets;\n\tu64 good_octets_sent;\n\tu32 good_frames_sent;\n\tu32 excessive_collision;\n\tu32 multicast_frames_sent;\n\tu32 broadcast_frames_sent;\n\tu32 unrec_mac_control_received;\n\tu32 fc_sent;\n\tu32 good_fc_received;\n\tu32 bad_fc_received;\n\tu32 undersize_received;\n\tu32 fragments_received;\n\tu32 oversize_received;\n\tu32 jabber_received;\n\tu32 mac_receive_error;\n\tu32 bad_crc_event;\n\tu32 collision;\n\tu32 late_collision;\n\t \n\tu32 rx_discard;\n\tu32 rx_overrun;\n};\n\nstruct rx_queue {\n\tint index;\n\n\tint rx_ring_size;\n\n\tint rx_desc_count;\n\tint rx_curr_desc;\n\tint rx_used_desc;\n\n\tstruct rx_desc *rx_desc_area;\n\tdma_addr_t rx_desc_dma;\n\tint rx_desc_area_size;\n\tstruct sk_buff **rx_skb;\n};\n\nstruct tx_queue {\n\tint index;\n\n\tint tx_ring_size;\n\n\tint tx_desc_count;\n\tint tx_curr_desc;\n\tint tx_used_desc;\n\n\tint tx_stop_threshold;\n\tint tx_wake_threshold;\n\n\tchar *tso_hdrs;\n\tdma_addr_t tso_hdrs_dma;\n\n\tstruct tx_desc *tx_desc_area;\n\tchar *tx_desc_mapping;  \n\tdma_addr_t tx_desc_dma;\n\tint tx_desc_area_size;\n\n\tstruct sk_buff_head tx_skb;\n\n\tunsigned long tx_packets;\n\tunsigned long tx_bytes;\n\tunsigned long tx_dropped;\n};\n\nstruct mv643xx_eth_private {\n\tstruct mv643xx_eth_shared_private *shared;\n\tvoid __iomem *base;\n\tint port_num;\n\n\tstruct net_device *dev;\n\n\tstruct timer_list mib_counters_timer;\n\tspinlock_t mib_counters_lock;\n\tstruct mib_counters mib_counters;\n\n\tstruct work_struct tx_timeout_task;\n\n\tstruct napi_struct napi;\n\tu32 int_mask;\n\tu8 oom;\n\tu8 work_link;\n\tu8 work_tx;\n\tu8 work_tx_end;\n\tu8 work_rx;\n\tu8 work_rx_refill;\n\n\tint skb_size;\n\n\t \n\tint rx_ring_size;\n\tunsigned long rx_desc_sram_addr;\n\tint rx_desc_sram_size;\n\tint rxq_count;\n\tstruct timer_list rx_oom;\n\tstruct rx_queue rxq[8];\n\n\t \n\tint tx_ring_size;\n\tunsigned long tx_desc_sram_addr;\n\tint tx_desc_sram_size;\n\tint txq_count;\n\tstruct tx_queue txq[8];\n\n\t \n\tstruct clk *clk;\n\tunsigned int t_clk;\n};\n\n\n \nstatic inline u32 rdl(struct mv643xx_eth_private *mp, int offset)\n{\n\treturn readl(mp->shared->base + offset);\n}\n\nstatic inline u32 rdlp(struct mv643xx_eth_private *mp, int offset)\n{\n\treturn readl(mp->base + offset);\n}\n\nstatic inline void wrl(struct mv643xx_eth_private *mp, int offset, u32 data)\n{\n\twritel(data, mp->shared->base + offset);\n}\n\nstatic inline void wrlp(struct mv643xx_eth_private *mp, int offset, u32 data)\n{\n\twritel(data, mp->base + offset);\n}\n\n\n \nstatic struct mv643xx_eth_private *rxq_to_mp(struct rx_queue *rxq)\n{\n\treturn container_of(rxq, struct mv643xx_eth_private, rxq[rxq->index]);\n}\n\nstatic struct mv643xx_eth_private *txq_to_mp(struct tx_queue *txq)\n{\n\treturn container_of(txq, struct mv643xx_eth_private, txq[txq->index]);\n}\n\nstatic void rxq_enable(struct rx_queue *rxq)\n{\n\tstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\n\twrlp(mp, RXQ_COMMAND, 1 << rxq->index);\n}\n\nstatic void rxq_disable(struct rx_queue *rxq)\n{\n\tstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\n\tu8 mask = 1 << rxq->index;\n\n\twrlp(mp, RXQ_COMMAND, mask << 8);\n\twhile (rdlp(mp, RXQ_COMMAND) & mask)\n\t\tudelay(10);\n}\n\nstatic void txq_reset_hw_ptr(struct tx_queue *txq)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tu32 addr;\n\n\taddr = (u32)txq->tx_desc_dma;\n\taddr += txq->tx_curr_desc * sizeof(struct tx_desc);\n\twrlp(mp, TXQ_CURRENT_DESC_PTR(txq->index), addr);\n}\n\nstatic void txq_enable(struct tx_queue *txq)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\twrlp(mp, TXQ_COMMAND, 1 << txq->index);\n}\n\nstatic void txq_disable(struct tx_queue *txq)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tu8 mask = 1 << txq->index;\n\n\twrlp(mp, TXQ_COMMAND, mask << 8);\n\twhile (rdlp(mp, TXQ_COMMAND) & mask)\n\t\tudelay(10);\n}\n\nstatic void txq_maybe_wake(struct tx_queue *txq)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tstruct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);\n\n\tif (netif_tx_queue_stopped(nq)) {\n\t\t__netif_tx_lock(nq, smp_processor_id());\n\t\tif (txq->tx_desc_count <= txq->tx_wake_threshold)\n\t\t\tnetif_tx_wake_queue(nq);\n\t\t__netif_tx_unlock(nq);\n\t}\n}\n\nstatic int rxq_process(struct rx_queue *rxq, int budget)\n{\n\tstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\n\tstruct net_device_stats *stats = &mp->dev->stats;\n\tint rx;\n\n\trx = 0;\n\twhile (rx < budget && rxq->rx_desc_count) {\n\t\tstruct rx_desc *rx_desc;\n\t\tunsigned int cmd_sts;\n\t\tstruct sk_buff *skb;\n\t\tu16 byte_cnt;\n\n\t\trx_desc = &rxq->rx_desc_area[rxq->rx_curr_desc];\n\n\t\tcmd_sts = rx_desc->cmd_sts;\n\t\tif (cmd_sts & BUFFER_OWNED_BY_DMA)\n\t\t\tbreak;\n\t\trmb();\n\n\t\tskb = rxq->rx_skb[rxq->rx_curr_desc];\n\t\trxq->rx_skb[rxq->rx_curr_desc] = NULL;\n\n\t\trxq->rx_curr_desc++;\n\t\tif (rxq->rx_curr_desc == rxq->rx_ring_size)\n\t\t\trxq->rx_curr_desc = 0;\n\n\t\tdma_unmap_single(mp->dev->dev.parent, rx_desc->buf_ptr,\n\t\t\t\t rx_desc->buf_size, DMA_FROM_DEVICE);\n\t\trxq->rx_desc_count--;\n\t\trx++;\n\n\t\tmp->work_rx_refill |= 1 << rxq->index;\n\n\t\tbyte_cnt = rx_desc->byte_cnt;\n\n\t\t \n\t\tstats->rx_packets++;\n\t\tstats->rx_bytes += byte_cnt - 2;\n\n\t\t \n\t\tif ((cmd_sts & (RX_FIRST_DESC | RX_LAST_DESC | ERROR_SUMMARY))\n\t\t\t!= (RX_FIRST_DESC | RX_LAST_DESC))\n\t\t\tgoto err;\n\n\t\t \n\t\tskb_put(skb, byte_cnt - 2 - 4);\n\n\t\tif (cmd_sts & LAYER_4_CHECKSUM_OK)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tskb->protocol = eth_type_trans(skb, mp->dev);\n\n\t\tnapi_gro_receive(&mp->napi, skb);\n\n\t\tcontinue;\n\nerr:\n\t\tstats->rx_dropped++;\n\n\t\tif ((cmd_sts & (RX_FIRST_DESC | RX_LAST_DESC)) !=\n\t\t\t(RX_FIRST_DESC | RX_LAST_DESC)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tnetdev_err(mp->dev,\n\t\t\t\t\t   \"received packet spanning multiple descriptors\\n\");\n\t\t}\n\n\t\tif (cmd_sts & ERROR_SUMMARY)\n\t\t\tstats->rx_errors++;\n\n\t\tdev_kfree_skb(skb);\n\t}\n\n\tif (rx < budget)\n\t\tmp->work_rx &= ~(1 << rxq->index);\n\n\treturn rx;\n}\n\nstatic int rxq_refill(struct rx_queue *rxq, int budget)\n{\n\tstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\n\tint refilled;\n\n\trefilled = 0;\n\twhile (refilled < budget && rxq->rx_desc_count < rxq->rx_ring_size) {\n\t\tstruct sk_buff *skb;\n\t\tint rx;\n\t\tstruct rx_desc *rx_desc;\n\t\tint size;\n\n\t\tskb = netdev_alloc_skb(mp->dev, mp->skb_size);\n\n\t\tif (skb == NULL) {\n\t\t\tmp->oom = 1;\n\t\t\tgoto oom;\n\t\t}\n\n\t\tif (SKB_DMA_REALIGN)\n\t\t\tskb_reserve(skb, SKB_DMA_REALIGN);\n\n\t\trefilled++;\n\t\trxq->rx_desc_count++;\n\n\t\trx = rxq->rx_used_desc++;\n\t\tif (rxq->rx_used_desc == rxq->rx_ring_size)\n\t\t\trxq->rx_used_desc = 0;\n\n\t\trx_desc = rxq->rx_desc_area + rx;\n\n\t\tsize = skb_end_pointer(skb) - skb->data;\n\t\trx_desc->buf_ptr = dma_map_single(mp->dev->dev.parent,\n\t\t\t\t\t\t  skb->data, size,\n\t\t\t\t\t\t  DMA_FROM_DEVICE);\n\t\trx_desc->buf_size = size;\n\t\trxq->rx_skb[rx] = skb;\n\t\twmb();\n\t\trx_desc->cmd_sts = BUFFER_OWNED_BY_DMA | RX_ENABLE_INTERRUPT;\n\t\twmb();\n\n\t\t \n\t\tskb_reserve(skb, 2);\n\t}\n\n\tif (refilled < budget)\n\t\tmp->work_rx_refill &= ~(1 << rxq->index);\n\noom:\n\treturn refilled;\n}\n\n\n \nstatic inline unsigned int has_tiny_unaligned_frags(struct sk_buff *skb)\n{\n\tint frag;\n\n\tfor (frag = 0; frag < skb_shinfo(skb)->nr_frags; frag++) {\n\t\tconst skb_frag_t *fragp = &skb_shinfo(skb)->frags[frag];\n\n\t\tif (skb_frag_size(fragp) <= 8 && skb_frag_off(fragp) & 7)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int skb_tx_csum(struct mv643xx_eth_private *mp, struct sk_buff *skb,\n\t\t       u16 *l4i_chk, u32 *command, int length)\n{\n\tint ret;\n\tu32 cmd = 0;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tint hdr_len;\n\t\tint tag_bytes;\n\n\t\tBUG_ON(skb->protocol != htons(ETH_P_IP) &&\n\t\t       skb->protocol != htons(ETH_P_8021Q));\n\n\t\thdr_len = (void *)ip_hdr(skb) - (void *)skb->data;\n\t\ttag_bytes = hdr_len - ETH_HLEN;\n\n\t\tif (length - hdr_len > mp->shared->tx_csum_limit ||\n\t\t    unlikely(tag_bytes & ~12)) {\n\t\t\tret = skb_checksum_help(skb);\n\t\t\tif (!ret)\n\t\t\t\tgoto no_csum;\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (tag_bytes & 4)\n\t\t\tcmd |= MAC_HDR_EXTRA_4_BYTES;\n\t\tif (tag_bytes & 8)\n\t\t\tcmd |= MAC_HDR_EXTRA_8_BYTES;\n\n\t\tcmd |= GEN_TCP_UDP_CHECKSUM | GEN_TCP_UDP_CHK_FULL |\n\t\t\t   GEN_IP_V4_CHECKSUM   |\n\t\t\t   ip_hdr(skb)->ihl << TX_IHL_SHIFT;\n\n\t\t \n\t\tswitch (ip_hdr(skb)->protocol) {\n\t\tcase IPPROTO_UDP:\n\t\t\tcmd |= UDP_FRAME;\n\t\t\t*l4i_chk = 0;\n\t\t\tbreak;\n\t\tcase IPPROTO_TCP:\n\t\t\t*l4i_chk = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN(1, \"protocol not supported\");\n\t\t}\n\t} else {\nno_csum:\n\t\t \n\t\tcmd |= 5 << TX_IHL_SHIFT;\n\t}\n\t*command = cmd;\n\treturn 0;\n}\n\nstatic inline int\ntxq_put_data_tso(struct net_device *dev, struct tx_queue *txq,\n\t\t struct sk_buff *skb, char *data, int length,\n\t\t bool last_tcp, bool is_last)\n{\n\tint tx_index;\n\tu32 cmd_sts;\n\tstruct tx_desc *desc;\n\n\ttx_index = txq->tx_curr_desc++;\n\tif (txq->tx_curr_desc == txq->tx_ring_size)\n\t\ttxq->tx_curr_desc = 0;\n\tdesc = &txq->tx_desc_area[tx_index];\n\ttxq->tx_desc_mapping[tx_index] = DESC_DMA_MAP_SINGLE;\n\n\tdesc->l4i_chk = 0;\n\tdesc->byte_cnt = length;\n\n\tif (length <= 8 && (uintptr_t)data & 0x7) {\n\t\t \n\t\tmemcpy(txq->tso_hdrs + tx_index * TSO_HEADER_SIZE,\n\t\t       data, length);\n\t\tdesc->buf_ptr = txq->tso_hdrs_dma\n\t\t\t+ tx_index * TSO_HEADER_SIZE;\n\t} else {\n\t\t \n\t\ttxq->tx_desc_mapping[tx_index] = DESC_DMA_MAP_SINGLE;\n\t\tdesc->buf_ptr = dma_map_single(dev->dev.parent, data,\n\t\t\tlength, DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dev->dev.parent,\n\t\t\t\t\t       desc->buf_ptr))) {\n\t\t\tWARN(1, \"dma_map_single failed!\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tcmd_sts = BUFFER_OWNED_BY_DMA;\n\tif (last_tcp) {\n\t\t \n\t\tcmd_sts |= ZERO_PADDING | TX_LAST_DESC;\n\t\t \n\t\tif (is_last)\n\t\t\tcmd_sts |= TX_ENABLE_INTERRUPT;\n\t}\n\tdesc->cmd_sts = cmd_sts;\n\treturn 0;\n}\n\nstatic inline void\ntxq_put_hdr_tso(struct sk_buff *skb, struct tx_queue *txq, int length,\n\t\tu32 *first_cmd_sts, bool first_desc)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tint hdr_len = skb_tcp_all_headers(skb);\n\tint tx_index;\n\tstruct tx_desc *desc;\n\tint ret;\n\tu32 cmd_csum = 0;\n\tu16 l4i_chk = 0;\n\tu32 cmd_sts;\n\n\ttx_index = txq->tx_curr_desc;\n\tdesc = &txq->tx_desc_area[tx_index];\n\n\tret = skb_tx_csum(mp, skb, &l4i_chk, &cmd_csum, length);\n\tif (ret)\n\t\tWARN(1, \"failed to prepare checksum!\");\n\n\t \n\tdesc->l4i_chk = 0;\n\n\tdesc->byte_cnt = hdr_len;\n\tdesc->buf_ptr = txq->tso_hdrs_dma +\n\t\t\ttxq->tx_curr_desc * TSO_HEADER_SIZE;\n\tcmd_sts = cmd_csum | BUFFER_OWNED_BY_DMA  | TX_FIRST_DESC |\n\t\t\t\t   GEN_CRC;\n\n\t \n\tif (first_desc)\n\t\t*first_cmd_sts = cmd_sts;\n\telse\n\t\tdesc->cmd_sts = cmd_sts;\n\n\ttxq->tx_curr_desc++;\n\tif (txq->tx_curr_desc == txq->tx_ring_size)\n\t\ttxq->tx_curr_desc = 0;\n}\n\nstatic int txq_submit_tso(struct tx_queue *txq, struct sk_buff *skb,\n\t\t\t  struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tint hdr_len, total_len, data_left, ret;\n\tint desc_count = 0;\n\tstruct tso_t tso;\n\tstruct tx_desc *first_tx_desc;\n\tu32 first_cmd_sts = 0;\n\n\t \n\tif ((txq->tx_desc_count + tso_count_descs(skb)) >= txq->tx_ring_size) {\n\t\tnetdev_dbg(dev, \"not enough descriptors for TSO!\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tfirst_tx_desc = &txq->tx_desc_area[txq->tx_curr_desc];\n\n\t \n\thdr_len = tso_start(skb, &tso);\n\n\ttotal_len = skb->len - hdr_len;\n\twhile (total_len > 0) {\n\t\tbool first_desc = (desc_count == 0);\n\t\tchar *hdr;\n\n\t\tdata_left = min_t(int, skb_shinfo(skb)->gso_size, total_len);\n\t\ttotal_len -= data_left;\n\t\tdesc_count++;\n\n\t\t \n\t\thdr = txq->tso_hdrs + txq->tx_curr_desc * TSO_HEADER_SIZE;\n\t\ttso_build_hdr(skb, hdr, &tso, data_left, total_len == 0);\n\t\ttxq_put_hdr_tso(skb, txq, data_left, &first_cmd_sts,\n\t\t\t\tfirst_desc);\n\n\t\twhile (data_left > 0) {\n\t\t\tint size;\n\t\t\tdesc_count++;\n\n\t\t\tsize = min_t(int, tso.size, data_left);\n\t\t\tret = txq_put_data_tso(dev, txq, skb, tso.data, size,\n\t\t\t\t\t       size == data_left,\n\t\t\t\t\t       total_len == 0);\n\t\t\tif (ret)\n\t\t\t\tgoto err_release;\n\t\t\tdata_left -= size;\n\t\t\ttso_build_data(skb, &tso, size);\n\t\t}\n\t}\n\n\t__skb_queue_tail(&txq->tx_skb, skb);\n\tskb_tx_timestamp(skb);\n\n\t \n\twmb();\n\tfirst_tx_desc->cmd_sts = first_cmd_sts;\n\n\t \n\tmp->work_tx_end &= ~(1 << txq->index);\n\n\t \n\twmb();\n\ttxq_enable(txq);\n\ttxq->tx_desc_count += desc_count;\n\treturn 0;\nerr_release:\n\t \n\treturn ret;\n}\n\nstatic void txq_submit_frag_skb(struct tx_queue *txq, struct sk_buff *skb)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tint nr_frags = skb_shinfo(skb)->nr_frags;\n\tint frag;\n\n\tfor (frag = 0; frag < nr_frags; frag++) {\n\t\tskb_frag_t *this_frag;\n\t\tint tx_index;\n\t\tstruct tx_desc *desc;\n\n\t\tthis_frag = &skb_shinfo(skb)->frags[frag];\n\t\ttx_index = txq->tx_curr_desc++;\n\t\tif (txq->tx_curr_desc == txq->tx_ring_size)\n\t\t\ttxq->tx_curr_desc = 0;\n\t\tdesc = &txq->tx_desc_area[tx_index];\n\t\ttxq->tx_desc_mapping[tx_index] = DESC_DMA_MAP_PAGE;\n\n\t\t \n\t\tif (frag == nr_frags - 1) {\n\t\t\tdesc->cmd_sts = BUFFER_OWNED_BY_DMA |\n\t\t\t\t\tZERO_PADDING | TX_LAST_DESC |\n\t\t\t\t\tTX_ENABLE_INTERRUPT;\n\t\t} else {\n\t\t\tdesc->cmd_sts = BUFFER_OWNED_BY_DMA;\n\t\t}\n\n\t\tdesc->l4i_chk = 0;\n\t\tdesc->byte_cnt = skb_frag_size(this_frag);\n\t\tdesc->buf_ptr = skb_frag_dma_map(mp->dev->dev.parent,\n\t\t\t\t\t\t this_frag, 0, desc->byte_cnt,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\t}\n}\n\nstatic int txq_submit_skb(struct tx_queue *txq, struct sk_buff *skb,\n\t\t\t  struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tint nr_frags = skb_shinfo(skb)->nr_frags;\n\tint tx_index;\n\tstruct tx_desc *desc;\n\tu32 cmd_sts;\n\tu16 l4i_chk;\n\tint length, ret;\n\n\tcmd_sts = 0;\n\tl4i_chk = 0;\n\n\tif (txq->tx_ring_size - txq->tx_desc_count < MAX_SKB_FRAGS + 1) {\n\t\tif (net_ratelimit())\n\t\t\tnetdev_err(dev, \"tx queue full?!\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tret = skb_tx_csum(mp, skb, &l4i_chk, &cmd_sts, skb->len);\n\tif (ret)\n\t\treturn ret;\n\tcmd_sts |= TX_FIRST_DESC | GEN_CRC | BUFFER_OWNED_BY_DMA;\n\n\ttx_index = txq->tx_curr_desc++;\n\tif (txq->tx_curr_desc == txq->tx_ring_size)\n\t\ttxq->tx_curr_desc = 0;\n\tdesc = &txq->tx_desc_area[tx_index];\n\ttxq->tx_desc_mapping[tx_index] = DESC_DMA_MAP_SINGLE;\n\n\tif (nr_frags) {\n\t\ttxq_submit_frag_skb(txq, skb);\n\t\tlength = skb_headlen(skb);\n\t} else {\n\t\tcmd_sts |= ZERO_PADDING | TX_LAST_DESC | TX_ENABLE_INTERRUPT;\n\t\tlength = skb->len;\n\t}\n\n\tdesc->l4i_chk = l4i_chk;\n\tdesc->byte_cnt = length;\n\tdesc->buf_ptr = dma_map_single(mp->dev->dev.parent, skb->data,\n\t\t\t\t       length, DMA_TO_DEVICE);\n\n\t__skb_queue_tail(&txq->tx_skb, skb);\n\n\tskb_tx_timestamp(skb);\n\n\t \n\twmb();\n\tdesc->cmd_sts = cmd_sts;\n\n\t \n\tmp->work_tx_end &= ~(1 << txq->index);\n\n\t \n\twmb();\n\ttxq_enable(txq);\n\n\ttxq->tx_desc_count += nr_frags + 1;\n\n\treturn 0;\n}\n\nstatic netdev_tx_t mv643xx_eth_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\tint length, queue, ret;\n\tstruct tx_queue *txq;\n\tstruct netdev_queue *nq;\n\n\tqueue = skb_get_queue_mapping(skb);\n\ttxq = mp->txq + queue;\n\tnq = netdev_get_tx_queue(dev, queue);\n\n\tif (has_tiny_unaligned_frags(skb) && __skb_linearize(skb)) {\n\t\tnetdev_printk(KERN_DEBUG, dev,\n\t\t\t      \"failed to linearize skb with tiny unaligned fragment\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tlength = skb->len;\n\n\tif (skb_is_gso(skb))\n\t\tret = txq_submit_tso(txq, skb, dev);\n\telse\n\t\tret = txq_submit_skb(txq, skb, dev);\n\tif (!ret) {\n\t\ttxq->tx_bytes += length;\n\t\ttxq->tx_packets++;\n\n\t\tif (txq->tx_desc_count >= txq->tx_stop_threshold)\n\t\t\tnetif_tx_stop_queue(nq);\n\t} else {\n\t\ttxq->tx_dropped++;\n\t\tdev_kfree_skb_any(skb);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\n\n \nstatic void txq_kick(struct tx_queue *txq)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tstruct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);\n\tu32 hw_desc_ptr;\n\tu32 expected_ptr;\n\n\t__netif_tx_lock(nq, smp_processor_id());\n\n\tif (rdlp(mp, TXQ_COMMAND) & (1 << txq->index))\n\t\tgoto out;\n\n\thw_desc_ptr = rdlp(mp, TXQ_CURRENT_DESC_PTR(txq->index));\n\texpected_ptr = (u32)txq->tx_desc_dma +\n\t\t\t\ttxq->tx_curr_desc * sizeof(struct tx_desc);\n\n\tif (hw_desc_ptr != expected_ptr)\n\t\ttxq_enable(txq);\n\nout:\n\t__netif_tx_unlock(nq);\n\n\tmp->work_tx_end &= ~(1 << txq->index);\n}\n\nstatic int txq_reclaim(struct tx_queue *txq, int budget, int force)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tstruct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);\n\tint reclaimed;\n\n\t__netif_tx_lock_bh(nq);\n\n\treclaimed = 0;\n\twhile (reclaimed < budget && txq->tx_desc_count > 0) {\n\t\tint tx_index;\n\t\tstruct tx_desc *desc;\n\t\tu32 cmd_sts;\n\t\tchar desc_dma_map;\n\n\t\ttx_index = txq->tx_used_desc;\n\t\tdesc = &txq->tx_desc_area[tx_index];\n\t\tdesc_dma_map = txq->tx_desc_mapping[tx_index];\n\n\t\tcmd_sts = desc->cmd_sts;\n\n\t\tif (cmd_sts & BUFFER_OWNED_BY_DMA) {\n\t\t\tif (!force)\n\t\t\t\tbreak;\n\t\t\tdesc->cmd_sts = cmd_sts & ~BUFFER_OWNED_BY_DMA;\n\t\t}\n\n\t\ttxq->tx_used_desc = tx_index + 1;\n\t\tif (txq->tx_used_desc == txq->tx_ring_size)\n\t\t\ttxq->tx_used_desc = 0;\n\n\t\treclaimed++;\n\t\ttxq->tx_desc_count--;\n\n\t\tif (!IS_TSO_HEADER(txq, desc->buf_ptr)) {\n\n\t\t\tif (desc_dma_map == DESC_DMA_MAP_PAGE)\n\t\t\t\tdma_unmap_page(mp->dev->dev.parent,\n\t\t\t\t\t       desc->buf_ptr,\n\t\t\t\t\t       desc->byte_cnt,\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\telse\n\t\t\t\tdma_unmap_single(mp->dev->dev.parent,\n\t\t\t\t\t\t desc->buf_ptr,\n\t\t\t\t\t\t desc->byte_cnt,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\t}\n\n\t\tif (cmd_sts & TX_ENABLE_INTERRUPT) {\n\t\t\tstruct sk_buff *skb = __skb_dequeue(&txq->tx_skb);\n\n\t\t\tif (!WARN_ON(!skb))\n\t\t\t\tdev_consume_skb_any(skb);\n\t\t}\n\n\t\tif (cmd_sts & ERROR_SUMMARY) {\n\t\t\tnetdev_info(mp->dev, \"tx error\\n\");\n\t\t\tmp->dev->stats.tx_errors++;\n\t\t}\n\n\t}\n\n\t__netif_tx_unlock_bh(nq);\n\n\tif (reclaimed < budget)\n\t\tmp->work_tx &= ~(1 << txq->index);\n\n\treturn reclaimed;\n}\n\n\n \n \nstatic void tx_set_rate(struct mv643xx_eth_private *mp, int rate, int burst)\n{\n\tint token_rate;\n\tint mtu;\n\tint bucket_size;\n\n\ttoken_rate = ((rate / 1000) * 64) / (mp->t_clk / 1000);\n\tif (token_rate > 1023)\n\t\ttoken_rate = 1023;\n\n\tmtu = (mp->dev->mtu + 255) >> 8;\n\tif (mtu > 63)\n\t\tmtu = 63;\n\n\tbucket_size = (burst + 255) >> 8;\n\tif (bucket_size > 65535)\n\t\tbucket_size = 65535;\n\n\tswitch (mp->shared->tx_bw_control) {\n\tcase TX_BW_CONTROL_OLD_LAYOUT:\n\t\twrlp(mp, TX_BW_RATE, token_rate);\n\t\twrlp(mp, TX_BW_MTU, mtu);\n\t\twrlp(mp, TX_BW_BURST, bucket_size);\n\t\tbreak;\n\tcase TX_BW_CONTROL_NEW_LAYOUT:\n\t\twrlp(mp, TX_BW_RATE_MOVED, token_rate);\n\t\twrlp(mp, TX_BW_MTU_MOVED, mtu);\n\t\twrlp(mp, TX_BW_BURST_MOVED, bucket_size);\n\t\tbreak;\n\t}\n}\n\nstatic void txq_set_rate(struct tx_queue *txq, int rate, int burst)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tint token_rate;\n\tint bucket_size;\n\n\ttoken_rate = ((rate / 1000) * 64) / (mp->t_clk / 1000);\n\tif (token_rate > 1023)\n\t\ttoken_rate = 1023;\n\n\tbucket_size = (burst + 255) >> 8;\n\tif (bucket_size > 65535)\n\t\tbucket_size = 65535;\n\n\twrlp(mp, TXQ_BW_TOKENS(txq->index), token_rate << 14);\n\twrlp(mp, TXQ_BW_CONF(txq->index), (bucket_size << 10) | token_rate);\n}\n\nstatic void txq_set_fixed_prio_mode(struct tx_queue *txq)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\tint off;\n\tu32 val;\n\n\t \n\toff = 0;\n\tswitch (mp->shared->tx_bw_control) {\n\tcase TX_BW_CONTROL_OLD_LAYOUT:\n\t\toff = TXQ_FIX_PRIO_CONF;\n\t\tbreak;\n\tcase TX_BW_CONTROL_NEW_LAYOUT:\n\t\toff = TXQ_FIX_PRIO_CONF_MOVED;\n\t\tbreak;\n\t}\n\n\tif (off) {\n\t\tval = rdlp(mp, off);\n\t\tval |= 1 << txq->index;\n\t\twrlp(mp, off, val);\n\t}\n}\n\n\n \nstatic void mv643xx_eth_adjust_link(struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\tu32 pscr = rdlp(mp, PORT_SERIAL_CONTROL);\n\tu32 autoneg_disable = FORCE_LINK_PASS |\n\t             DISABLE_AUTO_NEG_SPEED_GMII |\n\t\t     DISABLE_AUTO_NEG_FOR_FLOW_CTRL |\n\t\t     DISABLE_AUTO_NEG_FOR_DUPLEX;\n\n\tif (dev->phydev->autoneg == AUTONEG_ENABLE) {\n\t\t \n\t\tpscr &= ~autoneg_disable;\n\t\tgoto out_write;\n\t}\n\n\tpscr |= autoneg_disable;\n\n\tif (dev->phydev->speed == SPEED_1000) {\n\t\t \n\t\tpscr |= SET_GMII_SPEED_TO_1000;\n\t\tpscr |= SET_FULL_DUPLEX_MODE;\n\t\tgoto out_write;\n\t}\n\n\tpscr &= ~SET_GMII_SPEED_TO_1000;\n\n\tif (dev->phydev->speed == SPEED_100)\n\t\tpscr |= SET_MII_SPEED_TO_100;\n\telse\n\t\tpscr &= ~SET_MII_SPEED_TO_100;\n\n\tif (dev->phydev->duplex == DUPLEX_FULL)\n\t\tpscr |= SET_FULL_DUPLEX_MODE;\n\telse\n\t\tpscr &= ~SET_FULL_DUPLEX_MODE;\n\nout_write:\n\twrlp(mp, PORT_SERIAL_CONTROL, pscr);\n}\n\n \nstatic struct net_device_stats *mv643xx_eth_get_stats(struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\tstruct net_device_stats *stats = &dev->stats;\n\tunsigned long tx_packets = 0;\n\tunsigned long tx_bytes = 0;\n\tunsigned long tx_dropped = 0;\n\tint i;\n\n\tfor (i = 0; i < mp->txq_count; i++) {\n\t\tstruct tx_queue *txq = mp->txq + i;\n\n\t\ttx_packets += txq->tx_packets;\n\t\ttx_bytes += txq->tx_bytes;\n\t\ttx_dropped += txq->tx_dropped;\n\t}\n\n\tstats->tx_packets = tx_packets;\n\tstats->tx_bytes = tx_bytes;\n\tstats->tx_dropped = tx_dropped;\n\n\treturn stats;\n}\n\nstatic inline u32 mib_read(struct mv643xx_eth_private *mp, int offset)\n{\n\treturn rdl(mp, MIB_COUNTERS(mp->port_num) + offset);\n}\n\nstatic void mib_counters_clear(struct mv643xx_eth_private *mp)\n{\n\tint i;\n\n\tfor (i = 0; i < 0x80; i += 4)\n\t\tmib_read(mp, i);\n\n\t \n\trdlp(mp, RX_DISCARD_FRAME_CNT);\n\trdlp(mp, RX_OVERRUN_FRAME_CNT);\n}\n\nstatic void mib_counters_update(struct mv643xx_eth_private *mp)\n{\n\tstruct mib_counters *p = &mp->mib_counters;\n\n\tspin_lock_bh(&mp->mib_counters_lock);\n\tp->good_octets_received += mib_read(mp, 0x00);\n\tp->bad_octets_received += mib_read(mp, 0x08);\n\tp->internal_mac_transmit_err += mib_read(mp, 0x0c);\n\tp->good_frames_received += mib_read(mp, 0x10);\n\tp->bad_frames_received += mib_read(mp, 0x14);\n\tp->broadcast_frames_received += mib_read(mp, 0x18);\n\tp->multicast_frames_received += mib_read(mp, 0x1c);\n\tp->frames_64_octets += mib_read(mp, 0x20);\n\tp->frames_65_to_127_octets += mib_read(mp, 0x24);\n\tp->frames_128_to_255_octets += mib_read(mp, 0x28);\n\tp->frames_256_to_511_octets += mib_read(mp, 0x2c);\n\tp->frames_512_to_1023_octets += mib_read(mp, 0x30);\n\tp->frames_1024_to_max_octets += mib_read(mp, 0x34);\n\tp->good_octets_sent += mib_read(mp, 0x38);\n\tp->good_frames_sent += mib_read(mp, 0x40);\n\tp->excessive_collision += mib_read(mp, 0x44);\n\tp->multicast_frames_sent += mib_read(mp, 0x48);\n\tp->broadcast_frames_sent += mib_read(mp, 0x4c);\n\tp->unrec_mac_control_received += mib_read(mp, 0x50);\n\tp->fc_sent += mib_read(mp, 0x54);\n\tp->good_fc_received += mib_read(mp, 0x58);\n\tp->bad_fc_received += mib_read(mp, 0x5c);\n\tp->undersize_received += mib_read(mp, 0x60);\n\tp->fragments_received += mib_read(mp, 0x64);\n\tp->oversize_received += mib_read(mp, 0x68);\n\tp->jabber_received += mib_read(mp, 0x6c);\n\tp->mac_receive_error += mib_read(mp, 0x70);\n\tp->bad_crc_event += mib_read(mp, 0x74);\n\tp->collision += mib_read(mp, 0x78);\n\tp->late_collision += mib_read(mp, 0x7c);\n\t \n\tp->rx_discard += rdlp(mp, RX_DISCARD_FRAME_CNT);\n\tp->rx_overrun += rdlp(mp, RX_OVERRUN_FRAME_CNT);\n\tspin_unlock_bh(&mp->mib_counters_lock);\n}\n\nstatic void mib_counters_timer_wrapper(struct timer_list *t)\n{\n\tstruct mv643xx_eth_private *mp = from_timer(mp, t, mib_counters_timer);\n\tmib_counters_update(mp);\n\tmod_timer(&mp->mib_counters_timer, jiffies + 30 * HZ);\n}\n\n\n \n \nstatic unsigned int get_rx_coal(struct mv643xx_eth_private *mp)\n{\n\tu32 val = rdlp(mp, SDMA_CONFIG);\n\tu64 temp;\n\n\tif (mp->shared->extended_rx_coal_limit)\n\t\ttemp = ((val & 0x02000000) >> 10) | ((val & 0x003fff80) >> 7);\n\telse\n\t\ttemp = (val & 0x003fff00) >> 8;\n\n\ttemp *= 64000000;\n\ttemp += mp->t_clk / 2;\n\tdo_div(temp, mp->t_clk);\n\n\treturn (unsigned int)temp;\n}\n\nstatic void set_rx_coal(struct mv643xx_eth_private *mp, unsigned int usec)\n{\n\tu64 temp;\n\tu32 val;\n\n\ttemp = (u64)usec * mp->t_clk;\n\ttemp += 31999999;\n\tdo_div(temp, 64000000);\n\n\tval = rdlp(mp, SDMA_CONFIG);\n\tif (mp->shared->extended_rx_coal_limit) {\n\t\tif (temp > 0xffff)\n\t\t\ttemp = 0xffff;\n\t\tval &= ~0x023fff80;\n\t\tval |= (temp & 0x8000) << 10;\n\t\tval |= (temp & 0x7fff) << 7;\n\t} else {\n\t\tif (temp > 0x3fff)\n\t\t\ttemp = 0x3fff;\n\t\tval &= ~0x003fff00;\n\t\tval |= (temp & 0x3fff) << 8;\n\t}\n\twrlp(mp, SDMA_CONFIG, val);\n}\n\nstatic unsigned int get_tx_coal(struct mv643xx_eth_private *mp)\n{\n\tu64 temp;\n\n\ttemp = (rdlp(mp, TX_FIFO_URGENT_THRESHOLD) & 0x3fff0) >> 4;\n\ttemp *= 64000000;\n\ttemp += mp->t_clk / 2;\n\tdo_div(temp, mp->t_clk);\n\n\treturn (unsigned int)temp;\n}\n\nstatic void set_tx_coal(struct mv643xx_eth_private *mp, unsigned int usec)\n{\n\tu64 temp;\n\n\ttemp = (u64)usec * mp->t_clk;\n\ttemp += 31999999;\n\tdo_div(temp, 64000000);\n\n\tif (temp > 0x3fff)\n\t\ttemp = 0x3fff;\n\n\twrlp(mp, TX_FIFO_URGENT_THRESHOLD, temp << 4);\n}\n\n\n \nstruct mv643xx_eth_stats {\n\tchar stat_string[ETH_GSTRING_LEN];\n\tint sizeof_stat;\n\tint netdev_off;\n\tint mp_off;\n};\n\n#define SSTAT(m)\t\t\t\t\t\t\\\n\t{ #m, sizeof_field(struct net_device_stats, m),\t\t\\\n\t  offsetof(struct net_device, stats.m), -1 }\n\n#define MIBSTAT(m)\t\t\t\t\t\t\\\n\t{ #m, sizeof_field(struct mib_counters, m),\t\t\\\n\t  -1, offsetof(struct mv643xx_eth_private, mib_counters.m) }\n\nstatic const struct mv643xx_eth_stats mv643xx_eth_stats[] = {\n\tSSTAT(rx_packets),\n\tSSTAT(tx_packets),\n\tSSTAT(rx_bytes),\n\tSSTAT(tx_bytes),\n\tSSTAT(rx_errors),\n\tSSTAT(tx_errors),\n\tSSTAT(rx_dropped),\n\tSSTAT(tx_dropped),\n\tMIBSTAT(good_octets_received),\n\tMIBSTAT(bad_octets_received),\n\tMIBSTAT(internal_mac_transmit_err),\n\tMIBSTAT(good_frames_received),\n\tMIBSTAT(bad_frames_received),\n\tMIBSTAT(broadcast_frames_received),\n\tMIBSTAT(multicast_frames_received),\n\tMIBSTAT(frames_64_octets),\n\tMIBSTAT(frames_65_to_127_octets),\n\tMIBSTAT(frames_128_to_255_octets),\n\tMIBSTAT(frames_256_to_511_octets),\n\tMIBSTAT(frames_512_to_1023_octets),\n\tMIBSTAT(frames_1024_to_max_octets),\n\tMIBSTAT(good_octets_sent),\n\tMIBSTAT(good_frames_sent),\n\tMIBSTAT(excessive_collision),\n\tMIBSTAT(multicast_frames_sent),\n\tMIBSTAT(broadcast_frames_sent),\n\tMIBSTAT(unrec_mac_control_received),\n\tMIBSTAT(fc_sent),\n\tMIBSTAT(good_fc_received),\n\tMIBSTAT(bad_fc_received),\n\tMIBSTAT(undersize_received),\n\tMIBSTAT(fragments_received),\n\tMIBSTAT(oversize_received),\n\tMIBSTAT(jabber_received),\n\tMIBSTAT(mac_receive_error),\n\tMIBSTAT(bad_crc_event),\n\tMIBSTAT(collision),\n\tMIBSTAT(late_collision),\n\tMIBSTAT(rx_discard),\n\tMIBSTAT(rx_overrun),\n};\n\nstatic int\nmv643xx_eth_get_link_ksettings_phy(struct mv643xx_eth_private *mp,\n\t\t\t\t   struct ethtool_link_ksettings *cmd)\n{\n\tstruct net_device *dev = mp->dev;\n\n\tphy_ethtool_ksettings_get(dev->phydev, cmd);\n\n\t \n\tlinkmode_clear_bit(ETHTOOL_LINK_MODE_1000baseT_Half_BIT,\n\t\t\t   cmd->link_modes.supported);\n\tlinkmode_clear_bit(ETHTOOL_LINK_MODE_1000baseT_Half_BIT,\n\t\t\t   cmd->link_modes.advertising);\n\n\treturn 0;\n}\n\nstatic int\nmv643xx_eth_get_link_ksettings_phyless(struct mv643xx_eth_private *mp,\n\t\t\t\t       struct ethtool_link_ksettings *cmd)\n{\n\tu32 port_status;\n\tu32 supported, advertising;\n\n\tport_status = rdlp(mp, PORT_STATUS);\n\n\tsupported = SUPPORTED_MII;\n\tadvertising = ADVERTISED_MII;\n\tswitch (port_status & PORT_SPEED_MASK) {\n\tcase PORT_SPEED_10:\n\t\tcmd->base.speed = SPEED_10;\n\t\tbreak;\n\tcase PORT_SPEED_100:\n\t\tcmd->base.speed = SPEED_100;\n\t\tbreak;\n\tcase PORT_SPEED_1000:\n\t\tcmd->base.speed = SPEED_1000;\n\t\tbreak;\n\tdefault:\n\t\tcmd->base.speed = -1;\n\t\tbreak;\n\t}\n\tcmd->base.duplex = (port_status & FULL_DUPLEX) ?\n\t\tDUPLEX_FULL : DUPLEX_HALF;\n\tcmd->base.port = PORT_MII;\n\tcmd->base.phy_address = 0;\n\tcmd->base.autoneg = AUTONEG_DISABLE;\n\n\tethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.supported,\n\t\t\t\t\t\tsupported);\n\tethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.advertising,\n\t\t\t\t\t\tadvertising);\n\n\treturn 0;\n}\n\nstatic void\nmv643xx_eth_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\twol->supported = 0;\n\twol->wolopts = 0;\n\tif (dev->phydev)\n\t\tphy_ethtool_get_wol(dev->phydev, wol);\n}\n\nstatic int\nmv643xx_eth_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tint err;\n\n\tif (!dev->phydev)\n\t\treturn -EOPNOTSUPP;\n\n\terr = phy_ethtool_set_wol(dev->phydev, wol);\n\t \n\tif (err == -EOPNOTSUPP)\n\t\tnetdev_info(dev, \"The PHY does not support set_wol, was CONFIG_MARVELL_PHY enabled?\\n\");\n\treturn err;\n}\n\nstatic int\nmv643xx_eth_get_link_ksettings(struct net_device *dev,\n\t\t\t       struct ethtool_link_ksettings *cmd)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\n\tif (dev->phydev)\n\t\treturn mv643xx_eth_get_link_ksettings_phy(mp, cmd);\n\telse\n\t\treturn mv643xx_eth_get_link_ksettings_phyless(mp, cmd);\n}\n\nstatic int\nmv643xx_eth_set_link_ksettings(struct net_device *dev,\n\t\t\t       const struct ethtool_link_ksettings *cmd)\n{\n\tstruct ethtool_link_ksettings c = *cmd;\n\tu32 advertising;\n\tint ret;\n\n\tif (!dev->phydev)\n\t\treturn -EINVAL;\n\n\t \n\tethtool_convert_link_mode_to_legacy_u32(&advertising,\n\t\t\t\t\t\tc.link_modes.advertising);\n\tadvertising &= ~ADVERTISED_1000baseT_Half;\n\tethtool_convert_legacy_u32_to_link_mode(c.link_modes.advertising,\n\t\t\t\t\t\tadvertising);\n\n\tret = phy_ethtool_ksettings_set(dev->phydev, &c);\n\tif (!ret)\n\t\tmv643xx_eth_adjust_link(dev);\n\treturn ret;\n}\n\nstatic void mv643xx_eth_get_drvinfo(struct net_device *dev,\n\t\t\t\t    struct ethtool_drvinfo *drvinfo)\n{\n\tstrscpy(drvinfo->driver, mv643xx_eth_driver_name,\n\t\tsizeof(drvinfo->driver));\n\tstrscpy(drvinfo->version, mv643xx_eth_driver_version,\n\t\tsizeof(drvinfo->version));\n\tstrscpy(drvinfo->fw_version, \"N/A\", sizeof(drvinfo->fw_version));\n\tstrscpy(drvinfo->bus_info, \"platform\", sizeof(drvinfo->bus_info));\n}\n\nstatic int mv643xx_eth_get_coalesce(struct net_device *dev,\n\t\t\t\t    struct ethtool_coalesce *ec,\n\t\t\t\t    struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\n\tec->rx_coalesce_usecs = get_rx_coal(mp);\n\tec->tx_coalesce_usecs = get_tx_coal(mp);\n\n\treturn 0;\n}\n\nstatic int mv643xx_eth_set_coalesce(struct net_device *dev,\n\t\t\t\t    struct ethtool_coalesce *ec,\n\t\t\t\t    struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\n\tset_rx_coal(mp, ec->rx_coalesce_usecs);\n\tset_tx_coal(mp, ec->tx_coalesce_usecs);\n\n\treturn 0;\n}\n\nstatic void\nmv643xx_eth_get_ringparam(struct net_device *dev, struct ethtool_ringparam *er,\n\t\t\t  struct kernel_ethtool_ringparam *kernel_er,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\n\ter->rx_max_pending = 4096;\n\ter->tx_max_pending = 4096;\n\n\ter->rx_pending = mp->rx_ring_size;\n\ter->tx_pending = mp->tx_ring_size;\n}\n\nstatic int\nmv643xx_eth_set_ringparam(struct net_device *dev, struct ethtool_ringparam *er,\n\t\t\t  struct kernel_ethtool_ringparam *kernel_er,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\n\tif (er->rx_mini_pending || er->rx_jumbo_pending)\n\t\treturn -EINVAL;\n\n\tmp->rx_ring_size = min(er->rx_pending, 4096U);\n\tmp->tx_ring_size = clamp_t(unsigned int, er->tx_pending,\n\t\t\t\t   MV643XX_MAX_SKB_DESCS * 2, 4096);\n\tif (mp->tx_ring_size != er->tx_pending)\n\t\tnetdev_warn(dev, \"TX queue size set to %u (requested %u)\\n\",\n\t\t\t    mp->tx_ring_size, er->tx_pending);\n\n\tif (netif_running(dev)) {\n\t\tmv643xx_eth_stop(dev);\n\t\tif (mv643xx_eth_open(dev)) {\n\t\t\tnetdev_err(dev,\n\t\t\t\t   \"fatal error on re-opening device after ring param change\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n\nstatic int\nmv643xx_eth_set_features(struct net_device *dev, netdev_features_t features)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\tbool rx_csum = features & NETIF_F_RXCSUM;\n\n\twrlp(mp, PORT_CONFIG, rx_csum ? 0x02000000 : 0x00000000);\n\n\treturn 0;\n}\n\nstatic void mv643xx_eth_get_strings(struct net_device *dev,\n\t\t\t\t    uint32_t stringset, uint8_t *data)\n{\n\tint i;\n\n\tif (stringset == ETH_SS_STATS) {\n\t\tfor (i = 0; i < ARRAY_SIZE(mv643xx_eth_stats); i++) {\n\t\t\tmemcpy(data + i * ETH_GSTRING_LEN,\n\t\t\t\tmv643xx_eth_stats[i].stat_string,\n\t\t\t\tETH_GSTRING_LEN);\n\t\t}\n\t}\n}\n\nstatic void mv643xx_eth_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t\t  struct ethtool_stats *stats,\n\t\t\t\t\t  uint64_t *data)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\tint i;\n\n\tmv643xx_eth_get_stats(dev);\n\tmib_counters_update(mp);\n\n\tfor (i = 0; i < ARRAY_SIZE(mv643xx_eth_stats); i++) {\n\t\tconst struct mv643xx_eth_stats *stat;\n\t\tvoid *p;\n\n\t\tstat = mv643xx_eth_stats + i;\n\n\t\tif (stat->netdev_off >= 0)\n\t\t\tp = ((void *)mp->dev) + stat->netdev_off;\n\t\telse\n\t\t\tp = ((void *)mp) + stat->mp_off;\n\n\t\tdata[i] = (stat->sizeof_stat == 8) ?\n\t\t\t\t*(uint64_t *)p : *(uint32_t *)p;\n\t}\n}\n\nstatic int mv643xx_eth_get_sset_count(struct net_device *dev, int sset)\n{\n\tif (sset == ETH_SS_STATS)\n\t\treturn ARRAY_SIZE(mv643xx_eth_stats);\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic const struct ethtool_ops mv643xx_eth_ethtool_ops = {\n\t.supported_coalesce_params = ETHTOOL_COALESCE_USECS,\n\t.get_drvinfo\t\t= mv643xx_eth_get_drvinfo,\n\t.nway_reset\t\t= phy_ethtool_nway_reset,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_coalesce\t\t= mv643xx_eth_get_coalesce,\n\t.set_coalesce\t\t= mv643xx_eth_set_coalesce,\n\t.get_ringparam\t\t= mv643xx_eth_get_ringparam,\n\t.set_ringparam\t\t= mv643xx_eth_set_ringparam,\n\t.get_strings\t\t= mv643xx_eth_get_strings,\n\t.get_ethtool_stats\t= mv643xx_eth_get_ethtool_stats,\n\t.get_sset_count\t\t= mv643xx_eth_get_sset_count,\n\t.get_ts_info\t\t= ethtool_op_get_ts_info,\n\t.get_wol                = mv643xx_eth_get_wol,\n\t.set_wol                = mv643xx_eth_set_wol,\n\t.get_link_ksettings\t= mv643xx_eth_get_link_ksettings,\n\t.set_link_ksettings\t= mv643xx_eth_set_link_ksettings,\n};\n\n\n \nstatic void uc_addr_get(struct mv643xx_eth_private *mp, unsigned char *addr)\n{\n\tunsigned int mac_h = rdlp(mp, MAC_ADDR_HIGH);\n\tunsigned int mac_l = rdlp(mp, MAC_ADDR_LOW);\n\n\taddr[0] = (mac_h >> 24) & 0xff;\n\taddr[1] = (mac_h >> 16) & 0xff;\n\taddr[2] = (mac_h >> 8) & 0xff;\n\taddr[3] = mac_h & 0xff;\n\taddr[4] = (mac_l >> 8) & 0xff;\n\taddr[5] = mac_l & 0xff;\n}\n\nstatic void uc_addr_set(struct mv643xx_eth_private *mp, const u8 *addr)\n{\n\twrlp(mp, MAC_ADDR_HIGH,\n\t\t(addr[0] << 24) | (addr[1] << 16) | (addr[2] << 8) | addr[3]);\n\twrlp(mp, MAC_ADDR_LOW, (addr[4] << 8) | addr[5]);\n}\n\nstatic u32 uc_addr_filter_mask(struct net_device *dev)\n{\n\tstruct netdev_hw_addr *ha;\n\tu32 nibbles;\n\n\tif (dev->flags & IFF_PROMISC)\n\t\treturn 0;\n\n\tnibbles = 1 << (dev->dev_addr[5] & 0x0f);\n\tnetdev_for_each_uc_addr(ha, dev) {\n\t\tif (memcmp(dev->dev_addr, ha->addr, 5))\n\t\t\treturn 0;\n\t\tif ((dev->dev_addr[5] ^ ha->addr[5]) & 0xf0)\n\t\t\treturn 0;\n\n\t\tnibbles |= 1 << (ha->addr[5] & 0x0f);\n\t}\n\n\treturn nibbles;\n}\n\nstatic void mv643xx_eth_program_unicast_filter(struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\tu32 port_config;\n\tu32 nibbles;\n\tint i;\n\n\tuc_addr_set(mp, dev->dev_addr);\n\n\tport_config = rdlp(mp, PORT_CONFIG) & ~UNICAST_PROMISCUOUS_MODE;\n\n\tnibbles = uc_addr_filter_mask(dev);\n\tif (!nibbles) {\n\t\tport_config |= UNICAST_PROMISCUOUS_MODE;\n\t\tnibbles = 0xffff;\n\t}\n\n\tfor (i = 0; i < 16; i += 4) {\n\t\tint off = UNICAST_TABLE(mp->port_num) + i;\n\t\tu32 v;\n\n\t\tv = 0;\n\t\tif (nibbles & 1)\n\t\t\tv |= 0x00000001;\n\t\tif (nibbles & 2)\n\t\t\tv |= 0x00000100;\n\t\tif (nibbles & 4)\n\t\t\tv |= 0x00010000;\n\t\tif (nibbles & 8)\n\t\t\tv |= 0x01000000;\n\t\tnibbles >>= 4;\n\n\t\twrl(mp, off, v);\n\t}\n\n\twrlp(mp, PORT_CONFIG, port_config);\n}\n\nstatic int addr_crc(unsigned char *addr)\n{\n\tint crc = 0;\n\tint i;\n\n\tfor (i = 0; i < 6; i++) {\n\t\tint j;\n\n\t\tcrc = (crc ^ addr[i]) << 8;\n\t\tfor (j = 7; j >= 0; j--) {\n\t\t\tif (crc & (0x100 << j))\n\t\t\t\tcrc ^= 0x107 << j;\n\t\t}\n\t}\n\n\treturn crc;\n}\n\nstatic void mv643xx_eth_program_multicast_filter(struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\tu32 *mc_spec;\n\tu32 *mc_other;\n\tstruct netdev_hw_addr *ha;\n\tint i;\n\n\tif (dev->flags & (IFF_PROMISC | IFF_ALLMULTI))\n\t\tgoto promiscuous;\n\n\t \n\tmc_spec = kcalloc(128, sizeof(u32), GFP_ATOMIC);\n\tif (!mc_spec)\n\t\tgoto promiscuous;\n\tmc_other = &mc_spec[64];\n\n\tnetdev_for_each_mc_addr(ha, dev) {\n\t\tu8 *a = ha->addr;\n\t\tu32 *table;\n\t\tu8 entry;\n\n\t\tif (memcmp(a, \"\\x01\\x00\\x5e\\x00\\x00\", 5) == 0) {\n\t\t\ttable = mc_spec;\n\t\t\tentry = a[5];\n\t\t} else {\n\t\t\ttable = mc_other;\n\t\t\tentry = addr_crc(a);\n\t\t}\n\n\t\ttable[entry >> 2] |= 1 << (8 * (entry & 3));\n\t}\n\n\tfor (i = 0; i < 64; i++) {\n\t\twrl(mp, SPECIAL_MCAST_TABLE(mp->port_num) + i * sizeof(u32),\n\t\t    mc_spec[i]);\n\t\twrl(mp, OTHER_MCAST_TABLE(mp->port_num) + i * sizeof(u32),\n\t\t    mc_other[i]);\n\t}\n\n\tkfree(mc_spec);\n\treturn;\n\npromiscuous:\n\tfor (i = 0; i < 64; i++) {\n\t\twrl(mp, SPECIAL_MCAST_TABLE(mp->port_num) + i * sizeof(u32),\n\t\t    0x01010101u);\n\t\twrl(mp, OTHER_MCAST_TABLE(mp->port_num) + i * sizeof(u32),\n\t\t    0x01010101u);\n\t}\n}\n\nstatic void mv643xx_eth_set_rx_mode(struct net_device *dev)\n{\n\tmv643xx_eth_program_unicast_filter(dev);\n\tmv643xx_eth_program_multicast_filter(dev);\n}\n\nstatic int mv643xx_eth_set_mac_address(struct net_device *dev, void *addr)\n{\n\tstruct sockaddr *sa = addr;\n\n\tif (!is_valid_ether_addr(sa->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\teth_hw_addr_set(dev, sa->sa_data);\n\n\tnetif_addr_lock_bh(dev);\n\tmv643xx_eth_program_unicast_filter(dev);\n\tnetif_addr_unlock_bh(dev);\n\n\treturn 0;\n}\n\n\n \nstatic int rxq_init(struct mv643xx_eth_private *mp, int index)\n{\n\tstruct rx_queue *rxq = mp->rxq + index;\n\tstruct rx_desc *rx_desc;\n\tint size;\n\tint i;\n\n\trxq->index = index;\n\n\trxq->rx_ring_size = mp->rx_ring_size;\n\n\trxq->rx_desc_count = 0;\n\trxq->rx_curr_desc = 0;\n\trxq->rx_used_desc = 0;\n\n\tsize = rxq->rx_ring_size * sizeof(struct rx_desc);\n\n\tif (index == 0 && size <= mp->rx_desc_sram_size) {\n\t\trxq->rx_desc_area = ioremap(mp->rx_desc_sram_addr,\n\t\t\t\t\t\tmp->rx_desc_sram_size);\n\t\trxq->rx_desc_dma = mp->rx_desc_sram_addr;\n\t} else {\n\t\trxq->rx_desc_area = dma_alloc_coherent(mp->dev->dev.parent,\n\t\t\t\t\t\t       size, &rxq->rx_desc_dma,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t}\n\n\tif (rxq->rx_desc_area == NULL) {\n\t\tnetdev_err(mp->dev,\n\t\t\t   \"can't allocate rx ring (%d bytes)\\n\", size);\n\t\tgoto out;\n\t}\n\tmemset(rxq->rx_desc_area, 0, size);\n\n\trxq->rx_desc_area_size = size;\n\trxq->rx_skb = kcalloc(rxq->rx_ring_size, sizeof(*rxq->rx_skb),\n\t\t\t\t    GFP_KERNEL);\n\tif (rxq->rx_skb == NULL)\n\t\tgoto out_free;\n\n\trx_desc = rxq->rx_desc_area;\n\tfor (i = 0; i < rxq->rx_ring_size; i++) {\n\t\tint nexti;\n\n\t\tnexti = i + 1;\n\t\tif (nexti == rxq->rx_ring_size)\n\t\t\tnexti = 0;\n\n\t\trx_desc[i].next_desc_ptr = rxq->rx_desc_dma +\n\t\t\t\t\tnexti * sizeof(struct rx_desc);\n\t}\n\n\treturn 0;\n\n\nout_free:\n\tif (index == 0 && size <= mp->rx_desc_sram_size)\n\t\tiounmap(rxq->rx_desc_area);\n\telse\n\t\tdma_free_coherent(mp->dev->dev.parent, size,\n\t\t\t\t  rxq->rx_desc_area,\n\t\t\t\t  rxq->rx_desc_dma);\n\nout:\n\treturn -ENOMEM;\n}\n\nstatic void rxq_deinit(struct rx_queue *rxq)\n{\n\tstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\n\tint i;\n\n\trxq_disable(rxq);\n\n\tfor (i = 0; i < rxq->rx_ring_size; i++) {\n\t\tif (rxq->rx_skb[i]) {\n\t\t\tdev_consume_skb_any(rxq->rx_skb[i]);\n\t\t\trxq->rx_desc_count--;\n\t\t}\n\t}\n\n\tif (rxq->rx_desc_count) {\n\t\tnetdev_err(mp->dev, \"error freeing rx ring -- %d skbs stuck\\n\",\n\t\t\t   rxq->rx_desc_count);\n\t}\n\n\tif (rxq->index == 0 &&\n\t    rxq->rx_desc_area_size <= mp->rx_desc_sram_size)\n\t\tiounmap(rxq->rx_desc_area);\n\telse\n\t\tdma_free_coherent(mp->dev->dev.parent, rxq->rx_desc_area_size,\n\t\t\t\t  rxq->rx_desc_area, rxq->rx_desc_dma);\n\n\tkfree(rxq->rx_skb);\n}\n\nstatic int txq_init(struct mv643xx_eth_private *mp, int index)\n{\n\tstruct tx_queue *txq = mp->txq + index;\n\tstruct tx_desc *tx_desc;\n\tint size;\n\tint ret;\n\tint i;\n\n\ttxq->index = index;\n\n\ttxq->tx_ring_size = mp->tx_ring_size;\n\n\t \n\ttxq->tx_stop_threshold = txq->tx_ring_size - MV643XX_MAX_SKB_DESCS;\n\ttxq->tx_wake_threshold = txq->tx_stop_threshold / 2;\n\n\ttxq->tx_desc_count = 0;\n\ttxq->tx_curr_desc = 0;\n\ttxq->tx_used_desc = 0;\n\n\tsize = txq->tx_ring_size * sizeof(struct tx_desc);\n\n\tif (index == 0 && size <= mp->tx_desc_sram_size) {\n\t\ttxq->tx_desc_area = ioremap(mp->tx_desc_sram_addr,\n\t\t\t\t\t\tmp->tx_desc_sram_size);\n\t\ttxq->tx_desc_dma = mp->tx_desc_sram_addr;\n\t} else {\n\t\ttxq->tx_desc_area = dma_alloc_coherent(mp->dev->dev.parent,\n\t\t\t\t\t\t       size, &txq->tx_desc_dma,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t}\n\n\tif (txq->tx_desc_area == NULL) {\n\t\tnetdev_err(mp->dev,\n\t\t\t   \"can't allocate tx ring (%d bytes)\\n\", size);\n\t\treturn -ENOMEM;\n\t}\n\tmemset(txq->tx_desc_area, 0, size);\n\n\ttxq->tx_desc_area_size = size;\n\n\ttx_desc = txq->tx_desc_area;\n\tfor (i = 0; i < txq->tx_ring_size; i++) {\n\t\tstruct tx_desc *txd = tx_desc + i;\n\t\tint nexti;\n\n\t\tnexti = i + 1;\n\t\tif (nexti == txq->tx_ring_size)\n\t\t\tnexti = 0;\n\n\t\ttxd->cmd_sts = 0;\n\t\ttxd->next_desc_ptr = txq->tx_desc_dma +\n\t\t\t\t\tnexti * sizeof(struct tx_desc);\n\t}\n\n\ttxq->tx_desc_mapping = kcalloc(txq->tx_ring_size, sizeof(char),\n\t\t\t\t       GFP_KERNEL);\n\tif (!txq->tx_desc_mapping) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_desc_area;\n\t}\n\n\t \n\ttxq->tso_hdrs = dma_alloc_coherent(mp->dev->dev.parent,\n\t\t\t\t\t   txq->tx_ring_size * TSO_HEADER_SIZE,\n\t\t\t\t\t   &txq->tso_hdrs_dma, GFP_KERNEL);\n\tif (txq->tso_hdrs == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_desc_mapping;\n\t}\n\tskb_queue_head_init(&txq->tx_skb);\n\n\treturn 0;\n\nerr_free_desc_mapping:\n\tkfree(txq->tx_desc_mapping);\nerr_free_desc_area:\n\tif (index == 0 && size <= mp->tx_desc_sram_size)\n\t\tiounmap(txq->tx_desc_area);\n\telse\n\t\tdma_free_coherent(mp->dev->dev.parent, txq->tx_desc_area_size,\n\t\t\t\t  txq->tx_desc_area, txq->tx_desc_dma);\n\treturn ret;\n}\n\nstatic void txq_deinit(struct tx_queue *txq)\n{\n\tstruct mv643xx_eth_private *mp = txq_to_mp(txq);\n\n\ttxq_disable(txq);\n\ttxq_reclaim(txq, txq->tx_ring_size, 1);\n\n\tBUG_ON(txq->tx_used_desc != txq->tx_curr_desc);\n\n\tif (txq->index == 0 &&\n\t    txq->tx_desc_area_size <= mp->tx_desc_sram_size)\n\t\tiounmap(txq->tx_desc_area);\n\telse\n\t\tdma_free_coherent(mp->dev->dev.parent, txq->tx_desc_area_size,\n\t\t\t\t  txq->tx_desc_area, txq->tx_desc_dma);\n\tkfree(txq->tx_desc_mapping);\n\n\tif (txq->tso_hdrs)\n\t\tdma_free_coherent(mp->dev->dev.parent,\n\t\t\t\t  txq->tx_ring_size * TSO_HEADER_SIZE,\n\t\t\t\t  txq->tso_hdrs, txq->tso_hdrs_dma);\n}\n\n\n \nstatic int mv643xx_eth_collect_events(struct mv643xx_eth_private *mp)\n{\n\tu32 int_cause;\n\tu32 int_cause_ext;\n\n\tint_cause = rdlp(mp, INT_CAUSE) & mp->int_mask;\n\tif (int_cause == 0)\n\t\treturn 0;\n\n\tint_cause_ext = 0;\n\tif (int_cause & INT_EXT) {\n\t\tint_cause &= ~INT_EXT;\n\t\tint_cause_ext = rdlp(mp, INT_CAUSE_EXT);\n\t}\n\n\tif (int_cause) {\n\t\twrlp(mp, INT_CAUSE, ~int_cause);\n\t\tmp->work_tx_end |= ((int_cause & INT_TX_END) >> 19) &\n\t\t\t\t~(rdlp(mp, TXQ_COMMAND) & 0xff);\n\t\tmp->work_rx |= (int_cause & INT_RX) >> 2;\n\t}\n\n\tint_cause_ext &= INT_EXT_LINK_PHY | INT_EXT_TX;\n\tif (int_cause_ext) {\n\t\twrlp(mp, INT_CAUSE_EXT, ~int_cause_ext);\n\t\tif (int_cause_ext & INT_EXT_LINK_PHY)\n\t\t\tmp->work_link = 1;\n\t\tmp->work_tx |= int_cause_ext & INT_EXT_TX;\n\t}\n\n\treturn 1;\n}\n\nstatic irqreturn_t mv643xx_eth_irq(int irq, void *dev_id)\n{\n\tstruct net_device *dev = (struct net_device *)dev_id;\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\n\tif (unlikely(!mv643xx_eth_collect_events(mp)))\n\t\treturn IRQ_NONE;\n\n\twrlp(mp, INT_MASK, 0);\n\tnapi_schedule(&mp->napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void handle_link_event(struct mv643xx_eth_private *mp)\n{\n\tstruct net_device *dev = mp->dev;\n\tu32 port_status;\n\tint speed;\n\tint duplex;\n\tint fc;\n\n\tport_status = rdlp(mp, PORT_STATUS);\n\tif (!(port_status & LINK_UP)) {\n\t\tif (netif_carrier_ok(dev)) {\n\t\t\tint i;\n\n\t\t\tnetdev_info(dev, \"link down\\n\");\n\n\t\t\tnetif_carrier_off(dev);\n\n\t\t\tfor (i = 0; i < mp->txq_count; i++) {\n\t\t\t\tstruct tx_queue *txq = mp->txq + i;\n\n\t\t\t\ttxq_reclaim(txq, txq->tx_ring_size, 1);\n\t\t\t\ttxq_reset_hw_ptr(txq);\n\t\t\t}\n\t\t}\n\t\treturn;\n\t}\n\n\tswitch (port_status & PORT_SPEED_MASK) {\n\tcase PORT_SPEED_10:\n\t\tspeed = 10;\n\t\tbreak;\n\tcase PORT_SPEED_100:\n\t\tspeed = 100;\n\t\tbreak;\n\tcase PORT_SPEED_1000:\n\t\tspeed = 1000;\n\t\tbreak;\n\tdefault:\n\t\tspeed = -1;\n\t\tbreak;\n\t}\n\tduplex = (port_status & FULL_DUPLEX) ? 1 : 0;\n\tfc = (port_status & FLOW_CONTROL_ENABLED) ? 1 : 0;\n\n\tnetdev_info(dev, \"link up, %d Mb/s, %s duplex, flow control %sabled\\n\",\n\t\t    speed, duplex ? \"full\" : \"half\", fc ? \"en\" : \"dis\");\n\n\tif (!netif_carrier_ok(dev))\n\t\tnetif_carrier_on(dev);\n}\n\nstatic int mv643xx_eth_poll(struct napi_struct *napi, int budget)\n{\n\tstruct mv643xx_eth_private *mp;\n\tint work_done;\n\n\tmp = container_of(napi, struct mv643xx_eth_private, napi);\n\n\tif (unlikely(mp->oom)) {\n\t\tmp->oom = 0;\n\t\tdel_timer(&mp->rx_oom);\n\t}\n\n\twork_done = 0;\n\twhile (work_done < budget) {\n\t\tu8 queue_mask;\n\t\tint queue;\n\t\tint work_tbd;\n\n\t\tif (mp->work_link) {\n\t\t\tmp->work_link = 0;\n\t\t\thandle_link_event(mp);\n\t\t\twork_done++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tqueue_mask = mp->work_tx | mp->work_tx_end | mp->work_rx;\n\t\tif (likely(!mp->oom))\n\t\t\tqueue_mask |= mp->work_rx_refill;\n\n\t\tif (!queue_mask) {\n\t\t\tif (mv643xx_eth_collect_events(mp))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\n\t\tqueue = fls(queue_mask) - 1;\n\t\tqueue_mask = 1 << queue;\n\n\t\twork_tbd = budget - work_done;\n\t\tif (work_tbd > 16)\n\t\t\twork_tbd = 16;\n\n\t\tif (mp->work_tx_end & queue_mask) {\n\t\t\ttxq_kick(mp->txq + queue);\n\t\t} else if (mp->work_tx & queue_mask) {\n\t\t\twork_done += txq_reclaim(mp->txq + queue, work_tbd, 0);\n\t\t\ttxq_maybe_wake(mp->txq + queue);\n\t\t} else if (mp->work_rx & queue_mask) {\n\t\t\twork_done += rxq_process(mp->rxq + queue, work_tbd);\n\t\t} else if (!mp->oom && (mp->work_rx_refill & queue_mask)) {\n\t\t\twork_done += rxq_refill(mp->rxq + queue, work_tbd);\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (work_done < budget) {\n\t\tif (mp->oom)\n\t\t\tmod_timer(&mp->rx_oom, jiffies + (HZ / 10));\n\t\tnapi_complete_done(napi, work_done);\n\t\twrlp(mp, INT_MASK, mp->int_mask);\n\t}\n\n\treturn work_done;\n}\n\nstatic inline void oom_timer_wrapper(struct timer_list *t)\n{\n\tstruct mv643xx_eth_private *mp = from_timer(mp, t, rx_oom);\n\n\tnapi_schedule(&mp->napi);\n}\n\nstatic void port_start(struct mv643xx_eth_private *mp)\n{\n\tstruct net_device *dev = mp->dev;\n\tu32 pscr;\n\tint i;\n\n\t \n\tif (dev->phydev) {\n\t\tstruct ethtool_link_ksettings cmd;\n\n\t\tmv643xx_eth_get_link_ksettings(dev, &cmd);\n\t\tphy_init_hw(dev->phydev);\n\t\tmv643xx_eth_set_link_ksettings(\n\t\t\tdev, (const struct ethtool_link_ksettings *)&cmd);\n\t\tphy_start(dev->phydev);\n\t}\n\n\t \n\tpscr = rdlp(mp, PORT_SERIAL_CONTROL);\n\n\tpscr |= SERIAL_PORT_ENABLE;\n\twrlp(mp, PORT_SERIAL_CONTROL, pscr);\n\n\tpscr |= DO_NOT_FORCE_LINK_FAIL;\n\tif (!dev->phydev)\n\t\tpscr |= FORCE_LINK_PASS;\n\twrlp(mp, PORT_SERIAL_CONTROL, pscr);\n\n\t \n\ttx_set_rate(mp, 1000000000, 16777216);\n\tfor (i = 0; i < mp->txq_count; i++) {\n\t\tstruct tx_queue *txq = mp->txq + i;\n\n\t\ttxq_reset_hw_ptr(txq);\n\t\ttxq_set_rate(txq, 1000000000, 16777216);\n\t\ttxq_set_fixed_prio_mode(txq);\n\t}\n\n\t \n\tmv643xx_eth_set_features(mp->dev, mp->dev->features);\n\n\t \n\twrlp(mp, PORT_CONFIG_EXT, 0x00000000);\n\n\t \n\tmv643xx_eth_program_unicast_filter(mp->dev);\n\n\t \n\tfor (i = 0; i < mp->rxq_count; i++) {\n\t\tstruct rx_queue *rxq = mp->rxq + i;\n\t\tu32 addr;\n\n\t\taddr = (u32)rxq->rx_desc_dma;\n\t\taddr += rxq->rx_curr_desc * sizeof(struct rx_desc);\n\t\twrlp(mp, RXQ_CURRENT_DESC_PTR(i), addr);\n\n\t\trxq_enable(rxq);\n\t}\n}\n\nstatic void mv643xx_eth_recalc_skb_size(struct mv643xx_eth_private *mp)\n{\n\tint skb_size;\n\n\t \n\tskb_size = mp->dev->mtu + 36;\n\n\t \n\tmp->skb_size = (skb_size + 7) & ~7;\n\n\t \n\tmp->skb_size += SKB_DMA_REALIGN;\n}\n\nstatic int mv643xx_eth_open(struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\tint err;\n\tint i;\n\n\twrlp(mp, INT_CAUSE, 0);\n\twrlp(mp, INT_CAUSE_EXT, 0);\n\trdlp(mp, INT_CAUSE_EXT);\n\n\terr = request_irq(dev->irq, mv643xx_eth_irq,\n\t\t\t  IRQF_SHARED, dev->name, dev);\n\tif (err) {\n\t\tnetdev_err(dev, \"can't assign irq\\n\");\n\t\treturn -EAGAIN;\n\t}\n\n\tmv643xx_eth_recalc_skb_size(mp);\n\n\tnapi_enable(&mp->napi);\n\n\tmp->int_mask = INT_EXT;\n\n\tfor (i = 0; i < mp->rxq_count; i++) {\n\t\terr = rxq_init(mp, i);\n\t\tif (err) {\n\t\t\twhile (--i >= 0)\n\t\t\t\trxq_deinit(mp->rxq + i);\n\t\t\tgoto out;\n\t\t}\n\n\t\trxq_refill(mp->rxq + i, INT_MAX);\n\t\tmp->int_mask |= INT_RX_0 << i;\n\t}\n\n\tif (mp->oom) {\n\t\tmp->rx_oom.expires = jiffies + (HZ / 10);\n\t\tadd_timer(&mp->rx_oom);\n\t}\n\n\tfor (i = 0; i < mp->txq_count; i++) {\n\t\terr = txq_init(mp, i);\n\t\tif (err) {\n\t\t\twhile (--i >= 0)\n\t\t\t\ttxq_deinit(mp->txq + i);\n\t\t\tgoto out_free;\n\t\t}\n\t\tmp->int_mask |= INT_TX_END_0 << i;\n\t}\n\n\tadd_timer(&mp->mib_counters_timer);\n\tport_start(mp);\n\n\twrlp(mp, INT_MASK_EXT, INT_EXT_LINK_PHY | INT_EXT_TX);\n\twrlp(mp, INT_MASK, mp->int_mask);\n\n\treturn 0;\n\n\nout_free:\n\tfor (i = 0; i < mp->rxq_count; i++)\n\t\trxq_deinit(mp->rxq + i);\nout:\n\tnapi_disable(&mp->napi);\n\tfree_irq(dev->irq, dev);\n\n\treturn err;\n}\n\nstatic void port_reset(struct mv643xx_eth_private *mp)\n{\n\tunsigned int data;\n\tint i;\n\n\tfor (i = 0; i < mp->rxq_count; i++)\n\t\trxq_disable(mp->rxq + i);\n\tfor (i = 0; i < mp->txq_count; i++)\n\t\ttxq_disable(mp->txq + i);\n\n\twhile (1) {\n\t\tu32 ps = rdlp(mp, PORT_STATUS);\n\n\t\tif ((ps & (TX_IN_PROGRESS | TX_FIFO_EMPTY)) == TX_FIFO_EMPTY)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\t \n\tdata = rdlp(mp, PORT_SERIAL_CONTROL);\n\tdata &= ~(SERIAL_PORT_ENABLE\t\t|\n\t\t  DO_NOT_FORCE_LINK_FAIL\t|\n\t\t  FORCE_LINK_PASS);\n\twrlp(mp, PORT_SERIAL_CONTROL, data);\n}\n\nstatic int mv643xx_eth_stop(struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\tint i;\n\n\twrlp(mp, INT_MASK_EXT, 0x00000000);\n\twrlp(mp, INT_MASK, 0x00000000);\n\trdlp(mp, INT_MASK);\n\n\tnapi_disable(&mp->napi);\n\n\tdel_timer_sync(&mp->rx_oom);\n\n\tnetif_carrier_off(dev);\n\tif (dev->phydev)\n\t\tphy_stop(dev->phydev);\n\tfree_irq(dev->irq, dev);\n\n\tport_reset(mp);\n\tmv643xx_eth_get_stats(dev);\n\tmib_counters_update(mp);\n\tdel_timer_sync(&mp->mib_counters_timer);\n\n\tfor (i = 0; i < mp->rxq_count; i++)\n\t\trxq_deinit(mp->rxq + i);\n\tfor (i = 0; i < mp->txq_count; i++)\n\t\ttxq_deinit(mp->txq + i);\n\n\treturn 0;\n}\n\nstatic int mv643xx_eth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tint ret;\n\n\tif (!dev->phydev)\n\t\treturn -ENOTSUPP;\n\n\tret = phy_mii_ioctl(dev->phydev, ifr, cmd);\n\tif (!ret)\n\t\tmv643xx_eth_adjust_link(dev);\n\treturn ret;\n}\n\nstatic int mv643xx_eth_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\n\tdev->mtu = new_mtu;\n\tmv643xx_eth_recalc_skb_size(mp);\n\ttx_set_rate(mp, 1000000000, 16777216);\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\t \n\tmv643xx_eth_stop(dev);\n\tif (mv643xx_eth_open(dev)) {\n\t\tnetdev_err(dev,\n\t\t\t   \"fatal error on re-opening device after MTU change\\n\");\n\t}\n\n\treturn 0;\n}\n\nstatic void tx_timeout_task(struct work_struct *ugly)\n{\n\tstruct mv643xx_eth_private *mp;\n\n\tmp = container_of(ugly, struct mv643xx_eth_private, tx_timeout_task);\n\tif (netif_running(mp->dev)) {\n\t\tnetif_tx_stop_all_queues(mp->dev);\n\t\tport_reset(mp);\n\t\tport_start(mp);\n\t\tnetif_tx_wake_all_queues(mp->dev);\n\t}\n}\n\nstatic void mv643xx_eth_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\n\tnetdev_info(dev, \"tx timeout\\n\");\n\n\tschedule_work(&mp->tx_timeout_task);\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void mv643xx_eth_netpoll(struct net_device *dev)\n{\n\tstruct mv643xx_eth_private *mp = netdev_priv(dev);\n\n\twrlp(mp, INT_MASK, 0x00000000);\n\trdlp(mp, INT_MASK);\n\n\tmv643xx_eth_irq(dev->irq, dev);\n\n\twrlp(mp, INT_MASK, mp->int_mask);\n}\n#endif\n\n\n \nstatic void\nmv643xx_eth_conf_mbus_windows(struct mv643xx_eth_shared_private *msp,\n\t\t\t      const struct mbus_dram_target_info *dram)\n{\n\tvoid __iomem *base = msp->base;\n\tu32 win_enable;\n\tu32 win_protect;\n\tint i;\n\n\tfor (i = 0; i < 6; i++) {\n\t\twritel(0, base + WINDOW_BASE(i));\n\t\twritel(0, base + WINDOW_SIZE(i));\n\t\tif (i < 4)\n\t\t\twritel(0, base + WINDOW_REMAP_HIGH(i));\n\t}\n\n\twin_enable = 0x3f;\n\twin_protect = 0;\n\n\tfor (i = 0; i < dram->num_cs; i++) {\n\t\tconst struct mbus_dram_window *cs = dram->cs + i;\n\n\t\twritel((cs->base & 0xffff0000) |\n\t\t\t(cs->mbus_attr << 8) |\n\t\t\tdram->mbus_dram_target_id, base + WINDOW_BASE(i));\n\t\twritel((cs->size - 1) & 0xffff0000, base + WINDOW_SIZE(i));\n\n\t\twin_enable &= ~(1 << i);\n\t\twin_protect |= 3 << (2 * i);\n\t}\n\n\twritel(win_enable, base + WINDOW_BAR_ENABLE);\n\tmsp->win_protect = win_protect;\n}\n\nstatic void infer_hw_params(struct mv643xx_eth_shared_private *msp)\n{\n\t \n\twritel(0x02000000, msp->base + 0x0400 + SDMA_CONFIG);\n\tif (readl(msp->base + 0x0400 + SDMA_CONFIG) & 0x02000000)\n\t\tmsp->extended_rx_coal_limit = 1;\n\telse\n\t\tmsp->extended_rx_coal_limit = 0;\n\n\t \n\twritel(1, msp->base + 0x0400 + TX_BW_MTU_MOVED);\n\tif (readl(msp->base + 0x0400 + TX_BW_MTU_MOVED) & 1) {\n\t\tmsp->tx_bw_control = TX_BW_CONTROL_NEW_LAYOUT;\n\t} else {\n\t\twritel(7, msp->base + 0x0400 + TX_BW_RATE);\n\t\tif (readl(msp->base + 0x0400 + TX_BW_RATE) & 7)\n\t\t\tmsp->tx_bw_control = TX_BW_CONTROL_OLD_LAYOUT;\n\t\telse\n\t\t\tmsp->tx_bw_control = TX_BW_CONTROL_ABSENT;\n\t}\n}\n\n#if defined(CONFIG_OF)\nstatic const struct of_device_id mv643xx_eth_shared_ids[] = {\n\t{ .compatible = \"marvell,orion-eth\", },\n\t{ .compatible = \"marvell,kirkwood-eth\", },\n\t{ }\n};\nMODULE_DEVICE_TABLE(of, mv643xx_eth_shared_ids);\n#endif\n\n#ifdef CONFIG_OF_IRQ\n#define mv643xx_eth_property(_np, _name, _v)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tu32 tmp;\t\t\t\t\t\t\\\n\t\tif (!of_property_read_u32(_np, \"marvell,\" _name, &tmp))\t\\\n\t\t\t_v = tmp;\t\t\t\t\t\\\n\t} while (0)\n\nstatic struct platform_device *port_platdev[3];\n\nstatic void mv643xx_eth_shared_of_remove(void)\n{\n\tint n;\n\n\tfor (n = 0; n < 3; n++) {\n\t\tplatform_device_del(port_platdev[n]);\n\t\tport_platdev[n] = NULL;\n\t}\n}\n\nstatic int mv643xx_eth_shared_of_add_port(struct platform_device *pdev,\n\t\t\t\t\t  struct device_node *pnp)\n{\n\tstruct platform_device *ppdev;\n\tstruct mv643xx_eth_platform_data ppd;\n\tstruct resource res;\n\tint ret;\n\tint dev_num = 0;\n\n\tmemset(&ppd, 0, sizeof(ppd));\n\tppd.shared = pdev;\n\n\tmemset(&res, 0, sizeof(res));\n\tif (of_irq_to_resource(pnp, 0, &res) <= 0) {\n\t\tdev_err(&pdev->dev, \"missing interrupt on %pOFn\\n\", pnp);\n\t\treturn -EINVAL;\n\t}\n\n\tif (of_property_read_u32(pnp, \"reg\", &ppd.port_number)) {\n\t\tdev_err(&pdev->dev, \"missing reg property on %pOFn\\n\", pnp);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ppd.port_number >= 3) {\n\t\tdev_err(&pdev->dev, \"invalid reg property on %pOFn\\n\", pnp);\n\t\treturn -EINVAL;\n\t}\n\n\twhile (dev_num < 3 && port_platdev[dev_num])\n\t\tdev_num++;\n\n\tif (dev_num == 3) {\n\t\tdev_err(&pdev->dev, \"too many ports registered\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = of_get_mac_address(pnp, ppd.mac_addr);\n\tif (ret == -EPROBE_DEFER)\n\t\treturn ret;\n\n\tmv643xx_eth_property(pnp, \"tx-queue-size\", ppd.tx_queue_size);\n\tmv643xx_eth_property(pnp, \"tx-sram-addr\", ppd.tx_sram_addr);\n\tmv643xx_eth_property(pnp, \"tx-sram-size\", ppd.tx_sram_size);\n\tmv643xx_eth_property(pnp, \"rx-queue-size\", ppd.rx_queue_size);\n\tmv643xx_eth_property(pnp, \"rx-sram-addr\", ppd.rx_sram_addr);\n\tmv643xx_eth_property(pnp, \"rx-sram-size\", ppd.rx_sram_size);\n\n\tof_get_phy_mode(pnp, &ppd.interface);\n\n\tppd.phy_node = of_parse_phandle(pnp, \"phy-handle\", 0);\n\tif (!ppd.phy_node) {\n\t\tppd.phy_addr = MV643XX_ETH_PHY_NONE;\n\t\tof_property_read_u32(pnp, \"speed\", &ppd.speed);\n\t\tof_property_read_u32(pnp, \"duplex\", &ppd.duplex);\n\t}\n\n\tppdev = platform_device_alloc(MV643XX_ETH_NAME, dev_num);\n\tif (!ppdev)\n\t\treturn -ENOMEM;\n\tppdev->dev.coherent_dma_mask = DMA_BIT_MASK(32);\n\tppdev->dev.of_node = pnp;\n\n\tret = platform_device_add_resources(ppdev, &res, 1);\n\tif (ret)\n\t\tgoto port_err;\n\n\tret = platform_device_add_data(ppdev, &ppd, sizeof(ppd));\n\tif (ret)\n\t\tgoto port_err;\n\n\tret = platform_device_add(ppdev);\n\tif (ret)\n\t\tgoto port_err;\n\n\tport_platdev[dev_num] = ppdev;\n\n\treturn 0;\n\nport_err:\n\tplatform_device_put(ppdev);\n\treturn ret;\n}\n\nstatic int mv643xx_eth_shared_of_probe(struct platform_device *pdev)\n{\n\tstruct mv643xx_eth_shared_platform_data *pd;\n\tstruct device_node *pnp, *np = pdev->dev.of_node;\n\tint ret;\n\n\t \n\tif (!np)\n\t\treturn 0;\n\n\tpd = devm_kzalloc(&pdev->dev, sizeof(*pd), GFP_KERNEL);\n\tif (!pd)\n\t\treturn -ENOMEM;\n\tpdev->dev.platform_data = pd;\n\n\tmv643xx_eth_property(np, \"tx-checksum-limit\", pd->tx_csum_limit);\n\n\tfor_each_available_child_of_node(np, pnp) {\n\t\tret = mv643xx_eth_shared_of_add_port(pdev, pnp);\n\t\tif (ret) {\n\t\t\tof_node_put(pnp);\n\t\t\tmv643xx_eth_shared_of_remove();\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\n\n#else\nstatic inline int mv643xx_eth_shared_of_probe(struct platform_device *pdev)\n{\n\treturn 0;\n}\n\nstatic inline void mv643xx_eth_shared_of_remove(void)\n{\n}\n#endif\n\nstatic int mv643xx_eth_shared_probe(struct platform_device *pdev)\n{\n\tstatic int mv643xx_eth_version_printed;\n\tstruct mv643xx_eth_shared_platform_data *pd;\n\tstruct mv643xx_eth_shared_private *msp;\n\tconst struct mbus_dram_target_info *dram;\n\tstruct resource *res;\n\tint ret;\n\n\tif (!mv643xx_eth_version_printed++)\n\t\tpr_notice(\"MV-643xx 10/100/1000 ethernet driver version %s\\n\",\n\t\t\t  mv643xx_eth_driver_version);\n\n\tres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tif (res == NULL)\n\t\treturn -EINVAL;\n\n\tmsp = devm_kzalloc(&pdev->dev, sizeof(*msp), GFP_KERNEL);\n\tif (msp == NULL)\n\t\treturn -ENOMEM;\n\tplatform_set_drvdata(pdev, msp);\n\n\tmsp->base = devm_ioremap(&pdev->dev, res->start, resource_size(res));\n\tif (msp->base == NULL)\n\t\treturn -ENOMEM;\n\n\tmsp->clk = devm_clk_get(&pdev->dev, NULL);\n\tif (!IS_ERR(msp->clk))\n\t\tclk_prepare_enable(msp->clk);\n\n\t \n\tdram = mv_mbus_dram_info();\n\tif (dram)\n\t\tmv643xx_eth_conf_mbus_windows(msp, dram);\n\n\tret = mv643xx_eth_shared_of_probe(pdev);\n\tif (ret)\n\t\tgoto err_put_clk;\n\tpd = dev_get_platdata(&pdev->dev);\n\n\tmsp->tx_csum_limit = (pd != NULL && pd->tx_csum_limit) ?\n\t\t\t\t\tpd->tx_csum_limit : 9 * 1024;\n\tinfer_hw_params(msp);\n\n\treturn 0;\n\nerr_put_clk:\n\tif (!IS_ERR(msp->clk))\n\t\tclk_disable_unprepare(msp->clk);\n\treturn ret;\n}\n\nstatic int mv643xx_eth_shared_remove(struct platform_device *pdev)\n{\n\tstruct mv643xx_eth_shared_private *msp = platform_get_drvdata(pdev);\n\n\tmv643xx_eth_shared_of_remove();\n\tif (!IS_ERR(msp->clk))\n\t\tclk_disable_unprepare(msp->clk);\n\treturn 0;\n}\n\nstatic struct platform_driver mv643xx_eth_shared_driver = {\n\t.probe\t\t= mv643xx_eth_shared_probe,\n\t.remove\t\t= mv643xx_eth_shared_remove,\n\t.driver = {\n\t\t.name\t= MV643XX_ETH_SHARED_NAME,\n\t\t.of_match_table = of_match_ptr(mv643xx_eth_shared_ids),\n\t},\n};\n\nstatic void phy_addr_set(struct mv643xx_eth_private *mp, int phy_addr)\n{\n\tint addr_shift = 5 * mp->port_num;\n\tu32 data;\n\n\tdata = rdl(mp, PHY_ADDR);\n\tdata &= ~(0x1f << addr_shift);\n\tdata |= (phy_addr & 0x1f) << addr_shift;\n\twrl(mp, PHY_ADDR, data);\n}\n\nstatic int phy_addr_get(struct mv643xx_eth_private *mp)\n{\n\tunsigned int data;\n\n\tdata = rdl(mp, PHY_ADDR);\n\n\treturn (data >> (5 * mp->port_num)) & 0x1f;\n}\n\nstatic void set_params(struct mv643xx_eth_private *mp,\n\t\t       struct mv643xx_eth_platform_data *pd)\n{\n\tstruct net_device *dev = mp->dev;\n\tunsigned int tx_ring_size;\n\n\tif (is_valid_ether_addr(pd->mac_addr)) {\n\t\teth_hw_addr_set(dev, pd->mac_addr);\n\t} else {\n\t\tu8 addr[ETH_ALEN];\n\n\t\tuc_addr_get(mp, addr);\n\t\teth_hw_addr_set(dev, addr);\n\t}\n\n\tmp->rx_ring_size = DEFAULT_RX_QUEUE_SIZE;\n\tif (pd->rx_queue_size)\n\t\tmp->rx_ring_size = pd->rx_queue_size;\n\tmp->rx_desc_sram_addr = pd->rx_sram_addr;\n\tmp->rx_desc_sram_size = pd->rx_sram_size;\n\n\tmp->rxq_count = pd->rx_queue_count ? : 1;\n\n\ttx_ring_size = DEFAULT_TX_QUEUE_SIZE;\n\tif (pd->tx_queue_size)\n\t\ttx_ring_size = pd->tx_queue_size;\n\n\tmp->tx_ring_size = clamp_t(unsigned int, tx_ring_size,\n\t\t\t\t   MV643XX_MAX_SKB_DESCS * 2, 4096);\n\tif (mp->tx_ring_size != tx_ring_size)\n\t\tnetdev_warn(dev, \"TX queue size set to %u (requested %u)\\n\",\n\t\t\t    mp->tx_ring_size, tx_ring_size);\n\n\tmp->tx_desc_sram_addr = pd->tx_sram_addr;\n\tmp->tx_desc_sram_size = pd->tx_sram_size;\n\n\tmp->txq_count = pd->tx_queue_count ? : 1;\n}\n\nstatic int get_phy_mode(struct mv643xx_eth_private *mp)\n{\n\tstruct device *dev = mp->dev->dev.parent;\n\tphy_interface_t iface;\n\tint err;\n\n\tif (dev->of_node)\n\t\terr = of_get_phy_mode(dev->of_node, &iface);\n\n\t \n\tif (!dev->of_node || err)\n\t\tiface = PHY_INTERFACE_MODE_GMII;\n\treturn iface;\n}\n\nstatic struct phy_device *phy_scan(struct mv643xx_eth_private *mp,\n\t\t\t\t   int phy_addr)\n{\n\tstruct phy_device *phydev;\n\tint start;\n\tint num;\n\tint i;\n\tchar phy_id[MII_BUS_ID_SIZE + 3];\n\n\tif (phy_addr == MV643XX_ETH_PHY_ADDR_DEFAULT) {\n\t\tstart = phy_addr_get(mp) & 0x1f;\n\t\tnum = 32;\n\t} else {\n\t\tstart = phy_addr & 0x1f;\n\t\tnum = 1;\n\t}\n\n\t \n\tphydev = ERR_PTR(-ENODEV);\n\tfor (i = 0; i < num; i++) {\n\t\tint addr = (start + i) & 0x1f;\n\n\t\tsnprintf(phy_id, sizeof(phy_id), PHY_ID_FMT,\n\t\t\t\t\"orion-mdio-mii\", addr);\n\n\t\tphydev = phy_connect(mp->dev, phy_id, mv643xx_eth_adjust_link,\n\t\t\t\t     get_phy_mode(mp));\n\t\tif (!IS_ERR(phydev)) {\n\t\t\tphy_addr_set(mp, addr);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn phydev;\n}\n\nstatic void phy_init(struct mv643xx_eth_private *mp, int speed, int duplex)\n{\n\tstruct net_device *dev = mp->dev;\n\tstruct phy_device *phy = dev->phydev;\n\n\tif (speed == 0) {\n\t\tphy->autoneg = AUTONEG_ENABLE;\n\t\tphy->speed = 0;\n\t\tphy->duplex = 0;\n\t\tlinkmode_copy(phy->advertising, phy->supported);\n\t\tlinkmode_set_bit(ETHTOOL_LINK_MODE_Autoneg_BIT,\n\t\t\t\t phy->advertising);\n\t} else {\n\t\tphy->autoneg = AUTONEG_DISABLE;\n\t\tlinkmode_zero(phy->advertising);\n\t\tphy->speed = speed;\n\t\tphy->duplex = duplex;\n\t}\n\tphy_start_aneg(phy);\n}\n\nstatic void init_pscr(struct mv643xx_eth_private *mp, int speed, int duplex)\n{\n\tstruct net_device *dev = mp->dev;\n\tu32 pscr;\n\n\tpscr = rdlp(mp, PORT_SERIAL_CONTROL);\n\tif (pscr & SERIAL_PORT_ENABLE) {\n\t\tpscr &= ~SERIAL_PORT_ENABLE;\n\t\twrlp(mp, PORT_SERIAL_CONTROL, pscr);\n\t}\n\n\tpscr = MAX_RX_PACKET_9700BYTE | SERIAL_PORT_CONTROL_RESERVED;\n\tif (!dev->phydev) {\n\t\tpscr |= DISABLE_AUTO_NEG_SPEED_GMII;\n\t\tif (speed == SPEED_1000)\n\t\t\tpscr |= SET_GMII_SPEED_TO_1000;\n\t\telse if (speed == SPEED_100)\n\t\t\tpscr |= SET_MII_SPEED_TO_100;\n\n\t\tpscr |= DISABLE_AUTO_NEG_FOR_FLOW_CTRL;\n\n\t\tpscr |= DISABLE_AUTO_NEG_FOR_DUPLEX;\n\t\tif (duplex == DUPLEX_FULL)\n\t\t\tpscr |= SET_FULL_DUPLEX_MODE;\n\t}\n\n\twrlp(mp, PORT_SERIAL_CONTROL, pscr);\n}\n\nstatic const struct net_device_ops mv643xx_eth_netdev_ops = {\n\t.ndo_open\t\t= mv643xx_eth_open,\n\t.ndo_stop\t\t= mv643xx_eth_stop,\n\t.ndo_start_xmit\t\t= mv643xx_eth_xmit,\n\t.ndo_set_rx_mode\t= mv643xx_eth_set_rx_mode,\n\t.ndo_set_mac_address\t= mv643xx_eth_set_mac_address,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_eth_ioctl\t\t= mv643xx_eth_ioctl,\n\t.ndo_change_mtu\t\t= mv643xx_eth_change_mtu,\n\t.ndo_set_features\t= mv643xx_eth_set_features,\n\t.ndo_tx_timeout\t\t= mv643xx_eth_tx_timeout,\n\t.ndo_get_stats\t\t= mv643xx_eth_get_stats,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= mv643xx_eth_netpoll,\n#endif\n};\n\nstatic int mv643xx_eth_probe(struct platform_device *pdev)\n{\n\tstruct mv643xx_eth_platform_data *pd;\n\tstruct mv643xx_eth_private *mp;\n\tstruct net_device *dev;\n\tstruct phy_device *phydev = NULL;\n\tu32 psc1r;\n\tint err, irq;\n\n\tpd = dev_get_platdata(&pdev->dev);\n\tif (pd == NULL) {\n\t\tdev_err(&pdev->dev, \"no mv643xx_eth_platform_data\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (pd->shared == NULL) {\n\t\tdev_err(&pdev->dev, \"no mv643xx_eth_platform_data->shared\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tdev = alloc_etherdev_mq(sizeof(struct mv643xx_eth_private), 8);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\tmp = netdev_priv(dev);\n\tplatform_set_drvdata(pdev, mp);\n\n\tmp->shared = platform_get_drvdata(pd->shared);\n\tmp->base = mp->shared->base + 0x0400 + (pd->port_number << 10);\n\tmp->port_num = pd->port_number;\n\n\tmp->dev = dev;\n\n\tif (of_device_is_compatible(pdev->dev.of_node,\n\t\t\t\t    \"marvell,kirkwood-eth-port\")) {\n\t\tpsc1r = rdlp(mp, PORT_SERIAL_CONTROL1);\n\n\t\t \n\t\tpsc1r &= ~CLK125_BYPASS_EN;\n\n\t\t \n\t\tswitch (pd->interface) {\n\t\t \n\t\tcase PHY_INTERFACE_MODE_INTERNAL:\n\t\tcase PHY_INTERFACE_MODE_MII:\n\t\tcase PHY_INTERFACE_MODE_GMII:\n\t\t\tpsc1r &= ~RGMII_EN;\n\t\t\tbreak;\n\t\tcase PHY_INTERFACE_MODE_RGMII:\n\t\tcase PHY_INTERFACE_MODE_RGMII_ID:\n\t\tcase PHY_INTERFACE_MODE_RGMII_RXID:\n\t\tcase PHY_INTERFACE_MODE_RGMII_TXID:\n\t\t\tpsc1r |= RGMII_EN;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\n\t\twrlp(mp, PORT_SERIAL_CONTROL1, psc1r);\n\t}\n\n\t \n\tmp->t_clk = 133000000;\n\tmp->clk = devm_clk_get(&pdev->dev, NULL);\n\tif (!IS_ERR(mp->clk)) {\n\t\tclk_prepare_enable(mp->clk);\n\t\tmp->t_clk = clk_get_rate(mp->clk);\n\t} else if (!IS_ERR(mp->shared->clk)) {\n\t\tmp->t_clk = clk_get_rate(mp->shared->clk);\n\t}\n\n\tset_params(mp, pd);\n\tnetif_set_real_num_tx_queues(dev, mp->txq_count);\n\tnetif_set_real_num_rx_queues(dev, mp->rxq_count);\n\n\terr = 0;\n\tif (pd->phy_node) {\n\t\tphydev = of_phy_connect(mp->dev, pd->phy_node,\n\t\t\t\t\tmv643xx_eth_adjust_link, 0,\n\t\t\t\t\tget_phy_mode(mp));\n\t\tif (!phydev)\n\t\t\terr = -ENODEV;\n\t\telse\n\t\t\tphy_addr_set(mp, phydev->mdio.addr);\n\t} else if (pd->phy_addr != MV643XX_ETH_PHY_NONE) {\n\t\tphydev = phy_scan(mp, pd->phy_addr);\n\n\t\tif (IS_ERR(phydev))\n\t\t\terr = PTR_ERR(phydev);\n\t\telse\n\t\t\tphy_init(mp, pd->speed, pd->duplex);\n\t}\n\tif (err == -ENODEV) {\n\t\terr = -EPROBE_DEFER;\n\t\tgoto out;\n\t}\n\tif (err)\n\t\tgoto out;\n\n\tdev->ethtool_ops = &mv643xx_eth_ethtool_ops;\n\n\tinit_pscr(mp, pd->speed, pd->duplex);\n\n\n\tmib_counters_clear(mp);\n\n\ttimer_setup(&mp->mib_counters_timer, mib_counters_timer_wrapper, 0);\n\tmp->mib_counters_timer.expires = jiffies + 30 * HZ;\n\n\tspin_lock_init(&mp->mib_counters_lock);\n\n\tINIT_WORK(&mp->tx_timeout_task, tx_timeout_task);\n\n\tnetif_napi_add(dev, &mp->napi, mv643xx_eth_poll);\n\n\ttimer_setup(&mp->rx_oom, oom_timer_wrapper, 0);\n\n\n\tirq = platform_get_irq(pdev, 0);\n\tif (WARN_ON(irq < 0)) {\n\t\terr = irq;\n\t\tgoto out;\n\t}\n\tdev->irq = irq;\n\n\tdev->netdev_ops = &mv643xx_eth_netdev_ops;\n\n\tdev->watchdog_timeo = 2 * HZ;\n\tdev->base_addr = 0;\n\n\tdev->features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO;\n\tdev->vlan_features = dev->features;\n\n\tdev->features |= NETIF_F_RXCSUM;\n\tdev->hw_features = dev->features;\n\n\tdev->priv_flags |= IFF_UNICAST_FLT;\n\tnetif_set_tso_max_segs(dev, MV643XX_MAX_TSO_SEGS);\n\n\t \n\tdev->min_mtu = 64;\n\tdev->max_mtu = 9500;\n\n\tif (mp->shared->win_protect)\n\t\twrl(mp, WINDOW_PROTECT(mp->port_num), mp->shared->win_protect);\n\n\tnetif_carrier_off(dev);\n\n\twrlp(mp, SDMA_CONFIG, PORT_SDMA_CONFIG_DEFAULT_VALUE);\n\n\tset_rx_coal(mp, 250);\n\tset_tx_coal(mp, 0);\n\n\terr = register_netdev(dev);\n\tif (err)\n\t\tgoto out;\n\n\tnetdev_notice(dev, \"port %d with MAC address %pM\\n\",\n\t\t      mp->port_num, dev->dev_addr);\n\n\tif (mp->tx_desc_sram_size > 0)\n\t\tnetdev_notice(dev, \"configured with sram\\n\");\n\n\treturn 0;\n\nout:\n\tif (!IS_ERR(mp->clk))\n\t\tclk_disable_unprepare(mp->clk);\n\tfree_netdev(dev);\n\n\treturn err;\n}\n\nstatic int mv643xx_eth_remove(struct platform_device *pdev)\n{\n\tstruct mv643xx_eth_private *mp = platform_get_drvdata(pdev);\n\tstruct net_device *dev = mp->dev;\n\n\tunregister_netdev(mp->dev);\n\tif (dev->phydev)\n\t\tphy_disconnect(dev->phydev);\n\tcancel_work_sync(&mp->tx_timeout_task);\n\n\tif (!IS_ERR(mp->clk))\n\t\tclk_disable_unprepare(mp->clk);\n\n\tfree_netdev(mp->dev);\n\n\treturn 0;\n}\n\nstatic void mv643xx_eth_shutdown(struct platform_device *pdev)\n{\n\tstruct mv643xx_eth_private *mp = platform_get_drvdata(pdev);\n\n\t \n\twrlp(mp, INT_MASK, 0);\n\trdlp(mp, INT_MASK);\n\n\tif (netif_running(mp->dev))\n\t\tport_reset(mp);\n}\n\nstatic struct platform_driver mv643xx_eth_driver = {\n\t.probe\t\t= mv643xx_eth_probe,\n\t.remove\t\t= mv643xx_eth_remove,\n\t.shutdown\t= mv643xx_eth_shutdown,\n\t.driver = {\n\t\t.name\t= MV643XX_ETH_NAME,\n\t},\n};\n\nstatic struct platform_driver * const drivers[] = {\n\t&mv643xx_eth_shared_driver,\n\t&mv643xx_eth_driver,\n};\n\nstatic int __init mv643xx_eth_init_module(void)\n{\n\treturn platform_register_drivers(drivers, ARRAY_SIZE(drivers));\n}\nmodule_init(mv643xx_eth_init_module);\n\nstatic void __exit mv643xx_eth_cleanup_module(void)\n{\n\tplatform_unregister_drivers(drivers, ARRAY_SIZE(drivers));\n}\nmodule_exit(mv643xx_eth_cleanup_module);\n\nMODULE_AUTHOR(\"Rabeeh Khoury, Assaf Hoffman, Matthew Dharm, \"\n\t      \"Manish Lachwani, Dale Farnsworth and Lennert Buytenhek\");\nMODULE_DESCRIPTION(\"Ethernet driver for Marvell MV643XX\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"platform:\" MV643XX_ETH_SHARED_NAME);\nMODULE_ALIAS(\"platform:\" MV643XX_ETH_NAME);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}