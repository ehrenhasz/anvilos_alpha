{
  "module_name": "xgbe-drv.c",
  "hash_id": "111285abe12eec64080ba149d33d008c04471f2edc2ccf72ad8f23d3bf9b7bed",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/amd/xgbe/xgbe-drv.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/spinlock.h>\n#include <linux/tcp.h>\n#include <linux/if_vlan.h>\n#include <linux/interrupt.h>\n#include <linux/clk.h>\n#include <linux/if_ether.h>\n#include <linux/net_tstamp.h>\n#include <linux/phy.h>\n#include <net/vxlan.h>\n\n#include \"xgbe.h\"\n#include \"xgbe-common.h\"\n\nstatic unsigned int ecc_sec_info_threshold = 10;\nstatic unsigned int ecc_sec_warn_threshold = 10000;\nstatic unsigned int ecc_sec_period = 600;\nstatic unsigned int ecc_ded_threshold = 2;\nstatic unsigned int ecc_ded_period = 600;\n\n#ifdef CONFIG_AMD_XGBE_HAVE_ECC\n \nmodule_param(ecc_sec_info_threshold, uint, 0644);\nMODULE_PARM_DESC(ecc_sec_info_threshold,\n\t\t \" ECC corrected error informational threshold setting\");\n\nmodule_param(ecc_sec_warn_threshold, uint, 0644);\nMODULE_PARM_DESC(ecc_sec_warn_threshold,\n\t\t \" ECC corrected error warning threshold setting\");\n\nmodule_param(ecc_sec_period, uint, 0644);\nMODULE_PARM_DESC(ecc_sec_period, \" ECC corrected error period (in seconds)\");\n\nmodule_param(ecc_ded_threshold, uint, 0644);\nMODULE_PARM_DESC(ecc_ded_threshold, \" ECC detected error threshold setting\");\n\nmodule_param(ecc_ded_period, uint, 0644);\nMODULE_PARM_DESC(ecc_ded_period, \" ECC detected error period (in seconds)\");\n#endif\n\nstatic int xgbe_one_poll(struct napi_struct *, int);\nstatic int xgbe_all_poll(struct napi_struct *, int);\nstatic void xgbe_stop(struct xgbe_prv_data *);\n\nstatic void *xgbe_alloc_node(size_t size, int node)\n{\n\tvoid *mem;\n\n\tmem = kzalloc_node(size, GFP_KERNEL, node);\n\tif (!mem)\n\t\tmem = kzalloc(size, GFP_KERNEL);\n\n\treturn mem;\n}\n\nstatic void xgbe_free_channels(struct xgbe_prv_data *pdata)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(pdata->channel); i++) {\n\t\tif (!pdata->channel[i])\n\t\t\tcontinue;\n\n\t\tkfree(pdata->channel[i]->rx_ring);\n\t\tkfree(pdata->channel[i]->tx_ring);\n\t\tkfree(pdata->channel[i]);\n\n\t\tpdata->channel[i] = NULL;\n\t}\n\n\tpdata->channel_count = 0;\n}\n\nstatic int xgbe_alloc_channels(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_channel *channel;\n\tstruct xgbe_ring *ring;\n\tunsigned int count, i;\n\tunsigned int cpu;\n\tint node;\n\n\tcount = max_t(unsigned int, pdata->tx_ring_count, pdata->rx_ring_count);\n\tfor (i = 0; i < count; i++) {\n\t\t \n\t\tcpu = cpumask_local_spread(i, dev_to_node(pdata->dev));\n\n\t\t \n\t\tnode = cpu_to_node(cpu);\n\n\t\tchannel = xgbe_alloc_node(sizeof(*channel), node);\n\t\tif (!channel)\n\t\t\tgoto err_mem;\n\t\tpdata->channel[i] = channel;\n\n\t\tsnprintf(channel->name, sizeof(channel->name), \"channel-%u\", i);\n\t\tchannel->pdata = pdata;\n\t\tchannel->queue_index = i;\n\t\tchannel->dma_regs = pdata->xgmac_regs + DMA_CH_BASE +\n\t\t\t\t    (DMA_CH_INC * i);\n\t\tchannel->node = node;\n\t\tcpumask_set_cpu(cpu, &channel->affinity_mask);\n\n\t\tif (pdata->per_channel_irq)\n\t\t\tchannel->dma_irq = pdata->channel_irq[i];\n\n\t\tif (i < pdata->tx_ring_count) {\n\t\t\tring = xgbe_alloc_node(sizeof(*ring), node);\n\t\t\tif (!ring)\n\t\t\t\tgoto err_mem;\n\n\t\t\tspin_lock_init(&ring->lock);\n\t\t\tring->node = node;\n\n\t\t\tchannel->tx_ring = ring;\n\t\t}\n\n\t\tif (i < pdata->rx_ring_count) {\n\t\t\tring = xgbe_alloc_node(sizeof(*ring), node);\n\t\t\tif (!ring)\n\t\t\t\tgoto err_mem;\n\n\t\t\tspin_lock_init(&ring->lock);\n\t\t\tring->node = node;\n\n\t\t\tchannel->rx_ring = ring;\n\t\t}\n\n\t\tnetif_dbg(pdata, drv, pdata->netdev,\n\t\t\t  \"%s: cpu=%u, node=%d\\n\", channel->name, cpu, node);\n\n\t\tnetif_dbg(pdata, drv, pdata->netdev,\n\t\t\t  \"%s: dma_regs=%p, dma_irq=%d, tx=%p, rx=%p\\n\",\n\t\t\t  channel->name, channel->dma_regs, channel->dma_irq,\n\t\t\t  channel->tx_ring, channel->rx_ring);\n\t}\n\n\tpdata->channel_count = count;\n\n\treturn 0;\n\nerr_mem:\n\txgbe_free_channels(pdata);\n\n\treturn -ENOMEM;\n}\n\nstatic inline unsigned int xgbe_tx_avail_desc(struct xgbe_ring *ring)\n{\n\treturn (ring->rdesc_count - (ring->cur - ring->dirty));\n}\n\nstatic inline unsigned int xgbe_rx_dirty_desc(struct xgbe_ring *ring)\n{\n\treturn (ring->cur - ring->dirty);\n}\n\nstatic int xgbe_maybe_stop_tx_queue(struct xgbe_channel *channel,\n\t\t\t\t    struct xgbe_ring *ring, unsigned int count)\n{\n\tstruct xgbe_prv_data *pdata = channel->pdata;\n\n\tif (count > xgbe_tx_avail_desc(ring)) {\n\t\tnetif_info(pdata, drv, pdata->netdev,\n\t\t\t   \"Tx queue stopped, not enough descriptors available\\n\");\n\t\tnetif_stop_subqueue(pdata->netdev, channel->queue_index);\n\t\tring->tx.queue_stopped = 1;\n\n\t\t \n\t\tif (ring->tx.xmit_more)\n\t\t\tpdata->hw_if.tx_start_xmit(channel, ring);\n\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic int xgbe_calc_rx_buf_size(struct net_device *netdev, unsigned int mtu)\n{\n\tunsigned int rx_buf_size;\n\n\trx_buf_size = mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;\n\trx_buf_size = clamp_val(rx_buf_size, XGBE_RX_MIN_BUF_SIZE, PAGE_SIZE);\n\n\trx_buf_size = (rx_buf_size + XGBE_RX_BUF_ALIGN - 1) &\n\t\t      ~(XGBE_RX_BUF_ALIGN - 1);\n\n\treturn rx_buf_size;\n}\n\nstatic void xgbe_enable_rx_tx_int(struct xgbe_prv_data *pdata,\n\t\t\t\t  struct xgbe_channel *channel)\n{\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tenum xgbe_int int_id;\n\n\tif (channel->tx_ring && channel->rx_ring)\n\t\tint_id = XGMAC_INT_DMA_CH_SR_TI_RI;\n\telse if (channel->tx_ring)\n\t\tint_id = XGMAC_INT_DMA_CH_SR_TI;\n\telse if (channel->rx_ring)\n\t\tint_id = XGMAC_INT_DMA_CH_SR_RI;\n\telse\n\t\treturn;\n\n\thw_if->enable_int(channel, int_id);\n}\n\nstatic void xgbe_enable_rx_tx_ints(struct xgbe_prv_data *pdata)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < pdata->channel_count; i++)\n\t\txgbe_enable_rx_tx_int(pdata, pdata->channel[i]);\n}\n\nstatic void xgbe_disable_rx_tx_int(struct xgbe_prv_data *pdata,\n\t\t\t\t   struct xgbe_channel *channel)\n{\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tenum xgbe_int int_id;\n\n\tif (channel->tx_ring && channel->rx_ring)\n\t\tint_id = XGMAC_INT_DMA_CH_SR_TI_RI;\n\telse if (channel->tx_ring)\n\t\tint_id = XGMAC_INT_DMA_CH_SR_TI;\n\telse if (channel->rx_ring)\n\t\tint_id = XGMAC_INT_DMA_CH_SR_RI;\n\telse\n\t\treturn;\n\n\thw_if->disable_int(channel, int_id);\n}\n\nstatic void xgbe_disable_rx_tx_ints(struct xgbe_prv_data *pdata)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < pdata->channel_count; i++)\n\t\txgbe_disable_rx_tx_int(pdata, pdata->channel[i]);\n}\n\nstatic bool xgbe_ecc_sec(struct xgbe_prv_data *pdata, unsigned long *period,\n\t\t\t unsigned int *count, const char *area)\n{\n\tif (time_before(jiffies, *period)) {\n\t\t(*count)++;\n\t} else {\n\t\t*period = jiffies + (ecc_sec_period * HZ);\n\t\t*count = 1;\n\t}\n\n\tif (*count > ecc_sec_info_threshold)\n\t\tdev_warn_once(pdata->dev,\n\t\t\t      \"%s ECC corrected errors exceed informational threshold\\n\",\n\t\t\t      area);\n\n\tif (*count > ecc_sec_warn_threshold) {\n\t\tdev_warn_once(pdata->dev,\n\t\t\t      \"%s ECC corrected errors exceed warning threshold\\n\",\n\t\t\t      area);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool xgbe_ecc_ded(struct xgbe_prv_data *pdata, unsigned long *period,\n\t\t\t unsigned int *count, const char *area)\n{\n\tif (time_before(jiffies, *period)) {\n\t\t(*count)++;\n\t} else {\n\t\t*period = jiffies + (ecc_ded_period * HZ);\n\t\t*count = 1;\n\t}\n\n\tif (*count > ecc_ded_threshold) {\n\t\tnetdev_alert(pdata->netdev,\n\t\t\t     \"%s ECC detected errors exceed threshold\\n\",\n\t\t\t     area);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void xgbe_ecc_isr_task(struct tasklet_struct *t)\n{\n\tstruct xgbe_prv_data *pdata = from_tasklet(pdata, t, tasklet_ecc);\n\tunsigned int ecc_isr;\n\tbool stop = false;\n\n\t \n\tecc_isr = XP_IOREAD(pdata, XP_ECC_ISR);\n\tecc_isr &= XP_IOREAD(pdata, XP_ECC_IER);\n\tnetif_dbg(pdata, intr, pdata->netdev, \"ECC_ISR=%#010x\\n\", ecc_isr);\n\n\tif (XP_GET_BITS(ecc_isr, XP_ECC_ISR, TX_DED)) {\n\t\tstop |= xgbe_ecc_ded(pdata, &pdata->tx_ded_period,\n\t\t\t\t     &pdata->tx_ded_count, \"TX fifo\");\n\t}\n\n\tif (XP_GET_BITS(ecc_isr, XP_ECC_ISR, RX_DED)) {\n\t\tstop |= xgbe_ecc_ded(pdata, &pdata->rx_ded_period,\n\t\t\t\t     &pdata->rx_ded_count, \"RX fifo\");\n\t}\n\n\tif (XP_GET_BITS(ecc_isr, XP_ECC_ISR, DESC_DED)) {\n\t\tstop |= xgbe_ecc_ded(pdata, &pdata->desc_ded_period,\n\t\t\t\t     &pdata->desc_ded_count,\n\t\t\t\t     \"descriptor cache\");\n\t}\n\n\tif (stop) {\n\t\tpdata->hw_if.disable_ecc_ded(pdata);\n\t\tschedule_work(&pdata->stopdev_work);\n\t\tgoto out;\n\t}\n\n\tif (XP_GET_BITS(ecc_isr, XP_ECC_ISR, TX_SEC)) {\n\t\tif (xgbe_ecc_sec(pdata, &pdata->tx_sec_period,\n\t\t\t\t &pdata->tx_sec_count, \"TX fifo\"))\n\t\t\tpdata->hw_if.disable_ecc_sec(pdata, XGBE_ECC_SEC_TX);\n\t}\n\n\tif (XP_GET_BITS(ecc_isr, XP_ECC_ISR, RX_SEC))\n\t\tif (xgbe_ecc_sec(pdata, &pdata->rx_sec_period,\n\t\t\t\t &pdata->rx_sec_count, \"RX fifo\"))\n\t\t\tpdata->hw_if.disable_ecc_sec(pdata, XGBE_ECC_SEC_RX);\n\n\tif (XP_GET_BITS(ecc_isr, XP_ECC_ISR, DESC_SEC))\n\t\tif (xgbe_ecc_sec(pdata, &pdata->desc_sec_period,\n\t\t\t\t &pdata->desc_sec_count, \"descriptor cache\"))\n\t\t\tpdata->hw_if.disable_ecc_sec(pdata, XGBE_ECC_SEC_DESC);\n\nout:\n\t \n\tXP_IOWRITE(pdata, XP_ECC_ISR, ecc_isr);\n\n\t \n\tif (pdata->vdata->irq_reissue_support)\n\t\tXP_IOWRITE(pdata, XP_INT_REISSUE_EN, 1 << 1);\n}\n\nstatic irqreturn_t xgbe_ecc_isr(int irq, void *data)\n{\n\tstruct xgbe_prv_data *pdata = data;\n\n\tif (pdata->isr_as_tasklet)\n\t\ttasklet_schedule(&pdata->tasklet_ecc);\n\telse\n\t\txgbe_ecc_isr_task(&pdata->tasklet_ecc);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void xgbe_isr_task(struct tasklet_struct *t)\n{\n\tstruct xgbe_prv_data *pdata = from_tasklet(pdata, t, tasklet_dev);\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct xgbe_channel *channel;\n\tunsigned int dma_isr, dma_ch_isr;\n\tunsigned int mac_isr, mac_tssr, mac_mdioisr;\n\tunsigned int i;\n\n\t \n\tdma_isr = XGMAC_IOREAD(pdata, DMA_ISR);\n\tif (!dma_isr)\n\t\tgoto isr_done;\n\n\tnetif_dbg(pdata, intr, pdata->netdev, \"DMA_ISR=%#010x\\n\", dma_isr);\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tif (!(dma_isr & (1 << i)))\n\t\t\tcontinue;\n\n\t\tchannel = pdata->channel[i];\n\n\t\tdma_ch_isr = XGMAC_DMA_IOREAD(channel, DMA_CH_SR);\n\t\tnetif_dbg(pdata, intr, pdata->netdev, \"DMA_CH%u_ISR=%#010x\\n\",\n\t\t\t  i, dma_ch_isr);\n\n\t\t \n\t\tif (!pdata->per_channel_irq &&\n\t\t    (XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, TI) ||\n\t\t     XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, RI))) {\n\t\t\tif (napi_schedule_prep(&pdata->napi)) {\n\t\t\t\t \n\t\t\t\txgbe_disable_rx_tx_ints(pdata);\n\n\t\t\t\t \n\t\t\t\t__napi_schedule(&pdata->napi);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tXGMAC_SET_BITS(dma_ch_isr, DMA_CH_SR, TI, 0);\n\t\t\tXGMAC_SET_BITS(dma_ch_isr, DMA_CH_SR, RI, 0);\n\t\t}\n\n\t\tif (XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, RBU))\n\t\t\tpdata->ext_stats.rx_buffer_unavailable++;\n\n\t\t \n\t\tif (XGMAC_GET_BITS(dma_ch_isr, DMA_CH_SR, FBE))\n\t\t\tschedule_work(&pdata->restart_work);\n\n\t\t \n\t\tXGMAC_DMA_IOWRITE(channel, DMA_CH_SR, dma_ch_isr);\n\t}\n\n\tif (XGMAC_GET_BITS(dma_isr, DMA_ISR, MACIS)) {\n\t\tmac_isr = XGMAC_IOREAD(pdata, MAC_ISR);\n\n\t\tnetif_dbg(pdata, intr, pdata->netdev, \"MAC_ISR=%#010x\\n\",\n\t\t\t  mac_isr);\n\n\t\tif (XGMAC_GET_BITS(mac_isr, MAC_ISR, MMCTXIS))\n\t\t\thw_if->tx_mmc_int(pdata);\n\n\t\tif (XGMAC_GET_BITS(mac_isr, MAC_ISR, MMCRXIS))\n\t\t\thw_if->rx_mmc_int(pdata);\n\n\t\tif (XGMAC_GET_BITS(mac_isr, MAC_ISR, TSIS)) {\n\t\t\tmac_tssr = XGMAC_IOREAD(pdata, MAC_TSSR);\n\n\t\t\tnetif_dbg(pdata, intr, pdata->netdev,\n\t\t\t\t  \"MAC_TSSR=%#010x\\n\", mac_tssr);\n\n\t\t\tif (XGMAC_GET_BITS(mac_tssr, MAC_TSSR, TXTSC)) {\n\t\t\t\t \n\t\t\t\tpdata->tx_tstamp =\n\t\t\t\t\thw_if->get_tx_tstamp(pdata);\n\t\t\t\tqueue_work(pdata->dev_workqueue,\n\t\t\t\t\t   &pdata->tx_tstamp_work);\n\t\t\t}\n\t\t}\n\n\t\tif (XGMAC_GET_BITS(mac_isr, MAC_ISR, SMI)) {\n\t\t\tmac_mdioisr = XGMAC_IOREAD(pdata, MAC_MDIOISR);\n\n\t\t\tnetif_dbg(pdata, intr, pdata->netdev,\n\t\t\t\t  \"MAC_MDIOISR=%#010x\\n\", mac_mdioisr);\n\n\t\t\tif (XGMAC_GET_BITS(mac_mdioisr, MAC_MDIOISR,\n\t\t\t\t\t   SNGLCOMPINT))\n\t\t\t\tcomplete(&pdata->mdio_complete);\n\t\t}\n\t}\n\nisr_done:\n\t \n\tif (pdata->dev_irq == pdata->an_irq)\n\t\tpdata->phy_if.an_isr(pdata);\n\n\t \n\tif (pdata->vdata->ecc_support && (pdata->dev_irq == pdata->ecc_irq))\n\t\txgbe_ecc_isr_task(&pdata->tasklet_ecc);\n\n\t \n\tif (pdata->vdata->i2c_support && (pdata->dev_irq == pdata->i2c_irq))\n\t\tpdata->i2c_if.i2c_isr(pdata);\n\n\t \n\tif (pdata->vdata->irq_reissue_support) {\n\t\tunsigned int reissue_mask;\n\n\t\treissue_mask = 1 << 0;\n\t\tif (!pdata->per_channel_irq)\n\t\t\treissue_mask |= 0xffff << 4;\n\n\t\tXP_IOWRITE(pdata, XP_INT_REISSUE_EN, reissue_mask);\n\t}\n}\n\nstatic irqreturn_t xgbe_isr(int irq, void *data)\n{\n\tstruct xgbe_prv_data *pdata = data;\n\n\tif (pdata->isr_as_tasklet)\n\t\ttasklet_schedule(&pdata->tasklet_dev);\n\telse\n\t\txgbe_isr_task(&pdata->tasklet_dev);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t xgbe_dma_isr(int irq, void *data)\n{\n\tstruct xgbe_channel *channel = data;\n\tstruct xgbe_prv_data *pdata = channel->pdata;\n\tunsigned int dma_status;\n\n\t \n\tif (napi_schedule_prep(&channel->napi)) {\n\t\t \n\t\tif (pdata->channel_irq_mode)\n\t\t\txgbe_disable_rx_tx_int(pdata, channel);\n\t\telse\n\t\t\tdisable_irq_nosync(channel->dma_irq);\n\n\t\t \n\t\t__napi_schedule_irqoff(&channel->napi);\n\t}\n\n\t \n\tdma_status = 0;\n\tXGMAC_SET_BITS(dma_status, DMA_CH_SR, TI, 1);\n\tXGMAC_SET_BITS(dma_status, DMA_CH_SR, RI, 1);\n\tXGMAC_DMA_IOWRITE(channel, DMA_CH_SR, dma_status);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void xgbe_tx_timer(struct timer_list *t)\n{\n\tstruct xgbe_channel *channel = from_timer(channel, t, tx_timer);\n\tstruct xgbe_prv_data *pdata = channel->pdata;\n\tstruct napi_struct *napi;\n\n\tDBGPR(\"-->xgbe_tx_timer\\n\");\n\n\tnapi = (pdata->per_channel_irq) ? &channel->napi : &pdata->napi;\n\n\tif (napi_schedule_prep(napi)) {\n\t\t \n\t\tif (pdata->per_channel_irq)\n\t\t\tif (pdata->channel_irq_mode)\n\t\t\t\txgbe_disable_rx_tx_int(pdata, channel);\n\t\t\telse\n\t\t\t\tdisable_irq_nosync(channel->dma_irq);\n\t\telse\n\t\t\txgbe_disable_rx_tx_ints(pdata);\n\n\t\t \n\t\t__napi_schedule(napi);\n\t}\n\n\tchannel->tx_timer_active = 0;\n\n\tDBGPR(\"<--xgbe_tx_timer\\n\");\n}\n\nstatic void xgbe_service(struct work_struct *work)\n{\n\tstruct xgbe_prv_data *pdata = container_of(work,\n\t\t\t\t\t\t   struct xgbe_prv_data,\n\t\t\t\t\t\t   service_work);\n\n\tpdata->phy_if.phy_status(pdata);\n}\n\nstatic void xgbe_service_timer(struct timer_list *t)\n{\n\tstruct xgbe_prv_data *pdata = from_timer(pdata, t, service_timer);\n\tstruct xgbe_channel *channel;\n\tunsigned int i;\n\n\tqueue_work(pdata->dev_workqueue, &pdata->service_work);\n\n\tmod_timer(&pdata->service_timer, jiffies + HZ);\n\n\tif (!pdata->tx_usecs)\n\t\treturn;\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\t\tif (!channel->tx_ring || channel->tx_timer_active)\n\t\t\tbreak;\n\t\tchannel->tx_timer_active = 1;\n\t\tmod_timer(&channel->tx_timer,\n\t\t\t  jiffies + usecs_to_jiffies(pdata->tx_usecs));\n\t}\n}\n\nstatic void xgbe_init_timers(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_channel *channel;\n\tunsigned int i;\n\n\ttimer_setup(&pdata->service_timer, xgbe_service_timer, 0);\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\t\tif (!channel->tx_ring)\n\t\t\tbreak;\n\n\t\ttimer_setup(&channel->tx_timer, xgbe_tx_timer, 0);\n\t}\n}\n\nstatic void xgbe_start_timers(struct xgbe_prv_data *pdata)\n{\n\tmod_timer(&pdata->service_timer, jiffies + HZ);\n}\n\nstatic void xgbe_stop_timers(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_channel *channel;\n\tunsigned int i;\n\n\tdel_timer_sync(&pdata->service_timer);\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\t\tif (!channel->tx_ring)\n\t\t\tbreak;\n\n\t\t \n\t\tdel_timer_sync(&channel->tx_timer);\n\t\tchannel->tx_timer_active = 0;\n\t}\n}\n\nvoid xgbe_get_all_hw_features(struct xgbe_prv_data *pdata)\n{\n\tunsigned int mac_hfr0, mac_hfr1, mac_hfr2;\n\tstruct xgbe_hw_features *hw_feat = &pdata->hw_feat;\n\n\tmac_hfr0 = XGMAC_IOREAD(pdata, MAC_HWF0R);\n\tmac_hfr1 = XGMAC_IOREAD(pdata, MAC_HWF1R);\n\tmac_hfr2 = XGMAC_IOREAD(pdata, MAC_HWF2R);\n\n\tmemset(hw_feat, 0, sizeof(*hw_feat));\n\n\thw_feat->version = XGMAC_IOREAD(pdata, MAC_VR);\n\n\t \n\thw_feat->gmii        = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, GMIISEL);\n\thw_feat->vlhash      = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, VLHASH);\n\thw_feat->sma         = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, SMASEL);\n\thw_feat->rwk         = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, RWKSEL);\n\thw_feat->mgk         = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, MGKSEL);\n\thw_feat->mmc         = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, MMCSEL);\n\thw_feat->aoe         = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, ARPOFFSEL);\n\thw_feat->ts          = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, TSSEL);\n\thw_feat->eee         = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, EEESEL);\n\thw_feat->tx_coe      = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, TXCOESEL);\n\thw_feat->rx_coe      = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, RXCOESEL);\n\thw_feat->addn_mac    = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R,\n\t\t\t\t\t      ADDMACADRSEL);\n\thw_feat->ts_src      = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, TSSTSSEL);\n\thw_feat->sa_vlan_ins = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, SAVLANINS);\n\thw_feat->vxn         = XGMAC_GET_BITS(mac_hfr0, MAC_HWF0R, VXN);\n\n\t \n\thw_feat->rx_fifo_size  = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R,\n\t\t\t\t\t\tRXFIFOSIZE);\n\thw_feat->tx_fifo_size  = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R,\n\t\t\t\t\t\tTXFIFOSIZE);\n\thw_feat->adv_ts_hi     = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, ADVTHWORD);\n\thw_feat->dma_width     = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, ADDR64);\n\thw_feat->dcb           = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, DCBEN);\n\thw_feat->sph           = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, SPHEN);\n\thw_feat->tso           = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, TSOEN);\n\thw_feat->dma_debug     = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, DBGMEMA);\n\thw_feat->rss           = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, RSSEN);\n\thw_feat->tc_cnt\t       = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R, NUMTC);\n\thw_feat->hash_table_size = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R,\n\t\t\t\t\t\t  HASHTBLSZ);\n\thw_feat->l3l4_filter_num = XGMAC_GET_BITS(mac_hfr1, MAC_HWF1R,\n\t\t\t\t\t\t  L3L4FNUM);\n\n\t \n\thw_feat->rx_q_cnt     = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, RXQCNT);\n\thw_feat->tx_q_cnt     = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, TXQCNT);\n\thw_feat->rx_ch_cnt    = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, RXCHCNT);\n\thw_feat->tx_ch_cnt    = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, TXCHCNT);\n\thw_feat->pps_out_num  = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, PPSOUTNUM);\n\thw_feat->aux_snap_num = XGMAC_GET_BITS(mac_hfr2, MAC_HWF2R, AUXSNAPNUM);\n\n\t \n\tswitch (hw_feat->hash_table_size) {\n\tcase 0:\n\t\tbreak;\n\tcase 1:\n\t\thw_feat->hash_table_size = 64;\n\t\tbreak;\n\tcase 2:\n\t\thw_feat->hash_table_size = 128;\n\t\tbreak;\n\tcase 3:\n\t\thw_feat->hash_table_size = 256;\n\t\tbreak;\n\t}\n\n\t \n\tswitch (hw_feat->dma_width) {\n\tcase 0:\n\t\thw_feat->dma_width = 32;\n\t\tbreak;\n\tcase 1:\n\t\thw_feat->dma_width = 40;\n\t\tbreak;\n\tcase 2:\n\t\thw_feat->dma_width = 48;\n\t\tbreak;\n\tdefault:\n\t\thw_feat->dma_width = 32;\n\t}\n\n\t \n\thw_feat->rx_q_cnt++;\n\thw_feat->tx_q_cnt++;\n\thw_feat->rx_ch_cnt++;\n\thw_feat->tx_ch_cnt++;\n\thw_feat->tc_cnt++;\n\n\t \n\thw_feat->rx_fifo_size = 1 << (hw_feat->rx_fifo_size + 7);\n\thw_feat->tx_fifo_size = 1 << (hw_feat->tx_fifo_size + 7);\n\n\tif (netif_msg_probe(pdata)) {\n\t\tdev_dbg(pdata->dev, \"Hardware features:\\n\");\n\n\t\t \n\t\tdev_dbg(pdata->dev, \"  1GbE support              : %s\\n\",\n\t\t\thw_feat->gmii ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  VLAN hash filter          : %s\\n\",\n\t\t\thw_feat->vlhash ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  MDIO interface            : %s\\n\",\n\t\t\thw_feat->sma ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  Wake-up packet support    : %s\\n\",\n\t\t\thw_feat->rwk ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  Magic packet support      : %s\\n\",\n\t\t\thw_feat->mgk ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  Management counters       : %s\\n\",\n\t\t\thw_feat->mmc ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  ARP offload               : %s\\n\",\n\t\t\thw_feat->aoe ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  IEEE 1588-2008 Timestamp  : %s\\n\",\n\t\t\thw_feat->ts ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  Energy Efficient Ethernet : %s\\n\",\n\t\t\thw_feat->eee ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  TX checksum offload       : %s\\n\",\n\t\t\thw_feat->tx_coe ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  RX checksum offload       : %s\\n\",\n\t\t\thw_feat->rx_coe ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  Additional MAC addresses  : %u\\n\",\n\t\t\thw_feat->addn_mac);\n\t\tdev_dbg(pdata->dev, \"  Timestamp source          : %s\\n\",\n\t\t\t(hw_feat->ts_src == 1) ? \"internal\" :\n\t\t\t(hw_feat->ts_src == 2) ? \"external\" :\n\t\t\t(hw_feat->ts_src == 3) ? \"internal/external\" : \"n/a\");\n\t\tdev_dbg(pdata->dev, \"  SA/VLAN insertion         : %s\\n\",\n\t\t\thw_feat->sa_vlan_ins ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  VXLAN/NVGRE support       : %s\\n\",\n\t\t\thw_feat->vxn ? \"yes\" : \"no\");\n\n\t\t \n\t\tdev_dbg(pdata->dev, \"  RX fifo size              : %u\\n\",\n\t\t\thw_feat->rx_fifo_size);\n\t\tdev_dbg(pdata->dev, \"  TX fifo size              : %u\\n\",\n\t\t\thw_feat->tx_fifo_size);\n\t\tdev_dbg(pdata->dev, \"  IEEE 1588 high word       : %s\\n\",\n\t\t\thw_feat->adv_ts_hi ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  DMA width                 : %u\\n\",\n\t\t\thw_feat->dma_width);\n\t\tdev_dbg(pdata->dev, \"  Data Center Bridging      : %s\\n\",\n\t\t\thw_feat->dcb ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  Split header              : %s\\n\",\n\t\t\thw_feat->sph ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  TCP Segmentation Offload  : %s\\n\",\n\t\t\thw_feat->tso ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  Debug memory interface    : %s\\n\",\n\t\t\thw_feat->dma_debug ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  Receive Side Scaling      : %s\\n\",\n\t\t\thw_feat->rss ? \"yes\" : \"no\");\n\t\tdev_dbg(pdata->dev, \"  Traffic Class count       : %u\\n\",\n\t\t\thw_feat->tc_cnt);\n\t\tdev_dbg(pdata->dev, \"  Hash table size           : %u\\n\",\n\t\t\thw_feat->hash_table_size);\n\t\tdev_dbg(pdata->dev, \"  L3/L4 Filters             : %u\\n\",\n\t\t\thw_feat->l3l4_filter_num);\n\n\t\t \n\t\tdev_dbg(pdata->dev, \"  RX queue count            : %u\\n\",\n\t\t\thw_feat->rx_q_cnt);\n\t\tdev_dbg(pdata->dev, \"  TX queue count            : %u\\n\",\n\t\t\thw_feat->tx_q_cnt);\n\t\tdev_dbg(pdata->dev, \"  RX DMA channel count      : %u\\n\",\n\t\t\thw_feat->rx_ch_cnt);\n\t\tdev_dbg(pdata->dev, \"  TX DMA channel count      : %u\\n\",\n\t\t\thw_feat->rx_ch_cnt);\n\t\tdev_dbg(pdata->dev, \"  PPS outputs               : %u\\n\",\n\t\t\thw_feat->pps_out_num);\n\t\tdev_dbg(pdata->dev, \"  Auxiliary snapshot inputs : %u\\n\",\n\t\t\thw_feat->aux_snap_num);\n\t}\n}\n\nstatic int xgbe_vxlan_set_port(struct net_device *netdev, unsigned int table,\n\t\t\t       unsigned int entry, struct udp_tunnel_info *ti)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\n\tpdata->vxlan_port = be16_to_cpu(ti->port);\n\tpdata->hw_if.enable_vxlan(pdata);\n\n\treturn 0;\n}\n\nstatic int xgbe_vxlan_unset_port(struct net_device *netdev, unsigned int table,\n\t\t\t\t unsigned int entry, struct udp_tunnel_info *ti)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\n\tpdata->hw_if.disable_vxlan(pdata);\n\tpdata->vxlan_port = 0;\n\n\treturn 0;\n}\n\nstatic const struct udp_tunnel_nic_info xgbe_udp_tunnels = {\n\t.set_port\t= xgbe_vxlan_set_port,\n\t.unset_port\t= xgbe_vxlan_unset_port,\n\t.flags\t\t= UDP_TUNNEL_NIC_INFO_OPEN_ONLY,\n\t.tables\t\t= {\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_VXLAN, },\n\t},\n};\n\nconst struct udp_tunnel_nic_info *xgbe_get_udp_tunnel_info(void)\n{\n\treturn &xgbe_udp_tunnels;\n}\n\nstatic void xgbe_napi_enable(struct xgbe_prv_data *pdata, unsigned int add)\n{\n\tstruct xgbe_channel *channel;\n\tunsigned int i;\n\n\tif (pdata->per_channel_irq) {\n\t\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\t\tchannel = pdata->channel[i];\n\t\t\tif (add)\n\t\t\t\tnetif_napi_add(pdata->netdev, &channel->napi,\n\t\t\t\t\t       xgbe_one_poll);\n\n\t\t\tnapi_enable(&channel->napi);\n\t\t}\n\t} else {\n\t\tif (add)\n\t\t\tnetif_napi_add(pdata->netdev, &pdata->napi,\n\t\t\t\t       xgbe_all_poll);\n\n\t\tnapi_enable(&pdata->napi);\n\t}\n}\n\nstatic void xgbe_napi_disable(struct xgbe_prv_data *pdata, unsigned int del)\n{\n\tstruct xgbe_channel *channel;\n\tunsigned int i;\n\n\tif (pdata->per_channel_irq) {\n\t\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\t\tchannel = pdata->channel[i];\n\t\t\tnapi_disable(&channel->napi);\n\n\t\t\tif (del)\n\t\t\t\tnetif_napi_del(&channel->napi);\n\t\t}\n\t} else {\n\t\tnapi_disable(&pdata->napi);\n\n\t\tif (del)\n\t\t\tnetif_napi_del(&pdata->napi);\n\t}\n}\n\nstatic int xgbe_request_irqs(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_channel *channel;\n\tstruct net_device *netdev = pdata->netdev;\n\tunsigned int i;\n\tint ret;\n\n\ttasklet_setup(&pdata->tasklet_dev, xgbe_isr_task);\n\ttasklet_setup(&pdata->tasklet_ecc, xgbe_ecc_isr_task);\n\n\tret = devm_request_irq(pdata->dev, pdata->dev_irq, xgbe_isr, 0,\n\t\t\t       netdev_name(netdev), pdata);\n\tif (ret) {\n\t\tnetdev_alert(netdev, \"error requesting irq %d\\n\",\n\t\t\t     pdata->dev_irq);\n\t\treturn ret;\n\t}\n\n\tif (pdata->vdata->ecc_support && (pdata->dev_irq != pdata->ecc_irq)) {\n\t\tret = devm_request_irq(pdata->dev, pdata->ecc_irq, xgbe_ecc_isr,\n\t\t\t\t       0, pdata->ecc_name, pdata);\n\t\tif (ret) {\n\t\t\tnetdev_alert(netdev, \"error requesting ecc irq %d\\n\",\n\t\t\t\t     pdata->ecc_irq);\n\t\t\tgoto err_dev_irq;\n\t\t}\n\t}\n\n\tif (!pdata->per_channel_irq)\n\t\treturn 0;\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\t\tsnprintf(channel->dma_irq_name,\n\t\t\t sizeof(channel->dma_irq_name) - 1,\n\t\t\t \"%s-TxRx-%u\", netdev_name(netdev),\n\t\t\t channel->queue_index);\n\n\t\tret = devm_request_irq(pdata->dev, channel->dma_irq,\n\t\t\t\t       xgbe_dma_isr, 0,\n\t\t\t\t       channel->dma_irq_name, channel);\n\t\tif (ret) {\n\t\t\tnetdev_alert(netdev, \"error requesting irq %d\\n\",\n\t\t\t\t     channel->dma_irq);\n\t\t\tgoto err_dma_irq;\n\t\t}\n\n\t\tirq_set_affinity_hint(channel->dma_irq,\n\t\t\t\t      &channel->affinity_mask);\n\t}\n\n\treturn 0;\n\nerr_dma_irq:\n\t \n\tfor (i--; i < pdata->channel_count; i--) {\n\t\tchannel = pdata->channel[i];\n\n\t\tirq_set_affinity_hint(channel->dma_irq, NULL);\n\t\tdevm_free_irq(pdata->dev, channel->dma_irq, channel);\n\t}\n\n\tif (pdata->vdata->ecc_support && (pdata->dev_irq != pdata->ecc_irq))\n\t\tdevm_free_irq(pdata->dev, pdata->ecc_irq, pdata);\n\nerr_dev_irq:\n\tdevm_free_irq(pdata->dev, pdata->dev_irq, pdata);\n\n\treturn ret;\n}\n\nstatic void xgbe_free_irqs(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_channel *channel;\n\tunsigned int i;\n\n\tdevm_free_irq(pdata->dev, pdata->dev_irq, pdata);\n\n\ttasklet_kill(&pdata->tasklet_dev);\n\ttasklet_kill(&pdata->tasklet_ecc);\n\n\tif (pdata->vdata->ecc_support && (pdata->dev_irq != pdata->ecc_irq))\n\t\tdevm_free_irq(pdata->dev, pdata->ecc_irq, pdata);\n\n\tif (!pdata->per_channel_irq)\n\t\treturn;\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\n\t\tirq_set_affinity_hint(channel->dma_irq, NULL);\n\t\tdevm_free_irq(pdata->dev, channel->dma_irq, channel);\n\t}\n}\n\nvoid xgbe_init_tx_coalesce(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\n\tDBGPR(\"-->xgbe_init_tx_coalesce\\n\");\n\n\tpdata->tx_usecs = XGMAC_INIT_DMA_TX_USECS;\n\tpdata->tx_frames = XGMAC_INIT_DMA_TX_FRAMES;\n\n\thw_if->config_tx_coalesce(pdata);\n\n\tDBGPR(\"<--xgbe_init_tx_coalesce\\n\");\n}\n\nvoid xgbe_init_rx_coalesce(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\n\tDBGPR(\"-->xgbe_init_rx_coalesce\\n\");\n\n\tpdata->rx_riwt = hw_if->usec_to_riwt(pdata, XGMAC_INIT_DMA_RX_USECS);\n\tpdata->rx_usecs = XGMAC_INIT_DMA_RX_USECS;\n\tpdata->rx_frames = XGMAC_INIT_DMA_RX_FRAMES;\n\n\thw_if->config_rx_coalesce(pdata);\n\n\tDBGPR(\"<--xgbe_init_rx_coalesce\\n\");\n}\n\nstatic void xgbe_free_tx_data(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_desc_if *desc_if = &pdata->desc_if;\n\tstruct xgbe_ring *ring;\n\tstruct xgbe_ring_data *rdata;\n\tunsigned int i, j;\n\n\tDBGPR(\"-->xgbe_free_tx_data\\n\");\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tring = pdata->channel[i]->tx_ring;\n\t\tif (!ring)\n\t\t\tbreak;\n\n\t\tfor (j = 0; j < ring->rdesc_count; j++) {\n\t\t\trdata = XGBE_GET_DESC_DATA(ring, j);\n\t\t\tdesc_if->unmap_rdata(pdata, rdata);\n\t\t}\n\t}\n\n\tDBGPR(\"<--xgbe_free_tx_data\\n\");\n}\n\nstatic void xgbe_free_rx_data(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_desc_if *desc_if = &pdata->desc_if;\n\tstruct xgbe_ring *ring;\n\tstruct xgbe_ring_data *rdata;\n\tunsigned int i, j;\n\n\tDBGPR(\"-->xgbe_free_rx_data\\n\");\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tring = pdata->channel[i]->rx_ring;\n\t\tif (!ring)\n\t\t\tbreak;\n\n\t\tfor (j = 0; j < ring->rdesc_count; j++) {\n\t\t\trdata = XGBE_GET_DESC_DATA(ring, j);\n\t\t\tdesc_if->unmap_rdata(pdata, rdata);\n\t\t}\n\t}\n\n\tDBGPR(\"<--xgbe_free_rx_data\\n\");\n}\n\nstatic int xgbe_phy_reset(struct xgbe_prv_data *pdata)\n{\n\tpdata->phy_link = -1;\n\tpdata->phy_speed = SPEED_UNKNOWN;\n\n\treturn pdata->phy_if.phy_reset(pdata);\n}\n\nint xgbe_powerdown(struct net_device *netdev, unsigned int caller)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tunsigned long flags;\n\n\tDBGPR(\"-->xgbe_powerdown\\n\");\n\n\tif (!netif_running(netdev) ||\n\t    (caller == XGMAC_IOCTL_CONTEXT && pdata->power_down)) {\n\t\tnetdev_alert(netdev, \"Device is already powered down\\n\");\n\t\tDBGPR(\"<--xgbe_powerdown\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_irqsave(&pdata->lock, flags);\n\n\tif (caller == XGMAC_DRIVER_CONTEXT)\n\t\tnetif_device_detach(netdev);\n\n\tnetif_tx_stop_all_queues(netdev);\n\n\txgbe_stop_timers(pdata);\n\tflush_workqueue(pdata->dev_workqueue);\n\n\thw_if->powerdown_tx(pdata);\n\thw_if->powerdown_rx(pdata);\n\n\txgbe_napi_disable(pdata, 0);\n\n\tpdata->power_down = 1;\n\n\tspin_unlock_irqrestore(&pdata->lock, flags);\n\n\tDBGPR(\"<--xgbe_powerdown\\n\");\n\n\treturn 0;\n}\n\nint xgbe_powerup(struct net_device *netdev, unsigned int caller)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tunsigned long flags;\n\n\tDBGPR(\"-->xgbe_powerup\\n\");\n\n\tif (!netif_running(netdev) ||\n\t    (caller == XGMAC_IOCTL_CONTEXT && !pdata->power_down)) {\n\t\tnetdev_alert(netdev, \"Device is already powered up\\n\");\n\t\tDBGPR(\"<--xgbe_powerup\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_irqsave(&pdata->lock, flags);\n\n\tpdata->power_down = 0;\n\n\txgbe_napi_enable(pdata, 0);\n\n\thw_if->powerup_tx(pdata);\n\thw_if->powerup_rx(pdata);\n\n\tif (caller == XGMAC_DRIVER_CONTEXT)\n\t\tnetif_device_attach(netdev);\n\n\tnetif_tx_start_all_queues(netdev);\n\n\txgbe_start_timers(pdata);\n\n\tspin_unlock_irqrestore(&pdata->lock, flags);\n\n\tDBGPR(\"<--xgbe_powerup\\n\");\n\n\treturn 0;\n}\n\nstatic void xgbe_free_memory(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_desc_if *desc_if = &pdata->desc_if;\n\n\t \n\tdesc_if->free_ring_resources(pdata);\n\n\t \n\txgbe_free_channels(pdata);\n}\n\nstatic int xgbe_alloc_memory(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_desc_if *desc_if = &pdata->desc_if;\n\tstruct net_device *netdev = pdata->netdev;\n\tint ret;\n\n\tif (pdata->new_tx_ring_count) {\n\t\tpdata->tx_ring_count = pdata->new_tx_ring_count;\n\t\tpdata->tx_q_count = pdata->tx_ring_count;\n\t\tpdata->new_tx_ring_count = 0;\n\t}\n\n\tif (pdata->new_rx_ring_count) {\n\t\tpdata->rx_ring_count = pdata->new_rx_ring_count;\n\t\tpdata->new_rx_ring_count = 0;\n\t}\n\n\t \n\tpdata->rx_buf_size = xgbe_calc_rx_buf_size(netdev, netdev->mtu);\n\n\t \n\tret = xgbe_alloc_channels(pdata);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = desc_if->alloc_ring_resources(pdata);\n\tif (ret)\n\t\tgoto err_channels;\n\n\t \n\txgbe_init_timers(pdata);\n\n\treturn 0;\n\nerr_channels:\n\txgbe_free_memory(pdata);\n\n\treturn ret;\n}\n\nstatic int xgbe_start(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct xgbe_phy_if *phy_if = &pdata->phy_if;\n\tstruct net_device *netdev = pdata->netdev;\n\tunsigned int i;\n\tint ret;\n\n\t \n\tret = netif_set_real_num_tx_queues(netdev, pdata->tx_ring_count);\n\tif (ret) {\n\t\tnetdev_err(netdev, \"error setting real tx queue count\\n\");\n\t\treturn ret;\n\t}\n\n\tret = netif_set_real_num_rx_queues(netdev, pdata->rx_ring_count);\n\tif (ret) {\n\t\tnetdev_err(netdev, \"error setting real rx queue count\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tfor (i = 0; i < XGBE_RSS_MAX_TABLE_SIZE; i++)\n\t\tXGMAC_SET_BITS(pdata->rss_table[i], MAC_RSSDR, DMCH,\n\t\t\t       i % pdata->rx_ring_count);\n\n\tret = hw_if->init(pdata);\n\tif (ret)\n\t\treturn ret;\n\n\txgbe_napi_enable(pdata, 1);\n\n\tret = xgbe_request_irqs(pdata);\n\tif (ret)\n\t\tgoto err_napi;\n\n\tret = phy_if->phy_start(pdata);\n\tif (ret)\n\t\tgoto err_irqs;\n\n\thw_if->enable_tx(pdata);\n\thw_if->enable_rx(pdata);\n\n\tudp_tunnel_nic_reset_ntf(netdev);\n\n\tnetif_tx_start_all_queues(netdev);\n\n\txgbe_start_timers(pdata);\n\tqueue_work(pdata->dev_workqueue, &pdata->service_work);\n\n\tclear_bit(XGBE_STOPPED, &pdata->dev_state);\n\n\treturn 0;\n\nerr_irqs:\n\txgbe_free_irqs(pdata);\n\nerr_napi:\n\txgbe_napi_disable(pdata, 1);\n\n\thw_if->exit(pdata);\n\n\treturn ret;\n}\n\nstatic void xgbe_stop(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct xgbe_phy_if *phy_if = &pdata->phy_if;\n\tstruct xgbe_channel *channel;\n\tstruct net_device *netdev = pdata->netdev;\n\tstruct netdev_queue *txq;\n\tunsigned int i;\n\n\tDBGPR(\"-->xgbe_stop\\n\");\n\n\tif (test_bit(XGBE_STOPPED, &pdata->dev_state))\n\t\treturn;\n\n\tnetif_tx_stop_all_queues(netdev);\n\tnetif_carrier_off(pdata->netdev);\n\n\txgbe_stop_timers(pdata);\n\tflush_workqueue(pdata->dev_workqueue);\n\n\txgbe_vxlan_unset_port(netdev, 0, 0, NULL);\n\n\thw_if->disable_tx(pdata);\n\thw_if->disable_rx(pdata);\n\n\tphy_if->phy_stop(pdata);\n\n\txgbe_free_irqs(pdata);\n\n\txgbe_napi_disable(pdata, 1);\n\n\thw_if->exit(pdata);\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\t\tif (!channel->tx_ring)\n\t\t\tcontinue;\n\n\t\ttxq = netdev_get_tx_queue(netdev, channel->queue_index);\n\t\tnetdev_tx_reset_queue(txq);\n\t}\n\n\tset_bit(XGBE_STOPPED, &pdata->dev_state);\n\n\tDBGPR(\"<--xgbe_stop\\n\");\n}\n\nstatic void xgbe_stopdev(struct work_struct *work)\n{\n\tstruct xgbe_prv_data *pdata = container_of(work,\n\t\t\t\t\t\t   struct xgbe_prv_data,\n\t\t\t\t\t\t   stopdev_work);\n\n\trtnl_lock();\n\n\txgbe_stop(pdata);\n\n\txgbe_free_tx_data(pdata);\n\txgbe_free_rx_data(pdata);\n\n\trtnl_unlock();\n\n\tnetdev_alert(pdata->netdev, \"device stopped\\n\");\n}\n\nvoid xgbe_full_restart_dev(struct xgbe_prv_data *pdata)\n{\n\t \n\tif (!netif_running(pdata->netdev))\n\t\treturn;\n\n\txgbe_stop(pdata);\n\n\txgbe_free_memory(pdata);\n\txgbe_alloc_memory(pdata);\n\n\txgbe_start(pdata);\n}\n\nvoid xgbe_restart_dev(struct xgbe_prv_data *pdata)\n{\n\t \n\tif (!netif_running(pdata->netdev))\n\t\treturn;\n\n\txgbe_stop(pdata);\n\n\txgbe_free_tx_data(pdata);\n\txgbe_free_rx_data(pdata);\n\n\txgbe_start(pdata);\n}\n\nstatic void xgbe_restart(struct work_struct *work)\n{\n\tstruct xgbe_prv_data *pdata = container_of(work,\n\t\t\t\t\t\t   struct xgbe_prv_data,\n\t\t\t\t\t\t   restart_work);\n\n\trtnl_lock();\n\n\txgbe_restart_dev(pdata);\n\n\trtnl_unlock();\n}\n\nstatic void xgbe_tx_tstamp(struct work_struct *work)\n{\n\tstruct xgbe_prv_data *pdata = container_of(work,\n\t\t\t\t\t\t   struct xgbe_prv_data,\n\t\t\t\t\t\t   tx_tstamp_work);\n\tstruct skb_shared_hwtstamps hwtstamps;\n\tu64 nsec;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&pdata->tstamp_lock, flags);\n\tif (!pdata->tx_tstamp_skb)\n\t\tgoto unlock;\n\n\tif (pdata->tx_tstamp) {\n\t\tnsec = timecounter_cyc2time(&pdata->tstamp_tc,\n\t\t\t\t\t    pdata->tx_tstamp);\n\n\t\tmemset(&hwtstamps, 0, sizeof(hwtstamps));\n\t\thwtstamps.hwtstamp = ns_to_ktime(nsec);\n\t\tskb_tstamp_tx(pdata->tx_tstamp_skb, &hwtstamps);\n\t}\n\n\tdev_kfree_skb_any(pdata->tx_tstamp_skb);\n\n\tpdata->tx_tstamp_skb = NULL;\n\nunlock:\n\tspin_unlock_irqrestore(&pdata->tstamp_lock, flags);\n}\n\nstatic int xgbe_get_hwtstamp_settings(struct xgbe_prv_data *pdata,\n\t\t\t\t      struct ifreq *ifreq)\n{\n\tif (copy_to_user(ifreq->ifr_data, &pdata->tstamp_config,\n\t\t\t sizeof(pdata->tstamp_config)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int xgbe_set_hwtstamp_settings(struct xgbe_prv_data *pdata,\n\t\t\t\t      struct ifreq *ifreq)\n{\n\tstruct hwtstamp_config config;\n\tunsigned int mac_tscr;\n\n\tif (copy_from_user(&config, ifreq->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\tmac_tscr = 0;\n\n\tswitch (config.tx_type) {\n\tcase HWTSTAMP_TX_OFF:\n\t\tbreak;\n\n\tcase HWTSTAMP_TX_ON:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tswitch (config.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\tbreak;\n\n\tcase HWTSTAMP_FILTER_NTP_ALL:\n\tcase HWTSTAMP_FILTER_ALL:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENALL, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\t \n\tcase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\n\t\tfallthrough;\t \n\tcase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, SNAPTYPSEL, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\t \n\tcase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\n\t\tfallthrough;\t \n\tcase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\t \n\tcase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\n\t\tfallthrough;\t \n\tcase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSMSTRENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\t \n\tcase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, AV8021ASMEN, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, SNAPTYPSEL, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\t \n\tcase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, AV8021ASMEN, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\t \n\tcase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, AV8021ASMEN, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSMSTRENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\t \n\tcase HWTSTAMP_FILTER_PTP_V2_EVENT:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, SNAPTYPSEL, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\t \n\tcase HWTSTAMP_FILTER_PTP_V2_SYNC:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\t \n\tcase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSVER2ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV4ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSIPV6ENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSMSTRENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSEVNTENA, 1);\n\t\tXGMAC_SET_BITS(mac_tscr, MAC_TSCR, TSENA, 1);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tpdata->hw_if.config_tstamp(pdata, mac_tscr);\n\n\tmemcpy(&pdata->tstamp_config, &config, sizeof(config));\n\n\treturn 0;\n}\n\nstatic void xgbe_prep_tx_tstamp(struct xgbe_prv_data *pdata,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct xgbe_packet_data *packet)\n{\n\tunsigned long flags;\n\n\tif (XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES, PTP)) {\n\t\tspin_lock_irqsave(&pdata->tstamp_lock, flags);\n\t\tif (pdata->tx_tstamp_skb) {\n\t\t\t \n\t\t\tXGMAC_SET_BITS(packet->attributes,\n\t\t\t\t       TX_PACKET_ATTRIBUTES, PTP, 0);\n\t\t} else {\n\t\t\tpdata->tx_tstamp_skb = skb_get(skb);\n\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t}\n\t\tspin_unlock_irqrestore(&pdata->tstamp_lock, flags);\n\t}\n\n\tskb_tx_timestamp(skb);\n}\n\nstatic void xgbe_prep_vlan(struct sk_buff *skb, struct xgbe_packet_data *packet)\n{\n\tif (skb_vlan_tag_present(skb))\n\t\tpacket->vlan_ctag = skb_vlan_tag_get(skb);\n}\n\nstatic int xgbe_prep_tso(struct sk_buff *skb, struct xgbe_packet_data *packet)\n{\n\tint ret;\n\n\tif (!XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\n\t\t\t    TSO_ENABLE))\n\t\treturn 0;\n\n\tret = skb_cow_head(skb, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tif (XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES, VXLAN)) {\n\t\tpacket->header_len = skb_inner_tcp_all_headers(skb);\n\t\tpacket->tcp_header_len = inner_tcp_hdrlen(skb);\n\t} else {\n\t\tpacket->header_len = skb_tcp_all_headers(skb);\n\t\tpacket->tcp_header_len = tcp_hdrlen(skb);\n\t}\n\tpacket->tcp_payload_len = skb->len - packet->header_len;\n\tpacket->mss = skb_shinfo(skb)->gso_size;\n\n\tDBGPR(\"  packet->header_len=%u\\n\", packet->header_len);\n\tDBGPR(\"  packet->tcp_header_len=%u, packet->tcp_payload_len=%u\\n\",\n\t      packet->tcp_header_len, packet->tcp_payload_len);\n\tDBGPR(\"  packet->mss=%u\\n\", packet->mss);\n\n\t \n\tpacket->tx_packets = skb_shinfo(skb)->gso_segs;\n\tpacket->tx_bytes += (packet->tx_packets - 1) * packet->header_len;\n\n\treturn 0;\n}\n\nstatic bool xgbe_is_vxlan(struct sk_buff *skb)\n{\n\tif (!skb->encapsulation)\n\t\treturn false;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn false;\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\tif (ip_hdr(skb)->protocol != IPPROTO_UDP)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase htons(ETH_P_IPV6):\n\t\tif (ipv6_hdr(skb)->nexthdr != IPPROTO_UDP)\n\t\t\treturn false;\n\t\tbreak;\n\n\tdefault:\n\t\treturn false;\n\t}\n\n\tif (skb->inner_protocol_type != ENCAP_TYPE_ETHER ||\n\t    skb->inner_protocol != htons(ETH_P_TEB) ||\n\t    (skb_inner_mac_header(skb) - skb_transport_header(skb) !=\n\t     sizeof(struct udphdr) + sizeof(struct vxlanhdr)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int xgbe_is_tso(struct sk_buff *skb)\n{\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\tDBGPR(\"  TSO packet to be processed\\n\");\n\n\treturn 1;\n}\n\nstatic void xgbe_packet_info(struct xgbe_prv_data *pdata,\n\t\t\t     struct xgbe_ring *ring, struct sk_buff *skb,\n\t\t\t     struct xgbe_packet_data *packet)\n{\n\tskb_frag_t *frag;\n\tunsigned int context_desc;\n\tunsigned int len;\n\tunsigned int i;\n\n\tpacket->skb = skb;\n\n\tcontext_desc = 0;\n\tpacket->rdesc_count = 0;\n\n\tpacket->tx_packets = 1;\n\tpacket->tx_bytes = skb->len;\n\n\tif (xgbe_is_tso(skb)) {\n\t\t \n\t\tif (skb_shinfo(skb)->gso_size != ring->tx.cur_mss) {\n\t\t\tcontext_desc = 1;\n\t\t\tpacket->rdesc_count++;\n\t\t}\n\n\t\t \n\t\tpacket->rdesc_count++;\n\n\t\tXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\n\t\t\t       TSO_ENABLE, 1);\n\t\tXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\n\t\t\t       CSUM_ENABLE, 1);\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\n\t\t\t       CSUM_ENABLE, 1);\n\n\tif (xgbe_is_vxlan(skb))\n\t\tXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\n\t\t\t       VXLAN, 1);\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\t \n\t\tif (skb_vlan_tag_get(skb) != ring->tx.cur_vlan_ctag)\n\t\t\t \n\t\t\tif (!context_desc) {\n\t\t\t\tcontext_desc = 1;\n\t\t\t\tpacket->rdesc_count++;\n\t\t\t}\n\n\t\tXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\n\t\t\t       VLAN_CTAG, 1);\n\t}\n\n\tif ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\n\t    (pdata->tstamp_config.tx_type == HWTSTAMP_TX_ON))\n\t\tXGMAC_SET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\n\t\t\t       PTP, 1);\n\n\tfor (len = skb_headlen(skb); len;) {\n\t\tpacket->rdesc_count++;\n\t\tlen -= min_t(unsigned int, len, XGBE_TX_MAX_BUF_SIZE);\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\tfor (len = skb_frag_size(frag); len; ) {\n\t\t\tpacket->rdesc_count++;\n\t\t\tlen -= min_t(unsigned int, len, XGBE_TX_MAX_BUF_SIZE);\n\t\t}\n\t}\n}\n\nstatic int xgbe_open(struct net_device *netdev)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tint ret;\n\n\t \n\tsnprintf(pdata->an_name, sizeof(pdata->an_name) - 1, \"%s-pcs\",\n\t\t netdev_name(netdev));\n\n\tsnprintf(pdata->ecc_name, sizeof(pdata->ecc_name) - 1, \"%s-ecc\",\n\t\t netdev_name(netdev));\n\n\tsnprintf(pdata->i2c_name, sizeof(pdata->i2c_name) - 1, \"%s-i2c\",\n\t\t netdev_name(netdev));\n\n\t \n\tpdata->dev_workqueue =\n\t\tcreate_singlethread_workqueue(netdev_name(netdev));\n\tif (!pdata->dev_workqueue) {\n\t\tnetdev_err(netdev, \"device workqueue creation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpdata->an_workqueue =\n\t\tcreate_singlethread_workqueue(pdata->an_name);\n\tif (!pdata->an_workqueue) {\n\t\tnetdev_err(netdev, \"phy workqueue creation failed\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_dev_wq;\n\t}\n\n\t \n\tret = xgbe_phy_reset(pdata);\n\tif (ret)\n\t\tgoto err_an_wq;\n\n\t \n\tret = clk_prepare_enable(pdata->sysclk);\n\tif (ret) {\n\t\tnetdev_alert(netdev, \"dma clk_prepare_enable failed\\n\");\n\t\tgoto err_an_wq;\n\t}\n\n\tret = clk_prepare_enable(pdata->ptpclk);\n\tif (ret) {\n\t\tnetdev_alert(netdev, \"ptp clk_prepare_enable failed\\n\");\n\t\tgoto err_sysclk;\n\t}\n\n\tINIT_WORK(&pdata->service_work, xgbe_service);\n\tINIT_WORK(&pdata->restart_work, xgbe_restart);\n\tINIT_WORK(&pdata->stopdev_work, xgbe_stopdev);\n\tINIT_WORK(&pdata->tx_tstamp_work, xgbe_tx_tstamp);\n\n\tret = xgbe_alloc_memory(pdata);\n\tif (ret)\n\t\tgoto err_ptpclk;\n\n\tret = xgbe_start(pdata);\n\tif (ret)\n\t\tgoto err_mem;\n\n\tclear_bit(XGBE_DOWN, &pdata->dev_state);\n\n\treturn 0;\n\nerr_mem:\n\txgbe_free_memory(pdata);\n\nerr_ptpclk:\n\tclk_disable_unprepare(pdata->ptpclk);\n\nerr_sysclk:\n\tclk_disable_unprepare(pdata->sysclk);\n\nerr_an_wq:\n\tdestroy_workqueue(pdata->an_workqueue);\n\nerr_dev_wq:\n\tdestroy_workqueue(pdata->dev_workqueue);\n\n\treturn ret;\n}\n\nstatic int xgbe_close(struct net_device *netdev)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\n\t \n\txgbe_stop(pdata);\n\n\txgbe_free_memory(pdata);\n\n\t \n\tclk_disable_unprepare(pdata->ptpclk);\n\tclk_disable_unprepare(pdata->sysclk);\n\n\tdestroy_workqueue(pdata->an_workqueue);\n\n\tdestroy_workqueue(pdata->dev_workqueue);\n\n\tset_bit(XGBE_DOWN, &pdata->dev_state);\n\n\treturn 0;\n}\n\nstatic netdev_tx_t xgbe_xmit(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct xgbe_desc_if *desc_if = &pdata->desc_if;\n\tstruct xgbe_channel *channel;\n\tstruct xgbe_ring *ring;\n\tstruct xgbe_packet_data *packet;\n\tstruct netdev_queue *txq;\n\tnetdev_tx_t ret;\n\n\tDBGPR(\"-->xgbe_xmit: skb->len = %d\\n\", skb->len);\n\n\tchannel = pdata->channel[skb->queue_mapping];\n\ttxq = netdev_get_tx_queue(netdev, channel->queue_index);\n\tring = channel->tx_ring;\n\tpacket = &ring->packet_data;\n\n\tret = NETDEV_TX_OK;\n\n\tif (skb->len == 0) {\n\t\tnetif_err(pdata, tx_err, netdev,\n\t\t\t  \"empty skb received from stack\\n\");\n\t\tdev_kfree_skb_any(skb);\n\t\tgoto tx_netdev_return;\n\t}\n\n\t \n\tmemset(packet, 0, sizeof(*packet));\n\txgbe_packet_info(pdata, ring, skb, packet);\n\n\t \n\tret = xgbe_maybe_stop_tx_queue(channel, ring, packet->rdesc_count);\n\tif (ret)\n\t\tgoto tx_netdev_return;\n\n\tret = xgbe_prep_tso(skb, packet);\n\tif (ret) {\n\t\tnetif_err(pdata, tx_err, netdev,\n\t\t\t  \"error processing TSO packet\\n\");\n\t\tdev_kfree_skb_any(skb);\n\t\tgoto tx_netdev_return;\n\t}\n\txgbe_prep_vlan(skb, packet);\n\n\tif (!desc_if->map_tx_skb(channel, skb)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tgoto tx_netdev_return;\n\t}\n\n\txgbe_prep_tx_tstamp(pdata, skb, packet);\n\n\t \n\tnetdev_tx_sent_queue(txq, packet->tx_bytes);\n\n\t \n\thw_if->dev_xmit(channel);\n\n\tif (netif_msg_pktdata(pdata))\n\t\txgbe_print_pkt(netdev, skb, true);\n\n\t \n\txgbe_maybe_stop_tx_queue(channel, ring, XGBE_TX_MAX_DESCS);\n\n\tret = NETDEV_TX_OK;\n\ntx_netdev_return:\n\treturn ret;\n}\n\nstatic void xgbe_set_rx_mode(struct net_device *netdev)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\n\tDBGPR(\"-->xgbe_set_rx_mode\\n\");\n\n\thw_if->config_rx_mode(pdata);\n\n\tDBGPR(\"<--xgbe_set_rx_mode\\n\");\n}\n\nstatic int xgbe_set_mac_address(struct net_device *netdev, void *addr)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct sockaddr *saddr = addr;\n\n\tDBGPR(\"-->xgbe_set_mac_address\\n\");\n\n\tif (!is_valid_ether_addr(saddr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\teth_hw_addr_set(netdev, saddr->sa_data);\n\n\thw_if->set_mac_address(pdata, netdev->dev_addr);\n\n\tDBGPR(\"<--xgbe_set_mac_address\\n\");\n\n\treturn 0;\n}\n\nstatic int xgbe_ioctl(struct net_device *netdev, struct ifreq *ifreq, int cmd)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tint ret;\n\n\tswitch (cmd) {\n\tcase SIOCGHWTSTAMP:\n\t\tret = xgbe_get_hwtstamp_settings(pdata, ifreq);\n\t\tbreak;\n\n\tcase SIOCSHWTSTAMP:\n\t\tret = xgbe_set_hwtstamp_settings(pdata, ifreq);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EOPNOTSUPP;\n\t}\n\n\treturn ret;\n}\n\nstatic int xgbe_change_mtu(struct net_device *netdev, int mtu)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tint ret;\n\n\tDBGPR(\"-->xgbe_change_mtu\\n\");\n\n\tret = xgbe_calc_rx_buf_size(netdev, mtu);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tpdata->rx_buf_size = ret;\n\tnetdev->mtu = mtu;\n\n\txgbe_restart_dev(pdata);\n\n\tDBGPR(\"<--xgbe_change_mtu\\n\");\n\n\treturn 0;\n}\n\nstatic void xgbe_tx_timeout(struct net_device *netdev, unsigned int txqueue)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\n\tnetdev_warn(netdev, \"tx timeout, device restarting\\n\");\n\tschedule_work(&pdata->restart_work);\n}\n\nstatic void xgbe_get_stats64(struct net_device *netdev,\n\t\t\t     struct rtnl_link_stats64 *s)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_mmc_stats *pstats = &pdata->mmc_stats;\n\n\tDBGPR(\"-->%s\\n\", __func__);\n\n\tpdata->hw_if.read_mmc_stats(pdata);\n\n\ts->rx_packets = pstats->rxframecount_gb;\n\ts->rx_bytes = pstats->rxoctetcount_gb;\n\ts->rx_errors = pstats->rxframecount_gb -\n\t\t       pstats->rxbroadcastframes_g -\n\t\t       pstats->rxmulticastframes_g -\n\t\t       pstats->rxunicastframes_g;\n\ts->multicast = pstats->rxmulticastframes_g;\n\ts->rx_length_errors = pstats->rxlengtherror;\n\ts->rx_crc_errors = pstats->rxcrcerror;\n\ts->rx_fifo_errors = pstats->rxfifooverflow;\n\n\ts->tx_packets = pstats->txframecount_gb;\n\ts->tx_bytes = pstats->txoctetcount_gb;\n\ts->tx_errors = pstats->txframecount_gb - pstats->txframecount_g;\n\ts->tx_dropped = netdev->stats.tx_dropped;\n\n\tDBGPR(\"<--%s\\n\", __func__);\n}\n\nstatic int xgbe_vlan_rx_add_vid(struct net_device *netdev, __be16 proto,\n\t\t\t\tu16 vid)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\n\tDBGPR(\"-->%s\\n\", __func__);\n\n\tset_bit(vid, pdata->active_vlans);\n\thw_if->update_vlan_hash_table(pdata);\n\n\tDBGPR(\"<--%s\\n\", __func__);\n\n\treturn 0;\n}\n\nstatic int xgbe_vlan_rx_kill_vid(struct net_device *netdev, __be16 proto,\n\t\t\t\t u16 vid)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\n\tDBGPR(\"-->%s\\n\", __func__);\n\n\tclear_bit(vid, pdata->active_vlans);\n\thw_if->update_vlan_hash_table(pdata);\n\n\tDBGPR(\"<--%s\\n\", __func__);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void xgbe_poll_controller(struct net_device *netdev)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_channel *channel;\n\tunsigned int i;\n\n\tDBGPR(\"-->xgbe_poll_controller\\n\");\n\n\tif (pdata->per_channel_irq) {\n\t\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\t\tchannel = pdata->channel[i];\n\t\t\txgbe_dma_isr(channel->dma_irq, channel);\n\t\t}\n\t} else {\n\t\tdisable_irq(pdata->dev_irq);\n\t\txgbe_isr(pdata->dev_irq, pdata);\n\t\tenable_irq(pdata->dev_irq);\n\t}\n\n\tDBGPR(\"<--xgbe_poll_controller\\n\");\n}\n#endif  \n\nstatic int xgbe_setup_tc(struct net_device *netdev, enum tc_setup_type type,\n\t\t\t void *type_data)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct tc_mqprio_qopt *mqprio = type_data;\n\tu8 tc;\n\n\tif (type != TC_SETUP_QDISC_MQPRIO)\n\t\treturn -EOPNOTSUPP;\n\n\tmqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;\n\ttc = mqprio->num_tc;\n\n\tif (tc > pdata->hw_feat.tc_cnt)\n\t\treturn -EINVAL;\n\n\tpdata->num_tcs = tc;\n\tpdata->hw_if.config_tc(pdata);\n\n\treturn 0;\n}\n\nstatic netdev_features_t xgbe_fix_features(struct net_device *netdev,\n\t\t\t\t\t   netdev_features_t features)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tnetdev_features_t vxlan_base;\n\n\tvxlan_base = NETIF_F_GSO_UDP_TUNNEL | NETIF_F_RX_UDP_TUNNEL_PORT;\n\n\tif (!pdata->hw_feat.vxn)\n\t\treturn features;\n\n\t \n\tif ((features & NETIF_F_GSO_UDP_TUNNEL_CSUM) &&\n\t    !(features & NETIF_F_GSO_UDP_TUNNEL)) {\n\t\tnetdev_notice(netdev,\n\t\t\t      \"forcing tx udp tunnel support\\n\");\n\t\tfeatures |= NETIF_F_GSO_UDP_TUNNEL;\n\t}\n\n\t \n\tif ((features & vxlan_base) != vxlan_base) {\n\t\tnetdev_notice(netdev,\n\t\t\t      \"forcing both tx and rx udp tunnel support\\n\");\n\t\tfeatures |= vxlan_base;\n\t}\n\n\tif (features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n\t\tif (!(features & NETIF_F_GSO_UDP_TUNNEL_CSUM)) {\n\t\t\tnetdev_notice(netdev,\n\t\t\t\t      \"forcing tx udp tunnel checksumming on\\n\");\n\t\t\tfeatures |= NETIF_F_GSO_UDP_TUNNEL_CSUM;\n\t\t}\n\t} else {\n\t\tif (features & NETIF_F_GSO_UDP_TUNNEL_CSUM) {\n\t\t\tnetdev_notice(netdev,\n\t\t\t\t      \"forcing tx udp tunnel checksumming off\\n\");\n\t\t\tfeatures &= ~NETIF_F_GSO_UDP_TUNNEL_CSUM;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic int xgbe_set_features(struct net_device *netdev,\n\t\t\t     netdev_features_t features)\n{\n\tstruct xgbe_prv_data *pdata = netdev_priv(netdev);\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tnetdev_features_t rxhash, rxcsum, rxvlan, rxvlan_filter;\n\tint ret = 0;\n\n\trxhash = pdata->netdev_features & NETIF_F_RXHASH;\n\trxcsum = pdata->netdev_features & NETIF_F_RXCSUM;\n\trxvlan = pdata->netdev_features & NETIF_F_HW_VLAN_CTAG_RX;\n\trxvlan_filter = pdata->netdev_features & NETIF_F_HW_VLAN_CTAG_FILTER;\n\n\tif ((features & NETIF_F_RXHASH) && !rxhash)\n\t\tret = hw_if->enable_rss(pdata);\n\telse if (!(features & NETIF_F_RXHASH) && rxhash)\n\t\tret = hw_if->disable_rss(pdata);\n\tif (ret)\n\t\treturn ret;\n\n\tif ((features & NETIF_F_RXCSUM) && !rxcsum)\n\t\thw_if->enable_rx_csum(pdata);\n\telse if (!(features & NETIF_F_RXCSUM) && rxcsum)\n\t\thw_if->disable_rx_csum(pdata);\n\n\tif ((features & NETIF_F_HW_VLAN_CTAG_RX) && !rxvlan)\n\t\thw_if->enable_rx_vlan_stripping(pdata);\n\telse if (!(features & NETIF_F_HW_VLAN_CTAG_RX) && rxvlan)\n\t\thw_if->disable_rx_vlan_stripping(pdata);\n\n\tif ((features & NETIF_F_HW_VLAN_CTAG_FILTER) && !rxvlan_filter)\n\t\thw_if->enable_rx_vlan_filtering(pdata);\n\telse if (!(features & NETIF_F_HW_VLAN_CTAG_FILTER) && rxvlan_filter)\n\t\thw_if->disable_rx_vlan_filtering(pdata);\n\n\tpdata->netdev_features = features;\n\n\tDBGPR(\"<--xgbe_set_features\\n\");\n\n\treturn 0;\n}\n\nstatic netdev_features_t xgbe_features_check(struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *netdev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\tfeatures = vlan_features_check(skb, features);\n\tfeatures = vxlan_features_check(skb, features);\n\n\treturn features;\n}\n\nstatic const struct net_device_ops xgbe_netdev_ops = {\n\t.ndo_open\t\t= xgbe_open,\n\t.ndo_stop\t\t= xgbe_close,\n\t.ndo_start_xmit\t\t= xgbe_xmit,\n\t.ndo_set_rx_mode\t= xgbe_set_rx_mode,\n\t.ndo_set_mac_address\t= xgbe_set_mac_address,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_eth_ioctl\t\t= xgbe_ioctl,\n\t.ndo_change_mtu\t\t= xgbe_change_mtu,\n\t.ndo_tx_timeout\t\t= xgbe_tx_timeout,\n\t.ndo_get_stats64\t= xgbe_get_stats64,\n\t.ndo_vlan_rx_add_vid\t= xgbe_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= xgbe_vlan_rx_kill_vid,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= xgbe_poll_controller,\n#endif\n\t.ndo_setup_tc\t\t= xgbe_setup_tc,\n\t.ndo_fix_features\t= xgbe_fix_features,\n\t.ndo_set_features\t= xgbe_set_features,\n\t.ndo_features_check\t= xgbe_features_check,\n};\n\nconst struct net_device_ops *xgbe_get_netdev_ops(void)\n{\n\treturn &xgbe_netdev_ops;\n}\n\nstatic void xgbe_rx_refresh(struct xgbe_channel *channel)\n{\n\tstruct xgbe_prv_data *pdata = channel->pdata;\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct xgbe_desc_if *desc_if = &pdata->desc_if;\n\tstruct xgbe_ring *ring = channel->rx_ring;\n\tstruct xgbe_ring_data *rdata;\n\n\twhile (ring->dirty != ring->cur) {\n\t\trdata = XGBE_GET_DESC_DATA(ring, ring->dirty);\n\n\t\t \n\t\tdesc_if->unmap_rdata(pdata, rdata);\n\n\t\tif (desc_if->map_rx_buffer(pdata, ring, rdata))\n\t\t\tbreak;\n\n\t\thw_if->rx_desc_reset(pdata, rdata, ring->dirty);\n\n\t\tring->dirty++;\n\t}\n\n\t \n\twmb();\n\n\t \n\trdata = XGBE_GET_DESC_DATA(ring, ring->dirty - 1);\n\tXGMAC_DMA_IOWRITE(channel, DMA_CH_RDTR_LO,\n\t\t\t  lower_32_bits(rdata->rdesc_dma));\n}\n\nstatic struct sk_buff *xgbe_create_skb(struct xgbe_prv_data *pdata,\n\t\t\t\t       struct napi_struct *napi,\n\t\t\t\t       struct xgbe_ring_data *rdata,\n\t\t\t\t       unsigned int len)\n{\n\tstruct sk_buff *skb;\n\tu8 *packet;\n\n\tskb = napi_alloc_skb(napi, rdata->rx.hdr.dma_len);\n\tif (!skb)\n\t\treturn NULL;\n\n\t \n\tdma_sync_single_range_for_cpu(pdata->dev, rdata->rx.hdr.dma_base,\n\t\t\t\t      rdata->rx.hdr.dma_off,\n\t\t\t\t      rdata->rx.hdr.dma_len, DMA_FROM_DEVICE);\n\n\tpacket = page_address(rdata->rx.hdr.pa.pages) +\n\t\t rdata->rx.hdr.pa.pages_offset;\n\tskb_copy_to_linear_data(skb, packet, len);\n\tskb_put(skb, len);\n\n\treturn skb;\n}\n\nstatic unsigned int xgbe_rx_buf1_len(struct xgbe_ring_data *rdata,\n\t\t\t\t     struct xgbe_packet_data *packet)\n{\n\t \n\tif (!XGMAC_GET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES, FIRST))\n\t\treturn 0;\n\n\t \n\tif (rdata->rx.hdr_len)\n\t\treturn rdata->rx.hdr_len;\n\n\t \n\tif (!XGMAC_GET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES, LAST))\n\t\treturn rdata->rx.hdr.dma_len;\n\n\t \n\treturn min_t(unsigned int, rdata->rx.hdr.dma_len, rdata->rx.len);\n}\n\nstatic unsigned int xgbe_rx_buf2_len(struct xgbe_ring_data *rdata,\n\t\t\t\t     struct xgbe_packet_data *packet,\n\t\t\t\t     unsigned int len)\n{\n\t \n\tif (!XGMAC_GET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES, LAST))\n\t\treturn rdata->rx.buf.dma_len;\n\n\t \n\treturn rdata->rx.len - len;\n}\n\nstatic int xgbe_tx_poll(struct xgbe_channel *channel)\n{\n\tstruct xgbe_prv_data *pdata = channel->pdata;\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct xgbe_desc_if *desc_if = &pdata->desc_if;\n\tstruct xgbe_ring *ring = channel->tx_ring;\n\tstruct xgbe_ring_data *rdata;\n\tstruct xgbe_ring_desc *rdesc;\n\tstruct net_device *netdev = pdata->netdev;\n\tstruct netdev_queue *txq;\n\tint processed = 0;\n\tunsigned int tx_packets = 0, tx_bytes = 0;\n\tunsigned int cur;\n\n\tDBGPR(\"-->xgbe_tx_poll\\n\");\n\n\t \n\tif (!ring)\n\t\treturn 0;\n\n\tcur = ring->cur;\n\n\t \n\tsmp_rmb();\n\n\ttxq = netdev_get_tx_queue(netdev, channel->queue_index);\n\n\twhile ((processed < XGBE_TX_DESC_MAX_PROC) &&\n\t       (ring->dirty != cur)) {\n\t\trdata = XGBE_GET_DESC_DATA(ring, ring->dirty);\n\t\trdesc = rdata->rdesc;\n\n\t\tif (!hw_if->tx_complete(rdesc))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tif (netif_msg_tx_done(pdata))\n\t\t\txgbe_dump_tx_desc(pdata, ring, ring->dirty, 1, 0);\n\n\t\tif (hw_if->is_last_desc(rdesc)) {\n\t\t\ttx_packets += rdata->tx.packets;\n\t\t\ttx_bytes += rdata->tx.bytes;\n\t\t}\n\n\t\t \n\t\tdesc_if->unmap_rdata(pdata, rdata);\n\t\thw_if->tx_desc_reset(rdata);\n\n\t\tprocessed++;\n\t\tring->dirty++;\n\t}\n\n\tif (!processed)\n\t\treturn 0;\n\n\tnetdev_tx_completed_queue(txq, tx_packets, tx_bytes);\n\n\tif ((ring->tx.queue_stopped == 1) &&\n\t    (xgbe_tx_avail_desc(ring) > XGBE_TX_DESC_MIN_FREE)) {\n\t\tring->tx.queue_stopped = 0;\n\t\tnetif_tx_wake_queue(txq);\n\t}\n\n\tDBGPR(\"<--xgbe_tx_poll: processed=%d\\n\", processed);\n\n\treturn processed;\n}\n\nstatic int xgbe_rx_poll(struct xgbe_channel *channel, int budget)\n{\n\tstruct xgbe_prv_data *pdata = channel->pdata;\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct xgbe_ring *ring = channel->rx_ring;\n\tstruct xgbe_ring_data *rdata;\n\tstruct xgbe_packet_data *packet;\n\tstruct net_device *netdev = pdata->netdev;\n\tstruct napi_struct *napi;\n\tstruct sk_buff *skb;\n\tstruct skb_shared_hwtstamps *hwtstamps;\n\tunsigned int last, error, context_next, context;\n\tunsigned int len, buf1_len, buf2_len, max_len;\n\tunsigned int received = 0;\n\tint packet_count = 0;\n\n\tDBGPR(\"-->xgbe_rx_poll: budget=%d\\n\", budget);\n\n\t \n\tif (!ring)\n\t\treturn 0;\n\n\tlast = 0;\n\tcontext_next = 0;\n\n\tnapi = (pdata->per_channel_irq) ? &channel->napi : &pdata->napi;\n\n\trdata = XGBE_GET_DESC_DATA(ring, ring->cur);\n\tpacket = &ring->packet_data;\n\twhile (packet_count < budget) {\n\t\tDBGPR(\"  cur = %d\\n\", ring->cur);\n\n\t\t \n\t\tif (!received && rdata->state_saved) {\n\t\t\tskb = rdata->state.skb;\n\t\t\terror = rdata->state.error;\n\t\t\tlen = rdata->state.len;\n\t\t} else {\n\t\t\tmemset(packet, 0, sizeof(*packet));\n\t\t\tskb = NULL;\n\t\t\terror = 0;\n\t\t\tlen = 0;\n\t\t}\n\nread_again:\n\t\trdata = XGBE_GET_DESC_DATA(ring, ring->cur);\n\n\t\tif (xgbe_rx_dirty_desc(ring) > (XGBE_RX_DESC_CNT >> 3))\n\t\t\txgbe_rx_refresh(channel);\n\n\t\tif (hw_if->dev_read(channel))\n\t\t\tbreak;\n\n\t\treceived++;\n\t\tring->cur++;\n\n\t\tlast = XGMAC_GET_BITS(packet->attributes, RX_PACKET_ATTRIBUTES,\n\t\t\t\t      LAST);\n\t\tcontext_next = XGMAC_GET_BITS(packet->attributes,\n\t\t\t\t\t      RX_PACKET_ATTRIBUTES,\n\t\t\t\t\t      CONTEXT_NEXT);\n\t\tcontext = XGMAC_GET_BITS(packet->attributes,\n\t\t\t\t\t RX_PACKET_ATTRIBUTES,\n\t\t\t\t\t CONTEXT);\n\n\t\t \n\t\tif ((!last || context_next) && error)\n\t\t\tgoto read_again;\n\n\t\tif (error || packet->errors) {\n\t\t\tif (packet->errors)\n\t\t\t\tnetif_err(pdata, rx_err, netdev,\n\t\t\t\t\t  \"error in received packet\\n\");\n\t\t\tdev_kfree_skb(skb);\n\t\t\tgoto next_packet;\n\t\t}\n\n\t\tif (!context) {\n\t\t\t \n\t\t\tbuf1_len = xgbe_rx_buf1_len(rdata, packet);\n\t\t\tlen += buf1_len;\n\t\t\tbuf2_len = xgbe_rx_buf2_len(rdata, packet, len);\n\t\t\tlen += buf2_len;\n\n\t\t\tif (buf2_len > rdata->rx.buf.dma_len) {\n\t\t\t\t \n\t\t\t\terror = 1;\n\t\t\t\tgoto skip_data;\n\t\t\t}\n\n\t\t\tif (!skb) {\n\t\t\t\tskb = xgbe_create_skb(pdata, napi, rdata,\n\t\t\t\t\t\t      buf1_len);\n\t\t\t\tif (!skb) {\n\t\t\t\t\terror = 1;\n\t\t\t\t\tgoto skip_data;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (buf2_len) {\n\t\t\t\tdma_sync_single_range_for_cpu(pdata->dev,\n\t\t\t\t\t\t\trdata->rx.buf.dma_base,\n\t\t\t\t\t\t\trdata->rx.buf.dma_off,\n\t\t\t\t\t\t\trdata->rx.buf.dma_len,\n\t\t\t\t\t\t\tDMA_FROM_DEVICE);\n\n\t\t\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t\t\trdata->rx.buf.pa.pages,\n\t\t\t\t\t\trdata->rx.buf.pa.pages_offset,\n\t\t\t\t\t\tbuf2_len,\n\t\t\t\t\t\trdata->rx.buf.dma_len);\n\t\t\t\trdata->rx.buf.pa.pages = NULL;\n\t\t\t}\n\t\t}\n\nskip_data:\n\t\tif (!last || context_next)\n\t\t\tgoto read_again;\n\n\t\tif (!skb || error) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\tgoto next_packet;\n\t\t}\n\n\t\t \n\t\tmax_len = netdev->mtu + ETH_HLEN;\n\t\tif (!(netdev->features & NETIF_F_HW_VLAN_CTAG_RX) &&\n\t\t    (skb->protocol == htons(ETH_P_8021Q)))\n\t\t\tmax_len += VLAN_HLEN;\n\n\t\tif (skb->len > max_len) {\n\t\t\tnetif_err(pdata, rx_err, netdev,\n\t\t\t\t  \"packet length exceeds configured MTU\\n\");\n\t\t\tdev_kfree_skb(skb);\n\t\t\tgoto next_packet;\n\t\t}\n\n\t\tif (netif_msg_pktdata(pdata))\n\t\t\txgbe_print_pkt(netdev, skb, false);\n\n\t\tskb_checksum_none_assert(skb);\n\t\tif (XGMAC_GET_BITS(packet->attributes,\n\t\t\t\t   RX_PACKET_ATTRIBUTES, CSUM_DONE))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\tif (XGMAC_GET_BITS(packet->attributes,\n\t\t\t\t   RX_PACKET_ATTRIBUTES, TNP)) {\n\t\t\tskb->encapsulation = 1;\n\n\t\t\tif (XGMAC_GET_BITS(packet->attributes,\n\t\t\t\t\t   RX_PACKET_ATTRIBUTES, TNPCSUM_DONE))\n\t\t\t\tskb->csum_level = 1;\n\t\t}\n\n\t\tif (XGMAC_GET_BITS(packet->attributes,\n\t\t\t\t   RX_PACKET_ATTRIBUTES, VLAN_CTAG))\n\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t\t       packet->vlan_ctag);\n\n\t\tif (XGMAC_GET_BITS(packet->attributes,\n\t\t\t\t   RX_PACKET_ATTRIBUTES, RX_TSTAMP)) {\n\t\t\tu64 nsec;\n\n\t\t\tnsec = timecounter_cyc2time(&pdata->tstamp_tc,\n\t\t\t\t\t\t    packet->rx_tstamp);\n\t\t\thwtstamps = skb_hwtstamps(skb);\n\t\t\thwtstamps->hwtstamp = ns_to_ktime(nsec);\n\t\t}\n\n\t\tif (XGMAC_GET_BITS(packet->attributes,\n\t\t\t\t   RX_PACKET_ATTRIBUTES, RSS_HASH))\n\t\t\tskb_set_hash(skb, packet->rss_hash,\n\t\t\t\t     packet->rss_hash_type);\n\n\t\tskb->dev = netdev;\n\t\tskb->protocol = eth_type_trans(skb, netdev);\n\t\tskb_record_rx_queue(skb, channel->queue_index);\n\n\t\tnapi_gro_receive(napi, skb);\n\nnext_packet:\n\t\tpacket_count++;\n\t}\n\n\t \n\tif (received && (!last || context_next)) {\n\t\trdata = XGBE_GET_DESC_DATA(ring, ring->cur);\n\t\trdata->state_saved = 1;\n\t\trdata->state.skb = skb;\n\t\trdata->state.len = len;\n\t\trdata->state.error = error;\n\t}\n\n\tDBGPR(\"<--xgbe_rx_poll: packet_count = %d\\n\", packet_count);\n\n\treturn packet_count;\n}\n\nstatic int xgbe_one_poll(struct napi_struct *napi, int budget)\n{\n\tstruct xgbe_channel *channel = container_of(napi, struct xgbe_channel,\n\t\t\t\t\t\t    napi);\n\tstruct xgbe_prv_data *pdata = channel->pdata;\n\tint processed = 0;\n\n\tDBGPR(\"-->xgbe_one_poll: budget=%d\\n\", budget);\n\n\t \n\txgbe_tx_poll(channel);\n\n\t \n\tprocessed = xgbe_rx_poll(channel, budget);\n\n\t \n\tif ((processed < budget) && napi_complete_done(napi, processed)) {\n\t\t \n\t\tif (pdata->channel_irq_mode)\n\t\t\txgbe_enable_rx_tx_int(pdata, channel);\n\t\telse\n\t\t\tenable_irq(channel->dma_irq);\n\t}\n\n\tDBGPR(\"<--xgbe_one_poll: received = %d\\n\", processed);\n\n\treturn processed;\n}\n\nstatic int xgbe_all_poll(struct napi_struct *napi, int budget)\n{\n\tstruct xgbe_prv_data *pdata = container_of(napi, struct xgbe_prv_data,\n\t\t\t\t\t\t   napi);\n\tstruct xgbe_channel *channel;\n\tint ring_budget;\n\tint processed, last_processed;\n\tunsigned int i;\n\n\tDBGPR(\"-->xgbe_all_poll: budget=%d\\n\", budget);\n\n\tprocessed = 0;\n\tring_budget = budget / pdata->rx_ring_count;\n\tdo {\n\t\tlast_processed = processed;\n\n\t\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\t\tchannel = pdata->channel[i];\n\n\t\t\t \n\t\t\txgbe_tx_poll(channel);\n\n\t\t\t \n\t\t\tif (ring_budget > (budget - processed))\n\t\t\t\tring_budget = budget - processed;\n\t\t\tprocessed += xgbe_rx_poll(channel, ring_budget);\n\t\t}\n\t} while ((processed < budget) && (processed != last_processed));\n\n\t \n\tif ((processed < budget) && napi_complete_done(napi, processed)) {\n\t\t \n\t\txgbe_enable_rx_tx_ints(pdata);\n\t}\n\n\tDBGPR(\"<--xgbe_all_poll: received = %d\\n\", processed);\n\n\treturn processed;\n}\n\nvoid xgbe_dump_tx_desc(struct xgbe_prv_data *pdata, struct xgbe_ring *ring,\n\t\t       unsigned int idx, unsigned int count, unsigned int flag)\n{\n\tstruct xgbe_ring_data *rdata;\n\tstruct xgbe_ring_desc *rdesc;\n\n\twhile (count--) {\n\t\trdata = XGBE_GET_DESC_DATA(ring, idx);\n\t\trdesc = rdata->rdesc;\n\t\tnetdev_dbg(pdata->netdev,\n\t\t\t   \"TX_NORMAL_DESC[%d %s] = %08x:%08x:%08x:%08x\\n\", idx,\n\t\t\t   (flag == 1) ? \"QUEUED FOR TX\" : \"TX BY DEVICE\",\n\t\t\t   le32_to_cpu(rdesc->desc0),\n\t\t\t   le32_to_cpu(rdesc->desc1),\n\t\t\t   le32_to_cpu(rdesc->desc2),\n\t\t\t   le32_to_cpu(rdesc->desc3));\n\t\tidx++;\n\t}\n}\n\nvoid xgbe_dump_rx_desc(struct xgbe_prv_data *pdata, struct xgbe_ring *ring,\n\t\t       unsigned int idx)\n{\n\tstruct xgbe_ring_data *rdata;\n\tstruct xgbe_ring_desc *rdesc;\n\n\trdata = XGBE_GET_DESC_DATA(ring, idx);\n\trdesc = rdata->rdesc;\n\tnetdev_dbg(pdata->netdev,\n\t\t   \"RX_NORMAL_DESC[%d RX BY DEVICE] = %08x:%08x:%08x:%08x\\n\",\n\t\t   idx, le32_to_cpu(rdesc->desc0), le32_to_cpu(rdesc->desc1),\n\t\t   le32_to_cpu(rdesc->desc2), le32_to_cpu(rdesc->desc3));\n}\n\nvoid xgbe_print_pkt(struct net_device *netdev, struct sk_buff *skb, bool tx_rx)\n{\n\tstruct ethhdr *eth = (struct ethhdr *)skb->data;\n\tunsigned char buffer[128];\n\tunsigned int i;\n\n\tnetdev_dbg(netdev, \"\\n************** SKB dump ****************\\n\");\n\n\tnetdev_dbg(netdev, \"%s packet of %d bytes\\n\",\n\t\t   (tx_rx ? \"TX\" : \"RX\"), skb->len);\n\n\tnetdev_dbg(netdev, \"Dst MAC addr: %pM\\n\", eth->h_dest);\n\tnetdev_dbg(netdev, \"Src MAC addr: %pM\\n\", eth->h_source);\n\tnetdev_dbg(netdev, \"Protocol: %#06x\\n\", ntohs(eth->h_proto));\n\n\tfor (i = 0; i < skb->len; i += 32) {\n\t\tunsigned int len = min(skb->len - i, 32U);\n\n\t\thex_dump_to_buffer(&skb->data[i], len, 32, 1,\n\t\t\t\t   buffer, sizeof(buffer), false);\n\t\tnetdev_dbg(netdev, \"  %#06x: %s\\n\", i, buffer);\n\t}\n\n\tnetdev_dbg(netdev, \"\\n************** SKB dump ****************\\n\");\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}