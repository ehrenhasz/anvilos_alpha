{
  "module_name": "xgbe-desc.c",
  "hash_id": "32eae9e0242c621cca5a4e76b935560ef31dfc5c9f61588dfadfcaa904f50958",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/amd/xgbe/xgbe-desc.c",
  "human_readable_source": " \n\n#include \"xgbe.h\"\n#include \"xgbe-common.h\"\n\nstatic void xgbe_unmap_rdata(struct xgbe_prv_data *, struct xgbe_ring_data *);\n\nstatic void xgbe_free_ring(struct xgbe_prv_data *pdata,\n\t\t\t   struct xgbe_ring *ring)\n{\n\tstruct xgbe_ring_data *rdata;\n\tunsigned int i;\n\n\tif (!ring)\n\t\treturn;\n\n\tif (ring->rdata) {\n\t\tfor (i = 0; i < ring->rdesc_count; i++) {\n\t\t\trdata = XGBE_GET_DESC_DATA(ring, i);\n\t\t\txgbe_unmap_rdata(pdata, rdata);\n\t\t}\n\n\t\tkfree(ring->rdata);\n\t\tring->rdata = NULL;\n\t}\n\n\tif (ring->rx_hdr_pa.pages) {\n\t\tdma_unmap_page(pdata->dev, ring->rx_hdr_pa.pages_dma,\n\t\t\t       ring->rx_hdr_pa.pages_len, DMA_FROM_DEVICE);\n\t\tput_page(ring->rx_hdr_pa.pages);\n\n\t\tring->rx_hdr_pa.pages = NULL;\n\t\tring->rx_hdr_pa.pages_len = 0;\n\t\tring->rx_hdr_pa.pages_offset = 0;\n\t\tring->rx_hdr_pa.pages_dma = 0;\n\t}\n\n\tif (ring->rx_buf_pa.pages) {\n\t\tdma_unmap_page(pdata->dev, ring->rx_buf_pa.pages_dma,\n\t\t\t       ring->rx_buf_pa.pages_len, DMA_FROM_DEVICE);\n\t\tput_page(ring->rx_buf_pa.pages);\n\n\t\tring->rx_buf_pa.pages = NULL;\n\t\tring->rx_buf_pa.pages_len = 0;\n\t\tring->rx_buf_pa.pages_offset = 0;\n\t\tring->rx_buf_pa.pages_dma = 0;\n\t}\n\n\tif (ring->rdesc) {\n\t\tdma_free_coherent(pdata->dev,\n\t\t\t\t  (sizeof(struct xgbe_ring_desc) *\n\t\t\t\t   ring->rdesc_count),\n\t\t\t\t  ring->rdesc, ring->rdesc_dma);\n\t\tring->rdesc = NULL;\n\t}\n}\n\nstatic void xgbe_free_ring_resources(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_channel *channel;\n\tunsigned int i;\n\n\tDBGPR(\"-->xgbe_free_ring_resources\\n\");\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\t\txgbe_free_ring(pdata, channel->tx_ring);\n\t\txgbe_free_ring(pdata, channel->rx_ring);\n\t}\n\n\tDBGPR(\"<--xgbe_free_ring_resources\\n\");\n}\n\nstatic void *xgbe_alloc_node(size_t size, int node)\n{\n\tvoid *mem;\n\n\tmem = kzalloc_node(size, GFP_KERNEL, node);\n\tif (!mem)\n\t\tmem = kzalloc(size, GFP_KERNEL);\n\n\treturn mem;\n}\n\nstatic void *xgbe_dma_alloc_node(struct device *dev, size_t size,\n\t\t\t\t dma_addr_t *dma, int node)\n{\n\tvoid *mem;\n\tint cur_node = dev_to_node(dev);\n\n\tset_dev_node(dev, node);\n\tmem = dma_alloc_coherent(dev, size, dma, GFP_KERNEL);\n\tset_dev_node(dev, cur_node);\n\n\tif (!mem)\n\t\tmem = dma_alloc_coherent(dev, size, dma, GFP_KERNEL);\n\n\treturn mem;\n}\n\nstatic int xgbe_init_ring(struct xgbe_prv_data *pdata,\n\t\t\t  struct xgbe_ring *ring, unsigned int rdesc_count)\n{\n\tsize_t size;\n\n\tif (!ring)\n\t\treturn 0;\n\n\t \n\tsize = rdesc_count * sizeof(struct xgbe_ring_desc);\n\n\tring->rdesc_count = rdesc_count;\n\tring->rdesc = xgbe_dma_alloc_node(pdata->dev, size, &ring->rdesc_dma,\n\t\t\t\t\t  ring->node);\n\tif (!ring->rdesc)\n\t\treturn -ENOMEM;\n\n\t \n\tsize = rdesc_count * sizeof(struct xgbe_ring_data);\n\n\tring->rdata = xgbe_alloc_node(size, ring->node);\n\tif (!ring->rdata)\n\t\treturn -ENOMEM;\n\n\tnetif_dbg(pdata, drv, pdata->netdev,\n\t\t  \"rdesc=%p, rdesc_dma=%pad, rdata=%p, node=%d\\n\",\n\t\t  ring->rdesc, &ring->rdesc_dma, ring->rdata, ring->node);\n\n\treturn 0;\n}\n\nstatic int xgbe_alloc_ring_resources(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_channel *channel;\n\tunsigned int i;\n\tint ret;\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\t\tnetif_dbg(pdata, drv, pdata->netdev, \"%s - Tx ring:\\n\",\n\t\t\t  channel->name);\n\n\t\tret = xgbe_init_ring(pdata, channel->tx_ring,\n\t\t\t\t     pdata->tx_desc_count);\n\t\tif (ret) {\n\t\t\tnetdev_alert(pdata->netdev,\n\t\t\t\t     \"error initializing Tx ring\\n\");\n\t\t\tgoto err_ring;\n\t\t}\n\n\t\tnetif_dbg(pdata, drv, pdata->netdev, \"%s - Rx ring:\\n\",\n\t\t\t  channel->name);\n\n\t\tret = xgbe_init_ring(pdata, channel->rx_ring,\n\t\t\t\t     pdata->rx_desc_count);\n\t\tif (ret) {\n\t\t\tnetdev_alert(pdata->netdev,\n\t\t\t\t     \"error initializing Rx ring\\n\");\n\t\t\tgoto err_ring;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_ring:\n\txgbe_free_ring_resources(pdata);\n\n\treturn ret;\n}\n\nstatic int xgbe_alloc_pages(struct xgbe_prv_data *pdata,\n\t\t\t    struct xgbe_page_alloc *pa, int alloc_order,\n\t\t\t    int node)\n{\n\tstruct page *pages = NULL;\n\tdma_addr_t pages_dma;\n\tgfp_t gfp;\n\tint order;\n\nagain:\n\torder = alloc_order;\n\n\t \n\tgfp = GFP_ATOMIC | __GFP_COMP | __GFP_NOWARN;\n\twhile (order >= 0) {\n\t\tpages = alloc_pages_node(node, gfp, order);\n\t\tif (pages)\n\t\t\tbreak;\n\n\t\torder--;\n\t}\n\n\t \n\tif (!pages && (node != NUMA_NO_NODE)) {\n\t\tnode = NUMA_NO_NODE;\n\t\tgoto again;\n\t}\n\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\t \n\tpages_dma = dma_map_page(pdata->dev, pages, 0,\n\t\t\t\t PAGE_SIZE << order, DMA_FROM_DEVICE);\n\tif (dma_mapping_error(pdata->dev, pages_dma)) {\n\t\tput_page(pages);\n\t\treturn -ENOMEM;\n\t}\n\n\tpa->pages = pages;\n\tpa->pages_len = PAGE_SIZE << order;\n\tpa->pages_offset = 0;\n\tpa->pages_dma = pages_dma;\n\n\treturn 0;\n}\n\nstatic void xgbe_set_buffer_data(struct xgbe_buffer_data *bd,\n\t\t\t\t struct xgbe_page_alloc *pa,\n\t\t\t\t unsigned int len)\n{\n\tget_page(pa->pages);\n\tbd->pa = *pa;\n\n\tbd->dma_base = pa->pages_dma;\n\tbd->dma_off = pa->pages_offset;\n\tbd->dma_len = len;\n\n\tpa->pages_offset += len;\n\tif ((pa->pages_offset + len) > pa->pages_len) {\n\t\t \n\t\tbd->pa_unmap = *pa;\n\n\t\t \n\t\tpa->pages = NULL;\n\t\tpa->pages_len = 0;\n\t\tpa->pages_offset = 0;\n\t\tpa->pages_dma = 0;\n\t}\n}\n\nstatic int xgbe_map_rx_buffer(struct xgbe_prv_data *pdata,\n\t\t\t      struct xgbe_ring *ring,\n\t\t\t      struct xgbe_ring_data *rdata)\n{\n\tint ret;\n\n\tif (!ring->rx_hdr_pa.pages) {\n\t\tret = xgbe_alloc_pages(pdata, &ring->rx_hdr_pa, 0, ring->node);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (!ring->rx_buf_pa.pages) {\n\t\tret = xgbe_alloc_pages(pdata, &ring->rx_buf_pa,\n\t\t\t\t       PAGE_ALLOC_COSTLY_ORDER, ring->node);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\txgbe_set_buffer_data(&rdata->rx.hdr, &ring->rx_hdr_pa,\n\t\t\t     XGBE_SKB_ALLOC_SIZE);\n\n\t \n\txgbe_set_buffer_data(&rdata->rx.buf, &ring->rx_buf_pa,\n\t\t\t     pdata->rx_buf_size);\n\n\treturn 0;\n}\n\nstatic void xgbe_wrapper_tx_descriptor_init(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct xgbe_channel *channel;\n\tstruct xgbe_ring *ring;\n\tstruct xgbe_ring_data *rdata;\n\tstruct xgbe_ring_desc *rdesc;\n\tdma_addr_t rdesc_dma;\n\tunsigned int i, j;\n\n\tDBGPR(\"-->xgbe_wrapper_tx_descriptor_init\\n\");\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\t\tring = channel->tx_ring;\n\t\tif (!ring)\n\t\t\tbreak;\n\n\t\trdesc = ring->rdesc;\n\t\trdesc_dma = ring->rdesc_dma;\n\n\t\tfor (j = 0; j < ring->rdesc_count; j++) {\n\t\t\trdata = XGBE_GET_DESC_DATA(ring, j);\n\n\t\t\trdata->rdesc = rdesc;\n\t\t\trdata->rdesc_dma = rdesc_dma;\n\n\t\t\trdesc++;\n\t\t\trdesc_dma += sizeof(struct xgbe_ring_desc);\n\t\t}\n\n\t\tring->cur = 0;\n\t\tring->dirty = 0;\n\t\tmemset(&ring->tx, 0, sizeof(ring->tx));\n\n\t\thw_if->tx_desc_init(channel);\n\t}\n\n\tDBGPR(\"<--xgbe_wrapper_tx_descriptor_init\\n\");\n}\n\nstatic void xgbe_wrapper_rx_descriptor_init(struct xgbe_prv_data *pdata)\n{\n\tstruct xgbe_hw_if *hw_if = &pdata->hw_if;\n\tstruct xgbe_channel *channel;\n\tstruct xgbe_ring *ring;\n\tstruct xgbe_ring_desc *rdesc;\n\tstruct xgbe_ring_data *rdata;\n\tdma_addr_t rdesc_dma;\n\tunsigned int i, j;\n\n\tDBGPR(\"-->xgbe_wrapper_rx_descriptor_init\\n\");\n\n\tfor (i = 0; i < pdata->channel_count; i++) {\n\t\tchannel = pdata->channel[i];\n\t\tring = channel->rx_ring;\n\t\tif (!ring)\n\t\t\tbreak;\n\n\t\trdesc = ring->rdesc;\n\t\trdesc_dma = ring->rdesc_dma;\n\n\t\tfor (j = 0; j < ring->rdesc_count; j++) {\n\t\t\trdata = XGBE_GET_DESC_DATA(ring, j);\n\n\t\t\trdata->rdesc = rdesc;\n\t\t\trdata->rdesc_dma = rdesc_dma;\n\n\t\t\tif (xgbe_map_rx_buffer(pdata, ring, rdata))\n\t\t\t\tbreak;\n\n\t\t\trdesc++;\n\t\t\trdesc_dma += sizeof(struct xgbe_ring_desc);\n\t\t}\n\n\t\tring->cur = 0;\n\t\tring->dirty = 0;\n\n\t\thw_if->rx_desc_init(channel);\n\t}\n\n\tDBGPR(\"<--xgbe_wrapper_rx_descriptor_init\\n\");\n}\n\nstatic void xgbe_unmap_rdata(struct xgbe_prv_data *pdata,\n\t\t\t     struct xgbe_ring_data *rdata)\n{\n\tif (rdata->skb_dma) {\n\t\tif (rdata->mapped_as_page) {\n\t\t\tdma_unmap_page(pdata->dev, rdata->skb_dma,\n\t\t\t\t       rdata->skb_dma_len, DMA_TO_DEVICE);\n\t\t} else {\n\t\t\tdma_unmap_single(pdata->dev, rdata->skb_dma,\n\t\t\t\t\t rdata->skb_dma_len, DMA_TO_DEVICE);\n\t\t}\n\t\trdata->skb_dma = 0;\n\t\trdata->skb_dma_len = 0;\n\t}\n\n\tif (rdata->skb) {\n\t\tdev_kfree_skb_any(rdata->skb);\n\t\trdata->skb = NULL;\n\t}\n\n\tif (rdata->rx.hdr.pa.pages)\n\t\tput_page(rdata->rx.hdr.pa.pages);\n\n\tif (rdata->rx.hdr.pa_unmap.pages) {\n\t\tdma_unmap_page(pdata->dev, rdata->rx.hdr.pa_unmap.pages_dma,\n\t\t\t       rdata->rx.hdr.pa_unmap.pages_len,\n\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(rdata->rx.hdr.pa_unmap.pages);\n\t}\n\n\tif (rdata->rx.buf.pa.pages)\n\t\tput_page(rdata->rx.buf.pa.pages);\n\n\tif (rdata->rx.buf.pa_unmap.pages) {\n\t\tdma_unmap_page(pdata->dev, rdata->rx.buf.pa_unmap.pages_dma,\n\t\t\t       rdata->rx.buf.pa_unmap.pages_len,\n\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(rdata->rx.buf.pa_unmap.pages);\n\t}\n\n\tmemset(&rdata->tx, 0, sizeof(rdata->tx));\n\tmemset(&rdata->rx, 0, sizeof(rdata->rx));\n\n\trdata->mapped_as_page = 0;\n\n\tif (rdata->state_saved) {\n\t\trdata->state_saved = 0;\n\t\trdata->state.skb = NULL;\n\t\trdata->state.len = 0;\n\t\trdata->state.error = 0;\n\t}\n}\n\nstatic int xgbe_map_tx_skb(struct xgbe_channel *channel, struct sk_buff *skb)\n{\n\tstruct xgbe_prv_data *pdata = channel->pdata;\n\tstruct xgbe_ring *ring = channel->tx_ring;\n\tstruct xgbe_ring_data *rdata;\n\tstruct xgbe_packet_data *packet;\n\tskb_frag_t *frag;\n\tdma_addr_t skb_dma;\n\tunsigned int start_index, cur_index;\n\tunsigned int offset, tso, vlan, datalen, len;\n\tunsigned int i;\n\n\tDBGPR(\"-->xgbe_map_tx_skb: cur = %d\\n\", ring->cur);\n\n\toffset = 0;\n\tstart_index = ring->cur;\n\tcur_index = ring->cur;\n\n\tpacket = &ring->packet_data;\n\tpacket->rdesc_count = 0;\n\tpacket->length = 0;\n\n\ttso = XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\n\t\t\t     TSO_ENABLE);\n\tvlan = XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\n\t\t\t      VLAN_CTAG);\n\n\t \n\tif ((tso && (packet->mss != ring->tx.cur_mss)) ||\n\t    (vlan && (packet->vlan_ctag != ring->tx.cur_vlan_ctag)))\n\t\tcur_index++;\n\trdata = XGBE_GET_DESC_DATA(ring, cur_index);\n\n\tif (tso) {\n\t\t \n\t\tskb_dma = dma_map_single(pdata->dev, skb->data,\n\t\t\t\t\t packet->header_len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(pdata->dev, skb_dma)) {\n\t\t\tnetdev_alert(pdata->netdev, \"dma_map_single failed\\n\");\n\t\t\tgoto err_out;\n\t\t}\n\t\trdata->skb_dma = skb_dma;\n\t\trdata->skb_dma_len = packet->header_len;\n\t\tnetif_dbg(pdata, tx_queued, pdata->netdev,\n\t\t\t  \"skb header: index=%u, dma=%pad, len=%u\\n\",\n\t\t\t  cur_index, &skb_dma, packet->header_len);\n\n\t\toffset = packet->header_len;\n\n\t\tpacket->length += packet->header_len;\n\n\t\tcur_index++;\n\t\trdata = XGBE_GET_DESC_DATA(ring, cur_index);\n\t}\n\n\t \n\tfor (datalen = skb_headlen(skb) - offset; datalen; ) {\n\t\tlen = min_t(unsigned int, datalen, XGBE_TX_MAX_BUF_SIZE);\n\n\t\tskb_dma = dma_map_single(pdata->dev, skb->data + offset, len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(pdata->dev, skb_dma)) {\n\t\t\tnetdev_alert(pdata->netdev, \"dma_map_single failed\\n\");\n\t\t\tgoto err_out;\n\t\t}\n\t\trdata->skb_dma = skb_dma;\n\t\trdata->skb_dma_len = len;\n\t\tnetif_dbg(pdata, tx_queued, pdata->netdev,\n\t\t\t  \"skb data: index=%u, dma=%pad, len=%u\\n\",\n\t\t\t  cur_index, &skb_dma, len);\n\n\t\tdatalen -= len;\n\t\toffset += len;\n\n\t\tpacket->length += len;\n\n\t\tcur_index++;\n\t\trdata = XGBE_GET_DESC_DATA(ring, cur_index);\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tnetif_dbg(pdata, tx_queued, pdata->netdev,\n\t\t\t  \"mapping frag %u\\n\", i);\n\n\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\toffset = 0;\n\n\t\tfor (datalen = skb_frag_size(frag); datalen; ) {\n\t\t\tlen = min_t(unsigned int, datalen,\n\t\t\t\t    XGBE_TX_MAX_BUF_SIZE);\n\n\t\t\tskb_dma = skb_frag_dma_map(pdata->dev, frag, offset,\n\t\t\t\t\t\t   len, DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(pdata->dev, skb_dma)) {\n\t\t\t\tnetdev_alert(pdata->netdev,\n\t\t\t\t\t     \"skb_frag_dma_map failed\\n\");\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\trdata->skb_dma = skb_dma;\n\t\t\trdata->skb_dma_len = len;\n\t\t\trdata->mapped_as_page = 1;\n\t\t\tnetif_dbg(pdata, tx_queued, pdata->netdev,\n\t\t\t\t  \"skb frag: index=%u, dma=%pad, len=%u\\n\",\n\t\t\t\t  cur_index, &skb_dma, len);\n\n\t\t\tdatalen -= len;\n\t\t\toffset += len;\n\n\t\t\tpacket->length += len;\n\n\t\t\tcur_index++;\n\t\t\trdata = XGBE_GET_DESC_DATA(ring, cur_index);\n\t\t}\n\t}\n\n\t \n\trdata = XGBE_GET_DESC_DATA(ring, cur_index - 1);\n\trdata->skb = skb;\n\n\t \n\tpacket->rdesc_count = cur_index - start_index;\n\n\tDBGPR(\"<--xgbe_map_tx_skb: count=%u\\n\", packet->rdesc_count);\n\n\treturn packet->rdesc_count;\n\nerr_out:\n\twhile (start_index < cur_index) {\n\t\trdata = XGBE_GET_DESC_DATA(ring, start_index++);\n\t\txgbe_unmap_rdata(pdata, rdata);\n\t}\n\n\tDBGPR(\"<--xgbe_map_tx_skb: count=0\\n\");\n\n\treturn 0;\n}\n\nvoid xgbe_init_function_ptrs_desc(struct xgbe_desc_if *desc_if)\n{\n\tDBGPR(\"-->xgbe_init_function_ptrs_desc\\n\");\n\n\tdesc_if->alloc_ring_resources = xgbe_alloc_ring_resources;\n\tdesc_if->free_ring_resources = xgbe_free_ring_resources;\n\tdesc_if->map_tx_skb = xgbe_map_tx_skb;\n\tdesc_if->map_rx_buffer = xgbe_map_rx_buffer;\n\tdesc_if->unmap_rdata = xgbe_unmap_rdata;\n\tdesc_if->wrapper_tx_desc_init = xgbe_wrapper_tx_descriptor_init;\n\tdesc_if->wrapper_rx_desc_init = xgbe_wrapper_rx_descriptor_init;\n\n\tDBGPR(\"<--xgbe_init_function_ptrs_desc\\n\");\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}