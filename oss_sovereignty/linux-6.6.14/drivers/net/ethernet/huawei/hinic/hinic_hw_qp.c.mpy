{
  "module_name": "hinic_hw_qp.c",
  "hash_id": "b2255f1d42aca3bef8526f8106548cd07fdbb59cd19b8385fb9f481803d69ca0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/huawei/hinic/hinic_hw_qp.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/pci.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/vmalloc.h>\n#include <linux/errno.h>\n#include <linux/sizes.h>\n#include <linux/atomic.h>\n#include <linux/skbuff.h>\n#include <linux/io.h>\n#include <asm/barrier.h>\n#include <asm/byteorder.h>\n\n#include \"hinic_common.h\"\n#include \"hinic_hw_if.h\"\n#include \"hinic_hw_wqe.h\"\n#include \"hinic_hw_wq.h\"\n#include \"hinic_hw_qp_ctxt.h\"\n#include \"hinic_hw_qp.h\"\n#include \"hinic_hw_io.h\"\n\n#define SQ_DB_OFF               SZ_2K\n\n \n#define WQ_PREFETCH_MAX         2\n \n#define WQ_PREFETCH_MIN         1\n \n#define WQ_PREFETCH_THRESHOLD   256\n\n \n#define Q_CTXT_SIZE             48\n#define CTXT_RSVD               240\n\n#define SQ_CTXT_OFFSET(max_sqs, max_rqs, q_id)  \\\n\t\t(((max_rqs) + (max_sqs)) * CTXT_RSVD + (q_id) * Q_CTXT_SIZE)\n\n#define RQ_CTXT_OFFSET(max_sqs, max_rqs, q_id)  \\\n\t\t(((max_rqs) + (max_sqs)) * CTXT_RSVD + \\\n\t\t (max_sqs + (q_id)) * Q_CTXT_SIZE)\n\n#define SIZE_16BYTES(size)              (ALIGN(size, 16) >> 4)\n#define SIZE_8BYTES(size)               (ALIGN(size, 8) >> 3)\n#define SECT_SIZE_FROM_8BYTES(size)     ((size) << 3)\n\n#define SQ_DB_PI_HI_SHIFT       8\n#define SQ_DB_PI_HI(prod_idx)   ((prod_idx) >> SQ_DB_PI_HI_SHIFT)\n\n#define SQ_DB_PI_LOW_MASK       0xFF\n#define SQ_DB_PI_LOW(prod_idx)  ((prod_idx) & SQ_DB_PI_LOW_MASK)\n\n#define SQ_DB_ADDR(sq, pi)      ((u64 *)((sq)->db_base) + SQ_DB_PI_LOW(pi))\n\n#define SQ_MASKED_IDX(sq, idx)  ((idx) & (sq)->wq->mask)\n#define RQ_MASKED_IDX(rq, idx)  ((idx) & (rq)->wq->mask)\n\nenum sq_wqe_type {\n\tSQ_NORMAL_WQE = 0,\n};\n\nenum rq_completion_fmt {\n\tRQ_COMPLETE_SGE = 1\n};\n\nvoid hinic_qp_prepare_header(struct hinic_qp_ctxt_header *qp_ctxt_hdr,\n\t\t\t     enum hinic_qp_ctxt_type ctxt_type,\n\t\t\t     u16 num_queues, u16 max_queues)\n{\n\tu16 max_sqs = max_queues;\n\tu16 max_rqs = max_queues;\n\n\tqp_ctxt_hdr->num_queues = num_queues;\n\tqp_ctxt_hdr->queue_type = ctxt_type;\n\n\tif (ctxt_type == HINIC_QP_CTXT_TYPE_SQ)\n\t\tqp_ctxt_hdr->addr_offset = SQ_CTXT_OFFSET(max_sqs, max_rqs, 0);\n\telse\n\t\tqp_ctxt_hdr->addr_offset = RQ_CTXT_OFFSET(max_sqs, max_rqs, 0);\n\n\tqp_ctxt_hdr->addr_offset = SIZE_16BYTES(qp_ctxt_hdr->addr_offset);\n\n\thinic_cpu_to_be32(qp_ctxt_hdr, sizeof(*qp_ctxt_hdr));\n}\n\nvoid hinic_sq_prepare_ctxt(struct hinic_sq_ctxt *sq_ctxt,\n\t\t\t   struct hinic_sq *sq, u16 global_qid)\n{\n\tu32 wq_page_pfn_hi, wq_page_pfn_lo, wq_block_pfn_hi, wq_block_pfn_lo;\n\tu64 wq_page_addr, wq_page_pfn, wq_block_pfn;\n\tu16 pi_start, ci_start;\n\tstruct hinic_wq *wq;\n\n\twq = sq->wq;\n\tci_start = atomic_read(&wq->cons_idx);\n\tpi_start = atomic_read(&wq->prod_idx);\n\n\t \n\twq_page_addr = be64_to_cpu(*wq->block_vaddr);\n\n\twq_page_pfn = HINIC_WQ_PAGE_PFN(wq_page_addr);\n\twq_page_pfn_hi = upper_32_bits(wq_page_pfn);\n\twq_page_pfn_lo = lower_32_bits(wq_page_pfn);\n\n\t \n\tif (wq->num_q_pages == 1)\n\t\twq_block_pfn = HINIC_WQ_BLOCK_PFN(wq_page_addr);\n\telse\n\t\twq_block_pfn = HINIC_WQ_BLOCK_PFN(wq->block_paddr);\n\n\twq_block_pfn_hi = upper_32_bits(wq_block_pfn);\n\twq_block_pfn_lo = lower_32_bits(wq_block_pfn);\n\n\tsq_ctxt->ceq_attr = HINIC_SQ_CTXT_CEQ_ATTR_SET(global_qid,\n\t\t\t\t\t\t       GLOBAL_SQ_ID) |\n\t\t\t    HINIC_SQ_CTXT_CEQ_ATTR_SET(0, EN);\n\n\tsq_ctxt->ci_wrapped = HINIC_SQ_CTXT_CI_SET(ci_start, IDX) |\n\t\t\t      HINIC_SQ_CTXT_CI_SET(1, WRAPPED);\n\n\tsq_ctxt->wq_hi_pfn_pi =\n\t\t\tHINIC_SQ_CTXT_WQ_PAGE_SET(wq_page_pfn_hi, HI_PFN) |\n\t\t\tHINIC_SQ_CTXT_WQ_PAGE_SET(pi_start, PI);\n\n\tsq_ctxt->wq_lo_pfn = wq_page_pfn_lo;\n\n\tsq_ctxt->pref_cache =\n\t\tHINIC_SQ_CTXT_PREF_SET(WQ_PREFETCH_MIN, CACHE_MIN) |\n\t\tHINIC_SQ_CTXT_PREF_SET(WQ_PREFETCH_MAX, CACHE_MAX) |\n\t\tHINIC_SQ_CTXT_PREF_SET(WQ_PREFETCH_THRESHOLD, CACHE_THRESHOLD);\n\n\tsq_ctxt->pref_wrapped = 1;\n\n\tsq_ctxt->pref_wq_hi_pfn_ci =\n\t\tHINIC_SQ_CTXT_PREF_SET(ci_start, CI) |\n\t\tHINIC_SQ_CTXT_PREF_SET(wq_page_pfn_hi, WQ_HI_PFN);\n\n\tsq_ctxt->pref_wq_lo_pfn = wq_page_pfn_lo;\n\n\tsq_ctxt->wq_block_hi_pfn =\n\t\tHINIC_SQ_CTXT_WQ_BLOCK_SET(wq_block_pfn_hi, HI_PFN);\n\n\tsq_ctxt->wq_block_lo_pfn = wq_block_pfn_lo;\n\n\thinic_cpu_to_be32(sq_ctxt, sizeof(*sq_ctxt));\n}\n\nvoid hinic_rq_prepare_ctxt(struct hinic_rq_ctxt *rq_ctxt,\n\t\t\t   struct hinic_rq *rq, u16 global_qid)\n{\n\tu32 wq_page_pfn_hi, wq_page_pfn_lo, wq_block_pfn_hi, wq_block_pfn_lo;\n\tu64 wq_page_addr, wq_page_pfn, wq_block_pfn;\n\tu16 pi_start, ci_start;\n\tstruct hinic_wq *wq;\n\n\twq = rq->wq;\n\tci_start = atomic_read(&wq->cons_idx);\n\tpi_start = atomic_read(&wq->prod_idx);\n\n\t \n\twq_page_addr = be64_to_cpu(*wq->block_vaddr);\n\n\twq_page_pfn = HINIC_WQ_PAGE_PFN(wq_page_addr);\n\twq_page_pfn_hi = upper_32_bits(wq_page_pfn);\n\twq_page_pfn_lo = lower_32_bits(wq_page_pfn);\n\n\twq_block_pfn = HINIC_WQ_BLOCK_PFN(wq->block_paddr);\n\twq_block_pfn_hi = upper_32_bits(wq_block_pfn);\n\twq_block_pfn_lo = lower_32_bits(wq_block_pfn);\n\n\trq_ctxt->ceq_attr = HINIC_RQ_CTXT_CEQ_ATTR_SET(0, EN) |\n\t\t\t    HINIC_RQ_CTXT_CEQ_ATTR_SET(1, WRAPPED);\n\n\trq_ctxt->pi_intr_attr = HINIC_RQ_CTXT_PI_SET(pi_start, IDX) |\n\t\t\t\tHINIC_RQ_CTXT_PI_SET(rq->msix_entry, INTR);\n\n\trq_ctxt->wq_hi_pfn_ci = HINIC_RQ_CTXT_WQ_PAGE_SET(wq_page_pfn_hi,\n\t\t\t\t\t\t\t  HI_PFN) |\n\t\t\t\tHINIC_RQ_CTXT_WQ_PAGE_SET(ci_start, CI);\n\n\trq_ctxt->wq_lo_pfn = wq_page_pfn_lo;\n\n\trq_ctxt->pref_cache =\n\t\tHINIC_RQ_CTXT_PREF_SET(WQ_PREFETCH_MIN, CACHE_MIN) |\n\t\tHINIC_RQ_CTXT_PREF_SET(WQ_PREFETCH_MAX, CACHE_MAX) |\n\t\tHINIC_RQ_CTXT_PREF_SET(WQ_PREFETCH_THRESHOLD, CACHE_THRESHOLD);\n\n\trq_ctxt->pref_wrapped = 1;\n\n\trq_ctxt->pref_wq_hi_pfn_ci =\n\t\tHINIC_RQ_CTXT_PREF_SET(wq_page_pfn_hi, WQ_HI_PFN) |\n\t\tHINIC_RQ_CTXT_PREF_SET(ci_start, CI);\n\n\trq_ctxt->pref_wq_lo_pfn = wq_page_pfn_lo;\n\n\trq_ctxt->pi_paddr_hi = upper_32_bits(rq->pi_dma_addr);\n\trq_ctxt->pi_paddr_lo = lower_32_bits(rq->pi_dma_addr);\n\n\trq_ctxt->wq_block_hi_pfn =\n\t\tHINIC_RQ_CTXT_WQ_BLOCK_SET(wq_block_pfn_hi, HI_PFN);\n\n\trq_ctxt->wq_block_lo_pfn = wq_block_pfn_lo;\n\n\thinic_cpu_to_be32(rq_ctxt, sizeof(*rq_ctxt));\n}\n\n \nstatic int alloc_sq_skb_arr(struct hinic_sq *sq)\n{\n\tstruct hinic_wq *wq = sq->wq;\n\tsize_t skb_arr_size;\n\n\tskb_arr_size = wq->q_depth * sizeof(*sq->saved_skb);\n\tsq->saved_skb = vzalloc(skb_arr_size);\n\tif (!sq->saved_skb)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nstatic void free_sq_skb_arr(struct hinic_sq *sq)\n{\n\tvfree(sq->saved_skb);\n}\n\n \nstatic int alloc_rq_skb_arr(struct hinic_rq *rq)\n{\n\tstruct hinic_wq *wq = rq->wq;\n\tsize_t skb_arr_size;\n\n\tskb_arr_size = wq->q_depth * sizeof(*rq->saved_skb);\n\trq->saved_skb = vzalloc(skb_arr_size);\n\tif (!rq->saved_skb)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nstatic void free_rq_skb_arr(struct hinic_rq *rq)\n{\n\tvfree(rq->saved_skb);\n}\n\n \nint hinic_init_sq(struct hinic_sq *sq, struct hinic_hwif *hwif,\n\t\t  struct hinic_wq *wq, struct msix_entry *entry,\n\t\t  void *ci_addr, dma_addr_t ci_dma_addr,\n\t\t  void __iomem *db_base)\n{\n\tsq->hwif = hwif;\n\n\tsq->wq = wq;\n\n\tsq->irq = entry->vector;\n\tsq->msix_entry = entry->entry;\n\n\tsq->hw_ci_addr = ci_addr;\n\tsq->hw_ci_dma_addr = ci_dma_addr;\n\n\tsq->db_base = db_base + SQ_DB_OFF;\n\n\treturn alloc_sq_skb_arr(sq);\n}\n\n \nvoid hinic_clean_sq(struct hinic_sq *sq)\n{\n\tfree_sq_skb_arr(sq);\n}\n\n \nstatic int alloc_rq_cqe(struct hinic_rq *rq)\n{\n\tstruct hinic_hwif *hwif = rq->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tsize_t cqe_dma_size, cqe_size;\n\tstruct hinic_wq *wq = rq->wq;\n\tint j, i;\n\n\tcqe_size = wq->q_depth * sizeof(*rq->cqe);\n\trq->cqe = vzalloc(cqe_size);\n\tif (!rq->cqe)\n\t\treturn -ENOMEM;\n\n\tcqe_dma_size = wq->q_depth * sizeof(*rq->cqe_dma);\n\trq->cqe_dma = vzalloc(cqe_dma_size);\n\tif (!rq->cqe_dma)\n\t\tgoto err_cqe_dma_arr_alloc;\n\n\tfor (i = 0; i < wq->q_depth; i++) {\n\t\trq->cqe[i] = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\t\tsizeof(*rq->cqe[i]),\n\t\t\t\t\t\t&rq->cqe_dma[i], GFP_KERNEL);\n\t\tif (!rq->cqe[i])\n\t\t\tgoto err_cqe_alloc;\n\t}\n\n\treturn 0;\n\nerr_cqe_alloc:\n\tfor (j = 0; j < i; j++)\n\t\tdma_free_coherent(&pdev->dev, sizeof(*rq->cqe[j]), rq->cqe[j],\n\t\t\t\t  rq->cqe_dma[j]);\n\n\tvfree(rq->cqe_dma);\n\nerr_cqe_dma_arr_alloc:\n\tvfree(rq->cqe);\n\treturn -ENOMEM;\n}\n\n \nstatic void free_rq_cqe(struct hinic_rq *rq)\n{\n\tstruct hinic_hwif *hwif = rq->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tstruct hinic_wq *wq = rq->wq;\n\tint i;\n\n\tfor (i = 0; i < wq->q_depth; i++)\n\t\tdma_free_coherent(&pdev->dev, sizeof(*rq->cqe[i]), rq->cqe[i],\n\t\t\t\t  rq->cqe_dma[i]);\n\n\tvfree(rq->cqe_dma);\n\tvfree(rq->cqe);\n}\n\n \nint hinic_init_rq(struct hinic_rq *rq, struct hinic_hwif *hwif,\n\t\t  struct hinic_wq *wq, struct msix_entry *entry)\n{\n\tstruct pci_dev *pdev = hwif->pdev;\n\tsize_t pi_size;\n\tint err;\n\n\trq->hwif = hwif;\n\n\trq->wq = wq;\n\n\trq->irq = entry->vector;\n\trq->msix_entry = entry->entry;\n\n\trq->buf_sz = HINIC_RX_BUF_SZ;\n\n\terr = alloc_rq_skb_arr(rq);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to allocate rq priv data\\n\");\n\t\treturn err;\n\t}\n\n\terr = alloc_rq_cqe(rq);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to allocate rq cqe\\n\");\n\t\tgoto err_alloc_rq_cqe;\n\t}\n\n\t \n\tpi_size = ALIGN(sizeof(*rq->pi_virt_addr), sizeof(u32));\n\trq->pi_virt_addr = dma_alloc_coherent(&pdev->dev, pi_size,\n\t\t\t\t\t      &rq->pi_dma_addr, GFP_KERNEL);\n\tif (!rq->pi_virt_addr) {\n\t\terr = -ENOMEM;\n\t\tgoto err_pi_virt;\n\t}\n\n\treturn 0;\n\nerr_pi_virt:\n\tfree_rq_cqe(rq);\n\nerr_alloc_rq_cqe:\n\tfree_rq_skb_arr(rq);\n\treturn err;\n}\n\n \nvoid hinic_clean_rq(struct hinic_rq *rq)\n{\n\tstruct hinic_hwif *hwif = rq->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tsize_t pi_size;\n\n\tpi_size = ALIGN(sizeof(*rq->pi_virt_addr), sizeof(u32));\n\tdma_free_coherent(&pdev->dev, pi_size, rq->pi_virt_addr,\n\t\t\t  rq->pi_dma_addr);\n\n\tfree_rq_cqe(rq);\n\tfree_rq_skb_arr(rq);\n}\n\n \nint hinic_get_sq_free_wqebbs(struct hinic_sq *sq)\n{\n\tstruct hinic_wq *wq = sq->wq;\n\n\treturn atomic_read(&wq->delta) - 1;\n}\n\n \nint hinic_get_rq_free_wqebbs(struct hinic_rq *rq)\n{\n\tstruct hinic_wq *wq = rq->wq;\n\n\treturn atomic_read(&wq->delta) - 1;\n}\n\nstatic void sq_prepare_ctrl(struct hinic_sq_ctrl *ctrl, int nr_descs)\n{\n\tu32 ctrl_size, task_size, bufdesc_size;\n\n\tctrl_size = SIZE_8BYTES(sizeof(struct hinic_sq_ctrl));\n\ttask_size = SIZE_8BYTES(sizeof(struct hinic_sq_task));\n\tbufdesc_size = nr_descs * sizeof(struct hinic_sq_bufdesc);\n\tbufdesc_size = SIZE_8BYTES(bufdesc_size);\n\n\tctrl->ctrl_info = HINIC_SQ_CTRL_SET(bufdesc_size, BUFDESC_SECT_LEN) |\n\t\t\t  HINIC_SQ_CTRL_SET(task_size, TASKSECT_LEN)        |\n\t\t\t  HINIC_SQ_CTRL_SET(SQ_NORMAL_WQE, DATA_FORMAT)     |\n\t\t\t  HINIC_SQ_CTRL_SET(ctrl_size, LEN);\n\n\tctrl->queue_info = HINIC_SQ_CTRL_SET(HINIC_MSS_DEFAULT,\n\t\t\t\t\t     QUEUE_INFO_MSS) |\n\t\t\t   HINIC_SQ_CTRL_SET(1, QUEUE_INFO_UC);\n}\n\nstatic void sq_prepare_task(struct hinic_sq_task *task)\n{\n\ttask->pkt_info0 = 0;\n\ttask->pkt_info1 = 0;\n\ttask->pkt_info2 = 0;\n\n\ttask->ufo_v6_identify = 0;\n\n\ttask->pkt_info4 = HINIC_SQ_TASK_INFO4_SET(HINIC_L2TYPE_ETH, L2TYPE);\n\n\ttask->zero_pad = 0;\n}\n\nvoid hinic_task_set_l2hdr(struct hinic_sq_task *task, u32 len)\n{\n\ttask->pkt_info0 |= HINIC_SQ_TASK_INFO0_SET(len, L2HDR_LEN);\n}\n\nvoid hinic_task_set_outter_l3(struct hinic_sq_task *task,\n\t\t\t      enum hinic_l3_offload_type l3_type,\n\t\t\t      u32 network_len)\n{\n\ttask->pkt_info2 |= HINIC_SQ_TASK_INFO2_SET(l3_type, OUTER_L3TYPE) |\n\t\t\t   HINIC_SQ_TASK_INFO2_SET(network_len, OUTER_L3LEN);\n}\n\nvoid hinic_task_set_inner_l3(struct hinic_sq_task *task,\n\t\t\t     enum hinic_l3_offload_type l3_type,\n\t\t\t     u32 network_len)\n{\n\ttask->pkt_info0 |= HINIC_SQ_TASK_INFO0_SET(l3_type, INNER_L3TYPE);\n\ttask->pkt_info1 |= HINIC_SQ_TASK_INFO1_SET(network_len, INNER_L3LEN);\n}\n\nvoid hinic_task_set_tunnel_l4(struct hinic_sq_task *task,\n\t\t\t      enum hinic_l4_tunnel_type l4_type,\n\t\t\t      u32 tunnel_len)\n{\n\ttask->pkt_info2 |= HINIC_SQ_TASK_INFO2_SET(l4_type, TUNNEL_L4TYPE) |\n\t\t\t   HINIC_SQ_TASK_INFO2_SET(tunnel_len, TUNNEL_L4LEN);\n}\n\nvoid hinic_set_cs_inner_l4(struct hinic_sq_task *task, u32 *queue_info,\n\t\t\t   enum hinic_l4_offload_type l4_offload,\n\t\t\t   u32 l4_len, u32 offset)\n{\n\tu32 tcp_udp_cs = 0, sctp = 0;\n\tu32 mss = HINIC_MSS_DEFAULT;\n\n\tif (l4_offload == TCP_OFFLOAD_ENABLE ||\n\t    l4_offload == UDP_OFFLOAD_ENABLE)\n\t\ttcp_udp_cs = 1;\n\telse if (l4_offload == SCTP_OFFLOAD_ENABLE)\n\t\tsctp = 1;\n\n\ttask->pkt_info0 |= HINIC_SQ_TASK_INFO0_SET(l4_offload, L4_OFFLOAD);\n\ttask->pkt_info1 |= HINIC_SQ_TASK_INFO1_SET(l4_len, INNER_L4LEN);\n\n\t*queue_info |= HINIC_SQ_CTRL_SET(offset, QUEUE_INFO_PLDOFF) |\n\t\t       HINIC_SQ_CTRL_SET(tcp_udp_cs, QUEUE_INFO_TCPUDP_CS) |\n\t\t       HINIC_SQ_CTRL_SET(sctp, QUEUE_INFO_SCTP);\n\n\t*queue_info = HINIC_SQ_CTRL_CLEAR(*queue_info, QUEUE_INFO_MSS);\n\t*queue_info |= HINIC_SQ_CTRL_SET(mss, QUEUE_INFO_MSS);\n}\n\nvoid hinic_set_tso_inner_l4(struct hinic_sq_task *task, u32 *queue_info,\n\t\t\t    enum hinic_l4_offload_type l4_offload,\n\t\t\t    u32 l4_len, u32 offset, u32 ip_ident, u32 mss)\n{\n\tu32 tso = 0, ufo = 0;\n\n\tif (l4_offload == TCP_OFFLOAD_ENABLE)\n\t\ttso = 1;\n\telse if (l4_offload == UDP_OFFLOAD_ENABLE)\n\t\tufo = 1;\n\n\ttask->ufo_v6_identify = ip_ident;\n\n\ttask->pkt_info0 |= HINIC_SQ_TASK_INFO0_SET(l4_offload, L4_OFFLOAD);\n\ttask->pkt_info0 |= HINIC_SQ_TASK_INFO0_SET(tso || ufo, TSO_FLAG);\n\ttask->pkt_info1 |= HINIC_SQ_TASK_INFO1_SET(l4_len, INNER_L4LEN);\n\n\t*queue_info |= HINIC_SQ_CTRL_SET(offset, QUEUE_INFO_PLDOFF) |\n\t\t       HINIC_SQ_CTRL_SET(tso, QUEUE_INFO_TSO) |\n\t\t       HINIC_SQ_CTRL_SET(ufo, QUEUE_INFO_UFO) |\n\t\t       HINIC_SQ_CTRL_SET(!!l4_offload, QUEUE_INFO_TCPUDP_CS);\n\n\t \n\t*queue_info = HINIC_SQ_CTRL_CLEAR(*queue_info, QUEUE_INFO_MSS);\n\t*queue_info |= HINIC_SQ_CTRL_SET(mss, QUEUE_INFO_MSS);\n}\n\n \nvoid hinic_sq_prepare_wqe(struct hinic_sq *sq, struct hinic_sq_wqe *sq_wqe,\n\t\t\t  struct hinic_sge *sges, int nr_sges)\n{\n\tint i;\n\n\tsq_prepare_ctrl(&sq_wqe->ctrl, nr_sges);\n\n\tsq_prepare_task(&sq_wqe->task);\n\n\tfor (i = 0; i < nr_sges; i++)\n\t\tsq_wqe->buf_descs[i].sge = sges[i];\n}\n\n \nstatic u32 sq_prepare_db(struct hinic_sq *sq, u16 prod_idx, unsigned int cos)\n{\n\tstruct hinic_qp *qp = container_of(sq, struct hinic_qp, sq);\n\tu8 hi_prod_idx = SQ_DB_PI_HI(SQ_MASKED_IDX(sq, prod_idx));\n\n\t \n\treturn cpu_to_be32(HINIC_SQ_DB_INFO_SET(hi_prod_idx, PI_HI)     |\n\t\t\t   HINIC_SQ_DB_INFO_SET(HINIC_DB_SQ_TYPE, TYPE) |\n\t\t\t   HINIC_SQ_DB_INFO_SET(HINIC_DATA_PATH, PATH)  |\n\t\t\t   HINIC_SQ_DB_INFO_SET(cos, COS)               |\n\t\t\t   HINIC_SQ_DB_INFO_SET(qp->q_id, QID));\n}\n\n \nvoid hinic_sq_write_db(struct hinic_sq *sq, u16 prod_idx, unsigned int wqe_size,\n\t\t       unsigned int cos)\n{\n\tstruct hinic_wq *wq = sq->wq;\n\n\t \n\tprod_idx += ALIGN(wqe_size, wq->wqebb_size) / wq->wqebb_size;\n\tprod_idx = SQ_MASKED_IDX(sq, prod_idx);\n\n\twmb();   \n\n\twritel(sq_prepare_db(sq, prod_idx, cos), SQ_DB_ADDR(sq, prod_idx));\n}\n\n \nstruct hinic_sq_wqe *hinic_sq_get_wqe(struct hinic_sq *sq,\n\t\t\t\t      unsigned int wqe_size, u16 *prod_idx)\n{\n\tstruct hinic_hw_wqe *hw_wqe = hinic_get_wqe(sq->wq, wqe_size,\n\t\t\t\t\t\t    prod_idx);\n\n\tif (IS_ERR(hw_wqe))\n\t\treturn NULL;\n\n\treturn &hw_wqe->sq_wqe;\n}\n\n \nvoid hinic_sq_return_wqe(struct hinic_sq *sq, unsigned int wqe_size)\n{\n\thinic_return_wqe(sq->wq, wqe_size);\n}\n\n \nvoid hinic_sq_write_wqe(struct hinic_sq *sq, u16 prod_idx,\n\t\t\tstruct hinic_sq_wqe *sq_wqe,\n\t\t\tstruct sk_buff *skb, unsigned int wqe_size)\n{\n\tstruct hinic_hw_wqe *hw_wqe = (struct hinic_hw_wqe *)sq_wqe;\n\n\tsq->saved_skb[prod_idx] = skb;\n\n\t \n\thinic_cpu_to_be32(sq_wqe, wqe_size);\n\n\thinic_write_wqe(sq->wq, hw_wqe, wqe_size);\n}\n\n \nstruct hinic_sq_wqe *hinic_sq_read_wqebb(struct hinic_sq *sq,\n\t\t\t\t\t struct sk_buff **skb,\n\t\t\t\t\t unsigned int *wqe_size, u16 *cons_idx)\n{\n\tstruct hinic_hw_wqe *hw_wqe;\n\tstruct hinic_sq_wqe *sq_wqe;\n\tstruct hinic_sq_ctrl *ctrl;\n\tunsigned int buf_sect_len;\n\tu32 ctrl_info;\n\n\t \n\thw_wqe = hinic_read_wqe(sq->wq, sizeof(*ctrl), cons_idx);\n\tif (IS_ERR(hw_wqe))\n\t\treturn NULL;\n\n\t*skb = sq->saved_skb[*cons_idx];\n\n\tsq_wqe = &hw_wqe->sq_wqe;\n\tctrl = &sq_wqe->ctrl;\n\tctrl_info = be32_to_cpu(ctrl->ctrl_info);\n\tbuf_sect_len = HINIC_SQ_CTRL_GET(ctrl_info, BUFDESC_SECT_LEN);\n\n\t*wqe_size = sizeof(*ctrl) + sizeof(sq_wqe->task);\n\t*wqe_size += SECT_SIZE_FROM_8BYTES(buf_sect_len);\n\t*wqe_size = ALIGN(*wqe_size, sq->wq->wqebb_size);\n\n\treturn &hw_wqe->sq_wqe;\n}\n\n \nstruct hinic_sq_wqe *hinic_sq_read_wqe(struct hinic_sq *sq,\n\t\t\t\t       struct sk_buff **skb,\n\t\t\t\t       unsigned int wqe_size, u16 *cons_idx)\n{\n\tstruct hinic_hw_wqe *hw_wqe;\n\n\thw_wqe = hinic_read_wqe(sq->wq, wqe_size, cons_idx);\n\t*skb = sq->saved_skb[*cons_idx];\n\n\treturn &hw_wqe->sq_wqe;\n}\n\n \nvoid hinic_sq_put_wqe(struct hinic_sq *sq, unsigned int wqe_size)\n{\n\thinic_put_wqe(sq->wq, wqe_size);\n}\n\n \nvoid hinic_sq_get_sges(struct hinic_sq_wqe *sq_wqe, struct hinic_sge *sges,\n\t\t       int nr_sges)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_sges && i < HINIC_MAX_SQ_BUFDESCS; i++) {\n\t\tsges[i] = sq_wqe->buf_descs[i].sge;\n\t\thinic_be32_to_cpu(&sges[i], sizeof(sges[i]));\n\t}\n}\n\n \nstruct hinic_rq_wqe *hinic_rq_get_wqe(struct hinic_rq *rq,\n\t\t\t\t      unsigned int wqe_size, u16 *prod_idx)\n{\n\tstruct hinic_hw_wqe *hw_wqe = hinic_get_wqe(rq->wq, wqe_size,\n\t\t\t\t\t\t    prod_idx);\n\n\tif (IS_ERR(hw_wqe))\n\t\treturn NULL;\n\n\treturn &hw_wqe->rq_wqe;\n}\n\n \nvoid hinic_rq_write_wqe(struct hinic_rq *rq, u16 prod_idx,\n\t\t\tstruct hinic_rq_wqe *rq_wqe, struct sk_buff *skb)\n{\n\tstruct hinic_hw_wqe *hw_wqe = (struct hinic_hw_wqe *)rq_wqe;\n\n\trq->saved_skb[prod_idx] = skb;\n\n\t \n\thinic_cpu_to_be32(rq_wqe, sizeof(*rq_wqe));\n\n\thinic_write_wqe(rq->wq, hw_wqe, sizeof(*rq_wqe));\n}\n\n \nstruct hinic_rq_wqe *hinic_rq_read_wqe(struct hinic_rq *rq,\n\t\t\t\t       unsigned int wqe_size,\n\t\t\t\t       struct sk_buff **skb, u16 *cons_idx)\n{\n\tstruct hinic_hw_wqe *hw_wqe;\n\tstruct hinic_rq_cqe *cqe;\n\tint rx_done;\n\tu32 status;\n\n\thw_wqe = hinic_read_wqe(rq->wq, wqe_size, cons_idx);\n\tif (IS_ERR(hw_wqe))\n\t\treturn NULL;\n\n\tcqe = rq->cqe[*cons_idx];\n\n\tstatus = be32_to_cpu(cqe->status);\n\n\trx_done = HINIC_RQ_CQE_STATUS_GET(status, RXDONE);\n\tif (!rx_done)\n\t\treturn NULL;\n\n\t*skb = rq->saved_skb[*cons_idx];\n\n\treturn &hw_wqe->rq_wqe;\n}\n\n \nstruct hinic_rq_wqe *hinic_rq_read_next_wqe(struct hinic_rq *rq,\n\t\t\t\t\t    unsigned int wqe_size,\n\t\t\t\t\t    struct sk_buff **skb,\n\t\t\t\t\t    u16 *cons_idx)\n{\n\tstruct hinic_wq *wq = rq->wq;\n\tstruct hinic_hw_wqe *hw_wqe;\n\tunsigned int num_wqebbs;\n\n\twqe_size = ALIGN(wqe_size, wq->wqebb_size);\n\tnum_wqebbs = wqe_size / wq->wqebb_size;\n\n\t*cons_idx = RQ_MASKED_IDX(rq, *cons_idx + num_wqebbs);\n\n\t*skb = rq->saved_skb[*cons_idx];\n\n\thw_wqe = hinic_read_wqe_direct(wq, *cons_idx);\n\n\treturn &hw_wqe->rq_wqe;\n}\n\n \nvoid hinic_rq_put_wqe(struct hinic_rq *rq, u16 cons_idx,\n\t\t      unsigned int wqe_size)\n{\n\tstruct hinic_rq_cqe *cqe = rq->cqe[cons_idx];\n\tu32 status = be32_to_cpu(cqe->status);\n\n\tstatus = HINIC_RQ_CQE_STATUS_CLEAR(status, RXDONE);\n\n\t \n\tcqe->status = cpu_to_be32(status);\n\n\twmb();           \n\n\thinic_put_wqe(rq->wq, wqe_size);\n}\n\n \nvoid hinic_rq_get_sge(struct hinic_rq *rq, struct hinic_rq_wqe *rq_wqe,\n\t\t      u16 cons_idx, struct hinic_sge *sge)\n{\n\tstruct hinic_rq_cqe *cqe = rq->cqe[cons_idx];\n\tu32 len = be32_to_cpu(cqe->len);\n\n\tsge->hi_addr = be32_to_cpu(rq_wqe->buf_desc.hi_addr);\n\tsge->lo_addr = be32_to_cpu(rq_wqe->buf_desc.lo_addr);\n\tsge->len = HINIC_RQ_CQE_SGE_GET(len, LEN);\n}\n\n \nvoid hinic_rq_prepare_wqe(struct hinic_rq *rq, u16 prod_idx,\n\t\t\t  struct hinic_rq_wqe *rq_wqe, struct hinic_sge *sge)\n{\n\tstruct hinic_rq_cqe_sect *cqe_sect = &rq_wqe->cqe_sect;\n\tstruct hinic_rq_bufdesc *buf_desc = &rq_wqe->buf_desc;\n\tstruct hinic_rq_cqe *cqe = rq->cqe[prod_idx];\n\tstruct hinic_rq_ctrl *ctrl = &rq_wqe->ctrl;\n\tdma_addr_t cqe_dma = rq->cqe_dma[prod_idx];\n\n\tctrl->ctrl_info =\n\t\tHINIC_RQ_CTRL_SET(SIZE_8BYTES(sizeof(*ctrl)), LEN) |\n\t\tHINIC_RQ_CTRL_SET(SIZE_8BYTES(sizeof(*cqe_sect)),\n\t\t\t\t  COMPLETE_LEN)                    |\n\t\tHINIC_RQ_CTRL_SET(SIZE_8BYTES(sizeof(*buf_desc)),\n\t\t\t\t  BUFDESC_SECT_LEN)                |\n\t\tHINIC_RQ_CTRL_SET(RQ_COMPLETE_SGE, COMPLETE_FORMAT);\n\n\thinic_set_sge(&cqe_sect->sge, cqe_dma, sizeof(*cqe));\n\n\tbuf_desc->hi_addr = sge->hi_addr;\n\tbuf_desc->lo_addr = sge->lo_addr;\n}\n\n \nvoid hinic_rq_update(struct hinic_rq *rq, u16 prod_idx)\n{\n\t*rq->pi_virt_addr = cpu_to_be16(RQ_MASKED_IDX(rq, prod_idx + 1));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}