{
  "module_name": "hinic_tx.c",
  "hash_id": "60d25b917190a988c9d0e37e8f631e2375c36b619421714328e2599bb9df1e04",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/huawei/hinic/hinic_tx.c",
  "human_readable_source": "\n \n\n#include <linux/if_vlan.h>\n#include <linux/kernel.h>\n#include <linux/netdevice.h>\n#include <linux/u64_stats_sync.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/pci.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/skbuff.h>\n#include <linux/smp.h>\n#include <asm/byteorder.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/sctp.h>\n#include <linux/ipv6.h>\n#include <net/ipv6.h>\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n\n#include \"hinic_common.h\"\n#include \"hinic_hw_if.h\"\n#include \"hinic_hw_wqe.h\"\n#include \"hinic_hw_wq.h\"\n#include \"hinic_hw_qp.h\"\n#include \"hinic_hw_dev.h\"\n#include \"hinic_dev.h\"\n#include \"hinic_tx.h\"\n\n#define TX_IRQ_NO_PENDING               0\n#define TX_IRQ_NO_COALESC               0\n#define TX_IRQ_NO_LLI_TIMER             0\n#define TX_IRQ_NO_CREDIT                0\n#define TX_IRQ_NO_RESEND_TIMER          0\n\n#define CI_UPDATE_NO_PENDING            0\n#define CI_UPDATE_NO_COALESC            0\n\n#define HW_CONS_IDX(sq)                 be16_to_cpu(*(u16 *)((sq)->hw_ci_addr))\n\n#define MIN_SKB_LEN\t\t\t32\n\n#define\tMAX_PAYLOAD_OFFSET\t        221\n#define TRANSPORT_OFFSET(l4_hdr, skb)\t((u32)((l4_hdr) - (skb)->data))\n\nunion hinic_l3 {\n\tstruct iphdr *v4;\n\tstruct ipv6hdr *v6;\n\tunsigned char *hdr;\n};\n\nunion hinic_l4 {\n\tstruct tcphdr *tcp;\n\tstruct udphdr *udp;\n\tunsigned char *hdr;\n};\n\nenum hinic_offload_type {\n\tTX_OFFLOAD_TSO     = BIT(0),\n\tTX_OFFLOAD_CSUM    = BIT(1),\n\tTX_OFFLOAD_VLAN    = BIT(2),\n\tTX_OFFLOAD_INVALID = BIT(3),\n};\n\n \nstatic void hinic_txq_clean_stats(struct hinic_txq *txq)\n{\n\tstruct hinic_txq_stats *txq_stats = &txq->txq_stats;\n\n\tu64_stats_update_begin(&txq_stats->syncp);\n\ttxq_stats->pkts    = 0;\n\ttxq_stats->bytes   = 0;\n\ttxq_stats->tx_busy = 0;\n\ttxq_stats->tx_wake = 0;\n\ttxq_stats->tx_dropped = 0;\n\ttxq_stats->big_frags_pkts = 0;\n\tu64_stats_update_end(&txq_stats->syncp);\n}\n\n \nvoid hinic_txq_get_stats(struct hinic_txq *txq, struct hinic_txq_stats *stats)\n{\n\tstruct hinic_txq_stats *txq_stats = &txq->txq_stats;\n\tunsigned int start;\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(&txq_stats->syncp);\n\t\tstats->pkts    = txq_stats->pkts;\n\t\tstats->bytes   = txq_stats->bytes;\n\t\tstats->tx_busy = txq_stats->tx_busy;\n\t\tstats->tx_wake = txq_stats->tx_wake;\n\t\tstats->tx_dropped = txq_stats->tx_dropped;\n\t\tstats->big_frags_pkts = txq_stats->big_frags_pkts;\n\t} while (u64_stats_fetch_retry(&txq_stats->syncp, start));\n}\n\n \nstatic void txq_stats_init(struct hinic_txq *txq)\n{\n\tstruct hinic_txq_stats *txq_stats = &txq->txq_stats;\n\n\tu64_stats_init(&txq_stats->syncp);\n\thinic_txq_clean_stats(txq);\n}\n\n \nstatic int tx_map_skb(struct hinic_dev *nic_dev, struct sk_buff *skb,\n\t\t      struct hinic_sge *sges)\n{\n\tstruct hinic_hwdev *hwdev = nic_dev->hwdev;\n\tstruct hinic_hwif *hwif = hwdev->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tskb_frag_t *frag;\n\tdma_addr_t dma_addr;\n\tint i, j;\n\n\tdma_addr = dma_map_single(&pdev->dev, skb->data, skb_headlen(skb),\n\t\t\t\t  DMA_TO_DEVICE);\n\tif (dma_mapping_error(&pdev->dev, dma_addr)) {\n\t\tdev_err(&pdev->dev, \"Failed to map Tx skb data\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\thinic_set_sge(&sges[0], dma_addr, skb_headlen(skb));\n\n\tfor (i = 0 ; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i];\n\n\t\tdma_addr = skb_frag_dma_map(&pdev->dev, frag, 0,\n\t\t\t\t\t    skb_frag_size(frag),\n\t\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&pdev->dev, dma_addr)) {\n\t\t\tdev_err(&pdev->dev, \"Failed to map Tx skb frag\\n\");\n\t\t\tgoto err_tx_map;\n\t\t}\n\n\t\thinic_set_sge(&sges[i + 1], dma_addr, skb_frag_size(frag));\n\t}\n\n\treturn 0;\n\nerr_tx_map:\n\tfor (j = 0; j < i; j++)\n\t\tdma_unmap_page(&pdev->dev, hinic_sge_to_dma(&sges[j + 1]),\n\t\t\t       sges[j + 1].len, DMA_TO_DEVICE);\n\n\tdma_unmap_single(&pdev->dev, hinic_sge_to_dma(&sges[0]), sges[0].len,\n\t\t\t DMA_TO_DEVICE);\n\treturn -EFAULT;\n}\n\n \nstatic void tx_unmap_skb(struct hinic_dev *nic_dev, struct sk_buff *skb,\n\t\t\t struct hinic_sge *sges)\n{\n\tstruct hinic_hwdev *hwdev = nic_dev->hwdev;\n\tstruct hinic_hwif *hwif = hwdev->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tint i;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags ; i++)\n\t\tdma_unmap_page(&pdev->dev, hinic_sge_to_dma(&sges[i + 1]),\n\t\t\t       sges[i + 1].len, DMA_TO_DEVICE);\n\n\tdma_unmap_single(&pdev->dev, hinic_sge_to_dma(&sges[0]), sges[0].len,\n\t\t\t DMA_TO_DEVICE);\n}\n\nstatic void get_inner_l3_l4_type(struct sk_buff *skb, union hinic_l3 *ip,\n\t\t\t\t union hinic_l4 *l4,\n\t\t\t\t enum hinic_offload_type offload_type,\n\t\t\t\t enum hinic_l3_offload_type *l3_type,\n\t\t\t\t u8 *l4_proto)\n{\n\tu8 *exthdr;\n\n\tif (ip->v4->version == 4) {\n\t\t*l3_type = (offload_type == TX_OFFLOAD_CSUM) ?\n\t\t\t   IPV4_PKT_NO_CHKSUM_OFFLOAD :\n\t\t\t   IPV4_PKT_WITH_CHKSUM_OFFLOAD;\n\t\t*l4_proto = ip->v4->protocol;\n\t} else if (ip->v4->version == 6) {\n\t\t*l3_type = IPV6_PKT;\n\t\texthdr = ip->hdr + sizeof(*ip->v6);\n\t\t*l4_proto = ip->v6->nexthdr;\n\t\tif (exthdr != l4->hdr) {\n\t\t\tint start = exthdr - skb->data;\n\t\t\t__be16 frag_off;\n\n\t\t\tipv6_skip_exthdr(skb, start, l4_proto, &frag_off);\n\t\t}\n\t} else {\n\t\t*l3_type = L3TYPE_UNKNOWN;\n\t\t*l4_proto = 0;\n\t}\n}\n\nstatic void get_inner_l4_info(struct sk_buff *skb, union hinic_l4 *l4,\n\t\t\t      enum hinic_offload_type offload_type, u8 l4_proto,\n\t\t\t      enum hinic_l4_offload_type *l4_offload,\n\t\t\t      u32 *l4_len, u32 *offset)\n{\n\t*l4_offload = OFFLOAD_DISABLE;\n\t*offset = 0;\n\t*l4_len = 0;\n\n\tswitch (l4_proto) {\n\tcase IPPROTO_TCP:\n\t\t*l4_offload = TCP_OFFLOAD_ENABLE;\n\t\t \n\t\t*l4_len = l4->tcp->doff * 4;\n\t\t*offset = *l4_len + TRANSPORT_OFFSET(l4->hdr, skb);\n\t\tbreak;\n\n\tcase IPPROTO_UDP:\n\t\t*l4_offload = UDP_OFFLOAD_ENABLE;\n\t\t*l4_len = sizeof(struct udphdr);\n\t\t*offset = TRANSPORT_OFFSET(l4->hdr, skb);\n\t\tbreak;\n\n\tcase IPPROTO_SCTP:\n\t\t \n\t\tif (offload_type != TX_OFFLOAD_CSUM)\n\t\t\tbreak;\n\n\t\t*l4_offload = SCTP_OFFLOAD_ENABLE;\n\t\t*l4_len = sizeof(struct sctphdr);\n\t\t*offset = TRANSPORT_OFFSET(l4->hdr, skb);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic __sum16 csum_magic(union hinic_l3 *ip, unsigned short proto)\n{\n\treturn (ip->v4->version == 4) ?\n\t\tcsum_tcpudp_magic(ip->v4->saddr, ip->v4->daddr, 0, proto, 0) :\n\t\tcsum_ipv6_magic(&ip->v6->saddr, &ip->v6->daddr, 0, proto, 0);\n}\n\nstatic int offload_tso(struct hinic_sq_task *task, u32 *queue_info,\n\t\t       struct sk_buff *skb)\n{\n\tu32 offset, l4_len, ip_identify, network_hdr_len;\n\tenum hinic_l3_offload_type l3_offload;\n\tenum hinic_l4_offload_type l4_offload;\n\tunion hinic_l3 ip;\n\tunion hinic_l4 l4;\n\tu8 l4_proto;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\tif (skb_cow_head(skb, 0) < 0)\n\t\treturn -EPROTONOSUPPORT;\n\n\tif (skb->encapsulation) {\n\t\tu32 gso_type = skb_shinfo(skb)->gso_type;\n\t\tu32 tunnel_type = 0;\n\t\tu32 l4_tunnel_len;\n\n\t\tip.hdr = skb_network_header(skb);\n\t\tl4.hdr = skb_transport_header(skb);\n\t\tnetwork_hdr_len = skb_inner_network_header_len(skb);\n\n\t\tif (ip.v4->version == 4) {\n\t\t\tip.v4->tot_len = 0;\n\t\t\tl3_offload = IPV4_PKT_WITH_CHKSUM_OFFLOAD;\n\t\t} else if (ip.v4->version == 6) {\n\t\t\tl3_offload = IPV6_PKT;\n\t\t} else {\n\t\t\tl3_offload = 0;\n\t\t}\n\n\t\thinic_task_set_outter_l3(task, l3_offload,\n\t\t\t\t\t skb_network_header_len(skb));\n\n\t\tif (gso_type & SKB_GSO_UDP_TUNNEL_CSUM) {\n\t\t\tl4.udp->check = ~csum_magic(&ip, IPPROTO_UDP);\n\t\t\ttunnel_type = TUNNEL_UDP_CSUM;\n\t\t} else if (gso_type & SKB_GSO_UDP_TUNNEL) {\n\t\t\ttunnel_type = TUNNEL_UDP_NO_CSUM;\n\t\t}\n\n\t\tl4_tunnel_len = skb_inner_network_offset(skb) -\n\t\t\t\tskb_transport_offset(skb);\n\t\thinic_task_set_tunnel_l4(task, tunnel_type, l4_tunnel_len);\n\n\t\tip.hdr = skb_inner_network_header(skb);\n\t\tl4.hdr = skb_inner_transport_header(skb);\n\t} else {\n\t\tip.hdr = skb_network_header(skb);\n\t\tl4.hdr = skb_transport_header(skb);\n\t\tnetwork_hdr_len = skb_network_header_len(skb);\n\t}\n\n\t \n\tif (ip.v4->version == 4)\n\t\tip.v4->tot_len = 0;\n\telse\n\t\tip.v6->payload_len = 0;\n\n\tget_inner_l3_l4_type(skb, &ip, &l4, TX_OFFLOAD_TSO, &l3_offload,\n\t\t\t     &l4_proto);\n\n\thinic_task_set_inner_l3(task, l3_offload, network_hdr_len);\n\n\tip_identify = 0;\n\tif (l4_proto == IPPROTO_TCP)\n\t\tl4.tcp->check = ~csum_magic(&ip, IPPROTO_TCP);\n\n\tget_inner_l4_info(skb, &l4, TX_OFFLOAD_TSO, l4_proto, &l4_offload,\n\t\t\t  &l4_len, &offset);\n\n\thinic_set_tso_inner_l4(task, queue_info, l4_offload, l4_len, offset,\n\t\t\t       ip_identify, skb_shinfo(skb)->gso_size);\n\n\treturn 1;\n}\n\nstatic int offload_csum(struct hinic_sq_task *task, u32 *queue_info,\n\t\t\tstruct sk_buff *skb)\n{\n\tenum hinic_l4_offload_type l4_offload;\n\tu32 offset, l4_len, network_hdr_len;\n\tenum hinic_l3_offload_type l3_type;\n\tu32 tunnel_type = NOT_TUNNEL;\n\tunion hinic_l3 ip;\n\tunion hinic_l4 l4;\n\tu8 l4_proto;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (skb->encapsulation) {\n\t\tu32 l4_tunnel_len;\n\n\t\ttunnel_type = TUNNEL_UDP_NO_CSUM;\n\t\tip.hdr = skb_network_header(skb);\n\n\t\tif (ip.v4->version == 4) {\n\t\t\tl3_type = IPV4_PKT_NO_CHKSUM_OFFLOAD;\n\t\t\tl4_proto = ip.v4->protocol;\n\t\t} else if (ip.v4->version == 6) {\n\t\t\tunsigned char *exthdr;\n\t\t\t__be16 frag_off;\n\n\t\t\tl3_type = IPV6_PKT;\n\t\t\ttunnel_type = TUNNEL_UDP_CSUM;\n\t\t\texthdr = ip.hdr + sizeof(*ip.v6);\n\t\t\tl4_proto = ip.v6->nexthdr;\n\t\t\tl4.hdr = skb_transport_header(skb);\n\t\t\tif (l4.hdr != exthdr)\n\t\t\t\tipv6_skip_exthdr(skb, exthdr - skb->data,\n\t\t\t\t\t\t &l4_proto, &frag_off);\n\t\t} else {\n\t\t\tl3_type = L3TYPE_UNKNOWN;\n\t\t\tl4_proto = IPPROTO_RAW;\n\t\t}\n\n\t\thinic_task_set_outter_l3(task, l3_type,\n\t\t\t\t\t skb_network_header_len(skb));\n\n\t\tswitch (l4_proto) {\n\t\tcase IPPROTO_UDP:\n\t\t\tl4_tunnel_len = skb_inner_network_offset(skb) -\n\t\t\t\t\tskb_transport_offset(skb);\n\t\t\tip.hdr = skb_inner_network_header(skb);\n\t\t\tl4.hdr = skb_inner_transport_header(skb);\n\t\t\tnetwork_hdr_len = skb_inner_network_header_len(skb);\n\t\t\tbreak;\n\t\tcase IPPROTO_IPIP:\n\t\tcase IPPROTO_IPV6:\n\t\t\ttunnel_type = NOT_TUNNEL;\n\t\t\tl4_tunnel_len = 0;\n\n\t\t\tip.hdr = skb_inner_network_header(skb);\n\t\t\tl4.hdr = skb_transport_header(skb);\n\t\t\tnetwork_hdr_len = skb_network_header_len(skb);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tskb_checksum_help(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\thinic_task_set_tunnel_l4(task, tunnel_type, l4_tunnel_len);\n\t} else {\n\t\tip.hdr = skb_network_header(skb);\n\t\tl4.hdr = skb_transport_header(skb);\n\t\tnetwork_hdr_len = skb_network_header_len(skb);\n\t}\n\n\tget_inner_l3_l4_type(skb, &ip, &l4, TX_OFFLOAD_CSUM, &l3_type,\n\t\t\t     &l4_proto);\n\n\thinic_task_set_inner_l3(task, l3_type, network_hdr_len);\n\n\tget_inner_l4_info(skb, &l4, TX_OFFLOAD_CSUM, l4_proto, &l4_offload,\n\t\t\t  &l4_len, &offset);\n\n\thinic_set_cs_inner_l4(task, queue_info, l4_offload, l4_len, offset);\n\n\treturn 1;\n}\n\nstatic void offload_vlan(struct hinic_sq_task *task, u32 *queue_info,\n\t\t\t u16 vlan_tag, u16 vlan_pri)\n{\n\ttask->pkt_info0 |= HINIC_SQ_TASK_INFO0_SET(vlan_tag, VLAN_TAG) |\n\t\t\t\tHINIC_SQ_TASK_INFO0_SET(1U, VLAN_OFFLOAD);\n\n\t*queue_info |= HINIC_SQ_CTRL_SET(vlan_pri, QUEUE_INFO_PRI);\n}\n\nstatic int hinic_tx_offload(struct sk_buff *skb, struct hinic_sq_task *task,\n\t\t\t    u32 *queue_info)\n{\n\tenum hinic_offload_type offload = 0;\n\tu16 vlan_tag;\n\tint enabled;\n\n\tenabled = offload_tso(task, queue_info, skb);\n\tif (enabled > 0) {\n\t\toffload |= TX_OFFLOAD_TSO;\n\t} else if (enabled == 0) {\n\t\tenabled = offload_csum(task, queue_info, skb);\n\t\tif (enabled)\n\t\t\toffload |= TX_OFFLOAD_CSUM;\n\t} else {\n\t\treturn -EPROTONOSUPPORT;\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\tvlan_tag = skb_vlan_tag_get(skb);\n\t\toffload_vlan(task, queue_info, vlan_tag,\n\t\t\t     vlan_tag >> VLAN_PRIO_SHIFT);\n\t\toffload |= TX_OFFLOAD_VLAN;\n\t}\n\n\tif (offload)\n\t\thinic_task_set_l2hdr(task, skb_network_offset(skb));\n\n\t \n\tif (HINIC_SQ_CTRL_GET(*queue_info, QUEUE_INFO_PLDOFF) >\n\t    MAX_PAYLOAD_OFFSET) {\n\t\treturn -EPROTONOSUPPORT;\n\t}\n\n\t \n\tif (HINIC_SQ_CTRL_GET(*queue_info, QUEUE_INFO_MSS) < HINIC_MSS_MIN) {\n\t\t*queue_info = HINIC_SQ_CTRL_CLEAR(*queue_info, QUEUE_INFO_MSS);\n\t\t*queue_info |= HINIC_SQ_CTRL_SET(HINIC_MSS_MIN, QUEUE_INFO_MSS);\n\t}\n\n\treturn 0;\n}\n\nnetdev_tx_t hinic_lb_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct hinic_dev *nic_dev = netdev_priv(netdev);\n\tu16 prod_idx, q_id = skb->queue_mapping;\n\tstruct netdev_queue *netdev_txq;\n\tint nr_sges, err = NETDEV_TX_OK;\n\tstruct hinic_sq_wqe *sq_wqe;\n\tunsigned int wqe_size;\n\tstruct hinic_txq *txq;\n\tstruct hinic_qp *qp;\n\n\ttxq = &nic_dev->txqs[q_id];\n\tqp = container_of(txq->sq, struct hinic_qp, sq);\n\tnr_sges = skb_shinfo(skb)->nr_frags + 1;\n\n\terr = tx_map_skb(nic_dev, skb, txq->sges);\n\tif (err)\n\t\tgoto skb_error;\n\n\twqe_size = HINIC_SQ_WQE_SIZE(nr_sges);\n\n\tsq_wqe = hinic_sq_get_wqe(txq->sq, wqe_size, &prod_idx);\n\tif (!sq_wqe) {\n\t\tnetif_stop_subqueue(netdev, qp->q_id);\n\n\t\tsq_wqe = hinic_sq_get_wqe(txq->sq, wqe_size, &prod_idx);\n\t\tif (sq_wqe) {\n\t\t\tnetif_wake_subqueue(nic_dev->netdev, qp->q_id);\n\t\t\tgoto process_sq_wqe;\n\t\t}\n\n\t\ttx_unmap_skb(nic_dev, skb, txq->sges);\n\n\t\tu64_stats_update_begin(&txq->txq_stats.syncp);\n\t\ttxq->txq_stats.tx_busy++;\n\t\tu64_stats_update_end(&txq->txq_stats.syncp);\n\t\terr = NETDEV_TX_BUSY;\n\t\twqe_size = 0;\n\t\tgoto flush_skbs;\n\t}\n\nprocess_sq_wqe:\n\thinic_sq_prepare_wqe(txq->sq, sq_wqe, txq->sges, nr_sges);\n\thinic_sq_write_wqe(txq->sq, prod_idx, sq_wqe, skb, wqe_size);\n\nflush_skbs:\n\tnetdev_txq = netdev_get_tx_queue(netdev, q_id);\n\tif ((!netdev_xmit_more()) || (netif_xmit_stopped(netdev_txq)))\n\t\thinic_sq_write_db(txq->sq, prod_idx, wqe_size, 0);\n\n\treturn err;\n\nskb_error:\n\tdev_kfree_skb_any(skb);\n\tu64_stats_update_begin(&txq->txq_stats.syncp);\n\ttxq->txq_stats.tx_dropped++;\n\tu64_stats_update_end(&txq->txq_stats.syncp);\n\n\treturn NETDEV_TX_OK;\n}\n\nnetdev_tx_t hinic_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct hinic_dev *nic_dev = netdev_priv(netdev);\n\tu16 prod_idx, q_id = skb->queue_mapping;\n\tstruct netdev_queue *netdev_txq;\n\tint nr_sges, err = NETDEV_TX_OK;\n\tstruct hinic_sq_wqe *sq_wqe;\n\tunsigned int wqe_size;\n\tstruct hinic_txq *txq;\n\tstruct hinic_qp *qp;\n\n\ttxq = &nic_dev->txqs[q_id];\n\tqp = container_of(txq->sq, struct hinic_qp, sq);\n\n\tif (skb->len < MIN_SKB_LEN) {\n\t\tif (skb_pad(skb, MIN_SKB_LEN - skb->len)) {\n\t\t\tnetdev_err(netdev, \"Failed to pad skb\\n\");\n\t\t\tgoto update_error_stats;\n\t\t}\n\n\t\tskb->len = MIN_SKB_LEN;\n\t}\n\n\tnr_sges = skb_shinfo(skb)->nr_frags + 1;\n\tif (nr_sges > 17) {\n\t\tu64_stats_update_begin(&txq->txq_stats.syncp);\n\t\ttxq->txq_stats.big_frags_pkts++;\n\t\tu64_stats_update_end(&txq->txq_stats.syncp);\n\t}\n\n\tif (nr_sges > txq->max_sges) {\n\t\tnetdev_err(netdev, \"Too many Tx sges\\n\");\n\t\tgoto skb_error;\n\t}\n\n\terr = tx_map_skb(nic_dev, skb, txq->sges);\n\tif (err)\n\t\tgoto skb_error;\n\n\twqe_size = HINIC_SQ_WQE_SIZE(nr_sges);\n\n\tsq_wqe = hinic_sq_get_wqe(txq->sq, wqe_size, &prod_idx);\n\tif (!sq_wqe) {\n\t\tnetif_stop_subqueue(netdev, qp->q_id);\n\n\t\t \n\t\tsq_wqe = hinic_sq_get_wqe(txq->sq, wqe_size, &prod_idx);\n\t\tif (sq_wqe) {\n\t\t\tnetif_wake_subqueue(nic_dev->netdev, qp->q_id);\n\t\t\tgoto process_sq_wqe;\n\t\t}\n\n\t\ttx_unmap_skb(nic_dev, skb, txq->sges);\n\n\t\tu64_stats_update_begin(&txq->txq_stats.syncp);\n\t\ttxq->txq_stats.tx_busy++;\n\t\tu64_stats_update_end(&txq->txq_stats.syncp);\n\t\terr = NETDEV_TX_BUSY;\n\t\twqe_size = 0;\n\t\tgoto flush_skbs;\n\t}\n\nprocess_sq_wqe:\n\thinic_sq_prepare_wqe(txq->sq, sq_wqe, txq->sges, nr_sges);\n\n\terr = hinic_tx_offload(skb, &sq_wqe->task, &sq_wqe->ctrl.queue_info);\n\tif (err)\n\t\tgoto offload_error;\n\n\thinic_sq_write_wqe(txq->sq, prod_idx, sq_wqe, skb, wqe_size);\n\nflush_skbs:\n\tnetdev_txq = netdev_get_tx_queue(netdev, q_id);\n\tif ((!netdev_xmit_more()) || (netif_xmit_stopped(netdev_txq)))\n\t\thinic_sq_write_db(txq->sq, prod_idx, wqe_size, 0);\n\n\treturn err;\n\noffload_error:\n\thinic_sq_return_wqe(txq->sq, wqe_size);\n\ttx_unmap_skb(nic_dev, skb, txq->sges);\n\nskb_error:\n\tdev_kfree_skb_any(skb);\n\nupdate_error_stats:\n\tu64_stats_update_begin(&txq->txq_stats.syncp);\n\ttxq->txq_stats.tx_dropped++;\n\tu64_stats_update_end(&txq->txq_stats.syncp);\n\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic void tx_free_skb(struct hinic_dev *nic_dev, struct sk_buff *skb,\n\t\t\tstruct hinic_sge *sges)\n{\n\ttx_unmap_skb(nic_dev, skb, sges);\n\n\tdev_kfree_skb_any(skb);\n}\n\n \nstatic void free_all_tx_skbs(struct hinic_txq *txq)\n{\n\tstruct hinic_dev *nic_dev = netdev_priv(txq->netdev);\n\tstruct hinic_sq *sq = txq->sq;\n\tstruct hinic_sq_wqe *sq_wqe;\n\tunsigned int wqe_size;\n\tstruct sk_buff *skb;\n\tint nr_sges;\n\tu16 ci;\n\n\twhile ((sq_wqe = hinic_sq_read_wqebb(sq, &skb, &wqe_size, &ci))) {\n\t\tsq_wqe = hinic_sq_read_wqe(sq, &skb, wqe_size, &ci);\n\t\tif (!sq_wqe)\n\t\t\tbreak;\n\n\t\tnr_sges = skb_shinfo(skb)->nr_frags + 1;\n\n\t\thinic_sq_get_sges(sq_wqe, txq->free_sges, nr_sges);\n\n\t\thinic_sq_put_wqe(sq, wqe_size);\n\n\t\ttx_free_skb(nic_dev, skb, txq->free_sges);\n\t}\n}\n\n \nstatic int free_tx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct hinic_txq *txq = container_of(napi, struct hinic_txq, napi);\n\tstruct hinic_qp *qp = container_of(txq->sq, struct hinic_qp, sq);\n\tstruct hinic_dev *nic_dev = netdev_priv(txq->netdev);\n\tstruct netdev_queue *netdev_txq;\n\tstruct hinic_sq *sq = txq->sq;\n\tstruct hinic_wq *wq = sq->wq;\n\tstruct hinic_sq_wqe *sq_wqe;\n\tunsigned int wqe_size;\n\tint nr_sges, pkts = 0;\n\tstruct sk_buff *skb;\n\tu64 tx_bytes = 0;\n\tu16 hw_ci, sw_ci;\n\n\tdo {\n\t\thw_ci = HW_CONS_IDX(sq) & wq->mask;\n\n\t\tdma_rmb();\n\n\t\t \n\t\tsq_wqe = hinic_sq_read_wqebb(sq, &skb, &wqe_size, &sw_ci);\n\t\tif (!sq_wqe ||\n\t\t    (((hw_ci - sw_ci) & wq->mask) * wq->wqebb_size < wqe_size))\n\t\t\tbreak;\n\n\t\t \n\t\tif (wqe_size > wq->wqebb_size) {\n\t\t\tsq_wqe = hinic_sq_read_wqe(sq, &skb, wqe_size, &sw_ci);\n\t\t\tif (unlikely(!sq_wqe))\n\t\t\t\tbreak;\n\t\t}\n\n\t\ttx_bytes += skb->len;\n\t\tpkts++;\n\n\t\tnr_sges = skb_shinfo(skb)->nr_frags + 1;\n\n\t\thinic_sq_get_sges(sq_wqe, txq->free_sges, nr_sges);\n\n\t\thinic_sq_put_wqe(sq, wqe_size);\n\n\t\ttx_free_skb(nic_dev, skb, txq->free_sges);\n\t} while (pkts < budget);\n\n\tif (__netif_subqueue_stopped(nic_dev->netdev, qp->q_id) &&\n\t    hinic_get_sq_free_wqebbs(sq) >= HINIC_MIN_TX_NUM_WQEBBS(sq)) {\n\t\tnetdev_txq = netdev_get_tx_queue(txq->netdev, qp->q_id);\n\n\t\t__netif_tx_lock(netdev_txq, smp_processor_id());\n\t\tif (!netif_testing(nic_dev->netdev))\n\t\t\tnetif_wake_subqueue(nic_dev->netdev, qp->q_id);\n\n\t\t__netif_tx_unlock(netdev_txq);\n\n\t\tu64_stats_update_begin(&txq->txq_stats.syncp);\n\t\ttxq->txq_stats.tx_wake++;\n\t\tu64_stats_update_end(&txq->txq_stats.syncp);\n\t}\n\n\tu64_stats_update_begin(&txq->txq_stats.syncp);\n\ttxq->txq_stats.bytes += tx_bytes;\n\ttxq->txq_stats.pkts += pkts;\n\tu64_stats_update_end(&txq->txq_stats.syncp);\n\n\tif (pkts < budget) {\n\t\tnapi_complete(napi);\n\t\tif (!HINIC_IS_VF(nic_dev->hwdev->hwif))\n\t\t\thinic_hwdev_set_msix_state(nic_dev->hwdev,\n\t\t\t\t\t\t   sq->msix_entry,\n\t\t\t\t\t\t   HINIC_MSIX_ENABLE);\n\n\t\treturn pkts;\n\t}\n\n\treturn budget;\n}\n\nstatic irqreturn_t tx_irq(int irq, void *data)\n{\n\tstruct hinic_txq *txq = data;\n\tstruct hinic_dev *nic_dev;\n\n\tnic_dev = netdev_priv(txq->netdev);\n\n\tif (!HINIC_IS_VF(nic_dev->hwdev->hwif))\n\t\t \n\t\thinic_hwdev_set_msix_state(nic_dev->hwdev,\n\t\t\t\t\t   txq->sq->msix_entry,\n\t\t\t\t\t   HINIC_MSIX_DISABLE);\n\n\thinic_hwdev_msix_cnt_set(nic_dev->hwdev, txq->sq->msix_entry);\n\n\tnapi_schedule(&txq->napi);\n\treturn IRQ_HANDLED;\n}\n\nstatic int tx_request_irq(struct hinic_txq *txq)\n{\n\tstruct hinic_dev *nic_dev = netdev_priv(txq->netdev);\n\tstruct hinic_msix_config interrupt_info = {0};\n\tstruct hinic_intr_coal_info *intr_coal = NULL;\n\tstruct hinic_hwdev *hwdev = nic_dev->hwdev;\n\tstruct hinic_hwif *hwif = hwdev->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tstruct hinic_sq *sq = txq->sq;\n\tstruct hinic_qp *qp;\n\tint err;\n\n\tqp = container_of(sq, struct hinic_qp, sq);\n\n\tnetif_napi_add_weight(txq->netdev, &txq->napi, free_tx_poll,\n\t\t\t      nic_dev->tx_weight);\n\n\thinic_hwdev_msix_set(nic_dev->hwdev, sq->msix_entry,\n\t\t\t     TX_IRQ_NO_PENDING, TX_IRQ_NO_COALESC,\n\t\t\t     TX_IRQ_NO_LLI_TIMER, TX_IRQ_NO_CREDIT,\n\t\t\t     TX_IRQ_NO_RESEND_TIMER);\n\n\tintr_coal = &nic_dev->tx_intr_coalesce[qp->q_id];\n\tinterrupt_info.msix_index = sq->msix_entry;\n\tinterrupt_info.coalesce_timer_cnt = intr_coal->coalesce_timer_cfg;\n\tinterrupt_info.pending_cnt = intr_coal->pending_limt;\n\tinterrupt_info.resend_timer_cnt = intr_coal->resend_timer_cfg;\n\n\terr = hinic_set_interrupt_cfg(hwdev, &interrupt_info);\n\tif (err) {\n\t\tnetif_err(nic_dev, drv, txq->netdev,\n\t\t\t  \"Failed to set TX interrupt coalescing attribute\\n\");\n\t\tnetif_napi_del(&txq->napi);\n\t\treturn err;\n\t}\n\n\terr = request_irq(sq->irq, tx_irq, 0, txq->irq_name, txq);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to request Tx irq\\n\");\n\t\tnetif_napi_del(&txq->napi);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void tx_free_irq(struct hinic_txq *txq)\n{\n\tstruct hinic_sq *sq = txq->sq;\n\n\tfree_irq(sq->irq, txq);\n\tnetif_napi_del(&txq->napi);\n}\n\n \nint hinic_init_txq(struct hinic_txq *txq, struct hinic_sq *sq,\n\t\t   struct net_device *netdev)\n{\n\tstruct hinic_qp *qp = container_of(sq, struct hinic_qp, sq);\n\tstruct hinic_dev *nic_dev = netdev_priv(netdev);\n\tstruct hinic_hwdev *hwdev = nic_dev->hwdev;\n\tint err, irqname_len;\n\n\ttxq->netdev = netdev;\n\ttxq->sq = sq;\n\n\ttxq_stats_init(txq);\n\n\ttxq->max_sges = HINIC_MAX_SQ_BUFDESCS;\n\n\ttxq->sges = devm_kcalloc(&netdev->dev, txq->max_sges,\n\t\t\t\t sizeof(*txq->sges), GFP_KERNEL);\n\tif (!txq->sges)\n\t\treturn -ENOMEM;\n\n\ttxq->free_sges = devm_kcalloc(&netdev->dev, txq->max_sges,\n\t\t\t\t      sizeof(*txq->free_sges), GFP_KERNEL);\n\tif (!txq->free_sges) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_free_sges;\n\t}\n\n\tirqname_len = snprintf(NULL, 0, \"%s_txq%d\", netdev->name, qp->q_id) + 1;\n\ttxq->irq_name = devm_kzalloc(&netdev->dev, irqname_len, GFP_KERNEL);\n\tif (!txq->irq_name) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_irqname;\n\t}\n\n\tsprintf(txq->irq_name, \"%s_txq%d\", netdev->name, qp->q_id);\n\n\terr = hinic_hwdev_hw_ci_addr_set(hwdev, sq, CI_UPDATE_NO_PENDING,\n\t\t\t\t\t CI_UPDATE_NO_COALESC);\n\tif (err)\n\t\tgoto err_hw_ci;\n\n\terr = tx_request_irq(txq);\n\tif (err) {\n\t\tnetdev_err(netdev, \"Failed to request Tx irq\\n\");\n\t\tgoto err_req_tx_irq;\n\t}\n\n\treturn 0;\n\nerr_req_tx_irq:\nerr_hw_ci:\n\tdevm_kfree(&netdev->dev, txq->irq_name);\n\nerr_alloc_irqname:\n\tdevm_kfree(&netdev->dev, txq->free_sges);\n\nerr_alloc_free_sges:\n\tdevm_kfree(&netdev->dev, txq->sges);\n\treturn err;\n}\n\n \nvoid hinic_clean_txq(struct hinic_txq *txq)\n{\n\tstruct net_device *netdev = txq->netdev;\n\n\ttx_free_irq(txq);\n\n\tfree_all_tx_skbs(txq);\n\n\tdevm_kfree(&netdev->dev, txq->irq_name);\n\tdevm_kfree(&netdev->dev, txq->free_sges);\n\tdevm_kfree(&netdev->dev, txq->sges);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}