{
  "module_name": "hinic_hw_eqs.c",
  "hash_id": "dddf242e4881c6d8de3228ba9bca32f65434f7b8a92223a7913dfaf90304ec81",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/huawei/hinic/hinic_hw_eqs.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/errno.h>\n#include <linux/pci.h>\n#include <linux/device.h>\n#include <linux/workqueue.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/dma-mapping.h>\n#include <linux/log2.h>\n#include <asm/byteorder.h>\n#include <asm/barrier.h>\n\n#include \"hinic_hw_dev.h\"\n#include \"hinic_hw_csr.h\"\n#include \"hinic_hw_if.h\"\n#include \"hinic_hw_eqs.h\"\n\n#define HINIC_EQS_WQ_NAME                       \"hinic_eqs\"\n\n#define GET_EQ_NUM_PAGES(eq, pg_size)           \\\n\t\t(ALIGN((eq)->q_len * (eq)->elem_size, pg_size) / (pg_size))\n\n#define GET_EQ_NUM_ELEMS_IN_PG(eq, pg_size)     ((pg_size) / (eq)->elem_size)\n\n#define EQ_CONS_IDX_REG_ADDR(eq)        (((eq)->type == HINIC_AEQ) ? \\\n\t\t\tHINIC_CSR_AEQ_CONS_IDX_ADDR((eq)->q_id) : \\\n\t\t\tHINIC_CSR_CEQ_CONS_IDX_ADDR((eq)->q_id))\n\n#define EQ_PROD_IDX_REG_ADDR(eq)        (((eq)->type == HINIC_AEQ) ? \\\n\t\t\tHINIC_CSR_AEQ_PROD_IDX_ADDR((eq)->q_id) : \\\n\t\t\tHINIC_CSR_CEQ_PROD_IDX_ADDR((eq)->q_id))\n\n#define EQ_HI_PHYS_ADDR_REG(eq, pg_num) (((eq)->type == HINIC_AEQ) ? \\\n\t\t\tHINIC_CSR_AEQ_HI_PHYS_ADDR_REG((eq)->q_id, pg_num) : \\\n\t\t\tHINIC_CSR_CEQ_HI_PHYS_ADDR_REG((eq)->q_id, pg_num))\n\n#define EQ_LO_PHYS_ADDR_REG(eq, pg_num) (((eq)->type == HINIC_AEQ) ? \\\n\t\t\tHINIC_CSR_AEQ_LO_PHYS_ADDR_REG((eq)->q_id, pg_num) : \\\n\t\t\tHINIC_CSR_CEQ_LO_PHYS_ADDR_REG((eq)->q_id, pg_num))\n\n#define GET_EQ_ELEMENT(eq, idx)         \\\n\t\t((eq)->virt_addr[(idx) / (eq)->num_elem_in_pg] + \\\n\t\t (((idx) & ((eq)->num_elem_in_pg - 1)) * (eq)->elem_size))\n\n#define GET_AEQ_ELEM(eq, idx)           ((struct hinic_aeq_elem *) \\\n\t\t\t\t\tGET_EQ_ELEMENT(eq, idx))\n\n#define GET_CEQ_ELEM(eq, idx)           ((u32 *) \\\n\t\t\t\t\t GET_EQ_ELEMENT(eq, idx))\n\n#define GET_CURR_AEQ_ELEM(eq)           GET_AEQ_ELEM(eq, (eq)->cons_idx)\n\n#define GET_CURR_CEQ_ELEM(eq)           GET_CEQ_ELEM(eq, (eq)->cons_idx)\n\n#define PAGE_IN_4K(page_size)           ((page_size) >> 12)\n#define EQ_SET_HW_PAGE_SIZE_VAL(eq)     (ilog2(PAGE_IN_4K((eq)->page_size)))\n\n#define ELEMENT_SIZE_IN_32B(eq)         (((eq)->elem_size) >> 5)\n#define EQ_SET_HW_ELEM_SIZE_VAL(eq)     (ilog2(ELEMENT_SIZE_IN_32B(eq)))\n\n#define EQ_MAX_PAGES                    8\n\n#define CEQE_TYPE_SHIFT                 23\n#define CEQE_TYPE_MASK                  0x7\n\n#define CEQE_TYPE(ceqe)                 (((ceqe) >> CEQE_TYPE_SHIFT) &  \\\n\t\t\t\t\t CEQE_TYPE_MASK)\n\n#define CEQE_DATA_MASK                  0x3FFFFFF\n#define CEQE_DATA(ceqe)                 ((ceqe) & CEQE_DATA_MASK)\n\n#define aeq_to_aeqs(eq)                 \\\n\t\tcontainer_of((eq) - (eq)->q_id, struct hinic_aeqs, aeq[0])\n\n#define ceq_to_ceqs(eq)                 \\\n\t\tcontainer_of((eq) - (eq)->q_id, struct hinic_ceqs, ceq[0])\n\n#define work_to_aeq_work(work)          \\\n\t\tcontainer_of(work, struct hinic_eq_work, work)\n\n#define DMA_ATTR_AEQ_DEFAULT            0\n#define DMA_ATTR_CEQ_DEFAULT            0\n\n \n#define THRESH_CEQ_DEFAULT              0\n\nenum eq_int_mode {\n\tEQ_INT_MODE_ARMED,\n\tEQ_INT_MODE_ALWAYS\n};\n\nenum eq_arm_state {\n\tEQ_NOT_ARMED,\n\tEQ_ARMED\n};\n\n \nvoid hinic_aeq_register_hw_cb(struct hinic_aeqs *aeqs,\n\t\t\t      enum hinic_aeq_type event, void *handle,\n\t\t\t      void (*hwe_handler)(void *handle, void *data,\n\t\t\t\t\t\t  u8 size))\n{\n\tstruct hinic_hw_event_cb *hwe_cb = &aeqs->hwe_cb[event];\n\n\thwe_cb->hwe_handler = hwe_handler;\n\thwe_cb->handle = handle;\n\thwe_cb->hwe_state = HINIC_EQE_ENABLED;\n}\n\n \nvoid hinic_aeq_unregister_hw_cb(struct hinic_aeqs *aeqs,\n\t\t\t\tenum hinic_aeq_type event)\n{\n\tstruct hinic_hw_event_cb *hwe_cb = &aeqs->hwe_cb[event];\n\n\thwe_cb->hwe_state &= ~HINIC_EQE_ENABLED;\n\n\twhile (hwe_cb->hwe_state & HINIC_EQE_RUNNING)\n\t\tschedule();\n\n\thwe_cb->hwe_handler = NULL;\n}\n\n \nvoid hinic_ceq_register_cb(struct hinic_ceqs *ceqs,\n\t\t\t   enum hinic_ceq_type event, void *handle,\n\t\t\t   void (*handler)(void *handle, u32 ceqe_data))\n{\n\tstruct hinic_ceq_cb *ceq_cb = &ceqs->ceq_cb[event];\n\n\tceq_cb->handler = handler;\n\tceq_cb->handle = handle;\n\tceq_cb->ceqe_state = HINIC_EQE_ENABLED;\n}\n\n \nvoid hinic_ceq_unregister_cb(struct hinic_ceqs *ceqs,\n\t\t\t     enum hinic_ceq_type event)\n{\n\tstruct hinic_ceq_cb *ceq_cb = &ceqs->ceq_cb[event];\n\n\tceq_cb->ceqe_state &= ~HINIC_EQE_ENABLED;\n\n\twhile (ceq_cb->ceqe_state & HINIC_EQE_RUNNING)\n\t\tschedule();\n\n\tceq_cb->handler = NULL;\n}\n\nstatic u8 eq_cons_idx_checksum_set(u32 val)\n{\n\tu8 checksum = 0;\n\tint idx;\n\n\tfor (idx = 0; idx < 32; idx += 4)\n\t\tchecksum ^= ((val >> idx) & 0xF);\n\n\treturn (checksum & 0xF);\n}\n\n \nstatic void eq_update_ci(struct hinic_eq *eq, u32 arm_state)\n{\n\tu32 val, addr = EQ_CONS_IDX_REG_ADDR(eq);\n\n\t \n\tval = hinic_hwif_read_reg(eq->hwif, addr);\n\n\tval = HINIC_EQ_CI_CLEAR(val, IDX)       &\n\t      HINIC_EQ_CI_CLEAR(val, WRAPPED)   &\n\t      HINIC_EQ_CI_CLEAR(val, INT_ARMED) &\n\t      HINIC_EQ_CI_CLEAR(val, XOR_CHKSUM);\n\n\tval |= HINIC_EQ_CI_SET(eq->cons_idx, IDX)    |\n\t       HINIC_EQ_CI_SET(eq->wrapped, WRAPPED) |\n\t       HINIC_EQ_CI_SET(arm_state, INT_ARMED);\n\n\tval |= HINIC_EQ_CI_SET(eq_cons_idx_checksum_set(val), XOR_CHKSUM);\n\n\thinic_hwif_write_reg(eq->hwif, addr, val);\n}\n\n \nstatic void aeq_irq_handler(struct hinic_eq *eq)\n{\n\tstruct hinic_aeqs *aeqs = aeq_to_aeqs(eq);\n\tstruct hinic_hwif *hwif = aeqs->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tstruct hinic_aeq_elem *aeqe_curr;\n\tstruct hinic_hw_event_cb *hwe_cb;\n\tenum hinic_aeq_type event;\n\tunsigned long eqe_state;\n\tu32 aeqe_desc;\n\tint i, size;\n\n\tfor (i = 0; i < eq->q_len; i++) {\n\t\taeqe_curr = GET_CURR_AEQ_ELEM(eq);\n\n\t\t \n\t\taeqe_desc = be32_to_cpu(aeqe_curr->desc);\n\n\t\t \n\t\tif (HINIC_EQ_ELEM_DESC_GET(aeqe_desc, WRAPPED) == eq->wrapped)\n\t\t\tbreak;\n\n\t\tdma_rmb();\n\n\t\tevent = HINIC_EQ_ELEM_DESC_GET(aeqe_desc, TYPE);\n\t\tif (event >= HINIC_MAX_AEQ_EVENTS) {\n\t\t\tdev_err(&pdev->dev, \"Unknown AEQ Event %d\\n\", event);\n\t\t\treturn;\n\t\t}\n\n\t\tif (!HINIC_EQ_ELEM_DESC_GET(aeqe_desc, SRC)) {\n\t\t\thwe_cb = &aeqs->hwe_cb[event];\n\n\t\t\tsize = HINIC_EQ_ELEM_DESC_GET(aeqe_desc, SIZE);\n\n\t\t\teqe_state = cmpxchg(&hwe_cb->hwe_state,\n\t\t\t\t\t    HINIC_EQE_ENABLED,\n\t\t\t\t\t    HINIC_EQE_ENABLED |\n\t\t\t\t\t    HINIC_EQE_RUNNING);\n\t\t\tif (eqe_state == HINIC_EQE_ENABLED &&\n\t\t\t    hwe_cb->hwe_handler)\n\t\t\t\thwe_cb->hwe_handler(hwe_cb->handle,\n\t\t\t\t\t\t    aeqe_curr->data, size);\n\t\t\telse\n\t\t\t\tdev_err(&pdev->dev, \"Unhandled AEQ Event %d\\n\",\n\t\t\t\t\tevent);\n\n\t\t\thwe_cb->hwe_state &= ~HINIC_EQE_RUNNING;\n\t\t}\n\n\t\teq->cons_idx++;\n\n\t\tif (eq->cons_idx == eq->q_len) {\n\t\t\teq->cons_idx = 0;\n\t\t\teq->wrapped = !eq->wrapped;\n\t\t}\n\t}\n}\n\n \nstatic void ceq_event_handler(struct hinic_ceqs *ceqs, u32 ceqe)\n{\n\tstruct hinic_hwif *hwif = ceqs->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tstruct hinic_ceq_cb *ceq_cb;\n\tenum hinic_ceq_type event;\n\tunsigned long eqe_state;\n\n\tevent = CEQE_TYPE(ceqe);\n\tif (event >= HINIC_MAX_CEQ_EVENTS) {\n\t\tdev_err(&pdev->dev, \"Unknown CEQ event, event = %d\\n\", event);\n\t\treturn;\n\t}\n\n\tceq_cb = &ceqs->ceq_cb[event];\n\n\teqe_state = cmpxchg(&ceq_cb->ceqe_state,\n\t\t\t    HINIC_EQE_ENABLED,\n\t\t\t    HINIC_EQE_ENABLED | HINIC_EQE_RUNNING);\n\n\tif (eqe_state == HINIC_EQE_ENABLED && ceq_cb->handler)\n\t\tceq_cb->handler(ceq_cb->handle, CEQE_DATA(ceqe));\n\telse\n\t\tdev_err(&pdev->dev, \"Unhandled CEQ Event %d\\n\", event);\n\n\tceq_cb->ceqe_state &= ~HINIC_EQE_RUNNING;\n}\n\n \nstatic void ceq_irq_handler(struct hinic_eq *eq)\n{\n\tstruct hinic_ceqs *ceqs = ceq_to_ceqs(eq);\n\tu32 ceqe;\n\tint i;\n\n\tfor (i = 0; i < eq->q_len; i++) {\n\t\tceqe = *(GET_CURR_CEQ_ELEM(eq));\n\n\t\t \n\t\tceqe = be32_to_cpu(ceqe);\n\n\t\t \n\t\tif (HINIC_EQ_ELEM_DESC_GET(ceqe, WRAPPED) == eq->wrapped)\n\t\t\tbreak;\n\n\t\tceq_event_handler(ceqs, ceqe);\n\n\t\teq->cons_idx++;\n\n\t\tif (eq->cons_idx == eq->q_len) {\n\t\t\teq->cons_idx = 0;\n\t\t\teq->wrapped = !eq->wrapped;\n\t\t}\n\t}\n}\n\n \nstatic void eq_irq_handler(void *data)\n{\n\tstruct hinic_eq *eq = data;\n\n\tif (eq->type == HINIC_AEQ)\n\t\taeq_irq_handler(eq);\n\telse if (eq->type == HINIC_CEQ)\n\t\tceq_irq_handler(eq);\n\n\teq_update_ci(eq, EQ_ARMED);\n}\n\n \nstatic void eq_irq_work(struct work_struct *work)\n{\n\tstruct hinic_eq_work *aeq_work = work_to_aeq_work(work);\n\tstruct hinic_eq *aeq;\n\n\taeq = aeq_work->data;\n\teq_irq_handler(aeq);\n}\n\n \nstatic void ceq_tasklet(struct tasklet_struct *t)\n{\n\tstruct hinic_eq *ceq = from_tasklet(ceq, t, ceq_tasklet);\n\n\teq_irq_handler(ceq);\n}\n\n \nstatic irqreturn_t aeq_interrupt(int irq, void *data)\n{\n\tstruct hinic_eq_work *aeq_work;\n\tstruct hinic_eq *aeq = data;\n\tstruct hinic_aeqs *aeqs;\n\n\t \n\thinic_msix_attr_cnt_clear(aeq->hwif, aeq->msix_entry.entry);\n\n\taeq_work = &aeq->aeq_work;\n\taeq_work->data = aeq;\n\n\taeqs = aeq_to_aeqs(aeq);\n\tqueue_work(aeqs->workq, &aeq_work->work);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t ceq_interrupt(int irq, void *data)\n{\n\tstruct hinic_eq *ceq = data;\n\n\t \n\thinic_msix_attr_cnt_clear(ceq->hwif, ceq->msix_entry.entry);\n\n\ttasklet_schedule(&ceq->ceq_tasklet);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic u32 get_ctrl0_val(struct hinic_eq *eq, u32 addr)\n{\n\tstruct msix_entry *msix_entry = &eq->msix_entry;\n\tenum hinic_eq_type type = eq->type;\n\tu32 val, ctrl0;\n\n\tif (type == HINIC_AEQ) {\n\t\t \n\t\taddr = HINIC_CSR_AEQ_CTRL_0_ADDR(eq->q_id);\n\n\t\tval = hinic_hwif_read_reg(eq->hwif, addr);\n\n\t\tval = HINIC_AEQ_CTRL_0_CLEAR(val, INT_IDX)      &\n\t\t      HINIC_AEQ_CTRL_0_CLEAR(val, DMA_ATTR)     &\n\t\t      HINIC_AEQ_CTRL_0_CLEAR(val, PCI_INTF_IDX) &\n\t\t      HINIC_AEQ_CTRL_0_CLEAR(val, INT_MODE);\n\n\t\tctrl0 = HINIC_AEQ_CTRL_0_SET(msix_entry->entry, INT_IDX)     |\n\t\t\tHINIC_AEQ_CTRL_0_SET(DMA_ATTR_AEQ_DEFAULT, DMA_ATTR) |\n\t\t\tHINIC_AEQ_CTRL_0_SET(HINIC_HWIF_PCI_INTF(eq->hwif),\n\t\t\t\t\t     PCI_INTF_IDX)                   |\n\t\t\tHINIC_AEQ_CTRL_0_SET(EQ_INT_MODE_ARMED, INT_MODE);\n\n\t\tval |= ctrl0;\n\t} else {\n\t\t \n\t\taddr = HINIC_CSR_CEQ_CTRL_0_ADDR(eq->q_id);\n\n\t\tval = hinic_hwif_read_reg(eq->hwif, addr);\n\n\t\tval = HINIC_CEQ_CTRL_0_CLEAR(val, INTR_IDX)     &\n\t\t      HINIC_CEQ_CTRL_0_CLEAR(val, DMA_ATTR)     &\n\t\t      HINIC_CEQ_CTRL_0_CLEAR(val, KICK_THRESH)  &\n\t\t      HINIC_CEQ_CTRL_0_CLEAR(val, PCI_INTF_IDX) &\n\t\t      HINIC_CEQ_CTRL_0_CLEAR(val, INTR_MODE);\n\n\t\tctrl0 = HINIC_CEQ_CTRL_0_SET(msix_entry->entry, INTR_IDX)     |\n\t\t\tHINIC_CEQ_CTRL_0_SET(DMA_ATTR_CEQ_DEFAULT, DMA_ATTR)  |\n\t\t\tHINIC_CEQ_CTRL_0_SET(THRESH_CEQ_DEFAULT, KICK_THRESH) |\n\t\t\tHINIC_CEQ_CTRL_0_SET(HINIC_HWIF_PCI_INTF(eq->hwif),\n\t\t\t\t\t     PCI_INTF_IDX)                    |\n\t\t\tHINIC_CEQ_CTRL_0_SET(EQ_INT_MODE_ARMED, INTR_MODE);\n\n\t\tval |= ctrl0;\n\t}\n\treturn val;\n}\n\nstatic void set_ctrl0(struct hinic_eq *eq)\n{\n\tu32 val, addr;\n\n\tif (eq->type == HINIC_AEQ)\n\t\taddr = HINIC_CSR_AEQ_CTRL_0_ADDR(eq->q_id);\n\telse\n\t\taddr = HINIC_CSR_CEQ_CTRL_0_ADDR(eq->q_id);\n\n\tval = get_ctrl0_val(eq, addr);\n\n\thinic_hwif_write_reg(eq->hwif, addr, val);\n}\n\nstatic u32 get_ctrl1_val(struct hinic_eq *eq, u32 addr)\n{\n\tu32 page_size_val, elem_size, val, ctrl1;\n\tenum hinic_eq_type type = eq->type;\n\n\tif (type == HINIC_AEQ) {\n\t\t \n\t\taddr = HINIC_CSR_AEQ_CTRL_1_ADDR(eq->q_id);\n\n\t\tpage_size_val = EQ_SET_HW_PAGE_SIZE_VAL(eq);\n\t\telem_size = EQ_SET_HW_ELEM_SIZE_VAL(eq);\n\n\t\tval = hinic_hwif_read_reg(eq->hwif, addr);\n\n\t\tval = HINIC_AEQ_CTRL_1_CLEAR(val, LEN)          &\n\t\t      HINIC_AEQ_CTRL_1_CLEAR(val, ELEM_SIZE)    &\n\t\t      HINIC_AEQ_CTRL_1_CLEAR(val, PAGE_SIZE);\n\n\t\tctrl1 = HINIC_AEQ_CTRL_1_SET(eq->q_len, LEN)            |\n\t\t\tHINIC_AEQ_CTRL_1_SET(elem_size, ELEM_SIZE)      |\n\t\t\tHINIC_AEQ_CTRL_1_SET(page_size_val, PAGE_SIZE);\n\n\t\tval |= ctrl1;\n\t} else {\n\t\t \n\t\taddr = HINIC_CSR_CEQ_CTRL_1_ADDR(eq->q_id);\n\n\t\tpage_size_val = EQ_SET_HW_PAGE_SIZE_VAL(eq);\n\n\t\tval = hinic_hwif_read_reg(eq->hwif, addr);\n\n\t\tval = HINIC_CEQ_CTRL_1_CLEAR(val, LEN) &\n\t\t      HINIC_CEQ_CTRL_1_CLEAR(val, PAGE_SIZE);\n\n\t\tctrl1 = HINIC_CEQ_CTRL_1_SET(eq->q_len, LEN) |\n\t\t\tHINIC_CEQ_CTRL_1_SET(page_size_val, PAGE_SIZE);\n\n\t\tval |= ctrl1;\n\t}\n\treturn val;\n}\n\nstatic void set_ctrl1(struct hinic_eq *eq)\n{\n\tu32 addr, val;\n\n\tif (eq->type == HINIC_AEQ)\n\t\taddr = HINIC_CSR_AEQ_CTRL_1_ADDR(eq->q_id);\n\telse\n\t\taddr = HINIC_CSR_CEQ_CTRL_1_ADDR(eq->q_id);\n\n\tval = get_ctrl1_val(eq, addr);\n\n\thinic_hwif_write_reg(eq->hwif, addr, val);\n}\n\nstatic int set_ceq_ctrl_reg(struct hinic_eq *eq)\n{\n\tstruct hinic_ceq_ctrl_reg ceq_ctrl = {0};\n\tstruct hinic_hwdev *hwdev = eq->hwdev;\n\tu16 out_size = sizeof(ceq_ctrl);\n\tu16 in_size = sizeof(ceq_ctrl);\n\tstruct hinic_pfhwdev *pfhwdev;\n\tu32 addr;\n\tint err;\n\n\tpfhwdev = container_of(hwdev, struct hinic_pfhwdev, hwdev);\n\n\taddr = HINIC_CSR_CEQ_CTRL_0_ADDR(eq->q_id);\n\tceq_ctrl.ctrl0 = get_ctrl0_val(eq, addr);\n\taddr = HINIC_CSR_CEQ_CTRL_1_ADDR(eq->q_id);\n\tceq_ctrl.ctrl1 = get_ctrl1_val(eq, addr);\n\n\tceq_ctrl.func_id = HINIC_HWIF_FUNC_IDX(hwdev->hwif);\n\tceq_ctrl.q_id = eq->q_id;\n\n\terr = hinic_msg_to_mgmt(&pfhwdev->pf_to_mgmt, HINIC_MOD_COMM,\n\t\t\t\tHINIC_COMM_CMD_CEQ_CTRL_REG_WR_BY_UP,\n\t\t\t\t&ceq_ctrl, in_size,\n\t\t\t\t&ceq_ctrl, &out_size, HINIC_MGMT_MSG_SYNC);\n\tif (err || !out_size || ceq_ctrl.status) {\n\t\tdev_err(&hwdev->hwif->pdev->dev,\n\t\t\t\"Failed to set ceq %d ctrl reg, err: %d status: 0x%x, out_size: 0x%x\\n\",\n\t\t\teq->q_id, err, ceq_ctrl.status, out_size);\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int set_eq_ctrls(struct hinic_eq *eq)\n{\n\tif (HINIC_IS_VF(eq->hwif) && eq->type == HINIC_CEQ)\n\t\treturn set_ceq_ctrl_reg(eq);\n\n\tset_ctrl0(eq);\n\tset_ctrl1(eq);\n\treturn 0;\n}\n\n \nstatic void aeq_elements_init(struct hinic_eq *eq, u32 init_val)\n{\n\tstruct hinic_aeq_elem *aeqe;\n\tint i;\n\n\tfor (i = 0; i < eq->q_len; i++) {\n\t\taeqe = GET_AEQ_ELEM(eq, i);\n\t\taeqe->desc = cpu_to_be32(init_val);\n\t}\n\n\twmb();   \n}\n\n \nstatic void ceq_elements_init(struct hinic_eq *eq, u32 init_val)\n{\n\tu32 *ceqe;\n\tint i;\n\n\tfor (i = 0; i < eq->q_len; i++) {\n\t\tceqe = GET_CEQ_ELEM(eq, i);\n\t\t*(ceqe) = cpu_to_be32(init_val);\n\t}\n\n\twmb();   \n}\n\n \nstatic int alloc_eq_pages(struct hinic_eq *eq)\n{\n\tstruct hinic_hwif *hwif = eq->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tu32 init_val, addr, val;\n\tint err, pg;\n\n\teq->dma_addr = devm_kcalloc(&pdev->dev, eq->num_pages,\n\t\t\t\t    sizeof(*eq->dma_addr), GFP_KERNEL);\n\tif (!eq->dma_addr)\n\t\treturn -ENOMEM;\n\n\teq->virt_addr = devm_kcalloc(&pdev->dev, eq->num_pages,\n\t\t\t\t     sizeof(*eq->virt_addr), GFP_KERNEL);\n\tif (!eq->virt_addr) {\n\t\terr = -ENOMEM;\n\t\tgoto err_virt_addr_alloc;\n\t}\n\n\tfor (pg = 0; pg < eq->num_pages; pg++) {\n\t\teq->virt_addr[pg] = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\t\t       eq->page_size,\n\t\t\t\t\t\t       &eq->dma_addr[pg],\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!eq->virt_addr[pg]) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_dma_alloc;\n\t\t}\n\n\t\taddr = EQ_HI_PHYS_ADDR_REG(eq, pg);\n\t\tval = upper_32_bits(eq->dma_addr[pg]);\n\n\t\thinic_hwif_write_reg(hwif, addr, val);\n\n\t\taddr = EQ_LO_PHYS_ADDR_REG(eq, pg);\n\t\tval = lower_32_bits(eq->dma_addr[pg]);\n\n\t\thinic_hwif_write_reg(hwif, addr, val);\n\t}\n\n\tinit_val = HINIC_EQ_ELEM_DESC_SET(eq->wrapped, WRAPPED);\n\n\tif (eq->type == HINIC_AEQ)\n\t\taeq_elements_init(eq, init_val);\n\telse if (eq->type == HINIC_CEQ)\n\t\tceq_elements_init(eq, init_val);\n\n\treturn 0;\n\nerr_dma_alloc:\n\twhile (--pg >= 0)\n\t\tdma_free_coherent(&pdev->dev, eq->page_size,\n\t\t\t\t  eq->virt_addr[pg],\n\t\t\t\t  eq->dma_addr[pg]);\n\n\tdevm_kfree(&pdev->dev, eq->virt_addr);\n\nerr_virt_addr_alloc:\n\tdevm_kfree(&pdev->dev, eq->dma_addr);\n\treturn err;\n}\n\n \nstatic void free_eq_pages(struct hinic_eq *eq)\n{\n\tstruct hinic_hwif *hwif = eq->hwif;\n\tstruct pci_dev *pdev = hwif->pdev;\n\tint pg;\n\n\tfor (pg = 0; pg < eq->num_pages; pg++)\n\t\tdma_free_coherent(&pdev->dev, eq->page_size,\n\t\t\t\t  eq->virt_addr[pg],\n\t\t\t\t  eq->dma_addr[pg]);\n\n\tdevm_kfree(&pdev->dev, eq->virt_addr);\n\tdevm_kfree(&pdev->dev, eq->dma_addr);\n}\n\n \nstatic int init_eq(struct hinic_eq *eq, struct hinic_hwif *hwif,\n\t\t   enum hinic_eq_type type, int q_id, u32 q_len, u32 page_size,\n\t\t   struct msix_entry entry)\n{\n\tstruct pci_dev *pdev = hwif->pdev;\n\tint err;\n\n\teq->hwif = hwif;\n\teq->type = type;\n\teq->q_id = q_id;\n\teq->q_len = q_len;\n\teq->page_size = page_size;\n\n\t \n\thinic_hwif_write_reg(eq->hwif, EQ_CONS_IDX_REG_ADDR(eq), 0);\n\thinic_hwif_write_reg(eq->hwif, EQ_PROD_IDX_REG_ADDR(eq), 0);\n\n\teq->cons_idx = 0;\n\teq->wrapped = 0;\n\n\tif (type == HINIC_AEQ) {\n\t\teq->elem_size = HINIC_AEQE_SIZE;\n\t} else if (type == HINIC_CEQ) {\n\t\teq->elem_size = HINIC_CEQE_SIZE;\n\t} else {\n\t\tdev_err(&pdev->dev, \"Invalid EQ type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\teq->num_pages = GET_EQ_NUM_PAGES(eq, page_size);\n\teq->num_elem_in_pg = GET_EQ_NUM_ELEMS_IN_PG(eq, page_size);\n\n\teq->msix_entry = entry;\n\n\tif (eq->num_elem_in_pg & (eq->num_elem_in_pg - 1)) {\n\t\tdev_err(&pdev->dev, \"num elements in eq page != power of 2\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (eq->num_pages > EQ_MAX_PAGES) {\n\t\tdev_err(&pdev->dev, \"too many pages for eq\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = set_eq_ctrls(eq);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to set eq ctrls\\n\");\n\t\treturn err;\n\t}\n\n\teq_update_ci(eq, EQ_ARMED);\n\n\terr = alloc_eq_pages(eq);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to allocate pages for eq\\n\");\n\t\treturn err;\n\t}\n\n\tif (type == HINIC_AEQ) {\n\t\tstruct hinic_eq_work *aeq_work = &eq->aeq_work;\n\n\t\tINIT_WORK(&aeq_work->work, eq_irq_work);\n\t} else if (type == HINIC_CEQ) {\n\t\ttasklet_setup(&eq->ceq_tasklet, ceq_tasklet);\n\t}\n\n\t \n\thinic_msix_attr_set(eq->hwif, eq->msix_entry.entry,\n\t\t\t    HINIC_EQ_MSIX_PENDING_LIMIT_DEFAULT,\n\t\t\t    HINIC_EQ_MSIX_COALESC_TIMER_DEFAULT,\n\t\t\t    HINIC_EQ_MSIX_LLI_TIMER_DEFAULT,\n\t\t\t    HINIC_EQ_MSIX_LLI_CREDIT_LIMIT_DEFAULT,\n\t\t\t    HINIC_EQ_MSIX_RESEND_TIMER_DEFAULT);\n\n\tif (type == HINIC_AEQ) {\n\t\tsnprintf(eq->irq_name, sizeof(eq->irq_name), \"hinic_aeq%d@pci:%s\", eq->q_id,\n\t\t\t pci_name(pdev));\n\t\terr = request_irq(entry.vector, aeq_interrupt, 0, eq->irq_name, eq);\n\t} else if (type == HINIC_CEQ) {\n\t\tsnprintf(eq->irq_name, sizeof(eq->irq_name), \"hinic_ceq%d@pci:%s\", eq->q_id,\n\t\t\t pci_name(pdev));\n\t\terr = request_irq(entry.vector, ceq_interrupt, 0, eq->irq_name, eq);\n\t}\n\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to request irq for the EQ\\n\");\n\t\tgoto err_req_irq;\n\t}\n\n\treturn 0;\n\nerr_req_irq:\n\tfree_eq_pages(eq);\n\treturn err;\n}\n\n \nstatic void remove_eq(struct hinic_eq *eq)\n{\n\thinic_set_msix_state(eq->hwif, eq->msix_entry.entry,\n\t\t\t     HINIC_MSIX_DISABLE);\n\tfree_irq(eq->msix_entry.vector, eq);\n\n\tif (eq->type == HINIC_AEQ) {\n\t\tstruct hinic_eq_work *aeq_work = &eq->aeq_work;\n\n\t\tcancel_work_sync(&aeq_work->work);\n\t\t \n\t\thinic_hwif_write_reg(eq->hwif,\n\t\t\t\t     HINIC_CSR_AEQ_CTRL_1_ADDR(eq->q_id), 0);\n\t} else if (eq->type == HINIC_CEQ) {\n\t\ttasklet_kill(&eq->ceq_tasklet);\n\t\t \n\t\thinic_hwif_write_reg(eq->hwif,\n\t\t\t\t     HINIC_CSR_CEQ_CTRL_1_ADDR(eq->q_id), 0);\n\t}\n\n\t \n\teq->cons_idx = hinic_hwif_read_reg(eq->hwif, EQ_PROD_IDX_REG_ADDR(eq));\n\teq_update_ci(eq, EQ_NOT_ARMED);\n\n\tfree_eq_pages(eq);\n}\n\n \nint hinic_aeqs_init(struct hinic_aeqs *aeqs, struct hinic_hwif *hwif,\n\t\t    int num_aeqs, u32 q_len, u32 page_size,\n\t\t    struct msix_entry *msix_entries)\n{\n\tstruct pci_dev *pdev = hwif->pdev;\n\tint err, i, q_id;\n\n\taeqs->workq = create_singlethread_workqueue(HINIC_EQS_WQ_NAME);\n\tif (!aeqs->workq)\n\t\treturn -ENOMEM;\n\n\taeqs->hwif = hwif;\n\taeqs->num_aeqs = num_aeqs;\n\n\tfor (q_id = 0; q_id < num_aeqs; q_id++) {\n\t\terr = init_eq(&aeqs->aeq[q_id], hwif, HINIC_AEQ, q_id, q_len,\n\t\t\t      page_size, msix_entries[q_id]);\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev, \"Failed to init aeq %d\\n\", q_id);\n\t\t\tgoto err_init_aeq;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_init_aeq:\n\tfor (i = 0; i < q_id; i++)\n\t\tremove_eq(&aeqs->aeq[i]);\n\n\tdestroy_workqueue(aeqs->workq);\n\treturn err;\n}\n\n \nvoid hinic_aeqs_free(struct hinic_aeqs *aeqs)\n{\n\tint q_id;\n\n\tfor (q_id = 0; q_id < aeqs->num_aeqs ; q_id++)\n\t\tremove_eq(&aeqs->aeq[q_id]);\n\n\tdestroy_workqueue(aeqs->workq);\n}\n\n \nint hinic_ceqs_init(struct hinic_ceqs *ceqs, struct hinic_hwif *hwif,\n\t\t    int num_ceqs, u32 q_len, u32 page_size,\n\t\t    struct msix_entry *msix_entries)\n{\n\tstruct pci_dev *pdev = hwif->pdev;\n\tint i, q_id, err;\n\n\tceqs->hwif = hwif;\n\tceqs->num_ceqs = num_ceqs;\n\n\tfor (q_id = 0; q_id < num_ceqs; q_id++) {\n\t\tceqs->ceq[q_id].hwdev = ceqs->hwdev;\n\t\terr = init_eq(&ceqs->ceq[q_id], hwif, HINIC_CEQ, q_id, q_len,\n\t\t\t      page_size, msix_entries[q_id]);\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev, \"Failed to init ceq %d\\n\", q_id);\n\t\t\tgoto err_init_ceq;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_init_ceq:\n\tfor (i = 0; i < q_id; i++)\n\t\tremove_eq(&ceqs->ceq[i]);\n\n\treturn err;\n}\n\n \nvoid hinic_ceqs_free(struct hinic_ceqs *ceqs)\n{\n\tint q_id;\n\n\tfor (q_id = 0; q_id < ceqs->num_ceqs; q_id++)\n\t\tremove_eq(&ceqs->ceq[q_id]);\n}\n\nvoid hinic_dump_ceq_info(struct hinic_hwdev *hwdev)\n{\n\tstruct hinic_eq *eq = NULL;\n\tu32 addr, ci, pi;\n\tint q_id;\n\n\tfor (q_id = 0; q_id < hwdev->func_to_io.ceqs.num_ceqs; q_id++) {\n\t\teq = &hwdev->func_to_io.ceqs.ceq[q_id];\n\t\taddr = EQ_CONS_IDX_REG_ADDR(eq);\n\t\tci = hinic_hwif_read_reg(hwdev->hwif, addr);\n\t\taddr = EQ_PROD_IDX_REG_ADDR(eq);\n\t\tpi = hinic_hwif_read_reg(hwdev->hwif, addr);\n\t\tdev_err(&hwdev->hwif->pdev->dev, \"Ceq id: %d, ci: 0x%08x, sw_ci: 0x%08x, pi: 0x%x, tasklet_state: 0x%lx, wrap: %d, ceqe: 0x%x\\n\",\n\t\t\tq_id, ci, eq->cons_idx, pi,\n\t\t\teq->ceq_tasklet.state,\n\t\t\teq->wrapped, be32_to_cpu(*(__be32 *)(GET_CURR_CEQ_ELEM(eq))));\n\t}\n}\n\nvoid hinic_dump_aeq_info(struct hinic_hwdev *hwdev)\n{\n\tstruct hinic_aeq_elem *aeqe_pos = NULL;\n\tstruct hinic_eq *eq = NULL;\n\tu32 addr, ci, pi;\n\tint q_id;\n\n\tfor (q_id = 0; q_id < hwdev->aeqs.num_aeqs; q_id++) {\n\t\teq = &hwdev->aeqs.aeq[q_id];\n\t\taddr = EQ_CONS_IDX_REG_ADDR(eq);\n\t\tci = hinic_hwif_read_reg(hwdev->hwif, addr);\n\t\taddr = EQ_PROD_IDX_REG_ADDR(eq);\n\t\tpi = hinic_hwif_read_reg(hwdev->hwif, addr);\n\t\taeqe_pos = GET_CURR_AEQ_ELEM(eq);\n\t\tdev_err(&hwdev->hwif->pdev->dev, \"Aeq id: %d, ci: 0x%08x, pi: 0x%x, work_state: 0x%x, wrap: %d, desc: 0x%x\\n\",\n\t\t\tq_id, ci, pi, work_busy(&eq->aeq_work.work),\n\t\t\teq->wrapped, be32_to_cpu(aeqe_pos->desc));\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}