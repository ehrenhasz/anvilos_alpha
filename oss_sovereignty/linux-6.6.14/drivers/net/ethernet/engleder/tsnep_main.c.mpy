{
  "module_name": "tsnep_main.c",
  "hash_id": "3c1ec20f61159ec1e66d1e71dfa3fe71fb1d7400987fe968f8e3ae93f4ef4ed5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/engleder/tsnep_main.c",
  "human_readable_source": "\n \n\n \n\n#include \"tsnep.h\"\n#include \"tsnep_hw.h\"\n\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_net.h>\n#include <linux/of_mdio.h>\n#include <linux/interrupt.h>\n#include <linux/etherdevice.h>\n#include <linux/phy.h>\n#include <linux/iopoll.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/page_pool/helpers.h>\n#include <net/xdp_sock_drv.h>\n\n#define TSNEP_RX_OFFSET (max(NET_SKB_PAD, XDP_PACKET_HEADROOM) + NET_IP_ALIGN)\n#define TSNEP_HEADROOM ALIGN(TSNEP_RX_OFFSET, 4)\n#define TSNEP_MAX_RX_BUF_SIZE (PAGE_SIZE - TSNEP_HEADROOM - \\\n\t\t\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n \n#define TSNEP_XSK_RX_BUF_SIZE (ALIGN(TSNEP_RX_INLINE_METADATA_SIZE + \\\n\t\t\t\t     ETH_FRAME_LEN + ETH_FCS_LEN + \\\n\t\t\t\t     VLAN_HLEN * 2, 4))\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n#define DMA_ADDR_HIGH(dma_addr) ((u32)(((dma_addr) >> 32) & 0xFFFFFFFF))\n#else\n#define DMA_ADDR_HIGH(dma_addr) ((u32)(0))\n#endif\n#define DMA_ADDR_LOW(dma_addr) ((u32)((dma_addr) & 0xFFFFFFFF))\n\n#define TSNEP_COALESCE_USECS_DEFAULT 64\n#define TSNEP_COALESCE_USECS_MAX     ((ECM_INT_DELAY_MASK >> ECM_INT_DELAY_SHIFT) * \\\n\t\t\t\t      ECM_INT_DELAY_BASE_US + ECM_INT_DELAY_BASE_US - 1)\n\n#define TSNEP_TX_TYPE_SKB\tBIT(0)\n#define TSNEP_TX_TYPE_SKB_FRAG\tBIT(1)\n#define TSNEP_TX_TYPE_XDP_TX\tBIT(2)\n#define TSNEP_TX_TYPE_XDP_NDO\tBIT(3)\n#define TSNEP_TX_TYPE_XDP\t(TSNEP_TX_TYPE_XDP_TX | TSNEP_TX_TYPE_XDP_NDO)\n#define TSNEP_TX_TYPE_XSK\tBIT(4)\n\n#define TSNEP_XDP_TX\t\tBIT(0)\n#define TSNEP_XDP_REDIRECT\tBIT(1)\n\nstatic void tsnep_enable_irq(struct tsnep_adapter *adapter, u32 mask)\n{\n\tiowrite32(mask, adapter->addr + ECM_INT_ENABLE);\n}\n\nstatic void tsnep_disable_irq(struct tsnep_adapter *adapter, u32 mask)\n{\n\tmask |= ECM_INT_DISABLE;\n\tiowrite32(mask, adapter->addr + ECM_INT_ENABLE);\n}\n\nstatic irqreturn_t tsnep_irq(int irq, void *arg)\n{\n\tstruct tsnep_adapter *adapter = arg;\n\tu32 active = ioread32(adapter->addr + ECM_INT_ACTIVE);\n\n\t \n\tif (active != 0)\n\t\tiowrite32(active, adapter->addr + ECM_INT_ACKNOWLEDGE);\n\n\t \n\tif ((active & ECM_INT_LINK) != 0)\n\t\tphy_mac_interrupt(adapter->netdev->phydev);\n\n\t \n\tif ((active & adapter->queue[0].irq_mask) != 0) {\n\t\tif (napi_schedule_prep(&adapter->queue[0].napi)) {\n\t\t\ttsnep_disable_irq(adapter, adapter->queue[0].irq_mask);\n\t\t\t \n\t\t\t__napi_schedule(&adapter->queue[0].napi);\n\t\t}\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t tsnep_irq_txrx(int irq, void *arg)\n{\n\tstruct tsnep_queue *queue = arg;\n\n\t \n\tif (napi_schedule_prep(&queue->napi)) {\n\t\ttsnep_disable_irq(queue->adapter, queue->irq_mask);\n\t\t \n\t\t__napi_schedule(&queue->napi);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nint tsnep_set_irq_coalesce(struct tsnep_queue *queue, u32 usecs)\n{\n\tif (usecs > TSNEP_COALESCE_USECS_MAX)\n\t\treturn -ERANGE;\n\n\tusecs /= ECM_INT_DELAY_BASE_US;\n\tusecs <<= ECM_INT_DELAY_SHIFT;\n\tusecs &= ECM_INT_DELAY_MASK;\n\n\tqueue->irq_delay &= ~ECM_INT_DELAY_MASK;\n\tqueue->irq_delay |= usecs;\n\tiowrite8(queue->irq_delay, queue->irq_delay_addr);\n\n\treturn 0;\n}\n\nu32 tsnep_get_irq_coalesce(struct tsnep_queue *queue)\n{\n\tu32 usecs;\n\n\tusecs = (queue->irq_delay & ECM_INT_DELAY_MASK);\n\tusecs >>= ECM_INT_DELAY_SHIFT;\n\tusecs *= ECM_INT_DELAY_BASE_US;\n\n\treturn usecs;\n}\n\nstatic int tsnep_mdiobus_read(struct mii_bus *bus, int addr, int regnum)\n{\n\tstruct tsnep_adapter *adapter = bus->priv;\n\tu32 md;\n\tint retval;\n\n\tmd = ECM_MD_READ;\n\tif (!adapter->suppress_preamble)\n\t\tmd |= ECM_MD_PREAMBLE;\n\tmd |= (regnum << ECM_MD_ADDR_SHIFT) & ECM_MD_ADDR_MASK;\n\tmd |= (addr << ECM_MD_PHY_ADDR_SHIFT) & ECM_MD_PHY_ADDR_MASK;\n\tiowrite32(md, adapter->addr + ECM_MD_CONTROL);\n\tretval = readl_poll_timeout_atomic(adapter->addr + ECM_MD_STATUS, md,\n\t\t\t\t\t   !(md & ECM_MD_BUSY), 16, 1000);\n\tif (retval != 0)\n\t\treturn retval;\n\n\treturn (md & ECM_MD_DATA_MASK) >> ECM_MD_DATA_SHIFT;\n}\n\nstatic int tsnep_mdiobus_write(struct mii_bus *bus, int addr, int regnum,\n\t\t\t       u16 val)\n{\n\tstruct tsnep_adapter *adapter = bus->priv;\n\tu32 md;\n\tint retval;\n\n\tmd = ECM_MD_WRITE;\n\tif (!adapter->suppress_preamble)\n\t\tmd |= ECM_MD_PREAMBLE;\n\tmd |= (regnum << ECM_MD_ADDR_SHIFT) & ECM_MD_ADDR_MASK;\n\tmd |= (addr << ECM_MD_PHY_ADDR_SHIFT) & ECM_MD_PHY_ADDR_MASK;\n\tmd |= ((u32)val << ECM_MD_DATA_SHIFT) & ECM_MD_DATA_MASK;\n\tiowrite32(md, adapter->addr + ECM_MD_CONTROL);\n\tretval = readl_poll_timeout_atomic(adapter->addr + ECM_MD_STATUS, md,\n\t\t\t\t\t   !(md & ECM_MD_BUSY), 16, 1000);\n\tif (retval != 0)\n\t\treturn retval;\n\n\treturn 0;\n}\n\nstatic void tsnep_set_link_mode(struct tsnep_adapter *adapter)\n{\n\tu32 mode;\n\n\tswitch (adapter->phydev->speed) {\n\tcase SPEED_100:\n\t\tmode = ECM_LINK_MODE_100;\n\t\tbreak;\n\tcase SPEED_1000:\n\t\tmode = ECM_LINK_MODE_1000;\n\t\tbreak;\n\tdefault:\n\t\tmode = ECM_LINK_MODE_OFF;\n\t\tbreak;\n\t}\n\tiowrite32(mode, adapter->addr + ECM_STATUS);\n}\n\nstatic void tsnep_phy_link_status_change(struct net_device *netdev)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(netdev);\n\tstruct phy_device *phydev = netdev->phydev;\n\n\tif (phydev->link)\n\t\ttsnep_set_link_mode(adapter);\n\n\tphy_print_status(netdev->phydev);\n}\n\nstatic int tsnep_phy_loopback(struct tsnep_adapter *adapter, bool enable)\n{\n\tint retval;\n\n\tretval = phy_loopback(adapter->phydev, enable);\n\n\t \n\tif (!retval && enable)\n\t\ttsnep_set_link_mode(adapter);\n\n\treturn retval;\n}\n\nstatic int tsnep_phy_open(struct tsnep_adapter *adapter)\n{\n\tstruct phy_device *phydev;\n\tstruct ethtool_eee ethtool_eee;\n\tint retval;\n\n\tretval = phy_connect_direct(adapter->netdev, adapter->phydev,\n\t\t\t\t    tsnep_phy_link_status_change,\n\t\t\t\t    adapter->phy_mode);\n\tif (retval)\n\t\treturn retval;\n\tphydev = adapter->netdev->phydev;\n\n\t \n\tphy_remove_link_mode(phydev, ETHTOOL_LINK_MODE_10baseT_Half_BIT);\n\tphy_remove_link_mode(phydev, ETHTOOL_LINK_MODE_10baseT_Full_BIT);\n\tphy_remove_link_mode(phydev, ETHTOOL_LINK_MODE_100baseT_Half_BIT);\n\tphy_remove_link_mode(phydev, ETHTOOL_LINK_MODE_1000baseT_Half_BIT);\n\n\t \n\tmemset(&ethtool_eee, 0, sizeof(ethtool_eee));\n\tphy_ethtool_set_eee(adapter->phydev, &ethtool_eee);\n\n\tadapter->phydev->irq = PHY_MAC_INTERRUPT;\n\tphy_start(adapter->phydev);\n\n\treturn 0;\n}\n\nstatic void tsnep_phy_close(struct tsnep_adapter *adapter)\n{\n\tphy_stop(adapter->netdev->phydev);\n\tphy_disconnect(adapter->netdev->phydev);\n}\n\nstatic void tsnep_tx_ring_cleanup(struct tsnep_tx *tx)\n{\n\tstruct device *dmadev = tx->adapter->dmadev;\n\tint i;\n\n\tmemset(tx->entry, 0, sizeof(tx->entry));\n\n\tfor (i = 0; i < TSNEP_RING_PAGE_COUNT; i++) {\n\t\tif (tx->page[i]) {\n\t\t\tdma_free_coherent(dmadev, PAGE_SIZE, tx->page[i],\n\t\t\t\t\t  tx->page_dma[i]);\n\t\t\ttx->page[i] = NULL;\n\t\t\ttx->page_dma[i] = 0;\n\t\t}\n\t}\n}\n\nstatic int tsnep_tx_ring_create(struct tsnep_tx *tx)\n{\n\tstruct device *dmadev = tx->adapter->dmadev;\n\tstruct tsnep_tx_entry *entry;\n\tstruct tsnep_tx_entry *next_entry;\n\tint i, j;\n\tint retval;\n\n\tfor (i = 0; i < TSNEP_RING_PAGE_COUNT; i++) {\n\t\ttx->page[i] =\n\t\t\tdma_alloc_coherent(dmadev, PAGE_SIZE, &tx->page_dma[i],\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!tx->page[i]) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto alloc_failed;\n\t\t}\n\t\tfor (j = 0; j < TSNEP_RING_ENTRIES_PER_PAGE; j++) {\n\t\t\tentry = &tx->entry[TSNEP_RING_ENTRIES_PER_PAGE * i + j];\n\t\t\tentry->desc_wb = (struct tsnep_tx_desc_wb *)\n\t\t\t\t(((u8 *)tx->page[i]) + TSNEP_DESC_SIZE * j);\n\t\t\tentry->desc = (struct tsnep_tx_desc *)\n\t\t\t\t(((u8 *)entry->desc_wb) + TSNEP_DESC_OFFSET);\n\t\t\tentry->desc_dma = tx->page_dma[i] + TSNEP_DESC_SIZE * j;\n\t\t\tentry->owner_user_flag = false;\n\t\t}\n\t}\n\tfor (i = 0; i < TSNEP_RING_SIZE; i++) {\n\t\tentry = &tx->entry[i];\n\t\tnext_entry = &tx->entry[(i + 1) & TSNEP_RING_MASK];\n\t\tentry->desc->next = __cpu_to_le64(next_entry->desc_dma);\n\t}\n\n\treturn 0;\n\nalloc_failed:\n\ttsnep_tx_ring_cleanup(tx);\n\treturn retval;\n}\n\nstatic void tsnep_tx_init(struct tsnep_tx *tx)\n{\n\tdma_addr_t dma;\n\n\tdma = tx->entry[0].desc_dma | TSNEP_RESET_OWNER_COUNTER;\n\tiowrite32(DMA_ADDR_LOW(dma), tx->addr + TSNEP_TX_DESC_ADDR_LOW);\n\tiowrite32(DMA_ADDR_HIGH(dma), tx->addr + TSNEP_TX_DESC_ADDR_HIGH);\n\ttx->write = 0;\n\ttx->read = 0;\n\ttx->owner_counter = 1;\n\ttx->increment_owner_counter = TSNEP_RING_SIZE - 1;\n}\n\nstatic void tsnep_tx_enable(struct tsnep_tx *tx)\n{\n\tstruct netdev_queue *nq;\n\n\tnq = netdev_get_tx_queue(tx->adapter->netdev, tx->queue_index);\n\n\t__netif_tx_lock_bh(nq);\n\tnetif_tx_wake_queue(nq);\n\t__netif_tx_unlock_bh(nq);\n}\n\nstatic void tsnep_tx_disable(struct tsnep_tx *tx, struct napi_struct *napi)\n{\n\tstruct netdev_queue *nq;\n\tu32 val;\n\n\tnq = netdev_get_tx_queue(tx->adapter->netdev, tx->queue_index);\n\n\t__netif_tx_lock_bh(nq);\n\tnetif_tx_stop_queue(nq);\n\t__netif_tx_unlock_bh(nq);\n\n\t \n\treadx_poll_timeout(ioread32, tx->addr + TSNEP_CONTROL, val,\n\t\t\t   ((val & TSNEP_CONTROL_TX_ENABLE) == 0), 10000,\n\t\t\t   1000000);\n\n\t \n\twhile (READ_ONCE(tx->read) != tx->write) {\n\t\tnapi_schedule(napi);\n\t\tnapi_synchronize(napi);\n\t}\n}\n\nstatic void tsnep_tx_activate(struct tsnep_tx *tx, int index, int length,\n\t\t\t      bool last)\n{\n\tstruct tsnep_tx_entry *entry = &tx->entry[index];\n\n\tentry->properties = 0;\n\t \n\tif (entry->skb) {\n\t\tentry->properties = length & TSNEP_DESC_LENGTH_MASK;\n\t\tentry->properties |= TSNEP_DESC_INTERRUPT_FLAG;\n\t\tif ((entry->type & TSNEP_TX_TYPE_SKB) &&\n\t\t    (skb_shinfo(entry->skb)->tx_flags & SKBTX_IN_PROGRESS))\n\t\t\tentry->properties |= TSNEP_DESC_EXTENDED_WRITEBACK_FLAG;\n\n\t\t \n\t\tentry->owner_user_flag = !entry->owner_user_flag;\n\t}\n\tif (last)\n\t\tentry->properties |= TSNEP_TX_DESC_LAST_FRAGMENT_FLAG;\n\tif (index == tx->increment_owner_counter) {\n\t\ttx->owner_counter++;\n\t\tif (tx->owner_counter == 4)\n\t\t\ttx->owner_counter = 1;\n\t\ttx->increment_owner_counter--;\n\t\tif (tx->increment_owner_counter < 0)\n\t\t\ttx->increment_owner_counter = TSNEP_RING_SIZE - 1;\n\t}\n\tentry->properties |=\n\t\t(tx->owner_counter << TSNEP_DESC_OWNER_COUNTER_SHIFT) &\n\t\tTSNEP_DESC_OWNER_COUNTER_MASK;\n\tif (entry->owner_user_flag)\n\t\tentry->properties |= TSNEP_TX_DESC_OWNER_USER_FLAG;\n\tentry->desc->more_properties =\n\t\t__cpu_to_le32(entry->len & TSNEP_DESC_LENGTH_MASK);\n\n\t \n\tdma_wmb();\n\n\tentry->desc->properties = __cpu_to_le32(entry->properties);\n}\n\nstatic int tsnep_tx_desc_available(struct tsnep_tx *tx)\n{\n\tif (tx->read <= tx->write)\n\t\treturn TSNEP_RING_SIZE - tx->write + tx->read - 1;\n\telse\n\t\treturn tx->read - tx->write - 1;\n}\n\nstatic int tsnep_tx_map(struct sk_buff *skb, struct tsnep_tx *tx, int count)\n{\n\tstruct device *dmadev = tx->adapter->dmadev;\n\tstruct tsnep_tx_entry *entry;\n\tunsigned int len;\n\tdma_addr_t dma;\n\tint map_len = 0;\n\tint i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tentry = &tx->entry[(tx->write + i) & TSNEP_RING_MASK];\n\n\t\tif (!i) {\n\t\t\tlen = skb_headlen(skb);\n\t\t\tdma = dma_map_single(dmadev, skb->data, len,\n\t\t\t\t\t     DMA_TO_DEVICE);\n\n\t\t\tentry->type = TSNEP_TX_TYPE_SKB;\n\t\t} else {\n\t\t\tlen = skb_frag_size(&skb_shinfo(skb)->frags[i - 1]);\n\t\t\tdma = skb_frag_dma_map(dmadev,\n\t\t\t\t\t       &skb_shinfo(skb)->frags[i - 1],\n\t\t\t\t\t       0, len, DMA_TO_DEVICE);\n\n\t\t\tentry->type = TSNEP_TX_TYPE_SKB_FRAG;\n\t\t}\n\t\tif (dma_mapping_error(dmadev, dma))\n\t\t\treturn -ENOMEM;\n\n\t\tentry->len = len;\n\t\tdma_unmap_addr_set(entry, dma, dma);\n\n\t\tentry->desc->tx = __cpu_to_le64(dma);\n\n\t\tmap_len += len;\n\t}\n\n\treturn map_len;\n}\n\nstatic int tsnep_tx_unmap(struct tsnep_tx *tx, int index, int count)\n{\n\tstruct device *dmadev = tx->adapter->dmadev;\n\tstruct tsnep_tx_entry *entry;\n\tint map_len = 0;\n\tint i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tentry = &tx->entry[(index + i) & TSNEP_RING_MASK];\n\n\t\tif (entry->len) {\n\t\t\tif (entry->type & TSNEP_TX_TYPE_SKB)\n\t\t\t\tdma_unmap_single(dmadev,\n\t\t\t\t\t\t dma_unmap_addr(entry, dma),\n\t\t\t\t\t\t dma_unmap_len(entry, len),\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\telse if (entry->type &\n\t\t\t\t (TSNEP_TX_TYPE_SKB_FRAG | TSNEP_TX_TYPE_XDP_NDO))\n\t\t\t\tdma_unmap_page(dmadev,\n\t\t\t\t\t       dma_unmap_addr(entry, dma),\n\t\t\t\t\t       dma_unmap_len(entry, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\tmap_len += entry->len;\n\t\t\tentry->len = 0;\n\t\t}\n\t}\n\n\treturn map_len;\n}\n\nstatic netdev_tx_t tsnep_xmit_frame_ring(struct sk_buff *skb,\n\t\t\t\t\t struct tsnep_tx *tx)\n{\n\tint count = 1;\n\tstruct tsnep_tx_entry *entry;\n\tint length;\n\tint i;\n\tint retval;\n\n\tif (skb_shinfo(skb)->nr_frags > 0)\n\t\tcount += skb_shinfo(skb)->nr_frags;\n\n\tif (tsnep_tx_desc_available(tx) < count) {\n\t\t \n\t\tnetif_stop_subqueue(tx->adapter->netdev, tx->queue_index);\n\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tentry = &tx->entry[tx->write];\n\tentry->skb = skb;\n\n\tretval = tsnep_tx_map(skb, tx, count);\n\tif (retval < 0) {\n\t\ttsnep_tx_unmap(tx, tx->write, count);\n\t\tdev_kfree_skb_any(entry->skb);\n\t\tentry->skb = NULL;\n\n\t\ttx->dropped++;\n\n\t\treturn NETDEV_TX_OK;\n\t}\n\tlength = retval;\n\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\n\tfor (i = 0; i < count; i++)\n\t\ttsnep_tx_activate(tx, (tx->write + i) & TSNEP_RING_MASK, length,\n\t\t\t\t  i == count - 1);\n\ttx->write = (tx->write + count) & TSNEP_RING_MASK;\n\n\tskb_tx_timestamp(skb);\n\n\t \n\tdma_wmb();\n\n\tiowrite32(TSNEP_CONTROL_TX_ENABLE, tx->addr + TSNEP_CONTROL);\n\n\tif (tsnep_tx_desc_available(tx) < (MAX_SKB_FRAGS + 1)) {\n\t\t \n\t\tnetif_stop_subqueue(tx->adapter->netdev, tx->queue_index);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic int tsnep_xdp_tx_map(struct xdp_frame *xdpf, struct tsnep_tx *tx,\n\t\t\t    struct skb_shared_info *shinfo, int count, u32 type)\n{\n\tstruct device *dmadev = tx->adapter->dmadev;\n\tstruct tsnep_tx_entry *entry;\n\tstruct page *page;\n\tskb_frag_t *frag;\n\tunsigned int len;\n\tint map_len = 0;\n\tdma_addr_t dma;\n\tvoid *data;\n\tint i;\n\n\tfrag = NULL;\n\tlen = xdpf->len;\n\tfor (i = 0; i < count; i++) {\n\t\tentry = &tx->entry[(tx->write + i) & TSNEP_RING_MASK];\n\t\tif (type & TSNEP_TX_TYPE_XDP_NDO) {\n\t\t\tdata = unlikely(frag) ? skb_frag_address(frag) :\n\t\t\t\t\t\txdpf->data;\n\t\t\tdma = dma_map_single(dmadev, data, len, DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(dmadev, dma))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tentry->type = TSNEP_TX_TYPE_XDP_NDO;\n\t\t} else {\n\t\t\tpage = unlikely(frag) ? skb_frag_page(frag) :\n\t\t\t\t\t\tvirt_to_page(xdpf->data);\n\t\t\tdma = page_pool_get_dma_addr(page);\n\t\t\tif (unlikely(frag))\n\t\t\t\tdma += skb_frag_off(frag);\n\t\t\telse\n\t\t\t\tdma += sizeof(*xdpf) + xdpf->headroom;\n\t\t\tdma_sync_single_for_device(dmadev, dma, len,\n\t\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\n\t\t\tentry->type = TSNEP_TX_TYPE_XDP_TX;\n\t\t}\n\n\t\tentry->len = len;\n\t\tdma_unmap_addr_set(entry, dma, dma);\n\n\t\tentry->desc->tx = __cpu_to_le64(dma);\n\n\t\tmap_len += len;\n\n\t\tif (i + 1 < count) {\n\t\t\tfrag = &shinfo->frags[i];\n\t\t\tlen = skb_frag_size(frag);\n\t\t}\n\t}\n\n\treturn map_len;\n}\n\n \nstatic bool tsnep_xdp_xmit_frame_ring(struct xdp_frame *xdpf,\n\t\t\t\t      struct tsnep_tx *tx, u32 type)\n{\n\tstruct skb_shared_info *shinfo = xdp_get_shared_info_from_frame(xdpf);\n\tstruct tsnep_tx_entry *entry;\n\tint count, length, retval, i;\n\n\tcount = 1;\n\tif (unlikely(xdp_frame_has_frags(xdpf)))\n\t\tcount += shinfo->nr_frags;\n\n\t \n\tif (tsnep_tx_desc_available(tx) < (MAX_SKB_FRAGS + 1 + count))\n\t\treturn false;\n\n\tentry = &tx->entry[tx->write];\n\tentry->xdpf = xdpf;\n\n\tretval = tsnep_xdp_tx_map(xdpf, tx, shinfo, count, type);\n\tif (retval < 0) {\n\t\ttsnep_tx_unmap(tx, tx->write, count);\n\t\tentry->xdpf = NULL;\n\n\t\ttx->dropped++;\n\n\t\treturn false;\n\t}\n\tlength = retval;\n\n\tfor (i = 0; i < count; i++)\n\t\ttsnep_tx_activate(tx, (tx->write + i) & TSNEP_RING_MASK, length,\n\t\t\t\t  i == count - 1);\n\ttx->write = (tx->write + count) & TSNEP_RING_MASK;\n\n\t \n\tdma_wmb();\n\n\treturn true;\n}\n\nstatic void tsnep_xdp_xmit_flush(struct tsnep_tx *tx)\n{\n\tiowrite32(TSNEP_CONTROL_TX_ENABLE, tx->addr + TSNEP_CONTROL);\n}\n\nstatic bool tsnep_xdp_xmit_back(struct tsnep_adapter *adapter,\n\t\t\t\tstruct xdp_buff *xdp,\n\t\t\t\tstruct netdev_queue *tx_nq, struct tsnep_tx *tx)\n{\n\tstruct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);\n\tbool xmit;\n\n\tif (unlikely(!xdpf))\n\t\treturn false;\n\n\t__netif_tx_lock(tx_nq, smp_processor_id());\n\n\txmit = tsnep_xdp_xmit_frame_ring(xdpf, tx, TSNEP_TX_TYPE_XDP_TX);\n\n\t \n\tif (xmit)\n\t\ttxq_trans_cond_update(tx_nq);\n\n\t__netif_tx_unlock(tx_nq);\n\n\treturn xmit;\n}\n\nstatic int tsnep_xdp_tx_map_zc(struct xdp_desc *xdpd, struct tsnep_tx *tx)\n{\n\tstruct tsnep_tx_entry *entry;\n\tdma_addr_t dma;\n\n\tentry = &tx->entry[tx->write];\n\tentry->zc = true;\n\n\tdma = xsk_buff_raw_get_dma(tx->xsk_pool, xdpd->addr);\n\txsk_buff_raw_dma_sync_for_device(tx->xsk_pool, dma, xdpd->len);\n\n\tentry->type = TSNEP_TX_TYPE_XSK;\n\tentry->len = xdpd->len;\n\n\tentry->desc->tx = __cpu_to_le64(dma);\n\n\treturn xdpd->len;\n}\n\nstatic void tsnep_xdp_xmit_frame_ring_zc(struct xdp_desc *xdpd,\n\t\t\t\t\t struct tsnep_tx *tx)\n{\n\tint length;\n\n\tlength = tsnep_xdp_tx_map_zc(xdpd, tx);\n\n\ttsnep_tx_activate(tx, tx->write, length, true);\n\ttx->write = (tx->write + 1) & TSNEP_RING_MASK;\n}\n\nstatic void tsnep_xdp_xmit_zc(struct tsnep_tx *tx)\n{\n\tint desc_available = tsnep_tx_desc_available(tx);\n\tstruct xdp_desc *descs = tx->xsk_pool->tx_descs;\n\tint batch, i;\n\n\t \n\tif (desc_available <= (MAX_SKB_FRAGS + 1))\n\t\treturn;\n\tdesc_available -= MAX_SKB_FRAGS + 1;\n\n\tbatch = xsk_tx_peek_release_desc_batch(tx->xsk_pool, desc_available);\n\tfor (i = 0; i < batch; i++)\n\t\ttsnep_xdp_xmit_frame_ring_zc(&descs[i], tx);\n\n\tif (batch) {\n\t\t \n\t\tdma_wmb();\n\n\t\ttsnep_xdp_xmit_flush(tx);\n\t}\n}\n\nstatic bool tsnep_tx_poll(struct tsnep_tx *tx, int napi_budget)\n{\n\tstruct tsnep_tx_entry *entry;\n\tstruct netdev_queue *nq;\n\tint xsk_frames = 0;\n\tint budget = 128;\n\tint length;\n\tint count;\n\n\tnq = netdev_get_tx_queue(tx->adapter->netdev, tx->queue_index);\n\t__netif_tx_lock(nq, smp_processor_id());\n\n\tdo {\n\t\tif (tx->read == tx->write)\n\t\t\tbreak;\n\n\t\tentry = &tx->entry[tx->read];\n\t\tif ((__le32_to_cpu(entry->desc_wb->properties) &\n\t\t     TSNEP_TX_DESC_OWNER_MASK) !=\n\t\t    (entry->properties & TSNEP_TX_DESC_OWNER_MASK))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tcount = 1;\n\t\tif ((entry->type & TSNEP_TX_TYPE_SKB) &&\n\t\t    skb_shinfo(entry->skb)->nr_frags > 0)\n\t\t\tcount += skb_shinfo(entry->skb)->nr_frags;\n\t\telse if ((entry->type & TSNEP_TX_TYPE_XDP) &&\n\t\t\t xdp_frame_has_frags(entry->xdpf))\n\t\t\tcount += xdp_get_shared_info_from_frame(entry->xdpf)->nr_frags;\n\n\t\tlength = tsnep_tx_unmap(tx, tx->read, count);\n\n\t\tif ((entry->type & TSNEP_TX_TYPE_SKB) &&\n\t\t    (skb_shinfo(entry->skb)->tx_flags & SKBTX_IN_PROGRESS) &&\n\t\t    (__le32_to_cpu(entry->desc_wb->properties) &\n\t\t     TSNEP_DESC_EXTENDED_WRITEBACK_FLAG)) {\n\t\t\tstruct skb_shared_hwtstamps hwtstamps;\n\t\t\tu64 timestamp;\n\n\t\t\tif (skb_shinfo(entry->skb)->tx_flags &\n\t\t\t    SKBTX_HW_TSTAMP_USE_CYCLES)\n\t\t\t\ttimestamp =\n\t\t\t\t\t__le64_to_cpu(entry->desc_wb->counter);\n\t\t\telse\n\t\t\t\ttimestamp =\n\t\t\t\t\t__le64_to_cpu(entry->desc_wb->timestamp);\n\n\t\t\tmemset(&hwtstamps, 0, sizeof(hwtstamps));\n\t\t\thwtstamps.hwtstamp = ns_to_ktime(timestamp);\n\n\t\t\tskb_tstamp_tx(entry->skb, &hwtstamps);\n\t\t}\n\n\t\tif (entry->type & TSNEP_TX_TYPE_SKB)\n\t\t\tnapi_consume_skb(entry->skb, napi_budget);\n\t\telse if (entry->type & TSNEP_TX_TYPE_XDP)\n\t\t\txdp_return_frame_rx_napi(entry->xdpf);\n\t\telse\n\t\t\txsk_frames++;\n\t\t \n\t\tentry->skb = NULL;\n\n\t\ttx->read = (tx->read + count) & TSNEP_RING_MASK;\n\n\t\ttx->packets++;\n\t\ttx->bytes += length + ETH_FCS_LEN;\n\n\t\tbudget--;\n\t} while (likely(budget));\n\n\tif (tx->xsk_pool) {\n\t\tif (xsk_frames)\n\t\t\txsk_tx_completed(tx->xsk_pool, xsk_frames);\n\t\tif (xsk_uses_need_wakeup(tx->xsk_pool))\n\t\t\txsk_set_tx_need_wakeup(tx->xsk_pool);\n\t\ttsnep_xdp_xmit_zc(tx);\n\t}\n\n\tif ((tsnep_tx_desc_available(tx) >= ((MAX_SKB_FRAGS + 1) * 2)) &&\n\t    netif_tx_queue_stopped(nq)) {\n\t\tnetif_tx_wake_queue(nq);\n\t}\n\n\t__netif_tx_unlock(nq);\n\n\treturn budget != 0;\n}\n\nstatic bool tsnep_tx_pending(struct tsnep_tx *tx)\n{\n\tstruct tsnep_tx_entry *entry;\n\tstruct netdev_queue *nq;\n\tbool pending = false;\n\n\tnq = netdev_get_tx_queue(tx->adapter->netdev, tx->queue_index);\n\t__netif_tx_lock(nq, smp_processor_id());\n\n\tif (tx->read != tx->write) {\n\t\tentry = &tx->entry[tx->read];\n\t\tif ((__le32_to_cpu(entry->desc_wb->properties) &\n\t\t     TSNEP_TX_DESC_OWNER_MASK) ==\n\t\t    (entry->properties & TSNEP_TX_DESC_OWNER_MASK))\n\t\t\tpending = true;\n\t}\n\n\t__netif_tx_unlock(nq);\n\n\treturn pending;\n}\n\nstatic int tsnep_tx_open(struct tsnep_tx *tx)\n{\n\tint retval;\n\n\tretval = tsnep_tx_ring_create(tx);\n\tif (retval)\n\t\treturn retval;\n\n\ttsnep_tx_init(tx);\n\n\treturn 0;\n}\n\nstatic void tsnep_tx_close(struct tsnep_tx *tx)\n{\n\ttsnep_tx_ring_cleanup(tx);\n}\n\nstatic void tsnep_rx_ring_cleanup(struct tsnep_rx *rx)\n{\n\tstruct device *dmadev = rx->adapter->dmadev;\n\tstruct tsnep_rx_entry *entry;\n\tint i;\n\n\tfor (i = 0; i < TSNEP_RING_SIZE; i++) {\n\t\tentry = &rx->entry[i];\n\t\tif (!rx->xsk_pool && entry->page)\n\t\t\tpage_pool_put_full_page(rx->page_pool, entry->page,\n\t\t\t\t\t\tfalse);\n\t\tif (rx->xsk_pool && entry->xdp)\n\t\t\txsk_buff_free(entry->xdp);\n\t\t \n\t\tentry->page = NULL;\n\t}\n\n\tif (rx->page_pool)\n\t\tpage_pool_destroy(rx->page_pool);\n\n\tmemset(rx->entry, 0, sizeof(rx->entry));\n\n\tfor (i = 0; i < TSNEP_RING_PAGE_COUNT; i++) {\n\t\tif (rx->page[i]) {\n\t\t\tdma_free_coherent(dmadev, PAGE_SIZE, rx->page[i],\n\t\t\t\t\t  rx->page_dma[i]);\n\t\t\trx->page[i] = NULL;\n\t\t\trx->page_dma[i] = 0;\n\t\t}\n\t}\n}\n\nstatic int tsnep_rx_ring_create(struct tsnep_rx *rx)\n{\n\tstruct device *dmadev = rx->adapter->dmadev;\n\tstruct tsnep_rx_entry *entry;\n\tstruct page_pool_params pp_params = { 0 };\n\tstruct tsnep_rx_entry *next_entry;\n\tint i, j;\n\tint retval;\n\n\tfor (i = 0; i < TSNEP_RING_PAGE_COUNT; i++) {\n\t\trx->page[i] =\n\t\t\tdma_alloc_coherent(dmadev, PAGE_SIZE, &rx->page_dma[i],\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!rx->page[i]) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto failed;\n\t\t}\n\t\tfor (j = 0; j < TSNEP_RING_ENTRIES_PER_PAGE; j++) {\n\t\t\tentry = &rx->entry[TSNEP_RING_ENTRIES_PER_PAGE * i + j];\n\t\t\tentry->desc_wb = (struct tsnep_rx_desc_wb *)\n\t\t\t\t(((u8 *)rx->page[i]) + TSNEP_DESC_SIZE * j);\n\t\t\tentry->desc = (struct tsnep_rx_desc *)\n\t\t\t\t(((u8 *)entry->desc_wb) + TSNEP_DESC_OFFSET);\n\t\t\tentry->desc_dma = rx->page_dma[i] + TSNEP_DESC_SIZE * j;\n\t\t}\n\t}\n\n\tpp_params.flags = PP_FLAG_DMA_MAP | PP_FLAG_DMA_SYNC_DEV;\n\tpp_params.order = 0;\n\tpp_params.pool_size = TSNEP_RING_SIZE;\n\tpp_params.nid = dev_to_node(dmadev);\n\tpp_params.dev = dmadev;\n\tpp_params.dma_dir = DMA_BIDIRECTIONAL;\n\tpp_params.max_len = TSNEP_MAX_RX_BUF_SIZE;\n\tpp_params.offset = TSNEP_RX_OFFSET;\n\trx->page_pool = page_pool_create(&pp_params);\n\tif (IS_ERR(rx->page_pool)) {\n\t\tretval = PTR_ERR(rx->page_pool);\n\t\trx->page_pool = NULL;\n\t\tgoto failed;\n\t}\n\n\tfor (i = 0; i < TSNEP_RING_SIZE; i++) {\n\t\tentry = &rx->entry[i];\n\t\tnext_entry = &rx->entry[(i + 1) & TSNEP_RING_MASK];\n\t\tentry->desc->next = __cpu_to_le64(next_entry->desc_dma);\n\t}\n\n\treturn 0;\n\nfailed:\n\ttsnep_rx_ring_cleanup(rx);\n\treturn retval;\n}\n\nstatic void tsnep_rx_init(struct tsnep_rx *rx)\n{\n\tdma_addr_t dma;\n\n\tdma = rx->entry[0].desc_dma | TSNEP_RESET_OWNER_COUNTER;\n\tiowrite32(DMA_ADDR_LOW(dma), rx->addr + TSNEP_RX_DESC_ADDR_LOW);\n\tiowrite32(DMA_ADDR_HIGH(dma), rx->addr + TSNEP_RX_DESC_ADDR_HIGH);\n\trx->write = 0;\n\trx->read = 0;\n\trx->owner_counter = 1;\n\trx->increment_owner_counter = TSNEP_RING_SIZE - 1;\n}\n\nstatic void tsnep_rx_enable(struct tsnep_rx *rx)\n{\n\t \n\tdma_wmb();\n\n\tiowrite32(TSNEP_CONTROL_RX_ENABLE, rx->addr + TSNEP_CONTROL);\n}\n\nstatic void tsnep_rx_disable(struct tsnep_rx *rx)\n{\n\tu32 val;\n\n\tiowrite32(TSNEP_CONTROL_RX_DISABLE, rx->addr + TSNEP_CONTROL);\n\treadx_poll_timeout(ioread32, rx->addr + TSNEP_CONTROL, val,\n\t\t\t   ((val & TSNEP_CONTROL_RX_ENABLE) == 0), 10000,\n\t\t\t   1000000);\n}\n\nstatic int tsnep_rx_desc_available(struct tsnep_rx *rx)\n{\n\tif (rx->read <= rx->write)\n\t\treturn TSNEP_RING_SIZE - rx->write + rx->read - 1;\n\telse\n\t\treturn rx->read - rx->write - 1;\n}\n\nstatic void tsnep_rx_free_page_buffer(struct tsnep_rx *rx)\n{\n\tstruct page **page;\n\n\t \n\tpage = rx->page_buffer;\n\twhile (*page) {\n\t\tpage_pool_put_full_page(rx->page_pool, *page, false);\n\t\t*page = NULL;\n\t\tpage++;\n\t}\n}\n\nstatic int tsnep_rx_alloc_page_buffer(struct tsnep_rx *rx)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < TSNEP_RING_SIZE - 1; i++) {\n\t\trx->page_buffer[i] = page_pool_dev_alloc_pages(rx->page_pool);\n\t\tif (!rx->page_buffer[i]) {\n\t\t\ttsnep_rx_free_page_buffer(rx);\n\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void tsnep_rx_set_page(struct tsnep_rx *rx, struct tsnep_rx_entry *entry,\n\t\t\t      struct page *page)\n{\n\tentry->page = page;\n\tentry->len = TSNEP_MAX_RX_BUF_SIZE;\n\tentry->dma = page_pool_get_dma_addr(entry->page);\n\tentry->desc->rx = __cpu_to_le64(entry->dma + TSNEP_RX_OFFSET);\n}\n\nstatic int tsnep_rx_alloc_buffer(struct tsnep_rx *rx, int index)\n{\n\tstruct tsnep_rx_entry *entry = &rx->entry[index];\n\tstruct page *page;\n\n\tpage = page_pool_dev_alloc_pages(rx->page_pool);\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\ttsnep_rx_set_page(rx, entry, page);\n\n\treturn 0;\n}\n\nstatic void tsnep_rx_reuse_buffer(struct tsnep_rx *rx, int index)\n{\n\tstruct tsnep_rx_entry *entry = &rx->entry[index];\n\tstruct tsnep_rx_entry *read = &rx->entry[rx->read];\n\n\ttsnep_rx_set_page(rx, entry, read->page);\n\tread->page = NULL;\n}\n\nstatic void tsnep_rx_activate(struct tsnep_rx *rx, int index)\n{\n\tstruct tsnep_rx_entry *entry = &rx->entry[index];\n\n\t \n\tentry->properties = entry->len & TSNEP_DESC_LENGTH_MASK;\n\tentry->properties |= TSNEP_DESC_INTERRUPT_FLAG;\n\tif (index == rx->increment_owner_counter) {\n\t\trx->owner_counter++;\n\t\tif (rx->owner_counter == 4)\n\t\t\trx->owner_counter = 1;\n\t\trx->increment_owner_counter--;\n\t\tif (rx->increment_owner_counter < 0)\n\t\t\trx->increment_owner_counter = TSNEP_RING_SIZE - 1;\n\t}\n\tentry->properties |=\n\t\t(rx->owner_counter << TSNEP_DESC_OWNER_COUNTER_SHIFT) &\n\t\tTSNEP_DESC_OWNER_COUNTER_MASK;\n\n\t \n\tdma_wmb();\n\n\tentry->desc->properties = __cpu_to_le32(entry->properties);\n}\n\nstatic int tsnep_rx_alloc(struct tsnep_rx *rx, int count, bool reuse)\n{\n\tbool alloc_failed = false;\n\tint i, index;\n\n\tfor (i = 0; i < count && !alloc_failed; i++) {\n\t\tindex = (rx->write + i) & TSNEP_RING_MASK;\n\n\t\tif (unlikely(tsnep_rx_alloc_buffer(rx, index))) {\n\t\t\trx->alloc_failed++;\n\t\t\talloc_failed = true;\n\n\t\t\t \n\t\t\tif (i == 0 && reuse)\n\t\t\t\ttsnep_rx_reuse_buffer(rx, index);\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\n\t\ttsnep_rx_activate(rx, index);\n\t}\n\n\tif (i)\n\t\trx->write = (rx->write + i) & TSNEP_RING_MASK;\n\n\treturn i;\n}\n\nstatic int tsnep_rx_refill(struct tsnep_rx *rx, int count, bool reuse)\n{\n\tint desc_refilled;\n\n\tdesc_refilled = tsnep_rx_alloc(rx, count, reuse);\n\tif (desc_refilled)\n\t\ttsnep_rx_enable(rx);\n\n\treturn desc_refilled;\n}\n\nstatic void tsnep_rx_set_xdp(struct tsnep_rx *rx, struct tsnep_rx_entry *entry,\n\t\t\t     struct xdp_buff *xdp)\n{\n\tentry->xdp = xdp;\n\tentry->len = TSNEP_XSK_RX_BUF_SIZE;\n\tentry->dma = xsk_buff_xdp_get_dma(entry->xdp);\n\tentry->desc->rx = __cpu_to_le64(entry->dma);\n}\n\nstatic void tsnep_rx_reuse_buffer_zc(struct tsnep_rx *rx, int index)\n{\n\tstruct tsnep_rx_entry *entry = &rx->entry[index];\n\tstruct tsnep_rx_entry *read = &rx->entry[rx->read];\n\n\ttsnep_rx_set_xdp(rx, entry, read->xdp);\n\tread->xdp = NULL;\n}\n\nstatic int tsnep_rx_alloc_zc(struct tsnep_rx *rx, int count, bool reuse)\n{\n\tu32 allocated;\n\tint i;\n\n\tallocated = xsk_buff_alloc_batch(rx->xsk_pool, rx->xdp_batch, count);\n\tfor (i = 0; i < allocated; i++) {\n\t\tint index = (rx->write + i) & TSNEP_RING_MASK;\n\t\tstruct tsnep_rx_entry *entry = &rx->entry[index];\n\n\t\ttsnep_rx_set_xdp(rx, entry, rx->xdp_batch[i]);\n\t\ttsnep_rx_activate(rx, index);\n\t}\n\tif (i == 0) {\n\t\trx->alloc_failed++;\n\n\t\tif (reuse) {\n\t\t\ttsnep_rx_reuse_buffer_zc(rx, rx->write);\n\t\t\ttsnep_rx_activate(rx, rx->write);\n\t\t}\n\t}\n\n\tif (i)\n\t\trx->write = (rx->write + i) & TSNEP_RING_MASK;\n\n\treturn i;\n}\n\nstatic void tsnep_rx_free_zc(struct tsnep_rx *rx)\n{\n\tint i;\n\n\tfor (i = 0; i < TSNEP_RING_SIZE; i++) {\n\t\tstruct tsnep_rx_entry *entry = &rx->entry[i];\n\n\t\tif (entry->xdp)\n\t\t\txsk_buff_free(entry->xdp);\n\t\tentry->xdp = NULL;\n\t}\n}\n\nstatic int tsnep_rx_refill_zc(struct tsnep_rx *rx, int count, bool reuse)\n{\n\tint desc_refilled;\n\n\tdesc_refilled = tsnep_rx_alloc_zc(rx, count, reuse);\n\tif (desc_refilled)\n\t\ttsnep_rx_enable(rx);\n\n\treturn desc_refilled;\n}\n\nstatic bool tsnep_xdp_run_prog(struct tsnep_rx *rx, struct bpf_prog *prog,\n\t\t\t       struct xdp_buff *xdp, int *status,\n\t\t\t       struct netdev_queue *tx_nq, struct tsnep_tx *tx)\n{\n\tunsigned int length;\n\tunsigned int sync;\n\tu32 act;\n\n\tlength = xdp->data_end - xdp->data_hard_start - XDP_PACKET_HEADROOM;\n\n\tact = bpf_prog_run_xdp(prog, xdp);\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\treturn false;\n\tcase XDP_TX:\n\t\tif (!tsnep_xdp_xmit_back(rx->adapter, xdp, tx_nq, tx))\n\t\t\tgoto out_failure;\n\t\t*status |= TSNEP_XDP_TX;\n\t\treturn true;\n\tcase XDP_REDIRECT:\n\t\tif (xdp_do_redirect(rx->adapter->netdev, xdp, prog) < 0)\n\t\t\tgoto out_failure;\n\t\t*status |= TSNEP_XDP_REDIRECT;\n\t\treturn true;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rx->adapter->netdev, prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\nout_failure:\n\t\ttrace_xdp_exception(rx->adapter->netdev, prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\t \n\t\tsync = xdp->data_end - xdp->data_hard_start -\n\t\t       XDP_PACKET_HEADROOM;\n\t\tsync = max(sync, length);\n\t\tpage_pool_put_page(rx->page_pool, virt_to_head_page(xdp->data),\n\t\t\t\t   sync, true);\n\t\treturn true;\n\t}\n}\n\nstatic bool tsnep_xdp_run_prog_zc(struct tsnep_rx *rx, struct bpf_prog *prog,\n\t\t\t\t  struct xdp_buff *xdp, int *status,\n\t\t\t\t  struct netdev_queue *tx_nq,\n\t\t\t\t  struct tsnep_tx *tx)\n{\n\tu32 act;\n\n\tact = bpf_prog_run_xdp(prog, xdp);\n\n\t \n\tif (likely(act == XDP_REDIRECT)) {\n\t\tif (xdp_do_redirect(rx->adapter->netdev, xdp, prog) < 0)\n\t\t\tgoto out_failure;\n\t\t*status |= TSNEP_XDP_REDIRECT;\n\t\treturn true;\n\t}\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\treturn false;\n\tcase XDP_TX:\n\t\tif (!tsnep_xdp_xmit_back(rx->adapter, xdp, tx_nq, tx))\n\t\t\tgoto out_failure;\n\t\t*status |= TSNEP_XDP_TX;\n\t\treturn true;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rx->adapter->netdev, prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\nout_failure:\n\t\ttrace_xdp_exception(rx->adapter->netdev, prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\txsk_buff_free(xdp);\n\t\treturn true;\n\t}\n}\n\nstatic void tsnep_finalize_xdp(struct tsnep_adapter *adapter, int status,\n\t\t\t       struct netdev_queue *tx_nq, struct tsnep_tx *tx)\n{\n\tif (status & TSNEP_XDP_TX) {\n\t\t__netif_tx_lock(tx_nq, smp_processor_id());\n\t\ttsnep_xdp_xmit_flush(tx);\n\t\t__netif_tx_unlock(tx_nq);\n\t}\n\n\tif (status & TSNEP_XDP_REDIRECT)\n\t\txdp_do_flush();\n}\n\nstatic struct sk_buff *tsnep_build_skb(struct tsnep_rx *rx, struct page *page,\n\t\t\t\t       int length)\n{\n\tstruct sk_buff *skb;\n\n\tskb = napi_build_skb(page_address(page), PAGE_SIZE);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, TSNEP_RX_OFFSET + TSNEP_RX_INLINE_METADATA_SIZE);\n\t__skb_put(skb, length - ETH_FCS_LEN);\n\n\tif (rx->adapter->hwtstamp_config.rx_filter == HWTSTAMP_FILTER_ALL) {\n\t\tstruct skb_shared_hwtstamps *hwtstamps = skb_hwtstamps(skb);\n\t\tstruct tsnep_rx_inline *rx_inline =\n\t\t\t(struct tsnep_rx_inline *)(page_address(page) +\n\t\t\t\t\t\t   TSNEP_RX_OFFSET);\n\n\t\tskb_shinfo(skb)->tx_flags |=\n\t\t\tSKBTX_HW_TSTAMP_NETDEV;\n\t\tmemset(hwtstamps, 0, sizeof(*hwtstamps));\n\t\thwtstamps->netdev_data = rx_inline;\n\t}\n\n\tskb_record_rx_queue(skb, rx->queue_index);\n\tskb->protocol = eth_type_trans(skb, rx->adapter->netdev);\n\n\treturn skb;\n}\n\nstatic void tsnep_rx_page(struct tsnep_rx *rx, struct napi_struct *napi,\n\t\t\t  struct page *page, int length)\n{\n\tstruct sk_buff *skb;\n\n\tskb = tsnep_build_skb(rx, page, length);\n\tif (skb) {\n\t\tskb_mark_for_recycle(skb);\n\n\t\trx->packets++;\n\t\trx->bytes += length;\n\t\tif (skb->pkt_type == PACKET_MULTICAST)\n\t\t\trx->multicast++;\n\n\t\tnapi_gro_receive(napi, skb);\n\t} else {\n\t\tpage_pool_recycle_direct(rx->page_pool, page);\n\n\t\trx->dropped++;\n\t}\n}\n\nstatic int tsnep_rx_poll(struct tsnep_rx *rx, struct napi_struct *napi,\n\t\t\t int budget)\n{\n\tstruct device *dmadev = rx->adapter->dmadev;\n\tenum dma_data_direction dma_dir;\n\tstruct tsnep_rx_entry *entry;\n\tstruct netdev_queue *tx_nq;\n\tstruct bpf_prog *prog;\n\tstruct xdp_buff xdp;\n\tstruct tsnep_tx *tx;\n\tint desc_available;\n\tint xdp_status = 0;\n\tint done = 0;\n\tint length;\n\n\tdesc_available = tsnep_rx_desc_available(rx);\n\tdma_dir = page_pool_get_dma_dir(rx->page_pool);\n\tprog = READ_ONCE(rx->adapter->xdp_prog);\n\tif (prog) {\n\t\ttx_nq = netdev_get_tx_queue(rx->adapter->netdev,\n\t\t\t\t\t    rx->tx_queue_index);\n\t\ttx = &rx->adapter->tx[rx->tx_queue_index];\n\n\t\txdp_init_buff(&xdp, PAGE_SIZE, &rx->xdp_rxq);\n\t}\n\n\twhile (likely(done < budget) && (rx->read != rx->write)) {\n\t\tentry = &rx->entry[rx->read];\n\t\tif ((__le32_to_cpu(entry->desc_wb->properties) &\n\t\t     TSNEP_DESC_OWNER_COUNTER_MASK) !=\n\t\t    (entry->properties & TSNEP_DESC_OWNER_COUNTER_MASK))\n\t\t\tbreak;\n\t\tdone++;\n\n\t\tif (desc_available >= TSNEP_RING_RX_REFILL) {\n\t\t\tbool reuse = desc_available >= TSNEP_RING_RX_REUSE;\n\n\t\t\tdesc_available -= tsnep_rx_refill(rx, desc_available,\n\t\t\t\t\t\t\t  reuse);\n\t\t\tif (!entry->page) {\n\t\t\t\t \n\t\t\t\trx->read = (rx->read + 1) & TSNEP_RING_MASK;\n\t\t\t\tdesc_available++;\n\n\t\t\t\trx->dropped++;\n\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tdma_rmb();\n\n\t\tprefetch(page_address(entry->page) + TSNEP_RX_OFFSET);\n\t\tlength = __le32_to_cpu(entry->desc_wb->properties) &\n\t\t\t TSNEP_DESC_LENGTH_MASK;\n\t\tdma_sync_single_range_for_cpu(dmadev, entry->dma,\n\t\t\t\t\t      TSNEP_RX_OFFSET, length, dma_dir);\n\n\t\t \n\t\tlength -= TSNEP_RX_INLINE_METADATA_SIZE;\n\n\t\trx->read = (rx->read + 1) & TSNEP_RING_MASK;\n\t\tdesc_available++;\n\n\t\tif (prog) {\n\t\t\tbool consume;\n\n\t\t\txdp_prepare_buff(&xdp, page_address(entry->page),\n\t\t\t\t\t XDP_PACKET_HEADROOM + TSNEP_RX_INLINE_METADATA_SIZE,\n\t\t\t\t\t length, false);\n\n\t\t\tconsume = tsnep_xdp_run_prog(rx, prog, &xdp,\n\t\t\t\t\t\t     &xdp_status, tx_nq, tx);\n\t\t\tif (consume) {\n\t\t\t\trx->packets++;\n\t\t\t\trx->bytes += length;\n\n\t\t\t\tentry->page = NULL;\n\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\ttsnep_rx_page(rx, napi, entry->page, length);\n\t\tentry->page = NULL;\n\t}\n\n\tif (xdp_status)\n\t\ttsnep_finalize_xdp(rx->adapter, xdp_status, tx_nq, tx);\n\n\tif (desc_available)\n\t\ttsnep_rx_refill(rx, desc_available, false);\n\n\treturn done;\n}\n\nstatic int tsnep_rx_poll_zc(struct tsnep_rx *rx, struct napi_struct *napi,\n\t\t\t    int budget)\n{\n\tstruct tsnep_rx_entry *entry;\n\tstruct netdev_queue *tx_nq;\n\tstruct bpf_prog *prog;\n\tstruct tsnep_tx *tx;\n\tint desc_available;\n\tint xdp_status = 0;\n\tstruct page *page;\n\tint done = 0;\n\tint length;\n\n\tdesc_available = tsnep_rx_desc_available(rx);\n\tprog = READ_ONCE(rx->adapter->xdp_prog);\n\tif (prog) {\n\t\ttx_nq = netdev_get_tx_queue(rx->adapter->netdev,\n\t\t\t\t\t    rx->tx_queue_index);\n\t\ttx = &rx->adapter->tx[rx->tx_queue_index];\n\t}\n\n\twhile (likely(done < budget) && (rx->read != rx->write)) {\n\t\tentry = &rx->entry[rx->read];\n\t\tif ((__le32_to_cpu(entry->desc_wb->properties) &\n\t\t     TSNEP_DESC_OWNER_COUNTER_MASK) !=\n\t\t    (entry->properties & TSNEP_DESC_OWNER_COUNTER_MASK))\n\t\t\tbreak;\n\t\tdone++;\n\n\t\tif (desc_available >= TSNEP_RING_RX_REFILL) {\n\t\t\tbool reuse = desc_available >= TSNEP_RING_RX_REUSE;\n\n\t\t\tdesc_available -= tsnep_rx_refill_zc(rx, desc_available,\n\t\t\t\t\t\t\t     reuse);\n\t\t\tif (!entry->xdp) {\n\t\t\t\t \n\t\t\t\trx->read = (rx->read + 1) & TSNEP_RING_MASK;\n\t\t\t\tdesc_available++;\n\n\t\t\t\trx->dropped++;\n\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tdma_rmb();\n\n\t\tprefetch(entry->xdp->data);\n\t\tlength = __le32_to_cpu(entry->desc_wb->properties) &\n\t\t\t TSNEP_DESC_LENGTH_MASK;\n\t\txsk_buff_set_size(entry->xdp, length);\n\t\txsk_buff_dma_sync_for_cpu(entry->xdp, rx->xsk_pool);\n\n\t\t \n\t\tlength -= TSNEP_RX_INLINE_METADATA_SIZE;\n\n\t\trx->read = (rx->read + 1) & TSNEP_RING_MASK;\n\t\tdesc_available++;\n\n\t\tif (prog) {\n\t\t\tbool consume;\n\n\t\t\tentry->xdp->data += TSNEP_RX_INLINE_METADATA_SIZE;\n\t\t\tentry->xdp->data_meta += TSNEP_RX_INLINE_METADATA_SIZE;\n\n\t\t\tconsume = tsnep_xdp_run_prog_zc(rx, prog, entry->xdp,\n\t\t\t\t\t\t\t&xdp_status, tx_nq, tx);\n\t\t\tif (consume) {\n\t\t\t\trx->packets++;\n\t\t\t\trx->bytes += length;\n\n\t\t\t\tentry->xdp = NULL;\n\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tpage = page_pool_dev_alloc_pages(rx->page_pool);\n\t\tif (page) {\n\t\t\tmemcpy(page_address(page) + TSNEP_RX_OFFSET,\n\t\t\t       entry->xdp->data - TSNEP_RX_INLINE_METADATA_SIZE,\n\t\t\t       length + TSNEP_RX_INLINE_METADATA_SIZE);\n\t\t\ttsnep_rx_page(rx, napi, page, length);\n\t\t} else {\n\t\t\trx->dropped++;\n\t\t}\n\t\txsk_buff_free(entry->xdp);\n\t\tentry->xdp = NULL;\n\t}\n\n\tif (xdp_status)\n\t\ttsnep_finalize_xdp(rx->adapter, xdp_status, tx_nq, tx);\n\n\tif (desc_available)\n\t\tdesc_available -= tsnep_rx_refill_zc(rx, desc_available, false);\n\n\tif (xsk_uses_need_wakeup(rx->xsk_pool)) {\n\t\tif (desc_available)\n\t\t\txsk_set_rx_need_wakeup(rx->xsk_pool);\n\t\telse\n\t\t\txsk_clear_rx_need_wakeup(rx->xsk_pool);\n\n\t\treturn done;\n\t}\n\n\treturn desc_available ? budget : done;\n}\n\nstatic bool tsnep_rx_pending(struct tsnep_rx *rx)\n{\n\tstruct tsnep_rx_entry *entry;\n\n\tif (rx->read != rx->write) {\n\t\tentry = &rx->entry[rx->read];\n\t\tif ((__le32_to_cpu(entry->desc_wb->properties) &\n\t\t     TSNEP_DESC_OWNER_COUNTER_MASK) ==\n\t\t    (entry->properties & TSNEP_DESC_OWNER_COUNTER_MASK))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int tsnep_rx_open(struct tsnep_rx *rx)\n{\n\tint desc_available;\n\tint retval;\n\n\tretval = tsnep_rx_ring_create(rx);\n\tif (retval)\n\t\treturn retval;\n\n\ttsnep_rx_init(rx);\n\n\tdesc_available = tsnep_rx_desc_available(rx);\n\tif (rx->xsk_pool)\n\t\tretval = tsnep_rx_alloc_zc(rx, desc_available, false);\n\telse\n\t\tretval = tsnep_rx_alloc(rx, desc_available, false);\n\tif (retval != desc_available) {\n\t\tretval = -ENOMEM;\n\n\t\tgoto alloc_failed;\n\t}\n\n\t \n\tif (rx->xsk_pool) {\n\t\tretval = tsnep_rx_alloc_page_buffer(rx);\n\t\tif (retval)\n\t\t\tgoto alloc_failed;\n\t}\n\n\treturn 0;\n\nalloc_failed:\n\ttsnep_rx_ring_cleanup(rx);\n\treturn retval;\n}\n\nstatic void tsnep_rx_close(struct tsnep_rx *rx)\n{\n\tif (rx->xsk_pool)\n\t\ttsnep_rx_free_page_buffer(rx);\n\n\ttsnep_rx_ring_cleanup(rx);\n}\n\nstatic void tsnep_rx_reopen(struct tsnep_rx *rx)\n{\n\tstruct page **page = rx->page_buffer;\n\tint i;\n\n\ttsnep_rx_init(rx);\n\n\tfor (i = 0; i < TSNEP_RING_SIZE; i++) {\n\t\tstruct tsnep_rx_entry *entry = &rx->entry[i];\n\n\t\t \n\t\tentry->desc->properties = 0;\n\t\tentry->desc_wb->properties = 0;\n\n\t\t \n\t\tif (*page) {\n\t\t\ttsnep_rx_set_page(rx, entry, *page);\n\t\t\ttsnep_rx_activate(rx, rx->write);\n\t\t\trx->write++;\n\n\t\t\t*page = NULL;\n\t\t\tpage++;\n\t\t}\n\t}\n}\n\nstatic void tsnep_rx_reopen_xsk(struct tsnep_rx *rx)\n{\n\tstruct page **page = rx->page_buffer;\n\tu32 allocated;\n\tint i;\n\n\ttsnep_rx_init(rx);\n\n\t \n\tallocated = xsk_buff_alloc_batch(rx->xsk_pool, rx->xdp_batch,\n\t\t\t\t\t TSNEP_RING_SIZE - 1);\n\n\tfor (i = 0; i < TSNEP_RING_SIZE; i++) {\n\t\tstruct tsnep_rx_entry *entry = &rx->entry[i];\n\n\t\t \n\t\tif (entry->page) {\n\t\t\t*page = entry->page;\n\t\t\tentry->page = NULL;\n\n\t\t\tpage++;\n\t\t}\n\n\t\t \n\t\tentry->desc->properties = 0;\n\t\tentry->desc_wb->properties = 0;\n\n\t\tif (allocated) {\n\t\t\ttsnep_rx_set_xdp(rx, entry,\n\t\t\t\t\t rx->xdp_batch[allocated - 1]);\n\t\t\ttsnep_rx_activate(rx, rx->write);\n\t\t\trx->write++;\n\n\t\t\tallocated--;\n\t\t}\n\t}\n}\n\nstatic bool tsnep_pending(struct tsnep_queue *queue)\n{\n\tif (queue->tx && tsnep_tx_pending(queue->tx))\n\t\treturn true;\n\n\tif (queue->rx && tsnep_rx_pending(queue->rx))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int tsnep_poll(struct napi_struct *napi, int budget)\n{\n\tstruct tsnep_queue *queue = container_of(napi, struct tsnep_queue,\n\t\t\t\t\t\t napi);\n\tbool complete = true;\n\tint done = 0;\n\n\tif (queue->tx)\n\t\tcomplete = tsnep_tx_poll(queue->tx, budget);\n\n\t \n\tif (unlikely(budget <= 0))\n\t\treturn budget;\n\n\tif (queue->rx) {\n\t\tdone = queue->rx->xsk_pool ?\n\t\t       tsnep_rx_poll_zc(queue->rx, napi, budget) :\n\t\t       tsnep_rx_poll(queue->rx, napi, budget);\n\t\tif (done >= budget)\n\t\t\tcomplete = false;\n\t}\n\n\t \n\tif (!complete)\n\t\treturn budget;\n\n\tif (likely(napi_complete_done(napi, done))) {\n\t\ttsnep_enable_irq(queue->adapter, queue->irq_mask);\n\n\t\t \n\t\tif (tsnep_pending(queue)) {\n\t\t\ttsnep_disable_irq(queue->adapter, queue->irq_mask);\n\t\t\tnapi_schedule(napi);\n\t\t}\n\t}\n\n\treturn min(done, budget - 1);\n}\n\nstatic int tsnep_request_irq(struct tsnep_queue *queue, bool first)\n{\n\tconst char *name = netdev_name(queue->adapter->netdev);\n\tirq_handler_t handler;\n\tvoid *dev;\n\tint retval;\n\n\tif (first) {\n\t\tsprintf(queue->name, \"%s-mac\", name);\n\t\thandler = tsnep_irq;\n\t\tdev = queue->adapter;\n\t} else {\n\t\tif (queue->tx && queue->rx)\n\t\t\tsnprintf(queue->name, sizeof(queue->name), \"%s-txrx-%d\",\n\t\t\t\t name, queue->rx->queue_index);\n\t\telse if (queue->tx)\n\t\t\tsnprintf(queue->name, sizeof(queue->name), \"%s-tx-%d\",\n\t\t\t\t name, queue->tx->queue_index);\n\t\telse\n\t\t\tsnprintf(queue->name, sizeof(queue->name), \"%s-rx-%d\",\n\t\t\t\t name, queue->rx->queue_index);\n\t\thandler = tsnep_irq_txrx;\n\t\tdev = queue;\n\t}\n\n\tretval = request_irq(queue->irq, handler, 0, queue->name, dev);\n\tif (retval) {\n\t\t \n\t\tmemset(queue->name, 0, sizeof(queue->name));\n\t}\n\n\treturn retval;\n}\n\nstatic void tsnep_free_irq(struct tsnep_queue *queue, bool first)\n{\n\tvoid *dev;\n\n\tif (!strlen(queue->name))\n\t\treturn;\n\n\tif (first)\n\t\tdev = queue->adapter;\n\telse\n\t\tdev = queue;\n\n\tfree_irq(queue->irq, dev);\n\tmemset(queue->name, 0, sizeof(queue->name));\n}\n\nstatic void tsnep_queue_close(struct tsnep_queue *queue, bool first)\n{\n\tstruct tsnep_rx *rx = queue->rx;\n\n\ttsnep_free_irq(queue, first);\n\n\tif (rx) {\n\t\tif (xdp_rxq_info_is_reg(&rx->xdp_rxq))\n\t\t\txdp_rxq_info_unreg(&rx->xdp_rxq);\n\t\tif (xdp_rxq_info_is_reg(&rx->xdp_rxq_zc))\n\t\t\txdp_rxq_info_unreg(&rx->xdp_rxq_zc);\n\t}\n\n\tnetif_napi_del(&queue->napi);\n}\n\nstatic int tsnep_queue_open(struct tsnep_adapter *adapter,\n\t\t\t    struct tsnep_queue *queue, bool first)\n{\n\tstruct tsnep_rx *rx = queue->rx;\n\tstruct tsnep_tx *tx = queue->tx;\n\tint retval;\n\n\tnetif_napi_add(adapter->netdev, &queue->napi, tsnep_poll);\n\n\tif (rx) {\n\t\t \n\t\tif (tx)\n\t\t\trx->tx_queue_index = tx->queue_index;\n\t\telse if (rx->queue_index < adapter->num_tx_queues)\n\t\t\trx->tx_queue_index = rx->queue_index;\n\t\telse\n\t\t\trx->tx_queue_index = 0;\n\n\t\t \n\t\tretval = xdp_rxq_info_reg(&rx->xdp_rxq, adapter->netdev,\n\t\t\t\t\t  rx->queue_index, queue->napi.napi_id);\n\t\tif (retval)\n\t\t\tgoto failed;\n\t\tretval = xdp_rxq_info_reg_mem_model(&rx->xdp_rxq,\n\t\t\t\t\t\t    MEM_TYPE_PAGE_POOL,\n\t\t\t\t\t\t    rx->page_pool);\n\t\tif (retval)\n\t\t\tgoto failed;\n\t\tretval = xdp_rxq_info_reg(&rx->xdp_rxq_zc, adapter->netdev,\n\t\t\t\t\t  rx->queue_index, queue->napi.napi_id);\n\t\tif (retval)\n\t\t\tgoto failed;\n\t\tretval = xdp_rxq_info_reg_mem_model(&rx->xdp_rxq_zc,\n\t\t\t\t\t\t    MEM_TYPE_XSK_BUFF_POOL,\n\t\t\t\t\t\t    NULL);\n\t\tif (retval)\n\t\t\tgoto failed;\n\t\tif (rx->xsk_pool)\n\t\t\txsk_pool_set_rxq_info(rx->xsk_pool, &rx->xdp_rxq_zc);\n\t}\n\n\tretval = tsnep_request_irq(queue, first);\n\tif (retval) {\n\t\tnetif_err(adapter, drv, adapter->netdev,\n\t\t\t  \"can't get assigned irq %d.\\n\", queue->irq);\n\t\tgoto failed;\n\t}\n\n\treturn 0;\n\nfailed:\n\ttsnep_queue_close(queue, first);\n\n\treturn retval;\n}\n\nstatic void tsnep_queue_enable(struct tsnep_queue *queue)\n{\n\tnapi_enable(&queue->napi);\n\ttsnep_enable_irq(queue->adapter, queue->irq_mask);\n\n\tif (queue->tx)\n\t\ttsnep_tx_enable(queue->tx);\n\n\tif (queue->rx)\n\t\ttsnep_rx_enable(queue->rx);\n}\n\nstatic void tsnep_queue_disable(struct tsnep_queue *queue)\n{\n\tif (queue->tx)\n\t\ttsnep_tx_disable(queue->tx, &queue->napi);\n\n\tnapi_disable(&queue->napi);\n\ttsnep_disable_irq(queue->adapter, queue->irq_mask);\n\n\t \n\tif (queue->rx)\n\t\ttsnep_rx_disable(queue->rx);\n}\n\nstatic int tsnep_netdev_open(struct net_device *netdev)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(netdev);\n\tint i, retval;\n\n\tfor (i = 0; i < adapter->num_queues; i++) {\n\t\tif (adapter->queue[i].tx) {\n\t\t\tretval = tsnep_tx_open(adapter->queue[i].tx);\n\t\t\tif (retval)\n\t\t\t\tgoto failed;\n\t\t}\n\t\tif (adapter->queue[i].rx) {\n\t\t\tretval = tsnep_rx_open(adapter->queue[i].rx);\n\t\t\tif (retval)\n\t\t\t\tgoto failed;\n\t\t}\n\n\t\tretval = tsnep_queue_open(adapter, &adapter->queue[i], i == 0);\n\t\tif (retval)\n\t\t\tgoto failed;\n\t}\n\n\tretval = netif_set_real_num_tx_queues(adapter->netdev,\n\t\t\t\t\t      adapter->num_tx_queues);\n\tif (retval)\n\t\tgoto failed;\n\tretval = netif_set_real_num_rx_queues(adapter->netdev,\n\t\t\t\t\t      adapter->num_rx_queues);\n\tif (retval)\n\t\tgoto failed;\n\n\ttsnep_enable_irq(adapter, ECM_INT_LINK);\n\tretval = tsnep_phy_open(adapter);\n\tif (retval)\n\t\tgoto phy_failed;\n\n\tfor (i = 0; i < adapter->num_queues; i++)\n\t\ttsnep_queue_enable(&adapter->queue[i]);\n\n\treturn 0;\n\nphy_failed:\n\ttsnep_disable_irq(adapter, ECM_INT_LINK);\nfailed:\n\tfor (i = 0; i < adapter->num_queues; i++) {\n\t\ttsnep_queue_close(&adapter->queue[i], i == 0);\n\n\t\tif (adapter->queue[i].rx)\n\t\t\ttsnep_rx_close(adapter->queue[i].rx);\n\t\tif (adapter->queue[i].tx)\n\t\t\ttsnep_tx_close(adapter->queue[i].tx);\n\t}\n\treturn retval;\n}\n\nstatic int tsnep_netdev_close(struct net_device *netdev)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(netdev);\n\tint i;\n\n\ttsnep_disable_irq(adapter, ECM_INT_LINK);\n\ttsnep_phy_close(adapter);\n\n\tfor (i = 0; i < adapter->num_queues; i++) {\n\t\ttsnep_queue_disable(&adapter->queue[i]);\n\n\t\ttsnep_queue_close(&adapter->queue[i], i == 0);\n\n\t\tif (adapter->queue[i].rx)\n\t\t\ttsnep_rx_close(adapter->queue[i].rx);\n\t\tif (adapter->queue[i].tx)\n\t\t\ttsnep_tx_close(adapter->queue[i].tx);\n\t}\n\n\treturn 0;\n}\n\nint tsnep_enable_xsk(struct tsnep_queue *queue, struct xsk_buff_pool *pool)\n{\n\tbool running = netif_running(queue->adapter->netdev);\n\tu32 frame_size;\n\n\tframe_size = xsk_pool_get_rx_frame_size(pool);\n\tif (frame_size < TSNEP_XSK_RX_BUF_SIZE)\n\t\treturn -EOPNOTSUPP;\n\n\tqueue->rx->page_buffer = kcalloc(TSNEP_RING_SIZE,\n\t\t\t\t\t sizeof(*queue->rx->page_buffer),\n\t\t\t\t\t GFP_KERNEL);\n\tif (!queue->rx->page_buffer)\n\t\treturn -ENOMEM;\n\tqueue->rx->xdp_batch = kcalloc(TSNEP_RING_SIZE,\n\t\t\t\t       sizeof(*queue->rx->xdp_batch),\n\t\t\t\t       GFP_KERNEL);\n\tif (!queue->rx->xdp_batch) {\n\t\tkfree(queue->rx->page_buffer);\n\t\tqueue->rx->page_buffer = NULL;\n\n\t\treturn -ENOMEM;\n\t}\n\n\txsk_pool_set_rxq_info(pool, &queue->rx->xdp_rxq_zc);\n\n\tif (running)\n\t\ttsnep_queue_disable(queue);\n\n\tqueue->tx->xsk_pool = pool;\n\tqueue->rx->xsk_pool = pool;\n\n\tif (running) {\n\t\ttsnep_rx_reopen_xsk(queue->rx);\n\t\ttsnep_queue_enable(queue);\n\t}\n\n\treturn 0;\n}\n\nvoid tsnep_disable_xsk(struct tsnep_queue *queue)\n{\n\tbool running = netif_running(queue->adapter->netdev);\n\n\tif (running)\n\t\ttsnep_queue_disable(queue);\n\n\ttsnep_rx_free_zc(queue->rx);\n\n\tqueue->rx->xsk_pool = NULL;\n\tqueue->tx->xsk_pool = NULL;\n\n\tif (running) {\n\t\ttsnep_rx_reopen(queue->rx);\n\t\ttsnep_queue_enable(queue);\n\t}\n\n\tkfree(queue->rx->xdp_batch);\n\tqueue->rx->xdp_batch = NULL;\n\tkfree(queue->rx->page_buffer);\n\tqueue->rx->page_buffer = NULL;\n}\n\nstatic netdev_tx_t tsnep_netdev_xmit_frame(struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *netdev)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(netdev);\n\tu16 queue_mapping = skb_get_queue_mapping(skb);\n\n\tif (queue_mapping >= adapter->num_tx_queues)\n\t\tqueue_mapping = 0;\n\n\treturn tsnep_xmit_frame_ring(skb, &adapter->tx[queue_mapping]);\n}\n\nstatic int tsnep_netdev_ioctl(struct net_device *netdev, struct ifreq *ifr,\n\t\t\t      int cmd)\n{\n\tif (!netif_running(netdev))\n\t\treturn -EINVAL;\n\tif (cmd == SIOCSHWTSTAMP || cmd == SIOCGHWTSTAMP)\n\t\treturn tsnep_ptp_ioctl(netdev, ifr, cmd);\n\treturn phy_mii_ioctl(netdev->phydev, ifr, cmd);\n}\n\nstatic void tsnep_netdev_set_multicast(struct net_device *netdev)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(netdev);\n\n\tu16 rx_filter = 0;\n\n\t \n\tif (netdev->flags & IFF_PROMISC) {\n\t\trx_filter |= TSNEP_RX_FILTER_ACCEPT_ALL_MULTICASTS;\n\t\trx_filter |= TSNEP_RX_FILTER_ACCEPT_ALL_UNICASTS;\n\t} else if (!netdev_mc_empty(netdev) || (netdev->flags & IFF_ALLMULTI)) {\n\t\trx_filter |= TSNEP_RX_FILTER_ACCEPT_ALL_MULTICASTS;\n\t}\n\tiowrite16(rx_filter, adapter->addr + TSNEP_RX_FILTER);\n}\n\nstatic void tsnep_netdev_get_stats64(struct net_device *netdev,\n\t\t\t\t     struct rtnl_link_stats64 *stats)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(netdev);\n\tu32 reg;\n\tu32 val;\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstats->tx_packets += adapter->tx[i].packets;\n\t\tstats->tx_bytes += adapter->tx[i].bytes;\n\t\tstats->tx_dropped += adapter->tx[i].dropped;\n\t}\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstats->rx_packets += adapter->rx[i].packets;\n\t\tstats->rx_bytes += adapter->rx[i].bytes;\n\t\tstats->rx_dropped += adapter->rx[i].dropped;\n\t\tstats->multicast += adapter->rx[i].multicast;\n\n\t\treg = ioread32(adapter->addr + TSNEP_QUEUE(i) +\n\t\t\t       TSNEP_RX_STATISTIC);\n\t\tval = (reg & TSNEP_RX_STATISTIC_NO_DESC_MASK) >>\n\t\t      TSNEP_RX_STATISTIC_NO_DESC_SHIFT;\n\t\tstats->rx_dropped += val;\n\t\tval = (reg & TSNEP_RX_STATISTIC_BUFFER_TOO_SMALL_MASK) >>\n\t\t      TSNEP_RX_STATISTIC_BUFFER_TOO_SMALL_SHIFT;\n\t\tstats->rx_dropped += val;\n\t\tval = (reg & TSNEP_RX_STATISTIC_FIFO_OVERFLOW_MASK) >>\n\t\t      TSNEP_RX_STATISTIC_FIFO_OVERFLOW_SHIFT;\n\t\tstats->rx_errors += val;\n\t\tstats->rx_fifo_errors += val;\n\t\tval = (reg & TSNEP_RX_STATISTIC_INVALID_FRAME_MASK) >>\n\t\t      TSNEP_RX_STATISTIC_INVALID_FRAME_SHIFT;\n\t\tstats->rx_errors += val;\n\t\tstats->rx_frame_errors += val;\n\t}\n\n\treg = ioread32(adapter->addr + ECM_STAT);\n\tval = (reg & ECM_STAT_RX_ERR_MASK) >> ECM_STAT_RX_ERR_SHIFT;\n\tstats->rx_errors += val;\n\tval = (reg & ECM_STAT_INV_FRM_MASK) >> ECM_STAT_INV_FRM_SHIFT;\n\tstats->rx_errors += val;\n\tstats->rx_crc_errors += val;\n\tval = (reg & ECM_STAT_FWD_RX_ERR_MASK) >> ECM_STAT_FWD_RX_ERR_SHIFT;\n\tstats->rx_errors += val;\n}\n\nstatic void tsnep_mac_set_address(struct tsnep_adapter *adapter, u8 *addr)\n{\n\tiowrite32(*(u32 *)addr, adapter->addr + TSNEP_MAC_ADDRESS_LOW);\n\tiowrite16(*(u16 *)(addr + sizeof(u32)),\n\t\t  adapter->addr + TSNEP_MAC_ADDRESS_HIGH);\n\n\tether_addr_copy(adapter->mac_address, addr);\n\tnetif_info(adapter, drv, adapter->netdev, \"MAC address set to %pM\\n\",\n\t\t   addr);\n}\n\nstatic int tsnep_netdev_set_mac_address(struct net_device *netdev, void *addr)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(netdev);\n\tstruct sockaddr *sock_addr = addr;\n\tint retval;\n\n\tretval = eth_prepare_mac_addr_change(netdev, sock_addr);\n\tif (retval)\n\t\treturn retval;\n\teth_hw_addr_set(netdev, sock_addr->sa_data);\n\ttsnep_mac_set_address(adapter, sock_addr->sa_data);\n\n\treturn 0;\n}\n\nstatic int tsnep_netdev_set_features(struct net_device *netdev,\n\t\t\t\t     netdev_features_t features)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(netdev);\n\tnetdev_features_t changed = netdev->features ^ features;\n\tbool enable;\n\tint retval = 0;\n\n\tif (changed & NETIF_F_LOOPBACK) {\n\t\tenable = !!(features & NETIF_F_LOOPBACK);\n\t\tretval = tsnep_phy_loopback(adapter, enable);\n\t}\n\n\treturn retval;\n}\n\nstatic ktime_t tsnep_netdev_get_tstamp(struct net_device *netdev,\n\t\t\t\t       const struct skb_shared_hwtstamps *hwtstamps,\n\t\t\t\t       bool cycles)\n{\n\tstruct tsnep_rx_inline *rx_inline = hwtstamps->netdev_data;\n\tu64 timestamp;\n\n\tif (cycles)\n\t\ttimestamp = __le64_to_cpu(rx_inline->counter);\n\telse\n\t\ttimestamp = __le64_to_cpu(rx_inline->timestamp);\n\n\treturn ns_to_ktime(timestamp);\n}\n\nstatic int tsnep_netdev_bpf(struct net_device *dev, struct netdev_bpf *bpf)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(dev);\n\n\tswitch (bpf->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn tsnep_xdp_setup_prog(adapter, bpf->prog, bpf->extack);\n\tcase XDP_SETUP_XSK_POOL:\n\t\treturn tsnep_xdp_setup_pool(adapter, bpf->xsk.pool,\n\t\t\t\t\t    bpf->xsk.queue_id);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic struct tsnep_tx *tsnep_xdp_get_tx(struct tsnep_adapter *adapter, u32 cpu)\n{\n\tif (cpu >= TSNEP_MAX_QUEUES)\n\t\tcpu &= TSNEP_MAX_QUEUES - 1;\n\n\twhile (cpu >= adapter->num_tx_queues)\n\t\tcpu -= adapter->num_tx_queues;\n\n\treturn &adapter->tx[cpu];\n}\n\nstatic int tsnep_netdev_xdp_xmit(struct net_device *dev, int n,\n\t\t\t\t struct xdp_frame **xdp, u32 flags)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(dev);\n\tu32 cpu = smp_processor_id();\n\tstruct netdev_queue *nq;\n\tstruct tsnep_tx *tx;\n\tint nxmit;\n\tbool xmit;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\ttx = tsnep_xdp_get_tx(adapter, cpu);\n\tnq = netdev_get_tx_queue(adapter->netdev, tx->queue_index);\n\n\t__netif_tx_lock(nq, cpu);\n\n\tfor (nxmit = 0; nxmit < n; nxmit++) {\n\t\txmit = tsnep_xdp_xmit_frame_ring(xdp[nxmit], tx,\n\t\t\t\t\t\t TSNEP_TX_TYPE_XDP_NDO);\n\t\tif (!xmit)\n\t\t\tbreak;\n\n\t\t \n\t\ttxq_trans_cond_update(nq);\n\t}\n\n\tif (flags & XDP_XMIT_FLUSH)\n\t\ttsnep_xdp_xmit_flush(tx);\n\n\t__netif_tx_unlock(nq);\n\n\treturn nxmit;\n}\n\nstatic int tsnep_netdev_xsk_wakeup(struct net_device *dev, u32 queue_id,\n\t\t\t\t   u32 flags)\n{\n\tstruct tsnep_adapter *adapter = netdev_priv(dev);\n\tstruct tsnep_queue *queue;\n\n\tif (queue_id >= adapter->num_rx_queues ||\n\t    queue_id >= adapter->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tqueue = &adapter->queue[queue_id];\n\n\tif (!napi_if_scheduled_mark_missed(&queue->napi))\n\t\tnapi_schedule(&queue->napi);\n\n\treturn 0;\n}\n\nstatic const struct net_device_ops tsnep_netdev_ops = {\n\t.ndo_open = tsnep_netdev_open,\n\t.ndo_stop = tsnep_netdev_close,\n\t.ndo_start_xmit = tsnep_netdev_xmit_frame,\n\t.ndo_eth_ioctl = tsnep_netdev_ioctl,\n\t.ndo_set_rx_mode = tsnep_netdev_set_multicast,\n\t.ndo_get_stats64 = tsnep_netdev_get_stats64,\n\t.ndo_set_mac_address = tsnep_netdev_set_mac_address,\n\t.ndo_set_features = tsnep_netdev_set_features,\n\t.ndo_get_tstamp = tsnep_netdev_get_tstamp,\n\t.ndo_setup_tc = tsnep_tc_setup,\n\t.ndo_bpf = tsnep_netdev_bpf,\n\t.ndo_xdp_xmit = tsnep_netdev_xdp_xmit,\n\t.ndo_xsk_wakeup = tsnep_netdev_xsk_wakeup,\n};\n\nstatic int tsnep_mac_init(struct tsnep_adapter *adapter)\n{\n\tint retval;\n\n\t \n\tiowrite16(0, adapter->addr + TSNEP_RX_FILTER);\n\n\t \n\tretval = of_get_mac_address(adapter->pdev->dev.of_node,\n\t\t\t\t    adapter->mac_address);\n\tif (retval == -EPROBE_DEFER)\n\t\treturn retval;\n\tif (retval && !is_valid_ether_addr(adapter->mac_address)) {\n\t\t*(u32 *)adapter->mac_address =\n\t\t\tioread32(adapter->addr + TSNEP_MAC_ADDRESS_LOW);\n\t\t*(u16 *)(adapter->mac_address + sizeof(u32)) =\n\t\t\tioread16(adapter->addr + TSNEP_MAC_ADDRESS_HIGH);\n\t\tif (!is_valid_ether_addr(adapter->mac_address))\n\t\t\teth_random_addr(adapter->mac_address);\n\t}\n\n\ttsnep_mac_set_address(adapter, adapter->mac_address);\n\teth_hw_addr_set(adapter->netdev, adapter->mac_address);\n\n\treturn 0;\n}\n\nstatic int tsnep_mdio_init(struct tsnep_adapter *adapter)\n{\n\tstruct device_node *np = adapter->pdev->dev.of_node;\n\tint retval;\n\n\tif (np) {\n\t\tnp = of_get_child_by_name(np, \"mdio\");\n\t\tif (!np)\n\t\t\treturn 0;\n\n\t\tadapter->suppress_preamble =\n\t\t\tof_property_read_bool(np, \"suppress-preamble\");\n\t}\n\n\tadapter->mdiobus = devm_mdiobus_alloc(&adapter->pdev->dev);\n\tif (!adapter->mdiobus) {\n\t\tretval = -ENOMEM;\n\n\t\tgoto out;\n\t}\n\n\tadapter->mdiobus->priv = (void *)adapter;\n\tadapter->mdiobus->parent = &adapter->pdev->dev;\n\tadapter->mdiobus->read = tsnep_mdiobus_read;\n\tadapter->mdiobus->write = tsnep_mdiobus_write;\n\tadapter->mdiobus->name = TSNEP \"-mdiobus\";\n\tsnprintf(adapter->mdiobus->id, MII_BUS_ID_SIZE, \"%s\",\n\t\t adapter->pdev->name);\n\n\t \n\tadapter->mdiobus->phy_mask = 0x0000001;\n\n\tretval = of_mdiobus_register(adapter->mdiobus, np);\n\nout:\n\tof_node_put(np);\n\n\treturn retval;\n}\n\nstatic int tsnep_phy_init(struct tsnep_adapter *adapter)\n{\n\tstruct device_node *phy_node;\n\tint retval;\n\n\tretval = of_get_phy_mode(adapter->pdev->dev.of_node,\n\t\t\t\t &adapter->phy_mode);\n\tif (retval)\n\t\tadapter->phy_mode = PHY_INTERFACE_MODE_GMII;\n\n\tphy_node = of_parse_phandle(adapter->pdev->dev.of_node, \"phy-handle\",\n\t\t\t\t    0);\n\tadapter->phydev = of_phy_find_device(phy_node);\n\tof_node_put(phy_node);\n\tif (!adapter->phydev && adapter->mdiobus)\n\t\tadapter->phydev = phy_find_first(adapter->mdiobus);\n\tif (!adapter->phydev)\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\nstatic int tsnep_queue_init(struct tsnep_adapter *adapter, int queue_count)\n{\n\tu32 irq_mask = ECM_INT_TX_0 | ECM_INT_RX_0;\n\tchar name[8];\n\tint i;\n\tint retval;\n\n\t \n\tif (platform_irq_count(adapter->pdev) == 1)\n\t\tretval = platform_get_irq(adapter->pdev, 0);\n\telse\n\t\tretval = platform_get_irq_byname(adapter->pdev, \"mac\");\n\tif (retval < 0)\n\t\treturn retval;\n\tadapter->num_tx_queues = 1;\n\tadapter->num_rx_queues = 1;\n\tadapter->num_queues = 1;\n\tadapter->queue[0].adapter = adapter;\n\tadapter->queue[0].irq = retval;\n\tadapter->queue[0].tx = &adapter->tx[0];\n\tadapter->queue[0].tx->adapter = adapter;\n\tadapter->queue[0].tx->addr = adapter->addr + TSNEP_QUEUE(0);\n\tadapter->queue[0].tx->queue_index = 0;\n\tadapter->queue[0].rx = &adapter->rx[0];\n\tadapter->queue[0].rx->adapter = adapter;\n\tadapter->queue[0].rx->addr = adapter->addr + TSNEP_QUEUE(0);\n\tadapter->queue[0].rx->queue_index = 0;\n\tadapter->queue[0].irq_mask = irq_mask;\n\tadapter->queue[0].irq_delay_addr = adapter->addr + ECM_INT_DELAY;\n\tretval = tsnep_set_irq_coalesce(&adapter->queue[0],\n\t\t\t\t\tTSNEP_COALESCE_USECS_DEFAULT);\n\tif (retval < 0)\n\t\treturn retval;\n\n\tadapter->netdev->irq = adapter->queue[0].irq;\n\n\t \n\tfor (i = 1; i < queue_count; i++) {\n\t\tsprintf(name, \"txrx-%d\", i);\n\t\tretval = platform_get_irq_byname_optional(adapter->pdev, name);\n\t\tif (retval < 0)\n\t\t\tbreak;\n\n\t\tadapter->num_tx_queues++;\n\t\tadapter->num_rx_queues++;\n\t\tadapter->num_queues++;\n\t\tadapter->queue[i].adapter = adapter;\n\t\tadapter->queue[i].irq = retval;\n\t\tadapter->queue[i].tx = &adapter->tx[i];\n\t\tadapter->queue[i].tx->adapter = adapter;\n\t\tadapter->queue[i].tx->addr = adapter->addr + TSNEP_QUEUE(i);\n\t\tadapter->queue[i].tx->queue_index = i;\n\t\tadapter->queue[i].rx = &adapter->rx[i];\n\t\tadapter->queue[i].rx->adapter = adapter;\n\t\tadapter->queue[i].rx->addr = adapter->addr + TSNEP_QUEUE(i);\n\t\tadapter->queue[i].rx->queue_index = i;\n\t\tadapter->queue[i].irq_mask =\n\t\t\tirq_mask << (ECM_INT_TXRX_SHIFT * i);\n\t\tadapter->queue[i].irq_delay_addr =\n\t\t\tadapter->addr + ECM_INT_DELAY + ECM_INT_DELAY_OFFSET * i;\n\t\tretval = tsnep_set_irq_coalesce(&adapter->queue[i],\n\t\t\t\t\t\tTSNEP_COALESCE_USECS_DEFAULT);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\t}\n\n\treturn 0;\n}\n\nstatic int tsnep_probe(struct platform_device *pdev)\n{\n\tstruct tsnep_adapter *adapter;\n\tstruct net_device *netdev;\n\tstruct resource *io;\n\tu32 type;\n\tint revision;\n\tint version;\n\tint queue_count;\n\tint retval;\n\n\tnetdev = devm_alloc_etherdev_mqs(&pdev->dev,\n\t\t\t\t\t sizeof(struct tsnep_adapter),\n\t\t\t\t\t TSNEP_MAX_QUEUES, TSNEP_MAX_QUEUES);\n\tif (!netdev)\n\t\treturn -ENODEV;\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\tadapter = netdev_priv(netdev);\n\tplatform_set_drvdata(pdev, adapter);\n\tadapter->pdev = pdev;\n\tadapter->dmadev = &pdev->dev;\n\tadapter->netdev = netdev;\n\tadapter->msg_enable = NETIF_MSG_DRV | NETIF_MSG_PROBE |\n\t\t\t      NETIF_MSG_LINK | NETIF_MSG_IFUP |\n\t\t\t      NETIF_MSG_IFDOWN | NETIF_MSG_TX_QUEUED;\n\n\tnetdev->min_mtu = ETH_MIN_MTU;\n\tnetdev->max_mtu = TSNEP_MAX_FRAME_SIZE;\n\n\tmutex_init(&adapter->gate_control_lock);\n\tmutex_init(&adapter->rxnfc_lock);\n\tINIT_LIST_HEAD(&adapter->rxnfc_rules);\n\n\tio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\n\tadapter->addr = devm_ioremap_resource(&pdev->dev, io);\n\tif (IS_ERR(adapter->addr))\n\t\treturn PTR_ERR(adapter->addr);\n\tnetdev->mem_start = io->start;\n\tnetdev->mem_end = io->end;\n\n\ttype = ioread32(adapter->addr + ECM_TYPE);\n\trevision = (type & ECM_REVISION_MASK) >> ECM_REVISION_SHIFT;\n\tversion = (type & ECM_VERSION_MASK) >> ECM_VERSION_SHIFT;\n\tqueue_count = (type & ECM_QUEUE_COUNT_MASK) >> ECM_QUEUE_COUNT_SHIFT;\n\tadapter->gate_control = type & ECM_GATE_CONTROL;\n\tadapter->rxnfc_max = TSNEP_RX_ASSIGN_ETHER_TYPE_COUNT;\n\n\ttsnep_disable_irq(adapter, ECM_INT_ALL);\n\n\tretval = tsnep_queue_init(adapter, queue_count);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = dma_set_mask_and_coherent(&adapter->pdev->dev,\n\t\t\t\t\t   DMA_BIT_MASK(64));\n\tif (retval) {\n\t\tdev_err(&adapter->pdev->dev, \"no usable DMA configuration.\\n\");\n\t\treturn retval;\n\t}\n\n\tretval = tsnep_mac_init(adapter);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = tsnep_mdio_init(adapter);\n\tif (retval)\n\t\tgoto mdio_init_failed;\n\n\tretval = tsnep_phy_init(adapter);\n\tif (retval)\n\t\tgoto phy_init_failed;\n\n\tretval = tsnep_ptp_init(adapter);\n\tif (retval)\n\t\tgoto ptp_init_failed;\n\n\tretval = tsnep_tc_init(adapter);\n\tif (retval)\n\t\tgoto tc_init_failed;\n\n\tretval = tsnep_rxnfc_init(adapter);\n\tif (retval)\n\t\tgoto rxnfc_init_failed;\n\n\tnetdev->netdev_ops = &tsnep_netdev_ops;\n\tnetdev->ethtool_ops = &tsnep_ethtool_ops;\n\tnetdev->features = NETIF_F_SG;\n\tnetdev->hw_features = netdev->features | NETIF_F_LOOPBACK;\n\n\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |\n\t\t\t       NETDEV_XDP_ACT_NDO_XMIT |\n\t\t\t       NETDEV_XDP_ACT_NDO_XMIT_SG |\n\t\t\t       NETDEV_XDP_ACT_XSK_ZEROCOPY;\n\n\t \n\tnetif_carrier_off(netdev);\n\n\tretval = register_netdev(netdev);\n\tif (retval)\n\t\tgoto register_failed;\n\n\tdev_info(&adapter->pdev->dev, \"device version %d.%02d\\n\", version,\n\t\t revision);\n\tif (adapter->gate_control)\n\t\tdev_info(&adapter->pdev->dev, \"gate control detected\\n\");\n\n\treturn 0;\n\nregister_failed:\n\ttsnep_rxnfc_cleanup(adapter);\nrxnfc_init_failed:\n\ttsnep_tc_cleanup(adapter);\ntc_init_failed:\n\ttsnep_ptp_cleanup(adapter);\nptp_init_failed:\nphy_init_failed:\n\tif (adapter->mdiobus)\n\t\tmdiobus_unregister(adapter->mdiobus);\nmdio_init_failed:\n\treturn retval;\n}\n\nstatic int tsnep_remove(struct platform_device *pdev)\n{\n\tstruct tsnep_adapter *adapter = platform_get_drvdata(pdev);\n\n\tunregister_netdev(adapter->netdev);\n\n\ttsnep_rxnfc_cleanup(adapter);\n\n\ttsnep_tc_cleanup(adapter);\n\n\ttsnep_ptp_cleanup(adapter);\n\n\tif (adapter->mdiobus)\n\t\tmdiobus_unregister(adapter->mdiobus);\n\n\ttsnep_disable_irq(adapter, ECM_INT_ALL);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id tsnep_of_match[] = {\n\t{ .compatible = \"engleder,tsnep\", },\n{ },\n};\nMODULE_DEVICE_TABLE(of, tsnep_of_match);\n\nstatic struct platform_driver tsnep_driver = {\n\t.driver = {\n\t\t.name = TSNEP,\n\t\t.of_match_table = tsnep_of_match,\n\t},\n\t.probe = tsnep_probe,\n\t.remove = tsnep_remove,\n};\nmodule_platform_driver(tsnep_driver);\n\nMODULE_AUTHOR(\"Gerhard Engleder <gerhard@engleder-embedded.com>\");\nMODULE_DESCRIPTION(\"TSN endpoint Ethernet MAC driver\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}