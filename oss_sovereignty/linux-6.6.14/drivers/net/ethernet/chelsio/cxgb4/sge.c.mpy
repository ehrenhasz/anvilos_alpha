{
  "module_name": "sge.c",
  "hash_id": "9133ef85294055d59b5052c3cf233967b5153f815e8da7adb7c4202ab453ae1c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/cxgb4/sge.c",
  "human_readable_source": " \n\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <linux/dma-mapping.h>\n#include <linux/jiffies.h>\n#include <linux/prefetch.h>\n#include <linux/export.h>\n#include <net/xfrm.h>\n#include <net/ipv6.h>\n#include <net/tcp.h>\n#include <net/busy_poll.h>\n#ifdef CONFIG_CHELSIO_T4_FCOE\n#include <scsi/fc/fc_fcoe.h>\n#endif  \n#include \"cxgb4.h\"\n#include \"t4_regs.h\"\n#include \"t4_values.h\"\n#include \"t4_msg.h\"\n#include \"t4fw_api.h\"\n#include \"cxgb4_ptp.h\"\n#include \"cxgb4_uld.h\"\n#include \"cxgb4_tc_mqprio.h\"\n#include \"sched.h\"\n\n \n#if PAGE_SHIFT >= 16\n# define FL_PG_ORDER 0\n#else\n# define FL_PG_ORDER (16 - PAGE_SHIFT)\n#endif\n\n \n#define RX_COPY_THRES    256\n#define RX_PULL_LEN      128\n\n \n#define RX_PKT_SKB_LEN   512\n\n \n#define MAX_TX_RECLAIM 32\n\n \n#define MAX_RX_REFILL 16U\n\n \n#define RX_QCHECK_PERIOD (HZ / 2)\n\n \n#define TX_QCHECK_PERIOD (HZ / 2)\n\n \n#define MAX_TIMER_TX_RECLAIM 100\n\n \n#define NOMEM_TMR_IDX (SGE_NTIMERS - 1)\n\n \n#define TXQ_STOP_THRES (SGE_MAX_WR_LEN / sizeof(struct tx_desc))\n\n \n#define MAX_IMM_TX_PKT_LEN 256\n\n \n#define MAX_CTRL_WR_LEN SGE_MAX_WR_LEN\n\nstruct rx_sw_desc {                 \n\tstruct page *page;\n\tdma_addr_t dma_addr;\n};\n\n \n#define FL_MTU_SMALL 1500\n#define FL_MTU_LARGE 9000\n\nstatic inline unsigned int fl_mtu_bufsize(struct adapter *adapter,\n\t\t\t\t\t  unsigned int mtu)\n{\n\tstruct sge *s = &adapter->sge;\n\n\treturn ALIGN(s->pktshift + ETH_HLEN + VLAN_HLEN + mtu, s->fl_align);\n}\n\n#define FL_MTU_SMALL_BUFSIZE(adapter) fl_mtu_bufsize(adapter, FL_MTU_SMALL)\n#define FL_MTU_LARGE_BUFSIZE(adapter) fl_mtu_bufsize(adapter, FL_MTU_LARGE)\n\n \nenum {\n\tRX_BUF_FLAGS     = 0x1f,    \n\tRX_BUF_SIZE      = 0x0f,    \n\tRX_UNMAPPED_BUF  = 0x10,    \n\n\t \n\tRX_SMALL_PG_BUF  = 0x0,    \n\tRX_LARGE_PG_BUF  = 0x1,    \n\n\tRX_SMALL_MTU_BUF = 0x2,    \n\tRX_LARGE_MTU_BUF = 0x3,    \n};\n\nstatic int timer_pkt_quota[] = {1, 1, 2, 3, 4, 5};\n#define MIN_NAPI_WORK  1\n\nstatic inline dma_addr_t get_buf_addr(const struct rx_sw_desc *d)\n{\n\treturn d->dma_addr & ~(dma_addr_t)RX_BUF_FLAGS;\n}\n\nstatic inline bool is_buf_mapped(const struct rx_sw_desc *d)\n{\n\treturn !(d->dma_addr & RX_UNMAPPED_BUF);\n}\n\n \nstatic inline unsigned int txq_avail(const struct sge_txq *q)\n{\n\treturn q->size - 1 - q->in_use;\n}\n\n \nstatic inline unsigned int fl_cap(const struct sge_fl *fl)\n{\n\treturn fl->size - 8;    \n}\n\n \nstatic inline bool fl_starving(const struct adapter *adapter,\n\t\t\t       const struct sge_fl *fl)\n{\n\tconst struct sge *s = &adapter->sge;\n\n\treturn fl->avail - fl->pend_cred <= s->fl_starve_thres;\n}\n\nint cxgb4_map_skb(struct device *dev, const struct sk_buff *skb,\n\t\t  dma_addr_t *addr)\n{\n\tconst skb_frag_t *fp, *end;\n\tconst struct skb_shared_info *si;\n\n\t*addr = dma_map_single(dev, skb->data, skb_headlen(skb), DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, *addr))\n\t\tgoto out_err;\n\n\tsi = skb_shinfo(skb);\n\tend = &si->frags[si->nr_frags];\n\n\tfor (fp = si->frags; fp < end; fp++) {\n\t\t*++addr = skb_frag_dma_map(dev, fp, 0, skb_frag_size(fp),\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, *addr))\n\t\t\tgoto unwind;\n\t}\n\treturn 0;\n\nunwind:\n\twhile (fp-- > si->frags)\n\t\tdma_unmap_page(dev, *--addr, skb_frag_size(fp), DMA_TO_DEVICE);\n\n\tdma_unmap_single(dev, addr[-1], skb_headlen(skb), DMA_TO_DEVICE);\nout_err:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(cxgb4_map_skb);\n\nstatic void unmap_skb(struct device *dev, const struct sk_buff *skb,\n\t\t      const dma_addr_t *addr)\n{\n\tconst skb_frag_t *fp, *end;\n\tconst struct skb_shared_info *si;\n\n\tdma_unmap_single(dev, *addr++, skb_headlen(skb), DMA_TO_DEVICE);\n\n\tsi = skb_shinfo(skb);\n\tend = &si->frags[si->nr_frags];\n\tfor (fp = si->frags; fp < end; fp++)\n\t\tdma_unmap_page(dev, *addr++, skb_frag_size(fp), DMA_TO_DEVICE);\n}\n\n#ifdef CONFIG_NEED_DMA_MAP_STATE\n \nstatic void deferred_unmap_destructor(struct sk_buff *skb)\n{\n\tunmap_skb(skb->dev->dev.parent, skb, (dma_addr_t *)skb->head);\n}\n#endif\n\n \nvoid free_tx_desc(struct adapter *adap, struct sge_txq *q,\n\t\t  unsigned int n, bool unmap)\n{\n\tunsigned int cidx = q->cidx;\n\tstruct tx_sw_desc *d;\n\n\td = &q->sdesc[cidx];\n\twhile (n--) {\n\t\tif (d->skb) {                        \n\t\t\tif (unmap && d->addr[0]) {\n\t\t\t\tunmap_skb(adap->pdev_dev, d->skb, d->addr);\n\t\t\t\tmemset(d->addr, 0, sizeof(d->addr));\n\t\t\t}\n\t\t\tdev_consume_skb_any(d->skb);\n\t\t\td->skb = NULL;\n\t\t}\n\t\t++d;\n\t\tif (++cidx == q->size) {\n\t\t\tcidx = 0;\n\t\t\td = q->sdesc;\n\t\t}\n\t}\n\tq->cidx = cidx;\n}\n\n \nstatic inline int reclaimable(const struct sge_txq *q)\n{\n\tint hw_cidx = ntohs(READ_ONCE(q->stat->cidx));\n\thw_cidx -= q->cidx;\n\treturn hw_cidx < 0 ? hw_cidx + q->size : hw_cidx;\n}\n\n \nstatic inline int reclaim_completed_tx(struct adapter *adap, struct sge_txq *q,\n\t\t\t\t       int maxreclaim, bool unmap)\n{\n\tint reclaim = reclaimable(q);\n\n\tif (reclaim) {\n\t\t \n\t\tif (maxreclaim < 0)\n\t\t\tmaxreclaim = MAX_TX_RECLAIM;\n\t\tif (reclaim > maxreclaim)\n\t\t\treclaim = maxreclaim;\n\n\t\tfree_tx_desc(adap, q, reclaim, unmap);\n\t\tq->in_use -= reclaim;\n\t}\n\n\treturn reclaim;\n}\n\n \nvoid cxgb4_reclaim_completed_tx(struct adapter *adap, struct sge_txq *q,\n\t\t\t\tbool unmap)\n{\n\t(void)reclaim_completed_tx(adap, q, -1, unmap);\n}\nEXPORT_SYMBOL(cxgb4_reclaim_completed_tx);\n\nstatic inline int get_buf_size(struct adapter *adapter,\n\t\t\t       const struct rx_sw_desc *d)\n{\n\tstruct sge *s = &adapter->sge;\n\tunsigned int rx_buf_size_idx = d->dma_addr & RX_BUF_SIZE;\n\tint buf_size;\n\n\tswitch (rx_buf_size_idx) {\n\tcase RX_SMALL_PG_BUF:\n\t\tbuf_size = PAGE_SIZE;\n\t\tbreak;\n\n\tcase RX_LARGE_PG_BUF:\n\t\tbuf_size = PAGE_SIZE << s->fl_pg_order;\n\t\tbreak;\n\n\tcase RX_SMALL_MTU_BUF:\n\t\tbuf_size = FL_MTU_SMALL_BUFSIZE(adapter);\n\t\tbreak;\n\n\tcase RX_LARGE_MTU_BUF:\n\t\tbuf_size = FL_MTU_LARGE_BUFSIZE(adapter);\n\t\tbreak;\n\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn buf_size;\n}\n\n \nstatic void free_rx_bufs(struct adapter *adap, struct sge_fl *q, int n)\n{\n\twhile (n--) {\n\t\tstruct rx_sw_desc *d = &q->sdesc[q->cidx];\n\n\t\tif (is_buf_mapped(d))\n\t\t\tdma_unmap_page(adap->pdev_dev, get_buf_addr(d),\n\t\t\t\t       get_buf_size(adap, d),\n\t\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(d->page);\n\t\td->page = NULL;\n\t\tif (++q->cidx == q->size)\n\t\t\tq->cidx = 0;\n\t\tq->avail--;\n\t}\n}\n\n \nstatic void unmap_rx_buf(struct adapter *adap, struct sge_fl *q)\n{\n\tstruct rx_sw_desc *d = &q->sdesc[q->cidx];\n\n\tif (is_buf_mapped(d))\n\t\tdma_unmap_page(adap->pdev_dev, get_buf_addr(d),\n\t\t\t       get_buf_size(adap, d), DMA_FROM_DEVICE);\n\td->page = NULL;\n\tif (++q->cidx == q->size)\n\t\tq->cidx = 0;\n\tq->avail--;\n}\n\nstatic inline void ring_fl_db(struct adapter *adap, struct sge_fl *q)\n{\n\tif (q->pend_cred >= 8) {\n\t\tu32 val = adap->params.arch.sge_fl_db;\n\n\t\tif (is_t4(adap->params.chip))\n\t\t\tval |= PIDX_V(q->pend_cred / 8);\n\t\telse\n\t\t\tval |= PIDX_T5_V(q->pend_cred / 8);\n\n\t\t \n\t\twmb();\n\n\t\t \n\t\tif (unlikely(q->bar2_addr == NULL)) {\n\t\t\tt4_write_reg(adap, MYPF_REG(SGE_PF_KDOORBELL_A),\n\t\t\t\t     val | QID_V(q->cntxt_id));\n\t\t} else {\n\t\t\twritel(val | QID_V(q->bar2_qid),\n\t\t\t       q->bar2_addr + SGE_UDB_KDOORBELL);\n\n\t\t\t \n\t\t\twmb();\n\t\t}\n\t\tq->pend_cred &= 7;\n\t}\n}\n\nstatic inline void set_rx_sw_desc(struct rx_sw_desc *sd, struct page *pg,\n\t\t\t\t  dma_addr_t mapping)\n{\n\tsd->page = pg;\n\tsd->dma_addr = mapping;       \n}\n\n \nstatic unsigned int refill_fl(struct adapter *adap, struct sge_fl *q, int n,\n\t\t\t      gfp_t gfp)\n{\n\tstruct sge *s = &adap->sge;\n\tstruct page *pg;\n\tdma_addr_t mapping;\n\tunsigned int cred = q->avail;\n\t__be64 *d = &q->desc[q->pidx];\n\tstruct rx_sw_desc *sd = &q->sdesc[q->pidx];\n\tint node;\n\n#ifdef CONFIG_DEBUG_FS\n\tif (test_bit(q->cntxt_id - adap->sge.egr_start, adap->sge.blocked_fl))\n\t\tgoto out;\n#endif\n\n\tgfp |= __GFP_NOWARN;\n\tnode = dev_to_node(adap->pdev_dev);\n\n\tif (s->fl_pg_order == 0)\n\t\tgoto alloc_small_pages;\n\n\t \n\twhile (n) {\n\t\tpg = alloc_pages_node(node, gfp | __GFP_COMP, s->fl_pg_order);\n\t\tif (unlikely(!pg)) {\n\t\t\tq->large_alloc_failed++;\n\t\t\tbreak;        \n\t\t}\n\n\t\tmapping = dma_map_page(adap->pdev_dev, pg, 0,\n\t\t\t\t       PAGE_SIZE << s->fl_pg_order,\n\t\t\t\t       DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(adap->pdev_dev, mapping))) {\n\t\t\t__free_pages(pg, s->fl_pg_order);\n\t\t\tq->mapping_err++;\n\t\t\tgoto out;    \n\t\t}\n\t\tmapping |= RX_LARGE_PG_BUF;\n\t\t*d++ = cpu_to_be64(mapping);\n\n\t\tset_rx_sw_desc(sd, pg, mapping);\n\t\tsd++;\n\n\t\tq->avail++;\n\t\tif (++q->pidx == q->size) {\n\t\t\tq->pidx = 0;\n\t\t\tsd = q->sdesc;\n\t\t\td = q->desc;\n\t\t}\n\t\tn--;\n\t}\n\nalloc_small_pages:\n\twhile (n--) {\n\t\tpg = alloc_pages_node(node, gfp, 0);\n\t\tif (unlikely(!pg)) {\n\t\t\tq->alloc_failed++;\n\t\t\tbreak;\n\t\t}\n\n\t\tmapping = dma_map_page(adap->pdev_dev, pg, 0, PAGE_SIZE,\n\t\t\t\t       DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(adap->pdev_dev, mapping))) {\n\t\t\tput_page(pg);\n\t\t\tq->mapping_err++;\n\t\t\tgoto out;\n\t\t}\n\t\t*d++ = cpu_to_be64(mapping);\n\n\t\tset_rx_sw_desc(sd, pg, mapping);\n\t\tsd++;\n\n\t\tq->avail++;\n\t\tif (++q->pidx == q->size) {\n\t\t\tq->pidx = 0;\n\t\t\tsd = q->sdesc;\n\t\t\td = q->desc;\n\t\t}\n\t}\n\nout:\tcred = q->avail - cred;\n\tq->pend_cred += cred;\n\tring_fl_db(adap, q);\n\n\tif (unlikely(fl_starving(adap, q))) {\n\t\tsmp_wmb();\n\t\tq->low++;\n\t\tset_bit(q->cntxt_id - adap->sge.egr_start,\n\t\t\tadap->sge.starving_fl);\n\t}\n\n\treturn cred;\n}\n\nstatic inline void __refill_fl(struct adapter *adap, struct sge_fl *fl)\n{\n\trefill_fl(adap, fl, min(MAX_RX_REFILL, fl_cap(fl) - fl->avail),\n\t\t  GFP_ATOMIC);\n}\n\n \nstatic void *alloc_ring(struct device *dev, size_t nelem, size_t elem_size,\n\t\t\tsize_t sw_size, dma_addr_t *phys, void *metadata,\n\t\t\tsize_t stat_size, int node)\n{\n\tsize_t len = nelem * elem_size + stat_size;\n\tvoid *s = NULL;\n\tvoid *p = dma_alloc_coherent(dev, len, phys, GFP_KERNEL);\n\n\tif (!p)\n\t\treturn NULL;\n\tif (sw_size) {\n\t\ts = kcalloc_node(sw_size, nelem, GFP_KERNEL, node);\n\n\t\tif (!s) {\n\t\t\tdma_free_coherent(dev, len, p, *phys);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\tif (metadata)\n\t\t*(void **)metadata = s;\n\treturn p;\n}\n\n \nstatic inline unsigned int sgl_len(unsigned int n)\n{\n\t \n\tn--;\n\treturn (3 * n) / 2 + (n & 1) + 2;\n}\n\n \nstatic inline unsigned int flits_to_desc(unsigned int n)\n{\n\tBUG_ON(n > SGE_MAX_WR_LEN / 8);\n\treturn DIV_ROUND_UP(n, 8);\n}\n\n \nstatic inline int is_eth_imm(const struct sk_buff *skb, unsigned int chip_ver)\n{\n\tint hdrlen = 0;\n\n\tif (skb->encapsulation && skb_shinfo(skb)->gso_size &&\n\t    chip_ver > CHELSIO_T5) {\n\t\thdrlen = sizeof(struct cpl_tx_tnl_lso);\n\t\thdrlen += sizeof(struct cpl_tx_pkt_core);\n\t} else if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {\n\t\treturn 0;\n\t} else {\n\t\thdrlen = skb_shinfo(skb)->gso_size ?\n\t\t\t sizeof(struct cpl_tx_pkt_lso_core) : 0;\n\t\thdrlen += sizeof(struct cpl_tx_pkt);\n\t}\n\tif (skb->len <= MAX_IMM_TX_PKT_LEN - hdrlen)\n\t\treturn hdrlen;\n\treturn 0;\n}\n\n \nstatic inline unsigned int calc_tx_flits(const struct sk_buff *skb,\n\t\t\t\t\t unsigned int chip_ver)\n{\n\tunsigned int flits;\n\tint hdrlen = is_eth_imm(skb, chip_ver);\n\n\t \n\n\tif (hdrlen)\n\t\treturn DIV_ROUND_UP(skb->len + hdrlen, sizeof(__be64));\n\n\t \n\tflits = sgl_len(skb_shinfo(skb)->nr_frags + 1);\n\tif (skb_shinfo(skb)->gso_size) {\n\t\tif (skb->encapsulation && chip_ver > CHELSIO_T5) {\n\t\t\thdrlen = sizeof(struct fw_eth_tx_pkt_wr) +\n\t\t\t\t sizeof(struct cpl_tx_tnl_lso);\n\t\t} else if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {\n\t\t\tu32 pkt_hdrlen;\n\n\t\t\tpkt_hdrlen = eth_get_headlen(skb->dev, skb->data,\n\t\t\t\t\t\t     skb_headlen(skb));\n\t\t\thdrlen = sizeof(struct fw_eth_tx_eo_wr) +\n\t\t\t\t round_up(pkt_hdrlen, 16);\n\t\t} else {\n\t\t\thdrlen = sizeof(struct fw_eth_tx_pkt_wr) +\n\t\t\t\t sizeof(struct cpl_tx_pkt_lso_core);\n\t\t}\n\n\t\thdrlen += sizeof(struct cpl_tx_pkt_core);\n\t\tflits += (hdrlen / sizeof(__be64));\n\t} else {\n\t\tflits += (sizeof(struct fw_eth_tx_pkt_wr) +\n\t\t\t  sizeof(struct cpl_tx_pkt_core)) / sizeof(__be64);\n\t}\n\treturn flits;\n}\n\n \nstatic inline unsigned int calc_tx_descs(const struct sk_buff *skb,\n\t\t\t\t\t unsigned int chip_ver)\n{\n\treturn flits_to_desc(calc_tx_flits(skb, chip_ver));\n}\n\n \nvoid cxgb4_write_sgl(const struct sk_buff *skb, struct sge_txq *q,\n\t\t     struct ulptx_sgl *sgl, u64 *end, unsigned int start,\n\t\t     const dma_addr_t *addr)\n{\n\tunsigned int i, len;\n\tstruct ulptx_sge_pair *to;\n\tconst struct skb_shared_info *si = skb_shinfo(skb);\n\tunsigned int nfrags = si->nr_frags;\n\tstruct ulptx_sge_pair buf[MAX_SKB_FRAGS / 2 + 1];\n\n\tlen = skb_headlen(skb) - start;\n\tif (likely(len)) {\n\t\tsgl->len0 = htonl(len);\n\t\tsgl->addr0 = cpu_to_be64(addr[0] + start);\n\t\tnfrags++;\n\t} else {\n\t\tsgl->len0 = htonl(skb_frag_size(&si->frags[0]));\n\t\tsgl->addr0 = cpu_to_be64(addr[1]);\n\t}\n\n\tsgl->cmd_nsge = htonl(ULPTX_CMD_V(ULP_TX_SC_DSGL) |\n\t\t\t      ULPTX_NSGE_V(nfrags));\n\tif (likely(--nfrags == 0))\n\t\treturn;\n\t \n\tto = (u8 *)end > (u8 *)q->stat ? buf : sgl->sge;\n\n\tfor (i = (nfrags != si->nr_frags); nfrags >= 2; nfrags -= 2, to++) {\n\t\tto->len[0] = cpu_to_be32(skb_frag_size(&si->frags[i]));\n\t\tto->len[1] = cpu_to_be32(skb_frag_size(&si->frags[++i]));\n\t\tto->addr[0] = cpu_to_be64(addr[i]);\n\t\tto->addr[1] = cpu_to_be64(addr[++i]);\n\t}\n\tif (nfrags) {\n\t\tto->len[0] = cpu_to_be32(skb_frag_size(&si->frags[i]));\n\t\tto->len[1] = cpu_to_be32(0);\n\t\tto->addr[0] = cpu_to_be64(addr[i + 1]);\n\t}\n\tif (unlikely((u8 *)end > (u8 *)q->stat)) {\n\t\tunsigned int part0 = (u8 *)q->stat - (u8 *)sgl->sge, part1;\n\n\t\tif (likely(part0))\n\t\t\tmemcpy(sgl->sge, buf, part0);\n\t\tpart1 = (u8 *)end - (u8 *)q->stat;\n\t\tmemcpy(q->desc, (u8 *)buf + part0, part1);\n\t\tend = (void *)q->desc + part1;\n\t}\n\tif ((uintptr_t)end & 8)            \n\t\t*end = 0;\n}\nEXPORT_SYMBOL(cxgb4_write_sgl);\n\n \nvoid cxgb4_write_partial_sgl(const struct sk_buff *skb, struct sge_txq *q,\n\t\t\t     struct ulptx_sgl *sgl, u64 *end,\n\t\t\t     const dma_addr_t *addr, u32 start, u32 len)\n{\n\tstruct ulptx_sge_pair buf[MAX_SKB_FRAGS / 2 + 1] = {0}, *to;\n\tu32 frag_size, skb_linear_data_len = skb_headlen(skb);\n\tstruct skb_shared_info *si = skb_shinfo(skb);\n\tu8 i = 0, frag_idx = 0, nfrags = 0;\n\tskb_frag_t *frag;\n\n\t \n\tif (unlikely(start < skb_linear_data_len)) {\n\t\tfrag_size = min(len, skb_linear_data_len - start);\n\t\tsgl->len0 = htonl(frag_size);\n\t\tsgl->addr0 = cpu_to_be64(addr[0] + start);\n\t\tlen -= frag_size;\n\t\tnfrags++;\n\t} else {\n\t\tstart -= skb_linear_data_len;\n\t\tfrag = &si->frags[frag_idx];\n\t\tfrag_size = skb_frag_size(frag);\n\t\t \n\t\twhile (start >= frag_size) {\n\t\t\tstart -= frag_size;\n\t\t\tfrag_idx++;\n\t\t\tfrag = &si->frags[frag_idx];\n\t\t\tfrag_size = skb_frag_size(frag);\n\t\t}\n\n\t\tfrag_size = min(len, skb_frag_size(frag) - start);\n\t\tsgl->len0 = cpu_to_be32(frag_size);\n\t\tsgl->addr0 = cpu_to_be64(addr[frag_idx + 1] + start);\n\t\tlen -= frag_size;\n\t\tnfrags++;\n\t\tfrag_idx++;\n\t}\n\n\t \n\tif (!len)\n\t\tgoto done;\n\n\t \n\tto = (u8 *)end > (u8 *)q->stat ? buf : sgl->sge;\n\n\t \n\twhile (len) {\n\t\tfrag_size = min(len, skb_frag_size(&si->frags[frag_idx]));\n\t\tto->len[i & 1] = cpu_to_be32(frag_size);\n\t\tto->addr[i & 1] = cpu_to_be64(addr[frag_idx + 1]);\n\t\tif (i && (i & 1))\n\t\t\tto++;\n\t\tnfrags++;\n\t\tfrag_idx++;\n\t\ti++;\n\t\tlen -= frag_size;\n\t}\n\n\t \n\tif (i & 1)\n\t\tto->len[1] = cpu_to_be32(0);\n\n\t \n\tif (unlikely((u8 *)end > (u8 *)q->stat)) {\n\t\tu32 part0 = (u8 *)q->stat - (u8 *)sgl->sge, part1;\n\n\t\tif (likely(part0))\n\t\t\tmemcpy(sgl->sge, buf, part0);\n\t\tpart1 = (u8 *)end - (u8 *)q->stat;\n\t\tmemcpy(q->desc, (u8 *)buf + part0, part1);\n\t\tend = (void *)q->desc + part1;\n\t}\n\n\t \n\tif ((uintptr_t)end & 8)\n\t\t*end = 0;\ndone:\n\tsgl->cmd_nsge = htonl(ULPTX_CMD_V(ULP_TX_SC_DSGL) |\n\t\t\tULPTX_NSGE_V(nfrags));\n}\nEXPORT_SYMBOL(cxgb4_write_partial_sgl);\n\n \nstatic void cxgb_pio_copy(u64 __iomem *dst, u64 *src)\n{\n\tint count = 8;\n\n\twhile (count) {\n\t\twriteq(*src, dst);\n\t\tsrc++;\n\t\tdst++;\n\t\tcount--;\n\t}\n}\n\n \ninline void cxgb4_ring_tx_db(struct adapter *adap, struct sge_txq *q, int n)\n{\n\t \n\twmb();\n\n\t \n\tif (unlikely(q->bar2_addr == NULL)) {\n\t\tu32 val = PIDX_V(n);\n\t\tunsigned long flags;\n\n\t\t \n\t\tspin_lock_irqsave(&q->db_lock, flags);\n\t\tif (!q->db_disabled)\n\t\t\tt4_write_reg(adap, MYPF_REG(SGE_PF_KDOORBELL_A),\n\t\t\t\t     QID_V(q->cntxt_id) | val);\n\t\telse\n\t\t\tq->db_pidx_inc += n;\n\t\tq->db_pidx = q->pidx;\n\t\tspin_unlock_irqrestore(&q->db_lock, flags);\n\t} else {\n\t\tu32 val = PIDX_T5_V(n);\n\n\t\t \n\t\tWARN_ON(val & DBPRIO_F);\n\n\t\t \n\t\tif (n == 1 && q->bar2_qid == 0) {\n\t\t\tint index = (q->pidx\n\t\t\t\t     ? (q->pidx - 1)\n\t\t\t\t     : (q->size - 1));\n\t\t\tu64 *wr = (u64 *)&q->desc[index];\n\n\t\t\tcxgb_pio_copy((u64 __iomem *)\n\t\t\t\t      (q->bar2_addr + SGE_UDB_WCDOORBELL),\n\t\t\t\t      wr);\n\t\t} else {\n\t\t\twritel(val | QID_V(q->bar2_qid),\n\t\t\t       q->bar2_addr + SGE_UDB_KDOORBELL);\n\t\t}\n\n\t\t \n\t\twmb();\n\t}\n}\nEXPORT_SYMBOL(cxgb4_ring_tx_db);\n\n \nvoid cxgb4_inline_tx_skb(const struct sk_buff *skb,\n\t\t\t const struct sge_txq *q, void *pos)\n{\n\tint left = (void *)q->stat - pos;\n\tu64 *p;\n\n\tif (likely(skb->len <= left)) {\n\t\tif (likely(!skb->data_len))\n\t\t\tskb_copy_from_linear_data(skb, pos, skb->len);\n\t\telse\n\t\t\tskb_copy_bits(skb, 0, pos, skb->len);\n\t\tpos += skb->len;\n\t} else {\n\t\tskb_copy_bits(skb, 0, pos, left);\n\t\tskb_copy_bits(skb, left, q->desc, skb->len - left);\n\t\tpos = (void *)q->desc + (skb->len - left);\n\t}\n\n\t \n\tp = PTR_ALIGN(pos, 8);\n\tif ((uintptr_t)p & 8)\n\t\t*p = 0;\n}\nEXPORT_SYMBOL(cxgb4_inline_tx_skb);\n\nstatic void *inline_tx_skb_header(const struct sk_buff *skb,\n\t\t\t\t  const struct sge_txq *q,  void *pos,\n\t\t\t\t  int length)\n{\n\tu64 *p;\n\tint left = (void *)q->stat - pos;\n\n\tif (likely(length <= left)) {\n\t\tmemcpy(pos, skb->data, length);\n\t\tpos += length;\n\t} else {\n\t\tmemcpy(pos, skb->data, left);\n\t\tmemcpy(q->desc, skb->data + left, length - left);\n\t\tpos = (void *)q->desc + (length - left);\n\t}\n\t \n\tp = PTR_ALIGN(pos, 8);\n\tif ((uintptr_t)p & 8) {\n\t\t*p = 0;\n\t\treturn p + 1;\n\t}\n\treturn p;\n}\n\n \nstatic u64 hwcsum(enum chip_type chip, const struct sk_buff *skb)\n{\n\tint csum_type;\n\tbool inner_hdr_csum = false;\n\tu16 proto, ver;\n\n\tif (skb->encapsulation &&\n\t    (CHELSIO_CHIP_VERSION(chip) > CHELSIO_T5))\n\t\tinner_hdr_csum = true;\n\n\tif (inner_hdr_csum) {\n\t\tver = inner_ip_hdr(skb)->version;\n\t\tproto = (ver == 4) ? inner_ip_hdr(skb)->protocol :\n\t\t\tinner_ipv6_hdr(skb)->nexthdr;\n\t} else {\n\t\tver = ip_hdr(skb)->version;\n\t\tproto = (ver == 4) ? ip_hdr(skb)->protocol :\n\t\t\tipv6_hdr(skb)->nexthdr;\n\t}\n\n\tif (ver == 4) {\n\t\tif (proto == IPPROTO_TCP)\n\t\t\tcsum_type = TX_CSUM_TCPIP;\n\t\telse if (proto == IPPROTO_UDP)\n\t\t\tcsum_type = TX_CSUM_UDPIP;\n\t\telse {\nnocsum:\t\t\t \n\t\t\treturn TXPKT_L4CSUM_DIS_F;\n\t\t}\n\t} else {\n\t\t \n\t\tif (proto == IPPROTO_TCP)\n\t\t\tcsum_type = TX_CSUM_TCPIP6;\n\t\telse if (proto == IPPROTO_UDP)\n\t\t\tcsum_type = TX_CSUM_UDPIP6;\n\t\telse\n\t\t\tgoto nocsum;\n\t}\n\n\tif (likely(csum_type >= TX_CSUM_TCPIP)) {\n\t\tint eth_hdr_len, l4_len;\n\t\tu64 hdr_len;\n\n\t\tif (inner_hdr_csum) {\n\t\t\t \n\t\t\tl4_len = skb_inner_network_header_len(skb);\n\t\t\teth_hdr_len = skb_inner_network_offset(skb) - ETH_HLEN;\n\t\t} else {\n\t\t\tl4_len = skb_network_header_len(skb);\n\t\t\teth_hdr_len = skb_network_offset(skb) - ETH_HLEN;\n\t\t}\n\t\thdr_len = TXPKT_IPHDR_LEN_V(l4_len);\n\n\t\tif (CHELSIO_CHIP_VERSION(chip) <= CHELSIO_T5)\n\t\t\thdr_len |= TXPKT_ETHHDR_LEN_V(eth_hdr_len);\n\t\telse\n\t\t\thdr_len |= T6_TXPKT_ETHHDR_LEN_V(eth_hdr_len);\n\t\treturn TXPKT_CSUM_TYPE_V(csum_type) | hdr_len;\n\t} else {\n\t\tint start = skb_transport_offset(skb);\n\n\t\treturn TXPKT_CSUM_TYPE_V(csum_type) |\n\t\t\tTXPKT_CSUM_START_V(start) |\n\t\t\tTXPKT_CSUM_LOC_V(start + skb->csum_offset);\n\t}\n}\n\nstatic void eth_txq_stop(struct sge_eth_txq *q)\n{\n\tnetif_tx_stop_queue(q->txq);\n\tq->q.stops++;\n}\n\nstatic inline void txq_advance(struct sge_txq *q, unsigned int n)\n{\n\tq->in_use += n;\n\tq->pidx += n;\n\tif (q->pidx >= q->size)\n\t\tq->pidx -= q->size;\n}\n\n#ifdef CONFIG_CHELSIO_T4_FCOE\nstatic inline int\ncxgb_fcoe_offload(struct sk_buff *skb, struct adapter *adap,\n\t\t  const struct port_info *pi, u64 *cntrl)\n{\n\tconst struct cxgb_fcoe *fcoe = &pi->fcoe;\n\n\tif (!(fcoe->flags & CXGB_FCOE_ENABLED))\n\t\treturn 0;\n\n\tif (skb->protocol != htons(ETH_P_FCOE))\n\t\treturn 0;\n\n\tskb_reset_mac_header(skb);\n\tskb->mac_len = sizeof(struct ethhdr);\n\n\tskb_set_network_header(skb, skb->mac_len);\n\tskb_set_transport_header(skb, skb->mac_len + sizeof(struct fcoe_hdr));\n\n\tif (!cxgb_fcoe_sof_eof_supported(adap, skb))\n\t\treturn -ENOTSUPP;\n\n\t \n\t*cntrl = TXPKT_CSUM_TYPE_V(TX_CSUM_FCOE) |\n\t\t     TXPKT_L4CSUM_DIS_F | TXPKT_IPCSUM_DIS_F |\n\t\t     TXPKT_CSUM_START_V(CXGB_FCOE_TXPKT_CSUM_START) |\n\t\t     TXPKT_CSUM_END_V(CXGB_FCOE_TXPKT_CSUM_END) |\n\t\t     TXPKT_CSUM_LOC_V(CXGB_FCOE_TXPKT_CSUM_END);\n\treturn 0;\n}\n#endif  \n\n \nenum cpl_tx_tnl_lso_type cxgb_encap_offload_supported(struct sk_buff *skb)\n{\n\tu8 l4_hdr = 0;\n\tenum cpl_tx_tnl_lso_type tnl_type = TX_TNL_TYPE_OPAQUE;\n\tstruct port_info *pi = netdev_priv(skb->dev);\n\tstruct adapter *adapter = pi->adapter;\n\n\tif (skb->inner_protocol_type != ENCAP_TYPE_ETHER ||\n\t    skb->inner_protocol != htons(ETH_P_TEB))\n\t\treturn tnl_type;\n\n\tswitch (vlan_get_protocol(skb)) {\n\tcase htons(ETH_P_IP):\n\t\tl4_hdr = ip_hdr(skb)->protocol;\n\t\tbreak;\n\tcase htons(ETH_P_IPV6):\n\t\tl4_hdr = ipv6_hdr(skb)->nexthdr;\n\t\tbreak;\n\tdefault:\n\t\treturn tnl_type;\n\t}\n\n\tswitch (l4_hdr) {\n\tcase IPPROTO_UDP:\n\t\tif (adapter->vxlan_port == udp_hdr(skb)->dest)\n\t\t\ttnl_type = TX_TNL_TYPE_VXLAN;\n\t\telse if (adapter->geneve_port == udp_hdr(skb)->dest)\n\t\t\ttnl_type = TX_TNL_TYPE_GENEVE;\n\t\tbreak;\n\tdefault:\n\t\treturn tnl_type;\n\t}\n\n\treturn tnl_type;\n}\n\nstatic inline void t6_fill_tnl_lso(struct sk_buff *skb,\n\t\t\t\t   struct cpl_tx_tnl_lso *tnl_lso,\n\t\t\t\t   enum cpl_tx_tnl_lso_type tnl_type)\n{\n\tu32 val;\n\tint in_eth_xtra_len;\n\tint l3hdr_len = skb_network_header_len(skb);\n\tint eth_xtra_len = skb_network_offset(skb) - ETH_HLEN;\n\tconst struct skb_shared_info *ssi = skb_shinfo(skb);\n\tbool v6 = (ip_hdr(skb)->version == 6);\n\n\tval = CPL_TX_TNL_LSO_OPCODE_V(CPL_TX_TNL_LSO) |\n\t      CPL_TX_TNL_LSO_FIRST_F |\n\t      CPL_TX_TNL_LSO_LAST_F |\n\t      (v6 ? CPL_TX_TNL_LSO_IPV6OUT_F : 0) |\n\t      CPL_TX_TNL_LSO_ETHHDRLENOUT_V(eth_xtra_len / 4) |\n\t      CPL_TX_TNL_LSO_IPHDRLENOUT_V(l3hdr_len / 4) |\n\t      (v6 ? 0 : CPL_TX_TNL_LSO_IPHDRCHKOUT_F) |\n\t      CPL_TX_TNL_LSO_IPLENSETOUT_F |\n\t      (v6 ? 0 : CPL_TX_TNL_LSO_IPIDINCOUT_F);\n\ttnl_lso->op_to_IpIdSplitOut = htonl(val);\n\n\ttnl_lso->IpIdOffsetOut = 0;\n\n\t \n\tval = skb_inner_mac_header(skb) - skb_mac_header(skb);\n\tin_eth_xtra_len = skb_inner_network_header(skb) -\n\t\t\t  skb_inner_mac_header(skb) - ETH_HLEN;\n\n\tswitch (tnl_type) {\n\tcase TX_TNL_TYPE_VXLAN:\n\tcase TX_TNL_TYPE_GENEVE:\n\t\ttnl_lso->UdpLenSetOut_to_TnlHdrLen =\n\t\t\thtons(CPL_TX_TNL_LSO_UDPCHKCLROUT_F |\n\t\t\tCPL_TX_TNL_LSO_UDPLENSETOUT_F);\n\t\tbreak;\n\tdefault:\n\t\ttnl_lso->UdpLenSetOut_to_TnlHdrLen = 0;\n\t\tbreak;\n\t}\n\n\ttnl_lso->UdpLenSetOut_to_TnlHdrLen |=\n\t\t htons(CPL_TX_TNL_LSO_TNLHDRLEN_V(val) |\n\t\t       CPL_TX_TNL_LSO_TNLTYPE_V(tnl_type));\n\n\ttnl_lso->r1 = 0;\n\n\tval = CPL_TX_TNL_LSO_ETHHDRLEN_V(in_eth_xtra_len / 4) |\n\t      CPL_TX_TNL_LSO_IPV6_V(inner_ip_hdr(skb)->version == 6) |\n\t      CPL_TX_TNL_LSO_IPHDRLEN_V(skb_inner_network_header_len(skb) / 4) |\n\t      CPL_TX_TNL_LSO_TCPHDRLEN_V(inner_tcp_hdrlen(skb) / 4);\n\ttnl_lso->Flow_to_TcpHdrLen = htonl(val);\n\n\ttnl_lso->IpIdOffset = htons(0);\n\n\ttnl_lso->IpIdSplit_to_Mss = htons(CPL_TX_TNL_LSO_MSS_V(ssi->gso_size));\n\ttnl_lso->TCPSeqOffset = htonl(0);\n\ttnl_lso->EthLenOffset_Size = htonl(CPL_TX_TNL_LSO_SIZE_V(skb->len));\n}\n\nstatic inline void *write_tso_wr(struct adapter *adap, struct sk_buff *skb,\n\t\t\t\t struct cpl_tx_pkt_lso_core *lso)\n{\n\tint eth_xtra_len = skb_network_offset(skb) - ETH_HLEN;\n\tint l3hdr_len = skb_network_header_len(skb);\n\tconst struct skb_shared_info *ssi;\n\tbool ipv6 = false;\n\n\tssi = skb_shinfo(skb);\n\tif (ssi->gso_type & SKB_GSO_TCPV6)\n\t\tipv6 = true;\n\n\tlso->lso_ctrl = htonl(LSO_OPCODE_V(CPL_TX_PKT_LSO) |\n\t\t\t      LSO_FIRST_SLICE_F | LSO_LAST_SLICE_F |\n\t\t\t      LSO_IPV6_V(ipv6) |\n\t\t\t      LSO_ETHHDR_LEN_V(eth_xtra_len / 4) |\n\t\t\t      LSO_IPHDR_LEN_V(l3hdr_len / 4) |\n\t\t\t      LSO_TCPHDR_LEN_V(tcp_hdr(skb)->doff));\n\tlso->ipid_ofst = htons(0);\n\tlso->mss = htons(ssi->gso_size);\n\tlso->seqno_offset = htonl(0);\n\tif (is_t4(adap->params.chip))\n\t\tlso->len = htonl(skb->len);\n\telse\n\t\tlso->len = htonl(LSO_T5_XFER_SIZE_V(skb->len));\n\n\treturn (void *)(lso + 1);\n}\n\n \nint t4_sge_eth_txq_egress_update(struct adapter *adap, struct sge_eth_txq *eq,\n\t\t\t\t int maxreclaim)\n{\n\tunsigned int reclaimed, hw_cidx;\n\tstruct sge_txq *q = &eq->q;\n\tint hw_in_use;\n\n\tif (!q->in_use || !__netif_tx_trylock(eq->txq))\n\t\treturn 0;\n\n\t \n\treclaimed = reclaim_completed_tx(adap, &eq->q, maxreclaim, true);\n\n\thw_cidx = ntohs(READ_ONCE(q->stat->cidx));\n\thw_in_use = q->pidx - hw_cidx;\n\tif (hw_in_use < 0)\n\t\thw_in_use += q->size;\n\n\t \n\tif (netif_tx_queue_stopped(eq->txq) && hw_in_use < (q->size / 2)) {\n\t\tnetif_tx_wake_queue(eq->txq);\n\t\teq->q.restarts++;\n\t}\n\n\t__netif_tx_unlock(eq->txq);\n\treturn reclaimed;\n}\n\nstatic inline int cxgb4_validate_skb(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev,\n\t\t\t\t     u32 min_pkt_len)\n{\n\tu32 max_pkt_len;\n\n\t \n\tif (unlikely(skb->len < min_pkt_len))\n\t\treturn -EINVAL;\n\n\t \n\tmax_pkt_len = ETH_HLEN + dev->mtu;\n\n\tif (skb_vlan_tagged(skb))\n\t\tmax_pkt_len += VLAN_HLEN;\n\n\tif (!skb_shinfo(skb)->gso_size && (unlikely(skb->len > max_pkt_len)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void *write_eo_udp_wr(struct sk_buff *skb, struct fw_eth_tx_eo_wr *wr,\n\t\t\t     u32 hdr_len)\n{\n\twr->u.udpseg.type = FW_ETH_TX_EO_TYPE_UDPSEG;\n\twr->u.udpseg.ethlen = skb_network_offset(skb);\n\twr->u.udpseg.iplen = cpu_to_be16(skb_network_header_len(skb));\n\twr->u.udpseg.udplen = sizeof(struct udphdr);\n\twr->u.udpseg.rtplen = 0;\n\twr->u.udpseg.r4 = 0;\n\tif (skb_shinfo(skb)->gso_size)\n\t\twr->u.udpseg.mss = cpu_to_be16(skb_shinfo(skb)->gso_size);\n\telse\n\t\twr->u.udpseg.mss = cpu_to_be16(skb->len - hdr_len);\n\twr->u.udpseg.schedpktsize = wr->u.udpseg.mss;\n\twr->u.udpseg.plen = cpu_to_be32(skb->len - hdr_len);\n\n\treturn (void *)(wr + 1);\n}\n\n \nstatic netdev_tx_t cxgb4_eth_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tenum cpl_tx_tnl_lso_type tnl_type = TX_TNL_TYPE_OPAQUE;\n\tbool ptp_enabled = is_ptp_enabled(skb, dev);\n\tunsigned int last_desc, flits, ndesc;\n\tu32 wr_mid, ctrl0, op, sgl_off = 0;\n\tconst struct skb_shared_info *ssi;\n\tint len, qidx, credits, ret, left;\n\tstruct tx_sw_desc *sgl_sdesc;\n\tstruct fw_eth_tx_eo_wr *eowr;\n\tstruct fw_eth_tx_pkt_wr *wr;\n\tstruct cpl_tx_pkt_core *cpl;\n\tconst struct port_info *pi;\n\tbool immediate = false;\n\tu64 cntrl, *end, *sgl;\n\tstruct sge_eth_txq *q;\n\tunsigned int chip_ver;\n\tstruct adapter *adap;\n\n\tret = cxgb4_validate_skb(skb, dev, ETH_HLEN);\n\tif (ret)\n\t\tgoto out_free;\n\n\tpi = netdev_priv(dev);\n\tadap = pi->adapter;\n\tssi = skb_shinfo(skb);\n#if IS_ENABLED(CONFIG_CHELSIO_IPSEC_INLINE)\n\tif (xfrm_offload(skb) && !ssi->gso_size)\n\t\treturn adap->uld[CXGB4_ULD_IPSEC].tx_handler(skb, dev);\n#endif  \n\n#if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)\n\tif (tls_is_skb_tx_device_offloaded(skb) &&\n\t    (skb->len - skb_tcp_all_headers(skb)))\n\t\treturn adap->uld[CXGB4_ULD_KTLS].tx_handler(skb, dev);\n#endif  \n\n\tqidx = skb_get_queue_mapping(skb);\n\tif (ptp_enabled) {\n\t\tif (!(adap->ptp_tx_skb)) {\n\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t\tadap->ptp_tx_skb = skb_get(skb);\n\t\t} else {\n\t\t\tgoto out_free;\n\t\t}\n\t\tq = &adap->sge.ptptxq;\n\t} else {\n\t\tq = &adap->sge.ethtxq[qidx + pi->first_qset];\n\t}\n\tskb_tx_timestamp(skb);\n\n\treclaim_completed_tx(adap, &q->q, -1, true);\n\tcntrl = TXPKT_L4CSUM_DIS_F | TXPKT_IPCSUM_DIS_F;\n\n#ifdef CONFIG_CHELSIO_T4_FCOE\n\tret = cxgb_fcoe_offload(skb, adap, pi, &cntrl);\n\tif (unlikely(ret == -EOPNOTSUPP))\n\t\tgoto out_free;\n#endif  \n\n\tchip_ver = CHELSIO_CHIP_VERSION(adap->params.chip);\n\tflits = calc_tx_flits(skb, chip_ver);\n\tndesc = flits_to_desc(flits);\n\tcredits = txq_avail(&q->q) - ndesc;\n\n\tif (unlikely(credits < 0)) {\n\t\teth_txq_stop(q);\n\t\tdev_err(adap->pdev_dev,\n\t\t\t\"%s: Tx ring %u full while queue awake!\\n\",\n\t\t\tdev->name, qidx);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (is_eth_imm(skb, chip_ver))\n\t\timmediate = true;\n\n\tif (skb->encapsulation && chip_ver > CHELSIO_T5)\n\t\ttnl_type = cxgb_encap_offload_supported(skb);\n\n\tlast_desc = q->q.pidx + ndesc - 1;\n\tif (last_desc >= q->q.size)\n\t\tlast_desc -= q->q.size;\n\tsgl_sdesc = &q->q.sdesc[last_desc];\n\n\tif (!immediate &&\n\t    unlikely(cxgb4_map_skb(adap->pdev_dev, skb, sgl_sdesc->addr) < 0)) {\n\t\tmemset(sgl_sdesc->addr, 0, sizeof(sgl_sdesc->addr));\n\t\tq->mapping_err++;\n\t\tgoto out_free;\n\t}\n\n\twr_mid = FW_WR_LEN16_V(DIV_ROUND_UP(flits, 2));\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\t \n\t\teth_txq_stop(q);\n\t\tif (chip_ver > CHELSIO_T5)\n\t\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\n\twr = (void *)&q->q.desc[q->q.pidx];\n\teowr = (void *)&q->q.desc[q->q.pidx];\n\twr->equiq_to_len16 = htonl(wr_mid);\n\twr->r3 = cpu_to_be64(0);\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4)\n\t\tend = (u64 *)eowr + flits;\n\telse\n\t\tend = (u64 *)wr + flits;\n\n\tlen = immediate ? skb->len : 0;\n\tlen += sizeof(*cpl);\n\tif (ssi->gso_size && !(ssi->gso_type & SKB_GSO_UDP_L4)) {\n\t\tstruct cpl_tx_pkt_lso_core *lso = (void *)(wr + 1);\n\t\tstruct cpl_tx_tnl_lso *tnl_lso = (void *)(wr + 1);\n\n\t\tif (tnl_type)\n\t\t\tlen += sizeof(*tnl_lso);\n\t\telse\n\t\t\tlen += sizeof(*lso);\n\n\t\twr->op_immdlen = htonl(FW_WR_OP_V(FW_ETH_TX_PKT_WR) |\n\t\t\t\t       FW_WR_IMMDLEN_V(len));\n\t\tif (tnl_type) {\n\t\t\tstruct iphdr *iph = ip_hdr(skb);\n\n\t\t\tt6_fill_tnl_lso(skb, tnl_lso, tnl_type);\n\t\t\tcpl = (void *)(tnl_lso + 1);\n\t\t\t \n\t\t\tif (iph->version == 4) {\n\t\t\t\tiph->check = 0;\n\t\t\t\tiph->tot_len = 0;\n\t\t\t\tiph->check = ~ip_fast_csum((u8 *)iph, iph->ihl);\n\t\t\t}\n\t\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\t\tcntrl = hwcsum(adap->params.chip, skb);\n\t\t} else {\n\t\t\tcpl = write_tso_wr(adap, skb, lso);\n\t\t\tcntrl = hwcsum(adap->params.chip, skb);\n\t\t}\n\t\tsgl = (u64 *)(cpl + 1);  \n\t\tq->tso++;\n\t\tq->tx_cso += ssi->gso_segs;\n\t} else if (ssi->gso_size) {\n\t\tu64 *start;\n\t\tu32 hdrlen;\n\n\t\thdrlen = eth_get_headlen(dev, skb->data, skb_headlen(skb));\n\t\tlen += hdrlen;\n\t\twr->op_immdlen = cpu_to_be32(FW_WR_OP_V(FW_ETH_TX_EO_WR) |\n\t\t\t\t\t     FW_ETH_TX_EO_WR_IMMDLEN_V(len));\n\t\tcpl = write_eo_udp_wr(skb, eowr, hdrlen);\n\t\tcntrl = hwcsum(adap->params.chip, skb);\n\n\t\tstart = (u64 *)(cpl + 1);\n\t\tsgl = (u64 *)inline_tx_skb_header(skb, &q->q, (void *)start,\n\t\t\t\t\t\t  hdrlen);\n\t\tif (unlikely(start > sgl)) {\n\t\t\tleft = (u8 *)end - (u8 *)q->q.stat;\n\t\t\tend = (void *)q->q.desc + left;\n\t\t}\n\t\tsgl_off = hdrlen;\n\t\tq->uso++;\n\t\tq->tx_cso += ssi->gso_segs;\n\t} else {\n\t\tif (ptp_enabled)\n\t\t\top = FW_PTP_TX_PKT_WR;\n\t\telse\n\t\t\top = FW_ETH_TX_PKT_WR;\n\t\twr->op_immdlen = htonl(FW_WR_OP_V(op) |\n\t\t\t\t       FW_WR_IMMDLEN_V(len));\n\t\tcpl = (void *)(wr + 1);\n\t\tsgl = (u64 *)(cpl + 1);\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tcntrl = hwcsum(adap->params.chip, skb) |\n\t\t\t\tTXPKT_IPCSUM_DIS_F;\n\t\t\tq->tx_cso++;\n\t\t}\n\t}\n\n\tif (unlikely((u8 *)sgl >= (u8 *)q->q.stat)) {\n\t\t \n\t\tleft = (u8 *)end - (u8 *)q->q.stat;\n\t\tend = (void *)q->q.desc + left;\n\t\tsgl = (void *)q->q.desc;\n\t}\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tq->vlan_ins++;\n\t\tcntrl |= TXPKT_VLAN_VLD_F | TXPKT_VLAN_V(skb_vlan_tag_get(skb));\n#ifdef CONFIG_CHELSIO_T4_FCOE\n\t\tif (skb->protocol == htons(ETH_P_FCOE))\n\t\t\tcntrl |= TXPKT_VLAN_V(\n\t\t\t\t ((skb->priority & 0x7) << VLAN_PRIO_SHIFT));\n#endif  \n\t}\n\n\tctrl0 = TXPKT_OPCODE_V(CPL_TX_PKT_XT) | TXPKT_INTF_V(pi->tx_chan) |\n\t\tTXPKT_PF_V(adap->pf);\n\tif (ptp_enabled)\n\t\tctrl0 |= TXPKT_TSTAMP_F;\n#ifdef CONFIG_CHELSIO_T4_DCB\n\tif (is_t4(adap->params.chip))\n\t\tctrl0 |= TXPKT_OVLAN_IDX_V(q->dcb_prio);\n\telse\n\t\tctrl0 |= TXPKT_T5_OVLAN_IDX_V(q->dcb_prio);\n#endif\n\tcpl->ctrl0 = htonl(ctrl0);\n\tcpl->pack = htons(0);\n\tcpl->len = htons(skb->len);\n\tcpl->ctrl1 = cpu_to_be64(cntrl);\n\n\tif (immediate) {\n\t\tcxgb4_inline_tx_skb(skb, &q->q, sgl);\n\t\tdev_consume_skb_any(skb);\n\t} else {\n\t\tcxgb4_write_sgl(skb, &q->q, (void *)sgl, end, sgl_off,\n\t\t\t\tsgl_sdesc->addr);\n\t\tskb_orphan(skb);\n\t\tsgl_sdesc->skb = skb;\n\t}\n\n\ttxq_advance(&q->q, ndesc);\n\n\tcxgb4_ring_tx_db(adap, &q->q, ndesc);\n\treturn NETDEV_TX_OK;\n\nout_free:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\n \nenum {\n\t \n\tEQ_UNIT = SGE_EQ_IDXSIZE,\n\tFL_PER_EQ_UNIT = EQ_UNIT / sizeof(__be64),\n\tTXD_PER_EQ_UNIT = EQ_UNIT / sizeof(__be64),\n\n\tT4VF_ETHTXQ_MAX_HDR = (sizeof(struct fw_eth_tx_pkt_vm_wr) +\n\t\t\t       sizeof(struct cpl_tx_pkt_lso_core) +\n\t\t\t       sizeof(struct cpl_tx_pkt_core)) / sizeof(__be64),\n};\n\n \nstatic inline int t4vf_is_eth_imm(const struct sk_buff *skb)\n{\n\t \n\treturn false;\n}\n\n \nstatic inline unsigned int t4vf_calc_tx_flits(const struct sk_buff *skb)\n{\n\tunsigned int flits;\n\n\t \n\tif (t4vf_is_eth_imm(skb))\n\t\treturn DIV_ROUND_UP(skb->len + sizeof(struct cpl_tx_pkt),\n\t\t\t\t    sizeof(__be64));\n\n\t \n\tflits = sgl_len(skb_shinfo(skb)->nr_frags + 1);\n\tif (skb_shinfo(skb)->gso_size)\n\t\tflits += (sizeof(struct fw_eth_tx_pkt_vm_wr) +\n\t\t\t  sizeof(struct cpl_tx_pkt_lso_core) +\n\t\t\t  sizeof(struct cpl_tx_pkt_core)) / sizeof(__be64);\n\telse\n\t\tflits += (sizeof(struct fw_eth_tx_pkt_vm_wr) +\n\t\t\t  sizeof(struct cpl_tx_pkt_core)) / sizeof(__be64);\n\treturn flits;\n}\n\n \nstatic netdev_tx_t cxgb4_vf_eth_xmit(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev)\n{\n\tunsigned int last_desc, flits, ndesc;\n\tconst struct skb_shared_info *ssi;\n\tstruct fw_eth_tx_pkt_vm_wr *wr;\n\tstruct tx_sw_desc *sgl_sdesc;\n\tstruct cpl_tx_pkt_core *cpl;\n\tconst struct port_info *pi;\n\tstruct sge_eth_txq *txq;\n\tstruct adapter *adapter;\n\tint qidx, credits, ret;\n\tsize_t fw_hdr_copy_len;\n\tunsigned int chip_ver;\n\tu64 cntrl, *end;\n\tu32 wr_mid;\n\n\t \n\tBUILD_BUG_ON(sizeof(wr->firmware) !=\n\t\t     (sizeof(wr->ethmacdst) + sizeof(wr->ethmacsrc) +\n\t\t      sizeof(wr->ethtype) + sizeof(wr->vlantci)));\n\tfw_hdr_copy_len = sizeof(wr->firmware);\n\tret = cxgb4_validate_skb(skb, dev, fw_hdr_copy_len);\n\tif (ret)\n\t\tgoto out_free;\n\n\t \n\tpi = netdev_priv(dev);\n\tadapter = pi->adapter;\n\tqidx = skb_get_queue_mapping(skb);\n\tWARN_ON(qidx >= pi->nqsets);\n\ttxq = &adapter->sge.ethtxq[pi->first_qset + qidx];\n\n\t \n\treclaim_completed_tx(adapter, &txq->q, -1, true);\n\n\t \n\tflits = t4vf_calc_tx_flits(skb);\n\tndesc = flits_to_desc(flits);\n\tcredits = txq_avail(&txq->q) - ndesc;\n\n\tif (unlikely(credits < 0)) {\n\t\t \n\t\teth_txq_stop(txq);\n\t\tdev_err(adapter->pdev_dev,\n\t\t\t\"%s: TX ring %u full while queue awake!\\n\",\n\t\t\tdev->name, qidx);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tlast_desc = txq->q.pidx + ndesc - 1;\n\tif (last_desc >= txq->q.size)\n\t\tlast_desc -= txq->q.size;\n\tsgl_sdesc = &txq->q.sdesc[last_desc];\n\n\tif (!t4vf_is_eth_imm(skb) &&\n\t    unlikely(cxgb4_map_skb(adapter->pdev_dev, skb,\n\t\t\t\t   sgl_sdesc->addr) < 0)) {\n\t\t \n\t\tmemset(sgl_sdesc->addr, 0, sizeof(sgl_sdesc->addr));\n\t\ttxq->mapping_err++;\n\t\tgoto out_free;\n\t}\n\n\tchip_ver = CHELSIO_CHIP_VERSION(adapter->params.chip);\n\twr_mid = FW_WR_LEN16_V(DIV_ROUND_UP(flits, 2));\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\t \n\t\teth_txq_stop(txq);\n\t\tif (chip_ver > CHELSIO_T5)\n\t\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\n\t \n\tWARN_ON(DIV_ROUND_UP(T4VF_ETHTXQ_MAX_HDR, TXD_PER_EQ_UNIT) > 1);\n\twr = (void *)&txq->q.desc[txq->q.pidx];\n\twr->equiq_to_len16 = cpu_to_be32(wr_mid);\n\twr->r3[0] = cpu_to_be32(0);\n\twr->r3[1] = cpu_to_be32(0);\n\tskb_copy_from_linear_data(skb, &wr->firmware, fw_hdr_copy_len);\n\tend = (u64 *)wr + flits;\n\n\t \n\tssi = skb_shinfo(skb);\n\tif (ssi->gso_size) {\n\t\tstruct cpl_tx_pkt_lso_core *lso = (void *)(wr + 1);\n\t\tbool v6 = (ssi->gso_type & SKB_GSO_TCPV6) != 0;\n\t\tint l3hdr_len = skb_network_header_len(skb);\n\t\tint eth_xtra_len = skb_network_offset(skb) - ETH_HLEN;\n\n\t\twr->op_immdlen =\n\t\t\tcpu_to_be32(FW_WR_OP_V(FW_ETH_TX_PKT_VM_WR) |\n\t\t\t\t    FW_WR_IMMDLEN_V(sizeof(*lso) +\n\t\t\t\t\t\t    sizeof(*cpl)));\n\t\t  \n\t\tlso->lso_ctrl =\n\t\t\tcpu_to_be32(LSO_OPCODE_V(CPL_TX_PKT_LSO) |\n\t\t\t\t    LSO_FIRST_SLICE_F |\n\t\t\t\t    LSO_LAST_SLICE_F |\n\t\t\t\t    LSO_IPV6_V(v6) |\n\t\t\t\t    LSO_ETHHDR_LEN_V(eth_xtra_len / 4) |\n\t\t\t\t    LSO_IPHDR_LEN_V(l3hdr_len / 4) |\n\t\t\t\t    LSO_TCPHDR_LEN_V(tcp_hdr(skb)->doff));\n\t\tlso->ipid_ofst = cpu_to_be16(0);\n\t\tlso->mss = cpu_to_be16(ssi->gso_size);\n\t\tlso->seqno_offset = cpu_to_be32(0);\n\t\tif (is_t4(adapter->params.chip))\n\t\t\tlso->len = cpu_to_be32(skb->len);\n\t\telse\n\t\t\tlso->len = cpu_to_be32(LSO_T5_XFER_SIZE_V(skb->len));\n\n\t\t \n\t\tcpl = (void *)(lso + 1);\n\n\t\tif (chip_ver <= CHELSIO_T5)\n\t\t\tcntrl = TXPKT_ETHHDR_LEN_V(eth_xtra_len);\n\t\telse\n\t\t\tcntrl = T6_TXPKT_ETHHDR_LEN_V(eth_xtra_len);\n\n\t\tcntrl |= TXPKT_CSUM_TYPE_V(v6 ?\n\t\t\t\t\t   TX_CSUM_TCPIP6 : TX_CSUM_TCPIP) |\n\t\t\t TXPKT_IPHDR_LEN_V(l3hdr_len);\n\t\ttxq->tso++;\n\t\ttxq->tx_cso += ssi->gso_segs;\n\t} else {\n\t\tint len;\n\n\t\tlen = (t4vf_is_eth_imm(skb)\n\t\t       ? skb->len + sizeof(*cpl)\n\t\t       : sizeof(*cpl));\n\t\twr->op_immdlen =\n\t\t\tcpu_to_be32(FW_WR_OP_V(FW_ETH_TX_PKT_VM_WR) |\n\t\t\t\t    FW_WR_IMMDLEN_V(len));\n\n\t\t \n\t\tcpl = (void *)(wr + 1);\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tcntrl = hwcsum(adapter->params.chip, skb) |\n\t\t\t\tTXPKT_IPCSUM_DIS_F;\n\t\t\ttxq->tx_cso++;\n\t\t} else {\n\t\t\tcntrl = TXPKT_L4CSUM_DIS_F | TXPKT_IPCSUM_DIS_F;\n\t\t}\n\t}\n\n\t \n\tif (skb_vlan_tag_present(skb)) {\n\t\ttxq->vlan_ins++;\n\t\tcntrl |= TXPKT_VLAN_VLD_F | TXPKT_VLAN_V(skb_vlan_tag_get(skb));\n\t}\n\n\t  \n\tcpl->ctrl0 = cpu_to_be32(TXPKT_OPCODE_V(CPL_TX_PKT_XT) |\n\t\t\t\t TXPKT_INTF_V(pi->port_id) |\n\t\t\t\t TXPKT_PF_V(0));\n\tcpl->pack = cpu_to_be16(0);\n\tcpl->len = cpu_to_be16(skb->len);\n\tcpl->ctrl1 = cpu_to_be64(cntrl);\n\n\t \n\tif (t4vf_is_eth_imm(skb)) {\n\t\t \n\t\tcxgb4_inline_tx_skb(skb, &txq->q, cpl + 1);\n\t\tdev_consume_skb_any(skb);\n\t} else {\n\t\t \n\t\tstruct ulptx_sgl *sgl = (struct ulptx_sgl *)(cpl + 1);\n\t\tstruct sge_txq *tq = &txq->q;\n\n\t\t \n\t\tif (unlikely((void *)sgl == (void *)tq->stat)) {\n\t\t\tsgl = (void *)tq->desc;\n\t\t\tend = (void *)((void *)tq->desc +\n\t\t\t\t       ((void *)end - (void *)tq->stat));\n\t\t}\n\n\t\tcxgb4_write_sgl(skb, tq, sgl, end, 0, sgl_sdesc->addr);\n\t\tskb_orphan(skb);\n\t\tsgl_sdesc->skb = skb;\n\t}\n\n\t \n\ttxq_advance(&txq->q, ndesc);\n\n\tcxgb4_ring_tx_db(adapter, &txq->q, ndesc);\n\treturn NETDEV_TX_OK;\n\nout_free:\n\t \n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic inline void reclaim_completed_tx_imm(struct sge_txq *q)\n{\n\tint hw_cidx = ntohs(READ_ONCE(q->stat->cidx));\n\tint reclaim = hw_cidx - q->cidx;\n\n\tif (reclaim < 0)\n\t\treclaim += q->size;\n\n\tq->in_use -= reclaim;\n\tq->cidx = hw_cidx;\n}\n\nstatic inline void eosw_txq_advance_index(u32 *idx, u32 n, u32 max)\n{\n\tu32 val = *idx + n;\n\n\tif (val >= max)\n\t\tval -= max;\n\n\t*idx = val;\n}\n\nvoid cxgb4_eosw_txq_free_desc(struct adapter *adap,\n\t\t\t      struct sge_eosw_txq *eosw_txq, u32 ndesc)\n{\n\tstruct tx_sw_desc *d;\n\n\td = &eosw_txq->desc[eosw_txq->last_cidx];\n\twhile (ndesc--) {\n\t\tif (d->skb) {\n\t\t\tif (d->addr[0]) {\n\t\t\t\tunmap_skb(adap->pdev_dev, d->skb, d->addr);\n\t\t\t\tmemset(d->addr, 0, sizeof(d->addr));\n\t\t\t}\n\t\t\tdev_consume_skb_any(d->skb);\n\t\t\td->skb = NULL;\n\t\t}\n\t\teosw_txq_advance_index(&eosw_txq->last_cidx, 1,\n\t\t\t\t       eosw_txq->ndesc);\n\t\td = &eosw_txq->desc[eosw_txq->last_cidx];\n\t}\n}\n\nstatic inline void eosw_txq_advance(struct sge_eosw_txq *eosw_txq, u32 n)\n{\n\teosw_txq_advance_index(&eosw_txq->pidx, n, eosw_txq->ndesc);\n\teosw_txq->inuse += n;\n}\n\nstatic inline int eosw_txq_enqueue(struct sge_eosw_txq *eosw_txq,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tif (eosw_txq->inuse == eosw_txq->ndesc)\n\t\treturn -ENOMEM;\n\n\teosw_txq->desc[eosw_txq->pidx].skb = skb;\n\treturn 0;\n}\n\nstatic inline struct sk_buff *eosw_txq_peek(struct sge_eosw_txq *eosw_txq)\n{\n\treturn eosw_txq->desc[eosw_txq->last_pidx].skb;\n}\n\nstatic inline u8 ethofld_calc_tx_flits(struct adapter *adap,\n\t\t\t\t       struct sk_buff *skb, u32 hdr_len)\n{\n\tu8 flits, nsgl = 0;\n\tu32 wrlen;\n\n\twrlen = sizeof(struct fw_eth_tx_eo_wr) + sizeof(struct cpl_tx_pkt_core);\n\tif (skb_shinfo(skb)->gso_size &&\n\t    !(skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4))\n\t\twrlen += sizeof(struct cpl_tx_pkt_lso_core);\n\n\twrlen += roundup(hdr_len, 16);\n\n\t \n\tflits = DIV_ROUND_UP(wrlen, 8);\n\n\tif (skb_shinfo(skb)->nr_frags > 0) {\n\t\tif (skb_headlen(skb) - hdr_len)\n\t\t\tnsgl = sgl_len(skb_shinfo(skb)->nr_frags + 1);\n\t\telse\n\t\t\tnsgl = sgl_len(skb_shinfo(skb)->nr_frags);\n\t} else if (skb->len - hdr_len) {\n\t\tnsgl = sgl_len(1);\n\t}\n\n\treturn flits + nsgl;\n}\n\nstatic void *write_eo_wr(struct adapter *adap, struct sge_eosw_txq *eosw_txq,\n\t\t\t struct sk_buff *skb, struct fw_eth_tx_eo_wr *wr,\n\t\t\t u32 hdr_len, u32 wrlen)\n{\n\tconst struct skb_shared_info *ssi = skb_shinfo(skb);\n\tstruct cpl_tx_pkt_core *cpl;\n\tu32 immd_len, wrlen16;\n\tbool compl = false;\n\tu8 ver, proto;\n\n\tver = ip_hdr(skb)->version;\n\tproto = (ver == 6) ? ipv6_hdr(skb)->nexthdr : ip_hdr(skb)->protocol;\n\n\twrlen16 = DIV_ROUND_UP(wrlen, 16);\n\timmd_len = sizeof(struct cpl_tx_pkt_core);\n\tif (skb_shinfo(skb)->gso_size &&\n\t    !(skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4))\n\t\timmd_len += sizeof(struct cpl_tx_pkt_lso_core);\n\timmd_len += hdr_len;\n\n\tif (!eosw_txq->ncompl ||\n\t    (eosw_txq->last_compl + wrlen16) >=\n\t    (adap->params.ofldq_wr_cred / 2)) {\n\t\tcompl = true;\n\t\teosw_txq->ncompl++;\n\t\teosw_txq->last_compl = 0;\n\t}\n\n\twr->op_immdlen = cpu_to_be32(FW_WR_OP_V(FW_ETH_TX_EO_WR) |\n\t\t\t\t     FW_ETH_TX_EO_WR_IMMDLEN_V(immd_len) |\n\t\t\t\t     FW_WR_COMPL_V(compl));\n\twr->equiq_to_len16 = cpu_to_be32(FW_WR_LEN16_V(wrlen16) |\n\t\t\t\t\t FW_WR_FLOWID_V(eosw_txq->hwtid));\n\twr->r3 = 0;\n\tif (proto == IPPROTO_UDP) {\n\t\tcpl = write_eo_udp_wr(skb, wr, hdr_len);\n\t} else {\n\t\twr->u.tcpseg.type = FW_ETH_TX_EO_TYPE_TCPSEG;\n\t\twr->u.tcpseg.ethlen = skb_network_offset(skb);\n\t\twr->u.tcpseg.iplen = cpu_to_be16(skb_network_header_len(skb));\n\t\twr->u.tcpseg.tcplen = tcp_hdrlen(skb);\n\t\twr->u.tcpseg.tsclk_tsoff = 0;\n\t\twr->u.tcpseg.r4 = 0;\n\t\twr->u.tcpseg.r5 = 0;\n\t\twr->u.tcpseg.plen = cpu_to_be32(skb->len - hdr_len);\n\n\t\tif (ssi->gso_size) {\n\t\t\tstruct cpl_tx_pkt_lso_core *lso = (void *)(wr + 1);\n\n\t\t\twr->u.tcpseg.mss = cpu_to_be16(ssi->gso_size);\n\t\t\tcpl = write_tso_wr(adap, skb, lso);\n\t\t} else {\n\t\t\twr->u.tcpseg.mss = cpu_to_be16(0xffff);\n\t\t\tcpl = (void *)(wr + 1);\n\t\t}\n\t}\n\n\teosw_txq->cred -= wrlen16;\n\teosw_txq->last_compl += wrlen16;\n\treturn cpl;\n}\n\nstatic int ethofld_hard_xmit(struct net_device *dev,\n\t\t\t     struct sge_eosw_txq *eosw_txq)\n{\n\tstruct port_info *pi = netdev2pinfo(dev);\n\tstruct adapter *adap = netdev2adap(dev);\n\tu32 wrlen, wrlen16, hdr_len, data_len;\n\tenum sge_eosw_state next_state;\n\tu64 cntrl, *start, *end, *sgl;\n\tstruct sge_eohw_txq *eohw_txq;\n\tstruct cpl_tx_pkt_core *cpl;\n\tstruct fw_eth_tx_eo_wr *wr;\n\tbool skip_eotx_wr = false;\n\tstruct tx_sw_desc *d;\n\tstruct sk_buff *skb;\n\tint left, ret = 0;\n\tu8 flits, ndesc;\n\n\teohw_txq = &adap->sge.eohw_txq[eosw_txq->hwqid];\n\tspin_lock(&eohw_txq->lock);\n\treclaim_completed_tx_imm(&eohw_txq->q);\n\n\td = &eosw_txq->desc[eosw_txq->last_pidx];\n\tskb = d->skb;\n\tskb_tx_timestamp(skb);\n\n\twr = (struct fw_eth_tx_eo_wr *)&eohw_txq->q.desc[eohw_txq->q.pidx];\n\tif (unlikely(eosw_txq->state != CXGB4_EO_STATE_ACTIVE &&\n\t\t     eosw_txq->last_pidx == eosw_txq->flowc_idx)) {\n\t\thdr_len = skb->len;\n\t\tdata_len = 0;\n\t\tflits = DIV_ROUND_UP(hdr_len, 8);\n\t\tif (eosw_txq->state == CXGB4_EO_STATE_FLOWC_OPEN_SEND)\n\t\t\tnext_state = CXGB4_EO_STATE_FLOWC_OPEN_REPLY;\n\t\telse\n\t\t\tnext_state = CXGB4_EO_STATE_FLOWC_CLOSE_REPLY;\n\t\tskip_eotx_wr = true;\n\t} else {\n\t\thdr_len = eth_get_headlen(dev, skb->data, skb_headlen(skb));\n\t\tdata_len = skb->len - hdr_len;\n\t\tflits = ethofld_calc_tx_flits(adap, skb, hdr_len);\n\t}\n\tndesc = flits_to_desc(flits);\n\twrlen = flits * 8;\n\twrlen16 = DIV_ROUND_UP(wrlen, 16);\n\n\tleft = txq_avail(&eohw_txq->q) - ndesc;\n\n\t \n\tif (unlikely(left < 0 || wrlen16 > eosw_txq->cred)) {\n\t\tret = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (unlikely(skip_eotx_wr)) {\n\t\tstart = (u64 *)wr;\n\t\teosw_txq->state = next_state;\n\t\teosw_txq->cred -= wrlen16;\n\t\teosw_txq->ncompl++;\n\t\teosw_txq->last_compl = 0;\n\t\tgoto write_wr_headers;\n\t}\n\n\tcpl = write_eo_wr(adap, eosw_txq, skb, wr, hdr_len, wrlen);\n\tcntrl = hwcsum(adap->params.chip, skb);\n\tif (skb_vlan_tag_present(skb))\n\t\tcntrl |= TXPKT_VLAN_VLD_F | TXPKT_VLAN_V(skb_vlan_tag_get(skb));\n\n\tcpl->ctrl0 = cpu_to_be32(TXPKT_OPCODE_V(CPL_TX_PKT_XT) |\n\t\t\t\t TXPKT_INTF_V(pi->tx_chan) |\n\t\t\t\t TXPKT_PF_V(adap->pf));\n\tcpl->pack = 0;\n\tcpl->len = cpu_to_be16(skb->len);\n\tcpl->ctrl1 = cpu_to_be64(cntrl);\n\n\tstart = (u64 *)(cpl + 1);\n\nwrite_wr_headers:\n\tsgl = (u64 *)inline_tx_skb_header(skb, &eohw_txq->q, (void *)start,\n\t\t\t\t\t  hdr_len);\n\tif (data_len) {\n\t\tret = cxgb4_map_skb(adap->pdev_dev, skb, d->addr);\n\t\tif (unlikely(ret)) {\n\t\t\tmemset(d->addr, 0, sizeof(d->addr));\n\t\t\teohw_txq->mapping_err++;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tend = (u64 *)wr + flits;\n\t\tif (unlikely(start > sgl)) {\n\t\t\tleft = (u8 *)end - (u8 *)eohw_txq->q.stat;\n\t\t\tend = (void *)eohw_txq->q.desc + left;\n\t\t}\n\n\t\tif (unlikely((u8 *)sgl >= (u8 *)eohw_txq->q.stat)) {\n\t\t\t \n\t\t\tleft = (u8 *)end - (u8 *)eohw_txq->q.stat;\n\n\t\t\tend = (void *)eohw_txq->q.desc + left;\n\t\t\tsgl = (void *)eohw_txq->q.desc;\n\t\t}\n\n\t\tcxgb4_write_sgl(skb, &eohw_txq->q, (void *)sgl, end, hdr_len,\n\t\t\t\td->addr);\n\t}\n\n\tif (skb_shinfo(skb)->gso_size) {\n\t\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4)\n\t\t\teohw_txq->uso++;\n\t\telse\n\t\t\teohw_txq->tso++;\n\t\teohw_txq->tx_cso += skb_shinfo(skb)->gso_segs;\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\teohw_txq->tx_cso++;\n\t}\n\n\tif (skb_vlan_tag_present(skb))\n\t\teohw_txq->vlan_ins++;\n\n\ttxq_advance(&eohw_txq->q, ndesc);\n\tcxgb4_ring_tx_db(adap, &eohw_txq->q, ndesc);\n\teosw_txq_advance_index(&eosw_txq->last_pidx, 1, eosw_txq->ndesc);\n\nout_unlock:\n\tspin_unlock(&eohw_txq->lock);\n\treturn ret;\n}\n\nstatic void ethofld_xmit(struct net_device *dev, struct sge_eosw_txq *eosw_txq)\n{\n\tstruct sk_buff *skb;\n\tint pktcount, ret;\n\n\tswitch (eosw_txq->state) {\n\tcase CXGB4_EO_STATE_ACTIVE:\n\tcase CXGB4_EO_STATE_FLOWC_OPEN_SEND:\n\tcase CXGB4_EO_STATE_FLOWC_CLOSE_SEND:\n\t\tpktcount = eosw_txq->pidx - eosw_txq->last_pidx;\n\t\tif (pktcount < 0)\n\t\t\tpktcount += eosw_txq->ndesc;\n\t\tbreak;\n\tcase CXGB4_EO_STATE_FLOWC_OPEN_REPLY:\n\tcase CXGB4_EO_STATE_FLOWC_CLOSE_REPLY:\n\tcase CXGB4_EO_STATE_CLOSED:\n\tdefault:\n\t\treturn;\n\t}\n\n\twhile (pktcount--) {\n\t\tskb = eosw_txq_peek(eosw_txq);\n\t\tif (!skb) {\n\t\t\teosw_txq_advance_index(&eosw_txq->last_pidx, 1,\n\t\t\t\t\t       eosw_txq->ndesc);\n\t\t\tcontinue;\n\t\t}\n\n\t\tret = ethofld_hard_xmit(dev, eosw_txq);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n}\n\nstatic netdev_tx_t cxgb4_ethofld_xmit(struct sk_buff *skb,\n\t\t\t\t      struct net_device *dev)\n{\n\tstruct cxgb4_tc_port_mqprio *tc_port_mqprio;\n\tstruct port_info *pi = netdev2pinfo(dev);\n\tstruct adapter *adap = netdev2adap(dev);\n\tstruct sge_eosw_txq *eosw_txq;\n\tu32 qid;\n\tint ret;\n\n\tret = cxgb4_validate_skb(skb, dev, ETH_HLEN);\n\tif (ret)\n\t\tgoto out_free;\n\n\ttc_port_mqprio = &adap->tc_mqprio->port_mqprio[pi->port_id];\n\tqid = skb_get_queue_mapping(skb) - pi->nqsets;\n\teosw_txq = &tc_port_mqprio->eosw_txq[qid];\n\tspin_lock_bh(&eosw_txq->lock);\n\tif (eosw_txq->state != CXGB4_EO_STATE_ACTIVE)\n\t\tgoto out_unlock;\n\n\tret = eosw_txq_enqueue(eosw_txq, skb);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t \n\tskb_orphan(skb);\n\n\teosw_txq_advance(eosw_txq, 1);\n\tethofld_xmit(dev, eosw_txq);\n\tspin_unlock_bh(&eosw_txq->lock);\n\treturn NETDEV_TX_OK;\n\nout_unlock:\n\tspin_unlock_bh(&eosw_txq->lock);\nout_free:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\nnetdev_tx_t t4_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct port_info *pi = netdev_priv(dev);\n\tu16 qid = skb_get_queue_mapping(skb);\n\n\tif (unlikely(pi->eth_flags & PRIV_FLAG_PORT_TX_VM))\n\t\treturn cxgb4_vf_eth_xmit(skb, dev);\n\n\tif (unlikely(qid >= pi->nqsets))\n\t\treturn cxgb4_ethofld_xmit(skb, dev);\n\n\tif (is_ptp_enabled(skb, dev)) {\n\t\tstruct adapter *adap = netdev2adap(dev);\n\t\tnetdev_tx_t ret;\n\n\t\tspin_lock(&adap->ptp_lock);\n\t\tret = cxgb4_eth_xmit(skb, dev);\n\t\tspin_unlock(&adap->ptp_lock);\n\t\treturn ret;\n\t}\n\n\treturn cxgb4_eth_xmit(skb, dev);\n}\n\nstatic void eosw_txq_flush_pending_skbs(struct sge_eosw_txq *eosw_txq)\n{\n\tint pktcount = eosw_txq->pidx - eosw_txq->last_pidx;\n\tint pidx = eosw_txq->pidx;\n\tstruct sk_buff *skb;\n\n\tif (!pktcount)\n\t\treturn;\n\n\tif (pktcount < 0)\n\t\tpktcount += eosw_txq->ndesc;\n\n\twhile (pktcount--) {\n\t\tpidx--;\n\t\tif (pidx < 0)\n\t\t\tpidx += eosw_txq->ndesc;\n\n\t\tskb = eosw_txq->desc[pidx].skb;\n\t\tif (skb) {\n\t\t\tdev_consume_skb_any(skb);\n\t\t\teosw_txq->desc[pidx].skb = NULL;\n\t\t\teosw_txq->inuse--;\n\t\t}\n\t}\n\n\teosw_txq->pidx = eosw_txq->last_pidx + 1;\n}\n\n \nint cxgb4_ethofld_send_flowc(struct net_device *dev, u32 eotid, u32 tc)\n{\n\tstruct port_info *pi = netdev2pinfo(dev);\n\tstruct adapter *adap = netdev2adap(dev);\n\tenum sge_eosw_state next_state;\n\tstruct sge_eosw_txq *eosw_txq;\n\tu32 len, len16, nparams = 6;\n\tstruct fw_flowc_wr *flowc;\n\tstruct eotid_entry *entry;\n\tstruct sge_ofld_rxq *rxq;\n\tstruct sk_buff *skb;\n\tint ret = 0;\n\n\tlen = struct_size(flowc, mnemval, nparams);\n\tlen16 = DIV_ROUND_UP(len, 16);\n\n\tentry = cxgb4_lookup_eotid(&adap->tids, eotid);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\teosw_txq = (struct sge_eosw_txq *)entry->data;\n\tif (!eosw_txq)\n\t\treturn -ENOMEM;\n\n\tif (!(adap->flags & CXGB4_FW_OK)) {\n\t\t \n\t\tcomplete(&eosw_txq->completion);\n\t\treturn -EIO;\n\t}\n\n\tskb = alloc_skb(len, GFP_KERNEL);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tspin_lock_bh(&eosw_txq->lock);\n\tif (tc != FW_SCHED_CLS_NONE) {\n\t\tif (eosw_txq->state != CXGB4_EO_STATE_CLOSED)\n\t\t\tgoto out_free_skb;\n\n\t\tnext_state = CXGB4_EO_STATE_FLOWC_OPEN_SEND;\n\t} else {\n\t\tif (eosw_txq->state != CXGB4_EO_STATE_ACTIVE)\n\t\t\tgoto out_free_skb;\n\n\t\tnext_state = CXGB4_EO_STATE_FLOWC_CLOSE_SEND;\n\t}\n\n\tflowc = __skb_put(skb, len);\n\tmemset(flowc, 0, len);\n\n\trxq = &adap->sge.eohw_rxq[eosw_txq->hwqid];\n\tflowc->flowid_len16 = cpu_to_be32(FW_WR_LEN16_V(len16) |\n\t\t\t\t\t  FW_WR_FLOWID_V(eosw_txq->hwtid));\n\tflowc->op_to_nparams = cpu_to_be32(FW_WR_OP_V(FW_FLOWC_WR) |\n\t\t\t\t\t   FW_FLOWC_WR_NPARAMS_V(nparams) |\n\t\t\t\t\t   FW_WR_COMPL_V(1));\n\tflowc->mnemval[0].mnemonic = FW_FLOWC_MNEM_PFNVFN;\n\tflowc->mnemval[0].val = cpu_to_be32(FW_PFVF_CMD_PFN_V(adap->pf));\n\tflowc->mnemval[1].mnemonic = FW_FLOWC_MNEM_CH;\n\tflowc->mnemval[1].val = cpu_to_be32(pi->tx_chan);\n\tflowc->mnemval[2].mnemonic = FW_FLOWC_MNEM_PORT;\n\tflowc->mnemval[2].val = cpu_to_be32(pi->tx_chan);\n\tflowc->mnemval[3].mnemonic = FW_FLOWC_MNEM_IQID;\n\tflowc->mnemval[3].val = cpu_to_be32(rxq->rspq.abs_id);\n\tflowc->mnemval[4].mnemonic = FW_FLOWC_MNEM_SCHEDCLASS;\n\tflowc->mnemval[4].val = cpu_to_be32(tc);\n\tflowc->mnemval[5].mnemonic = FW_FLOWC_MNEM_EOSTATE;\n\tflowc->mnemval[5].val = cpu_to_be32(tc == FW_SCHED_CLS_NONE ?\n\t\t\t\t\t    FW_FLOWC_MNEM_EOSTATE_CLOSING :\n\t\t\t\t\t    FW_FLOWC_MNEM_EOSTATE_ESTABLISHED);\n\n\t \n\tif (tc == FW_SCHED_CLS_NONE)\n\t\teosw_txq_flush_pending_skbs(eosw_txq);\n\n\tret = eosw_txq_enqueue(eosw_txq, skb);\n\tif (ret)\n\t\tgoto out_free_skb;\n\n\teosw_txq->state = next_state;\n\teosw_txq->flowc_idx = eosw_txq->pidx;\n\teosw_txq_advance(eosw_txq, 1);\n\tethofld_xmit(dev, eosw_txq);\n\n\tspin_unlock_bh(&eosw_txq->lock);\n\treturn 0;\n\nout_free_skb:\n\tdev_consume_skb_any(skb);\n\tspin_unlock_bh(&eosw_txq->lock);\n\treturn ret;\n}\n\n \nstatic inline int is_imm(const struct sk_buff *skb)\n{\n\treturn skb->len <= MAX_CTRL_WR_LEN;\n}\n\n \nstatic void ctrlq_check_stop(struct sge_ctrl_txq *q, struct fw_wr_hdr *wr)\n{\n\treclaim_completed_tx_imm(&q->q);\n\tif (unlikely(txq_avail(&q->q) < TXQ_STOP_THRES)) {\n\t\twr->lo |= htonl(FW_WR_EQUEQ_F | FW_WR_EQUIQ_F);\n\t\tq->q.stops++;\n\t\tq->full = 1;\n\t}\n}\n\n#define CXGB4_SELFTEST_LB_STR \"CHELSIO_SELFTEST\"\n\nint cxgb4_selftest_lb_pkt(struct net_device *netdev)\n{\n\tstruct port_info *pi = netdev_priv(netdev);\n\tstruct adapter *adap = pi->adapter;\n\tstruct cxgb4_ethtool_lb_test *lb;\n\tint ret, i = 0, pkt_len, credits;\n\tstruct fw_eth_tx_pkt_wr *wr;\n\tstruct cpl_tx_pkt_core *cpl;\n\tu32 ctrl0, ndesc, flits;\n\tstruct sge_eth_txq *q;\n\tu8 *sgl;\n\n\tpkt_len = ETH_HLEN + sizeof(CXGB4_SELFTEST_LB_STR);\n\n\tflits = DIV_ROUND_UP(pkt_len + sizeof(*cpl) + sizeof(*wr),\n\t\t\t     sizeof(__be64));\n\tndesc = flits_to_desc(flits);\n\n\tlb = &pi->ethtool_lb;\n\tlb->loopback = 1;\n\n\tq = &adap->sge.ethtxq[pi->first_qset];\n\t__netif_tx_lock(q->txq, smp_processor_id());\n\n\treclaim_completed_tx(adap, &q->q, -1, true);\n\tcredits = txq_avail(&q->q) - ndesc;\n\tif (unlikely(credits < 0)) {\n\t\t__netif_tx_unlock(q->txq);\n\t\treturn -ENOMEM;\n\t}\n\n\twr = (void *)&q->q.desc[q->q.pidx];\n\tmemset(wr, 0, sizeof(struct tx_desc));\n\n\twr->op_immdlen = htonl(FW_WR_OP_V(FW_ETH_TX_PKT_WR) |\n\t\t\t       FW_WR_IMMDLEN_V(pkt_len +\n\t\t\t       sizeof(*cpl)));\n\twr->equiq_to_len16 = htonl(FW_WR_LEN16_V(DIV_ROUND_UP(flits, 2)));\n\twr->r3 = cpu_to_be64(0);\n\n\tcpl = (void *)(wr + 1);\n\tsgl = (u8 *)(cpl + 1);\n\n\tctrl0 = TXPKT_OPCODE_V(CPL_TX_PKT_XT) | TXPKT_PF_V(adap->pf) |\n\t\tTXPKT_INTF_V(pi->tx_chan + 4);\n\n\tcpl->ctrl0 = htonl(ctrl0);\n\tcpl->pack = htons(0);\n\tcpl->len = htons(pkt_len);\n\tcpl->ctrl1 = cpu_to_be64(TXPKT_L4CSUM_DIS_F | TXPKT_IPCSUM_DIS_F);\n\n\teth_broadcast_addr(sgl);\n\ti += ETH_ALEN;\n\tether_addr_copy(&sgl[i], netdev->dev_addr);\n\ti += ETH_ALEN;\n\n\tsnprintf(&sgl[i], sizeof(CXGB4_SELFTEST_LB_STR), \"%s\",\n\t\t CXGB4_SELFTEST_LB_STR);\n\n\tinit_completion(&lb->completion);\n\ttxq_advance(&q->q, ndesc);\n\tcxgb4_ring_tx_db(adap, &q->q, ndesc);\n\t__netif_tx_unlock(q->txq);\n\n\t \n\tret = wait_for_completion_timeout(&lb->completion, 10 * HZ);\n\tif (!ret)\n\t\tret = -ETIMEDOUT;\n\telse\n\t\tret = lb->result;\n\n\tlb->loopback = 0;\n\n\treturn ret;\n}\n\n \nstatic int ctrl_xmit(struct sge_ctrl_txq *q, struct sk_buff *skb)\n{\n\tunsigned int ndesc;\n\tstruct fw_wr_hdr *wr;\n\n\tif (unlikely(!is_imm(skb))) {\n\t\tWARN_ON(1);\n\t\tdev_kfree_skb(skb);\n\t\treturn NET_XMIT_DROP;\n\t}\n\n\tndesc = DIV_ROUND_UP(skb->len, sizeof(struct tx_desc));\n\tspin_lock(&q->sendq.lock);\n\n\tif (unlikely(q->full)) {\n\t\tskb->priority = ndesc;                   \n\t\t__skb_queue_tail(&q->sendq, skb);\n\t\tspin_unlock(&q->sendq.lock);\n\t\treturn NET_XMIT_CN;\n\t}\n\n\twr = (struct fw_wr_hdr *)&q->q.desc[q->q.pidx];\n\tcxgb4_inline_tx_skb(skb, &q->q, wr);\n\n\ttxq_advance(&q->q, ndesc);\n\tif (unlikely(txq_avail(&q->q) < TXQ_STOP_THRES))\n\t\tctrlq_check_stop(q, wr);\n\n\tcxgb4_ring_tx_db(q->adap, &q->q, ndesc);\n\tspin_unlock(&q->sendq.lock);\n\n\tkfree_skb(skb);\n\treturn NET_XMIT_SUCCESS;\n}\n\n \nstatic void restart_ctrlq(struct tasklet_struct *t)\n{\n\tstruct sk_buff *skb;\n\tunsigned int written = 0;\n\tstruct sge_ctrl_txq *q = from_tasklet(q, t, qresume_tsk);\n\n\tspin_lock(&q->sendq.lock);\n\treclaim_completed_tx_imm(&q->q);\n\tBUG_ON(txq_avail(&q->q) < TXQ_STOP_THRES);   \n\n\twhile ((skb = __skb_dequeue(&q->sendq)) != NULL) {\n\t\tstruct fw_wr_hdr *wr;\n\t\tunsigned int ndesc = skb->priority;      \n\n\t\twritten += ndesc;\n\t\t \n\t\twr = (struct fw_wr_hdr *)&q->q.desc[q->q.pidx];\n\t\ttxq_advance(&q->q, ndesc);\n\t\tspin_unlock(&q->sendq.lock);\n\n\t\tcxgb4_inline_tx_skb(skb, &q->q, wr);\n\t\tkfree_skb(skb);\n\n\t\tif (unlikely(txq_avail(&q->q) < TXQ_STOP_THRES)) {\n\t\t\tunsigned long old = q->q.stops;\n\n\t\t\tctrlq_check_stop(q, wr);\n\t\t\tif (q->q.stops != old) {           \n\t\t\t\tspin_lock(&q->sendq.lock);\n\t\t\t\tgoto ringdb;\n\t\t\t}\n\t\t}\n\t\tif (written > 16) {\n\t\t\tcxgb4_ring_tx_db(q->adap, &q->q, written);\n\t\t\twritten = 0;\n\t\t}\n\t\tspin_lock(&q->sendq.lock);\n\t}\n\tq->full = 0;\nringdb:\n\tif (written)\n\t\tcxgb4_ring_tx_db(q->adap, &q->q, written);\n\tspin_unlock(&q->sendq.lock);\n}\n\n \nint t4_mgmt_tx(struct adapter *adap, struct sk_buff *skb)\n{\n\tint ret;\n\n\tlocal_bh_disable();\n\tret = ctrl_xmit(&adap->sge.ctrlq[0], skb);\n\tlocal_bh_enable();\n\treturn ret;\n}\n\n \nstatic inline int is_ofld_imm(const struct sk_buff *skb)\n{\n\tstruct work_request_hdr *req = (struct work_request_hdr *)skb->data;\n\tunsigned long opcode = FW_WR_OP_G(ntohl(req->wr_hi));\n\n\tif (unlikely(opcode == FW_ULPTX_WR))\n\t\treturn skb->len <= MAX_IMM_ULPTX_WR_LEN;\n\telse if (opcode == FW_CRYPTO_LOOKASIDE_WR)\n\t\treturn skb->len <= SGE_MAX_WR_LEN;\n\telse\n\t\treturn skb->len <= MAX_IMM_OFLD_TX_DATA_WR_LEN;\n}\n\n \nstatic inline unsigned int calc_tx_flits_ofld(const struct sk_buff *skb)\n{\n\tunsigned int flits, cnt;\n\n\tif (is_ofld_imm(skb))\n\t\treturn DIV_ROUND_UP(skb->len, 8);\n\n\tflits = skb_transport_offset(skb) / 8U;    \n\tcnt = skb_shinfo(skb)->nr_frags;\n\tif (skb_tail_pointer(skb) != skb_transport_header(skb))\n\t\tcnt++;\n\treturn flits + sgl_len(cnt);\n}\n\n \nstatic void txq_stop_maperr(struct sge_uld_txq *q)\n{\n\tq->mapping_err++;\n\tq->q.stops++;\n\tset_bit(q->q.cntxt_id - q->adap->sge.egr_start,\n\t\tq->adap->sge.txq_maperr);\n}\n\n \nstatic void ofldtxq_stop(struct sge_uld_txq *q, struct fw_wr_hdr *wr)\n{\n\twr->lo |= htonl(FW_WR_EQUEQ_F | FW_WR_EQUIQ_F);\n\tq->q.stops++;\n\tq->full = 1;\n}\n\n \nstatic void service_ofldq(struct sge_uld_txq *q)\n\t__must_hold(&q->sendq.lock)\n{\n\tu64 *pos, *before, *end;\n\tint credits;\n\tstruct sk_buff *skb;\n\tstruct sge_txq *txq;\n\tunsigned int left;\n\tunsigned int written = 0;\n\tunsigned int flits, ndesc;\n\n\t \n\tif (q->service_ofldq_running)\n\t\treturn;\n\tq->service_ofldq_running = true;\n\n\twhile ((skb = skb_peek(&q->sendq)) != NULL && !q->full) {\n\t\t \n\t\tspin_unlock(&q->sendq.lock);\n\n\t\tcxgb4_reclaim_completed_tx(q->adap, &q->q, false);\n\n\t\tflits = skb->priority;                 \n\t\tndesc = flits_to_desc(flits);\n\t\tcredits = txq_avail(&q->q) - ndesc;\n\t\tBUG_ON(credits < 0);\n\t\tif (unlikely(credits < TXQ_STOP_THRES))\n\t\t\tofldtxq_stop(q, (struct fw_wr_hdr *)skb->data);\n\n\t\tpos = (u64 *)&q->q.desc[q->q.pidx];\n\t\tif (is_ofld_imm(skb))\n\t\t\tcxgb4_inline_tx_skb(skb, &q->q, pos);\n\t\telse if (cxgb4_map_skb(q->adap->pdev_dev, skb,\n\t\t\t\t       (dma_addr_t *)skb->head)) {\n\t\t\ttxq_stop_maperr(q);\n\t\t\tspin_lock(&q->sendq.lock);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tint last_desc, hdr_len = skb_transport_offset(skb);\n\n\t\t\t \n\t\t\tbefore = (u64 *)pos;\n\t\t\tend = (u64 *)pos + flits;\n\t\t\ttxq = &q->q;\n\t\t\tpos = (void *)inline_tx_skb_header(skb, &q->q,\n\t\t\t\t\t\t\t   (void *)pos,\n\t\t\t\t\t\t\t   hdr_len);\n\t\t\tif (before > (u64 *)pos) {\n\t\t\t\tleft = (u8 *)end - (u8 *)txq->stat;\n\t\t\t\tend = (void *)txq->desc + left;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (pos == (u64 *)txq->stat) {\n\t\t\t\tleft = (u8 *)end - (u8 *)txq->stat;\n\t\t\t\tend = (void *)txq->desc + left;\n\t\t\t\tpos = (void *)txq->desc;\n\t\t\t}\n\n\t\t\tcxgb4_write_sgl(skb, &q->q, (void *)pos,\n\t\t\t\t\tend, hdr_len,\n\t\t\t\t\t(dma_addr_t *)skb->head);\n#ifdef CONFIG_NEED_DMA_MAP_STATE\n\t\t\tskb->dev = q->adap->port[0];\n\t\t\tskb->destructor = deferred_unmap_destructor;\n#endif\n\t\t\tlast_desc = q->q.pidx + ndesc - 1;\n\t\t\tif (last_desc >= q->q.size)\n\t\t\t\tlast_desc -= q->q.size;\n\t\t\tq->q.sdesc[last_desc].skb = skb;\n\t\t}\n\n\t\ttxq_advance(&q->q, ndesc);\n\t\twritten += ndesc;\n\t\tif (unlikely(written > 32)) {\n\t\t\tcxgb4_ring_tx_db(q->adap, &q->q, written);\n\t\t\twritten = 0;\n\t\t}\n\n\t\t \n\t\tspin_lock(&q->sendq.lock);\n\t\t__skb_unlink(skb, &q->sendq);\n\t\tif (is_ofld_imm(skb))\n\t\t\tkfree_skb(skb);\n\t}\n\tif (likely(written))\n\t\tcxgb4_ring_tx_db(q->adap, &q->q, written);\n\n\t \n\tq->service_ofldq_running = false;\n}\n\n \nstatic int ofld_xmit(struct sge_uld_txq *q, struct sk_buff *skb)\n{\n\tskb->priority = calc_tx_flits_ofld(skb);        \n\tspin_lock(&q->sendq.lock);\n\n\t \n\t__skb_queue_tail(&q->sendq, skb);\n\tif (q->sendq.qlen == 1)\n\t\tservice_ofldq(q);\n\n\tspin_unlock(&q->sendq.lock);\n\treturn NET_XMIT_SUCCESS;\n}\n\n \nstatic void restart_ofldq(struct tasklet_struct *t)\n{\n\tstruct sge_uld_txq *q = from_tasklet(q, t, qresume_tsk);\n\n\tspin_lock(&q->sendq.lock);\n\tq->full = 0;             \n\tservice_ofldq(q);\n\tspin_unlock(&q->sendq.lock);\n}\n\n \nstatic inline unsigned int skb_txq(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping >> 1;\n}\n\n \nstatic inline unsigned int is_ctrl_pkt(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping & 1;\n}\n\nstatic inline int uld_send(struct adapter *adap, struct sk_buff *skb,\n\t\t\t   unsigned int tx_uld_type)\n{\n\tstruct sge_uld_txq_info *txq_info;\n\tstruct sge_uld_txq *txq;\n\tunsigned int idx = skb_txq(skb);\n\n\tif (unlikely(is_ctrl_pkt(skb))) {\n\t\t \n\t\tif (adap->tids.nsftids)\n\t\t\tidx = 0;\n\t\treturn ctrl_xmit(&adap->sge.ctrlq[idx], skb);\n\t}\n\n\ttxq_info = adap->sge.uld_txq_info[tx_uld_type];\n\tif (unlikely(!txq_info)) {\n\t\tWARN_ON(true);\n\t\tkfree_skb(skb);\n\t\treturn NET_XMIT_DROP;\n\t}\n\n\ttxq = &txq_info->uldtxq[idx];\n\treturn ofld_xmit(txq, skb);\n}\n\n \nint t4_ofld_send(struct adapter *adap, struct sk_buff *skb)\n{\n\tint ret;\n\n\tlocal_bh_disable();\n\tret = uld_send(adap, skb, CXGB4_TX_OFLD);\n\tlocal_bh_enable();\n\treturn ret;\n}\n\n \nint cxgb4_ofld_send(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn t4_ofld_send(netdev2adap(dev), skb);\n}\nEXPORT_SYMBOL(cxgb4_ofld_send);\n\nstatic void *inline_tx_header(const void *src,\n\t\t\t      const struct sge_txq *q,\n\t\t\t      void *pos, int length)\n{\n\tint left = (void *)q->stat - pos;\n\tu64 *p;\n\n\tif (likely(length <= left)) {\n\t\tmemcpy(pos, src, length);\n\t\tpos += length;\n\t} else {\n\t\tmemcpy(pos, src, left);\n\t\tmemcpy(q->desc, src + left, length - left);\n\t\tpos = (void *)q->desc + (length - left);\n\t}\n\t \n\tp = PTR_ALIGN(pos, 8);\n\tif ((uintptr_t)p & 8) {\n\t\t*p = 0;\n\t\treturn p + 1;\n\t}\n\treturn p;\n}\n\n \nstatic int ofld_xmit_direct(struct sge_uld_txq *q, const void *src,\n\t\t\t    unsigned int len)\n{\n\tunsigned int ndesc;\n\tint credits;\n\tu64 *pos;\n\n\t \n\tif (len > MAX_IMM_OFLD_TX_DATA_WR_LEN) {\n\t\tWARN_ON(1);\n\t\treturn NET_XMIT_DROP;\n\t}\n\n\t \n\tif (!spin_trylock(&q->sendq.lock))\n\t\treturn NET_XMIT_DROP;\n\n\tif (q->full || !skb_queue_empty(&q->sendq) ||\n\t    q->service_ofldq_running) {\n\t\tspin_unlock(&q->sendq.lock);\n\t\treturn NET_XMIT_DROP;\n\t}\n\tndesc = flits_to_desc(DIV_ROUND_UP(len, 8));\n\tcredits = txq_avail(&q->q) - ndesc;\n\tpos = (u64 *)&q->q.desc[q->q.pidx];\n\n\t \n\tinline_tx_header(src, &q->q, pos, len);\n\tif (unlikely(credits < TXQ_STOP_THRES))\n\t\tofldtxq_stop(q, (struct fw_wr_hdr *)pos);\n\ttxq_advance(&q->q, ndesc);\n\tcxgb4_ring_tx_db(q->adap, &q->q, ndesc);\n\n\tspin_unlock(&q->sendq.lock);\n\treturn NET_XMIT_SUCCESS;\n}\n\nint cxgb4_immdata_send(struct net_device *dev, unsigned int idx,\n\t\t       const void *src, unsigned int len)\n{\n\tstruct sge_uld_txq_info *txq_info;\n\tstruct sge_uld_txq *txq;\n\tstruct adapter *adap;\n\tint ret;\n\n\tadap = netdev2adap(dev);\n\n\tlocal_bh_disable();\n\ttxq_info = adap->sge.uld_txq_info[CXGB4_TX_OFLD];\n\tif (unlikely(!txq_info)) {\n\t\tWARN_ON(true);\n\t\tlocal_bh_enable();\n\t\treturn NET_XMIT_DROP;\n\t}\n\ttxq = &txq_info->uldtxq[idx];\n\n\tret = ofld_xmit_direct(txq, src, len);\n\tlocal_bh_enable();\n\treturn net_xmit_eval(ret);\n}\nEXPORT_SYMBOL(cxgb4_immdata_send);\n\n \nstatic int t4_crypto_send(struct adapter *adap, struct sk_buff *skb)\n{\n\tint ret;\n\n\tlocal_bh_disable();\n\tret = uld_send(adap, skb, CXGB4_TX_CRYPTO);\n\tlocal_bh_enable();\n\treturn ret;\n}\n\n \nint cxgb4_crypto_send(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn t4_crypto_send(netdev2adap(dev), skb);\n}\nEXPORT_SYMBOL(cxgb4_crypto_send);\n\nstatic inline void copy_frags(struct sk_buff *skb,\n\t\t\t      const struct pkt_gl *gl, unsigned int offset)\n{\n\tint i;\n\n\t \n\t__skb_fill_page_desc(skb, 0, gl->frags[0].page,\n\t\t\t     gl->frags[0].offset + offset,\n\t\t\t     gl->frags[0].size - offset);\n\tskb_shinfo(skb)->nr_frags = gl->nfrags;\n\tfor (i = 1; i < gl->nfrags; i++)\n\t\t__skb_fill_page_desc(skb, i, gl->frags[i].page,\n\t\t\t\t     gl->frags[i].offset,\n\t\t\t\t     gl->frags[i].size);\n\n\t \n\tget_page(gl->frags[gl->nfrags - 1].page);\n}\n\n \nstruct sk_buff *cxgb4_pktgl_to_skb(const struct pkt_gl *gl,\n\t\t\t\t   unsigned int skb_len, unsigned int pull_len)\n{\n\tstruct sk_buff *skb;\n\n\t \n\tif (gl->tot_len <= RX_COPY_THRES) {\n\t\tskb = dev_alloc_skb(gl->tot_len);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t\t__skb_put(skb, gl->tot_len);\n\t\tskb_copy_to_linear_data(skb, gl->va, gl->tot_len);\n\t} else {\n\t\tskb = dev_alloc_skb(skb_len);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t\t__skb_put(skb, pull_len);\n\t\tskb_copy_to_linear_data(skb, gl->va, pull_len);\n\n\t\tcopy_frags(skb, gl, pull_len);\n\t\tskb->len = gl->tot_len;\n\t\tskb->data_len = skb->len - pull_len;\n\t\tskb->truesize += skb->data_len;\n\t}\nout:\treturn skb;\n}\nEXPORT_SYMBOL(cxgb4_pktgl_to_skb);\n\n \nstatic void t4_pktgl_free(const struct pkt_gl *gl)\n{\n\tint n;\n\tconst struct page_frag *p;\n\n\tfor (p = gl->frags, n = gl->nfrags - 1; n--; p++)\n\t\tput_page(p->page);\n}\n\n \nstatic noinline int handle_trace_pkt(struct adapter *adap,\n\t\t\t\t     const struct pkt_gl *gl)\n{\n\tstruct sk_buff *skb;\n\n\tskb = cxgb4_pktgl_to_skb(gl, RX_PULL_LEN, RX_PULL_LEN);\n\tif (unlikely(!skb)) {\n\t\tt4_pktgl_free(gl);\n\t\treturn 0;\n\t}\n\n\tif (is_t4(adap->params.chip))\n\t\t__skb_pull(skb, sizeof(struct cpl_trace_pkt));\n\telse\n\t\t__skb_pull(skb, sizeof(struct cpl_t5_trace_pkt));\n\n\tskb_reset_mac_header(skb);\n\tskb->protocol = htons(0xffff);\n\tskb->dev = adap->port[0];\n\tnetif_receive_skb(skb);\n\treturn 0;\n}\n\n \nstatic void cxgb4_sgetim_to_hwtstamp(struct adapter *adap,\n\t\t\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t\t\t     u64 sgetstamp)\n{\n\tu64 ns;\n\tu64 tmp = (sgetstamp * 1000 * 1000 + adap->params.vpd.cclk / 2);\n\n\tns = div_u64(tmp, adap->params.vpd.cclk);\n\n\tmemset(hwtstamps, 0, sizeof(*hwtstamps));\n\thwtstamps->hwtstamp = ns_to_ktime(ns);\n}\n\nstatic void do_gro(struct sge_eth_rxq *rxq, const struct pkt_gl *gl,\n\t\t   const struct cpl_rx_pkt *pkt, unsigned long tnl_hdr_len)\n{\n\tstruct adapter *adapter = rxq->rspq.adap;\n\tstruct sge *s = &adapter->sge;\n\tstruct port_info *pi;\n\tint ret;\n\tstruct sk_buff *skb;\n\n\tskb = napi_get_frags(&rxq->rspq.napi);\n\tif (unlikely(!skb)) {\n\t\tt4_pktgl_free(gl);\n\t\trxq->stats.rx_drops++;\n\t\treturn;\n\t}\n\n\tcopy_frags(skb, gl, s->pktshift);\n\tif (tnl_hdr_len)\n\t\tskb->csum_level = 1;\n\tskb->len = gl->tot_len - s->pktshift;\n\tskb->data_len = skb->len;\n\tskb->truesize += skb->data_len;\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tskb_record_rx_queue(skb, rxq->rspq.idx);\n\tpi = netdev_priv(skb->dev);\n\tif (pi->rxtstamp)\n\t\tcxgb4_sgetim_to_hwtstamp(adapter, skb_hwtstamps(skb),\n\t\t\t\t\t gl->sgetstamp);\n\tif (rxq->rspq.netdev->features & NETIF_F_RXHASH)\n\t\tskb_set_hash(skb, (__force u32)pkt->rsshdr.hash_val,\n\t\t\t     PKT_HASH_TYPE_L3);\n\n\tif (unlikely(pkt->vlan_ex)) {\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ntohs(pkt->vlan));\n\t\trxq->stats.vlan_ex++;\n\t}\n\tret = napi_gro_frags(&rxq->rspq.napi);\n\tif (ret == GRO_HELD)\n\t\trxq->stats.lro_pkts++;\n\telse if (ret == GRO_MERGED || ret == GRO_MERGED_FREE)\n\t\trxq->stats.lro_merged++;\n\trxq->stats.pkts++;\n\trxq->stats.rx_cso++;\n}\n\nenum {\n\tRX_NON_PTP_PKT = 0,\n\tRX_PTP_PKT_SUC = 1,\n\tRX_PTP_PKT_ERR = 2\n};\n\n \nstatic noinline int t4_systim_to_hwstamp(struct adapter *adapter,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tstruct skb_shared_hwtstamps *hwtstamps;\n\tstruct cpl_rx_mps_pkt *cpl = NULL;\n\tunsigned char *data;\n\tint offset;\n\n\tcpl = (struct cpl_rx_mps_pkt *)skb->data;\n\tif (!(CPL_RX_MPS_PKT_TYPE_G(ntohl(cpl->op_to_r1_hi)) &\n\t     X_CPL_RX_MPS_PKT_TYPE_PTP))\n\t\treturn RX_PTP_PKT_ERR;\n\n\tdata = skb->data + sizeof(*cpl);\n\tskb_pull(skb, 2 * sizeof(u64) + sizeof(struct cpl_rx_mps_pkt));\n\toffset = ETH_HLEN + IPV4_HLEN(skb->data) + UDP_HLEN;\n\tif (skb->len < offset + OFF_PTP_SEQUENCE_ID + sizeof(short))\n\t\treturn RX_PTP_PKT_ERR;\n\n\thwtstamps = skb_hwtstamps(skb);\n\tmemset(hwtstamps, 0, sizeof(*hwtstamps));\n\thwtstamps->hwtstamp = ns_to_ktime(get_unaligned_be64(data));\n\n\treturn RX_PTP_PKT_SUC;\n}\n\n \nstatic int t4_rx_hststamp(struct adapter *adapter, const __be64 *rsp,\n\t\t\t  struct sge_eth_rxq *rxq, struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (unlikely((*(u8 *)rsp == CPL_RX_MPS_PKT) &&\n\t\t     !is_t4(adapter->params.chip))) {\n\t\tret = t4_systim_to_hwstamp(adapter, skb);\n\t\tif (ret == RX_PTP_PKT_ERR) {\n\t\t\tkfree_skb(skb);\n\t\t\trxq->stats.rx_drops++;\n\t\t}\n\t\treturn ret;\n\t}\n\treturn RX_NON_PTP_PKT;\n}\n\n \nstatic int t4_tx_hststamp(struct adapter *adapter, struct sk_buff *skb,\n\t\t\t  struct net_device *dev)\n{\n\tstruct port_info *pi = netdev_priv(dev);\n\n\tif (!is_t4(adapter->params.chip) && adapter->ptp_tx_skb) {\n\t\tcxgb4_ptp_read_hwstamp(adapter, pi);\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n \nstatic void t4_tx_completion_handler(struct sge_rspq *rspq,\n\t\t\t\t     const __be64 *rsp,\n\t\t\t\t     const struct pkt_gl *gl)\n{\n\tu8 opcode = ((const struct rss_header *)rsp)->opcode;\n\tstruct port_info *pi = netdev_priv(rspq->netdev);\n\tstruct adapter *adapter = rspq->adap;\n\tstruct sge *s = &adapter->sge;\n\tstruct sge_eth_txq *txq;\n\n\t \n\trsp++;\n\n\t \n\tif (unlikely(opcode == CPL_FW4_MSG &&\n\t\t     ((const struct cpl_fw4_msg *)rsp)->type ==\n\t\t\t\t\t\t\tFW_TYPE_RSSCPL)) {\n\t\trsp++;\n\t\topcode = ((const struct rss_header *)rsp)->opcode;\n\t\trsp++;\n\t}\n\n\tif (unlikely(opcode != CPL_SGE_EGR_UPDATE)) {\n\t\tpr_info(\"%s: unexpected FW4/CPL %#x on Rx queue\\n\",\n\t\t\t__func__, opcode);\n\t\treturn;\n\t}\n\n\ttxq = &s->ethtxq[pi->first_qset + rspq->idx];\n\n\t \n\tif (CHELSIO_CHIP_VERSION(adapter->params.chip) <= CHELSIO_T5) {\n\t\tstruct cpl_sge_egr_update *egr;\n\n\t\tegr = (struct cpl_sge_egr_update *)rsp;\n\t\tWRITE_ONCE(txq->q.stat->cidx, egr->cidx);\n\t}\n\n\tt4_sge_eth_txq_egress_update(adapter, txq, -1);\n}\n\nstatic int cxgb4_validate_lb_pkt(struct port_info *pi, const struct pkt_gl *si)\n{\n\tstruct adapter *adap = pi->adapter;\n\tstruct cxgb4_ethtool_lb_test *lb;\n\tstruct sge *s = &adap->sge;\n\tstruct net_device *netdev;\n\tu8 *data;\n\tint i;\n\n\tnetdev = adap->port[pi->port_id];\n\tlb = &pi->ethtool_lb;\n\tdata = si->va + s->pktshift;\n\n\ti = ETH_ALEN;\n\tif (!ether_addr_equal(data + i, netdev->dev_addr))\n\t\treturn -1;\n\n\ti += ETH_ALEN;\n\tif (strcmp(&data[i], CXGB4_SELFTEST_LB_STR))\n\t\tlb->result = -EIO;\n\n\tcomplete(&lb->completion);\n\treturn 0;\n}\n\n \nint t4_ethrx_handler(struct sge_rspq *q, const __be64 *rsp,\n\t\t     const struct pkt_gl *si)\n{\n\tbool csum_ok;\n\tstruct sk_buff *skb;\n\tconst struct cpl_rx_pkt *pkt;\n\tstruct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, rspq);\n\tstruct adapter *adapter = q->adap;\n\tstruct sge *s = &q->adap->sge;\n\tint cpl_trace_pkt = is_t4(q->adap->params.chip) ?\n\t\t\t    CPL_TRACE_PKT : CPL_TRACE_PKT_T5;\n\tu16 err_vec, tnl_hdr_len = 0;\n\tstruct port_info *pi;\n\tint ret = 0;\n\n\tpi = netdev_priv(q->netdev);\n\t \n\tif (unlikely((*(u8 *)rsp == CPL_FW4_MSG) ||\n\t\t     (*(u8 *)rsp == CPL_SGE_EGR_UPDATE))) {\n\t\tt4_tx_completion_handler(q, rsp, si);\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*(u8 *)rsp == cpl_trace_pkt))\n\t\treturn handle_trace_pkt(q->adap, si);\n\n\tpkt = (const struct cpl_rx_pkt *)rsp;\n\t \n\tif (q->adap->params.tp.rx_pkt_encap) {\n\t\terr_vec = T6_COMPR_RXERR_VEC_G(be16_to_cpu(pkt->err_vec));\n\t\ttnl_hdr_len = T6_RX_TNLHDR_LEN_G(ntohs(pkt->err_vec));\n\t} else {\n\t\terr_vec = be16_to_cpu(pkt->err_vec);\n\t}\n\n\tcsum_ok = pkt->csum_calc && !err_vec &&\n\t\t  (q->netdev->features & NETIF_F_RXCSUM);\n\n\tif (err_vec)\n\t\trxq->stats.bad_rx_pkts++;\n\n\tif (unlikely(pi->ethtool_lb.loopback && pkt->iff >= NCHAN)) {\n\t\tret = cxgb4_validate_lb_pkt(pi, si);\n\t\tif (!ret)\n\t\t\treturn 0;\n\t}\n\n\tif (((pkt->l2info & htonl(RXF_TCP_F)) ||\n\t     tnl_hdr_len) &&\n\t    (q->netdev->features & NETIF_F_GRO) && csum_ok && !pkt->ip_frag) {\n\t\tdo_gro(rxq, si, pkt, tnl_hdr_len);\n\t\treturn 0;\n\t}\n\n\tskb = cxgb4_pktgl_to_skb(si, RX_PKT_SKB_LEN, RX_PULL_LEN);\n\tif (unlikely(!skb)) {\n\t\tt4_pktgl_free(si);\n\t\trxq->stats.rx_drops++;\n\t\treturn 0;\n\t}\n\n\t \n\tif (unlikely(pi->ptp_enable)) {\n\t\tret = t4_rx_hststamp(adapter, rsp, rxq, skb);\n\t\tif (ret == RX_PTP_PKT_ERR)\n\t\t\treturn 0;\n\t}\n\tif (likely(!ret))\n\t\t__skb_pull(skb, s->pktshift);  \n\n\t \n\tif (unlikely(pi->ptp_enable && !ret &&\n\t\t     (pkt->l2info & htonl(RXF_UDP_F)) &&\n\t\t     cxgb4_ptp_is_ptp_rx(skb))) {\n\t\tif (!t4_tx_hststamp(adapter, skb, q->netdev))\n\t\t\treturn 0;\n\t}\n\n\tskb->protocol = eth_type_trans(skb, q->netdev);\n\tskb_record_rx_queue(skb, q->idx);\n\tif (skb->dev->features & NETIF_F_RXHASH)\n\t\tskb_set_hash(skb, (__force u32)pkt->rsshdr.hash_val,\n\t\t\t     PKT_HASH_TYPE_L3);\n\n\trxq->stats.pkts++;\n\n\tif (pi->rxtstamp)\n\t\tcxgb4_sgetim_to_hwtstamp(q->adap, skb_hwtstamps(skb),\n\t\t\t\t\t si->sgetstamp);\n\tif (csum_ok && (pkt->l2info & htonl(RXF_UDP_F | RXF_TCP_F))) {\n\t\tif (!pkt->ip_frag) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\trxq->stats.rx_cso++;\n\t\t} else if (pkt->l2info & htonl(RXF_IP_F)) {\n\t\t\t__sum16 c = (__force __sum16)pkt->csum;\n\t\t\tskb->csum = csum_unfold(c);\n\n\t\t\tif (tnl_hdr_len) {\n\t\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\t\tskb->csum_level = 1;\n\t\t\t} else {\n\t\t\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\t\t}\n\t\t\trxq->stats.rx_cso++;\n\t\t}\n\t} else {\n\t\tskb_checksum_none_assert(skb);\n#ifdef CONFIG_CHELSIO_T4_FCOE\n#define CPL_RX_PKT_FLAGS (RXF_PSH_F | RXF_SYN_F | RXF_UDP_F | \\\n\t\t\t  RXF_TCP_F | RXF_IP_F | RXF_IP6_F | RXF_LRO_F)\n\n\t\tif (!(pkt->l2info & cpu_to_be32(CPL_RX_PKT_FLAGS))) {\n\t\t\tif ((pkt->l2info & cpu_to_be32(RXF_FCOE_F)) &&\n\t\t\t    (pi->fcoe.flags & CXGB_FCOE_ENABLED)) {\n\t\t\t\tif (q->adap->params.tp.rx_pkt_encap)\n\t\t\t\t\tcsum_ok = err_vec &\n\t\t\t\t\t\t  T6_COMPR_RXERR_SUM_F;\n\t\t\t\telse\n\t\t\t\t\tcsum_ok = err_vec & RXERR_CSUM_F;\n\t\t\t\tif (!csum_ok)\n\t\t\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\t}\n\t\t}\n\n#undef CPL_RX_PKT_FLAGS\n#endif  \n\t}\n\n\tif (unlikely(pkt->vlan_ex)) {\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ntohs(pkt->vlan));\n\t\trxq->stats.vlan_ex++;\n\t}\n\tskb_mark_napi_id(skb, &q->napi);\n\tnetif_receive_skb(skb);\n\treturn 0;\n}\n\n \nstatic void restore_rx_bufs(const struct pkt_gl *si, struct sge_fl *q,\n\t\t\t    int frags)\n{\n\tstruct rx_sw_desc *d;\n\n\twhile (frags--) {\n\t\tif (q->cidx == 0)\n\t\t\tq->cidx = q->size - 1;\n\t\telse\n\t\t\tq->cidx--;\n\t\td = &q->sdesc[q->cidx];\n\t\td->page = si->frags[frags].page;\n\t\td->dma_addr |= RX_UNMAPPED_BUF;\n\t\tq->avail++;\n\t}\n}\n\n \nstatic inline bool is_new_response(const struct rsp_ctrl *r,\n\t\t\t\t   const struct sge_rspq *q)\n{\n\treturn (r->type_gen >> RSPD_GEN_S) == q->gen;\n}\n\n \nstatic inline void rspq_next(struct sge_rspq *q)\n{\n\tq->cur_desc = (void *)q->cur_desc + q->iqe_len;\n\tif (unlikely(++q->cidx == q->size)) {\n\t\tq->cidx = 0;\n\t\tq->gen ^= 1;\n\t\tq->cur_desc = q->desc;\n\t}\n}\n\n \nstatic int process_responses(struct sge_rspq *q, int budget)\n{\n\tint ret, rsp_type;\n\tint budget_left = budget;\n\tconst struct rsp_ctrl *rc;\n\tstruct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, rspq);\n\tstruct adapter *adapter = q->adap;\n\tstruct sge *s = &adapter->sge;\n\n\twhile (likely(budget_left)) {\n\t\trc = (void *)q->cur_desc + (q->iqe_len - sizeof(*rc));\n\t\tif (!is_new_response(rc, q)) {\n\t\t\tif (q->flush_handler)\n\t\t\t\tq->flush_handler(q);\n\t\t\tbreak;\n\t\t}\n\n\t\tdma_rmb();\n\t\trsp_type = RSPD_TYPE_G(rc->type_gen);\n\t\tif (likely(rsp_type == RSPD_TYPE_FLBUF_X)) {\n\t\t\tstruct page_frag *fp;\n\t\t\tstruct pkt_gl si;\n\t\t\tconst struct rx_sw_desc *rsd;\n\t\t\tu32 len = ntohl(rc->pldbuflen_qid), bufsz, frags;\n\n\t\t\tif (len & RSPD_NEWBUF_F) {\n\t\t\t\tif (likely(q->offset > 0)) {\n\t\t\t\t\tfree_rx_bufs(q->adap, &rxq->fl, 1);\n\t\t\t\t\tq->offset = 0;\n\t\t\t\t}\n\t\t\t\tlen = RSPD_LEN_G(len);\n\t\t\t}\n\t\t\tsi.tot_len = len;\n\n\t\t\t \n\t\t\tfor (frags = 0, fp = si.frags; ; frags++, fp++) {\n\t\t\t\trsd = &rxq->fl.sdesc[rxq->fl.cidx];\n\t\t\t\tbufsz = get_buf_size(adapter, rsd);\n\t\t\t\tfp->page = rsd->page;\n\t\t\t\tfp->offset = q->offset;\n\t\t\t\tfp->size = min(bufsz, len);\n\t\t\t\tlen -= fp->size;\n\t\t\t\tif (!len)\n\t\t\t\t\tbreak;\n\t\t\t\tunmap_rx_buf(q->adap, &rxq->fl);\n\t\t\t}\n\n\t\t\tsi.sgetstamp = SGE_TIMESTAMP_G(\n\t\t\t\t\tbe64_to_cpu(rc->last_flit));\n\t\t\t \n\t\t\tdma_sync_single_for_cpu(q->adap->pdev_dev,\n\t\t\t\t\t\tget_buf_addr(rsd),\n\t\t\t\t\t\tfp->size, DMA_FROM_DEVICE);\n\n\t\t\tsi.va = page_address(si.frags[0].page) +\n\t\t\t\tsi.frags[0].offset;\n\t\t\tprefetch(si.va);\n\n\t\t\tsi.nfrags = frags + 1;\n\t\t\tret = q->handler(q, q->cur_desc, &si);\n\t\t\tif (likely(ret == 0))\n\t\t\t\tq->offset += ALIGN(fp->size, s->fl_align);\n\t\t\telse\n\t\t\t\trestore_rx_bufs(&si, &rxq->fl, frags);\n\t\t} else if (likely(rsp_type == RSPD_TYPE_CPL_X)) {\n\t\t\tret = q->handler(q, q->cur_desc, NULL);\n\t\t} else {\n\t\t\tret = q->handler(q, (const __be64 *)rc, CXGB4_MSG_AN);\n\t\t}\n\n\t\tif (unlikely(ret)) {\n\t\t\t \n\t\t\tq->next_intr_params = QINTR_TIMER_IDX_V(NOMEM_TMR_IDX);\n\t\t\tbreak;\n\t\t}\n\n\t\trspq_next(q);\n\t\tbudget_left--;\n\t}\n\n\tif (q->offset >= 0 && fl_cap(&rxq->fl) - rxq->fl.avail >= 16)\n\t\t__refill_fl(q->adap, &rxq->fl);\n\treturn budget - budget_left;\n}\n\n \nstatic int napi_rx_handler(struct napi_struct *napi, int budget)\n{\n\tunsigned int params;\n\tstruct sge_rspq *q = container_of(napi, struct sge_rspq, napi);\n\tint work_done;\n\tu32 val;\n\n\twork_done = process_responses(q, budget);\n\tif (likely(work_done < budget)) {\n\t\tint timer_index;\n\n\t\tnapi_complete_done(napi, work_done);\n\t\ttimer_index = QINTR_TIMER_IDX_G(q->next_intr_params);\n\n\t\tif (q->adaptive_rx) {\n\t\t\tif (work_done > max(timer_pkt_quota[timer_index],\n\t\t\t\t\t    MIN_NAPI_WORK))\n\t\t\t\ttimer_index = (timer_index + 1);\n\t\t\telse\n\t\t\t\ttimer_index = timer_index - 1;\n\n\t\t\ttimer_index = clamp(timer_index, 0, SGE_TIMERREGS - 1);\n\t\t\tq->next_intr_params =\n\t\t\t\t\tQINTR_TIMER_IDX_V(timer_index) |\n\t\t\t\t\tQINTR_CNT_EN_V(0);\n\t\t\tparams = q->next_intr_params;\n\t\t} else {\n\t\t\tparams = q->next_intr_params;\n\t\t\tq->next_intr_params = q->intr_params;\n\t\t}\n\t} else\n\t\tparams = QINTR_TIMER_IDX_V(7);\n\n\tval = CIDXINC_V(work_done) | SEINTARM_V(params);\n\n\t \n\tif (unlikely(q->bar2_addr == NULL)) {\n\t\tt4_write_reg(q->adap, MYPF_REG(SGE_PF_GTS_A),\n\t\t\t     val | INGRESSQID_V((u32)q->cntxt_id));\n\t} else {\n\t\twritel(val | INGRESSQID_V(q->bar2_qid),\n\t\t       q->bar2_addr + SGE_UDB_GTS);\n\t\twmb();\n\t}\n\treturn work_done;\n}\n\nvoid cxgb4_ethofld_restart(struct tasklet_struct *t)\n{\n\tstruct sge_eosw_txq *eosw_txq = from_tasklet(eosw_txq, t,\n\t\t\t\t\t\t     qresume_tsk);\n\tint pktcount;\n\n\tspin_lock(&eosw_txq->lock);\n\tpktcount = eosw_txq->cidx - eosw_txq->last_cidx;\n\tif (pktcount < 0)\n\t\tpktcount += eosw_txq->ndesc;\n\n\tif (pktcount) {\n\t\tcxgb4_eosw_txq_free_desc(netdev2adap(eosw_txq->netdev),\n\t\t\t\t\t eosw_txq, pktcount);\n\t\teosw_txq->inuse -= pktcount;\n\t}\n\n\t \n\tethofld_xmit(eosw_txq->netdev, eosw_txq);\n\tspin_unlock(&eosw_txq->lock);\n}\n\n \nint cxgb4_ethofld_rx_handler(struct sge_rspq *q, const __be64 *rsp,\n\t\t\t     const struct pkt_gl *si)\n{\n\tu8 opcode = ((const struct rss_header *)rsp)->opcode;\n\n\t \n\trsp++;\n\n\tif (opcode == CPL_FW4_ACK) {\n\t\tconst struct cpl_fw4_ack *cpl;\n\t\tstruct sge_eosw_txq *eosw_txq;\n\t\tstruct eotid_entry *entry;\n\t\tstruct sk_buff *skb;\n\t\tu32 hdr_len, eotid;\n\t\tu8 flits, wrlen16;\n\t\tint credits;\n\n\t\tcpl = (const struct cpl_fw4_ack *)rsp;\n\t\teotid = CPL_FW4_ACK_FLOWID_G(ntohl(OPCODE_TID(cpl))) -\n\t\t\tq->adap->tids.eotid_base;\n\t\tentry = cxgb4_lookup_eotid(&q->adap->tids, eotid);\n\t\tif (!entry)\n\t\t\tgoto out_done;\n\n\t\teosw_txq = (struct sge_eosw_txq *)entry->data;\n\t\tif (!eosw_txq)\n\t\t\tgoto out_done;\n\n\t\tspin_lock(&eosw_txq->lock);\n\t\tcredits = cpl->credits;\n\t\twhile (credits > 0) {\n\t\t\tskb = eosw_txq->desc[eosw_txq->cidx].skb;\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\n\t\t\tif (unlikely((eosw_txq->state ==\n\t\t\t\t      CXGB4_EO_STATE_FLOWC_OPEN_REPLY ||\n\t\t\t\t      eosw_txq->state ==\n\t\t\t\t      CXGB4_EO_STATE_FLOWC_CLOSE_REPLY) &&\n\t\t\t\t     eosw_txq->cidx == eosw_txq->flowc_idx)) {\n\t\t\t\tflits = DIV_ROUND_UP(skb->len, 8);\n\t\t\t\tif (eosw_txq->state ==\n\t\t\t\t    CXGB4_EO_STATE_FLOWC_OPEN_REPLY)\n\t\t\t\t\teosw_txq->state = CXGB4_EO_STATE_ACTIVE;\n\t\t\t\telse\n\t\t\t\t\teosw_txq->state = CXGB4_EO_STATE_CLOSED;\n\t\t\t\tcomplete(&eosw_txq->completion);\n\t\t\t} else {\n\t\t\t\thdr_len = eth_get_headlen(eosw_txq->netdev,\n\t\t\t\t\t\t\t  skb->data,\n\t\t\t\t\t\t\t  skb_headlen(skb));\n\t\t\t\tflits = ethofld_calc_tx_flits(q->adap, skb,\n\t\t\t\t\t\t\t      hdr_len);\n\t\t\t}\n\t\t\teosw_txq_advance_index(&eosw_txq->cidx, 1,\n\t\t\t\t\t       eosw_txq->ndesc);\n\t\t\twrlen16 = DIV_ROUND_UP(flits * 8, 16);\n\t\t\tcredits -= wrlen16;\n\t\t}\n\n\t\teosw_txq->cred += cpl->credits;\n\t\teosw_txq->ncompl--;\n\n\t\tspin_unlock(&eosw_txq->lock);\n\n\t\t \n\t\ttasklet_schedule(&eosw_txq->qresume_tsk);\n\t}\n\nout_done:\n\treturn 0;\n}\n\n \nirqreturn_t t4_sge_intr_msix(int irq, void *cookie)\n{\n\tstruct sge_rspq *q = cookie;\n\n\tnapi_schedule(&q->napi);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic unsigned int process_intrq(struct adapter *adap)\n{\n\tunsigned int credits;\n\tconst struct rsp_ctrl *rc;\n\tstruct sge_rspq *q = &adap->sge.intrq;\n\tu32 val;\n\n\tspin_lock(&adap->sge.intrq_lock);\n\tfor (credits = 0; ; credits++) {\n\t\trc = (void *)q->cur_desc + (q->iqe_len - sizeof(*rc));\n\t\tif (!is_new_response(rc, q))\n\t\t\tbreak;\n\n\t\tdma_rmb();\n\t\tif (RSPD_TYPE_G(rc->type_gen) == RSPD_TYPE_INTR_X) {\n\t\t\tunsigned int qid = ntohl(rc->pldbuflen_qid);\n\n\t\t\tqid -= adap->sge.ingr_start;\n\t\t\tnapi_schedule(&adap->sge.ingr_map[qid]->napi);\n\t\t}\n\n\t\trspq_next(q);\n\t}\n\n\tval =  CIDXINC_V(credits) | SEINTARM_V(q->intr_params);\n\n\t \n\tif (unlikely(q->bar2_addr == NULL)) {\n\t\tt4_write_reg(adap, MYPF_REG(SGE_PF_GTS_A),\n\t\t\t     val | INGRESSQID_V(q->cntxt_id));\n\t} else {\n\t\twritel(val | INGRESSQID_V(q->bar2_qid),\n\t\t       q->bar2_addr + SGE_UDB_GTS);\n\t\twmb();\n\t}\n\tspin_unlock(&adap->sge.intrq_lock);\n\treturn credits;\n}\n\n \nstatic irqreturn_t t4_intr_msi(int irq, void *cookie)\n{\n\tstruct adapter *adap = cookie;\n\n\tif (adap->flags & CXGB4_MASTER_PF)\n\t\tt4_slow_intr_handler(adap);\n\tprocess_intrq(adap);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t t4_intr_intx(int irq, void *cookie)\n{\n\tstruct adapter *adap = cookie;\n\n\tt4_write_reg(adap, MYPF_REG(PCIE_PF_CLI_A), 0);\n\tif (((adap->flags & CXGB4_MASTER_PF) && t4_slow_intr_handler(adap)) |\n\t    process_intrq(adap))\n\t\treturn IRQ_HANDLED;\n\treturn IRQ_NONE;              \n}\n\n \nirq_handler_t t4_intr_handler(struct adapter *adap)\n{\n\tif (adap->flags & CXGB4_USING_MSIX)\n\t\treturn t4_sge_intr_msix;\n\tif (adap->flags & CXGB4_USING_MSI)\n\t\treturn t4_intr_msi;\n\treturn t4_intr_intx;\n}\n\nstatic void sge_rx_timer_cb(struct timer_list *t)\n{\n\tunsigned long m;\n\tunsigned int i;\n\tstruct adapter *adap = from_timer(adap, t, sge.rx_timer);\n\tstruct sge *s = &adap->sge;\n\n\tfor (i = 0; i < BITS_TO_LONGS(s->egr_sz); i++)\n\t\tfor (m = s->starving_fl[i]; m; m &= m - 1) {\n\t\t\tstruct sge_eth_rxq *rxq;\n\t\t\tunsigned int id = __ffs(m) + i * BITS_PER_LONG;\n\t\t\tstruct sge_fl *fl = s->egr_map[id];\n\n\t\t\tclear_bit(id, s->starving_fl);\n\t\t\tsmp_mb__after_atomic();\n\n\t\t\tif (fl_starving(adap, fl)) {\n\t\t\t\trxq = container_of(fl, struct sge_eth_rxq, fl);\n\t\t\t\tif (napi_reschedule(&rxq->rspq.napi))\n\t\t\t\t\tfl->starving++;\n\t\t\t\telse\n\t\t\t\t\tset_bit(id, s->starving_fl);\n\t\t\t}\n\t\t}\n\t \n\tif (!(adap->flags & CXGB4_MASTER_PF))\n\t\tgoto done;\n\n\tt4_idma_monitor(adap, &s->idma_monitor, HZ, RX_QCHECK_PERIOD);\n\ndone:\n\tmod_timer(&s->rx_timer, jiffies + RX_QCHECK_PERIOD);\n}\n\nstatic void sge_tx_timer_cb(struct timer_list *t)\n{\n\tstruct adapter *adap = from_timer(adap, t, sge.tx_timer);\n\tstruct sge *s = &adap->sge;\n\tunsigned long m, period;\n\tunsigned int i, budget;\n\n\tfor (i = 0; i < BITS_TO_LONGS(s->egr_sz); i++)\n\t\tfor (m = s->txq_maperr[i]; m; m &= m - 1) {\n\t\t\tunsigned long id = __ffs(m) + i * BITS_PER_LONG;\n\t\t\tstruct sge_uld_txq *txq = s->egr_map[id];\n\n\t\t\tclear_bit(id, s->txq_maperr);\n\t\t\ttasklet_schedule(&txq->qresume_tsk);\n\t\t}\n\n\tif (!is_t4(adap->params.chip)) {\n\t\tstruct sge_eth_txq *q = &s->ptptxq;\n\t\tint avail;\n\n\t\tspin_lock(&adap->ptp_lock);\n\t\tavail = reclaimable(&q->q);\n\n\t\tif (avail) {\n\t\t\tfree_tx_desc(adap, &q->q, avail, false);\n\t\t\tq->q.in_use -= avail;\n\t\t}\n\t\tspin_unlock(&adap->ptp_lock);\n\t}\n\n\tbudget = MAX_TIMER_TX_RECLAIM;\n\ti = s->ethtxq_rover;\n\tdo {\n\t\tbudget -= t4_sge_eth_txq_egress_update(adap, &s->ethtxq[i],\n\t\t\t\t\t\t       budget);\n\t\tif (!budget)\n\t\t\tbreak;\n\n\t\tif (++i >= s->ethqsets)\n\t\t\ti = 0;\n\t} while (i != s->ethtxq_rover);\n\ts->ethtxq_rover = i;\n\n\tif (budget == 0) {\n\t\t \n\t\tperiod = 2;\n\t} else {\n\t\t \n\t\tperiod = TX_QCHECK_PERIOD;\n\t}\n\n\tmod_timer(&s->tx_timer, jiffies + period);\n}\n\n \nstatic void __iomem *bar2_address(struct adapter *adapter,\n\t\t\t\t  unsigned int qid,\n\t\t\t\t  enum t4_bar2_qtype qtype,\n\t\t\t\t  unsigned int *pbar2_qid)\n{\n\tu64 bar2_qoffset;\n\tint ret;\n\n\tret = t4_bar2_sge_qregs(adapter, qid, qtype, 0,\n\t\t\t\t&bar2_qoffset, pbar2_qid);\n\tif (ret)\n\t\treturn NULL;\n\n\treturn adapter->bar2 + bar2_qoffset;\n}\n\n \nint t4_sge_alloc_rxq(struct adapter *adap, struct sge_rspq *iq, bool fwevtq,\n\t\t     struct net_device *dev, int intr_idx,\n\t\t     struct sge_fl *fl, rspq_handler_t hnd,\n\t\t     rspq_flush_handler_t flush_hnd, int cong)\n{\n\tint ret, flsz = 0;\n\tstruct fw_iq_cmd c;\n\tstruct sge *s = &adap->sge;\n\tstruct port_info *pi = netdev_priv(dev);\n\tint relaxed = !(adap->flags & CXGB4_ROOT_NO_RELAXED_ORDERING);\n\n\t \n\tiq->size = roundup(iq->size, 16);\n\n\tiq->desc = alloc_ring(adap->pdev_dev, iq->size, iq->iqe_len, 0,\n\t\t\t      &iq->phys_addr, NULL, 0,\n\t\t\t      dev_to_node(adap->pdev_dev));\n\tif (!iq->desc)\n\t\treturn -ENOMEM;\n\n\tmemset(&c, 0, sizeof(c));\n\tc.op_to_vfn = htonl(FW_CMD_OP_V(FW_IQ_CMD) | FW_CMD_REQUEST_F |\n\t\t\t    FW_CMD_WRITE_F | FW_CMD_EXEC_F |\n\t\t\t    FW_IQ_CMD_PFN_V(adap->pf) | FW_IQ_CMD_VFN_V(0));\n\tc.alloc_to_len16 = htonl(FW_IQ_CMD_ALLOC_F | FW_IQ_CMD_IQSTART_F |\n\t\t\t\t FW_LEN16(c));\n\tc.type_to_iqandstindex = htonl(FW_IQ_CMD_TYPE_V(FW_IQ_TYPE_FL_INT_CAP) |\n\t\tFW_IQ_CMD_IQASYNCH_V(fwevtq) | FW_IQ_CMD_VIID_V(pi->viid) |\n\t\tFW_IQ_CMD_IQANDST_V(intr_idx < 0) |\n\t\tFW_IQ_CMD_IQANUD_V(UPDATEDELIVERY_INTERRUPT_X) |\n\t\tFW_IQ_CMD_IQANDSTINDEX_V(intr_idx >= 0 ? intr_idx :\n\t\t\t\t\t\t\t-intr_idx - 1));\n\tc.iqdroprss_to_iqesize = htons(FW_IQ_CMD_IQPCIECH_V(pi->tx_chan) |\n\t\tFW_IQ_CMD_IQGTSMODE_F |\n\t\tFW_IQ_CMD_IQINTCNTTHRESH_V(iq->pktcnt_idx) |\n\t\tFW_IQ_CMD_IQESIZE_V(ilog2(iq->iqe_len) - 4));\n\tc.iqsize = htons(iq->size);\n\tc.iqaddr = cpu_to_be64(iq->phys_addr);\n\tif (cong >= 0)\n\t\tc.iqns_to_fl0congen = htonl(FW_IQ_CMD_IQFLINTCONGEN_F |\n\t\t\t\tFW_IQ_CMD_IQTYPE_V(cong ? FW_IQ_IQTYPE_NIC\n\t\t\t\t\t\t\t:  FW_IQ_IQTYPE_OFLD));\n\n\tif (fl) {\n\t\tunsigned int chip_ver =\n\t\t\tCHELSIO_CHIP_VERSION(adap->params.chip);\n\n\t\t \n\t\tif (fl->size < s->fl_starve_thres - 1 + 2 * 8)\n\t\t\tfl->size = s->fl_starve_thres - 1 + 2 * 8;\n\t\tfl->size = roundup(fl->size, 8);\n\t\tfl->desc = alloc_ring(adap->pdev_dev, fl->size, sizeof(__be64),\n\t\t\t\t      sizeof(struct rx_sw_desc), &fl->addr,\n\t\t\t\t      &fl->sdesc, s->stat_len,\n\t\t\t\t      dev_to_node(adap->pdev_dev));\n\t\tif (!fl->desc)\n\t\t\tgoto fl_nomem;\n\n\t\tflsz = fl->size / 8 + s->stat_len / sizeof(struct tx_desc);\n\t\tc.iqns_to_fl0congen |= htonl(FW_IQ_CMD_FL0PACKEN_F |\n\t\t\t\t\t     FW_IQ_CMD_FL0FETCHRO_V(relaxed) |\n\t\t\t\t\t     FW_IQ_CMD_FL0DATARO_V(relaxed) |\n\t\t\t\t\t     FW_IQ_CMD_FL0PADEN_F);\n\t\tif (cong >= 0)\n\t\t\tc.iqns_to_fl0congen |=\n\t\t\t\thtonl(FW_IQ_CMD_FL0CNGCHMAP_V(cong) |\n\t\t\t\t      FW_IQ_CMD_FL0CONGCIF_F |\n\t\t\t\t      FW_IQ_CMD_FL0CONGEN_F);\n\t\t \n\t\tc.fl0dcaen_to_fl0cidxfthresh =\n\t\t\thtons(FW_IQ_CMD_FL0FBMIN_V(chip_ver <= CHELSIO_T5 ?\n\t\t\t\t\t\t   FETCHBURSTMIN_128B_X :\n\t\t\t\t\t\t   FETCHBURSTMIN_64B_T6_X) |\n\t\t\t      FW_IQ_CMD_FL0FBMAX_V((chip_ver <= CHELSIO_T5) ?\n\t\t\t\t\t\t   FETCHBURSTMAX_512B_X :\n\t\t\t\t\t\t   FETCHBURSTMAX_256B_X));\n\t\tc.fl0size = htons(flsz);\n\t\tc.fl0addr = cpu_to_be64(fl->addr);\n\t}\n\n\tret = t4_wr_mbox(adap, adap->mbox, &c, sizeof(c), &c);\n\tif (ret)\n\t\tgoto err;\n\n\tnetif_napi_add(dev, &iq->napi, napi_rx_handler);\n\tiq->cur_desc = iq->desc;\n\tiq->cidx = 0;\n\tiq->gen = 1;\n\tiq->next_intr_params = iq->intr_params;\n\tiq->cntxt_id = ntohs(c.iqid);\n\tiq->abs_id = ntohs(c.physiqid);\n\tiq->bar2_addr = bar2_address(adap,\n\t\t\t\t     iq->cntxt_id,\n\t\t\t\t     T4_BAR2_QTYPE_INGRESS,\n\t\t\t\t     &iq->bar2_qid);\n\tiq->size--;                            \n\tiq->netdev = dev;\n\tiq->handler = hnd;\n\tiq->flush_handler = flush_hnd;\n\n\tmemset(&iq->lro_mgr, 0, sizeof(struct t4_lro_mgr));\n\tskb_queue_head_init(&iq->lro_mgr.lroq);\n\n\t \n\tiq->offset = fl ? 0 : -1;\n\n\tadap->sge.ingr_map[iq->cntxt_id - adap->sge.ingr_start] = iq;\n\n\tif (fl) {\n\t\tfl->cntxt_id = ntohs(c.fl0id);\n\t\tfl->avail = fl->pend_cred = 0;\n\t\tfl->pidx = fl->cidx = 0;\n\t\tfl->alloc_failed = fl->large_alloc_failed = fl->starving = 0;\n\t\tadap->sge.egr_map[fl->cntxt_id - adap->sge.egr_start] = fl;\n\n\t\t \n\t\tfl->bar2_addr = bar2_address(adap,\n\t\t\t\t\t     fl->cntxt_id,\n\t\t\t\t\t     T4_BAR2_QTYPE_EGRESS,\n\t\t\t\t\t     &fl->bar2_qid);\n\t\trefill_fl(adap, fl, fl_cap(fl), GFP_KERNEL);\n\t}\n\n\t \n\tif (!is_t4(adap->params.chip) && cong >= 0) {\n\t\tu32 param, val, ch_map = 0;\n\t\tint i;\n\t\tu16 cng_ch_bits_log = adap->params.arch.cng_ch_bits_log;\n\n\t\tparam = (FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DMAQ) |\n\t\t\t FW_PARAMS_PARAM_X_V(FW_PARAMS_PARAM_DMAQ_CONM_CTXT) |\n\t\t\t FW_PARAMS_PARAM_YZ_V(iq->cntxt_id));\n\t\tif (cong == 0) {\n\t\t\tval = CONMCTXT_CNGTPMODE_V(CONMCTXT_CNGTPMODE_QUEUE_X);\n\t\t} else {\n\t\t\tval =\n\t\t\t    CONMCTXT_CNGTPMODE_V(CONMCTXT_CNGTPMODE_CHANNEL_X);\n\t\t\tfor (i = 0; i < 4; i++) {\n\t\t\t\tif (cong & (1 << i))\n\t\t\t\t\tch_map |= 1 << (i << cng_ch_bits_log);\n\t\t\t}\n\t\t\tval |= CONMCTXT_CNGCHMAP_V(ch_map);\n\t\t}\n\t\tret = t4_set_params(adap, adap->mbox, adap->pf, 0, 1,\n\t\t\t\t    &param, &val);\n\t\tif (ret)\n\t\t\tdev_warn(adap->pdev_dev, \"Failed to set Congestion\"\n\t\t\t\t \" Manager Context for Ingress Queue %d: %d\\n\",\n\t\t\t\t iq->cntxt_id, -ret);\n\t}\n\n\treturn 0;\n\nfl_nomem:\n\tret = -ENOMEM;\nerr:\n\tif (iq->desc) {\n\t\tdma_free_coherent(adap->pdev_dev, iq->size * iq->iqe_len,\n\t\t\t\t  iq->desc, iq->phys_addr);\n\t\tiq->desc = NULL;\n\t}\n\tif (fl && fl->desc) {\n\t\tkfree(fl->sdesc);\n\t\tfl->sdesc = NULL;\n\t\tdma_free_coherent(adap->pdev_dev, flsz * sizeof(struct tx_desc),\n\t\t\t\t  fl->desc, fl->addr);\n\t\tfl->desc = NULL;\n\t}\n\treturn ret;\n}\n\nstatic void init_txq(struct adapter *adap, struct sge_txq *q, unsigned int id)\n{\n\tq->cntxt_id = id;\n\tq->bar2_addr = bar2_address(adap,\n\t\t\t\t    q->cntxt_id,\n\t\t\t\t    T4_BAR2_QTYPE_EGRESS,\n\t\t\t\t    &q->bar2_qid);\n\tq->in_use = 0;\n\tq->cidx = q->pidx = 0;\n\tq->stops = q->restarts = 0;\n\tq->stat = (void *)&q->desc[q->size];\n\tspin_lock_init(&q->db_lock);\n\tadap->sge.egr_map[id - adap->sge.egr_start] = q;\n}\n\n \nint t4_sge_alloc_eth_txq(struct adapter *adap, struct sge_eth_txq *txq,\n\t\t\t struct net_device *dev, struct netdev_queue *netdevq,\n\t\t\t unsigned int iqid, u8 dbqt)\n{\n\tunsigned int chip_ver = CHELSIO_CHIP_VERSION(adap->params.chip);\n\tstruct port_info *pi = netdev_priv(dev);\n\tstruct sge *s = &adap->sge;\n\tstruct fw_eq_eth_cmd c;\n\tint ret, nentries;\n\n\t \n\tnentries = txq->q.size + s->stat_len / sizeof(struct tx_desc);\n\n\ttxq->q.desc = alloc_ring(adap->pdev_dev, txq->q.size,\n\t\t\tsizeof(struct tx_desc), sizeof(struct tx_sw_desc),\n\t\t\t&txq->q.phys_addr, &txq->q.sdesc, s->stat_len,\n\t\t\tnetdev_queue_numa_node_read(netdevq));\n\tif (!txq->q.desc)\n\t\treturn -ENOMEM;\n\n\tmemset(&c, 0, sizeof(c));\n\tc.op_to_vfn = htonl(FW_CMD_OP_V(FW_EQ_ETH_CMD) | FW_CMD_REQUEST_F |\n\t\t\t    FW_CMD_WRITE_F | FW_CMD_EXEC_F |\n\t\t\t    FW_EQ_ETH_CMD_PFN_V(adap->pf) |\n\t\t\t    FW_EQ_ETH_CMD_VFN_V(0));\n\tc.alloc_to_len16 = htonl(FW_EQ_ETH_CMD_ALLOC_F |\n\t\t\t\t FW_EQ_ETH_CMD_EQSTART_F | FW_LEN16(c));\n\n\t \n\tc.autoequiqe_to_viid = htonl(((chip_ver <= CHELSIO_T5) ?\n\t\t\t\t      FW_EQ_ETH_CMD_AUTOEQUIQE_F :\n\t\t\t\t      FW_EQ_ETH_CMD_AUTOEQUEQE_F) |\n\t\t\t\t     FW_EQ_ETH_CMD_VIID_V(pi->viid));\n\n\tc.fetchszm_to_iqid =\n\t\thtonl(FW_EQ_ETH_CMD_HOSTFCMODE_V((chip_ver <= CHELSIO_T5) ?\n\t\t\t\t\t\t HOSTFCMODE_INGRESS_QUEUE_X :\n\t\t\t\t\t\t HOSTFCMODE_STATUS_PAGE_X) |\n\t\t      FW_EQ_ETH_CMD_PCIECHN_V(pi->tx_chan) |\n\t\t      FW_EQ_ETH_CMD_FETCHRO_F | FW_EQ_ETH_CMD_IQID_V(iqid));\n\n\t \n\tc.dcaen_to_eqsize =\n\t\thtonl(FW_EQ_ETH_CMD_FBMIN_V(chip_ver <= CHELSIO_T5\n\t\t\t\t\t    ? FETCHBURSTMIN_64B_X\n\t\t\t\t\t    : FETCHBURSTMIN_64B_T6_X) |\n\t\t      FW_EQ_ETH_CMD_FBMAX_V(FETCHBURSTMAX_512B_X) |\n\t\t      FW_EQ_ETH_CMD_CIDXFTHRESH_V(CIDXFLUSHTHRESH_32_X) |\n\t\t      FW_EQ_ETH_CMD_CIDXFTHRESHO_V(chip_ver == CHELSIO_T5) |\n\t\t      FW_EQ_ETH_CMD_EQSIZE_V(nentries));\n\n\tc.eqaddr = cpu_to_be64(txq->q.phys_addr);\n\n\t \n\tif (dbqt)\n\t\tc.timeren_timerix =\n\t\t\tcpu_to_be32(FW_EQ_ETH_CMD_TIMEREN_F |\n\t\t\t\t    FW_EQ_ETH_CMD_TIMERIX_V(txq->dbqtimerix));\n\n\tret = t4_wr_mbox(adap, adap->mbox, &c, sizeof(c), &c);\n\tif (ret) {\n\t\tkfree(txq->q.sdesc);\n\t\ttxq->q.sdesc = NULL;\n\t\tdma_free_coherent(adap->pdev_dev,\n\t\t\t\t  nentries * sizeof(struct tx_desc),\n\t\t\t\t  txq->q.desc, txq->q.phys_addr);\n\t\ttxq->q.desc = NULL;\n\t\treturn ret;\n\t}\n\n\ttxq->q.q_type = CXGB4_TXQ_ETH;\n\tinit_txq(adap, &txq->q, FW_EQ_ETH_CMD_EQID_G(ntohl(c.eqid_pkd)));\n\ttxq->txq = netdevq;\n\ttxq->tso = 0;\n\ttxq->uso = 0;\n\ttxq->tx_cso = 0;\n\ttxq->vlan_ins = 0;\n\ttxq->mapping_err = 0;\n\ttxq->dbqt = dbqt;\n\n\treturn 0;\n}\n\nint t4_sge_alloc_ctrl_txq(struct adapter *adap, struct sge_ctrl_txq *txq,\n\t\t\t  struct net_device *dev, unsigned int iqid,\n\t\t\t  unsigned int cmplqid)\n{\n\tunsigned int chip_ver = CHELSIO_CHIP_VERSION(adap->params.chip);\n\tstruct port_info *pi = netdev_priv(dev);\n\tstruct sge *s = &adap->sge;\n\tstruct fw_eq_ctrl_cmd c;\n\tint ret, nentries;\n\n\t \n\tnentries = txq->q.size + s->stat_len / sizeof(struct tx_desc);\n\n\ttxq->q.desc = alloc_ring(adap->pdev_dev, nentries,\n\t\t\t\t sizeof(struct tx_desc), 0, &txq->q.phys_addr,\n\t\t\t\t NULL, 0, dev_to_node(adap->pdev_dev));\n\tif (!txq->q.desc)\n\t\treturn -ENOMEM;\n\n\tc.op_to_vfn = htonl(FW_CMD_OP_V(FW_EQ_CTRL_CMD) | FW_CMD_REQUEST_F |\n\t\t\t    FW_CMD_WRITE_F | FW_CMD_EXEC_F |\n\t\t\t    FW_EQ_CTRL_CMD_PFN_V(adap->pf) |\n\t\t\t    FW_EQ_CTRL_CMD_VFN_V(0));\n\tc.alloc_to_len16 = htonl(FW_EQ_CTRL_CMD_ALLOC_F |\n\t\t\t\t FW_EQ_CTRL_CMD_EQSTART_F | FW_LEN16(c));\n\tc.cmpliqid_eqid = htonl(FW_EQ_CTRL_CMD_CMPLIQID_V(cmplqid));\n\tc.physeqid_pkd = htonl(0);\n\tc.fetchszm_to_iqid =\n\t\thtonl(FW_EQ_CTRL_CMD_HOSTFCMODE_V(HOSTFCMODE_STATUS_PAGE_X) |\n\t\t      FW_EQ_CTRL_CMD_PCIECHN_V(pi->tx_chan) |\n\t\t      FW_EQ_CTRL_CMD_FETCHRO_F | FW_EQ_CTRL_CMD_IQID_V(iqid));\n\tc.dcaen_to_eqsize =\n\t\thtonl(FW_EQ_CTRL_CMD_FBMIN_V(chip_ver <= CHELSIO_T5\n\t\t\t\t\t     ? FETCHBURSTMIN_64B_X\n\t\t\t\t\t     : FETCHBURSTMIN_64B_T6_X) |\n\t\t      FW_EQ_CTRL_CMD_FBMAX_V(FETCHBURSTMAX_512B_X) |\n\t\t      FW_EQ_CTRL_CMD_CIDXFTHRESH_V(CIDXFLUSHTHRESH_32_X) |\n\t\t      FW_EQ_CTRL_CMD_EQSIZE_V(nentries));\n\tc.eqaddr = cpu_to_be64(txq->q.phys_addr);\n\n\tret = t4_wr_mbox(adap, adap->mbox, &c, sizeof(c), &c);\n\tif (ret) {\n\t\tdma_free_coherent(adap->pdev_dev,\n\t\t\t\t  nentries * sizeof(struct tx_desc),\n\t\t\t\t  txq->q.desc, txq->q.phys_addr);\n\t\ttxq->q.desc = NULL;\n\t\treturn ret;\n\t}\n\n\ttxq->q.q_type = CXGB4_TXQ_CTRL;\n\tinit_txq(adap, &txq->q, FW_EQ_CTRL_CMD_EQID_G(ntohl(c.cmpliqid_eqid)));\n\ttxq->adap = adap;\n\tskb_queue_head_init(&txq->sendq);\n\ttasklet_setup(&txq->qresume_tsk, restart_ctrlq);\n\ttxq->full = 0;\n\treturn 0;\n}\n\nint t4_sge_mod_ctrl_txq(struct adapter *adap, unsigned int eqid,\n\t\t\tunsigned int cmplqid)\n{\n\tu32 param, val;\n\n\tparam = (FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DMAQ) |\n\t\t FW_PARAMS_PARAM_X_V(FW_PARAMS_PARAM_DMAQ_EQ_CMPLIQID_CTRL) |\n\t\t FW_PARAMS_PARAM_YZ_V(eqid));\n\tval = cmplqid;\n\treturn t4_set_params(adap, adap->mbox, adap->pf, 0, 1, &param, &val);\n}\n\nstatic int t4_sge_alloc_ofld_txq(struct adapter *adap, struct sge_txq *q,\n\t\t\t\t struct net_device *dev, u32 cmd, u32 iqid)\n{\n\tunsigned int chip_ver = CHELSIO_CHIP_VERSION(adap->params.chip);\n\tstruct port_info *pi = netdev_priv(dev);\n\tstruct sge *s = &adap->sge;\n\tstruct fw_eq_ofld_cmd c;\n\tu32 fb_min, nentries;\n\tint ret;\n\n\t \n\tnentries = q->size + s->stat_len / sizeof(struct tx_desc);\n\tq->desc = alloc_ring(adap->pdev_dev, q->size, sizeof(struct tx_desc),\n\t\t\t     sizeof(struct tx_sw_desc), &q->phys_addr,\n\t\t\t     &q->sdesc, s->stat_len, NUMA_NO_NODE);\n\tif (!q->desc)\n\t\treturn -ENOMEM;\n\n\tif (chip_ver <= CHELSIO_T5)\n\t\tfb_min = FETCHBURSTMIN_64B_X;\n\telse\n\t\tfb_min = FETCHBURSTMIN_64B_T6_X;\n\n\tmemset(&c, 0, sizeof(c));\n\tc.op_to_vfn = htonl(FW_CMD_OP_V(cmd) | FW_CMD_REQUEST_F |\n\t\t\t    FW_CMD_WRITE_F | FW_CMD_EXEC_F |\n\t\t\t    FW_EQ_OFLD_CMD_PFN_V(adap->pf) |\n\t\t\t    FW_EQ_OFLD_CMD_VFN_V(0));\n\tc.alloc_to_len16 = htonl(FW_EQ_OFLD_CMD_ALLOC_F |\n\t\t\t\t FW_EQ_OFLD_CMD_EQSTART_F | FW_LEN16(c));\n\tc.fetchszm_to_iqid =\n\t\thtonl(FW_EQ_OFLD_CMD_HOSTFCMODE_V(HOSTFCMODE_STATUS_PAGE_X) |\n\t\t      FW_EQ_OFLD_CMD_PCIECHN_V(pi->tx_chan) |\n\t\t      FW_EQ_OFLD_CMD_FETCHRO_F | FW_EQ_OFLD_CMD_IQID_V(iqid));\n\tc.dcaen_to_eqsize =\n\t\thtonl(FW_EQ_OFLD_CMD_FBMIN_V(fb_min) |\n\t\t      FW_EQ_OFLD_CMD_FBMAX_V(FETCHBURSTMAX_512B_X) |\n\t\t      FW_EQ_OFLD_CMD_CIDXFTHRESH_V(CIDXFLUSHTHRESH_32_X) |\n\t\t      FW_EQ_OFLD_CMD_EQSIZE_V(nentries));\n\tc.eqaddr = cpu_to_be64(q->phys_addr);\n\n\tret = t4_wr_mbox(adap, adap->mbox, &c, sizeof(c), &c);\n\tif (ret) {\n\t\tkfree(q->sdesc);\n\t\tq->sdesc = NULL;\n\t\tdma_free_coherent(adap->pdev_dev,\n\t\t\t\t  nentries * sizeof(struct tx_desc),\n\t\t\t\t  q->desc, q->phys_addr);\n\t\tq->desc = NULL;\n\t\treturn ret;\n\t}\n\n\tinit_txq(adap, q, FW_EQ_OFLD_CMD_EQID_G(ntohl(c.eqid_pkd)));\n\treturn 0;\n}\n\nint t4_sge_alloc_uld_txq(struct adapter *adap, struct sge_uld_txq *txq,\n\t\t\t struct net_device *dev, unsigned int iqid,\n\t\t\t unsigned int uld_type)\n{\n\tu32 cmd = FW_EQ_OFLD_CMD;\n\tint ret;\n\n\tif (unlikely(uld_type == CXGB4_TX_CRYPTO))\n\t\tcmd = FW_EQ_CTRL_CMD;\n\n\tret = t4_sge_alloc_ofld_txq(adap, &txq->q, dev, cmd, iqid);\n\tif (ret)\n\t\treturn ret;\n\n\ttxq->q.q_type = CXGB4_TXQ_ULD;\n\ttxq->adap = adap;\n\tskb_queue_head_init(&txq->sendq);\n\ttasklet_setup(&txq->qresume_tsk, restart_ofldq);\n\ttxq->full = 0;\n\ttxq->mapping_err = 0;\n\treturn 0;\n}\n\nint t4_sge_alloc_ethofld_txq(struct adapter *adap, struct sge_eohw_txq *txq,\n\t\t\t     struct net_device *dev, u32 iqid)\n{\n\tint ret;\n\n\tret = t4_sge_alloc_ofld_txq(adap, &txq->q, dev, FW_EQ_OFLD_CMD, iqid);\n\tif (ret)\n\t\treturn ret;\n\n\ttxq->q.q_type = CXGB4_TXQ_ULD;\n\tspin_lock_init(&txq->lock);\n\ttxq->adap = adap;\n\ttxq->tso = 0;\n\ttxq->uso = 0;\n\ttxq->tx_cso = 0;\n\ttxq->vlan_ins = 0;\n\ttxq->mapping_err = 0;\n\treturn 0;\n}\n\nvoid free_txq(struct adapter *adap, struct sge_txq *q)\n{\n\tstruct sge *s = &adap->sge;\n\n\tdma_free_coherent(adap->pdev_dev,\n\t\t\t  q->size * sizeof(struct tx_desc) + s->stat_len,\n\t\t\t  q->desc, q->phys_addr);\n\tq->cntxt_id = 0;\n\tq->sdesc = NULL;\n\tq->desc = NULL;\n}\n\nvoid free_rspq_fl(struct adapter *adap, struct sge_rspq *rq,\n\t\t  struct sge_fl *fl)\n{\n\tstruct sge *s = &adap->sge;\n\tunsigned int fl_id = fl ? fl->cntxt_id : 0xffff;\n\n\tadap->sge.ingr_map[rq->cntxt_id - adap->sge.ingr_start] = NULL;\n\tt4_iq_free(adap, adap->mbox, adap->pf, 0, FW_IQ_TYPE_FL_INT_CAP,\n\t\t   rq->cntxt_id, fl_id, 0xffff);\n\tdma_free_coherent(adap->pdev_dev, (rq->size + 1) * rq->iqe_len,\n\t\t\t  rq->desc, rq->phys_addr);\n\tnetif_napi_del(&rq->napi);\n\trq->netdev = NULL;\n\trq->cntxt_id = rq->abs_id = 0;\n\trq->desc = NULL;\n\n\tif (fl) {\n\t\tfree_rx_bufs(adap, fl, fl->avail);\n\t\tdma_free_coherent(adap->pdev_dev, fl->size * 8 + s->stat_len,\n\t\t\t\t  fl->desc, fl->addr);\n\t\tkfree(fl->sdesc);\n\t\tfl->sdesc = NULL;\n\t\tfl->cntxt_id = 0;\n\t\tfl->desc = NULL;\n\t}\n}\n\n \nvoid t4_free_ofld_rxqs(struct adapter *adap, int n, struct sge_ofld_rxq *q)\n{\n\tfor ( ; n; n--, q++)\n\t\tif (q->rspq.desc)\n\t\t\tfree_rspq_fl(adap, &q->rspq,\n\t\t\t\t     q->fl.size ? &q->fl : NULL);\n}\n\nvoid t4_sge_free_ethofld_txq(struct adapter *adap, struct sge_eohw_txq *txq)\n{\n\tif (txq->q.desc) {\n\t\tt4_ofld_eq_free(adap, adap->mbox, adap->pf, 0,\n\t\t\t\ttxq->q.cntxt_id);\n\t\tfree_tx_desc(adap, &txq->q, txq->q.in_use, false);\n\t\tkfree(txq->q.sdesc);\n\t\tfree_txq(adap, &txq->q);\n\t}\n}\n\n \nvoid t4_free_sge_resources(struct adapter *adap)\n{\n\tint i;\n\tstruct sge_eth_rxq *eq;\n\tstruct sge_eth_txq *etq;\n\n\t \n\tfor (i = 0; i < adap->sge.ethqsets; i++) {\n\t\teq = &adap->sge.ethrxq[i];\n\t\tif (eq->rspq.desc)\n\t\t\tt4_iq_stop(adap, adap->mbox, adap->pf, 0,\n\t\t\t\t   FW_IQ_TYPE_FL_INT_CAP,\n\t\t\t\t   eq->rspq.cntxt_id,\n\t\t\t\t   eq->fl.size ? eq->fl.cntxt_id : 0xffff,\n\t\t\t\t   0xffff);\n\t}\n\n\t \n\tfor (i = 0; i < adap->sge.ethqsets; i++) {\n\t\teq = &adap->sge.ethrxq[i];\n\t\tif (eq->rspq.desc)\n\t\t\tfree_rspq_fl(adap, &eq->rspq,\n\t\t\t\t     eq->fl.size ? &eq->fl : NULL);\n\t\tif (eq->msix) {\n\t\t\tcxgb4_free_msix_idx_in_bmap(adap, eq->msix->idx);\n\t\t\teq->msix = NULL;\n\t\t}\n\n\t\tetq = &adap->sge.ethtxq[i];\n\t\tif (etq->q.desc) {\n\t\t\tt4_eth_eq_free(adap, adap->mbox, adap->pf, 0,\n\t\t\t\t       etq->q.cntxt_id);\n\t\t\t__netif_tx_lock_bh(etq->txq);\n\t\t\tfree_tx_desc(adap, &etq->q, etq->q.in_use, true);\n\t\t\t__netif_tx_unlock_bh(etq->txq);\n\t\t\tkfree(etq->q.sdesc);\n\t\t\tfree_txq(adap, &etq->q);\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(adap->sge.ctrlq); i++) {\n\t\tstruct sge_ctrl_txq *cq = &adap->sge.ctrlq[i];\n\n\t\tif (cq->q.desc) {\n\t\t\ttasklet_kill(&cq->qresume_tsk);\n\t\t\tt4_ctrl_eq_free(adap, adap->mbox, adap->pf, 0,\n\t\t\t\t\tcq->q.cntxt_id);\n\t\t\t__skb_queue_purge(&cq->sendq);\n\t\t\tfree_txq(adap, &cq->q);\n\t\t}\n\t}\n\n\tif (adap->sge.fw_evtq.desc) {\n\t\tfree_rspq_fl(adap, &adap->sge.fw_evtq, NULL);\n\t\tif (adap->sge.fwevtq_msix_idx >= 0)\n\t\t\tcxgb4_free_msix_idx_in_bmap(adap,\n\t\t\t\t\t\t    adap->sge.fwevtq_msix_idx);\n\t}\n\n\tif (adap->sge.nd_msix_idx >= 0)\n\t\tcxgb4_free_msix_idx_in_bmap(adap, adap->sge.nd_msix_idx);\n\n\tif (adap->sge.intrq.desc)\n\t\tfree_rspq_fl(adap, &adap->sge.intrq, NULL);\n\n\tif (!is_t4(adap->params.chip)) {\n\t\tetq = &adap->sge.ptptxq;\n\t\tif (etq->q.desc) {\n\t\t\tt4_eth_eq_free(adap, adap->mbox, adap->pf, 0,\n\t\t\t\t       etq->q.cntxt_id);\n\t\t\tspin_lock_bh(&adap->ptp_lock);\n\t\t\tfree_tx_desc(adap, &etq->q, etq->q.in_use, true);\n\t\t\tspin_unlock_bh(&adap->ptp_lock);\n\t\t\tkfree(etq->q.sdesc);\n\t\t\tfree_txq(adap, &etq->q);\n\t\t}\n\t}\n\n\t \n\tmemset(adap->sge.egr_map, 0,\n\t       adap->sge.egr_sz * sizeof(*adap->sge.egr_map));\n}\n\nvoid t4_sge_start(struct adapter *adap)\n{\n\tadap->sge.ethtxq_rover = 0;\n\tmod_timer(&adap->sge.rx_timer, jiffies + RX_QCHECK_PERIOD);\n\tmod_timer(&adap->sge.tx_timer, jiffies + TX_QCHECK_PERIOD);\n}\n\n \nvoid t4_sge_stop(struct adapter *adap)\n{\n\tint i;\n\tstruct sge *s = &adap->sge;\n\n\tif (s->rx_timer.function)\n\t\tdel_timer_sync(&s->rx_timer);\n\tif (s->tx_timer.function)\n\t\tdel_timer_sync(&s->tx_timer);\n\n\tif (is_offload(adap)) {\n\t\tstruct sge_uld_txq_info *txq_info;\n\n\t\ttxq_info = adap->sge.uld_txq_info[CXGB4_TX_OFLD];\n\t\tif (txq_info) {\n\t\t\tstruct sge_uld_txq *txq = txq_info->uldtxq;\n\n\t\t\tfor_each_ofldtxq(&adap->sge, i) {\n\t\t\t\tif (txq->q.desc)\n\t\t\t\t\ttasklet_kill(&txq->qresume_tsk);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (is_pci_uld(adap)) {\n\t\tstruct sge_uld_txq_info *txq_info;\n\n\t\ttxq_info = adap->sge.uld_txq_info[CXGB4_TX_CRYPTO];\n\t\tif (txq_info) {\n\t\t\tstruct sge_uld_txq *txq = txq_info->uldtxq;\n\n\t\t\tfor_each_ofldtxq(&adap->sge, i) {\n\t\t\t\tif (txq->q.desc)\n\t\t\t\t\ttasklet_kill(&txq->qresume_tsk);\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(s->ctrlq); i++) {\n\t\tstruct sge_ctrl_txq *cq = &s->ctrlq[i];\n\n\t\tif (cq->q.desc)\n\t\t\ttasklet_kill(&cq->qresume_tsk);\n\t}\n}\n\n \n\nstatic int t4_sge_init_soft(struct adapter *adap)\n{\n\tstruct sge *s = &adap->sge;\n\tu32 fl_small_pg, fl_large_pg, fl_small_mtu, fl_large_mtu;\n\tu32 timer_value_0_and_1, timer_value_2_and_3, timer_value_4_and_5;\n\tu32 ingress_rx_threshold;\n\n\t \n\tif ((t4_read_reg(adap, SGE_CONTROL_A) & RXPKTCPLMODE_F) !=\n\t    RXPKTCPLMODE_V(RXPKTCPLMODE_SPLIT_X)) {\n\t\tdev_err(adap->pdev_dev, \"bad SGE CPL MODE\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\t#define READ_FL_BUF(x) \\\n\t\tt4_read_reg(adap, SGE_FL_BUFFER_SIZE0_A+(x)*sizeof(u32))\n\n\tfl_small_pg = READ_FL_BUF(RX_SMALL_PG_BUF);\n\tfl_large_pg = READ_FL_BUF(RX_LARGE_PG_BUF);\n\tfl_small_mtu = READ_FL_BUF(RX_SMALL_MTU_BUF);\n\tfl_large_mtu = READ_FL_BUF(RX_LARGE_MTU_BUF);\n\n\t \n\tif (fl_large_pg <= fl_small_pg)\n\t\tfl_large_pg = 0;\n\n\t#undef READ_FL_BUF\n\n\t \n\tif (fl_small_pg != PAGE_SIZE ||\n\t    (fl_large_pg & (fl_large_pg-1)) != 0) {\n\t\tdev_err(adap->pdev_dev, \"bad SGE FL page buffer sizes [%d, %d]\\n\",\n\t\t\tfl_small_pg, fl_large_pg);\n\t\treturn -EINVAL;\n\t}\n\tif (fl_large_pg)\n\t\ts->fl_pg_order = ilog2(fl_large_pg) - PAGE_SHIFT;\n\n\tif (fl_small_mtu < FL_MTU_SMALL_BUFSIZE(adap) ||\n\t    fl_large_mtu < FL_MTU_LARGE_BUFSIZE(adap)) {\n\t\tdev_err(adap->pdev_dev, \"bad SGE FL MTU sizes [%d, %d]\\n\",\n\t\t\tfl_small_mtu, fl_large_mtu);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\ttimer_value_0_and_1 = t4_read_reg(adap, SGE_TIMER_VALUE_0_AND_1_A);\n\ttimer_value_2_and_3 = t4_read_reg(adap, SGE_TIMER_VALUE_2_AND_3_A);\n\ttimer_value_4_and_5 = t4_read_reg(adap, SGE_TIMER_VALUE_4_AND_5_A);\n\ts->timer_val[0] = core_ticks_to_us(adap,\n\t\tTIMERVALUE0_G(timer_value_0_and_1));\n\ts->timer_val[1] = core_ticks_to_us(adap,\n\t\tTIMERVALUE1_G(timer_value_0_and_1));\n\ts->timer_val[2] = core_ticks_to_us(adap,\n\t\tTIMERVALUE2_G(timer_value_2_and_3));\n\ts->timer_val[3] = core_ticks_to_us(adap,\n\t\tTIMERVALUE3_G(timer_value_2_and_3));\n\ts->timer_val[4] = core_ticks_to_us(adap,\n\t\tTIMERVALUE4_G(timer_value_4_and_5));\n\ts->timer_val[5] = core_ticks_to_us(adap,\n\t\tTIMERVALUE5_G(timer_value_4_and_5));\n\n\tingress_rx_threshold = t4_read_reg(adap, SGE_INGRESS_RX_THRESHOLD_A);\n\ts->counter_val[0] = THRESHOLD_0_G(ingress_rx_threshold);\n\ts->counter_val[1] = THRESHOLD_1_G(ingress_rx_threshold);\n\ts->counter_val[2] = THRESHOLD_2_G(ingress_rx_threshold);\n\ts->counter_val[3] = THRESHOLD_3_G(ingress_rx_threshold);\n\n\treturn 0;\n}\n\n \nint t4_sge_init(struct adapter *adap)\n{\n\tstruct sge *s = &adap->sge;\n\tu32 sge_control, sge_conm_ctrl;\n\tint ret, egress_threshold;\n\n\t \n\tsge_control = t4_read_reg(adap, SGE_CONTROL_A);\n\ts->pktshift = PKTSHIFT_G(sge_control);\n\ts->stat_len = (sge_control & EGRSTATUSPAGESIZE_F) ? 128 : 64;\n\n\ts->fl_align = t4_fl_pkt_align(adap);\n\tret = t4_sge_init_soft(adap);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tsge_conm_ctrl = t4_read_reg(adap, SGE_CONM_CTRL_A);\n\tswitch (CHELSIO_CHIP_VERSION(adap->params.chip)) {\n\tcase CHELSIO_T4:\n\t\tegress_threshold = EGRTHRESHOLD_G(sge_conm_ctrl);\n\t\tbreak;\n\tcase CHELSIO_T5:\n\t\tegress_threshold = EGRTHRESHOLDPACKING_G(sge_conm_ctrl);\n\t\tbreak;\n\tcase CHELSIO_T6:\n\t\tegress_threshold = T6_EGRTHRESHOLDPACKING_G(sge_conm_ctrl);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(adap->pdev_dev, \"Unsupported Chip version %d\\n\",\n\t\t\tCHELSIO_CHIP_VERSION(adap->params.chip));\n\t\treturn -EINVAL;\n\t}\n\ts->fl_starve_thres = 2*egress_threshold + 1;\n\n\tt4_idma_monitor_init(adap, &s->idma_monitor);\n\n\t \n\ttimer_setup(&s->rx_timer, sge_rx_timer_cb, 0);\n\ttimer_setup(&s->tx_timer, sge_tx_timer_cb, 0);\n\n\tspin_lock_init(&s->intrq_lock);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}