{
  "module_name": "cxgb4_uld.c",
  "hash_id": "14b9cc43d6ee2f76307b7aae4de97714a619765a1a409d0af794ea1c37acc131",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/cxgb4/cxgb4_uld.c",
  "human_readable_source": " \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/debugfs.h>\n#include <linux/export.h>\n#include <linux/list.h>\n#include <linux/skbuff.h>\n#include <linux/pci.h>\n\n#include \"cxgb4.h\"\n#include \"cxgb4_uld.h\"\n#include \"t4_regs.h\"\n#include \"t4fw_api.h\"\n#include \"t4_msg.h\"\n\n#define for_each_uldrxq(m, i) for (i = 0; i < ((m)->nrxq + (m)->nciq); i++)\n\n \nstatic void uldrx_flush_handler(struct sge_rspq *q)\n{\n\tstruct adapter *adap = q->adap;\n\n\tif (adap->uld[q->uld].lro_flush)\n\t\tadap->uld[q->uld].lro_flush(&q->lro_mgr);\n}\n\n \nstatic int uldrx_handler(struct sge_rspq *q, const __be64 *rsp,\n\t\t\t const struct pkt_gl *gl)\n{\n\tstruct adapter *adap = q->adap;\n\tstruct sge_ofld_rxq *rxq = container_of(q, struct sge_ofld_rxq, rspq);\n\tint ret;\n\n\t \n\tif (((const struct rss_header *)rsp)->opcode == CPL_FW4_MSG &&\n\t    ((const struct cpl_fw4_msg *)(rsp + 1))->type == FW_TYPE_RSSCPL)\n\t\trsp += 2;\n\n\tif (q->flush_handler)\n\t\tret = adap->uld[q->uld].lro_rx_handler(adap->uld[q->uld].handle,\n\t\t\t\trsp, gl, &q->lro_mgr,\n\t\t\t\t&q->napi);\n\telse\n\t\tret = adap->uld[q->uld].rx_handler(adap->uld[q->uld].handle,\n\t\t\t\trsp, gl);\n\n\tif (ret) {\n\t\trxq->stats.nomem++;\n\t\treturn -1;\n\t}\n\n\tif (!gl)\n\t\trxq->stats.imm++;\n\telse if (gl == CXGB4_MSG_AN)\n\t\trxq->stats.an++;\n\telse\n\t\trxq->stats.pkts++;\n\treturn 0;\n}\n\nstatic int alloc_uld_rxqs(struct adapter *adap,\n\t\t\t  struct sge_uld_rxq_info *rxq_info, bool lro)\n{\n\tunsigned int nq = rxq_info->nrxq + rxq_info->nciq;\n\tstruct sge_ofld_rxq *q = rxq_info->uldrxq;\n\tunsigned short *ids = rxq_info->rspq_id;\n\tint i, err, msi_idx, que_idx = 0;\n\tstruct sge *s = &adap->sge;\n\tunsigned int per_chan;\n\n\tper_chan = rxq_info->nrxq / adap->params.nports;\n\n\tif (adap->flags & CXGB4_USING_MSIX)\n\t\tmsi_idx = 1;\n\telse\n\t\tmsi_idx = -((int)s->intrq.abs_id + 1);\n\n\tfor (i = 0; i < nq; i++, q++) {\n\t\tif (i == rxq_info->nrxq) {\n\t\t\t \n\t\t\tper_chan = rxq_info->nciq / adap->params.nports;\n\t\t\tque_idx = 0;\n\t\t}\n\n\t\tif (msi_idx >= 0) {\n\t\t\tmsi_idx = cxgb4_get_msix_idx_from_bmap(adap);\n\t\t\tif (msi_idx < 0) {\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tgoto freeout;\n\t\t\t}\n\n\t\t\tsnprintf(adap->msix_info[msi_idx].desc,\n\t\t\t\t sizeof(adap->msix_info[msi_idx].desc),\n\t\t\t\t \"%s-%s%d\",\n\t\t\t\t adap->port[0]->name, rxq_info->name, i);\n\n\t\t\tq->msix = &adap->msix_info[msi_idx];\n\t\t}\n\t\terr = t4_sge_alloc_rxq(adap, &q->rspq, false,\n\t\t\t\t       adap->port[que_idx++ / per_chan],\n\t\t\t\t       msi_idx,\n\t\t\t\t       q->fl.size ? &q->fl : NULL,\n\t\t\t\t       uldrx_handler,\n\t\t\t\t       lro ? uldrx_flush_handler : NULL,\n\t\t\t\t       0);\n\t\tif (err)\n\t\t\tgoto freeout;\n\n\t\tmemset(&q->stats, 0, sizeof(q->stats));\n\t\tif (ids)\n\t\t\tids[i] = q->rspq.abs_id;\n\t}\n\treturn 0;\nfreeout:\n\tq = rxq_info->uldrxq;\n\tfor ( ; i; i--, q++) {\n\t\tif (q->rspq.desc)\n\t\t\tfree_rspq_fl(adap, &q->rspq,\n\t\t\t\t     q->fl.size ? &q->fl : NULL);\n\t\tif (q->msix)\n\t\t\tcxgb4_free_msix_idx_in_bmap(adap, q->msix->idx);\n\t}\n\treturn err;\n}\n\nstatic int\nsetup_sge_queues_uld(struct adapter *adap, unsigned int uld_type, bool lro)\n{\n\tstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\n\tint i, ret;\n\n\tret = alloc_uld_rxqs(adap, rxq_info, lro);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (adap->flags & CXGB4_FULL_INIT_DONE && uld_type == CXGB4_ULD_RDMA) {\n\t\tstruct sge *s = &adap->sge;\n\t\tunsigned int cmplqid;\n\t\tu32 param, cmdop;\n\n\t\tcmdop = FW_PARAMS_PARAM_DMAQ_EQ_CMPLIQID_CTRL;\n\t\tfor_each_port(adap, i) {\n\t\t\tcmplqid = rxq_info->uldrxq[i].rspq.cntxt_id;\n\t\t\tparam = (FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DMAQ) |\n\t\t\t\t FW_PARAMS_PARAM_X_V(cmdop) |\n\t\t\t\t FW_PARAMS_PARAM_YZ_V(s->ctrlq[i].q.cntxt_id));\n\t\t\tret = t4_set_params(adap, adap->mbox, adap->pf,\n\t\t\t\t\t    0, 1, &param, &cmplqid);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic void t4_free_uld_rxqs(struct adapter *adap, int n,\n\t\t\t     struct sge_ofld_rxq *q)\n{\n\tfor ( ; n; n--, q++) {\n\t\tif (q->rspq.desc)\n\t\t\tfree_rspq_fl(adap, &q->rspq,\n\t\t\t\t     q->fl.size ? &q->fl : NULL);\n\t}\n}\n\nstatic void free_sge_queues_uld(struct adapter *adap, unsigned int uld_type)\n{\n\tstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\n\n\tif (adap->flags & CXGB4_FULL_INIT_DONE && uld_type == CXGB4_ULD_RDMA) {\n\t\tstruct sge *s = &adap->sge;\n\t\tu32 param, cmdop, cmplqid = 0;\n\t\tint i;\n\n\t\tcmdop = FW_PARAMS_PARAM_DMAQ_EQ_CMPLIQID_CTRL;\n\t\tfor_each_port(adap, i) {\n\t\t\tparam = (FW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DMAQ) |\n\t\t\t\t FW_PARAMS_PARAM_X_V(cmdop) |\n\t\t\t\t FW_PARAMS_PARAM_YZ_V(s->ctrlq[i].q.cntxt_id));\n\t\t\tt4_set_params(adap, adap->mbox, adap->pf,\n\t\t\t\t      0, 1, &param, &cmplqid);\n\t\t}\n\t}\n\n\tif (rxq_info->nciq)\n\t\tt4_free_uld_rxqs(adap, rxq_info->nciq,\n\t\t\t\t rxq_info->uldrxq + rxq_info->nrxq);\n\tt4_free_uld_rxqs(adap, rxq_info->nrxq, rxq_info->uldrxq);\n}\n\nstatic int cfg_queues_uld(struct adapter *adap, unsigned int uld_type,\n\t\t\t  const struct cxgb4_uld_info *uld_info)\n{\n\tstruct sge *s = &adap->sge;\n\tstruct sge_uld_rxq_info *rxq_info;\n\tint i, nrxq, ciq_size;\n\n\trxq_info = kzalloc(sizeof(*rxq_info), GFP_KERNEL);\n\tif (!rxq_info)\n\t\treturn -ENOMEM;\n\n\tif (adap->flags & CXGB4_USING_MSIX && uld_info->nrxq > s->nqs_per_uld) {\n\t\ti = s->nqs_per_uld;\n\t\trxq_info->nrxq = roundup(i, adap->params.nports);\n\t} else {\n\t\ti = min_t(int, uld_info->nrxq,\n\t\t\t  num_online_cpus());\n\t\trxq_info->nrxq = roundup(i, adap->params.nports);\n\t}\n\tif (!uld_info->ciq) {\n\t\trxq_info->nciq = 0;\n\t} else  {\n\t\tif (adap->flags & CXGB4_USING_MSIX)\n\t\t\trxq_info->nciq = min_t(int, s->nqs_per_uld,\n\t\t\t\t\t       num_online_cpus());\n\t\telse\n\t\t\trxq_info->nciq = min_t(int, MAX_OFLD_QSETS,\n\t\t\t\t\t       num_online_cpus());\n\t\trxq_info->nciq = ((rxq_info->nciq / adap->params.nports) *\n\t\t\t\t  adap->params.nports);\n\t\trxq_info->nciq = max_t(int, rxq_info->nciq,\n\t\t\t\t       adap->params.nports);\n\t}\n\n\tnrxq = rxq_info->nrxq + rxq_info->nciq;  \n\trxq_info->uldrxq = kcalloc(nrxq, sizeof(struct sge_ofld_rxq),\n\t\t\t\t   GFP_KERNEL);\n\tif (!rxq_info->uldrxq) {\n\t\tkfree(rxq_info);\n\t\treturn -ENOMEM;\n\t}\n\n\trxq_info->rspq_id = kcalloc(nrxq, sizeof(unsigned short), GFP_KERNEL);\n\tif (!rxq_info->rspq_id) {\n\t\tkfree(rxq_info->uldrxq);\n\t\tkfree(rxq_info);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < rxq_info->nrxq; i++) {\n\t\tstruct sge_ofld_rxq *r = &rxq_info->uldrxq[i];\n\n\t\tinit_rspq(adap, &r->rspq, 5, 1, uld_info->rxq_size, 64);\n\t\tr->rspq.uld = uld_type;\n\t\tr->fl.size = 72;\n\t}\n\n\tciq_size = 64 + adap->vres.cq.size + adap->tids.nftids;\n\tif (ciq_size > SGE_MAX_IQ_SIZE) {\n\t\tdev_warn(adap->pdev_dev, \"CIQ size too small for available IQs\\n\");\n\t\tciq_size = SGE_MAX_IQ_SIZE;\n\t}\n\n\tfor (i = rxq_info->nrxq; i < nrxq; i++) {\n\t\tstruct sge_ofld_rxq *r = &rxq_info->uldrxq[i];\n\n\t\tinit_rspq(adap, &r->rspq, 5, 1, ciq_size, 64);\n\t\tr->rspq.uld = uld_type;\n\t}\n\n\tmemcpy(rxq_info->name, uld_info->name, IFNAMSIZ);\n\tadap->sge.uld_rxq_info[uld_type] = rxq_info;\n\n\treturn 0;\n}\n\nstatic void free_queues_uld(struct adapter *adap, unsigned int uld_type)\n{\n\tstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\n\n\tadap->sge.uld_rxq_info[uld_type] = NULL;\n\tkfree(rxq_info->rspq_id);\n\tkfree(rxq_info->uldrxq);\n\tkfree(rxq_info);\n}\n\nstatic int\nrequest_msix_queue_irqs_uld(struct adapter *adap, unsigned int uld_type)\n{\n\tstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\n\tstruct msix_info *minfo;\n\tunsigned int idx;\n\tint err = 0;\n\n\tfor_each_uldrxq(rxq_info, idx) {\n\t\tminfo = rxq_info->uldrxq[idx].msix;\n\t\terr = request_irq(minfo->vec,\n\t\t\t\t  t4_sge_intr_msix, 0,\n\t\t\t\t  minfo->desc,\n\t\t\t\t  &rxq_info->uldrxq[idx].rspq);\n\t\tif (err)\n\t\t\tgoto unwind;\n\n\t\tcxgb4_set_msix_aff(adap, minfo->vec,\n\t\t\t\t   &minfo->aff_mask, idx);\n\t}\n\treturn 0;\n\nunwind:\n\twhile (idx-- > 0) {\n\t\tminfo = rxq_info->uldrxq[idx].msix;\n\t\tcxgb4_clear_msix_aff(minfo->vec, minfo->aff_mask);\n\t\tcxgb4_free_msix_idx_in_bmap(adap, minfo->idx);\n\t\tfree_irq(minfo->vec, &rxq_info->uldrxq[idx].rspq);\n\t}\n\treturn err;\n}\n\nstatic void\nfree_msix_queue_irqs_uld(struct adapter *adap, unsigned int uld_type)\n{\n\tstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\n\tstruct msix_info *minfo;\n\tunsigned int idx;\n\n\tfor_each_uldrxq(rxq_info, idx) {\n\t\tminfo = rxq_info->uldrxq[idx].msix;\n\t\tcxgb4_clear_msix_aff(minfo->vec, minfo->aff_mask);\n\t\tcxgb4_free_msix_idx_in_bmap(adap, minfo->idx);\n\t\tfree_irq(minfo->vec, &rxq_info->uldrxq[idx].rspq);\n\t}\n}\n\nstatic void enable_rx_uld(struct adapter *adap, unsigned int uld_type)\n{\n\tstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\n\tint idx;\n\n\tfor_each_uldrxq(rxq_info, idx) {\n\t\tstruct sge_rspq *q = &rxq_info->uldrxq[idx].rspq;\n\n\t\tif (!q)\n\t\t\tcontinue;\n\n\t\tcxgb4_enable_rx(adap, q);\n\t}\n}\n\nstatic void quiesce_rx_uld(struct adapter *adap, unsigned int uld_type)\n{\n\tstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\n\tint idx;\n\n\tfor_each_uldrxq(rxq_info, idx) {\n\t\tstruct sge_rspq *q = &rxq_info->uldrxq[idx].rspq;\n\n\t\tif (!q)\n\t\t\tcontinue;\n\n\t\tcxgb4_quiesce_rx(q);\n\t}\n}\n\nstatic void\nfree_sge_txq_uld(struct adapter *adap, struct sge_uld_txq_info *txq_info)\n{\n\tint nq = txq_info->ntxq;\n\tint i;\n\n\tfor (i = 0; i < nq; i++) {\n\t\tstruct sge_uld_txq *txq = &txq_info->uldtxq[i];\n\n\t\tif (txq && txq->q.desc) {\n\t\t\ttasklet_kill(&txq->qresume_tsk);\n\t\t\tt4_ofld_eq_free(adap, adap->mbox, adap->pf, 0,\n\t\t\t\t\ttxq->q.cntxt_id);\n\t\t\tfree_tx_desc(adap, &txq->q, txq->q.in_use, false);\n\t\t\tkfree(txq->q.sdesc);\n\t\t\t__skb_queue_purge(&txq->sendq);\n\t\t\tfree_txq(adap, &txq->q);\n\t\t}\n\t}\n}\n\nstatic int\nalloc_sge_txq_uld(struct adapter *adap, struct sge_uld_txq_info *txq_info,\n\t\t  unsigned int uld_type)\n{\n\tstruct sge *s = &adap->sge;\n\tint nq = txq_info->ntxq;\n\tint i, j, err;\n\n\tj = nq / adap->params.nports;\n\tfor (i = 0; i < nq; i++) {\n\t\tstruct sge_uld_txq *txq = &txq_info->uldtxq[i];\n\n\t\ttxq->q.size = 1024;\n\t\terr = t4_sge_alloc_uld_txq(adap, txq, adap->port[i / j],\n\t\t\t\t\t   s->fw_evtq.cntxt_id, uld_type);\n\t\tif (err)\n\t\t\tgoto freeout;\n\t}\n\treturn 0;\nfreeout:\n\tfree_sge_txq_uld(adap, txq_info);\n\treturn err;\n}\n\nstatic void\nrelease_sge_txq_uld(struct adapter *adap, unsigned int uld_type)\n{\n\tstruct sge_uld_txq_info *txq_info = NULL;\n\tint tx_uld_type = TX_ULD(uld_type);\n\n\ttxq_info = adap->sge.uld_txq_info[tx_uld_type];\n\n\tif (txq_info && atomic_dec_and_test(&txq_info->users)) {\n\t\tfree_sge_txq_uld(adap, txq_info);\n\t\tkfree(txq_info->uldtxq);\n\t\tkfree(txq_info);\n\t\tadap->sge.uld_txq_info[tx_uld_type] = NULL;\n\t}\n}\n\nstatic int\nsetup_sge_txq_uld(struct adapter *adap, unsigned int uld_type,\n\t\t  const struct cxgb4_uld_info *uld_info)\n{\n\tstruct sge_uld_txq_info *txq_info = NULL;\n\tint tx_uld_type, i;\n\n\ttx_uld_type = TX_ULD(uld_type);\n\ttxq_info = adap->sge.uld_txq_info[tx_uld_type];\n\n\tif ((tx_uld_type == CXGB4_TX_OFLD) && txq_info &&\n\t    (atomic_inc_return(&txq_info->users) > 1))\n\t\treturn 0;\n\n\ttxq_info = kzalloc(sizeof(*txq_info), GFP_KERNEL);\n\tif (!txq_info)\n\t\treturn -ENOMEM;\n\tif (uld_type == CXGB4_ULD_CRYPTO) {\n\t\ti = min_t(int, adap->vres.ncrypto_fc,\n\t\t\t  num_online_cpus());\n\t\ttxq_info->ntxq = rounddown(i, adap->params.nports);\n\t\tif (txq_info->ntxq <= 0) {\n\t\t\tdev_warn(adap->pdev_dev, \"Crypto Tx Queues can't be zero\\n\");\n\t\t\tkfree(txq_info);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t} else {\n\t\ti = min_t(int, uld_info->ntxq, num_online_cpus());\n\t\ttxq_info->ntxq = roundup(i, adap->params.nports);\n\t}\n\ttxq_info->uldtxq = kcalloc(txq_info->ntxq, sizeof(struct sge_uld_txq),\n\t\t\t\t   GFP_KERNEL);\n\tif (!txq_info->uldtxq) {\n\t\tkfree(txq_info);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (alloc_sge_txq_uld(adap, txq_info, tx_uld_type)) {\n\t\tkfree(txq_info->uldtxq);\n\t\tkfree(txq_info);\n\t\treturn -ENOMEM;\n\t}\n\n\tatomic_inc(&txq_info->users);\n\tadap->sge.uld_txq_info[tx_uld_type] = txq_info;\n\treturn 0;\n}\n\nstatic void uld_queue_init(struct adapter *adap, unsigned int uld_type,\n\t\t\t   struct cxgb4_lld_info *lli)\n{\n\tstruct sge_uld_rxq_info *rxq_info = adap->sge.uld_rxq_info[uld_type];\n\tint tx_uld_type = TX_ULD(uld_type);\n\tstruct sge_uld_txq_info *txq_info = adap->sge.uld_txq_info[tx_uld_type];\n\n\tlli->rxq_ids = rxq_info->rspq_id;\n\tlli->nrxq = rxq_info->nrxq;\n\tlli->ciq_ids = rxq_info->rspq_id + rxq_info->nrxq;\n\tlli->nciq = rxq_info->nciq;\n\tlli->ntxq = txq_info->ntxq;\n}\n\nint t4_uld_mem_alloc(struct adapter *adap)\n{\n\tstruct sge *s = &adap->sge;\n\n\tadap->uld = kcalloc(CXGB4_ULD_MAX, sizeof(*adap->uld), GFP_KERNEL);\n\tif (!adap->uld)\n\t\treturn -ENOMEM;\n\n\ts->uld_rxq_info = kcalloc(CXGB4_ULD_MAX,\n\t\t\t\t  sizeof(struct sge_uld_rxq_info *),\n\t\t\t\t  GFP_KERNEL);\n\tif (!s->uld_rxq_info)\n\t\tgoto err_uld;\n\n\ts->uld_txq_info = kcalloc(CXGB4_TX_MAX,\n\t\t\t\t  sizeof(struct sge_uld_txq_info *),\n\t\t\t\t  GFP_KERNEL);\n\tif (!s->uld_txq_info)\n\t\tgoto err_uld_rx;\n\treturn 0;\n\nerr_uld_rx:\n\tkfree(s->uld_rxq_info);\nerr_uld:\n\tkfree(adap->uld);\n\treturn -ENOMEM;\n}\n\nvoid t4_uld_mem_free(struct adapter *adap)\n{\n\tstruct sge *s = &adap->sge;\n\n\tkfree(s->uld_txq_info);\n\tkfree(s->uld_rxq_info);\n\tkfree(adap->uld);\n}\n\n \nstatic void cxgb4_shutdown_uld_adapter(struct adapter *adap, enum cxgb4_uld type)\n{\n\tif (adap->uld[type].handle) {\n\t\tadap->uld[type].handle = NULL;\n\t\tadap->uld[type].add = NULL;\n\t\trelease_sge_txq_uld(adap, type);\n\n\t\tif (adap->flags & CXGB4_FULL_INIT_DONE)\n\t\t\tquiesce_rx_uld(adap, type);\n\n\t\tif (adap->flags & CXGB4_USING_MSIX)\n\t\t\tfree_msix_queue_irqs_uld(adap, type);\n\n\t\tfree_sge_queues_uld(adap, type);\n\t\tfree_queues_uld(adap, type);\n\t}\n}\n\nvoid t4_uld_clean_up(struct adapter *adap)\n{\n\tunsigned int i;\n\n\tif (!is_uld(adap))\n\t\treturn;\n\n\tmutex_lock(&uld_mutex);\n\tfor (i = 0; i < CXGB4_ULD_MAX; i++) {\n\t\tif (!adap->uld[i].handle)\n\t\t\tcontinue;\n\n\t\tcxgb4_shutdown_uld_adapter(adap, i);\n\t}\n\tmutex_unlock(&uld_mutex);\n}\n\nstatic void uld_init(struct adapter *adap, struct cxgb4_lld_info *lld)\n{\n\tint i;\n\n\tlld->pdev = adap->pdev;\n\tlld->pf = adap->pf;\n\tlld->l2t = adap->l2t;\n\tlld->tids = &adap->tids;\n\tlld->ports = adap->port;\n\tlld->vr = &adap->vres;\n\tlld->mtus = adap->params.mtus;\n\tlld->nchan = adap->params.nports;\n\tlld->nports = adap->params.nports;\n\tlld->wr_cred = adap->params.ofldq_wr_cred;\n\tlld->crypto = adap->params.crypto;\n\tlld->iscsi_iolen = MAXRXDATA_G(t4_read_reg(adap, TP_PARA_REG2_A));\n\tlld->iscsi_tagmask = t4_read_reg(adap, ULP_RX_ISCSI_TAGMASK_A);\n\tlld->iscsi_pgsz_order = t4_read_reg(adap, ULP_RX_ISCSI_PSZ_A);\n\tlld->iscsi_llimit = t4_read_reg(adap, ULP_RX_ISCSI_LLIMIT_A);\n\tlld->iscsi_ppm = &adap->iscsi_ppm;\n\tlld->adapter_type = adap->params.chip;\n\tlld->cclk_ps = 1000000000 / adap->params.vpd.cclk;\n\tlld->udb_density = 1 << adap->params.sge.eq_qpp;\n\tlld->ucq_density = 1 << adap->params.sge.iq_qpp;\n\tlld->sge_host_page_size = 1 << (adap->params.sge.hps + 10);\n\tlld->filt_mode = adap->params.tp.vlan_pri_map;\n\t \n\tfor (i = 0; i < NCHAN; i++)\n\t\tlld->tx_modq[i] = i;\n\tlld->gts_reg = adap->regs + MYPF_REG(SGE_PF_GTS_A);\n\tlld->db_reg = adap->regs + MYPF_REG(SGE_PF_KDOORBELL_A);\n\tlld->fw_vers = adap->params.fw_vers;\n\tlld->dbfifo_int_thresh = dbfifo_int_thresh;\n\tlld->sge_ingpadboundary = adap->sge.fl_align;\n\tlld->sge_egrstatuspagesize = adap->sge.stat_len;\n\tlld->sge_pktshift = adap->sge.pktshift;\n\tlld->ulp_crypto = adap->params.crypto;\n\tlld->enable_fw_ofld_conn = adap->flags & CXGB4_FW_OFLD_CONN;\n\tlld->max_ordird_qp = adap->params.max_ordird_qp;\n\tlld->max_ird_adapter = adap->params.max_ird_adapter;\n\tlld->ulptx_memwrite_dsgl = adap->params.ulptx_memwrite_dsgl;\n\tlld->nodeid = dev_to_node(adap->pdev_dev);\n\tlld->fr_nsmr_tpte_wr_support = adap->params.fr_nsmr_tpte_wr_support;\n\tlld->write_w_imm_support = adap->params.write_w_imm_support;\n\tlld->write_cmpl_support = adap->params.write_cmpl_support;\n}\n\nstatic int uld_attach(struct adapter *adap, unsigned int uld)\n{\n\tstruct cxgb4_lld_info lli;\n\tvoid *handle;\n\n\tuld_init(adap, &lli);\n\tuld_queue_init(adap, uld, &lli);\n\n\thandle = adap->uld[uld].add(&lli);\n\tif (IS_ERR(handle)) {\n\t\tdev_warn(adap->pdev_dev,\n\t\t\t \"could not attach to the %s driver, error %ld\\n\",\n\t\t\t adap->uld[uld].name, PTR_ERR(handle));\n\t\treturn PTR_ERR(handle);\n\t}\n\n\tadap->uld[uld].handle = handle;\n\tt4_register_netevent_notifier();\n\n\tif (adap->flags & CXGB4_FULL_INIT_DONE)\n\t\tadap->uld[uld].state_change(handle, CXGB4_STATE_UP);\n\n\treturn 0;\n}\n\n#if IS_ENABLED(CONFIG_CHELSIO_TLS_DEVICE)\nstatic bool cxgb4_uld_in_use(struct adapter *adap)\n{\n\tconst struct tid_info *t = &adap->tids;\n\n\treturn (atomic_read(&t->conns_in_use) || t->stids_in_use);\n}\n\n \nint cxgb4_set_ktls_feature(struct adapter *adap, bool enable)\n{\n\tint ret = 0;\n\tu32 params =\n\t\tFW_PARAMS_MNEM_V(FW_PARAMS_MNEM_DEV) |\n\t\tFW_PARAMS_PARAM_X_V(FW_PARAMS_PARAM_DEV_KTLS_HW) |\n\t\tFW_PARAMS_PARAM_Y_V(enable) |\n\t\tFW_PARAMS_PARAM_Z_V(FW_PARAMS_PARAM_DEV_KTLS_HW_USER_ENABLE);\n\n\tif (enable) {\n\t\tif (!refcount_read(&adap->chcr_ktls.ktls_refcount)) {\n\t\t\t \n\t\t\tif (cxgb4_uld_in_use(adap)) {\n\t\t\t\tdev_dbg(adap->pdev_dev,\n\t\t\t\t\t\"ULD connections (tid/stid) active. Can't enable kTLS\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tret = t4_set_params(adap, adap->mbox, adap->pf,\n\t\t\t\t\t    0, 1, &params, &params);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\trefcount_set(&adap->chcr_ktls.ktls_refcount, 1);\n\t\t\tpr_debug(\"kTLS has been enabled. Restrictions placed on ULD support\\n\");\n\t\t} else {\n\t\t\t \n\t\t\trefcount_inc(&adap->chcr_ktls.ktls_refcount);\n\t\t}\n\t} else {\n\t\t \n\t\tif (!refcount_read(&adap->chcr_ktls.ktls_refcount))\n\t\t\treturn -EINVAL;\n\t\t \n\t\tif (refcount_dec_and_test(&adap->chcr_ktls.ktls_refcount)) {\n\t\t\tret = t4_set_params(adap, adap->mbox, adap->pf,\n\t\t\t\t\t    0, 1, &params, &params);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tpr_debug(\"kTLS is disabled. Restrictions on ULD support removed\\n\");\n\t\t}\n\t}\n\n\treturn ret;\n}\n#endif\n\nstatic void cxgb4_uld_alloc_resources(struct adapter *adap,\n\t\t\t\t      enum cxgb4_uld type,\n\t\t\t\t      const struct cxgb4_uld_info *p)\n{\n\tint ret = 0;\n\n\tif ((type == CXGB4_ULD_CRYPTO && !is_pci_uld(adap)) ||\n\t    (type != CXGB4_ULD_CRYPTO && !is_offload(adap)))\n\t\treturn;\n\tif (type == CXGB4_ULD_ISCSIT && is_t4(adap->params.chip))\n\t\treturn;\n\tret = cfg_queues_uld(adap, type, p);\n\tif (ret)\n\t\tgoto out;\n\tret = setup_sge_queues_uld(adap, type, p->lro);\n\tif (ret)\n\t\tgoto free_queues;\n\tif (adap->flags & CXGB4_USING_MSIX) {\n\t\tret = request_msix_queue_irqs_uld(adap, type);\n\t\tif (ret)\n\t\t\tgoto free_rxq;\n\t}\n\tif (adap->flags & CXGB4_FULL_INIT_DONE)\n\t\tenable_rx_uld(adap, type);\n\tif (adap->uld[type].add)\n\t\tgoto free_irq;\n\tret = setup_sge_txq_uld(adap, type, p);\n\tif (ret)\n\t\tgoto free_irq;\n\tadap->uld[type] = *p;\n\tret = uld_attach(adap, type);\n\tif (ret)\n\t\tgoto free_txq;\n\treturn;\nfree_txq:\n\trelease_sge_txq_uld(adap, type);\nfree_irq:\n\tif (adap->flags & CXGB4_FULL_INIT_DONE)\n\t\tquiesce_rx_uld(adap, type);\n\tif (adap->flags & CXGB4_USING_MSIX)\n\t\tfree_msix_queue_irqs_uld(adap, type);\nfree_rxq:\n\tfree_sge_queues_uld(adap, type);\nfree_queues:\n\tfree_queues_uld(adap, type);\nout:\n\tdev_warn(adap->pdev_dev,\n\t\t \"ULD registration failed for uld type %d\\n\", type);\n}\n\nvoid cxgb4_uld_enable(struct adapter *adap)\n{\n\tstruct cxgb4_uld_list *uld_entry;\n\n\tmutex_lock(&uld_mutex);\n\tlist_add_tail(&adap->list_node, &adapter_list);\n\tlist_for_each_entry(uld_entry, &uld_list, list_node)\n\t\tcxgb4_uld_alloc_resources(adap, uld_entry->uld_type,\n\t\t\t\t\t  &uld_entry->uld_info);\n\tmutex_unlock(&uld_mutex);\n}\n\n \nvoid cxgb4_register_uld(enum cxgb4_uld type,\n\t\t\tconst struct cxgb4_uld_info *p)\n{\n\tstruct cxgb4_uld_list *uld_entry;\n\tstruct adapter *adap;\n\n\tif (type >= CXGB4_ULD_MAX)\n\t\treturn;\n\n\tuld_entry = kzalloc(sizeof(*uld_entry), GFP_KERNEL);\n\tif (!uld_entry)\n\t\treturn;\n\n\tmemcpy(&uld_entry->uld_info, p, sizeof(struct cxgb4_uld_info));\n\tmutex_lock(&uld_mutex);\n\tlist_for_each_entry(adap, &adapter_list, list_node)\n\t\tcxgb4_uld_alloc_resources(adap, type, p);\n\n\tuld_entry->uld_type = type;\n\tlist_add_tail(&uld_entry->list_node, &uld_list);\n\tmutex_unlock(&uld_mutex);\n\treturn;\n}\nEXPORT_SYMBOL(cxgb4_register_uld);\n\n \nint cxgb4_unregister_uld(enum cxgb4_uld type)\n{\n\tstruct cxgb4_uld_list *uld_entry, *tmp;\n\tstruct adapter *adap;\n\n\tif (type >= CXGB4_ULD_MAX)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&uld_mutex);\n\tlist_for_each_entry(adap, &adapter_list, list_node) {\n\t\tif ((type == CXGB4_ULD_CRYPTO && !is_pci_uld(adap)) ||\n\t\t    (type != CXGB4_ULD_CRYPTO && !is_offload(adap)))\n\t\t\tcontinue;\n\t\tif (type == CXGB4_ULD_ISCSIT && is_t4(adap->params.chip))\n\t\t\tcontinue;\n\n\t\tcxgb4_shutdown_uld_adapter(adap, type);\n\t}\n\n\tlist_for_each_entry_safe(uld_entry, tmp, &uld_list, list_node) {\n\t\tif (uld_entry->uld_type == type) {\n\t\t\tlist_del(&uld_entry->list_node);\n\t\t\tkfree(uld_entry);\n\t\t}\n\t}\n\tmutex_unlock(&uld_mutex);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(cxgb4_unregister_uld);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}