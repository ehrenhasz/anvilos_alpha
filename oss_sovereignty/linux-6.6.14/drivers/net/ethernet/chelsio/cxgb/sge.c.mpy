{
  "module_name": "sge.c",
  "hash_id": "91f91378fe43c40fe02c37cbca047081bfd9320446ab657848e34197408759bf",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/cxgb/sge.c",
  "human_readable_source": "\n \n\n#include \"common.h\"\n\n#include <linux/types.h>\n#include <linux/errno.h>\n#include <linux/pci.h>\n#include <linux/ktime.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/skbuff.h>\n#include <linux/mm.h>\n#include <linux/tcp.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/if_arp.h>\n#include <linux/slab.h>\n#include <linux/prefetch.h>\n\n#include \"cpl5_cmd.h\"\n#include \"sge.h\"\n#include \"regs.h\"\n#include \"espi.h\"\n\n \n#define ETH_P_CPL5 0xf\n\n#define SGE_CMDQ_N\t\t2\n#define SGE_FREELQ_N\t\t2\n#define SGE_CMDQ0_E_N\t\t1024\n#define SGE_CMDQ1_E_N\t\t128\n#define SGE_FREEL_SIZE\t\t4096\n#define SGE_JUMBO_FREEL_SIZE\t512\n#define SGE_FREEL_REFILL_THRESH\t16\n#define SGE_RESPQ_E_N\t\t1024\n#define SGE_INTRTIMER_NRES\t1000\n#define SGE_RX_SM_BUF_SIZE\t1536\n#define SGE_TX_DESC_MAX_PLEN\t16384\n\n#define SGE_RESPQ_REPLENISH_THRES (SGE_RESPQ_E_N / 4)\n\n \n#define TX_RECLAIM_PERIOD (HZ / 4)\n\n#define M_CMD_LEN       0x7fffffff\n#define V_CMD_LEN(v)    (v)\n#define G_CMD_LEN(v)    ((v) & M_CMD_LEN)\n#define V_CMD_GEN1(v)   ((v) << 31)\n#define V_CMD_GEN2(v)   (v)\n#define F_CMD_DATAVALID (1 << 1)\n#define F_CMD_SOP       (1 << 2)\n#define V_CMD_EOP(v)    ((v) << 3)\n\n \n#if defined(__BIG_ENDIAN_BITFIELD)\nstruct cmdQ_e {\n\tu32 addr_lo;\n\tu32 len_gen;\n\tu32 flags;\n\tu32 addr_hi;\n};\n\nstruct freelQ_e {\n\tu32 addr_lo;\n\tu32 len_gen;\n\tu32 gen2;\n\tu32 addr_hi;\n};\n\nstruct respQ_e {\n\tu32 Qsleeping\t\t: 4;\n\tu32 Cmdq1CreditReturn\t: 5;\n\tu32 Cmdq1DmaComplete\t: 5;\n\tu32 Cmdq0CreditReturn\t: 5;\n\tu32 Cmdq0DmaComplete\t: 5;\n\tu32 FreelistQid\t\t: 2;\n\tu32 CreditValid\t\t: 1;\n\tu32 DataValid\t\t: 1;\n\tu32 Offload\t\t: 1;\n\tu32 Eop\t\t\t: 1;\n\tu32 Sop\t\t\t: 1;\n\tu32 GenerationBit\t: 1;\n\tu32 BufferLength;\n};\n#elif defined(__LITTLE_ENDIAN_BITFIELD)\nstruct cmdQ_e {\n\tu32 len_gen;\n\tu32 addr_lo;\n\tu32 addr_hi;\n\tu32 flags;\n};\n\nstruct freelQ_e {\n\tu32 len_gen;\n\tu32 addr_lo;\n\tu32 addr_hi;\n\tu32 gen2;\n};\n\nstruct respQ_e {\n\tu32 BufferLength;\n\tu32 GenerationBit\t: 1;\n\tu32 Sop\t\t\t: 1;\n\tu32 Eop\t\t\t: 1;\n\tu32 Offload\t\t: 1;\n\tu32 DataValid\t\t: 1;\n\tu32 CreditValid\t\t: 1;\n\tu32 FreelistQid\t\t: 2;\n\tu32 Cmdq0DmaComplete\t: 5;\n\tu32 Cmdq0CreditReturn\t: 5;\n\tu32 Cmdq1DmaComplete\t: 5;\n\tu32 Cmdq1CreditReturn\t: 5;\n\tu32 Qsleeping\t\t: 4;\n} ;\n#endif\n\n \nstruct cmdQ_ce {\n\tstruct sk_buff *skb;\n\tDEFINE_DMA_UNMAP_ADDR(dma_addr);\n\tDEFINE_DMA_UNMAP_LEN(dma_len);\n};\n\nstruct freelQ_ce {\n\tstruct sk_buff *skb;\n\tDEFINE_DMA_UNMAP_ADDR(dma_addr);\n\tDEFINE_DMA_UNMAP_LEN(dma_len);\n};\n\n \nstruct cmdQ {\n\tunsigned long   status;          \n\tunsigned int    in_use;          \n\tunsigned int\tsize;\t         \n\tunsigned int    processed;       \n\tunsigned int    cleaned;         \n\tunsigned int    stop_thres;      \n\tu16\t\tpidx;            \n\tu16\t\tcidx;            \n\tu8\t\tgenbit;          \n\tu8              sop;             \n\tstruct cmdQ_e  *entries;         \n\tstruct cmdQ_ce *centries;        \n\tdma_addr_t\tdma_addr;        \n\tspinlock_t\tlock;            \n};\n\nstruct freelQ {\n\tunsigned int\tcredits;         \n\tunsigned int\tsize;\t         \n\tu16\t\tpidx;            \n\tu16\t\tcidx;            \n\tu16\t\trx_buffer_size;  \n\tu16             dma_offset;      \n\tu16             recycleq_idx;    \n\tu8\t\tgenbit;\t         \n\tstruct freelQ_e\t*entries;        \n\tstruct freelQ_ce *centries;      \n\tdma_addr_t\tdma_addr;        \n};\n\nstruct respQ {\n\tunsigned int\tcredits;         \n\tunsigned int\tsize;\t         \n\tu16\t\tcidx;\t         \n\tu8\t\tgenbit;\t         \n\tstruct respQ_e *entries;         \n\tdma_addr_t\tdma_addr;        \n};\n\n \nenum {\n\tCMDQ_STAT_RUNNING = 1,           \n\tCMDQ_STAT_LAST_PKT_DB = 2        \n};\n\n \n\n \nstruct sched_port {\n\tunsigned int\tavail;\t\t \n\tunsigned int\tdrain_bits_per_1024ns;  \n\tunsigned int\tspeed;\t\t \n\tunsigned int\tmtu;\t\t \n\tstruct sk_buff_head skbq;\t \n};\n\n \nstruct sched {\n\tktime_t         last_updated;    \n\tunsigned int\tmax_avail;\t \n\tunsigned int\tport;\t\t \n\tunsigned int\tnum;\t\t \n\tstruct sched_port p[MAX_NPORTS];\n\tstruct tasklet_struct sched_tsk; \n\tstruct sge *sge;\n};\n\nstatic void restart_sched(struct tasklet_struct *t);\n\n\n \nstruct sge {\n\tstruct adapter *adapter;\t \n\tstruct net_device *netdev;       \n\tstruct freelQ\tfreelQ[SGE_FREELQ_N];  \n\tstruct respQ\trespQ;\t\t \n\tunsigned long   stopped_tx_queues;  \n\tunsigned int\trx_pkt_pad;      \n\tunsigned int\tjumbo_fl;        \n\tunsigned int\tintrtimer_nres;\t \n\tunsigned int    fixed_intrtimer; \n\tstruct timer_list tx_reclaim_timer;  \n\tstruct timer_list espibug_timer;\n\tunsigned long\tespibug_timeout;\n\tstruct sk_buff\t*espibug_skb[MAX_NPORTS];\n\tu32\t\tsge_control;\t \n\tstruct sge_intr_counts stats;\n\tstruct sge_port_stats __percpu *port_stats[MAX_NPORTS];\n\tstruct sched\t*tx_sched;\n\tstruct cmdQ cmdQ[SGE_CMDQ_N] ____cacheline_aligned_in_smp;\n};\n\nstatic const u8 ch_mac_addr[ETH_ALEN] = {\n\t0x0, 0x7, 0x43, 0x0, 0x0, 0x0\n};\n\n \nstatic void tx_sched_stop(struct sge *sge)\n{\n\tstruct sched *s = sge->tx_sched;\n\tint i;\n\n\ttasklet_kill(&s->sched_tsk);\n\n\tfor (i = 0; i < MAX_NPORTS; i++)\n\t\t__skb_queue_purge(&s->p[s->port].skbq);\n}\n\n \nunsigned int t1_sched_update_parms(struct sge *sge, unsigned int port,\n\t\t\t\t   unsigned int mtu, unsigned int speed)\n{\n\tstruct sched *s = sge->tx_sched;\n\tstruct sched_port *p = &s->p[port];\n\tunsigned int max_avail_segs;\n\n\tpr_debug(\"%s mtu=%d speed=%d\\n\", __func__, mtu, speed);\n\tif (speed)\n\t\tp->speed = speed;\n\tif (mtu)\n\t\tp->mtu = mtu;\n\n\tif (speed || mtu) {\n\t\tunsigned long long drain = 1024ULL * p->speed * (p->mtu - 40);\n\t\tdo_div(drain, (p->mtu + 50) * 1000);\n\t\tp->drain_bits_per_1024ns = (unsigned int) drain;\n\n\t\tif (p->speed < 1000)\n\t\t\tp->drain_bits_per_1024ns =\n\t\t\t\t90 * p->drain_bits_per_1024ns / 100;\n\t}\n\n\tif (board_info(sge->adapter)->board == CHBT_BOARD_CHT204) {\n\t\tp->drain_bits_per_1024ns -= 16;\n\t\ts->max_avail = max(4096U, p->mtu + 16 + 14 + 4);\n\t\tmax_avail_segs = max(1U, 4096 / (p->mtu - 40));\n\t} else {\n\t\ts->max_avail = 16384;\n\t\tmax_avail_segs = max(1U, 9000 / (p->mtu - 40));\n\t}\n\n\tpr_debug(\"t1_sched_update_parms: mtu %u speed %u max_avail %u \"\n\t\t \"max_avail_segs %u drain_bits_per_1024ns %u\\n\", p->mtu,\n\t\t p->speed, s->max_avail, max_avail_segs,\n\t\t p->drain_bits_per_1024ns);\n\n\treturn max_avail_segs * (p->mtu - 40);\n}\n\n#if 0\n\n \nvoid t1_sched_set_max_avail_bytes(struct sge *sge, unsigned int val)\n{\n\tstruct sched *s = sge->tx_sched;\n\tunsigned int i;\n\n\ts->max_avail = val;\n\tfor (i = 0; i < MAX_NPORTS; i++)\n\t\tt1_sched_update_parms(sge, i, 0, 0);\n}\n\n \nvoid t1_sched_set_drain_bits_per_us(struct sge *sge, unsigned int port,\n\t\t\t\t\t unsigned int val)\n{\n\tstruct sched *s = sge->tx_sched;\n\tstruct sched_port *p = &s->p[port];\n\tp->drain_bits_per_1024ns = val * 1024 / 1000;\n\tt1_sched_update_parms(sge, port, 0, 0);\n}\n\n#endif   \n\n \nstatic int tx_sched_init(struct sge *sge)\n{\n\tstruct sched *s;\n\tint i;\n\n\ts = kzalloc(sizeof (struct sched), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\tpr_debug(\"tx_sched_init\\n\");\n\ttasklet_setup(&s->sched_tsk, restart_sched);\n\ts->sge = sge;\n\tsge->tx_sched = s;\n\n\tfor (i = 0; i < MAX_NPORTS; i++) {\n\t\tskb_queue_head_init(&s->p[i].skbq);\n\t\tt1_sched_update_parms(sge, i, 1500, 1000);\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline int sched_update_avail(struct sge *sge)\n{\n\tstruct sched *s = sge->tx_sched;\n\tktime_t now = ktime_get();\n\tunsigned int i;\n\tlong long delta_time_ns;\n\n\tdelta_time_ns = ktime_to_ns(ktime_sub(now, s->last_updated));\n\n\tpr_debug(\"sched_update_avail delta=%lld\\n\", delta_time_ns);\n\tif (delta_time_ns < 15000)\n\t\treturn 0;\n\n\tfor (i = 0; i < MAX_NPORTS; i++) {\n\t\tstruct sched_port *p = &s->p[i];\n\t\tunsigned int delta_avail;\n\n\t\tdelta_avail = (p->drain_bits_per_1024ns * delta_time_ns) >> 13;\n\t\tp->avail = min(p->avail + delta_avail, s->max_avail);\n\t}\n\n\ts->last_updated = now;\n\n\treturn 1;\n}\n\n \nstatic struct sk_buff *sched_skb(struct sge *sge, struct sk_buff *skb,\n\t\t\t\tunsigned int credits)\n{\n\tstruct sched *s = sge->tx_sched;\n\tstruct sk_buff_head *skbq;\n\tunsigned int i, len, update = 1;\n\n\tpr_debug(\"sched_skb %p\\n\", skb);\n\tif (!skb) {\n\t\tif (!s->num)\n\t\t\treturn NULL;\n\t} else {\n\t\tskbq = &s->p[skb->dev->if_port].skbq;\n\t\t__skb_queue_tail(skbq, skb);\n\t\ts->num++;\n\t\tskb = NULL;\n\t}\n\n\tif (credits < MAX_SKB_FRAGS + 1)\n\t\tgoto out;\n\nagain:\n\tfor (i = 0; i < MAX_NPORTS; i++) {\n\t\ts->port = (s->port + 1) & (MAX_NPORTS - 1);\n\t\tskbq = &s->p[s->port].skbq;\n\n\t\tskb = skb_peek(skbq);\n\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tlen = skb->len;\n\t\tif (len <= s->p[s->port].avail) {\n\t\t\ts->p[s->port].avail -= len;\n\t\t\ts->num--;\n\t\t\t__skb_unlink(skb, skbq);\n\t\t\tgoto out;\n\t\t}\n\t\tskb = NULL;\n\t}\n\n\tif (update-- && sched_update_avail(sge))\n\t\tgoto again;\n\nout:\n\t \n\tif (s->num && !skb) {\n\t\tstruct cmdQ *q = &sge->cmdQ[0];\n\t\tclear_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\n\t\tif (test_and_set_bit(CMDQ_STAT_RUNNING, &q->status) == 0) {\n\t\t\tset_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\n\t\t\twritel(F_CMDQ0_ENABLE, sge->adapter->regs + A_SG_DOORBELL);\n\t\t}\n\t}\n\tpr_debug(\"sched_skb ret %p\\n\", skb);\n\n\treturn skb;\n}\n\n \nstatic inline void doorbell_pio(struct adapter *adapter, u32 val)\n{\n\twmb();\n\twritel(val, adapter->regs + A_SG_DOORBELL);\n}\n\n \nstatic void free_freelQ_buffers(struct pci_dev *pdev, struct freelQ *q)\n{\n\tunsigned int cidx = q->cidx;\n\n\twhile (q->credits--) {\n\t\tstruct freelQ_ce *ce = &q->centries[cidx];\n\n\t\tdma_unmap_single(&pdev->dev, dma_unmap_addr(ce, dma_addr),\n\t\t\t\t dma_unmap_len(ce, dma_len), DMA_FROM_DEVICE);\n\t\tdev_kfree_skb(ce->skb);\n\t\tce->skb = NULL;\n\t\tif (++cidx == q->size)\n\t\t\tcidx = 0;\n\t}\n}\n\n \nstatic void free_rx_resources(struct sge *sge)\n{\n\tstruct pci_dev *pdev = sge->adapter->pdev;\n\tunsigned int size, i;\n\n\tif (sge->respQ.entries) {\n\t\tsize = sizeof(struct respQ_e) * sge->respQ.size;\n\t\tdma_free_coherent(&pdev->dev, size, sge->respQ.entries,\n\t\t\t\t  sge->respQ.dma_addr);\n\t}\n\n\tfor (i = 0; i < SGE_FREELQ_N; i++) {\n\t\tstruct freelQ *q = &sge->freelQ[i];\n\n\t\tif (q->centries) {\n\t\t\tfree_freelQ_buffers(pdev, q);\n\t\t\tkfree(q->centries);\n\t\t}\n\t\tif (q->entries) {\n\t\t\tsize = sizeof(struct freelQ_e) * q->size;\n\t\t\tdma_free_coherent(&pdev->dev, size, q->entries,\n\t\t\t\t\t  q->dma_addr);\n\t\t}\n\t}\n}\n\n \nstatic int alloc_rx_resources(struct sge *sge, struct sge_params *p)\n{\n\tstruct pci_dev *pdev = sge->adapter->pdev;\n\tunsigned int size, i;\n\n\tfor (i = 0; i < SGE_FREELQ_N; i++) {\n\t\tstruct freelQ *q = &sge->freelQ[i];\n\n\t\tq->genbit = 1;\n\t\tq->size = p->freelQ_size[i];\n\t\tq->dma_offset = sge->rx_pkt_pad ? 0 : NET_IP_ALIGN;\n\t\tsize = sizeof(struct freelQ_e) * q->size;\n\t\tq->entries = dma_alloc_coherent(&pdev->dev, size,\n\t\t\t\t\t\t&q->dma_addr, GFP_KERNEL);\n\t\tif (!q->entries)\n\t\t\tgoto err_no_mem;\n\n\t\tsize = sizeof(struct freelQ_ce) * q->size;\n\t\tq->centries = kzalloc(size, GFP_KERNEL);\n\t\tif (!q->centries)\n\t\t\tgoto err_no_mem;\n\t}\n\n\t \n\tsge->freelQ[!sge->jumbo_fl].rx_buffer_size = SGE_RX_SM_BUF_SIZE +\n\t\tsizeof(struct cpl_rx_data) +\n\t\tsge->freelQ[!sge->jumbo_fl].dma_offset;\n\n\tsize = (16 * 1024) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tsge->freelQ[sge->jumbo_fl].rx_buffer_size = size;\n\n\t \n\tsge->freelQ[!sge->jumbo_fl].recycleq_idx = 0;\n\tsge->freelQ[sge->jumbo_fl].recycleq_idx = 1;\n\n\tsge->respQ.genbit = 1;\n\tsge->respQ.size = SGE_RESPQ_E_N;\n\tsge->respQ.credits = 0;\n\tsize = sizeof(struct respQ_e) * sge->respQ.size;\n\tsge->respQ.entries =\n\t\tdma_alloc_coherent(&pdev->dev, size, &sge->respQ.dma_addr,\n\t\t\t\t   GFP_KERNEL);\n\tif (!sge->respQ.entries)\n\t\tgoto err_no_mem;\n\treturn 0;\n\nerr_no_mem:\n\tfree_rx_resources(sge);\n\treturn -ENOMEM;\n}\n\n \nstatic void free_cmdQ_buffers(struct sge *sge, struct cmdQ *q, unsigned int n)\n{\n\tstruct cmdQ_ce *ce;\n\tstruct pci_dev *pdev = sge->adapter->pdev;\n\tunsigned int cidx = q->cidx;\n\n\tq->in_use -= n;\n\tce = &q->centries[cidx];\n\twhile (n--) {\n\t\tif (likely(dma_unmap_len(ce, dma_len))) {\n\t\t\tdma_unmap_single(&pdev->dev,\n\t\t\t\t\t dma_unmap_addr(ce, dma_addr),\n\t\t\t\t\t dma_unmap_len(ce, dma_len),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\tif (q->sop)\n\t\t\t\tq->sop = 0;\n\t\t}\n\t\tif (ce->skb) {\n\t\t\tdev_kfree_skb_any(ce->skb);\n\t\t\tq->sop = 1;\n\t\t}\n\t\tce++;\n\t\tif (++cidx == q->size) {\n\t\t\tcidx = 0;\n\t\t\tce = q->centries;\n\t\t}\n\t}\n\tq->cidx = cidx;\n}\n\n \nstatic void free_tx_resources(struct sge *sge)\n{\n\tstruct pci_dev *pdev = sge->adapter->pdev;\n\tunsigned int size, i;\n\n\tfor (i = 0; i < SGE_CMDQ_N; i++) {\n\t\tstruct cmdQ *q = &sge->cmdQ[i];\n\n\t\tif (q->centries) {\n\t\t\tif (q->in_use)\n\t\t\t\tfree_cmdQ_buffers(sge, q, q->in_use);\n\t\t\tkfree(q->centries);\n\t\t}\n\t\tif (q->entries) {\n\t\t\tsize = sizeof(struct cmdQ_e) * q->size;\n\t\t\tdma_free_coherent(&pdev->dev, size, q->entries,\n\t\t\t\t\t  q->dma_addr);\n\t\t}\n\t}\n}\n\n \nstatic int alloc_tx_resources(struct sge *sge, struct sge_params *p)\n{\n\tstruct pci_dev *pdev = sge->adapter->pdev;\n\tunsigned int size, i;\n\n\tfor (i = 0; i < SGE_CMDQ_N; i++) {\n\t\tstruct cmdQ *q = &sge->cmdQ[i];\n\n\t\tq->genbit = 1;\n\t\tq->sop = 1;\n\t\tq->size = p->cmdQ_size[i];\n\t\tq->in_use = 0;\n\t\tq->status = 0;\n\t\tq->processed = q->cleaned = 0;\n\t\tq->stop_thres = 0;\n\t\tspin_lock_init(&q->lock);\n\t\tsize = sizeof(struct cmdQ_e) * q->size;\n\t\tq->entries = dma_alloc_coherent(&pdev->dev, size,\n\t\t\t\t\t\t&q->dma_addr, GFP_KERNEL);\n\t\tif (!q->entries)\n\t\t\tgoto err_no_mem;\n\n\t\tsize = sizeof(struct cmdQ_ce) * q->size;\n\t\tq->centries = kzalloc(size, GFP_KERNEL);\n\t\tif (!q->centries)\n\t\t\tgoto err_no_mem;\n\t}\n\n\t \n\tsge->cmdQ[0].stop_thres = sge->adapter->params.nports *\n\t\t(MAX_SKB_FRAGS + 1);\n\treturn 0;\n\nerr_no_mem:\n\tfree_tx_resources(sge);\n\treturn -ENOMEM;\n}\n\nstatic inline void setup_ring_params(struct adapter *adapter, u64 addr,\n\t\t\t\t     u32 size, int base_reg_lo,\n\t\t\t\t     int base_reg_hi, int size_reg)\n{\n\twritel((u32)addr, adapter->regs + base_reg_lo);\n\twritel(addr >> 32, adapter->regs + base_reg_hi);\n\twritel(size, adapter->regs + size_reg);\n}\n\n \nvoid t1_vlan_mode(struct adapter *adapter, netdev_features_t features)\n{\n\tstruct sge *sge = adapter->sge;\n\n\tif (features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tsge->sge_control |= F_VLAN_XTRACT;\n\telse\n\t\tsge->sge_control &= ~F_VLAN_XTRACT;\n\tif (adapter->open_device_map) {\n\t\twritel(sge->sge_control, adapter->regs + A_SG_CONTROL);\n\t\treadl(adapter->regs + A_SG_CONTROL);    \n\t}\n}\n\n \nstatic void configure_sge(struct sge *sge, struct sge_params *p)\n{\n\tstruct adapter *ap = sge->adapter;\n\n\twritel(0, ap->regs + A_SG_CONTROL);\n\tsetup_ring_params(ap, sge->cmdQ[0].dma_addr, sge->cmdQ[0].size,\n\t\t\t  A_SG_CMD0BASELWR, A_SG_CMD0BASEUPR, A_SG_CMD0SIZE);\n\tsetup_ring_params(ap, sge->cmdQ[1].dma_addr, sge->cmdQ[1].size,\n\t\t\t  A_SG_CMD1BASELWR, A_SG_CMD1BASEUPR, A_SG_CMD1SIZE);\n\tsetup_ring_params(ap, sge->freelQ[0].dma_addr,\n\t\t\t  sge->freelQ[0].size, A_SG_FL0BASELWR,\n\t\t\t  A_SG_FL0BASEUPR, A_SG_FL0SIZE);\n\tsetup_ring_params(ap, sge->freelQ[1].dma_addr,\n\t\t\t  sge->freelQ[1].size, A_SG_FL1BASELWR,\n\t\t\t  A_SG_FL1BASEUPR, A_SG_FL1SIZE);\n\n\t \n\twritel(SGE_RX_SM_BUF_SIZE + 1, ap->regs + A_SG_FLTHRESHOLD);\n\n\tsetup_ring_params(ap, sge->respQ.dma_addr, sge->respQ.size,\n\t\t\t  A_SG_RSPBASELWR, A_SG_RSPBASEUPR, A_SG_RSPSIZE);\n\twritel((u32)sge->respQ.size - 1, ap->regs + A_SG_RSPQUEUECREDIT);\n\n\tsge->sge_control = F_CMDQ0_ENABLE | F_CMDQ1_ENABLE | F_FL0_ENABLE |\n\t\tF_FL1_ENABLE | F_CPL_ENABLE | F_RESPONSE_QUEUE_ENABLE |\n\t\tV_CMDQ_PRIORITY(2) | F_DISABLE_CMDQ1_GTS | F_ISCSI_COALESCE |\n\t\tV_RX_PKT_OFFSET(sge->rx_pkt_pad);\n\n#if defined(__BIG_ENDIAN_BITFIELD)\n\tsge->sge_control |= F_ENABLE_BIG_ENDIAN;\n#endif\n\n\t \n\tsge->intrtimer_nres = SGE_INTRTIMER_NRES * core_ticks_per_usec(ap);\n\n\tt1_sge_set_coalesce_params(sge, p);\n}\n\n \nstatic inline unsigned int jumbo_payload_capacity(const struct sge *sge)\n{\n\treturn sge->freelQ[sge->jumbo_fl].rx_buffer_size -\n\t\tsge->freelQ[sge->jumbo_fl].dma_offset -\n\t\tsizeof(struct cpl_rx_data);\n}\n\n \nvoid t1_sge_destroy(struct sge *sge)\n{\n\tint i;\n\n\tfor_each_port(sge->adapter, i)\n\t\tfree_percpu(sge->port_stats[i]);\n\n\tkfree(sge->tx_sched);\n\tfree_tx_resources(sge);\n\tfree_rx_resources(sge);\n\tkfree(sge);\n}\n\n \nstatic void refill_free_list(struct sge *sge, struct freelQ *q)\n{\n\tstruct pci_dev *pdev = sge->adapter->pdev;\n\tstruct freelQ_ce *ce = &q->centries[q->pidx];\n\tstruct freelQ_e *e = &q->entries[q->pidx];\n\tunsigned int dma_len = q->rx_buffer_size - q->dma_offset;\n\n\twhile (q->credits < q->size) {\n\t\tstruct sk_buff *skb;\n\t\tdma_addr_t mapping;\n\n\t\tskb = dev_alloc_skb(q->rx_buffer_size);\n\t\tif (!skb)\n\t\t\tbreak;\n\n\t\tskb_reserve(skb, q->dma_offset);\n\t\tmapping = dma_map_single(&pdev->dev, skb->data, dma_len,\n\t\t\t\t\t DMA_FROM_DEVICE);\n\t\tskb_reserve(skb, sge->rx_pkt_pad);\n\n\t\tce->skb = skb;\n\t\tdma_unmap_addr_set(ce, dma_addr, mapping);\n\t\tdma_unmap_len_set(ce, dma_len, dma_len);\n\t\te->addr_lo = (u32)mapping;\n\t\te->addr_hi = (u64)mapping >> 32;\n\t\te->len_gen = V_CMD_LEN(dma_len) | V_CMD_GEN1(q->genbit);\n\t\twmb();\n\t\te->gen2 = V_CMD_GEN2(q->genbit);\n\n\t\te++;\n\t\tce++;\n\t\tif (++q->pidx == q->size) {\n\t\t\tq->pidx = 0;\n\t\t\tq->genbit ^= 1;\n\t\t\tce = q->centries;\n\t\t\te = q->entries;\n\t\t}\n\t\tq->credits++;\n\t}\n}\n\n \nstatic void freelQs_empty(struct sge *sge)\n{\n\tstruct adapter *adapter = sge->adapter;\n\tu32 irq_reg = readl(adapter->regs + A_SG_INT_ENABLE);\n\tu32 irqholdoff_reg;\n\n\trefill_free_list(sge, &sge->freelQ[0]);\n\trefill_free_list(sge, &sge->freelQ[1]);\n\n\tif (sge->freelQ[0].credits > (sge->freelQ[0].size >> 2) &&\n\t    sge->freelQ[1].credits > (sge->freelQ[1].size >> 2)) {\n\t\tirq_reg |= F_FL_EXHAUSTED;\n\t\tirqholdoff_reg = sge->fixed_intrtimer;\n\t} else {\n\t\t \n\t\tirq_reg &= ~F_FL_EXHAUSTED;\n\t\tirqholdoff_reg = sge->intrtimer_nres;\n\t}\n\twritel(irqholdoff_reg, adapter->regs + A_SG_INTRTIMER);\n\twritel(irq_reg, adapter->regs + A_SG_INT_ENABLE);\n\n\t \n\tdoorbell_pio(adapter, F_FL0_ENABLE | F_FL1_ENABLE);\n}\n\n#define SGE_PL_INTR_MASK (F_PL_INTR_SGE_ERR | F_PL_INTR_SGE_DATA)\n#define SGE_INT_FATAL (F_RESPQ_OVERFLOW | F_PACKET_TOO_BIG | F_PACKET_MISMATCH)\n#define SGE_INT_ENABLE (F_RESPQ_EXHAUSTED | F_RESPQ_OVERFLOW | \\\n\t\t\tF_FL_EXHAUSTED | F_PACKET_TOO_BIG | F_PACKET_MISMATCH)\n\n \nvoid t1_sge_intr_disable(struct sge *sge)\n{\n\tu32 val = readl(sge->adapter->regs + A_PL_ENABLE);\n\n\twritel(val & ~SGE_PL_INTR_MASK, sge->adapter->regs + A_PL_ENABLE);\n\twritel(0, sge->adapter->regs + A_SG_INT_ENABLE);\n}\n\n \nvoid t1_sge_intr_enable(struct sge *sge)\n{\n\tu32 en = SGE_INT_ENABLE;\n\tu32 val = readl(sge->adapter->regs + A_PL_ENABLE);\n\n\tif (sge->adapter->port[0].dev->hw_features & NETIF_F_TSO)\n\t\ten &= ~F_PACKET_TOO_BIG;\n\twritel(en, sge->adapter->regs + A_SG_INT_ENABLE);\n\twritel(val | SGE_PL_INTR_MASK, sge->adapter->regs + A_PL_ENABLE);\n}\n\n \nvoid t1_sge_intr_clear(struct sge *sge)\n{\n\twritel(SGE_PL_INTR_MASK, sge->adapter->regs + A_PL_CAUSE);\n\twritel(0xffffffff, sge->adapter->regs + A_SG_INT_CAUSE);\n}\n\n \nbool t1_sge_intr_error_handler(struct sge *sge)\n{\n\tstruct adapter *adapter = sge->adapter;\n\tu32 cause = readl(adapter->regs + A_SG_INT_CAUSE);\n\tbool wake = false;\n\n\tif (adapter->port[0].dev->hw_features & NETIF_F_TSO)\n\t\tcause &= ~F_PACKET_TOO_BIG;\n\tif (cause & F_RESPQ_EXHAUSTED)\n\t\tsge->stats.respQ_empty++;\n\tif (cause & F_RESPQ_OVERFLOW) {\n\t\tsge->stats.respQ_overflow++;\n\t\tpr_alert(\"%s: SGE response queue overflow\\n\",\n\t\t\t adapter->name);\n\t}\n\tif (cause & F_FL_EXHAUSTED) {\n\t\tsge->stats.freelistQ_empty++;\n\t\tfreelQs_empty(sge);\n\t}\n\tif (cause & F_PACKET_TOO_BIG) {\n\t\tsge->stats.pkt_too_big++;\n\t\tpr_alert(\"%s: SGE max packet size exceeded\\n\",\n\t\t\t adapter->name);\n\t}\n\tif (cause & F_PACKET_MISMATCH) {\n\t\tsge->stats.pkt_mismatch++;\n\t\tpr_alert(\"%s: SGE packet mismatch\\n\", adapter->name);\n\t}\n\tif (cause & SGE_INT_FATAL) {\n\t\tt1_interrupts_disable(adapter);\n\t\tadapter->pending_thread_intr |= F_PL_INTR_SGE_ERR;\n\t\twake = true;\n\t}\n\n\twritel(cause, adapter->regs + A_SG_INT_CAUSE);\n\treturn wake;\n}\n\nconst struct sge_intr_counts *t1_sge_get_intr_counts(const struct sge *sge)\n{\n\treturn &sge->stats;\n}\n\nvoid t1_sge_get_port_stats(const struct sge *sge, int port,\n\t\t\t   struct sge_port_stats *ss)\n{\n\tint cpu;\n\n\tmemset(ss, 0, sizeof(*ss));\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct sge_port_stats *st = per_cpu_ptr(sge->port_stats[port], cpu);\n\n\t\tss->rx_cso_good += st->rx_cso_good;\n\t\tss->tx_cso += st->tx_cso;\n\t\tss->tx_tso += st->tx_tso;\n\t\tss->tx_need_hdrroom += st->tx_need_hdrroom;\n\t\tss->vlan_xtract += st->vlan_xtract;\n\t\tss->vlan_insert += st->vlan_insert;\n\t}\n}\n\n \nstatic void recycle_fl_buf(struct freelQ *fl, int idx)\n{\n\tstruct freelQ_e *from = &fl->entries[idx];\n\tstruct freelQ_e *to = &fl->entries[fl->pidx];\n\n\tfl->centries[fl->pidx] = fl->centries[idx];\n\tto->addr_lo = from->addr_lo;\n\tto->addr_hi = from->addr_hi;\n\tto->len_gen = G_CMD_LEN(from->len_gen) | V_CMD_GEN1(fl->genbit);\n\twmb();\n\tto->gen2 = V_CMD_GEN2(fl->genbit);\n\tfl->credits++;\n\n\tif (++fl->pidx == fl->size) {\n\t\tfl->pidx = 0;\n\t\tfl->genbit ^= 1;\n\t}\n}\n\nstatic int copybreak __read_mostly = 256;\nmodule_param(copybreak, int, 0);\nMODULE_PARM_DESC(copybreak, \"Receive copy threshold\");\n\n \nstatic inline struct sk_buff *get_packet(struct adapter *adapter,\n\t\t\t\t\t struct freelQ *fl, unsigned int len)\n{\n\tconst struct freelQ_ce *ce = &fl->centries[fl->cidx];\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct sk_buff *skb;\n\n\tif (len < copybreak) {\n\t\tskb = napi_alloc_skb(&adapter->napi, len);\n\t\tif (!skb)\n\t\t\tgoto use_orig_buf;\n\n\t\tskb_put(skb, len);\n\t\tdma_sync_single_for_cpu(&pdev->dev,\n\t\t\t\t\tdma_unmap_addr(ce, dma_addr),\n\t\t\t\t\tdma_unmap_len(ce, dma_len),\n\t\t\t\t\tDMA_FROM_DEVICE);\n\t\tskb_copy_from_linear_data(ce->skb, skb->data, len);\n\t\tdma_sync_single_for_device(&pdev->dev,\n\t\t\t\t\t   dma_unmap_addr(ce, dma_addr),\n\t\t\t\t\t   dma_unmap_len(ce, dma_len),\n\t\t\t\t\t   DMA_FROM_DEVICE);\n\t\trecycle_fl_buf(fl, fl->cidx);\n\t\treturn skb;\n\t}\n\nuse_orig_buf:\n\tif (fl->credits < 2) {\n\t\trecycle_fl_buf(fl, fl->cidx);\n\t\treturn NULL;\n\t}\n\n\tdma_unmap_single(&pdev->dev, dma_unmap_addr(ce, dma_addr),\n\t\t\t dma_unmap_len(ce, dma_len), DMA_FROM_DEVICE);\n\tskb = ce->skb;\n\tprefetch(skb->data);\n\n\tskb_put(skb, len);\n\treturn skb;\n}\n\n \nstatic void unexpected_offload(struct adapter *adapter, struct freelQ *fl)\n{\n\tstruct freelQ_ce *ce = &fl->centries[fl->cidx];\n\tstruct sk_buff *skb = ce->skb;\n\n\tdma_sync_single_for_cpu(&adapter->pdev->dev,\n\t\t\t\tdma_unmap_addr(ce, dma_addr),\n\t\t\t\tdma_unmap_len(ce, dma_len), DMA_FROM_DEVICE);\n\tpr_err(\"%s: unexpected offload packet, cmd %u\\n\",\n\t       adapter->name, *skb->data);\n\trecycle_fl_buf(fl, fl->cidx);\n}\n\n \nstatic inline unsigned int compute_large_page_tx_descs(struct sk_buff *skb)\n{\n\tunsigned int count = 0;\n\n\tif (PAGE_SIZE > SGE_TX_DESC_MAX_PLEN) {\n\t\tunsigned int nfrags = skb_shinfo(skb)->nr_frags;\n\t\tunsigned int i, len = skb_headlen(skb);\n\t\twhile (len > SGE_TX_DESC_MAX_PLEN) {\n\t\t\tcount++;\n\t\t\tlen -= SGE_TX_DESC_MAX_PLEN;\n\t\t}\n\t\tfor (i = 0; nfrags--; i++) {\n\t\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tlen = skb_frag_size(frag);\n\t\t\twhile (len > SGE_TX_DESC_MAX_PLEN) {\n\t\t\t\tcount++;\n\t\t\t\tlen -= SGE_TX_DESC_MAX_PLEN;\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}\n\n \nstatic inline void write_tx_desc(struct cmdQ_e *e, dma_addr_t mapping,\n\t\t\t\t unsigned int len, unsigned int gen,\n\t\t\t\t unsigned int eop)\n{\n\tBUG_ON(len > SGE_TX_DESC_MAX_PLEN);\n\n\te->addr_lo = (u32)mapping;\n\te->addr_hi = (u64)mapping >> 32;\n\te->len_gen = V_CMD_LEN(len) | V_CMD_GEN1(gen);\n\te->flags = F_CMD_DATAVALID | V_CMD_EOP(eop) | V_CMD_GEN2(gen);\n}\n\n \nstatic inline unsigned int write_large_page_tx_descs(unsigned int pidx,\n\t\t\t\t\t\t     struct cmdQ_e **e,\n\t\t\t\t\t\t     struct cmdQ_ce **ce,\n\t\t\t\t\t\t     unsigned int *gen,\n\t\t\t\t\t\t     dma_addr_t *desc_mapping,\n\t\t\t\t\t\t     unsigned int *desc_len,\n\t\t\t\t\t\t     unsigned int nfrags,\n\t\t\t\t\t\t     struct cmdQ *q)\n{\n\tif (PAGE_SIZE > SGE_TX_DESC_MAX_PLEN) {\n\t\tstruct cmdQ_e *e1 = *e;\n\t\tstruct cmdQ_ce *ce1 = *ce;\n\n\t\twhile (*desc_len > SGE_TX_DESC_MAX_PLEN) {\n\t\t\t*desc_len -= SGE_TX_DESC_MAX_PLEN;\n\t\t\twrite_tx_desc(e1, *desc_mapping, SGE_TX_DESC_MAX_PLEN,\n\t\t\t\t      *gen, nfrags == 0 && *desc_len == 0);\n\t\t\tce1->skb = NULL;\n\t\t\tdma_unmap_len_set(ce1, dma_len, 0);\n\t\t\t*desc_mapping += SGE_TX_DESC_MAX_PLEN;\n\t\t\tif (*desc_len) {\n\t\t\t\tce1++;\n\t\t\t\te1++;\n\t\t\t\tif (++pidx == q->size) {\n\t\t\t\t\tpidx = 0;\n\t\t\t\t\t*gen ^= 1;\n\t\t\t\t\tce1 = q->centries;\n\t\t\t\t\te1 = q->entries;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t*e = e1;\n\t\t*ce = ce1;\n\t}\n\treturn pidx;\n}\n\n \nstatic inline void write_tx_descs(struct adapter *adapter, struct sk_buff *skb,\n\t\t\t\t  unsigned int pidx, unsigned int gen,\n\t\t\t\t  struct cmdQ *q)\n{\n\tdma_addr_t mapping, desc_mapping;\n\tstruct cmdQ_e *e, *e1;\n\tstruct cmdQ_ce *ce;\n\tunsigned int i, flags, first_desc_len, desc_len,\n\t    nfrags = skb_shinfo(skb)->nr_frags;\n\n\te = e1 = &q->entries[pidx];\n\tce = &q->centries[pidx];\n\n\tmapping = dma_map_single(&adapter->pdev->dev, skb->data,\n\t\t\t\t skb_headlen(skb), DMA_TO_DEVICE);\n\n\tdesc_mapping = mapping;\n\tdesc_len = skb_headlen(skb);\n\n\tflags = F_CMD_DATAVALID | F_CMD_SOP |\n\t    V_CMD_EOP(nfrags == 0 && desc_len <= SGE_TX_DESC_MAX_PLEN) |\n\t    V_CMD_GEN2(gen);\n\tfirst_desc_len = (desc_len <= SGE_TX_DESC_MAX_PLEN) ?\n\t    desc_len : SGE_TX_DESC_MAX_PLEN;\n\te->addr_lo = (u32)desc_mapping;\n\te->addr_hi = (u64)desc_mapping >> 32;\n\te->len_gen = V_CMD_LEN(first_desc_len) | V_CMD_GEN1(gen);\n\tce->skb = NULL;\n\tdma_unmap_len_set(ce, dma_len, 0);\n\n\tif (PAGE_SIZE > SGE_TX_DESC_MAX_PLEN &&\n\t    desc_len > SGE_TX_DESC_MAX_PLEN) {\n\t\tdesc_mapping += first_desc_len;\n\t\tdesc_len -= first_desc_len;\n\t\te1++;\n\t\tce++;\n\t\tif (++pidx == q->size) {\n\t\t\tpidx = 0;\n\t\t\tgen ^= 1;\n\t\t\te1 = q->entries;\n\t\t\tce = q->centries;\n\t\t}\n\t\tpidx = write_large_page_tx_descs(pidx, &e1, &ce, &gen,\n\t\t\t\t\t\t &desc_mapping, &desc_len,\n\t\t\t\t\t\t nfrags, q);\n\n\t\tif (likely(desc_len))\n\t\t\twrite_tx_desc(e1, desc_mapping, desc_len, gen,\n\t\t\t\t      nfrags == 0);\n\t}\n\n\tce->skb = NULL;\n\tdma_unmap_addr_set(ce, dma_addr, mapping);\n\tdma_unmap_len_set(ce, dma_len, skb_headlen(skb));\n\n\tfor (i = 0; nfrags--; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\te1++;\n\t\tce++;\n\t\tif (++pidx == q->size) {\n\t\t\tpidx = 0;\n\t\t\tgen ^= 1;\n\t\t\te1 = q->entries;\n\t\t\tce = q->centries;\n\t\t}\n\n\t\tmapping = skb_frag_dma_map(&adapter->pdev->dev, frag, 0,\n\t\t\t\t\t   skb_frag_size(frag), DMA_TO_DEVICE);\n\t\tdesc_mapping = mapping;\n\t\tdesc_len = skb_frag_size(frag);\n\n\t\tpidx = write_large_page_tx_descs(pidx, &e1, &ce, &gen,\n\t\t\t\t\t\t &desc_mapping, &desc_len,\n\t\t\t\t\t\t nfrags, q);\n\t\tif (likely(desc_len))\n\t\t\twrite_tx_desc(e1, desc_mapping, desc_len, gen,\n\t\t\t\t      nfrags == 0);\n\t\tce->skb = NULL;\n\t\tdma_unmap_addr_set(ce, dma_addr, mapping);\n\t\tdma_unmap_len_set(ce, dma_len, skb_frag_size(frag));\n\t}\n\tce->skb = skb;\n\twmb();\n\te->flags = flags;\n}\n\n \nstatic inline void reclaim_completed_tx(struct sge *sge, struct cmdQ *q)\n{\n\tunsigned int reclaim = q->processed - q->cleaned;\n\n\tif (reclaim) {\n\t\tpr_debug(\"reclaim_completed_tx processed:%d cleaned:%d\\n\",\n\t\t\t q->processed, q->cleaned);\n\t\tfree_cmdQ_buffers(sge, q, reclaim);\n\t\tq->cleaned += reclaim;\n\t}\n}\n\n \nstatic void restart_sched(struct tasklet_struct *t)\n{\n\tstruct sched *s = from_tasklet(s, t, sched_tsk);\n\tstruct sge *sge = s->sge;\n\tstruct adapter *adapter = sge->adapter;\n\tstruct cmdQ *q = &sge->cmdQ[0];\n\tstruct sk_buff *skb;\n\tunsigned int credits, queued_skb = 0;\n\n\tspin_lock(&q->lock);\n\treclaim_completed_tx(sge, q);\n\n\tcredits = q->size - q->in_use;\n\tpr_debug(\"restart_sched credits=%d\\n\", credits);\n\twhile ((skb = sched_skb(sge, NULL, credits)) != NULL) {\n\t\tunsigned int genbit, pidx, count;\n\t        count = 1 + skb_shinfo(skb)->nr_frags;\n\t\tcount += compute_large_page_tx_descs(skb);\n\t\tq->in_use += count;\n\t\tgenbit = q->genbit;\n\t\tpidx = q->pidx;\n\t\tq->pidx += count;\n\t\tif (q->pidx >= q->size) {\n\t\t\tq->pidx -= q->size;\n\t\t\tq->genbit ^= 1;\n\t\t}\n\t\twrite_tx_descs(adapter, skb, pidx, genbit, q);\n\t        credits = q->size - q->in_use;\n\t\tqueued_skb = 1;\n\t}\n\n\tif (queued_skb) {\n\t\tclear_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\n\t\tif (test_and_set_bit(CMDQ_STAT_RUNNING, &q->status) == 0) {\n\t\t\tset_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\n\t\t\twritel(F_CMDQ0_ENABLE, adapter->regs + A_SG_DOORBELL);\n\t\t}\n\t}\n\tspin_unlock(&q->lock);\n}\n\n \nstatic void sge_rx(struct sge *sge, struct freelQ *fl, unsigned int len)\n{\n\tstruct sk_buff *skb;\n\tconst struct cpl_rx_pkt *p;\n\tstruct adapter *adapter = sge->adapter;\n\tstruct sge_port_stats *st;\n\tstruct net_device *dev;\n\n\tskb = get_packet(adapter, fl, len - sge->rx_pkt_pad);\n\tif (unlikely(!skb)) {\n\t\tsge->stats.rx_drops++;\n\t\treturn;\n\t}\n\n\tp = (const struct cpl_rx_pkt *) skb->data;\n\tif (p->iff >= adapter->params.nports) {\n\t\tkfree_skb(skb);\n\t\treturn;\n\t}\n\t__skb_pull(skb, sizeof(*p));\n\n\tst = this_cpu_ptr(sge->port_stats[p->iff]);\n\tdev = adapter->port[p->iff].dev;\n\n\tskb->protocol = eth_type_trans(skb, dev);\n\tif ((dev->features & NETIF_F_RXCSUM) && p->csum == 0xffff &&\n\t    skb->protocol == htons(ETH_P_IP) &&\n\t    (skb->data[9] == IPPROTO_TCP || skb->data[9] == IPPROTO_UDP)) {\n\t\t++st->rx_cso_good;\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t} else\n\t\tskb_checksum_none_assert(skb);\n\n\tif (p->vlan_valid) {\n\t\tst->vlan_xtract++;\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ntohs(p->vlan));\n\t}\n\tnetif_receive_skb(skb);\n}\n\n \nstatic inline int enough_free_Tx_descs(const struct cmdQ *q)\n{\n\tunsigned int r = q->processed - q->cleaned;\n\n\treturn q->in_use - r < (q->size >> 1);\n}\n\n \nstatic void restart_tx_queues(struct sge *sge)\n{\n\tstruct adapter *adap = sge->adapter;\n\tint i;\n\n\tif (!enough_free_Tx_descs(&sge->cmdQ[0]))\n\t\treturn;\n\n\tfor_each_port(adap, i) {\n\t\tstruct net_device *nd = adap->port[i].dev;\n\n\t\tif (test_and_clear_bit(nd->if_port, &sge->stopped_tx_queues) &&\n\t\t    netif_running(nd)) {\n\t\t\tsge->stats.cmdQ_restarted[2]++;\n\t\t\tnetif_wake_queue(nd);\n\t\t}\n\t}\n}\n\n \nstatic unsigned int update_tx_info(struct adapter *adapter,\n\t\t\t\t\t  unsigned int flags,\n\t\t\t\t\t  unsigned int pr0)\n{\n\tstruct sge *sge = adapter->sge;\n\tstruct cmdQ *cmdq = &sge->cmdQ[0];\n\n\tcmdq->processed += pr0;\n\tif (flags & (F_FL0_ENABLE | F_FL1_ENABLE)) {\n\t\tfreelQs_empty(sge);\n\t\tflags &= ~(F_FL0_ENABLE | F_FL1_ENABLE);\n\t}\n\tif (flags & F_CMDQ0_ENABLE) {\n\t\tclear_bit(CMDQ_STAT_RUNNING, &cmdq->status);\n\n\t\tif (cmdq->cleaned + cmdq->in_use != cmdq->processed &&\n\t\t    !test_and_set_bit(CMDQ_STAT_LAST_PKT_DB, &cmdq->status)) {\n\t\t\tset_bit(CMDQ_STAT_RUNNING, &cmdq->status);\n\t\t\twritel(F_CMDQ0_ENABLE, adapter->regs + A_SG_DOORBELL);\n\t\t}\n\t\tif (sge->tx_sched)\n\t\t\ttasklet_hi_schedule(&sge->tx_sched->sched_tsk);\n\n\t\tflags &= ~F_CMDQ0_ENABLE;\n\t}\n\n\tif (unlikely(sge->stopped_tx_queues != 0))\n\t\trestart_tx_queues(sge);\n\n\treturn flags;\n}\n\n \nstatic int process_responses(struct adapter *adapter, int budget)\n{\n\tstruct sge *sge = adapter->sge;\n\tstruct respQ *q = &sge->respQ;\n\tstruct respQ_e *e = &q->entries[q->cidx];\n\tint done = 0;\n\tunsigned int flags = 0;\n\tunsigned int cmdq_processed[SGE_CMDQ_N] = {0, 0};\n\n\twhile (done < budget && e->GenerationBit == q->genbit) {\n\t\tflags |= e->Qsleeping;\n\n\t\tcmdq_processed[0] += e->Cmdq0CreditReturn;\n\t\tcmdq_processed[1] += e->Cmdq1CreditReturn;\n\n\t\t \n\t\tif (unlikely((flags & F_CMDQ0_ENABLE) || cmdq_processed[0] > 64)) {\n\t\t\tflags = update_tx_info(adapter, flags, cmdq_processed[0]);\n\t\t\tcmdq_processed[0] = 0;\n\t\t}\n\n\t\tif (unlikely(cmdq_processed[1] > 16)) {\n\t\t\tsge->cmdQ[1].processed += cmdq_processed[1];\n\t\t\tcmdq_processed[1] = 0;\n\t\t}\n\n\t\tif (likely(e->DataValid)) {\n\t\t\tstruct freelQ *fl = &sge->freelQ[e->FreelistQid];\n\n\t\t\tBUG_ON(!e->Sop || !e->Eop);\n\t\t\tif (unlikely(e->Offload))\n\t\t\t\tunexpected_offload(adapter, fl);\n\t\t\telse\n\t\t\t\tsge_rx(sge, fl, e->BufferLength);\n\n\t\t\t++done;\n\n\t\t\t \n\t\t\tif (++fl->cidx == fl->size)\n\t\t\t\tfl->cidx = 0;\n\t\t\tprefetch(fl->centries[fl->cidx].skb);\n\n\t\t\tif (unlikely(--fl->credits <\n\t\t\t\t     fl->size - SGE_FREEL_REFILL_THRESH))\n\t\t\t\trefill_free_list(sge, fl);\n\t\t} else\n\t\t\tsge->stats.pure_rsps++;\n\n\t\te++;\n\t\tif (unlikely(++q->cidx == q->size)) {\n\t\t\tq->cidx = 0;\n\t\t\tq->genbit ^= 1;\n\t\t\te = q->entries;\n\t\t}\n\t\tprefetch(e);\n\n\t\tif (++q->credits > SGE_RESPQ_REPLENISH_THRES) {\n\t\t\twritel(q->credits, adapter->regs + A_SG_RSPQUEUECREDIT);\n\t\t\tq->credits = 0;\n\t\t}\n\t}\n\n\tflags = update_tx_info(adapter, flags, cmdq_processed[0]);\n\tsge->cmdQ[1].processed += cmdq_processed[1];\n\n\treturn done;\n}\n\nstatic inline int responses_pending(const struct adapter *adapter)\n{\n\tconst struct respQ *Q = &adapter->sge->respQ;\n\tconst struct respQ_e *e = &Q->entries[Q->cidx];\n\n\treturn e->GenerationBit == Q->genbit;\n}\n\n \nstatic int process_pure_responses(struct adapter *adapter)\n{\n\tstruct sge *sge = adapter->sge;\n\tstruct respQ *q = &sge->respQ;\n\tstruct respQ_e *e = &q->entries[q->cidx];\n\tconst struct freelQ *fl = &sge->freelQ[e->FreelistQid];\n\tunsigned int flags = 0;\n\tunsigned int cmdq_processed[SGE_CMDQ_N] = {0, 0};\n\n\tprefetch(fl->centries[fl->cidx].skb);\n\tif (e->DataValid)\n\t\treturn 1;\n\n\tdo {\n\t\tflags |= e->Qsleeping;\n\n\t\tcmdq_processed[0] += e->Cmdq0CreditReturn;\n\t\tcmdq_processed[1] += e->Cmdq1CreditReturn;\n\n\t\te++;\n\t\tif (unlikely(++q->cidx == q->size)) {\n\t\t\tq->cidx = 0;\n\t\t\tq->genbit ^= 1;\n\t\t\te = q->entries;\n\t\t}\n\t\tprefetch(e);\n\n\t\tif (++q->credits > SGE_RESPQ_REPLENISH_THRES) {\n\t\t\twritel(q->credits, adapter->regs + A_SG_RSPQUEUECREDIT);\n\t\t\tq->credits = 0;\n\t\t}\n\t\tsge->stats.pure_rsps++;\n\t} while (e->GenerationBit == q->genbit && !e->DataValid);\n\n\tflags = update_tx_info(adapter, flags, cmdq_processed[0]);\n\tsge->cmdQ[1].processed += cmdq_processed[1];\n\n\treturn e->GenerationBit == q->genbit;\n}\n\n \nint t1_poll(struct napi_struct *napi, int budget)\n{\n\tstruct adapter *adapter = container_of(napi, struct adapter, napi);\n\tint work_done = process_responses(adapter, budget);\n\n\tif (likely(work_done < budget)) {\n\t\tnapi_complete_done(napi, work_done);\n\t\twritel(adapter->sge->respQ.cidx,\n\t\t       adapter->regs + A_SG_SLEEPING);\n\t}\n\treturn work_done;\n}\n\nirqreturn_t t1_interrupt_thread(int irq, void *data)\n{\n\tstruct adapter *adapter = data;\n\tu32 pending_thread_intr;\n\n\tspin_lock_irq(&adapter->async_lock);\n\tpending_thread_intr = adapter->pending_thread_intr;\n\tadapter->pending_thread_intr = 0;\n\tspin_unlock_irq(&adapter->async_lock);\n\n\tif (!pending_thread_intr)\n\t\treturn IRQ_NONE;\n\n\tif (pending_thread_intr & F_PL_INTR_EXT)\n\t\tt1_elmer0_ext_intr_handler(adapter);\n\n\t \n\tif (pending_thread_intr & F_PL_INTR_SGE_ERR) {\n\t\tpr_alert(\"%s: encountered fatal error, operation suspended\\n\",\n\t\t\t adapter->name);\n\t\tt1_sge_stop(adapter->sge);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tspin_lock_irq(&adapter->async_lock);\n\tadapter->slow_intr_mask |= F_PL_INTR_EXT;\n\n\twritel(F_PL_INTR_EXT, adapter->regs + A_PL_CAUSE);\n\twritel(adapter->slow_intr_mask | F_PL_INTR_SGE_DATA,\n\t       adapter->regs + A_PL_ENABLE);\n\tspin_unlock_irq(&adapter->async_lock);\n\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t t1_interrupt(int irq, void *data)\n{\n\tstruct adapter *adapter = data;\n\tstruct sge *sge = adapter->sge;\n\tirqreturn_t handled;\n\n\tif (likely(responses_pending(adapter))) {\n\t\twritel(F_PL_INTR_SGE_DATA, adapter->regs + A_PL_CAUSE);\n\n\t\tif (napi_schedule_prep(&adapter->napi)) {\n\t\t\tif (process_pure_responses(adapter))\n\t\t\t\t__napi_schedule(&adapter->napi);\n\t\t\telse {\n\t\t\t\t \n\t\t\t\twritel(sge->respQ.cidx, adapter->regs + A_SG_SLEEPING);\n\t\t\t\t \n\t\t\t\tnapi_enable(&adapter->napi);\n\t\t\t}\n\t\t}\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tspin_lock(&adapter->async_lock);\n\thandled = t1_slow_intr_handler(adapter);\n\tspin_unlock(&adapter->async_lock);\n\n\tif (handled == IRQ_NONE)\n\t\tsge->stats.unhandled_irqs++;\n\n\treturn handled;\n}\n\n \nstatic int t1_sge_tx(struct sk_buff *skb, struct adapter *adapter,\n\t\t     unsigned int qid, struct net_device *dev)\n{\n\tstruct sge *sge = adapter->sge;\n\tstruct cmdQ *q = &sge->cmdQ[qid];\n\tunsigned int credits, pidx, genbit, count, use_sched_skb = 0;\n\n\tspin_lock(&q->lock);\n\n\treclaim_completed_tx(sge, q);\n\n\tpidx = q->pidx;\n\tcredits = q->size - q->in_use;\n\tcount = 1 + skb_shinfo(skb)->nr_frags;\n\tcount += compute_large_page_tx_descs(skb);\n\n\t \n\tif (unlikely(credits < count)) {\n\t\tif (!netif_queue_stopped(dev)) {\n\t\t\tnetif_stop_queue(dev);\n\t\t\tset_bit(dev->if_port, &sge->stopped_tx_queues);\n\t\t\tsge->stats.cmdQ_full[2]++;\n\t\t\tpr_err(\"%s: Tx ring full while queue awake!\\n\",\n\t\t\t       adapter->name);\n\t\t}\n\t\tspin_unlock(&q->lock);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (unlikely(credits - count < q->stop_thres)) {\n\t\tnetif_stop_queue(dev);\n\t\tset_bit(dev->if_port, &sge->stopped_tx_queues);\n\t\tsge->stats.cmdQ_full[2]++;\n\t}\n\n\t \n\tif (sge->tx_sched && !qid && skb->dev) {\nuse_sched:\n\t\tuse_sched_skb = 1;\n\t\t \n\t\tskb = sched_skb(sge, skb, credits);\n\t\tif (!skb) {\n\t\t\tspin_unlock(&q->lock);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tpidx = q->pidx;\n\t\tcount = 1 + skb_shinfo(skb)->nr_frags;\n\t\tcount += compute_large_page_tx_descs(skb);\n\t}\n\n\tq->in_use += count;\n\tgenbit = q->genbit;\n\tpidx = q->pidx;\n\tq->pidx += count;\n\tif (q->pidx >= q->size) {\n\t\tq->pidx -= q->size;\n\t\tq->genbit ^= 1;\n\t}\n\tspin_unlock(&q->lock);\n\n\twrite_tx_descs(adapter, skb, pidx, genbit, q);\n\n\t \n\tif (qid)\n\t\tdoorbell_pio(adapter, F_CMDQ1_ENABLE);\n\telse {\n\t\tclear_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\n\t\tif (test_and_set_bit(CMDQ_STAT_RUNNING, &q->status) == 0) {\n\t\t\tset_bit(CMDQ_STAT_LAST_PKT_DB, &q->status);\n\t\t\twritel(F_CMDQ0_ENABLE, adapter->regs + A_SG_DOORBELL);\n\t\t}\n\t}\n\n\tif (use_sched_skb) {\n\t\tif (spin_trylock(&q->lock)) {\n\t\t\tcredits = q->size - q->in_use;\n\t\t\tskb = NULL;\n\t\t\tgoto use_sched;\n\t\t}\n\t}\n\treturn NETDEV_TX_OK;\n}\n\n#define MK_ETH_TYPE_MSS(type, mss) (((mss) & 0x3FFF) | ((type) << 14))\n\n \nstatic inline int eth_hdr_len(const void *data)\n{\n\tconst struct ethhdr *e = data;\n\n\treturn e->h_proto == htons(ETH_P_8021Q) ? VLAN_ETH_HLEN : ETH_HLEN;\n}\n\n \nnetdev_tx_t t1_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct adapter *adapter = dev->ml_priv;\n\tstruct sge *sge = adapter->sge;\n\tstruct sge_port_stats *st = this_cpu_ptr(sge->port_stats[dev->if_port]);\n\tstruct cpl_tx_pkt *cpl;\n\tstruct sk_buff *orig_skb = skb;\n\tint ret;\n\n\tif (skb->protocol == htons(ETH_P_CPL5))\n\t\tgoto send;\n\n\t \n\tif (unlikely(skb_headroom(skb) < dev->hard_header_len - ETH_HLEN)) {\n\t\tskb = skb_realloc_headroom(skb, sizeof(struct cpl_tx_pkt_lso));\n\t\t++st->tx_need_hdrroom;\n\t\tdev_kfree_skb_any(orig_skb);\n\t\tif (!skb)\n\t\t\treturn NETDEV_TX_OK;\n\t}\n\n\tif (skb_shinfo(skb)->gso_size) {\n\t\tint eth_type;\n\t\tstruct cpl_tx_pkt_lso *hdr;\n\n\t\t++st->tx_tso;\n\n\t\teth_type = skb_network_offset(skb) == ETH_HLEN ?\n\t\t\tCPL_ETH_II : CPL_ETH_II_VLAN;\n\n\t\thdr = skb_push(skb, sizeof(*hdr));\n\t\thdr->opcode = CPL_TX_PKT_LSO;\n\t\thdr->ip_csum_dis = hdr->l4_csum_dis = 0;\n\t\thdr->ip_hdr_words = ip_hdr(skb)->ihl;\n\t\thdr->tcp_hdr_words = tcp_hdr(skb)->doff;\n\t\thdr->eth_type_mss = htons(MK_ETH_TYPE_MSS(eth_type,\n\t\t\t\t\t\t\t  skb_shinfo(skb)->gso_size));\n\t\thdr->len = htonl(skb->len - sizeof(*hdr));\n\t\tcpl = (struct cpl_tx_pkt *)hdr;\n\t} else {\n\t\t \n\t\tif (unlikely(skb->len < ETH_HLEN ||\n\t\t\t     skb->len > dev->mtu + eth_hdr_len(skb->data))) {\n\t\t\tnetdev_dbg(dev, \"packet size %d hdr %d mtu%d\\n\",\n\t\t\t\t   skb->len, eth_hdr_len(skb->data), dev->mtu);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t    ip_hdr(skb)->protocol == IPPROTO_UDP) {\n\t\t\tif (unlikely(skb_checksum_help(skb))) {\n\t\t\t\tnetdev_dbg(dev, \"unable to do udp checksum\\n\");\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\t\treturn NETDEV_TX_OK;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif ((unlikely(!adapter->sge->espibug_skb[dev->if_port]))) {\n\t\t\tif (skb->protocol == htons(ETH_P_ARP) &&\n\t\t\t    arp_hdr(skb)->ar_op == htons(ARPOP_REQUEST)) {\n\t\t\t\tadapter->sge->espibug_skb[dev->if_port] = skb;\n\t\t\t\t \n\t\t\t\tskb = skb_get(skb);\n\t\t\t}\n\t\t}\n\n\t\tcpl = __skb_push(skb, sizeof(*cpl));\n\t\tcpl->opcode = CPL_TX_PKT;\n\t\tcpl->ip_csum_dis = 1;     \n\t\tcpl->l4_csum_dis = skb->ip_summed == CHECKSUM_PARTIAL ? 0 : 1;\n\t\t \n\n\t\tst->tx_cso += (skb->ip_summed == CHECKSUM_PARTIAL);\n\t}\n\tcpl->iff = dev->if_port;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tcpl->vlan_valid = 1;\n\t\tcpl->vlan = htons(skb_vlan_tag_get(skb));\n\t\tst->vlan_insert++;\n\t} else\n\t\tcpl->vlan_valid = 0;\n\nsend:\n\tret = t1_sge_tx(skb, adapter, 0, dev);\n\n\t \n\tif (unlikely(ret != NETDEV_TX_OK && skb != orig_skb)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tret = NETDEV_TX_OK;\n\t}\n\treturn ret;\n}\n\n \nstatic void sge_tx_reclaim_cb(struct timer_list *t)\n{\n\tint i;\n\tstruct sge *sge = from_timer(sge, t, tx_reclaim_timer);\n\n\tfor (i = 0; i < SGE_CMDQ_N; ++i) {\n\t\tstruct cmdQ *q = &sge->cmdQ[i];\n\n\t\tif (!spin_trylock(&q->lock))\n\t\t\tcontinue;\n\n\t\treclaim_completed_tx(sge, q);\n\t\tif (i == 0 && q->in_use) {     \n\t\t\twritel(F_CMDQ0_ENABLE, sge->adapter->regs + A_SG_DOORBELL);\n\t\t}\n\t\tspin_unlock(&q->lock);\n\t}\n\tmod_timer(&sge->tx_reclaim_timer, jiffies + TX_RECLAIM_PERIOD);\n}\n\n \nint t1_sge_set_coalesce_params(struct sge *sge, struct sge_params *p)\n{\n\tsge->fixed_intrtimer = p->rx_coalesce_usecs *\n\t\tcore_ticks_per_usec(sge->adapter);\n\twritel(sge->fixed_intrtimer, sge->adapter->regs + A_SG_INTRTIMER);\n\treturn 0;\n}\n\n \nint t1_sge_configure(struct sge *sge, struct sge_params *p)\n{\n\tif (alloc_rx_resources(sge, p))\n\t\treturn -ENOMEM;\n\tif (alloc_tx_resources(sge, p)) {\n\t\tfree_rx_resources(sge);\n\t\treturn -ENOMEM;\n\t}\n\tconfigure_sge(sge, p);\n\n\t \n\tp->large_buf_capacity = jumbo_payload_capacity(sge);\n\treturn 0;\n}\n\n \nvoid t1_sge_stop(struct sge *sge)\n{\n\tint i;\n\twritel(0, sge->adapter->regs + A_SG_CONTROL);\n\treadl(sge->adapter->regs + A_SG_CONTROL);  \n\n\tif (is_T2(sge->adapter))\n\t\tdel_timer_sync(&sge->espibug_timer);\n\n\tdel_timer_sync(&sge->tx_reclaim_timer);\n\tif (sge->tx_sched)\n\t\ttx_sched_stop(sge);\n\n\tfor (i = 0; i < MAX_NPORTS; i++)\n\t\tkfree_skb(sge->espibug_skb[i]);\n}\n\n \nvoid t1_sge_start(struct sge *sge)\n{\n\trefill_free_list(sge, &sge->freelQ[0]);\n\trefill_free_list(sge, &sge->freelQ[1]);\n\n\twritel(sge->sge_control, sge->adapter->regs + A_SG_CONTROL);\n\tdoorbell_pio(sge->adapter, F_FL0_ENABLE | F_FL1_ENABLE);\n\treadl(sge->adapter->regs + A_SG_CONTROL);  \n\n\tmod_timer(&sge->tx_reclaim_timer, jiffies + TX_RECLAIM_PERIOD);\n\n\tif (is_T2(sge->adapter))\n\t\tmod_timer(&sge->espibug_timer, jiffies + sge->espibug_timeout);\n}\n\n \nstatic void espibug_workaround_t204(struct timer_list *t)\n{\n\tstruct sge *sge = from_timer(sge, t, espibug_timer);\n\tstruct adapter *adapter = sge->adapter;\n\tunsigned int nports = adapter->params.nports;\n\tu32 seop[MAX_NPORTS];\n\n\tif (adapter->open_device_map & PORT_MASK) {\n\t\tint i;\n\n\t\tif (t1_espi_get_mon_t204(adapter, &(seop[0]), 0) < 0)\n\t\t\treturn;\n\n\t\tfor (i = 0; i < nports; i++) {\n\t\t\tstruct sk_buff *skb = sge->espibug_skb[i];\n\n\t\t\tif (!netif_running(adapter->port[i].dev) ||\n\t\t\t    netif_queue_stopped(adapter->port[i].dev) ||\n\t\t\t    !seop[i] || ((seop[i] & 0xfff) != 0) || !skb)\n\t\t\t\tcontinue;\n\n\t\t\tif (!skb->cb[0]) {\n\t\t\t\tskb_copy_to_linear_data_offset(skb,\n\t\t\t\t\t\t    sizeof(struct cpl_tx_pkt),\n\t\t\t\t\t\t\t       ch_mac_addr,\n\t\t\t\t\t\t\t       ETH_ALEN);\n\t\t\t\tskb_copy_to_linear_data_offset(skb,\n\t\t\t\t\t\t\t       skb->len - 10,\n\t\t\t\t\t\t\t       ch_mac_addr,\n\t\t\t\t\t\t\t       ETH_ALEN);\n\t\t\t\tskb->cb[0] = 0xff;\n\t\t\t}\n\n\t\t\t \n\t\t\tskb = skb_get(skb);\n\t\t\tt1_sge_tx(skb, adapter, 0, adapter->port[i].dev);\n\t\t}\n\t}\n\tmod_timer(&sge->espibug_timer, jiffies + sge->espibug_timeout);\n}\n\nstatic void espibug_workaround(struct timer_list *t)\n{\n\tstruct sge *sge = from_timer(sge, t, espibug_timer);\n\tstruct adapter *adapter = sge->adapter;\n\n\tif (netif_running(adapter->port[0].dev)) {\n\t        struct sk_buff *skb = sge->espibug_skb[0];\n\t        u32 seop = t1_espi_get_mon(adapter, 0x930, 0);\n\n\t        if ((seop & 0xfff0fff) == 0xfff && skb) {\n\t                if (!skb->cb[0]) {\n\t                        skb_copy_to_linear_data_offset(skb,\n\t\t\t\t\t\t     sizeof(struct cpl_tx_pkt),\n\t\t\t\t\t\t\t       ch_mac_addr,\n\t\t\t\t\t\t\t       ETH_ALEN);\n\t                        skb_copy_to_linear_data_offset(skb,\n\t\t\t\t\t\t\t       skb->len - 10,\n\t\t\t\t\t\t\t       ch_mac_addr,\n\t\t\t\t\t\t\t       ETH_ALEN);\n\t                        skb->cb[0] = 0xff;\n\t                }\n\n\t                 \n\t                skb = skb_get(skb);\n\t                t1_sge_tx(skb, adapter, 0, adapter->port[0].dev);\n\t        }\n\t}\n\tmod_timer(&sge->espibug_timer, jiffies + sge->espibug_timeout);\n}\n\n \nstruct sge *t1_sge_create(struct adapter *adapter, struct sge_params *p)\n{\n\tstruct sge *sge = kzalloc(sizeof(*sge), GFP_KERNEL);\n\tint i;\n\n\tif (!sge)\n\t\treturn NULL;\n\n\tsge->adapter = adapter;\n\tsge->netdev = adapter->port[0].dev;\n\tsge->rx_pkt_pad = t1_is_T1B(adapter) ? 0 : 2;\n\tsge->jumbo_fl = t1_is_T1B(adapter) ? 1 : 0;\n\n\tfor_each_port(adapter, i) {\n\t\tsge->port_stats[i] = alloc_percpu(struct sge_port_stats);\n\t\tif (!sge->port_stats[i])\n\t\t\tgoto nomem_port;\n\t}\n\n\ttimer_setup(&sge->tx_reclaim_timer, sge_tx_reclaim_cb, 0);\n\n\tif (is_T2(sge->adapter)) {\n\t\ttimer_setup(&sge->espibug_timer,\n\t\t\t    adapter->params.nports > 1 ? espibug_workaround_t204 : espibug_workaround,\n\t\t\t    0);\n\n\t\tif (adapter->params.nports > 1)\n\t\t\ttx_sched_init(sge);\n\n\t\tsge->espibug_timeout = 1;\n\t\t \n\t\tif (adapter->params.nports > 1)\n\t\t\tsge->espibug_timeout = HZ/100;\n\t}\n\n\n\tp->cmdQ_size[0] = SGE_CMDQ0_E_N;\n\tp->cmdQ_size[1] = SGE_CMDQ1_E_N;\n\tp->freelQ_size[!sge->jumbo_fl] = SGE_FREEL_SIZE;\n\tp->freelQ_size[sge->jumbo_fl] = SGE_JUMBO_FREEL_SIZE;\n\tif (sge->tx_sched) {\n\t\tif (board_info(sge->adapter)->board == CHBT_BOARD_CHT204)\n\t\t\tp->rx_coalesce_usecs = 15;\n\t\telse\n\t\t\tp->rx_coalesce_usecs = 50;\n\t} else\n\t\tp->rx_coalesce_usecs = 50;\n\n\tp->coalesce_enable = 0;\n\tp->sample_interval_usecs = 0;\n\n\treturn sge;\nnomem_port:\n\twhile (i >= 0) {\n\t\tfree_percpu(sge->port_stats[i]);\n\t\t--i;\n\t}\n\tkfree(sge);\n\treturn NULL;\n\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}