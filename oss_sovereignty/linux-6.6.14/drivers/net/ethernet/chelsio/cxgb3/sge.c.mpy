{
  "module_name": "sge.c",
  "hash_id": "108ef4b5e06ff7b653a614e40d860f6b9f0e335f72311d23c58b7f6dbb060a1c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/cxgb3/sge.c",
  "human_readable_source": " \n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/prefetch.h>\n#include <net/arp.h>\n#include \"common.h\"\n#include \"regs.h\"\n#include \"sge_defs.h\"\n#include \"t3_cpl.h\"\n#include \"firmware_exports.h\"\n#include \"cxgb3_offload.h\"\n\n#define USE_GTS 0\n\n#define SGE_RX_SM_BUF_SIZE 1536\n\n#define SGE_RX_COPY_THRES  256\n#define SGE_RX_PULL_LEN    128\n\n#define SGE_PG_RSVD SMP_CACHE_BYTES\n \n#define FL0_PG_CHUNK_SIZE  2048\n#define FL0_PG_ORDER 0\n#define FL0_PG_ALLOC_SIZE (PAGE_SIZE << FL0_PG_ORDER)\n#define FL1_PG_CHUNK_SIZE (PAGE_SIZE > 8192 ? 16384 : 8192)\n#define FL1_PG_ORDER (PAGE_SIZE > 8192 ? 0 : 1)\n#define FL1_PG_ALLOC_SIZE (PAGE_SIZE << FL1_PG_ORDER)\n\n#define SGE_RX_DROP_THRES 16\n#define RX_RECLAIM_PERIOD (HZ/4)\n\n \n#define MAX_RX_REFILL 16U\n \n#define TX_RECLAIM_PERIOD (HZ / 4)\n#define TX_RECLAIM_TIMER_CHUNK 64U\n#define TX_RECLAIM_CHUNK 16U\n\n \n#define WR_LEN (WR_FLITS * 8)\n\n \nenum { TXQ_ETH, TXQ_OFLD, TXQ_CTRL };\n\n \nenum {\n\tTXQ_RUNNING = 1 << 0,\t \n\tTXQ_LAST_PKT_DB = 1 << 1,\t \n};\n\nstruct tx_desc {\n\t__be64 flit[TX_DESC_FLITS];\n};\n\nstruct rx_desc {\n\t__be32 addr_lo;\n\t__be32 len_gen;\n\t__be32 gen2;\n\t__be32 addr_hi;\n};\n\nstruct tx_sw_desc {\t\t \n\tstruct sk_buff *skb;\n\tu8 eop;        \n\tu8 addr_idx;   \n\tu8 fragidx;    \n\ts8 sflit;      \n};\n\nstruct rx_sw_desc {                 \n\tunion {\n\t\tstruct sk_buff *skb;\n\t\tstruct fl_pg_chunk pg_chunk;\n\t};\n\tDEFINE_DMA_UNMAP_ADDR(dma_addr);\n};\n\nstruct rsp_desc {\t\t \n\tstruct rss_header rss_hdr;\n\t__be32 flags;\n\t__be32 len_cq;\n\tstruct_group(immediate,\n\t\tu8 imm_data[47];\n\t\tu8 intr_gen;\n\t);\n};\n\n \nstruct deferred_unmap_info {\n\tstruct pci_dev *pdev;\n\tdma_addr_t addr[MAX_SKB_FRAGS + 1];\n};\n\n \nstatic u8 flit_desc_map[] = {\n\t0,\n#if SGE_NUM_GENBITS == 1\n\t1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n\t2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n\t3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n\t4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4\n#elif SGE_NUM_GENBITS == 2\n\t1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n\t2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n\t3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n\t4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n#else\n# error \"SGE_NUM_GENBITS must be 1 or 2\"\n#endif\n};\n\nstatic inline struct sge_qset *rspq_to_qset(const struct sge_rspq *q)\n{\n\treturn container_of(q, struct sge_qset, rspq);\n}\n\nstatic inline struct sge_qset *txq_to_qset(const struct sge_txq *q, int qidx)\n{\n\treturn container_of(q, struct sge_qset, txq[qidx]);\n}\n\n \nstatic inline void refill_rspq(struct adapter *adapter,\n\t\t\t       const struct sge_rspq *q, unsigned int credits)\n{\n\trmb();\n\tt3_write_reg(adapter, A_SG_RSPQ_CREDIT_RETURN,\n\t\t     V_RSPQ(q->cntxt_id) | V_CREDITS(credits));\n}\n\n \nstatic inline int need_skb_unmap(void)\n{\n#ifdef CONFIG_NEED_DMA_MAP_STATE\n\treturn 1;\n#else\n\treturn 0;\n#endif\n}\n\n \nstatic inline void unmap_skb(struct sk_buff *skb, struct sge_txq *q,\n\t\t\t     unsigned int cidx, struct pci_dev *pdev)\n{\n\tconst struct sg_ent *sgp;\n\tstruct tx_sw_desc *d = &q->sdesc[cidx];\n\tint nfrags, frag_idx, curflit, j = d->addr_idx;\n\n\tsgp = (struct sg_ent *)&q->desc[cidx].flit[d->sflit];\n\tfrag_idx = d->fragidx;\n\n\tif (frag_idx == 0 && skb_headlen(skb)) {\n\t\tdma_unmap_single(&pdev->dev, be64_to_cpu(sgp->addr[0]),\n\t\t\t\t skb_headlen(skb), DMA_TO_DEVICE);\n\t\tj = 1;\n\t}\n\n\tcurflit = d->sflit + 1 + j;\n\tnfrags = skb_shinfo(skb)->nr_frags;\n\n\twhile (frag_idx < nfrags && curflit < WR_FLITS) {\n\t\tdma_unmap_page(&pdev->dev, be64_to_cpu(sgp->addr[j]),\n\t\t\t       skb_frag_size(&skb_shinfo(skb)->frags[frag_idx]),\n\t\t\t       DMA_TO_DEVICE);\n\t\tj ^= 1;\n\t\tif (j == 0) {\n\t\t\tsgp++;\n\t\t\tcurflit++;\n\t\t}\n\t\tcurflit++;\n\t\tfrag_idx++;\n\t}\n\n\tif (frag_idx < nfrags) {    \n\t\td = cidx + 1 == q->size ? q->sdesc : d + 1;\n\t\td->fragidx = frag_idx;\n\t\td->addr_idx = j;\n\t\td->sflit = curflit - WR_FLITS - j;  \n\t}\n}\n\n \nstatic void free_tx_desc(struct adapter *adapter, struct sge_txq *q,\n\t\t\t unsigned int n)\n{\n\tstruct tx_sw_desc *d;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tunsigned int cidx = q->cidx;\n\n\tconst int need_unmap = need_skb_unmap() &&\n\t\t\t       q->cntxt_id >= FW_TUNNEL_SGEEC_START;\n\n\td = &q->sdesc[cidx];\n\twhile (n--) {\n\t\tif (d->skb) {\t \n\t\t\tif (need_unmap)\n\t\t\t\tunmap_skb(d->skb, q, cidx, pdev);\n\t\t\tif (d->eop) {\n\t\t\t\tdev_consume_skb_any(d->skb);\n\t\t\t\td->skb = NULL;\n\t\t\t}\n\t\t}\n\t\t++d;\n\t\tif (++cidx == q->size) {\n\t\t\tcidx = 0;\n\t\t\td = q->sdesc;\n\t\t}\n\t}\n\tq->cidx = cidx;\n}\n\n \nstatic inline unsigned int reclaim_completed_tx(struct adapter *adapter,\n\t\t\t\t\t\tstruct sge_txq *q,\n\t\t\t\t\t\tunsigned int chunk)\n{\n\tunsigned int reclaim = q->processed - q->cleaned;\n\n\treclaim = min(chunk, reclaim);\n\tif (reclaim) {\n\t\tfree_tx_desc(adapter, q, reclaim);\n\t\tq->cleaned += reclaim;\n\t\tq->in_use -= reclaim;\n\t}\n\treturn q->processed - q->cleaned;\n}\n\n \nstatic inline int should_restart_tx(const struct sge_txq *q)\n{\n\tunsigned int r = q->processed - q->cleaned;\n\n\treturn q->in_use - r < (q->size >> 1);\n}\n\nstatic void clear_rx_desc(struct pci_dev *pdev, const struct sge_fl *q,\n\t\t\t  struct rx_sw_desc *d)\n{\n\tif (q->use_pages && d->pg_chunk.page) {\n\t\t(*d->pg_chunk.p_cnt)--;\n\t\tif (!*d->pg_chunk.p_cnt)\n\t\t\tdma_unmap_page(&pdev->dev, d->pg_chunk.mapping,\n\t\t\t\t       q->alloc_size, DMA_FROM_DEVICE);\n\n\t\tput_page(d->pg_chunk.page);\n\t\td->pg_chunk.page = NULL;\n\t} else {\n\t\tdma_unmap_single(&pdev->dev, dma_unmap_addr(d, dma_addr),\n\t\t\t\t q->buf_size, DMA_FROM_DEVICE);\n\t\tkfree_skb(d->skb);\n\t\td->skb = NULL;\n\t}\n}\n\n \nstatic void free_rx_bufs(struct pci_dev *pdev, struct sge_fl *q)\n{\n\tunsigned int cidx = q->cidx;\n\n\twhile (q->credits--) {\n\t\tstruct rx_sw_desc *d = &q->sdesc[cidx];\n\n\n\t\tclear_rx_desc(pdev, q, d);\n\t\tif (++cidx == q->size)\n\t\t\tcidx = 0;\n\t}\n\n\tif (q->pg_chunk.page) {\n\t\t__free_pages(q->pg_chunk.page, q->order);\n\t\tq->pg_chunk.page = NULL;\n\t}\n}\n\n \nstatic inline int add_one_rx_buf(void *va, unsigned int len,\n\t\t\t\t struct rx_desc *d, struct rx_sw_desc *sd,\n\t\t\t\t unsigned int gen, struct pci_dev *pdev)\n{\n\tdma_addr_t mapping;\n\n\tmapping = dma_map_single(&pdev->dev, va, len, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(&pdev->dev, mapping)))\n\t\treturn -ENOMEM;\n\n\tdma_unmap_addr_set(sd, dma_addr, mapping);\n\n\td->addr_lo = cpu_to_be32(mapping);\n\td->addr_hi = cpu_to_be32((u64) mapping >> 32);\n\tdma_wmb();\n\td->len_gen = cpu_to_be32(V_FLD_GEN1(gen));\n\td->gen2 = cpu_to_be32(V_FLD_GEN2(gen));\n\treturn 0;\n}\n\nstatic inline int add_one_rx_chunk(dma_addr_t mapping, struct rx_desc *d,\n\t\t\t\t   unsigned int gen)\n{\n\td->addr_lo = cpu_to_be32(mapping);\n\td->addr_hi = cpu_to_be32((u64) mapping >> 32);\n\tdma_wmb();\n\td->len_gen = cpu_to_be32(V_FLD_GEN1(gen));\n\td->gen2 = cpu_to_be32(V_FLD_GEN2(gen));\n\treturn 0;\n}\n\nstatic int alloc_pg_chunk(struct adapter *adapter, struct sge_fl *q,\n\t\t\t  struct rx_sw_desc *sd, gfp_t gfp,\n\t\t\t  unsigned int order)\n{\n\tif (!q->pg_chunk.page) {\n\t\tdma_addr_t mapping;\n\n\t\tq->pg_chunk.page = alloc_pages(gfp, order);\n\t\tif (unlikely(!q->pg_chunk.page))\n\t\t\treturn -ENOMEM;\n\t\tq->pg_chunk.va = page_address(q->pg_chunk.page);\n\t\tq->pg_chunk.p_cnt = q->pg_chunk.va + (PAGE_SIZE << order) -\n\t\t\t\t    SGE_PG_RSVD;\n\t\tq->pg_chunk.offset = 0;\n\t\tmapping = dma_map_page(&adapter->pdev->dev, q->pg_chunk.page,\n\t\t\t\t       0, q->alloc_size, DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(&adapter->pdev->dev, mapping))) {\n\t\t\t__free_pages(q->pg_chunk.page, order);\n\t\t\tq->pg_chunk.page = NULL;\n\t\t\treturn -EIO;\n\t\t}\n\t\tq->pg_chunk.mapping = mapping;\n\t}\n\tsd->pg_chunk = q->pg_chunk;\n\n\tprefetch(sd->pg_chunk.p_cnt);\n\n\tq->pg_chunk.offset += q->buf_size;\n\tif (q->pg_chunk.offset == (PAGE_SIZE << order))\n\t\tq->pg_chunk.page = NULL;\n\telse {\n\t\tq->pg_chunk.va += q->buf_size;\n\t\tget_page(q->pg_chunk.page);\n\t}\n\n\tif (sd->pg_chunk.offset == 0)\n\t\t*sd->pg_chunk.p_cnt = 1;\n\telse\n\t\t*sd->pg_chunk.p_cnt += 1;\n\n\treturn 0;\n}\n\nstatic inline void ring_fl_db(struct adapter *adap, struct sge_fl *q)\n{\n\tif (q->pend_cred >= q->credits / 4) {\n\t\tq->pend_cred = 0;\n\t\twmb();\n\t\tt3_write_reg(adap, A_SG_KDOORBELL, V_EGRCNTX(q->cntxt_id));\n\t}\n}\n\n \nstatic int refill_fl(struct adapter *adap, struct sge_fl *q, int n, gfp_t gfp)\n{\n\tstruct rx_sw_desc *sd = &q->sdesc[q->pidx];\n\tstruct rx_desc *d = &q->desc[q->pidx];\n\tunsigned int count = 0;\n\n\twhile (n--) {\n\t\tdma_addr_t mapping;\n\t\tint err;\n\n\t\tif (q->use_pages) {\n\t\t\tif (unlikely(alloc_pg_chunk(adap, q, sd, gfp,\n\t\t\t\t\t\t    q->order))) {\nnomem:\t\t\t\tq->alloc_failed++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmapping = sd->pg_chunk.mapping + sd->pg_chunk.offset;\n\t\t\tdma_unmap_addr_set(sd, dma_addr, mapping);\n\n\t\t\tadd_one_rx_chunk(mapping, d, q->gen);\n\t\t\tdma_sync_single_for_device(&adap->pdev->dev, mapping,\n\t\t\t\t\t\t   q->buf_size - SGE_PG_RSVD,\n\t\t\t\t\t\t   DMA_FROM_DEVICE);\n\t\t} else {\n\t\t\tvoid *buf_start;\n\n\t\t\tstruct sk_buff *skb = alloc_skb(q->buf_size, gfp);\n\t\t\tif (!skb)\n\t\t\t\tgoto nomem;\n\n\t\t\tsd->skb = skb;\n\t\t\tbuf_start = skb->data;\n\t\t\terr = add_one_rx_buf(buf_start, q->buf_size, d, sd,\n\t\t\t\t\t     q->gen, adap->pdev);\n\t\t\tif (unlikely(err)) {\n\t\t\t\tclear_rx_desc(adap->pdev, q, sd);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\td++;\n\t\tsd++;\n\t\tif (++q->pidx == q->size) {\n\t\t\tq->pidx = 0;\n\t\t\tq->gen ^= 1;\n\t\t\tsd = q->sdesc;\n\t\t\td = q->desc;\n\t\t}\n\t\tcount++;\n\t}\n\n\tq->credits += count;\n\tq->pend_cred += count;\n\tring_fl_db(adap, q);\n\n\treturn count;\n}\n\nstatic inline void __refill_fl(struct adapter *adap, struct sge_fl *fl)\n{\n\trefill_fl(adap, fl, min(MAX_RX_REFILL, fl->size - fl->credits),\n\t\t  GFP_ATOMIC | __GFP_COMP);\n}\n\n \nstatic void recycle_rx_buf(struct adapter *adap, struct sge_fl *q,\n\t\t\t   unsigned int idx)\n{\n\tstruct rx_desc *from = &q->desc[idx];\n\tstruct rx_desc *to = &q->desc[q->pidx];\n\n\tq->sdesc[q->pidx] = q->sdesc[idx];\n\tto->addr_lo = from->addr_lo;\t \n\tto->addr_hi = from->addr_hi;\t \n\tdma_wmb();\n\tto->len_gen = cpu_to_be32(V_FLD_GEN1(q->gen));\n\tto->gen2 = cpu_to_be32(V_FLD_GEN2(q->gen));\n\n\tif (++q->pidx == q->size) {\n\t\tq->pidx = 0;\n\t\tq->gen ^= 1;\n\t}\n\n\tq->credits++;\n\tq->pend_cred++;\n\tring_fl_db(adap, q);\n}\n\n \nstatic void *alloc_ring(struct pci_dev *pdev, size_t nelem, size_t elem_size,\n\t\t\tsize_t sw_size, dma_addr_t * phys, void *metadata)\n{\n\tsize_t len = nelem * elem_size;\n\tvoid *s = NULL;\n\tvoid *p = dma_alloc_coherent(&pdev->dev, len, phys, GFP_KERNEL);\n\n\tif (!p)\n\t\treturn NULL;\n\tif (sw_size && metadata) {\n\t\ts = kcalloc(nelem, sw_size, GFP_KERNEL);\n\n\t\tif (!s) {\n\t\t\tdma_free_coherent(&pdev->dev, len, p, *phys);\n\t\t\treturn NULL;\n\t\t}\n\t\t*(void **)metadata = s;\n\t}\n\treturn p;\n}\n\n \nstatic void t3_reset_qset(struct sge_qset *q)\n{\n\tif (q->adap &&\n\t    !(q->adap->flags & NAPI_INIT)) {\n\t\tmemset(q, 0, sizeof(*q));\n\t\treturn;\n\t}\n\n\tq->adap = NULL;\n\tmemset(&q->rspq, 0, sizeof(q->rspq));\n\tmemset(q->fl, 0, sizeof(struct sge_fl) * SGE_RXQ_PER_SET);\n\tmemset(q->txq, 0, sizeof(struct sge_txq) * SGE_TXQ_PER_SET);\n\tq->txq_stopped = 0;\n\tq->tx_reclaim_timer.function = NULL;  \n\tq->rx_reclaim_timer.function = NULL;\n\tq->nomem = 0;\n\tnapi_free_frags(&q->napi);\n}\n\n\n \nstatic void t3_free_qset(struct adapter *adapter, struct sge_qset *q)\n{\n\tint i;\n\tstruct pci_dev *pdev = adapter->pdev;\n\n\tfor (i = 0; i < SGE_RXQ_PER_SET; ++i)\n\t\tif (q->fl[i].desc) {\n\t\t\tspin_lock_irq(&adapter->sge.reg_lock);\n\t\t\tt3_sge_disable_fl(adapter, q->fl[i].cntxt_id);\n\t\t\tspin_unlock_irq(&adapter->sge.reg_lock);\n\t\t\tfree_rx_bufs(pdev, &q->fl[i]);\n\t\t\tkfree(q->fl[i].sdesc);\n\t\t\tdma_free_coherent(&pdev->dev,\n\t\t\t\t\t  q->fl[i].size *\n\t\t\t\t\t  sizeof(struct rx_desc), q->fl[i].desc,\n\t\t\t\t\t  q->fl[i].phys_addr);\n\t\t}\n\n\tfor (i = 0; i < SGE_TXQ_PER_SET; ++i)\n\t\tif (q->txq[i].desc) {\n\t\t\tspin_lock_irq(&adapter->sge.reg_lock);\n\t\t\tt3_sge_enable_ecntxt(adapter, q->txq[i].cntxt_id, 0);\n\t\t\tspin_unlock_irq(&adapter->sge.reg_lock);\n\t\t\tif (q->txq[i].sdesc) {\n\t\t\t\tfree_tx_desc(adapter, &q->txq[i],\n\t\t\t\t\t     q->txq[i].in_use);\n\t\t\t\tkfree(q->txq[i].sdesc);\n\t\t\t}\n\t\t\tdma_free_coherent(&pdev->dev,\n\t\t\t\t\t  q->txq[i].size *\n\t\t\t\t\t  sizeof(struct tx_desc),\n\t\t\t\t\t  q->txq[i].desc, q->txq[i].phys_addr);\n\t\t\t__skb_queue_purge(&q->txq[i].sendq);\n\t\t}\n\n\tif (q->rspq.desc) {\n\t\tspin_lock_irq(&adapter->sge.reg_lock);\n\t\tt3_sge_disable_rspcntxt(adapter, q->rspq.cntxt_id);\n\t\tspin_unlock_irq(&adapter->sge.reg_lock);\n\t\tdma_free_coherent(&pdev->dev,\n\t\t\t\t  q->rspq.size * sizeof(struct rsp_desc),\n\t\t\t\t  q->rspq.desc, q->rspq.phys_addr);\n\t}\n\n\tt3_reset_qset(q);\n}\n\n \nstatic void init_qset_cntxt(struct sge_qset *qs, unsigned int id)\n{\n\tqs->rspq.cntxt_id = id;\n\tqs->fl[0].cntxt_id = 2 * id;\n\tqs->fl[1].cntxt_id = 2 * id + 1;\n\tqs->txq[TXQ_ETH].cntxt_id = FW_TUNNEL_SGEEC_START + id;\n\tqs->txq[TXQ_ETH].token = FW_TUNNEL_TID_START + id;\n\tqs->txq[TXQ_OFLD].cntxt_id = FW_OFLD_SGEEC_START + id;\n\tqs->txq[TXQ_CTRL].cntxt_id = FW_CTRL_SGEEC_START + id;\n\tqs->txq[TXQ_CTRL].token = FW_CTRL_TID_START + id;\n}\n\n \nstatic inline unsigned int sgl_len(unsigned int n)\n{\n\t \n\treturn (3 * n) / 2 + (n & 1);\n}\n\n \nstatic inline unsigned int flits_to_desc(unsigned int n)\n{\n\tBUG_ON(n >= ARRAY_SIZE(flit_desc_map));\n\treturn flit_desc_map[n];\n}\n\n \nstatic struct sk_buff *get_packet(struct adapter *adap, struct sge_fl *fl,\n\t\t\t\t  unsigned int len, unsigned int drop_thres)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct rx_sw_desc *sd = &fl->sdesc[fl->cidx];\n\n\tprefetch(sd->skb->data);\n\tfl->credits--;\n\n\tif (len <= SGE_RX_COPY_THRES) {\n\t\tskb = alloc_skb(len, GFP_ATOMIC);\n\t\tif (likely(skb != NULL)) {\n\t\t\t__skb_put(skb, len);\n\t\t\tdma_sync_single_for_cpu(&adap->pdev->dev,\n\t\t\t\t\t\tdma_unmap_addr(sd, dma_addr),\n\t\t\t\t\t\tlen, DMA_FROM_DEVICE);\n\t\t\tmemcpy(skb->data, sd->skb->data, len);\n\t\t\tdma_sync_single_for_device(&adap->pdev->dev,\n\t\t\t\t\t\t   dma_unmap_addr(sd, dma_addr),\n\t\t\t\t\t\t   len, DMA_FROM_DEVICE);\n\t\t} else if (!drop_thres)\n\t\t\tgoto use_orig_buf;\nrecycle:\n\t\trecycle_rx_buf(adap, fl, fl->cidx);\n\t\treturn skb;\n\t}\n\n\tif (unlikely(fl->credits < drop_thres) &&\n\t    refill_fl(adap, fl, min(MAX_RX_REFILL, fl->size - fl->credits - 1),\n\t\t      GFP_ATOMIC | __GFP_COMP) == 0)\n\t\tgoto recycle;\n\nuse_orig_buf:\n\tdma_unmap_single(&adap->pdev->dev, dma_unmap_addr(sd, dma_addr),\n\t\t\t fl->buf_size, DMA_FROM_DEVICE);\n\tskb = sd->skb;\n\tskb_put(skb, len);\n\t__refill_fl(adap, fl);\n\treturn skb;\n}\n\n \nstatic struct sk_buff *get_packet_pg(struct adapter *adap, struct sge_fl *fl,\n\t\t\t\t     struct sge_rspq *q, unsigned int len,\n\t\t\t\t     unsigned int drop_thres)\n{\n\tstruct sk_buff *newskb, *skb;\n\tstruct rx_sw_desc *sd = &fl->sdesc[fl->cidx];\n\n\tdma_addr_t dma_addr = dma_unmap_addr(sd, dma_addr);\n\n\tnewskb = skb = q->pg_skb;\n\tif (!skb && (len <= SGE_RX_COPY_THRES)) {\n\t\tnewskb = alloc_skb(len, GFP_ATOMIC);\n\t\tif (likely(newskb != NULL)) {\n\t\t\t__skb_put(newskb, len);\n\t\t\tdma_sync_single_for_cpu(&adap->pdev->dev, dma_addr,\n\t\t\t\t\t\tlen, DMA_FROM_DEVICE);\n\t\t\tmemcpy(newskb->data, sd->pg_chunk.va, len);\n\t\t\tdma_sync_single_for_device(&adap->pdev->dev, dma_addr,\n\t\t\t\t\t\t   len, DMA_FROM_DEVICE);\n\t\t} else if (!drop_thres)\n\t\t\treturn NULL;\nrecycle:\n\t\tfl->credits--;\n\t\trecycle_rx_buf(adap, fl, fl->cidx);\n\t\tq->rx_recycle_buf++;\n\t\treturn newskb;\n\t}\n\n\tif (unlikely(q->rx_recycle_buf || (!skb && fl->credits <= drop_thres)))\n\t\tgoto recycle;\n\n\tprefetch(sd->pg_chunk.p_cnt);\n\n\tif (!skb)\n\t\tnewskb = alloc_skb(SGE_RX_PULL_LEN, GFP_ATOMIC);\n\n\tif (unlikely(!newskb)) {\n\t\tif (!drop_thres)\n\t\t\treturn NULL;\n\t\tgoto recycle;\n\t}\n\n\tdma_sync_single_for_cpu(&adap->pdev->dev, dma_addr, len,\n\t\t\t\tDMA_FROM_DEVICE);\n\t(*sd->pg_chunk.p_cnt)--;\n\tif (!*sd->pg_chunk.p_cnt && sd->pg_chunk.page != fl->pg_chunk.page)\n\t\tdma_unmap_page(&adap->pdev->dev, sd->pg_chunk.mapping,\n\t\t\t       fl->alloc_size, DMA_FROM_DEVICE);\n\tif (!skb) {\n\t\t__skb_put(newskb, SGE_RX_PULL_LEN);\n\t\tmemcpy(newskb->data, sd->pg_chunk.va, SGE_RX_PULL_LEN);\n\t\tskb_fill_page_desc(newskb, 0, sd->pg_chunk.page,\n\t\t\t\t   sd->pg_chunk.offset + SGE_RX_PULL_LEN,\n\t\t\t\t   len - SGE_RX_PULL_LEN);\n\t\tnewskb->len = len;\n\t\tnewskb->data_len = len - SGE_RX_PULL_LEN;\n\t\tnewskb->truesize += newskb->data_len;\n\t} else {\n\t\tskb_fill_page_desc(newskb, skb_shinfo(newskb)->nr_frags,\n\t\t\t\t   sd->pg_chunk.page,\n\t\t\t\t   sd->pg_chunk.offset, len);\n\t\tnewskb->len += len;\n\t\tnewskb->data_len += len;\n\t\tnewskb->truesize += len;\n\t}\n\n\tfl->credits--;\n\t \n\treturn newskb;\n}\n\n \nstatic inline struct sk_buff *get_imm_packet(const struct rsp_desc *resp)\n{\n\tstruct sk_buff *skb = alloc_skb(IMMED_PKT_SIZE, GFP_ATOMIC);\n\n\tif (skb) {\n\t\t__skb_put(skb, IMMED_PKT_SIZE);\n\t\tBUILD_BUG_ON(IMMED_PKT_SIZE != sizeof(resp->immediate));\n\t\tskb_copy_to_linear_data(skb, &resp->immediate, IMMED_PKT_SIZE);\n\t}\n\treturn skb;\n}\n\n \nstatic inline unsigned int calc_tx_descs(const struct sk_buff *skb)\n{\n\tunsigned int flits;\n\n\tif (skb->len <= WR_LEN - sizeof(struct cpl_tx_pkt))\n\t\treturn 1;\n\n\tflits = sgl_len(skb_shinfo(skb)->nr_frags + 1) + 2;\n\tif (skb_shinfo(skb)->gso_size)\n\t\tflits++;\n\treturn flits_to_desc(flits);\n}\n\n \nstatic int map_skb(struct pci_dev *pdev, const struct sk_buff *skb,\n\t\t   dma_addr_t *addr)\n{\n\tconst skb_frag_t *fp, *end;\n\tconst struct skb_shared_info *si;\n\n\tif (skb_headlen(skb)) {\n\t\t*addr = dma_map_single(&pdev->dev, skb->data,\n\t\t\t\t       skb_headlen(skb), DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&pdev->dev, *addr))\n\t\t\tgoto out_err;\n\t\taddr++;\n\t}\n\n\tsi = skb_shinfo(skb);\n\tend = &si->frags[si->nr_frags];\n\n\tfor (fp = si->frags; fp < end; fp++) {\n\t\t*addr = skb_frag_dma_map(&pdev->dev, fp, 0, skb_frag_size(fp),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&pdev->dev, *addr))\n\t\t\tgoto unwind;\n\t\taddr++;\n\t}\n\treturn 0;\n\nunwind:\n\twhile (fp-- > si->frags)\n\t\tdma_unmap_page(&pdev->dev, *--addr, skb_frag_size(fp),\n\t\t\t       DMA_TO_DEVICE);\n\n\tdma_unmap_single(&pdev->dev, addr[-1], skb_headlen(skb),\n\t\t\t DMA_TO_DEVICE);\nout_err:\n\treturn -ENOMEM;\n}\n\n \nstatic inline unsigned int write_sgl(const struct sk_buff *skb,\n\t\t\t\t     struct sg_ent *sgp, unsigned char *start,\n\t\t\t\t     unsigned int len, const dma_addr_t *addr)\n{\n\tunsigned int i, j = 0, k = 0, nfrags;\n\n\tif (len) {\n\t\tsgp->len[0] = cpu_to_be32(len);\n\t\tsgp->addr[j++] = cpu_to_be64(addr[k++]);\n\t}\n\n\tnfrags = skb_shinfo(skb)->nr_frags;\n\tfor (i = 0; i < nfrags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tsgp->len[j] = cpu_to_be32(skb_frag_size(frag));\n\t\tsgp->addr[j] = cpu_to_be64(addr[k++]);\n\t\tj ^= 1;\n\t\tif (j == 0)\n\t\t\t++sgp;\n\t}\n\tif (j)\n\t\tsgp->len[j] = 0;\n\treturn ((nfrags + (len != 0)) * 3) / 2 + j;\n}\n\n \nstatic inline void check_ring_tx_db(struct adapter *adap, struct sge_txq *q)\n{\n#if USE_GTS\n\tclear_bit(TXQ_LAST_PKT_DB, &q->flags);\n\tif (test_and_set_bit(TXQ_RUNNING, &q->flags) == 0) {\n\t\tset_bit(TXQ_LAST_PKT_DB, &q->flags);\n\t\tt3_write_reg(adap, A_SG_KDOORBELL,\n\t\t\t     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));\n\t}\n#else\n\twmb();\t\t\t \n\tt3_write_reg(adap, A_SG_KDOORBELL,\n\t\t     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));\n#endif\n}\n\nstatic inline void wr_gen2(struct tx_desc *d, unsigned int gen)\n{\n#if SGE_NUM_GENBITS == 2\n\td->flit[TX_DESC_FLITS - 1] = cpu_to_be64(gen);\n#endif\n}\n\n \nstatic void write_wr_hdr_sgl(unsigned int ndesc, struct sk_buff *skb,\n\t\t\t     struct tx_desc *d, unsigned int pidx,\n\t\t\t     const struct sge_txq *q,\n\t\t\t     const struct sg_ent *sgl,\n\t\t\t     unsigned int flits, unsigned int sgl_flits,\n\t\t\t     unsigned int gen, __be32 wr_hi,\n\t\t\t     __be32 wr_lo)\n{\n\tstruct work_request_hdr *wrp = (struct work_request_hdr *)d;\n\tstruct tx_sw_desc *sd = &q->sdesc[pidx];\n\n\tsd->skb = skb;\n\tif (need_skb_unmap()) {\n\t\tsd->fragidx = 0;\n\t\tsd->addr_idx = 0;\n\t\tsd->sflit = flits;\n\t}\n\n\tif (likely(ndesc == 1)) {\n\t\tsd->eop = 1;\n\t\twrp->wr_hi = htonl(F_WR_SOP | F_WR_EOP | V_WR_DATATYPE(1) |\n\t\t\t\t   V_WR_SGLSFLT(flits)) | wr_hi;\n\t\tdma_wmb();\n\t\twrp->wr_lo = htonl(V_WR_LEN(flits + sgl_flits) |\n\t\t\t\t   V_WR_GEN(gen)) | wr_lo;\n\t\twr_gen2(d, gen);\n\t} else {\n\t\tunsigned int ogen = gen;\n\t\tconst u64 *fp = (const u64 *)sgl;\n\t\tstruct work_request_hdr *wp = wrp;\n\n\t\twrp->wr_hi = htonl(F_WR_SOP | V_WR_DATATYPE(1) |\n\t\t\t\t   V_WR_SGLSFLT(flits)) | wr_hi;\n\n\t\twhile (sgl_flits) {\n\t\t\tunsigned int avail = WR_FLITS - flits;\n\n\t\t\tif (avail > sgl_flits)\n\t\t\t\tavail = sgl_flits;\n\t\t\tmemcpy(&d->flit[flits], fp, avail * sizeof(*fp));\n\t\t\tsgl_flits -= avail;\n\t\t\tndesc--;\n\t\t\tif (!sgl_flits)\n\t\t\t\tbreak;\n\n\t\t\tfp += avail;\n\t\t\td++;\n\t\t\tsd->eop = 0;\n\t\t\tsd++;\n\t\t\tif (++pidx == q->size) {\n\t\t\t\tpidx = 0;\n\t\t\t\tgen ^= 1;\n\t\t\t\td = q->desc;\n\t\t\t\tsd = q->sdesc;\n\t\t\t}\n\n\t\t\tsd->skb = skb;\n\t\t\twrp = (struct work_request_hdr *)d;\n\t\t\twrp->wr_hi = htonl(V_WR_DATATYPE(1) |\n\t\t\t\t\t   V_WR_SGLSFLT(1)) | wr_hi;\n\t\t\twrp->wr_lo = htonl(V_WR_LEN(min(WR_FLITS,\n\t\t\t\t\t\t\tsgl_flits + 1)) |\n\t\t\t\t\t   V_WR_GEN(gen)) | wr_lo;\n\t\t\twr_gen2(d, gen);\n\t\t\tflits = 1;\n\t\t}\n\t\tsd->eop = 1;\n\t\twrp->wr_hi |= htonl(F_WR_EOP);\n\t\tdma_wmb();\n\t\twp->wr_lo = htonl(V_WR_LEN(WR_FLITS) | V_WR_GEN(ogen)) | wr_lo;\n\t\twr_gen2((struct tx_desc *)wp, ogen);\n\t\tWARN_ON(ndesc != 0);\n\t}\n}\n\n \nstatic void write_tx_pkt_wr(struct adapter *adap, struct sk_buff *skb,\n\t\t\t    const struct port_info *pi,\n\t\t\t    unsigned int pidx, unsigned int gen,\n\t\t\t    struct sge_txq *q, unsigned int ndesc,\n\t\t\t    unsigned int compl, const dma_addr_t *addr)\n{\n\tunsigned int flits, sgl_flits, cntrl, tso_info;\n\tstruct sg_ent *sgp, sgl[MAX_SKB_FRAGS / 2 + 1];\n\tstruct tx_desc *d = &q->desc[pidx];\n\tstruct cpl_tx_pkt *cpl = (struct cpl_tx_pkt *)d;\n\n\tcpl->len = htonl(skb->len);\n\tcntrl = V_TXPKT_INTF(pi->port_id);\n\n\tif (skb_vlan_tag_present(skb))\n\t\tcntrl |= F_TXPKT_VLAN_VLD | V_TXPKT_VLAN(skb_vlan_tag_get(skb));\n\n\ttso_info = V_LSO_MSS(skb_shinfo(skb)->gso_size);\n\tif (tso_info) {\n\t\tint eth_type;\n\t\tstruct cpl_tx_pkt_lso *hdr = (struct cpl_tx_pkt_lso *)cpl;\n\n\t\td->flit[2] = 0;\n\t\tcntrl |= V_TXPKT_OPCODE(CPL_TX_PKT_LSO);\n\t\thdr->cntrl = htonl(cntrl);\n\t\teth_type = skb_network_offset(skb) == ETH_HLEN ?\n\t\t    CPL_ETH_II : CPL_ETH_II_VLAN;\n\t\ttso_info |= V_LSO_ETH_TYPE(eth_type) |\n\t\t    V_LSO_IPHDR_WORDS(ip_hdr(skb)->ihl) |\n\t\t    V_LSO_TCPHDR_WORDS(tcp_hdr(skb)->doff);\n\t\thdr->lso_info = htonl(tso_info);\n\t\tflits = 3;\n\t} else {\n\t\tcntrl |= V_TXPKT_OPCODE(CPL_TX_PKT);\n\t\tcntrl |= F_TXPKT_IPCSUM_DIS;\t \n\t\tcntrl |= V_TXPKT_L4CSUM_DIS(skb->ip_summed != CHECKSUM_PARTIAL);\n\t\tcpl->cntrl = htonl(cntrl);\n\n\t\tif (skb->len <= WR_LEN - sizeof(*cpl)) {\n\t\t\tq->sdesc[pidx].skb = NULL;\n\t\t\tif (!skb->data_len)\n\t\t\t\tskb_copy_from_linear_data(skb, &d->flit[2],\n\t\t\t\t\t\t\t  skb->len);\n\t\t\telse\n\t\t\t\tskb_copy_bits(skb, 0, &d->flit[2], skb->len);\n\n\t\t\tflits = (skb->len + 7) / 8 + 2;\n\t\t\tcpl->wr.wr_hi = htonl(V_WR_BCNTLFLT(skb->len & 7) |\n\t\t\t\t\t      V_WR_OP(FW_WROPCODE_TUNNEL_TX_PKT)\n\t\t\t\t\t      | F_WR_SOP | F_WR_EOP | compl);\n\t\t\tdma_wmb();\n\t\t\tcpl->wr.wr_lo = htonl(V_WR_LEN(flits) | V_WR_GEN(gen) |\n\t\t\t\t\t      V_WR_TID(q->token));\n\t\t\twr_gen2(d, gen);\n\t\t\tdev_consume_skb_any(skb);\n\t\t\treturn;\n\t\t}\n\n\t\tflits = 2;\n\t}\n\n\tsgp = ndesc == 1 ? (struct sg_ent *)&d->flit[flits] : sgl;\n\tsgl_flits = write_sgl(skb, sgp, skb->data, skb_headlen(skb), addr);\n\n\twrite_wr_hdr_sgl(ndesc, skb, d, pidx, q, sgl, flits, sgl_flits, gen,\n\t\t\t htonl(V_WR_OP(FW_WROPCODE_TUNNEL_TX_PKT) | compl),\n\t\t\t htonl(V_WR_TID(q->token)));\n}\n\nstatic inline void t3_stop_tx_queue(struct netdev_queue *txq,\n\t\t\t\t    struct sge_qset *qs, struct sge_txq *q)\n{\n\tnetif_tx_stop_queue(txq);\n\tset_bit(TXQ_ETH, &qs->txq_stopped);\n\tq->stops++;\n}\n\n \nnetdev_tx_t t3_eth_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tint qidx;\n\tunsigned int ndesc, pidx, credits, gen, compl;\n\tconst struct port_info *pi = netdev_priv(dev);\n\tstruct adapter *adap = pi->adapter;\n\tstruct netdev_queue *txq;\n\tstruct sge_qset *qs;\n\tstruct sge_txq *q;\n\tdma_addr_t addr[MAX_SKB_FRAGS + 1];\n\n\t \n\tif (unlikely(skb->len < ETH_HLEN)) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tqidx = skb_get_queue_mapping(skb);\n\tqs = &pi->qs[qidx];\n\tq = &qs->txq[TXQ_ETH];\n\ttxq = netdev_get_tx_queue(dev, qidx);\n\n\treclaim_completed_tx(adap, q, TX_RECLAIM_CHUNK);\n\n\tcredits = q->size - q->in_use;\n\tndesc = calc_tx_descs(skb);\n\n\tif (unlikely(credits < ndesc)) {\n\t\tt3_stop_tx_queue(txq, qs, q);\n\t\tdev_err(&adap->pdev->dev,\n\t\t\t\"%s: Tx ring %u full while queue awake!\\n\",\n\t\t\tdev->name, q->cntxt_id & 7);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\tif (skb->len > (WR_LEN - sizeof(struct cpl_tx_pkt))) {\n\t\tif (unlikely(map_skb(adap->pdev, skb, addr) < 0)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t}\n\n\tq->in_use += ndesc;\n\tif (unlikely(credits - ndesc < q->stop_thres)) {\n\t\tt3_stop_tx_queue(txq, qs, q);\n\n\t\tif (should_restart_tx(q) &&\n\t\t    test_and_clear_bit(TXQ_ETH, &qs->txq_stopped)) {\n\t\t\tq->restarts++;\n\t\t\tnetif_tx_start_queue(txq);\n\t\t}\n\t}\n\n\tgen = q->gen;\n\tq->unacked += ndesc;\n\tcompl = (q->unacked & 8) << (S_WR_COMPL - 3);\n\tq->unacked &= 7;\n\tpidx = q->pidx;\n\tq->pidx += ndesc;\n\tif (q->pidx >= q->size) {\n\t\tq->pidx -= q->size;\n\t\tq->gen ^= 1;\n\t}\n\n\t \n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tqs->port_stats[SGE_PSTAT_TX_CSUM]++;\n\tif (skb_shinfo(skb)->gso_size)\n\t\tqs->port_stats[SGE_PSTAT_TSO]++;\n\tif (skb_vlan_tag_present(skb))\n\t\tqs->port_stats[SGE_PSTAT_VLANINS]++;\n\n\t \n\tif (likely(!skb_shared(skb)))\n\t\tskb_orphan(skb);\n\n\twrite_tx_pkt_wr(adap, skb, pi, pidx, gen, q, ndesc, compl, addr);\n\tcheck_ring_tx_db(adap, q);\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic inline void write_imm(struct tx_desc *d, struct sk_buff *skb,\n\t\t\t     unsigned int len, unsigned int gen)\n{\n\tstruct work_request_hdr *from = (struct work_request_hdr *)skb->data;\n\tstruct work_request_hdr *to = (struct work_request_hdr *)d;\n\n\tif (likely(!skb->data_len))\n\t\tmemcpy(&to[1], &from[1], len - sizeof(*from));\n\telse\n\t\tskb_copy_bits(skb, sizeof(*from), &to[1], len - sizeof(*from));\n\n\tto->wr_hi = from->wr_hi | htonl(F_WR_SOP | F_WR_EOP |\n\t\t\t\t\tV_WR_BCNTLFLT(len & 7));\n\tdma_wmb();\n\tto->wr_lo = from->wr_lo | htonl(V_WR_GEN(gen) |\n\t\t\t\t\tV_WR_LEN((len + 7) / 8));\n\twr_gen2(d, gen);\n\tkfree_skb(skb);\n}\n\n \nstatic inline int check_desc_avail(struct adapter *adap, struct sge_txq *q,\n\t\t\t\t   struct sk_buff *skb, unsigned int ndesc,\n\t\t\t\t   unsigned int qid)\n{\n\tif (unlikely(!skb_queue_empty(&q->sendq))) {\n\t      addq_exit:__skb_queue_tail(&q->sendq, skb);\n\t\treturn 1;\n\t}\n\tif (unlikely(q->size - q->in_use < ndesc)) {\n\t\tstruct sge_qset *qs = txq_to_qset(q, qid);\n\n\t\tset_bit(qid, &qs->txq_stopped);\n\t\tsmp_mb__after_atomic();\n\n\t\tif (should_restart_tx(q) &&\n\t\t    test_and_clear_bit(qid, &qs->txq_stopped))\n\t\t\treturn 2;\n\n\t\tq->stops++;\n\t\tgoto addq_exit;\n\t}\n\treturn 0;\n}\n\n \nstatic inline void reclaim_completed_tx_imm(struct sge_txq *q)\n{\n\tunsigned int reclaim = q->processed - q->cleaned;\n\n\tq->in_use -= reclaim;\n\tq->cleaned += reclaim;\n}\n\nstatic inline int immediate(const struct sk_buff *skb)\n{\n\treturn skb->len <= WR_LEN;\n}\n\n \nstatic int ctrl_xmit(struct adapter *adap, struct sge_txq *q,\n\t\t     struct sk_buff *skb)\n{\n\tint ret;\n\tstruct work_request_hdr *wrp = (struct work_request_hdr *)skb->data;\n\n\tif (unlikely(!immediate(skb))) {\n\t\tWARN_ON(1);\n\t\tdev_kfree_skb(skb);\n\t\treturn NET_XMIT_SUCCESS;\n\t}\n\n\twrp->wr_hi |= htonl(F_WR_SOP | F_WR_EOP);\n\twrp->wr_lo = htonl(V_WR_TID(q->token));\n\n\tspin_lock(&q->lock);\n      again:reclaim_completed_tx_imm(q);\n\n\tret = check_desc_avail(adap, q, skb, 1, TXQ_CTRL);\n\tif (unlikely(ret)) {\n\t\tif (ret == 1) {\n\t\t\tspin_unlock(&q->lock);\n\t\t\treturn NET_XMIT_CN;\n\t\t}\n\t\tgoto again;\n\t}\n\n\twrite_imm(&q->desc[q->pidx], skb, skb->len, q->gen);\n\n\tq->in_use++;\n\tif (++q->pidx >= q->size) {\n\t\tq->pidx = 0;\n\t\tq->gen ^= 1;\n\t}\n\tspin_unlock(&q->lock);\n\twmb();\n\tt3_write_reg(adap, A_SG_KDOORBELL,\n\t\t     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));\n\treturn NET_XMIT_SUCCESS;\n}\n\n \nstatic void restart_ctrlq(struct work_struct *w)\n{\n\tstruct sk_buff *skb;\n\tstruct sge_qset *qs = container_of(w, struct sge_qset,\n\t\t\t\t\t   txq[TXQ_CTRL].qresume_task);\n\tstruct sge_txq *q = &qs->txq[TXQ_CTRL];\n\n\tspin_lock(&q->lock);\n      again:reclaim_completed_tx_imm(q);\n\n\twhile (q->in_use < q->size &&\n\t       (skb = __skb_dequeue(&q->sendq)) != NULL) {\n\n\t\twrite_imm(&q->desc[q->pidx], skb, skb->len, q->gen);\n\n\t\tif (++q->pidx >= q->size) {\n\t\t\tq->pidx = 0;\n\t\t\tq->gen ^= 1;\n\t\t}\n\t\tq->in_use++;\n\t}\n\n\tif (!skb_queue_empty(&q->sendq)) {\n\t\tset_bit(TXQ_CTRL, &qs->txq_stopped);\n\t\tsmp_mb__after_atomic();\n\n\t\tif (should_restart_tx(q) &&\n\t\t    test_and_clear_bit(TXQ_CTRL, &qs->txq_stopped))\n\t\t\tgoto again;\n\t\tq->stops++;\n\t}\n\n\tspin_unlock(&q->lock);\n\twmb();\n\tt3_write_reg(qs->adap, A_SG_KDOORBELL,\n\t\t     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));\n}\n\n \nint t3_mgmt_tx(struct adapter *adap, struct sk_buff *skb)\n{\n\tint ret;\n\tlocal_bh_disable();\n\tret = ctrl_xmit(adap, &adap->sge.qs[0].txq[TXQ_CTRL], skb);\n\tlocal_bh_enable();\n\n\treturn ret;\n}\n\n \nstatic void deferred_unmap_destructor(struct sk_buff *skb)\n{\n\tint i;\n\tconst dma_addr_t *p;\n\tconst struct skb_shared_info *si;\n\tconst struct deferred_unmap_info *dui;\n\n\tdui = (struct deferred_unmap_info *)skb->head;\n\tp = dui->addr;\n\n\tif (skb_tail_pointer(skb) - skb_transport_header(skb))\n\t\tdma_unmap_single(&dui->pdev->dev, *p++,\n\t\t\t\t skb_tail_pointer(skb) - skb_transport_header(skb),\n\t\t\t\t DMA_TO_DEVICE);\n\n\tsi = skb_shinfo(skb);\n\tfor (i = 0; i < si->nr_frags; i++)\n\t\tdma_unmap_page(&dui->pdev->dev, *p++,\n\t\t\t       skb_frag_size(&si->frags[i]), DMA_TO_DEVICE);\n}\n\nstatic void setup_deferred_unmapping(struct sk_buff *skb, struct pci_dev *pdev,\n\t\t\t\t     const struct sg_ent *sgl, int sgl_flits)\n{\n\tdma_addr_t *p;\n\tstruct deferred_unmap_info *dui;\n\n\tdui = (struct deferred_unmap_info *)skb->head;\n\tdui->pdev = pdev;\n\tfor (p = dui->addr; sgl_flits >= 3; sgl++, sgl_flits -= 3) {\n\t\t*p++ = be64_to_cpu(sgl->addr[0]);\n\t\t*p++ = be64_to_cpu(sgl->addr[1]);\n\t}\n\tif (sgl_flits)\n\t\t*p = be64_to_cpu(sgl->addr[0]);\n}\n\n \nstatic void write_ofld_wr(struct adapter *adap, struct sk_buff *skb,\n\t\t\t  struct sge_txq *q, unsigned int pidx,\n\t\t\t  unsigned int gen, unsigned int ndesc,\n\t\t\t  const dma_addr_t *addr)\n{\n\tunsigned int sgl_flits, flits;\n\tstruct work_request_hdr *from;\n\tstruct sg_ent *sgp, sgl[MAX_SKB_FRAGS / 2 + 1];\n\tstruct tx_desc *d = &q->desc[pidx];\n\n\tif (immediate(skb)) {\n\t\tq->sdesc[pidx].skb = NULL;\n\t\twrite_imm(d, skb, skb->len, gen);\n\t\treturn;\n\t}\n\n\t \n\n\tfrom = (struct work_request_hdr *)skb->data;\n\tmemcpy(&d->flit[1], &from[1],\n\t       skb_transport_offset(skb) - sizeof(*from));\n\n\tflits = skb_transport_offset(skb) / 8;\n\tsgp = ndesc == 1 ? (struct sg_ent *)&d->flit[flits] : sgl;\n\tsgl_flits = write_sgl(skb, sgp, skb_transport_header(skb),\n\t\t\t      skb_tail_pointer(skb) - skb_transport_header(skb),\n\t\t\t      addr);\n\tif (need_skb_unmap()) {\n\t\tsetup_deferred_unmapping(skb, adap->pdev, sgp, sgl_flits);\n\t\tskb->destructor = deferred_unmap_destructor;\n\t}\n\n\twrite_wr_hdr_sgl(ndesc, skb, d, pidx, q, sgl, flits, sgl_flits,\n\t\t\t gen, from->wr_hi, from->wr_lo);\n}\n\n \nstatic inline unsigned int calc_tx_descs_ofld(const struct sk_buff *skb)\n{\n\tunsigned int flits, cnt;\n\n\tif (skb->len <= WR_LEN)\n\t\treturn 1;\t \n\n\tflits = skb_transport_offset(skb) / 8;\t \n\tcnt = skb_shinfo(skb)->nr_frags;\n\tif (skb_tail_pointer(skb) != skb_transport_header(skb))\n\t\tcnt++;\n\treturn flits_to_desc(flits + sgl_len(cnt));\n}\n\n \nstatic int ofld_xmit(struct adapter *adap, struct sge_txq *q,\n\t\t     struct sk_buff *skb)\n{\n\tint ret;\n\tunsigned int ndesc = calc_tx_descs_ofld(skb), pidx, gen;\n\n\tspin_lock(&q->lock);\nagain:\treclaim_completed_tx(adap, q, TX_RECLAIM_CHUNK);\n\n\tret = check_desc_avail(adap, q, skb, ndesc, TXQ_OFLD);\n\tif (unlikely(ret)) {\n\t\tif (ret == 1) {\n\t\t\tskb->priority = ndesc;\t \n\t\t\tspin_unlock(&q->lock);\n\t\t\treturn NET_XMIT_CN;\n\t\t}\n\t\tgoto again;\n\t}\n\n\tif (!immediate(skb) &&\n\t    map_skb(adap->pdev, skb, (dma_addr_t *)skb->head)) {\n\t\tspin_unlock(&q->lock);\n\t\treturn NET_XMIT_SUCCESS;\n\t}\n\n\tgen = q->gen;\n\tq->in_use += ndesc;\n\tpidx = q->pidx;\n\tq->pidx += ndesc;\n\tif (q->pidx >= q->size) {\n\t\tq->pidx -= q->size;\n\t\tq->gen ^= 1;\n\t}\n\tspin_unlock(&q->lock);\n\n\twrite_ofld_wr(adap, skb, q, pidx, gen, ndesc, (dma_addr_t *)skb->head);\n\tcheck_ring_tx_db(adap, q);\n\treturn NET_XMIT_SUCCESS;\n}\n\n \nstatic void restart_offloadq(struct work_struct *w)\n{\n\tstruct sk_buff *skb;\n\tstruct sge_qset *qs = container_of(w, struct sge_qset,\n\t\t\t\t\t   txq[TXQ_OFLD].qresume_task);\n\tstruct sge_txq *q = &qs->txq[TXQ_OFLD];\n\tconst struct port_info *pi = netdev_priv(qs->netdev);\n\tstruct adapter *adap = pi->adapter;\n\tunsigned int written = 0;\n\n\tspin_lock(&q->lock);\nagain:\treclaim_completed_tx(adap, q, TX_RECLAIM_CHUNK);\n\n\twhile ((skb = skb_peek(&q->sendq)) != NULL) {\n\t\tunsigned int gen, pidx;\n\t\tunsigned int ndesc = skb->priority;\n\n\t\tif (unlikely(q->size - q->in_use < ndesc)) {\n\t\t\tset_bit(TXQ_OFLD, &qs->txq_stopped);\n\t\t\tsmp_mb__after_atomic();\n\n\t\t\tif (should_restart_tx(q) &&\n\t\t\t    test_and_clear_bit(TXQ_OFLD, &qs->txq_stopped))\n\t\t\t\tgoto again;\n\t\t\tq->stops++;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!immediate(skb) &&\n\t\t    map_skb(adap->pdev, skb, (dma_addr_t *)skb->head))\n\t\t\tbreak;\n\n\t\tgen = q->gen;\n\t\tq->in_use += ndesc;\n\t\tpidx = q->pidx;\n\t\tq->pidx += ndesc;\n\t\twritten += ndesc;\n\t\tif (q->pidx >= q->size) {\n\t\t\tq->pidx -= q->size;\n\t\t\tq->gen ^= 1;\n\t\t}\n\t\t__skb_unlink(skb, &q->sendq);\n\t\tspin_unlock(&q->lock);\n\n\t\twrite_ofld_wr(adap, skb, q, pidx, gen, ndesc,\n\t\t\t      (dma_addr_t *)skb->head);\n\t\tspin_lock(&q->lock);\n\t}\n\tspin_unlock(&q->lock);\n\n#if USE_GTS\n\tset_bit(TXQ_RUNNING, &q->flags);\n\tset_bit(TXQ_LAST_PKT_DB, &q->flags);\n#endif\n\twmb();\n\tif (likely(written))\n\t\tt3_write_reg(adap, A_SG_KDOORBELL,\n\t\t\t     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));\n}\n\n \nstatic inline int queue_set(const struct sk_buff *skb)\n{\n\treturn skb->priority >> 1;\n}\n\n \nstatic inline int is_ctrl_pkt(const struct sk_buff *skb)\n{\n\treturn skb->priority & 1;\n}\n\n \nint t3_offload_tx(struct t3cdev *tdev, struct sk_buff *skb)\n{\n\tstruct adapter *adap = tdev2adap(tdev);\n\tstruct sge_qset *qs = &adap->sge.qs[queue_set(skb)];\n\n\tif (unlikely(is_ctrl_pkt(skb)))\n\t\treturn ctrl_xmit(adap, &qs->txq[TXQ_CTRL], skb);\n\n\treturn ofld_xmit(adap, &qs->txq[TXQ_OFLD], skb);\n}\n\n \nstatic inline void offload_enqueue(struct sge_rspq *q, struct sk_buff *skb)\n{\n\tint was_empty = skb_queue_empty(&q->rx_queue);\n\n\t__skb_queue_tail(&q->rx_queue, skb);\n\n\tif (was_empty) {\n\t\tstruct sge_qset *qs = rspq_to_qset(q);\n\n\t\tnapi_schedule(&qs->napi);\n\t}\n}\n\n \nstatic inline void deliver_partial_bundle(struct t3cdev *tdev,\n\t\t\t\t\t  struct sge_rspq *q,\n\t\t\t\t\t  struct sk_buff *skbs[], int n)\n{\n\tif (n) {\n\t\tq->offload_bundles++;\n\t\ttdev->recv(tdev, skbs, n);\n\t}\n}\n\n \nstatic int ofld_poll(struct napi_struct *napi, int budget)\n{\n\tstruct sge_qset *qs = container_of(napi, struct sge_qset, napi);\n\tstruct sge_rspq *q = &qs->rspq;\n\tstruct adapter *adapter = qs->adap;\n\tint work_done = 0;\n\n\twhile (work_done < budget) {\n\t\tstruct sk_buff *skb, *tmp, *skbs[RX_BUNDLE_SIZE];\n\t\tstruct sk_buff_head queue;\n\t\tint ngathered;\n\n\t\tspin_lock_irq(&q->lock);\n\t\t__skb_queue_head_init(&queue);\n\t\tskb_queue_splice_init(&q->rx_queue, &queue);\n\t\tif (skb_queue_empty(&queue)) {\n\t\t\tnapi_complete_done(napi, work_done);\n\t\t\tspin_unlock_irq(&q->lock);\n\t\t\treturn work_done;\n\t\t}\n\t\tspin_unlock_irq(&q->lock);\n\n\t\tngathered = 0;\n\t\tskb_queue_walk_safe(&queue, skb, tmp) {\n\t\t\tif (work_done >= budget)\n\t\t\t\tbreak;\n\t\t\twork_done++;\n\n\t\t\t__skb_unlink(skb, &queue);\n\t\t\tprefetch(skb->data);\n\t\t\tskbs[ngathered] = skb;\n\t\t\tif (++ngathered == RX_BUNDLE_SIZE) {\n\t\t\t\tq->offload_bundles++;\n\t\t\t\tadapter->tdev.recv(&adapter->tdev, skbs,\n\t\t\t\t\t\t   ngathered);\n\t\t\t\tngathered = 0;\n\t\t\t}\n\t\t}\n\t\tif (!skb_queue_empty(&queue)) {\n\t\t\t \n\t\t\tspin_lock_irq(&q->lock);\n\t\t\tskb_queue_splice(&queue, &q->rx_queue);\n\t\t\tspin_unlock_irq(&q->lock);\n\t\t}\n\t\tdeliver_partial_bundle(&adapter->tdev, q, skbs, ngathered);\n\t}\n\n\treturn work_done;\n}\n\n \nstatic inline int rx_offload(struct t3cdev *tdev, struct sge_rspq *rq,\n\t\t\t     struct sk_buff *skb, struct sk_buff *rx_gather[],\n\t\t\t     unsigned int gather_idx)\n{\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\tskb_reset_transport_header(skb);\n\n\tif (rq->polling) {\n\t\trx_gather[gather_idx++] = skb;\n\t\tif (gather_idx == RX_BUNDLE_SIZE) {\n\t\t\ttdev->recv(tdev, rx_gather, RX_BUNDLE_SIZE);\n\t\t\tgather_idx = 0;\n\t\t\trq->offload_bundles++;\n\t\t}\n\t} else\n\t\toffload_enqueue(rq, skb);\n\n\treturn gather_idx;\n}\n\n \nstatic void restart_tx(struct sge_qset *qs)\n{\n\tif (test_bit(TXQ_ETH, &qs->txq_stopped) &&\n\t    should_restart_tx(&qs->txq[TXQ_ETH]) &&\n\t    test_and_clear_bit(TXQ_ETH, &qs->txq_stopped)) {\n\t\tqs->txq[TXQ_ETH].restarts++;\n\t\tif (netif_running(qs->netdev))\n\t\t\tnetif_tx_wake_queue(qs->tx_q);\n\t}\n\n\tif (test_bit(TXQ_OFLD, &qs->txq_stopped) &&\n\t    should_restart_tx(&qs->txq[TXQ_OFLD]) &&\n\t    test_and_clear_bit(TXQ_OFLD, &qs->txq_stopped)) {\n\t\tqs->txq[TXQ_OFLD].restarts++;\n\n\t\t \n\t\tqueue_work(cxgb3_wq, &qs->txq[TXQ_OFLD].qresume_task);\n\t}\n\tif (test_bit(TXQ_CTRL, &qs->txq_stopped) &&\n\t    should_restart_tx(&qs->txq[TXQ_CTRL]) &&\n\t    test_and_clear_bit(TXQ_CTRL, &qs->txq_stopped)) {\n\t\tqs->txq[TXQ_CTRL].restarts++;\n\n\t\t \n\t\tqueue_work(cxgb3_wq, &qs->txq[TXQ_CTRL].qresume_task);\n\t}\n}\n\n \nstatic void cxgb3_arp_process(struct port_info *pi, struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct arphdr *arp;\n\tunsigned char *arp_ptr;\n\tunsigned char *sha;\n\t__be32 sip, tip;\n\n\tif (!dev)\n\t\treturn;\n\n\tskb_reset_network_header(skb);\n\tarp = arp_hdr(skb);\n\n\tif (arp->ar_op != htons(ARPOP_REQUEST))\n\t\treturn;\n\n\tarp_ptr = (unsigned char *)(arp + 1);\n\tsha = arp_ptr;\n\tarp_ptr += dev->addr_len;\n\tmemcpy(&sip, arp_ptr, sizeof(sip));\n\tarp_ptr += sizeof(sip);\n\tarp_ptr += dev->addr_len;\n\tmemcpy(&tip, arp_ptr, sizeof(tip));\n\n\tif (tip != pi->iscsi_ipv4addr)\n\t\treturn;\n\n\tarp_send(ARPOP_REPLY, ETH_P_ARP, sip, dev, tip, sha,\n\t\t pi->iscsic.mac_addr, sha);\n\n}\n\nstatic inline int is_arp(struct sk_buff *skb)\n{\n\treturn skb->protocol == htons(ETH_P_ARP);\n}\n\nstatic void cxgb3_process_iscsi_prov_pack(struct port_info *pi,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tif (is_arp(skb)) {\n\t\tcxgb3_arp_process(pi, skb);\n\t\treturn;\n\t}\n\n\tif (pi->iscsic.recv)\n\t\tpi->iscsic.recv(pi, skb);\n\n}\n\n \nstatic void rx_eth(struct adapter *adap, struct sge_rspq *rq,\n\t\t   struct sk_buff *skb, int pad, int lro)\n{\n\tstruct cpl_rx_pkt *p = (struct cpl_rx_pkt *)(skb->data + pad);\n\tstruct sge_qset *qs = rspq_to_qset(rq);\n\tstruct port_info *pi;\n\n\tskb_pull(skb, sizeof(*p) + pad);\n\tskb->protocol = eth_type_trans(skb, adap->port[p->iff]);\n\tpi = netdev_priv(skb->dev);\n\tif ((skb->dev->features & NETIF_F_RXCSUM) && p->csum_valid &&\n\t    p->csum == htons(0xffff) && !p->fragment) {\n\t\tqs->port_stats[SGE_PSTAT_RX_CSUM_GOOD]++;\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t} else\n\t\tskb_checksum_none_assert(skb);\n\tskb_record_rx_queue(skb, qs - &adap->sge.qs[pi->first_qset]);\n\n\tif (p->vlan_valid) {\n\t\tqs->port_stats[SGE_PSTAT_VLANEX]++;\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ntohs(p->vlan));\n\t}\n\tif (rq->polling) {\n\t\tif (lro)\n\t\t\tnapi_gro_receive(&qs->napi, skb);\n\t\telse {\n\t\t\tif (unlikely(pi->iscsic.flags))\n\t\t\t\tcxgb3_process_iscsi_prov_pack(pi, skb);\n\t\t\tnetif_receive_skb(skb);\n\t\t}\n\t} else\n\t\tnetif_rx(skb);\n}\n\nstatic inline int is_eth_tcp(u32 rss)\n{\n\treturn G_HASHTYPE(ntohl(rss)) == RSS_HASH_4_TUPLE;\n}\n\n \nstatic void lro_add_page(struct adapter *adap, struct sge_qset *qs,\n\t\t\t struct sge_fl *fl, int len, int complete)\n{\n\tstruct rx_sw_desc *sd = &fl->sdesc[fl->cidx];\n\tstruct port_info *pi = netdev_priv(qs->netdev);\n\tstruct sk_buff *skb = NULL;\n\tstruct cpl_rx_pkt *cpl;\n\tskb_frag_t *rx_frag;\n\tint nr_frags;\n\tint offset = 0;\n\n\tif (!qs->nomem) {\n\t\tskb = napi_get_frags(&qs->napi);\n\t\tqs->nomem = !skb;\n\t}\n\n\tfl->credits--;\n\n\tdma_sync_single_for_cpu(&adap->pdev->dev,\n\t\t\t\tdma_unmap_addr(sd, dma_addr),\n\t\t\t\tfl->buf_size - SGE_PG_RSVD, DMA_FROM_DEVICE);\n\n\t(*sd->pg_chunk.p_cnt)--;\n\tif (!*sd->pg_chunk.p_cnt && sd->pg_chunk.page != fl->pg_chunk.page)\n\t\tdma_unmap_page(&adap->pdev->dev, sd->pg_chunk.mapping,\n\t\t\t       fl->alloc_size, DMA_FROM_DEVICE);\n\n\tif (!skb) {\n\t\tput_page(sd->pg_chunk.page);\n\t\tif (complete)\n\t\t\tqs->nomem = 0;\n\t\treturn;\n\t}\n\n\trx_frag = skb_shinfo(skb)->frags;\n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\tif (!nr_frags) {\n\t\toffset = 2 + sizeof(struct cpl_rx_pkt);\n\t\tcpl = qs->lro_va = sd->pg_chunk.va + 2;\n\n\t\tif ((qs->netdev->features & NETIF_F_RXCSUM) &&\n\t\t     cpl->csum_valid && cpl->csum == htons(0xffff)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tqs->port_stats[SGE_PSTAT_RX_CSUM_GOOD]++;\n\t\t} else\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t} else\n\t\tcpl = qs->lro_va;\n\n\tlen -= offset;\n\n\trx_frag += nr_frags;\n\tskb_frag_fill_page_desc(rx_frag, sd->pg_chunk.page,\n\t\t\t\tsd->pg_chunk.offset + offset, len);\n\n\tskb->len += len;\n\tskb->data_len += len;\n\tskb->truesize += len;\n\tskb_shinfo(skb)->nr_frags++;\n\n\tif (!complete)\n\t\treturn;\n\n\tskb_record_rx_queue(skb, qs - &adap->sge.qs[pi->first_qset]);\n\n\tif (cpl->vlan_valid) {\n\t\tqs->port_stats[SGE_PSTAT_VLANEX]++;\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ntohs(cpl->vlan));\n\t}\n\tnapi_gro_frags(&qs->napi);\n}\n\n \nstatic inline void handle_rsp_cntrl_info(struct sge_qset *qs, u32 flags)\n{\n\tunsigned int credits;\n\n#if USE_GTS\n\tif (flags & F_RSPD_TXQ0_GTS)\n\t\tclear_bit(TXQ_RUNNING, &qs->txq[TXQ_ETH].flags);\n#endif\n\n\tcredits = G_RSPD_TXQ0_CR(flags);\n\tif (credits)\n\t\tqs->txq[TXQ_ETH].processed += credits;\n\n\tcredits = G_RSPD_TXQ2_CR(flags);\n\tif (credits)\n\t\tqs->txq[TXQ_CTRL].processed += credits;\n\n# if USE_GTS\n\tif (flags & F_RSPD_TXQ1_GTS)\n\t\tclear_bit(TXQ_RUNNING, &qs->txq[TXQ_OFLD].flags);\n# endif\n\tcredits = G_RSPD_TXQ1_CR(flags);\n\tif (credits)\n\t\tqs->txq[TXQ_OFLD].processed += credits;\n}\n\n \nstatic void check_ring_db(struct adapter *adap, struct sge_qset *qs,\n\t\t\t  unsigned int sleeping)\n{\n\tif (sleeping & F_RSPD_TXQ0_GTS) {\n\t\tstruct sge_txq *txq = &qs->txq[TXQ_ETH];\n\n\t\tif (txq->cleaned + txq->in_use != txq->processed &&\n\t\t    !test_and_set_bit(TXQ_LAST_PKT_DB, &txq->flags)) {\n\t\t\tset_bit(TXQ_RUNNING, &txq->flags);\n\t\t\tt3_write_reg(adap, A_SG_KDOORBELL, F_SELEGRCNTX |\n\t\t\t\t     V_EGRCNTX(txq->cntxt_id));\n\t\t}\n\t}\n\n\tif (sleeping & F_RSPD_TXQ1_GTS) {\n\t\tstruct sge_txq *txq = &qs->txq[TXQ_OFLD];\n\n\t\tif (txq->cleaned + txq->in_use != txq->processed &&\n\t\t    !test_and_set_bit(TXQ_LAST_PKT_DB, &txq->flags)) {\n\t\t\tset_bit(TXQ_RUNNING, &txq->flags);\n\t\t\tt3_write_reg(adap, A_SG_KDOORBELL, F_SELEGRCNTX |\n\t\t\t\t     V_EGRCNTX(txq->cntxt_id));\n\t\t}\n\t}\n}\n\n \nstatic inline int is_new_response(const struct rsp_desc *r,\n\t\t\t\t  const struct sge_rspq *q)\n{\n\treturn (r->intr_gen & F_RSPD_GEN2) == q->gen;\n}\n\nstatic inline void clear_rspq_bufstate(struct sge_rspq * const q)\n{\n\tq->pg_skb = NULL;\n\tq->rx_recycle_buf = 0;\n}\n\n#define RSPD_GTS_MASK  (F_RSPD_TXQ0_GTS | F_RSPD_TXQ1_GTS)\n#define RSPD_CTRL_MASK (RSPD_GTS_MASK | \\\n\t\t\tV_RSPD_TXQ0_CR(M_RSPD_TXQ0_CR) | \\\n\t\t\tV_RSPD_TXQ1_CR(M_RSPD_TXQ1_CR) | \\\n\t\t\tV_RSPD_TXQ2_CR(M_RSPD_TXQ2_CR))\n\n \n#define NOMEM_INTR_DELAY 2500\n\n \nstatic int process_responses(struct adapter *adap, struct sge_qset *qs,\n\t\t\t     int budget)\n{\n\tstruct sge_rspq *q = &qs->rspq;\n\tstruct rsp_desc *r = &q->desc[q->cidx];\n\tint budget_left = budget;\n\tunsigned int sleeping = 0;\n\tstruct sk_buff *offload_skbs[RX_BUNDLE_SIZE];\n\tint ngathered = 0;\n\n\tq->next_holdoff = q->holdoff_tmr;\n\n\twhile (likely(budget_left && is_new_response(r, q))) {\n\t\tint packet_complete, eth, ethpad = 2;\n\t\tint lro = !!(qs->netdev->features & NETIF_F_GRO);\n\t\tstruct sk_buff *skb = NULL;\n\t\tu32 len, flags;\n\t\t__be32 rss_hi, rss_lo;\n\n\t\tdma_rmb();\n\t\teth = r->rss_hdr.opcode == CPL_RX_PKT;\n\t\trss_hi = *(const __be32 *)r;\n\t\trss_lo = r->rss_hdr.rss_hash_val;\n\t\tflags = ntohl(r->flags);\n\n\t\tif (unlikely(flags & F_RSPD_ASYNC_NOTIF)) {\n\t\t\tskb = alloc_skb(AN_PKT_SIZE, GFP_ATOMIC);\n\t\t\tif (!skb)\n\t\t\t\tgoto no_mem;\n\n\t\t\t__skb_put_data(skb, r, AN_PKT_SIZE);\n\t\t\tskb->data[0] = CPL_ASYNC_NOTIF;\n\t\t\trss_hi = htonl(CPL_ASYNC_NOTIF << 24);\n\t\t\tq->async_notif++;\n\t\t} else if (flags & F_RSPD_IMM_DATA_VALID) {\n\t\t\tskb = get_imm_packet(r);\n\t\t\tif (unlikely(!skb)) {\nno_mem:\n\t\t\t\tq->next_holdoff = NOMEM_INTR_DELAY;\n\t\t\t\tq->nomem++;\n\t\t\t\t \n\t\t\t\tbudget_left--;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tq->imm_data++;\n\t\t\tethpad = 0;\n\t\t} else if ((len = ntohl(r->len_cq)) != 0) {\n\t\t\tstruct sge_fl *fl;\n\n\t\t\tlro &= eth && is_eth_tcp(rss_hi);\n\n\t\t\tfl = (len & F_RSPD_FLQ) ? &qs->fl[1] : &qs->fl[0];\n\t\t\tif (fl->use_pages) {\n\t\t\t\tvoid *addr = fl->sdesc[fl->cidx].pg_chunk.va;\n\n\t\t\t\tnet_prefetch(addr);\n\t\t\t\t__refill_fl(adap, fl);\n\t\t\t\tif (lro > 0) {\n\t\t\t\t\tlro_add_page(adap, qs, fl,\n\t\t\t\t\t\t     G_RSPD_LEN(len),\n\t\t\t\t\t\t     flags & F_RSPD_EOP);\n\t\t\t\t\tgoto next_fl;\n\t\t\t\t}\n\n\t\t\t\tskb = get_packet_pg(adap, fl, q,\n\t\t\t\t\t\t    G_RSPD_LEN(len),\n\t\t\t\t\t\t    eth ?\n\t\t\t\t\t\t    SGE_RX_DROP_THRES : 0);\n\t\t\t\tq->pg_skb = skb;\n\t\t\t} else\n\t\t\t\tskb = get_packet(adap, fl, G_RSPD_LEN(len),\n\t\t\t\t\t\t eth ? SGE_RX_DROP_THRES : 0);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\tif (!eth)\n\t\t\t\t\tgoto no_mem;\n\t\t\t\tq->rx_drops++;\n\t\t\t} else if (unlikely(r->rss_hdr.opcode == CPL_TRACE_PKT))\n\t\t\t\t__skb_pull(skb, 2);\nnext_fl:\n\t\t\tif (++fl->cidx == fl->size)\n\t\t\t\tfl->cidx = 0;\n\t\t} else\n\t\t\tq->pure_rsps++;\n\n\t\tif (flags & RSPD_CTRL_MASK) {\n\t\t\tsleeping |= flags & RSPD_GTS_MASK;\n\t\t\thandle_rsp_cntrl_info(qs, flags);\n\t\t}\n\n\t\tr++;\n\t\tif (unlikely(++q->cidx == q->size)) {\n\t\t\tq->cidx = 0;\n\t\t\tq->gen ^= 1;\n\t\t\tr = q->desc;\n\t\t}\n\t\tprefetch(r);\n\n\t\tif (++q->credits >= (q->size / 4)) {\n\t\t\trefill_rspq(adap, q, q->credits);\n\t\t\tq->credits = 0;\n\t\t}\n\n\t\tpacket_complete = flags &\n\t\t\t\t  (F_RSPD_EOP | F_RSPD_IMM_DATA_VALID |\n\t\t\t\t   F_RSPD_ASYNC_NOTIF);\n\n\t\tif (skb != NULL && packet_complete) {\n\t\t\tif (eth)\n\t\t\t\trx_eth(adap, q, skb, ethpad, lro);\n\t\t\telse {\n\t\t\t\tq->offload_pkts++;\n\t\t\t\t \n\t\t\t\tskb->csum = rss_hi;\n\t\t\t\tskb->priority = rss_lo;\n\t\t\t\tngathered = rx_offload(&adap->tdev, q, skb,\n\t\t\t\t\t\t       offload_skbs,\n\t\t\t\t\t\t       ngathered);\n\t\t\t}\n\n\t\t\tif (flags & F_RSPD_EOP)\n\t\t\t\tclear_rspq_bufstate(q);\n\t\t}\n\t\t--budget_left;\n\t}\n\n\tdeliver_partial_bundle(&adap->tdev, q, offload_skbs, ngathered);\n\n\tif (sleeping)\n\t\tcheck_ring_db(adap, qs, sleeping);\n\n\tsmp_mb();\t\t \n\tif (unlikely(qs->txq_stopped != 0))\n\t\trestart_tx(qs);\n\n\tbudget -= budget_left;\n\treturn budget;\n}\n\nstatic inline int is_pure_response(const struct rsp_desc *r)\n{\n\t__be32 n = r->flags & htonl(F_RSPD_ASYNC_NOTIF | F_RSPD_IMM_DATA_VALID);\n\n\treturn (n | r->len_cq) == 0;\n}\n\n \nstatic int napi_rx_handler(struct napi_struct *napi, int budget)\n{\n\tstruct sge_qset *qs = container_of(napi, struct sge_qset, napi);\n\tstruct adapter *adap = qs->adap;\n\tint work_done = process_responses(adap, qs, budget);\n\n\tif (likely(work_done < budget)) {\n\t\tnapi_complete_done(napi, work_done);\n\n\t\t \n\t\tt3_write_reg(adap, A_SG_GTS, V_RSPQ(qs->rspq.cntxt_id) |\n\t\t\t     V_NEWTIMER(qs->rspq.next_holdoff) |\n\t\t\t     V_NEWINDEX(qs->rspq.cidx));\n\t}\n\treturn work_done;\n}\n\n \nstatic inline int napi_is_scheduled(struct napi_struct *napi)\n{\n\treturn test_bit(NAPI_STATE_SCHED, &napi->state);\n}\n\n \nstatic int process_pure_responses(struct adapter *adap, struct sge_qset *qs,\n\t\t\t\t  struct rsp_desc *r)\n{\n\tstruct sge_rspq *q = &qs->rspq;\n\tunsigned int sleeping = 0;\n\n\tdo {\n\t\tu32 flags = ntohl(r->flags);\n\n\t\tr++;\n\t\tif (unlikely(++q->cidx == q->size)) {\n\t\t\tq->cidx = 0;\n\t\t\tq->gen ^= 1;\n\t\t\tr = q->desc;\n\t\t}\n\t\tprefetch(r);\n\n\t\tif (flags & RSPD_CTRL_MASK) {\n\t\t\tsleeping |= flags & RSPD_GTS_MASK;\n\t\t\thandle_rsp_cntrl_info(qs, flags);\n\t\t}\n\n\t\tq->pure_rsps++;\n\t\tif (++q->credits >= (q->size / 4)) {\n\t\t\trefill_rspq(adap, q, q->credits);\n\t\t\tq->credits = 0;\n\t\t}\n\t\tif (!is_new_response(r, q))\n\t\t\tbreak;\n\t\tdma_rmb();\n\t} while (is_pure_response(r));\n\n\tif (sleeping)\n\t\tcheck_ring_db(adap, qs, sleeping);\n\n\tsmp_mb();\t\t \n\tif (unlikely(qs->txq_stopped != 0))\n\t\trestart_tx(qs);\n\n\treturn is_new_response(r, q);\n}\n\n \nstatic inline int handle_responses(struct adapter *adap, struct sge_rspq *q)\n{\n\tstruct sge_qset *qs = rspq_to_qset(q);\n\tstruct rsp_desc *r = &q->desc[q->cidx];\n\n\tif (!is_new_response(r, q))\n\t\treturn -1;\n\tdma_rmb();\n\tif (is_pure_response(r) && process_pure_responses(adap, qs, r) == 0) {\n\t\tt3_write_reg(adap, A_SG_GTS, V_RSPQ(q->cntxt_id) |\n\t\t\t     V_NEWTIMER(q->holdoff_tmr) | V_NEWINDEX(q->cidx));\n\t\treturn 0;\n\t}\n\tnapi_schedule(&qs->napi);\n\treturn 1;\n}\n\n \nstatic irqreturn_t t3_sge_intr_msix(int irq, void *cookie)\n{\n\tstruct sge_qset *qs = cookie;\n\tstruct adapter *adap = qs->adap;\n\tstruct sge_rspq *q = &qs->rspq;\n\n\tspin_lock(&q->lock);\n\tif (process_responses(adap, qs, -1) == 0)\n\t\tq->unhandled_irqs++;\n\tt3_write_reg(adap, A_SG_GTS, V_RSPQ(q->cntxt_id) |\n\t\t     V_NEWTIMER(q->next_holdoff) | V_NEWINDEX(q->cidx));\n\tspin_unlock(&q->lock);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t t3_sge_intr_msix_napi(int irq, void *cookie)\n{\n\tstruct sge_qset *qs = cookie;\n\tstruct sge_rspq *q = &qs->rspq;\n\n\tspin_lock(&q->lock);\n\n\tif (handle_responses(qs->adap, q) < 0)\n\t\tq->unhandled_irqs++;\n\tspin_unlock(&q->lock);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t t3_intr_msi(int irq, void *cookie)\n{\n\tint new_packets = 0;\n\tstruct adapter *adap = cookie;\n\tstruct sge_rspq *q = &adap->sge.qs[0].rspq;\n\n\tspin_lock(&q->lock);\n\n\tif (process_responses(adap, &adap->sge.qs[0], -1)) {\n\t\tt3_write_reg(adap, A_SG_GTS, V_RSPQ(q->cntxt_id) |\n\t\t\t     V_NEWTIMER(q->next_holdoff) | V_NEWINDEX(q->cidx));\n\t\tnew_packets = 1;\n\t}\n\n\tif (adap->params.nports == 2 &&\n\t    process_responses(adap, &adap->sge.qs[1], -1)) {\n\t\tstruct sge_rspq *q1 = &adap->sge.qs[1].rspq;\n\n\t\tt3_write_reg(adap, A_SG_GTS, V_RSPQ(q1->cntxt_id) |\n\t\t\t     V_NEWTIMER(q1->next_holdoff) |\n\t\t\t     V_NEWINDEX(q1->cidx));\n\t\tnew_packets = 1;\n\t}\n\n\tif (!new_packets && t3_slow_intr_handler(adap) == 0)\n\t\tq->unhandled_irqs++;\n\n\tspin_unlock(&q->lock);\n\treturn IRQ_HANDLED;\n}\n\nstatic int rspq_check_napi(struct sge_qset *qs)\n{\n\tstruct sge_rspq *q = &qs->rspq;\n\n\tif (!napi_is_scheduled(&qs->napi) &&\n\t    is_new_response(&q->desc[q->cidx], q)) {\n\t\tnapi_schedule(&qs->napi);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n \nstatic irqreturn_t t3_intr_msi_napi(int irq, void *cookie)\n{\n\tint new_packets;\n\tstruct adapter *adap = cookie;\n\tstruct sge_rspq *q = &adap->sge.qs[0].rspq;\n\n\tspin_lock(&q->lock);\n\n\tnew_packets = rspq_check_napi(&adap->sge.qs[0]);\n\tif (adap->params.nports == 2)\n\t\tnew_packets += rspq_check_napi(&adap->sge.qs[1]);\n\tif (!new_packets && t3_slow_intr_handler(adap) == 0)\n\t\tq->unhandled_irqs++;\n\n\tspin_unlock(&q->lock);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic inline int process_responses_gts(struct adapter *adap,\n\t\t\t\t\tstruct sge_rspq *rq)\n{\n\tint work;\n\n\twork = process_responses(adap, rspq_to_qset(rq), -1);\n\tt3_write_reg(adap, A_SG_GTS, V_RSPQ(rq->cntxt_id) |\n\t\t     V_NEWTIMER(rq->next_holdoff) | V_NEWINDEX(rq->cidx));\n\treturn work;\n}\n\n \nstatic irqreturn_t t3_intr(int irq, void *cookie)\n{\n\tint work_done, w0, w1;\n\tstruct adapter *adap = cookie;\n\tstruct sge_rspq *q0 = &adap->sge.qs[0].rspq;\n\tstruct sge_rspq *q1 = &adap->sge.qs[1].rspq;\n\n\tspin_lock(&q0->lock);\n\n\tw0 = is_new_response(&q0->desc[q0->cidx], q0);\n\tw1 = adap->params.nports == 2 &&\n\t    is_new_response(&q1->desc[q1->cidx], q1);\n\n\tif (likely(w0 | w1)) {\n\t\tt3_write_reg(adap, A_PL_CLI, 0);\n\t\tt3_read_reg(adap, A_PL_CLI);\t \n\n\t\tif (likely(w0))\n\t\t\tprocess_responses_gts(adap, q0);\n\n\t\tif (w1)\n\t\t\tprocess_responses_gts(adap, q1);\n\n\t\twork_done = w0 | w1;\n\t} else\n\t\twork_done = t3_slow_intr_handler(adap);\n\n\tspin_unlock(&q0->lock);\n\treturn IRQ_RETVAL(work_done != 0);\n}\n\n \nstatic irqreturn_t t3b_intr(int irq, void *cookie)\n{\n\tu32 map;\n\tstruct adapter *adap = cookie;\n\tstruct sge_rspq *q0 = &adap->sge.qs[0].rspq;\n\n\tt3_write_reg(adap, A_PL_CLI, 0);\n\tmap = t3_read_reg(adap, A_SG_DATA_INTR);\n\n\tif (unlikely(!map))\t \n\t\treturn IRQ_NONE;\n\n\tspin_lock(&q0->lock);\n\n\tif (unlikely(map & F_ERRINTR))\n\t\tt3_slow_intr_handler(adap);\n\n\tif (likely(map & 1))\n\t\tprocess_responses_gts(adap, q0);\n\n\tif (map & 2)\n\t\tprocess_responses_gts(adap, &adap->sge.qs[1].rspq);\n\n\tspin_unlock(&q0->lock);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t t3b_intr_napi(int irq, void *cookie)\n{\n\tu32 map;\n\tstruct adapter *adap = cookie;\n\tstruct sge_qset *qs0 = &adap->sge.qs[0];\n\tstruct sge_rspq *q0 = &qs0->rspq;\n\n\tt3_write_reg(adap, A_PL_CLI, 0);\n\tmap = t3_read_reg(adap, A_SG_DATA_INTR);\n\n\tif (unlikely(!map))\t \n\t\treturn IRQ_NONE;\n\n\tspin_lock(&q0->lock);\n\n\tif (unlikely(map & F_ERRINTR))\n\t\tt3_slow_intr_handler(adap);\n\n\tif (likely(map & 1))\n\t\tnapi_schedule(&qs0->napi);\n\n\tif (map & 2)\n\t\tnapi_schedule(&adap->sge.qs[1].napi);\n\n\tspin_unlock(&q0->lock);\n\treturn IRQ_HANDLED;\n}\n\n \nirq_handler_t t3_intr_handler(struct adapter *adap, int polling)\n{\n\tif (adap->flags & USING_MSIX)\n\t\treturn polling ? t3_sge_intr_msix_napi : t3_sge_intr_msix;\n\tif (adap->flags & USING_MSI)\n\t\treturn polling ? t3_intr_msi_napi : t3_intr_msi;\n\tif (adap->params.rev > 0)\n\t\treturn polling ? t3b_intr_napi : t3b_intr;\n\treturn t3_intr;\n}\n\n#define SGE_PARERR (F_CPPARITYERROR | F_OCPARITYERROR | F_RCPARITYERROR | \\\n\t\t    F_IRPARITYERROR | V_ITPARITYERROR(M_ITPARITYERROR) | \\\n\t\t    V_FLPARITYERROR(M_FLPARITYERROR) | F_LODRBPARITYERROR | \\\n\t\t    F_HIDRBPARITYERROR | F_LORCQPARITYERROR | \\\n\t\t    F_HIRCQPARITYERROR)\n#define SGE_FRAMINGERR (F_UC_REQ_FRAMINGERROR | F_R_REQ_FRAMINGERROR)\n#define SGE_FATALERR (SGE_PARERR | SGE_FRAMINGERR | F_RSPQCREDITOVERFOW | \\\n\t\t      F_RSPQDISABLED)\n\n \nvoid t3_sge_err_intr_handler(struct adapter *adapter)\n{\n\tunsigned int v, status = t3_read_reg(adapter, A_SG_INT_CAUSE) &\n\t\t\t\t ~F_FLEMPTY;\n\n\tif (status & SGE_PARERR)\n\t\tCH_ALERT(adapter, \"SGE parity error (0x%x)\\n\",\n\t\t\t status & SGE_PARERR);\n\tif (status & SGE_FRAMINGERR)\n\t\tCH_ALERT(adapter, \"SGE framing error (0x%x)\\n\",\n\t\t\t status & SGE_FRAMINGERR);\n\n\tif (status & F_RSPQCREDITOVERFOW)\n\t\tCH_ALERT(adapter, \"SGE response queue credit overflow\\n\");\n\n\tif (status & F_RSPQDISABLED) {\n\t\tv = t3_read_reg(adapter, A_SG_RSPQ_FL_STATUS);\n\n\t\tCH_ALERT(adapter,\n\t\t\t \"packet delivered to disabled response queue \"\n\t\t\t \"(0x%x)\\n\", (v >> S_RSPQ0DISABLED) & 0xff);\n\t}\n\n\tif (status & (F_HIPIODRBDROPERR | F_LOPIODRBDROPERR))\n\t\tqueue_work(cxgb3_wq, &adapter->db_drop_task);\n\n\tif (status & (F_HIPRIORITYDBFULL | F_LOPRIORITYDBFULL))\n\t\tqueue_work(cxgb3_wq, &adapter->db_full_task);\n\n\tif (status & (F_HIPRIORITYDBEMPTY | F_LOPRIORITYDBEMPTY))\n\t\tqueue_work(cxgb3_wq, &adapter->db_empty_task);\n\n\tt3_write_reg(adapter, A_SG_INT_CAUSE, status);\n\tif (status &  SGE_FATALERR)\n\t\tt3_fatal_err(adapter);\n}\n\n \nstatic void sge_timer_tx(struct timer_list *t)\n{\n\tstruct sge_qset *qs = from_timer(qs, t, tx_reclaim_timer);\n\tstruct port_info *pi = netdev_priv(qs->netdev);\n\tstruct adapter *adap = pi->adapter;\n\tunsigned int tbd[SGE_TXQ_PER_SET] = {0, 0};\n\tunsigned long next_period;\n\n\tif (__netif_tx_trylock(qs->tx_q)) {\n                tbd[TXQ_ETH] = reclaim_completed_tx(adap, &qs->txq[TXQ_ETH],\n                                                     TX_RECLAIM_TIMER_CHUNK);\n\t\t__netif_tx_unlock(qs->tx_q);\n\t}\n\n\tif (spin_trylock(&qs->txq[TXQ_OFLD].lock)) {\n\t\ttbd[TXQ_OFLD] = reclaim_completed_tx(adap, &qs->txq[TXQ_OFLD],\n\t\t\t\t\t\t     TX_RECLAIM_TIMER_CHUNK);\n\t\tspin_unlock(&qs->txq[TXQ_OFLD].lock);\n\t}\n\n\tnext_period = TX_RECLAIM_PERIOD >>\n                      (max(tbd[TXQ_ETH], tbd[TXQ_OFLD]) /\n                      TX_RECLAIM_TIMER_CHUNK);\n\tmod_timer(&qs->tx_reclaim_timer, jiffies + next_period);\n}\n\n \nstatic void sge_timer_rx(struct timer_list *t)\n{\n\tspinlock_t *lock;\n\tstruct sge_qset *qs = from_timer(qs, t, rx_reclaim_timer);\n\tstruct port_info *pi = netdev_priv(qs->netdev);\n\tstruct adapter *adap = pi->adapter;\n\tu32 status;\n\n\tlock = adap->params.rev > 0 ?\n\t       &qs->rspq.lock : &adap->sge.qs[0].rspq.lock;\n\n\tif (!spin_trylock_irq(lock))\n\t\tgoto out;\n\n\tif (napi_is_scheduled(&qs->napi))\n\t\tgoto unlock;\n\n\tif (adap->params.rev < 4) {\n\t\tstatus = t3_read_reg(adap, A_SG_RSPQ_FL_STATUS);\n\n\t\tif (status & (1 << qs->rspq.cntxt_id)) {\n\t\t\tqs->rspq.starved++;\n\t\t\tif (qs->rspq.credits) {\n\t\t\t\tqs->rspq.credits--;\n\t\t\t\trefill_rspq(adap, &qs->rspq, 1);\n\t\t\t\tqs->rspq.restarted++;\n\t\t\t\tt3_write_reg(adap, A_SG_RSPQ_FL_STATUS,\n\t\t\t\t\t     1 << qs->rspq.cntxt_id);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (qs->fl[0].credits < qs->fl[0].size)\n\t\t__refill_fl(adap, &qs->fl[0]);\n\tif (qs->fl[1].credits < qs->fl[1].size)\n\t\t__refill_fl(adap, &qs->fl[1]);\n\nunlock:\n\tspin_unlock_irq(lock);\nout:\n\tmod_timer(&qs->rx_reclaim_timer, jiffies + RX_RECLAIM_PERIOD);\n}\n\n \nvoid t3_update_qset_coalesce(struct sge_qset *qs, const struct qset_params *p)\n{\n\tqs->rspq.holdoff_tmr = max(p->coalesce_usecs * 10, 1U); \n\tqs->rspq.polling = p->polling;\n\tqs->napi.poll = p->polling ? napi_rx_handler : ofld_poll;\n}\n\n \nint t3_sge_alloc_qset(struct adapter *adapter, unsigned int id, int nports,\n\t\t      int irq_vec_idx, const struct qset_params *p,\n\t\t      int ntxq, struct net_device *dev,\n\t\t      struct netdev_queue *netdevq)\n{\n\tint i, avail, ret = -ENOMEM;\n\tstruct sge_qset *q = &adapter->sge.qs[id];\n\n\tinit_qset_cntxt(q, id);\n\ttimer_setup(&q->tx_reclaim_timer, sge_timer_tx, 0);\n\ttimer_setup(&q->rx_reclaim_timer, sge_timer_rx, 0);\n\n\tq->fl[0].desc = alloc_ring(adapter->pdev, p->fl_size,\n\t\t\t\t   sizeof(struct rx_desc),\n\t\t\t\t   sizeof(struct rx_sw_desc),\n\t\t\t\t   &q->fl[0].phys_addr, &q->fl[0].sdesc);\n\tif (!q->fl[0].desc)\n\t\tgoto err;\n\n\tq->fl[1].desc = alloc_ring(adapter->pdev, p->jumbo_size,\n\t\t\t\t   sizeof(struct rx_desc),\n\t\t\t\t   sizeof(struct rx_sw_desc),\n\t\t\t\t   &q->fl[1].phys_addr, &q->fl[1].sdesc);\n\tif (!q->fl[1].desc)\n\t\tgoto err;\n\n\tq->rspq.desc = alloc_ring(adapter->pdev, p->rspq_size,\n\t\t\t\t  sizeof(struct rsp_desc), 0,\n\t\t\t\t  &q->rspq.phys_addr, NULL);\n\tif (!q->rspq.desc)\n\t\tgoto err;\n\n\tfor (i = 0; i < ntxq; ++i) {\n\t\t \n\t\tsize_t sz = i == TXQ_CTRL ? 0 : sizeof(struct tx_sw_desc);\n\n\t\tq->txq[i].desc = alloc_ring(adapter->pdev, p->txq_size[i],\n\t\t\t\t\t    sizeof(struct tx_desc), sz,\n\t\t\t\t\t    &q->txq[i].phys_addr,\n\t\t\t\t\t    &q->txq[i].sdesc);\n\t\tif (!q->txq[i].desc)\n\t\t\tgoto err;\n\n\t\tq->txq[i].gen = 1;\n\t\tq->txq[i].size = p->txq_size[i];\n\t\tspin_lock_init(&q->txq[i].lock);\n\t\tskb_queue_head_init(&q->txq[i].sendq);\n\t}\n\n\tINIT_WORK(&q->txq[TXQ_OFLD].qresume_task, restart_offloadq);\n\tINIT_WORK(&q->txq[TXQ_CTRL].qresume_task, restart_ctrlq);\n\n\tq->fl[0].gen = q->fl[1].gen = 1;\n\tq->fl[0].size = p->fl_size;\n\tq->fl[1].size = p->jumbo_size;\n\n\tq->rspq.gen = 1;\n\tq->rspq.size = p->rspq_size;\n\tspin_lock_init(&q->rspq.lock);\n\tskb_queue_head_init(&q->rspq.rx_queue);\n\n\tq->txq[TXQ_ETH].stop_thres = nports *\n\t    flits_to_desc(sgl_len(MAX_SKB_FRAGS + 1) + 3);\n\n#if FL0_PG_CHUNK_SIZE > 0\n\tq->fl[0].buf_size = FL0_PG_CHUNK_SIZE;\n#else\n\tq->fl[0].buf_size = SGE_RX_SM_BUF_SIZE + sizeof(struct cpl_rx_data);\n#endif\n#if FL1_PG_CHUNK_SIZE > 0\n\tq->fl[1].buf_size = FL1_PG_CHUNK_SIZE;\n#else\n\tq->fl[1].buf_size = is_offload(adapter) ?\n\t\t(16 * 1024) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) :\n\t\tMAX_FRAME_SIZE + 2 + sizeof(struct cpl_rx_pkt);\n#endif\n\n\tq->fl[0].use_pages = FL0_PG_CHUNK_SIZE > 0;\n\tq->fl[1].use_pages = FL1_PG_CHUNK_SIZE > 0;\n\tq->fl[0].order = FL0_PG_ORDER;\n\tq->fl[1].order = FL1_PG_ORDER;\n\tq->fl[0].alloc_size = FL0_PG_ALLOC_SIZE;\n\tq->fl[1].alloc_size = FL1_PG_ALLOC_SIZE;\n\n\tspin_lock_irq(&adapter->sge.reg_lock);\n\n\t \n\tret = t3_sge_init_rspcntxt(adapter, q->rspq.cntxt_id, irq_vec_idx,\n\t\t\t\t   q->rspq.phys_addr, q->rspq.size,\n\t\t\t\t   q->fl[0].buf_size - SGE_PG_RSVD, 1, 0);\n\tif (ret)\n\t\tgoto err_unlock;\n\n\tfor (i = 0; i < SGE_RXQ_PER_SET; ++i) {\n\t\tret = t3_sge_init_flcntxt(adapter, q->fl[i].cntxt_id, 0,\n\t\t\t\t\t  q->fl[i].phys_addr, q->fl[i].size,\n\t\t\t\t\t  q->fl[i].buf_size - SGE_PG_RSVD,\n\t\t\t\t\t  p->cong_thres, 1, 0);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\tret = t3_sge_init_ecntxt(adapter, q->txq[TXQ_ETH].cntxt_id, USE_GTS,\n\t\t\t\t SGE_CNTXT_ETH, id, q->txq[TXQ_ETH].phys_addr,\n\t\t\t\t q->txq[TXQ_ETH].size, q->txq[TXQ_ETH].token,\n\t\t\t\t 1, 0);\n\tif (ret)\n\t\tgoto err_unlock;\n\n\tif (ntxq > 1) {\n\t\tret = t3_sge_init_ecntxt(adapter, q->txq[TXQ_OFLD].cntxt_id,\n\t\t\t\t\t USE_GTS, SGE_CNTXT_OFLD, id,\n\t\t\t\t\t q->txq[TXQ_OFLD].phys_addr,\n\t\t\t\t\t q->txq[TXQ_OFLD].size, 0, 1, 0);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\tif (ntxq > 2) {\n\t\tret = t3_sge_init_ecntxt(adapter, q->txq[TXQ_CTRL].cntxt_id, 0,\n\t\t\t\t\t SGE_CNTXT_CTRL, id,\n\t\t\t\t\t q->txq[TXQ_CTRL].phys_addr,\n\t\t\t\t\t q->txq[TXQ_CTRL].size,\n\t\t\t\t\t q->txq[TXQ_CTRL].token, 1, 0);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\tspin_unlock_irq(&adapter->sge.reg_lock);\n\n\tq->adap = adapter;\n\tq->netdev = dev;\n\tq->tx_q = netdevq;\n\tt3_update_qset_coalesce(q, p);\n\n\tavail = refill_fl(adapter, &q->fl[0], q->fl[0].size,\n\t\t\t  GFP_KERNEL | __GFP_COMP);\n\tif (!avail) {\n\t\tCH_ALERT(adapter, \"free list queue 0 initialization failed\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tif (avail < q->fl[0].size)\n\t\tCH_WARN(adapter, \"free list queue 0 enabled with %d credits\\n\",\n\t\t\tavail);\n\n\tavail = refill_fl(adapter, &q->fl[1], q->fl[1].size,\n\t\t\t  GFP_KERNEL | __GFP_COMP);\n\tif (avail < q->fl[1].size)\n\t\tCH_WARN(adapter, \"free list queue 1 enabled with %d credits\\n\",\n\t\t\tavail);\n\trefill_rspq(adapter, &q->rspq, q->rspq.size - 1);\n\n\tt3_write_reg(adapter, A_SG_GTS, V_RSPQ(q->rspq.cntxt_id) |\n\t\t     V_NEWTIMER(q->rspq.holdoff_tmr));\n\n\treturn 0;\n\nerr_unlock:\n\tspin_unlock_irq(&adapter->sge.reg_lock);\nerr:\n\tt3_free_qset(adapter, q);\n\treturn ret;\n}\n\n \nvoid t3_start_sge_timers(struct adapter *adap)\n{\n\tint i;\n\n\tfor (i = 0; i < SGE_QSETS; ++i) {\n\t\tstruct sge_qset *q = &adap->sge.qs[i];\n\n\t\tif (q->tx_reclaim_timer.function)\n\t\t\tmod_timer(&q->tx_reclaim_timer,\n\t\t\t\t  jiffies + TX_RECLAIM_PERIOD);\n\n\t\tif (q->rx_reclaim_timer.function)\n\t\t\tmod_timer(&q->rx_reclaim_timer,\n\t\t\t\t  jiffies + RX_RECLAIM_PERIOD);\n\t}\n}\n\n \nvoid t3_stop_sge_timers(struct adapter *adap)\n{\n\tint i;\n\n\tfor (i = 0; i < SGE_QSETS; ++i) {\n\t\tstruct sge_qset *q = &adap->sge.qs[i];\n\n\t\tif (q->tx_reclaim_timer.function)\n\t\t\tdel_timer_sync(&q->tx_reclaim_timer);\n\t\tif (q->rx_reclaim_timer.function)\n\t\t\tdel_timer_sync(&q->rx_reclaim_timer);\n\t}\n}\n\n \nvoid t3_free_sge_resources(struct adapter *adap)\n{\n\tint i;\n\n\tfor (i = 0; i < SGE_QSETS; ++i)\n\t\tt3_free_qset(adap, &adap->sge.qs[i]);\n}\n\n \nvoid t3_sge_start(struct adapter *adap)\n{\n\tt3_set_reg_field(adap, A_SG_CONTROL, F_GLOBALENABLE, F_GLOBALENABLE);\n}\n\n \nvoid t3_sge_stop_dma(struct adapter *adap)\n{\n\tt3_set_reg_field(adap, A_SG_CONTROL, F_GLOBALENABLE, 0);\n}\n\n \nvoid t3_sge_stop(struct adapter *adap)\n{\n\tint i;\n\n\tt3_sge_stop_dma(adap);\n\n\t \n\tif (!(adap->flags & FULL_INIT_DONE))\n\t\treturn;\n\tfor (i = 0; i < SGE_QSETS; ++i) {\n\t\tstruct sge_qset *qs = &adap->sge.qs[i];\n\n\t\tcancel_work_sync(&qs->txq[TXQ_OFLD].qresume_task);\n\t\tcancel_work_sync(&qs->txq[TXQ_CTRL].qresume_task);\n\t}\n}\n\n \nvoid t3_sge_init(struct adapter *adap, struct sge_params *p)\n{\n\tunsigned int ctrl, ups = ffs(pci_resource_len(adap->pdev, 2) >> 12);\n\n\tctrl = F_DROPPKT | V_PKTSHIFT(2) | F_FLMODE | F_AVOIDCQOVFL |\n\t    F_CQCRDTCTRL | F_CONGMODE | F_TNLFLMODE | F_FATLPERREN |\n\t    V_HOSTPAGESIZE(PAGE_SHIFT - 11) | F_BIGENDIANINGRESS |\n\t    V_USERSPACESIZE(ups ? ups - 1 : 0) | F_ISCSICOALESCING;\n#if SGE_NUM_GENBITS == 1\n\tctrl |= F_EGRGENCTRL;\n#endif\n\tif (adap->params.rev > 0) {\n\t\tif (!(adap->flags & (USING_MSIX | USING_MSI)))\n\t\t\tctrl |= F_ONEINTMULTQ | F_OPTONEINTMULTQ;\n\t}\n\tt3_write_reg(adap, A_SG_CONTROL, ctrl);\n\tt3_write_reg(adap, A_SG_EGR_RCQ_DRB_THRSH, V_HIRCQDRBTHRSH(512) |\n\t\t     V_LORCQDRBTHRSH(512));\n\tt3_write_reg(adap, A_SG_TIMER_TICK, core_ticks_per_usec(adap) / 10);\n\tt3_write_reg(adap, A_SG_CMDQ_CREDIT_TH, V_THRESHOLD(32) |\n\t\t     V_TIMEOUT(200 * core_ticks_per_usec(adap)));\n\tt3_write_reg(adap, A_SG_HI_DRB_HI_THRSH,\n\t\t     adap->params.rev < T3_REV_C ? 1000 : 500);\n\tt3_write_reg(adap, A_SG_HI_DRB_LO_THRSH, 256);\n\tt3_write_reg(adap, A_SG_LO_DRB_HI_THRSH, 1000);\n\tt3_write_reg(adap, A_SG_LO_DRB_LO_THRSH, 256);\n\tt3_write_reg(adap, A_SG_OCO_BASE, V_BASE1(0xfff));\n\tt3_write_reg(adap, A_SG_DRB_PRI_THRESH, 63 * 1024);\n}\n\n \nvoid t3_sge_prep(struct adapter *adap, struct sge_params *p)\n{\n\tint i;\n\n\tp->max_pkt_size = (16 * 1024) - sizeof(struct cpl_rx_data) -\n\t    SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tfor (i = 0; i < SGE_QSETS; ++i) {\n\t\tstruct qset_params *q = p->qset + i;\n\n\t\tq->polling = adap->params.rev > 0;\n\t\tq->coalesce_usecs = 5;\n\t\tq->rspq_size = 1024;\n\t\tq->fl_size = 1024;\n\t\tq->jumbo_size = 512;\n\t\tq->txq_size[TXQ_ETH] = 1024;\n\t\tq->txq_size[TXQ_OFLD] = 1024;\n\t\tq->txq_size[TXQ_CTRL] = 256;\n\t\tq->cong_thres = 0;\n\t}\n\n\tspin_lock_init(&adap->sge.reg_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}