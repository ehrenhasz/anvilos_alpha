{
  "module_name": "chcr_ipsec.c",
  "hash_id": "bb215957db63611d7afc48f7a58d9328a7a20de2df17c2209620e3699568db73",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/inline_crypto/ch_ipsec/chcr_ipsec.c",
  "human_readable_source": " \n\n#define pr_fmt(fmt) \"ch_ipsec: \" fmt\n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/crypto.h>\n#include <linux/skbuff.h>\n#include <linux/rtnetlink.h>\n#include <linux/highmem.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <linux/netdevice.h>\n#include <net/esp.h>\n#include <net/xfrm.h>\n#include <crypto/aes.h>\n#include <crypto/algapi.h>\n#include <crypto/hash.h>\n#include <crypto/sha1.h>\n#include <crypto/sha2.h>\n#include <crypto/authenc.h>\n#include <crypto/internal/aead.h>\n#include <crypto/null.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/aead.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/internal/hash.h>\n\n#include \"chcr_ipsec.h\"\n\n \n#define MAX_IMM_TX_PKT_LEN 256\n#define GCM_ESP_IV_SIZE     8\n\nstatic LIST_HEAD(uld_ctx_list);\nstatic DEFINE_MUTEX(dev_mutex);\n\nstatic bool ch_ipsec_offload_ok(struct sk_buff *skb, struct xfrm_state *x);\nstatic int ch_ipsec_uld_state_change(void *handle, enum cxgb4_state new_state);\nstatic int ch_ipsec_xmit(struct sk_buff *skb, struct net_device *dev);\nstatic void *ch_ipsec_uld_add(const struct cxgb4_lld_info *infop);\nstatic void ch_ipsec_advance_esn_state(struct xfrm_state *x);\nstatic void ch_ipsec_xfrm_free_state(struct xfrm_state *x);\nstatic void ch_ipsec_xfrm_del_state(struct xfrm_state *x);\nstatic int ch_ipsec_xfrm_add_state(struct xfrm_state *x,\n\t\t\t\t   struct netlink_ext_ack *extack);\n\nstatic const struct xfrmdev_ops ch_ipsec_xfrmdev_ops = {\n\t.xdo_dev_state_add      = ch_ipsec_xfrm_add_state,\n\t.xdo_dev_state_delete   = ch_ipsec_xfrm_del_state,\n\t.xdo_dev_state_free     = ch_ipsec_xfrm_free_state,\n\t.xdo_dev_offload_ok     = ch_ipsec_offload_ok,\n\t.xdo_dev_state_advance_esn = ch_ipsec_advance_esn_state,\n};\n\nstatic struct cxgb4_uld_info ch_ipsec_uld_info = {\n\t.name = CHIPSEC_DRV_MODULE_NAME,\n\t.add = ch_ipsec_uld_add,\n\t.state_change = ch_ipsec_uld_state_change,\n\t.tx_handler = ch_ipsec_xmit,\n\t.xfrmdev_ops = &ch_ipsec_xfrmdev_ops,\n};\n\nstatic void *ch_ipsec_uld_add(const struct cxgb4_lld_info *infop)\n{\n\tstruct ipsec_uld_ctx *u_ctx;\n\n\tpr_info_once(\"%s - version %s\\n\", CHIPSEC_DRV_DESC,\n\t\t     CHIPSEC_DRV_VERSION);\n\tu_ctx = kzalloc(sizeof(*u_ctx), GFP_KERNEL);\n\tif (!u_ctx) {\n\t\tu_ctx = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\tu_ctx->lldi = *infop;\nout:\n\treturn u_ctx;\n}\n\nstatic int ch_ipsec_uld_state_change(void *handle, enum cxgb4_state new_state)\n{\n\tstruct ipsec_uld_ctx *u_ctx = handle;\n\n\tpr_debug(\"new_state %u\\n\", new_state);\n\tswitch (new_state) {\n\tcase CXGB4_STATE_UP:\n\t\tpr_info(\"%s: Up\\n\", pci_name(u_ctx->lldi.pdev));\n\t\tmutex_lock(&dev_mutex);\n\t\tlist_add_tail(&u_ctx->entry, &uld_ctx_list);\n\t\tmutex_unlock(&dev_mutex);\n\t\tbreak;\n\tcase CXGB4_STATE_START_RECOVERY:\n\tcase CXGB4_STATE_DOWN:\n\tcase CXGB4_STATE_DETACH:\n\t\tpr_info(\"%s: Down\\n\", pci_name(u_ctx->lldi.pdev));\n\t\tlist_del(&u_ctx->entry);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int ch_ipsec_setauthsize(struct xfrm_state *x,\n\t\t\t\tstruct ipsec_sa_entry *sa_entry)\n{\n\tint hmac_ctrl;\n\tint authsize = x->aead->alg_icv_len / 8;\n\n\tsa_entry->authsize = authsize;\n\n\tswitch (authsize) {\n\tcase ICV_8:\n\t\thmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;\n\t\tbreak;\n\tcase ICV_12:\n\t\thmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;\n\t\tbreak;\n\tcase ICV_16:\n\t\thmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn hmac_ctrl;\n}\n\nstatic int ch_ipsec_setkey(struct xfrm_state *x,\n\t\t\t   struct ipsec_sa_entry *sa_entry)\n{\n\tint keylen = (x->aead->alg_key_len + 7) / 8;\n\tunsigned char *key = x->aead->alg_key;\n\tint ck_size, key_ctx_size = 0;\n\tunsigned char ghash_h[AEAD_H_SIZE];\n\tstruct crypto_aes_ctx aes;\n\tint ret = 0;\n\n\tif (keylen > 3) {\n\t\tkeylen -= 4;   \n\t\tmemcpy(sa_entry->salt, key + keylen, 4);\n\t}\n\n\tif (keylen == AES_KEYSIZE_128) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\n\t} else if (keylen == AES_KEYSIZE_192) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\n\t} else if (keylen == AES_KEYSIZE_256) {\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\n\t} else {\n\t\tpr_err(\"GCM: Invalid key length %d\\n\", keylen);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemcpy(sa_entry->key, key, keylen);\n\tsa_entry->enckey_len = keylen;\n\tkey_ctx_size = sizeof(struct _key_ctx) +\n\t\t\t      ((DIV_ROUND_UP(keylen, 16)) << 4) +\n\t\t\t      AEAD_H_SIZE;\n\n\tsa_entry->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size,\n\t\t\t\t\t\t CHCR_KEYCTX_MAC_KEY_SIZE_128,\n\t\t\t\t\t\t 0, 0,\n\t\t\t\t\t\t key_ctx_size >> 4);\n\n\t \n\tret = aes_expandkey(&aes, key, keylen);\n\tif (ret) {\n\t\tsa_entry->enckey_len = 0;\n\t\tgoto out;\n\t}\n\tmemset(ghash_h, 0, AEAD_H_SIZE);\n\taes_encrypt(&aes, ghash_h, ghash_h);\n\tmemzero_explicit(&aes, sizeof(aes));\n\n\tmemcpy(sa_entry->key + (DIV_ROUND_UP(sa_entry->enckey_len, 16) *\n\t       16), ghash_h, AEAD_H_SIZE);\n\tsa_entry->kctx_len = ((DIV_ROUND_UP(sa_entry->enckey_len, 16)) << 4) +\n\t\t\t      AEAD_H_SIZE;\nout:\n\treturn ret;\n}\n\n \nstatic int ch_ipsec_xfrm_add_state(struct xfrm_state *x,\n\t\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct ipsec_sa_entry *sa_entry;\n\tint res = 0;\n\n\tif (x->props.aalgo != SADB_AALG_NONE) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Cannot offload authenticated xfrm states\");\n\t\treturn -EINVAL;\n\t}\n\tif (x->props.calgo != SADB_X_CALG_NONE) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Cannot offload compressed xfrm states\");\n\t\treturn -EINVAL;\n\t}\n\tif (x->props.family != AF_INET &&\n\t    x->props.family != AF_INET6) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Only IPv4/6 xfrm state offloaded\");\n\t\treturn -EINVAL;\n\t}\n\tif (x->props.mode != XFRM_MODE_TRANSPORT &&\n\t    x->props.mode != XFRM_MODE_TUNNEL) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Only transport and tunnel xfrm offload\");\n\t\treturn -EINVAL;\n\t}\n\tif (x->id.proto != IPPROTO_ESP) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Only ESP xfrm state offloaded\");\n\t\treturn -EINVAL;\n\t}\n\tif (x->encap) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Encapsulated xfrm state not offloaded\");\n\t\treturn -EINVAL;\n\t}\n\tif (!x->aead) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Cannot offload xfrm states without aead\");\n\t\treturn -EINVAL;\n\t}\n\tif (x->aead->alg_icv_len != 128 &&\n\t    x->aead->alg_icv_len != 96) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Cannot offload xfrm states with AEAD ICV length other than 96b & 128b\");\n\t\treturn -EINVAL;\n\t}\n\tif ((x->aead->alg_key_len != 128 + 32) &&\n\t    (x->aead->alg_key_len != 256 + 32)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"cannot offload xfrm states with AEAD key length other than 128/256 bit\");\n\t\treturn -EINVAL;\n\t}\n\tif (x->tfcpad) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Cannot offload xfrm states with tfc padding\");\n\t\treturn -EINVAL;\n\t}\n\tif (!x->geniv) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Cannot offload xfrm states without geniv\");\n\t\treturn -EINVAL;\n\t}\n\tif (strcmp(x->geniv, \"seqiv\")) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Cannot offload xfrm states with geniv other than seqiv\");\n\t\treturn -EINVAL;\n\t}\n\tif (x->xso.type != XFRM_DEV_OFFLOAD_CRYPTO) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Unsupported xfrm offload\");\n\t\treturn -EINVAL;\n\t}\n\n\tsa_entry = kzalloc(sizeof(*sa_entry), GFP_KERNEL);\n\tif (!sa_entry) {\n\t\tres = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tsa_entry->hmac_ctrl = ch_ipsec_setauthsize(x, sa_entry);\n\tif (x->props.flags & XFRM_STATE_ESN)\n\t\tsa_entry->esn = 1;\n\tch_ipsec_setkey(x, sa_entry);\n\tx->xso.offload_handle = (unsigned long)sa_entry;\n\ttry_module_get(THIS_MODULE);\nout:\n\treturn res;\n}\n\nstatic void ch_ipsec_xfrm_del_state(struct xfrm_state *x)\n{\n\t \n\tif (!x->xso.offload_handle)\n\t\treturn;\n}\n\nstatic void ch_ipsec_xfrm_free_state(struct xfrm_state *x)\n{\n\tstruct ipsec_sa_entry *sa_entry;\n\n\tif (!x->xso.offload_handle)\n\t\treturn;\n\n\tsa_entry = (struct ipsec_sa_entry *)x->xso.offload_handle;\n\tkfree(sa_entry);\n\tmodule_put(THIS_MODULE);\n}\n\nstatic bool ch_ipsec_offload_ok(struct sk_buff *skb, struct xfrm_state *x)\n{\n\tif (x->props.family == AF_INET) {\n\t\t \n\t\tif (ip_hdr(skb)->ihl > 5)\n\t\t\treturn false;\n\t} else {\n\t\t \n\t\tif (ipv6_ext_hdr(ipv6_hdr(skb)->nexthdr))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic void ch_ipsec_advance_esn_state(struct xfrm_state *x)\n{\n\t \n\tif (!x->xso.offload_handle)\n\t\treturn;\n}\n\nstatic int is_eth_imm(const struct sk_buff *skb,\n\t\t      struct ipsec_sa_entry *sa_entry)\n{\n\tunsigned int kctx_len;\n\tint hdrlen;\n\n\tkctx_len = sa_entry->kctx_len;\n\thdrlen = sizeof(struct fw_ulptx_wr) +\n\t\t sizeof(struct chcr_ipsec_req) + kctx_len;\n\n\thdrlen += sizeof(struct cpl_tx_pkt);\n\tif (sa_entry->esn)\n\t\thdrlen += (DIV_ROUND_UP(sizeof(struct chcr_ipsec_aadiv), 16)\n\t\t\t   << 4);\n\tif (skb->len <= MAX_IMM_TX_PKT_LEN - hdrlen)\n\t\treturn hdrlen;\n\treturn 0;\n}\n\nstatic unsigned int calc_tx_sec_flits(const struct sk_buff *skb,\n\t\t\t\t      struct ipsec_sa_entry *sa_entry,\n\t\t\t\t      bool *immediate)\n{\n\tunsigned int kctx_len;\n\tunsigned int flits;\n\tint aadivlen;\n\tint hdrlen;\n\n\tkctx_len = sa_entry->kctx_len;\n\thdrlen = is_eth_imm(skb, sa_entry);\n\taadivlen = sa_entry->esn ? DIV_ROUND_UP(sizeof(struct chcr_ipsec_aadiv),\n\t\t\t\t\t\t16) : 0;\n\taadivlen <<= 4;\n\n\t \n\n\tif (hdrlen) {\n\t\t*immediate = true;\n\t\treturn DIV_ROUND_UP(skb->len + hdrlen, sizeof(__be64));\n\t}\n\n\tflits = sgl_len(skb_shinfo(skb)->nr_frags + 1);\n\n\t \n\tflits += (sizeof(struct fw_ulptx_wr) +\n\t\t  sizeof(struct chcr_ipsec_req) +\n\t\t  kctx_len +\n\t\t  sizeof(struct cpl_tx_pkt_core) +\n\t\t  aadivlen) / sizeof(__be64);\n\treturn flits;\n}\n\nstatic void *copy_esn_pktxt(struct sk_buff *skb,\n\t\t\t    struct net_device *dev,\n\t\t\t    void *pos,\n\t\t\t    struct ipsec_sa_entry *sa_entry)\n{\n\tstruct chcr_ipsec_aadiv *aadiv;\n\tstruct ulptx_idata *sc_imm;\n\tstruct ip_esp_hdr *esphdr;\n\tstruct xfrm_offload *xo;\n\tstruct sge_eth_txq *q;\n\tstruct adapter *adap;\n\tstruct port_info *pi;\n\t__be64 seqno;\n\tu32 qidx;\n\tu32 seqlo;\n\tu8 *iv;\n\tint eoq;\n\tint len;\n\n\tpi = netdev_priv(dev);\n\tadap = pi->adapter;\n\tqidx = skb->queue_mapping;\n\tq = &adap->sge.ethtxq[qidx + pi->first_qset];\n\n\t \n\teoq = (void *)q->q.stat - pos;\n\tif (!eoq)\n\t\tpos = q->q.desc;\n\n\tlen = DIV_ROUND_UP(sizeof(struct chcr_ipsec_aadiv), 16) << 4;\n\tmemset(pos, 0, len);\n\taadiv = (struct chcr_ipsec_aadiv *)pos;\n\tesphdr = (struct ip_esp_hdr *)skb_transport_header(skb);\n\tiv = skb_transport_header(skb) + sizeof(struct ip_esp_hdr);\n\txo = xfrm_offload(skb);\n\n\taadiv->spi = (esphdr->spi);\n\tseqlo = ntohl(esphdr->seq_no);\n\tseqno = cpu_to_be64(seqlo + ((u64)xo->seq.hi << 32));\n\tmemcpy(aadiv->seq_no, &seqno, 8);\n\tiv = skb_transport_header(skb) + sizeof(struct ip_esp_hdr);\n\tmemcpy(aadiv->iv, iv, 8);\n\n\tif (is_eth_imm(skb, sa_entry) && !skb_is_nonlinear(skb)) {\n\t\tsc_imm = (struct ulptx_idata *)(pos +\n\t\t\t  (DIV_ROUND_UP(sizeof(struct chcr_ipsec_aadiv),\n\t\t\t\t\tsizeof(__be64)) << 3));\n\t\tsc_imm->cmd_more = FILL_CMD_MORE(0);\n\t\tsc_imm->len = cpu_to_be32(skb->len);\n\t}\n\tpos += len;\n\treturn pos;\n}\n\nstatic void *copy_cpltx_pktxt(struct sk_buff *skb,\n\t\t\t      struct net_device *dev,\n\t\t\t      void *pos,\n\t\t\t      struct ipsec_sa_entry *sa_entry)\n{\n\tstruct cpl_tx_pkt_core *cpl;\n\tstruct sge_eth_txq *q;\n\tstruct adapter *adap;\n\tstruct port_info *pi;\n\tu32 ctrl0, qidx;\n\tu64 cntrl = 0;\n\tint left;\n\n\tpi = netdev_priv(dev);\n\tadap = pi->adapter;\n\tqidx = skb->queue_mapping;\n\tq = &adap->sge.ethtxq[qidx + pi->first_qset];\n\n\tleft = (void *)q->q.stat - pos;\n\tif (!left)\n\t\tpos = q->q.desc;\n\n\tcpl = (struct cpl_tx_pkt_core *)pos;\n\n\tcntrl = TXPKT_L4CSUM_DIS_F | TXPKT_IPCSUM_DIS_F;\n\tctrl0 = TXPKT_OPCODE_V(CPL_TX_PKT_XT) | TXPKT_INTF_V(pi->tx_chan) |\n\t\t\t       TXPKT_PF_V(adap->pf);\n\tif (skb_vlan_tag_present(skb)) {\n\t\tq->vlan_ins++;\n\t\tcntrl |= TXPKT_VLAN_VLD_F | TXPKT_VLAN_V(skb_vlan_tag_get(skb));\n\t}\n\n\tcpl->ctrl0 = htonl(ctrl0);\n\tcpl->pack = htons(0);\n\tcpl->len = htons(skb->len);\n\tcpl->ctrl1 = cpu_to_be64(cntrl);\n\n\tpos += sizeof(struct cpl_tx_pkt_core);\n\t \n\tif (sa_entry->esn)\n\t\tpos = copy_esn_pktxt(skb, dev, pos, sa_entry);\n\treturn pos;\n}\n\nstatic void *copy_key_cpltx_pktxt(struct sk_buff *skb,\n\t\t\t\t  struct net_device *dev,\n\t\t\t\t  void *pos,\n\t\t\t\t  struct ipsec_sa_entry *sa_entry)\n{\n\tstruct _key_ctx *key_ctx;\n\tint left, eoq, key_len;\n\tstruct sge_eth_txq *q;\n\tstruct adapter *adap;\n\tstruct port_info *pi;\n\tunsigned int qidx;\n\n\tpi = netdev_priv(dev);\n\tadap = pi->adapter;\n\tqidx = skb->queue_mapping;\n\tq = &adap->sge.ethtxq[qidx + pi->first_qset];\n\tkey_len = sa_entry->kctx_len;\n\n\t \n\teoq = (void *)q->q.stat - pos;\n\tleft = eoq;\n\tif (!eoq) {\n\t\tpos = q->q.desc;\n\t\tleft = 64 * q->q.size;\n\t}\n\n\t \n\tkey_ctx = (struct _key_ctx *)pos;\n\tkey_ctx->ctx_hdr = sa_entry->key_ctx_hdr;\n\tmemcpy(key_ctx->salt, sa_entry->salt, MAX_SALT);\n\tpos += sizeof(struct _key_ctx);\n\tleft -= sizeof(struct _key_ctx);\n\n\tif (likely(key_len <= left)) {\n\t\tmemcpy(key_ctx->key, sa_entry->key, key_len);\n\t\tpos += key_len;\n\t} else {\n\t\tmemcpy(pos, sa_entry->key, left);\n\t\tmemcpy(q->q.desc, sa_entry->key + left,\n\t\t       key_len - left);\n\t\tpos = (u8 *)q->q.desc + (key_len - left);\n\t}\n\t \n\tpos = copy_cpltx_pktxt(skb, dev, pos, sa_entry);\n\n\treturn pos;\n}\n\nstatic void *ch_ipsec_crypto_wreq(struct sk_buff *skb,\n\t\t\t\t  struct net_device *dev,\n\t\t\t\t  void *pos,\n\t\t\t\t  int credits,\n\t\t\t\t  struct ipsec_sa_entry *sa_entry)\n{\n\tstruct port_info *pi = netdev_priv(dev);\n\tstruct adapter *adap = pi->adapter;\n\tunsigned int ivsize = GCM_ESP_IV_SIZE;\n\tstruct chcr_ipsec_wr *wr;\n\tbool immediate = false;\n\tu16 immdatalen = 0;\n\tunsigned int flits;\n\tu32 ivinoffset;\n\tu32 aadstart;\n\tu32 aadstop;\n\tu32 ciphstart;\n\tu16 sc_more = 0;\n\tu32 ivdrop = 0;\n\tu32 esnlen = 0;\n\tu32 wr_mid;\n\tu16 ndesc;\n\tint qidx = skb_get_queue_mapping(skb);\n\tstruct sge_eth_txq *q = &adap->sge.ethtxq[qidx + pi->first_qset];\n\tunsigned int kctx_len = sa_entry->kctx_len;\n\tint qid = q->q.cntxt_id;\n\n\tatomic_inc(&adap->ch_ipsec_stats.ipsec_cnt);\n\n\tflits = calc_tx_sec_flits(skb, sa_entry, &immediate);\n\tndesc = DIV_ROUND_UP(flits, 2);\n\tif (sa_entry->esn)\n\t\tivdrop = 1;\n\n\tif (immediate)\n\t\timmdatalen = skb->len;\n\n\tif (sa_entry->esn) {\n\t\tesnlen = sizeof(struct chcr_ipsec_aadiv);\n\t\tif (!skb_is_nonlinear(skb))\n\t\t\tsc_more  = 1;\n\t}\n\n\t \n\twr = (struct chcr_ipsec_wr *)pos;\n\twr->wreq.op_to_compl = htonl(FW_WR_OP_V(FW_ULPTX_WR));\n\twr_mid = FW_CRYPTO_LOOKASIDE_WR_LEN16_V(ndesc);\n\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\tnetif_tx_stop_queue(q->txq);\n\t\tq->q.stops++;\n\t\tif (!q->dbqt)\n\t\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\twr_mid |= FW_ULPTX_WR_DATA_F;\n\twr->wreq.flowid_len16 = htonl(wr_mid);\n\n\t \n\twr->req.ulptx.cmd_dest = FILL_ULPTX_CMD_DEST(pi->port_id, qid);\n\twr->req.ulptx.len = htonl(ndesc - 1);\n\n\t \n\twr->req.sc_imm.cmd_more = FILL_CMD_MORE(!immdatalen || sc_more);\n\twr->req.sc_imm.len = cpu_to_be32(sizeof(struct cpl_tx_sec_pdu) +\n\t\t\t\t\t sizeof(wr->req.key_ctx) +\n\t\t\t\t\t kctx_len +\n\t\t\t\t\t sizeof(struct cpl_tx_pkt_core) +\n\t\t\t\t\t esnlen +\n\t\t\t\t\t (esnlen ? 0 : immdatalen));\n\n\t \n\tivinoffset = sa_entry->esn ? (ESN_IV_INSERT_OFFSET + 1) :\n\t\t\t\t     (skb_transport_offset(skb) +\n\t\t\t\t      sizeof(struct ip_esp_hdr) + 1);\n\twr->req.sec_cpl.op_ivinsrtofst = htonl(\n\t\t\t\tCPL_TX_SEC_PDU_OPCODE_V(CPL_TX_SEC_PDU) |\n\t\t\t\tCPL_TX_SEC_PDU_CPLLEN_V(2) |\n\t\t\t\tCPL_TX_SEC_PDU_PLACEHOLDER_V(1) |\n\t\t\t\tCPL_TX_SEC_PDU_IVINSRTOFST_V(\n\t\t\t\t\t\t\t     ivinoffset));\n\n\twr->req.sec_cpl.pldlen = htonl(skb->len + esnlen);\n\taadstart = sa_entry->esn ? 1 : (skb_transport_offset(skb) + 1);\n\taadstop = sa_entry->esn ? ESN_IV_INSERT_OFFSET :\n\t\t\t\t  (skb_transport_offset(skb) +\n\t\t\t\t   sizeof(struct ip_esp_hdr));\n\tciphstart = skb_transport_offset(skb) + sizeof(struct ip_esp_hdr) +\n\t\t    GCM_ESP_IV_SIZE + 1;\n\tciphstart += sa_entry->esn ?  esnlen : 0;\n\n\twr->req.sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(\n\t\t\t\t\t\t\taadstart,\n\t\t\t\t\t\t\taadstop,\n\t\t\t\t\t\t\tciphstart, 0);\n\n\twr->req.sec_cpl.cipherstop_lo_authinsert =\n\t\tFILL_SEC_CPL_AUTHINSERT(0, ciphstart,\n\t\t\t\t\tsa_entry->authsize,\n\t\t\t\t\t sa_entry->authsize);\n\twr->req.sec_cpl.seqno_numivs =\n\t\tFILL_SEC_CPL_SCMD0_SEQNO(CHCR_ENCRYPT_OP, 1,\n\t\t\t\t\t CHCR_SCMD_CIPHER_MODE_AES_GCM,\n\t\t\t\t\t CHCR_SCMD_AUTH_MODE_GHASH,\n\t\t\t\t\t sa_entry->hmac_ctrl,\n\t\t\t\t\t ivsize >> 1);\n\twr->req.sec_cpl.ivgen_hdrlen =  FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,\n\t\t\t\t\t\t\t\t  0, ivdrop, 0);\n\n\tpos += sizeof(struct fw_ulptx_wr) +\n\t       sizeof(struct ulp_txpkt) +\n\t       sizeof(struct ulptx_idata) +\n\t       sizeof(struct cpl_tx_sec_pdu);\n\n\tpos = copy_key_cpltx_pktxt(skb, dev, pos, sa_entry);\n\n\treturn pos;\n}\n\n \nstatic unsigned int flits_to_desc(unsigned int n)\n{\n\tWARN_ON(n > SGE_MAX_WR_LEN / 8);\n\treturn DIV_ROUND_UP(n, 8);\n}\n\nstatic unsigned int txq_avail(const struct sge_txq *q)\n{\n\treturn q->size - 1 - q->in_use;\n}\n\nstatic void eth_txq_stop(struct sge_eth_txq *q)\n{\n\tnetif_tx_stop_queue(q->txq);\n\tq->q.stops++;\n}\n\nstatic void txq_advance(struct sge_txq *q, unsigned int n)\n{\n\tq->in_use += n;\n\tq->pidx += n;\n\tif (q->pidx >= q->size)\n\t\tq->pidx -= q->size;\n}\n\n \nint ch_ipsec_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct xfrm_state *x = xfrm_input_state(skb);\n\tunsigned int last_desc, ndesc, flits = 0;\n\tstruct ipsec_sa_entry *sa_entry;\n\tu64 *pos, *end, *before, *sgl;\n\tstruct tx_sw_desc *sgl_sdesc;\n\tint qidx, left, credits;\n\tbool immediate = false;\n\tstruct sge_eth_txq *q;\n\tstruct adapter *adap;\n\tstruct port_info *pi;\n\tstruct sec_path *sp;\n\n\tif (!x->xso.offload_handle)\n\t\treturn NETDEV_TX_BUSY;\n\n\tsa_entry = (struct ipsec_sa_entry *)x->xso.offload_handle;\n\n\tsp = skb_sec_path(skb);\n\tif (sp->len != 1) {\nout_free:       dev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tpi = netdev_priv(dev);\n\tadap = pi->adapter;\n\tqidx = skb->queue_mapping;\n\tq = &adap->sge.ethtxq[qidx + pi->first_qset];\n\n\tcxgb4_reclaim_completed_tx(adap, &q->q, true);\n\n\tflits = calc_tx_sec_flits(skb, sa_entry, &immediate);\n\tndesc = flits_to_desc(flits);\n\tcredits = txq_avail(&q->q) - ndesc;\n\n\tif (unlikely(credits < 0)) {\n\t\teth_txq_stop(q);\n\t\tdev_err(adap->pdev_dev,\n\t\t\t\"%s: Tx ring %u full while queue awake! cred:%d %d %d flits:%d\\n\",\n\t\t\tdev->name, qidx, credits, ndesc, txq_avail(&q->q),\n\t\t\tflits);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tlast_desc = q->q.pidx + ndesc - 1;\n\tif (last_desc >= q->q.size)\n\t\tlast_desc -= q->q.size;\n\tsgl_sdesc = &q->q.sdesc[last_desc];\n\n\tif (!immediate &&\n\t    unlikely(cxgb4_map_skb(adap->pdev_dev, skb, sgl_sdesc->addr) < 0)) {\n\t\tmemset(sgl_sdesc->addr, 0, sizeof(sgl_sdesc->addr));\n\t\tq->mapping_err++;\n\t\tgoto out_free;\n\t}\n\n\tpos = (u64 *)&q->q.desc[q->q.pidx];\n\tbefore = (u64 *)pos;\n\tend = (u64 *)pos + flits;\n\t \n\tpos = (void *)ch_ipsec_crypto_wreq(skb, dev, (void *)pos,\n\t\t\t\t\t   credits, sa_entry);\n\tif (before > (u64 *)pos) {\n\t\tleft = (u8 *)end - (u8 *)q->q.stat;\n\t\tend = (void *)q->q.desc + left;\n\t}\n\tif (pos == (u64 *)q->q.stat) {\n\t\tleft = (u8 *)end - (u8 *)q->q.stat;\n\t\tend = (void *)q->q.desc + left;\n\t\tpos = (void *)q->q.desc;\n\t}\n\n\tsgl = (void *)pos;\n\tif (immediate) {\n\t\tcxgb4_inline_tx_skb(skb, &q->q, sgl);\n\t\tdev_consume_skb_any(skb);\n\t} else {\n\t\tcxgb4_write_sgl(skb, &q->q, (void *)sgl, end,\n\t\t\t\t0, sgl_sdesc->addr);\n\t\tskb_orphan(skb);\n\t\tsgl_sdesc->skb = skb;\n\t}\n\ttxq_advance(&q->q, ndesc);\n\n\tcxgb4_ring_tx_db(adap, &q->q, ndesc);\n\treturn NETDEV_TX_OK;\n}\n\nstatic int __init ch_ipsec_init(void)\n{\n\tcxgb4_register_uld(CXGB4_ULD_IPSEC, &ch_ipsec_uld_info);\n\n\treturn 0;\n}\n\nstatic void __exit ch_ipsec_exit(void)\n{\n\tstruct ipsec_uld_ctx *u_ctx, *tmp;\n\tstruct adapter *adap;\n\n\tmutex_lock(&dev_mutex);\n\tlist_for_each_entry_safe(u_ctx, tmp, &uld_ctx_list, entry) {\n\t\tadap = pci_get_drvdata(u_ctx->lldi.pdev);\n\t\tatomic_set(&adap->ch_ipsec_stats.ipsec_cnt, 0);\n\t\tlist_del(&u_ctx->entry);\n\t\tkfree(u_ctx);\n\t}\n\tmutex_unlock(&dev_mutex);\n\tcxgb4_unregister_uld(CXGB4_ULD_IPSEC);\n}\n\nmodule_init(ch_ipsec_init);\nmodule_exit(ch_ipsec_exit);\n\nMODULE_DESCRIPTION(\"Crypto IPSEC for Chelsio Terminator cards.\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Chelsio Communications\");\nMODULE_VERSION(CHIPSEC_DRV_VERSION);\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}