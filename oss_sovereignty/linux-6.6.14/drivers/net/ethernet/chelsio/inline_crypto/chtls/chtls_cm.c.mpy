{
  "module_name": "chtls_cm.c",
  "hash_id": "40468a1129c6f6462cc2140cd60ffa06978d4850bbf1e754d6a0a55f257beedc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/inline_crypto/chtls/chtls_cm.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/list.h>\n#include <linux/workqueue.h>\n#include <linux/skbuff.h>\n#include <linux/timer.h>\n#include <linux/notifier.h>\n#include <linux/inetdevice.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/sched/signal.h>\n#include <linux/kallsyms.h>\n#include <linux/kprobes.h>\n#include <linux/if_vlan.h>\n#include <linux/ipv6.h>\n#include <net/ipv6.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/inet_common.h>\n#include <net/tcp.h>\n#include <net/dst.h>\n#include <net/tls.h>\n#include <net/addrconf.h>\n#include <net/secure_seq.h>\n\n#include \"chtls.h\"\n#include \"chtls_cm.h\"\n#include \"clip_tbl.h\"\n#include \"t4_tcb.h\"\n\n \nstatic unsigned char new_state[16] = {\n\t \n\t  TCP_CLOSE,\n\t  TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n\t  TCP_SYN_SENT,\n\t  TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n\t  TCP_FIN_WAIT1,\n\t  TCP_FIN_WAIT2,\n\t  TCP_CLOSE,\n\t  TCP_CLOSE,\n\t  TCP_LAST_ACK | TCP_ACTION_FIN,\n\t  TCP_LAST_ACK,\n\t  TCP_CLOSE,\n\t  TCP_CLOSING,\n};\n\nstatic struct chtls_sock *chtls_sock_create(struct chtls_dev *cdev)\n{\n\tstruct chtls_sock *csk = kzalloc(sizeof(*csk), GFP_ATOMIC);\n\n\tif (!csk)\n\t\treturn NULL;\n\n\tcsk->txdata_skb_cache = alloc_skb(TXDATA_SKB_LEN, GFP_ATOMIC);\n\tif (!csk->txdata_skb_cache) {\n\t\tkfree(csk);\n\t\treturn NULL;\n\t}\n\n\tkref_init(&csk->kref);\n\tcsk->cdev = cdev;\n\tskb_queue_head_init(&csk->txq);\n\tcsk->wr_skb_head = NULL;\n\tcsk->wr_skb_tail = NULL;\n\tcsk->mss = MAX_MSS;\n\tcsk->tlshws.ofld = 1;\n\tcsk->tlshws.txkey = -1;\n\tcsk->tlshws.rxkey = -1;\n\tcsk->tlshws.mfs = TLS_MFS;\n\tskb_queue_head_init(&csk->tlshws.sk_recv_queue);\n\treturn csk;\n}\n\nstatic void chtls_sock_release(struct kref *ref)\n{\n\tstruct chtls_sock *csk =\n\t\tcontainer_of(ref, struct chtls_sock, kref);\n\n\tkfree(csk);\n}\n\nstatic struct net_device *chtls_find_netdev(struct chtls_dev *cdev,\n\t\t\t\t\t    struct sock *sk)\n{\n\tstruct adapter *adap = pci_get_drvdata(cdev->pdev);\n\tstruct net_device *ndev = cdev->ports[0];\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct net_device *temp;\n\tint addr_type;\n#endif\n\tint i;\n\n\tswitch (sk->sk_family) {\n\tcase PF_INET:\n\t\tif (likely(!inet_sk(sk)->inet_rcv_saddr))\n\t\t\treturn ndev;\n\t\tndev = __ip_dev_find(&init_net, inet_sk(sk)->inet_rcv_saddr, false);\n\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase PF_INET6:\n\t\taddr_type = ipv6_addr_type(&sk->sk_v6_rcv_saddr);\n\t\tif (likely(addr_type == IPV6_ADDR_ANY))\n\t\t\treturn ndev;\n\n\t\tfor_each_netdev_rcu(&init_net, temp) {\n\t\t\tif (ipv6_chk_addr(&init_net, (struct in6_addr *)\n\t\t\t\t\t  &sk->sk_v6_rcv_saddr, temp, 1)) {\n\t\t\t\tndev = temp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\tbreak;\n#endif\n\tdefault:\n\t\treturn NULL;\n\t}\n\n\tif (!ndev)\n\t\treturn NULL;\n\n\tif (is_vlan_dev(ndev))\n\t\tndev = vlan_dev_real_dev(ndev);\n\n\tfor_each_port(adap, i)\n\t\tif (cdev->ports[i] == ndev)\n\t\t\treturn ndev;\n\treturn NULL;\n}\n\nstatic void assign_rxopt(struct sock *sk, unsigned int opt)\n{\n\tconst struct chtls_dev *cdev;\n\tstruct chtls_sock *csk;\n\tstruct tcp_sock *tp;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\ttp = tcp_sk(sk);\n\n\tcdev = csk->cdev;\n\ttp->tcp_header_len           = sizeof(struct tcphdr);\n\ttp->rx_opt.mss_clamp         = cdev->mtus[TCPOPT_MSS_G(opt)] - 40;\n\ttp->mss_cache                = tp->rx_opt.mss_clamp;\n\ttp->rx_opt.tstamp_ok         = TCPOPT_TSTAMP_G(opt);\n\ttp->rx_opt.snd_wscale        = TCPOPT_SACK_G(opt);\n\ttp->rx_opt.wscale_ok         = TCPOPT_WSCALE_OK_G(opt);\n\tSND_WSCALE(tp)               = TCPOPT_SND_WSCALE_G(opt);\n\tif (!tp->rx_opt.wscale_ok)\n\t\ttp->rx_opt.rcv_wscale = 0;\n\tif (tp->rx_opt.tstamp_ok) {\n\t\ttp->tcp_header_len += TCPOLEN_TSTAMP_ALIGNED;\n\t\ttp->rx_opt.mss_clamp -= TCPOLEN_TSTAMP_ALIGNED;\n\t} else if (csk->opt2 & TSTAMPS_EN_F) {\n\t\tcsk->opt2 &= ~TSTAMPS_EN_F;\n\t\tcsk->mtu_idx = TCPOPT_MSS_G(opt);\n\t}\n}\n\nstatic void chtls_purge_receive_queue(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {\n\t\tskb_dst_set(skb, (void *)NULL);\n\t\tkfree_skb(skb);\n\t}\n}\n\nstatic void chtls_purge_write_queue(struct sock *sk)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue(&csk->txq))) {\n\t\tsk->sk_wmem_queued -= skb->truesize;\n\t\t__kfree_skb(skb);\n\t}\n}\n\nstatic void chtls_purge_recv_queue(struct sock *sk)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct chtls_hws *tlsk = &csk->tlshws;\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue(&tlsk->sk_recv_queue)) != NULL) {\n\t\tskb_dst_set(skb, NULL);\n\t\tkfree_skb(skb);\n\t}\n}\n\nstatic void abort_arp_failure(void *handle, struct sk_buff *skb)\n{\n\tstruct cpl_abort_req *req = cplhdr(skb);\n\tstruct chtls_dev *cdev;\n\n\tcdev = (struct chtls_dev *)handle;\n\treq->cmd = CPL_ABORT_NO_RST;\n\tcxgb4_ofld_send(cdev->lldi->ports[0], skb);\n}\n\nstatic struct sk_buff *alloc_ctrl_skb(struct sk_buff *skb, int len)\n{\n\tif (likely(skb && !skb_shared(skb) && !skb_cloned(skb))) {\n\t\t__skb_trim(skb, 0);\n\t\trefcount_inc(&skb->users);\n\t} else {\n\t\tskb = alloc_skb(len, GFP_KERNEL | __GFP_NOFAIL);\n\t}\n\treturn skb;\n}\n\nstatic void chtls_send_abort(struct sock *sk, int mode, struct sk_buff *skb)\n{\n\tstruct cpl_abort_req *req;\n\tstruct chtls_sock *csk;\n\tstruct tcp_sock *tp;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\ttp = tcp_sk(sk);\n\n\tif (!skb)\n\t\tskb = alloc_ctrl_skb(csk->txdata_skb_cache, sizeof(*req));\n\n\treq = (struct cpl_abort_req *)skb_put(skb, sizeof(*req));\n\tINIT_TP_WR_CPL(req, CPL_ABORT_REQ, csk->tid);\n\tskb_set_queue_mapping(skb, (csk->txq_idx << 1) | CPL_PRIORITY_DATA);\n\treq->rsvd0 = htonl(tp->snd_nxt);\n\treq->rsvd1 = !csk_flag_nochk(csk, CSK_TX_DATA_SENT);\n\treq->cmd = mode;\n\tt4_set_arp_err_handler(skb, csk->cdev, abort_arp_failure);\n\tsend_or_defer(sk, tp, skb, mode == CPL_ABORT_SEND_RST);\n}\n\nstatic void chtls_send_reset(struct sock *sk, int mode, struct sk_buff *skb)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\n\tif (unlikely(csk_flag_nochk(csk, CSK_ABORT_SHUTDOWN) ||\n\t\t     !csk->cdev)) {\n\t\tif (sk->sk_state == TCP_SYN_RECV)\n\t\t\tcsk_set_flag(csk, CSK_RST_ABORTED);\n\t\tgoto out;\n\t}\n\n\tif (!csk_flag_nochk(csk, CSK_TX_DATA_SENT)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tif (send_tx_flowc_wr(sk, 0, tp->snd_nxt, tp->rcv_nxt) < 0)\n\t\t\tWARN_ONCE(1, \"send tx flowc error\");\n\t\tcsk_set_flag(csk, CSK_TX_DATA_SENT);\n\t}\n\n\tcsk_set_flag(csk, CSK_ABORT_RPL_PENDING);\n\tchtls_purge_write_queue(sk);\n\n\tcsk_set_flag(csk, CSK_ABORT_SHUTDOWN);\n\tif (sk->sk_state != TCP_SYN_RECV)\n\t\tchtls_send_abort(sk, mode, skb);\n\telse\n\t\tchtls_set_tcb_field_rpl_skb(sk, TCB_T_FLAGS_W,\n\t\t\t\t\t    TCB_T_FLAGS_V(TCB_T_FLAGS_M), 0,\n\t\t\t\t\t    TCB_FIELD_COOKIE_TFLAG, 1);\n\n\treturn;\nout:\n\tkfree_skb(skb);\n}\n\nstatic void release_tcp_port(struct sock *sk)\n{\n\tif (inet_csk(sk)->icsk_bind_hash)\n\t\tinet_put_port(sk);\n}\n\nstatic void tcp_uncork(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->nonagle & TCP_NAGLE_CORK) {\n\t\ttp->nonagle &= ~TCP_NAGLE_CORK;\n\t\tchtls_tcp_push(sk, 0);\n\t}\n}\n\nstatic void chtls_close_conn(struct sock *sk)\n{\n\tstruct cpl_close_con_req *req;\n\tstruct chtls_sock *csk;\n\tstruct sk_buff *skb;\n\tunsigned int tid;\n\tunsigned int len;\n\n\tlen = roundup(sizeof(struct cpl_close_con_req), 16);\n\tcsk = rcu_dereference_sk_user_data(sk);\n\ttid = csk->tid;\n\n\tskb = alloc_skb(len, GFP_KERNEL | __GFP_NOFAIL);\n\treq = (struct cpl_close_con_req *)__skb_put(skb, len);\n\tmemset(req, 0, len);\n\treq->wr.wr_hi = htonl(FW_WR_OP_V(FW_TP_WR) |\n\t\t\t      FW_WR_IMMDLEN_V(sizeof(*req) -\n\t\t\t\t\t      sizeof(req->wr)));\n\treq->wr.wr_mid = htonl(FW_WR_LEN16_V(DIV_ROUND_UP(sizeof(*req), 16)) |\n\t\t\t       FW_WR_FLOWID_V(tid));\n\n\tOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_CLOSE_CON_REQ, tid));\n\n\ttcp_uncork(sk);\n\tskb_entail(sk, skb, ULPCB_FLAG_NO_HDR | ULPCB_FLAG_NO_APPEND);\n\tif (sk->sk_state != TCP_SYN_SENT)\n\t\tchtls_push_frames(csk, 1);\n}\n\n \nstatic int make_close_transition(struct sock *sk)\n{\n\tint next = (int)new_state[sk->sk_state];\n\n\ttcp_set_state(sk, next & TCP_STATE_MASK);\n\treturn next & TCP_ACTION_FIN;\n}\n\nvoid chtls_close(struct sock *sk, long timeout)\n{\n\tint data_lost, prev_state;\n\tstruct chtls_sock *csk;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\n\tlock_sock(sk);\n\tsk->sk_shutdown |= SHUTDOWN_MASK;\n\n\tdata_lost = skb_queue_len(&sk->sk_receive_queue);\n\tdata_lost |= skb_queue_len(&csk->tlshws.sk_recv_queue);\n\tchtls_purge_recv_queue(sk);\n\tchtls_purge_receive_queue(sk);\n\n\tif (sk->sk_state == TCP_CLOSE) {\n\t\tgoto wait;\n\t} else if (data_lost || sk->sk_state == TCP_SYN_SENT) {\n\t\tchtls_send_reset(sk, CPL_ABORT_SEND_RST, NULL);\n\t\trelease_tcp_port(sk);\n\t\tgoto unlock;\n\t} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {\n\t\tsk->sk_prot->disconnect(sk, 0);\n\t} else if (make_close_transition(sk)) {\n\t\tchtls_close_conn(sk);\n\t}\nwait:\n\tif (timeout)\n\t\tsk_stream_wait_close(sk, timeout);\n\nunlock:\n\tprev_state = sk->sk_state;\n\tsock_hold(sk);\n\tsock_orphan(sk);\n\n\trelease_sock(sk);\n\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\n\tif (prev_state != TCP_CLOSE && sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\tif (sk->sk_state == TCP_FIN_WAIT2 && tcp_sk(sk)->linger2 < 0 &&\n\t    !csk_flag(sk, CSK_ABORT_SHUTDOWN)) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = alloc_skb(sizeof(struct cpl_abort_req), GFP_ATOMIC);\n\t\tif (skb)\n\t\t\tchtls_send_reset(sk, CPL_ABORT_SEND_RST, skb);\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tinet_csk_destroy_sock(sk);\n\nout:\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\tsock_put(sk);\n}\n\n \nstatic int wait_for_states(struct sock *sk, unsigned int states)\n{\n\tDECLARE_WAITQUEUE(wait, current);\n\tstruct socket_wq _sk_wq;\n\tlong current_timeo;\n\tint err = 0;\n\n\tcurrent_timeo = 200;\n\n\t \n\tif (!sk->sk_wq) {\n\t\tinit_waitqueue_head(&_sk_wq.wait);\n\t\t_sk_wq.fasync_list = NULL;\n\t\tinit_rcu_head_on_stack(&_sk_wq.rcu);\n\t\tRCU_INIT_POINTER(sk->sk_wq, &_sk_wq);\n\t}\n\n\tadd_wait_queue(sk_sleep(sk), &wait);\n\twhile (!sk_in_state(sk, states)) {\n\t\tif (!current_timeo) {\n\t\t\terr = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\terr = sock_intr_errno(current_timeo);\n\t\t\tbreak;\n\t\t}\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\trelease_sock(sk);\n\t\tif (!sk_in_state(sk, states))\n\t\t\tcurrent_timeo = schedule_timeout(current_timeo);\n\t\t__set_current_state(TASK_RUNNING);\n\t\tlock_sock(sk);\n\t}\n\tremove_wait_queue(sk_sleep(sk), &wait);\n\n\tif (rcu_dereference(sk->sk_wq) == &_sk_wq)\n\t\tsk->sk_wq = NULL;\n\treturn err;\n}\n\nint chtls_disconnect(struct sock *sk, int flags)\n{\n\tstruct tcp_sock *tp;\n\tint err;\n\n\ttp = tcp_sk(sk);\n\tchtls_purge_recv_queue(sk);\n\tchtls_purge_receive_queue(sk);\n\tchtls_purge_write_queue(sk);\n\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\tsk->sk_err = ECONNRESET;\n\t\tchtls_send_reset(sk, CPL_ABORT_SEND_RST, NULL);\n\t\terr = wait_for_states(sk, TCPF_CLOSE);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tchtls_purge_recv_queue(sk);\n\tchtls_purge_receive_queue(sk);\n\ttp->max_window = 0xFFFF << (tp->rx_opt.snd_wscale);\n\treturn tcp_disconnect(sk, flags);\n}\n\n#define SHUTDOWN_ELIGIBLE_STATE (TCPF_ESTABLISHED | \\\n\t\t\t\t TCPF_SYN_RECV | TCPF_CLOSE_WAIT)\nvoid chtls_shutdown(struct sock *sk, int how)\n{\n\tif ((how & SEND_SHUTDOWN) &&\n\t    sk_in_state(sk, SHUTDOWN_ELIGIBLE_STATE) &&\n\t    make_close_transition(sk))\n\t\tchtls_close_conn(sk);\n}\n\nvoid chtls_destroy_sock(struct sock *sk)\n{\n\tstruct chtls_sock *csk;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\tchtls_purge_recv_queue(sk);\n\tcsk->ulp_mode = ULP_MODE_NONE;\n\tchtls_purge_write_queue(sk);\n\tfree_tls_keyid(sk);\n\tkref_put(&csk->kref, chtls_sock_release);\n\tif (sk->sk_family == AF_INET)\n\t\tsk->sk_prot = &tcp_prot;\n#if IS_ENABLED(CONFIG_IPV6)\n\telse\n\t\tsk->sk_prot = &tcpv6_prot;\n#endif\n\tsk->sk_prot->destroy(sk);\n}\n\nstatic void reset_listen_child(struct sock *child)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(child);\n\tstruct sk_buff *skb;\n\n\tskb = alloc_ctrl_skb(csk->txdata_skb_cache,\n\t\t\t     sizeof(struct cpl_abort_req));\n\n\tchtls_send_reset(child, CPL_ABORT_SEND_RST, skb);\n\tsock_orphan(child);\n\tINC_ORPHAN_COUNT(child);\n\tif (child->sk_state == TCP_CLOSE)\n\t\tinet_csk_destroy_sock(child);\n}\n\nstatic void chtls_disconnect_acceptq(struct sock *listen_sk)\n{\n\tstruct request_sock **pprev;\n\n\tpprev = ACCEPT_QUEUE(listen_sk);\n\twhile (*pprev) {\n\t\tstruct request_sock *req = *pprev;\n\n\t\tif (req->rsk_ops == &chtls_rsk_ops ||\n\t\t    req->rsk_ops == &chtls_rsk_opsv6) {\n\t\t\tstruct sock *child = req->sk;\n\n\t\t\t*pprev = req->dl_next;\n\t\t\tsk_acceptq_removed(listen_sk);\n\t\t\treqsk_put(req);\n\t\t\tsock_hold(child);\n\t\t\tlocal_bh_disable();\n\t\t\tbh_lock_sock(child);\n\t\t\trelease_tcp_port(child);\n\t\t\treset_listen_child(child);\n\t\t\tbh_unlock_sock(child);\n\t\t\tlocal_bh_enable();\n\t\t\tsock_put(child);\n\t\t} else {\n\t\t\tpprev = &req->dl_next;\n\t\t}\n\t}\n}\n\nstatic int listen_hashfn(const struct sock *sk)\n{\n\treturn ((unsigned long)sk >> 10) & (LISTEN_INFO_HASH_SIZE - 1);\n}\n\nstatic struct listen_info *listen_hash_add(struct chtls_dev *cdev,\n\t\t\t\t\t   struct sock *sk,\n\t\t\t\t\t   unsigned int stid)\n{\n\tstruct listen_info *p = kmalloc(sizeof(*p), GFP_KERNEL);\n\n\tif (p) {\n\t\tint key = listen_hashfn(sk);\n\n\t\tp->sk = sk;\n\t\tp->stid = stid;\n\t\tspin_lock(&cdev->listen_lock);\n\t\tp->next = cdev->listen_hash_tab[key];\n\t\tcdev->listen_hash_tab[key] = p;\n\t\tspin_unlock(&cdev->listen_lock);\n\t}\n\treturn p;\n}\n\nstatic int listen_hash_find(struct chtls_dev *cdev,\n\t\t\t    struct sock *sk)\n{\n\tstruct listen_info *p;\n\tint stid = -1;\n\tint key;\n\n\tkey = listen_hashfn(sk);\n\n\tspin_lock(&cdev->listen_lock);\n\tfor (p = cdev->listen_hash_tab[key]; p; p = p->next)\n\t\tif (p->sk == sk) {\n\t\t\tstid = p->stid;\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&cdev->listen_lock);\n\treturn stid;\n}\n\nstatic int listen_hash_del(struct chtls_dev *cdev,\n\t\t\t   struct sock *sk)\n{\n\tstruct listen_info *p, **prev;\n\tint stid = -1;\n\tint key;\n\n\tkey = listen_hashfn(sk);\n\tprev = &cdev->listen_hash_tab[key];\n\n\tspin_lock(&cdev->listen_lock);\n\tfor (p = *prev; p; prev = &p->next, p = p->next)\n\t\tif (p->sk == sk) {\n\t\t\tstid = p->stid;\n\t\t\t*prev = p->next;\n\t\t\tkfree(p);\n\t\t\tbreak;\n\t\t}\n\tspin_unlock(&cdev->listen_lock);\n\treturn stid;\n}\n\nstatic void cleanup_syn_rcv_conn(struct sock *child, struct sock *parent)\n{\n\tstruct request_sock *req;\n\tstruct chtls_sock *csk;\n\n\tcsk = rcu_dereference_sk_user_data(child);\n\treq = csk->passive_reap_next;\n\n\treqsk_queue_removed(&inet_csk(parent)->icsk_accept_queue, req);\n\t__skb_unlink((struct sk_buff *)&csk->synq, &csk->listen_ctx->synq);\n\tchtls_reqsk_free(req);\n\tcsk->passive_reap_next = NULL;\n}\n\nstatic void chtls_reset_synq(struct listen_ctx *listen_ctx)\n{\n\tstruct sock *listen_sk = listen_ctx->lsk;\n\n\twhile (!skb_queue_empty(&listen_ctx->synq)) {\n\t\tstruct chtls_sock *csk =\n\t\t\tcontainer_of((struct synq *)skb_peek\n\t\t\t\t(&listen_ctx->synq), struct chtls_sock, synq);\n\t\tstruct sock *child = csk->sk;\n\n\t\tcleanup_syn_rcv_conn(child, listen_sk);\n\t\tsock_hold(child);\n\t\tlocal_bh_disable();\n\t\tbh_lock_sock(child);\n\t\trelease_tcp_port(child);\n\t\treset_listen_child(child);\n\t\tbh_unlock_sock(child);\n\t\tlocal_bh_enable();\n\t\tsock_put(child);\n\t}\n}\n\nint chtls_listen_start(struct chtls_dev *cdev, struct sock *sk)\n{\n\tstruct net_device *ndev;\n#if IS_ENABLED(CONFIG_IPV6)\n\tbool clip_valid = false;\n#endif\n\tstruct listen_ctx *ctx;\n\tstruct adapter *adap;\n\tstruct port_info *pi;\n\tint ret = 0;\n\tint stid;\n\n\trcu_read_lock();\n\tndev = chtls_find_netdev(cdev, sk);\n\trcu_read_unlock();\n\tif (!ndev)\n\t\treturn -EBADF;\n\n\tpi = netdev_priv(ndev);\n\tadap = pi->adapter;\n\tif (!(adap->flags & CXGB4_FULL_INIT_DONE))\n\t\treturn -EBADF;\n\n\tif (listen_hash_find(cdev, sk) >= 0)    \n\t\treturn -EADDRINUSE;\n\n\tctx = kmalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\t__module_get(THIS_MODULE);\n\tctx->lsk = sk;\n\tctx->cdev = cdev;\n\tctx->state = T4_LISTEN_START_PENDING;\n\tskb_queue_head_init(&ctx->synq);\n\n\tstid = cxgb4_alloc_stid(cdev->tids, sk->sk_family, ctx);\n\tif (stid < 0)\n\t\tgoto free_ctx;\n\n\tsock_hold(sk);\n\tif (!listen_hash_add(cdev, sk, stid))\n\t\tgoto free_stid;\n\n\tif (sk->sk_family == PF_INET) {\n\t\tret = cxgb4_create_server(ndev, stid,\n\t\t\t\t\t  inet_sk(sk)->inet_rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_sport, 0,\n\t\t\t\t\t  cdev->lldi->rxq_ids[0]);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else {\n\t\tint addr_type;\n\n\t\taddr_type = ipv6_addr_type(&sk->sk_v6_rcv_saddr);\n\t\tif (addr_type != IPV6_ADDR_ANY) {\n\t\t\tret = cxgb4_clip_get(ndev, (const u32 *)\n\t\t\t\t\t     &sk->sk_v6_rcv_saddr, 1);\n\t\t\tif (ret)\n\t\t\t\tgoto del_hash;\n\t\t\tclip_valid = true;\n\t\t}\n\t\tret = cxgb4_create_server6(ndev, stid,\n\t\t\t\t\t   &sk->sk_v6_rcv_saddr,\n\t\t\t\t\t   inet_sk(sk)->inet_sport,\n\t\t\t\t\t   cdev->lldi->rxq_ids[0]);\n#endif\n\t}\n\tif (ret > 0)\n\t\tret = net_xmit_errno(ret);\n\tif (ret)\n\t\tgoto del_hash;\n\treturn 0;\ndel_hash:\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (clip_valid)\n\t\tcxgb4_clip_release(ndev, (const u32 *)&sk->sk_v6_rcv_saddr, 1);\n#endif\n\tlisten_hash_del(cdev, sk);\nfree_stid:\n\tcxgb4_free_stid(cdev->tids, stid, sk->sk_family);\n\tsock_put(sk);\nfree_ctx:\n\tkfree(ctx);\n\tmodule_put(THIS_MODULE);\n\treturn -EBADF;\n}\n\nvoid chtls_listen_stop(struct chtls_dev *cdev, struct sock *sk)\n{\n\tstruct listen_ctx *listen_ctx;\n\tint stid;\n\n\tstid = listen_hash_del(cdev, sk);\n\tif (stid < 0)\n\t\treturn;\n\n\tlisten_ctx = (struct listen_ctx *)lookup_stid(cdev->tids, stid);\n\tchtls_reset_synq(listen_ctx);\n\n\tcxgb4_remove_server(cdev->lldi->ports[0], stid,\n\t\t\t    cdev->lldi->rxq_ids[0], sk->sk_family == PF_INET6);\n\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == PF_INET6) {\n\t\tstruct net_device *ndev = chtls_find_netdev(cdev, sk);\n\t\tint addr_type = 0;\n\n\t\taddr_type = ipv6_addr_type((const struct in6_addr *)\n\t\t\t\t\t  &sk->sk_v6_rcv_saddr);\n\t\tif (addr_type != IPV6_ADDR_ANY)\n\t\t\tcxgb4_clip_release(ndev, (const u32 *)\n\t\t\t\t\t   &sk->sk_v6_rcv_saddr, 1);\n\t}\n#endif\n\tchtls_disconnect_acceptq(sk);\n}\n\nstatic int chtls_pass_open_rpl(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_pass_open_rpl *rpl = cplhdr(skb) + RSS_HDR;\n\tunsigned int stid = GET_TID(rpl);\n\tstruct listen_ctx *listen_ctx;\n\n\tlisten_ctx = (struct listen_ctx *)lookup_stid(cdev->tids, stid);\n\tif (!listen_ctx)\n\t\treturn CPL_RET_BUF_DONE;\n\n\tif (listen_ctx->state == T4_LISTEN_START_PENDING) {\n\t\tlisten_ctx->state = T4_LISTEN_STARTED;\n\t\treturn CPL_RET_BUF_DONE;\n\t}\n\n\tif (rpl->status != CPL_ERR_NONE) {\n\t\tpr_info(\"Unexpected PASS_OPEN_RPL status %u for STID %u\\n\",\n\t\t\trpl->status, stid);\n\t} else {\n\t\tcxgb4_free_stid(cdev->tids, stid, listen_ctx->lsk->sk_family);\n\t\tsock_put(listen_ctx->lsk);\n\t\tkfree(listen_ctx);\n\t\tmodule_put(THIS_MODULE);\n\t}\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic int chtls_close_listsrv_rpl(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_close_listsvr_rpl *rpl = cplhdr(skb) + RSS_HDR;\n\tstruct listen_ctx *listen_ctx;\n\tunsigned int stid;\n\tvoid *data;\n\n\tstid = GET_TID(rpl);\n\tdata = lookup_stid(cdev->tids, stid);\n\tlisten_ctx = (struct listen_ctx *)data;\n\n\tif (rpl->status != CPL_ERR_NONE) {\n\t\tpr_info(\"Unexpected CLOSE_LISTSRV_RPL status %u for STID %u\\n\",\n\t\t\trpl->status, stid);\n\t} else {\n\t\tcxgb4_free_stid(cdev->tids, stid, listen_ctx->lsk->sk_family);\n\t\tsock_put(listen_ctx->lsk);\n\t\tkfree(listen_ctx);\n\t\tmodule_put(THIS_MODULE);\n\t}\n\treturn CPL_RET_BUF_DONE;\n}\n\nstatic void chtls_purge_wr_queue(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = dequeue_wr(sk)) != NULL)\n\t\tkfree_skb(skb);\n}\n\nstatic void chtls_release_resources(struct sock *sk)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct chtls_dev *cdev = csk->cdev;\n\tunsigned int tid = csk->tid;\n\tstruct tid_info *tids;\n\n\tif (!cdev)\n\t\treturn;\n\n\ttids = cdev->tids;\n\tkfree_skb(csk->txdata_skb_cache);\n\tcsk->txdata_skb_cache = NULL;\n\n\tif (csk->wr_credits != csk->wr_max_credits) {\n\t\tchtls_purge_wr_queue(sk);\n\t\tchtls_reset_wr_list(csk);\n\t}\n\n\tif (csk->l2t_entry) {\n\t\tcxgb4_l2t_release(csk->l2t_entry);\n\t\tcsk->l2t_entry = NULL;\n\t}\n\n\tif (sk->sk_state != TCP_SYN_SENT) {\n\t\tcxgb4_remove_tid(tids, csk->port_id, tid, sk->sk_family);\n\t\tsock_put(sk);\n\t}\n}\n\nstatic void chtls_conn_done(struct sock *sk)\n{\n\tif (sock_flag(sk, SOCK_DEAD))\n\t\tchtls_purge_receive_queue(sk);\n\tsk_wakeup_sleepers(sk, 0);\n\ttcp_done(sk);\n}\n\nstatic void do_abort_syn_rcv(struct sock *child, struct sock *parent)\n{\n\t \n\tif (likely(parent->sk_state == TCP_LISTEN)) {\n\t\tcleanup_syn_rcv_conn(child, parent);\n\t\t \n\t\tsock_orphan(child);\n\t\tINC_ORPHAN_COUNT(child);\n\t\tchtls_release_resources(child);\n\t\tchtls_conn_done(child);\n\t} else {\n\t\tif (csk_flag(child, CSK_RST_ABORTED)) {\n\t\t\tchtls_release_resources(child);\n\t\t\tchtls_conn_done(child);\n\t\t}\n\t}\n}\n\nstatic void pass_open_abort(struct sock *child, struct sock *parent,\n\t\t\t    struct sk_buff *skb)\n{\n\tdo_abort_syn_rcv(child, parent);\n\tkfree_skb(skb);\n}\n\nstatic void bl_pass_open_abort(struct sock *lsk, struct sk_buff *skb)\n{\n\tpass_open_abort(skb->sk, lsk, skb);\n}\n\nstatic void chtls_pass_open_arp_failure(struct sock *sk,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tconst struct request_sock *oreq;\n\tstruct chtls_sock *csk;\n\tstruct chtls_dev *cdev;\n\tstruct sock *parent;\n\tvoid *data;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\tcdev = csk->cdev;\n\n\t \n\tif (csk_flag(sk, CSK_ABORT_RPL_PENDING)) {\n\t\tkfree_skb(skb);\n\t\treturn;\n\t}\n\n\toreq = csk->passive_reap_next;\n\tdata = lookup_stid(cdev->tids, oreq->ts_recent);\n\tparent = ((struct listen_ctx *)data)->lsk;\n\n\tbh_lock_sock(parent);\n\tif (!sock_owned_by_user(parent)) {\n\t\tpass_open_abort(sk, parent, skb);\n\t} else {\n\t\tBLOG_SKB_CB(skb)->backlog_rcv = bl_pass_open_abort;\n\t\t__sk_add_backlog(parent, skb);\n\t}\n\tbh_unlock_sock(parent);\n}\n\nstatic void chtls_accept_rpl_arp_failure(void *handle,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tstruct sock *sk = (struct sock *)handle;\n\n\tsock_hold(sk);\n\tprocess_cpl_msg(chtls_pass_open_arp_failure, sk, skb);\n\tsock_put(sk);\n}\n\nstatic unsigned int chtls_select_mss(const struct chtls_sock *csk,\n\t\t\t\t     unsigned int pmtu,\n\t\t\t\t     struct cpl_pass_accept_req *req)\n{\n\tstruct chtls_dev *cdev;\n\tstruct dst_entry *dst;\n\tunsigned int tcpoptsz;\n\tunsigned int iphdrsz;\n\tunsigned int mtu_idx;\n\tstruct tcp_sock *tp;\n\tunsigned int mss;\n\tstruct sock *sk;\n\n\tmss = ntohs(req->tcpopt.mss);\n\tsk = csk->sk;\n\tdst = __sk_dst_get(sk);\n\tcdev = csk->cdev;\n\ttp = tcp_sk(sk);\n\ttcpoptsz = 0;\n\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == AF_INET6)\n\t\tiphdrsz = sizeof(struct ipv6hdr) + sizeof(struct tcphdr);\n\telse\n#endif\n\t\tiphdrsz = sizeof(struct iphdr) + sizeof(struct tcphdr);\n\tif (req->tcpopt.tstamp)\n\t\ttcpoptsz += round_up(TCPOLEN_TIMESTAMP, 4);\n\n\ttp->advmss = dst_metric_advmss(dst);\n\tif (USER_MSS(tp) && tp->advmss > USER_MSS(tp))\n\t\ttp->advmss = USER_MSS(tp);\n\tif (tp->advmss > pmtu - iphdrsz)\n\t\ttp->advmss = pmtu - iphdrsz;\n\tif (mss && tp->advmss > mss)\n\t\ttp->advmss = mss;\n\n\ttp->advmss = cxgb4_best_aligned_mtu(cdev->lldi->mtus,\n\t\t\t\t\t    iphdrsz + tcpoptsz,\n\t\t\t\t\t    tp->advmss - tcpoptsz,\n\t\t\t\t\t    8, &mtu_idx);\n\ttp->advmss -= iphdrsz;\n\n\tinet_csk(sk)->icsk_pmtu_cookie = pmtu;\n\treturn mtu_idx;\n}\n\nstatic unsigned int select_rcv_wscale(int space, int wscale_ok, int win_clamp)\n{\n\tint wscale = 0;\n\n\tif (space > MAX_RCV_WND)\n\t\tspace = MAX_RCV_WND;\n\tif (win_clamp && win_clamp < space)\n\t\tspace = win_clamp;\n\n\tif (wscale_ok) {\n\t\twhile (wscale < 14 && (65535 << wscale) < space)\n\t\t\twscale++;\n\t}\n\treturn wscale;\n}\n\nstatic void chtls_pass_accept_rpl(struct sk_buff *skb,\n\t\t\t\t  struct cpl_pass_accept_req *req,\n\t\t\t\t  unsigned int tid)\n\n{\n\tstruct cpl_t5_pass_accept_rpl *rpl5;\n\tstruct cxgb4_lld_info *lldi;\n\tconst struct tcphdr *tcph;\n\tconst struct tcp_sock *tp;\n\tstruct chtls_sock *csk;\n\tunsigned int len;\n\tstruct sock *sk;\n\tu32 opt2, hlen;\n\tu64 opt0;\n\n\tsk = skb->sk;\n\ttp = tcp_sk(sk);\n\tcsk = sk->sk_user_data;\n\tcsk->tid = tid;\n\tlldi = csk->cdev->lldi;\n\tlen = roundup(sizeof(*rpl5), 16);\n\n\trpl5 = __skb_put_zero(skb, len);\n\tINIT_TP_WR(rpl5, tid);\n\n\tOPCODE_TID(rpl5) = cpu_to_be32(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL,\n\t\t\t\t\t\t     csk->tid));\n\tcsk->mtu_idx = chtls_select_mss(csk, dst_mtu(__sk_dst_get(sk)),\n\t\t\t\t\treq);\n\topt0 = TCAM_BYPASS_F |\n\t       WND_SCALE_V(RCV_WSCALE(tp)) |\n\t       MSS_IDX_V(csk->mtu_idx) |\n\t       L2T_IDX_V(csk->l2t_entry->idx) |\n\t       NAGLE_V(!(tp->nonagle & TCP_NAGLE_OFF)) |\n\t       TX_CHAN_V(csk->tx_chan) |\n\t       SMAC_SEL_V(csk->smac_idx) |\n\t       DSCP_V(csk->tos >> 2) |\n\t       ULP_MODE_V(ULP_MODE_TLS) |\n\t       RCV_BUFSIZ_V(min(tp->rcv_wnd >> 10, RCV_BUFSIZ_M));\n\n\topt2 = RX_CHANNEL_V(0) |\n\t\tRSS_QUEUE_VALID_F | RSS_QUEUE_V(csk->rss_qid);\n\n\tif (!is_t5(lldi->adapter_type))\n\t\topt2 |= RX_FC_DISABLE_F;\n\tif (req->tcpopt.tstamp)\n\t\topt2 |= TSTAMPS_EN_F;\n\tif (req->tcpopt.sack)\n\t\topt2 |= SACK_EN_F;\n\thlen = ntohl(req->hdr_len);\n\n\ttcph = (struct tcphdr *)((u8 *)(req + 1) +\n\t\t\tT6_ETH_HDR_LEN_G(hlen) + T6_IP_HDR_LEN_G(hlen));\n\tif (tcph->ece && tcph->cwr)\n\t\topt2 |= CCTRL_ECN_V(1);\n\topt2 |= CONG_CNTRL_V(CONG_ALG_NEWRENO);\n\topt2 |= T5_ISS_F;\n\topt2 |= T5_OPT_2_VALID_F;\n\topt2 |= WND_SCALE_EN_V(WSCALE_OK(tp));\n\trpl5->opt0 = cpu_to_be64(opt0);\n\trpl5->opt2 = cpu_to_be32(opt2);\n\trpl5->iss = cpu_to_be32((get_random_u32() & ~7UL) - 1);\n\tset_wr_txq(skb, CPL_PRIORITY_SETUP, csk->port_id);\n\tt4_set_arp_err_handler(skb, sk, chtls_accept_rpl_arp_failure);\n\tcxgb4_l2t_send(csk->egress_dev, skb, csk->l2t_entry);\n}\n\nstatic void inet_inherit_port(struct sock *lsk, struct sock *newsk)\n{\n\tlocal_bh_disable();\n\t__inet_inherit_port(lsk, newsk);\n\tlocal_bh_enable();\n}\n\nstatic int chtls_backlog_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tif (skb->protocol) {\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\tBLOG_SKB_CB(skb)->backlog_rcv(sk, skb);\n\treturn 0;\n}\n\nstatic void chtls_set_tcp_window(struct chtls_sock *csk)\n{\n\tstruct net_device *ndev = csk->egress_dev;\n\tstruct port_info *pi = netdev_priv(ndev);\n\tunsigned int linkspeed;\n\tu8 scale;\n\n\tlinkspeed = pi->link_cfg.speed;\n\tscale = linkspeed / SPEED_10000;\n#define CHTLS_10G_RCVWIN (256 * 1024)\n\tcsk->rcv_win = CHTLS_10G_RCVWIN;\n\tif (scale)\n\t\tcsk->rcv_win *= scale;\n#define CHTLS_10G_SNDWIN (256 * 1024)\n\tcsk->snd_win = CHTLS_10G_SNDWIN;\n\tif (scale)\n\t\tcsk->snd_win *= scale;\n}\n\nstatic struct sock *chtls_recv_sock(struct sock *lsk,\n\t\t\t\t    struct request_sock *oreq,\n\t\t\t\t    void *network_hdr,\n\t\t\t\t    const struct cpl_pass_accept_req *req,\n\t\t\t\t    struct chtls_dev *cdev)\n{\n\tstruct adapter *adap = pci_get_drvdata(cdev->pdev);\n\tstruct neighbour *n = NULL;\n\tstruct inet_sock *newinet;\n\tconst struct iphdr *iph;\n\tstruct tls_context *ctx;\n\tstruct net_device *ndev;\n\tstruct chtls_sock *csk;\n\tstruct dst_entry *dst;\n\tstruct tcp_sock *tp;\n\tstruct sock *newsk;\n\tbool found = false;\n\tu16 port_id;\n\tint rxq_idx;\n\tint step, i;\n\n\tiph = (const struct iphdr *)network_hdr;\n\tnewsk = tcp_create_openreq_child(lsk, oreq, cdev->askb);\n\tif (!newsk)\n\t\tgoto free_oreq;\n\n\tif (lsk->sk_family == AF_INET) {\n\t\tdst = inet_csk_route_child_sock(lsk, newsk, oreq);\n\t\tif (!dst)\n\t\t\tgoto free_sk;\n\n\t\tn = dst_neigh_lookup(dst, &iph->saddr);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else {\n\t\tconst struct ipv6hdr *ip6h;\n\t\tstruct flowi6 fl6;\n\n\t\tip6h = (const struct ipv6hdr *)network_hdr;\n\t\tmemset(&fl6, 0, sizeof(fl6));\n\t\tfl6.flowi6_proto = IPPROTO_TCP;\n\t\tfl6.saddr = ip6h->daddr;\n\t\tfl6.daddr = ip6h->saddr;\n\t\tfl6.fl6_dport = inet_rsk(oreq)->ir_rmt_port;\n\t\tfl6.fl6_sport = htons(inet_rsk(oreq)->ir_num);\n\t\tsecurity_req_classify_flow(oreq, flowi6_to_flowi_common(&fl6));\n\t\tdst = ip6_dst_lookup_flow(sock_net(lsk), lsk, &fl6, NULL);\n\t\tif (IS_ERR(dst))\n\t\t\tgoto free_sk;\n\t\tn = dst_neigh_lookup(dst, &ip6h->saddr);\n#endif\n\t}\n\tif (!n || !n->dev)\n\t\tgoto free_dst;\n\n\tndev = n->dev;\n\tif (is_vlan_dev(ndev))\n\t\tndev = vlan_dev_real_dev(ndev);\n\n\tfor_each_port(adap, i)\n\t\tif (cdev->ports[i] == ndev)\n\t\t\tfound = true;\n\n\tif (!found)\n\t\tgoto free_dst;\n\n\tport_id = cxgb4_port_idx(ndev);\n\n\tcsk = chtls_sock_create(cdev);\n\tif (!csk)\n\t\tgoto free_dst;\n\n\tcsk->l2t_entry = cxgb4_l2t_get(cdev->lldi->l2t, n, ndev, 0);\n\tif (!csk->l2t_entry)\n\t\tgoto free_csk;\n\n\tnewsk->sk_user_data = csk;\n\tnewsk->sk_backlog_rcv = chtls_backlog_rcv;\n\n\ttp = tcp_sk(newsk);\n\tnewinet = inet_sk(newsk);\n\n\tif (iph->version == 0x4) {\n\t\tnewinet->inet_daddr = iph->saddr;\n\t\tnewinet->inet_rcv_saddr = iph->daddr;\n\t\tnewinet->inet_saddr = iph->daddr;\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else {\n\t\tstruct tcp6_sock *newtcp6sk = (struct tcp6_sock *)newsk;\n\t\tstruct inet_request_sock *treq = inet_rsk(oreq);\n\t\tstruct ipv6_pinfo *newnp = inet6_sk(newsk);\n\t\tstruct ipv6_pinfo *np = inet6_sk(lsk);\n\n\t\tinet_sk(newsk)->pinet6 = &newtcp6sk->inet6;\n\t\tmemcpy(newnp, np, sizeof(struct ipv6_pinfo));\n\t\tnewsk->sk_v6_daddr = treq->ir_v6_rmt_addr;\n\t\tnewsk->sk_v6_rcv_saddr = treq->ir_v6_loc_addr;\n\t\tinet6_sk(newsk)->saddr = treq->ir_v6_loc_addr;\n\t\tnewnp->ipv6_fl_list = NULL;\n\t\tnewnp->pktoptions = NULL;\n\t\tnewsk->sk_bound_dev_if = treq->ir_iif;\n\t\tnewinet->inet_opt = NULL;\n\t\tnewinet->inet_daddr = LOOPBACK4_IPV6;\n\t\tnewinet->inet_saddr = LOOPBACK4_IPV6;\n#endif\n\t}\n\n\toreq->ts_recent = PASS_OPEN_TID_G(ntohl(req->tos_stid));\n\tsk_setup_caps(newsk, dst);\n\tctx = tls_get_ctx(lsk);\n\tnewsk->sk_destruct = ctx->sk_destruct;\n\tnewsk->sk_prot_creator = lsk->sk_prot_creator;\n\tcsk->sk = newsk;\n\tcsk->passive_reap_next = oreq;\n\tcsk->tx_chan = cxgb4_port_chan(ndev);\n\tcsk->port_id = port_id;\n\tcsk->egress_dev = ndev;\n\tcsk->tos = PASS_OPEN_TOS_G(ntohl(req->tos_stid));\n\tchtls_set_tcp_window(csk);\n\ttp->rcv_wnd = csk->rcv_win;\n\tcsk->sndbuf = csk->snd_win;\n\tcsk->ulp_mode = ULP_MODE_TLS;\n\tstep = cdev->lldi->nrxq / cdev->lldi->nchan;\n\trxq_idx = port_id * step;\n\trxq_idx += cdev->round_robin_cnt++ % step;\n\tcsk->rss_qid = cdev->lldi->rxq_ids[rxq_idx];\n\tcsk->txq_idx = (rxq_idx < cdev->lldi->ntxq) ? rxq_idx :\n\t\t\tport_id * step;\n\tcsk->sndbuf = newsk->sk_sndbuf;\n\tcsk->smac_idx = ((struct port_info *)netdev_priv(ndev))->smt_idx;\n\tRCV_WSCALE(tp) = select_rcv_wscale(tcp_full_space(newsk),\n\t\t\t\t\t   READ_ONCE(sock_net(newsk)->\n\t\t\t\t\t\t     ipv4.sysctl_tcp_window_scaling),\n\t\t\t\t\t   tp->window_clamp);\n\tneigh_release(n);\n\tinet_inherit_port(lsk, newsk);\n\tcsk_set_flag(csk, CSK_CONN_INLINE);\n\tbh_unlock_sock(newsk);  \n\n\treturn newsk;\nfree_csk:\n\tchtls_sock_release(&csk->kref);\nfree_dst:\n\tif (n)\n\t\tneigh_release(n);\n\tdst_release(dst);\nfree_sk:\n\tinet_csk_prepare_forced_close(newsk);\n\ttcp_done(newsk);\nfree_oreq:\n\tchtls_reqsk_free(oreq);\n\treturn NULL;\n}\n\n \nstatic  void mk_tid_release(struct sk_buff *skb,\n\t\t\t    unsigned int chan, unsigned int tid)\n{\n\tstruct cpl_tid_release *req;\n\tunsigned int len;\n\n\tlen = roundup(sizeof(struct cpl_tid_release), 16);\n\treq = (struct cpl_tid_release *)__skb_put(skb, len);\n\tmemset(req, 0, len);\n\tset_wr_txq(skb, CPL_PRIORITY_SETUP, chan);\n\tINIT_TP_WR_CPL(req, CPL_TID_RELEASE, tid);\n}\n\nstatic int chtls_get_module(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (!try_module_get(icsk->icsk_ulp_ops->owner))\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic void chtls_pass_accept_request(struct sock *sk,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tstruct cpl_t5_pass_accept_rpl *rpl;\n\tstruct cpl_pass_accept_req *req;\n\tstruct listen_ctx *listen_ctx;\n\tstruct vlan_ethhdr *vlan_eh;\n\tstruct request_sock *oreq;\n\tstruct sk_buff *reply_skb;\n\tstruct chtls_sock *csk;\n\tstruct chtls_dev *cdev;\n\tstruct ipv6hdr *ip6h;\n\tstruct tcphdr *tcph;\n\tstruct sock *newsk;\n\tstruct ethhdr *eh;\n\tstruct iphdr *iph;\n\tvoid *network_hdr;\n\tunsigned int stid;\n\tunsigned int len;\n\tunsigned int tid;\n\tbool th_ecn, ect;\n\t__u8 ip_dsfield;  \n\tu16 eth_hdr_len;\n\tbool ecn_ok;\n\n\treq = cplhdr(skb) + RSS_HDR;\n\ttid = GET_TID(req);\n\tcdev = BLOG_SKB_CB(skb)->cdev;\n\tnewsk = lookup_tid(cdev->tids, tid);\n\tstid = PASS_OPEN_TID_G(ntohl(req->tos_stid));\n\tif (newsk) {\n\t\tpr_info(\"tid (%d) already in use\\n\", tid);\n\t\treturn;\n\t}\n\n\tlen = roundup(sizeof(*rpl), 16);\n\treply_skb = alloc_skb(len, GFP_ATOMIC);\n\tif (!reply_skb) {\n\t\tcxgb4_remove_tid(cdev->tids, 0, tid, sk->sk_family);\n\t\tkfree_skb(skb);\n\t\treturn;\n\t}\n\n\tif (sk->sk_state != TCP_LISTEN)\n\t\tgoto reject;\n\n\tif (inet_csk_reqsk_queue_is_full(sk))\n\t\tgoto reject;\n\n\tif (sk_acceptq_is_full(sk))\n\t\tgoto reject;\n\n\n\teth_hdr_len = T6_ETH_HDR_LEN_G(ntohl(req->hdr_len));\n\tif (eth_hdr_len == ETH_HLEN) {\n\t\teh = (struct ethhdr *)(req + 1);\n\t\tiph = (struct iphdr *)(eh + 1);\n\t\tip6h = (struct ipv6hdr *)(eh + 1);\n\t\tnetwork_hdr = (void *)(eh + 1);\n\t} else {\n\t\tvlan_eh = (struct vlan_ethhdr *)(req + 1);\n\t\tiph = (struct iphdr *)(vlan_eh + 1);\n\t\tip6h = (struct ipv6hdr *)(vlan_eh + 1);\n\t\tnetwork_hdr = (void *)(vlan_eh + 1);\n\t}\n\n\tif (iph->version == 0x4) {\n\t\ttcph = (struct tcphdr *)(iph + 1);\n\t\tskb_set_network_header(skb, (void *)iph - (void *)req);\n\t\toreq = inet_reqsk_alloc(&chtls_rsk_ops, sk, true);\n\t} else {\n\t\ttcph = (struct tcphdr *)(ip6h + 1);\n\t\tskb_set_network_header(skb, (void *)ip6h - (void *)req);\n\t\toreq = inet_reqsk_alloc(&chtls_rsk_opsv6, sk, false);\n\t}\n\n\tif (!oreq)\n\t\tgoto reject;\n\n\toreq->rsk_rcv_wnd = 0;\n\toreq->rsk_window_clamp = 0;\n\toreq->syncookie = 0;\n\toreq->mss = 0;\n\toreq->ts_recent = 0;\n\n\ttcp_rsk(oreq)->tfo_listener = false;\n\ttcp_rsk(oreq)->rcv_isn = ntohl(tcph->seq);\n\tchtls_set_req_port(oreq, tcph->source, tcph->dest);\n\tif (iph->version == 0x4) {\n\t\tchtls_set_req_addr(oreq, iph->daddr, iph->saddr);\n\t\tip_dsfield = ipv4_get_dsfield(iph);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else {\n\t\tinet_rsk(oreq)->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;\n\t\tinet_rsk(oreq)->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;\n\t\tip_dsfield = ipv6_get_dsfield(ipv6_hdr(skb));\n#endif\n\t}\n\tif (req->tcpopt.wsf <= 14 &&\n\t    READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_window_scaling)) {\n\t\tinet_rsk(oreq)->wscale_ok = 1;\n\t\tinet_rsk(oreq)->snd_wscale = req->tcpopt.wsf;\n\t}\n\tinet_rsk(oreq)->ir_iif = sk->sk_bound_dev_if;\n\tth_ecn = tcph->ece && tcph->cwr;\n\tif (th_ecn) {\n\t\tect = !INET_ECN_is_not_ect(ip_dsfield);\n\t\tecn_ok = READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_ecn);\n\t\tif ((!ect && ecn_ok) || tcp_ca_needs_ecn(sk))\n\t\t\tinet_rsk(oreq)->ecn_ok = 1;\n\t}\n\n\tnewsk = chtls_recv_sock(sk, oreq, network_hdr, req, cdev);\n\tif (!newsk)\n\t\tgoto reject;\n\n\tif (chtls_get_module(newsk))\n\t\tgoto reject;\n\tinet_csk_reqsk_queue_added(sk);\n\treply_skb->sk = newsk;\n\tchtls_install_cpl_ops(newsk);\n\tcxgb4_insert_tid(cdev->tids, newsk, tid, newsk->sk_family);\n\tcsk = rcu_dereference_sk_user_data(newsk);\n\tlisten_ctx = (struct listen_ctx *)lookup_stid(cdev->tids, stid);\n\tcsk->listen_ctx = listen_ctx;\n\t__skb_queue_tail(&listen_ctx->synq, (struct sk_buff *)&csk->synq);\n\tchtls_pass_accept_rpl(reply_skb, req, tid);\n\tkfree_skb(skb);\n\treturn;\n\nreject:\n\tmk_tid_release(reply_skb, 0, tid);\n\tcxgb4_ofld_send(cdev->lldi->ports[0], reply_skb);\n\tkfree_skb(skb);\n}\n\n \nstatic int chtls_pass_accept_req(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_pass_accept_req *req = cplhdr(skb) + RSS_HDR;\n\tstruct listen_ctx *ctx;\n\tunsigned int stid;\n\tunsigned int tid;\n\tstruct sock *lsk;\n\tvoid *data;\n\n\tstid = PASS_OPEN_TID_G(ntohl(req->tos_stid));\n\ttid = GET_TID(req);\n\n\tdata = lookup_stid(cdev->tids, stid);\n\tif (!data)\n\t\treturn 1;\n\n\tctx = (struct listen_ctx *)data;\n\tlsk = ctx->lsk;\n\n\tif (unlikely(tid_out_of_range(cdev->tids, tid))) {\n\t\tpr_info(\"passive open TID %u too large\\n\", tid);\n\t\treturn 1;\n\t}\n\n\tBLOG_SKB_CB(skb)->cdev = cdev;\n\tprocess_cpl_msg(chtls_pass_accept_request, lsk, skb);\n\treturn 0;\n}\n\n \nstatic void make_established(struct sock *sk, u32 snd_isn, unsigned int opt)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->pushed_seq = snd_isn;\n\ttp->write_seq = snd_isn;\n\ttp->snd_nxt = snd_isn;\n\ttp->snd_una = snd_isn;\n\tatomic_set(&inet_sk(sk)->inet_id, get_random_u16());\n\tassign_rxopt(sk, opt);\n\n\tif (tp->rcv_wnd > (RCV_BUFSIZ_M << 10))\n\t\ttp->rcv_wup -= tp->rcv_wnd - (RCV_BUFSIZ_M << 10);\n\n\tsmp_mb();\n\ttcp_set_state(sk, TCP_ESTABLISHED);\n}\n\nstatic void chtls_abort_conn(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct sk_buff *abort_skb;\n\n\tabort_skb = alloc_skb(sizeof(struct cpl_abort_req), GFP_ATOMIC);\n\tif (abort_skb)\n\t\tchtls_send_reset(sk, CPL_ABORT_SEND_RST, abort_skb);\n}\n\nstatic struct sock *reap_list;\nstatic DEFINE_SPINLOCK(reap_list_lock);\n\n \nDECLARE_TASK_FUNC(process_reap_list, task_param)\n{\n\tspin_lock_bh(&reap_list_lock);\n\twhile (reap_list) {\n\t\tstruct sock *sk = reap_list;\n\t\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\n\t\treap_list = csk->passive_reap_next;\n\t\tcsk->passive_reap_next = NULL;\n\t\tspin_unlock(&reap_list_lock);\n\t\tsock_hold(sk);\n\n\t\tbh_lock_sock(sk);\n\t\tchtls_abort_conn(sk, NULL);\n\t\tsock_orphan(sk);\n\t\tif (sk->sk_state == TCP_CLOSE)\n\t\t\tinet_csk_destroy_sock(sk);\n\t\tbh_unlock_sock(sk);\n\t\tsock_put(sk);\n\t\tspin_lock(&reap_list_lock);\n\t}\n\tspin_unlock_bh(&reap_list_lock);\n}\n\nstatic DECLARE_WORK(reap_task, process_reap_list);\n\nstatic void add_to_reap_list(struct sock *sk)\n{\n\tstruct chtls_sock *csk = sk->sk_user_data;\n\n\tlocal_bh_disable();\n\trelease_tcp_port(sk);  \n\n\tspin_lock(&reap_list_lock);\n\tcsk->passive_reap_next = reap_list;\n\treap_list = sk;\n\tif (!csk->passive_reap_next)\n\t\tschedule_work(&reap_task);\n\tspin_unlock(&reap_list_lock);\n\tlocal_bh_enable();\n}\n\nstatic void add_pass_open_to_parent(struct sock *child, struct sock *lsk,\n\t\t\t\t    struct chtls_dev *cdev)\n{\n\tstruct request_sock *oreq;\n\tstruct chtls_sock *csk;\n\n\tif (lsk->sk_state != TCP_LISTEN)\n\t\treturn;\n\n\tcsk = child->sk_user_data;\n\toreq = csk->passive_reap_next;\n\tcsk->passive_reap_next = NULL;\n\n\treqsk_queue_removed(&inet_csk(lsk)->icsk_accept_queue, oreq);\n\t__skb_unlink((struct sk_buff *)&csk->synq, &csk->listen_ctx->synq);\n\n\tif (sk_acceptq_is_full(lsk)) {\n\t\tchtls_reqsk_free(oreq);\n\t\tadd_to_reap_list(child);\n\t} else {\n\t\trefcount_set(&oreq->rsk_refcnt, 1);\n\t\tinet_csk_reqsk_queue_add(lsk, oreq, child);\n\t\tlsk->sk_data_ready(lsk);\n\t}\n}\n\nstatic void bl_add_pass_open_to_parent(struct sock *lsk, struct sk_buff *skb)\n{\n\tstruct sock *child = skb->sk;\n\n\tskb->sk = NULL;\n\tadd_pass_open_to_parent(child, lsk, BLOG_SKB_CB(skb)->cdev);\n\tkfree_skb(skb);\n}\n\nstatic int chtls_pass_establish(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_pass_establish *req = cplhdr(skb) + RSS_HDR;\n\tstruct chtls_sock *csk;\n\tstruct sock *lsk, *sk;\n\tunsigned int hwtid;\n\n\thwtid = GET_TID(req);\n\tsk = lookup_tid(cdev->tids, hwtid);\n\tif (!sk)\n\t\treturn (CPL_RET_UNKNOWN_TID | CPL_RET_BUF_DONE);\n\n\tbh_lock_sock(sk);\n\tif (unlikely(sock_owned_by_user(sk))) {\n\t\tkfree_skb(skb);\n\t} else {\n\t\tunsigned int stid;\n\t\tvoid *data;\n\n\t\tcsk = sk->sk_user_data;\n\t\tcsk->wr_max_credits = 64;\n\t\tcsk->wr_credits = 64;\n\t\tcsk->wr_unacked = 0;\n\t\tmake_established(sk, ntohl(req->snd_isn), ntohs(req->tcp_opt));\n\t\tstid = PASS_OPEN_TID_G(ntohl(req->tos_stid));\n\t\tsk->sk_state_change(sk);\n\t\tif (unlikely(sk->sk_socket))\n\t\t\tsk_wake_async(sk, 0, POLL_OUT);\n\n\t\tdata = lookup_stid(cdev->tids, stid);\n\t\tif (!data) {\n\t\t\t \n\t\t\tkfree_skb(skb);\n\t\t\tgoto unlock;\n\t\t}\n\t\tlsk = ((struct listen_ctx *)data)->lsk;\n\n\t\tbh_lock_sock(lsk);\n\t\tif (unlikely(skb_queue_empty(&csk->listen_ctx->synq))) {\n\t\t\t \n\t\t\tbh_unlock_sock(lsk);\n\t\t\tkfree_skb(skb);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (likely(!sock_owned_by_user(lsk))) {\n\t\t\tkfree_skb(skb);\n\t\t\tadd_pass_open_to_parent(sk, lsk, cdev);\n\t\t} else {\n\t\t\tskb->sk = sk;\n\t\t\tBLOG_SKB_CB(skb)->cdev = cdev;\n\t\t\tBLOG_SKB_CB(skb)->backlog_rcv =\n\t\t\t\tbl_add_pass_open_to_parent;\n\t\t\t__sk_add_backlog(lsk, skb);\n\t\t}\n\t\tbh_unlock_sock(lsk);\n\t}\nunlock:\n\tbh_unlock_sock(sk);\n\treturn 0;\n}\n\n \nstatic void handle_urg_ptr(struct sock *sk, u32 urg_seq)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\turg_seq--;\n\tif (tp->urg_data && !after(urg_seq, tp->urg_seq))\n\t\treturn;\t \n\n\tsk_send_sigurg(sk);\n\tif (tp->urg_seq == tp->copied_seq && tp->urg_data &&\n\t    !sock_flag(sk, SOCK_URGINLINE) &&\n\t    tp->copied_seq != tp->rcv_nxt) {\n\t\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\n\t\ttp->copied_seq++;\n\t\tif (skb && tp->copied_seq - ULP_SKB_CB(skb)->seq >= skb->len)\n\t\t\tchtls_free_skb(sk, skb);\n\t}\n\n\ttp->urg_data = TCP_URG_NOTYET;\n\ttp->urg_seq = urg_seq;\n}\n\nstatic void check_sk_callbacks(struct chtls_sock *csk)\n{\n\tstruct sock *sk = csk->sk;\n\n\tif (unlikely(sk->sk_user_data &&\n\t\t     !csk_flag_nochk(csk, CSK_CALLBACKS_CHKD)))\n\t\tcsk_set_flag(csk, CSK_CALLBACKS_CHKD);\n}\n\n \nstatic void handle_excess_rx(struct sock *sk, struct sk_buff *skb)\n{\n\tif (!csk_flag(sk, CSK_ABORT_SHUTDOWN))\n\t\tchtls_abort_conn(sk, skb);\n\n\tkfree_skb(skb);\n}\n\nstatic void chtls_recv_data(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct cpl_rx_data *hdr = cplhdr(skb) + RSS_HDR;\n\tstruct chtls_sock *csk;\n\tstruct tcp_sock *tp;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\ttp = tcp_sk(sk);\n\n\tif (unlikely(sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\thandle_excess_rx(sk, skb);\n\t\treturn;\n\t}\n\n\tULP_SKB_CB(skb)->seq = ntohl(hdr->seq);\n\tULP_SKB_CB(skb)->psh = hdr->psh;\n\tskb_ulp_mode(skb) = ULP_MODE_NONE;\n\n\tskb_reset_transport_header(skb);\n\t__skb_pull(skb, sizeof(*hdr) + RSS_HDR);\n\tif (!skb->data_len)\n\t\t__skb_trim(skb, ntohs(hdr->len));\n\n\tif (unlikely(hdr->urg))\n\t\thandle_urg_ptr(sk, tp->rcv_nxt + ntohs(hdr->urg));\n\tif (unlikely(tp->urg_data == TCP_URG_NOTYET &&\n\t\t     tp->urg_seq - tp->rcv_nxt < skb->len))\n\t\ttp->urg_data = TCP_URG_VALID |\n\t\t\t       skb->data[tp->urg_seq - tp->rcv_nxt];\n\n\tif (unlikely(hdr->dack_mode != csk->delack_mode)) {\n\t\tcsk->delack_mode = hdr->dack_mode;\n\t\tcsk->delack_seq = tp->rcv_nxt;\n\t}\n\n\ttcp_hdr(skb)->fin = 0;\n\ttp->rcv_nxt += skb->len;\n\n\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tcheck_sk_callbacks(csk);\n\t\tsk->sk_data_ready(sk);\n\t}\n}\n\nstatic int chtls_rx_data(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_rx_data *req = cplhdr(skb) + RSS_HDR;\n\tunsigned int hwtid = GET_TID(req);\n\tstruct sock *sk;\n\n\tsk = lookup_tid(cdev->tids, hwtid);\n\tif (unlikely(!sk)) {\n\t\tpr_err(\"can't find conn. for hwtid %u.\\n\", hwtid);\n\t\treturn -EINVAL;\n\t}\n\tskb_dst_set(skb, NULL);\n\tprocess_cpl_msg(chtls_recv_data, sk, skb);\n\treturn 0;\n}\n\nstatic void chtls_recv_pdu(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct cpl_tls_data *hdr = cplhdr(skb);\n\tstruct chtls_sock *csk;\n\tstruct chtls_hws *tlsk;\n\tstruct tcp_sock *tp;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\ttlsk = &csk->tlshws;\n\ttp = tcp_sk(sk);\n\n\tif (unlikely(sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\thandle_excess_rx(sk, skb);\n\t\treturn;\n\t}\n\n\tULP_SKB_CB(skb)->seq = ntohl(hdr->seq);\n\tULP_SKB_CB(skb)->flags = 0;\n\tskb_ulp_mode(skb) = ULP_MODE_TLS;\n\n\tskb_reset_transport_header(skb);\n\t__skb_pull(skb, sizeof(*hdr));\n\tif (!skb->data_len)\n\t\t__skb_trim(skb,\n\t\t\t   CPL_TLS_DATA_LENGTH_G(ntohl(hdr->length_pkd)));\n\n\tif (unlikely(tp->urg_data == TCP_URG_NOTYET && tp->urg_seq -\n\t\t     tp->rcv_nxt < skb->len))\n\t\ttp->urg_data = TCP_URG_VALID |\n\t\t\t       skb->data[tp->urg_seq - tp->rcv_nxt];\n\n\ttcp_hdr(skb)->fin = 0;\n\ttlsk->pldlen = CPL_TLS_DATA_LENGTH_G(ntohl(hdr->length_pkd));\n\t__skb_queue_tail(&tlsk->sk_recv_queue, skb);\n}\n\nstatic int chtls_rx_pdu(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_tls_data *req = cplhdr(skb);\n\tunsigned int hwtid = GET_TID(req);\n\tstruct sock *sk;\n\n\tsk = lookup_tid(cdev->tids, hwtid);\n\tif (unlikely(!sk)) {\n\t\tpr_err(\"can't find conn. for hwtid %u.\\n\", hwtid);\n\t\treturn -EINVAL;\n\t}\n\tskb_dst_set(skb, NULL);\n\tprocess_cpl_msg(chtls_recv_pdu, sk, skb);\n\treturn 0;\n}\n\nstatic void chtls_set_hdrlen(struct sk_buff *skb, unsigned int nlen)\n{\n\tstruct tlsrx_cmp_hdr *tls_cmp_hdr = cplhdr(skb);\n\n\tskb->hdr_len = ntohs((__force __be16)tls_cmp_hdr->length);\n\ttls_cmp_hdr->length = ntohs((__force __be16)nlen);\n}\n\nstatic void chtls_rx_hdr(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tlsrx_cmp_hdr *tls_hdr_pkt;\n\tstruct cpl_rx_tls_cmp *cmp_cpl;\n\tstruct sk_buff *skb_rec;\n\tstruct chtls_sock *csk;\n\tstruct chtls_hws *tlsk;\n\tstruct tcp_sock *tp;\n\n\tcmp_cpl = cplhdr(skb);\n\tcsk = rcu_dereference_sk_user_data(sk);\n\ttlsk = &csk->tlshws;\n\ttp = tcp_sk(sk);\n\n\tULP_SKB_CB(skb)->seq = ntohl(cmp_cpl->seq);\n\tULP_SKB_CB(skb)->flags = 0;\n\n\tskb_reset_transport_header(skb);\n\t__skb_pull(skb, sizeof(*cmp_cpl));\n\ttls_hdr_pkt = (struct tlsrx_cmp_hdr *)skb->data;\n\tif (tls_hdr_pkt->res_to_mac_error & TLSRX_HDR_PKT_ERROR_M)\n\t\ttls_hdr_pkt->type = CONTENT_TYPE_ERROR;\n\tif (!skb->data_len)\n\t\t__skb_trim(skb, TLS_HEADER_LENGTH);\n\n\ttp->rcv_nxt +=\n\t\tCPL_RX_TLS_CMP_PDULENGTH_G(ntohl(cmp_cpl->pdulength_length));\n\n\tULP_SKB_CB(skb)->flags |= ULPCB_FLAG_TLS_HDR;\n\tskb_rec = __skb_dequeue(&tlsk->sk_recv_queue);\n\tif (!skb_rec) {\n\t\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\t} else {\n\t\tchtls_set_hdrlen(skb, tlsk->pldlen);\n\t\ttlsk->pldlen = 0;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\t\t__skb_queue_tail(&sk->sk_receive_queue, skb_rec);\n\t}\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tcheck_sk_callbacks(csk);\n\t\tsk->sk_data_ready(sk);\n\t}\n}\n\nstatic int chtls_rx_cmp(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_rx_tls_cmp *req = cplhdr(skb);\n\tunsigned int hwtid = GET_TID(req);\n\tstruct sock *sk;\n\n\tsk = lookup_tid(cdev->tids, hwtid);\n\tif (unlikely(!sk)) {\n\t\tpr_err(\"can't find conn. for hwtid %u.\\n\", hwtid);\n\t\treturn -EINVAL;\n\t}\n\tskb_dst_set(skb, NULL);\n\tprocess_cpl_msg(chtls_rx_hdr, sk, skb);\n\n\treturn 0;\n}\n\nstatic void chtls_timewait(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->rcv_nxt++;\n\ttp->rx_opt.ts_recent_stamp = ktime_get_seconds();\n\ttp->srtt_us = 0;\n\ttcp_time_wait(sk, TCP_TIME_WAIT, 0);\n}\n\nstatic void chtls_peer_close(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\n\tif (csk_flag_nochk(csk, CSK_ABORT_RPL_PENDING))\n\t\tgoto out;\n\n\tsk->sk_shutdown |= RCV_SHUTDOWN;\n\tsock_set_flag(sk, SOCK_DONE);\n\n\tswitch (sk->sk_state) {\n\tcase TCP_SYN_RECV:\n\tcase TCP_ESTABLISHED:\n\t\ttcp_set_state(sk, TCP_CLOSE_WAIT);\n\t\tbreak;\n\tcase TCP_FIN_WAIT1:\n\t\ttcp_set_state(sk, TCP_CLOSING);\n\t\tbreak;\n\tcase TCP_FIN_WAIT2:\n\t\tchtls_release_resources(sk);\n\t\tif (csk_flag_nochk(csk, CSK_ABORT_RPL_PENDING))\n\t\t\tchtls_conn_done(sk);\n\t\telse\n\t\t\tchtls_timewait(sk);\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"cpl_peer_close in bad state %d\\n\", sk->sk_state);\n\t}\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_state_change(sk);\n\t\t \n\n\t\tif ((sk->sk_shutdown & SEND_SHUTDOWN) ||\n\t\t    sk->sk_state == TCP_CLOSE)\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_HUP);\n\t\telse\n\t\t\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\t}\nout:\n\tkfree_skb(skb);\n}\n\nstatic void chtls_close_con_rpl(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct cpl_close_con_rpl *rpl = cplhdr(skb) + RSS_HDR;\n\tstruct chtls_sock *csk;\n\tstruct tcp_sock *tp;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\n\tif (csk_flag_nochk(csk, CSK_ABORT_RPL_PENDING))\n\t\tgoto out;\n\n\ttp = tcp_sk(sk);\n\n\ttp->snd_una = ntohl(rpl->snd_nxt) - 1;   \n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSING:\n\t\tchtls_release_resources(sk);\n\t\tif (csk_flag_nochk(csk, CSK_ABORT_RPL_PENDING))\n\t\t\tchtls_conn_done(sk);\n\t\telse\n\t\t\tchtls_timewait(sk);\n\t\tbreak;\n\tcase TCP_LAST_ACK:\n\t\tchtls_release_resources(sk);\n\t\tchtls_conn_done(sk);\n\t\tbreak;\n\tcase TCP_FIN_WAIT1:\n\t\ttcp_set_state(sk, TCP_FIN_WAIT2);\n\t\tsk->sk_shutdown |= SEND_SHUTDOWN;\n\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_state_change(sk);\n\t\telse if (tcp_sk(sk)->linger2 < 0 &&\n\t\t\t !csk_flag_nochk(csk, CSK_ABORT_SHUTDOWN))\n\t\t\tchtls_abort_conn(sk, skb);\n\t\telse if (csk_flag_nochk(csk, CSK_TX_DATA_SENT))\n\t\t\tchtls_set_quiesce_ctrl(sk, 0);\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"close_con_rpl in bad state %d\\n\", sk->sk_state);\n\t}\nout:\n\tkfree_skb(skb);\n}\n\nstatic struct sk_buff *get_cpl_skb(struct sk_buff *skb,\n\t\t\t\t   size_t len, gfp_t gfp)\n{\n\tif (likely(!skb_is_nonlinear(skb) && !skb_cloned(skb))) {\n\t\tWARN_ONCE(skb->len < len, \"skb alloc error\");\n\t\t__skb_trim(skb, len);\n\t\tskb_get(skb);\n\t} else {\n\t\tskb = alloc_skb(len, gfp);\n\t\tif (skb)\n\t\t\t__skb_put(skb, len);\n\t}\n\treturn skb;\n}\n\nstatic void set_abort_rpl_wr(struct sk_buff *skb, unsigned int tid,\n\t\t\t     int cmd)\n{\n\tstruct cpl_abort_rpl *rpl = cplhdr(skb);\n\n\tINIT_TP_WR_CPL(rpl, CPL_ABORT_RPL, tid);\n\trpl->cmd = cmd;\n}\n\nstatic void send_defer_abort_rpl(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_abort_req_rss *req = cplhdr(skb);\n\tstruct sk_buff *reply_skb;\n\n\treply_skb = alloc_skb(sizeof(struct cpl_abort_rpl),\n\t\t\t      GFP_KERNEL | __GFP_NOFAIL);\n\t__skb_put(reply_skb, sizeof(struct cpl_abort_rpl));\n\tset_abort_rpl_wr(reply_skb, GET_TID(req),\n\t\t\t (req->status & CPL_ABORT_NO_RST));\n\tset_wr_txq(reply_skb, CPL_PRIORITY_DATA, req->status >> 1);\n\tcxgb4_ofld_send(cdev->lldi->ports[0], reply_skb);\n\tkfree_skb(skb);\n}\n\n \nstatic void t4_defer_reply(struct sk_buff *skb, struct chtls_dev *cdev,\n\t\t\t   defer_handler_t handler)\n{\n\tDEFERRED_SKB_CB(skb)->handler = handler;\n\tspin_lock_bh(&cdev->deferq.lock);\n\t__skb_queue_tail(&cdev->deferq, skb);\n\tif (skb_queue_len(&cdev->deferq) == 1)\n\t\tschedule_work(&cdev->deferq_task);\n\tspin_unlock_bh(&cdev->deferq.lock);\n}\n\nstatic void chtls_send_abort_rpl(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t struct chtls_dev *cdev,\n\t\t\t\t int status, int queue)\n{\n\tstruct cpl_abort_req_rss *req = cplhdr(skb) + RSS_HDR;\n\tstruct sk_buff *reply_skb;\n\tstruct chtls_sock *csk;\n\tunsigned int tid;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\ttid = GET_TID(req);\n\n\treply_skb = get_cpl_skb(skb, sizeof(struct cpl_abort_rpl), gfp_any());\n\tif (!reply_skb) {\n\t\treq->status = (queue << 1) | status;\n\t\tt4_defer_reply(skb, cdev, send_defer_abort_rpl);\n\t\treturn;\n\t}\n\n\tset_abort_rpl_wr(reply_skb, tid, status);\n\tkfree_skb(skb);\n\tset_wr_txq(reply_skb, CPL_PRIORITY_DATA, queue);\n\tif (csk_conn_inline(csk)) {\n\t\tstruct l2t_entry *e = csk->l2t_entry;\n\n\t\tif (e && sk->sk_state != TCP_SYN_RECV) {\n\t\t\tcxgb4_l2t_send(csk->egress_dev, reply_skb, e);\n\t\t\treturn;\n\t\t}\n\t}\n\tcxgb4_ofld_send(cdev->lldi->ports[0], reply_skb);\n}\n\n \nstatic void bl_abort_syn_rcv(struct sock *lsk, struct sk_buff *skb)\n{\n\tstruct chtls_sock *csk;\n\tstruct sock *child;\n\tint queue;\n\n\tchild = skb->sk;\n\tcsk = rcu_dereference_sk_user_data(child);\n\tqueue = csk->txq_idx;\n\n\tskb->sk\t= NULL;\n\tchtls_send_abort_rpl(child, skb, BLOG_SKB_CB(skb)->cdev,\n\t\t\t     CPL_ABORT_NO_RST, queue);\n\tdo_abort_syn_rcv(child, lsk);\n}\n\nstatic int abort_syn_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct request_sock *oreq;\n\tstruct listen_ctx *listen_ctx;\n\tstruct chtls_sock *csk;\n\tstruct chtls_dev *cdev;\n\tstruct sock *psk;\n\tvoid *ctx;\n\n\tcsk = sk->sk_user_data;\n\toreq = csk->passive_reap_next;\n\tcdev = csk->cdev;\n\n\tif (!oreq)\n\t\treturn -1;\n\n\tctx = lookup_stid(cdev->tids, oreq->ts_recent);\n\tif (!ctx)\n\t\treturn -1;\n\n\tlisten_ctx = (struct listen_ctx *)ctx;\n\tpsk = listen_ctx->lsk;\n\n\tbh_lock_sock(psk);\n\tif (!sock_owned_by_user(psk)) {\n\t\tint queue = csk->txq_idx;\n\n\t\tchtls_send_abort_rpl(sk, skb, cdev, CPL_ABORT_NO_RST, queue);\n\t\tdo_abort_syn_rcv(sk, psk);\n\t} else {\n\t\tskb->sk = sk;\n\t\tBLOG_SKB_CB(skb)->backlog_rcv = bl_abort_syn_rcv;\n\t\t__sk_add_backlog(psk, skb);\n\t}\n\tbh_unlock_sock(psk);\n\treturn 0;\n}\n\nstatic void chtls_abort_req_rss(struct sock *sk, struct sk_buff *skb)\n{\n\tconst struct cpl_abort_req_rss *req = cplhdr(skb) + RSS_HDR;\n\tstruct chtls_sock *csk = sk->sk_user_data;\n\tint rst_status = CPL_ABORT_NO_RST;\n\tint queue = csk->txq_idx;\n\n\tif (is_neg_adv(req->status)) {\n\t\tkfree_skb(skb);\n\t\treturn;\n\t}\n\n\tcsk_reset_flag(csk, CSK_ABORT_REQ_RCVD);\n\n\tif (!csk_flag_nochk(csk, CSK_ABORT_SHUTDOWN) &&\n\t    !csk_flag_nochk(csk, CSK_TX_DATA_SENT)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tif (send_tx_flowc_wr(sk, 0, tp->snd_nxt, tp->rcv_nxt) < 0)\n\t\t\tWARN_ONCE(1, \"send_tx_flowc error\");\n\t\tcsk_set_flag(csk, CSK_TX_DATA_SENT);\n\t}\n\n\tcsk_set_flag(csk, CSK_ABORT_SHUTDOWN);\n\n\tif (!csk_flag_nochk(csk, CSK_ABORT_RPL_PENDING)) {\n\t\tsk->sk_err = ETIMEDOUT;\n\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk_error_report(sk);\n\n\t\tif (sk->sk_state == TCP_SYN_RECV && !abort_syn_rcv(sk, skb))\n\t\t\treturn;\n\n\t}\n\n\tchtls_send_abort_rpl(sk, skb, BLOG_SKB_CB(skb)->cdev,\n\t\t\t     rst_status, queue);\n\tchtls_release_resources(sk);\n\tchtls_conn_done(sk);\n}\n\nstatic void chtls_abort_rpl_rss(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct cpl_abort_rpl_rss *rpl = cplhdr(skb) + RSS_HDR;\n\tstruct chtls_sock *csk;\n\tstruct chtls_dev *cdev;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\tcdev = csk->cdev;\n\n\tif (csk_flag_nochk(csk, CSK_ABORT_RPL_PENDING)) {\n\t\tcsk_reset_flag(csk, CSK_ABORT_RPL_PENDING);\n\t\tif (!csk_flag_nochk(csk, CSK_ABORT_REQ_RCVD)) {\n\t\t\tif (sk->sk_state == TCP_SYN_SENT) {\n\t\t\t\tcxgb4_remove_tid(cdev->tids,\n\t\t\t\t\t\t csk->port_id,\n\t\t\t\t\t\t GET_TID(rpl),\n\t\t\t\t\t\t sk->sk_family);\n\t\t\t\tsock_put(sk);\n\t\t\t}\n\t\t\tchtls_release_resources(sk);\n\t\t\tchtls_conn_done(sk);\n\t\t}\n\t}\n\tkfree_skb(skb);\n}\n\nstatic int chtls_conn_cpl(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_peer_close *req = cplhdr(skb) + RSS_HDR;\n\tvoid (*fn)(struct sock *sk, struct sk_buff *skb);\n\tunsigned int hwtid = GET_TID(req);\n\tstruct chtls_sock *csk;\n\tstruct sock *sk;\n\tu8 opcode;\n\n\topcode = ((const struct rss_header *)cplhdr(skb))->opcode;\n\n\tsk = lookup_tid(cdev->tids, hwtid);\n\tif (!sk)\n\t\tgoto rel_skb;\n\n\tcsk = sk->sk_user_data;\n\n\tswitch (opcode) {\n\tcase CPL_PEER_CLOSE:\n\t\tfn = chtls_peer_close;\n\t\tbreak;\n\tcase CPL_CLOSE_CON_RPL:\n\t\tfn = chtls_close_con_rpl;\n\t\tbreak;\n\tcase CPL_ABORT_REQ_RSS:\n\t\t \n\t\tBLOG_SKB_CB(skb)->cdev = csk->cdev;\n\t\tfn = chtls_abort_req_rss;\n\t\tbreak;\n\tcase CPL_ABORT_RPL_RSS:\n\t\tfn = chtls_abort_rpl_rss;\n\t\tbreak;\n\tdefault:\n\t\tgoto rel_skb;\n\t}\n\n\tprocess_cpl_msg(fn, sk, skb);\n\treturn 0;\n\nrel_skb:\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic void chtls_rx_ack(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct cpl_fw4_ack *hdr = cplhdr(skb) + RSS_HDR;\n\tstruct chtls_sock *csk = sk->sk_user_data;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 credits = hdr->credits;\n\tu32 snd_una;\n\n\tsnd_una = ntohl(hdr->snd_una);\n\tcsk->wr_credits += credits;\n\n\tif (csk->wr_unacked > csk->wr_max_credits - csk->wr_credits)\n\t\tcsk->wr_unacked = csk->wr_max_credits - csk->wr_credits;\n\n\twhile (credits) {\n\t\tstruct sk_buff *pskb = csk->wr_skb_head;\n\t\tu32 csum;\n\n\t\tif (unlikely(!pskb)) {\n\t\t\tif (csk->wr_nondata)\n\t\t\t\tcsk->wr_nondata -= credits;\n\t\t\tbreak;\n\t\t}\n\t\tcsum = (__force u32)pskb->csum;\n\t\tif (unlikely(credits < csum)) {\n\t\t\tpskb->csum = (__force __wsum)(csum - credits);\n\t\t\tbreak;\n\t\t}\n\t\tdequeue_wr(sk);\n\t\tcredits -= csum;\n\t\tkfree_skb(pskb);\n\t}\n\tif (hdr->seq_vld & CPL_FW4_ACK_FLAGS_SEQVAL) {\n\t\tif (unlikely(before(snd_una, tp->snd_una))) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn;\n\t\t}\n\n\t\tif (tp->snd_una != snd_una) {\n\t\t\ttp->snd_una = snd_una;\n\t\t\ttp->rcv_tstamp = tcp_jiffies32;\n\t\t\tif (tp->snd_una == tp->snd_nxt &&\n\t\t\t    !csk_flag_nochk(csk, CSK_TX_FAILOVER))\n\t\t\t\tcsk_reset_flag(csk, CSK_TX_WAIT_IDLE);\n\t\t}\n\t}\n\n\tif (hdr->seq_vld & CPL_FW4_ACK_FLAGS_CH) {\n\t\tunsigned int fclen16 = roundup(failover_flowc_wr_len, 16);\n\n\t\tcsk->wr_credits -= fclen16;\n\t\tcsk_reset_flag(csk, CSK_TX_WAIT_IDLE);\n\t\tcsk_reset_flag(csk, CSK_TX_FAILOVER);\n\t}\n\tif (skb_queue_len(&csk->txq) && chtls_push_frames(csk, 0))\n\t\tsk->sk_write_space(sk);\n\n\tkfree_skb(skb);\n}\n\nstatic int chtls_wr_ack(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_fw4_ack *rpl = cplhdr(skb) + RSS_HDR;\n\tunsigned int hwtid = GET_TID(rpl);\n\tstruct sock *sk;\n\n\tsk = lookup_tid(cdev->tids, hwtid);\n\tif (unlikely(!sk)) {\n\t\tpr_err(\"can't find conn. for hwtid %u.\\n\", hwtid);\n\t\treturn -EINVAL;\n\t}\n\tprocess_cpl_msg(chtls_rx_ack, sk, skb);\n\n\treturn 0;\n}\n\nstatic int chtls_set_tcb_rpl(struct chtls_dev *cdev, struct sk_buff *skb)\n{\n\tstruct cpl_set_tcb_rpl *rpl = cplhdr(skb) + RSS_HDR;\n\tunsigned int hwtid = GET_TID(rpl);\n\tstruct sock *sk;\n\n\tsk = lookup_tid(cdev->tids, hwtid);\n\n\t \n\tif (!sk)\n\t\treturn -EINVAL;\n\n\t \n\tif (TCB_COOKIE_G(rpl->cookie) == TCB_FIELD_COOKIE_TFLAG)\n\t\tchtls_send_abort(sk, CPL_ABORT_SEND_RST, NULL);\n\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nchtls_handler_func chtls_handlers[NUM_CPL_CMDS] = {\n\t[CPL_PASS_OPEN_RPL]     = chtls_pass_open_rpl,\n\t[CPL_CLOSE_LISTSRV_RPL] = chtls_close_listsrv_rpl,\n\t[CPL_PASS_ACCEPT_REQ]   = chtls_pass_accept_req,\n\t[CPL_PASS_ESTABLISH]    = chtls_pass_establish,\n\t[CPL_RX_DATA]           = chtls_rx_data,\n\t[CPL_TLS_DATA]          = chtls_rx_pdu,\n\t[CPL_RX_TLS_CMP]        = chtls_rx_cmp,\n\t[CPL_PEER_CLOSE]        = chtls_conn_cpl,\n\t[CPL_CLOSE_CON_RPL]     = chtls_conn_cpl,\n\t[CPL_ABORT_REQ_RSS]     = chtls_conn_cpl,\n\t[CPL_ABORT_RPL_RSS]     = chtls_conn_cpl,\n\t[CPL_FW4_ACK]\t\t= chtls_wr_ack,\n\t[CPL_SET_TCB_RPL]\t= chtls_set_tcb_rpl,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}