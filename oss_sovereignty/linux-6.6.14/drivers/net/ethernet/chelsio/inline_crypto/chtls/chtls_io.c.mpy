{
  "module_name": "chtls_io.c",
  "hash_id": "10788b9f0ec2d15b43d42c75f3f1df9ab4cffa3e2f5bc008274a4289ba3b473d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/inline_crypto/chtls/chtls_io.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/list.h>\n#include <linux/workqueue.h>\n#include <linux/skbuff.h>\n#include <linux/timer.h>\n#include <linux/notifier.h>\n#include <linux/inetdevice.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/sched/signal.h>\n#include <net/tcp.h>\n#include <net/busy_poll.h>\n#include <crypto/aes.h>\n\n#include \"chtls.h\"\n#include \"chtls_cm.h\"\n\nstatic bool is_tls_tx(struct chtls_sock *csk)\n{\n\treturn csk->tlshws.txkey >= 0;\n}\n\nstatic bool is_tls_rx(struct chtls_sock *csk)\n{\n\treturn csk->tlshws.rxkey >= 0;\n}\n\nstatic int data_sgl_len(const struct sk_buff *skb)\n{\n\tunsigned int cnt;\n\n\tcnt = skb_shinfo(skb)->nr_frags;\n\treturn sgl_len(cnt) * 8;\n}\n\nstatic int nos_ivs(struct sock *sk, unsigned int size)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\n\treturn DIV_ROUND_UP(size, csk->tlshws.mfs);\n}\n\nstatic int set_ivs_imm(struct sock *sk, const struct sk_buff *skb)\n{\n\tint ivs_size = nos_ivs(sk, skb->len) * CIPHER_BLOCK_SIZE;\n\tint hlen = TLS_WR_CPL_LEN + data_sgl_len(skb);\n\n\tif ((hlen + KEY_ON_MEM_SZ + ivs_size) <\n\t    MAX_IMM_OFLD_TX_DATA_WR_LEN) {\n\t\tULP_SKB_CB(skb)->ulp.tls.iv = 1;\n\t\treturn 1;\n\t}\n\tULP_SKB_CB(skb)->ulp.tls.iv = 0;\n\treturn 0;\n}\n\nstatic int max_ivs_size(struct sock *sk, int size)\n{\n\treturn nos_ivs(sk, size) * CIPHER_BLOCK_SIZE;\n}\n\nstatic int ivs_size(struct sock *sk, const struct sk_buff *skb)\n{\n\treturn set_ivs_imm(sk, skb) ? (nos_ivs(sk, skb->len) *\n\t\t CIPHER_BLOCK_SIZE) : 0;\n}\n\nstatic int flowc_wr_credits(int nparams, int *flowclenp)\n{\n\tint flowclen16, flowclen;\n\n\tflowclen = offsetof(struct fw_flowc_wr, mnemval[nparams]);\n\tflowclen16 = DIV_ROUND_UP(flowclen, 16);\n\tflowclen = flowclen16 * 16;\n\n\tif (flowclenp)\n\t\t*flowclenp = flowclen;\n\n\treturn flowclen16;\n}\n\nstatic struct sk_buff *create_flowc_wr_skb(struct sock *sk,\n\t\t\t\t\t   struct fw_flowc_wr *flowc,\n\t\t\t\t\t   int flowclen)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct sk_buff *skb;\n\n\tskb = alloc_skb(flowclen, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\t__skb_put_data(skb, flowc, flowclen);\n\tskb_set_queue_mapping(skb, (csk->txq_idx << 1) | CPL_PRIORITY_DATA);\n\n\treturn skb;\n}\n\nstatic int send_flowc_wr(struct sock *sk, struct fw_flowc_wr *flowc,\n\t\t\t int flowclen)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint flowclen16;\n\tint ret;\n\n\tflowclen16 = flowclen / 16;\n\n\tif (csk_flag(sk, CSK_TX_DATA_SENT)) {\n\t\tskb = create_flowc_wr_skb(sk, flowc, flowclen);\n\t\tif (!skb)\n\t\t\treturn -ENOMEM;\n\n\t\tskb_entail(sk, skb,\n\t\t\t   ULPCB_FLAG_NO_HDR | ULPCB_FLAG_NO_APPEND);\n\t\treturn 0;\n\t}\n\n\tret = cxgb4_immdata_send(csk->egress_dev,\n\t\t\t\t csk->txq_idx,\n\t\t\t\t flowc, flowclen);\n\tif (!ret)\n\t\treturn flowclen16;\n\tskb = create_flowc_wr_skb(sk, flowc, flowclen);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\tsend_or_defer(sk, tp, skb, 0);\n\treturn flowclen16;\n}\n\nstatic u8 tcp_state_to_flowc_state(u8 state)\n{\n\tswitch (state) {\n\tcase TCP_ESTABLISHED:\n\t\treturn FW_FLOWC_MNEM_TCPSTATE_ESTABLISHED;\n\tcase TCP_CLOSE_WAIT:\n\t\treturn FW_FLOWC_MNEM_TCPSTATE_CLOSEWAIT;\n\tcase TCP_FIN_WAIT1:\n\t\treturn FW_FLOWC_MNEM_TCPSTATE_FINWAIT1;\n\tcase TCP_CLOSING:\n\t\treturn FW_FLOWC_MNEM_TCPSTATE_CLOSING;\n\tcase TCP_LAST_ACK:\n\t\treturn FW_FLOWC_MNEM_TCPSTATE_LASTACK;\n\tcase TCP_FIN_WAIT2:\n\t\treturn FW_FLOWC_MNEM_TCPSTATE_FINWAIT2;\n\t}\n\n\treturn FW_FLOWC_MNEM_TCPSTATE_ESTABLISHED;\n}\n\nint send_tx_flowc_wr(struct sock *sk, int compl,\n\t\t     u32 snd_nxt, u32 rcv_nxt)\n{\n\tstruct flowc_packed {\n\t\tstruct fw_flowc_wr fc;\n\t\tstruct fw_flowc_mnemval mnemval[FW_FLOWC_MNEM_MAX];\n\t} __packed sflowc;\n\tint nparams, paramidx, flowclen16, flowclen;\n\tstruct fw_flowc_wr *flowc;\n\tstruct chtls_sock *csk;\n\tstruct tcp_sock *tp;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\ttp = tcp_sk(sk);\n\tmemset(&sflowc, 0, sizeof(sflowc));\n\tflowc = &sflowc.fc;\n\n#define FLOWC_PARAM(__m, __v) \\\n\tdo { \\\n\t\tflowc->mnemval[paramidx].mnemonic = FW_FLOWC_MNEM_##__m; \\\n\t\tflowc->mnemval[paramidx].val = cpu_to_be32(__v); \\\n\t\tparamidx++; \\\n\t} while (0)\n\n\tparamidx = 0;\n\n\tFLOWC_PARAM(PFNVFN, FW_PFVF_CMD_PFN_V(csk->cdev->lldi->pf));\n\tFLOWC_PARAM(CH, csk->tx_chan);\n\tFLOWC_PARAM(PORT, csk->tx_chan);\n\tFLOWC_PARAM(IQID, csk->rss_qid);\n\tFLOWC_PARAM(SNDNXT, tp->snd_nxt);\n\tFLOWC_PARAM(RCVNXT, tp->rcv_nxt);\n\tFLOWC_PARAM(SNDBUF, csk->sndbuf);\n\tFLOWC_PARAM(MSS, tp->mss_cache);\n\tFLOWC_PARAM(TCPSTATE, tcp_state_to_flowc_state(sk->sk_state));\n\n\tif (SND_WSCALE(tp))\n\t\tFLOWC_PARAM(RCV_SCALE, SND_WSCALE(tp));\n\n\tif (csk->ulp_mode == ULP_MODE_TLS)\n\t\tFLOWC_PARAM(ULD_MODE, ULP_MODE_TLS);\n\n\tif (csk->tlshws.fcplenmax)\n\t\tFLOWC_PARAM(TXDATAPLEN_MAX, csk->tlshws.fcplenmax);\n\n\tnparams = paramidx;\n#undef FLOWC_PARAM\n\n\tflowclen16 = flowc_wr_credits(nparams, &flowclen);\n\tflowc->op_to_nparams =\n\t\tcpu_to_be32(FW_WR_OP_V(FW_FLOWC_WR) |\n\t\t\t    FW_WR_COMPL_V(compl) |\n\t\t\t    FW_FLOWC_WR_NPARAMS_V(nparams));\n\tflowc->flowid_len16 = cpu_to_be32(FW_WR_LEN16_V(flowclen16) |\n\t\t\t\t\t  FW_WR_FLOWID_V(csk->tid));\n\n\treturn send_flowc_wr(sk, flowc, flowclen);\n}\n\n \nstatic int tls_copy_ivs(struct sock *sk, struct sk_buff *skb)\n\n{\n\tstruct chtls_sock *csk;\n\tunsigned char *iv_loc;\n\tstruct chtls_hws *hws;\n\tunsigned char *ivs;\n\tu16 number_of_ivs;\n\tstruct page *page;\n\tint err = 0;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\thws = &csk->tlshws;\n\tnumber_of_ivs = nos_ivs(sk, skb->len);\n\n\tif (number_of_ivs > MAX_IVS_PAGE) {\n\t\tpr_warn(\"MAX IVs in PAGE exceeded %d\\n\", number_of_ivs);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tivs = kmalloc_array(CIPHER_BLOCK_SIZE, number_of_ivs, GFP_ATOMIC);\n\tif (!ivs)\n\t\treturn -ENOMEM;\n\tget_random_bytes(ivs, number_of_ivs * CIPHER_BLOCK_SIZE);\n\n\tif (skb_ulp_tls_iv_imm(skb)) {\n\t\t \n\t\tiv_loc = (unsigned char *)__skb_push(skb, number_of_ivs *\n\t\t\t\t\t\tCIPHER_BLOCK_SIZE);\n\t\tif (iv_loc)\n\t\t\tmemcpy(iv_loc, ivs, number_of_ivs * CIPHER_BLOCK_SIZE);\n\n\t\thws->ivsize = number_of_ivs * CIPHER_BLOCK_SIZE;\n\t} else {\n\t\t \n\t\t \n\t\tskb_shinfo(skb)->nr_frags--;\n\t\tpage = alloc_pages(sk->sk_allocation | __GFP_COMP, 0);\n\t\tif (!page) {\n\t\t\tpr_info(\"%s : Page allocation for IVs failed\\n\",\n\t\t\t\t__func__);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(page_address(page), ivs, number_of_ivs *\n\t\t       CIPHER_BLOCK_SIZE);\n\t\tskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags, page, 0,\n\t\t\t\t   number_of_ivs * CIPHER_BLOCK_SIZE);\n\t\thws->ivsize = 0;\n\t}\nout:\n\tkfree(ivs);\n\treturn err;\n}\n\n \nstatic void tls_copy_tx_key(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct ulptx_sc_memrd *sc_memrd;\n\tstruct chtls_sock *csk;\n\tstruct chtls_dev *cdev;\n\tstruct ulptx_idata *sc;\n\tstruct chtls_hws *hws;\n\tu32 immdlen;\n\tint kaddr;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\thws = &csk->tlshws;\n\tcdev = csk->cdev;\n\n\timmdlen = sizeof(*sc) + sizeof(*sc_memrd);\n\tkaddr = keyid_to_addr(cdev->kmap.start, hws->txkey);\n\tsc = (struct ulptx_idata *)__skb_push(skb, immdlen);\n\tif (sc) {\n\t\tsc->cmd_more = htonl(ULPTX_CMD_V(ULP_TX_SC_NOOP));\n\t\tsc->len = htonl(0);\n\t\tsc_memrd = (struct ulptx_sc_memrd *)(sc + 1);\n\t\tsc_memrd->cmd_to_len =\n\t\t\t\thtonl(ULPTX_CMD_V(ULP_TX_SC_MEMRD) |\n\t\t\t\tULP_TX_SC_MORE_V(1) |\n\t\t\t\tULPTX_LEN16_V(hws->keylen >> 4));\n\t\tsc_memrd->addr = htonl(kaddr);\n\t}\n}\n\nstatic u64 tlstx_incr_seqnum(struct chtls_hws *hws)\n{\n\treturn hws->tx_seq_no++;\n}\n\nstatic bool is_sg_request(const struct sk_buff *skb)\n{\n\treturn skb->peeked ||\n\t\t(skb->len > MAX_IMM_ULPTX_WR_LEN);\n}\n\n \nstatic bool skb_urgent(struct sk_buff *skb)\n{\n\treturn ULP_SKB_CB(skb)->flags & ULPCB_FLAG_URG;\n}\n\n \nstatic unsigned char tls_content_type(unsigned char content_type)\n{\n\tswitch (content_type) {\n\tcase TLS_HDR_TYPE_CCS:\n\t\treturn CPL_TX_TLS_SFO_TYPE_CCS;\n\tcase TLS_HDR_TYPE_ALERT:\n\t\treturn CPL_TX_TLS_SFO_TYPE_ALERT;\n\tcase TLS_HDR_TYPE_HANDSHAKE:\n\t\treturn CPL_TX_TLS_SFO_TYPE_HANDSHAKE;\n\tcase TLS_HDR_TYPE_HEARTBEAT:\n\t\treturn CPL_TX_TLS_SFO_TYPE_HEARTBEAT;\n\t}\n\treturn CPL_TX_TLS_SFO_TYPE_DATA;\n}\n\nstatic void tls_tx_data_wr(struct sock *sk, struct sk_buff *skb,\n\t\t\t   int dlen, int tls_immd, u32 credits,\n\t\t\t   int expn, int pdus)\n{\n\tstruct fw_tlstx_data_wr *req_wr;\n\tstruct cpl_tx_tls_sfo *req_cpl;\n\tunsigned int wr_ulp_mode_force;\n\tstruct tls_scmd *updated_scmd;\n\tunsigned char data_type;\n\tstruct chtls_sock *csk;\n\tstruct net_device *dev;\n\tstruct chtls_hws *hws;\n\tstruct tls_scmd *scmd;\n\tstruct adapter *adap;\n\tunsigned char *req;\n\tint immd_len;\n\tint iv_imm;\n\tint len;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\tiv_imm = skb_ulp_tls_iv_imm(skb);\n\tdev = csk->egress_dev;\n\tadap = netdev2adap(dev);\n\thws = &csk->tlshws;\n\tscmd = &hws->scmd;\n\tlen = dlen + expn;\n\n\tdlen = (dlen < hws->mfs) ? dlen : hws->mfs;\n\tatomic_inc(&adap->chcr_stats.tls_pdu_tx);\n\n\tupdated_scmd = scmd;\n\tupdated_scmd->seqno_numivs &= 0xffffff80;\n\tupdated_scmd->seqno_numivs |= SCMD_NUM_IVS_V(pdus);\n\thws->scmd = *updated_scmd;\n\n\treq = (unsigned char *)__skb_push(skb, sizeof(struct cpl_tx_tls_sfo));\n\treq_cpl = (struct cpl_tx_tls_sfo *)req;\n\treq = (unsigned char *)__skb_push(skb, (sizeof(struct\n\t\t\t\tfw_tlstx_data_wr)));\n\n\treq_wr = (struct fw_tlstx_data_wr *)req;\n\timmd_len = (tls_immd ? dlen : 0);\n\treq_wr->op_to_immdlen =\n\t\thtonl(FW_WR_OP_V(FW_TLSTX_DATA_WR) |\n\t\tFW_TLSTX_DATA_WR_COMPL_V(1) |\n\t\tFW_TLSTX_DATA_WR_IMMDLEN_V(immd_len));\n\treq_wr->flowid_len16 = htonl(FW_TLSTX_DATA_WR_FLOWID_V(csk->tid) |\n\t\t\t\t     FW_TLSTX_DATA_WR_LEN16_V(credits));\n\twr_ulp_mode_force = TX_ULP_MODE_V(ULP_MODE_TLS);\n\n\tif (is_sg_request(skb))\n\t\twr_ulp_mode_force |= FW_OFLD_TX_DATA_WR_ALIGNPLD_F |\n\t\t\t((tcp_sk(sk)->nonagle & TCP_NAGLE_OFF) ? 0 :\n\t\t\tFW_OFLD_TX_DATA_WR_SHOVE_F);\n\n\treq_wr->lsodisable_to_flags =\n\t\t\thtonl(TX_ULP_MODE_V(ULP_MODE_TLS) |\n\t\t\t      TX_URG_V(skb_urgent(skb)) |\n\t\t\t      T6_TX_FORCE_F | wr_ulp_mode_force |\n\t\t\t      TX_SHOVE_V((!csk_flag(sk, CSK_TX_MORE_DATA)) &&\n\t\t\t\t\t skb_queue_empty(&csk->txq)));\n\n\treq_wr->ctxloc_to_exp =\n\t\t\thtonl(FW_TLSTX_DATA_WR_NUMIVS_V(pdus) |\n\t\t\t      FW_TLSTX_DATA_WR_EXP_V(expn) |\n\t\t\t      FW_TLSTX_DATA_WR_CTXLOC_V(CHTLS_KEY_CONTEXT_DDR) |\n\t\t\t      FW_TLSTX_DATA_WR_IVDSGL_V(!iv_imm) |\n\t\t\t      FW_TLSTX_DATA_WR_KEYSIZE_V(hws->keylen >> 4));\n\n\t \n\treq_wr->plen = htonl(len);\n\treq_wr->mfs = htons(hws->mfs);\n\treq_wr->adjustedplen_pkd =\n\t\thtons(FW_TLSTX_DATA_WR_ADJUSTEDPLEN_V(hws->adjustlen));\n\treq_wr->expinplenmax_pkd =\n\t\thtons(FW_TLSTX_DATA_WR_EXPINPLENMAX_V(hws->expansion));\n\treq_wr->pdusinplenmax_pkd =\n\t\tFW_TLSTX_DATA_WR_PDUSINPLENMAX_V(hws->pdus);\n\treq_wr->r10 = 0;\n\n\tdata_type = tls_content_type(ULP_SKB_CB(skb)->ulp.tls.type);\n\treq_cpl->op_to_seg_len = htonl(CPL_TX_TLS_SFO_OPCODE_V(CPL_TX_TLS_SFO) |\n\t\t\t\t       CPL_TX_TLS_SFO_DATA_TYPE_V(data_type) |\n\t\t\t\t       CPL_TX_TLS_SFO_CPL_LEN_V(2) |\n\t\t\t\t       CPL_TX_TLS_SFO_SEG_LEN_V(dlen));\n\treq_cpl->pld_len = htonl(len - expn);\n\n\treq_cpl->type_protover = htonl(CPL_TX_TLS_SFO_TYPE_V\n\t\t((data_type == CPL_TX_TLS_SFO_TYPE_HEARTBEAT) ?\n\t\tTLS_HDR_TYPE_HEARTBEAT : 0) |\n\t\tCPL_TX_TLS_SFO_PROTOVER_V(0));\n\n\t \n\treq_cpl->r1_lo = 0;\n\treq_cpl->seqno_numivs  = cpu_to_be32(hws->scmd.seqno_numivs);\n\treq_cpl->ivgen_hdrlen = cpu_to_be32(hws->scmd.ivgen_hdrlen);\n\treq_cpl->scmd1 = cpu_to_be64(tlstx_incr_seqnum(hws));\n}\n\n \nstatic int chtls_expansion_size(struct sock *sk, int data_len,\n\t\t\t\tint fullpdu,\n\t\t\t\tunsigned short *pducnt)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct chtls_hws *hws = &csk->tlshws;\n\tstruct tls_scmd *scmd = &hws->scmd;\n\tint fragsize = hws->mfs;\n\tint expnsize = 0;\n\tint fragleft;\n\tint fragcnt;\n\tint expppdu;\n\n\tif (SCMD_CIPH_MODE_G(scmd->seqno_numivs) ==\n\t    SCMD_CIPH_MODE_AES_GCM) {\n\t\texpppdu = GCM_TAG_SIZE + AEAD_EXPLICIT_DATA_SIZE +\n\t\t\t  TLS_HEADER_LENGTH;\n\n\t\tif (fullpdu) {\n\t\t\t*pducnt = data_len / (expppdu + fragsize);\n\t\t\tif (*pducnt > 32)\n\t\t\t\t*pducnt = 32;\n\t\t\telse if (!*pducnt)\n\t\t\t\t*pducnt = 1;\n\t\t\texpnsize = (*pducnt) * expppdu;\n\t\t\treturn expnsize;\n\t\t}\n\t\tfragcnt = (data_len / fragsize);\n\t\texpnsize =  fragcnt * expppdu;\n\t\tfragleft = data_len % fragsize;\n\t\tif (fragleft > 0)\n\t\t\texpnsize += expppdu;\n\t}\n\treturn expnsize;\n}\n\n \nstatic void make_tlstx_data_wr(struct sock *sk, struct sk_buff *skb,\n\t\t\t       int tls_tx_imm, int tls_len, u32 credits)\n{\n\tunsigned short pdus_per_ulp = 0;\n\tstruct chtls_sock *csk;\n\tstruct chtls_hws *hws;\n\tint expn_sz;\n\tint pdus;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\thws = &csk->tlshws;\n\tpdus = DIV_ROUND_UP(tls_len, hws->mfs);\n\texpn_sz = chtls_expansion_size(sk, tls_len, 0, NULL);\n\tif (!hws->compute) {\n\t\thws->expansion = chtls_expansion_size(sk,\n\t\t\t\t\t\t      hws->fcplenmax,\n\t\t\t\t\t\t      1, &pdus_per_ulp);\n\t\thws->pdus = pdus_per_ulp;\n\t\thws->adjustlen = hws->pdus *\n\t\t\t((hws->expansion / hws->pdus) + hws->mfs);\n\t\thws->compute = 1;\n\t}\n\tif (tls_copy_ivs(sk, skb))\n\t\treturn;\n\ttls_copy_tx_key(sk, skb);\n\ttls_tx_data_wr(sk, skb, tls_len, tls_tx_imm, credits, expn_sz, pdus);\n\thws->tx_seq_no += (pdus - 1);\n}\n\nstatic void make_tx_data_wr(struct sock *sk, struct sk_buff *skb,\n\t\t\t    unsigned int immdlen, int len,\n\t\t\t    u32 credits, u32 compl)\n{\n\tstruct fw_ofld_tx_data_wr *req;\n\tunsigned int wr_ulp_mode_force;\n\tstruct chtls_sock *csk;\n\tunsigned int opcode;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\topcode = FW_OFLD_TX_DATA_WR;\n\n\treq = (struct fw_ofld_tx_data_wr *)__skb_push(skb, sizeof(*req));\n\treq->op_to_immdlen = htonl(WR_OP_V(opcode) |\n\t\t\t\tFW_WR_COMPL_V(compl) |\n\t\t\t\tFW_WR_IMMDLEN_V(immdlen));\n\treq->flowid_len16 = htonl(FW_WR_FLOWID_V(csk->tid) |\n\t\t\t\tFW_WR_LEN16_V(credits));\n\n\twr_ulp_mode_force = TX_ULP_MODE_V(csk->ulp_mode);\n\tif (is_sg_request(skb))\n\t\twr_ulp_mode_force |= FW_OFLD_TX_DATA_WR_ALIGNPLD_F |\n\t\t\t((tcp_sk(sk)->nonagle & TCP_NAGLE_OFF) ? 0 :\n\t\t\t\tFW_OFLD_TX_DATA_WR_SHOVE_F);\n\n\treq->tunnel_to_proxy = htonl(wr_ulp_mode_force |\n\t\t\tTX_URG_V(skb_urgent(skb)) |\n\t\t\tTX_SHOVE_V((!csk_flag(sk, CSK_TX_MORE_DATA)) &&\n\t\t\t\t   skb_queue_empty(&csk->txq)));\n\treq->plen = htonl(len);\n}\n\nstatic int chtls_wr_size(struct chtls_sock *csk, const struct sk_buff *skb,\n\t\t\t bool size)\n{\n\tint wr_size;\n\n\twr_size = TLS_WR_CPL_LEN;\n\twr_size += KEY_ON_MEM_SZ;\n\twr_size += ivs_size(csk->sk, skb);\n\n\tif (size)\n\t\treturn wr_size;\n\n\t \n\tif (!skb_ulp_tls_iv_imm(skb))\n\t\tskb_shinfo(skb)->nr_frags++;\n\n\treturn wr_size;\n}\n\nstatic bool is_ofld_imm(struct chtls_sock *csk, const struct sk_buff *skb)\n{\n\tint length = skb->len;\n\n\tif (skb->peeked || skb->len > MAX_IMM_ULPTX_WR_LEN)\n\t\treturn false;\n\n\tif (likely(ULP_SKB_CB(skb)->flags & ULPCB_FLAG_NEED_HDR)) {\n\t\t \n\t\tif (csk->ulp_mode == ULP_MODE_TLS &&\n\t\t    skb_ulp_tls_inline(skb))\n\t\t\tlength += chtls_wr_size(csk, skb, true);\n\t\telse\n\t\t\tlength += sizeof(struct fw_ofld_tx_data_wr);\n\n\t\treturn length <= MAX_IMM_OFLD_TX_DATA_WR_LEN;\n\t}\n\treturn true;\n}\n\nstatic unsigned int calc_tx_flits(const struct sk_buff *skb,\n\t\t\t\t  unsigned int immdlen)\n{\n\tunsigned int flits, cnt;\n\n\tflits = immdlen / 8;    \n\tcnt = skb_shinfo(skb)->nr_frags;\n\tif (skb_tail_pointer(skb) != skb_transport_header(skb))\n\t\tcnt++;\n\treturn flits + sgl_len(cnt);\n}\n\nstatic void arp_failure_discard(void *handle, struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n}\n\nint chtls_push_frames(struct chtls_sock *csk, int comp)\n{\n\tstruct chtls_hws *hws = &csk->tlshws;\n\tstruct tcp_sock *tp;\n\tstruct sk_buff *skb;\n\tint total_size = 0;\n\tstruct sock *sk;\n\tint wr_size;\n\n\twr_size = sizeof(struct fw_ofld_tx_data_wr);\n\tsk = csk->sk;\n\ttp = tcp_sk(sk);\n\n\tif (unlikely(sk_in_state(sk, TCPF_SYN_SENT | TCPF_CLOSE)))\n\t\treturn 0;\n\n\tif (unlikely(csk_flag(sk, CSK_ABORT_SHUTDOWN)))\n\t\treturn 0;\n\n\twhile (csk->wr_credits && (skb = skb_peek(&csk->txq)) &&\n\t       (!(ULP_SKB_CB(skb)->flags & ULPCB_FLAG_HOLD) ||\n\t\tskb_queue_len(&csk->txq) > 1)) {\n\t\tunsigned int credit_len = skb->len;\n\t\tunsigned int credits_needed;\n\t\tunsigned int completion = 0;\n\t\tint tls_len = skb->len; \n\t\tunsigned int immdlen;\n\t\tint len = skb->len;     \n\t\tint flowclen16 = 0;\n\t\tint tls_tx_imm = 0;\n\n\t\timmdlen = skb->len;\n\t\tif (!is_ofld_imm(csk, skb)) {\n\t\t\timmdlen = skb_transport_offset(skb);\n\t\t\tif (skb_ulp_tls_inline(skb))\n\t\t\t\twr_size = chtls_wr_size(csk, skb, false);\n\t\t\tcredit_len = 8 * calc_tx_flits(skb, immdlen);\n\t\t} else {\n\t\t\tif (skb_ulp_tls_inline(skb)) {\n\t\t\t\twr_size = chtls_wr_size(csk, skb, false);\n\t\t\t\ttls_tx_imm = 1;\n\t\t\t}\n\t\t}\n\t\tif (likely(ULP_SKB_CB(skb)->flags & ULPCB_FLAG_NEED_HDR))\n\t\t\tcredit_len += wr_size;\n\t\tcredits_needed = DIV_ROUND_UP(credit_len, 16);\n\t\tif (!csk_flag_nochk(csk, CSK_TX_DATA_SENT)) {\n\t\t\tflowclen16 = send_tx_flowc_wr(sk, 1, tp->snd_nxt,\n\t\t\t\t\t\t      tp->rcv_nxt);\n\t\t\tif (flowclen16 <= 0)\n\t\t\t\tbreak;\n\t\t\tcsk->wr_credits -= flowclen16;\n\t\t\tcsk->wr_unacked += flowclen16;\n\t\t\tcsk->wr_nondata += flowclen16;\n\t\t\tcsk_set_flag(csk, CSK_TX_DATA_SENT);\n\t\t}\n\n\t\tif (csk->wr_credits < credits_needed) {\n\t\t\tif (skb_ulp_tls_inline(skb) &&\n\t\t\t    !skb_ulp_tls_iv_imm(skb))\n\t\t\t\tskb_shinfo(skb)->nr_frags--;\n\t\t\tbreak;\n\t\t}\n\n\t\t__skb_unlink(skb, &csk->txq);\n\t\tskb_set_queue_mapping(skb, (csk->txq_idx << 1) |\n\t\t\t\t      CPL_PRIORITY_DATA);\n\t\tif (hws->ofld)\n\t\t\thws->txqid = (skb->queue_mapping >> 1);\n\t\tskb->csum = (__force __wsum)(credits_needed + csk->wr_nondata);\n\t\tcsk->wr_credits -= credits_needed;\n\t\tcsk->wr_unacked += credits_needed;\n\t\tcsk->wr_nondata = 0;\n\t\tenqueue_wr(csk, skb);\n\n\t\tif (likely(ULP_SKB_CB(skb)->flags & ULPCB_FLAG_NEED_HDR)) {\n\t\t\tif ((comp && csk->wr_unacked == credits_needed) ||\n\t\t\t    (ULP_SKB_CB(skb)->flags & ULPCB_FLAG_COMPL) ||\n\t\t\t    csk->wr_unacked >= csk->wr_max_credits / 2) {\n\t\t\t\tcompletion = 1;\n\t\t\t\tcsk->wr_unacked = 0;\n\t\t\t}\n\t\t\tif (skb_ulp_tls_inline(skb))\n\t\t\t\tmake_tlstx_data_wr(sk, skb, tls_tx_imm,\n\t\t\t\t\t\t   tls_len, credits_needed);\n\t\t\telse\n\t\t\t\tmake_tx_data_wr(sk, skb, immdlen, len,\n\t\t\t\t\t\tcredits_needed, completion);\n\t\t\ttp->snd_nxt += len;\n\t\t\ttp->lsndtime = tcp_jiffies32;\n\t\t\tif (completion)\n\t\t\t\tULP_SKB_CB(skb)->flags &= ~ULPCB_FLAG_NEED_HDR;\n\t\t} else {\n\t\t\tstruct cpl_close_con_req *req = cplhdr(skb);\n\t\t\tunsigned int cmd  = CPL_OPCODE_G(ntohl\n\t\t\t\t\t     (OPCODE_TID(req)));\n\n\t\t\tif (cmd == CPL_CLOSE_CON_REQ)\n\t\t\t\tcsk_set_flag(csk,\n\t\t\t\t\t     CSK_CLOSE_CON_REQUESTED);\n\n\t\t\tif ((ULP_SKB_CB(skb)->flags & ULPCB_FLAG_COMPL) &&\n\t\t\t    (csk->wr_unacked >= csk->wr_max_credits / 2)) {\n\t\t\t\treq->wr.wr_hi |= htonl(FW_WR_COMPL_F);\n\t\t\t\tcsk->wr_unacked = 0;\n\t\t\t}\n\t\t}\n\t\ttotal_size += skb->truesize;\n\t\tif (ULP_SKB_CB(skb)->flags & ULPCB_FLAG_BARRIER)\n\t\t\tcsk_set_flag(csk, CSK_TX_WAIT_IDLE);\n\t\tt4_set_arp_err_handler(skb, NULL, arp_failure_discard);\n\t\tcxgb4_l2t_send(csk->egress_dev, skb, csk->l2t_entry);\n\t}\n\tsk->sk_wmem_queued -= total_size;\n\treturn total_size;\n}\n\nstatic void mark_urg(struct tcp_sock *tp, int flags,\n\t\t     struct sk_buff *skb)\n{\n\tif (unlikely(flags & MSG_OOB)) {\n\t\ttp->snd_up = tp->write_seq;\n\t\tULP_SKB_CB(skb)->flags = ULPCB_FLAG_URG |\n\t\t\t\t\t ULPCB_FLAG_BARRIER |\n\t\t\t\t\t ULPCB_FLAG_NO_APPEND |\n\t\t\t\t\t ULPCB_FLAG_NEED_HDR;\n\t}\n}\n\n \nstatic bool should_push(struct sock *sk)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct chtls_dev *cdev = csk->cdev;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t \n\tif (!cdev)\n\t\treturn false;\n\n\t \n\treturn csk->wr_credits == csk->wr_max_credits ||\n\t\t(tp->nonagle & TCP_NAGLE_OFF);\n}\n\n \nstatic bool corked(const struct tcp_sock *tp, int flags)\n{\n\treturn (flags & MSG_MORE) || (tp->nonagle & TCP_NAGLE_CORK);\n}\n\n \nstatic bool send_should_push(struct sock *sk, int flags)\n{\n\treturn should_push(sk) && !corked(tcp_sk(sk), flags);\n}\n\nvoid chtls_tcp_push(struct sock *sk, int flags)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tint qlen = skb_queue_len(&csk->txq);\n\n\tif (likely(qlen)) {\n\t\tstruct sk_buff *skb = skb_peek_tail(&csk->txq);\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tmark_urg(tp, flags, skb);\n\n\t\tif (!(ULP_SKB_CB(skb)->flags & ULPCB_FLAG_NO_APPEND) &&\n\t\t    corked(tp, flags)) {\n\t\t\tULP_SKB_CB(skb)->flags |= ULPCB_FLAG_HOLD;\n\t\t\treturn;\n\t\t}\n\n\t\tULP_SKB_CB(skb)->flags &= ~ULPCB_FLAG_HOLD;\n\t\tif (qlen == 1 &&\n\t\t    ((ULP_SKB_CB(skb)->flags & ULPCB_FLAG_NO_APPEND) ||\n\t\t     should_push(sk)))\n\t\t\tchtls_push_frames(csk, 1);\n\t}\n}\n\n \nstatic int select_size(struct sock *sk, int io_len, int flags, int len)\n{\n\tconst int pgbreak = SKB_MAX_HEAD(len);\n\n\t \n\tif (io_len > pgbreak)\n\t\treturn 0;\n\n\t \n\tif (!send_should_push(sk, flags))\n\t\treturn pgbreak;\n\n\treturn io_len;\n}\n\nvoid skb_entail(struct sock *sk, struct sk_buff *skb, int flags)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tULP_SKB_CB(skb)->seq = tp->write_seq;\n\tULP_SKB_CB(skb)->flags = flags;\n\t__skb_queue_tail(&csk->txq, skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\n\tif (TCP_PAGE(sk) && TCP_OFF(sk)) {\n\t\tput_page(TCP_PAGE(sk));\n\t\tTCP_PAGE(sk) = NULL;\n\t\tTCP_OFF(sk) = 0;\n\t}\n}\n\nstatic struct sk_buff *get_tx_skb(struct sock *sk, int size)\n{\n\tstruct sk_buff *skb;\n\n\tskb = alloc_skb(size + TX_HEADER_LEN, sk->sk_allocation);\n\tif (likely(skb)) {\n\t\tskb_reserve(skb, TX_HEADER_LEN);\n\t\tskb_entail(sk, skb, ULPCB_FLAG_NEED_HDR);\n\t\tskb_reset_transport_header(skb);\n\t}\n\treturn skb;\n}\n\nstatic struct sk_buff *get_record_skb(struct sock *sk, int size, bool zcopy)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct sk_buff *skb;\n\n\tskb = alloc_skb(((zcopy ? 0 : size) + TX_TLSHDR_LEN +\n\t\t\tKEY_ON_MEM_SZ + max_ivs_size(sk, size)),\n\t\t\tsk->sk_allocation);\n\tif (likely(skb)) {\n\t\tskb_reserve(skb, (TX_TLSHDR_LEN +\n\t\t\t    KEY_ON_MEM_SZ + max_ivs_size(sk, size)));\n\t\tskb_entail(sk, skb, ULPCB_FLAG_NEED_HDR);\n\t\tskb_reset_transport_header(skb);\n\t\tULP_SKB_CB(skb)->ulp.tls.ofld = 1;\n\t\tULP_SKB_CB(skb)->ulp.tls.type = csk->tlshws.type;\n\t}\n\treturn skb;\n}\n\nstatic void tx_skb_finalize(struct sk_buff *skb)\n{\n\tstruct ulp_skb_cb *cb = ULP_SKB_CB(skb);\n\n\tif (!(cb->flags & ULPCB_FLAG_NO_HDR))\n\t\tcb->flags = ULPCB_FLAG_NEED_HDR;\n\tcb->flags |= ULPCB_FLAG_NO_APPEND;\n}\n\nstatic void push_frames_if_head(struct sock *sk)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\n\tif (skb_queue_len(&csk->txq) == 1)\n\t\tchtls_push_frames(csk, 1);\n}\n\nstatic int chtls_skb_copy_to_page_nocache(struct sock *sk,\n\t\t\t\t\t  struct iov_iter *from,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  struct page *page,\n\t\t\t\t\t  int off, int copy)\n{\n\tint err;\n\n\terr = skb_do_copy_data_nocache(sk, skb, from, page_address(page) +\n\t\t\t\t       off, copy, skb->len);\n\tif (err)\n\t\treturn err;\n\n\tskb->len             += copy;\n\tskb->data_len        += copy;\n\tskb->truesize        += copy;\n\tsk->sk_wmem_queued   += copy;\n\treturn 0;\n}\n\nstatic bool csk_mem_free(struct chtls_dev *cdev, struct sock *sk)\n{\n\treturn (cdev->max_host_sndbuf - sk->sk_wmem_queued > 0);\n}\n\nstatic int csk_wait_memory(struct chtls_dev *cdev,\n\t\t\t   struct sock *sk, long *timeo_p)\n{\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\tint ret, err = 0;\n\tlong current_timeo;\n\tlong vm_wait = 0;\n\tbool noblock;\n\n\tcurrent_timeo = *timeo_p;\n\tnoblock = (*timeo_p ? false : true);\n\tif (csk_mem_free(cdev, sk)) {\n\t\tcurrent_timeo = get_random_u32_below(HZ / 5) + 2;\n\t\tvm_wait = get_random_u32_below(HZ / 5) + 2;\n\t}\n\n\tadd_wait_queue(sk_sleep(sk), &wait);\n\twhile (1) {\n\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\t\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\t\tgoto do_error;\n\t\tif (!*timeo_p) {\n\t\t\tif (noblock)\n\t\t\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\t\tgoto do_nonblock;\n\t\t}\n\t\tif (signal_pending(current))\n\t\t\tgoto do_interrupted;\n\t\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\t\tif (csk_mem_free(cdev, sk) && !vm_wait)\n\t\t\tbreak;\n\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\tsk->sk_write_pending++;\n\t\tret = sk_wait_event(sk, &current_timeo, sk->sk_err ||\n\t\t\t\t    (sk->sk_shutdown & SEND_SHUTDOWN) ||\n\t\t\t\t    (csk_mem_free(cdev, sk) && !vm_wait),\n\t\t\t\t    &wait);\n\t\tsk->sk_write_pending--;\n\t\tif (ret < 0)\n\t\t\tgoto do_error;\n\n\t\tif (vm_wait) {\n\t\t\tvm_wait -= current_timeo;\n\t\t\tcurrent_timeo = *timeo_p;\n\t\t\tif (current_timeo != MAX_SCHEDULE_TIMEOUT) {\n\t\t\t\tcurrent_timeo -= vm_wait;\n\t\t\t\tif (current_timeo < 0)\n\t\t\t\t\tcurrent_timeo = 0;\n\t\t\t}\n\t\t\tvm_wait = 0;\n\t\t}\n\t\t*timeo_p = current_timeo;\n\t}\ndo_rm_wq:\n\tremove_wait_queue(sk_sleep(sk), &wait);\n\treturn err;\ndo_error:\n\terr = -EPIPE;\n\tgoto do_rm_wq;\ndo_nonblock:\n\terr = -EAGAIN;\n\tgoto do_rm_wq;\ndo_interrupted:\n\terr = sock_intr_errno(*timeo_p);\n\tgoto do_rm_wq;\n}\n\nstatic int chtls_proccess_cmsg(struct sock *sk, struct msghdr *msg,\n\t\t\t       unsigned char *record_type)\n{\n\tstruct cmsghdr *cmsg;\n\tint rc = -EINVAL;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\treturn -EINVAL;\n\t\tif (cmsg->cmsg_level != SOL_TLS)\n\t\t\tcontinue;\n\n\t\tswitch (cmsg->cmsg_type) {\n\t\tcase TLS_SET_RECORD_TYPE:\n\t\t\tif (cmsg->cmsg_len < CMSG_LEN(sizeof(*record_type)))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (msg->msg_flags & MSG_MORE)\n\t\t\t\treturn -EINVAL;\n\n\t\t\t*record_type = *(unsigned char *)CMSG_DATA(cmsg);\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nint chtls_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct chtls_dev *cdev = csk->cdev;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint mss, flags, err;\n\tint recordsz = 0;\n\tint copied = 0;\n\tlong timeo;\n\n\tlock_sock(sk);\n\tflags = msg->msg_flags;\n\ttimeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\tif (!sk_in_state(sk, TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) {\n\t\terr = sk_stream_wait_connect(sk, &timeo);\n\t\tif (err)\n\t\t\tgoto out_err;\n\t}\n\n\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto out_err;\n\n\tmss = csk->mss;\n\tcsk_set_flag(csk, CSK_TX_MORE_DATA);\n\n\twhile (msg_data_left(msg)) {\n\t\tint copy = 0;\n\n\t\tskb = skb_peek_tail(&csk->txq);\n\t\tif (skb) {\n\t\t\tcopy = mss - skb->len;\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t}\n\t\tif (!csk_mem_free(cdev, sk))\n\t\t\tgoto wait_for_sndbuf;\n\n\t\tif (is_tls_tx(csk) && !csk->tlshws.txleft) {\n\t\t\tunsigned char record_type = TLS_RECORD_TYPE_DATA;\n\n\t\t\tif (unlikely(msg->msg_controllen)) {\n\t\t\t\terr = chtls_proccess_cmsg(sk, msg,\n\t\t\t\t\t\t\t  &record_type);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_err;\n\n\t\t\t\t \n\t\t\t\tif (skb)\n\t\t\t\t\ttx_skb_finalize(skb);\n\t\t\t}\n\n\t\t\trecordsz = size;\n\t\t\tcsk->tlshws.txleft = recordsz;\n\t\t\tcsk->tlshws.type = record_type;\n\t\t}\n\n\t\tif (!skb || (ULP_SKB_CB(skb)->flags & ULPCB_FLAG_NO_APPEND) ||\n\t\t    copy <= 0) {\nnew_buf:\n\t\t\tif (skb) {\n\t\t\t\ttx_skb_finalize(skb);\n\t\t\t\tpush_frames_if_head(sk);\n\t\t\t}\n\n\t\t\tif (is_tls_tx(csk)) {\n\t\t\t\tskb = get_record_skb(sk,\n\t\t\t\t\t\t     select_size(sk,\n\t\t\t\t\t\t\t\t recordsz,\n\t\t\t\t\t\t\t\t flags,\n\t\t\t\t\t\t\t\t TX_TLSHDR_LEN),\n\t\t\t\t\t\t\t\t false);\n\t\t\t} else {\n\t\t\t\tskb = get_tx_skb(sk,\n\t\t\t\t\t\t select_size(sk, size, flags,\n\t\t\t\t\t\t\t     TX_HEADER_LEN));\n\t\t\t}\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tcopy = mss;\n\t\t}\n\t\tif (copy > size)\n\t\t\tcopy = size;\n\n\t\tif (msg->msg_flags & MSG_SPLICE_PAGES) {\n\t\t\terr = skb_splice_from_iter(skb, &msg->msg_iter, copy,\n\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\tif (err < 0) {\n\t\t\t\tif (err == -EMSGSIZE)\n\t\t\t\t\tgoto new_buf;\n\t\t\t\tgoto do_fault;\n\t\t\t}\n\t\t\tcopy = err;\n\t\t\tsk_wmem_queued_add(sk, copy);\n\t\t} else if (skb_tailroom(skb) > 0) {\n\t\t\tcopy = min(copy, skb_tailroom(skb));\n\t\t\tif (is_tls_tx(csk))\n\t\t\t\tcopy = min_t(int, copy, csk->tlshws.txleft);\n\t\t\terr = skb_add_data_nocache(sk, skb,\n\t\t\t\t\t\t   &msg->msg_iter, copy);\n\t\t\tif (err)\n\t\t\t\tgoto do_fault;\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\tstruct page *page = TCP_PAGE(sk);\n\t\t\tint pg_size = PAGE_SIZE;\n\t\t\tint off = TCP_OFF(sk);\n\t\t\tbool merge;\n\n\t\t\tif (page)\n\t\t\t\tpg_size = page_size(page);\n\t\t\tif (off < pg_size &&\n\t\t\t    skb_can_coalesce(skb, i, page, off)) {\n\t\t\t\tmerge = true;\n\t\t\t\tgoto copy;\n\t\t\t}\n\t\t\tmerge = false;\n\t\t\tif (i == (is_tls_tx(csk) ? (MAX_SKB_FRAGS - 1) :\n\t\t\t    MAX_SKB_FRAGS))\n\t\t\t\tgoto new_buf;\n\n\t\t\tif (page && off == pg_size) {\n\t\t\t\tput_page(page);\n\t\t\t\tTCP_PAGE(sk) = page = NULL;\n\t\t\t\tpg_size = PAGE_SIZE;\n\t\t\t}\n\n\t\t\tif (!page) {\n\t\t\t\tgfp_t gfp = sk->sk_allocation;\n\t\t\t\tint order = cdev->send_page_order;\n\n\t\t\t\tif (order) {\n\t\t\t\t\tpage = alloc_pages(gfp | __GFP_COMP |\n\t\t\t\t\t\t\t   __GFP_NOWARN |\n\t\t\t\t\t\t\t   __GFP_NORETRY,\n\t\t\t\t\t\t\t   order);\n\t\t\t\t\tif (page)\n\t\t\t\t\t\tpg_size <<= order;\n\t\t\t\t}\n\t\t\t\tif (!page) {\n\t\t\t\t\tpage = alloc_page(gfp);\n\t\t\t\t\tpg_size = PAGE_SIZE;\n\t\t\t\t}\n\t\t\t\tif (!page)\n\t\t\t\t\tgoto wait_for_memory;\n\t\t\t\toff = 0;\n\t\t\t}\ncopy:\n\t\t\tif (copy > pg_size - off)\n\t\t\t\tcopy = pg_size - off;\n\t\t\tif (is_tls_tx(csk))\n\t\t\t\tcopy = min_t(int, copy, csk->tlshws.txleft);\n\n\t\t\terr = chtls_skb_copy_to_page_nocache(sk, &msg->msg_iter,\n\t\t\t\t\t\t\t     skb, page,\n\t\t\t\t\t\t\t     off, copy);\n\t\t\tif (unlikely(err)) {\n\t\t\t\tif (!TCP_PAGE(sk)) {\n\t\t\t\t\tTCP_PAGE(sk) = page;\n\t\t\t\t\tTCP_OFF(sk) = 0;\n\t\t\t\t}\n\t\t\t\tgoto do_fault;\n\t\t\t}\n\t\t\t \n\t\t\tif (merge) {\n\t\t\t\tskb_frag_size_add(\n\t\t\t\t\t\t&skb_shinfo(skb)->frags[i - 1],\n\t\t\t\t\t\tcopy);\n\t\t\t} else {\n\t\t\t\tskb_fill_page_desc(skb, i, page, off, copy);\n\t\t\t\tif (off + copy < pg_size) {\n\t\t\t\t\t \n\t\t\t\t\tget_page(page);\n\t\t\t\t\tTCP_PAGE(sk) = page;\n\t\t\t\t} else {\n\t\t\t\t\tTCP_PAGE(sk) = NULL;\n\t\t\t\t}\n\t\t\t}\n\t\t\tTCP_OFF(sk) = off + copy;\n\t\t}\n\t\tif (unlikely(skb->len == mss))\n\t\t\ttx_skb_finalize(skb);\n\t\ttp->write_seq += copy;\n\t\tcopied += copy;\n\t\tsize -= copy;\n\n\t\tif (is_tls_tx(csk))\n\t\t\tcsk->tlshws.txleft -= copy;\n\n\t\tif (corked(tp, flags) &&\n\t\t    (sk_stream_wspace(sk) < sk_stream_min_wspace(sk)))\n\t\t\tULP_SKB_CB(skb)->flags |= ULPCB_FLAG_NO_APPEND;\n\n\t\tif (size == 0)\n\t\t\tgoto out;\n\n\t\tif (ULP_SKB_CB(skb)->flags & ULPCB_FLAG_NO_APPEND)\n\t\t\tpush_frames_if_head(sk);\n\t\tcontinue;\nwait_for_sndbuf:\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\terr = csk_wait_memory(cdev, sk, &timeo);\n\t\tif (err)\n\t\t\tgoto do_error;\n\t}\nout:\n\tcsk_reset_flag(csk, CSK_TX_MORE_DATA);\n\tif (copied)\n\t\tchtls_tcp_push(sk, flags);\ndone:\n\trelease_sock(sk);\n\treturn copied;\ndo_fault:\n\tif (!skb->len) {\n\t\t__skb_unlink(skb, &csk->txq);\n\t\tsk->sk_wmem_queued -= skb->truesize;\n\t\t__kfree_skb(skb);\n\t}\ndo_error:\n\tif (copied)\n\t\tgoto out;\nout_err:\n\tif (csk_conn_inline(csk))\n\t\tcsk_reset_flag(csk, CSK_TX_MORE_DATA);\n\tcopied = sk_stream_error(sk, flags, err);\n\tgoto done;\n}\n\nvoid chtls_splice_eof(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tlock_sock(sk);\n\tchtls_tcp_push(sk, 0);\n\trelease_sock(sk);\n}\n\nstatic void chtls_select_window(struct sock *sk)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int wnd = tp->rcv_wnd;\n\n\twnd = max_t(unsigned int, wnd, tcp_full_space(sk));\n\twnd = max_t(unsigned int, MIN_RCV_WND, wnd);\n\n\tif (wnd > MAX_RCV_WND)\n\t\twnd = MAX_RCV_WND;\n\n \n\n\tif (wnd > tp->rcv_wnd) {\n\t\ttp->rcv_wup -= wnd - tp->rcv_wnd;\n\t\ttp->rcv_wnd = wnd;\n\t\t \n\t\tcsk_reset_flag(csk, CSK_UPDATE_RCV_WND);\n\t}\n}\n\n \nstatic u32 send_rx_credits(struct chtls_sock *csk, u32 credits)\n{\n\tstruct cpl_rx_data_ack *req;\n\tstruct sk_buff *skb;\n\n\tskb = alloc_skb(sizeof(*req), GFP_ATOMIC);\n\tif (!skb)\n\t\treturn 0;\n\t__skb_put(skb, sizeof(*req));\n\treq = (struct cpl_rx_data_ack *)skb->head;\n\n\tset_wr_txq(skb, CPL_PRIORITY_ACK, csk->port_id);\n\tINIT_TP_WR(req, csk->tid);\n\tOPCODE_TID(req) = cpu_to_be32(MK_OPCODE_TID(CPL_RX_DATA_ACK,\n\t\t\t\t\t\t    csk->tid));\n\treq->credit_dack = cpu_to_be32(RX_CREDITS_V(credits) |\n\t\t\t\t       RX_FORCE_ACK_F);\n\tcxgb4_ofld_send(csk->cdev->ports[csk->port_id], skb);\n\treturn credits;\n}\n\n#define CREDIT_RETURN_STATE (TCPF_ESTABLISHED | \\\n\t\t\t     TCPF_FIN_WAIT1 | \\\n\t\t\t     TCPF_FIN_WAIT2)\n\n \nstatic void chtls_cleanup_rbuf(struct sock *sk, int copied)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct tcp_sock *tp;\n\tint must_send;\n\tu32 credits;\n\tu32 thres;\n\n\tthres = 15 * 1024;\n\n\tif (!sk_in_state(sk, CREDIT_RETURN_STATE))\n\t\treturn;\n\n\tchtls_select_window(sk);\n\ttp = tcp_sk(sk);\n\tcredits = tp->copied_seq - tp->rcv_wup;\n\tif (unlikely(!credits))\n\t\treturn;\n\n \n\tmust_send = credits + 16384 >= tp->rcv_wnd;\n\n\tif (must_send || credits >= thres)\n\t\ttp->rcv_wup += send_rx_credits(csk, credits);\n}\n\nstatic int chtls_pt_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t\t    int flags, int *addr_len)\n{\n\tstruct chtls_sock *csk = rcu_dereference_sk_user_data(sk);\n\tstruct chtls_hws *hws = &csk->tlshws;\n\tstruct net_device *dev = csk->egress_dev;\n\tstruct adapter *adap = netdev2adap(dev);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned long avail;\n\tint buffers_freed;\n\tint copied = 0;\n\tint target;\n\tlong timeo;\n\tint ret;\n\n\tbuffers_freed = 0;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\n\tif (unlikely(csk_flag(sk, CSK_UPDATE_RCV_WND)))\n\t\tchtls_cleanup_rbuf(sk, copied);\n\n\tdo {\n\t\tstruct sk_buff *skb;\n\t\tu32 offset = 0;\n\n\t\tif (unlikely(tp->urg_data &&\n\t\t\t     tp->urg_seq == tp->copied_seq)) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = timeo ? sock_intr_errno(timeo) :\n\t\t\t\t\t-EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tgoto found_ok_skb;\n\t\tif (csk->wr_credits &&\n\t\t    skb_queue_len(&csk->txq) &&\n\t\t    chtls_push_frames(csk, csk->wr_credits ==\n\t\t\t\t      csk->wr_max_credits))\n\t\t\tsk->sk_write_space(sk);\n\n\t\tif (copied >= target && !READ_ONCE(sk->sk_backlog.tail))\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err || sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    signal_pending(current))\n\t\t\t\tbreak;\n\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (READ_ONCE(sk->sk_backlog.tail)) {\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t\tchtls_cleanup_rbuf(sk, copied);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copied >= target)\n\t\t\tbreak;\n\t\tchtls_cleanup_rbuf(sk, copied);\n\t\tret = sk_wait_data(sk, &timeo, NULL);\n\t\tif (ret < 0) {\n\t\t\tcopied = copied ? : ret;\n\t\t\tgoto unlock;\n\t\t}\n\t\tcontinue;\nfound_ok_skb:\n\t\tif (!skb->len) {\n\t\t\tskb_dst_set(skb, NULL);\n\t\t\t__skb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tkfree_skb(skb);\n\n\t\t\tif (!copied && !timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (copied < target) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\tlock_sock(sk);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\toffset = hws->copied_seq;\n\t\tavail = skb->len - offset;\n\t\tif (len < avail)\n\t\t\tavail = len;\n\n\t\tif (unlikely(tp->urg_data)) {\n\t\t\tu32 urg_offset = tp->urg_seq - tp->copied_seq;\n\n\t\t\tif (urg_offset < avail) {\n\t\t\t\tif (urg_offset) {\n\t\t\t\t\tavail = urg_offset;\n\t\t\t\t} else if (!sock_flag(sk, SOCK_URGINLINE)) {\n\t\t\t\t\t \n\t\t\t\t\ttp->copied_seq++;\n\t\t\t\t\toffset++;\n\t\t\t\t\tavail--;\n\t\t\t\t\tif (!avail)\n\t\t\t\t\t\tgoto skip_copy;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (ULP_SKB_CB(skb)->flags & ULPCB_FLAG_TLS_HDR) {\n\t\t\tstruct tls_hdr *thdr = (struct tls_hdr *)skb->data;\n\t\t\tint cerr = 0;\n\n\t\t\tcerr = put_cmsg(msg, SOL_TLS, TLS_GET_RECORD_TYPE,\n\t\t\t\t\tsizeof(thdr->type), &thdr->type);\n\n\t\t\tif (cerr && thdr->type != TLS_RECORD_TYPE_DATA) {\n\t\t\t\tcopied = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t \n\t\t\tgoto skip_copy;\n\t\t}\n\n\t\tif (skb_copy_datagram_msg(skb, offset, msg, avail)) {\n\t\t\tif (!copied) {\n\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tcopied += avail;\n\t\tlen -= avail;\n\t\thws->copied_seq += avail;\nskip_copy:\n\t\tif (tp->urg_data && after(tp->copied_seq, tp->urg_seq))\n\t\t\ttp->urg_data = 0;\n\n\t\tif ((avail + offset) >= skb->len) {\n\t\t\tstruct sk_buff *next_skb;\n\t\t\tif (ULP_SKB_CB(skb)->flags & ULPCB_FLAG_TLS_HDR) {\n\t\t\t\ttp->copied_seq += skb->len;\n\t\t\t\thws->rcvpld = skb->hdr_len;\n\t\t\t} else {\n\t\t\t\tatomic_inc(&adap->chcr_stats.tls_pdu_rx);\n\t\t\t\ttp->copied_seq += hws->rcvpld;\n\t\t\t}\n\t\t\tchtls_free_skb(sk, skb);\n\t\t\tbuffers_freed++;\n\t\t\thws->copied_seq = 0;\n\t\t\tnext_skb = skb_peek(&sk->sk_receive_queue);\n\t\t\tif (copied >= target && !next_skb)\n\t\t\t\tbreak;\n\t\t\tif (ULP_SKB_CB(next_skb)->flags & ULPCB_FLAG_TLS_HDR)\n\t\t\t\tbreak;\n\t\t}\n\t} while (len > 0);\n\n\tif (buffers_freed)\n\t\tchtls_cleanup_rbuf(sk, copied);\n\nunlock:\n\trelease_sock(sk);\n\treturn copied;\n}\n\n \nstatic int peekmsg(struct sock *sk, struct msghdr *msg,\n\t\t   size_t len, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 peek_seq, offset;\n\tstruct sk_buff *skb;\n\tint copied = 0;\n\tsize_t avail;           \n\tlong timeo;\n\tint ret;\n\n\tlock_sock(sk);\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\tpeek_seq = tp->copied_seq;\n\n\tdo {\n\t\tif (unlikely(tp->urg_data && tp->urg_seq == peek_seq)) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = timeo ? sock_intr_errno(timeo) :\n\t\t\t\t-EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tskb_queue_walk(&sk->sk_receive_queue, skb) {\n\t\t\toffset = peek_seq - ULP_SKB_CB(skb)->seq;\n\t\t\tif (offset < skb->len)\n\t\t\t\tgoto found_ok_skb;\n\t\t}\n\n\t\t \n\t\tif (copied)\n\t\t\tbreak;\n\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\tbreak;\n\t\tif (sk->sk_err) {\n\t\t\tcopied = sock_error(sk);\n\t\t\tbreak;\n\t\t}\n\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\tbreak;\n\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\tcopied = -ENOTCONN;\n\t\t\tbreak;\n\t\t}\n\t\tif (!timeo) {\n\t\t\tcopied = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tcopied = sock_intr_errno(timeo);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (READ_ONCE(sk->sk_backlog.tail)) {\n\t\t\t \n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else {\n\t\t\tret = sk_wait_data(sk, &timeo, NULL);\n\t\t\tif (ret < 0) {\n\t\t\t\t \n\t\t\t\tcopied = ret;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(peek_seq != tp->copied_seq)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tpr_info(\"TCP(%s:%d), race in MSG_PEEK.\\n\",\n\t\t\t\t\tcurrent->comm, current->pid);\n\t\t\tpeek_seq = tp->copied_seq;\n\t\t}\n\t\tcontinue;\n\nfound_ok_skb:\n\t\tavail = skb->len - offset;\n\t\tif (len < avail)\n\t\t\tavail = len;\n\t\t \n\t\tif (unlikely(tp->urg_data)) {\n\t\t\tu32 urg_offset = tp->urg_seq - peek_seq;\n\n\t\t\tif (urg_offset < avail) {\n\t\t\t\t \n\t\t\t\tif (!urg_offset) {  \n\t\t\t\t\tif (!sock_flag(sk, SOCK_URGINLINE)) {\n\t\t\t\t\t\tpeek_seq++;\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t\tavail--;\n\t\t\t\t\t}\n\t\t\t\t\tif (!avail)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tavail = urg_offset;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (likely(!(flags & MSG_TRUNC)))\n\t\t\tif (skb_copy_datagram_msg(skb, offset, msg, len)) {\n\t\t\t\tif (!copied) {\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\tpeek_seq += avail;\n\t\tcopied += avail;\n\t\tlen -= avail;\n\t} while (len > 0);\n\n\trelease_sock(sk);\n\treturn copied;\n}\n\nint chtls_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\t  int flags, int *addr_len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct chtls_sock *csk;\n\tunsigned long avail;     \n\tint buffers_freed;\n\tint copied = 0;\n\tlong timeo;\n\tint target;              \n\tint ret;\n\n\tbuffers_freed = 0;\n\n\tif (unlikely(flags & MSG_OOB))\n\t\treturn tcp_prot.recvmsg(sk, msg, len, flags, addr_len);\n\n\tif (unlikely(flags & MSG_PEEK))\n\t\treturn peekmsg(sk, msg, len, flags);\n\n\tif (sk_can_busy_loop(sk) &&\n\t    skb_queue_empty_lockless(&sk->sk_receive_queue) &&\n\t    sk->sk_state == TCP_ESTABLISHED)\n\t\tsk_busy_loop(sk, flags & MSG_DONTWAIT);\n\n\tlock_sock(sk);\n\tcsk = rcu_dereference_sk_user_data(sk);\n\n\tif (is_tls_rx(csk))\n\t\treturn chtls_pt_recvmsg(sk, msg, len, flags, addr_len);\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\n\tif (unlikely(csk_flag(sk, CSK_UPDATE_RCV_WND)))\n\t\tchtls_cleanup_rbuf(sk, copied);\n\n\tdo {\n\t\tstruct sk_buff *skb;\n\t\tu32 offset;\n\n\t\tif (unlikely(tp->urg_data && tp->urg_seq == tp->copied_seq)) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = timeo ? sock_intr_errno(timeo) :\n\t\t\t\t\t-EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tgoto found_ok_skb;\n\n\t\tif (csk->wr_credits &&\n\t\t    skb_queue_len(&csk->txq) &&\n\t\t    chtls_push_frames(csk, csk->wr_credits ==\n\t\t\t\t      csk->wr_max_credits))\n\t\t\tsk->sk_write_space(sk);\n\n\t\tif (copied >= target && !READ_ONCE(sk->sk_backlog.tail))\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err || sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    signal_pending(current))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (READ_ONCE(sk->sk_backlog.tail)) {\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t\tchtls_cleanup_rbuf(sk, copied);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copied >= target)\n\t\t\tbreak;\n\t\tchtls_cleanup_rbuf(sk, copied);\n\t\tret = sk_wait_data(sk, &timeo, NULL);\n\t\tif (ret < 0) {\n\t\t\tcopied = copied ? : ret;\n\t\t\tgoto unlock;\n\t\t}\n\t\tcontinue;\n\nfound_ok_skb:\n\t\tif (!skb->len) {\n\t\t\tchtls_kfree_skb(sk, skb);\n\t\t\tif (!copied && !timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (copied < target)\n\t\t\t\tcontinue;\n\n\t\t\tbreak;\n\t\t}\n\n\t\toffset = tp->copied_seq - ULP_SKB_CB(skb)->seq;\n\t\tavail = skb->len - offset;\n\t\tif (len < avail)\n\t\t\tavail = len;\n\n\t\tif (unlikely(tp->urg_data)) {\n\t\t\tu32 urg_offset = tp->urg_seq - tp->copied_seq;\n\n\t\t\tif (urg_offset < avail) {\n\t\t\t\tif (urg_offset) {\n\t\t\t\t\tavail = urg_offset;\n\t\t\t\t} else if (!sock_flag(sk, SOCK_URGINLINE)) {\n\t\t\t\t\ttp->copied_seq++;\n\t\t\t\t\toffset++;\n\t\t\t\t\tavail--;\n\t\t\t\t\tif (!avail)\n\t\t\t\t\t\tgoto skip_copy;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (likely(!(flags & MSG_TRUNC))) {\n\t\t\tif (skb_copy_datagram_msg(skb, offset,\n\t\t\t\t\t\t  msg, avail)) {\n\t\t\t\tif (!copied) {\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\ttp->copied_seq += avail;\n\t\tcopied += avail;\n\t\tlen -= avail;\n\nskip_copy:\n\t\tif (tp->urg_data && after(tp->copied_seq, tp->urg_seq))\n\t\t\ttp->urg_data = 0;\n\n\t\tif (avail + offset >= skb->len) {\n\t\t\tchtls_free_skb(sk, skb);\n\t\t\tbuffers_freed++;\n\n\t\t\tif  (copied >= target &&\n\t\t\t     !skb_peek(&sk->sk_receive_queue))\n\t\t\t\tbreak;\n\t\t}\n\t} while (len > 0);\n\n\tif (buffers_freed)\n\t\tchtls_cleanup_rbuf(sk, copied);\n\nunlock:\n\trelease_sock(sk);\n\treturn copied;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}