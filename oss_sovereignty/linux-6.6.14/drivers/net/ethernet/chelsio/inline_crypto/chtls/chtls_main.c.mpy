{
  "module_name": "chtls_main.c",
  "hash_id": "567b75d869b759311fbc70f1171b2411e4e5eca9ed7cd5984f1056a0a838caae",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/inline_crypto/chtls/chtls_main.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/socket.h>\n#include <linux/hash.h>\n#include <linux/in.h>\n#include <linux/net.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <net/ipv6.h>\n#include <net/transp_v6.h>\n#include <net/tcp.h>\n#include <net/tls.h>\n\n#include \"chtls.h\"\n#include \"chtls_cm.h\"\n\n#define DRV_NAME \"chtls\"\n\n \nstatic LIST_HEAD(cdev_list);\nstatic DEFINE_MUTEX(cdev_mutex);\n\nstatic DEFINE_MUTEX(notify_mutex);\nstatic RAW_NOTIFIER_HEAD(listen_notify_list);\nstatic struct proto chtls_cpl_prot, chtls_cpl_protv6;\nstruct request_sock_ops chtls_rsk_ops, chtls_rsk_opsv6;\nstatic uint send_page_order = (14 - PAGE_SHIFT < 0) ? 0 : 14 - PAGE_SHIFT;\n\nstatic void register_listen_notifier(struct notifier_block *nb)\n{\n\tmutex_lock(&notify_mutex);\n\traw_notifier_chain_register(&listen_notify_list, nb);\n\tmutex_unlock(&notify_mutex);\n}\n\nstatic void unregister_listen_notifier(struct notifier_block *nb)\n{\n\tmutex_lock(&notify_mutex);\n\traw_notifier_chain_unregister(&listen_notify_list, nb);\n\tmutex_unlock(&notify_mutex);\n}\n\nstatic int listen_notify_handler(struct notifier_block *this,\n\t\t\t\t unsigned long event, void *data)\n{\n\tstruct chtls_listen *clisten;\n\tint ret = NOTIFY_DONE;\n\n\tclisten = (struct chtls_listen *)data;\n\n\tswitch (event) {\n\tcase CHTLS_LISTEN_START:\n\t\tret = chtls_listen_start(clisten->cdev, clisten->sk);\n\t\tkfree(clisten);\n\t\tbreak;\n\tcase CHTLS_LISTEN_STOP:\n\t\tchtls_listen_stop(clisten->cdev, clisten->sk);\n\t\tkfree(clisten);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic struct notifier_block listen_notifier = {\n\t.notifier_call = listen_notify_handler\n};\n\nstatic int listen_backlog_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tif (likely(skb_transport_header(skb) != skb_network_header(skb)))\n\t\treturn tcp_v4_do_rcv(sk, skb);\n\tBLOG_SKB_CB(skb)->backlog_rcv(sk, skb);\n\treturn 0;\n}\n\nstatic int chtls_start_listen(struct chtls_dev *cdev, struct sock *sk)\n{\n\tstruct chtls_listen *clisten;\n\n\tif (sk->sk_protocol != IPPROTO_TCP)\n\t\treturn -EPROTONOSUPPORT;\n\n\tif (sk->sk_family == PF_INET &&\n\t    LOOPBACK(inet_sk(sk)->inet_rcv_saddr))\n\t\treturn -EADDRNOTAVAIL;\n\n\tsk->sk_backlog_rcv = listen_backlog_rcv;\n\tclisten = kmalloc(sizeof(*clisten), GFP_KERNEL);\n\tif (!clisten)\n\t\treturn -ENOMEM;\n\tclisten->cdev = cdev;\n\tclisten->sk = sk;\n\tmutex_lock(&notify_mutex);\n\traw_notifier_call_chain(&listen_notify_list,\n\t\t\t\t      CHTLS_LISTEN_START, clisten);\n\tmutex_unlock(&notify_mutex);\n\treturn 0;\n}\n\nstatic void chtls_stop_listen(struct chtls_dev *cdev, struct sock *sk)\n{\n\tstruct chtls_listen *clisten;\n\n\tif (sk->sk_protocol != IPPROTO_TCP)\n\t\treturn;\n\n\tclisten = kmalloc(sizeof(*clisten), GFP_KERNEL);\n\tif (!clisten)\n\t\treturn;\n\tclisten->cdev = cdev;\n\tclisten->sk = sk;\n\tmutex_lock(&notify_mutex);\n\traw_notifier_call_chain(&listen_notify_list,\n\t\t\t\tCHTLS_LISTEN_STOP, clisten);\n\tmutex_unlock(&notify_mutex);\n}\n\nstatic int chtls_inline_feature(struct tls_toe_device *dev)\n{\n\tstruct net_device *netdev;\n\tstruct chtls_dev *cdev;\n\tint i;\n\n\tcdev = to_chtls_dev(dev);\n\n\tfor (i = 0; i < cdev->lldi->nports; i++) {\n\t\tnetdev = cdev->ports[i];\n\t\tif (netdev->features & NETIF_F_HW_TLS_RECORD)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int chtls_create_hash(struct tls_toe_device *dev, struct sock *sk)\n{\n\tstruct chtls_dev *cdev = to_chtls_dev(dev);\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn chtls_start_listen(cdev, sk);\n\treturn 0;\n}\n\nstatic void chtls_destroy_hash(struct tls_toe_device *dev, struct sock *sk)\n{\n\tstruct chtls_dev *cdev = to_chtls_dev(dev);\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\tchtls_stop_listen(cdev, sk);\n}\n\nstatic void chtls_free_uld(struct chtls_dev *cdev)\n{\n\tint i;\n\n\ttls_toe_unregister_device(&cdev->tlsdev);\n\tkvfree(cdev->kmap.addr);\n\tidr_destroy(&cdev->hwtid_idr);\n\tfor (i = 0; i < (1 << RSPQ_HASH_BITS); i++)\n\t\tkfree_skb(cdev->rspq_skb_cache[i]);\n\tkfree(cdev->lldi);\n\tkfree_skb(cdev->askb);\n\tkfree(cdev);\n}\n\nstatic inline void chtls_dev_release(struct kref *kref)\n{\n\tstruct tls_toe_device *dev;\n\tstruct chtls_dev *cdev;\n\tstruct adapter *adap;\n\n\tdev = container_of(kref, struct tls_toe_device, kref);\n\tcdev = to_chtls_dev(dev);\n\n\t \n\tadap = pci_get_drvdata(cdev->pdev);\n\tatomic_set(&adap->chcr_stats.tls_pdu_tx, 0);\n\tatomic_set(&adap->chcr_stats.tls_pdu_rx, 0);\n\n\tchtls_free_uld(cdev);\n}\n\nstatic void chtls_register_dev(struct chtls_dev *cdev)\n{\n\tstruct tls_toe_device *tlsdev = &cdev->tlsdev;\n\n\tstrscpy(tlsdev->name, \"chtls\", TLS_TOE_DEVICE_NAME_MAX);\n\tstrlcat(tlsdev->name, cdev->lldi->ports[0]->name,\n\t\tTLS_TOE_DEVICE_NAME_MAX);\n\ttlsdev->feature = chtls_inline_feature;\n\ttlsdev->hash = chtls_create_hash;\n\ttlsdev->unhash = chtls_destroy_hash;\n\ttlsdev->release = chtls_dev_release;\n\tkref_init(&tlsdev->kref);\n\ttls_toe_register_device(tlsdev);\n\tcdev->cdev_state = CHTLS_CDEV_STATE_UP;\n}\n\nstatic void process_deferq(struct work_struct *task_param)\n{\n\tstruct chtls_dev *cdev = container_of(task_param,\n\t\t\t\tstruct chtls_dev, deferq_task);\n\tstruct sk_buff *skb;\n\n\tspin_lock_bh(&cdev->deferq.lock);\n\twhile ((skb = __skb_dequeue(&cdev->deferq)) != NULL) {\n\t\tspin_unlock_bh(&cdev->deferq.lock);\n\t\tDEFERRED_SKB_CB(skb)->handler(cdev, skb);\n\t\tspin_lock_bh(&cdev->deferq.lock);\n\t}\n\tspin_unlock_bh(&cdev->deferq.lock);\n}\n\nstatic int chtls_get_skb(struct chtls_dev *cdev)\n{\n\tcdev->askb = alloc_skb(sizeof(struct tcphdr), GFP_KERNEL);\n\tif (!cdev->askb)\n\t\treturn -ENOMEM;\n\n\tskb_put(cdev->askb, sizeof(struct tcphdr));\n\tskb_reset_transport_header(cdev->askb);\n\tmemset(cdev->askb->data, 0, cdev->askb->len);\n\treturn 0;\n}\n\nstatic void *chtls_uld_add(const struct cxgb4_lld_info *info)\n{\n\tstruct cxgb4_lld_info *lldi;\n\tstruct chtls_dev *cdev;\n\tint i, j;\n\n\tcdev = kzalloc(sizeof(*cdev), GFP_KERNEL);\n\tif (!cdev)\n\t\tgoto out;\n\n\tlldi = kzalloc(sizeof(*lldi), GFP_KERNEL);\n\tif (!lldi)\n\t\tgoto out_lldi;\n\n\tif (chtls_get_skb(cdev))\n\t\tgoto out_skb;\n\n\t*lldi = *info;\n\tcdev->lldi = lldi;\n\tcdev->pdev = lldi->pdev;\n\tcdev->tids = lldi->tids;\n\tcdev->ports = lldi->ports;\n\tcdev->mtus = lldi->mtus;\n\tcdev->tids = lldi->tids;\n\tcdev->pfvf = FW_VIID_PFN_G(cxgb4_port_viid(lldi->ports[0]))\n\t\t\t<< FW_VIID_PFN_S;\n\n\tfor (i = 0; i < (1 << RSPQ_HASH_BITS); i++) {\n\t\tunsigned int size = 64 - sizeof(struct rsp_ctrl) - 8;\n\n\t\tcdev->rspq_skb_cache[i] = __alloc_skb(size,\n\t\t\t\t\t\t      gfp_any(), 0,\n\t\t\t\t\t\t      lldi->nodeid);\n\t\tif (unlikely(!cdev->rspq_skb_cache[i]))\n\t\t\tgoto out_rspq_skb;\n\t}\n\n\tidr_init(&cdev->hwtid_idr);\n\tINIT_WORK(&cdev->deferq_task, process_deferq);\n\tspin_lock_init(&cdev->listen_lock);\n\tspin_lock_init(&cdev->idr_lock);\n\tcdev->send_page_order = min_t(uint, get_order(32768),\n\t\t\t\t      send_page_order);\n\tcdev->max_host_sndbuf = 48 * 1024;\n\n\tif (lldi->vr->key.size)\n\t\tif (chtls_init_kmap(cdev, lldi))\n\t\t\tgoto out_rspq_skb;\n\n\tmutex_lock(&cdev_mutex);\n\tlist_add_tail(&cdev->list, &cdev_list);\n\tmutex_unlock(&cdev_mutex);\n\n\treturn cdev;\nout_rspq_skb:\n\tfor (j = 0; j < i; j++)\n\t\tkfree_skb(cdev->rspq_skb_cache[j]);\n\tkfree_skb(cdev->askb);\nout_skb:\n\tkfree(lldi);\nout_lldi:\n\tkfree(cdev);\nout:\n\treturn NULL;\n}\n\nstatic void chtls_free_all_uld(void)\n{\n\tstruct chtls_dev *cdev, *tmp;\n\n\tmutex_lock(&cdev_mutex);\n\tlist_for_each_entry_safe(cdev, tmp, &cdev_list, list) {\n\t\tif (cdev->cdev_state == CHTLS_CDEV_STATE_UP) {\n\t\t\tlist_del(&cdev->list);\n\t\t\tkref_put(&cdev->tlsdev.kref, cdev->tlsdev.release);\n\t\t}\n\t}\n\tmutex_unlock(&cdev_mutex);\n}\n\nstatic int chtls_uld_state_change(void *handle, enum cxgb4_state new_state)\n{\n\tstruct chtls_dev *cdev = handle;\n\n\tswitch (new_state) {\n\tcase CXGB4_STATE_UP:\n\t\tchtls_register_dev(cdev);\n\t\tbreak;\n\tcase CXGB4_STATE_DOWN:\n\t\tbreak;\n\tcase CXGB4_STATE_START_RECOVERY:\n\t\tbreak;\n\tcase CXGB4_STATE_DETACH:\n\t\tmutex_lock(&cdev_mutex);\n\t\tlist_del(&cdev->list);\n\t\tmutex_unlock(&cdev_mutex);\n\t\tkref_put(&cdev->tlsdev.kref, cdev->tlsdev.release);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic struct sk_buff *copy_gl_to_skb_pkt(const struct pkt_gl *gl,\n\t\t\t\t\t  const __be64 *rsp,\n\t\t\t\t\t  u32 pktshift)\n{\n\tstruct sk_buff *skb;\n\n\t \n\tskb = alloc_skb(gl->tot_len + sizeof(struct cpl_pass_accept_req)\n\t\t\t- pktshift, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\t__skb_put(skb, gl->tot_len + sizeof(struct cpl_pass_accept_req)\n\t\t   - pktshift);\n\t \n\tskb_copy_to_linear_data(skb, rsp, sizeof(struct cpl_rx_pkt));\n\tskb_copy_to_linear_data_offset(skb, sizeof(struct cpl_pass_accept_req)\n\t\t\t\t       , gl->va + pktshift,\n\t\t\t\t       gl->tot_len - pktshift);\n\n\treturn skb;\n}\n\nstatic int chtls_recv_packet(struct chtls_dev *cdev,\n\t\t\t     const struct pkt_gl *gl, const __be64 *rsp)\n{\n\tunsigned int opcode = *(u8 *)rsp;\n\tstruct sk_buff *skb;\n\tint ret;\n\n\tskb = copy_gl_to_skb_pkt(gl, rsp, cdev->lldi->sge_pktshift);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tret = chtls_handlers[opcode](cdev, skb);\n\tif (ret & CPL_RET_BUF_DONE)\n\t\tkfree_skb(skb);\n\n\treturn 0;\n}\n\nstatic int chtls_recv_rsp(struct chtls_dev *cdev, const __be64 *rsp)\n{\n\tunsigned long rspq_bin;\n\tunsigned int opcode;\n\tstruct sk_buff *skb;\n\tunsigned int len;\n\tint ret;\n\n\tlen = 64 - sizeof(struct rsp_ctrl) - 8;\n\topcode = *(u8 *)rsp;\n\n\trspq_bin = hash_ptr((void *)rsp, RSPQ_HASH_BITS);\n\tskb = cdev->rspq_skb_cache[rspq_bin];\n\tif (skb && !skb_is_nonlinear(skb) &&\n\t    !skb_shared(skb) && !skb_cloned(skb)) {\n\t\trefcount_inc(&skb->users);\n\t\tif (refcount_read(&skb->users) == 2) {\n\t\t\t__skb_trim(skb, 0);\n\t\t\tif (skb_tailroom(skb) >= len)\n\t\t\t\tgoto copy_out;\n\t\t}\n\t\trefcount_dec(&skb->users);\n\t}\n\tskb = alloc_skb(len, GFP_ATOMIC);\n\tif (unlikely(!skb))\n\t\treturn -ENOMEM;\n\ncopy_out:\n\t__skb_put(skb, len);\n\tskb_copy_to_linear_data(skb, rsp, len);\n\tskb_reset_network_header(skb);\n\tskb_reset_transport_header(skb);\n\tret = chtls_handlers[opcode](cdev, skb);\n\n\tif (ret & CPL_RET_BUF_DONE)\n\t\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic void chtls_recv(struct chtls_dev *cdev,\n\t\t       struct sk_buff **skbs, const __be64 *rsp)\n{\n\tstruct sk_buff *skb = *skbs;\n\tunsigned int opcode;\n\tint ret;\n\n\topcode = *(u8 *)rsp;\n\n\t__skb_push(skb, sizeof(struct rss_header));\n\tskb_copy_to_linear_data(skb, rsp, sizeof(struct rss_header));\n\n\tret = chtls_handlers[opcode](cdev, skb);\n\tif (ret & CPL_RET_BUF_DONE)\n\t\tkfree_skb(skb);\n}\n\nstatic int chtls_uld_rx_handler(void *handle, const __be64 *rsp,\n\t\t\t\tconst struct pkt_gl *gl)\n{\n\tstruct chtls_dev *cdev = handle;\n\tunsigned int opcode;\n\tstruct sk_buff *skb;\n\n\topcode = *(u8 *)rsp;\n\n\tif (unlikely(opcode == CPL_RX_PKT)) {\n\t\tif (chtls_recv_packet(cdev, gl, rsp) < 0)\n\t\t\tgoto nomem;\n\t\treturn 0;\n\t}\n\n\tif (!gl)\n\t\treturn chtls_recv_rsp(cdev, rsp);\n\n#define RX_PULL_LEN 128\n\tskb = cxgb4_pktgl_to_skb(gl, RX_PULL_LEN, RX_PULL_LEN);\n\tif (unlikely(!skb))\n\t\tgoto nomem;\n\tchtls_recv(cdev, &skb, rsp);\n\treturn 0;\n\nnomem:\n\treturn -ENOMEM;\n}\n\nstatic int do_chtls_getsockopt(struct sock *sk, char __user *optval,\n\t\t\t       int __user *optlen)\n{\n\tstruct tls_crypto_info crypto_info = { 0 };\n\n\tcrypto_info.version = TLS_1_2_VERSION;\n\tif (copy_to_user(optval, &crypto_info, sizeof(struct tls_crypto_info)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int chtls_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\n\tif (level != SOL_TLS)\n\t\treturn ctx->sk_proto->getsockopt(sk, level,\n\t\t\t\t\t\t optname, optval, optlen);\n\n\treturn do_chtls_getsockopt(sk, optval, optlen);\n}\n\nstatic int do_chtls_setsockopt(struct sock *sk, int optname,\n\t\t\t       sockptr_t optval, unsigned int optlen)\n{\n\tstruct tls_crypto_info *crypto_info, tmp_crypto_info;\n\tstruct chtls_sock *csk;\n\tint keylen;\n\tint cipher_type;\n\tint rc = 0;\n\n\tcsk = rcu_dereference_sk_user_data(sk);\n\n\tif (sockptr_is_null(optval) || optlen < sizeof(*crypto_info)) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\trc = copy_from_sockptr(&tmp_crypto_info, optval, sizeof(*crypto_info));\n\tif (rc) {\n\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t \n\tif (tmp_crypto_info.version != TLS_1_2_VERSION) {\n\t\trc = -ENOTSUPP;\n\t\tgoto out;\n\t}\n\n\tcrypto_info = (struct tls_crypto_info *)&csk->tlshws.crypto_info;\n\n\t \n\tswitch (tmp_crypto_info.cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\t \n\t\tcrypto_info[0] = tmp_crypto_info;\n\t\t \n\t\trc = copy_from_sockptr_offset((char *)crypto_info +\n\t\t\t\tsizeof(*crypto_info),\n\t\t\t\toptval, sizeof(*crypto_info),\n\t\t\t\tsizeof(struct tls12_crypto_info_aes_gcm_128)\n\t\t\t\t- sizeof(*crypto_info));\n\n\t\tif (rc) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkeylen = TLS_CIPHER_AES_GCM_128_KEY_SIZE;\n\t\tcipher_type = TLS_CIPHER_AES_GCM_128;\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tcrypto_info[0] = tmp_crypto_info;\n\t\trc = copy_from_sockptr_offset((char *)crypto_info +\n\t\t\t\tsizeof(*crypto_info),\n\t\t\t\toptval, sizeof(*crypto_info),\n\t\t\t\tsizeof(struct tls12_crypto_info_aes_gcm_256)\n\t\t\t\t- sizeof(*crypto_info));\n\n\t\tif (rc) {\n\t\t\trc = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tkeylen = TLS_CIPHER_AES_GCM_256_KEY_SIZE;\n\t\tcipher_type = TLS_CIPHER_AES_GCM_256;\n\t\tbreak;\n\t}\n\tdefault:\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\trc = chtls_setkey(csk, keylen, optname, cipher_type);\nout:\n\treturn rc;\n}\n\nstatic int chtls_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    sockptr_t optval, unsigned int optlen)\n{\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\n\tif (level != SOL_TLS)\n\t\treturn ctx->sk_proto->setsockopt(sk, level,\n\t\t\t\t\t\t optname, optval, optlen);\n\n\treturn do_chtls_setsockopt(sk, optname, optval, optlen);\n}\n\nstatic struct cxgb4_uld_info chtls_uld_info = {\n\t.name = DRV_NAME,\n\t.nrxq = MAX_ULD_QSETS,\n\t.ntxq = MAX_ULD_QSETS,\n\t.rxq_size = 1024,\n\t.add = chtls_uld_add,\n\t.state_change = chtls_uld_state_change,\n\t.rx_handler = chtls_uld_rx_handler,\n};\n\nvoid chtls_install_cpl_ops(struct sock *sk)\n{\n\tif (sk->sk_family == AF_INET)\n\t\tsk->sk_prot = &chtls_cpl_prot;\n\telse\n\t\tsk->sk_prot = &chtls_cpl_protv6;\n}\n\nstatic void __init chtls_init_ulp_ops(void)\n{\n\tchtls_cpl_prot\t\t\t= tcp_prot;\n\tchtls_init_rsk_ops(&chtls_cpl_prot, &chtls_rsk_ops,\n\t\t\t   &tcp_prot, PF_INET);\n\tchtls_cpl_prot.close\t\t= chtls_close;\n\tchtls_cpl_prot.disconnect\t= chtls_disconnect;\n\tchtls_cpl_prot.destroy\t\t= chtls_destroy_sock;\n\tchtls_cpl_prot.shutdown\t\t= chtls_shutdown;\n\tchtls_cpl_prot.sendmsg\t\t= chtls_sendmsg;\n\tchtls_cpl_prot.splice_eof\t= chtls_splice_eof;\n\tchtls_cpl_prot.recvmsg\t\t= chtls_recvmsg;\n\tchtls_cpl_prot.setsockopt\t= chtls_setsockopt;\n\tchtls_cpl_prot.getsockopt\t= chtls_getsockopt;\n#if IS_ENABLED(CONFIG_IPV6)\n\tchtls_cpl_protv6\t\t= chtls_cpl_prot;\n\tchtls_init_rsk_ops(&chtls_cpl_protv6, &chtls_rsk_opsv6,\n\t\t\t   &tcpv6_prot, PF_INET6);\n#endif\n}\n\nstatic int __init chtls_register(void)\n{\n\tchtls_init_ulp_ops();\n\tregister_listen_notifier(&listen_notifier);\n\tcxgb4_register_uld(CXGB4_ULD_TLS, &chtls_uld_info);\n\treturn 0;\n}\n\nstatic void __exit chtls_unregister(void)\n{\n\tunregister_listen_notifier(&listen_notifier);\n\tchtls_free_all_uld();\n\tcxgb4_unregister_uld(CXGB4_ULD_TLS);\n}\n\nmodule_init(chtls_register);\nmodule_exit(chtls_unregister);\n\nMODULE_DESCRIPTION(\"Chelsio TLS Inline driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Chelsio Communications\");\nMODULE_VERSION(CHTLS_DRV_VERSION);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}