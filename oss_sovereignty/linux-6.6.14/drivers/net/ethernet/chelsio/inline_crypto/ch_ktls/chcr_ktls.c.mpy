{
  "module_name": "chcr_ktls.c",
  "hash_id": "291e804e05df378d74445cb47ef8eaad1b8d4207305e6798a878aac80f7b9363",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/inline_crypto/ch_ktls/chcr_ktls.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/skbuff.h>\n#include <linux/module.h>\n#include <linux/highmem.h>\n#include <linux/ip.h>\n#include <net/ipv6.h>\n#include <linux/netdevice.h>\n#include <crypto/aes.h>\n#include \"chcr_ktls.h\"\n\nstatic LIST_HEAD(uld_ctx_list);\nstatic DEFINE_MUTEX(dev_mutex);\n\n \nstatic int chcr_get_nfrags_to_send(struct sk_buff *skb, u32 start, u32 len)\n{\n\tstruct skb_shared_info *si = skb_shinfo(skb);\n\tu32 frag_size, skb_linear_data_len = skb_headlen(skb);\n\tu8 nfrags = 0, frag_idx = 0;\n\tskb_frag_t *frag;\n\n\t \n\tif (!skb_is_nonlinear(skb))\n\t\treturn 1;\n\n\tif (unlikely(start < skb_linear_data_len)) {\n\t\tfrag_size = min(len, skb_linear_data_len - start);\n\t} else {\n\t\tstart -= skb_linear_data_len;\n\n\t\tfrag = &si->frags[frag_idx];\n\t\tfrag_size = skb_frag_size(frag);\n\t\twhile (start >= frag_size) {\n\t\t\tstart -= frag_size;\n\t\t\tfrag_idx++;\n\t\t\tfrag = &si->frags[frag_idx];\n\t\t\tfrag_size = skb_frag_size(frag);\n\t\t}\n\t\tfrag_size = min(len, skb_frag_size(frag) - start);\n\t}\n\tlen -= frag_size;\n\tnfrags++;\n\n\twhile (len) {\n\t\tfrag_size = min(len, skb_frag_size(&si->frags[frag_idx]));\n\t\tlen -= frag_size;\n\t\tnfrags++;\n\t\tfrag_idx++;\n\t}\n\treturn nfrags;\n}\n\nstatic int chcr_init_tcb_fields(struct chcr_ktls_info *tx_info);\nstatic void clear_conn_resources(struct chcr_ktls_info *tx_info);\n \nstatic int chcr_ktls_save_keys(struct chcr_ktls_info *tx_info,\n\t\t\t       struct tls_crypto_info *crypto_info,\n\t\t\t       enum tls_offload_ctx_dir direction)\n{\n\tint ck_size, key_ctx_size, mac_key_size, keylen, ghash_size, ret;\n\tunsigned char ghash_h[TLS_CIPHER_AES_GCM_256_TAG_SIZE];\n\tstruct tls12_crypto_info_aes_gcm_128 *info_128_gcm;\n\tstruct ktls_key_ctx *kctx = &tx_info->key_ctx;\n\tstruct crypto_aes_ctx aes_ctx;\n\tunsigned char *key, *salt;\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128:\n\t\tinfo_128_gcm =\n\t\t\t(struct tls12_crypto_info_aes_gcm_128 *)crypto_info;\n\t\tkeylen = TLS_CIPHER_AES_GCM_128_KEY_SIZE;\n\t\tck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\n\t\ttx_info->salt_size = TLS_CIPHER_AES_GCM_128_SALT_SIZE;\n\t\tmac_key_size = CHCR_KEYCTX_MAC_KEY_SIZE_128;\n\t\ttx_info->iv_size = TLS_CIPHER_AES_GCM_128_IV_SIZE;\n\t\ttx_info->iv = be64_to_cpu(*(__be64 *)info_128_gcm->iv);\n\n\t\tghash_size = TLS_CIPHER_AES_GCM_128_TAG_SIZE;\n\t\tkey = info_128_gcm->key;\n\t\tsalt = info_128_gcm->salt;\n\t\ttx_info->record_no = *(u64 *)info_128_gcm->rec_seq;\n\n\t\t \n\t\ttx_info->scmd0_seqno_numivs =\n\t\t\tSCMD_SEQ_NO_CTRL_V(CHCR_SCMD_SEQ_NO_CTRL_64BIT) |\n\t\t\tSCMD_CIPH_AUTH_SEQ_CTRL_F |\n\t\t\tSCMD_PROTO_VERSION_V(CHCR_SCMD_PROTO_VERSION_TLS) |\n\t\t\tSCMD_CIPH_MODE_V(CHCR_SCMD_CIPHER_MODE_AES_GCM) |\n\t\t\tSCMD_AUTH_MODE_V(CHCR_SCMD_AUTH_MODE_GHASH) |\n\t\t\tSCMD_IV_SIZE_V(TLS_CIPHER_AES_GCM_128_IV_SIZE >> 1) |\n\t\t\tSCMD_NUM_IVS_V(1);\n\n\t\t \n\t\ttx_info->scmd0_ivgen_hdrlen = SCMD_KEY_CTX_INLINE_F;\n\n\t\t \n\t\ttx_info->scmd0_short_seqno_numivs =\n\t\t\tSCMD_CIPH_AUTH_SEQ_CTRL_F |\n\t\t\tSCMD_PROTO_VERSION_V(CHCR_SCMD_PROTO_VERSION_GENERIC) |\n\t\t\tSCMD_CIPH_MODE_V(CHCR_SCMD_CIPHER_MODE_AES_CTR) |\n\t\t\tSCMD_IV_SIZE_V(AES_BLOCK_LEN >> 1);\n\n\t\ttx_info->scmd0_short_ivgen_hdrlen =\n\t\t\ttx_info->scmd0_ivgen_hdrlen | SCMD_AADIVDROP_F;\n\n\t\tbreak;\n\n\tdefault:\n\t\tpr_err(\"GCM: cipher type 0x%x not supported\\n\",\n\t\t       crypto_info->cipher_type);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tkey_ctx_size = CHCR_KTLS_KEY_CTX_LEN +\n\t\t       roundup(keylen, 16) + ghash_size;\n\t \n\n\tret = aes_expandkey(&aes_ctx, key, keylen);\n\tif (ret)\n\t\tgoto out;\n\n\tmemset(ghash_h, 0, ghash_size);\n\taes_encrypt(&aes_ctx, ghash_h, ghash_h);\n\tmemzero_explicit(&aes_ctx, sizeof(aes_ctx));\n\n\t \n\tif (direction == TLS_OFFLOAD_CTX_DIR_TX) {\n\t\tkctx->ctx_hdr = FILL_KEY_CTX_HDR(ck_size,\n\t\t\t\t\t\t mac_key_size,\n\t\t\t\t\t\t key_ctx_size >> 4);\n\t} else {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tmemcpy(kctx->salt, salt, tx_info->salt_size);\n\tmemcpy(kctx->key, key, keylen);\n\tmemcpy(kctx->key + keylen, ghash_h, ghash_size);\n\ttx_info->key_ctx_len = key_ctx_size;\n\nout:\n\treturn ret;\n}\n\n \nstatic int chcr_ktls_act_open_req(struct sock *sk,\n\t\t\t\t  struct chcr_ktls_info *tx_info,\n\t\t\t\t  int atid)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct cpl_t6_act_open_req *cpl6;\n\tstruct cpl_act_open_req *cpl;\n\tstruct sk_buff *skb;\n\tunsigned int len;\n\tint qid_atid;\n\tu64 options;\n\n\tlen = sizeof(*cpl6);\n\tskb = alloc_skb(len, GFP_KERNEL);\n\tif (unlikely(!skb))\n\t\treturn -ENOMEM;\n\t \n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, tx_info->port_id);\n\n\tcpl6 = __skb_put_zero(skb, len);\n\tcpl = (struct cpl_act_open_req *)cpl6;\n\tINIT_TP_WR(cpl6, 0);\n\tqid_atid = TID_QID_V(tx_info->rx_qid) |\n\t\t   TID_TID_V(atid);\n\tOPCODE_TID(cpl) = htonl(MK_OPCODE_TID(CPL_ACT_OPEN_REQ, qid_atid));\n\tcpl->local_port = inet->inet_sport;\n\tcpl->peer_port = inet->inet_dport;\n\tcpl->local_ip = inet->inet_rcv_saddr;\n\tcpl->peer_ip = inet->inet_daddr;\n\n\t \n\toptions = TCAM_BYPASS_F | ULP_MODE_V(ULP_MODE_NONE) | NON_OFFLOAD_F |\n\t\t  SMAC_SEL_V(tx_info->smt_idx) | TX_CHAN_V(tx_info->tx_chan);\n\tcpl->opt0 = cpu_to_be64(options);\n\n\t \n\toptions =\n\t\tTX_QUEUE_V(tx_info->adap->params.tp.tx_modq[tx_info->tx_chan]);\n\tcpl->opt2 = htonl(options);\n\n\treturn cxgb4_l2t_send(tx_info->netdev, skb, tx_info->l2te);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n \nstatic int chcr_ktls_act_open_req6(struct sock *sk,\n\t\t\t\t   struct chcr_ktls_info *tx_info,\n\t\t\t\t   int atid)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct cpl_t6_act_open_req6 *cpl6;\n\tstruct cpl_act_open_req6 *cpl;\n\tstruct sk_buff *skb;\n\tunsigned int len;\n\tint qid_atid;\n\tu64 options;\n\n\tlen = sizeof(*cpl6);\n\tskb = alloc_skb(len, GFP_KERNEL);\n\tif (unlikely(!skb))\n\t\treturn -ENOMEM;\n\t \n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, tx_info->port_id);\n\n\tcpl6 = __skb_put_zero(skb, len);\n\tcpl = (struct cpl_act_open_req6 *)cpl6;\n\tINIT_TP_WR(cpl6, 0);\n\tqid_atid = TID_QID_V(tx_info->rx_qid) | TID_TID_V(atid);\n\tOPCODE_TID(cpl) = htonl(MK_OPCODE_TID(CPL_ACT_OPEN_REQ6, qid_atid));\n\tcpl->local_port = inet->inet_sport;\n\tcpl->peer_port = inet->inet_dport;\n\tcpl->local_ip_hi = *(__be64 *)&sk->sk_v6_rcv_saddr.in6_u.u6_addr8[0];\n\tcpl->local_ip_lo = *(__be64 *)&sk->sk_v6_rcv_saddr.in6_u.u6_addr8[8];\n\tcpl->peer_ip_hi = *(__be64 *)&sk->sk_v6_daddr.in6_u.u6_addr8[0];\n\tcpl->peer_ip_lo = *(__be64 *)&sk->sk_v6_daddr.in6_u.u6_addr8[8];\n\n\t \n\toptions = TCAM_BYPASS_F | ULP_MODE_V(ULP_MODE_NONE) | NON_OFFLOAD_F |\n\t\t  SMAC_SEL_V(tx_info->smt_idx) | TX_CHAN_V(tx_info->tx_chan);\n\tcpl->opt0 = cpu_to_be64(options);\n\t \n\toptions =\n\t\tTX_QUEUE_V(tx_info->adap->params.tp.tx_modq[tx_info->tx_chan]);\n\tcpl->opt2 = htonl(options);\n\n\treturn cxgb4_l2t_send(tx_info->netdev, skb, tx_info->l2te);\n}\n#endif  \n\n \nstatic int chcr_setup_connection(struct sock *sk,\n\t\t\t\t struct chcr_ktls_info *tx_info)\n{\n\tstruct tid_info *t = &tx_info->adap->tids;\n\tint atid, ret = 0;\n\n\tatid = cxgb4_alloc_atid(t, tx_info);\n\tif (atid == -1)\n\t\treturn -EINVAL;\n\n\ttx_info->atid = atid;\n\n\tif (tx_info->ip_family == AF_INET) {\n\t\tret = chcr_ktls_act_open_req(sk, tx_info, atid);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else {\n\t\tret = cxgb4_clip_get(tx_info->netdev, (const u32 *)\n\t\t\t\t     &sk->sk_v6_rcv_saddr,\n\t\t\t\t     1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = chcr_ktls_act_open_req6(sk, tx_info, atid);\n#endif\n\t}\n\n\t \n\tif (ret) {\n\t\tif (ret == NET_XMIT_CN) {\n\t\t\tret = 0;\n\t\t} else {\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\t \n\t\t\tif (tx_info->ip_family == AF_INET6)\n\t\t\t\tcxgb4_clip_release(tx_info->netdev,\n\t\t\t\t\t\t   (const u32 *)\n\t\t\t\t\t\t   &sk->sk_v6_rcv_saddr,\n\t\t\t\t\t\t   1);\n#endif\n\t\t\tcxgb4_free_atid(t, atid);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic int chcr_set_tcb_field(struct chcr_ktls_info *tx_info, u16 word,\n\t\t\t      u64 mask, u64 val, int no_reply)\n{\n\tstruct cpl_set_tcb_field *req;\n\tstruct sk_buff *skb;\n\n\tskb = alloc_skb(sizeof(struct cpl_set_tcb_field), GFP_ATOMIC);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\treq = (struct cpl_set_tcb_field *)__skb_put_zero(skb, sizeof(*req));\n\tINIT_TP_WR_CPL(req, CPL_SET_TCB_FIELD, tx_info->tid);\n\treq->reply_ctrl = htons(QUEUENO_V(tx_info->rx_qid) |\n\t\t\t\tNO_REPLY_V(no_reply));\n\treq->word_cookie = htons(TCB_WORD_V(word));\n\treq->mask = cpu_to_be64(mask);\n\treq->val = cpu_to_be64(val);\n\n\tset_wr_txq(skb, CPL_PRIORITY_CONTROL, tx_info->port_id);\n\treturn cxgb4_ofld_send(tx_info->netdev, skb);\n}\n\n \nstatic void chcr_ktls_dev_del(struct net_device *netdev,\n\t\t\t      struct tls_context *tls_ctx,\n\t\t\t      enum tls_offload_ctx_dir direction)\n{\n\tstruct chcr_ktls_ofld_ctx_tx *tx_ctx =\n\t\t\t\tchcr_get_ktls_tx_context(tls_ctx);\n\tstruct chcr_ktls_info *tx_info = tx_ctx->chcr_info;\n\tstruct ch_ktls_port_stats_debug *port_stats;\n\tstruct chcr_ktls_uld_ctx *u_ctx;\n\n\tif (!tx_info)\n\t\treturn;\n\n\tu_ctx = tx_info->adap->uld[CXGB4_ULD_KTLS].handle;\n\tif (u_ctx && u_ctx->detach)\n\t\treturn;\n\t \n\tif (tx_info->l2te)\n\t\tcxgb4_l2t_release(tx_info->l2te);\n\n#if IS_ENABLED(CONFIG_IPV6)\n\t \n\tif (tx_info->ip_family == AF_INET6)\n\t\tcxgb4_clip_release(netdev, (const u32 *)\n\t\t\t\t   &tx_info->sk->sk_v6_rcv_saddr,\n\t\t\t\t   1);\n#endif\n\n\t \n\tif (tx_info->tid != -1) {\n\t\tcxgb4_remove_tid(&tx_info->adap->tids, tx_info->tx_chan,\n\t\t\t\t tx_info->tid, tx_info->ip_family);\n\n\t\txa_erase(&u_ctx->tid_list, tx_info->tid);\n\t}\n\n\tport_stats = &tx_info->adap->ch_ktls_stats.ktls_port[tx_info->port_id];\n\tatomic64_inc(&port_stats->ktls_tx_connection_close);\n\tkvfree(tx_info);\n\ttx_ctx->chcr_info = NULL;\n\t \n\tmodule_put(THIS_MODULE);\n}\n\n \nstatic int chcr_ktls_dev_add(struct net_device *netdev, struct sock *sk,\n\t\t\t     enum tls_offload_ctx_dir direction,\n\t\t\t     struct tls_crypto_info *crypto_info,\n\t\t\t     u32 start_offload_tcp_sn)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct ch_ktls_port_stats_debug *port_stats;\n\tstruct chcr_ktls_ofld_ctx_tx *tx_ctx;\n\tstruct chcr_ktls_uld_ctx *u_ctx;\n\tstruct chcr_ktls_info *tx_info;\n\tstruct dst_entry *dst;\n\tstruct adapter *adap;\n\tstruct port_info *pi;\n\tstruct neighbour *n;\n\tu8 daaddr[16];\n\tint ret = -1;\n\n\ttx_ctx = chcr_get_ktls_tx_context(tls_ctx);\n\n\tpi = netdev_priv(netdev);\n\tadap = pi->adapter;\n\tport_stats = &adap->ch_ktls_stats.ktls_port[pi->port_id];\n\tatomic64_inc(&port_stats->ktls_tx_connection_open);\n\tu_ctx = adap->uld[CXGB4_ULD_KTLS].handle;\n\n\tif (direction == TLS_OFFLOAD_CTX_DIR_RX) {\n\t\tpr_err(\"not expecting for RX direction\\n\");\n\t\tgoto out;\n\t}\n\n\tif (tx_ctx->chcr_info)\n\t\tgoto out;\n\n\tif (u_ctx && u_ctx->detach)\n\t\tgoto out;\n\n\ttx_info = kvzalloc(sizeof(*tx_info), GFP_KERNEL);\n\tif (!tx_info)\n\t\tgoto out;\n\n\ttx_info->sk = sk;\n\tspin_lock_init(&tx_info->lock);\n\t \n\ttx_info->tid = -1;\n\ttx_info->atid = -1;\n\n\ttx_info->adap = adap;\n\ttx_info->netdev = netdev;\n\ttx_info->first_qset = pi->first_qset;\n\ttx_info->tx_chan = pi->tx_chan;\n\ttx_info->smt_idx = pi->smt_idx;\n\ttx_info->port_id = pi->port_id;\n\ttx_info->prev_ack = 0;\n\ttx_info->prev_win = 0;\n\n\ttx_info->rx_qid = chcr_get_first_rx_qid(adap);\n\tif (unlikely(tx_info->rx_qid < 0))\n\t\tgoto free_tx_info;\n\n\ttx_info->prev_seq = start_offload_tcp_sn;\n\ttx_info->tcp_start_seq_number = start_offload_tcp_sn;\n\n\t \n\tret = chcr_ktls_save_keys(tx_info, crypto_info, direction);\n\tif (ret < 0)\n\t\tgoto free_tx_info;\n\n\t \n\tif (sk->sk_family == AF_INET) {\n\t\tmemcpy(daaddr, &sk->sk_daddr, 4);\n\t\ttx_info->ip_family = AF_INET;\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else {\n\t\tif (!ipv6_only_sock(sk) &&\n\t\t    ipv6_addr_type(&sk->sk_v6_daddr) == IPV6_ADDR_MAPPED) {\n\t\t\tmemcpy(daaddr, &sk->sk_daddr, 4);\n\t\t\ttx_info->ip_family = AF_INET;\n\t\t} else {\n\t\t\tmemcpy(daaddr, sk->sk_v6_daddr.in6_u.u6_addr8, 16);\n\t\t\ttx_info->ip_family = AF_INET6;\n\t\t}\n#endif\n\t}\n\n\t \n\tdst = sk_dst_get(sk);\n\tif (!dst) {\n\t\tpr_err(\"DST entry not found\\n\");\n\t\tgoto free_tx_info;\n\t}\n\tn = dst_neigh_lookup(dst, daaddr);\n\tif (!n || !n->dev) {\n\t\tpr_err(\"neighbour not found\\n\");\n\t\tdst_release(dst);\n\t\tgoto free_tx_info;\n\t}\n\ttx_info->l2te  = cxgb4_l2t_get(adap->l2t, n, n->dev, 0);\n\n\tneigh_release(n);\n\tdst_release(dst);\n\n\tif (!tx_info->l2te) {\n\t\tpr_err(\"l2t entry not found\\n\");\n\t\tgoto free_tx_info;\n\t}\n\n\t \n\tif (!try_module_get(THIS_MODULE))\n\t\tgoto free_l2t;\n\n\tinit_completion(&tx_info->completion);\n\t \n\ttx_info->open_state = CH_KTLS_OPEN_PENDING;\n\n\tif (chcr_setup_connection(sk, tx_info))\n\t\tgoto put_module;\n\n\t \n\twait_for_completion_timeout(&tx_info->completion, 30 * HZ);\n\tspin_lock_bh(&tx_info->lock);\n\tif (tx_info->open_state) {\n\t\t \n\t\tif (tx_info->open_state == CH_KTLS_OPEN_PENDING)\n\t\t\ttx_info->pending_close = true;\n\t\telse\n\t\t\tspin_unlock_bh(&tx_info->lock);\n\t\t \n\t\tgoto put_module;\n\t}\n\tspin_unlock_bh(&tx_info->lock);\n\n\t \n\treinit_completion(&tx_info->completion);\n\t \n\ttx_info->open_state = CH_KTLS_OPEN_PENDING;\n\n\tif (chcr_init_tcb_fields(tx_info))\n\t\tgoto free_tid;\n\n\t \n\twait_for_completion_timeout(&tx_info->completion, 30 * HZ);\n\tspin_lock_bh(&tx_info->lock);\n\tif (tx_info->open_state) {\n\t\t \n\t\ttx_info->pending_close = true;\n\t\t \n\t\tgoto free_tid;\n\t}\n\tspin_unlock_bh(&tx_info->lock);\n\n\tif (!cxgb4_check_l2t_valid(tx_info->l2te))\n\t\tgoto free_tid;\n\n\tatomic64_inc(&port_stats->ktls_tx_ctx);\n\ttx_ctx->chcr_info = tx_info;\n\n\treturn 0;\n\nfree_tid:\n#if IS_ENABLED(CONFIG_IPV6)\n\t \n\tif (tx_info->ip_family == AF_INET6)\n\t\tcxgb4_clip_release(netdev, (const u32 *)\n\t\t\t\t   &sk->sk_v6_rcv_saddr,\n\t\t\t\t   1);\n#endif\n\tcxgb4_remove_tid(&tx_info->adap->tids, tx_info->tx_chan,\n\t\t\t tx_info->tid, tx_info->ip_family);\n\n\txa_erase(&u_ctx->tid_list, tx_info->tid);\n\nput_module:\n\t \n\tmodule_put(THIS_MODULE);\nfree_l2t:\n\tcxgb4_l2t_release(tx_info->l2te);\nfree_tx_info:\n\tif (tx_info->pending_close)\n\t\tspin_unlock_bh(&tx_info->lock);\n\telse\n\t\tkvfree(tx_info);\nout:\n\tatomic64_inc(&port_stats->ktls_tx_connection_fail);\n\treturn -1;\n}\n\n \nstatic int chcr_init_tcb_fields(struct chcr_ktls_info *tx_info)\n{\n\tint  ret = 0;\n\n\t \n\tret =\n\tchcr_set_tcb_field(tx_info, TCB_T_FLAGS_W,\n\t\t\t   TCB_T_FLAGS_V(TF_CORE_BYPASS_F | TF_NON_OFFLOAD_F),\n\t\t\t   TCB_T_FLAGS_V(TF_CORE_BYPASS_F), 1);\n\tif (ret)\n\t\treturn ret;\n\t \n\tret = chcr_set_tcb_field(tx_info, TCB_SND_UNA_RAW_W,\n\t\t\t\t TCB_SND_NXT_RAW_V(TCB_SND_NXT_RAW_M) |\n\t\t\t\t TCB_SND_UNA_RAW_V(TCB_SND_UNA_RAW_M),\n\t\t\t\t 0, 1);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = chcr_set_tcb_field(tx_info, TCB_SND_MAX_RAW_W,\n\t\t\t\t TCB_SND_MAX_RAW_V(TCB_SND_MAX_RAW_M),\n\t\t\t\t 0, 1);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = chcr_set_tcb_field(tx_info, TCB_L2T_IX_W,\n\t\t\t\t TCB_L2T_IX_V(TCB_L2T_IX_M),\n\t\t\t\t TCB_L2T_IX_V(tx_info->l2te->idx), 0);\n\treturn ret;\n}\n\n \nstatic int chcr_ktls_cpl_act_open_rpl(struct adapter *adap,\n\t\t\t\t      unsigned char *input)\n{\n\tconst struct cpl_act_open_rpl *p = (void *)input;\n\tstruct chcr_ktls_info *tx_info = NULL;\n\tstruct chcr_ktls_ofld_ctx_tx *tx_ctx;\n\tstruct chcr_ktls_uld_ctx *u_ctx;\n\tunsigned int atid, tid, status;\n\tstruct tls_context *tls_ctx;\n\tstruct tid_info *t;\n\tint ret = 0;\n\n\ttid = GET_TID(p);\n\tstatus = AOPEN_STATUS_G(ntohl(p->atid_status));\n\tatid = TID_TID_G(AOPEN_ATID_G(ntohl(p->atid_status)));\n\n\tt = &adap->tids;\n\ttx_info = lookup_atid(t, atid);\n\n\tif (!tx_info || tx_info->atid != atid) {\n\t\tpr_err(\"%s: incorrect tx_info or atid\\n\", __func__);\n\t\treturn -1;\n\t}\n\n\tcxgb4_free_atid(t, atid);\n\ttx_info->atid = -1;\n\n\tspin_lock(&tx_info->lock);\n\t \n\tif (tx_info->pending_close) {\n\t\tspin_unlock(&tx_info->lock);\n\t\tif (!status) {\n\t\t\tcxgb4_remove_tid(&tx_info->adap->tids, tx_info->tx_chan,\n\t\t\t\t\t tid, tx_info->ip_family);\n\t\t}\n\t\tkvfree(tx_info);\n\t\treturn 0;\n\t}\n\n\tif (!status) {\n\t\ttx_info->tid = tid;\n\t\tcxgb4_insert_tid(t, tx_info, tx_info->tid, tx_info->ip_family);\n\t\t \n\t\ttls_ctx = tls_get_ctx(tx_info->sk);\n\t\ttx_ctx = chcr_get_ktls_tx_context(tls_ctx);\n\t\tu_ctx = adap->uld[CXGB4_ULD_KTLS].handle;\n\t\tif (u_ctx) {\n\t\t\tret = xa_insert_bh(&u_ctx->tid_list, tid, tx_ctx,\n\t\t\t\t\t   GFP_NOWAIT);\n\t\t\tif (ret < 0) {\n\t\t\t\tpr_err(\"%s: Failed to allocate tid XA entry = %d\\n\",\n\t\t\t\t       __func__, tx_info->tid);\n\t\t\t\ttx_info->open_state = CH_KTLS_OPEN_FAILURE;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\ttx_info->open_state = CH_KTLS_OPEN_SUCCESS;\n\t} else {\n\t\ttx_info->open_state = CH_KTLS_OPEN_FAILURE;\n\t}\nout:\n\tspin_unlock(&tx_info->lock);\n\n\tcomplete(&tx_info->completion);\n\treturn ret;\n}\n\n \nstatic int chcr_ktls_cpl_set_tcb_rpl(struct adapter *adap, unsigned char *input)\n{\n\tconst struct cpl_set_tcb_rpl *p = (void *)input;\n\tstruct chcr_ktls_info *tx_info = NULL;\n\tstruct tid_info *t;\n\tu32 tid;\n\n\ttid = GET_TID(p);\n\n\tt = &adap->tids;\n\ttx_info = lookup_tid(t, tid);\n\n\tif (!tx_info || tx_info->tid != tid) {\n\t\tpr_err(\"%s: incorrect tx_info or tid\\n\", __func__);\n\t\treturn -1;\n\t}\n\n\tspin_lock(&tx_info->lock);\n\tif (tx_info->pending_close) {\n\t\tspin_unlock(&tx_info->lock);\n\t\tkvfree(tx_info);\n\t\treturn 0;\n\t}\n\ttx_info->open_state = CH_KTLS_OPEN_SUCCESS;\n\tspin_unlock(&tx_info->lock);\n\n\tcomplete(&tx_info->completion);\n\treturn 0;\n}\n\nstatic void *__chcr_write_cpl_set_tcb_ulp(struct chcr_ktls_info *tx_info,\n\t\t\t\t\tu32 tid, void *pos, u16 word,\n\t\t\t\t\tstruct sge_eth_txq *q, u64 mask,\n\t\t\t\t\tu64 val, u32 reply)\n{\n\tstruct cpl_set_tcb_field_core *cpl;\n\tstruct ulptx_idata *idata;\n\tstruct ulp_txpkt *txpkt;\n\n\t \n\ttxpkt = pos;\n\ttxpkt->cmd_dest = htonl(ULPTX_CMD_V(ULP_TX_PKT) |\n\t\t\t\tULP_TXPKT_CHANNELID_V(tx_info->port_id) |\n\t\t\t\tULP_TXPKT_FID_V(q->q.cntxt_id) |\n\t\t\t\tULP_TXPKT_RO_F);\n\ttxpkt->len = htonl(DIV_ROUND_UP(CHCR_SET_TCB_FIELD_LEN, 16));\n\n\t \n\tidata = (struct ulptx_idata *)(txpkt + 1);\n\tidata->cmd_more = htonl(ULPTX_CMD_V(ULP_TX_SC_IMM));\n\tidata->len = htonl(sizeof(*cpl));\n\tpos = idata + 1;\n\n\tcpl = pos;\n\t \n\tOPCODE_TID(cpl) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, tid));\n\tcpl->reply_ctrl = htons(QUEUENO_V(tx_info->rx_qid) |\n\t\t\tNO_REPLY_V(!reply));\n\tcpl->word_cookie = htons(TCB_WORD_V(word));\n\tcpl->mask = cpu_to_be64(mask);\n\tcpl->val = cpu_to_be64(val);\n\n\t \n\tidata = (struct ulptx_idata *)(cpl + 1);\n\tidata->cmd_more = htonl(ULPTX_CMD_V(ULP_TX_SC_NOOP));\n\tidata->len = htonl(0);\n\tpos = idata + 1;\n\n\treturn pos;\n}\n\n\n \nstatic void *chcr_write_cpl_set_tcb_ulp(struct chcr_ktls_info *tx_info,\n\t\t\t\t\tstruct sge_eth_txq *q, u32 tid,\n\t\t\t\t\tvoid *pos, u16 word, u64 mask,\n\t\t\t\t\tu64 val, u32 reply)\n{\n\tint left = (void *)q->q.stat - pos;\n\n\tif (unlikely(left < CHCR_SET_TCB_FIELD_LEN)) {\n\t\tif (!left) {\n\t\t\tpos = q->q.desc;\n\t\t} else {\n\t\t\tu8 buf[48] = {0};\n\n\t\t\t__chcr_write_cpl_set_tcb_ulp(tx_info, tid, buf, word, q,\n\t\t\t\t\t\t     mask, val, reply);\n\n\t\t\treturn chcr_copy_to_txd(buf, &q->q, pos,\n\t\t\t\t\t\tCHCR_SET_TCB_FIELD_LEN);\n\t\t}\n\t}\n\n\tpos = __chcr_write_cpl_set_tcb_ulp(tx_info, tid, pos, word, q,\n\t\t\t\t\t   mask, val, reply);\n\n\t \n\tif (left == CHCR_SET_TCB_FIELD_LEN)\n\t\tpos = q->q.desc;\n\n\treturn pos;\n}\n\n \nstatic int chcr_ktls_xmit_tcb_cpls(struct chcr_ktls_info *tx_info,\n\t\t\t\t   struct sge_eth_txq *q, u64 tcp_seq,\n\t\t\t\t   u64 tcp_ack, u64 tcp_win, bool offset)\n{\n\tbool first_wr = ((tx_info->prev_ack == 0) && (tx_info->prev_win == 0));\n\tstruct ch_ktls_port_stats_debug *port_stats;\n\tu32 len, cpl = 0, ndesc, wr_len, wr_mid = 0;\n\tstruct fw_ulptx_wr *wr;\n\tint credits;\n\tvoid *pos;\n\n\twr_len = sizeof(*wr);\n\t \n\tlen = wr_len + 4 * roundup(CHCR_SET_TCB_FIELD_LEN, 16);\n\tndesc = DIV_ROUND_UP(len, 64);\n\n\tcredits = chcr_txq_avail(&q->q) - ndesc;\n\tif (unlikely(credits < 0)) {\n\t\tchcr_eth_txq_stop(q);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\tchcr_eth_txq_stop(q);\n\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\n\tpos = &q->q.desc[q->q.pidx];\n\t \n\twr = pos;\n\tpos += wr_len;\n\t \n\tif (first_wr || tcp_seq != tx_info->prev_seq) {\n\t\tpos = chcr_write_cpl_set_tcb_ulp(tx_info, q, tx_info->tid, pos,\n\t\t\t\t\t\t TCB_TX_MAX_W,\n\t\t\t\t\t\t TCB_TX_MAX_V(TCB_TX_MAX_M),\n\t\t\t\t\t\t TCB_TX_MAX_V(tcp_seq), 0);\n\t\tcpl++;\n\t}\n\t \n\tif (tcp_seq != tx_info->prev_seq || offset) {\n\t\t \n\t\tport_stats =\n\t\t\t&tx_info->adap->ch_ktls_stats.ktls_port[tx_info->port_id];\n\t\tpos = chcr_write_cpl_set_tcb_ulp(tx_info, q, tx_info->tid, pos,\n\t\t\t\t\t\t TCB_SND_UNA_RAW_W,\n\t\t\t\t\t\t TCB_SND_UNA_RAW_V\n\t\t\t\t\t\t (TCB_SND_UNA_RAW_M),\n\t\t\t\t\t\t TCB_SND_UNA_RAW_V(0), 0);\n\t\tif (tcp_seq != tx_info->prev_seq)\n\t\t\tatomic64_inc(&port_stats->ktls_tx_ooo);\n\t\tcpl++;\n\t}\n\t \n\tif (first_wr || tx_info->prev_ack != tcp_ack) {\n\t\tpos = chcr_write_cpl_set_tcb_ulp(tx_info, q, tx_info->tid, pos,\n\t\t\t\t\t\t TCB_RCV_NXT_W,\n\t\t\t\t\t\t TCB_RCV_NXT_V(TCB_RCV_NXT_M),\n\t\t\t\t\t\t TCB_RCV_NXT_V(tcp_ack), 0);\n\t\ttx_info->prev_ack = tcp_ack;\n\t\tcpl++;\n\t}\n\t \n\tif (first_wr || tx_info->prev_win != tcp_win) {\n\t\tchcr_write_cpl_set_tcb_ulp(tx_info, q, tx_info->tid, pos,\n\t\t\t\t\t   TCB_RCV_WND_W,\n\t\t\t\t\t   TCB_RCV_WND_V(TCB_RCV_WND_M),\n\t\t\t\t\t   TCB_RCV_WND_V(tcp_win), 0);\n\t\ttx_info->prev_win = tcp_win;\n\t\tcpl++;\n\t}\n\n\tif (cpl) {\n\t\t \n\t\tlen = wr_len + cpl * roundup(CHCR_SET_TCB_FIELD_LEN, 16);\n\t\t \n\t\twr->op_to_compl = htonl(FW_WR_OP_V(FW_ULPTX_WR));\n\t\twr->cookie = 0;\n\t\t \n\t\twr->flowid_len16 = htonl(wr_mid |\n\t\t\t\t\t FW_WR_LEN16_V(DIV_ROUND_UP(len, 16)));\n\n\t\tndesc = DIV_ROUND_UP(len, 64);\n\t\tchcr_txq_advance(&q->q, ndesc);\n\t\tcxgb4_ring_tx_db(tx_info->adap, &q->q, ndesc);\n\t}\n\treturn 0;\n}\n\n \nstatic unsigned int\nchcr_ktls_get_tx_flits(u32 nr_frags, unsigned int key_ctx_len)\n{\n\treturn chcr_sgl_len(nr_frags) +\n\t       DIV_ROUND_UP(key_ctx_len + CHCR_KTLS_WR_SIZE, 8);\n}\n\n \nstatic int\nchcr_ktls_check_tcp_options(struct tcphdr *tcp)\n{\n\tint cnt, opt, optlen;\n\tu_char *cp;\n\n\tcp = (u_char *)(tcp + 1);\n\tcnt = (tcp->doff << 2) - sizeof(struct tcphdr);\n\tfor (; cnt > 0; cnt -= optlen, cp += optlen) {\n\t\topt = cp[0];\n\t\tif (opt == TCPOPT_EOL)\n\t\t\tbreak;\n\t\tif (opt == TCPOPT_NOP) {\n\t\t\toptlen = 1;\n\t\t} else {\n\t\t\tif (cnt < 2)\n\t\t\t\tbreak;\n\t\t\toptlen = cp[1];\n\t\t\tif (optlen < 2 || optlen > cnt)\n\t\t\t\tbreak;\n\t\t}\n\t\tswitch (opt) {\n\t\tcase TCPOPT_NOP:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic int\nchcr_ktls_write_tcp_options(struct chcr_ktls_info *tx_info, struct sk_buff *skb,\n\t\t\t    struct sge_eth_txq *q, uint32_t tx_chan)\n{\n\tstruct fw_eth_tx_pkt_wr *wr;\n\tstruct cpl_tx_pkt_core *cpl;\n\tu32 ctrl, iplen, maclen;\n\tstruct ipv6hdr *ip6;\n\tunsigned int ndesc;\n\tstruct tcphdr *tcp;\n\tint len16, pktlen;\n\tstruct iphdr *ip;\n\tu32 wr_mid = 0;\n\tint credits;\n\tu8 buf[150];\n\tu64 cntrl1;\n\tvoid *pos;\n\n\tiplen = skb_network_header_len(skb);\n\tmaclen = skb_mac_header_len(skb);\n\n\t \n\tpktlen = skb_tcp_all_headers(skb);\n\n\tctrl = sizeof(*cpl) + pktlen;\n\tlen16 = DIV_ROUND_UP(sizeof(*wr) + ctrl, 16);\n\t \n\tndesc = DIV_ROUND_UP(len16, 4);\n\n\tcredits = chcr_txq_avail(&q->q) - ndesc;\n\tif (unlikely(credits < 0)) {\n\t\tchcr_eth_txq_stop(q);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\tchcr_eth_txq_stop(q);\n\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\n\tpos = &q->q.desc[q->q.pidx];\n\twr = pos;\n\n\t \n\twr->op_immdlen = htonl(FW_WR_OP_V(FW_ETH_TX_PKT_WR) |\n\t\t\t       FW_WR_IMMDLEN_V(ctrl));\n\n\twr->equiq_to_len16 = htonl(wr_mid | FW_WR_LEN16_V(len16));\n\twr->r3 = 0;\n\n\tcpl = (void *)(wr + 1);\n\n\t \n\tcpl->ctrl0 = htonl(TXPKT_OPCODE_V(CPL_TX_PKT) | TXPKT_INTF_V(tx_chan) |\n\t\t\t   TXPKT_PF_V(tx_info->adap->pf));\n\tcpl->pack = 0;\n\tcpl->len = htons(pktlen);\n\n\tmemcpy(buf, skb->data, pktlen);\n\tif (!IS_ENABLED(CONFIG_IPV6) || tx_info->ip_family == AF_INET) {\n\t\t \n\t\tip = (struct iphdr *)(buf + maclen);\n\t\tip->tot_len = htons(pktlen - maclen);\n\t\tcntrl1 = TXPKT_CSUM_TYPE_V(TX_CSUM_TCPIP);\n\t} else {\n\t\tip6 = (struct ipv6hdr *)(buf + maclen);\n\t\tip6->payload_len = htons(pktlen - maclen - iplen);\n\t\tcntrl1 = TXPKT_CSUM_TYPE_V(TX_CSUM_TCPIP6);\n\t}\n\n\tcntrl1 |= T6_TXPKT_ETHHDR_LEN_V(maclen - ETH_HLEN) |\n\t\t  TXPKT_IPHDR_LEN_V(iplen);\n\t \n\tcpl->ctrl1 = cpu_to_be64(cntrl1);\n\n\tpos = cpl + 1;\n\n\t \n\ttcp = (struct tcphdr *)(buf + maclen + iplen);\n\n\tif (!tcp->fin)\n\t\ttcp->psh = 0;\n\telse\n\t\ttcp->seq = htonl(tx_info->prev_seq);\n\n\tchcr_copy_to_txd(buf, &q->q, pos, pktlen);\n\n\tchcr_txq_advance(&q->q, ndesc);\n\tcxgb4_ring_tx_db(tx_info->adap, &q->q, ndesc);\n\treturn 0;\n}\n\n \nstatic int chcr_ktls_xmit_wr_complete(struct sk_buff *skb,\n\t\t\t\t      struct chcr_ktls_info *tx_info,\n\t\t\t\t      struct sge_eth_txq *q, u32 tcp_seq,\n\t\t\t\t      bool is_last_wr, u32 data_len,\n\t\t\t\t      u32 skb_offset, u32 nfrags,\n\t\t\t\t      bool tcp_push, u32 mss)\n{\n\tu32 len16, wr_mid = 0, flits = 0, ndesc, cipher_start;\n\tstruct adapter *adap = tx_info->adap;\n\tint credits, left, last_desc;\n\tstruct tx_sw_desc *sgl_sdesc;\n\tstruct cpl_tx_data *tx_data;\n\tstruct cpl_tx_sec_pdu *cpl;\n\tstruct ulptx_idata *idata;\n\tstruct ulp_txpkt *ulptx;\n\tstruct fw_ulptx_wr *wr;\n\tvoid *pos;\n\tu64 *end;\n\n\t \n\tflits = chcr_ktls_get_tx_flits(nfrags, tx_info->key_ctx_len);\n\t \n\tndesc = chcr_flits_to_desc(flits);\n\t \n\tcredits = chcr_txq_avail(&q->q) - ndesc;\n\tif (unlikely(credits < 0)) {\n\t\tchcr_eth_txq_stop(q);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\t \n\t\tchcr_eth_txq_stop(q);\n\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\n\tlast_desc = q->q.pidx + ndesc - 1;\n\tif (last_desc >= q->q.size)\n\t\tlast_desc -= q->q.size;\n\tsgl_sdesc = &q->q.sdesc[last_desc];\n\n\tif (unlikely(cxgb4_map_skb(adap->pdev_dev, skb, sgl_sdesc->addr) < 0)) {\n\t\tmemset(sgl_sdesc->addr, 0, sizeof(sgl_sdesc->addr));\n\t\tq->mapping_err++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (!is_last_wr)\n\t\tskb_get(skb);\n\n\tpos = &q->q.desc[q->q.pidx];\n\tend = (u64 *)pos + flits;\n\t \n\twr = pos;\n\t \n\tlen16 = DIV_ROUND_UP(flits, 2);\n\twr->op_to_compl = htonl(FW_WR_OP_V(FW_ULPTX_WR));\n\twr->flowid_len16 = htonl(wr_mid | FW_WR_LEN16_V(len16));\n\twr->cookie = 0;\n\tpos += sizeof(*wr);\n\t \n\tulptx = pos;\n\tulptx->cmd_dest = htonl(ULPTX_CMD_V(ULP_TX_PKT) |\n\t\t\t\tULP_TXPKT_CHANNELID_V(tx_info->port_id) |\n\t\t\t\tULP_TXPKT_FID_V(q->q.cntxt_id) |\n\t\t\t\tULP_TXPKT_RO_F);\n\tulptx->len = htonl(len16 - 1);\n\t \n\tidata = (struct ulptx_idata *)(ulptx + 1);\n\tidata->cmd_more = htonl(ULPTX_CMD_V(ULP_TX_SC_IMM) | ULP_TX_SC_MORE_F);\n\t \n\tidata->len = htonl(sizeof(*cpl) + tx_info->key_ctx_len +\n\t\t\t   sizeof(*tx_data));\n\t \n\tcpl = (struct cpl_tx_sec_pdu *)(idata + 1);\n\tcpl->op_ivinsrtofst =\n\t\thtonl(CPL_TX_SEC_PDU_OPCODE_V(CPL_TX_SEC_PDU) |\n\t\t      CPL_TX_SEC_PDU_CPLLEN_V(CHCR_CPL_TX_SEC_PDU_LEN_64BIT) |\n\t\t      CPL_TX_SEC_PDU_PLACEHOLDER_V(1) |\n\t\t      CPL_TX_SEC_PDU_IVINSRTOFST_V(TLS_HEADER_SIZE + 1));\n\tcpl->pldlen = htonl(data_len);\n\n\t \n\tcipher_start = TLS_HEADER_SIZE + tx_info->iv_size + 1;\n\n\tcpl->aadstart_cipherstop_hi =\n\t\thtonl(CPL_TX_SEC_PDU_AADSTART_V(1) |\n\t\t      CPL_TX_SEC_PDU_AADSTOP_V(TLS_HEADER_SIZE) |\n\t\t      CPL_TX_SEC_PDU_CIPHERSTART_V(cipher_start));\n\n\t \n\tcpl->cipherstop_lo_authinsert =\n\thtonl(CPL_TX_SEC_PDU_AUTHSTART_V(cipher_start) |\n\t      CPL_TX_SEC_PDU_AUTHSTOP_V(TLS_CIPHER_AES_GCM_128_TAG_SIZE) |\n\t      CPL_TX_SEC_PDU_AUTHINSERT_V(TLS_CIPHER_AES_GCM_128_TAG_SIZE));\n\n\t \n\tcpl->seqno_numivs = htonl(tx_info->scmd0_seqno_numivs);\n\tcpl->ivgen_hdrlen = htonl(tx_info->scmd0_ivgen_hdrlen);\n\tcpl->scmd1 = cpu_to_be64(tx_info->record_no);\n\n\tpos = cpl + 1;\n\t \n\tleft = (void *)q->q.stat - pos;\n\tif (!left) {\n\t\tleft = (void *)end - (void *)q->q.stat;\n\t\tpos = q->q.desc;\n\t\tend = pos + left;\n\t}\n\n\tpos = chcr_copy_to_txd(&tx_info->key_ctx, &q->q, pos,\n\t\t\t       tx_info->key_ctx_len);\n\tleft = (void *)q->q.stat - pos;\n\n\tif (!left) {\n\t\tleft = (void *)end - (void *)q->q.stat;\n\t\tpos = q->q.desc;\n\t\tend = pos + left;\n\t}\n\t \n\ttx_data = (void *)pos;\n\tOPCODE_TID(tx_data) = htonl(MK_OPCODE_TID(CPL_TX_DATA, tx_info->tid));\n\ttx_data->len = htonl(TX_DATA_MSS_V(mss) | TX_LENGTH_V(data_len));\n\n\ttx_data->rsvd = htonl(tcp_seq);\n\n\ttx_data->flags = htonl(TX_BYPASS_F);\n\tif (tcp_push)\n\t\ttx_data->flags |= htonl(TX_PUSH_F | TX_SHOVE_F);\n\n\t \n\tpos = tx_data + 1;\n\tleft = (void *)q->q.stat - pos;\n\n\t \n\tif (!left) {\n\t\tleft = (void *)end - (void *)q->q.stat;\n\t\tpos = q->q.desc;\n\t\tend = pos + left;\n\t}\n\n\t \n\tcxgb4_write_partial_sgl(skb, &q->q, pos, end, sgl_sdesc->addr,\n\t\t\t\tskb_offset, data_len);\n\tsgl_sdesc->skb = skb;\n\n\tchcr_txq_advance(&q->q, ndesc);\n\tcxgb4_ring_tx_db(adap, &q->q, ndesc);\n\tatomic64_inc(&adap->ch_ktls_stats.ktls_tx_send_records);\n\n\treturn 0;\n}\n\n \nstatic int chcr_ktls_xmit_wr_short(struct sk_buff *skb,\n\t\t\t\t   struct chcr_ktls_info *tx_info,\n\t\t\t\t   struct sge_eth_txq *q,\n\t\t\t\t   u32 tcp_seq, bool tcp_push, u32 mss,\n\t\t\t\t   u32 tls_rec_offset, u8 *prior_data,\n\t\t\t\t   u32 prior_data_len, u32 data_len,\n\t\t\t\t   u32 skb_offset)\n{\n\tu32 len16, wr_mid = 0, cipher_start, nfrags;\n\tstruct adapter *adap = tx_info->adap;\n\tunsigned int flits = 0, ndesc;\n\tint credits, left, last_desc;\n\tstruct tx_sw_desc *sgl_sdesc;\n\tstruct cpl_tx_data *tx_data;\n\tstruct cpl_tx_sec_pdu *cpl;\n\tstruct ulptx_idata *idata;\n\tstruct ulp_txpkt *ulptx;\n\tstruct fw_ulptx_wr *wr;\n\t__be64 iv_record;\n\tvoid *pos;\n\tu64 *end;\n\n\tnfrags = chcr_get_nfrags_to_send(skb, skb_offset, data_len);\n\t \n\tflits = chcr_ktls_get_tx_flits(nfrags, tx_info->key_ctx_len) + 2;\n\t \n\tiv_record = cpu_to_be64(tx_info->iv + tx_info->record_no);\n\t \n\tif (prior_data_len)\n\t\tflits += 2;\n\t \n\tndesc = chcr_flits_to_desc(flits);\n\t \n\tcredits = chcr_txq_avail(&q->q) - ndesc;\n\tif (unlikely(credits < 0)) {\n\t\tchcr_eth_txq_stop(q);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\tchcr_eth_txq_stop(q);\n\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\n\tlast_desc = q->q.pidx + ndesc - 1;\n\tif (last_desc >= q->q.size)\n\t\tlast_desc -= q->q.size;\n\tsgl_sdesc = &q->q.sdesc[last_desc];\n\n\tif (unlikely(cxgb4_map_skb(adap->pdev_dev, skb, sgl_sdesc->addr) < 0)) {\n\t\tmemset(sgl_sdesc->addr, 0, sizeof(sgl_sdesc->addr));\n\t\tq->mapping_err++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tpos = &q->q.desc[q->q.pidx];\n\tend = (u64 *)pos + flits;\n\t \n\twr = pos;\n\t \n\tlen16 = DIV_ROUND_UP(flits, 2);\n\twr->op_to_compl = htonl(FW_WR_OP_V(FW_ULPTX_WR));\n\twr->flowid_len16 = htonl(wr_mid | FW_WR_LEN16_V(len16));\n\twr->cookie = 0;\n\tpos += sizeof(*wr);\n\t \n\tulptx = pos;\n\tulptx->cmd_dest = htonl(ULPTX_CMD_V(ULP_TX_PKT) |\n\t\t\t\tULP_TXPKT_CHANNELID_V(tx_info->port_id) |\n\t\t\t\tULP_TXPKT_FID_V(q->q.cntxt_id) |\n\t\t\t\tULP_TXPKT_RO_F);\n\tulptx->len = htonl(len16 - 1);\n\t \n\tidata = (struct ulptx_idata *)(ulptx + 1);\n\tidata->cmd_more = htonl(ULPTX_CMD_V(ULP_TX_SC_IMM) | ULP_TX_SC_MORE_F);\n\t \n\tidata->len = htonl(sizeof(*cpl) + tx_info->key_ctx_len +\n\t\t\t   sizeof(*tx_data) + AES_BLOCK_LEN + prior_data_len);\n\t \n\tcpl = (struct cpl_tx_sec_pdu *)(idata + 1);\n\t \n\tcipher_start =\n\t\tAES_BLOCK_LEN + 1 +\n\t\t(!tls_rec_offset ? TLS_HEADER_SIZE + tx_info->iv_size : 0);\n\n\tcpl->op_ivinsrtofst =\n\t\thtonl(CPL_TX_SEC_PDU_OPCODE_V(CPL_TX_SEC_PDU) |\n\t\t      CPL_TX_SEC_PDU_CPLLEN_V(CHCR_CPL_TX_SEC_PDU_LEN_64BIT) |\n\t\t      CPL_TX_SEC_PDU_IVINSRTOFST_V(1));\n\tcpl->pldlen = htonl(data_len + AES_BLOCK_LEN + prior_data_len);\n\tcpl->aadstart_cipherstop_hi =\n\t\thtonl(CPL_TX_SEC_PDU_CIPHERSTART_V(cipher_start));\n\tcpl->cipherstop_lo_authinsert = 0;\n\t \n\tcpl->seqno_numivs = htonl(tx_info->scmd0_short_seqno_numivs);\n\tcpl->ivgen_hdrlen = htonl(tx_info->scmd0_short_ivgen_hdrlen);\n\tcpl->scmd1 = 0;\n\n\tpos = cpl + 1;\n\t \n\tleft = (void *)q->q.stat - pos;\n\tif (!left) {\n\t\tleft = (void *)end - (void *)q->q.stat;\n\t\tpos = q->q.desc;\n\t\tend = pos + left;\n\t}\n\n\tpos = chcr_copy_to_txd(&tx_info->key_ctx, &q->q, pos,\n\t\t\t       tx_info->key_ctx_len);\n\tleft = (void *)q->q.stat - pos;\n\n\tif (!left) {\n\t\tleft = (void *)end - (void *)q->q.stat;\n\t\tpos = q->q.desc;\n\t\tend = pos + left;\n\t}\n\t \n\ttx_data = (void *)pos;\n\tOPCODE_TID(tx_data) = htonl(MK_OPCODE_TID(CPL_TX_DATA, tx_info->tid));\n\ttx_data->len = htonl(TX_DATA_MSS_V(mss) |\n\t\t\t     TX_LENGTH_V(data_len + prior_data_len));\n\ttx_data->rsvd = htonl(tcp_seq);\n\ttx_data->flags = htonl(TX_BYPASS_F);\n\tif (tcp_push)\n\t\ttx_data->flags |= htonl(TX_PUSH_F | TX_SHOVE_F);\n\n\t \n\tpos = tx_data + 1;\n\tleft = (void *)q->q.stat - pos;\n\n\t \n\tif (!left) {\n\t\tleft = (void *)end - (void *)q->q.stat;\n\t\tpos = q->q.desc;\n\t\tend = pos + left;\n\t}\n\t \n\tmemcpy(pos, tx_info->key_ctx.salt, tx_info->salt_size);\n\tmemcpy(pos + tx_info->salt_size, &iv_record, tx_info->iv_size);\n\t*(__be32 *)(pos + tx_info->salt_size + tx_info->iv_size) =\n\t\thtonl(2 + (tls_rec_offset ? ((tls_rec_offset -\n\t\t(TLS_HEADER_SIZE + tx_info->iv_size)) / AES_BLOCK_LEN) : 0));\n\n\tpos += 16;\n\t \n\tif (prior_data_len)\n\t\tpos = chcr_copy_to_txd(prior_data, &q->q, pos, 16);\n\t \n\tcxgb4_write_partial_sgl(skb, &q->q, pos, end, sgl_sdesc->addr,\n\t\t\t\tskb_offset, data_len);\n\tsgl_sdesc->skb = skb;\n\n\tchcr_txq_advance(&q->q, ndesc);\n\tcxgb4_ring_tx_db(adap, &q->q, ndesc);\n\n\treturn 0;\n}\n\n \nstatic int chcr_ktls_tx_plaintxt(struct chcr_ktls_info *tx_info,\n\t\t\t\t struct sk_buff *skb, u32 tcp_seq, u32 mss,\n\t\t\t\t bool tcp_push, struct sge_eth_txq *q,\n\t\t\t\t u32 port_id, u8 *prior_data,\n\t\t\t\t u32 data_len, u32 skb_offset,\n\t\t\t\t u32 prior_data_len)\n{\n\tint credits, left, len16, last_desc;\n\tunsigned int flits = 0, ndesc;\n\tstruct tx_sw_desc *sgl_sdesc;\n\tstruct cpl_tx_data *tx_data;\n\tstruct ulptx_idata *idata;\n\tstruct ulp_txpkt *ulptx;\n\tstruct fw_ulptx_wr *wr;\n\tu32 wr_mid = 0, nfrags;\n\tvoid *pos;\n\tu64 *end;\n\n\tflits = DIV_ROUND_UP(CHCR_PLAIN_TX_DATA_LEN, 8);\n\tnfrags = chcr_get_nfrags_to_send(skb, skb_offset, data_len);\n\tflits += chcr_sgl_len(nfrags);\n\tif (prior_data_len)\n\t\tflits += 2;\n\n\t \n\tlen16 = DIV_ROUND_UP(flits, 2);\n\t \n\tndesc = DIV_ROUND_UP(flits, 8);\n\n\tcredits = chcr_txq_avail(&q->q) - ndesc;\n\tif (unlikely(credits < 0)) {\n\t\tchcr_eth_txq_stop(q);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\tchcr_eth_txq_stop(q);\n\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\n\tlast_desc = q->q.pidx + ndesc - 1;\n\tif (last_desc >= q->q.size)\n\t\tlast_desc -= q->q.size;\n\tsgl_sdesc = &q->q.sdesc[last_desc];\n\n\tif (unlikely(cxgb4_map_skb(tx_info->adap->pdev_dev, skb,\n\t\t\t\t   sgl_sdesc->addr) < 0)) {\n\t\tmemset(sgl_sdesc->addr, 0, sizeof(sgl_sdesc->addr));\n\t\tq->mapping_err++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tpos = &q->q.desc[q->q.pidx];\n\tend = (u64 *)pos + flits;\n\t \n\twr = pos;\n\twr->op_to_compl = htonl(FW_WR_OP_V(FW_ULPTX_WR));\n\twr->flowid_len16 = htonl(wr_mid | FW_WR_LEN16_V(len16));\n\twr->cookie = 0;\n\t \n\tulptx = (struct ulp_txpkt *)(wr + 1);\n\tulptx->cmd_dest = htonl(ULPTX_CMD_V(ULP_TX_PKT) |\n\t\t\tULP_TXPKT_DATAMODIFY_V(0) |\n\t\t\tULP_TXPKT_CHANNELID_V(tx_info->port_id) |\n\t\t\tULP_TXPKT_DEST_V(0) |\n\t\t\tULP_TXPKT_FID_V(q->q.cntxt_id) | ULP_TXPKT_RO_V(1));\n\tulptx->len = htonl(len16 - 1);\n\t \n\tidata = (struct ulptx_idata *)(ulptx + 1);\n\tidata->cmd_more = htonl(ULPTX_CMD_V(ULP_TX_SC_IMM) | ULP_TX_SC_MORE_F);\n\tidata->len = htonl(sizeof(*tx_data) + prior_data_len);\n\t \n\ttx_data = (struct cpl_tx_data *)(idata + 1);\n\tOPCODE_TID(tx_data) = htonl(MK_OPCODE_TID(CPL_TX_DATA, tx_info->tid));\n\ttx_data->len = htonl(TX_DATA_MSS_V(mss) |\n\t\t\t     TX_LENGTH_V(data_len + prior_data_len));\n\t \n\ttx_data->rsvd = htonl(tcp_seq);\n\ttx_data->flags = htonl(TX_BYPASS_F);\n\tif (tcp_push)\n\t\ttx_data->flags |= htonl(TX_PUSH_F | TX_SHOVE_F);\n\n\tpos = tx_data + 1;\n\t \n\tif (prior_data_len)\n\t\tpos = chcr_copy_to_txd(prior_data, &q->q, pos, 16);\n\n\t \n\tleft = (void *)q->q.stat - pos;\n\n\t \n\tif (!left) {\n\t\tleft = (void *)end - (void *)q->q.stat;\n\t\tpos = q->q.desc;\n\t\tend = pos + left;\n\t}\n\t \n\tcxgb4_write_partial_sgl(skb, &q->q, pos, end, sgl_sdesc->addr,\n\t\t\t\tskb_offset, data_len);\n\tsgl_sdesc->skb = skb;\n\n\tchcr_txq_advance(&q->q, ndesc);\n\tcxgb4_ring_tx_db(tx_info->adap, &q->q, ndesc);\n\treturn 0;\n}\n\nstatic int chcr_ktls_tunnel_pkt(struct chcr_ktls_info *tx_info,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct sge_eth_txq *q)\n{\n\tu32 ctrl, iplen, maclen, wr_mid = 0, len16;\n\tstruct tx_sw_desc *sgl_sdesc;\n\tstruct fw_eth_tx_pkt_wr *wr;\n\tstruct cpl_tx_pkt_core *cpl;\n\tunsigned int flits, ndesc;\n\tint credits, last_desc;\n\tu64 cntrl1, *end;\n\tvoid *pos;\n\n\tctrl = sizeof(*cpl);\n\tflits = DIV_ROUND_UP(sizeof(*wr) + ctrl, 8);\n\n\tflits += chcr_sgl_len(skb_shinfo(skb)->nr_frags + 1);\n\tlen16 = DIV_ROUND_UP(flits, 2);\n\t \n\tndesc = DIV_ROUND_UP(flits, 8);\n\n\tcredits = chcr_txq_avail(&q->q) - ndesc;\n\tif (unlikely(credits < 0)) {\n\t\tchcr_eth_txq_stop(q);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\tchcr_eth_txq_stop(q);\n\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\n\tlast_desc = q->q.pidx + ndesc - 1;\n\tif (last_desc >= q->q.size)\n\t\tlast_desc -= q->q.size;\n\tsgl_sdesc = &q->q.sdesc[last_desc];\n\n\tif (unlikely(cxgb4_map_skb(tx_info->adap->pdev_dev, skb,\n\t\t\t\t   sgl_sdesc->addr) < 0)) {\n\t\tmemset(sgl_sdesc->addr, 0, sizeof(sgl_sdesc->addr));\n\t\tq->mapping_err++;\n\t\treturn -ENOMEM;\n\t}\n\n\tiplen = skb_network_header_len(skb);\n\tmaclen = skb_mac_header_len(skb);\n\n\tpos = &q->q.desc[q->q.pidx];\n\tend = (u64 *)pos + flits;\n\twr = pos;\n\n\t \n\twr->op_immdlen = htonl(FW_WR_OP_V(FW_ETH_TX_PKT_WR) |\n\t\t\t       FW_WR_IMMDLEN_V(ctrl));\n\n\twr->equiq_to_len16 = htonl(wr_mid | FW_WR_LEN16_V(len16));\n\twr->r3 = 0;\n\n\tcpl = (void *)(wr + 1);\n\n\t \n\tcpl->ctrl0 = htonl(TXPKT_OPCODE_V(CPL_TX_PKT) |\n\t\t\t   TXPKT_INTF_V(tx_info->tx_chan) |\n\t\t\t   TXPKT_PF_V(tx_info->adap->pf));\n\tcpl->pack = 0;\n\tcntrl1 = TXPKT_CSUM_TYPE_V(tx_info->ip_family == AF_INET ?\n\t\t\t\t   TX_CSUM_TCPIP : TX_CSUM_TCPIP6);\n\tcntrl1 |= T6_TXPKT_ETHHDR_LEN_V(maclen - ETH_HLEN) |\n\t\t  TXPKT_IPHDR_LEN_V(iplen);\n\t \n\tcpl->ctrl1 = cpu_to_be64(cntrl1);\n\tcpl->len = htons(skb->len);\n\n\tpos = cpl + 1;\n\n\tcxgb4_write_sgl(skb, &q->q, pos, end, 0, sgl_sdesc->addr);\n\tsgl_sdesc->skb = skb;\n\tchcr_txq_advance(&q->q, ndesc);\n\tcxgb4_ring_tx_db(tx_info->adap, &q->q, ndesc);\n\treturn 0;\n}\n\n \nstatic void chcr_ktls_copy_record_in_skb(struct sk_buff *nskb,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct tls_record_info *record)\n{\n\tint i = 0;\n\n\tfor (i = 0; i < record->num_frags; i++) {\n\t\tskb_shinfo(nskb)->frags[i] = record->frags[i];\n\t\t \n\t\t__skb_frag_ref(&skb_shinfo(nskb)->frags[i]);\n\t}\n\n\tskb_shinfo(nskb)->nr_frags = record->num_frags;\n\tnskb->data_len = record->len;\n\tnskb->len += record->len;\n\tnskb->truesize += record->len;\n\tnskb->sk = skb->sk;\n\tnskb->destructor = skb->destructor;\n\trefcount_add(nskb->truesize, &nskb->sk->sk_wmem_alloc);\n}\n\n \nstatic int chcr_end_part_handler(struct chcr_ktls_info *tx_info,\n\t\t\t\t struct sk_buff *skb,\n\t\t\t\t struct tls_record_info *record,\n\t\t\t\t u32 tcp_seq, int mss, bool tcp_push_no_fin,\n\t\t\t\t struct sge_eth_txq *q, u32 skb_offset,\n\t\t\t\t u32 tls_end_offset, bool last_wr)\n{\n\tbool free_skb_if_tx_fails = false;\n\tstruct sk_buff *nskb = NULL;\n\n\t \n\tif (tls_end_offset == record->len) {\n\t\tnskb = skb;\n\t\tatomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_complete_pkts);\n\t} else {\n\t\tnskb = alloc_skb(0, GFP_ATOMIC);\n\t\tif (!nskb) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NETDEV_TX_BUSY;\n\t\t}\n\n\t\t \n\t\tchcr_ktls_copy_record_in_skb(nskb, skb, record);\n\t\t \n\t\ttcp_seq = tls_record_start_seq(record);\n\t\t \n\t\tskb_offset = 0;\n\n\t\tif (last_wr)\n\t\t\tdev_kfree_skb_any(skb);\n\t\telse\n\t\t\tfree_skb_if_tx_fails = true;\n\n\t\tlast_wr = true;\n\n\t\tatomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_end_pkts);\n\t}\n\n\tif (chcr_ktls_xmit_wr_complete(nskb, tx_info, q, tcp_seq,\n\t\t\t\t       last_wr, record->len, skb_offset,\n\t\t\t\t       record->num_frags,\n\t\t\t\t       (last_wr && tcp_push_no_fin),\n\t\t\t\t       mss)) {\n\t\tif (free_skb_if_tx_fails)\n\t\t\tdev_kfree_skb_any(skb);\n\t\tgoto out;\n\t}\n\ttx_info->prev_seq = record->end_seq;\n\treturn 0;\nout:\n\tdev_kfree_skb_any(nskb);\n\treturn NETDEV_TX_BUSY;\n}\n\n \nstatic int chcr_short_record_handler(struct chcr_ktls_info *tx_info,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     struct tls_record_info *record,\n\t\t\t\t     u32 tcp_seq, int mss, bool tcp_push_no_fin,\n\t\t\t\t     u32 data_len, u32 skb_offset,\n\t\t\t\t     struct sge_eth_txq *q, u32 tls_end_offset)\n{\n\tu32 tls_rec_offset = tcp_seq - tls_record_start_seq(record);\n\tu8 prior_data[16] = {0};\n\tu32 prior_data_len = 0;\n\n\t \n\tint remaining_record = tls_end_offset - data_len;\n\n\tif (remaining_record > 0 &&\n\t    remaining_record < TLS_CIPHER_AES_GCM_128_TAG_SIZE) {\n\t\tint trimmed_len = 0;\n\n\t\tif (tls_end_offset > TLS_CIPHER_AES_GCM_128_TAG_SIZE)\n\t\t\ttrimmed_len = data_len -\n\t\t\t\t      (TLS_CIPHER_AES_GCM_128_TAG_SIZE -\n\t\t\t\t       remaining_record);\n\t\tif (!trimmed_len)\n\t\t\treturn FALLBACK;\n\n\t\tWARN_ON(trimmed_len > data_len);\n\n\t\tdata_len = trimmed_len;\n\t\tatomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_trimmed_pkts);\n\t}\n\n\t \n\tif (tls_rec_offset + data_len <= (TLS_HEADER_SIZE + tx_info->iv_size)) {\n\t\tif (chcr_ktls_tx_plaintxt(tx_info, skb, tcp_seq, mss,\n\t\t\t\t\t  tcp_push_no_fin, q,\n\t\t\t\t\t  tx_info->port_id, prior_data,\n\t\t\t\t\t  data_len, skb_offset, prior_data_len))\n\t\t\tgoto out;\n\n\t\ttx_info->prev_seq = tcp_seq + data_len;\n\t\treturn 0;\n\t}\n\n\t \n\tif (tls_rec_offset) {\n\t\t \n\t\tint remaining = 0;\n\n\t\tif (tls_rec_offset < (TLS_HEADER_SIZE + tx_info->iv_size)) {\n\t\t\tprior_data_len = tls_rec_offset;\n\t\t\ttls_rec_offset = 0;\n\t\t\tremaining = 0;\n\t\t} else {\n\t\t\tprior_data_len =\n\t\t\t\t(tls_rec_offset -\n\t\t\t\t(TLS_HEADER_SIZE + tx_info->iv_size))\n\t\t\t\t% AES_BLOCK_LEN;\n\t\t\tremaining = tls_rec_offset - prior_data_len;\n\t\t}\n\n\t\t \n\t\tif (prior_data_len) {\n\t\t\tint i = 0;\n\t\t\tskb_frag_t *f;\n\t\t\tint frag_size = 0, frag_delta = 0;\n\n\t\t\twhile (remaining > 0) {\n\t\t\t\tfrag_size = skb_frag_size(&record->frags[i]);\n\t\t\t\tif (remaining < frag_size)\n\t\t\t\t\tbreak;\n\n\t\t\t\tremaining -= frag_size;\n\t\t\t\ti++;\n\t\t\t}\n\t\t\tf = &record->frags[i];\n\t\t\tfrag_delta = skb_frag_size(f) - remaining;\n\n\t\t\tif (frag_delta >= prior_data_len) {\n\t\t\t\tmemcpy_from_page(prior_data, skb_frag_page(f),\n\t\t\t\t\t\t skb_frag_off(f) + remaining,\n\t\t\t\t\t\t prior_data_len);\n\t\t\t} else {\n\t\t\t\tmemcpy_from_page(prior_data, skb_frag_page(f),\n\t\t\t\t\t\t skb_frag_off(f) + remaining,\n\t\t\t\t\t\t frag_delta);\n\n\t\t\t\t \n\t\t\t\tf = &record->frags[i + 1];\n\n\t\t\t\tmemcpy_from_page(prior_data + frag_delta,\n\t\t\t\t\t\t skb_frag_page(f),\n\t\t\t\t\t\t skb_frag_off(f),\n\t\t\t\t\t\t prior_data_len - frag_delta);\n\t\t\t}\n\t\t\t \n\t\t\ttcp_seq -= prior_data_len;\n\t\t}\n\t\tatomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_middle_pkts);\n\t} else {\n\t\tatomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_start_pkts);\n\t}\n\n\tif (chcr_ktls_xmit_wr_short(skb, tx_info, q, tcp_seq, tcp_push_no_fin,\n\t\t\t\t    mss, tls_rec_offset, prior_data,\n\t\t\t\t    prior_data_len, data_len, skb_offset)) {\n\t\tgoto out;\n\t}\n\n\ttx_info->prev_seq = tcp_seq + data_len + prior_data_len;\n\treturn 0;\nout:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_BUSY;\n}\n\nstatic int chcr_ktls_sw_fallback(struct sk_buff *skb,\n\t\t\t\t struct chcr_ktls_info *tx_info,\n\t\t\t\t struct sge_eth_txq *q)\n{\n\tu32 data_len, skb_offset;\n\tstruct sk_buff *nskb;\n\tstruct tcphdr *th;\n\n\tnskb = tls_encrypt_skb(skb);\n\n\tif (!nskb)\n\t\treturn 0;\n\n\tth = tcp_hdr(nskb);\n\tskb_offset = skb_tcp_all_headers(nskb);\n\tdata_len = nskb->len - skb_offset;\n\tskb_tx_timestamp(nskb);\n\n\tif (chcr_ktls_tunnel_pkt(tx_info, nskb, q))\n\t\tgoto out;\n\n\ttx_info->prev_seq = ntohl(th->seq) + data_len;\n\tatomic64_inc(&tx_info->adap->ch_ktls_stats.ktls_tx_fallback);\n\treturn 0;\nout:\n\tdev_kfree_skb_any(nskb);\n\treturn 0;\n}\n \nstatic int chcr_ktls_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tu32 tls_end_offset, tcp_seq, skb_data_len, skb_offset;\n\tstruct ch_ktls_port_stats_debug *port_stats;\n\tstruct chcr_ktls_ofld_ctx_tx *tx_ctx;\n\tstruct ch_ktls_stats_debug *stats;\n\tstruct tcphdr *th = tcp_hdr(skb);\n\tint data_len, qidx, ret = 0, mss;\n\tstruct tls_record_info *record;\n\tstruct chcr_ktls_info *tx_info;\n\tstruct net_device *tls_netdev;\n\tstruct tls_context *tls_ctx;\n\tstruct sge_eth_txq *q;\n\tstruct adapter *adap;\n\tunsigned long flags;\n\n\ttcp_seq = ntohl(th->seq);\n\tskb_offset = skb_tcp_all_headers(skb);\n\tskb_data_len = skb->len - skb_offset;\n\tdata_len = skb_data_len;\n\n\tmss = skb_is_gso(skb) ? skb_shinfo(skb)->gso_size : data_len;\n\n\ttls_ctx = tls_get_ctx(skb->sk);\n\ttls_netdev = rcu_dereference_bh(tls_ctx->netdev);\n\t \n\tif (unlikely(tls_netdev && tls_netdev != dev))\n\t\tgoto out;\n\n\ttx_ctx = chcr_get_ktls_tx_context(tls_ctx);\n\ttx_info = tx_ctx->chcr_info;\n\n\tif (unlikely(!tx_info))\n\t\tgoto out;\n\n\tadap = tx_info->adap;\n\tstats = &adap->ch_ktls_stats;\n\tport_stats = &stats->ktls_port[tx_info->port_id];\n\n\tqidx = skb->queue_mapping;\n\tq = &adap->sge.ethtxq[qidx + tx_info->first_qset];\n\tcxgb4_reclaim_completed_tx(adap, &q->q, true);\n\t \n\tif (!th->fin && chcr_ktls_check_tcp_options(th)) {\n\t\tret = chcr_ktls_write_tcp_options(tx_info, skb, q,\n\t\t\t\t\t\t  tx_info->tx_chan);\n\t\tif (ret)\n\t\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\n\tspin_lock_irqsave(&tx_ctx->base.lock, flags);\n\n\tdo {\n\n\t\tcxgb4_reclaim_completed_tx(adap, &q->q, true);\n\t\t \n\t\trecord = tls_get_record(&tx_ctx->base, tcp_seq,\n\t\t\t\t\t&tx_info->record_no);\n\t\t \n\t\tif (unlikely(!record)) {\n\t\t\tspin_unlock_irqrestore(&tx_ctx->base.lock, flags);\n\t\t\tatomic64_inc(&port_stats->ktls_tx_drop_no_sync_data);\n\t\t\tgoto out;\n\t\t}\n\n\t\ttls_end_offset = record->end_seq - tcp_seq;\n\n\t\tpr_debug(\"seq 0x%x, end_seq 0x%x prev_seq 0x%x, datalen 0x%x\\n\",\n\t\t\t tcp_seq, record->end_seq, tx_info->prev_seq, data_len);\n\t\t \n\t\tif (skb_data_len == data_len) {\n\t\t\tu32 tx_max = tcp_seq;\n\n\t\t\tif (!tls_record_is_start_marker(record) &&\n\t\t\t    tls_end_offset < TLS_CIPHER_AES_GCM_128_TAG_SIZE)\n\t\t\t\ttx_max = record->end_seq -\n\t\t\t\t\tTLS_CIPHER_AES_GCM_128_TAG_SIZE;\n\n\t\t\tret = chcr_ktls_xmit_tcb_cpls(tx_info, q, tx_max,\n\t\t\t\t\t\t      ntohl(th->ack_seq),\n\t\t\t\t\t\t      ntohs(th->window),\n\t\t\t\t\t\t      tls_end_offset !=\n\t\t\t\t\t\t      record->len);\n\t\t\tif (ret) {\n\t\t\t\tspin_unlock_irqrestore(&tx_ctx->base.lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (th->fin)\n\t\t\t\tskb_get(skb);\n\t\t}\n\n\t\tif (unlikely(tls_record_is_start_marker(record))) {\n\t\t\tatomic64_inc(&port_stats->ktls_tx_skip_no_sync_data);\n\t\t\t \n\t\t\tif (tls_end_offset < data_len)\n\t\t\t\tskb_get(skb);\n\t\t\telse\n\t\t\t\ttls_end_offset = data_len;\n\n\t\t\tret = chcr_ktls_tx_plaintxt(tx_info, skb, tcp_seq, mss,\n\t\t\t\t\t\t    (!th->fin && th->psh), q,\n\t\t\t\t\t\t    tx_info->port_id, NULL,\n\t\t\t\t\t\t    tls_end_offset, skb_offset,\n\t\t\t\t\t\t    0);\n\n\t\t\tif (ret) {\n\t\t\t\t \n\t\t\t\tif (tls_end_offset < data_len)\n\t\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\t\tspin_unlock_irqrestore(&tx_ctx->base.lock, flags);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tdata_len -= tls_end_offset;\n\t\t\ttcp_seq = record->end_seq;\n\t\t\tskb_offset += tls_end_offset;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (tls_end_offset <= data_len) {\n\t\t\tret = chcr_end_part_handler(tx_info, skb, record,\n\t\t\t\t\t\t    tcp_seq, mss,\n\t\t\t\t\t\t    (!th->fin && th->psh), q,\n\t\t\t\t\t\t    skb_offset,\n\t\t\t\t\t\t    tls_end_offset,\n\t\t\t\t\t\t    skb_offset +\n\t\t\t\t\t\t    tls_end_offset == skb->len);\n\n\t\t\tdata_len -= tls_end_offset;\n\t\t\t \n\t\t\ttcp_seq += tls_end_offset;\n\t\t\tskb_offset += tls_end_offset;\n\t\t} else {\n\t\t\tret = chcr_short_record_handler(tx_info, skb,\n\t\t\t\t\t\t\trecord, tcp_seq, mss,\n\t\t\t\t\t\t\t(!th->fin && th->psh),\n\t\t\t\t\t\t\tdata_len, skb_offset,\n\t\t\t\t\t\t\tq, tls_end_offset);\n\t\t\tdata_len = 0;\n\t\t}\n\n\t\t \n\t\tif (ret) {\n\t\t\tspin_unlock_irqrestore(&tx_ctx->base.lock, flags);\n\t\t\tif (th->fin)\n\t\t\t\tdev_kfree_skb_any(skb);\n\n\t\t\tif (ret == FALLBACK)\n\t\t\t\treturn chcr_ktls_sw_fallback(skb, tx_info, q);\n\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\t \n\t\tWARN_ON(data_len < 0);\n\n\t} while (data_len > 0);\n\n\tspin_unlock_irqrestore(&tx_ctx->base.lock, flags);\n\tatomic64_inc(&port_stats->ktls_tx_encrypted_packets);\n\tatomic64_add(skb_data_len, &port_stats->ktls_tx_encrypted_bytes);\n\n\t \n\tif (th->fin) {\n\t\tchcr_ktls_write_tcp_options(tx_info, skb, q, tx_info->tx_chan);\n\t\tdev_kfree_skb_any(skb);\n\t}\n\n\treturn NETDEV_TX_OK;\nout:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void *chcr_ktls_uld_add(const struct cxgb4_lld_info *lldi)\n{\n\tstruct chcr_ktls_uld_ctx *u_ctx;\n\n\tpr_info_once(\"%s - version %s\\n\", CHCR_KTLS_DRV_DESC,\n\t\t     CHCR_KTLS_DRV_VERSION);\n\tu_ctx = kzalloc(sizeof(*u_ctx), GFP_KERNEL);\n\tif (!u_ctx) {\n\t\tu_ctx = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\tu_ctx->lldi = *lldi;\n\tu_ctx->detach = false;\n\txa_init_flags(&u_ctx->tid_list, XA_FLAGS_LOCK_BH);\nout:\n\treturn u_ctx;\n}\n\nstatic const struct tlsdev_ops chcr_ktls_ops = {\n\t.tls_dev_add = chcr_ktls_dev_add,\n\t.tls_dev_del = chcr_ktls_dev_del,\n};\n\nstatic chcr_handler_func work_handlers[NUM_CPL_CMDS] = {\n\t[CPL_ACT_OPEN_RPL] = chcr_ktls_cpl_act_open_rpl,\n\t[CPL_SET_TCB_RPL] = chcr_ktls_cpl_set_tcb_rpl,\n};\n\nstatic int chcr_ktls_uld_rx_handler(void *handle, const __be64 *rsp,\n\t\t\t\t    const struct pkt_gl *pgl)\n{\n\tconst struct cpl_act_open_rpl *rpl = (struct cpl_act_open_rpl *)rsp;\n\tstruct chcr_ktls_uld_ctx *u_ctx = handle;\n\tu8 opcode = rpl->ot.opcode;\n\tstruct adapter *adap;\n\n\tadap = pci_get_drvdata(u_ctx->lldi.pdev);\n\n\tif (!work_handlers[opcode]) {\n\t\tpr_err(\"Unsupported opcode %d received\\n\", opcode);\n\t\treturn 0;\n\t}\n\n\twork_handlers[opcode](adap, (unsigned char *)&rsp[1]);\n\treturn 0;\n}\n\nstatic void clear_conn_resources(struct chcr_ktls_info *tx_info)\n{\n\t \n\tif (tx_info->l2te)\n\t\tcxgb4_l2t_release(tx_info->l2te);\n\n#if IS_ENABLED(CONFIG_IPV6)\n\t \n\tif (tx_info->ip_family == AF_INET6)\n\t\tcxgb4_clip_release(tx_info->netdev, (const u32 *)\n\t\t\t\t   &tx_info->sk->sk_v6_rcv_saddr,\n\t\t\t\t   1);\n#endif\n\n\t \n\tif (tx_info->tid != -1)\n\t\tcxgb4_remove_tid(&tx_info->adap->tids, tx_info->tx_chan,\n\t\t\t\t tx_info->tid, tx_info->ip_family);\n}\n\nstatic void ch_ktls_reset_all_conn(struct chcr_ktls_uld_ctx *u_ctx)\n{\n\tstruct ch_ktls_port_stats_debug *port_stats;\n\tstruct chcr_ktls_ofld_ctx_tx *tx_ctx;\n\tstruct chcr_ktls_info *tx_info;\n\tunsigned long index;\n\n\txa_for_each(&u_ctx->tid_list, index, tx_ctx) {\n\t\ttx_info = tx_ctx->chcr_info;\n\t\tclear_conn_resources(tx_info);\n\t\tport_stats = &tx_info->adap->ch_ktls_stats.ktls_port[tx_info->port_id];\n\t\tatomic64_inc(&port_stats->ktls_tx_connection_close);\n\t\tkvfree(tx_info);\n\t\ttx_ctx->chcr_info = NULL;\n\t\t \n\t\tmodule_put(THIS_MODULE);\n\t}\n}\n\nstatic int chcr_ktls_uld_state_change(void *handle, enum cxgb4_state new_state)\n{\n\tstruct chcr_ktls_uld_ctx *u_ctx = handle;\n\n\tswitch (new_state) {\n\tcase CXGB4_STATE_UP:\n\t\tpr_info(\"%s: Up\\n\", pci_name(u_ctx->lldi.pdev));\n\t\tmutex_lock(&dev_mutex);\n\t\tlist_add_tail(&u_ctx->entry, &uld_ctx_list);\n\t\tmutex_unlock(&dev_mutex);\n\t\tbreak;\n\tcase CXGB4_STATE_START_RECOVERY:\n\tcase CXGB4_STATE_DOWN:\n\tcase CXGB4_STATE_DETACH:\n\t\tpr_info(\"%s: Down\\n\", pci_name(u_ctx->lldi.pdev));\n\t\tmutex_lock(&dev_mutex);\n\t\tu_ctx->detach = true;\n\t\tlist_del(&u_ctx->entry);\n\t\tch_ktls_reset_all_conn(u_ctx);\n\t\txa_destroy(&u_ctx->tid_list);\n\t\tmutex_unlock(&dev_mutex);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic struct cxgb4_uld_info chcr_ktls_uld_info = {\n\t.name = CHCR_KTLS_DRV_MODULE_NAME,\n\t.nrxq = 1,\n\t.rxq_size = 1024,\n\t.add = chcr_ktls_uld_add,\n\t.tx_handler = chcr_ktls_xmit,\n\t.rx_handler = chcr_ktls_uld_rx_handler,\n\t.state_change = chcr_ktls_uld_state_change,\n\t.tlsdev_ops = &chcr_ktls_ops,\n};\n\nstatic int __init chcr_ktls_init(void)\n{\n\tcxgb4_register_uld(CXGB4_ULD_KTLS, &chcr_ktls_uld_info);\n\treturn 0;\n}\n\nstatic void __exit chcr_ktls_exit(void)\n{\n\tstruct chcr_ktls_uld_ctx *u_ctx, *tmp;\n\tstruct adapter *adap;\n\n\tpr_info(\"driver unloaded\\n\");\n\n\tmutex_lock(&dev_mutex);\n\tlist_for_each_entry_safe(u_ctx, tmp, &uld_ctx_list, entry) {\n\t\tadap = pci_get_drvdata(u_ctx->lldi.pdev);\n\t\tmemset(&adap->ch_ktls_stats, 0, sizeof(adap->ch_ktls_stats));\n\t\tlist_del(&u_ctx->entry);\n\t\txa_destroy(&u_ctx->tid_list);\n\t\tkfree(u_ctx);\n\t}\n\tmutex_unlock(&dev_mutex);\n\tcxgb4_unregister_uld(CXGB4_ULD_KTLS);\n}\n\nmodule_init(chcr_ktls_init);\nmodule_exit(chcr_ktls_exit);\n\nMODULE_DESCRIPTION(\"Chelsio NIC TLS ULD driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Chelsio Communications\");\nMODULE_VERSION(CHCR_KTLS_DRV_VERSION);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}