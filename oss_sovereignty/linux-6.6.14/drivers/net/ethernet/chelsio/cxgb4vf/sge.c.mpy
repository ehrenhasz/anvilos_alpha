{
  "module_name": "sge.c",
  "hash_id": "ef3c828e5e5c5d97cab5e8a5f0e7167cc74ab0861811031817773d1ef6663bf8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/chelsio/cxgb4vf/sge.c",
  "human_readable_source": " \n\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ipv6.h>\n#include <net/tcp.h>\n#include <linux/dma-mapping.h>\n#include <linux/prefetch.h>\n\n#include \"t4vf_common.h\"\n#include \"t4vf_defs.h\"\n\n#include \"../cxgb4/t4_regs.h\"\n#include \"../cxgb4/t4_values.h\"\n#include \"../cxgb4/t4fw_api.h\"\n#include \"../cxgb4/t4_msg.h\"\n\n \nenum {\n\t \n\tEQ_UNIT = SGE_EQ_IDXSIZE,\n\tFL_PER_EQ_UNIT = EQ_UNIT / sizeof(__be64),\n\tTXD_PER_EQ_UNIT = EQ_UNIT / sizeof(__be64),\n\n\t \n\tMAX_TX_RECLAIM = 16,\n\n\t \n\tMAX_RX_REFILL = 16,\n\n\t \n\tRX_QCHECK_PERIOD = (HZ / 2),\n\n\t \n\tTX_QCHECK_PERIOD = (HZ / 2),\n\tMAX_TIMER_TX_RECLAIM = 100,\n\n\t \n\tETHTXQ_MAX_FRAGS = MAX_SKB_FRAGS + 1,\n\tETHTXQ_MAX_SGL_LEN = ((3 * (ETHTXQ_MAX_FRAGS-1))/2 +\n\t\t\t\t   ((ETHTXQ_MAX_FRAGS-1) & 1) +\n\t\t\t\t   2),\n\tETHTXQ_MAX_HDR = (sizeof(struct fw_eth_tx_pkt_vm_wr) +\n\t\t\t  sizeof(struct cpl_tx_pkt_lso_core) +\n\t\t\t  sizeof(struct cpl_tx_pkt_core)) / sizeof(__be64),\n\tETHTXQ_MAX_FLITS = ETHTXQ_MAX_SGL_LEN + ETHTXQ_MAX_HDR,\n\n\tETHTXQ_STOP_THRES = 1 + DIV_ROUND_UP(ETHTXQ_MAX_FLITS, TXD_PER_EQ_UNIT),\n\n\t \n\tMAX_IMM_TX_PKT_LEN = FW_WR_IMMDLEN_M,\n\n\t \n\tMAX_CTRL_WR_LEN = 256,\n\n\t \n\tMAX_IMM_TX_LEN = (MAX_IMM_TX_PKT_LEN > MAX_CTRL_WR_LEN\n\t\t\t  ? MAX_IMM_TX_PKT_LEN\n\t\t\t  : MAX_CTRL_WR_LEN),\n\n\t \n\tRX_COPY_THRES = 256,\n\tRX_PULL_LEN = 128,\n\n\t \n\tRX_SKB_LEN = 512,\n};\n\n \nstruct tx_sw_desc {\n\tstruct sk_buff *skb;\t\t \n\tstruct ulptx_sgl *sgl;\t\t \n};\n\n \nstruct rx_sw_desc {\n\tstruct page *page;\t\t \n\tdma_addr_t dma_addr;\t\t \n\t\t\t\t\t \n};\n\n \nenum {\n\tRX_LARGE_BUF    = 1 << 0,\t \n\tRX_UNMAPPED_BUF = 1 << 1,\t \n};\n\n \nstatic inline dma_addr_t get_buf_addr(const struct rx_sw_desc *sdesc)\n{\n\treturn sdesc->dma_addr & ~(dma_addr_t)(RX_LARGE_BUF | RX_UNMAPPED_BUF);\n}\n\n \nstatic inline bool is_buf_mapped(const struct rx_sw_desc *sdesc)\n{\n\treturn !(sdesc->dma_addr & RX_UNMAPPED_BUF);\n}\n\n \nstatic inline int need_skb_unmap(void)\n{\n#ifdef CONFIG_NEED_DMA_MAP_STATE\n\treturn 1;\n#else\n\treturn 0;\n#endif\n}\n\n \nstatic inline unsigned int txq_avail(const struct sge_txq *tq)\n{\n\treturn tq->size - 1 - tq->in_use;\n}\n\n \nstatic inline unsigned int fl_cap(const struct sge_fl *fl)\n{\n\treturn fl->size - FL_PER_EQ_UNIT;\n}\n\n \nstatic inline bool fl_starving(const struct adapter *adapter,\n\t\t\t       const struct sge_fl *fl)\n{\n\tconst struct sge *s = &adapter->sge;\n\n\treturn fl->avail - fl->pend_cred <= s->fl_starve_thres;\n}\n\n \nstatic int map_skb(struct device *dev, const struct sk_buff *skb,\n\t\t   dma_addr_t *addr)\n{\n\tconst skb_frag_t *fp, *end;\n\tconst struct skb_shared_info *si;\n\n\t*addr = dma_map_single(dev, skb->data, skb_headlen(skb), DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, *addr))\n\t\tgoto out_err;\n\n\tsi = skb_shinfo(skb);\n\tend = &si->frags[si->nr_frags];\n\tfor (fp = si->frags; fp < end; fp++) {\n\t\t*++addr = skb_frag_dma_map(dev, fp, 0, skb_frag_size(fp),\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, *addr))\n\t\t\tgoto unwind;\n\t}\n\treturn 0;\n\nunwind:\n\twhile (fp-- > si->frags)\n\t\tdma_unmap_page(dev, *--addr, skb_frag_size(fp), DMA_TO_DEVICE);\n\tdma_unmap_single(dev, addr[-1], skb_headlen(skb), DMA_TO_DEVICE);\n\nout_err:\n\treturn -ENOMEM;\n}\n\nstatic void unmap_sgl(struct device *dev, const struct sk_buff *skb,\n\t\t      const struct ulptx_sgl *sgl, const struct sge_txq *tq)\n{\n\tconst struct ulptx_sge_pair *p;\n\tunsigned int nfrags = skb_shinfo(skb)->nr_frags;\n\n\tif (likely(skb_headlen(skb)))\n\t\tdma_unmap_single(dev, be64_to_cpu(sgl->addr0),\n\t\t\t\t be32_to_cpu(sgl->len0), DMA_TO_DEVICE);\n\telse {\n\t\tdma_unmap_page(dev, be64_to_cpu(sgl->addr0),\n\t\t\t       be32_to_cpu(sgl->len0), DMA_TO_DEVICE);\n\t\tnfrags--;\n\t}\n\n\t \n\tfor (p = sgl->sge; nfrags >= 2; nfrags -= 2) {\n\t\tif (likely((u8 *)(p + 1) <= (u8 *)tq->stat)) {\nunmap:\n\t\t\tdma_unmap_page(dev, be64_to_cpu(p->addr[0]),\n\t\t\t\t       be32_to_cpu(p->len[0]), DMA_TO_DEVICE);\n\t\t\tdma_unmap_page(dev, be64_to_cpu(p->addr[1]),\n\t\t\t\t       be32_to_cpu(p->len[1]), DMA_TO_DEVICE);\n\t\t\tp++;\n\t\t} else if ((u8 *)p == (u8 *)tq->stat) {\n\t\t\tp = (const struct ulptx_sge_pair *)tq->desc;\n\t\t\tgoto unmap;\n\t\t} else if ((u8 *)p + 8 == (u8 *)tq->stat) {\n\t\t\tconst __be64 *addr = (const __be64 *)tq->desc;\n\n\t\t\tdma_unmap_page(dev, be64_to_cpu(addr[0]),\n\t\t\t\t       be32_to_cpu(p->len[0]), DMA_TO_DEVICE);\n\t\t\tdma_unmap_page(dev, be64_to_cpu(addr[1]),\n\t\t\t\t       be32_to_cpu(p->len[1]), DMA_TO_DEVICE);\n\t\t\tp = (const struct ulptx_sge_pair *)&addr[2];\n\t\t} else {\n\t\t\tconst __be64 *addr = (const __be64 *)tq->desc;\n\n\t\t\tdma_unmap_page(dev, be64_to_cpu(p->addr[0]),\n\t\t\t\t       be32_to_cpu(p->len[0]), DMA_TO_DEVICE);\n\t\t\tdma_unmap_page(dev, be64_to_cpu(addr[0]),\n\t\t\t\t       be32_to_cpu(p->len[1]), DMA_TO_DEVICE);\n\t\t\tp = (const struct ulptx_sge_pair *)&addr[1];\n\t\t}\n\t}\n\tif (nfrags) {\n\t\t__be64 addr;\n\n\t\tif ((u8 *)p == (u8 *)tq->stat)\n\t\t\tp = (const struct ulptx_sge_pair *)tq->desc;\n\t\taddr = ((u8 *)p + 16 <= (u8 *)tq->stat\n\t\t\t? p->addr[0]\n\t\t\t: *(const __be64 *)tq->desc);\n\t\tdma_unmap_page(dev, be64_to_cpu(addr), be32_to_cpu(p->len[0]),\n\t\t\t       DMA_TO_DEVICE);\n\t}\n}\n\n \nstatic void free_tx_desc(struct adapter *adapter, struct sge_txq *tq,\n\t\t\t unsigned int n, bool unmap)\n{\n\tstruct tx_sw_desc *sdesc;\n\tunsigned int cidx = tq->cidx;\n\tstruct device *dev = adapter->pdev_dev;\n\n\tconst int need_unmap = need_skb_unmap() && unmap;\n\n\tsdesc = &tq->sdesc[cidx];\n\twhile (n--) {\n\t\t \n\t\tif (sdesc->skb) {\n\t\t\tif (need_unmap)\n\t\t\t\tunmap_sgl(dev, sdesc->skb, sdesc->sgl, tq);\n\t\t\tdev_consume_skb_any(sdesc->skb);\n\t\t\tsdesc->skb = NULL;\n\t\t}\n\n\t\tsdesc++;\n\t\tif (++cidx == tq->size) {\n\t\t\tcidx = 0;\n\t\t\tsdesc = tq->sdesc;\n\t\t}\n\t}\n\ttq->cidx = cidx;\n}\n\n \nstatic inline int reclaimable(const struct sge_txq *tq)\n{\n\tint hw_cidx = be16_to_cpu(tq->stat->cidx);\n\tint reclaimable = hw_cidx - tq->cidx;\n\tif (reclaimable < 0)\n\t\treclaimable += tq->size;\n\treturn reclaimable;\n}\n\n \nstatic inline void reclaim_completed_tx(struct adapter *adapter,\n\t\t\t\t\tstruct sge_txq *tq,\n\t\t\t\t\tbool unmap)\n{\n\tint avail = reclaimable(tq);\n\n\tif (avail) {\n\t\t \n\t\tif (avail > MAX_TX_RECLAIM)\n\t\t\tavail = MAX_TX_RECLAIM;\n\n\t\tfree_tx_desc(adapter, tq, avail, unmap);\n\t\ttq->in_use -= avail;\n\t}\n}\n\n \nstatic inline int get_buf_size(const struct adapter *adapter,\n\t\t\t       const struct rx_sw_desc *sdesc)\n{\n\tconst struct sge *s = &adapter->sge;\n\n\treturn (s->fl_pg_order > 0 && (sdesc->dma_addr & RX_LARGE_BUF)\n\t\t? (PAGE_SIZE << s->fl_pg_order) : PAGE_SIZE);\n}\n\n \nstatic void free_rx_bufs(struct adapter *adapter, struct sge_fl *fl, int n)\n{\n\twhile (n--) {\n\t\tstruct rx_sw_desc *sdesc = &fl->sdesc[fl->cidx];\n\n\t\tif (is_buf_mapped(sdesc))\n\t\t\tdma_unmap_page(adapter->pdev_dev, get_buf_addr(sdesc),\n\t\t\t\t       get_buf_size(adapter, sdesc),\n\t\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(sdesc->page);\n\t\tsdesc->page = NULL;\n\t\tif (++fl->cidx == fl->size)\n\t\t\tfl->cidx = 0;\n\t\tfl->avail--;\n\t}\n}\n\n \nstatic void unmap_rx_buf(struct adapter *adapter, struct sge_fl *fl)\n{\n\tstruct rx_sw_desc *sdesc = &fl->sdesc[fl->cidx];\n\n\tif (is_buf_mapped(sdesc))\n\t\tdma_unmap_page(adapter->pdev_dev, get_buf_addr(sdesc),\n\t\t\t       get_buf_size(adapter, sdesc),\n\t\t\t       DMA_FROM_DEVICE);\n\tsdesc->page = NULL;\n\tif (++fl->cidx == fl->size)\n\t\tfl->cidx = 0;\n\tfl->avail--;\n}\n\n \nstatic inline void ring_fl_db(struct adapter *adapter, struct sge_fl *fl)\n{\n\tu32 val = adapter->params.arch.sge_fl_db;\n\n\t \n\tif (fl->pend_cred >= FL_PER_EQ_UNIT) {\n\t\tif (is_t4(adapter->params.chip))\n\t\t\tval |= PIDX_V(fl->pend_cred / FL_PER_EQ_UNIT);\n\t\telse\n\t\t\tval |= PIDX_T5_V(fl->pend_cred / FL_PER_EQ_UNIT);\n\n\t\t \n\t\twmb();\n\n\t\t \n\t\tif (unlikely(fl->bar2_addr == NULL)) {\n\t\t\tt4_write_reg(adapter,\n\t\t\t\t     T4VF_SGE_BASE_ADDR + SGE_VF_KDOORBELL,\n\t\t\t\t     QID_V(fl->cntxt_id) | val);\n\t\t} else {\n\t\t\twritel(val | QID_V(fl->bar2_qid),\n\t\t\t       fl->bar2_addr + SGE_UDB_KDOORBELL);\n\n\t\t\t \n\t\t\twmb();\n\t\t}\n\t\tfl->pend_cred %= FL_PER_EQ_UNIT;\n\t}\n}\n\n \nstatic inline void set_rx_sw_desc(struct rx_sw_desc *sdesc, struct page *page,\n\t\t\t\t  dma_addr_t dma_addr)\n{\n\tsdesc->page = page;\n\tsdesc->dma_addr = dma_addr;\n}\n\n \n#define POISON_BUF_VAL -1\n\nstatic inline void poison_buf(struct page *page, size_t sz)\n{\n#if POISON_BUF_VAL >= 0\n\tmemset(page_address(page), POISON_BUF_VAL, sz);\n#endif\n}\n\n \nstatic unsigned int refill_fl(struct adapter *adapter, struct sge_fl *fl,\n\t\t\t      int n, gfp_t gfp)\n{\n\tstruct sge *s = &adapter->sge;\n\tstruct page *page;\n\tdma_addr_t dma_addr;\n\tunsigned int cred = fl->avail;\n\t__be64 *d = &fl->desc[fl->pidx];\n\tstruct rx_sw_desc *sdesc = &fl->sdesc[fl->pidx];\n\n\t \n\tBUG_ON(fl->avail + n > fl->size - FL_PER_EQ_UNIT);\n\n\tgfp |= __GFP_NOWARN;\n\n\t \n\tif (s->fl_pg_order == 0)\n\t\tgoto alloc_small_pages;\n\n\twhile (n) {\n\t\tpage = __dev_alloc_pages(gfp, s->fl_pg_order);\n\t\tif (unlikely(!page)) {\n\t\t\t \n\t\t\tfl->large_alloc_failed++;\n\t\t\tbreak;\n\t\t}\n\t\tpoison_buf(page, PAGE_SIZE << s->fl_pg_order);\n\n\t\tdma_addr = dma_map_page(adapter->pdev_dev, page, 0,\n\t\t\t\t\tPAGE_SIZE << s->fl_pg_order,\n\t\t\t\t\tDMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(adapter->pdev_dev, dma_addr))) {\n\t\t\t \n\t\t\t__free_pages(page, s->fl_pg_order);\n\t\t\tgoto out;\n\t\t}\n\t\tdma_addr |= RX_LARGE_BUF;\n\t\t*d++ = cpu_to_be64(dma_addr);\n\n\t\tset_rx_sw_desc(sdesc, page, dma_addr);\n\t\tsdesc++;\n\n\t\tfl->avail++;\n\t\tif (++fl->pidx == fl->size) {\n\t\t\tfl->pidx = 0;\n\t\t\tsdesc = fl->sdesc;\n\t\t\td = fl->desc;\n\t\t}\n\t\tn--;\n\t}\n\nalloc_small_pages:\n\twhile (n--) {\n\t\tpage = __dev_alloc_page(gfp);\n\t\tif (unlikely(!page)) {\n\t\t\tfl->alloc_failed++;\n\t\t\tbreak;\n\t\t}\n\t\tpoison_buf(page, PAGE_SIZE);\n\n\t\tdma_addr = dma_map_page(adapter->pdev_dev, page, 0, PAGE_SIZE,\n\t\t\t\t       DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(adapter->pdev_dev, dma_addr))) {\n\t\t\tput_page(page);\n\t\t\tbreak;\n\t\t}\n\t\t*d++ = cpu_to_be64(dma_addr);\n\n\t\tset_rx_sw_desc(sdesc, page, dma_addr);\n\t\tsdesc++;\n\n\t\tfl->avail++;\n\t\tif (++fl->pidx == fl->size) {\n\t\t\tfl->pidx = 0;\n\t\t\tsdesc = fl->sdesc;\n\t\t\td = fl->desc;\n\t\t}\n\t}\n\nout:\n\t \n\tcred = fl->avail - cred;\n\tfl->pend_cred += cred;\n\tring_fl_db(adapter, fl);\n\n\tif (unlikely(fl_starving(adapter, fl))) {\n\t\tsmp_wmb();\n\t\tset_bit(fl->cntxt_id, adapter->sge.starving_fl);\n\t}\n\n\treturn cred;\n}\n\n \nstatic inline void __refill_fl(struct adapter *adapter, struct sge_fl *fl)\n{\n\trefill_fl(adapter, fl,\n\t\t  min((unsigned int)MAX_RX_REFILL, fl_cap(fl) - fl->avail),\n\t\t  GFP_ATOMIC);\n}\n\n \nstatic void *alloc_ring(struct device *dev, size_t nelem, size_t hwsize,\n\t\t\tsize_t swsize, dma_addr_t *busaddrp, void *swringp,\n\t\t\tsize_t stat_size)\n{\n\t \n\tsize_t hwlen = nelem * hwsize + stat_size;\n\tvoid *hwring = dma_alloc_coherent(dev, hwlen, busaddrp, GFP_KERNEL);\n\n\tif (!hwring)\n\t\treturn NULL;\n\n\t \n\tBUG_ON((swsize != 0) != (swringp != NULL));\n\tif (swsize) {\n\t\tvoid *swring = kcalloc(nelem, swsize, GFP_KERNEL);\n\n\t\tif (!swring) {\n\t\t\tdma_free_coherent(dev, hwlen, hwring, *busaddrp);\n\t\t\treturn NULL;\n\t\t}\n\t\t*(void **)swringp = swring;\n\t}\n\n\treturn hwring;\n}\n\n \nstatic inline unsigned int sgl_len(unsigned int n)\n{\n\t \n\tn--;\n\treturn (3 * n) / 2 + (n & 1) + 2;\n}\n\n \nstatic inline unsigned int flits_to_desc(unsigned int flits)\n{\n\tBUG_ON(flits > SGE_MAX_WR_LEN / sizeof(__be64));\n\treturn DIV_ROUND_UP(flits, TXD_PER_EQ_UNIT);\n}\n\n \nstatic inline int is_eth_imm(const struct sk_buff *skb)\n{\n\t \n\treturn false;\n}\n\n \nstatic inline unsigned int calc_tx_flits(const struct sk_buff *skb)\n{\n\tunsigned int flits;\n\n\t \n\tif (is_eth_imm(skb))\n\t\treturn DIV_ROUND_UP(skb->len + sizeof(struct cpl_tx_pkt),\n\t\t\t\t    sizeof(__be64));\n\n\t \n\tflits = sgl_len(skb_shinfo(skb)->nr_frags + 1);\n\tif (skb_shinfo(skb)->gso_size)\n\t\tflits += (sizeof(struct fw_eth_tx_pkt_vm_wr) +\n\t\t\t  sizeof(struct cpl_tx_pkt_lso_core) +\n\t\t\t  sizeof(struct cpl_tx_pkt_core)) / sizeof(__be64);\n\telse\n\t\tflits += (sizeof(struct fw_eth_tx_pkt_vm_wr) +\n\t\t\t  sizeof(struct cpl_tx_pkt_core)) / sizeof(__be64);\n\treturn flits;\n}\n\n \nstatic void write_sgl(const struct sk_buff *skb, struct sge_txq *tq,\n\t\t      struct ulptx_sgl *sgl, u64 *end, unsigned int start,\n\t\t      const dma_addr_t *addr)\n{\n\tunsigned int i, len;\n\tstruct ulptx_sge_pair *to;\n\tconst struct skb_shared_info *si = skb_shinfo(skb);\n\tunsigned int nfrags = si->nr_frags;\n\tstruct ulptx_sge_pair buf[MAX_SKB_FRAGS / 2 + 1];\n\n\tlen = skb_headlen(skb) - start;\n\tif (likely(len)) {\n\t\tsgl->len0 = htonl(len);\n\t\tsgl->addr0 = cpu_to_be64(addr[0] + start);\n\t\tnfrags++;\n\t} else {\n\t\tsgl->len0 = htonl(skb_frag_size(&si->frags[0]));\n\t\tsgl->addr0 = cpu_to_be64(addr[1]);\n\t}\n\n\tsgl->cmd_nsge = htonl(ULPTX_CMD_V(ULP_TX_SC_DSGL) |\n\t\t\t      ULPTX_NSGE_V(nfrags));\n\tif (likely(--nfrags == 0))\n\t\treturn;\n\t \n\tto = (u8 *)end > (u8 *)tq->stat ? buf : sgl->sge;\n\n\tfor (i = (nfrags != si->nr_frags); nfrags >= 2; nfrags -= 2, to++) {\n\t\tto->len[0] = cpu_to_be32(skb_frag_size(&si->frags[i]));\n\t\tto->len[1] = cpu_to_be32(skb_frag_size(&si->frags[++i]));\n\t\tto->addr[0] = cpu_to_be64(addr[i]);\n\t\tto->addr[1] = cpu_to_be64(addr[++i]);\n\t}\n\tif (nfrags) {\n\t\tto->len[0] = cpu_to_be32(skb_frag_size(&si->frags[i]));\n\t\tto->len[1] = cpu_to_be32(0);\n\t\tto->addr[0] = cpu_to_be64(addr[i + 1]);\n\t}\n\tif (unlikely((u8 *)end > (u8 *)tq->stat)) {\n\t\tunsigned int part0 = (u8 *)tq->stat - (u8 *)sgl->sge, part1;\n\n\t\tif (likely(part0))\n\t\t\tmemcpy(sgl->sge, buf, part0);\n\t\tpart1 = (u8 *)end - (u8 *)tq->stat;\n\t\tmemcpy(tq->desc, (u8 *)buf + part0, part1);\n\t\tend = (void *)tq->desc + part1;\n\t}\n\tif ((uintptr_t)end & 8)            \n\t\t*end = 0;\n}\n\n \nstatic inline void ring_tx_db(struct adapter *adapter, struct sge_txq *tq,\n\t\t\t      int n)\n{\n\t \n\twmb();\n\n\t \n\tif (unlikely(tq->bar2_addr == NULL)) {\n\t\tu32 val = PIDX_V(n);\n\n\t\tt4_write_reg(adapter, T4VF_SGE_BASE_ADDR + SGE_VF_KDOORBELL,\n\t\t\t     QID_V(tq->cntxt_id) | val);\n\t} else {\n\t\tu32 val = PIDX_T5_V(n);\n\n\t\t \n\t\tWARN_ON(val & DBPRIO_F);\n\n\t\t \n\t\tif (n == 1 && tq->bar2_qid == 0) {\n\t\t\tunsigned int index = (tq->pidx\n\t\t\t\t\t      ? (tq->pidx - 1)\n\t\t\t\t\t      : (tq->size - 1));\n\t\t\t__be64 *src = (__be64 *)&tq->desc[index];\n\t\t\t__be64 __iomem *dst = (__be64 __iomem *)(tq->bar2_addr +\n\t\t\t\t\t\t\t SGE_UDB_WCDOORBELL);\n\t\t\tunsigned int count = EQ_UNIT / sizeof(__be64);\n\n\t\t\t \n\t\t\twhile (count) {\n\t\t\t\t \n\t\t\t\twriteq((__force u64)*src, dst);\n\t\t\t\tsrc++;\n\t\t\t\tdst++;\n\t\t\t\tcount--;\n\t\t\t}\n\t\t} else\n\t\t\twritel(val | QID_V(tq->bar2_qid),\n\t\t\t       tq->bar2_addr + SGE_UDB_KDOORBELL);\n\n\t\t \n\t\twmb();\n\t}\n}\n\n \nstatic void inline_tx_skb(const struct sk_buff *skb, const struct sge_txq *tq,\n\t\t\t  void *pos)\n{\n\tu64 *p;\n\tint left = (void *)tq->stat - pos;\n\n\tif (likely(skb->len <= left)) {\n\t\tif (likely(!skb->data_len))\n\t\t\tskb_copy_from_linear_data(skb, pos, skb->len);\n\t\telse\n\t\t\tskb_copy_bits(skb, 0, pos, skb->len);\n\t\tpos += skb->len;\n\t} else {\n\t\tskb_copy_bits(skb, 0, pos, left);\n\t\tskb_copy_bits(skb, left, tq->desc, skb->len - left);\n\t\tpos = (void *)tq->desc + (skb->len - left);\n\t}\n\n\t \n\tp = PTR_ALIGN(pos, 8);\n\tif ((uintptr_t)p & 8)\n\t\t*p = 0;\n}\n\n \nstatic u64 hwcsum(enum chip_type chip, const struct sk_buff *skb)\n{\n\tint csum_type;\n\tconst struct iphdr *iph = ip_hdr(skb);\n\n\tif (iph->version == 4) {\n\t\tif (iph->protocol == IPPROTO_TCP)\n\t\t\tcsum_type = TX_CSUM_TCPIP;\n\t\telse if (iph->protocol == IPPROTO_UDP)\n\t\t\tcsum_type = TX_CSUM_UDPIP;\n\t\telse {\nnocsum:\n\t\t\t \n\t\t\treturn TXPKT_L4CSUM_DIS_F;\n\t\t}\n\t} else {\n\t\t \n\t\tconst struct ipv6hdr *ip6h = (const struct ipv6hdr *)iph;\n\n\t\tif (ip6h->nexthdr == IPPROTO_TCP)\n\t\t\tcsum_type = TX_CSUM_TCPIP6;\n\t\telse if (ip6h->nexthdr == IPPROTO_UDP)\n\t\t\tcsum_type = TX_CSUM_UDPIP6;\n\t\telse\n\t\t\tgoto nocsum;\n\t}\n\n\tif (likely(csum_type >= TX_CSUM_TCPIP)) {\n\t\tu64 hdr_len = TXPKT_IPHDR_LEN_V(skb_network_header_len(skb));\n\t\tint eth_hdr_len = skb_network_offset(skb) - ETH_HLEN;\n\n\t\tif (chip <= CHELSIO_T5)\n\t\t\thdr_len |= TXPKT_ETHHDR_LEN_V(eth_hdr_len);\n\t\telse\n\t\t\thdr_len |= T6_TXPKT_ETHHDR_LEN_V(eth_hdr_len);\n\t\treturn TXPKT_CSUM_TYPE_V(csum_type) | hdr_len;\n\t} else {\n\t\tint start = skb_transport_offset(skb);\n\n\t\treturn TXPKT_CSUM_TYPE_V(csum_type) |\n\t\t\tTXPKT_CSUM_START_V(start) |\n\t\t\tTXPKT_CSUM_LOC_V(start + skb->csum_offset);\n\t}\n}\n\n \nstatic void txq_stop(struct sge_eth_txq *txq)\n{\n\tnetif_tx_stop_queue(txq->txq);\n\ttxq->q.stops++;\n}\n\n \nstatic inline void txq_advance(struct sge_txq *tq, unsigned int n)\n{\n\ttq->in_use += n;\n\ttq->pidx += n;\n\tif (tq->pidx >= tq->size)\n\t\ttq->pidx -= tq->size;\n}\n\n \nnetdev_tx_t t4vf_eth_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tu32 wr_mid;\n\tu64 cntrl, *end;\n\tint qidx, credits, max_pkt_len;\n\tunsigned int flits, ndesc;\n\tstruct adapter *adapter;\n\tstruct sge_eth_txq *txq;\n\tconst struct port_info *pi;\n\tstruct fw_eth_tx_pkt_vm_wr *wr;\n\tstruct cpl_tx_pkt_core *cpl;\n\tconst struct skb_shared_info *ssi;\n\tdma_addr_t addr[MAX_SKB_FRAGS + 1];\n\tconst size_t fw_hdr_copy_len = sizeof(wr->firmware);\n\n\t \n\tif (unlikely(skb->len < fw_hdr_copy_len))\n\t\tgoto out_free;\n\n\t \n\tmax_pkt_len = ETH_HLEN + dev->mtu;\n\tif (skb_vlan_tagged(skb))\n\t\tmax_pkt_len += VLAN_HLEN;\n\tif (!skb_shinfo(skb)->gso_size && (unlikely(skb->len > max_pkt_len)))\n\t\tgoto out_free;\n\n\t \n\tpi = netdev_priv(dev);\n\tadapter = pi->adapter;\n\tqidx = skb_get_queue_mapping(skb);\n\tBUG_ON(qidx >= pi->nqsets);\n\ttxq = &adapter->sge.ethtxq[pi->first_qset + qidx];\n\n\tif (pi->vlan_id && !skb_vlan_tag_present(skb))\n\t\t__vlan_hwaccel_put_tag(skb, cpu_to_be16(ETH_P_8021Q),\n\t\t\t\t       pi->vlan_id);\n\n\t \n\treclaim_completed_tx(adapter, &txq->q, true);\n\n\t \n\tflits = calc_tx_flits(skb);\n\tndesc = flits_to_desc(flits);\n\tcredits = txq_avail(&txq->q) - ndesc;\n\n\tif (unlikely(credits < 0)) {\n\t\t \n\t\ttxq_stop(txq);\n\t\tdev_err(adapter->pdev_dev,\n\t\t\t\"%s: TX ring %u full while queue awake!\\n\",\n\t\t\tdev->name, qidx);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (!is_eth_imm(skb) &&\n\t    unlikely(map_skb(adapter->pdev_dev, skb, addr) < 0)) {\n\t\t \n\t\ttxq->mapping_err++;\n\t\tgoto out_free;\n\t}\n\n\twr_mid = FW_WR_LEN16_V(DIV_ROUND_UP(flits, 2));\n\tif (unlikely(credits < ETHTXQ_STOP_THRES)) {\n\t\t \n\t\ttxq_stop(txq);\n\t\twr_mid |= FW_WR_EQUEQ_F | FW_WR_EQUIQ_F;\n\t}\n\n\t \n\tBUG_ON(DIV_ROUND_UP(ETHTXQ_MAX_HDR, TXD_PER_EQ_UNIT) > 1);\n\twr = (void *)&txq->q.desc[txq->q.pidx];\n\twr->equiq_to_len16 = cpu_to_be32(wr_mid);\n\twr->r3[0] = cpu_to_be32(0);\n\twr->r3[1] = cpu_to_be32(0);\n\tskb_copy_from_linear_data(skb, &wr->firmware, fw_hdr_copy_len);\n\tend = (u64 *)wr + flits;\n\n\t \n\tssi = skb_shinfo(skb);\n\tif (ssi->gso_size) {\n\t\tstruct cpl_tx_pkt_lso_core *lso = (void *)(wr + 1);\n\t\tbool v6 = (ssi->gso_type & SKB_GSO_TCPV6) != 0;\n\t\tint l3hdr_len = skb_network_header_len(skb);\n\t\tint eth_xtra_len = skb_network_offset(skb) - ETH_HLEN;\n\n\t\twr->op_immdlen =\n\t\t\tcpu_to_be32(FW_WR_OP_V(FW_ETH_TX_PKT_VM_WR) |\n\t\t\t\t    FW_WR_IMMDLEN_V(sizeof(*lso) +\n\t\t\t\t\t\t    sizeof(*cpl)));\n\t\t \n\t\tlso->lso_ctrl =\n\t\t\tcpu_to_be32(LSO_OPCODE_V(CPL_TX_PKT_LSO) |\n\t\t\t\t    LSO_FIRST_SLICE_F |\n\t\t\t\t    LSO_LAST_SLICE_F |\n\t\t\t\t    LSO_IPV6_V(v6) |\n\t\t\t\t    LSO_ETHHDR_LEN_V(eth_xtra_len / 4) |\n\t\t\t\t    LSO_IPHDR_LEN_V(l3hdr_len / 4) |\n\t\t\t\t    LSO_TCPHDR_LEN_V(tcp_hdr(skb)->doff));\n\t\tlso->ipid_ofst = cpu_to_be16(0);\n\t\tlso->mss = cpu_to_be16(ssi->gso_size);\n\t\tlso->seqno_offset = cpu_to_be32(0);\n\t\tif (is_t4(adapter->params.chip))\n\t\t\tlso->len = cpu_to_be32(skb->len);\n\t\telse\n\t\t\tlso->len = cpu_to_be32(LSO_T5_XFER_SIZE_V(skb->len));\n\n\t\t \n\t\tcpl = (void *)(lso + 1);\n\n\t\tif (CHELSIO_CHIP_VERSION(adapter->params.chip) <= CHELSIO_T5)\n\t\t\tcntrl = TXPKT_ETHHDR_LEN_V(eth_xtra_len);\n\t\telse\n\t\t\tcntrl = T6_TXPKT_ETHHDR_LEN_V(eth_xtra_len);\n\n\t\tcntrl |= TXPKT_CSUM_TYPE_V(v6 ?\n\t\t\t\t\t   TX_CSUM_TCPIP6 : TX_CSUM_TCPIP) |\n\t\t\t TXPKT_IPHDR_LEN_V(l3hdr_len);\n\t\ttxq->tso++;\n\t\ttxq->tx_cso += ssi->gso_segs;\n\t} else {\n\t\tint len;\n\n\t\tlen = is_eth_imm(skb) ? skb->len + sizeof(*cpl) : sizeof(*cpl);\n\t\twr->op_immdlen =\n\t\t\tcpu_to_be32(FW_WR_OP_V(FW_ETH_TX_PKT_VM_WR) |\n\t\t\t\t    FW_WR_IMMDLEN_V(len));\n\n\t\t \n\t\tcpl = (void *)(wr + 1);\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tcntrl = hwcsum(adapter->params.chip, skb) |\n\t\t\t\tTXPKT_IPCSUM_DIS_F;\n\t\t\ttxq->tx_cso++;\n\t\t} else\n\t\t\tcntrl = TXPKT_L4CSUM_DIS_F | TXPKT_IPCSUM_DIS_F;\n\t}\n\n\t \n\tif (skb_vlan_tag_present(skb)) {\n\t\ttxq->vlan_ins++;\n\t\tcntrl |= TXPKT_VLAN_VLD_F | TXPKT_VLAN_V(skb_vlan_tag_get(skb));\n\t}\n\n\t \n\tcpl->ctrl0 = cpu_to_be32(TXPKT_OPCODE_V(CPL_TX_PKT_XT) |\n\t\t\t\t TXPKT_INTF_V(pi->port_id) |\n\t\t\t\t TXPKT_PF_V(0));\n\tcpl->pack = cpu_to_be16(0);\n\tcpl->len = cpu_to_be16(skb->len);\n\tcpl->ctrl1 = cpu_to_be64(cntrl);\n\n#ifdef T4_TRACE\n\tT4_TRACE5(adapter->tb[txq->q.cntxt_id & 7],\n\t\t  \"eth_xmit: ndesc %u, credits %u, pidx %u, len %u, frags %u\",\n\t\t  ndesc, credits, txq->q.pidx, skb->len, ssi->nr_frags);\n#endif\n\n\t \n\tif (is_eth_imm(skb)) {\n\t\t \n\t\tinline_tx_skb(skb, &txq->q, cpl + 1);\n\t\tdev_consume_skb_any(skb);\n\t} else {\n\t\t \n\t\tstruct ulptx_sgl *sgl = (struct ulptx_sgl *)(cpl + 1);\n\t\tstruct sge_txq *tq = &txq->q;\n\t\tint last_desc;\n\n\t\t \n\t\tif (unlikely((void *)sgl == (void *)tq->stat)) {\n\t\t\tsgl = (void *)tq->desc;\n\t\t\tend = ((void *)tq->desc + ((void *)end - (void *)tq->stat));\n\t\t}\n\n\t\twrite_sgl(skb, tq, sgl, end, 0, addr);\n\t\tskb_orphan(skb);\n\n\t\tlast_desc = tq->pidx + ndesc - 1;\n\t\tif (last_desc >= tq->size)\n\t\t\tlast_desc -= tq->size;\n\t\ttq->sdesc[last_desc].skb = skb;\n\t\ttq->sdesc[last_desc].sgl = sgl;\n\t}\n\n\t \n\ttxq_advance(&txq->q, ndesc);\n\tnetif_trans_update(dev);\n\tring_tx_db(adapter, &txq->q, ndesc);\n\treturn NETDEV_TX_OK;\n\nout_free:\n\t \n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic inline void copy_frags(struct sk_buff *skb,\n\t\t\t      const struct pkt_gl *gl,\n\t\t\t      unsigned int offset)\n{\n\tint i;\n\n\t \n\t__skb_fill_page_desc(skb, 0, gl->frags[0].page,\n\t\t\t     gl->frags[0].offset + offset,\n\t\t\t     gl->frags[0].size - offset);\n\tskb_shinfo(skb)->nr_frags = gl->nfrags;\n\tfor (i = 1; i < gl->nfrags; i++)\n\t\t__skb_fill_page_desc(skb, i, gl->frags[i].page,\n\t\t\t\t     gl->frags[i].offset,\n\t\t\t\t     gl->frags[i].size);\n\n\t \n\tget_page(gl->frags[gl->nfrags - 1].page);\n}\n\n \nstatic struct sk_buff *t4vf_pktgl_to_skb(const struct pkt_gl *gl,\n\t\t\t\t\t unsigned int skb_len,\n\t\t\t\t\t unsigned int pull_len)\n{\n\tstruct sk_buff *skb;\n\n\t \n\tif (gl->tot_len <= RX_COPY_THRES) {\n\t\t \n\t\tskb = alloc_skb(gl->tot_len, GFP_ATOMIC);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t\t__skb_put(skb, gl->tot_len);\n\t\tskb_copy_to_linear_data(skb, gl->va, gl->tot_len);\n\t} else {\n\t\tskb = alloc_skb(skb_len, GFP_ATOMIC);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t\t__skb_put(skb, pull_len);\n\t\tskb_copy_to_linear_data(skb, gl->va, pull_len);\n\n\t\tcopy_frags(skb, gl, pull_len);\n\t\tskb->len = gl->tot_len;\n\t\tskb->data_len = skb->len - pull_len;\n\t\tskb->truesize += skb->data_len;\n\t}\n\nout:\n\treturn skb;\n}\n\n \nstatic void t4vf_pktgl_free(const struct pkt_gl *gl)\n{\n\tint frag;\n\n\tfrag = gl->nfrags - 1;\n\twhile (frag--)\n\t\tput_page(gl->frags[frag].page);\n}\n\n \nstatic void do_gro(struct sge_eth_rxq *rxq, const struct pkt_gl *gl,\n\t\t   const struct cpl_rx_pkt *pkt)\n{\n\tstruct adapter *adapter = rxq->rspq.adapter;\n\tstruct sge *s = &adapter->sge;\n\tstruct port_info *pi;\n\tint ret;\n\tstruct sk_buff *skb;\n\n\tskb = napi_get_frags(&rxq->rspq.napi);\n\tif (unlikely(!skb)) {\n\t\tt4vf_pktgl_free(gl);\n\t\trxq->stats.rx_drops++;\n\t\treturn;\n\t}\n\n\tcopy_frags(skb, gl, s->pktshift);\n\tskb->len = gl->tot_len - s->pktshift;\n\tskb->data_len = skb->len;\n\tskb->truesize += skb->data_len;\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tskb_record_rx_queue(skb, rxq->rspq.idx);\n\tpi = netdev_priv(skb->dev);\n\n\tif (pkt->vlan_ex && !pi->vlan_id) {\n\t\t__vlan_hwaccel_put_tag(skb, cpu_to_be16(ETH_P_8021Q),\n\t\t\t\t\tbe16_to_cpu(pkt->vlan));\n\t\trxq->stats.vlan_ex++;\n\t}\n\tret = napi_gro_frags(&rxq->rspq.napi);\n\n\tif (ret == GRO_HELD)\n\t\trxq->stats.lro_pkts++;\n\telse if (ret == GRO_MERGED || ret == GRO_MERGED_FREE)\n\t\trxq->stats.lro_merged++;\n\trxq->stats.pkts++;\n\trxq->stats.rx_cso++;\n}\n\n \nint t4vf_ethrx_handler(struct sge_rspq *rspq, const __be64 *rsp,\n\t\t       const struct pkt_gl *gl)\n{\n\tstruct sk_buff *skb;\n\tconst struct cpl_rx_pkt *pkt = (void *)rsp;\n\tbool csum_ok = pkt->csum_calc && !pkt->err_vec &&\n\t\t       (rspq->netdev->features & NETIF_F_RXCSUM);\n\tstruct sge_eth_rxq *rxq = container_of(rspq, struct sge_eth_rxq, rspq);\n\tstruct adapter *adapter = rspq->adapter;\n\tstruct sge *s = &adapter->sge;\n\tstruct port_info *pi;\n\n\t \n\tif ((pkt->l2info & cpu_to_be32(RXF_TCP_F)) &&\n\t    (rspq->netdev->features & NETIF_F_GRO) && csum_ok &&\n\t    !pkt->ip_frag) {\n\t\tdo_gro(rxq, gl, pkt);\n\t\treturn 0;\n\t}\n\n\t \n\tskb = t4vf_pktgl_to_skb(gl, RX_SKB_LEN, RX_PULL_LEN);\n\tif (unlikely(!skb)) {\n\t\tt4vf_pktgl_free(gl);\n\t\trxq->stats.rx_drops++;\n\t\treturn 0;\n\t}\n\t__skb_pull(skb, s->pktshift);\n\tskb->protocol = eth_type_trans(skb, rspq->netdev);\n\tskb_record_rx_queue(skb, rspq->idx);\n\tpi = netdev_priv(skb->dev);\n\trxq->stats.pkts++;\n\n\tif (csum_ok && !pkt->err_vec &&\n\t    (be32_to_cpu(pkt->l2info) & (RXF_UDP_F | RXF_TCP_F))) {\n\t\tif (!pkt->ip_frag) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\trxq->stats.rx_cso++;\n\t\t} else if (pkt->l2info & htonl(RXF_IP_F)) {\n\t\t\t__sum16 c = (__force __sum16)pkt->csum;\n\t\t\tskb->csum = csum_unfold(c);\n\t\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\t\trxq->stats.rx_cso++;\n\t\t}\n\t} else\n\t\tskb_checksum_none_assert(skb);\n\n\tif (pkt->vlan_ex && !pi->vlan_id) {\n\t\trxq->stats.vlan_ex++;\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t       be16_to_cpu(pkt->vlan));\n\t}\n\n\tnetif_receive_skb(skb);\n\n\treturn 0;\n}\n\n \nstatic inline bool is_new_response(const struct rsp_ctrl *rc,\n\t\t\t\t   const struct sge_rspq *rspq)\n{\n\treturn ((rc->type_gen >> RSPD_GEN_S) & 0x1) == rspq->gen;\n}\n\n \nstatic void restore_rx_bufs(const struct pkt_gl *gl, struct sge_fl *fl,\n\t\t\t    int frags)\n{\n\tstruct rx_sw_desc *sdesc;\n\n\twhile (frags--) {\n\t\tif (fl->cidx == 0)\n\t\t\tfl->cidx = fl->size - 1;\n\t\telse\n\t\t\tfl->cidx--;\n\t\tsdesc = &fl->sdesc[fl->cidx];\n\t\tsdesc->page = gl->frags[frags].page;\n\t\tsdesc->dma_addr |= RX_UNMAPPED_BUF;\n\t\tfl->avail++;\n\t}\n}\n\n \nstatic inline void rspq_next(struct sge_rspq *rspq)\n{\n\trspq->cur_desc = (void *)rspq->cur_desc + rspq->iqe_len;\n\tif (unlikely(++rspq->cidx == rspq->size)) {\n\t\trspq->cidx = 0;\n\t\trspq->gen ^= 1;\n\t\trspq->cur_desc = rspq->desc;\n\t}\n}\n\n \nstatic int process_responses(struct sge_rspq *rspq, int budget)\n{\n\tstruct sge_eth_rxq *rxq = container_of(rspq, struct sge_eth_rxq, rspq);\n\tstruct adapter *adapter = rspq->adapter;\n\tstruct sge *s = &adapter->sge;\n\tint budget_left = budget;\n\n\twhile (likely(budget_left)) {\n\t\tint ret, rsp_type;\n\t\tconst struct rsp_ctrl *rc;\n\n\t\trc = (void *)rspq->cur_desc + (rspq->iqe_len - sizeof(*rc));\n\t\tif (!is_new_response(rc, rspq))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\t\trsp_type = RSPD_TYPE_G(rc->type_gen);\n\t\tif (likely(rsp_type == RSPD_TYPE_FLBUF_X)) {\n\t\t\tstruct page_frag *fp;\n\t\t\tstruct pkt_gl gl;\n\t\t\tconst struct rx_sw_desc *sdesc;\n\t\t\tu32 bufsz, frag;\n\t\t\tu32 len = be32_to_cpu(rc->pldbuflen_qid);\n\n\t\t\t \n\t\t\tif (len & RSPD_NEWBUF_F) {\n\t\t\t\t \n\t\t\t\tif (likely(rspq->offset > 0)) {\n\t\t\t\t\tfree_rx_bufs(rspq->adapter, &rxq->fl,\n\t\t\t\t\t\t     1);\n\t\t\t\t\trspq->offset = 0;\n\t\t\t\t}\n\t\t\t\tlen = RSPD_LEN_G(len);\n\t\t\t}\n\t\t\tgl.tot_len = len;\n\n\t\t\t \n\t\t\tfor (frag = 0, fp = gl.frags;  ; frag++, fp++) {\n\t\t\t\tBUG_ON(frag >= MAX_SKB_FRAGS);\n\t\t\t\tBUG_ON(rxq->fl.avail == 0);\n\t\t\t\tsdesc = &rxq->fl.sdesc[rxq->fl.cidx];\n\t\t\t\tbufsz = get_buf_size(adapter, sdesc);\n\t\t\t\tfp->page = sdesc->page;\n\t\t\t\tfp->offset = rspq->offset;\n\t\t\t\tfp->size = min(bufsz, len);\n\t\t\t\tlen -= fp->size;\n\t\t\t\tif (!len)\n\t\t\t\t\tbreak;\n\t\t\t\tunmap_rx_buf(rspq->adapter, &rxq->fl);\n\t\t\t}\n\t\t\tgl.nfrags = frag+1;\n\n\t\t\t \n\t\t\tdma_sync_single_for_cpu(rspq->adapter->pdev_dev,\n\t\t\t\t\t\tget_buf_addr(sdesc),\n\t\t\t\t\t\tfp->size, DMA_FROM_DEVICE);\n\t\t\tgl.va = (page_address(gl.frags[0].page) +\n\t\t\t\t gl.frags[0].offset);\n\t\t\tprefetch(gl.va);\n\n\t\t\t \n\t\t\tret = rspq->handler(rspq, rspq->cur_desc, &gl);\n\t\t\tif (likely(ret == 0))\n\t\t\t\trspq->offset += ALIGN(fp->size, s->fl_align);\n\t\t\telse\n\t\t\t\trestore_rx_bufs(&gl, &rxq->fl, frag);\n\t\t} else if (likely(rsp_type == RSPD_TYPE_CPL_X)) {\n\t\t\tret = rspq->handler(rspq, rspq->cur_desc, NULL);\n\t\t} else {\n\t\t\tWARN_ON(rsp_type > RSPD_TYPE_CPL_X);\n\t\t\tret = 0;\n\t\t}\n\n\t\tif (unlikely(ret)) {\n\t\t\t \n\t\t\tconst int NOMEM_TIMER_IDX = SGE_NTIMERS-1;\n\t\t\trspq->next_intr_params =\n\t\t\t\tQINTR_TIMER_IDX_V(NOMEM_TIMER_IDX);\n\t\t\tbreak;\n\t\t}\n\n\t\trspq_next(rspq);\n\t\tbudget_left--;\n\t}\n\n\t \n\tif (rspq->offset >= 0 &&\n\t    fl_cap(&rxq->fl) - rxq->fl.avail >= 2*FL_PER_EQ_UNIT)\n\t\t__refill_fl(rspq->adapter, &rxq->fl);\n\treturn budget - budget_left;\n}\n\n \nstatic int napi_rx_handler(struct napi_struct *napi, int budget)\n{\n\tunsigned int intr_params;\n\tstruct sge_rspq *rspq = container_of(napi, struct sge_rspq, napi);\n\tint work_done = process_responses(rspq, budget);\n\tu32 val;\n\n\tif (likely(work_done < budget)) {\n\t\tnapi_complete_done(napi, work_done);\n\t\tintr_params = rspq->next_intr_params;\n\t\trspq->next_intr_params = rspq->intr_params;\n\t} else\n\t\tintr_params = QINTR_TIMER_IDX_V(SGE_TIMER_UPD_CIDX);\n\n\tif (unlikely(work_done == 0))\n\t\trspq->unhandled_irqs++;\n\n\tval = CIDXINC_V(work_done) | SEINTARM_V(intr_params);\n\t \n\tif (unlikely(!rspq->bar2_addr)) {\n\t\tt4_write_reg(rspq->adapter,\n\t\t\t     T4VF_SGE_BASE_ADDR + SGE_VF_GTS,\n\t\t\t     val | INGRESSQID_V((u32)rspq->cntxt_id));\n\t} else {\n\t\twritel(val | INGRESSQID_V(rspq->bar2_qid),\n\t\t       rspq->bar2_addr + SGE_UDB_GTS);\n\t\twmb();\n\t}\n\treturn work_done;\n}\n\n \nirqreturn_t t4vf_sge_intr_msix(int irq, void *cookie)\n{\n\tstruct sge_rspq *rspq = cookie;\n\n\tnapi_schedule(&rspq->napi);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic unsigned int process_intrq(struct adapter *adapter)\n{\n\tstruct sge *s = &adapter->sge;\n\tstruct sge_rspq *intrq = &s->intrq;\n\tunsigned int work_done;\n\tu32 val;\n\n\tspin_lock(&adapter->sge.intrq_lock);\n\tfor (work_done = 0; ; work_done++) {\n\t\tconst struct rsp_ctrl *rc;\n\t\tunsigned int qid, iq_idx;\n\t\tstruct sge_rspq *rspq;\n\n\t\t \n\t\trc = (void *)intrq->cur_desc + (intrq->iqe_len - sizeof(*rc));\n\t\tif (!is_new_response(rc, intrq))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\t\tif (unlikely(RSPD_TYPE_G(rc->type_gen) != RSPD_TYPE_INTR_X)) {\n\t\t\tdev_err(adapter->pdev_dev,\n\t\t\t\t\"Unexpected INTRQ response type %d\\n\",\n\t\t\t\tRSPD_TYPE_G(rc->type_gen));\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tqid = RSPD_QID_G(be32_to_cpu(rc->pldbuflen_qid));\n\t\tiq_idx = IQ_IDX(s, qid);\n\t\tif (unlikely(iq_idx >= MAX_INGQ)) {\n\t\t\tdev_err(adapter->pdev_dev,\n\t\t\t\t\"Ingress QID %d out of range\\n\", qid);\n\t\t\tcontinue;\n\t\t}\n\t\trspq = s->ingr_map[iq_idx];\n\t\tif (unlikely(rspq == NULL)) {\n\t\t\tdev_err(adapter->pdev_dev,\n\t\t\t\t\"Ingress QID %d RSPQ=NULL\\n\", qid);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(rspq->abs_id != qid)) {\n\t\t\tdev_err(adapter->pdev_dev,\n\t\t\t\t\"Ingress QID %d refers to RSPQ %d\\n\",\n\t\t\t\tqid, rspq->abs_id);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tnapi_schedule(&rspq->napi);\n\t\trspq_next(intrq);\n\t}\n\n\tval = CIDXINC_V(work_done) | SEINTARM_V(intrq->intr_params);\n\t \n\tif (unlikely(!intrq->bar2_addr)) {\n\t\tt4_write_reg(adapter, T4VF_SGE_BASE_ADDR + SGE_VF_GTS,\n\t\t\t     val | INGRESSQID_V(intrq->cntxt_id));\n\t} else {\n\t\twritel(val | INGRESSQID_V(intrq->bar2_qid),\n\t\t       intrq->bar2_addr + SGE_UDB_GTS);\n\t\twmb();\n\t}\n\n\tspin_unlock(&adapter->sge.intrq_lock);\n\n\treturn work_done;\n}\n\n \nstatic irqreturn_t t4vf_intr_msi(int irq, void *cookie)\n{\n\tstruct adapter *adapter = cookie;\n\n\tprocess_intrq(adapter);\n\treturn IRQ_HANDLED;\n}\n\n \nirq_handler_t t4vf_intr_handler(struct adapter *adapter)\n{\n\tBUG_ON((adapter->flags &\n\t       (CXGB4VF_USING_MSIX | CXGB4VF_USING_MSI)) == 0);\n\tif (adapter->flags & CXGB4VF_USING_MSIX)\n\t\treturn t4vf_sge_intr_msix;\n\telse\n\t\treturn t4vf_intr_msi;\n}\n\n \nstatic void sge_rx_timer_cb(struct timer_list *t)\n{\n\tstruct adapter *adapter = from_timer(adapter, t, sge.rx_timer);\n\tstruct sge *s = &adapter->sge;\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(s->starving_fl); i++) {\n\t\tunsigned long m;\n\n\t\tfor (m = s->starving_fl[i]; m; m &= m - 1) {\n\t\t\tunsigned int id = __ffs(m) + i * BITS_PER_LONG;\n\t\t\tstruct sge_fl *fl = s->egr_map[id];\n\n\t\t\tclear_bit(id, s->starving_fl);\n\t\t\tsmp_mb__after_atomic();\n\n\t\t\t \n\t\t\tif (fl_starving(adapter, fl)) {\n\t\t\t\tstruct sge_eth_rxq *rxq;\n\n\t\t\t\trxq = container_of(fl, struct sge_eth_rxq, fl);\n\t\t\t\tif (napi_reschedule(&rxq->rspq.napi))\n\t\t\t\t\tfl->starving++;\n\t\t\t\telse\n\t\t\t\t\tset_bit(id, s->starving_fl);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tmod_timer(&s->rx_timer, jiffies + RX_QCHECK_PERIOD);\n}\n\n \nstatic void sge_tx_timer_cb(struct timer_list *t)\n{\n\tstruct adapter *adapter = from_timer(adapter, t, sge.tx_timer);\n\tstruct sge *s = &adapter->sge;\n\tunsigned int i, budget;\n\n\tbudget = MAX_TIMER_TX_RECLAIM;\n\ti = s->ethtxq_rover;\n\tdo {\n\t\tstruct sge_eth_txq *txq = &s->ethtxq[i];\n\n\t\tif (reclaimable(&txq->q) && __netif_tx_trylock(txq->txq)) {\n\t\t\tint avail = reclaimable(&txq->q);\n\n\t\t\tif (avail > budget)\n\t\t\t\tavail = budget;\n\n\t\t\tfree_tx_desc(adapter, &txq->q, avail, true);\n\t\t\ttxq->q.in_use -= avail;\n\t\t\t__netif_tx_unlock(txq->txq);\n\n\t\t\tbudget -= avail;\n\t\t\tif (!budget)\n\t\t\t\tbreak;\n\t\t}\n\n\t\ti++;\n\t\tif (i >= s->ethqsets)\n\t\t\ti = 0;\n\t} while (i != s->ethtxq_rover);\n\ts->ethtxq_rover = i;\n\n\t \n\tmod_timer(&s->tx_timer, jiffies + (budget ? TX_QCHECK_PERIOD : 2));\n}\n\n \nstatic void __iomem *bar2_address(struct adapter *adapter,\n\t\t\t\t  unsigned int qid,\n\t\t\t\t  enum t4_bar2_qtype qtype,\n\t\t\t\t  unsigned int *pbar2_qid)\n{\n\tu64 bar2_qoffset;\n\tint ret;\n\n\tret = t4vf_bar2_sge_qregs(adapter, qid, qtype,\n\t\t\t\t  &bar2_qoffset, pbar2_qid);\n\tif (ret)\n\t\treturn NULL;\n\n\treturn adapter->bar2 + bar2_qoffset;\n}\n\n \nint t4vf_sge_alloc_rxq(struct adapter *adapter, struct sge_rspq *rspq,\n\t\t       bool iqasynch, struct net_device *dev,\n\t\t       int intr_dest,\n\t\t       struct sge_fl *fl, rspq_handler_t hnd)\n{\n\tstruct sge *s = &adapter->sge;\n\tstruct port_info *pi = netdev_priv(dev);\n\tstruct fw_iq_cmd cmd, rpl;\n\tint ret, iqandst, flsz = 0;\n\tint relaxed = !(adapter->flags & CXGB4VF_ROOT_NO_RELAXED_ORDERING);\n\n\t \n\tif ((adapter->flags & CXGB4VF_USING_MSI) &&\n\t    rspq != &adapter->sge.intrq) {\n\t\tiqandst = SGE_INTRDST_IQ;\n\t\tintr_dest = adapter->sge.intrq.abs_id;\n\t} else\n\t\tiqandst = SGE_INTRDST_PCI;\n\n\t \n\trspq->size = roundup(rspq->size, 16);\n\trspq->desc = alloc_ring(adapter->pdev_dev, rspq->size, rspq->iqe_len,\n\t\t\t\t0, &rspq->phys_addr, NULL, 0);\n\tif (!rspq->desc)\n\t\treturn -ENOMEM;\n\n\t \n\tmemset(&cmd, 0, sizeof(cmd));\n\tcmd.op_to_vfn = cpu_to_be32(FW_CMD_OP_V(FW_IQ_CMD) |\n\t\t\t\t    FW_CMD_REQUEST_F |\n\t\t\t\t    FW_CMD_WRITE_F |\n\t\t\t\t    FW_CMD_EXEC_F);\n\tcmd.alloc_to_len16 = cpu_to_be32(FW_IQ_CMD_ALLOC_F |\n\t\t\t\t\t FW_IQ_CMD_IQSTART_F |\n\t\t\t\t\t FW_LEN16(cmd));\n\tcmd.type_to_iqandstindex =\n\t\tcpu_to_be32(FW_IQ_CMD_TYPE_V(FW_IQ_TYPE_FL_INT_CAP) |\n\t\t\t    FW_IQ_CMD_IQASYNCH_V(iqasynch) |\n\t\t\t    FW_IQ_CMD_VIID_V(pi->viid) |\n\t\t\t    FW_IQ_CMD_IQANDST_V(iqandst) |\n\t\t\t    FW_IQ_CMD_IQANUS_V(1) |\n\t\t\t    FW_IQ_CMD_IQANUD_V(SGE_UPDATEDEL_INTR) |\n\t\t\t    FW_IQ_CMD_IQANDSTINDEX_V(intr_dest));\n\tcmd.iqdroprss_to_iqesize =\n\t\tcpu_to_be16(FW_IQ_CMD_IQPCIECH_V(pi->port_id) |\n\t\t\t    FW_IQ_CMD_IQGTSMODE_F |\n\t\t\t    FW_IQ_CMD_IQINTCNTTHRESH_V(rspq->pktcnt_idx) |\n\t\t\t    FW_IQ_CMD_IQESIZE_V(ilog2(rspq->iqe_len) - 4));\n\tcmd.iqsize = cpu_to_be16(rspq->size);\n\tcmd.iqaddr = cpu_to_be64(rspq->phys_addr);\n\n\tif (fl) {\n\t\tunsigned int chip_ver =\n\t\t\tCHELSIO_CHIP_VERSION(adapter->params.chip);\n\t\t \n\t\tif (fl->size < s->fl_starve_thres - 1 + 2 * FL_PER_EQ_UNIT)\n\t\t\tfl->size = s->fl_starve_thres - 1 + 2 * FL_PER_EQ_UNIT;\n\t\tfl->size = roundup(fl->size, FL_PER_EQ_UNIT);\n\t\tfl->desc = alloc_ring(adapter->pdev_dev, fl->size,\n\t\t\t\t      sizeof(__be64), sizeof(struct rx_sw_desc),\n\t\t\t\t      &fl->addr, &fl->sdesc, s->stat_len);\n\t\tif (!fl->desc) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\t \n\t\tflsz = (fl->size / FL_PER_EQ_UNIT +\n\t\t\ts->stat_len / EQ_UNIT);\n\n\t\t \n\t\tcmd.iqns_to_fl0congen =\n\t\t\tcpu_to_be32(\n\t\t\t\tFW_IQ_CMD_FL0HOSTFCMODE_V(SGE_HOSTFCMODE_NONE) |\n\t\t\t\tFW_IQ_CMD_FL0PACKEN_F |\n\t\t\t\tFW_IQ_CMD_FL0FETCHRO_V(relaxed) |\n\t\t\t\tFW_IQ_CMD_FL0DATARO_V(relaxed) |\n\t\t\t\tFW_IQ_CMD_FL0PADEN_F);\n\n\t\t \n\t\tcmd.fl0dcaen_to_fl0cidxfthresh =\n\t\t\tcpu_to_be16(\n\t\t\t\tFW_IQ_CMD_FL0FBMIN_V(chip_ver <= CHELSIO_T5\n\t\t\t\t\t\t     ? FETCHBURSTMIN_128B_X\n\t\t\t\t\t\t     : FETCHBURSTMIN_64B_T6_X) |\n\t\t\t\tFW_IQ_CMD_FL0FBMAX_V((chip_ver <= CHELSIO_T5) ?\n\t\t\t\t\t\t     FETCHBURSTMAX_512B_X :\n\t\t\t\t\t\t     FETCHBURSTMAX_256B_X));\n\t\tcmd.fl0size = cpu_to_be16(flsz);\n\t\tcmd.fl0addr = cpu_to_be64(fl->addr);\n\t}\n\n\t \n\tret = t4vf_wr_mbox(adapter, &cmd, sizeof(cmd), &rpl);\n\tif (ret)\n\t\tgoto err;\n\n\tnetif_napi_add(dev, &rspq->napi, napi_rx_handler);\n\trspq->cur_desc = rspq->desc;\n\trspq->cidx = 0;\n\trspq->gen = 1;\n\trspq->next_intr_params = rspq->intr_params;\n\trspq->cntxt_id = be16_to_cpu(rpl.iqid);\n\trspq->bar2_addr = bar2_address(adapter,\n\t\t\t\t       rspq->cntxt_id,\n\t\t\t\t       T4_BAR2_QTYPE_INGRESS,\n\t\t\t\t       &rspq->bar2_qid);\n\trspq->abs_id = be16_to_cpu(rpl.physiqid);\n\trspq->size--;\t\t\t \n\trspq->adapter = adapter;\n\trspq->netdev = dev;\n\trspq->handler = hnd;\n\n\t \n\trspq->offset = fl ? 0 : -1;\n\n\tif (fl) {\n\t\tfl->cntxt_id = be16_to_cpu(rpl.fl0id);\n\t\tfl->avail = 0;\n\t\tfl->pend_cred = 0;\n\t\tfl->pidx = 0;\n\t\tfl->cidx = 0;\n\t\tfl->alloc_failed = 0;\n\t\tfl->large_alloc_failed = 0;\n\t\tfl->starving = 0;\n\n\t\t \n\t\tfl->bar2_addr = bar2_address(adapter,\n\t\t\t\t\t     fl->cntxt_id,\n\t\t\t\t\t     T4_BAR2_QTYPE_EGRESS,\n\t\t\t\t\t     &fl->bar2_qid);\n\n\t\trefill_fl(adapter, fl, fl_cap(fl), GFP_KERNEL);\n\t}\n\n\treturn 0;\n\nerr:\n\t \n\tif (rspq->desc) {\n\t\tdma_free_coherent(adapter->pdev_dev, rspq->size * rspq->iqe_len,\n\t\t\t\t  rspq->desc, rspq->phys_addr);\n\t\trspq->desc = NULL;\n\t}\n\tif (fl && fl->desc) {\n\t\tkfree(fl->sdesc);\n\t\tfl->sdesc = NULL;\n\t\tdma_free_coherent(adapter->pdev_dev, flsz * EQ_UNIT,\n\t\t\t\t  fl->desc, fl->addr);\n\t\tfl->desc = NULL;\n\t}\n\treturn ret;\n}\n\n \nint t4vf_sge_alloc_eth_txq(struct adapter *adapter, struct sge_eth_txq *txq,\n\t\t\t   struct net_device *dev, struct netdev_queue *devq,\n\t\t\t   unsigned int iqid)\n{\n\tunsigned int chip_ver = CHELSIO_CHIP_VERSION(adapter->params.chip);\n\tstruct port_info *pi = netdev_priv(dev);\n\tstruct fw_eq_eth_cmd cmd, rpl;\n\tstruct sge *s = &adapter->sge;\n\tint ret, nentries;\n\n\t \n\tnentries = txq->q.size + s->stat_len / sizeof(struct tx_desc);\n\n\t \n\ttxq->q.desc = alloc_ring(adapter->pdev_dev, txq->q.size,\n\t\t\t\t sizeof(struct tx_desc),\n\t\t\t\t sizeof(struct tx_sw_desc),\n\t\t\t\t &txq->q.phys_addr, &txq->q.sdesc, s->stat_len);\n\tif (!txq->q.desc)\n\t\treturn -ENOMEM;\n\n\t \n\tmemset(&cmd, 0, sizeof(cmd));\n\tcmd.op_to_vfn = cpu_to_be32(FW_CMD_OP_V(FW_EQ_ETH_CMD) |\n\t\t\t\t    FW_CMD_REQUEST_F |\n\t\t\t\t    FW_CMD_WRITE_F |\n\t\t\t\t    FW_CMD_EXEC_F);\n\tcmd.alloc_to_len16 = cpu_to_be32(FW_EQ_ETH_CMD_ALLOC_F |\n\t\t\t\t\t FW_EQ_ETH_CMD_EQSTART_F |\n\t\t\t\t\t FW_LEN16(cmd));\n\tcmd.autoequiqe_to_viid = cpu_to_be32(FW_EQ_ETH_CMD_AUTOEQUEQE_F |\n\t\t\t\t\t     FW_EQ_ETH_CMD_VIID_V(pi->viid));\n\tcmd.fetchszm_to_iqid =\n\t\tcpu_to_be32(FW_EQ_ETH_CMD_HOSTFCMODE_V(SGE_HOSTFCMODE_STPG) |\n\t\t\t    FW_EQ_ETH_CMD_PCIECHN_V(pi->port_id) |\n\t\t\t    FW_EQ_ETH_CMD_IQID_V(iqid));\n\tcmd.dcaen_to_eqsize =\n\t\tcpu_to_be32(FW_EQ_ETH_CMD_FBMIN_V(chip_ver <= CHELSIO_T5\n\t\t\t\t\t\t  ? FETCHBURSTMIN_64B_X\n\t\t\t\t\t\t  : FETCHBURSTMIN_64B_T6_X) |\n\t\t\t    FW_EQ_ETH_CMD_FBMAX_V(FETCHBURSTMAX_512B_X) |\n\t\t\t    FW_EQ_ETH_CMD_CIDXFTHRESH_V(\n\t\t\t\t\t\tCIDXFLUSHTHRESH_32_X) |\n\t\t\t    FW_EQ_ETH_CMD_EQSIZE_V(nentries));\n\tcmd.eqaddr = cpu_to_be64(txq->q.phys_addr);\n\n\t \n\tret = t4vf_wr_mbox(adapter, &cmd, sizeof(cmd), &rpl);\n\tif (ret) {\n\t\t \n\t\tkfree(txq->q.sdesc);\n\t\ttxq->q.sdesc = NULL;\n\t\tdma_free_coherent(adapter->pdev_dev,\n\t\t\t\t  nentries * sizeof(struct tx_desc),\n\t\t\t\t  txq->q.desc, txq->q.phys_addr);\n\t\ttxq->q.desc = NULL;\n\t\treturn ret;\n\t}\n\n\ttxq->q.in_use = 0;\n\ttxq->q.cidx = 0;\n\ttxq->q.pidx = 0;\n\ttxq->q.stat = (void *)&txq->q.desc[txq->q.size];\n\ttxq->q.cntxt_id = FW_EQ_ETH_CMD_EQID_G(be32_to_cpu(rpl.eqid_pkd));\n\ttxq->q.bar2_addr = bar2_address(adapter,\n\t\t\t\t\ttxq->q.cntxt_id,\n\t\t\t\t\tT4_BAR2_QTYPE_EGRESS,\n\t\t\t\t\t&txq->q.bar2_qid);\n\ttxq->q.abs_id =\n\t\tFW_EQ_ETH_CMD_PHYSEQID_G(be32_to_cpu(rpl.physeqid_pkd));\n\ttxq->txq = devq;\n\ttxq->tso = 0;\n\ttxq->tx_cso = 0;\n\ttxq->vlan_ins = 0;\n\ttxq->q.stops = 0;\n\ttxq->q.restarts = 0;\n\ttxq->mapping_err = 0;\n\treturn 0;\n}\n\n \nstatic void free_txq(struct adapter *adapter, struct sge_txq *tq)\n{\n\tstruct sge *s = &adapter->sge;\n\n\tdma_free_coherent(adapter->pdev_dev,\n\t\t\t  tq->size * sizeof(*tq->desc) + s->stat_len,\n\t\t\t  tq->desc, tq->phys_addr);\n\ttq->cntxt_id = 0;\n\ttq->sdesc = NULL;\n\ttq->desc = NULL;\n}\n\n \nstatic void free_rspq_fl(struct adapter *adapter, struct sge_rspq *rspq,\n\t\t\t struct sge_fl *fl)\n{\n\tstruct sge *s = &adapter->sge;\n\tunsigned int flid = fl ? fl->cntxt_id : 0xffff;\n\n\tt4vf_iq_free(adapter, FW_IQ_TYPE_FL_INT_CAP,\n\t\t     rspq->cntxt_id, flid, 0xffff);\n\tdma_free_coherent(adapter->pdev_dev, (rspq->size + 1) * rspq->iqe_len,\n\t\t\t  rspq->desc, rspq->phys_addr);\n\tnetif_napi_del(&rspq->napi);\n\trspq->netdev = NULL;\n\trspq->cntxt_id = 0;\n\trspq->abs_id = 0;\n\trspq->desc = NULL;\n\n\tif (fl) {\n\t\tfree_rx_bufs(adapter, fl, fl->avail);\n\t\tdma_free_coherent(adapter->pdev_dev,\n\t\t\t\t  fl->size * sizeof(*fl->desc) + s->stat_len,\n\t\t\t\t  fl->desc, fl->addr);\n\t\tkfree(fl->sdesc);\n\t\tfl->sdesc = NULL;\n\t\tfl->cntxt_id = 0;\n\t\tfl->desc = NULL;\n\t}\n}\n\n \nvoid t4vf_free_sge_resources(struct adapter *adapter)\n{\n\tstruct sge *s = &adapter->sge;\n\tstruct sge_eth_rxq *rxq = s->ethrxq;\n\tstruct sge_eth_txq *txq = s->ethtxq;\n\tstruct sge_rspq *evtq = &s->fw_evtq;\n\tstruct sge_rspq *intrq = &s->intrq;\n\tint qs;\n\n\tfor (qs = 0; qs < adapter->sge.ethqsets; qs++, rxq++, txq++) {\n\t\tif (rxq->rspq.desc)\n\t\t\tfree_rspq_fl(adapter, &rxq->rspq, &rxq->fl);\n\t\tif (txq->q.desc) {\n\t\t\tt4vf_eth_eq_free(adapter, txq->q.cntxt_id);\n\t\t\tfree_tx_desc(adapter, &txq->q, txq->q.in_use, true);\n\t\t\tkfree(txq->q.sdesc);\n\t\t\tfree_txq(adapter, &txq->q);\n\t\t}\n\t}\n\tif (evtq->desc)\n\t\tfree_rspq_fl(adapter, evtq, NULL);\n\tif (intrq->desc)\n\t\tfree_rspq_fl(adapter, intrq, NULL);\n}\n\n \nvoid t4vf_sge_start(struct adapter *adapter)\n{\n\tadapter->sge.ethtxq_rover = 0;\n\tmod_timer(&adapter->sge.rx_timer, jiffies + RX_QCHECK_PERIOD);\n\tmod_timer(&adapter->sge.tx_timer, jiffies + TX_QCHECK_PERIOD);\n}\n\n \nvoid t4vf_sge_stop(struct adapter *adapter)\n{\n\tstruct sge *s = &adapter->sge;\n\n\tif (s->rx_timer.function)\n\t\tdel_timer_sync(&s->rx_timer);\n\tif (s->tx_timer.function)\n\t\tdel_timer_sync(&s->tx_timer);\n}\n\n \nint t4vf_sge_init(struct adapter *adapter)\n{\n\tstruct sge_params *sge_params = &adapter->params.sge;\n\tu32 fl_small_pg = sge_params->sge_fl_buffer_size[0];\n\tu32 fl_large_pg = sge_params->sge_fl_buffer_size[1];\n\tstruct sge *s = &adapter->sge;\n\n\t \n\n\t \n\tif (fl_large_pg <= fl_small_pg)\n\t\tfl_large_pg = 0;\n\n\t \n\tif (fl_small_pg != PAGE_SIZE ||\n\t    (fl_large_pg & (fl_large_pg - 1)) != 0) {\n\t\tdev_err(adapter->pdev_dev, \"bad SGE FL buffer sizes [%d, %d]\\n\",\n\t\t\tfl_small_pg, fl_large_pg);\n\t\treturn -EINVAL;\n\t}\n\tif ((sge_params->sge_control & RXPKTCPLMODE_F) !=\n\t    RXPKTCPLMODE_V(RXPKTCPLMODE_SPLIT_X)) {\n\t\tdev_err(adapter->pdev_dev, \"bad SGE CPL MODE\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (fl_large_pg)\n\t\ts->fl_pg_order = ilog2(fl_large_pg) - PAGE_SHIFT;\n\ts->stat_len = ((sge_params->sge_control & EGRSTATUSPAGESIZE_F)\n\t\t\t? 128 : 64);\n\ts->pktshift = PKTSHIFT_G(sge_params->sge_control);\n\ts->fl_align = t4vf_fl_pkt_align(adapter);\n\n\t \n\tswitch (CHELSIO_CHIP_VERSION(adapter->params.chip)) {\n\tcase CHELSIO_T4:\n\t\ts->fl_starve_thres =\n\t\t   EGRTHRESHOLD_G(sge_params->sge_congestion_control);\n\t\tbreak;\n\tcase CHELSIO_T5:\n\t\ts->fl_starve_thres =\n\t\t   EGRTHRESHOLDPACKING_G(sge_params->sge_congestion_control);\n\t\tbreak;\n\tcase CHELSIO_T6:\n\tdefault:\n\t\ts->fl_starve_thres =\n\t\t   T6_EGRTHRESHOLDPACKING_G(sge_params->sge_congestion_control);\n\t\tbreak;\n\t}\n\ts->fl_starve_thres = s->fl_starve_thres * 2 + 1;\n\n\t \n\ttimer_setup(&s->rx_timer, sge_rx_timer_cb, 0);\n\ttimer_setup(&s->tx_timer, sge_tx_timer_cb, 0);\n\n\t \n\tspin_lock_init(&s->intrq_lock);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}