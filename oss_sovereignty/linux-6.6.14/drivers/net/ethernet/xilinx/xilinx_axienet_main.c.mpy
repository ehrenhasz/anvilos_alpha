{
  "module_name": "xilinx_axienet_main.c",
  "hash_id": "054ba6dfd5ea5cc3a71840b5ec51cc20140066293e2e95dc03fc5717a8d7f541",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/xilinx/xilinx_axienet_main.c",
  "human_readable_source": "\n \n\n#include <linux/clk.h>\n#include <linux/delay.h>\n#include <linux/etherdevice.h>\n#include <linux/module.h>\n#include <linux/netdevice.h>\n#include <linux/of.h>\n#include <linux/of_mdio.h>\n#include <linux/of_net.h>\n#include <linux/of_irq.h>\n#include <linux/of_address.h>\n#include <linux/platform_device.h>\n#include <linux/skbuff.h>\n#include <linux/math64.h>\n#include <linux/phy.h>\n#include <linux/mii.h>\n#include <linux/ethtool.h>\n\n#include \"xilinx_axienet.h\"\n\n \n#define TX_BD_NUM_DEFAULT\t\t128\n#define RX_BD_NUM_DEFAULT\t\t1024\n#define TX_BD_NUM_MIN\t\t\t(MAX_SKB_FRAGS + 1)\n#define TX_BD_NUM_MAX\t\t\t4096\n#define RX_BD_NUM_MAX\t\t\t4096\n\n \n#define DRIVER_NAME\t\t\"xaxienet\"\n#define DRIVER_DESCRIPTION\t\"Xilinx Axi Ethernet driver\"\n#define DRIVER_VERSION\t\t\"1.00a\"\n\n#define AXIENET_REGS_N\t\t40\n\n \nstatic const struct of_device_id axienet_of_match[] = {\n\t{ .compatible = \"xlnx,axi-ethernet-1.00.a\", },\n\t{ .compatible = \"xlnx,axi-ethernet-1.01.a\", },\n\t{ .compatible = \"xlnx,axi-ethernet-2.01.a\", },\n\t{},\n};\n\nMODULE_DEVICE_TABLE(of, axienet_of_match);\n\n \nstatic struct axienet_option axienet_options[] = {\n\t \n\t{\n\t\t.opt = XAE_OPTION_JUMBO,\n\t\t.reg = XAE_TC_OFFSET,\n\t\t.m_or = XAE_TC_JUM_MASK,\n\t}, {\n\t\t.opt = XAE_OPTION_JUMBO,\n\t\t.reg = XAE_RCW1_OFFSET,\n\t\t.m_or = XAE_RCW1_JUM_MASK,\n\t}, {  \n\t\t.opt = XAE_OPTION_VLAN,\n\t\t.reg = XAE_TC_OFFSET,\n\t\t.m_or = XAE_TC_VLAN_MASK,\n\t}, {\n\t\t.opt = XAE_OPTION_VLAN,\n\t\t.reg = XAE_RCW1_OFFSET,\n\t\t.m_or = XAE_RCW1_VLAN_MASK,\n\t}, {  \n\t\t.opt = XAE_OPTION_FCS_STRIP,\n\t\t.reg = XAE_RCW1_OFFSET,\n\t\t.m_or = XAE_RCW1_FCS_MASK,\n\t}, {  \n\t\t.opt = XAE_OPTION_FCS_INSERT,\n\t\t.reg = XAE_TC_OFFSET,\n\t\t.m_or = XAE_TC_FCS_MASK,\n\t}, {  \n\t\t.opt = XAE_OPTION_LENTYPE_ERR,\n\t\t.reg = XAE_RCW1_OFFSET,\n\t\t.m_or = XAE_RCW1_LT_DIS_MASK,\n\t}, {  \n\t\t.opt = XAE_OPTION_FLOW_CONTROL,\n\t\t.reg = XAE_FCC_OFFSET,\n\t\t.m_or = XAE_FCC_FCRX_MASK,\n\t}, {  \n\t\t.opt = XAE_OPTION_FLOW_CONTROL,\n\t\t.reg = XAE_FCC_OFFSET,\n\t\t.m_or = XAE_FCC_FCTX_MASK,\n\t}, {  \n\t\t.opt = XAE_OPTION_PROMISC,\n\t\t.reg = XAE_FMI_OFFSET,\n\t\t.m_or = XAE_FMI_PM_MASK,\n\t}, {  \n\t\t.opt = XAE_OPTION_TXEN,\n\t\t.reg = XAE_TC_OFFSET,\n\t\t.m_or = XAE_TC_TX_MASK,\n\t}, {  \n\t\t.opt = XAE_OPTION_RXEN,\n\t\t.reg = XAE_RCW1_OFFSET,\n\t\t.m_or = XAE_RCW1_RX_MASK,\n\t},\n\t{}\n};\n\n \nstatic inline u32 axienet_dma_in32(struct axienet_local *lp, off_t reg)\n{\n\treturn ioread32(lp->dma_regs + reg);\n}\n\nstatic void desc_set_phys_addr(struct axienet_local *lp, dma_addr_t addr,\n\t\t\t       struct axidma_bd *desc)\n{\n\tdesc->phys = lower_32_bits(addr);\n\tif (lp->features & XAE_FEATURE_DMA_64BIT)\n\t\tdesc->phys_msb = upper_32_bits(addr);\n}\n\nstatic dma_addr_t desc_get_phys_addr(struct axienet_local *lp,\n\t\t\t\t     struct axidma_bd *desc)\n{\n\tdma_addr_t ret = desc->phys;\n\n\tif (lp->features & XAE_FEATURE_DMA_64BIT)\n\t\tret |= ((dma_addr_t)desc->phys_msb << 16) << 16;\n\n\treturn ret;\n}\n\n \nstatic void axienet_dma_bd_release(struct net_device *ndev)\n{\n\tint i;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\t \n\tdma_free_coherent(lp->dev,\n\t\t\t  sizeof(*lp->tx_bd_v) * lp->tx_bd_num,\n\t\t\t  lp->tx_bd_v,\n\t\t\t  lp->tx_bd_p);\n\n\tif (!lp->rx_bd_v)\n\t\treturn;\n\n\tfor (i = 0; i < lp->rx_bd_num; i++) {\n\t\tdma_addr_t phys;\n\n\t\t \n\t\tif (!lp->rx_bd_v[i].skb)\n\t\t\tbreak;\n\n\t\tdev_kfree_skb(lp->rx_bd_v[i].skb);\n\n\t\t \n\t\tif (lp->rx_bd_v[i].cntrl) {\n\t\t\tphys = desc_get_phys_addr(lp, &lp->rx_bd_v[i]);\n\t\t\tdma_unmap_single(lp->dev, phys,\n\t\t\t\t\t lp->max_frm_size, DMA_FROM_DEVICE);\n\t\t}\n\t}\n\n\tdma_free_coherent(lp->dev,\n\t\t\t  sizeof(*lp->rx_bd_v) * lp->rx_bd_num,\n\t\t\t  lp->rx_bd_v,\n\t\t\t  lp->rx_bd_p);\n}\n\n \nstatic u32 axienet_usec_to_timer(struct axienet_local *lp, u32 coalesce_usec)\n{\n\tu32 result;\n\tu64 clk_rate = 125000000;  \n\n\tif (lp->axi_clk)\n\t\tclk_rate = clk_get_rate(lp->axi_clk);\n\n\t \n\tresult = DIV64_U64_ROUND_CLOSEST((u64)coalesce_usec * clk_rate,\n\t\t\t\t\t (u64)125000000);\n\tif (result > 255)\n\t\tresult = 255;\n\n\treturn result;\n}\n\n \nstatic void axienet_dma_start(struct axienet_local *lp)\n{\n\t \n\tlp->rx_dma_cr = (lp->coalesce_count_rx << XAXIDMA_COALESCE_SHIFT) |\n\t\t\tXAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_ERROR_MASK;\n\t \n\tif (lp->coalesce_count_rx > 1)\n\t\tlp->rx_dma_cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_rx)\n\t\t\t\t\t<< XAXIDMA_DELAY_SHIFT) |\n\t\t\t\t XAXIDMA_IRQ_DELAY_MASK;\n\taxienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, lp->rx_dma_cr);\n\n\t \n\tlp->tx_dma_cr = (lp->coalesce_count_tx << XAXIDMA_COALESCE_SHIFT) |\n\t\t\tXAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_ERROR_MASK;\n\t \n\tif (lp->coalesce_count_tx > 1)\n\t\tlp->tx_dma_cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_tx)\n\t\t\t\t\t<< XAXIDMA_DELAY_SHIFT) |\n\t\t\t\t XAXIDMA_IRQ_DELAY_MASK;\n\taxienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, lp->tx_dma_cr);\n\n\t \n\taxienet_dma_out_addr(lp, XAXIDMA_RX_CDESC_OFFSET, lp->rx_bd_p);\n\tlp->rx_dma_cr |= XAXIDMA_CR_RUNSTOP_MASK;\n\taxienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, lp->rx_dma_cr);\n\taxienet_dma_out_addr(lp, XAXIDMA_RX_TDESC_OFFSET, lp->rx_bd_p +\n\t\t\t     (sizeof(*lp->rx_bd_v) * (lp->rx_bd_num - 1)));\n\n\t \n\taxienet_dma_out_addr(lp, XAXIDMA_TX_CDESC_OFFSET, lp->tx_bd_p);\n\tlp->tx_dma_cr |= XAXIDMA_CR_RUNSTOP_MASK;\n\taxienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, lp->tx_dma_cr);\n}\n\n \nstatic int axienet_dma_bd_init(struct net_device *ndev)\n{\n\tint i;\n\tstruct sk_buff *skb;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\t \n\tlp->tx_bd_ci = 0;\n\tlp->tx_bd_tail = 0;\n\tlp->rx_bd_ci = 0;\n\n\t \n\tlp->tx_bd_v = dma_alloc_coherent(lp->dev,\n\t\t\t\t\t sizeof(*lp->tx_bd_v) * lp->tx_bd_num,\n\t\t\t\t\t &lp->tx_bd_p, GFP_KERNEL);\n\tif (!lp->tx_bd_v)\n\t\treturn -ENOMEM;\n\n\tlp->rx_bd_v = dma_alloc_coherent(lp->dev,\n\t\t\t\t\t sizeof(*lp->rx_bd_v) * lp->rx_bd_num,\n\t\t\t\t\t &lp->rx_bd_p, GFP_KERNEL);\n\tif (!lp->rx_bd_v)\n\t\tgoto out;\n\n\tfor (i = 0; i < lp->tx_bd_num; i++) {\n\t\tdma_addr_t addr = lp->tx_bd_p +\n\t\t\t\t  sizeof(*lp->tx_bd_v) *\n\t\t\t\t  ((i + 1) % lp->tx_bd_num);\n\n\t\tlp->tx_bd_v[i].next = lower_32_bits(addr);\n\t\tif (lp->features & XAE_FEATURE_DMA_64BIT)\n\t\t\tlp->tx_bd_v[i].next_msb = upper_32_bits(addr);\n\t}\n\n\tfor (i = 0; i < lp->rx_bd_num; i++) {\n\t\tdma_addr_t addr;\n\n\t\taddr = lp->rx_bd_p + sizeof(*lp->rx_bd_v) *\n\t\t\t((i + 1) % lp->rx_bd_num);\n\t\tlp->rx_bd_v[i].next = lower_32_bits(addr);\n\t\tif (lp->features & XAE_FEATURE_DMA_64BIT)\n\t\t\tlp->rx_bd_v[i].next_msb = upper_32_bits(addr);\n\n\t\tskb = netdev_alloc_skb_ip_align(ndev, lp->max_frm_size);\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tlp->rx_bd_v[i].skb = skb;\n\t\taddr = dma_map_single(lp->dev, skb->data,\n\t\t\t\t      lp->max_frm_size, DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(lp->dev, addr)) {\n\t\t\tnetdev_err(ndev, \"DMA mapping error\\n\");\n\t\t\tgoto out;\n\t\t}\n\t\tdesc_set_phys_addr(lp, addr, &lp->rx_bd_v[i]);\n\n\t\tlp->rx_bd_v[i].cntrl = lp->max_frm_size;\n\t}\n\n\taxienet_dma_start(lp);\n\n\treturn 0;\nout:\n\taxienet_dma_bd_release(ndev);\n\treturn -ENOMEM;\n}\n\n \nstatic void axienet_set_mac_address(struct net_device *ndev,\n\t\t\t\t    const void *address)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tif (address)\n\t\teth_hw_addr_set(ndev, address);\n\tif (!is_valid_ether_addr(ndev->dev_addr))\n\t\teth_hw_addr_random(ndev);\n\n\t \n\taxienet_iow(lp, XAE_UAW0_OFFSET,\n\t\t    (ndev->dev_addr[0]) |\n\t\t    (ndev->dev_addr[1] << 8) |\n\t\t    (ndev->dev_addr[2] << 16) |\n\t\t    (ndev->dev_addr[3] << 24));\n\taxienet_iow(lp, XAE_UAW1_OFFSET,\n\t\t    (((axienet_ior(lp, XAE_UAW1_OFFSET)) &\n\t\t      ~XAE_UAW1_UNICASTADDR_MASK) |\n\t\t     (ndev->dev_addr[4] |\n\t\t     (ndev->dev_addr[5] << 8))));\n}\n\n \nstatic int netdev_set_mac_address(struct net_device *ndev, void *p)\n{\n\tstruct sockaddr *addr = p;\n\taxienet_set_mac_address(ndev, addr->sa_data);\n\treturn 0;\n}\n\n \nstatic void axienet_set_multicast_list(struct net_device *ndev)\n{\n\tint i;\n\tu32 reg, af0reg, af1reg;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tif (ndev->flags & (IFF_ALLMULTI | IFF_PROMISC) ||\n\t    netdev_mc_count(ndev) > XAE_MULTICAST_CAM_TABLE_NUM) {\n\t\t \n\t\tndev->flags |= IFF_PROMISC;\n\t\treg = axienet_ior(lp, XAE_FMI_OFFSET);\n\t\treg |= XAE_FMI_PM_MASK;\n\t\taxienet_iow(lp, XAE_FMI_OFFSET, reg);\n\t\tdev_info(&ndev->dev, \"Promiscuous mode enabled.\\n\");\n\t} else if (!netdev_mc_empty(ndev)) {\n\t\tstruct netdev_hw_addr *ha;\n\n\t\ti = 0;\n\t\tnetdev_for_each_mc_addr(ha, ndev) {\n\t\t\tif (i >= XAE_MULTICAST_CAM_TABLE_NUM)\n\t\t\t\tbreak;\n\n\t\t\taf0reg = (ha->addr[0]);\n\t\t\taf0reg |= (ha->addr[1] << 8);\n\t\t\taf0reg |= (ha->addr[2] << 16);\n\t\t\taf0reg |= (ha->addr[3] << 24);\n\n\t\t\taf1reg = (ha->addr[4]);\n\t\t\taf1reg |= (ha->addr[5] << 8);\n\n\t\t\treg = axienet_ior(lp, XAE_FMI_OFFSET) & 0xFFFFFF00;\n\t\t\treg |= i;\n\n\t\t\taxienet_iow(lp, XAE_FMI_OFFSET, reg);\n\t\t\taxienet_iow(lp, XAE_AF0_OFFSET, af0reg);\n\t\t\taxienet_iow(lp, XAE_AF1_OFFSET, af1reg);\n\t\t\ti++;\n\t\t}\n\t} else {\n\t\treg = axienet_ior(lp, XAE_FMI_OFFSET);\n\t\treg &= ~XAE_FMI_PM_MASK;\n\n\t\taxienet_iow(lp, XAE_FMI_OFFSET, reg);\n\n\t\tfor (i = 0; i < XAE_MULTICAST_CAM_TABLE_NUM; i++) {\n\t\t\treg = axienet_ior(lp, XAE_FMI_OFFSET) & 0xFFFFFF00;\n\t\t\treg |= i;\n\n\t\t\taxienet_iow(lp, XAE_FMI_OFFSET, reg);\n\t\t\taxienet_iow(lp, XAE_AF0_OFFSET, 0);\n\t\t\taxienet_iow(lp, XAE_AF1_OFFSET, 0);\n\t\t}\n\n\t\tdev_info(&ndev->dev, \"Promiscuous mode disabled.\\n\");\n\t}\n}\n\n \nstatic void axienet_setoptions(struct net_device *ndev, u32 options)\n{\n\tint reg;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\tstruct axienet_option *tp = &axienet_options[0];\n\n\twhile (tp->opt) {\n\t\treg = ((axienet_ior(lp, tp->reg)) & ~(tp->m_or));\n\t\tif (options & tp->opt)\n\t\t\treg |= tp->m_or;\n\t\taxienet_iow(lp, tp->reg, reg);\n\t\ttp++;\n\t}\n\n\tlp->options |= options;\n}\n\nstatic int __axienet_device_reset(struct axienet_local *lp)\n{\n\tu32 value;\n\tint ret;\n\n\t \n\taxienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, XAXIDMA_CR_RESET_MASK);\n\tret = read_poll_timeout(axienet_dma_in32, value,\n\t\t\t\t!(value & XAXIDMA_CR_RESET_MASK),\n\t\t\t\tDELAY_OF_ONE_MILLISEC, 50000, false, lp,\n\t\t\t\tXAXIDMA_TX_CR_OFFSET);\n\tif (ret) {\n\t\tdev_err(lp->dev, \"%s: DMA reset timeout!\\n\", __func__);\n\t\treturn ret;\n\t}\n\n\t \n\tret = read_poll_timeout(axienet_ior, value,\n\t\t\t\tvalue & XAE_INT_PHYRSTCMPLT_MASK,\n\t\t\t\tDELAY_OF_ONE_MILLISEC, 50000, false, lp,\n\t\t\t\tXAE_IS_OFFSET);\n\tif (ret) {\n\t\tdev_err(lp->dev, \"%s: timeout waiting for PhyRstCmplt\\n\", __func__);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void axienet_dma_stop(struct axienet_local *lp)\n{\n\tint count;\n\tu32 cr, sr;\n\n\tcr = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);\n\tcr &= ~(XAXIDMA_CR_RUNSTOP_MASK | XAXIDMA_IRQ_ALL_MASK);\n\taxienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, cr);\n\tsynchronize_irq(lp->rx_irq);\n\n\tcr = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);\n\tcr &= ~(XAXIDMA_CR_RUNSTOP_MASK | XAXIDMA_IRQ_ALL_MASK);\n\taxienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, cr);\n\tsynchronize_irq(lp->tx_irq);\n\n\t \n\tsr = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);\n\tfor (count = 0; !(sr & XAXIDMA_SR_HALT_MASK) && count < 5; ++count) {\n\t\tmsleep(20);\n\t\tsr = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);\n\t}\n\n\tsr = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);\n\tfor (count = 0; !(sr & XAXIDMA_SR_HALT_MASK) && count < 5; ++count) {\n\t\tmsleep(20);\n\t\tsr = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);\n\t}\n\n\t \n\taxienet_lock_mii(lp);\n\t__axienet_device_reset(lp);\n\taxienet_unlock_mii(lp);\n}\n\n \nstatic int axienet_device_reset(struct net_device *ndev)\n{\n\tu32 axienet_status;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\tint ret;\n\n\tret = __axienet_device_reset(lp);\n\tif (ret)\n\t\treturn ret;\n\n\tlp->max_frm_size = XAE_MAX_VLAN_FRAME_SIZE;\n\tlp->options |= XAE_OPTION_VLAN;\n\tlp->options &= (~XAE_OPTION_JUMBO);\n\n\tif ((ndev->mtu > XAE_MTU) &&\n\t    (ndev->mtu <= XAE_JUMBO_MTU)) {\n\t\tlp->max_frm_size = ndev->mtu + VLAN_ETH_HLEN +\n\t\t\t\t\tXAE_TRL_SIZE;\n\n\t\tif (lp->max_frm_size <= lp->rxmem)\n\t\t\tlp->options |= XAE_OPTION_JUMBO;\n\t}\n\n\tret = axienet_dma_bd_init(ndev);\n\tif (ret) {\n\t\tnetdev_err(ndev, \"%s: descriptor allocation failed\\n\",\n\t\t\t   __func__);\n\t\treturn ret;\n\t}\n\n\taxienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);\n\taxienet_status &= ~XAE_RCW1_RX_MASK;\n\taxienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);\n\n\taxienet_status = axienet_ior(lp, XAE_IP_OFFSET);\n\tif (axienet_status & XAE_INT_RXRJECT_MASK)\n\t\taxienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);\n\taxienet_iow(lp, XAE_IE_OFFSET, lp->eth_irq > 0 ?\n\t\t    XAE_INT_RECV_ERROR_MASK : 0);\n\n\taxienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);\n\n\t \n\taxienet_setoptions(ndev, lp->options &\n\t\t\t   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));\n\taxienet_set_mac_address(ndev, NULL);\n\taxienet_set_multicast_list(ndev);\n\taxienet_setoptions(ndev, lp->options);\n\n\tnetif_trans_update(ndev);\n\n\treturn 0;\n}\n\n \nstatic int axienet_free_tx_chain(struct axienet_local *lp, u32 first_bd,\n\t\t\t\t int nr_bds, bool force, u32 *sizep, int budget)\n{\n\tstruct axidma_bd *cur_p;\n\tunsigned int status;\n\tdma_addr_t phys;\n\tint i;\n\n\tfor (i = 0; i < nr_bds; i++) {\n\t\tcur_p = &lp->tx_bd_v[(first_bd + i) % lp->tx_bd_num];\n\t\tstatus = cur_p->status;\n\n\t\t \n\t\tif (!force && !(status & XAXIDMA_BD_STS_COMPLETE_MASK))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\t\tphys = desc_get_phys_addr(lp, cur_p);\n\t\tdma_unmap_single(lp->dev, phys,\n\t\t\t\t (cur_p->cntrl & XAXIDMA_BD_CTRL_LENGTH_MASK),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\tif (cur_p->skb && (status & XAXIDMA_BD_STS_COMPLETE_MASK))\n\t\t\tnapi_consume_skb(cur_p->skb, budget);\n\n\t\tcur_p->app0 = 0;\n\t\tcur_p->app1 = 0;\n\t\tcur_p->app2 = 0;\n\t\tcur_p->app4 = 0;\n\t\tcur_p->skb = NULL;\n\t\t \n\t\twmb();\n\t\tcur_p->cntrl = 0;\n\t\tcur_p->status = 0;\n\n\t\tif (sizep)\n\t\t\t*sizep += status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;\n\t}\n\n\treturn i;\n}\n\n \nstatic inline int axienet_check_tx_bd_space(struct axienet_local *lp,\n\t\t\t\t\t    int num_frag)\n{\n\tstruct axidma_bd *cur_p;\n\n\t \n\trmb();\n\tcur_p = &lp->tx_bd_v[(READ_ONCE(lp->tx_bd_tail) + num_frag) %\n\t\t\t     lp->tx_bd_num];\n\tif (cur_p->cntrl)\n\t\treturn NETDEV_TX_BUSY;\n\treturn 0;\n}\n\n \nstatic int axienet_tx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct axienet_local *lp = container_of(napi, struct axienet_local, napi_tx);\n\tstruct net_device *ndev = lp->ndev;\n\tu32 size = 0;\n\tint packets;\n\n\tpackets = axienet_free_tx_chain(lp, lp->tx_bd_ci, budget, false, &size, budget);\n\n\tif (packets) {\n\t\tlp->tx_bd_ci += packets;\n\t\tif (lp->tx_bd_ci >= lp->tx_bd_num)\n\t\t\tlp->tx_bd_ci %= lp->tx_bd_num;\n\n\t\tu64_stats_update_begin(&lp->tx_stat_sync);\n\t\tu64_stats_add(&lp->tx_packets, packets);\n\t\tu64_stats_add(&lp->tx_bytes, size);\n\t\tu64_stats_update_end(&lp->tx_stat_sync);\n\n\t\t \n\t\tsmp_mb();\n\n\t\tif (!axienet_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1))\n\t\t\tnetif_wake_queue(ndev);\n\t}\n\n\tif (packets < budget && napi_complete_done(napi, packets)) {\n\t\t \n\t\taxienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, lp->tx_dma_cr);\n\t}\n\treturn packets;\n}\n\n \nstatic netdev_tx_t\naxienet_start_xmit(struct sk_buff *skb, struct net_device *ndev)\n{\n\tu32 ii;\n\tu32 num_frag;\n\tu32 csum_start_off;\n\tu32 csum_index_off;\n\tskb_frag_t *frag;\n\tdma_addr_t tail_p, phys;\n\tu32 orig_tail_ptr, new_tail_ptr;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\tstruct axidma_bd *cur_p;\n\n\torig_tail_ptr = lp->tx_bd_tail;\n\tnew_tail_ptr = orig_tail_ptr;\n\n\tnum_frag = skb_shinfo(skb)->nr_frags;\n\tcur_p = &lp->tx_bd_v[orig_tail_ptr];\n\n\tif (axienet_check_tx_bd_space(lp, num_frag + 1)) {\n\t\t \n\t\tnetif_stop_queue(ndev);\n\t\tif (net_ratelimit())\n\t\t\tnetdev_warn(ndev, \"TX ring unexpectedly full\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tif (lp->features & XAE_FEATURE_FULL_TX_CSUM) {\n\t\t\t \n\t\t\tcur_p->app0 |= 2;\n\t\t} else if (lp->features & XAE_FEATURE_PARTIAL_TX_CSUM) {\n\t\t\tcsum_start_off = skb_transport_offset(skb);\n\t\t\tcsum_index_off = csum_start_off + skb->csum_offset;\n\t\t\t \n\t\t\tcur_p->app0 |= 1;\n\t\t\tcur_p->app1 = (csum_start_off << 16) | csum_index_off;\n\t\t}\n\t} else if (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tcur_p->app0 |= 2;  \n\t}\n\n\tphys = dma_map_single(lp->dev, skb->data,\n\t\t\t      skb_headlen(skb), DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(lp->dev, phys))) {\n\t\tif (net_ratelimit())\n\t\t\tnetdev_err(ndev, \"TX DMA mapping error\\n\");\n\t\tndev->stats.tx_dropped++;\n\t\treturn NETDEV_TX_OK;\n\t}\n\tdesc_set_phys_addr(lp, phys, cur_p);\n\tcur_p->cntrl = skb_headlen(skb) | XAXIDMA_BD_CTRL_TXSOF_MASK;\n\n\tfor (ii = 0; ii < num_frag; ii++) {\n\t\tif (++new_tail_ptr >= lp->tx_bd_num)\n\t\t\tnew_tail_ptr = 0;\n\t\tcur_p = &lp->tx_bd_v[new_tail_ptr];\n\t\tfrag = &skb_shinfo(skb)->frags[ii];\n\t\tphys = dma_map_single(lp->dev,\n\t\t\t\t      skb_frag_address(frag),\n\t\t\t\t      skb_frag_size(frag),\n\t\t\t\t      DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(lp->dev, phys))) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tnetdev_err(ndev, \"TX DMA mapping error\\n\");\n\t\t\tndev->stats.tx_dropped++;\n\t\t\taxienet_free_tx_chain(lp, orig_tail_ptr, ii + 1,\n\t\t\t\t\t      true, NULL, 0);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tdesc_set_phys_addr(lp, phys, cur_p);\n\t\tcur_p->cntrl = skb_frag_size(frag);\n\t}\n\n\tcur_p->cntrl |= XAXIDMA_BD_CTRL_TXEOF_MASK;\n\tcur_p->skb = skb;\n\n\ttail_p = lp->tx_bd_p + sizeof(*lp->tx_bd_v) * new_tail_ptr;\n\tif (++new_tail_ptr >= lp->tx_bd_num)\n\t\tnew_tail_ptr = 0;\n\tWRITE_ONCE(lp->tx_bd_tail, new_tail_ptr);\n\n\t \n\taxienet_dma_out_addr(lp, XAXIDMA_TX_TDESC_OFFSET, tail_p);\n\n\t \n\tif (axienet_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1)) {\n\t\tnetif_stop_queue(ndev);\n\n\t\t \n\t\tsmp_mb();\n\n\t\t \n\t\tif (!axienet_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1))\n\t\t\tnetif_wake_queue(ndev);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic int axienet_rx_poll(struct napi_struct *napi, int budget)\n{\n\tu32 length;\n\tu32 csumstatus;\n\tu32 size = 0;\n\tint packets = 0;\n\tdma_addr_t tail_p = 0;\n\tstruct axidma_bd *cur_p;\n\tstruct sk_buff *skb, *new_skb;\n\tstruct axienet_local *lp = container_of(napi, struct axienet_local, napi_rx);\n\n\tcur_p = &lp->rx_bd_v[lp->rx_bd_ci];\n\n\twhile (packets < budget && (cur_p->status & XAXIDMA_BD_STS_COMPLETE_MASK)) {\n\t\tdma_addr_t phys;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tskb = cur_p->skb;\n\t\tcur_p->skb = NULL;\n\n\t\t \n\t\tif (likely(skb)) {\n\t\t\tlength = cur_p->app4 & 0x0000FFFF;\n\n\t\t\tphys = desc_get_phys_addr(lp, cur_p);\n\t\t\tdma_unmap_single(lp->dev, phys, lp->max_frm_size,\n\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t\tskb_put(skb, length);\n\t\t\tskb->protocol = eth_type_trans(skb, lp->ndev);\n\t\t\t \n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\t\t \n\t\t\tif (lp->features & XAE_FEATURE_FULL_RX_CSUM) {\n\t\t\t\tcsumstatus = (cur_p->app2 &\n\t\t\t\t\t      XAE_FULL_CSUM_STATUS_MASK) >> 3;\n\t\t\t\tif (csumstatus == XAE_IP_TCP_CSUM_VALIDATED ||\n\t\t\t\t    csumstatus == XAE_IP_UDP_CSUM_VALIDATED) {\n\t\t\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\t\t}\n\t\t\t} else if ((lp->features & XAE_FEATURE_PARTIAL_RX_CSUM) != 0 &&\n\t\t\t\t   skb->protocol == htons(ETH_P_IP) &&\n\t\t\t\t   skb->len > 64) {\n\t\t\t\tskb->csum = be32_to_cpu(cur_p->app3 & 0xFFFF);\n\t\t\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\t\t}\n\n\t\t\tnapi_gro_receive(napi, skb);\n\n\t\t\tsize += length;\n\t\t\tpackets++;\n\t\t}\n\n\t\tnew_skb = napi_alloc_skb(napi, lp->max_frm_size);\n\t\tif (!new_skb)\n\t\t\tbreak;\n\n\t\tphys = dma_map_single(lp->dev, new_skb->data,\n\t\t\t\t      lp->max_frm_size,\n\t\t\t\t      DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(lp->dev, phys))) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tnetdev_err(lp->ndev, \"RX DMA mapping error\\n\");\n\t\t\tdev_kfree_skb(new_skb);\n\t\t\tbreak;\n\t\t}\n\t\tdesc_set_phys_addr(lp, phys, cur_p);\n\n\t\tcur_p->cntrl = lp->max_frm_size;\n\t\tcur_p->status = 0;\n\t\tcur_p->skb = new_skb;\n\n\t\t \n\t\ttail_p = lp->rx_bd_p + sizeof(*lp->rx_bd_v) * lp->rx_bd_ci;\n\n\t\tif (++lp->rx_bd_ci >= lp->rx_bd_num)\n\t\t\tlp->rx_bd_ci = 0;\n\t\tcur_p = &lp->rx_bd_v[lp->rx_bd_ci];\n\t}\n\n\tu64_stats_update_begin(&lp->rx_stat_sync);\n\tu64_stats_add(&lp->rx_packets, packets);\n\tu64_stats_add(&lp->rx_bytes, size);\n\tu64_stats_update_end(&lp->rx_stat_sync);\n\n\tif (tail_p)\n\t\taxienet_dma_out_addr(lp, XAXIDMA_RX_TDESC_OFFSET, tail_p);\n\n\tif (packets < budget && napi_complete_done(napi, packets)) {\n\t\t \n\t\taxienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, lp->rx_dma_cr);\n\t}\n\treturn packets;\n}\n\n \nstatic irqreturn_t axienet_tx_irq(int irq, void *_ndev)\n{\n\tunsigned int status;\n\tstruct net_device *ndev = _ndev;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tstatus = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);\n\n\tif (!(status & XAXIDMA_IRQ_ALL_MASK))\n\t\treturn IRQ_NONE;\n\n\taxienet_dma_out32(lp, XAXIDMA_TX_SR_OFFSET, status);\n\n\tif (unlikely(status & XAXIDMA_IRQ_ERROR_MASK)) {\n\t\tnetdev_err(ndev, \"DMA Tx error 0x%x\\n\", status);\n\t\tnetdev_err(ndev, \"Current BD is at: 0x%x%08x\\n\",\n\t\t\t   (lp->tx_bd_v[lp->tx_bd_ci]).phys_msb,\n\t\t\t   (lp->tx_bd_v[lp->tx_bd_ci]).phys);\n\t\tschedule_work(&lp->dma_err_task);\n\t} else {\n\t\t \n\t\tu32 cr = lp->tx_dma_cr;\n\n\t\tcr &= ~(XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK);\n\t\taxienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, cr);\n\n\t\tnapi_schedule(&lp->napi_tx);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t axienet_rx_irq(int irq, void *_ndev)\n{\n\tunsigned int status;\n\tstruct net_device *ndev = _ndev;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tstatus = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);\n\n\tif (!(status & XAXIDMA_IRQ_ALL_MASK))\n\t\treturn IRQ_NONE;\n\n\taxienet_dma_out32(lp, XAXIDMA_RX_SR_OFFSET, status);\n\n\tif (unlikely(status & XAXIDMA_IRQ_ERROR_MASK)) {\n\t\tnetdev_err(ndev, \"DMA Rx error 0x%x\\n\", status);\n\t\tnetdev_err(ndev, \"Current BD is at: 0x%x%08x\\n\",\n\t\t\t   (lp->rx_bd_v[lp->rx_bd_ci]).phys_msb,\n\t\t\t   (lp->rx_bd_v[lp->rx_bd_ci]).phys);\n\t\tschedule_work(&lp->dma_err_task);\n\t} else {\n\t\t \n\t\tu32 cr = lp->rx_dma_cr;\n\n\t\tcr &= ~(XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK);\n\t\taxienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, cr);\n\n\t\tnapi_schedule(&lp->napi_rx);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t axienet_eth_irq(int irq, void *_ndev)\n{\n\tstruct net_device *ndev = _ndev;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\tunsigned int pending;\n\n\tpending = axienet_ior(lp, XAE_IP_OFFSET);\n\tif (!pending)\n\t\treturn IRQ_NONE;\n\n\tif (pending & XAE_INT_RXFIFOOVR_MASK)\n\t\tndev->stats.rx_missed_errors++;\n\n\tif (pending & XAE_INT_RXRJECT_MASK)\n\t\tndev->stats.rx_frame_errors++;\n\n\taxienet_iow(lp, XAE_IS_OFFSET, pending);\n\treturn IRQ_HANDLED;\n}\n\nstatic void axienet_dma_err_handler(struct work_struct *work);\n\n \nstatic int axienet_open(struct net_device *ndev)\n{\n\tint ret;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tdev_dbg(&ndev->dev, \"axienet_open()\\n\");\n\n\t \n\taxienet_lock_mii(lp);\n\tret = axienet_device_reset(ndev);\n\taxienet_unlock_mii(lp);\n\n\tret = phylink_of_phy_connect(lp->phylink, lp->dev->of_node, 0);\n\tif (ret) {\n\t\tdev_err(lp->dev, \"phylink_of_phy_connect() failed: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tphylink_start(lp->phylink);\n\n\t \n\tINIT_WORK(&lp->dma_err_task, axienet_dma_err_handler);\n\n\tnapi_enable(&lp->napi_rx);\n\tnapi_enable(&lp->napi_tx);\n\n\t \n\tret = request_irq(lp->tx_irq, axienet_tx_irq, IRQF_SHARED,\n\t\t\t  ndev->name, ndev);\n\tif (ret)\n\t\tgoto err_tx_irq;\n\t \n\tret = request_irq(lp->rx_irq, axienet_rx_irq, IRQF_SHARED,\n\t\t\t  ndev->name, ndev);\n\tif (ret)\n\t\tgoto err_rx_irq;\n\t \n\tif (lp->eth_irq > 0) {\n\t\tret = request_irq(lp->eth_irq, axienet_eth_irq, IRQF_SHARED,\n\t\t\t\t  ndev->name, ndev);\n\t\tif (ret)\n\t\t\tgoto err_eth_irq;\n\t}\n\n\treturn 0;\n\nerr_eth_irq:\n\tfree_irq(lp->rx_irq, ndev);\nerr_rx_irq:\n\tfree_irq(lp->tx_irq, ndev);\nerr_tx_irq:\n\tnapi_disable(&lp->napi_tx);\n\tnapi_disable(&lp->napi_rx);\n\tphylink_stop(lp->phylink);\n\tphylink_disconnect_phy(lp->phylink);\n\tcancel_work_sync(&lp->dma_err_task);\n\tdev_err(lp->dev, \"request_irq() failed\\n\");\n\treturn ret;\n}\n\n \nstatic int axienet_stop(struct net_device *ndev)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tdev_dbg(&ndev->dev, \"axienet_close()\\n\");\n\n\tnapi_disable(&lp->napi_tx);\n\tnapi_disable(&lp->napi_rx);\n\n\tphylink_stop(lp->phylink);\n\tphylink_disconnect_phy(lp->phylink);\n\n\taxienet_setoptions(ndev, lp->options &\n\t\t\t   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));\n\n\taxienet_dma_stop(lp);\n\n\taxienet_iow(lp, XAE_IE_OFFSET, 0);\n\n\tcancel_work_sync(&lp->dma_err_task);\n\n\tif (lp->eth_irq > 0)\n\t\tfree_irq(lp->eth_irq, ndev);\n\tfree_irq(lp->tx_irq, ndev);\n\tfree_irq(lp->rx_irq, ndev);\n\n\taxienet_dma_bd_release(ndev);\n\treturn 0;\n}\n\n \nstatic int axienet_change_mtu(struct net_device *ndev, int new_mtu)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tif (netif_running(ndev))\n\t\treturn -EBUSY;\n\n\tif ((new_mtu + VLAN_ETH_HLEN +\n\t\tXAE_TRL_SIZE) > lp->rxmem)\n\t\treturn -EINVAL;\n\n\tndev->mtu = new_mtu;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\n \nstatic void axienet_poll_controller(struct net_device *ndev)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\tdisable_irq(lp->tx_irq);\n\tdisable_irq(lp->rx_irq);\n\taxienet_rx_irq(lp->tx_irq, ndev);\n\taxienet_tx_irq(lp->rx_irq, ndev);\n\tenable_irq(lp->tx_irq);\n\tenable_irq(lp->rx_irq);\n}\n#endif\n\nstatic int axienet_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tstruct axienet_local *lp = netdev_priv(dev);\n\n\tif (!netif_running(dev))\n\t\treturn -EINVAL;\n\n\treturn phylink_mii_ioctl(lp->phylink, rq, cmd);\n}\n\nstatic void\naxienet_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)\n{\n\tstruct axienet_local *lp = netdev_priv(dev);\n\tunsigned int start;\n\n\tnetdev_stats_to_stats64(stats, &dev->stats);\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(&lp->rx_stat_sync);\n\t\tstats->rx_packets = u64_stats_read(&lp->rx_packets);\n\t\tstats->rx_bytes = u64_stats_read(&lp->rx_bytes);\n\t} while (u64_stats_fetch_retry(&lp->rx_stat_sync, start));\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(&lp->tx_stat_sync);\n\t\tstats->tx_packets = u64_stats_read(&lp->tx_packets);\n\t\tstats->tx_bytes = u64_stats_read(&lp->tx_bytes);\n\t} while (u64_stats_fetch_retry(&lp->tx_stat_sync, start));\n}\n\nstatic const struct net_device_ops axienet_netdev_ops = {\n\t.ndo_open = axienet_open,\n\t.ndo_stop = axienet_stop,\n\t.ndo_start_xmit = axienet_start_xmit,\n\t.ndo_get_stats64 = axienet_get_stats64,\n\t.ndo_change_mtu\t= axienet_change_mtu,\n\t.ndo_set_mac_address = netdev_set_mac_address,\n\t.ndo_validate_addr = eth_validate_addr,\n\t.ndo_eth_ioctl = axienet_ioctl,\n\t.ndo_set_rx_mode = axienet_set_multicast_list,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller = axienet_poll_controller,\n#endif\n};\n\n \nstatic void axienet_ethtools_get_drvinfo(struct net_device *ndev,\n\t\t\t\t\t struct ethtool_drvinfo *ed)\n{\n\tstrscpy(ed->driver, DRIVER_NAME, sizeof(ed->driver));\n\tstrscpy(ed->version, DRIVER_VERSION, sizeof(ed->version));\n}\n\n \nstatic int axienet_ethtools_get_regs_len(struct net_device *ndev)\n{\n\treturn sizeof(u32) * AXIENET_REGS_N;\n}\n\n \nstatic void axienet_ethtools_get_regs(struct net_device *ndev,\n\t\t\t\t      struct ethtool_regs *regs, void *ret)\n{\n\tu32 *data = (u32 *)ret;\n\tsize_t len = sizeof(u32) * AXIENET_REGS_N;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tregs->version = 0;\n\tregs->len = len;\n\n\tmemset(data, 0, len);\n\tdata[0] = axienet_ior(lp, XAE_RAF_OFFSET);\n\tdata[1] = axienet_ior(lp, XAE_TPF_OFFSET);\n\tdata[2] = axienet_ior(lp, XAE_IFGP_OFFSET);\n\tdata[3] = axienet_ior(lp, XAE_IS_OFFSET);\n\tdata[4] = axienet_ior(lp, XAE_IP_OFFSET);\n\tdata[5] = axienet_ior(lp, XAE_IE_OFFSET);\n\tdata[6] = axienet_ior(lp, XAE_TTAG_OFFSET);\n\tdata[7] = axienet_ior(lp, XAE_RTAG_OFFSET);\n\tdata[8] = axienet_ior(lp, XAE_UAWL_OFFSET);\n\tdata[9] = axienet_ior(lp, XAE_UAWU_OFFSET);\n\tdata[10] = axienet_ior(lp, XAE_TPID0_OFFSET);\n\tdata[11] = axienet_ior(lp, XAE_TPID1_OFFSET);\n\tdata[12] = axienet_ior(lp, XAE_PPST_OFFSET);\n\tdata[13] = axienet_ior(lp, XAE_RCW0_OFFSET);\n\tdata[14] = axienet_ior(lp, XAE_RCW1_OFFSET);\n\tdata[15] = axienet_ior(lp, XAE_TC_OFFSET);\n\tdata[16] = axienet_ior(lp, XAE_FCC_OFFSET);\n\tdata[17] = axienet_ior(lp, XAE_EMMC_OFFSET);\n\tdata[18] = axienet_ior(lp, XAE_PHYC_OFFSET);\n\tdata[19] = axienet_ior(lp, XAE_MDIO_MC_OFFSET);\n\tdata[20] = axienet_ior(lp, XAE_MDIO_MCR_OFFSET);\n\tdata[21] = axienet_ior(lp, XAE_MDIO_MWD_OFFSET);\n\tdata[22] = axienet_ior(lp, XAE_MDIO_MRD_OFFSET);\n\tdata[27] = axienet_ior(lp, XAE_UAW0_OFFSET);\n\tdata[28] = axienet_ior(lp, XAE_UAW1_OFFSET);\n\tdata[29] = axienet_ior(lp, XAE_FMI_OFFSET);\n\tdata[30] = axienet_ior(lp, XAE_AF0_OFFSET);\n\tdata[31] = axienet_ior(lp, XAE_AF1_OFFSET);\n\tdata[32] = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);\n\tdata[33] = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);\n\tdata[34] = axienet_dma_in32(lp, XAXIDMA_TX_CDESC_OFFSET);\n\tdata[35] = axienet_dma_in32(lp, XAXIDMA_TX_TDESC_OFFSET);\n\tdata[36] = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);\n\tdata[37] = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);\n\tdata[38] = axienet_dma_in32(lp, XAXIDMA_RX_CDESC_OFFSET);\n\tdata[39] = axienet_dma_in32(lp, XAXIDMA_RX_TDESC_OFFSET);\n}\n\nstatic void\naxienet_ethtools_get_ringparam(struct net_device *ndev,\n\t\t\t       struct ethtool_ringparam *ering,\n\t\t\t       struct kernel_ethtool_ringparam *kernel_ering,\n\t\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tering->rx_max_pending = RX_BD_NUM_MAX;\n\tering->rx_mini_max_pending = 0;\n\tering->rx_jumbo_max_pending = 0;\n\tering->tx_max_pending = TX_BD_NUM_MAX;\n\tering->rx_pending = lp->rx_bd_num;\n\tering->rx_mini_pending = 0;\n\tering->rx_jumbo_pending = 0;\n\tering->tx_pending = lp->tx_bd_num;\n}\n\nstatic int\naxienet_ethtools_set_ringparam(struct net_device *ndev,\n\t\t\t       struct ethtool_ringparam *ering,\n\t\t\t       struct kernel_ethtool_ringparam *kernel_ering,\n\t\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tif (ering->rx_pending > RX_BD_NUM_MAX ||\n\t    ering->rx_mini_pending ||\n\t    ering->rx_jumbo_pending ||\n\t    ering->tx_pending < TX_BD_NUM_MIN ||\n\t    ering->tx_pending > TX_BD_NUM_MAX)\n\t\treturn -EINVAL;\n\n\tif (netif_running(ndev))\n\t\treturn -EBUSY;\n\n\tlp->rx_bd_num = ering->rx_pending;\n\tlp->tx_bd_num = ering->tx_pending;\n\treturn 0;\n}\n\n \nstatic void\naxienet_ethtools_get_pauseparam(struct net_device *ndev,\n\t\t\t\tstruct ethtool_pauseparam *epauseparm)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tphylink_ethtool_get_pauseparam(lp->phylink, epauseparm);\n}\n\n \nstatic int\naxienet_ethtools_set_pauseparam(struct net_device *ndev,\n\t\t\t\tstruct ethtool_pauseparam *epauseparm)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\treturn phylink_ethtool_set_pauseparam(lp->phylink, epauseparm);\n}\n\n \nstatic int\naxienet_ethtools_get_coalesce(struct net_device *ndev,\n\t\t\t      struct ethtool_coalesce *ecoalesce,\n\t\t\t      struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tecoalesce->rx_max_coalesced_frames = lp->coalesce_count_rx;\n\tecoalesce->rx_coalesce_usecs = lp->coalesce_usec_rx;\n\tecoalesce->tx_max_coalesced_frames = lp->coalesce_count_tx;\n\tecoalesce->tx_coalesce_usecs = lp->coalesce_usec_tx;\n\treturn 0;\n}\n\n \nstatic int\naxienet_ethtools_set_coalesce(struct net_device *ndev,\n\t\t\t      struct ethtool_coalesce *ecoalesce,\n\t\t\t      struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tif (netif_running(ndev)) {\n\t\tnetdev_err(ndev,\n\t\t\t   \"Please stop netif before applying configuration\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (ecoalesce->rx_max_coalesced_frames)\n\t\tlp->coalesce_count_rx = ecoalesce->rx_max_coalesced_frames;\n\tif (ecoalesce->rx_coalesce_usecs)\n\t\tlp->coalesce_usec_rx = ecoalesce->rx_coalesce_usecs;\n\tif (ecoalesce->tx_max_coalesced_frames)\n\t\tlp->coalesce_count_tx = ecoalesce->tx_max_coalesced_frames;\n\tif (ecoalesce->tx_coalesce_usecs)\n\t\tlp->coalesce_usec_tx = ecoalesce->tx_coalesce_usecs;\n\n\treturn 0;\n}\n\nstatic int\naxienet_ethtools_get_link_ksettings(struct net_device *ndev,\n\t\t\t\t    struct ethtool_link_ksettings *cmd)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\treturn phylink_ethtool_ksettings_get(lp->phylink, cmd);\n}\n\nstatic int\naxienet_ethtools_set_link_ksettings(struct net_device *ndev,\n\t\t\t\t    const struct ethtool_link_ksettings *cmd)\n{\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\treturn phylink_ethtool_ksettings_set(lp->phylink, cmd);\n}\n\nstatic int axienet_ethtools_nway_reset(struct net_device *dev)\n{\n\tstruct axienet_local *lp = netdev_priv(dev);\n\n\treturn phylink_ethtool_nway_reset(lp->phylink);\n}\n\nstatic const struct ethtool_ops axienet_ethtool_ops = {\n\t.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES |\n\t\t\t\t     ETHTOOL_COALESCE_USECS,\n\t.get_drvinfo    = axienet_ethtools_get_drvinfo,\n\t.get_regs_len   = axienet_ethtools_get_regs_len,\n\t.get_regs       = axienet_ethtools_get_regs,\n\t.get_link       = ethtool_op_get_link,\n\t.get_ringparam\t= axienet_ethtools_get_ringparam,\n\t.set_ringparam\t= axienet_ethtools_set_ringparam,\n\t.get_pauseparam = axienet_ethtools_get_pauseparam,\n\t.set_pauseparam = axienet_ethtools_set_pauseparam,\n\t.get_coalesce   = axienet_ethtools_get_coalesce,\n\t.set_coalesce   = axienet_ethtools_set_coalesce,\n\t.get_link_ksettings = axienet_ethtools_get_link_ksettings,\n\t.set_link_ksettings = axienet_ethtools_set_link_ksettings,\n\t.nway_reset\t= axienet_ethtools_nway_reset,\n};\n\nstatic struct axienet_local *pcs_to_axienet_local(struct phylink_pcs *pcs)\n{\n\treturn container_of(pcs, struct axienet_local, pcs);\n}\n\nstatic void axienet_pcs_get_state(struct phylink_pcs *pcs,\n\t\t\t\t  struct phylink_link_state *state)\n{\n\tstruct mdio_device *pcs_phy = pcs_to_axienet_local(pcs)->pcs_phy;\n\n\tphylink_mii_c22_pcs_get_state(pcs_phy, state);\n}\n\nstatic void axienet_pcs_an_restart(struct phylink_pcs *pcs)\n{\n\tstruct mdio_device *pcs_phy = pcs_to_axienet_local(pcs)->pcs_phy;\n\n\tphylink_mii_c22_pcs_an_restart(pcs_phy);\n}\n\nstatic int axienet_pcs_config(struct phylink_pcs *pcs, unsigned int neg_mode,\n\t\t\t      phy_interface_t interface,\n\t\t\t      const unsigned long *advertising,\n\t\t\t      bool permit_pause_to_mac)\n{\n\tstruct mdio_device *pcs_phy = pcs_to_axienet_local(pcs)->pcs_phy;\n\tstruct net_device *ndev = pcs_to_axienet_local(pcs)->ndev;\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\tint ret;\n\n\tif (lp->switch_x_sgmii) {\n\t\tret = mdiodev_write(pcs_phy, XLNX_MII_STD_SELECT_REG,\n\t\t\t\t    interface == PHY_INTERFACE_MODE_SGMII ?\n\t\t\t\t\tXLNX_MII_STD_SELECT_SGMII : 0);\n\t\tif (ret < 0) {\n\t\t\tnetdev_warn(ndev,\n\t\t\t\t    \"Failed to switch PHY interface: %d\\n\",\n\t\t\t\t    ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tret = phylink_mii_c22_pcs_config(pcs_phy, interface, advertising,\n\t\t\t\t\t neg_mode);\n\tif (ret < 0)\n\t\tnetdev_warn(ndev, \"Failed to configure PCS: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic const struct phylink_pcs_ops axienet_pcs_ops = {\n\t.pcs_get_state = axienet_pcs_get_state,\n\t.pcs_config = axienet_pcs_config,\n\t.pcs_an_restart = axienet_pcs_an_restart,\n};\n\nstatic struct phylink_pcs *axienet_mac_select_pcs(struct phylink_config *config,\n\t\t\t\t\t\t  phy_interface_t interface)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tif (interface == PHY_INTERFACE_MODE_1000BASEX ||\n\t    interface ==  PHY_INTERFACE_MODE_SGMII)\n\t\treturn &lp->pcs;\n\n\treturn NULL;\n}\n\nstatic void axienet_mac_config(struct phylink_config *config, unsigned int mode,\n\t\t\t       const struct phylink_link_state *state)\n{\n\t \n}\n\nstatic void axienet_mac_link_down(struct phylink_config *config,\n\t\t\t\t  unsigned int mode,\n\t\t\t\t  phy_interface_t interface)\n{\n\t \n}\n\nstatic void axienet_mac_link_up(struct phylink_config *config,\n\t\t\t\tstruct phy_device *phy,\n\t\t\t\tunsigned int mode, phy_interface_t interface,\n\t\t\t\tint speed, int duplex,\n\t\t\t\tbool tx_pause, bool rx_pause)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\tu32 emmc_reg, fcc_reg;\n\n\temmc_reg = axienet_ior(lp, XAE_EMMC_OFFSET);\n\temmc_reg &= ~XAE_EMMC_LINKSPEED_MASK;\n\n\tswitch (speed) {\n\tcase SPEED_1000:\n\t\temmc_reg |= XAE_EMMC_LINKSPD_1000;\n\t\tbreak;\n\tcase SPEED_100:\n\t\temmc_reg |= XAE_EMMC_LINKSPD_100;\n\t\tbreak;\n\tcase SPEED_10:\n\t\temmc_reg |= XAE_EMMC_LINKSPD_10;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&ndev->dev,\n\t\t\t\"Speed other than 10, 100 or 1Gbps is not supported\\n\");\n\t\tbreak;\n\t}\n\n\taxienet_iow(lp, XAE_EMMC_OFFSET, emmc_reg);\n\n\tfcc_reg = axienet_ior(lp, XAE_FCC_OFFSET);\n\tif (tx_pause)\n\t\tfcc_reg |= XAE_FCC_FCTX_MASK;\n\telse\n\t\tfcc_reg &= ~XAE_FCC_FCTX_MASK;\n\tif (rx_pause)\n\t\tfcc_reg |= XAE_FCC_FCRX_MASK;\n\telse\n\t\tfcc_reg &= ~XAE_FCC_FCRX_MASK;\n\taxienet_iow(lp, XAE_FCC_OFFSET, fcc_reg);\n}\n\nstatic const struct phylink_mac_ops axienet_phylink_ops = {\n\t.mac_select_pcs = axienet_mac_select_pcs,\n\t.mac_config = axienet_mac_config,\n\t.mac_link_down = axienet_mac_link_down,\n\t.mac_link_up = axienet_mac_link_up,\n};\n\n \nstatic void axienet_dma_err_handler(struct work_struct *work)\n{\n\tu32 i;\n\tu32 axienet_status;\n\tstruct axidma_bd *cur_p;\n\tstruct axienet_local *lp = container_of(work, struct axienet_local,\n\t\t\t\t\t\tdma_err_task);\n\tstruct net_device *ndev = lp->ndev;\n\n\tnapi_disable(&lp->napi_tx);\n\tnapi_disable(&lp->napi_rx);\n\n\taxienet_setoptions(ndev, lp->options &\n\t\t\t   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));\n\n\taxienet_dma_stop(lp);\n\n\tfor (i = 0; i < lp->tx_bd_num; i++) {\n\t\tcur_p = &lp->tx_bd_v[i];\n\t\tif (cur_p->cntrl) {\n\t\t\tdma_addr_t addr = desc_get_phys_addr(lp, cur_p);\n\n\t\t\tdma_unmap_single(lp->dev, addr,\n\t\t\t\t\t (cur_p->cntrl &\n\t\t\t\t\t  XAXIDMA_BD_CTRL_LENGTH_MASK),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t}\n\t\tif (cur_p->skb)\n\t\t\tdev_kfree_skb_irq(cur_p->skb);\n\t\tcur_p->phys = 0;\n\t\tcur_p->phys_msb = 0;\n\t\tcur_p->cntrl = 0;\n\t\tcur_p->status = 0;\n\t\tcur_p->app0 = 0;\n\t\tcur_p->app1 = 0;\n\t\tcur_p->app2 = 0;\n\t\tcur_p->app3 = 0;\n\t\tcur_p->app4 = 0;\n\t\tcur_p->skb = NULL;\n\t}\n\n\tfor (i = 0; i < lp->rx_bd_num; i++) {\n\t\tcur_p = &lp->rx_bd_v[i];\n\t\tcur_p->status = 0;\n\t\tcur_p->app0 = 0;\n\t\tcur_p->app1 = 0;\n\t\tcur_p->app2 = 0;\n\t\tcur_p->app3 = 0;\n\t\tcur_p->app4 = 0;\n\t}\n\n\tlp->tx_bd_ci = 0;\n\tlp->tx_bd_tail = 0;\n\tlp->rx_bd_ci = 0;\n\n\taxienet_dma_start(lp);\n\n\taxienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);\n\taxienet_status &= ~XAE_RCW1_RX_MASK;\n\taxienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);\n\n\taxienet_status = axienet_ior(lp, XAE_IP_OFFSET);\n\tif (axienet_status & XAE_INT_RXRJECT_MASK)\n\t\taxienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);\n\taxienet_iow(lp, XAE_IE_OFFSET, lp->eth_irq > 0 ?\n\t\t    XAE_INT_RECV_ERROR_MASK : 0);\n\taxienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);\n\n\t \n\taxienet_setoptions(ndev, lp->options &\n\t\t\t   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));\n\taxienet_set_mac_address(ndev, NULL);\n\taxienet_set_multicast_list(ndev);\n\taxienet_setoptions(ndev, lp->options);\n\tnapi_enable(&lp->napi_rx);\n\tnapi_enable(&lp->napi_tx);\n}\n\n \nstatic int axienet_probe(struct platform_device *pdev)\n{\n\tint ret;\n\tstruct device_node *np;\n\tstruct axienet_local *lp;\n\tstruct net_device *ndev;\n\tstruct resource *ethres;\n\tu8 mac_addr[ETH_ALEN];\n\tint addr_width = 32;\n\tu32 value;\n\n\tndev = alloc_etherdev(sizeof(*lp));\n\tif (!ndev)\n\t\treturn -ENOMEM;\n\n\tplatform_set_drvdata(pdev, ndev);\n\n\tSET_NETDEV_DEV(ndev, &pdev->dev);\n\tndev->flags &= ~IFF_MULTICAST;   \n\tndev->features = NETIF_F_SG;\n\tndev->netdev_ops = &axienet_netdev_ops;\n\tndev->ethtool_ops = &axienet_ethtool_ops;\n\n\t \n\tndev->min_mtu = 64;\n\tndev->max_mtu = XAE_JUMBO_MTU;\n\n\tlp = netdev_priv(ndev);\n\tlp->ndev = ndev;\n\tlp->dev = &pdev->dev;\n\tlp->options = XAE_OPTION_DEFAULTS;\n\tlp->rx_bd_num = RX_BD_NUM_DEFAULT;\n\tlp->tx_bd_num = TX_BD_NUM_DEFAULT;\n\n\tu64_stats_init(&lp->rx_stat_sync);\n\tu64_stats_init(&lp->tx_stat_sync);\n\n\tnetif_napi_add(ndev, &lp->napi_rx, axienet_rx_poll);\n\tnetif_napi_add(ndev, &lp->napi_tx, axienet_tx_poll);\n\n\tlp->axi_clk = devm_clk_get_optional(&pdev->dev, \"s_axi_lite_clk\");\n\tif (!lp->axi_clk) {\n\t\t \n\t\tlp->axi_clk = devm_clk_get_optional(&pdev->dev, NULL);\n\t}\n\tif (IS_ERR(lp->axi_clk)) {\n\t\tret = PTR_ERR(lp->axi_clk);\n\t\tgoto free_netdev;\n\t}\n\tret = clk_prepare_enable(lp->axi_clk);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"Unable to enable AXI clock: %d\\n\", ret);\n\t\tgoto free_netdev;\n\t}\n\n\tlp->misc_clks[0].id = \"axis_clk\";\n\tlp->misc_clks[1].id = \"ref_clk\";\n\tlp->misc_clks[2].id = \"mgt_clk\";\n\n\tret = devm_clk_bulk_get_optional(&pdev->dev, XAE_NUM_MISC_CLOCKS, lp->misc_clks);\n\tif (ret)\n\t\tgoto cleanup_clk;\n\n\tret = clk_bulk_prepare_enable(XAE_NUM_MISC_CLOCKS, lp->misc_clks);\n\tif (ret)\n\t\tgoto cleanup_clk;\n\n\t \n\tlp->regs = devm_platform_get_and_ioremap_resource(pdev, 0, &ethres);\n\tif (IS_ERR(lp->regs)) {\n\t\tret = PTR_ERR(lp->regs);\n\t\tgoto cleanup_clk;\n\t}\n\tlp->regs_start = ethres->start;\n\n\t \n\tlp->features = 0;\n\n\tret = of_property_read_u32(pdev->dev.of_node, \"xlnx,txcsum\", &value);\n\tif (!ret) {\n\t\tswitch (value) {\n\t\tcase 1:\n\t\t\tlp->csum_offload_on_tx_path =\n\t\t\t\tXAE_FEATURE_PARTIAL_TX_CSUM;\n\t\t\tlp->features |= XAE_FEATURE_PARTIAL_TX_CSUM;\n\t\t\t \n\t\t\tndev->features |= NETIF_F_IP_CSUM;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tlp->csum_offload_on_tx_path =\n\t\t\t\tXAE_FEATURE_FULL_TX_CSUM;\n\t\t\tlp->features |= XAE_FEATURE_FULL_TX_CSUM;\n\t\t\t \n\t\t\tndev->features |= NETIF_F_IP_CSUM;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tlp->csum_offload_on_tx_path = XAE_NO_CSUM_OFFLOAD;\n\t\t}\n\t}\n\tret = of_property_read_u32(pdev->dev.of_node, \"xlnx,rxcsum\", &value);\n\tif (!ret) {\n\t\tswitch (value) {\n\t\tcase 1:\n\t\t\tlp->csum_offload_on_rx_path =\n\t\t\t\tXAE_FEATURE_PARTIAL_RX_CSUM;\n\t\t\tlp->features |= XAE_FEATURE_PARTIAL_RX_CSUM;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tlp->csum_offload_on_rx_path =\n\t\t\t\tXAE_FEATURE_FULL_RX_CSUM;\n\t\t\tlp->features |= XAE_FEATURE_FULL_RX_CSUM;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tlp->csum_offload_on_rx_path = XAE_NO_CSUM_OFFLOAD;\n\t\t}\n\t}\n\t \n\tof_property_read_u32(pdev->dev.of_node, \"xlnx,rxmem\", &lp->rxmem);\n\n\tlp->switch_x_sgmii = of_property_read_bool(pdev->dev.of_node,\n\t\t\t\t\t\t   \"xlnx,switch-x-sgmii\");\n\n\t \n\tret = of_property_read_u32(pdev->dev.of_node, \"xlnx,phy-type\", &value);\n\tif (!ret) {\n\t\tnetdev_warn(ndev, \"Please upgrade your device tree binary blob to use phy-mode\");\n\t\tswitch (value) {\n\t\tcase XAE_PHY_TYPE_MII:\n\t\t\tlp->phy_mode = PHY_INTERFACE_MODE_MII;\n\t\t\tbreak;\n\t\tcase XAE_PHY_TYPE_GMII:\n\t\t\tlp->phy_mode = PHY_INTERFACE_MODE_GMII;\n\t\t\tbreak;\n\t\tcase XAE_PHY_TYPE_RGMII_2_0:\n\t\t\tlp->phy_mode = PHY_INTERFACE_MODE_RGMII_ID;\n\t\t\tbreak;\n\t\tcase XAE_PHY_TYPE_SGMII:\n\t\t\tlp->phy_mode = PHY_INTERFACE_MODE_SGMII;\n\t\t\tbreak;\n\t\tcase XAE_PHY_TYPE_1000BASE_X:\n\t\t\tlp->phy_mode = PHY_INTERFACE_MODE_1000BASEX;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto cleanup_clk;\n\t\t}\n\t} else {\n\t\tret = of_get_phy_mode(pdev->dev.of_node, &lp->phy_mode);\n\t\tif (ret)\n\t\t\tgoto cleanup_clk;\n\t}\n\tif (lp->switch_x_sgmii && lp->phy_mode != PHY_INTERFACE_MODE_SGMII &&\n\t    lp->phy_mode != PHY_INTERFACE_MODE_1000BASEX) {\n\t\tdev_err(&pdev->dev, \"xlnx,switch-x-sgmii only supported with SGMII or 1000BaseX\\n\");\n\t\tret = -EINVAL;\n\t\tgoto cleanup_clk;\n\t}\n\n\t \n\tnp = of_parse_phandle(pdev->dev.of_node, \"axistream-connected\", 0);\n\tif (np) {\n\t\tstruct resource dmares;\n\n\t\tret = of_address_to_resource(np, 0, &dmares);\n\t\tif (ret) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"unable to get DMA resource\\n\");\n\t\t\tof_node_put(np);\n\t\t\tgoto cleanup_clk;\n\t\t}\n\t\tlp->dma_regs = devm_ioremap_resource(&pdev->dev,\n\t\t\t\t\t\t     &dmares);\n\t\tlp->rx_irq = irq_of_parse_and_map(np, 1);\n\t\tlp->tx_irq = irq_of_parse_and_map(np, 0);\n\t\tof_node_put(np);\n\t\tlp->eth_irq = platform_get_irq_optional(pdev, 0);\n\t} else {\n\t\t \n\t\tlp->dma_regs = devm_platform_get_and_ioremap_resource(pdev, 1, NULL);\n\t\tlp->rx_irq = platform_get_irq(pdev, 1);\n\t\tlp->tx_irq = platform_get_irq(pdev, 0);\n\t\tlp->eth_irq = platform_get_irq_optional(pdev, 2);\n\t}\n\tif (IS_ERR(lp->dma_regs)) {\n\t\tdev_err(&pdev->dev, \"could not map DMA regs\\n\");\n\t\tret = PTR_ERR(lp->dma_regs);\n\t\tgoto cleanup_clk;\n\t}\n\tif ((lp->rx_irq <= 0) || (lp->tx_irq <= 0)) {\n\t\tdev_err(&pdev->dev, \"could not determine irqs\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto cleanup_clk;\n\t}\n\n\t \n\tret = __axienet_device_reset(lp);\n\tif (ret)\n\t\tgoto cleanup_clk;\n\n\t \n\tif ((axienet_ior(lp, XAE_ID_OFFSET) >> 24) >= 0x9) {\n\t\tvoid __iomem *desc = lp->dma_regs + XAXIDMA_TX_CDESC_OFFSET + 4;\n\n\t\tiowrite32(0x0, desc);\n\t\tif (ioread32(desc) == 0) {\t \n\t\t\tiowrite32(0xffffffff, desc);\n\t\t\tif (ioread32(desc) > 0) {\n\t\t\t\tlp->features |= XAE_FEATURE_DMA_64BIT;\n\t\t\t\taddr_width = 64;\n\t\t\t\tdev_info(&pdev->dev,\n\t\t\t\t\t \"autodetected 64-bit DMA range\\n\");\n\t\t\t}\n\t\t\tiowrite32(0x0, desc);\n\t\t}\n\t}\n\tif (!IS_ENABLED(CONFIG_64BIT) && lp->features & XAE_FEATURE_DMA_64BIT) {\n\t\tdev_err(&pdev->dev, \"64-bit addressable DMA is not compatible with 32-bit archecture\\n\");\n\t\tret = -EINVAL;\n\t\tgoto cleanup_clk;\n\t}\n\n\tret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(addr_width));\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"No suitable DMA available\\n\");\n\t\tgoto cleanup_clk;\n\t}\n\n\t \n\tif (lp->eth_irq <= 0)\n\t\tdev_info(&pdev->dev, \"Ethernet core IRQ not defined\\n\");\n\n\t \n\tret = of_get_mac_address(pdev->dev.of_node, mac_addr);\n\tif (!ret) {\n\t\taxienet_set_mac_address(ndev, mac_addr);\n\t} else {\n\t\tdev_warn(&pdev->dev, \"could not find MAC address property: %d\\n\",\n\t\t\t ret);\n\t\taxienet_set_mac_address(ndev, NULL);\n\t}\n\n\tlp->coalesce_count_rx = XAXIDMA_DFT_RX_THRESHOLD;\n\tlp->coalesce_usec_rx = XAXIDMA_DFT_RX_USEC;\n\tlp->coalesce_count_tx = XAXIDMA_DFT_TX_THRESHOLD;\n\tlp->coalesce_usec_tx = XAXIDMA_DFT_TX_USEC;\n\n\tret = axienet_mdio_setup(lp);\n\tif (ret)\n\t\tdev_warn(&pdev->dev,\n\t\t\t \"error registering MDIO bus: %d\\n\", ret);\n\n\tif (lp->phy_mode == PHY_INTERFACE_MODE_SGMII ||\n\t    lp->phy_mode == PHY_INTERFACE_MODE_1000BASEX) {\n\t\tnp = of_parse_phandle(pdev->dev.of_node, \"pcs-handle\", 0);\n\t\tif (!np) {\n\t\t\t \n\t\t\tnp = of_parse_phandle(pdev->dev.of_node, \"phy-handle\", 0);\n\t\t}\n\t\tif (!np) {\n\t\t\tdev_err(&pdev->dev, \"pcs-handle (preferred) or phy-handle required for 1000BaseX/SGMII\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto cleanup_mdio;\n\t\t}\n\t\tlp->pcs_phy = of_mdio_find_device(np);\n\t\tif (!lp->pcs_phy) {\n\t\t\tret = -EPROBE_DEFER;\n\t\t\tof_node_put(np);\n\t\t\tgoto cleanup_mdio;\n\t\t}\n\t\tof_node_put(np);\n\t\tlp->pcs.ops = &axienet_pcs_ops;\n\t\tlp->pcs.neg_mode = true;\n\t\tlp->pcs.poll = true;\n\t}\n\n\tlp->phylink_config.dev = &ndev->dev;\n\tlp->phylink_config.type = PHYLINK_NETDEV;\n\tlp->phylink_config.mac_capabilities = MAC_SYM_PAUSE | MAC_ASYM_PAUSE |\n\t\tMAC_10FD | MAC_100FD | MAC_1000FD;\n\n\t__set_bit(lp->phy_mode, lp->phylink_config.supported_interfaces);\n\tif (lp->switch_x_sgmii) {\n\t\t__set_bit(PHY_INTERFACE_MODE_1000BASEX,\n\t\t\t  lp->phylink_config.supported_interfaces);\n\t\t__set_bit(PHY_INTERFACE_MODE_SGMII,\n\t\t\t  lp->phylink_config.supported_interfaces);\n\t}\n\n\tlp->phylink = phylink_create(&lp->phylink_config, pdev->dev.fwnode,\n\t\t\t\t     lp->phy_mode,\n\t\t\t\t     &axienet_phylink_ops);\n\tif (IS_ERR(lp->phylink)) {\n\t\tret = PTR_ERR(lp->phylink);\n\t\tdev_err(&pdev->dev, \"phylink_create error (%i)\\n\", ret);\n\t\tgoto cleanup_mdio;\n\t}\n\n\tret = register_netdev(lp->ndev);\n\tif (ret) {\n\t\tdev_err(lp->dev, \"register_netdev() error (%i)\\n\", ret);\n\t\tgoto cleanup_phylink;\n\t}\n\n\treturn 0;\n\ncleanup_phylink:\n\tphylink_destroy(lp->phylink);\n\ncleanup_mdio:\n\tif (lp->pcs_phy)\n\t\tput_device(&lp->pcs_phy->dev);\n\tif (lp->mii_bus)\n\t\taxienet_mdio_teardown(lp);\ncleanup_clk:\n\tclk_bulk_disable_unprepare(XAE_NUM_MISC_CLOCKS, lp->misc_clks);\n\tclk_disable_unprepare(lp->axi_clk);\n\nfree_netdev:\n\tfree_netdev(ndev);\n\n\treturn ret;\n}\n\nstatic int axienet_remove(struct platform_device *pdev)\n{\n\tstruct net_device *ndev = platform_get_drvdata(pdev);\n\tstruct axienet_local *lp = netdev_priv(ndev);\n\n\tunregister_netdev(ndev);\n\n\tif (lp->phylink)\n\t\tphylink_destroy(lp->phylink);\n\n\tif (lp->pcs_phy)\n\t\tput_device(&lp->pcs_phy->dev);\n\n\taxienet_mdio_teardown(lp);\n\n\tclk_bulk_disable_unprepare(XAE_NUM_MISC_CLOCKS, lp->misc_clks);\n\tclk_disable_unprepare(lp->axi_clk);\n\n\tfree_netdev(ndev);\n\n\treturn 0;\n}\n\nstatic void axienet_shutdown(struct platform_device *pdev)\n{\n\tstruct net_device *ndev = platform_get_drvdata(pdev);\n\n\trtnl_lock();\n\tnetif_device_detach(ndev);\n\n\tif (netif_running(ndev))\n\t\tdev_close(ndev);\n\n\trtnl_unlock();\n}\n\nstatic int axienet_suspend(struct device *dev)\n{\n\tstruct net_device *ndev = dev_get_drvdata(dev);\n\n\tif (!netif_running(ndev))\n\t\treturn 0;\n\n\tnetif_device_detach(ndev);\n\n\trtnl_lock();\n\taxienet_stop(ndev);\n\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic int axienet_resume(struct device *dev)\n{\n\tstruct net_device *ndev = dev_get_drvdata(dev);\n\n\tif (!netif_running(ndev))\n\t\treturn 0;\n\n\trtnl_lock();\n\taxienet_open(ndev);\n\trtnl_unlock();\n\n\tnetif_device_attach(ndev);\n\n\treturn 0;\n}\n\nstatic DEFINE_SIMPLE_DEV_PM_OPS(axienet_pm_ops,\n\t\t\t\taxienet_suspend, axienet_resume);\n\nstatic struct platform_driver axienet_driver = {\n\t.probe = axienet_probe,\n\t.remove = axienet_remove,\n\t.shutdown = axienet_shutdown,\n\t.driver = {\n\t\t .name = \"xilinx_axienet\",\n\t\t .pm = &axienet_pm_ops,\n\t\t .of_match_table = axienet_of_match,\n\t},\n};\n\nmodule_platform_driver(axienet_driver);\n\nMODULE_DESCRIPTION(\"Xilinx Axi Ethernet driver\");\nMODULE_AUTHOR(\"Xilinx\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}