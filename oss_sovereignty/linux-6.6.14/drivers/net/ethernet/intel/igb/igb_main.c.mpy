{
  "module_name": "igb_main.c",
  "hash_id": "ea32a39a109a69b5f68af5ec0ea4b347b3f5891556b2ec06440f45194ab63682",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/igb/igb_main.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/init.h>\n#include <linux/bitops.h>\n#include <linux/vmalloc.h>\n#include <linux/pagemap.h>\n#include <linux/netdevice.h>\n#include <linux/ipv6.h>\n#include <linux/slab.h>\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <linux/net_tstamp.h>\n#include <linux/mii.h>\n#include <linux/ethtool.h>\n#include <linux/if.h>\n#include <linux/if_vlan.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/sctp.h>\n#include <linux/if_ether.h>\n#include <linux/prefetch.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <linux/pm_runtime.h>\n#include <linux/etherdevice.h>\n#ifdef CONFIG_IGB_DCA\n#include <linux/dca.h>\n#endif\n#include <linux/i2c.h>\n#include \"igb.h\"\n\nenum queue_mode {\n\tQUEUE_MODE_STRICT_PRIORITY,\n\tQUEUE_MODE_STREAM_RESERVATION,\n};\n\nenum tx_queue_prio {\n\tTX_QUEUE_PRIO_HIGH,\n\tTX_QUEUE_PRIO_LOW,\n};\n\nchar igb_driver_name[] = \"igb\";\nstatic const char igb_driver_string[] =\n\t\t\t\t\"Intel(R) Gigabit Ethernet Network Driver\";\nstatic const char igb_copyright[] =\n\t\t\t\t\"Copyright (c) 2007-2014 Intel Corporation.\";\n\nstatic const struct e1000_info *igb_info_tbl[] = {\n\t[board_82575] = &e1000_82575_info,\n};\n\nstatic const struct pci_device_id igb_pci_tbl[] = {\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_1GBPS) },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_SGMII) },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_2_5GBPS) },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I211_COPPER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_FIBER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SGMII), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER_FLASHLESS), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES_FLASHLESS), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I350_COPPER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I350_FIBER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I350_SERDES), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_I350_SGMII), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_COPPER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_FIBER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_QUAD_FIBER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_SERDES), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_SGMII), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82580_COPPER_DUAL), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_DH89XXCC_SGMII), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_DH89XXCC_SERDES), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_DH89XXCC_BACKPLANE), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_DH89XXCC_SFP), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_NS), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_NS_SERDES), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_FIBER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_SERDES), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_SERDES_QUAD), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_QUAD_COPPER_ET2), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82576_QUAD_COPPER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82575EB_COPPER), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82575EB_FIBER_SERDES), board_82575 },\n\t{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82575GB_QUAD_COPPER), board_82575 },\n\t \n\t{0, }\n};\n\nMODULE_DEVICE_TABLE(pci, igb_pci_tbl);\n\nstatic int igb_setup_all_tx_resources(struct igb_adapter *);\nstatic int igb_setup_all_rx_resources(struct igb_adapter *);\nstatic void igb_free_all_tx_resources(struct igb_adapter *);\nstatic void igb_free_all_rx_resources(struct igb_adapter *);\nstatic void igb_setup_mrqc(struct igb_adapter *);\nstatic int igb_probe(struct pci_dev *, const struct pci_device_id *);\nstatic void igb_remove(struct pci_dev *pdev);\nstatic void igb_init_queue_configuration(struct igb_adapter *adapter);\nstatic int igb_sw_init(struct igb_adapter *);\nint igb_open(struct net_device *);\nint igb_close(struct net_device *);\nstatic void igb_configure(struct igb_adapter *);\nstatic void igb_configure_tx(struct igb_adapter *);\nstatic void igb_configure_rx(struct igb_adapter *);\nstatic void igb_clean_all_tx_rings(struct igb_adapter *);\nstatic void igb_clean_all_rx_rings(struct igb_adapter *);\nstatic void igb_clean_tx_ring(struct igb_ring *);\nstatic void igb_clean_rx_ring(struct igb_ring *);\nstatic void igb_set_rx_mode(struct net_device *);\nstatic void igb_update_phy_info(struct timer_list *);\nstatic void igb_watchdog(struct timer_list *);\nstatic void igb_watchdog_task(struct work_struct *);\nstatic netdev_tx_t igb_xmit_frame(struct sk_buff *skb, struct net_device *);\nstatic void igb_get_stats64(struct net_device *dev,\n\t\t\t    struct rtnl_link_stats64 *stats);\nstatic int igb_change_mtu(struct net_device *, int);\nstatic int igb_set_mac(struct net_device *, void *);\nstatic void igb_set_uta(struct igb_adapter *adapter, bool set);\nstatic irqreturn_t igb_intr(int irq, void *);\nstatic irqreturn_t igb_intr_msi(int irq, void *);\nstatic irqreturn_t igb_msix_other(int irq, void *);\nstatic irqreturn_t igb_msix_ring(int irq, void *);\n#ifdef CONFIG_IGB_DCA\nstatic void igb_update_dca(struct igb_q_vector *);\nstatic void igb_setup_dca(struct igb_adapter *);\n#endif  \nstatic int igb_poll(struct napi_struct *, int);\nstatic bool igb_clean_tx_irq(struct igb_q_vector *, int);\nstatic int igb_clean_rx_irq(struct igb_q_vector *, int);\nstatic int igb_ioctl(struct net_device *, struct ifreq *, int cmd);\nstatic void igb_tx_timeout(struct net_device *, unsigned int txqueue);\nstatic void igb_reset_task(struct work_struct *);\nstatic void igb_vlan_mode(struct net_device *netdev,\n\t\t\t  netdev_features_t features);\nstatic int igb_vlan_rx_add_vid(struct net_device *, __be16, u16);\nstatic int igb_vlan_rx_kill_vid(struct net_device *, __be16, u16);\nstatic void igb_restore_vlan(struct igb_adapter *);\nstatic void igb_rar_set_index(struct igb_adapter *, u32);\nstatic void igb_ping_all_vfs(struct igb_adapter *);\nstatic void igb_msg_task(struct igb_adapter *);\nstatic void igb_vmm_control(struct igb_adapter *);\nstatic int igb_set_vf_mac(struct igb_adapter *, int, unsigned char *);\nstatic void igb_flush_mac_table(struct igb_adapter *);\nstatic int igb_available_rars(struct igb_adapter *, u8);\nstatic void igb_set_default_mac_filter(struct igb_adapter *);\nstatic int igb_uc_sync(struct net_device *, const unsigned char *);\nstatic int igb_uc_unsync(struct net_device *, const unsigned char *);\nstatic void igb_restore_vf_multicasts(struct igb_adapter *adapter);\nstatic int igb_ndo_set_vf_mac(struct net_device *netdev, int vf, u8 *mac);\nstatic int igb_ndo_set_vf_vlan(struct net_device *netdev,\n\t\t\t       int vf, u16 vlan, u8 qos, __be16 vlan_proto);\nstatic int igb_ndo_set_vf_bw(struct net_device *, int, int, int);\nstatic int igb_ndo_set_vf_spoofchk(struct net_device *netdev, int vf,\n\t\t\t\t   bool setting);\nstatic int igb_ndo_set_vf_trust(struct net_device *netdev, int vf,\n\t\t\t\tbool setting);\nstatic int igb_ndo_get_vf_config(struct net_device *netdev, int vf,\n\t\t\t\t struct ifla_vf_info *ivi);\nstatic void igb_check_vf_rate_limit(struct igb_adapter *);\nstatic void igb_nfc_filter_exit(struct igb_adapter *adapter);\nstatic void igb_nfc_filter_restore(struct igb_adapter *adapter);\n\n#ifdef CONFIG_PCI_IOV\nstatic int igb_vf_configure(struct igb_adapter *adapter, int vf);\nstatic int igb_disable_sriov(struct pci_dev *dev, bool reinit);\n#endif\n\nstatic int igb_suspend(struct device *);\nstatic int igb_resume(struct device *);\nstatic int igb_runtime_suspend(struct device *dev);\nstatic int igb_runtime_resume(struct device *dev);\nstatic int igb_runtime_idle(struct device *dev);\n#ifdef CONFIG_PM\nstatic const struct dev_pm_ops igb_pm_ops = {\n\tSET_SYSTEM_SLEEP_PM_OPS(igb_suspend, igb_resume)\n\tSET_RUNTIME_PM_OPS(igb_runtime_suspend, igb_runtime_resume,\n\t\t\tigb_runtime_idle)\n};\n#endif\nstatic void igb_shutdown(struct pci_dev *);\nstatic int igb_pci_sriov_configure(struct pci_dev *dev, int num_vfs);\n#ifdef CONFIG_IGB_DCA\nstatic int igb_notify_dca(struct notifier_block *, unsigned long, void *);\nstatic struct notifier_block dca_notifier = {\n\t.notifier_call\t= igb_notify_dca,\n\t.next\t\t= NULL,\n\t.priority\t= 0\n};\n#endif\n#ifdef CONFIG_PCI_IOV\nstatic unsigned int max_vfs;\nmodule_param(max_vfs, uint, 0);\nMODULE_PARM_DESC(max_vfs, \"Maximum number of virtual functions to allocate per physical function\");\n#endif  \n\nstatic pci_ers_result_t igb_io_error_detected(struct pci_dev *,\n\t\t     pci_channel_state_t);\nstatic pci_ers_result_t igb_io_slot_reset(struct pci_dev *);\nstatic void igb_io_resume(struct pci_dev *);\n\nstatic const struct pci_error_handlers igb_err_handler = {\n\t.error_detected = igb_io_error_detected,\n\t.slot_reset = igb_io_slot_reset,\n\t.resume = igb_io_resume,\n};\n\nstatic void igb_init_dmac(struct igb_adapter *adapter, u32 pba);\n\nstatic struct pci_driver igb_driver = {\n\t.name     = igb_driver_name,\n\t.id_table = igb_pci_tbl,\n\t.probe    = igb_probe,\n\t.remove   = igb_remove,\n#ifdef CONFIG_PM\n\t.driver.pm = &igb_pm_ops,\n#endif\n\t.shutdown = igb_shutdown,\n\t.sriov_configure = igb_pci_sriov_configure,\n\t.err_handler = &igb_err_handler\n};\n\nMODULE_AUTHOR(\"Intel Corporation, <e1000-devel@lists.sourceforge.net>\");\nMODULE_DESCRIPTION(\"Intel(R) Gigabit Ethernet Network Driver\");\nMODULE_LICENSE(\"GPL v2\");\n\n#define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV|NETIF_MSG_PROBE|NETIF_MSG_LINK)\nstatic int debug = -1;\nmodule_param(debug, int, 0);\nMODULE_PARM_DESC(debug, \"Debug level (0=none,...,16=all)\");\n\nstruct igb_reg_info {\n\tu32 ofs;\n\tchar *name;\n};\n\nstatic const struct igb_reg_info igb_reg_info_tbl[] = {\n\n\t \n\t{E1000_CTRL, \"CTRL\"},\n\t{E1000_STATUS, \"STATUS\"},\n\t{E1000_CTRL_EXT, \"CTRL_EXT\"},\n\n\t \n\t{E1000_ICR, \"ICR\"},\n\n\t \n\t{E1000_RCTL, \"RCTL\"},\n\t{E1000_RDLEN(0), \"RDLEN\"},\n\t{E1000_RDH(0), \"RDH\"},\n\t{E1000_RDT(0), \"RDT\"},\n\t{E1000_RXDCTL(0), \"RXDCTL\"},\n\t{E1000_RDBAL(0), \"RDBAL\"},\n\t{E1000_RDBAH(0), \"RDBAH\"},\n\n\t \n\t{E1000_TCTL, \"TCTL\"},\n\t{E1000_TDBAL(0), \"TDBAL\"},\n\t{E1000_TDBAH(0), \"TDBAH\"},\n\t{E1000_TDLEN(0), \"TDLEN\"},\n\t{E1000_TDH(0), \"TDH\"},\n\t{E1000_TDT(0), \"TDT\"},\n\t{E1000_TXDCTL(0), \"TXDCTL\"},\n\t{E1000_TDFH, \"TDFH\"},\n\t{E1000_TDFT, \"TDFT\"},\n\t{E1000_TDFHS, \"TDFHS\"},\n\t{E1000_TDFPC, \"TDFPC\"},\n\n\t \n\t{}\n};\n\n \nstatic void igb_regdump(struct e1000_hw *hw, struct igb_reg_info *reginfo)\n{\n\tint n = 0;\n\tchar rname[16];\n\tu32 regs[8];\n\n\tswitch (reginfo->ofs) {\n\tcase E1000_RDLEN(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_RDLEN(n));\n\t\tbreak;\n\tcase E1000_RDH(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_RDH(n));\n\t\tbreak;\n\tcase E1000_RDT(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_RDT(n));\n\t\tbreak;\n\tcase E1000_RXDCTL(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_RXDCTL(n));\n\t\tbreak;\n\tcase E1000_RDBAL(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_RDBAL(n));\n\t\tbreak;\n\tcase E1000_RDBAH(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_RDBAH(n));\n\t\tbreak;\n\tcase E1000_TDBAL(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_TDBAL(n));\n\t\tbreak;\n\tcase E1000_TDBAH(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_TDBAH(n));\n\t\tbreak;\n\tcase E1000_TDLEN(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_TDLEN(n));\n\t\tbreak;\n\tcase E1000_TDH(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_TDH(n));\n\t\tbreak;\n\tcase E1000_TDT(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_TDT(n));\n\t\tbreak;\n\tcase E1000_TXDCTL(0):\n\t\tfor (n = 0; n < 4; n++)\n\t\t\tregs[n] = rd32(E1000_TXDCTL(n));\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"%-15s %08x\\n\", reginfo->name, rd32(reginfo->ofs));\n\t\treturn;\n\t}\n\n\tsnprintf(rname, 16, \"%s%s\", reginfo->name, \"[0-3]\");\n\tpr_info(\"%-15s %08x %08x %08x %08x\\n\", rname, regs[0], regs[1],\n\t\tregs[2], regs[3]);\n}\n\n \nstatic void igb_dump(struct igb_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct igb_reg_info *reginfo;\n\tstruct igb_ring *tx_ring;\n\tunion e1000_adv_tx_desc *tx_desc;\n\tstruct my_u0 { __le64 a; __le64 b; } *u0;\n\tstruct igb_ring *rx_ring;\n\tunion e1000_adv_rx_desc *rx_desc;\n\tu32 staterr;\n\tu16 i, n;\n\n\tif (!netif_msg_hw(adapter))\n\t\treturn;\n\n\t \n\tif (netdev) {\n\t\tdev_info(&adapter->pdev->dev, \"Net device Info\\n\");\n\t\tpr_info(\"Device Name     state            trans_start\\n\");\n\t\tpr_info(\"%-15s %016lX %016lX\\n\", netdev->name,\n\t\t\tnetdev->state, dev_trans_start(netdev));\n\t}\n\n\t \n\tdev_info(&adapter->pdev->dev, \"Register Dump\\n\");\n\tpr_info(\" Register Name   Value\\n\");\n\tfor (reginfo = (struct igb_reg_info *)igb_reg_info_tbl;\n\t     reginfo->name; reginfo++) {\n\t\tigb_regdump(hw, reginfo);\n\t}\n\n\t \n\tif (!netdev || !netif_running(netdev))\n\t\tgoto exit;\n\n\tdev_info(&adapter->pdev->dev, \"TX Rings Summary\\n\");\n\tpr_info(\"Queue [NTU] [NTC] [bi(ntc)->dma  ] leng ntw timestamp\\n\");\n\tfor (n = 0; n < adapter->num_tx_queues; n++) {\n\t\tstruct igb_tx_buffer *buffer_info;\n\t\ttx_ring = adapter->tx_ring[n];\n\t\tbuffer_info = &tx_ring->tx_buffer_info[tx_ring->next_to_clean];\n\t\tpr_info(\" %5d %5X %5X %016llX %04X %p %016llX\\n\",\n\t\t\tn, tx_ring->next_to_use, tx_ring->next_to_clean,\n\t\t\t(u64)dma_unmap_addr(buffer_info, dma),\n\t\t\tdma_unmap_len(buffer_info, len),\n\t\t\tbuffer_info->next_to_watch,\n\t\t\t(u64)buffer_info->time_stamp);\n\t}\n\n\t \n\tif (!netif_msg_tx_done(adapter))\n\t\tgoto rx_ring_summary;\n\n\tdev_info(&adapter->pdev->dev, \"TX Rings Dump\\n\");\n\n\t \n\n\tfor (n = 0; n < adapter->num_tx_queues; n++) {\n\t\ttx_ring = adapter->tx_ring[n];\n\t\tpr_info(\"------------------------------------\\n\");\n\t\tpr_info(\"TX QUEUE INDEX = %d\\n\", tx_ring->queue_index);\n\t\tpr_info(\"------------------------------------\\n\");\n\t\tpr_info(\"T [desc]     [address 63:0  ] [PlPOCIStDDM Ln] [bi->dma       ] leng  ntw timestamp        bi->skb\\n\");\n\n\t\tfor (i = 0; tx_ring->desc && (i < tx_ring->count); i++) {\n\t\t\tconst char *next_desc;\n\t\t\tstruct igb_tx_buffer *buffer_info;\n\t\t\ttx_desc = IGB_TX_DESC(tx_ring, i);\n\t\t\tbuffer_info = &tx_ring->tx_buffer_info[i];\n\t\t\tu0 = (struct my_u0 *)tx_desc;\n\t\t\tif (i == tx_ring->next_to_use &&\n\t\t\t    i == tx_ring->next_to_clean)\n\t\t\t\tnext_desc = \" NTC/U\";\n\t\t\telse if (i == tx_ring->next_to_use)\n\t\t\t\tnext_desc = \" NTU\";\n\t\t\telse if (i == tx_ring->next_to_clean)\n\t\t\t\tnext_desc = \" NTC\";\n\t\t\telse\n\t\t\t\tnext_desc = \"\";\n\n\t\t\tpr_info(\"T [0x%03X]    %016llX %016llX %016llX %04X  %p %016llX %p%s\\n\",\n\t\t\t\ti, le64_to_cpu(u0->a),\n\t\t\t\tle64_to_cpu(u0->b),\n\t\t\t\t(u64)dma_unmap_addr(buffer_info, dma),\n\t\t\t\tdma_unmap_len(buffer_info, len),\n\t\t\t\tbuffer_info->next_to_watch,\n\t\t\t\t(u64)buffer_info->time_stamp,\n\t\t\t\tbuffer_info->skb, next_desc);\n\n\t\t\tif (netif_msg_pktdata(adapter) && buffer_info->skb)\n\t\t\t\tprint_hex_dump(KERN_INFO, \"\",\n\t\t\t\t\tDUMP_PREFIX_ADDRESS,\n\t\t\t\t\t16, 1, buffer_info->skb->data,\n\t\t\t\t\tdma_unmap_len(buffer_info, len),\n\t\t\t\t\ttrue);\n\t\t}\n\t}\n\n\t \nrx_ring_summary:\n\tdev_info(&adapter->pdev->dev, \"RX Rings Summary\\n\");\n\tpr_info(\"Queue [NTU] [NTC]\\n\");\n\tfor (n = 0; n < adapter->num_rx_queues; n++) {\n\t\trx_ring = adapter->rx_ring[n];\n\t\tpr_info(\" %5d %5X %5X\\n\",\n\t\t\tn, rx_ring->next_to_use, rx_ring->next_to_clean);\n\t}\n\n\t \n\tif (!netif_msg_rx_status(adapter))\n\t\tgoto exit;\n\n\tdev_info(&adapter->pdev->dev, \"RX Rings Dump\\n\");\n\n\t \n\n\tfor (n = 0; n < adapter->num_rx_queues; n++) {\n\t\trx_ring = adapter->rx_ring[n];\n\t\tpr_info(\"------------------------------------\\n\");\n\t\tpr_info(\"RX QUEUE INDEX = %d\\n\", rx_ring->queue_index);\n\t\tpr_info(\"------------------------------------\\n\");\n\t\tpr_info(\"R  [desc]      [ PktBuf     A0] [  HeadBuf   DD] [bi->dma       ] [bi->skb] <-- Adv Rx Read format\\n\");\n\t\tpr_info(\"RWB[desc]      [PcsmIpSHl PtRs] [vl er S cks ln] ---------------- [bi->skb] <-- Adv Rx Write-Back format\\n\");\n\n\t\tfor (i = 0; i < rx_ring->count; i++) {\n\t\t\tconst char *next_desc;\n\t\t\tstruct igb_rx_buffer *buffer_info;\n\t\t\tbuffer_info = &rx_ring->rx_buffer_info[i];\n\t\t\trx_desc = IGB_RX_DESC(rx_ring, i);\n\t\t\tu0 = (struct my_u0 *)rx_desc;\n\t\t\tstaterr = le32_to_cpu(rx_desc->wb.upper.status_error);\n\n\t\t\tif (i == rx_ring->next_to_use)\n\t\t\t\tnext_desc = \" NTU\";\n\t\t\telse if (i == rx_ring->next_to_clean)\n\t\t\t\tnext_desc = \" NTC\";\n\t\t\telse\n\t\t\t\tnext_desc = \"\";\n\n\t\t\tif (staterr & E1000_RXD_STAT_DD) {\n\t\t\t\t \n\t\t\t\tpr_info(\"%s[0x%03X]     %016llX %016llX ---------------- %s\\n\",\n\t\t\t\t\t\"RWB\", i,\n\t\t\t\t\tle64_to_cpu(u0->a),\n\t\t\t\t\tle64_to_cpu(u0->b),\n\t\t\t\t\tnext_desc);\n\t\t\t} else {\n\t\t\t\tpr_info(\"%s[0x%03X]     %016llX %016llX %016llX %s\\n\",\n\t\t\t\t\t\"R  \", i,\n\t\t\t\t\tle64_to_cpu(u0->a),\n\t\t\t\t\tle64_to_cpu(u0->b),\n\t\t\t\t\t(u64)buffer_info->dma,\n\t\t\t\t\tnext_desc);\n\n\t\t\t\tif (netif_msg_pktdata(adapter) &&\n\t\t\t\t    buffer_info->dma && buffer_info->page) {\n\t\t\t\t\tprint_hex_dump(KERN_INFO, \"\",\n\t\t\t\t\t  DUMP_PREFIX_ADDRESS,\n\t\t\t\t\t  16, 1,\n\t\t\t\t\t  page_address(buffer_info->page) +\n\t\t\t\t\t\t      buffer_info->page_offset,\n\t\t\t\t\t  igb_rx_bufsz(rx_ring), true);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\nexit:\n\treturn;\n}\n\n \nstatic int igb_get_i2c_data(void *data)\n{\n\tstruct igb_adapter *adapter = (struct igb_adapter *)data;\n\tstruct e1000_hw *hw = &adapter->hw;\n\ts32 i2cctl = rd32(E1000_I2CPARAMS);\n\n\treturn !!(i2cctl & E1000_I2C_DATA_IN);\n}\n\n \nstatic void igb_set_i2c_data(void *data, int state)\n{\n\tstruct igb_adapter *adapter = (struct igb_adapter *)data;\n\tstruct e1000_hw *hw = &adapter->hw;\n\ts32 i2cctl = rd32(E1000_I2CPARAMS);\n\n\tif (state) {\n\t\ti2cctl |= E1000_I2C_DATA_OUT | E1000_I2C_DATA_OE_N;\n\t} else {\n\t\ti2cctl &= ~E1000_I2C_DATA_OE_N;\n\t\ti2cctl &= ~E1000_I2C_DATA_OUT;\n\t}\n\n\twr32(E1000_I2CPARAMS, i2cctl);\n\twrfl();\n}\n\n \nstatic void igb_set_i2c_clk(void *data, int state)\n{\n\tstruct igb_adapter *adapter = (struct igb_adapter *)data;\n\tstruct e1000_hw *hw = &adapter->hw;\n\ts32 i2cctl = rd32(E1000_I2CPARAMS);\n\n\tif (state) {\n\t\ti2cctl |= E1000_I2C_CLK_OUT | E1000_I2C_CLK_OE_N;\n\t} else {\n\t\ti2cctl &= ~E1000_I2C_CLK_OUT;\n\t\ti2cctl &= ~E1000_I2C_CLK_OE_N;\n\t}\n\twr32(E1000_I2CPARAMS, i2cctl);\n\twrfl();\n}\n\n \nstatic int igb_get_i2c_clk(void *data)\n{\n\tstruct igb_adapter *adapter = (struct igb_adapter *)data;\n\tstruct e1000_hw *hw = &adapter->hw;\n\ts32 i2cctl = rd32(E1000_I2CPARAMS);\n\n\treturn !!(i2cctl & E1000_I2C_CLK_IN);\n}\n\nstatic const struct i2c_algo_bit_data igb_i2c_algo = {\n\t.setsda\t\t= igb_set_i2c_data,\n\t.setscl\t\t= igb_set_i2c_clk,\n\t.getsda\t\t= igb_get_i2c_data,\n\t.getscl\t\t= igb_get_i2c_clk,\n\t.udelay\t\t= 5,\n\t.timeout\t= 20,\n};\n\n \nstruct net_device *igb_get_hw_dev(struct e1000_hw *hw)\n{\n\tstruct igb_adapter *adapter = hw->back;\n\treturn adapter->netdev;\n}\n\n \nstatic int __init igb_init_module(void)\n{\n\tint ret;\n\n\tpr_info(\"%s\\n\", igb_driver_string);\n\tpr_info(\"%s\\n\", igb_copyright);\n\n#ifdef CONFIG_IGB_DCA\n\tdca_register_notify(&dca_notifier);\n#endif\n\tret = pci_register_driver(&igb_driver);\n\treturn ret;\n}\n\nmodule_init(igb_init_module);\n\n \nstatic void __exit igb_exit_module(void)\n{\n#ifdef CONFIG_IGB_DCA\n\tdca_unregister_notify(&dca_notifier);\n#endif\n\tpci_unregister_driver(&igb_driver);\n}\n\nmodule_exit(igb_exit_module);\n\n#define Q_IDX_82576(i) (((i & 0x1) << 3) + (i >> 1))\n \nstatic void igb_cache_ring_register(struct igb_adapter *adapter)\n{\n\tint i = 0, j = 0;\n\tu32 rbase_offset = adapter->vfs_allocated_count;\n\n\tswitch (adapter->hw.mac.type) {\n\tcase e1000_82576:\n\t\t \n\t\tif (adapter->vfs_allocated_count) {\n\t\t\tfor (; i < adapter->rss_queues; i++)\n\t\t\t\tadapter->rx_ring[i]->reg_idx = rbase_offset +\n\t\t\t\t\t\t\t       Q_IDX_82576(i);\n\t\t}\n\t\tfallthrough;\n\tcase e1000_82575:\n\tcase e1000_82580:\n\tcase e1000_i350:\n\tcase e1000_i354:\n\tcase e1000_i210:\n\tcase e1000_i211:\n\tdefault:\n\t\tfor (; i < adapter->num_rx_queues; i++)\n\t\t\tadapter->rx_ring[i]->reg_idx = rbase_offset + i;\n\t\tfor (; j < adapter->num_tx_queues; j++)\n\t\t\tadapter->tx_ring[j]->reg_idx = rbase_offset + j;\n\t\tbreak;\n\t}\n}\n\nu32 igb_rd32(struct e1000_hw *hw, u32 reg)\n{\n\tstruct igb_adapter *igb = container_of(hw, struct igb_adapter, hw);\n\tu8 __iomem *hw_addr = READ_ONCE(hw->hw_addr);\n\tu32 value = 0;\n\n\tif (E1000_REMOVED(hw_addr))\n\t\treturn ~value;\n\n\tvalue = readl(&hw_addr[reg]);\n\n\t \n\tif (!(~value) && (!reg || !(~readl(hw_addr)))) {\n\t\tstruct net_device *netdev = igb->netdev;\n\t\thw->hw_addr = NULL;\n\t\tnetdev_err(netdev, \"PCIe link lost\\n\");\n\t\tWARN(pci_device_is_present(igb->pdev),\n\t\t     \"igb: Failed to read reg 0x%x!\\n\", reg);\n\t}\n\n\treturn value;\n}\n\n \nstatic void igb_write_ivar(struct e1000_hw *hw, int msix_vector,\n\t\t\t   int index, int offset)\n{\n\tu32 ivar = array_rd32(E1000_IVAR0, index);\n\n\t \n\tivar &= ~((u32)0xFF << offset);\n\n\t \n\tivar |= (msix_vector | E1000_IVAR_VALID) << offset;\n\n\tarray_wr32(E1000_IVAR0, index, ivar);\n}\n\n#define IGB_N0_QUEUE -1\nstatic void igb_assign_vector(struct igb_q_vector *q_vector, int msix_vector)\n{\n\tstruct igb_adapter *adapter = q_vector->adapter;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint rx_queue = IGB_N0_QUEUE;\n\tint tx_queue = IGB_N0_QUEUE;\n\tu32 msixbm = 0;\n\n\tif (q_vector->rx.ring)\n\t\trx_queue = q_vector->rx.ring->reg_idx;\n\tif (q_vector->tx.ring)\n\t\ttx_queue = q_vector->tx.ring->reg_idx;\n\n\tswitch (hw->mac.type) {\n\tcase e1000_82575:\n\t\t \n\t\tif (rx_queue > IGB_N0_QUEUE)\n\t\t\tmsixbm = E1000_EICR_RX_QUEUE0 << rx_queue;\n\t\tif (tx_queue > IGB_N0_QUEUE)\n\t\t\tmsixbm |= E1000_EICR_TX_QUEUE0 << tx_queue;\n\t\tif (!(adapter->flags & IGB_FLAG_HAS_MSIX) && msix_vector == 0)\n\t\t\tmsixbm |= E1000_EIMS_OTHER;\n\t\tarray_wr32(E1000_MSIXBM(0), msix_vector, msixbm);\n\t\tq_vector->eims_value = msixbm;\n\t\tbreak;\n\tcase e1000_82576:\n\t\t \n\t\tif (rx_queue > IGB_N0_QUEUE)\n\t\t\tigb_write_ivar(hw, msix_vector,\n\t\t\t\t       rx_queue & 0x7,\n\t\t\t\t       (rx_queue & 0x8) << 1);\n\t\tif (tx_queue > IGB_N0_QUEUE)\n\t\t\tigb_write_ivar(hw, msix_vector,\n\t\t\t\t       tx_queue & 0x7,\n\t\t\t\t       ((tx_queue & 0x8) << 1) + 8);\n\t\tq_vector->eims_value = BIT(msix_vector);\n\t\tbreak;\n\tcase e1000_82580:\n\tcase e1000_i350:\n\tcase e1000_i354:\n\tcase e1000_i210:\n\tcase e1000_i211:\n\t\t \n\t\tif (rx_queue > IGB_N0_QUEUE)\n\t\t\tigb_write_ivar(hw, msix_vector,\n\t\t\t\t       rx_queue >> 1,\n\t\t\t\t       (rx_queue & 0x1) << 4);\n\t\tif (tx_queue > IGB_N0_QUEUE)\n\t\t\tigb_write_ivar(hw, msix_vector,\n\t\t\t\t       tx_queue >> 1,\n\t\t\t\t       ((tx_queue & 0x1) << 4) + 8);\n\t\tq_vector->eims_value = BIT(msix_vector);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\t \n\tadapter->eims_enable_mask |= q_vector->eims_value;\n\n\t \n\tq_vector->set_itr = 1;\n}\n\n \nstatic void igb_configure_msix(struct igb_adapter *adapter)\n{\n\tu32 tmp;\n\tint i, vector = 0;\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\tadapter->eims_enable_mask = 0;\n\n\t \n\tswitch (hw->mac.type) {\n\tcase e1000_82575:\n\t\ttmp = rd32(E1000_CTRL_EXT);\n\t\t \n\t\ttmp |= E1000_CTRL_EXT_PBA_CLR;\n\n\t\t \n\t\ttmp |= E1000_CTRL_EXT_EIAME;\n\t\ttmp |= E1000_CTRL_EXT_IRCA;\n\n\t\twr32(E1000_CTRL_EXT, tmp);\n\n\t\t \n\t\tarray_wr32(E1000_MSIXBM(0), vector++, E1000_EIMS_OTHER);\n\t\tadapter->eims_other = E1000_EIMS_OTHER;\n\n\t\tbreak;\n\n\tcase e1000_82576:\n\tcase e1000_82580:\n\tcase e1000_i350:\n\tcase e1000_i354:\n\tcase e1000_i210:\n\tcase e1000_i211:\n\t\t \n\t\twr32(E1000_GPIE, E1000_GPIE_MSIX_MODE |\n\t\t     E1000_GPIE_PBA | E1000_GPIE_EIAME |\n\t\t     E1000_GPIE_NSICR);\n\n\t\t \n\t\tadapter->eims_other = BIT(vector);\n\t\ttmp = (vector++ | E1000_IVAR_VALID) << 8;\n\n\t\twr32(E1000_IVAR_MISC, tmp);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tbreak;\n\t}  \n\n\tadapter->eims_enable_mask |= adapter->eims_other;\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\tigb_assign_vector(adapter->q_vector[i], vector++);\n\n\twrfl();\n}\n\n \nstatic int igb_request_msix(struct igb_adapter *adapter)\n{\n\tunsigned int num_q_vectors = adapter->num_q_vectors;\n\tstruct net_device *netdev = adapter->netdev;\n\tint i, err = 0, vector = 0, free_vector = 0;\n\n\terr = request_irq(adapter->msix_entries[vector].vector,\n\t\t\t  igb_msix_other, 0, netdev->name, adapter);\n\tif (err)\n\t\tgoto err_out;\n\n\tif (num_q_vectors > MAX_Q_VECTORS) {\n\t\tnum_q_vectors = MAX_Q_VECTORS;\n\t\tdev_warn(&adapter->pdev->dev,\n\t\t\t \"The number of queue vectors (%d) is higher than max allowed (%d)\\n\",\n\t\t\t adapter->num_q_vectors, MAX_Q_VECTORS);\n\t}\n\tfor (i = 0; i < num_q_vectors; i++) {\n\t\tstruct igb_q_vector *q_vector = adapter->q_vector[i];\n\n\t\tvector++;\n\n\t\tq_vector->itr_register = adapter->io_addr + E1000_EITR(vector);\n\n\t\tif (q_vector->rx.ring && q_vector->tx.ring)\n\t\t\tsprintf(q_vector->name, \"%s-TxRx-%u\", netdev->name,\n\t\t\t\tq_vector->rx.ring->queue_index);\n\t\telse if (q_vector->tx.ring)\n\t\t\tsprintf(q_vector->name, \"%s-tx-%u\", netdev->name,\n\t\t\t\tq_vector->tx.ring->queue_index);\n\t\telse if (q_vector->rx.ring)\n\t\t\tsprintf(q_vector->name, \"%s-rx-%u\", netdev->name,\n\t\t\t\tq_vector->rx.ring->queue_index);\n\t\telse\n\t\t\tsprintf(q_vector->name, \"%s-unused\", netdev->name);\n\n\t\terr = request_irq(adapter->msix_entries[vector].vector,\n\t\t\t\t  igb_msix_ring, 0, q_vector->name,\n\t\t\t\t  q_vector);\n\t\tif (err)\n\t\t\tgoto err_free;\n\t}\n\n\tigb_configure_msix(adapter);\n\treturn 0;\n\nerr_free:\n\t \n\tfree_irq(adapter->msix_entries[free_vector++].vector, adapter);\n\n\tvector--;\n\tfor (i = 0; i < vector; i++) {\n\t\tfree_irq(adapter->msix_entries[free_vector++].vector,\n\t\t\t adapter->q_vector[i]);\n\t}\nerr_out:\n\treturn err;\n}\n\n \nstatic void igb_free_q_vector(struct igb_adapter *adapter, int v_idx)\n{\n\tstruct igb_q_vector *q_vector = adapter->q_vector[v_idx];\n\n\tadapter->q_vector[v_idx] = NULL;\n\n\t \n\tif (q_vector)\n\t\tkfree_rcu(q_vector, rcu);\n}\n\n \nstatic void igb_reset_q_vector(struct igb_adapter *adapter, int v_idx)\n{\n\tstruct igb_q_vector *q_vector = adapter->q_vector[v_idx];\n\n\t \n\tif (!q_vector)\n\t\treturn;\n\n\tif (q_vector->tx.ring)\n\t\tadapter->tx_ring[q_vector->tx.ring->queue_index] = NULL;\n\n\tif (q_vector->rx.ring)\n\t\tadapter->rx_ring[q_vector->rx.ring->queue_index] = NULL;\n\n\tnetif_napi_del(&q_vector->napi);\n\n}\n\nstatic void igb_reset_interrupt_capability(struct igb_adapter *adapter)\n{\n\tint v_idx = adapter->num_q_vectors;\n\n\tif (adapter->flags & IGB_FLAG_HAS_MSIX)\n\t\tpci_disable_msix(adapter->pdev);\n\telse if (adapter->flags & IGB_FLAG_HAS_MSI)\n\t\tpci_disable_msi(adapter->pdev);\n\n\twhile (v_idx--)\n\t\tigb_reset_q_vector(adapter, v_idx);\n}\n\n \nstatic void igb_free_q_vectors(struct igb_adapter *adapter)\n{\n\tint v_idx = adapter->num_q_vectors;\n\n\tadapter->num_tx_queues = 0;\n\tadapter->num_rx_queues = 0;\n\tadapter->num_q_vectors = 0;\n\n\twhile (v_idx--) {\n\t\tigb_reset_q_vector(adapter, v_idx);\n\t\tigb_free_q_vector(adapter, v_idx);\n\t}\n}\n\n \nstatic void igb_clear_interrupt_scheme(struct igb_adapter *adapter)\n{\n\tigb_free_q_vectors(adapter);\n\tigb_reset_interrupt_capability(adapter);\n}\n\n \nstatic void igb_set_interrupt_capability(struct igb_adapter *adapter, bool msix)\n{\n\tint err;\n\tint numvecs, i;\n\n\tif (!msix)\n\t\tgoto msi_only;\n\tadapter->flags |= IGB_FLAG_HAS_MSIX;\n\n\t \n\tadapter->num_rx_queues = adapter->rss_queues;\n\tif (adapter->vfs_allocated_count)\n\t\tadapter->num_tx_queues = 1;\n\telse\n\t\tadapter->num_tx_queues = adapter->rss_queues;\n\n\t \n\tnumvecs = adapter->num_rx_queues;\n\n\t \n\tif (!(adapter->flags & IGB_FLAG_QUEUE_PAIRS))\n\t\tnumvecs += adapter->num_tx_queues;\n\n\t \n\tadapter->num_q_vectors = numvecs;\n\n\t \n\tnumvecs++;\n\tfor (i = 0; i < numvecs; i++)\n\t\tadapter->msix_entries[i].entry = i;\n\n\terr = pci_enable_msix_range(adapter->pdev,\n\t\t\t\t    adapter->msix_entries,\n\t\t\t\t    numvecs,\n\t\t\t\t    numvecs);\n\tif (err > 0)\n\t\treturn;\n\n\tigb_reset_interrupt_capability(adapter);\n\n\t \nmsi_only:\n\tadapter->flags &= ~IGB_FLAG_HAS_MSIX;\n#ifdef CONFIG_PCI_IOV\n\t \n\tif (adapter->vf_data) {\n\t\tstruct e1000_hw *hw = &adapter->hw;\n\t\t \n\t\tpci_disable_sriov(adapter->pdev);\n\t\tmsleep(500);\n\n\t\tkfree(adapter->vf_mac_list);\n\t\tadapter->vf_mac_list = NULL;\n\t\tkfree(adapter->vf_data);\n\t\tadapter->vf_data = NULL;\n\t\twr32(E1000_IOVCTL, E1000_IOVCTL_REUSE_VFQ);\n\t\twrfl();\n\t\tmsleep(100);\n\t\tdev_info(&adapter->pdev->dev, \"IOV Disabled\\n\");\n\t}\n#endif\n\tadapter->vfs_allocated_count = 0;\n\tadapter->rss_queues = 1;\n\tadapter->flags |= IGB_FLAG_QUEUE_PAIRS;\n\tadapter->num_rx_queues = 1;\n\tadapter->num_tx_queues = 1;\n\tadapter->num_q_vectors = 1;\n\tif (!pci_enable_msi(adapter->pdev))\n\t\tadapter->flags |= IGB_FLAG_HAS_MSI;\n}\n\nstatic void igb_add_ring(struct igb_ring *ring,\n\t\t\t struct igb_ring_container *head)\n{\n\thead->ring = ring;\n\thead->count++;\n}\n\n \nstatic int igb_alloc_q_vector(struct igb_adapter *adapter,\n\t\t\t      int v_count, int v_idx,\n\t\t\t      int txr_count, int txr_idx,\n\t\t\t      int rxr_count, int rxr_idx)\n{\n\tstruct igb_q_vector *q_vector;\n\tstruct igb_ring *ring;\n\tint ring_count;\n\tsize_t size;\n\n\t \n\tif (txr_count > 1 || rxr_count > 1)\n\t\treturn -ENOMEM;\n\n\tring_count = txr_count + rxr_count;\n\tsize = kmalloc_size_roundup(struct_size(q_vector, ring, ring_count));\n\n\t \n\tq_vector = adapter->q_vector[v_idx];\n\tif (!q_vector) {\n\t\tq_vector = kzalloc(size, GFP_KERNEL);\n\t} else if (size > ksize(q_vector)) {\n\t\tstruct igb_q_vector *new_q_vector;\n\n\t\tnew_q_vector = kzalloc(size, GFP_KERNEL);\n\t\tif (new_q_vector)\n\t\t\tkfree_rcu(q_vector, rcu);\n\t\tq_vector = new_q_vector;\n\t} else {\n\t\tmemset(q_vector, 0, size);\n\t}\n\tif (!q_vector)\n\t\treturn -ENOMEM;\n\n\t \n\tnetif_napi_add(adapter->netdev, &q_vector->napi, igb_poll);\n\n\t \n\tadapter->q_vector[v_idx] = q_vector;\n\tq_vector->adapter = adapter;\n\n\t \n\tq_vector->tx.work_limit = adapter->tx_work_limit;\n\n\t \n\tq_vector->itr_register = adapter->io_addr + E1000_EITR(0);\n\tq_vector->itr_val = IGB_START_ITR;\n\n\t \n\tring = q_vector->ring;\n\n\t \n\tif (rxr_count) {\n\t\t \n\t\tif (!adapter->rx_itr_setting || adapter->rx_itr_setting > 3)\n\t\t\tq_vector->itr_val = adapter->rx_itr_setting;\n\t} else {\n\t\t \n\t\tif (!adapter->tx_itr_setting || adapter->tx_itr_setting > 3)\n\t\t\tq_vector->itr_val = adapter->tx_itr_setting;\n\t}\n\n\tif (txr_count) {\n\t\t \n\t\tring->dev = &adapter->pdev->dev;\n\t\tring->netdev = adapter->netdev;\n\n\t\t \n\t\tring->q_vector = q_vector;\n\n\t\t \n\t\tigb_add_ring(ring, &q_vector->tx);\n\n\t\t \n\t\tif (adapter->hw.mac.type == e1000_82575)\n\t\t\tset_bit(IGB_RING_FLAG_TX_CTX_IDX, &ring->flags);\n\n\t\t \n\t\tring->count = adapter->tx_ring_count;\n\t\tring->queue_index = txr_idx;\n\n\t\tring->cbs_enable = false;\n\t\tring->idleslope = 0;\n\t\tring->sendslope = 0;\n\t\tring->hicredit = 0;\n\t\tring->locredit = 0;\n\n\t\tu64_stats_init(&ring->tx_syncp);\n\t\tu64_stats_init(&ring->tx_syncp2);\n\n\t\t \n\t\tadapter->tx_ring[txr_idx] = ring;\n\n\t\t \n\t\tring++;\n\t}\n\n\tif (rxr_count) {\n\t\t \n\t\tring->dev = &adapter->pdev->dev;\n\t\tring->netdev = adapter->netdev;\n\n\t\t \n\t\tring->q_vector = q_vector;\n\n\t\t \n\t\tigb_add_ring(ring, &q_vector->rx);\n\n\t\t \n\t\tif (adapter->hw.mac.type >= e1000_82576)\n\t\t\tset_bit(IGB_RING_FLAG_RX_SCTP_CSUM, &ring->flags);\n\n\t\t \n\t\tif (adapter->hw.mac.type >= e1000_i350)\n\t\t\tset_bit(IGB_RING_FLAG_RX_LB_VLAN_BSWAP, &ring->flags);\n\n\t\t \n\t\tring->count = adapter->rx_ring_count;\n\t\tring->queue_index = rxr_idx;\n\n\t\tu64_stats_init(&ring->rx_syncp);\n\n\t\t \n\t\tadapter->rx_ring[rxr_idx] = ring;\n\t}\n\n\treturn 0;\n}\n\n\n \nstatic int igb_alloc_q_vectors(struct igb_adapter *adapter)\n{\n\tint q_vectors = adapter->num_q_vectors;\n\tint rxr_remaining = adapter->num_rx_queues;\n\tint txr_remaining = adapter->num_tx_queues;\n\tint rxr_idx = 0, txr_idx = 0, v_idx = 0;\n\tint err;\n\n\tif (q_vectors >= (rxr_remaining + txr_remaining)) {\n\t\tfor (; rxr_remaining; v_idx++) {\n\t\t\terr = igb_alloc_q_vector(adapter, q_vectors, v_idx,\n\t\t\t\t\t\t 0, 0, 1, rxr_idx);\n\n\t\t\tif (err)\n\t\t\t\tgoto err_out;\n\n\t\t\t \n\t\t\trxr_remaining--;\n\t\t\trxr_idx++;\n\t\t}\n\t}\n\n\tfor (; v_idx < q_vectors; v_idx++) {\n\t\tint rqpv = DIV_ROUND_UP(rxr_remaining, q_vectors - v_idx);\n\t\tint tqpv = DIV_ROUND_UP(txr_remaining, q_vectors - v_idx);\n\n\t\terr = igb_alloc_q_vector(adapter, q_vectors, v_idx,\n\t\t\t\t\t tqpv, txr_idx, rqpv, rxr_idx);\n\n\t\tif (err)\n\t\t\tgoto err_out;\n\n\t\t \n\t\trxr_remaining -= rqpv;\n\t\ttxr_remaining -= tqpv;\n\t\trxr_idx++;\n\t\ttxr_idx++;\n\t}\n\n\treturn 0;\n\nerr_out:\n\tadapter->num_tx_queues = 0;\n\tadapter->num_rx_queues = 0;\n\tadapter->num_q_vectors = 0;\n\n\twhile (v_idx--)\n\t\tigb_free_q_vector(adapter, v_idx);\n\n\treturn -ENOMEM;\n}\n\n \nstatic int igb_init_interrupt_scheme(struct igb_adapter *adapter, bool msix)\n{\n\tstruct pci_dev *pdev = adapter->pdev;\n\tint err;\n\n\tigb_set_interrupt_capability(adapter, msix);\n\n\terr = igb_alloc_q_vectors(adapter);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Unable to allocate memory for vectors\\n\");\n\t\tgoto err_alloc_q_vectors;\n\t}\n\n\tigb_cache_ring_register(adapter);\n\n\treturn 0;\n\nerr_alloc_q_vectors:\n\tigb_reset_interrupt_capability(adapter);\n\treturn err;\n}\n\n \nstatic int igb_request_irq(struct igb_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tint err = 0;\n\n\tif (adapter->flags & IGB_FLAG_HAS_MSIX) {\n\t\terr = igb_request_msix(adapter);\n\t\tif (!err)\n\t\t\tgoto request_done;\n\t\t \n\t\tigb_free_all_tx_resources(adapter);\n\t\tigb_free_all_rx_resources(adapter);\n\n\t\tigb_clear_interrupt_scheme(adapter);\n\t\terr = igb_init_interrupt_scheme(adapter, false);\n\t\tif (err)\n\t\t\tgoto request_done;\n\n\t\tigb_setup_all_tx_resources(adapter);\n\t\tigb_setup_all_rx_resources(adapter);\n\t\tigb_configure(adapter);\n\t}\n\n\tigb_assign_vector(adapter->q_vector[0], 0);\n\n\tif (adapter->flags & IGB_FLAG_HAS_MSI) {\n\t\terr = request_irq(pdev->irq, igb_intr_msi, 0,\n\t\t\t\t  netdev->name, adapter);\n\t\tif (!err)\n\t\t\tgoto request_done;\n\n\t\t \n\t\tigb_reset_interrupt_capability(adapter);\n\t\tadapter->flags &= ~IGB_FLAG_HAS_MSI;\n\t}\n\n\terr = request_irq(pdev->irq, igb_intr, IRQF_SHARED,\n\t\t\t  netdev->name, adapter);\n\n\tif (err)\n\t\tdev_err(&pdev->dev, \"Error %d getting interrupt\\n\",\n\t\t\terr);\n\nrequest_done:\n\treturn err;\n}\n\nstatic void igb_free_irq(struct igb_adapter *adapter)\n{\n\tif (adapter->flags & IGB_FLAG_HAS_MSIX) {\n\t\tint vector = 0, i;\n\n\t\tfree_irq(adapter->msix_entries[vector++].vector, adapter);\n\n\t\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\t\tfree_irq(adapter->msix_entries[vector++].vector,\n\t\t\t\t adapter->q_vector[i]);\n\t} else {\n\t\tfree_irq(adapter->pdev->irq, adapter);\n\t}\n}\n\n \nstatic void igb_irq_disable(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\t \n\tif (adapter->flags & IGB_FLAG_HAS_MSIX) {\n\t\tu32 regval = rd32(E1000_EIAM);\n\n\t\twr32(E1000_EIAM, regval & ~adapter->eims_enable_mask);\n\t\twr32(E1000_EIMC, adapter->eims_enable_mask);\n\t\tregval = rd32(E1000_EIAC);\n\t\twr32(E1000_EIAC, regval & ~adapter->eims_enable_mask);\n\t}\n\n\twr32(E1000_IAM, 0);\n\twr32(E1000_IMC, ~0);\n\twrfl();\n\tif (adapter->flags & IGB_FLAG_HAS_MSIX) {\n\t\tint i;\n\n\t\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\t\tsynchronize_irq(adapter->msix_entries[i].vector);\n\t} else {\n\t\tsynchronize_irq(adapter->pdev->irq);\n\t}\n}\n\n \nstatic void igb_irq_enable(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\tif (adapter->flags & IGB_FLAG_HAS_MSIX) {\n\t\tu32 ims = E1000_IMS_LSC | E1000_IMS_DOUTSYNC | E1000_IMS_DRSTA;\n\t\tu32 regval = rd32(E1000_EIAC);\n\n\t\twr32(E1000_EIAC, regval | adapter->eims_enable_mask);\n\t\tregval = rd32(E1000_EIAM);\n\t\twr32(E1000_EIAM, regval | adapter->eims_enable_mask);\n\t\twr32(E1000_EIMS, adapter->eims_enable_mask);\n\t\tif (adapter->vfs_allocated_count) {\n\t\t\twr32(E1000_MBVFIMR, 0xFF);\n\t\t\tims |= E1000_IMS_VMMB;\n\t\t}\n\t\twr32(E1000_IMS, ims);\n\t} else {\n\t\twr32(E1000_IMS, IMS_ENABLE_MASK |\n\t\t\t\tE1000_IMS_DRSTA);\n\t\twr32(E1000_IAM, IMS_ENABLE_MASK |\n\t\t\t\tE1000_IMS_DRSTA);\n\t}\n}\n\nstatic void igb_update_mng_vlan(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu16 pf_id = adapter->vfs_allocated_count;\n\tu16 vid = adapter->hw.mng_cookie.vlan_id;\n\tu16 old_vid = adapter->mng_vlan_id;\n\n\tif (hw->mng_cookie.status & E1000_MNG_DHCP_COOKIE_STATUS_VLAN) {\n\t\t \n\t\tigb_vfta_set(hw, vid, pf_id, true, true);\n\t\tadapter->mng_vlan_id = vid;\n\t} else {\n\t\tadapter->mng_vlan_id = IGB_MNG_VLAN_NONE;\n\t}\n\n\tif ((old_vid != (u16)IGB_MNG_VLAN_NONE) &&\n\t    (vid != old_vid) &&\n\t    !test_bit(old_vid, adapter->active_vlans)) {\n\t\t \n\t\tigb_vfta_set(hw, vid, pf_id, false, true);\n\t}\n}\n\n \nstatic void igb_release_hw_control(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 ctrl_ext;\n\n\t \n\tctrl_ext = rd32(E1000_CTRL_EXT);\n\twr32(E1000_CTRL_EXT,\n\t\t\tctrl_ext & ~E1000_CTRL_EXT_DRV_LOAD);\n}\n\n \nstatic void igb_get_hw_control(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 ctrl_ext;\n\n\t \n\tctrl_ext = rd32(E1000_CTRL_EXT);\n\twr32(E1000_CTRL_EXT,\n\t\t\tctrl_ext | E1000_CTRL_EXT_DRV_LOAD);\n}\n\nstatic void enable_fqtss(struct igb_adapter *adapter, bool enable)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\tWARN_ON(hw->mac.type != e1000_i210);\n\n\tif (enable)\n\t\tadapter->flags |= IGB_FLAG_FQTSS;\n\telse\n\t\tadapter->flags &= ~IGB_FLAG_FQTSS;\n\n\tif (netif_running(netdev))\n\t\tschedule_work(&adapter->reset_task);\n}\n\nstatic bool is_fqtss_enabled(struct igb_adapter *adapter)\n{\n\treturn (adapter->flags & IGB_FLAG_FQTSS) ? true : false;\n}\n\nstatic void set_tx_desc_fetch_prio(struct e1000_hw *hw, int queue,\n\t\t\t\t   enum tx_queue_prio prio)\n{\n\tu32 val;\n\n\tWARN_ON(hw->mac.type != e1000_i210);\n\tWARN_ON(queue < 0 || queue > 4);\n\n\tval = rd32(E1000_I210_TXDCTL(queue));\n\n\tif (prio == TX_QUEUE_PRIO_HIGH)\n\t\tval |= E1000_TXDCTL_PRIORITY;\n\telse\n\t\tval &= ~E1000_TXDCTL_PRIORITY;\n\n\twr32(E1000_I210_TXDCTL(queue), val);\n}\n\nstatic void set_queue_mode(struct e1000_hw *hw, int queue, enum queue_mode mode)\n{\n\tu32 val;\n\n\tWARN_ON(hw->mac.type != e1000_i210);\n\tWARN_ON(queue < 0 || queue > 1);\n\n\tval = rd32(E1000_I210_TQAVCC(queue));\n\n\tif (mode == QUEUE_MODE_STREAM_RESERVATION)\n\t\tval |= E1000_TQAVCC_QUEUEMODE;\n\telse\n\t\tval &= ~E1000_TQAVCC_QUEUEMODE;\n\n\twr32(E1000_I210_TQAVCC(queue), val);\n}\n\nstatic bool is_any_cbs_enabled(struct igb_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tif (adapter->tx_ring[i]->cbs_enable)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool is_any_txtime_enabled(struct igb_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tif (adapter->tx_ring[i]->launchtime_enable)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic void igb_config_tx_modes(struct igb_adapter *adapter, int queue)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct igb_ring *ring;\n\tu32 tqavcc, tqavctrl;\n\tu16 value;\n\n\tWARN_ON(hw->mac.type != e1000_i210);\n\tWARN_ON(queue < 0 || queue > 1);\n\tring = adapter->tx_ring[queue];\n\n\t \n\tif (ring->cbs_enable || ring->launchtime_enable) {\n\t\tset_tx_desc_fetch_prio(hw, queue, TX_QUEUE_PRIO_HIGH);\n\t\tset_queue_mode(hw, queue, QUEUE_MODE_STREAM_RESERVATION);\n\t} else {\n\t\tset_tx_desc_fetch_prio(hw, queue, TX_QUEUE_PRIO_LOW);\n\t\tset_queue_mode(hw, queue, QUEUE_MODE_STRICT_PRIORITY);\n\t}\n\n\t \n\tif (ring->cbs_enable || queue == 0) {\n\t\t \n\t\tif (queue == 0 && !ring->cbs_enable) {\n\t\t\t \n\t\t\tring->idleslope = 1000000;\n\t\t\tring->hicredit = ETH_FRAME_LEN;\n\t\t}\n\n\t\t \n\t\ttqavctrl = rd32(E1000_I210_TQAVCTRL);\n\t\ttqavctrl |= E1000_TQAVCTRL_DATATRANARB;\n\t\twr32(E1000_I210_TQAVCTRL, tqavctrl);\n\n\t\t \n\t\tvalue = DIV_ROUND_UP_ULL(ring->idleslope * 61034ULL, 1000000);\n\n\t\ttqavcc = rd32(E1000_I210_TQAVCC(queue));\n\t\ttqavcc &= ~E1000_TQAVCC_IDLESLOPE_MASK;\n\t\ttqavcc |= value;\n\t\twr32(E1000_I210_TQAVCC(queue), tqavcc);\n\n\t\twr32(E1000_I210_TQAVHC(queue),\n\t\t     0x80000000 + ring->hicredit * 0x7735);\n\t} else {\n\n\t\t \n\t\ttqavcc = rd32(E1000_I210_TQAVCC(queue));\n\t\ttqavcc &= ~E1000_TQAVCC_IDLESLOPE_MASK;\n\t\twr32(E1000_I210_TQAVCC(queue), tqavcc);\n\n\t\t \n\t\twr32(E1000_I210_TQAVHC(queue), 0);\n\n\t\t \n\t\tif (!is_any_cbs_enabled(adapter)) {\n\t\t\ttqavctrl = rd32(E1000_I210_TQAVCTRL);\n\t\t\ttqavctrl &= ~E1000_TQAVCTRL_DATATRANARB;\n\t\t\twr32(E1000_I210_TQAVCTRL, tqavctrl);\n\t\t}\n\t}\n\n\t \n\tif (ring->launchtime_enable) {\n\t\t \n\t\ttqavctrl = rd32(E1000_I210_TQAVCTRL);\n\t\ttqavctrl |= E1000_TQAVCTRL_DATATRANTIM |\n\t\t       E1000_TQAVCTRL_FETCHTIME_DELTA;\n\t\twr32(E1000_I210_TQAVCTRL, tqavctrl);\n\t} else {\n\t\t \n\t\tif (!is_any_txtime_enabled(adapter)) {\n\t\t\ttqavctrl = rd32(E1000_I210_TQAVCTRL);\n\t\t\ttqavctrl &= ~E1000_TQAVCTRL_DATATRANTIM;\n\t\t\ttqavctrl &= ~E1000_TQAVCTRL_FETCHTIME_DELTA;\n\t\t\twr32(E1000_I210_TQAVCTRL, tqavctrl);\n\t\t}\n\t}\n\n\t \n\n\tnetdev_dbg(netdev, \"Qav Tx mode: cbs %s, launchtime %s, queue %d idleslope %d sendslope %d hiCredit %d locredit %d\\n\",\n\t\t   ring->cbs_enable ? \"enabled\" : \"disabled\",\n\t\t   ring->launchtime_enable ? \"enabled\" : \"disabled\",\n\t\t   queue,\n\t\t   ring->idleslope, ring->sendslope,\n\t\t   ring->hicredit, ring->locredit);\n}\n\nstatic int igb_save_txtime_params(struct igb_adapter *adapter, int queue,\n\t\t\t\t  bool enable)\n{\n\tstruct igb_ring *ring;\n\n\tif (queue < 0 || queue > adapter->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tring = adapter->tx_ring[queue];\n\tring->launchtime_enable = enable;\n\n\treturn 0;\n}\n\nstatic int igb_save_cbs_params(struct igb_adapter *adapter, int queue,\n\t\t\t       bool enable, int idleslope, int sendslope,\n\t\t\t       int hicredit, int locredit)\n{\n\tstruct igb_ring *ring;\n\n\tif (queue < 0 || queue > adapter->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tring = adapter->tx_ring[queue];\n\n\tring->cbs_enable = enable;\n\tring->idleslope = idleslope;\n\tring->sendslope = sendslope;\n\tring->hicredit = hicredit;\n\tring->locredit = locredit;\n\n\treturn 0;\n}\n\n \nstatic void igb_setup_tx_mode(struct igb_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 val;\n\n\t \n\tif (hw->mac.type != e1000_i210)\n\t\treturn;\n\n\tif (is_fqtss_enabled(adapter)) {\n\t\tint i, max_queue;\n\n\t\t \n\t\tval = rd32(E1000_I210_TQAVCTRL);\n\t\tval |= E1000_TQAVCTRL_XMIT_MODE | E1000_TQAVCTRL_SP_WAIT_SR;\n\t\tval &= ~E1000_TQAVCTRL_DATAFETCHARB;\n\t\twr32(E1000_I210_TQAVCTRL, val);\n\n\t\t \n\t\tval = rd32(E1000_TXPBS);\n\t\tval &= ~I210_TXPBSIZE_MASK;\n\t\tval |= I210_TXPBSIZE_PB0_6KB | I210_TXPBSIZE_PB1_6KB |\n\t\t\tI210_TXPBSIZE_PB2_6KB | I210_TXPBSIZE_PB3_6KB;\n\t\twr32(E1000_TXPBS, val);\n\n\t\tval = rd32(E1000_RXPBS);\n\t\tval &= ~I210_RXPBSIZE_MASK;\n\t\tval |= I210_RXPBSIZE_PB_30KB;\n\t\twr32(E1000_RXPBS, val);\n\n\t\t \n\t\tval = (4096 - 1) / 64;\n\t\twr32(E1000_I210_DTXMXPKTSZ, val);\n\n\t\t \n\t\tmax_queue = (adapter->num_tx_queues < I210_SR_QUEUES_NUM) ?\n\t\t\t    adapter->num_tx_queues : I210_SR_QUEUES_NUM;\n\n\t\tfor (i = 0; i < max_queue; i++) {\n\t\t\tigb_config_tx_modes(adapter, i);\n\t\t}\n\t} else {\n\t\twr32(E1000_RXPBS, I210_RXPBSIZE_DEFAULT);\n\t\twr32(E1000_TXPBS, I210_TXPBSIZE_DEFAULT);\n\t\twr32(E1000_I210_DTXMXPKTSZ, I210_DTXMXPKTSZ_DEFAULT);\n\n\t\tval = rd32(E1000_I210_TQAVCTRL);\n\t\t \n\t\tval &= ~E1000_TQAVCTRL_XMIT_MODE;\n\t\twr32(E1000_I210_TQAVCTRL, val);\n\t}\n\n\tnetdev_dbg(netdev, \"FQTSS %s\\n\", (is_fqtss_enabled(adapter)) ?\n\t\t   \"enabled\" : \"disabled\");\n}\n\n \nstatic void igb_configure(struct igb_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tint i;\n\n\tigb_get_hw_control(adapter);\n\tigb_set_rx_mode(netdev);\n\tigb_setup_tx_mode(adapter);\n\n\tigb_restore_vlan(adapter);\n\n\tigb_setup_tctl(adapter);\n\tigb_setup_mrqc(adapter);\n\tigb_setup_rctl(adapter);\n\n\tigb_nfc_filter_restore(adapter);\n\tigb_configure_tx(adapter);\n\tigb_configure_rx(adapter);\n\n\tigb_rx_fifo_flush_82575(&adapter->hw);\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct igb_ring *ring = adapter->rx_ring[i];\n\t\tigb_alloc_rx_buffers(ring, igb_desc_unused(ring));\n\t}\n}\n\n \nvoid igb_power_up_link(struct igb_adapter *adapter)\n{\n\tigb_reset_phy(&adapter->hw);\n\n\tif (adapter->hw.phy.media_type == e1000_media_type_copper)\n\t\tigb_power_up_phy_copper(&adapter->hw);\n\telse\n\t\tigb_power_up_serdes_link_82575(&adapter->hw);\n\n\tigb_setup_link(&adapter->hw);\n}\n\n \nstatic void igb_power_down_link(struct igb_adapter *adapter)\n{\n\tif (adapter->hw.phy.media_type == e1000_media_type_copper)\n\t\tigb_power_down_phy_copper_82575(&adapter->hw);\n\telse\n\t\tigb_shutdown_serdes_link_82575(&adapter->hw);\n}\n\n \nstatic void igb_check_swap_media(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 ctrl_ext, connsw;\n\tbool swap_now = false;\n\n\tctrl_ext = rd32(E1000_CTRL_EXT);\n\tconnsw = rd32(E1000_CONNSW);\n\n\t \n\n\tif ((hw->phy.media_type == e1000_media_type_copper) &&\n\t    (!(connsw & E1000_CONNSW_AUTOSENSE_EN))) {\n\t\tswap_now = true;\n\t} else if ((hw->phy.media_type != e1000_media_type_copper) &&\n\t\t   !(connsw & E1000_CONNSW_SERDESD)) {\n\t\t \n\t\tif (adapter->copper_tries < 4) {\n\t\t\tadapter->copper_tries++;\n\t\t\tconnsw |= E1000_CONNSW_AUTOSENSE_CONF;\n\t\t\twr32(E1000_CONNSW, connsw);\n\t\t\treturn;\n\t\t} else {\n\t\t\tadapter->copper_tries = 0;\n\t\t\tif ((connsw & E1000_CONNSW_PHYSD) &&\n\t\t\t    (!(connsw & E1000_CONNSW_PHY_PDN))) {\n\t\t\t\tswap_now = true;\n\t\t\t\tconnsw &= ~E1000_CONNSW_AUTOSENSE_CONF;\n\t\t\t\twr32(E1000_CONNSW, connsw);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!swap_now)\n\t\treturn;\n\n\tswitch (hw->phy.media_type) {\n\tcase e1000_media_type_copper:\n\t\tnetdev_info(adapter->netdev,\n\t\t\t\"MAS: changing media to fiber/serdes\\n\");\n\t\tctrl_ext |=\n\t\t\tE1000_CTRL_EXT_LINK_MODE_PCIE_SERDES;\n\t\tadapter->flags |= IGB_FLAG_MEDIA_RESET;\n\t\tadapter->copper_tries = 0;\n\t\tbreak;\n\tcase e1000_media_type_internal_serdes:\n\tcase e1000_media_type_fiber:\n\t\tnetdev_info(adapter->netdev,\n\t\t\t\"MAS: changing media to copper\\n\");\n\t\tctrl_ext &=\n\t\t\t~E1000_CTRL_EXT_LINK_MODE_PCIE_SERDES;\n\t\tadapter->flags |= IGB_FLAG_MEDIA_RESET;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tnetdev_err(adapter->netdev,\n\t\t\t\"AMS: Invalid media type found, returning\\n\");\n\t\tbreak;\n\t}\n\twr32(E1000_CTRL_EXT, ctrl_ext);\n}\n\n \nint igb_up(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint i;\n\n\t \n\tigb_configure(adapter);\n\n\tclear_bit(__IGB_DOWN, &adapter->state);\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\tnapi_enable(&(adapter->q_vector[i]->napi));\n\n\tif (adapter->flags & IGB_FLAG_HAS_MSIX)\n\t\tigb_configure_msix(adapter);\n\telse\n\t\tigb_assign_vector(adapter->q_vector[0], 0);\n\n\t \n\trd32(E1000_TSICR);\n\trd32(E1000_ICR);\n\tigb_irq_enable(adapter);\n\n\t \n\tif (adapter->vfs_allocated_count) {\n\t\tu32 reg_data = rd32(E1000_CTRL_EXT);\n\n\t\treg_data |= E1000_CTRL_EXT_PFRSTD;\n\t\twr32(E1000_CTRL_EXT, reg_data);\n\t}\n\n\tnetif_tx_start_all_queues(adapter->netdev);\n\n\t \n\thw->mac.get_link_status = 1;\n\tschedule_work(&adapter->watchdog_task);\n\n\tif ((adapter->flags & IGB_FLAG_EEE) &&\n\t    (!hw->dev_spec._82575.eee_disable))\n\t\tadapter->eee_advert = MDIO_EEE_100TX | MDIO_EEE_1000T;\n\n\treturn 0;\n}\n\nvoid igb_down(struct igb_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 tctl, rctl;\n\tint i;\n\n\t \n\tset_bit(__IGB_DOWN, &adapter->state);\n\n\t \n\trctl = rd32(E1000_RCTL);\n\twr32(E1000_RCTL, rctl & ~E1000_RCTL_EN);\n\t \n\n\tigb_nfc_filter_exit(adapter);\n\n\tnetif_carrier_off(netdev);\n\tnetif_tx_stop_all_queues(netdev);\n\n\t \n\ttctl = rd32(E1000_TCTL);\n\ttctl &= ~E1000_TCTL_EN;\n\twr32(E1000_TCTL, tctl);\n\t \n\twrfl();\n\tusleep_range(10000, 11000);\n\n\tigb_irq_disable(adapter);\n\n\tadapter->flags &= ~IGB_FLAG_NEED_LINK_UPDATE;\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++) {\n\t\tif (adapter->q_vector[i]) {\n\t\t\tnapi_synchronize(&adapter->q_vector[i]->napi);\n\t\t\tnapi_disable(&adapter->q_vector[i]->napi);\n\t\t}\n\t}\n\n\tdel_timer_sync(&adapter->watchdog_timer);\n\tdel_timer_sync(&adapter->phy_info_timer);\n\n\t \n\tspin_lock(&adapter->stats64_lock);\n\tigb_update_stats(adapter);\n\tspin_unlock(&adapter->stats64_lock);\n\n\tadapter->link_speed = 0;\n\tadapter->link_duplex = 0;\n\n\tif (!pci_channel_offline(adapter->pdev))\n\t\tigb_reset(adapter);\n\n\t \n\tadapter->flags &= ~IGB_FLAG_VLAN_PROMISC;\n\n\tigb_clean_all_tx_rings(adapter);\n\tigb_clean_all_rx_rings(adapter);\n#ifdef CONFIG_IGB_DCA\n\n\t \n\tigb_setup_dca(adapter);\n#endif\n}\n\nvoid igb_reinit_locked(struct igb_adapter *adapter)\n{\n\twhile (test_and_set_bit(__IGB_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\tigb_down(adapter);\n\tigb_up(adapter);\n\tclear_bit(__IGB_RESETTING, &adapter->state);\n}\n\n \nstatic void igb_enable_mas(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 connsw = rd32(E1000_CONNSW);\n\n\t \n\tif ((hw->phy.media_type == e1000_media_type_copper) &&\n\t    (!(connsw & E1000_CONNSW_SERDESD))) {\n\t\tconnsw |= E1000_CONNSW_ENRGSRC;\n\t\tconnsw |= E1000_CONNSW_AUTOSENSE_EN;\n\t\twr32(E1000_CONNSW, connsw);\n\t\twrfl();\n\t}\n}\n\n#ifdef CONFIG_IGB_HWMON\n \nstatic void igb_set_i2c_bb(struct e1000_hw *hw)\n{\n\tu32 ctrl_ext;\n\ts32 i2cctl;\n\n\tctrl_ext = rd32(E1000_CTRL_EXT);\n\tctrl_ext |= E1000_CTRL_I2C_ENA;\n\twr32(E1000_CTRL_EXT, ctrl_ext);\n\twrfl();\n\n\ti2cctl = rd32(E1000_I2CPARAMS);\n\ti2cctl |= E1000_I2CBB_EN\n\t\t| E1000_I2C_CLK_OE_N\n\t\t| E1000_I2C_DATA_OE_N;\n\twr32(E1000_I2CPARAMS, i2cctl);\n\twrfl();\n}\n#endif\n\nvoid igb_reset(struct igb_adapter *adapter)\n{\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct e1000_mac_info *mac = &hw->mac;\n\tstruct e1000_fc_info *fc = &hw->fc;\n\tu32 pba, hwm;\n\n\t \n\tswitch (mac->type) {\n\tcase e1000_i350:\n\tcase e1000_i354:\n\tcase e1000_82580:\n\t\tpba = rd32(E1000_RXPBS);\n\t\tpba = igb_rxpbs_adjust_82580(pba);\n\t\tbreak;\n\tcase e1000_82576:\n\t\tpba = rd32(E1000_RXPBS);\n\t\tpba &= E1000_RXPBS_SIZE_MASK_82576;\n\t\tbreak;\n\tcase e1000_82575:\n\tcase e1000_i210:\n\tcase e1000_i211:\n\tdefault:\n\t\tpba = E1000_PBA_34K;\n\t\tbreak;\n\t}\n\n\tif (mac->type == e1000_82575) {\n\t\tu32 min_rx_space, min_tx_space, needed_tx_space;\n\n\t\t \n\t\twr32(E1000_PBA, pba);\n\n\t\t \n\t\tmin_rx_space = DIV_ROUND_UP(MAX_JUMBO_FRAME_SIZE, 1024);\n\n\t\t \n\t\tmin_tx_space = adapter->max_frame_size;\n\t\tmin_tx_space += sizeof(union e1000_adv_tx_desc) - ETH_FCS_LEN;\n\t\tmin_tx_space = DIV_ROUND_UP(min_tx_space, 512);\n\n\t\t \n\t\tneeded_tx_space = min_tx_space - (rd32(E1000_PBA) >> 16);\n\n\t\t \n\t\tif (needed_tx_space < pba) {\n\t\t\tpba -= needed_tx_space;\n\n\t\t\t \n\t\t\tif (pba < min_rx_space)\n\t\t\t\tpba = min_rx_space;\n\t\t}\n\n\t\t \n\t\twr32(E1000_PBA, pba);\n\t}\n\n\t \n\thwm = (pba << 10) - (adapter->max_frame_size + MAX_JUMBO_FRAME_SIZE);\n\n\tfc->high_water = hwm & 0xFFFFFFF0;\t \n\tfc->low_water = fc->high_water - 16;\n\tfc->pause_time = 0xFFFF;\n\tfc->send_xon = 1;\n\tfc->current_mode = fc->requested_mode;\n\n\t \n\tif (adapter->vfs_allocated_count) {\n\t\tint i;\n\n\t\tfor (i = 0 ; i < adapter->vfs_allocated_count; i++)\n\t\t\tadapter->vf_data[i].flags &= IGB_VF_FLAG_PF_SET_MAC;\n\n\t\t \n\t\tigb_ping_all_vfs(adapter);\n\n\t\t \n\t\twr32(E1000_VFRE, 0);\n\t\twr32(E1000_VFTE, 0);\n\t}\n\n\t \n\thw->mac.ops.reset_hw(hw);\n\twr32(E1000_WUC, 0);\n\n\tif (adapter->flags & IGB_FLAG_MEDIA_RESET) {\n\t\t \n\t\tadapter->ei.get_invariants(hw);\n\t\tadapter->flags &= ~IGB_FLAG_MEDIA_RESET;\n\t}\n\tif ((mac->type == e1000_82575 || mac->type == e1000_i350) &&\n\t    (adapter->flags & IGB_FLAG_MAS_ENABLE)) {\n\t\tigb_enable_mas(adapter);\n\t}\n\tif (hw->mac.ops.init_hw(hw))\n\t\tdev_err(&pdev->dev, \"Hardware Error\\n\");\n\n\t \n\tigb_flush_mac_table(adapter);\n\t__dev_uc_unsync(adapter->netdev, NULL);\n\n\t \n\tigb_set_default_mac_filter(adapter);\n\n\t \n\tif (!hw->mac.autoneg)\n\t\tigb_force_mac_fc(hw);\n\n\tigb_init_dmac(adapter, pba);\n#ifdef CONFIG_IGB_HWMON\n\t \n\tif (!test_bit(__IGB_DOWN, &adapter->state)) {\n\t\tif (mac->type == e1000_i350 && hw->bus.func == 0) {\n\t\t\t \n\t\t\tif (adapter->ets)\n\t\t\t\tigb_set_i2c_bb(hw);\n\t\t\tmac->ops.init_thermal_sensor_thresh(hw);\n\t\t}\n\t}\n#endif\n\t \n\tif (hw->phy.media_type == e1000_media_type_copper) {\n\t\tswitch (mac->type) {\n\t\tcase e1000_i350:\n\t\tcase e1000_i210:\n\t\tcase e1000_i211:\n\t\t\tigb_set_eee_i350(hw, true, true);\n\t\t\tbreak;\n\t\tcase e1000_i354:\n\t\t\tigb_set_eee_i354(hw, true, true);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!netif_running(adapter->netdev))\n\t\tigb_power_down_link(adapter);\n\n\tigb_update_mng_vlan(adapter);\n\n\t \n\twr32(E1000_VET, ETHERNET_IEEE_VLAN_TYPE);\n\n\t \n\tif (adapter->ptp_flags & IGB_PTP_ENABLED)\n\t\tigb_ptp_reset(adapter);\n\n\tigb_get_phy_info(hw);\n}\n\nstatic netdev_features_t igb_fix_features(struct net_device *netdev,\n\tnetdev_features_t features)\n{\n\t \n\tif (features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tfeatures |= NETIF_F_HW_VLAN_CTAG_TX;\n\telse\n\t\tfeatures &= ~NETIF_F_HW_VLAN_CTAG_TX;\n\n\treturn features;\n}\n\nstatic int igb_set_features(struct net_device *netdev,\n\tnetdev_features_t features)\n{\n\tnetdev_features_t changed = netdev->features ^ features;\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\tif (changed & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tigb_vlan_mode(netdev, features);\n\n\tif (!(changed & (NETIF_F_RXALL | NETIF_F_NTUPLE)))\n\t\treturn 0;\n\n\tif (!(features & NETIF_F_NTUPLE)) {\n\t\tstruct hlist_node *node2;\n\t\tstruct igb_nfc_filter *rule;\n\n\t\tspin_lock(&adapter->nfc_lock);\n\t\thlist_for_each_entry_safe(rule, node2,\n\t\t\t\t\t  &adapter->nfc_filter_list, nfc_node) {\n\t\t\tigb_erase_filter(adapter, rule);\n\t\t\thlist_del(&rule->nfc_node);\n\t\t\tkfree(rule);\n\t\t}\n\t\tspin_unlock(&adapter->nfc_lock);\n\t\tadapter->nfc_filter_count = 0;\n\t}\n\n\tnetdev->features = features;\n\n\tif (netif_running(netdev))\n\t\tigb_reinit_locked(adapter);\n\telse\n\t\tigb_reset(adapter);\n\n\treturn 1;\n}\n\nstatic int igb_ndo_fdb_add(struct ndmsg *ndm, struct nlattr *tb[],\n\t\t\t   struct net_device *dev,\n\t\t\t   const unsigned char *addr, u16 vid,\n\t\t\t   u16 flags,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n\t \n\tif (is_unicast_ether_addr(addr) || is_link_local_ether_addr(addr)) {\n\t\tstruct igb_adapter *adapter = netdev_priv(dev);\n\t\tint vfn = adapter->vfs_allocated_count;\n\n\t\tif (netdev_uc_count(dev) >= igb_available_rars(adapter, vfn))\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn ndo_dflt_fdb_add(ndm, tb, dev, addr, vid, flags);\n}\n\n#define IGB_MAX_MAC_HDR_LEN\t127\n#define IGB_MAX_NETWORK_HDR_LEN\t511\n\nstatic netdev_features_t\nigb_features_check(struct sk_buff *skb, struct net_device *dev,\n\t\t   netdev_features_t features)\n{\n\tunsigned int network_hdr_len, mac_hdr_len;\n\n\t \n\tmac_hdr_len = skb_network_header(skb) - skb->data;\n\tif (unlikely(mac_hdr_len > IGB_MAX_MAC_HDR_LEN))\n\t\treturn features & ~(NETIF_F_HW_CSUM |\n\t\t\t\t    NETIF_F_SCTP_CRC |\n\t\t\t\t    NETIF_F_GSO_UDP_L4 |\n\t\t\t\t    NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t    NETIF_F_TSO |\n\t\t\t\t    NETIF_F_TSO6);\n\n\tnetwork_hdr_len = skb_checksum_start(skb) - skb_network_header(skb);\n\tif (unlikely(network_hdr_len >  IGB_MAX_NETWORK_HDR_LEN))\n\t\treturn features & ~(NETIF_F_HW_CSUM |\n\t\t\t\t    NETIF_F_SCTP_CRC |\n\t\t\t\t    NETIF_F_GSO_UDP_L4 |\n\t\t\t\t    NETIF_F_TSO |\n\t\t\t\t    NETIF_F_TSO6);\n\n\t \n\tif (skb->encapsulation && !(features & NETIF_F_TSO_MANGLEID))\n\t\tfeatures &= ~NETIF_F_TSO;\n\n\treturn features;\n}\n\nstatic void igb_offload_apply(struct igb_adapter *adapter, s32 queue)\n{\n\tif (!is_fqtss_enabled(adapter)) {\n\t\tenable_fqtss(adapter, true);\n\t\treturn;\n\t}\n\n\tigb_config_tx_modes(adapter, queue);\n\n\tif (!is_any_cbs_enabled(adapter) && !is_any_txtime_enabled(adapter))\n\t\tenable_fqtss(adapter, false);\n}\n\nstatic int igb_offload_cbs(struct igb_adapter *adapter,\n\t\t\t   struct tc_cbs_qopt_offload *qopt)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint err;\n\n\t \n\tif (hw->mac.type != e1000_i210)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (qopt->queue < 0 || qopt->queue > 1)\n\t\treturn -EINVAL;\n\n\terr = igb_save_cbs_params(adapter, qopt->queue, qopt->enable,\n\t\t\t\t  qopt->idleslope, qopt->sendslope,\n\t\t\t\t  qopt->hicredit, qopt->locredit);\n\tif (err)\n\t\treturn err;\n\n\tigb_offload_apply(adapter, qopt->queue);\n\n\treturn 0;\n}\n\n#define ETHER_TYPE_FULL_MASK ((__force __be16)~0)\n#define VLAN_PRIO_FULL_MASK (0x07)\n\nstatic int igb_parse_cls_flower(struct igb_adapter *adapter,\n\t\t\t\tstruct flow_cls_offload *f,\n\t\t\t\tint traffic_class,\n\t\t\t\tstruct igb_nfc_filter *input)\n{\n\tstruct flow_rule *rule = flow_cls_offload_flow_rule(f);\n\tstruct flow_dissector *dissector = rule->match.dissector;\n\tstruct netlink_ext_ack *extack = f->common.extack;\n\n\tif (dissector->used_keys &\n\t    ~(BIT_ULL(FLOW_DISSECTOR_KEY_BASIC) |\n\t      BIT_ULL(FLOW_DISSECTOR_KEY_CONTROL) |\n\t      BIT_ULL(FLOW_DISSECTOR_KEY_ETH_ADDRS) |\n\t      BIT_ULL(FLOW_DISSECTOR_KEY_VLAN))) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Unsupported key used, only BASIC, CONTROL, ETH_ADDRS and VLAN are supported\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ETH_ADDRS)) {\n\t\tstruct flow_match_eth_addrs match;\n\n\t\tflow_rule_match_eth_addrs(rule, &match);\n\t\tif (!is_zero_ether_addr(match.mask->dst)) {\n\t\t\tif (!is_broadcast_ether_addr(match.mask->dst)) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Only full masks are supported for destination MAC address\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tinput->filter.match_flags |=\n\t\t\t\tIGB_FILTER_FLAG_DST_MAC_ADDR;\n\t\t\tether_addr_copy(input->filter.dst_addr, match.key->dst);\n\t\t}\n\n\t\tif (!is_zero_ether_addr(match.mask->src)) {\n\t\t\tif (!is_broadcast_ether_addr(match.mask->src)) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Only full masks are supported for source MAC address\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tinput->filter.match_flags |=\n\t\t\t\tIGB_FILTER_FLAG_SRC_MAC_ADDR;\n\t\t\tether_addr_copy(input->filter.src_addr, match.key->src);\n\t\t}\n\t}\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_BASIC)) {\n\t\tstruct flow_match_basic match;\n\n\t\tflow_rule_match_basic(rule, &match);\n\t\tif (match.mask->n_proto) {\n\t\t\tif (match.mask->n_proto != ETHER_TYPE_FULL_MASK) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Only full mask is supported for EtherType filter\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tinput->filter.match_flags |= IGB_FILTER_FLAG_ETHER_TYPE;\n\t\t\tinput->filter.etype = match.key->n_proto;\n\t\t}\n\t}\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_VLAN)) {\n\t\tstruct flow_match_vlan match;\n\n\t\tflow_rule_match_vlan(rule, &match);\n\t\tif (match.mask->vlan_priority) {\n\t\t\tif (match.mask->vlan_priority != VLAN_PRIO_FULL_MASK) {\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Only full mask is supported for VLAN priority\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tinput->filter.match_flags |= IGB_FILTER_FLAG_VLAN_TCI;\n\t\t\tinput->filter.vlan_tci =\n\t\t\t\t(__force __be16)match.key->vlan_priority;\n\t\t}\n\t}\n\n\tinput->action = traffic_class;\n\tinput->cookie = f->cookie;\n\n\treturn 0;\n}\n\nstatic int igb_configure_clsflower(struct igb_adapter *adapter,\n\t\t\t\t   struct flow_cls_offload *cls_flower)\n{\n\tstruct netlink_ext_ack *extack = cls_flower->common.extack;\n\tstruct igb_nfc_filter *filter, *f;\n\tint err, tc;\n\n\ttc = tc_classid_to_hwtc(adapter->netdev, cls_flower->classid);\n\tif (tc < 0) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Invalid traffic class\");\n\t\treturn -EINVAL;\n\t}\n\n\tfilter = kzalloc(sizeof(*filter), GFP_KERNEL);\n\tif (!filter)\n\t\treturn -ENOMEM;\n\n\terr = igb_parse_cls_flower(adapter, cls_flower, tc, filter);\n\tif (err < 0)\n\t\tgoto err_parse;\n\n\tspin_lock(&adapter->nfc_lock);\n\n\thlist_for_each_entry(f, &adapter->nfc_filter_list, nfc_node) {\n\t\tif (!memcmp(&f->filter, &filter->filter, sizeof(f->filter))) {\n\t\t\terr = -EEXIST;\n\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t   \"This filter is already set in ethtool\");\n\t\t\tgoto err_locked;\n\t\t}\n\t}\n\n\thlist_for_each_entry(f, &adapter->cls_flower_list, nfc_node) {\n\t\tif (!memcmp(&f->filter, &filter->filter, sizeof(f->filter))) {\n\t\t\terr = -EEXIST;\n\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t   \"This filter is already set in cls_flower\");\n\t\t\tgoto err_locked;\n\t\t}\n\t}\n\n\terr = igb_add_filter(adapter, filter);\n\tif (err < 0) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Could not add filter to the adapter\");\n\t\tgoto err_locked;\n\t}\n\n\thlist_add_head(&filter->nfc_node, &adapter->cls_flower_list);\n\n\tspin_unlock(&adapter->nfc_lock);\n\n\treturn 0;\n\nerr_locked:\n\tspin_unlock(&adapter->nfc_lock);\n\nerr_parse:\n\tkfree(filter);\n\n\treturn err;\n}\n\nstatic int igb_delete_clsflower(struct igb_adapter *adapter,\n\t\t\t\tstruct flow_cls_offload *cls_flower)\n{\n\tstruct igb_nfc_filter *filter;\n\tint err;\n\n\tspin_lock(&adapter->nfc_lock);\n\n\thlist_for_each_entry(filter, &adapter->cls_flower_list, nfc_node)\n\t\tif (filter->cookie == cls_flower->cookie)\n\t\t\tbreak;\n\n\tif (!filter) {\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\n\terr = igb_erase_filter(adapter, filter);\n\tif (err < 0)\n\t\tgoto out;\n\n\thlist_del(&filter->nfc_node);\n\tkfree(filter);\n\nout:\n\tspin_unlock(&adapter->nfc_lock);\n\n\treturn err;\n}\n\nstatic int igb_setup_tc_cls_flower(struct igb_adapter *adapter,\n\t\t\t\t   struct flow_cls_offload *cls_flower)\n{\n\tswitch (cls_flower->command) {\n\tcase FLOW_CLS_REPLACE:\n\t\treturn igb_configure_clsflower(adapter, cls_flower);\n\tcase FLOW_CLS_DESTROY:\n\t\treturn igb_delete_clsflower(adapter, cls_flower);\n\tcase FLOW_CLS_STATS:\n\t\treturn -EOPNOTSUPP;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int igb_setup_tc_block_cb(enum tc_setup_type type, void *type_data,\n\t\t\t\t void *cb_priv)\n{\n\tstruct igb_adapter *adapter = cb_priv;\n\n\tif (!tc_cls_can_offload_and_chain0(adapter->netdev, type_data))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (type) {\n\tcase TC_SETUP_CLSFLOWER:\n\t\treturn igb_setup_tc_cls_flower(adapter, type_data);\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int igb_offload_txtime(struct igb_adapter *adapter,\n\t\t\t      struct tc_etf_qopt_offload *qopt)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint err;\n\n\t \n\tif (hw->mac.type != e1000_i210)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (qopt->queue < 0 || qopt->queue > 1)\n\t\treturn -EINVAL;\n\n\terr = igb_save_txtime_params(adapter, qopt->queue, qopt->enable);\n\tif (err)\n\t\treturn err;\n\n\tigb_offload_apply(adapter, qopt->queue);\n\n\treturn 0;\n}\n\nstatic int igb_tc_query_caps(struct igb_adapter *adapter,\n\t\t\t     struct tc_query_caps_base *base)\n{\n\tswitch (base->type) {\n\tcase TC_SETUP_QDISC_TAPRIO: {\n\t\tstruct tc_taprio_caps *caps = base->caps;\n\n\t\tcaps->broken_mqprio = true;\n\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic LIST_HEAD(igb_block_cb_list);\n\nstatic int igb_setup_tc(struct net_device *dev, enum tc_setup_type type,\n\t\t\tvoid *type_data)\n{\n\tstruct igb_adapter *adapter = netdev_priv(dev);\n\n\tswitch (type) {\n\tcase TC_QUERY_CAPS:\n\t\treturn igb_tc_query_caps(adapter, type_data);\n\tcase TC_SETUP_QDISC_CBS:\n\t\treturn igb_offload_cbs(adapter, type_data);\n\tcase TC_SETUP_BLOCK:\n\t\treturn flow_block_cb_setup_simple(type_data,\n\t\t\t\t\t\t  &igb_block_cb_list,\n\t\t\t\t\t\t  igb_setup_tc_block_cb,\n\t\t\t\t\t\t  adapter, adapter, true);\n\n\tcase TC_SETUP_QDISC_ETF:\n\t\treturn igb_offload_txtime(adapter, type_data);\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int igb_xdp_setup(struct net_device *dev, struct netdev_bpf *bpf)\n{\n\tint i, frame_size = dev->mtu + IGB_ETH_PKT_HDR_PAD;\n\tstruct igb_adapter *adapter = netdev_priv(dev);\n\tstruct bpf_prog *prog = bpf->prog, *old_prog;\n\tbool running = netif_running(dev);\n\tbool need_reset;\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct igb_ring *ring = adapter->rx_ring[i];\n\n\t\tif (frame_size > igb_rx_bufsz(ring)) {\n\t\t\tNL_SET_ERR_MSG_MOD(bpf->extack,\n\t\t\t\t\t   \"The RX buffer size is too small for the frame size\");\n\t\t\tnetdev_warn(dev, \"XDP RX buffer size %d is too small for the frame size %d\\n\",\n\t\t\t\t    igb_rx_bufsz(ring), frame_size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\told_prog = xchg(&adapter->xdp_prog, prog);\n\tneed_reset = (!!prog != !!old_prog);\n\n\t \n\tif (need_reset && running) {\n\t\tigb_close(dev);\n\t} else {\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\t\t(void)xchg(&adapter->rx_ring[i]->xdp_prog,\n\t\t\t    adapter->xdp_prog);\n\t}\n\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\t \n\tif (!need_reset) {\n\t\treturn 0;\n\t} else {\n\t\tif (prog)\n\t\t\txdp_features_set_redirect_target(dev, true);\n\t\telse\n\t\t\txdp_features_clear_redirect_target(dev);\n\t}\n\n\tif (running)\n\t\tigb_open(dev);\n\n\treturn 0;\n}\n\nstatic int igb_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn igb_xdp_setup(dev, xdp);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic void igb_xdp_ring_update_tail(struct igb_ring *ring)\n{\n\t \n\twmb();\n\twritel(ring->next_to_use, ring->tail);\n}\n\nstatic struct igb_ring *igb_xdp_tx_queue_mapping(struct igb_adapter *adapter)\n{\n\tunsigned int r_idx = smp_processor_id();\n\n\tif (r_idx >= adapter->num_tx_queues)\n\t\tr_idx = r_idx % adapter->num_tx_queues;\n\n\treturn adapter->tx_ring[r_idx];\n}\n\nstatic int igb_xdp_xmit_back(struct igb_adapter *adapter, struct xdp_buff *xdp)\n{\n\tstruct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);\n\tint cpu = smp_processor_id();\n\tstruct igb_ring *tx_ring;\n\tstruct netdev_queue *nq;\n\tu32 ret;\n\n\tif (unlikely(!xdpf))\n\t\treturn IGB_XDP_CONSUMED;\n\n\t \n\ttx_ring = adapter->xdp_prog ? igb_xdp_tx_queue_mapping(adapter) : NULL;\n\tif (unlikely(!tx_ring))\n\t\treturn IGB_XDP_CONSUMED;\n\n\tnq = txring_txq(tx_ring);\n\t__netif_tx_lock(nq, cpu);\n\t \n\ttxq_trans_cond_update(nq);\n\tret = igb_xmit_xdp_ring(adapter, tx_ring, xdpf);\n\t__netif_tx_unlock(nq);\n\n\treturn ret;\n}\n\nstatic int igb_xdp_xmit(struct net_device *dev, int n,\n\t\t\tstruct xdp_frame **frames, u32 flags)\n{\n\tstruct igb_adapter *adapter = netdev_priv(dev);\n\tint cpu = smp_processor_id();\n\tstruct igb_ring *tx_ring;\n\tstruct netdev_queue *nq;\n\tint nxmit = 0;\n\tint i;\n\n\tif (unlikely(test_bit(__IGB_DOWN, &adapter->state)))\n\t\treturn -ENETDOWN;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\t \n\ttx_ring = adapter->xdp_prog ? igb_xdp_tx_queue_mapping(adapter) : NULL;\n\tif (unlikely(!tx_ring))\n\t\treturn -ENXIO;\n\n\tnq = txring_txq(tx_ring);\n\t__netif_tx_lock(nq, cpu);\n\n\t \n\ttxq_trans_cond_update(nq);\n\n\tfor (i = 0; i < n; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\t\tint err;\n\n\t\terr = igb_xmit_xdp_ring(adapter, tx_ring, xdpf);\n\t\tif (err != IGB_XDP_TX)\n\t\t\tbreak;\n\t\tnxmit++;\n\t}\n\n\t__netif_tx_unlock(nq);\n\n\tif (unlikely(flags & XDP_XMIT_FLUSH))\n\t\tigb_xdp_ring_update_tail(tx_ring);\n\n\treturn nxmit;\n}\n\nstatic const struct net_device_ops igb_netdev_ops = {\n\t.ndo_open\t\t= igb_open,\n\t.ndo_stop\t\t= igb_close,\n\t.ndo_start_xmit\t\t= igb_xmit_frame,\n\t.ndo_get_stats64\t= igb_get_stats64,\n\t.ndo_set_rx_mode\t= igb_set_rx_mode,\n\t.ndo_set_mac_address\t= igb_set_mac,\n\t.ndo_change_mtu\t\t= igb_change_mtu,\n\t.ndo_eth_ioctl\t\t= igb_ioctl,\n\t.ndo_tx_timeout\t\t= igb_tx_timeout,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_vlan_rx_add_vid\t= igb_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= igb_vlan_rx_kill_vid,\n\t.ndo_set_vf_mac\t\t= igb_ndo_set_vf_mac,\n\t.ndo_set_vf_vlan\t= igb_ndo_set_vf_vlan,\n\t.ndo_set_vf_rate\t= igb_ndo_set_vf_bw,\n\t.ndo_set_vf_spoofchk\t= igb_ndo_set_vf_spoofchk,\n\t.ndo_set_vf_trust\t= igb_ndo_set_vf_trust,\n\t.ndo_get_vf_config\t= igb_ndo_get_vf_config,\n\t.ndo_fix_features\t= igb_fix_features,\n\t.ndo_set_features\t= igb_set_features,\n\t.ndo_fdb_add\t\t= igb_ndo_fdb_add,\n\t.ndo_features_check\t= igb_features_check,\n\t.ndo_setup_tc\t\t= igb_setup_tc,\n\t.ndo_bpf\t\t= igb_xdp,\n\t.ndo_xdp_xmit\t\t= igb_xdp_xmit,\n};\n\n \nvoid igb_set_fw_version(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct e1000_fw_version fw;\n\n\tigb_get_fw_version(hw, &fw);\n\n\tswitch (hw->mac.type) {\n\tcase e1000_i210:\n\tcase e1000_i211:\n\t\tif (!(igb_get_flash_presence_i210(hw))) {\n\t\t\tsnprintf(adapter->fw_version,\n\t\t\t\t sizeof(adapter->fw_version),\n\t\t\t\t \"%2d.%2d-%d\",\n\t\t\t\t fw.invm_major, fw.invm_minor,\n\t\t\t\t fw.invm_img_type);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\t \n\t\tif (fw.or_valid) {\n\t\t\tsnprintf(adapter->fw_version,\n\t\t\t\t sizeof(adapter->fw_version),\n\t\t\t\t \"%d.%d, 0x%08x, %d.%d.%d\",\n\t\t\t\t fw.eep_major, fw.eep_minor, fw.etrack_id,\n\t\t\t\t fw.or_major, fw.or_build, fw.or_patch);\n\t\t \n\t\t} else if (fw.etrack_id != 0X0000) {\n\t\t\tsnprintf(adapter->fw_version,\n\t\t\t    sizeof(adapter->fw_version),\n\t\t\t    \"%d.%d, 0x%08x\",\n\t\t\t    fw.eep_major, fw.eep_minor, fw.etrack_id);\n\t\t} else {\n\t\tsnprintf(adapter->fw_version,\n\t\t    sizeof(adapter->fw_version),\n\t\t    \"%d.%d.%d\",\n\t\t    fw.eep_major, fw.eep_minor, fw.eep_build);\n\t\t}\n\t\tbreak;\n\t}\n}\n\n \nstatic void igb_init_mas(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu16 eeprom_data;\n\n\thw->nvm.ops.read(hw, NVM_COMPAT, 1, &eeprom_data);\n\tswitch (hw->bus.func) {\n\tcase E1000_FUNC_0:\n\t\tif (eeprom_data & IGB_MAS_ENABLE_0) {\n\t\t\tadapter->flags |= IGB_FLAG_MAS_ENABLE;\n\t\t\tnetdev_info(adapter->netdev,\n\t\t\t\t\"MAS: Enabling Media Autosense for port %d\\n\",\n\t\t\t\thw->bus.func);\n\t\t}\n\t\tbreak;\n\tcase E1000_FUNC_1:\n\t\tif (eeprom_data & IGB_MAS_ENABLE_1) {\n\t\t\tadapter->flags |= IGB_FLAG_MAS_ENABLE;\n\t\t\tnetdev_info(adapter->netdev,\n\t\t\t\t\"MAS: Enabling Media Autosense for port %d\\n\",\n\t\t\t\thw->bus.func);\n\t\t}\n\t\tbreak;\n\tcase E1000_FUNC_2:\n\t\tif (eeprom_data & IGB_MAS_ENABLE_2) {\n\t\t\tadapter->flags |= IGB_FLAG_MAS_ENABLE;\n\t\t\tnetdev_info(adapter->netdev,\n\t\t\t\t\"MAS: Enabling Media Autosense for port %d\\n\",\n\t\t\t\thw->bus.func);\n\t\t}\n\t\tbreak;\n\tcase E1000_FUNC_3:\n\t\tif (eeprom_data & IGB_MAS_ENABLE_3) {\n\t\t\tadapter->flags |= IGB_FLAG_MAS_ENABLE;\n\t\t\tnetdev_info(adapter->netdev,\n\t\t\t\t\"MAS: Enabling Media Autosense for port %d\\n\",\n\t\t\t\thw->bus.func);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tnetdev_err(adapter->netdev,\n\t\t\t\"MAS: Invalid port configuration, returning\\n\");\n\t\tbreak;\n\t}\n}\n\n \nstatic s32 igb_init_i2c(struct igb_adapter *adapter)\n{\n\ts32 status = 0;\n\n\t \n\tif (adapter->hw.mac.type != e1000_i350)\n\t\treturn 0;\n\n\t \n\tadapter->i2c_adap.owner = THIS_MODULE;\n\tadapter->i2c_algo = igb_i2c_algo;\n\tadapter->i2c_algo.data = adapter;\n\tadapter->i2c_adap.algo_data = &adapter->i2c_algo;\n\tadapter->i2c_adap.dev.parent = &adapter->pdev->dev;\n\tstrscpy(adapter->i2c_adap.name, \"igb BB\",\n\t\tsizeof(adapter->i2c_adap.name));\n\tstatus = i2c_bit_add_bus(&adapter->i2c_adap);\n\treturn status;\n}\n\n \nstatic int igb_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct net_device *netdev;\n\tstruct igb_adapter *adapter;\n\tstruct e1000_hw *hw;\n\tu16 eeprom_data = 0;\n\ts32 ret_val;\n\tstatic int global_quad_port_a;  \n\tconst struct e1000_info *ei = igb_info_tbl[ent->driver_data];\n\tu8 part_str[E1000_PBANUM_LENGTH];\n\tint err;\n\n\t \n\tif (pdev->is_virtfn) {\n\t\tWARN(1, KERN_ERR \"%s (%x:%x) should not be a VF!\\n\",\n\t\t\tpci_name(pdev), pdev->vendor, pdev->device);\n\t\treturn -EINVAL;\n\t}\n\n\terr = pci_enable_device_mem(pdev);\n\tif (err)\n\t\treturn err;\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"No usable DMA configuration, aborting\\n\");\n\t\tgoto err_dma;\n\t}\n\n\terr = pci_request_mem_regions(pdev, igb_driver_name);\n\tif (err)\n\t\tgoto err_pci_reg;\n\n\tpci_set_master(pdev);\n\tpci_save_state(pdev);\n\n\terr = -ENOMEM;\n\tnetdev = alloc_etherdev_mq(sizeof(struct igb_adapter),\n\t\t\t\t   IGB_MAX_TX_QUEUES);\n\tif (!netdev)\n\t\tgoto err_alloc_etherdev;\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\n\tpci_set_drvdata(pdev, netdev);\n\tadapter = netdev_priv(netdev);\n\tadapter->netdev = netdev;\n\tadapter->pdev = pdev;\n\thw = &adapter->hw;\n\thw->back = adapter;\n\tadapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\n\n\terr = -EIO;\n\tadapter->io_addr = pci_iomap(pdev, 0, 0);\n\tif (!adapter->io_addr)\n\t\tgoto err_ioremap;\n\t \n\thw->hw_addr = adapter->io_addr;\n\n\tnetdev->netdev_ops = &igb_netdev_ops;\n\tigb_set_ethtool_ops(netdev);\n\tnetdev->watchdog_timeo = 5 * HZ;\n\n\tstrncpy(netdev->name, pci_name(pdev), sizeof(netdev->name) - 1);\n\n\tnetdev->mem_start = pci_resource_start(pdev, 0);\n\tnetdev->mem_end = pci_resource_end(pdev, 0);\n\n\t \n\thw->vendor_id = pdev->vendor;\n\thw->device_id = pdev->device;\n\thw->revision_id = pdev->revision;\n\thw->subsystem_vendor_id = pdev->subsystem_vendor;\n\thw->subsystem_device_id = pdev->subsystem_device;\n\n\t \n\tmemcpy(&hw->mac.ops, ei->mac_ops, sizeof(hw->mac.ops));\n\tmemcpy(&hw->phy.ops, ei->phy_ops, sizeof(hw->phy.ops));\n\tmemcpy(&hw->nvm.ops, ei->nvm_ops, sizeof(hw->nvm.ops));\n\t \n\terr = ei->get_invariants(hw);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\t \n\terr = igb_sw_init(adapter);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\tigb_get_bus_info_pcie(hw);\n\n\thw->phy.autoneg_wait_to_complete = false;\n\n\t \n\tif (hw->phy.media_type == e1000_media_type_copper) {\n\t\thw->phy.mdix = AUTO_ALL_MODES;\n\t\thw->phy.disable_polarity_correction = false;\n\t\thw->phy.ms_type = e1000_ms_hw_default;\n\t}\n\n\tif (igb_check_reset_block(hw))\n\t\tdev_info(&pdev->dev,\n\t\t\t\"PHY reset is blocked due to SOL/IDER session.\\n\");\n\n\t \n\tnetdev->features |= NETIF_F_SG |\n\t\t\t    NETIF_F_TSO |\n\t\t\t    NETIF_F_TSO6 |\n\t\t\t    NETIF_F_RXHASH |\n\t\t\t    NETIF_F_RXCSUM |\n\t\t\t    NETIF_F_HW_CSUM;\n\n\tif (hw->mac.type >= e1000_82576)\n\t\tnetdev->features |= NETIF_F_SCTP_CRC | NETIF_F_GSO_UDP_L4;\n\n\tif (hw->mac.type >= e1000_i350)\n\t\tnetdev->features |= NETIF_F_HW_TC;\n\n#define IGB_GSO_PARTIAL_FEATURES (NETIF_F_GSO_GRE | \\\n\t\t\t\t  NETIF_F_GSO_GRE_CSUM | \\\n\t\t\t\t  NETIF_F_GSO_IPXIP4 | \\\n\t\t\t\t  NETIF_F_GSO_IPXIP6 | \\\n\t\t\t\t  NETIF_F_GSO_UDP_TUNNEL | \\\n\t\t\t\t  NETIF_F_GSO_UDP_TUNNEL_CSUM)\n\n\tnetdev->gso_partial_features = IGB_GSO_PARTIAL_FEATURES;\n\tnetdev->features |= NETIF_F_GSO_PARTIAL | IGB_GSO_PARTIAL_FEATURES;\n\n\t \n\tnetdev->hw_features |= netdev->features |\n\t\t\t       NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t       NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t       NETIF_F_RXALL;\n\n\tif (hw->mac.type >= e1000_i350)\n\t\tnetdev->hw_features |= NETIF_F_NTUPLE;\n\n\tnetdev->features |= NETIF_F_HIGHDMA;\n\n\tnetdev->vlan_features |= netdev->features | NETIF_F_TSO_MANGLEID;\n\tnetdev->mpls_features |= NETIF_F_HW_CSUM;\n\tnetdev->hw_enc_features |= netdev->vlan_features;\n\n\t \n\tnetdev->features |= NETIF_F_HW_VLAN_CTAG_FILTER |\n\t\t\t    NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t    NETIF_F_HW_VLAN_CTAG_TX;\n\n\tnetdev->priv_flags |= IFF_SUPP_NOFCS;\n\n\tnetdev->priv_flags |= IFF_UNICAST_FLT;\n\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT;\n\n\t \n\tnetdev->min_mtu = ETH_MIN_MTU;\n\tnetdev->max_mtu = MAX_STD_JUMBO_FRAME_SIZE;\n\n\tadapter->en_mng_pt = igb_enable_mng_pass_thru(hw);\n\n\t \n\thw->mac.ops.reset_hw(hw);\n\n\t \n\tswitch (hw->mac.type) {\n\tcase e1000_i210:\n\tcase e1000_i211:\n\t\tif (igb_get_flash_presence_i210(hw)) {\n\t\t\tif (hw->nvm.ops.validate(hw) < 0) {\n\t\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\t\"The NVM Checksum Is Not Valid\\n\");\n\t\t\t\terr = -EIO;\n\t\t\t\tgoto err_eeprom;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tif (hw->nvm.ops.validate(hw) < 0) {\n\t\t\tdev_err(&pdev->dev, \"The NVM Checksum Is Not Valid\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto err_eeprom;\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (eth_platform_get_mac_address(&pdev->dev, hw->mac.addr)) {\n\t\t \n\t\tif (hw->mac.ops.read_mac_addr(hw))\n\t\t\tdev_err(&pdev->dev, \"NVM Read Error\\n\");\n\t}\n\n\teth_hw_addr_set(netdev, hw->mac.addr);\n\n\tif (!is_valid_ether_addr(netdev->dev_addr)) {\n\t\tdev_err(&pdev->dev, \"Invalid MAC Address\\n\");\n\t\terr = -EIO;\n\t\tgoto err_eeprom;\n\t}\n\n\tigb_set_default_mac_filter(adapter);\n\n\t \n\tigb_set_fw_version(adapter);\n\n\t \n\tif (hw->mac.type == e1000_i210) {\n\t\twr32(E1000_RXPBS, I210_RXPBSIZE_DEFAULT);\n\t\twr32(E1000_TXPBS, I210_TXPBSIZE_DEFAULT);\n\t}\n\n\ttimer_setup(&adapter->watchdog_timer, igb_watchdog, 0);\n\ttimer_setup(&adapter->phy_info_timer, igb_update_phy_info, 0);\n\n\tINIT_WORK(&adapter->reset_task, igb_reset_task);\n\tINIT_WORK(&adapter->watchdog_task, igb_watchdog_task);\n\n\t \n\tadapter->fc_autoneg = true;\n\thw->mac.autoneg = true;\n\thw->phy.autoneg_advertised = 0x2f;\n\n\thw->fc.requested_mode = e1000_fc_default;\n\thw->fc.current_mode = e1000_fc_default;\n\n\tigb_validate_mdi_setting(hw);\n\n\t \n\tif (hw->bus.func == 0)\n\t\tadapter->flags |= IGB_FLAG_WOL_SUPPORTED;\n\n\t \n\tif (hw->mac.type >= e1000_82580)\n\t\thw->nvm.ops.read(hw, NVM_INIT_CONTROL3_PORT_A +\n\t\t\t\t NVM_82580_LAN_FUNC_OFFSET(hw->bus.func), 1,\n\t\t\t\t &eeprom_data);\n\telse if (hw->bus.func == 1)\n\t\thw->nvm.ops.read(hw, NVM_INIT_CONTROL3_PORT_B, 1, &eeprom_data);\n\n\tif (eeprom_data & IGB_EEPROM_APME)\n\t\tadapter->flags |= IGB_FLAG_WOL_SUPPORTED;\n\n\t \n\tswitch (pdev->device) {\n\tcase E1000_DEV_ID_82575GB_QUAD_COPPER:\n\t\tadapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;\n\t\tbreak;\n\tcase E1000_DEV_ID_82575EB_FIBER_SERDES:\n\tcase E1000_DEV_ID_82576_FIBER:\n\tcase E1000_DEV_ID_82576_SERDES:\n\t\t \n\t\tif (rd32(E1000_STATUS) & E1000_STATUS_FUNC_1)\n\t\t\tadapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;\n\t\tbreak;\n\tcase E1000_DEV_ID_82576_QUAD_COPPER:\n\tcase E1000_DEV_ID_82576_QUAD_COPPER_ET2:\n\t\t \n\t\tif (global_quad_port_a != 0)\n\t\t\tadapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;\n\t\telse\n\t\t\tadapter->flags |= IGB_FLAG_QUAD_PORT_A;\n\t\t \n\t\tif (++global_quad_port_a == 4)\n\t\t\tglobal_quad_port_a = 0;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tif (!device_can_wakeup(&adapter->pdev->dev))\n\t\t\tadapter->flags &= ~IGB_FLAG_WOL_SUPPORTED;\n\t}\n\n\t \n\tif (adapter->flags & IGB_FLAG_WOL_SUPPORTED)\n\t\tadapter->wol |= E1000_WUFC_MAG;\n\n\t \n\tif ((hw->mac.type == e1000_i350) &&\n\t    (pdev->subsystem_vendor == PCI_VENDOR_ID_HP)) {\n\t\tadapter->flags |= IGB_FLAG_WOL_SUPPORTED;\n\t\tadapter->wol = 0;\n\t}\n\n\t \n\tif (((hw->mac.type == e1000_i350) ||\n\t     (hw->mac.type == e1000_i354)) &&\n\t    (pdev->subsystem_vendor == PCI_VENDOR_ID_DELL)) {\n\t\tadapter->flags |= IGB_FLAG_WOL_SUPPORTED;\n\t\tadapter->wol = 0;\n\t}\n\tif (hw->mac.type == e1000_i350) {\n\t\tif (((pdev->subsystem_device == 0x5001) ||\n\t\t     (pdev->subsystem_device == 0x5002)) &&\n\t\t\t\t(hw->bus.func == 0)) {\n\t\t\tadapter->flags |= IGB_FLAG_WOL_SUPPORTED;\n\t\t\tadapter->wol = 0;\n\t\t}\n\t\tif (pdev->subsystem_device == 0x1F52)\n\t\t\tadapter->flags |= IGB_FLAG_WOL_SUPPORTED;\n\t}\n\n\tdevice_set_wakeup_enable(&adapter->pdev->dev,\n\t\t\t\t adapter->flags & IGB_FLAG_WOL_SUPPORTED);\n\n\t \n\tigb_reset(adapter);\n\n\t \n\terr = igb_init_i2c(adapter);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to init i2c interface\\n\");\n\t\tgoto err_eeprom;\n\t}\n\n\t \n\tigb_get_hw_control(adapter);\n\n\tstrcpy(netdev->name, \"eth%d\");\n\terr = register_netdev(netdev);\n\tif (err)\n\t\tgoto err_register;\n\n\t \n\tnetif_carrier_off(netdev);\n\n#ifdef CONFIG_IGB_DCA\n\tif (dca_add_requester(&pdev->dev) == 0) {\n\t\tadapter->flags |= IGB_FLAG_DCA_ENABLED;\n\t\tdev_info(&pdev->dev, \"DCA enabled\\n\");\n\t\tigb_setup_dca(adapter);\n\t}\n\n#endif\n#ifdef CONFIG_IGB_HWMON\n\t \n\tif (hw->mac.type == e1000_i350 && hw->bus.func == 0) {\n\t\tu16 ets_word;\n\n\t\t \n\t\thw->nvm.ops.read(hw, NVM_ETS_CFG, 1, &ets_word);\n\t\tif (ets_word != 0x0000 && ets_word != 0xFFFF)\n\t\t\tadapter->ets = true;\n\t\telse\n\t\t\tadapter->ets = false;\n\t\t \n\t\tif (adapter->ets)\n\t\t\tigb_set_i2c_bb(hw);\n\t\thw->mac.ops.init_thermal_sensor_thresh(hw);\n\t\tif (igb_sysfs_init(adapter))\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"failed to allocate sysfs resources\\n\");\n\t} else {\n\t\tadapter->ets = false;\n\t}\n#endif\n\t \n\tadapter->ei = *ei;\n\tif (hw->dev_spec._82575.mas_capable)\n\t\tigb_init_mas(adapter);\n\n\t \n\tigb_ptp_init(adapter);\n\n\tdev_info(&pdev->dev, \"Intel(R) Gigabit Ethernet Network Connection\\n\");\n\t \n\tif (hw->mac.type != e1000_i354) {\n\t\tdev_info(&pdev->dev, \"%s: (PCIe:%s:%s) %pM\\n\",\n\t\t\t netdev->name,\n\t\t\t ((hw->bus.speed == e1000_bus_speed_2500) ? \"2.5Gb/s\" :\n\t\t\t  (hw->bus.speed == e1000_bus_speed_5000) ? \"5.0Gb/s\" :\n\t\t\t   \"unknown\"),\n\t\t\t ((hw->bus.width == e1000_bus_width_pcie_x4) ?\n\t\t\t  \"Width x4\" :\n\t\t\t  (hw->bus.width == e1000_bus_width_pcie_x2) ?\n\t\t\t  \"Width x2\" :\n\t\t\t  (hw->bus.width == e1000_bus_width_pcie_x1) ?\n\t\t\t  \"Width x1\" : \"unknown\"), netdev->dev_addr);\n\t}\n\n\tif ((hw->mac.type == e1000_82576 &&\n\t     rd32(E1000_EECD) & E1000_EECD_PRES) ||\n\t    (hw->mac.type >= e1000_i210 ||\n\t     igb_get_flash_presence_i210(hw))) {\n\t\tret_val = igb_read_part_string(hw, part_str,\n\t\t\t\t\t       E1000_PBANUM_LENGTH);\n\t} else {\n\t\tret_val = -E1000_ERR_INVM_VALUE_NOT_FOUND;\n\t}\n\n\tif (ret_val)\n\t\tstrcpy(part_str, \"Unknown\");\n\tdev_info(&pdev->dev, \"%s: PBA No: %s\\n\", netdev->name, part_str);\n\tdev_info(&pdev->dev,\n\t\t\"Using %s interrupts. %d rx queue(s), %d tx queue(s)\\n\",\n\t\t(adapter->flags & IGB_FLAG_HAS_MSIX) ? \"MSI-X\" :\n\t\t(adapter->flags & IGB_FLAG_HAS_MSI) ? \"MSI\" : \"legacy\",\n\t\tadapter->num_rx_queues, adapter->num_tx_queues);\n\tif (hw->phy.media_type == e1000_media_type_copper) {\n\t\tswitch (hw->mac.type) {\n\t\tcase e1000_i350:\n\t\tcase e1000_i210:\n\t\tcase e1000_i211:\n\t\t\t \n\t\t\terr = igb_set_eee_i350(hw, true, true);\n\t\t\tif ((!err) &&\n\t\t\t    (!hw->dev_spec._82575.eee_disable)) {\n\t\t\t\tadapter->eee_advert =\n\t\t\t\t\tMDIO_EEE_100TX | MDIO_EEE_1000T;\n\t\t\t\tadapter->flags |= IGB_FLAG_EEE;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase e1000_i354:\n\t\t\tif ((rd32(E1000_CTRL_EXT) &\n\t\t\t    E1000_CTRL_EXT_LINK_MODE_SGMII)) {\n\t\t\t\terr = igb_set_eee_i354(hw, true, true);\n\t\t\t\tif ((!err) &&\n\t\t\t\t\t(!hw->dev_spec._82575.eee_disable)) {\n\t\t\t\t\tadapter->eee_advert =\n\t\t\t\t\t   MDIO_EEE_100TX | MDIO_EEE_1000T;\n\t\t\t\t\tadapter->flags |= IGB_FLAG_EEE;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tdev_pm_set_driver_flags(&pdev->dev, DPM_FLAG_NO_DIRECT_COMPLETE);\n\n\tpm_runtime_put_noidle(&pdev->dev);\n\treturn 0;\n\nerr_register:\n\tigb_release_hw_control(adapter);\n\tmemset(&adapter->i2c_adap, 0, sizeof(adapter->i2c_adap));\nerr_eeprom:\n\tif (!igb_check_reset_block(hw))\n\t\tigb_reset_phy(hw);\n\n\tif (hw->flash_address)\n\t\tiounmap(hw->flash_address);\nerr_sw_init:\n\tkfree(adapter->mac_table);\n\tkfree(adapter->shadow_vfta);\n\tigb_clear_interrupt_scheme(adapter);\n#ifdef CONFIG_PCI_IOV\n\tigb_disable_sriov(pdev, false);\n#endif\n\tpci_iounmap(pdev, adapter->io_addr);\nerr_ioremap:\n\tfree_netdev(netdev);\nerr_alloc_etherdev:\n\tpci_release_mem_regions(pdev);\nerr_pci_reg:\nerr_dma:\n\tpci_disable_device(pdev);\n\treturn err;\n}\n\n#ifdef CONFIG_PCI_IOV\nstatic int igb_sriov_reinit(struct pci_dev *dev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(dev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct pci_dev *pdev = adapter->pdev;\n\n\trtnl_lock();\n\n\tif (netif_running(netdev))\n\t\tigb_close(netdev);\n\telse\n\t\tigb_reset(adapter);\n\n\tigb_clear_interrupt_scheme(adapter);\n\n\tigb_init_queue_configuration(adapter);\n\n\tif (igb_init_interrupt_scheme(adapter, true)) {\n\t\trtnl_unlock();\n\t\tdev_err(&pdev->dev, \"Unable to allocate memory for queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (netif_running(netdev))\n\t\tigb_open(netdev);\n\n\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic int igb_disable_sriov(struct pci_dev *pdev, bool reinit)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tunsigned long flags;\n\n\t \n\tif (adapter->vf_data) {\n\t\t \n\t\tif (pci_vfs_assigned(pdev)) {\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"Cannot deallocate SR-IOV virtual functions while they are assigned - VFs will not be deallocated\\n\");\n\t\t\treturn -EPERM;\n\t\t} else {\n\t\t\tpci_disable_sriov(pdev);\n\t\t\tmsleep(500);\n\t\t}\n\t\tspin_lock_irqsave(&adapter->vfs_lock, flags);\n\t\tkfree(adapter->vf_mac_list);\n\t\tadapter->vf_mac_list = NULL;\n\t\tkfree(adapter->vf_data);\n\t\tadapter->vf_data = NULL;\n\t\tadapter->vfs_allocated_count = 0;\n\t\tspin_unlock_irqrestore(&adapter->vfs_lock, flags);\n\t\twr32(E1000_IOVCTL, E1000_IOVCTL_REUSE_VFQ);\n\t\twrfl();\n\t\tmsleep(100);\n\t\tdev_info(&pdev->dev, \"IOV Disabled\\n\");\n\n\t\t \n\t\tadapter->flags |= IGB_FLAG_DMAC;\n\t}\n\n\treturn reinit ? igb_sriov_reinit(pdev) : 0;\n}\n\nstatic int igb_enable_sriov(struct pci_dev *pdev, int num_vfs, bool reinit)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tint old_vfs = pci_num_vf(pdev);\n\tstruct vf_mac_filter *mac_list;\n\tint err = 0;\n\tint num_vf_mac_filters, i;\n\n\tif (!(adapter->flags & IGB_FLAG_HAS_MSIX) || num_vfs > 7) {\n\t\terr = -EPERM;\n\t\tgoto out;\n\t}\n\tif (!num_vfs)\n\t\tgoto out;\n\n\tif (old_vfs) {\n\t\tdev_info(&pdev->dev, \"%d pre-allocated VFs found - override max_vfs setting of %d\\n\",\n\t\t\t old_vfs, max_vfs);\n\t\tadapter->vfs_allocated_count = old_vfs;\n\t} else\n\t\tadapter->vfs_allocated_count = num_vfs;\n\n\tadapter->vf_data = kcalloc(adapter->vfs_allocated_count,\n\t\t\t\tsizeof(struct vf_data_storage), GFP_KERNEL);\n\n\t \n\tif (!adapter->vf_data) {\n\t\tadapter->vfs_allocated_count = 0;\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tnum_vf_mac_filters = adapter->hw.mac.rar_entry_count -\n\t\t\t     (1 + IGB_PF_MAC_FILTERS_RESERVED +\n\t\t\t      adapter->vfs_allocated_count);\n\n\tadapter->vf_mac_list = kcalloc(num_vf_mac_filters,\n\t\t\t\t       sizeof(struct vf_mac_filter),\n\t\t\t\t       GFP_KERNEL);\n\n\tmac_list = adapter->vf_mac_list;\n\tINIT_LIST_HEAD(&adapter->vf_macs.l);\n\n\tif (adapter->vf_mac_list) {\n\t\t \n\t\tfor (i = 0; i < num_vf_mac_filters; i++) {\n\t\t\tmac_list->vf = -1;\n\t\t\tmac_list->free = true;\n\t\t\tlist_add(&mac_list->l, &adapter->vf_macs.l);\n\t\t\tmac_list++;\n\t\t}\n\t} else {\n\t\t \n\t\tdev_err(&pdev->dev,\n\t\t\t\"Unable to allocate memory for VF MAC filter list\\n\");\n\t}\n\n\tdev_info(&pdev->dev, \"%d VFs allocated\\n\",\n\t\t adapter->vfs_allocated_count);\n\tfor (i = 0; i < adapter->vfs_allocated_count; i++)\n\t\tigb_vf_configure(adapter, i);\n\n\t \n\tadapter->flags &= ~IGB_FLAG_DMAC;\n\n\tif (reinit) {\n\t\terr = igb_sriov_reinit(pdev);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\n\t \n\tif (!old_vfs) {\n\t\terr = pci_enable_sriov(pdev, adapter->vfs_allocated_count);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\n\tgoto out;\n\nerr_out:\n\tkfree(adapter->vf_mac_list);\n\tadapter->vf_mac_list = NULL;\n\tkfree(adapter->vf_data);\n\tadapter->vf_data = NULL;\n\tadapter->vfs_allocated_count = 0;\nout:\n\treturn err;\n}\n\n#endif\n \nstatic void igb_remove_i2c(struct igb_adapter *adapter)\n{\n\t \n\ti2c_del_adapter(&adapter->i2c_adap);\n}\n\n \nstatic void igb_remove(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\tpm_runtime_get_noresume(&pdev->dev);\n#ifdef CONFIG_IGB_HWMON\n\tigb_sysfs_exit(adapter);\n#endif\n\tigb_remove_i2c(adapter);\n\tigb_ptp_stop(adapter);\n\t \n\tset_bit(__IGB_DOWN, &adapter->state);\n\tdel_timer_sync(&adapter->watchdog_timer);\n\tdel_timer_sync(&adapter->phy_info_timer);\n\n\tcancel_work_sync(&adapter->reset_task);\n\tcancel_work_sync(&adapter->watchdog_task);\n\n#ifdef CONFIG_IGB_DCA\n\tif (adapter->flags & IGB_FLAG_DCA_ENABLED) {\n\t\tdev_info(&pdev->dev, \"DCA disabled\\n\");\n\t\tdca_remove_requester(&pdev->dev);\n\t\tadapter->flags &= ~IGB_FLAG_DCA_ENABLED;\n\t\twr32(E1000_DCA_CTRL, E1000_DCA_CTRL_DCA_MODE_DISABLE);\n\t}\n#endif\n\n\t \n\tigb_release_hw_control(adapter);\n\n#ifdef CONFIG_PCI_IOV\n\tigb_disable_sriov(pdev, false);\n#endif\n\n\tunregister_netdev(netdev);\n\n\tigb_clear_interrupt_scheme(adapter);\n\n\tpci_iounmap(pdev, adapter->io_addr);\n\tif (hw->flash_address)\n\t\tiounmap(hw->flash_address);\n\tpci_release_mem_regions(pdev);\n\n\tkfree(adapter->mac_table);\n\tkfree(adapter->shadow_vfta);\n\tfree_netdev(netdev);\n\n\tpci_disable_device(pdev);\n}\n\n \nstatic void igb_probe_vfs(struct igb_adapter *adapter)\n{\n#ifdef CONFIG_PCI_IOV\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\t \n\tif ((hw->mac.type == e1000_i210) || (hw->mac.type == e1000_i211) ||\n\t    (hw->mac.type == e1000_82580))\n\t\treturn;\n\n\t \n\tigb_set_interrupt_capability(adapter, true);\n\tigb_reset_interrupt_capability(adapter);\n\n\tpci_sriov_set_totalvfs(pdev, 7);\n\tigb_enable_sriov(pdev, max_vfs, false);\n\n#endif  \n}\n\nunsigned int igb_get_max_rss_queues(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tunsigned int max_rss_queues;\n\n\t \n\tswitch (hw->mac.type) {\n\tcase e1000_i211:\n\t\tmax_rss_queues = IGB_MAX_RX_QUEUES_I211;\n\t\tbreak;\n\tcase e1000_82575:\n\tcase e1000_i210:\n\t\tmax_rss_queues = IGB_MAX_RX_QUEUES_82575;\n\t\tbreak;\n\tcase e1000_i350:\n\t\t \n\t\tif (!!adapter->vfs_allocated_count) {\n\t\t\tmax_rss_queues = 1;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase e1000_82576:\n\t\tif (!!adapter->vfs_allocated_count) {\n\t\t\tmax_rss_queues = 2;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase e1000_82580:\n\tcase e1000_i354:\n\tdefault:\n\t\tmax_rss_queues = IGB_MAX_RX_QUEUES;\n\t\tbreak;\n\t}\n\n\treturn max_rss_queues;\n}\n\nstatic void igb_init_queue_configuration(struct igb_adapter *adapter)\n{\n\tu32 max_rss_queues;\n\n\tmax_rss_queues = igb_get_max_rss_queues(adapter);\n\tadapter->rss_queues = min_t(u32, max_rss_queues, num_online_cpus());\n\n\tigb_set_flag_queue_pairs(adapter, max_rss_queues);\n}\n\nvoid igb_set_flag_queue_pairs(struct igb_adapter *adapter,\n\t\t\t      const u32 max_rss_queues)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\t \n\tswitch (hw->mac.type) {\n\tcase e1000_82575:\n\tcase e1000_i211:\n\t\t \n\t\tbreak;\n\tcase e1000_82576:\n\tcase e1000_82580:\n\tcase e1000_i350:\n\tcase e1000_i354:\n\tcase e1000_i210:\n\tdefault:\n\t\t \n\t\tif (adapter->rss_queues > (max_rss_queues / 2))\n\t\t\tadapter->flags |= IGB_FLAG_QUEUE_PAIRS;\n\t\telse\n\t\t\tadapter->flags &= ~IGB_FLAG_QUEUE_PAIRS;\n\t\tbreak;\n\t}\n}\n\n \nstatic int igb_sw_init(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct pci_dev *pdev = adapter->pdev;\n\n\tpci_read_config_word(pdev, PCI_COMMAND, &hw->bus.pci_cmd_word);\n\n\t \n\tadapter->tx_ring_count = IGB_DEFAULT_TXD;\n\tadapter->rx_ring_count = IGB_DEFAULT_RXD;\n\n\t \n\tadapter->rx_itr_setting = IGB_DEFAULT_ITR;\n\tadapter->tx_itr_setting = IGB_DEFAULT_ITR;\n\n\t \n\tadapter->tx_work_limit = IGB_DEFAULT_TX_WORK;\n\n\tadapter->max_frame_size = netdev->mtu + IGB_ETH_PKT_HDR_PAD;\n\tadapter->min_frame_size = ETH_ZLEN + ETH_FCS_LEN;\n\n\tspin_lock_init(&adapter->nfc_lock);\n\tspin_lock_init(&adapter->stats64_lock);\n\n\t \n\tspin_lock_init(&adapter->vfs_lock);\n#ifdef CONFIG_PCI_IOV\n\tswitch (hw->mac.type) {\n\tcase e1000_82576:\n\tcase e1000_i350:\n\t\tif (max_vfs > 7) {\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"Maximum of 7 VFs per PF, using max\\n\");\n\t\t\tmax_vfs = adapter->vfs_allocated_count = 7;\n\t\t} else\n\t\t\tadapter->vfs_allocated_count = max_vfs;\n\t\tif (adapter->vfs_allocated_count)\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"Enabling SR-IOV VFs using the module parameter is deprecated - please use the pci sysfs interface.\\n\");\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n#endif  \n\n\t \n\tadapter->flags |= IGB_FLAG_HAS_MSIX;\n\n\tadapter->mac_table = kcalloc(hw->mac.rar_entry_count,\n\t\t\t\t     sizeof(struct igb_mac_addr),\n\t\t\t\t     GFP_KERNEL);\n\tif (!adapter->mac_table)\n\t\treturn -ENOMEM;\n\n\tigb_probe_vfs(adapter);\n\n\tigb_init_queue_configuration(adapter);\n\n\t \n\tadapter->shadow_vfta = kcalloc(E1000_VLAN_FILTER_TBL_SIZE, sizeof(u32),\n\t\t\t\t       GFP_KERNEL);\n\tif (!adapter->shadow_vfta)\n\t\treturn -ENOMEM;\n\n\t \n\tif (igb_init_interrupt_scheme(adapter, true)) {\n\t\tdev_err(&pdev->dev, \"Unable to allocate memory for queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tigb_irq_disable(adapter);\n\n\tif (hw->mac.type >= e1000_i350)\n\t\tadapter->flags &= ~IGB_FLAG_DMAC;\n\n\tset_bit(__IGB_DOWN, &adapter->state);\n\treturn 0;\n}\n\n \nstatic int __igb_open(struct net_device *netdev, bool resuming)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tint err;\n\tint i;\n\n\t \n\tif (test_bit(__IGB_TESTING, &adapter->state)) {\n\t\tWARN_ON(resuming);\n\t\treturn -EBUSY;\n\t}\n\n\tif (!resuming)\n\t\tpm_runtime_get_sync(&pdev->dev);\n\n\tnetif_carrier_off(netdev);\n\n\t \n\terr = igb_setup_all_tx_resources(adapter);\n\tif (err)\n\t\tgoto err_setup_tx;\n\n\t \n\terr = igb_setup_all_rx_resources(adapter);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\tigb_power_up_link(adapter);\n\n\t \n\tigb_configure(adapter);\n\n\terr = igb_request_irq(adapter);\n\tif (err)\n\t\tgoto err_req_irq;\n\n\t \n\terr = netif_set_real_num_tx_queues(adapter->netdev,\n\t\t\t\t\t   adapter->num_tx_queues);\n\tif (err)\n\t\tgoto err_set_queues;\n\n\terr = netif_set_real_num_rx_queues(adapter->netdev,\n\t\t\t\t\t   adapter->num_rx_queues);\n\tif (err)\n\t\tgoto err_set_queues;\n\n\t \n\tclear_bit(__IGB_DOWN, &adapter->state);\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\tnapi_enable(&(adapter->q_vector[i]->napi));\n\n\t \n\trd32(E1000_TSICR);\n\trd32(E1000_ICR);\n\n\tigb_irq_enable(adapter);\n\n\t \n\tif (adapter->vfs_allocated_count) {\n\t\tu32 reg_data = rd32(E1000_CTRL_EXT);\n\n\t\treg_data |= E1000_CTRL_EXT_PFRSTD;\n\t\twr32(E1000_CTRL_EXT, reg_data);\n\t}\n\n\tnetif_tx_start_all_queues(netdev);\n\n\tif (!resuming)\n\t\tpm_runtime_put(&pdev->dev);\n\n\t \n\thw->mac.get_link_status = 1;\n\tschedule_work(&adapter->watchdog_task);\n\n\treturn 0;\n\nerr_set_queues:\n\tigb_free_irq(adapter);\nerr_req_irq:\n\tigb_release_hw_control(adapter);\n\tigb_power_down_link(adapter);\n\tigb_free_all_rx_resources(adapter);\nerr_setup_rx:\n\tigb_free_all_tx_resources(adapter);\nerr_setup_tx:\n\tigb_reset(adapter);\n\tif (!resuming)\n\t\tpm_runtime_put(&pdev->dev);\n\n\treturn err;\n}\n\nint igb_open(struct net_device *netdev)\n{\n\treturn __igb_open(netdev, false);\n}\n\n \nstatic int __igb_close(struct net_device *netdev, bool suspending)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct pci_dev *pdev = adapter->pdev;\n\n\tWARN_ON(test_bit(__IGB_RESETTING, &adapter->state));\n\n\tif (!suspending)\n\t\tpm_runtime_get_sync(&pdev->dev);\n\n\tigb_down(adapter);\n\tigb_free_irq(adapter);\n\n\tigb_free_all_tx_resources(adapter);\n\tigb_free_all_rx_resources(adapter);\n\n\tif (!suspending)\n\t\tpm_runtime_put_sync(&pdev->dev);\n\treturn 0;\n}\n\nint igb_close(struct net_device *netdev)\n{\n\tif (netif_device_present(netdev) || netdev->dismantle)\n\t\treturn __igb_close(netdev, false);\n\treturn 0;\n}\n\n \nint igb_setup_tx_resources(struct igb_ring *tx_ring)\n{\n\tstruct device *dev = tx_ring->dev;\n\tint size;\n\n\tsize = sizeof(struct igb_tx_buffer) * tx_ring->count;\n\n\ttx_ring->tx_buffer_info = vmalloc(size);\n\tif (!tx_ring->tx_buffer_info)\n\t\tgoto err;\n\n\t \n\ttx_ring->size = tx_ring->count * sizeof(union e1000_adv_tx_desc);\n\ttx_ring->size = ALIGN(tx_ring->size, 4096);\n\n\ttx_ring->desc = dma_alloc_coherent(dev, tx_ring->size,\n\t\t\t\t\t   &tx_ring->dma, GFP_KERNEL);\n\tif (!tx_ring->desc)\n\t\tgoto err;\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\n\treturn 0;\n\nerr:\n\tvfree(tx_ring->tx_buffer_info);\n\ttx_ring->tx_buffer_info = NULL;\n\tdev_err(dev, \"Unable to allocate memory for the Tx descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int igb_setup_all_tx_resources(struct igb_adapter *adapter)\n{\n\tstruct pci_dev *pdev = adapter->pdev;\n\tint i, err = 0;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\terr = igb_setup_tx_resources(adapter->tx_ring[i]);\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"Allocation for Tx Queue %u failed\\n\", i);\n\t\t\tfor (i--; i >= 0; i--)\n\t\t\t\tigb_free_tx_resources(adapter->tx_ring[i]);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\n \nvoid igb_setup_tctl(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 tctl;\n\n\t \n\twr32(E1000_TXDCTL(0), 0);\n\n\t \n\ttctl = rd32(E1000_TCTL);\n\ttctl &= ~E1000_TCTL_CT;\n\ttctl |= E1000_TCTL_PSP | E1000_TCTL_RTLC |\n\t\t(E1000_COLLISION_THRESHOLD << E1000_CT_SHIFT);\n\n\tigb_config_collision_dist(hw);\n\n\t \n\ttctl |= E1000_TCTL_EN;\n\n\twr32(E1000_TCTL, tctl);\n}\n\n \nvoid igb_configure_tx_ring(struct igb_adapter *adapter,\n\t\t\t   struct igb_ring *ring)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 txdctl = 0;\n\tu64 tdba = ring->dma;\n\tint reg_idx = ring->reg_idx;\n\n\twr32(E1000_TDLEN(reg_idx),\n\t     ring->count * sizeof(union e1000_adv_tx_desc));\n\twr32(E1000_TDBAL(reg_idx),\n\t     tdba & 0x00000000ffffffffULL);\n\twr32(E1000_TDBAH(reg_idx), tdba >> 32);\n\n\tring->tail = adapter->io_addr + E1000_TDT(reg_idx);\n\twr32(E1000_TDH(reg_idx), 0);\n\twritel(0, ring->tail);\n\n\ttxdctl |= IGB_TX_PTHRESH;\n\ttxdctl |= IGB_TX_HTHRESH << 8;\n\ttxdctl |= IGB_TX_WTHRESH << 16;\n\n\t \n\tmemset(ring->tx_buffer_info, 0,\n\t       sizeof(struct igb_tx_buffer) * ring->count);\n\n\ttxdctl |= E1000_TXDCTL_QUEUE_ENABLE;\n\twr32(E1000_TXDCTL(reg_idx), txdctl);\n}\n\n \nstatic void igb_configure_tx(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint i;\n\n\t \n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\twr32(E1000_TXDCTL(adapter->tx_ring[i]->reg_idx), 0);\n\n\twrfl();\n\tusleep_range(10000, 20000);\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tigb_configure_tx_ring(adapter, adapter->tx_ring[i]);\n}\n\n \nint igb_setup_rx_resources(struct igb_ring *rx_ring)\n{\n\tstruct igb_adapter *adapter = netdev_priv(rx_ring->netdev);\n\tstruct device *dev = rx_ring->dev;\n\tint size, res;\n\n\t \n\tif (xdp_rxq_info_is_reg(&rx_ring->xdp_rxq))\n\t\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\tres = xdp_rxq_info_reg(&rx_ring->xdp_rxq, rx_ring->netdev,\n\t\t\t       rx_ring->queue_index, 0);\n\tif (res < 0) {\n\t\tdev_err(dev, \"Failed to register xdp_rxq index %u\\n\",\n\t\t\trx_ring->queue_index);\n\t\treturn res;\n\t}\n\n\tsize = sizeof(struct igb_rx_buffer) * rx_ring->count;\n\n\trx_ring->rx_buffer_info = vmalloc(size);\n\tif (!rx_ring->rx_buffer_info)\n\t\tgoto err;\n\n\t \n\trx_ring->size = rx_ring->count * sizeof(union e1000_adv_rx_desc);\n\trx_ring->size = ALIGN(rx_ring->size, 4096);\n\n\trx_ring->desc = dma_alloc_coherent(dev, rx_ring->size,\n\t\t\t\t\t   &rx_ring->dma, GFP_KERNEL);\n\tif (!rx_ring->desc)\n\t\tgoto err;\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n\n\trx_ring->xdp_prog = adapter->xdp_prog;\n\n\treturn 0;\n\nerr:\n\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\tvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\tdev_err(dev, \"Unable to allocate memory for the Rx descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int igb_setup_all_rx_resources(struct igb_adapter *adapter)\n{\n\tstruct pci_dev *pdev = adapter->pdev;\n\tint i, err = 0;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\terr = igb_setup_rx_resources(adapter->rx_ring[i]);\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"Allocation for Rx Queue %u failed\\n\", i);\n\t\t\tfor (i--; i >= 0; i--)\n\t\t\t\tigb_free_rx_resources(adapter->rx_ring[i]);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\n \nstatic void igb_setup_mrqc(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 mrqc, rxcsum;\n\tu32 j, num_rx_queues;\n\tu32 rss_key[10];\n\n\tnetdev_rss_key_fill(rss_key, sizeof(rss_key));\n\tfor (j = 0; j < 10; j++)\n\t\twr32(E1000_RSSRK(j), rss_key[j]);\n\n\tnum_rx_queues = adapter->rss_queues;\n\n\tswitch (hw->mac.type) {\n\tcase e1000_82576:\n\t\t \n\t\tif (adapter->vfs_allocated_count)\n\t\t\tnum_rx_queues = 2;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (adapter->rss_indir_tbl_init != num_rx_queues) {\n\t\tfor (j = 0; j < IGB_RETA_SIZE; j++)\n\t\t\tadapter->rss_indir_tbl[j] =\n\t\t\t(j * num_rx_queues) / IGB_RETA_SIZE;\n\t\tadapter->rss_indir_tbl_init = num_rx_queues;\n\t}\n\tigb_write_rss_indir_tbl(adapter);\n\n\t \n\trxcsum = rd32(E1000_RXCSUM);\n\trxcsum |= E1000_RXCSUM_PCSD;\n\n\tif (adapter->hw.mac.type >= e1000_82576)\n\t\t \n\t\trxcsum |= E1000_RXCSUM_CRCOFL;\n\n\t \n\twr32(E1000_RXCSUM, rxcsum);\n\n\t \n\tmrqc = E1000_MRQC_RSS_FIELD_IPV4 |\n\t       E1000_MRQC_RSS_FIELD_IPV4_TCP |\n\t       E1000_MRQC_RSS_FIELD_IPV6 |\n\t       E1000_MRQC_RSS_FIELD_IPV6_TCP |\n\t       E1000_MRQC_RSS_FIELD_IPV6_TCP_EX;\n\n\tif (adapter->flags & IGB_FLAG_RSS_FIELD_IPV4_UDP)\n\t\tmrqc |= E1000_MRQC_RSS_FIELD_IPV4_UDP;\n\tif (adapter->flags & IGB_FLAG_RSS_FIELD_IPV6_UDP)\n\t\tmrqc |= E1000_MRQC_RSS_FIELD_IPV6_UDP;\n\n\t \n\tif (adapter->vfs_allocated_count) {\n\t\tif (hw->mac.type > e1000_82575) {\n\t\t\t \n\t\t\tu32 vtctl = rd32(E1000_VT_CTL);\n\n\t\t\tvtctl &= ~(E1000_VT_CTL_DEFAULT_POOL_MASK |\n\t\t\t\t   E1000_VT_CTL_DISABLE_DEF_POOL);\n\t\t\tvtctl |= adapter->vfs_allocated_count <<\n\t\t\t\tE1000_VT_CTL_DEFAULT_POOL_SHIFT;\n\t\t\twr32(E1000_VT_CTL, vtctl);\n\t\t}\n\t\tif (adapter->rss_queues > 1)\n\t\t\tmrqc |= E1000_MRQC_ENABLE_VMDQ_RSS_MQ;\n\t\telse\n\t\t\tmrqc |= E1000_MRQC_ENABLE_VMDQ;\n\t} else {\n\t\tmrqc |= E1000_MRQC_ENABLE_RSS_MQ;\n\t}\n\tigb_vmm_control(adapter);\n\n\twr32(E1000_MRQC, mrqc);\n}\n\n \nvoid igb_setup_rctl(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 rctl;\n\n\trctl = rd32(E1000_RCTL);\n\n\trctl &= ~(3 << E1000_RCTL_MO_SHIFT);\n\trctl &= ~(E1000_RCTL_LBM_TCVR | E1000_RCTL_LBM_MAC);\n\n\trctl |= E1000_RCTL_EN | E1000_RCTL_BAM | E1000_RCTL_RDMTS_HALF |\n\t\t(hw->mac.mc_filter_type << E1000_RCTL_MO_SHIFT);\n\n\t \n\trctl |= E1000_RCTL_SECRC;\n\n\t \n\trctl &= ~(E1000_RCTL_SBP | E1000_RCTL_SZ_256);\n\n\t \n\trctl |= E1000_RCTL_LPE;\n\n\t \n\twr32(E1000_RXDCTL(0), 0);\n\n\t \n\tif (adapter->vfs_allocated_count) {\n\t\t \n\t\twr32(E1000_QDE, ALL_QUEUES);\n\t}\n\n\t \n\tif (adapter->netdev->features & NETIF_F_RXALL) {\n\t\t \n\t\trctl |= (E1000_RCTL_SBP |  \n\t\t\t E1000_RCTL_BAM |  \n\t\t\t E1000_RCTL_PMCF);  \n\n\t\trctl &= ~(E1000_RCTL_DPF |  \n\t\t\t  E1000_RCTL_CFIEN);  \n\t\t \n\t}\n\n\twr32(E1000_RCTL, rctl);\n}\n\nstatic inline int igb_set_vf_rlpml(struct igb_adapter *adapter, int size,\n\t\t\t\t   int vfn)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 vmolr;\n\n\tif (size > MAX_JUMBO_FRAME_SIZE)\n\t\tsize = MAX_JUMBO_FRAME_SIZE;\n\n\tvmolr = rd32(E1000_VMOLR(vfn));\n\tvmolr &= ~E1000_VMOLR_RLPML_MASK;\n\tvmolr |= size | E1000_VMOLR_LPE;\n\twr32(E1000_VMOLR(vfn), vmolr);\n\n\treturn 0;\n}\n\nstatic inline void igb_set_vf_vlan_strip(struct igb_adapter *adapter,\n\t\t\t\t\t int vfn, bool enable)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 val, reg;\n\n\tif (hw->mac.type < e1000_82576)\n\t\treturn;\n\n\tif (hw->mac.type == e1000_i350)\n\t\treg = E1000_DVMOLR(vfn);\n\telse\n\t\treg = E1000_VMOLR(vfn);\n\n\tval = rd32(reg);\n\tif (enable)\n\t\tval |= E1000_VMOLR_STRVLAN;\n\telse\n\t\tval &= ~(E1000_VMOLR_STRVLAN);\n\twr32(reg, val);\n}\n\nstatic inline void igb_set_vmolr(struct igb_adapter *adapter,\n\t\t\t\t int vfn, bool aupe)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 vmolr;\n\n\t \n\tif (hw->mac.type < e1000_82576)\n\t\treturn;\n\n\tvmolr = rd32(E1000_VMOLR(vfn));\n\tif (aupe)\n\t\tvmolr |= E1000_VMOLR_AUPE;  \n\telse\n\t\tvmolr &= ~(E1000_VMOLR_AUPE);  \n\n\t \n\tvmolr &= ~(E1000_VMOLR_BAM | E1000_VMOLR_RSSE);\n\n\tif (adapter->rss_queues > 1 && vfn == adapter->vfs_allocated_count)\n\t\tvmolr |= E1000_VMOLR_RSSE;  \n\t \n\tif (vfn <= adapter->vfs_allocated_count)\n\t\tvmolr |= E1000_VMOLR_BAM;  \n\n\twr32(E1000_VMOLR(vfn), vmolr);\n}\n\n \nvoid igb_setup_srrctl(struct igb_adapter *adapter, struct igb_ring *ring)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint reg_idx = ring->reg_idx;\n\tu32 srrctl = 0;\n\n\tsrrctl = IGB_RX_HDR_LEN << E1000_SRRCTL_BSIZEHDRSIZE_SHIFT;\n\tif (ring_uses_large_buffer(ring))\n\t\tsrrctl |= IGB_RXBUFFER_3072 >> E1000_SRRCTL_BSIZEPKT_SHIFT;\n\telse\n\t\tsrrctl |= IGB_RXBUFFER_2048 >> E1000_SRRCTL_BSIZEPKT_SHIFT;\n\tsrrctl |= E1000_SRRCTL_DESCTYPE_ADV_ONEBUF;\n\tif (hw->mac.type >= e1000_82580)\n\t\tsrrctl |= E1000_SRRCTL_TIMESTAMP;\n\t \n\tif (adapter->vfs_allocated_count ||\n\t    (!(hw->fc.current_mode & e1000_fc_rx_pause) &&\n\t     adapter->num_rx_queues > 1))\n\t\tsrrctl |= E1000_SRRCTL_DROP_EN;\n\n\twr32(E1000_SRRCTL(reg_idx), srrctl);\n}\n\n \nvoid igb_configure_rx_ring(struct igb_adapter *adapter,\n\t\t\t   struct igb_ring *ring)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tunion e1000_adv_rx_desc *rx_desc;\n\tu64 rdba = ring->dma;\n\tint reg_idx = ring->reg_idx;\n\tu32 rxdctl = 0;\n\n\txdp_rxq_info_unreg_mem_model(&ring->xdp_rxq);\n\tWARN_ON(xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,\n\t\t\t\t\t   MEM_TYPE_PAGE_SHARED, NULL));\n\n\t \n\twr32(E1000_RXDCTL(reg_idx), 0);\n\n\t \n\twr32(E1000_RDBAL(reg_idx),\n\t     rdba & 0x00000000ffffffffULL);\n\twr32(E1000_RDBAH(reg_idx), rdba >> 32);\n\twr32(E1000_RDLEN(reg_idx),\n\t     ring->count * sizeof(union e1000_adv_rx_desc));\n\n\t \n\tring->tail = adapter->io_addr + E1000_RDT(reg_idx);\n\twr32(E1000_RDH(reg_idx), 0);\n\twritel(0, ring->tail);\n\n\t \n\tigb_setup_srrctl(adapter, ring);\n\n\t \n\tigb_set_vmolr(adapter, reg_idx & 0x7, true);\n\n\trxdctl |= IGB_RX_PTHRESH;\n\trxdctl |= IGB_RX_HTHRESH << 8;\n\trxdctl |= IGB_RX_WTHRESH << 16;\n\n\t \n\tmemset(ring->rx_buffer_info, 0,\n\t       sizeof(struct igb_rx_buffer) * ring->count);\n\n\t \n\trx_desc = IGB_RX_DESC(ring, 0);\n\trx_desc->wb.upper.length = 0;\n\n\t \n\trxdctl |= E1000_RXDCTL_QUEUE_ENABLE;\n\twr32(E1000_RXDCTL(reg_idx), rxdctl);\n}\n\nstatic void igb_set_rx_buffer_len(struct igb_adapter *adapter,\n\t\t\t\t  struct igb_ring *rx_ring)\n{\n#if (PAGE_SIZE < 8192)\n\tstruct e1000_hw *hw = &adapter->hw;\n#endif\n\n\t \n\tclear_ring_build_skb_enabled(rx_ring);\n\tclear_ring_uses_large_buffer(rx_ring);\n\n\tif (adapter->flags & IGB_FLAG_RX_LEGACY)\n\t\treturn;\n\n\tset_ring_build_skb_enabled(rx_ring);\n\n#if (PAGE_SIZE < 8192)\n\tif (adapter->max_frame_size > IGB_MAX_FRAME_BUILD_SKB ||\n\t    rd32(E1000_RCTL) & E1000_RCTL_SBP)\n\t\tset_ring_uses_large_buffer(rx_ring);\n#endif\n}\n\n \nstatic void igb_configure_rx(struct igb_adapter *adapter)\n{\n\tint i;\n\n\t \n\tigb_set_default_mac_filter(adapter);\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct igb_ring *rx_ring = adapter->rx_ring[i];\n\n\t\tigb_set_rx_buffer_len(adapter, rx_ring);\n\t\tigb_configure_rx_ring(adapter, rx_ring);\n\t}\n}\n\n \nvoid igb_free_tx_resources(struct igb_ring *tx_ring)\n{\n\tigb_clean_tx_ring(tx_ring);\n\n\tvfree(tx_ring->tx_buffer_info);\n\ttx_ring->tx_buffer_info = NULL;\n\n\t \n\tif (!tx_ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(tx_ring->dev, tx_ring->size,\n\t\t\t  tx_ring->desc, tx_ring->dma);\n\n\ttx_ring->desc = NULL;\n}\n\n \nstatic void igb_free_all_tx_resources(struct igb_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tif (adapter->tx_ring[i])\n\t\t\tigb_free_tx_resources(adapter->tx_ring[i]);\n}\n\n \nstatic void igb_clean_tx_ring(struct igb_ring *tx_ring)\n{\n\tu16 i = tx_ring->next_to_clean;\n\tstruct igb_tx_buffer *tx_buffer = &tx_ring->tx_buffer_info[i];\n\n\twhile (i != tx_ring->next_to_use) {\n\t\tunion e1000_adv_tx_desc *eop_desc, *tx_desc;\n\n\t\t \n\t\tif (tx_buffer->type == IGB_TYPE_SKB)\n\t\t\tdev_kfree_skb_any(tx_buffer->skb);\n\t\telse\n\t\t\txdp_return_frame(tx_buffer->xdpf);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\teop_desc = tx_buffer->next_to_watch;\n\t\ttx_desc = IGB_TX_DESC(tx_ring, i);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\t\ti = 0;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = IGB_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t}\n\n\t\ttx_buffer->next_to_watch = NULL;\n\n\t\t \n\t\ttx_buffer++;\n\t\ti++;\n\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\ti = 0;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t}\n\t}\n\n\t \n\tnetdev_tx_reset_queue(txring_txq(tx_ring));\n\n\t \n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n}\n\n \nstatic void igb_clean_all_tx_rings(struct igb_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tif (adapter->tx_ring[i])\n\t\t\tigb_clean_tx_ring(adapter->tx_ring[i]);\n}\n\n \nvoid igb_free_rx_resources(struct igb_ring *rx_ring)\n{\n\tigb_clean_rx_ring(rx_ring);\n\n\trx_ring->xdp_prog = NULL;\n\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\tvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\n\t \n\tif (!rx_ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(rx_ring->dev, rx_ring->size,\n\t\t\t  rx_ring->desc, rx_ring->dma);\n\n\trx_ring->desc = NULL;\n}\n\n \nstatic void igb_free_all_rx_resources(struct igb_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tif (adapter->rx_ring[i])\n\t\t\tigb_free_rx_resources(adapter->rx_ring[i]);\n}\n\n \nstatic void igb_clean_rx_ring(struct igb_ring *rx_ring)\n{\n\tu16 i = rx_ring->next_to_clean;\n\n\tdev_kfree_skb(rx_ring->skb);\n\trx_ring->skb = NULL;\n\n\t \n\twhile (i != rx_ring->next_to_alloc) {\n\t\tstruct igb_rx_buffer *buffer_info = &rx_ring->rx_buffer_info[i];\n\n\t\t \n\t\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t\t      buffer_info->dma,\n\t\t\t\t\t      buffer_info->page_offset,\n\t\t\t\t\t      igb_rx_bufsz(rx_ring),\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev,\n\t\t\t\t     buffer_info->dma,\n\t\t\t\t     igb_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t     IGB_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(buffer_info->page,\n\t\t\t\t\tbuffer_info->pagecnt_bias);\n\n\t\ti++;\n\t\tif (i == rx_ring->count)\n\t\t\ti = 0;\n\t}\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n}\n\n \nstatic void igb_clean_all_rx_rings(struct igb_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tif (adapter->rx_ring[i])\n\t\t\tigb_clean_rx_ring(adapter->rx_ring[i]);\n}\n\n \nstatic int igb_set_mac(struct net_device *netdev, void *p)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct sockaddr *addr = p;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\teth_hw_addr_set(netdev, addr->sa_data);\n\tmemcpy(hw->mac.addr, addr->sa_data, netdev->addr_len);\n\n\t \n\tigb_set_default_mac_filter(adapter);\n\n\treturn 0;\n}\n\n \nstatic int igb_write_mc_addr_list(struct net_device *netdev)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct netdev_hw_addr *ha;\n\tu8  *mta_list;\n\tint i;\n\n\tif (netdev_mc_empty(netdev)) {\n\t\t \n\t\tigb_update_mc_addr_list(hw, NULL, 0);\n\t\tigb_restore_vf_multicasts(adapter);\n\t\treturn 0;\n\t}\n\n\tmta_list = kcalloc(netdev_mc_count(netdev), 6, GFP_ATOMIC);\n\tif (!mta_list)\n\t\treturn -ENOMEM;\n\n\t \n\ti = 0;\n\tnetdev_for_each_mc_addr(ha, netdev)\n\t\tmemcpy(mta_list + (i++ * ETH_ALEN), ha->addr, ETH_ALEN);\n\n\tigb_update_mc_addr_list(hw, mta_list, i);\n\tkfree(mta_list);\n\n\treturn netdev_mc_count(netdev);\n}\n\nstatic int igb_vlan_promisc_enable(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 i, pf_id;\n\n\tswitch (hw->mac.type) {\n\tcase e1000_i210:\n\tcase e1000_i211:\n\tcase e1000_i350:\n\t\t \n\t\tif (adapter->netdev->features & NETIF_F_NTUPLE)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase e1000_82576:\n\tcase e1000_82580:\n\tcase e1000_i354:\n\t\t \n\t\tif (adapter->vfs_allocated_count)\n\t\t\tbreak;\n\t\tfallthrough;\n\tdefault:\n\t\treturn 1;\n\t}\n\n\t \n\tif (adapter->flags & IGB_FLAG_VLAN_PROMISC)\n\t\treturn 0;\n\n\tif (!adapter->vfs_allocated_count)\n\t\tgoto set_vfta;\n\n\t \n\tpf_id = adapter->vfs_allocated_count + E1000_VLVF_POOLSEL_SHIFT;\n\n\tfor (i = E1000_VLVF_ARRAY_SIZE; --i;) {\n\t\tu32 vlvf = rd32(E1000_VLVF(i));\n\n\t\tvlvf |= BIT(pf_id);\n\t\twr32(E1000_VLVF(i), vlvf);\n\t}\n\nset_vfta:\n\t \n\tfor (i = E1000_VLAN_FILTER_TBL_SIZE; i--;)\n\t\thw->mac.ops.write_vfta(hw, i, ~0U);\n\n\t \n\tadapter->flags |= IGB_FLAG_VLAN_PROMISC;\n\n\treturn 0;\n}\n\n#define VFTA_BLOCK_SIZE 8\nstatic void igb_scrub_vfta(struct igb_adapter *adapter, u32 vfta_offset)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 vfta[VFTA_BLOCK_SIZE] = { 0 };\n\tu32 vid_start = vfta_offset * 32;\n\tu32 vid_end = vid_start + (VFTA_BLOCK_SIZE * 32);\n\tu32 i, vid, word, bits, pf_id;\n\n\t \n\tvid = adapter->mng_vlan_id;\n\tif (vid >= vid_start && vid < vid_end)\n\t\tvfta[(vid - vid_start) / 32] |= BIT(vid % 32);\n\n\tif (!adapter->vfs_allocated_count)\n\t\tgoto set_vfta;\n\n\tpf_id = adapter->vfs_allocated_count + E1000_VLVF_POOLSEL_SHIFT;\n\n\tfor (i = E1000_VLVF_ARRAY_SIZE; --i;) {\n\t\tu32 vlvf = rd32(E1000_VLVF(i));\n\n\t\t \n\t\tvid = vlvf & VLAN_VID_MASK;\n\n\t\t \n\t\tif (vid < vid_start || vid >= vid_end)\n\t\t\tcontinue;\n\n\t\tif (vlvf & E1000_VLVF_VLANID_ENABLE) {\n\t\t\t \n\t\t\tvfta[(vid - vid_start) / 32] |= BIT(vid % 32);\n\n\t\t\t \n\t\t\tif (test_bit(vid, adapter->active_vlans))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tbits = ~BIT(pf_id);\n\t\tbits &= rd32(E1000_VLVF(i));\n\t\twr32(E1000_VLVF(i), bits);\n\t}\n\nset_vfta:\n\t \n\tfor (i = VFTA_BLOCK_SIZE; i--;) {\n\t\tvid = (vfta_offset + i) * 32;\n\t\tword = vid / BITS_PER_LONG;\n\t\tbits = vid % BITS_PER_LONG;\n\n\t\tvfta[i] |= adapter->active_vlans[word] >> bits;\n\n\t\thw->mac.ops.write_vfta(hw, vfta_offset + i, vfta[i]);\n\t}\n}\n\nstatic void igb_vlan_promisc_disable(struct igb_adapter *adapter)\n{\n\tu32 i;\n\n\t \n\tif (!(adapter->flags & IGB_FLAG_VLAN_PROMISC))\n\t\treturn;\n\n\t \n\tadapter->flags &= ~IGB_FLAG_VLAN_PROMISC;\n\n\tfor (i = 0; i < E1000_VLAN_FILTER_TBL_SIZE; i += VFTA_BLOCK_SIZE)\n\t\tigb_scrub_vfta(adapter, i);\n}\n\n \nstatic void igb_set_rx_mode(struct net_device *netdev)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tunsigned int vfn = adapter->vfs_allocated_count;\n\tu32 rctl = 0, vmolr = 0, rlpml = MAX_JUMBO_FRAME_SIZE;\n\tint count;\n\n\t \n\tif (netdev->flags & IFF_PROMISC) {\n\t\trctl |= E1000_RCTL_UPE | E1000_RCTL_MPE;\n\t\tvmolr |= E1000_VMOLR_MPME;\n\n\t\t \n\t\tif (hw->mac.type == e1000_82576)\n\t\t\tvmolr |= E1000_VMOLR_ROPE;\n\t} else {\n\t\tif (netdev->flags & IFF_ALLMULTI) {\n\t\t\trctl |= E1000_RCTL_MPE;\n\t\t\tvmolr |= E1000_VMOLR_MPME;\n\t\t} else {\n\t\t\t \n\t\t\tcount = igb_write_mc_addr_list(netdev);\n\t\t\tif (count < 0) {\n\t\t\t\trctl |= E1000_RCTL_MPE;\n\t\t\t\tvmolr |= E1000_VMOLR_MPME;\n\t\t\t} else if (count) {\n\t\t\t\tvmolr |= E1000_VMOLR_ROMPE;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (__dev_uc_sync(netdev, igb_uc_sync, igb_uc_unsync)) {\n\t\trctl |= E1000_RCTL_UPE;\n\t\tvmolr |= E1000_VMOLR_ROPE;\n\t}\n\n\t \n\trctl |= E1000_RCTL_VFE;\n\n\t \n\tif ((netdev->flags & IFF_PROMISC) ||\n\t    (netdev->features & NETIF_F_RXALL)) {\n\t\t \n\t\tif (igb_vlan_promisc_enable(adapter))\n\t\t\trctl &= ~E1000_RCTL_VFE;\n\t} else {\n\t\tigb_vlan_promisc_disable(adapter);\n\t}\n\n\t \n\trctl |= rd32(E1000_RCTL) & ~(E1000_RCTL_UPE | E1000_RCTL_MPE |\n\t\t\t\t     E1000_RCTL_VFE);\n\twr32(E1000_RCTL, rctl);\n\n#if (PAGE_SIZE < 8192)\n\tif (!adapter->vfs_allocated_count) {\n\t\tif (adapter->max_frame_size <= IGB_MAX_FRAME_BUILD_SKB)\n\t\t\trlpml = IGB_MAX_FRAME_BUILD_SKB;\n\t}\n#endif\n\twr32(E1000_RLPML, rlpml);\n\n\t \n\tif ((hw->mac.type < e1000_82576) || (hw->mac.type > e1000_i350))\n\t\treturn;\n\n\t \n\tigb_set_uta(adapter, !!(vmolr & E1000_VMOLR_ROPE));\n\n\tvmolr |= rd32(E1000_VMOLR(vfn)) &\n\t\t ~(E1000_VMOLR_ROPE | E1000_VMOLR_MPME | E1000_VMOLR_ROMPE);\n\n\t \n\tvmolr &= ~E1000_VMOLR_RLPML_MASK;\n#if (PAGE_SIZE < 8192)\n\tif (adapter->max_frame_size <= IGB_MAX_FRAME_BUILD_SKB)\n\t\tvmolr |= IGB_MAX_FRAME_BUILD_SKB;\n\telse\n#endif\n\t\tvmolr |= MAX_JUMBO_FRAME_SIZE;\n\tvmolr |= E1000_VMOLR_LPE;\n\n\twr32(E1000_VMOLR(vfn), vmolr);\n\n\tigb_restore_vf_multicasts(adapter);\n}\n\nstatic void igb_check_wvbr(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 wvbr = 0;\n\n\tswitch (hw->mac.type) {\n\tcase e1000_82576:\n\tcase e1000_i350:\n\t\twvbr = rd32(E1000_WVBR);\n\t\tif (!wvbr)\n\t\t\treturn;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tadapter->wvbr |= wvbr;\n}\n\n#define IGB_STAGGERED_QUEUE_OFFSET 8\n\nstatic void igb_spoof_check(struct igb_adapter *adapter)\n{\n\tint j;\n\n\tif (!adapter->wvbr)\n\t\treturn;\n\n\tfor (j = 0; j < adapter->vfs_allocated_count; j++) {\n\t\tif (adapter->wvbr & BIT(j) ||\n\t\t    adapter->wvbr & BIT(j + IGB_STAGGERED_QUEUE_OFFSET)) {\n\t\t\tdev_warn(&adapter->pdev->dev,\n\t\t\t\t\"Spoof event(s) detected on VF %d\\n\", j);\n\t\t\tadapter->wvbr &=\n\t\t\t\t~(BIT(j) |\n\t\t\t\t  BIT(j + IGB_STAGGERED_QUEUE_OFFSET));\n\t\t}\n\t}\n}\n\n \nstatic void igb_update_phy_info(struct timer_list *t)\n{\n\tstruct igb_adapter *adapter = from_timer(adapter, t, phy_info_timer);\n\tigb_get_phy_info(&adapter->hw);\n}\n\n \nbool igb_has_link(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tbool link_active = false;\n\n\t \n\tswitch (hw->phy.media_type) {\n\tcase e1000_media_type_copper:\n\t\tif (!hw->mac.get_link_status)\n\t\t\treturn true;\n\t\tfallthrough;\n\tcase e1000_media_type_internal_serdes:\n\t\thw->mac.ops.check_for_link(hw);\n\t\tlink_active = !hw->mac.get_link_status;\n\t\tbreak;\n\tdefault:\n\tcase e1000_media_type_unknown:\n\t\tbreak;\n\t}\n\n\tif (((hw->mac.type == e1000_i210) ||\n\t     (hw->mac.type == e1000_i211)) &&\n\t     (hw->phy.id == I210_I_PHY_ID)) {\n\t\tif (!netif_carrier_ok(adapter->netdev)) {\n\t\t\tadapter->flags &= ~IGB_FLAG_NEED_LINK_UPDATE;\n\t\t} else if (!(adapter->flags & IGB_FLAG_NEED_LINK_UPDATE)) {\n\t\t\tadapter->flags |= IGB_FLAG_NEED_LINK_UPDATE;\n\t\t\tadapter->link_check_timeout = jiffies;\n\t\t}\n\t}\n\n\treturn link_active;\n}\n\nstatic bool igb_thermal_sensor_event(struct e1000_hw *hw, u32 event)\n{\n\tbool ret = false;\n\tu32 ctrl_ext, thstat;\n\n\t \n\tif (hw->mac.type == e1000_i350) {\n\t\tthstat = rd32(E1000_THSTAT);\n\t\tctrl_ext = rd32(E1000_CTRL_EXT);\n\n\t\tif ((hw->phy.media_type == e1000_media_type_copper) &&\n\t\t    !(ctrl_ext & E1000_CTRL_EXT_LINK_MODE_SGMII))\n\t\t\tret = !!(thstat & event);\n\t}\n\n\treturn ret;\n}\n\n \nstatic void igb_check_lvmmc(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 lvmmc;\n\n\tlvmmc = rd32(E1000_LVMMC);\n\tif (lvmmc) {\n\t\tif (unlikely(net_ratelimit())) {\n\t\t\tnetdev_warn(adapter->netdev,\n\t\t\t\t    \"malformed Tx packet detected and dropped, LVMMC:0x%08x\\n\",\n\t\t\t\t    lvmmc);\n\t\t}\n\t}\n}\n\n \nstatic void igb_watchdog(struct timer_list *t)\n{\n\tstruct igb_adapter *adapter = from_timer(adapter, t, watchdog_timer);\n\t \n\tschedule_work(&adapter->watchdog_task);\n}\n\nstatic void igb_watchdog_task(struct work_struct *work)\n{\n\tstruct igb_adapter *adapter = container_of(work,\n\t\t\t\t\t\t   struct igb_adapter,\n\t\t\t\t\t\t   watchdog_task);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct e1000_phy_info *phy = &hw->phy;\n\tstruct net_device *netdev = adapter->netdev;\n\tu32 link;\n\tint i;\n\tu32 connsw;\n\tu16 phy_data, retry_count = 20;\n\n\tlink = igb_has_link(adapter);\n\n\tif (adapter->flags & IGB_FLAG_NEED_LINK_UPDATE) {\n\t\tif (time_after(jiffies, (adapter->link_check_timeout + HZ)))\n\t\t\tadapter->flags &= ~IGB_FLAG_NEED_LINK_UPDATE;\n\t\telse\n\t\t\tlink = false;\n\t}\n\n\t \n\tif (adapter->flags & IGB_FLAG_MAS_ENABLE) {\n\t\tif (hw->phy.media_type == e1000_media_type_copper) {\n\t\t\tconnsw = rd32(E1000_CONNSW);\n\t\t\tif (!(connsw & E1000_CONNSW_AUTOSENSE_EN))\n\t\t\t\tlink = 0;\n\t\t}\n\t}\n\tif (link) {\n\t\t \n\t\tif (hw->dev_spec._82575.media_changed) {\n\t\t\thw->dev_spec._82575.media_changed = false;\n\t\t\tadapter->flags |= IGB_FLAG_MEDIA_RESET;\n\t\t\tigb_reset(adapter);\n\t\t}\n\t\t \n\t\tpm_runtime_resume(netdev->dev.parent);\n\n\t\tif (!netif_carrier_ok(netdev)) {\n\t\t\tu32 ctrl;\n\n\t\t\thw->mac.ops.get_speed_and_duplex(hw,\n\t\t\t\t\t\t\t &adapter->link_speed,\n\t\t\t\t\t\t\t &adapter->link_duplex);\n\n\t\t\tctrl = rd32(E1000_CTRL);\n\t\t\t \n\t\t\tnetdev_info(netdev,\n\t\t\t       \"igb: %s NIC Link is Up %d Mbps %s Duplex, Flow Control: %s\\n\",\n\t\t\t       netdev->name,\n\t\t\t       adapter->link_speed,\n\t\t\t       adapter->link_duplex == FULL_DUPLEX ?\n\t\t\t       \"Full\" : \"Half\",\n\t\t\t       (ctrl & E1000_CTRL_TFCE) &&\n\t\t\t       (ctrl & E1000_CTRL_RFCE) ? \"RX/TX\" :\n\t\t\t       (ctrl & E1000_CTRL_RFCE) ?  \"RX\" :\n\t\t\t       (ctrl & E1000_CTRL_TFCE) ?  \"TX\" : \"None\");\n\n\t\t\t \n\t\t\tif ((adapter->flags & IGB_FLAG_EEE) &&\n\t\t\t\t(adapter->link_duplex == HALF_DUPLEX)) {\n\t\t\t\tdev_info(&adapter->pdev->dev,\n\t\t\t\t\"EEE Disabled: unsupported at half duplex. Re-enable using ethtool when at full duplex.\\n\");\n\t\t\t\tadapter->hw.dev_spec._82575.eee_disable = true;\n\t\t\t\tadapter->flags &= ~IGB_FLAG_EEE;\n\t\t\t}\n\n\t\t\t \n\t\t\tigb_check_downshift(hw);\n\t\t\tif (phy->speed_downgraded)\n\t\t\t\tnetdev_warn(netdev, \"Link Speed was downgraded by SmartSpeed\\n\");\n\n\t\t\t \n\t\t\tif (igb_thermal_sensor_event(hw,\n\t\t\t    E1000_THSTAT_LINK_THROTTLE))\n\t\t\t\tnetdev_info(netdev, \"The network adapter link speed was downshifted because it overheated\\n\");\n\n\t\t\t \n\t\t\tadapter->tx_timeout_factor = 1;\n\t\t\tswitch (adapter->link_speed) {\n\t\t\tcase SPEED_10:\n\t\t\t\tadapter->tx_timeout_factor = 14;\n\t\t\t\tbreak;\n\t\t\tcase SPEED_100:\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (adapter->link_speed != SPEED_1000 ||\n\t\t\t    !hw->phy.ops.read_reg)\n\t\t\t\tgoto no_wait;\n\n\t\t\t \nretry_read_status:\n\t\t\tif (!igb_read_phy_reg(hw, PHY_1000T_STATUS,\n\t\t\t\t\t      &phy_data)) {\n\t\t\t\tif (!(phy_data & SR_1000T_REMOTE_RX_STATUS) &&\n\t\t\t\t    retry_count) {\n\t\t\t\t\tmsleep(100);\n\t\t\t\t\tretry_count--;\n\t\t\t\t\tgoto retry_read_status;\n\t\t\t\t} else if (!retry_count) {\n\t\t\t\t\tdev_err(&adapter->pdev->dev, \"exceed max 2 second\\n\");\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_err(&adapter->pdev->dev, \"read 1000Base-T Status Reg\\n\");\n\t\t\t}\nno_wait:\n\t\t\tnetif_carrier_on(netdev);\n\n\t\t\tigb_ping_all_vfs(adapter);\n\t\t\tigb_check_vf_rate_limit(adapter);\n\n\t\t\t \n\t\t\tif (!test_bit(__IGB_DOWN, &adapter->state))\n\t\t\t\tmod_timer(&adapter->phy_info_timer,\n\t\t\t\t\t  round_jiffies(jiffies + 2 * HZ));\n\t\t}\n\t} else {\n\t\tif (netif_carrier_ok(netdev)) {\n\t\t\tadapter->link_speed = 0;\n\t\t\tadapter->link_duplex = 0;\n\n\t\t\t \n\t\t\tif (igb_thermal_sensor_event(hw,\n\t\t\t    E1000_THSTAT_PWR_DOWN)) {\n\t\t\t\tnetdev_err(netdev, \"The network adapter was stopped because it overheated\\n\");\n\t\t\t}\n\n\t\t\t \n\t\t\tnetdev_info(netdev, \"igb: %s NIC Link is Down\\n\",\n\t\t\t       netdev->name);\n\t\t\tnetif_carrier_off(netdev);\n\n\t\t\tigb_ping_all_vfs(adapter);\n\n\t\t\t \n\t\t\tif (!test_bit(__IGB_DOWN, &adapter->state))\n\t\t\t\tmod_timer(&adapter->phy_info_timer,\n\t\t\t\t\t  round_jiffies(jiffies + 2 * HZ));\n\n\t\t\t \n\t\t\tif (adapter->flags & IGB_FLAG_MAS_ENABLE) {\n\t\t\t\tigb_check_swap_media(adapter);\n\t\t\t\tif (adapter->flags & IGB_FLAG_MEDIA_RESET) {\n\t\t\t\t\tschedule_work(&adapter->reset_task);\n\t\t\t\t\t \n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t\tpm_schedule_suspend(netdev->dev.parent,\n\t\t\t\t\t    MSEC_PER_SEC * 5);\n\n\t\t \n\t\t} else if (!netif_carrier_ok(netdev) &&\n\t\t\t   (adapter->flags & IGB_FLAG_MAS_ENABLE)) {\n\t\t\tigb_check_swap_media(adapter);\n\t\t\tif (adapter->flags & IGB_FLAG_MEDIA_RESET) {\n\t\t\t\tschedule_work(&adapter->reset_task);\n\t\t\t\t \n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_lock(&adapter->stats64_lock);\n\tigb_update_stats(adapter);\n\tspin_unlock(&adapter->stats64_lock);\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igb_ring *tx_ring = adapter->tx_ring[i];\n\t\tif (!netif_carrier_ok(netdev)) {\n\t\t\t \n\t\t\tif (igb_desc_unused(tx_ring) + 1 < tx_ring->count) {\n\t\t\t\tadapter->tx_timeout_count++;\n\t\t\t\tschedule_work(&adapter->reset_task);\n\t\t\t\t \n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tset_bit(IGB_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags);\n\t}\n\n\t \n\tif (adapter->flags & IGB_FLAG_HAS_MSIX) {\n\t\tu32 eics = 0;\n\n\t\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\t\teics |= adapter->q_vector[i]->eims_value;\n\t\twr32(E1000_EICS, eics);\n\t} else {\n\t\twr32(E1000_ICS, E1000_ICS_RXDMT0);\n\t}\n\n\tigb_spoof_check(adapter);\n\tigb_ptp_rx_hang(adapter);\n\tigb_ptp_tx_hang(adapter);\n\n\t \n\tif ((adapter->hw.mac.type == e1000_i350) ||\n\t    (adapter->hw.mac.type == e1000_i354))\n\t\tigb_check_lvmmc(adapter);\n\n\t \n\tif (!test_bit(__IGB_DOWN, &adapter->state)) {\n\t\tif (adapter->flags & IGB_FLAG_NEED_LINK_UPDATE)\n\t\t\tmod_timer(&adapter->watchdog_timer,\n\t\t\t\t  round_jiffies(jiffies +  HZ));\n\t\telse\n\t\t\tmod_timer(&adapter->watchdog_timer,\n\t\t\t\t  round_jiffies(jiffies + 2 * HZ));\n\t}\n}\n\nenum latency_range {\n\tlowest_latency = 0,\n\tlow_latency = 1,\n\tbulk_latency = 2,\n\tlatency_invalid = 255\n};\n\n \nstatic void igb_update_ring_itr(struct igb_q_vector *q_vector)\n{\n\tint new_val = q_vector->itr_val;\n\tint avg_wire_size = 0;\n\tstruct igb_adapter *adapter = q_vector->adapter;\n\tunsigned int packets;\n\n\t \n\tif (adapter->link_speed != SPEED_1000) {\n\t\tnew_val = IGB_4K_ITR;\n\t\tgoto set_itr_val;\n\t}\n\n\tpackets = q_vector->rx.total_packets;\n\tif (packets)\n\t\tavg_wire_size = q_vector->rx.total_bytes / packets;\n\n\tpackets = q_vector->tx.total_packets;\n\tif (packets)\n\t\tavg_wire_size = max_t(u32, avg_wire_size,\n\t\t\t\t      q_vector->tx.total_bytes / packets);\n\n\t \n\tif (!avg_wire_size)\n\t\tgoto clear_counts;\n\n\t \n\tavg_wire_size += 24;\n\n\t \n\tavg_wire_size = min(avg_wire_size, 3000);\n\n\t \n\tif ((avg_wire_size > 300) && (avg_wire_size < 1200))\n\t\tnew_val = avg_wire_size / 3;\n\telse\n\t\tnew_val = avg_wire_size / 2;\n\n\t \n\tif (new_val < IGB_20K_ITR &&\n\t    ((q_vector->rx.ring && adapter->rx_itr_setting == 3) ||\n\t     (!q_vector->rx.ring && adapter->tx_itr_setting == 3)))\n\t\tnew_val = IGB_20K_ITR;\n\nset_itr_val:\n\tif (new_val != q_vector->itr_val) {\n\t\tq_vector->itr_val = new_val;\n\t\tq_vector->set_itr = 1;\n\t}\nclear_counts:\n\tq_vector->rx.total_bytes = 0;\n\tq_vector->rx.total_packets = 0;\n\tq_vector->tx.total_bytes = 0;\n\tq_vector->tx.total_packets = 0;\n}\n\n \nstatic void igb_update_itr(struct igb_q_vector *q_vector,\n\t\t\t   struct igb_ring_container *ring_container)\n{\n\tunsigned int packets = ring_container->total_packets;\n\tunsigned int bytes = ring_container->total_bytes;\n\tu8 itrval = ring_container->itr;\n\n\t \n\tif (packets == 0)\n\t\treturn;\n\n\tswitch (itrval) {\n\tcase lowest_latency:\n\t\t \n\t\tif (bytes/packets > 8000)\n\t\t\titrval = bulk_latency;\n\t\telse if ((packets < 5) && (bytes > 512))\n\t\t\titrval = low_latency;\n\t\tbreak;\n\tcase low_latency:   \n\t\tif (bytes > 10000) {\n\t\t\t \n\t\t\tif (bytes/packets > 8000)\n\t\t\t\titrval = bulk_latency;\n\t\t\telse if ((packets < 10) || ((bytes/packets) > 1200))\n\t\t\t\titrval = bulk_latency;\n\t\t\telse if ((packets > 35))\n\t\t\t\titrval = lowest_latency;\n\t\t} else if (bytes/packets > 2000) {\n\t\t\titrval = bulk_latency;\n\t\t} else if (packets <= 2 && bytes < 512) {\n\t\t\titrval = lowest_latency;\n\t\t}\n\t\tbreak;\n\tcase bulk_latency:  \n\t\tif (bytes > 25000) {\n\t\t\tif (packets > 35)\n\t\t\t\titrval = low_latency;\n\t\t} else if (bytes < 1500) {\n\t\t\titrval = low_latency;\n\t\t}\n\t\tbreak;\n\t}\n\n\t \n\tring_container->total_bytes = 0;\n\tring_container->total_packets = 0;\n\n\t \n\tring_container->itr = itrval;\n}\n\nstatic void igb_set_itr(struct igb_q_vector *q_vector)\n{\n\tstruct igb_adapter *adapter = q_vector->adapter;\n\tu32 new_itr = q_vector->itr_val;\n\tu8 current_itr = 0;\n\n\t \n\tif (adapter->link_speed != SPEED_1000) {\n\t\tcurrent_itr = 0;\n\t\tnew_itr = IGB_4K_ITR;\n\t\tgoto set_itr_now;\n\t}\n\n\tigb_update_itr(q_vector, &q_vector->tx);\n\tigb_update_itr(q_vector, &q_vector->rx);\n\n\tcurrent_itr = max(q_vector->rx.itr, q_vector->tx.itr);\n\n\t \n\tif (current_itr == lowest_latency &&\n\t    ((q_vector->rx.ring && adapter->rx_itr_setting == 3) ||\n\t     (!q_vector->rx.ring && adapter->tx_itr_setting == 3)))\n\t\tcurrent_itr = low_latency;\n\n\tswitch (current_itr) {\n\t \n\tcase lowest_latency:\n\t\tnew_itr = IGB_70K_ITR;  \n\t\tbreak;\n\tcase low_latency:\n\t\tnew_itr = IGB_20K_ITR;  \n\t\tbreak;\n\tcase bulk_latency:\n\t\tnew_itr = IGB_4K_ITR;   \n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\nset_itr_now:\n\tif (new_itr != q_vector->itr_val) {\n\t\t \n\t\tnew_itr = new_itr > q_vector->itr_val ?\n\t\t\t  max((new_itr * q_vector->itr_val) /\n\t\t\t  (new_itr + (q_vector->itr_val >> 2)),\n\t\t\t  new_itr) : new_itr;\n\t\t \n\t\tq_vector->itr_val = new_itr;\n\t\tq_vector->set_itr = 1;\n\t}\n}\n\nstatic void igb_tx_ctxtdesc(struct igb_ring *tx_ring,\n\t\t\t    struct igb_tx_buffer *first,\n\t\t\t    u32 vlan_macip_lens, u32 type_tucmd,\n\t\t\t    u32 mss_l4len_idx)\n{\n\tstruct e1000_adv_tx_context_desc *context_desc;\n\tu16 i = tx_ring->next_to_use;\n\tstruct timespec64 ts;\n\n\tcontext_desc = IGB_TX_CTXTDESC(tx_ring, i);\n\n\ti++;\n\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\t \n\ttype_tucmd |= E1000_TXD_CMD_DEXT | E1000_ADVTXD_DTYP_CTXT;\n\n\t \n\tif (test_bit(IGB_RING_FLAG_TX_CTX_IDX, &tx_ring->flags))\n\t\tmss_l4len_idx |= tx_ring->reg_idx << 4;\n\n\tcontext_desc->vlan_macip_lens\t= cpu_to_le32(vlan_macip_lens);\n\tcontext_desc->type_tucmd_mlhl\t= cpu_to_le32(type_tucmd);\n\tcontext_desc->mss_l4len_idx\t= cpu_to_le32(mss_l4len_idx);\n\n\t \n\tif (tx_ring->launchtime_enable) {\n\t\tts = ktime_to_timespec64(first->skb->tstamp);\n\t\tskb_txtime_consumed(first->skb);\n\t\tcontext_desc->seqnum_seed = cpu_to_le32(ts.tv_nsec / 32);\n\t} else {\n\t\tcontext_desc->seqnum_seed = 0;\n\t}\n}\n\nstatic int igb_tso(struct igb_ring *tx_ring,\n\t\t   struct igb_tx_buffer *first,\n\t\t   u8 *hdr_len)\n{\n\tu32 vlan_macip_lens, type_tucmd, mss_l4len_idx;\n\tstruct sk_buff *skb = first->skb;\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tstruct udphdr *udp;\n\t\tunsigned char *hdr;\n\t} l4;\n\tu32 paylen, l4_offset;\n\tint err;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\tip.hdr = skb_network_header(skb);\n\tl4.hdr = skb_checksum_start(skb);\n\n\t \n\ttype_tucmd = (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) ?\n\t\t      E1000_ADVTXD_TUCMD_L4T_UDP : E1000_ADVTXD_TUCMD_L4T_TCP;\n\n\t \n\tif (ip.v4->version == 4) {\n\t\tunsigned char *csum_start = skb_checksum_start(skb);\n\t\tunsigned char *trans_start = ip.hdr + (ip.v4->ihl * 4);\n\n\t\t \n\t\tip.v4->check = csum_fold(csum_partial(trans_start,\n\t\t\t\t\t\t      csum_start - trans_start,\n\t\t\t\t\t\t      0));\n\t\ttype_tucmd |= E1000_ADVTXD_TUCMD_IPV4;\n\n\t\tip.v4->tot_len = 0;\n\t\tfirst->tx_flags |= IGB_TX_FLAGS_TSO |\n\t\t\t\t   IGB_TX_FLAGS_CSUM |\n\t\t\t\t   IGB_TX_FLAGS_IPV4;\n\t} else {\n\t\tip.v6->payload_len = 0;\n\t\tfirst->tx_flags |= IGB_TX_FLAGS_TSO |\n\t\t\t\t   IGB_TX_FLAGS_CSUM;\n\t}\n\n\t \n\tl4_offset = l4.hdr - skb->data;\n\n\t \n\tpaylen = skb->len - l4_offset;\n\tif (type_tucmd & E1000_ADVTXD_TUCMD_L4T_TCP) {\n\t\t \n\t\t*hdr_len = (l4.tcp->doff * 4) + l4_offset;\n\t\tcsum_replace_by_diff(&l4.tcp->check,\n\t\t\t(__force __wsum)htonl(paylen));\n\t} else {\n\t\t \n\t\t*hdr_len = sizeof(*l4.udp) + l4_offset;\n\t\tcsum_replace_by_diff(&l4.udp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\t}\n\n\t \n\tfirst->gso_segs = skb_shinfo(skb)->gso_segs;\n\tfirst->bytecount += (first->gso_segs - 1) * *hdr_len;\n\n\t \n\tmss_l4len_idx = (*hdr_len - l4_offset) << E1000_ADVTXD_L4LEN_SHIFT;\n\tmss_l4len_idx |= skb_shinfo(skb)->gso_size << E1000_ADVTXD_MSS_SHIFT;\n\n\t \n\tvlan_macip_lens = l4.hdr - ip.hdr;\n\tvlan_macip_lens |= (ip.hdr - skb->data) << E1000_ADVTXD_MACLEN_SHIFT;\n\tvlan_macip_lens |= first->tx_flags & IGB_TX_FLAGS_VLAN_MASK;\n\n\tigb_tx_ctxtdesc(tx_ring, first, vlan_macip_lens,\n\t\t\ttype_tucmd, mss_l4len_idx);\n\n\treturn 1;\n}\n\nstatic void igb_tx_csum(struct igb_ring *tx_ring, struct igb_tx_buffer *first)\n{\n\tstruct sk_buff *skb = first->skb;\n\tu32 vlan_macip_lens = 0;\n\tu32 type_tucmd = 0;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL) {\ncsum_failed:\n\t\tif (!(first->tx_flags & IGB_TX_FLAGS_VLAN) &&\n\t\t    !tx_ring->launchtime_enable)\n\t\t\treturn;\n\t\tgoto no_csum;\n\t}\n\n\tswitch (skb->csum_offset) {\n\tcase offsetof(struct tcphdr, check):\n\t\ttype_tucmd = E1000_ADVTXD_TUCMD_L4T_TCP;\n\t\tfallthrough;\n\tcase offsetof(struct udphdr, check):\n\t\tbreak;\n\tcase offsetof(struct sctphdr, checksum):\n\t\t \n\t\tif (skb_csum_is_sctp(skb)) {\n\t\t\ttype_tucmd = E1000_ADVTXD_TUCMD_L4T_SCTP;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tskb_checksum_help(skb);\n\t\tgoto csum_failed;\n\t}\n\n\t \n\tfirst->tx_flags |= IGB_TX_FLAGS_CSUM;\n\tvlan_macip_lens = skb_checksum_start_offset(skb) -\n\t\t\t  skb_network_offset(skb);\nno_csum:\n\tvlan_macip_lens |= skb_network_offset(skb) << E1000_ADVTXD_MACLEN_SHIFT;\n\tvlan_macip_lens |= first->tx_flags & IGB_TX_FLAGS_VLAN_MASK;\n\n\tigb_tx_ctxtdesc(tx_ring, first, vlan_macip_lens, type_tucmd, 0);\n}\n\n#define IGB_SET_FLAG(_input, _flag, _result) \\\n\t((_flag <= _result) ? \\\n\t ((u32)(_input & _flag) * (_result / _flag)) : \\\n\t ((u32)(_input & _flag) / (_flag / _result)))\n\nstatic u32 igb_tx_cmd_type(struct sk_buff *skb, u32 tx_flags)\n{\n\t \n\tu32 cmd_type = E1000_ADVTXD_DTYP_DATA |\n\t\t       E1000_ADVTXD_DCMD_DEXT |\n\t\t       E1000_ADVTXD_DCMD_IFCS;\n\n\t \n\tcmd_type |= IGB_SET_FLAG(tx_flags, IGB_TX_FLAGS_VLAN,\n\t\t\t\t (E1000_ADVTXD_DCMD_VLE));\n\n\t \n\tcmd_type |= IGB_SET_FLAG(tx_flags, IGB_TX_FLAGS_TSO,\n\t\t\t\t (E1000_ADVTXD_DCMD_TSE));\n\n\t \n\tcmd_type |= IGB_SET_FLAG(tx_flags, IGB_TX_FLAGS_TSTAMP,\n\t\t\t\t (E1000_ADVTXD_MAC_TSTAMP));\n\n\t \n\tcmd_type ^= IGB_SET_FLAG(skb->no_fcs, 1, E1000_ADVTXD_DCMD_IFCS);\n\n\treturn cmd_type;\n}\n\nstatic void igb_tx_olinfo_status(struct igb_ring *tx_ring,\n\t\t\t\t union e1000_adv_tx_desc *tx_desc,\n\t\t\t\t u32 tx_flags, unsigned int paylen)\n{\n\tu32 olinfo_status = paylen << E1000_ADVTXD_PAYLEN_SHIFT;\n\n\t \n\tif (test_bit(IGB_RING_FLAG_TX_CTX_IDX, &tx_ring->flags))\n\t\tolinfo_status |= tx_ring->reg_idx << 4;\n\n\t \n\tolinfo_status |= IGB_SET_FLAG(tx_flags,\n\t\t\t\t      IGB_TX_FLAGS_CSUM,\n\t\t\t\t      (E1000_TXD_POPTS_TXSM << 8));\n\n\t \n\tolinfo_status |= IGB_SET_FLAG(tx_flags,\n\t\t\t\t      IGB_TX_FLAGS_IPV4,\n\t\t\t\t      (E1000_TXD_POPTS_IXSM << 8));\n\n\ttx_desc->read.olinfo_status = cpu_to_le32(olinfo_status);\n}\n\nstatic int __igb_maybe_stop_tx(struct igb_ring *tx_ring, const u16 size)\n{\n\tstruct net_device *netdev = tx_ring->netdev;\n\n\tnetif_stop_subqueue(netdev, tx_ring->queue_index);\n\n\t \n\tsmp_mb();\n\n\t \n\tif (igb_desc_unused(tx_ring) < size)\n\t\treturn -EBUSY;\n\n\t \n\tnetif_wake_subqueue(netdev, tx_ring->queue_index);\n\n\tu64_stats_update_begin(&tx_ring->tx_syncp2);\n\ttx_ring->tx_stats.restart_queue2++;\n\tu64_stats_update_end(&tx_ring->tx_syncp2);\n\n\treturn 0;\n}\n\nstatic inline int igb_maybe_stop_tx(struct igb_ring *tx_ring, const u16 size)\n{\n\tif (igb_desc_unused(tx_ring) >= size)\n\t\treturn 0;\n\treturn __igb_maybe_stop_tx(tx_ring, size);\n}\n\nstatic int igb_tx_map(struct igb_ring *tx_ring,\n\t\t      struct igb_tx_buffer *first,\n\t\t      const u8 hdr_len)\n{\n\tstruct sk_buff *skb = first->skb;\n\tstruct igb_tx_buffer *tx_buffer;\n\tunion e1000_adv_tx_desc *tx_desc;\n\tskb_frag_t *frag;\n\tdma_addr_t dma;\n\tunsigned int data_len, size;\n\tu32 tx_flags = first->tx_flags;\n\tu32 cmd_type = igb_tx_cmd_type(skb, tx_flags);\n\tu16 i = tx_ring->next_to_use;\n\n\ttx_desc = IGB_TX_DESC(tx_ring, i);\n\n\tigb_tx_olinfo_status(tx_ring, tx_desc, tx_flags, skb->len - hdr_len);\n\n\tsize = skb_headlen(skb);\n\tdata_len = skb->data_len;\n\n\tdma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);\n\n\ttx_buffer = first;\n\n\tfor (frag = &skb_shinfo(skb)->frags[0];; frag++) {\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto dma_error;\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, size);\n\t\tdma_unmap_addr_set(tx_buffer, dma, dma);\n\n\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\n\t\twhile (unlikely(size > IGB_MAX_DATA_PER_TXD)) {\n\t\t\ttx_desc->read.cmd_type_len =\n\t\t\t\tcpu_to_le32(cmd_type ^ IGB_MAX_DATA_PER_TXD);\n\n\t\t\ti++;\n\t\t\ttx_desc++;\n\t\t\tif (i == tx_ring->count) {\n\t\t\t\ttx_desc = IGB_TX_DESC(tx_ring, 0);\n\t\t\t\ti = 0;\n\t\t\t}\n\t\t\ttx_desc->read.olinfo_status = 0;\n\n\t\t\tdma += IGB_MAX_DATA_PER_TXD;\n\t\t\tsize -= IGB_MAX_DATA_PER_TXD;\n\n\t\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\t\t}\n\n\t\tif (likely(!data_len))\n\t\t\tbreak;\n\n\t\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type ^ size);\n\n\t\ti++;\n\t\ttx_desc++;\n\t\tif (i == tx_ring->count) {\n\t\t\ttx_desc = IGB_TX_DESC(tx_ring, 0);\n\t\t\ti = 0;\n\t\t}\n\t\ttx_desc->read.olinfo_status = 0;\n\n\t\tsize = skb_frag_size(frag);\n\t\tdata_len -= size;\n\n\t\tdma = skb_frag_dma_map(tx_ring->dev, frag, 0,\n\t\t\t\t       size, DMA_TO_DEVICE);\n\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t}\n\n\t \n\tcmd_type |= size | IGB_TXD_DCMD;\n\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\n\tnetdev_tx_sent_queue(txring_txq(tx_ring), first->bytecount);\n\n\t \n\tfirst->time_stamp = jiffies;\n\n\tskb_tx_timestamp(skb);\n\n\t \n\tdma_wmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\ti++;\n\tif (i == tx_ring->count)\n\t\ti = 0;\n\n\ttx_ring->next_to_use = i;\n\n\t \n\tigb_maybe_stop_tx(tx_ring, DESC_NEEDED);\n\n\tif (netif_xmit_stopped(txring_txq(tx_ring)) || !netdev_xmit_more()) {\n\t\twritel(i, tx_ring->tail);\n\t}\n\treturn 0;\n\ndma_error:\n\tdev_err(tx_ring->dev, \"TX DMA map failed\\n\");\n\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\n\t \n\twhile (tx_buffer != first) {\n\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\n\t\tif (i-- == 0)\n\t\t\ti += tx_ring->count;\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t}\n\n\tif (dma_unmap_len(tx_buffer, len))\n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\tdma_unmap_len_set(tx_buffer, len, 0);\n\n\tdev_kfree_skb_any(tx_buffer->skb);\n\ttx_buffer->skb = NULL;\n\n\ttx_ring->next_to_use = i;\n\n\treturn -1;\n}\n\nint igb_xmit_xdp_ring(struct igb_adapter *adapter,\n\t\t      struct igb_ring *tx_ring,\n\t\t      struct xdp_frame *xdpf)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_frame(xdpf);\n\tu8 nr_frags = unlikely(xdp_frame_has_frags(xdpf)) ? sinfo->nr_frags : 0;\n\tu16 count, i, index = tx_ring->next_to_use;\n\tstruct igb_tx_buffer *tx_head = &tx_ring->tx_buffer_info[index];\n\tstruct igb_tx_buffer *tx_buffer = tx_head;\n\tunion e1000_adv_tx_desc *tx_desc = IGB_TX_DESC(tx_ring, index);\n\tu32 len = xdpf->len, cmd_type, olinfo_status;\n\tvoid *data = xdpf->data;\n\n\tcount = TXD_USE_COUNT(len);\n\tfor (i = 0; i < nr_frags; i++)\n\t\tcount += TXD_USE_COUNT(skb_frag_size(&sinfo->frags[i]));\n\n\tif (igb_maybe_stop_tx(tx_ring, count + 3))\n\t\treturn IGB_XDP_CONSUMED;\n\n\ti = 0;\n\t \n\ttx_head->bytecount = xdp_get_frame_len(xdpf);\n\ttx_head->type = IGB_TYPE_XDP;\n\ttx_head->gso_segs = 1;\n\ttx_head->xdpf = xdpf;\n\n\tolinfo_status = tx_head->bytecount << E1000_ADVTXD_PAYLEN_SHIFT;\n\t \n\tif (test_bit(IGB_RING_FLAG_TX_CTX_IDX, &tx_ring->flags))\n\t\tolinfo_status |= tx_ring->reg_idx << 4;\n\ttx_desc->read.olinfo_status = cpu_to_le32(olinfo_status);\n\n\tfor (;;) {\n\t\tdma_addr_t dma;\n\n\t\tdma = dma_map_single(tx_ring->dev, data, len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto unmap;\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, len);\n\t\tdma_unmap_addr_set(tx_buffer, dma, dma);\n\n\t\t \n\t\tcmd_type = E1000_ADVTXD_DTYP_DATA | E1000_ADVTXD_DCMD_DEXT |\n\t\t\t   E1000_ADVTXD_DCMD_IFCS | len;\n\n\t\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\n\t\ttx_buffer->protocol = 0;\n\n\t\tif (++index == tx_ring->count)\n\t\t\tindex = 0;\n\n\t\tif (i == nr_frags)\n\t\t\tbreak;\n\n\t\ttx_buffer = &tx_ring->tx_buffer_info[index];\n\t\ttx_desc = IGB_TX_DESC(tx_ring, index);\n\t\ttx_desc->read.olinfo_status = 0;\n\n\t\tdata = skb_frag_address(&sinfo->frags[i]);\n\t\tlen = skb_frag_size(&sinfo->frags[i]);\n\t\ti++;\n\t}\n\ttx_desc->read.cmd_type_len |= cpu_to_le32(IGB_TXD_DCMD);\n\n\tnetdev_tx_sent_queue(txring_txq(tx_ring), tx_head->bytecount);\n\t \n\ttx_head->time_stamp = jiffies;\n\n\t \n\tsmp_wmb();\n\n\t \n\ttx_head->next_to_watch = tx_desc;\n\ttx_ring->next_to_use = index;\n\n\t \n\tigb_maybe_stop_tx(tx_ring, DESC_NEEDED);\n\n\tif (netif_xmit_stopped(txring_txq(tx_ring)) || !netdev_xmit_more())\n\t\twritel(index, tx_ring->tail);\n\n\treturn IGB_XDP_TX;\n\nunmap:\n\tfor (;;) {\n\t\ttx_buffer = &tx_ring->tx_buffer_info[index];\n\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\t\tif (tx_buffer == tx_head)\n\t\t\tbreak;\n\n\t\tif (!index)\n\t\t\tindex += tx_ring->count;\n\t\tindex--;\n\t}\n\n\treturn IGB_XDP_CONSUMED;\n}\n\nnetdev_tx_t igb_xmit_frame_ring(struct sk_buff *skb,\n\t\t\t\tstruct igb_ring *tx_ring)\n{\n\tstruct igb_tx_buffer *first;\n\tint tso;\n\tu32 tx_flags = 0;\n\tunsigned short f;\n\tu16 count = TXD_USE_COUNT(skb_headlen(skb));\n\t__be16 protocol = vlan_get_protocol(skb);\n\tu8 hdr_len = 0;\n\n\t \n\tfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++)\n\t\tcount += TXD_USE_COUNT(skb_frag_size(\n\t\t\t\t\t\t&skb_shinfo(skb)->frags[f]));\n\n\tif (igb_maybe_stop_tx(tx_ring, count + 3)) {\n\t\t \n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\tfirst = &tx_ring->tx_buffer_info[tx_ring->next_to_use];\n\tfirst->type = IGB_TYPE_SKB;\n\tfirst->skb = skb;\n\tfirst->bytecount = skb->len;\n\tfirst->gso_segs = 1;\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {\n\t\tstruct igb_adapter *adapter = netdev_priv(tx_ring->netdev);\n\n\t\tif (adapter->tstamp_config.tx_type == HWTSTAMP_TX_ON &&\n\t\t    !test_and_set_bit_lock(__IGB_PTP_TX_IN_PROGRESS,\n\t\t\t\t\t   &adapter->state)) {\n\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t\ttx_flags |= IGB_TX_FLAGS_TSTAMP;\n\n\t\t\tadapter->ptp_tx_skb = skb_get(skb);\n\t\t\tadapter->ptp_tx_start = jiffies;\n\t\t\tif (adapter->hw.mac.type == e1000_82576)\n\t\t\t\tschedule_work(&adapter->ptp_tx_work);\n\t\t} else {\n\t\t\tadapter->tx_hwtstamp_skipped++;\n\t\t}\n\t}\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\ttx_flags |= IGB_TX_FLAGS_VLAN;\n\t\ttx_flags |= (skb_vlan_tag_get(skb) << IGB_TX_FLAGS_VLAN_SHIFT);\n\t}\n\n\t \n\tfirst->tx_flags = tx_flags;\n\tfirst->protocol = protocol;\n\n\ttso = igb_tso(tx_ring, first, &hdr_len);\n\tif (tso < 0)\n\t\tgoto out_drop;\n\telse if (!tso)\n\t\tigb_tx_csum(tx_ring, first);\n\n\tif (igb_tx_map(tx_ring, first, hdr_len))\n\t\tgoto cleanup_tx_tstamp;\n\n\treturn NETDEV_TX_OK;\n\nout_drop:\n\tdev_kfree_skb_any(first->skb);\n\tfirst->skb = NULL;\ncleanup_tx_tstamp:\n\tif (unlikely(tx_flags & IGB_TX_FLAGS_TSTAMP)) {\n\t\tstruct igb_adapter *adapter = netdev_priv(tx_ring->netdev);\n\n\t\tdev_kfree_skb_any(adapter->ptp_tx_skb);\n\t\tadapter->ptp_tx_skb = NULL;\n\t\tif (adapter->hw.mac.type == e1000_82576)\n\t\t\tcancel_work_sync(&adapter->ptp_tx_work);\n\t\tclear_bit_unlock(__IGB_PTP_TX_IN_PROGRESS, &adapter->state);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic inline struct igb_ring *igb_tx_queue_mapping(struct igb_adapter *adapter,\n\t\t\t\t\t\t    struct sk_buff *skb)\n{\n\tunsigned int r_idx = skb->queue_mapping;\n\n\tif (r_idx >= adapter->num_tx_queues)\n\t\tr_idx = r_idx % adapter->num_tx_queues;\n\n\treturn adapter->tx_ring[r_idx];\n}\n\nstatic netdev_tx_t igb_xmit_frame(struct sk_buff *skb,\n\t\t\t\t  struct net_device *netdev)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\t \n\tif (skb_put_padto(skb, 17))\n\t\treturn NETDEV_TX_OK;\n\n\treturn igb_xmit_frame_ring(skb, igb_tx_queue_mapping(adapter, skb));\n}\n\n \nstatic void igb_tx_timeout(struct net_device *netdev, unsigned int __always_unused txqueue)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\t \n\tadapter->tx_timeout_count++;\n\n\tif (hw->mac.type >= e1000_82580)\n\t\thw->dev_spec._82575.global_device_reset = true;\n\n\tschedule_work(&adapter->reset_task);\n\twr32(E1000_EICS,\n\t     (adapter->eims_enable_mask & ~adapter->eims_other));\n}\n\nstatic void igb_reset_task(struct work_struct *work)\n{\n\tstruct igb_adapter *adapter;\n\tadapter = container_of(work, struct igb_adapter, reset_task);\n\n\trtnl_lock();\n\t \n\tif (test_bit(__IGB_DOWN, &adapter->state) ||\n\t    test_bit(__IGB_RESETTING, &adapter->state)) {\n\t\trtnl_unlock();\n\t\treturn;\n\t}\n\n\tigb_dump(adapter);\n\tnetdev_err(adapter->netdev, \"Reset adapter\\n\");\n\tigb_reinit_locked(adapter);\n\trtnl_unlock();\n}\n\n \nstatic void igb_get_stats64(struct net_device *netdev,\n\t\t\t    struct rtnl_link_stats64 *stats)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\tspin_lock(&adapter->stats64_lock);\n\tigb_update_stats(adapter);\n\tmemcpy(stats, &adapter->stats64, sizeof(*stats));\n\tspin_unlock(&adapter->stats64_lock);\n}\n\n \nstatic int igb_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tint max_frame = new_mtu + IGB_ETH_PKT_HDR_PAD;\n\n\tif (adapter->xdp_prog) {\n\t\tint i;\n\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tstruct igb_ring *ring = adapter->rx_ring[i];\n\n\t\t\tif (max_frame > igb_rx_bufsz(ring)) {\n\t\t\t\tnetdev_warn(adapter->netdev,\n\t\t\t\t\t    \"Requested MTU size is not supported with XDP. Max frame size is %d\\n\",\n\t\t\t\t\t    max_frame);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (max_frame < (ETH_FRAME_LEN + ETH_FCS_LEN))\n\t\tmax_frame = ETH_FRAME_LEN + ETH_FCS_LEN;\n\n\twhile (test_and_set_bit(__IGB_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\n\t \n\tadapter->max_frame_size = max_frame;\n\n\tif (netif_running(netdev))\n\t\tigb_down(adapter);\n\n\tnetdev_dbg(netdev, \"changing MTU from %d to %d\\n\",\n\t\t   netdev->mtu, new_mtu);\n\tnetdev->mtu = new_mtu;\n\n\tif (netif_running(netdev))\n\t\tigb_up(adapter);\n\telse\n\t\tigb_reset(adapter);\n\n\tclear_bit(__IGB_RESETTING, &adapter->state);\n\n\treturn 0;\n}\n\n \nvoid igb_update_stats(struct igb_adapter *adapter)\n{\n\tstruct rtnl_link_stats64 *net_stats = &adapter->stats64;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tu32 reg, mpc;\n\tint i;\n\tu64 bytes, packets;\n\tunsigned int start;\n\tu64 _bytes, _packets;\n\n\t \n\tif (adapter->link_speed == 0)\n\t\treturn;\n\tif (pci_channel_offline(pdev))\n\t\treturn;\n\n\tbytes = 0;\n\tpackets = 0;\n\n\trcu_read_lock();\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct igb_ring *ring = adapter->rx_ring[i];\n\t\tu32 rqdpc = rd32(E1000_RQDPC(i));\n\t\tif (hw->mac.type >= e1000_i210)\n\t\t\twr32(E1000_RQDPC(i), 0);\n\n\t\tif (rqdpc) {\n\t\t\tring->rx_stats.drops += rqdpc;\n\t\t\tnet_stats->rx_fifo_errors += rqdpc;\n\t\t}\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&ring->rx_syncp);\n\t\t\t_bytes = ring->rx_stats.bytes;\n\t\t\t_packets = ring->rx_stats.packets;\n\t\t} while (u64_stats_fetch_retry(&ring->rx_syncp, start));\n\t\tbytes += _bytes;\n\t\tpackets += _packets;\n\t}\n\n\tnet_stats->rx_bytes = bytes;\n\tnet_stats->rx_packets = packets;\n\n\tbytes = 0;\n\tpackets = 0;\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igb_ring *ring = adapter->tx_ring[i];\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&ring->tx_syncp);\n\t\t\t_bytes = ring->tx_stats.bytes;\n\t\t\t_packets = ring->tx_stats.packets;\n\t\t} while (u64_stats_fetch_retry(&ring->tx_syncp, start));\n\t\tbytes += _bytes;\n\t\tpackets += _packets;\n\t}\n\tnet_stats->tx_bytes = bytes;\n\tnet_stats->tx_packets = packets;\n\trcu_read_unlock();\n\n\t \n\tadapter->stats.crcerrs += rd32(E1000_CRCERRS);\n\tadapter->stats.gprc += rd32(E1000_GPRC);\n\tadapter->stats.gorc += rd32(E1000_GORCL);\n\trd32(E1000_GORCH);  \n\tadapter->stats.bprc += rd32(E1000_BPRC);\n\tadapter->stats.mprc += rd32(E1000_MPRC);\n\tadapter->stats.roc += rd32(E1000_ROC);\n\n\tadapter->stats.prc64 += rd32(E1000_PRC64);\n\tadapter->stats.prc127 += rd32(E1000_PRC127);\n\tadapter->stats.prc255 += rd32(E1000_PRC255);\n\tadapter->stats.prc511 += rd32(E1000_PRC511);\n\tadapter->stats.prc1023 += rd32(E1000_PRC1023);\n\tadapter->stats.prc1522 += rd32(E1000_PRC1522);\n\tadapter->stats.symerrs += rd32(E1000_SYMERRS);\n\tadapter->stats.sec += rd32(E1000_SEC);\n\n\tmpc = rd32(E1000_MPC);\n\tadapter->stats.mpc += mpc;\n\tnet_stats->rx_fifo_errors += mpc;\n\tadapter->stats.scc += rd32(E1000_SCC);\n\tadapter->stats.ecol += rd32(E1000_ECOL);\n\tadapter->stats.mcc += rd32(E1000_MCC);\n\tadapter->stats.latecol += rd32(E1000_LATECOL);\n\tadapter->stats.dc += rd32(E1000_DC);\n\tadapter->stats.rlec += rd32(E1000_RLEC);\n\tadapter->stats.xonrxc += rd32(E1000_XONRXC);\n\tadapter->stats.xontxc += rd32(E1000_XONTXC);\n\tadapter->stats.xoffrxc += rd32(E1000_XOFFRXC);\n\tadapter->stats.xofftxc += rd32(E1000_XOFFTXC);\n\tadapter->stats.fcruc += rd32(E1000_FCRUC);\n\tadapter->stats.gptc += rd32(E1000_GPTC);\n\tadapter->stats.gotc += rd32(E1000_GOTCL);\n\trd32(E1000_GOTCH);  \n\tadapter->stats.rnbc += rd32(E1000_RNBC);\n\tadapter->stats.ruc += rd32(E1000_RUC);\n\tadapter->stats.rfc += rd32(E1000_RFC);\n\tadapter->stats.rjc += rd32(E1000_RJC);\n\tadapter->stats.tor += rd32(E1000_TORH);\n\tadapter->stats.tot += rd32(E1000_TOTH);\n\tadapter->stats.tpr += rd32(E1000_TPR);\n\n\tadapter->stats.ptc64 += rd32(E1000_PTC64);\n\tadapter->stats.ptc127 += rd32(E1000_PTC127);\n\tadapter->stats.ptc255 += rd32(E1000_PTC255);\n\tadapter->stats.ptc511 += rd32(E1000_PTC511);\n\tadapter->stats.ptc1023 += rd32(E1000_PTC1023);\n\tadapter->stats.ptc1522 += rd32(E1000_PTC1522);\n\n\tadapter->stats.mptc += rd32(E1000_MPTC);\n\tadapter->stats.bptc += rd32(E1000_BPTC);\n\n\tadapter->stats.tpt += rd32(E1000_TPT);\n\tadapter->stats.colc += rd32(E1000_COLC);\n\n\tadapter->stats.algnerrc += rd32(E1000_ALGNERRC);\n\t \n\treg = rd32(E1000_CTRL_EXT);\n\tif (!(reg & E1000_CTRL_EXT_LINK_MODE_MASK)) {\n\t\tadapter->stats.rxerrc += rd32(E1000_RXERRC);\n\n\t\t \n\t\tif ((hw->mac.type != e1000_i210) &&\n\t\t    (hw->mac.type != e1000_i211))\n\t\t\tadapter->stats.tncrs += rd32(E1000_TNCRS);\n\t}\n\n\tadapter->stats.tsctc += rd32(E1000_TSCTC);\n\tadapter->stats.tsctfc += rd32(E1000_TSCTFC);\n\n\tadapter->stats.iac += rd32(E1000_IAC);\n\tadapter->stats.icrxoc += rd32(E1000_ICRXOC);\n\tadapter->stats.icrxptc += rd32(E1000_ICRXPTC);\n\tadapter->stats.icrxatc += rd32(E1000_ICRXATC);\n\tadapter->stats.ictxptc += rd32(E1000_ICTXPTC);\n\tadapter->stats.ictxatc += rd32(E1000_ICTXATC);\n\tadapter->stats.ictxqec += rd32(E1000_ICTXQEC);\n\tadapter->stats.ictxqmtc += rd32(E1000_ICTXQMTC);\n\tadapter->stats.icrxdmtc += rd32(E1000_ICRXDMTC);\n\n\t \n\tnet_stats->multicast = adapter->stats.mprc;\n\tnet_stats->collisions = adapter->stats.colc;\n\n\t \n\n\t \n\tnet_stats->rx_errors = adapter->stats.rxerrc +\n\t\tadapter->stats.crcerrs + adapter->stats.algnerrc +\n\t\tadapter->stats.ruc + adapter->stats.roc +\n\t\tadapter->stats.cexterr;\n\tnet_stats->rx_length_errors = adapter->stats.ruc +\n\t\t\t\t      adapter->stats.roc;\n\tnet_stats->rx_crc_errors = adapter->stats.crcerrs;\n\tnet_stats->rx_frame_errors = adapter->stats.algnerrc;\n\tnet_stats->rx_missed_errors = adapter->stats.mpc;\n\n\t \n\tnet_stats->tx_errors = adapter->stats.ecol +\n\t\t\t       adapter->stats.latecol;\n\tnet_stats->tx_aborted_errors = adapter->stats.ecol;\n\tnet_stats->tx_window_errors = adapter->stats.latecol;\n\tnet_stats->tx_carrier_errors = adapter->stats.tncrs;\n\n\t \n\n\t \n\tadapter->stats.mgptc += rd32(E1000_MGTPTC);\n\tadapter->stats.mgprc += rd32(E1000_MGTPRC);\n\tadapter->stats.mgpdc += rd32(E1000_MGTPDC);\n\n\t \n\treg = rd32(E1000_MANC);\n\tif (reg & E1000_MANC_EN_BMC2OS) {\n\t\tadapter->stats.o2bgptc += rd32(E1000_O2BGPTC);\n\t\tadapter->stats.o2bspc += rd32(E1000_O2BSPC);\n\t\tadapter->stats.b2ospc += rd32(E1000_B2OSPC);\n\t\tadapter->stats.b2ogprc += rd32(E1000_B2OGPRC);\n\t}\n}\n\nstatic void igb_perout(struct igb_adapter *adapter, int tsintr_tt)\n{\n\tint pin = ptp_find_pin(adapter->ptp_clock, PTP_PF_PEROUT, tsintr_tt);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct timespec64 ts;\n\tu32 tsauxc;\n\n\tif (pin < 0 || pin >= IGB_N_SDP)\n\t\treturn;\n\n\tspin_lock(&adapter->tmreg_lock);\n\n\tif (hw->mac.type == e1000_82580 ||\n\t    hw->mac.type == e1000_i354 ||\n\t    hw->mac.type == e1000_i350) {\n\t\ts64 ns = timespec64_to_ns(&adapter->perout[tsintr_tt].period);\n\t\tu32 systiml, systimh, level_mask, level, rem;\n\t\tu64 systim, now;\n\n\t\t \n\t\trd32(E1000_SYSTIMR);\n\t\tsystiml = rd32(E1000_SYSTIML);\n\t\tsystimh = rd32(E1000_SYSTIMH);\n\t\tsystim = (((u64)(systimh & 0xFF)) << 32) | ((u64)systiml);\n\t\tnow = timecounter_cyc2time(&adapter->tc, systim);\n\n\t\tif (pin < 2) {\n\t\t\tlevel_mask = (tsintr_tt == 1) ? 0x80000 : 0x40000;\n\t\t\tlevel = (rd32(E1000_CTRL) & level_mask) ? 1 : 0;\n\t\t} else {\n\t\t\tlevel_mask = (tsintr_tt == 1) ? 0x80 : 0x40;\n\t\t\tlevel = (rd32(E1000_CTRL_EXT) & level_mask) ? 1 : 0;\n\t\t}\n\n\t\tdiv_u64_rem(now, ns, &rem);\n\t\tsystim = systim + (ns - rem);\n\n\t\t \n\t\tdiv_u64_rem(now, ns << 1, &rem);\n\t\tif (rem < ns) {\n\t\t\t \n\t\t\tif (level == 0) {\n\t\t\t\t \n\t\t\t\tsystim += ns;\n\t\t\t\tpr_notice(\"igb: periodic output on %s missed falling edge\\n\",\n\t\t\t\t\t  adapter->sdp_config[pin].name);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (level == 1) {\n\t\t\t\t \n\t\t\t\tsystim += ns;\n\t\t\t\tpr_notice(\"igb: periodic output on %s missed rising edge\\n\",\n\t\t\t\t\t  adapter->sdp_config[pin].name);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tts.tv_nsec = (u32)systim;\n\t\tts.tv_sec  = ((u32)(systim >> 32)) & 0xFF;\n\t} else {\n\t\tts = timespec64_add(adapter->perout[tsintr_tt].start,\n\t\t\t\t    adapter->perout[tsintr_tt].period);\n\t}\n\n\t \n\twr32((tsintr_tt == 1) ? E1000_TRGTTIML1 : E1000_TRGTTIML0, ts.tv_nsec);\n\twr32((tsintr_tt == 1) ? E1000_TRGTTIMH1 : E1000_TRGTTIMH0, (u32)ts.tv_sec);\n\ttsauxc = rd32(E1000_TSAUXC);\n\ttsauxc |= TSAUXC_EN_TT0;\n\twr32(E1000_TSAUXC, tsauxc);\n\tadapter->perout[tsintr_tt].start = ts;\n\n\tspin_unlock(&adapter->tmreg_lock);\n}\n\nstatic void igb_extts(struct igb_adapter *adapter, int tsintr_tt)\n{\n\tint pin = ptp_find_pin(adapter->ptp_clock, PTP_PF_EXTTS, tsintr_tt);\n\tint auxstmpl = (tsintr_tt == 1) ? E1000_AUXSTMPL1 : E1000_AUXSTMPL0;\n\tint auxstmph = (tsintr_tt == 1) ? E1000_AUXSTMPH1 : E1000_AUXSTMPH0;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct ptp_clock_event event;\n\tstruct timespec64 ts;\n\tunsigned long flags;\n\n\tif (pin < 0 || pin >= IGB_N_SDP)\n\t\treturn;\n\n\tif (hw->mac.type == e1000_82580 ||\n\t    hw->mac.type == e1000_i354 ||\n\t    hw->mac.type == e1000_i350) {\n\t\tu64 ns = rd32(auxstmpl);\n\n\t\tns += ((u64)(rd32(auxstmph) & 0xFF)) << 32;\n\t\tspin_lock_irqsave(&adapter->tmreg_lock, flags);\n\t\tns = timecounter_cyc2time(&adapter->tc, ns);\n\t\tspin_unlock_irqrestore(&adapter->tmreg_lock, flags);\n\t\tts = ns_to_timespec64(ns);\n\t} else {\n\t\tts.tv_nsec = rd32(auxstmpl);\n\t\tts.tv_sec  = rd32(auxstmph);\n\t}\n\n\tevent.type = PTP_CLOCK_EXTTS;\n\tevent.index = tsintr_tt;\n\tevent.timestamp = ts.tv_sec * 1000000000ULL + ts.tv_nsec;\n\tptp_clock_event(adapter->ptp_clock, &event);\n}\n\nstatic void igb_tsync_interrupt(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 ack = 0, tsicr = rd32(E1000_TSICR);\n\tstruct ptp_clock_event event;\n\n\tif (tsicr & TSINTR_SYS_WRAP) {\n\t\tevent.type = PTP_CLOCK_PPS;\n\t\tif (adapter->ptp_caps.pps)\n\t\t\tptp_clock_event(adapter->ptp_clock, &event);\n\t\tack |= TSINTR_SYS_WRAP;\n\t}\n\n\tif (tsicr & E1000_TSICR_TXTS) {\n\t\t \n\t\tschedule_work(&adapter->ptp_tx_work);\n\t\tack |= E1000_TSICR_TXTS;\n\t}\n\n\tif (tsicr & TSINTR_TT0) {\n\t\tigb_perout(adapter, 0);\n\t\tack |= TSINTR_TT0;\n\t}\n\n\tif (tsicr & TSINTR_TT1) {\n\t\tigb_perout(adapter, 1);\n\t\tack |= TSINTR_TT1;\n\t}\n\n\tif (tsicr & TSINTR_AUTT0) {\n\t\tigb_extts(adapter, 0);\n\t\tack |= TSINTR_AUTT0;\n\t}\n\n\tif (tsicr & TSINTR_AUTT1) {\n\t\tigb_extts(adapter, 1);\n\t\tack |= TSINTR_AUTT1;\n\t}\n\n\t \n\twr32(E1000_TSICR, ack);\n}\n\nstatic irqreturn_t igb_msix_other(int irq, void *data)\n{\n\tstruct igb_adapter *adapter = data;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 icr = rd32(E1000_ICR);\n\t \n\n\tif (icr & E1000_ICR_DRSTA)\n\t\tschedule_work(&adapter->reset_task);\n\n\tif (icr & E1000_ICR_DOUTSYNC) {\n\t\t \n\t\tadapter->stats.doosync++;\n\t\t \n\t\tigb_check_wvbr(adapter);\n\t}\n\n\t \n\tif (icr & E1000_ICR_VMMB)\n\t\tigb_msg_task(adapter);\n\n\tif (icr & E1000_ICR_LSC) {\n\t\thw->mac.get_link_status = 1;\n\t\t \n\t\tif (!test_bit(__IGB_DOWN, &adapter->state))\n\t\t\tmod_timer(&adapter->watchdog_timer, jiffies + 1);\n\t}\n\n\tif (icr & E1000_ICR_TS)\n\t\tigb_tsync_interrupt(adapter);\n\n\twr32(E1000_EIMS, adapter->eims_other);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void igb_write_itr(struct igb_q_vector *q_vector)\n{\n\tstruct igb_adapter *adapter = q_vector->adapter;\n\tu32 itr_val = q_vector->itr_val & 0x7FFC;\n\n\tif (!q_vector->set_itr)\n\t\treturn;\n\n\tif (!itr_val)\n\t\titr_val = 0x4;\n\n\tif (adapter->hw.mac.type == e1000_82575)\n\t\titr_val |= itr_val << 16;\n\telse\n\t\titr_val |= E1000_EITR_CNT_IGNR;\n\n\twritel(itr_val, q_vector->itr_register);\n\tq_vector->set_itr = 0;\n}\n\nstatic irqreturn_t igb_msix_ring(int irq, void *data)\n{\n\tstruct igb_q_vector *q_vector = data;\n\n\t \n\tigb_write_itr(q_vector);\n\n\tnapi_schedule(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_IGB_DCA\nstatic void igb_update_tx_dca(struct igb_adapter *adapter,\n\t\t\t      struct igb_ring *tx_ring,\n\t\t\t      int cpu)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 txctrl = dca3_get_tag(tx_ring->dev, cpu);\n\n\tif (hw->mac.type != e1000_82575)\n\t\ttxctrl <<= E1000_DCA_TXCTRL_CPUID_SHIFT;\n\n\t \n\ttxctrl |= E1000_DCA_TXCTRL_DESC_RRO_EN |\n\t\t  E1000_DCA_TXCTRL_DATA_RRO_EN |\n\t\t  E1000_DCA_TXCTRL_DESC_DCA_EN;\n\n\twr32(E1000_DCA_TXCTRL(tx_ring->reg_idx), txctrl);\n}\n\nstatic void igb_update_rx_dca(struct igb_adapter *adapter,\n\t\t\t      struct igb_ring *rx_ring,\n\t\t\t      int cpu)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 rxctrl = dca3_get_tag(&adapter->pdev->dev, cpu);\n\n\tif (hw->mac.type != e1000_82575)\n\t\trxctrl <<= E1000_DCA_RXCTRL_CPUID_SHIFT;\n\n\t \n\trxctrl |= E1000_DCA_RXCTRL_DESC_RRO_EN |\n\t\t  E1000_DCA_RXCTRL_DESC_DCA_EN;\n\n\twr32(E1000_DCA_RXCTRL(rx_ring->reg_idx), rxctrl);\n}\n\nstatic void igb_update_dca(struct igb_q_vector *q_vector)\n{\n\tstruct igb_adapter *adapter = q_vector->adapter;\n\tint cpu = get_cpu();\n\n\tif (q_vector->cpu == cpu)\n\t\tgoto out_no_update;\n\n\tif (q_vector->tx.ring)\n\t\tigb_update_tx_dca(adapter, q_vector->tx.ring, cpu);\n\n\tif (q_vector->rx.ring)\n\t\tigb_update_rx_dca(adapter, q_vector->rx.ring, cpu);\n\n\tq_vector->cpu = cpu;\nout_no_update:\n\tput_cpu();\n}\n\nstatic void igb_setup_dca(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint i;\n\n\tif (!(adapter->flags & IGB_FLAG_DCA_ENABLED))\n\t\treturn;\n\n\t \n\twr32(E1000_DCA_CTRL, E1000_DCA_CTRL_DCA_MODE_CB2);\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++) {\n\t\tadapter->q_vector[i]->cpu = -1;\n\t\tigb_update_dca(adapter->q_vector[i]);\n\t}\n}\n\nstatic int __igb_notify_dca(struct device *dev, void *data)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tunsigned long event = *(unsigned long *)data;\n\n\tswitch (event) {\n\tcase DCA_PROVIDER_ADD:\n\t\t \n\t\tif (adapter->flags & IGB_FLAG_DCA_ENABLED)\n\t\t\tbreak;\n\t\tif (dca_add_requester(dev) == 0) {\n\t\t\tadapter->flags |= IGB_FLAG_DCA_ENABLED;\n\t\t\tdev_info(&pdev->dev, \"DCA enabled\\n\");\n\t\t\tigb_setup_dca(adapter);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;  \n\tcase DCA_PROVIDER_REMOVE:\n\t\tif (adapter->flags & IGB_FLAG_DCA_ENABLED) {\n\t\t\t \n\t\t\tdca_remove_requester(dev);\n\t\t\tdev_info(&pdev->dev, \"DCA disabled\\n\");\n\t\t\tadapter->flags &= ~IGB_FLAG_DCA_ENABLED;\n\t\t\twr32(E1000_DCA_CTRL, E1000_DCA_CTRL_DCA_MODE_DISABLE);\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int igb_notify_dca(struct notifier_block *nb, unsigned long event,\n\t\t\t  void *p)\n{\n\tint ret_val;\n\n\tret_val = driver_for_each_device(&igb_driver.driver, NULL, &event,\n\t\t\t\t\t __igb_notify_dca);\n\n\treturn ret_val ? NOTIFY_BAD : NOTIFY_DONE;\n}\n#endif  \n\n#ifdef CONFIG_PCI_IOV\nstatic int igb_vf_configure(struct igb_adapter *adapter, int vf)\n{\n\tunsigned char mac_addr[ETH_ALEN];\n\n\teth_zero_addr(mac_addr);\n\tigb_set_vf_mac(adapter, vf, mac_addr);\n\n\t \n\tadapter->vf_data[vf].spoofchk_enabled = true;\n\n\t \n\tadapter->vf_data[vf].trusted = false;\n\n\treturn 0;\n}\n\n#endif\nstatic void igb_ping_all_vfs(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 ping;\n\tint i;\n\n\tfor (i = 0 ; i < adapter->vfs_allocated_count; i++) {\n\t\tping = E1000_PF_CONTROL_MSG;\n\t\tif (adapter->vf_data[i].flags & IGB_VF_FLAG_CTS)\n\t\t\tping |= E1000_VT_MSGTYPE_CTS;\n\t\tigb_write_mbx(hw, &ping, 1, i);\n\t}\n}\n\nstatic int igb_set_vf_promisc(struct igb_adapter *adapter, u32 *msgbuf, u32 vf)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 vmolr = rd32(E1000_VMOLR(vf));\n\tstruct vf_data_storage *vf_data = &adapter->vf_data[vf];\n\n\tvf_data->flags &= ~(IGB_VF_FLAG_UNI_PROMISC |\n\t\t\t    IGB_VF_FLAG_MULTI_PROMISC);\n\tvmolr &= ~(E1000_VMOLR_ROPE | E1000_VMOLR_ROMPE | E1000_VMOLR_MPME);\n\n\tif (*msgbuf & E1000_VF_SET_PROMISC_MULTICAST) {\n\t\tvmolr |= E1000_VMOLR_MPME;\n\t\tvf_data->flags |= IGB_VF_FLAG_MULTI_PROMISC;\n\t\t*msgbuf &= ~E1000_VF_SET_PROMISC_MULTICAST;\n\t} else {\n\t\t \n\t\tif (vf_data->num_vf_mc_hashes > 30) {\n\t\t\tvmolr |= E1000_VMOLR_MPME;\n\t\t} else if (vf_data->num_vf_mc_hashes) {\n\t\t\tint j;\n\n\t\t\tvmolr |= E1000_VMOLR_ROMPE;\n\t\t\tfor (j = 0; j < vf_data->num_vf_mc_hashes; j++)\n\t\t\t\tigb_mta_set(hw, vf_data->vf_mc_hashes[j]);\n\t\t}\n\t}\n\n\twr32(E1000_VMOLR(vf), vmolr);\n\n\t \n\tif (*msgbuf & E1000_VT_MSGINFO_MASK)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int igb_set_vf_multicasts(struct igb_adapter *adapter,\n\t\t\t\t  u32 *msgbuf, u32 vf)\n{\n\tint n = (msgbuf[0] & E1000_VT_MSGINFO_MASK) >> E1000_VT_MSGINFO_SHIFT;\n\tu16 *hash_list = (u16 *)&msgbuf[1];\n\tstruct vf_data_storage *vf_data = &adapter->vf_data[vf];\n\tint i;\n\n\t \n\tvf_data->num_vf_mc_hashes = n;\n\n\t \n\tif (n > 30)\n\t\tn = 30;\n\n\t \n\tfor (i = 0; i < n; i++)\n\t\tvf_data->vf_mc_hashes[i] = hash_list[i];\n\n\t \n\tigb_set_rx_mode(adapter->netdev);\n\n\treturn 0;\n}\n\nstatic void igb_restore_vf_multicasts(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct vf_data_storage *vf_data;\n\tint i, j;\n\n\tfor (i = 0; i < adapter->vfs_allocated_count; i++) {\n\t\tu32 vmolr = rd32(E1000_VMOLR(i));\n\n\t\tvmolr &= ~(E1000_VMOLR_ROMPE | E1000_VMOLR_MPME);\n\n\t\tvf_data = &adapter->vf_data[i];\n\n\t\tif ((vf_data->num_vf_mc_hashes > 30) ||\n\t\t    (vf_data->flags & IGB_VF_FLAG_MULTI_PROMISC)) {\n\t\t\tvmolr |= E1000_VMOLR_MPME;\n\t\t} else if (vf_data->num_vf_mc_hashes) {\n\t\t\tvmolr |= E1000_VMOLR_ROMPE;\n\t\t\tfor (j = 0; j < vf_data->num_vf_mc_hashes; j++)\n\t\t\t\tigb_mta_set(hw, vf_data->vf_mc_hashes[j]);\n\t\t}\n\t\twr32(E1000_VMOLR(i), vmolr);\n\t}\n}\n\nstatic void igb_clear_vf_vfta(struct igb_adapter *adapter, u32 vf)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 pool_mask, vlvf_mask, i;\n\n\t \n\tpool_mask = E1000_VLVF_POOLSEL_MASK;\n\tvlvf_mask = BIT(E1000_VLVF_POOLSEL_SHIFT + vf);\n\n\t \n\tpool_mask &= ~BIT(E1000_VLVF_POOLSEL_SHIFT +\n\t\t\t     adapter->vfs_allocated_count);\n\n\t \n\tfor (i = E1000_VLVF_ARRAY_SIZE; i--;) {\n\t\tu32 vlvf = rd32(E1000_VLVF(i));\n\t\tu32 vfta_mask, vid, vfta;\n\n\t\t \n\t\tif (!(vlvf & vlvf_mask))\n\t\t\tcontinue;\n\n\t\t \n\t\tvlvf ^= vlvf_mask;\n\n\t\t \n\t\tif (vlvf & pool_mask)\n\t\t\tgoto update_vlvfb;\n\n\t\t \n\t\tif (vlvf & E1000_VLVF_POOLSEL_MASK)\n\t\t\tgoto update_vlvf;\n\n\t\tvid = vlvf & E1000_VLVF_VLANID_MASK;\n\t\tvfta_mask = BIT(vid % 32);\n\n\t\t \n\t\tvfta = adapter->shadow_vfta[vid / 32];\n\t\tif (vfta & vfta_mask)\n\t\t\thw->mac.ops.write_vfta(hw, vid / 32, vfta ^ vfta_mask);\nupdate_vlvf:\n\t\t \n\t\tif (adapter->flags & IGB_FLAG_VLAN_PROMISC)\n\t\t\tvlvf &= E1000_VLVF_POOLSEL_MASK;\n\t\telse\n\t\t\tvlvf = 0;\nupdate_vlvfb:\n\t\t \n\t\twr32(E1000_VLVF(i), vlvf);\n\t}\n}\n\nstatic int igb_find_vlvf_entry(struct e1000_hw *hw, u32 vlan)\n{\n\tu32 vlvf;\n\tint idx;\n\n\t \n\tif (vlan == 0)\n\t\treturn 0;\n\n\t \n\tfor (idx = E1000_VLVF_ARRAY_SIZE; --idx;) {\n\t\tvlvf = rd32(E1000_VLVF(idx));\n\t\tif ((vlvf & VLAN_VID_MASK) == vlan)\n\t\t\tbreak;\n\t}\n\n\treturn idx;\n}\n\nstatic void igb_update_pf_vlvf(struct igb_adapter *adapter, u32 vid)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 bits, pf_id;\n\tint idx;\n\n\tidx = igb_find_vlvf_entry(hw, vid);\n\tif (!idx)\n\t\treturn;\n\n\t \n\tpf_id = adapter->vfs_allocated_count + E1000_VLVF_POOLSEL_SHIFT;\n\tbits = ~BIT(pf_id) & E1000_VLVF_POOLSEL_MASK;\n\tbits &= rd32(E1000_VLVF(idx));\n\n\t \n\tif (!bits) {\n\t\tif (adapter->flags & IGB_FLAG_VLAN_PROMISC)\n\t\t\twr32(E1000_VLVF(idx), BIT(pf_id));\n\t\telse\n\t\t\twr32(E1000_VLVF(idx), 0);\n\t}\n}\n\nstatic s32 igb_set_vf_vlan(struct igb_adapter *adapter, u32 vid,\n\t\t\t   bool add, u32 vf)\n{\n\tint pf_id = adapter->vfs_allocated_count;\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint err;\n\n\t \n\tif (add && test_bit(vid, adapter->active_vlans)) {\n\t\terr = igb_vfta_set(hw, vid, pf_id, true, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = igb_vfta_set(hw, vid, vf, add, false);\n\n\tif (add && !err)\n\t\treturn err;\n\n\t \n\tif (test_bit(vid, adapter->active_vlans) ||\n\t    (adapter->flags & IGB_FLAG_VLAN_PROMISC))\n\t\tigb_update_pf_vlvf(adapter, vid);\n\n\treturn err;\n}\n\nstatic void igb_set_vmvir(struct igb_adapter *adapter, u32 vid, u32 vf)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\tif (vid)\n\t\twr32(E1000_VMVIR(vf), (vid | E1000_VMVIR_VLANA_DEFAULT));\n\telse\n\t\twr32(E1000_VMVIR(vf), 0);\n}\n\nstatic int igb_enable_port_vlan(struct igb_adapter *adapter, int vf,\n\t\t\t\tu16 vlan, u8 qos)\n{\n\tint err;\n\n\terr = igb_set_vf_vlan(adapter, vlan, true, vf);\n\tif (err)\n\t\treturn err;\n\n\tigb_set_vmvir(adapter, vlan | (qos << VLAN_PRIO_SHIFT), vf);\n\tigb_set_vmolr(adapter, vf, !vlan);\n\n\t \n\tif (vlan != adapter->vf_data[vf].pf_vlan)\n\t\tigb_set_vf_vlan(adapter, adapter->vf_data[vf].pf_vlan,\n\t\t\t\tfalse, vf);\n\n\tadapter->vf_data[vf].pf_vlan = vlan;\n\tadapter->vf_data[vf].pf_qos = qos;\n\tigb_set_vf_vlan_strip(adapter, vf, true);\n\tdev_info(&adapter->pdev->dev,\n\t\t \"Setting VLAN %d, QOS 0x%x on VF %d\\n\", vlan, qos, vf);\n\tif (test_bit(__IGB_DOWN, &adapter->state)) {\n\t\tdev_warn(&adapter->pdev->dev,\n\t\t\t \"The VF VLAN has been set, but the PF device is not up.\\n\");\n\t\tdev_warn(&adapter->pdev->dev,\n\t\t\t \"Bring the PF device up before attempting to use the VF device.\\n\");\n\t}\n\n\treturn err;\n}\n\nstatic int igb_disable_port_vlan(struct igb_adapter *adapter, int vf)\n{\n\t \n\tigb_set_vf_vlan(adapter, 0, true, vf);\n\n\tigb_set_vmvir(adapter, 0, vf);\n\tigb_set_vmolr(adapter, vf, true);\n\n\t \n\tif (adapter->vf_data[vf].pf_vlan)\n\t\tigb_set_vf_vlan(adapter, adapter->vf_data[vf].pf_vlan,\n\t\t\t\tfalse, vf);\n\n\tadapter->vf_data[vf].pf_vlan = 0;\n\tadapter->vf_data[vf].pf_qos = 0;\n\tigb_set_vf_vlan_strip(adapter, vf, false);\n\n\treturn 0;\n}\n\nstatic int igb_ndo_set_vf_vlan(struct net_device *netdev, int vf,\n\t\t\t       u16 vlan, u8 qos, __be16 vlan_proto)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\tif ((vf >= adapter->vfs_allocated_count) || (vlan > 4095) || (qos > 7))\n\t\treturn -EINVAL;\n\n\tif (vlan_proto != htons(ETH_P_8021Q))\n\t\treturn -EPROTONOSUPPORT;\n\n\treturn (vlan || qos) ? igb_enable_port_vlan(adapter, vf, vlan, qos) :\n\t\t\t       igb_disable_port_vlan(adapter, vf);\n}\n\nstatic int igb_set_vf_vlan_msg(struct igb_adapter *adapter, u32 *msgbuf, u32 vf)\n{\n\tint add = (msgbuf[0] & E1000_VT_MSGINFO_MASK) >> E1000_VT_MSGINFO_SHIFT;\n\tint vid = (msgbuf[1] & E1000_VLVF_VLANID_MASK);\n\tint ret;\n\n\tif (adapter->vf_data[vf].pf_vlan)\n\t\treturn -1;\n\n\t \n\tif (!vid && !add)\n\t\treturn 0;\n\n\tret = igb_set_vf_vlan(adapter, vid, !!add, vf);\n\tif (!ret)\n\t\tigb_set_vf_vlan_strip(adapter, vf, !!vid);\n\treturn ret;\n}\n\nstatic inline void igb_vf_reset(struct igb_adapter *adapter, u32 vf)\n{\n\tstruct vf_data_storage *vf_data = &adapter->vf_data[vf];\n\n\t \n\tvf_data->flags &= IGB_VF_FLAG_PF_SET_MAC;\n\tvf_data->last_nack = jiffies;\n\n\t \n\tigb_clear_vf_vfta(adapter, vf);\n\tigb_set_vf_vlan(adapter, vf_data->pf_vlan, true, vf);\n\tigb_set_vmvir(adapter, vf_data->pf_vlan |\n\t\t\t       (vf_data->pf_qos << VLAN_PRIO_SHIFT), vf);\n\tigb_set_vmolr(adapter, vf, !vf_data->pf_vlan);\n\tigb_set_vf_vlan_strip(adapter, vf, !!(vf_data->pf_vlan));\n\n\t \n\tadapter->vf_data[vf].num_vf_mc_hashes = 0;\n\n\t \n\tigb_set_rx_mode(adapter->netdev);\n}\n\nstatic void igb_vf_reset_event(struct igb_adapter *adapter, u32 vf)\n{\n\tunsigned char *vf_mac = adapter->vf_data[vf].vf_mac_addresses;\n\n\t \n\tif (!(adapter->vf_data[vf].flags & IGB_VF_FLAG_PF_SET_MAC))\n\t\teth_zero_addr(vf_mac);\n\n\t \n\tigb_vf_reset(adapter, vf);\n}\n\nstatic void igb_vf_reset_msg(struct igb_adapter *adapter, u32 vf)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tunsigned char *vf_mac = adapter->vf_data[vf].vf_mac_addresses;\n\tu32 reg, msgbuf[3] = {};\n\tu8 *addr = (u8 *)(&msgbuf[1]);\n\n\t \n\tigb_vf_reset(adapter, vf);\n\n\t \n\tigb_set_vf_mac(adapter, vf, vf_mac);\n\n\t \n\treg = rd32(E1000_VFTE);\n\twr32(E1000_VFTE, reg | BIT(vf));\n\treg = rd32(E1000_VFRE);\n\twr32(E1000_VFRE, reg | BIT(vf));\n\n\tadapter->vf_data[vf].flags |= IGB_VF_FLAG_CTS;\n\n\t \n\tif (!is_zero_ether_addr(vf_mac)) {\n\t\tmsgbuf[0] = E1000_VF_RESET | E1000_VT_MSGTYPE_ACK;\n\t\tmemcpy(addr, vf_mac, ETH_ALEN);\n\t} else {\n\t\tmsgbuf[0] = E1000_VF_RESET | E1000_VT_MSGTYPE_NACK;\n\t}\n\tigb_write_mbx(hw, msgbuf, 3, vf);\n}\n\nstatic void igb_flush_mac_table(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint i;\n\n\tfor (i = 0; i < hw->mac.rar_entry_count; i++) {\n\t\tadapter->mac_table[i].state &= ~IGB_MAC_STATE_IN_USE;\n\t\teth_zero_addr(adapter->mac_table[i].addr);\n\t\tadapter->mac_table[i].queue = 0;\n\t\tigb_rar_set_index(adapter, i);\n\t}\n}\n\nstatic int igb_available_rars(struct igb_adapter *adapter, u8 queue)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\t \n\tint rar_entries = hw->mac.rar_entry_count -\n\t\t\t  adapter->vfs_allocated_count;\n\tint i, count = 0;\n\n\tfor (i = 0; i < rar_entries; i++) {\n\t\t \n\t\tif (adapter->mac_table[i].state & IGB_MAC_STATE_DEFAULT)\n\t\t\tcontinue;\n\n\t\t \n\t\tif ((adapter->mac_table[i].state & IGB_MAC_STATE_IN_USE) &&\n\t\t    (adapter->mac_table[i].queue != queue))\n\t\t\tcontinue;\n\n\t\tcount++;\n\t}\n\n\treturn count;\n}\n\n \nstatic void igb_set_default_mac_filter(struct igb_adapter *adapter)\n{\n\tstruct igb_mac_addr *mac_table = &adapter->mac_table[0];\n\n\tether_addr_copy(mac_table->addr, adapter->hw.mac.addr);\n\tmac_table->queue = adapter->vfs_allocated_count;\n\tmac_table->state = IGB_MAC_STATE_DEFAULT | IGB_MAC_STATE_IN_USE;\n\n\tigb_rar_set_index(adapter, 0);\n}\n\n \nstatic bool igb_mac_entry_can_be_used(const struct igb_mac_addr *entry,\n\t\t\t\t      const u8 *addr, const u8 flags)\n{\n\tif (!(entry->state & IGB_MAC_STATE_IN_USE))\n\t\treturn true;\n\n\tif ((entry->state & IGB_MAC_STATE_SRC_ADDR) !=\n\t    (flags & IGB_MAC_STATE_SRC_ADDR))\n\t\treturn false;\n\n\tif (!ether_addr_equal(addr, entry->addr))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic int igb_add_mac_filter_flags(struct igb_adapter *adapter,\n\t\t\t\t    const u8 *addr, const u8 queue,\n\t\t\t\t    const u8 flags)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint rar_entries = hw->mac.rar_entry_count -\n\t\t\t  adapter->vfs_allocated_count;\n\tint i;\n\n\tif (is_zero_ether_addr(addr))\n\t\treturn -EINVAL;\n\n\t \n\tfor (i = 0; i < rar_entries; i++) {\n\t\tif (!igb_mac_entry_can_be_used(&adapter->mac_table[i],\n\t\t\t\t\t       addr, flags))\n\t\t\tcontinue;\n\n\t\tether_addr_copy(adapter->mac_table[i].addr, addr);\n\t\tadapter->mac_table[i].queue = queue;\n\t\tadapter->mac_table[i].state |= IGB_MAC_STATE_IN_USE | flags;\n\n\t\tigb_rar_set_index(adapter, i);\n\t\treturn i;\n\t}\n\n\treturn -ENOSPC;\n}\n\nstatic int igb_add_mac_filter(struct igb_adapter *adapter, const u8 *addr,\n\t\t\t      const u8 queue)\n{\n\treturn igb_add_mac_filter_flags(adapter, addr, queue, 0);\n}\n\n \nstatic int igb_del_mac_filter_flags(struct igb_adapter *adapter,\n\t\t\t\t    const u8 *addr, const u8 queue,\n\t\t\t\t    const u8 flags)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint rar_entries = hw->mac.rar_entry_count -\n\t\t\t  adapter->vfs_allocated_count;\n\tint i;\n\n\tif (is_zero_ether_addr(addr))\n\t\treturn -EINVAL;\n\n\t \n\tfor (i = 0; i < rar_entries; i++) {\n\t\tif (!(adapter->mac_table[i].state & IGB_MAC_STATE_IN_USE))\n\t\t\tcontinue;\n\t\tif ((adapter->mac_table[i].state & flags) != flags)\n\t\t\tcontinue;\n\t\tif (adapter->mac_table[i].queue != queue)\n\t\t\tcontinue;\n\t\tif (!ether_addr_equal(adapter->mac_table[i].addr, addr))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (adapter->mac_table[i].state & IGB_MAC_STATE_DEFAULT) {\n\t\t\tadapter->mac_table[i].state =\n\t\t\t\tIGB_MAC_STATE_DEFAULT | IGB_MAC_STATE_IN_USE;\n\t\t\tadapter->mac_table[i].queue =\n\t\t\t\tadapter->vfs_allocated_count;\n\t\t} else {\n\t\t\tadapter->mac_table[i].state = 0;\n\t\t\tadapter->mac_table[i].queue = 0;\n\t\t\teth_zero_addr(adapter->mac_table[i].addr);\n\t\t}\n\n\t\tigb_rar_set_index(adapter, i);\n\t\treturn 0;\n\t}\n\n\treturn -ENOENT;\n}\n\nstatic int igb_del_mac_filter(struct igb_adapter *adapter, const u8 *addr,\n\t\t\t      const u8 queue)\n{\n\treturn igb_del_mac_filter_flags(adapter, addr, queue, 0);\n}\n\nint igb_add_mac_steering_filter(struct igb_adapter *adapter,\n\t\t\t\tconst u8 *addr, u8 queue, u8 flags)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\t \n\tif (hw->mac.type != e1000_i210)\n\t\treturn -EOPNOTSUPP;\n\n\treturn igb_add_mac_filter_flags(adapter, addr, queue,\n\t\t\t\t\tIGB_MAC_STATE_QUEUE_STEERING | flags);\n}\n\nint igb_del_mac_steering_filter(struct igb_adapter *adapter,\n\t\t\t\tconst u8 *addr, u8 queue, u8 flags)\n{\n\treturn igb_del_mac_filter_flags(adapter, addr, queue,\n\t\t\t\t\tIGB_MAC_STATE_QUEUE_STEERING | flags);\n}\n\nstatic int igb_uc_sync(struct net_device *netdev, const unsigned char *addr)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tint ret;\n\n\tret = igb_add_mac_filter(adapter, addr, adapter->vfs_allocated_count);\n\n\treturn min_t(int, ret, 0);\n}\n\nstatic int igb_uc_unsync(struct net_device *netdev, const unsigned char *addr)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\tigb_del_mac_filter(adapter, addr, adapter->vfs_allocated_count);\n\n\treturn 0;\n}\n\nstatic int igb_set_vf_mac_filter(struct igb_adapter *adapter, const int vf,\n\t\t\t\t const u32 info, const u8 *addr)\n{\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct vf_data_storage *vf_data = &adapter->vf_data[vf];\n\tstruct list_head *pos;\n\tstruct vf_mac_filter *entry = NULL;\n\tint ret = 0;\n\n\tif ((vf_data->flags & IGB_VF_FLAG_PF_SET_MAC) &&\n\t    !vf_data->trusted) {\n\t\tdev_warn(&pdev->dev,\n\t\t\t \"VF %d requested MAC filter but is administratively denied\\n\",\n\t\t\t  vf);\n\t\treturn -EINVAL;\n\t}\n\tif (!is_valid_ether_addr(addr)) {\n\t\tdev_warn(&pdev->dev,\n\t\t\t \"VF %d attempted to set invalid MAC filter\\n\",\n\t\t\t  vf);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (info) {\n\tcase E1000_VF_MAC_FILTER_CLR:\n\t\t \n\t\tlist_for_each(pos, &adapter->vf_macs.l) {\n\t\t\tentry = list_entry(pos, struct vf_mac_filter, l);\n\t\t\tif (entry->vf == vf) {\n\t\t\t\tentry->vf = -1;\n\t\t\t\tentry->free = true;\n\t\t\t\tigb_del_mac_filter(adapter, entry->vf_mac, vf);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase E1000_VF_MAC_FILTER_ADD:\n\t\t \n\t\tlist_for_each(pos, &adapter->vf_macs.l) {\n\t\t\tentry = list_entry(pos, struct vf_mac_filter, l);\n\t\t\tif (entry->free)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (entry && entry->free) {\n\t\t\tentry->free = false;\n\t\t\tentry->vf = vf;\n\t\t\tether_addr_copy(entry->vf_mac, addr);\n\n\t\t\tret = igb_add_mac_filter(adapter, addr, vf);\n\t\t\tret = min_t(int, ret, 0);\n\t\t} else {\n\t\t\tret = -ENOSPC;\n\t\t}\n\n\t\tif (ret == -ENOSPC)\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"VF %d has requested MAC filter but there is no space for it\\n\",\n\t\t\t\t vf);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int igb_set_vf_mac_addr(struct igb_adapter *adapter, u32 *msg, int vf)\n{\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct vf_data_storage *vf_data = &adapter->vf_data[vf];\n\tu32 info = msg[0] & E1000_VT_MSGINFO_MASK;\n\n\t \n\tunsigned char *addr = (unsigned char *)&msg[1];\n\tint ret = 0;\n\n\tif (!info) {\n\t\tif ((vf_data->flags & IGB_VF_FLAG_PF_SET_MAC) &&\n\t\t    !vf_data->trusted) {\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"VF %d attempted to override administratively set MAC address\\nReload the VF driver to resume operations\\n\",\n\t\t\t\t vf);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!is_valid_ether_addr(addr)) {\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"VF %d attempted to set invalid MAC\\n\",\n\t\t\t\t vf);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tret = igb_set_vf_mac(adapter, vf, addr);\n\t} else {\n\t\tret = igb_set_vf_mac_filter(adapter, vf, info, addr);\n\t}\n\n\treturn ret;\n}\n\nstatic void igb_rcv_ack_from_vf(struct igb_adapter *adapter, u32 vf)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct vf_data_storage *vf_data = &adapter->vf_data[vf];\n\tu32 msg = E1000_VT_MSGTYPE_NACK;\n\n\t \n\tif (!(vf_data->flags & IGB_VF_FLAG_CTS) &&\n\t    time_after(jiffies, vf_data->last_nack + (2 * HZ))) {\n\t\tigb_write_mbx(hw, &msg, 1, vf);\n\t\tvf_data->last_nack = jiffies;\n\t}\n}\n\nstatic void igb_rcv_msg_from_vf(struct igb_adapter *adapter, u32 vf)\n{\n\tstruct pci_dev *pdev = adapter->pdev;\n\tu32 msgbuf[E1000_VFMAILBOX_SIZE];\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct vf_data_storage *vf_data = &adapter->vf_data[vf];\n\ts32 retval;\n\n\tretval = igb_read_mbx(hw, msgbuf, E1000_VFMAILBOX_SIZE, vf, false);\n\n\tif (retval) {\n\t\t \n\t\tdev_err(&pdev->dev, \"Error receiving message from VF\\n\");\n\t\tvf_data->flags &= ~IGB_VF_FLAG_CTS;\n\t\tif (!time_after(jiffies, vf_data->last_nack + (2 * HZ)))\n\t\t\tgoto unlock;\n\t\tgoto out;\n\t}\n\n\t \n\tif (msgbuf[0] & (E1000_VT_MSGTYPE_ACK | E1000_VT_MSGTYPE_NACK))\n\t\tgoto unlock;\n\n\t \n\tif (msgbuf[0] == E1000_VF_RESET) {\n\t\t \n\t\tigb_vf_reset_msg(adapter, vf);\n\t\treturn;\n\t}\n\n\tif (!(vf_data->flags & IGB_VF_FLAG_CTS)) {\n\t\tif (!time_after(jiffies, vf_data->last_nack + (2 * HZ)))\n\t\t\tgoto unlock;\n\t\tretval = -1;\n\t\tgoto out;\n\t}\n\n\tswitch ((msgbuf[0] & 0xFFFF)) {\n\tcase E1000_VF_SET_MAC_ADDR:\n\t\tretval = igb_set_vf_mac_addr(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase E1000_VF_SET_PROMISC:\n\t\tretval = igb_set_vf_promisc(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase E1000_VF_SET_MULTICAST:\n\t\tretval = igb_set_vf_multicasts(adapter, msgbuf, vf);\n\t\tbreak;\n\tcase E1000_VF_SET_LPE:\n\t\tretval = igb_set_vf_rlpml(adapter, msgbuf[1], vf);\n\t\tbreak;\n\tcase E1000_VF_SET_VLAN:\n\t\tretval = -1;\n\t\tif (vf_data->pf_vlan)\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"VF %d attempted to override administratively set VLAN tag\\nReload the VF driver to resume operations\\n\",\n\t\t\t\t vf);\n\t\telse\n\t\t\tretval = igb_set_vf_vlan_msg(adapter, msgbuf, vf);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&pdev->dev, \"Unhandled Msg %08x\\n\", msgbuf[0]);\n\t\tretval = -1;\n\t\tbreak;\n\t}\n\n\tmsgbuf[0] |= E1000_VT_MSGTYPE_CTS;\nout:\n\t \n\tif (retval)\n\t\tmsgbuf[0] |= E1000_VT_MSGTYPE_NACK;\n\telse\n\t\tmsgbuf[0] |= E1000_VT_MSGTYPE_ACK;\n\n\t \n\tigb_write_mbx(hw, msgbuf, 1, vf);\n\treturn;\n\nunlock:\n\tigb_unlock_mbx(hw, vf);\n}\n\nstatic void igb_msg_task(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tunsigned long flags;\n\tu32 vf;\n\n\tspin_lock_irqsave(&adapter->vfs_lock, flags);\n\tfor (vf = 0; vf < adapter->vfs_allocated_count; vf++) {\n\t\t \n\t\tif (!igb_check_for_rst(hw, vf))\n\t\t\tigb_vf_reset_event(adapter, vf);\n\n\t\t \n\t\tif (!igb_check_for_msg(hw, vf))\n\t\t\tigb_rcv_msg_from_vf(adapter, vf);\n\n\t\t \n\t\tif (!igb_check_for_ack(hw, vf))\n\t\t\tigb_rcv_ack_from_vf(adapter, vf);\n\t}\n\tspin_unlock_irqrestore(&adapter->vfs_lock, flags);\n}\n\n \nstatic void igb_set_uta(struct igb_adapter *adapter, bool set)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 uta = set ? ~0 : 0;\n\tint i;\n\n\t \n\tif (!adapter->vfs_allocated_count)\n\t\treturn;\n\n\tfor (i = hw->mac.uta_reg_count; i--;)\n\t\tarray_wr32(E1000_UTA, i, uta);\n}\n\n \nstatic irqreturn_t igb_intr_msi(int irq, void *data)\n{\n\tstruct igb_adapter *adapter = data;\n\tstruct igb_q_vector *q_vector = adapter->q_vector[0];\n\tstruct e1000_hw *hw = &adapter->hw;\n\t \n\tu32 icr = rd32(E1000_ICR);\n\n\tigb_write_itr(q_vector);\n\n\tif (icr & E1000_ICR_DRSTA)\n\t\tschedule_work(&adapter->reset_task);\n\n\tif (icr & E1000_ICR_DOUTSYNC) {\n\t\t \n\t\tadapter->stats.doosync++;\n\t}\n\n\tif (icr & (E1000_ICR_RXSEQ | E1000_ICR_LSC)) {\n\t\thw->mac.get_link_status = 1;\n\t\tif (!test_bit(__IGB_DOWN, &adapter->state))\n\t\t\tmod_timer(&adapter->watchdog_timer, jiffies + 1);\n\t}\n\n\tif (icr & E1000_ICR_TS)\n\t\tigb_tsync_interrupt(adapter);\n\n\tnapi_schedule(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t igb_intr(int irq, void *data)\n{\n\tstruct igb_adapter *adapter = data;\n\tstruct igb_q_vector *q_vector = adapter->q_vector[0];\n\tstruct e1000_hw *hw = &adapter->hw;\n\t \n\tu32 icr = rd32(E1000_ICR);\n\n\t \n\tif (!(icr & E1000_ICR_INT_ASSERTED))\n\t\treturn IRQ_NONE;\n\n\tigb_write_itr(q_vector);\n\n\tif (icr & E1000_ICR_DRSTA)\n\t\tschedule_work(&adapter->reset_task);\n\n\tif (icr & E1000_ICR_DOUTSYNC) {\n\t\t \n\t\tadapter->stats.doosync++;\n\t}\n\n\tif (icr & (E1000_ICR_RXSEQ | E1000_ICR_LSC)) {\n\t\thw->mac.get_link_status = 1;\n\t\t \n\t\tif (!test_bit(__IGB_DOWN, &adapter->state))\n\t\t\tmod_timer(&adapter->watchdog_timer, jiffies + 1);\n\t}\n\n\tif (icr & E1000_ICR_TS)\n\t\tigb_tsync_interrupt(adapter);\n\n\tnapi_schedule(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void igb_ring_irq_enable(struct igb_q_vector *q_vector)\n{\n\tstruct igb_adapter *adapter = q_vector->adapter;\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\tif ((q_vector->rx.ring && (adapter->rx_itr_setting & 3)) ||\n\t    (!q_vector->rx.ring && (adapter->tx_itr_setting & 3))) {\n\t\tif ((adapter->num_q_vectors == 1) && !adapter->vf_data)\n\t\t\tigb_set_itr(q_vector);\n\t\telse\n\t\t\tigb_update_ring_itr(q_vector);\n\t}\n\n\tif (!test_bit(__IGB_DOWN, &adapter->state)) {\n\t\tif (adapter->flags & IGB_FLAG_HAS_MSIX)\n\t\t\twr32(E1000_EIMS, q_vector->eims_value);\n\t\telse\n\t\t\tigb_irq_enable(adapter);\n\t}\n}\n\n \nstatic int igb_poll(struct napi_struct *napi, int budget)\n{\n\tstruct igb_q_vector *q_vector = container_of(napi,\n\t\t\t\t\t\t     struct igb_q_vector,\n\t\t\t\t\t\t     napi);\n\tbool clean_complete = true;\n\tint work_done = 0;\n\n#ifdef CONFIG_IGB_DCA\n\tif (q_vector->adapter->flags & IGB_FLAG_DCA_ENABLED)\n\t\tigb_update_dca(q_vector);\n#endif\n\tif (q_vector->tx.ring)\n\t\tclean_complete = igb_clean_tx_irq(q_vector, budget);\n\n\tif (q_vector->rx.ring) {\n\t\tint cleaned = igb_clean_rx_irq(q_vector, budget);\n\n\t\twork_done += cleaned;\n\t\tif (cleaned >= budget)\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (!clean_complete)\n\t\treturn budget;\n\n\t \n\tif (likely(napi_complete_done(napi, work_done)))\n\t\tigb_ring_irq_enable(q_vector);\n\n\treturn work_done;\n}\n\n \nstatic bool igb_clean_tx_irq(struct igb_q_vector *q_vector, int napi_budget)\n{\n\tstruct igb_adapter *adapter = q_vector->adapter;\n\tstruct igb_ring *tx_ring = q_vector->tx.ring;\n\tstruct igb_tx_buffer *tx_buffer;\n\tunion e1000_adv_tx_desc *tx_desc;\n\tunsigned int total_bytes = 0, total_packets = 0;\n\tunsigned int budget = q_vector->tx.work_limit;\n\tunsigned int i = tx_ring->next_to_clean;\n\n\tif (test_bit(__IGB_DOWN, &adapter->state))\n\t\treturn true;\n\n\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\ttx_desc = IGB_TX_DESC(tx_ring, i);\n\ti -= tx_ring->count;\n\n\tdo {\n\t\tunion e1000_adv_tx_desc *eop_desc = tx_buffer->next_to_watch;\n\n\t\t \n\t\tif (!eop_desc)\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\t \n\t\tif (!(eop_desc->wb.status & cpu_to_le32(E1000_TXD_STAT_DD)))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buffer->next_to_watch = NULL;\n\n\t\t \n\t\ttotal_bytes += tx_buffer->bytecount;\n\t\ttotal_packets += tx_buffer->gso_segs;\n\n\t\t \n\t\tif (tx_buffer->type == IGB_TYPE_SKB)\n\t\t\tnapi_consume_skb(tx_buffer->skb, napi_budget);\n\t\telse\n\t\t\txdp_return_frame(tx_buffer->xdpf);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(!i)) {\n\t\t\t\ti -= tx_ring->count;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = IGB_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len)) {\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\ttx_buffer++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\ttx_desc = IGB_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\t \n\t\tprefetch(tx_desc);\n\n\t\t \n\t\tbudget--;\n\t} while (likely(budget));\n\n\tnetdev_tx_completed_queue(txring_txq(tx_ring),\n\t\t\t\t  total_packets, total_bytes);\n\ti += tx_ring->count;\n\ttx_ring->next_to_clean = i;\n\tu64_stats_update_begin(&tx_ring->tx_syncp);\n\ttx_ring->tx_stats.bytes += total_bytes;\n\ttx_ring->tx_stats.packets += total_packets;\n\tu64_stats_update_end(&tx_ring->tx_syncp);\n\tq_vector->tx.total_bytes += total_bytes;\n\tq_vector->tx.total_packets += total_packets;\n\n\tif (test_bit(IGB_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags)) {\n\t\tstruct e1000_hw *hw = &adapter->hw;\n\n\t\t \n\t\tclear_bit(IGB_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags);\n\t\tif (tx_buffer->next_to_watch &&\n\t\t    time_after(jiffies, tx_buffer->time_stamp +\n\t\t\t       (adapter->tx_timeout_factor * HZ)) &&\n\t\t    !(rd32(E1000_STATUS) & E1000_STATUS_TXOFF)) {\n\n\t\t\t \n\t\t\tdev_err(tx_ring->dev,\n\t\t\t\t\"Detected Tx Unit Hang\\n\"\n\t\t\t\t\"  Tx Queue             <%d>\\n\"\n\t\t\t\t\"  TDH                  <%x>\\n\"\n\t\t\t\t\"  TDT                  <%x>\\n\"\n\t\t\t\t\"  next_to_use          <%x>\\n\"\n\t\t\t\t\"  next_to_clean        <%x>\\n\"\n\t\t\t\t\"buffer_info[next_to_clean]\\n\"\n\t\t\t\t\"  time_stamp           <%lx>\\n\"\n\t\t\t\t\"  next_to_watch        <%p>\\n\"\n\t\t\t\t\"  jiffies              <%lx>\\n\"\n\t\t\t\t\"  desc.status          <%x>\\n\",\n\t\t\t\ttx_ring->queue_index,\n\t\t\t\trd32(E1000_TDH(tx_ring->reg_idx)),\n\t\t\t\treadl(tx_ring->tail),\n\t\t\t\ttx_ring->next_to_use,\n\t\t\t\ttx_ring->next_to_clean,\n\t\t\t\ttx_buffer->time_stamp,\n\t\t\t\ttx_buffer->next_to_watch,\n\t\t\t\tjiffies,\n\t\t\t\ttx_buffer->next_to_watch->wb.status);\n\t\t\tnetif_stop_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\n\t\t\t \n\t\t\treturn true;\n\t\t}\n\t}\n\n#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)\n\tif (unlikely(total_packets &&\n\t    netif_carrier_ok(tx_ring->netdev) &&\n\t    igb_desc_unused(tx_ring) >= TX_WAKE_THRESHOLD)) {\n\t\t \n\t\tsmp_mb();\n\t\tif (__netif_subqueue_stopped(tx_ring->netdev,\n\t\t\t\t\t     tx_ring->queue_index) &&\n\t\t    !(test_bit(__IGB_DOWN, &adapter->state))) {\n\t\t\tnetif_wake_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\n\t\t\tu64_stats_update_begin(&tx_ring->tx_syncp);\n\t\t\ttx_ring->tx_stats.restart_queue++;\n\t\t\tu64_stats_update_end(&tx_ring->tx_syncp);\n\t\t}\n\t}\n\n\treturn !!budget;\n}\n\n \nstatic void igb_reuse_rx_page(struct igb_ring *rx_ring,\n\t\t\t      struct igb_rx_buffer *old_buff)\n{\n\tstruct igb_rx_buffer *new_buff;\n\tu16 nta = rx_ring->next_to_alloc;\n\n\tnew_buff = &rx_ring->rx_buffer_info[nta];\n\n\t \n\tnta++;\n\trx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;\n\n\t \n\tnew_buff->dma\t\t= old_buff->dma;\n\tnew_buff->page\t\t= old_buff->page;\n\tnew_buff->page_offset\t= old_buff->page_offset;\n\tnew_buff->pagecnt_bias\t= old_buff->pagecnt_bias;\n}\n\nstatic bool igb_can_reuse_rx_page(struct igb_rx_buffer *rx_buffer,\n\t\t\t\t  int rx_buf_pgcnt)\n{\n\tunsigned int pagecnt_bias = rx_buffer->pagecnt_bias;\n\tstruct page *page = rx_buffer->page;\n\n\t \n\tif (!dev_page_is_reusable(page))\n\t\treturn false;\n\n#if (PAGE_SIZE < 8192)\n\t \n\tif (unlikely((rx_buf_pgcnt - pagecnt_bias) > 1))\n\t\treturn false;\n#else\n#define IGB_LAST_OFFSET \\\n\t(SKB_WITH_OVERHEAD(PAGE_SIZE) - IGB_RXBUFFER_2048)\n\n\tif (rx_buffer->page_offset > IGB_LAST_OFFSET)\n\t\treturn false;\n#endif\n\n\t \n\tif (unlikely(pagecnt_bias == 1)) {\n\t\tpage_ref_add(page, USHRT_MAX - 1);\n\t\trx_buffer->pagecnt_bias = USHRT_MAX;\n\t}\n\n\treturn true;\n}\n\n \nstatic void igb_add_rx_frag(struct igb_ring *rx_ring,\n\t\t\t    struct igb_rx_buffer *rx_buffer,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    unsigned int size)\n{\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = igb_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = ring_uses_build_skb(rx_ring) ?\n\t\t\t\tSKB_DATA_ALIGN(IGB_SKB_PAD + size) :\n\t\t\t\tSKB_DATA_ALIGN(size);\n#endif\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_buffer->page,\n\t\t\trx_buffer->page_offset, size, truesize);\n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n}\n\nstatic struct sk_buff *igb_construct_skb(struct igb_ring *rx_ring,\n\t\t\t\t\t struct igb_rx_buffer *rx_buffer,\n\t\t\t\t\t struct xdp_buff *xdp,\n\t\t\t\t\t ktime_t timestamp)\n{\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = igb_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = SKB_DATA_ALIGN(xdp->data_end -\n\t\t\t\t\t       xdp->data_hard_start);\n#endif\n\tunsigned int size = xdp->data_end - xdp->data;\n\tunsigned int headlen;\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data);\n\n\t \n\tskb = napi_alloc_skb(&rx_ring->q_vector->napi, IGB_RX_HDR_LEN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tif (timestamp)\n\t\tskb_hwtstamps(skb)->hwtstamp = timestamp;\n\n\t \n\theadlen = size;\n\tif (headlen > IGB_RX_HDR_LEN)\n\t\theadlen = eth_get_headlen(skb->dev, xdp->data, IGB_RX_HDR_LEN);\n\n\t \n\tmemcpy(__skb_put(skb, headlen), xdp->data, ALIGN(headlen, sizeof(long)));\n\n\t \n\tsize -= headlen;\n\tif (size) {\n\t\tskb_add_rx_frag(skb, 0, rx_buffer->page,\n\t\t\t\t(xdp->data + headlen) - page_address(rx_buffer->page),\n\t\t\t\tsize, truesize);\n#if (PAGE_SIZE < 8192)\n\t\trx_buffer->page_offset ^= truesize;\n#else\n\t\trx_buffer->page_offset += truesize;\n#endif\n\t} else {\n\t\trx_buffer->pagecnt_bias++;\n\t}\n\n\treturn skb;\n}\n\nstatic struct sk_buff *igb_build_skb(struct igb_ring *rx_ring,\n\t\t\t\t     struct igb_rx_buffer *rx_buffer,\n\t\t\t\t     struct xdp_buff *xdp,\n\t\t\t\t     ktime_t timestamp)\n{\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = igb_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) +\n\t\t\t\tSKB_DATA_ALIGN(xdp->data_end -\n\t\t\t\t\t       xdp->data_hard_start);\n#endif\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data_meta);\n\n\t \n\tskb = napi_build_skb(xdp->data_hard_start, truesize);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\t__skb_put(skb, xdp->data_end - xdp->data);\n\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\tif (timestamp)\n\t\tskb_hwtstamps(skb)->hwtstamp = timestamp;\n\n\t \n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n\n\treturn skb;\n}\n\nstatic struct sk_buff *igb_run_xdp(struct igb_adapter *adapter,\n\t\t\t\t   struct igb_ring *rx_ring,\n\t\t\t\t   struct xdp_buff *xdp)\n{\n\tint err, result = IGB_XDP_PASS;\n\tstruct bpf_prog *xdp_prog;\n\tu32 act;\n\n\txdp_prog = READ_ONCE(rx_ring->xdp_prog);\n\n\tif (!xdp_prog)\n\t\tgoto xdp_out;\n\n\tprefetchw(xdp->data_hard_start);  \n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\tresult = igb_xdp_xmit_back(adapter, xdp);\n\t\tif (result == IGB_XDP_CONSUMED)\n\t\t\tgoto out_failure;\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\terr = xdp_do_redirect(adapter->netdev, xdp, xdp_prog);\n\t\tif (err)\n\t\t\tgoto out_failure;\n\t\tresult = IGB_XDP_REDIR;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(adapter->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\nout_failure:\n\t\ttrace_xdp_exception(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tresult = IGB_XDP_CONSUMED;\n\t\tbreak;\n\t}\nxdp_out:\n\treturn ERR_PTR(-result);\n}\n\nstatic unsigned int igb_rx_frame_truesize(struct igb_ring *rx_ring,\n\t\t\t\t\t  unsigned int size)\n{\n\tunsigned int truesize;\n\n#if (PAGE_SIZE < 8192)\n\ttruesize = igb_rx_pg_size(rx_ring) / 2;  \n#else\n\ttruesize = ring_uses_build_skb(rx_ring) ?\n\t\tSKB_DATA_ALIGN(IGB_SKB_PAD + size) +\n\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info)) :\n\t\tSKB_DATA_ALIGN(size);\n#endif\n\treturn truesize;\n}\n\nstatic void igb_rx_buffer_flip(struct igb_ring *rx_ring,\n\t\t\t       struct igb_rx_buffer *rx_buffer,\n\t\t\t       unsigned int size)\n{\n\tunsigned int truesize = igb_rx_frame_truesize(rx_ring, size);\n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n}\n\nstatic inline void igb_rx_checksum(struct igb_ring *ring,\n\t\t\t\t   union e1000_adv_rx_desc *rx_desc,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tskb_checksum_none_assert(skb);\n\n\t \n\tif (igb_test_staterr(rx_desc, E1000_RXD_STAT_IXSM))\n\t\treturn;\n\n\t \n\tif (!(ring->netdev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\t \n\tif (igb_test_staterr(rx_desc,\n\t\t\t     E1000_RXDEXT_STATERR_TCPE |\n\t\t\t     E1000_RXDEXT_STATERR_IPE)) {\n\t\t \n\t\tif (!((skb->len == 60) &&\n\t\t      test_bit(IGB_RING_FLAG_RX_SCTP_CSUM, &ring->flags))) {\n\t\t\tu64_stats_update_begin(&ring->rx_syncp);\n\t\t\tring->rx_stats.csum_err++;\n\t\t\tu64_stats_update_end(&ring->rx_syncp);\n\t\t}\n\t\t \n\t\treturn;\n\t}\n\t \n\tif (igb_test_staterr(rx_desc, E1000_RXD_STAT_TCPCS |\n\t\t\t\t      E1000_RXD_STAT_UDPCS))\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tdev_dbg(ring->dev, \"cksum success: bits %08X\\n\",\n\t\tle32_to_cpu(rx_desc->wb.upper.status_error));\n}\n\nstatic inline void igb_rx_hash(struct igb_ring *ring,\n\t\t\t       union e1000_adv_rx_desc *rx_desc,\n\t\t\t       struct sk_buff *skb)\n{\n\tif (ring->netdev->features & NETIF_F_RXHASH)\n\t\tskb_set_hash(skb,\n\t\t\t     le32_to_cpu(rx_desc->wb.lower.hi_dword.rss),\n\t\t\t     PKT_HASH_TYPE_L3);\n}\n\n \nstatic bool igb_is_non_eop(struct igb_ring *rx_ring,\n\t\t\t   union e1000_adv_rx_desc *rx_desc)\n{\n\tu32 ntc = rx_ring->next_to_clean + 1;\n\n\t \n\tntc = (ntc < rx_ring->count) ? ntc : 0;\n\trx_ring->next_to_clean = ntc;\n\n\tprefetch(IGB_RX_DESC(rx_ring, ntc));\n\n\tif (likely(igb_test_staterr(rx_desc, E1000_RXD_STAT_EOP)))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic bool igb_cleanup_headers(struct igb_ring *rx_ring,\n\t\t\t\tunion e1000_adv_rx_desc *rx_desc,\n\t\t\t\tstruct sk_buff *skb)\n{\n\t \n\tif (IS_ERR(skb))\n\t\treturn true;\n\n\tif (unlikely((igb_test_staterr(rx_desc,\n\t\t\t\t       E1000_RXDEXT_ERR_FRAME_ERR_MASK)))) {\n\t\tstruct net_device *netdev = rx_ring->netdev;\n\t\tif (!(netdev->features & NETIF_F_RXALL)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t \n\tif (eth_skb_pad(skb))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void igb_process_skb_fields(struct igb_ring *rx_ring,\n\t\t\t\t   union e1000_adv_rx_desc *rx_desc,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tstruct net_device *dev = rx_ring->netdev;\n\n\tigb_rx_hash(rx_ring, rx_desc, skb);\n\n\tigb_rx_checksum(rx_ring, rx_desc, skb);\n\n\tif (igb_test_staterr(rx_desc, E1000_RXDADV_STAT_TS) &&\n\t    !igb_test_staterr(rx_desc, E1000_RXDADV_STAT_TSIP))\n\t\tigb_ptp_rx_rgtstamp(rx_ring->q_vector, skb);\n\n\tif ((dev->features & NETIF_F_HW_VLAN_CTAG_RX) &&\n\t    igb_test_staterr(rx_desc, E1000_RXD_STAT_VP)) {\n\t\tu16 vid;\n\n\t\tif (igb_test_staterr(rx_desc, E1000_RXDEXT_STATERR_LB) &&\n\t\t    test_bit(IGB_RING_FLAG_RX_LB_VLAN_BSWAP, &rx_ring->flags))\n\t\t\tvid = be16_to_cpu((__force __be16)rx_desc->wb.upper.vlan);\n\t\telse\n\t\t\tvid = le16_to_cpu(rx_desc->wb.upper.vlan);\n\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);\n\t}\n\n\tskb_record_rx_queue(skb, rx_ring->queue_index);\n\n\tskb->protocol = eth_type_trans(skb, rx_ring->netdev);\n}\n\nstatic unsigned int igb_rx_offset(struct igb_ring *rx_ring)\n{\n\treturn ring_uses_build_skb(rx_ring) ? IGB_SKB_PAD : 0;\n}\n\nstatic struct igb_rx_buffer *igb_get_rx_buffer(struct igb_ring *rx_ring,\n\t\t\t\t\t       const unsigned int size, int *rx_buf_pgcnt)\n{\n\tstruct igb_rx_buffer *rx_buffer;\n\n\trx_buffer = &rx_ring->rx_buffer_info[rx_ring->next_to_clean];\n\t*rx_buf_pgcnt =\n#if (PAGE_SIZE < 8192)\n\t\tpage_count(rx_buffer->page);\n#else\n\t\t0;\n#endif\n\tprefetchw(rx_buffer->page);\n\n\t \n\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t      rx_buffer->dma,\n\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t      size,\n\t\t\t\t      DMA_FROM_DEVICE);\n\n\trx_buffer->pagecnt_bias--;\n\n\treturn rx_buffer;\n}\n\nstatic void igb_put_rx_buffer(struct igb_ring *rx_ring,\n\t\t\t      struct igb_rx_buffer *rx_buffer, int rx_buf_pgcnt)\n{\n\tif (igb_can_reuse_rx_page(rx_buffer, rx_buf_pgcnt)) {\n\t\t \n\t\tigb_reuse_rx_page(rx_ring, rx_buffer);\n\t} else {\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,\n\t\t\t\t     igb_rx_pg_size(rx_ring), DMA_FROM_DEVICE,\n\t\t\t\t     IGB_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(rx_buffer->page,\n\t\t\t\t\trx_buffer->pagecnt_bias);\n\t}\n\n\t \n\trx_buffer->page = NULL;\n}\n\nstatic int igb_clean_rx_irq(struct igb_q_vector *q_vector, const int budget)\n{\n\tstruct igb_adapter *adapter = q_vector->adapter;\n\tstruct igb_ring *rx_ring = q_vector->rx.ring;\n\tstruct sk_buff *skb = rx_ring->skb;\n\tunsigned int total_bytes = 0, total_packets = 0;\n\tu16 cleaned_count = igb_desc_unused(rx_ring);\n\tunsigned int xdp_xmit = 0;\n\tstruct xdp_buff xdp;\n\tu32 frame_sz = 0;\n\tint rx_buf_pgcnt;\n\n\t \n#if (PAGE_SIZE < 8192)\n\tframe_sz = igb_rx_frame_truesize(rx_ring, 0);\n#endif\n\txdp_init_buff(&xdp, frame_sz, &rx_ring->xdp_rxq);\n\n\twhile (likely(total_packets < budget)) {\n\t\tunion e1000_adv_rx_desc *rx_desc;\n\t\tstruct igb_rx_buffer *rx_buffer;\n\t\tktime_t timestamp = 0;\n\t\tint pkt_offset = 0;\n\t\tunsigned int size;\n\t\tvoid *pktbuf;\n\n\t\t \n\t\tif (cleaned_count >= IGB_RX_BUFFER_WRITE) {\n\t\t\tigb_alloc_rx_buffers(rx_ring, cleaned_count);\n\t\t\tcleaned_count = 0;\n\t\t}\n\n\t\trx_desc = IGB_RX_DESC(rx_ring, rx_ring->next_to_clean);\n\t\tsize = le16_to_cpu(rx_desc->wb.upper.length);\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\trx_buffer = igb_get_rx_buffer(rx_ring, size, &rx_buf_pgcnt);\n\t\tpktbuf = page_address(rx_buffer->page) + rx_buffer->page_offset;\n\n\t\t \n\t\tif (igb_test_staterr(rx_desc, E1000_RXDADV_STAT_TSIP)) {\n\t\t\tint ts_hdr_len;\n\n\t\t\tts_hdr_len = igb_ptp_rx_pktstamp(rx_ring->q_vector,\n\t\t\t\t\t\t\t pktbuf, &timestamp);\n\n\t\t\tpkt_offset += ts_hdr_len;\n\t\t\tsize -= ts_hdr_len;\n\t\t}\n\n\t\t \n\t\tif (!skb) {\n\t\t\tunsigned char *hard_start = pktbuf - igb_rx_offset(rx_ring);\n\t\t\tunsigned int offset = pkt_offset + igb_rx_offset(rx_ring);\n\n\t\t\txdp_prepare_buff(&xdp, hard_start, offset, size, true);\n\t\t\txdp_buff_clear_frags_flag(&xdp);\n#if (PAGE_SIZE > 4096)\n\t\t\t \n\t\t\txdp.frame_sz = igb_rx_frame_truesize(rx_ring, size);\n#endif\n\t\t\tskb = igb_run_xdp(adapter, rx_ring, &xdp);\n\t\t}\n\n\t\tif (IS_ERR(skb)) {\n\t\t\tunsigned int xdp_res = -PTR_ERR(skb);\n\n\t\t\tif (xdp_res & (IGB_XDP_TX | IGB_XDP_REDIR)) {\n\t\t\t\txdp_xmit |= xdp_res;\n\t\t\t\tigb_rx_buffer_flip(rx_ring, rx_buffer, size);\n\t\t\t} else {\n\t\t\t\trx_buffer->pagecnt_bias++;\n\t\t\t}\n\t\t\ttotal_packets++;\n\t\t\ttotal_bytes += size;\n\t\t} else if (skb)\n\t\t\tigb_add_rx_frag(rx_ring, rx_buffer, skb, size);\n\t\telse if (ring_uses_build_skb(rx_ring))\n\t\t\tskb = igb_build_skb(rx_ring, rx_buffer, &xdp,\n\t\t\t\t\t    timestamp);\n\t\telse\n\t\t\tskb = igb_construct_skb(rx_ring, rx_buffer,\n\t\t\t\t\t\t&xdp, timestamp);\n\n\t\t \n\t\tif (!skb) {\n\t\t\trx_ring->rx_stats.alloc_failed++;\n\t\t\trx_buffer->pagecnt_bias++;\n\t\t\tbreak;\n\t\t}\n\n\t\tigb_put_rx_buffer(rx_ring, rx_buffer, rx_buf_pgcnt);\n\t\tcleaned_count++;\n\n\t\t \n\t\tif (igb_is_non_eop(rx_ring, rx_desc))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (igb_cleanup_headers(rx_ring, rx_desc, skb)) {\n\t\t\tskb = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\ttotal_bytes += skb->len;\n\n\t\t \n\t\tigb_process_skb_fields(rx_ring, rx_desc, skb);\n\n\t\tnapi_gro_receive(&q_vector->napi, skb);\n\n\t\t \n\t\tskb = NULL;\n\n\t\t \n\t\ttotal_packets++;\n\t}\n\n\t \n\trx_ring->skb = skb;\n\n\tif (xdp_xmit & IGB_XDP_REDIR)\n\t\txdp_do_flush();\n\n\tif (xdp_xmit & IGB_XDP_TX) {\n\t\tstruct igb_ring *tx_ring = igb_xdp_tx_queue_mapping(adapter);\n\n\t\tigb_xdp_ring_update_tail(tx_ring);\n\t}\n\n\tu64_stats_update_begin(&rx_ring->rx_syncp);\n\trx_ring->rx_stats.packets += total_packets;\n\trx_ring->rx_stats.bytes += total_bytes;\n\tu64_stats_update_end(&rx_ring->rx_syncp);\n\tq_vector->rx.total_packets += total_packets;\n\tq_vector->rx.total_bytes += total_bytes;\n\n\tif (cleaned_count)\n\t\tigb_alloc_rx_buffers(rx_ring, cleaned_count);\n\n\treturn total_packets;\n}\n\nstatic bool igb_alloc_mapped_page(struct igb_ring *rx_ring,\n\t\t\t\t  struct igb_rx_buffer *bi)\n{\n\tstruct page *page = bi->page;\n\tdma_addr_t dma;\n\n\t \n\tif (likely(page))\n\t\treturn true;\n\n\t \n\tpage = dev_alloc_pages(igb_rx_pg_order(rx_ring));\n\tif (unlikely(!page)) {\n\t\trx_ring->rx_stats.alloc_failed++;\n\t\treturn false;\n\t}\n\n\t \n\tdma = dma_map_page_attrs(rx_ring->dev, page, 0,\n\t\t\t\t igb_rx_pg_size(rx_ring),\n\t\t\t\t DMA_FROM_DEVICE,\n\t\t\t\t IGB_RX_DMA_ATTR);\n\n\t \n\tif (dma_mapping_error(rx_ring->dev, dma)) {\n\t\t__free_pages(page, igb_rx_pg_order(rx_ring));\n\n\t\trx_ring->rx_stats.alloc_failed++;\n\t\treturn false;\n\t}\n\n\tbi->dma = dma;\n\tbi->page = page;\n\tbi->page_offset = igb_rx_offset(rx_ring);\n\tpage_ref_add(page, USHRT_MAX - 1);\n\tbi->pagecnt_bias = USHRT_MAX;\n\n\treturn true;\n}\n\n \nvoid igb_alloc_rx_buffers(struct igb_ring *rx_ring, u16 cleaned_count)\n{\n\tunion e1000_adv_rx_desc *rx_desc;\n\tstruct igb_rx_buffer *bi;\n\tu16 i = rx_ring->next_to_use;\n\tu16 bufsz;\n\n\t \n\tif (!cleaned_count)\n\t\treturn;\n\n\trx_desc = IGB_RX_DESC(rx_ring, i);\n\tbi = &rx_ring->rx_buffer_info[i];\n\ti -= rx_ring->count;\n\n\tbufsz = igb_rx_bufsz(rx_ring);\n\n\tdo {\n\t\tif (!igb_alloc_mapped_page(rx_ring, bi))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_sync_single_range_for_device(rx_ring->dev, bi->dma,\n\t\t\t\t\t\t bi->page_offset, bufsz,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t \n\t\trx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);\n\n\t\trx_desc++;\n\t\tbi++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\trx_desc = IGB_RX_DESC(rx_ring, 0);\n\t\t\tbi = rx_ring->rx_buffer_info;\n\t\t\ti -= rx_ring->count;\n\t\t}\n\n\t\t \n\t\trx_desc->wb.upper.length = 0;\n\n\t\tcleaned_count--;\n\t} while (cleaned_count);\n\n\ti += rx_ring->count;\n\n\tif (rx_ring->next_to_use != i) {\n\t\t \n\t\trx_ring->next_to_use = i;\n\n\t\t \n\t\trx_ring->next_to_alloc = i;\n\n\t\t \n\t\tdma_wmb();\n\t\twritel(i, rx_ring->tail);\n\t}\n}\n\n \nstatic int igb_mii_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct mii_ioctl_data *data = if_mii(ifr);\n\n\tif (adapter->hw.phy.media_type != e1000_media_type_copper)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\t\tdata->phy_id = adapter->hw.phy.addr;\n\t\tbreak;\n\tcase SIOCGMIIREG:\n\t\tif (igb_read_phy_reg(&adapter->hw, data->reg_num & 0x1F,\n\t\t\t\t     &data->val_out))\n\t\t\treturn -EIO;\n\t\tbreak;\n\tcase SIOCSMIIREG:\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}\n\n \nstatic int igb_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)\n{\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\tcase SIOCGMIIREG:\n\tcase SIOCSMIIREG:\n\t\treturn igb_mii_ioctl(netdev, ifr, cmd);\n\tcase SIOCGHWTSTAMP:\n\t\treturn igb_ptp_get_ts_config(netdev, ifr);\n\tcase SIOCSHWTSTAMP:\n\t\treturn igb_ptp_set_ts_config(netdev, ifr);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nvoid igb_read_pci_cfg(struct e1000_hw *hw, u32 reg, u16 *value)\n{\n\tstruct igb_adapter *adapter = hw->back;\n\n\tpci_read_config_word(adapter->pdev, reg, value);\n}\n\nvoid igb_write_pci_cfg(struct e1000_hw *hw, u32 reg, u16 *value)\n{\n\tstruct igb_adapter *adapter = hw->back;\n\n\tpci_write_config_word(adapter->pdev, reg, *value);\n}\n\ns32 igb_read_pcie_cap_reg(struct e1000_hw *hw, u32 reg, u16 *value)\n{\n\tstruct igb_adapter *adapter = hw->back;\n\n\tif (pcie_capability_read_word(adapter->pdev, reg, value))\n\t\treturn -E1000_ERR_CONFIG;\n\n\treturn 0;\n}\n\ns32 igb_write_pcie_cap_reg(struct e1000_hw *hw, u32 reg, u16 *value)\n{\n\tstruct igb_adapter *adapter = hw->back;\n\n\tif (pcie_capability_write_word(adapter->pdev, reg, *value))\n\t\treturn -E1000_ERR_CONFIG;\n\n\treturn 0;\n}\n\nstatic void igb_vlan_mode(struct net_device *netdev, netdev_features_t features)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 ctrl, rctl;\n\tbool enable = !!(features & NETIF_F_HW_VLAN_CTAG_RX);\n\n\tif (enable) {\n\t\t \n\t\tctrl = rd32(E1000_CTRL);\n\t\tctrl |= E1000_CTRL_VME;\n\t\twr32(E1000_CTRL, ctrl);\n\n\t\t \n\t\trctl = rd32(E1000_RCTL);\n\t\trctl &= ~E1000_RCTL_CFIEN;\n\t\twr32(E1000_RCTL, rctl);\n\t} else {\n\t\t \n\t\tctrl = rd32(E1000_CTRL);\n\t\tctrl &= ~E1000_CTRL_VME;\n\t\twr32(E1000_CTRL, ctrl);\n\t}\n\n\tigb_set_vf_vlan_strip(adapter, adapter->vfs_allocated_count, enable);\n}\n\nstatic int igb_vlan_rx_add_vid(struct net_device *netdev,\n\t\t\t       __be16 proto, u16 vid)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint pf_id = adapter->vfs_allocated_count;\n\n\t \n\tif (!vid || !(adapter->flags & IGB_FLAG_VLAN_PROMISC))\n\t\tigb_vfta_set(hw, vid, pf_id, true, !!vid);\n\n\tset_bit(vid, adapter->active_vlans);\n\n\treturn 0;\n}\n\nstatic int igb_vlan_rx_kill_vid(struct net_device *netdev,\n\t\t\t\t__be16 proto, u16 vid)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tint pf_id = adapter->vfs_allocated_count;\n\tstruct e1000_hw *hw = &adapter->hw;\n\n\t \n\tif (vid && !(adapter->flags & IGB_FLAG_VLAN_PROMISC))\n\t\tigb_vfta_set(hw, vid, pf_id, false, true);\n\n\tclear_bit(vid, adapter->active_vlans);\n\n\treturn 0;\n}\n\nstatic void igb_restore_vlan(struct igb_adapter *adapter)\n{\n\tu16 vid = 1;\n\n\tigb_vlan_mode(adapter->netdev, adapter->netdev->features);\n\tigb_vlan_rx_add_vid(adapter->netdev, htons(ETH_P_8021Q), 0);\n\n\tfor_each_set_bit_from(vid, adapter->active_vlans, VLAN_N_VID)\n\t\tigb_vlan_rx_add_vid(adapter->netdev, htons(ETH_P_8021Q), vid);\n}\n\nint igb_set_spd_dplx(struct igb_adapter *adapter, u32 spd, u8 dplx)\n{\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct e1000_mac_info *mac = &adapter->hw.mac;\n\n\tmac->autoneg = 0;\n\n\t \n\tif ((spd & 1) || (dplx & ~1))\n\t\tgoto err_inval;\n\n\t \n\tif (adapter->hw.phy.media_type == e1000_media_type_internal_serdes) {\n\t\tswitch (spd + dplx) {\n\t\tcase SPEED_10 + DUPLEX_HALF:\n\t\tcase SPEED_10 + DUPLEX_FULL:\n\t\tcase SPEED_100 + DUPLEX_HALF:\n\t\t\tgoto err_inval;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tswitch (spd + dplx) {\n\tcase SPEED_10 + DUPLEX_HALF:\n\t\tmac->forced_speed_duplex = ADVERTISE_10_HALF;\n\t\tbreak;\n\tcase SPEED_10 + DUPLEX_FULL:\n\t\tmac->forced_speed_duplex = ADVERTISE_10_FULL;\n\t\tbreak;\n\tcase SPEED_100 + DUPLEX_HALF:\n\t\tmac->forced_speed_duplex = ADVERTISE_100_HALF;\n\t\tbreak;\n\tcase SPEED_100 + DUPLEX_FULL:\n\t\tmac->forced_speed_duplex = ADVERTISE_100_FULL;\n\t\tbreak;\n\tcase SPEED_1000 + DUPLEX_FULL:\n\t\tmac->autoneg = 1;\n\t\tadapter->hw.phy.autoneg_advertised = ADVERTISE_1000_FULL;\n\t\tbreak;\n\tcase SPEED_1000 + DUPLEX_HALF:  \n\tdefault:\n\t\tgoto err_inval;\n\t}\n\n\t \n\tadapter->hw.phy.mdix = AUTO_ALL_MODES;\n\n\treturn 0;\n\nerr_inval:\n\tdev_err(&pdev->dev, \"Unsupported Speed/Duplex configuration\\n\");\n\treturn -EINVAL;\n}\n\nstatic int __igb_shutdown(struct pci_dev *pdev, bool *enable_wake,\n\t\t\t  bool runtime)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 ctrl, rctl, status;\n\tu32 wufc = runtime ? E1000_WUFC_LNKC : adapter->wol;\n\tbool wake;\n\n\trtnl_lock();\n\tnetif_device_detach(netdev);\n\n\tif (netif_running(netdev))\n\t\t__igb_close(netdev, true);\n\n\tigb_ptp_suspend(adapter);\n\n\tigb_clear_interrupt_scheme(adapter);\n\trtnl_unlock();\n\n\tstatus = rd32(E1000_STATUS);\n\tif (status & E1000_STATUS_LU)\n\t\twufc &= ~E1000_WUFC_LNKC;\n\n\tif (wufc) {\n\t\tigb_setup_rctl(adapter);\n\t\tigb_set_rx_mode(netdev);\n\n\t\t \n\t\tif (wufc & E1000_WUFC_MC) {\n\t\t\trctl = rd32(E1000_RCTL);\n\t\t\trctl |= E1000_RCTL_MPE;\n\t\t\twr32(E1000_RCTL, rctl);\n\t\t}\n\n\t\tctrl = rd32(E1000_CTRL);\n\t\tctrl |= E1000_CTRL_ADVD3WUC;\n\t\twr32(E1000_CTRL, ctrl);\n\n\t\t \n\t\tigb_disable_pcie_master(hw);\n\n\t\twr32(E1000_WUC, E1000_WUC_PME_EN);\n\t\twr32(E1000_WUFC, wufc);\n\t} else {\n\t\twr32(E1000_WUC, 0);\n\t\twr32(E1000_WUFC, 0);\n\t}\n\n\twake = wufc || adapter->en_mng_pt;\n\tif (!wake)\n\t\tigb_power_down_link(adapter);\n\telse\n\t\tigb_power_up_link(adapter);\n\n\tif (enable_wake)\n\t\t*enable_wake = wake;\n\n\t \n\tigb_release_hw_control(adapter);\n\n\tpci_disable_device(pdev);\n\n\treturn 0;\n}\n\nstatic void igb_deliver_wake_packet(struct net_device *netdev)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tstruct sk_buff *skb;\n\tu32 wupl;\n\n\twupl = rd32(E1000_WUPL) & E1000_WUPL_MASK;\n\n\t \n\tif ((wupl == 0) || (wupl > E1000_WUPM_BYTES))\n\t\treturn;\n\n\tskb = netdev_alloc_skb_ip_align(netdev, E1000_WUPM_BYTES);\n\tif (!skb)\n\t\treturn;\n\n\tskb_put(skb, wupl);\n\n\t \n\twupl = roundup(wupl, 4);\n\n\tmemcpy_fromio(skb->data, hw->hw_addr + E1000_WUPM_REG(0), wupl);\n\n\tskb->protocol = eth_type_trans(skb, netdev);\n\tnetif_rx(skb);\n}\n\nstatic int __maybe_unused igb_suspend(struct device *dev)\n{\n\treturn __igb_shutdown(to_pci_dev(dev), NULL, 0);\n}\n\nstatic int __maybe_unused __igb_resume(struct device *dev, bool rpm)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 err, val;\n\n\tpci_set_power_state(pdev, PCI_D0);\n\tpci_restore_state(pdev);\n\tpci_save_state(pdev);\n\n\tif (!pci_device_is_present(pdev))\n\t\treturn -ENODEV;\n\terr = pci_enable_device_mem(pdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"igb: Cannot enable PCI device from suspend\\n\");\n\t\treturn err;\n\t}\n\tpci_set_master(pdev);\n\n\tpci_enable_wake(pdev, PCI_D3hot, 0);\n\tpci_enable_wake(pdev, PCI_D3cold, 0);\n\n\tif (igb_init_interrupt_scheme(adapter, true)) {\n\t\tdev_err(&pdev->dev, \"Unable to allocate memory for queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tigb_reset(adapter);\n\n\t \n\tigb_get_hw_control(adapter);\n\n\tval = rd32(E1000_WUS);\n\tif (val & WAKE_PKT_WUS)\n\t\tigb_deliver_wake_packet(netdev);\n\n\twr32(E1000_WUS, ~0);\n\n\tif (!rpm)\n\t\trtnl_lock();\n\tif (!err && netif_running(netdev))\n\t\terr = __igb_open(netdev, true);\n\n\tif (!err)\n\t\tnetif_device_attach(netdev);\n\tif (!rpm)\n\t\trtnl_unlock();\n\n\treturn err;\n}\n\nstatic int __maybe_unused igb_resume(struct device *dev)\n{\n\treturn __igb_resume(dev, false);\n}\n\nstatic int __maybe_unused igb_runtime_idle(struct device *dev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\tif (!igb_has_link(adapter))\n\t\tpm_schedule_suspend(dev, MSEC_PER_SEC * 5);\n\n\treturn -EBUSY;\n}\n\nstatic int __maybe_unused igb_runtime_suspend(struct device *dev)\n{\n\treturn __igb_shutdown(to_pci_dev(dev), NULL, 1);\n}\n\nstatic int __maybe_unused igb_runtime_resume(struct device *dev)\n{\n\treturn __igb_resume(dev, true);\n}\n\nstatic void igb_shutdown(struct pci_dev *pdev)\n{\n\tbool wake;\n\n\t__igb_shutdown(pdev, &wake, 0);\n\n\tif (system_state == SYSTEM_POWER_OFF) {\n\t\tpci_wake_from_d3(pdev, wake);\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n\t}\n}\n\nstatic int igb_pci_sriov_configure(struct pci_dev *dev, int num_vfs)\n{\n#ifdef CONFIG_PCI_IOV\n\tint err;\n\n\tif (num_vfs == 0) {\n\t\treturn igb_disable_sriov(dev, true);\n\t} else {\n\t\terr = igb_enable_sriov(dev, num_vfs, true);\n\t\treturn err ? err : num_vfs;\n\t}\n#endif\n\treturn 0;\n}\n\n \nstatic pci_ers_result_t igb_io_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t      pci_channel_state_t state)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\tif (state == pci_channel_io_normal) {\n\t\tdev_warn(&pdev->dev, \"Non-correctable non-fatal error reported.\\n\");\n\t\treturn PCI_ERS_RESULT_CAN_RECOVER;\n\t}\n\n\tnetif_device_detach(netdev);\n\n\tif (state == pci_channel_io_perm_failure)\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\tif (netif_running(netdev))\n\t\tigb_down(adapter);\n\tpci_disable_device(pdev);\n\n\t \n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\n \nstatic pci_ers_result_t igb_io_slot_reset(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tpci_ers_result_t result;\n\n\tif (pci_enable_device_mem(pdev)) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Cannot re-enable PCI device after reset.\\n\");\n\t\tresult = PCI_ERS_RESULT_DISCONNECT;\n\t} else {\n\t\tpci_set_master(pdev);\n\t\tpci_restore_state(pdev);\n\t\tpci_save_state(pdev);\n\n\t\tpci_enable_wake(pdev, PCI_D3hot, 0);\n\t\tpci_enable_wake(pdev, PCI_D3cold, 0);\n\n\t\t \n\t\thw->hw_addr = adapter->io_addr;\n\n\t\tigb_reset(adapter);\n\t\twr32(E1000_WUS, ~0);\n\t\tresult = PCI_ERS_RESULT_RECOVERED;\n\t}\n\n\treturn result;\n}\n\n \nstatic void igb_io_resume(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\tif (netif_running(netdev)) {\n\t\tif (igb_up(adapter)) {\n\t\t\tdev_err(&pdev->dev, \"igb_up failed after reset\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\tnetif_device_attach(netdev);\n\n\t \n\tigb_get_hw_control(adapter);\n}\n\n \nstatic void igb_rar_set_index(struct igb_adapter *adapter, u32 index)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 rar_low, rar_high;\n\tu8 *addr = adapter->mac_table[index].addr;\n\n\t \n\trar_low = le32_to_cpup((__le32 *)(addr));\n\trar_high = le16_to_cpup((__le16 *)(addr + 4));\n\n\t \n\tif (adapter->mac_table[index].state & IGB_MAC_STATE_IN_USE) {\n\t\tif (is_valid_ether_addr(addr))\n\t\t\trar_high |= E1000_RAH_AV;\n\n\t\tif (adapter->mac_table[index].state & IGB_MAC_STATE_SRC_ADDR)\n\t\t\trar_high |= E1000_RAH_ASEL_SRC_ADDR;\n\n\t\tswitch (hw->mac.type) {\n\t\tcase e1000_82575:\n\t\tcase e1000_i210:\n\t\t\tif (adapter->mac_table[index].state &\n\t\t\t    IGB_MAC_STATE_QUEUE_STEERING)\n\t\t\t\trar_high |= E1000_RAH_QSEL_ENABLE;\n\n\t\t\trar_high |= E1000_RAH_POOL_1 *\n\t\t\t\t    adapter->mac_table[index].queue;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\trar_high |= E1000_RAH_POOL_1 <<\n\t\t\t\t    adapter->mac_table[index].queue;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\twr32(E1000_RAL(index), rar_low);\n\twrfl();\n\twr32(E1000_RAH(index), rar_high);\n\twrfl();\n}\n\nstatic int igb_set_vf_mac(struct igb_adapter *adapter,\n\t\t\t  int vf, unsigned char *mac_addr)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\t \n\tint rar_entry = hw->mac.rar_entry_count - (vf + 1);\n\tunsigned char *vf_mac_addr = adapter->vf_data[vf].vf_mac_addresses;\n\n\tether_addr_copy(vf_mac_addr, mac_addr);\n\tether_addr_copy(adapter->mac_table[rar_entry].addr, mac_addr);\n\tadapter->mac_table[rar_entry].queue = vf;\n\tadapter->mac_table[rar_entry].state |= IGB_MAC_STATE_IN_USE;\n\tigb_rar_set_index(adapter, rar_entry);\n\n\treturn 0;\n}\n\nstatic int igb_ndo_set_vf_mac(struct net_device *netdev, int vf, u8 *mac)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\tif (vf >= adapter->vfs_allocated_count)\n\t\treturn -EINVAL;\n\n\t \n\tif (is_zero_ether_addr(mac)) {\n\t\tadapter->vf_data[vf].flags &= ~IGB_VF_FLAG_PF_SET_MAC;\n\t\tdev_info(&adapter->pdev->dev,\n\t\t\t \"remove administratively set MAC on VF %d\\n\",\n\t\t\t vf);\n\t} else if (is_valid_ether_addr(mac)) {\n\t\tadapter->vf_data[vf].flags |= IGB_VF_FLAG_PF_SET_MAC;\n\t\tdev_info(&adapter->pdev->dev, \"setting MAC %pM on VF %d\\n\",\n\t\t\t mac, vf);\n\t\tdev_info(&adapter->pdev->dev,\n\t\t\t \"Reload the VF driver to make this change effective.\");\n\t\t \n\t\tif (test_bit(__IGB_DOWN, &adapter->state)) {\n\t\t\tdev_warn(&adapter->pdev->dev,\n\t\t\t\t \"The VF MAC address has been set, but the PF device is not up.\\n\");\n\t\t\tdev_warn(&adapter->pdev->dev,\n\t\t\t\t \"Bring the PF device up before attempting to use the VF device.\\n\");\n\t\t}\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\treturn igb_set_vf_mac(adapter, vf, mac);\n}\n\nstatic int igb_link_mbps(int internal_link_speed)\n{\n\tswitch (internal_link_speed) {\n\tcase SPEED_100:\n\t\treturn 100;\n\tcase SPEED_1000:\n\t\treturn 1000;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic void igb_set_vf_rate_limit(struct e1000_hw *hw, int vf, int tx_rate,\n\t\t\t\t  int link_speed)\n{\n\tint rf_dec, rf_int;\n\tu32 bcnrc_val;\n\n\tif (tx_rate != 0) {\n\t\t \n\t\trf_int = link_speed / tx_rate;\n\t\trf_dec = (link_speed - (rf_int * tx_rate));\n\t\trf_dec = (rf_dec * BIT(E1000_RTTBCNRC_RF_INT_SHIFT)) /\n\t\t\t tx_rate;\n\n\t\tbcnrc_val = E1000_RTTBCNRC_RS_ENA;\n\t\tbcnrc_val |= ((rf_int << E1000_RTTBCNRC_RF_INT_SHIFT) &\n\t\t\t      E1000_RTTBCNRC_RF_INT_MASK);\n\t\tbcnrc_val |= (rf_dec & E1000_RTTBCNRC_RF_DEC_MASK);\n\t} else {\n\t\tbcnrc_val = 0;\n\t}\n\n\twr32(E1000_RTTDQSEL, vf);  \n\t \n\twr32(E1000_RTTBCNRM, 0x14);\n\twr32(E1000_RTTBCNRC, bcnrc_val);\n}\n\nstatic void igb_check_vf_rate_limit(struct igb_adapter *adapter)\n{\n\tint actual_link_speed, i;\n\tbool reset_rate = false;\n\n\t \n\tif ((adapter->vf_rate_link_speed == 0) ||\n\t    (adapter->hw.mac.type != e1000_82576))\n\t\treturn;\n\n\tactual_link_speed = igb_link_mbps(adapter->link_speed);\n\tif (actual_link_speed != adapter->vf_rate_link_speed) {\n\t\treset_rate = true;\n\t\tadapter->vf_rate_link_speed = 0;\n\t\tdev_info(&adapter->pdev->dev,\n\t\t\t \"Link speed has been changed. VF Transmit rate is disabled\\n\");\n\t}\n\n\tfor (i = 0; i < adapter->vfs_allocated_count; i++) {\n\t\tif (reset_rate)\n\t\t\tadapter->vf_data[i].tx_rate = 0;\n\n\t\tigb_set_vf_rate_limit(&adapter->hw, i,\n\t\t\t\t      adapter->vf_data[i].tx_rate,\n\t\t\t\t      actual_link_speed);\n\t}\n}\n\nstatic int igb_ndo_set_vf_bw(struct net_device *netdev, int vf,\n\t\t\t     int min_tx_rate, int max_tx_rate)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tint actual_link_speed;\n\n\tif (hw->mac.type != e1000_82576)\n\t\treturn -EOPNOTSUPP;\n\n\tif (min_tx_rate)\n\t\treturn -EINVAL;\n\n\tactual_link_speed = igb_link_mbps(adapter->link_speed);\n\tif ((vf >= adapter->vfs_allocated_count) ||\n\t    (!(rd32(E1000_STATUS) & E1000_STATUS_LU)) ||\n\t    (max_tx_rate < 0) ||\n\t    (max_tx_rate > actual_link_speed))\n\t\treturn -EINVAL;\n\n\tadapter->vf_rate_link_speed = actual_link_speed;\n\tadapter->vf_data[vf].tx_rate = (u16)max_tx_rate;\n\tigb_set_vf_rate_limit(hw, vf, max_tx_rate, actual_link_speed);\n\n\treturn 0;\n}\n\nstatic int igb_ndo_set_vf_spoofchk(struct net_device *netdev, int vf,\n\t\t\t\t   bool setting)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 reg_val, reg_offset;\n\n\tif (!adapter->vfs_allocated_count)\n\t\treturn -EOPNOTSUPP;\n\n\tif (vf >= adapter->vfs_allocated_count)\n\t\treturn -EINVAL;\n\n\treg_offset = (hw->mac.type == e1000_82576) ? E1000_DTXSWC : E1000_TXSWC;\n\treg_val = rd32(reg_offset);\n\tif (setting)\n\t\treg_val |= (BIT(vf) |\n\t\t\t    BIT(vf + E1000_DTXSWC_VLAN_SPOOF_SHIFT));\n\telse\n\t\treg_val &= ~(BIT(vf) |\n\t\t\t     BIT(vf + E1000_DTXSWC_VLAN_SPOOF_SHIFT));\n\twr32(reg_offset, reg_val);\n\n\tadapter->vf_data[vf].spoofchk_enabled = setting;\n\treturn 0;\n}\n\nstatic int igb_ndo_set_vf_trust(struct net_device *netdev, int vf, bool setting)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\n\tif (vf >= adapter->vfs_allocated_count)\n\t\treturn -EINVAL;\n\tif (adapter->vf_data[vf].trusted == setting)\n\t\treturn 0;\n\n\tadapter->vf_data[vf].trusted = setting;\n\n\tdev_info(&adapter->pdev->dev, \"VF %u is %strusted\\n\",\n\t\t vf, setting ? \"\" : \"not \");\n\treturn 0;\n}\n\nstatic int igb_ndo_get_vf_config(struct net_device *netdev,\n\t\t\t\t int vf, struct ifla_vf_info *ivi)\n{\n\tstruct igb_adapter *adapter = netdev_priv(netdev);\n\tif (vf >= adapter->vfs_allocated_count)\n\t\treturn -EINVAL;\n\tivi->vf = vf;\n\tmemcpy(&ivi->mac, adapter->vf_data[vf].vf_mac_addresses, ETH_ALEN);\n\tivi->max_tx_rate = adapter->vf_data[vf].tx_rate;\n\tivi->min_tx_rate = 0;\n\tivi->vlan = adapter->vf_data[vf].pf_vlan;\n\tivi->qos = adapter->vf_data[vf].pf_qos;\n\tivi->spoofchk = adapter->vf_data[vf].spoofchk_enabled;\n\tivi->trusted = adapter->vf_data[vf].trusted;\n\treturn 0;\n}\n\nstatic void igb_vmm_control(struct igb_adapter *adapter)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 reg;\n\n\tswitch (hw->mac.type) {\n\tcase e1000_82575:\n\tcase e1000_i210:\n\tcase e1000_i211:\n\tcase e1000_i354:\n\tdefault:\n\t\t \n\t\treturn;\n\tcase e1000_82576:\n\t\t \n\t\treg = rd32(E1000_DTXCTL);\n\t\treg |= E1000_DTXCTL_VLAN_ADDED;\n\t\twr32(E1000_DTXCTL, reg);\n\t\tfallthrough;\n\tcase e1000_82580:\n\t\t \n\t\treg = rd32(E1000_RPLOLR);\n\t\treg |= E1000_RPLOLR_STRVLAN;\n\t\twr32(E1000_RPLOLR, reg);\n\t\tfallthrough;\n\tcase e1000_i350:\n\t\t \n\t\tbreak;\n\t}\n\n\tif (adapter->vfs_allocated_count) {\n\t\tigb_vmdq_set_loopback_pf(hw, true);\n\t\tigb_vmdq_set_replication_pf(hw, true);\n\t\tigb_vmdq_set_anti_spoofing_pf(hw, true,\n\t\t\t\t\t      adapter->vfs_allocated_count);\n\t} else {\n\t\tigb_vmdq_set_loopback_pf(hw, false);\n\t\tigb_vmdq_set_replication_pf(hw, false);\n\t}\n}\n\nstatic void igb_init_dmac(struct igb_adapter *adapter, u32 pba)\n{\n\tstruct e1000_hw *hw = &adapter->hw;\n\tu32 dmac_thr;\n\tu16 hwm;\n\tu32 reg;\n\n\tif (hw->mac.type > e1000_82580) {\n\t\tif (adapter->flags & IGB_FLAG_DMAC) {\n\t\t\t \n\t\t\twr32(E1000_DMCTXTH, 0);\n\n\t\t\t \n\t\t\thwm = 64 * (pba - 6);\n\t\t\treg = rd32(E1000_FCRTC);\n\t\t\treg &= ~E1000_FCRTC_RTH_COAL_MASK;\n\t\t\treg |= ((hwm << E1000_FCRTC_RTH_COAL_SHIFT)\n\t\t\t\t& E1000_FCRTC_RTH_COAL_MASK);\n\t\t\twr32(E1000_FCRTC, reg);\n\n\t\t\t \n\t\t\tdmac_thr = pba - 10;\n\t\t\treg = rd32(E1000_DMACR);\n\t\t\treg &= ~E1000_DMACR_DMACTHR_MASK;\n\t\t\treg |= ((dmac_thr << E1000_DMACR_DMACTHR_SHIFT)\n\t\t\t\t& E1000_DMACR_DMACTHR_MASK);\n\n\t\t\t \n\t\t\treg |= (E1000_DMACR_DMAC_EN | E1000_DMACR_DMAC_LX_MASK);\n\n\t\t\t \n\t\t\treg |= (1000 >> 5);\n\n\t\t\t \n\t\t\tif (hw->mac.type != e1000_i354)\n\t\t\t\treg &= ~E1000_DMACR_DC_BMC2OSW_EN;\n\t\t\twr32(E1000_DMACR, reg);\n\n\t\t\t \n\t\t\twr32(E1000_DMCRTRH, 0);\n\n\t\t\treg = (IGB_DMCTLX_DCFLUSH_DIS | 0x4);\n\n\t\t\twr32(E1000_DMCTLX, reg);\n\n\t\t\t \n\t\t\twr32(E1000_DMCTXTH, (IGB_MIN_TXPBSIZE -\n\t\t\t     (IGB_TX_BUF_4096 + adapter->max_frame_size)) >> 6);\n\t\t}\n\n\t\tif (hw->mac.type >= e1000_i210 ||\n\t\t    (adapter->flags & IGB_FLAG_DMAC)) {\n\t\t\treg = rd32(E1000_PCIEMISC);\n\t\t\treg |= E1000_PCIEMISC_LX_DECISION;\n\t\t\twr32(E1000_PCIEMISC, reg);\n\t\t}  \n\t} else if (hw->mac.type == e1000_82580) {\n\t\tu32 reg = rd32(E1000_PCIEMISC);\n\n\t\twr32(E1000_PCIEMISC, reg & ~E1000_PCIEMISC_LX_DECISION);\n\t\twr32(E1000_DMACR, 0);\n\t}\n}\n\n \ns32 igb_read_i2c_byte(struct e1000_hw *hw, u8 byte_offset,\n\t\t      u8 dev_addr, u8 *data)\n{\n\tstruct igb_adapter *adapter = container_of(hw, struct igb_adapter, hw);\n\tstruct i2c_client *this_client = adapter->i2c_client;\n\ts32 status;\n\tu16 swfw_mask = 0;\n\n\tif (!this_client)\n\t\treturn E1000_ERR_I2C;\n\n\tswfw_mask = E1000_SWFW_PHY0_SM;\n\n\tif (hw->mac.ops.acquire_swfw_sync(hw, swfw_mask))\n\t\treturn E1000_ERR_SWFW_SYNC;\n\n\tstatus = i2c_smbus_read_byte_data(this_client, byte_offset);\n\thw->mac.ops.release_swfw_sync(hw, swfw_mask);\n\n\tif (status < 0)\n\t\treturn E1000_ERR_I2C;\n\telse {\n\t\t*data = status;\n\t\treturn 0;\n\t}\n}\n\n \ns32 igb_write_i2c_byte(struct e1000_hw *hw, u8 byte_offset,\n\t\t       u8 dev_addr, u8 data)\n{\n\tstruct igb_adapter *adapter = container_of(hw, struct igb_adapter, hw);\n\tstruct i2c_client *this_client = adapter->i2c_client;\n\ts32 status;\n\tu16 swfw_mask = E1000_SWFW_PHY0_SM;\n\n\tif (!this_client)\n\t\treturn E1000_ERR_I2C;\n\n\tif (hw->mac.ops.acquire_swfw_sync(hw, swfw_mask))\n\t\treturn E1000_ERR_SWFW_SYNC;\n\tstatus = i2c_smbus_write_byte_data(this_client, byte_offset, data);\n\thw->mac.ops.release_swfw_sync(hw, swfw_mask);\n\n\tif (status)\n\t\treturn E1000_ERR_I2C;\n\telse\n\t\treturn 0;\n\n}\n\nint igb_reinit_queues(struct igb_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tint err = 0;\n\n\tif (netif_running(netdev))\n\t\tigb_close(netdev);\n\n\tigb_reset_interrupt_capability(adapter);\n\n\tif (igb_init_interrupt_scheme(adapter, true)) {\n\t\tdev_err(&pdev->dev, \"Unable to allocate memory for queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (netif_running(netdev))\n\t\terr = igb_open(netdev);\n\n\treturn err;\n}\n\nstatic void igb_nfc_filter_exit(struct igb_adapter *adapter)\n{\n\tstruct igb_nfc_filter *rule;\n\n\tspin_lock(&adapter->nfc_lock);\n\n\thlist_for_each_entry(rule, &adapter->nfc_filter_list, nfc_node)\n\t\tigb_erase_filter(adapter, rule);\n\n\thlist_for_each_entry(rule, &adapter->cls_flower_list, nfc_node)\n\t\tigb_erase_filter(adapter, rule);\n\n\tspin_unlock(&adapter->nfc_lock);\n}\n\nstatic void igb_nfc_filter_restore(struct igb_adapter *adapter)\n{\n\tstruct igb_nfc_filter *rule;\n\n\tspin_lock(&adapter->nfc_lock);\n\n\thlist_for_each_entry(rule, &adapter->nfc_filter_list, nfc_node)\n\t\tigb_add_filter(adapter, rule);\n\n\tspin_unlock(&adapter->nfc_lock);\n}\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}