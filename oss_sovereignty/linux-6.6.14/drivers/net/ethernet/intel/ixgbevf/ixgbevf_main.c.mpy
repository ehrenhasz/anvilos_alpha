{
  "module_name": "ixgbevf_main.c",
  "hash_id": "bc7c31b0dcecd610c48bc08901e2b8063742878ad9a9b92cb597b86b1327c6e1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/types.h>\n#include <linux/bitops.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/vmalloc.h>\n#include <linux/string.h>\n#include <linux/in.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/sctp.h>\n#include <linux/ipv6.h>\n#include <linux/slab.h>\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n#include <linux/ethtool.h>\n#include <linux/if.h>\n#include <linux/if_vlan.h>\n#include <linux/prefetch.h>\n#include <net/mpls.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <linux/atomic.h>\n#include <net/xfrm.h>\n\n#include \"ixgbevf.h\"\n\nconst char ixgbevf_driver_name[] = \"ixgbevf\";\nstatic const char ixgbevf_driver_string[] =\n\t\"Intel(R) 10 Gigabit PCI Express Virtual Function Network Driver\";\n\nstatic char ixgbevf_copyright[] =\n\t\"Copyright (c) 2009 - 2018 Intel Corporation.\";\n\nstatic const struct ixgbevf_info *ixgbevf_info_tbl[] = {\n\t[board_82599_vf]\t= &ixgbevf_82599_vf_info,\n\t[board_82599_vf_hv]\t= &ixgbevf_82599_vf_hv_info,\n\t[board_X540_vf]\t\t= &ixgbevf_X540_vf_info,\n\t[board_X540_vf_hv]\t= &ixgbevf_X540_vf_hv_info,\n\t[board_X550_vf]\t\t= &ixgbevf_X550_vf_info,\n\t[board_X550_vf_hv]\t= &ixgbevf_X550_vf_hv_info,\n\t[board_X550EM_x_vf]\t= &ixgbevf_X550EM_x_vf_info,\n\t[board_X550EM_x_vf_hv]\t= &ixgbevf_X550EM_x_vf_hv_info,\n\t[board_x550em_a_vf]\t= &ixgbevf_x550em_a_vf_info,\n};\n\n \nstatic const struct pci_device_id ixgbevf_pci_tbl[] = {\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_VF), board_82599_vf },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_VF_HV), board_82599_vf_hv },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X540_VF), board_X540_vf },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X540_VF_HV), board_X540_vf_hv },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550_VF), board_X550_vf },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550_VF_HV), board_X550_vf_hv },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_X_VF), board_X550EM_x_vf },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_X_VF_HV), board_X550EM_x_vf_hv},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_VF), board_x550em_a_vf },\n\t \n\t{0, }\n};\nMODULE_DEVICE_TABLE(pci, ixgbevf_pci_tbl);\n\nMODULE_AUTHOR(\"Intel Corporation, <linux.nics@intel.com>\");\nMODULE_DESCRIPTION(\"Intel(R) 10 Gigabit Virtual Function Network Driver\");\nMODULE_LICENSE(\"GPL v2\");\n\n#define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV|NETIF_MSG_PROBE|NETIF_MSG_LINK)\nstatic int debug = -1;\nmodule_param(debug, int, 0);\nMODULE_PARM_DESC(debug, \"Debug level (0=none,...,16=all)\");\n\nstatic struct workqueue_struct *ixgbevf_wq;\n\nstatic void ixgbevf_service_event_schedule(struct ixgbevf_adapter *adapter)\n{\n\tif (!test_bit(__IXGBEVF_DOWN, &adapter->state) &&\n\t    !test_bit(__IXGBEVF_REMOVING, &adapter->state) &&\n\t    !test_and_set_bit(__IXGBEVF_SERVICE_SCHED, &adapter->state))\n\t\tqueue_work(ixgbevf_wq, &adapter->service_task);\n}\n\nstatic void ixgbevf_service_event_complete(struct ixgbevf_adapter *adapter)\n{\n\tBUG_ON(!test_bit(__IXGBEVF_SERVICE_SCHED, &adapter->state));\n\n\t \n\tsmp_mb__before_atomic();\n\tclear_bit(__IXGBEVF_SERVICE_SCHED, &adapter->state);\n}\n\n \nstatic void ixgbevf_queue_reset_subtask(struct ixgbevf_adapter *adapter);\nstatic void ixgbevf_set_itr(struct ixgbevf_q_vector *q_vector);\nstatic void ixgbevf_free_all_rx_resources(struct ixgbevf_adapter *adapter);\nstatic bool ixgbevf_can_reuse_rx_page(struct ixgbevf_rx_buffer *rx_buffer);\nstatic void ixgbevf_reuse_rx_page(struct ixgbevf_ring *rx_ring,\n\t\t\t\t  struct ixgbevf_rx_buffer *old_buff);\n\nstatic void ixgbevf_remove_adapter(struct ixgbe_hw *hw)\n{\n\tstruct ixgbevf_adapter *adapter = hw->back;\n\n\tif (!hw->hw_addr)\n\t\treturn;\n\thw->hw_addr = NULL;\n\tdev_err(&adapter->pdev->dev, \"Adapter removed\\n\");\n\tif (test_bit(__IXGBEVF_SERVICE_INITED, &adapter->state))\n\t\tixgbevf_service_event_schedule(adapter);\n}\n\nstatic void ixgbevf_check_remove(struct ixgbe_hw *hw, u32 reg)\n{\n\tu32 value;\n\n\t \n\tif (reg == IXGBE_VFSTATUS) {\n\t\tixgbevf_remove_adapter(hw);\n\t\treturn;\n\t}\n\tvalue = ixgbevf_read_reg(hw, IXGBE_VFSTATUS);\n\tif (value == IXGBE_FAILED_READ_REG)\n\t\tixgbevf_remove_adapter(hw);\n}\n\nu32 ixgbevf_read_reg(struct ixgbe_hw *hw, u32 reg)\n{\n\tu8 __iomem *reg_addr = READ_ONCE(hw->hw_addr);\n\tu32 value;\n\n\tif (IXGBE_REMOVED(reg_addr))\n\t\treturn IXGBE_FAILED_READ_REG;\n\tvalue = readl(reg_addr + reg);\n\tif (unlikely(value == IXGBE_FAILED_READ_REG))\n\t\tixgbevf_check_remove(hw, reg);\n\treturn value;\n}\n\n \nstatic void ixgbevf_set_ivar(struct ixgbevf_adapter *adapter, s8 direction,\n\t\t\t     u8 queue, u8 msix_vector)\n{\n\tu32 ivar, index;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tif (direction == -1) {\n\t\t \n\t\tmsix_vector |= IXGBE_IVAR_ALLOC_VAL;\n\t\tivar = IXGBE_READ_REG(hw, IXGBE_VTIVAR_MISC);\n\t\tivar &= ~0xFF;\n\t\tivar |= msix_vector;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VTIVAR_MISC, ivar);\n\t} else {\n\t\t \n\t\tmsix_vector |= IXGBE_IVAR_ALLOC_VAL;\n\t\tindex = ((16 * (queue & 1)) + (8 * direction));\n\t\tivar = IXGBE_READ_REG(hw, IXGBE_VTIVAR(queue >> 1));\n\t\tivar &= ~(0xFF << index);\n\t\tivar |= (msix_vector << index);\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VTIVAR(queue >> 1), ivar);\n\t}\n}\n\nstatic u64 ixgbevf_get_tx_completed(struct ixgbevf_ring *ring)\n{\n\treturn ring->stats.packets;\n}\n\nstatic u32 ixgbevf_get_tx_pending(struct ixgbevf_ring *ring)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(ring->netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tu32 head = IXGBE_READ_REG(hw, IXGBE_VFTDH(ring->reg_idx));\n\tu32 tail = IXGBE_READ_REG(hw, IXGBE_VFTDT(ring->reg_idx));\n\n\tif (head != tail)\n\t\treturn (head < tail) ?\n\t\t\ttail - head : (tail + ring->count - head);\n\n\treturn 0;\n}\n\nstatic inline bool ixgbevf_check_tx_hang(struct ixgbevf_ring *tx_ring)\n{\n\tu32 tx_done = ixgbevf_get_tx_completed(tx_ring);\n\tu32 tx_done_old = tx_ring->tx_stats.tx_done_old;\n\tu32 tx_pending = ixgbevf_get_tx_pending(tx_ring);\n\n\tclear_check_for_tx_hang(tx_ring);\n\n\t \n\tif ((tx_done_old == tx_done) && tx_pending) {\n\t\t \n\t\treturn test_and_set_bit(__IXGBEVF_HANG_CHECK_ARMED,\n\t\t\t\t\t&tx_ring->state);\n\t}\n\t \n\tclear_bit(__IXGBEVF_HANG_CHECK_ARMED, &tx_ring->state);\n\n\t \n\ttx_ring->tx_stats.tx_done_old = tx_done;\n\n\treturn false;\n}\n\nstatic void ixgbevf_tx_timeout_reset(struct ixgbevf_adapter *adapter)\n{\n\t \n\tif (!test_bit(__IXGBEVF_DOWN, &adapter->state)) {\n\t\tset_bit(__IXGBEVF_RESET_REQUESTED, &adapter->state);\n\t\tixgbevf_service_event_schedule(adapter);\n\t}\n}\n\n \nstatic void ixgbevf_tx_timeout(struct net_device *netdev, unsigned int __always_unused txqueue)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\n\tixgbevf_tx_timeout_reset(adapter);\n}\n\n \nstatic bool ixgbevf_clean_tx_irq(struct ixgbevf_q_vector *q_vector,\n\t\t\t\t struct ixgbevf_ring *tx_ring, int napi_budget)\n{\n\tstruct ixgbevf_adapter *adapter = q_vector->adapter;\n\tstruct ixgbevf_tx_buffer *tx_buffer;\n\tunion ixgbe_adv_tx_desc *tx_desc;\n\tunsigned int total_bytes = 0, total_packets = 0, total_ipsec = 0;\n\tunsigned int budget = tx_ring->count / 2;\n\tunsigned int i = tx_ring->next_to_clean;\n\n\tif (test_bit(__IXGBEVF_DOWN, &adapter->state))\n\t\treturn true;\n\n\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\ttx_desc = IXGBEVF_TX_DESC(tx_ring, i);\n\ti -= tx_ring->count;\n\n\tdo {\n\t\tunion ixgbe_adv_tx_desc *eop_desc = tx_buffer->next_to_watch;\n\n\t\t \n\t\tif (!eop_desc)\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\t \n\t\tif (!(eop_desc->wb.status & cpu_to_le32(IXGBE_TXD_STAT_DD)))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buffer->next_to_watch = NULL;\n\n\t\t \n\t\ttotal_bytes += tx_buffer->bytecount;\n\t\ttotal_packets += tx_buffer->gso_segs;\n\t\tif (tx_buffer->tx_flags & IXGBE_TX_FLAGS_IPSEC)\n\t\t\ttotal_ipsec++;\n\n\t\t \n\t\tif (ring_is_xdp(tx_ring))\n\t\t\tpage_frag_free(tx_buffer->data);\n\t\telse\n\t\t\tnapi_consume_skb(tx_buffer->skb, napi_budget);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(!i)) {\n\t\t\t\ti -= tx_ring->count;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = IXGBEVF_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len)) {\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\ttx_buffer++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\ttx_desc = IXGBEVF_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\t \n\t\tprefetch(tx_desc);\n\n\t\t \n\t\tbudget--;\n\t} while (likely(budget));\n\n\ti += tx_ring->count;\n\ttx_ring->next_to_clean = i;\n\tu64_stats_update_begin(&tx_ring->syncp);\n\ttx_ring->stats.bytes += total_bytes;\n\ttx_ring->stats.packets += total_packets;\n\tu64_stats_update_end(&tx_ring->syncp);\n\tq_vector->tx.total_bytes += total_bytes;\n\tq_vector->tx.total_packets += total_packets;\n\tadapter->tx_ipsec += total_ipsec;\n\n\tif (check_for_tx_hang(tx_ring) && ixgbevf_check_tx_hang(tx_ring)) {\n\t\tstruct ixgbe_hw *hw = &adapter->hw;\n\t\tunion ixgbe_adv_tx_desc *eop_desc;\n\n\t\teop_desc = tx_ring->tx_buffer_info[i].next_to_watch;\n\n\t\tpr_err(\"Detected Tx Unit Hang%s\\n\"\n\t\t       \"  Tx Queue             <%d>\\n\"\n\t\t       \"  TDH, TDT             <%x>, <%x>\\n\"\n\t\t       \"  next_to_use          <%x>\\n\"\n\t\t       \"  next_to_clean        <%x>\\n\"\n\t\t       \"tx_buffer_info[next_to_clean]\\n\"\n\t\t       \"  next_to_watch        <%p>\\n\"\n\t\t       \"  eop_desc->wb.status  <%x>\\n\"\n\t\t       \"  time_stamp           <%lx>\\n\"\n\t\t       \"  jiffies              <%lx>\\n\",\n\t\t       ring_is_xdp(tx_ring) ? \" XDP\" : \"\",\n\t\t       tx_ring->queue_index,\n\t\t       IXGBE_READ_REG(hw, IXGBE_VFTDH(tx_ring->reg_idx)),\n\t\t       IXGBE_READ_REG(hw, IXGBE_VFTDT(tx_ring->reg_idx)),\n\t\t       tx_ring->next_to_use, i,\n\t\t       eop_desc, (eop_desc ? eop_desc->wb.status : 0),\n\t\t       tx_ring->tx_buffer_info[i].time_stamp, jiffies);\n\n\t\tif (!ring_is_xdp(tx_ring))\n\t\t\tnetif_stop_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\n\t\t \n\t\tixgbevf_tx_timeout_reset(adapter);\n\n\t\treturn true;\n\t}\n\n\tif (ring_is_xdp(tx_ring))\n\t\treturn !!budget;\n\n#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)\n\tif (unlikely(total_packets && netif_carrier_ok(tx_ring->netdev) &&\n\t\t     (ixgbevf_desc_unused(tx_ring) >= TX_WAKE_THRESHOLD))) {\n\t\t \n\t\tsmp_mb();\n\n\t\tif (__netif_subqueue_stopped(tx_ring->netdev,\n\t\t\t\t\t     tx_ring->queue_index) &&\n\t\t    !test_bit(__IXGBEVF_DOWN, &adapter->state)) {\n\t\t\tnetif_wake_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\t\t\t++tx_ring->tx_stats.restart_queue;\n\t\t}\n\t}\n\n\treturn !!budget;\n}\n\n \nstatic void ixgbevf_rx_skb(struct ixgbevf_q_vector *q_vector,\n\t\t\t   struct sk_buff *skb)\n{\n\tnapi_gro_receive(&q_vector->napi, skb);\n}\n\n#define IXGBE_RSS_L4_TYPES_MASK \\\n\t((1ul << IXGBE_RXDADV_RSSTYPE_IPV4_TCP) | \\\n\t (1ul << IXGBE_RXDADV_RSSTYPE_IPV4_UDP) | \\\n\t (1ul << IXGBE_RXDADV_RSSTYPE_IPV6_TCP) | \\\n\t (1ul << IXGBE_RXDADV_RSSTYPE_IPV6_UDP))\n\nstatic inline void ixgbevf_rx_hash(struct ixgbevf_ring *ring,\n\t\t\t\t   union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tu16 rss_type;\n\n\tif (!(ring->netdev->features & NETIF_F_RXHASH))\n\t\treturn;\n\n\trss_type = le16_to_cpu(rx_desc->wb.lower.lo_dword.hs_rss.pkt_info) &\n\t\t   IXGBE_RXDADV_RSSTYPE_MASK;\n\n\tif (!rss_type)\n\t\treturn;\n\n\tskb_set_hash(skb, le32_to_cpu(rx_desc->wb.lower.hi_dword.rss),\n\t\t     (IXGBE_RSS_L4_TYPES_MASK & (1ul << rss_type)) ?\n\t\t     PKT_HASH_TYPE_L4 : PKT_HASH_TYPE_L3);\n}\n\n \nstatic inline void ixgbevf_rx_checksum(struct ixgbevf_ring *ring,\n\t\t\t\t       union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tskb_checksum_none_assert(skb);\n\n\t \n\tif (!(ring->netdev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\t \n\tif (ixgbevf_test_staterr(rx_desc, IXGBE_RXD_STAT_IPCS) &&\n\t    ixgbevf_test_staterr(rx_desc, IXGBE_RXDADV_ERR_IPE)) {\n\t\tring->rx_stats.csum_err++;\n\t\treturn;\n\t}\n\n\tif (!ixgbevf_test_staterr(rx_desc, IXGBE_RXD_STAT_L4CS))\n\t\treturn;\n\n\tif (ixgbevf_test_staterr(rx_desc, IXGBE_RXDADV_ERR_TCPE)) {\n\t\tring->rx_stats.csum_err++;\n\t\treturn;\n\t}\n\n\t \n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n}\n\n \nstatic void ixgbevf_process_skb_fields(struct ixgbevf_ring *rx_ring,\n\t\t\t\t       union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tixgbevf_rx_hash(rx_ring, rx_desc, skb);\n\tixgbevf_rx_checksum(rx_ring, rx_desc, skb);\n\n\tif (ixgbevf_test_staterr(rx_desc, IXGBE_RXD_STAT_VP)) {\n\t\tu16 vid = le16_to_cpu(rx_desc->wb.upper.vlan);\n\t\tunsigned long *active_vlans = netdev_priv(rx_ring->netdev);\n\n\t\tif (test_bit(vid & VLAN_VID_MASK, active_vlans))\n\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);\n\t}\n\n\tif (ixgbevf_test_staterr(rx_desc, IXGBE_RXDADV_STAT_SECP))\n\t\tixgbevf_ipsec_rx(rx_ring, rx_desc, skb);\n\n\tskb->protocol = eth_type_trans(skb, rx_ring->netdev);\n}\n\nstatic\nstruct ixgbevf_rx_buffer *ixgbevf_get_rx_buffer(struct ixgbevf_ring *rx_ring,\n\t\t\t\t\t\tconst unsigned int size)\n{\n\tstruct ixgbevf_rx_buffer *rx_buffer;\n\n\trx_buffer = &rx_ring->rx_buffer_info[rx_ring->next_to_clean];\n\tprefetchw(rx_buffer->page);\n\n\t \n\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t      rx_buffer->dma,\n\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t      size,\n\t\t\t\t      DMA_FROM_DEVICE);\n\n\trx_buffer->pagecnt_bias--;\n\n\treturn rx_buffer;\n}\n\nstatic void ixgbevf_put_rx_buffer(struct ixgbevf_ring *rx_ring,\n\t\t\t\t  struct ixgbevf_rx_buffer *rx_buffer,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tif (ixgbevf_can_reuse_rx_page(rx_buffer)) {\n\t\t \n\t\tixgbevf_reuse_rx_page(rx_ring, rx_buffer);\n\t} else {\n\t\tif (IS_ERR(skb))\n\t\t\t \n\t\t\tdma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,\n\t\t\t\t\t     ixgbevf_rx_pg_size(rx_ring),\n\t\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t\t     IXGBEVF_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(rx_buffer->page,\n\t\t\t\t\trx_buffer->pagecnt_bias);\n\t}\n\n\t \n\trx_buffer->page = NULL;\n}\n\n \nstatic bool ixgbevf_is_non_eop(struct ixgbevf_ring *rx_ring,\n\t\t\t       union ixgbe_adv_rx_desc *rx_desc)\n{\n\tu32 ntc = rx_ring->next_to_clean + 1;\n\n\t \n\tntc = (ntc < rx_ring->count) ? ntc : 0;\n\trx_ring->next_to_clean = ntc;\n\n\tprefetch(IXGBEVF_RX_DESC(rx_ring, ntc));\n\n\tif (likely(ixgbevf_test_staterr(rx_desc, IXGBE_RXD_STAT_EOP)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic inline unsigned int ixgbevf_rx_offset(struct ixgbevf_ring *rx_ring)\n{\n\treturn ring_uses_build_skb(rx_ring) ? IXGBEVF_SKB_PAD : 0;\n}\n\nstatic bool ixgbevf_alloc_mapped_page(struct ixgbevf_ring *rx_ring,\n\t\t\t\t      struct ixgbevf_rx_buffer *bi)\n{\n\tstruct page *page = bi->page;\n\tdma_addr_t dma;\n\n\t \n\tif (likely(page))\n\t\treturn true;\n\n\t \n\tpage = dev_alloc_pages(ixgbevf_rx_pg_order(rx_ring));\n\tif (unlikely(!page)) {\n\t\trx_ring->rx_stats.alloc_rx_page_failed++;\n\t\treturn false;\n\t}\n\n\t \n\tdma = dma_map_page_attrs(rx_ring->dev, page, 0,\n\t\t\t\t ixgbevf_rx_pg_size(rx_ring),\n\t\t\t\t DMA_FROM_DEVICE, IXGBEVF_RX_DMA_ATTR);\n\n\t \n\tif (dma_mapping_error(rx_ring->dev, dma)) {\n\t\t__free_pages(page, ixgbevf_rx_pg_order(rx_ring));\n\n\t\trx_ring->rx_stats.alloc_rx_page_failed++;\n\t\treturn false;\n\t}\n\n\tbi->dma = dma;\n\tbi->page = page;\n\tbi->page_offset = ixgbevf_rx_offset(rx_ring);\n\tbi->pagecnt_bias = 1;\n\trx_ring->rx_stats.alloc_rx_page++;\n\n\treturn true;\n}\n\n \nstatic void ixgbevf_alloc_rx_buffers(struct ixgbevf_ring *rx_ring,\n\t\t\t\t     u16 cleaned_count)\n{\n\tunion ixgbe_adv_rx_desc *rx_desc;\n\tstruct ixgbevf_rx_buffer *bi;\n\tunsigned int i = rx_ring->next_to_use;\n\n\t \n\tif (!cleaned_count || !rx_ring->netdev)\n\t\treturn;\n\n\trx_desc = IXGBEVF_RX_DESC(rx_ring, i);\n\tbi = &rx_ring->rx_buffer_info[i];\n\ti -= rx_ring->count;\n\n\tdo {\n\t\tif (!ixgbevf_alloc_mapped_page(rx_ring, bi))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_sync_single_range_for_device(rx_ring->dev, bi->dma,\n\t\t\t\t\t\t bi->page_offset,\n\t\t\t\t\t\t ixgbevf_rx_bufsz(rx_ring),\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t \n\t\trx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);\n\n\t\trx_desc++;\n\t\tbi++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\trx_desc = IXGBEVF_RX_DESC(rx_ring, 0);\n\t\t\tbi = rx_ring->rx_buffer_info;\n\t\t\ti -= rx_ring->count;\n\t\t}\n\n\t\t \n\t\trx_desc->wb.upper.length = 0;\n\n\t\tcleaned_count--;\n\t} while (cleaned_count);\n\n\ti += rx_ring->count;\n\n\tif (rx_ring->next_to_use != i) {\n\t\t \n\t\trx_ring->next_to_use = i;\n\n\t\t \n\t\trx_ring->next_to_alloc = i;\n\n\t\t \n\t\twmb();\n\t\tixgbevf_write_tail(rx_ring, i);\n\t}\n}\n\n \nstatic bool ixgbevf_cleanup_headers(struct ixgbevf_ring *rx_ring,\n\t\t\t\t    union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t\t    struct sk_buff *skb)\n{\n\t \n\tif (IS_ERR(skb))\n\t\treturn true;\n\n\t \n\tif (unlikely(ixgbevf_test_staterr(rx_desc,\n\t\t\t\t\t  IXGBE_RXDADV_ERR_FRAME_ERR_MASK))) {\n\t\tstruct net_device *netdev = rx_ring->netdev;\n\n\t\tif (!(netdev->features & NETIF_F_RXALL)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t \n\tif (eth_skb_pad(skb))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void ixgbevf_reuse_rx_page(struct ixgbevf_ring *rx_ring,\n\t\t\t\t  struct ixgbevf_rx_buffer *old_buff)\n{\n\tstruct ixgbevf_rx_buffer *new_buff;\n\tu16 nta = rx_ring->next_to_alloc;\n\n\tnew_buff = &rx_ring->rx_buffer_info[nta];\n\n\t \n\tnta++;\n\trx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;\n\n\t \n\tnew_buff->page = old_buff->page;\n\tnew_buff->dma = old_buff->dma;\n\tnew_buff->page_offset = old_buff->page_offset;\n\tnew_buff->pagecnt_bias = old_buff->pagecnt_bias;\n}\n\nstatic bool ixgbevf_can_reuse_rx_page(struct ixgbevf_rx_buffer *rx_buffer)\n{\n\tunsigned int pagecnt_bias = rx_buffer->pagecnt_bias;\n\tstruct page *page = rx_buffer->page;\n\n\t \n\tif (!dev_page_is_reusable(page))\n\t\treturn false;\n\n#if (PAGE_SIZE < 8192)\n\t \n\tif (unlikely((page_ref_count(page) - pagecnt_bias) > 1))\n\t\treturn false;\n#else\n#define IXGBEVF_LAST_OFFSET \\\n\t(SKB_WITH_OVERHEAD(PAGE_SIZE) - IXGBEVF_RXBUFFER_2048)\n\n\tif (rx_buffer->page_offset > IXGBEVF_LAST_OFFSET)\n\t\treturn false;\n\n#endif\n\n\t \n\tif (unlikely(!pagecnt_bias)) {\n\t\tpage_ref_add(page, USHRT_MAX);\n\t\trx_buffer->pagecnt_bias = USHRT_MAX;\n\t}\n\n\treturn true;\n}\n\n \nstatic void ixgbevf_add_rx_frag(struct ixgbevf_ring *rx_ring,\n\t\t\t\tstruct ixgbevf_rx_buffer *rx_buffer,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tunsigned int size)\n{\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = ixgbevf_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = ring_uses_build_skb(rx_ring) ?\n\t\t\t\tSKB_DATA_ALIGN(IXGBEVF_SKB_PAD + size) :\n\t\t\t\tSKB_DATA_ALIGN(size);\n#endif\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_buffer->page,\n\t\t\trx_buffer->page_offset, size, truesize);\n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n}\n\nstatic\nstruct sk_buff *ixgbevf_construct_skb(struct ixgbevf_ring *rx_ring,\n\t\t\t\t      struct ixgbevf_rx_buffer *rx_buffer,\n\t\t\t\t      struct xdp_buff *xdp,\n\t\t\t\t      union ixgbe_adv_rx_desc *rx_desc)\n{\n\tunsigned int size = xdp->data_end - xdp->data;\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = ixgbevf_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = SKB_DATA_ALIGN(xdp->data_end -\n\t\t\t\t\t       xdp->data_hard_start);\n#endif\n\tunsigned int headlen;\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data);\n\n\t \n\n\t \n\tskb = napi_alloc_skb(&rx_ring->q_vector->napi, IXGBEVF_RX_HDR_SIZE);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\theadlen = size;\n\tif (headlen > IXGBEVF_RX_HDR_SIZE)\n\t\theadlen = eth_get_headlen(skb->dev, xdp->data,\n\t\t\t\t\t  IXGBEVF_RX_HDR_SIZE);\n\n\t \n\tmemcpy(__skb_put(skb, headlen), xdp->data,\n\t       ALIGN(headlen, sizeof(long)));\n\n\t \n\tsize -= headlen;\n\tif (size) {\n\t\tskb_add_rx_frag(skb, 0, rx_buffer->page,\n\t\t\t\t(xdp->data + headlen) -\n\t\t\t\t\tpage_address(rx_buffer->page),\n\t\t\t\tsize, truesize);\n#if (PAGE_SIZE < 8192)\n\t\trx_buffer->page_offset ^= truesize;\n#else\n\t\trx_buffer->page_offset += truesize;\n#endif\n\t} else {\n\t\trx_buffer->pagecnt_bias++;\n\t}\n\n\treturn skb;\n}\n\nstatic inline void ixgbevf_irq_enable_queues(struct ixgbevf_adapter *adapter,\n\t\t\t\t\t     u32 qmask)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VTEIMS, qmask);\n}\n\nstatic struct sk_buff *ixgbevf_build_skb(struct ixgbevf_ring *rx_ring,\n\t\t\t\t\t struct ixgbevf_rx_buffer *rx_buffer,\n\t\t\t\t\t struct xdp_buff *xdp,\n\t\t\t\t\t union ixgbe_adv_rx_desc *rx_desc)\n{\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = ixgbevf_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) +\n\t\t\t\tSKB_DATA_ALIGN(xdp->data_end -\n\t\t\t\t\t       xdp->data_hard_start);\n#endif\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data_meta);\n\n\t \n\tskb = napi_build_skb(xdp->data_hard_start, truesize);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\t__skb_put(skb, xdp->data_end - xdp->data);\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\t \n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n\n\treturn skb;\n}\n\n#define IXGBEVF_XDP_PASS 0\n#define IXGBEVF_XDP_CONSUMED 1\n#define IXGBEVF_XDP_TX 2\n\nstatic int ixgbevf_xmit_xdp_ring(struct ixgbevf_ring *ring,\n\t\t\t\t struct xdp_buff *xdp)\n{\n\tstruct ixgbevf_tx_buffer *tx_buffer;\n\tunion ixgbe_adv_tx_desc *tx_desc;\n\tu32 len, cmd_type;\n\tdma_addr_t dma;\n\tu16 i;\n\n\tlen = xdp->data_end - xdp->data;\n\n\tif (unlikely(!ixgbevf_desc_unused(ring)))\n\t\treturn IXGBEVF_XDP_CONSUMED;\n\n\tdma = dma_map_single(ring->dev, xdp->data, len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(ring->dev, dma))\n\t\treturn IXGBEVF_XDP_CONSUMED;\n\n\t \n\ti = ring->next_to_use;\n\ttx_buffer = &ring->tx_buffer_info[i];\n\n\tdma_unmap_len_set(tx_buffer, len, len);\n\tdma_unmap_addr_set(tx_buffer, dma, dma);\n\ttx_buffer->data = xdp->data;\n\ttx_buffer->bytecount = len;\n\ttx_buffer->gso_segs = 1;\n\ttx_buffer->protocol = 0;\n\n\t \n\tif (!test_bit(__IXGBEVF_TX_XDP_RING_PRIMED, &ring->state)) {\n\t\tstruct ixgbe_adv_tx_context_desc *context_desc;\n\n\t\tset_bit(__IXGBEVF_TX_XDP_RING_PRIMED, &ring->state);\n\n\t\tcontext_desc = IXGBEVF_TX_CTXTDESC(ring, 0);\n\t\tcontext_desc->vlan_macip_lens\t=\n\t\t\tcpu_to_le32(ETH_HLEN << IXGBE_ADVTXD_MACLEN_SHIFT);\n\t\tcontext_desc->fceof_saidx\t= 0;\n\t\tcontext_desc->type_tucmd_mlhl\t=\n\t\t\tcpu_to_le32(IXGBE_TXD_CMD_DEXT |\n\t\t\t\t    IXGBE_ADVTXD_DTYP_CTXT);\n\t\tcontext_desc->mss_l4len_idx\t= 0;\n\n\t\ti = 1;\n\t}\n\n\t \n\tcmd_type = IXGBE_ADVTXD_DTYP_DATA |\n\t\t   IXGBE_ADVTXD_DCMD_DEXT |\n\t\t   IXGBE_ADVTXD_DCMD_IFCS;\n\tcmd_type |= len | IXGBE_TXD_CMD;\n\n\ttx_desc = IXGBEVF_TX_DESC(ring, i);\n\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\n\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\ttx_desc->read.olinfo_status =\n\t\t\tcpu_to_le32((len << IXGBE_ADVTXD_PAYLEN_SHIFT) |\n\t\t\t\t    IXGBE_ADVTXD_CC);\n\n\t \n\tsmp_wmb();\n\n\t \n\ti++;\n\tif (i == ring->count)\n\t\ti = 0;\n\n\ttx_buffer->next_to_watch = tx_desc;\n\tring->next_to_use = i;\n\n\treturn IXGBEVF_XDP_TX;\n}\n\nstatic struct sk_buff *ixgbevf_run_xdp(struct ixgbevf_adapter *adapter,\n\t\t\t\t       struct ixgbevf_ring  *rx_ring,\n\t\t\t\t       struct xdp_buff *xdp)\n{\n\tint result = IXGBEVF_XDP_PASS;\n\tstruct ixgbevf_ring *xdp_ring;\n\tstruct bpf_prog *xdp_prog;\n\tu32 act;\n\n\txdp_prog = READ_ONCE(rx_ring->xdp_prog);\n\n\tif (!xdp_prog)\n\t\tgoto xdp_out;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\txdp_ring = adapter->xdp_ring[rx_ring->queue_index];\n\t\tresult = ixgbevf_xmit_xdp_ring(xdp_ring, xdp);\n\t\tif (result == IXGBEVF_XDP_CONSUMED)\n\t\t\tgoto out_failure;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\nout_failure:\n\t\ttrace_xdp_exception(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;  \n\tcase XDP_DROP:\n\t\tresult = IXGBEVF_XDP_CONSUMED;\n\t\tbreak;\n\t}\nxdp_out:\n\treturn ERR_PTR(-result);\n}\n\nstatic unsigned int ixgbevf_rx_frame_truesize(struct ixgbevf_ring *rx_ring,\n\t\t\t\t\t      unsigned int size)\n{\n\tunsigned int truesize;\n\n#if (PAGE_SIZE < 8192)\n\ttruesize = ixgbevf_rx_pg_size(rx_ring) / 2;  \n#else\n\ttruesize = ring_uses_build_skb(rx_ring) ?\n\t\tSKB_DATA_ALIGN(IXGBEVF_SKB_PAD + size) +\n\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info)) :\n\t\tSKB_DATA_ALIGN(size);\n#endif\n\treturn truesize;\n}\n\nstatic void ixgbevf_rx_buffer_flip(struct ixgbevf_ring *rx_ring,\n\t\t\t\t   struct ixgbevf_rx_buffer *rx_buffer,\n\t\t\t\t   unsigned int size)\n{\n\tunsigned int truesize = ixgbevf_rx_frame_truesize(rx_ring, size);\n\n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n}\n\nstatic int ixgbevf_clean_rx_irq(struct ixgbevf_q_vector *q_vector,\n\t\t\t\tstruct ixgbevf_ring *rx_ring,\n\t\t\t\tint budget)\n{\n\tunsigned int total_rx_bytes = 0, total_rx_packets = 0, frame_sz = 0;\n\tstruct ixgbevf_adapter *adapter = q_vector->adapter;\n\tu16 cleaned_count = ixgbevf_desc_unused(rx_ring);\n\tstruct sk_buff *skb = rx_ring->skb;\n\tbool xdp_xmit = false;\n\tstruct xdp_buff xdp;\n\n\t \n#if (PAGE_SIZE < 8192)\n\tframe_sz = ixgbevf_rx_frame_truesize(rx_ring, 0);\n#endif\n\txdp_init_buff(&xdp, frame_sz, &rx_ring->xdp_rxq);\n\n\twhile (likely(total_rx_packets < budget)) {\n\t\tstruct ixgbevf_rx_buffer *rx_buffer;\n\t\tunion ixgbe_adv_rx_desc *rx_desc;\n\t\tunsigned int size;\n\n\t\t \n\t\tif (cleaned_count >= IXGBEVF_RX_BUFFER_WRITE) {\n\t\t\tixgbevf_alloc_rx_buffers(rx_ring, cleaned_count);\n\t\t\tcleaned_count = 0;\n\t\t}\n\n\t\trx_desc = IXGBEVF_RX_DESC(rx_ring, rx_ring->next_to_clean);\n\t\tsize = le16_to_cpu(rx_desc->wb.upper.length);\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\t \n\t\trmb();\n\n\t\trx_buffer = ixgbevf_get_rx_buffer(rx_ring, size);\n\n\t\t \n\t\tif (!skb) {\n\t\t\tunsigned int offset = ixgbevf_rx_offset(rx_ring);\n\t\t\tunsigned char *hard_start;\n\n\t\t\thard_start = page_address(rx_buffer->page) +\n\t\t\t\t     rx_buffer->page_offset - offset;\n\t\t\txdp_prepare_buff(&xdp, hard_start, offset, size, true);\n#if (PAGE_SIZE > 4096)\n\t\t\t \n\t\t\txdp.frame_sz = ixgbevf_rx_frame_truesize(rx_ring, size);\n#endif\n\t\t\tskb = ixgbevf_run_xdp(adapter, rx_ring, &xdp);\n\t\t}\n\n\t\tif (IS_ERR(skb)) {\n\t\t\tif (PTR_ERR(skb) == -IXGBEVF_XDP_TX) {\n\t\t\t\txdp_xmit = true;\n\t\t\t\tixgbevf_rx_buffer_flip(rx_ring, rx_buffer,\n\t\t\t\t\t\t       size);\n\t\t\t} else {\n\t\t\t\trx_buffer->pagecnt_bias++;\n\t\t\t}\n\t\t\ttotal_rx_packets++;\n\t\t\ttotal_rx_bytes += size;\n\t\t} else if (skb) {\n\t\t\tixgbevf_add_rx_frag(rx_ring, rx_buffer, skb, size);\n\t\t} else if (ring_uses_build_skb(rx_ring)) {\n\t\t\tskb = ixgbevf_build_skb(rx_ring, rx_buffer,\n\t\t\t\t\t\t&xdp, rx_desc);\n\t\t} else {\n\t\t\tskb = ixgbevf_construct_skb(rx_ring, rx_buffer,\n\t\t\t\t\t\t    &xdp, rx_desc);\n\t\t}\n\n\t\t \n\t\tif (!skb) {\n\t\t\trx_ring->rx_stats.alloc_rx_buff_failed++;\n\t\t\trx_buffer->pagecnt_bias++;\n\t\t\tbreak;\n\t\t}\n\n\t\tixgbevf_put_rx_buffer(rx_ring, rx_buffer, skb);\n\t\tcleaned_count++;\n\n\t\t \n\t\tif (ixgbevf_is_non_eop(rx_ring, rx_desc))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (ixgbevf_cleanup_headers(rx_ring, rx_desc, skb)) {\n\t\t\tskb = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\ttotal_rx_bytes += skb->len;\n\n\t\t \n\t\tif ((skb->pkt_type == PACKET_BROADCAST ||\n\t\t     skb->pkt_type == PACKET_MULTICAST) &&\n\t\t    ether_addr_equal(rx_ring->netdev->dev_addr,\n\t\t\t\t     eth_hdr(skb)->h_source)) {\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tixgbevf_process_skb_fields(rx_ring, rx_desc, skb);\n\n\t\tixgbevf_rx_skb(q_vector, skb);\n\n\t\t \n\t\tskb = NULL;\n\n\t\t \n\t\ttotal_rx_packets++;\n\t}\n\n\t \n\trx_ring->skb = skb;\n\n\tif (xdp_xmit) {\n\t\tstruct ixgbevf_ring *xdp_ring =\n\t\t\tadapter->xdp_ring[rx_ring->queue_index];\n\n\t\t \n\t\twmb();\n\t\tixgbevf_write_tail(xdp_ring, xdp_ring->next_to_use);\n\t}\n\n\tu64_stats_update_begin(&rx_ring->syncp);\n\trx_ring->stats.packets += total_rx_packets;\n\trx_ring->stats.bytes += total_rx_bytes;\n\tu64_stats_update_end(&rx_ring->syncp);\n\tq_vector->rx.total_packets += total_rx_packets;\n\tq_vector->rx.total_bytes += total_rx_bytes;\n\n\treturn total_rx_packets;\n}\n\n \nstatic int ixgbevf_poll(struct napi_struct *napi, int budget)\n{\n\tstruct ixgbevf_q_vector *q_vector =\n\t\tcontainer_of(napi, struct ixgbevf_q_vector, napi);\n\tstruct ixgbevf_adapter *adapter = q_vector->adapter;\n\tstruct ixgbevf_ring *ring;\n\tint per_ring_budget, work_done = 0;\n\tbool clean_complete = true;\n\n\tixgbevf_for_each_ring(ring, q_vector->tx) {\n\t\tif (!ixgbevf_clean_tx_irq(q_vector, ring, budget))\n\t\t\tclean_complete = false;\n\t}\n\n\tif (budget <= 0)\n\t\treturn budget;\n\n\t \n\tif (q_vector->rx.count > 1)\n\t\tper_ring_budget = max(budget/q_vector->rx.count, 1);\n\telse\n\t\tper_ring_budget = budget;\n\n\tixgbevf_for_each_ring(ring, q_vector->rx) {\n\t\tint cleaned = ixgbevf_clean_rx_irq(q_vector, ring,\n\t\t\t\t\t\t   per_ring_budget);\n\t\twork_done += cleaned;\n\t\tif (cleaned >= per_ring_budget)\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (!clean_complete)\n\t\treturn budget;\n\n\t \n\tif (likely(napi_complete_done(napi, work_done))) {\n\t\tif (adapter->rx_itr_setting == 1)\n\t\t\tixgbevf_set_itr(q_vector);\n\t\tif (!test_bit(__IXGBEVF_DOWN, &adapter->state) &&\n\t\t    !test_bit(__IXGBEVF_REMOVING, &adapter->state))\n\t\t\tixgbevf_irq_enable_queues(adapter,\n\t\t\t\t\t\t  BIT(q_vector->v_idx));\n\t}\n\n\treturn min(work_done, budget - 1);\n}\n\n \nvoid ixgbevf_write_eitr(struct ixgbevf_q_vector *q_vector)\n{\n\tstruct ixgbevf_adapter *adapter = q_vector->adapter;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint v_idx = q_vector->v_idx;\n\tu32 itr_reg = q_vector->itr & IXGBE_MAX_EITR;\n\n\t \n\titr_reg |= IXGBE_EITR_CNT_WDIS;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VTEITR(v_idx), itr_reg);\n}\n\n \nstatic void ixgbevf_configure_msix(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbevf_q_vector *q_vector;\n\tint q_vectors, v_idx;\n\n\tq_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\n\tadapter->eims_enable_mask = 0;\n\n\t \n\tfor (v_idx = 0; v_idx < q_vectors; v_idx++) {\n\t\tstruct ixgbevf_ring *ring;\n\n\t\tq_vector = adapter->q_vector[v_idx];\n\n\t\tixgbevf_for_each_ring(ring, q_vector->rx)\n\t\t\tixgbevf_set_ivar(adapter, 0, ring->reg_idx, v_idx);\n\n\t\tixgbevf_for_each_ring(ring, q_vector->tx)\n\t\t\tixgbevf_set_ivar(adapter, 1, ring->reg_idx, v_idx);\n\n\t\tif (q_vector->tx.ring && !q_vector->rx.ring) {\n\t\t\t \n\t\t\tif (adapter->tx_itr_setting == 1)\n\t\t\t\tq_vector->itr = IXGBE_12K_ITR;\n\t\t\telse\n\t\t\t\tq_vector->itr = adapter->tx_itr_setting;\n\t\t} else {\n\t\t\t \n\t\t\tif (adapter->rx_itr_setting == 1)\n\t\t\t\tq_vector->itr = IXGBE_20K_ITR;\n\t\t\telse\n\t\t\t\tq_vector->itr = adapter->rx_itr_setting;\n\t\t}\n\n\t\t \n\t\tadapter->eims_enable_mask |= BIT(v_idx);\n\n\t\tixgbevf_write_eitr(q_vector);\n\t}\n\n\tixgbevf_set_ivar(adapter, -1, 1, v_idx);\n\t \n\tadapter->eims_other = BIT(v_idx);\n\tadapter->eims_enable_mask |= adapter->eims_other;\n}\n\nenum latency_range {\n\tlowest_latency = 0,\n\tlow_latency = 1,\n\tbulk_latency = 2,\n\tlatency_invalid = 255\n};\n\n \nstatic void ixgbevf_update_itr(struct ixgbevf_q_vector *q_vector,\n\t\t\t       struct ixgbevf_ring_container *ring_container)\n{\n\tint bytes = ring_container->total_bytes;\n\tint packets = ring_container->total_packets;\n\tu32 timepassed_us;\n\tu64 bytes_perint;\n\tu8 itr_setting = ring_container->itr;\n\n\tif (packets == 0)\n\t\treturn;\n\n\t \n\t \n\ttimepassed_us = q_vector->itr >> 2;\n\tif (timepassed_us == 0)\n\t\treturn;\n\n\tbytes_perint = bytes / timepassed_us;  \n\n\tswitch (itr_setting) {\n\tcase lowest_latency:\n\t\tif (bytes_perint > 10)\n\t\t\titr_setting = low_latency;\n\t\tbreak;\n\tcase low_latency:\n\t\tif (bytes_perint > 20)\n\t\t\titr_setting = bulk_latency;\n\t\telse if (bytes_perint <= 10)\n\t\t\titr_setting = lowest_latency;\n\t\tbreak;\n\tcase bulk_latency:\n\t\tif (bytes_perint <= 20)\n\t\t\titr_setting = low_latency;\n\t\tbreak;\n\t}\n\n\t \n\tring_container->total_bytes = 0;\n\tring_container->total_packets = 0;\n\n\t \n\tring_container->itr = itr_setting;\n}\n\nstatic void ixgbevf_set_itr(struct ixgbevf_q_vector *q_vector)\n{\n\tu32 new_itr = q_vector->itr;\n\tu8 current_itr;\n\n\tixgbevf_update_itr(q_vector, &q_vector->tx);\n\tixgbevf_update_itr(q_vector, &q_vector->rx);\n\n\tcurrent_itr = max(q_vector->rx.itr, q_vector->tx.itr);\n\n\tswitch (current_itr) {\n\t \n\tcase lowest_latency:\n\t\tnew_itr = IXGBE_100K_ITR;\n\t\tbreak;\n\tcase low_latency:\n\t\tnew_itr = IXGBE_20K_ITR;\n\t\tbreak;\n\tcase bulk_latency:\n\t\tnew_itr = IXGBE_12K_ITR;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (new_itr != q_vector->itr) {\n\t\t \n\t\tnew_itr = (10 * new_itr * q_vector->itr) /\n\t\t\t  ((9 * new_itr) + q_vector->itr);\n\n\t\t \n\t\tq_vector->itr = new_itr;\n\n\t\tixgbevf_write_eitr(q_vector);\n\t}\n}\n\nstatic irqreturn_t ixgbevf_msix_other(int irq, void *data)\n{\n\tstruct ixgbevf_adapter *adapter = data;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\thw->mac.get_link_status = 1;\n\n\tixgbevf_service_event_schedule(adapter);\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VTEIMS, adapter->eims_other);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t ixgbevf_msix_clean_rings(int irq, void *data)\n{\n\tstruct ixgbevf_q_vector *q_vector = data;\n\n\t \n\tif (q_vector->rx.ring || q_vector->tx.ring)\n\t\tnapi_schedule_irqoff(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int ixgbevf_request_msix_irqs(struct ixgbevf_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\n\tunsigned int ri = 0, ti = 0;\n\tint vector, err;\n\n\tfor (vector = 0; vector < q_vectors; vector++) {\n\t\tstruct ixgbevf_q_vector *q_vector = adapter->q_vector[vector];\n\t\tstruct msix_entry *entry = &adapter->msix_entries[vector];\n\n\t\tif (q_vector->tx.ring && q_vector->rx.ring) {\n\t\t\tsnprintf(q_vector->name, sizeof(q_vector->name),\n\t\t\t\t \"%s-TxRx-%u\", netdev->name, ri++);\n\t\t\tti++;\n\t\t} else if (q_vector->rx.ring) {\n\t\t\tsnprintf(q_vector->name, sizeof(q_vector->name),\n\t\t\t\t \"%s-rx-%u\", netdev->name, ri++);\n\t\t} else if (q_vector->tx.ring) {\n\t\t\tsnprintf(q_vector->name, sizeof(q_vector->name),\n\t\t\t\t \"%s-tx-%u\", netdev->name, ti++);\n\t\t} else {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\terr = request_irq(entry->vector, &ixgbevf_msix_clean_rings, 0,\n\t\t\t\t  q_vector->name, q_vector);\n\t\tif (err) {\n\t\t\thw_dbg(&adapter->hw,\n\t\t\t       \"request_irq failed for MSIX interrupt Error: %d\\n\",\n\t\t\t       err);\n\t\t\tgoto free_queue_irqs;\n\t\t}\n\t}\n\n\terr = request_irq(adapter->msix_entries[vector].vector,\n\t\t\t  &ixgbevf_msix_other, 0, netdev->name, adapter);\n\tif (err) {\n\t\thw_dbg(&adapter->hw, \"request_irq for msix_other failed: %d\\n\",\n\t\t       err);\n\t\tgoto free_queue_irqs;\n\t}\n\n\treturn 0;\n\nfree_queue_irqs:\n\twhile (vector) {\n\t\tvector--;\n\t\tfree_irq(adapter->msix_entries[vector].vector,\n\t\t\t adapter->q_vector[vector]);\n\t}\n\t \n\tadapter->num_msix_vectors = 0;\n\treturn err;\n}\n\n \nstatic int ixgbevf_request_irq(struct ixgbevf_adapter *adapter)\n{\n\tint err = ixgbevf_request_msix_irqs(adapter);\n\n\tif (err)\n\t\thw_dbg(&adapter->hw, \"request_irq failed, Error %d\\n\", err);\n\n\treturn err;\n}\n\nstatic void ixgbevf_free_irq(struct ixgbevf_adapter *adapter)\n{\n\tint i, q_vectors;\n\n\tif (!adapter->msix_entries)\n\t\treturn;\n\n\tq_vectors = adapter->num_msix_vectors;\n\ti = q_vectors - 1;\n\n\tfree_irq(adapter->msix_entries[i].vector, adapter);\n\ti--;\n\n\tfor (; i >= 0; i--) {\n\t\t \n\t\tif (!adapter->q_vector[i]->rx.ring &&\n\t\t    !adapter->q_vector[i]->tx.ring)\n\t\t\tcontinue;\n\n\t\tfree_irq(adapter->msix_entries[i].vector,\n\t\t\t adapter->q_vector[i]);\n\t}\n}\n\n \nstatic inline void ixgbevf_irq_disable(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VTEIAM, 0);\n\tIXGBE_WRITE_REG(hw, IXGBE_VTEIMC, ~0);\n\tIXGBE_WRITE_REG(hw, IXGBE_VTEIAC, 0);\n\n\tIXGBE_WRITE_FLUSH(hw);\n\n\tfor (i = 0; i < adapter->num_msix_vectors; i++)\n\t\tsynchronize_irq(adapter->msix_entries[i].vector);\n}\n\n \nstatic inline void ixgbevf_irq_enable(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VTEIAM, adapter->eims_enable_mask);\n\tIXGBE_WRITE_REG(hw, IXGBE_VTEIAC, adapter->eims_enable_mask);\n\tIXGBE_WRITE_REG(hw, IXGBE_VTEIMS, adapter->eims_enable_mask);\n}\n\n \nstatic void ixgbevf_configure_tx_ring(struct ixgbevf_adapter *adapter,\n\t\t\t\t      struct ixgbevf_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu64 tdba = ring->dma;\n\tint wait_loop = 10;\n\tu32 txdctl = IXGBE_TXDCTL_ENABLE;\n\tu8 reg_idx = ring->reg_idx;\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(reg_idx), IXGBE_TXDCTL_SWFLSH);\n\tIXGBE_WRITE_FLUSH(hw);\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VFTDBAL(reg_idx), tdba & DMA_BIT_MASK(32));\n\tIXGBE_WRITE_REG(hw, IXGBE_VFTDBAH(reg_idx), tdba >> 32);\n\tIXGBE_WRITE_REG(hw, IXGBE_VFTDLEN(reg_idx),\n\t\t\tring->count * sizeof(union ixgbe_adv_tx_desc));\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_VFTDWBAH(reg_idx), 0);\n\tIXGBE_WRITE_REG(hw, IXGBE_VFTDWBAL(reg_idx), 0);\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_VFDCA_TXCTRL(reg_idx),\n\t\t\t(IXGBE_DCA_TXCTRL_DESC_RRO_EN |\n\t\t\t IXGBE_DCA_TXCTRL_DATA_RRO_EN));\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_VFTDH(reg_idx), 0);\n\tIXGBE_WRITE_REG(hw, IXGBE_VFTDT(reg_idx), 0);\n\tring->tail = adapter->io_addr + IXGBE_VFTDT(reg_idx);\n\n\t \n\tring->next_to_clean = 0;\n\tring->next_to_use = 0;\n\n\t \n\ttxdctl |= (8 << 16);     \n\n\t \n\ttxdctl |= (1u << 8) |     \n\t\t   32;            \n\n\t \n\tmemset(ring->tx_buffer_info, 0,\n\t       sizeof(struct ixgbevf_tx_buffer) * ring->count);\n\n\tclear_bit(__IXGBEVF_HANG_CHECK_ARMED, &ring->state);\n\tclear_bit(__IXGBEVF_TX_XDP_RING_PRIMED, &ring->state);\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(reg_idx), txdctl);\n\n\t \n\tdo {\n\t\tusleep_range(1000, 2000);\n\t\ttxdctl = IXGBE_READ_REG(hw, IXGBE_VFTXDCTL(reg_idx));\n\t}  while (--wait_loop && !(txdctl & IXGBE_TXDCTL_ENABLE));\n\tif (!wait_loop)\n\t\thw_dbg(hw, \"Could not enable Tx Queue %d\\n\", reg_idx);\n}\n\n \nstatic void ixgbevf_configure_tx(struct ixgbevf_adapter *adapter)\n{\n\tu32 i;\n\n\t \n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tixgbevf_configure_tx_ring(adapter, adapter->tx_ring[i]);\n\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\tixgbevf_configure_tx_ring(adapter, adapter->xdp_ring[i]);\n}\n\n#define IXGBE_SRRCTL_BSIZEHDRSIZE_SHIFT\t2\n\nstatic void ixgbevf_configure_srrctl(struct ixgbevf_adapter *adapter,\n\t\t\t\t     struct ixgbevf_ring *ring, int index)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 srrctl;\n\n\tsrrctl = IXGBE_SRRCTL_DROP_EN;\n\n\tsrrctl |= IXGBEVF_RX_HDR_SIZE << IXGBE_SRRCTL_BSIZEHDRSIZE_SHIFT;\n\tif (ring_uses_large_buffer(ring))\n\t\tsrrctl |= IXGBEVF_RXBUFFER_3072 >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;\n\telse\n\t\tsrrctl |= IXGBEVF_RXBUFFER_2048 >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;\n\tsrrctl |= IXGBE_SRRCTL_DESCTYPE_ADV_ONEBUF;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VFSRRCTL(index), srrctl);\n}\n\nstatic void ixgbevf_setup_psrtype(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\t \n\tu32 psrtype = IXGBE_PSRTYPE_TCPHDR | IXGBE_PSRTYPE_UDPHDR |\n\t\t      IXGBE_PSRTYPE_IPV4HDR | IXGBE_PSRTYPE_IPV6HDR |\n\t\t      IXGBE_PSRTYPE_L2HDR;\n\n\tif (adapter->num_rx_queues > 1)\n\t\tpsrtype |= BIT(29);\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VFPSRTYPE, psrtype);\n}\n\n#define IXGBEVF_MAX_RX_DESC_POLL 10\nstatic void ixgbevf_disable_rx_queue(struct ixgbevf_adapter *adapter,\n\t\t\t\t     struct ixgbevf_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint wait_loop = IXGBEVF_MAX_RX_DESC_POLL;\n\tu32 rxdctl;\n\tu8 reg_idx = ring->reg_idx;\n\n\tif (IXGBE_REMOVED(hw->hw_addr))\n\t\treturn;\n\trxdctl = IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(reg_idx));\n\trxdctl &= ~IXGBE_RXDCTL_ENABLE;\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_VFRXDCTL(reg_idx), rxdctl);\n\n\t \n\tdo {\n\t\tudelay(10);\n\t\trxdctl = IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(reg_idx));\n\t} while (--wait_loop && (rxdctl & IXGBE_RXDCTL_ENABLE));\n\n\tif (!wait_loop)\n\t\tpr_err(\"RXDCTL.ENABLE queue %d not cleared while polling\\n\",\n\t\t       reg_idx);\n}\n\nstatic void ixgbevf_rx_desc_queue_enable(struct ixgbevf_adapter *adapter,\n\t\t\t\t\t struct ixgbevf_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint wait_loop = IXGBEVF_MAX_RX_DESC_POLL;\n\tu32 rxdctl;\n\tu8 reg_idx = ring->reg_idx;\n\n\tif (IXGBE_REMOVED(hw->hw_addr))\n\t\treturn;\n\tdo {\n\t\tusleep_range(1000, 2000);\n\t\trxdctl = IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(reg_idx));\n\t} while (--wait_loop && !(rxdctl & IXGBE_RXDCTL_ENABLE));\n\n\tif (!wait_loop)\n\t\tpr_err(\"RXDCTL.ENABLE queue %d not set while polling\\n\",\n\t\t       reg_idx);\n}\n\n \nstatic inline int ixgbevf_init_rss_key(struct ixgbevf_adapter *adapter)\n{\n\tu32 *rss_key;\n\n\tif (!adapter->rss_key) {\n\t\trss_key = kzalloc(IXGBEVF_RSS_HASH_KEY_SIZE, GFP_KERNEL);\n\t\tif (unlikely(!rss_key))\n\t\t\treturn -ENOMEM;\n\n\t\tnetdev_rss_key_fill(rss_key, IXGBEVF_RSS_HASH_KEY_SIZE);\n\t\tadapter->rss_key = rss_key;\n\t}\n\n\treturn 0;\n}\n\nstatic void ixgbevf_setup_vfmrqc(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 vfmrqc = 0, vfreta = 0;\n\tu16 rss_i = adapter->num_rx_queues;\n\tu8 i, j;\n\n\t \n\tfor (i = 0; i < IXGBEVF_VFRSSRK_REGS; i++)\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VFRSSRK(i), *(adapter->rss_key + i));\n\n\tfor (i = 0, j = 0; i < IXGBEVF_X550_VFRETA_SIZE; i++, j++) {\n\t\tif (j == rss_i)\n\t\t\tj = 0;\n\n\t\tadapter->rss_indir_tbl[i] = j;\n\n\t\tvfreta |= j << (i & 0x3) * 8;\n\t\tif ((i & 3) == 3) {\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_VFRETA(i >> 2), vfreta);\n\t\t\tvfreta = 0;\n\t\t}\n\t}\n\n\t \n\tvfmrqc |= IXGBE_VFMRQC_RSS_FIELD_IPV4 |\n\t\tIXGBE_VFMRQC_RSS_FIELD_IPV4_TCP |\n\t\tIXGBE_VFMRQC_RSS_FIELD_IPV6 |\n\t\tIXGBE_VFMRQC_RSS_FIELD_IPV6_TCP;\n\n\tvfmrqc |= IXGBE_VFMRQC_RSSEN;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VFMRQC, vfmrqc);\n}\n\nstatic void ixgbevf_configure_rx_ring(struct ixgbevf_adapter *adapter,\n\t\t\t\t      struct ixgbevf_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tunion ixgbe_adv_rx_desc *rx_desc;\n\tu64 rdba = ring->dma;\n\tu32 rxdctl;\n\tu8 reg_idx = ring->reg_idx;\n\n\t \n\trxdctl = IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(reg_idx));\n\tixgbevf_disable_rx_queue(adapter, ring);\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VFRDBAL(reg_idx), rdba & DMA_BIT_MASK(32));\n\tIXGBE_WRITE_REG(hw, IXGBE_VFRDBAH(reg_idx), rdba >> 32);\n\tIXGBE_WRITE_REG(hw, IXGBE_VFRDLEN(reg_idx),\n\t\t\tring->count * sizeof(union ixgbe_adv_rx_desc));\n\n#ifndef CONFIG_SPARC\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_VFDCA_RXCTRL(reg_idx),\n\t\t\tIXGBE_DCA_RXCTRL_DESC_RRO_EN);\n#else\n\tIXGBE_WRITE_REG(hw, IXGBE_VFDCA_RXCTRL(reg_idx),\n\t\t\tIXGBE_DCA_RXCTRL_DESC_RRO_EN |\n\t\t\tIXGBE_DCA_RXCTRL_DATA_WRO_EN);\n#endif\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_VFRDH(reg_idx), 0);\n\tIXGBE_WRITE_REG(hw, IXGBE_VFRDT(reg_idx), 0);\n\tring->tail = adapter->io_addr + IXGBE_VFRDT(reg_idx);\n\n\t \n\tmemset(ring->rx_buffer_info, 0,\n\t       sizeof(struct ixgbevf_rx_buffer) * ring->count);\n\n\t \n\trx_desc = IXGBEVF_RX_DESC(ring, 0);\n\trx_desc->wb.upper.length = 0;\n\n\t \n\tring->next_to_clean = 0;\n\tring->next_to_use = 0;\n\tring->next_to_alloc = 0;\n\n\tixgbevf_configure_srrctl(adapter, ring, reg_idx);\n\n\t \n\tif (adapter->hw.mac.type != ixgbe_mac_82599_vf) {\n\t\trxdctl &= ~(IXGBE_RXDCTL_RLPMLMASK |\n\t\t\t    IXGBE_RXDCTL_RLPML_EN);\n\n#if (PAGE_SIZE < 8192)\n\t\t \n\t\tif (ring_uses_build_skb(ring) &&\n\t\t    !ring_uses_large_buffer(ring))\n\t\t\trxdctl |= IXGBEVF_MAX_FRAME_BUILD_SKB |\n\t\t\t\t  IXGBE_RXDCTL_RLPML_EN;\n#endif\n\t}\n\n\trxdctl |= IXGBE_RXDCTL_ENABLE | IXGBE_RXDCTL_VME;\n\tIXGBE_WRITE_REG(hw, IXGBE_VFRXDCTL(reg_idx), rxdctl);\n\n\tixgbevf_rx_desc_queue_enable(adapter, ring);\n\tixgbevf_alloc_rx_buffers(ring, ixgbevf_desc_unused(ring));\n}\n\nstatic void ixgbevf_set_rx_buffer_len(struct ixgbevf_adapter *adapter,\n\t\t\t\t      struct ixgbevf_ring *rx_ring)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tunsigned int max_frame = netdev->mtu + ETH_HLEN + ETH_FCS_LEN;\n\n\t \n\tclear_ring_build_skb_enabled(rx_ring);\n\tclear_ring_uses_large_buffer(rx_ring);\n\n\tif (adapter->flags & IXGBEVF_FLAGS_LEGACY_RX)\n\t\treturn;\n\n\tif (PAGE_SIZE < 8192)\n\t\tif (max_frame > IXGBEVF_MAX_FRAME_BUILD_SKB)\n\t\t\tset_ring_uses_large_buffer(rx_ring);\n\n\t \n\tif (adapter->hw.mac.type == ixgbe_mac_82599_vf && !ring_uses_large_buffer(rx_ring))\n\t\treturn;\n\n\tset_ring_build_skb_enabled(rx_ring);\n}\n\n \nstatic void ixgbevf_configure_rx(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct net_device *netdev = adapter->netdev;\n\tint i, ret;\n\n\tixgbevf_setup_psrtype(adapter);\n\tif (hw->mac.type >= ixgbe_mac_X550_vf)\n\t\tixgbevf_setup_vfmrqc(adapter);\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\t \n\tret = hw->mac.ops.set_rlpml(hw, netdev->mtu + ETH_HLEN + ETH_FCS_LEN);\n\tspin_unlock_bh(&adapter->mbx_lock);\n\tif (ret)\n\t\tdev_err(&adapter->pdev->dev,\n\t\t\t\"Failed to set MTU at %d\\n\", netdev->mtu);\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct ixgbevf_ring *rx_ring = adapter->rx_ring[i];\n\n\t\tixgbevf_set_rx_buffer_len(adapter, rx_ring);\n\t\tixgbevf_configure_rx_ring(adapter, rx_ring);\n\t}\n}\n\nstatic int ixgbevf_vlan_rx_add_vid(struct net_device *netdev,\n\t\t\t\t   __be16 proto, u16 vid)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint err;\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\n\t \n\terr = hw->mac.ops.set_vfta(hw, vid, 0, true);\n\n\tspin_unlock_bh(&adapter->mbx_lock);\n\n\tif (err) {\n\t\tnetdev_err(netdev, \"VF could not set VLAN %d\\n\", vid);\n\n\t\t \n\t\tif (err == IXGBE_ERR_MBX)\n\t\t\treturn -EIO;\n\n\t\tif (err == IXGBE_ERR_INVALID_ARGUMENT)\n\t\t\treturn -EACCES;\n\t}\n\n\tset_bit(vid, adapter->active_vlans);\n\n\treturn err;\n}\n\nstatic int ixgbevf_vlan_rx_kill_vid(struct net_device *netdev,\n\t\t\t\t    __be16 proto, u16 vid)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint err;\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\n\t \n\terr = hw->mac.ops.set_vfta(hw, vid, 0, false);\n\n\tspin_unlock_bh(&adapter->mbx_lock);\n\n\tif (err)\n\t\tnetdev_err(netdev, \"Could not remove VLAN %d\\n\", vid);\n\n\tclear_bit(vid, adapter->active_vlans);\n\n\treturn err;\n}\n\nstatic void ixgbevf_restore_vlan(struct ixgbevf_adapter *adapter)\n{\n\tu16 vid;\n\n\tfor_each_set_bit(vid, adapter->active_vlans, VLAN_N_VID)\n\t\tixgbevf_vlan_rx_add_vid(adapter->netdev,\n\t\t\t\t\thtons(ETH_P_8021Q), vid);\n}\n\nstatic int ixgbevf_write_uc_addr_list(struct net_device *netdev)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint count = 0;\n\n\tif (!netdev_uc_empty(netdev)) {\n\t\tstruct netdev_hw_addr *ha;\n\n\t\tnetdev_for_each_uc_addr(ha, netdev) {\n\t\t\thw->mac.ops.set_uc_addr(hw, ++count, ha->addr);\n\t\t\tudelay(200);\n\t\t}\n\t} else {\n\t\t \n\t\thw->mac.ops.set_uc_addr(hw, 0, NULL);\n\t}\n\n\treturn count;\n}\n\n \nstatic void ixgbevf_set_rx_mode(struct net_device *netdev)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tunsigned int flags = netdev->flags;\n\tint xcast_mode;\n\n\t \n\tif (flags & IFF_PROMISC)\n\t\txcast_mode = IXGBEVF_XCAST_MODE_PROMISC;\n\telse if (flags & IFF_ALLMULTI)\n\t\txcast_mode = IXGBEVF_XCAST_MODE_ALLMULTI;\n\telse if (flags & (IFF_BROADCAST | IFF_MULTICAST))\n\t\txcast_mode = IXGBEVF_XCAST_MODE_MULTI;\n\telse\n\t\txcast_mode = IXGBEVF_XCAST_MODE_NONE;\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\n\thw->mac.ops.update_xcast_mode(hw, xcast_mode);\n\n\t \n\thw->mac.ops.update_mc_addr_list(hw, netdev);\n\n\tixgbevf_write_uc_addr_list(netdev);\n\n\tspin_unlock_bh(&adapter->mbx_lock);\n}\n\nstatic void ixgbevf_napi_enable_all(struct ixgbevf_adapter *adapter)\n{\n\tint q_idx;\n\tstruct ixgbevf_q_vector *q_vector;\n\tint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\n\n\tfor (q_idx = 0; q_idx < q_vectors; q_idx++) {\n\t\tq_vector = adapter->q_vector[q_idx];\n\t\tnapi_enable(&q_vector->napi);\n\t}\n}\n\nstatic void ixgbevf_napi_disable_all(struct ixgbevf_adapter *adapter)\n{\n\tint q_idx;\n\tstruct ixgbevf_q_vector *q_vector;\n\tint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\n\n\tfor (q_idx = 0; q_idx < q_vectors; q_idx++) {\n\t\tq_vector = adapter->q_vector[q_idx];\n\t\tnapi_disable(&q_vector->napi);\n\t}\n}\n\nstatic int ixgbevf_configure_dcb(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tunsigned int def_q = 0;\n\tunsigned int num_tcs = 0;\n\tunsigned int num_rx_queues = adapter->num_rx_queues;\n\tunsigned int num_tx_queues = adapter->num_tx_queues;\n\tint err;\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\n\t \n\terr = ixgbevf_get_queues(hw, &num_tcs, &def_q);\n\n\tspin_unlock_bh(&adapter->mbx_lock);\n\n\tif (err)\n\t\treturn err;\n\n\tif (num_tcs > 1) {\n\t\t \n\t\tnum_tx_queues = 1;\n\n\t\t \n\t\tadapter->tx_ring[0]->reg_idx = def_q;\n\n\t\t \n\t\tnum_rx_queues = num_tcs;\n\t}\n\n\t \n\tif ((adapter->num_rx_queues != num_rx_queues) ||\n\t    (adapter->num_tx_queues != num_tx_queues)) {\n\t\t \n\t\thw->mbx.timeout = 0;\n\n\t\t \n\t\tset_bit(__IXGBEVF_QUEUE_RESET_REQUESTED, &adapter->state);\n\t}\n\n\treturn 0;\n}\n\nstatic void ixgbevf_configure(struct ixgbevf_adapter *adapter)\n{\n\tixgbevf_configure_dcb(adapter);\n\n\tixgbevf_set_rx_mode(adapter->netdev);\n\n\tixgbevf_restore_vlan(adapter);\n\tixgbevf_ipsec_restore(adapter);\n\n\tixgbevf_configure_tx(adapter);\n\tixgbevf_configure_rx(adapter);\n}\n\nstatic void ixgbevf_save_reset_stats(struct ixgbevf_adapter *adapter)\n{\n\t \n\tif (adapter->stats.vfgprc || adapter->stats.vfgptc) {\n\t\tadapter->stats.saved_reset_vfgprc += adapter->stats.vfgprc -\n\t\t\tadapter->stats.base_vfgprc;\n\t\tadapter->stats.saved_reset_vfgptc += adapter->stats.vfgptc -\n\t\t\tadapter->stats.base_vfgptc;\n\t\tadapter->stats.saved_reset_vfgorc += adapter->stats.vfgorc -\n\t\t\tadapter->stats.base_vfgorc;\n\t\tadapter->stats.saved_reset_vfgotc += adapter->stats.vfgotc -\n\t\t\tadapter->stats.base_vfgotc;\n\t\tadapter->stats.saved_reset_vfmprc += adapter->stats.vfmprc -\n\t\t\tadapter->stats.base_vfmprc;\n\t}\n}\n\nstatic void ixgbevf_init_last_counter_stats(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tadapter->stats.last_vfgprc = IXGBE_READ_REG(hw, IXGBE_VFGPRC);\n\tadapter->stats.last_vfgorc = IXGBE_READ_REG(hw, IXGBE_VFGORC_LSB);\n\tadapter->stats.last_vfgorc |=\n\t\t(((u64)(IXGBE_READ_REG(hw, IXGBE_VFGORC_MSB))) << 32);\n\tadapter->stats.last_vfgptc = IXGBE_READ_REG(hw, IXGBE_VFGPTC);\n\tadapter->stats.last_vfgotc = IXGBE_READ_REG(hw, IXGBE_VFGOTC_LSB);\n\tadapter->stats.last_vfgotc |=\n\t\t(((u64)(IXGBE_READ_REG(hw, IXGBE_VFGOTC_MSB))) << 32);\n\tadapter->stats.last_vfmprc = IXGBE_READ_REG(hw, IXGBE_VFMPRC);\n\n\tadapter->stats.base_vfgprc = adapter->stats.last_vfgprc;\n\tadapter->stats.base_vfgorc = adapter->stats.last_vfgorc;\n\tadapter->stats.base_vfgptc = adapter->stats.last_vfgptc;\n\tadapter->stats.base_vfgotc = adapter->stats.last_vfgotc;\n\tadapter->stats.base_vfmprc = adapter->stats.last_vfmprc;\n}\n\nstatic void ixgbevf_negotiate_api(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstatic const int api[] = {\n\t\tixgbe_mbox_api_15,\n\t\tixgbe_mbox_api_14,\n\t\tixgbe_mbox_api_13,\n\t\tixgbe_mbox_api_12,\n\t\tixgbe_mbox_api_11,\n\t\tixgbe_mbox_api_10,\n\t\tixgbe_mbox_api_unknown\n\t};\n\tint err, idx = 0;\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\n\twhile (api[idx] != ixgbe_mbox_api_unknown) {\n\t\terr = hw->mac.ops.negotiate_api_version(hw, api[idx]);\n\t\tif (!err)\n\t\t\tbreak;\n\t\tidx++;\n\t}\n\n\tif (hw->api_version >= ixgbe_mbox_api_15) {\n\t\thw->mbx.ops.init_params(hw);\n\t\tmemcpy(&hw->mbx.ops, &ixgbevf_mbx_ops,\n\t\t       sizeof(struct ixgbe_mbx_operations));\n\t}\n\n\tspin_unlock_bh(&adapter->mbx_lock);\n}\n\nstatic void ixgbevf_up_complete(struct ixgbevf_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tbool state;\n\n\tixgbevf_configure_msix(adapter);\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\n\tif (is_valid_ether_addr(hw->mac.addr))\n\t\thw->mac.ops.set_rar(hw, 0, hw->mac.addr, 0);\n\telse\n\t\thw->mac.ops.set_rar(hw, 0, hw->mac.perm_addr, 0);\n\n\tspin_unlock_bh(&adapter->mbx_lock);\n\n\tstate = adapter->link_state;\n\thw->mac.ops.get_link_state(hw, &adapter->link_state);\n\tif (state && state != adapter->link_state)\n\t\tdev_info(&pdev->dev, \"VF is administratively disabled\\n\");\n\n\tsmp_mb__before_atomic();\n\tclear_bit(__IXGBEVF_DOWN, &adapter->state);\n\tixgbevf_napi_enable_all(adapter);\n\n\t \n\tIXGBE_READ_REG(hw, IXGBE_VTEICR);\n\tixgbevf_irq_enable(adapter);\n\n\t \n\tnetif_tx_start_all_queues(netdev);\n\n\tixgbevf_save_reset_stats(adapter);\n\tixgbevf_init_last_counter_stats(adapter);\n\n\thw->mac.get_link_status = 1;\n\tmod_timer(&adapter->service_timer, jiffies);\n}\n\nvoid ixgbevf_up(struct ixgbevf_adapter *adapter)\n{\n\tixgbevf_configure(adapter);\n\n\tixgbevf_up_complete(adapter);\n}\n\n \nstatic void ixgbevf_clean_rx_ring(struct ixgbevf_ring *rx_ring)\n{\n\tu16 i = rx_ring->next_to_clean;\n\n\t \n\tif (rx_ring->skb) {\n\t\tdev_kfree_skb(rx_ring->skb);\n\t\trx_ring->skb = NULL;\n\t}\n\n\t \n\twhile (i != rx_ring->next_to_alloc) {\n\t\tstruct ixgbevf_rx_buffer *rx_buffer;\n\n\t\trx_buffer = &rx_ring->rx_buffer_info[i];\n\n\t\t \n\t\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t\t      rx_buffer->dma,\n\t\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t\t      ixgbevf_rx_bufsz(rx_ring),\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev,\n\t\t\t\t     rx_buffer->dma,\n\t\t\t\t     ixgbevf_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t     IXGBEVF_RX_DMA_ATTR);\n\n\t\t__page_frag_cache_drain(rx_buffer->page,\n\t\t\t\t\trx_buffer->pagecnt_bias);\n\n\t\ti++;\n\t\tif (i == rx_ring->count)\n\t\t\ti = 0;\n\t}\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n}\n\n \nstatic void ixgbevf_clean_tx_ring(struct ixgbevf_ring *tx_ring)\n{\n\tu16 i = tx_ring->next_to_clean;\n\tstruct ixgbevf_tx_buffer *tx_buffer = &tx_ring->tx_buffer_info[i];\n\n\twhile (i != tx_ring->next_to_use) {\n\t\tunion ixgbe_adv_tx_desc *eop_desc, *tx_desc;\n\n\t\t \n\t\tif (ring_is_xdp(tx_ring))\n\t\t\tpage_frag_free(tx_buffer->data);\n\t\telse\n\t\t\tdev_kfree_skb_any(tx_buffer->skb);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\teop_desc = tx_buffer->next_to_watch;\n\t\ttx_desc = IXGBEVF_TX_DESC(tx_ring, i);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\t\ti = 0;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = IXGBEVF_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t}\n\n\t\t \n\t\ttx_buffer++;\n\t\ti++;\n\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\ti = 0;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t}\n\t}\n\n\t \n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\n}\n\n \nstatic void ixgbevf_clean_all_rx_rings(struct ixgbevf_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tixgbevf_clean_rx_ring(adapter->rx_ring[i]);\n}\n\n \nstatic void ixgbevf_clean_all_tx_rings(struct ixgbevf_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tixgbevf_clean_tx_ring(adapter->tx_ring[i]);\n\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\tixgbevf_clean_tx_ring(adapter->xdp_ring[i]);\n}\n\nvoid ixgbevf_down(struct ixgbevf_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\t \n\tif (test_and_set_bit(__IXGBEVF_DOWN, &adapter->state))\n\t\treturn;  \n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tixgbevf_disable_rx_queue(adapter, adapter->rx_ring[i]);\n\n\tusleep_range(10000, 20000);\n\n\tnetif_tx_stop_all_queues(netdev);\n\n\t \n\tnetif_carrier_off(netdev);\n\tnetif_tx_disable(netdev);\n\n\tixgbevf_irq_disable(adapter);\n\n\tixgbevf_napi_disable_all(adapter);\n\n\tdel_timer_sync(&adapter->service_timer);\n\n\t \n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tu8 reg_idx = adapter->tx_ring[i]->reg_idx;\n\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(reg_idx),\n\t\t\t\tIXGBE_TXDCTL_SWFLSH);\n\t}\n\n\tfor (i = 0; i < adapter->num_xdp_queues; i++) {\n\t\tu8 reg_idx = adapter->xdp_ring[i]->reg_idx;\n\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(reg_idx),\n\t\t\t\tIXGBE_TXDCTL_SWFLSH);\n\t}\n\n\tif (!pci_channel_offline(adapter->pdev))\n\t\tixgbevf_reset(adapter);\n\n\tixgbevf_clean_all_tx_rings(adapter);\n\tixgbevf_clean_all_rx_rings(adapter);\n}\n\nvoid ixgbevf_reinit_locked(struct ixgbevf_adapter *adapter)\n{\n\twhile (test_and_set_bit(__IXGBEVF_RESETTING, &adapter->state))\n\t\tmsleep(1);\n\n\tixgbevf_down(adapter);\n\tpci_set_master(adapter->pdev);\n\tixgbevf_up(adapter);\n\n\tclear_bit(__IXGBEVF_RESETTING, &adapter->state);\n}\n\nvoid ixgbevf_reset(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct net_device *netdev = adapter->netdev;\n\n\tif (hw->mac.ops.reset_hw(hw)) {\n\t\thw_dbg(hw, \"PF still resetting\\n\");\n\t} else {\n\t\thw->mac.ops.init_hw(hw);\n\t\tixgbevf_negotiate_api(adapter);\n\t}\n\n\tif (is_valid_ether_addr(adapter->hw.mac.addr)) {\n\t\teth_hw_addr_set(netdev, adapter->hw.mac.addr);\n\t\tether_addr_copy(netdev->perm_addr, adapter->hw.mac.addr);\n\t}\n\n\tadapter->last_reset = jiffies;\n}\n\nstatic int ixgbevf_acquire_msix_vectors(struct ixgbevf_adapter *adapter,\n\t\t\t\t\tint vectors)\n{\n\tint vector_threshold;\n\n\t \n\tvector_threshold = MIN_MSIX_COUNT;\n\n\t \n\tvectors = pci_enable_msix_range(adapter->pdev, adapter->msix_entries,\n\t\t\t\t\tvector_threshold, vectors);\n\n\tif (vectors < 0) {\n\t\tdev_err(&adapter->pdev->dev,\n\t\t\t\"Unable to allocate MSI-X interrupts\\n\");\n\t\tkfree(adapter->msix_entries);\n\t\tadapter->msix_entries = NULL;\n\t\treturn vectors;\n\t}\n\n\t \n\tadapter->num_msix_vectors = vectors;\n\n\treturn 0;\n}\n\n \nstatic void ixgbevf_set_num_queues(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tunsigned int def_q = 0;\n\tunsigned int num_tcs = 0;\n\tint err;\n\n\t \n\tadapter->num_rx_queues = 1;\n\tadapter->num_tx_queues = 1;\n\tadapter->num_xdp_queues = 0;\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\n\t \n\terr = ixgbevf_get_queues(hw, &num_tcs, &def_q);\n\n\tspin_unlock_bh(&adapter->mbx_lock);\n\n\tif (err)\n\t\treturn;\n\n\t \n\tif (num_tcs > 1) {\n\t\tadapter->num_rx_queues = num_tcs;\n\t} else {\n\t\tu16 rss = min_t(u16, num_online_cpus(), IXGBEVF_MAX_RSS_QUEUES);\n\n\t\tswitch (hw->api_version) {\n\t\tcase ixgbe_mbox_api_11:\n\t\tcase ixgbe_mbox_api_12:\n\t\tcase ixgbe_mbox_api_13:\n\t\tcase ixgbe_mbox_api_14:\n\t\tcase ixgbe_mbox_api_15:\n\t\t\tif (adapter->xdp_prog &&\n\t\t\t    hw->mac.max_tx_queues == rss)\n\t\t\t\trss = rss > 3 ? 2 : 1;\n\n\t\t\tadapter->num_rx_queues = rss;\n\t\t\tadapter->num_tx_queues = rss;\n\t\t\tadapter->num_xdp_queues = adapter->xdp_prog ? rss : 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic int ixgbevf_set_interrupt_capability(struct ixgbevf_adapter *adapter)\n{\n\tint vector, v_budget;\n\n\t \n\tv_budget = max(adapter->num_rx_queues, adapter->num_tx_queues);\n\tv_budget = min_t(int, v_budget, num_online_cpus());\n\tv_budget += NON_Q_VECTORS;\n\n\tadapter->msix_entries = kcalloc(v_budget,\n\t\t\t\t\tsizeof(struct msix_entry), GFP_KERNEL);\n\tif (!adapter->msix_entries)\n\t\treturn -ENOMEM;\n\n\tfor (vector = 0; vector < v_budget; vector++)\n\t\tadapter->msix_entries[vector].entry = vector;\n\n\t \n\treturn ixgbevf_acquire_msix_vectors(adapter, v_budget);\n}\n\nstatic void ixgbevf_add_ring(struct ixgbevf_ring *ring,\n\t\t\t     struct ixgbevf_ring_container *head)\n{\n\tring->next = head->ring;\n\thead->ring = ring;\n\thead->count++;\n}\n\n \nstatic int ixgbevf_alloc_q_vector(struct ixgbevf_adapter *adapter, int v_idx,\n\t\t\t\t  int txr_count, int txr_idx,\n\t\t\t\t  int xdp_count, int xdp_idx,\n\t\t\t\t  int rxr_count, int rxr_idx)\n{\n\tstruct ixgbevf_q_vector *q_vector;\n\tint reg_idx = txr_idx + xdp_idx;\n\tstruct ixgbevf_ring *ring;\n\tint ring_count, size;\n\n\tring_count = txr_count + xdp_count + rxr_count;\n\tsize = sizeof(*q_vector) + (sizeof(*ring) * ring_count);\n\n\t \n\tq_vector = kzalloc(size, GFP_KERNEL);\n\tif (!q_vector)\n\t\treturn -ENOMEM;\n\n\t \n\tnetif_napi_add(adapter->netdev, &q_vector->napi, ixgbevf_poll);\n\n\t \n\tadapter->q_vector[v_idx] = q_vector;\n\tq_vector->adapter = adapter;\n\tq_vector->v_idx = v_idx;\n\n\t \n\tring = q_vector->ring;\n\n\twhile (txr_count) {\n\t\t \n\t\tring->dev = &adapter->pdev->dev;\n\t\tring->netdev = adapter->netdev;\n\n\t\t \n\t\tring->q_vector = q_vector;\n\n\t\t \n\t\tixgbevf_add_ring(ring, &q_vector->tx);\n\n\t\t \n\t\tring->count = adapter->tx_ring_count;\n\t\tring->queue_index = txr_idx;\n\t\tring->reg_idx = reg_idx;\n\n\t\t \n\t\tadapter->tx_ring[txr_idx] = ring;\n\n\t\t \n\t\ttxr_count--;\n\t\ttxr_idx++;\n\t\treg_idx++;\n\n\t\t \n\t\tring++;\n\t}\n\n\twhile (xdp_count) {\n\t\t \n\t\tring->dev = &adapter->pdev->dev;\n\t\tring->netdev = adapter->netdev;\n\n\t\t \n\t\tring->q_vector = q_vector;\n\n\t\t \n\t\tixgbevf_add_ring(ring, &q_vector->tx);\n\n\t\t \n\t\tring->count = adapter->tx_ring_count;\n\t\tring->queue_index = xdp_idx;\n\t\tring->reg_idx = reg_idx;\n\t\tset_ring_xdp(ring);\n\n\t\t \n\t\tadapter->xdp_ring[xdp_idx] = ring;\n\n\t\t \n\t\txdp_count--;\n\t\txdp_idx++;\n\t\treg_idx++;\n\n\t\t \n\t\tring++;\n\t}\n\n\twhile (rxr_count) {\n\t\t \n\t\tring->dev = &adapter->pdev->dev;\n\t\tring->netdev = adapter->netdev;\n\n\t\t \n\t\tring->q_vector = q_vector;\n\n\t\t \n\t\tixgbevf_add_ring(ring, &q_vector->rx);\n\n\t\t \n\t\tring->count = adapter->rx_ring_count;\n\t\tring->queue_index = rxr_idx;\n\t\tring->reg_idx = rxr_idx;\n\n\t\t \n\t\tadapter->rx_ring[rxr_idx] = ring;\n\n\t\t \n\t\trxr_count--;\n\t\trxr_idx++;\n\n\t\t \n\t\tring++;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void ixgbevf_free_q_vector(struct ixgbevf_adapter *adapter, int v_idx)\n{\n\tstruct ixgbevf_q_vector *q_vector = adapter->q_vector[v_idx];\n\tstruct ixgbevf_ring *ring;\n\n\tixgbevf_for_each_ring(ring, q_vector->tx) {\n\t\tif (ring_is_xdp(ring))\n\t\t\tadapter->xdp_ring[ring->queue_index] = NULL;\n\t\telse\n\t\t\tadapter->tx_ring[ring->queue_index] = NULL;\n\t}\n\n\tixgbevf_for_each_ring(ring, q_vector->rx)\n\t\tadapter->rx_ring[ring->queue_index] = NULL;\n\n\tadapter->q_vector[v_idx] = NULL;\n\tnetif_napi_del(&q_vector->napi);\n\n\t \n\tkfree_rcu(q_vector, rcu);\n}\n\n \nstatic int ixgbevf_alloc_q_vectors(struct ixgbevf_adapter *adapter)\n{\n\tint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\n\tint rxr_remaining = adapter->num_rx_queues;\n\tint txr_remaining = adapter->num_tx_queues;\n\tint xdp_remaining = adapter->num_xdp_queues;\n\tint rxr_idx = 0, txr_idx = 0, xdp_idx = 0, v_idx = 0;\n\tint err;\n\n\tif (q_vectors >= (rxr_remaining + txr_remaining + xdp_remaining)) {\n\t\tfor (; rxr_remaining; v_idx++, q_vectors--) {\n\t\t\tint rqpv = DIV_ROUND_UP(rxr_remaining, q_vectors);\n\n\t\t\terr = ixgbevf_alloc_q_vector(adapter, v_idx,\n\t\t\t\t\t\t     0, 0, 0, 0, rqpv, rxr_idx);\n\t\t\tif (err)\n\t\t\t\tgoto err_out;\n\n\t\t\t \n\t\t\trxr_remaining -= rqpv;\n\t\t\trxr_idx += rqpv;\n\t\t}\n\t}\n\n\tfor (; q_vectors; v_idx++, q_vectors--) {\n\t\tint rqpv = DIV_ROUND_UP(rxr_remaining, q_vectors);\n\t\tint tqpv = DIV_ROUND_UP(txr_remaining, q_vectors);\n\t\tint xqpv = DIV_ROUND_UP(xdp_remaining, q_vectors);\n\n\t\terr = ixgbevf_alloc_q_vector(adapter, v_idx,\n\t\t\t\t\t     tqpv, txr_idx,\n\t\t\t\t\t     xqpv, xdp_idx,\n\t\t\t\t\t     rqpv, rxr_idx);\n\n\t\tif (err)\n\t\t\tgoto err_out;\n\n\t\t \n\t\trxr_remaining -= rqpv;\n\t\trxr_idx += rqpv;\n\t\ttxr_remaining -= tqpv;\n\t\ttxr_idx += tqpv;\n\t\txdp_remaining -= xqpv;\n\t\txdp_idx += xqpv;\n\t}\n\n\treturn 0;\n\nerr_out:\n\twhile (v_idx) {\n\t\tv_idx--;\n\t\tixgbevf_free_q_vector(adapter, v_idx);\n\t}\n\n\treturn -ENOMEM;\n}\n\n \nstatic void ixgbevf_free_q_vectors(struct ixgbevf_adapter *adapter)\n{\n\tint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\n\n\twhile (q_vectors) {\n\t\tq_vectors--;\n\t\tixgbevf_free_q_vector(adapter, q_vectors);\n\t}\n}\n\n \nstatic void ixgbevf_reset_interrupt_capability(struct ixgbevf_adapter *adapter)\n{\n\tif (!adapter->msix_entries)\n\t\treturn;\n\n\tpci_disable_msix(adapter->pdev);\n\tkfree(adapter->msix_entries);\n\tadapter->msix_entries = NULL;\n}\n\n \nstatic int ixgbevf_init_interrupt_scheme(struct ixgbevf_adapter *adapter)\n{\n\tint err;\n\n\t \n\tixgbevf_set_num_queues(adapter);\n\n\terr = ixgbevf_set_interrupt_capability(adapter);\n\tif (err) {\n\t\thw_dbg(&adapter->hw,\n\t\t       \"Unable to setup interrupt capabilities\\n\");\n\t\tgoto err_set_interrupt;\n\t}\n\n\terr = ixgbevf_alloc_q_vectors(adapter);\n\tif (err) {\n\t\thw_dbg(&adapter->hw, \"Unable to allocate memory for queue vectors\\n\");\n\t\tgoto err_alloc_q_vectors;\n\t}\n\n\thw_dbg(&adapter->hw, \"Multiqueue %s: Rx Queue count = %u, Tx Queue count = %u XDP Queue count %u\\n\",\n\t       (adapter->num_rx_queues > 1) ? \"Enabled\" : \"Disabled\",\n\t       adapter->num_rx_queues, adapter->num_tx_queues,\n\t       adapter->num_xdp_queues);\n\n\tset_bit(__IXGBEVF_DOWN, &adapter->state);\n\n\treturn 0;\nerr_alloc_q_vectors:\n\tixgbevf_reset_interrupt_capability(adapter);\nerr_set_interrupt:\n\treturn err;\n}\n\n \nstatic void ixgbevf_clear_interrupt_scheme(struct ixgbevf_adapter *adapter)\n{\n\tadapter->num_tx_queues = 0;\n\tadapter->num_xdp_queues = 0;\n\tadapter->num_rx_queues = 0;\n\n\tixgbevf_free_q_vectors(adapter);\n\tixgbevf_reset_interrupt_capability(adapter);\n}\n\n \nstatic int ixgbevf_sw_init(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct net_device *netdev = adapter->netdev;\n\tint err;\n\n\t \n\thw->vendor_id = pdev->vendor;\n\thw->device_id = pdev->device;\n\thw->revision_id = pdev->revision;\n\thw->subsystem_vendor_id = pdev->subsystem_vendor;\n\thw->subsystem_device_id = pdev->subsystem_device;\n\n\thw->mbx.ops.init_params(hw);\n\n\tif (hw->mac.type >= ixgbe_mac_X550_vf) {\n\t\terr = ixgbevf_init_rss_key(adapter);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\t \n\thw->mac.max_tx_queues = 2;\n\thw->mac.max_rx_queues = 2;\n\n\t \n\tspin_lock_init(&adapter->mbx_lock);\n\n\terr = hw->mac.ops.reset_hw(hw);\n\tif (err) {\n\t\tdev_info(&pdev->dev,\n\t\t\t \"PF still in reset state.  Is the PF interface up?\\n\");\n\t} else {\n\t\terr = hw->mac.ops.init_hw(hw);\n\t\tif (err) {\n\t\t\tpr_err(\"init_shared_code failed: %d\\n\", err);\n\t\t\tgoto out;\n\t\t}\n\t\tixgbevf_negotiate_api(adapter);\n\t\terr = hw->mac.ops.get_mac_addr(hw, hw->mac.addr);\n\t\tif (err)\n\t\t\tdev_info(&pdev->dev, \"Error reading MAC address\\n\");\n\t\telse if (is_zero_ether_addr(adapter->hw.mac.addr))\n\t\t\tdev_info(&pdev->dev,\n\t\t\t\t \"MAC address not assigned by administrator.\\n\");\n\t\teth_hw_addr_set(netdev, hw->mac.addr);\n\t}\n\n\tif (!is_valid_ether_addr(netdev->dev_addr)) {\n\t\tdev_info(&pdev->dev, \"Assigning random MAC address\\n\");\n\t\teth_hw_addr_random(netdev);\n\t\tether_addr_copy(hw->mac.addr, netdev->dev_addr);\n\t\tether_addr_copy(hw->mac.perm_addr, netdev->dev_addr);\n\t}\n\n\t \n\tadapter->rx_itr_setting = 1;\n\tadapter->tx_itr_setting = 1;\n\n\t \n\tadapter->tx_ring_count = IXGBEVF_DEFAULT_TXD;\n\tadapter->rx_ring_count = IXGBEVF_DEFAULT_RXD;\n\n\tadapter->link_state = true;\n\n\tset_bit(__IXGBEVF_DOWN, &adapter->state);\n\treturn 0;\n\nout:\n\treturn err;\n}\n\n#define UPDATE_VF_COUNTER_32bit(reg, last_counter, counter)\t\\\n\t{\t\t\t\t\t\t\t\\\n\t\tu32 current_counter = IXGBE_READ_REG(hw, reg);\t\\\n\t\tif (current_counter < last_counter)\t\t\\\n\t\t\tcounter += 0x100000000LL;\t\t\\\n\t\tlast_counter = current_counter;\t\t\t\\\n\t\tcounter &= 0xFFFFFFFF00000000LL;\t\t\\\n\t\tcounter |= current_counter;\t\t\t\\\n\t}\n\n#define UPDATE_VF_COUNTER_36bit(reg_lsb, reg_msb, last_counter, counter) \\\n\t{\t\t\t\t\t\t\t\t \\\n\t\tu64 current_counter_lsb = IXGBE_READ_REG(hw, reg_lsb);\t \\\n\t\tu64 current_counter_msb = IXGBE_READ_REG(hw, reg_msb);\t \\\n\t\tu64 current_counter = (current_counter_msb << 32) |\t \\\n\t\t\tcurrent_counter_lsb;\t\t\t\t \\\n\t\tif (current_counter < last_counter)\t\t\t \\\n\t\t\tcounter += 0x1000000000LL;\t\t\t \\\n\t\tlast_counter = current_counter;\t\t\t\t \\\n\t\tcounter &= 0xFFFFFFF000000000LL;\t\t\t \\\n\t\tcounter |= current_counter;\t\t\t\t \\\n\t}\n \nvoid ixgbevf_update_stats(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu64 alloc_rx_page_failed = 0, alloc_rx_buff_failed = 0;\n\tu64 alloc_rx_page = 0, hw_csum_rx_error = 0;\n\tint i;\n\n\tif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\n\t    test_bit(__IXGBEVF_RESETTING, &adapter->state))\n\t\treturn;\n\n\tUPDATE_VF_COUNTER_32bit(IXGBE_VFGPRC, adapter->stats.last_vfgprc,\n\t\t\t\tadapter->stats.vfgprc);\n\tUPDATE_VF_COUNTER_32bit(IXGBE_VFGPTC, adapter->stats.last_vfgptc,\n\t\t\t\tadapter->stats.vfgptc);\n\tUPDATE_VF_COUNTER_36bit(IXGBE_VFGORC_LSB, IXGBE_VFGORC_MSB,\n\t\t\t\tadapter->stats.last_vfgorc,\n\t\t\t\tadapter->stats.vfgorc);\n\tUPDATE_VF_COUNTER_36bit(IXGBE_VFGOTC_LSB, IXGBE_VFGOTC_MSB,\n\t\t\t\tadapter->stats.last_vfgotc,\n\t\t\t\tadapter->stats.vfgotc);\n\tUPDATE_VF_COUNTER_32bit(IXGBE_VFMPRC, adapter->stats.last_vfmprc,\n\t\t\t\tadapter->stats.vfmprc);\n\n\tfor (i = 0;  i  < adapter->num_rx_queues;  i++) {\n\t\tstruct ixgbevf_ring *rx_ring = adapter->rx_ring[i];\n\n\t\thw_csum_rx_error += rx_ring->rx_stats.csum_err;\n\t\talloc_rx_page_failed += rx_ring->rx_stats.alloc_rx_page_failed;\n\t\talloc_rx_buff_failed += rx_ring->rx_stats.alloc_rx_buff_failed;\n\t\talloc_rx_page += rx_ring->rx_stats.alloc_rx_page;\n\t}\n\n\tadapter->hw_csum_rx_error = hw_csum_rx_error;\n\tadapter->alloc_rx_page_failed = alloc_rx_page_failed;\n\tadapter->alloc_rx_buff_failed = alloc_rx_buff_failed;\n\tadapter->alloc_rx_page = alloc_rx_page;\n}\n\n \nstatic void ixgbevf_service_timer(struct timer_list *t)\n{\n\tstruct ixgbevf_adapter *adapter = from_timer(adapter, t,\n\t\t\t\t\t\t     service_timer);\n\n\t \n\tmod_timer(&adapter->service_timer, (HZ * 2) + jiffies);\n\n\tixgbevf_service_event_schedule(adapter);\n}\n\nstatic void ixgbevf_reset_subtask(struct ixgbevf_adapter *adapter)\n{\n\tif (!test_and_clear_bit(__IXGBEVF_RESET_REQUESTED, &adapter->state))\n\t\treturn;\n\n\trtnl_lock();\n\t \n\tif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\n\t    test_bit(__IXGBEVF_REMOVING, &adapter->state) ||\n\t    test_bit(__IXGBEVF_RESETTING, &adapter->state)) {\n\t\trtnl_unlock();\n\t\treturn;\n\t}\n\n\tadapter->tx_timeout_count++;\n\n\tixgbevf_reinit_locked(adapter);\n\trtnl_unlock();\n}\n\n \nstatic void ixgbevf_check_hang_subtask(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 eics = 0;\n\tint i;\n\n\t \n\tif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\n\t    test_bit(__IXGBEVF_RESETTING, &adapter->state))\n\t\treturn;\n\n\t \n\tif (netif_carrier_ok(adapter->netdev)) {\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\tset_check_for_tx_hang(adapter->tx_ring[i]);\n\t\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\t\tset_check_for_tx_hang(adapter->xdp_ring[i]);\n\t}\n\n\t \n\tfor (i = 0; i < adapter->num_msix_vectors - NON_Q_VECTORS; i++) {\n\t\tstruct ixgbevf_q_vector *qv = adapter->q_vector[i];\n\n\t\tif (qv->rx.ring || qv->tx.ring)\n\t\t\teics |= BIT(i);\n\t}\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_VTEICS, eics);\n}\n\n \nstatic void ixgbevf_watchdog_update_link(struct ixgbevf_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 link_speed = adapter->link_speed;\n\tbool link_up = adapter->link_up;\n\ts32 err;\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\n\terr = hw->mac.ops.check_link(hw, &link_speed, &link_up, false);\n\n\tspin_unlock_bh(&adapter->mbx_lock);\n\n\t \n\tif (err && time_after(jiffies, adapter->last_reset + (10 * HZ))) {\n\t\tset_bit(__IXGBEVF_RESET_REQUESTED, &adapter->state);\n\t\tlink_up = false;\n\t}\n\n\tadapter->link_up = link_up;\n\tadapter->link_speed = link_speed;\n}\n\n \nstatic void ixgbevf_watchdog_link_is_up(struct ixgbevf_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\n\t \n\tif (netif_carrier_ok(netdev))\n\t\treturn;\n\n\tdev_info(&adapter->pdev->dev, \"NIC Link is Up %s\\n\",\n\t\t (adapter->link_speed == IXGBE_LINK_SPEED_10GB_FULL) ?\n\t\t \"10 Gbps\" :\n\t\t (adapter->link_speed == IXGBE_LINK_SPEED_1GB_FULL) ?\n\t\t \"1 Gbps\" :\n\t\t (adapter->link_speed == IXGBE_LINK_SPEED_100_FULL) ?\n\t\t \"100 Mbps\" :\n\t\t \"unknown speed\");\n\n\tnetif_carrier_on(netdev);\n}\n\n \nstatic void ixgbevf_watchdog_link_is_down(struct ixgbevf_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\n\tadapter->link_speed = 0;\n\n\t \n\tif (!netif_carrier_ok(netdev))\n\t\treturn;\n\n\tdev_info(&adapter->pdev->dev, \"NIC Link is Down\\n\");\n\n\tnetif_carrier_off(netdev);\n}\n\n \nstatic void ixgbevf_watchdog_subtask(struct ixgbevf_adapter *adapter)\n{\n\t \n\tif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\n\t    test_bit(__IXGBEVF_RESETTING, &adapter->state))\n\t\treturn;\n\n\tixgbevf_watchdog_update_link(adapter);\n\n\tif (adapter->link_up && adapter->link_state)\n\t\tixgbevf_watchdog_link_is_up(adapter);\n\telse\n\t\tixgbevf_watchdog_link_is_down(adapter);\n\n\tixgbevf_update_stats(adapter);\n}\n\n \nstatic void ixgbevf_service_task(struct work_struct *work)\n{\n\tstruct ixgbevf_adapter *adapter = container_of(work,\n\t\t\t\t\t\t       struct ixgbevf_adapter,\n\t\t\t\t\t\t       service_task);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tif (IXGBE_REMOVED(hw->hw_addr)) {\n\t\tif (!test_bit(__IXGBEVF_DOWN, &adapter->state)) {\n\t\t\trtnl_lock();\n\t\t\tixgbevf_down(adapter);\n\t\t\trtnl_unlock();\n\t\t}\n\t\treturn;\n\t}\n\n\tixgbevf_queue_reset_subtask(adapter);\n\tixgbevf_reset_subtask(adapter);\n\tixgbevf_watchdog_subtask(adapter);\n\tixgbevf_check_hang_subtask(adapter);\n\n\tixgbevf_service_event_complete(adapter);\n}\n\n \nvoid ixgbevf_free_tx_resources(struct ixgbevf_ring *tx_ring)\n{\n\tixgbevf_clean_tx_ring(tx_ring);\n\n\tvfree(tx_ring->tx_buffer_info);\n\ttx_ring->tx_buffer_info = NULL;\n\n\t \n\tif (!tx_ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(tx_ring->dev, tx_ring->size, tx_ring->desc,\n\t\t\t  tx_ring->dma);\n\n\ttx_ring->desc = NULL;\n}\n\n \nstatic void ixgbevf_free_all_tx_resources(struct ixgbevf_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tif (adapter->tx_ring[i]->desc)\n\t\t\tixgbevf_free_tx_resources(adapter->tx_ring[i]);\n\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\tif (adapter->xdp_ring[i]->desc)\n\t\t\tixgbevf_free_tx_resources(adapter->xdp_ring[i]);\n}\n\n \nint ixgbevf_setup_tx_resources(struct ixgbevf_ring *tx_ring)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(tx_ring->netdev);\n\tint size;\n\n\tsize = sizeof(struct ixgbevf_tx_buffer) * tx_ring->count;\n\ttx_ring->tx_buffer_info = vmalloc(size);\n\tif (!tx_ring->tx_buffer_info)\n\t\tgoto err;\n\n\tu64_stats_init(&tx_ring->syncp);\n\n\t \n\ttx_ring->size = tx_ring->count * sizeof(union ixgbe_adv_tx_desc);\n\ttx_ring->size = ALIGN(tx_ring->size, 4096);\n\n\ttx_ring->desc = dma_alloc_coherent(tx_ring->dev, tx_ring->size,\n\t\t\t\t\t   &tx_ring->dma, GFP_KERNEL);\n\tif (!tx_ring->desc)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tvfree(tx_ring->tx_buffer_info);\n\ttx_ring->tx_buffer_info = NULL;\n\thw_dbg(&adapter->hw, \"Unable to allocate memory for the transmit descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int ixgbevf_setup_all_tx_resources(struct ixgbevf_adapter *adapter)\n{\n\tint i, j = 0, err = 0;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\terr = ixgbevf_setup_tx_resources(adapter->tx_ring[i]);\n\t\tif (!err)\n\t\t\tcontinue;\n\t\thw_dbg(&adapter->hw, \"Allocation for Tx Queue %u failed\\n\", i);\n\t\tgoto err_setup_tx;\n\t}\n\n\tfor (j = 0; j < adapter->num_xdp_queues; j++) {\n\t\terr = ixgbevf_setup_tx_resources(adapter->xdp_ring[j]);\n\t\tif (!err)\n\t\t\tcontinue;\n\t\thw_dbg(&adapter->hw, \"Allocation for XDP Queue %u failed\\n\", j);\n\t\tgoto err_setup_tx;\n\t}\n\n\treturn 0;\nerr_setup_tx:\n\t \n\twhile (j--)\n\t\tixgbevf_free_tx_resources(adapter->xdp_ring[j]);\n\twhile (i--)\n\t\tixgbevf_free_tx_resources(adapter->tx_ring[i]);\n\n\treturn err;\n}\n\n \nint ixgbevf_setup_rx_resources(struct ixgbevf_adapter *adapter,\n\t\t\t       struct ixgbevf_ring *rx_ring)\n{\n\tint size;\n\n\tsize = sizeof(struct ixgbevf_rx_buffer) * rx_ring->count;\n\trx_ring->rx_buffer_info = vmalloc(size);\n\tif (!rx_ring->rx_buffer_info)\n\t\tgoto err;\n\n\tu64_stats_init(&rx_ring->syncp);\n\n\t \n\trx_ring->size = rx_ring->count * sizeof(union ixgbe_adv_rx_desc);\n\trx_ring->size = ALIGN(rx_ring->size, 4096);\n\n\trx_ring->desc = dma_alloc_coherent(rx_ring->dev, rx_ring->size,\n\t\t\t\t\t   &rx_ring->dma, GFP_KERNEL);\n\n\tif (!rx_ring->desc)\n\t\tgoto err;\n\n\t \n\tif (xdp_rxq_info_reg(&rx_ring->xdp_rxq, adapter->netdev,\n\t\t\t     rx_ring->queue_index, 0) < 0)\n\t\tgoto err;\n\n\trx_ring->xdp_prog = adapter->xdp_prog;\n\n\treturn 0;\nerr:\n\tvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\tdev_err(rx_ring->dev, \"Unable to allocate memory for the Rx descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int ixgbevf_setup_all_rx_resources(struct ixgbevf_adapter *adapter)\n{\n\tint i, err = 0;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\terr = ixgbevf_setup_rx_resources(adapter, adapter->rx_ring[i]);\n\t\tif (!err)\n\t\t\tcontinue;\n\t\thw_dbg(&adapter->hw, \"Allocation for Rx Queue %u failed\\n\", i);\n\t\tgoto err_setup_rx;\n\t}\n\n\treturn 0;\nerr_setup_rx:\n\t \n\twhile (i--)\n\t\tixgbevf_free_rx_resources(adapter->rx_ring[i]);\n\treturn err;\n}\n\n \nvoid ixgbevf_free_rx_resources(struct ixgbevf_ring *rx_ring)\n{\n\tixgbevf_clean_rx_ring(rx_ring);\n\n\trx_ring->xdp_prog = NULL;\n\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\tvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\n\tdma_free_coherent(rx_ring->dev, rx_ring->size, rx_ring->desc,\n\t\t\t  rx_ring->dma);\n\n\trx_ring->desc = NULL;\n}\n\n \nstatic void ixgbevf_free_all_rx_resources(struct ixgbevf_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tif (adapter->rx_ring[i]->desc)\n\t\t\tixgbevf_free_rx_resources(adapter->rx_ring[i]);\n}\n\n \nint ixgbevf_open(struct net_device *netdev)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint err;\n\n\t \n\tif (!adapter->num_msix_vectors)\n\t\treturn -ENOMEM;\n\n\tif (hw->adapter_stopped) {\n\t\tixgbevf_reset(adapter);\n\t\t \n\t\tif (hw->adapter_stopped) {\n\t\t\terr = IXGBE_ERR_MBX;\n\t\t\tpr_err(\"Unable to start - perhaps the PF Driver isn't up yet\\n\");\n\t\t\tgoto err_setup_reset;\n\t\t}\n\t}\n\n\t \n\tif (test_bit(__IXGBEVF_TESTING, &adapter->state))\n\t\treturn -EBUSY;\n\n\tnetif_carrier_off(netdev);\n\n\t \n\terr = ixgbevf_setup_all_tx_resources(adapter);\n\tif (err)\n\t\tgoto err_setup_tx;\n\n\t \n\terr = ixgbevf_setup_all_rx_resources(adapter);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\tixgbevf_configure(adapter);\n\n\terr = ixgbevf_request_irq(adapter);\n\tif (err)\n\t\tgoto err_req_irq;\n\n\t \n\terr = netif_set_real_num_tx_queues(netdev, adapter->num_tx_queues);\n\tif (err)\n\t\tgoto err_set_queues;\n\n\terr = netif_set_real_num_rx_queues(netdev, adapter->num_rx_queues);\n\tif (err)\n\t\tgoto err_set_queues;\n\n\tixgbevf_up_complete(adapter);\n\n\treturn 0;\n\nerr_set_queues:\n\tixgbevf_free_irq(adapter);\nerr_req_irq:\n\tixgbevf_free_all_rx_resources(adapter);\nerr_setup_rx:\n\tixgbevf_free_all_tx_resources(adapter);\nerr_setup_tx:\n\tixgbevf_reset(adapter);\nerr_setup_reset:\n\n\treturn err;\n}\n\n \nstatic void ixgbevf_close_suspend(struct ixgbevf_adapter *adapter)\n{\n\tixgbevf_down(adapter);\n\tixgbevf_free_irq(adapter);\n\tixgbevf_free_all_tx_resources(adapter);\n\tixgbevf_free_all_rx_resources(adapter);\n}\n\n \nint ixgbevf_close(struct net_device *netdev)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\n\tif (netif_device_present(netdev))\n\t\tixgbevf_close_suspend(adapter);\n\n\treturn 0;\n}\n\nstatic void ixgbevf_queue_reset_subtask(struct ixgbevf_adapter *adapter)\n{\n\tstruct net_device *dev = adapter->netdev;\n\n\tif (!test_and_clear_bit(__IXGBEVF_QUEUE_RESET_REQUESTED,\n\t\t\t\t&adapter->state))\n\t\treturn;\n\n\t \n\tif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\n\t    test_bit(__IXGBEVF_RESETTING, &adapter->state))\n\t\treturn;\n\n\t \n\trtnl_lock();\n\n\tif (netif_running(dev))\n\t\tixgbevf_close(dev);\n\n\tixgbevf_clear_interrupt_scheme(adapter);\n\tixgbevf_init_interrupt_scheme(adapter);\n\n\tif (netif_running(dev))\n\t\tixgbevf_open(dev);\n\n\trtnl_unlock();\n}\n\nstatic void ixgbevf_tx_ctxtdesc(struct ixgbevf_ring *tx_ring,\n\t\t\t\tu32 vlan_macip_lens, u32 fceof_saidx,\n\t\t\t\tu32 type_tucmd, u32 mss_l4len_idx)\n{\n\tstruct ixgbe_adv_tx_context_desc *context_desc;\n\tu16 i = tx_ring->next_to_use;\n\n\tcontext_desc = IXGBEVF_TX_CTXTDESC(tx_ring, i);\n\n\ti++;\n\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\t \n\ttype_tucmd |= IXGBE_TXD_CMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;\n\n\tcontext_desc->vlan_macip_lens\t= cpu_to_le32(vlan_macip_lens);\n\tcontext_desc->fceof_saidx\t= cpu_to_le32(fceof_saidx);\n\tcontext_desc->type_tucmd_mlhl\t= cpu_to_le32(type_tucmd);\n\tcontext_desc->mss_l4len_idx\t= cpu_to_le32(mss_l4len_idx);\n}\n\nstatic int ixgbevf_tso(struct ixgbevf_ring *tx_ring,\n\t\t       struct ixgbevf_tx_buffer *first,\n\t\t       u8 *hdr_len,\n\t\t       struct ixgbevf_ipsec_tx_data *itd)\n{\n\tu32 vlan_macip_lens, type_tucmd, mss_l4len_idx;\n\tstruct sk_buff *skb = first->skb;\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tunsigned char *hdr;\n\t} l4;\n\tu32 paylen, l4_offset;\n\tu32 fceof_saidx = 0;\n\tint err;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (eth_p_mpls(first->protocol))\n\t\tip.hdr = skb_inner_network_header(skb);\n\telse\n\t\tip.hdr = skb_network_header(skb);\n\tl4.hdr = skb_checksum_start(skb);\n\n\t \n\ttype_tucmd = IXGBE_ADVTXD_TUCMD_L4T_TCP;\n\n\t \n\tif (ip.v4->version == 4) {\n\t\tunsigned char *csum_start = skb_checksum_start(skb);\n\t\tunsigned char *trans_start = ip.hdr + (ip.v4->ihl * 4);\n\t\tint len = csum_start - trans_start;\n\n\t\t \n\t\tip.v4->check = (skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL) ?\n\t\t\t\t\t   csum_fold(csum_partial(trans_start,\n\t\t\t\t\t\t\t\t  len, 0)) : 0;\n\t\ttype_tucmd |= IXGBE_ADVTXD_TUCMD_IPV4;\n\n\t\tip.v4->tot_len = 0;\n\t\tfirst->tx_flags |= IXGBE_TX_FLAGS_TSO |\n\t\t\t\t   IXGBE_TX_FLAGS_CSUM |\n\t\t\t\t   IXGBE_TX_FLAGS_IPV4;\n\t} else {\n\t\tip.v6->payload_len = 0;\n\t\tfirst->tx_flags |= IXGBE_TX_FLAGS_TSO |\n\t\t\t\t   IXGBE_TX_FLAGS_CSUM;\n\t}\n\n\t \n\tl4_offset = l4.hdr - skb->data;\n\n\t \n\t*hdr_len = (l4.tcp->doff * 4) + l4_offset;\n\n\t \n\tpaylen = skb->len - l4_offset;\n\tcsum_replace_by_diff(&l4.tcp->check, (__force __wsum)htonl(paylen));\n\n\t \n\tfirst->gso_segs = skb_shinfo(skb)->gso_segs;\n\tfirst->bytecount += (first->gso_segs - 1) * *hdr_len;\n\n\t \n\tmss_l4len_idx = (*hdr_len - l4_offset) << IXGBE_ADVTXD_L4LEN_SHIFT;\n\tmss_l4len_idx |= skb_shinfo(skb)->gso_size << IXGBE_ADVTXD_MSS_SHIFT;\n\tmss_l4len_idx |= (1u << IXGBE_ADVTXD_IDX_SHIFT);\n\n\tfceof_saidx |= itd->pfsa;\n\ttype_tucmd |= itd->flags | itd->trailer_len;\n\n\t \n\tvlan_macip_lens = l4.hdr - ip.hdr;\n\tvlan_macip_lens |= (ip.hdr - skb->data) << IXGBE_ADVTXD_MACLEN_SHIFT;\n\tvlan_macip_lens |= first->tx_flags & IXGBE_TX_FLAGS_VLAN_MASK;\n\n\tixgbevf_tx_ctxtdesc(tx_ring, vlan_macip_lens, fceof_saidx, type_tucmd,\n\t\t\t    mss_l4len_idx);\n\n\treturn 1;\n}\n\nstatic void ixgbevf_tx_csum(struct ixgbevf_ring *tx_ring,\n\t\t\t    struct ixgbevf_tx_buffer *first,\n\t\t\t    struct ixgbevf_ipsec_tx_data *itd)\n{\n\tstruct sk_buff *skb = first->skb;\n\tu32 vlan_macip_lens = 0;\n\tu32 fceof_saidx = 0;\n\tu32 type_tucmd = 0;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tgoto no_csum;\n\n\tswitch (skb->csum_offset) {\n\tcase offsetof(struct tcphdr, check):\n\t\ttype_tucmd = IXGBE_ADVTXD_TUCMD_L4T_TCP;\n\t\tfallthrough;\n\tcase offsetof(struct udphdr, check):\n\t\tbreak;\n\tcase offsetof(struct sctphdr, checksum):\n\t\t \n\t\tif (skb_csum_is_sctp(skb)) {\n\t\t\ttype_tucmd = IXGBE_ADVTXD_TUCMD_L4T_SCTP;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tskb_checksum_help(skb);\n\t\tgoto no_csum;\n\t}\n\n\tif (first->protocol == htons(ETH_P_IP))\n\t\ttype_tucmd |= IXGBE_ADVTXD_TUCMD_IPV4;\n\n\t \n\tfirst->tx_flags |= IXGBE_TX_FLAGS_CSUM;\n\tvlan_macip_lens = skb_checksum_start_offset(skb) -\n\t\t\t  skb_network_offset(skb);\nno_csum:\n\t \n\tvlan_macip_lens |= skb_network_offset(skb) << IXGBE_ADVTXD_MACLEN_SHIFT;\n\tvlan_macip_lens |= first->tx_flags & IXGBE_TX_FLAGS_VLAN_MASK;\n\n\tfceof_saidx |= itd->pfsa;\n\ttype_tucmd |= itd->flags | itd->trailer_len;\n\n\tixgbevf_tx_ctxtdesc(tx_ring, vlan_macip_lens,\n\t\t\t    fceof_saidx, type_tucmd, 0);\n}\n\nstatic __le32 ixgbevf_tx_cmd_type(u32 tx_flags)\n{\n\t \n\t__le32 cmd_type = cpu_to_le32(IXGBE_ADVTXD_DTYP_DATA |\n\t\t\t\t      IXGBE_ADVTXD_DCMD_IFCS |\n\t\t\t\t      IXGBE_ADVTXD_DCMD_DEXT);\n\n\t \n\tif (tx_flags & IXGBE_TX_FLAGS_VLAN)\n\t\tcmd_type |= cpu_to_le32(IXGBE_ADVTXD_DCMD_VLE);\n\n\t \n\tif (tx_flags & IXGBE_TX_FLAGS_TSO)\n\t\tcmd_type |= cpu_to_le32(IXGBE_ADVTXD_DCMD_TSE);\n\n\treturn cmd_type;\n}\n\nstatic void ixgbevf_tx_olinfo_status(union ixgbe_adv_tx_desc *tx_desc,\n\t\t\t\t     u32 tx_flags, unsigned int paylen)\n{\n\t__le32 olinfo_status = cpu_to_le32(paylen << IXGBE_ADVTXD_PAYLEN_SHIFT);\n\n\t \n\tif (tx_flags & IXGBE_TX_FLAGS_CSUM)\n\t\tolinfo_status |= cpu_to_le32(IXGBE_ADVTXD_POPTS_TXSM);\n\n\t \n\tif (tx_flags & IXGBE_TX_FLAGS_IPV4)\n\t\tolinfo_status |= cpu_to_le32(IXGBE_ADVTXD_POPTS_IXSM);\n\n\t \n\tif (tx_flags & IXGBE_TX_FLAGS_IPSEC)\n\t\tolinfo_status |= cpu_to_le32(IXGBE_ADVTXD_POPTS_IPSEC);\n\n\t \n\tif (tx_flags & (IXGBE_TX_FLAGS_TSO | IXGBE_TX_FLAGS_IPSEC))\n\t\tolinfo_status |= cpu_to_le32(1u << IXGBE_ADVTXD_IDX_SHIFT);\n\n\t \n\tolinfo_status |= cpu_to_le32(IXGBE_ADVTXD_CC);\n\n\ttx_desc->read.olinfo_status = olinfo_status;\n}\n\nstatic void ixgbevf_tx_map(struct ixgbevf_ring *tx_ring,\n\t\t\t   struct ixgbevf_tx_buffer *first,\n\t\t\t   const u8 hdr_len)\n{\n\tstruct sk_buff *skb = first->skb;\n\tstruct ixgbevf_tx_buffer *tx_buffer;\n\tunion ixgbe_adv_tx_desc *tx_desc;\n\tskb_frag_t *frag;\n\tdma_addr_t dma;\n\tunsigned int data_len, size;\n\tu32 tx_flags = first->tx_flags;\n\t__le32 cmd_type = ixgbevf_tx_cmd_type(tx_flags);\n\tu16 i = tx_ring->next_to_use;\n\n\ttx_desc = IXGBEVF_TX_DESC(tx_ring, i);\n\n\tixgbevf_tx_olinfo_status(tx_desc, tx_flags, skb->len - hdr_len);\n\n\tsize = skb_headlen(skb);\n\tdata_len = skb->data_len;\n\n\tdma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);\n\n\ttx_buffer = first;\n\n\tfor (frag = &skb_shinfo(skb)->frags[0];; frag++) {\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto dma_error;\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, size);\n\t\tdma_unmap_addr_set(tx_buffer, dma, dma);\n\n\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\n\t\twhile (unlikely(size > IXGBE_MAX_DATA_PER_TXD)) {\n\t\t\ttx_desc->read.cmd_type_len =\n\t\t\t\tcmd_type | cpu_to_le32(IXGBE_MAX_DATA_PER_TXD);\n\n\t\t\ti++;\n\t\t\ttx_desc++;\n\t\t\tif (i == tx_ring->count) {\n\t\t\t\ttx_desc = IXGBEVF_TX_DESC(tx_ring, 0);\n\t\t\t\ti = 0;\n\t\t\t}\n\t\t\ttx_desc->read.olinfo_status = 0;\n\n\t\t\tdma += IXGBE_MAX_DATA_PER_TXD;\n\t\t\tsize -= IXGBE_MAX_DATA_PER_TXD;\n\n\t\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\t\t}\n\n\t\tif (likely(!data_len))\n\t\t\tbreak;\n\n\t\ttx_desc->read.cmd_type_len = cmd_type | cpu_to_le32(size);\n\n\t\ti++;\n\t\ttx_desc++;\n\t\tif (i == tx_ring->count) {\n\t\t\ttx_desc = IXGBEVF_TX_DESC(tx_ring, 0);\n\t\t\ti = 0;\n\t\t}\n\t\ttx_desc->read.olinfo_status = 0;\n\n\t\tsize = skb_frag_size(frag);\n\t\tdata_len -= size;\n\n\t\tdma = skb_frag_dma_map(tx_ring->dev, frag, 0, size,\n\t\t\t\t       DMA_TO_DEVICE);\n\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t}\n\n\t \n\tcmd_type |= cpu_to_le32(size) | cpu_to_le32(IXGBE_TXD_CMD);\n\ttx_desc->read.cmd_type_len = cmd_type;\n\n\t \n\tfirst->time_stamp = jiffies;\n\n\tskb_tx_timestamp(skb);\n\n\t \n\twmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\ti++;\n\tif (i == tx_ring->count)\n\t\ti = 0;\n\n\ttx_ring->next_to_use = i;\n\n\t \n\tixgbevf_write_tail(tx_ring, i);\n\n\treturn;\ndma_error:\n\tdev_err(tx_ring->dev, \"TX DMA map failed\\n\");\n\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\n\t \n\twhile (tx_buffer != first) {\n\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\n\t\tif (i-- == 0)\n\t\t\ti += tx_ring->count;\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t}\n\n\tif (dma_unmap_len(tx_buffer, len))\n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\tdma_unmap_len_set(tx_buffer, len, 0);\n\n\tdev_kfree_skb_any(tx_buffer->skb);\n\ttx_buffer->skb = NULL;\n\n\ttx_ring->next_to_use = i;\n}\n\nstatic int __ixgbevf_maybe_stop_tx(struct ixgbevf_ring *tx_ring, int size)\n{\n\tnetif_stop_subqueue(tx_ring->netdev, tx_ring->queue_index);\n\t \n\tsmp_mb();\n\n\t \n\tif (likely(ixgbevf_desc_unused(tx_ring) < size))\n\t\treturn -EBUSY;\n\n\t \n\tnetif_start_subqueue(tx_ring->netdev, tx_ring->queue_index);\n\t++tx_ring->tx_stats.restart_queue;\n\n\treturn 0;\n}\n\nstatic int ixgbevf_maybe_stop_tx(struct ixgbevf_ring *tx_ring, int size)\n{\n\tif (likely(ixgbevf_desc_unused(tx_ring) >= size))\n\t\treturn 0;\n\treturn __ixgbevf_maybe_stop_tx(tx_ring, size);\n}\n\nstatic int ixgbevf_xmit_frame_ring(struct sk_buff *skb,\n\t\t\t\t   struct ixgbevf_ring *tx_ring)\n{\n\tstruct ixgbevf_tx_buffer *first;\n\tint tso;\n\tu32 tx_flags = 0;\n\tu16 count = TXD_USE_COUNT(skb_headlen(skb));\n\tstruct ixgbevf_ipsec_tx_data ipsec_tx = { 0 };\n#if PAGE_SIZE > IXGBE_MAX_DATA_PER_TXD\n\tunsigned short f;\n#endif\n\tu8 hdr_len = 0;\n\tu8 *dst_mac = skb_header_pointer(skb, 0, 0, NULL);\n\n\tif (!dst_mac || is_link_local_ether_addr(dst_mac)) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\t \n#if PAGE_SIZE > IXGBE_MAX_DATA_PER_TXD\n\tfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[f];\n\n\t\tcount += TXD_USE_COUNT(skb_frag_size(frag));\n\t}\n#else\n\tcount += skb_shinfo(skb)->nr_frags;\n#endif\n\tif (ixgbevf_maybe_stop_tx(tx_ring, count + 3)) {\n\t\ttx_ring->tx_stats.tx_busy++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\tfirst = &tx_ring->tx_buffer_info[tx_ring->next_to_use];\n\tfirst->skb = skb;\n\tfirst->bytecount = skb->len;\n\tfirst->gso_segs = 1;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\ttx_flags |= skb_vlan_tag_get(skb);\n\t\ttx_flags <<= IXGBE_TX_FLAGS_VLAN_SHIFT;\n\t\ttx_flags |= IXGBE_TX_FLAGS_VLAN;\n\t}\n\n\t \n\tfirst->tx_flags = tx_flags;\n\tfirst->protocol = vlan_get_protocol(skb);\n\n#ifdef CONFIG_IXGBEVF_IPSEC\n\tif (xfrm_offload(skb) && !ixgbevf_ipsec_tx(tx_ring, first, &ipsec_tx))\n\t\tgoto out_drop;\n#endif\n\ttso = ixgbevf_tso(tx_ring, first, &hdr_len, &ipsec_tx);\n\tif (tso < 0)\n\t\tgoto out_drop;\n\telse if (!tso)\n\t\tixgbevf_tx_csum(tx_ring, first, &ipsec_tx);\n\n\tixgbevf_tx_map(tx_ring, first, hdr_len);\n\n\tixgbevf_maybe_stop_tx(tx_ring, DESC_NEEDED);\n\n\treturn NETDEV_TX_OK;\n\nout_drop:\n\tdev_kfree_skb_any(first->skb);\n\tfirst->skb = NULL;\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic netdev_tx_t ixgbevf_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbevf_ring *tx_ring;\n\n\tif (skb->len <= 0) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\t \n\tif (skb->len < 17) {\n\t\tif (skb_padto(skb, 17))\n\t\t\treturn NETDEV_TX_OK;\n\t\tskb->len = 17;\n\t}\n\n\ttx_ring = adapter->tx_ring[skb->queue_mapping];\n\treturn ixgbevf_xmit_frame_ring(skb, tx_ring);\n}\n\n \nstatic int ixgbevf_set_mac(struct net_device *netdev, void *p)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct sockaddr *addr = p;\n\tint err;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\n\terr = hw->mac.ops.set_rar(hw, 0, addr->sa_data, 0);\n\n\tspin_unlock_bh(&adapter->mbx_lock);\n\n\tif (err)\n\t\treturn -EPERM;\n\n\tether_addr_copy(hw->mac.addr, addr->sa_data);\n\tether_addr_copy(hw->mac.perm_addr, addr->sa_data);\n\teth_hw_addr_set(netdev, addr->sa_data);\n\n\treturn 0;\n}\n\n \nstatic int ixgbevf_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN;\n\tint ret;\n\n\t \n\tif (adapter->xdp_prog) {\n\t\tdev_warn(&adapter->pdev->dev, \"MTU cannot be changed while XDP program is loaded\\n\");\n\t\treturn -EPERM;\n\t}\n\n\tspin_lock_bh(&adapter->mbx_lock);\n\t \n\tret = hw->mac.ops.set_rlpml(hw, max_frame);\n\tspin_unlock_bh(&adapter->mbx_lock);\n\tif (ret)\n\t\treturn -EINVAL;\n\n\thw_dbg(hw, \"changing MTU from %d to %d\\n\",\n\t       netdev->mtu, new_mtu);\n\n\t \n\tnetdev->mtu = new_mtu;\n\n\tif (netif_running(netdev))\n\t\tixgbevf_reinit_locked(adapter);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused ixgbevf_suspend(struct device *dev_d)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev_d);\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\n\trtnl_lock();\n\tnetif_device_detach(netdev);\n\n\tif (netif_running(netdev))\n\t\tixgbevf_close_suspend(adapter);\n\n\tixgbevf_clear_interrupt_scheme(adapter);\n\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic int __maybe_unused ixgbevf_resume(struct device *dev_d)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev_d);\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tu32 err;\n\n\tadapter->hw.hw_addr = adapter->io_addr;\n\tsmp_mb__before_atomic();\n\tclear_bit(__IXGBEVF_DISABLED, &adapter->state);\n\tpci_set_master(pdev);\n\n\tixgbevf_reset(adapter);\n\n\trtnl_lock();\n\terr = ixgbevf_init_interrupt_scheme(adapter);\n\tif (!err && netif_running(netdev))\n\t\terr = ixgbevf_open(netdev);\n\trtnl_unlock();\n\tif (err)\n\t\treturn err;\n\n\tnetif_device_attach(netdev);\n\n\treturn err;\n}\n\nstatic void ixgbevf_shutdown(struct pci_dev *pdev)\n{\n\tixgbevf_suspend(&pdev->dev);\n}\n\nstatic void ixgbevf_get_tx_ring_stats(struct rtnl_link_stats64 *stats,\n\t\t\t\t      const struct ixgbevf_ring *ring)\n{\n\tu64 bytes, packets;\n\tunsigned int start;\n\n\tif (ring) {\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&ring->syncp);\n\t\t\tbytes = ring->stats.bytes;\n\t\t\tpackets = ring->stats.packets;\n\t\t} while (u64_stats_fetch_retry(&ring->syncp, start));\n\t\tstats->tx_bytes += bytes;\n\t\tstats->tx_packets += packets;\n\t}\n}\n\nstatic void ixgbevf_get_stats(struct net_device *netdev,\n\t\t\t      struct rtnl_link_stats64 *stats)\n{\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\tunsigned int start;\n\tu64 bytes, packets;\n\tconst struct ixgbevf_ring *ring;\n\tint i;\n\n\tixgbevf_update_stats(adapter);\n\n\tstats->multicast = adapter->stats.vfmprc - adapter->stats.base_vfmprc;\n\n\trcu_read_lock();\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tring = adapter->rx_ring[i];\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&ring->syncp);\n\t\t\tbytes = ring->stats.bytes;\n\t\t\tpackets = ring->stats.packets;\n\t\t} while (u64_stats_fetch_retry(&ring->syncp, start));\n\t\tstats->rx_bytes += bytes;\n\t\tstats->rx_packets += packets;\n\t}\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tring = adapter->tx_ring[i];\n\t\tixgbevf_get_tx_ring_stats(stats, ring);\n\t}\n\n\tfor (i = 0; i < adapter->num_xdp_queues; i++) {\n\t\tring = adapter->xdp_ring[i];\n\t\tixgbevf_get_tx_ring_stats(stats, ring);\n\t}\n\trcu_read_unlock();\n}\n\n#define IXGBEVF_MAX_MAC_HDR_LEN\t\t127\n#define IXGBEVF_MAX_NETWORK_HDR_LEN\t511\n\nstatic netdev_features_t\nixgbevf_features_check(struct sk_buff *skb, struct net_device *dev,\n\t\t       netdev_features_t features)\n{\n\tunsigned int network_hdr_len, mac_hdr_len;\n\n\t \n\tmac_hdr_len = skb_network_header(skb) - skb->data;\n\tif (unlikely(mac_hdr_len > IXGBEVF_MAX_MAC_HDR_LEN))\n\t\treturn features & ~(NETIF_F_HW_CSUM |\n\t\t\t\t    NETIF_F_SCTP_CRC |\n\t\t\t\t    NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t    NETIF_F_TSO |\n\t\t\t\t    NETIF_F_TSO6);\n\n\tnetwork_hdr_len = skb_checksum_start(skb) - skb_network_header(skb);\n\tif (unlikely(network_hdr_len >  IXGBEVF_MAX_NETWORK_HDR_LEN))\n\t\treturn features & ~(NETIF_F_HW_CSUM |\n\t\t\t\t    NETIF_F_SCTP_CRC |\n\t\t\t\t    NETIF_F_TSO |\n\t\t\t\t    NETIF_F_TSO6);\n\n\t \n\tif (skb->encapsulation && !(features & NETIF_F_TSO_MANGLEID))\n\t\tfeatures &= ~NETIF_F_TSO;\n\n\treturn features;\n}\n\nstatic int ixgbevf_xdp_setup(struct net_device *dev, struct bpf_prog *prog)\n{\n\tint i, frame_size = dev->mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;\n\tstruct ixgbevf_adapter *adapter = netdev_priv(dev);\n\tstruct bpf_prog *old_prog;\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct ixgbevf_ring *ring = adapter->rx_ring[i];\n\n\t\tif (frame_size > ixgbevf_rx_bufsz(ring))\n\t\t\treturn -EINVAL;\n\t}\n\n\told_prog = xchg(&adapter->xdp_prog, prog);\n\n\t \n\tif (!!prog != !!old_prog) {\n\t\t \n\t\tif (netif_running(dev))\n\t\t\tixgbevf_close(dev);\n\n\t\tixgbevf_clear_interrupt_scheme(adapter);\n\t\tixgbevf_init_interrupt_scheme(adapter);\n\n\t\tif (netif_running(dev))\n\t\t\tixgbevf_open(dev);\n\t} else {\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\t\txchg(&adapter->rx_ring[i]->xdp_prog, adapter->xdp_prog);\n\t}\n\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\treturn 0;\n}\n\nstatic int ixgbevf_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn ixgbevf_xdp_setup(dev, xdp->prog);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic const struct net_device_ops ixgbevf_netdev_ops = {\n\t.ndo_open\t\t= ixgbevf_open,\n\t.ndo_stop\t\t= ixgbevf_close,\n\t.ndo_start_xmit\t\t= ixgbevf_xmit_frame,\n\t.ndo_set_rx_mode\t= ixgbevf_set_rx_mode,\n\t.ndo_get_stats64\t= ixgbevf_get_stats,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_mac_address\t= ixgbevf_set_mac,\n\t.ndo_change_mtu\t\t= ixgbevf_change_mtu,\n\t.ndo_tx_timeout\t\t= ixgbevf_tx_timeout,\n\t.ndo_vlan_rx_add_vid\t= ixgbevf_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= ixgbevf_vlan_rx_kill_vid,\n\t.ndo_features_check\t= ixgbevf_features_check,\n\t.ndo_bpf\t\t= ixgbevf_xdp,\n};\n\nstatic void ixgbevf_assign_netdev_ops(struct net_device *dev)\n{\n\tdev->netdev_ops = &ixgbevf_netdev_ops;\n\tixgbevf_set_ethtool_ops(dev);\n\tdev->watchdog_timeo = 5 * HZ;\n}\n\n \nstatic int ixgbevf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct net_device *netdev;\n\tstruct ixgbevf_adapter *adapter = NULL;\n\tstruct ixgbe_hw *hw = NULL;\n\tconst struct ixgbevf_info *ii = ixgbevf_info_tbl[ent->driver_data];\n\tbool disable_dev = false;\n\tint err;\n\n\terr = pci_enable_device(pdev);\n\tif (err)\n\t\treturn err;\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"No usable DMA configuration, aborting\\n\");\n\t\tgoto err_dma;\n\t}\n\n\terr = pci_request_regions(pdev, ixgbevf_driver_name);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"pci_request_regions failed 0x%x\\n\", err);\n\t\tgoto err_pci_reg;\n\t}\n\n\tpci_set_master(pdev);\n\n\tnetdev = alloc_etherdev_mq(sizeof(struct ixgbevf_adapter),\n\t\t\t\t   MAX_TX_QUEUES);\n\tif (!netdev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_etherdev;\n\t}\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\n\tadapter = netdev_priv(netdev);\n\n\tadapter->netdev = netdev;\n\tadapter->pdev = pdev;\n\thw = &adapter->hw;\n\thw->back = adapter;\n\tadapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\n\n\t \n\tpci_save_state(pdev);\n\n\thw->hw_addr = ioremap(pci_resource_start(pdev, 0),\n\t\t\t      pci_resource_len(pdev, 0));\n\tadapter->io_addr = hw->hw_addr;\n\tif (!hw->hw_addr) {\n\t\terr = -EIO;\n\t\tgoto err_ioremap;\n\t}\n\n\tixgbevf_assign_netdev_ops(netdev);\n\n\t \n\tmemcpy(&hw->mac.ops, ii->mac_ops, sizeof(hw->mac.ops));\n\thw->mac.type  = ii->mac;\n\n\tmemcpy(&hw->mbx.ops, &ixgbevf_mbx_ops_legacy,\n\t       sizeof(struct ixgbe_mbx_operations));\n\n\t \n\terr = ixgbevf_sw_init(adapter);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\t \n\tif (!is_valid_ether_addr(netdev->dev_addr)) {\n\t\tpr_err(\"invalid MAC address\\n\");\n\t\terr = -EIO;\n\t\tgoto err_sw_init;\n\t}\n\n\tnetdev->hw_features = NETIF_F_SG |\n\t\t\t      NETIF_F_TSO |\n\t\t\t      NETIF_F_TSO6 |\n\t\t\t      NETIF_F_RXCSUM |\n\t\t\t      NETIF_F_HW_CSUM |\n\t\t\t      NETIF_F_SCTP_CRC;\n\n#define IXGBEVF_GSO_PARTIAL_FEATURES (NETIF_F_GSO_GRE | \\\n\t\t\t\t      NETIF_F_GSO_GRE_CSUM | \\\n\t\t\t\t      NETIF_F_GSO_IPXIP4 | \\\n\t\t\t\t      NETIF_F_GSO_IPXIP6 | \\\n\t\t\t\t      NETIF_F_GSO_UDP_TUNNEL | \\\n\t\t\t\t      NETIF_F_GSO_UDP_TUNNEL_CSUM)\n\n\tnetdev->gso_partial_features = IXGBEVF_GSO_PARTIAL_FEATURES;\n\tnetdev->hw_features |= NETIF_F_GSO_PARTIAL |\n\t\t\t       IXGBEVF_GSO_PARTIAL_FEATURES;\n\n\tnetdev->features = netdev->hw_features | NETIF_F_HIGHDMA;\n\n\tnetdev->vlan_features |= netdev->features | NETIF_F_TSO_MANGLEID;\n\tnetdev->mpls_features |= NETIF_F_SG |\n\t\t\t\t NETIF_F_TSO |\n\t\t\t\t NETIF_F_TSO6 |\n\t\t\t\t NETIF_F_HW_CSUM;\n\tnetdev->mpls_features |= IXGBEVF_GSO_PARTIAL_FEATURES;\n\tnetdev->hw_enc_features |= netdev->vlan_features;\n\n\t \n\tnetdev->features |= NETIF_F_HW_VLAN_CTAG_FILTER |\n\t\t\t    NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t    NETIF_F_HW_VLAN_CTAG_TX;\n\n\tnetdev->priv_flags |= IFF_UNICAST_FLT;\n\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC;\n\n\t \n\tnetdev->min_mtu = ETH_MIN_MTU;\n\tswitch (adapter->hw.api_version) {\n\tcase ixgbe_mbox_api_11:\n\tcase ixgbe_mbox_api_12:\n\tcase ixgbe_mbox_api_13:\n\tcase ixgbe_mbox_api_14:\n\tcase ixgbe_mbox_api_15:\n\t\tnetdev->max_mtu = IXGBE_MAX_JUMBO_FRAME_SIZE -\n\t\t\t\t  (ETH_HLEN + ETH_FCS_LEN);\n\t\tbreak;\n\tdefault:\n\t\tif (adapter->hw.mac.type != ixgbe_mac_82599_vf)\n\t\t\tnetdev->max_mtu = IXGBE_MAX_JUMBO_FRAME_SIZE -\n\t\t\t\t\t  (ETH_HLEN + ETH_FCS_LEN);\n\t\telse\n\t\t\tnetdev->max_mtu = ETH_DATA_LEN + ETH_FCS_LEN;\n\t\tbreak;\n\t}\n\n\tif (IXGBE_REMOVED(hw->hw_addr)) {\n\t\terr = -EIO;\n\t\tgoto err_sw_init;\n\t}\n\n\ttimer_setup(&adapter->service_timer, ixgbevf_service_timer, 0);\n\n\tINIT_WORK(&adapter->service_task, ixgbevf_service_task);\n\tset_bit(__IXGBEVF_SERVICE_INITED, &adapter->state);\n\tclear_bit(__IXGBEVF_SERVICE_SCHED, &adapter->state);\n\n\terr = ixgbevf_init_interrupt_scheme(adapter);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\tstrcpy(netdev->name, \"eth%d\");\n\n\terr = register_netdev(netdev);\n\tif (err)\n\t\tgoto err_register;\n\n\tpci_set_drvdata(pdev, netdev);\n\tnetif_carrier_off(netdev);\n\tixgbevf_init_ipsec_offload(adapter);\n\n\tixgbevf_init_last_counter_stats(adapter);\n\n\t \n\tdev_info(&pdev->dev, \"%pM\\n\", netdev->dev_addr);\n\tdev_info(&pdev->dev, \"MAC: %d\\n\", hw->mac.type);\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_X550_vf:\n\t\tdev_info(&pdev->dev, \"Intel(R) X550 Virtual Function\\n\");\n\t\tbreak;\n\tcase ixgbe_mac_X540_vf:\n\t\tdev_info(&pdev->dev, \"Intel(R) X540 Virtual Function\\n\");\n\t\tbreak;\n\tcase ixgbe_mac_82599_vf:\n\tdefault:\n\t\tdev_info(&pdev->dev, \"Intel(R) 82599 Virtual Function\\n\");\n\t\tbreak;\n\t}\n\n\treturn 0;\n\nerr_register:\n\tixgbevf_clear_interrupt_scheme(adapter);\nerr_sw_init:\n\tixgbevf_reset_interrupt_capability(adapter);\n\tiounmap(adapter->io_addr);\n\tkfree(adapter->rss_key);\nerr_ioremap:\n\tdisable_dev = !test_and_set_bit(__IXGBEVF_DISABLED, &adapter->state);\n\tfree_netdev(netdev);\nerr_alloc_etherdev:\n\tpci_release_regions(pdev);\nerr_pci_reg:\nerr_dma:\n\tif (!adapter || disable_dev)\n\t\tpci_disable_device(pdev);\n\treturn err;\n}\n\n \nstatic void ixgbevf_remove(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct ixgbevf_adapter *adapter;\n\tbool disable_dev;\n\n\tif (!netdev)\n\t\treturn;\n\n\tadapter = netdev_priv(netdev);\n\n\tset_bit(__IXGBEVF_REMOVING, &adapter->state);\n\tcancel_work_sync(&adapter->service_task);\n\n\tif (netdev->reg_state == NETREG_REGISTERED)\n\t\tunregister_netdev(netdev);\n\n\tixgbevf_stop_ipsec_offload(adapter);\n\tixgbevf_clear_interrupt_scheme(adapter);\n\tixgbevf_reset_interrupt_capability(adapter);\n\n\tiounmap(adapter->io_addr);\n\tpci_release_regions(pdev);\n\n\thw_dbg(&adapter->hw, \"Remove complete\\n\");\n\n\tkfree(adapter->rss_key);\n\tdisable_dev = !test_and_set_bit(__IXGBEVF_DISABLED, &adapter->state);\n\tfree_netdev(netdev);\n\n\tif (disable_dev)\n\t\tpci_disable_device(pdev);\n}\n\n \nstatic pci_ers_result_t ixgbevf_io_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t\t  pci_channel_state_t state)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\n\tif (!test_bit(__IXGBEVF_SERVICE_INITED, &adapter->state))\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\trtnl_lock();\n\tnetif_device_detach(netdev);\n\n\tif (netif_running(netdev))\n\t\tixgbevf_close_suspend(adapter);\n\n\tif (state == pci_channel_io_perm_failure) {\n\t\trtnl_unlock();\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\tif (!test_and_set_bit(__IXGBEVF_DISABLED, &adapter->state))\n\t\tpci_disable_device(pdev);\n\trtnl_unlock();\n\n\t \n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\n \nstatic pci_ers_result_t ixgbevf_io_slot_reset(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\n\n\tif (pci_enable_device_mem(pdev)) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Cannot re-enable PCI device after reset.\\n\");\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\tadapter->hw.hw_addr = adapter->io_addr;\n\tsmp_mb__before_atomic();\n\tclear_bit(__IXGBEVF_DISABLED, &adapter->state);\n\tpci_set_master(pdev);\n\n\tixgbevf_reset(adapter);\n\n\treturn PCI_ERS_RESULT_RECOVERED;\n}\n\n \nstatic void ixgbevf_io_resume(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\n\trtnl_lock();\n\tif (netif_running(netdev))\n\t\tixgbevf_open(netdev);\n\n\tnetif_device_attach(netdev);\n\trtnl_unlock();\n}\n\n \nstatic const struct pci_error_handlers ixgbevf_err_handler = {\n\t.error_detected = ixgbevf_io_error_detected,\n\t.slot_reset = ixgbevf_io_slot_reset,\n\t.resume = ixgbevf_io_resume,\n};\n\nstatic SIMPLE_DEV_PM_OPS(ixgbevf_pm_ops, ixgbevf_suspend, ixgbevf_resume);\n\nstatic struct pci_driver ixgbevf_driver = {\n\t.name\t\t= ixgbevf_driver_name,\n\t.id_table\t= ixgbevf_pci_tbl,\n\t.probe\t\t= ixgbevf_probe,\n\t.remove\t\t= ixgbevf_remove,\n\n\t \n\t.driver.pm\t= &ixgbevf_pm_ops,\n\n\t.shutdown\t= ixgbevf_shutdown,\n\t.err_handler\t= &ixgbevf_err_handler\n};\n\n \nstatic int __init ixgbevf_init_module(void)\n{\n\tint err;\n\n\tpr_info(\"%s\\n\", ixgbevf_driver_string);\n\tpr_info(\"%s\\n\", ixgbevf_copyright);\n\tixgbevf_wq = create_singlethread_workqueue(ixgbevf_driver_name);\n\tif (!ixgbevf_wq) {\n\t\tpr_err(\"%s: Failed to create workqueue\\n\", ixgbevf_driver_name);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = pci_register_driver(&ixgbevf_driver);\n\tif (err) {\n\t\tdestroy_workqueue(ixgbevf_wq);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nmodule_init(ixgbevf_init_module);\n\n \nstatic void __exit ixgbevf_exit_module(void)\n{\n\tpci_unregister_driver(&ixgbevf_driver);\n\tif (ixgbevf_wq) {\n\t\tdestroy_workqueue(ixgbevf_wq);\n\t\tixgbevf_wq = NULL;\n\t}\n}\n\n#ifdef DEBUG\n \nchar *ixgbevf_get_hw_dev_name(struct ixgbe_hw *hw)\n{\n\tstruct ixgbevf_adapter *adapter = hw->back;\n\n\treturn adapter->netdev->name;\n}\n\n#endif\nmodule_exit(ixgbevf_exit_module);\n\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}