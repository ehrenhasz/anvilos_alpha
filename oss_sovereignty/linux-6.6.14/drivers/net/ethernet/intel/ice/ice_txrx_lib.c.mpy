{
  "module_name": "ice_txrx_lib.c",
  "hash_id": "1773b8ce9658fdc6797c16cc5eec0b05f2ef53ba95f54f8be28abeef46fbbcaa",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ice/ice_txrx_lib.c",
  "human_readable_source": "\n \n\n#include <linux/filter.h>\n\n#include \"ice_txrx_lib.h\"\n#include \"ice_eswitch.h\"\n#include \"ice_lib.h\"\n\n \nvoid ice_release_rx_desc(struct ice_rx_ring *rx_ring, u16 val)\n{\n\tu16 prev_ntu = rx_ring->next_to_use & ~0x7;\n\n\trx_ring->next_to_use = val;\n\n\t \n\trx_ring->next_to_alloc = val;\n\n\t \n\tval &= ~0x7;\n\tif (prev_ntu != val) {\n\t\t \n\t\twmb();\n\t\twritel(val, rx_ring->tail);\n\t}\n}\n\n \nstatic enum pkt_hash_types ice_ptype_to_htype(u16 ptype)\n{\n\tstruct ice_rx_ptype_decoded decoded = ice_decode_rx_desc_ptype(ptype);\n\n\tif (!decoded.known)\n\t\treturn PKT_HASH_TYPE_NONE;\n\tif (decoded.payload_layer == ICE_RX_PTYPE_PAYLOAD_LAYER_PAY4)\n\t\treturn PKT_HASH_TYPE_L4;\n\tif (decoded.payload_layer == ICE_RX_PTYPE_PAYLOAD_LAYER_PAY3)\n\t\treturn PKT_HASH_TYPE_L3;\n\tif (decoded.outer_ip == ICE_RX_PTYPE_OUTER_L2)\n\t\treturn PKT_HASH_TYPE_L2;\n\n\treturn PKT_HASH_TYPE_NONE;\n}\n\n \nstatic void\nice_rx_hash(struct ice_rx_ring *rx_ring, union ice_32b_rx_flex_desc *rx_desc,\n\t    struct sk_buff *skb, u16 rx_ptype)\n{\n\tstruct ice_32b_rx_flex_desc_nic *nic_mdid;\n\tu32 hash;\n\n\tif (!(rx_ring->netdev->features & NETIF_F_RXHASH))\n\t\treturn;\n\n\tif (rx_desc->wb.rxdid != ICE_RXDID_FLEX_NIC)\n\t\treturn;\n\n\tnic_mdid = (struct ice_32b_rx_flex_desc_nic *)rx_desc;\n\thash = le32_to_cpu(nic_mdid->rss_hash);\n\tskb_set_hash(skb, hash, ice_ptype_to_htype(rx_ptype));\n}\n\n \nstatic void\nice_rx_csum(struct ice_rx_ring *ring, struct sk_buff *skb,\n\t    union ice_32b_rx_flex_desc *rx_desc, u16 ptype)\n{\n\tstruct ice_rx_ptype_decoded decoded;\n\tu16 rx_status0, rx_status1;\n\tbool ipv4, ipv6;\n\n\trx_status0 = le16_to_cpu(rx_desc->wb.status_error0);\n\trx_status1 = le16_to_cpu(rx_desc->wb.status_error1);\n\n\tdecoded = ice_decode_rx_desc_ptype(ptype);\n\n\t \n\tskb->ip_summed = CHECKSUM_NONE;\n\tskb_checksum_none_assert(skb);\n\n\t \n\tif (!(ring->netdev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\t \n\tif (!(rx_status0 & BIT(ICE_RX_FLEX_DESC_STATUS0_L3L4P_S)))\n\t\treturn;\n\n\tif (!(decoded.known && decoded.outer_ip))\n\t\treturn;\n\n\tipv4 = (decoded.outer_ip == ICE_RX_PTYPE_OUTER_IP) &&\n\t       (decoded.outer_ip_ver == ICE_RX_PTYPE_OUTER_IPV4);\n\tipv6 = (decoded.outer_ip == ICE_RX_PTYPE_OUTER_IP) &&\n\t       (decoded.outer_ip_ver == ICE_RX_PTYPE_OUTER_IPV6);\n\n\tif (ipv4 && (rx_status0 & (BIT(ICE_RX_FLEX_DESC_STATUS0_XSUM_IPE_S) |\n\t\t\t\t   BIT(ICE_RX_FLEX_DESC_STATUS0_XSUM_EIPE_S))))\n\t\tgoto checksum_fail;\n\n\tif (ipv6 && (rx_status0 & (BIT(ICE_RX_FLEX_DESC_STATUS0_IPV6EXADD_S))))\n\t\tgoto checksum_fail;\n\n\t \n\tif (rx_status0 & BIT(ICE_RX_FLEX_DESC_STATUS0_XSUM_L4E_S))\n\t\tgoto checksum_fail;\n\n\t \n\tif ((rx_status1 & BIT(ICE_RX_FLEX_DESC_STATUS1_NAT_S)) &&\n\t    (rx_status0 & BIT(ICE_RX_FLEX_DESC_STATUS0_XSUM_EUDPE_S)))\n\t\tgoto checksum_fail;\n\n\t \n\tif (decoded.tunnel_type >= ICE_RX_PTYPE_TUNNEL_IP_GRENAT)\n\t\tskb->csum_level = 1;\n\n\t \n\tswitch (decoded.inner_prot) {\n\tcase ICE_RX_PTYPE_INNER_PROT_TCP:\n\tcase ICE_RX_PTYPE_INNER_PROT_UDP:\n\tcase ICE_RX_PTYPE_INNER_PROT_SCTP:\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn;\n\nchecksum_fail:\n\tring->vsi->back->hw_csum_rx_error++;\n}\n\n \nvoid\nice_process_skb_fields(struct ice_rx_ring *rx_ring,\n\t\t       union ice_32b_rx_flex_desc *rx_desc,\n\t\t       struct sk_buff *skb, u16 ptype)\n{\n\tice_rx_hash(rx_ring, rx_desc, skb, ptype);\n\n\t \n\tskb->protocol = eth_type_trans(skb, rx_ring->netdev);\n\n\tice_rx_csum(rx_ring, skb, rx_desc, ptype);\n\n\tif (rx_ring->ptp_rx)\n\t\tice_ptp_rx_hwtstamp(rx_ring, rx_desc, skb);\n}\n\n \nvoid\nice_receive_skb(struct ice_rx_ring *rx_ring, struct sk_buff *skb, u16 vlan_tag)\n{\n\tnetdev_features_t features = rx_ring->netdev->features;\n\tbool non_zero_vlan = !!(vlan_tag & VLAN_VID_MASK);\n\n\tif ((features & NETIF_F_HW_VLAN_CTAG_RX) && non_zero_vlan)\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tag);\n\telse if ((features & NETIF_F_HW_VLAN_STAG_RX) && non_zero_vlan)\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021AD), vlan_tag);\n\n\tnapi_gro_receive(&rx_ring->q_vector->napi, skb);\n}\n\n \nstatic void\nice_clean_xdp_tx_buf(struct device *dev, struct ice_tx_buf *tx_buf,\n\t\t     struct xdp_frame_bulk *bq)\n{\n\tdma_unmap_single(dev, dma_unmap_addr(tx_buf, dma),\n\t\t\t dma_unmap_len(tx_buf, len), DMA_TO_DEVICE);\n\tdma_unmap_len_set(tx_buf, len, 0);\n\n\tswitch (tx_buf->type) {\n\tcase ICE_TX_BUF_XDP_TX:\n\t\tpage_frag_free(tx_buf->raw_buf);\n\t\tbreak;\n\tcase ICE_TX_BUF_XDP_XMIT:\n\t\txdp_return_frame_bulk(tx_buf->xdpf, bq);\n\t\tbreak;\n\t}\n\n\ttx_buf->type = ICE_TX_BUF_EMPTY;\n}\n\n \nstatic u32 ice_clean_xdp_irq(struct ice_tx_ring *xdp_ring)\n{\n\tint total_bytes = 0, total_pkts = 0;\n\tstruct device *dev = xdp_ring->dev;\n\tu32 ntc = xdp_ring->next_to_clean;\n\tstruct ice_tx_desc *tx_desc;\n\tu32 cnt = xdp_ring->count;\n\tstruct xdp_frame_bulk bq;\n\tu32 frags, xdp_tx = 0;\n\tu32 ready_frames = 0;\n\tu32 idx;\n\tu32 ret;\n\n\tidx = xdp_ring->tx_buf[ntc].rs_idx;\n\ttx_desc = ICE_TX_DESC(xdp_ring, idx);\n\tif (tx_desc->cmd_type_offset_bsz &\n\t    cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE)) {\n\t\tif (idx >= ntc)\n\t\t\tready_frames = idx - ntc + 1;\n\t\telse\n\t\t\tready_frames = idx + cnt - ntc + 1;\n\t}\n\n\tif (unlikely(!ready_frames))\n\t\treturn 0;\n\tret = ready_frames;\n\n\txdp_frame_bulk_init(&bq);\n\trcu_read_lock();  \n\n\twhile (ready_frames) {\n\t\tstruct ice_tx_buf *tx_buf = &xdp_ring->tx_buf[ntc];\n\t\tstruct ice_tx_buf *head = tx_buf;\n\n\t\t \n\t\ttotal_bytes += tx_buf->bytecount;\n\t\tfrags = tx_buf->nr_frags;\n\t\ttotal_pkts++;\n\t\t \n\t\tready_frames -= frags + 1;\n\t\txdp_tx++;\n\n\t\tntc++;\n\t\tif (ntc == cnt)\n\t\t\tntc = 0;\n\n\t\tfor (int i = 0; i < frags; i++) {\n\t\t\ttx_buf = &xdp_ring->tx_buf[ntc];\n\n\t\t\tice_clean_xdp_tx_buf(dev, tx_buf, &bq);\n\t\t\tntc++;\n\t\t\tif (ntc == cnt)\n\t\t\t\tntc = 0;\n\t\t}\n\n\t\tice_clean_xdp_tx_buf(dev, head, &bq);\n\t}\n\n\txdp_flush_frame_bulk(&bq);\n\trcu_read_unlock();\n\n\ttx_desc->cmd_type_offset_bsz = 0;\n\txdp_ring->next_to_clean = ntc;\n\txdp_ring->xdp_tx_active -= xdp_tx;\n\tice_update_tx_ring_stats(xdp_ring, total_pkts, total_bytes);\n\n\treturn ret;\n}\n\n \nint __ice_xmit_xdp_ring(struct xdp_buff *xdp, struct ice_tx_ring *xdp_ring,\n\t\t\tbool frame)\n{\n\tstruct skb_shared_info *sinfo = NULL;\n\tu32 size = xdp->data_end - xdp->data;\n\tstruct device *dev = xdp_ring->dev;\n\tu32 ntu = xdp_ring->next_to_use;\n\tstruct ice_tx_desc *tx_desc;\n\tstruct ice_tx_buf *tx_head;\n\tstruct ice_tx_buf *tx_buf;\n\tu32 cnt = xdp_ring->count;\n\tvoid *data = xdp->data;\n\tu32 nr_frags = 0;\n\tu32 free_space;\n\tu32 frag = 0;\n\n\tfree_space = ICE_DESC_UNUSED(xdp_ring);\n\tif (free_space < ICE_RING_QUARTER(xdp_ring))\n\t\tfree_space += ice_clean_xdp_irq(xdp_ring);\n\n\tif (unlikely(!free_space))\n\t\tgoto busy;\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tnr_frags = sinfo->nr_frags;\n\t\tif (free_space < nr_frags + 1)\n\t\t\tgoto busy;\n\t}\n\n\ttx_desc = ICE_TX_DESC(xdp_ring, ntu);\n\ttx_head = &xdp_ring->tx_buf[ntu];\n\ttx_buf = tx_head;\n\n\tfor (;;) {\n\t\tdma_addr_t dma;\n\n\t\tdma = dma_map_single(dev, data, size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, dma))\n\t\t\tgoto dma_unmap;\n\n\t\t \n\t\tdma_unmap_len_set(tx_buf, len, size);\n\t\tdma_unmap_addr_set(tx_buf, dma, dma);\n\n\t\tif (frame) {\n\t\t\ttx_buf->type = ICE_TX_BUF_FRAG;\n\t\t} else {\n\t\t\ttx_buf->type = ICE_TX_BUF_XDP_TX;\n\t\t\ttx_buf->raw_buf = data;\n\t\t}\n\n\t\ttx_desc->buf_addr = cpu_to_le64(dma);\n\t\ttx_desc->cmd_type_offset_bsz = ice_build_ctob(0, 0, size, 0);\n\n\t\tntu++;\n\t\tif (ntu == cnt)\n\t\t\tntu = 0;\n\n\t\tif (frag == nr_frags)\n\t\t\tbreak;\n\n\t\ttx_desc = ICE_TX_DESC(xdp_ring, ntu);\n\t\ttx_buf = &xdp_ring->tx_buf[ntu];\n\n\t\tdata = skb_frag_address(&sinfo->frags[frag]);\n\t\tsize = skb_frag_size(&sinfo->frags[frag]);\n\t\tfrag++;\n\t}\n\n\t \n\ttx_head->bytecount = xdp_get_buff_len(xdp);\n\ttx_head->nr_frags = nr_frags;\n\n\tif (frame) {\n\t\ttx_head->type = ICE_TX_BUF_XDP_XMIT;\n\t\ttx_head->xdpf = xdp->data_hard_start;\n\t}\n\n\t \n\ttx_desc->cmd_type_offset_bsz |=\n\t\tcpu_to_le64(ICE_TX_DESC_CMD_EOP << ICE_TXD_QW1_CMD_S);\n\n\txdp_ring->xdp_tx_active++;\n\txdp_ring->next_to_use = ntu;\n\n\treturn ICE_XDP_TX;\n\ndma_unmap:\n\tfor (;;) {\n\t\ttx_buf = &xdp_ring->tx_buf[ntu];\n\t\tdma_unmap_page(dev, dma_unmap_addr(tx_buf, dma),\n\t\t\t       dma_unmap_len(tx_buf, len), DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(tx_buf, len, 0);\n\t\tif (tx_buf == tx_head)\n\t\t\tbreak;\n\n\t\tif (!ntu)\n\t\t\tntu += cnt;\n\t\tntu--;\n\t}\n\treturn ICE_XDP_CONSUMED;\n\nbusy:\n\txdp_ring->ring_stats->tx_stats.tx_busy++;\n\n\treturn ICE_XDP_CONSUMED;\n}\n\n \nvoid ice_finalize_xdp_rx(struct ice_tx_ring *xdp_ring, unsigned int xdp_res,\n\t\t\t u32 first_idx)\n{\n\tstruct ice_tx_buf *tx_buf = &xdp_ring->tx_buf[first_idx];\n\n\tif (xdp_res & ICE_XDP_REDIR)\n\t\txdp_do_flush_map();\n\n\tif (xdp_res & ICE_XDP_TX) {\n\t\tif (static_branch_unlikely(&ice_xdp_locking_key))\n\t\t\tspin_lock(&xdp_ring->tx_lock);\n\t\t \n\t\ttx_buf->rs_idx = ice_set_rs_bit(xdp_ring);\n\t\tice_xdp_ring_update_tail(xdp_ring);\n\t\tif (static_branch_unlikely(&ice_xdp_locking_key))\n\t\t\tspin_unlock(&xdp_ring->tx_lock);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}