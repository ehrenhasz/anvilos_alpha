{
  "module_name": "ice_base.c",
  "hash_id": "1d98d202eba8f8acf5e771209c2545fbc262950af28c80a0db4487997229668c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ice/ice_base.c",
  "human_readable_source": "\n \n\n#include <net/xdp_sock_drv.h>\n#include \"ice_base.h\"\n#include \"ice_lib.h\"\n#include \"ice_dcb_lib.h\"\n#include \"ice_sriov.h\"\n\n \nstatic int __ice_vsi_get_qs_contig(struct ice_qs_cfg *qs_cfg)\n{\n\tunsigned int offset, i;\n\n\tmutex_lock(qs_cfg->qs_mutex);\n\toffset = bitmap_find_next_zero_area(qs_cfg->pf_map, qs_cfg->pf_map_size,\n\t\t\t\t\t    0, qs_cfg->q_count, 0);\n\tif (offset >= qs_cfg->pf_map_size) {\n\t\tmutex_unlock(qs_cfg->qs_mutex);\n\t\treturn -ENOMEM;\n\t}\n\n\tbitmap_set(qs_cfg->pf_map, offset, qs_cfg->q_count);\n\tfor (i = 0; i < qs_cfg->q_count; i++)\n\t\tqs_cfg->vsi_map[i + qs_cfg->vsi_map_offset] = (u16)(i + offset);\n\tmutex_unlock(qs_cfg->qs_mutex);\n\n\treturn 0;\n}\n\n \nstatic int __ice_vsi_get_qs_sc(struct ice_qs_cfg *qs_cfg)\n{\n\tunsigned int i, index = 0;\n\n\tmutex_lock(qs_cfg->qs_mutex);\n\tfor (i = 0; i < qs_cfg->q_count; i++) {\n\t\tindex = find_next_zero_bit(qs_cfg->pf_map,\n\t\t\t\t\t   qs_cfg->pf_map_size, index);\n\t\tif (index >= qs_cfg->pf_map_size)\n\t\t\tgoto err_scatter;\n\t\tset_bit(index, qs_cfg->pf_map);\n\t\tqs_cfg->vsi_map[i + qs_cfg->vsi_map_offset] = (u16)index;\n\t}\n\tmutex_unlock(qs_cfg->qs_mutex);\n\n\treturn 0;\nerr_scatter:\n\tfor (index = 0; index < i; index++) {\n\t\tclear_bit(qs_cfg->vsi_map[index], qs_cfg->pf_map);\n\t\tqs_cfg->vsi_map[index + qs_cfg->vsi_map_offset] = 0;\n\t}\n\tmutex_unlock(qs_cfg->qs_mutex);\n\n\treturn -ENOMEM;\n}\n\n \nstatic int ice_pf_rxq_wait(struct ice_pf *pf, int pf_q, bool ena)\n{\n\tint i;\n\n\tfor (i = 0; i < ICE_Q_WAIT_MAX_RETRY; i++) {\n\t\tif (ena == !!(rd32(&pf->hw, QRX_CTRL(pf_q)) &\n\t\t\t      QRX_CTRL_QENA_STAT_M))\n\t\t\treturn 0;\n\n\t\tusleep_range(20, 40);\n\t}\n\n\treturn -ETIMEDOUT;\n}\n\n \nstatic int ice_vsi_alloc_q_vector(struct ice_vsi *vsi, u16 v_idx)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_q_vector *q_vector;\n\tint err;\n\n\t \n\tq_vector = kzalloc(sizeof(*q_vector), GFP_KERNEL);\n\tif (!q_vector)\n\t\treturn -ENOMEM;\n\n\tq_vector->vsi = vsi;\n\tq_vector->v_idx = v_idx;\n\tq_vector->tx.itr_setting = ICE_DFLT_TX_ITR;\n\tq_vector->rx.itr_setting = ICE_DFLT_RX_ITR;\n\tq_vector->tx.itr_mode = ITR_DYNAMIC;\n\tq_vector->rx.itr_mode = ITR_DYNAMIC;\n\tq_vector->tx.type = ICE_TX_CONTAINER;\n\tq_vector->rx.type = ICE_RX_CONTAINER;\n\tq_vector->irq.index = -ENOENT;\n\n\tif (vsi->type == ICE_VSI_VF) {\n\t\tq_vector->reg_idx = ice_calc_vf_reg_idx(vsi->vf, q_vector);\n\t\tgoto out;\n\t} else if (vsi->type == ICE_VSI_CTRL && vsi->vf) {\n\t\tstruct ice_vsi *ctrl_vsi = ice_get_vf_ctrl_vsi(pf, vsi);\n\n\t\tif (ctrl_vsi) {\n\t\t\tif (unlikely(!ctrl_vsi->q_vectors)) {\n\t\t\t\terr = -ENOENT;\n\t\t\t\tgoto err_free_q_vector;\n\t\t\t}\n\n\t\t\tq_vector->irq = ctrl_vsi->q_vectors[0]->irq;\n\t\t\tgoto skip_alloc;\n\t\t}\n\t}\n\n\tq_vector->irq = ice_alloc_irq(pf, vsi->irq_dyn_alloc);\n\tif (q_vector->irq.index < 0) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free_q_vector;\n\t}\n\nskip_alloc:\n\tq_vector->reg_idx = q_vector->irq.index;\n\n\t \n\tif (cpu_online(v_idx))\n\t\tcpumask_set_cpu(v_idx, &q_vector->affinity_mask);\n\n\t \n\tif (vsi->netdev)\n\t\tnetif_napi_add(vsi->netdev, &q_vector->napi, ice_napi_poll);\n\nout:\n\t \n\tvsi->q_vectors[v_idx] = q_vector;\n\n\treturn 0;\n\nerr_free_q_vector:\n\tkfree(q_vector);\n\n\treturn err;\n}\n\n \nstatic void ice_free_q_vector(struct ice_vsi *vsi, int v_idx)\n{\n\tstruct ice_q_vector *q_vector;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_tx_ring *tx_ring;\n\tstruct ice_rx_ring *rx_ring;\n\tstruct device *dev;\n\n\tdev = ice_pf_to_dev(pf);\n\tif (!vsi->q_vectors[v_idx]) {\n\t\tdev_dbg(dev, \"Queue vector at index %d not found\\n\", v_idx);\n\t\treturn;\n\t}\n\tq_vector = vsi->q_vectors[v_idx];\n\n\tice_for_each_tx_ring(tx_ring, q_vector->tx)\n\t\ttx_ring->q_vector = NULL;\n\tice_for_each_rx_ring(rx_ring, q_vector->rx)\n\t\trx_ring->q_vector = NULL;\n\n\t \n\tif (vsi->netdev)\n\t\tnetif_napi_del(&q_vector->napi);\n\n\t \n\tif (q_vector->irq.index < 0)\n\t\tgoto free_q_vector;\n\n\t \n\tif (vsi->type == ICE_VSI_CTRL && vsi->vf &&\n\t    ice_get_vf_ctrl_vsi(pf, vsi))\n\t\tgoto free_q_vector;\n\n\tice_free_irq(pf, q_vector->irq);\n\nfree_q_vector:\n\tkfree(q_vector);\n\tvsi->q_vectors[v_idx] = NULL;\n}\n\n \nstatic void ice_cfg_itr_gran(struct ice_hw *hw)\n{\n\tu32 regval = rd32(hw, GLINT_CTL);\n\n\t \n\tif (!(regval & GLINT_CTL_DIS_AUTOMASK_M) &&\n\t    (((regval & GLINT_CTL_ITR_GRAN_200_M) >>\n\t     GLINT_CTL_ITR_GRAN_200_S) == ICE_ITR_GRAN_US) &&\n\t    (((regval & GLINT_CTL_ITR_GRAN_100_M) >>\n\t     GLINT_CTL_ITR_GRAN_100_S) == ICE_ITR_GRAN_US) &&\n\t    (((regval & GLINT_CTL_ITR_GRAN_50_M) >>\n\t     GLINT_CTL_ITR_GRAN_50_S) == ICE_ITR_GRAN_US) &&\n\t    (((regval & GLINT_CTL_ITR_GRAN_25_M) >>\n\t      GLINT_CTL_ITR_GRAN_25_S) == ICE_ITR_GRAN_US))\n\t\treturn;\n\n\tregval = ((ICE_ITR_GRAN_US << GLINT_CTL_ITR_GRAN_200_S) &\n\t\t  GLINT_CTL_ITR_GRAN_200_M) |\n\t\t ((ICE_ITR_GRAN_US << GLINT_CTL_ITR_GRAN_100_S) &\n\t\t  GLINT_CTL_ITR_GRAN_100_M) |\n\t\t ((ICE_ITR_GRAN_US << GLINT_CTL_ITR_GRAN_50_S) &\n\t\t  GLINT_CTL_ITR_GRAN_50_M) |\n\t\t ((ICE_ITR_GRAN_US << GLINT_CTL_ITR_GRAN_25_S) &\n\t\t  GLINT_CTL_ITR_GRAN_25_M);\n\twr32(hw, GLINT_CTL, regval);\n}\n\n \nstatic u16 ice_calc_txq_handle(struct ice_vsi *vsi, struct ice_tx_ring *ring, u8 tc)\n{\n\tWARN_ONCE(ice_ring_is_xdp(ring) && tc, \"XDP ring can't belong to TC other than 0\\n\");\n\n\tif (ring->ch)\n\t\treturn ring->q_index - ring->ch->base_q;\n\n\t \n\treturn ring->q_index - vsi->tc_cfg.tc_info[tc].qoffset;\n}\n\n \nstatic u16 ice_eswitch_calc_txq_handle(struct ice_tx_ring *ring)\n{\n\tstruct ice_vsi *vsi = ring->vsi;\n\tint i;\n\n\tice_for_each_txq(vsi, i) {\n\t\tif (vsi->tx_rings[i] == ring)\n\t\t\treturn i;\n\t}\n\n\treturn ICE_INVAL_Q_INDEX;\n}\n\n \nstatic void ice_cfg_xps_tx_ring(struct ice_tx_ring *ring)\n{\n\tif (!ring->q_vector || !ring->netdev)\n\t\treturn;\n\n\t \n\tif (test_and_set_bit(ICE_TX_XPS_INIT_DONE, ring->xps_state))\n\t\treturn;\n\n\tnetif_set_xps_queue(ring->netdev, &ring->q_vector->affinity_mask,\n\t\t\t    ring->q_index);\n}\n\n \nstatic void\nice_setup_tx_ctx(struct ice_tx_ring *ring, struct ice_tlan_ctx *tlan_ctx, u16 pf_q)\n{\n\tstruct ice_vsi *vsi = ring->vsi;\n\tstruct ice_hw *hw = &vsi->back->hw;\n\n\ttlan_ctx->base = ring->dma >> ICE_TLAN_CTX_BASE_S;\n\n\ttlan_ctx->port_num = vsi->port_info->lport;\n\n\t \n\ttlan_ctx->qlen = ring->count;\n\n\tice_set_cgd_num(tlan_ctx, ring->dcb_tc);\n\n\t \n\ttlan_ctx->pf_num = hw->pf_id;\n\n\t \n\tswitch (vsi->type) {\n\tcase ICE_VSI_LB:\n\tcase ICE_VSI_CTRL:\n\tcase ICE_VSI_PF:\n\t\tif (ring->ch)\n\t\t\ttlan_ctx->vmvf_type = ICE_TLAN_CTX_VMVF_TYPE_VMQ;\n\t\telse\n\t\t\ttlan_ctx->vmvf_type = ICE_TLAN_CTX_VMVF_TYPE_PF;\n\t\tbreak;\n\tcase ICE_VSI_VF:\n\t\t \n\t\ttlan_ctx->vmvf_num = hw->func_caps.vf_base_id + vsi->vf->vf_id;\n\t\ttlan_ctx->vmvf_type = ICE_TLAN_CTX_VMVF_TYPE_VF;\n\t\tbreak;\n\tcase ICE_VSI_SWITCHDEV_CTRL:\n\t\ttlan_ctx->vmvf_type = ICE_TLAN_CTX_VMVF_TYPE_VMQ;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\t \n\tif (ring->ch)\n\t\ttlan_ctx->src_vsi = ring->ch->vsi_num;\n\telse\n\t\ttlan_ctx->src_vsi = ice_get_hw_vsi_num(hw, vsi->idx);\n\n\t \n\tswitch (vsi->type) {\n\tcase ICE_VSI_PF:\n\t\ttlan_ctx->tsyn_ena = 1;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\ttlan_ctx->tso_ena = ICE_TX_LEGACY;\n\ttlan_ctx->tso_qnum = pf_q;\n\n\t \n\ttlan_ctx->legacy_int = ICE_TX_LEGACY;\n}\n\n \nstatic unsigned int ice_rx_offset(struct ice_rx_ring *rx_ring)\n{\n\tif (ice_ring_uses_build_skb(rx_ring))\n\t\treturn ICE_SKB_PAD;\n\treturn 0;\n}\n\n \nstatic int ice_setup_rx_ctx(struct ice_rx_ring *ring)\n{\n\tstruct ice_vsi *vsi = ring->vsi;\n\tu32 rxdid = ICE_RXDID_FLEX_NIC;\n\tstruct ice_rlan_ctx rlan_ctx;\n\tstruct ice_hw *hw;\n\tu16 pf_q;\n\tint err;\n\n\thw = &vsi->back->hw;\n\n\t \n\tpf_q = vsi->rxq_map[ring->q_index];\n\n\t \n\tmemset(&rlan_ctx, 0, sizeof(rlan_ctx));\n\n\t \n\trlan_ctx.base = ring->dma >> ICE_RLAN_BASE_S;\n\n\trlan_ctx.qlen = ring->count;\n\n\t \n\trlan_ctx.dbuf = DIV_ROUND_UP(ring->rx_buf_len,\n\t\t\t\t     BIT_ULL(ICE_RLAN_CTX_DBUF_S));\n\n\t \n\trlan_ctx.dsize = 1;\n\n\t \n\trlan_ctx.crcstrip = !(ring->flags & ICE_RX_FLAGS_CRC_STRIP_DIS);\n\n\t \n\tif (ice_is_dvm_ena(hw))\n\t\tif (vsi->type == ICE_VSI_VF &&\n\t\t    ice_vf_is_port_vlan_ena(vsi->vf))\n\t\t\trlan_ctx.l2tsel = 1;\n\t\telse\n\t\t\trlan_ctx.l2tsel = 0;\n\telse\n\t\trlan_ctx.l2tsel = 1;\n\n\trlan_ctx.dtype = ICE_RX_DTYPE_NO_SPLIT;\n\trlan_ctx.hsplit_0 = ICE_RLAN_RX_HSPLIT_0_NO_SPLIT;\n\trlan_ctx.hsplit_1 = ICE_RLAN_RX_HSPLIT_1_NO_SPLIT;\n\n\t \n\trlan_ctx.showiv = 0;\n\n\t \n\trlan_ctx.rxmax = min_t(u32, vsi->max_frame,\n\t\t\t       ICE_MAX_CHAINED_RX_BUFS * ring->rx_buf_len);\n\n\t \n\trlan_ctx.lrxqthresh = 1;\n\n\t \n\tif (vsi->type != ICE_VSI_VF)\n\t\tice_write_qrxflxp_cntxt(hw, pf_q, rxdid, 0x3, true);\n\telse\n\t\tice_write_qrxflxp_cntxt(hw, pf_q, ICE_RXDID_LEGACY_1, 0x3,\n\t\t\t\t\tfalse);\n\n\t \n\terr = ice_write_rxq_ctx(hw, &rlan_ctx, pf_q);\n\tif (err) {\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Failed to set LAN Rx queue context for absolute Rx queue %d error: %d\\n\",\n\t\t\tpf_q, err);\n\t\treturn -EIO;\n\t}\n\n\tif (vsi->type == ICE_VSI_VF)\n\t\treturn 0;\n\n\t \n\tif (!vsi->netdev || test_bit(ICE_FLAG_LEGACY_RX, vsi->back->flags))\n\t\tice_clear_ring_build_skb_ena(ring);\n\telse\n\t\tice_set_ring_build_skb_ena(ring);\n\n\tring->rx_offset = ice_rx_offset(ring);\n\n\t \n\tring->tail = hw->hw_addr + QRX_TAIL(pf_q);\n\twritel(0, ring->tail);\n\n\treturn 0;\n}\n\n \nint ice_vsi_cfg_rxq(struct ice_rx_ring *ring)\n{\n\tstruct device *dev = ice_pf_to_dev(ring->vsi->back);\n\tu32 num_bufs = ICE_RX_DESC_UNUSED(ring);\n\tint err;\n\n\tring->rx_buf_len = ring->vsi->rx_buf_len;\n\n\tif (ring->vsi->type == ICE_VSI_PF) {\n\t\tif (!xdp_rxq_info_is_reg(&ring->xdp_rxq))\n\t\t\t \n\t\t\t__xdp_rxq_info_reg(&ring->xdp_rxq, ring->netdev,\n\t\t\t\t\t   ring->q_index,\n\t\t\t\t\t   ring->q_vector->napi.napi_id,\n\t\t\t\t\t   ring->vsi->rx_buf_len);\n\n\t\tring->xsk_pool = ice_xsk_pool(ring);\n\t\tif (ring->xsk_pool) {\n\t\t\txdp_rxq_info_unreg_mem_model(&ring->xdp_rxq);\n\n\t\t\tring->rx_buf_len =\n\t\t\t\txsk_pool_get_rx_frame_size(ring->xsk_pool);\n\t\t\terr = xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,\n\t\t\t\t\t\t\t MEM_TYPE_XSK_BUFF_POOL,\n\t\t\t\t\t\t\t NULL);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\txsk_pool_set_rxq_info(ring->xsk_pool, &ring->xdp_rxq);\n\n\t\t\tdev_info(dev, \"Registered XDP mem model MEM_TYPE_XSK_BUFF_POOL on Rx ring %d\\n\",\n\t\t\t\t ring->q_index);\n\t\t} else {\n\t\t\tif (!xdp_rxq_info_is_reg(&ring->xdp_rxq))\n\t\t\t\t \n\t\t\t\t__xdp_rxq_info_reg(&ring->xdp_rxq,\n\t\t\t\t\t\t   ring->netdev,\n\t\t\t\t\t\t   ring->q_index,\n\t\t\t\t\t\t   ring->q_vector->napi.napi_id,\n\t\t\t\t\t\t   ring->vsi->rx_buf_len);\n\n\t\t\terr = xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,\n\t\t\t\t\t\t\t MEM_TYPE_PAGE_SHARED,\n\t\t\t\t\t\t\t NULL);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\txdp_init_buff(&ring->xdp, ice_rx_pg_size(ring) / 2, &ring->xdp_rxq);\n\tring->xdp.data = NULL;\n\terr = ice_setup_rx_ctx(ring);\n\tif (err) {\n\t\tdev_err(dev, \"ice_setup_rx_ctx failed for RxQ %d, err %d\\n\",\n\t\t\tring->q_index, err);\n\t\treturn err;\n\t}\n\n\tif (ring->xsk_pool) {\n\t\tbool ok;\n\n\t\tif (!xsk_buff_can_alloc(ring->xsk_pool, num_bufs)) {\n\t\t\tdev_warn(dev, \"XSK buffer pool does not provide enough addresses to fill %d buffers on Rx ring %d\\n\",\n\t\t\t\t num_bufs, ring->q_index);\n\t\t\tdev_warn(dev, \"Change Rx ring/fill queue size to avoid performance issues\\n\");\n\n\t\t\treturn 0;\n\t\t}\n\n\t\tok = ice_alloc_rx_bufs_zc(ring, num_bufs);\n\t\tif (!ok) {\n\t\t\tu16 pf_q = ring->vsi->rxq_map[ring->q_index];\n\n\t\t\tdev_info(dev, \"Failed to allocate some buffers on XSK buffer pool enabled Rx ring %d (pf_q %d)\\n\",\n\t\t\t\t ring->q_index, pf_q);\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\tice_alloc_rx_bufs(ring, num_bufs);\n\n\treturn 0;\n}\n\n \nint __ice_vsi_get_qs(struct ice_qs_cfg *qs_cfg)\n{\n\tint ret = 0;\n\n\tret = __ice_vsi_get_qs_contig(qs_cfg);\n\tif (ret) {\n\t\t \n\t\tqs_cfg->mapping_mode = ICE_VSI_MAP_SCATTER;\n\t\tqs_cfg->q_count = min_t(unsigned int, qs_cfg->q_count,\n\t\t\t\t\tqs_cfg->scatter_count);\n\t\tret = __ice_vsi_get_qs_sc(qs_cfg);\n\t}\n\treturn ret;\n}\n\n \nint\nice_vsi_ctrl_one_rx_ring(struct ice_vsi *vsi, bool ena, u16 rxq_idx, bool wait)\n{\n\tint pf_q = vsi->rxq_map[rxq_idx];\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tu32 rx_reg;\n\n\trx_reg = rd32(hw, QRX_CTRL(pf_q));\n\n\t \n\tif (ena == !!(rx_reg & QRX_CTRL_QENA_STAT_M))\n\t\treturn 0;\n\n\t \n\tif (ena)\n\t\trx_reg |= QRX_CTRL_QENA_REQ_M;\n\telse\n\t\trx_reg &= ~QRX_CTRL_QENA_REQ_M;\n\twr32(hw, QRX_CTRL(pf_q), rx_reg);\n\n\tif (!wait)\n\t\treturn 0;\n\n\tice_flush(hw);\n\treturn ice_pf_rxq_wait(pf, pf_q, ena);\n}\n\n \nint ice_vsi_wait_one_rx_ring(struct ice_vsi *vsi, bool ena, u16 rxq_idx)\n{\n\tint pf_q = vsi->rxq_map[rxq_idx];\n\tstruct ice_pf *pf = vsi->back;\n\n\treturn ice_pf_rxq_wait(pf, pf_q, ena);\n}\n\n \nint ice_vsi_alloc_q_vectors(struct ice_vsi *vsi)\n{\n\tstruct device *dev = ice_pf_to_dev(vsi->back);\n\tu16 v_idx;\n\tint err;\n\n\tif (vsi->q_vectors[0]) {\n\t\tdev_dbg(dev, \"VSI %d has existing q_vectors\\n\", vsi->vsi_num);\n\t\treturn -EEXIST;\n\t}\n\n\tfor (v_idx = 0; v_idx < vsi->num_q_vectors; v_idx++) {\n\t\terr = ice_vsi_alloc_q_vector(vsi, v_idx);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\n\treturn 0;\n\nerr_out:\n\twhile (v_idx--)\n\t\tice_free_q_vector(vsi, v_idx);\n\n\tdev_err(dev, \"Failed to allocate %d q_vector for VSI %d, ret=%d\\n\",\n\t\tvsi->num_q_vectors, vsi->vsi_num, err);\n\tvsi->num_q_vectors = 0;\n\treturn err;\n}\n\n \nvoid ice_vsi_map_rings_to_vectors(struct ice_vsi *vsi)\n{\n\tint q_vectors = vsi->num_q_vectors;\n\tu16 tx_rings_rem, rx_rings_rem;\n\tint v_id;\n\n\t \n\ttx_rings_rem = vsi->num_txq;\n\trx_rings_rem = vsi->num_rxq;\n\n\tfor (v_id = 0; v_id < q_vectors; v_id++) {\n\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[v_id];\n\t\tu8 tx_rings_per_v, rx_rings_per_v;\n\t\tu16 q_id, q_base;\n\n\t\t \n\t\ttx_rings_per_v = (u8)DIV_ROUND_UP(tx_rings_rem,\n\t\t\t\t\t\t  q_vectors - v_id);\n\t\tq_vector->num_ring_tx = tx_rings_per_v;\n\t\tq_vector->tx.tx_ring = NULL;\n\t\tq_vector->tx.itr_idx = ICE_TX_ITR;\n\t\tq_base = vsi->num_txq - tx_rings_rem;\n\n\t\tfor (q_id = q_base; q_id < (q_base + tx_rings_per_v); q_id++) {\n\t\t\tstruct ice_tx_ring *tx_ring = vsi->tx_rings[q_id];\n\n\t\t\ttx_ring->q_vector = q_vector;\n\t\t\ttx_ring->next = q_vector->tx.tx_ring;\n\t\t\tq_vector->tx.tx_ring = tx_ring;\n\t\t}\n\t\ttx_rings_rem -= tx_rings_per_v;\n\n\t\t \n\t\trx_rings_per_v = (u8)DIV_ROUND_UP(rx_rings_rem,\n\t\t\t\t\t\t  q_vectors - v_id);\n\t\tq_vector->num_ring_rx = rx_rings_per_v;\n\t\tq_vector->rx.rx_ring = NULL;\n\t\tq_vector->rx.itr_idx = ICE_RX_ITR;\n\t\tq_base = vsi->num_rxq - rx_rings_rem;\n\n\t\tfor (q_id = q_base; q_id < (q_base + rx_rings_per_v); q_id++) {\n\t\t\tstruct ice_rx_ring *rx_ring = vsi->rx_rings[q_id];\n\n\t\t\trx_ring->q_vector = q_vector;\n\t\t\trx_ring->next = q_vector->rx.rx_ring;\n\t\t\tq_vector->rx.rx_ring = rx_ring;\n\t\t}\n\t\trx_rings_rem -= rx_rings_per_v;\n\t}\n}\n\n \nvoid ice_vsi_free_q_vectors(struct ice_vsi *vsi)\n{\n\tint v_idx;\n\n\tice_for_each_q_vector(vsi, v_idx)\n\t\tice_free_q_vector(vsi, v_idx);\n\n\tvsi->num_q_vectors = 0;\n}\n\n \nint\nice_vsi_cfg_txq(struct ice_vsi *vsi, struct ice_tx_ring *ring,\n\t\tstruct ice_aqc_add_tx_qgrp *qg_buf)\n{\n\tu8 buf_len = struct_size(qg_buf, txqs, 1);\n\tstruct ice_tlan_ctx tlan_ctx = { 0 };\n\tstruct ice_aqc_add_txqs_perq *txq;\n\tstruct ice_channel *ch = ring->ch;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tint status;\n\tu16 pf_q;\n\tu8 tc;\n\n\t \n\tice_cfg_xps_tx_ring(ring);\n\n\tpf_q = ring->reg_idx;\n\tice_setup_tx_ctx(ring, &tlan_ctx, pf_q);\n\t \n\tqg_buf->txqs[0].txq_id = cpu_to_le16(pf_q);\n\tice_set_ctx(hw, (u8 *)&tlan_ctx, qg_buf->txqs[0].txq_ctx,\n\t\t    ice_tlan_ctx_info);\n\n\t \n\tring->tail = hw->hw_addr + QTX_COMM_DBELL(pf_q);\n\n\tif (IS_ENABLED(CONFIG_DCB))\n\t\ttc = ring->dcb_tc;\n\telse\n\t\ttc = 0;\n\n\t \n\tif (vsi->type == ICE_VSI_SWITCHDEV_CTRL) {\n\t\tring->q_handle = ice_eswitch_calc_txq_handle(ring);\n\n\t\tif (ring->q_handle == ICE_INVAL_Q_INDEX)\n\t\t\treturn -ENODEV;\n\t} else {\n\t\tring->q_handle = ice_calc_txq_handle(vsi, ring, tc);\n\t}\n\n\tif (ch)\n\t\tstatus = ice_ena_vsi_txq(vsi->port_info, ch->ch_vsi->idx, 0,\n\t\t\t\t\t ring->q_handle, 1, qg_buf, buf_len,\n\t\t\t\t\t NULL);\n\telse\n\t\tstatus = ice_ena_vsi_txq(vsi->port_info, vsi->idx, tc,\n\t\t\t\t\t ring->q_handle, 1, qg_buf, buf_len,\n\t\t\t\t\t NULL);\n\tif (status) {\n\t\tdev_err(ice_pf_to_dev(pf), \"Failed to set LAN Tx queue context, error: %d\\n\",\n\t\t\tstatus);\n\t\treturn status;\n\t}\n\n\t \n\ttxq = &qg_buf->txqs[0];\n\tif (pf_q == le16_to_cpu(txq->txq_id))\n\t\tring->txq_teid = le32_to_cpu(txq->q_teid);\n\n\treturn 0;\n}\n\n \nvoid ice_cfg_itr(struct ice_hw *hw, struct ice_q_vector *q_vector)\n{\n\tice_cfg_itr_gran(hw);\n\n\tif (q_vector->num_ring_rx)\n\t\tice_write_itr(&q_vector->rx, q_vector->rx.itr_setting);\n\n\tif (q_vector->num_ring_tx)\n\t\tice_write_itr(&q_vector->tx, q_vector->tx.itr_setting);\n\n\tice_write_intrl(q_vector, q_vector->intrl);\n}\n\n \nvoid\nice_cfg_txq_interrupt(struct ice_vsi *vsi, u16 txq, u16 msix_idx, u16 itr_idx)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tu32 val;\n\n\titr_idx = (itr_idx << QINT_TQCTL_ITR_INDX_S) & QINT_TQCTL_ITR_INDX_M;\n\n\tval = QINT_TQCTL_CAUSE_ENA_M | itr_idx |\n\t      ((msix_idx << QINT_TQCTL_MSIX_INDX_S) & QINT_TQCTL_MSIX_INDX_M);\n\n\twr32(hw, QINT_TQCTL(vsi->txq_map[txq]), val);\n\tif (ice_is_xdp_ena_vsi(vsi)) {\n\t\tu32 xdp_txq = txq + vsi->num_xdp_txq;\n\n\t\twr32(hw, QINT_TQCTL(vsi->txq_map[xdp_txq]),\n\t\t     val);\n\t}\n\tice_flush(hw);\n}\n\n \nvoid\nice_cfg_rxq_interrupt(struct ice_vsi *vsi, u16 rxq, u16 msix_idx, u16 itr_idx)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tu32 val;\n\n\titr_idx = (itr_idx << QINT_RQCTL_ITR_INDX_S) & QINT_RQCTL_ITR_INDX_M;\n\n\tval = QINT_RQCTL_CAUSE_ENA_M | itr_idx |\n\t      ((msix_idx << QINT_RQCTL_MSIX_INDX_S) & QINT_RQCTL_MSIX_INDX_M);\n\n\twr32(hw, QINT_RQCTL(vsi->rxq_map[rxq]), val);\n\n\tice_flush(hw);\n}\n\n \nvoid ice_trigger_sw_intr(struct ice_hw *hw, struct ice_q_vector *q_vector)\n{\n\twr32(hw, GLINT_DYN_CTL(q_vector->reg_idx),\n\t     (ICE_ITR_NONE << GLINT_DYN_CTL_ITR_INDX_S) |\n\t     GLINT_DYN_CTL_SWINT_TRIG_M |\n\t     GLINT_DYN_CTL_INTENA_M);\n}\n\n \nint\nice_vsi_stop_tx_ring(struct ice_vsi *vsi, enum ice_disq_rst_src rst_src,\n\t\t     u16 rel_vmvf_num, struct ice_tx_ring *ring,\n\t\t     struct ice_txq_meta *txq_meta)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_q_vector *q_vector;\n\tstruct ice_hw *hw = &pf->hw;\n\tint status;\n\tu32 val;\n\n\t \n\tval = rd32(hw, QINT_TQCTL(ring->reg_idx));\n\tval &= ~QINT_TQCTL_CAUSE_ENA_M;\n\twr32(hw, QINT_TQCTL(ring->reg_idx), val);\n\n\t \n\tndelay(100);\n\n\t \n\tq_vector = ring->q_vector;\n\tif (q_vector && !(vsi->vf && ice_is_vf_disabled(vsi->vf)))\n\t\tice_trigger_sw_intr(hw, q_vector);\n\n\tstatus = ice_dis_vsi_txq(vsi->port_info, txq_meta->vsi_idx,\n\t\t\t\t txq_meta->tc, 1, &txq_meta->q_handle,\n\t\t\t\t &txq_meta->q_id, &txq_meta->q_teid, rst_src,\n\t\t\t\t rel_vmvf_num, NULL);\n\n\t \n\tif (status == -EBUSY) {\n\t\tdev_dbg(ice_pf_to_dev(vsi->back), \"Reset in progress. LAN Tx queues already disabled\\n\");\n\t} else if (status == -ENOENT) {\n\t\tdev_dbg(ice_pf_to_dev(vsi->back), \"LAN Tx queues do not exist, nothing to disable\\n\");\n\t} else if (status) {\n\t\tdev_dbg(ice_pf_to_dev(vsi->back), \"Failed to disable LAN Tx queues, error: %d\\n\",\n\t\t\tstatus);\n\t\treturn status;\n\t}\n\n\treturn 0;\n}\n\n \nvoid\nice_fill_txq_meta(struct ice_vsi *vsi, struct ice_tx_ring *ring,\n\t\t  struct ice_txq_meta *txq_meta)\n{\n\tstruct ice_channel *ch = ring->ch;\n\tu8 tc;\n\n\tif (IS_ENABLED(CONFIG_DCB))\n\t\ttc = ring->dcb_tc;\n\telse\n\t\ttc = 0;\n\n\ttxq_meta->q_id = ring->reg_idx;\n\ttxq_meta->q_teid = ring->txq_teid;\n\ttxq_meta->q_handle = ring->q_handle;\n\tif (ch) {\n\t\ttxq_meta->vsi_idx = ch->ch_vsi->idx;\n\t\ttxq_meta->tc = 0;\n\t} else {\n\t\ttxq_meta->vsi_idx = vsi->idx;\n\t\ttxq_meta->tc = tc;\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}