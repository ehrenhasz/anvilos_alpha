{
  "module_name": "ice_sriov.c",
  "hash_id": "1c5eede39528b7f3e12f779110274f91a2352a93b6a1dba155ea76679cbae4d9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ice/ice_sriov.c",
  "human_readable_source": "\n \n\n#include \"ice.h\"\n#include \"ice_vf_lib_private.h\"\n#include \"ice_base.h\"\n#include \"ice_lib.h\"\n#include \"ice_fltr.h\"\n#include \"ice_dcb_lib.h\"\n#include \"ice_flow.h\"\n#include \"ice_eswitch.h\"\n#include \"ice_virtchnl_allowlist.h\"\n#include \"ice_flex_pipe.h\"\n#include \"ice_vf_vsi_vlan_ops.h\"\n#include \"ice_vlan.h\"\n\n \nstatic void ice_free_vf_entries(struct ice_pf *pf)\n{\n\tstruct ice_vfs *vfs = &pf->vfs;\n\tstruct hlist_node *tmp;\n\tstruct ice_vf *vf;\n\tunsigned int bkt;\n\n\t \n\tlockdep_assert_held(&vfs->table_lock);\n\n\thash_for_each_safe(vfs->table, bkt, tmp, vf, entry) {\n\t\thash_del_rcu(&vf->entry);\n\t\tice_put_vf(vf);\n\t}\n}\n\n \nstatic void ice_free_vf_res(struct ice_vf *vf)\n{\n\tstruct ice_pf *pf = vf->pf;\n\tint i, last_vector_idx;\n\n\t \n\tclear_bit(ICE_VF_STATE_INIT, vf->vf_states);\n\tice_vf_fdir_exit(vf);\n\t \n\tif (vf->ctrl_vsi_idx != ICE_NO_VSI)\n\t\tice_vf_ctrl_vsi_release(vf);\n\n\t \n\tif (vf->lan_vsi_idx != ICE_NO_VSI) {\n\t\tice_vf_vsi_release(vf);\n\t\tvf->num_mac = 0;\n\t}\n\n\tlast_vector_idx = vf->first_vector_idx + pf->vfs.num_msix_per - 1;\n\n\t \n\tmemset(&vf->mdd_tx_events, 0, sizeof(vf->mdd_tx_events));\n\tmemset(&vf->mdd_rx_events, 0, sizeof(vf->mdd_rx_events));\n\n\t \n\tfor (i = vf->first_vector_idx; i <= last_vector_idx; i++) {\n\t\twr32(&pf->hw, GLINT_DYN_CTL(i), GLINT_DYN_CTL_CLEARPBA_M);\n\t\tice_flush(&pf->hw);\n\t}\n\t \n\tclear_bit(ICE_VF_STATE_MC_PROMISC, vf->vf_states);\n\tclear_bit(ICE_VF_STATE_UC_PROMISC, vf->vf_states);\n}\n\n \nstatic void ice_dis_vf_mappings(struct ice_vf *vf)\n{\n\tstruct ice_pf *pf = vf->pf;\n\tstruct ice_vsi *vsi;\n\tstruct device *dev;\n\tint first, last, v;\n\tstruct ice_hw *hw;\n\n\thw = &pf->hw;\n\tvsi = ice_get_vf_vsi(vf);\n\tif (WARN_ON(!vsi))\n\t\treturn;\n\n\tdev = ice_pf_to_dev(pf);\n\twr32(hw, VPINT_ALLOC(vf->vf_id), 0);\n\twr32(hw, VPINT_ALLOC_PCI(vf->vf_id), 0);\n\n\tfirst = vf->first_vector_idx;\n\tlast = first + pf->vfs.num_msix_per - 1;\n\tfor (v = first; v <= last; v++) {\n\t\tu32 reg;\n\n\t\treg = (((1 << GLINT_VECT2FUNC_IS_PF_S) &\n\t\t\tGLINT_VECT2FUNC_IS_PF_M) |\n\t\t       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &\n\t\t\tGLINT_VECT2FUNC_PF_NUM_M));\n\t\twr32(hw, GLINT_VECT2FUNC(v), reg);\n\t}\n\n\tif (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG)\n\t\twr32(hw, VPLAN_TX_QBASE(vf->vf_id), 0);\n\telse\n\t\tdev_err(dev, \"Scattered mode for VF Tx queues is not yet implemented\\n\");\n\n\tif (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG)\n\t\twr32(hw, VPLAN_RX_QBASE(vf->vf_id), 0);\n\telse\n\t\tdev_err(dev, \"Scattered mode for VF Rx queues is not yet implemented\\n\");\n}\n\n \nstatic int ice_sriov_free_msix_res(struct ice_pf *pf)\n{\n\tif (!pf)\n\t\treturn -EINVAL;\n\n\tpf->sriov_base_vector = 0;\n\n\treturn 0;\n}\n\n \nvoid ice_free_vfs(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_vfs *vfs = &pf->vfs;\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct ice_vf *vf;\n\tunsigned int bkt;\n\n\tif (!ice_has_vfs(pf))\n\t\treturn;\n\n\twhile (test_and_set_bit(ICE_VF_DIS, pf->state))\n\t\tusleep_range(1000, 2000);\n\n\t \n\tif (!pci_vfs_assigned(pf->pdev))\n\t\tpci_disable_sriov(pf->pdev);\n\telse\n\t\tdev_warn(dev, \"VFs are assigned - not disabling SR-IOV\\n\");\n\n\tmutex_lock(&vfs->table_lock);\n\n\tice_eswitch_release(pf);\n\n\tice_for_each_vf(pf, bkt, vf) {\n\t\tmutex_lock(&vf->cfg_lock);\n\n\t\tice_dis_vf_qs(vf);\n\n\t\tif (test_bit(ICE_VF_STATE_INIT, vf->vf_states)) {\n\t\t\t \n\t\t\tice_dis_vf_mappings(vf);\n\t\t\tset_bit(ICE_VF_STATE_DIS, vf->vf_states);\n\t\t\tice_free_vf_res(vf);\n\t\t}\n\n\t\tif (!pci_vfs_assigned(pf->pdev)) {\n\t\t\tu32 reg_idx, bit_idx;\n\n\t\t\treg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;\n\t\t\tbit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;\n\t\t\twr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));\n\t\t}\n\n\t\t \n\t\tlist_del(&vf->mbx_info.list_entry);\n\n\t\tmutex_unlock(&vf->cfg_lock);\n\t}\n\n\tif (ice_sriov_free_msix_res(pf))\n\t\tdev_err(dev, \"Failed to free MSIX resources used by SR-IOV\\n\");\n\n\tvfs->num_qps_per = 0;\n\tice_free_vf_entries(pf);\n\n\tmutex_unlock(&vfs->table_lock);\n\n\tclear_bit(ICE_VF_DIS, pf->state);\n\tclear_bit(ICE_FLAG_SRIOV_ENA, pf->flags);\n}\n\n \nstatic struct ice_vsi *ice_vf_vsi_setup(struct ice_vf *vf)\n{\n\tstruct ice_vsi_cfg_params params = {};\n\tstruct ice_pf *pf = vf->pf;\n\tstruct ice_vsi *vsi;\n\n\tparams.type = ICE_VSI_VF;\n\tparams.pi = ice_vf_get_port_info(vf);\n\tparams.vf = vf;\n\tparams.flags = ICE_VSI_FLAG_INIT;\n\n\tvsi = ice_vsi_setup(pf, &params);\n\n\tif (!vsi) {\n\t\tdev_err(ice_pf_to_dev(pf), \"Failed to create VF VSI\\n\");\n\t\tice_vf_invalidate_vsi(vf);\n\t\treturn NULL;\n\t}\n\n\tvf->lan_vsi_idx = vsi->idx;\n\tvf->lan_vsi_num = vsi->vsi_num;\n\n\treturn vsi;\n}\n\n \nstatic int ice_calc_vf_first_vector_idx(struct ice_pf *pf, struct ice_vf *vf)\n{\n\treturn pf->sriov_base_vector + vf->vf_id * pf->vfs.num_msix_per;\n}\n\n \nstatic void ice_ena_vf_msix_mappings(struct ice_vf *vf)\n{\n\tint device_based_first_msix, device_based_last_msix;\n\tint pf_based_first_msix, pf_based_last_msix, v;\n\tstruct ice_pf *pf = vf->pf;\n\tint device_based_vf_id;\n\tstruct ice_hw *hw;\n\tu32 reg;\n\n\thw = &pf->hw;\n\tpf_based_first_msix = vf->first_vector_idx;\n\tpf_based_last_msix = (pf_based_first_msix + pf->vfs.num_msix_per) - 1;\n\n\tdevice_based_first_msix = pf_based_first_msix +\n\t\tpf->hw.func_caps.common_cap.msix_vector_first_id;\n\tdevice_based_last_msix =\n\t\t(device_based_first_msix + pf->vfs.num_msix_per) - 1;\n\tdevice_based_vf_id = vf->vf_id + hw->func_caps.vf_base_id;\n\n\treg = (((device_based_first_msix << VPINT_ALLOC_FIRST_S) &\n\t\tVPINT_ALLOC_FIRST_M) |\n\t       ((device_based_last_msix << VPINT_ALLOC_LAST_S) &\n\t\tVPINT_ALLOC_LAST_M) | VPINT_ALLOC_VALID_M);\n\twr32(hw, VPINT_ALLOC(vf->vf_id), reg);\n\n\treg = (((device_based_first_msix << VPINT_ALLOC_PCI_FIRST_S)\n\t\t & VPINT_ALLOC_PCI_FIRST_M) |\n\t       ((device_based_last_msix << VPINT_ALLOC_PCI_LAST_S) &\n\t\tVPINT_ALLOC_PCI_LAST_M) | VPINT_ALLOC_PCI_VALID_M);\n\twr32(hw, VPINT_ALLOC_PCI(vf->vf_id), reg);\n\n\t \n\tfor (v = pf_based_first_msix; v <= pf_based_last_msix; v++) {\n\t\treg = (((device_based_vf_id << GLINT_VECT2FUNC_VF_NUM_S) &\n\t\t\tGLINT_VECT2FUNC_VF_NUM_M) |\n\t\t       ((hw->pf_id << GLINT_VECT2FUNC_PF_NUM_S) &\n\t\t\tGLINT_VECT2FUNC_PF_NUM_M));\n\t\twr32(hw, GLINT_VECT2FUNC(v), reg);\n\t}\n\n\t \n\twr32(hw, VPINT_MBX_CTL(device_based_vf_id), VPINT_MBX_CTL_CAUSE_ENA_M);\n}\n\n \nstatic void ice_ena_vf_q_mappings(struct ice_vf *vf, u16 max_txq, u16 max_rxq)\n{\n\tstruct device *dev = ice_pf_to_dev(vf->pf);\n\tstruct ice_vsi *vsi = ice_get_vf_vsi(vf);\n\tstruct ice_hw *hw = &vf->pf->hw;\n\tu32 reg;\n\n\tif (WARN_ON(!vsi))\n\t\treturn;\n\n\t \n\twr32(hw, VPLAN_TXQ_MAPENA(vf->vf_id), VPLAN_TXQ_MAPENA_TX_ENA_M);\n\n\t \n\tif (vsi->tx_mapping_mode == ICE_VSI_MAP_CONTIG) {\n\t\t \n\t\treg = (((vsi->txq_map[0] << VPLAN_TX_QBASE_VFFIRSTQ_S) &\n\t\t\tVPLAN_TX_QBASE_VFFIRSTQ_M) |\n\t\t       (((max_txq - 1) << VPLAN_TX_QBASE_VFNUMQ_S) &\n\t\t\tVPLAN_TX_QBASE_VFNUMQ_M));\n\t\twr32(hw, VPLAN_TX_QBASE(vf->vf_id), reg);\n\t} else {\n\t\tdev_err(dev, \"Scattered mode for VF Tx queues is not yet implemented\\n\");\n\t}\n\n\t \n\twr32(hw, VPLAN_RXQ_MAPENA(vf->vf_id), VPLAN_RXQ_MAPENA_RX_ENA_M);\n\n\t \n\tif (vsi->rx_mapping_mode == ICE_VSI_MAP_CONTIG) {\n\t\t \n\t\treg = (((vsi->rxq_map[0] << VPLAN_RX_QBASE_VFFIRSTQ_S) &\n\t\t\tVPLAN_RX_QBASE_VFFIRSTQ_M) |\n\t\t       (((max_rxq - 1) << VPLAN_RX_QBASE_VFNUMQ_S) &\n\t\t\tVPLAN_RX_QBASE_VFNUMQ_M));\n\t\twr32(hw, VPLAN_RX_QBASE(vf->vf_id), reg);\n\t} else {\n\t\tdev_err(dev, \"Scattered mode for VF Rx queues is not yet implemented\\n\");\n\t}\n}\n\n \nstatic void ice_ena_vf_mappings(struct ice_vf *vf)\n{\n\tstruct ice_vsi *vsi = ice_get_vf_vsi(vf);\n\n\tif (WARN_ON(!vsi))\n\t\treturn;\n\n\tice_ena_vf_msix_mappings(vf);\n\tice_ena_vf_q_mappings(vf, vsi->alloc_txq, vsi->alloc_rxq);\n}\n\n \nint ice_calc_vf_reg_idx(struct ice_vf *vf, struct ice_q_vector *q_vector)\n{\n\tstruct ice_pf *pf;\n\n\tif (!vf || !q_vector)\n\t\treturn -EINVAL;\n\n\tpf = vf->pf;\n\n\t \n\treturn pf->sriov_base_vector + pf->vfs.num_msix_per * vf->vf_id +\n\t\tq_vector->v_idx + 1;\n}\n\n \nstatic int ice_sriov_set_msix_res(struct ice_pf *pf, u16 num_msix_needed)\n{\n\tu16 total_vectors = pf->hw.func_caps.common_cap.num_msix_vectors;\n\tint vectors_used = ice_get_max_used_msix_vector(pf);\n\tint sriov_base_vector;\n\n\tsriov_base_vector = total_vectors - num_msix_needed;\n\n\t \n\tif (sriov_base_vector < vectors_used)\n\t\treturn -EINVAL;\n\n\tpf->sriov_base_vector = sriov_base_vector;\n\n\treturn 0;\n}\n\n \nstatic int ice_set_per_vf_res(struct ice_pf *pf, u16 num_vfs)\n{\n\tint vectors_used = ice_get_max_used_msix_vector(pf);\n\tu16 num_msix_per_vf, num_txq, num_rxq, avail_qs;\n\tint msix_avail_per_vf, msix_avail_for_sriov;\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tint err;\n\n\tlockdep_assert_held(&pf->vfs.table_lock);\n\n\tif (!num_vfs)\n\t\treturn -EINVAL;\n\n\t \n\tmsix_avail_for_sriov = pf->hw.func_caps.common_cap.num_msix_vectors -\n\t\tvectors_used;\n\tmsix_avail_per_vf = msix_avail_for_sriov / num_vfs;\n\tif (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MED) {\n\t\tnum_msix_per_vf = ICE_NUM_VF_MSIX_MED;\n\t} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_SMALL) {\n\t\tnum_msix_per_vf = ICE_NUM_VF_MSIX_SMALL;\n\t} else if (msix_avail_per_vf >= ICE_NUM_VF_MSIX_MULTIQ_MIN) {\n\t\tnum_msix_per_vf = ICE_NUM_VF_MSIX_MULTIQ_MIN;\n\t} else if (msix_avail_per_vf >= ICE_MIN_INTR_PER_VF) {\n\t\tnum_msix_per_vf = ICE_MIN_INTR_PER_VF;\n\t} else {\n\t\tdev_err(dev, \"Only %d MSI-X interrupts available for SR-IOV. Not enough to support minimum of %d MSI-X interrupts per VF for %d VFs\\n\",\n\t\t\tmsix_avail_for_sriov, ICE_MIN_INTR_PER_VF,\n\t\t\tnum_vfs);\n\t\treturn -ENOSPC;\n\t}\n\n\tnum_txq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,\n\t\t\tICE_MAX_RSS_QS_PER_VF);\n\tavail_qs = ice_get_avail_txq_count(pf) / num_vfs;\n\tif (!avail_qs)\n\t\tnum_txq = 0;\n\telse if (num_txq > avail_qs)\n\t\tnum_txq = rounddown_pow_of_two(avail_qs);\n\n\tnum_rxq = min_t(u16, num_msix_per_vf - ICE_NONQ_VECS_VF,\n\t\t\tICE_MAX_RSS_QS_PER_VF);\n\tavail_qs = ice_get_avail_rxq_count(pf) / num_vfs;\n\tif (!avail_qs)\n\t\tnum_rxq = 0;\n\telse if (num_rxq > avail_qs)\n\t\tnum_rxq = rounddown_pow_of_two(avail_qs);\n\n\tif (num_txq < ICE_MIN_QS_PER_VF || num_rxq < ICE_MIN_QS_PER_VF) {\n\t\tdev_err(dev, \"Not enough queues to support minimum of %d queue pairs per VF for %d VFs\\n\",\n\t\t\tICE_MIN_QS_PER_VF, num_vfs);\n\t\treturn -ENOSPC;\n\t}\n\n\terr = ice_sriov_set_msix_res(pf, num_msix_per_vf * num_vfs);\n\tif (err) {\n\t\tdev_err(dev, \"Unable to set MSI-X resources for %d VFs, err %d\\n\",\n\t\t\tnum_vfs, err);\n\t\treturn err;\n\t}\n\n\t \n\tpf->vfs.num_qps_per = min_t(int, num_txq, num_rxq);\n\tpf->vfs.num_msix_per = num_msix_per_vf;\n\tdev_info(dev, \"Enabling %d VFs with %d vectors and %d queues per VF\\n\",\n\t\t num_vfs, pf->vfs.num_msix_per, pf->vfs.num_qps_per);\n\n\treturn 0;\n}\n\n \nstatic int ice_init_vf_vsi_res(struct ice_vf *vf)\n{\n\tstruct ice_pf *pf = vf->pf;\n\tstruct ice_vsi *vsi;\n\tint err;\n\n\tvf->first_vector_idx = ice_calc_vf_first_vector_idx(pf, vf);\n\n\tvsi = ice_vf_vsi_setup(vf);\n\tif (!vsi)\n\t\treturn -ENOMEM;\n\n\terr = ice_vf_init_host_cfg(vf, vsi);\n\tif (err)\n\t\tgoto release_vsi;\n\n\treturn 0;\n\nrelease_vsi:\n\tice_vf_vsi_release(vf);\n\treturn err;\n}\n\n \nstatic int ice_start_vfs(struct ice_pf *pf)\n{\n\tstruct ice_hw *hw = &pf->hw;\n\tunsigned int bkt, it_cnt;\n\tstruct ice_vf *vf;\n\tint retval;\n\n\tlockdep_assert_held(&pf->vfs.table_lock);\n\n\tit_cnt = 0;\n\tice_for_each_vf(pf, bkt, vf) {\n\t\tvf->vf_ops->clear_reset_trigger(vf);\n\n\t\tretval = ice_init_vf_vsi_res(vf);\n\t\tif (retval) {\n\t\t\tdev_err(ice_pf_to_dev(pf), \"Failed to initialize VSI resources for VF %d, error %d\\n\",\n\t\t\t\tvf->vf_id, retval);\n\t\t\tgoto teardown;\n\t\t}\n\n\t\tset_bit(ICE_VF_STATE_INIT, vf->vf_states);\n\t\tice_ena_vf_mappings(vf);\n\t\twr32(hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);\n\t\tit_cnt++;\n\t}\n\n\tice_flush(hw);\n\treturn 0;\n\nteardown:\n\tice_for_each_vf(pf, bkt, vf) {\n\t\tif (it_cnt == 0)\n\t\t\tbreak;\n\n\t\tice_dis_vf_mappings(vf);\n\t\tice_vf_vsi_release(vf);\n\t\tit_cnt--;\n\t}\n\n\treturn retval;\n}\n\n \nstatic void ice_sriov_free_vf(struct ice_vf *vf)\n{\n\tmutex_destroy(&vf->cfg_lock);\n\n\tkfree_rcu(vf, rcu);\n}\n\n \nstatic void ice_sriov_clear_reset_state(struct ice_vf *vf)\n{\n\tstruct ice_hw *hw = &vf->pf->hw;\n\n\t \n\twr32(hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_INPROGRESS);\n}\n\n \nstatic void ice_sriov_clear_mbx_register(struct ice_vf *vf)\n{\n\tstruct ice_pf *pf = vf->pf;\n\n\twr32(&pf->hw, VF_MBX_ARQLEN(vf->vf_id), 0);\n\twr32(&pf->hw, VF_MBX_ATQLEN(vf->vf_id), 0);\n}\n\n \nstatic void ice_sriov_trigger_reset_register(struct ice_vf *vf, bool is_vflr)\n{\n\tstruct ice_pf *pf = vf->pf;\n\tu32 reg, reg_idx, bit_idx;\n\tunsigned int vf_abs_id, i;\n\tstruct device *dev;\n\tstruct ice_hw *hw;\n\n\tdev = ice_pf_to_dev(pf);\n\thw = &pf->hw;\n\tvf_abs_id = vf->vf_id + hw->func_caps.vf_base_id;\n\n\t \n\tif (!is_vflr) {\n\t\treg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));\n\t\treg |= VPGEN_VFRTRIG_VFSWR_M;\n\t\twr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);\n\t}\n\n\t \n\treg_idx = (vf_abs_id) / 32;\n\tbit_idx = (vf_abs_id) % 32;\n\twr32(hw, GLGEN_VFLRSTAT(reg_idx), BIT(bit_idx));\n\tice_flush(hw);\n\n\twr32(hw, PF_PCI_CIAA,\n\t     VF_DEVICE_STATUS | (vf_abs_id << PF_PCI_CIAA_VF_NUM_S));\n\tfor (i = 0; i < ICE_PCI_CIAD_WAIT_COUNT; i++) {\n\t\treg = rd32(hw, PF_PCI_CIAD);\n\t\t \n\t\tif ((reg & VF_TRANS_PENDING_M) == 0)\n\t\t\tbreak;\n\n\t\tdev_err(dev, \"VF %u PCI transactions stuck\\n\", vf->vf_id);\n\t\tudelay(ICE_PCI_CIAD_WAIT_DELAY_US);\n\t}\n}\n\n \nstatic bool ice_sriov_poll_reset_status(struct ice_vf *vf)\n{\n\tstruct ice_pf *pf = vf->pf;\n\tunsigned int i;\n\tu32 reg;\n\n\tfor (i = 0; i < 10; i++) {\n\t\t \n\t\treg = rd32(&pf->hw, VPGEN_VFRSTAT(vf->vf_id));\n\t\tif (reg & VPGEN_VFRSTAT_VFRD_M)\n\t\t\treturn true;\n\n\t\t \n\t\tusleep_range(10, 20);\n\t}\n\treturn false;\n}\n\n \nstatic void ice_sriov_clear_reset_trigger(struct ice_vf *vf)\n{\n\tstruct ice_hw *hw = &vf->pf->hw;\n\tu32 reg;\n\n\treg = rd32(hw, VPGEN_VFRTRIG(vf->vf_id));\n\treg &= ~VPGEN_VFRTRIG_VFSWR_M;\n\twr32(hw, VPGEN_VFRTRIG(vf->vf_id), reg);\n\tice_flush(hw);\n}\n\n \nstatic int ice_sriov_create_vsi(struct ice_vf *vf)\n{\n\tstruct ice_vsi *vsi;\n\n\tvsi = ice_vf_vsi_setup(vf);\n\tif (!vsi)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nstatic void ice_sriov_post_vsi_rebuild(struct ice_vf *vf)\n{\n\tice_ena_vf_mappings(vf);\n\twr32(&vf->pf->hw, VFGEN_RSTAT(vf->vf_id), VIRTCHNL_VFR_VFACTIVE);\n}\n\nstatic const struct ice_vf_ops ice_sriov_vf_ops = {\n\t.reset_type = ICE_VF_RESET,\n\t.free = ice_sriov_free_vf,\n\t.clear_reset_state = ice_sriov_clear_reset_state,\n\t.clear_mbx_register = ice_sriov_clear_mbx_register,\n\t.trigger_reset_register = ice_sriov_trigger_reset_register,\n\t.poll_reset_status = ice_sriov_poll_reset_status,\n\t.clear_reset_trigger = ice_sriov_clear_reset_trigger,\n\t.irq_close = NULL,\n\t.create_vsi = ice_sriov_create_vsi,\n\t.post_vsi_rebuild = ice_sriov_post_vsi_rebuild,\n};\n\n \nstatic int ice_create_vf_entries(struct ice_pf *pf, u16 num_vfs)\n{\n\tstruct ice_vfs *vfs = &pf->vfs;\n\tstruct ice_vf *vf;\n\tu16 vf_id;\n\tint err;\n\n\tlockdep_assert_held(&vfs->table_lock);\n\n\tfor (vf_id = 0; vf_id < num_vfs; vf_id++) {\n\t\tvf = kzalloc(sizeof(*vf), GFP_KERNEL);\n\t\tif (!vf) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_entries;\n\t\t}\n\t\tkref_init(&vf->refcnt);\n\n\t\tvf->pf = pf;\n\t\tvf->vf_id = vf_id;\n\n\t\t \n\t\tvf->vf_ops = &ice_sriov_vf_ops;\n\n\t\tice_initialize_vf_entry(vf);\n\n\t\tvf->vf_sw_id = pf->first_sw;\n\n\t\thash_add_rcu(vfs->table, &vf->entry, vf_id);\n\t}\n\n\treturn 0;\n\nerr_free_entries:\n\tice_free_vf_entries(pf);\n\treturn err;\n}\n\n \nstatic int ice_ena_vfs(struct ice_pf *pf, u16 num_vfs)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_hw *hw = &pf->hw;\n\tint ret;\n\n\t \n\twr32(hw, GLINT_DYN_CTL(pf->oicr_irq.index),\n\t     ICE_ITR_NONE << GLINT_DYN_CTL_ITR_INDX_S);\n\tset_bit(ICE_OICR_INTR_DIS, pf->state);\n\tice_flush(hw);\n\n\tret = pci_enable_sriov(pf->pdev, num_vfs);\n\tif (ret)\n\t\tgoto err_unroll_intr;\n\n\tmutex_lock(&pf->vfs.table_lock);\n\n\tret = ice_set_per_vf_res(pf, num_vfs);\n\tif (ret) {\n\t\tdev_err(dev, \"Not enough resources for %d VFs, err %d. Try with fewer number of VFs\\n\",\n\t\t\tnum_vfs, ret);\n\t\tgoto err_unroll_sriov;\n\t}\n\n\tret = ice_create_vf_entries(pf, num_vfs);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to allocate VF entries for %d VFs\\n\",\n\t\t\tnum_vfs);\n\t\tgoto err_unroll_sriov;\n\t}\n\n\tret = ice_start_vfs(pf);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to start %d VFs, err %d\\n\", num_vfs, ret);\n\t\tret = -EAGAIN;\n\t\tgoto err_unroll_vf_entries;\n\t}\n\n\tclear_bit(ICE_VF_DIS, pf->state);\n\n\tret = ice_eswitch_configure(pf);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to configure eswitch, err %d\\n\", ret);\n\t\tgoto err_unroll_sriov;\n\t}\n\n\t \n\tif (test_and_clear_bit(ICE_OICR_INTR_DIS, pf->state))\n\t\tice_irq_dynamic_ena(hw, NULL, NULL);\n\n\tmutex_unlock(&pf->vfs.table_lock);\n\n\treturn 0;\n\nerr_unroll_vf_entries:\n\tice_free_vf_entries(pf);\nerr_unroll_sriov:\n\tmutex_unlock(&pf->vfs.table_lock);\n\tpci_disable_sriov(pf->pdev);\nerr_unroll_intr:\n\t \n\tice_irq_dynamic_ena(hw, NULL, NULL);\n\tclear_bit(ICE_OICR_INTR_DIS, pf->state);\n\treturn ret;\n}\n\n \nstatic int ice_pci_sriov_ena(struct ice_pf *pf, int num_vfs)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tint err;\n\n\tif (!num_vfs) {\n\t\tice_free_vfs(pf);\n\t\treturn 0;\n\t}\n\n\tif (num_vfs > pf->vfs.num_supported) {\n\t\tdev_err(dev, \"Can't enable %d VFs, max VFs supported is %d\\n\",\n\t\t\tnum_vfs, pf->vfs.num_supported);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tdev_info(dev, \"Enabling %d VFs\\n\", num_vfs);\n\terr = ice_ena_vfs(pf, num_vfs);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to enable SR-IOV: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tset_bit(ICE_FLAG_SRIOV_ENA, pf->flags);\n\treturn 0;\n}\n\n \nstatic int ice_check_sriov_allowed(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\n\tif (!test_bit(ICE_FLAG_SRIOV_CAPABLE, pf->flags)) {\n\t\tdev_err(dev, \"This device is not capable of SR-IOV\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (ice_is_safe_mode(pf)) {\n\t\tdev_err(dev, \"SR-IOV cannot be configured - Device is in Safe Mode\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!ice_pf_state_is_nominal(pf)) {\n\t\tdev_err(dev, \"Cannot enable SR-IOV, device not ready\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\n \nint ice_sriov_configure(struct pci_dev *pdev, int num_vfs)\n{\n\tstruct ice_pf *pf = pci_get_drvdata(pdev);\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tint err;\n\n\terr = ice_check_sriov_allowed(pf);\n\tif (err)\n\t\treturn err;\n\n\tif (!num_vfs) {\n\t\tif (!pci_vfs_assigned(pdev)) {\n\t\t\tice_free_vfs(pf);\n\t\t\treturn 0;\n\t\t}\n\n\t\tdev_err(dev, \"can't free VFs because some are assigned to VMs.\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\terr = ice_pci_sriov_ena(pf, num_vfs);\n\tif (err)\n\t\treturn err;\n\n\treturn num_vfs;\n}\n\n \nvoid ice_process_vflr_event(struct ice_pf *pf)\n{\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct ice_vf *vf;\n\tunsigned int bkt;\n\tu32 reg;\n\n\tif (!test_and_clear_bit(ICE_VFLR_EVENT_PENDING, pf->state) ||\n\t    !ice_has_vfs(pf))\n\t\treturn;\n\n\tmutex_lock(&pf->vfs.table_lock);\n\tice_for_each_vf(pf, bkt, vf) {\n\t\tu32 reg_idx, bit_idx;\n\n\t\treg_idx = (hw->func_caps.vf_base_id + vf->vf_id) / 32;\n\t\tbit_idx = (hw->func_caps.vf_base_id + vf->vf_id) % 32;\n\t\t \n\t\treg = rd32(hw, GLGEN_VFLRSTAT(reg_idx));\n\t\tif (reg & BIT(bit_idx))\n\t\t\t \n\t\t\tice_reset_vf(vf, ICE_VF_RESET_VFLR | ICE_VF_RESET_LOCK);\n\t}\n\tmutex_unlock(&pf->vfs.table_lock);\n}\n\n \nstatic struct ice_vf *ice_get_vf_from_pfq(struct ice_pf *pf, u16 pfq)\n{\n\tstruct ice_vf *vf;\n\tunsigned int bkt;\n\n\trcu_read_lock();\n\tice_for_each_vf_rcu(pf, bkt, vf) {\n\t\tstruct ice_vsi *vsi;\n\t\tu16 rxq_idx;\n\n\t\tvsi = ice_get_vf_vsi(vf);\n\t\tif (!vsi)\n\t\t\tcontinue;\n\n\t\tice_for_each_rxq(vsi, rxq_idx)\n\t\t\tif (vsi->rxq_map[rxq_idx] == pfq) {\n\t\t\t\tstruct ice_vf *found;\n\n\t\t\t\tif (kref_get_unless_zero(&vf->refcnt))\n\t\t\t\t\tfound = vf;\n\t\t\t\telse\n\t\t\t\t\tfound = NULL;\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn found;\n\t\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn NULL;\n}\n\n \nstatic u32 ice_globalq_to_pfq(struct ice_pf *pf, u32 globalq)\n{\n\treturn globalq - pf->hw.func_caps.common_cap.rxq_first_id;\n}\n\n \nvoid\nice_vf_lan_overflow_event(struct ice_pf *pf, struct ice_rq_event_info *event)\n{\n\tu32 gldcb_rtctq, queue;\n\tstruct ice_vf *vf;\n\n\tgldcb_rtctq = le32_to_cpu(event->desc.params.lan_overflow.prtdcb_ruptq);\n\tdev_dbg(ice_pf_to_dev(pf), \"GLDCB_RTCTQ: 0x%08x\\n\", gldcb_rtctq);\n\n\t \n\tqueue = (gldcb_rtctq & GLDCB_RTCTQ_RXQNUM_M) >>\n\t\tGLDCB_RTCTQ_RXQNUM_S;\n\n\tvf = ice_get_vf_from_pfq(pf, ice_globalq_to_pfq(pf, queue));\n\tif (!vf)\n\t\treturn;\n\n\tice_reset_vf(vf, ICE_VF_RESET_NOTIFY | ICE_VF_RESET_LOCK);\n\tice_put_vf(vf);\n}\n\n \nint ice_set_vf_spoofchk(struct net_device *netdev, int vf_id, bool ena)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_pf *pf = np->vsi->back;\n\tstruct ice_vsi *vf_vsi;\n\tstruct device *dev;\n\tstruct ice_vf *vf;\n\tint ret;\n\n\tdev = ice_pf_to_dev(pf);\n\n\tvf = ice_get_vf_by_id(pf, vf_id);\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\tret = ice_check_vf_ready_for_cfg(vf);\n\tif (ret)\n\t\tgoto out_put_vf;\n\n\tvf_vsi = ice_get_vf_vsi(vf);\n\tif (!vf_vsi) {\n\t\tnetdev_err(netdev, \"VSI %d for VF %d is null\\n\",\n\t\t\t   vf->lan_vsi_idx, vf->vf_id);\n\t\tret = -EINVAL;\n\t\tgoto out_put_vf;\n\t}\n\n\tif (vf_vsi->type != ICE_VSI_VF) {\n\t\tnetdev_err(netdev, \"Type %d of VSI %d for VF %d is no ICE_VSI_VF\\n\",\n\t\t\t   vf_vsi->type, vf_vsi->vsi_num, vf->vf_id);\n\t\tret = -ENODEV;\n\t\tgoto out_put_vf;\n\t}\n\n\tif (ena == vf->spoofchk) {\n\t\tdev_dbg(dev, \"VF spoofchk already %s\\n\", ena ? \"ON\" : \"OFF\");\n\t\tret = 0;\n\t\tgoto out_put_vf;\n\t}\n\n\tret = ice_vsi_apply_spoofchk(vf_vsi, ena);\n\tif (ret)\n\t\tdev_err(dev, \"Failed to set spoofchk %s for VF %d VSI %d\\n error %d\\n\",\n\t\t\tena ? \"ON\" : \"OFF\", vf->vf_id, vf_vsi->vsi_num, ret);\n\telse\n\t\tvf->spoofchk = ena;\n\nout_put_vf:\n\tice_put_vf(vf);\n\treturn ret;\n}\n\n \nint\nice_get_vf_cfg(struct net_device *netdev, int vf_id, struct ifla_vf_info *ivi)\n{\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\tstruct ice_vf *vf;\n\tint ret;\n\n\tvf = ice_get_vf_by_id(pf, vf_id);\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\tret = ice_check_vf_ready_for_cfg(vf);\n\tif (ret)\n\t\tgoto out_put_vf;\n\n\tivi->vf = vf_id;\n\tether_addr_copy(ivi->mac, vf->hw_lan_addr);\n\n\t \n\tivi->vlan = ice_vf_get_port_vlan_id(vf);\n\tivi->qos = ice_vf_get_port_vlan_prio(vf);\n\tif (ice_vf_is_port_vlan_ena(vf))\n\t\tivi->vlan_proto = cpu_to_be16(ice_vf_get_port_vlan_tpid(vf));\n\n\tivi->trusted = vf->trusted;\n\tivi->spoofchk = vf->spoofchk;\n\tif (!vf->link_forced)\n\t\tivi->linkstate = IFLA_VF_LINK_STATE_AUTO;\n\telse if (vf->link_up)\n\t\tivi->linkstate = IFLA_VF_LINK_STATE_ENABLE;\n\telse\n\t\tivi->linkstate = IFLA_VF_LINK_STATE_DISABLE;\n\tivi->max_tx_rate = vf->max_tx_rate;\n\tivi->min_tx_rate = vf->min_tx_rate;\n\nout_put_vf:\n\tice_put_vf(vf);\n\treturn ret;\n}\n\n \nint ice_set_vf_mac(struct net_device *netdev, int vf_id, u8 *mac)\n{\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\tstruct ice_vf *vf;\n\tint ret;\n\n\tif (is_multicast_ether_addr(mac)) {\n\t\tnetdev_err(netdev, \"%pM not a valid unicast address\\n\", mac);\n\t\treturn -EINVAL;\n\t}\n\n\tvf = ice_get_vf_by_id(pf, vf_id);\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\t \n\tif (ether_addr_equal(vf->dev_lan_addr, mac) &&\n\t    ether_addr_equal(vf->hw_lan_addr, mac)) {\n\t\tret = 0;\n\t\tgoto out_put_vf;\n\t}\n\n\tret = ice_check_vf_ready_for_cfg(vf);\n\tif (ret)\n\t\tgoto out_put_vf;\n\n\tmutex_lock(&vf->cfg_lock);\n\n\t \n\tether_addr_copy(vf->dev_lan_addr, mac);\n\tether_addr_copy(vf->hw_lan_addr, mac);\n\tif (is_zero_ether_addr(mac)) {\n\t\t \n\t\tvf->pf_set_mac = false;\n\t\tnetdev_info(netdev, \"Removing MAC on VF %d. VF driver will be reinitialized\\n\",\n\t\t\t    vf->vf_id);\n\t} else {\n\t\t \n\t\tvf->pf_set_mac = true;\n\t\tnetdev_info(netdev, \"Setting MAC %pM on VF %d. VF driver will be reinitialized\\n\",\n\t\t\t    mac, vf_id);\n\t}\n\n\tice_reset_vf(vf, ICE_VF_RESET_NOTIFY);\n\tmutex_unlock(&vf->cfg_lock);\n\nout_put_vf:\n\tice_put_vf(vf);\n\treturn ret;\n}\n\n \nint ice_set_vf_trust(struct net_device *netdev, int vf_id, bool trusted)\n{\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\tstruct ice_vf *vf;\n\tint ret;\n\n\tvf = ice_get_vf_by_id(pf, vf_id);\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\tif (ice_is_eswitch_mode_switchdev(pf)) {\n\t\tdev_info(ice_pf_to_dev(pf), \"Trusted VF is forbidden in switchdev mode\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tret = ice_check_vf_ready_for_cfg(vf);\n\tif (ret)\n\t\tgoto out_put_vf;\n\n\t \n\tif (trusted == vf->trusted) {\n\t\tret = 0;\n\t\tgoto out_put_vf;\n\t}\n\n\tmutex_lock(&vf->cfg_lock);\n\n\tvf->trusted = trusted;\n\tice_reset_vf(vf, ICE_VF_RESET_NOTIFY);\n\tdev_info(ice_pf_to_dev(pf), \"VF %u is now %strusted\\n\",\n\t\t vf_id, trusted ? \"\" : \"un\");\n\n\tmutex_unlock(&vf->cfg_lock);\n\nout_put_vf:\n\tice_put_vf(vf);\n\treturn ret;\n}\n\n \nint ice_set_vf_link_state(struct net_device *netdev, int vf_id, int link_state)\n{\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\tstruct ice_vf *vf;\n\tint ret;\n\n\tvf = ice_get_vf_by_id(pf, vf_id);\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\tret = ice_check_vf_ready_for_cfg(vf);\n\tif (ret)\n\t\tgoto out_put_vf;\n\n\tswitch (link_state) {\n\tcase IFLA_VF_LINK_STATE_AUTO:\n\t\tvf->link_forced = false;\n\t\tbreak;\n\tcase IFLA_VF_LINK_STATE_ENABLE:\n\t\tvf->link_forced = true;\n\t\tvf->link_up = true;\n\t\tbreak;\n\tcase IFLA_VF_LINK_STATE_DISABLE:\n\t\tvf->link_forced = true;\n\t\tvf->link_up = false;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto out_put_vf;\n\t}\n\n\tice_vc_notify_vf_link_state(vf);\n\nout_put_vf:\n\tice_put_vf(vf);\n\treturn ret;\n}\n\n \nstatic int ice_calc_all_vfs_min_tx_rate(struct ice_pf *pf)\n{\n\tstruct ice_vf *vf;\n\tunsigned int bkt;\n\tint rate = 0;\n\n\trcu_read_lock();\n\tice_for_each_vf_rcu(pf, bkt, vf)\n\t\trate += vf->min_tx_rate;\n\trcu_read_unlock();\n\n\treturn rate;\n}\n\n \nstatic bool\nice_min_tx_rate_oversubscribed(struct ice_vf *vf, int min_tx_rate)\n{\n\tstruct ice_vsi *vsi = ice_get_vf_vsi(vf);\n\tint all_vfs_min_tx_rate;\n\tint link_speed_mbps;\n\n\tif (WARN_ON(!vsi))\n\t\treturn false;\n\n\tlink_speed_mbps = ice_get_link_speed_mbps(vsi);\n\tall_vfs_min_tx_rate = ice_calc_all_vfs_min_tx_rate(vf->pf);\n\n\t \n\tall_vfs_min_tx_rate -= vf->min_tx_rate;\n\n\tif (all_vfs_min_tx_rate + min_tx_rate > link_speed_mbps) {\n\t\tdev_err(ice_pf_to_dev(vf->pf), \"min_tx_rate of %d Mbps on VF %u would cause oversubscription of %d Mbps based on the current link speed %d Mbps\\n\",\n\t\t\tmin_tx_rate, vf->vf_id,\n\t\t\tall_vfs_min_tx_rate + min_tx_rate - link_speed_mbps,\n\t\t\tlink_speed_mbps);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nint\nice_set_vf_bw(struct net_device *netdev, int vf_id, int min_tx_rate,\n\t      int max_tx_rate)\n{\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\tstruct ice_vsi *vsi;\n\tstruct device *dev;\n\tstruct ice_vf *vf;\n\tint ret;\n\n\tdev = ice_pf_to_dev(pf);\n\n\tvf = ice_get_vf_by_id(pf, vf_id);\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\tret = ice_check_vf_ready_for_cfg(vf);\n\tif (ret)\n\t\tgoto out_put_vf;\n\n\tvsi = ice_get_vf_vsi(vf);\n\tif (!vsi) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_vf;\n\t}\n\n\tif (min_tx_rate && ice_is_dcb_active(pf)) {\n\t\tdev_err(dev, \"DCB on PF is currently enabled. VF min Tx rate limiting not allowed on this PF.\\n\");\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out_put_vf;\n\t}\n\n\tif (ice_min_tx_rate_oversubscribed(vf, min_tx_rate)) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_vf;\n\t}\n\n\tif (vf->min_tx_rate != (unsigned int)min_tx_rate) {\n\t\tret = ice_set_min_bw_limit(vsi, (u64)min_tx_rate * 1000);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Unable to set min-tx-rate for VF %d\\n\",\n\t\t\t\tvf->vf_id);\n\t\t\tgoto out_put_vf;\n\t\t}\n\n\t\tvf->min_tx_rate = min_tx_rate;\n\t}\n\n\tif (vf->max_tx_rate != (unsigned int)max_tx_rate) {\n\t\tret = ice_set_max_bw_limit(vsi, (u64)max_tx_rate * 1000);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Unable to set max-tx-rate for VF %d\\n\",\n\t\t\t\tvf->vf_id);\n\t\t\tgoto out_put_vf;\n\t\t}\n\n\t\tvf->max_tx_rate = max_tx_rate;\n\t}\n\nout_put_vf:\n\tice_put_vf(vf);\n\treturn ret;\n}\n\n \nint ice_get_vf_stats(struct net_device *netdev, int vf_id,\n\t\t     struct ifla_vf_stats *vf_stats)\n{\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\tstruct ice_eth_stats *stats;\n\tstruct ice_vsi *vsi;\n\tstruct ice_vf *vf;\n\tint ret;\n\n\tvf = ice_get_vf_by_id(pf, vf_id);\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\tret = ice_check_vf_ready_for_cfg(vf);\n\tif (ret)\n\t\tgoto out_put_vf;\n\n\tvsi = ice_get_vf_vsi(vf);\n\tif (!vsi) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_vf;\n\t}\n\n\tice_update_eth_stats(vsi);\n\tstats = &vsi->eth_stats;\n\n\tmemset(vf_stats, 0, sizeof(*vf_stats));\n\n\tvf_stats->rx_packets = stats->rx_unicast + stats->rx_broadcast +\n\t\tstats->rx_multicast;\n\tvf_stats->tx_packets = stats->tx_unicast + stats->tx_broadcast +\n\t\tstats->tx_multicast;\n\tvf_stats->rx_bytes   = stats->rx_bytes;\n\tvf_stats->tx_bytes   = stats->tx_bytes;\n\tvf_stats->broadcast  = stats->rx_broadcast;\n\tvf_stats->multicast  = stats->rx_multicast;\n\tvf_stats->rx_dropped = stats->rx_discards;\n\tvf_stats->tx_dropped = stats->tx_discards;\n\nout_put_vf:\n\tice_put_vf(vf);\n\treturn ret;\n}\n\n \nstatic bool\nice_is_supported_port_vlan_proto(struct ice_hw *hw, u16 vlan_proto)\n{\n\tbool is_supported = false;\n\n\tswitch (vlan_proto) {\n\tcase ETH_P_8021Q:\n\t\tis_supported = true;\n\t\tbreak;\n\tcase ETH_P_8021AD:\n\t\tif (ice_is_dvm_ena(hw))\n\t\t\tis_supported = true;\n\t\tbreak;\n\t}\n\n\treturn is_supported;\n}\n\n \nint\nice_set_vf_port_vlan(struct net_device *netdev, int vf_id, u16 vlan_id, u8 qos,\n\t\t     __be16 vlan_proto)\n{\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\tu16 local_vlan_proto = ntohs(vlan_proto);\n\tstruct device *dev;\n\tstruct ice_vf *vf;\n\tint ret;\n\n\tdev = ice_pf_to_dev(pf);\n\n\tif (vlan_id >= VLAN_N_VID || qos > 7) {\n\t\tdev_err(dev, \"Invalid Port VLAN parameters for VF %d, ID %d, QoS %d\\n\",\n\t\t\tvf_id, vlan_id, qos);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ice_is_supported_port_vlan_proto(&pf->hw, local_vlan_proto)) {\n\t\tdev_err(dev, \"VF VLAN protocol 0x%04x is not supported\\n\",\n\t\t\tlocal_vlan_proto);\n\t\treturn -EPROTONOSUPPORT;\n\t}\n\n\tvf = ice_get_vf_by_id(pf, vf_id);\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\tret = ice_check_vf_ready_for_cfg(vf);\n\tif (ret)\n\t\tgoto out_put_vf;\n\n\tif (ice_vf_get_port_vlan_prio(vf) == qos &&\n\t    ice_vf_get_port_vlan_tpid(vf) == local_vlan_proto &&\n\t    ice_vf_get_port_vlan_id(vf) == vlan_id) {\n\t\t \n\t\tdev_dbg(dev, \"Duplicate port VLAN %u, QoS %u, TPID 0x%04x request\\n\",\n\t\t\tvlan_id, qos, local_vlan_proto);\n\t\tret = 0;\n\t\tgoto out_put_vf;\n\t}\n\n\tmutex_lock(&vf->cfg_lock);\n\n\tvf->port_vlan_info = ICE_VLAN(local_vlan_proto, vlan_id, qos);\n\tif (ice_vf_is_port_vlan_ena(vf))\n\t\tdev_info(dev, \"Setting VLAN %u, QoS %u, TPID 0x%04x on VF %d\\n\",\n\t\t\t vlan_id, qos, local_vlan_proto, vf_id);\n\telse\n\t\tdev_info(dev, \"Clearing port VLAN on VF %d\\n\", vf_id);\n\n\tice_reset_vf(vf, ICE_VF_RESET_NOTIFY);\n\tmutex_unlock(&vf->cfg_lock);\n\nout_put_vf:\n\tice_put_vf(vf);\n\treturn ret;\n}\n\n \nvoid ice_print_vf_rx_mdd_event(struct ice_vf *vf)\n{\n\tstruct ice_pf *pf = vf->pf;\n\tstruct device *dev;\n\n\tdev = ice_pf_to_dev(pf);\n\n\tdev_info(dev, \"%d Rx Malicious Driver Detection events detected on PF %d VF %d MAC %pM. mdd-auto-reset-vfs=%s\\n\",\n\t\t vf->mdd_rx_events.count, pf->hw.pf_id, vf->vf_id,\n\t\t vf->dev_lan_addr,\n\t\t test_bit(ICE_FLAG_MDD_AUTO_RESET_VF, pf->flags)\n\t\t\t  ? \"on\" : \"off\");\n}\n\n \nvoid ice_print_vfs_mdd_events(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct ice_vf *vf;\n\tunsigned int bkt;\n\n\t \n\tif (!test_and_clear_bit(ICE_MDD_VF_PRINT_PENDING, pf->state))\n\t\treturn;\n\n\t \n\tif (time_is_after_jiffies(pf->vfs.last_printed_mdd_jiffies + HZ * 1))\n\t\treturn;\n\n\tpf->vfs.last_printed_mdd_jiffies = jiffies;\n\n\tmutex_lock(&pf->vfs.table_lock);\n\tice_for_each_vf(pf, bkt, vf) {\n\t\t \n\t\tif (vf->mdd_rx_events.count != vf->mdd_rx_events.last_printed) {\n\t\t\tvf->mdd_rx_events.last_printed =\n\t\t\t\t\t\t\tvf->mdd_rx_events.count;\n\t\t\tice_print_vf_rx_mdd_event(vf);\n\t\t}\n\n\t\t \n\t\tif (vf->mdd_tx_events.count != vf->mdd_tx_events.last_printed) {\n\t\t\tvf->mdd_tx_events.last_printed =\n\t\t\t\t\t\t\tvf->mdd_tx_events.count;\n\n\t\t\tdev_info(dev, \"%d Tx Malicious Driver Detection events detected on PF %d VF %d MAC %pM.\\n\",\n\t\t\t\t vf->mdd_tx_events.count, hw->pf_id, vf->vf_id,\n\t\t\t\t vf->dev_lan_addr);\n\t\t}\n\t}\n\tmutex_unlock(&pf->vfs.table_lock);\n}\n\n \nvoid ice_restore_all_vfs_msi_state(struct pci_dev *pdev)\n{\n\tu16 vf_id;\n\tint pos;\n\n\tif (!pci_num_vf(pdev))\n\t\treturn;\n\n\tpos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_SRIOV);\n\tif (pos) {\n\t\tstruct pci_dev *vfdev;\n\n\t\tpci_read_config_word(pdev, pos + PCI_SRIOV_VF_DID,\n\t\t\t\t     &vf_id);\n\t\tvfdev = pci_get_device(pdev->vendor, vf_id, NULL);\n\t\twhile (vfdev) {\n\t\t\tif (vfdev->is_virtfn && vfdev->physfn == pdev)\n\t\t\t\tpci_restore_msi_state(vfdev);\n\t\t\tvfdev = pci_get_device(pdev->vendor, vf_id,\n\t\t\t\t\t       vfdev);\n\t\t}\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}