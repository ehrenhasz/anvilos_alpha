{
  "module_name": "ice_txrx.c",
  "hash_id": "ca5f7cc04101e53686188f2a3940e198b658cbcb72e6f0542d6bd139d277de5e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ice/ice_txrx.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/mm.h>\n#include <linux/netdevice.h>\n#include <linux/prefetch.h>\n#include <linux/bpf_trace.h>\n#include <net/dsfield.h>\n#include <net/mpls.h>\n#include <net/xdp.h>\n#include \"ice_txrx_lib.h\"\n#include \"ice_lib.h\"\n#include \"ice.h\"\n#include \"ice_trace.h\"\n#include \"ice_dcb_lib.h\"\n#include \"ice_xsk.h\"\n#include \"ice_eswitch.h\"\n\n#define ICE_RX_HDR_SIZE\t\t256\n\n#define FDIR_DESC_RXDID 0x40\n#define ICE_FDIR_CLEAN_DELAY 10\n\n \nint\nice_prgm_fdir_fltr(struct ice_vsi *vsi, struct ice_fltr_desc *fdir_desc,\n\t\t   u8 *raw_packet)\n{\n\tstruct ice_tx_buf *tx_buf, *first;\n\tstruct ice_fltr_desc *f_desc;\n\tstruct ice_tx_desc *tx_desc;\n\tstruct ice_tx_ring *tx_ring;\n\tstruct device *dev;\n\tdma_addr_t dma;\n\tu32 td_cmd;\n\tu16 i;\n\n\t \n\tif (!vsi)\n\t\treturn -ENOENT;\n\ttx_ring = vsi->tx_rings[0];\n\tif (!tx_ring || !tx_ring->desc)\n\t\treturn -ENOENT;\n\tdev = tx_ring->dev;\n\n\t \n\tfor (i = ICE_FDIR_CLEAN_DELAY; ICE_DESC_UNUSED(tx_ring) < 2; i--) {\n\t\tif (!i)\n\t\t\treturn -EAGAIN;\n\t\tmsleep_interruptible(1);\n\t}\n\n\tdma = dma_map_single(dev, raw_packet, ICE_FDIR_MAX_RAW_PKT_SIZE,\n\t\t\t     DMA_TO_DEVICE);\n\n\tif (dma_mapping_error(dev, dma))\n\t\treturn -EINVAL;\n\n\t \n\ti = tx_ring->next_to_use;\n\tfirst = &tx_ring->tx_buf[i];\n\tf_desc = ICE_TX_FDIRDESC(tx_ring, i);\n\tmemcpy(f_desc, fdir_desc, sizeof(*f_desc));\n\n\ti++;\n\ti = (i < tx_ring->count) ? i : 0;\n\ttx_desc = ICE_TX_DESC(tx_ring, i);\n\ttx_buf = &tx_ring->tx_buf[i];\n\n\ti++;\n\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\tmemset(tx_buf, 0, sizeof(*tx_buf));\n\tdma_unmap_len_set(tx_buf, len, ICE_FDIR_MAX_RAW_PKT_SIZE);\n\tdma_unmap_addr_set(tx_buf, dma, dma);\n\n\ttx_desc->buf_addr = cpu_to_le64(dma);\n\ttd_cmd = ICE_TXD_LAST_DESC_CMD | ICE_TX_DESC_CMD_DUMMY |\n\t\t ICE_TX_DESC_CMD_RE;\n\n\ttx_buf->type = ICE_TX_BUF_DUMMY;\n\ttx_buf->raw_buf = raw_packet;\n\n\ttx_desc->cmd_type_offset_bsz =\n\t\tice_build_ctob(td_cmd, 0, ICE_FDIR_MAX_RAW_PKT_SIZE, 0);\n\n\t \n\twmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\twritel(tx_ring->next_to_use, tx_ring->tail);\n\n\treturn 0;\n}\n\n \nstatic void\nice_unmap_and_free_tx_buf(struct ice_tx_ring *ring, struct ice_tx_buf *tx_buf)\n{\n\tif (dma_unmap_len(tx_buf, len))\n\t\tdma_unmap_page(ring->dev,\n\t\t\t       dma_unmap_addr(tx_buf, dma),\n\t\t\t       dma_unmap_len(tx_buf, len),\n\t\t\t       DMA_TO_DEVICE);\n\n\tswitch (tx_buf->type) {\n\tcase ICE_TX_BUF_DUMMY:\n\t\tdevm_kfree(ring->dev, tx_buf->raw_buf);\n\t\tbreak;\n\tcase ICE_TX_BUF_SKB:\n\t\tdev_kfree_skb_any(tx_buf->skb);\n\t\tbreak;\n\tcase ICE_TX_BUF_XDP_TX:\n\t\tpage_frag_free(tx_buf->raw_buf);\n\t\tbreak;\n\tcase ICE_TX_BUF_XDP_XMIT:\n\t\txdp_return_frame(tx_buf->xdpf);\n\t\tbreak;\n\t}\n\n\ttx_buf->next_to_watch = NULL;\n\ttx_buf->type = ICE_TX_BUF_EMPTY;\n\tdma_unmap_len_set(tx_buf, len, 0);\n\t \n}\n\nstatic struct netdev_queue *txring_txq(const struct ice_tx_ring *ring)\n{\n\treturn netdev_get_tx_queue(ring->netdev, ring->q_index);\n}\n\n \nvoid ice_clean_tx_ring(struct ice_tx_ring *tx_ring)\n{\n\tu32 size;\n\tu16 i;\n\n\tif (ice_ring_is_xdp(tx_ring) && tx_ring->xsk_pool) {\n\t\tice_xsk_clean_xdp_ring(tx_ring);\n\t\tgoto tx_skip_free;\n\t}\n\n\t \n\tif (!tx_ring->tx_buf)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < tx_ring->count; i++)\n\t\tice_unmap_and_free_tx_buf(tx_ring, &tx_ring->tx_buf[i]);\n\ntx_skip_free:\n\tmemset(tx_ring->tx_buf, 0, sizeof(*tx_ring->tx_buf) * tx_ring->count);\n\n\tsize = ALIGN(tx_ring->count * sizeof(struct ice_tx_desc),\n\t\t     PAGE_SIZE);\n\t \n\tmemset(tx_ring->desc, 0, size);\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\n\tif (!tx_ring->netdev)\n\t\treturn;\n\n\t \n\tnetdev_tx_reset_queue(txring_txq(tx_ring));\n}\n\n \nvoid ice_free_tx_ring(struct ice_tx_ring *tx_ring)\n{\n\tu32 size;\n\n\tice_clean_tx_ring(tx_ring);\n\tdevm_kfree(tx_ring->dev, tx_ring->tx_buf);\n\ttx_ring->tx_buf = NULL;\n\n\tif (tx_ring->desc) {\n\t\tsize = ALIGN(tx_ring->count * sizeof(struct ice_tx_desc),\n\t\t\t     PAGE_SIZE);\n\t\tdmam_free_coherent(tx_ring->dev, size,\n\t\t\t\t   tx_ring->desc, tx_ring->dma);\n\t\ttx_ring->desc = NULL;\n\t}\n}\n\n \nstatic bool ice_clean_tx_irq(struct ice_tx_ring *tx_ring, int napi_budget)\n{\n\tunsigned int total_bytes = 0, total_pkts = 0;\n\tunsigned int budget = ICE_DFLT_IRQ_WORK;\n\tstruct ice_vsi *vsi = tx_ring->vsi;\n\ts16 i = tx_ring->next_to_clean;\n\tstruct ice_tx_desc *tx_desc;\n\tstruct ice_tx_buf *tx_buf;\n\n\t \n\tnetdev_txq_bql_complete_prefetchw(txring_txq(tx_ring));\n\n\ttx_buf = &tx_ring->tx_buf[i];\n\ttx_desc = ICE_TX_DESC(tx_ring, i);\n\ti -= tx_ring->count;\n\n\tprefetch(&vsi->state);\n\n\tdo {\n\t\tstruct ice_tx_desc *eop_desc = tx_buf->next_to_watch;\n\n\t\t \n\t\tif (!eop_desc)\n\t\t\tbreak;\n\n\t\t \n\t\tprefetchw(&tx_buf->skb->users);\n\n\t\tsmp_rmb();\t \n\n\t\tice_trace(clean_tx_irq, tx_ring, tx_desc, tx_buf);\n\t\t \n\t\tif (!(eop_desc->cmd_type_offset_bsz &\n\t\t      cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE)))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buf->next_to_watch = NULL;\n\n\t\t \n\t\ttotal_bytes += tx_buf->bytecount;\n\t\ttotal_pkts += tx_buf->gso_segs;\n\n\t\t \n\t\tnapi_consume_skb(tx_buf->skb, napi_budget);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buf, dma),\n\t\t\t\t dma_unmap_len(tx_buf, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\ttx_buf->type = ICE_TX_BUF_EMPTY;\n\t\tdma_unmap_len_set(tx_buf, len, 0);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\tice_trace(clean_tx_irq_unmap, tx_ring, tx_desc, tx_buf);\n\t\t\ttx_buf++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(!i)) {\n\t\t\t\ti -= tx_ring->count;\n\t\t\t\ttx_buf = tx_ring->tx_buf;\n\t\t\t\ttx_desc = ICE_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buf, len)) {\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buf, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buf, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\t\tdma_unmap_len_set(tx_buf, len, 0);\n\t\t\t}\n\t\t}\n\t\tice_trace(clean_tx_irq_unmap_eop, tx_ring, tx_desc, tx_buf);\n\n\t\t \n\t\ttx_buf++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buf = tx_ring->tx_buf;\n\t\t\ttx_desc = ICE_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\tprefetch(tx_desc);\n\n\t\t \n\t\tbudget--;\n\t} while (likely(budget));\n\n\ti += tx_ring->count;\n\ttx_ring->next_to_clean = i;\n\n\tice_update_tx_ring_stats(tx_ring, total_pkts, total_bytes);\n\tnetdev_tx_completed_queue(txring_txq(tx_ring), total_pkts, total_bytes);\n\n#define TX_WAKE_THRESHOLD ((s16)(DESC_NEEDED * 2))\n\tif (unlikely(total_pkts && netif_carrier_ok(tx_ring->netdev) &&\n\t\t     (ICE_DESC_UNUSED(tx_ring) >= TX_WAKE_THRESHOLD))) {\n\t\t \n\t\tsmp_mb();\n\t\tif (netif_tx_queue_stopped(txring_txq(tx_ring)) &&\n\t\t    !test_bit(ICE_VSI_DOWN, vsi->state)) {\n\t\t\tnetif_tx_wake_queue(txring_txq(tx_ring));\n\t\t\t++tx_ring->ring_stats->tx_stats.restart_q;\n\t\t}\n\t}\n\n\treturn !!budget;\n}\n\n \nint ice_setup_tx_ring(struct ice_tx_ring *tx_ring)\n{\n\tstruct device *dev = tx_ring->dev;\n\tu32 size;\n\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\t \n\tWARN_ON(tx_ring->tx_buf);\n\ttx_ring->tx_buf =\n\t\tdevm_kcalloc(dev, sizeof(*tx_ring->tx_buf), tx_ring->count,\n\t\t\t     GFP_KERNEL);\n\tif (!tx_ring->tx_buf)\n\t\treturn -ENOMEM;\n\n\t \n\tsize = ALIGN(tx_ring->count * sizeof(struct ice_tx_desc),\n\t\t     PAGE_SIZE);\n\ttx_ring->desc = dmam_alloc_coherent(dev, size, &tx_ring->dma,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!tx_ring->desc) {\n\t\tdev_err(dev, \"Unable to allocate memory for the Tx descriptor ring, size=%d\\n\",\n\t\t\tsize);\n\t\tgoto err;\n\t}\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\ttx_ring->ring_stats->tx_stats.prev_pkt = -1;\n\treturn 0;\n\nerr:\n\tdevm_kfree(dev, tx_ring->tx_buf);\n\ttx_ring->tx_buf = NULL;\n\treturn -ENOMEM;\n}\n\n \nvoid ice_clean_rx_ring(struct ice_rx_ring *rx_ring)\n{\n\tstruct xdp_buff *xdp = &rx_ring->xdp;\n\tstruct device *dev = rx_ring->dev;\n\tu32 size;\n\tu16 i;\n\n\t \n\tif (!rx_ring->rx_buf)\n\t\treturn;\n\n\tif (rx_ring->xsk_pool) {\n\t\tice_xsk_clean_rx_ring(rx_ring);\n\t\tgoto rx_skip_free;\n\t}\n\n\tif (xdp->data) {\n\t\txdp_return_buff(xdp);\n\t\txdp->data = NULL;\n\t}\n\n\t \n\tfor (i = 0; i < rx_ring->count; i++) {\n\t\tstruct ice_rx_buf *rx_buf = &rx_ring->rx_buf[i];\n\n\t\tif (!rx_buf->page)\n\t\t\tcontinue;\n\n\t\t \n\t\tdma_sync_single_range_for_cpu(dev, rx_buf->dma,\n\t\t\t\t\t      rx_buf->page_offset,\n\t\t\t\t\t      rx_ring->rx_buf_len,\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\n\t\t \n\t\tdma_unmap_page_attrs(dev, rx_buf->dma, ice_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE, ICE_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(rx_buf->page, rx_buf->pagecnt_bias);\n\n\t\trx_buf->page = NULL;\n\t\trx_buf->page_offset = 0;\n\t}\n\nrx_skip_free:\n\tif (rx_ring->xsk_pool)\n\t\tmemset(rx_ring->xdp_buf, 0, array_size(rx_ring->count, sizeof(*rx_ring->xdp_buf)));\n\telse\n\t\tmemset(rx_ring->rx_buf, 0, array_size(rx_ring->count, sizeof(*rx_ring->rx_buf)));\n\n\t \n\tsize = ALIGN(rx_ring->count * sizeof(union ice_32byte_rx_desc),\n\t\t     PAGE_SIZE);\n\tmemset(rx_ring->desc, 0, size);\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->first_desc = 0;\n\trx_ring->next_to_use = 0;\n}\n\n \nvoid ice_free_rx_ring(struct ice_rx_ring *rx_ring)\n{\n\tu32 size;\n\n\tice_clean_rx_ring(rx_ring);\n\tif (rx_ring->vsi->type == ICE_VSI_PF)\n\t\tif (xdp_rxq_info_is_reg(&rx_ring->xdp_rxq))\n\t\t\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\trx_ring->xdp_prog = NULL;\n\tif (rx_ring->xsk_pool) {\n\t\tkfree(rx_ring->xdp_buf);\n\t\trx_ring->xdp_buf = NULL;\n\t} else {\n\t\tkfree(rx_ring->rx_buf);\n\t\trx_ring->rx_buf = NULL;\n\t}\n\n\tif (rx_ring->desc) {\n\t\tsize = ALIGN(rx_ring->count * sizeof(union ice_32byte_rx_desc),\n\t\t\t     PAGE_SIZE);\n\t\tdmam_free_coherent(rx_ring->dev, size,\n\t\t\t\t   rx_ring->desc, rx_ring->dma);\n\t\trx_ring->desc = NULL;\n\t}\n}\n\n \nint ice_setup_rx_ring(struct ice_rx_ring *rx_ring)\n{\n\tstruct device *dev = rx_ring->dev;\n\tu32 size;\n\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\t \n\tWARN_ON(rx_ring->rx_buf);\n\trx_ring->rx_buf =\n\t\tkcalloc(rx_ring->count, sizeof(*rx_ring->rx_buf), GFP_KERNEL);\n\tif (!rx_ring->rx_buf)\n\t\treturn -ENOMEM;\n\n\t \n\tsize = ALIGN(rx_ring->count * sizeof(union ice_32byte_rx_desc),\n\t\t     PAGE_SIZE);\n\trx_ring->desc = dmam_alloc_coherent(dev, size, &rx_ring->dma,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!rx_ring->desc) {\n\t\tdev_err(dev, \"Unable to allocate memory for the Rx descriptor ring, size=%d\\n\",\n\t\t\tsize);\n\t\tgoto err;\n\t}\n\n\trx_ring->next_to_use = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->first_desc = 0;\n\n\tif (ice_is_xdp_ena_vsi(rx_ring->vsi))\n\t\tWRITE_ONCE(rx_ring->xdp_prog, rx_ring->vsi->xdp_prog);\n\n\tif (rx_ring->vsi->type == ICE_VSI_PF &&\n\t    !xdp_rxq_info_is_reg(&rx_ring->xdp_rxq))\n\t\tif (xdp_rxq_info_reg(&rx_ring->xdp_rxq, rx_ring->netdev,\n\t\t\t\t     rx_ring->q_index, rx_ring->q_vector->napi.napi_id))\n\t\t\tgoto err;\n\treturn 0;\n\nerr:\n\tkfree(rx_ring->rx_buf);\n\trx_ring->rx_buf = NULL;\n\treturn -ENOMEM;\n}\n\n \nstatic unsigned int\nice_rx_frame_truesize(struct ice_rx_ring *rx_ring, const unsigned int size)\n{\n\tunsigned int truesize;\n\n#if (PAGE_SIZE < 8192)\n\ttruesize = ice_rx_pg_size(rx_ring) / 2;  \n#else\n\ttruesize = rx_ring->rx_offset ?\n\t\tSKB_DATA_ALIGN(rx_ring->rx_offset + size) +\n\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info)) :\n\t\tSKB_DATA_ALIGN(size);\n#endif\n\treturn truesize;\n}\n\n \nstatic void\nice_run_xdp(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp,\n\t    struct bpf_prog *xdp_prog, struct ice_tx_ring *xdp_ring,\n\t    struct ice_rx_buf *rx_buf)\n{\n\tunsigned int ret = ICE_XDP_PASS;\n\tu32 act;\n\n\tif (!xdp_prog)\n\t\tgoto exit;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\tif (static_branch_unlikely(&ice_xdp_locking_key))\n\t\t\tspin_lock(&xdp_ring->tx_lock);\n\t\tret = __ice_xmit_xdp_ring(xdp, xdp_ring, false);\n\t\tif (static_branch_unlikely(&ice_xdp_locking_key))\n\t\t\tspin_unlock(&xdp_ring->tx_lock);\n\t\tif (ret == ICE_XDP_CONSUMED)\n\t\t\tgoto out_failure;\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\tif (xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog))\n\t\t\tgoto out_failure;\n\t\tret = ICE_XDP_REDIR;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\nout_failure:\n\t\ttrace_xdp_exception(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tret = ICE_XDP_CONSUMED;\n\t}\nexit:\n\trx_buf->act = ret;\n\tif (unlikely(xdp_buff_has_frags(xdp)))\n\t\tice_set_rx_bufs_act(xdp, rx_ring, ret);\n}\n\n \nstatic int ice_xmit_xdp_ring(const struct xdp_frame *xdpf,\n\t\t\t     struct ice_tx_ring *xdp_ring)\n{\n\tstruct xdp_buff xdp;\n\n\txdp.data_hard_start = (void *)xdpf;\n\txdp.data = xdpf->data;\n\txdp.data_end = xdp.data + xdpf->len;\n\txdp.frame_sz = xdpf->frame_sz;\n\txdp.flags = xdpf->flags;\n\n\treturn __ice_xmit_xdp_ring(&xdp, xdp_ring, true);\n}\n\n \nint\nice_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,\n\t     u32 flags)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(dev);\n\tunsigned int queue_index = smp_processor_id();\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_tx_ring *xdp_ring;\n\tstruct ice_tx_buf *tx_buf;\n\tint nxmit = 0, i;\n\n\tif (test_bit(ICE_VSI_DOWN, vsi->state))\n\t\treturn -ENETDOWN;\n\n\tif (!ice_is_xdp_ena_vsi(vsi))\n\t\treturn -ENXIO;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\tif (static_branch_unlikely(&ice_xdp_locking_key)) {\n\t\tqueue_index %= vsi->num_xdp_txq;\n\t\txdp_ring = vsi->xdp_rings[queue_index];\n\t\tspin_lock(&xdp_ring->tx_lock);\n\t} else {\n\t\t \n\t\tif (unlikely(queue_index >= vsi->num_xdp_txq))\n\t\t\treturn -ENXIO;\n\t\txdp_ring = vsi->xdp_rings[queue_index];\n\t}\n\n\ttx_buf = &xdp_ring->tx_buf[xdp_ring->next_to_use];\n\tfor (i = 0; i < n; i++) {\n\t\tconst struct xdp_frame *xdpf = frames[i];\n\t\tint err;\n\n\t\terr = ice_xmit_xdp_ring(xdpf, xdp_ring);\n\t\tif (err != ICE_XDP_TX)\n\t\t\tbreak;\n\t\tnxmit++;\n\t}\n\n\ttx_buf->rs_idx = ice_set_rs_bit(xdp_ring);\n\tif (unlikely(flags & XDP_XMIT_FLUSH))\n\t\tice_xdp_ring_update_tail(xdp_ring);\n\n\tif (static_branch_unlikely(&ice_xdp_locking_key))\n\t\tspin_unlock(&xdp_ring->tx_lock);\n\n\treturn nxmit;\n}\n\n \nstatic bool\nice_alloc_mapped_page(struct ice_rx_ring *rx_ring, struct ice_rx_buf *bi)\n{\n\tstruct page *page = bi->page;\n\tdma_addr_t dma;\n\n\t \n\tif (likely(page))\n\t\treturn true;\n\n\t \n\tpage = dev_alloc_pages(ice_rx_pg_order(rx_ring));\n\tif (unlikely(!page)) {\n\t\trx_ring->ring_stats->rx_stats.alloc_page_failed++;\n\t\treturn false;\n\t}\n\n\t \n\tdma = dma_map_page_attrs(rx_ring->dev, page, 0, ice_rx_pg_size(rx_ring),\n\t\t\t\t DMA_FROM_DEVICE, ICE_RX_DMA_ATTR);\n\n\t \n\tif (dma_mapping_error(rx_ring->dev, dma)) {\n\t\t__free_pages(page, ice_rx_pg_order(rx_ring));\n\t\trx_ring->ring_stats->rx_stats.alloc_page_failed++;\n\t\treturn false;\n\t}\n\n\tbi->dma = dma;\n\tbi->page = page;\n\tbi->page_offset = rx_ring->rx_offset;\n\tpage_ref_add(page, USHRT_MAX - 1);\n\tbi->pagecnt_bias = USHRT_MAX;\n\n\treturn true;\n}\n\n \nbool ice_alloc_rx_bufs(struct ice_rx_ring *rx_ring, unsigned int cleaned_count)\n{\n\tunion ice_32b_rx_flex_desc *rx_desc;\n\tu16 ntu = rx_ring->next_to_use;\n\tstruct ice_rx_buf *bi;\n\n\t \n\tif ((!rx_ring->netdev && rx_ring->vsi->type != ICE_VSI_CTRL) ||\n\t    !cleaned_count)\n\t\treturn false;\n\n\t \n\trx_desc = ICE_RX_DESC(rx_ring, ntu);\n\tbi = &rx_ring->rx_buf[ntu];\n\n\tdo {\n\t\t \n\t\tif (!ice_alloc_mapped_page(rx_ring, bi))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_sync_single_range_for_device(rx_ring->dev, bi->dma,\n\t\t\t\t\t\t bi->page_offset,\n\t\t\t\t\t\t rx_ring->rx_buf_len,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t \n\t\trx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);\n\n\t\trx_desc++;\n\t\tbi++;\n\t\tntu++;\n\t\tif (unlikely(ntu == rx_ring->count)) {\n\t\t\trx_desc = ICE_RX_DESC(rx_ring, 0);\n\t\t\tbi = rx_ring->rx_buf;\n\t\t\tntu = 0;\n\t\t}\n\n\t\t \n\t\trx_desc->wb.status_error0 = 0;\n\n\t\tcleaned_count--;\n\t} while (cleaned_count);\n\n\tif (rx_ring->next_to_use != ntu)\n\t\tice_release_rx_desc(rx_ring, ntu);\n\n\treturn !!cleaned_count;\n}\n\n \nstatic void\nice_rx_buf_adjust_pg_offset(struct ice_rx_buf *rx_buf, unsigned int size)\n{\n#if (PAGE_SIZE < 8192)\n\t \n\trx_buf->page_offset ^= size;\n#else\n\t \n\trx_buf->page_offset += size;\n#endif\n}\n\n \nstatic bool\nice_can_reuse_rx_page(struct ice_rx_buf *rx_buf)\n{\n\tunsigned int pagecnt_bias = rx_buf->pagecnt_bias;\n\tstruct page *page = rx_buf->page;\n\n\t \n\tif (!dev_page_is_reusable(page))\n\t\treturn false;\n\n#if (PAGE_SIZE < 8192)\n\t \n\tif (unlikely(rx_buf->pgcnt - pagecnt_bias > 1))\n\t\treturn false;\n#else\n#define ICE_LAST_OFFSET \\\n\t(SKB_WITH_OVERHEAD(PAGE_SIZE) - ICE_RXBUF_2048)\n\tif (rx_buf->page_offset > ICE_LAST_OFFSET)\n\t\treturn false;\n#endif  \n\n\t \n\tif (unlikely(pagecnt_bias == 1)) {\n\t\tpage_ref_add(page, USHRT_MAX - 1);\n\t\trx_buf->pagecnt_bias = USHRT_MAX;\n\t}\n\n\treturn true;\n}\n\n \nstatic int\nice_add_xdp_frag(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp,\n\t\t struct ice_rx_buf *rx_buf, const unsigned int size)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\n\tif (!size)\n\t\treturn 0;\n\n\tif (!xdp_buff_has_frags(xdp)) {\n\t\tsinfo->nr_frags = 0;\n\t\tsinfo->xdp_frags_size = 0;\n\t\txdp_buff_set_frags_flag(xdp);\n\t}\n\n\tif (unlikely(sinfo->nr_frags == MAX_SKB_FRAGS)) {\n\t\tif (unlikely(xdp_buff_has_frags(xdp)))\n\t\t\tice_set_rx_bufs_act(xdp, rx_ring, ICE_XDP_CONSUMED);\n\t\treturn -ENOMEM;\n\t}\n\n\t__skb_fill_page_desc_noacc(sinfo, sinfo->nr_frags++, rx_buf->page,\n\t\t\t\t   rx_buf->page_offset, size);\n\tsinfo->xdp_frags_size += size;\n\n\tif (page_is_pfmemalloc(rx_buf->page))\n\t\txdp_buff_set_frag_pfmemalloc(xdp);\n\n\treturn 0;\n}\n\n \nstatic void\nice_reuse_rx_page(struct ice_rx_ring *rx_ring, struct ice_rx_buf *old_buf)\n{\n\tu16 nta = rx_ring->next_to_alloc;\n\tstruct ice_rx_buf *new_buf;\n\n\tnew_buf = &rx_ring->rx_buf[nta];\n\n\t \n\tnta++;\n\trx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;\n\n\t \n\tnew_buf->dma = old_buf->dma;\n\tnew_buf->page = old_buf->page;\n\tnew_buf->page_offset = old_buf->page_offset;\n\tnew_buf->pagecnt_bias = old_buf->pagecnt_bias;\n}\n\n \nstatic struct ice_rx_buf *\nice_get_rx_buf(struct ice_rx_ring *rx_ring, const unsigned int size,\n\t       const unsigned int ntc)\n{\n\tstruct ice_rx_buf *rx_buf;\n\n\trx_buf = &rx_ring->rx_buf[ntc];\n\trx_buf->pgcnt =\n#if (PAGE_SIZE < 8192)\n\t\tpage_count(rx_buf->page);\n#else\n\t\t0;\n#endif\n\tprefetchw(rx_buf->page);\n\n\tif (!size)\n\t\treturn rx_buf;\n\t \n\tdma_sync_single_range_for_cpu(rx_ring->dev, rx_buf->dma,\n\t\t\t\t      rx_buf->page_offset, size,\n\t\t\t\t      DMA_FROM_DEVICE);\n\n\t \n\trx_buf->pagecnt_bias--;\n\n\treturn rx_buf;\n}\n\n \nstatic struct sk_buff *\nice_build_skb(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp)\n{\n\tu8 metasize = xdp->data - xdp->data_meta;\n\tstruct skb_shared_info *sinfo = NULL;\n\tunsigned int nr_frags;\n\tstruct sk_buff *skb;\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tnr_frags = sinfo->nr_frags;\n\t}\n\n\t \n\tnet_prefetch(xdp->data_meta);\n\t \n\tskb = napi_build_skb(xdp->data_hard_start, xdp->frame_sz);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_record_rx_queue(skb, rx_ring->q_index);\n\n\t \n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\t__skb_put(skb, xdp->data_end - xdp->data);\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\tif (unlikely(xdp_buff_has_frags(xdp)))\n\t\txdp_update_skb_shared_info(skb, nr_frags,\n\t\t\t\t\t   sinfo->xdp_frags_size,\n\t\t\t\t\t   nr_frags * xdp->frame_sz,\n\t\t\t\t\t   xdp_buff_is_frag_pfmemalloc(xdp));\n\n\treturn skb;\n}\n\n \nstatic struct sk_buff *\nice_construct_skb(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp)\n{\n\tunsigned int size = xdp->data_end - xdp->data;\n\tstruct skb_shared_info *sinfo = NULL;\n\tstruct ice_rx_buf *rx_buf;\n\tunsigned int nr_frags = 0;\n\tunsigned int headlen;\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data);\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tnr_frags = sinfo->nr_frags;\n\t}\n\n\t \n\tskb = __napi_alloc_skb(&rx_ring->q_vector->napi, ICE_RX_HDR_SIZE,\n\t\t\t       GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\trx_buf = &rx_ring->rx_buf[rx_ring->first_desc];\n\tskb_record_rx_queue(skb, rx_ring->q_index);\n\t \n\theadlen = size;\n\tif (headlen > ICE_RX_HDR_SIZE)\n\t\theadlen = eth_get_headlen(skb->dev, xdp->data, ICE_RX_HDR_SIZE);\n\n\t \n\tmemcpy(__skb_put(skb, headlen), xdp->data, ALIGN(headlen,\n\t\t\t\t\t\t\t sizeof(long)));\n\n\t \n\tsize -= headlen;\n\tif (size) {\n\t\t \n\t\tif (unlikely(nr_frags >= MAX_SKB_FRAGS - 1)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NULL;\n\t\t}\n\t\tskb_add_rx_frag(skb, 0, rx_buf->page,\n\t\t\t\trx_buf->page_offset + headlen, size,\n\t\t\t\txdp->frame_sz);\n\t} else {\n\t\t \n\t\trx_buf->act = ICE_SKB_CONSUMED;\n\t}\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\tstruct skb_shared_info *skinfo = skb_shinfo(skb);\n\n\t\tmemcpy(&skinfo->frags[skinfo->nr_frags], &sinfo->frags[0],\n\t\t       sizeof(skb_frag_t) * nr_frags);\n\n\t\txdp_update_skb_shared_info(skb, skinfo->nr_frags + nr_frags,\n\t\t\t\t\t   sinfo->xdp_frags_size,\n\t\t\t\t\t   nr_frags * xdp->frame_sz,\n\t\t\t\t\t   xdp_buff_is_frag_pfmemalloc(xdp));\n\t}\n\n\treturn skb;\n}\n\n \nstatic void\nice_put_rx_buf(struct ice_rx_ring *rx_ring, struct ice_rx_buf *rx_buf)\n{\n\tif (!rx_buf)\n\t\treturn;\n\n\tif (ice_can_reuse_rx_page(rx_buf)) {\n\t\t \n\t\tice_reuse_rx_page(rx_ring, rx_buf);\n\t} else {\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev, rx_buf->dma,\n\t\t\t\t     ice_rx_pg_size(rx_ring), DMA_FROM_DEVICE,\n\t\t\t\t     ICE_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(rx_buf->page, rx_buf->pagecnt_bias);\n\t}\n\n\t \n\trx_buf->page = NULL;\n}\n\n \nint ice_clean_rx_irq(struct ice_rx_ring *rx_ring, int budget)\n{\n\tunsigned int total_rx_bytes = 0, total_rx_pkts = 0;\n\tunsigned int offset = rx_ring->rx_offset;\n\tstruct xdp_buff *xdp = &rx_ring->xdp;\n\tu32 cached_ntc = rx_ring->first_desc;\n\tstruct ice_tx_ring *xdp_ring = NULL;\n\tstruct bpf_prog *xdp_prog = NULL;\n\tu32 ntc = rx_ring->next_to_clean;\n\tu32 cnt = rx_ring->count;\n\tu32 xdp_xmit = 0;\n\tu32 cached_ntu;\n\tbool failure;\n\tu32 first;\n\n\t \n#if (PAGE_SIZE < 8192)\n\txdp->frame_sz = ice_rx_frame_truesize(rx_ring, 0);\n#endif\n\n\txdp_prog = READ_ONCE(rx_ring->xdp_prog);\n\tif (xdp_prog) {\n\t\txdp_ring = rx_ring->xdp_ring;\n\t\tcached_ntu = xdp_ring->next_to_use;\n\t}\n\n\t \n\twhile (likely(total_rx_pkts < (unsigned int)budget)) {\n\t\tunion ice_32b_rx_flex_desc *rx_desc;\n\t\tstruct ice_rx_buf *rx_buf;\n\t\tstruct sk_buff *skb;\n\t\tunsigned int size;\n\t\tu16 stat_err_bits;\n\t\tu16 vlan_tag = 0;\n\t\tu16 rx_ptype;\n\n\t\t \n\t\trx_desc = ICE_RX_DESC(rx_ring, ntc);\n\n\t\t \n\t\tstat_err_bits = BIT(ICE_RX_FLEX_DESC_STATUS0_DD_S);\n\t\tif (!ice_test_staterr(rx_desc->wb.status_error0, stat_err_bits))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tice_trace(clean_rx_irq, rx_ring, rx_desc);\n\t\tif (rx_desc->wb.rxdid == FDIR_DESC_RXDID || !rx_ring->netdev) {\n\t\t\tstruct ice_vsi *ctrl_vsi = rx_ring->vsi;\n\n\t\t\tif (rx_desc->wb.rxdid == FDIR_DESC_RXDID &&\n\t\t\t    ctrl_vsi->vf)\n\t\t\t\tice_vc_fdir_irq_handler(ctrl_vsi, rx_desc);\n\t\t\tif (++ntc == cnt)\n\t\t\t\tntc = 0;\n\t\t\trx_ring->first_desc = ntc;\n\t\t\tcontinue;\n\t\t}\n\n\t\tsize = le16_to_cpu(rx_desc->wb.pkt_len) &\n\t\t\tICE_RX_FLX_DESC_PKT_LEN_M;\n\n\t\t \n\t\trx_buf = ice_get_rx_buf(rx_ring, size, ntc);\n\n\t\tif (!xdp->data) {\n\t\t\tvoid *hard_start;\n\n\t\t\thard_start = page_address(rx_buf->page) + rx_buf->page_offset -\n\t\t\t\t     offset;\n\t\t\txdp_prepare_buff(xdp, hard_start, offset, size, !!offset);\n#if (PAGE_SIZE > 4096)\n\t\t\t \n\t\t\txdp->frame_sz = ice_rx_frame_truesize(rx_ring, size);\n#endif\n\t\t\txdp_buff_clear_frags_flag(xdp);\n\t\t} else if (ice_add_xdp_frag(rx_ring, xdp, rx_buf, size)) {\n\t\t\tbreak;\n\t\t}\n\t\tif (++ntc == cnt)\n\t\t\tntc = 0;\n\n\t\t \n\t\tif (ice_is_non_eop(rx_ring, rx_desc))\n\t\t\tcontinue;\n\n\t\tice_run_xdp(rx_ring, xdp, xdp_prog, xdp_ring, rx_buf);\n\t\tif (rx_buf->act == ICE_XDP_PASS)\n\t\t\tgoto construct_skb;\n\t\ttotal_rx_bytes += xdp_get_buff_len(xdp);\n\t\ttotal_rx_pkts++;\n\n\t\txdp->data = NULL;\n\t\trx_ring->first_desc = ntc;\n\t\tcontinue;\nconstruct_skb:\n\t\tif (likely(ice_ring_uses_build_skb(rx_ring)))\n\t\t\tskb = ice_build_skb(rx_ring, xdp);\n\t\telse\n\t\t\tskb = ice_construct_skb(rx_ring, xdp);\n\t\t \n\t\tif (!skb) {\n\t\t\trx_ring->ring_stats->rx_stats.alloc_page_failed++;\n\t\t\trx_buf->act = ICE_XDP_CONSUMED;\n\t\t\tif (unlikely(xdp_buff_has_frags(xdp)))\n\t\t\t\tice_set_rx_bufs_act(xdp, rx_ring,\n\t\t\t\t\t\t    ICE_XDP_CONSUMED);\n\t\t\txdp->data = NULL;\n\t\t\trx_ring->first_desc = ntc;\n\t\t\tbreak;\n\t\t}\n\t\txdp->data = NULL;\n\t\trx_ring->first_desc = ntc;\n\n\t\tstat_err_bits = BIT(ICE_RX_FLEX_DESC_STATUS0_RXE_S);\n\t\tif (unlikely(ice_test_staterr(rx_desc->wb.status_error0,\n\t\t\t\t\t      stat_err_bits))) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tvlan_tag = ice_get_vlan_tag_from_rx_desc(rx_desc);\n\n\t\t \n\t\tif (eth_skb_pad(skb))\n\t\t\tcontinue;\n\n\t\t \n\t\ttotal_rx_bytes += skb->len;\n\n\t\t \n\t\trx_ptype = le16_to_cpu(rx_desc->wb.ptype_flex_flags0) &\n\t\t\tICE_RX_FLEX_DESC_PTYPE_M;\n\n\t\tice_process_skb_fields(rx_ring, rx_desc, skb, rx_ptype);\n\n\t\tice_trace(clean_rx_irq_indicate, rx_ring, rx_desc, skb);\n\t\t \n\t\tice_receive_skb(rx_ring, skb, vlan_tag);\n\n\t\t \n\t\ttotal_rx_pkts++;\n\t}\n\n\tfirst = rx_ring->first_desc;\n\twhile (cached_ntc != first) {\n\t\tstruct ice_rx_buf *buf = &rx_ring->rx_buf[cached_ntc];\n\n\t\tif (buf->act & (ICE_XDP_TX | ICE_XDP_REDIR)) {\n\t\t\tice_rx_buf_adjust_pg_offset(buf, xdp->frame_sz);\n\t\t\txdp_xmit |= buf->act;\n\t\t} else if (buf->act & ICE_XDP_CONSUMED) {\n\t\t\tbuf->pagecnt_bias++;\n\t\t} else if (buf->act == ICE_XDP_PASS) {\n\t\t\tice_rx_buf_adjust_pg_offset(buf, xdp->frame_sz);\n\t\t}\n\n\t\tice_put_rx_buf(rx_ring, buf);\n\t\tif (++cached_ntc >= cnt)\n\t\t\tcached_ntc = 0;\n\t}\n\trx_ring->next_to_clean = ntc;\n\t \n\tfailure = ice_alloc_rx_bufs(rx_ring, ICE_RX_DESC_UNUSED(rx_ring));\n\n\tif (xdp_xmit)\n\t\tice_finalize_xdp_rx(xdp_ring, xdp_xmit, cached_ntu);\n\n\tif (rx_ring->ring_stats)\n\t\tice_update_rx_ring_stats(rx_ring, total_rx_pkts,\n\t\t\t\t\t total_rx_bytes);\n\n\t \n\treturn failure ? budget : (int)total_rx_pkts;\n}\n\nstatic void __ice_update_sample(struct ice_q_vector *q_vector,\n\t\t\t\tstruct ice_ring_container *rc,\n\t\t\t\tstruct dim_sample *sample,\n\t\t\t\tbool is_tx)\n{\n\tu64 packets = 0, bytes = 0;\n\n\tif (is_tx) {\n\t\tstruct ice_tx_ring *tx_ring;\n\n\t\tice_for_each_tx_ring(tx_ring, *rc) {\n\t\t\tstruct ice_ring_stats *ring_stats;\n\n\t\t\tring_stats = tx_ring->ring_stats;\n\t\t\tif (!ring_stats)\n\t\t\t\tcontinue;\n\t\t\tpackets += ring_stats->stats.pkts;\n\t\t\tbytes += ring_stats->stats.bytes;\n\t\t}\n\t} else {\n\t\tstruct ice_rx_ring *rx_ring;\n\n\t\tice_for_each_rx_ring(rx_ring, *rc) {\n\t\t\tstruct ice_ring_stats *ring_stats;\n\n\t\t\tring_stats = rx_ring->ring_stats;\n\t\t\tif (!ring_stats)\n\t\t\t\tcontinue;\n\t\t\tpackets += ring_stats->stats.pkts;\n\t\t\tbytes += ring_stats->stats.bytes;\n\t\t}\n\t}\n\n\tdim_update_sample(q_vector->total_events, packets, bytes, sample);\n\tsample->comp_ctr = 0;\n\n\t \n\tif (ktime_ms_delta(sample->time, rc->dim.start_sample.time) >= 1000)\n\t\trc->dim.state = DIM_START_MEASURE;\n}\n\n \nstatic void ice_net_dim(struct ice_q_vector *q_vector)\n{\n\tstruct ice_ring_container *tx = &q_vector->tx;\n\tstruct ice_ring_container *rx = &q_vector->rx;\n\n\tif (ITR_IS_DYNAMIC(tx)) {\n\t\tstruct dim_sample dim_sample;\n\n\t\t__ice_update_sample(q_vector, tx, &dim_sample, true);\n\t\tnet_dim(&tx->dim, dim_sample);\n\t}\n\n\tif (ITR_IS_DYNAMIC(rx)) {\n\t\tstruct dim_sample dim_sample;\n\n\t\t__ice_update_sample(q_vector, rx, &dim_sample, false);\n\t\tnet_dim(&rx->dim, dim_sample);\n\t}\n}\n\n \nstatic u32 ice_buildreg_itr(u16 itr_idx, u16 itr)\n{\n\t \n\titr &= ICE_ITR_MASK;\n\n\treturn GLINT_DYN_CTL_INTENA_M | GLINT_DYN_CTL_CLEARPBA_M |\n\t\t(itr_idx << GLINT_DYN_CTL_ITR_INDX_S) |\n\t\t(itr << (GLINT_DYN_CTL_INTERVAL_S - ICE_ITR_GRAN_S));\n}\n\n \nstatic void ice_enable_interrupt(struct ice_q_vector *q_vector)\n{\n\tstruct ice_vsi *vsi = q_vector->vsi;\n\tbool wb_en = q_vector->wb_on_itr;\n\tu32 itr_val;\n\n\tif (test_bit(ICE_DOWN, vsi->state))\n\t\treturn;\n\n\t \n\tif (!wb_en) {\n\t\titr_val = ice_buildreg_itr(ICE_ITR_NONE, 0);\n\t} else {\n\t\tq_vector->wb_on_itr = false;\n\n\t\t \n\t\titr_val = ice_buildreg_itr(ICE_IDX_ITR2, ICE_ITR_20K);\n\t\titr_val |= GLINT_DYN_CTL_SWINT_TRIG_M |\n\t\t\t   ICE_IDX_ITR2 << GLINT_DYN_CTL_SW_ITR_INDX_S |\n\t\t\t   GLINT_DYN_CTL_SW_ITR_INDX_ENA_M;\n\t}\n\twr32(&vsi->back->hw, GLINT_DYN_CTL(q_vector->reg_idx), itr_val);\n}\n\n \nstatic void ice_set_wb_on_itr(struct ice_q_vector *q_vector)\n{\n\tstruct ice_vsi *vsi = q_vector->vsi;\n\n\t \n\tif (q_vector->wb_on_itr)\n\t\treturn;\n\n\t \n\twr32(&vsi->back->hw, GLINT_DYN_CTL(q_vector->reg_idx),\n\t     ((ICE_ITR_NONE << GLINT_DYN_CTL_ITR_INDX_S) &\n\t      GLINT_DYN_CTL_ITR_INDX_M) | GLINT_DYN_CTL_INTENA_MSK_M |\n\t     GLINT_DYN_CTL_WB_ON_ITR_M);\n\n\tq_vector->wb_on_itr = true;\n}\n\n \nint ice_napi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct ice_q_vector *q_vector =\n\t\t\t\tcontainer_of(napi, struct ice_q_vector, napi);\n\tstruct ice_tx_ring *tx_ring;\n\tstruct ice_rx_ring *rx_ring;\n\tbool clean_complete = true;\n\tint budget_per_ring;\n\tint work_done = 0;\n\n\t \n\tice_for_each_tx_ring(tx_ring, q_vector->tx) {\n\t\tbool wd;\n\n\t\tif (tx_ring->xsk_pool)\n\t\t\twd = ice_xmit_zc(tx_ring);\n\t\telse if (ice_ring_is_xdp(tx_ring))\n\t\t\twd = true;\n\t\telse\n\t\t\twd = ice_clean_tx_irq(tx_ring, budget);\n\n\t\tif (!wd)\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (unlikely(budget <= 0))\n\t\treturn budget;\n\n\t \n\tif (unlikely(q_vector->num_ring_rx > 1))\n\t\t \n\t\tbudget_per_ring = max_t(int, budget / q_vector->num_ring_rx, 1);\n\telse\n\t\t \n\t\tbudget_per_ring = budget;\n\n\tice_for_each_rx_ring(rx_ring, q_vector->rx) {\n\t\tint cleaned;\n\n\t\t \n\t\tcleaned = rx_ring->xsk_pool ?\n\t\t\t  ice_clean_rx_irq_zc(rx_ring, budget_per_ring) :\n\t\t\t  ice_clean_rx_irq(rx_ring, budget_per_ring);\n\t\twork_done += cleaned;\n\t\t \n\t\tif (cleaned >= budget_per_ring)\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (!clean_complete) {\n\t\t \n\t\tice_set_wb_on_itr(q_vector);\n\t\treturn budget;\n\t}\n\n\t \n\tif (napi_complete_done(napi, work_done)) {\n\t\tice_net_dim(q_vector);\n\t\tice_enable_interrupt(q_vector);\n\t} else {\n\t\tice_set_wb_on_itr(q_vector);\n\t}\n\n\treturn min_t(int, work_done, budget - 1);\n}\n\n \nstatic int __ice_maybe_stop_tx(struct ice_tx_ring *tx_ring, unsigned int size)\n{\n\tnetif_tx_stop_queue(txring_txq(tx_ring));\n\t \n\tsmp_mb();\n\n\t \n\tif (likely(ICE_DESC_UNUSED(tx_ring) < size))\n\t\treturn -EBUSY;\n\n\t \n\tnetif_tx_start_queue(txring_txq(tx_ring));\n\t++tx_ring->ring_stats->tx_stats.restart_q;\n\treturn 0;\n}\n\n \nstatic int ice_maybe_stop_tx(struct ice_tx_ring *tx_ring, unsigned int size)\n{\n\tif (likely(ICE_DESC_UNUSED(tx_ring) >= size))\n\t\treturn 0;\n\n\treturn __ice_maybe_stop_tx(tx_ring, size);\n}\n\n \nstatic void\nice_tx_map(struct ice_tx_ring *tx_ring, struct ice_tx_buf *first,\n\t   struct ice_tx_offload_params *off)\n{\n\tu64 td_offset, td_tag, td_cmd;\n\tu16 i = tx_ring->next_to_use;\n\tunsigned int data_len, size;\n\tstruct ice_tx_desc *tx_desc;\n\tstruct ice_tx_buf *tx_buf;\n\tstruct sk_buff *skb;\n\tskb_frag_t *frag;\n\tdma_addr_t dma;\n\tbool kick;\n\n\ttd_tag = off->td_l2tag1;\n\ttd_cmd = off->td_cmd;\n\ttd_offset = off->td_offset;\n\tskb = first->skb;\n\n\tdata_len = skb->data_len;\n\tsize = skb_headlen(skb);\n\n\ttx_desc = ICE_TX_DESC(tx_ring, i);\n\n\tif (first->tx_flags & ICE_TX_FLAGS_HW_VLAN) {\n\t\ttd_cmd |= (u64)ICE_TX_DESC_CMD_IL2TAG1;\n\t\ttd_tag = first->vid;\n\t}\n\n\tdma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);\n\n\ttx_buf = first;\n\n\tfor (frag = &skb_shinfo(skb)->frags[0];; frag++) {\n\t\tunsigned int max_data = ICE_MAX_DATA_PER_TXD_ALIGNED;\n\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto dma_error;\n\n\t\t \n\t\tdma_unmap_len_set(tx_buf, len, size);\n\t\tdma_unmap_addr_set(tx_buf, dma, dma);\n\n\t\t \n\t\tmax_data += -dma & (ICE_MAX_READ_REQ_SIZE - 1);\n\t\ttx_desc->buf_addr = cpu_to_le64(dma);\n\n\t\t \n\t\twhile (unlikely(size > ICE_MAX_DATA_PER_TXD)) {\n\t\t\ttx_desc->cmd_type_offset_bsz =\n\t\t\t\tice_build_ctob(td_cmd, td_offset, max_data,\n\t\t\t\t\t       td_tag);\n\n\t\t\ttx_desc++;\n\t\t\ti++;\n\n\t\t\tif (i == tx_ring->count) {\n\t\t\t\ttx_desc = ICE_TX_DESC(tx_ring, 0);\n\t\t\t\ti = 0;\n\t\t\t}\n\n\t\t\tdma += max_data;\n\t\t\tsize -= max_data;\n\n\t\t\tmax_data = ICE_MAX_DATA_PER_TXD_ALIGNED;\n\t\t\ttx_desc->buf_addr = cpu_to_le64(dma);\n\t\t}\n\n\t\tif (likely(!data_len))\n\t\t\tbreak;\n\n\t\ttx_desc->cmd_type_offset_bsz = ice_build_ctob(td_cmd, td_offset,\n\t\t\t\t\t\t\t      size, td_tag);\n\n\t\ttx_desc++;\n\t\ti++;\n\n\t\tif (i == tx_ring->count) {\n\t\t\ttx_desc = ICE_TX_DESC(tx_ring, 0);\n\t\t\ti = 0;\n\t\t}\n\n\t\tsize = skb_frag_size(frag);\n\t\tdata_len -= size;\n\n\t\tdma = skb_frag_dma_map(tx_ring->dev, frag, 0, size,\n\t\t\t\t       DMA_TO_DEVICE);\n\n\t\ttx_buf = &tx_ring->tx_buf[i];\n\t\ttx_buf->type = ICE_TX_BUF_FRAG;\n\t}\n\n\t \n\tskb_tx_timestamp(first->skb);\n\n\ti++;\n\tif (i == tx_ring->count)\n\t\ti = 0;\n\n\t \n\ttd_cmd |= (u64)ICE_TXD_LAST_DESC_CMD;\n\ttx_desc->cmd_type_offset_bsz =\n\t\t\tice_build_ctob(td_cmd, td_offset, size, td_tag);\n\n\t \n\twmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\ttx_ring->next_to_use = i;\n\n\tice_maybe_stop_tx(tx_ring, DESC_NEEDED);\n\n\t \n\tkick = __netdev_tx_sent_queue(txring_txq(tx_ring), first->bytecount,\n\t\t\t\t      netdev_xmit_more());\n\tif (kick)\n\t\t \n\t\twritel(i, tx_ring->tail);\n\n\treturn;\n\ndma_error:\n\t \n\tfor (;;) {\n\t\ttx_buf = &tx_ring->tx_buf[i];\n\t\tice_unmap_and_free_tx_buf(tx_ring, tx_buf);\n\t\tif (tx_buf == first)\n\t\t\tbreak;\n\t\tif (i == 0)\n\t\t\ti = tx_ring->count;\n\t\ti--;\n\t}\n\n\ttx_ring->next_to_use = i;\n}\n\n \nstatic\nint ice_tx_csum(struct ice_tx_buf *first, struct ice_tx_offload_params *off)\n{\n\tu32 l4_len = 0, l3_len = 0, l2_len = 0;\n\tstruct sk_buff *skb = first->skb;\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tunsigned char *hdr;\n\t} l4;\n\t__be16 frag_off, protocol;\n\tunsigned char *exthdr;\n\tu32 offset, cmd = 0;\n\tu8 l4_proto = 0;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tprotocol = vlan_get_protocol(skb);\n\n\tif (eth_p_mpls(protocol)) {\n\t\tip.hdr = skb_inner_network_header(skb);\n\t\tl4.hdr = skb_checksum_start(skb);\n\t} else {\n\t\tip.hdr = skb_network_header(skb);\n\t\tl4.hdr = skb_transport_header(skb);\n\t}\n\n\t \n\tl2_len = ip.hdr - skb->data;\n\toffset = (l2_len / 2) << ICE_TX_DESC_LEN_MACLEN_S;\n\n\t \n\tif (ip.v4->version == 4)\n\t\tfirst->tx_flags |= ICE_TX_FLAGS_IPV4;\n\telse if (ip.v6->version == 6)\n\t\tfirst->tx_flags |= ICE_TX_FLAGS_IPV6;\n\n\tif (skb->encapsulation) {\n\t\tbool gso_ena = false;\n\t\tu32 tunnel = 0;\n\n\t\t \n\t\tif (first->tx_flags & ICE_TX_FLAGS_IPV4) {\n\t\t\ttunnel |= (first->tx_flags & ICE_TX_FLAGS_TSO) ?\n\t\t\t\t  ICE_TX_CTX_EIPT_IPV4 :\n\t\t\t\t  ICE_TX_CTX_EIPT_IPV4_NO_CSUM;\n\t\t\tl4_proto = ip.v4->protocol;\n\t\t} else if (first->tx_flags & ICE_TX_FLAGS_IPV6) {\n\t\t\tint ret;\n\n\t\t\ttunnel |= ICE_TX_CTX_EIPT_IPV6;\n\t\t\texthdr = ip.hdr + sizeof(*ip.v6);\n\t\t\tl4_proto = ip.v6->nexthdr;\n\t\t\tret = ipv6_skip_exthdr(skb, exthdr - skb->data,\n\t\t\t\t\t       &l4_proto, &frag_off);\n\t\t\tif (ret < 0)\n\t\t\t\treturn -1;\n\t\t}\n\n\t\t \n\t\tswitch (l4_proto) {\n\t\tcase IPPROTO_UDP:\n\t\t\ttunnel |= ICE_TXD_CTX_UDP_TUNNELING;\n\t\t\tfirst->tx_flags |= ICE_TX_FLAGS_TUNNEL;\n\t\t\tbreak;\n\t\tcase IPPROTO_GRE:\n\t\t\ttunnel |= ICE_TXD_CTX_GRE_TUNNELING;\n\t\t\tfirst->tx_flags |= ICE_TX_FLAGS_TUNNEL;\n\t\t\tbreak;\n\t\tcase IPPROTO_IPIP:\n\t\tcase IPPROTO_IPV6:\n\t\t\tfirst->tx_flags |= ICE_TX_FLAGS_TUNNEL;\n\t\t\tl4.hdr = skb_inner_network_header(skb);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (first->tx_flags & ICE_TX_FLAGS_TSO)\n\t\t\t\treturn -1;\n\n\t\t\tskb_checksum_help(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\ttunnel |= ((l4.hdr - ip.hdr) / 4) <<\n\t\t\t  ICE_TXD_CTX_QW0_EIPLEN_S;\n\n\t\t \n\t\tip.hdr = skb_inner_network_header(skb);\n\n\t\t \n\t\ttunnel |= ((ip.hdr - l4.hdr) / 2) <<\n\t\t\t   ICE_TXD_CTX_QW0_NATLEN_S;\n\n\t\tgso_ena = skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL;\n\t\t \n\t\tif ((first->tx_flags & ICE_TX_FLAGS_TSO) && !gso_ena &&\n\t\t    (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM))\n\t\t\ttunnel |= ICE_TXD_CTX_QW0_L4T_CS_M;\n\n\t\t \n\t\toff->cd_tunnel_params |= tunnel;\n\n\t\t \n\t\toff->cd_qw1 |= (u64)ICE_TX_DESC_DTYPE_CTX;\n\n\t\t \n\t\tl4.hdr = skb_inner_transport_header(skb);\n\t\tl4_proto = 0;\n\n\t\t \n\t\tfirst->tx_flags &= ~(ICE_TX_FLAGS_IPV4 | ICE_TX_FLAGS_IPV6);\n\t\tif (ip.v4->version == 4)\n\t\t\tfirst->tx_flags |= ICE_TX_FLAGS_IPV4;\n\t\tif (ip.v6->version == 6)\n\t\t\tfirst->tx_flags |= ICE_TX_FLAGS_IPV6;\n\t}\n\n\t \n\tif (first->tx_flags & ICE_TX_FLAGS_IPV4) {\n\t\tl4_proto = ip.v4->protocol;\n\t\t \n\t\tif (first->tx_flags & ICE_TX_FLAGS_TSO)\n\t\t\tcmd |= ICE_TX_DESC_CMD_IIPT_IPV4_CSUM;\n\t\telse\n\t\t\tcmd |= ICE_TX_DESC_CMD_IIPT_IPV4;\n\n\t} else if (first->tx_flags & ICE_TX_FLAGS_IPV6) {\n\t\tcmd |= ICE_TX_DESC_CMD_IIPT_IPV6;\n\t\texthdr = ip.hdr + sizeof(*ip.v6);\n\t\tl4_proto = ip.v6->nexthdr;\n\t\tif (l4.hdr != exthdr)\n\t\t\tipv6_skip_exthdr(skb, exthdr - skb->data, &l4_proto,\n\t\t\t\t\t &frag_off);\n\t} else {\n\t\treturn -1;\n\t}\n\n\t \n\tl3_len = l4.hdr - ip.hdr;\n\toffset |= (l3_len / 4) << ICE_TX_DESC_LEN_IPLEN_S;\n\n\t \n\tswitch (l4_proto) {\n\tcase IPPROTO_TCP:\n\t\t \n\t\tcmd |= ICE_TX_DESC_CMD_L4T_EOFT_TCP;\n\t\tl4_len = l4.tcp->doff;\n\t\toffset |= l4_len << ICE_TX_DESC_LEN_L4_LEN_S;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\t \n\t\tcmd |= ICE_TX_DESC_CMD_L4T_EOFT_UDP;\n\t\tl4_len = (sizeof(struct udphdr) >> 2);\n\t\toffset |= l4_len << ICE_TX_DESC_LEN_L4_LEN_S;\n\t\tbreak;\n\tcase IPPROTO_SCTP:\n\t\t \n\t\tcmd |= ICE_TX_DESC_CMD_L4T_EOFT_SCTP;\n\t\tl4_len = sizeof(struct sctphdr) >> 2;\n\t\toffset |= l4_len << ICE_TX_DESC_LEN_L4_LEN_S;\n\t\tbreak;\n\n\tdefault:\n\t\tif (first->tx_flags & ICE_TX_FLAGS_TSO)\n\t\t\treturn -1;\n\t\tskb_checksum_help(skb);\n\t\treturn 0;\n\t}\n\n\toff->td_cmd |= cmd;\n\toff->td_offset |= offset;\n\treturn 1;\n}\n\n \nstatic void\nice_tx_prepare_vlan_flags(struct ice_tx_ring *tx_ring, struct ice_tx_buf *first)\n{\n\tstruct sk_buff *skb = first->skb;\n\n\t \n\tif (!skb_vlan_tag_present(skb) && eth_type_vlan(skb->protocol))\n\t\treturn;\n\n\t \n\tif (skb_vlan_tag_present(skb)) {\n\t\tfirst->vid = skb_vlan_tag_get(skb);\n\t\tif (tx_ring->flags & ICE_TX_FLAGS_RING_VLAN_L2TAG2)\n\t\t\tfirst->tx_flags |= ICE_TX_FLAGS_HW_OUTER_SINGLE_VLAN;\n\t\telse\n\t\t\tfirst->tx_flags |= ICE_TX_FLAGS_HW_VLAN;\n\t}\n\n\tice_tx_prepare_vlan_flags_dcb(tx_ring, first);\n}\n\n \nstatic\nint ice_tso(struct ice_tx_buf *first, struct ice_tx_offload_params *off)\n{\n\tstruct sk_buff *skb = first->skb;\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tstruct udphdr *udp;\n\t\tunsigned char *hdr;\n\t} l4;\n\tu64 cd_mss, cd_tso_len;\n\t__be16 protocol;\n\tu32 paylen;\n\tu8 l4_start;\n\tint err;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\tprotocol = vlan_get_protocol(skb);\n\n\tif (eth_p_mpls(protocol))\n\t\tip.hdr = skb_inner_network_header(skb);\n\telse\n\t\tip.hdr = skb_network_header(skb);\n\tl4.hdr = skb_checksum_start(skb);\n\n\t \n\tif (ip.v4->version == 4) {\n\t\tip.v4->tot_len = 0;\n\t\tip.v4->check = 0;\n\t} else {\n\t\tip.v6->payload_len = 0;\n\t}\n\n\tif (skb_shinfo(skb)->gso_type & (SKB_GSO_GRE |\n\t\t\t\t\t SKB_GSO_GRE_CSUM |\n\t\t\t\t\t SKB_GSO_IPXIP4 |\n\t\t\t\t\t SKB_GSO_IPXIP6 |\n\t\t\t\t\t SKB_GSO_UDP_TUNNEL |\n\t\t\t\t\t SKB_GSO_UDP_TUNNEL_CSUM)) {\n\t\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL) &&\n\t\t    (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM)) {\n\t\t\tl4.udp->len = 0;\n\n\t\t\t \n\t\t\tl4_start = (u8)(l4.hdr - skb->data);\n\n\t\t\t \n\t\t\tpaylen = skb->len - l4_start;\n\t\t\tcsum_replace_by_diff(&l4.udp->check,\n\t\t\t\t\t     (__force __wsum)htonl(paylen));\n\t\t}\n\n\t\t \n\t\tip.hdr = skb_inner_network_header(skb);\n\t\tl4.hdr = skb_inner_transport_header(skb);\n\n\t\t \n\t\tif (ip.v4->version == 4) {\n\t\t\tip.v4->tot_len = 0;\n\t\t\tip.v4->check = 0;\n\t\t} else {\n\t\t\tip.v6->payload_len = 0;\n\t\t}\n\t}\n\n\t \n\tl4_start = (u8)(l4.hdr - skb->data);\n\n\t \n\tpaylen = skb->len - l4_start;\n\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {\n\t\tcsum_replace_by_diff(&l4.udp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\t\t \n\t\toff->header_len = (u8)sizeof(l4.udp) + l4_start;\n\t} else {\n\t\tcsum_replace_by_diff(&l4.tcp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\t\t \n\t\toff->header_len = (u8)((l4.tcp->doff * 4) + l4_start);\n\t}\n\n\t \n\tfirst->gso_segs = skb_shinfo(skb)->gso_segs;\n\tfirst->bytecount += (first->gso_segs - 1) * off->header_len;\n\n\tcd_tso_len = skb->len - off->header_len;\n\tcd_mss = skb_shinfo(skb)->gso_size;\n\n\t \n\toff->cd_qw1 |= (u64)(ICE_TX_DESC_DTYPE_CTX |\n\t\t\t     (ICE_TX_CTX_DESC_TSO << ICE_TXD_CTX_QW1_CMD_S) |\n\t\t\t     (cd_tso_len << ICE_TXD_CTX_QW1_TSO_LEN_S) |\n\t\t\t     (cd_mss << ICE_TXD_CTX_QW1_MSS_S));\n\tfirst->tx_flags |= ICE_TX_FLAGS_TSO;\n\treturn 1;\n}\n\n \nstatic unsigned int ice_txd_use_count(unsigned int size)\n{\n\treturn ((size * 85) >> 20) + ICE_DESCS_FOR_SKB_DATA_PTR;\n}\n\n \nstatic unsigned int ice_xmit_desc_count(struct sk_buff *skb)\n{\n\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[0];\n\tunsigned int nr_frags = skb_shinfo(skb)->nr_frags;\n\tunsigned int count = 0, size = skb_headlen(skb);\n\n\tfor (;;) {\n\t\tcount += ice_txd_use_count(size);\n\n\t\tif (!nr_frags--)\n\t\t\tbreak;\n\n\t\tsize = skb_frag_size(frag++);\n\t}\n\n\treturn count;\n}\n\n \nstatic bool __ice_chk_linearize(struct sk_buff *skb)\n{\n\tconst skb_frag_t *frag, *stale;\n\tint nr_frags, sum;\n\n\t \n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\tif (nr_frags < (ICE_MAX_BUF_TXD - 1))\n\t\treturn false;\n\n\t \n\tnr_frags -= ICE_MAX_BUF_TXD - 2;\n\tfrag = &skb_shinfo(skb)->frags[0];\n\n\t \n\tsum = 1 - skb_shinfo(skb)->gso_size;\n\n\t \n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\n\t \n\tfor (stale = &skb_shinfo(skb)->frags[0];; stale++) {\n\t\tint stale_size = skb_frag_size(stale);\n\n\t\tsum += skb_frag_size(frag++);\n\n\t\t \n\t\tif (stale_size > ICE_MAX_DATA_PER_TXD) {\n\t\t\tint align_pad = -(skb_frag_off(stale)) &\n\t\t\t\t\t(ICE_MAX_READ_REQ_SIZE - 1);\n\n\t\t\tsum -= align_pad;\n\t\t\tstale_size -= align_pad;\n\n\t\t\tdo {\n\t\t\t\tsum -= ICE_MAX_DATA_PER_TXD_ALIGNED;\n\t\t\t\tstale_size -= ICE_MAX_DATA_PER_TXD_ALIGNED;\n\t\t\t} while (stale_size > ICE_MAX_DATA_PER_TXD);\n\t\t}\n\n\t\t \n\t\tif (sum < 0)\n\t\t\treturn true;\n\n\t\tif (!nr_frags--)\n\t\t\tbreak;\n\n\t\tsum -= stale_size;\n\t}\n\n\treturn false;\n}\n\n \nstatic bool ice_chk_linearize(struct sk_buff *skb, unsigned int count)\n{\n\t \n\tif (likely(count < ICE_MAX_BUF_TXD))\n\t\treturn false;\n\n\tif (skb_is_gso(skb))\n\t\treturn __ice_chk_linearize(skb);\n\n\t \n\treturn count != ICE_MAX_BUF_TXD;\n}\n\n \nstatic void\nice_tstamp(struct ice_tx_ring *tx_ring, struct sk_buff *skb,\n\t   struct ice_tx_buf *first, struct ice_tx_offload_params *off)\n{\n\ts8 idx;\n\n\t \n\tif (likely(!(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)))\n\t\treturn;\n\n\tif (!tx_ring->ptp_tx)\n\t\treturn;\n\n\t \n\tif (first->tx_flags & ICE_TX_FLAGS_TSO)\n\t\treturn;\n\n\t \n\tidx = ice_ptp_request_ts(tx_ring->tx_tstamps, skb);\n\tif (idx < 0) {\n\t\ttx_ring->vsi->back->ptp.tx_hwtstamp_skipped++;\n\t\treturn;\n\t}\n\n\toff->cd_qw1 |= (u64)(ICE_TX_DESC_DTYPE_CTX |\n\t\t\t     (ICE_TX_CTX_DESC_TSYN << ICE_TXD_CTX_QW1_CMD_S) |\n\t\t\t     ((u64)idx << ICE_TXD_CTX_QW1_TSO_LEN_S));\n\tfirst->tx_flags |= ICE_TX_FLAGS_TSYN;\n}\n\n \nstatic netdev_tx_t\nice_xmit_frame_ring(struct sk_buff *skb, struct ice_tx_ring *tx_ring)\n{\n\tstruct ice_tx_offload_params offload = { 0 };\n\tstruct ice_vsi *vsi = tx_ring->vsi;\n\tstruct ice_tx_buf *first;\n\tstruct ethhdr *eth;\n\tunsigned int count;\n\tint tso, csum;\n\n\tice_trace(xmit_frame_ring, tx_ring, skb);\n\n\tif (unlikely(ipv6_hopopt_jumbo_remove(skb)))\n\t\tgoto out_drop;\n\n\tcount = ice_xmit_desc_count(skb);\n\tif (ice_chk_linearize(skb, count)) {\n\t\tif (__skb_linearize(skb))\n\t\t\tgoto out_drop;\n\t\tcount = ice_txd_use_count(skb->len);\n\t\ttx_ring->ring_stats->tx_stats.tx_linearize++;\n\t}\n\n\t \n\tif (ice_maybe_stop_tx(tx_ring, count + ICE_DESCS_PER_CACHE_LINE +\n\t\t\t      ICE_DESCS_FOR_CTX_DESC)) {\n\t\ttx_ring->ring_stats->tx_stats.tx_busy++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\tnetdev_txq_bql_enqueue_prefetchw(txring_txq(tx_ring));\n\n\toffload.tx_ring = tx_ring;\n\n\t \n\tfirst = &tx_ring->tx_buf[tx_ring->next_to_use];\n\tfirst->skb = skb;\n\tfirst->type = ICE_TX_BUF_SKB;\n\tfirst->bytecount = max_t(unsigned int, skb->len, ETH_ZLEN);\n\tfirst->gso_segs = 1;\n\tfirst->tx_flags = 0;\n\n\t \n\tice_tx_prepare_vlan_flags(tx_ring, first);\n\tif (first->tx_flags & ICE_TX_FLAGS_HW_OUTER_SINGLE_VLAN) {\n\t\toffload.cd_qw1 |= (u64)(ICE_TX_DESC_DTYPE_CTX |\n\t\t\t\t\t(ICE_TX_CTX_DESC_IL2TAG2 <<\n\t\t\t\t\tICE_TXD_CTX_QW1_CMD_S));\n\t\toffload.cd_l2tag2 = first->vid;\n\t}\n\n\t \n\ttso = ice_tso(first, &offload);\n\tif (tso < 0)\n\t\tgoto out_drop;\n\n\t \n\tcsum = ice_tx_csum(first, &offload);\n\tif (csum < 0)\n\t\tgoto out_drop;\n\n\t \n\teth = (struct ethhdr *)skb_mac_header(skb);\n\tif (unlikely((skb->priority == TC_PRIO_CONTROL ||\n\t\t      eth->h_proto == htons(ETH_P_LLDP)) &&\n\t\t     vsi->type == ICE_VSI_PF &&\n\t\t     vsi->port_info->qos_cfg.is_sw_lldp))\n\t\toffload.cd_qw1 |= (u64)(ICE_TX_DESC_DTYPE_CTX |\n\t\t\t\t\tICE_TX_CTX_DESC_SWTCH_UPLINK <<\n\t\t\t\t\tICE_TXD_CTX_QW1_CMD_S);\n\n\tice_tstamp(tx_ring, skb, first, &offload);\n\tif (ice_is_switchdev_running(vsi->back))\n\t\tice_eswitch_set_target_vsi(skb, &offload);\n\n\tif (offload.cd_qw1 & ICE_TX_DESC_DTYPE_CTX) {\n\t\tstruct ice_tx_ctx_desc *cdesc;\n\t\tu16 i = tx_ring->next_to_use;\n\n\t\t \n\t\tcdesc = ICE_TX_CTX_DESC(tx_ring, i);\n\t\ti++;\n\t\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\t\t \n\t\tcdesc->tunneling_params = cpu_to_le32(offload.cd_tunnel_params);\n\t\tcdesc->l2tag2 = cpu_to_le16(offload.cd_l2tag2);\n\t\tcdesc->rsvd = cpu_to_le16(0);\n\t\tcdesc->qw1 = cpu_to_le64(offload.cd_qw1);\n\t}\n\n\tice_tx_map(tx_ring, first, &offload);\n\treturn NETDEV_TX_OK;\n\nout_drop:\n\tice_trace(xmit_frame_ring_drop, tx_ring, skb);\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\n \nnetdev_tx_t ice_start_xmit(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_tx_ring *tx_ring;\n\n\ttx_ring = vsi->tx_rings[skb->queue_mapping];\n\n\t \n\tif (skb_put_padto(skb, ICE_MIN_TX_LEN))\n\t\treturn NETDEV_TX_OK;\n\n\treturn ice_xmit_frame_ring(skb, tx_ring);\n}\n\n \nstatic u8 ice_get_dscp_up(struct ice_dcbx_cfg *dcbcfg, struct sk_buff *skb)\n{\n\tu8 dscp = 0;\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\tdscp = ipv4_get_dsfield(ip_hdr(skb)) >> 2;\n\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\tdscp = ipv6_get_dsfield(ipv6_hdr(skb)) >> 2;\n\n\treturn dcbcfg->dscp_map[dscp];\n}\n\nu16\nice_select_queue(struct net_device *netdev, struct sk_buff *skb,\n\t\t struct net_device *sb_dev)\n{\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\tstruct ice_dcbx_cfg *dcbcfg;\n\n\tdcbcfg = &pf->hw.port_info->qos_cfg.local_dcbx_cfg;\n\tif (dcbcfg->pfc_mode == ICE_QOS_MODE_DSCP)\n\t\tskb->priority = ice_get_dscp_up(dcbcfg, skb);\n\n\treturn netdev_pick_tx(netdev, skb, sb_dev);\n}\n\n \nvoid ice_clean_ctrl_tx_irq(struct ice_tx_ring *tx_ring)\n{\n\tstruct ice_vsi *vsi = tx_ring->vsi;\n\ts16 i = tx_ring->next_to_clean;\n\tint budget = ICE_DFLT_IRQ_WORK;\n\tstruct ice_tx_desc *tx_desc;\n\tstruct ice_tx_buf *tx_buf;\n\n\ttx_buf = &tx_ring->tx_buf[i];\n\ttx_desc = ICE_TX_DESC(tx_ring, i);\n\ti -= tx_ring->count;\n\n\tdo {\n\t\tstruct ice_tx_desc *eop_desc = tx_buf->next_to_watch;\n\n\t\t \n\t\tif (!eop_desc)\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\t \n\t\tif (!(eop_desc->cmd_type_offset_bsz &\n\t\t      cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE)))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buf->next_to_watch = NULL;\n\t\ttx_desc->buf_addr = 0;\n\t\ttx_desc->cmd_type_offset_bsz = 0;\n\n\t\t \n\t\ttx_buf++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buf = tx_ring->tx_buf;\n\t\t\ttx_desc = ICE_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\t \n\t\tif (dma_unmap_len(tx_buf, len))\n\t\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t\t dma_unmap_addr(tx_buf, dma),\n\t\t\t\t\t dma_unmap_len(tx_buf, len),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\tif (tx_buf->type == ICE_TX_BUF_DUMMY)\n\t\t\tdevm_kfree(tx_ring->dev, tx_buf->raw_buf);\n\n\t\t \n\t\ttx_buf->type = ICE_TX_BUF_EMPTY;\n\t\ttx_buf->tx_flags = 0;\n\t\ttx_buf->next_to_watch = NULL;\n\t\tdma_unmap_len_set(tx_buf, len, 0);\n\t\ttx_desc->buf_addr = 0;\n\t\ttx_desc->cmd_type_offset_bsz = 0;\n\n\t\t \n\t\ttx_buf++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buf = tx_ring->tx_buf;\n\t\t\ttx_desc = ICE_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\tbudget--;\n\t} while (likely(budget));\n\n\ti += tx_ring->count;\n\ttx_ring->next_to_clean = i;\n\n\t \n\tice_irq_dynamic_ena(&vsi->back->hw, vsi, vsi->q_vectors[0]);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}