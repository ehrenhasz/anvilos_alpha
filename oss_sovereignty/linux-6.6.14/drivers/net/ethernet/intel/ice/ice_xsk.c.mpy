{
  "module_name": "ice_xsk.c",
  "hash_id": "b2b02d9adefe6c8f5ae7ecd414041c84c833cfed3aa11bbf4671ecedf5b77b5e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ice/ice_xsk.c",
  "human_readable_source": "\n \n\n#include <linux/bpf_trace.h>\n#include <net/xdp_sock_drv.h>\n#include <net/xdp.h>\n#include \"ice.h\"\n#include \"ice_base.h\"\n#include \"ice_type.h\"\n#include \"ice_xsk.h\"\n#include \"ice_txrx.h\"\n#include \"ice_txrx_lib.h\"\n#include \"ice_lib.h\"\n\nstatic struct xdp_buff **ice_xdp_buf(struct ice_rx_ring *rx_ring, u32 idx)\n{\n\treturn &rx_ring->xdp_buf[idx];\n}\n\n \nstatic void ice_qp_reset_stats(struct ice_vsi *vsi, u16 q_idx)\n{\n\tstruct ice_vsi_stats *vsi_stat;\n\tstruct ice_pf *pf;\n\n\tpf = vsi->back;\n\tif (!pf->vsi_stats)\n\t\treturn;\n\n\tvsi_stat = pf->vsi_stats[vsi->idx];\n\tif (!vsi_stat)\n\t\treturn;\n\n\tmemset(&vsi_stat->rx_ring_stats[q_idx]->rx_stats, 0,\n\t       sizeof(vsi_stat->rx_ring_stats[q_idx]->rx_stats));\n\tmemset(&vsi_stat->tx_ring_stats[q_idx]->stats, 0,\n\t       sizeof(vsi_stat->tx_ring_stats[q_idx]->stats));\n\tif (ice_is_xdp_ena_vsi(vsi))\n\t\tmemset(&vsi->xdp_rings[q_idx]->ring_stats->stats, 0,\n\t\t       sizeof(vsi->xdp_rings[q_idx]->ring_stats->stats));\n}\n\n \nstatic void ice_qp_clean_rings(struct ice_vsi *vsi, u16 q_idx)\n{\n\tice_clean_tx_ring(vsi->tx_rings[q_idx]);\n\tif (ice_is_xdp_ena_vsi(vsi)) {\n\t\tsynchronize_rcu();\n\t\tice_clean_tx_ring(vsi->xdp_rings[q_idx]);\n\t}\n\tice_clean_rx_ring(vsi->rx_rings[q_idx]);\n}\n\n \nstatic void\nice_qvec_toggle_napi(struct ice_vsi *vsi, struct ice_q_vector *q_vector,\n\t\t     bool enable)\n{\n\tif (!vsi->netdev || !q_vector)\n\t\treturn;\n\n\tif (enable)\n\t\tnapi_enable(&q_vector->napi);\n\telse\n\t\tnapi_disable(&q_vector->napi);\n}\n\n \nstatic void\nice_qvec_dis_irq(struct ice_vsi *vsi, struct ice_rx_ring *rx_ring,\n\t\t struct ice_q_vector *q_vector)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tu16 reg;\n\tu32 val;\n\n\t \n\treg = rx_ring->reg_idx;\n\tval = rd32(hw, QINT_RQCTL(reg));\n\tval &= ~QINT_RQCTL_CAUSE_ENA_M;\n\twr32(hw, QINT_RQCTL(reg), val);\n\n\tif (q_vector) {\n\t\twr32(hw, GLINT_DYN_CTL(q_vector->reg_idx), 0);\n\t\tice_flush(hw);\n\t\tsynchronize_irq(q_vector->irq.virq);\n\t}\n}\n\n \nstatic void\nice_qvec_cfg_msix(struct ice_vsi *vsi, struct ice_q_vector *q_vector)\n{\n\tu16 reg_idx = q_vector->reg_idx;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct ice_tx_ring *tx_ring;\n\tstruct ice_rx_ring *rx_ring;\n\n\tice_cfg_itr(hw, q_vector);\n\n\tice_for_each_tx_ring(tx_ring, q_vector->tx)\n\t\tice_cfg_txq_interrupt(vsi, tx_ring->reg_idx, reg_idx,\n\t\t\t\t      q_vector->tx.itr_idx);\n\n\tice_for_each_rx_ring(rx_ring, q_vector->rx)\n\t\tice_cfg_rxq_interrupt(vsi, rx_ring->reg_idx, reg_idx,\n\t\t\t\t      q_vector->rx.itr_idx);\n\n\tice_flush(hw);\n}\n\n \nstatic void ice_qvec_ena_irq(struct ice_vsi *vsi, struct ice_q_vector *q_vector)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\n\tice_irq_dynamic_ena(hw, vsi, q_vector);\n\n\tice_flush(hw);\n}\n\n \nstatic int ice_qp_dis(struct ice_vsi *vsi, u16 q_idx)\n{\n\tstruct ice_txq_meta txq_meta = { };\n\tstruct ice_q_vector *q_vector;\n\tstruct ice_tx_ring *tx_ring;\n\tstruct ice_rx_ring *rx_ring;\n\tint timeout = 50;\n\tint err;\n\n\tif (q_idx >= vsi->num_rxq || q_idx >= vsi->num_txq)\n\t\treturn -EINVAL;\n\n\ttx_ring = vsi->tx_rings[q_idx];\n\trx_ring = vsi->rx_rings[q_idx];\n\tq_vector = rx_ring->q_vector;\n\n\twhile (test_and_set_bit(ICE_CFG_BUSY, vsi->state)) {\n\t\ttimeout--;\n\t\tif (!timeout)\n\t\t\treturn -EBUSY;\n\t\tusleep_range(1000, 2000);\n\t}\n\tnetif_tx_stop_queue(netdev_get_tx_queue(vsi->netdev, q_idx));\n\n\tice_fill_txq_meta(vsi, tx_ring, &txq_meta);\n\terr = ice_vsi_stop_tx_ring(vsi, ICE_NO_RESET, 0, tx_ring, &txq_meta);\n\tif (err)\n\t\treturn err;\n\tif (ice_is_xdp_ena_vsi(vsi)) {\n\t\tstruct ice_tx_ring *xdp_ring = vsi->xdp_rings[q_idx];\n\n\t\tmemset(&txq_meta, 0, sizeof(txq_meta));\n\t\tice_fill_txq_meta(vsi, xdp_ring, &txq_meta);\n\t\terr = ice_vsi_stop_tx_ring(vsi, ICE_NO_RESET, 0, xdp_ring,\n\t\t\t\t\t   &txq_meta);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tice_qvec_dis_irq(vsi, rx_ring, q_vector);\n\n\terr = ice_vsi_ctrl_one_rx_ring(vsi, false, q_idx, true);\n\tif (err)\n\t\treturn err;\n\n\tice_qvec_toggle_napi(vsi, q_vector, false);\n\tice_qp_clean_rings(vsi, q_idx);\n\tice_qp_reset_stats(vsi, q_idx);\n\n\treturn 0;\n}\n\n \nstatic int ice_qp_ena(struct ice_vsi *vsi, u16 q_idx)\n{\n\tstruct ice_aqc_add_tx_qgrp *qg_buf;\n\tstruct ice_q_vector *q_vector;\n\tstruct ice_tx_ring *tx_ring;\n\tstruct ice_rx_ring *rx_ring;\n\tu16 size;\n\tint err;\n\n\tif (q_idx >= vsi->num_rxq || q_idx >= vsi->num_txq)\n\t\treturn -EINVAL;\n\n\tsize = struct_size(qg_buf, txqs, 1);\n\tqg_buf = kzalloc(size, GFP_KERNEL);\n\tif (!qg_buf)\n\t\treturn -ENOMEM;\n\n\tqg_buf->num_txqs = 1;\n\n\ttx_ring = vsi->tx_rings[q_idx];\n\trx_ring = vsi->rx_rings[q_idx];\n\tq_vector = rx_ring->q_vector;\n\n\terr = ice_vsi_cfg_txq(vsi, tx_ring, qg_buf);\n\tif (err)\n\t\tgoto free_buf;\n\n\tif (ice_is_xdp_ena_vsi(vsi)) {\n\t\tstruct ice_tx_ring *xdp_ring = vsi->xdp_rings[q_idx];\n\n\t\tmemset(qg_buf, 0, size);\n\t\tqg_buf->num_txqs = 1;\n\t\terr = ice_vsi_cfg_txq(vsi, xdp_ring, qg_buf);\n\t\tif (err)\n\t\t\tgoto free_buf;\n\t\tice_set_ring_xdp(xdp_ring);\n\t\tice_tx_xsk_pool(vsi, q_idx);\n\t}\n\n\terr = ice_vsi_cfg_rxq(rx_ring);\n\tif (err)\n\t\tgoto free_buf;\n\n\tice_qvec_cfg_msix(vsi, q_vector);\n\n\terr = ice_vsi_ctrl_one_rx_ring(vsi, true, q_idx, true);\n\tif (err)\n\t\tgoto free_buf;\n\n\tclear_bit(ICE_CFG_BUSY, vsi->state);\n\tice_qvec_toggle_napi(vsi, q_vector, true);\n\tice_qvec_ena_irq(vsi, q_vector);\n\n\tnetif_tx_start_queue(netdev_get_tx_queue(vsi->netdev, q_idx));\nfree_buf:\n\tkfree(qg_buf);\n\treturn err;\n}\n\n \nstatic int ice_xsk_pool_disable(struct ice_vsi *vsi, u16 qid)\n{\n\tstruct xsk_buff_pool *pool = xsk_get_pool_from_qid(vsi->netdev, qid);\n\n\tif (!pool)\n\t\treturn -EINVAL;\n\n\tclear_bit(qid, vsi->af_xdp_zc_qps);\n\txsk_pool_dma_unmap(pool, ICE_RX_DMA_ATTR);\n\n\treturn 0;\n}\n\n \nstatic int\nice_xsk_pool_enable(struct ice_vsi *vsi, struct xsk_buff_pool *pool, u16 qid)\n{\n\tint err;\n\n\tif (vsi->type != ICE_VSI_PF)\n\t\treturn -EINVAL;\n\n\tif (qid >= vsi->netdev->real_num_rx_queues ||\n\t    qid >= vsi->netdev->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\terr = xsk_pool_dma_map(pool, ice_pf_to_dev(vsi->back),\n\t\t\t       ICE_RX_DMA_ATTR);\n\tif (err)\n\t\treturn err;\n\n\tset_bit(qid, vsi->af_xdp_zc_qps);\n\n\treturn 0;\n}\n\n \nstatic int\nice_realloc_rx_xdp_bufs(struct ice_rx_ring *rx_ring, bool pool_present)\n{\n\tsize_t elem_size = pool_present ? sizeof(*rx_ring->xdp_buf) :\n\t\t\t\t\t  sizeof(*rx_ring->rx_buf);\n\tvoid *sw_ring = kcalloc(rx_ring->count, elem_size, GFP_KERNEL);\n\n\tif (!sw_ring)\n\t\treturn -ENOMEM;\n\n\tif (pool_present) {\n\t\tkfree(rx_ring->rx_buf);\n\t\trx_ring->rx_buf = NULL;\n\t\trx_ring->xdp_buf = sw_ring;\n\t} else {\n\t\tkfree(rx_ring->xdp_buf);\n\t\trx_ring->xdp_buf = NULL;\n\t\trx_ring->rx_buf = sw_ring;\n\t}\n\n\treturn 0;\n}\n\n \nint ice_realloc_zc_buf(struct ice_vsi *vsi, bool zc)\n{\n\tstruct ice_rx_ring *rx_ring;\n\tunsigned long q;\n\n\tfor_each_set_bit(q, vsi->af_xdp_zc_qps,\n\t\t\t max_t(int, vsi->alloc_txq, vsi->alloc_rxq)) {\n\t\trx_ring = vsi->rx_rings[q];\n\t\tif (ice_realloc_rx_xdp_bufs(rx_ring, zc))\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nint ice_xsk_pool_setup(struct ice_vsi *vsi, struct xsk_buff_pool *pool, u16 qid)\n{\n\tbool if_running, pool_present = !!pool;\n\tint ret = 0, pool_failure = 0;\n\n\tif (qid >= vsi->num_rxq || qid >= vsi->num_txq) {\n\t\tnetdev_err(vsi->netdev, \"Please use queue id in scope of combined queues count\\n\");\n\t\tpool_failure = -EINVAL;\n\t\tgoto failure;\n\t}\n\n\tif_running = netif_running(vsi->netdev) && ice_is_xdp_ena_vsi(vsi);\n\n\tif (if_running) {\n\t\tstruct ice_rx_ring *rx_ring = vsi->rx_rings[qid];\n\n\t\tret = ice_qp_dis(vsi, qid);\n\t\tif (ret) {\n\t\t\tnetdev_err(vsi->netdev, \"ice_qp_dis error = %d\\n\", ret);\n\t\t\tgoto xsk_pool_if_up;\n\t\t}\n\n\t\tret = ice_realloc_rx_xdp_bufs(rx_ring, pool_present);\n\t\tif (ret)\n\t\t\tgoto xsk_pool_if_up;\n\t}\n\n\tpool_failure = pool_present ? ice_xsk_pool_enable(vsi, pool, qid) :\n\t\t\t\t      ice_xsk_pool_disable(vsi, qid);\n\nxsk_pool_if_up:\n\tif (if_running) {\n\t\tret = ice_qp_ena(vsi, qid);\n\t\tif (!ret && pool_present)\n\t\t\tnapi_schedule(&vsi->rx_rings[qid]->xdp_ring->q_vector->napi);\n\t\telse if (ret)\n\t\t\tnetdev_err(vsi->netdev, \"ice_qp_ena error = %d\\n\", ret);\n\t}\n\nfailure:\n\tif (pool_failure) {\n\t\tnetdev_err(vsi->netdev, \"Could not %sable buffer pool, error = %d\\n\",\n\t\t\t   pool_present ? \"en\" : \"dis\", pool_failure);\n\t\treturn pool_failure;\n\t}\n\n\treturn ret;\n}\n\n \nstatic u16 ice_fill_rx_descs(struct xsk_buff_pool *pool, struct xdp_buff **xdp,\n\t\t\t     union ice_32b_rx_flex_desc *rx_desc, u16 count)\n{\n\tdma_addr_t dma;\n\tu16 buffs;\n\tint i;\n\n\tbuffs = xsk_buff_alloc_batch(pool, xdp, count);\n\tfor (i = 0; i < buffs; i++) {\n\t\tdma = xsk_buff_xdp_get_dma(*xdp);\n\t\trx_desc->read.pkt_addr = cpu_to_le64(dma);\n\t\trx_desc->wb.status_error0 = 0;\n\n\t\trx_desc++;\n\t\txdp++;\n\t}\n\n\treturn buffs;\n}\n\n \nstatic bool __ice_alloc_rx_bufs_zc(struct ice_rx_ring *rx_ring, u16 count)\n{\n\tu32 nb_buffs_extra = 0, nb_buffs = 0;\n\tunion ice_32b_rx_flex_desc *rx_desc;\n\tu16 ntu = rx_ring->next_to_use;\n\tu16 total_count = count;\n\tstruct xdp_buff **xdp;\n\n\trx_desc = ICE_RX_DESC(rx_ring, ntu);\n\txdp = ice_xdp_buf(rx_ring, ntu);\n\n\tif (ntu + count >= rx_ring->count) {\n\t\tnb_buffs_extra = ice_fill_rx_descs(rx_ring->xsk_pool, xdp,\n\t\t\t\t\t\t   rx_desc,\n\t\t\t\t\t\t   rx_ring->count - ntu);\n\t\tif (nb_buffs_extra != rx_ring->count - ntu) {\n\t\t\tntu += nb_buffs_extra;\n\t\t\tgoto exit;\n\t\t}\n\t\trx_desc = ICE_RX_DESC(rx_ring, 0);\n\t\txdp = ice_xdp_buf(rx_ring, 0);\n\t\tntu = 0;\n\t\tcount -= nb_buffs_extra;\n\t\tice_release_rx_desc(rx_ring, 0);\n\t}\n\n\tnb_buffs = ice_fill_rx_descs(rx_ring->xsk_pool, xdp, rx_desc, count);\n\n\tntu += nb_buffs;\n\tif (ntu == rx_ring->count)\n\t\tntu = 0;\n\nexit:\n\tif (rx_ring->next_to_use != ntu)\n\t\tice_release_rx_desc(rx_ring, ntu);\n\n\treturn total_count == (nb_buffs_extra + nb_buffs);\n}\n\n \nbool ice_alloc_rx_bufs_zc(struct ice_rx_ring *rx_ring, u16 count)\n{\n\tu16 rx_thresh = ICE_RING_QUARTER(rx_ring);\n\tu16 leftover, i, tail_bumps;\n\n\ttail_bumps = count / rx_thresh;\n\tleftover = count - (tail_bumps * rx_thresh);\n\n\tfor (i = 0; i < tail_bumps; i++)\n\t\tif (!__ice_alloc_rx_bufs_zc(rx_ring, rx_thresh))\n\t\t\treturn false;\n\treturn __ice_alloc_rx_bufs_zc(rx_ring, leftover);\n}\n\n \nstatic struct sk_buff *\nice_construct_skb_zc(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp)\n{\n\tunsigned int totalsize = xdp->data_end - xdp->data_meta;\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n\tstruct skb_shared_info *sinfo = NULL;\n\tstruct sk_buff *skb;\n\tu32 nr_frags = 0;\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tnr_frags = sinfo->nr_frags;\n\t}\n\tnet_prefetch(xdp->data_meta);\n\n\tskb = __napi_alloc_skb(&rx_ring->q_vector->napi, totalsize,\n\t\t\t       GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemcpy(__skb_put(skb, totalsize), xdp->data_meta,\n\t       ALIGN(totalsize, sizeof(long)));\n\n\tif (metasize) {\n\t\tskb_metadata_set(skb, metasize);\n\t\t__skb_pull(skb, metasize);\n\t}\n\n\tif (likely(!xdp_buff_has_frags(xdp)))\n\t\tgoto out;\n\n\tfor (int i = 0; i < nr_frags; i++) {\n\t\tstruct skb_shared_info *skinfo = skb_shinfo(skb);\n\t\tskb_frag_t *frag = &sinfo->frags[i];\n\t\tstruct page *page;\n\t\tvoid *addr;\n\n\t\tpage = dev_alloc_page();\n\t\tif (!page) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NULL;\n\t\t}\n\t\taddr = page_to_virt(page);\n\n\t\tmemcpy(addr, skb_frag_page(frag), skb_frag_size(frag));\n\n\t\t__skb_fill_page_desc_noacc(skinfo, skinfo->nr_frags++,\n\t\t\t\t\t   addr, 0, skb_frag_size(frag));\n\t}\n\nout:\n\txsk_buff_free(xdp);\n\treturn skb;\n}\n\n \nstatic u32 ice_clean_xdp_irq_zc(struct ice_tx_ring *xdp_ring)\n{\n\tu16 ntc = xdp_ring->next_to_clean;\n\tstruct ice_tx_desc *tx_desc;\n\tu16 cnt = xdp_ring->count;\n\tstruct ice_tx_buf *tx_buf;\n\tu16 completed_frames = 0;\n\tu16 xsk_frames = 0;\n\tu16 last_rs;\n\tint i;\n\n\tlast_rs = xdp_ring->next_to_use ? xdp_ring->next_to_use - 1 : cnt - 1;\n\ttx_desc = ICE_TX_DESC(xdp_ring, last_rs);\n\tif (tx_desc->cmd_type_offset_bsz &\n\t    cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE)) {\n\t\tif (last_rs >= ntc)\n\t\t\tcompleted_frames = last_rs - ntc + 1;\n\t\telse\n\t\t\tcompleted_frames = last_rs + cnt - ntc + 1;\n\t}\n\n\tif (!completed_frames)\n\t\treturn 0;\n\n\tif (likely(!xdp_ring->xdp_tx_active)) {\n\t\txsk_frames = completed_frames;\n\t\tgoto skip;\n\t}\n\n\tntc = xdp_ring->next_to_clean;\n\tfor (i = 0; i < completed_frames; i++) {\n\t\ttx_buf = &xdp_ring->tx_buf[ntc];\n\n\t\tif (tx_buf->type == ICE_TX_BUF_XSK_TX) {\n\t\t\ttx_buf->type = ICE_TX_BUF_EMPTY;\n\t\t\txsk_buff_free(tx_buf->xdp);\n\t\t\txdp_ring->xdp_tx_active--;\n\t\t} else {\n\t\t\txsk_frames++;\n\t\t}\n\n\t\tntc++;\n\t\tif (ntc >= xdp_ring->count)\n\t\t\tntc = 0;\n\t}\nskip:\n\ttx_desc->cmd_type_offset_bsz = 0;\n\txdp_ring->next_to_clean += completed_frames;\n\tif (xdp_ring->next_to_clean >= cnt)\n\t\txdp_ring->next_to_clean -= cnt;\n\tif (xsk_frames)\n\t\txsk_tx_completed(xdp_ring->xsk_pool, xsk_frames);\n\n\treturn completed_frames;\n}\n\n \nstatic int ice_xmit_xdp_tx_zc(struct xdp_buff *xdp,\n\t\t\t      struct ice_tx_ring *xdp_ring)\n{\n\tstruct skb_shared_info *sinfo = NULL;\n\tu32 size = xdp->data_end - xdp->data;\n\tu32 ntu = xdp_ring->next_to_use;\n\tstruct ice_tx_desc *tx_desc;\n\tstruct ice_tx_buf *tx_buf;\n\tstruct xdp_buff *head;\n\tu32 nr_frags = 0;\n\tu32 free_space;\n\tu32 frag = 0;\n\n\tfree_space = ICE_DESC_UNUSED(xdp_ring);\n\tif (free_space < ICE_RING_QUARTER(xdp_ring))\n\t\tfree_space += ice_clean_xdp_irq_zc(xdp_ring);\n\n\tif (unlikely(!free_space))\n\t\tgoto busy;\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tnr_frags = sinfo->nr_frags;\n\t\tif (free_space < nr_frags + 1)\n\t\t\tgoto busy;\n\t}\n\n\ttx_desc = ICE_TX_DESC(xdp_ring, ntu);\n\ttx_buf = &xdp_ring->tx_buf[ntu];\n\thead = xdp;\n\n\tfor (;;) {\n\t\tdma_addr_t dma;\n\n\t\tdma = xsk_buff_xdp_get_dma(xdp);\n\t\txsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma, size);\n\n\t\ttx_buf->xdp = xdp;\n\t\ttx_buf->type = ICE_TX_BUF_XSK_TX;\n\t\ttx_desc->buf_addr = cpu_to_le64(dma);\n\t\ttx_desc->cmd_type_offset_bsz = ice_build_ctob(0, 0, size, 0);\n\t\t \n\t\txdp_ring->xdp_tx_active++;\n\n\t\tif (++ntu == xdp_ring->count)\n\t\t\tntu = 0;\n\n\t\tif (frag == nr_frags)\n\t\t\tbreak;\n\n\t\ttx_desc = ICE_TX_DESC(xdp_ring, ntu);\n\t\ttx_buf = &xdp_ring->tx_buf[ntu];\n\n\t\txdp = xsk_buff_get_frag(head);\n\t\tsize = skb_frag_size(&sinfo->frags[frag]);\n\t\tfrag++;\n\t}\n\n\txdp_ring->next_to_use = ntu;\n\t \n\ttx_desc->cmd_type_offset_bsz |=\n\t\tcpu_to_le64(ICE_TX_DESC_CMD_EOP << ICE_TXD_QW1_CMD_S);\n\n\treturn ICE_XDP_TX;\n\nbusy:\n\txdp_ring->ring_stats->tx_stats.tx_busy++;\n\n\treturn ICE_XDP_CONSUMED;\n}\n\n \nstatic int\nice_run_xdp_zc(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp,\n\t       struct bpf_prog *xdp_prog, struct ice_tx_ring *xdp_ring)\n{\n\tint err, result = ICE_XDP_PASS;\n\tu32 act;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\tif (likely(act == XDP_REDIRECT)) {\n\t\terr = xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog);\n\t\tif (!err)\n\t\t\treturn ICE_XDP_REDIR;\n\t\tif (xsk_uses_need_wakeup(rx_ring->xsk_pool) && err == -ENOBUFS)\n\t\t\tresult = ICE_XDP_EXIT;\n\t\telse\n\t\t\tresult = ICE_XDP_CONSUMED;\n\t\tgoto out_failure;\n\t}\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\tresult = ice_xmit_xdp_tx_zc(xdp, xdp_ring);\n\t\tif (result == ICE_XDP_CONSUMED)\n\t\t\tgoto out_failure;\n\t\tbreak;\n\tcase XDP_DROP:\n\t\tresult = ICE_XDP_CONSUMED;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\tresult = ICE_XDP_CONSUMED;\nout_failure:\n\t\ttrace_xdp_exception(rx_ring->netdev, xdp_prog, act);\n\t\tbreak;\n\t}\n\n\treturn result;\n}\n\nstatic int\nice_add_xsk_frag(struct ice_rx_ring *rx_ring, struct xdp_buff *first,\n\t\t struct xdp_buff *xdp, const unsigned int size)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(first);\n\n\tif (!size)\n\t\treturn 0;\n\n\tif (!xdp_buff_has_frags(first)) {\n\t\tsinfo->nr_frags = 0;\n\t\tsinfo->xdp_frags_size = 0;\n\t\txdp_buff_set_frags_flag(first);\n\t}\n\n\tif (unlikely(sinfo->nr_frags == MAX_SKB_FRAGS)) {\n\t\txsk_buff_free(first);\n\t\treturn -ENOMEM;\n\t}\n\n\t__skb_fill_page_desc_noacc(sinfo, sinfo->nr_frags++,\n\t\t\t\t   virt_to_page(xdp->data_hard_start), 0, size);\n\tsinfo->xdp_frags_size += size;\n\txsk_buff_add_frag(xdp);\n\n\treturn 0;\n}\n\n \nint ice_clean_rx_irq_zc(struct ice_rx_ring *rx_ring, int budget)\n{\n\tunsigned int total_rx_bytes = 0, total_rx_packets = 0;\n\tstruct xsk_buff_pool *xsk_pool = rx_ring->xsk_pool;\n\tu32 ntc = rx_ring->next_to_clean;\n\tu32 ntu = rx_ring->next_to_use;\n\tstruct xdp_buff *first = NULL;\n\tstruct ice_tx_ring *xdp_ring;\n\tunsigned int xdp_xmit = 0;\n\tstruct bpf_prog *xdp_prog;\n\tu32 cnt = rx_ring->count;\n\tbool failure = false;\n\tint entries_to_alloc;\n\n\t \n\txdp_prog = READ_ONCE(rx_ring->xdp_prog);\n\txdp_ring = rx_ring->xdp_ring;\n\n\tif (ntc != rx_ring->first_desc)\n\t\tfirst = *ice_xdp_buf(rx_ring, rx_ring->first_desc);\n\n\twhile (likely(total_rx_packets < (unsigned int)budget)) {\n\t\tunion ice_32b_rx_flex_desc *rx_desc;\n\t\tunsigned int size, xdp_res = 0;\n\t\tstruct xdp_buff *xdp;\n\t\tstruct sk_buff *skb;\n\t\tu16 stat_err_bits;\n\t\tu16 vlan_tag = 0;\n\t\tu16 rx_ptype;\n\n\t\trx_desc = ICE_RX_DESC(rx_ring, ntc);\n\n\t\tstat_err_bits = BIT(ICE_RX_FLEX_DESC_STATUS0_DD_S);\n\t\tif (!ice_test_staterr(rx_desc->wb.status_error0, stat_err_bits))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tif (unlikely(ntc == ntu))\n\t\t\tbreak;\n\n\t\txdp = *ice_xdp_buf(rx_ring, ntc);\n\n\t\tsize = le16_to_cpu(rx_desc->wb.pkt_len) &\n\t\t\t\t   ICE_RX_FLX_DESC_PKT_LEN_M;\n\n\t\txsk_buff_set_size(xdp, size);\n\t\txsk_buff_dma_sync_for_cpu(xdp, xsk_pool);\n\n\t\tif (!first) {\n\t\t\tfirst = xdp;\n\t\t\txdp_buff_clear_frags_flag(first);\n\t\t} else if (ice_add_xsk_frag(rx_ring, first, xdp, size)) {\n\t\t\tbreak;\n\t\t}\n\n\t\tif (++ntc == cnt)\n\t\t\tntc = 0;\n\n\t\tif (ice_is_non_eop(rx_ring, rx_desc))\n\t\t\tcontinue;\n\n\t\txdp_res = ice_run_xdp_zc(rx_ring, first, xdp_prog, xdp_ring);\n\t\tif (likely(xdp_res & (ICE_XDP_TX | ICE_XDP_REDIR))) {\n\t\t\txdp_xmit |= xdp_res;\n\t\t} else if (xdp_res == ICE_XDP_EXIT) {\n\t\t\tfailure = true;\n\t\t\tfirst = NULL;\n\t\t\trx_ring->first_desc = ntc;\n\t\t\tbreak;\n\t\t} else if (xdp_res == ICE_XDP_CONSUMED) {\n\t\t\txsk_buff_free(first);\n\t\t} else if (xdp_res == ICE_XDP_PASS) {\n\t\t\tgoto construct_skb;\n\t\t}\n\n\t\ttotal_rx_bytes += xdp_get_buff_len(first);\n\t\ttotal_rx_packets++;\n\n\t\tfirst = NULL;\n\t\trx_ring->first_desc = ntc;\n\t\tcontinue;\n\nconstruct_skb:\n\t\t \n\t\tskb = ice_construct_skb_zc(rx_ring, first);\n\t\tif (!skb) {\n\t\t\trx_ring->ring_stats->rx_stats.alloc_buf_failed++;\n\t\t\tbreak;\n\t\t}\n\n\t\tfirst = NULL;\n\t\trx_ring->first_desc = ntc;\n\n\t\tif (eth_skb_pad(skb)) {\n\t\t\tskb = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\ttotal_rx_bytes += skb->len;\n\t\ttotal_rx_packets++;\n\n\t\tvlan_tag = ice_get_vlan_tag_from_rx_desc(rx_desc);\n\n\t\trx_ptype = le16_to_cpu(rx_desc->wb.ptype_flex_flags0) &\n\t\t\t\t       ICE_RX_FLEX_DESC_PTYPE_M;\n\n\t\tice_process_skb_fields(rx_ring, rx_desc, skb, rx_ptype);\n\t\tice_receive_skb(rx_ring, skb, vlan_tag);\n\t}\n\n\trx_ring->next_to_clean = ntc;\n\tentries_to_alloc = ICE_RX_DESC_UNUSED(rx_ring);\n\tif (entries_to_alloc > ICE_RING_QUARTER(rx_ring))\n\t\tfailure |= !ice_alloc_rx_bufs_zc(rx_ring, entries_to_alloc);\n\n\tice_finalize_xdp_rx(xdp_ring, xdp_xmit, 0);\n\tice_update_rx_ring_stats(rx_ring, total_rx_packets, total_rx_bytes);\n\n\tif (xsk_uses_need_wakeup(xsk_pool)) {\n\t\t \n\t\tif (failure || ntc == rx_ring->next_to_use)\n\t\t\txsk_set_rx_need_wakeup(xsk_pool);\n\t\telse\n\t\t\txsk_clear_rx_need_wakeup(xsk_pool);\n\n\t\treturn (int)total_rx_packets;\n\t}\n\n\treturn failure ? budget : (int)total_rx_packets;\n}\n\n \nstatic void ice_xmit_pkt(struct ice_tx_ring *xdp_ring, struct xdp_desc *desc,\n\t\t\t unsigned int *total_bytes)\n{\n\tstruct ice_tx_desc *tx_desc;\n\tdma_addr_t dma;\n\n\tdma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, desc->addr);\n\txsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma, desc->len);\n\n\ttx_desc = ICE_TX_DESC(xdp_ring, xdp_ring->next_to_use++);\n\ttx_desc->buf_addr = cpu_to_le64(dma);\n\ttx_desc->cmd_type_offset_bsz = ice_build_ctob(xsk_is_eop_desc(desc),\n\t\t\t\t\t\t      0, desc->len, 0);\n\n\t*total_bytes += desc->len;\n}\n\n \nstatic void ice_xmit_pkt_batch(struct ice_tx_ring *xdp_ring, struct xdp_desc *descs,\n\t\t\t       unsigned int *total_bytes)\n{\n\tu16 ntu = xdp_ring->next_to_use;\n\tstruct ice_tx_desc *tx_desc;\n\tu32 i;\n\n\tloop_unrolled_for(i = 0; i < PKTS_PER_BATCH; i++) {\n\t\tdma_addr_t dma;\n\n\t\tdma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, descs[i].addr);\n\t\txsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma, descs[i].len);\n\n\t\ttx_desc = ICE_TX_DESC(xdp_ring, ntu++);\n\t\ttx_desc->buf_addr = cpu_to_le64(dma);\n\t\ttx_desc->cmd_type_offset_bsz = ice_build_ctob(xsk_is_eop_desc(&descs[i]),\n\t\t\t\t\t\t\t      0, descs[i].len, 0);\n\n\t\t*total_bytes += descs[i].len;\n\t}\n\n\txdp_ring->next_to_use = ntu;\n}\n\n \nstatic void ice_fill_tx_hw_ring(struct ice_tx_ring *xdp_ring, struct xdp_desc *descs,\n\t\t\t\tu32 nb_pkts, unsigned int *total_bytes)\n{\n\tu32 batched, leftover, i;\n\n\tbatched = ALIGN_DOWN(nb_pkts, PKTS_PER_BATCH);\n\tleftover = nb_pkts & (PKTS_PER_BATCH - 1);\n\tfor (i = 0; i < batched; i += PKTS_PER_BATCH)\n\t\tice_xmit_pkt_batch(xdp_ring, &descs[i], total_bytes);\n\tfor (; i < batched + leftover; i++)\n\t\tice_xmit_pkt(xdp_ring, &descs[i], total_bytes);\n}\n\n \nbool ice_xmit_zc(struct ice_tx_ring *xdp_ring)\n{\n\tstruct xdp_desc *descs = xdp_ring->xsk_pool->tx_descs;\n\tu32 nb_pkts, nb_processed = 0;\n\tunsigned int total_bytes = 0;\n\tint budget;\n\n\tice_clean_xdp_irq_zc(xdp_ring);\n\n\tbudget = ICE_DESC_UNUSED(xdp_ring);\n\tbudget = min_t(u16, budget, ICE_RING_QUARTER(xdp_ring));\n\n\tnb_pkts = xsk_tx_peek_release_desc_batch(xdp_ring->xsk_pool, budget);\n\tif (!nb_pkts)\n\t\treturn true;\n\n\tif (xdp_ring->next_to_use + nb_pkts >= xdp_ring->count) {\n\t\tnb_processed = xdp_ring->count - xdp_ring->next_to_use;\n\t\tice_fill_tx_hw_ring(xdp_ring, descs, nb_processed, &total_bytes);\n\t\txdp_ring->next_to_use = 0;\n\t}\n\n\tice_fill_tx_hw_ring(xdp_ring, &descs[nb_processed], nb_pkts - nb_processed,\n\t\t\t    &total_bytes);\n\n\tice_set_rs_bit(xdp_ring);\n\tice_xdp_ring_update_tail(xdp_ring);\n\tice_update_tx_ring_stats(xdp_ring, nb_pkts, total_bytes);\n\n\tif (xsk_uses_need_wakeup(xdp_ring->xsk_pool))\n\t\txsk_set_tx_need_wakeup(xdp_ring->xsk_pool);\n\n\treturn nb_pkts < budget;\n}\n\n \nint\nice_xsk_wakeup(struct net_device *netdev, u32 queue_id,\n\t       u32 __always_unused flags)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_q_vector *q_vector;\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_tx_ring *ring;\n\n\tif (test_bit(ICE_VSI_DOWN, vsi->state))\n\t\treturn -ENETDOWN;\n\n\tif (!ice_is_xdp_ena_vsi(vsi))\n\t\treturn -EINVAL;\n\n\tif (queue_id >= vsi->num_txq || queue_id >= vsi->num_rxq)\n\t\treturn -EINVAL;\n\n\tring = vsi->rx_rings[queue_id]->xdp_ring;\n\n\tif (!ring->xsk_pool)\n\t\treturn -EINVAL;\n\n\t \n\tq_vector = ring->q_vector;\n\tif (!napi_if_scheduled_mark_missed(&q_vector->napi))\n\t\tice_trigger_sw_intr(&vsi->back->hw, q_vector);\n\n\treturn 0;\n}\n\n \nbool ice_xsk_any_rx_ring_ena(struct ice_vsi *vsi)\n{\n\tint i;\n\n\tice_for_each_rxq(vsi, i) {\n\t\tif (xsk_get_pool_from_qid(vsi->netdev, i))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nvoid ice_xsk_clean_rx_ring(struct ice_rx_ring *rx_ring)\n{\n\tu16 ntc = rx_ring->next_to_clean;\n\tu16 ntu = rx_ring->next_to_use;\n\n\twhile (ntc != ntu) {\n\t\tstruct xdp_buff *xdp = *ice_xdp_buf(rx_ring, ntc);\n\n\t\txsk_buff_free(xdp);\n\t\tntc++;\n\t\tif (ntc >= rx_ring->count)\n\t\t\tntc = 0;\n\t}\n}\n\n \nvoid ice_xsk_clean_xdp_ring(struct ice_tx_ring *xdp_ring)\n{\n\tu16 ntc = xdp_ring->next_to_clean, ntu = xdp_ring->next_to_use;\n\tu32 xsk_frames = 0;\n\n\twhile (ntc != ntu) {\n\t\tstruct ice_tx_buf *tx_buf = &xdp_ring->tx_buf[ntc];\n\n\t\tif (tx_buf->type == ICE_TX_BUF_XSK_TX) {\n\t\t\ttx_buf->type = ICE_TX_BUF_EMPTY;\n\t\t\txsk_buff_free(tx_buf->xdp);\n\t\t} else {\n\t\t\txsk_frames++;\n\t\t}\n\n\t\tntc++;\n\t\tif (ntc >= xdp_ring->count)\n\t\t\tntc = 0;\n\t}\n\n\tif (xsk_frames)\n\t\txsk_tx_completed(xdp_ring->xsk_pool, xsk_frames);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}