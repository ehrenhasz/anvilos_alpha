{
  "module_name": "ice_main.c",
  "hash_id": "4419e29ffb74c80590671f72d64ac57f91bc14645e1bbb96ebfbf800654c3857",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ice/ice_main.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <generated/utsrelease.h>\n#include <linux/crash_dump.h>\n#include \"ice.h\"\n#include \"ice_base.h\"\n#include \"ice_lib.h\"\n#include \"ice_fltr.h\"\n#include \"ice_dcb_lib.h\"\n#include \"ice_dcb_nl.h\"\n#include \"ice_devlink.h\"\n \n#define CREATE_TRACE_POINTS\n#include \"ice_trace.h\"\n#include \"ice_eswitch.h\"\n#include \"ice_tc_lib.h\"\n#include \"ice_vsi_vlan_ops.h\"\n#include <net/xdp_sock_drv.h>\n\n#define DRV_SUMMARY\t\"Intel(R) Ethernet Connection E800 Series Linux Driver\"\nstatic const char ice_driver_string[] = DRV_SUMMARY;\nstatic const char ice_copyright[] = \"Copyright (c) 2018, Intel Corporation.\";\n\n \n#define ICE_DDP_PKG_PATH\t\"intel/ice/ddp/\"\n#define ICE_DDP_PKG_FILE\tICE_DDP_PKG_PATH \"ice.pkg\"\n\nMODULE_AUTHOR(\"Intel Corporation, <linux.nics@intel.com>\");\nMODULE_DESCRIPTION(DRV_SUMMARY);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_FIRMWARE(ICE_DDP_PKG_FILE);\n\nstatic int debug = -1;\nmodule_param(debug, int, 0644);\n#ifndef CONFIG_DYNAMIC_DEBUG\nMODULE_PARM_DESC(debug, \"netif level (0=none,...,16=all), hw debug_mask (0x8XXXXXXX)\");\n#else\nMODULE_PARM_DESC(debug, \"netif level (0=none,...,16=all)\");\n#endif  \n\nDEFINE_STATIC_KEY_FALSE(ice_xdp_locking_key);\nEXPORT_SYMBOL(ice_xdp_locking_key);\n\n \nstruct device *ice_hw_to_dev(struct ice_hw *hw)\n{\n\tstruct ice_pf *pf = container_of(hw, struct ice_pf, hw);\n\n\treturn &pf->pdev->dev;\n}\n\nstatic struct workqueue_struct *ice_wq;\nstruct workqueue_struct *ice_lag_wq;\nstatic const struct net_device_ops ice_netdev_safe_mode_ops;\nstatic const struct net_device_ops ice_netdev_ops;\n\nstatic void ice_rebuild(struct ice_pf *pf, enum ice_reset_req reset_type);\n\nstatic void ice_vsi_release_all(struct ice_pf *pf);\n\nstatic int ice_rebuild_channels(struct ice_pf *pf);\nstatic void ice_remove_q_channels(struct ice_vsi *vsi, bool rem_adv_fltr);\n\nstatic int\nice_indr_setup_tc_cb(struct net_device *netdev, struct Qdisc *sch,\n\t\t     void *cb_priv, enum tc_setup_type type, void *type_data,\n\t\t     void *data,\n\t\t     void (*cleanup)(struct flow_block_cb *block_cb));\n\nbool netif_is_ice(const struct net_device *dev)\n{\n\treturn dev && (dev->netdev_ops == &ice_netdev_ops);\n}\n\n \nstatic u16 ice_get_tx_pending(struct ice_tx_ring *ring)\n{\n\tu16 head, tail;\n\n\thead = ring->next_to_clean;\n\ttail = ring->next_to_use;\n\n\tif (head != tail)\n\t\treturn (head < tail) ?\n\t\t\ttail - head : (tail + ring->count - head);\n\treturn 0;\n}\n\n \nstatic void ice_check_for_hang_subtask(struct ice_pf *pf)\n{\n\tstruct ice_vsi *vsi = NULL;\n\tstruct ice_hw *hw;\n\tunsigned int i;\n\tint packets;\n\tu32 v;\n\n\tice_for_each_vsi(pf, v)\n\t\tif (pf->vsi[v] && pf->vsi[v]->type == ICE_VSI_PF) {\n\t\t\tvsi = pf->vsi[v];\n\t\t\tbreak;\n\t\t}\n\n\tif (!vsi || test_bit(ICE_VSI_DOWN, vsi->state))\n\t\treturn;\n\n\tif (!(vsi->netdev && netif_carrier_ok(vsi->netdev)))\n\t\treturn;\n\n\thw = &vsi->back->hw;\n\n\tice_for_each_txq(vsi, i) {\n\t\tstruct ice_tx_ring *tx_ring = vsi->tx_rings[i];\n\t\tstruct ice_ring_stats *ring_stats;\n\n\t\tif (!tx_ring)\n\t\t\tcontinue;\n\t\tif (ice_ring_ch_enabled(tx_ring))\n\t\t\tcontinue;\n\n\t\tring_stats = tx_ring->ring_stats;\n\t\tif (!ring_stats)\n\t\t\tcontinue;\n\n\t\tif (tx_ring->desc) {\n\t\t\t \n\t\t\tpackets = ring_stats->stats.pkts & INT_MAX;\n\t\t\tif (ring_stats->tx_stats.prev_pkt == packets) {\n\t\t\t\t \n\t\t\t\tice_trigger_sw_intr(hw, tx_ring->q_vector);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tsmp_rmb();\n\t\t\tring_stats->tx_stats.prev_pkt =\n\t\t\t    ice_get_tx_pending(tx_ring) ? packets : -1;\n\t\t}\n\t}\n}\n\n \nstatic int ice_init_mac_fltr(struct ice_pf *pf)\n{\n\tstruct ice_vsi *vsi;\n\tu8 *perm_addr;\n\n\tvsi = ice_get_main_vsi(pf);\n\tif (!vsi)\n\t\treturn -EINVAL;\n\n\tperm_addr = vsi->port_info->mac.perm_addr;\n\treturn ice_fltr_add_mac_and_broadcast(vsi, perm_addr, ICE_FWD_TO_VSI);\n}\n\n \nstatic int ice_add_mac_to_sync_list(struct net_device *netdev, const u8 *addr)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\n\tif (ice_fltr_add_mac_to_list(vsi, &vsi->tmp_sync_list, addr,\n\t\t\t\t     ICE_FWD_TO_VSI))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n \nstatic int ice_add_mac_to_unsync_list(struct net_device *netdev, const u8 *addr)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\n\t \n\tif (ether_addr_equal(addr, netdev->dev_addr))\n\t\treturn 0;\n\n\tif (ice_fltr_add_mac_to_list(vsi, &vsi->tmp_unsync_list, addr,\n\t\t\t\t     ICE_FWD_TO_VSI))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n \nstatic bool ice_vsi_fltr_changed(struct ice_vsi *vsi)\n{\n\treturn test_bit(ICE_VSI_UMAC_FLTR_CHANGED, vsi->state) ||\n\t       test_bit(ICE_VSI_MMAC_FLTR_CHANGED, vsi->state);\n}\n\n \nstatic int ice_set_promisc(struct ice_vsi *vsi, u8 promisc_m)\n{\n\tint status;\n\n\tif (vsi->type != ICE_VSI_PF)\n\t\treturn 0;\n\n\tif (ice_vsi_has_non_zero_vlans(vsi)) {\n\t\tpromisc_m |= (ICE_PROMISC_VLAN_RX | ICE_PROMISC_VLAN_TX);\n\t\tstatus = ice_fltr_set_vlan_vsi_promisc(&vsi->back->hw, vsi,\n\t\t\t\t\t\t       promisc_m);\n\t} else {\n\t\tstatus = ice_fltr_set_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t\t\t  promisc_m, 0);\n\t}\n\tif (status && status != -EEXIST)\n\t\treturn status;\n\n\tnetdev_dbg(vsi->netdev, \"set promisc filter bits for VSI %i: 0x%x\\n\",\n\t\t   vsi->vsi_num, promisc_m);\n\treturn 0;\n}\n\n \nstatic int ice_clear_promisc(struct ice_vsi *vsi, u8 promisc_m)\n{\n\tint status;\n\n\tif (vsi->type != ICE_VSI_PF)\n\t\treturn 0;\n\n\tif (ice_vsi_has_non_zero_vlans(vsi)) {\n\t\tpromisc_m |= (ICE_PROMISC_VLAN_RX | ICE_PROMISC_VLAN_TX);\n\t\tstatus = ice_fltr_clear_vlan_vsi_promisc(&vsi->back->hw, vsi,\n\t\t\t\t\t\t\t promisc_m);\n\t} else {\n\t\tstatus = ice_fltr_clear_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t\t\t    promisc_m, 0);\n\t}\n\n\tnetdev_dbg(vsi->netdev, \"clear promisc filter bits for VSI %i: 0x%x\\n\",\n\t\t   vsi->vsi_num, promisc_m);\n\treturn status;\n}\n\n \nstatic int ice_vsi_sync_fltr(struct ice_vsi *vsi)\n{\n\tstruct ice_vsi_vlan_ops *vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);\n\tstruct device *dev = ice_pf_to_dev(vsi->back);\n\tstruct net_device *netdev = vsi->netdev;\n\tbool promisc_forced_on = false;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tu32 changed_flags = 0;\n\tint err;\n\n\tif (!vsi->netdev)\n\t\treturn -EINVAL;\n\n\twhile (test_and_set_bit(ICE_CFG_BUSY, vsi->state))\n\t\tusleep_range(1000, 2000);\n\n\tchanged_flags = vsi->current_netdev_flags ^ vsi->netdev->flags;\n\tvsi->current_netdev_flags = vsi->netdev->flags;\n\n\tINIT_LIST_HEAD(&vsi->tmp_sync_list);\n\tINIT_LIST_HEAD(&vsi->tmp_unsync_list);\n\n\tif (ice_vsi_fltr_changed(vsi)) {\n\t\tclear_bit(ICE_VSI_UMAC_FLTR_CHANGED, vsi->state);\n\t\tclear_bit(ICE_VSI_MMAC_FLTR_CHANGED, vsi->state);\n\n\t\t \n\t\tnetif_addr_lock_bh(netdev);\n\t\t__dev_uc_sync(netdev, ice_add_mac_to_sync_list,\n\t\t\t      ice_add_mac_to_unsync_list);\n\t\t__dev_mc_sync(netdev, ice_add_mac_to_sync_list,\n\t\t\t      ice_add_mac_to_unsync_list);\n\t\t \n\t\tnetif_addr_unlock_bh(netdev);\n\t}\n\n\t \n\terr = ice_fltr_remove_mac_list(vsi, &vsi->tmp_unsync_list);\n\tice_fltr_free_list(dev, &vsi->tmp_unsync_list);\n\tif (err) {\n\t\tnetdev_err(netdev, \"Failed to delete MAC filters\\n\");\n\t\t \n\t\tif (err == -ENOMEM)\n\t\t\tgoto out;\n\t}\n\n\t \n\terr = ice_fltr_add_mac_list(vsi, &vsi->tmp_sync_list);\n\tice_fltr_free_list(dev, &vsi->tmp_sync_list);\n\t \n\tif (err && err != -EEXIST) {\n\t\tnetdev_err(netdev, \"Failed to add MAC filters\\n\");\n\t\t \n\t\tif (hw->adminq.sq_last_status == ICE_AQ_RC_ENOSPC &&\n\t\t    !test_and_set_bit(ICE_FLTR_OVERFLOW_PROMISC,\n\t\t\t\t      vsi->state)) {\n\t\t\tpromisc_forced_on = true;\n\t\t\tnetdev_warn(netdev, \"Reached MAC filter limit, forcing promisc mode on VSI %d\\n\",\n\t\t\t\t    vsi->vsi_num);\n\t\t} else {\n\t\t\tgoto out;\n\t\t}\n\t}\n\terr = 0;\n\t \n\tif (changed_flags & IFF_ALLMULTI) {\n\t\tif (vsi->current_netdev_flags & IFF_ALLMULTI) {\n\t\t\terr = ice_set_promisc(vsi, ICE_MCAST_PROMISC_BITS);\n\t\t\tif (err) {\n\t\t\t\tvsi->current_netdev_flags &= ~IFF_ALLMULTI;\n\t\t\t\tgoto out_promisc;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\terr = ice_clear_promisc(vsi, ICE_MCAST_PROMISC_BITS);\n\t\t\tif (err) {\n\t\t\t\tvsi->current_netdev_flags |= IFF_ALLMULTI;\n\t\t\t\tgoto out_promisc;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (((changed_flags & IFF_PROMISC) || promisc_forced_on) ||\n\t    test_bit(ICE_VSI_PROMISC_CHANGED, vsi->state)) {\n\t\tclear_bit(ICE_VSI_PROMISC_CHANGED, vsi->state);\n\t\tif (vsi->current_netdev_flags & IFF_PROMISC) {\n\t\t\t \n\t\t\tif (!ice_is_dflt_vsi_in_use(vsi->port_info)) {\n\t\t\t\terr = ice_set_dflt_vsi(vsi);\n\t\t\t\tif (err && err != -EEXIST) {\n\t\t\t\t\tnetdev_err(netdev, \"Error %d setting default VSI %i Rx rule\\n\",\n\t\t\t\t\t\t   err, vsi->vsi_num);\n\t\t\t\t\tvsi->current_netdev_flags &=\n\t\t\t\t\t\t~IFF_PROMISC;\n\t\t\t\t\tgoto out_promisc;\n\t\t\t\t}\n\t\t\t\terr = 0;\n\t\t\t\tvlan_ops->dis_rx_filtering(vsi);\n\n\t\t\t\t \n\t\t\t\terr = ice_set_promisc(vsi,\n\t\t\t\t\t\t      ICE_MCAST_PROMISC_BITS);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_promisc;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (ice_is_vsi_dflt_vsi(vsi)) {\n\t\t\t\terr = ice_clear_dflt_vsi(vsi);\n\t\t\t\tif (err) {\n\t\t\t\t\tnetdev_err(netdev, \"Error %d clearing default VSI %i Rx rule\\n\",\n\t\t\t\t\t\t   err, vsi->vsi_num);\n\t\t\t\t\tvsi->current_netdev_flags |=\n\t\t\t\t\t\tIFF_PROMISC;\n\t\t\t\t\tgoto out_promisc;\n\t\t\t\t}\n\t\t\t\tif (vsi->netdev->features &\n\t\t\t\t    NETIF_F_HW_VLAN_CTAG_FILTER)\n\t\t\t\t\tvlan_ops->ena_rx_filtering(vsi);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!(vsi->current_netdev_flags & IFF_ALLMULTI)) {\n\t\t\t\terr = ice_clear_promisc(vsi,\n\t\t\t\t\t\t\tICE_MCAST_PROMISC_BITS);\n\t\t\t\tif (err) {\n\t\t\t\t\tnetdev_err(netdev, \"Error %d clearing multicast promiscuous on VSI %i\\n\",\n\t\t\t\t\t\t   err, vsi->vsi_num);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tgoto exit;\n\nout_promisc:\n\tset_bit(ICE_VSI_PROMISC_CHANGED, vsi->state);\n\tgoto exit;\nout:\n\t \n\tset_bit(ICE_VSI_UMAC_FLTR_CHANGED, vsi->state);\n\tset_bit(ICE_VSI_MMAC_FLTR_CHANGED, vsi->state);\nexit:\n\tclear_bit(ICE_CFG_BUSY, vsi->state);\n\treturn err;\n}\n\n \nstatic void ice_sync_fltr_subtask(struct ice_pf *pf)\n{\n\tint v;\n\n\tif (!pf || !(test_bit(ICE_FLAG_FLTR_SYNC, pf->flags)))\n\t\treturn;\n\n\tclear_bit(ICE_FLAG_FLTR_SYNC, pf->flags);\n\n\tice_for_each_vsi(pf, v)\n\t\tif (pf->vsi[v] && ice_vsi_fltr_changed(pf->vsi[v]) &&\n\t\t    ice_vsi_sync_fltr(pf->vsi[v])) {\n\t\t\t \n\t\t\tset_bit(ICE_FLAG_FLTR_SYNC, pf->flags);\n\t\t\tbreak;\n\t\t}\n}\n\n \nstatic void ice_pf_dis_all_vsi(struct ice_pf *pf, bool locked)\n{\n\tint node;\n\tint v;\n\n\tice_for_each_vsi(pf, v)\n\t\tif (pf->vsi[v])\n\t\t\tice_dis_vsi(pf->vsi[v], locked);\n\n\tfor (node = 0; node < ICE_MAX_PF_AGG_NODES; node++)\n\t\tpf->pf_agg_node[node].num_vsis = 0;\n\n\tfor (node = 0; node < ICE_MAX_VF_AGG_NODES; node++)\n\t\tpf->vf_agg_node[node].num_vsis = 0;\n}\n\n \nstatic void ice_clear_sw_switch_recipes(struct ice_pf *pf)\n{\n\tstruct ice_sw_recipe *recp;\n\tu8 i;\n\n\trecp = pf->hw.switch_info->recp_list;\n\tfor (i = 0; i < ICE_MAX_NUM_RECIPES; i++)\n\t\trecp[i].recp_created = false;\n}\n\n \nstatic void\nice_prepare_for_reset(struct ice_pf *pf, enum ice_reset_req reset_type)\n{\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct ice_vsi *vsi;\n\tstruct ice_vf *vf;\n\tunsigned int bkt;\n\n\tdev_dbg(ice_pf_to_dev(pf), \"reset_type=%d\\n\", reset_type);\n\n\t \n\tif (test_bit(ICE_PREPARED_FOR_RESET, pf->state))\n\t\treturn;\n\n\tice_unplug_aux_dev(pf);\n\n\t \n\tif (ice_check_sq_alive(hw, &hw->mailboxq))\n\t\tice_vc_notify_reset(pf);\n\n\t \n\tmutex_lock(&pf->vfs.table_lock);\n\tice_for_each_vf(pf, bkt, vf)\n\t\tice_set_vf_state_dis(vf);\n\tmutex_unlock(&pf->vfs.table_lock);\n\n\tif (ice_is_eswitch_mode_switchdev(pf)) {\n\t\tif (reset_type != ICE_RESET_PFR)\n\t\t\tice_clear_sw_switch_recipes(pf);\n\t}\n\n\t \n\tvsi = ice_get_main_vsi(pf);\n\tif (!vsi)\n\t\tgoto skip;\n\n\t \n\tvsi->orig_rss_size = 0;\n\n\tif (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {\n\t\tif (reset_type == ICE_RESET_PFR) {\n\t\t\tvsi->old_ena_tc = vsi->all_enatc;\n\t\t\tvsi->old_numtc = vsi->all_numtc;\n\t\t} else {\n\t\t\tice_remove_q_channels(vsi, true);\n\n\t\t\t \n\t\t\tvsi->old_ena_tc = 0;\n\t\t\tvsi->all_enatc = 0;\n\t\t\tvsi->old_numtc = 0;\n\t\t\tvsi->all_numtc = 0;\n\t\t\tvsi->req_txq = 0;\n\t\t\tvsi->req_rxq = 0;\n\t\t\tclear_bit(ICE_FLAG_TC_MQPRIO, pf->flags);\n\t\t\tmemset(&vsi->mqprio_qopt, 0, sizeof(vsi->mqprio_qopt));\n\t\t}\n\t}\nskip:\n\n\t \n\tice_clear_hw_tbls(hw);\n\t \n\tice_pf_dis_all_vsi(pf, false);\n\n\tif (test_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags))\n\t\tice_ptp_prepare_for_reset(pf);\n\n\tif (ice_is_feature_supported(pf, ICE_F_GNSS))\n\t\tice_gnss_exit(pf);\n\n\tif (hw->port_info)\n\t\tice_sched_clear_port(hw->port_info);\n\n\tice_shutdown_all_ctrlq(hw);\n\n\tset_bit(ICE_PREPARED_FOR_RESET, pf->state);\n}\n\n \nstatic void ice_do_reset(struct ice_pf *pf, enum ice_reset_req reset_type)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_hw *hw = &pf->hw;\n\n\tdev_dbg(dev, \"reset_type 0x%x requested\\n\", reset_type);\n\n\tif (pf->lag && pf->lag->bonded && reset_type == ICE_RESET_PFR) {\n\t\tdev_dbg(dev, \"PFR on a bonded interface, promoting to CORER\\n\");\n\t\treset_type = ICE_RESET_CORER;\n\t}\n\n\tice_prepare_for_reset(pf, reset_type);\n\n\t \n\tif (ice_reset(hw, reset_type)) {\n\t\tdev_err(dev, \"reset %d failed\\n\", reset_type);\n\t\tset_bit(ICE_RESET_FAILED, pf->state);\n\t\tclear_bit(ICE_RESET_OICR_RECV, pf->state);\n\t\tclear_bit(ICE_PREPARED_FOR_RESET, pf->state);\n\t\tclear_bit(ICE_PFR_REQ, pf->state);\n\t\tclear_bit(ICE_CORER_REQ, pf->state);\n\t\tclear_bit(ICE_GLOBR_REQ, pf->state);\n\t\twake_up(&pf->reset_wait_queue);\n\t\treturn;\n\t}\n\n\t \n\tif (reset_type == ICE_RESET_PFR) {\n\t\tpf->pfr_count++;\n\t\tice_rebuild(pf, reset_type);\n\t\tclear_bit(ICE_PREPARED_FOR_RESET, pf->state);\n\t\tclear_bit(ICE_PFR_REQ, pf->state);\n\t\twake_up(&pf->reset_wait_queue);\n\t\tice_reset_all_vfs(pf);\n\t}\n}\n\n \nstatic void ice_reset_subtask(struct ice_pf *pf)\n{\n\tenum ice_reset_req reset_type = ICE_RESET_INVAL;\n\n\t \n\tif (test_bit(ICE_RESET_OICR_RECV, pf->state)) {\n\t\t \n\t\tif (test_and_clear_bit(ICE_CORER_RECV, pf->state))\n\t\t\treset_type = ICE_RESET_CORER;\n\t\tif (test_and_clear_bit(ICE_GLOBR_RECV, pf->state))\n\t\t\treset_type = ICE_RESET_GLOBR;\n\t\tif (test_and_clear_bit(ICE_EMPR_RECV, pf->state))\n\t\t\treset_type = ICE_RESET_EMPR;\n\t\t \n\t\tif (reset_type == ICE_RESET_INVAL)\n\t\t\treturn;\n\t\tice_prepare_for_reset(pf, reset_type);\n\n\t\t \n\t\tif (ice_check_reset(&pf->hw)) {\n\t\t\tset_bit(ICE_RESET_FAILED, pf->state);\n\t\t} else {\n\t\t\t \n\t\t\tpf->hw.reset_ongoing = false;\n\t\t\tice_rebuild(pf, reset_type);\n\t\t\t \n\t\t\tclear_bit(ICE_RESET_OICR_RECV, pf->state);\n\t\t\tclear_bit(ICE_PREPARED_FOR_RESET, pf->state);\n\t\t\tclear_bit(ICE_PFR_REQ, pf->state);\n\t\t\tclear_bit(ICE_CORER_REQ, pf->state);\n\t\t\tclear_bit(ICE_GLOBR_REQ, pf->state);\n\t\t\twake_up(&pf->reset_wait_queue);\n\t\t\tice_reset_all_vfs(pf);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t \n\tif (test_bit(ICE_PFR_REQ, pf->state)) {\n\t\treset_type = ICE_RESET_PFR;\n\t\tif (pf->lag && pf->lag->bonded) {\n\t\t\tdev_dbg(ice_pf_to_dev(pf), \"PFR on a bonded interface, promoting to CORER\\n\");\n\t\t\treset_type = ICE_RESET_CORER;\n\t\t}\n\t}\n\tif (test_bit(ICE_CORER_REQ, pf->state))\n\t\treset_type = ICE_RESET_CORER;\n\tif (test_bit(ICE_GLOBR_REQ, pf->state))\n\t\treset_type = ICE_RESET_GLOBR;\n\t \n\tif (reset_type == ICE_RESET_INVAL)\n\t\treturn;\n\n\t \n\tif (!test_bit(ICE_DOWN, pf->state) &&\n\t    !test_bit(ICE_CFG_BUSY, pf->state)) {\n\t\tice_do_reset(pf, reset_type);\n\t}\n}\n\n \nstatic void ice_print_topo_conflict(struct ice_vsi *vsi)\n{\n\tswitch (vsi->port_info->phy.link_info.topo_media_conflict) {\n\tcase ICE_AQ_LINK_TOPO_CONFLICT:\n\tcase ICE_AQ_LINK_MEDIA_CONFLICT:\n\tcase ICE_AQ_LINK_TOPO_UNREACH_PRT:\n\tcase ICE_AQ_LINK_TOPO_UNDRUTIL_PRT:\n\tcase ICE_AQ_LINK_TOPO_UNDRUTIL_MEDIA:\n\t\tnetdev_info(vsi->netdev, \"Potential misconfiguration of the Ethernet port detected. If it was not intended, please use the Intel (R) Ethernet Port Configuration Tool to address the issue.\\n\");\n\t\tbreak;\n\tcase ICE_AQ_LINK_TOPO_UNSUPP_MEDIA:\n\t\tif (test_bit(ICE_FLAG_LINK_LENIENT_MODE_ENA, vsi->back->flags))\n\t\t\tnetdev_warn(vsi->netdev, \"An unsupported module type was detected. Refer to the Intel(R) Ethernet Adapters and Devices User Guide for a list of supported modules\\n\");\n\t\telse\n\t\t\tnetdev_err(vsi->netdev, \"Rx/Tx is disabled on this device because an unsupported module type was detected. Refer to the Intel(R) Ethernet Adapters and Devices User Guide for a list of supported modules.\\n\");\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n \nvoid ice_print_link_msg(struct ice_vsi *vsi, bool isup)\n{\n\tstruct ice_aqc_get_phy_caps_data *caps;\n\tconst char *an_advertised;\n\tconst char *fec_req;\n\tconst char *speed;\n\tconst char *fec;\n\tconst char *fc;\n\tconst char *an;\n\tint status;\n\n\tif (!vsi)\n\t\treturn;\n\n\tif (vsi->current_isup == isup)\n\t\treturn;\n\n\tvsi->current_isup = isup;\n\n\tif (!isup) {\n\t\tnetdev_info(vsi->netdev, \"NIC Link is Down\\n\");\n\t\treturn;\n\t}\n\n\tswitch (vsi->port_info->phy.link_info.link_speed) {\n\tcase ICE_AQ_LINK_SPEED_100GB:\n\t\tspeed = \"100 G\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_SPEED_50GB:\n\t\tspeed = \"50 G\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_SPEED_40GB:\n\t\tspeed = \"40 G\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_SPEED_25GB:\n\t\tspeed = \"25 G\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_SPEED_20GB:\n\t\tspeed = \"20 G\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_SPEED_10GB:\n\t\tspeed = \"10 G\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_SPEED_5GB:\n\t\tspeed = \"5 G\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_SPEED_2500MB:\n\t\tspeed = \"2.5 G\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_SPEED_1000MB:\n\t\tspeed = \"1 G\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_SPEED_100MB:\n\t\tspeed = \"100 M\";\n\t\tbreak;\n\tdefault:\n\t\tspeed = \"Unknown \";\n\t\tbreak;\n\t}\n\n\tswitch (vsi->port_info->fc.current_mode) {\n\tcase ICE_FC_FULL:\n\t\tfc = \"Rx/Tx\";\n\t\tbreak;\n\tcase ICE_FC_TX_PAUSE:\n\t\tfc = \"Tx\";\n\t\tbreak;\n\tcase ICE_FC_RX_PAUSE:\n\t\tfc = \"Rx\";\n\t\tbreak;\n\tcase ICE_FC_NONE:\n\t\tfc = \"None\";\n\t\tbreak;\n\tdefault:\n\t\tfc = \"Unknown\";\n\t\tbreak;\n\t}\n\n\t \n\tswitch (vsi->port_info->phy.link_info.fec_info) {\n\tcase ICE_AQ_LINK_25G_RS_528_FEC_EN:\n\tcase ICE_AQ_LINK_25G_RS_544_FEC_EN:\n\t\tfec = \"RS-FEC\";\n\t\tbreak;\n\tcase ICE_AQ_LINK_25G_KR_FEC_EN:\n\t\tfec = \"FC-FEC/BASE-R\";\n\t\tbreak;\n\tdefault:\n\t\tfec = \"NONE\";\n\t\tbreak;\n\t}\n\n\t \n\tif (vsi->port_info->phy.link_info.an_info & ICE_AQ_AN_COMPLETED)\n\t\tan = \"True\";\n\telse\n\t\tan = \"False\";\n\n\t \n\tcaps = kzalloc(sizeof(*caps), GFP_KERNEL);\n\tif (!caps) {\n\t\tfec_req = \"Unknown\";\n\t\tan_advertised = \"Unknown\";\n\t\tgoto done;\n\t}\n\n\tstatus = ice_aq_get_phy_caps(vsi->port_info, false,\n\t\t\t\t     ICE_AQC_REPORT_ACTIVE_CFG, caps, NULL);\n\tif (status)\n\t\tnetdev_info(vsi->netdev, \"Get phy capability failed.\\n\");\n\n\tan_advertised = ice_is_phy_caps_an_enabled(caps) ? \"On\" : \"Off\";\n\n\tif (caps->link_fec_options & ICE_AQC_PHY_FEC_25G_RS_528_REQ ||\n\t    caps->link_fec_options & ICE_AQC_PHY_FEC_25G_RS_544_REQ)\n\t\tfec_req = \"RS-FEC\";\n\telse if (caps->link_fec_options & ICE_AQC_PHY_FEC_10G_KR_40G_KR4_REQ ||\n\t\t caps->link_fec_options & ICE_AQC_PHY_FEC_25G_KR_REQ)\n\t\tfec_req = \"FC-FEC/BASE-R\";\n\telse\n\t\tfec_req = \"NONE\";\n\n\tkfree(caps);\n\ndone:\n\tnetdev_info(vsi->netdev, \"NIC Link is up %sbps Full Duplex, Requested FEC: %s, Negotiated FEC: %s, Autoneg Advertised: %s, Autoneg Negotiated: %s, Flow Control: %s\\n\",\n\t\t    speed, fec_req, fec, an_advertised, an, fc);\n\tice_print_topo_conflict(vsi);\n}\n\n \nstatic void ice_vsi_link_event(struct ice_vsi *vsi, bool link_up)\n{\n\tif (!vsi)\n\t\treturn;\n\n\tif (test_bit(ICE_VSI_DOWN, vsi->state) || !vsi->netdev)\n\t\treturn;\n\n\tif (vsi->type == ICE_VSI_PF) {\n\t\tif (link_up == netif_carrier_ok(vsi->netdev))\n\t\t\treturn;\n\n\t\tif (link_up) {\n\t\t\tnetif_carrier_on(vsi->netdev);\n\t\t\tnetif_tx_wake_all_queues(vsi->netdev);\n\t\t} else {\n\t\t\tnetif_carrier_off(vsi->netdev);\n\t\t\tnetif_tx_stop_all_queues(vsi->netdev);\n\t\t}\n\t}\n}\n\n \nstatic void ice_set_dflt_mib(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tu8 mib_type, *buf, *lldpmib = NULL;\n\tu16 len, typelen, offset = 0;\n\tstruct ice_lldp_org_tlv *tlv;\n\tstruct ice_hw *hw = &pf->hw;\n\tu32 ouisubtype;\n\n\tmib_type = SET_LOCAL_MIB_TYPE_LOCAL_MIB;\n\tlldpmib = kzalloc(ICE_LLDPDU_SIZE, GFP_KERNEL);\n\tif (!lldpmib) {\n\t\tdev_dbg(dev, \"%s Failed to allocate MIB memory\\n\",\n\t\t\t__func__);\n\t\treturn;\n\t}\n\n\t \n\ttlv = (struct ice_lldp_org_tlv *)lldpmib;\n\ttypelen = ((ICE_TLV_TYPE_ORG << ICE_LLDP_TLV_TYPE_S) |\n\t\t   ICE_IEEE_ETS_TLV_LEN);\n\ttlv->typelen = htons(typelen);\n\touisubtype = ((ICE_IEEE_8021QAZ_OUI << ICE_LLDP_TLV_OUI_S) |\n\t\t      ICE_IEEE_SUBTYPE_ETS_CFG);\n\ttlv->ouisubtype = htonl(ouisubtype);\n\n\tbuf = tlv->tlvinfo;\n\tbuf[0] = 0;\n\n\t \n\tbuf[5] = 0x64;\n\tlen = (typelen & ICE_LLDP_TLV_LEN_M) >> ICE_LLDP_TLV_LEN_S;\n\toffset += len + 2;\n\ttlv = (struct ice_lldp_org_tlv *)\n\t\t((char *)tlv + sizeof(tlv->typelen) + len);\n\n\t \n\tbuf = tlv->tlvinfo;\n\ttlv->typelen = htons(typelen);\n\n\touisubtype = ((ICE_IEEE_8021QAZ_OUI << ICE_LLDP_TLV_OUI_S) |\n\t\t      ICE_IEEE_SUBTYPE_ETS_REC);\n\ttlv->ouisubtype = htonl(ouisubtype);\n\n\t \n\tbuf[5] = 0x64;\n\toffset += len + 2;\n\ttlv = (struct ice_lldp_org_tlv *)\n\t\t((char *)tlv + sizeof(tlv->typelen) + len);\n\n\t \n\ttypelen = ((ICE_TLV_TYPE_ORG << ICE_LLDP_TLV_TYPE_S) |\n\t\t   ICE_IEEE_PFC_TLV_LEN);\n\ttlv->typelen = htons(typelen);\n\n\touisubtype = ((ICE_IEEE_8021QAZ_OUI << ICE_LLDP_TLV_OUI_S) |\n\t\t      ICE_IEEE_SUBTYPE_PFC_CFG);\n\ttlv->ouisubtype = htonl(ouisubtype);\n\n\t \n\tbuf[0] = 0x08;\n\tlen = (typelen & ICE_LLDP_TLV_LEN_M) >> ICE_LLDP_TLV_LEN_S;\n\toffset += len + 2;\n\n\tif (ice_aq_set_lldp_mib(hw, mib_type, (void *)lldpmib, offset, NULL))\n\t\tdev_dbg(dev, \"%s Failed to set default LLDP MIB\\n\", __func__);\n\n\tkfree(lldpmib);\n}\n\n \nstatic void ice_check_phy_fw_load(struct ice_pf *pf, u8 link_cfg_err)\n{\n\tif (!(link_cfg_err & ICE_AQ_LINK_EXTERNAL_PHY_LOAD_FAILURE)) {\n\t\tclear_bit(ICE_FLAG_PHY_FW_LOAD_FAILED, pf->flags);\n\t\treturn;\n\t}\n\n\tif (test_bit(ICE_FLAG_PHY_FW_LOAD_FAILED, pf->flags))\n\t\treturn;\n\n\tif (link_cfg_err & ICE_AQ_LINK_EXTERNAL_PHY_LOAD_FAILURE) {\n\t\tdev_err(ice_pf_to_dev(pf), \"Device failed to load the FW for the external PHY. Please download and install the latest NVM for your device and try again\\n\");\n\t\tset_bit(ICE_FLAG_PHY_FW_LOAD_FAILED, pf->flags);\n\t}\n}\n\n \nstatic void ice_check_module_power(struct ice_pf *pf, u8 link_cfg_err)\n{\n\t \n\tif (!(link_cfg_err & (ICE_AQ_LINK_INVAL_MAX_POWER_LIMIT |\n\t\t\t      ICE_AQ_LINK_MODULE_POWER_UNSUPPORTED))) {\n\t\tclear_bit(ICE_FLAG_MOD_POWER_UNSUPPORTED, pf->flags);\n\t\treturn;\n\t}\n\n\t \n\tif (test_bit(ICE_FLAG_MOD_POWER_UNSUPPORTED, pf->flags))\n\t\treturn;\n\n\tif (link_cfg_err & ICE_AQ_LINK_INVAL_MAX_POWER_LIMIT) {\n\t\tdev_err(ice_pf_to_dev(pf), \"The installed module is incompatible with the device's NVM image. Cannot start link\\n\");\n\t\tset_bit(ICE_FLAG_MOD_POWER_UNSUPPORTED, pf->flags);\n\t} else if (link_cfg_err & ICE_AQ_LINK_MODULE_POWER_UNSUPPORTED) {\n\t\tdev_err(ice_pf_to_dev(pf), \"The module's power requirements exceed the device's power supply. Cannot start link\\n\");\n\t\tset_bit(ICE_FLAG_MOD_POWER_UNSUPPORTED, pf->flags);\n\t}\n}\n\n \nstatic void ice_check_link_cfg_err(struct ice_pf *pf, u8 link_cfg_err)\n{\n\tice_check_module_power(pf, link_cfg_err);\n\tice_check_phy_fw_load(pf, link_cfg_err);\n}\n\n \nstatic int\nice_link_event(struct ice_pf *pf, struct ice_port_info *pi, bool link_up,\n\t       u16 link_speed)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_phy_info *phy_info;\n\tstruct ice_vsi *vsi;\n\tu16 old_link_speed;\n\tbool old_link;\n\tint status;\n\n\tphy_info = &pi->phy;\n\tphy_info->link_info_old = phy_info->link_info;\n\n\told_link = !!(phy_info->link_info_old.link_info & ICE_AQ_LINK_UP);\n\told_link_speed = phy_info->link_info_old.link_speed;\n\n\t \n\tstatus = ice_update_link_info(pi);\n\tif (status)\n\t\tdev_dbg(dev, \"Failed to update link status on port %d, err %d aq_err %s\\n\",\n\t\t\tpi->lport, status,\n\t\t\tice_aq_str(pi->hw->adminq.sq_last_status));\n\n\tice_check_link_cfg_err(pf, pi->phy.link_info.link_cfg_err);\n\n\t \n\tif (phy_info->link_info.link_info & ICE_AQ_LINK_UP)\n\t\tlink_up = true;\n\n\tvsi = ice_get_main_vsi(pf);\n\tif (!vsi || !vsi->port_info)\n\t\treturn -EINVAL;\n\n\t \n\tif (!test_bit(ICE_FLAG_NO_MEDIA, pf->flags) &&\n\t    !(pi->phy.link_info.link_info & ICE_AQ_MEDIA_AVAILABLE)) {\n\t\tset_bit(ICE_FLAG_NO_MEDIA, pf->flags);\n\t\tice_set_link(vsi, false);\n\t}\n\n\t \n\tif (link_up == old_link && link_speed == old_link_speed)\n\t\treturn 0;\n\n\tice_ptp_link_change(pf, pf->hw.pf_id, link_up);\n\n\tif (ice_is_dcb_active(pf)) {\n\t\tif (test_bit(ICE_FLAG_DCB_ENA, pf->flags))\n\t\t\tice_dcb_rebuild(pf);\n\t} else {\n\t\tif (link_up)\n\t\t\tice_set_dflt_mib(pf);\n\t}\n\tice_vsi_link_event(vsi, link_up);\n\tice_print_link_msg(vsi, link_up);\n\n\tice_vc_notify_link_state(pf);\n\n\treturn 0;\n}\n\n \nstatic void ice_watchdog_subtask(struct ice_pf *pf)\n{\n\tint i;\n\n\t \n\tif (test_bit(ICE_DOWN, pf->state) ||\n\t    test_bit(ICE_CFG_BUSY, pf->state))\n\t\treturn;\n\n\t \n\tif (time_before(jiffies,\n\t\t\tpf->serv_tmr_prev + pf->serv_tmr_period))\n\t\treturn;\n\n\tpf->serv_tmr_prev = jiffies;\n\n\t \n\tice_update_pf_stats(pf);\n\tice_for_each_vsi(pf, i)\n\t\tif (pf->vsi[i] && pf->vsi[i]->netdev)\n\t\t\tice_update_vsi_stats(pf->vsi[i]);\n}\n\n \nstatic int ice_init_link_events(struct ice_port_info *pi)\n{\n\tu16 mask;\n\n\tmask = ~((u16)(ICE_AQ_LINK_EVENT_UPDOWN | ICE_AQ_LINK_EVENT_MEDIA_NA |\n\t\t       ICE_AQ_LINK_EVENT_MODULE_QUAL_FAIL |\n\t\t       ICE_AQ_LINK_EVENT_PHY_FW_LOAD_FAIL));\n\n\tif (ice_aq_set_event_mask(pi->hw, pi->lport, mask, NULL)) {\n\t\tdev_dbg(ice_hw_to_dev(pi->hw), \"Failed to set link event mask for port %d\\n\",\n\t\t\tpi->lport);\n\t\treturn -EIO;\n\t}\n\n\tif (ice_aq_get_link_info(pi, true, NULL, NULL)) {\n\t\tdev_dbg(ice_hw_to_dev(pi->hw), \"Failed to enable link events for port %d\\n\",\n\t\t\tpi->lport);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\nice_handle_link_event(struct ice_pf *pf, struct ice_rq_event_info *event)\n{\n\tstruct ice_aqc_get_link_status_data *link_data;\n\tstruct ice_port_info *port_info;\n\tint status;\n\n\tlink_data = (struct ice_aqc_get_link_status_data *)event->msg_buf;\n\tport_info = pf->hw.port_info;\n\tif (!port_info)\n\t\treturn -EINVAL;\n\n\tstatus = ice_link_event(pf, port_info,\n\t\t\t\t!!(link_data->link_info & ICE_AQ_LINK_UP),\n\t\t\t\tle16_to_cpu(link_data->link_speed));\n\tif (status)\n\t\tdev_dbg(ice_pf_to_dev(pf), \"Could not process link event, error %d\\n\",\n\t\t\tstatus);\n\n\treturn status;\n}\n\n \nvoid ice_aq_prep_for_event(struct ice_pf *pf, struct ice_aq_task *task,\n\t\t\t   u16 opcode)\n{\n\tINIT_HLIST_NODE(&task->entry);\n\ttask->opcode = opcode;\n\ttask->state = ICE_AQ_TASK_WAITING;\n\n\tspin_lock_bh(&pf->aq_wait_lock);\n\thlist_add_head(&task->entry, &pf->aq_wait_list);\n\tspin_unlock_bh(&pf->aq_wait_lock);\n}\n\n \nint ice_aq_wait_for_event(struct ice_pf *pf, struct ice_aq_task *task,\n\t\t\t  unsigned long timeout)\n{\n\tenum ice_aq_task_state *state = &task->state;\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tunsigned long start = jiffies;\n\tlong ret;\n\tint err;\n\n\tret = wait_event_interruptible_timeout(pf->aq_wait_queue,\n\t\t\t\t\t       *state != ICE_AQ_TASK_WAITING,\n\t\t\t\t\t       timeout);\n\tswitch (*state) {\n\tcase ICE_AQ_TASK_NOT_PREPARED:\n\t\tWARN(1, \"call to %s without ice_aq_prep_for_event()\", __func__);\n\t\terr = -EINVAL;\n\t\tbreak;\n\tcase ICE_AQ_TASK_WAITING:\n\t\terr = ret < 0 ? ret : -ETIMEDOUT;\n\t\tbreak;\n\tcase ICE_AQ_TASK_CANCELED:\n\t\terr = ret < 0 ? ret : -ECANCELED;\n\t\tbreak;\n\tcase ICE_AQ_TASK_COMPLETE:\n\t\terr = ret < 0 ? ret : 0;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"Unexpected AdminQ wait task state %u\", *state);\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\n\tdev_dbg(dev, \"Waited %u msecs (max %u msecs) for firmware response to op 0x%04x\\n\",\n\t\tjiffies_to_msecs(jiffies - start),\n\t\tjiffies_to_msecs(timeout),\n\t\ttask->opcode);\n\n\tspin_lock_bh(&pf->aq_wait_lock);\n\thlist_del(&task->entry);\n\tspin_unlock_bh(&pf->aq_wait_lock);\n\n\treturn err;\n}\n\n \nstatic void ice_aq_check_events(struct ice_pf *pf, u16 opcode,\n\t\t\t\tstruct ice_rq_event_info *event)\n{\n\tstruct ice_rq_event_info *task_ev;\n\tstruct ice_aq_task *task;\n\tbool found = false;\n\n\tspin_lock_bh(&pf->aq_wait_lock);\n\thlist_for_each_entry(task, &pf->aq_wait_list, entry) {\n\t\tif (task->state != ICE_AQ_TASK_WAITING)\n\t\t\tcontinue;\n\t\tif (task->opcode != opcode)\n\t\t\tcontinue;\n\n\t\ttask_ev = &task->event;\n\t\tmemcpy(&task_ev->desc, &event->desc, sizeof(event->desc));\n\t\ttask_ev->msg_len = event->msg_len;\n\n\t\t \n\t\tif (task_ev->msg_buf && task_ev->buf_len >= event->buf_len) {\n\t\t\tmemcpy(task_ev->msg_buf, event->msg_buf,\n\t\t\t       event->buf_len);\n\t\t\ttask_ev->buf_len = event->buf_len;\n\t\t}\n\n\t\ttask->state = ICE_AQ_TASK_COMPLETE;\n\t\tfound = true;\n\t}\n\tspin_unlock_bh(&pf->aq_wait_lock);\n\n\tif (found)\n\t\twake_up(&pf->aq_wait_queue);\n}\n\n \nstatic void ice_aq_cancel_waiting_tasks(struct ice_pf *pf)\n{\n\tstruct ice_aq_task *task;\n\n\tspin_lock_bh(&pf->aq_wait_lock);\n\thlist_for_each_entry(task, &pf->aq_wait_list, entry)\n\t\ttask->state = ICE_AQ_TASK_CANCELED;\n\tspin_unlock_bh(&pf->aq_wait_lock);\n\n\twake_up(&pf->aq_wait_queue);\n}\n\n#define ICE_MBX_OVERFLOW_WATERMARK 64\n\n \nstatic int __ice_clean_ctrlq(struct ice_pf *pf, enum ice_ctl_q q_type)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_rq_event_info event;\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct ice_ctl_q_info *cq;\n\tu16 pending, i = 0;\n\tconst char *qtype;\n\tu32 oldval, val;\n\n\t \n\tif (test_bit(ICE_RESET_FAILED, pf->state))\n\t\treturn 0;\n\n\tswitch (q_type) {\n\tcase ICE_CTL_Q_ADMIN:\n\t\tcq = &hw->adminq;\n\t\tqtype = \"Admin\";\n\t\tbreak;\n\tcase ICE_CTL_Q_SB:\n\t\tcq = &hw->sbq;\n\t\tqtype = \"Sideband\";\n\t\tbreak;\n\tcase ICE_CTL_Q_MAILBOX:\n\t\tcq = &hw->mailboxq;\n\t\tqtype = \"Mailbox\";\n\t\t \n\t\thw->mbx_snapshot.mbx_buf.state = ICE_MAL_VF_DETECT_STATE_NEW_SNAPSHOT;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(dev, \"Unknown control queue type 0x%x\\n\", q_type);\n\t\treturn 0;\n\t}\n\n\t \n\tval = rd32(hw, cq->rq.len);\n\tif (val & (PF_FW_ARQLEN_ARQVFE_M | PF_FW_ARQLEN_ARQOVFL_M |\n\t\t   PF_FW_ARQLEN_ARQCRIT_M)) {\n\t\toldval = val;\n\t\tif (val & PF_FW_ARQLEN_ARQVFE_M)\n\t\t\tdev_dbg(dev, \"%s Receive Queue VF Error detected\\n\",\n\t\t\t\tqtype);\n\t\tif (val & PF_FW_ARQLEN_ARQOVFL_M) {\n\t\t\tdev_dbg(dev, \"%s Receive Queue Overflow Error detected\\n\",\n\t\t\t\tqtype);\n\t\t}\n\t\tif (val & PF_FW_ARQLEN_ARQCRIT_M)\n\t\t\tdev_dbg(dev, \"%s Receive Queue Critical Error detected\\n\",\n\t\t\t\tqtype);\n\t\tval &= ~(PF_FW_ARQLEN_ARQVFE_M | PF_FW_ARQLEN_ARQOVFL_M |\n\t\t\t PF_FW_ARQLEN_ARQCRIT_M);\n\t\tif (oldval != val)\n\t\t\twr32(hw, cq->rq.len, val);\n\t}\n\n\tval = rd32(hw, cq->sq.len);\n\tif (val & (PF_FW_ATQLEN_ATQVFE_M | PF_FW_ATQLEN_ATQOVFL_M |\n\t\t   PF_FW_ATQLEN_ATQCRIT_M)) {\n\t\toldval = val;\n\t\tif (val & PF_FW_ATQLEN_ATQVFE_M)\n\t\t\tdev_dbg(dev, \"%s Send Queue VF Error detected\\n\",\n\t\t\t\tqtype);\n\t\tif (val & PF_FW_ATQLEN_ATQOVFL_M) {\n\t\t\tdev_dbg(dev, \"%s Send Queue Overflow Error detected\\n\",\n\t\t\t\tqtype);\n\t\t}\n\t\tif (val & PF_FW_ATQLEN_ATQCRIT_M)\n\t\t\tdev_dbg(dev, \"%s Send Queue Critical Error detected\\n\",\n\t\t\t\tqtype);\n\t\tval &= ~(PF_FW_ATQLEN_ATQVFE_M | PF_FW_ATQLEN_ATQOVFL_M |\n\t\t\t PF_FW_ATQLEN_ATQCRIT_M);\n\t\tif (oldval != val)\n\t\t\twr32(hw, cq->sq.len, val);\n\t}\n\n\tevent.buf_len = cq->rq_buf_size;\n\tevent.msg_buf = kzalloc(event.buf_len, GFP_KERNEL);\n\tif (!event.msg_buf)\n\t\treturn 0;\n\n\tdo {\n\t\tstruct ice_mbx_data data = {};\n\t\tu16 opcode;\n\t\tint ret;\n\n\t\tret = ice_clean_rq_elem(hw, cq, &event, &pending);\n\t\tif (ret == -EALREADY)\n\t\t\tbreak;\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"%s Receive Queue event error %d\\n\", qtype,\n\t\t\t\tret);\n\t\t\tbreak;\n\t\t}\n\n\t\topcode = le16_to_cpu(event.desc.opcode);\n\n\t\t \n\t\tice_aq_check_events(pf, opcode, &event);\n\n\t\tswitch (opcode) {\n\t\tcase ice_aqc_opc_get_link_status:\n\t\t\tif (ice_handle_link_event(pf, &event))\n\t\t\t\tdev_err(dev, \"Could not handle link event\\n\");\n\t\t\tbreak;\n\t\tcase ice_aqc_opc_event_lan_overflow:\n\t\t\tice_vf_lan_overflow_event(pf, &event);\n\t\t\tbreak;\n\t\tcase ice_mbx_opc_send_msg_to_pf:\n\t\t\tdata.num_msg_proc = i;\n\t\t\tdata.num_pending_arq = pending;\n\t\t\tdata.max_num_msgs_mbx = hw->mailboxq.num_rq_entries;\n\t\t\tdata.async_watermark_val = ICE_MBX_OVERFLOW_WATERMARK;\n\n\t\t\tice_vc_process_vf_msg(pf, &event, &data);\n\t\t\tbreak;\n\t\tcase ice_aqc_opc_fw_logging:\n\t\t\tice_output_fw_log(hw, &event.desc, event.msg_buf);\n\t\t\tbreak;\n\t\tcase ice_aqc_opc_lldp_set_mib_change:\n\t\t\tice_dcb_process_lldp_set_mib_change(pf, &event);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_dbg(dev, \"%s Receive Queue unknown event 0x%04x ignored\\n\",\n\t\t\t\tqtype, opcode);\n\t\t\tbreak;\n\t\t}\n\t} while (pending && (i++ < ICE_DFLT_IRQ_WORK));\n\n\tkfree(event.msg_buf);\n\n\treturn pending && (i == ICE_DFLT_IRQ_WORK);\n}\n\n \nstatic bool ice_ctrlq_pending(struct ice_hw *hw, struct ice_ctl_q_info *cq)\n{\n\tu16 ntu;\n\n\tntu = (u16)(rd32(hw, cq->rq.head) & cq->rq.head_mask);\n\treturn cq->rq.next_to_clean != ntu;\n}\n\n \nstatic void ice_clean_adminq_subtask(struct ice_pf *pf)\n{\n\tstruct ice_hw *hw = &pf->hw;\n\n\tif (!test_bit(ICE_ADMINQ_EVENT_PENDING, pf->state))\n\t\treturn;\n\n\tif (__ice_clean_ctrlq(pf, ICE_CTL_Q_ADMIN))\n\t\treturn;\n\n\tclear_bit(ICE_ADMINQ_EVENT_PENDING, pf->state);\n\n\t \n\tif (ice_ctrlq_pending(hw, &hw->adminq))\n\t\t__ice_clean_ctrlq(pf, ICE_CTL_Q_ADMIN);\n\n\tice_flush(hw);\n}\n\n \nstatic void ice_clean_mailboxq_subtask(struct ice_pf *pf)\n{\n\tstruct ice_hw *hw = &pf->hw;\n\n\tif (!test_bit(ICE_MAILBOXQ_EVENT_PENDING, pf->state))\n\t\treturn;\n\n\tif (__ice_clean_ctrlq(pf, ICE_CTL_Q_MAILBOX))\n\t\treturn;\n\n\tclear_bit(ICE_MAILBOXQ_EVENT_PENDING, pf->state);\n\n\tif (ice_ctrlq_pending(hw, &hw->mailboxq))\n\t\t__ice_clean_ctrlq(pf, ICE_CTL_Q_MAILBOX);\n\n\tice_flush(hw);\n}\n\n \nstatic void ice_clean_sbq_subtask(struct ice_pf *pf)\n{\n\tstruct ice_hw *hw = &pf->hw;\n\n\t \n\tif (!ice_is_sbq_supported(hw)) {\n\t\tclear_bit(ICE_SIDEBANDQ_EVENT_PENDING, pf->state);\n\t\treturn;\n\t}\n\n\tif (!test_bit(ICE_SIDEBANDQ_EVENT_PENDING, pf->state))\n\t\treturn;\n\n\tif (__ice_clean_ctrlq(pf, ICE_CTL_Q_SB))\n\t\treturn;\n\n\tclear_bit(ICE_SIDEBANDQ_EVENT_PENDING, pf->state);\n\n\tif (ice_ctrlq_pending(hw, &hw->sbq))\n\t\t__ice_clean_ctrlq(pf, ICE_CTL_Q_SB);\n\n\tice_flush(hw);\n}\n\n \nvoid ice_service_task_schedule(struct ice_pf *pf)\n{\n\tif (!test_bit(ICE_SERVICE_DIS, pf->state) &&\n\t    !test_and_set_bit(ICE_SERVICE_SCHED, pf->state) &&\n\t    !test_bit(ICE_NEEDS_RESTART, pf->state))\n\t\tqueue_work(ice_wq, &pf->serv_task);\n}\n\n \nstatic void ice_service_task_complete(struct ice_pf *pf)\n{\n\tWARN_ON(!test_bit(ICE_SERVICE_SCHED, pf->state));\n\n\t \n\tsmp_mb__before_atomic();\n\tclear_bit(ICE_SERVICE_SCHED, pf->state);\n}\n\n \nstatic int ice_service_task_stop(struct ice_pf *pf)\n{\n\tint ret;\n\n\tret = test_and_set_bit(ICE_SERVICE_DIS, pf->state);\n\n\tif (pf->serv_tmr.function)\n\t\tdel_timer_sync(&pf->serv_tmr);\n\tif (pf->serv_task.func)\n\t\tcancel_work_sync(&pf->serv_task);\n\n\tclear_bit(ICE_SERVICE_SCHED, pf->state);\n\treturn ret;\n}\n\n \nstatic void ice_service_task_restart(struct ice_pf *pf)\n{\n\tclear_bit(ICE_SERVICE_DIS, pf->state);\n\tice_service_task_schedule(pf);\n}\n\n \nstatic void ice_service_timer(struct timer_list *t)\n{\n\tstruct ice_pf *pf = from_timer(pf, t, serv_tmr);\n\n\tmod_timer(&pf->serv_tmr, round_jiffies(pf->serv_tmr_period + jiffies));\n\tice_service_task_schedule(pf);\n}\n\n \nstatic void ice_handle_mdd_event(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct ice_vf *vf;\n\tunsigned int bkt;\n\tu32 reg;\n\n\tif (!test_and_clear_bit(ICE_MDD_EVENT_PENDING, pf->state)) {\n\t\t \n\t\tice_print_vfs_mdd_events(pf);\n\t\treturn;\n\t}\n\n\t \n\treg = rd32(hw, GL_MDET_TX_PQM);\n\tif (reg & GL_MDET_TX_PQM_VALID_M) {\n\t\tu8 pf_num = (reg & GL_MDET_TX_PQM_PF_NUM_M) >>\n\t\t\t\tGL_MDET_TX_PQM_PF_NUM_S;\n\t\tu16 vf_num = (reg & GL_MDET_TX_PQM_VF_NUM_M) >>\n\t\t\t\tGL_MDET_TX_PQM_VF_NUM_S;\n\t\tu8 event = (reg & GL_MDET_TX_PQM_MAL_TYPE_M) >>\n\t\t\t\tGL_MDET_TX_PQM_MAL_TYPE_S;\n\t\tu16 queue = ((reg & GL_MDET_TX_PQM_QNUM_M) >>\n\t\t\t\tGL_MDET_TX_PQM_QNUM_S);\n\n\t\tif (netif_msg_tx_err(pf))\n\t\t\tdev_info(dev, \"Malicious Driver Detection event %d on TX queue %d PF# %d VF# %d\\n\",\n\t\t\t\t event, queue, pf_num, vf_num);\n\t\twr32(hw, GL_MDET_TX_PQM, 0xffffffff);\n\t}\n\n\treg = rd32(hw, GL_MDET_TX_TCLAN);\n\tif (reg & GL_MDET_TX_TCLAN_VALID_M) {\n\t\tu8 pf_num = (reg & GL_MDET_TX_TCLAN_PF_NUM_M) >>\n\t\t\t\tGL_MDET_TX_TCLAN_PF_NUM_S;\n\t\tu16 vf_num = (reg & GL_MDET_TX_TCLAN_VF_NUM_M) >>\n\t\t\t\tGL_MDET_TX_TCLAN_VF_NUM_S;\n\t\tu8 event = (reg & GL_MDET_TX_TCLAN_MAL_TYPE_M) >>\n\t\t\t\tGL_MDET_TX_TCLAN_MAL_TYPE_S;\n\t\tu16 queue = ((reg & GL_MDET_TX_TCLAN_QNUM_M) >>\n\t\t\t\tGL_MDET_TX_TCLAN_QNUM_S);\n\n\t\tif (netif_msg_tx_err(pf))\n\t\t\tdev_info(dev, \"Malicious Driver Detection event %d on TX queue %d PF# %d VF# %d\\n\",\n\t\t\t\t event, queue, pf_num, vf_num);\n\t\twr32(hw, GL_MDET_TX_TCLAN, 0xffffffff);\n\t}\n\n\treg = rd32(hw, GL_MDET_RX);\n\tif (reg & GL_MDET_RX_VALID_M) {\n\t\tu8 pf_num = (reg & GL_MDET_RX_PF_NUM_M) >>\n\t\t\t\tGL_MDET_RX_PF_NUM_S;\n\t\tu16 vf_num = (reg & GL_MDET_RX_VF_NUM_M) >>\n\t\t\t\tGL_MDET_RX_VF_NUM_S;\n\t\tu8 event = (reg & GL_MDET_RX_MAL_TYPE_M) >>\n\t\t\t\tGL_MDET_RX_MAL_TYPE_S;\n\t\tu16 queue = ((reg & GL_MDET_RX_QNUM_M) >>\n\t\t\t\tGL_MDET_RX_QNUM_S);\n\n\t\tif (netif_msg_rx_err(pf))\n\t\t\tdev_info(dev, \"Malicious Driver Detection event %d on RX queue %d PF# %d VF# %d\\n\",\n\t\t\t\t event, queue, pf_num, vf_num);\n\t\twr32(hw, GL_MDET_RX, 0xffffffff);\n\t}\n\n\t \n\treg = rd32(hw, PF_MDET_TX_PQM);\n\tif (reg & PF_MDET_TX_PQM_VALID_M) {\n\t\twr32(hw, PF_MDET_TX_PQM, 0xFFFF);\n\t\tif (netif_msg_tx_err(pf))\n\t\t\tdev_info(dev, \"Malicious Driver Detection event TX_PQM detected on PF\\n\");\n\t}\n\n\treg = rd32(hw, PF_MDET_TX_TCLAN);\n\tif (reg & PF_MDET_TX_TCLAN_VALID_M) {\n\t\twr32(hw, PF_MDET_TX_TCLAN, 0xFFFF);\n\t\tif (netif_msg_tx_err(pf))\n\t\t\tdev_info(dev, \"Malicious Driver Detection event TX_TCLAN detected on PF\\n\");\n\t}\n\n\treg = rd32(hw, PF_MDET_RX);\n\tif (reg & PF_MDET_RX_VALID_M) {\n\t\twr32(hw, PF_MDET_RX, 0xFFFF);\n\t\tif (netif_msg_rx_err(pf))\n\t\t\tdev_info(dev, \"Malicious Driver Detection event RX detected on PF\\n\");\n\t}\n\n\t \n\tmutex_lock(&pf->vfs.table_lock);\n\tice_for_each_vf(pf, bkt, vf) {\n\t\treg = rd32(hw, VP_MDET_TX_PQM(vf->vf_id));\n\t\tif (reg & VP_MDET_TX_PQM_VALID_M) {\n\t\t\twr32(hw, VP_MDET_TX_PQM(vf->vf_id), 0xFFFF);\n\t\t\tvf->mdd_tx_events.count++;\n\t\t\tset_bit(ICE_MDD_VF_PRINT_PENDING, pf->state);\n\t\t\tif (netif_msg_tx_err(pf))\n\t\t\t\tdev_info(dev, \"Malicious Driver Detection event TX_PQM detected on VF %d\\n\",\n\t\t\t\t\t vf->vf_id);\n\t\t}\n\n\t\treg = rd32(hw, VP_MDET_TX_TCLAN(vf->vf_id));\n\t\tif (reg & VP_MDET_TX_TCLAN_VALID_M) {\n\t\t\twr32(hw, VP_MDET_TX_TCLAN(vf->vf_id), 0xFFFF);\n\t\t\tvf->mdd_tx_events.count++;\n\t\t\tset_bit(ICE_MDD_VF_PRINT_PENDING, pf->state);\n\t\t\tif (netif_msg_tx_err(pf))\n\t\t\t\tdev_info(dev, \"Malicious Driver Detection event TX_TCLAN detected on VF %d\\n\",\n\t\t\t\t\t vf->vf_id);\n\t\t}\n\n\t\treg = rd32(hw, VP_MDET_TX_TDPU(vf->vf_id));\n\t\tif (reg & VP_MDET_TX_TDPU_VALID_M) {\n\t\t\twr32(hw, VP_MDET_TX_TDPU(vf->vf_id), 0xFFFF);\n\t\t\tvf->mdd_tx_events.count++;\n\t\t\tset_bit(ICE_MDD_VF_PRINT_PENDING, pf->state);\n\t\t\tif (netif_msg_tx_err(pf))\n\t\t\t\tdev_info(dev, \"Malicious Driver Detection event TX_TDPU detected on VF %d\\n\",\n\t\t\t\t\t vf->vf_id);\n\t\t}\n\n\t\treg = rd32(hw, VP_MDET_RX(vf->vf_id));\n\t\tif (reg & VP_MDET_RX_VALID_M) {\n\t\t\twr32(hw, VP_MDET_RX(vf->vf_id), 0xFFFF);\n\t\t\tvf->mdd_rx_events.count++;\n\t\t\tset_bit(ICE_MDD_VF_PRINT_PENDING, pf->state);\n\t\t\tif (netif_msg_rx_err(pf))\n\t\t\t\tdev_info(dev, \"Malicious Driver Detection event RX detected on VF %d\\n\",\n\t\t\t\t\t vf->vf_id);\n\n\t\t\t \n\t\t\tif (test_bit(ICE_FLAG_MDD_AUTO_RESET_VF, pf->flags)) {\n\t\t\t\t \n\t\t\t\tice_print_vf_rx_mdd_event(vf);\n\t\t\t\tice_reset_vf(vf, ICE_VF_RESET_LOCK);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&pf->vfs.table_lock);\n\n\tice_print_vfs_mdd_events(pf);\n}\n\n \nstatic int ice_force_phys_link_state(struct ice_vsi *vsi, bool link_up)\n{\n\tstruct ice_aqc_get_phy_caps_data *pcaps;\n\tstruct ice_aqc_set_phy_cfg_data *cfg;\n\tstruct ice_port_info *pi;\n\tstruct device *dev;\n\tint retcode;\n\n\tif (!vsi || !vsi->port_info || !vsi->back)\n\t\treturn -EINVAL;\n\tif (vsi->type != ICE_VSI_PF)\n\t\treturn 0;\n\n\tdev = ice_pf_to_dev(vsi->back);\n\n\tpi = vsi->port_info;\n\n\tpcaps = kzalloc(sizeof(*pcaps), GFP_KERNEL);\n\tif (!pcaps)\n\t\treturn -ENOMEM;\n\n\tretcode = ice_aq_get_phy_caps(pi, false, ICE_AQC_REPORT_ACTIVE_CFG, pcaps,\n\t\t\t\t      NULL);\n\tif (retcode) {\n\t\tdev_err(dev, \"Failed to get phy capabilities, VSI %d error %d\\n\",\n\t\t\tvsi->vsi_num, retcode);\n\t\tretcode = -EIO;\n\t\tgoto out;\n\t}\n\n\t \n\tif (link_up == !!(pcaps->caps & ICE_AQC_PHY_EN_LINK) &&\n\t    link_up == !!(pi->phy.link_info.link_info & ICE_AQ_LINK_UP))\n\t\tgoto out;\n\n\t \n\tcfg = kmemdup(&pi->phy.curr_user_phy_cfg, sizeof(*cfg), GFP_KERNEL);\n\tif (!cfg) {\n\t\tretcode = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tcfg->caps |= ICE_AQ_PHY_ENA_AUTO_LINK_UPDT;\n\tif (link_up)\n\t\tcfg->caps |= ICE_AQ_PHY_ENA_LINK;\n\telse\n\t\tcfg->caps &= ~ICE_AQ_PHY_ENA_LINK;\n\n\tretcode = ice_aq_set_phy_cfg(&vsi->back->hw, pi, cfg, NULL);\n\tif (retcode) {\n\t\tdev_err(dev, \"Failed to set phy config, VSI %d error %d\\n\",\n\t\t\tvsi->vsi_num, retcode);\n\t\tretcode = -EIO;\n\t}\n\n\tkfree(cfg);\nout:\n\tkfree(pcaps);\n\treturn retcode;\n}\n\n \nstatic int ice_init_nvm_phy_type(struct ice_port_info *pi)\n{\n\tstruct ice_aqc_get_phy_caps_data *pcaps;\n\tstruct ice_pf *pf = pi->hw->back;\n\tint err;\n\n\tpcaps = kzalloc(sizeof(*pcaps), GFP_KERNEL);\n\tif (!pcaps)\n\t\treturn -ENOMEM;\n\n\terr = ice_aq_get_phy_caps(pi, false, ICE_AQC_REPORT_TOPO_CAP_NO_MEDIA,\n\t\t\t\t  pcaps, NULL);\n\n\tif (err) {\n\t\tdev_err(ice_pf_to_dev(pf), \"Get PHY capability failed.\\n\");\n\t\tgoto out;\n\t}\n\n\tpf->nvm_phy_type_hi = pcaps->phy_type_high;\n\tpf->nvm_phy_type_lo = pcaps->phy_type_low;\n\nout:\n\tkfree(pcaps);\n\treturn err;\n}\n\n \nstatic void ice_init_link_dflt_override(struct ice_port_info *pi)\n{\n\tstruct ice_link_default_override_tlv *ldo;\n\tstruct ice_pf *pf = pi->hw->back;\n\n\tldo = &pf->link_dflt_override;\n\tif (ice_get_link_default_override(ldo, pi))\n\t\treturn;\n\n\tif (!(ldo->options & ICE_LINK_OVERRIDE_PORT_DIS))\n\t\treturn;\n\n\t \n\tset_bit(ICE_FLAG_TOTAL_PORT_SHUTDOWN_ENA, pf->flags);\n\tset_bit(ICE_FLAG_LINK_DOWN_ON_CLOSE_ENA, pf->flags);\n}\n\n \nstatic void ice_init_phy_cfg_dflt_override(struct ice_port_info *pi)\n{\n\tstruct ice_link_default_override_tlv *ldo;\n\tstruct ice_aqc_set_phy_cfg_data *cfg;\n\tstruct ice_phy_info *phy = &pi->phy;\n\tstruct ice_pf *pf = pi->hw->back;\n\n\tldo = &pf->link_dflt_override;\n\n\t \n\tcfg = &phy->curr_user_phy_cfg;\n\n\tif (ldo->phy_type_low || ldo->phy_type_high) {\n\t\tcfg->phy_type_low = pf->nvm_phy_type_lo &\n\t\t\t\t    cpu_to_le64(ldo->phy_type_low);\n\t\tcfg->phy_type_high = pf->nvm_phy_type_hi &\n\t\t\t\t     cpu_to_le64(ldo->phy_type_high);\n\t}\n\tcfg->link_fec_opt = ldo->fec_options;\n\tphy->curr_user_fec_req = ICE_FEC_AUTO;\n\n\tset_bit(ICE_LINK_DEFAULT_OVERRIDE_PENDING, pf->state);\n}\n\n \nstatic int ice_init_phy_user_cfg(struct ice_port_info *pi)\n{\n\tstruct ice_aqc_get_phy_caps_data *pcaps;\n\tstruct ice_phy_info *phy = &pi->phy;\n\tstruct ice_pf *pf = pi->hw->back;\n\tint err;\n\n\tif (!(phy->link_info.link_info & ICE_AQ_MEDIA_AVAILABLE))\n\t\treturn -EIO;\n\n\tpcaps = kzalloc(sizeof(*pcaps), GFP_KERNEL);\n\tif (!pcaps)\n\t\treturn -ENOMEM;\n\n\tif (ice_fw_supports_report_dflt_cfg(pi->hw))\n\t\terr = ice_aq_get_phy_caps(pi, false, ICE_AQC_REPORT_DFLT_CFG,\n\t\t\t\t\t  pcaps, NULL);\n\telse\n\t\terr = ice_aq_get_phy_caps(pi, false, ICE_AQC_REPORT_TOPO_CAP_MEDIA,\n\t\t\t\t\t  pcaps, NULL);\n\tif (err) {\n\t\tdev_err(ice_pf_to_dev(pf), \"Get PHY capability failed.\\n\");\n\t\tgoto err_out;\n\t}\n\n\tice_copy_phy_caps_to_cfg(pi, pcaps, &pi->phy.curr_user_phy_cfg);\n\n\t \n\tif (ice_fw_supports_link_override(pi->hw) &&\n\t    !(pcaps->module_compliance_enforcement &\n\t      ICE_AQC_MOD_ENFORCE_STRICT_MODE)) {\n\t\tset_bit(ICE_FLAG_LINK_LENIENT_MODE_ENA, pf->flags);\n\n\t\t \n\t\tif (!ice_fw_supports_report_dflt_cfg(pi->hw) &&\n\t\t    (pf->link_dflt_override.options & ICE_LINK_OVERRIDE_EN)) {\n\t\t\tice_init_phy_cfg_dflt_override(pi);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tphy->curr_user_fec_req = ice_caps_to_fec_mode(pcaps->caps,\n\t\t\t\t\t\t      pcaps->link_fec_options);\n\tphy->curr_user_fc_req = ice_caps_to_fc_mode(pcaps->caps);\n\nout:\n\tphy->curr_user_speed_req = ICE_AQ_LINK_SPEED_M;\n\tset_bit(ICE_PHY_INIT_COMPLETE, pf->state);\nerr_out:\n\tkfree(pcaps);\n\treturn err;\n}\n\n \nstatic int ice_configure_phy(struct ice_vsi *vsi)\n{\n\tstruct device *dev = ice_pf_to_dev(vsi->back);\n\tstruct ice_port_info *pi = vsi->port_info;\n\tstruct ice_aqc_get_phy_caps_data *pcaps;\n\tstruct ice_aqc_set_phy_cfg_data *cfg;\n\tstruct ice_phy_info *phy = &pi->phy;\n\tstruct ice_pf *pf = vsi->back;\n\tint err;\n\n\t \n\tif (!(phy->link_info.link_info & ICE_AQ_MEDIA_AVAILABLE))\n\t\treturn -ENOMEDIUM;\n\n\tice_print_topo_conflict(vsi);\n\n\tif (!test_bit(ICE_FLAG_LINK_LENIENT_MODE_ENA, pf->flags) &&\n\t    phy->link_info.topo_media_conflict == ICE_AQ_LINK_TOPO_UNSUPP_MEDIA)\n\t\treturn -EPERM;\n\n\tif (test_bit(ICE_FLAG_LINK_DOWN_ON_CLOSE_ENA, pf->flags))\n\t\treturn ice_force_phys_link_state(vsi, true);\n\n\tpcaps = kzalloc(sizeof(*pcaps), GFP_KERNEL);\n\tif (!pcaps)\n\t\treturn -ENOMEM;\n\n\t \n\terr = ice_aq_get_phy_caps(pi, false, ICE_AQC_REPORT_ACTIVE_CFG, pcaps,\n\t\t\t\t  NULL);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to get PHY configuration, VSI %d error %d\\n\",\n\t\t\tvsi->vsi_num, err);\n\t\tgoto done;\n\t}\n\n\t \n\tif (pcaps->caps & ICE_AQC_PHY_EN_LINK &&\n\t    ice_phy_caps_equals_cfg(pcaps, &phy->curr_user_phy_cfg))\n\t\tgoto done;\n\n\t \n\tmemset(pcaps, 0, sizeof(*pcaps));\n\tif (ice_fw_supports_report_dflt_cfg(pi->hw))\n\t\terr = ice_aq_get_phy_caps(pi, false, ICE_AQC_REPORT_DFLT_CFG,\n\t\t\t\t\t  pcaps, NULL);\n\telse\n\t\terr = ice_aq_get_phy_caps(pi, false, ICE_AQC_REPORT_TOPO_CAP_MEDIA,\n\t\t\t\t\t  pcaps, NULL);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to get PHY caps, VSI %d error %d\\n\",\n\t\t\tvsi->vsi_num, err);\n\t\tgoto done;\n\t}\n\n\tcfg = kzalloc(sizeof(*cfg), GFP_KERNEL);\n\tif (!cfg) {\n\t\terr = -ENOMEM;\n\t\tgoto done;\n\t}\n\n\tice_copy_phy_caps_to_cfg(pi, pcaps, cfg);\n\n\t \n\tif (test_and_clear_bit(ICE_LINK_DEFAULT_OVERRIDE_PENDING,\n\t\t\t       vsi->back->state)) {\n\t\tcfg->phy_type_low = phy->curr_user_phy_cfg.phy_type_low;\n\t\tcfg->phy_type_high = phy->curr_user_phy_cfg.phy_type_high;\n\t} else {\n\t\tu64 phy_low = 0, phy_high = 0;\n\n\t\tice_update_phy_type(&phy_low, &phy_high,\n\t\t\t\t    pi->phy.curr_user_speed_req);\n\t\tcfg->phy_type_low = pcaps->phy_type_low & cpu_to_le64(phy_low);\n\t\tcfg->phy_type_high = pcaps->phy_type_high &\n\t\t\t\t     cpu_to_le64(phy_high);\n\t}\n\n\t \n\tif (!cfg->phy_type_low && !cfg->phy_type_high) {\n\t\tcfg->phy_type_low = pcaps->phy_type_low;\n\t\tcfg->phy_type_high = pcaps->phy_type_high;\n\t}\n\n\t \n\tice_cfg_phy_fec(pi, cfg, phy->curr_user_fec_req);\n\n\t \n\tif (cfg->link_fec_opt !=\n\t    (cfg->link_fec_opt & pcaps->link_fec_options)) {\n\t\tcfg->caps |= pcaps->caps & ICE_AQC_PHY_EN_AUTO_FEC;\n\t\tcfg->link_fec_opt = pcaps->link_fec_options;\n\t}\n\n\t \n\tice_cfg_phy_fc(pi, cfg, phy->curr_user_fc_req);\n\n\t \n\tcfg->caps |= ICE_AQ_PHY_ENA_AUTO_LINK_UPDT | ICE_AQ_PHY_ENA_LINK;\n\n\terr = ice_aq_set_phy_cfg(&pf->hw, pi, cfg, NULL);\n\tif (err)\n\t\tdev_err(dev, \"Failed to set phy config, VSI %d error %d\\n\",\n\t\t\tvsi->vsi_num, err);\n\n\tkfree(cfg);\ndone:\n\tkfree(pcaps);\n\treturn err;\n}\n\n \nstatic void ice_check_media_subtask(struct ice_pf *pf)\n{\n\tstruct ice_port_info *pi;\n\tstruct ice_vsi *vsi;\n\tint err;\n\n\t \n\tif (!test_bit(ICE_FLAG_NO_MEDIA, pf->flags))\n\t\treturn;\n\n\tvsi = ice_get_main_vsi(pf);\n\tif (!vsi)\n\t\treturn;\n\n\t \n\tpi = vsi->port_info;\n\terr = ice_update_link_info(pi);\n\tif (err)\n\t\treturn;\n\n\tice_check_link_cfg_err(pf, pi->phy.link_info.link_cfg_err);\n\n\tif (pi->phy.link_info.link_info & ICE_AQ_MEDIA_AVAILABLE) {\n\t\tif (!test_bit(ICE_PHY_INIT_COMPLETE, pf->state))\n\t\t\tice_init_phy_user_cfg(pi);\n\n\t\t \n\t\tif (test_bit(ICE_VSI_DOWN, vsi->state) &&\n\t\t    test_bit(ICE_FLAG_LINK_DOWN_ON_CLOSE_ENA, vsi->back->flags))\n\t\t\treturn;\n\n\t\terr = ice_configure_phy(vsi);\n\t\tif (!err)\n\t\t\tclear_bit(ICE_FLAG_NO_MEDIA, pf->flags);\n\n\t\t \n\t}\n}\n\n \nstatic void ice_service_task(struct work_struct *work)\n{\n\tstruct ice_pf *pf = container_of(work, struct ice_pf, serv_task);\n\tunsigned long start_time = jiffies;\n\n\t \n\n\t \n\tice_reset_subtask(pf);\n\n\t \n\tif (ice_is_reset_in_progress(pf->state) ||\n\t    test_bit(ICE_SUSPENDED, pf->state) ||\n\t    test_bit(ICE_NEEDS_RESTART, pf->state)) {\n\t\tice_service_task_complete(pf);\n\t\treturn;\n\t}\n\n\tif (test_and_clear_bit(ICE_AUX_ERR_PENDING, pf->state)) {\n\t\tstruct iidc_event *event;\n\n\t\tevent = kzalloc(sizeof(*event), GFP_KERNEL);\n\t\tif (event) {\n\t\t\tset_bit(IIDC_EVENT_CRIT_ERR, event->type);\n\t\t\t \n\t\t\tswap(event->reg, pf->oicr_err_reg);\n\t\t\tice_send_event_to_aux(pf, event);\n\t\t\tkfree(event);\n\t\t}\n\t}\n\n\t \n\tif (test_and_clear_bit(ICE_FLAG_UNPLUG_AUX_DEV, pf->flags))\n\t\tice_unplug_aux_dev(pf);\n\n\t \n\tif (test_and_clear_bit(ICE_FLAG_PLUG_AUX_DEV, pf->flags))\n\t\tice_plug_aux_dev(pf);\n\n\tif (test_and_clear_bit(ICE_FLAG_MTU_CHANGED, pf->flags)) {\n\t\tstruct iidc_event *event;\n\n\t\tevent = kzalloc(sizeof(*event), GFP_KERNEL);\n\t\tif (event) {\n\t\t\tset_bit(IIDC_EVENT_AFTER_MTU_CHANGE, event->type);\n\t\t\tice_send_event_to_aux(pf, event);\n\t\t\tkfree(event);\n\t\t}\n\t}\n\n\tice_clean_adminq_subtask(pf);\n\tice_check_media_subtask(pf);\n\tice_check_for_hang_subtask(pf);\n\tice_sync_fltr_subtask(pf);\n\tice_handle_mdd_event(pf);\n\tice_watchdog_subtask(pf);\n\n\tif (ice_is_safe_mode(pf)) {\n\t\tice_service_task_complete(pf);\n\t\treturn;\n\t}\n\n\tice_process_vflr_event(pf);\n\tice_clean_mailboxq_subtask(pf);\n\tice_clean_sbq_subtask(pf);\n\tice_sync_arfs_fltrs(pf);\n\tice_flush_fdir_ctx(pf);\n\n\t \n\tice_service_task_complete(pf);\n\n\t \n\tif (time_after(jiffies, (start_time + pf->serv_tmr_period)) ||\n\t    test_bit(ICE_MDD_EVENT_PENDING, pf->state) ||\n\t    test_bit(ICE_VFLR_EVENT_PENDING, pf->state) ||\n\t    test_bit(ICE_MAILBOXQ_EVENT_PENDING, pf->state) ||\n\t    test_bit(ICE_FD_VF_FLUSH_CTX, pf->state) ||\n\t    test_bit(ICE_SIDEBANDQ_EVENT_PENDING, pf->state) ||\n\t    test_bit(ICE_ADMINQ_EVENT_PENDING, pf->state))\n\t\tmod_timer(&pf->serv_tmr, jiffies);\n}\n\n \nstatic void ice_set_ctrlq_len(struct ice_hw *hw)\n{\n\thw->adminq.num_rq_entries = ICE_AQ_LEN;\n\thw->adminq.num_sq_entries = ICE_AQ_LEN;\n\thw->adminq.rq_buf_size = ICE_AQ_MAX_BUF_LEN;\n\thw->adminq.sq_buf_size = ICE_AQ_MAX_BUF_LEN;\n\thw->mailboxq.num_rq_entries = PF_MBX_ARQLEN_ARQLEN_M;\n\thw->mailboxq.num_sq_entries = ICE_MBXSQ_LEN;\n\thw->mailboxq.rq_buf_size = ICE_MBXQ_MAX_BUF_LEN;\n\thw->mailboxq.sq_buf_size = ICE_MBXQ_MAX_BUF_LEN;\n\thw->sbq.num_rq_entries = ICE_SBQ_LEN;\n\thw->sbq.num_sq_entries = ICE_SBQ_LEN;\n\thw->sbq.rq_buf_size = ICE_SBQ_MAX_BUF_LEN;\n\thw->sbq.sq_buf_size = ICE_SBQ_MAX_BUF_LEN;\n}\n\n \nint ice_schedule_reset(struct ice_pf *pf, enum ice_reset_req reset)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\n\t \n\tif (test_bit(ICE_RESET_FAILED, pf->state)) {\n\t\tdev_dbg(dev, \"earlier reset has failed\\n\");\n\t\treturn -EIO;\n\t}\n\t \n\tif (ice_is_reset_in_progress(pf->state)) {\n\t\tdev_dbg(dev, \"Reset already in progress\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tswitch (reset) {\n\tcase ICE_RESET_PFR:\n\t\tset_bit(ICE_PFR_REQ, pf->state);\n\t\tbreak;\n\tcase ICE_RESET_CORER:\n\t\tset_bit(ICE_CORER_REQ, pf->state);\n\t\tbreak;\n\tcase ICE_RESET_GLOBR:\n\t\tset_bit(ICE_GLOBR_REQ, pf->state);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tice_service_task_schedule(pf);\n\treturn 0;\n}\n\n \nstatic void\nice_irq_affinity_notify(struct irq_affinity_notify *notify,\n\t\t\tconst cpumask_t *mask)\n{\n\tstruct ice_q_vector *q_vector =\n\t\tcontainer_of(notify, struct ice_q_vector, affinity_notify);\n\n\tcpumask_copy(&q_vector->affinity_mask, mask);\n}\n\n \nstatic void ice_irq_affinity_release(struct kref __always_unused *ref) {}\n\n \nstatic int ice_vsi_ena_irq(struct ice_vsi *vsi)\n{\n\tstruct ice_hw *hw = &vsi->back->hw;\n\tint i;\n\n\tice_for_each_q_vector(vsi, i)\n\t\tice_irq_dynamic_ena(hw, vsi, vsi->q_vectors[i]);\n\n\tice_flush(hw);\n\treturn 0;\n}\n\n \nstatic int ice_vsi_req_irq_msix(struct ice_vsi *vsi, char *basename)\n{\n\tint q_vectors = vsi->num_q_vectors;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\tint rx_int_idx = 0;\n\tint tx_int_idx = 0;\n\tint vector, err;\n\tint irq_num;\n\n\tdev = ice_pf_to_dev(pf);\n\tfor (vector = 0; vector < q_vectors; vector++) {\n\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[vector];\n\n\t\tirq_num = q_vector->irq.virq;\n\n\t\tif (q_vector->tx.tx_ring && q_vector->rx.rx_ring) {\n\t\t\tsnprintf(q_vector->name, sizeof(q_vector->name) - 1,\n\t\t\t\t \"%s-%s-%d\", basename, \"TxRx\", rx_int_idx++);\n\t\t\ttx_int_idx++;\n\t\t} else if (q_vector->rx.rx_ring) {\n\t\t\tsnprintf(q_vector->name, sizeof(q_vector->name) - 1,\n\t\t\t\t \"%s-%s-%d\", basename, \"rx\", rx_int_idx++);\n\t\t} else if (q_vector->tx.tx_ring) {\n\t\t\tsnprintf(q_vector->name, sizeof(q_vector->name) - 1,\n\t\t\t\t \"%s-%s-%d\", basename, \"tx\", tx_int_idx++);\n\t\t} else {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\tif (vsi->type == ICE_VSI_CTRL && vsi->vf)\n\t\t\terr = devm_request_irq(dev, irq_num, vsi->irq_handler,\n\t\t\t\t\t       IRQF_SHARED, q_vector->name,\n\t\t\t\t\t       q_vector);\n\t\telse\n\t\t\terr = devm_request_irq(dev, irq_num, vsi->irq_handler,\n\t\t\t\t\t       0, q_vector->name, q_vector);\n\t\tif (err) {\n\t\t\tnetdev_err(vsi->netdev, \"MSIX request_irq failed, error: %d\\n\",\n\t\t\t\t   err);\n\t\t\tgoto free_q_irqs;\n\t\t}\n\n\t\t \n\t\tif (!IS_ENABLED(CONFIG_RFS_ACCEL)) {\n\t\t\tstruct irq_affinity_notify *affinity_notify;\n\n\t\t\taffinity_notify = &q_vector->affinity_notify;\n\t\t\taffinity_notify->notify = ice_irq_affinity_notify;\n\t\t\taffinity_notify->release = ice_irq_affinity_release;\n\t\t\tirq_set_affinity_notifier(irq_num, affinity_notify);\n\t\t}\n\n\t\t \n\t\tirq_set_affinity_hint(irq_num, &q_vector->affinity_mask);\n\t}\n\n\terr = ice_set_cpu_rx_rmap(vsi);\n\tif (err) {\n\t\tnetdev_err(vsi->netdev, \"Failed to setup CPU RMAP on VSI %u: %pe\\n\",\n\t\t\t   vsi->vsi_num, ERR_PTR(err));\n\t\tgoto free_q_irqs;\n\t}\n\n\tvsi->irqs_ready = true;\n\treturn 0;\n\nfree_q_irqs:\n\twhile (vector--) {\n\t\tirq_num = vsi->q_vectors[vector]->irq.virq;\n\t\tif (!IS_ENABLED(CONFIG_RFS_ACCEL))\n\t\t\tirq_set_affinity_notifier(irq_num, NULL);\n\t\tirq_set_affinity_hint(irq_num, NULL);\n\t\tdevm_free_irq(dev, irq_num, &vsi->q_vectors[vector]);\n\t}\n\treturn err;\n}\n\n \nstatic int ice_xdp_alloc_setup_rings(struct ice_vsi *vsi)\n{\n\tstruct device *dev = ice_pf_to_dev(vsi->back);\n\tstruct ice_tx_desc *tx_desc;\n\tint i, j;\n\n\tice_for_each_xdp_txq(vsi, i) {\n\t\tu16 xdp_q_idx = vsi->alloc_txq + i;\n\t\tstruct ice_ring_stats *ring_stats;\n\t\tstruct ice_tx_ring *xdp_ring;\n\n\t\txdp_ring = kzalloc(sizeof(*xdp_ring), GFP_KERNEL);\n\t\tif (!xdp_ring)\n\t\t\tgoto free_xdp_rings;\n\n\t\tring_stats = kzalloc(sizeof(*ring_stats), GFP_KERNEL);\n\t\tif (!ring_stats) {\n\t\t\tice_free_tx_ring(xdp_ring);\n\t\t\tgoto free_xdp_rings;\n\t\t}\n\n\t\txdp_ring->ring_stats = ring_stats;\n\t\txdp_ring->q_index = xdp_q_idx;\n\t\txdp_ring->reg_idx = vsi->txq_map[xdp_q_idx];\n\t\txdp_ring->vsi = vsi;\n\t\txdp_ring->netdev = NULL;\n\t\txdp_ring->dev = dev;\n\t\txdp_ring->count = vsi->num_tx_desc;\n\t\tWRITE_ONCE(vsi->xdp_rings[i], xdp_ring);\n\t\tif (ice_setup_tx_ring(xdp_ring))\n\t\t\tgoto free_xdp_rings;\n\t\tice_set_ring_xdp(xdp_ring);\n\t\tspin_lock_init(&xdp_ring->tx_lock);\n\t\tfor (j = 0; j < xdp_ring->count; j++) {\n\t\t\ttx_desc = ICE_TX_DESC(xdp_ring, j);\n\t\t\ttx_desc->cmd_type_offset_bsz = 0;\n\t\t}\n\t}\n\n\treturn 0;\n\nfree_xdp_rings:\n\tfor (; i >= 0; i--) {\n\t\tif (vsi->xdp_rings[i] && vsi->xdp_rings[i]->desc) {\n\t\t\tkfree_rcu(vsi->xdp_rings[i]->ring_stats, rcu);\n\t\t\tvsi->xdp_rings[i]->ring_stats = NULL;\n\t\t\tice_free_tx_ring(vsi->xdp_rings[i]);\n\t\t}\n\t}\n\treturn -ENOMEM;\n}\n\n \nstatic void ice_vsi_assign_bpf_prog(struct ice_vsi *vsi, struct bpf_prog *prog)\n{\n\tstruct bpf_prog *old_prog;\n\tint i;\n\n\told_prog = xchg(&vsi->xdp_prog, prog);\n\tice_for_each_rxq(vsi, i)\n\t\tWRITE_ONCE(vsi->rx_rings[i]->xdp_prog, vsi->xdp_prog);\n\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n}\n\n \nint ice_prepare_xdp_rings(struct ice_vsi *vsi, struct bpf_prog *prog)\n{\n\tu16 max_txqs[ICE_MAX_TRAFFIC_CLASS] = { 0 };\n\tint xdp_rings_rem = vsi->num_xdp_txq;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_qs_cfg xdp_qs_cfg = {\n\t\t.qs_mutex = &pf->avail_q_mutex,\n\t\t.pf_map = pf->avail_txqs,\n\t\t.pf_map_size = pf->max_pf_txqs,\n\t\t.q_count = vsi->num_xdp_txq,\n\t\t.scatter_count = ICE_MAX_SCATTER_TXQS,\n\t\t.vsi_map = vsi->txq_map,\n\t\t.vsi_map_offset = vsi->alloc_txq,\n\t\t.mapping_mode = ICE_VSI_MAP_CONTIG\n\t};\n\tstruct device *dev;\n\tint i, v_idx;\n\tint status;\n\n\tdev = ice_pf_to_dev(pf);\n\tvsi->xdp_rings = devm_kcalloc(dev, vsi->num_xdp_txq,\n\t\t\t\t      sizeof(*vsi->xdp_rings), GFP_KERNEL);\n\tif (!vsi->xdp_rings)\n\t\treturn -ENOMEM;\n\n\tvsi->xdp_mapping_mode = xdp_qs_cfg.mapping_mode;\n\tif (__ice_vsi_get_qs(&xdp_qs_cfg))\n\t\tgoto err_map_xdp;\n\n\tif (static_key_enabled(&ice_xdp_locking_key))\n\t\tnetdev_warn(vsi->netdev,\n\t\t\t    \"Could not allocate one XDP Tx ring per CPU, XDP_TX/XDP_REDIRECT actions will be slower\\n\");\n\n\tif (ice_xdp_alloc_setup_rings(vsi))\n\t\tgoto clear_xdp_rings;\n\n\t \n\tice_for_each_q_vector(vsi, v_idx) {\n\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[v_idx];\n\t\tint xdp_rings_per_v, q_id, q_base;\n\n\t\txdp_rings_per_v = DIV_ROUND_UP(xdp_rings_rem,\n\t\t\t\t\t       vsi->num_q_vectors - v_idx);\n\t\tq_base = vsi->num_xdp_txq - xdp_rings_rem;\n\n\t\tfor (q_id = q_base; q_id < (q_base + xdp_rings_per_v); q_id++) {\n\t\t\tstruct ice_tx_ring *xdp_ring = vsi->xdp_rings[q_id];\n\n\t\t\txdp_ring->q_vector = q_vector;\n\t\t\txdp_ring->next = q_vector->tx.tx_ring;\n\t\t\tq_vector->tx.tx_ring = xdp_ring;\n\t\t}\n\t\txdp_rings_rem -= xdp_rings_per_v;\n\t}\n\n\tice_for_each_rxq(vsi, i) {\n\t\tif (static_key_enabled(&ice_xdp_locking_key)) {\n\t\t\tvsi->rx_rings[i]->xdp_ring = vsi->xdp_rings[i % vsi->num_xdp_txq];\n\t\t} else {\n\t\t\tstruct ice_q_vector *q_vector = vsi->rx_rings[i]->q_vector;\n\t\t\tstruct ice_tx_ring *ring;\n\n\t\t\tice_for_each_tx_ring(ring, q_vector->tx) {\n\t\t\t\tif (ice_ring_is_xdp(ring)) {\n\t\t\t\t\tvsi->rx_rings[i]->xdp_ring = ring;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tice_tx_xsk_pool(vsi, i);\n\t}\n\n\t \n\tif (ice_is_reset_in_progress(pf->state))\n\t\treturn 0;\n\n\t \n\tfor (i = 0; i < vsi->tc_cfg.numtc; i++)\n\t\tmax_txqs[i] = vsi->num_txq + vsi->num_xdp_txq;\n\n\tstatus = ice_cfg_vsi_lan(vsi->port_info, vsi->idx, vsi->tc_cfg.ena_tc,\n\t\t\t\t max_txqs);\n\tif (status) {\n\t\tdev_err(dev, \"Failed VSI LAN queue config for XDP, error: %d\\n\",\n\t\t\tstatus);\n\t\tgoto clear_xdp_rings;\n\t}\n\n\t \n\tif (!ice_is_xdp_ena_vsi(vsi))\n\t\tice_vsi_assign_bpf_prog(vsi, prog);\n\n\treturn 0;\nclear_xdp_rings:\n\tice_for_each_xdp_txq(vsi, i)\n\t\tif (vsi->xdp_rings[i]) {\n\t\t\tkfree_rcu(vsi->xdp_rings[i], rcu);\n\t\t\tvsi->xdp_rings[i] = NULL;\n\t\t}\n\nerr_map_xdp:\n\tmutex_lock(&pf->avail_q_mutex);\n\tice_for_each_xdp_txq(vsi, i) {\n\t\tclear_bit(vsi->txq_map[i + vsi->alloc_txq], pf->avail_txqs);\n\t\tvsi->txq_map[i + vsi->alloc_txq] = ICE_INVAL_Q_INDEX;\n\t}\n\tmutex_unlock(&pf->avail_q_mutex);\n\n\tdevm_kfree(dev, vsi->xdp_rings);\n\treturn -ENOMEM;\n}\n\n \nint ice_destroy_xdp_rings(struct ice_vsi *vsi)\n{\n\tu16 max_txqs[ICE_MAX_TRAFFIC_CLASS] = { 0 };\n\tstruct ice_pf *pf = vsi->back;\n\tint i, v_idx;\n\n\t \n\tif (ice_is_reset_in_progress(pf->state) || !vsi->q_vectors[0])\n\t\tgoto free_qmap;\n\n\tice_for_each_q_vector(vsi, v_idx) {\n\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[v_idx];\n\t\tstruct ice_tx_ring *ring;\n\n\t\tice_for_each_tx_ring(ring, q_vector->tx)\n\t\t\tif (!ring->tx_buf || !ice_ring_is_xdp(ring))\n\t\t\t\tbreak;\n\n\t\t \n\t\tq_vector->tx.tx_ring = ring;\n\t}\n\nfree_qmap:\n\tmutex_lock(&pf->avail_q_mutex);\n\tice_for_each_xdp_txq(vsi, i) {\n\t\tclear_bit(vsi->txq_map[i + vsi->alloc_txq], pf->avail_txqs);\n\t\tvsi->txq_map[i + vsi->alloc_txq] = ICE_INVAL_Q_INDEX;\n\t}\n\tmutex_unlock(&pf->avail_q_mutex);\n\n\tice_for_each_xdp_txq(vsi, i)\n\t\tif (vsi->xdp_rings[i]) {\n\t\t\tif (vsi->xdp_rings[i]->desc) {\n\t\t\t\tsynchronize_rcu();\n\t\t\t\tice_free_tx_ring(vsi->xdp_rings[i]);\n\t\t\t}\n\t\t\tkfree_rcu(vsi->xdp_rings[i]->ring_stats, rcu);\n\t\t\tvsi->xdp_rings[i]->ring_stats = NULL;\n\t\t\tkfree_rcu(vsi->xdp_rings[i], rcu);\n\t\t\tvsi->xdp_rings[i] = NULL;\n\t\t}\n\n\tdevm_kfree(ice_pf_to_dev(pf), vsi->xdp_rings);\n\tvsi->xdp_rings = NULL;\n\n\tif (static_key_enabled(&ice_xdp_locking_key))\n\t\tstatic_branch_dec(&ice_xdp_locking_key);\n\n\tif (ice_is_reset_in_progress(pf->state) || !vsi->q_vectors[0])\n\t\treturn 0;\n\n\tice_vsi_assign_bpf_prog(vsi, NULL);\n\n\t \n\tfor (i = 0; i < vsi->tc_cfg.numtc; i++)\n\t\tmax_txqs[i] = vsi->num_txq;\n\n\t \n\tvsi->num_xdp_txq = 0;\n\n\treturn ice_cfg_vsi_lan(vsi->port_info, vsi->idx, vsi->tc_cfg.ena_tc,\n\t\t\t       max_txqs);\n}\n\n \nstatic void ice_vsi_rx_napi_schedule(struct ice_vsi *vsi)\n{\n\tint i;\n\n\tice_for_each_rxq(vsi, i) {\n\t\tstruct ice_rx_ring *rx_ring = vsi->rx_rings[i];\n\n\t\tif (rx_ring->xsk_pool)\n\t\t\tnapi_schedule(&rx_ring->q_vector->napi);\n\t}\n}\n\n \nint ice_vsi_determine_xdp_res(struct ice_vsi *vsi)\n{\n\tu16 avail = ice_get_avail_txq_count(vsi->back);\n\tu16 cpus = num_possible_cpus();\n\n\tif (avail < cpus / 2)\n\t\treturn -ENOMEM;\n\n\tvsi->num_xdp_txq = min_t(u16, avail, cpus);\n\n\tif (vsi->num_xdp_txq < cpus)\n\t\tstatic_branch_inc(&ice_xdp_locking_key);\n\n\treturn 0;\n}\n\n \nstatic int ice_max_xdp_frame_size(struct ice_vsi *vsi)\n{\n\tif (test_bit(ICE_FLAG_LEGACY_RX, vsi->back->flags))\n\t\treturn ICE_RXBUF_1664;\n\telse\n\t\treturn ICE_RXBUF_3072;\n}\n\n \nstatic int\nice_xdp_setup_prog(struct ice_vsi *vsi, struct bpf_prog *prog,\n\t\t   struct netlink_ext_ack *extack)\n{\n\tunsigned int frame_size = vsi->netdev->mtu + ICE_ETH_PKT_HDR_PAD;\n\tbool if_running = netif_running(vsi->netdev);\n\tint ret = 0, xdp_ring_err = 0;\n\n\tif (prog && !prog->aux->xdp_has_frags) {\n\t\tif (frame_size > ice_max_xdp_frame_size(vsi)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t   \"MTU is too large for linear frames and XDP prog does not support frags\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t}\n\n\t \n\tif (ice_is_xdp_ena_vsi(vsi) == !!prog) {\n\t\tice_vsi_assign_bpf_prog(vsi, prog);\n\t\treturn 0;\n\t}\n\n\t \n\tif (if_running && !test_and_set_bit(ICE_VSI_DOWN, vsi->state)) {\n\t\tret = ice_down(vsi);\n\t\tif (ret) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Preparing device for XDP attach failed\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (!ice_is_xdp_ena_vsi(vsi) && prog) {\n\t\txdp_ring_err = ice_vsi_determine_xdp_res(vsi);\n\t\tif (xdp_ring_err) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Not enough Tx resources for XDP\");\n\t\t} else {\n\t\t\txdp_ring_err = ice_prepare_xdp_rings(vsi, prog);\n\t\t\tif (xdp_ring_err)\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Setting up XDP Tx resources failed\");\n\t\t}\n\t\txdp_features_set_redirect_target(vsi->netdev, true);\n\t\t \n\t\txdp_ring_err = ice_realloc_zc_buf(vsi, true);\n\t\tif (xdp_ring_err)\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Setting up XDP Rx resources failed\");\n\t} else if (ice_is_xdp_ena_vsi(vsi) && !prog) {\n\t\txdp_features_clear_redirect_target(vsi->netdev);\n\t\txdp_ring_err = ice_destroy_xdp_rings(vsi);\n\t\tif (xdp_ring_err)\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Freeing XDP Tx resources failed\");\n\t\t \n\t\txdp_ring_err = ice_realloc_zc_buf(vsi, false);\n\t\tif (xdp_ring_err)\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Freeing XDP Rx resources failed\");\n\t}\n\n\tif (if_running)\n\t\tret = ice_up(vsi);\n\n\tif (!ret && prog)\n\t\tice_vsi_rx_napi_schedule(vsi);\n\n\treturn (ret || xdp_ring_err) ? -ENOMEM : 0;\n}\n\n \nstatic int ice_xdp_safe_mode(struct net_device __always_unused *dev,\n\t\t\t     struct netdev_bpf *xdp)\n{\n\tNL_SET_ERR_MSG_MOD(xdp->extack,\n\t\t\t   \"Please provide working DDP firmware package in order to use XDP\\n\"\n\t\t\t   \"Refer to Documentation/networking/device_drivers/ethernet/intel/ice.rst\");\n\treturn -EOPNOTSUPP;\n}\n\n \nstatic int ice_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(dev);\n\tstruct ice_vsi *vsi = np->vsi;\n\n\tif (vsi->type != ICE_VSI_PF) {\n\t\tNL_SET_ERR_MSG_MOD(xdp->extack, \"XDP can be loaded only on PF VSI\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn ice_xdp_setup_prog(vsi, xdp->prog, xdp->extack);\n\tcase XDP_SETUP_XSK_POOL:\n\t\treturn ice_xsk_pool_setup(vsi, xdp->xsk.pool,\n\t\t\t\t\t  xdp->xsk.queue_id);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n \nstatic void ice_ena_misc_vector(struct ice_pf *pf)\n{\n\tstruct ice_hw *hw = &pf->hw;\n\tu32 val;\n\n\t \n\tval = rd32(hw, GL_MDCK_TX_TDPU);\n\tval |= GL_MDCK_TX_TDPU_RCU_ANTISPOOF_ITR_DIS_M;\n\twr32(hw, GL_MDCK_TX_TDPU, val);\n\n\t \n\twr32(hw, PFINT_OICR_ENA, 0);\t \n\trd32(hw, PFINT_OICR);\t\t \n\n\tval = (PFINT_OICR_ECC_ERR_M |\n\t       PFINT_OICR_MAL_DETECT_M |\n\t       PFINT_OICR_GRST_M |\n\t       PFINT_OICR_PCI_EXCEPTION_M |\n\t       PFINT_OICR_VFLR_M |\n\t       PFINT_OICR_HMC_ERR_M |\n\t       PFINT_OICR_PE_PUSH_M |\n\t       PFINT_OICR_PE_CRITERR_M);\n\n\twr32(hw, PFINT_OICR_ENA, val);\n\n\t \n\twr32(hw, GLINT_DYN_CTL(pf->oicr_irq.index),\n\t     GLINT_DYN_CTL_SW_ITR_INDX_M | GLINT_DYN_CTL_INTENA_MSK_M);\n}\n\n \nstatic irqreturn_t ice_misc_intr(int __always_unused irq, void *data)\n{\n\tstruct ice_pf *pf = (struct ice_pf *)data;\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct device *dev;\n\tu32 oicr, ena_mask;\n\n\tdev = ice_pf_to_dev(pf);\n\tset_bit(ICE_ADMINQ_EVENT_PENDING, pf->state);\n\tset_bit(ICE_MAILBOXQ_EVENT_PENDING, pf->state);\n\tset_bit(ICE_SIDEBANDQ_EVENT_PENDING, pf->state);\n\n\toicr = rd32(hw, PFINT_OICR);\n\tena_mask = rd32(hw, PFINT_OICR_ENA);\n\n\tif (oicr & PFINT_OICR_SWINT_M) {\n\t\tena_mask &= ~PFINT_OICR_SWINT_M;\n\t\tpf->sw_int_count++;\n\t}\n\n\tif (oicr & PFINT_OICR_MAL_DETECT_M) {\n\t\tena_mask &= ~PFINT_OICR_MAL_DETECT_M;\n\t\tset_bit(ICE_MDD_EVENT_PENDING, pf->state);\n\t}\n\tif (oicr & PFINT_OICR_VFLR_M) {\n\t\t \n\t\tif (test_bit(ICE_VF_RESETS_DISABLED, pf->state)) {\n\t\t\tu32 reg = rd32(hw, PFINT_OICR_ENA);\n\n\t\t\treg &= ~PFINT_OICR_VFLR_M;\n\t\t\twr32(hw, PFINT_OICR_ENA, reg);\n\t\t} else {\n\t\t\tena_mask &= ~PFINT_OICR_VFLR_M;\n\t\t\tset_bit(ICE_VFLR_EVENT_PENDING, pf->state);\n\t\t}\n\t}\n\n\tif (oicr & PFINT_OICR_GRST_M) {\n\t\tu32 reset;\n\n\t\t \n\t\tena_mask &= ~PFINT_OICR_GRST_M;\n\t\treset = (rd32(hw, GLGEN_RSTAT) & GLGEN_RSTAT_RESET_TYPE_M) >>\n\t\t\tGLGEN_RSTAT_RESET_TYPE_S;\n\n\t\tif (reset == ICE_RESET_CORER)\n\t\t\tpf->corer_count++;\n\t\telse if (reset == ICE_RESET_GLOBR)\n\t\t\tpf->globr_count++;\n\t\telse if (reset == ICE_RESET_EMPR)\n\t\t\tpf->empr_count++;\n\t\telse\n\t\t\tdev_dbg(dev, \"Invalid reset type %d\\n\", reset);\n\n\t\t \n\t\tif (!test_and_set_bit(ICE_RESET_OICR_RECV, pf->state)) {\n\t\t\tif (reset == ICE_RESET_CORER)\n\t\t\t\tset_bit(ICE_CORER_RECV, pf->state);\n\t\t\telse if (reset == ICE_RESET_GLOBR)\n\t\t\t\tset_bit(ICE_GLOBR_RECV, pf->state);\n\t\t\telse\n\t\t\t\tset_bit(ICE_EMPR_RECV, pf->state);\n\n\t\t\t \n\t\t\thw->reset_ongoing = true;\n\t\t}\n\t}\n\n\tif (oicr & PFINT_OICR_TSYN_TX_M) {\n\t\tena_mask &= ~PFINT_OICR_TSYN_TX_M;\n\t\tif (!hw->reset_ongoing)\n\t\t\tset_bit(ICE_MISC_THREAD_TX_TSTAMP, pf->misc_thread);\n\t}\n\n\tif (oicr & PFINT_OICR_TSYN_EVNT_M) {\n\t\tu8 tmr_idx = hw->func_caps.ts_func_info.tmr_index_owned;\n\t\tu32 gltsyn_stat = rd32(hw, GLTSYN_STAT(tmr_idx));\n\n\t\tena_mask &= ~PFINT_OICR_TSYN_EVNT_M;\n\n\t\tif (hw->func_caps.ts_func_info.src_tmr_owned) {\n\t\t\t \n\t\t\tpf->ptp.ext_ts_irq |= gltsyn_stat &\n\t\t\t\t\t      (GLTSYN_STAT_EVENT0_M |\n\t\t\t\t\t       GLTSYN_STAT_EVENT1_M |\n\t\t\t\t\t       GLTSYN_STAT_EVENT2_M);\n\n\t\t\tset_bit(ICE_MISC_THREAD_EXTTS_EVENT, pf->misc_thread);\n\t\t}\n\t}\n\n#define ICE_AUX_CRIT_ERR (PFINT_OICR_PE_CRITERR_M | PFINT_OICR_HMC_ERR_M | PFINT_OICR_PE_PUSH_M)\n\tif (oicr & ICE_AUX_CRIT_ERR) {\n\t\tpf->oicr_err_reg |= oicr;\n\t\tset_bit(ICE_AUX_ERR_PENDING, pf->state);\n\t\tena_mask &= ~ICE_AUX_CRIT_ERR;\n\t}\n\n\t \n\toicr &= ena_mask;\n\tif (oicr) {\n\t\tdev_dbg(dev, \"unhandled interrupt oicr=0x%08x\\n\", oicr);\n\t\t \n\t\tif (oicr & (PFINT_OICR_PCI_EXCEPTION_M |\n\t\t\t    PFINT_OICR_ECC_ERR_M)) {\n\t\t\tset_bit(ICE_PFR_REQ, pf->state);\n\t\t}\n\t}\n\n\treturn IRQ_WAKE_THREAD;\n}\n\n \nstatic irqreturn_t ice_misc_intr_thread_fn(int __always_unused irq, void *data)\n{\n\tstruct ice_pf *pf = data;\n\tstruct ice_hw *hw;\n\n\thw = &pf->hw;\n\n\tif (ice_is_reset_in_progress(pf->state))\n\t\treturn IRQ_HANDLED;\n\n\tice_service_task_schedule(pf);\n\n\tif (test_and_clear_bit(ICE_MISC_THREAD_EXTTS_EVENT, pf->misc_thread))\n\t\tice_ptp_extts_event(pf);\n\n\tif (test_and_clear_bit(ICE_MISC_THREAD_TX_TSTAMP, pf->misc_thread)) {\n\t\t \n\t\tif (ice_ptp_process_ts(pf) == ICE_TX_TSTAMP_WORK_PENDING) {\n\t\t\twr32(hw, PFINT_OICR, PFINT_OICR_TSYN_TX_M);\n\t\t\tice_flush(hw);\n\t\t}\n\t}\n\n\tice_irq_dynamic_ena(hw, NULL, NULL);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void ice_dis_ctrlq_interrupts(struct ice_hw *hw)\n{\n\t \n\twr32(hw, PFINT_FW_CTL,\n\t     rd32(hw, PFINT_FW_CTL) & ~PFINT_FW_CTL_CAUSE_ENA_M);\n\n\t \n\twr32(hw, PFINT_MBX_CTL,\n\t     rd32(hw, PFINT_MBX_CTL) & ~PFINT_MBX_CTL_CAUSE_ENA_M);\n\n\twr32(hw, PFINT_SB_CTL,\n\t     rd32(hw, PFINT_SB_CTL) & ~PFINT_SB_CTL_CAUSE_ENA_M);\n\n\t \n\twr32(hw, PFINT_OICR_CTL,\n\t     rd32(hw, PFINT_OICR_CTL) & ~PFINT_OICR_CTL_CAUSE_ENA_M);\n\n\tice_flush(hw);\n}\n\n \nstatic void ice_free_irq_msix_misc(struct ice_pf *pf)\n{\n\tint misc_irq_num = pf->oicr_irq.virq;\n\tstruct ice_hw *hw = &pf->hw;\n\n\tice_dis_ctrlq_interrupts(hw);\n\n\t \n\twr32(hw, PFINT_OICR_ENA, 0);\n\tice_flush(hw);\n\n\tsynchronize_irq(misc_irq_num);\n\tdevm_free_irq(ice_pf_to_dev(pf), misc_irq_num, pf);\n\n\tice_free_irq(pf, pf->oicr_irq);\n}\n\n \nstatic void ice_ena_ctrlq_interrupts(struct ice_hw *hw, u16 reg_idx)\n{\n\tu32 val;\n\n\tval = ((reg_idx & PFINT_OICR_CTL_MSIX_INDX_M) |\n\t       PFINT_OICR_CTL_CAUSE_ENA_M);\n\twr32(hw, PFINT_OICR_CTL, val);\n\n\t \n\tval = ((reg_idx & PFINT_FW_CTL_MSIX_INDX_M) |\n\t       PFINT_FW_CTL_CAUSE_ENA_M);\n\twr32(hw, PFINT_FW_CTL, val);\n\n\t \n\tval = ((reg_idx & PFINT_MBX_CTL_MSIX_INDX_M) |\n\t       PFINT_MBX_CTL_CAUSE_ENA_M);\n\twr32(hw, PFINT_MBX_CTL, val);\n\n\t \n\tval = ((reg_idx & PFINT_SB_CTL_MSIX_INDX_M) |\n\t       PFINT_SB_CTL_CAUSE_ENA_M);\n\twr32(hw, PFINT_SB_CTL, val);\n\n\tice_flush(hw);\n}\n\n \nstatic int ice_req_irq_msix_misc(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct msi_map oicr_irq;\n\tint err = 0;\n\n\tif (!pf->int_name[0])\n\t\tsnprintf(pf->int_name, sizeof(pf->int_name) - 1, \"%s-%s:misc\",\n\t\t\t dev_driver_string(dev), dev_name(dev));\n\n\t \n\tif (ice_is_reset_in_progress(pf->state))\n\t\tgoto skip_req_irq;\n\n\t \n\toicr_irq = ice_alloc_irq(pf, false);\n\tif (oicr_irq.index < 0)\n\t\treturn oicr_irq.index;\n\n\tpf->oicr_irq = oicr_irq;\n\terr = devm_request_threaded_irq(dev, pf->oicr_irq.virq, ice_misc_intr,\n\t\t\t\t\tice_misc_intr_thread_fn, 0,\n\t\t\t\t\tpf->int_name, pf);\n\tif (err) {\n\t\tdev_err(dev, \"devm_request_threaded_irq for %s failed: %d\\n\",\n\t\t\tpf->int_name, err);\n\t\tice_free_irq(pf, pf->oicr_irq);\n\t\treturn err;\n\t}\n\nskip_req_irq:\n\tice_ena_misc_vector(pf);\n\n\tice_ena_ctrlq_interrupts(hw, pf->oicr_irq.index);\n\twr32(hw, GLINT_ITR(ICE_RX_ITR, pf->oicr_irq.index),\n\t     ITR_REG_ALIGN(ICE_ITR_8K) >> ICE_ITR_GRAN_S);\n\n\tice_flush(hw);\n\tice_irq_dynamic_ena(hw, NULL, NULL);\n\n\treturn 0;\n}\n\n \nstatic void ice_napi_add(struct ice_vsi *vsi)\n{\n\tint v_idx;\n\n\tif (!vsi->netdev)\n\t\treturn;\n\n\tice_for_each_q_vector(vsi, v_idx)\n\t\tnetif_napi_add(vsi->netdev, &vsi->q_vectors[v_idx]->napi,\n\t\t\t       ice_napi_poll);\n}\n\n \nstatic void ice_set_ops(struct ice_vsi *vsi)\n{\n\tstruct net_device *netdev = vsi->netdev;\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\n\tif (ice_is_safe_mode(pf)) {\n\t\tnetdev->netdev_ops = &ice_netdev_safe_mode_ops;\n\t\tice_set_ethtool_safe_mode_ops(netdev);\n\t\treturn;\n\t}\n\n\tnetdev->netdev_ops = &ice_netdev_ops;\n\tnetdev->udp_tunnel_nic_info = &pf->hw.udp_tunnel_nic;\n\tice_set_ethtool_ops(netdev);\n\n\tif (vsi->type != ICE_VSI_PF)\n\t\treturn;\n\n\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |\n\t\t\t       NETDEV_XDP_ACT_XSK_ZEROCOPY |\n\t\t\t       NETDEV_XDP_ACT_RX_SG;\n\tnetdev->xdp_zc_max_segs = ICE_MAX_BUF_TXD;\n}\n\n \nstatic void ice_set_netdev_features(struct net_device *netdev)\n{\n\tstruct ice_pf *pf = ice_netdev_to_pf(netdev);\n\tbool is_dvm_ena = ice_is_dvm_ena(&pf->hw);\n\tnetdev_features_t csumo_features;\n\tnetdev_features_t vlano_features;\n\tnetdev_features_t dflt_features;\n\tnetdev_features_t tso_features;\n\n\tif (ice_is_safe_mode(pf)) {\n\t\t \n\t\tnetdev->features = NETIF_F_SG | NETIF_F_HIGHDMA;\n\t\tnetdev->hw_features = netdev->features;\n\t\treturn;\n\t}\n\n\tdflt_features = NETIF_F_SG\t|\n\t\t\tNETIF_F_HIGHDMA\t|\n\t\t\tNETIF_F_NTUPLE\t|\n\t\t\tNETIF_F_RXHASH;\n\n\tcsumo_features = NETIF_F_RXCSUM\t  |\n\t\t\t NETIF_F_IP_CSUM  |\n\t\t\t NETIF_F_SCTP_CRC |\n\t\t\t NETIF_F_IPV6_CSUM;\n\n\tvlano_features = NETIF_F_HW_VLAN_CTAG_FILTER |\n\t\t\t NETIF_F_HW_VLAN_CTAG_TX     |\n\t\t\t NETIF_F_HW_VLAN_CTAG_RX;\n\n\t \n\tif (is_dvm_ena)\n\t\tvlano_features |= NETIF_F_HW_VLAN_STAG_FILTER;\n\n\ttso_features = NETIF_F_TSO\t\t\t|\n\t\t       NETIF_F_TSO_ECN\t\t\t|\n\t\t       NETIF_F_TSO6\t\t\t|\n\t\t       NETIF_F_GSO_GRE\t\t\t|\n\t\t       NETIF_F_GSO_UDP_TUNNEL\t\t|\n\t\t       NETIF_F_GSO_GRE_CSUM\t\t|\n\t\t       NETIF_F_GSO_UDP_TUNNEL_CSUM\t|\n\t\t       NETIF_F_GSO_PARTIAL\t\t|\n\t\t       NETIF_F_GSO_IPXIP4\t\t|\n\t\t       NETIF_F_GSO_IPXIP6\t\t|\n\t\t       NETIF_F_GSO_UDP_L4;\n\n\tnetdev->gso_partial_features |= NETIF_F_GSO_UDP_TUNNEL_CSUM |\n\t\t\t\t\tNETIF_F_GSO_GRE_CSUM;\n\t \n\tnetdev->hw_features = dflt_features | csumo_features |\n\t\t\t      vlano_features | tso_features;\n\n\t \n\tnetdev->mpls_features =  NETIF_F_HW_CSUM |\n\t\t\t\t NETIF_F_TSO     |\n\t\t\t\t NETIF_F_TSO6;\n\n\t \n\tnetdev->features |= netdev->hw_features;\n\n\tnetdev->hw_features |= NETIF_F_HW_TC;\n\tnetdev->hw_features |= NETIF_F_LOOPBACK;\n\n\t \n\tnetdev->hw_enc_features |= dflt_features | csumo_features |\n\t\t\t\t   tso_features;\n\tnetdev->vlan_features |= dflt_features | csumo_features |\n\t\t\t\t tso_features;\n\n\t \n\tif (is_dvm_ena)\n\t\tnetdev->hw_features |= NETIF_F_HW_VLAN_STAG_RX |\n\t\t\tNETIF_F_HW_VLAN_STAG_TX;\n\n\t \n\tnetdev->hw_features |= NETIF_F_RXFCS;\n\n\tnetif_set_tso_max_size(netdev, ICE_MAX_TSO_SIZE);\n}\n\n \nvoid ice_fill_rss_lut(u8 *lut, u16 rss_table_size, u16 rss_size)\n{\n\tu16 i;\n\n\tfor (i = 0; i < rss_table_size; i++)\n\t\tlut[i] = i % rss_size;\n}\n\n \nstatic struct ice_vsi *\nice_pf_vsi_setup(struct ice_pf *pf, struct ice_port_info *pi)\n{\n\tstruct ice_vsi_cfg_params params = {};\n\n\tparams.type = ICE_VSI_PF;\n\tparams.pi = pi;\n\tparams.flags = ICE_VSI_FLAG_INIT;\n\n\treturn ice_vsi_setup(pf, &params);\n}\n\nstatic struct ice_vsi *\nice_chnl_vsi_setup(struct ice_pf *pf, struct ice_port_info *pi,\n\t\t   struct ice_channel *ch)\n{\n\tstruct ice_vsi_cfg_params params = {};\n\n\tparams.type = ICE_VSI_CHNL;\n\tparams.pi = pi;\n\tparams.ch = ch;\n\tparams.flags = ICE_VSI_FLAG_INIT;\n\n\treturn ice_vsi_setup(pf, &params);\n}\n\n \nstatic struct ice_vsi *\nice_ctrl_vsi_setup(struct ice_pf *pf, struct ice_port_info *pi)\n{\n\tstruct ice_vsi_cfg_params params = {};\n\n\tparams.type = ICE_VSI_CTRL;\n\tparams.pi = pi;\n\tparams.flags = ICE_VSI_FLAG_INIT;\n\n\treturn ice_vsi_setup(pf, &params);\n}\n\n \nstruct ice_vsi *\nice_lb_vsi_setup(struct ice_pf *pf, struct ice_port_info *pi)\n{\n\tstruct ice_vsi_cfg_params params = {};\n\n\tparams.type = ICE_VSI_LB;\n\tparams.pi = pi;\n\tparams.flags = ICE_VSI_FLAG_INIT;\n\n\treturn ice_vsi_setup(pf, &params);\n}\n\n \nstatic int\nice_vlan_rx_add_vid(struct net_device *netdev, __be16 proto, u16 vid)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi_vlan_ops *vlan_ops;\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_vlan vlan;\n\tint ret;\n\n\t \n\tif (!vid)\n\t\treturn 0;\n\n\twhile (test_and_set_bit(ICE_CFG_BUSY, vsi->state))\n\t\tusleep_range(1000, 2000);\n\n\t \n\tif (vsi->current_netdev_flags & IFF_ALLMULTI) {\n\t\tret = ice_fltr_set_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t\t       ICE_MCAST_VLAN_PROMISC_BITS,\n\t\t\t\t\t       vid);\n\t\tif (ret)\n\t\t\tgoto finish;\n\t}\n\n\tvlan_ops = ice_get_compat_vsi_vlan_ops(vsi);\n\n\t \n\tvlan = ICE_VLAN(be16_to_cpu(proto), vid, 0);\n\tret = vlan_ops->add_vlan(vsi, &vlan);\n\tif (ret)\n\t\tgoto finish;\n\n\t \n\tif ((vsi->current_netdev_flags & IFF_ALLMULTI) &&\n\t    ice_vsi_num_non_zero_vlans(vsi) == 1) {\n\t\tice_fltr_clear_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t\t   ICE_MCAST_PROMISC_BITS, 0);\n\t\tice_fltr_set_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t\t ICE_MCAST_VLAN_PROMISC_BITS, 0);\n\t}\n\nfinish:\n\tclear_bit(ICE_CFG_BUSY, vsi->state);\n\n\treturn ret;\n}\n\n \nstatic int\nice_vlan_rx_kill_vid(struct net_device *netdev, __be16 proto, u16 vid)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi_vlan_ops *vlan_ops;\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_vlan vlan;\n\tint ret;\n\n\t \n\tif (!vid)\n\t\treturn 0;\n\n\twhile (test_and_set_bit(ICE_CFG_BUSY, vsi->state))\n\t\tusleep_range(1000, 2000);\n\n\tret = ice_clear_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t    ICE_MCAST_VLAN_PROMISC_BITS, vid);\n\tif (ret) {\n\t\tnetdev_err(netdev, \"Error clearing multicast promiscuous mode on VSI %i\\n\",\n\t\t\t   vsi->vsi_num);\n\t\tvsi->current_netdev_flags |= IFF_ALLMULTI;\n\t}\n\n\tvlan_ops = ice_get_compat_vsi_vlan_ops(vsi);\n\n\t \n\tvlan = ICE_VLAN(be16_to_cpu(proto), vid, 0);\n\tret = vlan_ops->del_vlan(vsi, &vlan);\n\tif (ret)\n\t\tgoto finish;\n\n\t \n\tif (vsi->current_netdev_flags & IFF_ALLMULTI)\n\t\tice_fltr_clear_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t\t   ICE_MCAST_VLAN_PROMISC_BITS, vid);\n\n\tif (!ice_vsi_has_non_zero_vlans(vsi)) {\n\t\t \n\t\tif (vsi->current_netdev_flags & IFF_ALLMULTI) {\n\t\t\tice_fltr_clear_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t\t\t   ICE_MCAST_VLAN_PROMISC_BITS,\n\t\t\t\t\t\t   0);\n\t\t\tice_fltr_set_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t\t\t ICE_MCAST_PROMISC_BITS, 0);\n\t\t}\n\t}\n\nfinish:\n\tclear_bit(ICE_CFG_BUSY, vsi->state);\n\n\treturn ret;\n}\n\n \nstatic void ice_rep_indr_tc_block_unbind(void *cb_priv)\n{\n\tstruct ice_indr_block_priv *indr_priv = cb_priv;\n\n\tlist_del(&indr_priv->list);\n\tkfree(indr_priv);\n}\n\n \nstatic void ice_tc_indir_block_unregister(struct ice_vsi *vsi)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(vsi->netdev);\n\n\tflow_indr_dev_unregister(ice_indr_setup_tc_cb, np,\n\t\t\t\t ice_rep_indr_tc_block_unbind);\n}\n\n \nstatic int ice_tc_indir_block_register(struct ice_vsi *vsi)\n{\n\tstruct ice_netdev_priv *np;\n\n\tif (!vsi || !vsi->netdev)\n\t\treturn -EINVAL;\n\n\tnp = netdev_priv(vsi->netdev);\n\n\tINIT_LIST_HEAD(&np->tc_indr_block_priv_list);\n\treturn flow_indr_dev_register(ice_indr_setup_tc_cb, np);\n}\n\n \nstatic u16\nice_get_avail_q_count(unsigned long *pf_qmap, struct mutex *lock, u16 size)\n{\n\tunsigned long bit;\n\tu16 count = 0;\n\n\tmutex_lock(lock);\n\tfor_each_clear_bit(bit, pf_qmap, size)\n\t\tcount++;\n\tmutex_unlock(lock);\n\n\treturn count;\n}\n\n \nu16 ice_get_avail_txq_count(struct ice_pf *pf)\n{\n\treturn ice_get_avail_q_count(pf->avail_txqs, &pf->avail_q_mutex,\n\t\t\t\t     pf->max_pf_txqs);\n}\n\n \nu16 ice_get_avail_rxq_count(struct ice_pf *pf)\n{\n\treturn ice_get_avail_q_count(pf->avail_rxqs, &pf->avail_q_mutex,\n\t\t\t\t     pf->max_pf_rxqs);\n}\n\n \nstatic void ice_deinit_pf(struct ice_pf *pf)\n{\n\tice_service_task_stop(pf);\n\tmutex_destroy(&pf->lag_mutex);\n\tmutex_destroy(&pf->adev_mutex);\n\tmutex_destroy(&pf->sw_mutex);\n\tmutex_destroy(&pf->tc_mutex);\n\tmutex_destroy(&pf->avail_q_mutex);\n\tmutex_destroy(&pf->vfs.table_lock);\n\n\tif (pf->avail_txqs) {\n\t\tbitmap_free(pf->avail_txqs);\n\t\tpf->avail_txqs = NULL;\n\t}\n\n\tif (pf->avail_rxqs) {\n\t\tbitmap_free(pf->avail_rxqs);\n\t\tpf->avail_rxqs = NULL;\n\t}\n\n\tif (pf->ptp.clock)\n\t\tptp_clock_unregister(pf->ptp.clock);\n}\n\n \nstatic void ice_set_pf_caps(struct ice_pf *pf)\n{\n\tstruct ice_hw_func_caps *func_caps = &pf->hw.func_caps;\n\n\tclear_bit(ICE_FLAG_RDMA_ENA, pf->flags);\n\tif (func_caps->common_cap.rdma)\n\t\tset_bit(ICE_FLAG_RDMA_ENA, pf->flags);\n\tclear_bit(ICE_FLAG_DCB_CAPABLE, pf->flags);\n\tif (func_caps->common_cap.dcb)\n\t\tset_bit(ICE_FLAG_DCB_CAPABLE, pf->flags);\n\tclear_bit(ICE_FLAG_SRIOV_CAPABLE, pf->flags);\n\tif (func_caps->common_cap.sr_iov_1_1) {\n\t\tset_bit(ICE_FLAG_SRIOV_CAPABLE, pf->flags);\n\t\tpf->vfs.num_supported = min_t(int, func_caps->num_allocd_vfs,\n\t\t\t\t\t      ICE_MAX_SRIOV_VFS);\n\t}\n\tclear_bit(ICE_FLAG_RSS_ENA, pf->flags);\n\tif (func_caps->common_cap.rss_table_size)\n\t\tset_bit(ICE_FLAG_RSS_ENA, pf->flags);\n\n\tclear_bit(ICE_FLAG_FD_ENA, pf->flags);\n\tif (func_caps->fd_fltr_guar > 0 || func_caps->fd_fltr_best_effort > 0) {\n\t\tu16 unused;\n\n\t\t \n\t\tpf->ctrl_vsi_idx = ICE_NO_VSI;\n\t\tset_bit(ICE_FLAG_FD_ENA, pf->flags);\n\t\t \n\t\tice_alloc_fd_guar_item(&pf->hw, &unused,\n\t\t\t\t       func_caps->fd_fltr_guar);\n\t\t \n\t\tice_alloc_fd_shrd_item(&pf->hw, &unused,\n\t\t\t\t       func_caps->fd_fltr_best_effort);\n\t}\n\n\tclear_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags);\n\tif (func_caps->common_cap.ieee_1588)\n\t\tset_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags);\n\n\tpf->max_pf_txqs = func_caps->common_cap.num_txq;\n\tpf->max_pf_rxqs = func_caps->common_cap.num_rxq;\n}\n\n \nstatic int ice_init_pf(struct ice_pf *pf)\n{\n\tice_set_pf_caps(pf);\n\n\tmutex_init(&pf->sw_mutex);\n\tmutex_init(&pf->tc_mutex);\n\tmutex_init(&pf->adev_mutex);\n\tmutex_init(&pf->lag_mutex);\n\n\tINIT_HLIST_HEAD(&pf->aq_wait_list);\n\tspin_lock_init(&pf->aq_wait_lock);\n\tinit_waitqueue_head(&pf->aq_wait_queue);\n\n\tinit_waitqueue_head(&pf->reset_wait_queue);\n\n\t \n\ttimer_setup(&pf->serv_tmr, ice_service_timer, 0);\n\tpf->serv_tmr_period = HZ;\n\tINIT_WORK(&pf->serv_task, ice_service_task);\n\tclear_bit(ICE_SERVICE_SCHED, pf->state);\n\n\tmutex_init(&pf->avail_q_mutex);\n\tpf->avail_txqs = bitmap_zalloc(pf->max_pf_txqs, GFP_KERNEL);\n\tif (!pf->avail_txqs)\n\t\treturn -ENOMEM;\n\n\tpf->avail_rxqs = bitmap_zalloc(pf->max_pf_rxqs, GFP_KERNEL);\n\tif (!pf->avail_rxqs) {\n\t\tbitmap_free(pf->avail_txqs);\n\t\tpf->avail_txqs = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_init(&pf->vfs.table_lock);\n\thash_init(pf->vfs.table);\n\tice_mbx_init_snapshot(&pf->hw);\n\n\treturn 0;\n}\n\n \nbool ice_is_wol_supported(struct ice_hw *hw)\n{\n\tu16 wol_ctrl;\n\n\t \n\tif (ice_read_sr_word(hw, ICE_SR_NVM_WOL_CFG, &wol_ctrl))\n\t\treturn false;\n\n\treturn !(BIT(hw->port_info->lport) & wol_ctrl);\n}\n\n \nint ice_vsi_recfg_qs(struct ice_vsi *vsi, int new_rx, int new_tx, bool locked)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tint err = 0, timeout = 50;\n\n\tif (!new_rx && !new_tx)\n\t\treturn -EINVAL;\n\n\twhile (test_and_set_bit(ICE_CFG_BUSY, pf->state)) {\n\t\ttimeout--;\n\t\tif (!timeout)\n\t\t\treturn -EBUSY;\n\t\tusleep_range(1000, 2000);\n\t}\n\n\tif (new_tx)\n\t\tvsi->req_txq = (u16)new_tx;\n\tif (new_rx)\n\t\tvsi->req_rxq = (u16)new_rx;\n\n\t \n\tif (!netif_running(vsi->netdev)) {\n\t\tice_vsi_rebuild(vsi, ICE_VSI_FLAG_NO_INIT);\n\t\tdev_dbg(ice_pf_to_dev(pf), \"Link is down, queue count change happens when link is brought up\\n\");\n\t\tgoto done;\n\t}\n\n\tice_vsi_close(vsi);\n\tice_vsi_rebuild(vsi, ICE_VSI_FLAG_NO_INIT);\n\tice_pf_dcb_recfg(pf, locked);\n\tice_vsi_open(vsi);\ndone:\n\tclear_bit(ICE_CFG_BUSY, pf->state);\n\treturn err;\n}\n\n \nstatic void ice_set_safe_mode_vlan_cfg(struct ice_pf *pf)\n{\n\tstruct ice_vsi *vsi = ice_get_main_vsi(pf);\n\tstruct ice_vsi_ctx *ctxt;\n\tstruct ice_hw *hw;\n\tint status;\n\n\tif (!vsi)\n\t\treturn;\n\n\tctxt = kzalloc(sizeof(*ctxt), GFP_KERNEL);\n\tif (!ctxt)\n\t\treturn;\n\n\thw = &pf->hw;\n\tctxt->info = vsi->info;\n\n\tctxt->info.valid_sections =\n\t\tcpu_to_le16(ICE_AQ_VSI_PROP_VLAN_VALID |\n\t\t\t    ICE_AQ_VSI_PROP_SECURITY_VALID |\n\t\t\t    ICE_AQ_VSI_PROP_SW_VALID);\n\n\t \n\tctxt->info.sec_flags &= ~(ICE_AQ_VSI_SEC_TX_VLAN_PRUNE_ENA <<\n\t\t\t\t  ICE_AQ_VSI_SEC_TX_PRUNE_ENA_S);\n\n\t \n\tctxt->info.sw_flags2 &= ~ICE_AQ_VSI_SW_FLAG_RX_VLAN_PRUNE_ENA;\n\n\t \n\tctxt->info.inner_vlan_flags = ICE_AQ_VSI_INNER_VLAN_TX_MODE_ALL |\n\t\tICE_AQ_VSI_INNER_VLAN_EMODE_NOTHING;\n\n\tstatus = ice_update_vsi(hw, vsi->idx, ctxt, NULL);\n\tif (status) {\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Failed to update VSI for safe mode VLANs, err %d aq_err %s\\n\",\n\t\t\tstatus, ice_aq_str(hw->adminq.sq_last_status));\n\t} else {\n\t\tvsi->info.sec_flags = ctxt->info.sec_flags;\n\t\tvsi->info.sw_flags2 = ctxt->info.sw_flags2;\n\t\tvsi->info.inner_vlan_flags = ctxt->info.inner_vlan_flags;\n\t}\n\n\tkfree(ctxt);\n}\n\n \nstatic void ice_log_pkg_init(struct ice_hw *hw, enum ice_ddp_state state)\n{\n\tstruct ice_pf *pf = hw->back;\n\tstruct device *dev;\n\n\tdev = ice_pf_to_dev(pf);\n\n\tswitch (state) {\n\tcase ICE_DDP_PKG_SUCCESS:\n\t\tdev_info(dev, \"The DDP package was successfully loaded: %s version %d.%d.%d.%d\\n\",\n\t\t\t hw->active_pkg_name,\n\t\t\t hw->active_pkg_ver.major,\n\t\t\t hw->active_pkg_ver.minor,\n\t\t\t hw->active_pkg_ver.update,\n\t\t\t hw->active_pkg_ver.draft);\n\t\tbreak;\n\tcase ICE_DDP_PKG_SAME_VERSION_ALREADY_LOADED:\n\t\tdev_info(dev, \"DDP package already present on device: %s version %d.%d.%d.%d\\n\",\n\t\t\t hw->active_pkg_name,\n\t\t\t hw->active_pkg_ver.major,\n\t\t\t hw->active_pkg_ver.minor,\n\t\t\t hw->active_pkg_ver.update,\n\t\t\t hw->active_pkg_ver.draft);\n\t\tbreak;\n\tcase ICE_DDP_PKG_ALREADY_LOADED_NOT_SUPPORTED:\n\t\tdev_err(dev, \"The device has a DDP package that is not supported by the driver.  The device has package '%s' version %d.%d.x.x.  The driver requires version %d.%d.x.x.  Entering Safe Mode.\\n\",\n\t\t\thw->active_pkg_name,\n\t\t\thw->active_pkg_ver.major,\n\t\t\thw->active_pkg_ver.minor,\n\t\t\tICE_PKG_SUPP_VER_MAJ, ICE_PKG_SUPP_VER_MNR);\n\t\tbreak;\n\tcase ICE_DDP_PKG_COMPATIBLE_ALREADY_LOADED:\n\t\tdev_info(dev, \"The driver could not load the DDP package file because a compatible DDP package is already present on the device.  The device has package '%s' version %d.%d.%d.%d.  The package file found by the driver: '%s' version %d.%d.%d.%d.\\n\",\n\t\t\t hw->active_pkg_name,\n\t\t\t hw->active_pkg_ver.major,\n\t\t\t hw->active_pkg_ver.minor,\n\t\t\t hw->active_pkg_ver.update,\n\t\t\t hw->active_pkg_ver.draft,\n\t\t\t hw->pkg_name,\n\t\t\t hw->pkg_ver.major,\n\t\t\t hw->pkg_ver.minor,\n\t\t\t hw->pkg_ver.update,\n\t\t\t hw->pkg_ver.draft);\n\t\tbreak;\n\tcase ICE_DDP_PKG_FW_MISMATCH:\n\t\tdev_err(dev, \"The firmware loaded on the device is not compatible with the DDP package.  Please update the device's NVM.  Entering safe mode.\\n\");\n\t\tbreak;\n\tcase ICE_DDP_PKG_INVALID_FILE:\n\t\tdev_err(dev, \"The DDP package file is invalid. Entering Safe Mode.\\n\");\n\t\tbreak;\n\tcase ICE_DDP_PKG_FILE_VERSION_TOO_HIGH:\n\t\tdev_err(dev, \"The DDP package file version is higher than the driver supports.  Please use an updated driver.  Entering Safe Mode.\\n\");\n\t\tbreak;\n\tcase ICE_DDP_PKG_FILE_VERSION_TOO_LOW:\n\t\tdev_err(dev, \"The DDP package file version is lower than the driver supports.  The driver requires version %d.%d.x.x.  Please use an updated DDP Package file.  Entering Safe Mode.\\n\",\n\t\t\tICE_PKG_SUPP_VER_MAJ, ICE_PKG_SUPP_VER_MNR);\n\t\tbreak;\n\tcase ICE_DDP_PKG_FILE_SIGNATURE_INVALID:\n\t\tdev_err(dev, \"The DDP package could not be loaded because its signature is not valid.  Please use a valid DDP Package.  Entering Safe Mode.\\n\");\n\t\tbreak;\n\tcase ICE_DDP_PKG_FILE_REVISION_TOO_LOW:\n\t\tdev_err(dev, \"The DDP Package could not be loaded because its security revision is too low.  Please use an updated DDP Package.  Entering Safe Mode.\\n\");\n\t\tbreak;\n\tcase ICE_DDP_PKG_LOAD_ERROR:\n\t\tdev_err(dev, \"An error occurred on the device while loading the DDP package.  The device will be reset.\\n\");\n\t\t \n\t\tif (ice_check_reset(hw))\n\t\t\tdev_err(dev, \"Error resetting device. Please reload the driver\\n\");\n\t\tbreak;\n\tcase ICE_DDP_PKG_ERR:\n\tdefault:\n\t\tdev_err(dev, \"An unknown error occurred when loading the DDP package.  Entering Safe Mode.\\n\");\n\t\tbreak;\n\t}\n}\n\n \nstatic void\nice_load_pkg(const struct firmware *firmware, struct ice_pf *pf)\n{\n\tenum ice_ddp_state state = ICE_DDP_PKG_ERR;\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_hw *hw = &pf->hw;\n\n\t \n\tif (firmware && !hw->pkg_copy) {\n\t\tstate = ice_copy_and_init_pkg(hw, firmware->data,\n\t\t\t\t\t      firmware->size);\n\t\tice_log_pkg_init(hw, state);\n\t} else if (!firmware && hw->pkg_copy) {\n\t\t \n\t\tstate = ice_init_pkg(hw, hw->pkg_copy, hw->pkg_size);\n\t\tice_log_pkg_init(hw, state);\n\t} else {\n\t\tdev_err(dev, \"The DDP package file failed to load. Entering Safe Mode.\\n\");\n\t}\n\n\tif (!ice_is_init_pkg_successful(state)) {\n\t\t \n\t\tclear_bit(ICE_FLAG_ADV_FEATURES, pf->flags);\n\t\treturn;\n\t}\n\n\t \n\tset_bit(ICE_FLAG_ADV_FEATURES, pf->flags);\n}\n\n \nstatic void ice_verify_cacheline_size(struct ice_pf *pf)\n{\n\tif (rd32(&pf->hw, GLPCI_CNF2) & GLPCI_CNF2_CACHELINE_SIZE_M)\n\t\tdev_warn(ice_pf_to_dev(pf), \"%d Byte cache line assumption is invalid, driver may have Tx timeouts!\\n\",\n\t\t\t ICE_CACHE_LINE_BYTES);\n}\n\n \nstatic int ice_send_version(struct ice_pf *pf)\n{\n\tstruct ice_driver_ver dv;\n\n\tdv.major_ver = 0xff;\n\tdv.minor_ver = 0xff;\n\tdv.build_ver = 0xff;\n\tdv.subbuild_ver = 0;\n\tstrscpy((char *)dv.driver_string, UTS_RELEASE,\n\t\tsizeof(dv.driver_string));\n\treturn ice_aq_send_driver_ver(&pf->hw, &dv, NULL);\n}\n\n \nstatic int ice_init_fdir(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_vsi *ctrl_vsi;\n\tint err;\n\n\t \n\tctrl_vsi = ice_ctrl_vsi_setup(pf, pf->hw.port_info);\n\tif (!ctrl_vsi) {\n\t\tdev_dbg(dev, \"could not create control VSI\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\terr = ice_vsi_open_ctrl(ctrl_vsi);\n\tif (err) {\n\t\tdev_dbg(dev, \"could not open control VSI\\n\");\n\t\tgoto err_vsi_open;\n\t}\n\n\tmutex_init(&pf->hw.fdir_fltr_lock);\n\n\terr = ice_fdir_create_dflt_rules(pf);\n\tif (err)\n\t\tgoto err_fdir_rule;\n\n\treturn 0;\n\nerr_fdir_rule:\n\tice_fdir_release_flows(&pf->hw);\n\tice_vsi_close(ctrl_vsi);\nerr_vsi_open:\n\tice_vsi_release(ctrl_vsi);\n\tif (pf->ctrl_vsi_idx != ICE_NO_VSI) {\n\t\tpf->vsi[pf->ctrl_vsi_idx] = NULL;\n\t\tpf->ctrl_vsi_idx = ICE_NO_VSI;\n\t}\n\treturn err;\n}\n\nstatic void ice_deinit_fdir(struct ice_pf *pf)\n{\n\tstruct ice_vsi *vsi = ice_get_ctrl_vsi(pf);\n\n\tif (!vsi)\n\t\treturn;\n\n\tice_vsi_manage_fdir(vsi, false);\n\tice_vsi_release(vsi);\n\tif (pf->ctrl_vsi_idx != ICE_NO_VSI) {\n\t\tpf->vsi[pf->ctrl_vsi_idx] = NULL;\n\t\tpf->ctrl_vsi_idx = ICE_NO_VSI;\n\t}\n\n\tmutex_destroy(&(&pf->hw)->fdir_fltr_lock);\n}\n\n \nstatic char *ice_get_opt_fw_name(struct ice_pf *pf)\n{\n\t \n\tstruct pci_dev *pdev = pf->pdev;\n\tchar *opt_fw_filename;\n\tu64 dsn;\n\n\t \n\tdsn = pci_get_dsn(pdev);\n\tif (!dsn)\n\t\treturn NULL;\n\n\topt_fw_filename = kzalloc(NAME_MAX, GFP_KERNEL);\n\tif (!opt_fw_filename)\n\t\treturn NULL;\n\n\tsnprintf(opt_fw_filename, NAME_MAX, \"%sice-%016llx.pkg\",\n\t\t ICE_DDP_PKG_PATH, dsn);\n\n\treturn opt_fw_filename;\n}\n\n \nstatic void ice_request_fw(struct ice_pf *pf)\n{\n\tchar *opt_fw_filename = ice_get_opt_fw_name(pf);\n\tconst struct firmware *firmware = NULL;\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tint err = 0;\n\n\t \n\tif (opt_fw_filename) {\n\t\terr = firmware_request_nowarn(&firmware, opt_fw_filename, dev);\n\t\tif (err) {\n\t\t\tkfree(opt_fw_filename);\n\t\t\tgoto dflt_pkg_load;\n\t\t}\n\n\t\t \n\t\tice_load_pkg(firmware, pf);\n\t\tkfree(opt_fw_filename);\n\t\trelease_firmware(firmware);\n\t\treturn;\n\t}\n\ndflt_pkg_load:\n\terr = request_firmware(&firmware, ICE_DDP_PKG_FILE, dev);\n\tif (err) {\n\t\tdev_err(dev, \"The DDP package file was not found or could not be read. Entering Safe Mode\\n\");\n\t\treturn;\n\t}\n\n\t \n\tice_load_pkg(firmware, pf);\n\trelease_firmware(firmware);\n}\n\n \nstatic void ice_print_wake_reason(struct ice_pf *pf)\n{\n\tu32 wus = pf->wakeup_reason;\n\tconst char *wake_str;\n\n\t \n\tif (!wus)\n\t\treturn;\n\n\tif (wus & PFPM_WUS_LNKC_M)\n\t\twake_str = \"Link\\n\";\n\telse if (wus & PFPM_WUS_MAG_M)\n\t\twake_str = \"Magic Packet\\n\";\n\telse if (wus & PFPM_WUS_MNG_M)\n\t\twake_str = \"Management\\n\";\n\telse if (wus & PFPM_WUS_FW_RST_WK_M)\n\t\twake_str = \"Firmware Reset\\n\";\n\telse\n\t\twake_str = \"Unknown\\n\";\n\n\tdev_info(ice_pf_to_dev(pf), \"Wake reason: %s\", wake_str);\n}\n\n \nstatic int ice_register_netdev(struct ice_vsi *vsi)\n{\n\tint err;\n\n\tif (!vsi || !vsi->netdev)\n\t\treturn -EIO;\n\n\terr = register_netdev(vsi->netdev);\n\tif (err)\n\t\treturn err;\n\n\tset_bit(ICE_VSI_NETDEV_REGISTERED, vsi->state);\n\tnetif_carrier_off(vsi->netdev);\n\tnetif_tx_stop_all_queues(vsi->netdev);\n\n\treturn 0;\n}\n\nstatic void ice_unregister_netdev(struct ice_vsi *vsi)\n{\n\tif (!vsi || !vsi->netdev)\n\t\treturn;\n\n\tunregister_netdev(vsi->netdev);\n\tclear_bit(ICE_VSI_NETDEV_REGISTERED, vsi->state);\n}\n\n \nstatic int ice_cfg_netdev(struct ice_vsi *vsi)\n{\n\tstruct ice_netdev_priv *np;\n\tstruct net_device *netdev;\n\tu8 mac_addr[ETH_ALEN];\n\n\tnetdev = alloc_etherdev_mqs(sizeof(*np), vsi->alloc_txq,\n\t\t\t\t    vsi->alloc_rxq);\n\tif (!netdev)\n\t\treturn -ENOMEM;\n\n\tset_bit(ICE_VSI_NETDEV_ALLOCD, vsi->state);\n\tvsi->netdev = netdev;\n\tnp = netdev_priv(netdev);\n\tnp->vsi = vsi;\n\n\tice_set_netdev_features(netdev);\n\tice_set_ops(vsi);\n\n\tif (vsi->type == ICE_VSI_PF) {\n\t\tSET_NETDEV_DEV(netdev, ice_pf_to_dev(vsi->back));\n\t\tether_addr_copy(mac_addr, vsi->port_info->mac.perm_addr);\n\t\teth_hw_addr_set(netdev, mac_addr);\n\t}\n\n\tnetdev->priv_flags |= IFF_UNICAST_FLT;\n\n\t \n\tice_vsi_cfg_netdev_tc(vsi, vsi->tc_cfg.ena_tc);\n\n\tnetdev->max_mtu = ICE_MAX_MTU;\n\n\treturn 0;\n}\n\nstatic void ice_decfg_netdev(struct ice_vsi *vsi)\n{\n\tclear_bit(ICE_VSI_NETDEV_ALLOCD, vsi->state);\n\tfree_netdev(vsi->netdev);\n\tvsi->netdev = NULL;\n}\n\nstatic int ice_start_eth(struct ice_vsi *vsi)\n{\n\tint err;\n\n\terr = ice_init_mac_fltr(vsi->back);\n\tif (err)\n\t\treturn err;\n\n\terr = ice_vsi_open(vsi);\n\tif (err)\n\t\tice_fltr_remove_all(vsi);\n\n\treturn err;\n}\n\nstatic void ice_stop_eth(struct ice_vsi *vsi)\n{\n\tice_fltr_remove_all(vsi);\n\tice_vsi_close(vsi);\n}\n\nstatic int ice_init_eth(struct ice_pf *pf)\n{\n\tstruct ice_vsi *vsi = ice_get_main_vsi(pf);\n\tint err;\n\n\tif (!vsi)\n\t\treturn -EINVAL;\n\n\t \n\tINIT_LIST_HEAD(&vsi->ch_list);\n\n\terr = ice_cfg_netdev(vsi);\n\tif (err)\n\t\treturn err;\n\t \n\tice_dcbnl_setup(vsi);\n\n\terr = ice_init_mac_fltr(pf);\n\tif (err)\n\t\tgoto err_init_mac_fltr;\n\n\terr = ice_devlink_create_pf_port(pf);\n\tif (err)\n\t\tgoto err_devlink_create_pf_port;\n\n\tSET_NETDEV_DEVLINK_PORT(vsi->netdev, &pf->devlink_port);\n\n\terr = ice_register_netdev(vsi);\n\tif (err)\n\t\tgoto err_register_netdev;\n\n\terr = ice_tc_indir_block_register(vsi);\n\tif (err)\n\t\tgoto err_tc_indir_block_register;\n\n\tice_napi_add(vsi);\n\n\treturn 0;\n\nerr_tc_indir_block_register:\n\tice_unregister_netdev(vsi);\nerr_register_netdev:\n\tice_devlink_destroy_pf_port(pf);\nerr_devlink_create_pf_port:\nerr_init_mac_fltr:\n\tice_decfg_netdev(vsi);\n\treturn err;\n}\n\nstatic void ice_deinit_eth(struct ice_pf *pf)\n{\n\tstruct ice_vsi *vsi = ice_get_main_vsi(pf);\n\n\tif (!vsi)\n\t\treturn;\n\n\tice_vsi_close(vsi);\n\tice_unregister_netdev(vsi);\n\tice_devlink_destroy_pf_port(pf);\n\tice_tc_indir_block_unregister(vsi);\n\tice_decfg_netdev(vsi);\n}\n\n \nstatic int ice_wait_for_fw(struct ice_hw *hw, u32 timeout)\n{\n\tint fw_loading;\n\tu32 elapsed = 0;\n\n\twhile (elapsed <= timeout) {\n\t\tfw_loading = rd32(hw, GL_MNG_FWSM) & GL_MNG_FWSM_FW_LOADING_M;\n\n\t\t \n\t\tif (fw_loading) {\n\t\t\telapsed += 100;\n\t\t\tmsleep(100);\n\t\t\tcontinue;\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -ETIMEDOUT;\n}\n\nstatic int ice_init_dev(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_hw *hw = &pf->hw;\n\tint err;\n\n\terr = ice_init_hw(hw);\n\tif (err) {\n\t\tdev_err(dev, \"ice_init_hw failed: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\t \n\tif (ice_is_pf_c827(hw)) {\n\t\terr = ice_wait_for_fw(hw, 30000);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"ice_wait_for_fw timed out\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tice_init_feature_support(pf);\n\n\tice_request_fw(pf);\n\n\t \n\tif (ice_is_safe_mode(pf)) {\n\t\t \n\t\tice_set_safe_mode_caps(hw);\n\t}\n\n\terr = ice_init_pf(pf);\n\tif (err) {\n\t\tdev_err(dev, \"ice_init_pf failed: %d\\n\", err);\n\t\tgoto err_init_pf;\n\t}\n\n\tpf->hw.udp_tunnel_nic.set_port = ice_udp_tunnel_set_port;\n\tpf->hw.udp_tunnel_nic.unset_port = ice_udp_tunnel_unset_port;\n\tpf->hw.udp_tunnel_nic.flags = UDP_TUNNEL_NIC_INFO_MAY_SLEEP;\n\tpf->hw.udp_tunnel_nic.shared = &pf->hw.udp_tunnel_shared;\n\tif (pf->hw.tnl.valid_count[TNL_VXLAN]) {\n\t\tpf->hw.udp_tunnel_nic.tables[0].n_entries =\n\t\t\tpf->hw.tnl.valid_count[TNL_VXLAN];\n\t\tpf->hw.udp_tunnel_nic.tables[0].tunnel_types =\n\t\t\tUDP_TUNNEL_TYPE_VXLAN;\n\t}\n\tif (pf->hw.tnl.valid_count[TNL_GENEVE]) {\n\t\tpf->hw.udp_tunnel_nic.tables[1].n_entries =\n\t\t\tpf->hw.tnl.valid_count[TNL_GENEVE];\n\t\tpf->hw.udp_tunnel_nic.tables[1].tunnel_types =\n\t\t\tUDP_TUNNEL_TYPE_GENEVE;\n\t}\n\n\terr = ice_init_interrupt_scheme(pf);\n\tif (err) {\n\t\tdev_err(dev, \"ice_init_interrupt_scheme failed: %d\\n\", err);\n\t\terr = -EIO;\n\t\tgoto err_init_interrupt_scheme;\n\t}\n\n\t \n\terr = ice_req_irq_msix_misc(pf);\n\tif (err) {\n\t\tdev_err(dev, \"setup of misc vector failed: %d\\n\", err);\n\t\tgoto err_req_irq_msix_misc;\n\t}\n\n\treturn 0;\n\nerr_req_irq_msix_misc:\n\tice_clear_interrupt_scheme(pf);\nerr_init_interrupt_scheme:\n\tice_deinit_pf(pf);\nerr_init_pf:\n\tice_deinit_hw(hw);\n\treturn err;\n}\n\nstatic void ice_deinit_dev(struct ice_pf *pf)\n{\n\tice_free_irq_msix_misc(pf);\n\tice_deinit_pf(pf);\n\tice_deinit_hw(&pf->hw);\n\n\t \n\tice_reset(&pf->hw, ICE_RESET_PFR);\n\tpci_wait_for_pending_transaction(pf->pdev);\n\tice_clear_interrupt_scheme(pf);\n}\n\nstatic void ice_init_features(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\n\tif (ice_is_safe_mode(pf))\n\t\treturn;\n\n\t \n\tif (test_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags))\n\t\tice_ptp_init(pf);\n\n\tif (ice_is_feature_supported(pf, ICE_F_GNSS))\n\t\tice_gnss_init(pf);\n\n\t \n\tif (ice_init_fdir(pf))\n\t\tdev_err(dev, \"could not initialize flow director\\n\");\n\n\t \n\tif (ice_init_pf_dcb(pf, false)) {\n\t\tclear_bit(ICE_FLAG_DCB_CAPABLE, pf->flags);\n\t\tclear_bit(ICE_FLAG_DCB_ENA, pf->flags);\n\t} else {\n\t\tice_cfg_lldp_mib_change(&pf->hw, true);\n\t}\n\n\tif (ice_init_lag(pf))\n\t\tdev_warn(dev, \"Failed to init link aggregation support\\n\");\n}\n\nstatic void ice_deinit_features(struct ice_pf *pf)\n{\n\tif (ice_is_safe_mode(pf))\n\t\treturn;\n\n\tice_deinit_lag(pf);\n\tif (test_bit(ICE_FLAG_DCB_CAPABLE, pf->flags))\n\t\tice_cfg_lldp_mib_change(&pf->hw, false);\n\tice_deinit_fdir(pf);\n\tif (ice_is_feature_supported(pf, ICE_F_GNSS))\n\t\tice_gnss_exit(pf);\n\tif (test_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags))\n\t\tice_ptp_release(pf);\n}\n\nstatic void ice_init_wakeup(struct ice_pf *pf)\n{\n\t \n\tpf->wakeup_reason = rd32(&pf->hw, PFPM_WUS);\n\n\t \n\tice_print_wake_reason(pf);\n\n\t \n\twr32(&pf->hw, PFPM_WUS, U32_MAX);\n\n\t \n\tdevice_set_wakeup_enable(ice_pf_to_dev(pf), false);\n}\n\nstatic int ice_init_link(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tint err;\n\n\terr = ice_init_link_events(pf->hw.port_info);\n\tif (err) {\n\t\tdev_err(dev, \"ice_init_link_events failed: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\t \n\terr = ice_init_nvm_phy_type(pf->hw.port_info);\n\tif (err)\n\t\tdev_err(dev, \"ice_init_nvm_phy_type failed: %d\\n\", err);\n\n\t \n\terr = ice_update_link_info(pf->hw.port_info);\n\tif (err)\n\t\tdev_err(dev, \"ice_update_link_info failed: %d\\n\", err);\n\n\tice_init_link_dflt_override(pf->hw.port_info);\n\n\tice_check_link_cfg_err(pf,\n\t\t\t       pf->hw.port_info->phy.link_info.link_cfg_err);\n\n\t \n\tif (pf->hw.port_info->phy.link_info.link_info &\n\t    ICE_AQ_MEDIA_AVAILABLE) {\n\t\t \n\t\terr = ice_init_phy_user_cfg(pf->hw.port_info);\n\t\tif (err)\n\t\t\tdev_err(dev, \"ice_init_phy_user_cfg failed: %d\\n\", err);\n\n\t\tif (!test_bit(ICE_FLAG_LINK_DOWN_ON_CLOSE_ENA, pf->flags)) {\n\t\t\tstruct ice_vsi *vsi = ice_get_main_vsi(pf);\n\n\t\t\tif (vsi)\n\t\t\t\tice_configure_phy(vsi);\n\t\t}\n\t} else {\n\t\tset_bit(ICE_FLAG_NO_MEDIA, pf->flags);\n\t}\n\n\treturn err;\n}\n\nstatic int ice_init_pf_sw(struct ice_pf *pf)\n{\n\tbool dvm = ice_is_dvm_ena(&pf->hw);\n\tstruct ice_vsi *vsi;\n\tint err;\n\n\t \n\tpf->first_sw = kzalloc(sizeof(*pf->first_sw), GFP_KERNEL);\n\tif (!pf->first_sw)\n\t\treturn -ENOMEM;\n\n\tif (pf->hw.evb_veb)\n\t\tpf->first_sw->bridge_mode = BRIDGE_MODE_VEB;\n\telse\n\t\tpf->first_sw->bridge_mode = BRIDGE_MODE_VEPA;\n\n\tpf->first_sw->pf = pf;\n\n\t \n\tpf->first_sw->sw_id = pf->hw.port_info->sw_id;\n\n\terr = ice_aq_set_port_params(pf->hw.port_info, dvm, NULL);\n\tif (err)\n\t\tgoto err_aq_set_port_params;\n\n\tvsi = ice_pf_vsi_setup(pf, pf->hw.port_info);\n\tif (!vsi) {\n\t\terr = -ENOMEM;\n\t\tgoto err_pf_vsi_setup;\n\t}\n\n\treturn 0;\n\nerr_pf_vsi_setup:\nerr_aq_set_port_params:\n\tkfree(pf->first_sw);\n\treturn err;\n}\n\nstatic void ice_deinit_pf_sw(struct ice_pf *pf)\n{\n\tstruct ice_vsi *vsi = ice_get_main_vsi(pf);\n\n\tif (!vsi)\n\t\treturn;\n\n\tice_vsi_release(vsi);\n\tkfree(pf->first_sw);\n}\n\nstatic int ice_alloc_vsis(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\n\tpf->num_alloc_vsi = pf->hw.func_caps.guar_num_vsi;\n\tif (!pf->num_alloc_vsi)\n\t\treturn -EIO;\n\n\tif (pf->num_alloc_vsi > UDP_TUNNEL_NIC_MAX_SHARING_DEVICES) {\n\t\tdev_warn(dev,\n\t\t\t \"limiting the VSI count due to UDP tunnel limitation %d > %d\\n\",\n\t\t\t pf->num_alloc_vsi, UDP_TUNNEL_NIC_MAX_SHARING_DEVICES);\n\t\tpf->num_alloc_vsi = UDP_TUNNEL_NIC_MAX_SHARING_DEVICES;\n\t}\n\n\tpf->vsi = devm_kcalloc(dev, pf->num_alloc_vsi, sizeof(*pf->vsi),\n\t\t\t       GFP_KERNEL);\n\tif (!pf->vsi)\n\t\treturn -ENOMEM;\n\n\tpf->vsi_stats = devm_kcalloc(dev, pf->num_alloc_vsi,\n\t\t\t\t     sizeof(*pf->vsi_stats), GFP_KERNEL);\n\tif (!pf->vsi_stats) {\n\t\tdevm_kfree(dev, pf->vsi);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void ice_dealloc_vsis(struct ice_pf *pf)\n{\n\tdevm_kfree(ice_pf_to_dev(pf), pf->vsi_stats);\n\tpf->vsi_stats = NULL;\n\n\tpf->num_alloc_vsi = 0;\n\tdevm_kfree(ice_pf_to_dev(pf), pf->vsi);\n\tpf->vsi = NULL;\n}\n\nstatic int ice_init_devlink(struct ice_pf *pf)\n{\n\tint err;\n\n\terr = ice_devlink_register_params(pf);\n\tif (err)\n\t\treturn err;\n\n\tice_devlink_init_regions(pf);\n\tice_devlink_register(pf);\n\n\treturn 0;\n}\n\nstatic void ice_deinit_devlink(struct ice_pf *pf)\n{\n\tice_devlink_unregister(pf);\n\tice_devlink_destroy_regions(pf);\n\tice_devlink_unregister_params(pf);\n}\n\nstatic int ice_init(struct ice_pf *pf)\n{\n\tint err;\n\n\terr = ice_init_dev(pf);\n\tif (err)\n\t\treturn err;\n\n\terr = ice_alloc_vsis(pf);\n\tif (err)\n\t\tgoto err_alloc_vsis;\n\n\terr = ice_init_pf_sw(pf);\n\tif (err)\n\t\tgoto err_init_pf_sw;\n\n\tice_init_wakeup(pf);\n\n\terr = ice_init_link(pf);\n\tif (err)\n\t\tgoto err_init_link;\n\n\terr = ice_send_version(pf);\n\tif (err)\n\t\tgoto err_init_link;\n\n\tice_verify_cacheline_size(pf);\n\n\tif (ice_is_safe_mode(pf))\n\t\tice_set_safe_mode_vlan_cfg(pf);\n\telse\n\t\t \n\t\tpcie_print_link_status(pf->pdev);\n\n\t \n\tclear_bit(ICE_DOWN, pf->state);\n\tclear_bit(ICE_SERVICE_DIS, pf->state);\n\n\t \n\tmod_timer(&pf->serv_tmr, round_jiffies(jiffies + pf->serv_tmr_period));\n\n\treturn 0;\n\nerr_init_link:\n\tice_deinit_pf_sw(pf);\nerr_init_pf_sw:\n\tice_dealloc_vsis(pf);\nerr_alloc_vsis:\n\tice_deinit_dev(pf);\n\treturn err;\n}\n\nstatic void ice_deinit(struct ice_pf *pf)\n{\n\tset_bit(ICE_SERVICE_DIS, pf->state);\n\tset_bit(ICE_DOWN, pf->state);\n\n\tice_deinit_pf_sw(pf);\n\tice_dealloc_vsis(pf);\n\tice_deinit_dev(pf);\n}\n\n \nint ice_load(struct ice_pf *pf)\n{\n\tstruct ice_vsi_cfg_params params = {};\n\tstruct ice_vsi *vsi;\n\tint err;\n\n\terr = ice_init_dev(pf);\n\tif (err)\n\t\treturn err;\n\n\tvsi = ice_get_main_vsi(pf);\n\n\tparams = ice_vsi_to_params(vsi);\n\tparams.flags = ICE_VSI_FLAG_INIT;\n\n\trtnl_lock();\n\terr = ice_vsi_cfg(vsi, &params);\n\tif (err)\n\t\tgoto err_vsi_cfg;\n\n\terr = ice_start_eth(ice_get_main_vsi(pf));\n\tif (err)\n\t\tgoto err_start_eth;\n\trtnl_unlock();\n\n\terr = ice_init_rdma(pf);\n\tif (err)\n\t\tgoto err_init_rdma;\n\n\tice_init_features(pf);\n\tice_service_task_restart(pf);\n\n\tclear_bit(ICE_DOWN, pf->state);\n\n\treturn 0;\n\nerr_init_rdma:\n\tice_vsi_close(ice_get_main_vsi(pf));\n\trtnl_lock();\nerr_start_eth:\n\tice_vsi_decfg(ice_get_main_vsi(pf));\nerr_vsi_cfg:\n\trtnl_unlock();\n\tice_deinit_dev(pf);\n\treturn err;\n}\n\n \nvoid ice_unload(struct ice_pf *pf)\n{\n\tice_deinit_features(pf);\n\tice_deinit_rdma(pf);\n\trtnl_lock();\n\tice_stop_eth(ice_get_main_vsi(pf));\n\tice_vsi_decfg(ice_get_main_vsi(pf));\n\trtnl_unlock();\n\tice_deinit_dev(pf);\n}\n\n \nstatic int\nice_probe(struct pci_dev *pdev, const struct pci_device_id __always_unused *ent)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct ice_pf *pf;\n\tstruct ice_hw *hw;\n\tint err;\n\n\tif (pdev->is_virtfn) {\n\t\tdev_err(dev, \"can't probe a virtual function\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (is_kdump_kernel()) {\n\t\tpci_save_state(pdev);\n\t\tpci_clear_master(pdev);\n\t\terr = pcie_flr(pdev);\n\t\tif (err)\n\t\t\treturn err;\n\t\tpci_restore_state(pdev);\n\t}\n\n\t \n\terr = pcim_enable_device(pdev);\n\tif (err)\n\t\treturn err;\n\n\terr = pcim_iomap_regions(pdev, BIT(ICE_BAR0), dev_driver_string(dev));\n\tif (err) {\n\t\tdev_err(dev, \"BAR0 I/O map error %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tpf = ice_allocate_pf(dev);\n\tif (!pf)\n\t\treturn -ENOMEM;\n\n\t \n\tpf->aux_idx = -1;\n\n\t \n\terr = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_err(dev, \"DMA configuration failed: 0x%x\\n\", err);\n\t\treturn err;\n\t}\n\n\tpci_set_master(pdev);\n\n\tpf->pdev = pdev;\n\tpci_set_drvdata(pdev, pf);\n\tset_bit(ICE_DOWN, pf->state);\n\t \n\tset_bit(ICE_SERVICE_DIS, pf->state);\n\n\thw = &pf->hw;\n\thw->hw_addr = pcim_iomap_table(pdev)[ICE_BAR0];\n\tpci_save_state(pdev);\n\n\thw->back = pf;\n\thw->port_info = NULL;\n\thw->vendor_id = pdev->vendor;\n\thw->device_id = pdev->device;\n\tpci_read_config_byte(pdev, PCI_REVISION_ID, &hw->revision_id);\n\thw->subsystem_vendor_id = pdev->subsystem_vendor;\n\thw->subsystem_device_id = pdev->subsystem_device;\n\thw->bus.device = PCI_SLOT(pdev->devfn);\n\thw->bus.func = PCI_FUNC(pdev->devfn);\n\tice_set_ctrlq_len(hw);\n\n\tpf->msg_enable = netif_msg_init(debug, ICE_DFLT_NETIF_M);\n\n#ifndef CONFIG_DYNAMIC_DEBUG\n\tif (debug < -1)\n\t\thw->debug_mask = debug;\n#endif\n\n\terr = ice_init(pf);\n\tif (err)\n\t\tgoto err_init;\n\n\terr = ice_init_eth(pf);\n\tif (err)\n\t\tgoto err_init_eth;\n\n\terr = ice_init_rdma(pf);\n\tif (err)\n\t\tgoto err_init_rdma;\n\n\terr = ice_init_devlink(pf);\n\tif (err)\n\t\tgoto err_init_devlink;\n\n\tice_init_features(pf);\n\n\treturn 0;\n\nerr_init_devlink:\n\tice_deinit_rdma(pf);\nerr_init_rdma:\n\tice_deinit_eth(pf);\nerr_init_eth:\n\tice_deinit(pf);\nerr_init:\n\tpci_disable_device(pdev);\n\treturn err;\n}\n\n \nstatic void ice_set_wake(struct ice_pf *pf)\n{\n\tstruct ice_hw *hw = &pf->hw;\n\tbool wol = pf->wol_ena;\n\n\t \n\twr32(hw, PFPM_WUS, U32_MAX);\n\n\t \n\twr32(hw, PFPM_APM, wol ? PFPM_APM_APME_M : 0);\n\n\t \n\twr32(hw, PFPM_WUFC, wol ? PFPM_WUFC_MAG_M : 0);\n}\n\n \nstatic void ice_setup_mc_magic_wake(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_hw *hw = &pf->hw;\n\tu8 mac_addr[ETH_ALEN];\n\tstruct ice_vsi *vsi;\n\tint status;\n\tu8 flags;\n\n\tif (!pf->wol_ena)\n\t\treturn;\n\n\tvsi = ice_get_main_vsi(pf);\n\tif (!vsi)\n\t\treturn;\n\n\t \n\tif (vsi->netdev)\n\t\tether_addr_copy(mac_addr, vsi->netdev->dev_addr);\n\telse\n\t\tether_addr_copy(mac_addr, vsi->port_info->mac.perm_addr);\n\n\tflags = ICE_AQC_MAN_MAC_WR_MC_MAG_EN |\n\t\tICE_AQC_MAN_MAC_UPDATE_LAA_WOL |\n\t\tICE_AQC_MAN_MAC_WR_WOL_LAA_PFR_KEEP;\n\n\tstatus = ice_aq_manage_mac_write(hw, mac_addr, flags, NULL);\n\tif (status)\n\t\tdev_err(dev, \"Failed to enable Multicast Magic Packet wake, err %d aq_err %s\\n\",\n\t\t\tstatus, ice_aq_str(hw->adminq.sq_last_status));\n}\n\n \nstatic void ice_remove(struct pci_dev *pdev)\n{\n\tstruct ice_pf *pf = pci_get_drvdata(pdev);\n\tint i;\n\n\tfor (i = 0; i < ICE_MAX_RESET_WAIT; i++) {\n\t\tif (!ice_is_reset_in_progress(pf->state))\n\t\t\tbreak;\n\t\tmsleep(100);\n\t}\n\n\tif (test_bit(ICE_FLAG_SRIOV_ENA, pf->flags)) {\n\t\tset_bit(ICE_VF_RESETS_DISABLED, pf->state);\n\t\tice_free_vfs(pf);\n\t}\n\n\tice_service_task_stop(pf);\n\tice_aq_cancel_waiting_tasks(pf);\n\tset_bit(ICE_DOWN, pf->state);\n\n\tif (!ice_is_safe_mode(pf))\n\t\tice_remove_arfs(pf);\n\tice_deinit_features(pf);\n\tice_deinit_devlink(pf);\n\tice_deinit_rdma(pf);\n\tice_deinit_eth(pf);\n\tice_deinit(pf);\n\n\tice_vsi_release_all(pf);\n\n\tice_setup_mc_magic_wake(pf);\n\tice_set_wake(pf);\n\n\tpci_disable_device(pdev);\n}\n\n \nstatic void ice_shutdown(struct pci_dev *pdev)\n{\n\tstruct ice_pf *pf = pci_get_drvdata(pdev);\n\n\tice_remove(pdev);\n\n\tif (system_state == SYSTEM_POWER_OFF) {\n\t\tpci_wake_from_d3(pdev, pf->wol_ena);\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n\t}\n}\n\n#ifdef CONFIG_PM\n \nstatic void ice_prepare_for_shutdown(struct ice_pf *pf)\n{\n\tstruct ice_hw *hw = &pf->hw;\n\tu32 v;\n\n\t \n\tif (ice_check_sq_alive(hw, &hw->mailboxq))\n\t\tice_vc_notify_reset(pf);\n\n\tdev_dbg(ice_pf_to_dev(pf), \"Tearing down internal switch for shutdown\\n\");\n\n\t \n\tice_pf_dis_all_vsi(pf, false);\n\n\tice_for_each_vsi(pf, v)\n\t\tif (pf->vsi[v])\n\t\t\tpf->vsi[v]->vsi_num = 0;\n\n\tice_shutdown_all_ctrlq(hw);\n}\n\n \nstatic int ice_reinit_interrupt_scheme(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tint ret, v;\n\n\t \n\n\tret = ice_init_interrupt_scheme(pf);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to re-initialize interrupt %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\t \n\tice_for_each_vsi(pf, v) {\n\t\tif (!pf->vsi[v])\n\t\t\tcontinue;\n\n\t\tret = ice_vsi_alloc_q_vectors(pf->vsi[v]);\n\t\tif (ret)\n\t\t\tgoto err_reinit;\n\t\tice_vsi_map_rings_to_vectors(pf->vsi[v]);\n\t}\n\n\tret = ice_req_irq_msix_misc(pf);\n\tif (ret) {\n\t\tdev_err(dev, \"Setting up misc vector failed after device suspend %d\\n\",\n\t\t\tret);\n\t\tgoto err_reinit;\n\t}\n\n\treturn 0;\n\nerr_reinit:\n\twhile (v--)\n\t\tif (pf->vsi[v])\n\t\t\tice_vsi_free_q_vectors(pf->vsi[v]);\n\n\treturn ret;\n}\n\n \nstatic int __maybe_unused ice_suspend(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct ice_pf *pf;\n\tint disabled, v;\n\n\tpf = pci_get_drvdata(pdev);\n\n\tif (!ice_pf_state_is_nominal(pf)) {\n\t\tdev_err(dev, \"Device is not ready, no need to suspend it\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tdisabled = ice_service_task_stop(pf);\n\n\tice_unplug_aux_dev(pf);\n\n\t \n\tif (test_and_set_bit(ICE_SUSPENDED, pf->state)) {\n\t\tif (!disabled)\n\t\t\tice_service_task_restart(pf);\n\t\treturn 0;\n\t}\n\n\tif (test_bit(ICE_DOWN, pf->state) ||\n\t    ice_is_reset_in_progress(pf->state)) {\n\t\tdev_err(dev, \"can't suspend device in reset or already down\\n\");\n\t\tif (!disabled)\n\t\t\tice_service_task_restart(pf);\n\t\treturn 0;\n\t}\n\n\tice_setup_mc_magic_wake(pf);\n\n\tice_prepare_for_shutdown(pf);\n\n\tice_set_wake(pf);\n\n\t \n\tice_free_irq_msix_misc(pf);\n\tice_for_each_vsi(pf, v) {\n\t\tif (!pf->vsi[v])\n\t\t\tcontinue;\n\t\tice_vsi_free_q_vectors(pf->vsi[v]);\n\t}\n\tice_clear_interrupt_scheme(pf);\n\n\tpci_save_state(pdev);\n\tpci_wake_from_d3(pdev, pf->wol_ena);\n\tpci_set_power_state(pdev, PCI_D3hot);\n\treturn 0;\n}\n\n \nstatic int __maybe_unused ice_resume(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tenum ice_reset_req reset_type;\n\tstruct ice_pf *pf;\n\tstruct ice_hw *hw;\n\tint ret;\n\n\tpci_set_power_state(pdev, PCI_D0);\n\tpci_restore_state(pdev);\n\tpci_save_state(pdev);\n\n\tif (!pci_device_is_present(pdev))\n\t\treturn -ENODEV;\n\n\tret = pci_enable_device_mem(pdev);\n\tif (ret) {\n\t\tdev_err(dev, \"Cannot enable device after suspend\\n\");\n\t\treturn ret;\n\t}\n\n\tpf = pci_get_drvdata(pdev);\n\thw = &pf->hw;\n\n\tpf->wakeup_reason = rd32(hw, PFPM_WUS);\n\tice_print_wake_reason(pf);\n\n\t \n\tret = ice_reinit_interrupt_scheme(pf);\n\tif (ret)\n\t\tdev_err(dev, \"Cannot restore interrupt scheme: %d\\n\", ret);\n\n\tclear_bit(ICE_DOWN, pf->state);\n\t \n\treset_type = ICE_RESET_PFR;\n\t \n\tclear_bit(ICE_SERVICE_DIS, pf->state);\n\n\tif (ice_schedule_reset(pf, reset_type))\n\t\tdev_err(dev, \"Reset during resume failed.\\n\");\n\n\tclear_bit(ICE_SUSPENDED, pf->state);\n\tice_service_task_restart(pf);\n\n\t \n\tmod_timer(&pf->serv_tmr, round_jiffies(jiffies + pf->serv_tmr_period));\n\n\treturn 0;\n}\n#endif  \n\n \nstatic pci_ers_result_t\nice_pci_err_detected(struct pci_dev *pdev, pci_channel_state_t err)\n{\n\tstruct ice_pf *pf = pci_get_drvdata(pdev);\n\n\tif (!pf) {\n\t\tdev_err(&pdev->dev, \"%s: unrecoverable device error %d\\n\",\n\t\t\t__func__, err);\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\tif (!test_bit(ICE_SUSPENDED, pf->state)) {\n\t\tice_service_task_stop(pf);\n\n\t\tif (!test_bit(ICE_PREPARED_FOR_RESET, pf->state)) {\n\t\t\tset_bit(ICE_PFR_REQ, pf->state);\n\t\t\tice_prepare_for_reset(pf, ICE_RESET_PFR);\n\t\t}\n\t}\n\n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\n \nstatic pci_ers_result_t ice_pci_err_slot_reset(struct pci_dev *pdev)\n{\n\tstruct ice_pf *pf = pci_get_drvdata(pdev);\n\tpci_ers_result_t result;\n\tint err;\n\tu32 reg;\n\n\terr = pci_enable_device_mem(pdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot re-enable PCI device after reset, error %d\\n\",\n\t\t\terr);\n\t\tresult = PCI_ERS_RESULT_DISCONNECT;\n\t} else {\n\t\tpci_set_master(pdev);\n\t\tpci_restore_state(pdev);\n\t\tpci_save_state(pdev);\n\t\tpci_wake_from_d3(pdev, false);\n\n\t\t \n\t\treg = rd32(&pf->hw, GLGEN_RTRIG);\n\t\tif (!reg)\n\t\t\tresult = PCI_ERS_RESULT_RECOVERED;\n\t\telse\n\t\t\tresult = PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\treturn result;\n}\n\n \nstatic void ice_pci_err_resume(struct pci_dev *pdev)\n{\n\tstruct ice_pf *pf = pci_get_drvdata(pdev);\n\n\tif (!pf) {\n\t\tdev_err(&pdev->dev, \"%s failed, device is unrecoverable\\n\",\n\t\t\t__func__);\n\t\treturn;\n\t}\n\n\tif (test_bit(ICE_SUSPENDED, pf->state)) {\n\t\tdev_dbg(&pdev->dev, \"%s failed to resume normal operations!\\n\",\n\t\t\t__func__);\n\t\treturn;\n\t}\n\n\tice_restore_all_vfs_msi_state(pdev);\n\n\tice_do_reset(pf, ICE_RESET_PFR);\n\tice_service_task_restart(pf);\n\tmod_timer(&pf->serv_tmr, round_jiffies(jiffies + pf->serv_tmr_period));\n}\n\n \nstatic void ice_pci_err_reset_prepare(struct pci_dev *pdev)\n{\n\tstruct ice_pf *pf = pci_get_drvdata(pdev);\n\n\tif (!test_bit(ICE_SUSPENDED, pf->state)) {\n\t\tice_service_task_stop(pf);\n\n\t\tif (!test_bit(ICE_PREPARED_FOR_RESET, pf->state)) {\n\t\t\tset_bit(ICE_PFR_REQ, pf->state);\n\t\t\tice_prepare_for_reset(pf, ICE_RESET_PFR);\n\t\t}\n\t}\n}\n\n \nstatic void ice_pci_err_reset_done(struct pci_dev *pdev)\n{\n\tice_pci_err_resume(pdev);\n}\n\n \nstatic const struct pci_device_id ice_pci_tbl[] = {\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E810C_BACKPLANE), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E810C_QSFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E810C_SFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E810_XXV_BACKPLANE), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E810_XXV_QSFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E810_XXV_SFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823C_BACKPLANE), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823C_QSFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823C_SFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823C_10G_BASE_T), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823C_SGMII), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822C_BACKPLANE), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822C_QSFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822C_SFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822C_10G_BASE_T), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822C_SGMII), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822L_BACKPLANE), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822L_SFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822L_10G_BASE_T), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822L_SGMII), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823L_BACKPLANE), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823L_SFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823L_10G_BASE_T), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823L_1GBE), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E823L_QSFP), 0 },\n\t{ PCI_VDEVICE(INTEL, ICE_DEV_ID_E822_SI_DFLT), 0 },\n\t \n\t{ 0, }\n};\nMODULE_DEVICE_TABLE(pci, ice_pci_tbl);\n\nstatic __maybe_unused SIMPLE_DEV_PM_OPS(ice_pm_ops, ice_suspend, ice_resume);\n\nstatic const struct pci_error_handlers ice_pci_err_handler = {\n\t.error_detected = ice_pci_err_detected,\n\t.slot_reset = ice_pci_err_slot_reset,\n\t.reset_prepare = ice_pci_err_reset_prepare,\n\t.reset_done = ice_pci_err_reset_done,\n\t.resume = ice_pci_err_resume\n};\n\nstatic struct pci_driver ice_driver = {\n\t.name = KBUILD_MODNAME,\n\t.id_table = ice_pci_tbl,\n\t.probe = ice_probe,\n\t.remove = ice_remove,\n#ifdef CONFIG_PM\n\t.driver.pm = &ice_pm_ops,\n#endif  \n\t.shutdown = ice_shutdown,\n\t.sriov_configure = ice_sriov_configure,\n\t.err_handler = &ice_pci_err_handler\n};\n\n \nstatic int __init ice_module_init(void)\n{\n\tint status = -ENOMEM;\n\n\tpr_info(\"%s\\n\", ice_driver_string);\n\tpr_info(\"%s\\n\", ice_copyright);\n\n\tice_wq = alloc_workqueue(\"%s\", 0, 0, KBUILD_MODNAME);\n\tif (!ice_wq) {\n\t\tpr_err(\"Failed to create workqueue\\n\");\n\t\treturn status;\n\t}\n\n\tice_lag_wq = alloc_ordered_workqueue(\"ice_lag_wq\", 0);\n\tif (!ice_lag_wq) {\n\t\tpr_err(\"Failed to create LAG workqueue\\n\");\n\t\tgoto err_dest_wq;\n\t}\n\n\tstatus = pci_register_driver(&ice_driver);\n\tif (status) {\n\t\tpr_err(\"failed to register PCI driver, err %d\\n\", status);\n\t\tgoto err_dest_lag_wq;\n\t}\n\n\treturn 0;\n\nerr_dest_lag_wq:\n\tdestroy_workqueue(ice_lag_wq);\nerr_dest_wq:\n\tdestroy_workqueue(ice_wq);\n\treturn status;\n}\nmodule_init(ice_module_init);\n\n \nstatic void __exit ice_module_exit(void)\n{\n\tpci_unregister_driver(&ice_driver);\n\tdestroy_workqueue(ice_wq);\n\tdestroy_workqueue(ice_lag_wq);\n\tpr_info(\"module unloaded\\n\");\n}\nmodule_exit(ice_module_exit);\n\n \nstatic int ice_set_mac_address(struct net_device *netdev, void *pi)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct sockaddr *addr = pi;\n\tu8 old_mac[ETH_ALEN];\n\tu8 flags = 0;\n\tu8 *mac;\n\tint err;\n\n\tmac = (u8 *)addr->sa_data;\n\n\tif (!is_valid_ether_addr(mac))\n\t\treturn -EADDRNOTAVAIL;\n\n\tif (test_bit(ICE_DOWN, pf->state) ||\n\t    ice_is_reset_in_progress(pf->state)) {\n\t\tnetdev_err(netdev, \"can't set mac %pM. device not ready\\n\",\n\t\t\t   mac);\n\t\treturn -EBUSY;\n\t}\n\n\tif (ice_chnl_dmac_fltr_cnt(pf)) {\n\t\tnetdev_err(netdev, \"can't set mac %pM. Device has tc-flower filters, delete all of them and try again\\n\",\n\t\t\t   mac);\n\t\treturn -EAGAIN;\n\t}\n\n\tnetif_addr_lock_bh(netdev);\n\tether_addr_copy(old_mac, netdev->dev_addr);\n\t \n\teth_hw_addr_set(netdev, mac);\n\tnetif_addr_unlock_bh(netdev);\n\n\t \n\terr = ice_fltr_remove_mac(vsi, old_mac, ICE_FWD_TO_VSI);\n\tif (err && err != -ENOENT) {\n\t\terr = -EADDRNOTAVAIL;\n\t\tgoto err_update_filters;\n\t}\n\n\t \n\terr = ice_fltr_add_mac(vsi, mac, ICE_FWD_TO_VSI);\n\tif (err == -EEXIST) {\n\t\t \n\t\tnetdev_dbg(netdev, \"filter for MAC %pM already exists\\n\", mac);\n\n\t\treturn 0;\n\t} else if (err) {\n\t\t \n\t\terr = -EADDRNOTAVAIL;\n\t}\n\nerr_update_filters:\n\tif (err) {\n\t\tnetdev_err(netdev, \"can't set MAC %pM. filter update failed\\n\",\n\t\t\t   mac);\n\t\tnetif_addr_lock_bh(netdev);\n\t\teth_hw_addr_set(netdev, old_mac);\n\t\tnetif_addr_unlock_bh(netdev);\n\t\treturn err;\n\t}\n\n\tnetdev_dbg(vsi->netdev, \"updated MAC address to %pM\\n\",\n\t\t   netdev->dev_addr);\n\n\t \n\tflags = ICE_AQC_MAN_MAC_UPDATE_LAA_WOL;\n\terr = ice_aq_manage_mac_write(hw, mac, flags, NULL);\n\tif (err) {\n\t\tnetdev_err(netdev, \"can't set MAC %pM. write to firmware failed error %d\\n\",\n\t\t\t   mac, err);\n\t}\n\treturn 0;\n}\n\n \nstatic void ice_set_rx_mode(struct net_device *netdev)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\n\tif (!vsi || ice_is_switchdev_running(vsi->back))\n\t\treturn;\n\n\t \n\tset_bit(ICE_VSI_UMAC_FLTR_CHANGED, vsi->state);\n\tset_bit(ICE_VSI_MMAC_FLTR_CHANGED, vsi->state);\n\tset_bit(ICE_FLAG_FLTR_SYNC, vsi->back->flags);\n\n\t \n\tice_service_task_schedule(vsi->back);\n}\n\n \nstatic int\nice_set_tx_maxrate(struct net_device *netdev, int queue_index, u32 maxrate)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tu16 q_handle;\n\tint status;\n\tu8 tc;\n\n\t \n\tif (maxrate && (maxrate > (ICE_SCHED_MAX_BW / 1000))) {\n\t\tnetdev_err(netdev, \"Invalid max rate %d specified for the queue %d\\n\",\n\t\t\t   maxrate, queue_index);\n\t\treturn -EINVAL;\n\t}\n\n\tq_handle = vsi->tx_rings[queue_index]->q_handle;\n\ttc = ice_dcb_get_tc(vsi, queue_index);\n\n\tvsi = ice_locate_vsi_using_queue(vsi, queue_index);\n\tif (!vsi) {\n\t\tnetdev_err(netdev, \"Invalid VSI for given queue %d\\n\",\n\t\t\t   queue_index);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!maxrate)\n\t\tstatus = ice_cfg_q_bw_dflt_lmt(vsi->port_info, vsi->idx, tc,\n\t\t\t\t\t       q_handle, ICE_MAX_BW);\n\telse\n\t\tstatus = ice_cfg_q_bw_lmt(vsi->port_info, vsi->idx, tc,\n\t\t\t\t\t  q_handle, ICE_MAX_BW, maxrate * 1000);\n\tif (status)\n\t\tnetdev_err(netdev, \"Unable to set Tx max rate, error %d\\n\",\n\t\t\t   status);\n\n\treturn status;\n}\n\n \nstatic int\nice_fdb_add(struct ndmsg *ndm, struct nlattr __always_unused *tb[],\n\t    struct net_device *dev, const unsigned char *addr, u16 vid,\n\t    u16 flags, struct netlink_ext_ack __always_unused *extack)\n{\n\tint err;\n\n\tif (vid) {\n\t\tnetdev_err(dev, \"VLANs aren't supported yet for dev_uc|mc_add()\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (ndm->ndm_state && !(ndm->ndm_state & NUD_PERMANENT)) {\n\t\tnetdev_err(dev, \"FDB only supports static addresses\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (is_unicast_ether_addr(addr) || is_link_local_ether_addr(addr))\n\t\terr = dev_uc_add_excl(dev, addr);\n\telse if (is_multicast_ether_addr(addr))\n\t\terr = dev_mc_add_excl(dev, addr);\n\telse\n\t\terr = -EINVAL;\n\n\t \n\tif (err == -EEXIST && !(flags & NLM_F_EXCL))\n\t\terr = 0;\n\n\treturn err;\n}\n\n \nstatic int\nice_fdb_del(struct ndmsg *ndm, __always_unused struct nlattr *tb[],\n\t    struct net_device *dev, const unsigned char *addr,\n\t    __always_unused u16 vid, struct netlink_ext_ack *extack)\n{\n\tint err;\n\n\tif (ndm->ndm_state & NUD_PERMANENT) {\n\t\tnetdev_err(dev, \"FDB only supports static addresses\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (is_unicast_ether_addr(addr))\n\t\terr = dev_uc_del(dev, addr);\n\telse if (is_multicast_ether_addr(addr))\n\t\terr = dev_mc_del(dev, addr);\n\telse\n\t\terr = -EINVAL;\n\n\treturn err;\n}\n\n#define NETIF_VLAN_OFFLOAD_FEATURES\t(NETIF_F_HW_VLAN_CTAG_RX | \\\n\t\t\t\t\t NETIF_F_HW_VLAN_CTAG_TX | \\\n\t\t\t\t\t NETIF_F_HW_VLAN_STAG_RX | \\\n\t\t\t\t\t NETIF_F_HW_VLAN_STAG_TX)\n\n#define NETIF_VLAN_STRIPPING_FEATURES\t(NETIF_F_HW_VLAN_CTAG_RX | \\\n\t\t\t\t\t NETIF_F_HW_VLAN_STAG_RX)\n\n#define NETIF_VLAN_FILTERING_FEATURES\t(NETIF_F_HW_VLAN_CTAG_FILTER | \\\n\t\t\t\t\t NETIF_F_HW_VLAN_STAG_FILTER)\n\n \nstatic netdev_features_t\nice_fix_features(struct net_device *netdev, netdev_features_t features)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tnetdev_features_t req_vlan_fltr, cur_vlan_fltr;\n\tbool cur_ctag, cur_stag, req_ctag, req_stag;\n\n\tcur_vlan_fltr = netdev->features & NETIF_VLAN_FILTERING_FEATURES;\n\tcur_ctag = cur_vlan_fltr & NETIF_F_HW_VLAN_CTAG_FILTER;\n\tcur_stag = cur_vlan_fltr & NETIF_F_HW_VLAN_STAG_FILTER;\n\n\treq_vlan_fltr = features & NETIF_VLAN_FILTERING_FEATURES;\n\treq_ctag = req_vlan_fltr & NETIF_F_HW_VLAN_CTAG_FILTER;\n\treq_stag = req_vlan_fltr & NETIF_F_HW_VLAN_STAG_FILTER;\n\n\tif (req_vlan_fltr != cur_vlan_fltr) {\n\t\tif (ice_is_dvm_ena(&np->vsi->back->hw)) {\n\t\t\tif (req_ctag && req_stag) {\n\t\t\t\tfeatures |= NETIF_VLAN_FILTERING_FEATURES;\n\t\t\t} else if (!req_ctag && !req_stag) {\n\t\t\t\tfeatures &= ~NETIF_VLAN_FILTERING_FEATURES;\n\t\t\t} else if ((!cur_ctag && req_ctag && !cur_stag) ||\n\t\t\t\t   (!cur_stag && req_stag && !cur_ctag)) {\n\t\t\t\tfeatures |= NETIF_VLAN_FILTERING_FEATURES;\n\t\t\t\tnetdev_warn(netdev,  \"802.1Q and 802.1ad VLAN filtering must be either both on or both off. VLAN filtering has been enabled for both types.\\n\");\n\t\t\t} else if ((cur_ctag && !req_ctag && cur_stag) ||\n\t\t\t\t   (cur_stag && !req_stag && cur_ctag)) {\n\t\t\t\tfeatures &= ~NETIF_VLAN_FILTERING_FEATURES;\n\t\t\t\tnetdev_warn(netdev,  \"802.1Q and 802.1ad VLAN filtering must be either both on or both off. VLAN filtering has been disabled for both types.\\n\");\n\t\t\t}\n\t\t} else {\n\t\t\tif (req_vlan_fltr & NETIF_F_HW_VLAN_STAG_FILTER)\n\t\t\t\tnetdev_warn(netdev, \"cannot support requested 802.1ad filtering setting in SVM mode\\n\");\n\n\t\t\tif (req_vlan_fltr & NETIF_F_HW_VLAN_CTAG_FILTER)\n\t\t\t\tfeatures |= NETIF_F_HW_VLAN_CTAG_FILTER;\n\t\t}\n\t}\n\n\tif ((features & (NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_CTAG_TX)) &&\n\t    (features & (NETIF_F_HW_VLAN_STAG_RX | NETIF_F_HW_VLAN_STAG_TX))) {\n\t\tnetdev_warn(netdev, \"cannot support CTAG and STAG VLAN stripping and/or insertion simultaneously since CTAG and STAG offloads are mutually exclusive, clearing STAG offload settings\\n\");\n\t\tfeatures &= ~(NETIF_F_HW_VLAN_STAG_RX |\n\t\t\t      NETIF_F_HW_VLAN_STAG_TX);\n\t}\n\n\tif (!(netdev->features & NETIF_F_RXFCS) &&\n\t    (features & NETIF_F_RXFCS) &&\n\t    (features & NETIF_VLAN_STRIPPING_FEATURES) &&\n\t    !ice_vsi_has_non_zero_vlans(np->vsi)) {\n\t\tnetdev_warn(netdev, \"Disabling VLAN stripping as FCS/CRC stripping is also disabled and there is no VLAN configured\\n\");\n\t\tfeatures &= ~NETIF_VLAN_STRIPPING_FEATURES;\n\t}\n\n\treturn features;\n}\n\n \nstatic int\nice_set_vlan_offload_features(struct ice_vsi *vsi, netdev_features_t features)\n{\n\tbool enable_stripping = true, enable_insertion = true;\n\tstruct ice_vsi_vlan_ops *vlan_ops;\n\tint strip_err = 0, insert_err = 0;\n\tu16 vlan_ethertype = 0;\n\n\tvlan_ops = ice_get_compat_vsi_vlan_ops(vsi);\n\n\tif (features & (NETIF_F_HW_VLAN_STAG_RX | NETIF_F_HW_VLAN_STAG_TX))\n\t\tvlan_ethertype = ETH_P_8021AD;\n\telse if (features & (NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_CTAG_TX))\n\t\tvlan_ethertype = ETH_P_8021Q;\n\n\tif (!(features & (NETIF_F_HW_VLAN_STAG_RX | NETIF_F_HW_VLAN_CTAG_RX)))\n\t\tenable_stripping = false;\n\tif (!(features & (NETIF_F_HW_VLAN_STAG_TX | NETIF_F_HW_VLAN_CTAG_TX)))\n\t\tenable_insertion = false;\n\n\tif (enable_stripping)\n\t\tstrip_err = vlan_ops->ena_stripping(vsi, vlan_ethertype);\n\telse\n\t\tstrip_err = vlan_ops->dis_stripping(vsi);\n\n\tif (enable_insertion)\n\t\tinsert_err = vlan_ops->ena_insertion(vsi, vlan_ethertype);\n\telse\n\t\tinsert_err = vlan_ops->dis_insertion(vsi);\n\n\tif (strip_err || insert_err)\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\n \nstatic int\nice_set_vlan_filtering_features(struct ice_vsi *vsi, netdev_features_t features)\n{\n\tstruct ice_vsi_vlan_ops *vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);\n\tint err = 0;\n\n\t \n\tif (features &\n\t    (NETIF_F_HW_VLAN_CTAG_FILTER | NETIF_F_HW_VLAN_STAG_FILTER))\n\t\terr = vlan_ops->ena_rx_filtering(vsi);\n\telse\n\t\terr = vlan_ops->dis_rx_filtering(vsi);\n\n\treturn err;\n}\n\n \nstatic int\nice_set_vlan_features(struct net_device *netdev, netdev_features_t features)\n{\n\tnetdev_features_t current_vlan_features, requested_vlan_features;\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tint err;\n\n\tcurrent_vlan_features = netdev->features & NETIF_VLAN_OFFLOAD_FEATURES;\n\trequested_vlan_features = features & NETIF_VLAN_OFFLOAD_FEATURES;\n\tif (current_vlan_features ^ requested_vlan_features) {\n\t\tif ((features & NETIF_F_RXFCS) &&\n\t\t    (features & NETIF_VLAN_STRIPPING_FEATURES)) {\n\t\t\tdev_err(ice_pf_to_dev(vsi->back),\n\t\t\t\t\"To enable VLAN stripping, you must first enable FCS/CRC stripping\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\terr = ice_set_vlan_offload_features(vsi, features);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tcurrent_vlan_features = netdev->features &\n\t\tNETIF_VLAN_FILTERING_FEATURES;\n\trequested_vlan_features = features & NETIF_VLAN_FILTERING_FEATURES;\n\tif (current_vlan_features ^ requested_vlan_features) {\n\t\terr = ice_set_vlan_filtering_features(vsi, features);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int ice_set_loopback(struct ice_vsi *vsi, bool ena)\n{\n\tbool if_running = netif_running(vsi->netdev);\n\tint ret;\n\n\tif (if_running && !test_and_set_bit(ICE_VSI_DOWN, vsi->state)) {\n\t\tret = ice_down(vsi);\n\t\tif (ret) {\n\t\t\tnetdev_err(vsi->netdev, \"Preparing device to toggle loopback failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\tret = ice_aq_set_mac_loopback(&vsi->back->hw, ena, NULL);\n\tif (ret)\n\t\tnetdev_err(vsi->netdev, \"Failed to toggle loopback state\\n\");\n\tif (if_running)\n\t\tret = ice_up(vsi);\n\n\treturn ret;\n}\n\n \nstatic int\nice_set_features(struct net_device *netdev, netdev_features_t features)\n{\n\tnetdev_features_t changed = netdev->features ^ features;\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_pf *pf = vsi->back;\n\tint ret = 0;\n\n\t \n\tif (ice_is_safe_mode(pf)) {\n\t\tdev_err(ice_pf_to_dev(pf),\n\t\t\t\"Device is in Safe Mode - not enabling advanced netdev features\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tif (ice_is_reset_in_progress(pf->state)) {\n\t\tdev_err(ice_pf_to_dev(pf),\n\t\t\t\"Device is resetting, changing advanced netdev features temporarily unavailable.\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tif (changed & NETIF_F_RXHASH)\n\t\tice_vsi_manage_rss_lut(vsi, !!(features & NETIF_F_RXHASH));\n\n\tret = ice_set_vlan_features(netdev, features);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (changed & NETIF_F_RXFCS) {\n\t\tif ((features & NETIF_F_RXFCS) &&\n\t\t    (features & NETIF_VLAN_STRIPPING_FEATURES)) {\n\t\t\tdev_err(ice_pf_to_dev(vsi->back),\n\t\t\t\t\"To disable FCS/CRC stripping, you must first disable VLAN stripping\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tice_vsi_cfg_crc_strip(vsi, !!(features & NETIF_F_RXFCS));\n\t\tret = ice_down_up(vsi);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (changed & NETIF_F_NTUPLE) {\n\t\tbool ena = !!(features & NETIF_F_NTUPLE);\n\n\t\tice_vsi_manage_fdir(vsi, ena);\n\t\tena ? ice_init_arfs(vsi) : ice_clear_arfs(vsi);\n\t}\n\n\t \n\tif (!(features & NETIF_F_HW_TC) && ice_is_adq_active(pf)) {\n\t\tdev_err(ice_pf_to_dev(pf), \"ADQ is active, can't turn hw_tc_offload off\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (changed & NETIF_F_HW_TC) {\n\t\tbool ena = !!(features & NETIF_F_HW_TC);\n\n\t\tena ? set_bit(ICE_FLAG_CLS_FLOWER, pf->flags) :\n\t\t      clear_bit(ICE_FLAG_CLS_FLOWER, pf->flags);\n\t}\n\n\tif (changed & NETIF_F_LOOPBACK)\n\t\tret = ice_set_loopback(vsi, !!(features & NETIF_F_LOOPBACK));\n\n\treturn ret;\n}\n\n \nstatic int ice_vsi_vlan_setup(struct ice_vsi *vsi)\n{\n\tint err;\n\n\terr = ice_set_vlan_offload_features(vsi, vsi->netdev->features);\n\tif (err)\n\t\treturn err;\n\n\terr = ice_set_vlan_filtering_features(vsi, vsi->netdev->features);\n\tif (err)\n\t\treturn err;\n\n\treturn ice_vsi_add_vlan_zero(vsi);\n}\n\n \nint ice_vsi_cfg_lan(struct ice_vsi *vsi)\n{\n\tint err;\n\n\tif (vsi->netdev && vsi->type == ICE_VSI_PF) {\n\t\tice_set_rx_mode(vsi->netdev);\n\n\t\terr = ice_vsi_vlan_setup(vsi);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tice_vsi_cfg_dcb_rings(vsi);\n\n\terr = ice_vsi_cfg_lan_txqs(vsi);\n\tif (!err && ice_is_xdp_ena_vsi(vsi))\n\t\terr = ice_vsi_cfg_xdp_txqs(vsi);\n\tif (!err)\n\t\terr = ice_vsi_cfg_rxqs(vsi);\n\n\treturn err;\n}\n\n \nstruct ice_dim {\n\t \n\tu16 itr;\n};\n\n \nstatic const struct ice_dim rx_profile[] = {\n\t{2},     \n\t{8},     \n\t{16},    \n\t{62},    \n\t{126}    \n};\n\n \nstatic const struct ice_dim tx_profile[] = {\n\t{2},     \n\t{8},     \n\t{40},    \n\t{128},   \n\t{256}    \n};\n\nstatic void ice_tx_dim_work(struct work_struct *work)\n{\n\tstruct ice_ring_container *rc;\n\tstruct dim *dim;\n\tu16 itr;\n\n\tdim = container_of(work, struct dim, work);\n\trc = dim->priv;\n\n\tWARN_ON(dim->profile_ix >= ARRAY_SIZE(tx_profile));\n\n\t \n\titr = tx_profile[dim->profile_ix].itr;\n\n\tice_trace(tx_dim_work, container_of(rc, struct ice_q_vector, tx), dim);\n\tice_write_itr(rc, itr);\n\n\tdim->state = DIM_START_MEASURE;\n}\n\nstatic void ice_rx_dim_work(struct work_struct *work)\n{\n\tstruct ice_ring_container *rc;\n\tstruct dim *dim;\n\tu16 itr;\n\n\tdim = container_of(work, struct dim, work);\n\trc = dim->priv;\n\n\tWARN_ON(dim->profile_ix >= ARRAY_SIZE(rx_profile));\n\n\t \n\titr = rx_profile[dim->profile_ix].itr;\n\n\tice_trace(rx_dim_work, container_of(rc, struct ice_q_vector, rx), dim);\n\tice_write_itr(rc, itr);\n\n\tdim->state = DIM_START_MEASURE;\n}\n\n#define ICE_DIM_DEFAULT_PROFILE_IX 1\n\n \nstatic void ice_init_moderation(struct ice_q_vector *q_vector)\n{\n\tstruct ice_ring_container *rc;\n\tbool tx_dynamic, rx_dynamic;\n\n\trc = &q_vector->tx;\n\tINIT_WORK(&rc->dim.work, ice_tx_dim_work);\n\trc->dim.mode = DIM_CQ_PERIOD_MODE_START_FROM_EQE;\n\trc->dim.profile_ix = ICE_DIM_DEFAULT_PROFILE_IX;\n\trc->dim.priv = rc;\n\ttx_dynamic = ITR_IS_DYNAMIC(rc);\n\n\t \n\tice_write_itr(rc, tx_dynamic ?\n\t\t      tx_profile[rc->dim.profile_ix].itr : rc->itr_setting);\n\n\trc = &q_vector->rx;\n\tINIT_WORK(&rc->dim.work, ice_rx_dim_work);\n\trc->dim.mode = DIM_CQ_PERIOD_MODE_START_FROM_EQE;\n\trc->dim.profile_ix = ICE_DIM_DEFAULT_PROFILE_IX;\n\trc->dim.priv = rc;\n\trx_dynamic = ITR_IS_DYNAMIC(rc);\n\n\t \n\tice_write_itr(rc, rx_dynamic ? rx_profile[rc->dim.profile_ix].itr :\n\t\t\t\t       rc->itr_setting);\n\n\tice_set_q_vector_intrl(q_vector);\n}\n\n \nstatic void ice_napi_enable_all(struct ice_vsi *vsi)\n{\n\tint q_idx;\n\n\tif (!vsi->netdev)\n\t\treturn;\n\n\tice_for_each_q_vector(vsi, q_idx) {\n\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[q_idx];\n\n\t\tice_init_moderation(q_vector);\n\n\t\tif (q_vector->rx.rx_ring || q_vector->tx.tx_ring)\n\t\t\tnapi_enable(&q_vector->napi);\n\t}\n}\n\n \nstatic int ice_up_complete(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tint err;\n\n\tice_vsi_cfg_msix(vsi);\n\n\t \n\terr = ice_vsi_start_all_rx_rings(vsi);\n\tif (err)\n\t\treturn err;\n\n\tclear_bit(ICE_VSI_DOWN, vsi->state);\n\tice_napi_enable_all(vsi);\n\tice_vsi_ena_irq(vsi);\n\n\tif (vsi->port_info &&\n\t    (vsi->port_info->phy.link_info.link_info & ICE_AQ_LINK_UP) &&\n\t    vsi->netdev && vsi->type == ICE_VSI_PF) {\n\t\tice_print_link_msg(vsi, true);\n\t\tnetif_tx_start_all_queues(vsi->netdev);\n\t\tnetif_carrier_on(vsi->netdev);\n\t\tice_ptp_link_change(pf, pf->hw.pf_id, true);\n\t}\n\n\t \n\tice_update_eth_stats(vsi);\n\n\tif (vsi->type == ICE_VSI_PF)\n\t\tice_service_task_schedule(pf);\n\n\treturn 0;\n}\n\n \nint ice_up(struct ice_vsi *vsi)\n{\n\tint err;\n\n\terr = ice_vsi_cfg_lan(vsi);\n\tif (!err)\n\t\terr = ice_up_complete(vsi);\n\n\treturn err;\n}\n\n \nvoid\nice_fetch_u64_stats_per_ring(struct u64_stats_sync *syncp,\n\t\t\t     struct ice_q_stats stats, u64 *pkts, u64 *bytes)\n{\n\tunsigned int start;\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(syncp);\n\t\t*pkts = stats.pkts;\n\t\t*bytes = stats.bytes;\n\t} while (u64_stats_fetch_retry(syncp, start));\n}\n\n \nstatic void\nice_update_vsi_tx_ring_stats(struct ice_vsi *vsi,\n\t\t\t     struct rtnl_link_stats64 *vsi_stats,\n\t\t\t     struct ice_tx_ring **rings, u16 count)\n{\n\tu16 i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tstruct ice_tx_ring *ring;\n\t\tu64 pkts = 0, bytes = 0;\n\n\t\tring = READ_ONCE(rings[i]);\n\t\tif (!ring || !ring->ring_stats)\n\t\t\tcontinue;\n\t\tice_fetch_u64_stats_per_ring(&ring->ring_stats->syncp,\n\t\t\t\t\t     ring->ring_stats->stats, &pkts,\n\t\t\t\t\t     &bytes);\n\t\tvsi_stats->tx_packets += pkts;\n\t\tvsi_stats->tx_bytes += bytes;\n\t\tvsi->tx_restart += ring->ring_stats->tx_stats.restart_q;\n\t\tvsi->tx_busy += ring->ring_stats->tx_stats.tx_busy;\n\t\tvsi->tx_linearize += ring->ring_stats->tx_stats.tx_linearize;\n\t}\n}\n\n \nstatic void ice_update_vsi_ring_stats(struct ice_vsi *vsi)\n{\n\tstruct rtnl_link_stats64 *net_stats, *stats_prev;\n\tstruct rtnl_link_stats64 *vsi_stats;\n\tu64 pkts, bytes;\n\tint i;\n\n\tvsi_stats = kzalloc(sizeof(*vsi_stats), GFP_ATOMIC);\n\tif (!vsi_stats)\n\t\treturn;\n\n\t \n\tvsi->tx_restart = 0;\n\tvsi->tx_busy = 0;\n\tvsi->tx_linearize = 0;\n\tvsi->rx_buf_failed = 0;\n\tvsi->rx_page_failed = 0;\n\n\trcu_read_lock();\n\n\t \n\tice_update_vsi_tx_ring_stats(vsi, vsi_stats, vsi->tx_rings,\n\t\t\t\t     vsi->num_txq);\n\n\t \n\tice_for_each_rxq(vsi, i) {\n\t\tstruct ice_rx_ring *ring = READ_ONCE(vsi->rx_rings[i]);\n\t\tstruct ice_ring_stats *ring_stats;\n\n\t\tring_stats = ring->ring_stats;\n\t\tice_fetch_u64_stats_per_ring(&ring_stats->syncp,\n\t\t\t\t\t     ring_stats->stats, &pkts,\n\t\t\t\t\t     &bytes);\n\t\tvsi_stats->rx_packets += pkts;\n\t\tvsi_stats->rx_bytes += bytes;\n\t\tvsi->rx_buf_failed += ring_stats->rx_stats.alloc_buf_failed;\n\t\tvsi->rx_page_failed += ring_stats->rx_stats.alloc_page_failed;\n\t}\n\n\t \n\tif (ice_is_xdp_ena_vsi(vsi))\n\t\tice_update_vsi_tx_ring_stats(vsi, vsi_stats, vsi->xdp_rings,\n\t\t\t\t\t     vsi->num_xdp_txq);\n\n\trcu_read_unlock();\n\n\tnet_stats = &vsi->net_stats;\n\tstats_prev = &vsi->net_stats_prev;\n\n\t \n\tif (vsi_stats->tx_packets < stats_prev->tx_packets ||\n\t    vsi_stats->rx_packets < stats_prev->rx_packets) {\n\t\tstats_prev->tx_packets = 0;\n\t\tstats_prev->tx_bytes = 0;\n\t\tstats_prev->rx_packets = 0;\n\t\tstats_prev->rx_bytes = 0;\n\t}\n\n\t \n\tnet_stats->tx_packets += vsi_stats->tx_packets - stats_prev->tx_packets;\n\tnet_stats->tx_bytes += vsi_stats->tx_bytes - stats_prev->tx_bytes;\n\tnet_stats->rx_packets += vsi_stats->rx_packets - stats_prev->rx_packets;\n\tnet_stats->rx_bytes += vsi_stats->rx_bytes - stats_prev->rx_bytes;\n\n\tstats_prev->tx_packets = vsi_stats->tx_packets;\n\tstats_prev->tx_bytes = vsi_stats->tx_bytes;\n\tstats_prev->rx_packets = vsi_stats->rx_packets;\n\tstats_prev->rx_bytes = vsi_stats->rx_bytes;\n\n\tkfree(vsi_stats);\n}\n\n \nvoid ice_update_vsi_stats(struct ice_vsi *vsi)\n{\n\tstruct rtnl_link_stats64 *cur_ns = &vsi->net_stats;\n\tstruct ice_eth_stats *cur_es = &vsi->eth_stats;\n\tstruct ice_pf *pf = vsi->back;\n\n\tif (test_bit(ICE_VSI_DOWN, vsi->state) ||\n\t    test_bit(ICE_CFG_BUSY, pf->state))\n\t\treturn;\n\n\t \n\tice_update_vsi_ring_stats(vsi);\n\n\t \n\tice_update_eth_stats(vsi);\n\n\tcur_ns->tx_errors = cur_es->tx_errors;\n\tcur_ns->rx_dropped = cur_es->rx_discards;\n\tcur_ns->tx_dropped = cur_es->tx_discards;\n\tcur_ns->multicast = cur_es->rx_multicast;\n\n\t \n\tif (vsi->type == ICE_VSI_PF) {\n\t\tcur_ns->rx_crc_errors = pf->stats.crc_errors;\n\t\tcur_ns->rx_errors = pf->stats.crc_errors +\n\t\t\t\t    pf->stats.illegal_bytes +\n\t\t\t\t    pf->stats.rx_len_errors +\n\t\t\t\t    pf->stats.rx_undersize +\n\t\t\t\t    pf->hw_csum_rx_error +\n\t\t\t\t    pf->stats.rx_jabber +\n\t\t\t\t    pf->stats.rx_fragments +\n\t\t\t\t    pf->stats.rx_oversize;\n\t\tcur_ns->rx_length_errors = pf->stats.rx_len_errors;\n\t\t \n\t\tcur_ns->rx_missed_errors = pf->stats.eth.rx_discards;\n\t}\n}\n\n \nvoid ice_update_pf_stats(struct ice_pf *pf)\n{\n\tstruct ice_hw_port_stats *prev_ps, *cur_ps;\n\tstruct ice_hw *hw = &pf->hw;\n\tu16 fd_ctr_base;\n\tu8 port;\n\n\tport = hw->port_info->lport;\n\tprev_ps = &pf->stats_prev;\n\tcur_ps = &pf->stats;\n\n\tif (ice_is_reset_in_progress(pf->state))\n\t\tpf->stat_prev_loaded = false;\n\n\tice_stat_update40(hw, GLPRT_GORCL(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->eth.rx_bytes,\n\t\t\t  &cur_ps->eth.rx_bytes);\n\n\tice_stat_update40(hw, GLPRT_UPRCL(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->eth.rx_unicast,\n\t\t\t  &cur_ps->eth.rx_unicast);\n\n\tice_stat_update40(hw, GLPRT_MPRCL(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->eth.rx_multicast,\n\t\t\t  &cur_ps->eth.rx_multicast);\n\n\tice_stat_update40(hw, GLPRT_BPRCL(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->eth.rx_broadcast,\n\t\t\t  &cur_ps->eth.rx_broadcast);\n\n\tice_stat_update32(hw, PRTRPB_RDPC, pf->stat_prev_loaded,\n\t\t\t  &prev_ps->eth.rx_discards,\n\t\t\t  &cur_ps->eth.rx_discards);\n\n\tice_stat_update40(hw, GLPRT_GOTCL(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->eth.tx_bytes,\n\t\t\t  &cur_ps->eth.tx_bytes);\n\n\tice_stat_update40(hw, GLPRT_UPTCL(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->eth.tx_unicast,\n\t\t\t  &cur_ps->eth.tx_unicast);\n\n\tice_stat_update40(hw, GLPRT_MPTCL(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->eth.tx_multicast,\n\t\t\t  &cur_ps->eth.tx_multicast);\n\n\tice_stat_update40(hw, GLPRT_BPTCL(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->eth.tx_broadcast,\n\t\t\t  &cur_ps->eth.tx_broadcast);\n\n\tice_stat_update32(hw, GLPRT_TDOLD(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->tx_dropped_link_down,\n\t\t\t  &cur_ps->tx_dropped_link_down);\n\n\tice_stat_update40(hw, GLPRT_PRC64L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_size_64, &cur_ps->rx_size_64);\n\n\tice_stat_update40(hw, GLPRT_PRC127L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_size_127, &cur_ps->rx_size_127);\n\n\tice_stat_update40(hw, GLPRT_PRC255L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_size_255, &cur_ps->rx_size_255);\n\n\tice_stat_update40(hw, GLPRT_PRC511L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_size_511, &cur_ps->rx_size_511);\n\n\tice_stat_update40(hw, GLPRT_PRC1023L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_size_1023, &cur_ps->rx_size_1023);\n\n\tice_stat_update40(hw, GLPRT_PRC1522L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_size_1522, &cur_ps->rx_size_1522);\n\n\tice_stat_update40(hw, GLPRT_PRC9522L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_size_big, &cur_ps->rx_size_big);\n\n\tice_stat_update40(hw, GLPRT_PTC64L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->tx_size_64, &cur_ps->tx_size_64);\n\n\tice_stat_update40(hw, GLPRT_PTC127L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->tx_size_127, &cur_ps->tx_size_127);\n\n\tice_stat_update40(hw, GLPRT_PTC255L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->tx_size_255, &cur_ps->tx_size_255);\n\n\tice_stat_update40(hw, GLPRT_PTC511L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->tx_size_511, &cur_ps->tx_size_511);\n\n\tice_stat_update40(hw, GLPRT_PTC1023L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->tx_size_1023, &cur_ps->tx_size_1023);\n\n\tice_stat_update40(hw, GLPRT_PTC1522L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->tx_size_1522, &cur_ps->tx_size_1522);\n\n\tice_stat_update40(hw, GLPRT_PTC9522L(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->tx_size_big, &cur_ps->tx_size_big);\n\n\tfd_ctr_base = hw->fd_ctr_base;\n\n\tice_stat_update40(hw,\n\t\t\t  GLSTAT_FD_CNT0L(ICE_FD_SB_STAT_IDX(fd_ctr_base)),\n\t\t\t  pf->stat_prev_loaded, &prev_ps->fd_sb_match,\n\t\t\t  &cur_ps->fd_sb_match);\n\tice_stat_update32(hw, GLPRT_LXONRXC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->link_xon_rx, &cur_ps->link_xon_rx);\n\n\tice_stat_update32(hw, GLPRT_LXOFFRXC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->link_xoff_rx, &cur_ps->link_xoff_rx);\n\n\tice_stat_update32(hw, GLPRT_LXONTXC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->link_xon_tx, &cur_ps->link_xon_tx);\n\n\tice_stat_update32(hw, GLPRT_LXOFFTXC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->link_xoff_tx, &cur_ps->link_xoff_tx);\n\n\tice_update_dcb_stats(pf);\n\n\tice_stat_update32(hw, GLPRT_CRCERRS(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->crc_errors, &cur_ps->crc_errors);\n\n\tice_stat_update32(hw, GLPRT_ILLERRC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->illegal_bytes, &cur_ps->illegal_bytes);\n\n\tice_stat_update32(hw, GLPRT_MLFC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->mac_local_faults,\n\t\t\t  &cur_ps->mac_local_faults);\n\n\tice_stat_update32(hw, GLPRT_MRFC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->mac_remote_faults,\n\t\t\t  &cur_ps->mac_remote_faults);\n\n\tice_stat_update32(hw, GLPRT_RLEC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_len_errors, &cur_ps->rx_len_errors);\n\n\tice_stat_update32(hw, GLPRT_RUC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_undersize, &cur_ps->rx_undersize);\n\n\tice_stat_update32(hw, GLPRT_RFC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_fragments, &cur_ps->rx_fragments);\n\n\tice_stat_update32(hw, GLPRT_ROC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_oversize, &cur_ps->rx_oversize);\n\n\tice_stat_update32(hw, GLPRT_RJC(port), pf->stat_prev_loaded,\n\t\t\t  &prev_ps->rx_jabber, &cur_ps->rx_jabber);\n\n\tcur_ps->fd_sb_status = test_bit(ICE_FLAG_FD_ENA, pf->flags) ? 1 : 0;\n\n\tpf->stat_prev_loaded = true;\n}\n\n \nstatic\nvoid ice_get_stats64(struct net_device *netdev, struct rtnl_link_stats64 *stats)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct rtnl_link_stats64 *vsi_stats;\n\tstruct ice_vsi *vsi = np->vsi;\n\n\tvsi_stats = &vsi->net_stats;\n\n\tif (!vsi->num_txq || !vsi->num_rxq)\n\t\treturn;\n\n\t \n\tif (!test_bit(ICE_VSI_DOWN, vsi->state))\n\t\tice_update_vsi_ring_stats(vsi);\n\tstats->tx_packets = vsi_stats->tx_packets;\n\tstats->tx_bytes = vsi_stats->tx_bytes;\n\tstats->rx_packets = vsi_stats->rx_packets;\n\tstats->rx_bytes = vsi_stats->rx_bytes;\n\n\t \n\tstats->multicast = vsi_stats->multicast;\n\tstats->tx_errors = vsi_stats->tx_errors;\n\tstats->tx_dropped = vsi_stats->tx_dropped;\n\tstats->rx_errors = vsi_stats->rx_errors;\n\tstats->rx_dropped = vsi_stats->rx_dropped;\n\tstats->rx_crc_errors = vsi_stats->rx_crc_errors;\n\tstats->rx_length_errors = vsi_stats->rx_length_errors;\n}\n\n \nstatic void ice_napi_disable_all(struct ice_vsi *vsi)\n{\n\tint q_idx;\n\n\tif (!vsi->netdev)\n\t\treturn;\n\n\tice_for_each_q_vector(vsi, q_idx) {\n\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[q_idx];\n\n\t\tif (q_vector->rx.rx_ring || q_vector->tx.tx_ring)\n\t\t\tnapi_disable(&q_vector->napi);\n\n\t\tcancel_work_sync(&q_vector->tx.dim.work);\n\t\tcancel_work_sync(&q_vector->rx.dim.work);\n\t}\n}\n\n \nint ice_down(struct ice_vsi *vsi)\n{\n\tint i, tx_err, rx_err, vlan_err = 0;\n\n\tWARN_ON(!test_bit(ICE_VSI_DOWN, vsi->state));\n\n\tif (vsi->netdev && vsi->type == ICE_VSI_PF) {\n\t\tvlan_err = ice_vsi_del_vlan_zero(vsi);\n\t\tice_ptp_link_change(vsi->back, vsi->back->hw.pf_id, false);\n\t\tnetif_carrier_off(vsi->netdev);\n\t\tnetif_tx_disable(vsi->netdev);\n\t} else if (vsi->type == ICE_VSI_SWITCHDEV_CTRL) {\n\t\tice_eswitch_stop_all_tx_queues(vsi->back);\n\t}\n\n\tice_vsi_dis_irq(vsi);\n\n\ttx_err = ice_vsi_stop_lan_tx_rings(vsi, ICE_NO_RESET, 0);\n\tif (tx_err)\n\t\tnetdev_err(vsi->netdev, \"Failed stop Tx rings, VSI %d error %d\\n\",\n\t\t\t   vsi->vsi_num, tx_err);\n\tif (!tx_err && ice_is_xdp_ena_vsi(vsi)) {\n\t\ttx_err = ice_vsi_stop_xdp_tx_rings(vsi);\n\t\tif (tx_err)\n\t\t\tnetdev_err(vsi->netdev, \"Failed stop XDP rings, VSI %d error %d\\n\",\n\t\t\t\t   vsi->vsi_num, tx_err);\n\t}\n\n\trx_err = ice_vsi_stop_all_rx_rings(vsi);\n\tif (rx_err)\n\t\tnetdev_err(vsi->netdev, \"Failed stop Rx rings, VSI %d error %d\\n\",\n\t\t\t   vsi->vsi_num, rx_err);\n\n\tice_napi_disable_all(vsi);\n\n\tice_for_each_txq(vsi, i)\n\t\tice_clean_tx_ring(vsi->tx_rings[i]);\n\n\tif (ice_is_xdp_ena_vsi(vsi))\n\t\tice_for_each_xdp_txq(vsi, i)\n\t\t\tice_clean_tx_ring(vsi->xdp_rings[i]);\n\n\tice_for_each_rxq(vsi, i)\n\t\tice_clean_rx_ring(vsi->rx_rings[i]);\n\n\tif (tx_err || rx_err || vlan_err) {\n\t\tnetdev_err(vsi->netdev, \"Failed to close VSI 0x%04X on switch 0x%04X\\n\",\n\t\t\t   vsi->vsi_num, vsi->vsw->sw_id);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n \nint ice_down_up(struct ice_vsi *vsi)\n{\n\tint ret;\n\n\t \n\tif (test_and_set_bit(ICE_VSI_DOWN, vsi->state))\n\t\treturn 0;\n\n\tret = ice_down(vsi);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ice_up(vsi);\n\tif (ret) {\n\t\tnetdev_err(vsi->netdev, \"reallocating resources failed during netdev features change, may need to reload driver\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nint ice_vsi_setup_tx_rings(struct ice_vsi *vsi)\n{\n\tint i, err = 0;\n\n\tif (!vsi->num_txq) {\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"VSI %d has 0 Tx queues\\n\",\n\t\t\tvsi->vsi_num);\n\t\treturn -EINVAL;\n\t}\n\n\tice_for_each_txq(vsi, i) {\n\t\tstruct ice_tx_ring *ring = vsi->tx_rings[i];\n\n\t\tif (!ring)\n\t\t\treturn -EINVAL;\n\n\t\tif (vsi->netdev)\n\t\t\tring->netdev = vsi->netdev;\n\t\terr = ice_setup_tx_ring(ring);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n \nint ice_vsi_setup_rx_rings(struct ice_vsi *vsi)\n{\n\tint i, err = 0;\n\n\tif (!vsi->num_rxq) {\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"VSI %d has 0 Rx queues\\n\",\n\t\t\tvsi->vsi_num);\n\t\treturn -EINVAL;\n\t}\n\n\tice_for_each_rxq(vsi, i) {\n\t\tstruct ice_rx_ring *ring = vsi->rx_rings[i];\n\n\t\tif (!ring)\n\t\t\treturn -EINVAL;\n\n\t\tif (vsi->netdev)\n\t\t\tring->netdev = vsi->netdev;\n\t\terr = ice_setup_rx_ring(ring);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n \nint ice_vsi_open_ctrl(struct ice_vsi *vsi)\n{\n\tchar int_name[ICE_INT_NAME_STR_LEN];\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\tint err;\n\n\tdev = ice_pf_to_dev(pf);\n\t \n\terr = ice_vsi_setup_tx_rings(vsi);\n\tif (err)\n\t\tgoto err_setup_tx;\n\n\terr = ice_vsi_setup_rx_rings(vsi);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\terr = ice_vsi_cfg_lan(vsi);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\tsnprintf(int_name, sizeof(int_name) - 1, \"%s-%s:ctrl\",\n\t\t dev_driver_string(dev), dev_name(dev));\n\terr = ice_vsi_req_irq_msix(vsi, int_name);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\tice_vsi_cfg_msix(vsi);\n\n\terr = ice_vsi_start_all_rx_rings(vsi);\n\tif (err)\n\t\tgoto err_up_complete;\n\n\tclear_bit(ICE_VSI_DOWN, vsi->state);\n\tice_vsi_ena_irq(vsi);\n\n\treturn 0;\n\nerr_up_complete:\n\tice_down(vsi);\nerr_setup_rx:\n\tice_vsi_free_rx_rings(vsi);\nerr_setup_tx:\n\tice_vsi_free_tx_rings(vsi);\n\n\treturn err;\n}\n\n \nint ice_vsi_open(struct ice_vsi *vsi)\n{\n\tchar int_name[ICE_INT_NAME_STR_LEN];\n\tstruct ice_pf *pf = vsi->back;\n\tint err;\n\n\t \n\terr = ice_vsi_setup_tx_rings(vsi);\n\tif (err)\n\t\tgoto err_setup_tx;\n\n\terr = ice_vsi_setup_rx_rings(vsi);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\terr = ice_vsi_cfg_lan(vsi);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\tsnprintf(int_name, sizeof(int_name) - 1, \"%s-%s\",\n\t\t dev_driver_string(ice_pf_to_dev(pf)), vsi->netdev->name);\n\terr = ice_vsi_req_irq_msix(vsi, int_name);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\tice_vsi_cfg_netdev_tc(vsi, vsi->tc_cfg.ena_tc);\n\n\tif (vsi->type == ICE_VSI_PF) {\n\t\t \n\t\terr = netif_set_real_num_tx_queues(vsi->netdev, vsi->num_txq);\n\t\tif (err)\n\t\t\tgoto err_set_qs;\n\n\t\terr = netif_set_real_num_rx_queues(vsi->netdev, vsi->num_rxq);\n\t\tif (err)\n\t\t\tgoto err_set_qs;\n\t}\n\n\terr = ice_up_complete(vsi);\n\tif (err)\n\t\tgoto err_up_complete;\n\n\treturn 0;\n\nerr_up_complete:\n\tice_down(vsi);\nerr_set_qs:\n\tice_vsi_free_irq(vsi);\nerr_setup_rx:\n\tice_vsi_free_rx_rings(vsi);\nerr_setup_tx:\n\tice_vsi_free_tx_rings(vsi);\n\n\treturn err;\n}\n\n \nstatic void ice_vsi_release_all(struct ice_pf *pf)\n{\n\tint err, i;\n\n\tif (!pf->vsi)\n\t\treturn;\n\n\tice_for_each_vsi(pf, i) {\n\t\tif (!pf->vsi[i])\n\t\t\tcontinue;\n\n\t\tif (pf->vsi[i]->type == ICE_VSI_CHNL)\n\t\t\tcontinue;\n\n\t\terr = ice_vsi_release(pf->vsi[i]);\n\t\tif (err)\n\t\t\tdev_dbg(ice_pf_to_dev(pf), \"Failed to release pf->vsi[%d], err %d, vsi_num = %d\\n\",\n\t\t\t\ti, err, pf->vsi[i]->vsi_num);\n\t}\n}\n\n \nstatic int ice_vsi_rebuild_by_type(struct ice_pf *pf, enum ice_vsi_type type)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tint i, err;\n\n\tice_for_each_vsi(pf, i) {\n\t\tstruct ice_vsi *vsi = pf->vsi[i];\n\n\t\tif (!vsi || vsi->type != type)\n\t\t\tcontinue;\n\n\t\t \n\t\terr = ice_vsi_rebuild(vsi, ICE_VSI_FLAG_INIT);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"rebuild VSI failed, err %d, VSI index %d, type %s\\n\",\n\t\t\t\terr, vsi->idx, ice_vsi_type_str(type));\n\t\t\treturn err;\n\t\t}\n\n\t\t \n\t\terr = ice_replay_vsi(&pf->hw, vsi->idx);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"replay VSI failed, error %d, VSI index %d, type %s\\n\",\n\t\t\t\terr, vsi->idx, ice_vsi_type_str(type));\n\t\t\treturn err;\n\t\t}\n\n\t\t \n\t\tvsi->vsi_num = ice_get_hw_vsi_num(&pf->hw, vsi->idx);\n\n\t\t \n\t\terr = ice_ena_vsi(vsi, false);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"enable VSI failed, err %d, VSI index %d, type %s\\n\",\n\t\t\t\terr, vsi->idx, ice_vsi_type_str(type));\n\t\t\treturn err;\n\t\t}\n\n\t\tdev_info(dev, \"VSI rebuilt. VSI index %d, type %s\\n\", vsi->idx,\n\t\t\t ice_vsi_type_str(type));\n\t}\n\n\treturn 0;\n}\n\n \nstatic void ice_update_pf_netdev_link(struct ice_pf *pf)\n{\n\tbool link_up;\n\tint i;\n\n\tice_for_each_vsi(pf, i) {\n\t\tstruct ice_vsi *vsi = pf->vsi[i];\n\n\t\tif (!vsi || vsi->type != ICE_VSI_PF)\n\t\t\treturn;\n\n\t\tice_get_link_status(pf->vsi[i]->port_info, &link_up);\n\t\tif (link_up) {\n\t\t\tnetif_carrier_on(pf->vsi[i]->netdev);\n\t\t\tnetif_tx_wake_all_queues(pf->vsi[i]->netdev);\n\t\t} else {\n\t\t\tnetif_carrier_off(pf->vsi[i]->netdev);\n\t\t\tnetif_tx_stop_all_queues(pf->vsi[i]->netdev);\n\t\t}\n\t}\n}\n\n \nstatic void ice_rebuild(struct ice_pf *pf, enum ice_reset_req reset_type)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_hw *hw = &pf->hw;\n\tbool dvm;\n\tint err;\n\n\tif (test_bit(ICE_DOWN, pf->state))\n\t\tgoto clear_recovery;\n\n\tdev_dbg(dev, \"rebuilding PF after reset_type=%d\\n\", reset_type);\n\n#define ICE_EMP_RESET_SLEEP_MS 5000\n\tif (reset_type == ICE_RESET_EMPR) {\n\t\t \n\t\tpf->fw_emp_reset_disabled = false;\n\n\t\tmsleep(ICE_EMP_RESET_SLEEP_MS);\n\t}\n\n\terr = ice_init_all_ctrlq(hw);\n\tif (err) {\n\t\tdev_err(dev, \"control queues init failed %d\\n\", err);\n\t\tgoto err_init_ctrlq;\n\t}\n\n\t \n\tif (!ice_is_safe_mode(pf)) {\n\t\t \n\t\tif (reset_type == ICE_RESET_PFR)\n\t\t\tice_fill_blk_tbls(hw);\n\t\telse\n\t\t\t \n\t\t\tice_load_pkg(NULL, pf);\n\t}\n\n\terr = ice_clear_pf_cfg(hw);\n\tif (err) {\n\t\tdev_err(dev, \"clear PF configuration failed %d\\n\", err);\n\t\tgoto err_init_ctrlq;\n\t}\n\n\tice_clear_pxe_mode(hw);\n\n\terr = ice_init_nvm(hw);\n\tif (err) {\n\t\tdev_err(dev, \"ice_init_nvm failed %d\\n\", err);\n\t\tgoto err_init_ctrlq;\n\t}\n\n\terr = ice_get_caps(hw);\n\tif (err) {\n\t\tdev_err(dev, \"ice_get_caps failed %d\\n\", err);\n\t\tgoto err_init_ctrlq;\n\t}\n\n\terr = ice_aq_set_mac_cfg(hw, ICE_AQ_SET_MAC_FRAME_SIZE_MAX, NULL);\n\tif (err) {\n\t\tdev_err(dev, \"set_mac_cfg failed %d\\n\", err);\n\t\tgoto err_init_ctrlq;\n\t}\n\n\tdvm = ice_is_dvm_ena(hw);\n\n\terr = ice_aq_set_port_params(pf->hw.port_info, dvm, NULL);\n\tif (err)\n\t\tgoto err_init_ctrlq;\n\n\terr = ice_sched_init_port(hw->port_info);\n\tif (err)\n\t\tgoto err_sched_init_port;\n\n\t \n\terr = ice_req_irq_msix_misc(pf);\n\tif (err) {\n\t\tdev_err(dev, \"misc vector setup failed: %d\\n\", err);\n\t\tgoto err_sched_init_port;\n\t}\n\n\tif (test_bit(ICE_FLAG_FD_ENA, pf->flags)) {\n\t\twr32(hw, PFQF_FD_ENA, PFQF_FD_ENA_FD_ENA_M);\n\t\tif (!rd32(hw, PFQF_FD_SIZE)) {\n\t\t\tu16 unused, guar, b_effort;\n\n\t\t\tguar = hw->func_caps.fd_fltr_guar;\n\t\t\tb_effort = hw->func_caps.fd_fltr_best_effort;\n\n\t\t\t \n\t\t\tice_alloc_fd_guar_item(hw, &unused, guar);\n\t\t\t \n\t\t\tice_alloc_fd_shrd_item(hw, &unused, b_effort);\n\t\t}\n\t}\n\n\tif (test_bit(ICE_FLAG_DCB_ENA, pf->flags))\n\t\tice_dcb_rebuild(pf);\n\n\t \n\tif (test_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags))\n\t\tice_ptp_reset(pf);\n\n\tif (ice_is_feature_supported(pf, ICE_F_GNSS))\n\t\tice_gnss_init(pf);\n\n\t \n\terr = ice_vsi_rebuild_by_type(pf, ICE_VSI_PF);\n\tif (err) {\n\t\tdev_err(dev, \"PF VSI rebuild failed: %d\\n\", err);\n\t\tgoto err_vsi_rebuild;\n\t}\n\n\t \n\tif (test_bit(ICE_FLAG_PTP_SUPPORTED, pf->flags))\n\t\tice_ptp_cfg_timestamp(pf, false);\n\n\terr = ice_vsi_rebuild_by_type(pf, ICE_VSI_SWITCHDEV_CTRL);\n\tif (err) {\n\t\tdev_err(dev, \"Switchdev CTRL VSI rebuild failed: %d\\n\", err);\n\t\tgoto err_vsi_rebuild;\n\t}\n\n\tif (reset_type == ICE_RESET_PFR) {\n\t\terr = ice_rebuild_channels(pf);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"failed to rebuild and replay ADQ VSIs, err %d\\n\",\n\t\t\t\terr);\n\t\t\tgoto err_vsi_rebuild;\n\t\t}\n\t}\n\n\t \n\tif (test_bit(ICE_FLAG_FD_ENA, pf->flags)) {\n\t\terr = ice_vsi_rebuild_by_type(pf, ICE_VSI_CTRL);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"control VSI rebuild failed: %d\\n\", err);\n\t\t\tgoto err_vsi_rebuild;\n\t\t}\n\n\t\t \n\t\tif (hw->fdir_prof)\n\t\t\tice_fdir_replay_flows(hw);\n\n\t\t \n\t\tice_fdir_replay_fltrs(pf);\n\n\t\tice_rebuild_arfs(pf);\n\t}\n\n\tice_update_pf_netdev_link(pf);\n\n\t \n\terr = ice_send_version(pf);\n\tif (err) {\n\t\tdev_err(dev, \"Rebuild failed due to error sending driver version: %d\\n\",\n\t\t\terr);\n\t\tgoto err_vsi_rebuild;\n\t}\n\n\tice_replay_post(hw);\n\n\t \n\tclear_bit(ICE_RESET_FAILED, pf->state);\n\n\tice_plug_aux_dev(pf);\n\tif (ice_is_feature_supported(pf, ICE_F_SRIOV_LAG))\n\t\tice_lag_rebuild(pf);\n\treturn;\n\nerr_vsi_rebuild:\nerr_sched_init_port:\n\tice_sched_cleanup_all(hw);\nerr_init_ctrlq:\n\tice_shutdown_all_ctrlq(hw);\n\tset_bit(ICE_RESET_FAILED, pf->state);\nclear_recovery:\n\t \n\tset_bit(ICE_NEEDS_RESTART, pf->state);\n\tdev_err(dev, \"Rebuild failed, unload and reload driver\\n\");\n}\n\n \nstatic int ice_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct bpf_prog *prog;\n\tu8 count = 0;\n\tint err = 0;\n\n\tif (new_mtu == (int)netdev->mtu) {\n\t\tnetdev_warn(netdev, \"MTU is already %u\\n\", netdev->mtu);\n\t\treturn 0;\n\t}\n\n\tprog = vsi->xdp_prog;\n\tif (prog && !prog->aux->xdp_has_frags) {\n\t\tint frame_size = ice_max_xdp_frame_size(vsi);\n\n\t\tif (new_mtu + ICE_ETH_PKT_HDR_PAD > frame_size) {\n\t\t\tnetdev_err(netdev, \"max MTU for XDP usage is %d\\n\",\n\t\t\t\t   frame_size - ICE_ETH_PKT_HDR_PAD);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (test_bit(ICE_FLAG_LEGACY_RX, pf->flags)) {\n\t\tif (new_mtu + ICE_ETH_PKT_HDR_PAD > ICE_MAX_FRAME_LEGACY_RX) {\n\t\t\tnetdev_err(netdev, \"Too big MTU for legacy-rx; Max is %d\\n\",\n\t\t\t\t   ICE_MAX_FRAME_LEGACY_RX - ICE_ETH_PKT_HDR_PAD);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tdo {\n\t\tif (ice_is_reset_in_progress(pf->state)) {\n\t\t\tcount++;\n\t\t\tusleep_range(1000, 2000);\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\n\t} while (count < 100);\n\n\tif (count == 100) {\n\t\tnetdev_err(netdev, \"can't change MTU. Device is busy\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tnetdev->mtu = (unsigned int)new_mtu;\n\terr = ice_down_up(vsi);\n\tif (err)\n\t\treturn err;\n\n\tnetdev_dbg(netdev, \"changed MTU to %d\\n\", new_mtu);\n\tset_bit(ICE_FLAG_MTU_CHANGED, pf->flags);\n\n\treturn err;\n}\n\n \nstatic int ice_eth_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_pf *pf = np->vsi->back;\n\n\tswitch (cmd) {\n\tcase SIOCGHWTSTAMP:\n\t\treturn ice_ptp_get_ts_config(pf, ifr);\n\tcase SIOCSHWTSTAMP:\n\t\treturn ice_ptp_set_ts_config(pf, ifr);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\n \nconst char *ice_aq_str(enum ice_aq_err aq_err)\n{\n\tswitch (aq_err) {\n\tcase ICE_AQ_RC_OK:\n\t\treturn \"OK\";\n\tcase ICE_AQ_RC_EPERM:\n\t\treturn \"ICE_AQ_RC_EPERM\";\n\tcase ICE_AQ_RC_ENOENT:\n\t\treturn \"ICE_AQ_RC_ENOENT\";\n\tcase ICE_AQ_RC_ENOMEM:\n\t\treturn \"ICE_AQ_RC_ENOMEM\";\n\tcase ICE_AQ_RC_EBUSY:\n\t\treturn \"ICE_AQ_RC_EBUSY\";\n\tcase ICE_AQ_RC_EEXIST:\n\t\treturn \"ICE_AQ_RC_EEXIST\";\n\tcase ICE_AQ_RC_EINVAL:\n\t\treturn \"ICE_AQ_RC_EINVAL\";\n\tcase ICE_AQ_RC_ENOSPC:\n\t\treturn \"ICE_AQ_RC_ENOSPC\";\n\tcase ICE_AQ_RC_ENOSYS:\n\t\treturn \"ICE_AQ_RC_ENOSYS\";\n\tcase ICE_AQ_RC_EMODE:\n\t\treturn \"ICE_AQ_RC_EMODE\";\n\tcase ICE_AQ_RC_ENOSEC:\n\t\treturn \"ICE_AQ_RC_ENOSEC\";\n\tcase ICE_AQ_RC_EBADSIG:\n\t\treturn \"ICE_AQ_RC_EBADSIG\";\n\tcase ICE_AQ_RC_ESVN:\n\t\treturn \"ICE_AQ_RC_ESVN\";\n\tcase ICE_AQ_RC_EBADMAN:\n\t\treturn \"ICE_AQ_RC_EBADMAN\";\n\tcase ICE_AQ_RC_EBADBUF:\n\t\treturn \"ICE_AQ_RC_EBADBUF\";\n\t}\n\n\treturn \"ICE_AQ_RC_UNKNOWN\";\n}\n\n \nint ice_set_rss_lut(struct ice_vsi *vsi, u8 *lut, u16 lut_size)\n{\n\tstruct ice_aq_get_set_rss_lut_params params = {};\n\tstruct ice_hw *hw = &vsi->back->hw;\n\tint status;\n\n\tif (!lut)\n\t\treturn -EINVAL;\n\n\tparams.vsi_handle = vsi->idx;\n\tparams.lut_size = lut_size;\n\tparams.lut_type = vsi->rss_lut_type;\n\tparams.lut = lut;\n\n\tstatus = ice_aq_set_rss_lut(hw, &params);\n\tif (status)\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Cannot set RSS lut, err %d aq_err %s\\n\",\n\t\t\tstatus, ice_aq_str(hw->adminq.sq_last_status));\n\n\treturn status;\n}\n\n \nint ice_set_rss_key(struct ice_vsi *vsi, u8 *seed)\n{\n\tstruct ice_hw *hw = &vsi->back->hw;\n\tint status;\n\n\tif (!seed)\n\t\treturn -EINVAL;\n\n\tstatus = ice_aq_set_rss_key(hw, vsi->idx, (struct ice_aqc_get_set_rss_keys *)seed);\n\tif (status)\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Cannot set RSS key, err %d aq_err %s\\n\",\n\t\t\tstatus, ice_aq_str(hw->adminq.sq_last_status));\n\n\treturn status;\n}\n\n \nint ice_get_rss_lut(struct ice_vsi *vsi, u8 *lut, u16 lut_size)\n{\n\tstruct ice_aq_get_set_rss_lut_params params = {};\n\tstruct ice_hw *hw = &vsi->back->hw;\n\tint status;\n\n\tif (!lut)\n\t\treturn -EINVAL;\n\n\tparams.vsi_handle = vsi->idx;\n\tparams.lut_size = lut_size;\n\tparams.lut_type = vsi->rss_lut_type;\n\tparams.lut = lut;\n\n\tstatus = ice_aq_get_rss_lut(hw, &params);\n\tif (status)\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Cannot get RSS lut, err %d aq_err %s\\n\",\n\t\t\tstatus, ice_aq_str(hw->adminq.sq_last_status));\n\n\treturn status;\n}\n\n \nint ice_get_rss_key(struct ice_vsi *vsi, u8 *seed)\n{\n\tstruct ice_hw *hw = &vsi->back->hw;\n\tint status;\n\n\tif (!seed)\n\t\treturn -EINVAL;\n\n\tstatus = ice_aq_get_rss_key(hw, vsi->idx, (struct ice_aqc_get_set_rss_keys *)seed);\n\tif (status)\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Cannot get RSS key, err %d aq_err %s\\n\",\n\t\t\tstatus, ice_aq_str(hw->adminq.sq_last_status));\n\n\treturn status;\n}\n\n \nstatic int\nice_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,\n\t\t   struct net_device *dev, u32 filter_mask, int nlflags)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(dev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_pf *pf = vsi->back;\n\tu16 bmode;\n\n\tbmode = pf->first_sw->bridge_mode;\n\n\treturn ndo_dflt_bridge_getlink(skb, pid, seq, dev, bmode, 0, 0, nlflags,\n\t\t\t\t       filter_mask, NULL);\n}\n\n \nstatic int ice_vsi_update_bridge_mode(struct ice_vsi *vsi, u16 bmode)\n{\n\tstruct ice_aqc_vsi_props *vsi_props;\n\tstruct ice_hw *hw = &vsi->back->hw;\n\tstruct ice_vsi_ctx *ctxt;\n\tint ret;\n\n\tvsi_props = &vsi->info;\n\n\tctxt = kzalloc(sizeof(*ctxt), GFP_KERNEL);\n\tif (!ctxt)\n\t\treturn -ENOMEM;\n\n\tctxt->info = vsi->info;\n\n\tif (bmode == BRIDGE_MODE_VEB)\n\t\t \n\t\tctxt->info.sw_flags |= ICE_AQ_VSI_SW_FLAG_ALLOW_LB;\n\telse\n\t\t \n\t\tctxt->info.sw_flags &= ~ICE_AQ_VSI_SW_FLAG_ALLOW_LB;\n\tctxt->info.valid_sections = cpu_to_le16(ICE_AQ_VSI_PROP_SW_VALID);\n\n\tret = ice_update_vsi(hw, vsi->idx, ctxt, NULL);\n\tif (ret) {\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"update VSI for bridge mode failed, bmode = %d err %d aq_err %s\\n\",\n\t\t\tbmode, ret, ice_aq_str(hw->adminq.sq_last_status));\n\t\tgoto out;\n\t}\n\t \n\tvsi_props->sw_flags = ctxt->info.sw_flags;\n\nout:\n\tkfree(ctxt);\n\treturn ret;\n}\n\n \nstatic int\nice_bridge_setlink(struct net_device *dev, struct nlmsghdr *nlh,\n\t\t   u16 __always_unused flags,\n\t\t   struct netlink_ext_ack __always_unused *extack)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(dev);\n\tstruct ice_pf *pf = np->vsi->back;\n\tstruct nlattr *attr, *br_spec;\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct ice_sw *pf_sw;\n\tint rem, v, err = 0;\n\n\tpf_sw = pf->first_sw;\n\t \n\tbr_spec = nlmsg_find_attr(nlh, sizeof(struct ifinfomsg), IFLA_AF_SPEC);\n\n\tnla_for_each_nested(attr, br_spec, rem) {\n\t\t__u16 mode;\n\n\t\tif (nla_type(attr) != IFLA_BRIDGE_MODE)\n\t\t\tcontinue;\n\t\tmode = nla_get_u16(attr);\n\t\tif (mode != BRIDGE_MODE_VEPA && mode != BRIDGE_MODE_VEB)\n\t\t\treturn -EINVAL;\n\t\t \n\t\tif (mode == pf_sw->bridge_mode)\n\t\t\tcontinue;\n\t\t \n\t\tice_for_each_vsi(pf, v) {\n\t\t\tif (!pf->vsi[v])\n\t\t\t\tcontinue;\n\t\t\terr = ice_vsi_update_bridge_mode(pf->vsi[v], mode);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\thw->evb_veb = (mode == BRIDGE_MODE_VEB);\n\t\t \n\t\terr = ice_update_sw_rule_bridge_mode(hw);\n\t\tif (err) {\n\t\t\tnetdev_err(dev, \"switch rule update failed, mode = %d err %d aq_err %s\\n\",\n\t\t\t\t   mode, err,\n\t\t\t\t   ice_aq_str(hw->adminq.sq_last_status));\n\t\t\t \n\t\t\thw->evb_veb = (pf_sw->bridge_mode == BRIDGE_MODE_VEB);\n\t\t\treturn err;\n\t\t}\n\n\t\tpf_sw->bridge_mode = mode;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void ice_tx_timeout(struct net_device *netdev, unsigned int txqueue)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_tx_ring *tx_ring = NULL;\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_pf *pf = vsi->back;\n\tu32 i;\n\n\tpf->tx_timeout_count++;\n\n\t \n\tif (ice_is_pfc_causing_hung_q(pf, txqueue)) {\n\t\tdev_info(ice_pf_to_dev(pf), \"Fake Tx hang detected on queue %u, timeout caused by PFC storm\\n\",\n\t\t\t txqueue);\n\t\treturn;\n\t}\n\n\t \n\tice_for_each_txq(vsi, i)\n\t\tif (vsi->tx_rings[i] && vsi->tx_rings[i]->desc)\n\t\t\tif (txqueue == vsi->tx_rings[i]->q_index) {\n\t\t\t\ttx_ring = vsi->tx_rings[i];\n\t\t\t\tbreak;\n\t\t\t}\n\n\t \n\tif (time_after(jiffies, (pf->tx_timeout_last_recovery + HZ * 20)))\n\t\tpf->tx_timeout_recovery_level = 1;\n\telse if (time_before(jiffies, (pf->tx_timeout_last_recovery +\n\t\t\t\t       netdev->watchdog_timeo)))\n\t\treturn;\n\n\tif (tx_ring) {\n\t\tstruct ice_hw *hw = &pf->hw;\n\t\tu32 head, val = 0;\n\n\t\thead = (rd32(hw, QTX_COMM_HEAD(vsi->txq_map[txqueue])) &\n\t\t\tQTX_COMM_HEAD_HEAD_M) >> QTX_COMM_HEAD_HEAD_S;\n\t\t \n\t\tval = rd32(hw, GLINT_DYN_CTL(tx_ring->q_vector->reg_idx));\n\n\t\tnetdev_info(netdev, \"tx_timeout: VSI_num: %d, Q %u, NTC: 0x%x, HW_HEAD: 0x%x, NTU: 0x%x, INT: 0x%x\\n\",\n\t\t\t    vsi->vsi_num, txqueue, tx_ring->next_to_clean,\n\t\t\t    head, tx_ring->next_to_use, val);\n\t}\n\n\tpf->tx_timeout_last_recovery = jiffies;\n\tnetdev_info(netdev, \"tx_timeout recovery level %d, txqueue %u\\n\",\n\t\t    pf->tx_timeout_recovery_level, txqueue);\n\n\tswitch (pf->tx_timeout_recovery_level) {\n\tcase 1:\n\t\tset_bit(ICE_PFR_REQ, pf->state);\n\t\tbreak;\n\tcase 2:\n\t\tset_bit(ICE_CORER_REQ, pf->state);\n\t\tbreak;\n\tcase 3:\n\t\tset_bit(ICE_GLOBR_REQ, pf->state);\n\t\tbreak;\n\tdefault:\n\t\tnetdev_err(netdev, \"tx_timeout recovery unsuccessful, device is in unrecoverable state.\\n\");\n\t\tset_bit(ICE_DOWN, pf->state);\n\t\tset_bit(ICE_VSI_NEEDS_RESTART, vsi->state);\n\t\tset_bit(ICE_SERVICE_DIS, pf->state);\n\t\tbreak;\n\t}\n\n\tice_service_task_schedule(pf);\n\tpf->tx_timeout_recovery_level++;\n}\n\n \nstatic int\nice_setup_tc_cls_flower(struct ice_netdev_priv *np,\n\t\t\tstruct net_device *filter_dev,\n\t\t\tstruct flow_cls_offload *cls_flower)\n{\n\tstruct ice_vsi *vsi = np->vsi;\n\n\tif (cls_flower->common.chain_index)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (cls_flower->command) {\n\tcase FLOW_CLS_REPLACE:\n\t\treturn ice_add_cls_flower(filter_dev, vsi, cls_flower);\n\tcase FLOW_CLS_DESTROY:\n\t\treturn ice_del_cls_flower(vsi, cls_flower);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n \nstatic int\nice_setup_tc_block_cb(enum tc_setup_type type, void *type_data, void *cb_priv)\n{\n\tstruct ice_netdev_priv *np = cb_priv;\n\n\tswitch (type) {\n\tcase TC_SETUP_CLSFLOWER:\n\t\treturn ice_setup_tc_cls_flower(np, np->vsi->netdev,\n\t\t\t\t\t       type_data);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\n \nstatic int\nice_validate_mqprio_qopt(struct ice_vsi *vsi,\n\t\t\t struct tc_mqprio_qopt_offload *mqprio_qopt)\n{\n\tint non_power_of_2_qcount = 0;\n\tstruct ice_pf *pf = vsi->back;\n\tint max_rss_q_cnt = 0;\n\tu64 sum_min_rate = 0;\n\tstruct device *dev;\n\tint i, speed;\n\tu8 num_tc;\n\n\tif (vsi->type != ICE_VSI_PF)\n\t\treturn -EINVAL;\n\n\tif (mqprio_qopt->qopt.offset[0] != 0 ||\n\t    mqprio_qopt->qopt.num_tc < 1 ||\n\t    mqprio_qopt->qopt.num_tc > ICE_CHNL_MAX_TC)\n\t\treturn -EINVAL;\n\n\tdev = ice_pf_to_dev(pf);\n\tvsi->ch_rss_size = 0;\n\tnum_tc = mqprio_qopt->qopt.num_tc;\n\tspeed = ice_get_link_speed_kbps(vsi);\n\n\tfor (i = 0; num_tc; i++) {\n\t\tint qcount = mqprio_qopt->qopt.count[i];\n\t\tu64 max_rate, min_rate, rem;\n\n\t\tif (!qcount)\n\t\t\treturn -EINVAL;\n\n\t\tif (is_power_of_2(qcount)) {\n\t\t\tif (non_power_of_2_qcount &&\n\t\t\t    qcount > non_power_of_2_qcount) {\n\t\t\t\tdev_err(dev, \"qcount[%d] cannot be greater than non power of 2 qcount[%d]\\n\",\n\t\t\t\t\tqcount, non_power_of_2_qcount);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (qcount > max_rss_q_cnt)\n\t\t\t\tmax_rss_q_cnt = qcount;\n\t\t} else {\n\t\t\tif (non_power_of_2_qcount &&\n\t\t\t    qcount != non_power_of_2_qcount) {\n\t\t\t\tdev_err(dev, \"Only one non power of 2 qcount allowed[%d,%d]\\n\",\n\t\t\t\t\tqcount, non_power_of_2_qcount);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (qcount < max_rss_q_cnt) {\n\t\t\t\tdev_err(dev, \"non power of 2 qcount[%d] cannot be less than other qcount[%d]\\n\",\n\t\t\t\t\tqcount, max_rss_q_cnt);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tmax_rss_q_cnt = qcount;\n\t\t\tnon_power_of_2_qcount = qcount;\n\t\t}\n\n\t\t \n\t\tmax_rate = mqprio_qopt->max_rate[i];\n\t\tmax_rate = div_u64(max_rate, ICE_BW_KBPS_DIVISOR);\n\n\t\t \n\t\tmin_rate = mqprio_qopt->min_rate[i];\n\t\tmin_rate = div_u64(min_rate, ICE_BW_KBPS_DIVISOR);\n\t\tsum_min_rate += min_rate;\n\n\t\tif (min_rate && min_rate < ICE_MIN_BW_LIMIT) {\n\t\t\tdev_err(dev, \"TC%d: min_rate(%llu Kbps) < %u Kbps\\n\", i,\n\t\t\t\tmin_rate, ICE_MIN_BW_LIMIT);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (max_rate && max_rate > speed) {\n\t\t\tdev_err(dev, \"TC%d: max_rate(%llu Kbps) > link speed of %u Kbps\\n\",\n\t\t\t\ti, max_rate, speed);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\titer_div_u64_rem(min_rate, ICE_MIN_BW_LIMIT, &rem);\n\t\tif (rem) {\n\t\t\tdev_err(dev, \"TC%d: Min Rate not multiple of %u Kbps\",\n\t\t\t\ti, ICE_MIN_BW_LIMIT);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\titer_div_u64_rem(max_rate, ICE_MIN_BW_LIMIT, &rem);\n\t\tif (rem) {\n\t\t\tdev_err(dev, \"TC%d: Max Rate not multiple of %u Kbps\",\n\t\t\t\ti, ICE_MIN_BW_LIMIT);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tif (max_rate && min_rate > max_rate) {\n\t\t\tdev_err(dev, \"min_rate %llu Kbps can't be more than max_rate %llu Kbps\\n\",\n\t\t\t\tmin_rate, max_rate);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (i >= mqprio_qopt->qopt.num_tc - 1)\n\t\t\tbreak;\n\t\tif (mqprio_qopt->qopt.offset[i + 1] !=\n\t\t    (mqprio_qopt->qopt.offset[i] + qcount))\n\t\t\treturn -EINVAL;\n\t}\n\tif (vsi->num_rxq <\n\t    (mqprio_qopt->qopt.offset[i] + mqprio_qopt->qopt.count[i]))\n\t\treturn -EINVAL;\n\tif (vsi->num_txq <\n\t    (mqprio_qopt->qopt.offset[i] + mqprio_qopt->qopt.count[i]))\n\t\treturn -EINVAL;\n\n\tif (sum_min_rate && sum_min_rate > (u64)speed) {\n\t\tdev_err(dev, \"Invalid min Tx rate(%llu) Kbps > speed (%u) Kbps specified\\n\",\n\t\t\tsum_min_rate, speed);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tvsi->ch_rss_size = max_rss_q_cnt;\n\n\treturn 0;\n}\n\n \nstatic int ice_add_vsi_to_fdir(struct ice_pf *pf, struct ice_vsi *vsi)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tbool added = false;\n\tstruct ice_hw *hw;\n\tint flow;\n\n\tif (!(vsi->num_gfltr || vsi->num_bfltr))\n\t\treturn -EINVAL;\n\n\thw = &pf->hw;\n\tfor (flow = 0; flow < ICE_FLTR_PTYPE_MAX; flow++) {\n\t\tstruct ice_fd_hw_prof *prof;\n\t\tint tun, status;\n\t\tu64 entry_h;\n\n\t\tif (!(hw->fdir_prof && hw->fdir_prof[flow] &&\n\t\t      hw->fdir_prof[flow]->cnt))\n\t\t\tcontinue;\n\n\t\tfor (tun = 0; tun < ICE_FD_HW_SEG_MAX; tun++) {\n\t\t\tenum ice_flow_priority prio;\n\t\t\tu64 prof_id;\n\n\t\t\t \n\t\t\tprio = ICE_FLOW_PRIO_NORMAL;\n\t\t\tprof = hw->fdir_prof[flow];\n\t\t\tprof_id = flow + tun * ICE_FLTR_PTYPE_MAX;\n\t\t\tstatus = ice_flow_add_entry(hw, ICE_BLK_FD, prof_id,\n\t\t\t\t\t\t    prof->vsi_h[0], vsi->idx,\n\t\t\t\t\t\t    prio, prof->fdir_seg[tun],\n\t\t\t\t\t\t    &entry_h);\n\t\t\tif (status) {\n\t\t\t\tdev_err(dev, \"channel VSI idx %d, not able to add to group %d\\n\",\n\t\t\t\t\tvsi->idx, flow);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tprof->entry_h[prof->cnt][tun] = entry_h;\n\t\t}\n\n\t\t \n\t\tprof->vsi_h[prof->cnt] = vsi->idx;\n\t\tprof->cnt++;\n\n\t\tadded = true;\n\t\tdev_dbg(dev, \"VSI idx %d added to fdir group %d\\n\", vsi->idx,\n\t\t\tflow);\n\t}\n\n\tif (!added)\n\t\tdev_dbg(dev, \"VSI idx %d not added to fdir groups\\n\", vsi->idx);\n\n\treturn 0;\n}\n\n \nstatic int ice_add_channel(struct ice_pf *pf, u16 sw_id, struct ice_channel *ch)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_vsi *vsi;\n\n\tif (ch->type != ICE_VSI_CHNL) {\n\t\tdev_err(dev, \"add new VSI failed, ch->type %d\\n\", ch->type);\n\t\treturn -EINVAL;\n\t}\n\n\tvsi = ice_chnl_vsi_setup(pf, pf->hw.port_info, ch);\n\tif (!vsi || vsi->type != ICE_VSI_CHNL) {\n\t\tdev_err(dev, \"create chnl VSI failure\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tice_add_vsi_to_fdir(pf, vsi);\n\n\tch->sw_id = sw_id;\n\tch->vsi_num = vsi->vsi_num;\n\tch->info.mapping_flags = vsi->info.mapping_flags;\n\tch->ch_vsi = vsi;\n\t \n\tvsi->ch = ch;\n\n\tmemcpy(&ch->info.q_mapping, &vsi->info.q_mapping,\n\t       sizeof(vsi->info.q_mapping));\n\tmemcpy(&ch->info.tc_mapping, vsi->info.tc_mapping,\n\t       sizeof(vsi->info.tc_mapping));\n\n\treturn 0;\n}\n\n \nstatic void ice_chnl_cfg_res(struct ice_vsi *vsi, struct ice_channel *ch)\n{\n\tint i;\n\n\tfor (i = 0; i < ch->num_txq; i++) {\n\t\tstruct ice_q_vector *tx_q_vector, *rx_q_vector;\n\t\tstruct ice_ring_container *rc;\n\t\tstruct ice_tx_ring *tx_ring;\n\t\tstruct ice_rx_ring *rx_ring;\n\n\t\ttx_ring = vsi->tx_rings[ch->base_q + i];\n\t\trx_ring = vsi->rx_rings[ch->base_q + i];\n\t\tif (!tx_ring || !rx_ring)\n\t\t\tcontinue;\n\n\t\t \n\t\ttx_ring->ch = ch;\n\t\trx_ring->ch = ch;\n\n\t\t \n\t\ttx_q_vector = tx_ring->q_vector;\n\t\trx_q_vector = rx_ring->q_vector;\n\t\tif (!tx_q_vector && !rx_q_vector)\n\t\t\tcontinue;\n\n\t\tif (tx_q_vector) {\n\t\t\ttx_q_vector->ch = ch;\n\t\t\t \n\t\t\trc = &tx_q_vector->tx;\n\t\t\tif (!ITR_IS_DYNAMIC(rc))\n\t\t\t\tice_write_itr(rc, rc->itr_setting);\n\t\t}\n\t\tif (rx_q_vector) {\n\t\t\trx_q_vector->ch = ch;\n\t\t\t \n\t\t\trc = &rx_q_vector->rx;\n\t\t\tif (!ITR_IS_DYNAMIC(rc))\n\t\t\t\tice_write_itr(rc, rc->itr_setting);\n\t\t}\n\t}\n\n\t \n\tif (ch->num_txq || ch->num_rxq)\n\t\tice_flush(&vsi->back->hw);\n}\n\n \nstatic void\nice_cfg_chnl_all_res(struct ice_vsi *vsi, struct ice_channel *ch)\n{\n\t \n\tice_chnl_cfg_res(vsi, ch);\n}\n\n \nstatic int\nice_setup_hw_channel(struct ice_pf *pf, struct ice_vsi *vsi,\n\t\t     struct ice_channel *ch, u16 sw_id, u8 type)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tint ret;\n\n\tch->base_q = vsi->next_base_q;\n\tch->type = type;\n\n\tret = ice_add_channel(pf, sw_id, ch);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to add_channel using sw_id %u\\n\", sw_id);\n\t\treturn ret;\n\t}\n\n\t \n\tice_cfg_chnl_all_res(vsi, ch);\n\n\t \n\tvsi->next_base_q = vsi->next_base_q + ch->num_rxq;\n\tdev_dbg(dev, \"added channel: vsi_num %u, num_rxq %u\\n\", ch->vsi_num,\n\t\tch->num_rxq);\n\n\treturn 0;\n}\n\n \nstatic bool\nice_setup_channel(struct ice_pf *pf, struct ice_vsi *vsi,\n\t\t  struct ice_channel *ch)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tu16 sw_id;\n\tint ret;\n\n\tif (vsi->type != ICE_VSI_PF) {\n\t\tdev_err(dev, \"unsupported parent VSI type(%d)\\n\", vsi->type);\n\t\treturn false;\n\t}\n\n\tsw_id = pf->first_sw->sw_id;\n\n\t \n\tret = ice_setup_hw_channel(pf, vsi, ch, sw_id, ICE_VSI_CHNL);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to setup hw_channel\\n\");\n\t\treturn false;\n\t}\n\tdev_dbg(dev, \"successfully created channel()\\n\");\n\n\treturn ch->ch_vsi ? true : false;\n}\n\n \nstatic int\nice_set_bw_limit(struct ice_vsi *vsi, u64 max_tx_rate, u64 min_tx_rate)\n{\n\tint err;\n\n\terr = ice_set_min_bw_limit(vsi, min_tx_rate);\n\tif (err)\n\t\treturn err;\n\n\treturn ice_set_max_bw_limit(vsi, max_tx_rate);\n}\n\n \nstatic int ice_create_q_channel(struct ice_vsi *vsi, struct ice_channel *ch)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\n\tif (!ch)\n\t\treturn -EINVAL;\n\n\tdev = ice_pf_to_dev(pf);\n\tif (!ch->num_txq || !ch->num_rxq) {\n\t\tdev_err(dev, \"Invalid num_queues requested: %d\\n\", ch->num_rxq);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!vsi->cnt_q_avail || vsi->cnt_q_avail < ch->num_txq) {\n\t\tdev_err(dev, \"cnt_q_avail (%u) less than num_queues %d\\n\",\n\t\t\tvsi->cnt_q_avail, ch->num_txq);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ice_setup_channel(pf, vsi, ch)) {\n\t\tdev_info(dev, \"Failed to setup channel\\n\");\n\t\treturn -EINVAL;\n\t}\n\t \n\tif (ch->ch_vsi && (ch->max_tx_rate || ch->min_tx_rate)) {\n\t\tint ret;\n\n\t\tret = ice_set_bw_limit(ch->ch_vsi, ch->max_tx_rate,\n\t\t\t\t       ch->min_tx_rate);\n\t\tif (ret)\n\t\t\tdev_err(dev, \"failed to set Tx rate of %llu Kbps for VSI(%u)\\n\",\n\t\t\t\tch->max_tx_rate, ch->ch_vsi->vsi_num);\n\t\telse\n\t\t\tdev_dbg(dev, \"set Tx rate of %llu Kbps for VSI(%u)\\n\",\n\t\t\t\tch->max_tx_rate, ch->ch_vsi->vsi_num);\n\t}\n\n\tvsi->cnt_q_avail -= ch->num_txq;\n\n\treturn 0;\n}\n\n \nstatic void ice_rem_all_chnl_fltrs(struct ice_pf *pf)\n{\n\tstruct ice_tc_flower_fltr *fltr;\n\tstruct hlist_node *node;\n\n\t \n\thlist_for_each_entry_safe(fltr, node,\n\t\t\t\t  &pf->tc_flower_fltr_list,\n\t\t\t\t  tc_flower_node) {\n\t\tstruct ice_rule_query_data rule;\n\t\tint status;\n\n\t\t \n\t\tif (!ice_is_chnl_fltr(fltr))\n\t\t\tcontinue;\n\n\t\trule.rid = fltr->rid;\n\t\trule.rule_id = fltr->rule_id;\n\t\trule.vsi_handle = fltr->dest_vsi_handle;\n\t\tstatus = ice_rem_adv_rule_by_id(&pf->hw, &rule);\n\t\tif (status) {\n\t\t\tif (status == -ENOENT)\n\t\t\t\tdev_dbg(ice_pf_to_dev(pf), \"TC flower filter (rule_id %u) does not exist\\n\",\n\t\t\t\t\trule.rule_id);\n\t\t\telse\n\t\t\t\tdev_err(ice_pf_to_dev(pf), \"failed to delete TC flower filter, status %d\\n\",\n\t\t\t\t\tstatus);\n\t\t} else if (fltr->dest_vsi) {\n\t\t\t \n\t\t\tif (fltr->dest_vsi->type == ICE_VSI_CHNL) {\n\t\t\t\tu32 flags = fltr->flags;\n\n\t\t\t\tfltr->dest_vsi->num_chnl_fltr--;\n\t\t\t\tif (flags & (ICE_TC_FLWR_FIELD_DST_MAC |\n\t\t\t\t\t     ICE_TC_FLWR_FIELD_ENC_DST_MAC))\n\t\t\t\t\tpf->num_dmac_chnl_fltrs--;\n\t\t\t}\n\t\t}\n\n\t\thlist_del(&fltr->tc_flower_node);\n\t\tkfree(fltr);\n\t}\n}\n\n \nstatic void ice_remove_q_channels(struct ice_vsi *vsi, bool rem_fltr)\n{\n\tstruct ice_channel *ch, *ch_tmp;\n\tstruct ice_pf *pf = vsi->back;\n\tint i;\n\n\t \n\tif (rem_fltr)\n\t\tice_rem_all_chnl_fltrs(pf);\n\n\t \n\tif  (vsi->netdev->features & NETIF_F_NTUPLE) {\n\t\tstruct ice_hw *hw = &pf->hw;\n\n\t\tmutex_lock(&hw->fdir_fltr_lock);\n\t\tice_fdir_del_all_fltrs(vsi);\n\t\tmutex_unlock(&hw->fdir_fltr_lock);\n\t}\n\n\t \n\tlist_for_each_entry_safe(ch, ch_tmp, &vsi->ch_list, list) {\n\t\tstruct ice_vsi *ch_vsi;\n\n\t\tlist_del(&ch->list);\n\t\tch_vsi = ch->ch_vsi;\n\t\tif (!ch_vsi) {\n\t\t\tkfree(ch);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < ch->num_rxq; i++) {\n\t\t\tstruct ice_tx_ring *tx_ring;\n\t\t\tstruct ice_rx_ring *rx_ring;\n\n\t\t\ttx_ring = vsi->tx_rings[ch->base_q + i];\n\t\t\trx_ring = vsi->rx_rings[ch->base_q + i];\n\t\t\tif (tx_ring) {\n\t\t\t\ttx_ring->ch = NULL;\n\t\t\t\tif (tx_ring->q_vector)\n\t\t\t\t\ttx_ring->q_vector->ch = NULL;\n\t\t\t}\n\t\t\tif (rx_ring) {\n\t\t\t\trx_ring->ch = NULL;\n\t\t\t\tif (rx_ring->q_vector)\n\t\t\t\t\trx_ring->q_vector->ch = NULL;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tice_fdir_rem_adq_chnl(&pf->hw, ch->ch_vsi->idx);\n\n\t\t \n\t\tice_rm_vsi_lan_cfg(ch->ch_vsi->port_info, ch->ch_vsi->idx);\n\n\t\t \n\t\tice_vsi_delete(ch->ch_vsi);\n\n\t\t \n\t\tkfree(ch);\n\t}\n\n\t \n\tice_for_each_chnl_tc(i)\n\t\tvsi->tc_map_vsi[i] = NULL;\n\n\t \n\tvsi->all_enatc = 0;\n\tvsi->all_numtc = 0;\n}\n\n \nstatic int ice_rebuild_channels(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_vsi *main_vsi;\n\tbool rem_adv_fltr = true;\n\tstruct ice_channel *ch;\n\tstruct ice_vsi *vsi;\n\tint tc_idx = 1;\n\tint i, err;\n\n\tmain_vsi = ice_get_main_vsi(pf);\n\tif (!main_vsi)\n\t\treturn 0;\n\n\tif (!test_bit(ICE_FLAG_TC_MQPRIO, pf->flags) ||\n\t    main_vsi->old_numtc == 1)\n\t\treturn 0;  \n\n\t \n\terr = ice_vsi_cfg_tc(main_vsi, main_vsi->old_ena_tc);\n\tif (err) {\n\t\tdev_err(dev, \"failed configuring TC(ena_tc:0x%02x) for HW VSI=%u\\n\",\n\t\t\tmain_vsi->old_ena_tc, main_vsi->vsi_num);\n\t\treturn err;\n\t}\n\n\t \n\tice_for_each_vsi(pf, i) {\n\t\tenum ice_vsi_type type;\n\n\t\tvsi = pf->vsi[i];\n\t\tif (!vsi || vsi->type != ICE_VSI_CHNL)\n\t\t\tcontinue;\n\n\t\ttype = vsi->type;\n\n\t\t \n\t\terr = ice_vsi_rebuild(vsi, ICE_VSI_FLAG_INIT);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"VSI (type:%s) at index %d rebuild failed, err %d\\n\",\n\t\t\t\tice_vsi_type_str(type), vsi->idx, err);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\t \n\t\tvsi->vsi_num = ice_get_hw_vsi_num(&pf->hw, vsi->idx);\n\n\t\t \n\t\terr = ice_replay_vsi(&pf->hw, vsi->idx);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"VSI (type:%s) replay failed, err %d, VSI index %d\\n\",\n\t\t\t\tice_vsi_type_str(type), err, vsi->idx);\n\t\t\trem_adv_fltr = false;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tdev_info(dev, \"VSI (type:%s) at index %d rebuilt successfully\\n\",\n\t\t\t ice_vsi_type_str(type), vsi->idx);\n\n\t\t \n\t\tmain_vsi->tc_map_vsi[tc_idx++] = vsi;\n\t}\n\n\t \n\tlist_for_each_entry(ch, &main_vsi->ch_list, list) {\n\t\tstruct ice_vsi *ch_vsi;\n\n\t\tch_vsi = ch->ch_vsi;\n\t\tif (!ch_vsi)\n\t\t\tcontinue;\n\n\t\t \n\t\tice_cfg_chnl_all_res(main_vsi, ch);\n\n\t\t \n\t\tif (!ch->max_tx_rate && !ch->min_tx_rate)\n\t\t\tcontinue;\n\n\t\terr = ice_set_bw_limit(ch_vsi, ch->max_tx_rate,\n\t\t\t\t       ch->min_tx_rate);\n\t\tif (err)\n\t\t\tdev_err(dev, \"failed (err:%d) to rebuild BW rate limit, max_tx_rate: %llu Kbps, min_tx_rate: %llu Kbps for VSI(%u)\\n\",\n\t\t\t\terr, ch->max_tx_rate, ch->min_tx_rate,\n\t\t\t\tch_vsi->vsi_num);\n\t\telse\n\t\t\tdev_dbg(dev, \"successfully rebuild BW rate limit, max_tx_rate: %llu Kbps, min_tx_rate: %llu Kbps for VSI(%u)\\n\",\n\t\t\t\tch->max_tx_rate, ch->min_tx_rate,\n\t\t\t\tch_vsi->vsi_num);\n\t}\n\n\t \n\tif (main_vsi->ch_rss_size)\n\t\tice_vsi_cfg_rss_lut_key(main_vsi);\n\n\treturn 0;\n\ncleanup:\n\tice_remove_q_channels(main_vsi, rem_adv_fltr);\n\treturn err;\n}\n\n \nstatic int ice_create_q_channels(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_channel *ch;\n\tint ret = 0, i;\n\n\tice_for_each_chnl_tc(i) {\n\t\tif (!(vsi->all_enatc & BIT(i)))\n\t\t\tcontinue;\n\n\t\tch = kzalloc(sizeof(*ch), GFP_KERNEL);\n\t\tif (!ch) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free;\n\t\t}\n\t\tINIT_LIST_HEAD(&ch->list);\n\t\tch->num_rxq = vsi->mqprio_qopt.qopt.count[i];\n\t\tch->num_txq = vsi->mqprio_qopt.qopt.count[i];\n\t\tch->base_q = vsi->mqprio_qopt.qopt.offset[i];\n\t\tch->max_tx_rate = vsi->mqprio_qopt.max_rate[i];\n\t\tch->min_tx_rate = vsi->mqprio_qopt.min_rate[i];\n\n\t\t \n\t\tif (ch->max_tx_rate)\n\t\t\tch->max_tx_rate = div_u64(ch->max_tx_rate,\n\t\t\t\t\t\t  ICE_BW_KBPS_DIVISOR);\n\t\tif (ch->min_tx_rate)\n\t\t\tch->min_tx_rate = div_u64(ch->min_tx_rate,\n\t\t\t\t\t\t  ICE_BW_KBPS_DIVISOR);\n\n\t\tret = ice_create_q_channel(vsi, ch);\n\t\tif (ret) {\n\t\t\tdev_err(ice_pf_to_dev(pf),\n\t\t\t\t\"failed creating channel TC:%d\\n\", i);\n\t\t\tkfree(ch);\n\t\t\tgoto err_free;\n\t\t}\n\t\tlist_add_tail(&ch->list, &vsi->ch_list);\n\t\tvsi->tc_map_vsi[i] = ch->ch_vsi;\n\t\tdev_dbg(ice_pf_to_dev(pf),\n\t\t\t\"successfully created channel: VSI %pK\\n\", ch->ch_vsi);\n\t}\n\treturn 0;\n\nerr_free:\n\tice_remove_q_channels(vsi, false);\n\n\treturn ret;\n}\n\n \nstatic int ice_setup_tc_mqprio_qdisc(struct net_device *netdev, void *type_data)\n{\n\tstruct tc_mqprio_qopt_offload *mqprio_qopt = type_data;\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_pf *pf = vsi->back;\n\tu16 mode, ena_tc_qdisc = 0;\n\tint cur_txq, cur_rxq;\n\tu8 hw = 0, num_tcf;\n\tstruct device *dev;\n\tint ret, i;\n\n\tdev = ice_pf_to_dev(pf);\n\tnum_tcf = mqprio_qopt->qopt.num_tc;\n\thw = mqprio_qopt->qopt.hw;\n\tmode = mqprio_qopt->mode;\n\tif (!hw) {\n\t\tclear_bit(ICE_FLAG_TC_MQPRIO, pf->flags);\n\t\tvsi->ch_rss_size = 0;\n\t\tmemcpy(&vsi->mqprio_qopt, mqprio_qopt, sizeof(*mqprio_qopt));\n\t\tgoto config_tcf;\n\t}\n\n\t \n\tfor (i = 0; i < num_tcf; i++)\n\t\tena_tc_qdisc |= BIT(i);\n\n\tswitch (mode) {\n\tcase TC_MQPRIO_MODE_CHANNEL:\n\n\t\tif (pf->hw.port_info->is_custom_tx_enabled) {\n\t\t\tdev_err(dev, \"Custom Tx scheduler feature enabled, can't configure ADQ\\n\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tice_tear_down_devlink_rate_tree(pf);\n\n\t\tret = ice_validate_mqprio_qopt(vsi, mqprio_qopt);\n\t\tif (ret) {\n\t\t\tnetdev_err(netdev, \"failed to validate_mqprio_qopt(), ret %d\\n\",\n\t\t\t\t   ret);\n\t\t\treturn ret;\n\t\t}\n\t\tmemcpy(&vsi->mqprio_qopt, mqprio_qopt, sizeof(*mqprio_qopt));\n\t\tset_bit(ICE_FLAG_TC_MQPRIO, pf->flags);\n\t\t \n\t\tif (vsi->netdev->features & NETIF_F_HW_TC)\n\t\t\tset_bit(ICE_FLAG_CLS_FLOWER, pf->flags);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\nconfig_tcf:\n\n\t \n\tif (ena_tc_qdisc == vsi->tc_cfg.ena_tc &&\n\t    mode != TC_MQPRIO_MODE_CHANNEL)\n\t\treturn 0;\n\n\t \n\tice_dis_vsi(vsi, true);\n\n\tif (!hw && !test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))\n\t\tice_remove_q_channels(vsi, true);\n\n\tif (!hw && !test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {\n\t\tvsi->req_txq = min_t(int, ice_get_avail_txq_count(pf),\n\t\t\t\t     num_online_cpus());\n\t\tvsi->req_rxq = min_t(int, ice_get_avail_rxq_count(pf),\n\t\t\t\t     num_online_cpus());\n\t} else {\n\t\t \n\t\tu16 offset = 0, qcount_tx = 0, qcount_rx = 0;\n\n\t\tfor (i = 0; i < num_tcf; i++) {\n\t\t\tif (!(ena_tc_qdisc & BIT(i)))\n\t\t\t\tcontinue;\n\n\t\t\toffset = vsi->mqprio_qopt.qopt.offset[i];\n\t\t\tqcount_rx = vsi->mqprio_qopt.qopt.count[i];\n\t\t\tqcount_tx = vsi->mqprio_qopt.qopt.count[i];\n\t\t}\n\t\tvsi->req_txq = offset + qcount_tx;\n\t\tvsi->req_rxq = offset + qcount_rx;\n\n\t\t \n\t\tvsi->orig_rss_size = vsi->rss_size;\n\t}\n\n\t \n\tcur_txq = vsi->num_txq;\n\tcur_rxq = vsi->num_rxq;\n\n\t \n\tret = ice_vsi_rebuild(vsi, ICE_VSI_FLAG_NO_INIT);\n\tif (ret) {\n\t\t \n\t\tdev_info(dev, \"Rebuild failed with new queues, try with current number of queues\\n\");\n\t\tvsi->req_txq = cur_txq;\n\t\tvsi->req_rxq = cur_rxq;\n\t\tclear_bit(ICE_RESET_FAILED, pf->state);\n\t\tif (ice_vsi_rebuild(vsi, ICE_VSI_FLAG_NO_INIT)) {\n\t\t\tdev_err(dev, \"Rebuild of main VSI failed again\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tvsi->all_numtc = num_tcf;\n\tvsi->all_enatc = ena_tc_qdisc;\n\tret = ice_vsi_cfg_tc(vsi, ena_tc_qdisc);\n\tif (ret) {\n\t\tnetdev_err(netdev, \"failed configuring TC for VSI id=%d\\n\",\n\t\t\t   vsi->vsi_num);\n\t\tgoto exit;\n\t}\n\n\tif (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {\n\t\tu64 max_tx_rate = vsi->mqprio_qopt.max_rate[0];\n\t\tu64 min_tx_rate = vsi->mqprio_qopt.min_rate[0];\n\n\t\t \n\t\tif (max_tx_rate || min_tx_rate) {\n\t\t\t \n\t\t\tif (max_tx_rate)\n\t\t\t\tmax_tx_rate = div_u64(max_tx_rate, ICE_BW_KBPS_DIVISOR);\n\t\t\tif (min_tx_rate)\n\t\t\t\tmin_tx_rate = div_u64(min_tx_rate, ICE_BW_KBPS_DIVISOR);\n\n\t\t\tret = ice_set_bw_limit(vsi, max_tx_rate, min_tx_rate);\n\t\t\tif (!ret) {\n\t\t\t\tdev_dbg(dev, \"set Tx rate max %llu min %llu for VSI(%u)\\n\",\n\t\t\t\t\tmax_tx_rate, min_tx_rate, vsi->vsi_num);\n\t\t\t} else {\n\t\t\t\tdev_err(dev, \"failed to set Tx rate max %llu min %llu for VSI(%u)\\n\",\n\t\t\t\t\tmax_tx_rate, min_tx_rate, vsi->vsi_num);\n\t\t\t\tgoto exit;\n\t\t\t}\n\t\t}\n\t\tret = ice_create_q_channels(vsi);\n\t\tif (ret) {\n\t\t\tnetdev_err(netdev, \"failed configuring queue channels\\n\");\n\t\t\tgoto exit;\n\t\t} else {\n\t\t\tnetdev_dbg(netdev, \"successfully configured channels\\n\");\n\t\t}\n\t}\n\n\tif (vsi->ch_rss_size)\n\t\tice_vsi_cfg_rss_lut_key(vsi);\n\nexit:\n\t \n\tif (ret) {\n\t\tvsi->all_numtc = 0;\n\t\tvsi->all_enatc = 0;\n\t}\n\t \n\tice_ena_vsi(vsi, true);\n\n\treturn ret;\n}\n\nstatic LIST_HEAD(ice_block_cb_list);\n\nstatic int\nice_setup_tc(struct net_device *netdev, enum tc_setup_type type,\n\t     void *type_data)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_pf *pf = np->vsi->back;\n\tbool locked = false;\n\tint err;\n\n\tswitch (type) {\n\tcase TC_SETUP_BLOCK:\n\t\treturn flow_block_cb_setup_simple(type_data,\n\t\t\t\t\t\t  &ice_block_cb_list,\n\t\t\t\t\t\t  ice_setup_tc_block_cb,\n\t\t\t\t\t\t  np, np, true);\n\tcase TC_SETUP_QDISC_MQPRIO:\n\t\tif (ice_is_eswitch_mode_switchdev(pf)) {\n\t\t\tnetdev_err(netdev, \"TC MQPRIO offload not supported, switchdev is enabled\\n\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (pf->adev) {\n\t\t\tmutex_lock(&pf->adev_mutex);\n\t\t\tdevice_lock(&pf->adev->dev);\n\t\t\tlocked = true;\n\t\t\tif (pf->adev->dev.driver) {\n\t\t\t\tnetdev_err(netdev, \"Cannot change qdisc when RDMA is active\\n\");\n\t\t\t\terr = -EBUSY;\n\t\t\t\tgoto adev_unlock;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tmutex_lock(&pf->tc_mutex);\n\t\terr = ice_setup_tc_mqprio_qdisc(netdev, type_data);\n\t\tmutex_unlock(&pf->tc_mutex);\n\nadev_unlock:\n\t\tif (locked) {\n\t\t\tdevice_unlock(&pf->adev->dev);\n\t\t\tmutex_unlock(&pf->adev_mutex);\n\t\t}\n\t\treturn err;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn -EOPNOTSUPP;\n}\n\nstatic struct ice_indr_block_priv *\nice_indr_block_priv_lookup(struct ice_netdev_priv *np,\n\t\t\t   struct net_device *netdev)\n{\n\tstruct ice_indr_block_priv *cb_priv;\n\n\tlist_for_each_entry(cb_priv, &np->tc_indr_block_priv_list, list) {\n\t\tif (!cb_priv->netdev)\n\t\t\treturn NULL;\n\t\tif (cb_priv->netdev == netdev)\n\t\t\treturn cb_priv;\n\t}\n\treturn NULL;\n}\n\nstatic int\nice_indr_setup_block_cb(enum tc_setup_type type, void *type_data,\n\t\t\tvoid *indr_priv)\n{\n\tstruct ice_indr_block_priv *priv = indr_priv;\n\tstruct ice_netdev_priv *np = priv->np;\n\n\tswitch (type) {\n\tcase TC_SETUP_CLSFLOWER:\n\t\treturn ice_setup_tc_cls_flower(np, priv->netdev,\n\t\t\t\t\t       (struct flow_cls_offload *)\n\t\t\t\t\t       type_data);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int\nice_indr_setup_tc_block(struct net_device *netdev, struct Qdisc *sch,\n\t\t\tstruct ice_netdev_priv *np,\n\t\t\tstruct flow_block_offload *f, void *data,\n\t\t\tvoid (*cleanup)(struct flow_block_cb *block_cb))\n{\n\tstruct ice_indr_block_priv *indr_priv;\n\tstruct flow_block_cb *block_cb;\n\n\tif (!ice_is_tunnel_supported(netdev) &&\n\t    !(is_vlan_dev(netdev) &&\n\t      vlan_dev_real_dev(netdev) == np->vsi->netdev))\n\t\treturn -EOPNOTSUPP;\n\n\tif (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (f->command) {\n\tcase FLOW_BLOCK_BIND:\n\t\tindr_priv = ice_indr_block_priv_lookup(np, netdev);\n\t\tif (indr_priv)\n\t\t\treturn -EEXIST;\n\n\t\tindr_priv = kzalloc(sizeof(*indr_priv), GFP_KERNEL);\n\t\tif (!indr_priv)\n\t\t\treturn -ENOMEM;\n\n\t\tindr_priv->netdev = netdev;\n\t\tindr_priv->np = np;\n\t\tlist_add(&indr_priv->list, &np->tc_indr_block_priv_list);\n\n\t\tblock_cb =\n\t\t\tflow_indr_block_cb_alloc(ice_indr_setup_block_cb,\n\t\t\t\t\t\t indr_priv, indr_priv,\n\t\t\t\t\t\t ice_rep_indr_tc_block_unbind,\n\t\t\t\t\t\t f, netdev, sch, data, np,\n\t\t\t\t\t\t cleanup);\n\n\t\tif (IS_ERR(block_cb)) {\n\t\t\tlist_del(&indr_priv->list);\n\t\t\tkfree(indr_priv);\n\t\t\treturn PTR_ERR(block_cb);\n\t\t}\n\t\tflow_block_cb_add(block_cb, f);\n\t\tlist_add_tail(&block_cb->driver_list, &ice_block_cb_list);\n\t\tbreak;\n\tcase FLOW_BLOCK_UNBIND:\n\t\tindr_priv = ice_indr_block_priv_lookup(np, netdev);\n\t\tif (!indr_priv)\n\t\t\treturn -ENOENT;\n\n\t\tblock_cb = flow_block_cb_lookup(f->block,\n\t\t\t\t\t\tice_indr_setup_block_cb,\n\t\t\t\t\t\tindr_priv);\n\t\tif (!block_cb)\n\t\t\treturn -ENOENT;\n\n\t\tflow_indr_block_cb_remove(block_cb, f);\n\n\t\tlist_del(&block_cb->driver_list);\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}\n\nstatic int\nice_indr_setup_tc_cb(struct net_device *netdev, struct Qdisc *sch,\n\t\t     void *cb_priv, enum tc_setup_type type, void *type_data,\n\t\t     void *data,\n\t\t     void (*cleanup)(struct flow_block_cb *block_cb))\n{\n\tswitch (type) {\n\tcase TC_SETUP_BLOCK:\n\t\treturn ice_indr_setup_tc_block(netdev, sch, cb_priv, type_data,\n\t\t\t\t\t       data, cleanup);\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\n \nint ice_open(struct net_device *netdev)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_pf *pf = np->vsi->back;\n\n\tif (ice_is_reset_in_progress(pf->state)) {\n\t\tnetdev_err(netdev, \"can't open net device while reset is in progress\");\n\t\treturn -EBUSY;\n\t}\n\n\treturn ice_open_internal(netdev);\n}\n\n \nint ice_open_internal(struct net_device *netdev)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_port_info *pi;\n\tint err;\n\n\tif (test_bit(ICE_NEEDS_RESTART, pf->state)) {\n\t\tnetdev_err(netdev, \"driver needs to be unloaded and reloaded\\n\");\n\t\treturn -EIO;\n\t}\n\n\tnetif_carrier_off(netdev);\n\n\tpi = vsi->port_info;\n\terr = ice_update_link_info(pi);\n\tif (err) {\n\t\tnetdev_err(netdev, \"Failed to get link info, error %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tice_check_link_cfg_err(pf, pi->phy.link_info.link_cfg_err);\n\n\t \n\tif (pi->phy.link_info.link_info & ICE_AQ_MEDIA_AVAILABLE) {\n\t\tclear_bit(ICE_FLAG_NO_MEDIA, pf->flags);\n\t\tif (!test_bit(ICE_PHY_INIT_COMPLETE, pf->state)) {\n\t\t\terr = ice_init_phy_user_cfg(pi);\n\t\t\tif (err) {\n\t\t\t\tnetdev_err(netdev, \"Failed to initialize PHY settings, error %d\\n\",\n\t\t\t\t\t   err);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\terr = ice_configure_phy(vsi);\n\t\tif (err) {\n\t\t\tnetdev_err(netdev, \"Failed to set physical link up, error %d\\n\",\n\t\t\t\t   err);\n\t\t\treturn err;\n\t\t}\n\t} else {\n\t\tset_bit(ICE_FLAG_NO_MEDIA, pf->flags);\n\t\tice_set_link(vsi, false);\n\t}\n\n\terr = ice_vsi_open(vsi);\n\tif (err)\n\t\tnetdev_err(netdev, \"Failed to open VSI 0x%04X on switch 0x%04X\\n\",\n\t\t\t   vsi->vsi_num, vsi->vsw->sw_id);\n\n\t \n\tudp_tunnel_get_rx_info(netdev);\n\n\treturn err;\n}\n\n \nint ice_stop(struct net_device *netdev)\n{\n\tstruct ice_netdev_priv *np = netdev_priv(netdev);\n\tstruct ice_vsi *vsi = np->vsi;\n\tstruct ice_pf *pf = vsi->back;\n\n\tif (ice_is_reset_in_progress(pf->state)) {\n\t\tnetdev_err(netdev, \"can't stop net device while reset is in progress\");\n\t\treturn -EBUSY;\n\t}\n\n\tif (test_bit(ICE_FLAG_LINK_DOWN_ON_CLOSE_ENA, vsi->back->flags)) {\n\t\tint link_err = ice_force_phys_link_state(vsi, false);\n\n\t\tif (link_err) {\n\t\t\tif (link_err == -ENOMEDIUM)\n\t\t\t\tnetdev_info(vsi->netdev, \"Skipping link reconfig - no media attached, VSI %d\\n\",\n\t\t\t\t\t    vsi->vsi_num);\n\t\t\telse\n\t\t\t\tnetdev_err(vsi->netdev, \"Failed to set physical link down, VSI %d error %d\\n\",\n\t\t\t\t\t   vsi->vsi_num, link_err);\n\n\t\t\tice_vsi_close(vsi);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tice_vsi_close(vsi);\n\n\treturn 0;\n}\n\n \nstatic netdev_features_t\nice_features_check(struct sk_buff *skb,\n\t\t   struct net_device __always_unused *netdev,\n\t\t   netdev_features_t features)\n{\n\tbool gso = skb_is_gso(skb);\n\tsize_t len;\n\n\t \n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn features;\n\n\t \n\tif (gso && (skb_shinfo(skb)->gso_size < ICE_TXD_CTX_MIN_MSS))\n\t\tfeatures &= ~NETIF_F_GSO_MASK;\n\n\tlen = skb_network_offset(skb);\n\tif (len > ICE_TXD_MACLEN_MAX || len & 0x1)\n\t\tgoto out_rm_features;\n\n\tlen = skb_network_header_len(skb);\n\tif (len > ICE_TXD_IPLEN_MAX || len & 0x1)\n\t\tgoto out_rm_features;\n\n\tif (skb->encapsulation) {\n\t\t \n\t\tif (gso && (skb_shinfo(skb)->gso_type &\n\t\t\t    (SKB_GSO_GRE | SKB_GSO_UDP_TUNNEL))) {\n\t\t\tlen = skb_inner_network_header(skb) -\n\t\t\t      skb_transport_header(skb);\n\t\t\tif (len > ICE_TXD_L4LEN_MAX || len & 0x1)\n\t\t\t\tgoto out_rm_features;\n\t\t}\n\n\t\tlen = skb_inner_network_header_len(skb);\n\t\tif (len > ICE_TXD_IPLEN_MAX || len & 0x1)\n\t\t\tgoto out_rm_features;\n\t}\n\n\treturn features;\nout_rm_features:\n\treturn features & ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n}\n\nstatic const struct net_device_ops ice_netdev_safe_mode_ops = {\n\t.ndo_open = ice_open,\n\t.ndo_stop = ice_stop,\n\t.ndo_start_xmit = ice_start_xmit,\n\t.ndo_set_mac_address = ice_set_mac_address,\n\t.ndo_validate_addr = eth_validate_addr,\n\t.ndo_change_mtu = ice_change_mtu,\n\t.ndo_get_stats64 = ice_get_stats64,\n\t.ndo_tx_timeout = ice_tx_timeout,\n\t.ndo_bpf = ice_xdp_safe_mode,\n};\n\nstatic const struct net_device_ops ice_netdev_ops = {\n\t.ndo_open = ice_open,\n\t.ndo_stop = ice_stop,\n\t.ndo_start_xmit = ice_start_xmit,\n\t.ndo_select_queue = ice_select_queue,\n\t.ndo_features_check = ice_features_check,\n\t.ndo_fix_features = ice_fix_features,\n\t.ndo_set_rx_mode = ice_set_rx_mode,\n\t.ndo_set_mac_address = ice_set_mac_address,\n\t.ndo_validate_addr = eth_validate_addr,\n\t.ndo_change_mtu = ice_change_mtu,\n\t.ndo_get_stats64 = ice_get_stats64,\n\t.ndo_set_tx_maxrate = ice_set_tx_maxrate,\n\t.ndo_eth_ioctl = ice_eth_ioctl,\n\t.ndo_set_vf_spoofchk = ice_set_vf_spoofchk,\n\t.ndo_set_vf_mac = ice_set_vf_mac,\n\t.ndo_get_vf_config = ice_get_vf_cfg,\n\t.ndo_set_vf_trust = ice_set_vf_trust,\n\t.ndo_set_vf_vlan = ice_set_vf_port_vlan,\n\t.ndo_set_vf_link_state = ice_set_vf_link_state,\n\t.ndo_get_vf_stats = ice_get_vf_stats,\n\t.ndo_set_vf_rate = ice_set_vf_bw,\n\t.ndo_vlan_rx_add_vid = ice_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid = ice_vlan_rx_kill_vid,\n\t.ndo_setup_tc = ice_setup_tc,\n\t.ndo_set_features = ice_set_features,\n\t.ndo_bridge_getlink = ice_bridge_getlink,\n\t.ndo_bridge_setlink = ice_bridge_setlink,\n\t.ndo_fdb_add = ice_fdb_add,\n\t.ndo_fdb_del = ice_fdb_del,\n#ifdef CONFIG_RFS_ACCEL\n\t.ndo_rx_flow_steer = ice_rx_flow_steer,\n#endif\n\t.ndo_tx_timeout = ice_tx_timeout,\n\t.ndo_bpf = ice_xdp,\n\t.ndo_xdp_xmit = ice_xdp_xmit,\n\t.ndo_xsk_wakeup = ice_xsk_wakeup,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}