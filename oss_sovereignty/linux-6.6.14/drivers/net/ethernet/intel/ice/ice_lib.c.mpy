{
  "module_name": "ice_lib.c",
  "hash_id": "483d8058eae5e08aaec596444b07ddf9cc5ae62e87614a7a0fdf1b8af40660e2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ice/ice_lib.c",
  "human_readable_source": "\n \n\n#include \"ice.h\"\n#include \"ice_base.h\"\n#include \"ice_flow.h\"\n#include \"ice_lib.h\"\n#include \"ice_fltr.h\"\n#include \"ice_dcb_lib.h\"\n#include \"ice_devlink.h\"\n#include \"ice_vsi_vlan_ops.h\"\n\n \nconst char *ice_vsi_type_str(enum ice_vsi_type vsi_type)\n{\n\tswitch (vsi_type) {\n\tcase ICE_VSI_PF:\n\t\treturn \"ICE_VSI_PF\";\n\tcase ICE_VSI_VF:\n\t\treturn \"ICE_VSI_VF\";\n\tcase ICE_VSI_CTRL:\n\t\treturn \"ICE_VSI_CTRL\";\n\tcase ICE_VSI_CHNL:\n\t\treturn \"ICE_VSI_CHNL\";\n\tcase ICE_VSI_LB:\n\t\treturn \"ICE_VSI_LB\";\n\tcase ICE_VSI_SWITCHDEV_CTRL:\n\t\treturn \"ICE_VSI_SWITCHDEV_CTRL\";\n\tdefault:\n\t\treturn \"unknown\";\n\t}\n}\n\n \nstatic int ice_vsi_ctrl_all_rx_rings(struct ice_vsi *vsi, bool ena)\n{\n\tint ret = 0;\n\tu16 i;\n\n\tice_for_each_rxq(vsi, i)\n\t\tice_vsi_ctrl_one_rx_ring(vsi, ena, i, false);\n\n\tice_flush(&vsi->back->hw);\n\n\tice_for_each_rxq(vsi, i) {\n\t\tret = ice_vsi_wait_one_rx_ring(vsi, ena, i);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int ice_vsi_alloc_arrays(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\n\tdev = ice_pf_to_dev(pf);\n\tif (vsi->type == ICE_VSI_CHNL)\n\t\treturn 0;\n\n\t \n\tvsi->tx_rings = devm_kcalloc(dev, vsi->alloc_txq,\n\t\t\t\t     sizeof(*vsi->tx_rings), GFP_KERNEL);\n\tif (!vsi->tx_rings)\n\t\treturn -ENOMEM;\n\n\tvsi->rx_rings = devm_kcalloc(dev, vsi->alloc_rxq,\n\t\t\t\t     sizeof(*vsi->rx_rings), GFP_KERNEL);\n\tif (!vsi->rx_rings)\n\t\tgoto err_rings;\n\n\t \n\tvsi->txq_map = devm_kcalloc(dev, (vsi->alloc_txq + num_possible_cpus()),\n\t\t\t\t    sizeof(*vsi->txq_map), GFP_KERNEL);\n\n\tif (!vsi->txq_map)\n\t\tgoto err_txq_map;\n\n\tvsi->rxq_map = devm_kcalloc(dev, vsi->alloc_rxq,\n\t\t\t\t    sizeof(*vsi->rxq_map), GFP_KERNEL);\n\tif (!vsi->rxq_map)\n\t\tgoto err_rxq_map;\n\n\t \n\tif (vsi->type == ICE_VSI_LB)\n\t\treturn 0;\n\n\t \n\tvsi->q_vectors = devm_kcalloc(dev, vsi->num_q_vectors,\n\t\t\t\t      sizeof(*vsi->q_vectors), GFP_KERNEL);\n\tif (!vsi->q_vectors)\n\t\tgoto err_vectors;\n\n\tvsi->af_xdp_zc_qps = bitmap_zalloc(max_t(int, vsi->alloc_txq, vsi->alloc_rxq), GFP_KERNEL);\n\tif (!vsi->af_xdp_zc_qps)\n\t\tgoto err_zc_qps;\n\n\treturn 0;\n\nerr_zc_qps:\n\tdevm_kfree(dev, vsi->q_vectors);\nerr_vectors:\n\tdevm_kfree(dev, vsi->rxq_map);\nerr_rxq_map:\n\tdevm_kfree(dev, vsi->txq_map);\nerr_txq_map:\n\tdevm_kfree(dev, vsi->rx_rings);\nerr_rings:\n\tdevm_kfree(dev, vsi->tx_rings);\n\treturn -ENOMEM;\n}\n\n \nstatic void ice_vsi_set_num_desc(struct ice_vsi *vsi)\n{\n\tswitch (vsi->type) {\n\tcase ICE_VSI_PF:\n\tcase ICE_VSI_SWITCHDEV_CTRL:\n\tcase ICE_VSI_CTRL:\n\tcase ICE_VSI_LB:\n\t\t \n\t\tif (!vsi->num_rx_desc)\n\t\t\tvsi->num_rx_desc = ICE_DFLT_NUM_RX_DESC;\n\t\tif (!vsi->num_tx_desc)\n\t\t\tvsi->num_tx_desc = ICE_DFLT_NUM_TX_DESC;\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(ice_pf_to_dev(vsi->back), \"Not setting number of Tx/Rx descriptors for VSI type %d\\n\",\n\t\t\tvsi->type);\n\t\tbreak;\n\t}\n}\n\n \nstatic void ice_vsi_set_num_qs(struct ice_vsi *vsi)\n{\n\tenum ice_vsi_type vsi_type = vsi->type;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_vf *vf = vsi->vf;\n\n\tif (WARN_ON(vsi_type == ICE_VSI_VF && !vf))\n\t\treturn;\n\n\tswitch (vsi_type) {\n\tcase ICE_VSI_PF:\n\t\tif (vsi->req_txq) {\n\t\t\tvsi->alloc_txq = vsi->req_txq;\n\t\t\tvsi->num_txq = vsi->req_txq;\n\t\t} else {\n\t\t\tvsi->alloc_txq = min3(pf->num_lan_msix,\n\t\t\t\t\t      ice_get_avail_txq_count(pf),\n\t\t\t\t\t      (u16)num_online_cpus());\n\t\t}\n\n\t\tpf->num_lan_tx = vsi->alloc_txq;\n\n\t\t \n\t\tif (!test_bit(ICE_FLAG_RSS_ENA, pf->flags)) {\n\t\t\tvsi->alloc_rxq = 1;\n\t\t} else {\n\t\t\tif (vsi->req_rxq) {\n\t\t\t\tvsi->alloc_rxq = vsi->req_rxq;\n\t\t\t\tvsi->num_rxq = vsi->req_rxq;\n\t\t\t} else {\n\t\t\t\tvsi->alloc_rxq = min3(pf->num_lan_msix,\n\t\t\t\t\t\t      ice_get_avail_rxq_count(pf),\n\t\t\t\t\t\t      (u16)num_online_cpus());\n\t\t\t}\n\t\t}\n\n\t\tpf->num_lan_rx = vsi->alloc_rxq;\n\n\t\tvsi->num_q_vectors = min_t(int, pf->num_lan_msix,\n\t\t\t\t\t   max_t(int, vsi->alloc_rxq,\n\t\t\t\t\t\t vsi->alloc_txq));\n\t\tbreak;\n\tcase ICE_VSI_SWITCHDEV_CTRL:\n\t\t \n\t\tvsi->alloc_txq = ice_get_num_vfs(pf);\n\t\tvsi->alloc_rxq = vsi->alloc_txq;\n\t\tvsi->num_q_vectors = 1;\n\t\tbreak;\n\tcase ICE_VSI_VF:\n\t\tif (vf->num_req_qs)\n\t\t\tvf->num_vf_qs = vf->num_req_qs;\n\t\tvsi->alloc_txq = vf->num_vf_qs;\n\t\tvsi->alloc_rxq = vf->num_vf_qs;\n\t\t \n\t\tvsi->num_q_vectors = pf->vfs.num_msix_per - ICE_NONQ_VECS_VF;\n\t\tbreak;\n\tcase ICE_VSI_CTRL:\n\t\tvsi->alloc_txq = 1;\n\t\tvsi->alloc_rxq = 1;\n\t\tvsi->num_q_vectors = 1;\n\t\tbreak;\n\tcase ICE_VSI_CHNL:\n\t\tvsi->alloc_txq = 0;\n\t\tvsi->alloc_rxq = 0;\n\t\tbreak;\n\tcase ICE_VSI_LB:\n\t\tvsi->alloc_txq = 1;\n\t\tvsi->alloc_rxq = 1;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(ice_pf_to_dev(pf), \"Unknown VSI type %d\\n\", vsi_type);\n\t\tbreak;\n\t}\n\n\tice_vsi_set_num_desc(vsi);\n}\n\n \nstatic int ice_get_free_slot(void *array, int size, int curr)\n{\n\tint **tmp_array = (int **)array;\n\tint next;\n\n\tif (curr < (size - 1) && !tmp_array[curr + 1]) {\n\t\tnext = curr + 1;\n\t} else {\n\t\tint i = 0;\n\n\t\twhile ((i < size) && (tmp_array[i]))\n\t\t\ti++;\n\t\tif (i == size)\n\t\t\tnext = ICE_NO_VSI;\n\t\telse\n\t\t\tnext = i;\n\t}\n\treturn next;\n}\n\n \nstatic void ice_vsi_delete_from_hw(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_vsi_ctx *ctxt;\n\tint status;\n\n\tice_fltr_remove_all(vsi);\n\tctxt = kzalloc(sizeof(*ctxt), GFP_KERNEL);\n\tif (!ctxt)\n\t\treturn;\n\n\tif (vsi->type == ICE_VSI_VF)\n\t\tctxt->vf_num = vsi->vf->vf_id;\n\tctxt->vsi_num = vsi->vsi_num;\n\n\tmemcpy(&ctxt->info, &vsi->info, sizeof(ctxt->info));\n\n\tstatus = ice_free_vsi(&pf->hw, vsi->idx, ctxt, false, NULL);\n\tif (status)\n\t\tdev_err(ice_pf_to_dev(pf), \"Failed to delete VSI %i in FW - error: %d\\n\",\n\t\t\tvsi->vsi_num, status);\n\n\tkfree(ctxt);\n}\n\n \nstatic void ice_vsi_free_arrays(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\n\tdev = ice_pf_to_dev(pf);\n\n\tbitmap_free(vsi->af_xdp_zc_qps);\n\tvsi->af_xdp_zc_qps = NULL;\n\t \n\tdevm_kfree(dev, vsi->q_vectors);\n\tvsi->q_vectors = NULL;\n\tdevm_kfree(dev, vsi->tx_rings);\n\tvsi->tx_rings = NULL;\n\tdevm_kfree(dev, vsi->rx_rings);\n\tvsi->rx_rings = NULL;\n\tdevm_kfree(dev, vsi->txq_map);\n\tvsi->txq_map = NULL;\n\tdevm_kfree(dev, vsi->rxq_map);\n\tvsi->rxq_map = NULL;\n}\n\n \nstatic void ice_vsi_free_stats(struct ice_vsi *vsi)\n{\n\tstruct ice_vsi_stats *vsi_stat;\n\tstruct ice_pf *pf = vsi->back;\n\tint i;\n\n\tif (vsi->type == ICE_VSI_CHNL)\n\t\treturn;\n\tif (!pf->vsi_stats)\n\t\treturn;\n\n\tvsi_stat = pf->vsi_stats[vsi->idx];\n\tif (!vsi_stat)\n\t\treturn;\n\n\tice_for_each_alloc_txq(vsi, i) {\n\t\tif (vsi_stat->tx_ring_stats[i]) {\n\t\t\tkfree_rcu(vsi_stat->tx_ring_stats[i], rcu);\n\t\t\tWRITE_ONCE(vsi_stat->tx_ring_stats[i], NULL);\n\t\t}\n\t}\n\n\tice_for_each_alloc_rxq(vsi, i) {\n\t\tif (vsi_stat->rx_ring_stats[i]) {\n\t\t\tkfree_rcu(vsi_stat->rx_ring_stats[i], rcu);\n\t\t\tWRITE_ONCE(vsi_stat->rx_ring_stats[i], NULL);\n\t\t}\n\t}\n\n\tkfree(vsi_stat->tx_ring_stats);\n\tkfree(vsi_stat->rx_ring_stats);\n\tkfree(vsi_stat);\n\tpf->vsi_stats[vsi->idx] = NULL;\n}\n\n \nstatic int ice_vsi_alloc_ring_stats(struct ice_vsi *vsi)\n{\n\tstruct ice_ring_stats **tx_ring_stats;\n\tstruct ice_ring_stats **rx_ring_stats;\n\tstruct ice_vsi_stats *vsi_stats;\n\tstruct ice_pf *pf = vsi->back;\n\tu16 i;\n\n\tvsi_stats = pf->vsi_stats[vsi->idx];\n\ttx_ring_stats = vsi_stats->tx_ring_stats;\n\trx_ring_stats = vsi_stats->rx_ring_stats;\n\n\t \n\tice_for_each_alloc_txq(vsi, i) {\n\t\tstruct ice_ring_stats *ring_stats;\n\t\tstruct ice_tx_ring *ring;\n\n\t\tring = vsi->tx_rings[i];\n\t\tring_stats = tx_ring_stats[i];\n\n\t\tif (!ring_stats) {\n\t\t\tring_stats = kzalloc(sizeof(*ring_stats), GFP_KERNEL);\n\t\t\tif (!ring_stats)\n\t\t\t\tgoto err_out;\n\n\t\t\tWRITE_ONCE(tx_ring_stats[i], ring_stats);\n\t\t}\n\n\t\tring->ring_stats = ring_stats;\n\t}\n\n\t \n\tice_for_each_alloc_rxq(vsi, i) {\n\t\tstruct ice_ring_stats *ring_stats;\n\t\tstruct ice_rx_ring *ring;\n\n\t\tring = vsi->rx_rings[i];\n\t\tring_stats = rx_ring_stats[i];\n\n\t\tif (!ring_stats) {\n\t\t\tring_stats = kzalloc(sizeof(*ring_stats), GFP_KERNEL);\n\t\t\tif (!ring_stats)\n\t\t\t\tgoto err_out;\n\n\t\t\tWRITE_ONCE(rx_ring_stats[i], ring_stats);\n\t\t}\n\n\t\tring->ring_stats = ring_stats;\n\t}\n\n\treturn 0;\n\nerr_out:\n\tice_vsi_free_stats(vsi);\n\treturn -ENOMEM;\n}\n\n \nstatic void ice_vsi_free(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = NULL;\n\tstruct device *dev;\n\n\tif (!vsi || !vsi->back)\n\t\treturn;\n\n\tpf = vsi->back;\n\tdev = ice_pf_to_dev(pf);\n\n\tif (!pf->vsi[vsi->idx] || pf->vsi[vsi->idx] != vsi) {\n\t\tdev_dbg(dev, \"vsi does not exist at pf->vsi[%d]\\n\", vsi->idx);\n\t\treturn;\n\t}\n\n\tmutex_lock(&pf->sw_mutex);\n\t \n\n\tpf->vsi[vsi->idx] = NULL;\n\tpf->next_vsi = vsi->idx;\n\n\tice_vsi_free_stats(vsi);\n\tice_vsi_free_arrays(vsi);\n\tmutex_unlock(&pf->sw_mutex);\n\tdevm_kfree(dev, vsi);\n}\n\nvoid ice_vsi_delete(struct ice_vsi *vsi)\n{\n\tice_vsi_delete_from_hw(vsi);\n\tice_vsi_free(vsi);\n}\n\n \nstatic irqreturn_t ice_msix_clean_ctrl_vsi(int __always_unused irq, void *data)\n{\n\tstruct ice_q_vector *q_vector = (struct ice_q_vector *)data;\n\n\tif (!q_vector->tx.tx_ring)\n\t\treturn IRQ_HANDLED;\n\n#define FDIR_RX_DESC_CLEAN_BUDGET 64\n\tice_clean_rx_irq(q_vector->rx.rx_ring, FDIR_RX_DESC_CLEAN_BUDGET);\n\tice_clean_ctrl_tx_irq(q_vector->tx.tx_ring);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t ice_msix_clean_rings(int __always_unused irq, void *data)\n{\n\tstruct ice_q_vector *q_vector = (struct ice_q_vector *)data;\n\n\tif (!q_vector->tx.tx_ring && !q_vector->rx.rx_ring)\n\t\treturn IRQ_HANDLED;\n\n\tq_vector->total_events++;\n\n\tnapi_schedule(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t ice_eswitch_msix_clean_rings(int __always_unused irq, void *data)\n{\n\tstruct ice_q_vector *q_vector = (struct ice_q_vector *)data;\n\tstruct ice_pf *pf = q_vector->vsi->back;\n\tstruct ice_vf *vf;\n\tunsigned int bkt;\n\n\tif (!q_vector->tx.tx_ring && !q_vector->rx.rx_ring)\n\t\treturn IRQ_HANDLED;\n\n\trcu_read_lock();\n\tice_for_each_vf_rcu(pf, bkt, vf)\n\t\tnapi_schedule(&vf->repr->q_vector->napi);\n\trcu_read_unlock();\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int ice_vsi_alloc_stat_arrays(struct ice_vsi *vsi)\n{\n\tstruct ice_vsi_stats *vsi_stat;\n\tstruct ice_pf *pf = vsi->back;\n\n\tif (vsi->type == ICE_VSI_CHNL)\n\t\treturn 0;\n\tif (!pf->vsi_stats)\n\t\treturn -ENOENT;\n\n\tif (pf->vsi_stats[vsi->idx])\n\t \n\t\treturn 0;\n\n\tvsi_stat = kzalloc(sizeof(*vsi_stat), GFP_KERNEL);\n\tif (!vsi_stat)\n\t\treturn -ENOMEM;\n\n\tvsi_stat->tx_ring_stats =\n\t\tkcalloc(vsi->alloc_txq, sizeof(*vsi_stat->tx_ring_stats),\n\t\t\tGFP_KERNEL);\n\tif (!vsi_stat->tx_ring_stats)\n\t\tgoto err_alloc_tx;\n\n\tvsi_stat->rx_ring_stats =\n\t\tkcalloc(vsi->alloc_rxq, sizeof(*vsi_stat->rx_ring_stats),\n\t\t\tGFP_KERNEL);\n\tif (!vsi_stat->rx_ring_stats)\n\t\tgoto err_alloc_rx;\n\n\tpf->vsi_stats[vsi->idx] = vsi_stat;\n\n\treturn 0;\n\nerr_alloc_rx:\n\tkfree(vsi_stat->rx_ring_stats);\nerr_alloc_tx:\n\tkfree(vsi_stat->tx_ring_stats);\n\tkfree(vsi_stat);\n\tpf->vsi_stats[vsi->idx] = NULL;\n\treturn -ENOMEM;\n}\n\n \nstatic int\nice_vsi_alloc_def(struct ice_vsi *vsi, struct ice_channel *ch)\n{\n\tif (vsi->type != ICE_VSI_CHNL) {\n\t\tice_vsi_set_num_qs(vsi);\n\t\tif (ice_vsi_alloc_arrays(vsi))\n\t\t\treturn -ENOMEM;\n\t}\n\n\tswitch (vsi->type) {\n\tcase ICE_VSI_SWITCHDEV_CTRL:\n\t\t \n\t\tvsi->irq_handler = ice_eswitch_msix_clean_rings;\n\t\tbreak;\n\tcase ICE_VSI_PF:\n\t\t \n\t\tvsi->irq_handler = ice_msix_clean_rings;\n\t\tbreak;\n\tcase ICE_VSI_CTRL:\n\t\t \n\t\tvsi->irq_handler = ice_msix_clean_ctrl_vsi;\n\t\tbreak;\n\tcase ICE_VSI_CHNL:\n\t\tif (!ch)\n\t\t\treturn -EINVAL;\n\n\t\tvsi->num_rxq = ch->num_rxq;\n\t\tvsi->num_txq = ch->num_txq;\n\t\tvsi->next_base_q = ch->base_q;\n\t\tbreak;\n\tcase ICE_VSI_VF:\n\tcase ICE_VSI_LB:\n\t\tbreak;\n\tdefault:\n\t\tice_vsi_free_arrays(vsi);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct ice_vsi *ice_vsi_alloc(struct ice_pf *pf)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_vsi *vsi = NULL;\n\n\t \n\tmutex_lock(&pf->sw_mutex);\n\n\t \n\tif (pf->next_vsi == ICE_NO_VSI) {\n\t\tdev_dbg(dev, \"out of VSI slots!\\n\");\n\t\tgoto unlock_pf;\n\t}\n\n\tvsi = devm_kzalloc(dev, sizeof(*vsi), GFP_KERNEL);\n\tif (!vsi)\n\t\tgoto unlock_pf;\n\n\tvsi->back = pf;\n\tset_bit(ICE_VSI_DOWN, vsi->state);\n\n\t \n\tvsi->idx = pf->next_vsi;\n\tpf->vsi[pf->next_vsi] = vsi;\n\n\t \n\tpf->next_vsi = ice_get_free_slot(pf->vsi, pf->num_alloc_vsi,\n\t\t\t\t\t pf->next_vsi);\n\nunlock_pf:\n\tmutex_unlock(&pf->sw_mutex);\n\treturn vsi;\n}\n\n \nstatic int ice_alloc_fd_res(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tu32 g_val, b_val;\n\n\t \n\tif (!test_bit(ICE_FLAG_FD_ENA, pf->flags))\n\t\treturn -EPERM;\n\n\tif (!(vsi->type == ICE_VSI_PF || vsi->type == ICE_VSI_VF ||\n\t      vsi->type == ICE_VSI_CHNL))\n\t\treturn -EPERM;\n\n\t \n\tg_val = pf->hw.func_caps.fd_fltr_guar;\n\tif (!g_val)\n\t\treturn -EPERM;\n\n\t \n\tb_val = pf->hw.func_caps.fd_fltr_best_effort;\n\tif (!b_val)\n\t\treturn -EPERM;\n\n\t \n#define ICE_PF_VSI_GFLTR\t64\n\n\t \n\tif (vsi->type == ICE_VSI_PF) {\n\t\tvsi->num_gfltr = g_val;\n\t\t \n\t\tif (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags)) {\n\t\t\tif (g_val < ICE_PF_VSI_GFLTR)\n\t\t\t\treturn -EPERM;\n\t\t\t \n\t\t\tvsi->num_gfltr = ICE_PF_VSI_GFLTR;\n\t\t}\n\n\t\t \n\t\tvsi->num_bfltr = b_val;\n\t} else if (vsi->type == ICE_VSI_VF) {\n\t\tvsi->num_gfltr = 0;\n\n\t\t \n\t\tvsi->num_bfltr = b_val;\n\t} else {\n\t\tstruct ice_vsi *main_vsi;\n\t\tint numtc;\n\n\t\tmain_vsi = ice_get_main_vsi(pf);\n\t\tif (!main_vsi)\n\t\t\treturn -EPERM;\n\n\t\tif (!main_vsi->all_numtc)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tnumtc = main_vsi->all_numtc - ICE_CHNL_START_TC;\n\n\t\t \n\t\tif (numtc < ICE_CHNL_START_TC)\n\t\t\treturn -EPERM;\n\n\t\tg_val -= ICE_PF_VSI_GFLTR;\n\t\t \n\t\tvsi->num_gfltr = g_val / numtc;\n\n\t\t \n\t\tvsi->num_bfltr = b_val;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int ice_vsi_get_qs(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_qs_cfg tx_qs_cfg = {\n\t\t.qs_mutex = &pf->avail_q_mutex,\n\t\t.pf_map = pf->avail_txqs,\n\t\t.pf_map_size = pf->max_pf_txqs,\n\t\t.q_count = vsi->alloc_txq,\n\t\t.scatter_count = ICE_MAX_SCATTER_TXQS,\n\t\t.vsi_map = vsi->txq_map,\n\t\t.vsi_map_offset = 0,\n\t\t.mapping_mode = ICE_VSI_MAP_CONTIG\n\t};\n\tstruct ice_qs_cfg rx_qs_cfg = {\n\t\t.qs_mutex = &pf->avail_q_mutex,\n\t\t.pf_map = pf->avail_rxqs,\n\t\t.pf_map_size = pf->max_pf_rxqs,\n\t\t.q_count = vsi->alloc_rxq,\n\t\t.scatter_count = ICE_MAX_SCATTER_RXQS,\n\t\t.vsi_map = vsi->rxq_map,\n\t\t.vsi_map_offset = 0,\n\t\t.mapping_mode = ICE_VSI_MAP_CONTIG\n\t};\n\tint ret;\n\n\tif (vsi->type == ICE_VSI_CHNL)\n\t\treturn 0;\n\n\tret = __ice_vsi_get_qs(&tx_qs_cfg);\n\tif (ret)\n\t\treturn ret;\n\tvsi->tx_mapping_mode = tx_qs_cfg.mapping_mode;\n\n\tret = __ice_vsi_get_qs(&rx_qs_cfg);\n\tif (ret)\n\t\treturn ret;\n\tvsi->rx_mapping_mode = rx_qs_cfg.mapping_mode;\n\n\treturn 0;\n}\n\n \nstatic void ice_vsi_put_qs(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tint i;\n\n\tmutex_lock(&pf->avail_q_mutex);\n\n\tice_for_each_alloc_txq(vsi, i) {\n\t\tclear_bit(vsi->txq_map[i], pf->avail_txqs);\n\t\tvsi->txq_map[i] = ICE_INVAL_Q_INDEX;\n\t}\n\n\tice_for_each_alloc_rxq(vsi, i) {\n\t\tclear_bit(vsi->rxq_map[i], pf->avail_rxqs);\n\t\tvsi->rxq_map[i] = ICE_INVAL_Q_INDEX;\n\t}\n\n\tmutex_unlock(&pf->avail_q_mutex);\n}\n\n \nbool ice_is_safe_mode(struct ice_pf *pf)\n{\n\treturn !test_bit(ICE_FLAG_ADV_FEATURES, pf->flags);\n}\n\n \nbool ice_is_rdma_ena(struct ice_pf *pf)\n{\n\treturn test_bit(ICE_FLAG_RDMA_ENA, pf->flags);\n}\n\n \nstatic void ice_vsi_clean_rss_flow_fld(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tint status;\n\n\tif (ice_is_safe_mode(pf))\n\t\treturn;\n\n\tstatus = ice_rem_vsi_rss_cfg(&pf->hw, vsi->idx);\n\tif (status)\n\t\tdev_dbg(ice_pf_to_dev(pf), \"ice_rem_vsi_rss_cfg failed for vsi = %d, error = %d\\n\",\n\t\t\tvsi->vsi_num, status);\n}\n\n \nstatic void ice_rss_clean(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\n\tdev = ice_pf_to_dev(pf);\n\n\tdevm_kfree(dev, vsi->rss_hkey_user);\n\tdevm_kfree(dev, vsi->rss_lut_user);\n\n\tice_vsi_clean_rss_flow_fld(vsi);\n\t \n\tif (!ice_is_safe_mode(pf))\n\t\tice_rem_vsi_rss_list(&pf->hw, vsi->idx);\n}\n\n \nstatic void ice_vsi_set_rss_params(struct ice_vsi *vsi)\n{\n\tstruct ice_hw_common_caps *cap;\n\tstruct ice_pf *pf = vsi->back;\n\tu16 max_rss_size;\n\n\tif (!test_bit(ICE_FLAG_RSS_ENA, pf->flags)) {\n\t\tvsi->rss_size = 1;\n\t\treturn;\n\t}\n\n\tcap = &pf->hw.func_caps.common_cap;\n\tmax_rss_size = BIT(cap->rss_table_entry_width);\n\tswitch (vsi->type) {\n\tcase ICE_VSI_CHNL:\n\tcase ICE_VSI_PF:\n\t\t \n\t\tvsi->rss_table_size = (u16)cap->rss_table_size;\n\t\tif (vsi->type == ICE_VSI_CHNL)\n\t\t\tvsi->rss_size = min_t(u16, vsi->num_rxq, max_rss_size);\n\t\telse\n\t\t\tvsi->rss_size = min_t(u16, num_online_cpus(),\n\t\t\t\t\t      max_rss_size);\n\t\tvsi->rss_lut_type = ICE_LUT_PF;\n\t\tbreak;\n\tcase ICE_VSI_SWITCHDEV_CTRL:\n\t\tvsi->rss_table_size = ICE_LUT_VSI_SIZE;\n\t\tvsi->rss_size = min_t(u16, num_online_cpus(), max_rss_size);\n\t\tvsi->rss_lut_type = ICE_LUT_VSI;\n\t\tbreak;\n\tcase ICE_VSI_VF:\n\t\t \n\t\tvsi->rss_table_size = ICE_LUT_VSI_SIZE;\n\t\tvsi->rss_size = ICE_MAX_RSS_QS_PER_VF;\n\t\tvsi->rss_lut_type = ICE_LUT_VSI;\n\t\tbreak;\n\tcase ICE_VSI_LB:\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(ice_pf_to_dev(pf), \"Unsupported VSI type %s\\n\",\n\t\t\tice_vsi_type_str(vsi->type));\n\t\tbreak;\n\t}\n}\n\n \nstatic void ice_set_dflt_vsi_ctx(struct ice_hw *hw, struct ice_vsi_ctx *ctxt)\n{\n\tu32 table = 0;\n\n\tmemset(&ctxt->info, 0, sizeof(ctxt->info));\n\t \n\tctxt->alloc_from_pool = true;\n\t \n\tctxt->info.sw_flags = ICE_AQ_VSI_SW_FLAG_SRC_PRUNE;\n\t \n\tctxt->info.sw_flags2 = ICE_AQ_VSI_SW_FLAG_LAN_ENA;\n\t \n\tctxt->info.inner_vlan_flags = ((ICE_AQ_VSI_INNER_VLAN_TX_MODE_ALL &\n\t\t\t\t  ICE_AQ_VSI_INNER_VLAN_TX_MODE_M) >>\n\t\t\t\t ICE_AQ_VSI_INNER_VLAN_TX_MODE_S);\n\t \n\tif (ice_is_dvm_ena(hw)) {\n\t\tctxt->info.inner_vlan_flags |=\n\t\t\tICE_AQ_VSI_INNER_VLAN_EMODE_NOTHING;\n\t\tctxt->info.outer_vlan_flags =\n\t\t\t(ICE_AQ_VSI_OUTER_VLAN_TX_MODE_ALL <<\n\t\t\t ICE_AQ_VSI_OUTER_VLAN_TX_MODE_S) &\n\t\t\tICE_AQ_VSI_OUTER_VLAN_TX_MODE_M;\n\t\tctxt->info.outer_vlan_flags |=\n\t\t\t(ICE_AQ_VSI_OUTER_TAG_VLAN_8100 <<\n\t\t\t ICE_AQ_VSI_OUTER_TAG_TYPE_S) &\n\t\t\tICE_AQ_VSI_OUTER_TAG_TYPE_M;\n\t\tctxt->info.outer_vlan_flags |=\n\t\t\tFIELD_PREP(ICE_AQ_VSI_OUTER_VLAN_EMODE_M,\n\t\t\t\t   ICE_AQ_VSI_OUTER_VLAN_EMODE_NOTHING);\n\t}\n\t \n\ttable |= ICE_UP_TABLE_TRANSLATE(0, 0);\n\ttable |= ICE_UP_TABLE_TRANSLATE(1, 1);\n\ttable |= ICE_UP_TABLE_TRANSLATE(2, 2);\n\ttable |= ICE_UP_TABLE_TRANSLATE(3, 3);\n\ttable |= ICE_UP_TABLE_TRANSLATE(4, 4);\n\ttable |= ICE_UP_TABLE_TRANSLATE(5, 5);\n\ttable |= ICE_UP_TABLE_TRANSLATE(6, 6);\n\ttable |= ICE_UP_TABLE_TRANSLATE(7, 7);\n\tctxt->info.ingress_table = cpu_to_le32(table);\n\tctxt->info.egress_table = cpu_to_le32(table);\n\t \n\tctxt->info.outer_up_table = cpu_to_le32(table);\n\t \n}\n\n \nstatic int ice_vsi_setup_q_map(struct ice_vsi *vsi, struct ice_vsi_ctx *ctxt)\n{\n\tu16 offset = 0, qmap = 0, tx_count = 0, rx_count = 0, pow = 0;\n\tu16 num_txq_per_tc, num_rxq_per_tc;\n\tu16 qcount_tx = vsi->alloc_txq;\n\tu16 qcount_rx = vsi->alloc_rxq;\n\tu8 netdev_tc = 0;\n\tint i;\n\n\tif (!vsi->tc_cfg.numtc) {\n\t\t \n\t\tvsi->tc_cfg.numtc = 1;\n\t\tvsi->tc_cfg.ena_tc = 1;\n\t}\n\n\tnum_rxq_per_tc = min_t(u16, qcount_rx / vsi->tc_cfg.numtc, ICE_MAX_RXQS_PER_TC);\n\tif (!num_rxq_per_tc)\n\t\tnum_rxq_per_tc = 1;\n\tnum_txq_per_tc = qcount_tx / vsi->tc_cfg.numtc;\n\tif (!num_txq_per_tc)\n\t\tnum_txq_per_tc = 1;\n\n\t \n\tpow = (u16)order_base_2(num_rxq_per_tc);\n\n\t \n\tice_for_each_traffic_class(i) {\n\t\tif (!(vsi->tc_cfg.ena_tc & BIT(i))) {\n\t\t\t \n\t\t\tvsi->tc_cfg.tc_info[i].qoffset = 0;\n\t\t\tvsi->tc_cfg.tc_info[i].qcount_rx = 1;\n\t\t\tvsi->tc_cfg.tc_info[i].qcount_tx = 1;\n\t\t\tvsi->tc_cfg.tc_info[i].netdev_tc = 0;\n\t\t\tctxt->info.tc_mapping[i] = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tvsi->tc_cfg.tc_info[i].qoffset = offset;\n\t\tvsi->tc_cfg.tc_info[i].qcount_rx = num_rxq_per_tc;\n\t\tvsi->tc_cfg.tc_info[i].qcount_tx = num_txq_per_tc;\n\t\tvsi->tc_cfg.tc_info[i].netdev_tc = netdev_tc++;\n\n\t\tqmap = ((offset << ICE_AQ_VSI_TC_Q_OFFSET_S) &\n\t\t\tICE_AQ_VSI_TC_Q_OFFSET_M) |\n\t\t\t((pow << ICE_AQ_VSI_TC_Q_NUM_S) &\n\t\t\t ICE_AQ_VSI_TC_Q_NUM_M);\n\t\toffset += num_rxq_per_tc;\n\t\ttx_count += num_txq_per_tc;\n\t\tctxt->info.tc_mapping[i] = cpu_to_le16(qmap);\n\t}\n\n\t \n\tif (offset)\n\t\trx_count = offset;\n\telse\n\t\trx_count = num_rxq_per_tc;\n\n\tif (rx_count > vsi->alloc_rxq) {\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Trying to use more Rx queues (%u), than were allocated (%u)!\\n\",\n\t\t\trx_count, vsi->alloc_rxq);\n\t\treturn -EINVAL;\n\t}\n\n\tif (tx_count > vsi->alloc_txq) {\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Trying to use more Tx queues (%u), than were allocated (%u)!\\n\",\n\t\t\ttx_count, vsi->alloc_txq);\n\t\treturn -EINVAL;\n\t}\n\n\tvsi->num_txq = tx_count;\n\tvsi->num_rxq = rx_count;\n\n\tif (vsi->type == ICE_VSI_VF && vsi->num_txq != vsi->num_rxq) {\n\t\tdev_dbg(ice_pf_to_dev(vsi->back), \"VF VSI should have same number of Tx and Rx queues. Hence making them equal\\n\");\n\t\t \n\t\tvsi->num_txq = vsi->num_rxq;\n\t}\n\n\t \n\tctxt->info.mapping_flags |= cpu_to_le16(ICE_AQ_VSI_Q_MAP_CONTIG);\n\t \n\tctxt->info.q_mapping[0] = cpu_to_le16(vsi->rxq_map[0]);\n\tctxt->info.q_mapping[1] = cpu_to_le16(vsi->num_rxq);\n\n\treturn 0;\n}\n\n \nstatic void ice_set_fd_vsi_ctx(struct ice_vsi_ctx *ctxt, struct ice_vsi *vsi)\n{\n\tu8 dflt_q_group, dflt_q_prio;\n\tu16 dflt_q, report_q, val;\n\n\tif (vsi->type != ICE_VSI_PF && vsi->type != ICE_VSI_CTRL &&\n\t    vsi->type != ICE_VSI_VF && vsi->type != ICE_VSI_CHNL)\n\t\treturn;\n\n\tval = ICE_AQ_VSI_PROP_FLOW_DIR_VALID;\n\tctxt->info.valid_sections |= cpu_to_le16(val);\n\tdflt_q = 0;\n\tdflt_q_group = 0;\n\treport_q = 0;\n\tdflt_q_prio = 0;\n\n\t \n\tval = ICE_AQ_VSI_FD_ENABLE | ICE_AQ_VSI_FD_PROG_ENABLE;\n\tctxt->info.fd_options = cpu_to_le16(val);\n\t \n\tctxt->info.max_fd_fltr_dedicated =\n\t\t\tcpu_to_le16(vsi->num_gfltr);\n\t \n\tctxt->info.max_fd_fltr_shared =\n\t\t\tcpu_to_le16(vsi->num_bfltr);\n\t \n\tval = ((dflt_q << ICE_AQ_VSI_FD_DEF_Q_S) &\n\t       ICE_AQ_VSI_FD_DEF_Q_M);\n\t \n\tval |= ((dflt_q_group << ICE_AQ_VSI_FD_DEF_GRP_S) &\n\t\tICE_AQ_VSI_FD_DEF_GRP_M);\n\tctxt->info.fd_def_q = cpu_to_le16(val);\n\t \n\tval = ((report_q << ICE_AQ_VSI_FD_REPORT_Q_S) &\n\t       ICE_AQ_VSI_FD_REPORT_Q_M);\n\t \n\tval |= ((dflt_q_prio << ICE_AQ_VSI_FD_DEF_PRIORITY_S) &\n\t\tICE_AQ_VSI_FD_DEF_PRIORITY_M);\n\tctxt->info.fd_report_opt = cpu_to_le16(val);\n}\n\n \nstatic void ice_set_rss_vsi_ctx(struct ice_vsi_ctx *ctxt, struct ice_vsi *vsi)\n{\n\tu8 lut_type, hash_type;\n\tstruct device *dev;\n\tstruct ice_pf *pf;\n\n\tpf = vsi->back;\n\tdev = ice_pf_to_dev(pf);\n\n\tswitch (vsi->type) {\n\tcase ICE_VSI_CHNL:\n\tcase ICE_VSI_PF:\n\t\t \n\t\tlut_type = ICE_AQ_VSI_Q_OPT_RSS_LUT_PF;\n\t\thash_type = ICE_AQ_VSI_Q_OPT_RSS_TPLZ;\n\t\tbreak;\n\tcase ICE_VSI_VF:\n\t\t \n\t\tlut_type = ICE_AQ_VSI_Q_OPT_RSS_LUT_VSI;\n\t\thash_type = ICE_AQ_VSI_Q_OPT_RSS_TPLZ;\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(dev, \"Unsupported VSI type %s\\n\",\n\t\t\tice_vsi_type_str(vsi->type));\n\t\treturn;\n\t}\n\n\tctxt->info.q_opt_rss = ((lut_type << ICE_AQ_VSI_Q_OPT_RSS_LUT_S) &\n\t\t\t\tICE_AQ_VSI_Q_OPT_RSS_LUT_M) |\n\t\t\t\t(hash_type & ICE_AQ_VSI_Q_OPT_RSS_HASH_M);\n}\n\nstatic void\nice_chnl_vsi_setup_q_map(struct ice_vsi *vsi, struct ice_vsi_ctx *ctxt)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tu16 qcount, qmap;\n\tu8 offset = 0;\n\tint pow;\n\n\tqcount = min_t(int, vsi->num_rxq, pf->num_lan_msix);\n\n\tpow = order_base_2(qcount);\n\tqmap = ((offset << ICE_AQ_VSI_TC_Q_OFFSET_S) &\n\t\t ICE_AQ_VSI_TC_Q_OFFSET_M) |\n\t\t ((pow << ICE_AQ_VSI_TC_Q_NUM_S) &\n\t\t   ICE_AQ_VSI_TC_Q_NUM_M);\n\n\tctxt->info.tc_mapping[0] = cpu_to_le16(qmap);\n\tctxt->info.mapping_flags |= cpu_to_le16(ICE_AQ_VSI_Q_MAP_CONTIG);\n\tctxt->info.q_mapping[0] = cpu_to_le16(vsi->next_base_q);\n\tctxt->info.q_mapping[1] = cpu_to_le16(qcount);\n}\n\n \nstatic bool ice_vsi_is_vlan_pruning_ena(struct ice_vsi *vsi)\n{\n\treturn vsi->info.sw_flags2 & ICE_AQ_VSI_SW_FLAG_RX_VLAN_PRUNE_ENA;\n}\n\n \nstatic int ice_vsi_init(struct ice_vsi *vsi, u32 vsi_flags)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct ice_vsi_ctx *ctxt;\n\tstruct device *dev;\n\tint ret = 0;\n\n\tdev = ice_pf_to_dev(pf);\n\tctxt = kzalloc(sizeof(*ctxt), GFP_KERNEL);\n\tif (!ctxt)\n\t\treturn -ENOMEM;\n\n\tswitch (vsi->type) {\n\tcase ICE_VSI_CTRL:\n\tcase ICE_VSI_LB:\n\tcase ICE_VSI_PF:\n\t\tctxt->flags = ICE_AQ_VSI_TYPE_PF;\n\t\tbreak;\n\tcase ICE_VSI_SWITCHDEV_CTRL:\n\tcase ICE_VSI_CHNL:\n\t\tctxt->flags = ICE_AQ_VSI_TYPE_VMDQ2;\n\t\tbreak;\n\tcase ICE_VSI_VF:\n\t\tctxt->flags = ICE_AQ_VSI_TYPE_VF;\n\t\t \n\t\tctxt->vf_num = vsi->vf->vf_id + hw->func_caps.vf_base_id;\n\t\tbreak;\n\tdefault:\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\t \n\tif (vsi->type == ICE_VSI_CHNL) {\n\t\tstruct ice_vsi *main_vsi;\n\n\t\tmain_vsi = ice_get_main_vsi(pf);\n\t\tif (main_vsi && ice_vsi_is_vlan_pruning_ena(main_vsi))\n\t\t\tctxt->info.sw_flags2 |=\n\t\t\t\tICE_AQ_VSI_SW_FLAG_RX_VLAN_PRUNE_ENA;\n\t\telse\n\t\t\tctxt->info.sw_flags2 &=\n\t\t\t\t~ICE_AQ_VSI_SW_FLAG_RX_VLAN_PRUNE_ENA;\n\t}\n\n\tice_set_dflt_vsi_ctx(hw, ctxt);\n\tif (test_bit(ICE_FLAG_FD_ENA, pf->flags))\n\t\tice_set_fd_vsi_ctx(ctxt, vsi);\n\t \n\tif (vsi->vsw->bridge_mode == BRIDGE_MODE_VEB)\n\t\tctxt->info.sw_flags |= ICE_AQ_VSI_SW_FLAG_ALLOW_LB;\n\n\t \n\tif (test_bit(ICE_FLAG_RSS_ENA, pf->flags) &&\n\t    vsi->type != ICE_VSI_CTRL) {\n\t\tice_set_rss_vsi_ctx(ctxt, vsi);\n\t\t \n\t\tif (!(vsi_flags & ICE_VSI_FLAG_INIT))\n\t\t\tctxt->info.valid_sections |=\n\t\t\t\tcpu_to_le16(ICE_AQ_VSI_PROP_Q_OPT_VALID);\n\t}\n\n\tctxt->info.sw_id = vsi->port_info->sw_id;\n\tif (vsi->type == ICE_VSI_CHNL) {\n\t\tice_chnl_vsi_setup_q_map(vsi, ctxt);\n\t} else {\n\t\tret = ice_vsi_setup_q_map(vsi, ctxt);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (!(vsi_flags & ICE_VSI_FLAG_INIT))\n\t\t\t \n\t\t\t \n\t\t\tctxt->info.valid_sections |=\n\t\t\t\tcpu_to_le16(ICE_AQ_VSI_PROP_RXQ_MAP_VALID);\n\t}\n\n\t \n\tif (vsi->type == ICE_VSI_PF) {\n\t\tctxt->info.sec_flags |= ICE_AQ_VSI_SEC_FLAG_ALLOW_DEST_OVRD;\n\t\tctxt->info.valid_sections |=\n\t\t\tcpu_to_le16(ICE_AQ_VSI_PROP_SECURITY_VALID);\n\t}\n\n\tif (vsi_flags & ICE_VSI_FLAG_INIT) {\n\t\tret = ice_add_vsi(hw, vsi->idx, ctxt, NULL);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Add VSI failed, err %d\\n\", ret);\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tret = ice_update_vsi(hw, vsi->idx, ctxt, NULL);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"Update VSI failed, err %d\\n\", ret);\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tvsi->info = ctxt->info;\n\n\t \n\tvsi->vsi_num = ctxt->vsi_num;\n\nout:\n\tkfree(ctxt);\n\treturn ret;\n}\n\n \nstatic void ice_vsi_clear_rings(struct ice_vsi *vsi)\n{\n\tint i;\n\n\t \n\tif (vsi->q_vectors) {\n\t\tice_for_each_q_vector(vsi, i) {\n\t\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[i];\n\n\t\t\tif (q_vector) {\n\t\t\t\tq_vector->tx.tx_ring = NULL;\n\t\t\t\tq_vector->rx.rx_ring = NULL;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (vsi->tx_rings) {\n\t\tice_for_each_alloc_txq(vsi, i) {\n\t\t\tif (vsi->tx_rings[i]) {\n\t\t\t\tkfree_rcu(vsi->tx_rings[i], rcu);\n\t\t\t\tWRITE_ONCE(vsi->tx_rings[i], NULL);\n\t\t\t}\n\t\t}\n\t}\n\tif (vsi->rx_rings) {\n\t\tice_for_each_alloc_rxq(vsi, i) {\n\t\t\tif (vsi->rx_rings[i]) {\n\t\t\t\tkfree_rcu(vsi->rx_rings[i], rcu);\n\t\t\t\tWRITE_ONCE(vsi->rx_rings[i], NULL);\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nstatic int ice_vsi_alloc_rings(struct ice_vsi *vsi)\n{\n\tbool dvm_ena = ice_is_dvm_ena(&vsi->back->hw);\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\tu16 i;\n\n\tdev = ice_pf_to_dev(pf);\n\t \n\tice_for_each_alloc_txq(vsi, i) {\n\t\tstruct ice_tx_ring *ring;\n\n\t\t \n\t\tring = kzalloc(sizeof(*ring), GFP_KERNEL);\n\n\t\tif (!ring)\n\t\t\tgoto err_out;\n\n\t\tring->q_index = i;\n\t\tring->reg_idx = vsi->txq_map[i];\n\t\tring->vsi = vsi;\n\t\tring->tx_tstamps = &pf->ptp.port.tx;\n\t\tring->dev = dev;\n\t\tring->count = vsi->num_tx_desc;\n\t\tring->txq_teid = ICE_INVAL_TEID;\n\t\tif (dvm_ena)\n\t\t\tring->flags |= ICE_TX_FLAGS_RING_VLAN_L2TAG2;\n\t\telse\n\t\t\tring->flags |= ICE_TX_FLAGS_RING_VLAN_L2TAG1;\n\t\tWRITE_ONCE(vsi->tx_rings[i], ring);\n\t}\n\n\t \n\tice_for_each_alloc_rxq(vsi, i) {\n\t\tstruct ice_rx_ring *ring;\n\n\t\t \n\t\tring = kzalloc(sizeof(*ring), GFP_KERNEL);\n\t\tif (!ring)\n\t\t\tgoto err_out;\n\n\t\tring->q_index = i;\n\t\tring->reg_idx = vsi->rxq_map[i];\n\t\tring->vsi = vsi;\n\t\tring->netdev = vsi->netdev;\n\t\tring->dev = dev;\n\t\tring->count = vsi->num_rx_desc;\n\t\tring->cached_phctime = pf->ptp.cached_phc_time;\n\t\tWRITE_ONCE(vsi->rx_rings[i], ring);\n\t}\n\n\treturn 0;\n\nerr_out:\n\tice_vsi_clear_rings(vsi);\n\treturn -ENOMEM;\n}\n\n \nvoid ice_vsi_manage_rss_lut(struct ice_vsi *vsi, bool ena)\n{\n\tu8 *lut;\n\n\tlut = kzalloc(vsi->rss_table_size, GFP_KERNEL);\n\tif (!lut)\n\t\treturn;\n\n\tif (ena) {\n\t\tif (vsi->rss_lut_user)\n\t\t\tmemcpy(lut, vsi->rss_lut_user, vsi->rss_table_size);\n\t\telse\n\t\t\tice_fill_rss_lut(lut, vsi->rss_table_size,\n\t\t\t\t\t vsi->rss_size);\n\t}\n\n\tice_set_rss_lut(vsi, lut, vsi->rss_table_size);\n\tkfree(lut);\n}\n\n \nvoid ice_vsi_cfg_crc_strip(struct ice_vsi *vsi, bool disable)\n{\n\tint i;\n\n\tice_for_each_rxq(vsi, i)\n\t\tif (disable)\n\t\t\tvsi->rx_rings[i]->flags |= ICE_RX_FLAGS_CRC_STRIP_DIS;\n\t\telse\n\t\t\tvsi->rx_rings[i]->flags &= ~ICE_RX_FLAGS_CRC_STRIP_DIS;\n}\n\n \nint ice_vsi_cfg_rss_lut_key(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\tu8 *lut, *key;\n\tint err;\n\n\tdev = ice_pf_to_dev(pf);\n\tif (vsi->type == ICE_VSI_PF && vsi->ch_rss_size &&\n\t    (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))) {\n\t\tvsi->rss_size = min_t(u16, vsi->rss_size, vsi->ch_rss_size);\n\t} else {\n\t\tvsi->rss_size = min_t(u16, vsi->rss_size, vsi->num_rxq);\n\n\t\t \n\t\tif (vsi->orig_rss_size && vsi->rss_size < vsi->orig_rss_size &&\n\t\t    vsi->orig_rss_size <= vsi->num_rxq) {\n\t\t\tvsi->rss_size = vsi->orig_rss_size;\n\t\t\t \n\t\t\tvsi->orig_rss_size = 0;\n\t\t}\n\t}\n\n\tlut = kzalloc(vsi->rss_table_size, GFP_KERNEL);\n\tif (!lut)\n\t\treturn -ENOMEM;\n\n\tif (vsi->rss_lut_user)\n\t\tmemcpy(lut, vsi->rss_lut_user, vsi->rss_table_size);\n\telse\n\t\tice_fill_rss_lut(lut, vsi->rss_table_size, vsi->rss_size);\n\n\terr = ice_set_rss_lut(vsi, lut, vsi->rss_table_size);\n\tif (err) {\n\t\tdev_err(dev, \"set_rss_lut failed, error %d\\n\", err);\n\t\tgoto ice_vsi_cfg_rss_exit;\n\t}\n\n\tkey = kzalloc(ICE_GET_SET_RSS_KEY_EXTEND_KEY_SIZE, GFP_KERNEL);\n\tif (!key) {\n\t\terr = -ENOMEM;\n\t\tgoto ice_vsi_cfg_rss_exit;\n\t}\n\n\tif (vsi->rss_hkey_user)\n\t\tmemcpy(key, vsi->rss_hkey_user, ICE_GET_SET_RSS_KEY_EXTEND_KEY_SIZE);\n\telse\n\t\tnetdev_rss_key_fill((void *)key, ICE_GET_SET_RSS_KEY_EXTEND_KEY_SIZE);\n\n\terr = ice_set_rss_key(vsi, key);\n\tif (err)\n\t\tdev_err(dev, \"set_rss_key failed, error %d\\n\", err);\n\n\tkfree(key);\nice_vsi_cfg_rss_exit:\n\tkfree(lut);\n\treturn err;\n}\n\n \nstatic void ice_vsi_set_vf_rss_flow_fld(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\tint status;\n\n\tdev = ice_pf_to_dev(pf);\n\tif (ice_is_safe_mode(pf)) {\n\t\tdev_dbg(dev, \"Advanced RSS disabled. Package download failed, vsi num = %d\\n\",\n\t\t\tvsi->vsi_num);\n\t\treturn;\n\t}\n\n\tstatus = ice_add_avf_rss_cfg(&pf->hw, vsi->idx, ICE_DEFAULT_RSS_HENA);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_avf_rss_cfg failed for vsi = %d, error = %d\\n\",\n\t\t\tvsi->vsi_num, status);\n}\n\n \nstatic void ice_vsi_set_rss_flow_fld(struct ice_vsi *vsi)\n{\n\tu16 vsi_handle = vsi->idx, vsi_num = vsi->vsi_num;\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tstruct device *dev;\n\tint status;\n\n\tdev = ice_pf_to_dev(pf);\n\tif (ice_is_safe_mode(pf)) {\n\t\tdev_dbg(dev, \"Advanced RSS disabled. Package download failed, vsi num = %d\\n\",\n\t\t\tvsi_num);\n\t\treturn;\n\t}\n\t \n\tstatus = ice_add_rss_cfg(hw, vsi_handle, ICE_FLOW_HASH_IPV4,\n\t\t\t\t ICE_FLOW_SEG_HDR_IPV4);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_rss_cfg failed for ipv4 flow, vsi = %d, error = %d\\n\",\n\t\t\tvsi_num, status);\n\n\t \n\tstatus = ice_add_rss_cfg(hw, vsi_handle, ICE_FLOW_HASH_IPV6,\n\t\t\t\t ICE_FLOW_SEG_HDR_IPV6);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_rss_cfg failed for ipv6 flow, vsi = %d, error = %d\\n\",\n\t\t\tvsi_num, status);\n\n\t \n\tstatus = ice_add_rss_cfg(hw, vsi_handle, ICE_HASH_TCP_IPV4,\n\t\t\t\t ICE_FLOW_SEG_HDR_TCP | ICE_FLOW_SEG_HDR_IPV4);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_rss_cfg failed for tcp4 flow, vsi = %d, error = %d\\n\",\n\t\t\tvsi_num, status);\n\n\t \n\tstatus = ice_add_rss_cfg(hw, vsi_handle, ICE_HASH_UDP_IPV4,\n\t\t\t\t ICE_FLOW_SEG_HDR_UDP | ICE_FLOW_SEG_HDR_IPV4);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_rss_cfg failed for udp4 flow, vsi = %d, error = %d\\n\",\n\t\t\tvsi_num, status);\n\n\t \n\tstatus = ice_add_rss_cfg(hw, vsi_handle, ICE_FLOW_HASH_IPV4,\n\t\t\t\t ICE_FLOW_SEG_HDR_SCTP | ICE_FLOW_SEG_HDR_IPV4);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_rss_cfg failed for sctp4 flow, vsi = %d, error = %d\\n\",\n\t\t\tvsi_num, status);\n\n\t \n\tstatus = ice_add_rss_cfg(hw, vsi_handle, ICE_HASH_TCP_IPV6,\n\t\t\t\t ICE_FLOW_SEG_HDR_TCP | ICE_FLOW_SEG_HDR_IPV6);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_rss_cfg failed for tcp6 flow, vsi = %d, error = %d\\n\",\n\t\t\tvsi_num, status);\n\n\t \n\tstatus = ice_add_rss_cfg(hw, vsi_handle, ICE_HASH_UDP_IPV6,\n\t\t\t\t ICE_FLOW_SEG_HDR_UDP | ICE_FLOW_SEG_HDR_IPV6);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_rss_cfg failed for udp6 flow, vsi = %d, error = %d\\n\",\n\t\t\tvsi_num, status);\n\n\t \n\tstatus = ice_add_rss_cfg(hw, vsi_handle, ICE_FLOW_HASH_IPV6,\n\t\t\t\t ICE_FLOW_SEG_HDR_SCTP | ICE_FLOW_SEG_HDR_IPV6);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_rss_cfg failed for sctp6 flow, vsi = %d, error = %d\\n\",\n\t\t\tvsi_num, status);\n\n\tstatus = ice_add_rss_cfg(hw, vsi_handle, ICE_FLOW_HASH_ESP_SPI,\n\t\t\t\t ICE_FLOW_SEG_HDR_ESP);\n\tif (status)\n\t\tdev_dbg(dev, \"ice_add_rss_cfg failed for esp/spi flow, vsi = %d, error = %d\\n\",\n\t\t\tvsi_num, status);\n}\n\n \nstatic void ice_vsi_cfg_frame_size(struct ice_vsi *vsi)\n{\n\tif (!vsi->netdev || test_bit(ICE_FLAG_LEGACY_RX, vsi->back->flags)) {\n\t\tvsi->max_frame = ICE_MAX_FRAME_LEGACY_RX;\n\t\tvsi->rx_buf_len = ICE_RXBUF_1664;\n#if (PAGE_SIZE < 8192)\n\t} else if (!ICE_2K_TOO_SMALL_WITH_PADDING &&\n\t\t   (vsi->netdev->mtu <= ETH_DATA_LEN)) {\n\t\tvsi->max_frame = ICE_RXBUF_1536 - NET_IP_ALIGN;\n\t\tvsi->rx_buf_len = ICE_RXBUF_1536 - NET_IP_ALIGN;\n#endif\n\t} else {\n\t\tvsi->max_frame = ICE_AQ_SET_MAC_FRAME_SIZE_MAX;\n\t\tvsi->rx_buf_len = ICE_RXBUF_3072;\n\t}\n}\n\n \nbool ice_pf_state_is_nominal(struct ice_pf *pf)\n{\n\tDECLARE_BITMAP(check_bits, ICE_STATE_NBITS) = { 0 };\n\n\tif (!pf)\n\t\treturn false;\n\n\tbitmap_set(check_bits, 0, ICE_STATE_NOMINAL_CHECK_BITS);\n\tif (bitmap_intersects(pf->state, check_bits, ICE_STATE_NBITS))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nvoid ice_update_eth_stats(struct ice_vsi *vsi)\n{\n\tstruct ice_eth_stats *prev_es, *cur_es;\n\tstruct ice_hw *hw = &vsi->back->hw;\n\tstruct ice_pf *pf = vsi->back;\n\tu16 vsi_num = vsi->vsi_num;     \n\n\tprev_es = &vsi->eth_stats_prev;\n\tcur_es = &vsi->eth_stats;\n\n\tif (ice_is_reset_in_progress(pf->state))\n\t\tvsi->stat_offsets_loaded = false;\n\n\tice_stat_update40(hw, GLV_GORCL(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->rx_bytes, &cur_es->rx_bytes);\n\n\tice_stat_update40(hw, GLV_UPRCL(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->rx_unicast, &cur_es->rx_unicast);\n\n\tice_stat_update40(hw, GLV_MPRCL(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->rx_multicast, &cur_es->rx_multicast);\n\n\tice_stat_update40(hw, GLV_BPRCL(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->rx_broadcast, &cur_es->rx_broadcast);\n\n\tice_stat_update32(hw, GLV_RDPC(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->rx_discards, &cur_es->rx_discards);\n\n\tice_stat_update40(hw, GLV_GOTCL(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->tx_bytes, &cur_es->tx_bytes);\n\n\tice_stat_update40(hw, GLV_UPTCL(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->tx_unicast, &cur_es->tx_unicast);\n\n\tice_stat_update40(hw, GLV_MPTCL(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->tx_multicast, &cur_es->tx_multicast);\n\n\tice_stat_update40(hw, GLV_BPTCL(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->tx_broadcast, &cur_es->tx_broadcast);\n\n\tice_stat_update32(hw, GLV_TEPC(vsi_num), vsi->stat_offsets_loaded,\n\t\t\t  &prev_es->tx_errors, &cur_es->tx_errors);\n\n\tvsi->stat_offsets_loaded = true;\n}\n\n \nvoid\nice_write_qrxflxp_cntxt(struct ice_hw *hw, u16 pf_q, u32 rxdid, u32 prio,\n\t\t\tbool ena_ts)\n{\n\tint regval = rd32(hw, QRXFLXP_CNTXT(pf_q));\n\n\t \n\tregval &= ~(QRXFLXP_CNTXT_RXDID_IDX_M |\n\t\t    QRXFLXP_CNTXT_RXDID_PRIO_M |\n\t\t    QRXFLXP_CNTXT_TS_M);\n\n\tregval |= (rxdid << QRXFLXP_CNTXT_RXDID_IDX_S) &\n\t\tQRXFLXP_CNTXT_RXDID_IDX_M;\n\n\tregval |= (prio << QRXFLXP_CNTXT_RXDID_PRIO_S) &\n\t\tQRXFLXP_CNTXT_RXDID_PRIO_M;\n\n\tif (ena_ts)\n\t\t \n\t\tregval |= QRXFLXP_CNTXT_TS_M;\n\n\twr32(hw, QRXFLXP_CNTXT(pf_q), regval);\n}\n\nint ice_vsi_cfg_single_rxq(struct ice_vsi *vsi, u16 q_idx)\n{\n\tif (q_idx >= vsi->num_rxq)\n\t\treturn -EINVAL;\n\n\treturn ice_vsi_cfg_rxq(vsi->rx_rings[q_idx]);\n}\n\nint ice_vsi_cfg_single_txq(struct ice_vsi *vsi, struct ice_tx_ring **tx_rings, u16 q_idx)\n{\n\tstruct ice_aqc_add_tx_qgrp *qg_buf;\n\tint err;\n\n\tif (q_idx >= vsi->alloc_txq || !tx_rings || !tx_rings[q_idx])\n\t\treturn -EINVAL;\n\n\tqg_buf = kzalloc(struct_size(qg_buf, txqs, 1), GFP_KERNEL);\n\tif (!qg_buf)\n\t\treturn -ENOMEM;\n\n\tqg_buf->num_txqs = 1;\n\n\terr = ice_vsi_cfg_txq(vsi, tx_rings[q_idx], qg_buf);\n\tkfree(qg_buf);\n\treturn err;\n}\n\n \nint ice_vsi_cfg_rxqs(struct ice_vsi *vsi)\n{\n\tu16 i;\n\n\tif (vsi->type == ICE_VSI_VF)\n\t\tgoto setup_rings;\n\n\tice_vsi_cfg_frame_size(vsi);\nsetup_rings:\n\t \n\tice_for_each_rxq(vsi, i) {\n\t\tint err = ice_vsi_cfg_rxq(vsi->rx_rings[i]);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\nice_vsi_cfg_txqs(struct ice_vsi *vsi, struct ice_tx_ring **rings, u16 count)\n{\n\tstruct ice_aqc_add_tx_qgrp *qg_buf;\n\tu16 q_idx = 0;\n\tint err = 0;\n\n\tqg_buf = kzalloc(struct_size(qg_buf, txqs, 1), GFP_KERNEL);\n\tif (!qg_buf)\n\t\treturn -ENOMEM;\n\n\tqg_buf->num_txqs = 1;\n\n\tfor (q_idx = 0; q_idx < count; q_idx++) {\n\t\terr = ice_vsi_cfg_txq(vsi, rings[q_idx], qg_buf);\n\t\tif (err)\n\t\t\tgoto err_cfg_txqs;\n\t}\n\nerr_cfg_txqs:\n\tkfree(qg_buf);\n\treturn err;\n}\n\n \nint ice_vsi_cfg_lan_txqs(struct ice_vsi *vsi)\n{\n\treturn ice_vsi_cfg_txqs(vsi, vsi->tx_rings, vsi->num_txq);\n}\n\n \nint ice_vsi_cfg_xdp_txqs(struct ice_vsi *vsi)\n{\n\tint ret;\n\tint i;\n\n\tret = ice_vsi_cfg_txqs(vsi, vsi->xdp_rings, vsi->num_xdp_txq);\n\tif (ret)\n\t\treturn ret;\n\n\tice_for_each_rxq(vsi, i)\n\t\tice_tx_xsk_pool(vsi, i);\n\n\treturn 0;\n}\n\n \nstatic u32 ice_intrl_usec_to_reg(u8 intrl, u8 gran)\n{\n\tu32 val = intrl / gran;\n\n\tif (val)\n\t\treturn val | GLINT_RATE_INTRL_ENA_M;\n\treturn 0;\n}\n\n \nvoid ice_write_intrl(struct ice_q_vector *q_vector, u8 intrl)\n{\n\tstruct ice_hw *hw = &q_vector->vsi->back->hw;\n\n\twr32(hw, GLINT_RATE(q_vector->reg_idx),\n\t     ice_intrl_usec_to_reg(intrl, ICE_INTRL_GRAN_ABOVE_25));\n}\n\nstatic struct ice_q_vector *ice_pull_qvec_from_rc(struct ice_ring_container *rc)\n{\n\tswitch (rc->type) {\n\tcase ICE_RX_CONTAINER:\n\t\tif (rc->rx_ring)\n\t\t\treturn rc->rx_ring->q_vector;\n\t\tbreak;\n\tcase ICE_TX_CONTAINER:\n\t\tif (rc->tx_ring)\n\t\t\treturn rc->tx_ring->q_vector;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void __ice_write_itr(struct ice_q_vector *q_vector,\n\t\t\t    struct ice_ring_container *rc, u16 itr)\n{\n\tstruct ice_hw *hw = &q_vector->vsi->back->hw;\n\n\twr32(hw, GLINT_ITR(rc->itr_idx, q_vector->reg_idx),\n\t     ITR_REG_ALIGN(itr) >> ICE_ITR_GRAN_S);\n}\n\n \nvoid ice_write_itr(struct ice_ring_container *rc, u16 itr)\n{\n\tstruct ice_q_vector *q_vector;\n\n\tq_vector = ice_pull_qvec_from_rc(rc);\n\tif (!q_vector)\n\t\treturn;\n\n\t__ice_write_itr(q_vector, rc, itr);\n}\n\n \nvoid ice_set_q_vector_intrl(struct ice_q_vector *q_vector)\n{\n\tif (ITR_IS_DYNAMIC(&q_vector->tx) || ITR_IS_DYNAMIC(&q_vector->rx)) {\n\t\t \n\t\tice_write_intrl(q_vector, 4);\n\t} else {\n\t\tice_write_intrl(q_vector, q_vector->intrl);\n\t}\n}\n\n \nvoid ice_vsi_cfg_msix(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tu16 txq = 0, rxq = 0;\n\tint i, q;\n\n\tice_for_each_q_vector(vsi, i) {\n\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[i];\n\t\tu16 reg_idx = q_vector->reg_idx;\n\n\t\tice_cfg_itr(hw, q_vector);\n\n\t\t \n\t\tfor (q = 0; q < q_vector->num_ring_tx; q++) {\n\t\t\tice_cfg_txq_interrupt(vsi, txq, reg_idx,\n\t\t\t\t\t      q_vector->tx.itr_idx);\n\t\t\ttxq++;\n\t\t}\n\n\t\tfor (q = 0; q < q_vector->num_ring_rx; q++) {\n\t\t\tice_cfg_rxq_interrupt(vsi, rxq, reg_idx,\n\t\t\t\t\t      q_vector->rx.itr_idx);\n\t\t\trxq++;\n\t\t}\n\t}\n}\n\n \nint ice_vsi_start_all_rx_rings(struct ice_vsi *vsi)\n{\n\treturn ice_vsi_ctrl_all_rx_rings(vsi, true);\n}\n\n \nint ice_vsi_stop_all_rx_rings(struct ice_vsi *vsi)\n{\n\treturn ice_vsi_ctrl_all_rx_rings(vsi, false);\n}\n\n \nstatic int\nice_vsi_stop_tx_rings(struct ice_vsi *vsi, enum ice_disq_rst_src rst_src,\n\t\t      u16 rel_vmvf_num, struct ice_tx_ring **rings, u16 count)\n{\n\tu16 q_idx;\n\n\tif (vsi->num_txq > ICE_LAN_TXQ_MAX_QDIS)\n\t\treturn -EINVAL;\n\n\tfor (q_idx = 0; q_idx < count; q_idx++) {\n\t\tstruct ice_txq_meta txq_meta = { };\n\t\tint status;\n\n\t\tif (!rings || !rings[q_idx])\n\t\t\treturn -EINVAL;\n\n\t\tice_fill_txq_meta(vsi, rings[q_idx], &txq_meta);\n\t\tstatus = ice_vsi_stop_tx_ring(vsi, rst_src, rel_vmvf_num,\n\t\t\t\t\t      rings[q_idx], &txq_meta);\n\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\n\treturn 0;\n}\n\n \nint\nice_vsi_stop_lan_tx_rings(struct ice_vsi *vsi, enum ice_disq_rst_src rst_src,\n\t\t\t  u16 rel_vmvf_num)\n{\n\treturn ice_vsi_stop_tx_rings(vsi, rst_src, rel_vmvf_num, vsi->tx_rings, vsi->num_txq);\n}\n\n \nint ice_vsi_stop_xdp_tx_rings(struct ice_vsi *vsi)\n{\n\treturn ice_vsi_stop_tx_rings(vsi, ICE_NO_RESET, 0, vsi->xdp_rings, vsi->num_xdp_txq);\n}\n\n \nbool ice_vsi_is_rx_queue_active(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tint i;\n\n\tice_for_each_rxq(vsi, i) {\n\t\tu32 rx_reg;\n\t\tint pf_q;\n\n\t\tpf_q = vsi->rxq_map[i];\n\t\trx_reg = rd32(hw, QRX_CTRL(pf_q));\n\t\tif (rx_reg & QRX_CTRL_QENA_STAT_M)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void ice_vsi_set_tc_cfg(struct ice_vsi *vsi)\n{\n\tif (!test_bit(ICE_FLAG_DCB_ENA, vsi->back->flags)) {\n\t\tvsi->tc_cfg.ena_tc = ICE_DFLT_TRAFFIC_CLASS;\n\t\tvsi->tc_cfg.numtc = 1;\n\t\treturn;\n\t}\n\n\t \n\tice_vsi_set_dcb_tc_cfg(vsi);\n}\n\n \nvoid ice_cfg_sw_lldp(struct ice_vsi *vsi, bool tx, bool create)\n{\n\tint (*eth_fltr)(struct ice_vsi *v, u16 type, u16 flag,\n\t\t\tenum ice_sw_fwd_act_type act);\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\tint status;\n\n\tdev = ice_pf_to_dev(pf);\n\teth_fltr = create ? ice_fltr_add_eth : ice_fltr_remove_eth;\n\n\tif (tx) {\n\t\tstatus = eth_fltr(vsi, ETH_P_LLDP, ICE_FLTR_TX,\n\t\t\t\t  ICE_DROP_PACKET);\n\t} else {\n\t\tif (ice_fw_supports_lldp_fltr_ctrl(&pf->hw)) {\n\t\t\tstatus = ice_lldp_fltr_add_remove(&pf->hw, vsi->vsi_num,\n\t\t\t\t\t\t\t  create);\n\t\t} else {\n\t\t\tstatus = eth_fltr(vsi, ETH_P_LLDP, ICE_FLTR_RX,\n\t\t\t\t\t  ICE_FWD_TO_VSI);\n\t\t}\n\t}\n\n\tif (status)\n\t\tdev_dbg(dev, \"Fail %s %s LLDP rule on VSI %i error: %d\\n\",\n\t\t\tcreate ? \"adding\" : \"removing\", tx ? \"TX\" : \"RX\",\n\t\t\tvsi->vsi_num, status);\n}\n\n \nstatic void ice_set_agg_vsi(struct ice_vsi *vsi)\n{\n\tstruct device *dev = ice_pf_to_dev(vsi->back);\n\tstruct ice_agg_node *agg_node_iter = NULL;\n\tu32 agg_id = ICE_INVALID_AGG_NODE_ID;\n\tstruct ice_agg_node *agg_node = NULL;\n\tint node_offset, max_agg_nodes = 0;\n\tstruct ice_port_info *port_info;\n\tstruct ice_pf *pf = vsi->back;\n\tu32 agg_node_id_start = 0;\n\tint status;\n\n\t \n\tport_info = pf->hw.port_info;\n\tif (!port_info)\n\t\treturn;\n\n\tswitch (vsi->type) {\n\tcase ICE_VSI_CTRL:\n\tcase ICE_VSI_CHNL:\n\tcase ICE_VSI_LB:\n\tcase ICE_VSI_PF:\n\tcase ICE_VSI_SWITCHDEV_CTRL:\n\t\tmax_agg_nodes = ICE_MAX_PF_AGG_NODES;\n\t\tagg_node_id_start = ICE_PF_AGG_NODE_ID_START;\n\t\tagg_node_iter = &pf->pf_agg_node[0];\n\t\tbreak;\n\tcase ICE_VSI_VF:\n\t\t \n\t\tmax_agg_nodes = ICE_MAX_VF_AGG_NODES;\n\t\tagg_node_id_start = ICE_VF_AGG_NODE_ID_START;\n\t\tagg_node_iter = &pf->vf_agg_node[0];\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tdev_dbg(dev, \"unexpected VSI type %s\\n\",\n\t\t\tice_vsi_type_str(vsi->type));\n\t\treturn;\n\t}\n\n\t \n\tfor (node_offset = 0; node_offset < max_agg_nodes; node_offset++) {\n\t\t \n\t\tif (agg_node_iter->num_vsis &&\n\t\t    agg_node_iter->num_vsis == ICE_MAX_VSIS_IN_AGG_NODE) {\n\t\t\tagg_node_iter++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (agg_node_iter->valid &&\n\t\t    agg_node_iter->agg_id != ICE_INVALID_AGG_NODE_ID) {\n\t\t\tagg_id = agg_node_iter->agg_id;\n\t\t\tagg_node = agg_node_iter;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (agg_node_iter->agg_id == ICE_INVALID_AGG_NODE_ID) {\n\t\t\tagg_id = node_offset + agg_node_id_start;\n\t\t\tagg_node = agg_node_iter;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tagg_node_iter++;\n\t}\n\n\tif (!agg_node)\n\t\treturn;\n\n\t \n\tif (!agg_node->valid) {\n\t\tstatus = ice_cfg_agg(port_info, agg_id, ICE_AGG_TYPE_AGG,\n\t\t\t\t     (u8)vsi->tc_cfg.ena_tc);\n\t\tif (status) {\n\t\t\tdev_err(dev, \"unable to create aggregator node with agg_id %u\\n\",\n\t\t\t\tagg_id);\n\t\t\treturn;\n\t\t}\n\t\t \n\t\tagg_node->valid = true;\n\t\tagg_node->agg_id = agg_id;\n\t}\n\n\t \n\tstatus = ice_move_vsi_to_agg(port_info, agg_id, vsi->idx,\n\t\t\t\t     (u8)vsi->tc_cfg.ena_tc);\n\tif (status) {\n\t\tdev_err(dev, \"unable to move VSI idx %u into aggregator %u node\",\n\t\t\tvsi->idx, agg_id);\n\t\treturn;\n\t}\n\n\t \n\tagg_node->num_vsis++;\n\n\t \n\tvsi->agg_node = agg_node;\n\tdev_dbg(dev, \"successfully moved VSI idx %u tc_bitmap 0x%x) into aggregator node %d which has num_vsis %u\\n\",\n\t\tvsi->idx, vsi->tc_cfg.ena_tc, vsi->agg_node->agg_id,\n\t\tvsi->agg_node->num_vsis);\n}\n\nstatic int ice_vsi_cfg_tc_lan(struct ice_pf *pf, struct ice_vsi *vsi)\n{\n\tu16 max_txqs[ICE_MAX_TRAFFIC_CLASS] = { 0 };\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tint ret, i;\n\n\t \n\tice_for_each_traffic_class(i) {\n\t\tif (!(vsi->tc_cfg.ena_tc & BIT(i)))\n\t\t\tcontinue;\n\n\t\tif (vsi->type == ICE_VSI_CHNL) {\n\t\t\tif (!vsi->alloc_txq && vsi->num_txq)\n\t\t\t\tmax_txqs[i] = vsi->num_txq;\n\t\t\telse\n\t\t\t\tmax_txqs[i] = pf->num_lan_tx;\n\t\t} else {\n\t\t\tmax_txqs[i] = vsi->alloc_txq;\n\t\t}\n\n\t\tif (vsi->type == ICE_VSI_PF)\n\t\t\tmax_txqs[i] += vsi->num_xdp_txq;\n\t}\n\n\tdev_dbg(dev, \"vsi->tc_cfg.ena_tc = %d\\n\", vsi->tc_cfg.ena_tc);\n\tret = ice_cfg_vsi_lan(vsi->port_info, vsi->idx, vsi->tc_cfg.ena_tc,\n\t\t\t      max_txqs);\n\tif (ret) {\n\t\tdev_err(dev, \"VSI %d failed lan queue config, error %d\\n\",\n\t\t\tvsi->vsi_num, ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\nice_vsi_cfg_def(struct ice_vsi *vsi, struct ice_vsi_cfg_params *params)\n{\n\tstruct device *dev = ice_pf_to_dev(vsi->back);\n\tstruct ice_pf *pf = vsi->back;\n\tint ret;\n\n\tvsi->vsw = pf->first_sw;\n\n\tret = ice_vsi_alloc_def(vsi, params->ch);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = ice_vsi_alloc_stat_arrays(vsi);\n\tif (ret)\n\t\tgoto unroll_vsi_alloc;\n\n\tice_alloc_fd_res(vsi);\n\n\tret = ice_vsi_get_qs(vsi);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to allocate queues. vsi->idx = %d\\n\",\n\t\t\tvsi->idx);\n\t\tgoto unroll_vsi_alloc_stat;\n\t}\n\n\t \n\tice_vsi_set_rss_params(vsi);\n\n\t \n\tice_vsi_set_tc_cfg(vsi);\n\n\t \n\tret = ice_vsi_init(vsi, params->flags);\n\tif (ret)\n\t\tgoto unroll_get_qs;\n\n\tice_vsi_init_vlan_ops(vsi);\n\n\tswitch (vsi->type) {\n\tcase ICE_VSI_CTRL:\n\tcase ICE_VSI_SWITCHDEV_CTRL:\n\tcase ICE_VSI_PF:\n\t\tret = ice_vsi_alloc_q_vectors(vsi);\n\t\tif (ret)\n\t\t\tgoto unroll_vsi_init;\n\n\t\tret = ice_vsi_alloc_rings(vsi);\n\t\tif (ret)\n\t\t\tgoto unroll_vector_base;\n\n\t\tret = ice_vsi_alloc_ring_stats(vsi);\n\t\tif (ret)\n\t\t\tgoto unroll_vector_base;\n\n\t\tice_vsi_map_rings_to_vectors(vsi);\n\t\tvsi->stat_offsets_loaded = false;\n\n\t\tif (ice_is_xdp_ena_vsi(vsi)) {\n\t\t\tret = ice_vsi_determine_xdp_res(vsi);\n\t\t\tif (ret)\n\t\t\t\tgoto unroll_vector_base;\n\t\t\tret = ice_prepare_xdp_rings(vsi, vsi->xdp_prog);\n\t\t\tif (ret)\n\t\t\t\tgoto unroll_vector_base;\n\t\t}\n\n\t\t \n\t\tif (vsi->type != ICE_VSI_CTRL)\n\t\t\t \n\t\t\tif (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) {\n\t\t\t\tice_vsi_cfg_rss_lut_key(vsi);\n\t\t\t\tice_vsi_set_rss_flow_fld(vsi);\n\t\t\t}\n\t\tice_init_arfs(vsi);\n\t\tbreak;\n\tcase ICE_VSI_CHNL:\n\t\tif (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) {\n\t\t\tice_vsi_cfg_rss_lut_key(vsi);\n\t\t\tice_vsi_set_rss_flow_fld(vsi);\n\t\t}\n\t\tbreak;\n\tcase ICE_VSI_VF:\n\t\t \n\t\tret = ice_vsi_alloc_q_vectors(vsi);\n\t\tif (ret)\n\t\t\tgoto unroll_vsi_init;\n\n\t\tret = ice_vsi_alloc_rings(vsi);\n\t\tif (ret)\n\t\t\tgoto unroll_alloc_q_vector;\n\n\t\tret = ice_vsi_alloc_ring_stats(vsi);\n\t\tif (ret)\n\t\t\tgoto unroll_vector_base;\n\n\t\tvsi->stat_offsets_loaded = false;\n\n\t\t \n\t\tif (test_bit(ICE_FLAG_RSS_ENA, pf->flags)) {\n\t\t\tice_vsi_cfg_rss_lut_key(vsi);\n\t\t\tice_vsi_set_vf_rss_flow_fld(vsi);\n\t\t}\n\t\tbreak;\n\tcase ICE_VSI_LB:\n\t\tret = ice_vsi_alloc_rings(vsi);\n\t\tif (ret)\n\t\t\tgoto unroll_vsi_init;\n\n\t\tret = ice_vsi_alloc_ring_stats(vsi);\n\t\tif (ret)\n\t\t\tgoto unroll_vector_base;\n\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto unroll_vsi_init;\n\t}\n\n\treturn 0;\n\nunroll_vector_base:\n\t \nunroll_alloc_q_vector:\n\tice_vsi_free_q_vectors(vsi);\nunroll_vsi_init:\n\tice_vsi_delete_from_hw(vsi);\nunroll_get_qs:\n\tice_vsi_put_qs(vsi);\nunroll_vsi_alloc_stat:\n\tice_vsi_free_stats(vsi);\nunroll_vsi_alloc:\n\tice_vsi_free_arrays(vsi);\n\treturn ret;\n}\n\n \nint ice_vsi_cfg(struct ice_vsi *vsi, struct ice_vsi_cfg_params *params)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tint ret;\n\n\tif (WARN_ON(params->type == ICE_VSI_VF && !params->vf))\n\t\treturn -EINVAL;\n\n\tvsi->type = params->type;\n\tvsi->port_info = params->pi;\n\n\t \n\tvsi->vf = params->vf;\n\n\tret = ice_vsi_cfg_def(vsi, params);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ice_vsi_cfg_tc_lan(vsi->back, vsi);\n\tif (ret)\n\t\tice_vsi_decfg(vsi);\n\n\tif (vsi->type == ICE_VSI_CTRL) {\n\t\tif (vsi->vf) {\n\t\t\tWARN_ON(vsi->vf->ctrl_vsi_idx != ICE_NO_VSI);\n\t\t\tvsi->vf->ctrl_vsi_idx = vsi->idx;\n\t\t} else {\n\t\t\tWARN_ON(pf->ctrl_vsi_idx != ICE_NO_VSI);\n\t\t\tpf->ctrl_vsi_idx = vsi->idx;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nvoid ice_vsi_decfg(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tint err;\n\n\t \n\tif (!ice_is_safe_mode(pf) && vsi->type == ICE_VSI_PF &&\n\t    !test_bit(ICE_FLAG_FW_LLDP_AGENT, pf->flags))\n\t\tice_cfg_sw_lldp(vsi, false, false);\n\n\tice_rm_vsi_lan_cfg(vsi->port_info, vsi->idx);\n\terr = ice_rm_vsi_rdma_cfg(vsi->port_info, vsi->idx);\n\tif (err)\n\t\tdev_err(ice_pf_to_dev(pf), \"Failed to remove RDMA scheduler config for VSI %u, err %d\\n\",\n\t\t\tvsi->vsi_num, err);\n\n\tif (ice_is_xdp_ena_vsi(vsi))\n\t\t \n\t\tice_destroy_xdp_rings(vsi);\n\n\tice_vsi_clear_rings(vsi);\n\tice_vsi_free_q_vectors(vsi);\n\tice_vsi_put_qs(vsi);\n\tice_vsi_free_arrays(vsi);\n\n\t \n\n\tif (vsi->type == ICE_VSI_VF &&\n\t    vsi->agg_node && vsi->agg_node->valid)\n\t\tvsi->agg_node->num_vsis--;\n}\n\n \nstruct ice_vsi *\nice_vsi_setup(struct ice_pf *pf, struct ice_vsi_cfg_params *params)\n{\n\tstruct device *dev = ice_pf_to_dev(pf);\n\tstruct ice_vsi *vsi;\n\tint ret;\n\n\t \n\tif (WARN_ON(!(params->flags & ICE_VSI_FLAG_INIT)) ||\n\t    WARN_ON(!params->pi))\n\t\treturn NULL;\n\n\tvsi = ice_vsi_alloc(pf);\n\tif (!vsi) {\n\t\tdev_err(dev, \"could not allocate VSI\\n\");\n\t\treturn NULL;\n\t}\n\n\tret = ice_vsi_cfg(vsi, params);\n\tif (ret)\n\t\tgoto err_vsi_cfg;\n\n\t \n\tif (!ice_is_safe_mode(pf) && vsi->type == ICE_VSI_PF) {\n\t\tice_fltr_add_eth(vsi, ETH_P_PAUSE, ICE_FLTR_TX,\n\t\t\t\t ICE_DROP_PACKET);\n\t\tice_cfg_sw_lldp(vsi, true, true);\n\t}\n\n\tif (!vsi->agg_node)\n\t\tice_set_agg_vsi(vsi);\n\n\treturn vsi;\n\nerr_vsi_cfg:\n\tice_vsi_free(vsi);\n\n\treturn NULL;\n}\n\n \nstatic void ice_vsi_release_msix(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tu32 txq = 0;\n\tu32 rxq = 0;\n\tint i, q;\n\n\tice_for_each_q_vector(vsi, i) {\n\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[i];\n\n\t\tice_write_intrl(q_vector, 0);\n\t\tfor (q = 0; q < q_vector->num_ring_tx; q++) {\n\t\t\tice_write_itr(&q_vector->tx, 0);\n\t\t\twr32(hw, QINT_TQCTL(vsi->txq_map[txq]), 0);\n\t\t\tif (ice_is_xdp_ena_vsi(vsi)) {\n\t\t\t\tu32 xdp_txq = txq + vsi->num_xdp_txq;\n\n\t\t\t\twr32(hw, QINT_TQCTL(vsi->txq_map[xdp_txq]), 0);\n\t\t\t}\n\t\t\ttxq++;\n\t\t}\n\n\t\tfor (q = 0; q < q_vector->num_ring_rx; q++) {\n\t\t\tice_write_itr(&q_vector->rx, 0);\n\t\t\twr32(hw, QINT_RQCTL(vsi->rxq_map[rxq]), 0);\n\t\t\trxq++;\n\t\t}\n\t}\n\n\tice_flush(hw);\n}\n\n \nvoid ice_vsi_free_irq(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tint i;\n\n\tif (!vsi->q_vectors || !vsi->irqs_ready)\n\t\treturn;\n\n\tice_vsi_release_msix(vsi);\n\tif (vsi->type == ICE_VSI_VF)\n\t\treturn;\n\n\tvsi->irqs_ready = false;\n\tice_free_cpu_rx_rmap(vsi);\n\n\tice_for_each_q_vector(vsi, i) {\n\t\tint irq_num;\n\n\t\tirq_num = vsi->q_vectors[i]->irq.virq;\n\n\t\t \n\t\tif (!vsi->q_vectors[i] ||\n\t\t    !(vsi->q_vectors[i]->num_ring_tx ||\n\t\t      vsi->q_vectors[i]->num_ring_rx))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!IS_ENABLED(CONFIG_RFS_ACCEL))\n\t\t\tirq_set_affinity_notifier(irq_num, NULL);\n\n\t\t \n\t\tirq_set_affinity_hint(irq_num, NULL);\n\t\tsynchronize_irq(irq_num);\n\t\tdevm_free_irq(ice_pf_to_dev(pf), irq_num, vsi->q_vectors[i]);\n\t}\n}\n\n \nvoid ice_vsi_free_tx_rings(struct ice_vsi *vsi)\n{\n\tint i;\n\n\tif (!vsi->tx_rings)\n\t\treturn;\n\n\tice_for_each_txq(vsi, i)\n\t\tif (vsi->tx_rings[i] && vsi->tx_rings[i]->desc)\n\t\t\tice_free_tx_ring(vsi->tx_rings[i]);\n}\n\n \nvoid ice_vsi_free_rx_rings(struct ice_vsi *vsi)\n{\n\tint i;\n\n\tif (!vsi->rx_rings)\n\t\treturn;\n\n\tice_for_each_rxq(vsi, i)\n\t\tif (vsi->rx_rings[i] && vsi->rx_rings[i]->desc)\n\t\t\tice_free_rx_ring(vsi->rx_rings[i]);\n}\n\n \nvoid ice_vsi_close(struct ice_vsi *vsi)\n{\n\tif (!test_and_set_bit(ICE_VSI_DOWN, vsi->state))\n\t\tice_down(vsi);\n\n\tice_vsi_free_irq(vsi);\n\tice_vsi_free_tx_rings(vsi);\n\tice_vsi_free_rx_rings(vsi);\n}\n\n \nint ice_ena_vsi(struct ice_vsi *vsi, bool locked)\n{\n\tint err = 0;\n\n\tif (!test_bit(ICE_VSI_NEEDS_RESTART, vsi->state))\n\t\treturn 0;\n\n\tclear_bit(ICE_VSI_NEEDS_RESTART, vsi->state);\n\n\tif (vsi->netdev && vsi->type == ICE_VSI_PF) {\n\t\tif (netif_running(vsi->netdev)) {\n\t\t\tif (!locked)\n\t\t\t\trtnl_lock();\n\n\t\t\terr = ice_open_internal(vsi->netdev);\n\n\t\t\tif (!locked)\n\t\t\t\trtnl_unlock();\n\t\t}\n\t} else if (vsi->type == ICE_VSI_CTRL) {\n\t\terr = ice_vsi_open_ctrl(vsi);\n\t}\n\n\treturn err;\n}\n\n \nvoid ice_dis_vsi(struct ice_vsi *vsi, bool locked)\n{\n\tif (test_bit(ICE_VSI_DOWN, vsi->state))\n\t\treturn;\n\n\tset_bit(ICE_VSI_NEEDS_RESTART, vsi->state);\n\n\tif (vsi->type == ICE_VSI_PF && vsi->netdev) {\n\t\tif (netif_running(vsi->netdev)) {\n\t\t\tif (!locked)\n\t\t\t\trtnl_lock();\n\n\t\t\tice_vsi_close(vsi);\n\n\t\t\tif (!locked)\n\t\t\t\trtnl_unlock();\n\t\t} else {\n\t\t\tice_vsi_close(vsi);\n\t\t}\n\t} else if (vsi->type == ICE_VSI_CTRL ||\n\t\t   vsi->type == ICE_VSI_SWITCHDEV_CTRL) {\n\t\tice_vsi_close(vsi);\n\t}\n}\n\n \nvoid ice_vsi_dis_irq(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_hw *hw = &pf->hw;\n\tu32 val;\n\tint i;\n\n\t \n\tif (vsi->tx_rings) {\n\t\tice_for_each_txq(vsi, i) {\n\t\t\tif (vsi->tx_rings[i]) {\n\t\t\t\tu16 reg;\n\n\t\t\t\treg = vsi->tx_rings[i]->reg_idx;\n\t\t\t\tval = rd32(hw, QINT_TQCTL(reg));\n\t\t\t\tval &= ~QINT_TQCTL_CAUSE_ENA_M;\n\t\t\t\twr32(hw, QINT_TQCTL(reg), val);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (vsi->rx_rings) {\n\t\tice_for_each_rxq(vsi, i) {\n\t\t\tif (vsi->rx_rings[i]) {\n\t\t\t\tu16 reg;\n\n\t\t\t\treg = vsi->rx_rings[i]->reg_idx;\n\t\t\t\tval = rd32(hw, QINT_RQCTL(reg));\n\t\t\t\tval &= ~QINT_RQCTL_CAUSE_ENA_M;\n\t\t\t\twr32(hw, QINT_RQCTL(reg), val);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tice_for_each_q_vector(vsi, i) {\n\t\tif (!vsi->q_vectors[i])\n\t\t\tcontinue;\n\t\twr32(hw, GLINT_DYN_CTL(vsi->q_vectors[i]->reg_idx), 0);\n\t}\n\n\tice_flush(hw);\n\n\t \n\tif (vsi->type == ICE_VSI_VF)\n\t\treturn;\n\n\tice_for_each_q_vector(vsi, i)\n\t\tsynchronize_irq(vsi->q_vectors[i]->irq.virq);\n}\n\n \nint ice_vsi_release(struct ice_vsi *vsi)\n{\n\tstruct ice_pf *pf;\n\n\tif (!vsi->back)\n\t\treturn -ENODEV;\n\tpf = vsi->back;\n\n\tif (test_bit(ICE_FLAG_RSS_ENA, pf->flags))\n\t\tice_rss_clean(vsi);\n\n\tice_vsi_close(vsi);\n\tice_vsi_decfg(vsi);\n\n\t \n\tif (!ice_is_reset_in_progress(pf->state))\n\t\tice_vsi_delete(vsi);\n\n\treturn 0;\n}\n\n \nstatic int\nice_vsi_rebuild_get_coalesce(struct ice_vsi *vsi,\n\t\t\t     struct ice_coalesce_stored *coalesce)\n{\n\tint i;\n\n\tice_for_each_q_vector(vsi, i) {\n\t\tstruct ice_q_vector *q_vector = vsi->q_vectors[i];\n\n\t\tcoalesce[i].itr_tx = q_vector->tx.itr_settings;\n\t\tcoalesce[i].itr_rx = q_vector->rx.itr_settings;\n\t\tcoalesce[i].intrl = q_vector->intrl;\n\n\t\tif (i < vsi->num_txq)\n\t\t\tcoalesce[i].tx_valid = true;\n\t\tif (i < vsi->num_rxq)\n\t\t\tcoalesce[i].rx_valid = true;\n\t}\n\n\treturn vsi->num_q_vectors;\n}\n\n \nstatic void\nice_vsi_rebuild_set_coalesce(struct ice_vsi *vsi,\n\t\t\t     struct ice_coalesce_stored *coalesce, int size)\n{\n\tstruct ice_ring_container *rc;\n\tint i;\n\n\tif ((size && !coalesce) || !vsi)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < size && i < vsi->num_q_vectors; i++) {\n\t\t \n\t\tif (i < vsi->alloc_rxq && coalesce[i].rx_valid) {\n\t\t\trc = &vsi->q_vectors[i]->rx;\n\t\t\trc->itr_settings = coalesce[i].itr_rx;\n\t\t\tice_write_itr(rc, rc->itr_setting);\n\t\t} else if (i < vsi->alloc_rxq) {\n\t\t\trc = &vsi->q_vectors[i]->rx;\n\t\t\trc->itr_settings = coalesce[0].itr_rx;\n\t\t\tice_write_itr(rc, rc->itr_setting);\n\t\t}\n\n\t\tif (i < vsi->alloc_txq && coalesce[i].tx_valid) {\n\t\t\trc = &vsi->q_vectors[i]->tx;\n\t\t\trc->itr_settings = coalesce[i].itr_tx;\n\t\t\tice_write_itr(rc, rc->itr_setting);\n\t\t} else if (i < vsi->alloc_txq) {\n\t\t\trc = &vsi->q_vectors[i]->tx;\n\t\t\trc->itr_settings = coalesce[0].itr_tx;\n\t\t\tice_write_itr(rc, rc->itr_setting);\n\t\t}\n\n\t\tvsi->q_vectors[i]->intrl = coalesce[i].intrl;\n\t\tice_set_q_vector_intrl(vsi->q_vectors[i]);\n\t}\n\n\t \n\tfor (; i < vsi->num_q_vectors; i++) {\n\t\t \n\t\trc = &vsi->q_vectors[i]->tx;\n\t\trc->itr_settings = coalesce[0].itr_tx;\n\t\tice_write_itr(rc, rc->itr_setting);\n\n\t\t \n\t\trc = &vsi->q_vectors[i]->rx;\n\t\trc->itr_settings = coalesce[0].itr_rx;\n\t\tice_write_itr(rc, rc->itr_setting);\n\n\t\tvsi->q_vectors[i]->intrl = coalesce[0].intrl;\n\t\tice_set_q_vector_intrl(vsi->q_vectors[i]);\n\t}\n}\n\n \nstatic void\nice_vsi_realloc_stat_arrays(struct ice_vsi *vsi, int prev_txq, int prev_rxq)\n{\n\tstruct ice_vsi_stats *vsi_stat;\n\tstruct ice_pf *pf = vsi->back;\n\tint i;\n\n\tif (!prev_txq || !prev_rxq)\n\t\treturn;\n\tif (vsi->type == ICE_VSI_CHNL)\n\t\treturn;\n\n\tvsi_stat = pf->vsi_stats[vsi->idx];\n\n\tif (vsi->num_txq < prev_txq) {\n\t\tfor (i = vsi->num_txq; i < prev_txq; i++) {\n\t\t\tif (vsi_stat->tx_ring_stats[i]) {\n\t\t\t\tkfree_rcu(vsi_stat->tx_ring_stats[i], rcu);\n\t\t\t\tWRITE_ONCE(vsi_stat->tx_ring_stats[i], NULL);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (vsi->num_rxq < prev_rxq) {\n\t\tfor (i = vsi->num_rxq; i < prev_rxq; i++) {\n\t\t\tif (vsi_stat->rx_ring_stats[i]) {\n\t\t\t\tkfree_rcu(vsi_stat->rx_ring_stats[i], rcu);\n\t\t\t\tWRITE_ONCE(vsi_stat->rx_ring_stats[i], NULL);\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nint ice_vsi_rebuild(struct ice_vsi *vsi, u32 vsi_flags)\n{\n\tstruct ice_vsi_cfg_params params = {};\n\tstruct ice_coalesce_stored *coalesce;\n\tint ret, prev_txq, prev_rxq;\n\tint prev_num_q_vectors = 0;\n\tstruct ice_pf *pf;\n\n\tif (!vsi)\n\t\treturn -EINVAL;\n\n\tparams = ice_vsi_to_params(vsi);\n\tparams.flags = vsi_flags;\n\n\tpf = vsi->back;\n\tif (WARN_ON(vsi->type == ICE_VSI_VF && !vsi->vf))\n\t\treturn -EINVAL;\n\n\tcoalesce = kcalloc(vsi->num_q_vectors,\n\t\t\t   sizeof(struct ice_coalesce_stored), GFP_KERNEL);\n\tif (!coalesce)\n\t\treturn -ENOMEM;\n\n\tprev_num_q_vectors = ice_vsi_rebuild_get_coalesce(vsi, coalesce);\n\n\tprev_txq = vsi->num_txq;\n\tprev_rxq = vsi->num_rxq;\n\n\tice_vsi_decfg(vsi);\n\tret = ice_vsi_cfg_def(vsi, &params);\n\tif (ret)\n\t\tgoto err_vsi_cfg;\n\n\tret = ice_vsi_cfg_tc_lan(pf, vsi);\n\tif (ret) {\n\t\tif (vsi_flags & ICE_VSI_FLAG_INIT) {\n\t\t\tret = -EIO;\n\t\t\tgoto err_vsi_cfg_tc_lan;\n\t\t}\n\n\t\tkfree(coalesce);\n\t\treturn ice_schedule_reset(pf, ICE_RESET_PFR);\n\t}\n\n\tice_vsi_realloc_stat_arrays(vsi, prev_txq, prev_rxq);\n\n\tice_vsi_rebuild_set_coalesce(vsi, coalesce, prev_num_q_vectors);\n\tkfree(coalesce);\n\n\treturn 0;\n\nerr_vsi_cfg_tc_lan:\n\tice_vsi_decfg(vsi);\nerr_vsi_cfg:\n\tkfree(coalesce);\n\treturn ret;\n}\n\n \nbool ice_is_reset_in_progress(unsigned long *state)\n{\n\treturn test_bit(ICE_RESET_OICR_RECV, state) ||\n\t       test_bit(ICE_PFR_REQ, state) ||\n\t       test_bit(ICE_CORER_REQ, state) ||\n\t       test_bit(ICE_GLOBR_REQ, state);\n}\n\n \nint ice_wait_for_reset(struct ice_pf *pf, unsigned long timeout)\n{\n\tlong ret;\n\n\tret = wait_event_interruptible_timeout(pf->reset_wait_queue,\n\t\t\t\t\t       !ice_is_reset_in_progress(pf->state),\n\t\t\t\t\t       timeout);\n\tif (ret < 0)\n\t\treturn ret;\n\telse if (!ret)\n\t\treturn -EBUSY;\n\telse\n\t\treturn 0;\n}\n\n \nstatic void ice_vsi_update_q_map(struct ice_vsi *vsi, struct ice_vsi_ctx *ctx)\n{\n\tvsi->info.mapping_flags = ctx->info.mapping_flags;\n\tmemcpy(&vsi->info.q_mapping, &ctx->info.q_mapping,\n\t       sizeof(vsi->info.q_mapping));\n\tmemcpy(&vsi->info.tc_mapping, ctx->info.tc_mapping,\n\t       sizeof(vsi->info.tc_mapping));\n}\n\n \nvoid ice_vsi_cfg_netdev_tc(struct ice_vsi *vsi, u8 ena_tc)\n{\n\tstruct net_device *netdev = vsi->netdev;\n\tstruct ice_pf *pf = vsi->back;\n\tint numtc = vsi->tc_cfg.numtc;\n\tstruct ice_dcbx_cfg *dcbcfg;\n\tu8 netdev_tc;\n\tint i;\n\n\tif (!netdev)\n\t\treturn;\n\n\t \n\tif (vsi->type == ICE_VSI_CHNL)\n\t\treturn;\n\n\tif (!ena_tc) {\n\t\tnetdev_reset_tc(netdev);\n\t\treturn;\n\t}\n\n\tif (vsi->type == ICE_VSI_PF && ice_is_adq_active(pf))\n\t\tnumtc = vsi->all_numtc;\n\n\tif (netdev_set_num_tc(netdev, numtc))\n\t\treturn;\n\n\tdcbcfg = &pf->hw.port_info->qos_cfg.local_dcbx_cfg;\n\n\tice_for_each_traffic_class(i)\n\t\tif (vsi->tc_cfg.ena_tc & BIT(i))\n\t\t\tnetdev_set_tc_queue(netdev,\n\t\t\t\t\t    vsi->tc_cfg.tc_info[i].netdev_tc,\n\t\t\t\t\t    vsi->tc_cfg.tc_info[i].qcount_tx,\n\t\t\t\t\t    vsi->tc_cfg.tc_info[i].qoffset);\n\t \n\tice_for_each_chnl_tc(i) {\n\t\tif (!(vsi->all_enatc & BIT(i)))\n\t\t\tbreak;\n\t\tif (!vsi->mqprio_qopt.qopt.count[i])\n\t\t\tbreak;\n\t\tnetdev_set_tc_queue(netdev, i,\n\t\t\t\t    vsi->mqprio_qopt.qopt.count[i],\n\t\t\t\t    vsi->mqprio_qopt.qopt.offset[i]);\n\t}\n\n\tif (test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))\n\t\treturn;\n\n\tfor (i = 0; i < ICE_MAX_USER_PRIORITY; i++) {\n\t\tu8 ets_tc = dcbcfg->etscfg.prio_table[i];\n\n\t\t \n\t\tnetdev_tc = vsi->tc_cfg.tc_info[ets_tc].netdev_tc;\n\t\tnetdev_set_prio_tc_map(netdev, i, netdev_tc);\n\t}\n}\n\n \nstatic int\nice_vsi_setup_q_map_mqprio(struct ice_vsi *vsi, struct ice_vsi_ctx *ctxt,\n\t\t\t   u8 ena_tc)\n{\n\tu16 pow, offset = 0, qcount_tx = 0, qcount_rx = 0, qmap;\n\tu16 tc0_offset = vsi->mqprio_qopt.qopt.offset[0];\n\tint tc0_qcount = vsi->mqprio_qopt.qopt.count[0];\n\tu16 new_txq, new_rxq;\n\tu8 netdev_tc = 0;\n\tint i;\n\n\tvsi->tc_cfg.ena_tc = ena_tc ? ena_tc : 1;\n\n\tpow = order_base_2(tc0_qcount);\n\tqmap = ((tc0_offset << ICE_AQ_VSI_TC_Q_OFFSET_S) &\n\t\tICE_AQ_VSI_TC_Q_OFFSET_M) |\n\t\t((pow << ICE_AQ_VSI_TC_Q_NUM_S) & ICE_AQ_VSI_TC_Q_NUM_M);\n\n\tice_for_each_traffic_class(i) {\n\t\tif (!(vsi->tc_cfg.ena_tc & BIT(i))) {\n\t\t\t \n\t\t\tvsi->tc_cfg.tc_info[i].qoffset = 0;\n\t\t\tvsi->tc_cfg.tc_info[i].qcount_rx = 1;\n\t\t\tvsi->tc_cfg.tc_info[i].qcount_tx = 1;\n\t\t\tvsi->tc_cfg.tc_info[i].netdev_tc = 0;\n\t\t\tctxt->info.tc_mapping[i] = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\toffset = vsi->mqprio_qopt.qopt.offset[i];\n\t\tqcount_rx = vsi->mqprio_qopt.qopt.count[i];\n\t\tqcount_tx = vsi->mqprio_qopt.qopt.count[i];\n\t\tvsi->tc_cfg.tc_info[i].qoffset = offset;\n\t\tvsi->tc_cfg.tc_info[i].qcount_rx = qcount_rx;\n\t\tvsi->tc_cfg.tc_info[i].qcount_tx = qcount_tx;\n\t\tvsi->tc_cfg.tc_info[i].netdev_tc = netdev_tc++;\n\t}\n\n\tif (vsi->all_numtc && vsi->all_numtc != vsi->tc_cfg.numtc) {\n\t\tice_for_each_chnl_tc(i) {\n\t\t\tif (!(vsi->all_enatc & BIT(i)))\n\t\t\t\tcontinue;\n\t\t\toffset = vsi->mqprio_qopt.qopt.offset[i];\n\t\t\tqcount_rx = vsi->mqprio_qopt.qopt.count[i];\n\t\t\tqcount_tx = vsi->mqprio_qopt.qopt.count[i];\n\t\t}\n\t}\n\n\tnew_txq = offset + qcount_tx;\n\tif (new_txq > vsi->alloc_txq) {\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Trying to use more Tx queues (%u), than were allocated (%u)!\\n\",\n\t\t\tnew_txq, vsi->alloc_txq);\n\t\treturn -EINVAL;\n\t}\n\n\tnew_rxq = offset + qcount_rx;\n\tif (new_rxq > vsi->alloc_rxq) {\n\t\tdev_err(ice_pf_to_dev(vsi->back), \"Trying to use more Rx queues (%u), than were allocated (%u)!\\n\",\n\t\t\tnew_rxq, vsi->alloc_rxq);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tvsi->num_txq = new_txq;\n\tvsi->num_rxq = new_rxq;\n\n\t \n\tctxt->info.tc_mapping[0] = cpu_to_le16(qmap);\n\tctxt->info.q_mapping[0] = cpu_to_le16(vsi->rxq_map[0]);\n\tctxt->info.q_mapping[1] = cpu_to_le16(tc0_qcount);\n\n\t \n\tif (tc0_qcount && tc0_qcount < vsi->num_rxq) {\n\t\tvsi->cnt_q_avail = vsi->num_rxq - tc0_qcount;\n\t\tvsi->next_base_q = tc0_qcount;\n\t}\n\tdev_dbg(ice_pf_to_dev(vsi->back), \"vsi->num_txq = %d\\n\",  vsi->num_txq);\n\tdev_dbg(ice_pf_to_dev(vsi->back), \"vsi->num_rxq = %d\\n\",  vsi->num_rxq);\n\tdev_dbg(ice_pf_to_dev(vsi->back), \"all_numtc %u, all_enatc: 0x%04x, tc_cfg.numtc %u\\n\",\n\t\tvsi->all_numtc, vsi->all_enatc, vsi->tc_cfg.numtc);\n\n\treturn 0;\n}\n\n \nint ice_vsi_cfg_tc(struct ice_vsi *vsi, u8 ena_tc)\n{\n\tu16 max_txqs[ICE_MAX_TRAFFIC_CLASS] = { 0 };\n\tstruct ice_pf *pf = vsi->back;\n\tstruct ice_tc_cfg old_tc_cfg;\n\tstruct ice_vsi_ctx *ctx;\n\tstruct device *dev;\n\tint i, ret = 0;\n\tu8 num_tc = 0;\n\n\tdev = ice_pf_to_dev(pf);\n\tif (vsi->tc_cfg.ena_tc == ena_tc &&\n\t    vsi->mqprio_qopt.mode != TC_MQPRIO_MODE_CHANNEL)\n\t\treturn 0;\n\n\tice_for_each_traffic_class(i) {\n\t\t \n\t\tif (ena_tc & BIT(i))\n\t\t\tnum_tc++;\n\t\t \n\t\tmax_txqs[i] = vsi->alloc_txq;\n\t\t \n\t\tif (vsi->type == ICE_VSI_CHNL &&\n\t\t    test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))\n\t\t\tmax_txqs[i] = vsi->num_txq;\n\t}\n\n\tmemcpy(&old_tc_cfg, &vsi->tc_cfg, sizeof(old_tc_cfg));\n\tvsi->tc_cfg.ena_tc = ena_tc;\n\tvsi->tc_cfg.numtc = num_tc;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->vf_num = 0;\n\tctx->info = vsi->info;\n\n\tif (vsi->type == ICE_VSI_PF &&\n\t    test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))\n\t\tret = ice_vsi_setup_q_map_mqprio(vsi, ctx, ena_tc);\n\telse\n\t\tret = ice_vsi_setup_q_map(vsi, ctx);\n\n\tif (ret) {\n\t\tmemcpy(&vsi->tc_cfg, &old_tc_cfg, sizeof(vsi->tc_cfg));\n\t\tgoto out;\n\t}\n\n\t \n\tctx->info.valid_sections = cpu_to_le16(ICE_AQ_VSI_PROP_RXQ_MAP_VALID);\n\tret = ice_update_vsi(&pf->hw, vsi->idx, ctx, NULL);\n\tif (ret) {\n\t\tdev_info(dev, \"Failed VSI Update\\n\");\n\t\tgoto out;\n\t}\n\n\tif (vsi->type == ICE_VSI_PF &&\n\t    test_bit(ICE_FLAG_TC_MQPRIO, pf->flags))\n\t\tret = ice_cfg_vsi_lan(vsi->port_info, vsi->idx, 1, max_txqs);\n\telse\n\t\tret = ice_cfg_vsi_lan(vsi->port_info, vsi->idx,\n\t\t\t\t      vsi->tc_cfg.ena_tc, max_txqs);\n\n\tif (ret) {\n\t\tdev_err(dev, \"VSI %d failed TC config, error %d\\n\",\n\t\t\tvsi->vsi_num, ret);\n\t\tgoto out;\n\t}\n\tice_vsi_update_q_map(vsi, ctx);\n\tvsi->info.valid_sections = 0;\n\n\tice_vsi_cfg_netdev_tc(vsi, ena_tc);\nout:\n\tkfree(ctx);\n\treturn ret;\n}\n\n \nstatic void ice_update_ring_stats(struct ice_q_stats *stats, u64 pkts, u64 bytes)\n{\n\tstats->bytes += bytes;\n\tstats->pkts += pkts;\n}\n\n \nvoid ice_update_tx_ring_stats(struct ice_tx_ring *tx_ring, u64 pkts, u64 bytes)\n{\n\tu64_stats_update_begin(&tx_ring->ring_stats->syncp);\n\tice_update_ring_stats(&tx_ring->ring_stats->stats, pkts, bytes);\n\tu64_stats_update_end(&tx_ring->ring_stats->syncp);\n}\n\n \nvoid ice_update_rx_ring_stats(struct ice_rx_ring *rx_ring, u64 pkts, u64 bytes)\n{\n\tu64_stats_update_begin(&rx_ring->ring_stats->syncp);\n\tice_update_ring_stats(&rx_ring->ring_stats->stats, pkts, bytes);\n\tu64_stats_update_end(&rx_ring->ring_stats->syncp);\n}\n\n \nbool ice_is_dflt_vsi_in_use(struct ice_port_info *pi)\n{\n\tbool exists = false;\n\n\tice_check_if_dflt_vsi(pi, 0, &exists);\n\treturn exists;\n}\n\n \nbool ice_is_vsi_dflt_vsi(struct ice_vsi *vsi)\n{\n\treturn ice_check_if_dflt_vsi(vsi->port_info, vsi->idx, NULL);\n}\n\n \nint ice_set_dflt_vsi(struct ice_vsi *vsi)\n{\n\tstruct device *dev;\n\tint status;\n\n\tif (!vsi)\n\t\treturn -EINVAL;\n\n\tdev = ice_pf_to_dev(vsi->back);\n\n\tif (ice_lag_is_switchdev_running(vsi->back)) {\n\t\tdev_dbg(dev, \"VSI %d passed is a part of LAG containing interfaces in switchdev mode, nothing to do\\n\",\n\t\t\tvsi->vsi_num);\n\t\treturn 0;\n\t}\n\n\t \n\tif (ice_is_vsi_dflt_vsi(vsi)) {\n\t\tdev_dbg(dev, \"VSI %d passed in is already the default forwarding VSI, nothing to do\\n\",\n\t\t\tvsi->vsi_num);\n\t\treturn 0;\n\t}\n\n\tstatus = ice_cfg_dflt_vsi(vsi->port_info, vsi->idx, true, ICE_FLTR_RX);\n\tif (status) {\n\t\tdev_err(dev, \"Failed to set VSI %d as the default forwarding VSI, error %d\\n\",\n\t\t\tvsi->vsi_num, status);\n\t\treturn status;\n\t}\n\n\treturn 0;\n}\n\n \nint ice_clear_dflt_vsi(struct ice_vsi *vsi)\n{\n\tstruct device *dev;\n\tint status;\n\n\tif (!vsi)\n\t\treturn -EINVAL;\n\n\tdev = ice_pf_to_dev(vsi->back);\n\n\t \n\tif (!ice_is_dflt_vsi_in_use(vsi->port_info))\n\t\treturn -ENODEV;\n\n\tstatus = ice_cfg_dflt_vsi(vsi->port_info, vsi->idx, false,\n\t\t\t\t  ICE_FLTR_RX);\n\tif (status) {\n\t\tdev_err(dev, \"Failed to clear the default forwarding VSI %d, error %d\\n\",\n\t\t\tvsi->vsi_num, status);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n \nint ice_get_link_speed_mbps(struct ice_vsi *vsi)\n{\n\tunsigned int link_speed;\n\n\tlink_speed = vsi->port_info->phy.link_info.link_speed;\n\n\treturn (int)ice_get_link_speed(fls(link_speed) - 1);\n}\n\n \nint ice_get_link_speed_kbps(struct ice_vsi *vsi)\n{\n\tint speed_mbps;\n\n\tspeed_mbps = ice_get_link_speed_mbps(vsi);\n\n\treturn speed_mbps * 1000;\n}\n\n \nint ice_set_min_bw_limit(struct ice_vsi *vsi, u64 min_tx_rate)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\tint status;\n\tint speed;\n\n\tdev = ice_pf_to_dev(pf);\n\tif (!vsi->port_info) {\n\t\tdev_dbg(dev, \"VSI %d, type %u specified doesn't have valid port_info\\n\",\n\t\t\tvsi->idx, vsi->type);\n\t\treturn -EINVAL;\n\t}\n\n\tspeed = ice_get_link_speed_kbps(vsi);\n\tif (min_tx_rate > (u64)speed) {\n\t\tdev_err(dev, \"invalid min Tx rate %llu Kbps specified for %s %d is greater than current link speed %u Kbps\\n\",\n\t\t\tmin_tx_rate, ice_vsi_type_str(vsi->type), vsi->idx,\n\t\t\tspeed);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (min_tx_rate) {\n\t\tstatus = ice_cfg_vsi_bw_lmt_per_tc(vsi->port_info, vsi->idx, 0,\n\t\t\t\t\t\t   ICE_MIN_BW, min_tx_rate);\n\t\tif (status) {\n\t\t\tdev_err(dev, \"failed to set min Tx rate(%llu Kbps) for %s %d\\n\",\n\t\t\t\tmin_tx_rate, ice_vsi_type_str(vsi->type),\n\t\t\t\tvsi->idx);\n\t\t\treturn status;\n\t\t}\n\n\t\tdev_dbg(dev, \"set min Tx rate(%llu Kbps) for %s\\n\",\n\t\t\tmin_tx_rate, ice_vsi_type_str(vsi->type));\n\t} else {\n\t\tstatus = ice_cfg_vsi_bw_dflt_lmt_per_tc(vsi->port_info,\n\t\t\t\t\t\t\tvsi->idx, 0,\n\t\t\t\t\t\t\tICE_MIN_BW);\n\t\tif (status) {\n\t\t\tdev_err(dev, \"failed to clear min Tx rate configuration for %s %d\\n\",\n\t\t\t\tice_vsi_type_str(vsi->type), vsi->idx);\n\t\t\treturn status;\n\t\t}\n\n\t\tdev_dbg(dev, \"cleared min Tx rate configuration for %s %d\\n\",\n\t\t\tice_vsi_type_str(vsi->type), vsi->idx);\n\t}\n\n\treturn 0;\n}\n\n \nint ice_set_max_bw_limit(struct ice_vsi *vsi, u64 max_tx_rate)\n{\n\tstruct ice_pf *pf = vsi->back;\n\tstruct device *dev;\n\tint status;\n\tint speed;\n\n\tdev = ice_pf_to_dev(pf);\n\tif (!vsi->port_info) {\n\t\tdev_dbg(dev, \"VSI %d, type %u specified doesn't have valid port_info\\n\",\n\t\t\tvsi->idx, vsi->type);\n\t\treturn -EINVAL;\n\t}\n\n\tspeed = ice_get_link_speed_kbps(vsi);\n\tif (max_tx_rate > (u64)speed) {\n\t\tdev_err(dev, \"invalid max Tx rate %llu Kbps specified for %s %d is greater than current link speed %u Kbps\\n\",\n\t\t\tmax_tx_rate, ice_vsi_type_str(vsi->type), vsi->idx,\n\t\t\tspeed);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (max_tx_rate) {\n\t\tstatus = ice_cfg_vsi_bw_lmt_per_tc(vsi->port_info, vsi->idx, 0,\n\t\t\t\t\t\t   ICE_MAX_BW, max_tx_rate);\n\t\tif (status) {\n\t\t\tdev_err(dev, \"failed setting max Tx rate(%llu Kbps) for %s %d\\n\",\n\t\t\t\tmax_tx_rate, ice_vsi_type_str(vsi->type),\n\t\t\t\tvsi->idx);\n\t\t\treturn status;\n\t\t}\n\n\t\tdev_dbg(dev, \"set max Tx rate(%llu Kbps) for %s %d\\n\",\n\t\t\tmax_tx_rate, ice_vsi_type_str(vsi->type), vsi->idx);\n\t} else {\n\t\tstatus = ice_cfg_vsi_bw_dflt_lmt_per_tc(vsi->port_info,\n\t\t\t\t\t\t\tvsi->idx, 0,\n\t\t\t\t\t\t\tICE_MAX_BW);\n\t\tif (status) {\n\t\t\tdev_err(dev, \"failed clearing max Tx rate configuration for %s %d\\n\",\n\t\t\t\tice_vsi_type_str(vsi->type), vsi->idx);\n\t\t\treturn status;\n\t\t}\n\n\t\tdev_dbg(dev, \"cleared max Tx rate configuration for %s %d\\n\",\n\t\t\tice_vsi_type_str(vsi->type), vsi->idx);\n\t}\n\n\treturn 0;\n}\n\n \nint ice_set_link(struct ice_vsi *vsi, bool ena)\n{\n\tstruct device *dev = ice_pf_to_dev(vsi->back);\n\tstruct ice_port_info *pi = vsi->port_info;\n\tstruct ice_hw *hw = pi->hw;\n\tint status;\n\n\tif (vsi->type != ICE_VSI_PF)\n\t\treturn -EINVAL;\n\n\tstatus = ice_aq_set_link_restart_an(pi, ena, NULL);\n\n\t \n\tif (status == -EIO) {\n\t\tif (hw->adminq.sq_last_status == ICE_AQ_RC_EMODE)\n\t\t\tdev_dbg(dev, \"can't set link to %s, err %d aq_err %s. not fatal, continuing\\n\",\n\t\t\t\t(ena ? \"ON\" : \"OFF\"), status,\n\t\t\t\tice_aq_str(hw->adminq.sq_last_status));\n\t} else if (status) {\n\t\tdev_err(dev, \"can't set link to %s, err %d aq_err %s\\n\",\n\t\t\t(ena ? \"ON\" : \"OFF\"), status,\n\t\t\tice_aq_str(hw->adminq.sq_last_status));\n\t\treturn status;\n\t}\n\n\treturn 0;\n}\n\n \nint ice_vsi_add_vlan_zero(struct ice_vsi *vsi)\n{\n\tstruct ice_vsi_vlan_ops *vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);\n\tstruct ice_vlan vlan;\n\tint err;\n\n\tvlan = ICE_VLAN(0, 0, 0);\n\terr = vlan_ops->add_vlan(vsi, &vlan);\n\tif (err && err != -EEXIST)\n\t\treturn err;\n\n\t \n\tif (!ice_is_dvm_ena(&vsi->back->hw))\n\t\treturn 0;\n\n\tvlan = ICE_VLAN(ETH_P_8021Q, 0, 0);\n\terr = vlan_ops->add_vlan(vsi, &vlan);\n\tif (err && err != -EEXIST)\n\t\treturn err;\n\n\treturn 0;\n}\n\n \nint ice_vsi_del_vlan_zero(struct ice_vsi *vsi)\n{\n\tstruct ice_vsi_vlan_ops *vlan_ops = ice_get_compat_vsi_vlan_ops(vsi);\n\tstruct ice_vlan vlan;\n\tint err;\n\n\tvlan = ICE_VLAN(0, 0, 0);\n\terr = vlan_ops->del_vlan(vsi, &vlan);\n\tif (err && err != -EEXIST)\n\t\treturn err;\n\n\t \n\tif (!ice_is_dvm_ena(&vsi->back->hw))\n\t\treturn 0;\n\n\tvlan = ICE_VLAN(ETH_P_8021Q, 0, 0);\n\terr = vlan_ops->del_vlan(vsi, &vlan);\n\tif (err && err != -EEXIST)\n\t\treturn err;\n\n\t \n\treturn ice_clear_vsi_promisc(&vsi->back->hw, vsi->idx,\n\t\t\t\t    ICE_MCAST_VLAN_PROMISC_BITS, 0);\n}\n\n \nstatic u16 ice_vsi_num_zero_vlans(struct ice_vsi *vsi)\n{\n#define ICE_DVM_NUM_ZERO_VLAN_FLTRS\t2\n#define ICE_SVM_NUM_ZERO_VLAN_FLTRS\t1\n\t \n\tif (vsi->type == ICE_VSI_VF) {\n\t\tif (WARN_ON(!vsi->vf))\n\t\t\treturn 0;\n\n\t\tif (ice_vf_is_port_vlan_ena(vsi->vf))\n\t\t\treturn 0;\n\t}\n\n\tif (ice_is_dvm_ena(&vsi->back->hw))\n\t\treturn ICE_DVM_NUM_ZERO_VLAN_FLTRS;\n\telse\n\t\treturn ICE_SVM_NUM_ZERO_VLAN_FLTRS;\n}\n\n \nbool ice_vsi_has_non_zero_vlans(struct ice_vsi *vsi)\n{\n\treturn (vsi->num_vlan > ice_vsi_num_zero_vlans(vsi));\n}\n\n \nu16 ice_vsi_num_non_zero_vlans(struct ice_vsi *vsi)\n{\n\treturn (vsi->num_vlan - ice_vsi_num_zero_vlans(vsi));\n}\n\n \nbool ice_is_feature_supported(struct ice_pf *pf, enum ice_feature f)\n{\n\tif (f < 0 || f >= ICE_F_MAX)\n\t\treturn false;\n\n\treturn test_bit(f, pf->features);\n}\n\n \nvoid ice_set_feature_support(struct ice_pf *pf, enum ice_feature f)\n{\n\tif (f < 0 || f >= ICE_F_MAX)\n\t\treturn;\n\n\tset_bit(f, pf->features);\n}\n\n \nvoid ice_clear_feature_support(struct ice_pf *pf, enum ice_feature f)\n{\n\tif (f < 0 || f >= ICE_F_MAX)\n\t\treturn;\n\n\tclear_bit(f, pf->features);\n}\n\n \nvoid ice_init_feature_support(struct ice_pf *pf)\n{\n\tswitch (pf->hw.device_id) {\n\tcase ICE_DEV_ID_E810C_BACKPLANE:\n\tcase ICE_DEV_ID_E810C_QSFP:\n\tcase ICE_DEV_ID_E810C_SFP:\n\t\tice_set_feature_support(pf, ICE_F_DSCP);\n\t\tice_set_feature_support(pf, ICE_F_PTP_EXTTS);\n\t\tif (ice_is_e810t(&pf->hw)) {\n\t\t\tice_set_feature_support(pf, ICE_F_SMA_CTRL);\n\t\t\tif (ice_gnss_is_gps_present(&pf->hw))\n\t\t\t\tice_set_feature_support(pf, ICE_F_GNSS);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n \nint\nice_vsi_update_security(struct ice_vsi *vsi, void (*fill)(struct ice_vsi_ctx *))\n{\n\tstruct ice_vsi_ctx ctx = { 0 };\n\n\tctx.info = vsi->info;\n\tctx.info.valid_sections = cpu_to_le16(ICE_AQ_VSI_PROP_SECURITY_VALID);\n\tfill(&ctx);\n\n\tif (ice_update_vsi(&vsi->back->hw, vsi->idx, &ctx, NULL))\n\t\treturn -ENODEV;\n\n\tvsi->info = ctx.info;\n\treturn 0;\n}\n\n \nvoid ice_vsi_ctx_set_antispoof(struct ice_vsi_ctx *ctx)\n{\n\tctx->info.sec_flags |= ICE_AQ_VSI_SEC_FLAG_ENA_MAC_ANTI_SPOOF |\n\t\t\t       (ICE_AQ_VSI_SEC_TX_VLAN_PRUNE_ENA <<\n\t\t\t\tICE_AQ_VSI_SEC_TX_PRUNE_ENA_S);\n}\n\n \nvoid ice_vsi_ctx_clear_antispoof(struct ice_vsi_ctx *ctx)\n{\n\tctx->info.sec_flags &= ~ICE_AQ_VSI_SEC_FLAG_ENA_MAC_ANTI_SPOOF &\n\t\t\t       ~(ICE_AQ_VSI_SEC_TX_VLAN_PRUNE_ENA <<\n\t\t\t\t ICE_AQ_VSI_SEC_TX_PRUNE_ENA_S);\n}\n\n \nvoid ice_vsi_ctx_set_allow_override(struct ice_vsi_ctx *ctx)\n{\n\tctx->info.sec_flags |= ICE_AQ_VSI_SEC_FLAG_ALLOW_DEST_OVRD;\n}\n\n \nvoid ice_vsi_ctx_clear_allow_override(struct ice_vsi_ctx *ctx)\n{\n\tctx->info.sec_flags &= ~ICE_AQ_VSI_SEC_FLAG_ALLOW_DEST_OVRD;\n}\n\n \nint\nice_vsi_update_local_lb(struct ice_vsi *vsi, bool set)\n{\n\tstruct ice_vsi_ctx ctx = {\n\t\t.info\t= vsi->info,\n\t};\n\n\tctx.info.valid_sections = cpu_to_le16(ICE_AQ_VSI_PROP_SW_VALID);\n\tif (set)\n\t\tctx.info.sw_flags |= ICE_AQ_VSI_SW_FLAG_LOCAL_LB;\n\telse\n\t\tctx.info.sw_flags &= ~ICE_AQ_VSI_SW_FLAG_LOCAL_LB;\n\n\tif (ice_update_vsi(&vsi->back->hw, vsi->idx, &ctx, NULL))\n\t\treturn -ENODEV;\n\n\tvsi->info = ctx.info;\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}