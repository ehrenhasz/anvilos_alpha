{
  "module_name": "ixgbe_main.c",
  "hash_id": "7565dc3f29d61dce0b06d25f10ddbb88a869520b012c12087adbac1e239e47bf",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/vmalloc.h>\n#include <linux/string.h>\n#include <linux/in.h>\n#include <linux/interrupt.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/sctp.h>\n#include <linux/pkt_sched.h>\n#include <linux/ipv6.h>\n#include <linux/slab.h>\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/if.h>\n#include <linux/if_vlan.h>\n#include <linux/if_macvlan.h>\n#include <linux/if_bridge.h>\n#include <linux/prefetch.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <linux/atomic.h>\n#include <linux/numa.h>\n#include <generated/utsrelease.h>\n#include <scsi/fc/fc_fcoe.h>\n#include <net/udp_tunnel.h>\n#include <net/pkt_cls.h>\n#include <net/tc_act/tc_gact.h>\n#include <net/tc_act/tc_mirred.h>\n#include <net/vxlan.h>\n#include <net/mpls.h>\n#include <net/netdev_queues.h>\n#include <net/xdp_sock_drv.h>\n#include <net/xfrm.h>\n\n#include \"ixgbe.h\"\n#include \"ixgbe_common.h\"\n#include \"ixgbe_dcb_82599.h\"\n#include \"ixgbe_phy.h\"\n#include \"ixgbe_sriov.h\"\n#include \"ixgbe_model.h\"\n#include \"ixgbe_txrx_common.h\"\n\nchar ixgbe_driver_name[] = \"ixgbe\";\nstatic const char ixgbe_driver_string[] =\n\t\t\t      \"Intel(R) 10 Gigabit PCI Express Network Driver\";\n#ifdef IXGBE_FCOE\nchar ixgbe_default_device_descr[] =\n\t\t\t      \"Intel(R) 10 Gigabit Network Connection\";\n#else\nstatic char ixgbe_default_device_descr[] =\n\t\t\t      \"Intel(R) 10 Gigabit Network Connection\";\n#endif\nstatic const char ixgbe_copyright[] =\n\t\t\t\t\"Copyright (c) 1999-2016 Intel Corporation.\";\n\nstatic const char ixgbe_overheat_msg[] = \"Network adapter has been stopped because it has over heated. Restart the computer. If the problem persists, power off the system and replace the adapter\";\n\nstatic const struct ixgbe_info *ixgbe_info_tbl[] = {\n\t[board_82598]\t\t= &ixgbe_82598_info,\n\t[board_82599]\t\t= &ixgbe_82599_info,\n\t[board_X540]\t\t= &ixgbe_X540_info,\n\t[board_X550]\t\t= &ixgbe_X550_info,\n\t[board_X550EM_x]\t= &ixgbe_X550EM_x_info,\n\t[board_x550em_x_fw]\t= &ixgbe_x550em_x_fw_info,\n\t[board_x550em_a]\t= &ixgbe_x550em_a_info,\n\t[board_x550em_a_fw]\t= &ixgbe_x550em_a_fw_info,\n};\n\n \nstatic const struct pci_device_id ixgbe_pci_tbl[] = {\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598AF_DUAL_PORT), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598AF_SINGLE_PORT), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598AT), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598AT2), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598EB_CX4), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598_CX4_DUAL_PORT), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598_DA_DUAL_PORT), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598_SR_DUAL_PORT_EM), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598EB_XF_LR), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598EB_SFP_LOM), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82598_BX), board_82598 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_KX4), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_XAUI_LOM), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_KR), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_SFP), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_SFP_EM), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_KX4_MEZZ), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_CX4), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_BACKPLANE_FCOE), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_SFP_FCOE), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_T3_LOM), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_COMBO_BACKPLANE), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X540T), board_X540 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_SFP_SF2), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_LS), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_QSFP_SF_QP), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599EN_SFP), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_82599_SFP_SF_QP), board_82599 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X540T1), board_X540 },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550T), board_X550},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550T1), board_X550},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_X_KX4), board_X550EM_x},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_X_XFI), board_X550EM_x},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_X_KR), board_X550EM_x},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_X_10G_T), board_X550EM_x},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_X_SFP), board_X550EM_x},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_X_1G_T), board_x550em_x_fw},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_KR), board_x550em_a },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_KR_L), board_x550em_a },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_SFP_N), board_x550em_a },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_SGMII), board_x550em_a },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_SGMII_L), board_x550em_a },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_10G_T), board_x550em_a},\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_SFP), board_x550em_a },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_1G_T), board_x550em_a_fw },\n\t{PCI_VDEVICE(INTEL, IXGBE_DEV_ID_X550EM_A_1G_T_L), board_x550em_a_fw },\n\t \n\t{0, }\n};\nMODULE_DEVICE_TABLE(pci, ixgbe_pci_tbl);\n\n#ifdef CONFIG_IXGBE_DCA\nstatic int ixgbe_notify_dca(struct notifier_block *, unsigned long event,\n\t\t\t    void *p);\nstatic struct notifier_block dca_notifier = {\n\t.notifier_call = ixgbe_notify_dca,\n\t.next          = NULL,\n\t.priority      = 0\n};\n#endif\n\n#ifdef CONFIG_PCI_IOV\nstatic unsigned int max_vfs;\nmodule_param(max_vfs, uint, 0);\nMODULE_PARM_DESC(max_vfs,\n\t\t \"Maximum number of virtual functions to allocate per physical function - default is zero and maximum value is 63. (Deprecated)\");\n#endif  \n\nstatic bool allow_unsupported_sfp;\nmodule_param(allow_unsupported_sfp, bool, 0);\nMODULE_PARM_DESC(allow_unsupported_sfp,\n\t\t \"Allow unsupported and untested SFP+ modules on 82599-based adapters\");\n\n#define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV|NETIF_MSG_PROBE|NETIF_MSG_LINK)\nstatic int debug = -1;\nmodule_param(debug, int, 0);\nMODULE_PARM_DESC(debug, \"Debug level (0=none,...,16=all)\");\n\nMODULE_AUTHOR(\"Intel Corporation, <linux.nics@intel.com>\");\nMODULE_DESCRIPTION(\"Intel(R) 10 Gigabit PCI Express Network Driver\");\nMODULE_LICENSE(\"GPL v2\");\n\nDEFINE_STATIC_KEY_FALSE(ixgbe_xdp_locking_key);\nEXPORT_SYMBOL(ixgbe_xdp_locking_key);\n\nstatic struct workqueue_struct *ixgbe_wq;\n\nstatic bool ixgbe_check_cfg_remove(struct ixgbe_hw *hw, struct pci_dev *pdev);\nstatic void ixgbe_watchdog_link_is_down(struct ixgbe_adapter *);\n\nstatic const struct net_device_ops ixgbe_netdev_ops;\n\nstatic bool netif_is_ixgbe(struct net_device *dev)\n{\n\treturn dev && (dev->netdev_ops == &ixgbe_netdev_ops);\n}\n\nstatic int ixgbe_read_pci_cfg_word_parent(struct ixgbe_adapter *adapter,\n\t\t\t\t\t  u32 reg, u16 *value)\n{\n\tstruct pci_dev *parent_dev;\n\tstruct pci_bus *parent_bus;\n\n\tparent_bus = adapter->pdev->bus->parent;\n\tif (!parent_bus)\n\t\treturn -1;\n\n\tparent_dev = parent_bus->self;\n\tif (!parent_dev)\n\t\treturn -1;\n\n\tif (!pci_is_pcie(parent_dev))\n\t\treturn -1;\n\n\tpcie_capability_read_word(parent_dev, reg, value);\n\tif (*value == IXGBE_FAILED_READ_CFG_WORD &&\n\t    ixgbe_check_cfg_remove(&adapter->hw, parent_dev))\n\t\treturn -1;\n\treturn 0;\n}\n\nstatic s32 ixgbe_get_parent_bus_info(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu16 link_status = 0;\n\tint err;\n\n\thw->bus.type = ixgbe_bus_type_pci_express;\n\n\t \n\terr = ixgbe_read_pci_cfg_word_parent(adapter, 18, &link_status);\n\n\t \n\tif (err)\n\t\treturn err;\n\n\thw->bus.width = ixgbe_convert_bus_width(link_status);\n\thw->bus.speed = ixgbe_convert_bus_speed(link_status);\n\n\treturn 0;\n}\n\n \nstatic inline bool ixgbe_pcie_from_parent(struct ixgbe_hw *hw)\n{\n\tswitch (hw->device_id) {\n\tcase IXGBE_DEV_ID_82599_SFP_SF_QP:\n\tcase IXGBE_DEV_ID_82599_QSFP_SF_QP:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void ixgbe_check_minimum_link(struct ixgbe_adapter *adapter,\n\t\t\t\t     int expected_gts)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct pci_dev *pdev;\n\n\t \n\tif (hw->bus.type == ixgbe_bus_type_internal)\n\t\treturn;\n\n\t \n\tif (ixgbe_pcie_from_parent(&adapter->hw))\n\t\tpdev = adapter->pdev->bus->parent->self;\n\telse\n\t\tpdev = adapter->pdev;\n\n\tpcie_print_link_status(pdev);\n}\n\nstatic void ixgbe_service_event_schedule(struct ixgbe_adapter *adapter)\n{\n\tif (!test_bit(__IXGBE_DOWN, &adapter->state) &&\n\t    !test_bit(__IXGBE_REMOVING, &adapter->state) &&\n\t    !test_and_set_bit(__IXGBE_SERVICE_SCHED, &adapter->state))\n\t\tqueue_work(ixgbe_wq, &adapter->service_task);\n}\n\nstatic void ixgbe_remove_adapter(struct ixgbe_hw *hw)\n{\n\tstruct ixgbe_adapter *adapter = hw->back;\n\n\tif (!hw->hw_addr)\n\t\treturn;\n\thw->hw_addr = NULL;\n\te_dev_err(\"Adapter removed\\n\");\n\tif (test_bit(__IXGBE_SERVICE_INITED, &adapter->state))\n\t\tixgbe_service_event_schedule(adapter);\n}\n\nstatic u32 ixgbe_check_remove(struct ixgbe_hw *hw, u32 reg)\n{\n\tu8 __iomem *reg_addr;\n\tu32 value;\n\tint i;\n\n\treg_addr = READ_ONCE(hw->hw_addr);\n\tif (ixgbe_removed(reg_addr))\n\t\treturn IXGBE_FAILED_READ_REG;\n\n\t \n\tfor (i = 0; i < IXGBE_FAILED_READ_RETRIES; i++) {\n\t\tvalue = readl(reg_addr + IXGBE_STATUS);\n\t\tif (value != IXGBE_FAILED_READ_REG)\n\t\t\tbreak;\n\t\tmdelay(3);\n\t}\n\n\tif (value == IXGBE_FAILED_READ_REG)\n\t\tixgbe_remove_adapter(hw);\n\telse\n\t\tvalue = readl(reg_addr + reg);\n\treturn value;\n}\n\n \nu32 ixgbe_read_reg(struct ixgbe_hw *hw, u32 reg)\n{\n\tu8 __iomem *reg_addr = READ_ONCE(hw->hw_addr);\n\tu32 value;\n\n\tif (ixgbe_removed(reg_addr))\n\t\treturn IXGBE_FAILED_READ_REG;\n\tif (unlikely(hw->phy.nw_mng_if_sel &\n\t\t     IXGBE_NW_MNG_IF_SEL_SGMII_ENABLE)) {\n\t\tstruct ixgbe_adapter *adapter;\n\t\tint i;\n\n\t\tfor (i = 0; i < 200; ++i) {\n\t\t\tvalue = readl(reg_addr + IXGBE_MAC_SGMII_BUSY);\n\t\t\tif (likely(!value))\n\t\t\t\tgoto writes_completed;\n\t\t\tif (value == IXGBE_FAILED_READ_REG) {\n\t\t\t\tixgbe_remove_adapter(hw);\n\t\t\t\treturn IXGBE_FAILED_READ_REG;\n\t\t\t}\n\t\t\tudelay(5);\n\t\t}\n\n\t\tadapter = hw->back;\n\t\te_warn(hw, \"register writes incomplete %08x\\n\", value);\n\t}\n\nwrites_completed:\n\tvalue = readl(reg_addr + reg);\n\tif (unlikely(value == IXGBE_FAILED_READ_REG))\n\t\tvalue = ixgbe_check_remove(hw, reg);\n\treturn value;\n}\n\nstatic bool ixgbe_check_cfg_remove(struct ixgbe_hw *hw, struct pci_dev *pdev)\n{\n\tu16 value;\n\n\tpci_read_config_word(pdev, PCI_VENDOR_ID, &value);\n\tif (value == IXGBE_FAILED_READ_CFG_WORD) {\n\t\tixgbe_remove_adapter(hw);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nu16 ixgbe_read_pci_cfg_word(struct ixgbe_hw *hw, u32 reg)\n{\n\tstruct ixgbe_adapter *adapter = hw->back;\n\tu16 value;\n\n\tif (ixgbe_removed(hw->hw_addr))\n\t\treturn IXGBE_FAILED_READ_CFG_WORD;\n\tpci_read_config_word(adapter->pdev, reg, &value);\n\tif (value == IXGBE_FAILED_READ_CFG_WORD &&\n\t    ixgbe_check_cfg_remove(hw, adapter->pdev))\n\t\treturn IXGBE_FAILED_READ_CFG_WORD;\n\treturn value;\n}\n\n#ifdef CONFIG_PCI_IOV\nstatic u32 ixgbe_read_pci_cfg_dword(struct ixgbe_hw *hw, u32 reg)\n{\n\tstruct ixgbe_adapter *adapter = hw->back;\n\tu32 value;\n\n\tif (ixgbe_removed(hw->hw_addr))\n\t\treturn IXGBE_FAILED_READ_CFG_DWORD;\n\tpci_read_config_dword(adapter->pdev, reg, &value);\n\tif (value == IXGBE_FAILED_READ_CFG_DWORD &&\n\t    ixgbe_check_cfg_remove(hw, adapter->pdev))\n\t\treturn IXGBE_FAILED_READ_CFG_DWORD;\n\treturn value;\n}\n#endif  \n\nvoid ixgbe_write_pci_cfg_word(struct ixgbe_hw *hw, u32 reg, u16 value)\n{\n\tstruct ixgbe_adapter *adapter = hw->back;\n\n\tif (ixgbe_removed(hw->hw_addr))\n\t\treturn;\n\tpci_write_config_word(adapter->pdev, reg, value);\n}\n\nstatic void ixgbe_service_event_complete(struct ixgbe_adapter *adapter)\n{\n\tBUG_ON(!test_bit(__IXGBE_SERVICE_SCHED, &adapter->state));\n\n\t \n\tsmp_mb__before_atomic();\n\tclear_bit(__IXGBE_SERVICE_SCHED, &adapter->state);\n}\n\nstruct ixgbe_reg_info {\n\tu32 ofs;\n\tchar *name;\n};\n\nstatic const struct ixgbe_reg_info ixgbe_reg_info_tbl[] = {\n\n\t \n\t{IXGBE_CTRL, \"CTRL\"},\n\t{IXGBE_STATUS, \"STATUS\"},\n\t{IXGBE_CTRL_EXT, \"CTRL_EXT\"},\n\n\t \n\t{IXGBE_EICR, \"EICR\"},\n\n\t \n\t{IXGBE_SRRCTL(0), \"SRRCTL\"},\n\t{IXGBE_DCA_RXCTRL(0), \"DRXCTL\"},\n\t{IXGBE_RDLEN(0), \"RDLEN\"},\n\t{IXGBE_RDH(0), \"RDH\"},\n\t{IXGBE_RDT(0), \"RDT\"},\n\t{IXGBE_RXDCTL(0), \"RXDCTL\"},\n\t{IXGBE_RDBAL(0), \"RDBAL\"},\n\t{IXGBE_RDBAH(0), \"RDBAH\"},\n\n\t \n\t{IXGBE_TDBAL(0), \"TDBAL\"},\n\t{IXGBE_TDBAH(0), \"TDBAH\"},\n\t{IXGBE_TDLEN(0), \"TDLEN\"},\n\t{IXGBE_TDH(0), \"TDH\"},\n\t{IXGBE_TDT(0), \"TDT\"},\n\t{IXGBE_TXDCTL(0), \"TXDCTL\"},\n\n\t \n\t{ .name = NULL }\n};\n\n\n \nstatic void ixgbe_regdump(struct ixgbe_hw *hw, struct ixgbe_reg_info *reginfo)\n{\n\tint i;\n\tchar rname[16];\n\tu32 regs[64];\n\n\tswitch (reginfo->ofs) {\n\tcase IXGBE_SRRCTL(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_SRRCTL(i));\n\t\tbreak;\n\tcase IXGBE_DCA_RXCTRL(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_DCA_RXCTRL(i));\n\t\tbreak;\n\tcase IXGBE_RDLEN(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_RDLEN(i));\n\t\tbreak;\n\tcase IXGBE_RDH(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_RDH(i));\n\t\tbreak;\n\tcase IXGBE_RDT(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_RDT(i));\n\t\tbreak;\n\tcase IXGBE_RXDCTL(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_RXDCTL(i));\n\t\tbreak;\n\tcase IXGBE_RDBAL(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_RDBAL(i));\n\t\tbreak;\n\tcase IXGBE_RDBAH(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_RDBAH(i));\n\t\tbreak;\n\tcase IXGBE_TDBAL(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_TDBAL(i));\n\t\tbreak;\n\tcase IXGBE_TDBAH(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_TDBAH(i));\n\t\tbreak;\n\tcase IXGBE_TDLEN(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_TDLEN(i));\n\t\tbreak;\n\tcase IXGBE_TDH(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_TDH(i));\n\t\tbreak;\n\tcase IXGBE_TDT(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_TDT(i));\n\t\tbreak;\n\tcase IXGBE_TXDCTL(0):\n\t\tfor (i = 0; i < 64; i++)\n\t\t\tregs[i] = IXGBE_READ_REG(hw, IXGBE_TXDCTL(i));\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"%-15s %08x\\n\",\n\t\t\treginfo->name, IXGBE_READ_REG(hw, reginfo->ofs));\n\t\treturn;\n\t}\n\n\ti = 0;\n\twhile (i < 64) {\n\t\tint j;\n\t\tchar buf[9 * 8 + 1];\n\t\tchar *p = buf;\n\n\t\tsnprintf(rname, 16, \"%s[%d-%d]\", reginfo->name, i, i + 7);\n\t\tfor (j = 0; j < 8; j++)\n\t\t\tp += sprintf(p, \" %08x\", regs[i++]);\n\t\tpr_err(\"%-15s%s\\n\", rname, buf);\n\t}\n\n}\n\nstatic void ixgbe_print_buffer(struct ixgbe_ring *ring, int n)\n{\n\tstruct ixgbe_tx_buffer *tx_buffer;\n\n\ttx_buffer = &ring->tx_buffer_info[ring->next_to_clean];\n\tpr_info(\" %5d %5X %5X %016llX %08X %p %016llX\\n\",\n\t\tn, ring->next_to_use, ring->next_to_clean,\n\t\t(u64)dma_unmap_addr(tx_buffer, dma),\n\t\tdma_unmap_len(tx_buffer, len),\n\t\ttx_buffer->next_to_watch,\n\t\t(u64)tx_buffer->time_stamp);\n}\n\n \nstatic void ixgbe_dump(struct ixgbe_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct ixgbe_reg_info *reginfo;\n\tint n = 0;\n\tstruct ixgbe_ring *ring;\n\tstruct ixgbe_tx_buffer *tx_buffer;\n\tunion ixgbe_adv_tx_desc *tx_desc;\n\tstruct my_u0 { u64 a; u64 b; } *u0;\n\tstruct ixgbe_ring *rx_ring;\n\tunion ixgbe_adv_rx_desc *rx_desc;\n\tstruct ixgbe_rx_buffer *rx_buffer_info;\n\tint i = 0;\n\n\tif (!netif_msg_hw(adapter))\n\t\treturn;\n\n\t \n\tif (netdev) {\n\t\tdev_info(&adapter->pdev->dev, \"Net device Info\\n\");\n\t\tpr_info(\"Device Name     state            \"\n\t\t\t\"trans_start\\n\");\n\t\tpr_info(\"%-15s %016lX %016lX\\n\",\n\t\t\tnetdev->name,\n\t\t\tnetdev->state,\n\t\t\tdev_trans_start(netdev));\n\t}\n\n\t \n\tdev_info(&adapter->pdev->dev, \"Register Dump\\n\");\n\tpr_info(\" Register Name   Value\\n\");\n\tfor (reginfo = (struct ixgbe_reg_info *)ixgbe_reg_info_tbl;\n\t     reginfo->name; reginfo++) {\n\t\tixgbe_regdump(hw, reginfo);\n\t}\n\n\t \n\tif (!netdev || !netif_running(netdev))\n\t\treturn;\n\n\tdev_info(&adapter->pdev->dev, \"TX Rings Summary\\n\");\n\tpr_info(\" %s     %s              %s        %s\\n\",\n\t\t\"Queue [NTU] [NTC] [bi(ntc)->dma  ]\",\n\t\t\"leng\", \"ntw\", \"timestamp\");\n\tfor (n = 0; n < adapter->num_tx_queues; n++) {\n\t\tring = adapter->tx_ring[n];\n\t\tixgbe_print_buffer(ring, n);\n\t}\n\n\tfor (n = 0; n < adapter->num_xdp_queues; n++) {\n\t\tring = adapter->xdp_ring[n];\n\t\tixgbe_print_buffer(ring, n);\n\t}\n\n\t \n\tif (!netif_msg_tx_done(adapter))\n\t\tgoto rx_ring_summary;\n\n\tdev_info(&adapter->pdev->dev, \"TX Rings Dump\\n\");\n\n\t \n\n\tfor (n = 0; n < adapter->num_tx_queues; n++) {\n\t\tring = adapter->tx_ring[n];\n\t\tpr_info(\"------------------------------------\\n\");\n\t\tpr_info(\"TX QUEUE INDEX = %d\\n\", ring->queue_index);\n\t\tpr_info(\"------------------------------------\\n\");\n\t\tpr_info(\"%s%s    %s              %s        %s          %s\\n\",\n\t\t\t\"T [desc]     [address 63:0  ] \",\n\t\t\t\"[PlPOIdStDDt Ln] [bi->dma       ] \",\n\t\t\t\"leng\", \"ntw\", \"timestamp\", \"bi->skb\");\n\n\t\tfor (i = 0; ring->desc && (i < ring->count); i++) {\n\t\t\ttx_desc = IXGBE_TX_DESC(ring, i);\n\t\t\ttx_buffer = &ring->tx_buffer_info[i];\n\t\t\tu0 = (struct my_u0 *)tx_desc;\n\t\t\tif (dma_unmap_len(tx_buffer, len) > 0) {\n\t\t\t\tconst char *ring_desc;\n\n\t\t\t\tif (i == ring->next_to_use &&\n\t\t\t\t    i == ring->next_to_clean)\n\t\t\t\t\tring_desc = \" NTC/U\";\n\t\t\t\telse if (i == ring->next_to_use)\n\t\t\t\t\tring_desc = \" NTU\";\n\t\t\t\telse if (i == ring->next_to_clean)\n\t\t\t\t\tring_desc = \" NTC\";\n\t\t\t\telse\n\t\t\t\t\tring_desc = \"\";\n\t\t\t\tpr_info(\"T [0x%03X]    %016llX %016llX %016llX %08X %p %016llX %p%s\",\n\t\t\t\t\ti,\n\t\t\t\t\tle64_to_cpu((__force __le64)u0->a),\n\t\t\t\t\tle64_to_cpu((__force __le64)u0->b),\n\t\t\t\t\t(u64)dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\tdma_unmap_len(tx_buffer, len),\n\t\t\t\t\ttx_buffer->next_to_watch,\n\t\t\t\t\t(u64)tx_buffer->time_stamp,\n\t\t\t\t\ttx_buffer->skb,\n\t\t\t\t\tring_desc);\n\n\t\t\t\tif (netif_msg_pktdata(adapter) &&\n\t\t\t\t    tx_buffer->skb)\n\t\t\t\t\tprint_hex_dump(KERN_INFO, \"\",\n\t\t\t\t\t\tDUMP_PREFIX_ADDRESS, 16, 1,\n\t\t\t\t\t\ttx_buffer->skb->data,\n\t\t\t\t\t\tdma_unmap_len(tx_buffer, len),\n\t\t\t\t\t\ttrue);\n\t\t\t}\n\t\t}\n\t}\n\n\t \nrx_ring_summary:\n\tdev_info(&adapter->pdev->dev, \"RX Rings Summary\\n\");\n\tpr_info(\"Queue [NTU] [NTC]\\n\");\n\tfor (n = 0; n < adapter->num_rx_queues; n++) {\n\t\trx_ring = adapter->rx_ring[n];\n\t\tpr_info(\"%5d %5X %5X\\n\",\n\t\t\tn, rx_ring->next_to_use, rx_ring->next_to_clean);\n\t}\n\n\t \n\tif (!netif_msg_rx_status(adapter))\n\t\treturn;\n\n\tdev_info(&adapter->pdev->dev, \"RX Rings Dump\\n\");\n\n\t \n\n\tfor (n = 0; n < adapter->num_rx_queues; n++) {\n\t\trx_ring = adapter->rx_ring[n];\n\t\tpr_info(\"------------------------------------\\n\");\n\t\tpr_info(\"RX QUEUE INDEX = %d\\n\", rx_ring->queue_index);\n\t\tpr_info(\"------------------------------------\\n\");\n\t\tpr_info(\"%s%s%s\\n\",\n\t\t\t\"R  [desc]      [ PktBuf     A0] \",\n\t\t\t\"[  HeadBuf   DD] [bi->dma       ] [bi->skb       ] \",\n\t\t\t\"<-- Adv Rx Read format\");\n\t\tpr_info(\"%s%s%s\\n\",\n\t\t\t\"RWB[desc]      [PcsmIpSHl PtRs] \",\n\t\t\t\"[vl er S cks ln] ---------------- [bi->skb       ] \",\n\t\t\t\"<-- Adv Rx Write-Back format\");\n\n\t\tfor (i = 0; i < rx_ring->count; i++) {\n\t\t\tconst char *ring_desc;\n\n\t\t\tif (i == rx_ring->next_to_use)\n\t\t\t\tring_desc = \" NTU\";\n\t\t\telse if (i == rx_ring->next_to_clean)\n\t\t\t\tring_desc = \" NTC\";\n\t\t\telse\n\t\t\t\tring_desc = \"\";\n\n\t\t\trx_buffer_info = &rx_ring->rx_buffer_info[i];\n\t\t\trx_desc = IXGBE_RX_DESC(rx_ring, i);\n\t\t\tu0 = (struct my_u0 *)rx_desc;\n\t\t\tif (rx_desc->wb.upper.length) {\n\t\t\t\t \n\t\t\t\tpr_info(\"RWB[0x%03X]     %016llX %016llX ---------------- %p%s\\n\",\n\t\t\t\t\ti,\n\t\t\t\t\tle64_to_cpu((__force __le64)u0->a),\n\t\t\t\t\tle64_to_cpu((__force __le64)u0->b),\n\t\t\t\t\trx_buffer_info->skb,\n\t\t\t\t\tring_desc);\n\t\t\t} else {\n\t\t\t\tpr_info(\"R  [0x%03X]     %016llX %016llX %016llX %p%s\\n\",\n\t\t\t\t\ti,\n\t\t\t\t\tle64_to_cpu((__force __le64)u0->a),\n\t\t\t\t\tle64_to_cpu((__force __le64)u0->b),\n\t\t\t\t\t(u64)rx_buffer_info->dma,\n\t\t\t\t\trx_buffer_info->skb,\n\t\t\t\t\tring_desc);\n\n\t\t\t\tif (netif_msg_pktdata(adapter) &&\n\t\t\t\t    rx_buffer_info->dma) {\n\t\t\t\t\tprint_hex_dump(KERN_INFO, \"\",\n\t\t\t\t\t   DUMP_PREFIX_ADDRESS, 16, 1,\n\t\t\t\t\t   page_address(rx_buffer_info->page) +\n\t\t\t\t\t\t    rx_buffer_info->page_offset,\n\t\t\t\t\t   ixgbe_rx_bufsz(rx_ring), true);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void ixgbe_release_hw_control(struct ixgbe_adapter *adapter)\n{\n\tu32 ctrl_ext;\n\n\t \n\tctrl_ext = IXGBE_READ_REG(&adapter->hw, IXGBE_CTRL_EXT);\n\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_CTRL_EXT,\n\t\t\tctrl_ext & ~IXGBE_CTRL_EXT_DRV_LOAD);\n}\n\nstatic void ixgbe_get_hw_control(struct ixgbe_adapter *adapter)\n{\n\tu32 ctrl_ext;\n\n\t \n\tctrl_ext = IXGBE_READ_REG(&adapter->hw, IXGBE_CTRL_EXT);\n\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_CTRL_EXT,\n\t\t\tctrl_ext | IXGBE_CTRL_EXT_DRV_LOAD);\n}\n\n \nstatic void ixgbe_set_ivar(struct ixgbe_adapter *adapter, s8 direction,\n\t\t\t   u8 queue, u8 msix_vector)\n{\n\tu32 ivar, index;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tmsix_vector |= IXGBE_IVAR_ALLOC_VAL;\n\t\tif (direction == -1)\n\t\t\tdirection = 0;\n\t\tindex = (((direction * 64) + queue) >> 2) & 0x1F;\n\t\tivar = IXGBE_READ_REG(hw, IXGBE_IVAR(index));\n\t\tivar &= ~(0xFF << (8 * (queue & 0x3)));\n\t\tivar |= (msix_vector << (8 * (queue & 0x3)));\n\t\tIXGBE_WRITE_REG(hw, IXGBE_IVAR(index), ivar);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tif (direction == -1) {\n\t\t\t \n\t\t\tmsix_vector |= IXGBE_IVAR_ALLOC_VAL;\n\t\t\tindex = ((queue & 1) * 8);\n\t\t\tivar = IXGBE_READ_REG(&adapter->hw, IXGBE_IVAR_MISC);\n\t\t\tivar &= ~(0xFF << index);\n\t\t\tivar |= (msix_vector << index);\n\t\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_IVAR_MISC, ivar);\n\t\t\tbreak;\n\t\t} else {\n\t\t\t \n\t\t\tmsix_vector |= IXGBE_IVAR_ALLOC_VAL;\n\t\t\tindex = ((16 * (queue & 1)) + (8 * direction));\n\t\t\tivar = IXGBE_READ_REG(hw, IXGBE_IVAR(queue >> 1));\n\t\t\tivar &= ~(0xFF << index);\n\t\t\tivar |= (msix_vector << index);\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_IVAR(queue >> 1), ivar);\n\t\t\tbreak;\n\t\t}\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nvoid ixgbe_irq_rearm_queues(struct ixgbe_adapter *adapter,\n\t\t\t    u64 qmask)\n{\n\tu32 mask;\n\n\tswitch (adapter->hw.mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tmask = (IXGBE_EIMS_RTX_QUEUE & qmask);\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EICS, mask);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tmask = (qmask & 0xFFFFFFFF);\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EICS_EX(0), mask);\n\t\tmask = (qmask >> 32);\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EICS_EX(1), mask);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void ixgbe_update_xoff_rx_lfc(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct ixgbe_hw_stats *hwstats = &adapter->stats;\n\tint i;\n\tu32 data;\n\n\tif ((hw->fc.current_mode != ixgbe_fc_full) &&\n\t    (hw->fc.current_mode != ixgbe_fc_rx_pause))\n\t\treturn;\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tdata = IXGBE_READ_REG(hw, IXGBE_LXOFFRXC);\n\t\tbreak;\n\tdefault:\n\t\tdata = IXGBE_READ_REG(hw, IXGBE_LXOFFRXCNT);\n\t}\n\thwstats->lxoffrxc += data;\n\n\t \n\tif (!data)\n\t\treturn;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tclear_bit(__IXGBE_HANG_CHECK_ARMED,\n\t\t\t  &adapter->tx_ring[i]->state);\n\n\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\tclear_bit(__IXGBE_HANG_CHECK_ARMED,\n\t\t\t  &adapter->xdp_ring[i]->state);\n}\n\nstatic void ixgbe_update_xoff_received(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct ixgbe_hw_stats *hwstats = &adapter->stats;\n\tu32 xoff[8] = {0};\n\tu8 tc;\n\tint i;\n\tbool pfc_en = adapter->dcb_cfg.pfc_mode_enable;\n\n\tif (adapter->ixgbe_ieee_pfc)\n\t\tpfc_en |= !!(adapter->ixgbe_ieee_pfc->pfc_en);\n\n\tif (!(adapter->flags & IXGBE_FLAG_DCB_ENABLED) || !pfc_en) {\n\t\tixgbe_update_xoff_rx_lfc(adapter);\n\t\treturn;\n\t}\n\n\t \n\tfor (i = 0; i < MAX_TX_PACKET_BUFFERS; i++) {\n\t\tu32 pxoffrxc;\n\n\t\tswitch (hw->mac.type) {\n\t\tcase ixgbe_mac_82598EB:\n\t\t\tpxoffrxc = IXGBE_READ_REG(hw, IXGBE_PXOFFRXC(i));\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpxoffrxc = IXGBE_READ_REG(hw, IXGBE_PXOFFRXCNT(i));\n\t\t}\n\t\thwstats->pxoffrxc[i] += pxoffrxc;\n\t\t \n\t\ttc = netdev_get_prio_tc_map(adapter->netdev, i);\n\t\txoff[tc] += pxoffrxc;\n\t}\n\n\t \n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct ixgbe_ring *tx_ring = adapter->tx_ring[i];\n\n\t\ttc = tx_ring->dcb_tc;\n\t\tif (xoff[tc])\n\t\t\tclear_bit(__IXGBE_HANG_CHECK_ARMED, &tx_ring->state);\n\t}\n\n\tfor (i = 0; i < adapter->num_xdp_queues; i++) {\n\t\tstruct ixgbe_ring *xdp_ring = adapter->xdp_ring[i];\n\n\t\ttc = xdp_ring->dcb_tc;\n\t\tif (xoff[tc])\n\t\t\tclear_bit(__IXGBE_HANG_CHECK_ARMED, &xdp_ring->state);\n\t}\n}\n\nstatic u64 ixgbe_get_tx_completed(struct ixgbe_ring *ring)\n{\n\treturn ring->stats.packets;\n}\n\nstatic u64 ixgbe_get_tx_pending(struct ixgbe_ring *ring)\n{\n\tunsigned int head, tail;\n\n\thead = ring->next_to_clean;\n\ttail = ring->next_to_use;\n\n\treturn ((head <= tail) ? tail : tail + ring->count) - head;\n}\n\nstatic inline bool ixgbe_check_tx_hang(struct ixgbe_ring *tx_ring)\n{\n\tu32 tx_done = ixgbe_get_tx_completed(tx_ring);\n\tu32 tx_done_old = tx_ring->tx_stats.tx_done_old;\n\tu32 tx_pending = ixgbe_get_tx_pending(tx_ring);\n\n\tclear_check_for_tx_hang(tx_ring);\n\n\t \n\tif (tx_done_old == tx_done && tx_pending)\n\t\t \n\t\treturn test_and_set_bit(__IXGBE_HANG_CHECK_ARMED,\n\t\t\t\t\t&tx_ring->state);\n\t \n\ttx_ring->tx_stats.tx_done_old = tx_done;\n\t \n\tclear_bit(__IXGBE_HANG_CHECK_ARMED, &tx_ring->state);\n\n\treturn false;\n}\n\n \nstatic void ixgbe_tx_timeout_reset(struct ixgbe_adapter *adapter)\n{\n\n\t \n\tif (!test_bit(__IXGBE_DOWN, &adapter->state)) {\n\t\tset_bit(__IXGBE_RESET_REQUESTED, &adapter->state);\n\t\te_warn(drv, \"initiating reset due to tx timeout\\n\");\n\t\tixgbe_service_event_schedule(adapter);\n\t}\n}\n\n \nstatic int ixgbe_tx_maxrate(struct net_device *netdev,\n\t\t\t    int queue_index, u32 maxrate)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 bcnrc_val = ixgbe_link_mbps(adapter);\n\n\tif (!maxrate)\n\t\treturn 0;\n\n\t \n\tbcnrc_val <<= IXGBE_RTTBCNRC_RF_INT_SHIFT;\n\tbcnrc_val /= maxrate;\n\n\t \n\tbcnrc_val &= IXGBE_RTTBCNRC_RF_INT_MASK |\n\tIXGBE_RTTBCNRC_RF_DEC_MASK;\n\n\t \n\tbcnrc_val |= IXGBE_RTTBCNRC_RS_ENA;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_RTTDQSEL, queue_index);\n\tIXGBE_WRITE_REG(hw, IXGBE_RTTBCNRC, bcnrc_val);\n\n\treturn 0;\n}\n\n \nstatic bool ixgbe_clean_tx_irq(struct ixgbe_q_vector *q_vector,\n\t\t\t       struct ixgbe_ring *tx_ring, int napi_budget)\n{\n\tstruct ixgbe_adapter *adapter = q_vector->adapter;\n\tstruct ixgbe_tx_buffer *tx_buffer;\n\tunion ixgbe_adv_tx_desc *tx_desc;\n\tunsigned int total_bytes = 0, total_packets = 0, total_ipsec = 0;\n\tunsigned int budget = q_vector->tx.work_limit;\n\tunsigned int i = tx_ring->next_to_clean;\n\tstruct netdev_queue *txq;\n\n\tif (test_bit(__IXGBE_DOWN, &adapter->state))\n\t\treturn true;\n\n\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\ttx_desc = IXGBE_TX_DESC(tx_ring, i);\n\ti -= tx_ring->count;\n\n\tdo {\n\t\tunion ixgbe_adv_tx_desc *eop_desc = tx_buffer->next_to_watch;\n\n\t\t \n\t\tif (!eop_desc)\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\t \n\t\tif (!(eop_desc->wb.status & cpu_to_le32(IXGBE_TXD_STAT_DD)))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buffer->next_to_watch = NULL;\n\n\t\t \n\t\ttotal_bytes += tx_buffer->bytecount;\n\t\ttotal_packets += tx_buffer->gso_segs;\n\t\tif (tx_buffer->tx_flags & IXGBE_TX_FLAGS_IPSEC)\n\t\t\ttotal_ipsec++;\n\n\t\t \n\t\tif (ring_is_xdp(tx_ring))\n\t\t\txdp_return_frame(tx_buffer->xdpf);\n\t\telse\n\t\t\tnapi_consume_skb(tx_buffer->skb, napi_budget);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(!i)) {\n\t\t\t\ti -= tx_ring->count;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = IXGBE_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len)) {\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\ttx_buffer++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\ttx_desc = IXGBE_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\t \n\t\tprefetch(tx_desc);\n\n\t\t \n\t\tbudget--;\n\t} while (likely(budget));\n\n\ti += tx_ring->count;\n\ttx_ring->next_to_clean = i;\n\tu64_stats_update_begin(&tx_ring->syncp);\n\ttx_ring->stats.bytes += total_bytes;\n\ttx_ring->stats.packets += total_packets;\n\tu64_stats_update_end(&tx_ring->syncp);\n\tq_vector->tx.total_bytes += total_bytes;\n\tq_vector->tx.total_packets += total_packets;\n\tadapter->tx_ipsec += total_ipsec;\n\n\tif (check_for_tx_hang(tx_ring) && ixgbe_check_tx_hang(tx_ring)) {\n\t\t \n\t\tstruct ixgbe_hw *hw = &adapter->hw;\n\t\te_err(drv, \"Detected Tx Unit Hang %s\\n\"\n\t\t\t\"  Tx Queue             <%d>\\n\"\n\t\t\t\"  TDH, TDT             <%x>, <%x>\\n\"\n\t\t\t\"  next_to_use          <%x>\\n\"\n\t\t\t\"  next_to_clean        <%x>\\n\"\n\t\t\t\"tx_buffer_info[next_to_clean]\\n\"\n\t\t\t\"  time_stamp           <%lx>\\n\"\n\t\t\t\"  jiffies              <%lx>\\n\",\n\t\t\tring_is_xdp(tx_ring) ? \"(XDP)\" : \"\",\n\t\t\ttx_ring->queue_index,\n\t\t\tIXGBE_READ_REG(hw, IXGBE_TDH(tx_ring->reg_idx)),\n\t\t\tIXGBE_READ_REG(hw, IXGBE_TDT(tx_ring->reg_idx)),\n\t\t\ttx_ring->next_to_use, i,\n\t\t\ttx_ring->tx_buffer_info[i].time_stamp, jiffies);\n\n\t\tif (!ring_is_xdp(tx_ring))\n\t\t\tnetif_stop_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\n\t\te_info(probe,\n\t\t       \"tx hang %d detected on queue %d, resetting adapter\\n\",\n\t\t\tadapter->tx_timeout_count + 1, tx_ring->queue_index);\n\n\t\t \n\t\tixgbe_tx_timeout_reset(adapter);\n\n\t\t \n\t\treturn true;\n\t}\n\n\tif (ring_is_xdp(tx_ring))\n\t\treturn !!budget;\n\n#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)\n\ttxq = netdev_get_tx_queue(tx_ring->netdev, tx_ring->queue_index);\n\tif (!__netif_txq_completed_wake(txq, total_packets, total_bytes,\n\t\t\t\t\tixgbe_desc_unused(tx_ring),\n\t\t\t\t\tTX_WAKE_THRESHOLD,\n\t\t\t\t\t!netif_carrier_ok(tx_ring->netdev) ||\n\t\t\t\t\ttest_bit(__IXGBE_DOWN, &adapter->state)))\n\t\t++tx_ring->tx_stats.restart_queue;\n\n\treturn !!budget;\n}\n\n#ifdef CONFIG_IXGBE_DCA\nstatic void ixgbe_update_tx_dca(struct ixgbe_adapter *adapter,\n\t\t\t\tstruct ixgbe_ring *tx_ring,\n\t\t\t\tint cpu)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 txctrl = 0;\n\tu16 reg_offset;\n\n\tif (adapter->flags & IXGBE_FLAG_DCA_ENABLED)\n\t\ttxctrl = dca3_get_tag(tx_ring->dev, cpu);\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\treg_offset = IXGBE_DCA_TXCTRL(tx_ring->reg_idx);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\t\treg_offset = IXGBE_DCA_TXCTRL_82599(tx_ring->reg_idx);\n\t\ttxctrl <<= IXGBE_DCA_TXCTRL_CPUID_SHIFT_82599;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn;\n\t}\n\n\t \n\ttxctrl |= IXGBE_DCA_TXCTRL_DESC_RRO_EN |\n\t\t  IXGBE_DCA_TXCTRL_DATA_RRO_EN |\n\t\t  IXGBE_DCA_TXCTRL_DESC_DCA_EN;\n\n\tIXGBE_WRITE_REG(hw, reg_offset, txctrl);\n}\n\nstatic void ixgbe_update_rx_dca(struct ixgbe_adapter *adapter,\n\t\t\t\tstruct ixgbe_ring *rx_ring,\n\t\t\t\tint cpu)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 rxctrl = 0;\n\tu8 reg_idx = rx_ring->reg_idx;\n\n\tif (adapter->flags & IXGBE_FLAG_DCA_ENABLED)\n\t\trxctrl = dca3_get_tag(rx_ring->dev, cpu);\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\t\trxctrl <<= IXGBE_DCA_RXCTRL_CPUID_SHIFT_82599;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\trxctrl |= IXGBE_DCA_RXCTRL_DESC_RRO_EN |\n\t\t  IXGBE_DCA_RXCTRL_DATA_DCA_EN |\n\t\t  IXGBE_DCA_RXCTRL_DESC_DCA_EN;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_DCA_RXCTRL(reg_idx), rxctrl);\n}\n\nstatic void ixgbe_update_dca(struct ixgbe_q_vector *q_vector)\n{\n\tstruct ixgbe_adapter *adapter = q_vector->adapter;\n\tstruct ixgbe_ring *ring;\n\tint cpu = get_cpu();\n\n\tif (q_vector->cpu == cpu)\n\t\tgoto out_no_update;\n\n\tixgbe_for_each_ring(ring, q_vector->tx)\n\t\tixgbe_update_tx_dca(adapter, ring, cpu);\n\n\tixgbe_for_each_ring(ring, q_vector->rx)\n\t\tixgbe_update_rx_dca(adapter, ring, cpu);\n\n\tq_vector->cpu = cpu;\nout_no_update:\n\tput_cpu();\n}\n\nstatic void ixgbe_setup_dca(struct ixgbe_adapter *adapter)\n{\n\tint i;\n\n\t \n\tif (adapter->flags & IXGBE_FLAG_DCA_ENABLED)\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_DCA_CTRL,\n\t\t\t\tIXGBE_DCA_CTRL_DCA_MODE_CB2);\n\telse\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_DCA_CTRL,\n\t\t\t\tIXGBE_DCA_CTRL_DCA_DISABLE);\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++) {\n\t\tadapter->q_vector[i]->cpu = -1;\n\t\tixgbe_update_dca(adapter->q_vector[i]);\n\t}\n}\n\nstatic int __ixgbe_notify_dca(struct device *dev, void *data)\n{\n\tstruct ixgbe_adapter *adapter = dev_get_drvdata(dev);\n\tunsigned long event = *(unsigned long *)data;\n\n\tif (!(adapter->flags & IXGBE_FLAG_DCA_CAPABLE))\n\t\treturn 0;\n\n\tswitch (event) {\n\tcase DCA_PROVIDER_ADD:\n\t\t \n\t\tif (adapter->flags & IXGBE_FLAG_DCA_ENABLED)\n\t\t\tbreak;\n\t\tif (dca_add_requester(dev) == 0) {\n\t\t\tadapter->flags |= IXGBE_FLAG_DCA_ENABLED;\n\t\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_DCA_CTRL,\n\t\t\t\t\tIXGBE_DCA_CTRL_DCA_MODE_CB2);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;  \n\tcase DCA_PROVIDER_REMOVE:\n\t\tif (adapter->flags & IXGBE_FLAG_DCA_ENABLED) {\n\t\t\tdca_remove_requester(dev);\n\t\t\tadapter->flags &= ~IXGBE_FLAG_DCA_ENABLED;\n\t\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_DCA_CTRL,\n\t\t\t\t\tIXGBE_DCA_CTRL_DCA_DISABLE);\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n#endif  \n\n#define IXGBE_RSS_L4_TYPES_MASK \\\n\t((1ul << IXGBE_RXDADV_RSSTYPE_IPV4_TCP) | \\\n\t (1ul << IXGBE_RXDADV_RSSTYPE_IPV4_UDP) | \\\n\t (1ul << IXGBE_RXDADV_RSSTYPE_IPV6_TCP) | \\\n\t (1ul << IXGBE_RXDADV_RSSTYPE_IPV6_UDP))\n\nstatic inline void ixgbe_rx_hash(struct ixgbe_ring *ring,\n\t\t\t\t union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t\t struct sk_buff *skb)\n{\n\tu16 rss_type;\n\n\tif (!(ring->netdev->features & NETIF_F_RXHASH))\n\t\treturn;\n\n\trss_type = le16_to_cpu(rx_desc->wb.lower.lo_dword.hs_rss.pkt_info) &\n\t\t   IXGBE_RXDADV_RSSTYPE_MASK;\n\n\tif (!rss_type)\n\t\treturn;\n\n\tskb_set_hash(skb, le32_to_cpu(rx_desc->wb.lower.hi_dword.rss),\n\t\t     (IXGBE_RSS_L4_TYPES_MASK & (1ul << rss_type)) ?\n\t\t     PKT_HASH_TYPE_L4 : PKT_HASH_TYPE_L3);\n}\n\n#ifdef IXGBE_FCOE\n \nstatic inline bool ixgbe_rx_is_fcoe(struct ixgbe_ring *ring,\n\t\t\t\t    union ixgbe_adv_rx_desc *rx_desc)\n{\n\t__le16 pkt_info = rx_desc->wb.lower.lo_dword.hs_rss.pkt_info;\n\n\treturn test_bit(__IXGBE_RX_FCOE, &ring->state) &&\n\t       ((pkt_info & cpu_to_le16(IXGBE_RXDADV_PKTTYPE_ETQF_MASK)) ==\n\t\t(cpu_to_le16(IXGBE_ETQF_FILTER_FCOE <<\n\t\t\t     IXGBE_RXDADV_PKTTYPE_ETQF_SHIFT)));\n}\n\n#endif  \n \nstatic inline void ixgbe_rx_checksum(struct ixgbe_ring *ring,\n\t\t\t\t     union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t\t     struct sk_buff *skb)\n{\n\t__le16 pkt_info = rx_desc->wb.lower.lo_dword.hs_rss.pkt_info;\n\tbool encap_pkt = false;\n\n\tskb_checksum_none_assert(skb);\n\n\t \n\tif (!(ring->netdev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\t \n\tif (pkt_info & cpu_to_le16(IXGBE_RXDADV_PKTTYPE_VXLAN)) {\n\t\tencap_pkt = true;\n\t\tskb->encapsulation = 1;\n\t}\n\n\t \n\tif (ixgbe_test_staterr(rx_desc, IXGBE_RXD_STAT_IPCS) &&\n\t    ixgbe_test_staterr(rx_desc, IXGBE_RXDADV_ERR_IPE)) {\n\t\tring->rx_stats.csum_err++;\n\t\treturn;\n\t}\n\n\tif (!ixgbe_test_staterr(rx_desc, IXGBE_RXD_STAT_L4CS))\n\t\treturn;\n\n\tif (ixgbe_test_staterr(rx_desc, IXGBE_RXDADV_ERR_TCPE)) {\n\t\t \n\t\tif ((pkt_info & cpu_to_le16(IXGBE_RXDADV_PKTTYPE_UDP)) &&\n\t\t    test_bit(__IXGBE_RX_CSUM_UDP_ZERO_ERR, &ring->state))\n\t\t\treturn;\n\n\t\tring->rx_stats.csum_err++;\n\t\treturn;\n\t}\n\n\t \n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tif (encap_pkt) {\n\t\tif (!ixgbe_test_staterr(rx_desc, IXGBE_RXD_STAT_OUTERIPCS))\n\t\t\treturn;\n\n\t\tif (ixgbe_test_staterr(rx_desc, IXGBE_RXDADV_ERR_OUTERIPER)) {\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t\treturn;\n\t\t}\n\t\t \n\t\tskb->csum_level = 1;\n\t}\n}\n\nstatic unsigned int ixgbe_rx_offset(struct ixgbe_ring *rx_ring)\n{\n\treturn ring_uses_build_skb(rx_ring) ? IXGBE_SKB_PAD : 0;\n}\n\nstatic bool ixgbe_alloc_mapped_page(struct ixgbe_ring *rx_ring,\n\t\t\t\t    struct ixgbe_rx_buffer *bi)\n{\n\tstruct page *page = bi->page;\n\tdma_addr_t dma;\n\n\t \n\tif (likely(page))\n\t\treturn true;\n\n\t \n\tpage = dev_alloc_pages(ixgbe_rx_pg_order(rx_ring));\n\tif (unlikely(!page)) {\n\t\trx_ring->rx_stats.alloc_rx_page_failed++;\n\t\treturn false;\n\t}\n\n\t \n\tdma = dma_map_page_attrs(rx_ring->dev, page, 0,\n\t\t\t\t ixgbe_rx_pg_size(rx_ring),\n\t\t\t\t DMA_FROM_DEVICE,\n\t\t\t\t IXGBE_RX_DMA_ATTR);\n\n\t \n\tif (dma_mapping_error(rx_ring->dev, dma)) {\n\t\t__free_pages(page, ixgbe_rx_pg_order(rx_ring));\n\n\t\trx_ring->rx_stats.alloc_rx_page_failed++;\n\t\treturn false;\n\t}\n\n\tbi->dma = dma;\n\tbi->page = page;\n\tbi->page_offset = rx_ring->rx_offset;\n\tpage_ref_add(page, USHRT_MAX - 1);\n\tbi->pagecnt_bias = USHRT_MAX;\n\trx_ring->rx_stats.alloc_rx_page++;\n\n\treturn true;\n}\n\n \nvoid ixgbe_alloc_rx_buffers(struct ixgbe_ring *rx_ring, u16 cleaned_count)\n{\n\tunion ixgbe_adv_rx_desc *rx_desc;\n\tstruct ixgbe_rx_buffer *bi;\n\tu16 i = rx_ring->next_to_use;\n\tu16 bufsz;\n\n\t \n\tif (!cleaned_count)\n\t\treturn;\n\n\trx_desc = IXGBE_RX_DESC(rx_ring, i);\n\tbi = &rx_ring->rx_buffer_info[i];\n\ti -= rx_ring->count;\n\n\tbufsz = ixgbe_rx_bufsz(rx_ring);\n\n\tdo {\n\t\tif (!ixgbe_alloc_mapped_page(rx_ring, bi))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_sync_single_range_for_device(rx_ring->dev, bi->dma,\n\t\t\t\t\t\t bi->page_offset, bufsz,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t \n\t\trx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);\n\n\t\trx_desc++;\n\t\tbi++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\trx_desc = IXGBE_RX_DESC(rx_ring, 0);\n\t\t\tbi = rx_ring->rx_buffer_info;\n\t\t\ti -= rx_ring->count;\n\t\t}\n\n\t\t \n\t\trx_desc->wb.upper.length = 0;\n\n\t\tcleaned_count--;\n\t} while (cleaned_count);\n\n\ti += rx_ring->count;\n\n\tif (rx_ring->next_to_use != i) {\n\t\trx_ring->next_to_use = i;\n\n\t\t \n\t\trx_ring->next_to_alloc = i;\n\n\t\t \n\t\twmb();\n\t\twritel(i, rx_ring->tail);\n\t}\n}\n\nstatic void ixgbe_set_rsc_gso_size(struct ixgbe_ring *ring,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tu16 hdr_len = skb_headlen(skb);\n\n\t \n\tskb_shinfo(skb)->gso_size = DIV_ROUND_UP((skb->len - hdr_len),\n\t\t\t\t\t\t IXGBE_CB(skb)->append_cnt);\n\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\n}\n\nstatic void ixgbe_update_rsc_stats(struct ixgbe_ring *rx_ring,\n\t\t\t\t   struct sk_buff *skb)\n{\n\t \n\tif (!IXGBE_CB(skb)->append_cnt)\n\t\treturn;\n\n\trx_ring->rx_stats.rsc_count += IXGBE_CB(skb)->append_cnt;\n\trx_ring->rx_stats.rsc_flush++;\n\n\tixgbe_set_rsc_gso_size(rx_ring, skb);\n\n\t \n\tIXGBE_CB(skb)->append_cnt = 0;\n}\n\n \nvoid ixgbe_process_skb_fields(struct ixgbe_ring *rx_ring,\n\t\t\t      union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t      struct sk_buff *skb)\n{\n\tstruct net_device *dev = rx_ring->netdev;\n\tu32 flags = rx_ring->q_vector->adapter->flags;\n\n\tixgbe_update_rsc_stats(rx_ring, skb);\n\n\tixgbe_rx_hash(rx_ring, rx_desc, skb);\n\n\tixgbe_rx_checksum(rx_ring, rx_desc, skb);\n\n\tif (unlikely(flags & IXGBE_FLAG_RX_HWTSTAMP_ENABLED))\n\t\tixgbe_ptp_rx_hwtstamp(rx_ring, rx_desc, skb);\n\n\tif ((dev->features & NETIF_F_HW_VLAN_CTAG_RX) &&\n\t    ixgbe_test_staterr(rx_desc, IXGBE_RXD_STAT_VP)) {\n\t\tu16 vid = le16_to_cpu(rx_desc->wb.upper.vlan);\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);\n\t}\n\n\tif (ixgbe_test_staterr(rx_desc, IXGBE_RXDADV_STAT_SECP))\n\t\tixgbe_ipsec_rx(rx_ring, rx_desc, skb);\n\n\t \n\tif (netif_is_ixgbe(dev))\n\t\tskb_record_rx_queue(skb, rx_ring->queue_index);\n\telse\n\t\tmacvlan_count_rx(netdev_priv(dev), skb->len + ETH_HLEN, true,\n\t\t\t\t false);\n\n\tskb->protocol = eth_type_trans(skb, dev);\n}\n\nvoid ixgbe_rx_skb(struct ixgbe_q_vector *q_vector,\n\t\t  struct sk_buff *skb)\n{\n\tnapi_gro_receive(&q_vector->napi, skb);\n}\n\n \nstatic bool ixgbe_is_non_eop(struct ixgbe_ring *rx_ring,\n\t\t\t     union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t     struct sk_buff *skb)\n{\n\tu32 ntc = rx_ring->next_to_clean + 1;\n\n\t \n\tntc = (ntc < rx_ring->count) ? ntc : 0;\n\trx_ring->next_to_clean = ntc;\n\n\tprefetch(IXGBE_RX_DESC(rx_ring, ntc));\n\n\t \n\tif (ring_is_rsc_enabled(rx_ring)) {\n\t\t__le32 rsc_enabled = rx_desc->wb.lower.lo_dword.data &\n\t\t\t\t     cpu_to_le32(IXGBE_RXDADV_RSCCNT_MASK);\n\n\t\tif (unlikely(rsc_enabled)) {\n\t\t\tu32 rsc_cnt = le32_to_cpu(rsc_enabled);\n\n\t\t\trsc_cnt >>= IXGBE_RXDADV_RSCCNT_SHIFT;\n\t\t\tIXGBE_CB(skb)->append_cnt += rsc_cnt - 1;\n\n\t\t\t \n\t\t\tntc = le32_to_cpu(rx_desc->wb.upper.status_error);\n\t\t\tntc &= IXGBE_RXDADV_NEXTP_MASK;\n\t\t\tntc >>= IXGBE_RXDADV_NEXTP_SHIFT;\n\t\t}\n\t}\n\n\t \n\tif (likely(ixgbe_test_staterr(rx_desc, IXGBE_RXD_STAT_EOP)))\n\t\treturn false;\n\n\t \n\trx_ring->rx_buffer_info[ntc].skb = skb;\n\trx_ring->rx_stats.non_eop_descs++;\n\n\treturn true;\n}\n\n \nstatic void ixgbe_pull_tail(struct ixgbe_ring *rx_ring,\n\t\t\t    struct sk_buff *skb)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[0];\n\tunsigned char *va;\n\tunsigned int pull_len;\n\n\t \n\tva = skb_frag_address(frag);\n\n\t \n\tpull_len = eth_get_headlen(skb->dev, va, IXGBE_RX_HDR_SIZE);\n\n\t \n\tskb_copy_to_linear_data(skb, va, ALIGN(pull_len, sizeof(long)));\n\n\t \n\tskb_frag_size_sub(frag, pull_len);\n\tskb_frag_off_add(frag, pull_len);\n\tskb->data_len -= pull_len;\n\tskb->tail += pull_len;\n}\n\n \nstatic void ixgbe_dma_sync_frag(struct ixgbe_ring *rx_ring,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tif (ring_uses_build_skb(rx_ring)) {\n\t\tunsigned long mask = (unsigned long)ixgbe_rx_pg_size(rx_ring) - 1;\n\t\tunsigned long offset = (unsigned long)(skb->data) & mask;\n\n\t\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t\t      IXGBE_CB(skb)->dma,\n\t\t\t\t\t      offset,\n\t\t\t\t\t      skb_headlen(skb),\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\t} else {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[0];\n\n\t\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t\t      IXGBE_CB(skb)->dma,\n\t\t\t\t\t      skb_frag_off(frag),\n\t\t\t\t\t      skb_frag_size(frag),\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\t}\n\n\t \n\tif (unlikely(IXGBE_CB(skb)->page_released)) {\n\t\tdma_unmap_page_attrs(rx_ring->dev, IXGBE_CB(skb)->dma,\n\t\t\t\t     ixgbe_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t     IXGBE_RX_DMA_ATTR);\n\t}\n}\n\n \nbool ixgbe_cleanup_headers(struct ixgbe_ring *rx_ring,\n\t\t\t   union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t   struct sk_buff *skb)\n{\n\tstruct net_device *netdev = rx_ring->netdev;\n\n\t \n\tif (IS_ERR(skb))\n\t\treturn true;\n\n\t \n\tif (!netdev ||\n\t    (unlikely(ixgbe_test_staterr(rx_desc,\n\t\t\t\t\t IXGBE_RXDADV_ERR_FRAME_ERR_MASK) &&\n\t     !(netdev->features & NETIF_F_RXALL)))) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn true;\n\t}\n\n\t \n\tif (!skb_headlen(skb))\n\t\tixgbe_pull_tail(rx_ring, skb);\n\n#ifdef IXGBE_FCOE\n\t \n\tif (ixgbe_rx_is_fcoe(rx_ring, rx_desc))\n\t\treturn false;\n\n#endif\n\t \n\tif (eth_skb_pad(skb))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void ixgbe_reuse_rx_page(struct ixgbe_ring *rx_ring,\n\t\t\t\tstruct ixgbe_rx_buffer *old_buff)\n{\n\tstruct ixgbe_rx_buffer *new_buff;\n\tu16 nta = rx_ring->next_to_alloc;\n\n\tnew_buff = &rx_ring->rx_buffer_info[nta];\n\n\t \n\tnta++;\n\trx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;\n\n\t \n\tnew_buff->dma\t\t= old_buff->dma;\n\tnew_buff->page\t\t= old_buff->page;\n\tnew_buff->page_offset\t= old_buff->page_offset;\n\tnew_buff->pagecnt_bias\t= old_buff->pagecnt_bias;\n}\n\nstatic bool ixgbe_can_reuse_rx_page(struct ixgbe_rx_buffer *rx_buffer,\n\t\t\t\t    int rx_buffer_pgcnt)\n{\n\tunsigned int pagecnt_bias = rx_buffer->pagecnt_bias;\n\tstruct page *page = rx_buffer->page;\n\n\t \n\tif (!dev_page_is_reusable(page))\n\t\treturn false;\n\n#if (PAGE_SIZE < 8192)\n\t \n\tif (unlikely((rx_buffer_pgcnt - pagecnt_bias) > 1))\n\t\treturn false;\n#else\n\t \n#define IXGBE_LAST_OFFSET \\\n\t(SKB_WITH_OVERHEAD(PAGE_SIZE) - IXGBE_RXBUFFER_3K)\n\tif (rx_buffer->page_offset > IXGBE_LAST_OFFSET)\n\t\treturn false;\n#endif\n\n\t \n\tif (unlikely(pagecnt_bias == 1)) {\n\t\tpage_ref_add(page, USHRT_MAX - 1);\n\t\trx_buffer->pagecnt_bias = USHRT_MAX;\n\t}\n\n\treturn true;\n}\n\n \nstatic void ixgbe_add_rx_frag(struct ixgbe_ring *rx_ring,\n\t\t\t      struct ixgbe_rx_buffer *rx_buffer,\n\t\t\t      struct sk_buff *skb,\n\t\t\t      unsigned int size)\n{\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = ixgbe_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = rx_ring->rx_offset ?\n\t\t\t\tSKB_DATA_ALIGN(rx_ring->rx_offset + size) :\n\t\t\t\tSKB_DATA_ALIGN(size);\n#endif\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_buffer->page,\n\t\t\trx_buffer->page_offset, size, truesize);\n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n}\n\nstatic struct ixgbe_rx_buffer *ixgbe_get_rx_buffer(struct ixgbe_ring *rx_ring,\n\t\t\t\t\t\t   union ixgbe_adv_rx_desc *rx_desc,\n\t\t\t\t\t\t   struct sk_buff **skb,\n\t\t\t\t\t\t   const unsigned int size,\n\t\t\t\t\t\t   int *rx_buffer_pgcnt)\n{\n\tstruct ixgbe_rx_buffer *rx_buffer;\n\n\trx_buffer = &rx_ring->rx_buffer_info[rx_ring->next_to_clean];\n\t*rx_buffer_pgcnt =\n#if (PAGE_SIZE < 8192)\n\t\tpage_count(rx_buffer->page);\n#else\n\t\t0;\n#endif\n\tprefetchw(rx_buffer->page);\n\t*skb = rx_buffer->skb;\n\n\t \n\tif (!ixgbe_test_staterr(rx_desc, IXGBE_RXD_STAT_EOP)) {\n\t\tif (!*skb)\n\t\t\tgoto skip_sync;\n\t} else {\n\t\tif (*skb)\n\t\t\tixgbe_dma_sync_frag(rx_ring, *skb);\n\t}\n\n\t \n\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t      rx_buffer->dma,\n\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t      size,\n\t\t\t\t      DMA_FROM_DEVICE);\nskip_sync:\n\trx_buffer->pagecnt_bias--;\n\n\treturn rx_buffer;\n}\n\nstatic void ixgbe_put_rx_buffer(struct ixgbe_ring *rx_ring,\n\t\t\t\tstruct ixgbe_rx_buffer *rx_buffer,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tint rx_buffer_pgcnt)\n{\n\tif (ixgbe_can_reuse_rx_page(rx_buffer, rx_buffer_pgcnt)) {\n\t\t \n\t\tixgbe_reuse_rx_page(rx_ring, rx_buffer);\n\t} else {\n\t\tif (!IS_ERR(skb) && IXGBE_CB(skb)->dma == rx_buffer->dma) {\n\t\t\t \n\t\t\tIXGBE_CB(skb)->page_released = true;\n\t\t} else {\n\t\t\t \n\t\t\tdma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,\n\t\t\t\t\t     ixgbe_rx_pg_size(rx_ring),\n\t\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t\t     IXGBE_RX_DMA_ATTR);\n\t\t}\n\t\t__page_frag_cache_drain(rx_buffer->page,\n\t\t\t\t\trx_buffer->pagecnt_bias);\n\t}\n\n\t \n\trx_buffer->page = NULL;\n\trx_buffer->skb = NULL;\n}\n\nstatic struct sk_buff *ixgbe_construct_skb(struct ixgbe_ring *rx_ring,\n\t\t\t\t\t   struct ixgbe_rx_buffer *rx_buffer,\n\t\t\t\t\t   struct xdp_buff *xdp,\n\t\t\t\t\t   union ixgbe_adv_rx_desc *rx_desc)\n{\n\tunsigned int size = xdp->data_end - xdp->data;\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = ixgbe_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = SKB_DATA_ALIGN(xdp->data_end -\n\t\t\t\t\t       xdp->data_hard_start);\n#endif\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data);\n\n\t \n\n\t \n\tskb = napi_alloc_skb(&rx_ring->q_vector->napi, IXGBE_RX_HDR_SIZE);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tif (size > IXGBE_RX_HDR_SIZE) {\n\t\tif (!ixgbe_test_staterr(rx_desc, IXGBE_RXD_STAT_EOP))\n\t\t\tIXGBE_CB(skb)->dma = rx_buffer->dma;\n\n\t\tskb_add_rx_frag(skb, 0, rx_buffer->page,\n\t\t\t\txdp->data - page_address(rx_buffer->page),\n\t\t\t\tsize, truesize);\n#if (PAGE_SIZE < 8192)\n\t\trx_buffer->page_offset ^= truesize;\n#else\n\t\trx_buffer->page_offset += truesize;\n#endif\n\t} else {\n\t\tmemcpy(__skb_put(skb, size),\n\t\t       xdp->data, ALIGN(size, sizeof(long)));\n\t\trx_buffer->pagecnt_bias++;\n\t}\n\n\treturn skb;\n}\n\nstatic struct sk_buff *ixgbe_build_skb(struct ixgbe_ring *rx_ring,\n\t\t\t\t       struct ixgbe_rx_buffer *rx_buffer,\n\t\t\t\t       struct xdp_buff *xdp,\n\t\t\t\t       union ixgbe_adv_rx_desc *rx_desc)\n{\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = ixgbe_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) +\n\t\t\t\tSKB_DATA_ALIGN(xdp->data_end -\n\t\t\t\t\t       xdp->data_hard_start);\n#endif\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data_meta);\n\n\t \n\tskb = napi_build_skb(xdp->data_hard_start, truesize);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\t__skb_put(skb, xdp->data_end - xdp->data);\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\t \n\tif (!ixgbe_test_staterr(rx_desc, IXGBE_RXD_STAT_EOP))\n\t\tIXGBE_CB(skb)->dma = rx_buffer->dma;\n\n\t \n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n\n\treturn skb;\n}\n\nstatic struct sk_buff *ixgbe_run_xdp(struct ixgbe_adapter *adapter,\n\t\t\t\t     struct ixgbe_ring *rx_ring,\n\t\t\t\t     struct xdp_buff *xdp)\n{\n\tint err, result = IXGBE_XDP_PASS;\n\tstruct bpf_prog *xdp_prog;\n\tstruct ixgbe_ring *ring;\n\tstruct xdp_frame *xdpf;\n\tu32 act;\n\n\txdp_prog = READ_ONCE(rx_ring->xdp_prog);\n\n\tif (!xdp_prog)\n\t\tgoto xdp_out;\n\n\tprefetchw(xdp->data_hard_start);  \n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\txdpf = xdp_convert_buff_to_frame(xdp);\n\t\tif (unlikely(!xdpf))\n\t\t\tgoto out_failure;\n\t\tring = ixgbe_determine_xdp_ring(adapter);\n\t\tif (static_branch_unlikely(&ixgbe_xdp_locking_key))\n\t\t\tspin_lock(&ring->tx_lock);\n\t\tresult = ixgbe_xmit_xdp_ring(ring, xdpf);\n\t\tif (static_branch_unlikely(&ixgbe_xdp_locking_key))\n\t\t\tspin_unlock(&ring->tx_lock);\n\t\tif (result == IXGBE_XDP_CONSUMED)\n\t\t\tgoto out_failure;\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\terr = xdp_do_redirect(adapter->netdev, xdp, xdp_prog);\n\t\tif (err)\n\t\t\tgoto out_failure;\n\t\tresult = IXGBE_XDP_REDIR;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\nout_failure:\n\t\ttrace_xdp_exception(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;  \n\tcase XDP_DROP:\n\t\tresult = IXGBE_XDP_CONSUMED;\n\t\tbreak;\n\t}\nxdp_out:\n\treturn ERR_PTR(-result);\n}\n\nstatic unsigned int ixgbe_rx_frame_truesize(struct ixgbe_ring *rx_ring,\n\t\t\t\t\t    unsigned int size)\n{\n\tunsigned int truesize;\n\n#if (PAGE_SIZE < 8192)\n\ttruesize = ixgbe_rx_pg_size(rx_ring) / 2;  \n#else\n\ttruesize = rx_ring->rx_offset ?\n\t\tSKB_DATA_ALIGN(rx_ring->rx_offset + size) +\n\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info)) :\n\t\tSKB_DATA_ALIGN(size);\n#endif\n\treturn truesize;\n}\n\nstatic void ixgbe_rx_buffer_flip(struct ixgbe_ring *rx_ring,\n\t\t\t\t struct ixgbe_rx_buffer *rx_buffer,\n\t\t\t\t unsigned int size)\n{\n\tunsigned int truesize = ixgbe_rx_frame_truesize(rx_ring, size);\n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n}\n\n \nstatic int ixgbe_clean_rx_irq(struct ixgbe_q_vector *q_vector,\n\t\t\t       struct ixgbe_ring *rx_ring,\n\t\t\t       const int budget)\n{\n\tunsigned int total_rx_bytes = 0, total_rx_packets = 0, frame_sz = 0;\n\tstruct ixgbe_adapter *adapter = q_vector->adapter;\n#ifdef IXGBE_FCOE\n\tint ddp_bytes;\n\tunsigned int mss = 0;\n#endif  \n\tu16 cleaned_count = ixgbe_desc_unused(rx_ring);\n\tunsigned int offset = rx_ring->rx_offset;\n\tunsigned int xdp_xmit = 0;\n\tstruct xdp_buff xdp;\n\n\t \n#if (PAGE_SIZE < 8192)\n\tframe_sz = ixgbe_rx_frame_truesize(rx_ring, 0);\n#endif\n\txdp_init_buff(&xdp, frame_sz, &rx_ring->xdp_rxq);\n\n\twhile (likely(total_rx_packets < budget)) {\n\t\tunion ixgbe_adv_rx_desc *rx_desc;\n\t\tstruct ixgbe_rx_buffer *rx_buffer;\n\t\tstruct sk_buff *skb;\n\t\tint rx_buffer_pgcnt;\n\t\tunsigned int size;\n\n\t\t \n\t\tif (cleaned_count >= IXGBE_RX_BUFFER_WRITE) {\n\t\t\tixgbe_alloc_rx_buffers(rx_ring, cleaned_count);\n\t\t\tcleaned_count = 0;\n\t\t}\n\n\t\trx_desc = IXGBE_RX_DESC(rx_ring, rx_ring->next_to_clean);\n\t\tsize = le16_to_cpu(rx_desc->wb.upper.length);\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\trx_buffer = ixgbe_get_rx_buffer(rx_ring, rx_desc, &skb, size, &rx_buffer_pgcnt);\n\n\t\t \n\t\tif (!skb) {\n\t\t\tunsigned char *hard_start;\n\n\t\t\thard_start = page_address(rx_buffer->page) +\n\t\t\t\t     rx_buffer->page_offset - offset;\n\t\t\txdp_prepare_buff(&xdp, hard_start, offset, size, true);\n\t\t\txdp_buff_clear_frags_flag(&xdp);\n#if (PAGE_SIZE > 4096)\n\t\t\t \n\t\t\txdp.frame_sz = ixgbe_rx_frame_truesize(rx_ring, size);\n#endif\n\t\t\tskb = ixgbe_run_xdp(adapter, rx_ring, &xdp);\n\t\t}\n\n\t\tif (IS_ERR(skb)) {\n\t\t\tunsigned int xdp_res = -PTR_ERR(skb);\n\n\t\t\tif (xdp_res & (IXGBE_XDP_TX | IXGBE_XDP_REDIR)) {\n\t\t\t\txdp_xmit |= xdp_res;\n\t\t\t\tixgbe_rx_buffer_flip(rx_ring, rx_buffer, size);\n\t\t\t} else {\n\t\t\t\trx_buffer->pagecnt_bias++;\n\t\t\t}\n\t\t\ttotal_rx_packets++;\n\t\t\ttotal_rx_bytes += size;\n\t\t} else if (skb) {\n\t\t\tixgbe_add_rx_frag(rx_ring, rx_buffer, skb, size);\n\t\t} else if (ring_uses_build_skb(rx_ring)) {\n\t\t\tskb = ixgbe_build_skb(rx_ring, rx_buffer,\n\t\t\t\t\t      &xdp, rx_desc);\n\t\t} else {\n\t\t\tskb = ixgbe_construct_skb(rx_ring, rx_buffer,\n\t\t\t\t\t\t  &xdp, rx_desc);\n\t\t}\n\n\t\t \n\t\tif (!skb) {\n\t\t\trx_ring->rx_stats.alloc_rx_buff_failed++;\n\t\t\trx_buffer->pagecnt_bias++;\n\t\t\tbreak;\n\t\t}\n\n\t\tixgbe_put_rx_buffer(rx_ring, rx_buffer, skb, rx_buffer_pgcnt);\n\t\tcleaned_count++;\n\n\t\t \n\t\tif (ixgbe_is_non_eop(rx_ring, rx_desc, skb))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (ixgbe_cleanup_headers(rx_ring, rx_desc, skb))\n\t\t\tcontinue;\n\n\t\t \n\t\ttotal_rx_bytes += skb->len;\n\n\t\t \n\t\tixgbe_process_skb_fields(rx_ring, rx_desc, skb);\n\n#ifdef IXGBE_FCOE\n\t\t \n\t\tif (ixgbe_rx_is_fcoe(rx_ring, rx_desc)) {\n\t\t\tddp_bytes = ixgbe_fcoe_ddp(adapter, rx_desc, skb);\n\t\t\t \n\t\t\tif (ddp_bytes > 0) {\n\t\t\t\tif (!mss) {\n\t\t\t\t\tmss = rx_ring->netdev->mtu -\n\t\t\t\t\t\tsizeof(struct fcoe_hdr) -\n\t\t\t\t\t\tsizeof(struct fc_frame_header) -\n\t\t\t\t\t\tsizeof(struct fcoe_crc_eof);\n\t\t\t\t\tif (mss > 512)\n\t\t\t\t\t\tmss &= ~511;\n\t\t\t\t}\n\t\t\t\ttotal_rx_bytes += ddp_bytes;\n\t\t\t\ttotal_rx_packets += DIV_ROUND_UP(ddp_bytes,\n\t\t\t\t\t\t\t\t mss);\n\t\t\t}\n\t\t\tif (!ddp_bytes) {\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n#endif  \n\t\tixgbe_rx_skb(q_vector, skb);\n\n\t\t \n\t\ttotal_rx_packets++;\n\t}\n\n\tif (xdp_xmit & IXGBE_XDP_REDIR)\n\t\txdp_do_flush_map();\n\n\tif (xdp_xmit & IXGBE_XDP_TX) {\n\t\tstruct ixgbe_ring *ring = ixgbe_determine_xdp_ring(adapter);\n\n\t\tixgbe_xdp_ring_update_tail_locked(ring);\n\t}\n\n\tu64_stats_update_begin(&rx_ring->syncp);\n\trx_ring->stats.packets += total_rx_packets;\n\trx_ring->stats.bytes += total_rx_bytes;\n\tu64_stats_update_end(&rx_ring->syncp);\n\tq_vector->rx.total_packets += total_rx_packets;\n\tq_vector->rx.total_bytes += total_rx_bytes;\n\n\treturn total_rx_packets;\n}\n\n \nstatic void ixgbe_configure_msix(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_q_vector *q_vector;\n\tint v_idx;\n\tu32 mask;\n\n\t \n\tif (adapter->num_vfs > 32) {\n\t\tu32 eitrsel = BIT(adapter->num_vfs - 32) - 1;\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EITRSEL, eitrsel);\n\t}\n\n\t \n\tfor (v_idx = 0; v_idx < adapter->num_q_vectors; v_idx++) {\n\t\tstruct ixgbe_ring *ring;\n\t\tq_vector = adapter->q_vector[v_idx];\n\n\t\tixgbe_for_each_ring(ring, q_vector->rx)\n\t\t\tixgbe_set_ivar(adapter, 0, ring->reg_idx, v_idx);\n\n\t\tixgbe_for_each_ring(ring, q_vector->tx)\n\t\t\tixgbe_set_ivar(adapter, 1, ring->reg_idx, v_idx);\n\n\t\tixgbe_write_eitr(q_vector);\n\t}\n\n\tswitch (adapter->hw.mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tixgbe_set_ivar(adapter, -1, IXGBE_IVAR_OTHER_CAUSES_INDEX,\n\t\t\t       v_idx);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tixgbe_set_ivar(adapter, -1, 1, v_idx);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EITR(v_idx), 1950);\n\n\t \n\tmask = IXGBE_EIMS_ENABLE_MASK;\n\tmask &= ~(IXGBE_EIMS_OTHER |\n\t\t  IXGBE_EIMS_MAILBOX |\n\t\t  IXGBE_EIMS_LSC);\n\n\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EIAC, mask);\n}\n\n \nstatic void ixgbe_update_itr(struct ixgbe_q_vector *q_vector,\n\t\t\t     struct ixgbe_ring_container *ring_container)\n{\n\tunsigned int itr = IXGBE_ITR_ADAPTIVE_MIN_USECS |\n\t\t\t   IXGBE_ITR_ADAPTIVE_LATENCY;\n\tunsigned int avg_wire_size, packets, bytes;\n\tunsigned long next_update = jiffies;\n\n\t \n\tif (!ring_container->ring)\n\t\treturn;\n\n\t \n\tif (time_after(next_update, ring_container->next_update))\n\t\tgoto clear_counts;\n\n\tpackets = ring_container->total_packets;\n\n\t \n\tif (!packets) {\n\t\titr = (q_vector->itr >> 2) + IXGBE_ITR_ADAPTIVE_MIN_INC;\n\t\tif (itr > IXGBE_ITR_ADAPTIVE_MAX_USECS)\n\t\t\titr = IXGBE_ITR_ADAPTIVE_MAX_USECS;\n\t\titr += ring_container->itr & IXGBE_ITR_ADAPTIVE_LATENCY;\n\t\tgoto clear_counts;\n\t}\n\n\tbytes = ring_container->total_bytes;\n\n\t \n\tif (packets < 4 && bytes < 9000) {\n\t\titr = IXGBE_ITR_ADAPTIVE_LATENCY;\n\t\tgoto adjust_by_size;\n\t}\n\n\t \n\tif (packets < 48) {\n\t\titr = (q_vector->itr >> 2) + IXGBE_ITR_ADAPTIVE_MIN_INC;\n\t\tif (itr > IXGBE_ITR_ADAPTIVE_MAX_USECS)\n\t\t\titr = IXGBE_ITR_ADAPTIVE_MAX_USECS;\n\t\tgoto clear_counts;\n\t}\n\n\t \n\tif (packets < 96) {\n\t\titr = q_vector->itr >> 2;\n\t\tgoto clear_counts;\n\t}\n\n\t \n\tif (packets < 256) {\n\t\titr = q_vector->itr >> 3;\n\t\tif (itr < IXGBE_ITR_ADAPTIVE_MIN_USECS)\n\t\t\titr = IXGBE_ITR_ADAPTIVE_MIN_USECS;\n\t\tgoto clear_counts;\n\t}\n\n\t \n\titr = IXGBE_ITR_ADAPTIVE_BULK;\n\nadjust_by_size:\n\t \n\tavg_wire_size = bytes / packets;\n\n\t \n\tif (avg_wire_size <= 60) {\n\t\t \n\t\tavg_wire_size = 5120;\n\t} else if (avg_wire_size <= 316) {\n\t\t \n\t\tavg_wire_size *= 40;\n\t\tavg_wire_size += 2720;\n\t} else if (avg_wire_size <= 1084) {\n\t\t \n\t\tavg_wire_size *= 15;\n\t\tavg_wire_size += 11452;\n\t} else if (avg_wire_size < 1968) {\n\t\t \n\t\tavg_wire_size *= 5;\n\t\tavg_wire_size += 22420;\n\t} else {\n\t\t \n\t\tavg_wire_size = 32256;\n\t}\n\n\t \n\tif (itr & IXGBE_ITR_ADAPTIVE_LATENCY)\n\t\tavg_wire_size >>= 1;\n\n\t \n\tswitch (q_vector->adapter->link_speed) {\n\tcase IXGBE_LINK_SPEED_10GB_FULL:\n\tcase IXGBE_LINK_SPEED_100_FULL:\n\tdefault:\n\t\titr += DIV_ROUND_UP(avg_wire_size,\n\t\t\t\t    IXGBE_ITR_ADAPTIVE_MIN_INC * 256) *\n\t\t       IXGBE_ITR_ADAPTIVE_MIN_INC;\n\t\tbreak;\n\tcase IXGBE_LINK_SPEED_2_5GB_FULL:\n\tcase IXGBE_LINK_SPEED_1GB_FULL:\n\tcase IXGBE_LINK_SPEED_10_FULL:\n\t\tif (avg_wire_size > 8064)\n\t\t\tavg_wire_size = 8064;\n\t\titr += DIV_ROUND_UP(avg_wire_size,\n\t\t\t\t    IXGBE_ITR_ADAPTIVE_MIN_INC * 64) *\n\t\t       IXGBE_ITR_ADAPTIVE_MIN_INC;\n\t\tbreak;\n\t}\n\nclear_counts:\n\t \n\tring_container->itr = itr;\n\n\t \n\tring_container->next_update = next_update + 1;\n\n\tring_container->total_bytes = 0;\n\tring_container->total_packets = 0;\n}\n\n \nvoid ixgbe_write_eitr(struct ixgbe_q_vector *q_vector)\n{\n\tstruct ixgbe_adapter *adapter = q_vector->adapter;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint v_idx = q_vector->v_idx;\n\tu32 itr_reg = q_vector->itr & IXGBE_MAX_EITR;\n\n\tswitch (adapter->hw.mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\t \n\t\titr_reg |= (itr_reg << 16);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\t \n\t\titr_reg |= IXGBE_EITR_CNT_WDIS;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tIXGBE_WRITE_REG(hw, IXGBE_EITR(v_idx), itr_reg);\n}\n\nstatic void ixgbe_set_itr(struct ixgbe_q_vector *q_vector)\n{\n\tu32 new_itr;\n\n\tixgbe_update_itr(q_vector, &q_vector->tx);\n\tixgbe_update_itr(q_vector, &q_vector->rx);\n\n\t \n\tnew_itr = min(q_vector->rx.itr, q_vector->tx.itr);\n\n\t \n\tnew_itr &= ~IXGBE_ITR_ADAPTIVE_LATENCY;\n\tnew_itr <<= 2;\n\n\tif (new_itr != q_vector->itr) {\n\t\t \n\t\tq_vector->itr = new_itr;\n\n\t\tixgbe_write_eitr(q_vector);\n\t}\n}\n\n \nstatic void ixgbe_check_overtemp_subtask(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 eicr = adapter->interrupt_event;\n\ts32 rc;\n\n\tif (test_bit(__IXGBE_DOWN, &adapter->state))\n\t\treturn;\n\n\tif (!(adapter->flags2 & IXGBE_FLAG2_TEMP_SENSOR_EVENT))\n\t\treturn;\n\n\tadapter->flags2 &= ~IXGBE_FLAG2_TEMP_SENSOR_EVENT;\n\n\tswitch (hw->device_id) {\n\tcase IXGBE_DEV_ID_82599_T3_LOM:\n\t\t \n\t\tif (!(eicr & IXGBE_EICR_GPI_SDP0_8259X) &&\n\t\t    !(eicr & IXGBE_EICR_LSC))\n\t\t\treturn;\n\n\t\tif (!(eicr & IXGBE_EICR_LSC) && hw->mac.ops.check_link) {\n\t\t\tu32 speed;\n\t\t\tbool link_up = false;\n\n\t\t\thw->mac.ops.check_link(hw, &speed, &link_up, false);\n\n\t\t\tif (link_up)\n\t\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tif (hw->phy.ops.check_overtemp(hw) != IXGBE_ERR_OVERTEMP)\n\t\t\treturn;\n\n\t\tbreak;\n\tcase IXGBE_DEV_ID_X550EM_A_1G_T:\n\tcase IXGBE_DEV_ID_X550EM_A_1G_T_L:\n\t\trc = hw->phy.ops.check_overtemp(hw);\n\t\tif (rc != IXGBE_ERR_OVERTEMP)\n\t\t\treturn;\n\t\tbreak;\n\tdefault:\n\t\tif (adapter->hw.mac.type >= ixgbe_mac_X540)\n\t\t\treturn;\n\t\tif (!(eicr & IXGBE_EICR_GPI_SDP0(hw)))\n\t\t\treturn;\n\t\tbreak;\n\t}\n\te_crit(drv, \"%s\\n\", ixgbe_overheat_msg);\n\n\tadapter->interrupt_event = 0;\n}\n\nstatic void ixgbe_check_fan_failure(struct ixgbe_adapter *adapter, u32 eicr)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tif ((adapter->flags & IXGBE_FLAG_FAN_FAIL_CAPABLE) &&\n\t    (eicr & IXGBE_EICR_GPI_SDP1(hw))) {\n\t\te_crit(probe, \"Fan has stopped, replace the adapter\\n\");\n\t\t \n\t\tIXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_GPI_SDP1(hw));\n\t}\n}\n\nstatic void ixgbe_check_overtemp_event(struct ixgbe_adapter *adapter, u32 eicr)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tif (!(adapter->flags2 & IXGBE_FLAG2_TEMP_SENSOR_CAPABLE))\n\t\treturn;\n\n\tswitch (adapter->hw.mac.type) {\n\tcase ixgbe_mac_82599EB:\n\t\t \n\t\tif (((eicr & IXGBE_EICR_GPI_SDP0(hw)) ||\n\t\t     (eicr & IXGBE_EICR_LSC)) &&\n\t\t    (!test_bit(__IXGBE_DOWN, &adapter->state))) {\n\t\t\tadapter->interrupt_event = eicr;\n\t\t\tadapter->flags2 |= IXGBE_FLAG2_TEMP_SENSOR_EVENT;\n\t\t\tixgbe_service_event_schedule(adapter);\n\t\t\treturn;\n\t\t}\n\t\treturn;\n\tcase ixgbe_mac_x550em_a:\n\t\tif (eicr & IXGBE_EICR_GPI_SDP0_X550EM_a) {\n\t\t\tadapter->interrupt_event = eicr;\n\t\t\tadapter->flags2 |= IXGBE_FLAG2_TEMP_SENSOR_EVENT;\n\t\t\tixgbe_service_event_schedule(adapter);\n\t\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EIMC,\n\t\t\t\t\tIXGBE_EICR_GPI_SDP0_X550EM_a);\n\t\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EICR,\n\t\t\t\t\tIXGBE_EICR_GPI_SDP0_X550EM_a);\n\t\t}\n\t\treturn;\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X540:\n\t\tif (!(eicr & IXGBE_EICR_TS))\n\t\t\treturn;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\te_crit(drv, \"%s\\n\", ixgbe_overheat_msg);\n}\n\nstatic inline bool ixgbe_is_sfp(struct ixgbe_hw *hw)\n{\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tif (hw->phy.type == ixgbe_phy_nl)\n\t\t\treturn true;\n\t\treturn false;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tswitch (hw->mac.ops.get_media_type(hw)) {\n\t\tcase ixgbe_media_type_fiber:\n\t\tcase ixgbe_media_type_fiber_qsfp:\n\t\t\treturn true;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void ixgbe_check_sfp_event(struct ixgbe_adapter *adapter, u32 eicr)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 eicr_mask = IXGBE_EICR_GPI_SDP2(hw);\n\n\tif (!ixgbe_is_sfp(hw))\n\t\treturn;\n\n\t \n\tif (hw->mac.type >= ixgbe_mac_X540)\n\t\teicr_mask = IXGBE_EICR_GPI_SDP0_X540;\n\n\tif (eicr & eicr_mask) {\n\t\t \n\t\tIXGBE_WRITE_REG(hw, IXGBE_EICR, eicr_mask);\n\t\tif (!test_bit(__IXGBE_DOWN, &adapter->state)) {\n\t\t\tadapter->flags2 |= IXGBE_FLAG2_SFP_NEEDS_RESET;\n\t\t\tadapter->sfp_poll_time = 0;\n\t\t\tixgbe_service_event_schedule(adapter);\n\t\t}\n\t}\n\n\tif (adapter->hw.mac.type == ixgbe_mac_82599EB &&\n\t    (eicr & IXGBE_EICR_GPI_SDP1(hw))) {\n\t\t \n\t\tIXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_GPI_SDP1(hw));\n\t\tif (!test_bit(__IXGBE_DOWN, &adapter->state)) {\n\t\t\tadapter->flags |= IXGBE_FLAG_NEED_LINK_CONFIG;\n\t\t\tixgbe_service_event_schedule(adapter);\n\t\t}\n\t}\n}\n\nstatic void ixgbe_check_lsc(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tadapter->lsc_int++;\n\tadapter->flags |= IXGBE_FLAG_NEED_LINK_UPDATE;\n\tadapter->link_check_timeout = jiffies;\n\tif (!test_bit(__IXGBE_DOWN, &adapter->state)) {\n\t\tIXGBE_WRITE_REG(hw, IXGBE_EIMC, IXGBE_EIMC_LSC);\n\t\tIXGBE_WRITE_FLUSH(hw);\n\t\tixgbe_service_event_schedule(adapter);\n\t}\n}\n\nstatic inline void ixgbe_irq_enable_queues(struct ixgbe_adapter *adapter,\n\t\t\t\t\t   u64 qmask)\n{\n\tu32 mask;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tmask = (IXGBE_EIMS_RTX_QUEUE & qmask);\n\t\tIXGBE_WRITE_REG(hw, IXGBE_EIMS, mask);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tmask = (qmask & 0xFFFFFFFF);\n\t\tif (mask)\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_EIMS_EX(0), mask);\n\t\tmask = (qmask >> 32);\n\t\tif (mask)\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_EIMS_EX(1), mask);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\t \n}\n\n \nstatic inline void ixgbe_irq_enable(struct ixgbe_adapter *adapter, bool queues,\n\t\t\t\t    bool flush)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 mask = (IXGBE_EIMS_ENABLE_MASK & ~IXGBE_EIMS_RTX_QUEUE);\n\n\t \n\tif (adapter->flags & IXGBE_FLAG_NEED_LINK_UPDATE)\n\t\tmask &= ~IXGBE_EIMS_LSC;\n\n\tif (adapter->flags2 & IXGBE_FLAG2_TEMP_SENSOR_CAPABLE)\n\t\tswitch (adapter->hw.mac.type) {\n\t\tcase ixgbe_mac_82599EB:\n\t\t\tmask |= IXGBE_EIMS_GPI_SDP0(hw);\n\t\t\tbreak;\n\t\tcase ixgbe_mac_X540:\n\t\tcase ixgbe_mac_X550:\n\t\tcase ixgbe_mac_X550EM_x:\n\t\tcase ixgbe_mac_x550em_a:\n\t\t\tmask |= IXGBE_EIMS_TS;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\tif (adapter->flags & IXGBE_FLAG_FAN_FAIL_CAPABLE)\n\t\tmask |= IXGBE_EIMS_GPI_SDP1(hw);\n\tswitch (adapter->hw.mac.type) {\n\tcase ixgbe_mac_82599EB:\n\t\tmask |= IXGBE_EIMS_GPI_SDP1(hw);\n\t\tmask |= IXGBE_EIMS_GPI_SDP2(hw);\n\t\tfallthrough;\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tif (adapter->hw.device_id == IXGBE_DEV_ID_X550EM_X_SFP ||\n\t\t    adapter->hw.device_id == IXGBE_DEV_ID_X550EM_A_SFP ||\n\t\t    adapter->hw.device_id == IXGBE_DEV_ID_X550EM_A_SFP_N)\n\t\t\tmask |= IXGBE_EIMS_GPI_SDP0(&adapter->hw);\n\t\tif (adapter->hw.phy.type == ixgbe_phy_x550em_ext_t)\n\t\t\tmask |= IXGBE_EICR_GPI_SDP0_X540;\n\t\tmask |= IXGBE_EIMS_ECC;\n\t\tmask |= IXGBE_EIMS_MAILBOX;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif ((adapter->flags & IXGBE_FLAG_FDIR_HASH_CAPABLE) &&\n\t    !(adapter->flags2 & IXGBE_FLAG2_FDIR_REQUIRES_REINIT))\n\t\tmask |= IXGBE_EIMS_FLOW_DIR;\n\n\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EIMS, mask);\n\tif (queues)\n\t\tixgbe_irq_enable_queues(adapter, ~0);\n\tif (flush)\n\t\tIXGBE_WRITE_FLUSH(&adapter->hw);\n}\n\nstatic irqreturn_t ixgbe_msix_other(int irq, void *data)\n{\n\tstruct ixgbe_adapter *adapter = data;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 eicr;\n\n\t \n\teicr = IXGBE_READ_REG(hw, IXGBE_EICS);\n\n\t \n\teicr &= 0xFFFF0000;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_EICR, eicr);\n\n\tif (eicr & IXGBE_EICR_LSC)\n\t\tixgbe_check_lsc(adapter);\n\n\tif (eicr & IXGBE_EICR_MAILBOX)\n\t\tixgbe_msg_task(adapter);\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tif (hw->phy.type == ixgbe_phy_x550em_ext_t &&\n\t\t    (eicr & IXGBE_EICR_GPI_SDP0_X540)) {\n\t\t\tadapter->flags2 |= IXGBE_FLAG2_PHY_INTERRUPT;\n\t\t\tixgbe_service_event_schedule(adapter);\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_EICR,\n\t\t\t\t\tIXGBE_EICR_GPI_SDP0_X540);\n\t\t}\n\t\tif (eicr & IXGBE_EICR_ECC) {\n\t\t\te_info(link, \"Received ECC Err, initiating reset\\n\");\n\t\t\tset_bit(__IXGBE_RESET_REQUESTED, &adapter->state);\n\t\t\tixgbe_service_event_schedule(adapter);\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_ECC);\n\t\t}\n\t\t \n\t\tif (eicr & IXGBE_EICR_FLOW_DIR) {\n\t\t\tint reinit_count = 0;\n\t\t\tint i;\n\t\t\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\t\t\tstruct ixgbe_ring *ring = adapter->tx_ring[i];\n\t\t\t\tif (test_and_clear_bit(__IXGBE_TX_FDIR_INIT_DONE,\n\t\t\t\t\t\t       &ring->state))\n\t\t\t\t\treinit_count++;\n\t\t\t}\n\t\t\tif (reinit_count) {\n\t\t\t\t \n\t\t\t\tIXGBE_WRITE_REG(hw, IXGBE_EIMC, IXGBE_EIMC_FLOW_DIR);\n\t\t\t\tadapter->flags2 |= IXGBE_FLAG2_FDIR_REQUIRES_REINIT;\n\t\t\t\tixgbe_service_event_schedule(adapter);\n\t\t\t}\n\t\t}\n\t\tixgbe_check_sfp_event(adapter, eicr);\n\t\tixgbe_check_overtemp_event(adapter, eicr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tixgbe_check_fan_failure(adapter, eicr);\n\n\tif (unlikely(eicr & IXGBE_EICR_TIMESYNC))\n\t\tixgbe_ptp_check_pps_event(adapter);\n\n\t \n\tif (!test_bit(__IXGBE_DOWN, &adapter->state))\n\t\tixgbe_irq_enable(adapter, false, false);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t ixgbe_msix_clean_rings(int irq, void *data)\n{\n\tstruct ixgbe_q_vector *q_vector = data;\n\n\t \n\n\tif (q_vector->rx.ring || q_vector->tx.ring)\n\t\tnapi_schedule_irqoff(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n \nint ixgbe_poll(struct napi_struct *napi, int budget)\n{\n\tstruct ixgbe_q_vector *q_vector =\n\t\t\t\tcontainer_of(napi, struct ixgbe_q_vector, napi);\n\tstruct ixgbe_adapter *adapter = q_vector->adapter;\n\tstruct ixgbe_ring *ring;\n\tint per_ring_budget, work_done = 0;\n\tbool clean_complete = true;\n\n#ifdef CONFIG_IXGBE_DCA\n\tif (adapter->flags & IXGBE_FLAG_DCA_ENABLED)\n\t\tixgbe_update_dca(q_vector);\n#endif\n\n\tixgbe_for_each_ring(ring, q_vector->tx) {\n\t\tbool wd = ring->xsk_pool ?\n\t\t\t  ixgbe_clean_xdp_tx_irq(q_vector, ring, budget) :\n\t\t\t  ixgbe_clean_tx_irq(q_vector, ring, budget);\n\n\t\tif (!wd)\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (budget <= 0)\n\t\treturn budget;\n\n\t \n\tif (q_vector->rx.count > 1)\n\t\tper_ring_budget = max(budget/q_vector->rx.count, 1);\n\telse\n\t\tper_ring_budget = budget;\n\n\tixgbe_for_each_ring(ring, q_vector->rx) {\n\t\tint cleaned = ring->xsk_pool ?\n\t\t\t      ixgbe_clean_rx_irq_zc(q_vector, ring,\n\t\t\t\t\t\t    per_ring_budget) :\n\t\t\t      ixgbe_clean_rx_irq(q_vector, ring,\n\t\t\t\t\t\t per_ring_budget);\n\n\t\twork_done += cleaned;\n\t\tif (cleaned >= per_ring_budget)\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (!clean_complete)\n\t\treturn budget;\n\n\t \n\tif (likely(napi_complete_done(napi, work_done))) {\n\t\tif (adapter->rx_itr_setting & 1)\n\t\t\tixgbe_set_itr(q_vector);\n\t\tif (!test_bit(__IXGBE_DOWN, &adapter->state))\n\t\t\tixgbe_irq_enable_queues(adapter,\n\t\t\t\t\t\tBIT_ULL(q_vector->v_idx));\n\t}\n\n\treturn min(work_done, budget - 1);\n}\n\n \nstatic int ixgbe_request_msix_irqs(struct ixgbe_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tunsigned int ri = 0, ti = 0;\n\tint vector, err;\n\n\tfor (vector = 0; vector < adapter->num_q_vectors; vector++) {\n\t\tstruct ixgbe_q_vector *q_vector = adapter->q_vector[vector];\n\t\tstruct msix_entry *entry = &adapter->msix_entries[vector];\n\n\t\tif (q_vector->tx.ring && q_vector->rx.ring) {\n\t\t\tsnprintf(q_vector->name, sizeof(q_vector->name),\n\t\t\t\t \"%s-TxRx-%u\", netdev->name, ri++);\n\t\t\tti++;\n\t\t} else if (q_vector->rx.ring) {\n\t\t\tsnprintf(q_vector->name, sizeof(q_vector->name),\n\t\t\t\t \"%s-rx-%u\", netdev->name, ri++);\n\t\t} else if (q_vector->tx.ring) {\n\t\t\tsnprintf(q_vector->name, sizeof(q_vector->name),\n\t\t\t\t \"%s-tx-%u\", netdev->name, ti++);\n\t\t} else {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\terr = request_irq(entry->vector, &ixgbe_msix_clean_rings, 0,\n\t\t\t\t  q_vector->name, q_vector);\n\t\tif (err) {\n\t\t\te_err(probe, \"request_irq failed for MSIX interrupt \"\n\t\t\t      \"Error: %d\\n\", err);\n\t\t\tgoto free_queue_irqs;\n\t\t}\n\t\t \n\t\tif (adapter->flags & IXGBE_FLAG_FDIR_HASH_CAPABLE) {\n\t\t\t \n\t\t\tirq_update_affinity_hint(entry->vector,\n\t\t\t\t\t\t &q_vector->affinity_mask);\n\t\t}\n\t}\n\n\terr = request_irq(adapter->msix_entries[vector].vector,\n\t\t\t  ixgbe_msix_other, 0, netdev->name, adapter);\n\tif (err) {\n\t\te_err(probe, \"request_irq for msix_other failed: %d\\n\", err);\n\t\tgoto free_queue_irqs;\n\t}\n\n\treturn 0;\n\nfree_queue_irqs:\n\twhile (vector) {\n\t\tvector--;\n\t\tirq_update_affinity_hint(adapter->msix_entries[vector].vector,\n\t\t\t\t\t NULL);\n\t\tfree_irq(adapter->msix_entries[vector].vector,\n\t\t\t adapter->q_vector[vector]);\n\t}\n\tadapter->flags &= ~IXGBE_FLAG_MSIX_ENABLED;\n\tpci_disable_msix(adapter->pdev);\n\tkfree(adapter->msix_entries);\n\tadapter->msix_entries = NULL;\n\treturn err;\n}\n\n \nstatic irqreturn_t ixgbe_intr(int irq, void *data)\n{\n\tstruct ixgbe_adapter *adapter = data;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct ixgbe_q_vector *q_vector = adapter->q_vector[0];\n\tu32 eicr;\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_EIMC, IXGBE_IRQ_CLEAR_MASK);\n\n\t \n\teicr = IXGBE_READ_REG(hw, IXGBE_EICR);\n\tif (!eicr) {\n\t\t \n\t\tif (!test_bit(__IXGBE_DOWN, &adapter->state))\n\t\t\tixgbe_irq_enable(adapter, true, true);\n\t\treturn IRQ_NONE;\t \n\t}\n\n\tif (eicr & IXGBE_EICR_LSC)\n\t\tixgbe_check_lsc(adapter);\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82599EB:\n\t\tixgbe_check_sfp_event(adapter, eicr);\n\t\tfallthrough;\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tif (eicr & IXGBE_EICR_ECC) {\n\t\t\te_info(link, \"Received ECC Err, initiating reset\\n\");\n\t\t\tset_bit(__IXGBE_RESET_REQUESTED, &adapter->state);\n\t\t\tixgbe_service_event_schedule(adapter);\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_EICR, IXGBE_EICR_ECC);\n\t\t}\n\t\tixgbe_check_overtemp_event(adapter, eicr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tixgbe_check_fan_failure(adapter, eicr);\n\tif (unlikely(eicr & IXGBE_EICR_TIMESYNC))\n\t\tixgbe_ptp_check_pps_event(adapter);\n\n\t \n\tnapi_schedule_irqoff(&q_vector->napi);\n\n\t \n\tif (!test_bit(__IXGBE_DOWN, &adapter->state))\n\t\tixgbe_irq_enable(adapter, false, false);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int ixgbe_request_irq(struct ixgbe_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tint err;\n\n\tif (adapter->flags & IXGBE_FLAG_MSIX_ENABLED)\n\t\terr = ixgbe_request_msix_irqs(adapter);\n\telse if (adapter->flags & IXGBE_FLAG_MSI_ENABLED)\n\t\terr = request_irq(adapter->pdev->irq, ixgbe_intr, 0,\n\t\t\t\t  netdev->name, adapter);\n\telse\n\t\terr = request_irq(adapter->pdev->irq, ixgbe_intr, IRQF_SHARED,\n\t\t\t\t  netdev->name, adapter);\n\n\tif (err)\n\t\te_err(probe, \"request_irq failed, Error %d\\n\", err);\n\n\treturn err;\n}\n\nstatic void ixgbe_free_irq(struct ixgbe_adapter *adapter)\n{\n\tint vector;\n\n\tif (!(adapter->flags & IXGBE_FLAG_MSIX_ENABLED)) {\n\t\tfree_irq(adapter->pdev->irq, adapter);\n\t\treturn;\n\t}\n\n\tif (!adapter->msix_entries)\n\t\treturn;\n\n\tfor (vector = 0; vector < adapter->num_q_vectors; vector++) {\n\t\tstruct ixgbe_q_vector *q_vector = adapter->q_vector[vector];\n\t\tstruct msix_entry *entry = &adapter->msix_entries[vector];\n\n\t\t \n\t\tif (!q_vector->rx.ring && !q_vector->tx.ring)\n\t\t\tcontinue;\n\n\t\t \n\t\tirq_update_affinity_hint(entry->vector, NULL);\n\n\t\tfree_irq(entry->vector, q_vector);\n\t}\n\n\tfree_irq(adapter->msix_entries[vector].vector, adapter);\n}\n\n \nstatic inline void ixgbe_irq_disable(struct ixgbe_adapter *adapter)\n{\n\tswitch (adapter->hw.mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EIMC, ~0);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EIMC, 0xFFFF0000);\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EIMC_EX(0), ~0);\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EIMC_EX(1), ~0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tIXGBE_WRITE_FLUSH(&adapter->hw);\n\tif (adapter->flags & IXGBE_FLAG_MSIX_ENABLED) {\n\t\tint vector;\n\n\t\tfor (vector = 0; vector < adapter->num_q_vectors; vector++)\n\t\t\tsynchronize_irq(adapter->msix_entries[vector].vector);\n\n\t\tsynchronize_irq(adapter->msix_entries[vector++].vector);\n\t} else {\n\t\tsynchronize_irq(adapter->pdev->irq);\n\t}\n}\n\n \nstatic void ixgbe_configure_msi_and_legacy(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_q_vector *q_vector = adapter->q_vector[0];\n\n\tixgbe_write_eitr(q_vector);\n\n\tixgbe_set_ivar(adapter, 0, 0, 0);\n\tixgbe_set_ivar(adapter, 1, 0, 0);\n\n\te_info(hw, \"Legacy interrupt IVAR setup done\\n\");\n}\n\n \nvoid ixgbe_configure_tx_ring(struct ixgbe_adapter *adapter,\n\t\t\t     struct ixgbe_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu64 tdba = ring->dma;\n\tint wait_loop = 10;\n\tu32 txdctl = IXGBE_TXDCTL_ENABLE;\n\tu8 reg_idx = ring->reg_idx;\n\n\tring->xsk_pool = NULL;\n\tif (ring_is_xdp(ring))\n\t\tring->xsk_pool = ixgbe_xsk_pool(adapter, ring);\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_TXDCTL(reg_idx), 0);\n\tIXGBE_WRITE_FLUSH(hw);\n\n\tIXGBE_WRITE_REG(hw, IXGBE_TDBAL(reg_idx),\n\t\t\t(tdba & DMA_BIT_MASK(32)));\n\tIXGBE_WRITE_REG(hw, IXGBE_TDBAH(reg_idx), (tdba >> 32));\n\tIXGBE_WRITE_REG(hw, IXGBE_TDLEN(reg_idx),\n\t\t\tring->count * sizeof(union ixgbe_adv_tx_desc));\n\tIXGBE_WRITE_REG(hw, IXGBE_TDH(reg_idx), 0);\n\tIXGBE_WRITE_REG(hw, IXGBE_TDT(reg_idx), 0);\n\tring->tail = adapter->io_addr + IXGBE_TDT(reg_idx);\n\n\t \n\tif (!ring->q_vector || (ring->q_vector->itr < IXGBE_100K_ITR))\n\t\ttxdctl |= 1u << 16;\t \n\telse\n\t\ttxdctl |= 8u << 16;\t \n\n\t \n\ttxdctl |= (1u << 8) |\t \n\t\t   32;\t\t \n\n\t \n\tif (adapter->flags & IXGBE_FLAG_FDIR_HASH_CAPABLE) {\n\t\tring->atr_sample_rate = adapter->atr_sample_rate;\n\t\tring->atr_count = 0;\n\t\tset_bit(__IXGBE_TX_FDIR_INIT_DONE, &ring->state);\n\t} else {\n\t\tring->atr_sample_rate = 0;\n\t}\n\n\t \n\tif (!test_and_set_bit(__IXGBE_TX_XPS_INIT_DONE, &ring->state)) {\n\t\tstruct ixgbe_q_vector *q_vector = ring->q_vector;\n\n\t\tif (q_vector)\n\t\t\tnetif_set_xps_queue(ring->netdev,\n\t\t\t\t\t    &q_vector->affinity_mask,\n\t\t\t\t\t    ring->queue_index);\n\t}\n\n\tclear_bit(__IXGBE_HANG_CHECK_ARMED, &ring->state);\n\n\t \n\tmemset(ring->tx_buffer_info, 0,\n\t       sizeof(struct ixgbe_tx_buffer) * ring->count);\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_TXDCTL(reg_idx), txdctl);\n\n\t \n\tif (hw->mac.type == ixgbe_mac_82598EB &&\n\t    !(IXGBE_READ_REG(hw, IXGBE_LINKS) & IXGBE_LINKS_UP))\n\t\treturn;\n\n\t \n\tdo {\n\t\tusleep_range(1000, 2000);\n\t\ttxdctl = IXGBE_READ_REG(hw, IXGBE_TXDCTL(reg_idx));\n\t} while (--wait_loop && !(txdctl & IXGBE_TXDCTL_ENABLE));\n\tif (!wait_loop)\n\t\thw_dbg(hw, \"Could not enable Tx Queue %d\\n\", reg_idx);\n}\n\nstatic void ixgbe_setup_mtqc(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 rttdcs, mtqc;\n\tu8 tcs = adapter->hw_tcs;\n\n\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\treturn;\n\n\t \n\trttdcs = IXGBE_READ_REG(hw, IXGBE_RTTDCS);\n\trttdcs |= IXGBE_RTTDCS_ARBDIS;\n\tIXGBE_WRITE_REG(hw, IXGBE_RTTDCS, rttdcs);\n\n\t \n\tif (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED) {\n\t\tmtqc = IXGBE_MTQC_VT_ENA;\n\t\tif (tcs > 4)\n\t\t\tmtqc |= IXGBE_MTQC_RT_ENA | IXGBE_MTQC_8TC_8TQ;\n\t\telse if (tcs > 1)\n\t\t\tmtqc |= IXGBE_MTQC_RT_ENA | IXGBE_MTQC_4TC_4TQ;\n\t\telse if (adapter->ring_feature[RING_F_VMDQ].mask ==\n\t\t\t IXGBE_82599_VMDQ_4Q_MASK)\n\t\t\tmtqc |= IXGBE_MTQC_32VF;\n\t\telse\n\t\t\tmtqc |= IXGBE_MTQC_64VF;\n\t} else {\n\t\tif (tcs > 4) {\n\t\t\tmtqc = IXGBE_MTQC_RT_ENA | IXGBE_MTQC_8TC_8TQ;\n\t\t} else if (tcs > 1) {\n\t\t\tmtqc = IXGBE_MTQC_RT_ENA | IXGBE_MTQC_4TC_4TQ;\n\t\t} else {\n\t\t\tu8 max_txq = adapter->num_tx_queues +\n\t\t\t\tadapter->num_xdp_queues;\n\t\t\tif (max_txq > 63)\n\t\t\t\tmtqc = IXGBE_MTQC_RT_ENA | IXGBE_MTQC_4TC_4TQ;\n\t\t\telse\n\t\t\t\tmtqc = IXGBE_MTQC_64Q_1PB;\n\t\t}\n\t}\n\n\tIXGBE_WRITE_REG(hw, IXGBE_MTQC, mtqc);\n\n\t \n\tif (tcs) {\n\t\tu32 sectx = IXGBE_READ_REG(hw, IXGBE_SECTXMINIFG);\n\t\tsectx |= IXGBE_SECTX_DCB;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_SECTXMINIFG, sectx);\n\t}\n\n\t \n\trttdcs &= ~IXGBE_RTTDCS_ARBDIS;\n\tIXGBE_WRITE_REG(hw, IXGBE_RTTDCS, rttdcs);\n}\n\n \nstatic void ixgbe_configure_tx(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 dmatxctl;\n\tu32 i;\n\n\tixgbe_setup_mtqc(adapter);\n\n\tif (hw->mac.type != ixgbe_mac_82598EB) {\n\t\t \n\t\tdmatxctl = IXGBE_READ_REG(hw, IXGBE_DMATXCTL);\n\t\tdmatxctl |= IXGBE_DMATXCTL_TE;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_DMATXCTL, dmatxctl);\n\t}\n\n\t \n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tixgbe_configure_tx_ring(adapter, adapter->tx_ring[i]);\n\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\tixgbe_configure_tx_ring(adapter, adapter->xdp_ring[i]);\n}\n\nstatic void ixgbe_enable_rx_drop(struct ixgbe_adapter *adapter,\n\t\t\t\t struct ixgbe_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu8 reg_idx = ring->reg_idx;\n\tu32 srrctl = IXGBE_READ_REG(hw, IXGBE_SRRCTL(reg_idx));\n\n\tsrrctl |= IXGBE_SRRCTL_DROP_EN;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_SRRCTL(reg_idx), srrctl);\n}\n\nstatic void ixgbe_disable_rx_drop(struct ixgbe_adapter *adapter,\n\t\t\t\t  struct ixgbe_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu8 reg_idx = ring->reg_idx;\n\tu32 srrctl = IXGBE_READ_REG(hw, IXGBE_SRRCTL(reg_idx));\n\n\tsrrctl &= ~IXGBE_SRRCTL_DROP_EN;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_SRRCTL(reg_idx), srrctl);\n}\n\n#ifdef CONFIG_IXGBE_DCB\nvoid ixgbe_set_rx_drop_en(struct ixgbe_adapter *adapter)\n#else\nstatic void ixgbe_set_rx_drop_en(struct ixgbe_adapter *adapter)\n#endif\n{\n\tint i;\n\tbool pfc_en = adapter->dcb_cfg.pfc_mode_enable;\n\n\tif (adapter->ixgbe_ieee_pfc)\n\t\tpfc_en |= !!(adapter->ixgbe_ieee_pfc->pfc_en);\n\n\t \n\tif (adapter->num_vfs || (adapter->num_rx_queues > 1 &&\n\t    !(adapter->hw.fc.current_mode & ixgbe_fc_tx_pause) && !pfc_en)) {\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\t\tixgbe_enable_rx_drop(adapter, adapter->rx_ring[i]);\n\t} else {\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\t\tixgbe_disable_rx_drop(adapter, adapter->rx_ring[i]);\n\t}\n}\n\n#define IXGBE_SRRCTL_BSIZEHDRSIZE_SHIFT 2\n\nstatic void ixgbe_configure_srrctl(struct ixgbe_adapter *adapter,\n\t\t\t\t   struct ixgbe_ring *rx_ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 srrctl;\n\tu8 reg_idx = rx_ring->reg_idx;\n\n\tif (hw->mac.type == ixgbe_mac_82598EB) {\n\t\tu16 mask = adapter->ring_feature[RING_F_RSS].mask;\n\n\t\t \n\t\treg_idx &= mask;\n\t}\n\n\t \n\tsrrctl = IXGBE_RX_HDR_SIZE << IXGBE_SRRCTL_BSIZEHDRSIZE_SHIFT;\n\n\t \n\tif (rx_ring->xsk_pool) {\n\t\tu32 xsk_buf_len = xsk_pool_get_rx_frame_size(rx_ring->xsk_pool);\n\n\t\t \n\t\tif (hw->mac.type != ixgbe_mac_82599EB)\n\t\t\tsrrctl |= PAGE_SIZE >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;\n\t\telse\n\t\t\tsrrctl |= xsk_buf_len >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;\n\t} else if (test_bit(__IXGBE_RX_3K_BUFFER, &rx_ring->state)) {\n\t\tsrrctl |= IXGBE_RXBUFFER_3K >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;\n\t} else {\n\t\tsrrctl |= IXGBE_RXBUFFER_2K >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;\n\t}\n\n\t \n\tsrrctl |= IXGBE_SRRCTL_DESCTYPE_ADV_ONEBUF;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_SRRCTL(reg_idx), srrctl);\n}\n\n \nu32 ixgbe_rss_indir_tbl_entries(struct ixgbe_adapter *adapter)\n{\n\tif (adapter->hw.mac.type < ixgbe_mac_X550)\n\t\treturn 128;\n\telse if (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED)\n\t\treturn 64;\n\telse\n\t\treturn 512;\n}\n\n \nvoid ixgbe_store_key(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\tfor (i = 0; i < 10; i++)\n\t\tIXGBE_WRITE_REG(hw, IXGBE_RSSRK(i), adapter->rss_key[i]);\n}\n\n \nstatic inline int ixgbe_init_rss_key(struct ixgbe_adapter *adapter)\n{\n\tu32 *rss_key;\n\n\tif (!adapter->rss_key) {\n\t\trss_key = kzalloc(IXGBE_RSS_KEY_SIZE, GFP_KERNEL);\n\t\tif (unlikely(!rss_key))\n\t\t\treturn -ENOMEM;\n\n\t\tnetdev_rss_key_fill(rss_key, IXGBE_RSS_KEY_SIZE);\n\t\tadapter->rss_key = rss_key;\n\t}\n\n\treturn 0;\n}\n\n \nvoid ixgbe_store_reta(struct ixgbe_adapter *adapter)\n{\n\tu32 i, reta_entries = ixgbe_rss_indir_tbl_entries(adapter);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 reta = 0;\n\tu32 indices_multi;\n\tu8 *indir_tbl = adapter->rss_indir_tbl;\n\n\t \n\tif (adapter->hw.mac.type == ixgbe_mac_82598EB)\n\t\tindices_multi = 0x11;\n\telse\n\t\tindices_multi = 0x1;\n\n\t \n\tfor (i = 0; i < reta_entries; i++) {\n\t\treta |= indices_multi * indir_tbl[i] << (i & 0x3) * 8;\n\t\tif ((i & 3) == 3) {\n\t\t\tif (i < 128)\n\t\t\t\tIXGBE_WRITE_REG(hw, IXGBE_RETA(i >> 2), reta);\n\t\t\telse\n\t\t\t\tIXGBE_WRITE_REG(hw, IXGBE_ERETA((i >> 2) - 32),\n\t\t\t\t\t\treta);\n\t\t\treta = 0;\n\t\t}\n\t}\n}\n\n \nstatic void ixgbe_store_vfreta(struct ixgbe_adapter *adapter)\n{\n\tu32 i, reta_entries = ixgbe_rss_indir_tbl_entries(adapter);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 vfreta = 0;\n\n\t \n\tfor (i = 0; i < reta_entries; i++) {\n\t\tu16 pool = adapter->num_rx_pools;\n\n\t\tvfreta |= (u32)adapter->rss_indir_tbl[i] << (i & 0x3) * 8;\n\t\tif ((i & 3) != 3)\n\t\t\tcontinue;\n\n\t\twhile (pool--)\n\t\t\tIXGBE_WRITE_REG(hw,\n\t\t\t\t\tIXGBE_PFVFRETA(i >> 2, VMDQ_P(pool)),\n\t\t\t\t\tvfreta);\n\t\tvfreta = 0;\n\t}\n}\n\nstatic void ixgbe_setup_reta(struct ixgbe_adapter *adapter)\n{\n\tu32 i, j;\n\tu32 reta_entries = ixgbe_rss_indir_tbl_entries(adapter);\n\tu16 rss_i = adapter->ring_feature[RING_F_RSS].indices;\n\n\t \n\tif ((adapter->flags & IXGBE_FLAG_SRIOV_ENABLED) && (rss_i < 4))\n\t\trss_i = 4;\n\n\t \n\tixgbe_store_key(adapter);\n\n\t \n\tmemset(adapter->rss_indir_tbl, 0, sizeof(adapter->rss_indir_tbl));\n\n\tfor (i = 0, j = 0; i < reta_entries; i++, j++) {\n\t\tif (j == rss_i)\n\t\t\tj = 0;\n\n\t\tadapter->rss_indir_tbl[i] = j;\n\t}\n\n\tixgbe_store_reta(adapter);\n}\n\nstatic void ixgbe_setup_vfreta(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu16 rss_i = adapter->ring_feature[RING_F_RSS].indices;\n\tint i, j;\n\n\t \n\tfor (i = 0; i < 10; i++) {\n\t\tu16 pool = adapter->num_rx_pools;\n\n\t\twhile (pool--)\n\t\t\tIXGBE_WRITE_REG(hw,\n\t\t\t\t\tIXGBE_PFVFRSSRK(i, VMDQ_P(pool)),\n\t\t\t\t\t*(adapter->rss_key + i));\n\t}\n\n\t \n\tfor (i = 0, j = 0; i < 64; i++, j++) {\n\t\tif (j == rss_i)\n\t\t\tj = 0;\n\n\t\tadapter->rss_indir_tbl[i] = j;\n\t}\n\n\tixgbe_store_vfreta(adapter);\n}\n\nstatic void ixgbe_setup_mrqc(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 mrqc = 0, rss_field = 0, vfmrqc = 0;\n\tu32 rxcsum;\n\n\t \n\trxcsum = IXGBE_READ_REG(hw, IXGBE_RXCSUM);\n\trxcsum |= IXGBE_RXCSUM_PCSD;\n\tIXGBE_WRITE_REG(hw, IXGBE_RXCSUM, rxcsum);\n\n\tif (adapter->hw.mac.type == ixgbe_mac_82598EB) {\n\t\tif (adapter->ring_feature[RING_F_RSS].mask)\n\t\t\tmrqc = IXGBE_MRQC_RSSEN;\n\t} else {\n\t\tu8 tcs = adapter->hw_tcs;\n\n\t\tif (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED) {\n\t\t\tif (tcs > 4)\n\t\t\t\tmrqc = IXGBE_MRQC_VMDQRT8TCEN;\t \n\t\t\telse if (tcs > 1)\n\t\t\t\tmrqc = IXGBE_MRQC_VMDQRT4TCEN;\t \n\t\t\telse if (adapter->ring_feature[RING_F_VMDQ].mask ==\n\t\t\t\t IXGBE_82599_VMDQ_4Q_MASK)\n\t\t\t\tmrqc = IXGBE_MRQC_VMDQRSS32EN;\n\t\t\telse\n\t\t\t\tmrqc = IXGBE_MRQC_VMDQRSS64EN;\n\n\t\t\t \n\t\t\tif (hw->mac.type >= ixgbe_mac_X550)\n\t\t\t\tmrqc |= IXGBE_MRQC_L3L4TXSWEN;\n\t\t} else {\n\t\t\tif (tcs > 4)\n\t\t\t\tmrqc = IXGBE_MRQC_RTRSS8TCEN;\n\t\t\telse if (tcs > 1)\n\t\t\t\tmrqc = IXGBE_MRQC_RTRSS4TCEN;\n\t\t\telse\n\t\t\t\tmrqc = IXGBE_MRQC_RSSEN;\n\t\t}\n\t}\n\n\t \n\trss_field |= IXGBE_MRQC_RSS_FIELD_IPV4 |\n\t\t     IXGBE_MRQC_RSS_FIELD_IPV4_TCP |\n\t\t     IXGBE_MRQC_RSS_FIELD_IPV6 |\n\t\t     IXGBE_MRQC_RSS_FIELD_IPV6_TCP;\n\n\tif (adapter->flags2 & IXGBE_FLAG2_RSS_FIELD_IPV4_UDP)\n\t\trss_field |= IXGBE_MRQC_RSS_FIELD_IPV4_UDP;\n\tif (adapter->flags2 & IXGBE_FLAG2_RSS_FIELD_IPV6_UDP)\n\t\trss_field |= IXGBE_MRQC_RSS_FIELD_IPV6_UDP;\n\n\tif ((hw->mac.type >= ixgbe_mac_X550) &&\n\t    (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED)) {\n\t\tu16 pool = adapter->num_rx_pools;\n\n\t\t \n\t\tmrqc |= IXGBE_MRQC_MULTIPLE_RSS;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_MRQC, mrqc);\n\n\t\t \n\t\tixgbe_setup_vfreta(adapter);\n\t\tvfmrqc = IXGBE_MRQC_RSSEN;\n\t\tvfmrqc |= rss_field;\n\n\t\twhile (pool--)\n\t\t\tIXGBE_WRITE_REG(hw,\n\t\t\t\t\tIXGBE_PFVFMRQC(VMDQ_P(pool)),\n\t\t\t\t\tvfmrqc);\n\t} else {\n\t\tixgbe_setup_reta(adapter);\n\t\tmrqc |= rss_field;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_MRQC, mrqc);\n\t}\n}\n\n \nstatic void ixgbe_configure_rscctl(struct ixgbe_adapter *adapter,\n\t\t\t\t   struct ixgbe_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 rscctrl;\n\tu8 reg_idx = ring->reg_idx;\n\n\tif (!ring_is_rsc_enabled(ring))\n\t\treturn;\n\n\trscctrl = IXGBE_READ_REG(hw, IXGBE_RSCCTL(reg_idx));\n\trscctrl |= IXGBE_RSCCTL_RSCEN;\n\t \n\trscctrl |= IXGBE_RSCCTL_MAXDESC_16;\n\tIXGBE_WRITE_REG(hw, IXGBE_RSCCTL(reg_idx), rscctrl);\n}\n\n#define IXGBE_MAX_RX_DESC_POLL 10\nstatic void ixgbe_rx_desc_queue_enable(struct ixgbe_adapter *adapter,\n\t\t\t\t       struct ixgbe_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint wait_loop = IXGBE_MAX_RX_DESC_POLL;\n\tu32 rxdctl;\n\tu8 reg_idx = ring->reg_idx;\n\n\tif (ixgbe_removed(hw->hw_addr))\n\t\treturn;\n\t \n\tif (hw->mac.type == ixgbe_mac_82598EB &&\n\t    !(IXGBE_READ_REG(hw, IXGBE_LINKS) & IXGBE_LINKS_UP))\n\t\treturn;\n\n\tdo {\n\t\tusleep_range(1000, 2000);\n\t\trxdctl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(reg_idx));\n\t} while (--wait_loop && !(rxdctl & IXGBE_RXDCTL_ENABLE));\n\n\tif (!wait_loop) {\n\t\te_err(drv, \"RXDCTL.ENABLE on Rx queue %d not set within \"\n\t\t      \"the polling period\\n\", reg_idx);\n\t}\n}\n\nvoid ixgbe_configure_rx_ring(struct ixgbe_adapter *adapter,\n\t\t\t     struct ixgbe_ring *ring)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tunion ixgbe_adv_rx_desc *rx_desc;\n\tu64 rdba = ring->dma;\n\tu32 rxdctl;\n\tu8 reg_idx = ring->reg_idx;\n\n\txdp_rxq_info_unreg_mem_model(&ring->xdp_rxq);\n\tring->xsk_pool = ixgbe_xsk_pool(adapter, ring);\n\tif (ring->xsk_pool) {\n\t\tWARN_ON(xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,\n\t\t\t\t\t\t   MEM_TYPE_XSK_BUFF_POOL,\n\t\t\t\t\t\t   NULL));\n\t\txsk_pool_set_rxq_info(ring->xsk_pool, &ring->xdp_rxq);\n\t} else {\n\t\tWARN_ON(xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,\n\t\t\t\t\t\t   MEM_TYPE_PAGE_SHARED, NULL));\n\t}\n\n\t \n\trxdctl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(reg_idx));\n\trxdctl &= ~IXGBE_RXDCTL_ENABLE;\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_RXDCTL(reg_idx), rxdctl);\n\tIXGBE_WRITE_FLUSH(hw);\n\n\tIXGBE_WRITE_REG(hw, IXGBE_RDBAL(reg_idx), (rdba & DMA_BIT_MASK(32)));\n\tIXGBE_WRITE_REG(hw, IXGBE_RDBAH(reg_idx), (rdba >> 32));\n\tIXGBE_WRITE_REG(hw, IXGBE_RDLEN(reg_idx),\n\t\t\tring->count * sizeof(union ixgbe_adv_rx_desc));\n\t \n\tIXGBE_WRITE_FLUSH(hw);\n\n\tIXGBE_WRITE_REG(hw, IXGBE_RDH(reg_idx), 0);\n\tIXGBE_WRITE_REG(hw, IXGBE_RDT(reg_idx), 0);\n\tring->tail = adapter->io_addr + IXGBE_RDT(reg_idx);\n\n\tixgbe_configure_srrctl(adapter, ring);\n\tixgbe_configure_rscctl(adapter, ring);\n\n\tif (hw->mac.type == ixgbe_mac_82598EB) {\n\t\t \n\t\trxdctl &= ~0x3FFFFF;\n\t\trxdctl |=  0x080420;\n#if (PAGE_SIZE < 8192)\n\t \n\t} else if (hw->mac.type != ixgbe_mac_82599EB) {\n\t\trxdctl &= ~(IXGBE_RXDCTL_RLPMLMASK |\n\t\t\t    IXGBE_RXDCTL_RLPML_EN);\n\n\t\t \n\t\tif (ring_uses_build_skb(ring) &&\n\t\t    !test_bit(__IXGBE_RX_3K_BUFFER, &ring->state))\n\t\t\trxdctl |= IXGBE_MAX_2K_FRAME_BUILD_SKB |\n\t\t\t\t  IXGBE_RXDCTL_RLPML_EN;\n#endif\n\t}\n\n\tring->rx_offset = ixgbe_rx_offset(ring);\n\n\tif (ring->xsk_pool && hw->mac.type != ixgbe_mac_82599EB) {\n\t\tu32 xsk_buf_len = xsk_pool_get_rx_frame_size(ring->xsk_pool);\n\n\t\trxdctl &= ~(IXGBE_RXDCTL_RLPMLMASK |\n\t\t\t    IXGBE_RXDCTL_RLPML_EN);\n\t\trxdctl |= xsk_buf_len | IXGBE_RXDCTL_RLPML_EN;\n\n\t\tring->rx_buf_len = xsk_buf_len;\n\t}\n\n\t \n\tmemset(ring->rx_buffer_info, 0,\n\t       sizeof(struct ixgbe_rx_buffer) * ring->count);\n\n\t \n\trx_desc = IXGBE_RX_DESC(ring, 0);\n\trx_desc->wb.upper.length = 0;\n\n\t \n\trxdctl |= IXGBE_RXDCTL_ENABLE;\n\tIXGBE_WRITE_REG(hw, IXGBE_RXDCTL(reg_idx), rxdctl);\n\n\tixgbe_rx_desc_queue_enable(adapter, ring);\n\tif (ring->xsk_pool)\n\t\tixgbe_alloc_rx_buffers_zc(ring, ixgbe_desc_unused(ring));\n\telse\n\t\tixgbe_alloc_rx_buffers(ring, ixgbe_desc_unused(ring));\n}\n\nstatic void ixgbe_setup_psrtype(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint rss_i = adapter->ring_feature[RING_F_RSS].indices;\n\tu16 pool = adapter->num_rx_pools;\n\n\t \n\tu32 psrtype = IXGBE_PSRTYPE_TCPHDR |\n\t\t      IXGBE_PSRTYPE_UDPHDR |\n\t\t      IXGBE_PSRTYPE_IPV4HDR |\n\t\t      IXGBE_PSRTYPE_L2HDR |\n\t\t      IXGBE_PSRTYPE_IPV6HDR;\n\n\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\treturn;\n\n\tif (rss_i > 3)\n\t\tpsrtype |= 2u << 29;\n\telse if (rss_i > 1)\n\t\tpsrtype |= 1u << 29;\n\n\twhile (pool--)\n\t\tIXGBE_WRITE_REG(hw, IXGBE_PSRTYPE(VMDQ_P(pool)), psrtype);\n}\n\nstatic void ixgbe_configure_virtualization(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu16 pool = adapter->num_rx_pools;\n\tu32 reg_offset, vf_shift, vmolr;\n\tu32 gcr_ext, vmdctl;\n\tint i;\n\n\tif (!(adapter->flags & IXGBE_FLAG_SRIOV_ENABLED))\n\t\treturn;\n\n\tvmdctl = IXGBE_READ_REG(hw, IXGBE_VT_CTL);\n\tvmdctl |= IXGBE_VMD_CTL_VMDQ_EN;\n\tvmdctl &= ~IXGBE_VT_CTL_POOL_MASK;\n\tvmdctl |= VMDQ_P(0) << IXGBE_VT_CTL_POOL_SHIFT;\n\tvmdctl |= IXGBE_VT_CTL_REPLEN;\n\tIXGBE_WRITE_REG(hw, IXGBE_VT_CTL, vmdctl);\n\n\t \n\tvmolr = IXGBE_VMOLR_AUPE;\n\twhile (pool--)\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VMOLR(VMDQ_P(pool)), vmolr);\n\n\tvf_shift = VMDQ_P(0) % 32;\n\treg_offset = (VMDQ_P(0) >= 32) ? 1 : 0;\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_VFRE(reg_offset), GENMASK(31, vf_shift));\n\tIXGBE_WRITE_REG(hw, IXGBE_VFRE(reg_offset ^ 1), reg_offset - 1);\n\tIXGBE_WRITE_REG(hw, IXGBE_VFTE(reg_offset), GENMASK(31, vf_shift));\n\tIXGBE_WRITE_REG(hw, IXGBE_VFTE(reg_offset ^ 1), reg_offset - 1);\n\tif (adapter->bridge_mode == BRIDGE_MODE_VEB)\n\t\tIXGBE_WRITE_REG(hw, IXGBE_PFDTXGSWC, IXGBE_PFDTXGSWC_VT_LBEN);\n\n\t \n\thw->mac.ops.set_vmdq(hw, 0, VMDQ_P(0));\n\n\t \n\tadapter->flags2 &= ~IXGBE_FLAG2_VLAN_PROMISC;\n\n\t \n\tswitch (adapter->ring_feature[RING_F_VMDQ].mask) {\n\tcase IXGBE_82599_VMDQ_8Q_MASK:\n\t\tgcr_ext = IXGBE_GCR_EXT_VT_MODE_16;\n\t\tbreak;\n\tcase IXGBE_82599_VMDQ_4Q_MASK:\n\t\tgcr_ext = IXGBE_GCR_EXT_VT_MODE_32;\n\t\tbreak;\n\tdefault:\n\t\tgcr_ext = IXGBE_GCR_EXT_VT_MODE_64;\n\t\tbreak;\n\t}\n\n\tIXGBE_WRITE_REG(hw, IXGBE_GCR_EXT, gcr_ext);\n\n\tfor (i = 0; i < adapter->num_vfs; i++) {\n\t\t \n\t\tixgbe_ndo_set_vf_spoofchk(adapter->netdev, i,\n\t\t\t\t\t  adapter->vfinfo[i].spoofchk_enabled);\n\n\t\t \n\t\tixgbe_ndo_set_vf_rss_query_en(adapter->netdev, i,\n\t\t\t\t\t  adapter->vfinfo[i].rss_query_enabled);\n\t}\n}\n\nstatic void ixgbe_set_rx_buffer_len(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct net_device *netdev = adapter->netdev;\n\tint max_frame = netdev->mtu + ETH_HLEN + ETH_FCS_LEN;\n\tstruct ixgbe_ring *rx_ring;\n\tint i;\n\tu32 mhadd, hlreg0;\n\n#ifdef IXGBE_FCOE\n\t \n\tif ((adapter->flags & IXGBE_FLAG_FCOE_ENABLED) &&\n\t    (max_frame < IXGBE_FCOE_JUMBO_FRAME_SIZE))\n\t\tmax_frame = IXGBE_FCOE_JUMBO_FRAME_SIZE;\n\n#endif  \n\n\t \n\tif (max_frame < (ETH_FRAME_LEN + ETH_FCS_LEN))\n\t\tmax_frame = (ETH_FRAME_LEN + ETH_FCS_LEN);\n\n\tmhadd = IXGBE_READ_REG(hw, IXGBE_MHADD);\n\tif (max_frame != (mhadd >> IXGBE_MHADD_MFS_SHIFT)) {\n\t\tmhadd &= ~IXGBE_MHADD_MFS_MASK;\n\t\tmhadd |= max_frame << IXGBE_MHADD_MFS_SHIFT;\n\n\t\tIXGBE_WRITE_REG(hw, IXGBE_MHADD, mhadd);\n\t}\n\n\thlreg0 = IXGBE_READ_REG(hw, IXGBE_HLREG0);\n\t \n\thlreg0 |= IXGBE_HLREG0_JUMBOEN;\n\tIXGBE_WRITE_REG(hw, IXGBE_HLREG0, hlreg0);\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\trx_ring = adapter->rx_ring[i];\n\n\t\tclear_ring_rsc_enabled(rx_ring);\n\t\tclear_bit(__IXGBE_RX_3K_BUFFER, &rx_ring->state);\n\t\tclear_bit(__IXGBE_RX_BUILD_SKB_ENABLED, &rx_ring->state);\n\n\t\tif (adapter->flags2 & IXGBE_FLAG2_RSC_ENABLED)\n\t\t\tset_ring_rsc_enabled(rx_ring);\n\n\t\tif (test_bit(__IXGBE_RX_FCOE, &rx_ring->state))\n\t\t\tset_bit(__IXGBE_RX_3K_BUFFER, &rx_ring->state);\n\n\t\tif (adapter->flags2 & IXGBE_FLAG2_RX_LEGACY)\n\t\t\tcontinue;\n\n\t\tset_bit(__IXGBE_RX_BUILD_SKB_ENABLED, &rx_ring->state);\n\n#if (PAGE_SIZE < 8192)\n\t\tif (adapter->flags2 & IXGBE_FLAG2_RSC_ENABLED)\n\t\t\tset_bit(__IXGBE_RX_3K_BUFFER, &rx_ring->state);\n\n\t\tif (IXGBE_2K_TOO_SMALL_WITH_PADDING ||\n\t\t    (max_frame > (ETH_FRAME_LEN + ETH_FCS_LEN)))\n\t\t\tset_bit(__IXGBE_RX_3K_BUFFER, &rx_ring->state);\n#endif\n\t}\n}\n\nstatic void ixgbe_setup_rdrxctl(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 rdrxctl = IXGBE_READ_REG(hw, IXGBE_RDRXCTL);\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\t \n\t\trdrxctl |= IXGBE_RDRXCTL_MVMEN;\n\t\tbreak;\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tif (adapter->num_vfs)\n\t\t\trdrxctl |= IXGBE_RDRXCTL_PSP;\n\t\tfallthrough;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\t\t \n\t\tIXGBE_WRITE_REG(hw, IXGBE_RSCDBU,\n\t\t   (IXGBE_RSCDBU_RSCACKDIS | IXGBE_READ_REG(hw, IXGBE_RSCDBU)));\n\t\trdrxctl &= ~IXGBE_RDRXCTL_RSCFRSTSIZE;\n\t\t \n\t\trdrxctl |= (IXGBE_RDRXCTL_RSCACKC | IXGBE_RDRXCTL_FCOE_WRFIX);\n\t\trdrxctl |= IXGBE_RDRXCTL_CRCSTRIP;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn;\n\t}\n\n\tIXGBE_WRITE_REG(hw, IXGBE_RDRXCTL, rdrxctl);\n}\n\n \nstatic void ixgbe_configure_rx(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\tu32 rxctrl, rfctl;\n\n\t \n\thw->mac.ops.disable_rx(hw);\n\n\tixgbe_setup_psrtype(adapter);\n\tixgbe_setup_rdrxctl(adapter);\n\n\t \n\trfctl = IXGBE_READ_REG(hw, IXGBE_RFCTL);\n\trfctl &= ~IXGBE_RFCTL_RSC_DIS;\n\tif (!(adapter->flags2 & IXGBE_FLAG2_RSC_ENABLED))\n\t\trfctl |= IXGBE_RFCTL_RSC_DIS;\n\n\t \n\trfctl |= (IXGBE_RFCTL_NFSW_DIS | IXGBE_RFCTL_NFSR_DIS);\n\tIXGBE_WRITE_REG(hw, IXGBE_RFCTL, rfctl);\n\n\t \n\tixgbe_setup_mrqc(adapter);\n\n\t \n\tixgbe_set_rx_buffer_len(adapter);\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tixgbe_configure_rx_ring(adapter, adapter->rx_ring[i]);\n\n\trxctrl = IXGBE_READ_REG(hw, IXGBE_RXCTRL);\n\t \n\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\trxctrl |= IXGBE_RXCTRL_DMBYPS;\n\n\t \n\trxctrl |= IXGBE_RXCTRL_RXEN;\n\thw->mac.ops.enable_rx_dma(hw, rxctrl);\n}\n\nstatic int ixgbe_vlan_rx_add_vid(struct net_device *netdev,\n\t\t\t\t __be16 proto, u16 vid)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\t \n\tif (!vid || !(adapter->flags2 & IXGBE_FLAG2_VLAN_PROMISC))\n\t\thw->mac.ops.set_vfta(&adapter->hw, vid, VMDQ_P(0), true, !!vid);\n\n\tset_bit(vid, adapter->active_vlans);\n\n\treturn 0;\n}\n\nstatic int ixgbe_find_vlvf_entry(struct ixgbe_hw *hw, u32 vlan)\n{\n\tu32 vlvf;\n\tint idx;\n\n\t \n\tif (vlan == 0)\n\t\treturn 0;\n\n\t \n\tfor (idx = IXGBE_VLVF_ENTRIES; --idx;) {\n\t\tvlvf = IXGBE_READ_REG(hw, IXGBE_VLVF(idx));\n\t\tif ((vlvf & VLAN_VID_MASK) == vlan)\n\t\t\tbreak;\n\t}\n\n\treturn idx;\n}\n\nvoid ixgbe_update_pf_promisc_vlvf(struct ixgbe_adapter *adapter, u32 vid)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 bits, word;\n\tint idx;\n\n\tidx = ixgbe_find_vlvf_entry(hw, vid);\n\tif (!idx)\n\t\treturn;\n\n\t \n\tword = idx * 2 + (VMDQ_P(0) / 32);\n\tbits = ~BIT(VMDQ_P(0) % 32);\n\tbits &= IXGBE_READ_REG(hw, IXGBE_VLVFB(word));\n\n\t \n\tif (!bits && !IXGBE_READ_REG(hw, IXGBE_VLVFB(word ^ 1))) {\n\t\tif (!(adapter->flags2 & IXGBE_FLAG2_VLAN_PROMISC))\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_VLVFB(word), 0);\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VLVF(idx), 0);\n\t}\n}\n\nstatic int ixgbe_vlan_rx_kill_vid(struct net_device *netdev,\n\t\t\t\t  __be16 proto, u16 vid)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\t \n\tif (vid && !(adapter->flags2 & IXGBE_FLAG2_VLAN_PROMISC))\n\t\thw->mac.ops.set_vfta(hw, vid, VMDQ_P(0), false, true);\n\n\tclear_bit(vid, adapter->active_vlans);\n\n\treturn 0;\n}\n\n \nstatic void ixgbe_vlan_strip_disable(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 vlnctrl;\n\tint i, j;\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tvlnctrl = IXGBE_READ_REG(hw, IXGBE_VLNCTRL);\n\t\tvlnctrl &= ~IXGBE_VLNCTRL_VME;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VLNCTRL, vlnctrl);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tstruct ixgbe_ring *ring = adapter->rx_ring[i];\n\n\t\t\tif (!netif_is_ixgbe(ring->netdev))\n\t\t\t\tcontinue;\n\n\t\t\tj = ring->reg_idx;\n\t\t\tvlnctrl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(j));\n\t\t\tvlnctrl &= ~IXGBE_RXDCTL_VME;\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_RXDCTL(j), vlnctrl);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n \nstatic void ixgbe_vlan_strip_enable(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 vlnctrl;\n\tint i, j;\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tvlnctrl = IXGBE_READ_REG(hw, IXGBE_VLNCTRL);\n\t\tvlnctrl |= IXGBE_VLNCTRL_VME;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VLNCTRL, vlnctrl);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tstruct ixgbe_ring *ring = adapter->rx_ring[i];\n\n\t\t\tif (!netif_is_ixgbe(ring->netdev))\n\t\t\t\tcontinue;\n\n\t\t\tj = ring->reg_idx;\n\t\t\tvlnctrl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(j));\n\t\t\tvlnctrl |= IXGBE_RXDCTL_VME;\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_RXDCTL(j), vlnctrl);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void ixgbe_vlan_promisc_enable(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 vlnctrl, i;\n\n\tvlnctrl = IXGBE_READ_REG(hw, IXGBE_VLNCTRL);\n\n\tif (adapter->flags & IXGBE_FLAG_VMDQ_ENABLED) {\n\t \n\t\tvlnctrl |= IXGBE_VLNCTRL_VFE;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VLNCTRL, vlnctrl);\n\t} else {\n\t\tvlnctrl &= ~IXGBE_VLNCTRL_VFE;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VLNCTRL, vlnctrl);\n\t\treturn;\n\t}\n\n\t \n\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\treturn;\n\n\t \n\tif (adapter->flags2 & IXGBE_FLAG2_VLAN_PROMISC)\n\t\treturn;\n\n\t \n\tadapter->flags2 |= IXGBE_FLAG2_VLAN_PROMISC;\n\n\t \n\tfor (i = IXGBE_VLVF_ENTRIES; --i;) {\n\t\tu32 reg_offset = IXGBE_VLVFB(i * 2 + VMDQ_P(0) / 32);\n\t\tu32 vlvfb = IXGBE_READ_REG(hw, reg_offset);\n\n\t\tvlvfb |= BIT(VMDQ_P(0) % 32);\n\t\tIXGBE_WRITE_REG(hw, reg_offset, vlvfb);\n\t}\n\n\t \n\tfor (i = hw->mac.vft_size; i--;)\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VFTA(i), ~0U);\n}\n\n#define VFTA_BLOCK_SIZE 8\nstatic void ixgbe_scrub_vfta(struct ixgbe_adapter *adapter, u32 vfta_offset)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 vfta[VFTA_BLOCK_SIZE] = { 0 };\n\tu32 vid_start = vfta_offset * 32;\n\tu32 vid_end = vid_start + (VFTA_BLOCK_SIZE * 32);\n\tu32 i, vid, word, bits;\n\n\tfor (i = IXGBE_VLVF_ENTRIES; --i;) {\n\t\tu32 vlvf = IXGBE_READ_REG(hw, IXGBE_VLVF(i));\n\n\t\t \n\t\tvid = vlvf & VLAN_VID_MASK;\n\n\t\t \n\t\tif (vid < vid_start || vid >= vid_end)\n\t\t\tcontinue;\n\n\t\tif (vlvf) {\n\t\t\t \n\t\t\tvfta[(vid - vid_start) / 32] |= BIT(vid % 32);\n\n\t\t\t \n\t\t\tif (test_bit(vid, adapter->active_vlans))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tword = i * 2 + VMDQ_P(0) / 32;\n\t\tbits = ~BIT(VMDQ_P(0) % 32);\n\t\tbits &= IXGBE_READ_REG(hw, IXGBE_VLVFB(word));\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VLVFB(word), bits);\n\t}\n\n\t \n\tfor (i = VFTA_BLOCK_SIZE; i--;) {\n\t\tvid = (vfta_offset + i) * 32;\n\t\tword = vid / BITS_PER_LONG;\n\t\tbits = vid % BITS_PER_LONG;\n\n\t\tvfta[i] |= adapter->active_vlans[word] >> bits;\n\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VFTA(vfta_offset + i), vfta[i]);\n\t}\n}\n\nstatic void ixgbe_vlan_promisc_disable(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 vlnctrl, i;\n\n\t \n\tvlnctrl = IXGBE_READ_REG(hw, IXGBE_VLNCTRL);\n\tvlnctrl |= IXGBE_VLNCTRL_VFE;\n\tIXGBE_WRITE_REG(hw, IXGBE_VLNCTRL, vlnctrl);\n\n\tif (!(adapter->flags & IXGBE_FLAG_VMDQ_ENABLED) ||\n\t    hw->mac.type == ixgbe_mac_82598EB)\n\t\treturn;\n\n\t \n\tif (!(adapter->flags2 & IXGBE_FLAG2_VLAN_PROMISC))\n\t\treturn;\n\n\t \n\tadapter->flags2 &= ~IXGBE_FLAG2_VLAN_PROMISC;\n\n\tfor (i = 0; i < hw->mac.vft_size; i += VFTA_BLOCK_SIZE)\n\t\tixgbe_scrub_vfta(adapter, i);\n}\n\nstatic void ixgbe_restore_vlan(struct ixgbe_adapter *adapter)\n{\n\tu16 vid = 1;\n\n\tixgbe_vlan_rx_add_vid(adapter->netdev, htons(ETH_P_8021Q), 0);\n\n\tfor_each_set_bit_from(vid, adapter->active_vlans, VLAN_N_VID)\n\t\tixgbe_vlan_rx_add_vid(adapter->netdev, htons(ETH_P_8021Q), vid);\n}\n\n \nstatic int ixgbe_write_mc_addr_list(struct net_device *netdev)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tif (!netif_running(netdev))\n\t\treturn 0;\n\n\tif (hw->mac.ops.update_mc_addr_list)\n\t\thw->mac.ops.update_mc_addr_list(hw, netdev);\n\telse\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_PCI_IOV\n\tixgbe_restore_vf_multicasts(adapter);\n#endif\n\n\treturn netdev_mc_count(netdev);\n}\n\n#ifdef CONFIG_PCI_IOV\nvoid ixgbe_full_sync_mac_table(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_mac_addr *mac_table = &adapter->mac_table[0];\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\tfor (i = 0; i < hw->mac.num_rar_entries; i++, mac_table++) {\n\t\tmac_table->state &= ~IXGBE_MAC_STATE_MODIFIED;\n\n\t\tif (mac_table->state & IXGBE_MAC_STATE_IN_USE)\n\t\t\thw->mac.ops.set_rar(hw, i,\n\t\t\t\t\t    mac_table->addr,\n\t\t\t\t\t    mac_table->pool,\n\t\t\t\t\t    IXGBE_RAH_AV);\n\t\telse\n\t\t\thw->mac.ops.clear_rar(hw, i);\n\t}\n}\n\n#endif\nstatic void ixgbe_sync_mac_table(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_mac_addr *mac_table = &adapter->mac_table[0];\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\tfor (i = 0; i < hw->mac.num_rar_entries; i++, mac_table++) {\n\t\tif (!(mac_table->state & IXGBE_MAC_STATE_MODIFIED))\n\t\t\tcontinue;\n\n\t\tmac_table->state &= ~IXGBE_MAC_STATE_MODIFIED;\n\n\t\tif (mac_table->state & IXGBE_MAC_STATE_IN_USE)\n\t\t\thw->mac.ops.set_rar(hw, i,\n\t\t\t\t\t    mac_table->addr,\n\t\t\t\t\t    mac_table->pool,\n\t\t\t\t\t    IXGBE_RAH_AV);\n\t\telse\n\t\t\thw->mac.ops.clear_rar(hw, i);\n\t}\n}\n\nstatic void ixgbe_flush_sw_mac_table(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_mac_addr *mac_table = &adapter->mac_table[0];\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\tfor (i = 0; i < hw->mac.num_rar_entries; i++, mac_table++) {\n\t\tmac_table->state |= IXGBE_MAC_STATE_MODIFIED;\n\t\tmac_table->state &= ~IXGBE_MAC_STATE_IN_USE;\n\t}\n\n\tixgbe_sync_mac_table(adapter);\n}\n\nstatic int ixgbe_available_rars(struct ixgbe_adapter *adapter, u16 pool)\n{\n\tstruct ixgbe_mac_addr *mac_table = &adapter->mac_table[0];\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i, count = 0;\n\n\tfor (i = 0; i < hw->mac.num_rar_entries; i++, mac_table++) {\n\t\t \n\t\tif (mac_table->state & IXGBE_MAC_STATE_DEFAULT)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (mac_table->state & IXGBE_MAC_STATE_IN_USE) {\n\t\t\tif (mac_table->pool != pool)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tcount++;\n\t}\n\n\treturn count;\n}\n\n \nstatic void ixgbe_mac_set_default_filter(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_mac_addr *mac_table = &adapter->mac_table[0];\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tmemcpy(&mac_table->addr, hw->mac.addr, ETH_ALEN);\n\tmac_table->pool = VMDQ_P(0);\n\n\tmac_table->state = IXGBE_MAC_STATE_DEFAULT | IXGBE_MAC_STATE_IN_USE;\n\n\thw->mac.ops.set_rar(hw, 0, mac_table->addr, mac_table->pool,\n\t\t\t    IXGBE_RAH_AV);\n}\n\nint ixgbe_add_mac_filter(struct ixgbe_adapter *adapter,\n\t\t\t const u8 *addr, u16 pool)\n{\n\tstruct ixgbe_mac_addr *mac_table = &adapter->mac_table[0];\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\tif (is_zero_ether_addr(addr))\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < hw->mac.num_rar_entries; i++, mac_table++) {\n\t\tif (mac_table->state & IXGBE_MAC_STATE_IN_USE)\n\t\t\tcontinue;\n\n\t\tether_addr_copy(mac_table->addr, addr);\n\t\tmac_table->pool = pool;\n\n\t\tmac_table->state |= IXGBE_MAC_STATE_MODIFIED |\n\t\t\t\t    IXGBE_MAC_STATE_IN_USE;\n\n\t\tixgbe_sync_mac_table(adapter);\n\n\t\treturn i;\n\t}\n\n\treturn -ENOMEM;\n}\n\nint ixgbe_del_mac_filter(struct ixgbe_adapter *adapter,\n\t\t\t const u8 *addr, u16 pool)\n{\n\tstruct ixgbe_mac_addr *mac_table = &adapter->mac_table[0];\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\tif (is_zero_ether_addr(addr))\n\t\treturn -EINVAL;\n\n\t \n\tfor (i = 0; i < hw->mac.num_rar_entries; i++, mac_table++) {\n\t\t \n\t\tif (!(mac_table->state & IXGBE_MAC_STATE_IN_USE))\n\t\t\tcontinue;\n\t\t \n\t\tif (mac_table->pool != pool)\n\t\t\tcontinue;\n\t\t \n\t\tif (!ether_addr_equal(addr, mac_table->addr))\n\t\t\tcontinue;\n\n\t\tmac_table->state |= IXGBE_MAC_STATE_MODIFIED;\n\t\tmac_table->state &= ~IXGBE_MAC_STATE_IN_USE;\n\n\t\tixgbe_sync_mac_table(adapter);\n\n\t\treturn 0;\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic int ixgbe_uc_sync(struct net_device *netdev, const unsigned char *addr)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tint ret;\n\n\tret = ixgbe_add_mac_filter(adapter, addr, VMDQ_P(0));\n\n\treturn min_t(int, ret, 0);\n}\n\nstatic int ixgbe_uc_unsync(struct net_device *netdev, const unsigned char *addr)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\n\tixgbe_del_mac_filter(adapter, addr, VMDQ_P(0));\n\n\treturn 0;\n}\n\n \nvoid ixgbe_set_rx_mode(struct net_device *netdev)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 fctrl, vmolr = IXGBE_VMOLR_BAM | IXGBE_VMOLR_AUPE;\n\tnetdev_features_t features = netdev->features;\n\tint count;\n\n\t \n\tfctrl = IXGBE_READ_REG(hw, IXGBE_FCTRL);\n\n\t \n\tfctrl &= ~IXGBE_FCTRL_SBP;  \n\tfctrl |= IXGBE_FCTRL_BAM;\n\tfctrl |= IXGBE_FCTRL_DPF;  \n\tfctrl |= IXGBE_FCTRL_PMCF;\n\n\t \n\tfctrl &= ~(IXGBE_FCTRL_UPE | IXGBE_FCTRL_MPE);\n\tif (netdev->flags & IFF_PROMISC) {\n\t\thw->addr_ctrl.user_set_promisc = true;\n\t\tfctrl |= (IXGBE_FCTRL_UPE | IXGBE_FCTRL_MPE);\n\t\tvmolr |= IXGBE_VMOLR_MPE;\n\t\tfeatures &= ~NETIF_F_HW_VLAN_CTAG_FILTER;\n\t} else {\n\t\tif (netdev->flags & IFF_ALLMULTI) {\n\t\t\tfctrl |= IXGBE_FCTRL_MPE;\n\t\t\tvmolr |= IXGBE_VMOLR_MPE;\n\t\t}\n\t\thw->addr_ctrl.user_set_promisc = false;\n\t}\n\n\t \n\tif (__dev_uc_sync(netdev, ixgbe_uc_sync, ixgbe_uc_unsync)) {\n\t\tfctrl |= IXGBE_FCTRL_UPE;\n\t\tvmolr |= IXGBE_VMOLR_ROPE;\n\t}\n\n\t \n\tcount = ixgbe_write_mc_addr_list(netdev);\n\tif (count < 0) {\n\t\tfctrl |= IXGBE_FCTRL_MPE;\n\t\tvmolr |= IXGBE_VMOLR_MPE;\n\t} else if (count) {\n\t\tvmolr |= IXGBE_VMOLR_ROMPE;\n\t}\n\n\tif (hw->mac.type != ixgbe_mac_82598EB) {\n\t\tvmolr |= IXGBE_READ_REG(hw, IXGBE_VMOLR(VMDQ_P(0))) &\n\t\t\t ~(IXGBE_VMOLR_MPE | IXGBE_VMOLR_ROMPE |\n\t\t\t   IXGBE_VMOLR_ROPE);\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VMOLR(VMDQ_P(0)), vmolr);\n\t}\n\n\t \n\tif (features & NETIF_F_RXALL) {\n\t\t \n\t\tfctrl |= (IXGBE_FCTRL_SBP |  \n\t\t\t  IXGBE_FCTRL_BAM |  \n\t\t\t  IXGBE_FCTRL_PMCF);  \n\n\t\tfctrl &= ~(IXGBE_FCTRL_DPF);\n\t\t \n\t}\n\n\tIXGBE_WRITE_REG(hw, IXGBE_FCTRL, fctrl);\n\n\tif (features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tixgbe_vlan_strip_enable(adapter);\n\telse\n\t\tixgbe_vlan_strip_disable(adapter);\n\n\tif (features & NETIF_F_HW_VLAN_CTAG_FILTER)\n\t\tixgbe_vlan_promisc_disable(adapter);\n\telse\n\t\tixgbe_vlan_promisc_enable(adapter);\n}\n\nstatic void ixgbe_napi_enable_all(struct ixgbe_adapter *adapter)\n{\n\tint q_idx;\n\n\tfor (q_idx = 0; q_idx < adapter->num_q_vectors; q_idx++)\n\t\tnapi_enable(&adapter->q_vector[q_idx]->napi);\n}\n\nstatic void ixgbe_napi_disable_all(struct ixgbe_adapter *adapter)\n{\n\tint q_idx;\n\n\tfor (q_idx = 0; q_idx < adapter->num_q_vectors; q_idx++)\n\t\tnapi_disable(&adapter->q_vector[q_idx]->napi);\n}\n\nstatic int ixgbe_udp_tunnel_sync(struct net_device *dev, unsigned int table)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct udp_tunnel_info ti;\n\n\tudp_tunnel_nic_get_port(dev, table, 0, &ti);\n\tif (ti.type == UDP_TUNNEL_TYPE_VXLAN)\n\t\tadapter->vxlan_port = ti.port;\n\telse\n\t\tadapter->geneve_port = ti.port;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_VXLANCTRL,\n\t\t\tntohs(adapter->vxlan_port) |\n\t\t\tntohs(adapter->geneve_port) <<\n\t\t\t\tIXGBE_VXLANCTRL_GENEVE_UDPPORT_SHIFT);\n\treturn 0;\n}\n\nstatic const struct udp_tunnel_nic_info ixgbe_udp_tunnels_x550 = {\n\t.sync_table\t= ixgbe_udp_tunnel_sync,\n\t.flags\t\t= UDP_TUNNEL_NIC_INFO_IPV4_ONLY,\n\t.tables\t\t= {\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_VXLAN,  },\n\t},\n};\n\nstatic const struct udp_tunnel_nic_info ixgbe_udp_tunnels_x550em_a = {\n\t.sync_table\t= ixgbe_udp_tunnel_sync,\n\t.flags\t\t= UDP_TUNNEL_NIC_INFO_IPV4_ONLY,\n\t.tables\t\t= {\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_VXLAN,  },\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_GENEVE, },\n\t},\n};\n\n#ifdef CONFIG_IXGBE_DCB\n \nstatic void ixgbe_configure_dcb(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint max_frame = adapter->netdev->mtu + ETH_HLEN + ETH_FCS_LEN;\n\n\tif (!(adapter->flags & IXGBE_FLAG_DCB_ENABLED)) {\n\t\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\t\tnetif_set_tso_max_size(adapter->netdev, 65536);\n\t\treturn;\n\t}\n\n\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\tnetif_set_tso_max_size(adapter->netdev, 32768);\n\n#ifdef IXGBE_FCOE\n\tif (adapter->netdev->features & NETIF_F_FCOE_MTU)\n\t\tmax_frame = max(max_frame, IXGBE_FCOE_JUMBO_FRAME_SIZE);\n#endif\n\n\t \n\tif (adapter->dcbx_cap & DCB_CAP_DCBX_VER_CEE) {\n\t\tixgbe_dcb_calculate_tc_credits(hw, &adapter->dcb_cfg, max_frame,\n\t\t\t\t\t\tDCB_TX_CONFIG);\n\t\tixgbe_dcb_calculate_tc_credits(hw, &adapter->dcb_cfg, max_frame,\n\t\t\t\t\t\tDCB_RX_CONFIG);\n\t\tixgbe_dcb_hw_config(hw, &adapter->dcb_cfg);\n\t} else if (adapter->ixgbe_ieee_ets && adapter->ixgbe_ieee_pfc) {\n\t\tixgbe_dcb_hw_ets(&adapter->hw,\n\t\t\t\t adapter->ixgbe_ieee_ets,\n\t\t\t\t max_frame);\n\t\tixgbe_dcb_hw_pfc_config(&adapter->hw,\n\t\t\t\t\tadapter->ixgbe_ieee_pfc->pfc_en,\n\t\t\t\t\tadapter->ixgbe_ieee_ets->prio_tc);\n\t}\n\n\t \n\tif (hw->mac.type != ixgbe_mac_82598EB) {\n\t\tu32 msb = 0;\n\t\tu16 rss_i = adapter->ring_feature[RING_F_RSS].indices - 1;\n\n\t\twhile (rss_i) {\n\t\t\tmsb++;\n\t\t\trss_i >>= 1;\n\t\t}\n\n\t\t \n\t\tIXGBE_WRITE_REG(hw, IXGBE_RQTC, msb * 0x11111111);\n\t}\n}\n#endif\n\n \n#define IXGBE_ETH_FRAMING 20\n\n \nstatic int ixgbe_hpbthresh(struct ixgbe_adapter *adapter, int pb)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct net_device *dev = adapter->netdev;\n\tint link, tc, kb, marker;\n\tu32 dv_id, rx_pba;\n\n\t \n\ttc = link = dev->mtu + ETH_HLEN + ETH_FCS_LEN + IXGBE_ETH_FRAMING;\n\n#ifdef IXGBE_FCOE\n\t \n\tif ((dev->features & NETIF_F_FCOE_MTU) &&\n\t    (tc < IXGBE_FCOE_JUMBO_FRAME_SIZE) &&\n\t    (pb == ixgbe_fcoe_get_tc(adapter)))\n\t\ttc = IXGBE_FCOE_JUMBO_FRAME_SIZE;\n#endif\n\n\t \n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tdv_id = IXGBE_DV_X540(link, tc);\n\t\tbreak;\n\tdefault:\n\t\tdv_id = IXGBE_DV(link, tc);\n\t\tbreak;\n\t}\n\n\t \n\tif (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED)\n\t\tdv_id += IXGBE_B2BT(tc);\n\n\t \n\tkb = IXGBE_BT2KB(dv_id);\n\trx_pba = IXGBE_READ_REG(hw, IXGBE_RXPBSIZE(pb)) >> 10;\n\n\tmarker = rx_pba - kb;\n\n\t \n\tif (marker < 0) {\n\t\te_warn(drv, \"Packet Buffer(%i) can not provide enough\"\n\t\t\t    \"headroom to support flow control.\"\n\t\t\t    \"Decrease MTU or number of traffic classes\\n\", pb);\n\t\tmarker = tc + 1;\n\t}\n\n\treturn marker;\n}\n\n \nstatic int ixgbe_lpbthresh(struct ixgbe_adapter *adapter, int pb)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct net_device *dev = adapter->netdev;\n\tint tc;\n\tu32 dv_id;\n\n\t \n\ttc = dev->mtu + ETH_HLEN + ETH_FCS_LEN;\n\n#ifdef IXGBE_FCOE\n\t \n\tif ((dev->features & NETIF_F_FCOE_MTU) &&\n\t    (tc < IXGBE_FCOE_JUMBO_FRAME_SIZE) &&\n\t    (pb == netdev_get_prio_tc_map(dev, adapter->fcoe.up)))\n\t\ttc = IXGBE_FCOE_JUMBO_FRAME_SIZE;\n#endif\n\n\t \n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tdv_id = IXGBE_LOW_DV_X540(tc);\n\t\tbreak;\n\tdefault:\n\t\tdv_id = IXGBE_LOW_DV(tc);\n\t\tbreak;\n\t}\n\n\t \n\treturn IXGBE_BT2KB(dv_id);\n}\n\n \nstatic void ixgbe_pbthresh_setup(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint num_tc = adapter->hw_tcs;\n\tint i;\n\n\tif (!num_tc)\n\t\tnum_tc = 1;\n\n\tfor (i = 0; i < num_tc; i++) {\n\t\thw->fc.high_water[i] = ixgbe_hpbthresh(adapter, i);\n\t\thw->fc.low_water[i] = ixgbe_lpbthresh(adapter, i);\n\n\t\t \n\t\tif (hw->fc.low_water[i] > hw->fc.high_water[i])\n\t\t\thw->fc.low_water[i] = 0;\n\t}\n\n\tfor (; i < MAX_TRAFFIC_CLASS; i++)\n\t\thw->fc.high_water[i] = 0;\n}\n\nstatic void ixgbe_configure_pb(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint hdrm;\n\tu8 tc = adapter->hw_tcs;\n\n\tif (adapter->flags & IXGBE_FLAG_FDIR_HASH_CAPABLE ||\n\t    adapter->flags & IXGBE_FLAG_FDIR_PERFECT_CAPABLE)\n\t\thdrm = 32 << adapter->fdir_pballoc;\n\telse\n\t\thdrm = 0;\n\n\thw->mac.ops.set_rxpba(hw, tc, hdrm, PBA_STRATEGY_EQUAL);\n\tixgbe_pbthresh_setup(adapter);\n}\n\nstatic void ixgbe_fdir_filter_restore(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct hlist_node *node2;\n\tstruct ixgbe_fdir_filter *filter;\n\tu8 queue;\n\n\tspin_lock(&adapter->fdir_perfect_lock);\n\n\tif (!hlist_empty(&adapter->fdir_filter_list))\n\t\tixgbe_fdir_set_input_mask_82599(hw, &adapter->fdir_mask);\n\n\thlist_for_each_entry_safe(filter, node2,\n\t\t\t\t  &adapter->fdir_filter_list, fdir_node) {\n\t\tif (filter->action == IXGBE_FDIR_DROP_QUEUE) {\n\t\t\tqueue = IXGBE_FDIR_DROP_QUEUE;\n\t\t} else {\n\t\t\tu32 ring = ethtool_get_flow_spec_ring(filter->action);\n\t\t\tu8 vf = ethtool_get_flow_spec_ring_vf(filter->action);\n\n\t\t\tif (!vf && (ring >= adapter->num_rx_queues)) {\n\t\t\t\te_err(drv, \"FDIR restore failed without VF, ring: %u\\n\",\n\t\t\t\t      ring);\n\t\t\t\tcontinue;\n\t\t\t} else if (vf &&\n\t\t\t\t   ((vf > adapter->num_vfs) ||\n\t\t\t\t     ring >= adapter->num_rx_queues_per_pool)) {\n\t\t\t\te_err(drv, \"FDIR restore failed with VF, vf: %hhu, ring: %u\\n\",\n\t\t\t\t      vf, ring);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!vf)\n\t\t\t\tqueue = adapter->rx_ring[ring]->reg_idx;\n\t\t\telse\n\t\t\t\tqueue = ((vf - 1) *\n\t\t\t\t\tadapter->num_rx_queues_per_pool) + ring;\n\t\t}\n\n\t\tixgbe_fdir_write_perfect_filter_82599(hw,\n\t\t\t\t&filter->filter, filter->sw_idx, queue);\n\t}\n\n\tspin_unlock(&adapter->fdir_perfect_lock);\n}\n\n \nstatic void ixgbe_clean_rx_ring(struct ixgbe_ring *rx_ring)\n{\n\tu16 i = rx_ring->next_to_clean;\n\tstruct ixgbe_rx_buffer *rx_buffer = &rx_ring->rx_buffer_info[i];\n\n\tif (rx_ring->xsk_pool) {\n\t\tixgbe_xsk_clean_rx_ring(rx_ring);\n\t\tgoto skip_free;\n\t}\n\n\t \n\twhile (i != rx_ring->next_to_alloc) {\n\t\tif (rx_buffer->skb) {\n\t\t\tstruct sk_buff *skb = rx_buffer->skb;\n\t\t\tif (IXGBE_CB(skb)->page_released)\n\t\t\t\tdma_unmap_page_attrs(rx_ring->dev,\n\t\t\t\t\t\t     IXGBE_CB(skb)->dma,\n\t\t\t\t\t\t     ixgbe_rx_pg_size(rx_ring),\n\t\t\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t\t\t     IXGBE_RX_DMA_ATTR);\n\t\t\tdev_kfree_skb(skb);\n\t\t}\n\n\t\t \n\t\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t\t      rx_buffer->dma,\n\t\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t\t      ixgbe_rx_bufsz(rx_ring),\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,\n\t\t\t\t     ixgbe_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t     IXGBE_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(rx_buffer->page,\n\t\t\t\t\trx_buffer->pagecnt_bias);\n\n\t\ti++;\n\t\trx_buffer++;\n\t\tif (i == rx_ring->count) {\n\t\t\ti = 0;\n\t\t\trx_buffer = rx_ring->rx_buffer_info;\n\t\t}\n\t}\n\nskip_free:\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n}\n\nstatic int ixgbe_fwd_ring_up(struct ixgbe_adapter *adapter,\n\t\t\t     struct ixgbe_fwd_adapter *accel)\n{\n\tu16 rss_i = adapter->ring_feature[RING_F_RSS].indices;\n\tint num_tc = netdev_get_num_tc(adapter->netdev);\n\tstruct net_device *vdev = accel->netdev;\n\tint i, baseq, err;\n\n\tbaseq = accel->pool * adapter->num_rx_queues_per_pool;\n\tnetdev_dbg(vdev, \"pool %i:%i queues %i:%i\\n\",\n\t\t   accel->pool, adapter->num_rx_pools,\n\t\t   baseq, baseq + adapter->num_rx_queues_per_pool);\n\n\taccel->rx_base_queue = baseq;\n\taccel->tx_base_queue = baseq;\n\n\t \n\tfor (i = 0; i < num_tc; i++)\n\t\tnetdev_bind_sb_channel_queue(adapter->netdev, vdev,\n\t\t\t\t\t     i, rss_i, baseq + (rss_i * i));\n\n\tfor (i = 0; i < adapter->num_rx_queues_per_pool; i++)\n\t\tadapter->rx_ring[baseq + i]->netdev = vdev;\n\n\t \n\twmb();\n\n\t \n\terr = ixgbe_add_mac_filter(adapter, vdev->dev_addr,\n\t\t\t\t   VMDQ_P(accel->pool));\n\tif (err >= 0)\n\t\treturn 0;\n\n\t \n\tmacvlan_release_l2fw_offload(vdev);\n\n\tfor (i = 0; i < adapter->num_rx_queues_per_pool; i++)\n\t\tadapter->rx_ring[baseq + i]->netdev = NULL;\n\n\tnetdev_err(vdev, \"L2FW offload disabled due to L2 filter error\\n\");\n\n\t \n\tnetdev_unbind_sb_channel(adapter->netdev, vdev);\n\tnetdev_set_sb_channel(vdev, 0);\n\n\tclear_bit(accel->pool, adapter->fwd_bitmask);\n\tkfree(accel);\n\n\treturn err;\n}\n\nstatic int ixgbe_macvlan_up(struct net_device *vdev,\n\t\t\t    struct netdev_nested_priv *priv)\n{\n\tstruct ixgbe_adapter *adapter = (struct ixgbe_adapter *)priv->data;\n\tstruct ixgbe_fwd_adapter *accel;\n\n\tif (!netif_is_macvlan(vdev))\n\t\treturn 0;\n\n\taccel = macvlan_accel_priv(vdev);\n\tif (!accel)\n\t\treturn 0;\n\n\tixgbe_fwd_ring_up(adapter, accel);\n\n\treturn 0;\n}\n\nstatic void ixgbe_configure_dfwd(struct ixgbe_adapter *adapter)\n{\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)adapter,\n\t};\n\n\tnetdev_walk_all_upper_dev_rcu(adapter->netdev,\n\t\t\t\t      ixgbe_macvlan_up, &priv);\n}\n\nstatic void ixgbe_configure(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tixgbe_configure_pb(adapter);\n#ifdef CONFIG_IXGBE_DCB\n\tixgbe_configure_dcb(adapter);\n#endif\n\t \n\tixgbe_configure_virtualization(adapter);\n\n\tixgbe_set_rx_mode(adapter->netdev);\n\tixgbe_restore_vlan(adapter);\n\tixgbe_ipsec_restore(adapter);\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\t\thw->mac.ops.disable_rx_buff(hw);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (adapter->flags & IXGBE_FLAG_FDIR_HASH_CAPABLE) {\n\t\tixgbe_init_fdir_signature_82599(&adapter->hw,\n\t\t\t\t\t\tadapter->fdir_pballoc);\n\t} else if (adapter->flags & IXGBE_FLAG_FDIR_PERFECT_CAPABLE) {\n\t\tixgbe_init_fdir_perfect_82599(&adapter->hw,\n\t\t\t\t\t      adapter->fdir_pballoc);\n\t\tixgbe_fdir_filter_restore(adapter);\n\t}\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\t\thw->mac.ops.enable_rx_buff(hw);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n#ifdef CONFIG_IXGBE_DCA\n\t \n\tif (adapter->flags & IXGBE_FLAG_DCA_CAPABLE)\n\t\tixgbe_setup_dca(adapter);\n#endif  \n\n#ifdef IXGBE_FCOE\n\t \n\tixgbe_configure_fcoe(adapter);\n\n#endif  \n\tixgbe_configure_tx(adapter);\n\tixgbe_configure_rx(adapter);\n\tixgbe_configure_dfwd(adapter);\n}\n\n \nstatic void ixgbe_sfp_link_config(struct ixgbe_adapter *adapter)\n{\n\t \n\tif (adapter->hw.mac.type == ixgbe_mac_82598EB)\n\t\tadapter->flags2 |= IXGBE_FLAG2_SEARCH_FOR_SFP;\n\n\tadapter->flags2 |= IXGBE_FLAG2_SFP_NEEDS_RESET;\n\tadapter->sfp_poll_time = 0;\n}\n\n \nstatic int ixgbe_non_sfp_link_config(struct ixgbe_hw *hw)\n{\n\tu32 speed;\n\tbool autoneg, link_up = false;\n\tint ret = IXGBE_ERR_LINK_SETUP;\n\n\tif (hw->mac.ops.check_link)\n\t\tret = hw->mac.ops.check_link(hw, &speed, &link_up, false);\n\n\tif (ret)\n\t\treturn ret;\n\n\tspeed = hw->phy.autoneg_advertised;\n\tif (!speed && hw->mac.ops.get_link_capabilities) {\n\t\tret = hw->mac.ops.get_link_capabilities(hw, &speed,\n\t\t\t\t\t\t\t&autoneg);\n\t\t \n\t\tspeed &= ~(IXGBE_LINK_SPEED_5GB_FULL |\n\t\t\t   IXGBE_LINK_SPEED_2_5GB_FULL);\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (hw->mac.ops.setup_link)\n\t\tret = hw->mac.ops.setup_link(hw, speed, link_up);\n\n\treturn ret;\n}\n\n \nstatic void ixgbe_clear_vf_stats_counters(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\tfor (i = 0; i < adapter->num_vfs; i++) {\n\t\tadapter->vfinfo[i].last_vfstats.gprc =\n\t\t\tIXGBE_READ_REG(hw, IXGBE_PVFGPRC(i));\n\t\tadapter->vfinfo[i].saved_rst_vfstats.gprc +=\n\t\t\tadapter->vfinfo[i].vfstats.gprc;\n\t\tadapter->vfinfo[i].vfstats.gprc = 0;\n\t\tadapter->vfinfo[i].last_vfstats.gptc =\n\t\t\tIXGBE_READ_REG(hw, IXGBE_PVFGPTC(i));\n\t\tadapter->vfinfo[i].saved_rst_vfstats.gptc +=\n\t\t\tadapter->vfinfo[i].vfstats.gptc;\n\t\tadapter->vfinfo[i].vfstats.gptc = 0;\n\t\tadapter->vfinfo[i].last_vfstats.gorc =\n\t\t\tIXGBE_READ_REG(hw, IXGBE_PVFGORC_LSB(i));\n\t\tadapter->vfinfo[i].saved_rst_vfstats.gorc +=\n\t\t\tadapter->vfinfo[i].vfstats.gorc;\n\t\tadapter->vfinfo[i].vfstats.gorc = 0;\n\t\tadapter->vfinfo[i].last_vfstats.gotc =\n\t\t\tIXGBE_READ_REG(hw, IXGBE_PVFGOTC_LSB(i));\n\t\tadapter->vfinfo[i].saved_rst_vfstats.gotc +=\n\t\t\tadapter->vfinfo[i].vfstats.gotc;\n\t\tadapter->vfinfo[i].vfstats.gotc = 0;\n\t\tadapter->vfinfo[i].last_vfstats.mprc =\n\t\t\tIXGBE_READ_REG(hw, IXGBE_PVFMPRC(i));\n\t\tadapter->vfinfo[i].saved_rst_vfstats.mprc +=\n\t\t\tadapter->vfinfo[i].vfstats.mprc;\n\t\tadapter->vfinfo[i].vfstats.mprc = 0;\n\t}\n}\n\nstatic void ixgbe_setup_gpie(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 gpie = 0;\n\n\tif (adapter->flags & IXGBE_FLAG_MSIX_ENABLED) {\n\t\tgpie = IXGBE_GPIE_MSIX_MODE | IXGBE_GPIE_PBA_SUPPORT |\n\t\t       IXGBE_GPIE_OCD;\n\t\tgpie |= IXGBE_GPIE_EIAME;\n\t\t \n\t\tswitch (hw->mac.type) {\n\t\tcase ixgbe_mac_82598EB:\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_EIAM, IXGBE_EICS_RTX_QUEUE);\n\t\t\tbreak;\n\t\tcase ixgbe_mac_82599EB:\n\t\tcase ixgbe_mac_X540:\n\t\tcase ixgbe_mac_X550:\n\t\tcase ixgbe_mac_X550EM_x:\n\t\tcase ixgbe_mac_x550em_a:\n\t\tdefault:\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_EIAM_EX(0), 0xFFFFFFFF);\n\t\t\tIXGBE_WRITE_REG(hw, IXGBE_EIAM_EX(1), 0xFFFFFFFF);\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\t \n\t\tIXGBE_WRITE_REG(hw, IXGBE_EIAM, IXGBE_EICS_RTX_QUEUE);\n\t}\n\n\t \n\t \n\n\tif (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED) {\n\t\tgpie &= ~IXGBE_GPIE_VTMODE_MASK;\n\n\t\tswitch (adapter->ring_feature[RING_F_VMDQ].mask) {\n\t\tcase IXGBE_82599_VMDQ_8Q_MASK:\n\t\t\tgpie |= IXGBE_GPIE_VTMODE_16;\n\t\t\tbreak;\n\t\tcase IXGBE_82599_VMDQ_4Q_MASK:\n\t\t\tgpie |= IXGBE_GPIE_VTMODE_32;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgpie |= IXGBE_GPIE_VTMODE_64;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (adapter->flags2 & IXGBE_FLAG2_TEMP_SENSOR_CAPABLE) {\n\t\tswitch (adapter->hw.mac.type) {\n\t\tcase ixgbe_mac_82599EB:\n\t\t\tgpie |= IXGBE_SDP0_GPIEN_8259X;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (adapter->flags & IXGBE_FLAG_FAN_FAIL_CAPABLE)\n\t\tgpie |= IXGBE_SDP1_GPIEN(hw);\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82599EB:\n\t\tgpie |= IXGBE_SDP1_GPIEN_8259X | IXGBE_SDP2_GPIEN_8259X;\n\t\tbreak;\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tgpie |= IXGBE_SDP0_GPIEN_X540;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tIXGBE_WRITE_REG(hw, IXGBE_GPIE, gpie);\n}\n\nstatic void ixgbe_up_complete(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint err;\n\tu32 ctrl_ext;\n\n\tixgbe_get_hw_control(adapter);\n\tixgbe_setup_gpie(adapter);\n\n\tif (adapter->flags & IXGBE_FLAG_MSIX_ENABLED)\n\t\tixgbe_configure_msix(adapter);\n\telse\n\t\tixgbe_configure_msi_and_legacy(adapter);\n\n\t \n\tif (hw->mac.ops.enable_tx_laser)\n\t\thw->mac.ops.enable_tx_laser(hw);\n\n\tif (hw->phy.ops.set_phy_power)\n\t\thw->phy.ops.set_phy_power(hw, true);\n\n\tsmp_mb__before_atomic();\n\tclear_bit(__IXGBE_DOWN, &adapter->state);\n\tixgbe_napi_enable_all(adapter);\n\n\tif (ixgbe_is_sfp(hw)) {\n\t\tixgbe_sfp_link_config(adapter);\n\t} else {\n\t\terr = ixgbe_non_sfp_link_config(hw);\n\t\tif (err)\n\t\t\te_err(probe, \"link_config FAILED %d\\n\", err);\n\t}\n\n\t \n\tIXGBE_READ_REG(hw, IXGBE_EICR);\n\tixgbe_irq_enable(adapter, true, true);\n\n\t \n\tif (adapter->flags & IXGBE_FLAG_FAN_FAIL_CAPABLE) {\n\t\tu32 esdp = IXGBE_READ_REG(hw, IXGBE_ESDP);\n\t\tif (esdp & IXGBE_ESDP_SDP1)\n\t\t\te_crit(drv, \"Fan has stopped, replace the adapter\\n\");\n\t}\n\n\t \n\tadapter->flags |= IXGBE_FLAG_NEED_LINK_UPDATE;\n\tadapter->link_check_timeout = jiffies;\n\tmod_timer(&adapter->service_timer, jiffies);\n\n\tixgbe_clear_vf_stats_counters(adapter);\n\t \n\tctrl_ext = IXGBE_READ_REG(hw, IXGBE_CTRL_EXT);\n\tctrl_ext |= IXGBE_CTRL_EXT_PFRSTD;\n\tIXGBE_WRITE_REG(hw, IXGBE_CTRL_EXT, ctrl_ext);\n\n\t \n\tixgbe_set_all_vfs(adapter);\n}\n\nvoid ixgbe_reinit_locked(struct ixgbe_adapter *adapter)\n{\n\t \n\tnetif_trans_update(adapter->netdev);\n\n\twhile (test_and_set_bit(__IXGBE_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\tif (adapter->hw.phy.type == ixgbe_phy_fw)\n\t\tixgbe_watchdog_link_is_down(adapter);\n\tixgbe_down(adapter);\n\t \n\tif (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED)\n\t\tmsleep(2000);\n\tixgbe_up(adapter);\n\tclear_bit(__IXGBE_RESETTING, &adapter->state);\n}\n\nvoid ixgbe_up(struct ixgbe_adapter *adapter)\n{\n\t \n\tixgbe_configure(adapter);\n\n\tixgbe_up_complete(adapter);\n}\n\nstatic unsigned long ixgbe_get_completion_timeout(struct ixgbe_adapter *adapter)\n{\n\tu16 devctl2;\n\n\tpcie_capability_read_word(adapter->pdev, PCI_EXP_DEVCTL2, &devctl2);\n\n\tswitch (devctl2 & IXGBE_PCIDEVCTRL2_TIMEO_MASK) {\n\tcase IXGBE_PCIDEVCTRL2_17_34s:\n\tcase IXGBE_PCIDEVCTRL2_4_8s:\n\t\t \n\tcase IXGBE_PCIDEVCTRL2_1_2s:\n\t\treturn 2000000ul;\t \n\tcase IXGBE_PCIDEVCTRL2_260_520ms:\n\t\treturn 520000ul;\t \n\tcase IXGBE_PCIDEVCTRL2_65_130ms:\n\t\treturn 130000ul;\t \n\tcase IXGBE_PCIDEVCTRL2_16_32ms:\n\t\treturn 32000ul;\t\t \n\tcase IXGBE_PCIDEVCTRL2_1_2ms:\n\t\treturn 2000ul;\t\t \n\tcase IXGBE_PCIDEVCTRL2_50_100us:\n\t\treturn 100ul;\t\t \n\tcase IXGBE_PCIDEVCTRL2_16_32ms_def:\n\t\treturn 32000ul;\t\t \n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\treturn 32000ul;\n}\n\nvoid ixgbe_disable_rx(struct ixgbe_adapter *adapter)\n{\n\tunsigned long wait_delay, delay_interval;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i, wait_loop;\n\tu32 rxdctl;\n\n\t \n\thw->mac.ops.disable_rx(hw);\n\n\tif (ixgbe_removed(hw->hw_addr))\n\t\treturn;\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct ixgbe_ring *ring = adapter->rx_ring[i];\n\t\tu8 reg_idx = ring->reg_idx;\n\n\t\trxdctl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(reg_idx));\n\t\trxdctl &= ~IXGBE_RXDCTL_ENABLE;\n\t\trxdctl |= IXGBE_RXDCTL_SWFLSH;\n\n\t\t \n\t\tIXGBE_WRITE_REG(hw, IXGBE_RXDCTL(reg_idx), rxdctl);\n\t}\n\n\t \n\tif (hw->mac.type == ixgbe_mac_82598EB &&\n\t    !(IXGBE_READ_REG(hw, IXGBE_LINKS) & IXGBE_LINKS_UP))\n\t\treturn;\n\n\t \n\tdelay_interval = ixgbe_get_completion_timeout(adapter) / 100;\n\n\twait_loop = IXGBE_MAX_RX_DESC_POLL;\n\twait_delay = delay_interval;\n\n\twhile (wait_loop--) {\n\t\tusleep_range(wait_delay, wait_delay + 10);\n\t\twait_delay += delay_interval * 2;\n\t\trxdctl = 0;\n\n\t\t \n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tstruct ixgbe_ring *ring = adapter->rx_ring[i];\n\t\t\tu8 reg_idx = ring->reg_idx;\n\n\t\t\trxdctl |= IXGBE_READ_REG(hw, IXGBE_RXDCTL(reg_idx));\n\t\t}\n\n\t\tif (!(rxdctl & IXGBE_RXDCTL_ENABLE))\n\t\t\treturn;\n\t}\n\n\te_err(drv,\n\t      \"RXDCTL.ENABLE for one or more queues not cleared within the polling period\\n\");\n}\n\nvoid ixgbe_disable_tx(struct ixgbe_adapter *adapter)\n{\n\tunsigned long wait_delay, delay_interval;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i, wait_loop;\n\tu32 txdctl;\n\n\tif (ixgbe_removed(hw->hw_addr))\n\t\treturn;\n\n\t \n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct ixgbe_ring *ring = adapter->tx_ring[i];\n\t\tu8 reg_idx = ring->reg_idx;\n\n\t\tIXGBE_WRITE_REG(hw, IXGBE_TXDCTL(reg_idx), IXGBE_TXDCTL_SWFLSH);\n\t}\n\n\t \n\tfor (i = 0; i < adapter->num_xdp_queues; i++) {\n\t\tstruct ixgbe_ring *ring = adapter->xdp_ring[i];\n\t\tu8 reg_idx = ring->reg_idx;\n\n\t\tIXGBE_WRITE_REG(hw, IXGBE_TXDCTL(reg_idx), IXGBE_TXDCTL_SWFLSH);\n\t}\n\n\t \n\tif (!(IXGBE_READ_REG(hw, IXGBE_LINKS) & IXGBE_LINKS_UP))\n\t\tgoto dma_engine_disable;\n\n\t \n\tdelay_interval = ixgbe_get_completion_timeout(adapter) / 100;\n\n\twait_loop = IXGBE_MAX_RX_DESC_POLL;\n\twait_delay = delay_interval;\n\n\twhile (wait_loop--) {\n\t\tusleep_range(wait_delay, wait_delay + 10);\n\t\twait_delay += delay_interval * 2;\n\t\ttxdctl = 0;\n\n\t\t \n\t\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\t\tstruct ixgbe_ring *ring = adapter->tx_ring[i];\n\t\t\tu8 reg_idx = ring->reg_idx;\n\n\t\t\ttxdctl |= IXGBE_READ_REG(hw, IXGBE_TXDCTL(reg_idx));\n\t\t}\n\t\tfor (i = 0; i < adapter->num_xdp_queues; i++) {\n\t\t\tstruct ixgbe_ring *ring = adapter->xdp_ring[i];\n\t\t\tu8 reg_idx = ring->reg_idx;\n\n\t\t\ttxdctl |= IXGBE_READ_REG(hw, IXGBE_TXDCTL(reg_idx));\n\t\t}\n\n\t\tif (!(txdctl & IXGBE_TXDCTL_ENABLE))\n\t\t\tgoto dma_engine_disable;\n\t}\n\n\te_err(drv,\n\t      \"TXDCTL.ENABLE for one or more queues not cleared within the polling period\\n\");\n\ndma_engine_disable:\n\t \n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tIXGBE_WRITE_REG(hw, IXGBE_DMATXCTL,\n\t\t\t\t(IXGBE_READ_REG(hw, IXGBE_DMATXCTL) &\n\t\t\t\t ~IXGBE_DMATXCTL_TE));\n\t\tfallthrough;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nvoid ixgbe_reset(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct net_device *netdev = adapter->netdev;\n\tint err;\n\n\tif (ixgbe_removed(hw->hw_addr))\n\t\treturn;\n\t \n\twhile (test_and_set_bit(__IXGBE_IN_SFP_INIT, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\n\t \n\tadapter->flags2 &= ~(IXGBE_FLAG2_SEARCH_FOR_SFP |\n\t\t\t     IXGBE_FLAG2_SFP_NEEDS_RESET);\n\tadapter->flags &= ~IXGBE_FLAG_NEED_LINK_CONFIG;\n\n\terr = hw->mac.ops.init_hw(hw);\n\tswitch (err) {\n\tcase 0:\n\tcase IXGBE_ERR_SFP_NOT_PRESENT:\n\tcase IXGBE_ERR_SFP_NOT_SUPPORTED:\n\t\tbreak;\n\tcase IXGBE_ERR_PRIMARY_REQUESTS_PENDING:\n\t\te_dev_err(\"primary disable timed out\\n\");\n\t\tbreak;\n\tcase IXGBE_ERR_EEPROM_VERSION:\n\t\t \n\t\te_dev_warn(\"This device is a pre-production adapter/LOM. \"\n\t\t\t   \"Please be aware there may be issues associated with \"\n\t\t\t   \"your hardware.  If you are experiencing problems \"\n\t\t\t   \"please contact your Intel or hardware \"\n\t\t\t   \"representative who provided you with this \"\n\t\t\t   \"hardware.\\n\");\n\t\tbreak;\n\tdefault:\n\t\te_dev_err(\"Hardware Error: %d\\n\", err);\n\t}\n\n\tclear_bit(__IXGBE_IN_SFP_INIT, &adapter->state);\n\n\t \n\tixgbe_flush_sw_mac_table(adapter);\n\t__dev_uc_unsync(netdev, NULL);\n\n\t \n\tixgbe_mac_set_default_filter(adapter);\n\n\t \n\tif (hw->mac.san_mac_rar_index)\n\t\thw->mac.ops.set_vmdq_san_mac(hw, VMDQ_P(0));\n\n\tif (test_bit(__IXGBE_PTP_RUNNING, &adapter->state))\n\t\tixgbe_ptp_reset(adapter);\n\n\tif (hw->phy.ops.set_phy_power) {\n\t\tif (!netif_running(adapter->netdev) && !adapter->wol)\n\t\t\thw->phy.ops.set_phy_power(hw, false);\n\t\telse\n\t\t\thw->phy.ops.set_phy_power(hw, true);\n\t}\n}\n\n \nstatic void ixgbe_clean_tx_ring(struct ixgbe_ring *tx_ring)\n{\n\tu16 i = tx_ring->next_to_clean;\n\tstruct ixgbe_tx_buffer *tx_buffer = &tx_ring->tx_buffer_info[i];\n\n\tif (tx_ring->xsk_pool) {\n\t\tixgbe_xsk_clean_tx_ring(tx_ring);\n\t\tgoto out;\n\t}\n\n\twhile (i != tx_ring->next_to_use) {\n\t\tunion ixgbe_adv_tx_desc *eop_desc, *tx_desc;\n\n\t\t \n\t\tif (ring_is_xdp(tx_ring))\n\t\t\txdp_return_frame(tx_buffer->xdpf);\n\t\telse\n\t\t\tdev_kfree_skb_any(tx_buffer->skb);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\teop_desc = tx_buffer->next_to_watch;\n\t\ttx_desc = IXGBE_TX_DESC(tx_ring, i);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\t\ti = 0;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = IXGBE_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t}\n\n\t\t \n\t\ttx_buffer++;\n\t\ti++;\n\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\ti = 0;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t}\n\t}\n\n\t \n\tif (!ring_is_xdp(tx_ring))\n\t\tnetdev_tx_reset_queue(txring_txq(tx_ring));\n\nout:\n\t \n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n}\n\n \nstatic void ixgbe_clean_all_rx_rings(struct ixgbe_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tixgbe_clean_rx_ring(adapter->rx_ring[i]);\n}\n\n \nstatic void ixgbe_clean_all_tx_rings(struct ixgbe_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tixgbe_clean_tx_ring(adapter->tx_ring[i]);\n\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\tixgbe_clean_tx_ring(adapter->xdp_ring[i]);\n}\n\nstatic void ixgbe_fdir_filter_exit(struct ixgbe_adapter *adapter)\n{\n\tstruct hlist_node *node2;\n\tstruct ixgbe_fdir_filter *filter;\n\n\tspin_lock(&adapter->fdir_perfect_lock);\n\n\thlist_for_each_entry_safe(filter, node2,\n\t\t\t\t  &adapter->fdir_filter_list, fdir_node) {\n\t\thlist_del(&filter->fdir_node);\n\t\tkfree(filter);\n\t}\n\tadapter->fdir_filter_count = 0;\n\n\tspin_unlock(&adapter->fdir_perfect_lock);\n}\n\nvoid ixgbe_down(struct ixgbe_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\t \n\tif (test_and_set_bit(__IXGBE_DOWN, &adapter->state))\n\t\treturn;  \n\n\t \n\tnetif_tx_stop_all_queues(netdev);\n\n\t \n\tnetif_carrier_off(netdev);\n\tnetif_tx_disable(netdev);\n\n\t \n\tixgbe_disable_rx(adapter);\n\n\t \n\tif (adapter->xdp_ring[0])\n\t\tsynchronize_rcu();\n\n\tixgbe_irq_disable(adapter);\n\n\tixgbe_napi_disable_all(adapter);\n\n\tclear_bit(__IXGBE_RESET_REQUESTED, &adapter->state);\n\tadapter->flags2 &= ~IXGBE_FLAG2_FDIR_REQUIRES_REINIT;\n\tadapter->flags &= ~IXGBE_FLAG_NEED_LINK_UPDATE;\n\n\tdel_timer_sync(&adapter->service_timer);\n\n\tif (adapter->num_vfs) {\n\t\t \n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_EITRSEL, 0);\n\n\t\t \n\t\tfor (i = 0 ; i < adapter->num_vfs; i++)\n\t\t\tadapter->vfinfo[i].clear_to_send = false;\n\n\t\t \n\t\tixgbe_set_all_vfs(adapter);\n\t}\n\n\t \n\tixgbe_disable_tx(adapter);\n\n\tif (!pci_channel_offline(adapter->pdev))\n\t\tixgbe_reset(adapter);\n\n\t \n\tif (hw->mac.ops.disable_tx_laser)\n\t\thw->mac.ops.disable_tx_laser(hw);\n\n\tixgbe_clean_all_tx_rings(adapter);\n\tixgbe_clean_all_rx_rings(adapter);\n}\n\n \nstatic void ixgbe_set_eee_capable(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tswitch (hw->device_id) {\n\tcase IXGBE_DEV_ID_X550EM_A_1G_T:\n\tcase IXGBE_DEV_ID_X550EM_A_1G_T_L:\n\t\tif (!hw->phy.eee_speeds_supported)\n\t\t\tbreak;\n\t\tadapter->flags2 |= IXGBE_FLAG2_EEE_CAPABLE;\n\t\tif (!hw->phy.eee_speeds_advertised)\n\t\t\tbreak;\n\t\tadapter->flags2 |= IXGBE_FLAG2_EEE_ENABLED;\n\t\tbreak;\n\tdefault:\n\t\tadapter->flags2 &= ~IXGBE_FLAG2_EEE_CAPABLE;\n\t\tadapter->flags2 &= ~IXGBE_FLAG2_EEE_ENABLED;\n\t\tbreak;\n\t}\n}\n\n \nstatic void ixgbe_tx_timeout(struct net_device *netdev, unsigned int __always_unused txqueue)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\n\t \n\tixgbe_tx_timeout_reset(adapter);\n}\n\n#ifdef CONFIG_IXGBE_DCB\nstatic void ixgbe_init_dcb(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct tc_configuration *tc;\n\tint j;\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\tcase ixgbe_mac_82599EB:\n\t\tadapter->dcb_cfg.num_tcs.pg_tcs = MAX_TRAFFIC_CLASS;\n\t\tadapter->dcb_cfg.num_tcs.pfc_tcs = MAX_TRAFFIC_CLASS;\n\t\tbreak;\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\t\tadapter->dcb_cfg.num_tcs.pg_tcs = X540_TRAFFIC_CLASS;\n\t\tadapter->dcb_cfg.num_tcs.pfc_tcs = X540_TRAFFIC_CLASS;\n\t\tbreak;\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\tdefault:\n\t\tadapter->dcb_cfg.num_tcs.pg_tcs = DEF_TRAFFIC_CLASS;\n\t\tadapter->dcb_cfg.num_tcs.pfc_tcs = DEF_TRAFFIC_CLASS;\n\t\tbreak;\n\t}\n\n\t \n\tfor (j = 0; j < MAX_TRAFFIC_CLASS; j++) {\n\t\ttc = &adapter->dcb_cfg.tc_config[j];\n\t\ttc->path[DCB_TX_CONFIG].bwg_id = 0;\n\t\ttc->path[DCB_TX_CONFIG].bwg_percent = 12 + (j & 1);\n\t\ttc->path[DCB_RX_CONFIG].bwg_id = 0;\n\t\ttc->path[DCB_RX_CONFIG].bwg_percent = 12 + (j & 1);\n\t\ttc->dcb_pfc = pfc_disabled;\n\t}\n\n\t \n\ttc = &adapter->dcb_cfg.tc_config[0];\n\ttc->path[DCB_TX_CONFIG].up_to_tc_bitmap = 0xFF;\n\ttc->path[DCB_RX_CONFIG].up_to_tc_bitmap = 0xFF;\n\n\tadapter->dcb_cfg.bw_percentage[DCB_TX_CONFIG][0] = 100;\n\tadapter->dcb_cfg.bw_percentage[DCB_RX_CONFIG][0] = 100;\n\tadapter->dcb_cfg.pfc_mode_enable = false;\n\tadapter->dcb_set_bitmap = 0x00;\n\tif (adapter->flags & IXGBE_FLAG_DCB_CAPABLE)\n\t\tadapter->dcbx_cap = DCB_CAP_DCBX_HOST | DCB_CAP_DCBX_VER_CEE;\n\tmemcpy(&adapter->temp_dcb_cfg, &adapter->dcb_cfg,\n\t       sizeof(adapter->temp_dcb_cfg));\n}\n#endif\n\n \nstatic int ixgbe_sw_init(struct ixgbe_adapter *adapter,\n\t\t\t const struct ixgbe_info *ii)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tunsigned int rss, fdir;\n\tu32 fwsm;\n\tint i;\n\n\t \n\n\thw->vendor_id = pdev->vendor;\n\thw->device_id = pdev->device;\n\thw->revision_id = pdev->revision;\n\thw->subsystem_vendor_id = pdev->subsystem_vendor;\n\thw->subsystem_device_id = pdev->subsystem_device;\n\n\t \n\tii->get_invariants(hw);\n\n\t \n\trss = min_t(int, ixgbe_max_rss_indices(adapter), num_online_cpus());\n\tadapter->ring_feature[RING_F_RSS].limit = rss;\n\tadapter->flags2 |= IXGBE_FLAG2_RSC_CAPABLE;\n\tadapter->max_q_vectors = MAX_Q_VECTORS_82599;\n\tadapter->atr_sample_rate = 20;\n\tfdir = min_t(int, IXGBE_MAX_FDIR_INDICES, num_online_cpus());\n\tadapter->ring_feature[RING_F_FDIR].limit = fdir;\n\tadapter->fdir_pballoc = IXGBE_FDIR_PBALLOC_64K;\n\tadapter->ring_feature[RING_F_VMDQ].limit = 1;\n#ifdef CONFIG_IXGBE_DCA\n\tadapter->flags |= IXGBE_FLAG_DCA_CAPABLE;\n#endif\n#ifdef CONFIG_IXGBE_DCB\n\tadapter->flags |= IXGBE_FLAG_DCB_CAPABLE;\n\tadapter->flags &= ~IXGBE_FLAG_DCB_ENABLED;\n#endif\n#ifdef IXGBE_FCOE\n\tadapter->flags |= IXGBE_FLAG_FCOE_CAPABLE;\n\tadapter->flags &= ~IXGBE_FLAG_FCOE_ENABLED;\n#ifdef CONFIG_IXGBE_DCB\n\t \n\tadapter->fcoe.up = IXGBE_FCOE_DEFTC;\n#endif  \n#endif  \n\n\t \n\tadapter->jump_tables[0] = kzalloc(sizeof(*adapter->jump_tables[0]),\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!adapter->jump_tables[0])\n\t\treturn -ENOMEM;\n\tadapter->jump_tables[0]->mat = ixgbe_ipv4_fields;\n\n\tfor (i = 1; i < IXGBE_MAX_LINK_HANDLE; i++)\n\t\tadapter->jump_tables[i] = NULL;\n\n\tadapter->mac_table = kcalloc(hw->mac.num_rar_entries,\n\t\t\t\t     sizeof(struct ixgbe_mac_addr),\n\t\t\t\t     GFP_KERNEL);\n\tif (!adapter->mac_table)\n\t\treturn -ENOMEM;\n\n\tif (ixgbe_init_rss_key(adapter))\n\t\treturn -ENOMEM;\n\n\tadapter->af_xdp_zc_qps = bitmap_zalloc(IXGBE_MAX_XDP_QS, GFP_KERNEL);\n\tif (!adapter->af_xdp_zc_qps)\n\t\treturn -ENOMEM;\n\n\t \n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tadapter->flags2 &= ~IXGBE_FLAG2_RSC_CAPABLE;\n\n\t\tif (hw->device_id == IXGBE_DEV_ID_82598AT)\n\t\t\tadapter->flags |= IXGBE_FLAG_FAN_FAIL_CAPABLE;\n\n\t\tadapter->max_q_vectors = MAX_Q_VECTORS_82598;\n\t\tadapter->ring_feature[RING_F_FDIR].limit = 0;\n\t\tadapter->atr_sample_rate = 0;\n\t\tadapter->fdir_pballoc = 0;\n#ifdef IXGBE_FCOE\n\t\tadapter->flags &= ~IXGBE_FLAG_FCOE_CAPABLE;\n\t\tadapter->flags &= ~IXGBE_FLAG_FCOE_ENABLED;\n#ifdef CONFIG_IXGBE_DCB\n\t\tadapter->fcoe.up = 0;\n#endif  \n#endif  \n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\t\tif (hw->device_id == IXGBE_DEV_ID_82599_T3_LOM)\n\t\t\tadapter->flags2 |= IXGBE_FLAG2_TEMP_SENSOR_CAPABLE;\n\t\tbreak;\n\tcase ixgbe_mac_X540:\n\t\tfwsm = IXGBE_READ_REG(hw, IXGBE_FWSM(hw));\n\t\tif (fwsm & IXGBE_FWSM_TS_ENABLED)\n\t\t\tadapter->flags2 |= IXGBE_FLAG2_TEMP_SENSOR_CAPABLE;\n\t\tbreak;\n\tcase ixgbe_mac_x550em_a:\n\t\tswitch (hw->device_id) {\n\t\tcase IXGBE_DEV_ID_X550EM_A_1G_T:\n\t\tcase IXGBE_DEV_ID_X550EM_A_1G_T_L:\n\t\t\tadapter->flags2 |= IXGBE_FLAG2_TEMP_SENSOR_CAPABLE;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase ixgbe_mac_X550EM_x:\n#ifdef CONFIG_IXGBE_DCB\n\t\tadapter->flags &= ~IXGBE_FLAG_DCB_CAPABLE;\n#endif\n#ifdef IXGBE_FCOE\n\t\tadapter->flags &= ~IXGBE_FLAG_FCOE_CAPABLE;\n#ifdef CONFIG_IXGBE_DCB\n\t\tadapter->fcoe.up = 0;\n#endif  \n#endif  \n\t\tfallthrough;\n\tcase ixgbe_mac_X550:\n\t\tif (hw->mac.type == ixgbe_mac_X550)\n\t\t\tadapter->flags2 |= IXGBE_FLAG2_TEMP_SENSOR_CAPABLE;\n#ifdef CONFIG_IXGBE_DCA\n\t\tadapter->flags &= ~IXGBE_FLAG_DCA_CAPABLE;\n#endif\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n#ifdef IXGBE_FCOE\n\t \n\tspin_lock_init(&adapter->fcoe.lock);\n\n#endif\n\t \n\tspin_lock_init(&adapter->fdir_perfect_lock);\n\n\t \n\tspin_lock_init(&adapter->vfs_lock);\n\n#ifdef CONFIG_IXGBE_DCB\n\tixgbe_init_dcb(adapter);\n#endif\n\tixgbe_init_ipsec_offload(adapter);\n\n\t \n\thw->fc.requested_mode = ixgbe_fc_full;\n\thw->fc.current_mode = ixgbe_fc_full;\t \n\tixgbe_pbthresh_setup(adapter);\n\thw->fc.pause_time = IXGBE_DEFAULT_FCPAUSE;\n\thw->fc.send_xon = true;\n\thw->fc.disable_fc_autoneg = ixgbe_device_supports_autoneg_fc(hw);\n\n#ifdef CONFIG_PCI_IOV\n\tif (max_vfs > 0)\n\t\te_dev_warn(\"Enabling SR-IOV VFs using the max_vfs module parameter is deprecated - please use the pci sysfs interface instead.\\n\");\n\n\t \n\tif (hw->mac.type != ixgbe_mac_82598EB) {\n\t\tif (max_vfs > IXGBE_MAX_VFS_DRV_LIMIT) {\n\t\t\tmax_vfs = 0;\n\t\t\te_dev_warn(\"max_vfs parameter out of range. Not assigning any SR-IOV VFs\\n\");\n\t\t}\n\t}\n#endif  \n\n\t \n\tadapter->rx_itr_setting = 1;\n\tadapter->tx_itr_setting = 1;\n\n\t \n\tadapter->tx_ring_count = IXGBE_DEFAULT_TXD;\n\tadapter->rx_ring_count = IXGBE_DEFAULT_RXD;\n\n\t \n\tadapter->tx_work_limit = IXGBE_DEFAULT_TX_WORK;\n\n\t \n\tif (ixgbe_init_eeprom_params_generic(hw)) {\n\t\te_dev_err(\"EEPROM initialization failed\\n\");\n\t\treturn -EIO;\n\t}\n\n\t \n\tset_bit(0, adapter->fwd_bitmask);\n\tset_bit(__IXGBE_DOWN, &adapter->state);\n\n\t \n\tif (nr_cpu_ids > IXGBE_MAX_XDP_QS)\n\t\tstatic_branch_enable(&ixgbe_xdp_locking_key);\n\n\treturn 0;\n}\n\n \nint ixgbe_setup_tx_resources(struct ixgbe_ring *tx_ring)\n{\n\tstruct device *dev = tx_ring->dev;\n\tint orig_node = dev_to_node(dev);\n\tint ring_node = NUMA_NO_NODE;\n\tint size;\n\n\tsize = sizeof(struct ixgbe_tx_buffer) * tx_ring->count;\n\n\tif (tx_ring->q_vector)\n\t\tring_node = tx_ring->q_vector->numa_node;\n\n\ttx_ring->tx_buffer_info = vmalloc_node(size, ring_node);\n\tif (!tx_ring->tx_buffer_info)\n\t\ttx_ring->tx_buffer_info = vmalloc(size);\n\tif (!tx_ring->tx_buffer_info)\n\t\tgoto err;\n\n\t \n\ttx_ring->size = tx_ring->count * sizeof(union ixgbe_adv_tx_desc);\n\ttx_ring->size = ALIGN(tx_ring->size, 4096);\n\n\tset_dev_node(dev, ring_node);\n\ttx_ring->desc = dma_alloc_coherent(dev,\n\t\t\t\t\t   tx_ring->size,\n\t\t\t\t\t   &tx_ring->dma,\n\t\t\t\t\t   GFP_KERNEL);\n\tset_dev_node(dev, orig_node);\n\tif (!tx_ring->desc)\n\t\ttx_ring->desc = dma_alloc_coherent(dev, tx_ring->size,\n\t\t\t\t\t\t   &tx_ring->dma, GFP_KERNEL);\n\tif (!tx_ring->desc)\n\t\tgoto err;\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\treturn 0;\n\nerr:\n\tvfree(tx_ring->tx_buffer_info);\n\ttx_ring->tx_buffer_info = NULL;\n\tdev_err(dev, \"Unable to allocate memory for the Tx descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int ixgbe_setup_all_tx_resources(struct ixgbe_adapter *adapter)\n{\n\tint i, j = 0, err = 0;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\terr = ixgbe_setup_tx_resources(adapter->tx_ring[i]);\n\t\tif (!err)\n\t\t\tcontinue;\n\n\t\te_err(probe, \"Allocation for Tx Queue %u failed\\n\", i);\n\t\tgoto err_setup_tx;\n\t}\n\tfor (j = 0; j < adapter->num_xdp_queues; j++) {\n\t\terr = ixgbe_setup_tx_resources(adapter->xdp_ring[j]);\n\t\tif (!err)\n\t\t\tcontinue;\n\n\t\te_err(probe, \"Allocation for Tx Queue %u failed\\n\", j);\n\t\tgoto err_setup_tx;\n\t}\n\n\treturn 0;\nerr_setup_tx:\n\t \n\twhile (j--)\n\t\tixgbe_free_tx_resources(adapter->xdp_ring[j]);\n\twhile (i--)\n\t\tixgbe_free_tx_resources(adapter->tx_ring[i]);\n\treturn err;\n}\n\nstatic int ixgbe_rx_napi_id(struct ixgbe_ring *rx_ring)\n{\n\tstruct ixgbe_q_vector *q_vector = rx_ring->q_vector;\n\n\treturn q_vector ? q_vector->napi.napi_id : 0;\n}\n\n \nint ixgbe_setup_rx_resources(struct ixgbe_adapter *adapter,\n\t\t\t     struct ixgbe_ring *rx_ring)\n{\n\tstruct device *dev = rx_ring->dev;\n\tint orig_node = dev_to_node(dev);\n\tint ring_node = NUMA_NO_NODE;\n\tint size;\n\n\tsize = sizeof(struct ixgbe_rx_buffer) * rx_ring->count;\n\n\tif (rx_ring->q_vector)\n\t\tring_node = rx_ring->q_vector->numa_node;\n\n\trx_ring->rx_buffer_info = vmalloc_node(size, ring_node);\n\tif (!rx_ring->rx_buffer_info)\n\t\trx_ring->rx_buffer_info = vmalloc(size);\n\tif (!rx_ring->rx_buffer_info)\n\t\tgoto err;\n\n\t \n\trx_ring->size = rx_ring->count * sizeof(union ixgbe_adv_rx_desc);\n\trx_ring->size = ALIGN(rx_ring->size, 4096);\n\n\tset_dev_node(dev, ring_node);\n\trx_ring->desc = dma_alloc_coherent(dev,\n\t\t\t\t\t   rx_ring->size,\n\t\t\t\t\t   &rx_ring->dma,\n\t\t\t\t\t   GFP_KERNEL);\n\tset_dev_node(dev, orig_node);\n\tif (!rx_ring->desc)\n\t\trx_ring->desc = dma_alloc_coherent(dev, rx_ring->size,\n\t\t\t\t\t\t   &rx_ring->dma, GFP_KERNEL);\n\tif (!rx_ring->desc)\n\t\tgoto err;\n\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n\n\t \n\tif (xdp_rxq_info_reg(&rx_ring->xdp_rxq, adapter->netdev,\n\t\t\t     rx_ring->queue_index, ixgbe_rx_napi_id(rx_ring)) < 0)\n\t\tgoto err;\n\n\tWRITE_ONCE(rx_ring->xdp_prog, adapter->xdp_prog);\n\n\treturn 0;\nerr:\n\tvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\tdev_err(dev, \"Unable to allocate memory for the Rx descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int ixgbe_setup_all_rx_resources(struct ixgbe_adapter *adapter)\n{\n\tint i, err = 0;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\terr = ixgbe_setup_rx_resources(adapter, adapter->rx_ring[i]);\n\t\tif (!err)\n\t\t\tcontinue;\n\n\t\te_err(probe, \"Allocation for Rx Queue %u failed\\n\", i);\n\t\tgoto err_setup_rx;\n\t}\n\n#ifdef IXGBE_FCOE\n\terr = ixgbe_setup_fcoe_ddp_resources(adapter);\n\tif (!err)\n#endif\n\t\treturn 0;\nerr_setup_rx:\n\t \n\twhile (i--)\n\t\tixgbe_free_rx_resources(adapter->rx_ring[i]);\n\treturn err;\n}\n\n \nvoid ixgbe_free_tx_resources(struct ixgbe_ring *tx_ring)\n{\n\tixgbe_clean_tx_ring(tx_ring);\n\n\tvfree(tx_ring->tx_buffer_info);\n\ttx_ring->tx_buffer_info = NULL;\n\n\t \n\tif (!tx_ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(tx_ring->dev, tx_ring->size,\n\t\t\t  tx_ring->desc, tx_ring->dma);\n\n\ttx_ring->desc = NULL;\n}\n\n \nstatic void ixgbe_free_all_tx_resources(struct ixgbe_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tif (adapter->tx_ring[i]->desc)\n\t\t\tixgbe_free_tx_resources(adapter->tx_ring[i]);\n\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\tif (adapter->xdp_ring[i]->desc)\n\t\t\tixgbe_free_tx_resources(adapter->xdp_ring[i]);\n}\n\n \nvoid ixgbe_free_rx_resources(struct ixgbe_ring *rx_ring)\n{\n\tixgbe_clean_rx_ring(rx_ring);\n\n\trx_ring->xdp_prog = NULL;\n\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\tvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\n\t \n\tif (!rx_ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(rx_ring->dev, rx_ring->size,\n\t\t\t  rx_ring->desc, rx_ring->dma);\n\n\trx_ring->desc = NULL;\n}\n\n \nstatic void ixgbe_free_all_rx_resources(struct ixgbe_adapter *adapter)\n{\n\tint i;\n\n#ifdef IXGBE_FCOE\n\tixgbe_free_fcoe_ddp_resources(adapter);\n\n#endif\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tif (adapter->rx_ring[i]->desc)\n\t\t\tixgbe_free_rx_resources(adapter->rx_ring[i]);\n}\n\n \nstatic int ixgbe_max_xdp_frame_size(struct ixgbe_adapter *adapter)\n{\n\tif (PAGE_SIZE >= 8192 || adapter->flags2 & IXGBE_FLAG2_RX_LEGACY)\n\t\treturn IXGBE_RXBUFFER_2K;\n\telse\n\t\treturn IXGBE_RXBUFFER_3K;\n}\n\n \nstatic int ixgbe_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\n\tif (ixgbe_enabled_xdp_adapter(adapter)) {\n\t\tint new_frame_size = new_mtu + IXGBE_PKT_HDR_PAD;\n\n\t\tif (new_frame_size > ixgbe_max_xdp_frame_size(adapter)) {\n\t\t\te_warn(probe, \"Requested MTU size is not supported with XDP\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tif ((adapter->flags & IXGBE_FLAG_SRIOV_ENABLED) &&\n\t    (adapter->hw.mac.type == ixgbe_mac_82599EB) &&\n\t    (new_mtu > ETH_DATA_LEN))\n\t\te_warn(probe, \"Setting MTU > 1500 will disable legacy VFs\\n\");\n\n\tnetdev_dbg(netdev, \"changing MTU from %d to %d\\n\",\n\t\t   netdev->mtu, new_mtu);\n\n\t \n\tnetdev->mtu = new_mtu;\n\n\tif (netif_running(netdev))\n\t\tixgbe_reinit_locked(adapter);\n\n\treturn 0;\n}\n\n \nint ixgbe_open(struct net_device *netdev)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint err, queues;\n\n\t \n\tif (test_bit(__IXGBE_TESTING, &adapter->state))\n\t\treturn -EBUSY;\n\n\tnetif_carrier_off(netdev);\n\n\t \n\terr = ixgbe_setup_all_tx_resources(adapter);\n\tif (err)\n\t\tgoto err_setup_tx;\n\n\t \n\terr = ixgbe_setup_all_rx_resources(adapter);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\tixgbe_configure(adapter);\n\n\terr = ixgbe_request_irq(adapter);\n\tif (err)\n\t\tgoto err_req_irq;\n\n\t \n\tqueues = adapter->num_tx_queues;\n\terr = netif_set_real_num_tx_queues(netdev, queues);\n\tif (err)\n\t\tgoto err_set_queues;\n\n\tqueues = adapter->num_rx_queues;\n\terr = netif_set_real_num_rx_queues(netdev, queues);\n\tif (err)\n\t\tgoto err_set_queues;\n\n\tixgbe_ptp_init(adapter);\n\n\tixgbe_up_complete(adapter);\n\n\tudp_tunnel_nic_reset_ntf(netdev);\n\n\treturn 0;\n\nerr_set_queues:\n\tixgbe_free_irq(adapter);\nerr_req_irq:\n\tixgbe_free_all_rx_resources(adapter);\n\tif (hw->phy.ops.set_phy_power && !adapter->wol)\n\t\thw->phy.ops.set_phy_power(&adapter->hw, false);\nerr_setup_rx:\n\tixgbe_free_all_tx_resources(adapter);\nerr_setup_tx:\n\tixgbe_reset(adapter);\n\n\treturn err;\n}\n\nstatic void ixgbe_close_suspend(struct ixgbe_adapter *adapter)\n{\n\tixgbe_ptp_suspend(adapter);\n\n\tif (adapter->hw.phy.ops.enter_lplu) {\n\t\tadapter->hw.phy.reset_disable = true;\n\t\tixgbe_down(adapter);\n\t\tadapter->hw.phy.ops.enter_lplu(&adapter->hw);\n\t\tadapter->hw.phy.reset_disable = false;\n\t} else {\n\t\tixgbe_down(adapter);\n\t}\n\n\tixgbe_free_irq(adapter);\n\n\tixgbe_free_all_tx_resources(adapter);\n\tixgbe_free_all_rx_resources(adapter);\n}\n\n \nint ixgbe_close(struct net_device *netdev)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\n\tixgbe_ptp_stop(adapter);\n\n\tif (netif_device_present(netdev))\n\t\tixgbe_close_suspend(adapter);\n\n\tixgbe_fdir_filter_exit(adapter);\n\n\tixgbe_release_hw_control(adapter);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused ixgbe_resume(struct device *dev_d)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev_d);\n\tstruct ixgbe_adapter *adapter = pci_get_drvdata(pdev);\n\tstruct net_device *netdev = adapter->netdev;\n\tu32 err;\n\n\tadapter->hw.hw_addr = adapter->io_addr;\n\n\terr = pci_enable_device_mem(pdev);\n\tif (err) {\n\t\te_dev_err(\"Cannot enable PCI device from suspend\\n\");\n\t\treturn err;\n\t}\n\tsmp_mb__before_atomic();\n\tclear_bit(__IXGBE_DISABLED, &adapter->state);\n\tpci_set_master(pdev);\n\n\tdevice_wakeup_disable(dev_d);\n\n\tixgbe_reset(adapter);\n\n\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_WUS, ~0);\n\n\trtnl_lock();\n\terr = ixgbe_init_interrupt_scheme(adapter);\n\tif (!err && netif_running(netdev))\n\t\terr = ixgbe_open(netdev);\n\n\n\tif (!err)\n\t\tnetif_device_attach(netdev);\n\trtnl_unlock();\n\n\treturn err;\n}\n\nstatic int __ixgbe_shutdown(struct pci_dev *pdev, bool *enable_wake)\n{\n\tstruct ixgbe_adapter *adapter = pci_get_drvdata(pdev);\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 ctrl;\n\tu32 wufc = adapter->wol;\n\n\trtnl_lock();\n\tnetif_device_detach(netdev);\n\n\tif (netif_running(netdev))\n\t\tixgbe_close_suspend(adapter);\n\n\tixgbe_clear_interrupt_scheme(adapter);\n\trtnl_unlock();\n\n\tif (hw->mac.ops.stop_link_on_d3)\n\t\thw->mac.ops.stop_link_on_d3(hw);\n\n\tif (wufc) {\n\t\tu32 fctrl;\n\n\t\tixgbe_set_rx_mode(netdev);\n\n\t\t \n\t\tif (hw->mac.ops.enable_tx_laser)\n\t\t\thw->mac.ops.enable_tx_laser(hw);\n\n\t\t \n\t\tfctrl = IXGBE_READ_REG(hw, IXGBE_FCTRL);\n\t\tfctrl |= IXGBE_FCTRL_MPE;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_FCTRL, fctrl);\n\n\t\tctrl = IXGBE_READ_REG(hw, IXGBE_CTRL);\n\t\tctrl |= IXGBE_CTRL_GIO_DIS;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_CTRL, ctrl);\n\n\t\tIXGBE_WRITE_REG(hw, IXGBE_WUFC, wufc);\n\t} else {\n\t\tIXGBE_WRITE_REG(hw, IXGBE_WUC, 0);\n\t\tIXGBE_WRITE_REG(hw, IXGBE_WUFC, 0);\n\t}\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\tpci_wake_from_d3(pdev, false);\n\t\tbreak;\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tpci_wake_from_d3(pdev, !!wufc);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t*enable_wake = !!wufc;\n\tif (hw->phy.ops.set_phy_power && !*enable_wake)\n\t\thw->phy.ops.set_phy_power(hw, false);\n\n\tixgbe_release_hw_control(adapter);\n\n\tif (!test_and_set_bit(__IXGBE_DISABLED, &adapter->state))\n\t\tpci_disable_device(pdev);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused ixgbe_suspend(struct device *dev_d)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev_d);\n\tint retval;\n\tbool wake;\n\n\tretval = __ixgbe_shutdown(pdev, &wake);\n\n\tdevice_set_wakeup_enable(dev_d, wake);\n\n\treturn retval;\n}\n\nstatic void ixgbe_shutdown(struct pci_dev *pdev)\n{\n\tbool wake;\n\n\t__ixgbe_shutdown(pdev, &wake);\n\n\tif (system_state == SYSTEM_POWER_OFF) {\n\t\tpci_wake_from_d3(pdev, wake);\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n\t}\n}\n\n \nvoid ixgbe_update_stats(struct ixgbe_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct ixgbe_hw_stats *hwstats = &adapter->stats;\n\tu64 total_mpc = 0;\n\tu32 i, missed_rx = 0, mpc, bprc, lxon, lxoff, xon_off_tot;\n\tu64 non_eop_descs = 0, restart_queue = 0, tx_busy = 0;\n\tu64 alloc_rx_page_failed = 0, alloc_rx_buff_failed = 0;\n\tu64 alloc_rx_page = 0;\n\tu64 bytes = 0, packets = 0, hw_csum_rx_error = 0;\n\n\tif (test_bit(__IXGBE_DOWN, &adapter->state) ||\n\t    test_bit(__IXGBE_RESETTING, &adapter->state))\n\t\treturn;\n\n\tif (adapter->flags2 & IXGBE_FLAG2_RSC_ENABLED) {\n\t\tu64 rsc_count = 0;\n\t\tu64 rsc_flush = 0;\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\trsc_count += adapter->rx_ring[i]->rx_stats.rsc_count;\n\t\t\trsc_flush += adapter->rx_ring[i]->rx_stats.rsc_flush;\n\t\t}\n\t\tadapter->rsc_total_count = rsc_count;\n\t\tadapter->rsc_total_flush = rsc_flush;\n\t}\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct ixgbe_ring *rx_ring = READ_ONCE(adapter->rx_ring[i]);\n\n\t\tif (!rx_ring)\n\t\t\tcontinue;\n\t\tnon_eop_descs += rx_ring->rx_stats.non_eop_descs;\n\t\talloc_rx_page += rx_ring->rx_stats.alloc_rx_page;\n\t\talloc_rx_page_failed += rx_ring->rx_stats.alloc_rx_page_failed;\n\t\talloc_rx_buff_failed += rx_ring->rx_stats.alloc_rx_buff_failed;\n\t\thw_csum_rx_error += rx_ring->rx_stats.csum_err;\n\t\tbytes += rx_ring->stats.bytes;\n\t\tpackets += rx_ring->stats.packets;\n\t}\n\tadapter->non_eop_descs = non_eop_descs;\n\tadapter->alloc_rx_page = alloc_rx_page;\n\tadapter->alloc_rx_page_failed = alloc_rx_page_failed;\n\tadapter->alloc_rx_buff_failed = alloc_rx_buff_failed;\n\tadapter->hw_csum_rx_error = hw_csum_rx_error;\n\tnetdev->stats.rx_bytes = bytes;\n\tnetdev->stats.rx_packets = packets;\n\n\tbytes = 0;\n\tpackets = 0;\n\t \n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct ixgbe_ring *tx_ring = READ_ONCE(adapter->tx_ring[i]);\n\n\t\tif (!tx_ring)\n\t\t\tcontinue;\n\t\trestart_queue += tx_ring->tx_stats.restart_queue;\n\t\ttx_busy += tx_ring->tx_stats.tx_busy;\n\t\tbytes += tx_ring->stats.bytes;\n\t\tpackets += tx_ring->stats.packets;\n\t}\n\tfor (i = 0; i < adapter->num_xdp_queues; i++) {\n\t\tstruct ixgbe_ring *xdp_ring = READ_ONCE(adapter->xdp_ring[i]);\n\n\t\tif (!xdp_ring)\n\t\t\tcontinue;\n\t\trestart_queue += xdp_ring->tx_stats.restart_queue;\n\t\ttx_busy += xdp_ring->tx_stats.tx_busy;\n\t\tbytes += xdp_ring->stats.bytes;\n\t\tpackets += xdp_ring->stats.packets;\n\t}\n\tadapter->restart_queue = restart_queue;\n\tadapter->tx_busy = tx_busy;\n\tnetdev->stats.tx_bytes = bytes;\n\tnetdev->stats.tx_packets = packets;\n\n\thwstats->crcerrs += IXGBE_READ_REG(hw, IXGBE_CRCERRS);\n\n\t \n\tfor (i = 0; i < 8; i++) {\n\t\t \n\t\tmpc = IXGBE_READ_REG(hw, IXGBE_MPC(i));\n\t\tmissed_rx += mpc;\n\t\thwstats->mpc[i] += mpc;\n\t\ttotal_mpc += hwstats->mpc[i];\n\t\thwstats->pxontxc[i] += IXGBE_READ_REG(hw, IXGBE_PXONTXC(i));\n\t\thwstats->pxofftxc[i] += IXGBE_READ_REG(hw, IXGBE_PXOFFTXC(i));\n\t\tswitch (hw->mac.type) {\n\t\tcase ixgbe_mac_82598EB:\n\t\t\thwstats->rnbc[i] += IXGBE_READ_REG(hw, IXGBE_RNBC(i));\n\t\t\thwstats->qbtc[i] += IXGBE_READ_REG(hw, IXGBE_QBTC(i));\n\t\t\thwstats->qbrc[i] += IXGBE_READ_REG(hw, IXGBE_QBRC(i));\n\t\t\thwstats->pxonrxc[i] +=\n\t\t\t\tIXGBE_READ_REG(hw, IXGBE_PXONRXC(i));\n\t\t\tbreak;\n\t\tcase ixgbe_mac_82599EB:\n\t\tcase ixgbe_mac_X540:\n\t\tcase ixgbe_mac_X550:\n\t\tcase ixgbe_mac_X550EM_x:\n\t\tcase ixgbe_mac_x550em_a:\n\t\t\thwstats->pxonrxc[i] +=\n\t\t\t\tIXGBE_READ_REG(hw, IXGBE_PXONRXCNT(i));\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < 16; i++) {\n\t\thwstats->qptc[i] += IXGBE_READ_REG(hw, IXGBE_QPTC(i));\n\t\thwstats->qprc[i] += IXGBE_READ_REG(hw, IXGBE_QPRC(i));\n\t\tif ((hw->mac.type == ixgbe_mac_82599EB) ||\n\t\t    (hw->mac.type == ixgbe_mac_X540) ||\n\t\t    (hw->mac.type == ixgbe_mac_X550) ||\n\t\t    (hw->mac.type == ixgbe_mac_X550EM_x) ||\n\t\t    (hw->mac.type == ixgbe_mac_x550em_a)) {\n\t\t\thwstats->qbtc[i] += IXGBE_READ_REG(hw, IXGBE_QBTC_L(i));\n\t\t\tIXGBE_READ_REG(hw, IXGBE_QBTC_H(i));  \n\t\t\thwstats->qbrc[i] += IXGBE_READ_REG(hw, IXGBE_QBRC_L(i));\n\t\t\tIXGBE_READ_REG(hw, IXGBE_QBRC_H(i));  \n\t\t}\n\t}\n\n\thwstats->gprc += IXGBE_READ_REG(hw, IXGBE_GPRC);\n\t \n\thwstats->gprc -= missed_rx;\n\n\tixgbe_update_xoff_received(adapter);\n\n\t \n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\thwstats->lxonrxc += IXGBE_READ_REG(hw, IXGBE_LXONRXC);\n\t\thwstats->gorc += IXGBE_READ_REG(hw, IXGBE_GORCH);\n\t\thwstats->gotc += IXGBE_READ_REG(hw, IXGBE_GOTCH);\n\t\thwstats->tor += IXGBE_READ_REG(hw, IXGBE_TORH);\n\t\tbreak;\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\t \n\t\thwstats->o2bgptc += IXGBE_READ_REG(hw, IXGBE_O2BGPTC);\n\t\thwstats->o2bspc += IXGBE_READ_REG(hw, IXGBE_O2BSPC);\n\t\thwstats->b2ospc += IXGBE_READ_REG(hw, IXGBE_B2OSPC);\n\t\thwstats->b2ogprc += IXGBE_READ_REG(hw, IXGBE_B2OGPRC);\n\t\tfallthrough;\n\tcase ixgbe_mac_82599EB:\n\t\tfor (i = 0; i < 16; i++)\n\t\t\tadapter->hw_rx_no_dma_resources +=\n\t\t\t\t\t     IXGBE_READ_REG(hw, IXGBE_QPRDC(i));\n\t\thwstats->gorc += IXGBE_READ_REG(hw, IXGBE_GORCL);\n\t\tIXGBE_READ_REG(hw, IXGBE_GORCH);  \n\t\thwstats->gotc += IXGBE_READ_REG(hw, IXGBE_GOTCL);\n\t\tIXGBE_READ_REG(hw, IXGBE_GOTCH);  \n\t\thwstats->tor += IXGBE_READ_REG(hw, IXGBE_TORL);\n\t\tIXGBE_READ_REG(hw, IXGBE_TORH);  \n\t\thwstats->lxonrxc += IXGBE_READ_REG(hw, IXGBE_LXONRXCNT);\n\t\thwstats->fdirmatch += IXGBE_READ_REG(hw, IXGBE_FDIRMATCH);\n\t\thwstats->fdirmiss += IXGBE_READ_REG(hw, IXGBE_FDIRMISS);\n#ifdef IXGBE_FCOE\n\t\thwstats->fccrc += IXGBE_READ_REG(hw, IXGBE_FCCRC);\n\t\thwstats->fcoerpdc += IXGBE_READ_REG(hw, IXGBE_FCOERPDC);\n\t\thwstats->fcoeprc += IXGBE_READ_REG(hw, IXGBE_FCOEPRC);\n\t\thwstats->fcoeptc += IXGBE_READ_REG(hw, IXGBE_FCOEPTC);\n\t\thwstats->fcoedwrc += IXGBE_READ_REG(hw, IXGBE_FCOEDWRC);\n\t\thwstats->fcoedwtc += IXGBE_READ_REG(hw, IXGBE_FCOEDWTC);\n\t\t \n\t\tif (adapter->fcoe.ddp_pool) {\n\t\t\tstruct ixgbe_fcoe *fcoe = &adapter->fcoe;\n\t\t\tstruct ixgbe_fcoe_ddp_pool *ddp_pool;\n\t\t\tunsigned int cpu;\n\t\t\tu64 noddp = 0, noddp_ext_buff = 0;\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tddp_pool = per_cpu_ptr(fcoe->ddp_pool, cpu);\n\t\t\t\tnoddp += ddp_pool->noddp;\n\t\t\t\tnoddp_ext_buff += ddp_pool->noddp_ext_buff;\n\t\t\t}\n\t\t\thwstats->fcoe_noddp = noddp;\n\t\t\thwstats->fcoe_noddp_ext_buff = noddp_ext_buff;\n\t\t}\n#endif  \n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tbprc = IXGBE_READ_REG(hw, IXGBE_BPRC);\n\thwstats->bprc += bprc;\n\thwstats->mprc += IXGBE_READ_REG(hw, IXGBE_MPRC);\n\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\thwstats->mprc -= bprc;\n\thwstats->roc += IXGBE_READ_REG(hw, IXGBE_ROC);\n\thwstats->prc64 += IXGBE_READ_REG(hw, IXGBE_PRC64);\n\thwstats->prc127 += IXGBE_READ_REG(hw, IXGBE_PRC127);\n\thwstats->prc255 += IXGBE_READ_REG(hw, IXGBE_PRC255);\n\thwstats->prc511 += IXGBE_READ_REG(hw, IXGBE_PRC511);\n\thwstats->prc1023 += IXGBE_READ_REG(hw, IXGBE_PRC1023);\n\thwstats->prc1522 += IXGBE_READ_REG(hw, IXGBE_PRC1522);\n\thwstats->rlec += IXGBE_READ_REG(hw, IXGBE_RLEC);\n\tlxon = IXGBE_READ_REG(hw, IXGBE_LXONTXC);\n\thwstats->lxontxc += lxon;\n\tlxoff = IXGBE_READ_REG(hw, IXGBE_LXOFFTXC);\n\thwstats->lxofftxc += lxoff;\n\thwstats->gptc += IXGBE_READ_REG(hw, IXGBE_GPTC);\n\thwstats->mptc += IXGBE_READ_REG(hw, IXGBE_MPTC);\n\t \n\txon_off_tot = lxon + lxoff;\n\thwstats->gptc -= xon_off_tot;\n\thwstats->mptc -= xon_off_tot;\n\thwstats->gotc -= (xon_off_tot * (ETH_ZLEN + ETH_FCS_LEN));\n\thwstats->ruc += IXGBE_READ_REG(hw, IXGBE_RUC);\n\thwstats->rfc += IXGBE_READ_REG(hw, IXGBE_RFC);\n\thwstats->rjc += IXGBE_READ_REG(hw, IXGBE_RJC);\n\thwstats->tpr += IXGBE_READ_REG(hw, IXGBE_TPR);\n\thwstats->ptc64 += IXGBE_READ_REG(hw, IXGBE_PTC64);\n\thwstats->ptc64 -= xon_off_tot;\n\thwstats->ptc127 += IXGBE_READ_REG(hw, IXGBE_PTC127);\n\thwstats->ptc255 += IXGBE_READ_REG(hw, IXGBE_PTC255);\n\thwstats->ptc511 += IXGBE_READ_REG(hw, IXGBE_PTC511);\n\thwstats->ptc1023 += IXGBE_READ_REG(hw, IXGBE_PTC1023);\n\thwstats->ptc1522 += IXGBE_READ_REG(hw, IXGBE_PTC1522);\n\thwstats->bptc += IXGBE_READ_REG(hw, IXGBE_BPTC);\n\n\t \n\tnetdev->stats.multicast = hwstats->mprc;\n\n\t \n\tnetdev->stats.rx_errors = hwstats->crcerrs + hwstats->rlec;\n\tnetdev->stats.rx_dropped = 0;\n\tnetdev->stats.rx_length_errors = hwstats->rlec;\n\tnetdev->stats.rx_crc_errors = hwstats->crcerrs;\n\tnetdev->stats.rx_missed_errors = total_mpc;\n\n\t \n\tif (!test_bit(__IXGBE_RESETTING, &adapter->state)) {\n\t\tfor (i = 0; i < adapter->num_vfs; i++) {\n\t\t\tUPDATE_VF_COUNTER_32bit(IXGBE_PVFGPRC(i),\n\t\t\t\t\t\tadapter->vfinfo[i].last_vfstats.gprc,\n\t\t\t\t\t\tadapter->vfinfo[i].vfstats.gprc);\n\t\t\tUPDATE_VF_COUNTER_32bit(IXGBE_PVFGPTC(i),\n\t\t\t\t\t\tadapter->vfinfo[i].last_vfstats.gptc,\n\t\t\t\t\t\tadapter->vfinfo[i].vfstats.gptc);\n\t\t\tUPDATE_VF_COUNTER_36bit(IXGBE_PVFGORC_LSB(i),\n\t\t\t\t\t\tIXGBE_PVFGORC_MSB(i),\n\t\t\t\t\t\tadapter->vfinfo[i].last_vfstats.gorc,\n\t\t\t\t\t\tadapter->vfinfo[i].vfstats.gorc);\n\t\t\tUPDATE_VF_COUNTER_36bit(IXGBE_PVFGOTC_LSB(i),\n\t\t\t\t\t\tIXGBE_PVFGOTC_MSB(i),\n\t\t\t\t\t\tadapter->vfinfo[i].last_vfstats.gotc,\n\t\t\t\t\t\tadapter->vfinfo[i].vfstats.gotc);\n\t\t\tUPDATE_VF_COUNTER_32bit(IXGBE_PVFMPRC(i),\n\t\t\t\t\t\tadapter->vfinfo[i].last_vfstats.mprc,\n\t\t\t\t\t\tadapter->vfinfo[i].vfstats.mprc);\n\t\t}\n\t}\n}\n\n \nstatic void ixgbe_fdir_reinit_subtask(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tint i;\n\n\tif (!(adapter->flags2 & IXGBE_FLAG2_FDIR_REQUIRES_REINIT))\n\t\treturn;\n\n\tadapter->flags2 &= ~IXGBE_FLAG2_FDIR_REQUIRES_REINIT;\n\n\t \n\tif (test_bit(__IXGBE_DOWN, &adapter->state))\n\t\treturn;\n\n\t \n\tif (!(adapter->flags & IXGBE_FLAG_FDIR_HASH_CAPABLE))\n\t\treturn;\n\n\tadapter->fdir_overflow++;\n\n\tif (ixgbe_reinit_fdir_tables_82599(hw) == 0) {\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\tset_bit(__IXGBE_TX_FDIR_INIT_DONE,\n\t\t\t\t&(adapter->tx_ring[i]->state));\n\t\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\t\tset_bit(__IXGBE_TX_FDIR_INIT_DONE,\n\t\t\t\t&adapter->xdp_ring[i]->state);\n\t\t \n\t\tIXGBE_WRITE_REG(hw, IXGBE_EIMS, IXGBE_EIMS_FLOW_DIR);\n\t} else {\n\t\te_err(probe, \"failed to finish FDIR re-initialization, \"\n\t\t      \"ignored adding FDIR ATR filters\\n\");\n\t}\n}\n\n \nstatic void ixgbe_check_hang_subtask(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu64 eics = 0;\n\tint i;\n\n\t \n\tif (test_bit(__IXGBE_DOWN, &adapter->state) ||\n\t    test_bit(__IXGBE_REMOVING, &adapter->state) ||\n\t    test_bit(__IXGBE_RESETTING, &adapter->state))\n\t\treturn;\n\n\t \n\tif (netif_carrier_ok(adapter->netdev)) {\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\tset_check_for_tx_hang(adapter->tx_ring[i]);\n\t\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\t\tset_check_for_tx_hang(adapter->xdp_ring[i]);\n\t}\n\n\tif (!(adapter->flags & IXGBE_FLAG_MSIX_ENABLED)) {\n\t\t \n\t\tIXGBE_WRITE_REG(hw, IXGBE_EICS,\n\t\t\t(IXGBE_EICS_TCP_TIMER | IXGBE_EICS_OTHER));\n\t} else {\n\t\t \n\t\tfor (i = 0; i < adapter->num_q_vectors; i++) {\n\t\t\tstruct ixgbe_q_vector *qv = adapter->q_vector[i];\n\t\t\tif (qv->rx.ring || qv->tx.ring)\n\t\t\t\teics |= BIT_ULL(i);\n\t\t}\n\t}\n\n\t \n\tixgbe_irq_rearm_queues(adapter, eics);\n}\n\n \nstatic void ixgbe_watchdog_update_link(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 link_speed = adapter->link_speed;\n\tbool link_up = adapter->link_up;\n\tbool pfc_en = adapter->dcb_cfg.pfc_mode_enable;\n\n\tif (!(adapter->flags & IXGBE_FLAG_NEED_LINK_UPDATE))\n\t\treturn;\n\n\tif (hw->mac.ops.check_link) {\n\t\thw->mac.ops.check_link(hw, &link_speed, &link_up, false);\n\t} else {\n\t\t \n\t\tlink_speed = IXGBE_LINK_SPEED_10GB_FULL;\n\t\tlink_up = true;\n\t}\n\n\tif (adapter->ixgbe_ieee_pfc)\n\t\tpfc_en |= !!(adapter->ixgbe_ieee_pfc->pfc_en);\n\n\tif (link_up && !((adapter->flags & IXGBE_FLAG_DCB_ENABLED) && pfc_en)) {\n\t\thw->mac.ops.fc_enable(hw);\n\t\tixgbe_set_rx_drop_en(adapter);\n\t}\n\n\tif (link_up ||\n\t    time_after(jiffies, (adapter->link_check_timeout +\n\t\t\t\t IXGBE_TRY_LINK_TIMEOUT))) {\n\t\tadapter->flags &= ~IXGBE_FLAG_NEED_LINK_UPDATE;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_EIMS, IXGBE_EIMC_LSC);\n\t\tIXGBE_WRITE_FLUSH(hw);\n\t}\n\n\tadapter->link_up = link_up;\n\tadapter->link_speed = link_speed;\n}\n\nstatic void ixgbe_update_default_up(struct ixgbe_adapter *adapter)\n{\n#ifdef CONFIG_IXGBE_DCB\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct dcb_app app = {\n\t\t\t      .selector = IEEE_8021QAZ_APP_SEL_ETHERTYPE,\n\t\t\t      .protocol = 0,\n\t\t\t     };\n\tu8 up = 0;\n\n\tif (adapter->dcbx_cap & DCB_CAP_DCBX_VER_IEEE)\n\t\tup = dcb_ieee_getapp_mask(netdev, &app);\n\n\tadapter->default_up = (up > 1) ? (ffs(up) - 1) : 0;\n#endif\n}\n\n \nstatic void ixgbe_watchdog_link_is_up(struct ixgbe_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 link_speed = adapter->link_speed;\n\tconst char *speed_str;\n\tbool flow_rx, flow_tx;\n\n\t \n\tif (netif_carrier_ok(netdev))\n\t\treturn;\n\n\tadapter->flags2 &= ~IXGBE_FLAG2_SEARCH_FOR_SFP;\n\n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB: {\n\t\tu32 frctl = IXGBE_READ_REG(hw, IXGBE_FCTRL);\n\t\tu32 rmcs = IXGBE_READ_REG(hw, IXGBE_RMCS);\n\t\tflow_rx = !!(frctl & IXGBE_FCTRL_RFCE);\n\t\tflow_tx = !!(rmcs & IXGBE_RMCS_TFCE_802_3X);\n\t}\n\t\tbreak;\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\tcase ixgbe_mac_82599EB: {\n\t\tu32 mflcn = IXGBE_READ_REG(hw, IXGBE_MFLCN);\n\t\tu32 fccfg = IXGBE_READ_REG(hw, IXGBE_FCCFG);\n\t\tflow_rx = !!(mflcn & IXGBE_MFLCN_RFCE);\n\t\tflow_tx = !!(fccfg & IXGBE_FCCFG_TFCE_802_3X);\n\t}\n\t\tbreak;\n\tdefault:\n\t\tflow_tx = false;\n\t\tflow_rx = false;\n\t\tbreak;\n\t}\n\n\tadapter->last_rx_ptp_check = jiffies;\n\n\tif (test_bit(__IXGBE_PTP_RUNNING, &adapter->state))\n\t\tixgbe_ptp_start_cyclecounter(adapter);\n\n\tswitch (link_speed) {\n\tcase IXGBE_LINK_SPEED_10GB_FULL:\n\t\tspeed_str = \"10 Gbps\";\n\t\tbreak;\n\tcase IXGBE_LINK_SPEED_5GB_FULL:\n\t\tspeed_str = \"5 Gbps\";\n\t\tbreak;\n\tcase IXGBE_LINK_SPEED_2_5GB_FULL:\n\t\tspeed_str = \"2.5 Gbps\";\n\t\tbreak;\n\tcase IXGBE_LINK_SPEED_1GB_FULL:\n\t\tspeed_str = \"1 Gbps\";\n\t\tbreak;\n\tcase IXGBE_LINK_SPEED_100_FULL:\n\t\tspeed_str = \"100 Mbps\";\n\t\tbreak;\n\tcase IXGBE_LINK_SPEED_10_FULL:\n\t\tspeed_str = \"10 Mbps\";\n\t\tbreak;\n\tdefault:\n\t\tspeed_str = \"unknown speed\";\n\t\tbreak;\n\t}\n\te_info(drv, \"NIC Link is Up %s, Flow Control: %s\\n\", speed_str,\n\t       ((flow_rx && flow_tx) ? \"RX/TX\" :\n\t       (flow_rx ? \"RX\" :\n\t       (flow_tx ? \"TX\" : \"None\"))));\n\n\tnetif_carrier_on(netdev);\n\tixgbe_check_vf_rate_limit(adapter);\n\n\t \n\tnetif_tx_wake_all_queues(adapter->netdev);\n\n\t \n\tixgbe_update_default_up(adapter);\n\n\t \n\tixgbe_ping_all_vfs(adapter);\n}\n\n \nstatic void ixgbe_watchdog_link_is_down(struct ixgbe_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tadapter->link_up = false;\n\tadapter->link_speed = 0;\n\n\t \n\tif (!netif_carrier_ok(netdev))\n\t\treturn;\n\n\t \n\tif (ixgbe_is_sfp(hw) && hw->mac.type == ixgbe_mac_82598EB)\n\t\tadapter->flags2 |= IXGBE_FLAG2_SEARCH_FOR_SFP;\n\n\tif (test_bit(__IXGBE_PTP_RUNNING, &adapter->state))\n\t\tixgbe_ptp_start_cyclecounter(adapter);\n\n\te_info(drv, \"NIC Link is Down\\n\");\n\tnetif_carrier_off(netdev);\n\n\t \n\tixgbe_ping_all_vfs(adapter);\n}\n\nstatic bool ixgbe_ring_tx_pending(struct ixgbe_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct ixgbe_ring *tx_ring = adapter->tx_ring[i];\n\n\t\tif (tx_ring->next_to_use != tx_ring->next_to_clean)\n\t\t\treturn true;\n\t}\n\n\tfor (i = 0; i < adapter->num_xdp_queues; i++) {\n\t\tstruct ixgbe_ring *ring = adapter->xdp_ring[i];\n\n\t\tif (ring->next_to_use != ring->next_to_clean)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool ixgbe_vf_tx_pending(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct ixgbe_ring_feature *vmdq = &adapter->ring_feature[RING_F_VMDQ];\n\tu32 q_per_pool = __ALIGN_MASK(1, ~vmdq->mask);\n\n\tint i, j;\n\n\tif (!adapter->num_vfs)\n\t\treturn false;\n\n\t \n\tif (hw->mac.type >= ixgbe_mac_X550)\n\t\treturn false;\n\n\tfor (i = 0; i < adapter->num_vfs; i++) {\n\t\tfor (j = 0; j < q_per_pool; j++) {\n\t\t\tu32 h, t;\n\n\t\t\th = IXGBE_READ_REG(hw, IXGBE_PVFTDHN(q_per_pool, i, j));\n\t\t\tt = IXGBE_READ_REG(hw, IXGBE_PVFTDTN(q_per_pool, i, j));\n\n\t\t\tif (h != t)\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\n \nstatic void ixgbe_watchdog_flush_tx(struct ixgbe_adapter *adapter)\n{\n\tif (!netif_carrier_ok(adapter->netdev)) {\n\t\tif (ixgbe_ring_tx_pending(adapter) ||\n\t\t    ixgbe_vf_tx_pending(adapter)) {\n\t\t\t \n\t\t\te_warn(drv, \"initiating reset to clear Tx work after link loss\\n\");\n\t\t\tset_bit(__IXGBE_RESET_REQUESTED, &adapter->state);\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_PCI_IOV\nstatic void ixgbe_bad_vf_abort(struct ixgbe_adapter *adapter, u32 vf)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tif (adapter->hw.mac.type == ixgbe_mac_82599EB &&\n\t    adapter->flags2 & IXGBE_FLAG2_AUTO_DISABLE_VF) {\n\t\tadapter->vfinfo[vf].primary_abort_count++;\n\t\tif (adapter->vfinfo[vf].primary_abort_count ==\n\t\t    IXGBE_PRIMARY_ABORT_LIMIT) {\n\t\t\tixgbe_set_vf_link_state(adapter, vf,\n\t\t\t\t\t\tIFLA_VF_LINK_STATE_DISABLE);\n\t\t\tadapter->vfinfo[vf].primary_abort_count = 0;\n\n\t\t\te_info(drv,\n\t\t\t       \"Malicious Driver Detection event detected on PF %d VF %d MAC: %pM mdd-disable-vf=on\",\n\t\t\t       hw->bus.func, vf,\n\t\t\t       adapter->vfinfo[vf].vf_mac_addresses);\n\t\t}\n\t}\n}\n\nstatic void ixgbe_check_for_bad_vf(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tunsigned int vf;\n\tu32 gpc;\n\n\tif (!(netif_carrier_ok(adapter->netdev)))\n\t\treturn;\n\n\tgpc = IXGBE_READ_REG(hw, IXGBE_TXDGPC);\n\tif (gpc)  \n\t\treturn;\n\t \n\n\tif (!pdev)\n\t\treturn;\n\n\t \n\tfor (vf = 0; vf < adapter->num_vfs; ++vf) {\n\t\tstruct pci_dev *vfdev = adapter->vfinfo[vf].vfdev;\n\t\tu16 status_reg;\n\n\t\tif (!vfdev)\n\t\t\tcontinue;\n\t\tpci_read_config_word(vfdev, PCI_STATUS, &status_reg);\n\t\tif (status_reg != IXGBE_FAILED_READ_CFG_WORD &&\n\t\t    status_reg & PCI_STATUS_REC_MASTER_ABORT) {\n\t\t\tixgbe_bad_vf_abort(adapter, vf);\n\t\t\tpcie_flr(vfdev);\n\t\t}\n\t}\n}\n\nstatic void ixgbe_spoof_check(struct ixgbe_adapter *adapter)\n{\n\tu32 ssvpc;\n\n\t \n\tif (adapter->hw.mac.type == ixgbe_mac_82598EB ||\n\t    adapter->num_vfs == 0)\n\t\treturn;\n\n\tssvpc = IXGBE_READ_REG(&adapter->hw, IXGBE_SSVPC);\n\n\t \n\tif (!ssvpc)\n\t\treturn;\n\n\te_warn(drv, \"%u Spoofed packets detected\\n\", ssvpc);\n}\n#else\nstatic void ixgbe_spoof_check(struct ixgbe_adapter __always_unused *adapter)\n{\n}\n\nstatic void\nixgbe_check_for_bad_vf(struct ixgbe_adapter __always_unused *adapter)\n{\n}\n#endif  \n\n\n \nstatic void ixgbe_watchdog_subtask(struct ixgbe_adapter *adapter)\n{\n\t \n\tif (test_bit(__IXGBE_DOWN, &adapter->state) ||\n\t    test_bit(__IXGBE_REMOVING, &adapter->state) ||\n\t    test_bit(__IXGBE_RESETTING, &adapter->state))\n\t\treturn;\n\n\tixgbe_watchdog_update_link(adapter);\n\n\tif (adapter->link_up)\n\t\tixgbe_watchdog_link_is_up(adapter);\n\telse\n\t\tixgbe_watchdog_link_is_down(adapter);\n\n\tixgbe_check_for_bad_vf(adapter);\n\tixgbe_spoof_check(adapter);\n\tixgbe_update_stats(adapter);\n\n\tixgbe_watchdog_flush_tx(adapter);\n}\n\n \nstatic void ixgbe_sfp_detection_subtask(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\ts32 err;\n\n\t \n\tif (!(adapter->flags2 & IXGBE_FLAG2_SEARCH_FOR_SFP) &&\n\t    !(adapter->flags2 & IXGBE_FLAG2_SFP_NEEDS_RESET))\n\t\treturn;\n\n\tif (adapter->sfp_poll_time &&\n\t    time_after(adapter->sfp_poll_time, jiffies))\n\t\treturn;  \n\n\t \n\tif (test_and_set_bit(__IXGBE_IN_SFP_INIT, &adapter->state))\n\t\treturn;\n\n\tadapter->sfp_poll_time = jiffies + IXGBE_SFP_POLL_JIFFIES - 1;\n\n\terr = hw->phy.ops.identify_sfp(hw);\n\tif (err == IXGBE_ERR_SFP_NOT_SUPPORTED)\n\t\tgoto sfp_out;\n\n\tif (err == IXGBE_ERR_SFP_NOT_PRESENT) {\n\t\t \n\t\tadapter->flags2 |= IXGBE_FLAG2_SFP_NEEDS_RESET;\n\t}\n\n\t \n\tif (err)\n\t\tgoto sfp_out;\n\n\t \n\tif (!(adapter->flags2 & IXGBE_FLAG2_SFP_NEEDS_RESET))\n\t\tgoto sfp_out;\n\n\tadapter->flags2 &= ~IXGBE_FLAG2_SFP_NEEDS_RESET;\n\n\t \n\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\terr = hw->phy.ops.reset(hw);\n\telse\n\t\terr = hw->mac.ops.setup_sfp(hw);\n\n\tif (err == IXGBE_ERR_SFP_NOT_SUPPORTED)\n\t\tgoto sfp_out;\n\n\tadapter->flags |= IXGBE_FLAG_NEED_LINK_CONFIG;\n\te_info(probe, \"detected SFP+: %d\\n\", hw->phy.sfp_type);\n\nsfp_out:\n\tclear_bit(__IXGBE_IN_SFP_INIT, &adapter->state);\n\n\tif ((err == IXGBE_ERR_SFP_NOT_SUPPORTED) &&\n\t    (adapter->netdev->reg_state == NETREG_REGISTERED)) {\n\t\te_dev_err(\"failed to initialize because an unsupported \"\n\t\t\t  \"SFP+ module type was detected.\\n\");\n\t\te_dev_err(\"Reload the driver after installing a \"\n\t\t\t  \"supported module.\\n\");\n\t\tunregister_netdev(adapter->netdev);\n\t}\n}\n\n \nstatic void ixgbe_sfp_link_config_subtask(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 cap_speed;\n\tu32 speed;\n\tbool autoneg = false;\n\n\tif (!(adapter->flags & IXGBE_FLAG_NEED_LINK_CONFIG))\n\t\treturn;\n\n\t \n\tif (test_and_set_bit(__IXGBE_IN_SFP_INIT, &adapter->state))\n\t\treturn;\n\n\tadapter->flags &= ~IXGBE_FLAG_NEED_LINK_CONFIG;\n\n\thw->mac.ops.get_link_capabilities(hw, &cap_speed, &autoneg);\n\n\t \n\tif (!autoneg && (cap_speed & IXGBE_LINK_SPEED_10GB_FULL))\n\t\tspeed = IXGBE_LINK_SPEED_10GB_FULL;\n\telse\n\t\tspeed = cap_speed & (IXGBE_LINK_SPEED_10GB_FULL |\n\t\t\t\t     IXGBE_LINK_SPEED_1GB_FULL);\n\n\tif (hw->mac.ops.setup_link)\n\t\thw->mac.ops.setup_link(hw, speed, true);\n\n\tadapter->flags |= IXGBE_FLAG_NEED_LINK_UPDATE;\n\tadapter->link_check_timeout = jiffies;\n\tclear_bit(__IXGBE_IN_SFP_INIT, &adapter->state);\n}\n\n \nstatic void ixgbe_service_timer(struct timer_list *t)\n{\n\tstruct ixgbe_adapter *adapter = from_timer(adapter, t, service_timer);\n\tunsigned long next_event_offset;\n\n\t \n\tif (adapter->flags & IXGBE_FLAG_NEED_LINK_UPDATE)\n\t\tnext_event_offset = HZ / 10;\n\telse\n\t\tnext_event_offset = HZ * 2;\n\n\t \n\tmod_timer(&adapter->service_timer, next_event_offset + jiffies);\n\n\tixgbe_service_event_schedule(adapter);\n}\n\nstatic void ixgbe_phy_interrupt_subtask(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 status;\n\n\tif (!(adapter->flags2 & IXGBE_FLAG2_PHY_INTERRUPT))\n\t\treturn;\n\n\tadapter->flags2 &= ~IXGBE_FLAG2_PHY_INTERRUPT;\n\n\tif (!hw->phy.ops.handle_lasi)\n\t\treturn;\n\n\tstatus = hw->phy.ops.handle_lasi(&adapter->hw);\n\tif (status != IXGBE_ERR_OVERTEMP)\n\t\treturn;\n\n\te_crit(drv, \"%s\\n\", ixgbe_overheat_msg);\n}\n\nstatic void ixgbe_reset_subtask(struct ixgbe_adapter *adapter)\n{\n\tif (!test_and_clear_bit(__IXGBE_RESET_REQUESTED, &adapter->state))\n\t\treturn;\n\n\trtnl_lock();\n\t \n\tif (test_bit(__IXGBE_DOWN, &adapter->state) ||\n\t    test_bit(__IXGBE_REMOVING, &adapter->state) ||\n\t    test_bit(__IXGBE_RESETTING, &adapter->state)) {\n\t\trtnl_unlock();\n\t\treturn;\n\t}\n\n\tixgbe_dump(adapter);\n\tnetdev_err(adapter->netdev, \"Reset adapter\\n\");\n\tadapter->tx_timeout_count++;\n\n\tixgbe_reinit_locked(adapter);\n\trtnl_unlock();\n}\n\n \nstatic bool ixgbe_check_fw_error(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 fwsm;\n\n\t \n\tfwsm = IXGBE_READ_REG(hw, IXGBE_FWSM(hw));\n\n\tif (fwsm & IXGBE_FWSM_EXT_ERR_IND_MASK ||\n\t    !(fwsm & IXGBE_FWSM_FW_VAL_BIT))\n\t\te_dev_warn(\"Warning firmware error detected FWSM: 0x%08X\\n\",\n\t\t\t   fwsm);\n\n\tif (hw->mac.ops.fw_recovery_mode && hw->mac.ops.fw_recovery_mode(hw)) {\n\t\te_dev_err(\"Firmware recovery mode detected. Limiting functionality. Refer to the Intel(R) Ethernet Adapters and Devices User Guide for details on firmware recovery mode.\\n\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic void ixgbe_service_task(struct work_struct *work)\n{\n\tstruct ixgbe_adapter *adapter = container_of(work,\n\t\t\t\t\t\t     struct ixgbe_adapter,\n\t\t\t\t\t\t     service_task);\n\tif (ixgbe_removed(adapter->hw.hw_addr)) {\n\t\tif (!test_bit(__IXGBE_DOWN, &adapter->state)) {\n\t\t\trtnl_lock();\n\t\t\tixgbe_down(adapter);\n\t\t\trtnl_unlock();\n\t\t}\n\t\tixgbe_service_event_complete(adapter);\n\t\treturn;\n\t}\n\tif (ixgbe_check_fw_error(adapter)) {\n\t\tif (!test_bit(__IXGBE_DOWN, &adapter->state))\n\t\t\tunregister_netdev(adapter->netdev);\n\t\tixgbe_service_event_complete(adapter);\n\t\treturn;\n\t}\n\tixgbe_reset_subtask(adapter);\n\tixgbe_phy_interrupt_subtask(adapter);\n\tixgbe_sfp_detection_subtask(adapter);\n\tixgbe_sfp_link_config_subtask(adapter);\n\tixgbe_check_overtemp_subtask(adapter);\n\tixgbe_watchdog_subtask(adapter);\n\tixgbe_fdir_reinit_subtask(adapter);\n\tixgbe_check_hang_subtask(adapter);\n\n\tif (test_bit(__IXGBE_PTP_RUNNING, &adapter->state)) {\n\t\tixgbe_ptp_overflow_check(adapter);\n\t\tif (adapter->flags & IXGBE_FLAG_RX_HWTSTAMP_IN_REGISTER)\n\t\t\tixgbe_ptp_rx_hang(adapter);\n\t\tixgbe_ptp_tx_hang(adapter);\n\t}\n\n\tixgbe_service_event_complete(adapter);\n}\n\nstatic int ixgbe_tso(struct ixgbe_ring *tx_ring,\n\t\t     struct ixgbe_tx_buffer *first,\n\t\t     u8 *hdr_len,\n\t\t     struct ixgbe_ipsec_tx_data *itd)\n{\n\tu32 vlan_macip_lens, type_tucmd, mss_l4len_idx;\n\tstruct sk_buff *skb = first->skb;\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tstruct udphdr *udp;\n\t\tunsigned char *hdr;\n\t} l4;\n\tu32 paylen, l4_offset;\n\tu32 fceof_saidx = 0;\n\tint err;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (eth_p_mpls(first->protocol))\n\t\tip.hdr = skb_inner_network_header(skb);\n\telse\n\t\tip.hdr = skb_network_header(skb);\n\tl4.hdr = skb_checksum_start(skb);\n\n\t \n\ttype_tucmd = (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) ?\n\t\t      IXGBE_ADVTXD_TUCMD_L4T_UDP : IXGBE_ADVTXD_TUCMD_L4T_TCP;\n\n\t \n\tif (ip.v4->version == 4) {\n\t\tunsigned char *csum_start = skb_checksum_start(skb);\n\t\tunsigned char *trans_start = ip.hdr + (ip.v4->ihl * 4);\n\t\tint len = csum_start - trans_start;\n\n\t\t \n\t\tip.v4->check = (skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL) ?\n\t\t\t\t\t   csum_fold(csum_partial(trans_start,\n\t\t\t\t\t\t\t\t  len, 0)) : 0;\n\t\ttype_tucmd |= IXGBE_ADVTXD_TUCMD_IPV4;\n\n\t\tip.v4->tot_len = 0;\n\t\tfirst->tx_flags |= IXGBE_TX_FLAGS_TSO |\n\t\t\t\t   IXGBE_TX_FLAGS_CSUM |\n\t\t\t\t   IXGBE_TX_FLAGS_IPV4;\n\t} else {\n\t\tip.v6->payload_len = 0;\n\t\tfirst->tx_flags |= IXGBE_TX_FLAGS_TSO |\n\t\t\t\t   IXGBE_TX_FLAGS_CSUM;\n\t}\n\n\t \n\tl4_offset = l4.hdr - skb->data;\n\n\t \n\tpaylen = skb->len - l4_offset;\n\n\tif (type_tucmd & IXGBE_ADVTXD_TUCMD_L4T_TCP) {\n\t\t \n\t\t*hdr_len = (l4.tcp->doff * 4) + l4_offset;\n\t\tcsum_replace_by_diff(&l4.tcp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\t} else {\n\t\t \n\t\t*hdr_len = sizeof(*l4.udp) + l4_offset;\n\t\tcsum_replace_by_diff(&l4.udp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\t}\n\n\t \n\tfirst->gso_segs = skb_shinfo(skb)->gso_segs;\n\tfirst->bytecount += (first->gso_segs - 1) * *hdr_len;\n\n\t \n\tmss_l4len_idx = (*hdr_len - l4_offset) << IXGBE_ADVTXD_L4LEN_SHIFT;\n\tmss_l4len_idx |= skb_shinfo(skb)->gso_size << IXGBE_ADVTXD_MSS_SHIFT;\n\n\tfceof_saidx |= itd->sa_idx;\n\ttype_tucmd |= itd->flags | itd->trailer_len;\n\n\t \n\tvlan_macip_lens = l4.hdr - ip.hdr;\n\tvlan_macip_lens |= (ip.hdr - skb->data) << IXGBE_ADVTXD_MACLEN_SHIFT;\n\tvlan_macip_lens |= first->tx_flags & IXGBE_TX_FLAGS_VLAN_MASK;\n\n\tixgbe_tx_ctxtdesc(tx_ring, vlan_macip_lens, fceof_saidx, type_tucmd,\n\t\t\t  mss_l4len_idx);\n\n\treturn 1;\n}\n\nstatic void ixgbe_tx_csum(struct ixgbe_ring *tx_ring,\n\t\t\t  struct ixgbe_tx_buffer *first,\n\t\t\t  struct ixgbe_ipsec_tx_data *itd)\n{\n\tstruct sk_buff *skb = first->skb;\n\tu32 vlan_macip_lens = 0;\n\tu32 fceof_saidx = 0;\n\tu32 type_tucmd = 0;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL) {\ncsum_failed:\n\t\tif (!(first->tx_flags & (IXGBE_TX_FLAGS_HW_VLAN |\n\t\t\t\t\t IXGBE_TX_FLAGS_CC)))\n\t\t\treturn;\n\t\tgoto no_csum;\n\t}\n\n\tswitch (skb->csum_offset) {\n\tcase offsetof(struct tcphdr, check):\n\t\ttype_tucmd = IXGBE_ADVTXD_TUCMD_L4T_TCP;\n\t\tfallthrough;\n\tcase offsetof(struct udphdr, check):\n\t\tbreak;\n\tcase offsetof(struct sctphdr, checksum):\n\t\t \n\t\tif (skb_csum_is_sctp(skb)) {\n\t\t\ttype_tucmd = IXGBE_ADVTXD_TUCMD_L4T_SCTP;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tskb_checksum_help(skb);\n\t\tgoto csum_failed;\n\t}\n\n\t \n\tfirst->tx_flags |= IXGBE_TX_FLAGS_CSUM;\n\tvlan_macip_lens = skb_checksum_start_offset(skb) -\n\t\t\t  skb_network_offset(skb);\nno_csum:\n\t \n\tvlan_macip_lens |= skb_network_offset(skb) << IXGBE_ADVTXD_MACLEN_SHIFT;\n\tvlan_macip_lens |= first->tx_flags & IXGBE_TX_FLAGS_VLAN_MASK;\n\n\tfceof_saidx |= itd->sa_idx;\n\ttype_tucmd |= itd->flags | itd->trailer_len;\n\n\tixgbe_tx_ctxtdesc(tx_ring, vlan_macip_lens, fceof_saidx, type_tucmd, 0);\n}\n\n#define IXGBE_SET_FLAG(_input, _flag, _result) \\\n\t((_flag <= _result) ? \\\n\t ((u32)(_input & _flag) * (_result / _flag)) : \\\n\t ((u32)(_input & _flag) / (_flag / _result)))\n\nstatic u32 ixgbe_tx_cmd_type(struct sk_buff *skb, u32 tx_flags)\n{\n\t \n\tu32 cmd_type = IXGBE_ADVTXD_DTYP_DATA |\n\t\t       IXGBE_ADVTXD_DCMD_DEXT |\n\t\t       IXGBE_ADVTXD_DCMD_IFCS;\n\n\t \n\tcmd_type |= IXGBE_SET_FLAG(tx_flags, IXGBE_TX_FLAGS_HW_VLAN,\n\t\t\t\t   IXGBE_ADVTXD_DCMD_VLE);\n\n\t \n\tcmd_type |= IXGBE_SET_FLAG(tx_flags, IXGBE_TX_FLAGS_TSO,\n\t\t\t\t   IXGBE_ADVTXD_DCMD_TSE);\n\n\t \n\tcmd_type |= IXGBE_SET_FLAG(tx_flags, IXGBE_TX_FLAGS_TSTAMP,\n\t\t\t\t   IXGBE_ADVTXD_MAC_TSTAMP);\n\n\t \n\tcmd_type ^= IXGBE_SET_FLAG(skb->no_fcs, 1, IXGBE_ADVTXD_DCMD_IFCS);\n\n\treturn cmd_type;\n}\n\nstatic void ixgbe_tx_olinfo_status(union ixgbe_adv_tx_desc *tx_desc,\n\t\t\t\t   u32 tx_flags, unsigned int paylen)\n{\n\tu32 olinfo_status = paylen << IXGBE_ADVTXD_PAYLEN_SHIFT;\n\n\t \n\tolinfo_status |= IXGBE_SET_FLAG(tx_flags,\n\t\t\t\t\tIXGBE_TX_FLAGS_CSUM,\n\t\t\t\t\tIXGBE_ADVTXD_POPTS_TXSM);\n\n\t \n\tolinfo_status |= IXGBE_SET_FLAG(tx_flags,\n\t\t\t\t\tIXGBE_TX_FLAGS_IPV4,\n\t\t\t\t\tIXGBE_ADVTXD_POPTS_IXSM);\n\n\t \n\tolinfo_status |= IXGBE_SET_FLAG(tx_flags,\n\t\t\t\t\tIXGBE_TX_FLAGS_IPSEC,\n\t\t\t\t\tIXGBE_ADVTXD_POPTS_IPSEC);\n\n\t \n\tolinfo_status |= IXGBE_SET_FLAG(tx_flags,\n\t\t\t\t\tIXGBE_TX_FLAGS_CC,\n\t\t\t\t\tIXGBE_ADVTXD_CC);\n\n\ttx_desc->read.olinfo_status = cpu_to_le32(olinfo_status);\n}\n\nstatic int __ixgbe_maybe_stop_tx(struct ixgbe_ring *tx_ring, u16 size)\n{\n\tif (!netif_subqueue_try_stop(tx_ring->netdev, tx_ring->queue_index,\n\t\t\t\t     ixgbe_desc_unused(tx_ring), size))\n\t\treturn -EBUSY;\n\n\t++tx_ring->tx_stats.restart_queue;\n\treturn 0;\n}\n\nstatic inline int ixgbe_maybe_stop_tx(struct ixgbe_ring *tx_ring, u16 size)\n{\n\tif (likely(ixgbe_desc_unused(tx_ring) >= size))\n\t\treturn 0;\n\n\treturn __ixgbe_maybe_stop_tx(tx_ring, size);\n}\n\nstatic int ixgbe_tx_map(struct ixgbe_ring *tx_ring,\n\t\t\tstruct ixgbe_tx_buffer *first,\n\t\t\tconst u8 hdr_len)\n{\n\tstruct sk_buff *skb = first->skb;\n\tstruct ixgbe_tx_buffer *tx_buffer;\n\tunion ixgbe_adv_tx_desc *tx_desc;\n\tskb_frag_t *frag;\n\tdma_addr_t dma;\n\tunsigned int data_len, size;\n\tu32 tx_flags = first->tx_flags;\n\tu32 cmd_type = ixgbe_tx_cmd_type(skb, tx_flags);\n\tu16 i = tx_ring->next_to_use;\n\n\ttx_desc = IXGBE_TX_DESC(tx_ring, i);\n\n\tixgbe_tx_olinfo_status(tx_desc, tx_flags, skb->len - hdr_len);\n\n\tsize = skb_headlen(skb);\n\tdata_len = skb->data_len;\n\n#ifdef IXGBE_FCOE\n\tif (tx_flags & IXGBE_TX_FLAGS_FCOE) {\n\t\tif (data_len < sizeof(struct fcoe_crc_eof)) {\n\t\t\tsize -= sizeof(struct fcoe_crc_eof) - data_len;\n\t\t\tdata_len = 0;\n\t\t} else {\n\t\t\tdata_len -= sizeof(struct fcoe_crc_eof);\n\t\t}\n\t}\n\n#endif\n\tdma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);\n\n\ttx_buffer = first;\n\n\tfor (frag = &skb_shinfo(skb)->frags[0];; frag++) {\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto dma_error;\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, size);\n\t\tdma_unmap_addr_set(tx_buffer, dma, dma);\n\n\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\n\t\twhile (unlikely(size > IXGBE_MAX_DATA_PER_TXD)) {\n\t\t\ttx_desc->read.cmd_type_len =\n\t\t\t\tcpu_to_le32(cmd_type ^ IXGBE_MAX_DATA_PER_TXD);\n\n\t\t\ti++;\n\t\t\ttx_desc++;\n\t\t\tif (i == tx_ring->count) {\n\t\t\t\ttx_desc = IXGBE_TX_DESC(tx_ring, 0);\n\t\t\t\ti = 0;\n\t\t\t}\n\t\t\ttx_desc->read.olinfo_status = 0;\n\n\t\t\tdma += IXGBE_MAX_DATA_PER_TXD;\n\t\t\tsize -= IXGBE_MAX_DATA_PER_TXD;\n\n\t\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\t\t}\n\n\t\tif (likely(!data_len))\n\t\t\tbreak;\n\n\t\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type ^ size);\n\n\t\ti++;\n\t\ttx_desc++;\n\t\tif (i == tx_ring->count) {\n\t\t\ttx_desc = IXGBE_TX_DESC(tx_ring, 0);\n\t\t\ti = 0;\n\t\t}\n\t\ttx_desc->read.olinfo_status = 0;\n\n#ifdef IXGBE_FCOE\n\t\tsize = min_t(unsigned int, data_len, skb_frag_size(frag));\n#else\n\t\tsize = skb_frag_size(frag);\n#endif\n\t\tdata_len -= size;\n\n\t\tdma = skb_frag_dma_map(tx_ring->dev, frag, 0, size,\n\t\t\t\t       DMA_TO_DEVICE);\n\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t}\n\n\t \n\tcmd_type |= size | IXGBE_TXD_CMD;\n\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\n\tnetdev_tx_sent_queue(txring_txq(tx_ring), first->bytecount);\n\n\t \n\tfirst->time_stamp = jiffies;\n\n\tskb_tx_timestamp(skb);\n\n\t \n\twmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\ti++;\n\tif (i == tx_ring->count)\n\t\ti = 0;\n\n\ttx_ring->next_to_use = i;\n\n\tixgbe_maybe_stop_tx(tx_ring, DESC_NEEDED);\n\n\tif (netif_xmit_stopped(txring_txq(tx_ring)) || !netdev_xmit_more()) {\n\t\twritel(i, tx_ring->tail);\n\t}\n\n\treturn 0;\ndma_error:\n\tdev_err(tx_ring->dev, \"TX DMA map failed\\n\");\n\n\t \n\tfor (;;) {\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\t\tif (tx_buffer == first)\n\t\t\tbreak;\n\t\tif (i == 0)\n\t\t\ti += tx_ring->count;\n\t\ti--;\n\t}\n\n\tdev_kfree_skb_any(first->skb);\n\tfirst->skb = NULL;\n\n\ttx_ring->next_to_use = i;\n\n\treturn -1;\n}\n\nstatic void ixgbe_atr(struct ixgbe_ring *ring,\n\t\t      struct ixgbe_tx_buffer *first)\n{\n\tstruct ixgbe_q_vector *q_vector = ring->q_vector;\n\tunion ixgbe_atr_hash_dword input = { .dword = 0 };\n\tunion ixgbe_atr_hash_dword common = { .dword = 0 };\n\tunion {\n\t\tunsigned char *network;\n\t\tstruct iphdr *ipv4;\n\t\tstruct ipv6hdr *ipv6;\n\t} hdr;\n\tstruct tcphdr *th;\n\tunsigned int hlen;\n\tstruct sk_buff *skb;\n\t__be16 vlan_id;\n\tint l4_proto;\n\n\t \n\tif (!q_vector)\n\t\treturn;\n\n\t \n\tif (!ring->atr_sample_rate)\n\t\treturn;\n\n\tring->atr_count++;\n\n\t \n\tif ((first->protocol != htons(ETH_P_IP)) &&\n\t    (first->protocol != htons(ETH_P_IPV6)))\n\t\treturn;\n\n\t \n\tskb = first->skb;\n\thdr.network = skb_network_header(skb);\n\tif (unlikely(hdr.network <= skb->data))\n\t\treturn;\n\tif (skb->encapsulation &&\n\t    first->protocol == htons(ETH_P_IP) &&\n\t    hdr.ipv4->protocol == IPPROTO_UDP) {\n\t\tstruct ixgbe_adapter *adapter = q_vector->adapter;\n\n\t\tif (unlikely(skb_tail_pointer(skb) < hdr.network +\n\t\t\t     vxlan_headroom(0)))\n\t\t\treturn;\n\n\t\t \n\t\tif (adapter->vxlan_port &&\n\t\t    udp_hdr(skb)->dest == adapter->vxlan_port)\n\t\t\thdr.network = skb_inner_network_header(skb);\n\n\t\tif (adapter->geneve_port &&\n\t\t    udp_hdr(skb)->dest == adapter->geneve_port)\n\t\t\thdr.network = skb_inner_network_header(skb);\n\t}\n\n\t \n\tif (unlikely(skb_tail_pointer(skb) < hdr.network + 40))\n\t\treturn;\n\n\t \n\tswitch (hdr.ipv4->version) {\n\tcase IPVERSION:\n\t\t \n\t\thlen = (hdr.network[0] & 0x0F) << 2;\n\t\tl4_proto = hdr.ipv4->protocol;\n\t\tbreak;\n\tcase 6:\n\t\thlen = hdr.network - skb->data;\n\t\tl4_proto = ipv6_find_hdr(skb, &hlen, IPPROTO_TCP, NULL, NULL);\n\t\thlen -= hdr.network - skb->data;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (l4_proto != IPPROTO_TCP)\n\t\treturn;\n\n\tif (unlikely(skb_tail_pointer(skb) < hdr.network +\n\t\t     hlen + sizeof(struct tcphdr)))\n\t\treturn;\n\n\tth = (struct tcphdr *)(hdr.network + hlen);\n\n\t \n\tif (th->fin)\n\t\treturn;\n\n\t \n\tif (!th->syn && (ring->atr_count < ring->atr_sample_rate))\n\t\treturn;\n\n\t \n\tring->atr_count = 0;\n\n\tvlan_id = htons(first->tx_flags >> IXGBE_TX_FLAGS_VLAN_SHIFT);\n\n\t \n\tinput.formatted.vlan_id = vlan_id;\n\n\t \n\tif (first->tx_flags & (IXGBE_TX_FLAGS_SW_VLAN | IXGBE_TX_FLAGS_HW_VLAN))\n\t\tcommon.port.src ^= th->dest ^ htons(ETH_P_8021Q);\n\telse\n\t\tcommon.port.src ^= th->dest ^ first->protocol;\n\tcommon.port.dst ^= th->source;\n\n\tswitch (hdr.ipv4->version) {\n\tcase IPVERSION:\n\t\tinput.formatted.flow_type = IXGBE_ATR_FLOW_TYPE_TCPV4;\n\t\tcommon.ip ^= hdr.ipv4->saddr ^ hdr.ipv4->daddr;\n\t\tbreak;\n\tcase 6:\n\t\tinput.formatted.flow_type = IXGBE_ATR_FLOW_TYPE_TCPV6;\n\t\tcommon.ip ^= hdr.ipv6->saddr.s6_addr32[0] ^\n\t\t\t     hdr.ipv6->saddr.s6_addr32[1] ^\n\t\t\t     hdr.ipv6->saddr.s6_addr32[2] ^\n\t\t\t     hdr.ipv6->saddr.s6_addr32[3] ^\n\t\t\t     hdr.ipv6->daddr.s6_addr32[0] ^\n\t\t\t     hdr.ipv6->daddr.s6_addr32[1] ^\n\t\t\t     hdr.ipv6->daddr.s6_addr32[2] ^\n\t\t\t     hdr.ipv6->daddr.s6_addr32[3];\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (hdr.network != skb_network_header(skb))\n\t\tinput.formatted.flow_type |= IXGBE_ATR_L4TYPE_TUNNEL_MASK;\n\n\t \n\tixgbe_fdir_add_signature_filter_82599(&q_vector->adapter->hw,\n\t\t\t\t\t      input, common, ring->queue_index);\n}\n\n#ifdef IXGBE_FCOE\nstatic u16 ixgbe_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t\t      struct net_device *sb_dev)\n{\n\tstruct ixgbe_adapter *adapter;\n\tstruct ixgbe_ring_feature *f;\n\tint txq;\n\n\tif (sb_dev) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\t\tstruct net_device *vdev = sb_dev;\n\n\t\ttxq = vdev->tc_to_txq[tc].offset;\n\t\ttxq += reciprocal_scale(skb_get_hash(skb),\n\t\t\t\t\tvdev->tc_to_txq[tc].count);\n\n\t\treturn txq;\n\t}\n\n\t \n\tswitch (vlan_get_protocol(skb)) {\n\tcase htons(ETH_P_FCOE):\n\tcase htons(ETH_P_FIP):\n\t\tadapter = netdev_priv(dev);\n\n\t\tif (!sb_dev && (adapter->flags & IXGBE_FLAG_FCOE_ENABLED))\n\t\t\tbreak;\n\t\tfallthrough;\n\tdefault:\n\t\treturn netdev_pick_tx(dev, skb, sb_dev);\n\t}\n\n\tf = &adapter->ring_feature[RING_F_FCOE];\n\n\ttxq = skb_rx_queue_recorded(skb) ? skb_get_rx_queue(skb) :\n\t\t\t\t\t   smp_processor_id();\n\n\twhile (txq >= f->indices)\n\t\ttxq -= f->indices;\n\n\treturn txq + f->offset;\n}\n\n#endif\nint ixgbe_xmit_xdp_ring(struct ixgbe_ring *ring,\n\t\t\tstruct xdp_frame *xdpf)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_frame(xdpf);\n\tu8 nr_frags = unlikely(xdp_frame_has_frags(xdpf)) ? sinfo->nr_frags : 0;\n\tu16 i = 0, index = ring->next_to_use;\n\tstruct ixgbe_tx_buffer *tx_head = &ring->tx_buffer_info[index];\n\tstruct ixgbe_tx_buffer *tx_buff = tx_head;\n\tunion ixgbe_adv_tx_desc *tx_desc = IXGBE_TX_DESC(ring, index);\n\tu32 cmd_type, len = xdpf->len;\n\tvoid *data = xdpf->data;\n\n\tif (unlikely(ixgbe_desc_unused(ring) < 1 + nr_frags))\n\t\treturn IXGBE_XDP_CONSUMED;\n\n\ttx_head->bytecount = xdp_get_frame_len(xdpf);\n\ttx_head->gso_segs = 1;\n\ttx_head->xdpf = xdpf;\n\n\ttx_desc->read.olinfo_status =\n\t\tcpu_to_le32(tx_head->bytecount << IXGBE_ADVTXD_PAYLEN_SHIFT);\n\n\tfor (;;) {\n\t\tdma_addr_t dma;\n\n\t\tdma = dma_map_single(ring->dev, data, len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(ring->dev, dma))\n\t\t\tgoto unmap;\n\n\t\tdma_unmap_len_set(tx_buff, len, len);\n\t\tdma_unmap_addr_set(tx_buff, dma, dma);\n\n\t\tcmd_type = IXGBE_ADVTXD_DTYP_DATA | IXGBE_ADVTXD_DCMD_DEXT |\n\t\t\t   IXGBE_ADVTXD_DCMD_IFCS | len;\n\t\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\t\ttx_buff->protocol = 0;\n\n\t\tif (++index == ring->count)\n\t\t\tindex = 0;\n\n\t\tif (i == nr_frags)\n\t\t\tbreak;\n\n\t\ttx_buff = &ring->tx_buffer_info[index];\n\t\ttx_desc = IXGBE_TX_DESC(ring, index);\n\t\ttx_desc->read.olinfo_status = 0;\n\n\t\tdata = skb_frag_address(&sinfo->frags[i]);\n\t\tlen = skb_frag_size(&sinfo->frags[i]);\n\t\ti++;\n\t}\n\t \n\ttx_desc->read.cmd_type_len |= cpu_to_le32(IXGBE_TXD_CMD);\n\n\t \n\tsmp_wmb();\n\n\ttx_head->next_to_watch = tx_desc;\n\tring->next_to_use = index;\n\n\treturn IXGBE_XDP_TX;\n\nunmap:\n\tfor (;;) {\n\t\ttx_buff = &ring->tx_buffer_info[index];\n\t\tif (dma_unmap_len(tx_buff, len))\n\t\t\tdma_unmap_page(ring->dev, dma_unmap_addr(tx_buff, dma),\n\t\t\t\t       dma_unmap_len(tx_buff, len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(tx_buff, len, 0);\n\t\tif (tx_buff == tx_head)\n\t\t\tbreak;\n\n\t\tif (!index)\n\t\t\tindex += ring->count;\n\t\tindex--;\n\t}\n\n\treturn IXGBE_XDP_CONSUMED;\n}\n\nnetdev_tx_t ixgbe_xmit_frame_ring(struct sk_buff *skb,\n\t\t\t  struct ixgbe_adapter *adapter,\n\t\t\t  struct ixgbe_ring *tx_ring)\n{\n\tstruct ixgbe_tx_buffer *first;\n\tint tso;\n\tu32 tx_flags = 0;\n\tunsigned short f;\n\tu16 count = TXD_USE_COUNT(skb_headlen(skb));\n\tstruct ixgbe_ipsec_tx_data ipsec_tx = { 0 };\n\t__be16 protocol = skb->protocol;\n\tu8 hdr_len = 0;\n\n\t \n\tfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++)\n\t\tcount += TXD_USE_COUNT(skb_frag_size(\n\t\t\t\t\t\t&skb_shinfo(skb)->frags[f]));\n\n\tif (ixgbe_maybe_stop_tx(tx_ring, count + 3)) {\n\t\ttx_ring->tx_stats.tx_busy++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\tfirst = &tx_ring->tx_buffer_info[tx_ring->next_to_use];\n\tfirst->skb = skb;\n\tfirst->bytecount = skb->len;\n\tfirst->gso_segs = 1;\n\n\t \n\tif (skb_vlan_tag_present(skb)) {\n\t\ttx_flags |= skb_vlan_tag_get(skb) << IXGBE_TX_FLAGS_VLAN_SHIFT;\n\t\ttx_flags |= IXGBE_TX_FLAGS_HW_VLAN;\n\t \n\t} else if (protocol == htons(ETH_P_8021Q)) {\n\t\tstruct vlan_hdr *vhdr, _vhdr;\n\t\tvhdr = skb_header_pointer(skb, ETH_HLEN, sizeof(_vhdr), &_vhdr);\n\t\tif (!vhdr)\n\t\t\tgoto out_drop;\n\n\t\ttx_flags |= ntohs(vhdr->h_vlan_TCI) <<\n\t\t\t\t  IXGBE_TX_FLAGS_VLAN_SHIFT;\n\t\ttx_flags |= IXGBE_TX_FLAGS_SW_VLAN;\n\t}\n\tprotocol = vlan_get_protocol(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\n\t    adapter->ptp_clock) {\n\t\tif (adapter->tstamp_config.tx_type == HWTSTAMP_TX_ON &&\n\t\t    !test_and_set_bit_lock(__IXGBE_PTP_TX_IN_PROGRESS,\n\t\t\t\t\t   &adapter->state)) {\n\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t\ttx_flags |= IXGBE_TX_FLAGS_TSTAMP;\n\n\t\t\t \n\t\t\tadapter->ptp_tx_skb = skb_get(skb);\n\t\t\tadapter->ptp_tx_start = jiffies;\n\t\t\tschedule_work(&adapter->ptp_tx_work);\n\t\t} else {\n\t\t\tadapter->tx_hwtstamp_skipped++;\n\t\t}\n\t}\n\n#ifdef CONFIG_PCI_IOV\n\t \n\tif (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED)\n\t\ttx_flags |= IXGBE_TX_FLAGS_CC;\n\n#endif\n\t \n\tif ((adapter->flags & IXGBE_FLAG_DCB_ENABLED) &&\n\t    ((tx_flags & (IXGBE_TX_FLAGS_HW_VLAN | IXGBE_TX_FLAGS_SW_VLAN)) ||\n\t     (skb->priority != TC_PRIO_CONTROL))) {\n\t\ttx_flags &= ~IXGBE_TX_FLAGS_VLAN_PRIO_MASK;\n\t\ttx_flags |= (skb->priority & 0x7) <<\n\t\t\t\t\tIXGBE_TX_FLAGS_VLAN_PRIO_SHIFT;\n\t\tif (tx_flags & IXGBE_TX_FLAGS_SW_VLAN) {\n\t\t\tstruct vlan_ethhdr *vhdr;\n\n\t\t\tif (skb_cow_head(skb, 0))\n\t\t\t\tgoto out_drop;\n\t\t\tvhdr = skb_vlan_eth_hdr(skb);\n\t\t\tvhdr->h_vlan_TCI = htons(tx_flags >>\n\t\t\t\t\t\t IXGBE_TX_FLAGS_VLAN_SHIFT);\n\t\t} else {\n\t\t\ttx_flags |= IXGBE_TX_FLAGS_HW_VLAN;\n\t\t}\n\t}\n\n\t \n\tfirst->tx_flags = tx_flags;\n\tfirst->protocol = protocol;\n\n#ifdef IXGBE_FCOE\n\t \n\tif ((protocol == htons(ETH_P_FCOE)) &&\n\t    (tx_ring->netdev->features & (NETIF_F_FSO | NETIF_F_FCOE_CRC))) {\n\t\ttso = ixgbe_fso(tx_ring, first, &hdr_len);\n\t\tif (tso < 0)\n\t\t\tgoto out_drop;\n\n\t\tgoto xmit_fcoe;\n\t}\n\n#endif  \n\n#ifdef CONFIG_IXGBE_IPSEC\n\tif (xfrm_offload(skb) &&\n\t    !ixgbe_ipsec_tx(tx_ring, first, &ipsec_tx))\n\t\tgoto out_drop;\n#endif\n\ttso = ixgbe_tso(tx_ring, first, &hdr_len, &ipsec_tx);\n\tif (tso < 0)\n\t\tgoto out_drop;\n\telse if (!tso)\n\t\tixgbe_tx_csum(tx_ring, first, &ipsec_tx);\n\n\t \n\tif (test_bit(__IXGBE_TX_FDIR_INIT_DONE, &tx_ring->state))\n\t\tixgbe_atr(tx_ring, first);\n\n#ifdef IXGBE_FCOE\nxmit_fcoe:\n#endif  \n\tif (ixgbe_tx_map(tx_ring, first, hdr_len))\n\t\tgoto cleanup_tx_timestamp;\n\n\treturn NETDEV_TX_OK;\n\nout_drop:\n\tdev_kfree_skb_any(first->skb);\n\tfirst->skb = NULL;\ncleanup_tx_timestamp:\n\tif (unlikely(tx_flags & IXGBE_TX_FLAGS_TSTAMP)) {\n\t\tdev_kfree_skb_any(adapter->ptp_tx_skb);\n\t\tadapter->ptp_tx_skb = NULL;\n\t\tcancel_work_sync(&adapter->ptp_tx_work);\n\t\tclear_bit_unlock(__IXGBE_PTP_TX_IN_PROGRESS, &adapter->state);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic netdev_tx_t __ixgbe_xmit_frame(struct sk_buff *skb,\n\t\t\t\t      struct net_device *netdev,\n\t\t\t\t      struct ixgbe_ring *ring)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_ring *tx_ring;\n\n\t \n\tif (skb_put_padto(skb, 17))\n\t\treturn NETDEV_TX_OK;\n\n\ttx_ring = ring ? ring : adapter->tx_ring[skb_get_queue_mapping(skb)];\n\tif (unlikely(test_bit(__IXGBE_TX_DISABLED, &tx_ring->state)))\n\t\treturn NETDEV_TX_BUSY;\n\n\treturn ixgbe_xmit_frame_ring(skb, adapter, tx_ring);\n}\n\nstatic netdev_tx_t ixgbe_xmit_frame(struct sk_buff *skb,\n\t\t\t\t    struct net_device *netdev)\n{\n\treturn __ixgbe_xmit_frame(skb, netdev, NULL);\n}\n\n \nstatic int ixgbe_set_mac(struct net_device *netdev, void *p)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct sockaddr *addr = p;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\teth_hw_addr_set(netdev, addr->sa_data);\n\tmemcpy(hw->mac.addr, addr->sa_data, netdev->addr_len);\n\n\tixgbe_mac_set_default_filter(adapter);\n\n\treturn 0;\n}\n\nstatic int\nixgbe_mdio_read(struct net_device *netdev, int prtad, int devad, u16 addr)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu16 value;\n\tint rc;\n\n\tif (adapter->mii_bus) {\n\t\tint regnum = addr;\n\n\t\tif (devad != MDIO_DEVAD_NONE)\n\t\t\treturn mdiobus_c45_read(adapter->mii_bus, prtad,\n\t\t\t\t\t\tdevad, regnum);\n\n\t\treturn mdiobus_read(adapter->mii_bus, prtad, regnum);\n\t}\n\n\tif (prtad != hw->phy.mdio.prtad)\n\t\treturn -EINVAL;\n\trc = hw->phy.ops.read_reg(hw, addr, devad, &value);\n\tif (!rc)\n\t\trc = value;\n\treturn rc;\n}\n\nstatic int ixgbe_mdio_write(struct net_device *netdev, int prtad, int devad,\n\t\t\t    u16 addr, u16 value)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tif (adapter->mii_bus) {\n\t\tint regnum = addr;\n\n\t\tif (devad != MDIO_DEVAD_NONE)\n\t\t\treturn mdiobus_c45_write(adapter->mii_bus, prtad, devad,\n\t\t\t\t\t\t regnum, value);\n\n\t\treturn mdiobus_write(adapter->mii_bus, prtad, regnum, value);\n\t}\n\n\tif (prtad != hw->phy.mdio.prtad)\n\t\treturn -EINVAL;\n\treturn hw->phy.ops.write_reg(hw, addr, devad, value);\n}\n\nstatic int ixgbe_ioctl(struct net_device *netdev, struct ifreq *req, int cmd)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\n\tswitch (cmd) {\n\tcase SIOCSHWTSTAMP:\n\t\treturn ixgbe_ptp_set_ts_config(adapter, req);\n\tcase SIOCGHWTSTAMP:\n\t\treturn ixgbe_ptp_get_ts_config(adapter, req);\n\tcase SIOCGMIIPHY:\n\t\tif (!adapter->hw.phy.ops.read_reg)\n\t\t\treturn -EOPNOTSUPP;\n\t\tfallthrough;\n\tdefault:\n\t\treturn mdio_mii_ioctl(&adapter->hw.phy.mdio, if_mii(req), cmd);\n\t}\n}\n\n \nstatic int ixgbe_add_sanmac_netdev(struct net_device *dev)\n{\n\tint err = 0;\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\tif (is_valid_ether_addr(hw->mac.san_addr)) {\n\t\trtnl_lock();\n\t\terr = dev_addr_add(dev, hw->mac.san_addr, NETDEV_HW_ADDR_T_SAN);\n\t\trtnl_unlock();\n\n\t\t \n\t\thw->mac.ops.set_vmdq_san_mac(hw, VMDQ_P(0));\n\t}\n\treturn err;\n}\n\n \nstatic int ixgbe_del_sanmac_netdev(struct net_device *dev)\n{\n\tint err = 0;\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\tstruct ixgbe_mac_info *mac = &adapter->hw.mac;\n\n\tif (is_valid_ether_addr(mac->san_addr)) {\n\t\trtnl_lock();\n\t\terr = dev_addr_del(dev, mac->san_addr, NETDEV_HW_ADDR_T_SAN);\n\t\trtnl_unlock();\n\t}\n\treturn err;\n}\n\nstatic void ixgbe_get_ring_stats64(struct rtnl_link_stats64 *stats,\n\t\t\t\t   struct ixgbe_ring *ring)\n{\n\tu64 bytes, packets;\n\tunsigned int start;\n\n\tif (ring) {\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&ring->syncp);\n\t\t\tpackets = ring->stats.packets;\n\t\t\tbytes   = ring->stats.bytes;\n\t\t} while (u64_stats_fetch_retry(&ring->syncp, start));\n\t\tstats->tx_packets += packets;\n\t\tstats->tx_bytes   += bytes;\n\t}\n}\n\nstatic void ixgbe_get_stats64(struct net_device *netdev,\n\t\t\t      struct rtnl_link_stats64 *stats)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tint i;\n\n\trcu_read_lock();\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct ixgbe_ring *ring = READ_ONCE(adapter->rx_ring[i]);\n\t\tu64 bytes, packets;\n\t\tunsigned int start;\n\n\t\tif (ring) {\n\t\t\tdo {\n\t\t\t\tstart = u64_stats_fetch_begin(&ring->syncp);\n\t\t\t\tpackets = ring->stats.packets;\n\t\t\t\tbytes   = ring->stats.bytes;\n\t\t\t} while (u64_stats_fetch_retry(&ring->syncp, start));\n\t\t\tstats->rx_packets += packets;\n\t\t\tstats->rx_bytes   += bytes;\n\t\t}\n\t}\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct ixgbe_ring *ring = READ_ONCE(adapter->tx_ring[i]);\n\n\t\tixgbe_get_ring_stats64(stats, ring);\n\t}\n\tfor (i = 0; i < adapter->num_xdp_queues; i++) {\n\t\tstruct ixgbe_ring *ring = READ_ONCE(adapter->xdp_ring[i]);\n\n\t\tixgbe_get_ring_stats64(stats, ring);\n\t}\n\trcu_read_unlock();\n\n\t \n\tstats->multicast\t= netdev->stats.multicast;\n\tstats->rx_errors\t= netdev->stats.rx_errors;\n\tstats->rx_length_errors\t= netdev->stats.rx_length_errors;\n\tstats->rx_crc_errors\t= netdev->stats.rx_crc_errors;\n\tstats->rx_missed_errors\t= netdev->stats.rx_missed_errors;\n}\n\nstatic int ixgbe_ndo_get_vf_stats(struct net_device *netdev, int vf,\n\t\t\t\t  struct ifla_vf_stats *vf_stats)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\n\tif (vf < 0 || vf >= adapter->num_vfs)\n\t\treturn -EINVAL;\n\n\tvf_stats->rx_packets = adapter->vfinfo[vf].vfstats.gprc;\n\tvf_stats->rx_bytes   = adapter->vfinfo[vf].vfstats.gorc;\n\tvf_stats->tx_packets = adapter->vfinfo[vf].vfstats.gptc;\n\tvf_stats->tx_bytes   = adapter->vfinfo[vf].vfstats.gotc;\n\tvf_stats->multicast  = adapter->vfinfo[vf].vfstats.mprc;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_IXGBE_DCB\n \nstatic void ixgbe_validate_rtr(struct ixgbe_adapter *adapter, u8 tc)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu32 reg, rsave;\n\tint i;\n\n\t \n\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\treturn;\n\n\treg = IXGBE_READ_REG(hw, IXGBE_RTRUP2TC);\n\trsave = reg;\n\n\tfor (i = 0; i < MAX_TRAFFIC_CLASS; i++) {\n\t\tu8 up2tc = reg >> (i * IXGBE_RTRUP2TC_UP_SHIFT);\n\n\t\t \n\t\tif (up2tc > tc)\n\t\t\treg &= ~(0x7 << IXGBE_RTRUP2TC_UP_SHIFT);\n\t}\n\n\tif (reg != rsave)\n\t\tIXGBE_WRITE_REG(hw, IXGBE_RTRUP2TC, reg);\n\n\treturn;\n}\n\n \nstatic void ixgbe_set_prio_tc_map(struct ixgbe_adapter *adapter)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tstruct ixgbe_dcb_config *dcb_cfg = &adapter->dcb_cfg;\n\tstruct ieee_ets *ets = adapter->ixgbe_ieee_ets;\n\tu8 prio;\n\n\tfor (prio = 0; prio < MAX_USER_PRIORITY; prio++) {\n\t\tu8 tc = 0;\n\n\t\tif (adapter->dcbx_cap & DCB_CAP_DCBX_VER_CEE)\n\t\t\ttc = ixgbe_dcb_get_tc_from_up(dcb_cfg, 0, prio);\n\t\telse if (ets)\n\t\t\ttc = ets->prio_tc[prio];\n\n\t\tnetdev_set_prio_tc_map(dev, prio, tc);\n\t}\n}\n\n#endif  \nstatic int ixgbe_reassign_macvlan_pool(struct net_device *vdev,\n\t\t\t\t       struct netdev_nested_priv *priv)\n{\n\tstruct ixgbe_adapter *adapter = (struct ixgbe_adapter *)priv->data;\n\tstruct ixgbe_fwd_adapter *accel;\n\tint pool;\n\n\t \n\tif (!netif_is_macvlan(vdev))\n\t\treturn 0;\n\n\t \n\taccel = macvlan_accel_priv(vdev);\n\tif (!accel)\n\t\treturn 0;\n\n\t \n\tpool = find_first_zero_bit(adapter->fwd_bitmask, adapter->num_rx_pools);\n\tif (pool < adapter->num_rx_pools) {\n\t\tset_bit(pool, adapter->fwd_bitmask);\n\t\taccel->pool = pool;\n\t\treturn 0;\n\t}\n\n\t \n\tnetdev_err(vdev, \"L2FW offload disabled due to lack of queue resources\\n\");\n\tmacvlan_release_l2fw_offload(vdev);\n\n\t \n\tnetdev_unbind_sb_channel(adapter->netdev, vdev);\n\tnetdev_set_sb_channel(vdev, 0);\n\n\tkfree(accel);\n\n\treturn 0;\n}\n\nstatic void ixgbe_defrag_macvlan_pools(struct net_device *dev)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\tstruct netdev_nested_priv priv = {\n\t\t.data = (void *)adapter,\n\t};\n\n\t \n\tbitmap_clear(adapter->fwd_bitmask, 1, 63);\n\n\t \n\tnetdev_walk_all_upper_dev_rcu(dev, ixgbe_reassign_macvlan_pool,\n\t\t\t\t      &priv);\n}\n\n \nint ixgbe_setup_tc(struct net_device *dev, u8 tc)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\n\t \n\tif (tc > adapter->dcb_cfg.num_tcs.pg_tcs)\n\t\treturn -EINVAL;\n\n\tif (hw->mac.type == ixgbe_mac_82598EB && tc && tc < MAX_TRAFFIC_CLASS)\n\t\treturn -EINVAL;\n\n\t \n\tif (netif_running(dev))\n\t\tixgbe_close(dev);\n\telse\n\t\tixgbe_reset(adapter);\n\n\tixgbe_clear_interrupt_scheme(adapter);\n\n#ifdef CONFIG_IXGBE_DCB\n\tif (tc) {\n\t\tif (adapter->xdp_prog) {\n\t\t\te_warn(probe, \"DCB is not supported with XDP\\n\");\n\n\t\t\tixgbe_init_interrupt_scheme(adapter);\n\t\t\tif (netif_running(dev))\n\t\t\t\tixgbe_open(dev);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tnetdev_set_num_tc(dev, tc);\n\t\tixgbe_set_prio_tc_map(adapter);\n\n\t\tadapter->hw_tcs = tc;\n\t\tadapter->flags |= IXGBE_FLAG_DCB_ENABLED;\n\n\t\tif (adapter->hw.mac.type == ixgbe_mac_82598EB) {\n\t\t\tadapter->last_lfc_mode = adapter->hw.fc.requested_mode;\n\t\t\tadapter->hw.fc.requested_mode = ixgbe_fc_none;\n\t\t}\n\t} else {\n\t\tnetdev_reset_tc(dev);\n\n\t\tif (adapter->hw.mac.type == ixgbe_mac_82598EB)\n\t\t\tadapter->hw.fc.requested_mode = adapter->last_lfc_mode;\n\n\t\tadapter->flags &= ~IXGBE_FLAG_DCB_ENABLED;\n\t\tadapter->hw_tcs = tc;\n\n\t\tadapter->temp_dcb_cfg.pfc_mode_enable = false;\n\t\tadapter->dcb_cfg.pfc_mode_enable = false;\n\t}\n\n\tixgbe_validate_rtr(adapter, tc);\n\n#endif  \n\tixgbe_init_interrupt_scheme(adapter);\n\n\tixgbe_defrag_macvlan_pools(dev);\n\n\tif (netif_running(dev))\n\t\treturn ixgbe_open(dev);\n\n\treturn 0;\n}\n\nstatic int ixgbe_delete_clsu32(struct ixgbe_adapter *adapter,\n\t\t\t       struct tc_cls_u32_offload *cls)\n{\n\tu32 hdl = cls->knode.handle;\n\tu32 uhtid = TC_U32_USERHTID(cls->knode.handle);\n\tu32 loc = cls->knode.handle & 0xfffff;\n\tint err = 0, i, j;\n\tstruct ixgbe_jump_table *jump = NULL;\n\n\tif (loc > IXGBE_MAX_HW_ENTRIES)\n\t\treturn -EINVAL;\n\n\tif ((uhtid != 0x800) && (uhtid >= IXGBE_MAX_LINK_HANDLE))\n\t\treturn -EINVAL;\n\n\t \n\tif (uhtid != 0x800) {\n\t\tjump = adapter->jump_tables[uhtid];\n\t\tif (!jump)\n\t\t\treturn -EINVAL;\n\t\tif (!test_bit(loc - 1, jump->child_loc_map))\n\t\t\treturn -EINVAL;\n\t\tclear_bit(loc - 1, jump->child_loc_map);\n\t}\n\n\t \n\tfor (i = 1; i < IXGBE_MAX_LINK_HANDLE; i++) {\n\t\tjump = adapter->jump_tables[i];\n\t\tif (jump && jump->link_hdl == hdl) {\n\t\t\t \n\t\t\tfor (j = 0; j < IXGBE_MAX_HW_ENTRIES; j++) {\n\t\t\t\tif (!test_bit(j, jump->child_loc_map))\n\t\t\t\t\tcontinue;\n\t\t\t\tspin_lock(&adapter->fdir_perfect_lock);\n\t\t\t\terr = ixgbe_update_ethtool_fdir_entry(adapter,\n\t\t\t\t\t\t\t\t      NULL,\n\t\t\t\t\t\t\t\t      j + 1);\n\t\t\t\tspin_unlock(&adapter->fdir_perfect_lock);\n\t\t\t\tclear_bit(j, jump->child_loc_map);\n\t\t\t}\n\t\t\t \n\t\t\tkfree(jump->input);\n\t\t\tkfree(jump->mask);\n\t\t\tkfree(jump);\n\t\t\tadapter->jump_tables[i] = NULL;\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tspin_lock(&adapter->fdir_perfect_lock);\n\terr = ixgbe_update_ethtool_fdir_entry(adapter, NULL, loc);\n\tspin_unlock(&adapter->fdir_perfect_lock);\n\treturn err;\n}\n\nstatic int ixgbe_configure_clsu32_add_hnode(struct ixgbe_adapter *adapter,\n\t\t\t\t\t    struct tc_cls_u32_offload *cls)\n{\n\tu32 uhtid = TC_U32_USERHTID(cls->hnode.handle);\n\n\tif (uhtid >= IXGBE_MAX_LINK_HANDLE)\n\t\treturn -EINVAL;\n\n\t \n\tif (cls->hnode.divisor > 0)\n\t\treturn -EINVAL;\n\n\tset_bit(uhtid - 1, &adapter->tables);\n\treturn 0;\n}\n\nstatic int ixgbe_configure_clsu32_del_hnode(struct ixgbe_adapter *adapter,\n\t\t\t\t\t    struct tc_cls_u32_offload *cls)\n{\n\tu32 uhtid = TC_U32_USERHTID(cls->hnode.handle);\n\n\tif (uhtid >= IXGBE_MAX_LINK_HANDLE)\n\t\treturn -EINVAL;\n\n\tclear_bit(uhtid - 1, &adapter->tables);\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_CLS_ACT\nstruct upper_walk_data {\n\tstruct ixgbe_adapter *adapter;\n\tu64 action;\n\tint ifindex;\n\tu8 queue;\n};\n\nstatic int get_macvlan_queue(struct net_device *upper,\n\t\t\t     struct netdev_nested_priv *priv)\n{\n\tif (netif_is_macvlan(upper)) {\n\t\tstruct ixgbe_fwd_adapter *vadapter = macvlan_accel_priv(upper);\n\t\tstruct ixgbe_adapter *adapter;\n\t\tstruct upper_walk_data *data;\n\t\tint ifindex;\n\n\t\tdata = (struct upper_walk_data *)priv->data;\n\t\tifindex = data->ifindex;\n\t\tadapter = data->adapter;\n\t\tif (vadapter && upper->ifindex == ifindex) {\n\t\t\tdata->queue = adapter->rx_ring[vadapter->rx_base_queue]->reg_idx;\n\t\t\tdata->action = data->queue;\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int handle_redirect_action(struct ixgbe_adapter *adapter, int ifindex,\n\t\t\t\t  u8 *queue, u64 *action)\n{\n\tstruct ixgbe_ring_feature *vmdq = &adapter->ring_feature[RING_F_VMDQ];\n\tunsigned int num_vfs = adapter->num_vfs, vf;\n\tstruct netdev_nested_priv priv;\n\tstruct upper_walk_data data;\n\tstruct net_device *upper;\n\n\t \n\tfor (vf = 0; vf < num_vfs; ++vf) {\n\t\tupper = pci_get_drvdata(adapter->vfinfo[vf].vfdev);\n\t\tif (upper->ifindex == ifindex) {\n\t\t\t*queue = vf * __ALIGN_MASK(1, ~vmdq->mask);\n\t\t\t*action = vf + 1;\n\t\t\t*action <<= ETHTOOL_RX_FLOW_SPEC_RING_VF_OFF;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tdata.adapter = adapter;\n\tdata.ifindex = ifindex;\n\tdata.action = 0;\n\tdata.queue = 0;\n\tpriv.data = (void *)&data;\n\tif (netdev_walk_all_upper_dev_rcu(adapter->netdev,\n\t\t\t\t\t  get_macvlan_queue, &priv)) {\n\t\t*action = data.action;\n\t\t*queue = data.queue;\n\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int parse_tc_actions(struct ixgbe_adapter *adapter,\n\t\t\t    struct tcf_exts *exts, u64 *action, u8 *queue)\n{\n\tconst struct tc_action *a;\n\tint i;\n\n\tif (!tcf_exts_has_actions(exts))\n\t\treturn -EINVAL;\n\n\ttcf_exts_for_each_action(i, a, exts) {\n\t\t \n\t\tif (is_tcf_gact_shot(a)) {\n\t\t\t*action = IXGBE_FDIR_DROP_QUEUE;\n\t\t\t*queue = IXGBE_FDIR_DROP_QUEUE;\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tif (is_tcf_mirred_egress_redirect(a)) {\n\t\t\tstruct net_device *dev = tcf_mirred_dev(a);\n\n\t\t\tif (!dev)\n\t\t\t\treturn -EINVAL;\n\t\t\treturn handle_redirect_action(adapter, dev->ifindex,\n\t\t\t\t\t\t      queue, action);\n\t\t}\n\n\t\treturn -EINVAL;\n\t}\n\n\treturn -EINVAL;\n}\n#else\nstatic int parse_tc_actions(struct ixgbe_adapter *adapter,\n\t\t\t    struct tcf_exts *exts, u64 *action, u8 *queue)\n{\n\treturn -EINVAL;\n}\n#endif  \n\nstatic int ixgbe_clsu32_build_input(struct ixgbe_fdir_filter *input,\n\t\t\t\t    union ixgbe_atr_input *mask,\n\t\t\t\t    struct tc_cls_u32_offload *cls,\n\t\t\t\t    struct ixgbe_mat_field *field_ptr,\n\t\t\t\t    struct ixgbe_nexthdr *nexthdr)\n{\n\tint i, j, off;\n\t__be32 val, m;\n\tbool found_entry = false, found_jump_field = false;\n\n\tfor (i = 0; i < cls->knode.sel->nkeys; i++) {\n\t\toff = cls->knode.sel->keys[i].off;\n\t\tval = cls->knode.sel->keys[i].val;\n\t\tm = cls->knode.sel->keys[i].mask;\n\n\t\tfor (j = 0; field_ptr[j].val; j++) {\n\t\t\tif (field_ptr[j].off == off) {\n\t\t\t\tfield_ptr[j].val(input, mask, (__force u32)val,\n\t\t\t\t\t\t (__force u32)m);\n\t\t\t\tinput->filter.formatted.flow_type |=\n\t\t\t\t\tfield_ptr[j].type;\n\t\t\t\tfound_entry = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (nexthdr) {\n\t\t\tif (nexthdr->off == cls->knode.sel->keys[i].off &&\n\t\t\t    nexthdr->val ==\n\t\t\t    (__force u32)cls->knode.sel->keys[i].val &&\n\t\t\t    nexthdr->mask ==\n\t\t\t    (__force u32)cls->knode.sel->keys[i].mask)\n\t\t\t\tfound_jump_field = true;\n\t\t\telse\n\t\t\t\tcontinue;\n\t\t}\n\t}\n\n\tif (nexthdr && !found_jump_field)\n\t\treturn -EINVAL;\n\n\tif (!found_entry)\n\t\treturn 0;\n\n\tmask->formatted.flow_type = IXGBE_ATR_L4TYPE_IPV6_MASK |\n\t\t\t\t    IXGBE_ATR_L4TYPE_MASK;\n\n\tif (input->filter.formatted.flow_type == IXGBE_ATR_FLOW_TYPE_IPV4)\n\t\tmask->formatted.flow_type &= IXGBE_ATR_L4TYPE_IPV6_MASK;\n\n\treturn 0;\n}\n\nstatic int ixgbe_configure_clsu32(struct ixgbe_adapter *adapter,\n\t\t\t\t  struct tc_cls_u32_offload *cls)\n{\n\t__be16 protocol = cls->common.protocol;\n\tu32 loc = cls->knode.handle & 0xfffff;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct ixgbe_mat_field *field_ptr;\n\tstruct ixgbe_fdir_filter *input = NULL;\n\tunion ixgbe_atr_input *mask = NULL;\n\tstruct ixgbe_jump_table *jump = NULL;\n\tint i, err = -EINVAL;\n\tu8 queue;\n\tu32 uhtid, link_uhtid;\n\n\tuhtid = TC_U32_USERHTID(cls->knode.handle);\n\tlink_uhtid = TC_U32_USERHTID(cls->knode.link_handle);\n\n\t \n\tif (protocol != htons(ETH_P_IP))\n\t\treturn err;\n\n\tif (loc >= ((1024 << adapter->fdir_pballoc) - 2)) {\n\t\te_err(drv, \"Location out of range\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tif (uhtid == 0x800) {\n\t\tfield_ptr = (adapter->jump_tables[0])->mat;\n\t} else {\n\t\tif (uhtid >= IXGBE_MAX_LINK_HANDLE)\n\t\t\treturn err;\n\t\tif (!adapter->jump_tables[uhtid])\n\t\t\treturn err;\n\t\tfield_ptr = (adapter->jump_tables[uhtid])->mat;\n\t}\n\n\tif (!field_ptr)\n\t\treturn err;\n\n\t \n\n\tif (link_uhtid) {\n\t\tstruct ixgbe_nexthdr *nexthdr = ixgbe_ipv4_jumps;\n\n\t\tif (link_uhtid >= IXGBE_MAX_LINK_HANDLE)\n\t\t\treturn err;\n\n\t\tif (!test_bit(link_uhtid - 1, &adapter->tables))\n\t\t\treturn err;\n\n\t\t \n\t\tif (adapter->jump_tables[link_uhtid] &&\n\t\t    (adapter->jump_tables[link_uhtid])->link_hdl) {\n\t\t\te_err(drv, \"Link filter exists for link: %x\\n\",\n\t\t\t      link_uhtid);\n\t\t\treturn err;\n\t\t}\n\n\t\tfor (i = 0; nexthdr[i].jump; i++) {\n\t\t\tif (nexthdr[i].o != cls->knode.sel->offoff ||\n\t\t\t    nexthdr[i].s != cls->knode.sel->offshift ||\n\t\t\t    nexthdr[i].m !=\n\t\t\t    (__force u32)cls->knode.sel->offmask)\n\t\t\t\treturn err;\n\n\t\t\tjump = kzalloc(sizeof(*jump), GFP_KERNEL);\n\t\t\tif (!jump)\n\t\t\t\treturn -ENOMEM;\n\t\t\tinput = kzalloc(sizeof(*input), GFP_KERNEL);\n\t\t\tif (!input) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto free_jump;\n\t\t\t}\n\t\t\tmask = kzalloc(sizeof(*mask), GFP_KERNEL);\n\t\t\tif (!mask) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto free_input;\n\t\t\t}\n\t\t\tjump->input = input;\n\t\t\tjump->mask = mask;\n\t\t\tjump->link_hdl = cls->knode.handle;\n\n\t\t\terr = ixgbe_clsu32_build_input(input, mask, cls,\n\t\t\t\t\t\t       field_ptr, &nexthdr[i]);\n\t\t\tif (!err) {\n\t\t\t\tjump->mat = nexthdr[i].jump;\n\t\t\t\tadapter->jump_tables[link_uhtid] = jump;\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tkfree(mask);\n\t\t\t\tkfree(input);\n\t\t\t\tkfree(jump);\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tinput = kzalloc(sizeof(*input), GFP_KERNEL);\n\tif (!input)\n\t\treturn -ENOMEM;\n\tmask = kzalloc(sizeof(*mask), GFP_KERNEL);\n\tif (!mask) {\n\t\terr = -ENOMEM;\n\t\tgoto free_input;\n\t}\n\n\tif ((uhtid != 0x800) && (adapter->jump_tables[uhtid])) {\n\t\tif ((adapter->jump_tables[uhtid])->input)\n\t\t\tmemcpy(input, (adapter->jump_tables[uhtid])->input,\n\t\t\t       sizeof(*input));\n\t\tif ((adapter->jump_tables[uhtid])->mask)\n\t\t\tmemcpy(mask, (adapter->jump_tables[uhtid])->mask,\n\t\t\t       sizeof(*mask));\n\n\t\t \n\t\tfor (i = 1; i < IXGBE_MAX_LINK_HANDLE; i++) {\n\t\t\tstruct ixgbe_jump_table *link = adapter->jump_tables[i];\n\n\t\t\tif (link && (test_bit(loc - 1, link->child_loc_map))) {\n\t\t\t\te_err(drv, \"Filter exists in location: %x\\n\",\n\t\t\t\t      loc);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t}\n\terr = ixgbe_clsu32_build_input(input, mask, cls, field_ptr, NULL);\n\tif (err)\n\t\tgoto err_out;\n\n\terr = parse_tc_actions(adapter, cls->knode.exts, &input->action,\n\t\t\t       &queue);\n\tif (err < 0)\n\t\tgoto err_out;\n\n\tinput->sw_idx = loc;\n\n\tspin_lock(&adapter->fdir_perfect_lock);\n\n\tif (hlist_empty(&adapter->fdir_filter_list)) {\n\t\tmemcpy(&adapter->fdir_mask, mask, sizeof(*mask));\n\t\terr = ixgbe_fdir_set_input_mask_82599(hw, mask);\n\t\tif (err)\n\t\t\tgoto err_out_w_lock;\n\t} else if (memcmp(&adapter->fdir_mask, mask, sizeof(*mask))) {\n\t\terr = -EINVAL;\n\t\tgoto err_out_w_lock;\n\t}\n\n\tixgbe_atr_compute_perfect_hash_82599(&input->filter, mask);\n\terr = ixgbe_fdir_write_perfect_filter_82599(hw, &input->filter,\n\t\t\t\t\t\t    input->sw_idx, queue);\n\tif (err)\n\t\tgoto err_out_w_lock;\n\n\tixgbe_update_ethtool_fdir_entry(adapter, input, input->sw_idx);\n\tspin_unlock(&adapter->fdir_perfect_lock);\n\n\tif ((uhtid != 0x800) && (adapter->jump_tables[uhtid]))\n\t\tset_bit(loc - 1, (adapter->jump_tables[uhtid])->child_loc_map);\n\n\tkfree(mask);\n\treturn err;\nerr_out_w_lock:\n\tspin_unlock(&adapter->fdir_perfect_lock);\nerr_out:\n\tkfree(mask);\nfree_input:\n\tkfree(input);\nfree_jump:\n\tkfree(jump);\n\treturn err;\n}\n\nstatic int ixgbe_setup_tc_cls_u32(struct ixgbe_adapter *adapter,\n\t\t\t\t  struct tc_cls_u32_offload *cls_u32)\n{\n\tswitch (cls_u32->command) {\n\tcase TC_CLSU32_NEW_KNODE:\n\tcase TC_CLSU32_REPLACE_KNODE:\n\t\treturn ixgbe_configure_clsu32(adapter, cls_u32);\n\tcase TC_CLSU32_DELETE_KNODE:\n\t\treturn ixgbe_delete_clsu32(adapter, cls_u32);\n\tcase TC_CLSU32_NEW_HNODE:\n\tcase TC_CLSU32_REPLACE_HNODE:\n\t\treturn ixgbe_configure_clsu32_add_hnode(adapter, cls_u32);\n\tcase TC_CLSU32_DELETE_HNODE:\n\t\treturn ixgbe_configure_clsu32_del_hnode(adapter, cls_u32);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int ixgbe_setup_tc_block_cb(enum tc_setup_type type, void *type_data,\n\t\t\t\t   void *cb_priv)\n{\n\tstruct ixgbe_adapter *adapter = cb_priv;\n\n\tif (!tc_cls_can_offload_and_chain0(adapter->netdev, type_data))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (type) {\n\tcase TC_SETUP_CLSU32:\n\t\treturn ixgbe_setup_tc_cls_u32(adapter, type_data);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int ixgbe_setup_tc_mqprio(struct net_device *dev,\n\t\t\t\t struct tc_mqprio_qopt *mqprio)\n{\n\tmqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;\n\treturn ixgbe_setup_tc(dev, mqprio->num_tc);\n}\n\nstatic LIST_HEAD(ixgbe_block_cb_list);\n\nstatic int __ixgbe_setup_tc(struct net_device *dev, enum tc_setup_type type,\n\t\t\t    void *type_data)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\n\tswitch (type) {\n\tcase TC_SETUP_BLOCK:\n\t\treturn flow_block_cb_setup_simple(type_data,\n\t\t\t\t\t\t  &ixgbe_block_cb_list,\n\t\t\t\t\t\t  ixgbe_setup_tc_block_cb,\n\t\t\t\t\t\t  adapter, adapter, true);\n\tcase TC_SETUP_QDISC_MQPRIO:\n\t\treturn ixgbe_setup_tc_mqprio(dev, type_data);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\n#ifdef CONFIG_PCI_IOV\nvoid ixgbe_sriov_reinit(struct ixgbe_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\n\trtnl_lock();\n\tixgbe_setup_tc(netdev, adapter->hw_tcs);\n\trtnl_unlock();\n}\n\n#endif\nvoid ixgbe_do_reset(struct net_device *netdev)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\n\tif (netif_running(netdev))\n\t\tixgbe_reinit_locked(adapter);\n\telse\n\t\tixgbe_reset(adapter);\n}\n\nstatic netdev_features_t ixgbe_fix_features(struct net_device *netdev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\n\t \n\tif (!(features & NETIF_F_RXCSUM))\n\t\tfeatures &= ~NETIF_F_LRO;\n\n\t \n\tif (!(adapter->flags2 & IXGBE_FLAG2_RSC_CAPABLE))\n\t\tfeatures &= ~NETIF_F_LRO;\n\n\tif (adapter->xdp_prog && (features & NETIF_F_LRO)) {\n\t\te_dev_err(\"LRO is not supported with XDP\\n\");\n\t\tfeatures &= ~NETIF_F_LRO;\n\t}\n\n\treturn features;\n}\n\nstatic void ixgbe_reset_l2fw_offload(struct ixgbe_adapter *adapter)\n{\n\tint rss = min_t(int, ixgbe_max_rss_indices(adapter),\n\t\t\tnum_online_cpus());\n\n\t \n\tif (!adapter->ring_feature[RING_F_VMDQ].offset)\n\t\tadapter->flags &= ~(IXGBE_FLAG_VMDQ_ENABLED |\n\t\t\t\t    IXGBE_FLAG_SRIOV_ENABLED);\n\n\tadapter->ring_feature[RING_F_RSS].limit = rss;\n\tadapter->ring_feature[RING_F_VMDQ].limit = 1;\n\n\tixgbe_setup_tc(adapter->netdev, adapter->hw_tcs);\n}\n\nstatic int ixgbe_set_features(struct net_device *netdev,\n\t\t\t      netdev_features_t features)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(netdev);\n\tnetdev_features_t changed = netdev->features ^ features;\n\tbool need_reset = false;\n\n\t \n\tif (!(features & NETIF_F_LRO)) {\n\t\tif (adapter->flags2 & IXGBE_FLAG2_RSC_ENABLED)\n\t\t\tneed_reset = true;\n\t\tadapter->flags2 &= ~IXGBE_FLAG2_RSC_ENABLED;\n\t} else if ((adapter->flags2 & IXGBE_FLAG2_RSC_CAPABLE) &&\n\t\t   !(adapter->flags2 & IXGBE_FLAG2_RSC_ENABLED)) {\n\t\tif (adapter->rx_itr_setting == 1 ||\n\t\t    adapter->rx_itr_setting > IXGBE_MIN_RSC_ITR) {\n\t\t\tadapter->flags2 |= IXGBE_FLAG2_RSC_ENABLED;\n\t\t\tneed_reset = true;\n\t\t} else if ((changed ^ features) & NETIF_F_LRO) {\n\t\t\te_info(probe, \"rx-usecs set too low, \"\n\t\t\t       \"disabling RSC\\n\");\n\t\t}\n\t}\n\n\t \n\tif ((features & NETIF_F_NTUPLE) || (features & NETIF_F_HW_TC)) {\n\t\t \n\t\tif (!(adapter->flags & IXGBE_FLAG_FDIR_PERFECT_CAPABLE))\n\t\t\tneed_reset = true;\n\n\t\tadapter->flags &= ~IXGBE_FLAG_FDIR_HASH_CAPABLE;\n\t\tadapter->flags |= IXGBE_FLAG_FDIR_PERFECT_CAPABLE;\n\t} else {\n\t\t \n\t\tif (adapter->flags & IXGBE_FLAG_FDIR_PERFECT_CAPABLE)\n\t\t\tneed_reset = true;\n\n\t\tadapter->flags &= ~IXGBE_FLAG_FDIR_PERFECT_CAPABLE;\n\n\t\t \n\t\tif (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED ||\n\t\t     \n\t\t    (adapter->hw_tcs > 1) ||\n\t\t     \n\t\t    (adapter->ring_feature[RING_F_RSS].limit <= 1) ||\n\t\t     \n\t\t    (!adapter->atr_sample_rate))\n\t\t\t;  \n\t\telse  \n\t\t\tadapter->flags |= IXGBE_FLAG_FDIR_HASH_CAPABLE;\n\t}\n\n\tif (changed & NETIF_F_RXALL)\n\t\tneed_reset = true;\n\n\tnetdev->features = features;\n\n\tif ((changed & NETIF_F_HW_L2FW_DOFFLOAD) && adapter->num_rx_pools > 1)\n\t\tixgbe_reset_l2fw_offload(adapter);\n\telse if (need_reset)\n\t\tixgbe_do_reset(netdev);\n\telse if (changed & (NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t    NETIF_F_HW_VLAN_CTAG_FILTER))\n\t\tixgbe_set_rx_mode(netdev);\n\n\treturn 1;\n}\n\nstatic int ixgbe_ndo_fdb_add(struct ndmsg *ndm, struct nlattr *tb[],\n\t\t\t     struct net_device *dev,\n\t\t\t     const unsigned char *addr, u16 vid,\n\t\t\t     u16 flags,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\t \n\tif (is_unicast_ether_addr(addr) || is_link_local_ether_addr(addr)) {\n\t\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\t\tu16 pool = VMDQ_P(0);\n\n\t\tif (netdev_uc_count(dev) >= ixgbe_available_rars(adapter, pool))\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn ndo_dflt_fdb_add(ndm, tb, dev, addr, vid, flags);\n}\n\n \nstatic int ixgbe_configure_bridge_mode(struct ixgbe_adapter *adapter,\n\t\t\t\t       __u16 mode)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tunsigned int p, num_pools;\n\tu32 vmdctl;\n\n\tswitch (mode) {\n\tcase BRIDGE_MODE_VEPA:\n\t\t \n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_PFDTXGSWC, 0);\n\n\t\t \n\t\tvmdctl = IXGBE_READ_REG(hw, IXGBE_VMD_CTL);\n\t\tvmdctl |= IXGBE_VT_CTL_REPLEN;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VMD_CTL, vmdctl);\n\n\t\t \n\t\tnum_pools = adapter->num_vfs + adapter->num_rx_pools;\n\t\tfor (p = 0; p < num_pools; p++) {\n\t\t\tif (hw->mac.ops.set_source_address_pruning)\n\t\t\t\thw->mac.ops.set_source_address_pruning(hw,\n\t\t\t\t\t\t\t\t       true,\n\t\t\t\t\t\t\t\t       p);\n\t\t}\n\t\tbreak;\n\tcase BRIDGE_MODE_VEB:\n\t\t \n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_PFDTXGSWC,\n\t\t\t\tIXGBE_PFDTXGSWC_VT_LBEN);\n\n\t\t \n\t\tvmdctl = IXGBE_READ_REG(hw, IXGBE_VMD_CTL);\n\t\tif (!adapter->num_vfs)\n\t\t\tvmdctl &= ~IXGBE_VT_CTL_REPLEN;\n\t\tIXGBE_WRITE_REG(hw, IXGBE_VMD_CTL, vmdctl);\n\n\t\t \n\t\tnum_pools = adapter->num_vfs + adapter->num_rx_pools;\n\t\tfor (p = 0; p < num_pools; p++) {\n\t\t\tif (hw->mac.ops.set_source_address_pruning)\n\t\t\t\thw->mac.ops.set_source_address_pruning(hw,\n\t\t\t\t\t\t\t\t       false,\n\t\t\t\t\t\t\t\t       p);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tadapter->bridge_mode = mode;\n\n\te_info(drv, \"enabling bridge mode: %s\\n\",\n\t       mode == BRIDGE_MODE_VEPA ? \"VEPA\" : \"VEB\");\n\n\treturn 0;\n}\n\nstatic int ixgbe_ndo_bridge_setlink(struct net_device *dev,\n\t\t\t\t    struct nlmsghdr *nlh, u16 flags,\n\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\tstruct nlattr *attr, *br_spec;\n\tint rem;\n\n\tif (!(adapter->flags & IXGBE_FLAG_SRIOV_ENABLED))\n\t\treturn -EOPNOTSUPP;\n\n\tbr_spec = nlmsg_find_attr(nlh, sizeof(struct ifinfomsg), IFLA_AF_SPEC);\n\tif (!br_spec)\n\t\treturn -EINVAL;\n\n\tnla_for_each_nested(attr, br_spec, rem) {\n\t\tint status;\n\t\t__u16 mode;\n\n\t\tif (nla_type(attr) != IFLA_BRIDGE_MODE)\n\t\t\tcontinue;\n\n\t\tmode = nla_get_u16(attr);\n\t\tstatus = ixgbe_configure_bridge_mode(adapter, mode);\n\t\tif (status)\n\t\t\treturn status;\n\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int ixgbe_ndo_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,\n\t\t\t\t    struct net_device *dev,\n\t\t\t\t    u32 filter_mask, int nlflags)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\n\tif (!(adapter->flags & IXGBE_FLAG_SRIOV_ENABLED))\n\t\treturn 0;\n\n\treturn ndo_dflt_bridge_getlink(skb, pid, seq, dev,\n\t\t\t\t       adapter->bridge_mode, 0, 0, nlflags,\n\t\t\t\t       filter_mask, NULL);\n}\n\nstatic void *ixgbe_fwd_add(struct net_device *pdev, struct net_device *vdev)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(pdev);\n\tstruct ixgbe_fwd_adapter *accel;\n\tint tcs = adapter->hw_tcs ? : 1;\n\tint pool, err;\n\n\tif (adapter->xdp_prog) {\n\t\te_warn(probe, \"L2FW offload is not supported with XDP\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t \n\tif (!macvlan_supports_dest_filter(vdev))\n\t\treturn ERR_PTR(-EMEDIUMTYPE);\n\n\t \n\tif (netif_is_multiqueue(vdev))\n\t\treturn ERR_PTR(-ERANGE);\n\n\tpool = find_first_zero_bit(adapter->fwd_bitmask, adapter->num_rx_pools);\n\tif (pool == adapter->num_rx_pools) {\n\t\tu16 used_pools = adapter->num_vfs + adapter->num_rx_pools;\n\t\tu16 reserved_pools;\n\n\t\tif (((adapter->flags & IXGBE_FLAG_DCB_ENABLED) &&\n\t\t     adapter->num_rx_pools >= (MAX_TX_QUEUES / tcs)) ||\n\t\t    adapter->num_rx_pools > IXGBE_MAX_MACVLANS)\n\t\t\treturn ERR_PTR(-EBUSY);\n\n\t\t \n\t\tif (used_pools >= IXGBE_MAX_VF_FUNCTIONS)\n\t\t\treturn ERR_PTR(-EBUSY);\n\n\t\t \n\t\tadapter->flags |= IXGBE_FLAG_VMDQ_ENABLED |\n\t\t\t\t  IXGBE_FLAG_SRIOV_ENABLED;\n\n\t\t \n\t\tif (used_pools < 32 && adapter->num_rx_pools < 16)\n\t\t\treserved_pools = min_t(u16,\n\t\t\t\t\t       32 - used_pools,\n\t\t\t\t\t       16 - adapter->num_rx_pools);\n\t\telse if (adapter->num_rx_pools < 32)\n\t\t\treserved_pools = min_t(u16,\n\t\t\t\t\t       64 - used_pools,\n\t\t\t\t\t       32 - adapter->num_rx_pools);\n\t\telse\n\t\t\treserved_pools = 64 - used_pools;\n\n\n\t\tif (!reserved_pools)\n\t\t\treturn ERR_PTR(-EBUSY);\n\n\t\tadapter->ring_feature[RING_F_VMDQ].limit += reserved_pools;\n\n\t\t \n\t\terr = ixgbe_setup_tc(pdev, adapter->hw_tcs);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\n\t\tif (pool >= adapter->num_rx_pools)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\taccel = kzalloc(sizeof(*accel), GFP_KERNEL);\n\tif (!accel)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tset_bit(pool, adapter->fwd_bitmask);\n\tnetdev_set_sb_channel(vdev, pool);\n\taccel->pool = pool;\n\taccel->netdev = vdev;\n\n\tif (!netif_running(pdev))\n\t\treturn accel;\n\n\terr = ixgbe_fwd_ring_up(adapter, accel);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn accel;\n}\n\nstatic void ixgbe_fwd_del(struct net_device *pdev, void *priv)\n{\n\tstruct ixgbe_fwd_adapter *accel = priv;\n\tstruct ixgbe_adapter *adapter = netdev_priv(pdev);\n\tunsigned int rxbase = accel->rx_base_queue;\n\tunsigned int i;\n\n\t \n\tixgbe_del_mac_filter(adapter, accel->netdev->dev_addr,\n\t\t\t     VMDQ_P(accel->pool));\n\n\t \n\tusleep_range(10000, 20000);\n\n\tfor (i = 0; i < adapter->num_rx_queues_per_pool; i++) {\n\t\tstruct ixgbe_ring *ring = adapter->rx_ring[rxbase + i];\n\t\tstruct ixgbe_q_vector *qv = ring->q_vector;\n\n\t\t \n\t\tif (netif_running(adapter->netdev))\n\t\t\tnapi_synchronize(&qv->napi);\n\t\tring->netdev = NULL;\n\t}\n\n\t \n\tnetdev_unbind_sb_channel(pdev, accel->netdev);\n\tnetdev_set_sb_channel(accel->netdev, 0);\n\n\tclear_bit(accel->pool, adapter->fwd_bitmask);\n\tkfree(accel);\n}\n\n#define IXGBE_MAX_MAC_HDR_LEN\t\t127\n#define IXGBE_MAX_NETWORK_HDR_LEN\t511\n\nstatic netdev_features_t\nixgbe_features_check(struct sk_buff *skb, struct net_device *dev,\n\t\t     netdev_features_t features)\n{\n\tunsigned int network_hdr_len, mac_hdr_len;\n\n\t \n\tmac_hdr_len = skb_network_header(skb) - skb->data;\n\tif (unlikely(mac_hdr_len > IXGBE_MAX_MAC_HDR_LEN))\n\t\treturn features & ~(NETIF_F_HW_CSUM |\n\t\t\t\t    NETIF_F_SCTP_CRC |\n\t\t\t\t    NETIF_F_GSO_UDP_L4 |\n\t\t\t\t    NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t    NETIF_F_TSO |\n\t\t\t\t    NETIF_F_TSO6);\n\n\tnetwork_hdr_len = skb_checksum_start(skb) - skb_network_header(skb);\n\tif (unlikely(network_hdr_len >  IXGBE_MAX_NETWORK_HDR_LEN))\n\t\treturn features & ~(NETIF_F_HW_CSUM |\n\t\t\t\t    NETIF_F_SCTP_CRC |\n\t\t\t\t    NETIF_F_GSO_UDP_L4 |\n\t\t\t\t    NETIF_F_TSO |\n\t\t\t\t    NETIF_F_TSO6);\n\n\t \n\tif (skb->encapsulation && !(features & NETIF_F_TSO_MANGLEID)) {\n#ifdef CONFIG_IXGBE_IPSEC\n\t\tif (!secpath_exists(skb))\n#endif\n\t\t\tfeatures &= ~NETIF_F_TSO;\n\t}\n\n\treturn features;\n}\n\nstatic int ixgbe_xdp_setup(struct net_device *dev, struct bpf_prog *prog)\n{\n\tint i, frame_size = dev->mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\tstruct bpf_prog *old_prog;\n\tbool need_reset;\n\tint num_queues;\n\n\tif (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED)\n\t\treturn -EINVAL;\n\n\tif (adapter->flags & IXGBE_FLAG_DCB_ENABLED)\n\t\treturn -EINVAL;\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct ixgbe_ring *ring = adapter->rx_ring[i];\n\n\t\tif (ring_is_rsc_enabled(ring))\n\t\t\treturn -EINVAL;\n\n\t\tif (frame_size > ixgbe_rx_bufsz(ring))\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (nr_cpu_ids > IXGBE_MAX_XDP_QS * 2)\n\t\treturn -ENOMEM;\n\n\told_prog = xchg(&adapter->xdp_prog, prog);\n\tneed_reset = (!!prog != !!old_prog);\n\n\t \n\tif (need_reset) {\n\t\tint err;\n\n\t\tif (!prog)\n\t\t\t \n\t\t\tsynchronize_rcu();\n\t\terr = ixgbe_setup_tc(dev, adapter->hw_tcs);\n\n\t\tif (err)\n\t\t\treturn -EINVAL;\n\t\tif (!prog)\n\t\t\txdp_features_clear_redirect_target(dev);\n\t} else {\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tWRITE_ONCE(adapter->rx_ring[i]->xdp_prog,\n\t\t\t\t   adapter->xdp_prog);\n\t\t}\n\t}\n\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\t \n\tif (need_reset && prog) {\n\t\tnum_queues = min_t(int, adapter->num_rx_queues,\n\t\t\t\t   adapter->num_xdp_queues);\n\t\tfor (i = 0; i < num_queues; i++)\n\t\t\tif (adapter->xdp_ring[i]->xsk_pool)\n\t\t\t\t(void)ixgbe_xsk_wakeup(adapter->netdev, i,\n\t\t\t\t\t\t       XDP_WAKEUP_RX);\n\t\txdp_features_set_redirect_target(dev, true);\n\t}\n\n\treturn 0;\n}\n\nstatic int ixgbe_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn ixgbe_xdp_setup(dev, xdp->prog);\n\tcase XDP_SETUP_XSK_POOL:\n\t\treturn ixgbe_xsk_pool_setup(adapter, xdp->xsk.pool,\n\t\t\t\t\t    xdp->xsk.queue_id);\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nvoid ixgbe_xdp_ring_update_tail(struct ixgbe_ring *ring)\n{\n\t \n\twmb();\n\twritel(ring->next_to_use, ring->tail);\n}\n\nvoid ixgbe_xdp_ring_update_tail_locked(struct ixgbe_ring *ring)\n{\n\tif (static_branch_unlikely(&ixgbe_xdp_locking_key))\n\t\tspin_lock(&ring->tx_lock);\n\tixgbe_xdp_ring_update_tail(ring);\n\tif (static_branch_unlikely(&ixgbe_xdp_locking_key))\n\t\tspin_unlock(&ring->tx_lock);\n}\n\nstatic int ixgbe_xdp_xmit(struct net_device *dev, int n,\n\t\t\t  struct xdp_frame **frames, u32 flags)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\tstruct ixgbe_ring *ring;\n\tint nxmit = 0;\n\tint i;\n\n\tif (unlikely(test_bit(__IXGBE_DOWN, &adapter->state)))\n\t\treturn -ENETDOWN;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\t \n\tring = adapter->xdp_prog ? ixgbe_determine_xdp_ring(adapter) : NULL;\n\tif (unlikely(!ring))\n\t\treturn -ENXIO;\n\n\tif (unlikely(test_bit(__IXGBE_TX_DISABLED, &ring->state)))\n\t\treturn -ENXIO;\n\n\tif (static_branch_unlikely(&ixgbe_xdp_locking_key))\n\t\tspin_lock(&ring->tx_lock);\n\n\tfor (i = 0; i < n; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\t\tint err;\n\n\t\terr = ixgbe_xmit_xdp_ring(ring, xdpf);\n\t\tif (err != IXGBE_XDP_TX)\n\t\t\tbreak;\n\t\tnxmit++;\n\t}\n\n\tif (unlikely(flags & XDP_XMIT_FLUSH))\n\t\tixgbe_xdp_ring_update_tail(ring);\n\n\tif (static_branch_unlikely(&ixgbe_xdp_locking_key))\n\t\tspin_unlock(&ring->tx_lock);\n\n\treturn nxmit;\n}\n\nstatic const struct net_device_ops ixgbe_netdev_ops = {\n\t.ndo_open\t\t= ixgbe_open,\n\t.ndo_stop\t\t= ixgbe_close,\n\t.ndo_start_xmit\t\t= ixgbe_xmit_frame,\n\t.ndo_set_rx_mode\t= ixgbe_set_rx_mode,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_mac_address\t= ixgbe_set_mac,\n\t.ndo_change_mtu\t\t= ixgbe_change_mtu,\n\t.ndo_tx_timeout\t\t= ixgbe_tx_timeout,\n\t.ndo_set_tx_maxrate\t= ixgbe_tx_maxrate,\n\t.ndo_vlan_rx_add_vid\t= ixgbe_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= ixgbe_vlan_rx_kill_vid,\n\t.ndo_eth_ioctl\t\t= ixgbe_ioctl,\n\t.ndo_set_vf_mac\t\t= ixgbe_ndo_set_vf_mac,\n\t.ndo_set_vf_vlan\t= ixgbe_ndo_set_vf_vlan,\n\t.ndo_set_vf_rate\t= ixgbe_ndo_set_vf_bw,\n\t.ndo_set_vf_spoofchk\t= ixgbe_ndo_set_vf_spoofchk,\n\t.ndo_set_vf_link_state\t= ixgbe_ndo_set_vf_link_state,\n\t.ndo_set_vf_rss_query_en = ixgbe_ndo_set_vf_rss_query_en,\n\t.ndo_set_vf_trust\t= ixgbe_ndo_set_vf_trust,\n\t.ndo_get_vf_config\t= ixgbe_ndo_get_vf_config,\n\t.ndo_get_vf_stats\t= ixgbe_ndo_get_vf_stats,\n\t.ndo_get_stats64\t= ixgbe_get_stats64,\n\t.ndo_setup_tc\t\t= __ixgbe_setup_tc,\n#ifdef IXGBE_FCOE\n\t.ndo_select_queue\t= ixgbe_select_queue,\n\t.ndo_fcoe_ddp_setup = ixgbe_fcoe_ddp_get,\n\t.ndo_fcoe_ddp_target = ixgbe_fcoe_ddp_target,\n\t.ndo_fcoe_ddp_done = ixgbe_fcoe_ddp_put,\n\t.ndo_fcoe_enable = ixgbe_fcoe_enable,\n\t.ndo_fcoe_disable = ixgbe_fcoe_disable,\n\t.ndo_fcoe_get_wwn = ixgbe_fcoe_get_wwn,\n\t.ndo_fcoe_get_hbainfo = ixgbe_fcoe_get_hbainfo,\n#endif  \n\t.ndo_set_features = ixgbe_set_features,\n\t.ndo_fix_features = ixgbe_fix_features,\n\t.ndo_fdb_add\t\t= ixgbe_ndo_fdb_add,\n\t.ndo_bridge_setlink\t= ixgbe_ndo_bridge_setlink,\n\t.ndo_bridge_getlink\t= ixgbe_ndo_bridge_getlink,\n\t.ndo_dfwd_add_station\t= ixgbe_fwd_add,\n\t.ndo_dfwd_del_station\t= ixgbe_fwd_del,\n\t.ndo_features_check\t= ixgbe_features_check,\n\t.ndo_bpf\t\t= ixgbe_xdp,\n\t.ndo_xdp_xmit\t\t= ixgbe_xdp_xmit,\n\t.ndo_xsk_wakeup         = ixgbe_xsk_wakeup,\n};\n\nstatic void ixgbe_disable_txr_hw(struct ixgbe_adapter *adapter,\n\t\t\t\t struct ixgbe_ring *tx_ring)\n{\n\tunsigned long wait_delay, delay_interval;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu8 reg_idx = tx_ring->reg_idx;\n\tint wait_loop;\n\tu32 txdctl;\n\n\tIXGBE_WRITE_REG(hw, IXGBE_TXDCTL(reg_idx), IXGBE_TXDCTL_SWFLSH);\n\n\t \n\tdelay_interval = ixgbe_get_completion_timeout(adapter) / 100;\n\n\twait_loop = IXGBE_MAX_RX_DESC_POLL;\n\twait_delay = delay_interval;\n\n\twhile (wait_loop--) {\n\t\tusleep_range(wait_delay, wait_delay + 10);\n\t\twait_delay += delay_interval * 2;\n\t\ttxdctl = IXGBE_READ_REG(hw, IXGBE_TXDCTL(reg_idx));\n\n\t\tif (!(txdctl & IXGBE_TXDCTL_ENABLE))\n\t\t\treturn;\n\t}\n\n\te_err(drv, \"TXDCTL.ENABLE not cleared within the polling period\\n\");\n}\n\nstatic void ixgbe_disable_txr(struct ixgbe_adapter *adapter,\n\t\t\t      struct ixgbe_ring *tx_ring)\n{\n\tset_bit(__IXGBE_TX_DISABLED, &tx_ring->state);\n\tixgbe_disable_txr_hw(adapter, tx_ring);\n}\n\nstatic void ixgbe_disable_rxr_hw(struct ixgbe_adapter *adapter,\n\t\t\t\t struct ixgbe_ring *rx_ring)\n{\n\tunsigned long wait_delay, delay_interval;\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu8 reg_idx = rx_ring->reg_idx;\n\tint wait_loop;\n\tu32 rxdctl;\n\n\trxdctl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(reg_idx));\n\trxdctl &= ~IXGBE_RXDCTL_ENABLE;\n\trxdctl |= IXGBE_RXDCTL_SWFLSH;\n\n\t \n\tIXGBE_WRITE_REG(hw, IXGBE_RXDCTL(reg_idx), rxdctl);\n\n\t \n\tif (hw->mac.type == ixgbe_mac_82598EB &&\n\t    !(IXGBE_READ_REG(hw, IXGBE_LINKS) & IXGBE_LINKS_UP))\n\t\treturn;\n\n\t \n\tdelay_interval = ixgbe_get_completion_timeout(adapter) / 100;\n\n\twait_loop = IXGBE_MAX_RX_DESC_POLL;\n\twait_delay = delay_interval;\n\n\twhile (wait_loop--) {\n\t\tusleep_range(wait_delay, wait_delay + 10);\n\t\twait_delay += delay_interval * 2;\n\t\trxdctl = IXGBE_READ_REG(hw, IXGBE_RXDCTL(reg_idx));\n\n\t\tif (!(rxdctl & IXGBE_RXDCTL_ENABLE))\n\t\t\treturn;\n\t}\n\n\te_err(drv, \"RXDCTL.ENABLE not cleared within the polling period\\n\");\n}\n\nstatic void ixgbe_reset_txr_stats(struct ixgbe_ring *tx_ring)\n{\n\tmemset(&tx_ring->stats, 0, sizeof(tx_ring->stats));\n\tmemset(&tx_ring->tx_stats, 0, sizeof(tx_ring->tx_stats));\n}\n\nstatic void ixgbe_reset_rxr_stats(struct ixgbe_ring *rx_ring)\n{\n\tmemset(&rx_ring->stats, 0, sizeof(rx_ring->stats));\n\tmemset(&rx_ring->rx_stats, 0, sizeof(rx_ring->rx_stats));\n}\n\n \nvoid ixgbe_txrx_ring_disable(struct ixgbe_adapter *adapter, int ring)\n{\n\tstruct ixgbe_ring *rx_ring, *tx_ring, *xdp_ring;\n\n\trx_ring = adapter->rx_ring[ring];\n\ttx_ring = adapter->tx_ring[ring];\n\txdp_ring = adapter->xdp_ring[ring];\n\n\tixgbe_disable_txr(adapter, tx_ring);\n\tif (xdp_ring)\n\t\tixgbe_disable_txr(adapter, xdp_ring);\n\tixgbe_disable_rxr_hw(adapter, rx_ring);\n\n\tif (xdp_ring)\n\t\tsynchronize_rcu();\n\n\t \n\tnapi_disable(&rx_ring->q_vector->napi);\n\n\tixgbe_clean_tx_ring(tx_ring);\n\tif (xdp_ring)\n\t\tixgbe_clean_tx_ring(xdp_ring);\n\tixgbe_clean_rx_ring(rx_ring);\n\n\tixgbe_reset_txr_stats(tx_ring);\n\tif (xdp_ring)\n\t\tixgbe_reset_txr_stats(xdp_ring);\n\tixgbe_reset_rxr_stats(rx_ring);\n}\n\n \nvoid ixgbe_txrx_ring_enable(struct ixgbe_adapter *adapter, int ring)\n{\n\tstruct ixgbe_ring *rx_ring, *tx_ring, *xdp_ring;\n\n\trx_ring = adapter->rx_ring[ring];\n\ttx_ring = adapter->tx_ring[ring];\n\txdp_ring = adapter->xdp_ring[ring];\n\n\t \n\tnapi_enable(&rx_ring->q_vector->napi);\n\n\tixgbe_configure_tx_ring(adapter, tx_ring);\n\tif (xdp_ring)\n\t\tixgbe_configure_tx_ring(adapter, xdp_ring);\n\tixgbe_configure_rx_ring(adapter, rx_ring);\n\n\tclear_bit(__IXGBE_TX_DISABLED, &tx_ring->state);\n\tif (xdp_ring)\n\t\tclear_bit(__IXGBE_TX_DISABLED, &xdp_ring->state);\n}\n\n \nstatic inline int ixgbe_enumerate_functions(struct ixgbe_adapter *adapter)\n{\n\tstruct pci_dev *entry, *pdev = adapter->pdev;\n\tint physfns = 0;\n\n\t \n\tif (ixgbe_pcie_from_parent(&adapter->hw))\n\t\tphysfns = 4;\n\n\tlist_for_each_entry(entry, &adapter->pdev->bus->devices, bus_list) {\n\t\t \n\t\tif (entry->is_virtfn)\n\t\t\tcontinue;\n\n\t\t \n\t\tif ((entry->vendor != pdev->vendor) ||\n\t\t    (entry->device != pdev->device))\n\t\t\treturn -1;\n\n\t\tphysfns++;\n\t}\n\n\treturn physfns;\n}\n\n \nbool ixgbe_wol_supported(struct ixgbe_adapter *adapter, u16 device_id,\n\t\t\t u16 subdevice_id)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tu16 wol_cap = adapter->eeprom_cap & IXGBE_DEVICE_CAPS_WOL_MASK;\n\n\t \n\tif (hw->mac.type == ixgbe_mac_82598EB)\n\t\treturn false;\n\n\t \n\tif (hw->mac.type >= ixgbe_mac_X540) {\n\t\tif ((wol_cap == IXGBE_DEVICE_CAPS_WOL_PORT0_1) ||\n\t\t    ((wol_cap == IXGBE_DEVICE_CAPS_WOL_PORT0) &&\n\t\t     (hw->bus.func == 0)))\n\t\t\treturn true;\n\t}\n\n\t \n\tswitch (device_id) {\n\tcase IXGBE_DEV_ID_82599_SFP:\n\t\t \n\t\tswitch (subdevice_id) {\n\t\tcase IXGBE_SUBDEV_ID_82599_560FLR:\n\t\tcase IXGBE_SUBDEV_ID_82599_LOM_SNAP6:\n\t\tcase IXGBE_SUBDEV_ID_82599_SFP_WOL0:\n\t\tcase IXGBE_SUBDEV_ID_82599_SFP_2OCP:\n\t\t\t \n\t\t\tif (hw->bus.func != 0)\n\t\t\t\tbreak;\n\t\t\tfallthrough;\n\t\tcase IXGBE_SUBDEV_ID_82599_SP_560FLR:\n\t\tcase IXGBE_SUBDEV_ID_82599_SFP:\n\t\tcase IXGBE_SUBDEV_ID_82599_RNDC:\n\t\tcase IXGBE_SUBDEV_ID_82599_ECNA_DP:\n\t\tcase IXGBE_SUBDEV_ID_82599_SFP_1OCP:\n\t\tcase IXGBE_SUBDEV_ID_82599_SFP_LOM_OEM1:\n\t\tcase IXGBE_SUBDEV_ID_82599_SFP_LOM_OEM2:\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\tcase IXGBE_DEV_ID_82599EN_SFP:\n\t\t \n\t\tswitch (subdevice_id) {\n\t\tcase IXGBE_SUBDEV_ID_82599EN_SFP_OCP1:\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\tcase IXGBE_DEV_ID_82599_COMBO_BACKPLANE:\n\t\t \n\t\tif (subdevice_id != IXGBE_SUBDEV_ID_82599_KX4_KR_MEZZ)\n\t\t\treturn true;\n\t\tbreak;\n\tcase IXGBE_DEV_ID_82599_KX4:\n\t\treturn  true;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\n \nstatic void ixgbe_set_fw_version(struct ixgbe_adapter *adapter)\n{\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct ixgbe_nvm_version nvm_ver;\n\n\tixgbe_get_oem_prod_version(hw, &nvm_ver);\n\tif (nvm_ver.oem_valid) {\n\t\tsnprintf(adapter->eeprom_id, sizeof(adapter->eeprom_id),\n\t\t\t \"%x.%x.%x\", nvm_ver.oem_major, nvm_ver.oem_minor,\n\t\t\t nvm_ver.oem_release);\n\t\treturn;\n\t}\n\n\tixgbe_get_etk_id(hw, &nvm_ver);\n\tixgbe_get_orom_version(hw, &nvm_ver);\n\n\tif (nvm_ver.or_valid) {\n\t\tsnprintf(adapter->eeprom_id, sizeof(adapter->eeprom_id),\n\t\t\t \"0x%08x, %d.%d.%d\", nvm_ver.etk_id, nvm_ver.or_major,\n\t\t\t nvm_ver.or_build, nvm_ver.or_patch);\n\t\treturn;\n\t}\n\n\t \n\tsnprintf(adapter->eeprom_id, sizeof(adapter->eeprom_id),\n\t\t \"0x%08x\", nvm_ver.etk_id);\n}\n\n \nstatic int ixgbe_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct net_device *netdev;\n\tstruct ixgbe_adapter *adapter = NULL;\n\tstruct ixgbe_hw *hw;\n\tconst struct ixgbe_info *ii = ixgbe_info_tbl[ent->driver_data];\n\tunsigned int indices = MAX_TX_QUEUES;\n\tu8 part_str[IXGBE_PBANUM_LENGTH];\n\tint i, err, expected_gts;\n\tbool disable_dev = false;\n#ifdef IXGBE_FCOE\n\tu16 device_caps;\n#endif\n\tu32 eec;\n\n\t \n\tif (pdev->is_virtfn) {\n\t\tWARN(1, KERN_ERR \"%s (%hx:%hx) should not be a VF!\\n\",\n\t\t     pci_name(pdev), pdev->vendor, pdev->device);\n\t\treturn -EINVAL;\n\t}\n\n\terr = pci_enable_device_mem(pdev);\n\tif (err)\n\t\treturn err;\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"No usable DMA configuration, aborting\\n\");\n\t\tgoto err_dma;\n\t}\n\n\terr = pci_request_mem_regions(pdev, ixgbe_driver_name);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"pci_request_selected_regions failed 0x%x\\n\", err);\n\t\tgoto err_pci_reg;\n\t}\n\n\tpci_set_master(pdev);\n\tpci_save_state(pdev);\n\n\tif (ii->mac == ixgbe_mac_82598EB) {\n#ifdef CONFIG_IXGBE_DCB\n\t\t \n\t\tindices = 4 * MAX_TRAFFIC_CLASS;\n#else\n\t\tindices = IXGBE_MAX_RSS_INDICES;\n#endif\n\t}\n\n\tnetdev = alloc_etherdev_mq(sizeof(struct ixgbe_adapter), indices);\n\tif (!netdev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_etherdev;\n\t}\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\n\tadapter = netdev_priv(netdev);\n\n\tadapter->netdev = netdev;\n\tadapter->pdev = pdev;\n\thw = &adapter->hw;\n\thw->back = adapter;\n\tadapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\n\n\thw->hw_addr = ioremap(pci_resource_start(pdev, 0),\n\t\t\t      pci_resource_len(pdev, 0));\n\tadapter->io_addr = hw->hw_addr;\n\tif (!hw->hw_addr) {\n\t\terr = -EIO;\n\t\tgoto err_ioremap;\n\t}\n\n\tnetdev->netdev_ops = &ixgbe_netdev_ops;\n\tixgbe_set_ethtool_ops(netdev);\n\tnetdev->watchdog_timeo = 5 * HZ;\n\tstrscpy(netdev->name, pci_name(pdev), sizeof(netdev->name));\n\n\t \n\thw->mac.ops   = *ii->mac_ops;\n\thw->mac.type  = ii->mac;\n\thw->mvals     = ii->mvals;\n\tif (ii->link_ops)\n\t\thw->link.ops  = *ii->link_ops;\n\n\t \n\thw->eeprom.ops = *ii->eeprom_ops;\n\teec = IXGBE_READ_REG(hw, IXGBE_EEC(hw));\n\tif (ixgbe_removed(hw->hw_addr)) {\n\t\terr = -EIO;\n\t\tgoto err_ioremap;\n\t}\n\t \n\tif (!(eec & BIT(8)))\n\t\thw->eeprom.ops.read = &ixgbe_read_eeprom_bit_bang_generic;\n\n\t \n\thw->phy.ops = *ii->phy_ops;\n\thw->phy.sfp_type = ixgbe_sfp_type_unknown;\n\t \n\thw->phy.mdio.prtad = MDIO_PRTAD_NONE;\n\thw->phy.mdio.mmds = 0;\n\thw->phy.mdio.mode_support = MDIO_SUPPORTS_C45 | MDIO_EMULATE_C22;\n\thw->phy.mdio.dev = netdev;\n\thw->phy.mdio.mdio_read = ixgbe_mdio_read;\n\thw->phy.mdio.mdio_write = ixgbe_mdio_write;\n\n\t \n\terr = ixgbe_sw_init(adapter, ii);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\tif (adapter->hw.mac.type == ixgbe_mac_82599EB)\n\t\tadapter->flags2 |= IXGBE_FLAG2_AUTO_DISABLE_VF;\n\n\tswitch (adapter->hw.mac.type) {\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\t\tnetdev->udp_tunnel_nic_info = &ixgbe_udp_tunnels_x550;\n\t\tbreak;\n\tcase ixgbe_mac_x550em_a:\n\t\tnetdev->udp_tunnel_nic_info = &ixgbe_udp_tunnels_x550em_a;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tif (hw->mac.ops.init_swfw_sync)\n\t\thw->mac.ops.init_swfw_sync(hw);\n\n\t \n\tswitch (adapter->hw.mac.type) {\n\tcase ixgbe_mac_82599EB:\n\tcase ixgbe_mac_X540:\n\tcase ixgbe_mac_X550:\n\tcase ixgbe_mac_X550EM_x:\n\tcase ixgbe_mac_x550em_a:\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_WUS, ~0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tif (adapter->flags & IXGBE_FLAG_FAN_FAIL_CAPABLE) {\n\t\tu32 esdp = IXGBE_READ_REG(hw, IXGBE_ESDP);\n\t\tif (esdp & IXGBE_ESDP_SDP1)\n\t\t\te_crit(probe, \"Fan has stopped, replace the adapter\\n\");\n\t}\n\n\tif (allow_unsupported_sfp)\n\t\thw->allow_unsupported_sfp = allow_unsupported_sfp;\n\n\t \n\thw->phy.reset_if_overtemp = true;\n\terr = hw->mac.ops.reset_hw(hw);\n\thw->phy.reset_if_overtemp = false;\n\tixgbe_set_eee_capable(adapter);\n\tif (err == IXGBE_ERR_SFP_NOT_PRESENT) {\n\t\terr = 0;\n\t} else if (err == IXGBE_ERR_SFP_NOT_SUPPORTED) {\n\t\te_dev_err(\"failed to load because an unsupported SFP+ or QSFP module type was detected.\\n\");\n\t\te_dev_err(\"Reload the driver after installing a supported module.\\n\");\n\t\tgoto err_sw_init;\n\t} else if (err) {\n\t\te_dev_err(\"HW Init failed: %d\\n\", err);\n\t\tgoto err_sw_init;\n\t}\n\n#ifdef CONFIG_PCI_IOV\n\t \n\tif (adapter->hw.mac.type == ixgbe_mac_82598EB)\n\t\tgoto skip_sriov;\n\t \n\tixgbe_init_mbx_params_pf(hw);\n\thw->mbx.ops = ii->mbx_ops;\n\tpci_sriov_set_totalvfs(pdev, IXGBE_MAX_VFS_DRV_LIMIT);\n\tixgbe_enable_sriov(adapter, max_vfs);\nskip_sriov:\n\n#endif\n\tnetdev->features = NETIF_F_SG |\n\t\t\t   NETIF_F_TSO |\n\t\t\t   NETIF_F_TSO6 |\n\t\t\t   NETIF_F_RXHASH |\n\t\t\t   NETIF_F_RXCSUM |\n\t\t\t   NETIF_F_HW_CSUM;\n\n#define IXGBE_GSO_PARTIAL_FEATURES (NETIF_F_GSO_GRE | \\\n\t\t\t\t    NETIF_F_GSO_GRE_CSUM | \\\n\t\t\t\t    NETIF_F_GSO_IPXIP4 | \\\n\t\t\t\t    NETIF_F_GSO_IPXIP6 | \\\n\t\t\t\t    NETIF_F_GSO_UDP_TUNNEL | \\\n\t\t\t\t    NETIF_F_GSO_UDP_TUNNEL_CSUM)\n\n\tnetdev->gso_partial_features = IXGBE_GSO_PARTIAL_FEATURES;\n\tnetdev->features |= NETIF_F_GSO_PARTIAL |\n\t\t\t    IXGBE_GSO_PARTIAL_FEATURES;\n\n\tif (hw->mac.type >= ixgbe_mac_82599EB)\n\t\tnetdev->features |= NETIF_F_SCTP_CRC | NETIF_F_GSO_UDP_L4;\n\n#ifdef CONFIG_IXGBE_IPSEC\n#define IXGBE_ESP_FEATURES\t(NETIF_F_HW_ESP | \\\n\t\t\t\t NETIF_F_HW_ESP_TX_CSUM | \\\n\t\t\t\t NETIF_F_GSO_ESP)\n\n\tif (adapter->ipsec)\n\t\tnetdev->features |= IXGBE_ESP_FEATURES;\n#endif\n\t \n\tnetdev->hw_features |= netdev->features |\n\t\t\t       NETIF_F_HW_VLAN_CTAG_FILTER |\n\t\t\t       NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t       NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t       NETIF_F_RXALL |\n\t\t\t       NETIF_F_HW_L2FW_DOFFLOAD;\n\n\tif (hw->mac.type >= ixgbe_mac_82599EB)\n\t\tnetdev->hw_features |= NETIF_F_NTUPLE |\n\t\t\t\t       NETIF_F_HW_TC;\n\n\tnetdev->features |= NETIF_F_HIGHDMA;\n\n\tnetdev->vlan_features |= netdev->features | NETIF_F_TSO_MANGLEID;\n\tnetdev->hw_enc_features |= netdev->vlan_features;\n\tnetdev->mpls_features |= NETIF_F_SG |\n\t\t\t\t NETIF_F_TSO |\n\t\t\t\t NETIF_F_TSO6 |\n\t\t\t\t NETIF_F_HW_CSUM;\n\tnetdev->mpls_features |= IXGBE_GSO_PARTIAL_FEATURES;\n\n\t \n\tnetdev->features |= NETIF_F_HW_VLAN_CTAG_FILTER |\n\t\t\t    NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t    NETIF_F_HW_VLAN_CTAG_TX;\n\n\tnetdev->priv_flags |= IFF_UNICAST_FLT;\n\tnetdev->priv_flags |= IFF_SUPP_NOFCS;\n\n\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |\n\t\t\t       NETDEV_XDP_ACT_XSK_ZEROCOPY;\n\n\t \n\tnetdev->min_mtu = ETH_MIN_MTU;\n\tnetdev->max_mtu = IXGBE_MAX_JUMBO_FRAME_SIZE - (ETH_HLEN + ETH_FCS_LEN);\n\n#ifdef CONFIG_IXGBE_DCB\n\tif (adapter->flags & IXGBE_FLAG_DCB_CAPABLE)\n\t\tnetdev->dcbnl_ops = &ixgbe_dcbnl_ops;\n#endif\n\n#ifdef IXGBE_FCOE\n\tif (adapter->flags & IXGBE_FLAG_FCOE_CAPABLE) {\n\t\tunsigned int fcoe_l;\n\n\t\tif (hw->mac.ops.get_device_caps) {\n\t\t\thw->mac.ops.get_device_caps(hw, &device_caps);\n\t\t\tif (device_caps & IXGBE_DEVICE_CAPS_FCOE_OFFLOADS)\n\t\t\t\tadapter->flags &= ~IXGBE_FLAG_FCOE_CAPABLE;\n\t\t}\n\n\n\t\tfcoe_l = min_t(int, IXGBE_FCRETA_SIZE, num_online_cpus());\n\t\tadapter->ring_feature[RING_F_FCOE].limit = fcoe_l;\n\n\t\tnetdev->features |= NETIF_F_FSO |\n\t\t\t\t    NETIF_F_FCOE_CRC;\n\n\t\tnetdev->vlan_features |= NETIF_F_FSO |\n\t\t\t\t\t NETIF_F_FCOE_CRC |\n\t\t\t\t\t NETIF_F_FCOE_MTU;\n\t}\n#endif  \n\tif (adapter->flags2 & IXGBE_FLAG2_RSC_CAPABLE)\n\t\tnetdev->hw_features |= NETIF_F_LRO;\n\tif (adapter->flags2 & IXGBE_FLAG2_RSC_ENABLED)\n\t\tnetdev->features |= NETIF_F_LRO;\n\n\tif (ixgbe_check_fw_error(adapter)) {\n\t\terr = -EIO;\n\t\tgoto err_sw_init;\n\t}\n\n\t \n\tif (hw->eeprom.ops.validate_checksum(hw, NULL) < 0) {\n\t\te_dev_err(\"The EEPROM Checksum Is Not Valid\\n\");\n\t\terr = -EIO;\n\t\tgoto err_sw_init;\n\t}\n\n\teth_platform_get_mac_address(&adapter->pdev->dev,\n\t\t\t\t     adapter->hw.mac.perm_addr);\n\n\teth_hw_addr_set(netdev, hw->mac.perm_addr);\n\n\tif (!is_valid_ether_addr(netdev->dev_addr)) {\n\t\te_dev_err(\"invalid MAC address\\n\");\n\t\terr = -EIO;\n\t\tgoto err_sw_init;\n\t}\n\n\t \n\tether_addr_copy(hw->mac.addr, hw->mac.perm_addr);\n\tixgbe_mac_set_default_filter(adapter);\n\n\ttimer_setup(&adapter->service_timer, ixgbe_service_timer, 0);\n\n\tif (ixgbe_removed(hw->hw_addr)) {\n\t\terr = -EIO;\n\t\tgoto err_sw_init;\n\t}\n\tINIT_WORK(&adapter->service_task, ixgbe_service_task);\n\tset_bit(__IXGBE_SERVICE_INITED, &adapter->state);\n\tclear_bit(__IXGBE_SERVICE_SCHED, &adapter->state);\n\n\terr = ixgbe_init_interrupt_scheme(adapter);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tu64_stats_init(&adapter->rx_ring[i]->syncp);\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tu64_stats_init(&adapter->tx_ring[i]->syncp);\n\tfor (i = 0; i < adapter->num_xdp_queues; i++)\n\t\tu64_stats_init(&adapter->xdp_ring[i]->syncp);\n\n\t \n\tadapter->wol = 0;\n\thw->eeprom.ops.read(hw, 0x2c, &adapter->eeprom_cap);\n\thw->wol_enabled = ixgbe_wol_supported(adapter, pdev->device,\n\t\t\t\t\t\tpdev->subsystem_device);\n\tif (hw->wol_enabled)\n\t\tadapter->wol = IXGBE_WUFC_MAG;\n\n\tdevice_set_wakeup_enable(&adapter->pdev->dev, adapter->wol);\n\n\t \n\tixgbe_set_fw_version(adapter);\n\n\t \n\tif (ixgbe_pcie_from_parent(hw))\n\t\tixgbe_get_parent_bus_info(adapter);\n\telse\n\t\t hw->mac.ops.get_bus_info(hw);\n\n\t \n\tswitch (hw->mac.type) {\n\tcase ixgbe_mac_82598EB:\n\t\texpected_gts = min(ixgbe_enumerate_functions(adapter) * 10, 16);\n\t\tbreak;\n\tdefault:\n\t\texpected_gts = ixgbe_enumerate_functions(adapter) * 10;\n\t\tbreak;\n\t}\n\n\t \n\tif (expected_gts > 0)\n\t\tixgbe_check_minimum_link(adapter, expected_gts);\n\n\terr = ixgbe_read_pba_string_generic(hw, part_str, sizeof(part_str));\n\tif (err)\n\t\tstrscpy(part_str, \"Unknown\", sizeof(part_str));\n\tif (ixgbe_is_sfp(hw) && hw->phy.sfp_type != ixgbe_sfp_type_not_present)\n\t\te_dev_info(\"MAC: %d, PHY: %d, SFP+: %d, PBA No: %s\\n\",\n\t\t\t   hw->mac.type, hw->phy.type, hw->phy.sfp_type,\n\t\t\t   part_str);\n\telse\n\t\te_dev_info(\"MAC: %d, PHY: %d, PBA No: %s\\n\",\n\t\t\t   hw->mac.type, hw->phy.type, part_str);\n\n\te_dev_info(\"%pM\\n\", netdev->dev_addr);\n\n\t \n\terr = hw->mac.ops.start_hw(hw);\n\tif (err == IXGBE_ERR_EEPROM_VERSION) {\n\t\t \n\t\te_dev_warn(\"This device is a pre-production adapter/LOM. \"\n\t\t\t   \"Please be aware there may be issues associated \"\n\t\t\t   \"with your hardware.  If you are experiencing \"\n\t\t\t   \"problems please contact your Intel or hardware \"\n\t\t\t   \"representative who provided you with this \"\n\t\t\t   \"hardware.\\n\");\n\t}\n\tstrcpy(netdev->name, \"eth%d\");\n\tpci_set_drvdata(pdev, adapter);\n\terr = register_netdev(netdev);\n\tif (err)\n\t\tgoto err_register;\n\n\n\t \n\tif (hw->mac.ops.disable_tx_laser)\n\t\thw->mac.ops.disable_tx_laser(hw);\n\n\t \n\tnetif_carrier_off(netdev);\n\n#ifdef CONFIG_IXGBE_DCA\n\tif (dca_add_requester(&pdev->dev) == 0) {\n\t\tadapter->flags |= IXGBE_FLAG_DCA_ENABLED;\n\t\tixgbe_setup_dca(adapter);\n\t}\n#endif\n\tif (adapter->flags & IXGBE_FLAG_SRIOV_ENABLED) {\n\t\te_info(probe, \"IOV is enabled with %d VFs\\n\", adapter->num_vfs);\n\t\tfor (i = 0; i < adapter->num_vfs; i++)\n\t\t\tixgbe_vf_configuration(pdev, (i | 0x10000000));\n\t}\n\n\t \n\tif (hw->mac.ops.set_fw_drv_ver)\n\t\thw->mac.ops.set_fw_drv_ver(hw, 0xFF, 0xFF, 0xFF, 0xFF,\n\t\t\t\t\t   sizeof(UTS_RELEASE) - 1,\n\t\t\t\t\t   UTS_RELEASE);\n\n\t \n\tixgbe_add_sanmac_netdev(netdev);\n\n\te_dev_info(\"%s\\n\", ixgbe_default_device_descr);\n\n#ifdef CONFIG_IXGBE_HWMON\n\tif (ixgbe_sysfs_init(adapter))\n\t\te_err(probe, \"failed to allocate sysfs resources\\n\");\n#endif  \n\n\tixgbe_dbg_adapter_init(adapter);\n\n\t \n\tif (ixgbe_mng_enabled(hw) && ixgbe_is_sfp(hw) && hw->mac.ops.setup_link)\n\t\thw->mac.ops.setup_link(hw,\n\t\t\tIXGBE_LINK_SPEED_10GB_FULL | IXGBE_LINK_SPEED_1GB_FULL,\n\t\t\ttrue);\n\n\terr = ixgbe_mii_bus_init(hw);\n\tif (err)\n\t\tgoto err_netdev;\n\n\treturn 0;\n\nerr_netdev:\n\tunregister_netdev(netdev);\nerr_register:\n\tixgbe_release_hw_control(adapter);\n\tixgbe_clear_interrupt_scheme(adapter);\nerr_sw_init:\n\tixgbe_disable_sriov(adapter);\n\tadapter->flags2 &= ~IXGBE_FLAG2_SEARCH_FOR_SFP;\n\tiounmap(adapter->io_addr);\n\tkfree(adapter->jump_tables[0]);\n\tkfree(adapter->mac_table);\n\tkfree(adapter->rss_key);\n\tbitmap_free(adapter->af_xdp_zc_qps);\nerr_ioremap:\n\tdisable_dev = !test_and_set_bit(__IXGBE_DISABLED, &adapter->state);\n\tfree_netdev(netdev);\nerr_alloc_etherdev:\n\tpci_release_mem_regions(pdev);\nerr_pci_reg:\nerr_dma:\n\tif (!adapter || disable_dev)\n\t\tpci_disable_device(pdev);\n\treturn err;\n}\n\n \nstatic void ixgbe_remove(struct pci_dev *pdev)\n{\n\tstruct ixgbe_adapter *adapter = pci_get_drvdata(pdev);\n\tstruct net_device *netdev;\n\tbool disable_dev;\n\tint i;\n\n\t \n\tif (!adapter)\n\t\treturn;\n\n\tnetdev  = adapter->netdev;\n\tixgbe_dbg_adapter_exit(adapter);\n\n\tset_bit(__IXGBE_REMOVING, &adapter->state);\n\tcancel_work_sync(&adapter->service_task);\n\n\tif (adapter->mii_bus)\n\t\tmdiobus_unregister(adapter->mii_bus);\n\n#ifdef CONFIG_IXGBE_DCA\n\tif (adapter->flags & IXGBE_FLAG_DCA_ENABLED) {\n\t\tadapter->flags &= ~IXGBE_FLAG_DCA_ENABLED;\n\t\tdca_remove_requester(&pdev->dev);\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_DCA_CTRL,\n\t\t\t\tIXGBE_DCA_CTRL_DCA_DISABLE);\n\t}\n\n#endif\n#ifdef CONFIG_IXGBE_HWMON\n\tixgbe_sysfs_exit(adapter);\n#endif  \n\n\t \n\tixgbe_del_sanmac_netdev(netdev);\n\n#ifdef CONFIG_PCI_IOV\n\tixgbe_disable_sriov(adapter);\n#endif\n\tif (netdev->reg_state == NETREG_REGISTERED)\n\t\tunregister_netdev(netdev);\n\n\tixgbe_stop_ipsec_offload(adapter);\n\tixgbe_clear_interrupt_scheme(adapter);\n\n\tixgbe_release_hw_control(adapter);\n\n#ifdef CONFIG_DCB\n\tkfree(adapter->ixgbe_ieee_pfc);\n\tkfree(adapter->ixgbe_ieee_ets);\n\n#endif\n\tiounmap(adapter->io_addr);\n\tpci_release_mem_regions(pdev);\n\n\te_dev_info(\"complete\\n\");\n\n\tfor (i = 0; i < IXGBE_MAX_LINK_HANDLE; i++) {\n\t\tif (adapter->jump_tables[i]) {\n\t\t\tkfree(adapter->jump_tables[i]->input);\n\t\t\tkfree(adapter->jump_tables[i]->mask);\n\t\t}\n\t\tkfree(adapter->jump_tables[i]);\n\t}\n\n\tkfree(adapter->mac_table);\n\tkfree(adapter->rss_key);\n\tbitmap_free(adapter->af_xdp_zc_qps);\n\tdisable_dev = !test_and_set_bit(__IXGBE_DISABLED, &adapter->state);\n\tfree_netdev(netdev);\n\n\tif (disable_dev)\n\t\tpci_disable_device(pdev);\n}\n\n \nstatic pci_ers_result_t ixgbe_io_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t\tpci_channel_state_t state)\n{\n\tstruct ixgbe_adapter *adapter = pci_get_drvdata(pdev);\n\tstruct net_device *netdev = adapter->netdev;\n\n#ifdef CONFIG_PCI_IOV\n\tstruct ixgbe_hw *hw = &adapter->hw;\n\tstruct pci_dev *bdev, *vfdev;\n\tu32 dw0, dw1, dw2, dw3;\n\tint vf, pos;\n\tu16 req_id, pf_func;\n\n\tif (adapter->hw.mac.type == ixgbe_mac_82598EB ||\n\t    adapter->num_vfs == 0)\n\t\tgoto skip_bad_vf_detection;\n\n\tbdev = pdev->bus->self;\n\twhile (bdev && (pci_pcie_type(bdev) != PCI_EXP_TYPE_ROOT_PORT))\n\t\tbdev = bdev->bus->self;\n\n\tif (!bdev)\n\t\tgoto skip_bad_vf_detection;\n\n\tpos = pci_find_ext_capability(bdev, PCI_EXT_CAP_ID_ERR);\n\tif (!pos)\n\t\tgoto skip_bad_vf_detection;\n\n\tdw0 = ixgbe_read_pci_cfg_dword(hw, pos + PCI_ERR_HEADER_LOG);\n\tdw1 = ixgbe_read_pci_cfg_dword(hw, pos + PCI_ERR_HEADER_LOG + 4);\n\tdw2 = ixgbe_read_pci_cfg_dword(hw, pos + PCI_ERR_HEADER_LOG + 8);\n\tdw3 = ixgbe_read_pci_cfg_dword(hw, pos + PCI_ERR_HEADER_LOG + 12);\n\tif (ixgbe_removed(hw->hw_addr))\n\t\tgoto skip_bad_vf_detection;\n\n\treq_id = dw1 >> 16;\n\t \n\tif (!(req_id & 0x0080))\n\t\tgoto skip_bad_vf_detection;\n\n\tpf_func = req_id & 0x01;\n\tif ((pf_func & 1) == (pdev->devfn & 1)) {\n\t\tunsigned int device_id;\n\n\t\tvf = (req_id & 0x7F) >> 1;\n\t\te_dev_err(\"VF %d has caused a PCIe error\\n\", vf);\n\t\te_dev_err(\"TLP: dw0: %8.8x\\tdw1: %8.8x\\tdw2: \"\n\t\t\t\t\"%8.8x\\tdw3: %8.8x\\n\",\n\t\tdw0, dw1, dw2, dw3);\n\t\tswitch (adapter->hw.mac.type) {\n\t\tcase ixgbe_mac_82599EB:\n\t\t\tdevice_id = IXGBE_82599_VF_DEVICE_ID;\n\t\t\tbreak;\n\t\tcase ixgbe_mac_X540:\n\t\t\tdevice_id = IXGBE_X540_VF_DEVICE_ID;\n\t\t\tbreak;\n\t\tcase ixgbe_mac_X550:\n\t\t\tdevice_id = IXGBE_DEV_ID_X550_VF;\n\t\t\tbreak;\n\t\tcase ixgbe_mac_X550EM_x:\n\t\t\tdevice_id = IXGBE_DEV_ID_X550EM_X_VF;\n\t\t\tbreak;\n\t\tcase ixgbe_mac_x550em_a:\n\t\t\tdevice_id = IXGBE_DEV_ID_X550EM_A_VF;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdevice_id = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tvfdev = pci_get_device(PCI_VENDOR_ID_INTEL, device_id, NULL);\n\t\twhile (vfdev) {\n\t\t\tif (vfdev->devfn == (req_id & 0xFF))\n\t\t\t\tbreak;\n\t\t\tvfdev = pci_get_device(PCI_VENDOR_ID_INTEL,\n\t\t\t\t\t       device_id, vfdev);\n\t\t}\n\t\t \n\t\tif (vfdev) {\n\t\t\tpcie_flr(vfdev);\n\t\t\t \n\t\t\tpci_dev_put(vfdev);\n\t\t}\n\t}\n\n\t \n\tadapter->vferr_refcount++;\n\n\treturn PCI_ERS_RESULT_RECOVERED;\n\nskip_bad_vf_detection:\n#endif  \n\tif (!test_bit(__IXGBE_SERVICE_INITED, &adapter->state))\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\tif (!netif_device_present(netdev))\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\trtnl_lock();\n\tnetif_device_detach(netdev);\n\n\tif (netif_running(netdev))\n\t\tixgbe_close_suspend(adapter);\n\n\tif (state == pci_channel_io_perm_failure) {\n\t\trtnl_unlock();\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\tif (!test_and_set_bit(__IXGBE_DISABLED, &adapter->state))\n\t\tpci_disable_device(pdev);\n\trtnl_unlock();\n\n\t \n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\n \nstatic pci_ers_result_t ixgbe_io_slot_reset(struct pci_dev *pdev)\n{\n\tstruct ixgbe_adapter *adapter = pci_get_drvdata(pdev);\n\tpci_ers_result_t result;\n\n\tif (pci_enable_device_mem(pdev)) {\n\t\te_err(probe, \"Cannot re-enable PCI device after reset.\\n\");\n\t\tresult = PCI_ERS_RESULT_DISCONNECT;\n\t} else {\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(__IXGBE_DISABLED, &adapter->state);\n\t\tadapter->hw.hw_addr = adapter->io_addr;\n\t\tpci_set_master(pdev);\n\t\tpci_restore_state(pdev);\n\t\tpci_save_state(pdev);\n\n\t\tpci_wake_from_d3(pdev, false);\n\n\t\tixgbe_reset(adapter);\n\t\tIXGBE_WRITE_REG(&adapter->hw, IXGBE_WUS, ~0);\n\t\tresult = PCI_ERS_RESULT_RECOVERED;\n\t}\n\n\treturn result;\n}\n\n \nstatic void ixgbe_io_resume(struct pci_dev *pdev)\n{\n\tstruct ixgbe_adapter *adapter = pci_get_drvdata(pdev);\n\tstruct net_device *netdev = adapter->netdev;\n\n#ifdef CONFIG_PCI_IOV\n\tif (adapter->vferr_refcount) {\n\t\te_info(drv, \"Resuming after VF err\\n\");\n\t\tadapter->vferr_refcount--;\n\t\treturn;\n\t}\n\n#endif\n\trtnl_lock();\n\tif (netif_running(netdev))\n\t\tixgbe_open(netdev);\n\n\tnetif_device_attach(netdev);\n\trtnl_unlock();\n}\n\nstatic const struct pci_error_handlers ixgbe_err_handler = {\n\t.error_detected = ixgbe_io_error_detected,\n\t.slot_reset = ixgbe_io_slot_reset,\n\t.resume = ixgbe_io_resume,\n};\n\nstatic SIMPLE_DEV_PM_OPS(ixgbe_pm_ops, ixgbe_suspend, ixgbe_resume);\n\nstatic struct pci_driver ixgbe_driver = {\n\t.name      = ixgbe_driver_name,\n\t.id_table  = ixgbe_pci_tbl,\n\t.probe     = ixgbe_probe,\n\t.remove    = ixgbe_remove,\n\t.driver.pm = &ixgbe_pm_ops,\n\t.shutdown  = ixgbe_shutdown,\n\t.sriov_configure = ixgbe_pci_sriov_configure,\n\t.err_handler = &ixgbe_err_handler\n};\n\n \nstatic int __init ixgbe_init_module(void)\n{\n\tint ret;\n\tpr_info(\"%s\\n\", ixgbe_driver_string);\n\tpr_info(\"%s\\n\", ixgbe_copyright);\n\n\tixgbe_wq = create_singlethread_workqueue(ixgbe_driver_name);\n\tif (!ixgbe_wq) {\n\t\tpr_err(\"%s: Failed to create workqueue\\n\", ixgbe_driver_name);\n\t\treturn -ENOMEM;\n\t}\n\n\tixgbe_dbg_init();\n\n\tret = pci_register_driver(&ixgbe_driver);\n\tif (ret) {\n\t\tdestroy_workqueue(ixgbe_wq);\n\t\tixgbe_dbg_exit();\n\t\treturn ret;\n\t}\n\n#ifdef CONFIG_IXGBE_DCA\n\tdca_register_notify(&dca_notifier);\n#endif\n\n\treturn 0;\n}\n\nmodule_init(ixgbe_init_module);\n\n \nstatic void __exit ixgbe_exit_module(void)\n{\n#ifdef CONFIG_IXGBE_DCA\n\tdca_unregister_notify(&dca_notifier);\n#endif\n\tpci_unregister_driver(&ixgbe_driver);\n\n\tixgbe_dbg_exit();\n\tif (ixgbe_wq) {\n\t\tdestroy_workqueue(ixgbe_wq);\n\t\tixgbe_wq = NULL;\n\t}\n}\n\n#ifdef CONFIG_IXGBE_DCA\nstatic int ixgbe_notify_dca(struct notifier_block *nb, unsigned long event,\n\t\t\t    void *p)\n{\n\tint ret_val;\n\n\tret_val = driver_for_each_device(&ixgbe_driver.driver, NULL, &event,\n\t\t\t\t\t __ixgbe_notify_dca);\n\n\treturn ret_val ? NOTIFY_BAD : NOTIFY_DONE;\n}\n\n#endif  \n\nmodule_exit(ixgbe_exit_module);\n\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}