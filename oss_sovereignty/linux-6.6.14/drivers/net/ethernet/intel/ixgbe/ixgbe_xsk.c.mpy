{
  "module_name": "ixgbe_xsk.c",
  "hash_id": "317082cc1c9ca4359de009840eb12ba76de5e541ec14b8c394ea4a065d2ce019",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/ixgbe/ixgbe_xsk.c",
  "human_readable_source": "\n \n\n#include <linux/bpf_trace.h>\n#include <net/xdp_sock_drv.h>\n#include <net/xdp.h>\n\n#include \"ixgbe.h\"\n#include \"ixgbe_txrx_common.h\"\n\nstruct xsk_buff_pool *ixgbe_xsk_pool(struct ixgbe_adapter *adapter,\n\t\t\t\t     struct ixgbe_ring *ring)\n{\n\tbool xdp_on = READ_ONCE(adapter->xdp_prog);\n\tint qid = ring->ring_idx;\n\n\tif (!xdp_on || !test_bit(qid, adapter->af_xdp_zc_qps))\n\t\treturn NULL;\n\n\treturn xsk_get_pool_from_qid(adapter->netdev, qid);\n}\n\nstatic int ixgbe_xsk_pool_enable(struct ixgbe_adapter *adapter,\n\t\t\t\t struct xsk_buff_pool *pool,\n\t\t\t\t u16 qid)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tbool if_running;\n\tint err;\n\n\tif (qid >= adapter->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (qid >= netdev->real_num_rx_queues ||\n\t    qid >= netdev->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\terr = xsk_pool_dma_map(pool, &adapter->pdev->dev, IXGBE_RX_DMA_ATTR);\n\tif (err)\n\t\treturn err;\n\n\tif_running = netif_running(adapter->netdev) &&\n\t\t     ixgbe_enabled_xdp_adapter(adapter);\n\n\tif (if_running)\n\t\tixgbe_txrx_ring_disable(adapter, qid);\n\n\tset_bit(qid, adapter->af_xdp_zc_qps);\n\n\tif (if_running) {\n\t\tixgbe_txrx_ring_enable(adapter, qid);\n\n\t\t \n\t\terr = ixgbe_xsk_wakeup(adapter->netdev, qid, XDP_WAKEUP_RX);\n\t\tif (err) {\n\t\t\tclear_bit(qid, adapter->af_xdp_zc_qps);\n\t\t\txsk_pool_dma_unmap(pool, IXGBE_RX_DMA_ATTR);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int ixgbe_xsk_pool_disable(struct ixgbe_adapter *adapter, u16 qid)\n{\n\tstruct xsk_buff_pool *pool;\n\tbool if_running;\n\n\tpool = xsk_get_pool_from_qid(adapter->netdev, qid);\n\tif (!pool)\n\t\treturn -EINVAL;\n\n\tif_running = netif_running(adapter->netdev) &&\n\t\t     ixgbe_enabled_xdp_adapter(adapter);\n\n\tif (if_running)\n\t\tixgbe_txrx_ring_disable(adapter, qid);\n\n\tclear_bit(qid, adapter->af_xdp_zc_qps);\n\txsk_pool_dma_unmap(pool, IXGBE_RX_DMA_ATTR);\n\n\tif (if_running)\n\t\tixgbe_txrx_ring_enable(adapter, qid);\n\n\treturn 0;\n}\n\nint ixgbe_xsk_pool_setup(struct ixgbe_adapter *adapter,\n\t\t\t struct xsk_buff_pool *pool,\n\t\t\t u16 qid)\n{\n\treturn pool ? ixgbe_xsk_pool_enable(adapter, pool, qid) :\n\t\tixgbe_xsk_pool_disable(adapter, qid);\n}\n\nstatic int ixgbe_run_xdp_zc(struct ixgbe_adapter *adapter,\n\t\t\t    struct ixgbe_ring *rx_ring,\n\t\t\t    struct xdp_buff *xdp)\n{\n\tint err, result = IXGBE_XDP_PASS;\n\tstruct bpf_prog *xdp_prog;\n\tstruct ixgbe_ring *ring;\n\tstruct xdp_frame *xdpf;\n\tu32 act;\n\n\txdp_prog = READ_ONCE(rx_ring->xdp_prog);\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\tif (likely(act == XDP_REDIRECT)) {\n\t\terr = xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog);\n\t\tif (!err)\n\t\t\treturn IXGBE_XDP_REDIR;\n\t\tif (xsk_uses_need_wakeup(rx_ring->xsk_pool) && err == -ENOBUFS)\n\t\t\tresult = IXGBE_XDP_EXIT;\n\t\telse\n\t\t\tresult = IXGBE_XDP_CONSUMED;\n\t\tgoto out_failure;\n\t}\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\txdpf = xdp_convert_buff_to_frame(xdp);\n\t\tif (unlikely(!xdpf))\n\t\t\tgoto out_failure;\n\t\tring = ixgbe_determine_xdp_ring(adapter);\n\t\tif (static_branch_unlikely(&ixgbe_xdp_locking_key))\n\t\t\tspin_lock(&ring->tx_lock);\n\t\tresult = ixgbe_xmit_xdp_ring(ring, xdpf);\n\t\tif (static_branch_unlikely(&ixgbe_xdp_locking_key))\n\t\t\tspin_unlock(&ring->tx_lock);\n\t\tif (result == IXGBE_XDP_CONSUMED)\n\t\t\tgoto out_failure;\n\t\tbreak;\n\tcase XDP_DROP:\n\t\tresult = IXGBE_XDP_CONSUMED;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\tresult = IXGBE_XDP_CONSUMED;\nout_failure:\n\t\ttrace_xdp_exception(rx_ring->netdev, xdp_prog, act);\n\t}\n\treturn result;\n}\n\nbool ixgbe_alloc_rx_buffers_zc(struct ixgbe_ring *rx_ring, u16 count)\n{\n\tunion ixgbe_adv_rx_desc *rx_desc;\n\tstruct ixgbe_rx_buffer *bi;\n\tu16 i = rx_ring->next_to_use;\n\tdma_addr_t dma;\n\tbool ok = true;\n\n\t \n\tif (!count)\n\t\treturn true;\n\n\trx_desc = IXGBE_RX_DESC(rx_ring, i);\n\tbi = &rx_ring->rx_buffer_info[i];\n\ti -= rx_ring->count;\n\n\tdo {\n\t\tbi->xdp = xsk_buff_alloc(rx_ring->xsk_pool);\n\t\tif (!bi->xdp) {\n\t\t\tok = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tdma = xsk_buff_xdp_get_dma(bi->xdp);\n\n\t\t \n\t\trx_desc->read.pkt_addr = cpu_to_le64(dma);\n\n\t\trx_desc++;\n\t\tbi++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\trx_desc = IXGBE_RX_DESC(rx_ring, 0);\n\t\t\tbi = rx_ring->rx_buffer_info;\n\t\t\ti -= rx_ring->count;\n\t\t}\n\n\t\t \n\t\trx_desc->wb.upper.length = 0;\n\n\t\tcount--;\n\t} while (count);\n\n\ti += rx_ring->count;\n\n\tif (rx_ring->next_to_use != i) {\n\t\trx_ring->next_to_use = i;\n\n\t\t \n\t\twmb();\n\t\twritel(i, rx_ring->tail);\n\t}\n\n\treturn ok;\n}\n\nstatic struct sk_buff *ixgbe_construct_skb_zc(struct ixgbe_ring *rx_ring,\n\t\t\t\t\t      const struct xdp_buff *xdp)\n{\n\tunsigned int totalsize = xdp->data_end - xdp->data_meta;\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n\tstruct sk_buff *skb;\n\n\tnet_prefetch(xdp->data_meta);\n\n\t \n\tskb = __napi_alloc_skb(&rx_ring->q_vector->napi, totalsize,\n\t\t\t       GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemcpy(__skb_put(skb, totalsize), xdp->data_meta,\n\t       ALIGN(totalsize, sizeof(long)));\n\n\tif (metasize) {\n\t\tskb_metadata_set(skb, metasize);\n\t\t__skb_pull(skb, metasize);\n\t}\n\n\treturn skb;\n}\n\nstatic void ixgbe_inc_ntc(struct ixgbe_ring *rx_ring)\n{\n\tu32 ntc = rx_ring->next_to_clean + 1;\n\n\tntc = (ntc < rx_ring->count) ? ntc : 0;\n\trx_ring->next_to_clean = ntc;\n\tprefetch(IXGBE_RX_DESC(rx_ring, ntc));\n}\n\nint ixgbe_clean_rx_irq_zc(struct ixgbe_q_vector *q_vector,\n\t\t\t  struct ixgbe_ring *rx_ring,\n\t\t\t  const int budget)\n{\n\tunsigned int total_rx_bytes = 0, total_rx_packets = 0;\n\tstruct ixgbe_adapter *adapter = q_vector->adapter;\n\tu16 cleaned_count = ixgbe_desc_unused(rx_ring);\n\tunsigned int xdp_res, xdp_xmit = 0;\n\tbool failure = false;\n\tstruct sk_buff *skb;\n\n\twhile (likely(total_rx_packets < budget)) {\n\t\tunion ixgbe_adv_rx_desc *rx_desc;\n\t\tstruct ixgbe_rx_buffer *bi;\n\t\tunsigned int size;\n\n\t\t \n\t\tif (cleaned_count >= IXGBE_RX_BUFFER_WRITE) {\n\t\t\tfailure = failure ||\n\t\t\t\t  !ixgbe_alloc_rx_buffers_zc(rx_ring,\n\t\t\t\t\t\t\t     cleaned_count);\n\t\t\tcleaned_count = 0;\n\t\t}\n\n\t\trx_desc = IXGBE_RX_DESC(rx_ring, rx_ring->next_to_clean);\n\t\tsize = le16_to_cpu(rx_desc->wb.upper.length);\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tbi = &rx_ring->rx_buffer_info[rx_ring->next_to_clean];\n\n\t\tif (unlikely(!ixgbe_test_staterr(rx_desc,\n\t\t\t\t\t\t IXGBE_RXD_STAT_EOP))) {\n\t\t\tstruct ixgbe_rx_buffer *next_bi;\n\n\t\t\txsk_buff_free(bi->xdp);\n\t\t\tbi->xdp = NULL;\n\t\t\tixgbe_inc_ntc(rx_ring);\n\t\t\tnext_bi =\n\t\t\t       &rx_ring->rx_buffer_info[rx_ring->next_to_clean];\n\t\t\tnext_bi->discard = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (unlikely(bi->discard)) {\n\t\t\txsk_buff_free(bi->xdp);\n\t\t\tbi->xdp = NULL;\n\t\t\tbi->discard = false;\n\t\t\tixgbe_inc_ntc(rx_ring);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbi->xdp->data_end = bi->xdp->data + size;\n\t\txsk_buff_dma_sync_for_cpu(bi->xdp, rx_ring->xsk_pool);\n\t\txdp_res = ixgbe_run_xdp_zc(adapter, rx_ring, bi->xdp);\n\n\t\tif (likely(xdp_res & (IXGBE_XDP_TX | IXGBE_XDP_REDIR))) {\n\t\t\txdp_xmit |= xdp_res;\n\t\t} else if (xdp_res == IXGBE_XDP_EXIT) {\n\t\t\tfailure = true;\n\t\t\tbreak;\n\t\t} else if (xdp_res == IXGBE_XDP_CONSUMED) {\n\t\t\txsk_buff_free(bi->xdp);\n\t\t} else if (xdp_res == IXGBE_XDP_PASS) {\n\t\t\tgoto construct_skb;\n\t\t}\n\n\t\tbi->xdp = NULL;\n\t\ttotal_rx_packets++;\n\t\ttotal_rx_bytes += size;\n\n\t\tcleaned_count++;\n\t\tixgbe_inc_ntc(rx_ring);\n\t\tcontinue;\n\nconstruct_skb:\n\t\t \n\t\tskb = ixgbe_construct_skb_zc(rx_ring, bi->xdp);\n\t\tif (!skb) {\n\t\t\trx_ring->rx_stats.alloc_rx_buff_failed++;\n\t\t\tbreak;\n\t\t}\n\n\t\txsk_buff_free(bi->xdp);\n\t\tbi->xdp = NULL;\n\n\t\tcleaned_count++;\n\t\tixgbe_inc_ntc(rx_ring);\n\n\t\tif (eth_skb_pad(skb))\n\t\t\tcontinue;\n\n\t\ttotal_rx_bytes += skb->len;\n\t\ttotal_rx_packets++;\n\n\t\tixgbe_process_skb_fields(rx_ring, rx_desc, skb);\n\t\tixgbe_rx_skb(q_vector, skb);\n\t}\n\n\tif (xdp_xmit & IXGBE_XDP_REDIR)\n\t\txdp_do_flush_map();\n\n\tif (xdp_xmit & IXGBE_XDP_TX) {\n\t\tstruct ixgbe_ring *ring = ixgbe_determine_xdp_ring(adapter);\n\n\t\tixgbe_xdp_ring_update_tail_locked(ring);\n\t}\n\n\tu64_stats_update_begin(&rx_ring->syncp);\n\trx_ring->stats.packets += total_rx_packets;\n\trx_ring->stats.bytes += total_rx_bytes;\n\tu64_stats_update_end(&rx_ring->syncp);\n\tq_vector->rx.total_packets += total_rx_packets;\n\tq_vector->rx.total_bytes += total_rx_bytes;\n\n\tif (xsk_uses_need_wakeup(rx_ring->xsk_pool)) {\n\t\tif (failure || rx_ring->next_to_clean == rx_ring->next_to_use)\n\t\t\txsk_set_rx_need_wakeup(rx_ring->xsk_pool);\n\t\telse\n\t\t\txsk_clear_rx_need_wakeup(rx_ring->xsk_pool);\n\n\t\treturn (int)total_rx_packets;\n\t}\n\treturn failure ? budget : (int)total_rx_packets;\n}\n\nvoid ixgbe_xsk_clean_rx_ring(struct ixgbe_ring *rx_ring)\n{\n\tstruct ixgbe_rx_buffer *bi;\n\tu16 i;\n\n\tfor (i = 0; i < rx_ring->count; i++) {\n\t\tbi = &rx_ring->rx_buffer_info[i];\n\n\t\tif (!bi->xdp)\n\t\t\tcontinue;\n\n\t\txsk_buff_free(bi->xdp);\n\t\tbi->xdp = NULL;\n\t}\n}\n\nstatic bool ixgbe_xmit_zc(struct ixgbe_ring *xdp_ring, unsigned int budget)\n{\n\tstruct xsk_buff_pool *pool = xdp_ring->xsk_pool;\n\tunion ixgbe_adv_tx_desc *tx_desc = NULL;\n\tstruct ixgbe_tx_buffer *tx_bi;\n\tbool work_done = true;\n\tstruct xdp_desc desc;\n\tdma_addr_t dma;\n\tu32 cmd_type;\n\n\twhile (budget-- > 0) {\n\t\tif (unlikely(!ixgbe_desc_unused(xdp_ring))) {\n\t\t\twork_done = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!netif_carrier_ok(xdp_ring->netdev))\n\t\t\tbreak;\n\n\t\tif (!xsk_tx_peek_desc(pool, &desc))\n\t\t\tbreak;\n\n\t\tdma = xsk_buff_raw_get_dma(pool, desc.addr);\n\t\txsk_buff_raw_dma_sync_for_device(pool, dma, desc.len);\n\n\t\ttx_bi = &xdp_ring->tx_buffer_info[xdp_ring->next_to_use];\n\t\ttx_bi->bytecount = desc.len;\n\t\ttx_bi->xdpf = NULL;\n\t\ttx_bi->gso_segs = 1;\n\n\t\ttx_desc = IXGBE_TX_DESC(xdp_ring, xdp_ring->next_to_use);\n\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\n\t\t \n\t\tcmd_type = IXGBE_ADVTXD_DTYP_DATA |\n\t\t\t   IXGBE_ADVTXD_DCMD_DEXT |\n\t\t\t   IXGBE_ADVTXD_DCMD_IFCS;\n\t\tcmd_type |= desc.len | IXGBE_TXD_CMD;\n\t\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\t\ttx_desc->read.olinfo_status =\n\t\t\tcpu_to_le32(desc.len << IXGBE_ADVTXD_PAYLEN_SHIFT);\n\n\t\txdp_ring->next_to_use++;\n\t\tif (xdp_ring->next_to_use == xdp_ring->count)\n\t\t\txdp_ring->next_to_use = 0;\n\t}\n\n\tif (tx_desc) {\n\t\tixgbe_xdp_ring_update_tail(xdp_ring);\n\t\txsk_tx_release(pool);\n\t}\n\n\treturn !!budget && work_done;\n}\n\nstatic void ixgbe_clean_xdp_tx_buffer(struct ixgbe_ring *tx_ring,\n\t\t\t\t      struct ixgbe_tx_buffer *tx_bi)\n{\n\txdp_return_frame(tx_bi->xdpf);\n\tdma_unmap_single(tx_ring->dev,\n\t\t\t dma_unmap_addr(tx_bi, dma),\n\t\t\t dma_unmap_len(tx_bi, len), DMA_TO_DEVICE);\n\tdma_unmap_len_set(tx_bi, len, 0);\n}\n\nbool ixgbe_clean_xdp_tx_irq(struct ixgbe_q_vector *q_vector,\n\t\t\t    struct ixgbe_ring *tx_ring, int napi_budget)\n{\n\tu16 ntc = tx_ring->next_to_clean, ntu = tx_ring->next_to_use;\n\tunsigned int total_packets = 0, total_bytes = 0;\n\tstruct xsk_buff_pool *pool = tx_ring->xsk_pool;\n\tunion ixgbe_adv_tx_desc *tx_desc;\n\tstruct ixgbe_tx_buffer *tx_bi;\n\tu32 xsk_frames = 0;\n\n\ttx_bi = &tx_ring->tx_buffer_info[ntc];\n\ttx_desc = IXGBE_TX_DESC(tx_ring, ntc);\n\n\twhile (ntc != ntu) {\n\t\tif (!(tx_desc->wb.status & cpu_to_le32(IXGBE_TXD_STAT_DD)))\n\t\t\tbreak;\n\n\t\ttotal_bytes += tx_bi->bytecount;\n\t\ttotal_packets += tx_bi->gso_segs;\n\n\t\tif (tx_bi->xdpf)\n\t\t\tixgbe_clean_xdp_tx_buffer(tx_ring, tx_bi);\n\t\telse\n\t\t\txsk_frames++;\n\n\t\ttx_bi->xdpf = NULL;\n\n\t\ttx_bi++;\n\t\ttx_desc++;\n\t\tntc++;\n\t\tif (unlikely(ntc == tx_ring->count)) {\n\t\t\tntc = 0;\n\t\t\ttx_bi = tx_ring->tx_buffer_info;\n\t\t\ttx_desc = IXGBE_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\t \n\t\tprefetch(tx_desc);\n\t}\n\n\ttx_ring->next_to_clean = ntc;\n\n\tu64_stats_update_begin(&tx_ring->syncp);\n\ttx_ring->stats.bytes += total_bytes;\n\ttx_ring->stats.packets += total_packets;\n\tu64_stats_update_end(&tx_ring->syncp);\n\tq_vector->tx.total_bytes += total_bytes;\n\tq_vector->tx.total_packets += total_packets;\n\n\tif (xsk_frames)\n\t\txsk_tx_completed(pool, xsk_frames);\n\n\tif (xsk_uses_need_wakeup(pool))\n\t\txsk_set_tx_need_wakeup(pool);\n\n\treturn ixgbe_xmit_zc(tx_ring, q_vector->tx.work_limit);\n}\n\nint ixgbe_xsk_wakeup(struct net_device *dev, u32 qid, u32 flags)\n{\n\tstruct ixgbe_adapter *adapter = netdev_priv(dev);\n\tstruct ixgbe_ring *ring;\n\n\tif (test_bit(__IXGBE_DOWN, &adapter->state))\n\t\treturn -ENETDOWN;\n\n\tif (!READ_ONCE(adapter->xdp_prog))\n\t\treturn -EINVAL;\n\n\tif (qid >= adapter->num_xdp_queues)\n\t\treturn -EINVAL;\n\n\tring = adapter->xdp_ring[qid];\n\n\tif (test_bit(__IXGBE_TX_DISABLED, &ring->state))\n\t\treturn -ENETDOWN;\n\n\tif (!ring->xsk_pool)\n\t\treturn -EINVAL;\n\n\tif (!napi_if_scheduled_mark_missed(&ring->q_vector->napi)) {\n\t\tu64 eics = BIT_ULL(ring->q_vector->v_idx);\n\n\t\tixgbe_irq_rearm_queues(adapter, eics);\n\t}\n\n\treturn 0;\n}\n\nvoid ixgbe_xsk_clean_tx_ring(struct ixgbe_ring *tx_ring)\n{\n\tu16 ntc = tx_ring->next_to_clean, ntu = tx_ring->next_to_use;\n\tstruct xsk_buff_pool *pool = tx_ring->xsk_pool;\n\tstruct ixgbe_tx_buffer *tx_bi;\n\tu32 xsk_frames = 0;\n\n\twhile (ntc != ntu) {\n\t\ttx_bi = &tx_ring->tx_buffer_info[ntc];\n\n\t\tif (tx_bi->xdpf)\n\t\t\tixgbe_clean_xdp_tx_buffer(tx_ring, tx_bi);\n\t\telse\n\t\t\txsk_frames++;\n\n\t\ttx_bi->xdpf = NULL;\n\n\t\tntc++;\n\t\tif (ntc == tx_ring->count)\n\t\t\tntc = 0;\n\t}\n\n\tif (xsk_frames)\n\t\txsk_tx_completed(pool, xsk_frames);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}