{
  "module_name": "i40e_txrx.c",
  "hash_id": "09e27f13c64c5951266c7aecc8851bf4818bfd1d8fe1e04a7b358f1f594ee4b1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/i40e/i40e_txrx.c",
  "human_readable_source": "\n \n\n#include <linux/prefetch.h>\n#include <linux/bpf_trace.h>\n#include <net/mpls.h>\n#include <net/xdp.h>\n#include \"i40e.h\"\n#include \"i40e_trace.h\"\n#include \"i40e_prototype.h\"\n#include \"i40e_txrx_common.h\"\n#include \"i40e_xsk.h\"\n\n#define I40E_TXD_CMD (I40E_TX_DESC_CMD_EOP | I40E_TX_DESC_CMD_RS)\n \nstatic void i40e_fdir(struct i40e_ring *tx_ring,\n\t\t      struct i40e_fdir_filter *fdata, bool add)\n{\n\tstruct i40e_filter_program_desc *fdir_desc;\n\tstruct i40e_pf *pf = tx_ring->vsi->back;\n\tu32 flex_ptype, dtype_cmd;\n\tu16 i;\n\n\t \n\ti = tx_ring->next_to_use;\n\tfdir_desc = I40E_TX_FDIRDESC(tx_ring, i);\n\n\ti++;\n\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\tflex_ptype = I40E_TXD_FLTR_QW0_QINDEX_MASK &\n\t\t     (fdata->q_index << I40E_TXD_FLTR_QW0_QINDEX_SHIFT);\n\n\tflex_ptype |= I40E_TXD_FLTR_QW0_FLEXOFF_MASK &\n\t\t      (fdata->flex_off << I40E_TXD_FLTR_QW0_FLEXOFF_SHIFT);\n\n\tflex_ptype |= I40E_TXD_FLTR_QW0_PCTYPE_MASK &\n\t\t      (fdata->pctype << I40E_TXD_FLTR_QW0_PCTYPE_SHIFT);\n\n\t \n\tflex_ptype |= I40E_TXD_FLTR_QW0_DEST_VSI_MASK &\n\t\t      ((u32)(fdata->dest_vsi ? : pf->vsi[pf->lan_vsi]->id) <<\n\t\t       I40E_TXD_FLTR_QW0_DEST_VSI_SHIFT);\n\n\tdtype_cmd = I40E_TX_DESC_DTYPE_FILTER_PROG;\n\n\tdtype_cmd |= add ?\n\t\t     I40E_FILTER_PROGRAM_DESC_PCMD_ADD_UPDATE <<\n\t\t     I40E_TXD_FLTR_QW1_PCMD_SHIFT :\n\t\t     I40E_FILTER_PROGRAM_DESC_PCMD_REMOVE <<\n\t\t     I40E_TXD_FLTR_QW1_PCMD_SHIFT;\n\n\tdtype_cmd |= I40E_TXD_FLTR_QW1_DEST_MASK &\n\t\t     (fdata->dest_ctl << I40E_TXD_FLTR_QW1_DEST_SHIFT);\n\n\tdtype_cmd |= I40E_TXD_FLTR_QW1_FD_STATUS_MASK &\n\t\t     (fdata->fd_status << I40E_TXD_FLTR_QW1_FD_STATUS_SHIFT);\n\n\tif (fdata->cnt_index) {\n\t\tdtype_cmd |= I40E_TXD_FLTR_QW1_CNT_ENA_MASK;\n\t\tdtype_cmd |= I40E_TXD_FLTR_QW1_CNTINDEX_MASK &\n\t\t\t     ((u32)fdata->cnt_index <<\n\t\t\t      I40E_TXD_FLTR_QW1_CNTINDEX_SHIFT);\n\t}\n\n\tfdir_desc->qindex_flex_ptype_vsi = cpu_to_le32(flex_ptype);\n\tfdir_desc->rsvd = cpu_to_le32(0);\n\tfdir_desc->dtype_cmd_cntindex = cpu_to_le32(dtype_cmd);\n\tfdir_desc->fd_id = cpu_to_le32(fdata->fd_id);\n}\n\n#define I40E_FD_CLEAN_DELAY 10\n \nstatic int i40e_program_fdir_filter(struct i40e_fdir_filter *fdir_data,\n\t\t\t\t    u8 *raw_packet, struct i40e_pf *pf,\n\t\t\t\t    bool add)\n{\n\tstruct i40e_tx_buffer *tx_buf, *first;\n\tstruct i40e_tx_desc *tx_desc;\n\tstruct i40e_ring *tx_ring;\n\tstruct i40e_vsi *vsi;\n\tstruct device *dev;\n\tdma_addr_t dma;\n\tu32 td_cmd = 0;\n\tu16 i;\n\n\t \n\tvsi = i40e_find_vsi_by_type(pf, I40E_VSI_FDIR);\n\tif (!vsi)\n\t\treturn -ENOENT;\n\n\ttx_ring = vsi->tx_rings[0];\n\tdev = tx_ring->dev;\n\n\t \n\tfor (i = I40E_FD_CLEAN_DELAY; I40E_DESC_UNUSED(tx_ring) < 2; i--) {\n\t\tif (!i)\n\t\t\treturn -EAGAIN;\n\t\tmsleep_interruptible(1);\n\t}\n\n\tdma = dma_map_single(dev, raw_packet,\n\t\t\t     I40E_FDIR_MAX_RAW_PACKET_SIZE, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma))\n\t\tgoto dma_fail;\n\n\t \n\ti = tx_ring->next_to_use;\n\tfirst = &tx_ring->tx_bi[i];\n\ti40e_fdir(tx_ring, fdir_data, add);\n\n\t \n\ti = tx_ring->next_to_use;\n\ttx_desc = I40E_TX_DESC(tx_ring, i);\n\ttx_buf = &tx_ring->tx_bi[i];\n\n\ttx_ring->next_to_use = ((i + 1) < tx_ring->count) ? i + 1 : 0;\n\n\tmemset(tx_buf, 0, sizeof(struct i40e_tx_buffer));\n\n\t \n\tdma_unmap_len_set(tx_buf, len, I40E_FDIR_MAX_RAW_PACKET_SIZE);\n\tdma_unmap_addr_set(tx_buf, dma, dma);\n\n\ttx_desc->buffer_addr = cpu_to_le64(dma);\n\ttd_cmd = I40E_TXD_CMD | I40E_TX_DESC_CMD_DUMMY;\n\n\ttx_buf->tx_flags = I40E_TX_FLAGS_FD_SB;\n\ttx_buf->raw_buf = (void *)raw_packet;\n\n\ttx_desc->cmd_type_offset_bsz =\n\t\tbuild_ctob(td_cmd, 0, I40E_FDIR_MAX_RAW_PACKET_SIZE, 0);\n\n\t \n\twmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\twritel(tx_ring->next_to_use, tx_ring->tail);\n\treturn 0;\n\ndma_fail:\n\treturn -1;\n}\n\n \nstatic char *i40e_create_dummy_packet(u8 *dummy_packet, bool ipv4, u8 l4proto,\n\t\t\t\t      struct i40e_fdir_filter *data)\n{\n\tbool is_vlan = !!data->vlan_tag;\n\tstruct vlan_hdr vlan = {};\n\tstruct ipv6hdr ipv6 = {};\n\tstruct ethhdr eth = {};\n\tstruct iphdr ip = {};\n\tu8 *tmp;\n\n\tif (ipv4) {\n\t\teth.h_proto = cpu_to_be16(ETH_P_IP);\n\t\tip.protocol = l4proto;\n\t\tip.version = 0x4;\n\t\tip.ihl = 0x5;\n\n\t\tip.daddr = data->dst_ip;\n\t\tip.saddr = data->src_ip;\n\t} else {\n\t\teth.h_proto = cpu_to_be16(ETH_P_IPV6);\n\t\tipv6.nexthdr = l4proto;\n\t\tipv6.version = 0x6;\n\n\t\tmemcpy(&ipv6.saddr.in6_u.u6_addr32, data->src_ip6,\n\t\t       sizeof(__be32) * 4);\n\t\tmemcpy(&ipv6.daddr.in6_u.u6_addr32, data->dst_ip6,\n\t\t       sizeof(__be32) * 4);\n\t}\n\n\tif (is_vlan) {\n\t\tvlan.h_vlan_TCI = data->vlan_tag;\n\t\tvlan.h_vlan_encapsulated_proto = eth.h_proto;\n\t\teth.h_proto = data->vlan_etype;\n\t}\n\n\ttmp = dummy_packet;\n\tmemcpy(tmp, &eth, sizeof(eth));\n\ttmp += sizeof(eth);\n\n\tif (is_vlan) {\n\t\tmemcpy(tmp, &vlan, sizeof(vlan));\n\t\ttmp += sizeof(vlan);\n\t}\n\n\tif (ipv4) {\n\t\tmemcpy(tmp, &ip, sizeof(ip));\n\t\ttmp += sizeof(ip);\n\t} else {\n\t\tmemcpy(tmp, &ipv6, sizeof(ipv6));\n\t\ttmp += sizeof(ipv6);\n\t}\n\n\treturn tmp;\n}\n\n \nstatic void i40e_create_dummy_udp_packet(u8 *raw_packet, bool ipv4, u8 l4proto,\n\t\t\t\t\t struct i40e_fdir_filter *data)\n{\n\tstruct udphdr *udp;\n\tu8 *tmp;\n\n\ttmp = i40e_create_dummy_packet(raw_packet, ipv4, IPPROTO_UDP, data);\n\tudp = (struct udphdr *)(tmp);\n\tudp->dest = data->dst_port;\n\tudp->source = data->src_port;\n}\n\n \nstatic void i40e_create_dummy_tcp_packet(u8 *raw_packet, bool ipv4, u8 l4proto,\n\t\t\t\t\t struct i40e_fdir_filter *data)\n{\n\tstruct tcphdr *tcp;\n\tu8 *tmp;\n\t \n\tstatic const char tcp_packet[] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n\t\t0x50, 0x11, 0x0, 0x72, 0, 0, 0, 0};\n\n\ttmp = i40e_create_dummy_packet(raw_packet, ipv4, IPPROTO_TCP, data);\n\n\ttcp = (struct tcphdr *)tmp;\n\tmemcpy(tcp, tcp_packet, sizeof(tcp_packet));\n\ttcp->dest = data->dst_port;\n\ttcp->source = data->src_port;\n}\n\n \nstatic void i40e_create_dummy_sctp_packet(u8 *raw_packet, bool ipv4,\n\t\t\t\t\t  u8 l4proto,\n\t\t\t\t\t  struct i40e_fdir_filter *data)\n{\n\tstruct sctphdr *sctp;\n\tu8 *tmp;\n\n\ttmp = i40e_create_dummy_packet(raw_packet, ipv4, IPPROTO_SCTP, data);\n\n\tsctp = (struct sctphdr *)tmp;\n\tsctp->dest = data->dst_port;\n\tsctp->source = data->src_port;\n}\n\n \nstatic int i40e_prepare_fdir_filter(struct i40e_pf *pf,\n\t\t\t\t    struct i40e_fdir_filter *fd_data,\n\t\t\t\t    bool add, char *packet_addr,\n\t\t\t\t    int payload_offset, u8 pctype)\n{\n\tint ret;\n\n\tif (fd_data->flex_filter) {\n\t\tu8 *payload;\n\t\t__be16 pattern = fd_data->flex_word;\n\t\tu16 off = fd_data->flex_offset;\n\n\t\tpayload = packet_addr + payload_offset;\n\n\t\t \n\t\tif (!!fd_data->vlan_tag)\n\t\t\tpayload += VLAN_HLEN;\n\n\t\t*((__force __be16 *)(payload + off)) = pattern;\n\t}\n\n\tfd_data->pctype = pctype;\n\tret = i40e_program_fdir_filter(fd_data, packet_addr, pf, add);\n\tif (ret) {\n\t\tdev_info(&pf->pdev->dev,\n\t\t\t \"PCTYPE:%d, Filter command send failed for fd_id:%d (ret = %d)\\n\",\n\t\t\t fd_data->pctype, fd_data->fd_id, ret);\n\t\t \n\t\treturn -EOPNOTSUPP;\n\t} else if (I40E_DEBUG_FD & pf->hw.debug_mask) {\n\t\tif (add)\n\t\t\tdev_info(&pf->pdev->dev,\n\t\t\t\t \"Filter OK for PCTYPE %d loc = %d\\n\",\n\t\t\t\t fd_data->pctype, fd_data->fd_id);\n\t\telse\n\t\t\tdev_info(&pf->pdev->dev,\n\t\t\t\t \"Filter deleted for PCTYPE %d loc = %d\\n\",\n\t\t\t\t fd_data->pctype, fd_data->fd_id);\n\t}\n\n\treturn ret;\n}\n\n \nstatic void i40e_change_filter_num(bool ipv4, bool add, u16 *ipv4_filter_num,\n\t\t\t\t   u16 *ipv6_filter_num)\n{\n\tif (add) {\n\t\tif (ipv4)\n\t\t\t(*ipv4_filter_num)++;\n\t\telse\n\t\t\t(*ipv6_filter_num)++;\n\t} else {\n\t\tif (ipv4)\n\t\t\t(*ipv4_filter_num)--;\n\t\telse\n\t\t\t(*ipv6_filter_num)--;\n\t}\n}\n\n#define I40E_UDPIP_DUMMY_PACKET_LEN\t42\n#define I40E_UDPIP6_DUMMY_PACKET_LEN\t62\n \nstatic int i40e_add_del_fdir_udp(struct i40e_vsi *vsi,\n\t\t\t\t struct i40e_fdir_filter *fd_data,\n\t\t\t\t bool add,\n\t\t\t\t bool ipv4)\n{\n\tstruct i40e_pf *pf = vsi->back;\n\tu8 *raw_packet;\n\tint ret;\n\n\traw_packet = kzalloc(I40E_FDIR_MAX_RAW_PACKET_SIZE, GFP_KERNEL);\n\tif (!raw_packet)\n\t\treturn -ENOMEM;\n\n\ti40e_create_dummy_udp_packet(raw_packet, ipv4, IPPROTO_UDP, fd_data);\n\n\tif (ipv4)\n\t\tret = i40e_prepare_fdir_filter\n\t\t\t(pf, fd_data, add, raw_packet,\n\t\t\t I40E_UDPIP_DUMMY_PACKET_LEN,\n\t\t\t I40E_FILTER_PCTYPE_NONF_IPV4_UDP);\n\telse\n\t\tret = i40e_prepare_fdir_filter\n\t\t\t(pf, fd_data, add, raw_packet,\n\t\t\t I40E_UDPIP6_DUMMY_PACKET_LEN,\n\t\t\t I40E_FILTER_PCTYPE_NONF_IPV6_UDP);\n\n\tif (ret) {\n\t\tkfree(raw_packet);\n\t\treturn ret;\n\t}\n\n\ti40e_change_filter_num(ipv4, add, &pf->fd_udp4_filter_cnt,\n\t\t\t       &pf->fd_udp6_filter_cnt);\n\n\treturn 0;\n}\n\n#define I40E_TCPIP_DUMMY_PACKET_LEN\t54\n#define I40E_TCPIP6_DUMMY_PACKET_LEN\t74\n \nstatic int i40e_add_del_fdir_tcp(struct i40e_vsi *vsi,\n\t\t\t\t struct i40e_fdir_filter *fd_data,\n\t\t\t\t bool add,\n\t\t\t\t bool ipv4)\n{\n\tstruct i40e_pf *pf = vsi->back;\n\tu8 *raw_packet;\n\tint ret;\n\n\traw_packet = kzalloc(I40E_FDIR_MAX_RAW_PACKET_SIZE, GFP_KERNEL);\n\tif (!raw_packet)\n\t\treturn -ENOMEM;\n\n\ti40e_create_dummy_tcp_packet(raw_packet, ipv4, IPPROTO_TCP, fd_data);\n\tif (ipv4)\n\t\tret = i40e_prepare_fdir_filter\n\t\t\t(pf, fd_data, add, raw_packet,\n\t\t\t I40E_TCPIP_DUMMY_PACKET_LEN,\n\t\t\t I40E_FILTER_PCTYPE_NONF_IPV4_TCP);\n\telse\n\t\tret = i40e_prepare_fdir_filter\n\t\t\t(pf, fd_data, add, raw_packet,\n\t\t\t I40E_TCPIP6_DUMMY_PACKET_LEN,\n\t\t\t I40E_FILTER_PCTYPE_NONF_IPV6_TCP);\n\n\tif (ret) {\n\t\tkfree(raw_packet);\n\t\treturn ret;\n\t}\n\n\ti40e_change_filter_num(ipv4, add, &pf->fd_tcp4_filter_cnt,\n\t\t\t       &pf->fd_tcp6_filter_cnt);\n\n\tif (add) {\n\t\tif ((pf->flags & I40E_FLAG_FD_ATR_ENABLED) &&\n\t\t    I40E_DEBUG_FD & pf->hw.debug_mask)\n\t\t\tdev_info(&pf->pdev->dev, \"Forcing ATR off, sideband rules for TCP/IPv4 flow being applied\\n\");\n\t\tset_bit(__I40E_FD_ATR_AUTO_DISABLED, pf->state);\n\t}\n\treturn 0;\n}\n\n#define I40E_SCTPIP_DUMMY_PACKET_LEN\t46\n#define I40E_SCTPIP6_DUMMY_PACKET_LEN\t66\n \nstatic int i40e_add_del_fdir_sctp(struct i40e_vsi *vsi,\n\t\t\t\t  struct i40e_fdir_filter *fd_data,\n\t\t\t\t  bool add,\n\t\t\t\t  bool ipv4)\n{\n\tstruct i40e_pf *pf = vsi->back;\n\tu8 *raw_packet;\n\tint ret;\n\n\traw_packet = kzalloc(I40E_FDIR_MAX_RAW_PACKET_SIZE, GFP_KERNEL);\n\tif (!raw_packet)\n\t\treturn -ENOMEM;\n\n\ti40e_create_dummy_sctp_packet(raw_packet, ipv4, IPPROTO_SCTP, fd_data);\n\n\tif (ipv4)\n\t\tret = i40e_prepare_fdir_filter\n\t\t\t(pf, fd_data, add, raw_packet,\n\t\t\t I40E_SCTPIP_DUMMY_PACKET_LEN,\n\t\t\t I40E_FILTER_PCTYPE_NONF_IPV4_SCTP);\n\telse\n\t\tret = i40e_prepare_fdir_filter\n\t\t\t(pf, fd_data, add, raw_packet,\n\t\t\t I40E_SCTPIP6_DUMMY_PACKET_LEN,\n\t\t\t I40E_FILTER_PCTYPE_NONF_IPV6_SCTP);\n\n\tif (ret) {\n\t\tkfree(raw_packet);\n\t\treturn ret;\n\t}\n\n\ti40e_change_filter_num(ipv4, add, &pf->fd_sctp4_filter_cnt,\n\t\t\t       &pf->fd_sctp6_filter_cnt);\n\n\treturn 0;\n}\n\n#define I40E_IP_DUMMY_PACKET_LEN\t34\n#define I40E_IP6_DUMMY_PACKET_LEN\t54\n \nstatic int i40e_add_del_fdir_ip(struct i40e_vsi *vsi,\n\t\t\t\tstruct i40e_fdir_filter *fd_data,\n\t\t\t\tbool add,\n\t\t\t\tbool ipv4)\n{\n\tstruct i40e_pf *pf = vsi->back;\n\tint payload_offset;\n\tu8 *raw_packet;\n\tint iter_start;\n\tint iter_end;\n\tint ret;\n\tint i;\n\n\tif (ipv4) {\n\t\titer_start = I40E_FILTER_PCTYPE_NONF_IPV4_OTHER;\n\t\titer_end = I40E_FILTER_PCTYPE_FRAG_IPV4;\n\t} else {\n\t\titer_start = I40E_FILTER_PCTYPE_NONF_IPV6_OTHER;\n\t\titer_end = I40E_FILTER_PCTYPE_FRAG_IPV6;\n\t}\n\n\tfor (i = iter_start; i <= iter_end; i++) {\n\t\traw_packet = kzalloc(I40E_FDIR_MAX_RAW_PACKET_SIZE, GFP_KERNEL);\n\t\tif (!raw_packet)\n\t\t\treturn -ENOMEM;\n\n\t\t \n\t\t(void)i40e_create_dummy_packet\n\t\t\t(raw_packet, ipv4, (ipv4) ? IPPROTO_IP : IPPROTO_NONE,\n\t\t\t fd_data);\n\n\t\tpayload_offset = (ipv4) ? I40E_IP_DUMMY_PACKET_LEN :\n\t\t\tI40E_IP6_DUMMY_PACKET_LEN;\n\t\tret = i40e_prepare_fdir_filter(pf, fd_data, add, raw_packet,\n\t\t\t\t\t       payload_offset, i);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\ti40e_change_filter_num(ipv4, add, &pf->fd_ip4_filter_cnt,\n\t\t\t       &pf->fd_ip6_filter_cnt);\n\n\treturn 0;\nerr:\n\tkfree(raw_packet);\n\treturn ret;\n}\n\n \nint i40e_add_del_fdir(struct i40e_vsi *vsi,\n\t\t      struct i40e_fdir_filter *input, bool add)\n{\n\tenum ip_ver { ipv6 = 0, ipv4 = 1 };\n\tstruct i40e_pf *pf = vsi->back;\n\tint ret;\n\n\tswitch (input->flow_type & ~FLOW_EXT) {\n\tcase TCP_V4_FLOW:\n\t\tret = i40e_add_del_fdir_tcp(vsi, input, add, ipv4);\n\t\tbreak;\n\tcase UDP_V4_FLOW:\n\t\tret = i40e_add_del_fdir_udp(vsi, input, add, ipv4);\n\t\tbreak;\n\tcase SCTP_V4_FLOW:\n\t\tret = i40e_add_del_fdir_sctp(vsi, input, add, ipv4);\n\t\tbreak;\n\tcase TCP_V6_FLOW:\n\t\tret = i40e_add_del_fdir_tcp(vsi, input, add, ipv6);\n\t\tbreak;\n\tcase UDP_V6_FLOW:\n\t\tret = i40e_add_del_fdir_udp(vsi, input, add, ipv6);\n\t\tbreak;\n\tcase SCTP_V6_FLOW:\n\t\tret = i40e_add_del_fdir_sctp(vsi, input, add, ipv6);\n\t\tbreak;\n\tcase IP_USER_FLOW:\n\t\tswitch (input->ipl4_proto) {\n\t\tcase IPPROTO_TCP:\n\t\t\tret = i40e_add_del_fdir_tcp(vsi, input, add, ipv4);\n\t\t\tbreak;\n\t\tcase IPPROTO_UDP:\n\t\t\tret = i40e_add_del_fdir_udp(vsi, input, add, ipv4);\n\t\t\tbreak;\n\t\tcase IPPROTO_SCTP:\n\t\t\tret = i40e_add_del_fdir_sctp(vsi, input, add, ipv4);\n\t\t\tbreak;\n\t\tcase IPPROTO_IP:\n\t\t\tret = i40e_add_del_fdir_ip(vsi, input, add, ipv4);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tdev_info(&pf->pdev->dev, \"Unsupported IPv4 protocol 0x%02x\\n\",\n\t\t\t\t input->ipl4_proto);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase IPV6_USER_FLOW:\n\t\tswitch (input->ipl4_proto) {\n\t\tcase IPPROTO_TCP:\n\t\t\tret = i40e_add_del_fdir_tcp(vsi, input, add, ipv6);\n\t\t\tbreak;\n\t\tcase IPPROTO_UDP:\n\t\t\tret = i40e_add_del_fdir_udp(vsi, input, add, ipv6);\n\t\t\tbreak;\n\t\tcase IPPROTO_SCTP:\n\t\t\tret = i40e_add_del_fdir_sctp(vsi, input, add, ipv6);\n\t\t\tbreak;\n\t\tcase IPPROTO_IP:\n\t\t\tret = i40e_add_del_fdir_ip(vsi, input, add, ipv6);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tdev_info(&pf->pdev->dev, \"Unsupported IPv6 protocol 0x%02x\\n\",\n\t\t\t\t input->ipl4_proto);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdev_info(&pf->pdev->dev, \"Unsupported flow type 0x%02x\\n\",\n\t\t\t input->flow_type);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\treturn ret;\n}\n\n \nstatic void i40e_fd_handle_status(struct i40e_ring *rx_ring, u64 qword0_raw,\n\t\t\t\t  u64 qword1, u8 prog_id)\n{\n\tstruct i40e_pf *pf = rx_ring->vsi->back;\n\tstruct pci_dev *pdev = pf->pdev;\n\tstruct i40e_16b_rx_wb_qw0 *qw0;\n\tu32 fcnt_prog, fcnt_avail;\n\tu32 error;\n\n\tqw0 = (struct i40e_16b_rx_wb_qw0 *)&qword0_raw;\n\terror = (qword1 & I40E_RX_PROG_STATUS_DESC_QW1_ERROR_MASK) >>\n\t\tI40E_RX_PROG_STATUS_DESC_QW1_ERROR_SHIFT;\n\n\tif (error == BIT(I40E_RX_PROG_STATUS_DESC_FD_TBL_FULL_SHIFT)) {\n\t\tpf->fd_inv = le32_to_cpu(qw0->hi_dword.fd_id);\n\t\tif (qw0->hi_dword.fd_id != 0 ||\n\t\t    (I40E_DEBUG_FD & pf->hw.debug_mask))\n\t\t\tdev_warn(&pdev->dev, \"ntuple filter loc = %d, could not be added\\n\",\n\t\t\t\t pf->fd_inv);\n\n\t\t \n\t\tif (test_bit(__I40E_FD_FLUSH_REQUESTED, pf->state))\n\t\t\treturn;\n\n\t\tpf->fd_add_err++;\n\t\t \n\t\tpf->fd_atr_cnt = i40e_get_current_atr_cnt(pf);\n\n\t\tif (qw0->hi_dword.fd_id == 0 &&\n\t\t    test_bit(__I40E_FD_SB_AUTO_DISABLED, pf->state)) {\n\t\t\t \n\t\t\tset_bit(__I40E_FD_ATR_AUTO_DISABLED, pf->state);\n\t\t\tset_bit(__I40E_FD_FLUSH_REQUESTED, pf->state);\n\t\t}\n\n\t\t \n\t\tfcnt_prog = i40e_get_global_fd_count(pf);\n\t\tfcnt_avail = pf->fdir_pf_filter_count;\n\t\t \n\t\tif (fcnt_prog >= (fcnt_avail - I40E_FDIR_BUFFER_FULL_MARGIN)) {\n\t\t\tif ((pf->flags & I40E_FLAG_FD_SB_ENABLED) &&\n\t\t\t    !test_and_set_bit(__I40E_FD_SB_AUTO_DISABLED,\n\t\t\t\t\t      pf->state))\n\t\t\t\tif (I40E_DEBUG_FD & pf->hw.debug_mask)\n\t\t\t\t\tdev_warn(&pdev->dev, \"FD filter space full, new ntuple rules will not be added\\n\");\n\t\t}\n\t} else if (error == BIT(I40E_RX_PROG_STATUS_DESC_NO_FD_ENTRY_SHIFT)) {\n\t\tif (I40E_DEBUG_FD & pf->hw.debug_mask)\n\t\t\tdev_info(&pdev->dev, \"ntuple filter fd_id = %d, could not be removed\\n\",\n\t\t\t\t qw0->hi_dword.fd_id);\n\t}\n}\n\n \nstatic void i40e_unmap_and_free_tx_resource(struct i40e_ring *ring,\n\t\t\t\t\t    struct i40e_tx_buffer *tx_buffer)\n{\n\tif (tx_buffer->skb) {\n\t\tif (tx_buffer->tx_flags & I40E_TX_FLAGS_FD_SB)\n\t\t\tkfree(tx_buffer->raw_buf);\n\t\telse if (ring_is_xdp(ring))\n\t\t\txdp_return_frame(tx_buffer->xdpf);\n\t\telse\n\t\t\tdev_kfree_skb_any(tx_buffer->skb);\n\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\tdma_unmap_single(ring->dev,\n\t\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t} else if (dma_unmap_len(tx_buffer, len)) {\n\t\tdma_unmap_page(ring->dev,\n\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t       DMA_TO_DEVICE);\n\t}\n\n\ttx_buffer->next_to_watch = NULL;\n\ttx_buffer->skb = NULL;\n\tdma_unmap_len_set(tx_buffer, len, 0);\n\t \n}\n\n \nvoid i40e_clean_tx_ring(struct i40e_ring *tx_ring)\n{\n\tunsigned long bi_size;\n\tu16 i;\n\n\tif (ring_is_xdp(tx_ring) && tx_ring->xsk_pool) {\n\t\ti40e_xsk_clean_tx_ring(tx_ring);\n\t} else {\n\t\t \n\t\tif (!tx_ring->tx_bi)\n\t\t\treturn;\n\n\t\t \n\t\tfor (i = 0; i < tx_ring->count; i++)\n\t\t\ti40e_unmap_and_free_tx_resource(tx_ring,\n\t\t\t\t\t\t\t&tx_ring->tx_bi[i]);\n\t}\n\n\tbi_size = sizeof(struct i40e_tx_buffer) * tx_ring->count;\n\tmemset(tx_ring->tx_bi, 0, bi_size);\n\n\t \n\tmemset(tx_ring->desc, 0, tx_ring->size);\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\n\tif (!tx_ring->netdev)\n\t\treturn;\n\n\t \n\tnetdev_tx_reset_queue(txring_txq(tx_ring));\n}\n\n \nvoid i40e_free_tx_resources(struct i40e_ring *tx_ring)\n{\n\ti40e_clean_tx_ring(tx_ring);\n\tkfree(tx_ring->tx_bi);\n\ttx_ring->tx_bi = NULL;\n\n\tif (tx_ring->desc) {\n\t\tdma_free_coherent(tx_ring->dev, tx_ring->size,\n\t\t\t\t  tx_ring->desc, tx_ring->dma);\n\t\ttx_ring->desc = NULL;\n\t}\n}\n\n \nu32 i40e_get_tx_pending(struct i40e_ring *ring, bool in_sw)\n{\n\tu32 head, tail;\n\n\tif (!in_sw) {\n\t\thead = i40e_get_head(ring);\n\t\ttail = readl(ring->tail);\n\t} else {\n\t\thead = ring->next_to_clean;\n\t\ttail = ring->next_to_use;\n\t}\n\n\tif (head != tail)\n\t\treturn (head < tail) ?\n\t\t\ttail - head : (tail + ring->count - head);\n\n\treturn 0;\n}\n\n \nvoid i40e_detect_recover_hung(struct i40e_vsi *vsi)\n{\n\tstruct i40e_ring *tx_ring = NULL;\n\tstruct net_device *netdev;\n\tunsigned int i;\n\tint packets;\n\n\tif (!vsi)\n\t\treturn;\n\n\tif (test_bit(__I40E_VSI_DOWN, vsi->state))\n\t\treturn;\n\n\tnetdev = vsi->netdev;\n\tif (!netdev)\n\t\treturn;\n\n\tif (!netif_carrier_ok(netdev))\n\t\treturn;\n\n\tfor (i = 0; i < vsi->num_queue_pairs; i++) {\n\t\ttx_ring = vsi->tx_rings[i];\n\t\tif (tx_ring && tx_ring->desc) {\n\t\t\t \n\t\t\tpackets = tx_ring->stats.packets & INT_MAX;\n\t\t\tif (tx_ring->tx_stats.prev_pkt_ctr == packets) {\n\t\t\t\ti40e_force_wb(vsi, tx_ring->q_vector);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tsmp_rmb();\n\t\t\ttx_ring->tx_stats.prev_pkt_ctr =\n\t\t\t    i40e_get_tx_pending(tx_ring, true) ? packets : -1;\n\t\t}\n\t}\n}\n\n \nstatic bool i40e_clean_tx_irq(struct i40e_vsi *vsi,\n\t\t\t      struct i40e_ring *tx_ring, int napi_budget,\n\t\t\t      unsigned int *tx_cleaned)\n{\n\tint i = tx_ring->next_to_clean;\n\tstruct i40e_tx_buffer *tx_buf;\n\tstruct i40e_tx_desc *tx_head;\n\tstruct i40e_tx_desc *tx_desc;\n\tunsigned int total_bytes = 0, total_packets = 0;\n\tunsigned int budget = vsi->work_limit;\n\n\ttx_buf = &tx_ring->tx_bi[i];\n\ttx_desc = I40E_TX_DESC(tx_ring, i);\n\ti -= tx_ring->count;\n\n\ttx_head = I40E_TX_DESC(tx_ring, i40e_get_head(tx_ring));\n\n\tdo {\n\t\tstruct i40e_tx_desc *eop_desc = tx_buf->next_to_watch;\n\n\t\t \n\t\tif (!eop_desc)\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\ti40e_trace(clean_tx_irq, tx_ring, tx_desc, tx_buf);\n\t\t \n\t\tif (tx_head == tx_desc)\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buf->next_to_watch = NULL;\n\n\t\t \n\t\ttotal_bytes += tx_buf->bytecount;\n\t\ttotal_packets += tx_buf->gso_segs;\n\n\t\t \n\t\tif (ring_is_xdp(tx_ring))\n\t\t\txdp_return_frame(tx_buf->xdpf);\n\t\telse\n\t\t\tnapi_consume_skb(tx_buf->skb, napi_budget);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buf, dma),\n\t\t\t\t dma_unmap_len(tx_buf, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\ttx_buf->skb = NULL;\n\t\tdma_unmap_len_set(tx_buf, len, 0);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ti40e_trace(clean_tx_irq_unmap,\n\t\t\t\t   tx_ring, tx_desc, tx_buf);\n\n\t\t\ttx_buf++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(!i)) {\n\t\t\t\ti -= tx_ring->count;\n\t\t\t\ttx_buf = tx_ring->tx_bi;\n\t\t\t\ttx_desc = I40E_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buf, len)) {\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buf, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buf, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\t\tdma_unmap_len_set(tx_buf, len, 0);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\ttx_buf++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buf = tx_ring->tx_bi;\n\t\t\ttx_desc = I40E_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\tprefetch(tx_desc);\n\n\t\t \n\t\tbudget--;\n\t} while (likely(budget));\n\n\ti += tx_ring->count;\n\ttx_ring->next_to_clean = i;\n\ti40e_update_tx_stats(tx_ring, total_packets, total_bytes);\n\ti40e_arm_wb(tx_ring, vsi, budget);\n\n\tif (ring_is_xdp(tx_ring))\n\t\treturn !!budget;\n\n\t \n\tnetdev_tx_completed_queue(txring_txq(tx_ring),\n\t\t\t\t  total_packets, total_bytes);\n\n#define TX_WAKE_THRESHOLD ((s16)(DESC_NEEDED * 2))\n\tif (unlikely(total_packets && netif_carrier_ok(tx_ring->netdev) &&\n\t\t     (I40E_DESC_UNUSED(tx_ring) >= TX_WAKE_THRESHOLD))) {\n\t\t \n\t\tsmp_mb();\n\t\tif (__netif_subqueue_stopped(tx_ring->netdev,\n\t\t\t\t\t     tx_ring->queue_index) &&\n\t\t   !test_bit(__I40E_VSI_DOWN, vsi->state)) {\n\t\t\tnetif_wake_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\t\t\t++tx_ring->tx_stats.restart_queue;\n\t\t}\n\t}\n\n\t*tx_cleaned = total_packets;\n\treturn !!budget;\n}\n\n \nstatic void i40e_enable_wb_on_itr(struct i40e_vsi *vsi,\n\t\t\t\t  struct i40e_q_vector *q_vector)\n{\n\tu16 flags = q_vector->tx.ring[0].flags;\n\tu32 val;\n\n\tif (!(flags & I40E_TXR_FLAGS_WB_ON_ITR))\n\t\treturn;\n\n\tif (q_vector->arm_wb_state)\n\t\treturn;\n\n\tif (vsi->back->flags & I40E_FLAG_MSIX_ENABLED) {\n\t\tval = I40E_PFINT_DYN_CTLN_WB_ON_ITR_MASK |\n\t\t      I40E_PFINT_DYN_CTLN_ITR_INDX_MASK;  \n\n\t\twr32(&vsi->back->hw,\n\t\t     I40E_PFINT_DYN_CTLN(q_vector->reg_idx),\n\t\t     val);\n\t} else {\n\t\tval = I40E_PFINT_DYN_CTL0_WB_ON_ITR_MASK |\n\t\t      I40E_PFINT_DYN_CTL0_ITR_INDX_MASK;  \n\n\t\twr32(&vsi->back->hw, I40E_PFINT_DYN_CTL0, val);\n\t}\n\tq_vector->arm_wb_state = true;\n}\n\n \nvoid i40e_force_wb(struct i40e_vsi *vsi, struct i40e_q_vector *q_vector)\n{\n\tif (vsi->back->flags & I40E_FLAG_MSIX_ENABLED) {\n\t\tu32 val = I40E_PFINT_DYN_CTLN_INTENA_MASK |\n\t\t\t  I40E_PFINT_DYN_CTLN_ITR_INDX_MASK |  \n\t\t\t  I40E_PFINT_DYN_CTLN_SWINT_TRIG_MASK |\n\t\t\t  I40E_PFINT_DYN_CTLN_SW_ITR_INDX_ENA_MASK;\n\t\t\t   \n\n\t\twr32(&vsi->back->hw,\n\t\t     I40E_PFINT_DYN_CTLN(q_vector->reg_idx), val);\n\t} else {\n\t\tu32 val = I40E_PFINT_DYN_CTL0_INTENA_MASK |\n\t\t\t  I40E_PFINT_DYN_CTL0_ITR_INDX_MASK |  \n\t\t\t  I40E_PFINT_DYN_CTL0_SWINT_TRIG_MASK |\n\t\t\t  I40E_PFINT_DYN_CTL0_SW_ITR_INDX_ENA_MASK;\n\t\t\t \n\n\t\twr32(&vsi->back->hw, I40E_PFINT_DYN_CTL0, val);\n\t}\n}\n\nstatic inline bool i40e_container_is_rx(struct i40e_q_vector *q_vector,\n\t\t\t\t\tstruct i40e_ring_container *rc)\n{\n\treturn &q_vector->rx == rc;\n}\n\nstatic inline unsigned int i40e_itr_divisor(struct i40e_q_vector *q_vector)\n{\n\tunsigned int divisor;\n\n\tswitch (q_vector->vsi->back->hw.phy.link_info.link_speed) {\n\tcase I40E_LINK_SPEED_40GB:\n\t\tdivisor = I40E_ITR_ADAPTIVE_MIN_INC * 1024;\n\t\tbreak;\n\tcase I40E_LINK_SPEED_25GB:\n\tcase I40E_LINK_SPEED_20GB:\n\t\tdivisor = I40E_ITR_ADAPTIVE_MIN_INC * 512;\n\t\tbreak;\n\tdefault:\n\tcase I40E_LINK_SPEED_10GB:\n\t\tdivisor = I40E_ITR_ADAPTIVE_MIN_INC * 256;\n\t\tbreak;\n\tcase I40E_LINK_SPEED_1GB:\n\tcase I40E_LINK_SPEED_100MB:\n\t\tdivisor = I40E_ITR_ADAPTIVE_MIN_INC * 32;\n\t\tbreak;\n\t}\n\n\treturn divisor;\n}\n\n \nstatic void i40e_update_itr(struct i40e_q_vector *q_vector,\n\t\t\t    struct i40e_ring_container *rc)\n{\n\tunsigned int avg_wire_size, packets, bytes, itr;\n\tunsigned long next_update = jiffies;\n\n\t \n\tif (!rc->ring || !ITR_IS_DYNAMIC(rc->ring->itr_setting))\n\t\treturn;\n\n\t \n\titr = i40e_container_is_rx(q_vector, rc) ?\n\t      I40E_ITR_ADAPTIVE_MIN_USECS | I40E_ITR_ADAPTIVE_LATENCY :\n\t      I40E_ITR_ADAPTIVE_MAX_USECS | I40E_ITR_ADAPTIVE_LATENCY;\n\n\t \n\tif (time_after(next_update, rc->next_update))\n\t\tgoto clear_counts;\n\n\t \n\tif (q_vector->itr_countdown) {\n\t\titr = rc->target_itr;\n\t\tgoto clear_counts;\n\t}\n\n\tpackets = rc->total_packets;\n\tbytes = rc->total_bytes;\n\n\tif (i40e_container_is_rx(q_vector, rc)) {\n\t\t \n\t\tif (packets && packets < 4 && bytes < 9000 &&\n\t\t    (q_vector->tx.target_itr & I40E_ITR_ADAPTIVE_LATENCY)) {\n\t\t\titr = I40E_ITR_ADAPTIVE_LATENCY;\n\t\t\tgoto adjust_by_size;\n\t\t}\n\t} else if (packets < 4) {\n\t\t \n\t\tif (rc->target_itr == I40E_ITR_ADAPTIVE_MAX_USECS &&\n\t\t    (q_vector->rx.target_itr & I40E_ITR_MASK) ==\n\t\t     I40E_ITR_ADAPTIVE_MAX_USECS)\n\t\t\tgoto clear_counts;\n\t} else if (packets > 32) {\n\t\t \n\t\trc->target_itr &= ~I40E_ITR_ADAPTIVE_LATENCY;\n\t}\n\n\t \n\tif (packets < 56) {\n\t\titr = rc->target_itr + I40E_ITR_ADAPTIVE_MIN_INC;\n\t\tif ((itr & I40E_ITR_MASK) > I40E_ITR_ADAPTIVE_MAX_USECS) {\n\t\t\titr &= I40E_ITR_ADAPTIVE_LATENCY;\n\t\t\titr += I40E_ITR_ADAPTIVE_MAX_USECS;\n\t\t}\n\t\tgoto clear_counts;\n\t}\n\n\tif (packets <= 256) {\n\t\titr = min(q_vector->tx.current_itr, q_vector->rx.current_itr);\n\t\titr &= I40E_ITR_MASK;\n\n\t\t \n\t\tif (packets <= 112)\n\t\t\tgoto clear_counts;\n\n\t\t \n\t\titr /= 2;\n\t\titr &= I40E_ITR_MASK;\n\t\tif (itr < I40E_ITR_ADAPTIVE_MIN_USECS)\n\t\t\titr = I40E_ITR_ADAPTIVE_MIN_USECS;\n\n\t\tgoto clear_counts;\n\t}\n\n\t \n\titr = I40E_ITR_ADAPTIVE_BULK;\n\nadjust_by_size:\n\t \n\tavg_wire_size = bytes / packets;\n\n\t \n\tif (avg_wire_size <= 60) {\n\t\t \n\t\tavg_wire_size = 4096;\n\t} else if (avg_wire_size <= 380) {\n\t\t \n\t\tavg_wire_size *= 40;\n\t\tavg_wire_size += 1696;\n\t} else if (avg_wire_size <= 1084) {\n\t\t \n\t\tavg_wire_size *= 15;\n\t\tavg_wire_size += 11452;\n\t} else if (avg_wire_size <= 1980) {\n\t\t \n\t\tavg_wire_size *= 5;\n\t\tavg_wire_size += 22420;\n\t} else {\n\t\t \n\t\tavg_wire_size = 32256;\n\t}\n\n\t \n\tif (itr & I40E_ITR_ADAPTIVE_LATENCY)\n\t\tavg_wire_size /= 2;\n\n\t \n\titr += DIV_ROUND_UP(avg_wire_size, i40e_itr_divisor(q_vector)) *\n\t       I40E_ITR_ADAPTIVE_MIN_INC;\n\n\tif ((itr & I40E_ITR_MASK) > I40E_ITR_ADAPTIVE_MAX_USECS) {\n\t\titr &= I40E_ITR_ADAPTIVE_LATENCY;\n\t\titr += I40E_ITR_ADAPTIVE_MAX_USECS;\n\t}\n\nclear_counts:\n\t \n\trc->target_itr = itr;\n\n\t \n\trc->next_update = next_update + 1;\n\n\trc->total_bytes = 0;\n\trc->total_packets = 0;\n}\n\nstatic struct i40e_rx_buffer *i40e_rx_bi(struct i40e_ring *rx_ring, u32 idx)\n{\n\treturn &rx_ring->rx_bi[idx];\n}\n\n \nstatic void i40e_reuse_rx_page(struct i40e_ring *rx_ring,\n\t\t\t       struct i40e_rx_buffer *old_buff)\n{\n\tstruct i40e_rx_buffer *new_buff;\n\tu16 nta = rx_ring->next_to_alloc;\n\n\tnew_buff = i40e_rx_bi(rx_ring, nta);\n\n\t \n\tnta++;\n\trx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;\n\n\t \n\tnew_buff->dma\t\t= old_buff->dma;\n\tnew_buff->page\t\t= old_buff->page;\n\tnew_buff->page_offset\t= old_buff->page_offset;\n\tnew_buff->pagecnt_bias\t= old_buff->pagecnt_bias;\n\n\t \n\told_buff->page = NULL;\n}\n\n \nvoid i40e_clean_programming_status(struct i40e_ring *rx_ring, u64 qword0_raw,\n\t\t\t\t   u64 qword1)\n{\n\tu8 id;\n\n\tid = (qword1 & I40E_RX_PROG_STATUS_DESC_QW1_PROGID_MASK) >>\n\t\t  I40E_RX_PROG_STATUS_DESC_QW1_PROGID_SHIFT;\n\n\tif (id == I40E_RX_PROG_STATUS_DESC_FD_FILTER_STATUS)\n\t\ti40e_fd_handle_status(rx_ring, qword0_raw, qword1, id);\n}\n\n \nint i40e_setup_tx_descriptors(struct i40e_ring *tx_ring)\n{\n\tstruct device *dev = tx_ring->dev;\n\tint bi_size;\n\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\t \n\tWARN_ON(tx_ring->tx_bi);\n\tbi_size = sizeof(struct i40e_tx_buffer) * tx_ring->count;\n\ttx_ring->tx_bi = kzalloc(bi_size, GFP_KERNEL);\n\tif (!tx_ring->tx_bi)\n\t\tgoto err;\n\n\tu64_stats_init(&tx_ring->syncp);\n\n\t \n\ttx_ring->size = tx_ring->count * sizeof(struct i40e_tx_desc);\n\t \n\ttx_ring->size += sizeof(u32);\n\ttx_ring->size = ALIGN(tx_ring->size, 4096);\n\ttx_ring->desc = dma_alloc_coherent(dev, tx_ring->size,\n\t\t\t\t\t   &tx_ring->dma, GFP_KERNEL);\n\tif (!tx_ring->desc) {\n\t\tdev_info(dev, \"Unable to allocate memory for the Tx descriptor ring, size=%d\\n\",\n\t\t\t tx_ring->size);\n\t\tgoto err;\n\t}\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\ttx_ring->tx_stats.prev_pkt_ctr = -1;\n\treturn 0;\n\nerr:\n\tkfree(tx_ring->tx_bi);\n\ttx_ring->tx_bi = NULL;\n\treturn -ENOMEM;\n}\n\nstatic void i40e_clear_rx_bi(struct i40e_ring *rx_ring)\n{\n\tmemset(rx_ring->rx_bi, 0, sizeof(*rx_ring->rx_bi) * rx_ring->count);\n}\n\n \nvoid i40e_clean_rx_ring(struct i40e_ring *rx_ring)\n{\n\tu16 i;\n\n\t \n\tif (!rx_ring->rx_bi)\n\t\treturn;\n\n\tif (rx_ring->xsk_pool) {\n\t\ti40e_xsk_clean_rx_ring(rx_ring);\n\t\tgoto skip_free;\n\t}\n\n\t \n\tfor (i = 0; i < rx_ring->count; i++) {\n\t\tstruct i40e_rx_buffer *rx_bi = i40e_rx_bi(rx_ring, i);\n\n\t\tif (!rx_bi->page)\n\t\t\tcontinue;\n\n\t\t \n\t\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t\t      rx_bi->dma,\n\t\t\t\t\t      rx_bi->page_offset,\n\t\t\t\t\t      rx_ring->rx_buf_len,\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev, rx_bi->dma,\n\t\t\t\t     i40e_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t     I40E_RX_DMA_ATTR);\n\n\t\t__page_frag_cache_drain(rx_bi->page, rx_bi->pagecnt_bias);\n\n\t\trx_bi->page = NULL;\n\t\trx_bi->page_offset = 0;\n\t}\n\nskip_free:\n\tif (rx_ring->xsk_pool)\n\t\ti40e_clear_rx_bi_zc(rx_ring);\n\telse\n\t\ti40e_clear_rx_bi(rx_ring);\n\n\t \n\tmemset(rx_ring->desc, 0, rx_ring->size);\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_process = 0;\n\trx_ring->next_to_use = 0;\n}\n\n \nvoid i40e_free_rx_resources(struct i40e_ring *rx_ring)\n{\n\ti40e_clean_rx_ring(rx_ring);\n\tif (rx_ring->vsi->type == I40E_VSI_MAIN)\n\t\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\trx_ring->xdp_prog = NULL;\n\tkfree(rx_ring->rx_bi);\n\trx_ring->rx_bi = NULL;\n\n\tif (rx_ring->desc) {\n\t\tdma_free_coherent(rx_ring->dev, rx_ring->size,\n\t\t\t\t  rx_ring->desc, rx_ring->dma);\n\t\trx_ring->desc = NULL;\n\t}\n}\n\n \nint i40e_setup_rx_descriptors(struct i40e_ring *rx_ring)\n{\n\tstruct device *dev = rx_ring->dev;\n\tint err;\n\n\tu64_stats_init(&rx_ring->syncp);\n\n\t \n\trx_ring->size = rx_ring->count * sizeof(union i40e_rx_desc);\n\trx_ring->size = ALIGN(rx_ring->size, 4096);\n\trx_ring->desc = dma_alloc_coherent(dev, rx_ring->size,\n\t\t\t\t\t   &rx_ring->dma, GFP_KERNEL);\n\n\tif (!rx_ring->desc) {\n\t\tdev_info(dev, \"Unable to allocate memory for the Rx descriptor ring, size=%d\\n\",\n\t\t\t rx_ring->size);\n\t\treturn -ENOMEM;\n\t}\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_process = 0;\n\trx_ring->next_to_use = 0;\n\n\t \n\tif (rx_ring->vsi->type == I40E_VSI_MAIN) {\n\t\terr = xdp_rxq_info_reg(&rx_ring->xdp_rxq, rx_ring->netdev,\n\t\t\t\t       rx_ring->queue_index, rx_ring->q_vector->napi.napi_id);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\trx_ring->xdp_prog = rx_ring->vsi->xdp_prog;\n\n\trx_ring->rx_bi =\n\t\tkcalloc(rx_ring->count, sizeof(*rx_ring->rx_bi), GFP_KERNEL);\n\tif (!rx_ring->rx_bi)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nvoid i40e_release_rx_desc(struct i40e_ring *rx_ring, u32 val)\n{\n\trx_ring->next_to_use = val;\n\n\t \n\trx_ring->next_to_alloc = val;\n\n\t \n\twmb();\n\twritel(val, rx_ring->tail);\n}\n\n#if (PAGE_SIZE >= 8192)\nstatic unsigned int i40e_rx_frame_truesize(struct i40e_ring *rx_ring,\n\t\t\t\t\t   unsigned int size)\n{\n\tunsigned int truesize;\n\n\ttruesize = rx_ring->rx_offset ?\n\t\tSKB_DATA_ALIGN(size + rx_ring->rx_offset) +\n\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info)) :\n\t\tSKB_DATA_ALIGN(size);\n\treturn truesize;\n}\n#endif\n\n \nstatic bool i40e_alloc_mapped_page(struct i40e_ring *rx_ring,\n\t\t\t\t   struct i40e_rx_buffer *bi)\n{\n\tstruct page *page = bi->page;\n\tdma_addr_t dma;\n\n\t \n\tif (likely(page)) {\n\t\trx_ring->rx_stats.page_reuse_count++;\n\t\treturn true;\n\t}\n\n\t \n\tpage = dev_alloc_pages(i40e_rx_pg_order(rx_ring));\n\tif (unlikely(!page)) {\n\t\trx_ring->rx_stats.alloc_page_failed++;\n\t\treturn false;\n\t}\n\n\trx_ring->rx_stats.page_alloc_count++;\n\n\t \n\tdma = dma_map_page_attrs(rx_ring->dev, page, 0,\n\t\t\t\t i40e_rx_pg_size(rx_ring),\n\t\t\t\t DMA_FROM_DEVICE,\n\t\t\t\t I40E_RX_DMA_ATTR);\n\n\t \n\tif (dma_mapping_error(rx_ring->dev, dma)) {\n\t\t__free_pages(page, i40e_rx_pg_order(rx_ring));\n\t\trx_ring->rx_stats.alloc_page_failed++;\n\t\treturn false;\n\t}\n\n\tbi->dma = dma;\n\tbi->page = page;\n\tbi->page_offset = rx_ring->rx_offset;\n\tpage_ref_add(page, USHRT_MAX - 1);\n\tbi->pagecnt_bias = USHRT_MAX;\n\n\treturn true;\n}\n\n \nbool i40e_alloc_rx_buffers(struct i40e_ring *rx_ring, u16 cleaned_count)\n{\n\tu16 ntu = rx_ring->next_to_use;\n\tunion i40e_rx_desc *rx_desc;\n\tstruct i40e_rx_buffer *bi;\n\n\t \n\tif (!rx_ring->netdev || !cleaned_count)\n\t\treturn false;\n\n\trx_desc = I40E_RX_DESC(rx_ring, ntu);\n\tbi = i40e_rx_bi(rx_ring, ntu);\n\n\tdo {\n\t\tif (!i40e_alloc_mapped_page(rx_ring, bi))\n\t\t\tgoto no_buffers;\n\n\t\t \n\t\tdma_sync_single_range_for_device(rx_ring->dev, bi->dma,\n\t\t\t\t\t\t bi->page_offset,\n\t\t\t\t\t\t rx_ring->rx_buf_len,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t \n\t\trx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);\n\n\t\trx_desc++;\n\t\tbi++;\n\t\tntu++;\n\t\tif (unlikely(ntu == rx_ring->count)) {\n\t\t\trx_desc = I40E_RX_DESC(rx_ring, 0);\n\t\t\tbi = i40e_rx_bi(rx_ring, 0);\n\t\t\tntu = 0;\n\t\t}\n\n\t\t \n\t\trx_desc->wb.qword1.status_error_len = 0;\n\n\t\tcleaned_count--;\n\t} while (cleaned_count);\n\n\tif (rx_ring->next_to_use != ntu)\n\t\ti40e_release_rx_desc(rx_ring, ntu);\n\n\treturn false;\n\nno_buffers:\n\tif (rx_ring->next_to_use != ntu)\n\t\ti40e_release_rx_desc(rx_ring, ntu);\n\n\t \n\treturn true;\n}\n\n \nstatic inline void i40e_rx_checksum(struct i40e_vsi *vsi,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    union i40e_rx_desc *rx_desc)\n{\n\tstruct i40e_rx_ptype_decoded decoded;\n\tu32 rx_error, rx_status;\n\tbool ipv4, ipv6;\n\tu8 ptype;\n\tu64 qword;\n\n\tqword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);\n\tptype = (qword & I40E_RXD_QW1_PTYPE_MASK) >> I40E_RXD_QW1_PTYPE_SHIFT;\n\trx_error = (qword & I40E_RXD_QW1_ERROR_MASK) >>\n\t\t   I40E_RXD_QW1_ERROR_SHIFT;\n\trx_status = (qword & I40E_RXD_QW1_STATUS_MASK) >>\n\t\t    I40E_RXD_QW1_STATUS_SHIFT;\n\tdecoded = decode_rx_desc_ptype(ptype);\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tskb_checksum_none_assert(skb);\n\n\t \n\tif (!(vsi->netdev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\t \n\tif (!(rx_status & BIT(I40E_RX_DESC_STATUS_L3L4P_SHIFT)))\n\t\treturn;\n\n\t \n\tif (!(decoded.known && decoded.outer_ip))\n\t\treturn;\n\n\tipv4 = (decoded.outer_ip == I40E_RX_PTYPE_OUTER_IP) &&\n\t       (decoded.outer_ip_ver == I40E_RX_PTYPE_OUTER_IPV4);\n\tipv6 = (decoded.outer_ip == I40E_RX_PTYPE_OUTER_IP) &&\n\t       (decoded.outer_ip_ver == I40E_RX_PTYPE_OUTER_IPV6);\n\n\tif (ipv4 &&\n\t    (rx_error & (BIT(I40E_RX_DESC_ERROR_IPE_SHIFT) |\n\t\t\t BIT(I40E_RX_DESC_ERROR_EIPE_SHIFT))))\n\t\tgoto checksum_fail;\n\n\t \n\tif (ipv6 &&\n\t    rx_status & BIT(I40E_RX_DESC_STATUS_IPV6EXADD_SHIFT))\n\t\t \n\t\treturn;\n\n\t \n\tif (rx_error & BIT(I40E_RX_DESC_ERROR_L4E_SHIFT))\n\t\tgoto checksum_fail;\n\n\t \n\tif (rx_error & BIT(I40E_RX_DESC_ERROR_PPRS_SHIFT))\n\t\treturn;\n\n\t \n\tif (decoded.tunnel_type >= I40E_RX_PTYPE_TUNNEL_IP_GRENAT)\n\t\tskb->csum_level = 1;\n\n\t \n\tswitch (decoded.inner_prot) {\n\tcase I40E_RX_PTYPE_INNER_PROT_TCP:\n\tcase I40E_RX_PTYPE_INNER_PROT_UDP:\n\tcase I40E_RX_PTYPE_INNER_PROT_SCTP:\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tfallthrough;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn;\n\nchecksum_fail:\n\tvsi->back->hw_csum_rx_error++;\n}\n\n \nstatic inline int i40e_ptype_to_htype(u8 ptype)\n{\n\tstruct i40e_rx_ptype_decoded decoded = decode_rx_desc_ptype(ptype);\n\n\tif (!decoded.known)\n\t\treturn PKT_HASH_TYPE_NONE;\n\n\tif (decoded.outer_ip == I40E_RX_PTYPE_OUTER_IP &&\n\t    decoded.payload_layer == I40E_RX_PTYPE_PAYLOAD_LAYER_PAY4)\n\t\treturn PKT_HASH_TYPE_L4;\n\telse if (decoded.outer_ip == I40E_RX_PTYPE_OUTER_IP &&\n\t\t decoded.payload_layer == I40E_RX_PTYPE_PAYLOAD_LAYER_PAY3)\n\t\treturn PKT_HASH_TYPE_L3;\n\telse\n\t\treturn PKT_HASH_TYPE_L2;\n}\n\n \nstatic inline void i40e_rx_hash(struct i40e_ring *ring,\n\t\t\t\tunion i40e_rx_desc *rx_desc,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tu8 rx_ptype)\n{\n\tu32 hash;\n\tconst __le64 rss_mask =\n\t\tcpu_to_le64((u64)I40E_RX_DESC_FLTSTAT_RSS_HASH <<\n\t\t\t    I40E_RX_DESC_STATUS_FLTSTAT_SHIFT);\n\n\tif (!(ring->netdev->features & NETIF_F_RXHASH))\n\t\treturn;\n\n\tif ((rx_desc->wb.qword1.status_error_len & rss_mask) == rss_mask) {\n\t\thash = le32_to_cpu(rx_desc->wb.qword0.hi_dword.rss);\n\t\tskb_set_hash(skb, hash, i40e_ptype_to_htype(rx_ptype));\n\t}\n}\n\n \nvoid i40e_process_skb_fields(struct i40e_ring *rx_ring,\n\t\t\t     union i40e_rx_desc *rx_desc, struct sk_buff *skb)\n{\n\tu64 qword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);\n\tu32 rx_status = (qword & I40E_RXD_QW1_STATUS_MASK) >>\n\t\t\tI40E_RXD_QW1_STATUS_SHIFT;\n\tu32 tsynvalid = rx_status & I40E_RXD_QW1_STATUS_TSYNVALID_MASK;\n\tu32 tsyn = (rx_status & I40E_RXD_QW1_STATUS_TSYNINDX_MASK) >>\n\t\t   I40E_RXD_QW1_STATUS_TSYNINDX_SHIFT;\n\tu8 rx_ptype = (qword & I40E_RXD_QW1_PTYPE_MASK) >>\n\t\t      I40E_RXD_QW1_PTYPE_SHIFT;\n\n\tif (unlikely(tsynvalid))\n\t\ti40e_ptp_rx_hwtstamp(rx_ring->vsi->back, skb, tsyn);\n\n\ti40e_rx_hash(rx_ring, rx_desc, skb, rx_ptype);\n\n\ti40e_rx_checksum(rx_ring->vsi, skb, rx_desc);\n\n\tskb_record_rx_queue(skb, rx_ring->queue_index);\n\n\tif (qword & BIT(I40E_RX_DESC_STATUS_L2TAG1P_SHIFT)) {\n\t\t__le16 vlan_tag = rx_desc->wb.qword0.lo_dword.l2tag1;\n\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t       le16_to_cpu(vlan_tag));\n\t}\n\n\t \n\tskb->protocol = eth_type_trans(skb, rx_ring->netdev);\n}\n\n \nstatic bool i40e_cleanup_headers(struct i40e_ring *rx_ring, struct sk_buff *skb,\n\t\t\t\t union i40e_rx_desc *rx_desc)\n\n{\n\t \n\tif (unlikely(i40e_test_staterr(rx_desc,\n\t\t\t\t       BIT(I40E_RXD_QW1_ERROR_SHIFT)))) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn true;\n\t}\n\n\t \n\tif (eth_skb_pad(skb))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool i40e_can_reuse_rx_page(struct i40e_rx_buffer *rx_buffer,\n\t\t\t\t   struct i40e_rx_queue_stats *rx_stats)\n{\n\tunsigned int pagecnt_bias = rx_buffer->pagecnt_bias;\n\tstruct page *page = rx_buffer->page;\n\n\t \n\tif (!dev_page_is_reusable(page)) {\n\t\trx_stats->page_waive_count++;\n\t\treturn false;\n\t}\n\n#if (PAGE_SIZE < 8192)\n\t \n\tif (unlikely((rx_buffer->page_count - pagecnt_bias) > 1)) {\n\t\trx_stats->page_busy_count++;\n\t\treturn false;\n\t}\n#else\n#define I40E_LAST_OFFSET \\\n\t(SKB_WITH_OVERHEAD(PAGE_SIZE) - I40E_RXBUFFER_2048)\n\tif (rx_buffer->page_offset > I40E_LAST_OFFSET) {\n\t\trx_stats->page_busy_count++;\n\t\treturn false;\n\t}\n#endif\n\n\t \n\tif (unlikely(pagecnt_bias == 1)) {\n\t\tpage_ref_add(page, USHRT_MAX - 1);\n\t\trx_buffer->pagecnt_bias = USHRT_MAX;\n\t}\n\n\treturn true;\n}\n\n \nstatic void i40e_rx_buffer_flip(struct i40e_rx_buffer *rx_buffer,\n\t\t\t\tunsigned int truesize)\n{\n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n}\n\n \nstatic struct i40e_rx_buffer *i40e_get_rx_buffer(struct i40e_ring *rx_ring,\n\t\t\t\t\t\t const unsigned int size)\n{\n\tstruct i40e_rx_buffer *rx_buffer;\n\n\trx_buffer = i40e_rx_bi(rx_ring, rx_ring->next_to_process);\n\trx_buffer->page_count =\n#if (PAGE_SIZE < 8192)\n\t\tpage_count(rx_buffer->page);\n#else\n\t\t0;\n#endif\n\tprefetch_page_address(rx_buffer->page);\n\n\t \n\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t      rx_buffer->dma,\n\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t      size,\n\t\t\t\t      DMA_FROM_DEVICE);\n\n\t \n\trx_buffer->pagecnt_bias--;\n\n\treturn rx_buffer;\n}\n\n \nstatic void i40e_put_rx_buffer(struct i40e_ring *rx_ring,\n\t\t\t       struct i40e_rx_buffer *rx_buffer)\n{\n\tif (i40e_can_reuse_rx_page(rx_buffer, &rx_ring->rx_stats)) {\n\t\t \n\t\ti40e_reuse_rx_page(rx_ring, rx_buffer);\n\t} else {\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,\n\t\t\t\t     i40e_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE, I40E_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(rx_buffer->page,\n\t\t\t\t\trx_buffer->pagecnt_bias);\n\t\t \n\t\trx_buffer->page = NULL;\n\t}\n}\n\n \nstatic void i40e_process_rx_buffs(struct i40e_ring *rx_ring, int xdp_res,\n\t\t\t\t  struct xdp_buff *xdp)\n{\n\tu32 next = rx_ring->next_to_clean;\n\tstruct i40e_rx_buffer *rx_buffer;\n\n\txdp->flags = 0;\n\n\twhile (1) {\n\t\trx_buffer = i40e_rx_bi(rx_ring, next);\n\t\tif (++next == rx_ring->count)\n\t\t\tnext = 0;\n\n\t\tif (!rx_buffer->page)\n\t\t\tcontinue;\n\n\t\tif (xdp_res == I40E_XDP_CONSUMED)\n\t\t\trx_buffer->pagecnt_bias++;\n\t\telse\n\t\t\ti40e_rx_buffer_flip(rx_buffer, xdp->frame_sz);\n\n\t\t \n\t\tif (next == rx_ring->next_to_process)\n\t\t\treturn;\n\n\t\ti40e_put_rx_buffer(rx_ring, rx_buffer);\n\t}\n}\n\n \nstatic struct sk_buff *i40e_construct_skb(struct i40e_ring *rx_ring,\n\t\t\t\t\t  struct xdp_buff *xdp,\n\t\t\t\t\t  u32 nr_frags)\n{\n\tunsigned int size = xdp->data_end - xdp->data;\n\tstruct i40e_rx_buffer *rx_buffer;\n\tunsigned int headlen;\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data);\n\n\t \n\n\t \n\tskb = __napi_alloc_skb(&rx_ring->q_vector->napi,\n\t\t\t       I40E_RX_HDR_SIZE,\n\t\t\t       GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\theadlen = size;\n\tif (headlen > I40E_RX_HDR_SIZE)\n\t\theadlen = eth_get_headlen(skb->dev, xdp->data,\n\t\t\t\t\t  I40E_RX_HDR_SIZE);\n\n\t \n\tmemcpy(__skb_put(skb, headlen), xdp->data,\n\t       ALIGN(headlen, sizeof(long)));\n\n\trx_buffer = i40e_rx_bi(rx_ring, rx_ring->next_to_clean);\n\t \n\tsize -= headlen;\n\tif (size) {\n\t\tif (unlikely(nr_frags >= MAX_SKB_FRAGS)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NULL;\n\t\t}\n\t\tskb_add_rx_frag(skb, 0, rx_buffer->page,\n\t\t\t\trx_buffer->page_offset + headlen,\n\t\t\t\tsize, xdp->frame_sz);\n\t\t \n\t\ti40e_rx_buffer_flip(rx_buffer, xdp->frame_sz);\n\t} else {\n\t\t \n\t\trx_buffer->pagecnt_bias++;\n\t}\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\tstruct skb_shared_info *sinfo, *skinfo = skb_shinfo(skb);\n\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tmemcpy(&skinfo->frags[skinfo->nr_frags], &sinfo->frags[0],\n\t\t       sizeof(skb_frag_t) * nr_frags);\n\n\t\txdp_update_skb_shared_info(skb, skinfo->nr_frags + nr_frags,\n\t\t\t\t\t   sinfo->xdp_frags_size,\n\t\t\t\t\t   nr_frags * xdp->frame_sz,\n\t\t\t\t\t   xdp_buff_is_frag_pfmemalloc(xdp));\n\n\t\t \n\t\tif (++rx_ring->next_to_clean == rx_ring->count)\n\t\t\trx_ring->next_to_clean = 0;\n\n\t\ti40e_process_rx_buffs(rx_ring, I40E_XDP_PASS, xdp);\n\t}\n\n\treturn skb;\n}\n\n \nstatic struct sk_buff *i40e_build_skb(struct i40e_ring *rx_ring,\n\t\t\t\t      struct xdp_buff *xdp,\n\t\t\t\t      u32 nr_frags)\n{\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data_meta);\n\n\t \n\tskb = napi_build_skb(xdp->data_hard_start, xdp->frame_sz);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\t__skb_put(skb, xdp->data_end - xdp->data);\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\tstruct skb_shared_info *sinfo;\n\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\txdp_update_skb_shared_info(skb, nr_frags,\n\t\t\t\t\t   sinfo->xdp_frags_size,\n\t\t\t\t\t   nr_frags * xdp->frame_sz,\n\t\t\t\t\t   xdp_buff_is_frag_pfmemalloc(xdp));\n\n\t\ti40e_process_rx_buffs(rx_ring, I40E_XDP_PASS, xdp);\n\t} else {\n\t\tstruct i40e_rx_buffer *rx_buffer;\n\n\t\trx_buffer = i40e_rx_bi(rx_ring, rx_ring->next_to_clean);\n\t\t \n\t\ti40e_rx_buffer_flip(rx_buffer, xdp->frame_sz);\n\t}\n\n\treturn skb;\n}\n\n \nbool i40e_is_non_eop(struct i40e_ring *rx_ring,\n\t\t     union i40e_rx_desc *rx_desc)\n{\n\t \n#define I40E_RXD_EOF BIT(I40E_RX_DESC_STATUS_EOF_SHIFT)\n\tif (likely(i40e_test_staterr(rx_desc, I40E_RXD_EOF)))\n\t\treturn false;\n\n\trx_ring->rx_stats.non_eop_descs++;\n\n\treturn true;\n}\n\nstatic int i40e_xmit_xdp_ring(struct xdp_frame *xdpf,\n\t\t\t      struct i40e_ring *xdp_ring);\n\nint i40e_xmit_xdp_tx_ring(struct xdp_buff *xdp, struct i40e_ring *xdp_ring)\n{\n\tstruct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);\n\n\tif (unlikely(!xdpf))\n\t\treturn I40E_XDP_CONSUMED;\n\n\treturn i40e_xmit_xdp_ring(xdpf, xdp_ring);\n}\n\n \nstatic int i40e_run_xdp(struct i40e_ring *rx_ring, struct xdp_buff *xdp, struct bpf_prog *xdp_prog)\n{\n\tint err, result = I40E_XDP_PASS;\n\tstruct i40e_ring *xdp_ring;\n\tu32 act;\n\n\tif (!xdp_prog)\n\t\tgoto xdp_out;\n\n\tprefetchw(xdp->data_hard_start);  \n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\txdp_ring = rx_ring->vsi->xdp_rings[rx_ring->queue_index];\n\t\tresult = i40e_xmit_xdp_tx_ring(xdp, xdp_ring);\n\t\tif (result == I40E_XDP_CONSUMED)\n\t\t\tgoto out_failure;\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\terr = xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog);\n\t\tif (err)\n\t\t\tgoto out_failure;\n\t\tresult = I40E_XDP_REDIR;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\nout_failure:\n\t\ttrace_xdp_exception(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;  \n\tcase XDP_DROP:\n\t\tresult = I40E_XDP_CONSUMED;\n\t\tbreak;\n\t}\nxdp_out:\n\treturn result;\n}\n\n \nvoid i40e_xdp_ring_update_tail(struct i40e_ring *xdp_ring)\n{\n\t \n\twmb();\n\twritel_relaxed(xdp_ring->next_to_use, xdp_ring->tail);\n}\n\n \nvoid i40e_update_rx_stats(struct i40e_ring *rx_ring,\n\t\t\t  unsigned int total_rx_bytes,\n\t\t\t  unsigned int total_rx_packets)\n{\n\tu64_stats_update_begin(&rx_ring->syncp);\n\trx_ring->stats.packets += total_rx_packets;\n\trx_ring->stats.bytes += total_rx_bytes;\n\tu64_stats_update_end(&rx_ring->syncp);\n\trx_ring->q_vector->rx.total_packets += total_rx_packets;\n\trx_ring->q_vector->rx.total_bytes += total_rx_bytes;\n}\n\n \nvoid i40e_finalize_xdp_rx(struct i40e_ring *rx_ring, unsigned int xdp_res)\n{\n\tif (xdp_res & I40E_XDP_REDIR)\n\t\txdp_do_flush_map();\n\n\tif (xdp_res & I40E_XDP_TX) {\n\t\tstruct i40e_ring *xdp_ring =\n\t\t\trx_ring->vsi->xdp_rings[rx_ring->queue_index];\n\n\t\ti40e_xdp_ring_update_tail(xdp_ring);\n\t}\n}\n\n \nstatic void i40e_inc_ntp(struct i40e_ring *rx_ring)\n{\n\tu32 ntp = rx_ring->next_to_process + 1;\n\n\tntp = (ntp < rx_ring->count) ? ntp : 0;\n\trx_ring->next_to_process = ntp;\n\tprefetch(I40E_RX_DESC(rx_ring, ntp));\n}\n\n \nstatic int i40e_add_xdp_frag(struct xdp_buff *xdp, u32 *nr_frags,\n\t\t\t     struct i40e_rx_buffer *rx_buffer, u32 size)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\n\tif (!xdp_buff_has_frags(xdp)) {\n\t\tsinfo->nr_frags = 0;\n\t\tsinfo->xdp_frags_size = 0;\n\t\txdp_buff_set_frags_flag(xdp);\n\t} else if (unlikely(sinfo->nr_frags >= MAX_SKB_FRAGS)) {\n\t\t \n\t\treturn -ENOMEM;\n\t}\n\n\t__skb_fill_page_desc_noacc(sinfo, sinfo->nr_frags++, rx_buffer->page,\n\t\t\t\t   rx_buffer->page_offset, size);\n\n\tsinfo->xdp_frags_size += size;\n\n\tif (page_is_pfmemalloc(rx_buffer->page))\n\t\txdp_buff_set_frag_pfmemalloc(xdp);\n\t*nr_frags = sinfo->nr_frags;\n\n\treturn 0;\n}\n\n \nstatic void i40e_consume_xdp_buff(struct i40e_ring *rx_ring,\n\t\t\t\t  struct xdp_buff *xdp,\n\t\t\t\t  struct i40e_rx_buffer *rx_buffer)\n{\n\ti40e_process_rx_buffs(rx_ring, I40E_XDP_CONSUMED, xdp);\n\ti40e_put_rx_buffer(rx_ring, rx_buffer);\n\trx_ring->next_to_clean = rx_ring->next_to_process;\n\txdp->data = NULL;\n}\n\n \nstatic int i40e_clean_rx_irq(struct i40e_ring *rx_ring, int budget,\n\t\t\t     unsigned int *rx_cleaned)\n{\n\tunsigned int total_rx_bytes = 0, total_rx_packets = 0;\n\tu16 cleaned_count = I40E_DESC_UNUSED(rx_ring);\n\tu16 clean_threshold = rx_ring->count / 2;\n\tunsigned int offset = rx_ring->rx_offset;\n\tstruct xdp_buff *xdp = &rx_ring->xdp;\n\tunsigned int xdp_xmit = 0;\n\tstruct bpf_prog *xdp_prog;\n\tbool failure = false;\n\tint xdp_res = 0;\n\n\txdp_prog = READ_ONCE(rx_ring->xdp_prog);\n\n\twhile (likely(total_rx_packets < (unsigned int)budget)) {\n\t\tu16 ntp = rx_ring->next_to_process;\n\t\tstruct i40e_rx_buffer *rx_buffer;\n\t\tunion i40e_rx_desc *rx_desc;\n\t\tstruct sk_buff *skb;\n\t\tunsigned int size;\n\t\tu32 nfrags = 0;\n\t\tbool neop;\n\t\tu64 qword;\n\n\t\t \n\t\tif (cleaned_count >= clean_threshold) {\n\t\t\tfailure = failure ||\n\t\t\t\t  i40e_alloc_rx_buffers(rx_ring, cleaned_count);\n\t\t\tcleaned_count = 0;\n\t\t}\n\n\t\trx_desc = I40E_RX_DESC(rx_ring, ntp);\n\n\t\t \n\t\tqword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);\n\n\t\t \n\t\tdma_rmb();\n\n\t\tif (i40e_rx_is_programming_status(qword)) {\n\t\t\ti40e_clean_programming_status(rx_ring,\n\t\t\t\t\t\t      rx_desc->raw.qword[0],\n\t\t\t\t\t\t      qword);\n\t\t\trx_buffer = i40e_rx_bi(rx_ring, ntp);\n\t\t\ti40e_inc_ntp(rx_ring);\n\t\t\ti40e_reuse_rx_page(rx_ring, rx_buffer);\n\t\t\t \n\t\t\tif (rx_ring->next_to_clean == ntp) {\n\t\t\t\trx_ring->next_to_clean =\n\t\t\t\t\trx_ring->next_to_process;\n\t\t\t\tcleaned_count++;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tsize = (qword & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>\n\t\t       I40E_RXD_QW1_LENGTH_PBUF_SHIFT;\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\ti40e_trace(clean_rx_irq, rx_ring, rx_desc, xdp);\n\t\t \n\t\trx_buffer = i40e_get_rx_buffer(rx_ring, size);\n\n\t\tneop = i40e_is_non_eop(rx_ring, rx_desc);\n\t\ti40e_inc_ntp(rx_ring);\n\n\t\tif (!xdp->data) {\n\t\t\tunsigned char *hard_start;\n\n\t\t\thard_start = page_address(rx_buffer->page) +\n\t\t\t\t     rx_buffer->page_offset - offset;\n\t\t\txdp_prepare_buff(xdp, hard_start, offset, size, true);\n#if (PAGE_SIZE > 4096)\n\t\t\t \n\t\t\txdp->frame_sz = i40e_rx_frame_truesize(rx_ring, size);\n#endif\n\t\t} else if (i40e_add_xdp_frag(xdp, &nfrags, rx_buffer, size) &&\n\t\t\t   !neop) {\n\t\t\t \n\t\t\ti40e_consume_xdp_buff(rx_ring, xdp, rx_buffer);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (neop)\n\t\t\tcontinue;\n\n\t\txdp_res = i40e_run_xdp(rx_ring, xdp, xdp_prog);\n\n\t\tif (xdp_res) {\n\t\t\txdp_xmit |= xdp_res & (I40E_XDP_TX | I40E_XDP_REDIR);\n\n\t\t\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\t\t\ti40e_process_rx_buffs(rx_ring, xdp_res, xdp);\n\t\t\t\tsize = xdp_get_buff_len(xdp);\n\t\t\t} else if (xdp_res & (I40E_XDP_TX | I40E_XDP_REDIR)) {\n\t\t\t\ti40e_rx_buffer_flip(rx_buffer, xdp->frame_sz);\n\t\t\t} else {\n\t\t\t\trx_buffer->pagecnt_bias++;\n\t\t\t}\n\t\t\ttotal_rx_bytes += size;\n\t\t} else {\n\t\t\tif (ring_uses_build_skb(rx_ring))\n\t\t\t\tskb = i40e_build_skb(rx_ring, xdp, nfrags);\n\t\t\telse\n\t\t\t\tskb = i40e_construct_skb(rx_ring, xdp, nfrags);\n\n\t\t\t \n\t\t\tif (!skb) {\n\t\t\t\trx_ring->rx_stats.alloc_buff_failed++;\n\t\t\t\ti40e_consume_xdp_buff(rx_ring, xdp, rx_buffer);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (i40e_cleanup_headers(rx_ring, skb, rx_desc))\n\t\t\t\tgoto process_next;\n\n\t\t\t \n\t\t\ttotal_rx_bytes += skb->len;\n\n\t\t\t \n\t\t\ti40e_process_skb_fields(rx_ring, rx_desc, skb);\n\n\t\t\ti40e_trace(clean_rx_irq_rx, rx_ring, rx_desc, xdp);\n\t\t\tnapi_gro_receive(&rx_ring->q_vector->napi, skb);\n\t\t}\n\n\t\t \n\t\ttotal_rx_packets++;\nprocess_next:\n\t\tcleaned_count += nfrags + 1;\n\t\ti40e_put_rx_buffer(rx_ring, rx_buffer);\n\t\trx_ring->next_to_clean = rx_ring->next_to_process;\n\n\t\txdp->data = NULL;\n\t}\n\n\ti40e_finalize_xdp_rx(rx_ring, xdp_xmit);\n\n\ti40e_update_rx_stats(rx_ring, total_rx_bytes, total_rx_packets);\n\n\t*rx_cleaned = total_rx_packets;\n\n\t \n\treturn failure ? budget : (int)total_rx_packets;\n}\n\nstatic inline u32 i40e_buildreg_itr(const int type, u16 itr)\n{\n\tu32 val;\n\n\t \n\titr &= I40E_ITR_MASK;\n\n\tval = I40E_PFINT_DYN_CTLN_INTENA_MASK |\n\t      (type << I40E_PFINT_DYN_CTLN_ITR_INDX_SHIFT) |\n\t      (itr << (I40E_PFINT_DYN_CTLN_INTERVAL_SHIFT - 1));\n\n\treturn val;\n}\n\n \n#define INTREG I40E_PFINT_DYN_CTLN\n\n \n#define ITR_COUNTDOWN_START 3\n\n \nstatic inline void i40e_update_enable_itr(struct i40e_vsi *vsi,\n\t\t\t\t\t  struct i40e_q_vector *q_vector)\n{\n\tstruct i40e_hw *hw = &vsi->back->hw;\n\tu32 intval;\n\n\t \n\tif (!(vsi->back->flags & I40E_FLAG_MSIX_ENABLED)) {\n\t\ti40e_irq_dynamic_enable_icr0(vsi->back);\n\t\treturn;\n\t}\n\n\t \n\ti40e_update_itr(q_vector, &q_vector->tx);\n\ti40e_update_itr(q_vector, &q_vector->rx);\n\n\t \n\tif (q_vector->rx.target_itr < q_vector->rx.current_itr) {\n\t\t \n\t\tintval = i40e_buildreg_itr(I40E_RX_ITR,\n\t\t\t\t\t   q_vector->rx.target_itr);\n\t\tq_vector->rx.current_itr = q_vector->rx.target_itr;\n\t\tq_vector->itr_countdown = ITR_COUNTDOWN_START;\n\t} else if ((q_vector->tx.target_itr < q_vector->tx.current_itr) ||\n\t\t   ((q_vector->rx.target_itr - q_vector->rx.current_itr) <\n\t\t    (q_vector->tx.target_itr - q_vector->tx.current_itr))) {\n\t\t \n\t\tintval = i40e_buildreg_itr(I40E_TX_ITR,\n\t\t\t\t\t   q_vector->tx.target_itr);\n\t\tq_vector->tx.current_itr = q_vector->tx.target_itr;\n\t\tq_vector->itr_countdown = ITR_COUNTDOWN_START;\n\t} else if (q_vector->rx.current_itr != q_vector->rx.target_itr) {\n\t\t \n\t\tintval = i40e_buildreg_itr(I40E_RX_ITR,\n\t\t\t\t\t   q_vector->rx.target_itr);\n\t\tq_vector->rx.current_itr = q_vector->rx.target_itr;\n\t\tq_vector->itr_countdown = ITR_COUNTDOWN_START;\n\t} else {\n\t\t \n\t\tintval = i40e_buildreg_itr(I40E_ITR_NONE, 0);\n\t\tif (q_vector->itr_countdown)\n\t\t\tq_vector->itr_countdown--;\n\t}\n\n\tif (!test_bit(__I40E_VSI_DOWN, vsi->state))\n\t\twr32(hw, INTREG(q_vector->reg_idx), intval);\n}\n\n \nint i40e_napi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct i40e_q_vector *q_vector =\n\t\t\t       container_of(napi, struct i40e_q_vector, napi);\n\tstruct i40e_vsi *vsi = q_vector->vsi;\n\tstruct i40e_ring *ring;\n\tbool tx_clean_complete = true;\n\tbool rx_clean_complete = true;\n\tunsigned int tx_cleaned = 0;\n\tunsigned int rx_cleaned = 0;\n\tbool clean_complete = true;\n\tbool arm_wb = false;\n\tint budget_per_ring;\n\tint work_done = 0;\n\n\tif (test_bit(__I40E_VSI_DOWN, vsi->state)) {\n\t\tnapi_complete(napi);\n\t\treturn 0;\n\t}\n\n\t \n\ti40e_for_each_ring(ring, q_vector->tx) {\n\t\tbool wd = ring->xsk_pool ?\n\t\t\t  i40e_clean_xdp_tx_irq(vsi, ring) :\n\t\t\t  i40e_clean_tx_irq(vsi, ring, budget, &tx_cleaned);\n\n\t\tif (!wd) {\n\t\t\tclean_complete = tx_clean_complete = false;\n\t\t\tcontinue;\n\t\t}\n\t\tarm_wb |= ring->arm_wb;\n\t\tring->arm_wb = false;\n\t}\n\n\t \n\tif (budget <= 0)\n\t\tgoto tx_only;\n\n\t \n\tif (unlikely(q_vector->num_ringpairs > 1))\n\t\t \n\t\tbudget_per_ring = max_t(int, budget / q_vector->num_ringpairs, 1);\n\telse\n\t\t \n\t\tbudget_per_ring = budget;\n\n\ti40e_for_each_ring(ring, q_vector->rx) {\n\t\tint cleaned = ring->xsk_pool ?\n\t\t\t      i40e_clean_rx_irq_zc(ring, budget_per_ring) :\n\t\t\t      i40e_clean_rx_irq(ring, budget_per_ring, &rx_cleaned);\n\n\t\twork_done += cleaned;\n\t\t \n\t\tif (cleaned >= budget_per_ring)\n\t\t\tclean_complete = rx_clean_complete = false;\n\t}\n\n\tif (!i40e_enabled_xdp_vsi(vsi))\n\t\ttrace_i40e_napi_poll(napi, q_vector, budget, budget_per_ring, rx_cleaned,\n\t\t\t\t     tx_cleaned, rx_clean_complete, tx_clean_complete);\n\n\t \n\tif (!clean_complete) {\n\t\tint cpu_id = smp_processor_id();\n\n\t\t \n\t\tif (!cpumask_test_cpu(cpu_id, &q_vector->affinity_mask)) {\n\t\t\t \n\t\t\tnapi_complete_done(napi, work_done);\n\n\t\t\t \n\t\t\ti40e_force_wb(vsi, q_vector);\n\n\t\t\t \n\t\t\treturn budget - 1;\n\t\t}\ntx_only:\n\t\tif (arm_wb) {\n\t\t\tq_vector->tx.ring[0].tx_stats.tx_force_wb++;\n\t\t\ti40e_enable_wb_on_itr(vsi, q_vector);\n\t\t}\n\t\treturn budget;\n\t}\n\n\tif (q_vector->tx.ring[0].flags & I40E_TXR_FLAGS_WB_ON_ITR)\n\t\tq_vector->arm_wb_state = false;\n\n\t \n\tif (likely(napi_complete_done(napi, work_done)))\n\t\ti40e_update_enable_itr(vsi, q_vector);\n\n\treturn min(work_done, budget - 1);\n}\n\n \nstatic void i40e_atr(struct i40e_ring *tx_ring, struct sk_buff *skb,\n\t\t     u32 tx_flags)\n{\n\tstruct i40e_filter_program_desc *fdir_desc;\n\tstruct i40e_pf *pf = tx_ring->vsi->back;\n\tunion {\n\t\tunsigned char *network;\n\t\tstruct iphdr *ipv4;\n\t\tstruct ipv6hdr *ipv6;\n\t} hdr;\n\tstruct tcphdr *th;\n\tunsigned int hlen;\n\tu32 flex_ptype, dtype_cmd;\n\tint l4_proto;\n\tu16 i;\n\n\t \n\tif (!(pf->flags & I40E_FLAG_FD_ATR_ENABLED))\n\t\treturn;\n\n\tif (test_bit(__I40E_FD_ATR_AUTO_DISABLED, pf->state))\n\t\treturn;\n\n\t \n\tif (!tx_ring->atr_sample_rate)\n\t\treturn;\n\n\t \n\tif (!(tx_flags & (I40E_TX_FLAGS_IPV4 | I40E_TX_FLAGS_IPV6)))\n\t\treturn;\n\n\t \n\thdr.network = (tx_flags & I40E_TX_FLAGS_UDP_TUNNEL) ?\n\t\t      skb_inner_network_header(skb) : skb_network_header(skb);\n\n\t \n\tif (tx_flags & I40E_TX_FLAGS_IPV4) {\n\t\t \n\t\thlen = (hdr.network[0] & 0x0F) << 2;\n\t\tl4_proto = hdr.ipv4->protocol;\n\t} else {\n\t\t \n\t\tunsigned int inner_hlen = hdr.network - skb->data;\n\t\tunsigned int h_offset = inner_hlen;\n\n\t\t \n\t\tl4_proto =\n\t\t  ipv6_find_hdr(skb, &h_offset, IPPROTO_TCP, NULL, NULL);\n\t\t \n\t\thlen = h_offset - inner_hlen;\n\t}\n\n\tif (l4_proto != IPPROTO_TCP)\n\t\treturn;\n\n\tth = (struct tcphdr *)(hdr.network + hlen);\n\n\t \n\tif (th->syn && test_bit(__I40E_FD_ATR_AUTO_DISABLED, pf->state))\n\t\treturn;\n\tif (pf->flags & I40E_FLAG_HW_ATR_EVICT_ENABLED) {\n\t\t \n\t\tif (th->fin || th->rst)\n\t\t\treturn;\n\t}\n\n\ttx_ring->atr_count++;\n\n\t \n\tif (!th->fin &&\n\t    !th->syn &&\n\t    !th->rst &&\n\t    (tx_ring->atr_count < tx_ring->atr_sample_rate))\n\t\treturn;\n\n\ttx_ring->atr_count = 0;\n\n\t \n\ti = tx_ring->next_to_use;\n\tfdir_desc = I40E_TX_FDIRDESC(tx_ring, i);\n\n\ti++;\n\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\tflex_ptype = (tx_ring->queue_index << I40E_TXD_FLTR_QW0_QINDEX_SHIFT) &\n\t\t      I40E_TXD_FLTR_QW0_QINDEX_MASK;\n\tflex_ptype |= (tx_flags & I40E_TX_FLAGS_IPV4) ?\n\t\t      (I40E_FILTER_PCTYPE_NONF_IPV4_TCP <<\n\t\t       I40E_TXD_FLTR_QW0_PCTYPE_SHIFT) :\n\t\t      (I40E_FILTER_PCTYPE_NONF_IPV6_TCP <<\n\t\t       I40E_TXD_FLTR_QW0_PCTYPE_SHIFT);\n\n\tflex_ptype |= tx_ring->vsi->id << I40E_TXD_FLTR_QW0_DEST_VSI_SHIFT;\n\n\tdtype_cmd = I40E_TX_DESC_DTYPE_FILTER_PROG;\n\n\tdtype_cmd |= (th->fin || th->rst) ?\n\t\t     (I40E_FILTER_PROGRAM_DESC_PCMD_REMOVE <<\n\t\t      I40E_TXD_FLTR_QW1_PCMD_SHIFT) :\n\t\t     (I40E_FILTER_PROGRAM_DESC_PCMD_ADD_UPDATE <<\n\t\t      I40E_TXD_FLTR_QW1_PCMD_SHIFT);\n\n\tdtype_cmd |= I40E_FILTER_PROGRAM_DESC_DEST_DIRECT_PACKET_QINDEX <<\n\t\t     I40E_TXD_FLTR_QW1_DEST_SHIFT;\n\n\tdtype_cmd |= I40E_FILTER_PROGRAM_DESC_FD_STATUS_FD_ID <<\n\t\t     I40E_TXD_FLTR_QW1_FD_STATUS_SHIFT;\n\n\tdtype_cmd |= I40E_TXD_FLTR_QW1_CNT_ENA_MASK;\n\tif (!(tx_flags & I40E_TX_FLAGS_UDP_TUNNEL))\n\t\tdtype_cmd |=\n\t\t\t((u32)I40E_FD_ATR_STAT_IDX(pf->hw.pf_id) <<\n\t\t\tI40E_TXD_FLTR_QW1_CNTINDEX_SHIFT) &\n\t\t\tI40E_TXD_FLTR_QW1_CNTINDEX_MASK;\n\telse\n\t\tdtype_cmd |=\n\t\t\t((u32)I40E_FD_ATR_TUNNEL_STAT_IDX(pf->hw.pf_id) <<\n\t\t\tI40E_TXD_FLTR_QW1_CNTINDEX_SHIFT) &\n\t\t\tI40E_TXD_FLTR_QW1_CNTINDEX_MASK;\n\n\tif (pf->flags & I40E_FLAG_HW_ATR_EVICT_ENABLED)\n\t\tdtype_cmd |= I40E_TXD_FLTR_QW1_ATR_MASK;\n\n\tfdir_desc->qindex_flex_ptype_vsi = cpu_to_le32(flex_ptype);\n\tfdir_desc->rsvd = cpu_to_le32(0);\n\tfdir_desc->dtype_cmd_cntindex = cpu_to_le32(dtype_cmd);\n\tfdir_desc->fd_id = cpu_to_le32(0);\n}\n\n \nstatic inline int i40e_tx_prepare_vlan_flags(struct sk_buff *skb,\n\t\t\t\t\t     struct i40e_ring *tx_ring,\n\t\t\t\t\t     u32 *flags)\n{\n\t__be16 protocol = skb->protocol;\n\tu32  tx_flags = 0;\n\n\tif (protocol == htons(ETH_P_8021Q) &&\n\t    !(tx_ring->netdev->features & NETIF_F_HW_VLAN_CTAG_TX)) {\n\t\t \n\t\tskb->protocol = vlan_get_protocol(skb);\n\t\tgoto out;\n\t}\n\n\t \n\tif (skb_vlan_tag_present(skb)) {\n\t\ttx_flags |= skb_vlan_tag_get(skb) << I40E_TX_FLAGS_VLAN_SHIFT;\n\t\ttx_flags |= I40E_TX_FLAGS_HW_VLAN;\n\t \n\t} else if (protocol == htons(ETH_P_8021Q)) {\n\t\tstruct vlan_hdr *vhdr, _vhdr;\n\n\t\tvhdr = skb_header_pointer(skb, ETH_HLEN, sizeof(_vhdr), &_vhdr);\n\t\tif (!vhdr)\n\t\t\treturn -EINVAL;\n\n\t\tprotocol = vhdr->h_vlan_encapsulated_proto;\n\t\ttx_flags |= ntohs(vhdr->h_vlan_TCI) << I40E_TX_FLAGS_VLAN_SHIFT;\n\t\ttx_flags |= I40E_TX_FLAGS_SW_VLAN;\n\t}\n\n\tif (!(tx_ring->vsi->back->flags & I40E_FLAG_DCB_ENABLED))\n\t\tgoto out;\n\n\t \n\tif ((tx_flags & (I40E_TX_FLAGS_HW_VLAN | I40E_TX_FLAGS_SW_VLAN)) ||\n\t    (skb->priority != TC_PRIO_CONTROL)) {\n\t\ttx_flags &= ~I40E_TX_FLAGS_VLAN_PRIO_MASK;\n\t\ttx_flags |= (skb->priority & 0x7) <<\n\t\t\t\tI40E_TX_FLAGS_VLAN_PRIO_SHIFT;\n\t\tif (tx_flags & I40E_TX_FLAGS_SW_VLAN) {\n\t\t\tstruct vlan_ethhdr *vhdr;\n\t\t\tint rc;\n\n\t\t\trc = skb_cow_head(skb, 0);\n\t\t\tif (rc < 0)\n\t\t\t\treturn rc;\n\t\t\tvhdr = skb_vlan_eth_hdr(skb);\n\t\t\tvhdr->h_vlan_TCI = htons(tx_flags >>\n\t\t\t\t\t\t I40E_TX_FLAGS_VLAN_SHIFT);\n\t\t} else {\n\t\t\ttx_flags |= I40E_TX_FLAGS_HW_VLAN;\n\t\t}\n\t}\n\nout:\n\t*flags = tx_flags;\n\treturn 0;\n}\n\n \nstatic int i40e_tso(struct i40e_tx_buffer *first, u8 *hdr_len,\n\t\t    u64 *cd_type_cmd_tso_mss)\n{\n\tstruct sk_buff *skb = first->skb;\n\tu64 cd_cmd, cd_tso_len, cd_mss;\n\t__be16 protocol;\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tstruct udphdr *udp;\n\t\tunsigned char *hdr;\n\t} l4;\n\tu32 paylen, l4_offset;\n\tu16 gso_size;\n\tint err;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\tprotocol = vlan_get_protocol(skb);\n\n\tif (eth_p_mpls(protocol))\n\t\tip.hdr = skb_inner_network_header(skb);\n\telse\n\t\tip.hdr = skb_network_header(skb);\n\tl4.hdr = skb_checksum_start(skb);\n\n\t \n\tif (ip.v4->version == 4) {\n\t\tip.v4->tot_len = 0;\n\t\tip.v4->check = 0;\n\n\t\tfirst->tx_flags |= I40E_TX_FLAGS_TSO;\n\t} else {\n\t\tip.v6->payload_len = 0;\n\t\tfirst->tx_flags |= I40E_TX_FLAGS_TSO;\n\t}\n\n\tif (skb_shinfo(skb)->gso_type & (SKB_GSO_GRE |\n\t\t\t\t\t SKB_GSO_GRE_CSUM |\n\t\t\t\t\t SKB_GSO_IPXIP4 |\n\t\t\t\t\t SKB_GSO_IPXIP6 |\n\t\t\t\t\t SKB_GSO_UDP_TUNNEL |\n\t\t\t\t\t SKB_GSO_UDP_TUNNEL_CSUM)) {\n\t\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL) &&\n\t\t    (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM)) {\n\t\t\tl4.udp->len = 0;\n\n\t\t\t \n\t\t\tl4_offset = l4.hdr - skb->data;\n\n\t\t\t \n\t\t\tpaylen = skb->len - l4_offset;\n\t\t\tcsum_replace_by_diff(&l4.udp->check,\n\t\t\t\t\t     (__force __wsum)htonl(paylen));\n\t\t}\n\n\t\t \n\t\tip.hdr = skb_inner_network_header(skb);\n\t\tl4.hdr = skb_inner_transport_header(skb);\n\n\t\t \n\t\tif (ip.v4->version == 4) {\n\t\t\tip.v4->tot_len = 0;\n\t\t\tip.v4->check = 0;\n\t\t} else {\n\t\t\tip.v6->payload_len = 0;\n\t\t}\n\t}\n\n\t \n\tl4_offset = l4.hdr - skb->data;\n\n\t \n\tpaylen = skb->len - l4_offset;\n\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {\n\t\tcsum_replace_by_diff(&l4.udp->check, (__force __wsum)htonl(paylen));\n\t\t \n\t\t*hdr_len = sizeof(*l4.udp) + l4_offset;\n\t} else {\n\t\tcsum_replace_by_diff(&l4.tcp->check, (__force __wsum)htonl(paylen));\n\t\t \n\t\t*hdr_len = (l4.tcp->doff * 4) + l4_offset;\n\t}\n\n\t \n\tgso_size = skb_shinfo(skb)->gso_size;\n\n\t \n\tfirst->gso_segs = skb_shinfo(skb)->gso_segs;\n\tfirst->bytecount += (first->gso_segs - 1) * *hdr_len;\n\n\t \n\tcd_cmd = I40E_TX_CTX_DESC_TSO;\n\tcd_tso_len = skb->len - *hdr_len;\n\tcd_mss = gso_size;\n\t*cd_type_cmd_tso_mss |= (cd_cmd << I40E_TXD_CTX_QW1_CMD_SHIFT) |\n\t\t\t\t(cd_tso_len << I40E_TXD_CTX_QW1_TSO_LEN_SHIFT) |\n\t\t\t\t(cd_mss << I40E_TXD_CTX_QW1_MSS_SHIFT);\n\treturn 1;\n}\n\n \nstatic int i40e_tsyn(struct i40e_ring *tx_ring, struct sk_buff *skb,\n\t\t     u32 tx_flags, u64 *cd_type_cmd_tso_mss)\n{\n\tstruct i40e_pf *pf;\n\n\tif (likely(!(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)))\n\t\treturn 0;\n\n\t \n\tif (tx_flags & I40E_TX_FLAGS_TSO)\n\t\treturn 0;\n\n\t \n\tpf = i40e_netdev_to_pf(tx_ring->netdev);\n\tif (!(pf->flags & I40E_FLAG_PTP))\n\t\treturn 0;\n\n\tif (pf->ptp_tx &&\n\t    !test_and_set_bit_lock(__I40E_PTP_TX_IN_PROGRESS, pf->state)) {\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\tpf->ptp_tx_start = jiffies;\n\t\tpf->ptp_tx_skb = skb_get(skb);\n\t} else {\n\t\tpf->tx_hwtstamp_skipped++;\n\t\treturn 0;\n\t}\n\n\t*cd_type_cmd_tso_mss |= (u64)I40E_TX_CTX_DESC_TSYN <<\n\t\t\t\tI40E_TXD_CTX_QW1_CMD_SHIFT;\n\n\treturn 1;\n}\n\n \nstatic int i40e_tx_enable_csum(struct sk_buff *skb, u32 *tx_flags,\n\t\t\t       u32 *td_cmd, u32 *td_offset,\n\t\t\t       struct i40e_ring *tx_ring,\n\t\t\t       u32 *cd_tunneling)\n{\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tstruct udphdr *udp;\n\t\tunsigned char *hdr;\n\t} l4;\n\tunsigned char *exthdr;\n\tu32 offset, cmd = 0;\n\t__be16 frag_off;\n\t__be16 protocol;\n\tu8 l4_proto = 0;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tprotocol = vlan_get_protocol(skb);\n\n\tif (eth_p_mpls(protocol)) {\n\t\tip.hdr = skb_inner_network_header(skb);\n\t\tl4.hdr = skb_checksum_start(skb);\n\t} else {\n\t\tip.hdr = skb_network_header(skb);\n\t\tl4.hdr = skb_transport_header(skb);\n\t}\n\n\t \n\tif (ip.v4->version == 4)\n\t\t*tx_flags |= I40E_TX_FLAGS_IPV4;\n\telse\n\t\t*tx_flags |= I40E_TX_FLAGS_IPV6;\n\n\t \n\toffset = ((ip.hdr - skb->data) / 2) << I40E_TX_DESC_LENGTH_MACLEN_SHIFT;\n\n\tif (skb->encapsulation) {\n\t\tu32 tunnel = 0;\n\t\t \n\t\tif (*tx_flags & I40E_TX_FLAGS_IPV4) {\n\t\t\ttunnel |= (*tx_flags & I40E_TX_FLAGS_TSO) ?\n\t\t\t\t  I40E_TX_CTX_EXT_IP_IPV4 :\n\t\t\t\t  I40E_TX_CTX_EXT_IP_IPV4_NO_CSUM;\n\n\t\t\tl4_proto = ip.v4->protocol;\n\t\t} else if (*tx_flags & I40E_TX_FLAGS_IPV6) {\n\t\t\tint ret;\n\n\t\t\ttunnel |= I40E_TX_CTX_EXT_IP_IPV6;\n\n\t\t\texthdr = ip.hdr + sizeof(*ip.v6);\n\t\t\tl4_proto = ip.v6->nexthdr;\n\t\t\tret = ipv6_skip_exthdr(skb, exthdr - skb->data,\n\t\t\t\t\t       &l4_proto, &frag_off);\n\t\t\tif (ret < 0)\n\t\t\t\treturn -1;\n\t\t}\n\n\t\t \n\t\tswitch (l4_proto) {\n\t\tcase IPPROTO_UDP:\n\t\t\ttunnel |= I40E_TXD_CTX_UDP_TUNNELING;\n\t\t\t*tx_flags |= I40E_TX_FLAGS_UDP_TUNNEL;\n\t\t\tbreak;\n\t\tcase IPPROTO_GRE:\n\t\t\ttunnel |= I40E_TXD_CTX_GRE_TUNNELING;\n\t\t\t*tx_flags |= I40E_TX_FLAGS_UDP_TUNNEL;\n\t\t\tbreak;\n\t\tcase IPPROTO_IPIP:\n\t\tcase IPPROTO_IPV6:\n\t\t\t*tx_flags |= I40E_TX_FLAGS_UDP_TUNNEL;\n\t\t\tl4.hdr = skb_inner_network_header(skb);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (*tx_flags & I40E_TX_FLAGS_TSO)\n\t\t\t\treturn -1;\n\n\t\t\tskb_checksum_help(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\ttunnel |= ((l4.hdr - ip.hdr) / 4) <<\n\t\t\t  I40E_TXD_CTX_QW0_EXT_IPLEN_SHIFT;\n\n\t\t \n\t\tip.hdr = skb_inner_network_header(skb);\n\n\t\t \n\t\ttunnel |= ((ip.hdr - l4.hdr) / 2) <<\n\t\t\t  I40E_TXD_CTX_QW0_NATLEN_SHIFT;\n\n\t\t \n\t\tif ((*tx_flags & I40E_TX_FLAGS_TSO) &&\n\t\t    !(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL) &&\n\t\t    (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM))\n\t\t\ttunnel |= I40E_TXD_CTX_QW0_L4T_CS_MASK;\n\n\t\t \n\t\t*cd_tunneling |= tunnel;\n\n\t\t \n\t\tl4.hdr = skb_inner_transport_header(skb);\n\t\tl4_proto = 0;\n\n\t\t \n\t\t*tx_flags &= ~(I40E_TX_FLAGS_IPV4 | I40E_TX_FLAGS_IPV6);\n\t\tif (ip.v4->version == 4)\n\t\t\t*tx_flags |= I40E_TX_FLAGS_IPV4;\n\t\tif (ip.v6->version == 6)\n\t\t\t*tx_flags |= I40E_TX_FLAGS_IPV6;\n\t}\n\n\t \n\tif (*tx_flags & I40E_TX_FLAGS_IPV4) {\n\t\tl4_proto = ip.v4->protocol;\n\t\t \n\t\tcmd |= (*tx_flags & I40E_TX_FLAGS_TSO) ?\n\t\t       I40E_TX_DESC_CMD_IIPT_IPV4_CSUM :\n\t\t       I40E_TX_DESC_CMD_IIPT_IPV4;\n\t} else if (*tx_flags & I40E_TX_FLAGS_IPV6) {\n\t\tcmd |= I40E_TX_DESC_CMD_IIPT_IPV6;\n\n\t\texthdr = ip.hdr + sizeof(*ip.v6);\n\t\tl4_proto = ip.v6->nexthdr;\n\t\tif (l4.hdr != exthdr)\n\t\t\tipv6_skip_exthdr(skb, exthdr - skb->data,\n\t\t\t\t\t &l4_proto, &frag_off);\n\t}\n\n\t \n\toffset |= ((l4.hdr - ip.hdr) / 4) << I40E_TX_DESC_LENGTH_IPLEN_SHIFT;\n\n\t \n\tswitch (l4_proto) {\n\tcase IPPROTO_TCP:\n\t\t \n\t\tcmd |= I40E_TX_DESC_CMD_L4T_EOFT_TCP;\n\t\toffset |= l4.tcp->doff << I40E_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;\n\t\tbreak;\n\tcase IPPROTO_SCTP:\n\t\t \n\t\tcmd |= I40E_TX_DESC_CMD_L4T_EOFT_SCTP;\n\t\toffset |= (sizeof(struct sctphdr) >> 2) <<\n\t\t\t  I40E_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\t \n\t\tcmd |= I40E_TX_DESC_CMD_L4T_EOFT_UDP;\n\t\toffset |= (sizeof(struct udphdr) >> 2) <<\n\t\t\t  I40E_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;\n\t\tbreak;\n\tdefault:\n\t\tif (*tx_flags & I40E_TX_FLAGS_TSO)\n\t\t\treturn -1;\n\t\tskb_checksum_help(skb);\n\t\treturn 0;\n\t}\n\n\t*td_cmd |= cmd;\n\t*td_offset |= offset;\n\n\treturn 1;\n}\n\n \nstatic void i40e_create_tx_ctx(struct i40e_ring *tx_ring,\n\t\t\t       const u64 cd_type_cmd_tso_mss,\n\t\t\t       const u32 cd_tunneling, const u32 cd_l2tag2)\n{\n\tstruct i40e_tx_context_desc *context_desc;\n\tint i = tx_ring->next_to_use;\n\n\tif ((cd_type_cmd_tso_mss == I40E_TX_DESC_DTYPE_CONTEXT) &&\n\t    !cd_tunneling && !cd_l2tag2)\n\t\treturn;\n\n\t \n\tcontext_desc = I40E_TX_CTXTDESC(tx_ring, i);\n\n\ti++;\n\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\t \n\tcontext_desc->tunneling_params = cpu_to_le32(cd_tunneling);\n\tcontext_desc->l2tag2 = cpu_to_le16(cd_l2tag2);\n\tcontext_desc->rsvd = cpu_to_le16(0);\n\tcontext_desc->type_cmd_tso_mss = cpu_to_le64(cd_type_cmd_tso_mss);\n}\n\n \nint __i40e_maybe_stop_tx(struct i40e_ring *tx_ring, int size)\n{\n\tnetif_stop_subqueue(tx_ring->netdev, tx_ring->queue_index);\n\t \n\tsmp_mb();\n\n\t++tx_ring->tx_stats.tx_stopped;\n\n\t \n\tif (likely(I40E_DESC_UNUSED(tx_ring) < size))\n\t\treturn -EBUSY;\n\n\t \n\tnetif_start_subqueue(tx_ring->netdev, tx_ring->queue_index);\n\t++tx_ring->tx_stats.restart_queue;\n\treturn 0;\n}\n\n \nbool __i40e_chk_linearize(struct sk_buff *skb)\n{\n\tconst skb_frag_t *frag, *stale;\n\tint nr_frags, sum;\n\n\t \n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\tif (nr_frags < (I40E_MAX_BUFFER_TXD - 1))\n\t\treturn false;\n\n\t \n\tnr_frags -= I40E_MAX_BUFFER_TXD - 2;\n\tfrag = &skb_shinfo(skb)->frags[0];\n\n\t \n\tsum = 1 - skb_shinfo(skb)->gso_size;\n\n\t \n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\n\t \n\tfor (stale = &skb_shinfo(skb)->frags[0];; stale++) {\n\t\tint stale_size = skb_frag_size(stale);\n\n\t\tsum += skb_frag_size(frag++);\n\n\t\t \n\t\tif (stale_size > I40E_MAX_DATA_PER_TXD) {\n\t\t\tint align_pad = -(skb_frag_off(stale)) &\n\t\t\t\t\t(I40E_MAX_READ_REQ_SIZE - 1);\n\n\t\t\tsum -= align_pad;\n\t\t\tstale_size -= align_pad;\n\n\t\t\tdo {\n\t\t\t\tsum -= I40E_MAX_DATA_PER_TXD_ALIGNED;\n\t\t\t\tstale_size -= I40E_MAX_DATA_PER_TXD_ALIGNED;\n\t\t\t} while (stale_size > I40E_MAX_DATA_PER_TXD);\n\t\t}\n\n\t\t \n\t\tif (sum < 0)\n\t\t\treturn true;\n\n\t\tif (!nr_frags--)\n\t\t\tbreak;\n\n\t\tsum -= stale_size;\n\t}\n\n\treturn false;\n}\n\n \nstatic inline int i40e_tx_map(struct i40e_ring *tx_ring, struct sk_buff *skb,\n\t\t\t      struct i40e_tx_buffer *first, u32 tx_flags,\n\t\t\t      const u8 hdr_len, u32 td_cmd, u32 td_offset)\n{\n\tunsigned int data_len = skb->data_len;\n\tunsigned int size = skb_headlen(skb);\n\tskb_frag_t *frag;\n\tstruct i40e_tx_buffer *tx_bi;\n\tstruct i40e_tx_desc *tx_desc;\n\tu16 i = tx_ring->next_to_use;\n\tu32 td_tag = 0;\n\tdma_addr_t dma;\n\tu16 desc_count = 1;\n\n\tif (tx_flags & I40E_TX_FLAGS_HW_VLAN) {\n\t\ttd_cmd |= I40E_TX_DESC_CMD_IL2TAG1;\n\t\ttd_tag = (tx_flags & I40E_TX_FLAGS_VLAN_MASK) >>\n\t\t\t I40E_TX_FLAGS_VLAN_SHIFT;\n\t}\n\n\tfirst->tx_flags = tx_flags;\n\n\tdma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);\n\n\ttx_desc = I40E_TX_DESC(tx_ring, i);\n\ttx_bi = first;\n\n\tfor (frag = &skb_shinfo(skb)->frags[0];; frag++) {\n\t\tunsigned int max_data = I40E_MAX_DATA_PER_TXD_ALIGNED;\n\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto dma_error;\n\n\t\t \n\t\tdma_unmap_len_set(tx_bi, len, size);\n\t\tdma_unmap_addr_set(tx_bi, dma, dma);\n\n\t\t \n\t\tmax_data += -dma & (I40E_MAX_READ_REQ_SIZE - 1);\n\t\ttx_desc->buffer_addr = cpu_to_le64(dma);\n\n\t\twhile (unlikely(size > I40E_MAX_DATA_PER_TXD)) {\n\t\t\ttx_desc->cmd_type_offset_bsz =\n\t\t\t\tbuild_ctob(td_cmd, td_offset,\n\t\t\t\t\t   max_data, td_tag);\n\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tdesc_count++;\n\n\t\t\tif (i == tx_ring->count) {\n\t\t\t\ttx_desc = I40E_TX_DESC(tx_ring, 0);\n\t\t\t\ti = 0;\n\t\t\t}\n\n\t\t\tdma += max_data;\n\t\t\tsize -= max_data;\n\n\t\t\tmax_data = I40E_MAX_DATA_PER_TXD_ALIGNED;\n\t\t\ttx_desc->buffer_addr = cpu_to_le64(dma);\n\t\t}\n\n\t\tif (likely(!data_len))\n\t\t\tbreak;\n\n\t\ttx_desc->cmd_type_offset_bsz = build_ctob(td_cmd, td_offset,\n\t\t\t\t\t\t\t  size, td_tag);\n\n\t\ttx_desc++;\n\t\ti++;\n\t\tdesc_count++;\n\n\t\tif (i == tx_ring->count) {\n\t\t\ttx_desc = I40E_TX_DESC(tx_ring, 0);\n\t\t\ti = 0;\n\t\t}\n\n\t\tsize = skb_frag_size(frag);\n\t\tdata_len -= size;\n\n\t\tdma = skb_frag_dma_map(tx_ring->dev, frag, 0, size,\n\t\t\t\t       DMA_TO_DEVICE);\n\n\t\ttx_bi = &tx_ring->tx_bi[i];\n\t}\n\n\tnetdev_tx_sent_queue(txring_txq(tx_ring), first->bytecount);\n\n\ti++;\n\tif (i == tx_ring->count)\n\t\ti = 0;\n\n\ttx_ring->next_to_use = i;\n\n\ti40e_maybe_stop_tx(tx_ring, DESC_NEEDED);\n\n\t \n\ttd_cmd |= I40E_TX_DESC_CMD_EOP;\n\n\t \n\tdesc_count |= ++tx_ring->packet_stride;\n\n\tif (desc_count >= WB_STRIDE) {\n\t\t \n\t\ttd_cmd |= I40E_TX_DESC_CMD_RS;\n\t\ttx_ring->packet_stride = 0;\n\t}\n\n\ttx_desc->cmd_type_offset_bsz =\n\t\t\tbuild_ctob(td_cmd, td_offset, size, td_tag);\n\n\tskb_tx_timestamp(skb);\n\n\t \n\twmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\t \n\tif (netif_xmit_stopped(txring_txq(tx_ring)) || !netdev_xmit_more()) {\n\t\twritel(i, tx_ring->tail);\n\t}\n\n\treturn 0;\n\ndma_error:\n\tdev_info(tx_ring->dev, \"TX DMA map failed\\n\");\n\n\t \n\tfor (;;) {\n\t\ttx_bi = &tx_ring->tx_bi[i];\n\t\ti40e_unmap_and_free_tx_resource(tx_ring, tx_bi);\n\t\tif (tx_bi == first)\n\t\t\tbreak;\n\t\tif (i == 0)\n\t\t\ti = tx_ring->count;\n\t\ti--;\n\t}\n\n\ttx_ring->next_to_use = i;\n\n\treturn -1;\n}\n\nstatic u16 i40e_swdcb_skb_tx_hash(struct net_device *dev,\n\t\t\t\t  const struct sk_buff *skb,\n\t\t\t\t  u16 num_tx_queues)\n{\n\tu32 jhash_initval_salt = 0xd631614b;\n\tu32 hash;\n\n\tif (skb->sk && skb->sk->sk_hash)\n\t\thash = skb->sk->sk_hash;\n\telse\n\t\thash = (__force u16)skb->protocol ^ skb->hash;\n\n\thash = jhash_1word(hash, jhash_initval_salt);\n\n\treturn (u16)(((u64)hash * num_tx_queues) >> 32);\n}\n\nu16 i40e_lan_select_queue(struct net_device *netdev,\n\t\t\t  struct sk_buff *skb,\n\t\t\t  struct net_device __always_unused *sb_dev)\n{\n\tstruct i40e_netdev_priv *np = netdev_priv(netdev);\n\tstruct i40e_vsi *vsi = np->vsi;\n\tstruct i40e_hw *hw;\n\tu16 qoffset;\n\tu16 qcount;\n\tu8 tclass;\n\tu16 hash;\n\tu8 prio;\n\n\t \n\tif (vsi->tc_config.numtc == 1 ||\n\t    i40e_is_tc_mqprio_enabled(vsi->back))\n\t\treturn netdev_pick_tx(netdev, skb, sb_dev);\n\n\tprio = skb->priority;\n\thw = &vsi->back->hw;\n\ttclass = hw->local_dcbx_config.etscfg.prioritytable[prio];\n\t \n\tif (unlikely(!(vsi->tc_config.enabled_tc & BIT(tclass))))\n\t\ttclass = 0;\n\n\t \n\tqcount = vsi->tc_config.tc_info[tclass].qcount;\n\thash = i40e_swdcb_skb_tx_hash(netdev, skb, qcount);\n\n\tqoffset = vsi->tc_config.tc_info[tclass].qoffset;\n\treturn qoffset + hash;\n}\n\n \nstatic int i40e_xmit_xdp_ring(struct xdp_frame *xdpf,\n\t\t\t      struct i40e_ring *xdp_ring)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_frame(xdpf);\n\tu8 nr_frags = unlikely(xdp_frame_has_frags(xdpf)) ? sinfo->nr_frags : 0;\n\tu16 i = 0, index = xdp_ring->next_to_use;\n\tstruct i40e_tx_buffer *tx_head = &xdp_ring->tx_bi[index];\n\tstruct i40e_tx_buffer *tx_bi = tx_head;\n\tstruct i40e_tx_desc *tx_desc = I40E_TX_DESC(xdp_ring, index);\n\tvoid *data = xdpf->data;\n\tu32 size = xdpf->len;\n\n\tif (unlikely(I40E_DESC_UNUSED(xdp_ring) < 1 + nr_frags)) {\n\t\txdp_ring->tx_stats.tx_busy++;\n\t\treturn I40E_XDP_CONSUMED;\n\t}\n\n\ttx_head->bytecount = xdp_get_frame_len(xdpf);\n\ttx_head->gso_segs = 1;\n\ttx_head->xdpf = xdpf;\n\n\tfor (;;) {\n\t\tdma_addr_t dma;\n\n\t\tdma = dma_map_single(xdp_ring->dev, data, size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(xdp_ring->dev, dma))\n\t\t\tgoto unmap;\n\n\t\t \n\t\tdma_unmap_len_set(tx_bi, len, size);\n\t\tdma_unmap_addr_set(tx_bi, dma, dma);\n\n\t\ttx_desc->buffer_addr = cpu_to_le64(dma);\n\t\ttx_desc->cmd_type_offset_bsz =\n\t\t\tbuild_ctob(I40E_TX_DESC_CMD_ICRC, 0, size, 0);\n\n\t\tif (++index == xdp_ring->count)\n\t\t\tindex = 0;\n\n\t\tif (i == nr_frags)\n\t\t\tbreak;\n\n\t\ttx_bi = &xdp_ring->tx_bi[index];\n\t\ttx_desc = I40E_TX_DESC(xdp_ring, index);\n\n\t\tdata = skb_frag_address(&sinfo->frags[i]);\n\t\tsize = skb_frag_size(&sinfo->frags[i]);\n\t\ti++;\n\t}\n\n\ttx_desc->cmd_type_offset_bsz |=\n\t\tcpu_to_le64(I40E_TXD_CMD << I40E_TXD_QW1_CMD_SHIFT);\n\n\t \n\tsmp_wmb();\n\n\txdp_ring->xdp_tx_active++;\n\n\ttx_head->next_to_watch = tx_desc;\n\txdp_ring->next_to_use = index;\n\n\treturn I40E_XDP_TX;\n\nunmap:\n\tfor (;;) {\n\t\ttx_bi = &xdp_ring->tx_bi[index];\n\t\tif (dma_unmap_len(tx_bi, len))\n\t\t\tdma_unmap_page(xdp_ring->dev,\n\t\t\t\t       dma_unmap_addr(tx_bi, dma),\n\t\t\t\t       dma_unmap_len(tx_bi, len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(tx_bi, len, 0);\n\t\tif (tx_bi == tx_head)\n\t\t\tbreak;\n\n\t\tif (!index)\n\t\t\tindex += xdp_ring->count;\n\t\tindex--;\n\t}\n\n\treturn I40E_XDP_CONSUMED;\n}\n\n \nstatic netdev_tx_t i40e_xmit_frame_ring(struct sk_buff *skb,\n\t\t\t\t\tstruct i40e_ring *tx_ring)\n{\n\tu64 cd_type_cmd_tso_mss = I40E_TX_DESC_DTYPE_CONTEXT;\n\tu32 cd_tunneling = 0, cd_l2tag2 = 0;\n\tstruct i40e_tx_buffer *first;\n\tu32 td_offset = 0;\n\tu32 tx_flags = 0;\n\tu32 td_cmd = 0;\n\tu8 hdr_len = 0;\n\tint tso, count;\n\tint tsyn;\n\n\t \n\tprefetch(skb->data);\n\n\ti40e_trace(xmit_frame_ring, skb, tx_ring);\n\n\tcount = i40e_xmit_descriptor_count(skb);\n\tif (i40e_chk_linearize(skb, count)) {\n\t\tif (__skb_linearize(skb)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tcount = i40e_txd_use_count(skb->len);\n\t\ttx_ring->tx_stats.tx_linearize++;\n\t}\n\n\t \n\tif (i40e_maybe_stop_tx(tx_ring, count + 4 + 1)) {\n\t\ttx_ring->tx_stats.tx_busy++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\tfirst = &tx_ring->tx_bi[tx_ring->next_to_use];\n\tfirst->skb = skb;\n\tfirst->bytecount = skb->len;\n\tfirst->gso_segs = 1;\n\n\t \n\tif (i40e_tx_prepare_vlan_flags(skb, tx_ring, &tx_flags))\n\t\tgoto out_drop;\n\n\ttso = i40e_tso(first, &hdr_len, &cd_type_cmd_tso_mss);\n\n\tif (tso < 0)\n\t\tgoto out_drop;\n\telse if (tso)\n\t\ttx_flags |= I40E_TX_FLAGS_TSO;\n\n\t \n\ttso = i40e_tx_enable_csum(skb, &tx_flags, &td_cmd, &td_offset,\n\t\t\t\t  tx_ring, &cd_tunneling);\n\tif (tso < 0)\n\t\tgoto out_drop;\n\n\ttsyn = i40e_tsyn(tx_ring, skb, tx_flags, &cd_type_cmd_tso_mss);\n\n\tif (tsyn)\n\t\ttx_flags |= I40E_TX_FLAGS_TSYN;\n\n\t \n\ttd_cmd |= I40E_TX_DESC_CMD_ICRC;\n\n\ti40e_create_tx_ctx(tx_ring, cd_type_cmd_tso_mss,\n\t\t\t   cd_tunneling, cd_l2tag2);\n\n\t \n\ti40e_atr(tx_ring, skb, tx_flags);\n\n\tif (i40e_tx_map(tx_ring, skb, first, tx_flags, hdr_len,\n\t\t\ttd_cmd, td_offset))\n\t\tgoto cleanup_tx_tstamp;\n\n\treturn NETDEV_TX_OK;\n\nout_drop:\n\ti40e_trace(xmit_frame_ring_drop, first->skb, tx_ring);\n\tdev_kfree_skb_any(first->skb);\n\tfirst->skb = NULL;\ncleanup_tx_tstamp:\n\tif (unlikely(tx_flags & I40E_TX_FLAGS_TSYN)) {\n\t\tstruct i40e_pf *pf = i40e_netdev_to_pf(tx_ring->netdev);\n\n\t\tdev_kfree_skb_any(pf->ptp_tx_skb);\n\t\tpf->ptp_tx_skb = NULL;\n\t\tclear_bit_unlock(__I40E_PTP_TX_IN_PROGRESS, pf->state);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\n \nnetdev_tx_t i40e_lan_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct i40e_netdev_priv *np = netdev_priv(netdev);\n\tstruct i40e_vsi *vsi = np->vsi;\n\tstruct i40e_ring *tx_ring = vsi->tx_rings[skb->queue_mapping];\n\n\t \n\tif (skb_put_padto(skb, I40E_MIN_TX_LEN))\n\t\treturn NETDEV_TX_OK;\n\n\treturn i40e_xmit_frame_ring(skb, tx_ring);\n}\n\n \nint i40e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,\n\t\t  u32 flags)\n{\n\tstruct i40e_netdev_priv *np = netdev_priv(dev);\n\tunsigned int queue_index = smp_processor_id();\n\tstruct i40e_vsi *vsi = np->vsi;\n\tstruct i40e_pf *pf = vsi->back;\n\tstruct i40e_ring *xdp_ring;\n\tint nxmit = 0;\n\tint i;\n\n\tif (test_bit(__I40E_VSI_DOWN, vsi->state))\n\t\treturn -ENETDOWN;\n\n\tif (!i40e_enabled_xdp_vsi(vsi) || queue_index >= vsi->num_queue_pairs ||\n\t    test_bit(__I40E_CONFIG_BUSY, pf->state))\n\t\treturn -ENXIO;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\txdp_ring = vsi->xdp_rings[queue_index];\n\n\tfor (i = 0; i < n; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\t\tint err;\n\n\t\terr = i40e_xmit_xdp_ring(xdpf, xdp_ring);\n\t\tif (err != I40E_XDP_TX)\n\t\t\tbreak;\n\t\tnxmit++;\n\t}\n\n\tif (unlikely(flags & XDP_XMIT_FLUSH))\n\t\ti40e_xdp_ring_update_tail(xdp_ring);\n\n\treturn nxmit;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}