{
  "module_name": "i40e_xsk.c",
  "hash_id": "ed956ae771bec0d7be43ba209bae312aaf8bf9649e8b53ed48c1e68681490614",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/i40e/i40e_xsk.c",
  "human_readable_source": "\n \n\n#include <linux/bpf_trace.h>\n#include <linux/stringify.h>\n#include <net/xdp_sock_drv.h>\n#include <net/xdp.h>\n\n#include \"i40e.h\"\n#include \"i40e_txrx_common.h\"\n#include \"i40e_xsk.h\"\n\nvoid i40e_clear_rx_bi_zc(struct i40e_ring *rx_ring)\n{\n\tmemset(rx_ring->rx_bi_zc, 0,\n\t       sizeof(*rx_ring->rx_bi_zc) * rx_ring->count);\n}\n\nstatic struct xdp_buff **i40e_rx_bi(struct i40e_ring *rx_ring, u32 idx)\n{\n\treturn &rx_ring->rx_bi_zc[idx];\n}\n\n \nstatic int i40e_realloc_rx_xdp_bi(struct i40e_ring *rx_ring, bool pool_present)\n{\n\tsize_t elem_size = pool_present ? sizeof(*rx_ring->rx_bi_zc) :\n\t\t\t\t\t  sizeof(*rx_ring->rx_bi);\n\tvoid *sw_ring = kcalloc(rx_ring->count, elem_size, GFP_KERNEL);\n\n\tif (!sw_ring)\n\t\treturn -ENOMEM;\n\n\tif (pool_present) {\n\t\tkfree(rx_ring->rx_bi);\n\t\trx_ring->rx_bi = NULL;\n\t\trx_ring->rx_bi_zc = sw_ring;\n\t} else {\n\t\tkfree(rx_ring->rx_bi_zc);\n\t\trx_ring->rx_bi_zc = NULL;\n\t\trx_ring->rx_bi = sw_ring;\n\t}\n\treturn 0;\n}\n\n \nint i40e_realloc_rx_bi_zc(struct i40e_vsi *vsi, bool zc)\n{\n\tstruct i40e_ring *rx_ring;\n\tunsigned long q;\n\n\tfor_each_set_bit(q, vsi->af_xdp_zc_qps, vsi->alloc_queue_pairs) {\n\t\trx_ring = vsi->rx_rings[q];\n\t\tif (i40e_realloc_rx_xdp_bi(rx_ring, zc))\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\n \nstatic int i40e_xsk_pool_enable(struct i40e_vsi *vsi,\n\t\t\t\tstruct xsk_buff_pool *pool,\n\t\t\t\tu16 qid)\n{\n\tstruct net_device *netdev = vsi->netdev;\n\tbool if_running;\n\tint err;\n\n\tif (vsi->type != I40E_VSI_MAIN)\n\t\treturn -EINVAL;\n\n\tif (qid >= vsi->num_queue_pairs)\n\t\treturn -EINVAL;\n\n\tif (qid >= netdev->real_num_rx_queues ||\n\t    qid >= netdev->real_num_tx_queues)\n\t\treturn -EINVAL;\n\n\terr = xsk_pool_dma_map(pool, &vsi->back->pdev->dev, I40E_RX_DMA_ATTR);\n\tif (err)\n\t\treturn err;\n\n\tset_bit(qid, vsi->af_xdp_zc_qps);\n\n\tif_running = netif_running(vsi->netdev) && i40e_enabled_xdp_vsi(vsi);\n\n\tif (if_running) {\n\t\terr = i40e_queue_pair_disable(vsi, qid);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = i40e_realloc_rx_xdp_bi(vsi->rx_rings[qid], true);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = i40e_queue_pair_enable(vsi, qid);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t \n\t\terr = i40e_xsk_wakeup(vsi->netdev, qid, XDP_WAKEUP_RX);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int i40e_xsk_pool_disable(struct i40e_vsi *vsi, u16 qid)\n{\n\tstruct net_device *netdev = vsi->netdev;\n\tstruct xsk_buff_pool *pool;\n\tbool if_running;\n\tint err;\n\n\tpool = xsk_get_pool_from_qid(netdev, qid);\n\tif (!pool)\n\t\treturn -EINVAL;\n\n\tif_running = netif_running(vsi->netdev) && i40e_enabled_xdp_vsi(vsi);\n\n\tif (if_running) {\n\t\terr = i40e_queue_pair_disable(vsi, qid);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tclear_bit(qid, vsi->af_xdp_zc_qps);\n\txsk_pool_dma_unmap(pool, I40E_RX_DMA_ATTR);\n\n\tif (if_running) {\n\t\terr = i40e_realloc_rx_xdp_bi(vsi->rx_rings[qid], false);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = i40e_queue_pair_enable(vsi, qid);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nint i40e_xsk_pool_setup(struct i40e_vsi *vsi, struct xsk_buff_pool *pool,\n\t\t\tu16 qid)\n{\n\treturn pool ? i40e_xsk_pool_enable(vsi, pool, qid) :\n\t\ti40e_xsk_pool_disable(vsi, qid);\n}\n\n \nstatic int i40e_run_xdp_zc(struct i40e_ring *rx_ring, struct xdp_buff *xdp,\n\t\t\t   struct bpf_prog *xdp_prog)\n{\n\tint err, result = I40E_XDP_PASS;\n\tstruct i40e_ring *xdp_ring;\n\tu32 act;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\n\tif (likely(act == XDP_REDIRECT)) {\n\t\terr = xdp_do_redirect(rx_ring->netdev, xdp, xdp_prog);\n\t\tif (!err)\n\t\t\treturn I40E_XDP_REDIR;\n\t\tif (xsk_uses_need_wakeup(rx_ring->xsk_pool) && err == -ENOBUFS)\n\t\t\tresult = I40E_XDP_EXIT;\n\t\telse\n\t\t\tresult = I40E_XDP_CONSUMED;\n\t\tgoto out_failure;\n\t}\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\txdp_ring = rx_ring->vsi->xdp_rings[rx_ring->queue_index];\n\t\tresult = i40e_xmit_xdp_tx_ring(xdp, xdp_ring);\n\t\tif (result == I40E_XDP_CONSUMED)\n\t\t\tgoto out_failure;\n\t\tbreak;\n\tcase XDP_DROP:\n\t\tresult = I40E_XDP_CONSUMED;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(rx_ring->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\tresult = I40E_XDP_CONSUMED;\nout_failure:\n\t\ttrace_xdp_exception(rx_ring->netdev, xdp_prog, act);\n\t}\n\treturn result;\n}\n\nbool i40e_alloc_rx_buffers_zc(struct i40e_ring *rx_ring, u16 count)\n{\n\tu16 ntu = rx_ring->next_to_use;\n\tunion i40e_rx_desc *rx_desc;\n\tstruct xdp_buff **xdp;\n\tu32 nb_buffs, i;\n\tdma_addr_t dma;\n\n\trx_desc = I40E_RX_DESC(rx_ring, ntu);\n\txdp = i40e_rx_bi(rx_ring, ntu);\n\n\tnb_buffs = min_t(u16, count, rx_ring->count - ntu);\n\tnb_buffs = xsk_buff_alloc_batch(rx_ring->xsk_pool, xdp, nb_buffs);\n\tif (!nb_buffs)\n\t\treturn false;\n\n\ti = nb_buffs;\n\twhile (i--) {\n\t\tdma = xsk_buff_xdp_get_dma(*xdp);\n\t\trx_desc->read.pkt_addr = cpu_to_le64(dma);\n\t\trx_desc->read.hdr_addr = 0;\n\n\t\trx_desc++;\n\t\txdp++;\n\t}\n\n\tntu += nb_buffs;\n\tif (ntu == rx_ring->count) {\n\t\trx_desc = I40E_RX_DESC(rx_ring, 0);\n\t\tntu = 0;\n\t}\n\n\t \n\trx_desc->wb.qword1.status_error_len = 0;\n\ti40e_release_rx_desc(rx_ring, ntu);\n\n\treturn count == nb_buffs;\n}\n\n \nstatic struct sk_buff *i40e_construct_skb_zc(struct i40e_ring *rx_ring,\n\t\t\t\t\t     struct xdp_buff *xdp)\n{\n\tunsigned int totalsize = xdp->data_end - xdp->data_meta;\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n\tstruct skb_shared_info *sinfo = NULL;\n\tstruct sk_buff *skb;\n\tu32 nr_frags = 0;\n\n\tif (unlikely(xdp_buff_has_frags(xdp))) {\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tnr_frags = sinfo->nr_frags;\n\t}\n\tnet_prefetch(xdp->data_meta);\n\n\t \n\tskb = __napi_alloc_skb(&rx_ring->q_vector->napi, totalsize,\n\t\t\t       GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!skb))\n\t\tgoto out;\n\n\tmemcpy(__skb_put(skb, totalsize), xdp->data_meta,\n\t       ALIGN(totalsize, sizeof(long)));\n\n\tif (metasize) {\n\t\tskb_metadata_set(skb, metasize);\n\t\t__skb_pull(skb, metasize);\n\t}\n\n\tif (likely(!xdp_buff_has_frags(xdp)))\n\t\tgoto out;\n\n\tfor (int i = 0; i < nr_frags; i++) {\n\t\tstruct skb_shared_info *skinfo = skb_shinfo(skb);\n\t\tskb_frag_t *frag = &sinfo->frags[i];\n\t\tstruct page *page;\n\t\tvoid *addr;\n\n\t\tpage = dev_alloc_page();\n\t\tif (!page) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NULL;\n\t\t}\n\t\taddr = page_to_virt(page);\n\n\t\tmemcpy(addr, skb_frag_page(frag), skb_frag_size(frag));\n\n\t\t__skb_fill_page_desc_noacc(skinfo, skinfo->nr_frags++,\n\t\t\t\t\t   addr, 0, skb_frag_size(frag));\n\t}\n\nout:\n\txsk_buff_free(xdp);\n\treturn skb;\n}\n\nstatic void i40e_handle_xdp_result_zc(struct i40e_ring *rx_ring,\n\t\t\t\t      struct xdp_buff *xdp_buff,\n\t\t\t\t      union i40e_rx_desc *rx_desc,\n\t\t\t\t      unsigned int *rx_packets,\n\t\t\t\t      unsigned int *rx_bytes,\n\t\t\t\t      unsigned int xdp_res,\n\t\t\t\t      bool *failure)\n{\n\tstruct sk_buff *skb;\n\n\t*rx_packets = 1;\n\t*rx_bytes = xdp_get_buff_len(xdp_buff);\n\n\tif (likely(xdp_res == I40E_XDP_REDIR) || xdp_res == I40E_XDP_TX)\n\t\treturn;\n\n\tif (xdp_res == I40E_XDP_EXIT) {\n\t\t*failure = true;\n\t\treturn;\n\t}\n\n\tif (xdp_res == I40E_XDP_CONSUMED) {\n\t\txsk_buff_free(xdp_buff);\n\t\treturn;\n\t}\n\tif (xdp_res == I40E_XDP_PASS) {\n\t\t \n\t\tskb = i40e_construct_skb_zc(rx_ring, xdp_buff);\n\t\tif (!skb) {\n\t\t\trx_ring->rx_stats.alloc_buff_failed++;\n\t\t\t*rx_packets = 0;\n\t\t\t*rx_bytes = 0;\n\t\t\treturn;\n\t\t}\n\n\t\tif (eth_skb_pad(skb)) {\n\t\t\t*rx_packets = 0;\n\t\t\t*rx_bytes = 0;\n\t\t\treturn;\n\t\t}\n\n\t\ti40e_process_skb_fields(rx_ring, rx_desc, skb);\n\t\tnapi_gro_receive(&rx_ring->q_vector->napi, skb);\n\t\treturn;\n\t}\n\n\t \n\tWARN_ON_ONCE(1);\n}\n\nstatic int\ni40e_add_xsk_frag(struct i40e_ring *rx_ring, struct xdp_buff *first,\n\t\t  struct xdp_buff *xdp, const unsigned int size)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(first);\n\n\tif (!xdp_buff_has_frags(first)) {\n\t\tsinfo->nr_frags = 0;\n\t\tsinfo->xdp_frags_size = 0;\n\t\txdp_buff_set_frags_flag(first);\n\t}\n\n\tif (unlikely(sinfo->nr_frags == MAX_SKB_FRAGS)) {\n\t\txsk_buff_free(first);\n\t\treturn -ENOMEM;\n\t}\n\n\t__skb_fill_page_desc_noacc(sinfo, sinfo->nr_frags++,\n\t\t\t\t   virt_to_page(xdp->data_hard_start), 0, size);\n\tsinfo->xdp_frags_size += size;\n\txsk_buff_add_frag(xdp);\n\n\treturn 0;\n}\n\n \nint i40e_clean_rx_irq_zc(struct i40e_ring *rx_ring, int budget)\n{\n\tunsigned int total_rx_bytes = 0, total_rx_packets = 0;\n\tu16 next_to_process = rx_ring->next_to_process;\n\tu16 next_to_clean = rx_ring->next_to_clean;\n\tunsigned int xdp_res, xdp_xmit = 0;\n\tstruct xdp_buff *first = NULL;\n\tu32 count = rx_ring->count;\n\tstruct bpf_prog *xdp_prog;\n\tu32 entries_to_alloc;\n\tbool failure = false;\n\n\tif (next_to_process != next_to_clean)\n\t\tfirst = *i40e_rx_bi(rx_ring, next_to_clean);\n\n\t \n\txdp_prog = READ_ONCE(rx_ring->xdp_prog);\n\n\twhile (likely(total_rx_packets < (unsigned int)budget)) {\n\t\tunion i40e_rx_desc *rx_desc;\n\t\tunsigned int rx_packets;\n\t\tunsigned int rx_bytes;\n\t\tstruct xdp_buff *bi;\n\t\tunsigned int size;\n\t\tu64 qword;\n\n\t\trx_desc = I40E_RX_DESC(rx_ring, next_to_process);\n\t\tqword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);\n\n\t\t \n\t\tdma_rmb();\n\n\t\tif (i40e_rx_is_programming_status(qword)) {\n\t\t\ti40e_clean_programming_status(rx_ring,\n\t\t\t\t\t\t      rx_desc->raw.qword[0],\n\t\t\t\t\t\t      qword);\n\t\t\tbi = *i40e_rx_bi(rx_ring, next_to_process);\n\t\t\txsk_buff_free(bi);\n\t\t\tif (++next_to_process == count)\n\t\t\t\tnext_to_process = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tsize = (qword & I40E_RXD_QW1_LENGTH_PBUF_MASK) >>\n\t\t       I40E_RXD_QW1_LENGTH_PBUF_SHIFT;\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\tbi = *i40e_rx_bi(rx_ring, next_to_process);\n\t\txsk_buff_set_size(bi, size);\n\t\txsk_buff_dma_sync_for_cpu(bi, rx_ring->xsk_pool);\n\n\t\tif (!first)\n\t\t\tfirst = bi;\n\t\telse if (i40e_add_xsk_frag(rx_ring, first, bi, size))\n\t\t\tbreak;\n\n\t\tif (++next_to_process == count)\n\t\t\tnext_to_process = 0;\n\n\t\tif (i40e_is_non_eop(rx_ring, rx_desc))\n\t\t\tcontinue;\n\n\t\txdp_res = i40e_run_xdp_zc(rx_ring, first, xdp_prog);\n\t\ti40e_handle_xdp_result_zc(rx_ring, first, rx_desc, &rx_packets,\n\t\t\t\t\t  &rx_bytes, xdp_res, &failure);\n\t\tfirst->flags = 0;\n\t\tnext_to_clean = next_to_process;\n\t\tif (failure)\n\t\t\tbreak;\n\t\ttotal_rx_packets += rx_packets;\n\t\ttotal_rx_bytes += rx_bytes;\n\t\txdp_xmit |= xdp_res & (I40E_XDP_TX | I40E_XDP_REDIR);\n\t\tfirst = NULL;\n\t}\n\n\trx_ring->next_to_clean = next_to_clean;\n\trx_ring->next_to_process = next_to_process;\n\n\tentries_to_alloc = I40E_DESC_UNUSED(rx_ring);\n\tif (entries_to_alloc >= I40E_RX_BUFFER_WRITE)\n\t\tfailure |= !i40e_alloc_rx_buffers_zc(rx_ring, entries_to_alloc);\n\n\ti40e_finalize_xdp_rx(rx_ring, xdp_xmit);\n\ti40e_update_rx_stats(rx_ring, total_rx_bytes, total_rx_packets);\n\n\tif (xsk_uses_need_wakeup(rx_ring->xsk_pool)) {\n\t\tif (failure || next_to_clean == rx_ring->next_to_use)\n\t\t\txsk_set_rx_need_wakeup(rx_ring->xsk_pool);\n\t\telse\n\t\t\txsk_clear_rx_need_wakeup(rx_ring->xsk_pool);\n\n\t\treturn (int)total_rx_packets;\n\t}\n\treturn failure ? budget : (int)total_rx_packets;\n}\n\nstatic void i40e_xmit_pkt(struct i40e_ring *xdp_ring, struct xdp_desc *desc,\n\t\t\t  unsigned int *total_bytes)\n{\n\tu32 cmd = I40E_TX_DESC_CMD_ICRC | xsk_is_eop_desc(desc);\n\tstruct i40e_tx_desc *tx_desc;\n\tdma_addr_t dma;\n\n\tdma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, desc->addr);\n\txsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma, desc->len);\n\n\ttx_desc = I40E_TX_DESC(xdp_ring, xdp_ring->next_to_use++);\n\ttx_desc->buffer_addr = cpu_to_le64(dma);\n\ttx_desc->cmd_type_offset_bsz = build_ctob(cmd, 0, desc->len, 0);\n\n\t*total_bytes += desc->len;\n}\n\nstatic void i40e_xmit_pkt_batch(struct i40e_ring *xdp_ring, struct xdp_desc *desc,\n\t\t\t\tunsigned int *total_bytes)\n{\n\tu16 ntu = xdp_ring->next_to_use;\n\tstruct i40e_tx_desc *tx_desc;\n\tdma_addr_t dma;\n\tu32 i;\n\n\tloop_unrolled_for(i = 0; i < PKTS_PER_BATCH; i++) {\n\t\tu32 cmd = I40E_TX_DESC_CMD_ICRC | xsk_is_eop_desc(&desc[i]);\n\n\t\tdma = xsk_buff_raw_get_dma(xdp_ring->xsk_pool, desc[i].addr);\n\t\txsk_buff_raw_dma_sync_for_device(xdp_ring->xsk_pool, dma, desc[i].len);\n\n\t\ttx_desc = I40E_TX_DESC(xdp_ring, ntu++);\n\t\ttx_desc->buffer_addr = cpu_to_le64(dma);\n\t\ttx_desc->cmd_type_offset_bsz = build_ctob(cmd, 0, desc[i].len, 0);\n\n\t\t*total_bytes += desc[i].len;\n\t}\n\n\txdp_ring->next_to_use = ntu;\n}\n\nstatic void i40e_fill_tx_hw_ring(struct i40e_ring *xdp_ring, struct xdp_desc *descs, u32 nb_pkts,\n\t\t\t\t unsigned int *total_bytes)\n{\n\tu32 batched, leftover, i;\n\n\tbatched = nb_pkts & ~(PKTS_PER_BATCH - 1);\n\tleftover = nb_pkts & (PKTS_PER_BATCH - 1);\n\tfor (i = 0; i < batched; i += PKTS_PER_BATCH)\n\t\ti40e_xmit_pkt_batch(xdp_ring, &descs[i], total_bytes);\n\tfor (i = batched; i < batched + leftover; i++)\n\t\ti40e_xmit_pkt(xdp_ring, &descs[i], total_bytes);\n}\n\nstatic void i40e_set_rs_bit(struct i40e_ring *xdp_ring)\n{\n\tu16 ntu = xdp_ring->next_to_use ? xdp_ring->next_to_use - 1 : xdp_ring->count - 1;\n\tstruct i40e_tx_desc *tx_desc;\n\n\ttx_desc = I40E_TX_DESC(xdp_ring, ntu);\n\ttx_desc->cmd_type_offset_bsz |= cpu_to_le64(I40E_TX_DESC_CMD_RS << I40E_TXD_QW1_CMD_SHIFT);\n}\n\n \nstatic bool i40e_xmit_zc(struct i40e_ring *xdp_ring, unsigned int budget)\n{\n\tstruct xdp_desc *descs = xdp_ring->xsk_pool->tx_descs;\n\tu32 nb_pkts, nb_processed = 0;\n\tunsigned int total_bytes = 0;\n\n\tnb_pkts = xsk_tx_peek_release_desc_batch(xdp_ring->xsk_pool, budget);\n\tif (!nb_pkts)\n\t\treturn true;\n\n\tif (xdp_ring->next_to_use + nb_pkts >= xdp_ring->count) {\n\t\tnb_processed = xdp_ring->count - xdp_ring->next_to_use;\n\t\ti40e_fill_tx_hw_ring(xdp_ring, descs, nb_processed, &total_bytes);\n\t\txdp_ring->next_to_use = 0;\n\t}\n\n\ti40e_fill_tx_hw_ring(xdp_ring, &descs[nb_processed], nb_pkts - nb_processed,\n\t\t\t     &total_bytes);\n\n\t \n\ti40e_set_rs_bit(xdp_ring);\n\ti40e_xdp_ring_update_tail(xdp_ring);\n\n\ti40e_update_tx_stats(xdp_ring, nb_pkts, total_bytes);\n\n\treturn nb_pkts < budget;\n}\n\n \nstatic void i40e_clean_xdp_tx_buffer(struct i40e_ring *tx_ring,\n\t\t\t\t     struct i40e_tx_buffer *tx_bi)\n{\n\txdp_return_frame(tx_bi->xdpf);\n\ttx_ring->xdp_tx_active--;\n\tdma_unmap_single(tx_ring->dev,\n\t\t\t dma_unmap_addr(tx_bi, dma),\n\t\t\t dma_unmap_len(tx_bi, len), DMA_TO_DEVICE);\n\tdma_unmap_len_set(tx_bi, len, 0);\n}\n\n \nbool i40e_clean_xdp_tx_irq(struct i40e_vsi *vsi, struct i40e_ring *tx_ring)\n{\n\tstruct xsk_buff_pool *bp = tx_ring->xsk_pool;\n\tu32 i, completed_frames, xsk_frames = 0;\n\tu32 head_idx = i40e_get_head(tx_ring);\n\tstruct i40e_tx_buffer *tx_bi;\n\tunsigned int ntc;\n\n\tif (head_idx < tx_ring->next_to_clean)\n\t\thead_idx += tx_ring->count;\n\tcompleted_frames = head_idx - tx_ring->next_to_clean;\n\n\tif (completed_frames == 0)\n\t\tgoto out_xmit;\n\n\tif (likely(!tx_ring->xdp_tx_active)) {\n\t\txsk_frames = completed_frames;\n\t\tgoto skip;\n\t}\n\n\tntc = tx_ring->next_to_clean;\n\n\tfor (i = 0; i < completed_frames; i++) {\n\t\ttx_bi = &tx_ring->tx_bi[ntc];\n\n\t\tif (tx_bi->xdpf) {\n\t\t\ti40e_clean_xdp_tx_buffer(tx_ring, tx_bi);\n\t\t\ttx_bi->xdpf = NULL;\n\t\t} else {\n\t\t\txsk_frames++;\n\t\t}\n\n\t\tif (++ntc >= tx_ring->count)\n\t\t\tntc = 0;\n\t}\n\nskip:\n\ttx_ring->next_to_clean += completed_frames;\n\tif (unlikely(tx_ring->next_to_clean >= tx_ring->count))\n\t\ttx_ring->next_to_clean -= tx_ring->count;\n\n\tif (xsk_frames)\n\t\txsk_tx_completed(bp, xsk_frames);\n\n\ti40e_arm_wb(tx_ring, vsi, completed_frames);\n\nout_xmit:\n\tif (xsk_uses_need_wakeup(tx_ring->xsk_pool))\n\t\txsk_set_tx_need_wakeup(tx_ring->xsk_pool);\n\n\treturn i40e_xmit_zc(tx_ring, I40E_DESC_UNUSED(tx_ring));\n}\n\n \nint i40e_xsk_wakeup(struct net_device *dev, u32 queue_id, u32 flags)\n{\n\tstruct i40e_netdev_priv *np = netdev_priv(dev);\n\tstruct i40e_vsi *vsi = np->vsi;\n\tstruct i40e_pf *pf = vsi->back;\n\tstruct i40e_ring *ring;\n\n\tif (test_bit(__I40E_CONFIG_BUSY, pf->state))\n\t\treturn -EAGAIN;\n\n\tif (test_bit(__I40E_VSI_DOWN, vsi->state))\n\t\treturn -ENETDOWN;\n\n\tif (!i40e_enabled_xdp_vsi(vsi))\n\t\treturn -EINVAL;\n\n\tif (queue_id >= vsi->num_queue_pairs)\n\t\treturn -EINVAL;\n\n\tif (!vsi->xdp_rings[queue_id]->xsk_pool)\n\t\treturn -EINVAL;\n\n\tring = vsi->xdp_rings[queue_id];\n\n\t \n\tif (!napi_if_scheduled_mark_missed(&ring->q_vector->napi))\n\t\ti40e_force_wb(vsi, ring->q_vector);\n\n\treturn 0;\n}\n\nvoid i40e_xsk_clean_rx_ring(struct i40e_ring *rx_ring)\n{\n\tu16 ntc = rx_ring->next_to_clean;\n\tu16 ntu = rx_ring->next_to_use;\n\n\twhile (ntc != ntu) {\n\t\tstruct xdp_buff *rx_bi = *i40e_rx_bi(rx_ring, ntc);\n\n\t\txsk_buff_free(rx_bi);\n\t\tntc++;\n\t\tif (ntc >= rx_ring->count)\n\t\t\tntc = 0;\n\t}\n}\n\n \nvoid i40e_xsk_clean_tx_ring(struct i40e_ring *tx_ring)\n{\n\tu16 ntc = tx_ring->next_to_clean, ntu = tx_ring->next_to_use;\n\tstruct xsk_buff_pool *bp = tx_ring->xsk_pool;\n\tstruct i40e_tx_buffer *tx_bi;\n\tu32 xsk_frames = 0;\n\n\twhile (ntc != ntu) {\n\t\ttx_bi = &tx_ring->tx_bi[ntc];\n\n\t\tif (tx_bi->xdpf)\n\t\t\ti40e_clean_xdp_tx_buffer(tx_ring, tx_bi);\n\t\telse\n\t\t\txsk_frames++;\n\n\t\ttx_bi->xdpf = NULL;\n\n\t\tntc++;\n\t\tif (ntc >= tx_ring->count)\n\t\t\tntc = 0;\n\t}\n\n\tif (xsk_frames)\n\t\txsk_tx_completed(bp, xsk_frames);\n}\n\n \nbool i40e_xsk_any_rx_ring_enabled(struct i40e_vsi *vsi)\n{\n\tstruct net_device *netdev = vsi->netdev;\n\tint i;\n\n\tfor (i = 0; i < vsi->num_queue_pairs; i++) {\n\t\tif (xsk_get_pool_from_qid(netdev, i))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}