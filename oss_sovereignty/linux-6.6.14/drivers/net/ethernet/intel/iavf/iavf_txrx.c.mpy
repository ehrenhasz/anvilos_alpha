{
  "module_name": "iavf_txrx.c",
  "hash_id": "1c3b1a5b427824f78f0d22898a4e6dbc4d2c74d8740e05a71a6eb5dd5eab23fd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/iavf/iavf_txrx.c",
  "human_readable_source": "\n \n\n#include <linux/prefetch.h>\n\n#include \"iavf.h\"\n#include \"iavf_trace.h\"\n#include \"iavf_prototype.h\"\n\nstatic inline __le64 build_ctob(u32 td_cmd, u32 td_offset, unsigned int size,\n\t\t\t\tu32 td_tag)\n{\n\treturn cpu_to_le64(IAVF_TX_DESC_DTYPE_DATA |\n\t\t\t   ((u64)td_cmd  << IAVF_TXD_QW1_CMD_SHIFT) |\n\t\t\t   ((u64)td_offset << IAVF_TXD_QW1_OFFSET_SHIFT) |\n\t\t\t   ((u64)size  << IAVF_TXD_QW1_TX_BUF_SZ_SHIFT) |\n\t\t\t   ((u64)td_tag  << IAVF_TXD_QW1_L2TAG1_SHIFT));\n}\n\n#define IAVF_TXD_CMD (IAVF_TX_DESC_CMD_EOP | IAVF_TX_DESC_CMD_RS)\n\n \nstatic void iavf_unmap_and_free_tx_resource(struct iavf_ring *ring,\n\t\t\t\t\t    struct iavf_tx_buffer *tx_buffer)\n{\n\tif (tx_buffer->skb) {\n\t\tif (tx_buffer->tx_flags & IAVF_TX_FLAGS_FD_SB)\n\t\t\tkfree(tx_buffer->raw_buf);\n\t\telse\n\t\t\tdev_kfree_skb_any(tx_buffer->skb);\n\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\tdma_unmap_single(ring->dev,\n\t\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t} else if (dma_unmap_len(tx_buffer, len)) {\n\t\tdma_unmap_page(ring->dev,\n\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t       DMA_TO_DEVICE);\n\t}\n\n\ttx_buffer->next_to_watch = NULL;\n\ttx_buffer->skb = NULL;\n\tdma_unmap_len_set(tx_buffer, len, 0);\n\t \n}\n\n \nstatic void iavf_clean_tx_ring(struct iavf_ring *tx_ring)\n{\n\tunsigned long bi_size;\n\tu16 i;\n\n\t \n\tif (!tx_ring->tx_bi)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < tx_ring->count; i++)\n\t\tiavf_unmap_and_free_tx_resource(tx_ring, &tx_ring->tx_bi[i]);\n\n\tbi_size = sizeof(struct iavf_tx_buffer) * tx_ring->count;\n\tmemset(tx_ring->tx_bi, 0, bi_size);\n\n\t \n\tmemset(tx_ring->desc, 0, tx_ring->size);\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\n\tif (!tx_ring->netdev)\n\t\treturn;\n\n\t \n\tnetdev_tx_reset_queue(txring_txq(tx_ring));\n}\n\n \nvoid iavf_free_tx_resources(struct iavf_ring *tx_ring)\n{\n\tiavf_clean_tx_ring(tx_ring);\n\tkfree(tx_ring->tx_bi);\n\ttx_ring->tx_bi = NULL;\n\n\tif (tx_ring->desc) {\n\t\tdma_free_coherent(tx_ring->dev, tx_ring->size,\n\t\t\t\t  tx_ring->desc, tx_ring->dma);\n\t\ttx_ring->desc = NULL;\n\t}\n}\n\n \nstatic u32 iavf_get_tx_pending(struct iavf_ring *ring, bool in_sw)\n{\n\tu32 head, tail;\n\n\t \n\thead = ring->next_to_clean;\n\ttail = ring->next_to_use;\n\n\tif (head != tail)\n\t\treturn (head < tail) ?\n\t\t\ttail - head : (tail + ring->count - head);\n\n\treturn 0;\n}\n\n \nstatic void iavf_force_wb(struct iavf_vsi *vsi, struct iavf_q_vector *q_vector)\n{\n\tu32 val = IAVF_VFINT_DYN_CTLN1_INTENA_MASK |\n\t\t  IAVF_VFINT_DYN_CTLN1_ITR_INDX_MASK |  \n\t\t  IAVF_VFINT_DYN_CTLN1_SWINT_TRIG_MASK |\n\t\t  IAVF_VFINT_DYN_CTLN1_SW_ITR_INDX_ENA_MASK\n\t\t   ;\n\n\twr32(&vsi->back->hw,\n\t     IAVF_VFINT_DYN_CTLN1(q_vector->reg_idx),\n\t     val);\n}\n\n \nvoid iavf_detect_recover_hung(struct iavf_vsi *vsi)\n{\n\tstruct iavf_ring *tx_ring = NULL;\n\tstruct net_device *netdev;\n\tunsigned int i;\n\tint packets;\n\n\tif (!vsi)\n\t\treturn;\n\n\tif (test_bit(__IAVF_VSI_DOWN, vsi->state))\n\t\treturn;\n\n\tnetdev = vsi->netdev;\n\tif (!netdev)\n\t\treturn;\n\n\tif (!netif_carrier_ok(netdev))\n\t\treturn;\n\n\tfor (i = 0; i < vsi->back->num_active_queues; i++) {\n\t\ttx_ring = &vsi->back->tx_rings[i];\n\t\tif (tx_ring && tx_ring->desc) {\n\t\t\t \n\t\t\tpackets = tx_ring->stats.packets & INT_MAX;\n\t\t\tif (tx_ring->tx_stats.prev_pkt_ctr == packets) {\n\t\t\t\tiavf_force_wb(vsi, tx_ring->q_vector);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tsmp_rmb();\n\t\t\ttx_ring->tx_stats.prev_pkt_ctr =\n\t\t\t  iavf_get_tx_pending(tx_ring, true) ? packets : -1;\n\t\t}\n\t}\n}\n\n#define WB_STRIDE 4\n\n \nstatic bool iavf_clean_tx_irq(struct iavf_vsi *vsi,\n\t\t\t      struct iavf_ring *tx_ring, int napi_budget)\n{\n\tint i = tx_ring->next_to_clean;\n\tstruct iavf_tx_buffer *tx_buf;\n\tstruct iavf_tx_desc *tx_desc;\n\tunsigned int total_bytes = 0, total_packets = 0;\n\tunsigned int budget = IAVF_DEFAULT_IRQ_WORK;\n\n\ttx_buf = &tx_ring->tx_bi[i];\n\ttx_desc = IAVF_TX_DESC(tx_ring, i);\n\ti -= tx_ring->count;\n\n\tdo {\n\t\tstruct iavf_tx_desc *eop_desc = tx_buf->next_to_watch;\n\n\t\t \n\t\tif (!eop_desc)\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\tiavf_trace(clean_tx_irq, tx_ring, tx_desc, tx_buf);\n\t\t \n\t\tif (!(eop_desc->cmd_type_offset_bsz &\n\t\t      cpu_to_le64(IAVF_TX_DESC_DTYPE_DESC_DONE)))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buf->next_to_watch = NULL;\n\n\t\t \n\t\ttotal_bytes += tx_buf->bytecount;\n\t\ttotal_packets += tx_buf->gso_segs;\n\n\t\t \n\t\tnapi_consume_skb(tx_buf->skb, napi_budget);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buf, dma),\n\t\t\t\t dma_unmap_len(tx_buf, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\ttx_buf->skb = NULL;\n\t\tdma_unmap_len_set(tx_buf, len, 0);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\tiavf_trace(clean_tx_irq_unmap,\n\t\t\t\t   tx_ring, tx_desc, tx_buf);\n\n\t\t\ttx_buf++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(!i)) {\n\t\t\t\ti -= tx_ring->count;\n\t\t\t\ttx_buf = tx_ring->tx_bi;\n\t\t\t\ttx_desc = IAVF_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buf, len)) {\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buf, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buf, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\t\tdma_unmap_len_set(tx_buf, len, 0);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\ttx_buf++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buf = tx_ring->tx_bi;\n\t\t\ttx_desc = IAVF_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\tprefetch(tx_desc);\n\n\t\t \n\t\tbudget--;\n\t} while (likely(budget));\n\n\ti += tx_ring->count;\n\ttx_ring->next_to_clean = i;\n\tu64_stats_update_begin(&tx_ring->syncp);\n\ttx_ring->stats.bytes += total_bytes;\n\ttx_ring->stats.packets += total_packets;\n\tu64_stats_update_end(&tx_ring->syncp);\n\ttx_ring->q_vector->tx.total_bytes += total_bytes;\n\ttx_ring->q_vector->tx.total_packets += total_packets;\n\n\tif (tx_ring->flags & IAVF_TXR_FLAGS_WB_ON_ITR) {\n\t\t \n\t\tunsigned int j = iavf_get_tx_pending(tx_ring, false);\n\n\t\tif (budget &&\n\t\t    ((j / WB_STRIDE) == 0) && (j > 0) &&\n\t\t    !test_bit(__IAVF_VSI_DOWN, vsi->state) &&\n\t\t    (IAVF_DESC_UNUSED(tx_ring) != tx_ring->count))\n\t\t\ttx_ring->arm_wb = true;\n\t}\n\n\t \n\tnetdev_tx_completed_queue(txring_txq(tx_ring),\n\t\t\t\t  total_packets, total_bytes);\n\n#define TX_WAKE_THRESHOLD ((s16)(DESC_NEEDED * 2))\n\tif (unlikely(total_packets && netif_carrier_ok(tx_ring->netdev) &&\n\t\t     (IAVF_DESC_UNUSED(tx_ring) >= TX_WAKE_THRESHOLD))) {\n\t\t \n\t\tsmp_mb();\n\t\tif (__netif_subqueue_stopped(tx_ring->netdev,\n\t\t\t\t\t     tx_ring->queue_index) &&\n\t\t   !test_bit(__IAVF_VSI_DOWN, vsi->state)) {\n\t\t\tnetif_wake_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\t\t\t++tx_ring->tx_stats.restart_queue;\n\t\t}\n\t}\n\n\treturn !!budget;\n}\n\n \nstatic void iavf_enable_wb_on_itr(struct iavf_vsi *vsi,\n\t\t\t\t  struct iavf_q_vector *q_vector)\n{\n\tu16 flags = q_vector->tx.ring[0].flags;\n\tu32 val;\n\n\tif (!(flags & IAVF_TXR_FLAGS_WB_ON_ITR))\n\t\treturn;\n\n\tif (q_vector->arm_wb_state)\n\t\treturn;\n\n\tval = IAVF_VFINT_DYN_CTLN1_WB_ON_ITR_MASK |\n\t      IAVF_VFINT_DYN_CTLN1_ITR_INDX_MASK;  \n\n\twr32(&vsi->back->hw,\n\t     IAVF_VFINT_DYN_CTLN1(q_vector->reg_idx), val);\n\tq_vector->arm_wb_state = true;\n}\n\nstatic inline bool iavf_container_is_rx(struct iavf_q_vector *q_vector,\n\t\t\t\t\tstruct iavf_ring_container *rc)\n{\n\treturn &q_vector->rx == rc;\n}\n\n#define IAVF_AIM_MULTIPLIER_100G\t2560\n#define IAVF_AIM_MULTIPLIER_50G\t\t1280\n#define IAVF_AIM_MULTIPLIER_40G\t\t1024\n#define IAVF_AIM_MULTIPLIER_20G\t\t512\n#define IAVF_AIM_MULTIPLIER_10G\t\t256\n#define IAVF_AIM_MULTIPLIER_1G\t\t32\n\nstatic unsigned int iavf_mbps_itr_multiplier(u32 speed_mbps)\n{\n\tswitch (speed_mbps) {\n\tcase SPEED_100000:\n\t\treturn IAVF_AIM_MULTIPLIER_100G;\n\tcase SPEED_50000:\n\t\treturn IAVF_AIM_MULTIPLIER_50G;\n\tcase SPEED_40000:\n\t\treturn IAVF_AIM_MULTIPLIER_40G;\n\tcase SPEED_25000:\n\tcase SPEED_20000:\n\t\treturn IAVF_AIM_MULTIPLIER_20G;\n\tcase SPEED_10000:\n\tdefault:\n\t\treturn IAVF_AIM_MULTIPLIER_10G;\n\tcase SPEED_1000:\n\tcase SPEED_100:\n\t\treturn IAVF_AIM_MULTIPLIER_1G;\n\t}\n}\n\nstatic unsigned int\niavf_virtchnl_itr_multiplier(enum virtchnl_link_speed speed_virtchnl)\n{\n\tswitch (speed_virtchnl) {\n\tcase VIRTCHNL_LINK_SPEED_40GB:\n\t\treturn IAVF_AIM_MULTIPLIER_40G;\n\tcase VIRTCHNL_LINK_SPEED_25GB:\n\tcase VIRTCHNL_LINK_SPEED_20GB:\n\t\treturn IAVF_AIM_MULTIPLIER_20G;\n\tcase VIRTCHNL_LINK_SPEED_10GB:\n\tdefault:\n\t\treturn IAVF_AIM_MULTIPLIER_10G;\n\tcase VIRTCHNL_LINK_SPEED_1GB:\n\tcase VIRTCHNL_LINK_SPEED_100MB:\n\t\treturn IAVF_AIM_MULTIPLIER_1G;\n\t}\n}\n\nstatic unsigned int iavf_itr_divisor(struct iavf_adapter *adapter)\n{\n\tif (ADV_LINK_SUPPORT(adapter))\n\t\treturn IAVF_ITR_ADAPTIVE_MIN_INC *\n\t\t\tiavf_mbps_itr_multiplier(adapter->link_speed_mbps);\n\telse\n\t\treturn IAVF_ITR_ADAPTIVE_MIN_INC *\n\t\t\tiavf_virtchnl_itr_multiplier(adapter->link_speed);\n}\n\n \nstatic void iavf_update_itr(struct iavf_q_vector *q_vector,\n\t\t\t    struct iavf_ring_container *rc)\n{\n\tunsigned int avg_wire_size, packets, bytes, itr;\n\tunsigned long next_update = jiffies;\n\n\t \n\tif (!rc->ring || !ITR_IS_DYNAMIC(rc->ring->itr_setting))\n\t\treturn;\n\n\t \n\titr = iavf_container_is_rx(q_vector, rc) ?\n\t      IAVF_ITR_ADAPTIVE_MIN_USECS | IAVF_ITR_ADAPTIVE_LATENCY :\n\t      IAVF_ITR_ADAPTIVE_MAX_USECS | IAVF_ITR_ADAPTIVE_LATENCY;\n\n\t \n\tif (time_after(next_update, rc->next_update))\n\t\tgoto clear_counts;\n\n\t \n\tif (q_vector->itr_countdown) {\n\t\titr = rc->target_itr;\n\t\tgoto clear_counts;\n\t}\n\n\tpackets = rc->total_packets;\n\tbytes = rc->total_bytes;\n\n\tif (iavf_container_is_rx(q_vector, rc)) {\n\t\t \n\t\tif (packets && packets < 4 && bytes < 9000 &&\n\t\t    (q_vector->tx.target_itr & IAVF_ITR_ADAPTIVE_LATENCY)) {\n\t\t\titr = IAVF_ITR_ADAPTIVE_LATENCY;\n\t\t\tgoto adjust_by_size;\n\t\t}\n\t} else if (packets < 4) {\n\t\t \n\t\tif (rc->target_itr == IAVF_ITR_ADAPTIVE_MAX_USECS &&\n\t\t    (q_vector->rx.target_itr & IAVF_ITR_MASK) ==\n\t\t     IAVF_ITR_ADAPTIVE_MAX_USECS)\n\t\t\tgoto clear_counts;\n\t} else if (packets > 32) {\n\t\t \n\t\trc->target_itr &= ~IAVF_ITR_ADAPTIVE_LATENCY;\n\t}\n\n\t \n\tif (packets < 56) {\n\t\titr = rc->target_itr + IAVF_ITR_ADAPTIVE_MIN_INC;\n\t\tif ((itr & IAVF_ITR_MASK) > IAVF_ITR_ADAPTIVE_MAX_USECS) {\n\t\t\titr &= IAVF_ITR_ADAPTIVE_LATENCY;\n\t\t\titr += IAVF_ITR_ADAPTIVE_MAX_USECS;\n\t\t}\n\t\tgoto clear_counts;\n\t}\n\n\tif (packets <= 256) {\n\t\titr = min(q_vector->tx.current_itr, q_vector->rx.current_itr);\n\t\titr &= IAVF_ITR_MASK;\n\n\t\t \n\t\tif (packets <= 112)\n\t\t\tgoto clear_counts;\n\n\t\t \n\t\titr /= 2;\n\t\titr &= IAVF_ITR_MASK;\n\t\tif (itr < IAVF_ITR_ADAPTIVE_MIN_USECS)\n\t\t\titr = IAVF_ITR_ADAPTIVE_MIN_USECS;\n\n\t\tgoto clear_counts;\n\t}\n\n\t \n\titr = IAVF_ITR_ADAPTIVE_BULK;\n\nadjust_by_size:\n\t \n\tavg_wire_size = bytes / packets;\n\n\t \n\tif (avg_wire_size <= 60) {\n\t\t \n\t\tavg_wire_size = 4096;\n\t} else if (avg_wire_size <= 380) {\n\t\t \n\t\tavg_wire_size *= 40;\n\t\tavg_wire_size += 1696;\n\t} else if (avg_wire_size <= 1084) {\n\t\t \n\t\tavg_wire_size *= 15;\n\t\tavg_wire_size += 11452;\n\t} else if (avg_wire_size <= 1980) {\n\t\t \n\t\tavg_wire_size *= 5;\n\t\tavg_wire_size += 22420;\n\t} else {\n\t\t \n\t\tavg_wire_size = 32256;\n\t}\n\n\t \n\tif (itr & IAVF_ITR_ADAPTIVE_LATENCY)\n\t\tavg_wire_size /= 2;\n\n\t \n\titr += DIV_ROUND_UP(avg_wire_size,\n\t\t\t    iavf_itr_divisor(q_vector->adapter)) *\n\t\tIAVF_ITR_ADAPTIVE_MIN_INC;\n\n\tif ((itr & IAVF_ITR_MASK) > IAVF_ITR_ADAPTIVE_MAX_USECS) {\n\t\titr &= IAVF_ITR_ADAPTIVE_LATENCY;\n\t\titr += IAVF_ITR_ADAPTIVE_MAX_USECS;\n\t}\n\nclear_counts:\n\t \n\trc->target_itr = itr;\n\n\t \n\trc->next_update = next_update + 1;\n\n\trc->total_bytes = 0;\n\trc->total_packets = 0;\n}\n\n \nint iavf_setup_tx_descriptors(struct iavf_ring *tx_ring)\n{\n\tstruct device *dev = tx_ring->dev;\n\tint bi_size;\n\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\t \n\tWARN_ON(tx_ring->tx_bi);\n\tbi_size = sizeof(struct iavf_tx_buffer) * tx_ring->count;\n\ttx_ring->tx_bi = kzalloc(bi_size, GFP_KERNEL);\n\tif (!tx_ring->tx_bi)\n\t\tgoto err;\n\n\t \n\ttx_ring->size = tx_ring->count * sizeof(struct iavf_tx_desc);\n\ttx_ring->size = ALIGN(tx_ring->size, 4096);\n\ttx_ring->desc = dma_alloc_coherent(dev, tx_ring->size,\n\t\t\t\t\t   &tx_ring->dma, GFP_KERNEL);\n\tif (!tx_ring->desc) {\n\t\tdev_info(dev, \"Unable to allocate memory for the Tx descriptor ring, size=%d\\n\",\n\t\t\t tx_ring->size);\n\t\tgoto err;\n\t}\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\ttx_ring->tx_stats.prev_pkt_ctr = -1;\n\treturn 0;\n\nerr:\n\tkfree(tx_ring->tx_bi);\n\ttx_ring->tx_bi = NULL;\n\treturn -ENOMEM;\n}\n\n \nstatic void iavf_clean_rx_ring(struct iavf_ring *rx_ring)\n{\n\tunsigned long bi_size;\n\tu16 i;\n\n\t \n\tif (!rx_ring->rx_bi)\n\t\treturn;\n\n\tif (rx_ring->skb) {\n\t\tdev_kfree_skb(rx_ring->skb);\n\t\trx_ring->skb = NULL;\n\t}\n\n\t \n\tfor (i = 0; i < rx_ring->count; i++) {\n\t\tstruct iavf_rx_buffer *rx_bi = &rx_ring->rx_bi[i];\n\n\t\tif (!rx_bi->page)\n\t\t\tcontinue;\n\n\t\t \n\t\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t\t      rx_bi->dma,\n\t\t\t\t\t      rx_bi->page_offset,\n\t\t\t\t\t      rx_ring->rx_buf_len,\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev, rx_bi->dma,\n\t\t\t\t     iavf_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t     IAVF_RX_DMA_ATTR);\n\n\t\t__page_frag_cache_drain(rx_bi->page, rx_bi->pagecnt_bias);\n\n\t\trx_bi->page = NULL;\n\t\trx_bi->page_offset = 0;\n\t}\n\n\tbi_size = sizeof(struct iavf_rx_buffer) * rx_ring->count;\n\tmemset(rx_ring->rx_bi, 0, bi_size);\n\n\t \n\tmemset(rx_ring->desc, 0, rx_ring->size);\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n}\n\n \nvoid iavf_free_rx_resources(struct iavf_ring *rx_ring)\n{\n\tiavf_clean_rx_ring(rx_ring);\n\tkfree(rx_ring->rx_bi);\n\trx_ring->rx_bi = NULL;\n\n\tif (rx_ring->desc) {\n\t\tdma_free_coherent(rx_ring->dev, rx_ring->size,\n\t\t\t\t  rx_ring->desc, rx_ring->dma);\n\t\trx_ring->desc = NULL;\n\t}\n}\n\n \nint iavf_setup_rx_descriptors(struct iavf_ring *rx_ring)\n{\n\tstruct device *dev = rx_ring->dev;\n\tint bi_size;\n\n\t \n\tWARN_ON(rx_ring->rx_bi);\n\tbi_size = sizeof(struct iavf_rx_buffer) * rx_ring->count;\n\trx_ring->rx_bi = kzalloc(bi_size, GFP_KERNEL);\n\tif (!rx_ring->rx_bi)\n\t\tgoto err;\n\n\tu64_stats_init(&rx_ring->syncp);\n\n\t \n\trx_ring->size = rx_ring->count * sizeof(union iavf_32byte_rx_desc);\n\trx_ring->size = ALIGN(rx_ring->size, 4096);\n\trx_ring->desc = dma_alloc_coherent(dev, rx_ring->size,\n\t\t\t\t\t   &rx_ring->dma, GFP_KERNEL);\n\n\tif (!rx_ring->desc) {\n\t\tdev_info(dev, \"Unable to allocate memory for the Rx descriptor ring, size=%d\\n\",\n\t\t\t rx_ring->size);\n\t\tgoto err;\n\t}\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n\n\treturn 0;\nerr:\n\tkfree(rx_ring->rx_bi);\n\trx_ring->rx_bi = NULL;\n\treturn -ENOMEM;\n}\n\n \nstatic inline void iavf_release_rx_desc(struct iavf_ring *rx_ring, u32 val)\n{\n\trx_ring->next_to_use = val;\n\n\t \n\trx_ring->next_to_alloc = val;\n\n\t \n\twmb();\n\twritel(val, rx_ring->tail);\n}\n\n \nstatic inline unsigned int iavf_rx_offset(struct iavf_ring *rx_ring)\n{\n\treturn ring_uses_build_skb(rx_ring) ? IAVF_SKB_PAD : 0;\n}\n\n \nstatic bool iavf_alloc_mapped_page(struct iavf_ring *rx_ring,\n\t\t\t\t   struct iavf_rx_buffer *bi)\n{\n\tstruct page *page = bi->page;\n\tdma_addr_t dma;\n\n\t \n\tif (likely(page)) {\n\t\trx_ring->rx_stats.page_reuse_count++;\n\t\treturn true;\n\t}\n\n\t \n\tpage = dev_alloc_pages(iavf_rx_pg_order(rx_ring));\n\tif (unlikely(!page)) {\n\t\trx_ring->rx_stats.alloc_page_failed++;\n\t\treturn false;\n\t}\n\n\t \n\tdma = dma_map_page_attrs(rx_ring->dev, page, 0,\n\t\t\t\t iavf_rx_pg_size(rx_ring),\n\t\t\t\t DMA_FROM_DEVICE,\n\t\t\t\t IAVF_RX_DMA_ATTR);\n\n\t \n\tif (dma_mapping_error(rx_ring->dev, dma)) {\n\t\t__free_pages(page, iavf_rx_pg_order(rx_ring));\n\t\trx_ring->rx_stats.alloc_page_failed++;\n\t\treturn false;\n\t}\n\n\tbi->dma = dma;\n\tbi->page = page;\n\tbi->page_offset = iavf_rx_offset(rx_ring);\n\n\t \n\tbi->pagecnt_bias = 1;\n\n\treturn true;\n}\n\n \nstatic void iavf_receive_skb(struct iavf_ring *rx_ring,\n\t\t\t     struct sk_buff *skb, u16 vlan_tag)\n{\n\tstruct iavf_q_vector *q_vector = rx_ring->q_vector;\n\n\tif ((rx_ring->netdev->features & NETIF_F_HW_VLAN_CTAG_RX) &&\n\t    (vlan_tag & VLAN_VID_MASK))\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tag);\n\telse if ((rx_ring->netdev->features & NETIF_F_HW_VLAN_STAG_RX) &&\n\t\t vlan_tag & VLAN_VID_MASK)\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021AD), vlan_tag);\n\n\tnapi_gro_receive(&q_vector->napi, skb);\n}\n\n \nbool iavf_alloc_rx_buffers(struct iavf_ring *rx_ring, u16 cleaned_count)\n{\n\tu16 ntu = rx_ring->next_to_use;\n\tunion iavf_rx_desc *rx_desc;\n\tstruct iavf_rx_buffer *bi;\n\n\t \n\tif (!rx_ring->netdev || !cleaned_count)\n\t\treturn false;\n\n\trx_desc = IAVF_RX_DESC(rx_ring, ntu);\n\tbi = &rx_ring->rx_bi[ntu];\n\n\tdo {\n\t\tif (!iavf_alloc_mapped_page(rx_ring, bi))\n\t\t\tgoto no_buffers;\n\n\t\t \n\t\tdma_sync_single_range_for_device(rx_ring->dev, bi->dma,\n\t\t\t\t\t\t bi->page_offset,\n\t\t\t\t\t\t rx_ring->rx_buf_len,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t \n\t\trx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);\n\n\t\trx_desc++;\n\t\tbi++;\n\t\tntu++;\n\t\tif (unlikely(ntu == rx_ring->count)) {\n\t\t\trx_desc = IAVF_RX_DESC(rx_ring, 0);\n\t\t\tbi = rx_ring->rx_bi;\n\t\t\tntu = 0;\n\t\t}\n\n\t\t \n\t\trx_desc->wb.qword1.status_error_len = 0;\n\n\t\tcleaned_count--;\n\t} while (cleaned_count);\n\n\tif (rx_ring->next_to_use != ntu)\n\t\tiavf_release_rx_desc(rx_ring, ntu);\n\n\treturn false;\n\nno_buffers:\n\tif (rx_ring->next_to_use != ntu)\n\t\tiavf_release_rx_desc(rx_ring, ntu);\n\n\t \n\treturn true;\n}\n\n \nstatic inline void iavf_rx_checksum(struct iavf_vsi *vsi,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    union iavf_rx_desc *rx_desc)\n{\n\tstruct iavf_rx_ptype_decoded decoded;\n\tu32 rx_error, rx_status;\n\tbool ipv4, ipv6;\n\tu8 ptype;\n\tu64 qword;\n\n\tqword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);\n\tptype = (qword & IAVF_RXD_QW1_PTYPE_MASK) >> IAVF_RXD_QW1_PTYPE_SHIFT;\n\trx_error = (qword & IAVF_RXD_QW1_ERROR_MASK) >>\n\t\t   IAVF_RXD_QW1_ERROR_SHIFT;\n\trx_status = (qword & IAVF_RXD_QW1_STATUS_MASK) >>\n\t\t    IAVF_RXD_QW1_STATUS_SHIFT;\n\tdecoded = decode_rx_desc_ptype(ptype);\n\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\tskb_checksum_none_assert(skb);\n\n\t \n\tif (!(vsi->netdev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\t \n\tif (!(rx_status & BIT(IAVF_RX_DESC_STATUS_L3L4P_SHIFT)))\n\t\treturn;\n\n\t \n\tif (!(decoded.known && decoded.outer_ip))\n\t\treturn;\n\n\tipv4 = (decoded.outer_ip == IAVF_RX_PTYPE_OUTER_IP) &&\n\t       (decoded.outer_ip_ver == IAVF_RX_PTYPE_OUTER_IPV4);\n\tipv6 = (decoded.outer_ip == IAVF_RX_PTYPE_OUTER_IP) &&\n\t       (decoded.outer_ip_ver == IAVF_RX_PTYPE_OUTER_IPV6);\n\n\tif (ipv4 &&\n\t    (rx_error & (BIT(IAVF_RX_DESC_ERROR_IPE_SHIFT) |\n\t\t\t BIT(IAVF_RX_DESC_ERROR_EIPE_SHIFT))))\n\t\tgoto checksum_fail;\n\n\t \n\tif (ipv6 &&\n\t    rx_status & BIT(IAVF_RX_DESC_STATUS_IPV6EXADD_SHIFT))\n\t\t \n\t\treturn;\n\n\t \n\tif (rx_error & BIT(IAVF_RX_DESC_ERROR_L4E_SHIFT))\n\t\tgoto checksum_fail;\n\n\t \n\tif (rx_error & BIT(IAVF_RX_DESC_ERROR_PPRS_SHIFT))\n\t\treturn;\n\n\t \n\tswitch (decoded.inner_prot) {\n\tcase IAVF_RX_PTYPE_INNER_PROT_TCP:\n\tcase IAVF_RX_PTYPE_INNER_PROT_UDP:\n\tcase IAVF_RX_PTYPE_INNER_PROT_SCTP:\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tfallthrough;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn;\n\nchecksum_fail:\n\tvsi->back->hw_csum_rx_error++;\n}\n\n \nstatic inline int iavf_ptype_to_htype(u8 ptype)\n{\n\tstruct iavf_rx_ptype_decoded decoded = decode_rx_desc_ptype(ptype);\n\n\tif (!decoded.known)\n\t\treturn PKT_HASH_TYPE_NONE;\n\n\tif (decoded.outer_ip == IAVF_RX_PTYPE_OUTER_IP &&\n\t    decoded.payload_layer == IAVF_RX_PTYPE_PAYLOAD_LAYER_PAY4)\n\t\treturn PKT_HASH_TYPE_L4;\n\telse if (decoded.outer_ip == IAVF_RX_PTYPE_OUTER_IP &&\n\t\t decoded.payload_layer == IAVF_RX_PTYPE_PAYLOAD_LAYER_PAY3)\n\t\treturn PKT_HASH_TYPE_L3;\n\telse\n\t\treturn PKT_HASH_TYPE_L2;\n}\n\n \nstatic inline void iavf_rx_hash(struct iavf_ring *ring,\n\t\t\t\tunion iavf_rx_desc *rx_desc,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tu8 rx_ptype)\n{\n\tu32 hash;\n\tconst __le64 rss_mask =\n\t\tcpu_to_le64((u64)IAVF_RX_DESC_FLTSTAT_RSS_HASH <<\n\t\t\t    IAVF_RX_DESC_STATUS_FLTSTAT_SHIFT);\n\n\tif (!(ring->netdev->features & NETIF_F_RXHASH))\n\t\treturn;\n\n\tif ((rx_desc->wb.qword1.status_error_len & rss_mask) == rss_mask) {\n\t\thash = le32_to_cpu(rx_desc->wb.qword0.hi_dword.rss);\n\t\tskb_set_hash(skb, hash, iavf_ptype_to_htype(rx_ptype));\n\t}\n}\n\n \nstatic inline\nvoid iavf_process_skb_fields(struct iavf_ring *rx_ring,\n\t\t\t     union iavf_rx_desc *rx_desc, struct sk_buff *skb,\n\t\t\t     u8 rx_ptype)\n{\n\tiavf_rx_hash(rx_ring, rx_desc, skb, rx_ptype);\n\n\tiavf_rx_checksum(rx_ring->vsi, skb, rx_desc);\n\n\tskb_record_rx_queue(skb, rx_ring->queue_index);\n\n\t \n\tskb->protocol = eth_type_trans(skb, rx_ring->netdev);\n}\n\n \nstatic bool iavf_cleanup_headers(struct iavf_ring *rx_ring, struct sk_buff *skb)\n{\n\t \n\tif (eth_skb_pad(skb))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void iavf_reuse_rx_page(struct iavf_ring *rx_ring,\n\t\t\t       struct iavf_rx_buffer *old_buff)\n{\n\tstruct iavf_rx_buffer *new_buff;\n\tu16 nta = rx_ring->next_to_alloc;\n\n\tnew_buff = &rx_ring->rx_bi[nta];\n\n\t \n\tnta++;\n\trx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;\n\n\t \n\tnew_buff->dma\t\t= old_buff->dma;\n\tnew_buff->page\t\t= old_buff->page;\n\tnew_buff->page_offset\t= old_buff->page_offset;\n\tnew_buff->pagecnt_bias\t= old_buff->pagecnt_bias;\n}\n\n \nstatic bool iavf_can_reuse_rx_page(struct iavf_rx_buffer *rx_buffer)\n{\n\tunsigned int pagecnt_bias = rx_buffer->pagecnt_bias;\n\tstruct page *page = rx_buffer->page;\n\n\t \n\tif (!dev_page_is_reusable(page))\n\t\treturn false;\n\n#if (PAGE_SIZE < 8192)\n\t \n\tif (unlikely((page_count(page) - pagecnt_bias) > 1))\n\t\treturn false;\n#else\n#define IAVF_LAST_OFFSET \\\n\t(SKB_WITH_OVERHEAD(PAGE_SIZE) - IAVF_RXBUFFER_2048)\n\tif (rx_buffer->page_offset > IAVF_LAST_OFFSET)\n\t\treturn false;\n#endif\n\n\t \n\tif (unlikely(!pagecnt_bias)) {\n\t\tpage_ref_add(page, USHRT_MAX);\n\t\trx_buffer->pagecnt_bias = USHRT_MAX;\n\t}\n\n\treturn true;\n}\n\n \nstatic void iavf_add_rx_frag(struct iavf_ring *rx_ring,\n\t\t\t     struct iavf_rx_buffer *rx_buffer,\n\t\t\t     struct sk_buff *skb,\n\t\t\t     unsigned int size)\n{\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = iavf_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = SKB_DATA_ALIGN(size + iavf_rx_offset(rx_ring));\n#endif\n\n\tif (!size)\n\t\treturn;\n\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_buffer->page,\n\t\t\trx_buffer->page_offset, size, truesize);\n\n\t \n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n}\n\n \nstatic struct iavf_rx_buffer *iavf_get_rx_buffer(struct iavf_ring *rx_ring,\n\t\t\t\t\t\t const unsigned int size)\n{\n\tstruct iavf_rx_buffer *rx_buffer;\n\n\trx_buffer = &rx_ring->rx_bi[rx_ring->next_to_clean];\n\tprefetchw(rx_buffer->page);\n\tif (!size)\n\t\treturn rx_buffer;\n\n\t \n\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t      rx_buffer->dma,\n\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t      size,\n\t\t\t\t      DMA_FROM_DEVICE);\n\n\t \n\trx_buffer->pagecnt_bias--;\n\n\treturn rx_buffer;\n}\n\n \nstatic struct sk_buff *iavf_construct_skb(struct iavf_ring *rx_ring,\n\t\t\t\t\t  struct iavf_rx_buffer *rx_buffer,\n\t\t\t\t\t  unsigned int size)\n{\n\tvoid *va;\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = iavf_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = SKB_DATA_ALIGN(size);\n#endif\n\tunsigned int headlen;\n\tstruct sk_buff *skb;\n\n\tif (!rx_buffer)\n\t\treturn NULL;\n\t \n\tva = page_address(rx_buffer->page) + rx_buffer->page_offset;\n\tnet_prefetch(va);\n\n\t \n\tskb = __napi_alloc_skb(&rx_ring->q_vector->napi,\n\t\t\t       IAVF_RX_HDR_SIZE,\n\t\t\t       GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\theadlen = size;\n\tif (headlen > IAVF_RX_HDR_SIZE)\n\t\theadlen = eth_get_headlen(skb->dev, va, IAVF_RX_HDR_SIZE);\n\n\t \n\tmemcpy(__skb_put(skb, headlen), va, ALIGN(headlen, sizeof(long)));\n\n\t \n\tsize -= headlen;\n\tif (size) {\n\t\tskb_add_rx_frag(skb, 0, rx_buffer->page,\n\t\t\t\trx_buffer->page_offset + headlen,\n\t\t\t\tsize, truesize);\n\n\t\t \n#if (PAGE_SIZE < 8192)\n\t\trx_buffer->page_offset ^= truesize;\n#else\n\t\trx_buffer->page_offset += truesize;\n#endif\n\t} else {\n\t\t \n\t\trx_buffer->pagecnt_bias++;\n\t}\n\n\treturn skb;\n}\n\n \nstatic struct sk_buff *iavf_build_skb(struct iavf_ring *rx_ring,\n\t\t\t\t      struct iavf_rx_buffer *rx_buffer,\n\t\t\t\t      unsigned int size)\n{\n\tvoid *va;\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = iavf_rx_pg_size(rx_ring) / 2;\n#else\n\tunsigned int truesize = SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) +\n\t\t\t\tSKB_DATA_ALIGN(IAVF_SKB_PAD + size);\n#endif\n\tstruct sk_buff *skb;\n\n\tif (!rx_buffer || !size)\n\t\treturn NULL;\n\t \n\tva = page_address(rx_buffer->page) + rx_buffer->page_offset;\n\tnet_prefetch(va);\n\n\t \n\tskb = napi_build_skb(va - IAVF_SKB_PAD, truesize);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, IAVF_SKB_PAD);\n\t__skb_put(skb, size);\n\n\t \n#if (PAGE_SIZE < 8192)\n\trx_buffer->page_offset ^= truesize;\n#else\n\trx_buffer->page_offset += truesize;\n#endif\n\n\treturn skb;\n}\n\n \nstatic void iavf_put_rx_buffer(struct iavf_ring *rx_ring,\n\t\t\t       struct iavf_rx_buffer *rx_buffer)\n{\n\tif (!rx_buffer)\n\t\treturn;\n\n\tif (iavf_can_reuse_rx_page(rx_buffer)) {\n\t\t \n\t\tiavf_reuse_rx_page(rx_ring, rx_buffer);\n\t\trx_ring->rx_stats.page_reuse_count++;\n\t} else {\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,\n\t\t\t\t     iavf_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE, IAVF_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(rx_buffer->page,\n\t\t\t\t\trx_buffer->pagecnt_bias);\n\t}\n\n\t \n\trx_buffer->page = NULL;\n}\n\n \nstatic bool iavf_is_non_eop(struct iavf_ring *rx_ring,\n\t\t\t    union iavf_rx_desc *rx_desc,\n\t\t\t    struct sk_buff *skb)\n{\n\tu32 ntc = rx_ring->next_to_clean + 1;\n\n\t \n\tntc = (ntc < rx_ring->count) ? ntc : 0;\n\trx_ring->next_to_clean = ntc;\n\n\tprefetch(IAVF_RX_DESC(rx_ring, ntc));\n\n\t \n#define IAVF_RXD_EOF BIT(IAVF_RX_DESC_STATUS_EOF_SHIFT)\n\tif (likely(iavf_test_staterr(rx_desc, IAVF_RXD_EOF)))\n\t\treturn false;\n\n\trx_ring->rx_stats.non_eop_descs++;\n\n\treturn true;\n}\n\n \nstatic int iavf_clean_rx_irq(struct iavf_ring *rx_ring, int budget)\n{\n\tunsigned int total_rx_bytes = 0, total_rx_packets = 0;\n\tstruct sk_buff *skb = rx_ring->skb;\n\tu16 cleaned_count = IAVF_DESC_UNUSED(rx_ring);\n\tbool failure = false;\n\n\twhile (likely(total_rx_packets < (unsigned int)budget)) {\n\t\tstruct iavf_rx_buffer *rx_buffer;\n\t\tunion iavf_rx_desc *rx_desc;\n\t\tunsigned int size;\n\t\tu16 vlan_tag = 0;\n\t\tu8 rx_ptype;\n\t\tu64 qword;\n\n\t\t \n\t\tif (cleaned_count >= IAVF_RX_BUFFER_WRITE) {\n\t\t\tfailure = failure ||\n\t\t\t\t  iavf_alloc_rx_buffers(rx_ring, cleaned_count);\n\t\t\tcleaned_count = 0;\n\t\t}\n\n\t\trx_desc = IAVF_RX_DESC(rx_ring, rx_ring->next_to_clean);\n\n\t\t \n\t\tqword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);\n\n\t\t \n\t\tdma_rmb();\n#define IAVF_RXD_DD BIT(IAVF_RX_DESC_STATUS_DD_SHIFT)\n\t\tif (!iavf_test_staterr(rx_desc, IAVF_RXD_DD))\n\t\t\tbreak;\n\n\t\tsize = (qword & IAVF_RXD_QW1_LENGTH_PBUF_MASK) >>\n\t\t       IAVF_RXD_QW1_LENGTH_PBUF_SHIFT;\n\n\t\tiavf_trace(clean_rx_irq, rx_ring, rx_desc, skb);\n\t\trx_buffer = iavf_get_rx_buffer(rx_ring, size);\n\n\t\t \n\t\tif (skb)\n\t\t\tiavf_add_rx_frag(rx_ring, rx_buffer, skb, size);\n\t\telse if (ring_uses_build_skb(rx_ring))\n\t\t\tskb = iavf_build_skb(rx_ring, rx_buffer, size);\n\t\telse\n\t\t\tskb = iavf_construct_skb(rx_ring, rx_buffer, size);\n\n\t\t \n\t\tif (!skb) {\n\t\t\trx_ring->rx_stats.alloc_buff_failed++;\n\t\t\tif (rx_buffer && size)\n\t\t\t\trx_buffer->pagecnt_bias++;\n\t\t\tbreak;\n\t\t}\n\n\t\tiavf_put_rx_buffer(rx_ring, rx_buffer);\n\t\tcleaned_count++;\n\n\t\tif (iavf_is_non_eop(rx_ring, rx_desc, skb))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (unlikely(iavf_test_staterr(rx_desc, BIT(IAVF_RXD_QW1_ERROR_SHIFT)))) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tskb = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (iavf_cleanup_headers(rx_ring, skb)) {\n\t\t\tskb = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\ttotal_rx_bytes += skb->len;\n\n\t\tqword = le64_to_cpu(rx_desc->wb.qword1.status_error_len);\n\t\trx_ptype = (qword & IAVF_RXD_QW1_PTYPE_MASK) >>\n\t\t\t   IAVF_RXD_QW1_PTYPE_SHIFT;\n\n\t\t \n\t\tiavf_process_skb_fields(rx_ring, rx_desc, skb, rx_ptype);\n\n\t\tif (qword & BIT(IAVF_RX_DESC_STATUS_L2TAG1P_SHIFT) &&\n\t\t    rx_ring->flags & IAVF_TXRX_FLAGS_VLAN_TAG_LOC_L2TAG1)\n\t\t\tvlan_tag = le16_to_cpu(rx_desc->wb.qword0.lo_dword.l2tag1);\n\t\tif (rx_desc->wb.qword2.ext_status &\n\t\t    cpu_to_le16(BIT(IAVF_RX_DESC_EXT_STATUS_L2TAG2P_SHIFT)) &&\n\t\t    rx_ring->flags & IAVF_RXR_FLAGS_VLAN_TAG_LOC_L2TAG2_2)\n\t\t\tvlan_tag = le16_to_cpu(rx_desc->wb.qword2.l2tag2_2);\n\n\t\tiavf_trace(clean_rx_irq_rx, rx_ring, rx_desc, skb);\n\t\tiavf_receive_skb(rx_ring, skb, vlan_tag);\n\t\tskb = NULL;\n\n\t\t \n\t\ttotal_rx_packets++;\n\t}\n\n\trx_ring->skb = skb;\n\n\tu64_stats_update_begin(&rx_ring->syncp);\n\trx_ring->stats.packets += total_rx_packets;\n\trx_ring->stats.bytes += total_rx_bytes;\n\tu64_stats_update_end(&rx_ring->syncp);\n\trx_ring->q_vector->rx.total_packets += total_rx_packets;\n\trx_ring->q_vector->rx.total_bytes += total_rx_bytes;\n\n\t \n\treturn failure ? budget : (int)total_rx_packets;\n}\n\nstatic inline u32 iavf_buildreg_itr(const int type, u16 itr)\n{\n\tu32 val;\n\n\t \n\titr &= IAVF_ITR_MASK;\n\n\tval = IAVF_VFINT_DYN_CTLN1_INTENA_MASK |\n\t      (type << IAVF_VFINT_DYN_CTLN1_ITR_INDX_SHIFT) |\n\t      (itr << (IAVF_VFINT_DYN_CTLN1_INTERVAL_SHIFT - 1));\n\n\treturn val;\n}\n\n \n#define INTREG IAVF_VFINT_DYN_CTLN1\n\n \n#define ITR_COUNTDOWN_START 3\n\n \nstatic inline void iavf_update_enable_itr(struct iavf_vsi *vsi,\n\t\t\t\t\t  struct iavf_q_vector *q_vector)\n{\n\tstruct iavf_hw *hw = &vsi->back->hw;\n\tu32 intval;\n\n\t \n\tiavf_update_itr(q_vector, &q_vector->tx);\n\tiavf_update_itr(q_vector, &q_vector->rx);\n\n\t \n\tif (q_vector->rx.target_itr < q_vector->rx.current_itr) {\n\t\t \n\t\tintval = iavf_buildreg_itr(IAVF_RX_ITR,\n\t\t\t\t\t   q_vector->rx.target_itr);\n\t\tq_vector->rx.current_itr = q_vector->rx.target_itr;\n\t\tq_vector->itr_countdown = ITR_COUNTDOWN_START;\n\t} else if ((q_vector->tx.target_itr < q_vector->tx.current_itr) ||\n\t\t   ((q_vector->rx.target_itr - q_vector->rx.current_itr) <\n\t\t    (q_vector->tx.target_itr - q_vector->tx.current_itr))) {\n\t\t \n\t\tintval = iavf_buildreg_itr(IAVF_TX_ITR,\n\t\t\t\t\t   q_vector->tx.target_itr);\n\t\tq_vector->tx.current_itr = q_vector->tx.target_itr;\n\t\tq_vector->itr_countdown = ITR_COUNTDOWN_START;\n\t} else if (q_vector->rx.current_itr != q_vector->rx.target_itr) {\n\t\t \n\t\tintval = iavf_buildreg_itr(IAVF_RX_ITR,\n\t\t\t\t\t   q_vector->rx.target_itr);\n\t\tq_vector->rx.current_itr = q_vector->rx.target_itr;\n\t\tq_vector->itr_countdown = ITR_COUNTDOWN_START;\n\t} else {\n\t\t \n\t\tintval = iavf_buildreg_itr(IAVF_ITR_NONE, 0);\n\t\tif (q_vector->itr_countdown)\n\t\t\tq_vector->itr_countdown--;\n\t}\n\n\tif (!test_bit(__IAVF_VSI_DOWN, vsi->state))\n\t\twr32(hw, INTREG(q_vector->reg_idx), intval);\n}\n\n \nint iavf_napi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct iavf_q_vector *q_vector =\n\t\t\t       container_of(napi, struct iavf_q_vector, napi);\n\tstruct iavf_vsi *vsi = q_vector->vsi;\n\tstruct iavf_ring *ring;\n\tbool clean_complete = true;\n\tbool arm_wb = false;\n\tint budget_per_ring;\n\tint work_done = 0;\n\n\tif (test_bit(__IAVF_VSI_DOWN, vsi->state)) {\n\t\tnapi_complete(napi);\n\t\treturn 0;\n\t}\n\n\t \n\tiavf_for_each_ring(ring, q_vector->tx) {\n\t\tif (!iavf_clean_tx_irq(vsi, ring, budget)) {\n\t\t\tclean_complete = false;\n\t\t\tcontinue;\n\t\t}\n\t\tarm_wb |= ring->arm_wb;\n\t\tring->arm_wb = false;\n\t}\n\n\t \n\tif (budget <= 0)\n\t\tgoto tx_only;\n\n\t \n\tbudget_per_ring = max(budget/q_vector->num_ringpairs, 1);\n\n\tiavf_for_each_ring(ring, q_vector->rx) {\n\t\tint cleaned = iavf_clean_rx_irq(ring, budget_per_ring);\n\n\t\twork_done += cleaned;\n\t\t \n\t\tif (cleaned >= budget_per_ring)\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (!clean_complete) {\n\t\tint cpu_id = smp_processor_id();\n\n\t\t \n\t\tif (!cpumask_test_cpu(cpu_id, &q_vector->affinity_mask)) {\n\t\t\t \n\t\t\tnapi_complete_done(napi, work_done);\n\n\t\t\t \n\t\t\tiavf_force_wb(vsi, q_vector);\n\n\t\t\t \n\t\t\treturn budget - 1;\n\t\t}\ntx_only:\n\t\tif (arm_wb) {\n\t\t\tq_vector->tx.ring[0].tx_stats.tx_force_wb++;\n\t\t\tiavf_enable_wb_on_itr(vsi, q_vector);\n\t\t}\n\t\treturn budget;\n\t}\n\n\tif (vsi->back->flags & IAVF_TXR_FLAGS_WB_ON_ITR)\n\t\tq_vector->arm_wb_state = false;\n\n\t \n\tif (likely(napi_complete_done(napi, work_done)))\n\t\tiavf_update_enable_itr(vsi, q_vector);\n\n\treturn min_t(int, work_done, budget - 1);\n}\n\n \nstatic void iavf_tx_prepare_vlan_flags(struct sk_buff *skb,\n\t\t\t\t       struct iavf_ring *tx_ring, u32 *flags)\n{\n\tu32  tx_flags = 0;\n\n\n\t \n\tif (!skb_vlan_tag_present(skb))\n\t\treturn;\n\n\ttx_flags |= skb_vlan_tag_get(skb) << IAVF_TX_FLAGS_VLAN_SHIFT;\n\tif (tx_ring->flags & IAVF_TXR_FLAGS_VLAN_TAG_LOC_L2TAG2) {\n\t\ttx_flags |= IAVF_TX_FLAGS_HW_OUTER_SINGLE_VLAN;\n\t} else if (tx_ring->flags & IAVF_TXRX_FLAGS_VLAN_TAG_LOC_L2TAG1) {\n\t\ttx_flags |= IAVF_TX_FLAGS_HW_VLAN;\n\t} else {\n\t\tdev_dbg(tx_ring->dev, \"Unsupported Tx VLAN tag location requested\\n\");\n\t\treturn;\n\t}\n\n\t*flags = tx_flags;\n}\n\n \nstatic int iavf_tso(struct iavf_tx_buffer *first, u8 *hdr_len,\n\t\t    u64 *cd_type_cmd_tso_mss)\n{\n\tstruct sk_buff *skb = first->skb;\n\tu64 cd_cmd, cd_tso_len, cd_mss;\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tstruct udphdr *udp;\n\t\tunsigned char *hdr;\n\t} l4;\n\tu32 paylen, l4_offset;\n\tu16 gso_segs, gso_size;\n\tint err;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\tip.hdr = skb_network_header(skb);\n\tl4.hdr = skb_transport_header(skb);\n\n\t \n\tif (ip.v4->version == 4) {\n\t\tip.v4->tot_len = 0;\n\t\tip.v4->check = 0;\n\t} else {\n\t\tip.v6->payload_len = 0;\n\t}\n\n\tif (skb_shinfo(skb)->gso_type & (SKB_GSO_GRE |\n\t\t\t\t\t SKB_GSO_GRE_CSUM |\n\t\t\t\t\t SKB_GSO_IPXIP4 |\n\t\t\t\t\t SKB_GSO_IPXIP6 |\n\t\t\t\t\t SKB_GSO_UDP_TUNNEL |\n\t\t\t\t\t SKB_GSO_UDP_TUNNEL_CSUM)) {\n\t\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL) &&\n\t\t    (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM)) {\n\t\t\tl4.udp->len = 0;\n\n\t\t\t \n\t\t\tl4_offset = l4.hdr - skb->data;\n\n\t\t\t \n\t\t\tpaylen = skb->len - l4_offset;\n\t\t\tcsum_replace_by_diff(&l4.udp->check,\n\t\t\t\t\t     (__force __wsum)htonl(paylen));\n\t\t}\n\n\t\t \n\t\tip.hdr = skb_inner_network_header(skb);\n\t\tl4.hdr = skb_inner_transport_header(skb);\n\n\t\t \n\t\tif (ip.v4->version == 4) {\n\t\t\tip.v4->tot_len = 0;\n\t\t\tip.v4->check = 0;\n\t\t} else {\n\t\t\tip.v6->payload_len = 0;\n\t\t}\n\t}\n\n\t \n\tl4_offset = l4.hdr - skb->data;\n\t \n\tpaylen = skb->len - l4_offset;\n\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {\n\t\tcsum_replace_by_diff(&l4.udp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\t\t \n\t\t*hdr_len = (u8)sizeof(l4.udp) + l4_offset;\n\t} else {\n\t\tcsum_replace_by_diff(&l4.tcp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\t\t \n\t\t*hdr_len = (u8)((l4.tcp->doff * 4) + l4_offset);\n\t}\n\n\t \n\tgso_size = skb_shinfo(skb)->gso_size;\n\tgso_segs = skb_shinfo(skb)->gso_segs;\n\n\t \n\tfirst->gso_segs = gso_segs;\n\tfirst->bytecount += (first->gso_segs - 1) * *hdr_len;\n\n\t \n\tcd_cmd = IAVF_TX_CTX_DESC_TSO;\n\tcd_tso_len = skb->len - *hdr_len;\n\tcd_mss = gso_size;\n\t*cd_type_cmd_tso_mss |= (cd_cmd << IAVF_TXD_CTX_QW1_CMD_SHIFT) |\n\t\t\t\t(cd_tso_len << IAVF_TXD_CTX_QW1_TSO_LEN_SHIFT) |\n\t\t\t\t(cd_mss << IAVF_TXD_CTX_QW1_MSS_SHIFT);\n\treturn 1;\n}\n\n \nstatic int iavf_tx_enable_csum(struct sk_buff *skb, u32 *tx_flags,\n\t\t\t       u32 *td_cmd, u32 *td_offset,\n\t\t\t       struct iavf_ring *tx_ring,\n\t\t\t       u32 *cd_tunneling)\n{\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tstruct udphdr *udp;\n\t\tunsigned char *hdr;\n\t} l4;\n\tunsigned char *exthdr;\n\tu32 offset, cmd = 0;\n\t__be16 frag_off;\n\tu8 l4_proto = 0;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tip.hdr = skb_network_header(skb);\n\tl4.hdr = skb_transport_header(skb);\n\n\t \n\toffset = ((ip.hdr - skb->data) / 2) << IAVF_TX_DESC_LENGTH_MACLEN_SHIFT;\n\n\tif (skb->encapsulation) {\n\t\tu32 tunnel = 0;\n\t\t \n\t\tif (*tx_flags & IAVF_TX_FLAGS_IPV4) {\n\t\t\ttunnel |= (*tx_flags & IAVF_TX_FLAGS_TSO) ?\n\t\t\t\t  IAVF_TX_CTX_EXT_IP_IPV4 :\n\t\t\t\t  IAVF_TX_CTX_EXT_IP_IPV4_NO_CSUM;\n\n\t\t\tl4_proto = ip.v4->protocol;\n\t\t} else if (*tx_flags & IAVF_TX_FLAGS_IPV6) {\n\t\t\ttunnel |= IAVF_TX_CTX_EXT_IP_IPV6;\n\n\t\t\texthdr = ip.hdr + sizeof(*ip.v6);\n\t\t\tl4_proto = ip.v6->nexthdr;\n\t\t\tif (l4.hdr != exthdr)\n\t\t\t\tipv6_skip_exthdr(skb, exthdr - skb->data,\n\t\t\t\t\t\t &l4_proto, &frag_off);\n\t\t}\n\n\t\t \n\t\tswitch (l4_proto) {\n\t\tcase IPPROTO_UDP:\n\t\t\ttunnel |= IAVF_TXD_CTX_UDP_TUNNELING;\n\t\t\t*tx_flags |= IAVF_TX_FLAGS_VXLAN_TUNNEL;\n\t\t\tbreak;\n\t\tcase IPPROTO_GRE:\n\t\t\ttunnel |= IAVF_TXD_CTX_GRE_TUNNELING;\n\t\t\t*tx_flags |= IAVF_TX_FLAGS_VXLAN_TUNNEL;\n\t\t\tbreak;\n\t\tcase IPPROTO_IPIP:\n\t\tcase IPPROTO_IPV6:\n\t\t\t*tx_flags |= IAVF_TX_FLAGS_VXLAN_TUNNEL;\n\t\t\tl4.hdr = skb_inner_network_header(skb);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (*tx_flags & IAVF_TX_FLAGS_TSO)\n\t\t\t\treturn -1;\n\n\t\t\tskb_checksum_help(skb);\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\ttunnel |= ((l4.hdr - ip.hdr) / 4) <<\n\t\t\t  IAVF_TXD_CTX_QW0_EXT_IPLEN_SHIFT;\n\n\t\t \n\t\tip.hdr = skb_inner_network_header(skb);\n\n\t\t \n\t\ttunnel |= ((ip.hdr - l4.hdr) / 2) <<\n\t\t\t  IAVF_TXD_CTX_QW0_NATLEN_SHIFT;\n\n\t\t \n\t\tif ((*tx_flags & IAVF_TX_FLAGS_TSO) &&\n\t\t    !(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL) &&\n\t\t    (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM))\n\t\t\ttunnel |= IAVF_TXD_CTX_QW0_L4T_CS_MASK;\n\n\t\t \n\t\t*cd_tunneling |= tunnel;\n\n\t\t \n\t\tl4.hdr = skb_inner_transport_header(skb);\n\t\tl4_proto = 0;\n\n\t\t \n\t\t*tx_flags &= ~(IAVF_TX_FLAGS_IPV4 | IAVF_TX_FLAGS_IPV6);\n\t\tif (ip.v4->version == 4)\n\t\t\t*tx_flags |= IAVF_TX_FLAGS_IPV4;\n\t\tif (ip.v6->version == 6)\n\t\t\t*tx_flags |= IAVF_TX_FLAGS_IPV6;\n\t}\n\n\t \n\tif (*tx_flags & IAVF_TX_FLAGS_IPV4) {\n\t\tl4_proto = ip.v4->protocol;\n\t\t \n\t\tcmd |= (*tx_flags & IAVF_TX_FLAGS_TSO) ?\n\t\t       IAVF_TX_DESC_CMD_IIPT_IPV4_CSUM :\n\t\t       IAVF_TX_DESC_CMD_IIPT_IPV4;\n\t} else if (*tx_flags & IAVF_TX_FLAGS_IPV6) {\n\t\tcmd |= IAVF_TX_DESC_CMD_IIPT_IPV6;\n\n\t\texthdr = ip.hdr + sizeof(*ip.v6);\n\t\tl4_proto = ip.v6->nexthdr;\n\t\tif (l4.hdr != exthdr)\n\t\t\tipv6_skip_exthdr(skb, exthdr - skb->data,\n\t\t\t\t\t &l4_proto, &frag_off);\n\t}\n\n\t \n\toffset |= ((l4.hdr - ip.hdr) / 4) << IAVF_TX_DESC_LENGTH_IPLEN_SHIFT;\n\n\t \n\tswitch (l4_proto) {\n\tcase IPPROTO_TCP:\n\t\t \n\t\tcmd |= IAVF_TX_DESC_CMD_L4T_EOFT_TCP;\n\t\toffset |= l4.tcp->doff << IAVF_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;\n\t\tbreak;\n\tcase IPPROTO_SCTP:\n\t\t \n\t\tcmd |= IAVF_TX_DESC_CMD_L4T_EOFT_SCTP;\n\t\toffset |= (sizeof(struct sctphdr) >> 2) <<\n\t\t\t  IAVF_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\t \n\t\tcmd |= IAVF_TX_DESC_CMD_L4T_EOFT_UDP;\n\t\toffset |= (sizeof(struct udphdr) >> 2) <<\n\t\t\t  IAVF_TX_DESC_LENGTH_L4_FC_LEN_SHIFT;\n\t\tbreak;\n\tdefault:\n\t\tif (*tx_flags & IAVF_TX_FLAGS_TSO)\n\t\t\treturn -1;\n\t\tskb_checksum_help(skb);\n\t\treturn 0;\n\t}\n\n\t*td_cmd |= cmd;\n\t*td_offset |= offset;\n\n\treturn 1;\n}\n\n \nstatic void iavf_create_tx_ctx(struct iavf_ring *tx_ring,\n\t\t\t       const u64 cd_type_cmd_tso_mss,\n\t\t\t       const u32 cd_tunneling, const u32 cd_l2tag2)\n{\n\tstruct iavf_tx_context_desc *context_desc;\n\tint i = tx_ring->next_to_use;\n\n\tif ((cd_type_cmd_tso_mss == IAVF_TX_DESC_DTYPE_CONTEXT) &&\n\t    !cd_tunneling && !cd_l2tag2)\n\t\treturn;\n\n\t \n\tcontext_desc = IAVF_TX_CTXTDESC(tx_ring, i);\n\n\ti++;\n\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\t \n\tcontext_desc->tunneling_params = cpu_to_le32(cd_tunneling);\n\tcontext_desc->l2tag2 = cpu_to_le16(cd_l2tag2);\n\tcontext_desc->rsvd = cpu_to_le16(0);\n\tcontext_desc->type_cmd_tso_mss = cpu_to_le64(cd_type_cmd_tso_mss);\n}\n\n \nbool __iavf_chk_linearize(struct sk_buff *skb)\n{\n\tconst skb_frag_t *frag, *stale;\n\tint nr_frags, sum;\n\n\t \n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\tif (nr_frags < (IAVF_MAX_BUFFER_TXD - 1))\n\t\treturn false;\n\n\t \n\tnr_frags -= IAVF_MAX_BUFFER_TXD - 2;\n\tfrag = &skb_shinfo(skb)->frags[0];\n\n\t \n\tsum = 1 - skb_shinfo(skb)->gso_size;\n\n\t \n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\tsum += skb_frag_size(frag++);\n\n\t \n\tfor (stale = &skb_shinfo(skb)->frags[0];; stale++) {\n\t\tint stale_size = skb_frag_size(stale);\n\n\t\tsum += skb_frag_size(frag++);\n\n\t\t \n\t\tif (stale_size > IAVF_MAX_DATA_PER_TXD) {\n\t\t\tint align_pad = -(skb_frag_off(stale)) &\n\t\t\t\t\t(IAVF_MAX_READ_REQ_SIZE - 1);\n\n\t\t\tsum -= align_pad;\n\t\t\tstale_size -= align_pad;\n\n\t\t\tdo {\n\t\t\t\tsum -= IAVF_MAX_DATA_PER_TXD_ALIGNED;\n\t\t\t\tstale_size -= IAVF_MAX_DATA_PER_TXD_ALIGNED;\n\t\t\t} while (stale_size > IAVF_MAX_DATA_PER_TXD);\n\t\t}\n\n\t\t \n\t\tif (sum < 0)\n\t\t\treturn true;\n\n\t\tif (!nr_frags--)\n\t\t\tbreak;\n\n\t\tsum -= stale_size;\n\t}\n\n\treturn false;\n}\n\n \nint __iavf_maybe_stop_tx(struct iavf_ring *tx_ring, int size)\n{\n\tnetif_stop_subqueue(tx_ring->netdev, tx_ring->queue_index);\n\t \n\tsmp_mb();\n\n\t \n\tif (likely(IAVF_DESC_UNUSED(tx_ring) < size))\n\t\treturn -EBUSY;\n\n\t \n\tnetif_start_subqueue(tx_ring->netdev, tx_ring->queue_index);\n\t++tx_ring->tx_stats.restart_queue;\n\treturn 0;\n}\n\n \nstatic inline void iavf_tx_map(struct iavf_ring *tx_ring, struct sk_buff *skb,\n\t\t\t       struct iavf_tx_buffer *first, u32 tx_flags,\n\t\t\t       const u8 hdr_len, u32 td_cmd, u32 td_offset)\n{\n\tunsigned int data_len = skb->data_len;\n\tunsigned int size = skb_headlen(skb);\n\tskb_frag_t *frag;\n\tstruct iavf_tx_buffer *tx_bi;\n\tstruct iavf_tx_desc *tx_desc;\n\tu16 i = tx_ring->next_to_use;\n\tu32 td_tag = 0;\n\tdma_addr_t dma;\n\n\tif (tx_flags & IAVF_TX_FLAGS_HW_VLAN) {\n\t\ttd_cmd |= IAVF_TX_DESC_CMD_IL2TAG1;\n\t\ttd_tag = (tx_flags & IAVF_TX_FLAGS_VLAN_MASK) >>\n\t\t\t IAVF_TX_FLAGS_VLAN_SHIFT;\n\t}\n\n\tfirst->tx_flags = tx_flags;\n\n\tdma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);\n\n\ttx_desc = IAVF_TX_DESC(tx_ring, i);\n\ttx_bi = first;\n\n\tfor (frag = &skb_shinfo(skb)->frags[0];; frag++) {\n\t\tunsigned int max_data = IAVF_MAX_DATA_PER_TXD_ALIGNED;\n\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto dma_error;\n\n\t\t \n\t\tdma_unmap_len_set(tx_bi, len, size);\n\t\tdma_unmap_addr_set(tx_bi, dma, dma);\n\n\t\t \n\t\tmax_data += -dma & (IAVF_MAX_READ_REQ_SIZE - 1);\n\t\ttx_desc->buffer_addr = cpu_to_le64(dma);\n\n\t\twhile (unlikely(size > IAVF_MAX_DATA_PER_TXD)) {\n\t\t\ttx_desc->cmd_type_offset_bsz =\n\t\t\t\tbuild_ctob(td_cmd, td_offset,\n\t\t\t\t\t   max_data, td_tag);\n\n\t\t\ttx_desc++;\n\t\t\ti++;\n\n\t\t\tif (i == tx_ring->count) {\n\t\t\t\ttx_desc = IAVF_TX_DESC(tx_ring, 0);\n\t\t\t\ti = 0;\n\t\t\t}\n\n\t\t\tdma += max_data;\n\t\t\tsize -= max_data;\n\n\t\t\tmax_data = IAVF_MAX_DATA_PER_TXD_ALIGNED;\n\t\t\ttx_desc->buffer_addr = cpu_to_le64(dma);\n\t\t}\n\n\t\tif (likely(!data_len))\n\t\t\tbreak;\n\n\t\ttx_desc->cmd_type_offset_bsz = build_ctob(td_cmd, td_offset,\n\t\t\t\t\t\t\t  size, td_tag);\n\n\t\ttx_desc++;\n\t\ti++;\n\n\t\tif (i == tx_ring->count) {\n\t\t\ttx_desc = IAVF_TX_DESC(tx_ring, 0);\n\t\t\ti = 0;\n\t\t}\n\n\t\tsize = skb_frag_size(frag);\n\t\tdata_len -= size;\n\n\t\tdma = skb_frag_dma_map(tx_ring->dev, frag, 0, size,\n\t\t\t\t       DMA_TO_DEVICE);\n\n\t\ttx_bi = &tx_ring->tx_bi[i];\n\t}\n\n\tnetdev_tx_sent_queue(txring_txq(tx_ring), first->bytecount);\n\n\ti++;\n\tif (i == tx_ring->count)\n\t\ti = 0;\n\n\ttx_ring->next_to_use = i;\n\n\tiavf_maybe_stop_tx(tx_ring, DESC_NEEDED);\n\n\t \n\ttd_cmd |= IAVF_TXD_CMD;\n\ttx_desc->cmd_type_offset_bsz =\n\t\t\tbuild_ctob(td_cmd, td_offset, size, td_tag);\n\n\tskb_tx_timestamp(skb);\n\n\t \n\twmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\t \n\tif (netif_xmit_stopped(txring_txq(tx_ring)) || !netdev_xmit_more()) {\n\t\twritel(i, tx_ring->tail);\n\t}\n\n\treturn;\n\ndma_error:\n\tdev_info(tx_ring->dev, \"TX DMA map failed\\n\");\n\n\t \n\tfor (;;) {\n\t\ttx_bi = &tx_ring->tx_bi[i];\n\t\tiavf_unmap_and_free_tx_resource(tx_ring, tx_bi);\n\t\tif (tx_bi == first)\n\t\t\tbreak;\n\t\tif (i == 0)\n\t\t\ti = tx_ring->count;\n\t\ti--;\n\t}\n\n\ttx_ring->next_to_use = i;\n}\n\n \nstatic netdev_tx_t iavf_xmit_frame_ring(struct sk_buff *skb,\n\t\t\t\t\tstruct iavf_ring *tx_ring)\n{\n\tu64 cd_type_cmd_tso_mss = IAVF_TX_DESC_DTYPE_CONTEXT;\n\tu32 cd_tunneling = 0, cd_l2tag2 = 0;\n\tstruct iavf_tx_buffer *first;\n\tu32 td_offset = 0;\n\tu32 tx_flags = 0;\n\t__be16 protocol;\n\tu32 td_cmd = 0;\n\tu8 hdr_len = 0;\n\tint tso, count;\n\n\t \n\tprefetch(skb->data);\n\n\tiavf_trace(xmit_frame_ring, skb, tx_ring);\n\n\tcount = iavf_xmit_descriptor_count(skb);\n\tif (iavf_chk_linearize(skb, count)) {\n\t\tif (__skb_linearize(skb)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tcount = iavf_txd_use_count(skb->len);\n\t\ttx_ring->tx_stats.tx_linearize++;\n\t}\n\n\t \n\tif (iavf_maybe_stop_tx(tx_ring, count + 4 + 1)) {\n\t\ttx_ring->tx_stats.tx_busy++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\tfirst = &tx_ring->tx_bi[tx_ring->next_to_use];\n\tfirst->skb = skb;\n\tfirst->bytecount = skb->len;\n\tfirst->gso_segs = 1;\n\n\t \n\tiavf_tx_prepare_vlan_flags(skb, tx_ring, &tx_flags);\n\tif (tx_flags & IAVF_TX_FLAGS_HW_OUTER_SINGLE_VLAN) {\n\t\tcd_type_cmd_tso_mss |= IAVF_TX_CTX_DESC_IL2TAG2 <<\n\t\t\tIAVF_TXD_CTX_QW1_CMD_SHIFT;\n\t\tcd_l2tag2 = (tx_flags & IAVF_TX_FLAGS_VLAN_MASK) >>\n\t\t\tIAVF_TX_FLAGS_VLAN_SHIFT;\n\t}\n\n\t \n\tprotocol = vlan_get_protocol(skb);\n\n\t \n\tif (protocol == htons(ETH_P_IP))\n\t\ttx_flags |= IAVF_TX_FLAGS_IPV4;\n\telse if (protocol == htons(ETH_P_IPV6))\n\t\ttx_flags |= IAVF_TX_FLAGS_IPV6;\n\n\ttso = iavf_tso(first, &hdr_len, &cd_type_cmd_tso_mss);\n\n\tif (tso < 0)\n\t\tgoto out_drop;\n\telse if (tso)\n\t\ttx_flags |= IAVF_TX_FLAGS_TSO;\n\n\t \n\ttso = iavf_tx_enable_csum(skb, &tx_flags, &td_cmd, &td_offset,\n\t\t\t\t  tx_ring, &cd_tunneling);\n\tif (tso < 0)\n\t\tgoto out_drop;\n\n\t \n\ttd_cmd |= IAVF_TX_DESC_CMD_ICRC;\n\n\tiavf_create_tx_ctx(tx_ring, cd_type_cmd_tso_mss,\n\t\t\t   cd_tunneling, cd_l2tag2);\n\n\tiavf_tx_map(tx_ring, skb, first, tx_flags, hdr_len,\n\t\t    td_cmd, td_offset);\n\n\treturn NETDEV_TX_OK;\n\nout_drop:\n\tiavf_trace(xmit_frame_ring_drop, first->skb, tx_ring);\n\tdev_kfree_skb_any(first->skb);\n\tfirst->skb = NULL;\n\treturn NETDEV_TX_OK;\n}\n\n \nnetdev_tx_t iavf_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct iavf_adapter *adapter = netdev_priv(netdev);\n\tstruct iavf_ring *tx_ring = &adapter->tx_rings[skb->queue_mapping];\n\n\t \n\tif (unlikely(skb->len < IAVF_MIN_TX_LEN)) {\n\t\tif (skb_pad(skb, IAVF_MIN_TX_LEN - skb->len))\n\t\t\treturn NETDEV_TX_OK;\n\t\tskb->len = IAVF_MIN_TX_LEN;\n\t\tskb_set_tail_pointer(skb, IAVF_MIN_TX_LEN);\n\t}\n\n\treturn iavf_xmit_frame_ring(skb, tx_ring);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}