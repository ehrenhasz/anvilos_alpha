{
  "module_name": "igc_main.c",
  "hash_id": "daf7ec083b25b79e2ba6dbc382d24307bfc8f35f288c65ae31edd0f358ef5a30",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/intel/igc/igc_main.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/if_vlan.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/ip.h>\n#include <linux/pm_runtime.h>\n#include <net/pkt_sched.h>\n#include <linux/bpf_trace.h>\n#include <net/xdp_sock_drv.h>\n#include <linux/pci.h>\n\n#include <net/ipv6.h>\n\n#include \"igc.h\"\n#include \"igc_hw.h\"\n#include \"igc_tsn.h\"\n#include \"igc_xdp.h\"\n\n#define DRV_SUMMARY\t\"Intel(R) 2.5G Ethernet Linux Driver\"\n\n#define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK)\n\n#define IGC_XDP_PASS\t\t0\n#define IGC_XDP_CONSUMED\tBIT(0)\n#define IGC_XDP_TX\t\tBIT(1)\n#define IGC_XDP_REDIRECT\tBIT(2)\n\nstatic int debug = -1;\n\nMODULE_AUTHOR(\"Intel Corporation, <linux.nics@intel.com>\");\nMODULE_DESCRIPTION(DRV_SUMMARY);\nMODULE_LICENSE(\"GPL v2\");\nmodule_param(debug, int, 0);\nMODULE_PARM_DESC(debug, \"Debug level (0=none,...,16=all)\");\n\nchar igc_driver_name[] = \"igc\";\nstatic const char igc_driver_string[] = DRV_SUMMARY;\nstatic const char igc_copyright[] =\n\t\"Copyright(c) 2018 Intel Corporation.\";\n\nstatic const struct igc_info *igc_info_tbl[] = {\n\t[board_base] = &igc_base_info,\n};\n\nstatic const struct pci_device_id igc_pci_tbl[] = {\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I225_LM), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I225_V), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I225_I), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I220_V), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I225_K), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I225_K2), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I226_K), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I225_LMVP), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I226_LMVP), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I225_IT), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I226_LM), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I226_V), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I226_IT), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I221_V), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I226_BLANK_NVM), board_base },\n\t{ PCI_VDEVICE(INTEL, IGC_DEV_ID_I225_BLANK_NVM), board_base },\n\t \n\t{0, }\n};\n\nMODULE_DEVICE_TABLE(pci, igc_pci_tbl);\n\nenum latency_range {\n\tlowest_latency = 0,\n\tlow_latency = 1,\n\tbulk_latency = 2,\n\tlatency_invalid = 255\n};\n\nvoid igc_reset(struct igc_adapter *adapter)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tstruct igc_hw *hw = &adapter->hw;\n\tstruct igc_fc_info *fc = &hw->fc;\n\tu32 pba, hwm;\n\n\t \n\tpba = IGC_PBA_34K;\n\n\t \n\thwm = (pba << 10) - (adapter->max_frame_size + MAX_JUMBO_FRAME_SIZE);\n\n\tfc->high_water = hwm & 0xFFFFFFF0;\t \n\tfc->low_water = fc->high_water - 16;\n\tfc->pause_time = 0xFFFF;\n\tfc->send_xon = 1;\n\tfc->current_mode = fc->requested_mode;\n\n\thw->mac.ops.reset_hw(hw);\n\n\tif (hw->mac.ops.init_hw(hw))\n\t\tnetdev_err(dev, \"Error on hardware initialization\\n\");\n\n\t \n\tigc_set_eee_i225(hw, true, true, true);\n\n\tif (!netif_running(adapter->netdev))\n\t\tigc_power_down_phy_copper_base(&adapter->hw);\n\n\t \n\twr32(IGC_VET, ETH_P_8021Q);\n\n\t \n\tigc_ptp_reset(adapter);\n\n\t \n\tigc_tsn_reset(adapter);\n\n\tigc_get_phy_info(hw);\n}\n\n \nstatic void igc_power_up_link(struct igc_adapter *adapter)\n{\n\tigc_reset_phy(&adapter->hw);\n\n\tigc_power_up_phy_copper(&adapter->hw);\n\n\tigc_setup_link(&adapter->hw);\n}\n\n \nstatic void igc_release_hw_control(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 ctrl_ext;\n\n\tif (!pci_device_is_present(adapter->pdev))\n\t\treturn;\n\n\t \n\tctrl_ext = rd32(IGC_CTRL_EXT);\n\twr32(IGC_CTRL_EXT,\n\t     ctrl_ext & ~IGC_CTRL_EXT_DRV_LOAD);\n}\n\n \nstatic void igc_get_hw_control(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 ctrl_ext;\n\n\t \n\tctrl_ext = rd32(IGC_CTRL_EXT);\n\twr32(IGC_CTRL_EXT,\n\t     ctrl_ext | IGC_CTRL_EXT_DRV_LOAD);\n}\n\nstatic void igc_unmap_tx_buffer(struct device *dev, struct igc_tx_buffer *buf)\n{\n\tdma_unmap_single(dev, dma_unmap_addr(buf, dma),\n\t\t\t dma_unmap_len(buf, len), DMA_TO_DEVICE);\n\n\tdma_unmap_len_set(buf, len, 0);\n}\n\n \nstatic void igc_clean_tx_ring(struct igc_ring *tx_ring)\n{\n\tu16 i = tx_ring->next_to_clean;\n\tstruct igc_tx_buffer *tx_buffer = &tx_ring->tx_buffer_info[i];\n\tu32 xsk_frames = 0;\n\n\twhile (i != tx_ring->next_to_use) {\n\t\tunion igc_adv_tx_desc *eop_desc, *tx_desc;\n\n\t\tswitch (tx_buffer->type) {\n\t\tcase IGC_TX_BUFFER_TYPE_XSK:\n\t\t\txsk_frames++;\n\t\t\tbreak;\n\t\tcase IGC_TX_BUFFER_TYPE_XDP:\n\t\t\txdp_return_frame(tx_buffer->xdpf);\n\t\t\tigc_unmap_tx_buffer(tx_ring->dev, tx_buffer);\n\t\t\tbreak;\n\t\tcase IGC_TX_BUFFER_TYPE_SKB:\n\t\t\tdev_kfree_skb_any(tx_buffer->skb);\n\t\t\tigc_unmap_tx_buffer(tx_ring->dev, tx_buffer);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tnetdev_warn_once(tx_ring->netdev, \"Unknown Tx buffer type\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\teop_desc = tx_buffer->next_to_watch;\n\t\ttx_desc = IGC_TX_DESC(tx_ring, i);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\t\ti = 0;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = IGC_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\t\tigc_unmap_tx_buffer(tx_ring->dev, tx_buffer);\n\t\t}\n\n\t\ttx_buffer->next_to_watch = NULL;\n\n\t\t \n\t\ttx_buffer++;\n\t\ti++;\n\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\ti = 0;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t}\n\t}\n\n\tif (tx_ring->xsk_pool && xsk_frames)\n\t\txsk_tx_completed(tx_ring->xsk_pool, xsk_frames);\n\n\t \n\tnetdev_tx_reset_queue(txring_txq(tx_ring));\n\n\t \n\tmemset(tx_ring->tx_buffer_info, 0,\n\t       sizeof(*tx_ring->tx_buffer_info) * tx_ring->count);\n\n\t \n\tmemset(tx_ring->desc, 0, tx_ring->size);\n\n\t \n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n}\n\n \nvoid igc_free_tx_resources(struct igc_ring *tx_ring)\n{\n\tigc_disable_tx_ring(tx_ring);\n\n\tvfree(tx_ring->tx_buffer_info);\n\ttx_ring->tx_buffer_info = NULL;\n\n\t \n\tif (!tx_ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(tx_ring->dev, tx_ring->size,\n\t\t\t  tx_ring->desc, tx_ring->dma);\n\n\ttx_ring->desc = NULL;\n}\n\n \nstatic void igc_free_all_tx_resources(struct igc_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tigc_free_tx_resources(adapter->tx_ring[i]);\n}\n\n \nstatic void igc_clean_all_tx_rings(struct igc_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tif (adapter->tx_ring[i])\n\t\t\tigc_clean_tx_ring(adapter->tx_ring[i]);\n}\n\nstatic void igc_disable_tx_ring_hw(struct igc_ring *ring)\n{\n\tstruct igc_hw *hw = &ring->q_vector->adapter->hw;\n\tu8 idx = ring->reg_idx;\n\tu32 txdctl;\n\n\ttxdctl = rd32(IGC_TXDCTL(idx));\n\ttxdctl &= ~IGC_TXDCTL_QUEUE_ENABLE;\n\ttxdctl |= IGC_TXDCTL_SWFLUSH;\n\twr32(IGC_TXDCTL(idx), txdctl);\n}\n\n \nstatic void igc_disable_all_tx_rings_hw(struct igc_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igc_ring *tx_ring = adapter->tx_ring[i];\n\n\t\tigc_disable_tx_ring_hw(tx_ring);\n\t}\n}\n\n \nint igc_setup_tx_resources(struct igc_ring *tx_ring)\n{\n\tstruct net_device *ndev = tx_ring->netdev;\n\tstruct device *dev = tx_ring->dev;\n\tint size = 0;\n\n\tsize = sizeof(struct igc_tx_buffer) * tx_ring->count;\n\ttx_ring->tx_buffer_info = vzalloc(size);\n\tif (!tx_ring->tx_buffer_info)\n\t\tgoto err;\n\n\t \n\ttx_ring->size = tx_ring->count * sizeof(union igc_adv_tx_desc);\n\ttx_ring->size = ALIGN(tx_ring->size, 4096);\n\n\ttx_ring->desc = dma_alloc_coherent(dev, tx_ring->size,\n\t\t\t\t\t   &tx_ring->dma, GFP_KERNEL);\n\n\tif (!tx_ring->desc)\n\t\tgoto err;\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\n\treturn 0;\n\nerr:\n\tvfree(tx_ring->tx_buffer_info);\n\tnetdev_err(ndev, \"Unable to allocate memory for Tx descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int igc_setup_all_tx_resources(struct igc_adapter *adapter)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tint i, err = 0;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\terr = igc_setup_tx_resources(adapter->tx_ring[i]);\n\t\tif (err) {\n\t\t\tnetdev_err(dev, \"Error on Tx queue %u setup\\n\", i);\n\t\t\tfor (i--; i >= 0; i--)\n\t\t\t\tigc_free_tx_resources(adapter->tx_ring[i]);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic void igc_clean_rx_ring_page_shared(struct igc_ring *rx_ring)\n{\n\tu16 i = rx_ring->next_to_clean;\n\n\tdev_kfree_skb(rx_ring->skb);\n\trx_ring->skb = NULL;\n\n\t \n\twhile (i != rx_ring->next_to_alloc) {\n\t\tstruct igc_rx_buffer *buffer_info = &rx_ring->rx_buffer_info[i];\n\n\t\t \n\t\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t\t      buffer_info->dma,\n\t\t\t\t\t      buffer_info->page_offset,\n\t\t\t\t\t      igc_rx_bufsz(rx_ring),\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev,\n\t\t\t\t     buffer_info->dma,\n\t\t\t\t     igc_rx_pg_size(rx_ring),\n\t\t\t\t     DMA_FROM_DEVICE,\n\t\t\t\t     IGC_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(buffer_info->page,\n\t\t\t\t\tbuffer_info->pagecnt_bias);\n\n\t\ti++;\n\t\tif (i == rx_ring->count)\n\t\t\ti = 0;\n\t}\n}\n\nstatic void igc_clean_rx_ring_xsk_pool(struct igc_ring *ring)\n{\n\tstruct igc_rx_buffer *bi;\n\tu16 i;\n\n\tfor (i = 0; i < ring->count; i++) {\n\t\tbi = &ring->rx_buffer_info[i];\n\t\tif (!bi->xdp)\n\t\t\tcontinue;\n\n\t\txsk_buff_free(bi->xdp);\n\t\tbi->xdp = NULL;\n\t}\n}\n\n \nstatic void igc_clean_rx_ring(struct igc_ring *ring)\n{\n\tif (ring->xsk_pool)\n\t\tigc_clean_rx_ring_xsk_pool(ring);\n\telse\n\t\tigc_clean_rx_ring_page_shared(ring);\n\n\tclear_ring_uses_large_buffer(ring);\n\n\tring->next_to_alloc = 0;\n\tring->next_to_clean = 0;\n\tring->next_to_use = 0;\n}\n\n \nstatic void igc_clean_all_rx_rings(struct igc_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tif (adapter->rx_ring[i])\n\t\t\tigc_clean_rx_ring(adapter->rx_ring[i]);\n}\n\n \nvoid igc_free_rx_resources(struct igc_ring *rx_ring)\n{\n\tigc_clean_rx_ring(rx_ring);\n\n\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\n\tvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\n\t \n\tif (!rx_ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(rx_ring->dev, rx_ring->size,\n\t\t\t  rx_ring->desc, rx_ring->dma);\n\n\trx_ring->desc = NULL;\n}\n\n \nstatic void igc_free_all_rx_resources(struct igc_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tigc_free_rx_resources(adapter->rx_ring[i]);\n}\n\n \nint igc_setup_rx_resources(struct igc_ring *rx_ring)\n{\n\tstruct net_device *ndev = rx_ring->netdev;\n\tstruct device *dev = rx_ring->dev;\n\tu8 index = rx_ring->queue_index;\n\tint size, desc_len, res;\n\n\t \n\tif (xdp_rxq_info_is_reg(&rx_ring->xdp_rxq))\n\t\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\tres = xdp_rxq_info_reg(&rx_ring->xdp_rxq, ndev, index,\n\t\t\t       rx_ring->q_vector->napi.napi_id);\n\tif (res < 0) {\n\t\tnetdev_err(ndev, \"Failed to register xdp_rxq index %u\\n\",\n\t\t\t   index);\n\t\treturn res;\n\t}\n\n\tsize = sizeof(struct igc_rx_buffer) * rx_ring->count;\n\trx_ring->rx_buffer_info = vzalloc(size);\n\tif (!rx_ring->rx_buffer_info)\n\t\tgoto err;\n\n\tdesc_len = sizeof(union igc_adv_rx_desc);\n\n\t \n\trx_ring->size = rx_ring->count * desc_len;\n\trx_ring->size = ALIGN(rx_ring->size, 4096);\n\n\trx_ring->desc = dma_alloc_coherent(dev, rx_ring->size,\n\t\t\t\t\t   &rx_ring->dma, GFP_KERNEL);\n\n\tif (!rx_ring->desc)\n\t\tgoto err;\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n\n\treturn 0;\n\nerr:\n\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\tvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\tnetdev_err(ndev, \"Unable to allocate memory for Rx descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int igc_setup_all_rx_resources(struct igc_adapter *adapter)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tint i, err = 0;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\terr = igc_setup_rx_resources(adapter->rx_ring[i]);\n\t\tif (err) {\n\t\t\tnetdev_err(dev, \"Error on Rx queue %u setup\\n\", i);\n\t\t\tfor (i--; i >= 0; i--)\n\t\t\t\tigc_free_rx_resources(adapter->rx_ring[i]);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic struct xsk_buff_pool *igc_get_xsk_pool(struct igc_adapter *adapter,\n\t\t\t\t\t      struct igc_ring *ring)\n{\n\tif (!igc_xdp_is_enabled(adapter) ||\n\t    !test_bit(IGC_RING_FLAG_AF_XDP_ZC, &ring->flags))\n\t\treturn NULL;\n\n\treturn xsk_get_pool_from_qid(ring->netdev, ring->queue_index);\n}\n\n \nstatic void igc_configure_rx_ring(struct igc_adapter *adapter,\n\t\t\t\t  struct igc_ring *ring)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tunion igc_adv_rx_desc *rx_desc;\n\tint reg_idx = ring->reg_idx;\n\tu32 srrctl = 0, rxdctl = 0;\n\tu64 rdba = ring->dma;\n\tu32 buf_size;\n\n\txdp_rxq_info_unreg_mem_model(&ring->xdp_rxq);\n\tring->xsk_pool = igc_get_xsk_pool(adapter, ring);\n\tif (ring->xsk_pool) {\n\t\tWARN_ON(xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,\n\t\t\t\t\t\t   MEM_TYPE_XSK_BUFF_POOL,\n\t\t\t\t\t\t   NULL));\n\t\txsk_pool_set_rxq_info(ring->xsk_pool, &ring->xdp_rxq);\n\t} else {\n\t\tWARN_ON(xdp_rxq_info_reg_mem_model(&ring->xdp_rxq,\n\t\t\t\t\t\t   MEM_TYPE_PAGE_SHARED,\n\t\t\t\t\t\t   NULL));\n\t}\n\n\tif (igc_xdp_is_enabled(adapter))\n\t\tset_ring_uses_large_buffer(ring);\n\n\t \n\twr32(IGC_RXDCTL(reg_idx), 0);\n\n\t \n\twr32(IGC_RDBAL(reg_idx),\n\t     rdba & 0x00000000ffffffffULL);\n\twr32(IGC_RDBAH(reg_idx), rdba >> 32);\n\twr32(IGC_RDLEN(reg_idx),\n\t     ring->count * sizeof(union igc_adv_rx_desc));\n\n\t \n\tring->tail = adapter->io_addr + IGC_RDT(reg_idx);\n\twr32(IGC_RDH(reg_idx), 0);\n\twritel(0, ring->tail);\n\n\t \n\tring->next_to_clean = 0;\n\tring->next_to_use = 0;\n\n\tif (ring->xsk_pool)\n\t\tbuf_size = xsk_pool_get_rx_frame_size(ring->xsk_pool);\n\telse if (ring_uses_large_buffer(ring))\n\t\tbuf_size = IGC_RXBUFFER_3072;\n\telse\n\t\tbuf_size = IGC_RXBUFFER_2048;\n\n\tsrrctl = rd32(IGC_SRRCTL(reg_idx));\n\tsrrctl &= ~(IGC_SRRCTL_BSIZEPKT_MASK | IGC_SRRCTL_BSIZEHDR_MASK |\n\t\t    IGC_SRRCTL_DESCTYPE_MASK);\n\tsrrctl |= IGC_SRRCTL_BSIZEHDR(IGC_RX_HDR_LEN);\n\tsrrctl |= IGC_SRRCTL_BSIZEPKT(buf_size);\n\tsrrctl |= IGC_SRRCTL_DESCTYPE_ADV_ONEBUF;\n\n\twr32(IGC_SRRCTL(reg_idx), srrctl);\n\n\trxdctl |= IGC_RX_PTHRESH;\n\trxdctl |= IGC_RX_HTHRESH << 8;\n\trxdctl |= IGC_RX_WTHRESH << 16;\n\n\t \n\tmemset(ring->rx_buffer_info, 0,\n\t       sizeof(struct igc_rx_buffer) * ring->count);\n\n\t \n\trx_desc = IGC_RX_DESC(ring, 0);\n\trx_desc->wb.upper.length = 0;\n\n\t \n\trxdctl |= IGC_RXDCTL_QUEUE_ENABLE;\n\n\twr32(IGC_RXDCTL(reg_idx), rxdctl);\n}\n\n \nstatic void igc_configure_rx(struct igc_adapter *adapter)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tigc_configure_rx_ring(adapter, adapter->rx_ring[i]);\n}\n\n \nstatic void igc_configure_tx_ring(struct igc_adapter *adapter,\n\t\t\t\t  struct igc_ring *ring)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint reg_idx = ring->reg_idx;\n\tu64 tdba = ring->dma;\n\tu32 txdctl = 0;\n\n\tring->xsk_pool = igc_get_xsk_pool(adapter, ring);\n\n\t \n\twr32(IGC_TXDCTL(reg_idx), 0);\n\twrfl();\n\n\twr32(IGC_TDLEN(reg_idx),\n\t     ring->count * sizeof(union igc_adv_tx_desc));\n\twr32(IGC_TDBAL(reg_idx),\n\t     tdba & 0x00000000ffffffffULL);\n\twr32(IGC_TDBAH(reg_idx), tdba >> 32);\n\n\tring->tail = adapter->io_addr + IGC_TDT(reg_idx);\n\twr32(IGC_TDH(reg_idx), 0);\n\twritel(0, ring->tail);\n\n\ttxdctl |= IGC_TX_PTHRESH;\n\ttxdctl |= IGC_TX_HTHRESH << 8;\n\ttxdctl |= IGC_TX_WTHRESH << 16;\n\n\ttxdctl |= IGC_TXDCTL_QUEUE_ENABLE;\n\twr32(IGC_TXDCTL(reg_idx), txdctl);\n}\n\n \nstatic void igc_configure_tx(struct igc_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tigc_configure_tx_ring(adapter, adapter->tx_ring[i]);\n}\n\n \nstatic void igc_setup_mrqc(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 j, num_rx_queues;\n\tu32 mrqc, rxcsum;\n\tu32 rss_key[10];\n\n\tnetdev_rss_key_fill(rss_key, sizeof(rss_key));\n\tfor (j = 0; j < 10; j++)\n\t\twr32(IGC_RSSRK(j), rss_key[j]);\n\n\tnum_rx_queues = adapter->rss_queues;\n\n\tif (adapter->rss_indir_tbl_init != num_rx_queues) {\n\t\tfor (j = 0; j < IGC_RETA_SIZE; j++)\n\t\t\tadapter->rss_indir_tbl[j] =\n\t\t\t(j * num_rx_queues) / IGC_RETA_SIZE;\n\t\tadapter->rss_indir_tbl_init = num_rx_queues;\n\t}\n\tigc_write_rss_indir_tbl(adapter);\n\n\t \n\trxcsum = rd32(IGC_RXCSUM);\n\trxcsum |= IGC_RXCSUM_PCSD;\n\n\t \n\trxcsum |= IGC_RXCSUM_CRCOFL;\n\n\t \n\twr32(IGC_RXCSUM, rxcsum);\n\n\t \n\tmrqc = IGC_MRQC_RSS_FIELD_IPV4 |\n\t       IGC_MRQC_RSS_FIELD_IPV4_TCP |\n\t       IGC_MRQC_RSS_FIELD_IPV6 |\n\t       IGC_MRQC_RSS_FIELD_IPV6_TCP |\n\t       IGC_MRQC_RSS_FIELD_IPV6_TCP_EX;\n\n\tif (adapter->flags & IGC_FLAG_RSS_FIELD_IPV4_UDP)\n\t\tmrqc |= IGC_MRQC_RSS_FIELD_IPV4_UDP;\n\tif (adapter->flags & IGC_FLAG_RSS_FIELD_IPV6_UDP)\n\t\tmrqc |= IGC_MRQC_RSS_FIELD_IPV6_UDP;\n\n\tmrqc |= IGC_MRQC_ENABLE_RSS_MQ;\n\n\twr32(IGC_MRQC, mrqc);\n}\n\n \nstatic void igc_setup_rctl(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 rctl;\n\n\trctl = rd32(IGC_RCTL);\n\n\trctl &= ~(3 << IGC_RCTL_MO_SHIFT);\n\trctl &= ~(IGC_RCTL_LBM_TCVR | IGC_RCTL_LBM_MAC);\n\n\trctl |= IGC_RCTL_EN | IGC_RCTL_BAM | IGC_RCTL_RDMTS_HALF |\n\t\t(hw->mac.mc_filter_type << IGC_RCTL_MO_SHIFT);\n\n\t \n\trctl |= IGC_RCTL_SECRC;\n\n\t \n\trctl &= ~(IGC_RCTL_SBP | IGC_RCTL_SZ_256);\n\n\t \n\trctl |= IGC_RCTL_LPE;\n\n\t \n\twr32(IGC_RXDCTL(0), 0);\n\n\t \n\tif (adapter->netdev->features & NETIF_F_RXALL) {\n\t\t \n\t\trctl |= (IGC_RCTL_SBP |  \n\t\t\t IGC_RCTL_BAM |  \n\t\t\t IGC_RCTL_PMCF);  \n\n\t\trctl &= ~(IGC_RCTL_DPF |  \n\t\t\t  IGC_RCTL_CFIEN);  \n\t}\n\n\twr32(IGC_RCTL, rctl);\n}\n\n \nstatic void igc_setup_tctl(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 tctl;\n\n\t \n\twr32(IGC_TXDCTL(0), 0);\n\n\t \n\ttctl = rd32(IGC_TCTL);\n\ttctl &= ~IGC_TCTL_CT;\n\ttctl |= IGC_TCTL_PSP | IGC_TCTL_RTLC |\n\t\t(IGC_COLLISION_THRESHOLD << IGC_CT_SHIFT);\n\n\t \n\ttctl |= IGC_TCTL_EN;\n\n\twr32(IGC_TCTL, tctl);\n}\n\n \nstatic void igc_set_mac_filter_hw(struct igc_adapter *adapter, int index,\n\t\t\t\t  enum igc_mac_filter_type type,\n\t\t\t\t  const u8 *addr, int queue)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 ral, rah;\n\n\tif (WARN_ON(index >= hw->mac.rar_entry_count))\n\t\treturn;\n\n\tral = le32_to_cpup((__le32 *)(addr));\n\trah = le16_to_cpup((__le16 *)(addr + 4));\n\n\tif (type == IGC_MAC_FILTER_TYPE_SRC) {\n\t\trah &= ~IGC_RAH_ASEL_MASK;\n\t\trah |= IGC_RAH_ASEL_SRC_ADDR;\n\t}\n\n\tif (queue >= 0) {\n\t\trah &= ~IGC_RAH_QSEL_MASK;\n\t\trah |= (queue << IGC_RAH_QSEL_SHIFT);\n\t\trah |= IGC_RAH_QSEL_ENABLE;\n\t}\n\n\trah |= IGC_RAH_AV;\n\n\twr32(IGC_RAL(index), ral);\n\twr32(IGC_RAH(index), rah);\n\n\tnetdev_dbg(dev, \"MAC address filter set in HW: index %d\", index);\n}\n\n \nstatic void igc_clear_mac_filter_hw(struct igc_adapter *adapter, int index)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tstruct igc_hw *hw = &adapter->hw;\n\n\tif (WARN_ON(index >= hw->mac.rar_entry_count))\n\t\treturn;\n\n\twr32(IGC_RAL(index), 0);\n\twr32(IGC_RAH(index), 0);\n\n\tnetdev_dbg(dev, \"MAC address filter cleared in HW: index %d\", index);\n}\n\n \nstatic void igc_set_default_mac_filter(struct igc_adapter *adapter)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tu8 *addr = adapter->hw.mac.addr;\n\n\tnetdev_dbg(dev, \"Set default MAC address filter: address %pM\", addr);\n\n\tigc_set_mac_filter_hw(adapter, 0, IGC_MAC_FILTER_TYPE_DST, addr, -1);\n}\n\n \nstatic int igc_set_mac(struct net_device *netdev, void *p)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct igc_hw *hw = &adapter->hw;\n\tstruct sockaddr *addr = p;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\teth_hw_addr_set(netdev, addr->sa_data);\n\tmemcpy(hw->mac.addr, addr->sa_data, netdev->addr_len);\n\n\t \n\tigc_set_default_mac_filter(adapter);\n\n\treturn 0;\n}\n\n \nstatic int igc_write_mc_addr_list(struct net_device *netdev)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct igc_hw *hw = &adapter->hw;\n\tstruct netdev_hw_addr *ha;\n\tu8  *mta_list;\n\tint i;\n\n\tif (netdev_mc_empty(netdev)) {\n\t\t \n\t\tigc_update_mc_addr_list(hw, NULL, 0);\n\t\treturn 0;\n\t}\n\n\tmta_list = kcalloc(netdev_mc_count(netdev), 6, GFP_ATOMIC);\n\tif (!mta_list)\n\t\treturn -ENOMEM;\n\n\t \n\ti = 0;\n\tnetdev_for_each_mc_addr(ha, netdev)\n\t\tmemcpy(mta_list + (i++ * ETH_ALEN), ha->addr, ETH_ALEN);\n\n\tigc_update_mc_addr_list(hw, mta_list, i);\n\tkfree(mta_list);\n\n\treturn netdev_mc_count(netdev);\n}\n\nstatic __le32 igc_tx_launchtime(struct igc_ring *ring, ktime_t txtime,\n\t\t\t\tbool *first_flag, bool *insert_empty)\n{\n\tstruct igc_adapter *adapter = netdev_priv(ring->netdev);\n\tktime_t cycle_time = adapter->cycle_time;\n\tktime_t base_time = adapter->base_time;\n\tktime_t now = ktime_get_clocktai();\n\tktime_t baset_est, end_of_cycle;\n\ts32 launchtime;\n\ts64 n;\n\n\tn = div64_s64(ktime_sub_ns(now, base_time), cycle_time);\n\n\tbaset_est = ktime_add_ns(base_time, cycle_time * (n));\n\tend_of_cycle = ktime_add_ns(baset_est, cycle_time);\n\n\tif (ktime_compare(txtime, end_of_cycle) >= 0) {\n\t\tif (baset_est != ring->last_ff_cycle) {\n\t\t\t*first_flag = true;\n\t\t\tring->last_ff_cycle = baset_est;\n\n\t\t\tif (ktime_compare(end_of_cycle, ring->last_tx_cycle) > 0)\n\t\t\t\t*insert_empty = true;\n\t\t}\n\t}\n\n\t \n\tif ((ktime_sub_ns(end_of_cycle, now) < 5 * NSEC_PER_USEC))\n\t\tnetdev_warn(ring->netdev, \"Packet with txtime=%llu may not be honoured\\n\",\n\t\t\t    txtime);\n\n\tring->last_tx_cycle = end_of_cycle;\n\n\tlaunchtime = ktime_sub_ns(txtime, baset_est);\n\tif (launchtime > 0)\n\t\tdiv_s64_rem(launchtime, cycle_time, &launchtime);\n\telse\n\t\tlaunchtime = 0;\n\n\treturn cpu_to_le32(launchtime);\n}\n\nstatic int igc_init_empty_frame(struct igc_ring *ring,\n\t\t\t\tstruct igc_tx_buffer *buffer,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tunsigned int size;\n\tdma_addr_t dma;\n\n\tsize = skb_headlen(skb);\n\n\tdma = dma_map_single(ring->dev, skb->data, size, DMA_TO_DEVICE);\n\tif (dma_mapping_error(ring->dev, dma)) {\n\t\tnetdev_err_once(ring->netdev, \"Failed to map DMA for TX\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tbuffer->skb = skb;\n\tbuffer->protocol = 0;\n\tbuffer->bytecount = skb->len;\n\tbuffer->gso_segs = 1;\n\tbuffer->time_stamp = jiffies;\n\tdma_unmap_len_set(buffer, len, skb->len);\n\tdma_unmap_addr_set(buffer, dma, dma);\n\n\treturn 0;\n}\n\nstatic int igc_init_tx_empty_descriptor(struct igc_ring *ring,\n\t\t\t\t\tstruct sk_buff *skb,\n\t\t\t\t\tstruct igc_tx_buffer *first)\n{\n\tunion igc_adv_tx_desc *desc;\n\tu32 cmd_type, olinfo_status;\n\tint err;\n\n\tif (!igc_desc_unused(ring))\n\t\treturn -EBUSY;\n\n\terr = igc_init_empty_frame(ring, first, skb);\n\tif (err)\n\t\treturn err;\n\n\tcmd_type = IGC_ADVTXD_DTYP_DATA | IGC_ADVTXD_DCMD_DEXT |\n\t\t   IGC_ADVTXD_DCMD_IFCS | IGC_TXD_DCMD |\n\t\t   first->bytecount;\n\tolinfo_status = first->bytecount << IGC_ADVTXD_PAYLEN_SHIFT;\n\n\tdesc = IGC_TX_DESC(ring, ring->next_to_use);\n\tdesc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\tdesc->read.olinfo_status = cpu_to_le32(olinfo_status);\n\tdesc->read.buffer_addr = cpu_to_le64(dma_unmap_addr(first, dma));\n\n\tnetdev_tx_sent_queue(txring_txq(ring), skb->len);\n\n\tfirst->next_to_watch = desc;\n\n\tring->next_to_use++;\n\tif (ring->next_to_use == ring->count)\n\t\tring->next_to_use = 0;\n\n\treturn 0;\n}\n\n#define IGC_EMPTY_FRAME_SIZE 60\n\nstatic void igc_tx_ctxtdesc(struct igc_ring *tx_ring,\n\t\t\t    __le32 launch_time, bool first_flag,\n\t\t\t    u32 vlan_macip_lens, u32 type_tucmd,\n\t\t\t    u32 mss_l4len_idx)\n{\n\tstruct igc_adv_tx_context_desc *context_desc;\n\tu16 i = tx_ring->next_to_use;\n\n\tcontext_desc = IGC_TX_CTXTDESC(tx_ring, i);\n\n\ti++;\n\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\t \n\ttype_tucmd |= IGC_TXD_CMD_DEXT | IGC_ADVTXD_DTYP_CTXT;\n\n\t \n\tif (test_bit(IGC_RING_FLAG_TX_CTX_IDX, &tx_ring->flags))\n\t\tmss_l4len_idx |= tx_ring->reg_idx << 4;\n\n\tif (first_flag)\n\t\tmss_l4len_idx |= IGC_ADVTXD_TSN_CNTX_FIRST;\n\n\tcontext_desc->vlan_macip_lens\t= cpu_to_le32(vlan_macip_lens);\n\tcontext_desc->type_tucmd_mlhl\t= cpu_to_le32(type_tucmd);\n\tcontext_desc->mss_l4len_idx\t= cpu_to_le32(mss_l4len_idx);\n\tcontext_desc->launch_time\t= launch_time;\n}\n\nstatic void igc_tx_csum(struct igc_ring *tx_ring, struct igc_tx_buffer *first,\n\t\t\t__le32 launch_time, bool first_flag)\n{\n\tstruct sk_buff *skb = first->skb;\n\tu32 vlan_macip_lens = 0;\n\tu32 type_tucmd = 0;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL) {\ncsum_failed:\n\t\tif (!(first->tx_flags & IGC_TX_FLAGS_VLAN) &&\n\t\t    !tx_ring->launchtime_enable)\n\t\t\treturn;\n\t\tgoto no_csum;\n\t}\n\n\tswitch (skb->csum_offset) {\n\tcase offsetof(struct tcphdr, check):\n\t\ttype_tucmd = IGC_ADVTXD_TUCMD_L4T_TCP;\n\t\tfallthrough;\n\tcase offsetof(struct udphdr, check):\n\t\tbreak;\n\tcase offsetof(struct sctphdr, checksum):\n\t\t \n\t\tif (skb_csum_is_sctp(skb)) {\n\t\t\ttype_tucmd = IGC_ADVTXD_TUCMD_L4T_SCTP;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tskb_checksum_help(skb);\n\t\tgoto csum_failed;\n\t}\n\n\t \n\tfirst->tx_flags |= IGC_TX_FLAGS_CSUM;\n\tvlan_macip_lens = skb_checksum_start_offset(skb) -\n\t\t\t  skb_network_offset(skb);\nno_csum:\n\tvlan_macip_lens |= skb_network_offset(skb) << IGC_ADVTXD_MACLEN_SHIFT;\n\tvlan_macip_lens |= first->tx_flags & IGC_TX_FLAGS_VLAN_MASK;\n\n\tigc_tx_ctxtdesc(tx_ring, launch_time, first_flag,\n\t\t\tvlan_macip_lens, type_tucmd, 0);\n}\n\nstatic int __igc_maybe_stop_tx(struct igc_ring *tx_ring, const u16 size)\n{\n\tstruct net_device *netdev = tx_ring->netdev;\n\n\tnetif_stop_subqueue(netdev, tx_ring->queue_index);\n\n\t \n\tsmp_mb();\n\n\t \n\tif (igc_desc_unused(tx_ring) < size)\n\t\treturn -EBUSY;\n\n\t \n\tnetif_wake_subqueue(netdev, tx_ring->queue_index);\n\n\tu64_stats_update_begin(&tx_ring->tx_syncp2);\n\ttx_ring->tx_stats.restart_queue2++;\n\tu64_stats_update_end(&tx_ring->tx_syncp2);\n\n\treturn 0;\n}\n\nstatic inline int igc_maybe_stop_tx(struct igc_ring *tx_ring, const u16 size)\n{\n\tif (igc_desc_unused(tx_ring) >= size)\n\t\treturn 0;\n\treturn __igc_maybe_stop_tx(tx_ring, size);\n}\n\n#define IGC_SET_FLAG(_input, _flag, _result) \\\n\t(((_flag) <= (_result)) ?\t\t\t\t\\\n\t ((u32)((_input) & (_flag)) * ((_result) / (_flag))) :\t\\\n\t ((u32)((_input) & (_flag)) / ((_flag) / (_result))))\n\nstatic u32 igc_tx_cmd_type(struct sk_buff *skb, u32 tx_flags)\n{\n\t \n\tu32 cmd_type = IGC_ADVTXD_DTYP_DATA |\n\t\t       IGC_ADVTXD_DCMD_DEXT |\n\t\t       IGC_ADVTXD_DCMD_IFCS;\n\n\t \n\tcmd_type |= IGC_SET_FLAG(tx_flags, IGC_TX_FLAGS_VLAN,\n\t\t\t\t IGC_ADVTXD_DCMD_VLE);\n\n\t \n\tcmd_type |= IGC_SET_FLAG(tx_flags, IGC_TX_FLAGS_TSO,\n\t\t\t\t (IGC_ADVTXD_DCMD_TSE));\n\n\t \n\tcmd_type |= IGC_SET_FLAG(tx_flags, IGC_TX_FLAGS_TSTAMP,\n\t\t\t\t (IGC_ADVTXD_MAC_TSTAMP));\n\n\tcmd_type |= IGC_SET_FLAG(tx_flags, IGC_TX_FLAGS_TSTAMP_1,\n\t\t\t\t (IGC_ADVTXD_TSTAMP_REG_1));\n\n\tcmd_type |= IGC_SET_FLAG(tx_flags, IGC_TX_FLAGS_TSTAMP_2,\n\t\t\t\t (IGC_ADVTXD_TSTAMP_REG_2));\n\n\tcmd_type |= IGC_SET_FLAG(tx_flags, IGC_TX_FLAGS_TSTAMP_3,\n\t\t\t\t (IGC_ADVTXD_TSTAMP_REG_3));\n\n\t \n\tcmd_type ^= IGC_SET_FLAG(skb->no_fcs, 1, IGC_ADVTXD_DCMD_IFCS);\n\n\treturn cmd_type;\n}\n\nstatic void igc_tx_olinfo_status(struct igc_ring *tx_ring,\n\t\t\t\t union igc_adv_tx_desc *tx_desc,\n\t\t\t\t u32 tx_flags, unsigned int paylen)\n{\n\tu32 olinfo_status = paylen << IGC_ADVTXD_PAYLEN_SHIFT;\n\n\t \n\tolinfo_status |= (tx_flags & IGC_TX_FLAGS_CSUM) *\n\t\t\t  ((IGC_TXD_POPTS_TXSM << 8) /\n\t\t\t  IGC_TX_FLAGS_CSUM);\n\n\t \n\tolinfo_status |= (tx_flags & IGC_TX_FLAGS_IPV4) *\n\t\t\t  (((IGC_TXD_POPTS_IXSM << 8)) /\n\t\t\t  IGC_TX_FLAGS_IPV4);\n\n\ttx_desc->read.olinfo_status = cpu_to_le32(olinfo_status);\n}\n\nstatic int igc_tx_map(struct igc_ring *tx_ring,\n\t\t      struct igc_tx_buffer *first,\n\t\t      const u8 hdr_len)\n{\n\tstruct sk_buff *skb = first->skb;\n\tstruct igc_tx_buffer *tx_buffer;\n\tunion igc_adv_tx_desc *tx_desc;\n\tu32 tx_flags = first->tx_flags;\n\tskb_frag_t *frag;\n\tu16 i = tx_ring->next_to_use;\n\tunsigned int data_len, size;\n\tdma_addr_t dma;\n\tu32 cmd_type;\n\n\tcmd_type = igc_tx_cmd_type(skb, tx_flags);\n\ttx_desc = IGC_TX_DESC(tx_ring, i);\n\n\tigc_tx_olinfo_status(tx_ring, tx_desc, tx_flags, skb->len - hdr_len);\n\n\tsize = skb_headlen(skb);\n\tdata_len = skb->data_len;\n\n\tdma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);\n\n\ttx_buffer = first;\n\n\tfor (frag = &skb_shinfo(skb)->frags[0];; frag++) {\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto dma_error;\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, size);\n\t\tdma_unmap_addr_set(tx_buffer, dma, dma);\n\n\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\n\t\twhile (unlikely(size > IGC_MAX_DATA_PER_TXD)) {\n\t\t\ttx_desc->read.cmd_type_len =\n\t\t\t\tcpu_to_le32(cmd_type ^ IGC_MAX_DATA_PER_TXD);\n\n\t\t\ti++;\n\t\t\ttx_desc++;\n\t\t\tif (i == tx_ring->count) {\n\t\t\t\ttx_desc = IGC_TX_DESC(tx_ring, 0);\n\t\t\t\ti = 0;\n\t\t\t}\n\t\t\ttx_desc->read.olinfo_status = 0;\n\n\t\t\tdma += IGC_MAX_DATA_PER_TXD;\n\t\t\tsize -= IGC_MAX_DATA_PER_TXD;\n\n\t\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\t\t}\n\n\t\tif (likely(!data_len))\n\t\t\tbreak;\n\n\t\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type ^ size);\n\n\t\ti++;\n\t\ttx_desc++;\n\t\tif (i == tx_ring->count) {\n\t\t\ttx_desc = IGC_TX_DESC(tx_ring, 0);\n\t\t\ti = 0;\n\t\t}\n\t\ttx_desc->read.olinfo_status = 0;\n\n\t\tsize = skb_frag_size(frag);\n\t\tdata_len -= size;\n\n\t\tdma = skb_frag_dma_map(tx_ring->dev, frag, 0,\n\t\t\t\t       size, DMA_TO_DEVICE);\n\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t}\n\n\t \n\tcmd_type |= size | IGC_TXD_DCMD;\n\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\n\tnetdev_tx_sent_queue(txring_txq(tx_ring), first->bytecount);\n\n\t \n\tfirst->time_stamp = jiffies;\n\n\tskb_tx_timestamp(skb);\n\n\t \n\twmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\ti++;\n\tif (i == tx_ring->count)\n\t\ti = 0;\n\n\ttx_ring->next_to_use = i;\n\n\t \n\tigc_maybe_stop_tx(tx_ring, DESC_NEEDED);\n\n\tif (netif_xmit_stopped(txring_txq(tx_ring)) || !netdev_xmit_more()) {\n\t\twritel(i, tx_ring->tail);\n\t}\n\n\treturn 0;\ndma_error:\n\tnetdev_err(tx_ring->netdev, \"TX DMA map failed\\n\");\n\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\n\t \n\twhile (tx_buffer != first) {\n\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\tigc_unmap_tx_buffer(tx_ring->dev, tx_buffer);\n\n\t\tif (i-- == 0)\n\t\t\ti += tx_ring->count;\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t}\n\n\tif (dma_unmap_len(tx_buffer, len))\n\t\tigc_unmap_tx_buffer(tx_ring->dev, tx_buffer);\n\n\tdev_kfree_skb_any(tx_buffer->skb);\n\ttx_buffer->skb = NULL;\n\n\ttx_ring->next_to_use = i;\n\n\treturn -1;\n}\n\nstatic int igc_tso(struct igc_ring *tx_ring,\n\t\t   struct igc_tx_buffer *first,\n\t\t   __le32 launch_time, bool first_flag,\n\t\t   u8 *hdr_len)\n{\n\tu32 vlan_macip_lens, type_tucmd, mss_l4len_idx;\n\tstruct sk_buff *skb = first->skb;\n\tunion {\n\t\tstruct iphdr *v4;\n\t\tstruct ipv6hdr *v6;\n\t\tunsigned char *hdr;\n\t} ip;\n\tunion {\n\t\tstruct tcphdr *tcp;\n\t\tstruct udphdr *udp;\n\t\tunsigned char *hdr;\n\t} l4;\n\tu32 paylen, l4_offset;\n\tint err;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\tip.hdr = skb_network_header(skb);\n\tl4.hdr = skb_checksum_start(skb);\n\n\t \n\ttype_tucmd = IGC_ADVTXD_TUCMD_L4T_TCP;\n\n\t \n\tif (ip.v4->version == 4) {\n\t\tunsigned char *csum_start = skb_checksum_start(skb);\n\t\tunsigned char *trans_start = ip.hdr + (ip.v4->ihl * 4);\n\n\t\t \n\t\tip.v4->check = csum_fold(csum_partial(trans_start,\n\t\t\t\t\t\t      csum_start - trans_start,\n\t\t\t\t\t\t      0));\n\t\ttype_tucmd |= IGC_ADVTXD_TUCMD_IPV4;\n\n\t\tip.v4->tot_len = 0;\n\t\tfirst->tx_flags |= IGC_TX_FLAGS_TSO |\n\t\t\t\t   IGC_TX_FLAGS_CSUM |\n\t\t\t\t   IGC_TX_FLAGS_IPV4;\n\t} else {\n\t\tip.v6->payload_len = 0;\n\t\tfirst->tx_flags |= IGC_TX_FLAGS_TSO |\n\t\t\t\t   IGC_TX_FLAGS_CSUM;\n\t}\n\n\t \n\tl4_offset = l4.hdr - skb->data;\n\n\t \n\tpaylen = skb->len - l4_offset;\n\tif (type_tucmd & IGC_ADVTXD_TUCMD_L4T_TCP) {\n\t\t \n\t\t*hdr_len = (l4.tcp->doff * 4) + l4_offset;\n\t\tcsum_replace_by_diff(&l4.tcp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\t} else {\n\t\t \n\t\t*hdr_len = sizeof(*l4.udp) + l4_offset;\n\t\tcsum_replace_by_diff(&l4.udp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\t}\n\n\t \n\tfirst->gso_segs = skb_shinfo(skb)->gso_segs;\n\tfirst->bytecount += (first->gso_segs - 1) * *hdr_len;\n\n\t \n\tmss_l4len_idx = (*hdr_len - l4_offset) << IGC_ADVTXD_L4LEN_SHIFT;\n\tmss_l4len_idx |= skb_shinfo(skb)->gso_size << IGC_ADVTXD_MSS_SHIFT;\n\n\t \n\tvlan_macip_lens = l4.hdr - ip.hdr;\n\tvlan_macip_lens |= (ip.hdr - skb->data) << IGC_ADVTXD_MACLEN_SHIFT;\n\tvlan_macip_lens |= first->tx_flags & IGC_TX_FLAGS_VLAN_MASK;\n\n\tigc_tx_ctxtdesc(tx_ring, launch_time, first_flag,\n\t\t\tvlan_macip_lens, type_tucmd, mss_l4len_idx);\n\n\treturn 1;\n}\n\nstatic bool igc_request_tx_tstamp(struct igc_adapter *adapter, struct sk_buff *skb, u32 *flags)\n{\n\tint i;\n\n\tfor (i = 0; i < IGC_MAX_TX_TSTAMP_REGS; i++) {\n\t\tstruct igc_tx_timestamp_request *tstamp = &adapter->tx_tstamp[i];\n\n\t\tif (tstamp->skb)\n\t\t\tcontinue;\n\n\t\ttstamp->skb = skb_get(skb);\n\t\ttstamp->start = jiffies;\n\t\t*flags = tstamp->flags;\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic netdev_tx_t igc_xmit_frame_ring(struct sk_buff *skb,\n\t\t\t\t       struct igc_ring *tx_ring)\n{\n\tstruct igc_adapter *adapter = netdev_priv(tx_ring->netdev);\n\tbool first_flag = false, insert_empty = false;\n\tu16 count = TXD_USE_COUNT(skb_headlen(skb));\n\t__be16 protocol = vlan_get_protocol(skb);\n\tstruct igc_tx_buffer *first;\n\t__le32 launch_time = 0;\n\tu32 tx_flags = 0;\n\tunsigned short f;\n\tktime_t txtime;\n\tu8 hdr_len = 0;\n\tint tso = 0;\n\n\t \n\tfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++)\n\t\tcount += TXD_USE_COUNT(skb_frag_size(\n\t\t\t\t\t\t&skb_shinfo(skb)->frags[f]));\n\n\tif (igc_maybe_stop_tx(tx_ring, count + 5)) {\n\t\t \n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (!tx_ring->launchtime_enable)\n\t\tgoto done;\n\n\ttxtime = skb->tstamp;\n\tskb->tstamp = ktime_set(0, 0);\n\tlaunch_time = igc_tx_launchtime(tx_ring, txtime, &first_flag, &insert_empty);\n\n\tif (insert_empty) {\n\t\tstruct igc_tx_buffer *empty_info;\n\t\tstruct sk_buff *empty;\n\t\tvoid *data;\n\n\t\tempty_info = &tx_ring->tx_buffer_info[tx_ring->next_to_use];\n\t\tempty = alloc_skb(IGC_EMPTY_FRAME_SIZE, GFP_ATOMIC);\n\t\tif (!empty)\n\t\t\tgoto done;\n\n\t\tdata = skb_put(empty, IGC_EMPTY_FRAME_SIZE);\n\t\tmemset(data, 0, IGC_EMPTY_FRAME_SIZE);\n\n\t\tigc_tx_ctxtdesc(tx_ring, 0, false, 0, 0, 0);\n\n\t\tif (igc_init_tx_empty_descriptor(tx_ring,\n\t\t\t\t\t\t empty,\n\t\t\t\t\t\t empty_info) < 0)\n\t\t\tdev_kfree_skb_any(empty);\n\t}\n\ndone:\n\t \n\tfirst = &tx_ring->tx_buffer_info[tx_ring->next_to_use];\n\tfirst->type = IGC_TX_BUFFER_TYPE_SKB;\n\tfirst->skb = skb;\n\tfirst->bytecount = skb->len;\n\tfirst->gso_segs = 1;\n\n\tif (adapter->qbv_transition || tx_ring->oper_gate_closed)\n\t\tgoto out_drop;\n\n\tif (tx_ring->max_sdu > 0 && first->bytecount > tx_ring->max_sdu) {\n\t\tadapter->stats.txdrop++;\n\t\tgoto out_drop;\n\t}\n\n\tif (unlikely(test_bit(IGC_RING_FLAG_TX_HWTSTAMP, &tx_ring->flags) &&\n\t\t     skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {\n\t\t \n\t\tunsigned long flags;\n\t\tu32 tstamp_flags;\n\n\t\tspin_lock_irqsave(&adapter->ptp_tx_lock, flags);\n\t\tif (igc_request_tx_tstamp(adapter, skb, &tstamp_flags)) {\n\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t\ttx_flags |= IGC_TX_FLAGS_TSTAMP | tstamp_flags;\n\t\t} else {\n\t\t\tadapter->tx_hwtstamp_skipped++;\n\t\t}\n\n\t\tspin_unlock_irqrestore(&adapter->ptp_tx_lock, flags);\n\t}\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\ttx_flags |= IGC_TX_FLAGS_VLAN;\n\t\ttx_flags |= (skb_vlan_tag_get(skb) << IGC_TX_FLAGS_VLAN_SHIFT);\n\t}\n\n\t \n\tfirst->tx_flags = tx_flags;\n\tfirst->protocol = protocol;\n\n\ttso = igc_tso(tx_ring, first, launch_time, first_flag, &hdr_len);\n\tif (tso < 0)\n\t\tgoto out_drop;\n\telse if (!tso)\n\t\tigc_tx_csum(tx_ring, first, launch_time, first_flag);\n\n\tigc_tx_map(tx_ring, first, hdr_len);\n\n\treturn NETDEV_TX_OK;\n\nout_drop:\n\tdev_kfree_skb_any(first->skb);\n\tfirst->skb = NULL;\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic inline struct igc_ring *igc_tx_queue_mapping(struct igc_adapter *adapter,\n\t\t\t\t\t\t    struct sk_buff *skb)\n{\n\tunsigned int r_idx = skb->queue_mapping;\n\n\tif (r_idx >= adapter->num_tx_queues)\n\t\tr_idx = r_idx % adapter->num_tx_queues;\n\n\treturn adapter->tx_ring[r_idx];\n}\n\nstatic netdev_tx_t igc_xmit_frame(struct sk_buff *skb,\n\t\t\t\t  struct net_device *netdev)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\t \n\tif (skb->len < 17) {\n\t\tif (skb_padto(skb, 17))\n\t\t\treturn NETDEV_TX_OK;\n\t\tskb->len = 17;\n\t}\n\n\treturn igc_xmit_frame_ring(skb, igc_tx_queue_mapping(adapter, skb));\n}\n\nstatic void igc_rx_checksum(struct igc_ring *ring,\n\t\t\t    union igc_adv_rx_desc *rx_desc,\n\t\t\t    struct sk_buff *skb)\n{\n\tskb_checksum_none_assert(skb);\n\n\t \n\tif (igc_test_staterr(rx_desc, IGC_RXD_STAT_IXSM))\n\t\treturn;\n\n\t \n\tif (!(ring->netdev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\t \n\tif (igc_test_staterr(rx_desc,\n\t\t\t     IGC_RXDEXT_STATERR_L4E |\n\t\t\t     IGC_RXDEXT_STATERR_IPE)) {\n\t\t \n\t\tif (!(skb->len == 60 &&\n\t\t      test_bit(IGC_RING_FLAG_RX_SCTP_CSUM, &ring->flags))) {\n\t\t\tu64_stats_update_begin(&ring->rx_syncp);\n\t\t\tring->rx_stats.csum_err++;\n\t\t\tu64_stats_update_end(&ring->rx_syncp);\n\t\t}\n\t\t \n\t\treturn;\n\t}\n\t \n\tif (igc_test_staterr(rx_desc, IGC_RXD_STAT_TCPCS |\n\t\t\t\t      IGC_RXD_STAT_UDPCS))\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tnetdev_dbg(ring->netdev, \"cksum success: bits %08X\\n\",\n\t\t   le32_to_cpu(rx_desc->wb.upper.status_error));\n}\n\n \nstatic const enum pkt_hash_types igc_rss_type_table[IGC_RSS_TYPE_MAX_TABLE] = {\n\t[IGC_RSS_TYPE_NO_HASH]\t\t= PKT_HASH_TYPE_L2,\n\t[IGC_RSS_TYPE_HASH_TCP_IPV4]\t= PKT_HASH_TYPE_L4,\n\t[IGC_RSS_TYPE_HASH_IPV4]\t= PKT_HASH_TYPE_L3,\n\t[IGC_RSS_TYPE_HASH_TCP_IPV6]\t= PKT_HASH_TYPE_L4,\n\t[IGC_RSS_TYPE_HASH_IPV6_EX]\t= PKT_HASH_TYPE_L3,\n\t[IGC_RSS_TYPE_HASH_IPV6]\t= PKT_HASH_TYPE_L3,\n\t[IGC_RSS_TYPE_HASH_TCP_IPV6_EX] = PKT_HASH_TYPE_L4,\n\t[IGC_RSS_TYPE_HASH_UDP_IPV4]\t= PKT_HASH_TYPE_L4,\n\t[IGC_RSS_TYPE_HASH_UDP_IPV6]\t= PKT_HASH_TYPE_L4,\n\t[IGC_RSS_TYPE_HASH_UDP_IPV6_EX] = PKT_HASH_TYPE_L4,\n\t[10] = PKT_HASH_TYPE_NONE,  \n\t[11] = PKT_HASH_TYPE_NONE,  \n\t[12] = PKT_HASH_TYPE_NONE,  \n\t[13] = PKT_HASH_TYPE_NONE,\n\t[14] = PKT_HASH_TYPE_NONE,\n\t[15] = PKT_HASH_TYPE_NONE,\n};\n\nstatic inline void igc_rx_hash(struct igc_ring *ring,\n\t\t\t       union igc_adv_rx_desc *rx_desc,\n\t\t\t       struct sk_buff *skb)\n{\n\tif (ring->netdev->features & NETIF_F_RXHASH) {\n\t\tu32 rss_hash = le32_to_cpu(rx_desc->wb.lower.hi_dword.rss);\n\t\tu32 rss_type = igc_rss_type(rx_desc);\n\n\t\tskb_set_hash(skb, rss_hash, igc_rss_type_table[rss_type]);\n\t}\n}\n\nstatic void igc_rx_vlan(struct igc_ring *rx_ring,\n\t\t\tunion igc_adv_rx_desc *rx_desc,\n\t\t\tstruct sk_buff *skb)\n{\n\tstruct net_device *dev = rx_ring->netdev;\n\tu16 vid;\n\n\tif ((dev->features & NETIF_F_HW_VLAN_CTAG_RX) &&\n\t    igc_test_staterr(rx_desc, IGC_RXD_STAT_VP)) {\n\t\tif (igc_test_staterr(rx_desc, IGC_RXDEXT_STATERR_LB) &&\n\t\t    test_bit(IGC_RING_FLAG_RX_LB_VLAN_BSWAP, &rx_ring->flags))\n\t\t\tvid = be16_to_cpu((__force __be16)rx_desc->wb.upper.vlan);\n\t\telse\n\t\t\tvid = le16_to_cpu(rx_desc->wb.upper.vlan);\n\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);\n\t}\n}\n\n \nstatic void igc_process_skb_fields(struct igc_ring *rx_ring,\n\t\t\t\t   union igc_adv_rx_desc *rx_desc,\n\t\t\t\t   struct sk_buff *skb)\n{\n\tigc_rx_hash(rx_ring, rx_desc, skb);\n\n\tigc_rx_checksum(rx_ring, rx_desc, skb);\n\n\tigc_rx_vlan(rx_ring, rx_desc, skb);\n\n\tskb_record_rx_queue(skb, rx_ring->queue_index);\n\n\tskb->protocol = eth_type_trans(skb, rx_ring->netdev);\n}\n\nstatic void igc_vlan_mode(struct net_device *netdev, netdev_features_t features)\n{\n\tbool enable = !!(features & NETIF_F_HW_VLAN_CTAG_RX);\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 ctrl;\n\n\tctrl = rd32(IGC_CTRL);\n\n\tif (enable) {\n\t\t \n\t\tctrl |= IGC_CTRL_VME;\n\t} else {\n\t\t \n\t\tctrl &= ~IGC_CTRL_VME;\n\t}\n\twr32(IGC_CTRL, ctrl);\n}\n\nstatic void igc_restore_vlan(struct igc_adapter *adapter)\n{\n\tigc_vlan_mode(adapter->netdev, adapter->netdev->features);\n}\n\nstatic struct igc_rx_buffer *igc_get_rx_buffer(struct igc_ring *rx_ring,\n\t\t\t\t\t       const unsigned int size,\n\t\t\t\t\t       int *rx_buffer_pgcnt)\n{\n\tstruct igc_rx_buffer *rx_buffer;\n\n\trx_buffer = &rx_ring->rx_buffer_info[rx_ring->next_to_clean];\n\t*rx_buffer_pgcnt =\n#if (PAGE_SIZE < 8192)\n\t\tpage_count(rx_buffer->page);\n#else\n\t\t0;\n#endif\n\tprefetchw(rx_buffer->page);\n\n\t \n\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t      rx_buffer->dma,\n\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t      size,\n\t\t\t\t      DMA_FROM_DEVICE);\n\n\trx_buffer->pagecnt_bias--;\n\n\treturn rx_buffer;\n}\n\nstatic void igc_rx_buffer_flip(struct igc_rx_buffer *buffer,\n\t\t\t       unsigned int truesize)\n{\n#if (PAGE_SIZE < 8192)\n\tbuffer->page_offset ^= truesize;\n#else\n\tbuffer->page_offset += truesize;\n#endif\n}\n\nstatic unsigned int igc_get_rx_frame_truesize(struct igc_ring *ring,\n\t\t\t\t\t      unsigned int size)\n{\n\tunsigned int truesize;\n\n#if (PAGE_SIZE < 8192)\n\ttruesize = igc_rx_pg_size(ring) / 2;\n#else\n\ttruesize = ring_uses_build_skb(ring) ?\n\t\t   SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) +\n\t\t   SKB_DATA_ALIGN(IGC_SKB_PAD + size) :\n\t\t   SKB_DATA_ALIGN(size);\n#endif\n\treturn truesize;\n}\n\n \nstatic void igc_add_rx_frag(struct igc_ring *rx_ring,\n\t\t\t    struct igc_rx_buffer *rx_buffer,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    unsigned int size)\n{\n\tunsigned int truesize;\n\n#if (PAGE_SIZE < 8192)\n\ttruesize = igc_rx_pg_size(rx_ring) / 2;\n#else\n\ttruesize = ring_uses_build_skb(rx_ring) ?\n\t\t   SKB_DATA_ALIGN(IGC_SKB_PAD + size) :\n\t\t   SKB_DATA_ALIGN(size);\n#endif\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_buffer->page,\n\t\t\trx_buffer->page_offset, size, truesize);\n\n\tigc_rx_buffer_flip(rx_buffer, truesize);\n}\n\nstatic struct sk_buff *igc_build_skb(struct igc_ring *rx_ring,\n\t\t\t\t     struct igc_rx_buffer *rx_buffer,\n\t\t\t\t     struct xdp_buff *xdp)\n{\n\tunsigned int size = xdp->data_end - xdp->data;\n\tunsigned int truesize = igc_get_rx_frame_truesize(rx_ring, size);\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data_meta);\n\n\t \n\tskb = napi_build_skb(xdp->data_hard_start, truesize);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\t__skb_put(skb, size);\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\tigc_rx_buffer_flip(rx_buffer, truesize);\n\treturn skb;\n}\n\nstatic struct sk_buff *igc_construct_skb(struct igc_ring *rx_ring,\n\t\t\t\t\t struct igc_rx_buffer *rx_buffer,\n\t\t\t\t\t struct xdp_buff *xdp,\n\t\t\t\t\t ktime_t timestamp)\n{\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n\tunsigned int size = xdp->data_end - xdp->data;\n\tunsigned int truesize = igc_get_rx_frame_truesize(rx_ring, size);\n\tvoid *va = xdp->data;\n\tunsigned int headlen;\n\tstruct sk_buff *skb;\n\n\t \n\tnet_prefetch(xdp->data_meta);\n\n\t \n\tskb = napi_alloc_skb(&rx_ring->q_vector->napi,\n\t\t\t     IGC_RX_HDR_LEN + metasize);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tif (timestamp)\n\t\tskb_hwtstamps(skb)->hwtstamp = timestamp;\n\n\t \n\theadlen = size;\n\tif (headlen > IGC_RX_HDR_LEN)\n\t\theadlen = eth_get_headlen(skb->dev, va, IGC_RX_HDR_LEN);\n\n\t \n\tmemcpy(__skb_put(skb, headlen + metasize), xdp->data_meta,\n\t       ALIGN(headlen + metasize, sizeof(long)));\n\n\tif (metasize) {\n\t\tskb_metadata_set(skb, metasize);\n\t\t__skb_pull(skb, metasize);\n\t}\n\n\t \n\tsize -= headlen;\n\tif (size) {\n\t\tskb_add_rx_frag(skb, 0, rx_buffer->page,\n\t\t\t\t(va + headlen) - page_address(rx_buffer->page),\n\t\t\t\tsize, truesize);\n\t\tigc_rx_buffer_flip(rx_buffer, truesize);\n\t} else {\n\t\trx_buffer->pagecnt_bias++;\n\t}\n\n\treturn skb;\n}\n\n \nstatic void igc_reuse_rx_page(struct igc_ring *rx_ring,\n\t\t\t      struct igc_rx_buffer *old_buff)\n{\n\tu16 nta = rx_ring->next_to_alloc;\n\tstruct igc_rx_buffer *new_buff;\n\n\tnew_buff = &rx_ring->rx_buffer_info[nta];\n\n\t \n\tnta++;\n\trx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;\n\n\t \n\tnew_buff->dma\t\t= old_buff->dma;\n\tnew_buff->page\t\t= old_buff->page;\n\tnew_buff->page_offset\t= old_buff->page_offset;\n\tnew_buff->pagecnt_bias\t= old_buff->pagecnt_bias;\n}\n\nstatic bool igc_can_reuse_rx_page(struct igc_rx_buffer *rx_buffer,\n\t\t\t\t  int rx_buffer_pgcnt)\n{\n\tunsigned int pagecnt_bias = rx_buffer->pagecnt_bias;\n\tstruct page *page = rx_buffer->page;\n\n\t \n\tif (!dev_page_is_reusable(page))\n\t\treturn false;\n\n#if (PAGE_SIZE < 8192)\n\t \n\tif (unlikely((rx_buffer_pgcnt - pagecnt_bias) > 1))\n\t\treturn false;\n#else\n#define IGC_LAST_OFFSET \\\n\t(SKB_WITH_OVERHEAD(PAGE_SIZE) - IGC_RXBUFFER_2048)\n\n\tif (rx_buffer->page_offset > IGC_LAST_OFFSET)\n\t\treturn false;\n#endif\n\n\t \n\tif (unlikely(pagecnt_bias == 1)) {\n\t\tpage_ref_add(page, USHRT_MAX - 1);\n\t\trx_buffer->pagecnt_bias = USHRT_MAX;\n\t}\n\n\treturn true;\n}\n\n \nstatic bool igc_is_non_eop(struct igc_ring *rx_ring,\n\t\t\t   union igc_adv_rx_desc *rx_desc)\n{\n\tu32 ntc = rx_ring->next_to_clean + 1;\n\n\t \n\tntc = (ntc < rx_ring->count) ? ntc : 0;\n\trx_ring->next_to_clean = ntc;\n\n\tprefetch(IGC_RX_DESC(rx_ring, ntc));\n\n\tif (likely(igc_test_staterr(rx_desc, IGC_RXD_STAT_EOP)))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic bool igc_cleanup_headers(struct igc_ring *rx_ring,\n\t\t\t\tunion igc_adv_rx_desc *rx_desc,\n\t\t\t\tstruct sk_buff *skb)\n{\n\t \n\tif (IS_ERR(skb))\n\t\treturn true;\n\n\tif (unlikely(igc_test_staterr(rx_desc, IGC_RXDEXT_STATERR_RXE))) {\n\t\tstruct net_device *netdev = rx_ring->netdev;\n\n\t\tif (!(netdev->features & NETIF_F_RXALL)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t \n\tif (eth_skb_pad(skb))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void igc_put_rx_buffer(struct igc_ring *rx_ring,\n\t\t\t      struct igc_rx_buffer *rx_buffer,\n\t\t\t      int rx_buffer_pgcnt)\n{\n\tif (igc_can_reuse_rx_page(rx_buffer, rx_buffer_pgcnt)) {\n\t\t \n\t\tigc_reuse_rx_page(rx_ring, rx_buffer);\n\t} else {\n\t\t \n\t\tdma_unmap_page_attrs(rx_ring->dev, rx_buffer->dma,\n\t\t\t\t     igc_rx_pg_size(rx_ring), DMA_FROM_DEVICE,\n\t\t\t\t     IGC_RX_DMA_ATTR);\n\t\t__page_frag_cache_drain(rx_buffer->page,\n\t\t\t\t\trx_buffer->pagecnt_bias);\n\t}\n\n\t \n\trx_buffer->page = NULL;\n}\n\nstatic inline unsigned int igc_rx_offset(struct igc_ring *rx_ring)\n{\n\tstruct igc_adapter *adapter = rx_ring->q_vector->adapter;\n\n\tif (ring_uses_build_skb(rx_ring))\n\t\treturn IGC_SKB_PAD;\n\tif (igc_xdp_is_enabled(adapter))\n\t\treturn XDP_PACKET_HEADROOM;\n\n\treturn 0;\n}\n\nstatic bool igc_alloc_mapped_page(struct igc_ring *rx_ring,\n\t\t\t\t  struct igc_rx_buffer *bi)\n{\n\tstruct page *page = bi->page;\n\tdma_addr_t dma;\n\n\t \n\tif (likely(page))\n\t\treturn true;\n\n\t \n\tpage = dev_alloc_pages(igc_rx_pg_order(rx_ring));\n\tif (unlikely(!page)) {\n\t\trx_ring->rx_stats.alloc_failed++;\n\t\treturn false;\n\t}\n\n\t \n\tdma = dma_map_page_attrs(rx_ring->dev, page, 0,\n\t\t\t\t igc_rx_pg_size(rx_ring),\n\t\t\t\t DMA_FROM_DEVICE,\n\t\t\t\t IGC_RX_DMA_ATTR);\n\n\t \n\tif (dma_mapping_error(rx_ring->dev, dma)) {\n\t\t__free_page(page);\n\n\t\trx_ring->rx_stats.alloc_failed++;\n\t\treturn false;\n\t}\n\n\tbi->dma = dma;\n\tbi->page = page;\n\tbi->page_offset = igc_rx_offset(rx_ring);\n\tpage_ref_add(page, USHRT_MAX - 1);\n\tbi->pagecnt_bias = USHRT_MAX;\n\n\treturn true;\n}\n\n \nstatic void igc_alloc_rx_buffers(struct igc_ring *rx_ring, u16 cleaned_count)\n{\n\tunion igc_adv_rx_desc *rx_desc;\n\tu16 i = rx_ring->next_to_use;\n\tstruct igc_rx_buffer *bi;\n\tu16 bufsz;\n\n\t \n\tif (!cleaned_count)\n\t\treturn;\n\n\trx_desc = IGC_RX_DESC(rx_ring, i);\n\tbi = &rx_ring->rx_buffer_info[i];\n\ti -= rx_ring->count;\n\n\tbufsz = igc_rx_bufsz(rx_ring);\n\n\tdo {\n\t\tif (!igc_alloc_mapped_page(rx_ring, bi))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_sync_single_range_for_device(rx_ring->dev, bi->dma,\n\t\t\t\t\t\t bi->page_offset, bufsz,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t \n\t\trx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);\n\n\t\trx_desc++;\n\t\tbi++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\trx_desc = IGC_RX_DESC(rx_ring, 0);\n\t\t\tbi = rx_ring->rx_buffer_info;\n\t\t\ti -= rx_ring->count;\n\t\t}\n\n\t\t \n\t\trx_desc->wb.upper.length = 0;\n\n\t\tcleaned_count--;\n\t} while (cleaned_count);\n\n\ti += rx_ring->count;\n\n\tif (rx_ring->next_to_use != i) {\n\t\t \n\t\trx_ring->next_to_use = i;\n\n\t\t \n\t\trx_ring->next_to_alloc = i;\n\n\t\t \n\t\twmb();\n\t\twritel(i, rx_ring->tail);\n\t}\n}\n\nstatic bool igc_alloc_rx_buffers_zc(struct igc_ring *ring, u16 count)\n{\n\tunion igc_adv_rx_desc *desc;\n\tu16 i = ring->next_to_use;\n\tstruct igc_rx_buffer *bi;\n\tdma_addr_t dma;\n\tbool ok = true;\n\n\tif (!count)\n\t\treturn ok;\n\n\tXSK_CHECK_PRIV_TYPE(struct igc_xdp_buff);\n\n\tdesc = IGC_RX_DESC(ring, i);\n\tbi = &ring->rx_buffer_info[i];\n\ti -= ring->count;\n\n\tdo {\n\t\tbi->xdp = xsk_buff_alloc(ring->xsk_pool);\n\t\tif (!bi->xdp) {\n\t\t\tok = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tdma = xsk_buff_xdp_get_dma(bi->xdp);\n\t\tdesc->read.pkt_addr = cpu_to_le64(dma);\n\n\t\tdesc++;\n\t\tbi++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\tdesc = IGC_RX_DESC(ring, 0);\n\t\t\tbi = ring->rx_buffer_info;\n\t\t\ti -= ring->count;\n\t\t}\n\n\t\t \n\t\tdesc->wb.upper.length = 0;\n\n\t\tcount--;\n\t} while (count);\n\n\ti += ring->count;\n\n\tif (ring->next_to_use != i) {\n\t\tring->next_to_use = i;\n\n\t\t \n\t\twmb();\n\t\twritel(i, ring->tail);\n\t}\n\n\treturn ok;\n}\n\n \nstatic int igc_xdp_init_tx_descriptor(struct igc_ring *ring,\n\t\t\t\t      struct xdp_frame *xdpf)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_frame(xdpf);\n\tu8 nr_frags = unlikely(xdp_frame_has_frags(xdpf)) ? sinfo->nr_frags : 0;\n\tu16 count, index = ring->next_to_use;\n\tstruct igc_tx_buffer *head = &ring->tx_buffer_info[index];\n\tstruct igc_tx_buffer *buffer = head;\n\tunion igc_adv_tx_desc *desc = IGC_TX_DESC(ring, index);\n\tu32 olinfo_status, len = xdpf->len, cmd_type;\n\tvoid *data = xdpf->data;\n\tu16 i;\n\n\tcount = TXD_USE_COUNT(len);\n\tfor (i = 0; i < nr_frags; i++)\n\t\tcount += TXD_USE_COUNT(skb_frag_size(&sinfo->frags[i]));\n\n\tif (igc_maybe_stop_tx(ring, count + 3)) {\n\t\t \n\t\treturn -EBUSY;\n\t}\n\n\ti = 0;\n\thead->bytecount = xdp_get_frame_len(xdpf);\n\thead->type = IGC_TX_BUFFER_TYPE_XDP;\n\thead->gso_segs = 1;\n\thead->xdpf = xdpf;\n\n\tolinfo_status = head->bytecount << IGC_ADVTXD_PAYLEN_SHIFT;\n\tdesc->read.olinfo_status = cpu_to_le32(olinfo_status);\n\n\tfor (;;) {\n\t\tdma_addr_t dma;\n\n\t\tdma = dma_map_single(ring->dev, data, len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(ring->dev, dma)) {\n\t\t\tnetdev_err_once(ring->netdev,\n\t\t\t\t\t\"Failed to map DMA for TX\\n\");\n\t\t\tgoto unmap;\n\t\t}\n\n\t\tdma_unmap_len_set(buffer, len, len);\n\t\tdma_unmap_addr_set(buffer, dma, dma);\n\n\t\tcmd_type = IGC_ADVTXD_DTYP_DATA | IGC_ADVTXD_DCMD_DEXT |\n\t\t\t   IGC_ADVTXD_DCMD_IFCS | len;\n\n\t\tdesc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\t\tdesc->read.buffer_addr = cpu_to_le64(dma);\n\n\t\tbuffer->protocol = 0;\n\n\t\tif (++index == ring->count)\n\t\t\tindex = 0;\n\n\t\tif (i == nr_frags)\n\t\t\tbreak;\n\n\t\tbuffer = &ring->tx_buffer_info[index];\n\t\tdesc = IGC_TX_DESC(ring, index);\n\t\tdesc->read.olinfo_status = 0;\n\n\t\tdata = skb_frag_address(&sinfo->frags[i]);\n\t\tlen = skb_frag_size(&sinfo->frags[i]);\n\t\ti++;\n\t}\n\tdesc->read.cmd_type_len |= cpu_to_le32(IGC_TXD_DCMD);\n\n\tnetdev_tx_sent_queue(txring_txq(ring), head->bytecount);\n\t \n\thead->time_stamp = jiffies;\n\t \n\thead->next_to_watch = desc;\n\tring->next_to_use = index;\n\n\treturn 0;\n\nunmap:\n\tfor (;;) {\n\t\tbuffer = &ring->tx_buffer_info[index];\n\t\tif (dma_unmap_len(buffer, len))\n\t\t\tdma_unmap_page(ring->dev,\n\t\t\t\t       dma_unmap_addr(buffer, dma),\n\t\t\t\t       dma_unmap_len(buffer, len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(buffer, len, 0);\n\t\tif (buffer == head)\n\t\t\tbreak;\n\n\t\tif (!index)\n\t\t\tindex += ring->count;\n\t\tindex--;\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic struct igc_ring *igc_xdp_get_tx_ring(struct igc_adapter *adapter,\n\t\t\t\t\t    int cpu)\n{\n\tint index = cpu;\n\n\tif (unlikely(index < 0))\n\t\tindex = 0;\n\n\twhile (index >= adapter->num_tx_queues)\n\t\tindex -= adapter->num_tx_queues;\n\n\treturn adapter->tx_ring[index];\n}\n\nstatic int igc_xdp_xmit_back(struct igc_adapter *adapter, struct xdp_buff *xdp)\n{\n\tstruct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);\n\tint cpu = smp_processor_id();\n\tstruct netdev_queue *nq;\n\tstruct igc_ring *ring;\n\tint res;\n\n\tif (unlikely(!xdpf))\n\t\treturn -EFAULT;\n\n\tring = igc_xdp_get_tx_ring(adapter, cpu);\n\tnq = txring_txq(ring);\n\n\t__netif_tx_lock(nq, cpu);\n\t \n\ttxq_trans_cond_update(nq);\n\tres = igc_xdp_init_tx_descriptor(ring, xdpf);\n\t__netif_tx_unlock(nq);\n\treturn res;\n}\n\n \nstatic int __igc_xdp_run_prog(struct igc_adapter *adapter,\n\t\t\t      struct bpf_prog *prog,\n\t\t\t      struct xdp_buff *xdp)\n{\n\tu32 act = bpf_prog_run_xdp(prog, xdp);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\treturn IGC_XDP_PASS;\n\tcase XDP_TX:\n\t\tif (igc_xdp_xmit_back(adapter, xdp) < 0)\n\t\t\tgoto out_failure;\n\t\treturn IGC_XDP_TX;\n\tcase XDP_REDIRECT:\n\t\tif (xdp_do_redirect(adapter->netdev, xdp, prog) < 0)\n\t\t\tgoto out_failure;\n\t\treturn IGC_XDP_REDIRECT;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(adapter->netdev, prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\nout_failure:\n\t\ttrace_xdp_exception(adapter->netdev, prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\treturn IGC_XDP_CONSUMED;\n\t}\n}\n\nstatic struct sk_buff *igc_xdp_run_prog(struct igc_adapter *adapter,\n\t\t\t\t\tstruct xdp_buff *xdp)\n{\n\tstruct bpf_prog *prog;\n\tint res;\n\n\tprog = READ_ONCE(adapter->xdp_prog);\n\tif (!prog) {\n\t\tres = IGC_XDP_PASS;\n\t\tgoto out;\n\t}\n\n\tres = __igc_xdp_run_prog(adapter, prog, xdp);\n\nout:\n\treturn ERR_PTR(-res);\n}\n\n \nstatic void igc_flush_tx_descriptors(struct igc_ring *ring)\n{\n\t \n\twmb();\n\twritel(ring->next_to_use, ring->tail);\n}\n\nstatic void igc_finalize_xdp(struct igc_adapter *adapter, int status)\n{\n\tint cpu = smp_processor_id();\n\tstruct netdev_queue *nq;\n\tstruct igc_ring *ring;\n\n\tif (status & IGC_XDP_TX) {\n\t\tring = igc_xdp_get_tx_ring(adapter, cpu);\n\t\tnq = txring_txq(ring);\n\n\t\t__netif_tx_lock(nq, cpu);\n\t\tigc_flush_tx_descriptors(ring);\n\t\t__netif_tx_unlock(nq);\n\t}\n\n\tif (status & IGC_XDP_REDIRECT)\n\t\txdp_do_flush();\n}\n\nstatic void igc_update_rx_stats(struct igc_q_vector *q_vector,\n\t\t\t\tunsigned int packets, unsigned int bytes)\n{\n\tstruct igc_ring *ring = q_vector->rx.ring;\n\n\tu64_stats_update_begin(&ring->rx_syncp);\n\tring->rx_stats.packets += packets;\n\tring->rx_stats.bytes += bytes;\n\tu64_stats_update_end(&ring->rx_syncp);\n\n\tq_vector->rx.total_packets += packets;\n\tq_vector->rx.total_bytes += bytes;\n}\n\nstatic int igc_clean_rx_irq(struct igc_q_vector *q_vector, const int budget)\n{\n\tunsigned int total_bytes = 0, total_packets = 0;\n\tstruct igc_adapter *adapter = q_vector->adapter;\n\tstruct igc_ring *rx_ring = q_vector->rx.ring;\n\tstruct sk_buff *skb = rx_ring->skb;\n\tu16 cleaned_count = igc_desc_unused(rx_ring);\n\tint xdp_status = 0, rx_buffer_pgcnt;\n\n\twhile (likely(total_packets < budget)) {\n\t\tunion igc_adv_rx_desc *rx_desc;\n\t\tstruct igc_rx_buffer *rx_buffer;\n\t\tunsigned int size, truesize;\n\t\tstruct igc_xdp_buff ctx;\n\t\tktime_t timestamp = 0;\n\t\tint pkt_offset = 0;\n\t\tvoid *pktbuf;\n\n\t\t \n\t\tif (cleaned_count >= IGC_RX_BUFFER_WRITE) {\n\t\t\tigc_alloc_rx_buffers(rx_ring, cleaned_count);\n\t\t\tcleaned_count = 0;\n\t\t}\n\n\t\trx_desc = IGC_RX_DESC(rx_ring, rx_ring->next_to_clean);\n\t\tsize = le16_to_cpu(rx_desc->wb.upper.length);\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\trx_buffer = igc_get_rx_buffer(rx_ring, size, &rx_buffer_pgcnt);\n\t\ttruesize = igc_get_rx_frame_truesize(rx_ring, size);\n\n\t\tpktbuf = page_address(rx_buffer->page) + rx_buffer->page_offset;\n\n\t\tif (igc_test_staterr(rx_desc, IGC_RXDADV_STAT_TSIP)) {\n\t\t\ttimestamp = igc_ptp_rx_pktstamp(q_vector->adapter,\n\t\t\t\t\t\t\tpktbuf);\n\t\t\tctx.rx_ts = timestamp;\n\t\t\tpkt_offset = IGC_TS_HDR_LEN;\n\t\t\tsize -= IGC_TS_HDR_LEN;\n\t\t}\n\n\t\tif (!skb) {\n\t\t\txdp_init_buff(&ctx.xdp, truesize, &rx_ring->xdp_rxq);\n\t\t\txdp_prepare_buff(&ctx.xdp, pktbuf - igc_rx_offset(rx_ring),\n\t\t\t\t\t igc_rx_offset(rx_ring) + pkt_offset,\n\t\t\t\t\t size, true);\n\t\t\txdp_buff_clear_frags_flag(&ctx.xdp);\n\t\t\tctx.rx_desc = rx_desc;\n\n\t\t\tskb = igc_xdp_run_prog(adapter, &ctx.xdp);\n\t\t}\n\n\t\tif (IS_ERR(skb)) {\n\t\t\tunsigned int xdp_res = -PTR_ERR(skb);\n\n\t\t\tswitch (xdp_res) {\n\t\t\tcase IGC_XDP_CONSUMED:\n\t\t\t\trx_buffer->pagecnt_bias++;\n\t\t\t\tbreak;\n\t\t\tcase IGC_XDP_TX:\n\t\t\tcase IGC_XDP_REDIRECT:\n\t\t\t\tigc_rx_buffer_flip(rx_buffer, truesize);\n\t\t\t\txdp_status |= xdp_res;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\ttotal_packets++;\n\t\t\ttotal_bytes += size;\n\t\t} else if (skb)\n\t\t\tigc_add_rx_frag(rx_ring, rx_buffer, skb, size);\n\t\telse if (ring_uses_build_skb(rx_ring))\n\t\t\tskb = igc_build_skb(rx_ring, rx_buffer, &ctx.xdp);\n\t\telse\n\t\t\tskb = igc_construct_skb(rx_ring, rx_buffer, &ctx.xdp,\n\t\t\t\t\t\ttimestamp);\n\n\t\t \n\t\tif (!skb) {\n\t\t\trx_ring->rx_stats.alloc_failed++;\n\t\t\trx_buffer->pagecnt_bias++;\n\t\t\tbreak;\n\t\t}\n\n\t\tigc_put_rx_buffer(rx_ring, rx_buffer, rx_buffer_pgcnt);\n\t\tcleaned_count++;\n\n\t\t \n\t\tif (igc_is_non_eop(rx_ring, rx_desc))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (igc_cleanup_headers(rx_ring, rx_desc, skb)) {\n\t\t\tskb = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\ttotal_bytes += skb->len;\n\n\t\t \n\t\tigc_process_skb_fields(rx_ring, rx_desc, skb);\n\n\t\tnapi_gro_receive(&q_vector->napi, skb);\n\n\t\t \n\t\tskb = NULL;\n\n\t\t \n\t\ttotal_packets++;\n\t}\n\n\tif (xdp_status)\n\t\tigc_finalize_xdp(adapter, xdp_status);\n\n\t \n\trx_ring->skb = skb;\n\n\tigc_update_rx_stats(q_vector, total_packets, total_bytes);\n\n\tif (cleaned_count)\n\t\tigc_alloc_rx_buffers(rx_ring, cleaned_count);\n\n\treturn total_packets;\n}\n\nstatic struct sk_buff *igc_construct_skb_zc(struct igc_ring *ring,\n\t\t\t\t\t    struct xdp_buff *xdp)\n{\n\tunsigned int totalsize = xdp->data_end - xdp->data_meta;\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n\tstruct sk_buff *skb;\n\n\tnet_prefetch(xdp->data_meta);\n\n\tskb = __napi_alloc_skb(&ring->q_vector->napi, totalsize,\n\t\t\t       GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tmemcpy(__skb_put(skb, totalsize), xdp->data_meta,\n\t       ALIGN(totalsize, sizeof(long)));\n\n\tif (metasize) {\n\t\tskb_metadata_set(skb, metasize);\n\t\t__skb_pull(skb, metasize);\n\t}\n\n\treturn skb;\n}\n\nstatic void igc_dispatch_skb_zc(struct igc_q_vector *q_vector,\n\t\t\t\tunion igc_adv_rx_desc *desc,\n\t\t\t\tstruct xdp_buff *xdp,\n\t\t\t\tktime_t timestamp)\n{\n\tstruct igc_ring *ring = q_vector->rx.ring;\n\tstruct sk_buff *skb;\n\n\tskb = igc_construct_skb_zc(ring, xdp);\n\tif (!skb) {\n\t\tring->rx_stats.alloc_failed++;\n\t\treturn;\n\t}\n\n\tif (timestamp)\n\t\tskb_hwtstamps(skb)->hwtstamp = timestamp;\n\n\tif (igc_cleanup_headers(ring, desc, skb))\n\t\treturn;\n\n\tigc_process_skb_fields(ring, desc, skb);\n\tnapi_gro_receive(&q_vector->napi, skb);\n}\n\nstatic struct igc_xdp_buff *xsk_buff_to_igc_ctx(struct xdp_buff *xdp)\n{\n\t \n       return (struct igc_xdp_buff *)xdp;\n}\n\nstatic int igc_clean_rx_irq_zc(struct igc_q_vector *q_vector, const int budget)\n{\n\tstruct igc_adapter *adapter = q_vector->adapter;\n\tstruct igc_ring *ring = q_vector->rx.ring;\n\tu16 cleaned_count = igc_desc_unused(ring);\n\tint total_bytes = 0, total_packets = 0;\n\tu16 ntc = ring->next_to_clean;\n\tstruct bpf_prog *prog;\n\tbool failure = false;\n\tint xdp_status = 0;\n\n\trcu_read_lock();\n\n\tprog = READ_ONCE(adapter->xdp_prog);\n\n\twhile (likely(total_packets < budget)) {\n\t\tunion igc_adv_rx_desc *desc;\n\t\tstruct igc_rx_buffer *bi;\n\t\tstruct igc_xdp_buff *ctx;\n\t\tktime_t timestamp = 0;\n\t\tunsigned int size;\n\t\tint res;\n\n\t\tdesc = IGC_RX_DESC(ring, ntc);\n\t\tsize = le16_to_cpu(desc->wb.upper.length);\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tbi = &ring->rx_buffer_info[ntc];\n\n\t\tctx = xsk_buff_to_igc_ctx(bi->xdp);\n\t\tctx->rx_desc = desc;\n\n\t\tif (igc_test_staterr(desc, IGC_RXDADV_STAT_TSIP)) {\n\t\t\ttimestamp = igc_ptp_rx_pktstamp(q_vector->adapter,\n\t\t\t\t\t\t\tbi->xdp->data);\n\t\t\tctx->rx_ts = timestamp;\n\n\t\t\tbi->xdp->data += IGC_TS_HDR_LEN;\n\n\t\t\t \n\t\t\tbi->xdp->data_meta += IGC_TS_HDR_LEN;\n\t\t\tsize -= IGC_TS_HDR_LEN;\n\t\t}\n\n\t\tbi->xdp->data_end = bi->xdp->data + size;\n\t\txsk_buff_dma_sync_for_cpu(bi->xdp, ring->xsk_pool);\n\n\t\tres = __igc_xdp_run_prog(adapter, prog, bi->xdp);\n\t\tswitch (res) {\n\t\tcase IGC_XDP_PASS:\n\t\t\tigc_dispatch_skb_zc(q_vector, desc, bi->xdp, timestamp);\n\t\t\tfallthrough;\n\t\tcase IGC_XDP_CONSUMED:\n\t\t\txsk_buff_free(bi->xdp);\n\t\t\tbreak;\n\t\tcase IGC_XDP_TX:\n\t\tcase IGC_XDP_REDIRECT:\n\t\t\txdp_status |= res;\n\t\t\tbreak;\n\t\t}\n\n\t\tbi->xdp = NULL;\n\t\ttotal_bytes += size;\n\t\ttotal_packets++;\n\t\tcleaned_count++;\n\t\tntc++;\n\t\tif (ntc == ring->count)\n\t\t\tntc = 0;\n\t}\n\n\tring->next_to_clean = ntc;\n\trcu_read_unlock();\n\n\tif (cleaned_count >= IGC_RX_BUFFER_WRITE)\n\t\tfailure = !igc_alloc_rx_buffers_zc(ring, cleaned_count);\n\n\tif (xdp_status)\n\t\tigc_finalize_xdp(adapter, xdp_status);\n\n\tigc_update_rx_stats(q_vector, total_packets, total_bytes);\n\n\tif (xsk_uses_need_wakeup(ring->xsk_pool)) {\n\t\tif (failure || ring->next_to_clean == ring->next_to_use)\n\t\t\txsk_set_rx_need_wakeup(ring->xsk_pool);\n\t\telse\n\t\t\txsk_clear_rx_need_wakeup(ring->xsk_pool);\n\t\treturn total_packets;\n\t}\n\n\treturn failure ? budget : total_packets;\n}\n\nstatic void igc_update_tx_stats(struct igc_q_vector *q_vector,\n\t\t\t\tunsigned int packets, unsigned int bytes)\n{\n\tstruct igc_ring *ring = q_vector->tx.ring;\n\n\tu64_stats_update_begin(&ring->tx_syncp);\n\tring->tx_stats.bytes += bytes;\n\tring->tx_stats.packets += packets;\n\tu64_stats_update_end(&ring->tx_syncp);\n\n\tq_vector->tx.total_bytes += bytes;\n\tq_vector->tx.total_packets += packets;\n}\n\nstatic void igc_xdp_xmit_zc(struct igc_ring *ring)\n{\n\tstruct xsk_buff_pool *pool = ring->xsk_pool;\n\tstruct netdev_queue *nq = txring_txq(ring);\n\tunion igc_adv_tx_desc *tx_desc = NULL;\n\tint cpu = smp_processor_id();\n\tstruct xdp_desc xdp_desc;\n\tu16 budget, ntu;\n\n\tif (!netif_carrier_ok(ring->netdev))\n\t\treturn;\n\n\t__netif_tx_lock(nq, cpu);\n\n\t \n\ttxq_trans_cond_update(nq);\n\n\tntu = ring->next_to_use;\n\tbudget = igc_desc_unused(ring);\n\n\twhile (xsk_tx_peek_desc(pool, &xdp_desc) && budget--) {\n\t\tu32 cmd_type, olinfo_status;\n\t\tstruct igc_tx_buffer *bi;\n\t\tdma_addr_t dma;\n\n\t\tcmd_type = IGC_ADVTXD_DTYP_DATA | IGC_ADVTXD_DCMD_DEXT |\n\t\t\t   IGC_ADVTXD_DCMD_IFCS | IGC_TXD_DCMD |\n\t\t\t   xdp_desc.len;\n\t\tolinfo_status = xdp_desc.len << IGC_ADVTXD_PAYLEN_SHIFT;\n\n\t\tdma = xsk_buff_raw_get_dma(pool, xdp_desc.addr);\n\t\txsk_buff_raw_dma_sync_for_device(pool, dma, xdp_desc.len);\n\n\t\ttx_desc = IGC_TX_DESC(ring, ntu);\n\t\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\t\ttx_desc->read.olinfo_status = cpu_to_le32(olinfo_status);\n\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\n\t\tbi = &ring->tx_buffer_info[ntu];\n\t\tbi->type = IGC_TX_BUFFER_TYPE_XSK;\n\t\tbi->protocol = 0;\n\t\tbi->bytecount = xdp_desc.len;\n\t\tbi->gso_segs = 1;\n\t\tbi->time_stamp = jiffies;\n\t\tbi->next_to_watch = tx_desc;\n\n\t\tnetdev_tx_sent_queue(txring_txq(ring), xdp_desc.len);\n\n\t\tntu++;\n\t\tif (ntu == ring->count)\n\t\t\tntu = 0;\n\t}\n\n\tring->next_to_use = ntu;\n\tif (tx_desc) {\n\t\tigc_flush_tx_descriptors(ring);\n\t\txsk_tx_release(pool);\n\t}\n\n\t__netif_tx_unlock(nq);\n}\n\n \nstatic bool igc_clean_tx_irq(struct igc_q_vector *q_vector, int napi_budget)\n{\n\tstruct igc_adapter *adapter = q_vector->adapter;\n\tunsigned int total_bytes = 0, total_packets = 0;\n\tunsigned int budget = q_vector->tx.work_limit;\n\tstruct igc_ring *tx_ring = q_vector->tx.ring;\n\tunsigned int i = tx_ring->next_to_clean;\n\tstruct igc_tx_buffer *tx_buffer;\n\tunion igc_adv_tx_desc *tx_desc;\n\tu32 xsk_frames = 0;\n\n\tif (test_bit(__IGC_DOWN, &adapter->state))\n\t\treturn true;\n\n\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\ttx_desc = IGC_TX_DESC(tx_ring, i);\n\ti -= tx_ring->count;\n\n\tdo {\n\t\tunion igc_adv_tx_desc *eop_desc = tx_buffer->next_to_watch;\n\n\t\t \n\t\tif (!eop_desc)\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\t \n\t\tif (!(eop_desc->wb.status & cpu_to_le32(IGC_TXD_STAT_DD)))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buffer->next_to_watch = NULL;\n\n\t\t \n\t\ttotal_bytes += tx_buffer->bytecount;\n\t\ttotal_packets += tx_buffer->gso_segs;\n\n\t\tswitch (tx_buffer->type) {\n\t\tcase IGC_TX_BUFFER_TYPE_XSK:\n\t\t\txsk_frames++;\n\t\t\tbreak;\n\t\tcase IGC_TX_BUFFER_TYPE_XDP:\n\t\t\txdp_return_frame(tx_buffer->xdpf);\n\t\t\tigc_unmap_tx_buffer(tx_ring->dev, tx_buffer);\n\t\t\tbreak;\n\t\tcase IGC_TX_BUFFER_TYPE_SKB:\n\t\t\tnapi_consume_skb(tx_buffer->skb, napi_budget);\n\t\t\tigc_unmap_tx_buffer(tx_ring->dev, tx_buffer);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tnetdev_warn_once(tx_ring->netdev, \"Unknown Tx buffer type\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(!i)) {\n\t\t\t\ti -= tx_ring->count;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = IGC_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\t\tigc_unmap_tx_buffer(tx_ring->dev, tx_buffer);\n\t\t}\n\n\t\t \n\t\ttx_buffer++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\ttx_desc = IGC_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\t \n\t\tprefetch(tx_desc);\n\n\t\t \n\t\tbudget--;\n\t} while (likely(budget));\n\n\tnetdev_tx_completed_queue(txring_txq(tx_ring),\n\t\t\t\t  total_packets, total_bytes);\n\n\ti += tx_ring->count;\n\ttx_ring->next_to_clean = i;\n\n\tigc_update_tx_stats(q_vector, total_packets, total_bytes);\n\n\tif (tx_ring->xsk_pool) {\n\t\tif (xsk_frames)\n\t\t\txsk_tx_completed(tx_ring->xsk_pool, xsk_frames);\n\t\tif (xsk_uses_need_wakeup(tx_ring->xsk_pool))\n\t\t\txsk_set_tx_need_wakeup(tx_ring->xsk_pool);\n\t\tigc_xdp_xmit_zc(tx_ring);\n\t}\n\n\tif (test_bit(IGC_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags)) {\n\t\tstruct igc_hw *hw = &adapter->hw;\n\n\t\t \n\t\tclear_bit(IGC_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags);\n\t\tif (tx_buffer->next_to_watch &&\n\t\t    time_after(jiffies, tx_buffer->time_stamp +\n\t\t    (adapter->tx_timeout_factor * HZ)) &&\n\t\t    !(rd32(IGC_STATUS) & IGC_STATUS_TXOFF) &&\n\t\t    (rd32(IGC_TDH(tx_ring->reg_idx)) != readl(tx_ring->tail)) &&\n\t\t    !tx_ring->oper_gate_closed) {\n\t\t\t \n\t\t\tnetdev_err(tx_ring->netdev,\n\t\t\t\t   \"Detected Tx Unit Hang\\n\"\n\t\t\t\t   \"  Tx Queue             <%d>\\n\"\n\t\t\t\t   \"  TDH                  <%x>\\n\"\n\t\t\t\t   \"  TDT                  <%x>\\n\"\n\t\t\t\t   \"  next_to_use          <%x>\\n\"\n\t\t\t\t   \"  next_to_clean        <%x>\\n\"\n\t\t\t\t   \"buffer_info[next_to_clean]\\n\"\n\t\t\t\t   \"  time_stamp           <%lx>\\n\"\n\t\t\t\t   \"  next_to_watch        <%p>\\n\"\n\t\t\t\t   \"  jiffies              <%lx>\\n\"\n\t\t\t\t   \"  desc.status          <%x>\\n\",\n\t\t\t\t   tx_ring->queue_index,\n\t\t\t\t   rd32(IGC_TDH(tx_ring->reg_idx)),\n\t\t\t\t   readl(tx_ring->tail),\n\t\t\t\t   tx_ring->next_to_use,\n\t\t\t\t   tx_ring->next_to_clean,\n\t\t\t\t   tx_buffer->time_stamp,\n\t\t\t\t   tx_buffer->next_to_watch,\n\t\t\t\t   jiffies,\n\t\t\t\t   tx_buffer->next_to_watch->wb.status);\n\t\t\tnetif_stop_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\n\t\t\t \n\t\t\treturn true;\n\t\t}\n\t}\n\n#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)\n\tif (unlikely(total_packets &&\n\t\t     netif_carrier_ok(tx_ring->netdev) &&\n\t\t     igc_desc_unused(tx_ring) >= TX_WAKE_THRESHOLD)) {\n\t\t \n\t\tsmp_mb();\n\t\tif (__netif_subqueue_stopped(tx_ring->netdev,\n\t\t\t\t\t     tx_ring->queue_index) &&\n\t\t    !(test_bit(__IGC_DOWN, &adapter->state))) {\n\t\t\tnetif_wake_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\n\t\t\tu64_stats_update_begin(&tx_ring->tx_syncp);\n\t\t\ttx_ring->tx_stats.restart_queue++;\n\t\t\tu64_stats_update_end(&tx_ring->tx_syncp);\n\t\t}\n\t}\n\n\treturn !!budget;\n}\n\nstatic int igc_find_mac_filter(struct igc_adapter *adapter,\n\t\t\t       enum igc_mac_filter_type type, const u8 *addr)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint max_entries = hw->mac.rar_entry_count;\n\tu32 ral, rah;\n\tint i;\n\n\tfor (i = 0; i < max_entries; i++) {\n\t\tral = rd32(IGC_RAL(i));\n\t\trah = rd32(IGC_RAH(i));\n\n\t\tif (!(rah & IGC_RAH_AV))\n\t\t\tcontinue;\n\t\tif (!!(rah & IGC_RAH_ASEL_SRC_ADDR) != type)\n\t\t\tcontinue;\n\t\tif ((rah & IGC_RAH_RAH_MASK) !=\n\t\t    le16_to_cpup((__le16 *)(addr + 4)))\n\t\t\tcontinue;\n\t\tif (ral != le32_to_cpup((__le32 *)(addr)))\n\t\t\tcontinue;\n\n\t\treturn i;\n\t}\n\n\treturn -1;\n}\n\nstatic int igc_get_avail_mac_filter_slot(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint max_entries = hw->mac.rar_entry_count;\n\tu32 rah;\n\tint i;\n\n\tfor (i = 0; i < max_entries; i++) {\n\t\trah = rd32(IGC_RAH(i));\n\n\t\tif (!(rah & IGC_RAH_AV))\n\t\t\treturn i;\n\t}\n\n\treturn -1;\n}\n\n \nstatic int igc_add_mac_filter(struct igc_adapter *adapter,\n\t\t\t      enum igc_mac_filter_type type, const u8 *addr,\n\t\t\t      int queue)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tint index;\n\n\tindex = igc_find_mac_filter(adapter, type, addr);\n\tif (index >= 0)\n\t\tgoto update_filter;\n\n\tindex = igc_get_avail_mac_filter_slot(adapter);\n\tif (index < 0)\n\t\treturn -ENOSPC;\n\n\tnetdev_dbg(dev, \"Add MAC address filter: index %d type %s address %pM queue %d\\n\",\n\t\t   index, type == IGC_MAC_FILTER_TYPE_DST ? \"dst\" : \"src\",\n\t\t   addr, queue);\n\nupdate_filter:\n\tigc_set_mac_filter_hw(adapter, index, type, addr, queue);\n\treturn 0;\n}\n\n \nstatic void igc_del_mac_filter(struct igc_adapter *adapter,\n\t\t\t       enum igc_mac_filter_type type, const u8 *addr)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tint index;\n\n\tindex = igc_find_mac_filter(adapter, type, addr);\n\tif (index < 0)\n\t\treturn;\n\n\tif (index == 0) {\n\t\t \n\t\tnetdev_dbg(dev, \"Disable default MAC filter queue assignment\");\n\n\t\tigc_set_mac_filter_hw(adapter, 0, type, addr, -1);\n\t} else {\n\t\tnetdev_dbg(dev, \"Delete MAC address filter: index %d type %s address %pM\\n\",\n\t\t\t   index,\n\t\t\t   type == IGC_MAC_FILTER_TYPE_DST ? \"dst\" : \"src\",\n\t\t\t   addr);\n\n\t\tigc_clear_mac_filter_hw(adapter, index);\n\t}\n}\n\n \nstatic int igc_add_vlan_prio_filter(struct igc_adapter *adapter, int prio,\n\t\t\t\t    int queue)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 vlanpqf;\n\n\tvlanpqf = rd32(IGC_VLANPQF);\n\n\tif (vlanpqf & IGC_VLANPQF_VALID(prio)) {\n\t\tnetdev_dbg(dev, \"VLAN priority filter already in use\\n\");\n\t\treturn -EEXIST;\n\t}\n\n\tvlanpqf |= IGC_VLANPQF_QSEL(prio, queue);\n\tvlanpqf |= IGC_VLANPQF_VALID(prio);\n\n\twr32(IGC_VLANPQF, vlanpqf);\n\n\tnetdev_dbg(dev, \"Add VLAN priority filter: prio %d queue %d\\n\",\n\t\t   prio, queue);\n\treturn 0;\n}\n\n \nstatic void igc_del_vlan_prio_filter(struct igc_adapter *adapter, int prio)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 vlanpqf;\n\n\tvlanpqf = rd32(IGC_VLANPQF);\n\n\tvlanpqf &= ~IGC_VLANPQF_VALID(prio);\n\tvlanpqf &= ~IGC_VLANPQF_QSEL(prio, IGC_VLANPQF_QUEUE_MASK);\n\n\twr32(IGC_VLANPQF, vlanpqf);\n\n\tnetdev_dbg(adapter->netdev, \"Delete VLAN priority filter: prio %d\\n\",\n\t\t   prio);\n}\n\nstatic int igc_get_avail_etype_filter_slot(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint i;\n\n\tfor (i = 0; i < MAX_ETYPE_FILTER; i++) {\n\t\tu32 etqf = rd32(IGC_ETQF(i));\n\n\t\tif (!(etqf & IGC_ETQF_FILTER_ENABLE))\n\t\t\treturn i;\n\t}\n\n\treturn -1;\n}\n\n \nstatic int igc_add_etype_filter(struct igc_adapter *adapter, u16 etype,\n\t\t\t\tint queue)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint index;\n\tu32 etqf;\n\n\tindex = igc_get_avail_etype_filter_slot(adapter);\n\tif (index < 0)\n\t\treturn -ENOSPC;\n\n\tetqf = rd32(IGC_ETQF(index));\n\n\tetqf &= ~IGC_ETQF_ETYPE_MASK;\n\tetqf |= etype;\n\n\tif (queue >= 0) {\n\t\tetqf &= ~IGC_ETQF_QUEUE_MASK;\n\t\tetqf |= (queue << IGC_ETQF_QUEUE_SHIFT);\n\t\tetqf |= IGC_ETQF_QUEUE_ENABLE;\n\t}\n\n\tetqf |= IGC_ETQF_FILTER_ENABLE;\n\n\twr32(IGC_ETQF(index), etqf);\n\n\tnetdev_dbg(adapter->netdev, \"Add ethertype filter: etype %04x queue %d\\n\",\n\t\t   etype, queue);\n\treturn 0;\n}\n\nstatic int igc_find_etype_filter(struct igc_adapter *adapter, u16 etype)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint i;\n\n\tfor (i = 0; i < MAX_ETYPE_FILTER; i++) {\n\t\tu32 etqf = rd32(IGC_ETQF(i));\n\n\t\tif ((etqf & IGC_ETQF_ETYPE_MASK) == etype)\n\t\t\treturn i;\n\t}\n\n\treturn -1;\n}\n\n \nstatic void igc_del_etype_filter(struct igc_adapter *adapter, u16 etype)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint index;\n\n\tindex = igc_find_etype_filter(adapter, etype);\n\tif (index < 0)\n\t\treturn;\n\n\twr32(IGC_ETQF(index), 0);\n\n\tnetdev_dbg(adapter->netdev, \"Delete ethertype filter: etype %04x\\n\",\n\t\t   etype);\n}\n\nstatic int igc_flex_filter_select(struct igc_adapter *adapter,\n\t\t\t\t  struct igc_flex_filter *input,\n\t\t\t\t  u32 *fhft)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu8 fhft_index;\n\tu32 fhftsl;\n\n\tif (input->index >= MAX_FLEX_FILTER) {\n\t\tdev_err(&adapter->pdev->dev, \"Wrong Flex Filter index selected!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tfhftsl = rd32(IGC_FHFTSL);\n\tfhftsl &= ~IGC_FHFTSL_FTSL_MASK;\n\tswitch (input->index) {\n\tcase 0 ... 7:\n\t\tfhftsl |= 0x00;\n\t\tbreak;\n\tcase 8 ... 15:\n\t\tfhftsl |= 0x01;\n\t\tbreak;\n\tcase 16 ... 23:\n\t\tfhftsl |= 0x02;\n\t\tbreak;\n\tcase 24 ... 31:\n\t\tfhftsl |= 0x03;\n\t\tbreak;\n\t}\n\twr32(IGC_FHFTSL, fhftsl);\n\n\t \n\tfhft_index = input->index % 8;\n\n\t*fhft = (fhft_index < 4) ? IGC_FHFT(fhft_index) :\n\t\tIGC_FHFT_EXT(fhft_index - 4);\n\n\treturn 0;\n}\n\nstatic int igc_write_flex_filter_ll(struct igc_adapter *adapter,\n\t\t\t\t    struct igc_flex_filter *input)\n{\n\tstruct device *dev = &adapter->pdev->dev;\n\tstruct igc_hw *hw = &adapter->hw;\n\tu8 *data = input->data;\n\tu8 *mask = input->mask;\n\tu32 queuing;\n\tu32 fhft;\n\tu32 wufc;\n\tint ret;\n\tint i;\n\n\t \n\tif (input->length % 8 != 0) {\n\t\tdev_err(dev, \"The length of a flex filter has to be 8 byte aligned!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tret = igc_flex_filter_select(adapter, input, &fhft);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\twufc = rd32(IGC_WUFC);\n\twufc &= ~IGC_WUFC_FLEX_HQ;\n\twr32(IGC_WUFC, wufc);\n\n\t \n\tqueuing = input->length & IGC_FHFT_LENGTH_MASK;\n\tqueuing |= (input->rx_queue << IGC_FHFT_QUEUE_SHIFT) & IGC_FHFT_QUEUE_MASK;\n\tqueuing |= (input->prio << IGC_FHFT_PRIO_SHIFT) & IGC_FHFT_PRIO_MASK;\n\n\tif (input->immediate_irq)\n\t\tqueuing |= IGC_FHFT_IMM_INT;\n\n\tif (input->drop)\n\t\tqueuing |= IGC_FHFT_DROP;\n\n\twr32(fhft + 0xFC, queuing);\n\n\t \n\tfor (i = 0; i < 16; ++i) {\n\t\tconst size_t data_idx = i * 8;\n\t\tconst size_t row_idx = i * 16;\n\t\tu32 dw0 =\n\t\t\t(data[data_idx + 0] << 0) |\n\t\t\t(data[data_idx + 1] << 8) |\n\t\t\t(data[data_idx + 2] << 16) |\n\t\t\t(data[data_idx + 3] << 24);\n\t\tu32 dw1 =\n\t\t\t(data[data_idx + 4] << 0) |\n\t\t\t(data[data_idx + 5] << 8) |\n\t\t\t(data[data_idx + 6] << 16) |\n\t\t\t(data[data_idx + 7] << 24);\n\t\tu32 tmp;\n\n\t\t \n\t\twr32(fhft + row_idx, dw0);\n\t\twr32(fhft + row_idx + 4, dw1);\n\n\t\t \n\t\ttmp = rd32(fhft + row_idx + 8);\n\t\ttmp &= ~GENMASK(7, 0);\n\t\ttmp |= mask[i];\n\t\twr32(fhft + row_idx + 8, tmp);\n\t}\n\n\t \n\twufc |= IGC_WUFC_FLEX_HQ;\n\tif (input->index > 8) {\n\t\t \n\t\tu32 wufc_ext = rd32(IGC_WUFC_EXT);\n\n\t\twufc_ext |= (IGC_WUFC_EXT_FLX8 << (input->index - 8));\n\n\t\twr32(IGC_WUFC_EXT, wufc_ext);\n\t} else {\n\t\twufc |= (IGC_WUFC_FLX0 << input->index);\n\t}\n\twr32(IGC_WUFC, wufc);\n\n\tdev_dbg(&adapter->pdev->dev, \"Added flex filter %u to HW.\\n\",\n\t\tinput->index);\n\n\treturn 0;\n}\n\nstatic void igc_flex_filter_add_field(struct igc_flex_filter *flex,\n\t\t\t\t      const void *src, unsigned int offset,\n\t\t\t\t      size_t len, const void *mask)\n{\n\tint i;\n\n\t \n\tmemcpy(&flex->data[offset], src, len);\n\n\t \n\tfor (i = 0; i < len; ++i) {\n\t\tconst unsigned int idx = i + offset;\n\t\tconst u8 *ptr = mask;\n\n\t\tif (mask) {\n\t\t\tif (ptr[i] & 0xff)\n\t\t\t\tflex->mask[idx / 8] |= BIT(idx % 8);\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tflex->mask[idx / 8] |= BIT(idx % 8);\n\t}\n}\n\nstatic int igc_find_avail_flex_filter_slot(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 wufc, wufc_ext;\n\tint i;\n\n\twufc = rd32(IGC_WUFC);\n\twufc_ext = rd32(IGC_WUFC_EXT);\n\n\tfor (i = 0; i < MAX_FLEX_FILTER; i++) {\n\t\tif (i < 8) {\n\t\t\tif (!(wufc & (IGC_WUFC_FLX0 << i)))\n\t\t\t\treturn i;\n\t\t} else {\n\t\t\tif (!(wufc_ext & (IGC_WUFC_EXT_FLX8 << (i - 8))))\n\t\t\t\treturn i;\n\t\t}\n\t}\n\n\treturn -ENOSPC;\n}\n\nstatic bool igc_flex_filter_in_use(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 wufc, wufc_ext;\n\n\twufc = rd32(IGC_WUFC);\n\twufc_ext = rd32(IGC_WUFC_EXT);\n\n\tif (wufc & IGC_WUFC_FILTER_MASK)\n\t\treturn true;\n\n\tif (wufc_ext & IGC_WUFC_EXT_FILTER_MASK)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int igc_add_flex_filter(struct igc_adapter *adapter,\n\t\t\t       struct igc_nfc_rule *rule)\n{\n\tstruct igc_flex_filter flex = { };\n\tstruct igc_nfc_filter *filter = &rule->filter;\n\tunsigned int eth_offset, user_offset;\n\tint ret, index;\n\tbool vlan;\n\n\tindex = igc_find_avail_flex_filter_slot(adapter);\n\tif (index < 0)\n\t\treturn -ENOSPC;\n\n\t \n\tflex.index    = index;\n\tflex.length   = 32;\n\tflex.rx_queue = rule->action;\n\n\tvlan = rule->filter.vlan_tci || rule->filter.vlan_etype;\n\teth_offset = vlan ? 16 : 12;\n\tuser_offset = vlan ? 18 : 14;\n\n\t \n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_DST_MAC_ADDR)\n\t\tigc_flex_filter_add_field(&flex, &filter->dst_addr, 0,\n\t\t\t\t\t  ETH_ALEN, NULL);\n\n\t \n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_SRC_MAC_ADDR)\n\t\tigc_flex_filter_add_field(&flex, &filter->src_addr, 6,\n\t\t\t\t\t  ETH_ALEN, NULL);\n\n\t \n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_VLAN_ETYPE)\n\t\tigc_flex_filter_add_field(&flex, &filter->vlan_etype, 12,\n\t\t\t\t\t  sizeof(filter->vlan_etype),\n\t\t\t\t\t  NULL);\n\n\t \n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_VLAN_TCI)\n\t\tigc_flex_filter_add_field(&flex, &filter->vlan_tci, 14,\n\t\t\t\t\t  sizeof(filter->vlan_tci), NULL);\n\n\t \n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_ETHER_TYPE) {\n\t\t__be16 etype = cpu_to_be16(filter->etype);\n\n\t\tigc_flex_filter_add_field(&flex, &etype, eth_offset,\n\t\t\t\t\t  sizeof(etype), NULL);\n\t}\n\n\t \n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_USER_DATA)\n\t\tigc_flex_filter_add_field(&flex, &filter->user_data,\n\t\t\t\t\t  user_offset,\n\t\t\t\t\t  sizeof(filter->user_data),\n\t\t\t\t\t  filter->user_mask);\n\n\t \n\tret = igc_write_flex_filter_ll(adapter, &flex);\n\tif (ret)\n\t\treturn ret;\n\n\tfilter->flex_index = index;\n\n\treturn 0;\n}\n\nstatic void igc_del_flex_filter(struct igc_adapter *adapter,\n\t\t\t\tu16 reg_index)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 wufc;\n\n\t \n\tif (reg_index > 8) {\n\t\tu32 wufc_ext = rd32(IGC_WUFC_EXT);\n\n\t\twufc_ext &= ~(IGC_WUFC_EXT_FLX8 << (reg_index - 8));\n\t\twr32(IGC_WUFC_EXT, wufc_ext);\n\t} else {\n\t\twufc = rd32(IGC_WUFC);\n\n\t\twufc &= ~(IGC_WUFC_FLX0 << reg_index);\n\t\twr32(IGC_WUFC, wufc);\n\t}\n\n\tif (igc_flex_filter_in_use(adapter))\n\t\treturn;\n\n\t \n\twufc = rd32(IGC_WUFC);\n\twufc &= ~IGC_WUFC_FLEX_HQ;\n\twr32(IGC_WUFC, wufc);\n}\n\nstatic int igc_enable_nfc_rule(struct igc_adapter *adapter,\n\t\t\t       struct igc_nfc_rule *rule)\n{\n\tint err;\n\n\tif (rule->flex) {\n\t\treturn igc_add_flex_filter(adapter, rule);\n\t}\n\n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_ETHER_TYPE) {\n\t\terr = igc_add_etype_filter(adapter, rule->filter.etype,\n\t\t\t\t\t   rule->action);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_SRC_MAC_ADDR) {\n\t\terr = igc_add_mac_filter(adapter, IGC_MAC_FILTER_TYPE_SRC,\n\t\t\t\t\t rule->filter.src_addr, rule->action);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_DST_MAC_ADDR) {\n\t\terr = igc_add_mac_filter(adapter, IGC_MAC_FILTER_TYPE_DST,\n\t\t\t\t\t rule->filter.dst_addr, rule->action);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_VLAN_TCI) {\n\t\tint prio = (rule->filter.vlan_tci & VLAN_PRIO_MASK) >>\n\t\t\t   VLAN_PRIO_SHIFT;\n\n\t\terr = igc_add_vlan_prio_filter(adapter, prio, rule->action);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void igc_disable_nfc_rule(struct igc_adapter *adapter,\n\t\t\t\t const struct igc_nfc_rule *rule)\n{\n\tif (rule->flex) {\n\t\tigc_del_flex_filter(adapter, rule->filter.flex_index);\n\t\treturn;\n\t}\n\n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_ETHER_TYPE)\n\t\tigc_del_etype_filter(adapter, rule->filter.etype);\n\n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_VLAN_TCI) {\n\t\tint prio = (rule->filter.vlan_tci & VLAN_PRIO_MASK) >>\n\t\t\t   VLAN_PRIO_SHIFT;\n\n\t\tigc_del_vlan_prio_filter(adapter, prio);\n\t}\n\n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_SRC_MAC_ADDR)\n\t\tigc_del_mac_filter(adapter, IGC_MAC_FILTER_TYPE_SRC,\n\t\t\t\t   rule->filter.src_addr);\n\n\tif (rule->filter.match_flags & IGC_FILTER_FLAG_DST_MAC_ADDR)\n\t\tigc_del_mac_filter(adapter, IGC_MAC_FILTER_TYPE_DST,\n\t\t\t\t   rule->filter.dst_addr);\n}\n\n \nstruct igc_nfc_rule *igc_get_nfc_rule(struct igc_adapter *adapter,\n\t\t\t\t      u32 location)\n{\n\tstruct igc_nfc_rule *rule;\n\n\tlist_for_each_entry(rule, &adapter->nfc_rule_list, list) {\n\t\tif (rule->location == location)\n\t\t\treturn rule;\n\t\tif (rule->location > location)\n\t\t\tbreak;\n\t}\n\n\treturn NULL;\n}\n\n \nvoid igc_del_nfc_rule(struct igc_adapter *adapter, struct igc_nfc_rule *rule)\n{\n\tigc_disable_nfc_rule(adapter, rule);\n\n\tlist_del(&rule->list);\n\tadapter->nfc_rule_count--;\n\n\tkfree(rule);\n}\n\nstatic void igc_flush_nfc_rules(struct igc_adapter *adapter)\n{\n\tstruct igc_nfc_rule *rule, *tmp;\n\n\tmutex_lock(&adapter->nfc_rule_lock);\n\n\tlist_for_each_entry_safe(rule, tmp, &adapter->nfc_rule_list, list)\n\t\tigc_del_nfc_rule(adapter, rule);\n\n\tmutex_unlock(&adapter->nfc_rule_lock);\n}\n\n \nint igc_add_nfc_rule(struct igc_adapter *adapter, struct igc_nfc_rule *rule)\n{\n\tstruct igc_nfc_rule *pred, *cur;\n\tint err;\n\n\terr = igc_enable_nfc_rule(adapter, rule);\n\tif (err)\n\t\treturn err;\n\n\tpred = NULL;\n\tlist_for_each_entry(cur, &adapter->nfc_rule_list, list) {\n\t\tif (cur->location >= rule->location)\n\t\t\tbreak;\n\t\tpred = cur;\n\t}\n\n\tlist_add(&rule->list, pred ? &pred->list : &adapter->nfc_rule_list);\n\tadapter->nfc_rule_count++;\n\treturn 0;\n}\n\nstatic void igc_restore_nfc_rules(struct igc_adapter *adapter)\n{\n\tstruct igc_nfc_rule *rule;\n\n\tmutex_lock(&adapter->nfc_rule_lock);\n\n\tlist_for_each_entry_reverse(rule, &adapter->nfc_rule_list, list)\n\t\tigc_enable_nfc_rule(adapter, rule);\n\n\tmutex_unlock(&adapter->nfc_rule_lock);\n}\n\nstatic int igc_uc_sync(struct net_device *netdev, const unsigned char *addr)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\treturn igc_add_mac_filter(adapter, IGC_MAC_FILTER_TYPE_DST, addr, -1);\n}\n\nstatic int igc_uc_unsync(struct net_device *netdev, const unsigned char *addr)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\tigc_del_mac_filter(adapter, IGC_MAC_FILTER_TYPE_DST, addr);\n\treturn 0;\n}\n\n \nstatic void igc_set_rx_mode(struct net_device *netdev)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 rctl = 0, rlpml = MAX_JUMBO_FRAME_SIZE;\n\tint count;\n\n\t \n\tif (netdev->flags & IFF_PROMISC) {\n\t\trctl |= IGC_RCTL_UPE | IGC_RCTL_MPE;\n\t} else {\n\t\tif (netdev->flags & IFF_ALLMULTI) {\n\t\t\trctl |= IGC_RCTL_MPE;\n\t\t} else {\n\t\t\t \n\t\t\tcount = igc_write_mc_addr_list(netdev);\n\t\t\tif (count < 0)\n\t\t\t\trctl |= IGC_RCTL_MPE;\n\t\t}\n\t}\n\n\t \n\tif (__dev_uc_sync(netdev, igc_uc_sync, igc_uc_unsync))\n\t\trctl |= IGC_RCTL_UPE;\n\n\t \n\trctl |= rd32(IGC_RCTL) & ~(IGC_RCTL_UPE | IGC_RCTL_MPE);\n\twr32(IGC_RCTL, rctl);\n\n#if (PAGE_SIZE < 8192)\n\tif (adapter->max_frame_size <= IGC_MAX_FRAME_BUILD_SKB)\n\t\trlpml = IGC_MAX_FRAME_BUILD_SKB;\n#endif\n\twr32(IGC_RLPML, rlpml);\n}\n\n \nstatic void igc_configure(struct igc_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tint i = 0;\n\n\tigc_get_hw_control(adapter);\n\tigc_set_rx_mode(netdev);\n\n\tigc_restore_vlan(adapter);\n\n\tigc_setup_tctl(adapter);\n\tigc_setup_mrqc(adapter);\n\tigc_setup_rctl(adapter);\n\n\tigc_set_default_mac_filter(adapter);\n\tigc_restore_nfc_rules(adapter);\n\n\tigc_configure_tx(adapter);\n\tigc_configure_rx(adapter);\n\n\tigc_rx_fifo_flush_base(&adapter->hw);\n\n\t \n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct igc_ring *ring = adapter->rx_ring[i];\n\n\t\tif (ring->xsk_pool)\n\t\t\tigc_alloc_rx_buffers_zc(ring, igc_desc_unused(ring));\n\t\telse\n\t\t\tigc_alloc_rx_buffers(ring, igc_desc_unused(ring));\n\t}\n}\n\n \nstatic void igc_write_ivar(struct igc_hw *hw, int msix_vector,\n\t\t\t   int index, int offset)\n{\n\tu32 ivar = array_rd32(IGC_IVAR0, index);\n\n\t \n\tivar &= ~((u32)0xFF << offset);\n\n\t \n\tivar |= (msix_vector | IGC_IVAR_VALID) << offset;\n\n\tarray_wr32(IGC_IVAR0, index, ivar);\n}\n\nstatic void igc_assign_vector(struct igc_q_vector *q_vector, int msix_vector)\n{\n\tstruct igc_adapter *adapter = q_vector->adapter;\n\tstruct igc_hw *hw = &adapter->hw;\n\tint rx_queue = IGC_N0_QUEUE;\n\tint tx_queue = IGC_N0_QUEUE;\n\n\tif (q_vector->rx.ring)\n\t\trx_queue = q_vector->rx.ring->reg_idx;\n\tif (q_vector->tx.ring)\n\t\ttx_queue = q_vector->tx.ring->reg_idx;\n\n\tswitch (hw->mac.type) {\n\tcase igc_i225:\n\t\tif (rx_queue > IGC_N0_QUEUE)\n\t\t\tigc_write_ivar(hw, msix_vector,\n\t\t\t\t       rx_queue >> 1,\n\t\t\t\t       (rx_queue & 0x1) << 4);\n\t\tif (tx_queue > IGC_N0_QUEUE)\n\t\t\tigc_write_ivar(hw, msix_vector,\n\t\t\t\t       tx_queue >> 1,\n\t\t\t\t       ((tx_queue & 0x1) << 4) + 8);\n\t\tq_vector->eims_value = BIT(msix_vector);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(hw->mac.type != igc_i225, \"Wrong MAC type\\n\");\n\t\tbreak;\n\t}\n\n\t \n\tadapter->eims_enable_mask |= q_vector->eims_value;\n\n\t \n\tq_vector->set_itr = 1;\n}\n\n \nstatic void igc_configure_msix(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint i, vector = 0;\n\tu32 tmp;\n\n\tadapter->eims_enable_mask = 0;\n\n\t \n\tswitch (hw->mac.type) {\n\tcase igc_i225:\n\t\t \n\t\twr32(IGC_GPIE, IGC_GPIE_MSIX_MODE |\n\t\t     IGC_GPIE_PBA | IGC_GPIE_EIAME |\n\t\t     IGC_GPIE_NSICR);\n\n\t\t \n\t\tadapter->eims_other = BIT(vector);\n\t\ttmp = (vector++ | IGC_IVAR_VALID) << 8;\n\n\t\twr32(IGC_IVAR_MISC, tmp);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tbreak;\n\t}  \n\n\tadapter->eims_enable_mask |= adapter->eims_other;\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\tigc_assign_vector(adapter->q_vector[i], vector++);\n\n\twrfl();\n}\n\n \nstatic void igc_irq_enable(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\n\tif (adapter->msix_entries) {\n\t\tu32 ims = IGC_IMS_LSC | IGC_IMS_DOUTSYNC | IGC_IMS_DRSTA;\n\t\tu32 regval = rd32(IGC_EIAC);\n\n\t\twr32(IGC_EIAC, regval | adapter->eims_enable_mask);\n\t\tregval = rd32(IGC_EIAM);\n\t\twr32(IGC_EIAM, regval | adapter->eims_enable_mask);\n\t\twr32(IGC_EIMS, adapter->eims_enable_mask);\n\t\twr32(IGC_IMS, ims);\n\t} else {\n\t\twr32(IGC_IMS, IMS_ENABLE_MASK | IGC_IMS_DRSTA);\n\t\twr32(IGC_IAM, IMS_ENABLE_MASK | IGC_IMS_DRSTA);\n\t}\n}\n\n \nstatic void igc_irq_disable(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\n\tif (adapter->msix_entries) {\n\t\tu32 regval = rd32(IGC_EIAM);\n\n\t\twr32(IGC_EIAM, regval & ~adapter->eims_enable_mask);\n\t\twr32(IGC_EIMC, adapter->eims_enable_mask);\n\t\tregval = rd32(IGC_EIAC);\n\t\twr32(IGC_EIAC, regval & ~adapter->eims_enable_mask);\n\t}\n\n\twr32(IGC_IAM, 0);\n\twr32(IGC_IMC, ~0);\n\twrfl();\n\n\tif (adapter->msix_entries) {\n\t\tint vector = 0, i;\n\n\t\tsynchronize_irq(adapter->msix_entries[vector++].vector);\n\n\t\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\t\tsynchronize_irq(adapter->msix_entries[vector++].vector);\n\t} else {\n\t\tsynchronize_irq(adapter->pdev->irq);\n\t}\n}\n\nvoid igc_set_flag_queue_pairs(struct igc_adapter *adapter,\n\t\t\t      const u32 max_rss_queues)\n{\n\t \n\t \n\tif (adapter->rss_queues > (max_rss_queues / 2))\n\t\tadapter->flags |= IGC_FLAG_QUEUE_PAIRS;\n\telse\n\t\tadapter->flags &= ~IGC_FLAG_QUEUE_PAIRS;\n}\n\nunsigned int igc_get_max_rss_queues(struct igc_adapter *adapter)\n{\n\treturn IGC_MAX_RX_QUEUES;\n}\n\nstatic void igc_init_queue_configuration(struct igc_adapter *adapter)\n{\n\tu32 max_rss_queues;\n\n\tmax_rss_queues = igc_get_max_rss_queues(adapter);\n\tadapter->rss_queues = min_t(u32, max_rss_queues, num_online_cpus());\n\n\tigc_set_flag_queue_pairs(adapter, max_rss_queues);\n}\n\n \nstatic void igc_reset_q_vector(struct igc_adapter *adapter, int v_idx)\n{\n\tstruct igc_q_vector *q_vector = adapter->q_vector[v_idx];\n\n\t \n\tif (!q_vector)\n\t\treturn;\n\n\tif (q_vector->tx.ring)\n\t\tadapter->tx_ring[q_vector->tx.ring->queue_index] = NULL;\n\n\tif (q_vector->rx.ring)\n\t\tadapter->rx_ring[q_vector->rx.ring->queue_index] = NULL;\n\n\tnetif_napi_del(&q_vector->napi);\n}\n\n \nstatic void igc_free_q_vector(struct igc_adapter *adapter, int v_idx)\n{\n\tstruct igc_q_vector *q_vector = adapter->q_vector[v_idx];\n\n\tadapter->q_vector[v_idx] = NULL;\n\n\t \n\tif (q_vector)\n\t\tkfree_rcu(q_vector, rcu);\n}\n\n \nstatic void igc_free_q_vectors(struct igc_adapter *adapter)\n{\n\tint v_idx = adapter->num_q_vectors;\n\n\tadapter->num_tx_queues = 0;\n\tadapter->num_rx_queues = 0;\n\tadapter->num_q_vectors = 0;\n\n\twhile (v_idx--) {\n\t\tigc_reset_q_vector(adapter, v_idx);\n\t\tigc_free_q_vector(adapter, v_idx);\n\t}\n}\n\n \nstatic void igc_update_itr(struct igc_q_vector *q_vector,\n\t\t\t   struct igc_ring_container *ring_container)\n{\n\tunsigned int packets = ring_container->total_packets;\n\tunsigned int bytes = ring_container->total_bytes;\n\tu8 itrval = ring_container->itr;\n\n\t \n\tif (packets == 0)\n\t\treturn;\n\n\tswitch (itrval) {\n\tcase lowest_latency:\n\t\t \n\t\tif (bytes / packets > 8000)\n\t\t\titrval = bulk_latency;\n\t\telse if ((packets < 5) && (bytes > 512))\n\t\t\titrval = low_latency;\n\t\tbreak;\n\tcase low_latency:   \n\t\tif (bytes > 10000) {\n\t\t\t \n\t\t\tif (bytes / packets > 8000)\n\t\t\t\titrval = bulk_latency;\n\t\t\telse if ((packets < 10) || ((bytes / packets) > 1200))\n\t\t\t\titrval = bulk_latency;\n\t\t\telse if ((packets > 35))\n\t\t\t\titrval = lowest_latency;\n\t\t} else if (bytes / packets > 2000) {\n\t\t\titrval = bulk_latency;\n\t\t} else if (packets <= 2 && bytes < 512) {\n\t\t\titrval = lowest_latency;\n\t\t}\n\t\tbreak;\n\tcase bulk_latency:  \n\t\tif (bytes > 25000) {\n\t\t\tif (packets > 35)\n\t\t\t\titrval = low_latency;\n\t\t} else if (bytes < 1500) {\n\t\t\titrval = low_latency;\n\t\t}\n\t\tbreak;\n\t}\n\n\t \n\tring_container->total_bytes = 0;\n\tring_container->total_packets = 0;\n\n\t \n\tring_container->itr = itrval;\n}\n\nstatic void igc_set_itr(struct igc_q_vector *q_vector)\n{\n\tstruct igc_adapter *adapter = q_vector->adapter;\n\tu32 new_itr = q_vector->itr_val;\n\tu8 current_itr = 0;\n\n\t \n\tswitch (adapter->link_speed) {\n\tcase SPEED_10:\n\tcase SPEED_100:\n\t\tcurrent_itr = 0;\n\t\tnew_itr = IGC_4K_ITR;\n\t\tgoto set_itr_now;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tigc_update_itr(q_vector, &q_vector->tx);\n\tigc_update_itr(q_vector, &q_vector->rx);\n\n\tcurrent_itr = max(q_vector->rx.itr, q_vector->tx.itr);\n\n\t \n\tif (current_itr == lowest_latency &&\n\t    ((q_vector->rx.ring && adapter->rx_itr_setting == 3) ||\n\t    (!q_vector->rx.ring && adapter->tx_itr_setting == 3)))\n\t\tcurrent_itr = low_latency;\n\n\tswitch (current_itr) {\n\t \n\tcase lowest_latency:\n\t\tnew_itr = IGC_70K_ITR;  \n\t\tbreak;\n\tcase low_latency:\n\t\tnew_itr = IGC_20K_ITR;  \n\t\tbreak;\n\tcase bulk_latency:\n\t\tnew_itr = IGC_4K_ITR;   \n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\nset_itr_now:\n\tif (new_itr != q_vector->itr_val) {\n\t\t \n\t\tnew_itr = new_itr > q_vector->itr_val ?\n\t\t\t  max((new_itr * q_vector->itr_val) /\n\t\t\t  (new_itr + (q_vector->itr_val >> 2)),\n\t\t\t  new_itr) : new_itr;\n\t\t \n\t\tq_vector->itr_val = new_itr;\n\t\tq_vector->set_itr = 1;\n\t}\n}\n\nstatic void igc_reset_interrupt_capability(struct igc_adapter *adapter)\n{\n\tint v_idx = adapter->num_q_vectors;\n\n\tif (adapter->msix_entries) {\n\t\tpci_disable_msix(adapter->pdev);\n\t\tkfree(adapter->msix_entries);\n\t\tadapter->msix_entries = NULL;\n\t} else if (adapter->flags & IGC_FLAG_HAS_MSI) {\n\t\tpci_disable_msi(adapter->pdev);\n\t}\n\n\twhile (v_idx--)\n\t\tigc_reset_q_vector(adapter, v_idx);\n}\n\n \nstatic void igc_set_interrupt_capability(struct igc_adapter *adapter,\n\t\t\t\t\t bool msix)\n{\n\tint numvecs, i;\n\tint err;\n\n\tif (!msix)\n\t\tgoto msi_only;\n\tadapter->flags |= IGC_FLAG_HAS_MSIX;\n\n\t \n\tadapter->num_rx_queues = adapter->rss_queues;\n\n\tadapter->num_tx_queues = adapter->rss_queues;\n\n\t \n\tnumvecs = adapter->num_rx_queues;\n\n\t \n\tif (!(adapter->flags & IGC_FLAG_QUEUE_PAIRS))\n\t\tnumvecs += adapter->num_tx_queues;\n\n\t \n\tadapter->num_q_vectors = numvecs;\n\n\t \n\tnumvecs++;\n\n\tadapter->msix_entries = kcalloc(numvecs, sizeof(struct msix_entry),\n\t\t\t\t\tGFP_KERNEL);\n\n\tif (!adapter->msix_entries)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < numvecs; i++)\n\t\tadapter->msix_entries[i].entry = i;\n\n\terr = pci_enable_msix_range(adapter->pdev,\n\t\t\t\t    adapter->msix_entries,\n\t\t\t\t    numvecs,\n\t\t\t\t    numvecs);\n\tif (err > 0)\n\t\treturn;\n\n\tkfree(adapter->msix_entries);\n\tadapter->msix_entries = NULL;\n\n\tigc_reset_interrupt_capability(adapter);\n\nmsi_only:\n\tadapter->flags &= ~IGC_FLAG_HAS_MSIX;\n\n\tadapter->rss_queues = 1;\n\tadapter->flags |= IGC_FLAG_QUEUE_PAIRS;\n\tadapter->num_rx_queues = 1;\n\tadapter->num_tx_queues = 1;\n\tadapter->num_q_vectors = 1;\n\tif (!pci_enable_msi(adapter->pdev))\n\t\tadapter->flags |= IGC_FLAG_HAS_MSI;\n}\n\n \nstatic void igc_update_ring_itr(struct igc_q_vector *q_vector)\n{\n\tstruct igc_adapter *adapter = q_vector->adapter;\n\tint new_val = q_vector->itr_val;\n\tint avg_wire_size = 0;\n\tunsigned int packets;\n\n\t \n\tswitch (adapter->link_speed) {\n\tcase SPEED_10:\n\tcase SPEED_100:\n\t\tnew_val = IGC_4K_ITR;\n\t\tgoto set_itr_val;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tpackets = q_vector->rx.total_packets;\n\tif (packets)\n\t\tavg_wire_size = q_vector->rx.total_bytes / packets;\n\n\tpackets = q_vector->tx.total_packets;\n\tif (packets)\n\t\tavg_wire_size = max_t(u32, avg_wire_size,\n\t\t\t\t      q_vector->tx.total_bytes / packets);\n\n\t \n\tif (!avg_wire_size)\n\t\tgoto clear_counts;\n\n\t \n\tavg_wire_size += 24;\n\n\t \n\tavg_wire_size = min(avg_wire_size, 3000);\n\n\t \n\tif (avg_wire_size > 300 && avg_wire_size < 1200)\n\t\tnew_val = avg_wire_size / 3;\n\telse\n\t\tnew_val = avg_wire_size / 2;\n\n\t \n\tif (new_val < IGC_20K_ITR &&\n\t    ((q_vector->rx.ring && adapter->rx_itr_setting == 3) ||\n\t    (!q_vector->rx.ring && adapter->tx_itr_setting == 3)))\n\t\tnew_val = IGC_20K_ITR;\n\nset_itr_val:\n\tif (new_val != q_vector->itr_val) {\n\t\tq_vector->itr_val = new_val;\n\t\tq_vector->set_itr = 1;\n\t}\nclear_counts:\n\tq_vector->rx.total_bytes = 0;\n\tq_vector->rx.total_packets = 0;\n\tq_vector->tx.total_bytes = 0;\n\tq_vector->tx.total_packets = 0;\n}\n\nstatic void igc_ring_irq_enable(struct igc_q_vector *q_vector)\n{\n\tstruct igc_adapter *adapter = q_vector->adapter;\n\tstruct igc_hw *hw = &adapter->hw;\n\n\tif ((q_vector->rx.ring && (adapter->rx_itr_setting & 3)) ||\n\t    (!q_vector->rx.ring && (adapter->tx_itr_setting & 3))) {\n\t\tif (adapter->num_q_vectors == 1)\n\t\t\tigc_set_itr(q_vector);\n\t\telse\n\t\t\tigc_update_ring_itr(q_vector);\n\t}\n\n\tif (!test_bit(__IGC_DOWN, &adapter->state)) {\n\t\tif (adapter->msix_entries)\n\t\t\twr32(IGC_EIMS, q_vector->eims_value);\n\t\telse\n\t\t\tigc_irq_enable(adapter);\n\t}\n}\n\nstatic void igc_add_ring(struct igc_ring *ring,\n\t\t\t struct igc_ring_container *head)\n{\n\thead->ring = ring;\n\thead->count++;\n}\n\n \nstatic void igc_cache_ring_register(struct igc_adapter *adapter)\n{\n\tint i = 0, j = 0;\n\n\tswitch (adapter->hw.mac.type) {\n\tcase igc_i225:\n\tdefault:\n\t\tfor (; i < adapter->num_rx_queues; i++)\n\t\t\tadapter->rx_ring[i]->reg_idx = i;\n\t\tfor (; j < adapter->num_tx_queues; j++)\n\t\t\tadapter->tx_ring[j]->reg_idx = j;\n\t\tbreak;\n\t}\n}\n\n \nstatic int igc_poll(struct napi_struct *napi, int budget)\n{\n\tstruct igc_q_vector *q_vector = container_of(napi,\n\t\t\t\t\t\t     struct igc_q_vector,\n\t\t\t\t\t\t     napi);\n\tstruct igc_ring *rx_ring = q_vector->rx.ring;\n\tbool clean_complete = true;\n\tint work_done = 0;\n\n\tif (q_vector->tx.ring)\n\t\tclean_complete = igc_clean_tx_irq(q_vector, budget);\n\n\tif (rx_ring) {\n\t\tint cleaned = rx_ring->xsk_pool ?\n\t\t\t      igc_clean_rx_irq_zc(q_vector, budget) :\n\t\t\t      igc_clean_rx_irq(q_vector, budget);\n\n\t\twork_done += cleaned;\n\t\tif (cleaned >= budget)\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (!clean_complete)\n\t\treturn budget;\n\n\t \n\tif (likely(napi_complete_done(napi, work_done)))\n\t\tigc_ring_irq_enable(q_vector);\n\n\treturn min(work_done, budget - 1);\n}\n\n \nstatic int igc_alloc_q_vector(struct igc_adapter *adapter,\n\t\t\t      unsigned int v_count, unsigned int v_idx,\n\t\t\t      unsigned int txr_count, unsigned int txr_idx,\n\t\t\t      unsigned int rxr_count, unsigned int rxr_idx)\n{\n\tstruct igc_q_vector *q_vector;\n\tstruct igc_ring *ring;\n\tint ring_count;\n\n\t \n\tif (txr_count > 1 || rxr_count > 1)\n\t\treturn -ENOMEM;\n\n\tring_count = txr_count + rxr_count;\n\n\t \n\tq_vector = adapter->q_vector[v_idx];\n\tif (!q_vector)\n\t\tq_vector = kzalloc(struct_size(q_vector, ring, ring_count),\n\t\t\t\t   GFP_KERNEL);\n\telse\n\t\tmemset(q_vector, 0, struct_size(q_vector, ring, ring_count));\n\tif (!q_vector)\n\t\treturn -ENOMEM;\n\n\t \n\tnetif_napi_add(adapter->netdev, &q_vector->napi, igc_poll);\n\n\t \n\tadapter->q_vector[v_idx] = q_vector;\n\tq_vector->adapter = adapter;\n\n\t \n\tq_vector->tx.work_limit = adapter->tx_work_limit;\n\n\t \n\tq_vector->itr_register = adapter->io_addr + IGC_EITR(0);\n\tq_vector->itr_val = IGC_START_ITR;\n\n\t \n\tring = q_vector->ring;\n\n\t \n\tif (rxr_count) {\n\t\t \n\t\tif (!adapter->rx_itr_setting || adapter->rx_itr_setting > 3)\n\t\t\tq_vector->itr_val = adapter->rx_itr_setting;\n\t} else {\n\t\t \n\t\tif (!adapter->tx_itr_setting || adapter->tx_itr_setting > 3)\n\t\t\tq_vector->itr_val = adapter->tx_itr_setting;\n\t}\n\n\tif (txr_count) {\n\t\t \n\t\tring->dev = &adapter->pdev->dev;\n\t\tring->netdev = adapter->netdev;\n\n\t\t \n\t\tring->q_vector = q_vector;\n\n\t\t \n\t\tigc_add_ring(ring, &q_vector->tx);\n\n\t\t \n\t\tring->count = adapter->tx_ring_count;\n\t\tring->queue_index = txr_idx;\n\n\t\t \n\t\tadapter->tx_ring[txr_idx] = ring;\n\n\t\t \n\t\tring++;\n\t}\n\n\tif (rxr_count) {\n\t\t \n\t\tring->dev = &adapter->pdev->dev;\n\t\tring->netdev = adapter->netdev;\n\n\t\t \n\t\tring->q_vector = q_vector;\n\n\t\t \n\t\tigc_add_ring(ring, &q_vector->rx);\n\n\t\t \n\t\tring->count = adapter->rx_ring_count;\n\t\tring->queue_index = rxr_idx;\n\n\t\t \n\t\tadapter->rx_ring[rxr_idx] = ring;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int igc_alloc_q_vectors(struct igc_adapter *adapter)\n{\n\tint rxr_remaining = adapter->num_rx_queues;\n\tint txr_remaining = adapter->num_tx_queues;\n\tint rxr_idx = 0, txr_idx = 0, v_idx = 0;\n\tint q_vectors = adapter->num_q_vectors;\n\tint err;\n\n\tif (q_vectors >= (rxr_remaining + txr_remaining)) {\n\t\tfor (; rxr_remaining; v_idx++) {\n\t\t\terr = igc_alloc_q_vector(adapter, q_vectors, v_idx,\n\t\t\t\t\t\t 0, 0, 1, rxr_idx);\n\n\t\t\tif (err)\n\t\t\t\tgoto err_out;\n\n\t\t\t \n\t\t\trxr_remaining--;\n\t\t\trxr_idx++;\n\t\t}\n\t}\n\n\tfor (; v_idx < q_vectors; v_idx++) {\n\t\tint rqpv = DIV_ROUND_UP(rxr_remaining, q_vectors - v_idx);\n\t\tint tqpv = DIV_ROUND_UP(txr_remaining, q_vectors - v_idx);\n\n\t\terr = igc_alloc_q_vector(adapter, q_vectors, v_idx,\n\t\t\t\t\t tqpv, txr_idx, rqpv, rxr_idx);\n\n\t\tif (err)\n\t\t\tgoto err_out;\n\n\t\t \n\t\trxr_remaining -= rqpv;\n\t\ttxr_remaining -= tqpv;\n\t\trxr_idx++;\n\t\ttxr_idx++;\n\t}\n\n\treturn 0;\n\nerr_out:\n\tadapter->num_tx_queues = 0;\n\tadapter->num_rx_queues = 0;\n\tadapter->num_q_vectors = 0;\n\n\twhile (v_idx--)\n\t\tigc_free_q_vector(adapter, v_idx);\n\n\treturn -ENOMEM;\n}\n\n \nstatic int igc_init_interrupt_scheme(struct igc_adapter *adapter, bool msix)\n{\n\tstruct net_device *dev = adapter->netdev;\n\tint err = 0;\n\n\tigc_set_interrupt_capability(adapter, msix);\n\n\terr = igc_alloc_q_vectors(adapter);\n\tif (err) {\n\t\tnetdev_err(dev, \"Unable to allocate memory for vectors\\n\");\n\t\tgoto err_alloc_q_vectors;\n\t}\n\n\tigc_cache_ring_register(adapter);\n\n\treturn 0;\n\nerr_alloc_q_vectors:\n\tigc_reset_interrupt_capability(adapter);\n\treturn err;\n}\n\n \nstatic int igc_sw_init(struct igc_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct igc_hw *hw = &adapter->hw;\n\n\tpci_read_config_word(pdev, PCI_COMMAND, &hw->bus.pci_cmd_word);\n\n\t \n\tadapter->tx_ring_count = IGC_DEFAULT_TXD;\n\tadapter->rx_ring_count = IGC_DEFAULT_RXD;\n\n\t \n\tadapter->rx_itr_setting = IGC_DEFAULT_ITR;\n\tadapter->tx_itr_setting = IGC_DEFAULT_ITR;\n\n\t \n\tadapter->tx_work_limit = IGC_DEFAULT_TX_WORK;\n\n\t \n\tadapter->max_frame_size = netdev->mtu + ETH_HLEN + ETH_FCS_LEN +\n\t\t\t\tVLAN_HLEN;\n\tadapter->min_frame_size = ETH_ZLEN + ETH_FCS_LEN;\n\n\tmutex_init(&adapter->nfc_rule_lock);\n\tINIT_LIST_HEAD(&adapter->nfc_rule_list);\n\tadapter->nfc_rule_count = 0;\n\n\tspin_lock_init(&adapter->stats64_lock);\n\tspin_lock_init(&adapter->qbv_tx_lock);\n\t \n\tadapter->flags |= IGC_FLAG_HAS_MSIX;\n\n\tigc_init_queue_configuration(adapter);\n\n\t \n\tif (igc_init_interrupt_scheme(adapter, true)) {\n\t\tnetdev_err(netdev, \"Unable to allocate memory for queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tigc_irq_disable(adapter);\n\n\tset_bit(__IGC_DOWN, &adapter->state);\n\n\treturn 0;\n}\n\n \nvoid igc_up(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint i = 0;\n\n\t \n\tigc_configure(adapter);\n\n\tclear_bit(__IGC_DOWN, &adapter->state);\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\tnapi_enable(&adapter->q_vector[i]->napi);\n\n\tif (adapter->msix_entries)\n\t\tigc_configure_msix(adapter);\n\telse\n\t\tigc_assign_vector(adapter->q_vector[0], 0);\n\n\t \n\trd32(IGC_ICR);\n\tigc_irq_enable(adapter);\n\n\tnetif_tx_start_all_queues(adapter->netdev);\n\n\t \n\thw->mac.get_link_status = true;\n\tschedule_work(&adapter->watchdog_task);\n}\n\n \nvoid igc_update_stats(struct igc_adapter *adapter)\n{\n\tstruct rtnl_link_stats64 *net_stats = &adapter->stats64;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct igc_hw *hw = &adapter->hw;\n\tu64 _bytes, _packets;\n\tu64 bytes, packets;\n\tunsigned int start;\n\tu32 mpc;\n\tint i;\n\n\t \n\tif (adapter->link_speed == 0)\n\t\treturn;\n\tif (pci_channel_offline(pdev))\n\t\treturn;\n\n\tpackets = 0;\n\tbytes = 0;\n\n\trcu_read_lock();\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct igc_ring *ring = adapter->rx_ring[i];\n\t\tu32 rqdpc = rd32(IGC_RQDPC(i));\n\n\t\tif (hw->mac.type >= igc_i225)\n\t\t\twr32(IGC_RQDPC(i), 0);\n\n\t\tif (rqdpc) {\n\t\t\tring->rx_stats.drops += rqdpc;\n\t\t\tnet_stats->rx_fifo_errors += rqdpc;\n\t\t}\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&ring->rx_syncp);\n\t\t\t_bytes = ring->rx_stats.bytes;\n\t\t\t_packets = ring->rx_stats.packets;\n\t\t} while (u64_stats_fetch_retry(&ring->rx_syncp, start));\n\t\tbytes += _bytes;\n\t\tpackets += _packets;\n\t}\n\n\tnet_stats->rx_bytes = bytes;\n\tnet_stats->rx_packets = packets;\n\n\tpackets = 0;\n\tbytes = 0;\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igc_ring *ring = adapter->tx_ring[i];\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&ring->tx_syncp);\n\t\t\t_bytes = ring->tx_stats.bytes;\n\t\t\t_packets = ring->tx_stats.packets;\n\t\t} while (u64_stats_fetch_retry(&ring->tx_syncp, start));\n\t\tbytes += _bytes;\n\t\tpackets += _packets;\n\t}\n\tnet_stats->tx_bytes = bytes;\n\tnet_stats->tx_packets = packets;\n\trcu_read_unlock();\n\n\t \n\tadapter->stats.crcerrs += rd32(IGC_CRCERRS);\n\tadapter->stats.gprc += rd32(IGC_GPRC);\n\tadapter->stats.gorc += rd32(IGC_GORCL);\n\trd32(IGC_GORCH);  \n\tadapter->stats.bprc += rd32(IGC_BPRC);\n\tadapter->stats.mprc += rd32(IGC_MPRC);\n\tadapter->stats.roc += rd32(IGC_ROC);\n\n\tadapter->stats.prc64 += rd32(IGC_PRC64);\n\tadapter->stats.prc127 += rd32(IGC_PRC127);\n\tadapter->stats.prc255 += rd32(IGC_PRC255);\n\tadapter->stats.prc511 += rd32(IGC_PRC511);\n\tadapter->stats.prc1023 += rd32(IGC_PRC1023);\n\tadapter->stats.prc1522 += rd32(IGC_PRC1522);\n\tadapter->stats.tlpic += rd32(IGC_TLPIC);\n\tadapter->stats.rlpic += rd32(IGC_RLPIC);\n\tadapter->stats.hgptc += rd32(IGC_HGPTC);\n\n\tmpc = rd32(IGC_MPC);\n\tadapter->stats.mpc += mpc;\n\tnet_stats->rx_fifo_errors += mpc;\n\tadapter->stats.scc += rd32(IGC_SCC);\n\tadapter->stats.ecol += rd32(IGC_ECOL);\n\tadapter->stats.mcc += rd32(IGC_MCC);\n\tadapter->stats.latecol += rd32(IGC_LATECOL);\n\tadapter->stats.dc += rd32(IGC_DC);\n\tadapter->stats.rlec += rd32(IGC_RLEC);\n\tadapter->stats.xonrxc += rd32(IGC_XONRXC);\n\tadapter->stats.xontxc += rd32(IGC_XONTXC);\n\tadapter->stats.xoffrxc += rd32(IGC_XOFFRXC);\n\tadapter->stats.xofftxc += rd32(IGC_XOFFTXC);\n\tadapter->stats.fcruc += rd32(IGC_FCRUC);\n\tadapter->stats.gptc += rd32(IGC_GPTC);\n\tadapter->stats.gotc += rd32(IGC_GOTCL);\n\trd32(IGC_GOTCH);  \n\tadapter->stats.rnbc += rd32(IGC_RNBC);\n\tadapter->stats.ruc += rd32(IGC_RUC);\n\tadapter->stats.rfc += rd32(IGC_RFC);\n\tadapter->stats.rjc += rd32(IGC_RJC);\n\tadapter->stats.tor += rd32(IGC_TORH);\n\tadapter->stats.tot += rd32(IGC_TOTH);\n\tadapter->stats.tpr += rd32(IGC_TPR);\n\n\tadapter->stats.ptc64 += rd32(IGC_PTC64);\n\tadapter->stats.ptc127 += rd32(IGC_PTC127);\n\tadapter->stats.ptc255 += rd32(IGC_PTC255);\n\tadapter->stats.ptc511 += rd32(IGC_PTC511);\n\tadapter->stats.ptc1023 += rd32(IGC_PTC1023);\n\tadapter->stats.ptc1522 += rd32(IGC_PTC1522);\n\n\tadapter->stats.mptc += rd32(IGC_MPTC);\n\tadapter->stats.bptc += rd32(IGC_BPTC);\n\n\tadapter->stats.tpt += rd32(IGC_TPT);\n\tadapter->stats.colc += rd32(IGC_COLC);\n\tadapter->stats.colc += rd32(IGC_RERC);\n\n\tadapter->stats.algnerrc += rd32(IGC_ALGNERRC);\n\n\tadapter->stats.tsctc += rd32(IGC_TSCTC);\n\n\tadapter->stats.iac += rd32(IGC_IAC);\n\n\t \n\tnet_stats->multicast = adapter->stats.mprc;\n\tnet_stats->collisions = adapter->stats.colc;\n\n\t \n\n\t \n\tnet_stats->rx_errors = adapter->stats.rxerrc +\n\t\tadapter->stats.crcerrs + adapter->stats.algnerrc +\n\t\tadapter->stats.ruc + adapter->stats.roc +\n\t\tadapter->stats.cexterr;\n\tnet_stats->rx_length_errors = adapter->stats.ruc +\n\t\t\t\t      adapter->stats.roc;\n\tnet_stats->rx_crc_errors = adapter->stats.crcerrs;\n\tnet_stats->rx_frame_errors = adapter->stats.algnerrc;\n\tnet_stats->rx_missed_errors = adapter->stats.mpc;\n\n\t \n\tnet_stats->tx_errors = adapter->stats.ecol +\n\t\t\t       adapter->stats.latecol;\n\tnet_stats->tx_aborted_errors = adapter->stats.ecol;\n\tnet_stats->tx_window_errors = adapter->stats.latecol;\n\tnet_stats->tx_carrier_errors = adapter->stats.tncrs;\n\n\t \n\tnet_stats->tx_dropped = adapter->stats.txdrop;\n\n\t \n\tadapter->stats.mgptc += rd32(IGC_MGTPTC);\n\tadapter->stats.mgprc += rd32(IGC_MGTPRC);\n\tadapter->stats.mgpdc += rd32(IGC_MGTPDC);\n}\n\n \nvoid igc_down(struct igc_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 tctl, rctl;\n\tint i = 0;\n\n\tset_bit(__IGC_DOWN, &adapter->state);\n\n\tigc_ptp_suspend(adapter);\n\n\tif (pci_device_is_present(adapter->pdev)) {\n\t\t \n\t\trctl = rd32(IGC_RCTL);\n\t\twr32(IGC_RCTL, rctl & ~IGC_RCTL_EN);\n\t\t \n\t}\n\t \n\tnetif_trans_update(netdev);\n\n\tnetif_carrier_off(netdev);\n\tnetif_tx_stop_all_queues(netdev);\n\n\tif (pci_device_is_present(adapter->pdev)) {\n\t\t \n\t\ttctl = rd32(IGC_TCTL);\n\t\ttctl &= ~IGC_TCTL_EN;\n\t\twr32(IGC_TCTL, tctl);\n\t\t \n\t\twrfl();\n\t\tusleep_range(10000, 20000);\n\n\t\tigc_irq_disable(adapter);\n\t}\n\n\tadapter->flags &= ~IGC_FLAG_NEED_LINK_UPDATE;\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++) {\n\t\tif (adapter->q_vector[i]) {\n\t\t\tnapi_synchronize(&adapter->q_vector[i]->napi);\n\t\t\tnapi_disable(&adapter->q_vector[i]->napi);\n\t\t}\n\t}\n\n\tdel_timer_sync(&adapter->watchdog_timer);\n\tdel_timer_sync(&adapter->phy_info_timer);\n\n\t \n\tspin_lock(&adapter->stats64_lock);\n\tigc_update_stats(adapter);\n\tspin_unlock(&adapter->stats64_lock);\n\n\tadapter->link_speed = 0;\n\tadapter->link_duplex = 0;\n\n\tif (!pci_channel_offline(adapter->pdev))\n\t\tigc_reset(adapter);\n\n\t \n\tadapter->flags &= ~IGC_FLAG_VLAN_PROMISC;\n\n\tigc_disable_all_tx_rings_hw(adapter);\n\tigc_clean_all_tx_rings(adapter);\n\tigc_clean_all_rx_rings(adapter);\n}\n\nvoid igc_reinit_locked(struct igc_adapter *adapter)\n{\n\twhile (test_and_set_bit(__IGC_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\tigc_down(adapter);\n\tigc_up(adapter);\n\tclear_bit(__IGC_RESETTING, &adapter->state);\n}\n\nstatic void igc_reset_task(struct work_struct *work)\n{\n\tstruct igc_adapter *adapter;\n\n\tadapter = container_of(work, struct igc_adapter, reset_task);\n\n\trtnl_lock();\n\t \n\tif (test_bit(__IGC_DOWN, &adapter->state) ||\n\t    test_bit(__IGC_RESETTING, &adapter->state)) {\n\t\trtnl_unlock();\n\t\treturn;\n\t}\n\n\tigc_rings_dump(adapter);\n\tigc_regs_dump(adapter);\n\tnetdev_err(adapter->netdev, \"Reset adapter\\n\");\n\tigc_reinit_locked(adapter);\n\trtnl_unlock();\n}\n\n \nstatic int igc_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tint max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN;\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\tif (igc_xdp_is_enabled(adapter) && new_mtu > ETH_DATA_LEN) {\n\t\tnetdev_dbg(netdev, \"Jumbo frames not supported with XDP\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (max_frame < (ETH_FRAME_LEN + ETH_FCS_LEN))\n\t\tmax_frame = ETH_FRAME_LEN + ETH_FCS_LEN;\n\n\twhile (test_and_set_bit(__IGC_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\n\t \n\tadapter->max_frame_size = max_frame;\n\n\tif (netif_running(netdev))\n\t\tigc_down(adapter);\n\n\tnetdev_dbg(netdev, \"changing MTU from %d to %d\\n\", netdev->mtu, new_mtu);\n\tnetdev->mtu = new_mtu;\n\n\tif (netif_running(netdev))\n\t\tigc_up(adapter);\n\telse\n\t\tigc_reset(adapter);\n\n\tclear_bit(__IGC_RESETTING, &adapter->state);\n\n\treturn 0;\n}\n\n \nstatic void igc_tx_timeout(struct net_device *netdev,\n\t\t\t   unsigned int __always_unused txqueue)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct igc_hw *hw = &adapter->hw;\n\n\t \n\tadapter->tx_timeout_count++;\n\tschedule_work(&adapter->reset_task);\n\twr32(IGC_EICS,\n\t     (adapter->eims_enable_mask & ~adapter->eims_other));\n}\n\n \nstatic void igc_get_stats64(struct net_device *netdev,\n\t\t\t    struct rtnl_link_stats64 *stats)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\tspin_lock(&adapter->stats64_lock);\n\tif (!test_bit(__IGC_RESETTING, &adapter->state))\n\t\tigc_update_stats(adapter);\n\tmemcpy(stats, &adapter->stats64, sizeof(*stats));\n\tspin_unlock(&adapter->stats64_lock);\n}\n\nstatic netdev_features_t igc_fix_features(struct net_device *netdev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\t \n\tif (features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tfeatures |= NETIF_F_HW_VLAN_CTAG_TX;\n\telse\n\t\tfeatures &= ~NETIF_F_HW_VLAN_CTAG_TX;\n\n\treturn features;\n}\n\nstatic int igc_set_features(struct net_device *netdev,\n\t\t\t    netdev_features_t features)\n{\n\tnetdev_features_t changed = netdev->features ^ features;\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\tif (changed & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tigc_vlan_mode(netdev, features);\n\n\t \n\tif (!(changed & (NETIF_F_RXALL | NETIF_F_NTUPLE)))\n\t\treturn 0;\n\n\tif (!(features & NETIF_F_NTUPLE))\n\t\tigc_flush_nfc_rules(adapter);\n\n\tnetdev->features = features;\n\n\tif (netif_running(netdev))\n\t\tigc_reinit_locked(adapter);\n\telse\n\t\tigc_reset(adapter);\n\n\treturn 1;\n}\n\nstatic netdev_features_t\nigc_features_check(struct sk_buff *skb, struct net_device *dev,\n\t\t   netdev_features_t features)\n{\n\tunsigned int network_hdr_len, mac_hdr_len;\n\n\t \n\tmac_hdr_len = skb_network_header(skb) - skb->data;\n\tif (unlikely(mac_hdr_len > IGC_MAX_MAC_HDR_LEN))\n\t\treturn features & ~(NETIF_F_HW_CSUM |\n\t\t\t\t    NETIF_F_SCTP_CRC |\n\t\t\t\t    NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t    NETIF_F_TSO |\n\t\t\t\t    NETIF_F_TSO6);\n\n\tnetwork_hdr_len = skb_checksum_start(skb) - skb_network_header(skb);\n\tif (unlikely(network_hdr_len >  IGC_MAX_NETWORK_HDR_LEN))\n\t\treturn features & ~(NETIF_F_HW_CSUM |\n\t\t\t\t    NETIF_F_SCTP_CRC |\n\t\t\t\t    NETIF_F_TSO |\n\t\t\t\t    NETIF_F_TSO6);\n\n\t \n\tif (skb->encapsulation && !(features & NETIF_F_TSO_MANGLEID))\n\t\tfeatures &= ~NETIF_F_TSO;\n\n\treturn features;\n}\n\nstatic void igc_tsync_interrupt(struct igc_adapter *adapter)\n{\n\tu32 ack, tsauxc, sec, nsec, tsicr;\n\tstruct igc_hw *hw = &adapter->hw;\n\tstruct ptp_clock_event event;\n\tstruct timespec64 ts;\n\n\ttsicr = rd32(IGC_TSICR);\n\tack = 0;\n\n\tif (tsicr & IGC_TSICR_SYS_WRAP) {\n\t\tevent.type = PTP_CLOCK_PPS;\n\t\tif (adapter->ptp_caps.pps)\n\t\t\tptp_clock_event(adapter->ptp_clock, &event);\n\t\tack |= IGC_TSICR_SYS_WRAP;\n\t}\n\n\tif (tsicr & IGC_TSICR_TXTS) {\n\t\t \n\t\tigc_ptp_tx_tstamp_event(adapter);\n\t\tack |= IGC_TSICR_TXTS;\n\t}\n\n\tif (tsicr & IGC_TSICR_TT0) {\n\t\tspin_lock(&adapter->tmreg_lock);\n\t\tts = timespec64_add(adapter->perout[0].start,\n\t\t\t\t    adapter->perout[0].period);\n\t\twr32(IGC_TRGTTIML0, ts.tv_nsec | IGC_TT_IO_TIMER_SEL_SYSTIM0);\n\t\twr32(IGC_TRGTTIMH0, (u32)ts.tv_sec);\n\t\ttsauxc = rd32(IGC_TSAUXC);\n\t\ttsauxc |= IGC_TSAUXC_EN_TT0;\n\t\twr32(IGC_TSAUXC, tsauxc);\n\t\tadapter->perout[0].start = ts;\n\t\tspin_unlock(&adapter->tmreg_lock);\n\t\tack |= IGC_TSICR_TT0;\n\t}\n\n\tif (tsicr & IGC_TSICR_TT1) {\n\t\tspin_lock(&adapter->tmreg_lock);\n\t\tts = timespec64_add(adapter->perout[1].start,\n\t\t\t\t    adapter->perout[1].period);\n\t\twr32(IGC_TRGTTIML1, ts.tv_nsec | IGC_TT_IO_TIMER_SEL_SYSTIM0);\n\t\twr32(IGC_TRGTTIMH1, (u32)ts.tv_sec);\n\t\ttsauxc = rd32(IGC_TSAUXC);\n\t\ttsauxc |= IGC_TSAUXC_EN_TT1;\n\t\twr32(IGC_TSAUXC, tsauxc);\n\t\tadapter->perout[1].start = ts;\n\t\tspin_unlock(&adapter->tmreg_lock);\n\t\tack |= IGC_TSICR_TT1;\n\t}\n\n\tif (tsicr & IGC_TSICR_AUTT0) {\n\t\tnsec = rd32(IGC_AUXSTMPL0);\n\t\tsec  = rd32(IGC_AUXSTMPH0);\n\t\tevent.type = PTP_CLOCK_EXTTS;\n\t\tevent.index = 0;\n\t\tevent.timestamp = sec * NSEC_PER_SEC + nsec;\n\t\tptp_clock_event(adapter->ptp_clock, &event);\n\t\tack |= IGC_TSICR_AUTT0;\n\t}\n\n\tif (tsicr & IGC_TSICR_AUTT1) {\n\t\tnsec = rd32(IGC_AUXSTMPL1);\n\t\tsec  = rd32(IGC_AUXSTMPH1);\n\t\tevent.type = PTP_CLOCK_EXTTS;\n\t\tevent.index = 1;\n\t\tevent.timestamp = sec * NSEC_PER_SEC + nsec;\n\t\tptp_clock_event(adapter->ptp_clock, &event);\n\t\tack |= IGC_TSICR_AUTT1;\n\t}\n\n\t \n\twr32(IGC_TSICR, ack);\n}\n\n \nstatic irqreturn_t igc_msix_other(int irq, void *data)\n{\n\tstruct igc_adapter *adapter = data;\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 icr = rd32(IGC_ICR);\n\n\t \n\tif (icr & IGC_ICR_DRSTA)\n\t\tschedule_work(&adapter->reset_task);\n\n\tif (icr & IGC_ICR_DOUTSYNC) {\n\t\t \n\t\tadapter->stats.doosync++;\n\t}\n\n\tif (icr & IGC_ICR_LSC) {\n\t\thw->mac.get_link_status = true;\n\t\t \n\t\tif (!test_bit(__IGC_DOWN, &adapter->state))\n\t\t\tmod_timer(&adapter->watchdog_timer, jiffies + 1);\n\t}\n\n\tif (icr & IGC_ICR_TS)\n\t\tigc_tsync_interrupt(adapter);\n\n\twr32(IGC_EIMS, adapter->eims_other);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void igc_write_itr(struct igc_q_vector *q_vector)\n{\n\tu32 itr_val = q_vector->itr_val & IGC_QVECTOR_MASK;\n\n\tif (!q_vector->set_itr)\n\t\treturn;\n\n\tif (!itr_val)\n\t\titr_val = IGC_ITR_VAL_MASK;\n\n\titr_val |= IGC_EITR_CNT_IGNR;\n\n\twritel(itr_val, q_vector->itr_register);\n\tq_vector->set_itr = 0;\n}\n\nstatic irqreturn_t igc_msix_ring(int irq, void *data)\n{\n\tstruct igc_q_vector *q_vector = data;\n\n\t \n\tigc_write_itr(q_vector);\n\n\tnapi_schedule(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int igc_request_msix(struct igc_adapter *adapter)\n{\n\tunsigned int num_q_vectors = adapter->num_q_vectors;\n\tint i = 0, err = 0, vector = 0, free_vector = 0;\n\tstruct net_device *netdev = adapter->netdev;\n\n\terr = request_irq(adapter->msix_entries[vector].vector,\n\t\t\t  &igc_msix_other, 0, netdev->name, adapter);\n\tif (err)\n\t\tgoto err_out;\n\n\tif (num_q_vectors > MAX_Q_VECTORS) {\n\t\tnum_q_vectors = MAX_Q_VECTORS;\n\t\tdev_warn(&adapter->pdev->dev,\n\t\t\t \"The number of queue vectors (%d) is higher than max allowed (%d)\\n\",\n\t\t\t adapter->num_q_vectors, MAX_Q_VECTORS);\n\t}\n\tfor (i = 0; i < num_q_vectors; i++) {\n\t\tstruct igc_q_vector *q_vector = adapter->q_vector[i];\n\n\t\tvector++;\n\n\t\tq_vector->itr_register = adapter->io_addr + IGC_EITR(vector);\n\n\t\tif (q_vector->rx.ring && q_vector->tx.ring)\n\t\t\tsprintf(q_vector->name, \"%s-TxRx-%u\", netdev->name,\n\t\t\t\tq_vector->rx.ring->queue_index);\n\t\telse if (q_vector->tx.ring)\n\t\t\tsprintf(q_vector->name, \"%s-tx-%u\", netdev->name,\n\t\t\t\tq_vector->tx.ring->queue_index);\n\t\telse if (q_vector->rx.ring)\n\t\t\tsprintf(q_vector->name, \"%s-rx-%u\", netdev->name,\n\t\t\t\tq_vector->rx.ring->queue_index);\n\t\telse\n\t\t\tsprintf(q_vector->name, \"%s-unused\", netdev->name);\n\n\t\terr = request_irq(adapter->msix_entries[vector].vector,\n\t\t\t\t  igc_msix_ring, 0, q_vector->name,\n\t\t\t\t  q_vector);\n\t\tif (err)\n\t\t\tgoto err_free;\n\t}\n\n\tigc_configure_msix(adapter);\n\treturn 0;\n\nerr_free:\n\t \n\tfree_irq(adapter->msix_entries[free_vector++].vector, adapter);\n\n\tvector--;\n\tfor (i = 0; i < vector; i++) {\n\t\tfree_irq(adapter->msix_entries[free_vector++].vector,\n\t\t\t adapter->q_vector[i]);\n\t}\nerr_out:\n\treturn err;\n}\n\n \nstatic void igc_clear_interrupt_scheme(struct igc_adapter *adapter)\n{\n\tigc_free_q_vectors(adapter);\n\tigc_reset_interrupt_capability(adapter);\n}\n\n \nstatic void igc_update_phy_info(struct timer_list *t)\n{\n\tstruct igc_adapter *adapter = from_timer(adapter, t, phy_info_timer);\n\n\tigc_get_phy_info(&adapter->hw);\n}\n\n \nbool igc_has_link(struct igc_adapter *adapter)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tbool link_active = false;\n\n\t \n\tif (!hw->mac.get_link_status)\n\t\treturn true;\n\thw->mac.ops.check_for_link(hw);\n\tlink_active = !hw->mac.get_link_status;\n\n\tif (hw->mac.type == igc_i225) {\n\t\tif (!netif_carrier_ok(adapter->netdev)) {\n\t\t\tadapter->flags &= ~IGC_FLAG_NEED_LINK_UPDATE;\n\t\t} else if (!(adapter->flags & IGC_FLAG_NEED_LINK_UPDATE)) {\n\t\t\tadapter->flags |= IGC_FLAG_NEED_LINK_UPDATE;\n\t\t\tadapter->link_check_timeout = jiffies;\n\t\t}\n\t}\n\n\treturn link_active;\n}\n\n \nstatic void igc_watchdog(struct timer_list *t)\n{\n\tstruct igc_adapter *adapter = from_timer(adapter, t, watchdog_timer);\n\t \n\tschedule_work(&adapter->watchdog_task);\n}\n\nstatic void igc_watchdog_task(struct work_struct *work)\n{\n\tstruct igc_adapter *adapter = container_of(work,\n\t\t\t\t\t\t   struct igc_adapter,\n\t\t\t\t\t\t   watchdog_task);\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct igc_hw *hw = &adapter->hw;\n\tstruct igc_phy_info *phy = &hw->phy;\n\tu16 phy_data, retry_count = 20;\n\tu32 link;\n\tint i;\n\n\tlink = igc_has_link(adapter);\n\n\tif (adapter->flags & IGC_FLAG_NEED_LINK_UPDATE) {\n\t\tif (time_after(jiffies, (adapter->link_check_timeout + HZ)))\n\t\t\tadapter->flags &= ~IGC_FLAG_NEED_LINK_UPDATE;\n\t\telse\n\t\t\tlink = false;\n\t}\n\n\tif (link) {\n\t\t \n\t\tpm_runtime_resume(netdev->dev.parent);\n\n\t\tif (!netif_carrier_ok(netdev)) {\n\t\t\tu32 ctrl;\n\n\t\t\thw->mac.ops.get_speed_and_duplex(hw,\n\t\t\t\t\t\t\t &adapter->link_speed,\n\t\t\t\t\t\t\t &adapter->link_duplex);\n\n\t\t\tctrl = rd32(IGC_CTRL);\n\t\t\t \n\t\t\tnetdev_info(netdev,\n\t\t\t\t    \"NIC Link is Up %d Mbps %s Duplex, Flow Control: %s\\n\",\n\t\t\t\t    adapter->link_speed,\n\t\t\t\t    adapter->link_duplex == FULL_DUPLEX ?\n\t\t\t\t    \"Full\" : \"Half\",\n\t\t\t\t    (ctrl & IGC_CTRL_TFCE) &&\n\t\t\t\t    (ctrl & IGC_CTRL_RFCE) ? \"RX/TX\" :\n\t\t\t\t    (ctrl & IGC_CTRL_RFCE) ?  \"RX\" :\n\t\t\t\t    (ctrl & IGC_CTRL_TFCE) ?  \"TX\" : \"None\");\n\n\t\t\t \n\t\t\tif ((adapter->flags & IGC_FLAG_EEE) &&\n\t\t\t    adapter->link_duplex == HALF_DUPLEX) {\n\t\t\t\tnetdev_info(netdev,\n\t\t\t\t\t    \"EEE Disabled: unsupported at half duplex. Re-enable using ethtool when at full duplex\\n\");\n\t\t\t\tadapter->hw.dev_spec._base.eee_enable = false;\n\t\t\t\tadapter->flags &= ~IGC_FLAG_EEE;\n\t\t\t}\n\n\t\t\t \n\t\t\tigc_check_downshift(hw);\n\t\t\tif (phy->speed_downgraded)\n\t\t\t\tnetdev_warn(netdev, \"Link Speed was downgraded by SmartSpeed\\n\");\n\n\t\t\t \n\t\t\tadapter->tx_timeout_factor = 1;\n\t\t\tswitch (adapter->link_speed) {\n\t\t\tcase SPEED_10:\n\t\t\t\tadapter->tx_timeout_factor = 14;\n\t\t\t\tbreak;\n\t\t\tcase SPEED_100:\n\t\t\tcase SPEED_1000:\n\t\t\tcase SPEED_2500:\n\t\t\t\tadapter->tx_timeout_factor = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tigc_tsn_adjust_txtime_offset(adapter);\n\n\t\t\tif (adapter->link_speed != SPEED_1000)\n\t\t\t\tgoto no_wait;\n\n\t\t\t \nretry_read_status:\n\t\t\tif (!igc_read_phy_reg(hw, PHY_1000T_STATUS,\n\t\t\t\t\t      &phy_data)) {\n\t\t\t\tif (!(phy_data & SR_1000T_REMOTE_RX_STATUS) &&\n\t\t\t\t    retry_count) {\n\t\t\t\t\tmsleep(100);\n\t\t\t\t\tretry_count--;\n\t\t\t\t\tgoto retry_read_status;\n\t\t\t\t} else if (!retry_count) {\n\t\t\t\t\tnetdev_err(netdev, \"exceed max 2 second\\n\");\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tnetdev_err(netdev, \"read 1000Base-T Status Reg\\n\");\n\t\t\t}\nno_wait:\n\t\t\tnetif_carrier_on(netdev);\n\n\t\t\t \n\t\t\tif (!test_bit(__IGC_DOWN, &adapter->state))\n\t\t\t\tmod_timer(&adapter->phy_info_timer,\n\t\t\t\t\t  round_jiffies(jiffies + 2 * HZ));\n\t\t}\n\t} else {\n\t\tif (netif_carrier_ok(netdev)) {\n\t\t\tadapter->link_speed = 0;\n\t\t\tadapter->link_duplex = 0;\n\n\t\t\t \n\t\t\tnetdev_info(netdev, \"NIC Link is Down\\n\");\n\t\t\tnetif_carrier_off(netdev);\n\n\t\t\t \n\t\t\tif (!test_bit(__IGC_DOWN, &adapter->state))\n\t\t\t\tmod_timer(&adapter->phy_info_timer,\n\t\t\t\t\t  round_jiffies(jiffies + 2 * HZ));\n\n\t\t\tpm_schedule_suspend(netdev->dev.parent,\n\t\t\t\t\t    MSEC_PER_SEC * 5);\n\t\t}\n\t}\n\n\tspin_lock(&adapter->stats64_lock);\n\tigc_update_stats(adapter);\n\tspin_unlock(&adapter->stats64_lock);\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igc_ring *tx_ring = adapter->tx_ring[i];\n\n\t\tif (!netif_carrier_ok(netdev)) {\n\t\t\t \n\t\t\tif (igc_desc_unused(tx_ring) + 1 < tx_ring->count) {\n\t\t\t\tadapter->tx_timeout_count++;\n\t\t\t\tschedule_work(&adapter->reset_task);\n\t\t\t\t \n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tset_bit(IGC_RING_FLAG_TX_DETECT_HANG, &tx_ring->flags);\n\t}\n\n\t \n\tif (adapter->flags & IGC_FLAG_HAS_MSIX) {\n\t\tu32 eics = 0;\n\n\t\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\t\teics |= adapter->q_vector[i]->eims_value;\n\t\twr32(IGC_EICS, eics);\n\t} else {\n\t\twr32(IGC_ICS, IGC_ICS_RXDMT0);\n\t}\n\n\tigc_ptp_tx_hang(adapter);\n\n\t \n\tif (!test_bit(__IGC_DOWN, &adapter->state)) {\n\t\tif (adapter->flags & IGC_FLAG_NEED_LINK_UPDATE)\n\t\t\tmod_timer(&adapter->watchdog_timer,\n\t\t\t\t  round_jiffies(jiffies +  HZ));\n\t\telse\n\t\t\tmod_timer(&adapter->watchdog_timer,\n\t\t\t\t  round_jiffies(jiffies + 2 * HZ));\n\t}\n}\n\n \nstatic irqreturn_t igc_intr_msi(int irq, void *data)\n{\n\tstruct igc_adapter *adapter = data;\n\tstruct igc_q_vector *q_vector = adapter->q_vector[0];\n\tstruct igc_hw *hw = &adapter->hw;\n\t \n\tu32 icr = rd32(IGC_ICR);\n\n\tigc_write_itr(q_vector);\n\n\tif (icr & IGC_ICR_DRSTA)\n\t\tschedule_work(&adapter->reset_task);\n\n\tif (icr & IGC_ICR_DOUTSYNC) {\n\t\t \n\t\tadapter->stats.doosync++;\n\t}\n\n\tif (icr & (IGC_ICR_RXSEQ | IGC_ICR_LSC)) {\n\t\thw->mac.get_link_status = true;\n\t\tif (!test_bit(__IGC_DOWN, &adapter->state))\n\t\t\tmod_timer(&adapter->watchdog_timer, jiffies + 1);\n\t}\n\n\tif (icr & IGC_ICR_TS)\n\t\tigc_tsync_interrupt(adapter);\n\n\tnapi_schedule(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t igc_intr(int irq, void *data)\n{\n\tstruct igc_adapter *adapter = data;\n\tstruct igc_q_vector *q_vector = adapter->q_vector[0];\n\tstruct igc_hw *hw = &adapter->hw;\n\t \n\tu32 icr = rd32(IGC_ICR);\n\n\t \n\tif (!(icr & IGC_ICR_INT_ASSERTED))\n\t\treturn IRQ_NONE;\n\n\tigc_write_itr(q_vector);\n\n\tif (icr & IGC_ICR_DRSTA)\n\t\tschedule_work(&adapter->reset_task);\n\n\tif (icr & IGC_ICR_DOUTSYNC) {\n\t\t \n\t\tadapter->stats.doosync++;\n\t}\n\n\tif (icr & (IGC_ICR_RXSEQ | IGC_ICR_LSC)) {\n\t\thw->mac.get_link_status = true;\n\t\t \n\t\tif (!test_bit(__IGC_DOWN, &adapter->state))\n\t\t\tmod_timer(&adapter->watchdog_timer, jiffies + 1);\n\t}\n\n\tif (icr & IGC_ICR_TS)\n\t\tigc_tsync_interrupt(adapter);\n\n\tnapi_schedule(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void igc_free_irq(struct igc_adapter *adapter)\n{\n\tif (adapter->msix_entries) {\n\t\tint vector = 0, i;\n\n\t\tfree_irq(adapter->msix_entries[vector++].vector, adapter);\n\n\t\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\t\tfree_irq(adapter->msix_entries[vector++].vector,\n\t\t\t\t adapter->q_vector[i]);\n\t} else {\n\t\tfree_irq(adapter->pdev->irq, adapter);\n\t}\n}\n\n \nstatic int igc_request_irq(struct igc_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct pci_dev *pdev = adapter->pdev;\n\tint err = 0;\n\n\tif (adapter->flags & IGC_FLAG_HAS_MSIX) {\n\t\terr = igc_request_msix(adapter);\n\t\tif (!err)\n\t\t\tgoto request_done;\n\t\t \n\t\tigc_free_all_tx_resources(adapter);\n\t\tigc_free_all_rx_resources(adapter);\n\n\t\tigc_clear_interrupt_scheme(adapter);\n\t\terr = igc_init_interrupt_scheme(adapter, false);\n\t\tif (err)\n\t\t\tgoto request_done;\n\t\tigc_setup_all_tx_resources(adapter);\n\t\tigc_setup_all_rx_resources(adapter);\n\t\tigc_configure(adapter);\n\t}\n\n\tigc_assign_vector(adapter->q_vector[0], 0);\n\n\tif (adapter->flags & IGC_FLAG_HAS_MSI) {\n\t\terr = request_irq(pdev->irq, &igc_intr_msi, 0,\n\t\t\t\t  netdev->name, adapter);\n\t\tif (!err)\n\t\t\tgoto request_done;\n\n\t\t \n\t\tigc_reset_interrupt_capability(adapter);\n\t\tadapter->flags &= ~IGC_FLAG_HAS_MSI;\n\t}\n\n\terr = request_irq(pdev->irq, &igc_intr, IRQF_SHARED,\n\t\t\t  netdev->name, adapter);\n\n\tif (err)\n\t\tnetdev_err(netdev, \"Error %d getting interrupt\\n\", err);\n\nrequest_done:\n\treturn err;\n}\n\n \nstatic int __igc_open(struct net_device *netdev, bool resuming)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct pci_dev *pdev = adapter->pdev;\n\tstruct igc_hw *hw = &adapter->hw;\n\tint err = 0;\n\tint i = 0;\n\n\t \n\n\tif (test_bit(__IGC_TESTING, &adapter->state)) {\n\t\tWARN_ON(resuming);\n\t\treturn -EBUSY;\n\t}\n\n\tif (!resuming)\n\t\tpm_runtime_get_sync(&pdev->dev);\n\n\tnetif_carrier_off(netdev);\n\n\t \n\terr = igc_setup_all_tx_resources(adapter);\n\tif (err)\n\t\tgoto err_setup_tx;\n\n\t \n\terr = igc_setup_all_rx_resources(adapter);\n\tif (err)\n\t\tgoto err_setup_rx;\n\n\tigc_power_up_link(adapter);\n\n\tigc_configure(adapter);\n\n\terr = igc_request_irq(adapter);\n\tif (err)\n\t\tgoto err_req_irq;\n\n\t \n\terr = netif_set_real_num_tx_queues(netdev, adapter->num_tx_queues);\n\tif (err)\n\t\tgoto err_set_queues;\n\n\terr = netif_set_real_num_rx_queues(netdev, adapter->num_rx_queues);\n\tif (err)\n\t\tgoto err_set_queues;\n\n\tclear_bit(__IGC_DOWN, &adapter->state);\n\n\tfor (i = 0; i < adapter->num_q_vectors; i++)\n\t\tnapi_enable(&adapter->q_vector[i]->napi);\n\n\t \n\trd32(IGC_ICR);\n\tigc_irq_enable(adapter);\n\n\tif (!resuming)\n\t\tpm_runtime_put(&pdev->dev);\n\n\tnetif_tx_start_all_queues(netdev);\n\n\t \n\thw->mac.get_link_status = true;\n\tschedule_work(&adapter->watchdog_task);\n\n\treturn IGC_SUCCESS;\n\nerr_set_queues:\n\tigc_free_irq(adapter);\nerr_req_irq:\n\tigc_release_hw_control(adapter);\n\tigc_power_down_phy_copper_base(&adapter->hw);\n\tigc_free_all_rx_resources(adapter);\nerr_setup_rx:\n\tigc_free_all_tx_resources(adapter);\nerr_setup_tx:\n\tigc_reset(adapter);\n\tif (!resuming)\n\t\tpm_runtime_put(&pdev->dev);\n\n\treturn err;\n}\n\nint igc_open(struct net_device *netdev)\n{\n\treturn __igc_open(netdev, false);\n}\n\n \nstatic int __igc_close(struct net_device *netdev, bool suspending)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct pci_dev *pdev = adapter->pdev;\n\n\tWARN_ON(test_bit(__IGC_RESETTING, &adapter->state));\n\n\tif (!suspending)\n\t\tpm_runtime_get_sync(&pdev->dev);\n\n\tigc_down(adapter);\n\n\tigc_release_hw_control(adapter);\n\n\tigc_free_irq(adapter);\n\n\tigc_free_all_tx_resources(adapter);\n\tigc_free_all_rx_resources(adapter);\n\n\tif (!suspending)\n\t\tpm_runtime_put_sync(&pdev->dev);\n\n\treturn 0;\n}\n\nint igc_close(struct net_device *netdev)\n{\n\tif (netif_device_present(netdev) || netdev->dismantle)\n\t\treturn __igc_close(netdev, false);\n\treturn 0;\n}\n\n \nstatic int igc_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)\n{\n\tswitch (cmd) {\n\tcase SIOCGHWTSTAMP:\n\t\treturn igc_ptp_get_ts_config(netdev, ifr);\n\tcase SIOCSHWTSTAMP:\n\t\treturn igc_ptp_set_ts_config(netdev, ifr);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int igc_save_launchtime_params(struct igc_adapter *adapter, int queue,\n\t\t\t\t      bool enable)\n{\n\tstruct igc_ring *ring;\n\n\tif (queue < 0 || queue >= adapter->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tring = adapter->tx_ring[queue];\n\tring->launchtime_enable = enable;\n\n\treturn 0;\n}\n\nstatic bool is_base_time_past(ktime_t base_time, const struct timespec64 *now)\n{\n\tstruct timespec64 b;\n\n\tb = ktime_to_timespec64(base_time);\n\n\treturn timespec64_compare(now, &b) > 0;\n}\n\nstatic bool validate_schedule(struct igc_adapter *adapter,\n\t\t\t      const struct tc_taprio_qopt_offload *qopt)\n{\n\tint queue_uses[IGC_MAX_TX_QUEUES] = { };\n\tstruct igc_hw *hw = &adapter->hw;\n\tstruct timespec64 now;\n\tsize_t n;\n\n\tif (qopt->cycle_time_extension)\n\t\treturn false;\n\n\tigc_ptp_read(adapter, &now);\n\n\t \n\tif (!is_base_time_past(qopt->base_time, &now) &&\n\t    igc_is_device_id_i225(hw))\n\t\treturn false;\n\n\tfor (n = 0; n < qopt->num_entries; n++) {\n\t\tconst struct tc_taprio_sched_entry *e, *prev;\n\t\tint i;\n\n\t\tprev = n ? &qopt->entries[n - 1] : NULL;\n\t\te = &qopt->entries[n];\n\n\t\t \n\t\tif (e->command != TC_TAPRIO_CMD_SET_GATES)\n\t\t\treturn false;\n\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\tif (e->gate_mask & BIT(i)) {\n\t\t\t\tqueue_uses[i]++;\n\n\t\t\t\t \n\t\t\t\tif (queue_uses[i] > 1 &&\n\t\t\t\t    !(prev->gate_mask & BIT(i)))\n\t\t\t\t\treturn false;\n\t\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic int igc_tsn_enable_launchtime(struct igc_adapter *adapter,\n\t\t\t\t     struct tc_etf_qopt_offload *qopt)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint err;\n\n\tif (hw->mac.type != igc_i225)\n\t\treturn -EOPNOTSUPP;\n\n\terr = igc_save_launchtime_params(adapter, qopt->queue, qopt->enable);\n\tif (err)\n\t\treturn err;\n\n\treturn igc_tsn_offload_apply(adapter);\n}\n\nstatic int igc_qbv_clear_schedule(struct igc_adapter *adapter)\n{\n\tunsigned long flags;\n\tint i;\n\n\tadapter->base_time = 0;\n\tadapter->cycle_time = NSEC_PER_SEC;\n\tadapter->taprio_offload_enable = false;\n\tadapter->qbv_config_change_errors = 0;\n\tadapter->qbv_count = 0;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igc_ring *ring = adapter->tx_ring[i];\n\n\t\tring->start_time = 0;\n\t\tring->end_time = NSEC_PER_SEC;\n\t\tring->max_sdu = 0;\n\t}\n\n\tspin_lock_irqsave(&adapter->qbv_tx_lock, flags);\n\n\tadapter->qbv_transition = false;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igc_ring *ring = adapter->tx_ring[i];\n\n\t\tring->oper_gate_closed = false;\n\t\tring->admin_gate_closed = false;\n\t}\n\n\tspin_unlock_irqrestore(&adapter->qbv_tx_lock, flags);\n\n\treturn 0;\n}\n\nstatic int igc_tsn_clear_schedule(struct igc_adapter *adapter)\n{\n\tigc_qbv_clear_schedule(adapter);\n\n\treturn 0;\n}\n\nstatic void igc_taprio_stats(struct net_device *dev,\n\t\t\t     struct tc_taprio_qopt_stats *stats)\n{\n\t \n\tstats->tx_overruns = 0;\n}\n\nstatic void igc_taprio_queue_stats(struct net_device *dev,\n\t\t\t\t   struct tc_taprio_qopt_queue_stats *queue_stats)\n{\n\tstruct tc_taprio_qopt_stats *stats = &queue_stats->stats;\n\n\t \n\tstats->tx_overruns = 0;\n}\n\nstatic int igc_save_qbv_schedule(struct igc_adapter *adapter,\n\t\t\t\t struct tc_taprio_qopt_offload *qopt)\n{\n\tbool queue_configured[IGC_MAX_TX_QUEUES] = { };\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 start_time = 0, end_time = 0;\n\tstruct timespec64 now;\n\tunsigned long flags;\n\tsize_t n;\n\tint i;\n\n\tswitch (qopt->cmd) {\n\tcase TAPRIO_CMD_REPLACE:\n\t\tbreak;\n\tcase TAPRIO_CMD_DESTROY:\n\t\treturn igc_tsn_clear_schedule(adapter);\n\tcase TAPRIO_CMD_STATS:\n\t\tigc_taprio_stats(adapter->netdev, &qopt->stats);\n\t\treturn 0;\n\tcase TAPRIO_CMD_QUEUE_STATS:\n\t\tigc_taprio_queue_stats(adapter->netdev, &qopt->queue_stats);\n\t\treturn 0;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (qopt->base_time < 0)\n\t\treturn -ERANGE;\n\n\tif (igc_is_device_id_i225(hw) && adapter->taprio_offload_enable)\n\t\treturn -EALREADY;\n\n\tif (!validate_schedule(adapter, qopt))\n\t\treturn -EINVAL;\n\n\tadapter->cycle_time = qopt->cycle_time;\n\tadapter->base_time = qopt->base_time;\n\tadapter->taprio_offload_enable = true;\n\n\tigc_ptp_read(adapter, &now);\n\n\tfor (n = 0; n < qopt->num_entries; n++) {\n\t\tstruct tc_taprio_sched_entry *e = &qopt->entries[n];\n\n\t\tend_time += e->interval;\n\n\t\t \n\t\tif (end_time > adapter->cycle_time ||\n\t\t    n + 1 == qopt->num_entries)\n\t\t\tend_time = adapter->cycle_time;\n\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\t\tstruct igc_ring *ring = adapter->tx_ring[i];\n\n\t\t\tif (!(e->gate_mask & BIT(i)))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (!queue_configured[i])\n\t\t\t\tring->start_time = start_time;\n\t\t\tring->end_time = end_time;\n\n\t\t\tif (ring->start_time >= adapter->cycle_time)\n\t\t\t\tqueue_configured[i] = false;\n\t\t\telse\n\t\t\t\tqueue_configured[i] = true;\n\t\t}\n\n\t\tstart_time += e->interval;\n\t}\n\n\tspin_lock_irqsave(&adapter->qbv_tx_lock, flags);\n\n\t \n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igc_ring *ring = adapter->tx_ring[i];\n\n\t\tif (!is_base_time_past(qopt->base_time, &now)) {\n\t\t\tring->admin_gate_closed = false;\n\t\t} else {\n\t\t\tring->oper_gate_closed = false;\n\t\t\tring->admin_gate_closed = false;\n\t\t}\n\n\t\tif (!queue_configured[i]) {\n\t\t\tif (!is_base_time_past(qopt->base_time, &now))\n\t\t\t\tring->admin_gate_closed = true;\n\t\t\telse\n\t\t\t\tring->oper_gate_closed = true;\n\n\t\t\tring->start_time = end_time;\n\t\t\tring->end_time = end_time;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&adapter->qbv_tx_lock, flags);\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igc_ring *ring = adapter->tx_ring[i];\n\t\tstruct net_device *dev = adapter->netdev;\n\n\t\tif (qopt->max_sdu[i])\n\t\t\tring->max_sdu = qopt->max_sdu[i] + dev->hard_header_len - ETH_TLEN;\n\t\telse\n\t\t\tring->max_sdu = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic int igc_tsn_enable_qbv_scheduling(struct igc_adapter *adapter,\n\t\t\t\t\t struct tc_taprio_qopt_offload *qopt)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint err;\n\n\tif (hw->mac.type != igc_i225)\n\t\treturn -EOPNOTSUPP;\n\n\terr = igc_save_qbv_schedule(adapter, qopt);\n\tif (err)\n\t\treturn err;\n\n\treturn igc_tsn_offload_apply(adapter);\n}\n\nstatic int igc_save_cbs_params(struct igc_adapter *adapter, int queue,\n\t\t\t       bool enable, int idleslope, int sendslope,\n\t\t\t       int hicredit, int locredit)\n{\n\tbool cbs_status[IGC_MAX_SR_QUEUES] = { false };\n\tstruct net_device *netdev = adapter->netdev;\n\tstruct igc_ring *ring;\n\tint i;\n\n\t \n\tif (queue < 0 || queue > 1)\n\t\treturn -EINVAL;\n\n\tring = adapter->tx_ring[queue];\n\n\tfor (i = 0; i < IGC_MAX_SR_QUEUES; i++)\n\t\tif (adapter->tx_ring[i])\n\t\t\tcbs_status[i] = adapter->tx_ring[i]->cbs_enable;\n\n\t \n\tif (enable) {\n\t\tif (queue == 1 && !cbs_status[0]) {\n\t\t\tnetdev_err(netdev,\n\t\t\t\t   \"Enabling CBS on queue1 before queue0\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tif (queue == 0 && cbs_status[1]) {\n\t\t\tnetdev_err(netdev,\n\t\t\t\t   \"Disabling CBS on queue0 before queue1\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tring->cbs_enable = enable;\n\tring->idleslope = idleslope;\n\tring->sendslope = sendslope;\n\tring->hicredit = hicredit;\n\tring->locredit = locredit;\n\n\treturn 0;\n}\n\nstatic int igc_tsn_enable_cbs(struct igc_adapter *adapter,\n\t\t\t      struct tc_cbs_qopt_offload *qopt)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tint err;\n\n\tif (hw->mac.type != igc_i225)\n\t\treturn -EOPNOTSUPP;\n\n\tif (qopt->queue < 0 || qopt->queue > 1)\n\t\treturn -EINVAL;\n\n\terr = igc_save_cbs_params(adapter, qopt->queue, qopt->enable,\n\t\t\t\t  qopt->idleslope, qopt->sendslope,\n\t\t\t\t  qopt->hicredit, qopt->locredit);\n\tif (err)\n\t\treturn err;\n\n\treturn igc_tsn_offload_apply(adapter);\n}\n\nstatic int igc_tc_query_caps(struct igc_adapter *adapter,\n\t\t\t     struct tc_query_caps_base *base)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\n\tswitch (base->type) {\n\tcase TC_SETUP_QDISC_TAPRIO: {\n\t\tstruct tc_taprio_caps *caps = base->caps;\n\n\t\tcaps->broken_mqprio = true;\n\n\t\tif (hw->mac.type == igc_i225) {\n\t\t\tcaps->supports_queue_max_sdu = true;\n\t\t\tcaps->gate_mask_per_txq = true;\n\t\t}\n\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int igc_setup_tc(struct net_device *dev, enum tc_setup_type type,\n\t\t\tvoid *type_data)\n{\n\tstruct igc_adapter *adapter = netdev_priv(dev);\n\n\tadapter->tc_setup_type = type;\n\n\tswitch (type) {\n\tcase TC_QUERY_CAPS:\n\t\treturn igc_tc_query_caps(adapter, type_data);\n\tcase TC_SETUP_QDISC_TAPRIO:\n\t\treturn igc_tsn_enable_qbv_scheduling(adapter, type_data);\n\n\tcase TC_SETUP_QDISC_ETF:\n\t\treturn igc_tsn_enable_launchtime(adapter, type_data);\n\n\tcase TC_SETUP_QDISC_CBS:\n\t\treturn igc_tsn_enable_cbs(adapter, type_data);\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int igc_bpf(struct net_device *dev, struct netdev_bpf *bpf)\n{\n\tstruct igc_adapter *adapter = netdev_priv(dev);\n\n\tswitch (bpf->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn igc_xdp_set_prog(adapter, bpf->prog, bpf->extack);\n\tcase XDP_SETUP_XSK_POOL:\n\t\treturn igc_xdp_setup_pool(adapter, bpf->xsk.pool,\n\t\t\t\t\t  bpf->xsk.queue_id);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int igc_xdp_xmit(struct net_device *dev, int num_frames,\n\t\t\tstruct xdp_frame **frames, u32 flags)\n{\n\tstruct igc_adapter *adapter = netdev_priv(dev);\n\tint cpu = smp_processor_id();\n\tstruct netdev_queue *nq;\n\tstruct igc_ring *ring;\n\tint i, drops;\n\n\tif (unlikely(!netif_carrier_ok(dev)))\n\t\treturn -ENETDOWN;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\tring = igc_xdp_get_tx_ring(adapter, cpu);\n\tnq = txring_txq(ring);\n\n\t__netif_tx_lock(nq, cpu);\n\n\t \n\ttxq_trans_cond_update(nq);\n\n\tdrops = 0;\n\tfor (i = 0; i < num_frames; i++) {\n\t\tint err;\n\t\tstruct xdp_frame *xdpf = frames[i];\n\n\t\terr = igc_xdp_init_tx_descriptor(ring, xdpf);\n\t\tif (err) {\n\t\t\txdp_return_frame_rx_napi(xdpf);\n\t\t\tdrops++;\n\t\t}\n\t}\n\n\tif (flags & XDP_XMIT_FLUSH)\n\t\tigc_flush_tx_descriptors(ring);\n\n\t__netif_tx_unlock(nq);\n\n\treturn num_frames - drops;\n}\n\nstatic void igc_trigger_rxtxq_interrupt(struct igc_adapter *adapter,\n\t\t\t\t\tstruct igc_q_vector *q_vector)\n{\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 eics = 0;\n\n\teics |= q_vector->eims_value;\n\twr32(IGC_EICS, eics);\n}\n\nint igc_xsk_wakeup(struct net_device *dev, u32 queue_id, u32 flags)\n{\n\tstruct igc_adapter *adapter = netdev_priv(dev);\n\tstruct igc_q_vector *q_vector;\n\tstruct igc_ring *ring;\n\n\tif (test_bit(__IGC_DOWN, &adapter->state))\n\t\treturn -ENETDOWN;\n\n\tif (!igc_xdp_is_enabled(adapter))\n\t\treturn -ENXIO;\n\n\tif (queue_id >= adapter->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tring = adapter->rx_ring[queue_id];\n\n\tif (!ring->xsk_pool)\n\t\treturn -ENXIO;\n\n\tq_vector = adapter->q_vector[queue_id];\n\tif (!napi_if_scheduled_mark_missed(&q_vector->napi))\n\t\tigc_trigger_rxtxq_interrupt(adapter, q_vector);\n\n\treturn 0;\n}\n\nstatic const struct net_device_ops igc_netdev_ops = {\n\t.ndo_open\t\t= igc_open,\n\t.ndo_stop\t\t= igc_close,\n\t.ndo_start_xmit\t\t= igc_xmit_frame,\n\t.ndo_set_rx_mode\t= igc_set_rx_mode,\n\t.ndo_set_mac_address\t= igc_set_mac,\n\t.ndo_change_mtu\t\t= igc_change_mtu,\n\t.ndo_tx_timeout\t\t= igc_tx_timeout,\n\t.ndo_get_stats64\t= igc_get_stats64,\n\t.ndo_fix_features\t= igc_fix_features,\n\t.ndo_set_features\t= igc_set_features,\n\t.ndo_features_check\t= igc_features_check,\n\t.ndo_eth_ioctl\t\t= igc_ioctl,\n\t.ndo_setup_tc\t\t= igc_setup_tc,\n\t.ndo_bpf\t\t= igc_bpf,\n\t.ndo_xdp_xmit\t\t= igc_xdp_xmit,\n\t.ndo_xsk_wakeup\t\t= igc_xsk_wakeup,\n};\n\n \nvoid igc_read_pci_cfg(struct igc_hw *hw, u32 reg, u16 *value)\n{\n\tstruct igc_adapter *adapter = hw->back;\n\n\tpci_read_config_word(adapter->pdev, reg, value);\n}\n\nvoid igc_write_pci_cfg(struct igc_hw *hw, u32 reg, u16 *value)\n{\n\tstruct igc_adapter *adapter = hw->back;\n\n\tpci_write_config_word(adapter->pdev, reg, *value);\n}\n\ns32 igc_read_pcie_cap_reg(struct igc_hw *hw, u32 reg, u16 *value)\n{\n\tstruct igc_adapter *adapter = hw->back;\n\n\tif (!pci_is_pcie(adapter->pdev))\n\t\treturn -IGC_ERR_CONFIG;\n\n\tpcie_capability_read_word(adapter->pdev, reg, value);\n\n\treturn IGC_SUCCESS;\n}\n\ns32 igc_write_pcie_cap_reg(struct igc_hw *hw, u32 reg, u16 *value)\n{\n\tstruct igc_adapter *adapter = hw->back;\n\n\tif (!pci_is_pcie(adapter->pdev))\n\t\treturn -IGC_ERR_CONFIG;\n\n\tpcie_capability_write_word(adapter->pdev, reg, *value);\n\n\treturn IGC_SUCCESS;\n}\n\nu32 igc_rd32(struct igc_hw *hw, u32 reg)\n{\n\tstruct igc_adapter *igc = container_of(hw, struct igc_adapter, hw);\n\tu8 __iomem *hw_addr = READ_ONCE(hw->hw_addr);\n\tu32 value = 0;\n\n\tif (IGC_REMOVED(hw_addr))\n\t\treturn ~value;\n\n\tvalue = readl(&hw_addr[reg]);\n\n\t \n\tif (!(~value) && (!reg || !(~readl(hw_addr)))) {\n\t\tstruct net_device *netdev = igc->netdev;\n\n\t\thw->hw_addr = NULL;\n\t\tnetif_device_detach(netdev);\n\t\tnetdev_err(netdev, \"PCIe link lost, device now detached\\n\");\n\t\tWARN(pci_device_is_present(igc->pdev),\n\t\t     \"igc: Failed to read reg 0x%x!\\n\", reg);\n\t}\n\n\treturn value;\n}\n\n \nstatic enum xdp_rss_hash_type igc_xdp_rss_type[IGC_RSS_TYPE_MAX_TABLE] = {\n\t[IGC_RSS_TYPE_NO_HASH]\t\t= XDP_RSS_TYPE_L2,\n\t[IGC_RSS_TYPE_HASH_TCP_IPV4]\t= XDP_RSS_TYPE_L4_IPV4_TCP,\n\t[IGC_RSS_TYPE_HASH_IPV4]\t= XDP_RSS_TYPE_L3_IPV4,\n\t[IGC_RSS_TYPE_HASH_TCP_IPV6]\t= XDP_RSS_TYPE_L4_IPV6_TCP,\n\t[IGC_RSS_TYPE_HASH_IPV6_EX]\t= XDP_RSS_TYPE_L3_IPV6_EX,\n\t[IGC_RSS_TYPE_HASH_IPV6]\t= XDP_RSS_TYPE_L3_IPV6,\n\t[IGC_RSS_TYPE_HASH_TCP_IPV6_EX] = XDP_RSS_TYPE_L4_IPV6_TCP_EX,\n\t[IGC_RSS_TYPE_HASH_UDP_IPV4]\t= XDP_RSS_TYPE_L4_IPV4_UDP,\n\t[IGC_RSS_TYPE_HASH_UDP_IPV6]\t= XDP_RSS_TYPE_L4_IPV6_UDP,\n\t[IGC_RSS_TYPE_HASH_UDP_IPV6_EX] = XDP_RSS_TYPE_L4_IPV6_UDP_EX,\n\t[10] = XDP_RSS_TYPE_NONE,  \n\t[11] = XDP_RSS_TYPE_NONE,  \n\t[12] = XDP_RSS_TYPE_NONE,  \n\t[13] = XDP_RSS_TYPE_NONE,\n\t[14] = XDP_RSS_TYPE_NONE,\n\t[15] = XDP_RSS_TYPE_NONE,\n};\n\nstatic int igc_xdp_rx_hash(const struct xdp_md *_ctx, u32 *hash,\n\t\t\t   enum xdp_rss_hash_type *rss_type)\n{\n\tconst struct igc_xdp_buff *ctx = (void *)_ctx;\n\n\tif (!(ctx->xdp.rxq->dev->features & NETIF_F_RXHASH))\n\t\treturn -ENODATA;\n\n\t*hash = le32_to_cpu(ctx->rx_desc->wb.lower.hi_dword.rss);\n\t*rss_type = igc_xdp_rss_type[igc_rss_type(ctx->rx_desc)];\n\n\treturn 0;\n}\n\nstatic int igc_xdp_rx_timestamp(const struct xdp_md *_ctx, u64 *timestamp)\n{\n\tconst struct igc_xdp_buff *ctx = (void *)_ctx;\n\n\tif (igc_test_staterr(ctx->rx_desc, IGC_RXDADV_STAT_TSIP)) {\n\t\t*timestamp = ctx->rx_ts;\n\n\t\treturn 0;\n\t}\n\n\treturn -ENODATA;\n}\n\nstatic const struct xdp_metadata_ops igc_xdp_metadata_ops = {\n\t.xmo_rx_hash\t\t\t= igc_xdp_rx_hash,\n\t.xmo_rx_timestamp\t\t= igc_xdp_rx_timestamp,\n};\n\nstatic enum hrtimer_restart igc_qbv_scheduling_timer(struct hrtimer *timer)\n{\n\tstruct igc_adapter *adapter = container_of(timer, struct igc_adapter,\n\t\t\t\t\t\t   hrtimer);\n\tunsigned long flags;\n\tunsigned int i;\n\n\tspin_lock_irqsave(&adapter->qbv_tx_lock, flags);\n\n\tadapter->qbv_transition = true;\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct igc_ring *tx_ring = adapter->tx_ring[i];\n\n\t\tif (tx_ring->admin_gate_closed) {\n\t\t\ttx_ring->admin_gate_closed = false;\n\t\t\ttx_ring->oper_gate_closed = true;\n\t\t} else {\n\t\t\ttx_ring->oper_gate_closed = false;\n\t\t}\n\t}\n\tadapter->qbv_transition = false;\n\n\tspin_unlock_irqrestore(&adapter->qbv_tx_lock, flags);\n\n\treturn HRTIMER_NORESTART;\n}\n\n \nstatic int igc_probe(struct pci_dev *pdev,\n\t\t     const struct pci_device_id *ent)\n{\n\tstruct igc_adapter *adapter;\n\tstruct net_device *netdev;\n\tstruct igc_hw *hw;\n\tconst struct igc_info *ei = igc_info_tbl[ent->driver_data];\n\tint err;\n\n\terr = pci_enable_device_mem(pdev);\n\tif (err)\n\t\treturn err;\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"No usable DMA configuration, aborting\\n\");\n\t\tgoto err_dma;\n\t}\n\n\terr = pci_request_mem_regions(pdev, igc_driver_name);\n\tif (err)\n\t\tgoto err_pci_reg;\n\n\terr = pci_enable_ptm(pdev, NULL);\n\tif (err < 0)\n\t\tdev_info(&pdev->dev, \"PCIe PTM not supported by PCIe bus/controller\\n\");\n\n\tpci_set_master(pdev);\n\n\terr = -ENOMEM;\n\tnetdev = alloc_etherdev_mq(sizeof(struct igc_adapter),\n\t\t\t\t   IGC_MAX_TX_QUEUES);\n\n\tif (!netdev)\n\t\tgoto err_alloc_etherdev;\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\n\tpci_set_drvdata(pdev, netdev);\n\tadapter = netdev_priv(netdev);\n\tadapter->netdev = netdev;\n\tadapter->pdev = pdev;\n\thw = &adapter->hw;\n\thw->back = adapter;\n\tadapter->port_num = hw->bus.func;\n\tadapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\n\n\terr = pci_save_state(pdev);\n\tif (err)\n\t\tgoto err_ioremap;\n\n\terr = -EIO;\n\tadapter->io_addr = ioremap(pci_resource_start(pdev, 0),\n\t\t\t\t   pci_resource_len(pdev, 0));\n\tif (!adapter->io_addr)\n\t\tgoto err_ioremap;\n\n\t \n\thw->hw_addr = adapter->io_addr;\n\n\tnetdev->netdev_ops = &igc_netdev_ops;\n\tnetdev->xdp_metadata_ops = &igc_xdp_metadata_ops;\n\tigc_ethtool_set_ops(netdev);\n\tnetdev->watchdog_timeo = 5 * HZ;\n\n\tnetdev->mem_start = pci_resource_start(pdev, 0);\n\tnetdev->mem_end = pci_resource_end(pdev, 0);\n\n\t \n\thw->vendor_id = pdev->vendor;\n\thw->device_id = pdev->device;\n\thw->revision_id = pdev->revision;\n\thw->subsystem_vendor_id = pdev->subsystem_vendor;\n\thw->subsystem_device_id = pdev->subsystem_device;\n\n\t \n\tmemcpy(&hw->mac.ops, ei->mac_ops, sizeof(hw->mac.ops));\n\tmemcpy(&hw->phy.ops, ei->phy_ops, sizeof(hw->phy.ops));\n\n\t \n\terr = ei->get_invariants(hw);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\t \n\tnetdev->features |= NETIF_F_SG;\n\tnetdev->features |= NETIF_F_TSO;\n\tnetdev->features |= NETIF_F_TSO6;\n\tnetdev->features |= NETIF_F_TSO_ECN;\n\tnetdev->features |= NETIF_F_RXHASH;\n\tnetdev->features |= NETIF_F_RXCSUM;\n\tnetdev->features |= NETIF_F_HW_CSUM;\n\tnetdev->features |= NETIF_F_SCTP_CRC;\n\tnetdev->features |= NETIF_F_HW_TC;\n\n#define IGC_GSO_PARTIAL_FEATURES (NETIF_F_GSO_GRE | \\\n\t\t\t\t  NETIF_F_GSO_GRE_CSUM | \\\n\t\t\t\t  NETIF_F_GSO_IPXIP4 | \\\n\t\t\t\t  NETIF_F_GSO_IPXIP6 | \\\n\t\t\t\t  NETIF_F_GSO_UDP_TUNNEL | \\\n\t\t\t\t  NETIF_F_GSO_UDP_TUNNEL_CSUM)\n\n\tnetdev->gso_partial_features = IGC_GSO_PARTIAL_FEATURES;\n\tnetdev->features |= NETIF_F_GSO_PARTIAL | IGC_GSO_PARTIAL_FEATURES;\n\n\t \n\terr = igc_sw_init(adapter);\n\tif (err)\n\t\tgoto err_sw_init;\n\n\t \n\tnetdev->hw_features |= NETIF_F_NTUPLE;\n\tnetdev->hw_features |= NETIF_F_HW_VLAN_CTAG_TX;\n\tnetdev->hw_features |= NETIF_F_HW_VLAN_CTAG_RX;\n\tnetdev->hw_features |= netdev->features;\n\n\tnetdev->features |= NETIF_F_HIGHDMA;\n\n\tnetdev->vlan_features |= netdev->features | NETIF_F_TSO_MANGLEID;\n\tnetdev->mpls_features |= NETIF_F_HW_CSUM;\n\tnetdev->hw_enc_features |= netdev->vlan_features;\n\n\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |\n\t\t\t       NETDEV_XDP_ACT_XSK_ZEROCOPY;\n\n\t \n\tnetdev->min_mtu = ETH_MIN_MTU;\n\tnetdev->max_mtu = MAX_STD_JUMBO_FRAME_SIZE;\n\n\t \n\thw->mac.ops.reset_hw(hw);\n\n\tif (igc_get_flash_presence_i225(hw)) {\n\t\tif (hw->nvm.ops.validate(hw) < 0) {\n\t\t\tdev_err(&pdev->dev, \"The NVM Checksum Is Not Valid\\n\");\n\t\t\terr = -EIO;\n\t\t\tgoto err_eeprom;\n\t\t}\n\t}\n\n\tif (eth_platform_get_mac_address(&pdev->dev, hw->mac.addr)) {\n\t\t \n\t\tif (hw->mac.ops.read_mac_addr(hw))\n\t\t\tdev_err(&pdev->dev, \"NVM Read Error\\n\");\n\t}\n\n\teth_hw_addr_set(netdev, hw->mac.addr);\n\n\tif (!is_valid_ether_addr(netdev->dev_addr)) {\n\t\tdev_err(&pdev->dev, \"Invalid MAC Address\\n\");\n\t\terr = -EIO;\n\t\tgoto err_eeprom;\n\t}\n\n\t \n\twr32(IGC_RXPBS, I225_RXPBSIZE_DEFAULT);\n\twr32(IGC_TXPBS, I225_TXPBSIZE_DEFAULT);\n\n\ttimer_setup(&adapter->watchdog_timer, igc_watchdog, 0);\n\ttimer_setup(&adapter->phy_info_timer, igc_update_phy_info, 0);\n\n\tINIT_WORK(&adapter->reset_task, igc_reset_task);\n\tINIT_WORK(&adapter->watchdog_task, igc_watchdog_task);\n\n\thrtimer_init(&adapter->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tadapter->hrtimer.function = &igc_qbv_scheduling_timer;\n\n\t \n\tadapter->fc_autoneg = true;\n\thw->mac.autoneg = true;\n\thw->phy.autoneg_advertised = 0xaf;\n\n\thw->fc.requested_mode = igc_fc_default;\n\thw->fc.current_mode = igc_fc_default;\n\n\t \n\tadapter->flags |= IGC_FLAG_WOL_SUPPORTED;\n\n\t \n\tif (adapter->flags & IGC_FLAG_WOL_SUPPORTED)\n\t\tadapter->wol |= IGC_WUFC_MAG;\n\n\tdevice_set_wakeup_enable(&adapter->pdev->dev,\n\t\t\t\t adapter->flags & IGC_FLAG_WOL_SUPPORTED);\n\n\tigc_ptp_init(adapter);\n\n\tigc_tsn_clear_schedule(adapter);\n\n\t \n\tigc_reset(adapter);\n\n\t \n\tigc_get_hw_control(adapter);\n\n\tstrncpy(netdev->name, \"eth%d\", IFNAMSIZ);\n\terr = register_netdev(netdev);\n\tif (err)\n\t\tgoto err_register;\n\n\t  \n\tnetif_carrier_off(netdev);\n\n\t \n\tadapter->ei = *ei;\n\n\t \n\tpcie_print_link_status(pdev);\n\tnetdev_info(netdev, \"MAC: %pM\\n\", netdev->dev_addr);\n\n\tdev_pm_set_driver_flags(&pdev->dev, DPM_FLAG_NO_DIRECT_COMPLETE);\n\t \n\thw->dev_spec._base.eee_enable = false;\n\tadapter->flags &= ~IGC_FLAG_EEE;\n\tigc_set_eee_i225(hw, false, false, false);\n\n\tpm_runtime_put_noidle(&pdev->dev);\n\n\treturn 0;\n\nerr_register:\n\tigc_release_hw_control(adapter);\nerr_eeprom:\n\tif (!igc_check_reset_block(hw))\n\t\tigc_reset_phy(hw);\nerr_sw_init:\n\tigc_clear_interrupt_scheme(adapter);\n\tiounmap(adapter->io_addr);\nerr_ioremap:\n\tfree_netdev(netdev);\nerr_alloc_etherdev:\n\tpci_release_mem_regions(pdev);\nerr_pci_reg:\nerr_dma:\n\tpci_disable_device(pdev);\n\treturn err;\n}\n\n \nstatic void igc_remove(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\tpm_runtime_get_noresume(&pdev->dev);\n\n\tigc_flush_nfc_rules(adapter);\n\n\tigc_ptp_stop(adapter);\n\n\tpci_disable_ptm(pdev);\n\tpci_clear_master(pdev);\n\n\tset_bit(__IGC_DOWN, &adapter->state);\n\n\tdel_timer_sync(&adapter->watchdog_timer);\n\tdel_timer_sync(&adapter->phy_info_timer);\n\n\tcancel_work_sync(&adapter->reset_task);\n\tcancel_work_sync(&adapter->watchdog_task);\n\thrtimer_cancel(&adapter->hrtimer);\n\n\t \n\tigc_release_hw_control(adapter);\n\tunregister_netdev(netdev);\n\n\tigc_clear_interrupt_scheme(adapter);\n\tpci_iounmap(pdev, adapter->io_addr);\n\tpci_release_mem_regions(pdev);\n\n\tfree_netdev(netdev);\n\n\tpci_disable_device(pdev);\n}\n\nstatic int __igc_shutdown(struct pci_dev *pdev, bool *enable_wake,\n\t\t\t  bool runtime)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tu32 wufc = runtime ? IGC_WUFC_LNKC : adapter->wol;\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 ctrl, rctl, status;\n\tbool wake;\n\n\trtnl_lock();\n\tnetif_device_detach(netdev);\n\n\tif (netif_running(netdev))\n\t\t__igc_close(netdev, true);\n\n\tigc_ptp_suspend(adapter);\n\n\tigc_clear_interrupt_scheme(adapter);\n\trtnl_unlock();\n\n\tstatus = rd32(IGC_STATUS);\n\tif (status & IGC_STATUS_LU)\n\t\twufc &= ~IGC_WUFC_LNKC;\n\n\tif (wufc) {\n\t\tigc_setup_rctl(adapter);\n\t\tigc_set_rx_mode(netdev);\n\n\t\t \n\t\tif (wufc & IGC_WUFC_MC) {\n\t\t\trctl = rd32(IGC_RCTL);\n\t\t\trctl |= IGC_RCTL_MPE;\n\t\t\twr32(IGC_RCTL, rctl);\n\t\t}\n\n\t\tctrl = rd32(IGC_CTRL);\n\t\tctrl |= IGC_CTRL_ADVD3WUC;\n\t\twr32(IGC_CTRL, ctrl);\n\n\t\t \n\t\tigc_disable_pcie_master(hw);\n\n\t\twr32(IGC_WUC, IGC_WUC_PME_EN);\n\t\twr32(IGC_WUFC, wufc);\n\t} else {\n\t\twr32(IGC_WUC, 0);\n\t\twr32(IGC_WUFC, 0);\n\t}\n\n\twake = wufc || adapter->en_mng_pt;\n\tif (!wake)\n\t\tigc_power_down_phy_copper_base(&adapter->hw);\n\telse\n\t\tigc_power_up_link(adapter);\n\n\tif (enable_wake)\n\t\t*enable_wake = wake;\n\n\t \n\tigc_release_hw_control(adapter);\n\n\tpci_disable_device(pdev);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_PM\nstatic int __maybe_unused igc_runtime_suspend(struct device *dev)\n{\n\treturn __igc_shutdown(to_pci_dev(dev), NULL, 1);\n}\n\nstatic void igc_deliver_wake_packet(struct net_device *netdev)\n{\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct igc_hw *hw = &adapter->hw;\n\tstruct sk_buff *skb;\n\tu32 wupl;\n\n\twupl = rd32(IGC_WUPL) & IGC_WUPL_MASK;\n\n\t \n\tif (wupl == 0 || wupl > IGC_WUPM_BYTES)\n\t\treturn;\n\n\tskb = netdev_alloc_skb_ip_align(netdev, IGC_WUPM_BYTES);\n\tif (!skb)\n\t\treturn;\n\n\tskb_put(skb, wupl);\n\n\t \n\twupl = roundup(wupl, 4);\n\n\tmemcpy_fromio(skb->data, hw->hw_addr + IGC_WUPM_REG(0), wupl);\n\n\tskb->protocol = eth_type_trans(skb, netdev);\n\tnetif_rx(skb);\n}\n\nstatic int __maybe_unused igc_resume(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct igc_hw *hw = &adapter->hw;\n\tu32 err, val;\n\n\tpci_set_power_state(pdev, PCI_D0);\n\tpci_restore_state(pdev);\n\tpci_save_state(pdev);\n\n\tif (!pci_device_is_present(pdev))\n\t\treturn -ENODEV;\n\terr = pci_enable_device_mem(pdev);\n\tif (err) {\n\t\tnetdev_err(netdev, \"Cannot enable PCI device from suspend\\n\");\n\t\treturn err;\n\t}\n\tpci_set_master(pdev);\n\n\tpci_enable_wake(pdev, PCI_D3hot, 0);\n\tpci_enable_wake(pdev, PCI_D3cold, 0);\n\n\tif (igc_init_interrupt_scheme(adapter, true)) {\n\t\tnetdev_err(netdev, \"Unable to allocate memory for queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tigc_reset(adapter);\n\n\t \n\tigc_get_hw_control(adapter);\n\n\tval = rd32(IGC_WUS);\n\tif (val & WAKE_PKT_WUS)\n\t\tigc_deliver_wake_packet(netdev);\n\n\twr32(IGC_WUS, ~0);\n\n\trtnl_lock();\n\tif (!err && netif_running(netdev))\n\t\terr = __igc_open(netdev, true);\n\n\tif (!err)\n\t\tnetif_device_attach(netdev);\n\trtnl_unlock();\n\n\treturn err;\n}\n\nstatic int __maybe_unused igc_runtime_resume(struct device *dev)\n{\n\treturn igc_resume(dev);\n}\n\nstatic int __maybe_unused igc_suspend(struct device *dev)\n{\n\treturn __igc_shutdown(to_pci_dev(dev), NULL, 0);\n}\n\nstatic int __maybe_unused igc_runtime_idle(struct device *dev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev);\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\tif (!igc_has_link(adapter))\n\t\tpm_schedule_suspend(dev, MSEC_PER_SEC * 5);\n\n\treturn -EBUSY;\n}\n#endif  \n\nstatic void igc_shutdown(struct pci_dev *pdev)\n{\n\tbool wake;\n\n\t__igc_shutdown(pdev, &wake, 0);\n\n\tif (system_state == SYSTEM_POWER_OFF) {\n\t\tpci_wake_from_d3(pdev, wake);\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n\t}\n}\n\n \nstatic pci_ers_result_t igc_io_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t      pci_channel_state_t state)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\tnetif_device_detach(netdev);\n\n\tif (state == pci_channel_io_perm_failure)\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\tif (netif_running(netdev))\n\t\tigc_down(adapter);\n\tpci_disable_device(pdev);\n\n\t \n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\n \nstatic pci_ers_result_t igc_io_slot_reset(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\tstruct igc_hw *hw = &adapter->hw;\n\tpci_ers_result_t result;\n\n\tif (pci_enable_device_mem(pdev)) {\n\t\tnetdev_err(netdev, \"Could not re-enable PCI device after reset\\n\");\n\t\tresult = PCI_ERS_RESULT_DISCONNECT;\n\t} else {\n\t\tpci_set_master(pdev);\n\t\tpci_restore_state(pdev);\n\t\tpci_save_state(pdev);\n\n\t\tpci_enable_wake(pdev, PCI_D3hot, 0);\n\t\tpci_enable_wake(pdev, PCI_D3cold, 0);\n\n\t\t \n\t\thw->hw_addr = adapter->io_addr;\n\n\t\tigc_reset(adapter);\n\t\twr32(IGC_WUS, ~0);\n\t\tresult = PCI_ERS_RESULT_RECOVERED;\n\t}\n\n\treturn result;\n}\n\n \nstatic void igc_io_resume(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct igc_adapter *adapter = netdev_priv(netdev);\n\n\trtnl_lock();\n\tif (netif_running(netdev)) {\n\t\tif (igc_open(netdev)) {\n\t\t\tnetdev_err(netdev, \"igc_open failed after reset\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\tnetif_device_attach(netdev);\n\n\t \n\tigc_get_hw_control(adapter);\n\trtnl_unlock();\n}\n\nstatic const struct pci_error_handlers igc_err_handler = {\n\t.error_detected = igc_io_error_detected,\n\t.slot_reset = igc_io_slot_reset,\n\t.resume = igc_io_resume,\n};\n\n#ifdef CONFIG_PM\nstatic const struct dev_pm_ops igc_pm_ops = {\n\tSET_SYSTEM_SLEEP_PM_OPS(igc_suspend, igc_resume)\n\tSET_RUNTIME_PM_OPS(igc_runtime_suspend, igc_runtime_resume,\n\t\t\t   igc_runtime_idle)\n};\n#endif\n\nstatic struct pci_driver igc_driver = {\n\t.name     = igc_driver_name,\n\t.id_table = igc_pci_tbl,\n\t.probe    = igc_probe,\n\t.remove   = igc_remove,\n#ifdef CONFIG_PM\n\t.driver.pm = &igc_pm_ops,\n#endif\n\t.shutdown = igc_shutdown,\n\t.err_handler = &igc_err_handler,\n};\n\n \nint igc_reinit_queues(struct igc_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\tint err = 0;\n\n\tif (netif_running(netdev))\n\t\tigc_close(netdev);\n\n\tigc_reset_interrupt_capability(adapter);\n\n\tif (igc_init_interrupt_scheme(adapter, true)) {\n\t\tnetdev_err(netdev, \"Unable to allocate memory for queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (netif_running(netdev))\n\t\terr = igc_open(netdev);\n\n\treturn err;\n}\n\n \nstruct net_device *igc_get_hw_dev(struct igc_hw *hw)\n{\n\tstruct igc_adapter *adapter = hw->back;\n\n\treturn adapter->netdev;\n}\n\nstatic void igc_disable_rx_ring_hw(struct igc_ring *ring)\n{\n\tstruct igc_hw *hw = &ring->q_vector->adapter->hw;\n\tu8 idx = ring->reg_idx;\n\tu32 rxdctl;\n\n\trxdctl = rd32(IGC_RXDCTL(idx));\n\trxdctl &= ~IGC_RXDCTL_QUEUE_ENABLE;\n\trxdctl |= IGC_RXDCTL_SWFLUSH;\n\twr32(IGC_RXDCTL(idx), rxdctl);\n}\n\nvoid igc_disable_rx_ring(struct igc_ring *ring)\n{\n\tigc_disable_rx_ring_hw(ring);\n\tigc_clean_rx_ring(ring);\n}\n\nvoid igc_enable_rx_ring(struct igc_ring *ring)\n{\n\tstruct igc_adapter *adapter = ring->q_vector->adapter;\n\n\tigc_configure_rx_ring(adapter, ring);\n\n\tif (ring->xsk_pool)\n\t\tigc_alloc_rx_buffers_zc(ring, igc_desc_unused(ring));\n\telse\n\t\tigc_alloc_rx_buffers(ring, igc_desc_unused(ring));\n}\n\nvoid igc_disable_tx_ring(struct igc_ring *ring)\n{\n\tigc_disable_tx_ring_hw(ring);\n\tigc_clean_tx_ring(ring);\n}\n\nvoid igc_enable_tx_ring(struct igc_ring *ring)\n{\n\tstruct igc_adapter *adapter = ring->q_vector->adapter;\n\n\tigc_configure_tx_ring(adapter, ring);\n}\n\n \nstatic int __init igc_init_module(void)\n{\n\tint ret;\n\n\tpr_info(\"%s\\n\", igc_driver_string);\n\tpr_info(\"%s\\n\", igc_copyright);\n\n\tret = pci_register_driver(&igc_driver);\n\treturn ret;\n}\n\nmodule_init(igc_init_module);\n\n \nstatic void __exit igc_exit_module(void)\n{\n\tpci_unregister_driver(&igc_driver);\n}\n\nmodule_exit(igc_exit_module);\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}