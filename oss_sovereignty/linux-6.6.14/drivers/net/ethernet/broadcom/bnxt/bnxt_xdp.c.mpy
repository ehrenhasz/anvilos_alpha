{
  "module_name": "bnxt_xdp.c",
  "hash_id": "ef3ac3483031e5861b4bd2f2d1dcd907eaf4cec2e434ae1393aa1bb9e17c5e00",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/broadcom/bnxt/bnxt_xdp.c",
  "human_readable_source": " \n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <linux/filter.h>\n#include <net/page_pool/helpers.h>\n#include \"bnxt_hsi.h\"\n#include \"bnxt.h\"\n#include \"bnxt_xdp.h\"\n\nDEFINE_STATIC_KEY_FALSE(bnxt_xdp_locking_key);\n\nstruct bnxt_sw_tx_bd *bnxt_xmit_bd(struct bnxt *bp,\n\t\t\t\t   struct bnxt_tx_ring_info *txr,\n\t\t\t\t   dma_addr_t mapping, u32 len,\n\t\t\t\t   struct xdp_buff *xdp)\n{\n\tstruct skb_shared_info *sinfo;\n\tstruct bnxt_sw_tx_bd *tx_buf;\n\tstruct tx_bd *txbd;\n\tint num_frags = 0;\n\tu32 flags;\n\tu16 prod;\n\tint i;\n\n\tif (xdp && xdp_buff_has_frags(xdp)) {\n\t\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tnum_frags = sinfo->nr_frags;\n\t}\n\n\t \n\tprod = txr->tx_prod;\n\ttx_buf = &txr->tx_buf_ring[prod];\n\ttx_buf->nr_frags = num_frags;\n\tif (xdp)\n\t\ttx_buf->page = virt_to_head_page(xdp->data);\n\n\ttxbd = &txr->tx_desc_ring[TX_RING(prod)][TX_IDX(prod)];\n\tflags = (len << TX_BD_LEN_SHIFT) |\n\t\t((num_frags + 1) << TX_BD_FLAGS_BD_CNT_SHIFT) |\n\t\tbnxt_lhint_arr[len >> 9];\n\ttxbd->tx_bd_len_flags_type = cpu_to_le32(flags);\n\ttxbd->tx_bd_opaque = prod;\n\ttxbd->tx_bd_haddr = cpu_to_le64(mapping);\n\n\t \n\tfor (i = 0; i < num_frags ; i++) {\n\t\tskb_frag_t *frag = &sinfo->frags[i];\n\t\tstruct bnxt_sw_tx_bd *frag_tx_buf;\n\t\tdma_addr_t frag_mapping;\n\t\tint frag_len;\n\n\t\tprod = NEXT_TX(prod);\n\t\tWRITE_ONCE(txr->tx_prod, prod);\n\n\t\t \n\t\tfrag_tx_buf = &txr->tx_buf_ring[prod];\n\t\tfrag_tx_buf->page = skb_frag_page(frag);\n\n\t\ttxbd = &txr->tx_desc_ring[TX_RING(prod)][TX_IDX(prod)];\n\n\t\tfrag_len = skb_frag_size(frag);\n\t\tflags = frag_len << TX_BD_LEN_SHIFT;\n\t\ttxbd->tx_bd_len_flags_type = cpu_to_le32(flags);\n\t\tfrag_mapping = page_pool_get_dma_addr(skb_frag_page(frag)) +\n\t\t\t       skb_frag_off(frag);\n\t\ttxbd->tx_bd_haddr = cpu_to_le64(frag_mapping);\n\n\t\tlen = frag_len;\n\t}\n\n\tflags &= ~TX_BD_LEN;\n\ttxbd->tx_bd_len_flags_type = cpu_to_le32(((len) << TX_BD_LEN_SHIFT) | flags |\n\t\t\tTX_BD_FLAGS_PACKET_END);\n\t \n\twmb();\n\tprod = NEXT_TX(prod);\n\tWRITE_ONCE(txr->tx_prod, prod);\n\n\treturn tx_buf;\n}\n\nstatic void __bnxt_xmit_xdp(struct bnxt *bp, struct bnxt_tx_ring_info *txr,\n\t\t\t    dma_addr_t mapping, u32 len, u16 rx_prod,\n\t\t\t    struct xdp_buff *xdp)\n{\n\tstruct bnxt_sw_tx_bd *tx_buf;\n\n\ttx_buf = bnxt_xmit_bd(bp, txr, mapping, len, xdp);\n\ttx_buf->rx_prod = rx_prod;\n\ttx_buf->action = XDP_TX;\n\n}\n\nstatic void __bnxt_xmit_xdp_redirect(struct bnxt *bp,\n\t\t\t\t     struct bnxt_tx_ring_info *txr,\n\t\t\t\t     dma_addr_t mapping, u32 len,\n\t\t\t\t     struct xdp_frame *xdpf)\n{\n\tstruct bnxt_sw_tx_bd *tx_buf;\n\n\ttx_buf = bnxt_xmit_bd(bp, txr, mapping, len, NULL);\n\ttx_buf->action = XDP_REDIRECT;\n\ttx_buf->xdpf = xdpf;\n\tdma_unmap_addr_set(tx_buf, mapping, mapping);\n\tdma_unmap_len_set(tx_buf, len, 0);\n}\n\nvoid bnxt_tx_int_xdp(struct bnxt *bp, struct bnxt_napi *bnapi, int budget)\n{\n\tstruct bnxt_tx_ring_info *txr = bnapi->tx_ring;\n\tstruct bnxt_rx_ring_info *rxr = bnapi->rx_ring;\n\tbool rx_doorbell_needed = false;\n\tint nr_pkts = bnapi->tx_pkts;\n\tstruct bnxt_sw_tx_bd *tx_buf;\n\tu16 tx_cons = txr->tx_cons;\n\tu16 last_tx_cons = tx_cons;\n\tint i, j, frags;\n\n\tif (!budget)\n\t\treturn;\n\n\tfor (i = 0; i < nr_pkts; i++) {\n\t\ttx_buf = &txr->tx_buf_ring[tx_cons];\n\n\t\tif (tx_buf->action == XDP_REDIRECT) {\n\t\t\tstruct pci_dev *pdev = bp->pdev;\n\n\t\t\tdma_unmap_single(&pdev->dev,\n\t\t\t\t\t dma_unmap_addr(tx_buf, mapping),\n\t\t\t\t\t dma_unmap_len(tx_buf, len),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\txdp_return_frame(tx_buf->xdpf);\n\t\t\ttx_buf->action = 0;\n\t\t\ttx_buf->xdpf = NULL;\n\t\t} else if (tx_buf->action == XDP_TX) {\n\t\t\ttx_buf->action = 0;\n\t\t\trx_doorbell_needed = true;\n\t\t\tlast_tx_cons = tx_cons;\n\n\t\t\tfrags = tx_buf->nr_frags;\n\t\t\tfor (j = 0; j < frags; j++) {\n\t\t\t\ttx_cons = NEXT_TX(tx_cons);\n\t\t\t\ttx_buf = &txr->tx_buf_ring[tx_cons];\n\t\t\t\tpage_pool_recycle_direct(rxr->page_pool, tx_buf->page);\n\t\t\t}\n\t\t} else {\n\t\t\tbnxt_sched_reset_txr(bp, txr, i);\n\t\t\treturn;\n\t\t}\n\t\ttx_cons = NEXT_TX(tx_cons);\n\t}\n\n\tbnapi->tx_pkts = 0;\n\tWRITE_ONCE(txr->tx_cons, tx_cons);\n\tif (rx_doorbell_needed) {\n\t\ttx_buf = &txr->tx_buf_ring[last_tx_cons];\n\t\tbnxt_db_write(bp, &rxr->rx_db, tx_buf->rx_prod);\n\n\t}\n}\n\nbool bnxt_xdp_attached(struct bnxt *bp, struct bnxt_rx_ring_info *rxr)\n{\n\tstruct bpf_prog *xdp_prog = READ_ONCE(rxr->xdp_prog);\n\n\treturn !!xdp_prog;\n}\n\nvoid bnxt_xdp_buff_init(struct bnxt *bp, struct bnxt_rx_ring_info *rxr,\n\t\t\tu16 cons, u8 *data_ptr, unsigned int len,\n\t\t\tstruct xdp_buff *xdp)\n{\n\tu32 buflen = BNXT_RX_PAGE_SIZE;\n\tstruct bnxt_sw_rx_bd *rx_buf;\n\tstruct pci_dev *pdev;\n\tdma_addr_t mapping;\n\tu32 offset;\n\n\tpdev = bp->pdev;\n\trx_buf = &rxr->rx_buf_ring[cons];\n\toffset = bp->rx_offset;\n\n\tmapping = rx_buf->mapping - bp->rx_dma_offset;\n\tdma_sync_single_for_cpu(&pdev->dev, mapping + offset, len, bp->rx_dir);\n\n\txdp_init_buff(xdp, buflen, &rxr->xdp_rxq);\n\txdp_prepare_buff(xdp, data_ptr - offset, offset, len, false);\n}\n\nvoid bnxt_xdp_buff_frags_free(struct bnxt_rx_ring_info *rxr,\n\t\t\t      struct xdp_buff *xdp)\n{\n\tstruct skb_shared_info *shinfo;\n\tint i;\n\n\tif (!xdp || !xdp_buff_has_frags(xdp))\n\t\treturn;\n\tshinfo = xdp_get_shared_info_from_buff(xdp);\n\tfor (i = 0; i < shinfo->nr_frags; i++) {\n\t\tstruct page *page = skb_frag_page(&shinfo->frags[i]);\n\n\t\tpage_pool_recycle_direct(rxr->page_pool, page);\n\t}\n\tshinfo->nr_frags = 0;\n}\n\n \nbool bnxt_rx_xdp(struct bnxt *bp, struct bnxt_rx_ring_info *rxr, u16 cons,\n\t\t struct xdp_buff xdp, struct page *page, u8 **data_ptr,\n\t\t unsigned int *len, u8 *event)\n{\n\tstruct bpf_prog *xdp_prog = READ_ONCE(rxr->xdp_prog);\n\tstruct bnxt_tx_ring_info *txr;\n\tstruct bnxt_sw_rx_bd *rx_buf;\n\tstruct pci_dev *pdev;\n\tdma_addr_t mapping;\n\tu32 tx_needed = 1;\n\tvoid *orig_data;\n\tu32 tx_avail;\n\tu32 offset;\n\tu32 act;\n\n\tif (!xdp_prog)\n\t\treturn false;\n\n\tpdev = bp->pdev;\n\toffset = bp->rx_offset;\n\n\ttxr = rxr->bnapi->tx_ring;\n\t \n\torig_data = xdp.data;\n\n\tact = bpf_prog_run_xdp(xdp_prog, &xdp);\n\n\ttx_avail = bnxt_tx_avail(bp, txr);\n\t \n\tif (tx_avail != bp->tx_ring_size)\n\t\t*event &= ~BNXT_RX_EVENT;\n\n\t*len = xdp.data_end - xdp.data;\n\tif (orig_data != xdp.data) {\n\t\toffset = xdp.data - xdp.data_hard_start;\n\t\t*data_ptr = xdp.data_hard_start + offset;\n\t}\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\treturn false;\n\n\tcase XDP_TX:\n\t\trx_buf = &rxr->rx_buf_ring[cons];\n\t\tmapping = rx_buf->mapping - bp->rx_dma_offset;\n\t\t*event = 0;\n\n\t\tif (unlikely(xdp_buff_has_frags(&xdp))) {\n\t\t\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(&xdp);\n\n\t\t\ttx_needed += sinfo->nr_frags;\n\t\t\t*event = BNXT_AGG_EVENT;\n\t\t}\n\n\t\tif (tx_avail < tx_needed) {\n\t\t\ttrace_xdp_exception(bp->dev, xdp_prog, act);\n\t\t\tbnxt_xdp_buff_frags_free(rxr, &xdp);\n\t\t\tbnxt_reuse_rx_data(rxr, cons, page);\n\t\t\treturn true;\n\t\t}\n\n\t\tdma_sync_single_for_device(&pdev->dev, mapping + offset, *len,\n\t\t\t\t\t   bp->rx_dir);\n\n\t\t*event |= BNXT_TX_EVENT;\n\t\t__bnxt_xmit_xdp(bp, txr, mapping + offset, *len,\n\t\t\t\tNEXT_RX(rxr->rx_prod), &xdp);\n\t\tbnxt_reuse_rx_data(rxr, cons, page);\n\t\treturn true;\n\tcase XDP_REDIRECT:\n\t\t \n\t\trx_buf = &rxr->rx_buf_ring[cons];\n\t\tmapping = rx_buf->mapping - bp->rx_dma_offset;\n\t\tdma_unmap_page_attrs(&pdev->dev, mapping,\n\t\t\t\t     BNXT_RX_PAGE_SIZE, bp->rx_dir,\n\t\t\t\t     DMA_ATTR_WEAK_ORDERING);\n\n\t\t \n\t\tif (bnxt_alloc_rx_data(bp, rxr, rxr->rx_prod, GFP_ATOMIC)) {\n\t\t\ttrace_xdp_exception(bp->dev, xdp_prog, act);\n\t\t\tbnxt_xdp_buff_frags_free(rxr, &xdp);\n\t\t\tbnxt_reuse_rx_data(rxr, cons, page);\n\t\t\treturn true;\n\t\t}\n\n\t\tif (xdp_do_redirect(bp->dev, &xdp, xdp_prog)) {\n\t\t\ttrace_xdp_exception(bp->dev, xdp_prog, act);\n\t\t\tpage_pool_recycle_direct(rxr->page_pool, page);\n\t\t\treturn true;\n\t\t}\n\n\t\t*event |= BNXT_REDIRECT_EVENT;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(bp->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(bp->dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tbnxt_xdp_buff_frags_free(rxr, &xdp);\n\t\tbnxt_reuse_rx_data(rxr, cons, page);\n\t\tbreak;\n\t}\n\treturn true;\n}\n\nint bnxt_xdp_xmit(struct net_device *dev, int num_frames,\n\t\t  struct xdp_frame **frames, u32 flags)\n{\n\tstruct bnxt *bp = netdev_priv(dev);\n\tstruct bpf_prog *xdp_prog = READ_ONCE(bp->xdp_prog);\n\tstruct pci_dev *pdev = bp->pdev;\n\tstruct bnxt_tx_ring_info *txr;\n\tdma_addr_t mapping;\n\tint nxmit = 0;\n\tint ring;\n\tint i;\n\n\tif (!test_bit(BNXT_STATE_OPEN, &bp->state) ||\n\t    !bp->tx_nr_rings_xdp ||\n\t    !xdp_prog)\n\t\treturn -EINVAL;\n\n\tring = smp_processor_id() % bp->tx_nr_rings_xdp;\n\ttxr = &bp->tx_ring[ring];\n\n\tif (READ_ONCE(txr->dev_state) == BNXT_DEV_STATE_CLOSING)\n\t\treturn -EINVAL;\n\n\tif (static_branch_unlikely(&bnxt_xdp_locking_key))\n\t\tspin_lock(&txr->xdp_tx_lock);\n\n\tfor (i = 0; i < num_frames; i++) {\n\t\tstruct xdp_frame *xdp = frames[i];\n\n\t\tif (!bnxt_tx_avail(bp, txr))\n\t\t\tbreak;\n\n\t\tmapping = dma_map_single(&pdev->dev, xdp->data, xdp->len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\n\t\tif (dma_mapping_error(&pdev->dev, mapping))\n\t\t\tbreak;\n\n\t\t__bnxt_xmit_xdp_redirect(bp, txr, mapping, xdp->len, xdp);\n\t\tnxmit++;\n\t}\n\n\tif (flags & XDP_XMIT_FLUSH) {\n\t\t \n\t\twmb();\n\t\tbnxt_db_write(bp, &txr->tx_db, txr->tx_prod);\n\t}\n\n\tif (static_branch_unlikely(&bnxt_xdp_locking_key))\n\t\tspin_unlock(&txr->xdp_tx_lock);\n\n\treturn nxmit;\n}\n\n \nstatic int bnxt_xdp_set(struct bnxt *bp, struct bpf_prog *prog)\n{\n\tstruct net_device *dev = bp->dev;\n\tint tx_xdp = 0, rc, tc;\n\tstruct bpf_prog *old;\n\n\tif (prog && !prog->aux->xdp_has_frags &&\n\t    bp->dev->mtu > BNXT_MAX_PAGE_MODE_MTU) {\n\t\tnetdev_warn(dev, \"MTU %d larger than %d without XDP frag support.\\n\",\n\t\t\t    bp->dev->mtu, BNXT_MAX_PAGE_MODE_MTU);\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (!(bp->flags & BNXT_FLAG_SHARED_RINGS)) {\n\t\tnetdev_warn(dev, \"ethtool rx/tx channels must be combined to support XDP.\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (prog)\n\t\ttx_xdp = bp->rx_nr_rings;\n\n\ttc = netdev_get_num_tc(dev);\n\tif (!tc)\n\t\ttc = 1;\n\trc = bnxt_check_rings(bp, bp->tx_nr_rings_per_tc, bp->rx_nr_rings,\n\t\t\t      true, tc, tx_xdp);\n\tif (rc) {\n\t\tnetdev_warn(dev, \"Unable to reserve enough TX rings to support XDP.\\n\");\n\t\treturn rc;\n\t}\n\tif (netif_running(dev))\n\t\tbnxt_close_nic(bp, true, false);\n\n\told = xchg(&bp->xdp_prog, prog);\n\tif (old)\n\t\tbpf_prog_put(old);\n\n\tif (prog) {\n\t\tbnxt_set_rx_skb_mode(bp, true);\n\t\txdp_features_set_redirect_target(dev, true);\n\t} else {\n\t\tint rx, tx;\n\n\t\txdp_features_clear_redirect_target(dev);\n\t\tbnxt_set_rx_skb_mode(bp, false);\n\t\tbnxt_get_max_rings(bp, &rx, &tx, true);\n\t\tif (rx > 1) {\n\t\t\tbp->flags &= ~BNXT_FLAG_NO_AGG_RINGS;\n\t\t\tbp->dev->hw_features |= NETIF_F_LRO;\n\t\t}\n\t}\n\tbp->tx_nr_rings_xdp = tx_xdp;\n\tbp->tx_nr_rings = bp->tx_nr_rings_per_tc * tc + tx_xdp;\n\tbp->cp_nr_rings = max_t(int, bp->tx_nr_rings, bp->rx_nr_rings);\n\tbnxt_set_tpa_flags(bp);\n\tbnxt_set_ring_params(bp);\n\n\tif (netif_running(dev))\n\t\treturn bnxt_open_nic(bp, true, false);\n\n\treturn 0;\n}\n\nint bnxt_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct bnxt *bp = netdev_priv(dev);\n\tint rc;\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\trc = bnxt_xdp_set(bp, xdp->prog);\n\t\tbreak;\n\tdefault:\n\t\trc = -EINVAL;\n\t\tbreak;\n\t}\n\treturn rc;\n}\n\nstruct sk_buff *\nbnxt_xdp_build_skb(struct bnxt *bp, struct sk_buff *skb, u8 num_frags,\n\t\t   struct page_pool *pool, struct xdp_buff *xdp,\n\t\t   struct rx_cmp_ext *rxcmp1)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\n\tif (!skb)\n\t\treturn NULL;\n\tskb_checksum_none_assert(skb);\n\tif (RX_CMP_L4_CS_OK(rxcmp1)) {\n\t\tif (bp->dev->features & NETIF_F_RXCSUM) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tskb->csum_level = RX_CMP_ENCAP(rxcmp1);\n\t\t}\n\t}\n\txdp_update_skb_shared_info(skb, num_frags,\n\t\t\t\t   sinfo->xdp_frags_size,\n\t\t\t\t   BNXT_RX_PAGE_SIZE * sinfo->nr_frags,\n\t\t\t\t   xdp_buff_is_frag_pfmemalloc(xdp));\n\treturn skb;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}