{
  "module_name": "bnxt_sriov.c",
  "hash_id": "14da56c6dcc74d4c9ae24dd4edf55c19165ef805741727a82aaeb3099f9dc9e6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/broadcom/bnxt/bnxt_sriov.c",
  "human_readable_source": " \n\n#include <linux/ethtool.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/interrupt.h>\n#include <linux/etherdevice.h>\n#include \"bnxt_hsi.h\"\n#include \"bnxt.h\"\n#include \"bnxt_hwrm.h\"\n#include \"bnxt_ulp.h\"\n#include \"bnxt_sriov.h\"\n#include \"bnxt_vfr.h\"\n#include \"bnxt_ethtool.h\"\n\n#ifdef CONFIG_BNXT_SRIOV\nstatic int bnxt_hwrm_fwd_async_event_cmpl(struct bnxt *bp,\n\t\t\t\t\t  struct bnxt_vf_info *vf, u16 event_id)\n{\n\tstruct hwrm_fwd_async_event_cmpl_input *req;\n\tstruct hwrm_async_event_cmpl *async_cmpl;\n\tint rc = 0;\n\n\trc = hwrm_req_init(bp, req, HWRM_FWD_ASYNC_EVENT_CMPL);\n\tif (rc)\n\t\tgoto exit;\n\n\tif (vf)\n\t\treq->encap_async_event_target_id = cpu_to_le16(vf->fw_fid);\n\telse\n\t\t \n\t\treq->encap_async_event_target_id = cpu_to_le16(0xffff);\n\tasync_cmpl =\n\t\t(struct hwrm_async_event_cmpl *)req->encap_async_event_cmpl;\n\tasync_cmpl->type = cpu_to_le16(ASYNC_EVENT_CMPL_TYPE_HWRM_ASYNC_EVENT);\n\tasync_cmpl->event_id = cpu_to_le16(event_id);\n\n\trc = hwrm_req_send(bp, req);\nexit:\n\tif (rc)\n\t\tnetdev_err(bp->dev, \"hwrm_fwd_async_event_cmpl failed. rc:%d\\n\",\n\t\t\t   rc);\n\treturn rc;\n}\n\nstatic int bnxt_vf_ndo_prep(struct bnxt *bp, int vf_id)\n{\n\tif (!bp->pf.active_vfs) {\n\t\tnetdev_err(bp->dev, \"vf ndo called though sriov is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (vf_id >= bp->pf.active_vfs) {\n\t\tnetdev_err(bp->dev, \"Invalid VF id %d\\n\", vf_id);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nint bnxt_set_vf_spoofchk(struct net_device *dev, int vf_id, bool setting)\n{\n\tstruct bnxt *bp = netdev_priv(dev);\n\tstruct hwrm_func_cfg_input *req;\n\tbool old_setting = false;\n\tstruct bnxt_vf_info *vf;\n\tu32 func_flags;\n\tint rc;\n\n\tif (bp->hwrm_spec_code < 0x10701)\n\t\treturn -ENOTSUPP;\n\n\trc = bnxt_vf_ndo_prep(bp, vf_id);\n\tif (rc)\n\t\treturn rc;\n\n\tvf = &bp->pf.vf[vf_id];\n\tif (vf->flags & BNXT_VF_SPOOFCHK)\n\t\told_setting = true;\n\tif (old_setting == setting)\n\t\treturn 0;\n\n\tif (setting)\n\t\tfunc_flags = FUNC_CFG_REQ_FLAGS_SRC_MAC_ADDR_CHECK_ENABLE;\n\telse\n\t\tfunc_flags = FUNC_CFG_REQ_FLAGS_SRC_MAC_ADDR_CHECK_DISABLE;\n\t \n\trc = hwrm_req_init(bp, req, HWRM_FUNC_CFG);\n\tif (!rc) {\n\t\treq->fid = cpu_to_le16(vf->fw_fid);\n\t\treq->flags = cpu_to_le32(func_flags);\n\t\trc = hwrm_req_send(bp, req);\n\t\tif (!rc) {\n\t\t\tif (setting)\n\t\t\t\tvf->flags |= BNXT_VF_SPOOFCHK;\n\t\t\telse\n\t\t\t\tvf->flags &= ~BNXT_VF_SPOOFCHK;\n\t\t}\n\t}\n\treturn rc;\n}\n\nstatic int bnxt_hwrm_func_qcfg_flags(struct bnxt *bp, struct bnxt_vf_info *vf)\n{\n\tstruct hwrm_func_qcfg_output *resp;\n\tstruct hwrm_func_qcfg_input *req;\n\tint rc;\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_QCFG);\n\tif (rc)\n\t\treturn rc;\n\n\treq->fid = cpu_to_le16(BNXT_PF(bp) ? vf->fw_fid : 0xffff);\n\tresp = hwrm_req_hold(bp, req);\n\trc = hwrm_req_send(bp, req);\n\tif (!rc)\n\t\tvf->func_qcfg_flags = le16_to_cpu(resp->flags);\n\thwrm_req_drop(bp, req);\n\treturn rc;\n}\n\nbool bnxt_is_trusted_vf(struct bnxt *bp, struct bnxt_vf_info *vf)\n{\n\tif (BNXT_PF(bp) && !(bp->fw_cap & BNXT_FW_CAP_TRUSTED_VF))\n\t\treturn !!(vf->flags & BNXT_VF_TRUST);\n\n\tbnxt_hwrm_func_qcfg_flags(bp, vf);\n\treturn !!(vf->func_qcfg_flags & FUNC_QCFG_RESP_FLAGS_TRUSTED_VF);\n}\n\nstatic int bnxt_hwrm_set_trusted_vf(struct bnxt *bp, struct bnxt_vf_info *vf)\n{\n\tstruct hwrm_func_cfg_input *req;\n\tint rc;\n\n\tif (!(bp->fw_cap & BNXT_FW_CAP_TRUSTED_VF))\n\t\treturn 0;\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_CFG);\n\tif (rc)\n\t\treturn rc;\n\n\treq->fid = cpu_to_le16(vf->fw_fid);\n\tif (vf->flags & BNXT_VF_TRUST)\n\t\treq->flags = cpu_to_le32(FUNC_CFG_REQ_FLAGS_TRUSTED_VF_ENABLE);\n\telse\n\t\treq->flags = cpu_to_le32(FUNC_CFG_REQ_FLAGS_TRUSTED_VF_DISABLE);\n\treturn hwrm_req_send(bp, req);\n}\n\nint bnxt_set_vf_trust(struct net_device *dev, int vf_id, bool trusted)\n{\n\tstruct bnxt *bp = netdev_priv(dev);\n\tstruct bnxt_vf_info *vf;\n\n\tif (bnxt_vf_ndo_prep(bp, vf_id))\n\t\treturn -EINVAL;\n\n\tvf = &bp->pf.vf[vf_id];\n\tif (trusted)\n\t\tvf->flags |= BNXT_VF_TRUST;\n\telse\n\t\tvf->flags &= ~BNXT_VF_TRUST;\n\n\tbnxt_hwrm_set_trusted_vf(bp, vf);\n\treturn 0;\n}\n\nint bnxt_get_vf_config(struct net_device *dev, int vf_id,\n\t\t       struct ifla_vf_info *ivi)\n{\n\tstruct bnxt *bp = netdev_priv(dev);\n\tstruct bnxt_vf_info *vf;\n\tint rc;\n\n\trc = bnxt_vf_ndo_prep(bp, vf_id);\n\tif (rc)\n\t\treturn rc;\n\n\tivi->vf = vf_id;\n\tvf = &bp->pf.vf[vf_id];\n\n\tif (is_valid_ether_addr(vf->mac_addr))\n\t\tmemcpy(&ivi->mac, vf->mac_addr, ETH_ALEN);\n\telse\n\t\tmemcpy(&ivi->mac, vf->vf_mac_addr, ETH_ALEN);\n\tivi->max_tx_rate = vf->max_tx_rate;\n\tivi->min_tx_rate = vf->min_tx_rate;\n\tivi->vlan = vf->vlan;\n\tif (vf->flags & BNXT_VF_QOS)\n\t\tivi->qos = vf->vlan >> VLAN_PRIO_SHIFT;\n\telse\n\t\tivi->qos = 0;\n\tivi->spoofchk = !!(vf->flags & BNXT_VF_SPOOFCHK);\n\tivi->trusted = bnxt_is_trusted_vf(bp, vf);\n\tif (!(vf->flags & BNXT_VF_LINK_FORCED))\n\t\tivi->linkstate = IFLA_VF_LINK_STATE_AUTO;\n\telse if (vf->flags & BNXT_VF_LINK_UP)\n\t\tivi->linkstate = IFLA_VF_LINK_STATE_ENABLE;\n\telse\n\t\tivi->linkstate = IFLA_VF_LINK_STATE_DISABLE;\n\n\treturn 0;\n}\n\nint bnxt_set_vf_mac(struct net_device *dev, int vf_id, u8 *mac)\n{\n\tstruct bnxt *bp = netdev_priv(dev);\n\tstruct hwrm_func_cfg_input *req;\n\tstruct bnxt_vf_info *vf;\n\tint rc;\n\n\trc = bnxt_vf_ndo_prep(bp, vf_id);\n\tif (rc)\n\t\treturn rc;\n\t \n\tif (is_multicast_ether_addr(mac)) {\n\t\tnetdev_err(dev, \"Invalid VF ethernet address\\n\");\n\t\treturn -EINVAL;\n\t}\n\tvf = &bp->pf.vf[vf_id];\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_CFG);\n\tif (rc)\n\t\treturn rc;\n\n\tmemcpy(vf->mac_addr, mac, ETH_ALEN);\n\n\treq->fid = cpu_to_le16(vf->fw_fid);\n\treq->enables = cpu_to_le32(FUNC_CFG_REQ_ENABLES_DFLT_MAC_ADDR);\n\tmemcpy(req->dflt_mac_addr, mac, ETH_ALEN);\n\treturn hwrm_req_send(bp, req);\n}\n\nint bnxt_set_vf_vlan(struct net_device *dev, int vf_id, u16 vlan_id, u8 qos,\n\t\t     __be16 vlan_proto)\n{\n\tstruct bnxt *bp = netdev_priv(dev);\n\tstruct hwrm_func_cfg_input *req;\n\tstruct bnxt_vf_info *vf;\n\tu16 vlan_tag;\n\tint rc;\n\n\tif (bp->hwrm_spec_code < 0x10201)\n\t\treturn -ENOTSUPP;\n\n\tif (vlan_proto != htons(ETH_P_8021Q))\n\t\treturn -EPROTONOSUPPORT;\n\n\trc = bnxt_vf_ndo_prep(bp, vf_id);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (vlan_id > 4095 || qos)\n\t\treturn -EINVAL;\n\n\tvf = &bp->pf.vf[vf_id];\n\tvlan_tag = vlan_id;\n\tif (vlan_tag == vf->vlan)\n\t\treturn 0;\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_CFG);\n\tif (!rc) {\n\t\treq->fid = cpu_to_le16(vf->fw_fid);\n\t\treq->dflt_vlan = cpu_to_le16(vlan_tag);\n\t\treq->enables = cpu_to_le32(FUNC_CFG_REQ_ENABLES_DFLT_VLAN);\n\t\trc = hwrm_req_send(bp, req);\n\t\tif (!rc)\n\t\t\tvf->vlan = vlan_tag;\n\t}\n\treturn rc;\n}\n\nint bnxt_set_vf_bw(struct net_device *dev, int vf_id, int min_tx_rate,\n\t\t   int max_tx_rate)\n{\n\tstruct bnxt *bp = netdev_priv(dev);\n\tstruct hwrm_func_cfg_input *req;\n\tstruct bnxt_vf_info *vf;\n\tu32 pf_link_speed;\n\tint rc;\n\n\trc = bnxt_vf_ndo_prep(bp, vf_id);\n\tif (rc)\n\t\treturn rc;\n\n\tvf = &bp->pf.vf[vf_id];\n\tpf_link_speed = bnxt_fw_to_ethtool_speed(bp->link_info.link_speed);\n\tif (max_tx_rate > pf_link_speed) {\n\t\tnetdev_info(bp->dev, \"max tx rate %d exceed PF link speed for VF %d\\n\",\n\t\t\t    max_tx_rate, vf_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (min_tx_rate > pf_link_speed) {\n\t\tnetdev_info(bp->dev, \"min tx rate %d is invalid for VF %d\\n\",\n\t\t\t    min_tx_rate, vf_id);\n\t\treturn -EINVAL;\n\t}\n\tif (min_tx_rate == vf->min_tx_rate && max_tx_rate == vf->max_tx_rate)\n\t\treturn 0;\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_CFG);\n\tif (!rc) {\n\t\treq->fid = cpu_to_le16(vf->fw_fid);\n\t\treq->enables = cpu_to_le32(FUNC_CFG_REQ_ENABLES_MAX_BW |\n\t\t\t\t\t   FUNC_CFG_REQ_ENABLES_MIN_BW);\n\t\treq->max_bw = cpu_to_le32(max_tx_rate);\n\t\treq->min_bw = cpu_to_le32(min_tx_rate);\n\t\trc = hwrm_req_send(bp, req);\n\t\tif (!rc) {\n\t\t\tvf->min_tx_rate = min_tx_rate;\n\t\t\tvf->max_tx_rate = max_tx_rate;\n\t\t}\n\t}\n\treturn rc;\n}\n\nint bnxt_set_vf_link_state(struct net_device *dev, int vf_id, int link)\n{\n\tstruct bnxt *bp = netdev_priv(dev);\n\tstruct bnxt_vf_info *vf;\n\tint rc;\n\n\trc = bnxt_vf_ndo_prep(bp, vf_id);\n\tif (rc)\n\t\treturn rc;\n\n\tvf = &bp->pf.vf[vf_id];\n\n\tvf->flags &= ~(BNXT_VF_LINK_UP | BNXT_VF_LINK_FORCED);\n\tswitch (link) {\n\tcase IFLA_VF_LINK_STATE_AUTO:\n\t\tvf->flags |= BNXT_VF_LINK_UP;\n\t\tbreak;\n\tcase IFLA_VF_LINK_STATE_DISABLE:\n\t\tvf->flags |= BNXT_VF_LINK_FORCED;\n\t\tbreak;\n\tcase IFLA_VF_LINK_STATE_ENABLE:\n\t\tvf->flags |= BNXT_VF_LINK_UP | BNXT_VF_LINK_FORCED;\n\t\tbreak;\n\tdefault:\n\t\tnetdev_err(bp->dev, \"Invalid link option\\n\");\n\t\trc = -EINVAL;\n\t\tbreak;\n\t}\n\tif (vf->flags & (BNXT_VF_LINK_UP | BNXT_VF_LINK_FORCED))\n\t\trc = bnxt_hwrm_fwd_async_event_cmpl(bp, vf,\n\t\t\tASYNC_EVENT_CMPL_EVENT_ID_LINK_STATUS_CHANGE);\n\treturn rc;\n}\n\nstatic int bnxt_set_vf_attr(struct bnxt *bp, int num_vfs)\n{\n\tint i;\n\tstruct bnxt_vf_info *vf;\n\n\tfor (i = 0; i < num_vfs; i++) {\n\t\tvf = &bp->pf.vf[i];\n\t\tmemset(vf, 0, sizeof(*vf));\n\t}\n\treturn 0;\n}\n\nstatic int bnxt_hwrm_func_vf_resource_free(struct bnxt *bp, int num_vfs)\n{\n\tstruct hwrm_func_vf_resc_free_input *req;\n\tstruct bnxt_pf_info *pf = &bp->pf;\n\tint i, rc;\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_VF_RESC_FREE);\n\tif (rc)\n\t\treturn rc;\n\n\thwrm_req_hold(bp, req);\n\tfor (i = pf->first_vf_id; i < pf->first_vf_id + num_vfs; i++) {\n\t\treq->vf_id = cpu_to_le16(i);\n\t\trc = hwrm_req_send(bp, req);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\thwrm_req_drop(bp, req);\n\treturn rc;\n}\n\nstatic void bnxt_free_vf_resources(struct bnxt *bp)\n{\n\tstruct pci_dev *pdev = bp->pdev;\n\tint i;\n\n\tkfree(bp->pf.vf_event_bmap);\n\tbp->pf.vf_event_bmap = NULL;\n\n\tfor (i = 0; i < 4; i++) {\n\t\tif (bp->pf.hwrm_cmd_req_addr[i]) {\n\t\t\tdma_free_coherent(&pdev->dev, BNXT_PAGE_SIZE,\n\t\t\t\t\t  bp->pf.hwrm_cmd_req_addr[i],\n\t\t\t\t\t  bp->pf.hwrm_cmd_req_dma_addr[i]);\n\t\t\tbp->pf.hwrm_cmd_req_addr[i] = NULL;\n\t\t}\n\t}\n\n\tbp->pf.active_vfs = 0;\n\tkfree(bp->pf.vf);\n\tbp->pf.vf = NULL;\n}\n\nstatic int bnxt_alloc_vf_resources(struct bnxt *bp, int num_vfs)\n{\n\tstruct pci_dev *pdev = bp->pdev;\n\tu32 nr_pages, size, i, j, k = 0;\n\n\tbp->pf.vf = kcalloc(num_vfs, sizeof(struct bnxt_vf_info), GFP_KERNEL);\n\tif (!bp->pf.vf)\n\t\treturn -ENOMEM;\n\n\tbnxt_set_vf_attr(bp, num_vfs);\n\n\tsize = num_vfs * BNXT_HWRM_REQ_MAX_SIZE;\n\tnr_pages = size / BNXT_PAGE_SIZE;\n\tif (size & (BNXT_PAGE_SIZE - 1))\n\t\tnr_pages++;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tbp->pf.hwrm_cmd_req_addr[i] =\n\t\t\tdma_alloc_coherent(&pdev->dev, BNXT_PAGE_SIZE,\n\t\t\t\t\t   &bp->pf.hwrm_cmd_req_dma_addr[i],\n\t\t\t\t\t   GFP_KERNEL);\n\n\t\tif (!bp->pf.hwrm_cmd_req_addr[i])\n\t\t\treturn -ENOMEM;\n\n\t\tfor (j = 0; j < BNXT_HWRM_REQS_PER_PAGE && k < num_vfs; j++) {\n\t\t\tstruct bnxt_vf_info *vf = &bp->pf.vf[k];\n\n\t\t\tvf->hwrm_cmd_req_addr = bp->pf.hwrm_cmd_req_addr[i] +\n\t\t\t\t\t\tj * BNXT_HWRM_REQ_MAX_SIZE;\n\t\t\tvf->hwrm_cmd_req_dma_addr =\n\t\t\t\tbp->pf.hwrm_cmd_req_dma_addr[i] + j *\n\t\t\t\tBNXT_HWRM_REQ_MAX_SIZE;\n\t\t\tk++;\n\t\t}\n\t}\n\n\t \n\tbp->pf.vf_event_bmap = kzalloc(16, GFP_KERNEL);\n\tif (!bp->pf.vf_event_bmap)\n\t\treturn -ENOMEM;\n\n\tbp->pf.hwrm_cmd_req_pages = nr_pages;\n\treturn 0;\n}\n\nstatic int bnxt_hwrm_func_buf_rgtr(struct bnxt *bp)\n{\n\tstruct hwrm_func_buf_rgtr_input *req;\n\tint rc;\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_BUF_RGTR);\n\tif (rc)\n\t\treturn rc;\n\n\treq->req_buf_num_pages = cpu_to_le16(bp->pf.hwrm_cmd_req_pages);\n\treq->req_buf_page_size = cpu_to_le16(BNXT_PAGE_SHIFT);\n\treq->req_buf_len = cpu_to_le16(BNXT_HWRM_REQ_MAX_SIZE);\n\treq->req_buf_page_addr0 = cpu_to_le64(bp->pf.hwrm_cmd_req_dma_addr[0]);\n\treq->req_buf_page_addr1 = cpu_to_le64(bp->pf.hwrm_cmd_req_dma_addr[1]);\n\treq->req_buf_page_addr2 = cpu_to_le64(bp->pf.hwrm_cmd_req_dma_addr[2]);\n\treq->req_buf_page_addr3 = cpu_to_le64(bp->pf.hwrm_cmd_req_dma_addr[3]);\n\n\treturn hwrm_req_send(bp, req);\n}\n\nstatic int __bnxt_set_vf_params(struct bnxt *bp, int vf_id)\n{\n\tstruct hwrm_func_cfg_input *req;\n\tstruct bnxt_vf_info *vf;\n\tint rc;\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_CFG);\n\tif (rc)\n\t\treturn rc;\n\n\tvf = &bp->pf.vf[vf_id];\n\treq->fid = cpu_to_le16(vf->fw_fid);\n\n\tif (is_valid_ether_addr(vf->mac_addr)) {\n\t\treq->enables |= cpu_to_le32(FUNC_CFG_REQ_ENABLES_DFLT_MAC_ADDR);\n\t\tmemcpy(req->dflt_mac_addr, vf->mac_addr, ETH_ALEN);\n\t}\n\tif (vf->vlan) {\n\t\treq->enables |= cpu_to_le32(FUNC_CFG_REQ_ENABLES_DFLT_VLAN);\n\t\treq->dflt_vlan = cpu_to_le16(vf->vlan);\n\t}\n\tif (vf->max_tx_rate) {\n\t\treq->enables |= cpu_to_le32(FUNC_CFG_REQ_ENABLES_MAX_BW |\n\t\t\t\t\t    FUNC_CFG_REQ_ENABLES_MIN_BW);\n\t\treq->max_bw = cpu_to_le32(vf->max_tx_rate);\n\t\treq->min_bw = cpu_to_le32(vf->min_tx_rate);\n\t}\n\tif (vf->flags & BNXT_VF_TRUST)\n\t\treq->flags |= cpu_to_le32(FUNC_CFG_REQ_FLAGS_TRUSTED_VF_ENABLE);\n\n\treturn hwrm_req_send(bp, req);\n}\n\n \nstatic int bnxt_hwrm_func_vf_resc_cfg(struct bnxt *bp, int num_vfs, bool reset)\n{\n\tstruct hwrm_func_vf_resource_cfg_input *req;\n\tstruct bnxt_hw_resc *hw_resc = &bp->hw_resc;\n\tu16 vf_tx_rings, vf_rx_rings, vf_cp_rings;\n\tu16 vf_stat_ctx, vf_vnics, vf_ring_grps;\n\tstruct bnxt_pf_info *pf = &bp->pf;\n\tint i, rc = 0, min = 1;\n\tu16 vf_msix = 0;\n\tu16 vf_rss;\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_VF_RESOURCE_CFG);\n\tif (rc)\n\t\treturn rc;\n\n\tif (bp->flags & BNXT_FLAG_CHIP_P5) {\n\t\tvf_msix = hw_resc->max_nqs - bnxt_nq_rings_in_use(bp);\n\t\tvf_ring_grps = 0;\n\t} else {\n\t\tvf_ring_grps = hw_resc->max_hw_ring_grps - bp->rx_nr_rings;\n\t}\n\tvf_cp_rings = bnxt_get_avail_cp_rings_for_en(bp);\n\tvf_stat_ctx = bnxt_get_avail_stat_ctxs_for_en(bp);\n\tif (bp->flags & BNXT_FLAG_AGG_RINGS)\n\t\tvf_rx_rings = hw_resc->max_rx_rings - bp->rx_nr_rings * 2;\n\telse\n\t\tvf_rx_rings = hw_resc->max_rx_rings - bp->rx_nr_rings;\n\tvf_tx_rings = hw_resc->max_tx_rings - bp->tx_nr_rings;\n\tvf_vnics = hw_resc->max_vnics - bp->nr_vnics;\n\tvf_vnics = min_t(u16, vf_vnics, vf_rx_rings);\n\tvf_rss = hw_resc->max_rsscos_ctxs - bp->rsscos_nr_ctxs;\n\n\treq->min_rsscos_ctx = cpu_to_le16(BNXT_VF_MIN_RSS_CTX);\n\tif (pf->vf_resv_strategy == BNXT_VF_RESV_STRATEGY_MINIMAL_STATIC) {\n\t\tmin = 0;\n\t\treq->min_rsscos_ctx = cpu_to_le16(min);\n\t}\n\tif (pf->vf_resv_strategy == BNXT_VF_RESV_STRATEGY_MINIMAL ||\n\t    pf->vf_resv_strategy == BNXT_VF_RESV_STRATEGY_MINIMAL_STATIC) {\n\t\treq->min_cmpl_rings = cpu_to_le16(min);\n\t\treq->min_tx_rings = cpu_to_le16(min);\n\t\treq->min_rx_rings = cpu_to_le16(min);\n\t\treq->min_l2_ctxs = cpu_to_le16(min);\n\t\treq->min_vnics = cpu_to_le16(min);\n\t\treq->min_stat_ctx = cpu_to_le16(min);\n\t\tif (!(bp->flags & BNXT_FLAG_CHIP_P5))\n\t\t\treq->min_hw_ring_grps = cpu_to_le16(min);\n\t} else {\n\t\tvf_cp_rings /= num_vfs;\n\t\tvf_tx_rings /= num_vfs;\n\t\tvf_rx_rings /= num_vfs;\n\t\tvf_vnics /= num_vfs;\n\t\tvf_stat_ctx /= num_vfs;\n\t\tvf_ring_grps /= num_vfs;\n\t\tvf_rss /= num_vfs;\n\n\t\treq->min_cmpl_rings = cpu_to_le16(vf_cp_rings);\n\t\treq->min_tx_rings = cpu_to_le16(vf_tx_rings);\n\t\treq->min_rx_rings = cpu_to_le16(vf_rx_rings);\n\t\treq->min_l2_ctxs = cpu_to_le16(BNXT_VF_MAX_L2_CTX);\n\t\treq->min_vnics = cpu_to_le16(vf_vnics);\n\t\treq->min_stat_ctx = cpu_to_le16(vf_stat_ctx);\n\t\treq->min_hw_ring_grps = cpu_to_le16(vf_ring_grps);\n\t\treq->min_rsscos_ctx = cpu_to_le16(vf_rss);\n\t}\n\treq->max_cmpl_rings = cpu_to_le16(vf_cp_rings);\n\treq->max_tx_rings = cpu_to_le16(vf_tx_rings);\n\treq->max_rx_rings = cpu_to_le16(vf_rx_rings);\n\treq->max_l2_ctxs = cpu_to_le16(BNXT_VF_MAX_L2_CTX);\n\treq->max_vnics = cpu_to_le16(vf_vnics);\n\treq->max_stat_ctx = cpu_to_le16(vf_stat_ctx);\n\treq->max_hw_ring_grps = cpu_to_le16(vf_ring_grps);\n\treq->max_rsscos_ctx = cpu_to_le16(vf_rss);\n\tif (bp->flags & BNXT_FLAG_CHIP_P5)\n\t\treq->max_msix = cpu_to_le16(vf_msix / num_vfs);\n\n\thwrm_req_hold(bp, req);\n\tfor (i = 0; i < num_vfs; i++) {\n\t\tif (reset)\n\t\t\t__bnxt_set_vf_params(bp, i);\n\n\t\treq->vf_id = cpu_to_le16(pf->first_vf_id + i);\n\t\trc = hwrm_req_send(bp, req);\n\t\tif (rc)\n\t\t\tbreak;\n\t\tpf->active_vfs = i + 1;\n\t\tpf->vf[i].fw_fid = pf->first_vf_id + i;\n\t}\n\n\tif (pf->active_vfs) {\n\t\tu16 n = pf->active_vfs;\n\n\t\thw_resc->max_tx_rings -= le16_to_cpu(req->min_tx_rings) * n;\n\t\thw_resc->max_rx_rings -= le16_to_cpu(req->min_rx_rings) * n;\n\t\thw_resc->max_hw_ring_grps -=\n\t\t\tle16_to_cpu(req->min_hw_ring_grps) * n;\n\t\thw_resc->max_cp_rings -= le16_to_cpu(req->min_cmpl_rings) * n;\n\t\thw_resc->max_rsscos_ctxs -=\n\t\t\tle16_to_cpu(req->min_rsscos_ctx) * n;\n\t\thw_resc->max_stat_ctxs -= le16_to_cpu(req->min_stat_ctx) * n;\n\t\thw_resc->max_vnics -= le16_to_cpu(req->min_vnics) * n;\n\t\tif (bp->flags & BNXT_FLAG_CHIP_P5)\n\t\t\thw_resc->max_nqs -= vf_msix;\n\n\t\trc = pf->active_vfs;\n\t}\n\thwrm_req_drop(bp, req);\n\treturn rc;\n}\n\n \nstatic int bnxt_hwrm_func_cfg(struct bnxt *bp, int num_vfs)\n{\n\tu16 vf_tx_rings, vf_rx_rings, vf_cp_rings, vf_stat_ctx, vf_vnics;\n\tstruct bnxt_hw_resc *hw_resc = &bp->hw_resc;\n\tstruct bnxt_pf_info *pf = &bp->pf;\n\tstruct hwrm_func_cfg_input *req;\n\tint total_vf_tx_rings = 0;\n\tu16 vf_ring_grps;\n\tu32 mtu, i;\n\tint rc;\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_CFG);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tvf_cp_rings = bnxt_get_avail_cp_rings_for_en(bp) / num_vfs;\n\tvf_stat_ctx = bnxt_get_avail_stat_ctxs_for_en(bp) / num_vfs;\n\tif (bp->flags & BNXT_FLAG_AGG_RINGS)\n\t\tvf_rx_rings = (hw_resc->max_rx_rings - bp->rx_nr_rings * 2) /\n\t\t\t      num_vfs;\n\telse\n\t\tvf_rx_rings = (hw_resc->max_rx_rings - bp->rx_nr_rings) /\n\t\t\t      num_vfs;\n\tvf_ring_grps = (hw_resc->max_hw_ring_grps - bp->rx_nr_rings) / num_vfs;\n\tvf_tx_rings = (hw_resc->max_tx_rings - bp->tx_nr_rings) / num_vfs;\n\tvf_vnics = (hw_resc->max_vnics - bp->nr_vnics) / num_vfs;\n\tvf_vnics = min_t(u16, vf_vnics, vf_rx_rings);\n\n\treq->enables = cpu_to_le32(FUNC_CFG_REQ_ENABLES_ADMIN_MTU |\n\t\t\t\t   FUNC_CFG_REQ_ENABLES_MRU |\n\t\t\t\t   FUNC_CFG_REQ_ENABLES_NUM_RSSCOS_CTXS |\n\t\t\t\t   FUNC_CFG_REQ_ENABLES_NUM_STAT_CTXS |\n\t\t\t\t   FUNC_CFG_REQ_ENABLES_NUM_CMPL_RINGS |\n\t\t\t\t   FUNC_CFG_REQ_ENABLES_NUM_TX_RINGS |\n\t\t\t\t   FUNC_CFG_REQ_ENABLES_NUM_RX_RINGS |\n\t\t\t\t   FUNC_CFG_REQ_ENABLES_NUM_L2_CTXS |\n\t\t\t\t   FUNC_CFG_REQ_ENABLES_NUM_VNICS |\n\t\t\t\t   FUNC_CFG_REQ_ENABLES_NUM_HW_RING_GRPS);\n\n\tmtu = bp->dev->mtu + ETH_HLEN + VLAN_HLEN;\n\treq->mru = cpu_to_le16(mtu);\n\treq->admin_mtu = cpu_to_le16(mtu);\n\n\treq->num_rsscos_ctxs = cpu_to_le16(1);\n\treq->num_cmpl_rings = cpu_to_le16(vf_cp_rings);\n\treq->num_tx_rings = cpu_to_le16(vf_tx_rings);\n\treq->num_rx_rings = cpu_to_le16(vf_rx_rings);\n\treq->num_hw_ring_grps = cpu_to_le16(vf_ring_grps);\n\treq->num_l2_ctxs = cpu_to_le16(4);\n\n\treq->num_vnics = cpu_to_le16(vf_vnics);\n\t \n\treq->num_stat_ctxs = cpu_to_le16(vf_stat_ctx);\n\n\thwrm_req_hold(bp, req);\n\tfor (i = 0; i < num_vfs; i++) {\n\t\tint vf_tx_rsvd = vf_tx_rings;\n\n\t\treq->fid = cpu_to_le16(pf->first_vf_id + i);\n\t\trc = hwrm_req_send(bp, req);\n\t\tif (rc)\n\t\t\tbreak;\n\t\tpf->active_vfs = i + 1;\n\t\tpf->vf[i].fw_fid = le16_to_cpu(req->fid);\n\t\trc = __bnxt_hwrm_get_tx_rings(bp, pf->vf[i].fw_fid,\n\t\t\t\t\t      &vf_tx_rsvd);\n\t\tif (rc)\n\t\t\tbreak;\n\t\ttotal_vf_tx_rings += vf_tx_rsvd;\n\t}\n\thwrm_req_drop(bp, req);\n\tif (pf->active_vfs) {\n\t\thw_resc->max_tx_rings -= total_vf_tx_rings;\n\t\thw_resc->max_rx_rings -= vf_rx_rings * num_vfs;\n\t\thw_resc->max_hw_ring_grps -= vf_ring_grps * num_vfs;\n\t\thw_resc->max_cp_rings -= vf_cp_rings * num_vfs;\n\t\thw_resc->max_rsscos_ctxs -= num_vfs;\n\t\thw_resc->max_stat_ctxs -= vf_stat_ctx * num_vfs;\n\t\thw_resc->max_vnics -= vf_vnics * num_vfs;\n\t\trc = pf->active_vfs;\n\t}\n\treturn rc;\n}\n\nstatic int bnxt_func_cfg(struct bnxt *bp, int num_vfs, bool reset)\n{\n\tif (BNXT_NEW_RM(bp))\n\t\treturn bnxt_hwrm_func_vf_resc_cfg(bp, num_vfs, reset);\n\telse\n\t\treturn bnxt_hwrm_func_cfg(bp, num_vfs);\n}\n\nint bnxt_cfg_hw_sriov(struct bnxt *bp, int *num_vfs, bool reset)\n{\n\tint rc;\n\n\t \n\trc = bnxt_hwrm_func_buf_rgtr(bp);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\trc = bnxt_func_cfg(bp, *num_vfs, reset);\n\tif (rc != *num_vfs) {\n\t\tif (rc <= 0) {\n\t\t\tnetdev_warn(bp->dev, \"Unable to reserve resources for SRIOV.\\n\");\n\t\t\t*num_vfs = 0;\n\t\t\treturn rc;\n\t\t}\n\t\tnetdev_warn(bp->dev, \"Only able to reserve resources for %d VFs.\\n\",\n\t\t\t    rc);\n\t\t*num_vfs = rc;\n\t}\n\n\treturn 0;\n}\n\nstatic int bnxt_sriov_enable(struct bnxt *bp, int *num_vfs)\n{\n\tint rc = 0, vfs_supported;\n\tint min_rx_rings, min_tx_rings, min_rss_ctxs;\n\tstruct bnxt_hw_resc *hw_resc = &bp->hw_resc;\n\tint tx_ok = 0, rx_ok = 0, rss_ok = 0;\n\tint avail_cp, avail_stat;\n\n\t \n\tvfs_supported = *num_vfs;\n\n\tavail_cp = bnxt_get_avail_cp_rings_for_en(bp);\n\tavail_stat = bnxt_get_avail_stat_ctxs_for_en(bp);\n\tavail_cp = min_t(int, avail_cp, avail_stat);\n\n\twhile (vfs_supported) {\n\t\tmin_rx_rings = vfs_supported;\n\t\tmin_tx_rings = vfs_supported;\n\t\tmin_rss_ctxs = vfs_supported;\n\n\t\tif (bp->flags & BNXT_FLAG_AGG_RINGS) {\n\t\t\tif (hw_resc->max_rx_rings - bp->rx_nr_rings * 2 >=\n\t\t\t    min_rx_rings)\n\t\t\t\trx_ok = 1;\n\t\t} else {\n\t\t\tif (hw_resc->max_rx_rings - bp->rx_nr_rings >=\n\t\t\t    min_rx_rings)\n\t\t\t\trx_ok = 1;\n\t\t}\n\t\tif (hw_resc->max_vnics - bp->nr_vnics < min_rx_rings ||\n\t\t    avail_cp < min_rx_rings)\n\t\t\trx_ok = 0;\n\n\t\tif (hw_resc->max_tx_rings - bp->tx_nr_rings >= min_tx_rings &&\n\t\t    avail_cp >= min_tx_rings)\n\t\t\ttx_ok = 1;\n\n\t\tif (hw_resc->max_rsscos_ctxs - bp->rsscos_nr_ctxs >=\n\t\t    min_rss_ctxs)\n\t\t\trss_ok = 1;\n\n\t\tif (tx_ok && rx_ok && rss_ok)\n\t\t\tbreak;\n\n\t\tvfs_supported--;\n\t}\n\n\tif (!vfs_supported) {\n\t\tnetdev_err(bp->dev, \"Cannot enable VF's as all resources are used by PF\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (vfs_supported != *num_vfs) {\n\t\tnetdev_info(bp->dev, \"Requested VFs %d, can enable %d\\n\",\n\t\t\t    *num_vfs, vfs_supported);\n\t\t*num_vfs = vfs_supported;\n\t}\n\n\trc = bnxt_alloc_vf_resources(bp, *num_vfs);\n\tif (rc)\n\t\tgoto err_out1;\n\n\trc = bnxt_cfg_hw_sriov(bp, num_vfs, false);\n\tif (rc)\n\t\tgoto err_out2;\n\n\trc = pci_enable_sriov(bp->pdev, *num_vfs);\n\tif (rc)\n\t\tgoto err_out2;\n\n\tif (bp->eswitch_mode != DEVLINK_ESWITCH_MODE_SWITCHDEV)\n\t\treturn 0;\n\n\t \n\tdevl_lock(bp->dl);\n\trc = bnxt_vf_reps_create(bp);\n\tdevl_unlock(bp->dl);\n\tif (rc) {\n\t\tnetdev_info(bp->dev, \"Cannot enable VFS as representors cannot be created\\n\");\n\t\tgoto err_out3;\n\t}\n\n\treturn 0;\n\nerr_out3:\n\t \n\tpci_disable_sriov(bp->pdev);\n\nerr_out2:\n\t \n\tbnxt_hwrm_func_vf_resource_free(bp, *num_vfs);\n\n\t \n\tbnxt_hwrm_func_qcaps(bp);\n\nerr_out1:\n\tbnxt_free_vf_resources(bp);\n\n\treturn rc;\n}\n\nvoid bnxt_sriov_disable(struct bnxt *bp)\n{\n\tu16 num_vfs = pci_num_vf(bp->pdev);\n\n\tif (!num_vfs)\n\t\treturn;\n\n\t \n\tdevl_lock(bp->dl);\n\tbnxt_vf_reps_destroy(bp);\n\n\tif (pci_vfs_assigned(bp->pdev)) {\n\t\tbnxt_hwrm_fwd_async_event_cmpl(\n\t\t\tbp, NULL, ASYNC_EVENT_CMPL_EVENT_ID_PF_DRVR_UNLOAD);\n\t\tnetdev_warn(bp->dev, \"Unable to free %d VFs because some are assigned to VMs.\\n\",\n\t\t\t    num_vfs);\n\t} else {\n\t\tpci_disable_sriov(bp->pdev);\n\t\t \n\t\tbnxt_hwrm_func_vf_resource_free(bp, num_vfs);\n\t}\n\tdevl_unlock(bp->dl);\n\n\tbnxt_free_vf_resources(bp);\n\n\t \n\trtnl_lock();\n\tbnxt_restore_pf_fw_resources(bp);\n\trtnl_unlock();\n}\n\nint bnxt_sriov_configure(struct pci_dev *pdev, int num_vfs)\n{\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct bnxt *bp = netdev_priv(dev);\n\n\tif (!(bp->flags & BNXT_FLAG_USING_MSIX)) {\n\t\tnetdev_warn(dev, \"Not allow SRIOV if the irq mode is not MSIX\\n\");\n\t\treturn 0;\n\t}\n\n\trtnl_lock();\n\tif (!netif_running(dev)) {\n\t\tnetdev_warn(dev, \"Reject SRIOV config request since if is down!\\n\");\n\t\trtnl_unlock();\n\t\treturn 0;\n\t}\n\tif (test_bit(BNXT_STATE_IN_FW_RESET, &bp->state)) {\n\t\tnetdev_warn(dev, \"Reject SRIOV config request when FW reset is in progress\\n\");\n\t\trtnl_unlock();\n\t\treturn 0;\n\t}\n\tbp->sriov_cfg = true;\n\trtnl_unlock();\n\n\tif (pci_vfs_assigned(bp->pdev)) {\n\t\tnetdev_warn(dev, \"Unable to configure SRIOV since some VFs are assigned to VMs.\\n\");\n\t\tnum_vfs = 0;\n\t\tgoto sriov_cfg_exit;\n\t}\n\n\t \n\tif (num_vfs && num_vfs == bp->pf.active_vfs)\n\t\tgoto sriov_cfg_exit;\n\n\t \n\tbnxt_sriov_disable(bp);\n\tif (!num_vfs)\n\t\tgoto sriov_cfg_exit;\n\n\tbnxt_sriov_enable(bp, &num_vfs);\n\nsriov_cfg_exit:\n\tbp->sriov_cfg = false;\n\twake_up(&bp->sriov_cfg_wait);\n\n\treturn num_vfs;\n}\n\nstatic int bnxt_hwrm_fwd_resp(struct bnxt *bp, struct bnxt_vf_info *vf,\n\t\t\t      void *encap_resp, __le64 encap_resp_addr,\n\t\t\t      __le16 encap_resp_cpr, u32 msg_size)\n{\n\tstruct hwrm_fwd_resp_input *req;\n\tint rc;\n\n\tif (BNXT_FWD_RESP_SIZE_ERR(msg_size))\n\t\treturn -EINVAL;\n\n\trc = hwrm_req_init(bp, req, HWRM_FWD_RESP);\n\tif (!rc) {\n\t\t \n\t\treq->target_id = cpu_to_le16(vf->fw_fid);\n\t\treq->encap_resp_target_id = cpu_to_le16(vf->fw_fid);\n\t\treq->encap_resp_len = cpu_to_le16(msg_size);\n\t\treq->encap_resp_addr = encap_resp_addr;\n\t\treq->encap_resp_cmpl_ring = encap_resp_cpr;\n\t\tmemcpy(req->encap_resp, encap_resp, msg_size);\n\n\t\trc = hwrm_req_send(bp, req);\n\t}\n\tif (rc)\n\t\tnetdev_err(bp->dev, \"hwrm_fwd_resp failed. rc:%d\\n\", rc);\n\treturn rc;\n}\n\nstatic int bnxt_hwrm_fwd_err_resp(struct bnxt *bp, struct bnxt_vf_info *vf,\n\t\t\t\t  u32 msg_size)\n{\n\tstruct hwrm_reject_fwd_resp_input *req;\n\tint rc;\n\n\tif (BNXT_REJ_FWD_RESP_SIZE_ERR(msg_size))\n\t\treturn -EINVAL;\n\n\trc = hwrm_req_init(bp, req, HWRM_REJECT_FWD_RESP);\n\tif (!rc) {\n\t\t \n\t\treq->target_id = cpu_to_le16(vf->fw_fid);\n\t\treq->encap_resp_target_id = cpu_to_le16(vf->fw_fid);\n\t\tmemcpy(req->encap_request, vf->hwrm_cmd_req_addr, msg_size);\n\n\t\trc = hwrm_req_send(bp, req);\n\t}\n\tif (rc)\n\t\tnetdev_err(bp->dev, \"hwrm_fwd_err_resp failed. rc:%d\\n\", rc);\n\treturn rc;\n}\n\nstatic int bnxt_hwrm_exec_fwd_resp(struct bnxt *bp, struct bnxt_vf_info *vf,\n\t\t\t\t   u32 msg_size)\n{\n\tstruct hwrm_exec_fwd_resp_input *req;\n\tint rc;\n\n\tif (BNXT_EXEC_FWD_RESP_SIZE_ERR(msg_size))\n\t\treturn -EINVAL;\n\n\trc = hwrm_req_init(bp, req, HWRM_EXEC_FWD_RESP);\n\tif (!rc) {\n\t\t \n\t\treq->target_id = cpu_to_le16(vf->fw_fid);\n\t\treq->encap_resp_target_id = cpu_to_le16(vf->fw_fid);\n\t\tmemcpy(req->encap_request, vf->hwrm_cmd_req_addr, msg_size);\n\n\t\trc = hwrm_req_send(bp, req);\n\t}\n\tif (rc)\n\t\tnetdev_err(bp->dev, \"hwrm_exec_fw_resp failed. rc:%d\\n\", rc);\n\treturn rc;\n}\n\nstatic int bnxt_vf_configure_mac(struct bnxt *bp, struct bnxt_vf_info *vf)\n{\n\tu32 msg_size = sizeof(struct hwrm_func_vf_cfg_input);\n\tstruct hwrm_func_vf_cfg_input *req =\n\t\t(struct hwrm_func_vf_cfg_input *)vf->hwrm_cmd_req_addr;\n\n\t \n\tif (req->enables & cpu_to_le32(FUNC_VF_CFG_REQ_ENABLES_DFLT_MAC_ADDR)) {\n\t\tbool trust = bnxt_is_trusted_vf(bp, vf);\n\n\t\tif (is_valid_ether_addr(req->dflt_mac_addr) &&\n\t\t    (trust || !is_valid_ether_addr(vf->mac_addr) ||\n\t\t     ether_addr_equal(req->dflt_mac_addr, vf->mac_addr))) {\n\t\t\tether_addr_copy(vf->vf_mac_addr, req->dflt_mac_addr);\n\t\t\treturn bnxt_hwrm_exec_fwd_resp(bp, vf, msg_size);\n\t\t}\n\t\treturn bnxt_hwrm_fwd_err_resp(bp, vf, msg_size);\n\t}\n\treturn bnxt_hwrm_exec_fwd_resp(bp, vf, msg_size);\n}\n\nstatic int bnxt_vf_validate_set_mac(struct bnxt *bp, struct bnxt_vf_info *vf)\n{\n\tu32 msg_size = sizeof(struct hwrm_cfa_l2_filter_alloc_input);\n\tstruct hwrm_cfa_l2_filter_alloc_input *req =\n\t\t(struct hwrm_cfa_l2_filter_alloc_input *)vf->hwrm_cmd_req_addr;\n\tbool mac_ok = false;\n\n\tif (!is_valid_ether_addr((const u8 *)req->l2_addr))\n\t\treturn bnxt_hwrm_fwd_err_resp(bp, vf, msg_size);\n\n\t \n\tif (bnxt_is_trusted_vf(bp, vf)) {\n\t\tmac_ok = true;\n\t} else if (is_valid_ether_addr(vf->mac_addr)) {\n\t\tif (ether_addr_equal((const u8 *)req->l2_addr, vf->mac_addr))\n\t\t\tmac_ok = true;\n\t} else if (is_valid_ether_addr(vf->vf_mac_addr)) {\n\t\tif (ether_addr_equal((const u8 *)req->l2_addr, vf->vf_mac_addr))\n\t\t\tmac_ok = true;\n\t} else {\n\t\t \n\t\tmac_ok = true;\n\t}\n\tif (mac_ok)\n\t\treturn bnxt_hwrm_exec_fwd_resp(bp, vf, msg_size);\n\treturn bnxt_hwrm_fwd_err_resp(bp, vf, msg_size);\n}\n\nstatic int bnxt_vf_set_link(struct bnxt *bp, struct bnxt_vf_info *vf)\n{\n\tint rc = 0;\n\n\tif (!(vf->flags & BNXT_VF_LINK_FORCED)) {\n\t\t \n\t\trc = bnxt_hwrm_exec_fwd_resp(\n\t\t\tbp, vf, sizeof(struct hwrm_port_phy_qcfg_input));\n\t} else {\n\t\tstruct hwrm_port_phy_qcfg_output phy_qcfg_resp = {0};\n\t\tstruct hwrm_port_phy_qcfg_input *phy_qcfg_req;\n\n\t\tphy_qcfg_req =\n\t\t(struct hwrm_port_phy_qcfg_input *)vf->hwrm_cmd_req_addr;\n\t\tmutex_lock(&bp->link_lock);\n\t\tmemcpy(&phy_qcfg_resp, &bp->link_info.phy_qcfg_resp,\n\t\t       sizeof(phy_qcfg_resp));\n\t\tmutex_unlock(&bp->link_lock);\n\t\tphy_qcfg_resp.resp_len = cpu_to_le16(sizeof(phy_qcfg_resp));\n\t\tphy_qcfg_resp.seq_id = phy_qcfg_req->seq_id;\n\t\tphy_qcfg_resp.valid = 1;\n\n\t\tif (vf->flags & BNXT_VF_LINK_UP) {\n\t\t\t \n\t\t\tif (phy_qcfg_resp.link !=\n\t\t\t    PORT_PHY_QCFG_RESP_LINK_LINK) {\n\t\t\t\tphy_qcfg_resp.link =\n\t\t\t\t\tPORT_PHY_QCFG_RESP_LINK_LINK;\n\t\t\t\tphy_qcfg_resp.link_speed = cpu_to_le16(\n\t\t\t\t\tPORT_PHY_QCFG_RESP_LINK_SPEED_10GB);\n\t\t\t\tphy_qcfg_resp.duplex_cfg =\n\t\t\t\t\tPORT_PHY_QCFG_RESP_DUPLEX_CFG_FULL;\n\t\t\t\tphy_qcfg_resp.duplex_state =\n\t\t\t\t\tPORT_PHY_QCFG_RESP_DUPLEX_STATE_FULL;\n\t\t\t\tphy_qcfg_resp.pause =\n\t\t\t\t\t(PORT_PHY_QCFG_RESP_PAUSE_TX |\n\t\t\t\t\t PORT_PHY_QCFG_RESP_PAUSE_RX);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tphy_qcfg_resp.link = PORT_PHY_QCFG_RESP_LINK_NO_LINK;\n\t\t\tphy_qcfg_resp.link_speed = 0;\n\t\t\tphy_qcfg_resp.duplex_state =\n\t\t\t\tPORT_PHY_QCFG_RESP_DUPLEX_STATE_HALF;\n\t\t\tphy_qcfg_resp.pause = 0;\n\t\t}\n\t\trc = bnxt_hwrm_fwd_resp(bp, vf, &phy_qcfg_resp,\n\t\t\t\t\tphy_qcfg_req->resp_addr,\n\t\t\t\t\tphy_qcfg_req->cmpl_ring,\n\t\t\t\t\tsizeof(phy_qcfg_resp));\n\t}\n\treturn rc;\n}\n\nstatic int bnxt_vf_req_validate_snd(struct bnxt *bp, struct bnxt_vf_info *vf)\n{\n\tint rc = 0;\n\tstruct input *encap_req = vf->hwrm_cmd_req_addr;\n\tu32 req_type = le16_to_cpu(encap_req->req_type);\n\n\tswitch (req_type) {\n\tcase HWRM_FUNC_VF_CFG:\n\t\trc = bnxt_vf_configure_mac(bp, vf);\n\t\tbreak;\n\tcase HWRM_CFA_L2_FILTER_ALLOC:\n\t\trc = bnxt_vf_validate_set_mac(bp, vf);\n\t\tbreak;\n\tcase HWRM_FUNC_CFG:\n\t\t \n\t\trc = bnxt_hwrm_exec_fwd_resp(\n\t\t\tbp, vf, sizeof(struct hwrm_func_cfg_input));\n\t\tbreak;\n\tcase HWRM_PORT_PHY_QCFG:\n\t\trc = bnxt_vf_set_link(bp, vf);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn rc;\n}\n\nvoid bnxt_hwrm_exec_fwd_req(struct bnxt *bp)\n{\n\tu32 i = 0, active_vfs = bp->pf.active_vfs, vf_id;\n\n\t \n\twhile (1) {\n\t\tvf_id = find_next_bit(bp->pf.vf_event_bmap, active_vfs, i);\n\t\tif (vf_id >= active_vfs)\n\t\t\tbreak;\n\n\t\tclear_bit(vf_id, bp->pf.vf_event_bmap);\n\t\tbnxt_vf_req_validate_snd(bp, &bp->pf.vf[vf_id]);\n\t\ti = vf_id + 1;\n\t}\n}\n\nint bnxt_approve_mac(struct bnxt *bp, const u8 *mac, bool strict)\n{\n\tstruct hwrm_func_vf_cfg_input *req;\n\tint rc = 0;\n\n\tif (!BNXT_VF(bp))\n\t\treturn 0;\n\n\tif (bp->hwrm_spec_code < 0x10202) {\n\t\tif (is_valid_ether_addr(bp->vf.mac_addr))\n\t\t\trc = -EADDRNOTAVAIL;\n\t\tgoto mac_done;\n\t}\n\n\trc = hwrm_req_init(bp, req, HWRM_FUNC_VF_CFG);\n\tif (rc)\n\t\tgoto mac_done;\n\n\treq->enables = cpu_to_le32(FUNC_VF_CFG_REQ_ENABLES_DFLT_MAC_ADDR);\n\tmemcpy(req->dflt_mac_addr, mac, ETH_ALEN);\n\tif (!strict)\n\t\thwrm_req_flags(bp, req, BNXT_HWRM_CTX_SILENT);\n\trc = hwrm_req_send(bp, req);\nmac_done:\n\tif (rc && strict) {\n\t\trc = -EADDRNOTAVAIL;\n\t\tnetdev_warn(bp->dev, \"VF MAC address %pM not approved by the PF\\n\",\n\t\t\t    mac);\n\t\treturn rc;\n\t}\n\treturn 0;\n}\n\nvoid bnxt_update_vf_mac(struct bnxt *bp)\n{\n\tstruct hwrm_func_qcaps_output *resp;\n\tstruct hwrm_func_qcaps_input *req;\n\tbool inform_pf = false;\n\n\tif (hwrm_req_init(bp, req, HWRM_FUNC_QCAPS))\n\t\treturn;\n\n\treq->fid = cpu_to_le16(0xffff);\n\n\tresp = hwrm_req_hold(bp, req);\n\tif (hwrm_req_send(bp, req))\n\t\tgoto update_vf_mac_exit;\n\n\t \n\tif (!ether_addr_equal(resp->mac_address, bp->vf.mac_addr)) {\n\t\tmemcpy(bp->vf.mac_addr, resp->mac_address, ETH_ALEN);\n\t\t \n\t\tif (!is_valid_ether_addr(bp->vf.mac_addr))\n\t\t\tinform_pf = true;\n\t}\n\n\t \n\tif (is_valid_ether_addr(bp->vf.mac_addr))\n\t\teth_hw_addr_set(bp->dev, bp->vf.mac_addr);\nupdate_vf_mac_exit:\n\thwrm_req_drop(bp, req);\n\tif (inform_pf)\n\t\tbnxt_approve_mac(bp, bp->dev->dev_addr, false);\n}\n\n#else\n\nint bnxt_cfg_hw_sriov(struct bnxt *bp, int *num_vfs, bool reset)\n{\n\tif (*num_vfs)\n\t\treturn -EOPNOTSUPP;\n\treturn 0;\n}\n\nvoid bnxt_sriov_disable(struct bnxt *bp)\n{\n}\n\nvoid bnxt_hwrm_exec_fwd_req(struct bnxt *bp)\n{\n\tnetdev_err(bp->dev, \"Invalid VF message received when SRIOV is not enable\\n\");\n}\n\nvoid bnxt_update_vf_mac(struct bnxt *bp)\n{\n}\n\nint bnxt_approve_mac(struct bnxt *bp, const u8 *mac, bool strict)\n{\n\treturn 0;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}