{
  "module_name": "bcmgenet.c",
  "hash_id": "62695c729fbdab63a03dbfafd81014f369c34b551610529cc995a5643cb772dd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/broadcom/genet/bcmgenet.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\t\t\t\"bcmgenet: \" fmt\n\n#include <linux/acpi.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/interrupt.h>\n#include <linux/string.h>\n#include <linux/if_ether.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/delay.h>\n#include <linux/platform_device.h>\n#include <linux/dma-mapping.h>\n#include <linux/pm.h>\n#include <linux/clk.h>\n#include <net/arp.h>\n\n#include <linux/mii.h>\n#include <linux/ethtool.h>\n#include <linux/netdevice.h>\n#include <linux/inetdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/skbuff.h>\n#include <linux/in.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/phy.h>\n#include <linux/platform_data/bcmgenet.h>\n\n#include <asm/unaligned.h>\n\n#include \"bcmgenet.h\"\n\n \n#define GENET_MAX_MQ_CNT\t4\n\n \n#define GENET_Q0_PRIORITY\t0\n\n#define GENET_Q16_RX_BD_CNT\t\\\n\t(TOTAL_DESC - priv->hw_params->rx_queues * priv->hw_params->rx_bds_per_q)\n#define GENET_Q16_TX_BD_CNT\t\\\n\t(TOTAL_DESC - priv->hw_params->tx_queues * priv->hw_params->tx_bds_per_q)\n\n#define RX_BUF_LENGTH\t\t2048\n#define SKB_ALIGNMENT\t\t32\n\n \n#define WORDS_PER_BD(p)\t\t(p->hw_params->words_per_bd)\n#define DMA_DESC_SIZE\t\t(WORDS_PER_BD(priv) * sizeof(u32))\n\n#define GENET_TDMA_REG_OFF\t(priv->hw_params->tdma_offset + \\\n\t\t\t\tTOTAL_DESC * DMA_DESC_SIZE)\n\n#define GENET_RDMA_REG_OFF\t(priv->hw_params->rdma_offset + \\\n\t\t\t\tTOTAL_DESC * DMA_DESC_SIZE)\n\n \nstatic void bcmgenet_set_rx_mode(struct net_device *dev);\n\nstatic inline void bcmgenet_writel(u32 value, void __iomem *offset)\n{\n\t \n\tif (IS_ENABLED(CONFIG_MIPS) && IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))\n\t\t__raw_writel(value, offset);\n\telse\n\t\twritel_relaxed(value, offset);\n}\n\nstatic inline u32 bcmgenet_readl(void __iomem *offset)\n{\n\tif (IS_ENABLED(CONFIG_MIPS) && IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))\n\t\treturn __raw_readl(offset);\n\telse\n\t\treturn readl_relaxed(offset);\n}\n\nstatic inline void dmadesc_set_length_status(struct bcmgenet_priv *priv,\n\t\t\t\t\t     void __iomem *d, u32 value)\n{\n\tbcmgenet_writel(value, d + DMA_DESC_LENGTH_STATUS);\n}\n\nstatic inline void dmadesc_set_addr(struct bcmgenet_priv *priv,\n\t\t\t\t    void __iomem *d,\n\t\t\t\t    dma_addr_t addr)\n{\n\tbcmgenet_writel(lower_32_bits(addr), d + DMA_DESC_ADDRESS_LO);\n\n\t \n#ifdef CONFIG_PHYS_ADDR_T_64BIT\n\tif (priv->hw_params->flags & GENET_HAS_40BITS)\n\t\tbcmgenet_writel(upper_32_bits(addr), d + DMA_DESC_ADDRESS_HI);\n#endif\n}\n\n \nstatic inline void dmadesc_set(struct bcmgenet_priv *priv,\n\t\t\t       void __iomem *d, dma_addr_t addr, u32 val)\n{\n\tdmadesc_set_addr(priv, d, addr);\n\tdmadesc_set_length_status(priv, d, val);\n}\n\n#define GENET_VER_FMT\t\"%1d.%1d EPHY: 0x%04x\"\n\n#define GENET_MSG_DEFAULT\t(NETIF_MSG_DRV | NETIF_MSG_PROBE | \\\n\t\t\t\tNETIF_MSG_LINK)\n\nstatic inline u32 bcmgenet_rbuf_ctrl_get(struct bcmgenet_priv *priv)\n{\n\tif (GENET_IS_V1(priv))\n\t\treturn bcmgenet_rbuf_readl(priv, RBUF_FLUSH_CTRL_V1);\n\telse\n\t\treturn bcmgenet_sys_readl(priv, SYS_RBUF_FLUSH_CTRL);\n}\n\nstatic inline void bcmgenet_rbuf_ctrl_set(struct bcmgenet_priv *priv, u32 val)\n{\n\tif (GENET_IS_V1(priv))\n\t\tbcmgenet_rbuf_writel(priv, val, RBUF_FLUSH_CTRL_V1);\n\telse\n\t\tbcmgenet_sys_writel(priv, val, SYS_RBUF_FLUSH_CTRL);\n}\n\n \nstatic inline u32 bcmgenet_tbuf_ctrl_get(struct bcmgenet_priv *priv)\n{\n\tif (GENET_IS_V1(priv))\n\t\treturn bcmgenet_rbuf_readl(priv, TBUF_CTRL_V1);\n\telse\n\t\treturn bcmgenet_readl(priv->base +\n\t\t\t\t      priv->hw_params->tbuf_offset + TBUF_CTRL);\n}\n\nstatic inline void bcmgenet_tbuf_ctrl_set(struct bcmgenet_priv *priv, u32 val)\n{\n\tif (GENET_IS_V1(priv))\n\t\tbcmgenet_rbuf_writel(priv, val, TBUF_CTRL_V1);\n\telse\n\t\tbcmgenet_writel(val, priv->base +\n\t\t\t\tpriv->hw_params->tbuf_offset + TBUF_CTRL);\n}\n\nstatic inline u32 bcmgenet_bp_mc_get(struct bcmgenet_priv *priv)\n{\n\tif (GENET_IS_V1(priv))\n\t\treturn bcmgenet_rbuf_readl(priv, TBUF_BP_MC_V1);\n\telse\n\t\treturn bcmgenet_readl(priv->base +\n\t\t\t\t      priv->hw_params->tbuf_offset + TBUF_BP_MC);\n}\n\nstatic inline void bcmgenet_bp_mc_set(struct bcmgenet_priv *priv, u32 val)\n{\n\tif (GENET_IS_V1(priv))\n\t\tbcmgenet_rbuf_writel(priv, val, TBUF_BP_MC_V1);\n\telse\n\t\tbcmgenet_writel(val, priv->base +\n\t\t\t\tpriv->hw_params->tbuf_offset + TBUF_BP_MC);\n}\n\n \nenum dma_reg {\n\tDMA_RING_CFG = 0,\n\tDMA_CTRL,\n\tDMA_STATUS,\n\tDMA_SCB_BURST_SIZE,\n\tDMA_ARB_CTRL,\n\tDMA_PRIORITY_0,\n\tDMA_PRIORITY_1,\n\tDMA_PRIORITY_2,\n\tDMA_INDEX2RING_0,\n\tDMA_INDEX2RING_1,\n\tDMA_INDEX2RING_2,\n\tDMA_INDEX2RING_3,\n\tDMA_INDEX2RING_4,\n\tDMA_INDEX2RING_5,\n\tDMA_INDEX2RING_6,\n\tDMA_INDEX2RING_7,\n\tDMA_RING0_TIMEOUT,\n\tDMA_RING1_TIMEOUT,\n\tDMA_RING2_TIMEOUT,\n\tDMA_RING3_TIMEOUT,\n\tDMA_RING4_TIMEOUT,\n\tDMA_RING5_TIMEOUT,\n\tDMA_RING6_TIMEOUT,\n\tDMA_RING7_TIMEOUT,\n\tDMA_RING8_TIMEOUT,\n\tDMA_RING9_TIMEOUT,\n\tDMA_RING10_TIMEOUT,\n\tDMA_RING11_TIMEOUT,\n\tDMA_RING12_TIMEOUT,\n\tDMA_RING13_TIMEOUT,\n\tDMA_RING14_TIMEOUT,\n\tDMA_RING15_TIMEOUT,\n\tDMA_RING16_TIMEOUT,\n};\n\nstatic const u8 bcmgenet_dma_regs_v3plus[] = {\n\t[DMA_RING_CFG]\t\t= 0x00,\n\t[DMA_CTRL]\t\t= 0x04,\n\t[DMA_STATUS]\t\t= 0x08,\n\t[DMA_SCB_BURST_SIZE]\t= 0x0C,\n\t[DMA_ARB_CTRL]\t\t= 0x2C,\n\t[DMA_PRIORITY_0]\t= 0x30,\n\t[DMA_PRIORITY_1]\t= 0x34,\n\t[DMA_PRIORITY_2]\t= 0x38,\n\t[DMA_RING0_TIMEOUT]\t= 0x2C,\n\t[DMA_RING1_TIMEOUT]\t= 0x30,\n\t[DMA_RING2_TIMEOUT]\t= 0x34,\n\t[DMA_RING3_TIMEOUT]\t= 0x38,\n\t[DMA_RING4_TIMEOUT]\t= 0x3c,\n\t[DMA_RING5_TIMEOUT]\t= 0x40,\n\t[DMA_RING6_TIMEOUT]\t= 0x44,\n\t[DMA_RING7_TIMEOUT]\t= 0x48,\n\t[DMA_RING8_TIMEOUT]\t= 0x4c,\n\t[DMA_RING9_TIMEOUT]\t= 0x50,\n\t[DMA_RING10_TIMEOUT]\t= 0x54,\n\t[DMA_RING11_TIMEOUT]\t= 0x58,\n\t[DMA_RING12_TIMEOUT]\t= 0x5c,\n\t[DMA_RING13_TIMEOUT]\t= 0x60,\n\t[DMA_RING14_TIMEOUT]\t= 0x64,\n\t[DMA_RING15_TIMEOUT]\t= 0x68,\n\t[DMA_RING16_TIMEOUT]\t= 0x6C,\n\t[DMA_INDEX2RING_0]\t= 0x70,\n\t[DMA_INDEX2RING_1]\t= 0x74,\n\t[DMA_INDEX2RING_2]\t= 0x78,\n\t[DMA_INDEX2RING_3]\t= 0x7C,\n\t[DMA_INDEX2RING_4]\t= 0x80,\n\t[DMA_INDEX2RING_5]\t= 0x84,\n\t[DMA_INDEX2RING_6]\t= 0x88,\n\t[DMA_INDEX2RING_7]\t= 0x8C,\n};\n\nstatic const u8 bcmgenet_dma_regs_v2[] = {\n\t[DMA_RING_CFG]\t\t= 0x00,\n\t[DMA_CTRL]\t\t= 0x04,\n\t[DMA_STATUS]\t\t= 0x08,\n\t[DMA_SCB_BURST_SIZE]\t= 0x0C,\n\t[DMA_ARB_CTRL]\t\t= 0x30,\n\t[DMA_PRIORITY_0]\t= 0x34,\n\t[DMA_PRIORITY_1]\t= 0x38,\n\t[DMA_PRIORITY_2]\t= 0x3C,\n\t[DMA_RING0_TIMEOUT]\t= 0x2C,\n\t[DMA_RING1_TIMEOUT]\t= 0x30,\n\t[DMA_RING2_TIMEOUT]\t= 0x34,\n\t[DMA_RING3_TIMEOUT]\t= 0x38,\n\t[DMA_RING4_TIMEOUT]\t= 0x3c,\n\t[DMA_RING5_TIMEOUT]\t= 0x40,\n\t[DMA_RING6_TIMEOUT]\t= 0x44,\n\t[DMA_RING7_TIMEOUT]\t= 0x48,\n\t[DMA_RING8_TIMEOUT]\t= 0x4c,\n\t[DMA_RING9_TIMEOUT]\t= 0x50,\n\t[DMA_RING10_TIMEOUT]\t= 0x54,\n\t[DMA_RING11_TIMEOUT]\t= 0x58,\n\t[DMA_RING12_TIMEOUT]\t= 0x5c,\n\t[DMA_RING13_TIMEOUT]\t= 0x60,\n\t[DMA_RING14_TIMEOUT]\t= 0x64,\n\t[DMA_RING15_TIMEOUT]\t= 0x68,\n\t[DMA_RING16_TIMEOUT]\t= 0x6C,\n};\n\nstatic const u8 bcmgenet_dma_regs_v1[] = {\n\t[DMA_CTRL]\t\t= 0x00,\n\t[DMA_STATUS]\t\t= 0x04,\n\t[DMA_SCB_BURST_SIZE]\t= 0x0C,\n\t[DMA_ARB_CTRL]\t\t= 0x30,\n\t[DMA_PRIORITY_0]\t= 0x34,\n\t[DMA_PRIORITY_1]\t= 0x38,\n\t[DMA_PRIORITY_2]\t= 0x3C,\n\t[DMA_RING0_TIMEOUT]\t= 0x2C,\n\t[DMA_RING1_TIMEOUT]\t= 0x30,\n\t[DMA_RING2_TIMEOUT]\t= 0x34,\n\t[DMA_RING3_TIMEOUT]\t= 0x38,\n\t[DMA_RING4_TIMEOUT]\t= 0x3c,\n\t[DMA_RING5_TIMEOUT]\t= 0x40,\n\t[DMA_RING6_TIMEOUT]\t= 0x44,\n\t[DMA_RING7_TIMEOUT]\t= 0x48,\n\t[DMA_RING8_TIMEOUT]\t= 0x4c,\n\t[DMA_RING9_TIMEOUT]\t= 0x50,\n\t[DMA_RING10_TIMEOUT]\t= 0x54,\n\t[DMA_RING11_TIMEOUT]\t= 0x58,\n\t[DMA_RING12_TIMEOUT]\t= 0x5c,\n\t[DMA_RING13_TIMEOUT]\t= 0x60,\n\t[DMA_RING14_TIMEOUT]\t= 0x64,\n\t[DMA_RING15_TIMEOUT]\t= 0x68,\n\t[DMA_RING16_TIMEOUT]\t= 0x6C,\n};\n\n \nstatic const u8 *bcmgenet_dma_regs;\n\nstatic inline struct bcmgenet_priv *dev_to_priv(struct device *dev)\n{\n\treturn netdev_priv(dev_get_drvdata(dev));\n}\n\nstatic inline u32 bcmgenet_tdma_readl(struct bcmgenet_priv *priv,\n\t\t\t\t      enum dma_reg r)\n{\n\treturn bcmgenet_readl(priv->base + GENET_TDMA_REG_OFF +\n\t\t\t      DMA_RINGS_SIZE + bcmgenet_dma_regs[r]);\n}\n\nstatic inline void bcmgenet_tdma_writel(struct bcmgenet_priv *priv,\n\t\t\t\t\tu32 val, enum dma_reg r)\n{\n\tbcmgenet_writel(val, priv->base + GENET_TDMA_REG_OFF +\n\t\t\tDMA_RINGS_SIZE + bcmgenet_dma_regs[r]);\n}\n\nstatic inline u32 bcmgenet_rdma_readl(struct bcmgenet_priv *priv,\n\t\t\t\t      enum dma_reg r)\n{\n\treturn bcmgenet_readl(priv->base + GENET_RDMA_REG_OFF +\n\t\t\t      DMA_RINGS_SIZE + bcmgenet_dma_regs[r]);\n}\n\nstatic inline void bcmgenet_rdma_writel(struct bcmgenet_priv *priv,\n\t\t\t\t\tu32 val, enum dma_reg r)\n{\n\tbcmgenet_writel(val, priv->base + GENET_RDMA_REG_OFF +\n\t\t\tDMA_RINGS_SIZE + bcmgenet_dma_regs[r]);\n}\n\n \nenum dma_ring_reg {\n\tTDMA_READ_PTR = 0,\n\tRDMA_WRITE_PTR = TDMA_READ_PTR,\n\tTDMA_READ_PTR_HI,\n\tRDMA_WRITE_PTR_HI = TDMA_READ_PTR_HI,\n\tTDMA_CONS_INDEX,\n\tRDMA_PROD_INDEX = TDMA_CONS_INDEX,\n\tTDMA_PROD_INDEX,\n\tRDMA_CONS_INDEX = TDMA_PROD_INDEX,\n\tDMA_RING_BUF_SIZE,\n\tDMA_START_ADDR,\n\tDMA_START_ADDR_HI,\n\tDMA_END_ADDR,\n\tDMA_END_ADDR_HI,\n\tDMA_MBUF_DONE_THRESH,\n\tTDMA_FLOW_PERIOD,\n\tRDMA_XON_XOFF_THRESH = TDMA_FLOW_PERIOD,\n\tTDMA_WRITE_PTR,\n\tRDMA_READ_PTR = TDMA_WRITE_PTR,\n\tTDMA_WRITE_PTR_HI,\n\tRDMA_READ_PTR_HI = TDMA_WRITE_PTR_HI\n};\n\n \nstatic const u8 genet_dma_ring_regs_v4[] = {\n\t[TDMA_READ_PTR]\t\t\t= 0x00,\n\t[TDMA_READ_PTR_HI]\t\t= 0x04,\n\t[TDMA_CONS_INDEX]\t\t= 0x08,\n\t[TDMA_PROD_INDEX]\t\t= 0x0C,\n\t[DMA_RING_BUF_SIZE]\t\t= 0x10,\n\t[DMA_START_ADDR]\t\t= 0x14,\n\t[DMA_START_ADDR_HI]\t\t= 0x18,\n\t[DMA_END_ADDR]\t\t\t= 0x1C,\n\t[DMA_END_ADDR_HI]\t\t= 0x20,\n\t[DMA_MBUF_DONE_THRESH]\t\t= 0x24,\n\t[TDMA_FLOW_PERIOD]\t\t= 0x28,\n\t[TDMA_WRITE_PTR]\t\t= 0x2C,\n\t[TDMA_WRITE_PTR_HI]\t\t= 0x30,\n};\n\nstatic const u8 genet_dma_ring_regs_v123[] = {\n\t[TDMA_READ_PTR]\t\t\t= 0x00,\n\t[TDMA_CONS_INDEX]\t\t= 0x04,\n\t[TDMA_PROD_INDEX]\t\t= 0x08,\n\t[DMA_RING_BUF_SIZE]\t\t= 0x0C,\n\t[DMA_START_ADDR]\t\t= 0x10,\n\t[DMA_END_ADDR]\t\t\t= 0x14,\n\t[DMA_MBUF_DONE_THRESH]\t\t= 0x18,\n\t[TDMA_FLOW_PERIOD]\t\t= 0x1C,\n\t[TDMA_WRITE_PTR]\t\t= 0x20,\n};\n\n \nstatic const u8 *genet_dma_ring_regs;\n\nstatic inline u32 bcmgenet_tdma_ring_readl(struct bcmgenet_priv *priv,\n\t\t\t\t\t   unsigned int ring,\n\t\t\t\t\t   enum dma_ring_reg r)\n{\n\treturn bcmgenet_readl(priv->base + GENET_TDMA_REG_OFF +\n\t\t\t      (DMA_RING_SIZE * ring) +\n\t\t\t      genet_dma_ring_regs[r]);\n}\n\nstatic inline void bcmgenet_tdma_ring_writel(struct bcmgenet_priv *priv,\n\t\t\t\t\t     unsigned int ring, u32 val,\n\t\t\t\t\t     enum dma_ring_reg r)\n{\n\tbcmgenet_writel(val, priv->base + GENET_TDMA_REG_OFF +\n\t\t\t(DMA_RING_SIZE * ring) +\n\t\t\tgenet_dma_ring_regs[r]);\n}\n\nstatic inline u32 bcmgenet_rdma_ring_readl(struct bcmgenet_priv *priv,\n\t\t\t\t\t   unsigned int ring,\n\t\t\t\t\t   enum dma_ring_reg r)\n{\n\treturn bcmgenet_readl(priv->base + GENET_RDMA_REG_OFF +\n\t\t\t      (DMA_RING_SIZE * ring) +\n\t\t\t      genet_dma_ring_regs[r]);\n}\n\nstatic inline void bcmgenet_rdma_ring_writel(struct bcmgenet_priv *priv,\n\t\t\t\t\t     unsigned int ring, u32 val,\n\t\t\t\t\t     enum dma_ring_reg r)\n{\n\tbcmgenet_writel(val, priv->base + GENET_RDMA_REG_OFF +\n\t\t\t(DMA_RING_SIZE * ring) +\n\t\t\tgenet_dma_ring_regs[r]);\n}\n\nstatic void bcmgenet_hfb_enable_filter(struct bcmgenet_priv *priv, u32 f_index)\n{\n\tu32 offset;\n\tu32 reg;\n\n\toffset = HFB_FLT_ENABLE_V3PLUS + (f_index < 32) * sizeof(u32);\n\treg = bcmgenet_hfb_reg_readl(priv, offset);\n\treg |= (1 << (f_index % 32));\n\tbcmgenet_hfb_reg_writel(priv, reg, offset);\n\treg = bcmgenet_hfb_reg_readl(priv, HFB_CTRL);\n\treg |= RBUF_HFB_EN;\n\tbcmgenet_hfb_reg_writel(priv, reg, HFB_CTRL);\n}\n\nstatic void bcmgenet_hfb_disable_filter(struct bcmgenet_priv *priv, u32 f_index)\n{\n\tu32 offset, reg, reg1;\n\n\toffset = HFB_FLT_ENABLE_V3PLUS;\n\treg = bcmgenet_hfb_reg_readl(priv, offset);\n\treg1 = bcmgenet_hfb_reg_readl(priv, offset + sizeof(u32));\n\tif  (f_index < 32) {\n\t\treg1 &= ~(1 << (f_index % 32));\n\t\tbcmgenet_hfb_reg_writel(priv, reg1, offset + sizeof(u32));\n\t} else {\n\t\treg &= ~(1 << (f_index % 32));\n\t\tbcmgenet_hfb_reg_writel(priv, reg, offset);\n\t}\n\tif (!reg && !reg1) {\n\t\treg = bcmgenet_hfb_reg_readl(priv, HFB_CTRL);\n\t\treg &= ~RBUF_HFB_EN;\n\t\tbcmgenet_hfb_reg_writel(priv, reg, HFB_CTRL);\n\t}\n}\n\nstatic void bcmgenet_hfb_set_filter_rx_queue_mapping(struct bcmgenet_priv *priv,\n\t\t\t\t\t\t     u32 f_index, u32 rx_queue)\n{\n\tu32 offset;\n\tu32 reg;\n\n\toffset = f_index / 8;\n\treg = bcmgenet_rdma_readl(priv, DMA_INDEX2RING_0 + offset);\n\treg &= ~(0xF << (4 * (f_index % 8)));\n\treg |= ((rx_queue & 0xF) << (4 * (f_index % 8)));\n\tbcmgenet_rdma_writel(priv, reg, DMA_INDEX2RING_0 + offset);\n}\n\nstatic void bcmgenet_hfb_set_filter_length(struct bcmgenet_priv *priv,\n\t\t\t\t\t   u32 f_index, u32 f_length)\n{\n\tu32 offset;\n\tu32 reg;\n\n\toffset = HFB_FLT_LEN_V3PLUS +\n\t\t ((priv->hw_params->hfb_filter_cnt - 1 - f_index) / 4) *\n\t\t sizeof(u32);\n\treg = bcmgenet_hfb_reg_readl(priv, offset);\n\treg &= ~(0xFF << (8 * (f_index % 4)));\n\treg |= ((f_length & 0xFF) << (8 * (f_index % 4)));\n\tbcmgenet_hfb_reg_writel(priv, reg, offset);\n}\n\nstatic int bcmgenet_hfb_validate_mask(void *mask, size_t size)\n{\n\twhile (size) {\n\t\tswitch (*(unsigned char *)mask++) {\n\t\tcase 0x00:\n\t\tcase 0x0f:\n\t\tcase 0xf0:\n\t\tcase 0xff:\n\t\t\tsize--;\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#define VALIDATE_MASK(x) \\\n\tbcmgenet_hfb_validate_mask(&(x), sizeof(x))\n\nstatic int bcmgenet_hfb_insert_data(struct bcmgenet_priv *priv, u32 f_index,\n\t\t\t\t    u32 offset, void *val, void *mask,\n\t\t\t\t    size_t size)\n{\n\tu32 index, tmp;\n\n\tindex = f_index * priv->hw_params->hfb_filter_size + offset / 2;\n\ttmp = bcmgenet_hfb_readl(priv, index * sizeof(u32));\n\n\twhile (size--) {\n\t\tif (offset++ & 1) {\n\t\t\ttmp &= ~0x300FF;\n\t\t\ttmp |= (*(unsigned char *)val++);\n\t\t\tswitch ((*(unsigned char *)mask++)) {\n\t\t\tcase 0xFF:\n\t\t\t\ttmp |= 0x30000;\n\t\t\t\tbreak;\n\t\t\tcase 0xF0:\n\t\t\t\ttmp |= 0x20000;\n\t\t\t\tbreak;\n\t\t\tcase 0x0F:\n\t\t\t\ttmp |= 0x10000;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbcmgenet_hfb_writel(priv, tmp, index++ * sizeof(u32));\n\t\t\tif (size)\n\t\t\t\ttmp = bcmgenet_hfb_readl(priv,\n\t\t\t\t\t\t\t index * sizeof(u32));\n\t\t} else {\n\t\t\ttmp &= ~0xCFF00;\n\t\t\ttmp |= (*(unsigned char *)val++) << 8;\n\t\t\tswitch ((*(unsigned char *)mask++)) {\n\t\t\tcase 0xFF:\n\t\t\t\ttmp |= 0xC0000;\n\t\t\t\tbreak;\n\t\t\tcase 0xF0:\n\t\t\t\ttmp |= 0x80000;\n\t\t\t\tbreak;\n\t\t\tcase 0x0F:\n\t\t\t\ttmp |= 0x40000;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!size)\n\t\t\t\tbcmgenet_hfb_writel(priv, tmp, index * sizeof(u32));\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void bcmgenet_hfb_create_rxnfc_filter(struct bcmgenet_priv *priv,\n\t\t\t\t\t     struct bcmgenet_rxnfc_rule *rule)\n{\n\tstruct ethtool_rx_flow_spec *fs = &rule->fs;\n\tu32 offset = 0, f_length = 0, f;\n\tu8 val_8, mask_8;\n\t__be16 val_16;\n\tu16 mask_16;\n\tsize_t size;\n\n\tf = fs->location;\n\tif (fs->flow_type & FLOW_MAC_EXT) {\n\t\tbcmgenet_hfb_insert_data(priv, f, 0,\n\t\t\t\t\t &fs->h_ext.h_dest, &fs->m_ext.h_dest,\n\t\t\t\t\t sizeof(fs->h_ext.h_dest));\n\t}\n\n\tif (fs->flow_type & FLOW_EXT) {\n\t\tif (fs->m_ext.vlan_etype ||\n\t\t    fs->m_ext.vlan_tci) {\n\t\t\tbcmgenet_hfb_insert_data(priv, f, 12,\n\t\t\t\t\t\t &fs->h_ext.vlan_etype,\n\t\t\t\t\t\t &fs->m_ext.vlan_etype,\n\t\t\t\t\t\t sizeof(fs->h_ext.vlan_etype));\n\t\t\tbcmgenet_hfb_insert_data(priv, f, 14,\n\t\t\t\t\t\t &fs->h_ext.vlan_tci,\n\t\t\t\t\t\t &fs->m_ext.vlan_tci,\n\t\t\t\t\t\t sizeof(fs->h_ext.vlan_tci));\n\t\t\toffset += VLAN_HLEN;\n\t\t\tf_length += DIV_ROUND_UP(VLAN_HLEN, 2);\n\t\t}\n\t}\n\n\tswitch (fs->flow_type & ~(FLOW_EXT | FLOW_MAC_EXT)) {\n\tcase ETHER_FLOW:\n\t\tf_length += DIV_ROUND_UP(ETH_HLEN, 2);\n\t\tbcmgenet_hfb_insert_data(priv, f, 0,\n\t\t\t\t\t &fs->h_u.ether_spec.h_dest,\n\t\t\t\t\t &fs->m_u.ether_spec.h_dest,\n\t\t\t\t\t sizeof(fs->h_u.ether_spec.h_dest));\n\t\tbcmgenet_hfb_insert_data(priv, f, ETH_ALEN,\n\t\t\t\t\t &fs->h_u.ether_spec.h_source,\n\t\t\t\t\t &fs->m_u.ether_spec.h_source,\n\t\t\t\t\t sizeof(fs->h_u.ether_spec.h_source));\n\t\tbcmgenet_hfb_insert_data(priv, f, (2 * ETH_ALEN) + offset,\n\t\t\t\t\t &fs->h_u.ether_spec.h_proto,\n\t\t\t\t\t &fs->m_u.ether_spec.h_proto,\n\t\t\t\t\t sizeof(fs->h_u.ether_spec.h_proto));\n\t\tbreak;\n\tcase IP_USER_FLOW:\n\t\tf_length += DIV_ROUND_UP(ETH_HLEN + 20, 2);\n\t\t \n\t\tval_16 = htons(ETH_P_IP);\n\t\tmask_16 = 0xFFFF;\n\t\tbcmgenet_hfb_insert_data(priv, f, (2 * ETH_ALEN) + offset,\n\t\t\t\t\t &val_16, &mask_16, sizeof(val_16));\n\t\tbcmgenet_hfb_insert_data(priv, f, 15 + offset,\n\t\t\t\t\t &fs->h_u.usr_ip4_spec.tos,\n\t\t\t\t\t &fs->m_u.usr_ip4_spec.tos,\n\t\t\t\t\t sizeof(fs->h_u.usr_ip4_spec.tos));\n\t\tbcmgenet_hfb_insert_data(priv, f, 23 + offset,\n\t\t\t\t\t &fs->h_u.usr_ip4_spec.proto,\n\t\t\t\t\t &fs->m_u.usr_ip4_spec.proto,\n\t\t\t\t\t sizeof(fs->h_u.usr_ip4_spec.proto));\n\t\tbcmgenet_hfb_insert_data(priv, f, 26 + offset,\n\t\t\t\t\t &fs->h_u.usr_ip4_spec.ip4src,\n\t\t\t\t\t &fs->m_u.usr_ip4_spec.ip4src,\n\t\t\t\t\t sizeof(fs->h_u.usr_ip4_spec.ip4src));\n\t\tbcmgenet_hfb_insert_data(priv, f, 30 + offset,\n\t\t\t\t\t &fs->h_u.usr_ip4_spec.ip4dst,\n\t\t\t\t\t &fs->m_u.usr_ip4_spec.ip4dst,\n\t\t\t\t\t sizeof(fs->h_u.usr_ip4_spec.ip4dst));\n\t\tif (!fs->m_u.usr_ip4_spec.l4_4_bytes)\n\t\t\tbreak;\n\n\t\t \n\t\tval_8 = 0x45;\n\t\tmask_8 = 0xFF;\n\t\tbcmgenet_hfb_insert_data(priv, f, ETH_HLEN + offset,\n\t\t\t\t\t &val_8, &mask_8,\n\t\t\t\t\t sizeof(val_8));\n\t\tsize = sizeof(fs->h_u.usr_ip4_spec.l4_4_bytes);\n\t\tbcmgenet_hfb_insert_data(priv, f,\n\t\t\t\t\t ETH_HLEN + 20 + offset,\n\t\t\t\t\t &fs->h_u.usr_ip4_spec.l4_4_bytes,\n\t\t\t\t\t &fs->m_u.usr_ip4_spec.l4_4_bytes,\n\t\t\t\t\t size);\n\t\tf_length += DIV_ROUND_UP(size, 2);\n\t\tbreak;\n\t}\n\n\tbcmgenet_hfb_set_filter_length(priv, f, 2 * f_length);\n\tif (!fs->ring_cookie || fs->ring_cookie == RX_CLS_FLOW_WAKE) {\n\t\t \n\t\tbcmgenet_hfb_set_filter_rx_queue_mapping(priv, f, 0);\n\t\trule->state = BCMGENET_RXNFC_STATE_DISABLED;\n\t} else {\n\t\t \n\t\tbcmgenet_hfb_set_filter_rx_queue_mapping(priv, f,\n\t\t\t\t\t\t\t fs->ring_cookie);\n\t\tbcmgenet_hfb_enable_filter(priv, f);\n\t\trule->state = BCMGENET_RXNFC_STATE_ENABLED;\n\t}\n}\n\n \nstatic void bcmgenet_hfb_clear_filter(struct bcmgenet_priv *priv, u32 f_index)\n{\n\tu32 base, i;\n\n\tbase = f_index * priv->hw_params->hfb_filter_size;\n\tfor (i = 0; i < priv->hw_params->hfb_filter_size; i++)\n\t\tbcmgenet_hfb_writel(priv, 0x0, (base + i) * sizeof(u32));\n}\n\nstatic void bcmgenet_hfb_clear(struct bcmgenet_priv *priv)\n{\n\tu32 i;\n\n\tif (GENET_IS_V1(priv) || GENET_IS_V2(priv))\n\t\treturn;\n\n\tbcmgenet_hfb_reg_writel(priv, 0x0, HFB_CTRL);\n\tbcmgenet_hfb_reg_writel(priv, 0x0, HFB_FLT_ENABLE_V3PLUS);\n\tbcmgenet_hfb_reg_writel(priv, 0x0, HFB_FLT_ENABLE_V3PLUS + 4);\n\n\tfor (i = DMA_INDEX2RING_0; i <= DMA_INDEX2RING_7; i++)\n\t\tbcmgenet_rdma_writel(priv, 0x0, i);\n\n\tfor (i = 0; i < (priv->hw_params->hfb_filter_cnt / 4); i++)\n\t\tbcmgenet_hfb_reg_writel(priv, 0x0,\n\t\t\t\t\tHFB_FLT_LEN_V3PLUS + i * sizeof(u32));\n\n\tfor (i = 0; i < priv->hw_params->hfb_filter_cnt; i++)\n\t\tbcmgenet_hfb_clear_filter(priv, i);\n}\n\nstatic void bcmgenet_hfb_init(struct bcmgenet_priv *priv)\n{\n\tint i;\n\n\tINIT_LIST_HEAD(&priv->rxnfc_list);\n\tif (GENET_IS_V1(priv) || GENET_IS_V2(priv))\n\t\treturn;\n\n\tfor (i = 0; i < MAX_NUM_OF_FS_RULES; i++) {\n\t\tINIT_LIST_HEAD(&priv->rxnfc_rules[i].list);\n\t\tpriv->rxnfc_rules[i].state = BCMGENET_RXNFC_STATE_UNUSED;\n\t}\n\n\tbcmgenet_hfb_clear(priv);\n}\n\nstatic int bcmgenet_begin(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\t \n\treturn clk_prepare_enable(priv->clk);\n}\n\nstatic void bcmgenet_complete(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\t \n\tclk_disable_unprepare(priv->clk);\n}\n\nstatic int bcmgenet_get_link_ksettings(struct net_device *dev,\n\t\t\t\t       struct ethtool_link_ksettings *cmd)\n{\n\tif (!netif_running(dev))\n\t\treturn -EINVAL;\n\n\tif (!dev->phydev)\n\t\treturn -ENODEV;\n\n\tphy_ethtool_ksettings_get(dev->phydev, cmd);\n\n\treturn 0;\n}\n\nstatic int bcmgenet_set_link_ksettings(struct net_device *dev,\n\t\t\t\t       const struct ethtool_link_ksettings *cmd)\n{\n\tif (!netif_running(dev))\n\t\treturn -EINVAL;\n\n\tif (!dev->phydev)\n\t\treturn -ENODEV;\n\n\treturn phy_ethtool_ksettings_set(dev->phydev, cmd);\n}\n\nstatic int bcmgenet_set_features(struct net_device *dev,\n\t\t\t\t netdev_features_t features)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tu32 reg;\n\tint ret;\n\n\tret = clk_prepare_enable(priv->clk);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treg = bcmgenet_umac_readl(priv, UMAC_CMD);\n\tpriv->crc_fwd_en = !!(reg & CMD_CRC_FWD);\n\n\tclk_disable_unprepare(priv->clk);\n\n\treturn ret;\n}\n\nstatic u32 bcmgenet_get_msglevel(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\treturn priv->msg_enable;\n}\n\nstatic void bcmgenet_set_msglevel(struct net_device *dev, u32 level)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\tpriv->msg_enable = level;\n}\n\nstatic int bcmgenet_get_coalesce(struct net_device *dev,\n\t\t\t\t struct ethtool_coalesce *ec,\n\t\t\t\t struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct bcmgenet_rx_ring *ring;\n\tunsigned int i;\n\n\tec->tx_max_coalesced_frames =\n\t\tbcmgenet_tdma_ring_readl(priv, DESC_INDEX,\n\t\t\t\t\t DMA_MBUF_DONE_THRESH);\n\tec->rx_max_coalesced_frames =\n\t\tbcmgenet_rdma_ring_readl(priv, DESC_INDEX,\n\t\t\t\t\t DMA_MBUF_DONE_THRESH);\n\tec->rx_coalesce_usecs =\n\t\tbcmgenet_rdma_readl(priv, DMA_RING16_TIMEOUT) * 8192 / 1000;\n\n\tfor (i = 0; i < priv->hw_params->rx_queues; i++) {\n\t\tring = &priv->rx_rings[i];\n\t\tec->use_adaptive_rx_coalesce |= ring->dim.use_dim;\n\t}\n\tring = &priv->rx_rings[DESC_INDEX];\n\tec->use_adaptive_rx_coalesce |= ring->dim.use_dim;\n\n\treturn 0;\n}\n\nstatic void bcmgenet_set_rx_coalesce(struct bcmgenet_rx_ring *ring,\n\t\t\t\t     u32 usecs, u32 pkts)\n{\n\tstruct bcmgenet_priv *priv = ring->priv;\n\tunsigned int i = ring->index;\n\tu32 reg;\n\n\tbcmgenet_rdma_ring_writel(priv, i, pkts, DMA_MBUF_DONE_THRESH);\n\n\treg = bcmgenet_rdma_readl(priv, DMA_RING0_TIMEOUT + i);\n\treg &= ~DMA_TIMEOUT_MASK;\n\treg |= DIV_ROUND_UP(usecs * 1000, 8192);\n\tbcmgenet_rdma_writel(priv, reg, DMA_RING0_TIMEOUT + i);\n}\n\nstatic void bcmgenet_set_ring_rx_coalesce(struct bcmgenet_rx_ring *ring,\n\t\t\t\t\t  struct ethtool_coalesce *ec)\n{\n\tstruct dim_cq_moder moder;\n\tu32 usecs, pkts;\n\n\tring->rx_coalesce_usecs = ec->rx_coalesce_usecs;\n\tring->rx_max_coalesced_frames = ec->rx_max_coalesced_frames;\n\tusecs = ring->rx_coalesce_usecs;\n\tpkts = ring->rx_max_coalesced_frames;\n\n\tif (ec->use_adaptive_rx_coalesce && !ring->dim.use_dim) {\n\t\tmoder = net_dim_get_def_rx_moderation(ring->dim.dim.mode);\n\t\tusecs = moder.usec;\n\t\tpkts = moder.pkts;\n\t}\n\n\tring->dim.use_dim = ec->use_adaptive_rx_coalesce;\n\tbcmgenet_set_rx_coalesce(ring, usecs, pkts);\n}\n\nstatic int bcmgenet_set_coalesce(struct net_device *dev,\n\t\t\t\t struct ethtool_coalesce *ec,\n\t\t\t\t struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tunsigned int i;\n\n\t \n\tif (ec->tx_max_coalesced_frames > DMA_INTR_THRESHOLD_MASK ||\n\t    ec->tx_max_coalesced_frames == 0 ||\n\t    ec->rx_max_coalesced_frames > DMA_INTR_THRESHOLD_MASK ||\n\t    ec->rx_coalesce_usecs > (DMA_TIMEOUT_MASK * 8) + 1)\n\t\treturn -EINVAL;\n\n\tif (ec->rx_coalesce_usecs == 0 && ec->rx_max_coalesced_frames == 0)\n\t\treturn -EINVAL;\n\n\t \n\n\t \n\tfor (i = 0; i < priv->hw_params->tx_queues; i++)\n\t\tbcmgenet_tdma_ring_writel(priv, i,\n\t\t\t\t\t  ec->tx_max_coalesced_frames,\n\t\t\t\t\t  DMA_MBUF_DONE_THRESH);\n\tbcmgenet_tdma_ring_writel(priv, DESC_INDEX,\n\t\t\t\t  ec->tx_max_coalesced_frames,\n\t\t\t\t  DMA_MBUF_DONE_THRESH);\n\n\tfor (i = 0; i < priv->hw_params->rx_queues; i++)\n\t\tbcmgenet_set_ring_rx_coalesce(&priv->rx_rings[i], ec);\n\tbcmgenet_set_ring_rx_coalesce(&priv->rx_rings[DESC_INDEX], ec);\n\n\treturn 0;\n}\n\nstatic void bcmgenet_get_pauseparam(struct net_device *dev,\n\t\t\t\t    struct ethtool_pauseparam *epause)\n{\n\tstruct bcmgenet_priv *priv;\n\tu32 umac_cmd;\n\n\tpriv = netdev_priv(dev);\n\n\tepause->autoneg = priv->autoneg_pause;\n\n\tif (netif_carrier_ok(dev)) {\n\t\t \n\t\tumac_cmd = bcmgenet_umac_readl(priv, UMAC_CMD);\n\t\tepause->tx_pause = !(umac_cmd & CMD_TX_PAUSE_IGNORE);\n\t\tepause->rx_pause = !(umac_cmd & CMD_RX_PAUSE_IGNORE);\n\t} else {\n\t\t \n\t\tepause->tx_pause = priv->tx_pause;\n\t\tepause->rx_pause = priv->rx_pause;\n\t}\n}\n\nstatic int bcmgenet_set_pauseparam(struct net_device *dev,\n\t\t\t\t   struct ethtool_pauseparam *epause)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\tif (!dev->phydev)\n\t\treturn -ENODEV;\n\n\tif (!phy_validate_pause(dev->phydev, epause))\n\t\treturn -EINVAL;\n\n\tpriv->autoneg_pause = !!epause->autoneg;\n\tpriv->tx_pause = !!epause->tx_pause;\n\tpriv->rx_pause = !!epause->rx_pause;\n\n\tbcmgenet_phy_pause_set(dev, priv->rx_pause, priv->tx_pause);\n\n\treturn 0;\n}\n\n \nenum bcmgenet_stat_type {\n\tBCMGENET_STAT_NETDEV = -1,\n\tBCMGENET_STAT_MIB_RX,\n\tBCMGENET_STAT_MIB_TX,\n\tBCMGENET_STAT_RUNT,\n\tBCMGENET_STAT_MISC,\n\tBCMGENET_STAT_SOFT,\n};\n\nstruct bcmgenet_stats {\n\tchar stat_string[ETH_GSTRING_LEN];\n\tint stat_sizeof;\n\tint stat_offset;\n\tenum bcmgenet_stat_type type;\n\t \n\tu16 reg_offset;\n};\n\n#define STAT_NETDEV(m) { \\\n\t.stat_string = __stringify(m), \\\n\t.stat_sizeof = sizeof(((struct net_device_stats *)0)->m), \\\n\t.stat_offset = offsetof(struct net_device_stats, m), \\\n\t.type = BCMGENET_STAT_NETDEV, \\\n}\n\n#define STAT_GENET_MIB(str, m, _type) { \\\n\t.stat_string = str, \\\n\t.stat_sizeof = sizeof(((struct bcmgenet_priv *)0)->m), \\\n\t.stat_offset = offsetof(struct bcmgenet_priv, m), \\\n\t.type = _type, \\\n}\n\n#define STAT_GENET_MIB_RX(str, m) STAT_GENET_MIB(str, m, BCMGENET_STAT_MIB_RX)\n#define STAT_GENET_MIB_TX(str, m) STAT_GENET_MIB(str, m, BCMGENET_STAT_MIB_TX)\n#define STAT_GENET_RUNT(str, m) STAT_GENET_MIB(str, m, BCMGENET_STAT_RUNT)\n#define STAT_GENET_SOFT_MIB(str, m) STAT_GENET_MIB(str, m, BCMGENET_STAT_SOFT)\n\n#define STAT_GENET_MISC(str, m, offset) { \\\n\t.stat_string = str, \\\n\t.stat_sizeof = sizeof(((struct bcmgenet_priv *)0)->m), \\\n\t.stat_offset = offsetof(struct bcmgenet_priv, m), \\\n\t.type = BCMGENET_STAT_MISC, \\\n\t.reg_offset = offset, \\\n}\n\n#define STAT_GENET_Q(num) \\\n\tSTAT_GENET_SOFT_MIB(\"txq\" __stringify(num) \"_packets\", \\\n\t\t\ttx_rings[num].packets), \\\n\tSTAT_GENET_SOFT_MIB(\"txq\" __stringify(num) \"_bytes\", \\\n\t\t\ttx_rings[num].bytes), \\\n\tSTAT_GENET_SOFT_MIB(\"rxq\" __stringify(num) \"_bytes\", \\\n\t\t\trx_rings[num].bytes),\t \\\n\tSTAT_GENET_SOFT_MIB(\"rxq\" __stringify(num) \"_packets\", \\\n\t\t\trx_rings[num].packets), \\\n\tSTAT_GENET_SOFT_MIB(\"rxq\" __stringify(num) \"_errors\", \\\n\t\t\trx_rings[num].errors), \\\n\tSTAT_GENET_SOFT_MIB(\"rxq\" __stringify(num) \"_dropped\", \\\n\t\t\trx_rings[num].dropped)\n\n \n#define BCMGENET_STAT_OFFSET\t0xc\n\n \nstatic const struct bcmgenet_stats bcmgenet_gstrings_stats[] = {\n\t \n\tSTAT_NETDEV(rx_packets),\n\tSTAT_NETDEV(tx_packets),\n\tSTAT_NETDEV(rx_bytes),\n\tSTAT_NETDEV(tx_bytes),\n\tSTAT_NETDEV(rx_errors),\n\tSTAT_NETDEV(tx_errors),\n\tSTAT_NETDEV(rx_dropped),\n\tSTAT_NETDEV(tx_dropped),\n\tSTAT_NETDEV(multicast),\n\t \n\tSTAT_GENET_MIB_RX(\"rx_64_octets\", mib.rx.pkt_cnt.cnt_64),\n\tSTAT_GENET_MIB_RX(\"rx_65_127_oct\", mib.rx.pkt_cnt.cnt_127),\n\tSTAT_GENET_MIB_RX(\"rx_128_255_oct\", mib.rx.pkt_cnt.cnt_255),\n\tSTAT_GENET_MIB_RX(\"rx_256_511_oct\", mib.rx.pkt_cnt.cnt_511),\n\tSTAT_GENET_MIB_RX(\"rx_512_1023_oct\", mib.rx.pkt_cnt.cnt_1023),\n\tSTAT_GENET_MIB_RX(\"rx_1024_1518_oct\", mib.rx.pkt_cnt.cnt_1518),\n\tSTAT_GENET_MIB_RX(\"rx_vlan_1519_1522_oct\", mib.rx.pkt_cnt.cnt_mgv),\n\tSTAT_GENET_MIB_RX(\"rx_1522_2047_oct\", mib.rx.pkt_cnt.cnt_2047),\n\tSTAT_GENET_MIB_RX(\"rx_2048_4095_oct\", mib.rx.pkt_cnt.cnt_4095),\n\tSTAT_GENET_MIB_RX(\"rx_4096_9216_oct\", mib.rx.pkt_cnt.cnt_9216),\n\tSTAT_GENET_MIB_RX(\"rx_pkts\", mib.rx.pkt),\n\tSTAT_GENET_MIB_RX(\"rx_bytes\", mib.rx.bytes),\n\tSTAT_GENET_MIB_RX(\"rx_multicast\", mib.rx.mca),\n\tSTAT_GENET_MIB_RX(\"rx_broadcast\", mib.rx.bca),\n\tSTAT_GENET_MIB_RX(\"rx_fcs\", mib.rx.fcs),\n\tSTAT_GENET_MIB_RX(\"rx_control\", mib.rx.cf),\n\tSTAT_GENET_MIB_RX(\"rx_pause\", mib.rx.pf),\n\tSTAT_GENET_MIB_RX(\"rx_unknown\", mib.rx.uo),\n\tSTAT_GENET_MIB_RX(\"rx_align\", mib.rx.aln),\n\tSTAT_GENET_MIB_RX(\"rx_outrange\", mib.rx.flr),\n\tSTAT_GENET_MIB_RX(\"rx_code\", mib.rx.cde),\n\tSTAT_GENET_MIB_RX(\"rx_carrier\", mib.rx.fcr),\n\tSTAT_GENET_MIB_RX(\"rx_oversize\", mib.rx.ovr),\n\tSTAT_GENET_MIB_RX(\"rx_jabber\", mib.rx.jbr),\n\tSTAT_GENET_MIB_RX(\"rx_mtu_err\", mib.rx.mtue),\n\tSTAT_GENET_MIB_RX(\"rx_good_pkts\", mib.rx.pok),\n\tSTAT_GENET_MIB_RX(\"rx_unicast\", mib.rx.uc),\n\tSTAT_GENET_MIB_RX(\"rx_ppp\", mib.rx.ppp),\n\tSTAT_GENET_MIB_RX(\"rx_crc\", mib.rx.rcrc),\n\t \n\tSTAT_GENET_MIB_TX(\"tx_64_octets\", mib.tx.pkt_cnt.cnt_64),\n\tSTAT_GENET_MIB_TX(\"tx_65_127_oct\", mib.tx.pkt_cnt.cnt_127),\n\tSTAT_GENET_MIB_TX(\"tx_128_255_oct\", mib.tx.pkt_cnt.cnt_255),\n\tSTAT_GENET_MIB_TX(\"tx_256_511_oct\", mib.tx.pkt_cnt.cnt_511),\n\tSTAT_GENET_MIB_TX(\"tx_512_1023_oct\", mib.tx.pkt_cnt.cnt_1023),\n\tSTAT_GENET_MIB_TX(\"tx_1024_1518_oct\", mib.tx.pkt_cnt.cnt_1518),\n\tSTAT_GENET_MIB_TX(\"tx_vlan_1519_1522_oct\", mib.tx.pkt_cnt.cnt_mgv),\n\tSTAT_GENET_MIB_TX(\"tx_1522_2047_oct\", mib.tx.pkt_cnt.cnt_2047),\n\tSTAT_GENET_MIB_TX(\"tx_2048_4095_oct\", mib.tx.pkt_cnt.cnt_4095),\n\tSTAT_GENET_MIB_TX(\"tx_4096_9216_oct\", mib.tx.pkt_cnt.cnt_9216),\n\tSTAT_GENET_MIB_TX(\"tx_pkts\", mib.tx.pkts),\n\tSTAT_GENET_MIB_TX(\"tx_multicast\", mib.tx.mca),\n\tSTAT_GENET_MIB_TX(\"tx_broadcast\", mib.tx.bca),\n\tSTAT_GENET_MIB_TX(\"tx_pause\", mib.tx.pf),\n\tSTAT_GENET_MIB_TX(\"tx_control\", mib.tx.cf),\n\tSTAT_GENET_MIB_TX(\"tx_fcs_err\", mib.tx.fcs),\n\tSTAT_GENET_MIB_TX(\"tx_oversize\", mib.tx.ovr),\n\tSTAT_GENET_MIB_TX(\"tx_defer\", mib.tx.drf),\n\tSTAT_GENET_MIB_TX(\"tx_excess_defer\", mib.tx.edf),\n\tSTAT_GENET_MIB_TX(\"tx_single_col\", mib.tx.scl),\n\tSTAT_GENET_MIB_TX(\"tx_multi_col\", mib.tx.mcl),\n\tSTAT_GENET_MIB_TX(\"tx_late_col\", mib.tx.lcl),\n\tSTAT_GENET_MIB_TX(\"tx_excess_col\", mib.tx.ecl),\n\tSTAT_GENET_MIB_TX(\"tx_frags\", mib.tx.frg),\n\tSTAT_GENET_MIB_TX(\"tx_total_col\", mib.tx.ncl),\n\tSTAT_GENET_MIB_TX(\"tx_jabber\", mib.tx.jbr),\n\tSTAT_GENET_MIB_TX(\"tx_bytes\", mib.tx.bytes),\n\tSTAT_GENET_MIB_TX(\"tx_good_pkts\", mib.tx.pok),\n\tSTAT_GENET_MIB_TX(\"tx_unicast\", mib.tx.uc),\n\t \n\tSTAT_GENET_RUNT(\"rx_runt_pkts\", mib.rx_runt_cnt),\n\tSTAT_GENET_RUNT(\"rx_runt_valid_fcs\", mib.rx_runt_fcs),\n\tSTAT_GENET_RUNT(\"rx_runt_inval_fcs_align\", mib.rx_runt_fcs_align),\n\tSTAT_GENET_RUNT(\"rx_runt_bytes\", mib.rx_runt_bytes),\n\t \n\tSTAT_GENET_MISC(\"rbuf_ovflow_cnt\", mib.rbuf_ovflow_cnt,\n\t\t\tUMAC_RBUF_OVFL_CNT_V1),\n\tSTAT_GENET_MISC(\"rbuf_err_cnt\", mib.rbuf_err_cnt,\n\t\t\tUMAC_RBUF_ERR_CNT_V1),\n\tSTAT_GENET_MISC(\"mdf_err_cnt\", mib.mdf_err_cnt, UMAC_MDF_ERR_CNT),\n\tSTAT_GENET_SOFT_MIB(\"alloc_rx_buff_failed\", mib.alloc_rx_buff_failed),\n\tSTAT_GENET_SOFT_MIB(\"rx_dma_failed\", mib.rx_dma_failed),\n\tSTAT_GENET_SOFT_MIB(\"tx_dma_failed\", mib.tx_dma_failed),\n\tSTAT_GENET_SOFT_MIB(\"tx_realloc_tsb\", mib.tx_realloc_tsb),\n\tSTAT_GENET_SOFT_MIB(\"tx_realloc_tsb_failed\",\n\t\t\t    mib.tx_realloc_tsb_failed),\n\t \n\tSTAT_GENET_Q(0),\n\tSTAT_GENET_Q(1),\n\tSTAT_GENET_Q(2),\n\tSTAT_GENET_Q(3),\n\tSTAT_GENET_Q(16),\n};\n\n#define BCMGENET_STATS_LEN\tARRAY_SIZE(bcmgenet_gstrings_stats)\n\nstatic void bcmgenet_get_drvinfo(struct net_device *dev,\n\t\t\t\t struct ethtool_drvinfo *info)\n{\n\tstrscpy(info->driver, \"bcmgenet\", sizeof(info->driver));\n}\n\nstatic int bcmgenet_get_sset_count(struct net_device *dev, int string_set)\n{\n\tswitch (string_set) {\n\tcase ETH_SS_STATS:\n\t\treturn BCMGENET_STATS_LEN;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic void bcmgenet_get_strings(struct net_device *dev, u32 stringset,\n\t\t\t\t u8 *data)\n{\n\tint i;\n\n\tswitch (stringset) {\n\tcase ETH_SS_STATS:\n\t\tfor (i = 0; i < BCMGENET_STATS_LEN; i++) {\n\t\t\tmemcpy(data + i * ETH_GSTRING_LEN,\n\t\t\t       bcmgenet_gstrings_stats[i].stat_string,\n\t\t\t       ETH_GSTRING_LEN);\n\t\t}\n\t\tbreak;\n\t}\n}\n\nstatic u32 bcmgenet_update_stat_misc(struct bcmgenet_priv *priv, u16 offset)\n{\n\tu16 new_offset;\n\tu32 val;\n\n\tswitch (offset) {\n\tcase UMAC_RBUF_OVFL_CNT_V1:\n\t\tif (GENET_IS_V2(priv))\n\t\t\tnew_offset = RBUF_OVFL_CNT_V2;\n\t\telse\n\t\t\tnew_offset = RBUF_OVFL_CNT_V3PLUS;\n\n\t\tval = bcmgenet_rbuf_readl(priv,\tnew_offset);\n\t\t \n\t\tif (val == ~0)\n\t\t\tbcmgenet_rbuf_writel(priv, 0, new_offset);\n\t\tbreak;\n\tcase UMAC_RBUF_ERR_CNT_V1:\n\t\tif (GENET_IS_V2(priv))\n\t\t\tnew_offset = RBUF_ERR_CNT_V2;\n\t\telse\n\t\t\tnew_offset = RBUF_ERR_CNT_V3PLUS;\n\n\t\tval = bcmgenet_rbuf_readl(priv,\tnew_offset);\n\t\t \n\t\tif (val == ~0)\n\t\t\tbcmgenet_rbuf_writel(priv, 0, new_offset);\n\t\tbreak;\n\tdefault:\n\t\tval = bcmgenet_umac_readl(priv, offset);\n\t\t \n\t\tif (val == ~0)\n\t\t\tbcmgenet_umac_writel(priv, 0, offset);\n\t\tbreak;\n\t}\n\n\treturn val;\n}\n\nstatic void bcmgenet_update_mib_counters(struct bcmgenet_priv *priv)\n{\n\tint i, j = 0;\n\n\tfor (i = 0; i < BCMGENET_STATS_LEN; i++) {\n\t\tconst struct bcmgenet_stats *s;\n\t\tu8 offset = 0;\n\t\tu32 val = 0;\n\t\tchar *p;\n\n\t\ts = &bcmgenet_gstrings_stats[i];\n\t\tswitch (s->type) {\n\t\tcase BCMGENET_STAT_NETDEV:\n\t\tcase BCMGENET_STAT_SOFT:\n\t\t\tcontinue;\n\t\tcase BCMGENET_STAT_RUNT:\n\t\t\toffset += BCMGENET_STAT_OFFSET;\n\t\t\tfallthrough;\n\t\tcase BCMGENET_STAT_MIB_TX:\n\t\t\toffset += BCMGENET_STAT_OFFSET;\n\t\t\tfallthrough;\n\t\tcase BCMGENET_STAT_MIB_RX:\n\t\t\tval = bcmgenet_umac_readl(priv,\n\t\t\t\t\t\t  UMAC_MIB_START + j + offset);\n\t\t\toffset = 0;\t \n\t\t\tbreak;\n\t\tcase BCMGENET_STAT_MISC:\n\t\t\tif (GENET_IS_V1(priv)) {\n\t\t\t\tval = bcmgenet_umac_readl(priv, s->reg_offset);\n\t\t\t\t \n\t\t\t\tif (val == ~0)\n\t\t\t\t\tbcmgenet_umac_writel(priv, 0,\n\t\t\t\t\t\t\t     s->reg_offset);\n\t\t\t} else {\n\t\t\t\tval = bcmgenet_update_stat_misc(priv,\n\t\t\t\t\t\t\t\ts->reg_offset);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tj += s->stat_sizeof;\n\t\tp = (char *)priv + s->stat_offset;\n\t\t*(u32 *)p = val;\n\t}\n}\n\nstatic void bcmgenet_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t       struct ethtool_stats *stats,\n\t\t\t\t       u64 *data)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tint i;\n\n\tif (netif_running(dev))\n\t\tbcmgenet_update_mib_counters(priv);\n\n\tdev->netdev_ops->ndo_get_stats(dev);\n\n\tfor (i = 0; i < BCMGENET_STATS_LEN; i++) {\n\t\tconst struct bcmgenet_stats *s;\n\t\tchar *p;\n\n\t\ts = &bcmgenet_gstrings_stats[i];\n\t\tif (s->type == BCMGENET_STAT_NETDEV)\n\t\t\tp = (char *)&dev->stats;\n\t\telse\n\t\t\tp = (char *)priv;\n\t\tp += s->stat_offset;\n\t\tif (sizeof(unsigned long) != sizeof(u32) &&\n\t\t    s->stat_sizeof == sizeof(unsigned long))\n\t\t\tdata[i] = *(unsigned long *)p;\n\t\telse\n\t\t\tdata[i] = *(u32 *)p;\n\t}\n}\n\nvoid bcmgenet_eee_enable_set(struct net_device *dev, bool enable,\n\t\t\t     bool tx_lpi_enabled)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tu32 off = priv->hw_params->tbuf_offset + TBUF_ENERGY_CTRL;\n\tu32 reg;\n\n\tif (enable && !priv->clk_eee_enabled) {\n\t\tclk_prepare_enable(priv->clk_eee);\n\t\tpriv->clk_eee_enabled = true;\n\t}\n\n\treg = bcmgenet_umac_readl(priv, UMAC_EEE_CTRL);\n\tif (enable)\n\t\treg |= EEE_EN;\n\telse\n\t\treg &= ~EEE_EN;\n\tbcmgenet_umac_writel(priv, reg, UMAC_EEE_CTRL);\n\n\t \n\treg = bcmgenet_readl(priv->base + off);\n\tif (tx_lpi_enabled)\n\t\treg |= TBUF_EEE_EN | TBUF_PM_EN;\n\telse\n\t\treg &= ~(TBUF_EEE_EN | TBUF_PM_EN);\n\tbcmgenet_writel(reg, priv->base + off);\n\n\t \n\treg = bcmgenet_rbuf_readl(priv, RBUF_ENERGY_CTRL);\n\tif (enable)\n\t\treg |= RBUF_EEE_EN | RBUF_PM_EN;\n\telse\n\t\treg &= ~(RBUF_EEE_EN | RBUF_PM_EN);\n\tbcmgenet_rbuf_writel(priv, reg, RBUF_ENERGY_CTRL);\n\n\tif (!enable && priv->clk_eee_enabled) {\n\t\tclk_disable_unprepare(priv->clk_eee);\n\t\tpriv->clk_eee_enabled = false;\n\t}\n\n\tpriv->eee.eee_enabled = enable;\n\tpriv->eee.eee_active = enable;\n\tpriv->eee.tx_lpi_enabled = tx_lpi_enabled;\n}\n\nstatic int bcmgenet_get_eee(struct net_device *dev, struct ethtool_eee *e)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct ethtool_eee *p = &priv->eee;\n\n\tif (GENET_IS_V1(priv))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!dev->phydev)\n\t\treturn -ENODEV;\n\n\te->eee_enabled = p->eee_enabled;\n\te->eee_active = p->eee_active;\n\te->tx_lpi_enabled = p->tx_lpi_enabled;\n\te->tx_lpi_timer = bcmgenet_umac_readl(priv, UMAC_EEE_LPI_TIMER);\n\n\treturn phy_ethtool_get_eee(dev->phydev, e);\n}\n\nstatic int bcmgenet_set_eee(struct net_device *dev, struct ethtool_eee *e)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct ethtool_eee *p = &priv->eee;\n\n\tif (GENET_IS_V1(priv))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!dev->phydev)\n\t\treturn -ENODEV;\n\n\tp->eee_enabled = e->eee_enabled;\n\n\tif (!p->eee_enabled) {\n\t\tbcmgenet_eee_enable_set(dev, false, false);\n\t} else {\n\t\tp->eee_active = phy_init_eee(dev->phydev, false) >= 0;\n\t\tbcmgenet_umac_writel(priv, e->tx_lpi_timer, UMAC_EEE_LPI_TIMER);\n\t\tbcmgenet_eee_enable_set(dev, p->eee_active, e->tx_lpi_enabled);\n\t}\n\n\treturn phy_ethtool_set_eee(dev->phydev, e);\n}\n\nstatic int bcmgenet_validate_flow(struct net_device *dev,\n\t\t\t\t  struct ethtool_rxnfc *cmd)\n{\n\tstruct ethtool_usrip4_spec *l4_mask;\n\tstruct ethhdr *eth_mask;\n\n\tif (cmd->fs.location >= MAX_NUM_OF_FS_RULES &&\n\t    cmd->fs.location != RX_CLS_LOC_ANY) {\n\t\tnetdev_err(dev, \"rxnfc: Invalid location (%d)\\n\",\n\t\t\t   cmd->fs.location);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (cmd->fs.flow_type & ~(FLOW_EXT | FLOW_MAC_EXT)) {\n\tcase IP_USER_FLOW:\n\t\tl4_mask = &cmd->fs.m_u.usr_ip4_spec;\n\t\t \n\t\tif (VALIDATE_MASK(l4_mask->ip4src) ||\n\t\t    VALIDATE_MASK(l4_mask->ip4dst) ||\n\t\t    VALIDATE_MASK(l4_mask->l4_4_bytes) ||\n\t\t    VALIDATE_MASK(l4_mask->proto) ||\n\t\t    VALIDATE_MASK(l4_mask->ip_ver) ||\n\t\t    VALIDATE_MASK(l4_mask->tos)) {\n\t\t\tnetdev_err(dev, \"rxnfc: Unsupported mask\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase ETHER_FLOW:\n\t\teth_mask = &cmd->fs.m_u.ether_spec;\n\t\t \n\t\tif (VALIDATE_MASK(eth_mask->h_dest) ||\n\t\t    VALIDATE_MASK(eth_mask->h_source) ||\n\t\t    VALIDATE_MASK(eth_mask->h_proto)) {\n\t\t\tnetdev_err(dev, \"rxnfc: Unsupported mask\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tnetdev_err(dev, \"rxnfc: Unsupported flow type (0x%x)\\n\",\n\t\t\t   cmd->fs.flow_type);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((cmd->fs.flow_type & FLOW_EXT)) {\n\t\t \n\t\tif (VALIDATE_MASK(cmd->fs.m_ext.vlan_etype) ||\n\t\t    VALIDATE_MASK(cmd->fs.m_ext.vlan_tci)) {\n\t\t\tnetdev_err(dev, \"rxnfc: Unsupported mask\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (cmd->fs.m_ext.data[0] || cmd->fs.m_ext.data[1]) {\n\t\t\tnetdev_err(dev, \"rxnfc: user-def not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif ((cmd->fs.flow_type & FLOW_MAC_EXT)) {\n\t\t \n\t\tif (VALIDATE_MASK(cmd->fs.m_ext.h_dest)) {\n\t\t\tnetdev_err(dev, \"rxnfc: Unsupported mask\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int bcmgenet_insert_flow(struct net_device *dev,\n\t\t\t\tstruct ethtool_rxnfc *cmd)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct bcmgenet_rxnfc_rule *loc_rule;\n\tint err, i;\n\n\tif (priv->hw_params->hfb_filter_size < 128) {\n\t\tnetdev_err(dev, \"rxnfc: Not supported by this device\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd->fs.ring_cookie > priv->hw_params->rx_queues &&\n\t    cmd->fs.ring_cookie != RX_CLS_FLOW_WAKE) {\n\t\tnetdev_err(dev, \"rxnfc: Unsupported action (%llu)\\n\",\n\t\t\t   cmd->fs.ring_cookie);\n\t\treturn -EINVAL;\n\t}\n\n\terr = bcmgenet_validate_flow(dev, cmd);\n\tif (err)\n\t\treturn err;\n\n\tif (cmd->fs.location == RX_CLS_LOC_ANY) {\n\t\tlist_for_each_entry(loc_rule, &priv->rxnfc_list, list) {\n\t\t\tcmd->fs.location = loc_rule->fs.location;\n\t\t\terr = memcmp(&loc_rule->fs, &cmd->fs,\n\t\t\t\t     sizeof(struct ethtool_rx_flow_spec));\n\t\t\tif (!err)\n\t\t\t\t \n\t\t\t\treturn 0;\n\t\t}\n\t\tfor (i = 0; i < MAX_NUM_OF_FS_RULES; i++) {\n\t\t\tloc_rule = &priv->rxnfc_rules[i];\n\t\t\tif (loc_rule->state == BCMGENET_RXNFC_STATE_UNUSED) {\n\t\t\t\tcmd->fs.location = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == MAX_NUM_OF_FS_RULES) {\n\t\t\tcmd->fs.location = RX_CLS_LOC_ANY;\n\t\t\treturn -ENOSPC;\n\t\t}\n\t} else {\n\t\tloc_rule = &priv->rxnfc_rules[cmd->fs.location];\n\t}\n\tif (loc_rule->state == BCMGENET_RXNFC_STATE_ENABLED)\n\t\tbcmgenet_hfb_disable_filter(priv, cmd->fs.location);\n\tif (loc_rule->state != BCMGENET_RXNFC_STATE_UNUSED) {\n\t\tlist_del(&loc_rule->list);\n\t\tbcmgenet_hfb_clear_filter(priv, cmd->fs.location);\n\t}\n\tloc_rule->state = BCMGENET_RXNFC_STATE_UNUSED;\n\tmemcpy(&loc_rule->fs, &cmd->fs,\n\t       sizeof(struct ethtool_rx_flow_spec));\n\n\tbcmgenet_hfb_create_rxnfc_filter(priv, loc_rule);\n\n\tlist_add_tail(&loc_rule->list, &priv->rxnfc_list);\n\n\treturn 0;\n}\n\nstatic int bcmgenet_delete_flow(struct net_device *dev,\n\t\t\t\tstruct ethtool_rxnfc *cmd)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct bcmgenet_rxnfc_rule *rule;\n\tint err = 0;\n\n\tif (cmd->fs.location >= MAX_NUM_OF_FS_RULES)\n\t\treturn -EINVAL;\n\n\trule = &priv->rxnfc_rules[cmd->fs.location];\n\tif (rule->state == BCMGENET_RXNFC_STATE_UNUSED) {\n\t\terr =  -ENOENT;\n\t\tgoto out;\n\t}\n\n\tif (rule->state == BCMGENET_RXNFC_STATE_ENABLED)\n\t\tbcmgenet_hfb_disable_filter(priv, cmd->fs.location);\n\tif (rule->state != BCMGENET_RXNFC_STATE_UNUSED) {\n\t\tlist_del(&rule->list);\n\t\tbcmgenet_hfb_clear_filter(priv, cmd->fs.location);\n\t}\n\trule->state = BCMGENET_RXNFC_STATE_UNUSED;\n\tmemset(&rule->fs, 0, sizeof(struct ethtool_rx_flow_spec));\n\nout:\n\treturn err;\n}\n\nstatic int bcmgenet_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tint err = 0;\n\n\tswitch (cmd->cmd) {\n\tcase ETHTOOL_SRXCLSRLINS:\n\t\terr = bcmgenet_insert_flow(dev, cmd);\n\t\tbreak;\n\tcase ETHTOOL_SRXCLSRLDEL:\n\t\terr = bcmgenet_delete_flow(dev, cmd);\n\t\tbreak;\n\tdefault:\n\t\tnetdev_warn(priv->dev, \"Unsupported ethtool command. (%d)\\n\",\n\t\t\t    cmd->cmd);\n\t\treturn -EINVAL;\n\t}\n\n\treturn err;\n}\n\nstatic int bcmgenet_get_flow(struct net_device *dev, struct ethtool_rxnfc *cmd,\n\t\t\t     int loc)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct bcmgenet_rxnfc_rule *rule;\n\tint err = 0;\n\n\tif (loc < 0 || loc >= MAX_NUM_OF_FS_RULES)\n\t\treturn -EINVAL;\n\n\trule = &priv->rxnfc_rules[loc];\n\tif (rule->state == BCMGENET_RXNFC_STATE_UNUSED)\n\t\terr = -ENOENT;\n\telse\n\t\tmemcpy(&cmd->fs, &rule->fs,\n\t\t       sizeof(struct ethtool_rx_flow_spec));\n\n\treturn err;\n}\n\nstatic int bcmgenet_get_num_flows(struct bcmgenet_priv *priv)\n{\n\tstruct list_head *pos;\n\tint res = 0;\n\n\tlist_for_each(pos, &priv->rxnfc_list)\n\t\tres++;\n\n\treturn res;\n}\n\nstatic int bcmgenet_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd,\n\t\t\t      u32 *rule_locs)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct bcmgenet_rxnfc_rule *rule;\n\tint err = 0;\n\tint i = 0;\n\n\tswitch (cmd->cmd) {\n\tcase ETHTOOL_GRXRINGS:\n\t\tcmd->data = priv->hw_params->rx_queues ?: 1;\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRLCNT:\n\t\tcmd->rule_cnt = bcmgenet_get_num_flows(priv);\n\t\tcmd->data = MAX_NUM_OF_FS_RULES | RX_CLS_LOC_SPECIAL;\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRULE:\n\t\terr = bcmgenet_get_flow(dev, cmd, cmd->fs.location);\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRLALL:\n\t\tlist_for_each_entry(rule, &priv->rxnfc_list, list)\n\t\t\tif (i < cmd->rule_cnt)\n\t\t\t\trule_locs[i++] = rule->fs.location;\n\t\tcmd->rule_cnt = i;\n\t\tcmd->data = MAX_NUM_OF_FS_RULES;\n\t\tbreak;\n\tdefault:\n\t\terr = -EOPNOTSUPP;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n \nstatic const struct ethtool_ops bcmgenet_ethtool_ops = {\n\t.supported_coalesce_params = ETHTOOL_COALESCE_RX_USECS |\n\t\t\t\t     ETHTOOL_COALESCE_MAX_FRAMES |\n\t\t\t\t     ETHTOOL_COALESCE_USE_ADAPTIVE_RX,\n\t.begin\t\t\t= bcmgenet_begin,\n\t.complete\t\t= bcmgenet_complete,\n\t.get_strings\t\t= bcmgenet_get_strings,\n\t.get_sset_count\t\t= bcmgenet_get_sset_count,\n\t.get_ethtool_stats\t= bcmgenet_get_ethtool_stats,\n\t.get_drvinfo\t\t= bcmgenet_get_drvinfo,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_msglevel\t\t= bcmgenet_get_msglevel,\n\t.set_msglevel\t\t= bcmgenet_set_msglevel,\n\t.get_wol\t\t= bcmgenet_get_wol,\n\t.set_wol\t\t= bcmgenet_set_wol,\n\t.get_eee\t\t= bcmgenet_get_eee,\n\t.set_eee\t\t= bcmgenet_set_eee,\n\t.nway_reset\t\t= phy_ethtool_nway_reset,\n\t.get_coalesce\t\t= bcmgenet_get_coalesce,\n\t.set_coalesce\t\t= bcmgenet_set_coalesce,\n\t.get_link_ksettings\t= bcmgenet_get_link_ksettings,\n\t.set_link_ksettings\t= bcmgenet_set_link_ksettings,\n\t.get_ts_info\t\t= ethtool_op_get_ts_info,\n\t.get_rxnfc\t\t= bcmgenet_get_rxnfc,\n\t.set_rxnfc\t\t= bcmgenet_set_rxnfc,\n\t.get_pauseparam\t\t= bcmgenet_get_pauseparam,\n\t.set_pauseparam\t\t= bcmgenet_set_pauseparam,\n};\n\n \nstatic int bcmgenet_power_down(struct bcmgenet_priv *priv,\n\t\t\t\tenum bcmgenet_power_mode mode)\n{\n\tint ret = 0;\n\tu32 reg;\n\n\tswitch (mode) {\n\tcase GENET_POWER_CABLE_SENSE:\n\t\tphy_detach(priv->dev->phydev);\n\t\tbreak;\n\n\tcase GENET_POWER_WOL_MAGIC:\n\t\tret = bcmgenet_wol_power_down_cfg(priv, mode);\n\t\tbreak;\n\n\tcase GENET_POWER_PASSIVE:\n\t\t \n\t\tif (priv->hw_params->flags & GENET_HAS_EXT) {\n\t\t\treg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);\n\t\t\tif (GENET_IS_V5(priv) && !priv->ephy_16nm)\n\t\t\t\treg |= EXT_PWR_DOWN_PHY_EN |\n\t\t\t\t       EXT_PWR_DOWN_PHY_RD |\n\t\t\t\t       EXT_PWR_DOWN_PHY_SD |\n\t\t\t\t       EXT_PWR_DOWN_PHY_RX |\n\t\t\t\t       EXT_PWR_DOWN_PHY_TX |\n\t\t\t\t       EXT_IDDQ_GLBL_PWR;\n\t\t\telse\n\t\t\t\treg |= EXT_PWR_DOWN_PHY;\n\n\t\t\treg |= (EXT_PWR_DOWN_DLL | EXT_PWR_DOWN_BIAS);\n\t\t\tbcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);\n\n\t\t\tbcmgenet_phy_power_set(priv->dev, false);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void bcmgenet_power_up(struct bcmgenet_priv *priv,\n\t\t\t      enum bcmgenet_power_mode mode)\n{\n\tu32 reg;\n\n\tif (!(priv->hw_params->flags & GENET_HAS_EXT))\n\t\treturn;\n\n\treg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);\n\n\tswitch (mode) {\n\tcase GENET_POWER_PASSIVE:\n\t\treg &= ~(EXT_PWR_DOWN_DLL | EXT_PWR_DOWN_BIAS |\n\t\t\t EXT_ENERGY_DET_MASK);\n\t\tif (GENET_IS_V5(priv) && !priv->ephy_16nm) {\n\t\t\treg &= ~(EXT_PWR_DOWN_PHY_EN |\n\t\t\t\t EXT_PWR_DOWN_PHY_RD |\n\t\t\t\t EXT_PWR_DOWN_PHY_SD |\n\t\t\t\t EXT_PWR_DOWN_PHY_RX |\n\t\t\t\t EXT_PWR_DOWN_PHY_TX |\n\t\t\t\t EXT_IDDQ_GLBL_PWR);\n\t\t\treg |=   EXT_PHY_RESET;\n\t\t\tbcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);\n\t\t\tmdelay(1);\n\n\t\t\treg &=  ~EXT_PHY_RESET;\n\t\t} else {\n\t\t\treg &= ~EXT_PWR_DOWN_PHY;\n\t\t\treg |= EXT_PWR_DN_EN_LD;\n\t\t}\n\t\tbcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);\n\t\tbcmgenet_phy_power_set(priv->dev, true);\n\t\tbreak;\n\n\tcase GENET_POWER_CABLE_SENSE:\n\t\t \n\t\tif (!GENET_IS_V5(priv)) {\n\t\t\treg |= EXT_PWR_DN_EN_LD;\n\t\t\tbcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);\n\t\t}\n\t\tbreak;\n\tcase GENET_POWER_WOL_MAGIC:\n\t\tbcmgenet_wol_power_up_cfg(priv, mode);\n\t\treturn;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic struct enet_cb *bcmgenet_get_txcb(struct bcmgenet_priv *priv,\n\t\t\t\t\t struct bcmgenet_tx_ring *ring)\n{\n\tstruct enet_cb *tx_cb_ptr;\n\n\ttx_cb_ptr = ring->cbs;\n\ttx_cb_ptr += ring->write_ptr - ring->cb_ptr;\n\n\t \n\tif (ring->write_ptr == ring->end_ptr)\n\t\tring->write_ptr = ring->cb_ptr;\n\telse\n\t\tring->write_ptr++;\n\n\treturn tx_cb_ptr;\n}\n\nstatic struct enet_cb *bcmgenet_put_txcb(struct bcmgenet_priv *priv,\n\t\t\t\t\t struct bcmgenet_tx_ring *ring)\n{\n\tstruct enet_cb *tx_cb_ptr;\n\n\ttx_cb_ptr = ring->cbs;\n\ttx_cb_ptr += ring->write_ptr - ring->cb_ptr;\n\n\t \n\tif (ring->write_ptr == ring->cb_ptr)\n\t\tring->write_ptr = ring->end_ptr;\n\telse\n\t\tring->write_ptr--;\n\n\treturn tx_cb_ptr;\n}\n\nstatic inline void bcmgenet_rx_ring16_int_disable(struct bcmgenet_rx_ring *ring)\n{\n\tbcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_RXDMA_DONE,\n\t\t\t\t INTRL2_CPU_MASK_SET);\n}\n\nstatic inline void bcmgenet_rx_ring16_int_enable(struct bcmgenet_rx_ring *ring)\n{\n\tbcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_RXDMA_DONE,\n\t\t\t\t INTRL2_CPU_MASK_CLEAR);\n}\n\nstatic inline void bcmgenet_rx_ring_int_disable(struct bcmgenet_rx_ring *ring)\n{\n\tbcmgenet_intrl2_1_writel(ring->priv,\n\t\t\t\t 1 << (UMAC_IRQ1_RX_INTR_SHIFT + ring->index),\n\t\t\t\t INTRL2_CPU_MASK_SET);\n}\n\nstatic inline void bcmgenet_rx_ring_int_enable(struct bcmgenet_rx_ring *ring)\n{\n\tbcmgenet_intrl2_1_writel(ring->priv,\n\t\t\t\t 1 << (UMAC_IRQ1_RX_INTR_SHIFT + ring->index),\n\t\t\t\t INTRL2_CPU_MASK_CLEAR);\n}\n\nstatic inline void bcmgenet_tx_ring16_int_disable(struct bcmgenet_tx_ring *ring)\n{\n\tbcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_TXDMA_DONE,\n\t\t\t\t INTRL2_CPU_MASK_SET);\n}\n\nstatic inline void bcmgenet_tx_ring16_int_enable(struct bcmgenet_tx_ring *ring)\n{\n\tbcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_TXDMA_DONE,\n\t\t\t\t INTRL2_CPU_MASK_CLEAR);\n}\n\nstatic inline void bcmgenet_tx_ring_int_enable(struct bcmgenet_tx_ring *ring)\n{\n\tbcmgenet_intrl2_1_writel(ring->priv, 1 << ring->index,\n\t\t\t\t INTRL2_CPU_MASK_CLEAR);\n}\n\nstatic inline void bcmgenet_tx_ring_int_disable(struct bcmgenet_tx_ring *ring)\n{\n\tbcmgenet_intrl2_1_writel(ring->priv, 1 << ring->index,\n\t\t\t\t INTRL2_CPU_MASK_SET);\n}\n\n \nstatic struct sk_buff *bcmgenet_free_tx_cb(struct device *dev,\n\t\t\t\t\t   struct enet_cb *cb)\n{\n\tstruct sk_buff *skb;\n\n\tskb = cb->skb;\n\n\tif (skb) {\n\t\tcb->skb = NULL;\n\t\tif (cb == GENET_CB(skb)->first_cb)\n\t\t\tdma_unmap_single(dev, dma_unmap_addr(cb, dma_addr),\n\t\t\t\t\t dma_unmap_len(cb, dma_len),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_page(dev, dma_unmap_addr(cb, dma_addr),\n\t\t\t\t       dma_unmap_len(cb, dma_len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_addr_set(cb, dma_addr, 0);\n\n\t\tif (cb == GENET_CB(skb)->last_cb)\n\t\t\treturn skb;\n\n\t} else if (dma_unmap_addr(cb, dma_addr)) {\n\t\tdma_unmap_page(dev,\n\t\t\t       dma_unmap_addr(cb, dma_addr),\n\t\t\t       dma_unmap_len(cb, dma_len),\n\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_addr_set(cb, dma_addr, 0);\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct sk_buff *bcmgenet_free_rx_cb(struct device *dev,\n\t\t\t\t\t   struct enet_cb *cb)\n{\n\tstruct sk_buff *skb;\n\n\tskb = cb->skb;\n\tcb->skb = NULL;\n\n\tif (dma_unmap_addr(cb, dma_addr)) {\n\t\tdma_unmap_single(dev, dma_unmap_addr(cb, dma_addr),\n\t\t\t\t dma_unmap_len(cb, dma_len), DMA_FROM_DEVICE);\n\t\tdma_unmap_addr_set(cb, dma_addr, 0);\n\t}\n\n\treturn skb;\n}\n\n \nstatic unsigned int __bcmgenet_tx_reclaim(struct net_device *dev,\n\t\t\t\t\t  struct bcmgenet_tx_ring *ring)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tunsigned int txbds_processed = 0;\n\tunsigned int bytes_compl = 0;\n\tunsigned int pkts_compl = 0;\n\tunsigned int txbds_ready;\n\tunsigned int c_index;\n\tstruct sk_buff *skb;\n\n\t \n\tif (ring->index == DESC_INDEX)\n\t\tbcmgenet_intrl2_0_writel(priv, UMAC_IRQ_TXDMA_DONE,\n\t\t\t\t\t INTRL2_CPU_CLEAR);\n\telse\n\t\tbcmgenet_intrl2_1_writel(priv, (1 << ring->index),\n\t\t\t\t\t INTRL2_CPU_CLEAR);\n\n\t \n\tc_index = bcmgenet_tdma_ring_readl(priv, ring->index, TDMA_CONS_INDEX)\n\t\t& DMA_C_INDEX_MASK;\n\ttxbds_ready = (c_index - ring->c_index) & DMA_C_INDEX_MASK;\n\n\tnetif_dbg(priv, tx_done, dev,\n\t\t  \"%s ring=%d old_c_index=%u c_index=%u txbds_ready=%u\\n\",\n\t\t  __func__, ring->index, ring->c_index, c_index, txbds_ready);\n\n\t \n\twhile (txbds_processed < txbds_ready) {\n\t\tskb = bcmgenet_free_tx_cb(&priv->pdev->dev,\n\t\t\t\t\t  &priv->tx_cbs[ring->clean_ptr]);\n\t\tif (skb) {\n\t\t\tpkts_compl++;\n\t\t\tbytes_compl += GENET_CB(skb)->bytes_sent;\n\t\t\tdev_consume_skb_any(skb);\n\t\t}\n\n\t\ttxbds_processed++;\n\t\tif (likely(ring->clean_ptr < ring->end_ptr))\n\t\t\tring->clean_ptr++;\n\t\telse\n\t\t\tring->clean_ptr = ring->cb_ptr;\n\t}\n\n\tring->free_bds += txbds_processed;\n\tring->c_index = c_index;\n\n\tring->packets += pkts_compl;\n\tring->bytes += bytes_compl;\n\n\tnetdev_tx_completed_queue(netdev_get_tx_queue(dev, ring->queue),\n\t\t\t\t  pkts_compl, bytes_compl);\n\n\treturn txbds_processed;\n}\n\nstatic unsigned int bcmgenet_tx_reclaim(struct net_device *dev,\n\t\t\t\tstruct bcmgenet_tx_ring *ring)\n{\n\tunsigned int released;\n\n\tspin_lock_bh(&ring->lock);\n\treleased = __bcmgenet_tx_reclaim(dev, ring);\n\tspin_unlock_bh(&ring->lock);\n\n\treturn released;\n}\n\nstatic int bcmgenet_tx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct bcmgenet_tx_ring *ring =\n\t\tcontainer_of(napi, struct bcmgenet_tx_ring, napi);\n\tunsigned int work_done = 0;\n\tstruct netdev_queue *txq;\n\n\tspin_lock(&ring->lock);\n\twork_done = __bcmgenet_tx_reclaim(ring->priv->dev, ring);\n\tif (ring->free_bds > (MAX_SKB_FRAGS + 1)) {\n\t\ttxq = netdev_get_tx_queue(ring->priv->dev, ring->queue);\n\t\tnetif_tx_wake_queue(txq);\n\t}\n\tspin_unlock(&ring->lock);\n\n\tif (work_done == 0) {\n\t\tnapi_complete(napi);\n\t\tring->int_enable(ring);\n\n\t\treturn 0;\n\t}\n\n\treturn budget;\n}\n\nstatic void bcmgenet_tx_reclaim_all(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tint i;\n\n\tif (netif_is_multiqueue(dev)) {\n\t\tfor (i = 0; i < priv->hw_params->tx_queues; i++)\n\t\t\tbcmgenet_tx_reclaim(dev, &priv->tx_rings[i]);\n\t}\n\n\tbcmgenet_tx_reclaim(dev, &priv->tx_rings[DESC_INDEX]);\n}\n\n \nstatic struct sk_buff *bcmgenet_add_tsb(struct net_device *dev,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct status_64 *status = NULL;\n\tstruct sk_buff *new_skb;\n\tu16 offset;\n\tu8 ip_proto;\n\t__be16 ip_ver;\n\tu32 tx_csum_info;\n\n\tif (unlikely(skb_headroom(skb) < sizeof(*status))) {\n\t\t \n\t\tnew_skb = skb_realloc_headroom(skb, sizeof(*status));\n\t\tif (!new_skb) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tpriv->mib.tx_realloc_tsb_failed++;\n\t\t\tdev->stats.tx_dropped++;\n\t\t\treturn NULL;\n\t\t}\n\t\tdev_consume_skb_any(skb);\n\t\tskb = new_skb;\n\t\tpriv->mib.tx_realloc_tsb++;\n\t}\n\n\tskb_push(skb, sizeof(*status));\n\tstatus = (struct status_64 *)skb->data;\n\n\tif (skb->ip_summed  == CHECKSUM_PARTIAL) {\n\t\tip_ver = skb->protocol;\n\t\tswitch (ip_ver) {\n\t\tcase htons(ETH_P_IP):\n\t\t\tip_proto = ip_hdr(skb)->protocol;\n\t\t\tbreak;\n\t\tcase htons(ETH_P_IPV6):\n\t\t\tip_proto = ipv6_hdr(skb)->nexthdr;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tip_proto = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\toffset = skb_checksum_start_offset(skb) - sizeof(*status);\n\t\ttx_csum_info = (offset << STATUS_TX_CSUM_START_SHIFT) |\n\t\t\t\t(offset + skb->csum_offset) |\n\t\t\t\tSTATUS_TX_CSUM_LV;\n\n\t\t \n\t\tif (ip_proto == IPPROTO_UDP)\n\t\t\ttx_csum_info |= STATUS_TX_CSUM_PROTO_UDP;\n\n\t\tstatus->tx_csum_info = tx_csum_info;\n\t}\n\n\treturn skb;\n}\n\nstatic void bcmgenet_hide_tsb(struct sk_buff *skb)\n{\n\t__skb_pull(skb, sizeof(struct status_64));\n}\n\nstatic netdev_tx_t bcmgenet_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct device *kdev = &priv->pdev->dev;\n\tstruct bcmgenet_tx_ring *ring = NULL;\n\tstruct enet_cb *tx_cb_ptr;\n\tstruct netdev_queue *txq;\n\tint nr_frags, index;\n\tdma_addr_t mapping;\n\tunsigned int size;\n\tskb_frag_t *frag;\n\tu32 len_stat;\n\tint ret;\n\tint i;\n\n\tindex = skb_get_queue_mapping(skb);\n\t \n\tif (index == 0)\n\t\tindex = DESC_INDEX;\n\telse\n\t\tindex -= 1;\n\n\tring = &priv->tx_rings[index];\n\ttxq = netdev_get_tx_queue(dev, ring->queue);\n\n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\tspin_lock(&ring->lock);\n\tif (ring->free_bds <= (nr_frags + 1)) {\n\t\tif (!netif_tx_queue_stopped(txq))\n\t\t\tnetif_tx_stop_queue(txq);\n\t\tret = NETDEV_TX_BUSY;\n\t\tgoto out;\n\t}\n\n\t \n\tGENET_CB(skb)->bytes_sent = skb->len;\n\n\t \n\tskb = bcmgenet_add_tsb(dev, skb);\n\tif (!skb) {\n\t\tret = NETDEV_TX_OK;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i <= nr_frags; i++) {\n\t\ttx_cb_ptr = bcmgenet_get_txcb(priv, ring);\n\n\t\tBUG_ON(!tx_cb_ptr);\n\n\t\tif (!i) {\n\t\t\t \n\t\t\tGENET_CB(skb)->first_cb = tx_cb_ptr;\n\t\t\tsize = skb_headlen(skb);\n\t\t\tmapping = dma_map_single(kdev, skb->data, size,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\t} else {\n\t\t\t \n\t\t\tfrag = &skb_shinfo(skb)->frags[i - 1];\n\t\t\tsize = skb_frag_size(frag);\n\t\t\tmapping = skb_frag_dma_map(kdev, frag, 0, size,\n\t\t\t\t\t\t   DMA_TO_DEVICE);\n\t\t}\n\n\t\tret = dma_mapping_error(kdev, mapping);\n\t\tif (ret) {\n\t\t\tpriv->mib.tx_dma_failed++;\n\t\t\tnetif_err(priv, tx_err, dev, \"Tx DMA map failed\\n\");\n\t\t\tret = NETDEV_TX_OK;\n\t\t\tgoto out_unmap_frags;\n\t\t}\n\t\tdma_unmap_addr_set(tx_cb_ptr, dma_addr, mapping);\n\t\tdma_unmap_len_set(tx_cb_ptr, dma_len, size);\n\n\t\ttx_cb_ptr->skb = skb;\n\n\t\tlen_stat = (size << DMA_BUFLENGTH_SHIFT) |\n\t\t\t   (priv->hw_params->qtag_mask << DMA_TX_QTAG_SHIFT);\n\n\t\t \n\t\tlen_stat |= DMA_TX_APPEND_CRC;\n\n\t\tif (!i) {\n\t\t\tlen_stat |= DMA_SOP;\n\t\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\t\tlen_stat |= DMA_TX_DO_CSUM;\n\t\t}\n\t\tif (i == nr_frags)\n\t\t\tlen_stat |= DMA_EOP;\n\n\t\tdmadesc_set(priv, tx_cb_ptr->bd_addr, mapping, len_stat);\n\t}\n\n\tGENET_CB(skb)->last_cb = tx_cb_ptr;\n\n\tbcmgenet_hide_tsb(skb);\n\tskb_tx_timestamp(skb);\n\n\t \n\tring->free_bds -= nr_frags + 1;\n\tring->prod_index += nr_frags + 1;\n\tring->prod_index &= DMA_P_INDEX_MASK;\n\n\tnetdev_tx_sent_queue(txq, GENET_CB(skb)->bytes_sent);\n\n\tif (ring->free_bds <= (MAX_SKB_FRAGS + 1))\n\t\tnetif_tx_stop_queue(txq);\n\n\tif (!netdev_xmit_more() || netif_xmit_stopped(txq))\n\t\t \n\t\tbcmgenet_tdma_ring_writel(priv, ring->index,\n\t\t\t\t\t  ring->prod_index, TDMA_PROD_INDEX);\nout:\n\tspin_unlock(&ring->lock);\n\n\treturn ret;\n\nout_unmap_frags:\n\t \n\tbcmgenet_put_txcb(priv, ring);\n\n\t \n\twhile (i-- > 0) {\n\t\ttx_cb_ptr = bcmgenet_put_txcb(priv, ring);\n\t\tbcmgenet_free_tx_cb(kdev, tx_cb_ptr);\n\t}\n\n\tdev_kfree_skb(skb);\n\tgoto out;\n}\n\nstatic struct sk_buff *bcmgenet_rx_refill(struct bcmgenet_priv *priv,\n\t\t\t\t\t  struct enet_cb *cb)\n{\n\tstruct device *kdev = &priv->pdev->dev;\n\tstruct sk_buff *skb;\n\tstruct sk_buff *rx_skb;\n\tdma_addr_t mapping;\n\n\t \n\tskb = __netdev_alloc_skb(priv->dev, priv->rx_buf_len + SKB_ALIGNMENT,\n\t\t\t\t GFP_ATOMIC | __GFP_NOWARN);\n\tif (!skb) {\n\t\tpriv->mib.alloc_rx_buff_failed++;\n\t\tnetif_err(priv, rx_err, priv->dev,\n\t\t\t  \"%s: Rx skb allocation failed\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\t \n\tmapping = dma_map_single(kdev, skb->data, priv->rx_buf_len,\n\t\t\t\t DMA_FROM_DEVICE);\n\tif (dma_mapping_error(kdev, mapping)) {\n\t\tpriv->mib.rx_dma_failed++;\n\t\tdev_kfree_skb_any(skb);\n\t\tnetif_err(priv, rx_err, priv->dev,\n\t\t\t  \"%s: Rx skb DMA mapping failed\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\t \n\trx_skb = bcmgenet_free_rx_cb(kdev, cb);\n\n\t \n\tcb->skb = skb;\n\tdma_unmap_addr_set(cb, dma_addr, mapping);\n\tdma_unmap_len_set(cb, dma_len, priv->rx_buf_len);\n\tdmadesc_set_addr(priv, cb->bd_addr, mapping);\n\n\t \n\treturn rx_skb;\n}\n\n \nstatic unsigned int bcmgenet_desc_rx(struct bcmgenet_rx_ring *ring,\n\t\t\t\t     unsigned int budget)\n{\n\tstruct bcmgenet_priv *priv = ring->priv;\n\tstruct net_device *dev = priv->dev;\n\tstruct enet_cb *cb;\n\tstruct sk_buff *skb;\n\tu32 dma_length_status;\n\tunsigned long dma_flag;\n\tint len;\n\tunsigned int rxpktprocessed = 0, rxpkttoprocess;\n\tunsigned int bytes_processed = 0;\n\tunsigned int p_index, mask;\n\tunsigned int discards;\n\n\t \n\tif (ring->index == DESC_INDEX) {\n\t\tbcmgenet_intrl2_0_writel(priv, UMAC_IRQ_RXDMA_DONE,\n\t\t\t\t\t INTRL2_CPU_CLEAR);\n\t} else {\n\t\tmask = 1 << (UMAC_IRQ1_RX_INTR_SHIFT + ring->index);\n\t\tbcmgenet_intrl2_1_writel(priv,\n\t\t\t\t\t mask,\n\t\t\t\t\t INTRL2_CPU_CLEAR);\n\t}\n\n\tp_index = bcmgenet_rdma_ring_readl(priv, ring->index, RDMA_PROD_INDEX);\n\n\tdiscards = (p_index >> DMA_P_INDEX_DISCARD_CNT_SHIFT) &\n\t\t   DMA_P_INDEX_DISCARD_CNT_MASK;\n\tif (discards > ring->old_discards) {\n\t\tdiscards = discards - ring->old_discards;\n\t\tring->errors += discards;\n\t\tring->old_discards += discards;\n\n\t\t \n\t\tif (ring->old_discards >= 0xC000) {\n\t\t\tring->old_discards = 0;\n\t\t\tbcmgenet_rdma_ring_writel(priv, ring->index, 0,\n\t\t\t\t\t\t  RDMA_PROD_INDEX);\n\t\t}\n\t}\n\n\tp_index &= DMA_P_INDEX_MASK;\n\trxpkttoprocess = (p_index - ring->c_index) & DMA_C_INDEX_MASK;\n\n\tnetif_dbg(priv, rx_status, dev,\n\t\t  \"RDMA: rxpkttoprocess=%d\\n\", rxpkttoprocess);\n\n\twhile ((rxpktprocessed < rxpkttoprocess) &&\n\t       (rxpktprocessed < budget)) {\n\t\tstruct status_64 *status;\n\t\t__be16 rx_csum;\n\n\t\tcb = &priv->rx_cbs[ring->read_ptr];\n\t\tskb = bcmgenet_rx_refill(priv, cb);\n\n\t\tif (unlikely(!skb)) {\n\t\t\tring->dropped++;\n\t\t\tgoto next;\n\t\t}\n\n\t\tstatus = (struct status_64 *)skb->data;\n\t\tdma_length_status = status->length_status;\n\t\tif (dev->features & NETIF_F_RXCSUM) {\n\t\t\trx_csum = (__force __be16)(status->rx_csum & 0xffff);\n\t\t\tif (rx_csum) {\n\t\t\t\tskb->csum = (__force __wsum)ntohs(rx_csum);\n\t\t\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tdma_flag = dma_length_status & 0xffff;\n\t\tlen = dma_length_status >> DMA_BUFLENGTH_SHIFT;\n\n\t\tnetif_dbg(priv, rx_status, dev,\n\t\t\t  \"%s:p_ind=%d c_ind=%d read_ptr=%d len_stat=0x%08x\\n\",\n\t\t\t  __func__, p_index, ring->c_index,\n\t\t\t  ring->read_ptr, dma_length_status);\n\n\t\tif (unlikely(len > RX_BUF_LENGTH)) {\n\t\t\tnetif_err(priv, rx_status, dev, \"oversized packet\\n\");\n\t\t\tdev->stats.rx_length_errors++;\n\t\t\tdev->stats.rx_errors++;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (unlikely(!(dma_flag & DMA_EOP) || !(dma_flag & DMA_SOP))) {\n\t\t\tnetif_err(priv, rx_status, dev,\n\t\t\t\t  \"dropping fragmented packet!\\n\");\n\t\t\tring->errors++;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (unlikely(dma_flag & (DMA_RX_CRC_ERROR |\n\t\t\t\t\t\tDMA_RX_OV |\n\t\t\t\t\t\tDMA_RX_NO |\n\t\t\t\t\t\tDMA_RX_LG |\n\t\t\t\t\t\tDMA_RX_RXER))) {\n\t\t\tnetif_err(priv, rx_status, dev, \"dma_flag=0x%x\\n\",\n\t\t\t\t  (unsigned int)dma_flag);\n\t\t\tif (dma_flag & DMA_RX_CRC_ERROR)\n\t\t\t\tdev->stats.rx_crc_errors++;\n\t\t\tif (dma_flag & DMA_RX_OV)\n\t\t\t\tdev->stats.rx_over_errors++;\n\t\t\tif (dma_flag & DMA_RX_NO)\n\t\t\t\tdev->stats.rx_frame_errors++;\n\t\t\tif (dma_flag & DMA_RX_LG)\n\t\t\t\tdev->stats.rx_length_errors++;\n\t\t\tdev->stats.rx_errors++;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto next;\n\t\t}  \n\n\t\tskb_put(skb, len);\n\n\t\t \n\t\tskb_pull(skb, 66);\n\t\tlen -= 66;\n\n\t\tif (priv->crc_fwd_en) {\n\t\t\tskb_trim(skb, len - ETH_FCS_LEN);\n\t\t\tlen -= ETH_FCS_LEN;\n\t\t}\n\n\t\tbytes_processed += len;\n\n\t\t \n\t\tskb->protocol = eth_type_trans(skb, priv->dev);\n\t\tring->packets++;\n\t\tring->bytes += len;\n\t\tif (dma_flag & DMA_RX_MULT)\n\t\t\tdev->stats.multicast++;\n\n\t\t \n\t\tnapi_gro_receive(&ring->napi, skb);\n\t\tnetif_dbg(priv, rx_status, dev, \"pushed up to kernel\\n\");\n\nnext:\n\t\trxpktprocessed++;\n\t\tif (likely(ring->read_ptr < ring->end_ptr))\n\t\t\tring->read_ptr++;\n\t\telse\n\t\t\tring->read_ptr = ring->cb_ptr;\n\n\t\tring->c_index = (ring->c_index + 1) & DMA_C_INDEX_MASK;\n\t\tbcmgenet_rdma_ring_writel(priv, ring->index, ring->c_index, RDMA_CONS_INDEX);\n\t}\n\n\tring->dim.bytes = bytes_processed;\n\tring->dim.packets = rxpktprocessed;\n\n\treturn rxpktprocessed;\n}\n\n \nstatic int bcmgenet_rx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct bcmgenet_rx_ring *ring = container_of(napi,\n\t\t\tstruct bcmgenet_rx_ring, napi);\n\tstruct dim_sample dim_sample = {};\n\tunsigned int work_done;\n\n\twork_done = bcmgenet_desc_rx(ring, budget);\n\n\tif (work_done < budget) {\n\t\tnapi_complete_done(napi, work_done);\n\t\tring->int_enable(ring);\n\t}\n\n\tif (ring->dim.use_dim) {\n\t\tdim_update_sample(ring->dim.event_ctr, ring->dim.packets,\n\t\t\t\t  ring->dim.bytes, &dim_sample);\n\t\tnet_dim(&ring->dim.dim, dim_sample);\n\t}\n\n\treturn work_done;\n}\n\nstatic void bcmgenet_dim_work(struct work_struct *work)\n{\n\tstruct dim *dim = container_of(work, struct dim, work);\n\tstruct bcmgenet_net_dim *ndim =\n\t\t\tcontainer_of(dim, struct bcmgenet_net_dim, dim);\n\tstruct bcmgenet_rx_ring *ring =\n\t\t\tcontainer_of(ndim, struct bcmgenet_rx_ring, dim);\n\tstruct dim_cq_moder cur_profile =\n\t\t\tnet_dim_get_rx_moderation(dim->mode, dim->profile_ix);\n\n\tbcmgenet_set_rx_coalesce(ring, cur_profile.usec, cur_profile.pkts);\n\tdim->state = DIM_START_MEASURE;\n}\n\n \nstatic int bcmgenet_alloc_rx_buffers(struct bcmgenet_priv *priv,\n\t\t\t\t     struct bcmgenet_rx_ring *ring)\n{\n\tstruct enet_cb *cb;\n\tstruct sk_buff *skb;\n\tint i;\n\n\tnetif_dbg(priv, hw, priv->dev, \"%s\\n\", __func__);\n\n\t \n\tfor (i = 0; i < ring->size; i++) {\n\t\tcb = ring->cbs + i;\n\t\tskb = bcmgenet_rx_refill(priv, cb);\n\t\tif (skb)\n\t\t\tdev_consume_skb_any(skb);\n\t\tif (!cb->skb)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void bcmgenet_free_rx_buffers(struct bcmgenet_priv *priv)\n{\n\tstruct sk_buff *skb;\n\tstruct enet_cb *cb;\n\tint i;\n\n\tfor (i = 0; i < priv->num_rx_bds; i++) {\n\t\tcb = &priv->rx_cbs[i];\n\n\t\tskb = bcmgenet_free_rx_cb(&priv->pdev->dev, cb);\n\t\tif (skb)\n\t\t\tdev_consume_skb_any(skb);\n\t}\n}\n\nstatic void umac_enable_set(struct bcmgenet_priv *priv, u32 mask, bool enable)\n{\n\tu32 reg;\n\n\treg = bcmgenet_umac_readl(priv, UMAC_CMD);\n\tif (reg & CMD_SW_RESET)\n\t\treturn;\n\tif (enable)\n\t\treg |= mask;\n\telse\n\t\treg &= ~mask;\n\tbcmgenet_umac_writel(priv, reg, UMAC_CMD);\n\n\t \n\tif (enable == 0)\n\t\tusleep_range(1000, 2000);\n}\n\nstatic void reset_umac(struct bcmgenet_priv *priv)\n{\n\t \n\tbcmgenet_rbuf_ctrl_set(priv, 0);\n\tudelay(10);\n\n\t \n\tbcmgenet_umac_writel(priv, CMD_SW_RESET, UMAC_CMD);\n\tudelay(2);\n}\n\nstatic void bcmgenet_intr_disable(struct bcmgenet_priv *priv)\n{\n\t \n\tbcmgenet_intrl2_0_writel(priv, 0xFFFFFFFF, INTRL2_CPU_MASK_SET);\n\tbcmgenet_intrl2_0_writel(priv, 0xFFFFFFFF, INTRL2_CPU_CLEAR);\n\tbcmgenet_intrl2_1_writel(priv, 0xFFFFFFFF, INTRL2_CPU_MASK_SET);\n\tbcmgenet_intrl2_1_writel(priv, 0xFFFFFFFF, INTRL2_CPU_CLEAR);\n}\n\nstatic void bcmgenet_link_intr_enable(struct bcmgenet_priv *priv)\n{\n\tu32 int0_enable = 0;\n\n\t \n\tif (priv->internal_phy) {\n\t\tint0_enable |= UMAC_IRQ_LINK_EVENT;\n\t\tif (GENET_IS_V1(priv) || GENET_IS_V2(priv) || GENET_IS_V3(priv))\n\t\t\tint0_enable |= UMAC_IRQ_PHY_DET_R;\n\t} else if (priv->ext_phy) {\n\t\tint0_enable |= UMAC_IRQ_LINK_EVENT;\n\t} else if (priv->phy_interface == PHY_INTERFACE_MODE_MOCA) {\n\t\tif (priv->hw_params->flags & GENET_HAS_MOCA_LINK_DET)\n\t\t\tint0_enable |= UMAC_IRQ_LINK_EVENT;\n\t}\n\tbcmgenet_intrl2_0_writel(priv, int0_enable, INTRL2_CPU_MASK_CLEAR);\n}\n\nstatic void init_umac(struct bcmgenet_priv *priv)\n{\n\tstruct device *kdev = &priv->pdev->dev;\n\tu32 reg;\n\tu32 int0_enable = 0;\n\n\tdev_dbg(&priv->pdev->dev, \"bcmgenet: init_umac\\n\");\n\n\treset_umac(priv);\n\n\t \n\tbcmgenet_umac_writel(priv,\n\t\t\t     MIB_RESET_RX | MIB_RESET_TX | MIB_RESET_RUNT,\n\t\t\t     UMAC_MIB_CTRL);\n\tbcmgenet_umac_writel(priv, 0, UMAC_MIB_CTRL);\n\n\tbcmgenet_umac_writel(priv, ENET_MAX_MTU_SIZE, UMAC_MAX_FRAME_LEN);\n\n\t \n\treg = bcmgenet_tbuf_ctrl_get(priv);\n\treg |= TBUF_64B_EN;\n\tbcmgenet_tbuf_ctrl_set(priv, reg);\n\n\t \n\treg = bcmgenet_rbuf_readl(priv, RBUF_CTRL);\n\treg |= RBUF_ALIGN_2B | RBUF_64B_EN;\n\tbcmgenet_rbuf_writel(priv, reg, RBUF_CTRL);\n\n\t \n\treg = bcmgenet_rbuf_readl(priv, RBUF_CHK_CTRL);\n\treg |= RBUF_RXCHK_EN | RBUF_L3_PARSE_DIS;\n\t \n\tif (priv->crc_fwd_en)\n\t\treg |= RBUF_SKIP_FCS;\n\telse\n\t\treg &= ~RBUF_SKIP_FCS;\n\tbcmgenet_rbuf_writel(priv, reg, RBUF_CHK_CTRL);\n\n\tif (!GENET_IS_V1(priv) && !GENET_IS_V2(priv))\n\t\tbcmgenet_rbuf_writel(priv, 1, RBUF_TBUF_SIZE_CTRL);\n\n\tbcmgenet_intr_disable(priv);\n\n\t \n\tif (priv->phy_interface == PHY_INTERFACE_MODE_MOCA) {\n\t\treg = bcmgenet_bp_mc_get(priv);\n\t\treg |= BIT(priv->hw_params->bp_in_en_shift);\n\n\t\t \n\t\tif (netif_is_multiqueue(priv->dev))\n\t\t\treg |= priv->hw_params->bp_in_mask;\n\t\telse\n\t\t\treg &= ~priv->hw_params->bp_in_mask;\n\t\tbcmgenet_bp_mc_set(priv, reg);\n\t}\n\n\t \n\tif (priv->hw_params->flags & GENET_HAS_MDIO_INTR)\n\t\tint0_enable |= (UMAC_IRQ_MDIO_DONE | UMAC_IRQ_MDIO_ERROR);\n\n\tbcmgenet_intrl2_0_writel(priv, int0_enable, INTRL2_CPU_MASK_CLEAR);\n\n\tdev_dbg(kdev, \"done init umac\\n\");\n}\n\nstatic void bcmgenet_init_dim(struct bcmgenet_rx_ring *ring,\n\t\t\t      void (*cb)(struct work_struct *work))\n{\n\tstruct bcmgenet_net_dim *dim = &ring->dim;\n\n\tINIT_WORK(&dim->dim.work, cb);\n\tdim->dim.mode = DIM_CQ_PERIOD_MODE_START_FROM_EQE;\n\tdim->event_ctr = 0;\n\tdim->packets = 0;\n\tdim->bytes = 0;\n}\n\nstatic void bcmgenet_init_rx_coalesce(struct bcmgenet_rx_ring *ring)\n{\n\tstruct bcmgenet_net_dim *dim = &ring->dim;\n\tstruct dim_cq_moder moder;\n\tu32 usecs, pkts;\n\n\tusecs = ring->rx_coalesce_usecs;\n\tpkts = ring->rx_max_coalesced_frames;\n\n\t \n\tif (dim->use_dim) {\n\t\tmoder = net_dim_get_def_rx_moderation(dim->dim.mode);\n\t\tusecs = moder.usec;\n\t\tpkts = moder.pkts;\n\t}\n\n\tbcmgenet_set_rx_coalesce(ring, usecs, pkts);\n}\n\n \nstatic void bcmgenet_init_tx_ring(struct bcmgenet_priv *priv,\n\t\t\t\t  unsigned int index, unsigned int size,\n\t\t\t\t  unsigned int start_ptr, unsigned int end_ptr)\n{\n\tstruct bcmgenet_tx_ring *ring = &priv->tx_rings[index];\n\tu32 words_per_bd = WORDS_PER_BD(priv);\n\tu32 flow_period_val = 0;\n\n\tspin_lock_init(&ring->lock);\n\tring->priv = priv;\n\tring->index = index;\n\tif (index == DESC_INDEX) {\n\t\tring->queue = 0;\n\t\tring->int_enable = bcmgenet_tx_ring16_int_enable;\n\t\tring->int_disable = bcmgenet_tx_ring16_int_disable;\n\t} else {\n\t\tring->queue = index + 1;\n\t\tring->int_enable = bcmgenet_tx_ring_int_enable;\n\t\tring->int_disable = bcmgenet_tx_ring_int_disable;\n\t}\n\tring->cbs = priv->tx_cbs + start_ptr;\n\tring->size = size;\n\tring->clean_ptr = start_ptr;\n\tring->c_index = 0;\n\tring->free_bds = size;\n\tring->write_ptr = start_ptr;\n\tring->cb_ptr = start_ptr;\n\tring->end_ptr = end_ptr - 1;\n\tring->prod_index = 0;\n\n\t \n\tif (index != DESC_INDEX)\n\t\tflow_period_val = ENET_MAX_MTU_SIZE << 16;\n\n\tbcmgenet_tdma_ring_writel(priv, index, 0, TDMA_PROD_INDEX);\n\tbcmgenet_tdma_ring_writel(priv, index, 0, TDMA_CONS_INDEX);\n\tbcmgenet_tdma_ring_writel(priv, index, 1, DMA_MBUF_DONE_THRESH);\n\t \n\tbcmgenet_tdma_ring_writel(priv, index, flow_period_val,\n\t\t\t\t  TDMA_FLOW_PERIOD);\n\tbcmgenet_tdma_ring_writel(priv, index,\n\t\t\t\t  ((size << DMA_RING_SIZE_SHIFT) |\n\t\t\t\t   RX_BUF_LENGTH), DMA_RING_BUF_SIZE);\n\n\t \n\tbcmgenet_tdma_ring_writel(priv, index, start_ptr * words_per_bd,\n\t\t\t\t  DMA_START_ADDR);\n\tbcmgenet_tdma_ring_writel(priv, index, start_ptr * words_per_bd,\n\t\t\t\t  TDMA_READ_PTR);\n\tbcmgenet_tdma_ring_writel(priv, index, start_ptr * words_per_bd,\n\t\t\t\t  TDMA_WRITE_PTR);\n\tbcmgenet_tdma_ring_writel(priv, index, end_ptr * words_per_bd - 1,\n\t\t\t\t  DMA_END_ADDR);\n\n\t \n\tnetif_napi_add_tx(priv->dev, &ring->napi, bcmgenet_tx_poll);\n}\n\n \nstatic int bcmgenet_init_rx_ring(struct bcmgenet_priv *priv,\n\t\t\t\t unsigned int index, unsigned int size,\n\t\t\t\t unsigned int start_ptr, unsigned int end_ptr)\n{\n\tstruct bcmgenet_rx_ring *ring = &priv->rx_rings[index];\n\tu32 words_per_bd = WORDS_PER_BD(priv);\n\tint ret;\n\n\tring->priv = priv;\n\tring->index = index;\n\tif (index == DESC_INDEX) {\n\t\tring->int_enable = bcmgenet_rx_ring16_int_enable;\n\t\tring->int_disable = bcmgenet_rx_ring16_int_disable;\n\t} else {\n\t\tring->int_enable = bcmgenet_rx_ring_int_enable;\n\t\tring->int_disable = bcmgenet_rx_ring_int_disable;\n\t}\n\tring->cbs = priv->rx_cbs + start_ptr;\n\tring->size = size;\n\tring->c_index = 0;\n\tring->read_ptr = start_ptr;\n\tring->cb_ptr = start_ptr;\n\tring->end_ptr = end_ptr - 1;\n\n\tret = bcmgenet_alloc_rx_buffers(priv, ring);\n\tif (ret)\n\t\treturn ret;\n\n\tbcmgenet_init_dim(ring, bcmgenet_dim_work);\n\tbcmgenet_init_rx_coalesce(ring);\n\n\t \n\tnetif_napi_add(priv->dev, &ring->napi, bcmgenet_rx_poll);\n\n\tbcmgenet_rdma_ring_writel(priv, index, 0, RDMA_PROD_INDEX);\n\tbcmgenet_rdma_ring_writel(priv, index, 0, RDMA_CONS_INDEX);\n\tbcmgenet_rdma_ring_writel(priv, index,\n\t\t\t\t  ((size << DMA_RING_SIZE_SHIFT) |\n\t\t\t\t   RX_BUF_LENGTH), DMA_RING_BUF_SIZE);\n\tbcmgenet_rdma_ring_writel(priv, index,\n\t\t\t\t  (DMA_FC_THRESH_LO <<\n\t\t\t\t   DMA_XOFF_THRESHOLD_SHIFT) |\n\t\t\t\t   DMA_FC_THRESH_HI, RDMA_XON_XOFF_THRESH);\n\n\t \n\tbcmgenet_rdma_ring_writel(priv, index, start_ptr * words_per_bd,\n\t\t\t\t  DMA_START_ADDR);\n\tbcmgenet_rdma_ring_writel(priv, index, start_ptr * words_per_bd,\n\t\t\t\t  RDMA_READ_PTR);\n\tbcmgenet_rdma_ring_writel(priv, index, start_ptr * words_per_bd,\n\t\t\t\t  RDMA_WRITE_PTR);\n\tbcmgenet_rdma_ring_writel(priv, index, end_ptr * words_per_bd - 1,\n\t\t\t\t  DMA_END_ADDR);\n\n\treturn ret;\n}\n\nstatic void bcmgenet_enable_tx_napi(struct bcmgenet_priv *priv)\n{\n\tunsigned int i;\n\tstruct bcmgenet_tx_ring *ring;\n\n\tfor (i = 0; i < priv->hw_params->tx_queues; ++i) {\n\t\tring = &priv->tx_rings[i];\n\t\tnapi_enable(&ring->napi);\n\t\tring->int_enable(ring);\n\t}\n\n\tring = &priv->tx_rings[DESC_INDEX];\n\tnapi_enable(&ring->napi);\n\tring->int_enable(ring);\n}\n\nstatic void bcmgenet_disable_tx_napi(struct bcmgenet_priv *priv)\n{\n\tunsigned int i;\n\tstruct bcmgenet_tx_ring *ring;\n\n\tfor (i = 0; i < priv->hw_params->tx_queues; ++i) {\n\t\tring = &priv->tx_rings[i];\n\t\tnapi_disable(&ring->napi);\n\t}\n\n\tring = &priv->tx_rings[DESC_INDEX];\n\tnapi_disable(&ring->napi);\n}\n\nstatic void bcmgenet_fini_tx_napi(struct bcmgenet_priv *priv)\n{\n\tunsigned int i;\n\tstruct bcmgenet_tx_ring *ring;\n\n\tfor (i = 0; i < priv->hw_params->tx_queues; ++i) {\n\t\tring = &priv->tx_rings[i];\n\t\tnetif_napi_del(&ring->napi);\n\t}\n\n\tring = &priv->tx_rings[DESC_INDEX];\n\tnetif_napi_del(&ring->napi);\n}\n\n \nstatic void bcmgenet_init_tx_queues(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tu32 i, dma_enable;\n\tu32 dma_ctrl, ring_cfg;\n\tu32 dma_priority[3] = {0, 0, 0};\n\n\tdma_ctrl = bcmgenet_tdma_readl(priv, DMA_CTRL);\n\tdma_enable = dma_ctrl & DMA_EN;\n\tdma_ctrl &= ~DMA_EN;\n\tbcmgenet_tdma_writel(priv, dma_ctrl, DMA_CTRL);\n\n\tdma_ctrl = 0;\n\tring_cfg = 0;\n\n\t \n\tbcmgenet_tdma_writel(priv, DMA_ARBITER_SP, DMA_ARB_CTRL);\n\n\t \n\tfor (i = 0; i < priv->hw_params->tx_queues; i++) {\n\t\tbcmgenet_init_tx_ring(priv, i, priv->hw_params->tx_bds_per_q,\n\t\t\t\t      i * priv->hw_params->tx_bds_per_q,\n\t\t\t\t      (i + 1) * priv->hw_params->tx_bds_per_q);\n\t\tring_cfg |= (1 << i);\n\t\tdma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\n\t\tdma_priority[DMA_PRIO_REG_INDEX(i)] |=\n\t\t\t((GENET_Q0_PRIORITY + i) << DMA_PRIO_REG_SHIFT(i));\n\t}\n\n\t \n\tbcmgenet_init_tx_ring(priv, DESC_INDEX, GENET_Q16_TX_BD_CNT,\n\t\t\t      priv->hw_params->tx_queues *\n\t\t\t      priv->hw_params->tx_bds_per_q,\n\t\t\t      TOTAL_DESC);\n\tring_cfg |= (1 << DESC_INDEX);\n\tdma_ctrl |= (1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT));\n\tdma_priority[DMA_PRIO_REG_INDEX(DESC_INDEX)] |=\n\t\t((GENET_Q0_PRIORITY + priv->hw_params->tx_queues) <<\n\t\t DMA_PRIO_REG_SHIFT(DESC_INDEX));\n\n\t \n\tbcmgenet_tdma_writel(priv, dma_priority[0], DMA_PRIORITY_0);\n\tbcmgenet_tdma_writel(priv, dma_priority[1], DMA_PRIORITY_1);\n\tbcmgenet_tdma_writel(priv, dma_priority[2], DMA_PRIORITY_2);\n\n\t \n\tbcmgenet_tdma_writel(priv, ring_cfg, DMA_RING_CFG);\n\n\t \n\tif (dma_enable)\n\t\tdma_ctrl |= DMA_EN;\n\tbcmgenet_tdma_writel(priv, dma_ctrl, DMA_CTRL);\n}\n\nstatic void bcmgenet_enable_rx_napi(struct bcmgenet_priv *priv)\n{\n\tunsigned int i;\n\tstruct bcmgenet_rx_ring *ring;\n\n\tfor (i = 0; i < priv->hw_params->rx_queues; ++i) {\n\t\tring = &priv->rx_rings[i];\n\t\tnapi_enable(&ring->napi);\n\t\tring->int_enable(ring);\n\t}\n\n\tring = &priv->rx_rings[DESC_INDEX];\n\tnapi_enable(&ring->napi);\n\tring->int_enable(ring);\n}\n\nstatic void bcmgenet_disable_rx_napi(struct bcmgenet_priv *priv)\n{\n\tunsigned int i;\n\tstruct bcmgenet_rx_ring *ring;\n\n\tfor (i = 0; i < priv->hw_params->rx_queues; ++i) {\n\t\tring = &priv->rx_rings[i];\n\t\tnapi_disable(&ring->napi);\n\t\tcancel_work_sync(&ring->dim.dim.work);\n\t}\n\n\tring = &priv->rx_rings[DESC_INDEX];\n\tnapi_disable(&ring->napi);\n\tcancel_work_sync(&ring->dim.dim.work);\n}\n\nstatic void bcmgenet_fini_rx_napi(struct bcmgenet_priv *priv)\n{\n\tunsigned int i;\n\tstruct bcmgenet_rx_ring *ring;\n\n\tfor (i = 0; i < priv->hw_params->rx_queues; ++i) {\n\t\tring = &priv->rx_rings[i];\n\t\tnetif_napi_del(&ring->napi);\n\t}\n\n\tring = &priv->rx_rings[DESC_INDEX];\n\tnetif_napi_del(&ring->napi);\n}\n\n \nstatic int bcmgenet_init_rx_queues(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tu32 i;\n\tu32 dma_enable;\n\tu32 dma_ctrl;\n\tu32 ring_cfg;\n\tint ret;\n\n\tdma_ctrl = bcmgenet_rdma_readl(priv, DMA_CTRL);\n\tdma_enable = dma_ctrl & DMA_EN;\n\tdma_ctrl &= ~DMA_EN;\n\tbcmgenet_rdma_writel(priv, dma_ctrl, DMA_CTRL);\n\n\tdma_ctrl = 0;\n\tring_cfg = 0;\n\n\t \n\tfor (i = 0; i < priv->hw_params->rx_queues; i++) {\n\t\tret = bcmgenet_init_rx_ring(priv, i,\n\t\t\t\t\t    priv->hw_params->rx_bds_per_q,\n\t\t\t\t\t    i * priv->hw_params->rx_bds_per_q,\n\t\t\t\t\t    (i + 1) *\n\t\t\t\t\t    priv->hw_params->rx_bds_per_q);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tring_cfg |= (1 << i);\n\t\tdma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\n\t}\n\n\t \n\tret = bcmgenet_init_rx_ring(priv, DESC_INDEX, GENET_Q16_RX_BD_CNT,\n\t\t\t\t    priv->hw_params->rx_queues *\n\t\t\t\t    priv->hw_params->rx_bds_per_q,\n\t\t\t\t    TOTAL_DESC);\n\tif (ret)\n\t\treturn ret;\n\n\tring_cfg |= (1 << DESC_INDEX);\n\tdma_ctrl |= (1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT));\n\n\t \n\tbcmgenet_rdma_writel(priv, ring_cfg, DMA_RING_CFG);\n\n\t \n\tif (dma_enable)\n\t\tdma_ctrl |= DMA_EN;\n\tbcmgenet_rdma_writel(priv, dma_ctrl, DMA_CTRL);\n\n\treturn 0;\n}\n\nstatic int bcmgenet_dma_teardown(struct bcmgenet_priv *priv)\n{\n\tint ret = 0;\n\tint timeout = 0;\n\tu32 reg;\n\tu32 dma_ctrl;\n\tint i;\n\n\t \n\treg = bcmgenet_tdma_readl(priv, DMA_CTRL);\n\treg &= ~DMA_EN;\n\tbcmgenet_tdma_writel(priv, reg, DMA_CTRL);\n\n\t \n\twhile (timeout++ < DMA_TIMEOUT_VAL) {\n\t\treg = bcmgenet_tdma_readl(priv, DMA_STATUS);\n\t\tif (reg & DMA_DISABLED)\n\t\t\tbreak;\n\n\t\tudelay(1);\n\t}\n\n\tif (timeout == DMA_TIMEOUT_VAL) {\n\t\tnetdev_warn(priv->dev, \"Timed out while disabling TX DMA\\n\");\n\t\tret = -ETIMEDOUT;\n\t}\n\n\t \n\tusleep_range(10000, 20000);\n\n\t \n\treg = bcmgenet_rdma_readl(priv, DMA_CTRL);\n\treg &= ~DMA_EN;\n\tbcmgenet_rdma_writel(priv, reg, DMA_CTRL);\n\n\ttimeout = 0;\n\t \n\twhile (timeout++ < DMA_TIMEOUT_VAL) {\n\t\treg = bcmgenet_rdma_readl(priv, DMA_STATUS);\n\t\tif (reg & DMA_DISABLED)\n\t\t\tbreak;\n\n\t\tudelay(1);\n\t}\n\n\tif (timeout == DMA_TIMEOUT_VAL) {\n\t\tnetdev_warn(priv->dev, \"Timed out while disabling RX DMA\\n\");\n\t\tret = -ETIMEDOUT;\n\t}\n\n\tdma_ctrl = 0;\n\tfor (i = 0; i < priv->hw_params->rx_queues; i++)\n\t\tdma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\n\treg = bcmgenet_rdma_readl(priv, DMA_CTRL);\n\treg &= ~dma_ctrl;\n\tbcmgenet_rdma_writel(priv, reg, DMA_CTRL);\n\n\tdma_ctrl = 0;\n\tfor (i = 0; i < priv->hw_params->tx_queues; i++)\n\t\tdma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\n\treg = bcmgenet_tdma_readl(priv, DMA_CTRL);\n\treg &= ~dma_ctrl;\n\tbcmgenet_tdma_writel(priv, reg, DMA_CTRL);\n\n\treturn ret;\n}\n\nstatic void bcmgenet_fini_dma(struct bcmgenet_priv *priv)\n{\n\tstruct netdev_queue *txq;\n\tint i;\n\n\tbcmgenet_fini_rx_napi(priv);\n\tbcmgenet_fini_tx_napi(priv);\n\n\tfor (i = 0; i < priv->num_tx_bds; i++)\n\t\tdev_kfree_skb(bcmgenet_free_tx_cb(&priv->pdev->dev,\n\t\t\t\t\t\t  priv->tx_cbs + i));\n\n\tfor (i = 0; i < priv->hw_params->tx_queues; i++) {\n\t\ttxq = netdev_get_tx_queue(priv->dev, priv->tx_rings[i].queue);\n\t\tnetdev_tx_reset_queue(txq);\n\t}\n\n\ttxq = netdev_get_tx_queue(priv->dev, priv->tx_rings[DESC_INDEX].queue);\n\tnetdev_tx_reset_queue(txq);\n\n\tbcmgenet_free_rx_buffers(priv);\n\tkfree(priv->rx_cbs);\n\tkfree(priv->tx_cbs);\n}\n\n \nstatic int bcmgenet_init_dma(struct bcmgenet_priv *priv)\n{\n\tint ret;\n\tunsigned int i;\n\tstruct enet_cb *cb;\n\n\tnetif_dbg(priv, hw, priv->dev, \"%s\\n\", __func__);\n\n\t \n\tpriv->rx_bds = priv->base + priv->hw_params->rdma_offset;\n\tpriv->num_rx_bds = TOTAL_DESC;\n\tpriv->rx_cbs = kcalloc(priv->num_rx_bds, sizeof(struct enet_cb),\n\t\t\t       GFP_KERNEL);\n\tif (!priv->rx_cbs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < priv->num_rx_bds; i++) {\n\t\tcb = priv->rx_cbs + i;\n\t\tcb->bd_addr = priv->rx_bds + i * DMA_DESC_SIZE;\n\t}\n\n\t \n\tpriv->tx_bds = priv->base + priv->hw_params->tdma_offset;\n\tpriv->num_tx_bds = TOTAL_DESC;\n\tpriv->tx_cbs = kcalloc(priv->num_tx_bds, sizeof(struct enet_cb),\n\t\t\t       GFP_KERNEL);\n\tif (!priv->tx_cbs) {\n\t\tkfree(priv->rx_cbs);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < priv->num_tx_bds; i++) {\n\t\tcb = priv->tx_cbs + i;\n\t\tcb->bd_addr = priv->tx_bds + i * DMA_DESC_SIZE;\n\t}\n\n\t \n\tbcmgenet_rdma_writel(priv, priv->dma_max_burst_length,\n\t\t\t     DMA_SCB_BURST_SIZE);\n\n\t \n\tret = bcmgenet_init_rx_queues(priv->dev);\n\tif (ret) {\n\t\tnetdev_err(priv->dev, \"failed to initialize Rx queues\\n\");\n\t\tbcmgenet_free_rx_buffers(priv);\n\t\tkfree(priv->rx_cbs);\n\t\tkfree(priv->tx_cbs);\n\t\treturn ret;\n\t}\n\n\t \n\tbcmgenet_tdma_writel(priv, priv->dma_max_burst_length,\n\t\t\t     DMA_SCB_BURST_SIZE);\n\n\t \n\tbcmgenet_init_tx_queues(priv->dev);\n\n\treturn 0;\n}\n\n \nstatic void bcmgenet_irq_task(struct work_struct *work)\n{\n\tunsigned int status;\n\tstruct bcmgenet_priv *priv = container_of(\n\t\t\twork, struct bcmgenet_priv, bcmgenet_irq_work);\n\n\tnetif_dbg(priv, intr, priv->dev, \"%s\\n\", __func__);\n\n\tspin_lock_irq(&priv->lock);\n\tstatus = priv->irq0_stat;\n\tpriv->irq0_stat = 0;\n\tspin_unlock_irq(&priv->lock);\n\n\tif (status & UMAC_IRQ_PHY_DET_R &&\n\t    priv->dev->phydev->autoneg != AUTONEG_ENABLE) {\n\t\tphy_init_hw(priv->dev->phydev);\n\t\tgenphy_config_aneg(priv->dev->phydev);\n\t}\n\n\t \n\tif (status & UMAC_IRQ_LINK_EVENT)\n\t\tphy_mac_interrupt(priv->dev->phydev);\n\n}\n\n \nstatic irqreturn_t bcmgenet_isr1(int irq, void *dev_id)\n{\n\tstruct bcmgenet_priv *priv = dev_id;\n\tstruct bcmgenet_rx_ring *rx_ring;\n\tstruct bcmgenet_tx_ring *tx_ring;\n\tunsigned int index, status;\n\n\t \n\tstatus = bcmgenet_intrl2_1_readl(priv, INTRL2_CPU_STAT) &\n\t\t~bcmgenet_intrl2_1_readl(priv, INTRL2_CPU_MASK_STATUS);\n\n\t \n\tbcmgenet_intrl2_1_writel(priv, status, INTRL2_CPU_CLEAR);\n\n\tnetif_dbg(priv, intr, priv->dev,\n\t\t  \"%s: IRQ=0x%x\\n\", __func__, status);\n\n\t \n\tfor (index = 0; index < priv->hw_params->rx_queues; index++) {\n\t\tif (!(status & BIT(UMAC_IRQ1_RX_INTR_SHIFT + index)))\n\t\t\tcontinue;\n\n\t\trx_ring = &priv->rx_rings[index];\n\t\trx_ring->dim.event_ctr++;\n\n\t\tif (likely(napi_schedule_prep(&rx_ring->napi))) {\n\t\t\trx_ring->int_disable(rx_ring);\n\t\t\t__napi_schedule_irqoff(&rx_ring->napi);\n\t\t}\n\t}\n\n\t \n\tfor (index = 0; index < priv->hw_params->tx_queues; index++) {\n\t\tif (!(status & BIT(index)))\n\t\t\tcontinue;\n\n\t\ttx_ring = &priv->tx_rings[index];\n\n\t\tif (likely(napi_schedule_prep(&tx_ring->napi))) {\n\t\t\ttx_ring->int_disable(tx_ring);\n\t\t\t__napi_schedule_irqoff(&tx_ring->napi);\n\t\t}\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t bcmgenet_isr0(int irq, void *dev_id)\n{\n\tstruct bcmgenet_priv *priv = dev_id;\n\tstruct bcmgenet_rx_ring *rx_ring;\n\tstruct bcmgenet_tx_ring *tx_ring;\n\tunsigned int status;\n\tunsigned long flags;\n\n\t \n\tstatus = bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_STAT) &\n\t\t~bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_MASK_STATUS);\n\n\t \n\tbcmgenet_intrl2_0_writel(priv, status, INTRL2_CPU_CLEAR);\n\n\tnetif_dbg(priv, intr, priv->dev,\n\t\t  \"IRQ=0x%x\\n\", status);\n\n\tif (status & UMAC_IRQ_RXDMA_DONE) {\n\t\trx_ring = &priv->rx_rings[DESC_INDEX];\n\t\trx_ring->dim.event_ctr++;\n\n\t\tif (likely(napi_schedule_prep(&rx_ring->napi))) {\n\t\t\trx_ring->int_disable(rx_ring);\n\t\t\t__napi_schedule_irqoff(&rx_ring->napi);\n\t\t}\n\t}\n\n\tif (status & UMAC_IRQ_TXDMA_DONE) {\n\t\ttx_ring = &priv->tx_rings[DESC_INDEX];\n\n\t\tif (likely(napi_schedule_prep(&tx_ring->napi))) {\n\t\t\ttx_ring->int_disable(tx_ring);\n\t\t\t__napi_schedule_irqoff(&tx_ring->napi);\n\t\t}\n\t}\n\n\tif ((priv->hw_params->flags & GENET_HAS_MDIO_INTR) &&\n\t\tstatus & (UMAC_IRQ_MDIO_DONE | UMAC_IRQ_MDIO_ERROR)) {\n\t\twake_up(&priv->wq);\n\t}\n\n\t \n\tstatus &= (UMAC_IRQ_LINK_EVENT | UMAC_IRQ_PHY_DET_R);\n\tif (status) {\n\t\t \n\t\tspin_lock_irqsave(&priv->lock, flags);\n\t\tpriv->irq0_stat |= status;\n\t\tspin_unlock_irqrestore(&priv->lock, flags);\n\n\t\tschedule_work(&priv->bcmgenet_irq_work);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t bcmgenet_wol_isr(int irq, void *dev_id)\n{\n\t \n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void bcmgenet_poll_controller(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\t \n\tdisable_irq(priv->irq0);\n\tbcmgenet_isr0(priv->irq0, priv);\n\tenable_irq(priv->irq0);\n\n\t \n\tdisable_irq(priv->irq1);\n\tbcmgenet_isr1(priv->irq1, priv);\n\tenable_irq(priv->irq1);\n}\n#endif\n\nstatic void bcmgenet_umac_reset(struct bcmgenet_priv *priv)\n{\n\tu32 reg;\n\n\treg = bcmgenet_rbuf_ctrl_get(priv);\n\treg |= BIT(1);\n\tbcmgenet_rbuf_ctrl_set(priv, reg);\n\tudelay(10);\n\n\treg &= ~BIT(1);\n\tbcmgenet_rbuf_ctrl_set(priv, reg);\n\tudelay(10);\n}\n\nstatic void bcmgenet_set_hw_addr(struct bcmgenet_priv *priv,\n\t\t\t\t const unsigned char *addr)\n{\n\tbcmgenet_umac_writel(priv, get_unaligned_be32(&addr[0]), UMAC_MAC0);\n\tbcmgenet_umac_writel(priv, get_unaligned_be16(&addr[4]), UMAC_MAC1);\n}\n\nstatic void bcmgenet_get_hw_addr(struct bcmgenet_priv *priv,\n\t\t\t\t unsigned char *addr)\n{\n\tu32 addr_tmp;\n\n\taddr_tmp = bcmgenet_umac_readl(priv, UMAC_MAC0);\n\tput_unaligned_be32(addr_tmp, &addr[0]);\n\taddr_tmp = bcmgenet_umac_readl(priv, UMAC_MAC1);\n\tput_unaligned_be16(addr_tmp, &addr[4]);\n}\n\n \nstatic u32 bcmgenet_dma_disable(struct bcmgenet_priv *priv)\n{\n\tunsigned int i;\n\tu32 reg;\n\tu32 dma_ctrl;\n\n\t \n\tdma_ctrl = 1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT) | DMA_EN;\n\tfor (i = 0; i < priv->hw_params->tx_queues; i++)\n\t\tdma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\n\treg = bcmgenet_tdma_readl(priv, DMA_CTRL);\n\treg &= ~dma_ctrl;\n\tbcmgenet_tdma_writel(priv, reg, DMA_CTRL);\n\n\tdma_ctrl = 1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT) | DMA_EN;\n\tfor (i = 0; i < priv->hw_params->rx_queues; i++)\n\t\tdma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\n\treg = bcmgenet_rdma_readl(priv, DMA_CTRL);\n\treg &= ~dma_ctrl;\n\tbcmgenet_rdma_writel(priv, reg, DMA_CTRL);\n\n\tbcmgenet_umac_writel(priv, 1, UMAC_TX_FLUSH);\n\tudelay(10);\n\tbcmgenet_umac_writel(priv, 0, UMAC_TX_FLUSH);\n\n\treturn dma_ctrl;\n}\n\nstatic void bcmgenet_enable_dma(struct bcmgenet_priv *priv, u32 dma_ctrl)\n{\n\tu32 reg;\n\n\treg = bcmgenet_rdma_readl(priv, DMA_CTRL);\n\treg |= dma_ctrl;\n\tbcmgenet_rdma_writel(priv, reg, DMA_CTRL);\n\n\treg = bcmgenet_tdma_readl(priv, DMA_CTRL);\n\treg |= dma_ctrl;\n\tbcmgenet_tdma_writel(priv, reg, DMA_CTRL);\n}\n\nstatic void bcmgenet_netif_start(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\t \n\tbcmgenet_set_rx_mode(dev);\n\tbcmgenet_enable_rx_napi(priv);\n\n\tumac_enable_set(priv, CMD_TX_EN | CMD_RX_EN, true);\n\n\tbcmgenet_enable_tx_napi(priv);\n\n\t \n\tbcmgenet_link_intr_enable(priv);\n\n\tphy_start(dev->phydev);\n}\n\nstatic int bcmgenet_open(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tunsigned long dma_ctrl;\n\tint ret;\n\n\tnetif_dbg(priv, ifup, dev, \"bcmgenet_open\\n\");\n\n\t \n\tclk_prepare_enable(priv->clk);\n\n\t \n\tif (priv->internal_phy)\n\t\tbcmgenet_power_up(priv, GENET_POWER_PASSIVE);\n\n\t \n\tbcmgenet_umac_reset(priv);\n\n\tinit_umac(priv);\n\n\t \n\tbcmgenet_set_features(dev, dev->features);\n\n\tbcmgenet_set_hw_addr(priv, dev->dev_addr);\n\n\t \n\tdma_ctrl = bcmgenet_dma_disable(priv);\n\n\t \n\tret = bcmgenet_init_dma(priv);\n\tif (ret) {\n\t\tnetdev_err(dev, \"failed to initialize DMA\\n\");\n\t\tgoto err_clk_disable;\n\t}\n\n\t \n\tbcmgenet_enable_dma(priv, dma_ctrl);\n\n\t \n\tbcmgenet_hfb_init(priv);\n\n\tret = request_irq(priv->irq0, bcmgenet_isr0, IRQF_SHARED,\n\t\t\t  dev->name, priv);\n\tif (ret < 0) {\n\t\tnetdev_err(dev, \"can't request IRQ %d\\n\", priv->irq0);\n\t\tgoto err_fini_dma;\n\t}\n\n\tret = request_irq(priv->irq1, bcmgenet_isr1, IRQF_SHARED,\n\t\t\t  dev->name, priv);\n\tif (ret < 0) {\n\t\tnetdev_err(dev, \"can't request IRQ %d\\n\", priv->irq1);\n\t\tgoto err_irq0;\n\t}\n\n\tret = bcmgenet_mii_probe(dev);\n\tif (ret) {\n\t\tnetdev_err(dev, \"failed to connect to PHY\\n\");\n\t\tgoto err_irq1;\n\t}\n\n\tbcmgenet_phy_pause_set(dev, priv->rx_pause, priv->tx_pause);\n\n\tbcmgenet_netif_start(dev);\n\n\tnetif_tx_start_all_queues(dev);\n\n\treturn 0;\n\nerr_irq1:\n\tfree_irq(priv->irq1, priv);\nerr_irq0:\n\tfree_irq(priv->irq0, priv);\nerr_fini_dma:\n\tbcmgenet_dma_teardown(priv);\n\tbcmgenet_fini_dma(priv);\nerr_clk_disable:\n\tif (priv->internal_phy)\n\t\tbcmgenet_power_down(priv, GENET_POWER_PASSIVE);\n\tclk_disable_unprepare(priv->clk);\n\treturn ret;\n}\n\nstatic void bcmgenet_netif_stop(struct net_device *dev, bool stop_phy)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\tbcmgenet_disable_tx_napi(priv);\n\tnetif_tx_disable(dev);\n\n\t \n\tumac_enable_set(priv, CMD_RX_EN, false);\n\n\tbcmgenet_dma_teardown(priv);\n\n\t \n\tumac_enable_set(priv, CMD_TX_EN, false);\n\n\tif (stop_phy)\n\t\tphy_stop(dev->phydev);\n\tbcmgenet_disable_rx_napi(priv);\n\tbcmgenet_intr_disable(priv);\n\n\t \n\tcancel_work_sync(&priv->bcmgenet_irq_work);\n\n\t \n\tbcmgenet_tx_reclaim_all(dev);\n\tbcmgenet_fini_dma(priv);\n}\n\nstatic int bcmgenet_close(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tint ret = 0;\n\n\tnetif_dbg(priv, ifdown, dev, \"bcmgenet_close\\n\");\n\n\tbcmgenet_netif_stop(dev, false);\n\n\t \n\tphy_disconnect(dev->phydev);\n\n\tfree_irq(priv->irq0, priv);\n\tfree_irq(priv->irq1, priv);\n\n\tif (priv->internal_phy)\n\t\tret = bcmgenet_power_down(priv, GENET_POWER_PASSIVE);\n\n\tclk_disable_unprepare(priv->clk);\n\n\treturn ret;\n}\n\nstatic void bcmgenet_dump_tx_queue(struct bcmgenet_tx_ring *ring)\n{\n\tstruct bcmgenet_priv *priv = ring->priv;\n\tu32 p_index, c_index, intsts, intmsk;\n\tstruct netdev_queue *txq;\n\tunsigned int free_bds;\n\tbool txq_stopped;\n\n\tif (!netif_msg_tx_err(priv))\n\t\treturn;\n\n\ttxq = netdev_get_tx_queue(priv->dev, ring->queue);\n\n\tspin_lock(&ring->lock);\n\tif (ring->index == DESC_INDEX) {\n\t\tintsts = ~bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_MASK_STATUS);\n\t\tintmsk = UMAC_IRQ_TXDMA_DONE | UMAC_IRQ_TXDMA_MBDONE;\n\t} else {\n\t\tintsts = ~bcmgenet_intrl2_1_readl(priv, INTRL2_CPU_MASK_STATUS);\n\t\tintmsk = 1 << ring->index;\n\t}\n\tc_index = bcmgenet_tdma_ring_readl(priv, ring->index, TDMA_CONS_INDEX);\n\tp_index = bcmgenet_tdma_ring_readl(priv, ring->index, TDMA_PROD_INDEX);\n\ttxq_stopped = netif_tx_queue_stopped(txq);\n\tfree_bds = ring->free_bds;\n\tspin_unlock(&ring->lock);\n\n\tnetif_err(priv, tx_err, priv->dev, \"Ring %d queue %d status summary\\n\"\n\t\t  \"TX queue status: %s, interrupts: %s\\n\"\n\t\t  \"(sw)free_bds: %d (sw)size: %d\\n\"\n\t\t  \"(sw)p_index: %d (hw)p_index: %d\\n\"\n\t\t  \"(sw)c_index: %d (hw)c_index: %d\\n\"\n\t\t  \"(sw)clean_p: %d (sw)write_p: %d\\n\"\n\t\t  \"(sw)cb_ptr: %d (sw)end_ptr: %d\\n\",\n\t\t  ring->index, ring->queue,\n\t\t  txq_stopped ? \"stopped\" : \"active\",\n\t\t  intsts & intmsk ? \"enabled\" : \"disabled\",\n\t\t  free_bds, ring->size,\n\t\t  ring->prod_index, p_index & DMA_P_INDEX_MASK,\n\t\t  ring->c_index, c_index & DMA_C_INDEX_MASK,\n\t\t  ring->clean_ptr, ring->write_ptr,\n\t\t  ring->cb_ptr, ring->end_ptr);\n}\n\nstatic void bcmgenet_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tu32 int0_enable = 0;\n\tu32 int1_enable = 0;\n\tunsigned int q;\n\n\tnetif_dbg(priv, tx_err, dev, \"bcmgenet_timeout\\n\");\n\n\tfor (q = 0; q < priv->hw_params->tx_queues; q++)\n\t\tbcmgenet_dump_tx_queue(&priv->tx_rings[q]);\n\tbcmgenet_dump_tx_queue(&priv->tx_rings[DESC_INDEX]);\n\n\tbcmgenet_tx_reclaim_all(dev);\n\n\tfor (q = 0; q < priv->hw_params->tx_queues; q++)\n\t\tint1_enable |= (1 << q);\n\n\tint0_enable = UMAC_IRQ_TXDMA_DONE;\n\n\t \n\tbcmgenet_intrl2_0_writel(priv, int0_enable, INTRL2_CPU_MASK_CLEAR);\n\tbcmgenet_intrl2_1_writel(priv, int1_enable, INTRL2_CPU_MASK_CLEAR);\n\n\tnetif_trans_update(dev);\n\n\tdev->stats.tx_errors++;\n\n\tnetif_tx_wake_all_queues(dev);\n}\n\n#define MAX_MDF_FILTER\t17\n\nstatic inline void bcmgenet_set_mdf_addr(struct bcmgenet_priv *priv,\n\t\t\t\t\t const unsigned char *addr,\n\t\t\t\t\t int *i)\n{\n\tbcmgenet_umac_writel(priv, addr[0] << 8 | addr[1],\n\t\t\t     UMAC_MDF_ADDR + (*i * 4));\n\tbcmgenet_umac_writel(priv, addr[2] << 24 | addr[3] << 16 |\n\t\t\t     addr[4] << 8 | addr[5],\n\t\t\t     UMAC_MDF_ADDR + ((*i + 1) * 4));\n\t*i += 2;\n}\n\nstatic void bcmgenet_set_rx_mode(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct netdev_hw_addr *ha;\n\tint i, nfilter;\n\tu32 reg;\n\n\tnetif_dbg(priv, hw, dev, \"%s: %08X\\n\", __func__, dev->flags);\n\n\t \n\tnfilter = netdev_uc_count(dev) + netdev_mc_count(dev) + 2;\n\n\t \n\treg = bcmgenet_umac_readl(priv, UMAC_CMD);\n\tif ((dev->flags & (IFF_PROMISC | IFF_ALLMULTI)) ||\n\t    (nfilter > MAX_MDF_FILTER)) {\n\t\treg |= CMD_PROMISC;\n\t\tbcmgenet_umac_writel(priv, reg, UMAC_CMD);\n\t\tbcmgenet_umac_writel(priv, 0, UMAC_MDF_CTRL);\n\t\treturn;\n\t} else {\n\t\treg &= ~CMD_PROMISC;\n\t\tbcmgenet_umac_writel(priv, reg, UMAC_CMD);\n\t}\n\n\t \n\ti = 0;\n\t \n\tbcmgenet_set_mdf_addr(priv, dev->broadcast, &i);\n\t \n\tbcmgenet_set_mdf_addr(priv, dev->dev_addr, &i);\n\n\t \n\tnetdev_for_each_uc_addr(ha, dev)\n\t\tbcmgenet_set_mdf_addr(priv, ha->addr, &i);\n\n\t \n\tnetdev_for_each_mc_addr(ha, dev)\n\t\tbcmgenet_set_mdf_addr(priv, ha->addr, &i);\n\n\t \n\treg = GENMASK(MAX_MDF_FILTER - 1, MAX_MDF_FILTER - nfilter);\n\tbcmgenet_umac_writel(priv, reg, UMAC_MDF_CTRL);\n}\n\n \nstatic int bcmgenet_set_mac_addr(struct net_device *dev, void *p)\n{\n\tstruct sockaddr *addr = p;\n\n\t \n\tif (netif_running(dev))\n\t\treturn -EBUSY;\n\n\teth_hw_addr_set(dev, addr->sa_data);\n\n\treturn 0;\n}\n\nstatic struct net_device_stats *bcmgenet_get_stats(struct net_device *dev)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tunsigned long tx_bytes = 0, tx_packets = 0;\n\tunsigned long rx_bytes = 0, rx_packets = 0;\n\tunsigned long rx_errors = 0, rx_dropped = 0;\n\tstruct bcmgenet_tx_ring *tx_ring;\n\tstruct bcmgenet_rx_ring *rx_ring;\n\tunsigned int q;\n\n\tfor (q = 0; q < priv->hw_params->tx_queues; q++) {\n\t\ttx_ring = &priv->tx_rings[q];\n\t\ttx_bytes += tx_ring->bytes;\n\t\ttx_packets += tx_ring->packets;\n\t}\n\ttx_ring = &priv->tx_rings[DESC_INDEX];\n\ttx_bytes += tx_ring->bytes;\n\ttx_packets += tx_ring->packets;\n\n\tfor (q = 0; q < priv->hw_params->rx_queues; q++) {\n\t\trx_ring = &priv->rx_rings[q];\n\n\t\trx_bytes += rx_ring->bytes;\n\t\trx_packets += rx_ring->packets;\n\t\trx_errors += rx_ring->errors;\n\t\trx_dropped += rx_ring->dropped;\n\t}\n\trx_ring = &priv->rx_rings[DESC_INDEX];\n\trx_bytes += rx_ring->bytes;\n\trx_packets += rx_ring->packets;\n\trx_errors += rx_ring->errors;\n\trx_dropped += rx_ring->dropped;\n\n\tdev->stats.tx_bytes = tx_bytes;\n\tdev->stats.tx_packets = tx_packets;\n\tdev->stats.rx_bytes = rx_bytes;\n\tdev->stats.rx_packets = rx_packets;\n\tdev->stats.rx_errors = rx_errors;\n\tdev->stats.rx_missed_errors = rx_errors;\n\tdev->stats.rx_dropped = rx_dropped;\n\treturn &dev->stats;\n}\n\nstatic int bcmgenet_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\tif (!dev->phydev || !phy_is_pseudo_fixed_link(dev->phydev) ||\n\t    priv->phy_interface != PHY_INTERFACE_MODE_MOCA)\n\t\treturn -EOPNOTSUPP;\n\n\tif (new_carrier)\n\t\tnetif_carrier_on(dev);\n\telse\n\t\tnetif_carrier_off(dev);\n\n\treturn 0;\n}\n\nstatic const struct net_device_ops bcmgenet_netdev_ops = {\n\t.ndo_open\t\t= bcmgenet_open,\n\t.ndo_stop\t\t= bcmgenet_close,\n\t.ndo_start_xmit\t\t= bcmgenet_xmit,\n\t.ndo_tx_timeout\t\t= bcmgenet_timeout,\n\t.ndo_set_rx_mode\t= bcmgenet_set_rx_mode,\n\t.ndo_set_mac_address\t= bcmgenet_set_mac_addr,\n\t.ndo_eth_ioctl\t\t= phy_do_ioctl_running,\n\t.ndo_set_features\t= bcmgenet_set_features,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= bcmgenet_poll_controller,\n#endif\n\t.ndo_get_stats\t\t= bcmgenet_get_stats,\n\t.ndo_change_carrier\t= bcmgenet_change_carrier,\n};\n\n \nstatic struct bcmgenet_hw_params bcmgenet_hw_params[] = {\n\t[GENET_V1] = {\n\t\t.tx_queues = 0,\n\t\t.tx_bds_per_q = 0,\n\t\t.rx_queues = 0,\n\t\t.rx_bds_per_q = 0,\n\t\t.bp_in_en_shift = 16,\n\t\t.bp_in_mask = 0xffff,\n\t\t.hfb_filter_cnt = 16,\n\t\t.qtag_mask = 0x1F,\n\t\t.hfb_offset = 0x1000,\n\t\t.rdma_offset = 0x2000,\n\t\t.tdma_offset = 0x3000,\n\t\t.words_per_bd = 2,\n\t},\n\t[GENET_V2] = {\n\t\t.tx_queues = 4,\n\t\t.tx_bds_per_q = 32,\n\t\t.rx_queues = 0,\n\t\t.rx_bds_per_q = 0,\n\t\t.bp_in_en_shift = 16,\n\t\t.bp_in_mask = 0xffff,\n\t\t.hfb_filter_cnt = 16,\n\t\t.qtag_mask = 0x1F,\n\t\t.tbuf_offset = 0x0600,\n\t\t.hfb_offset = 0x1000,\n\t\t.hfb_reg_offset = 0x2000,\n\t\t.rdma_offset = 0x3000,\n\t\t.tdma_offset = 0x4000,\n\t\t.words_per_bd = 2,\n\t\t.flags = GENET_HAS_EXT,\n\t},\n\t[GENET_V3] = {\n\t\t.tx_queues = 4,\n\t\t.tx_bds_per_q = 32,\n\t\t.rx_queues = 0,\n\t\t.rx_bds_per_q = 0,\n\t\t.bp_in_en_shift = 17,\n\t\t.bp_in_mask = 0x1ffff,\n\t\t.hfb_filter_cnt = 48,\n\t\t.hfb_filter_size = 128,\n\t\t.qtag_mask = 0x3F,\n\t\t.tbuf_offset = 0x0600,\n\t\t.hfb_offset = 0x8000,\n\t\t.hfb_reg_offset = 0xfc00,\n\t\t.rdma_offset = 0x10000,\n\t\t.tdma_offset = 0x11000,\n\t\t.words_per_bd = 2,\n\t\t.flags = GENET_HAS_EXT | GENET_HAS_MDIO_INTR |\n\t\t\t GENET_HAS_MOCA_LINK_DET,\n\t},\n\t[GENET_V4] = {\n\t\t.tx_queues = 4,\n\t\t.tx_bds_per_q = 32,\n\t\t.rx_queues = 0,\n\t\t.rx_bds_per_q = 0,\n\t\t.bp_in_en_shift = 17,\n\t\t.bp_in_mask = 0x1ffff,\n\t\t.hfb_filter_cnt = 48,\n\t\t.hfb_filter_size = 128,\n\t\t.qtag_mask = 0x3F,\n\t\t.tbuf_offset = 0x0600,\n\t\t.hfb_offset = 0x8000,\n\t\t.hfb_reg_offset = 0xfc00,\n\t\t.rdma_offset = 0x2000,\n\t\t.tdma_offset = 0x4000,\n\t\t.words_per_bd = 3,\n\t\t.flags = GENET_HAS_40BITS | GENET_HAS_EXT |\n\t\t\t GENET_HAS_MDIO_INTR | GENET_HAS_MOCA_LINK_DET,\n\t},\n\t[GENET_V5] = {\n\t\t.tx_queues = 4,\n\t\t.tx_bds_per_q = 32,\n\t\t.rx_queues = 0,\n\t\t.rx_bds_per_q = 0,\n\t\t.bp_in_en_shift = 17,\n\t\t.bp_in_mask = 0x1ffff,\n\t\t.hfb_filter_cnt = 48,\n\t\t.hfb_filter_size = 128,\n\t\t.qtag_mask = 0x3F,\n\t\t.tbuf_offset = 0x0600,\n\t\t.hfb_offset = 0x8000,\n\t\t.hfb_reg_offset = 0xfc00,\n\t\t.rdma_offset = 0x2000,\n\t\t.tdma_offset = 0x4000,\n\t\t.words_per_bd = 3,\n\t\t.flags = GENET_HAS_40BITS | GENET_HAS_EXT |\n\t\t\t GENET_HAS_MDIO_INTR | GENET_HAS_MOCA_LINK_DET,\n\t},\n};\n\n \nstatic void bcmgenet_set_hw_params(struct bcmgenet_priv *priv)\n{\n\tstruct bcmgenet_hw_params *params;\n\tu32 reg;\n\tu8 major;\n\tu16 gphy_rev;\n\n\tif (GENET_IS_V5(priv) || GENET_IS_V4(priv)) {\n\t\tbcmgenet_dma_regs = bcmgenet_dma_regs_v3plus;\n\t\tgenet_dma_ring_regs = genet_dma_ring_regs_v4;\n\t} else if (GENET_IS_V3(priv)) {\n\t\tbcmgenet_dma_regs = bcmgenet_dma_regs_v3plus;\n\t\tgenet_dma_ring_regs = genet_dma_ring_regs_v123;\n\t} else if (GENET_IS_V2(priv)) {\n\t\tbcmgenet_dma_regs = bcmgenet_dma_regs_v2;\n\t\tgenet_dma_ring_regs = genet_dma_ring_regs_v123;\n\t} else if (GENET_IS_V1(priv)) {\n\t\tbcmgenet_dma_regs = bcmgenet_dma_regs_v1;\n\t\tgenet_dma_ring_regs = genet_dma_ring_regs_v123;\n\t}\n\n\t \n\tpriv->hw_params = &bcmgenet_hw_params[priv->version];\n\tparams = priv->hw_params;\n\n\t \n\treg = bcmgenet_sys_readl(priv, SYS_REV_CTRL);\n\tmajor = (reg >> 24 & 0x0f);\n\tif (major == 6)\n\t\tmajor = 5;\n\telse if (major == 5)\n\t\tmajor = 4;\n\telse if (major == 0)\n\t\tmajor = 1;\n\tif (major != priv->version) {\n\t\tdev_err(&priv->pdev->dev,\n\t\t\t\"GENET version mismatch, got: %d, configured for: %d\\n\",\n\t\t\tmajor, priv->version);\n\t}\n\n\t \n\tdev_info(&priv->pdev->dev, \"GENET \" GENET_VER_FMT,\n\t\t major, (reg >> 16) & 0x0f, reg & 0xffff);\n\n\t \n\tgphy_rev = reg & 0xffff;\n\n\tif (GENET_IS_V5(priv)) {\n\t\t \n\t\tif (gphy_rev != 0) {\n\t\t\tpr_warn(\"GENET is reporting EPHY revision: 0x%04x\\n\",\n\t\t\t\tgphy_rev);\n\t\t}\n\t \n\t} else if (gphy_rev == 0 || gphy_rev == 0x01ff) {\n\t\tpr_warn(\"Invalid GPHY revision detected: 0x%04x\\n\", gphy_rev);\n\t\treturn;\n\t \n\t} else if ((gphy_rev & 0xf0) != 0) {\n\t\tpriv->gphy_rev = gphy_rev << 8;\n\t \n\t} else if ((gphy_rev & 0xff00) != 0) {\n\t\tpriv->gphy_rev = gphy_rev;\n\t}\n\n#ifdef CONFIG_PHYS_ADDR_T_64BIT\n\tif (!(params->flags & GENET_HAS_40BITS))\n\t\tpr_warn(\"GENET does not support 40-bits PA\\n\");\n#endif\n\n\tpr_debug(\"Configuration for version: %d\\n\"\n\t\t\"TXq: %1d, TXqBDs: %1d, RXq: %1d, RXqBDs: %1d\\n\"\n\t\t\"BP << en: %2d, BP msk: 0x%05x\\n\"\n\t\t\"HFB count: %2d, QTAQ msk: 0x%05x\\n\"\n\t\t\"TBUF: 0x%04x, HFB: 0x%04x, HFBreg: 0x%04x\\n\"\n\t\t\"RDMA: 0x%05x, TDMA: 0x%05x\\n\"\n\t\t\"Words/BD: %d\\n\",\n\t\tpriv->version,\n\t\tparams->tx_queues, params->tx_bds_per_q,\n\t\tparams->rx_queues, params->rx_bds_per_q,\n\t\tparams->bp_in_en_shift, params->bp_in_mask,\n\t\tparams->hfb_filter_cnt, params->qtag_mask,\n\t\tparams->tbuf_offset, params->hfb_offset,\n\t\tparams->hfb_reg_offset,\n\t\tparams->rdma_offset, params->tdma_offset,\n\t\tparams->words_per_bd);\n}\n\nstruct bcmgenet_plat_data {\n\tenum bcmgenet_version version;\n\tu32 dma_max_burst_length;\n\tbool ephy_16nm;\n};\n\nstatic const struct bcmgenet_plat_data v1_plat_data = {\n\t.version = GENET_V1,\n\t.dma_max_burst_length = DMA_MAX_BURST_LENGTH,\n};\n\nstatic const struct bcmgenet_plat_data v2_plat_data = {\n\t.version = GENET_V2,\n\t.dma_max_burst_length = DMA_MAX_BURST_LENGTH,\n};\n\nstatic const struct bcmgenet_plat_data v3_plat_data = {\n\t.version = GENET_V3,\n\t.dma_max_burst_length = DMA_MAX_BURST_LENGTH,\n};\n\nstatic const struct bcmgenet_plat_data v4_plat_data = {\n\t.version = GENET_V4,\n\t.dma_max_burst_length = DMA_MAX_BURST_LENGTH,\n};\n\nstatic const struct bcmgenet_plat_data v5_plat_data = {\n\t.version = GENET_V5,\n\t.dma_max_burst_length = DMA_MAX_BURST_LENGTH,\n};\n\nstatic const struct bcmgenet_plat_data bcm2711_plat_data = {\n\t.version = GENET_V5,\n\t.dma_max_burst_length = 0x08,\n};\n\nstatic const struct bcmgenet_plat_data bcm7712_plat_data = {\n\t.version = GENET_V5,\n\t.dma_max_burst_length = DMA_MAX_BURST_LENGTH,\n\t.ephy_16nm = true,\n};\n\nstatic const struct of_device_id bcmgenet_match[] = {\n\t{ .compatible = \"brcm,genet-v1\", .data = &v1_plat_data },\n\t{ .compatible = \"brcm,genet-v2\", .data = &v2_plat_data },\n\t{ .compatible = \"brcm,genet-v3\", .data = &v3_plat_data },\n\t{ .compatible = \"brcm,genet-v4\", .data = &v4_plat_data },\n\t{ .compatible = \"brcm,genet-v5\", .data = &v5_plat_data },\n\t{ .compatible = \"brcm,bcm2711-genet-v5\", .data = &bcm2711_plat_data },\n\t{ .compatible = \"brcm,bcm7712-genet-v5\", .data = &bcm7712_plat_data },\n\t{ },\n};\nMODULE_DEVICE_TABLE(of, bcmgenet_match);\n\nstatic int bcmgenet_probe(struct platform_device *pdev)\n{\n\tstruct bcmgenet_platform_data *pd = pdev->dev.platform_data;\n\tconst struct bcmgenet_plat_data *pdata;\n\tstruct bcmgenet_priv *priv;\n\tstruct net_device *dev;\n\tunsigned int i;\n\tint err = -EIO;\n\n\t \n\tdev = alloc_etherdev_mqs(sizeof(*priv), GENET_MAX_MQ_CNT + 1,\n\t\t\t\t GENET_MAX_MQ_CNT + 1);\n\tif (!dev) {\n\t\tdev_err(&pdev->dev, \"can't allocate net device\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv = netdev_priv(dev);\n\tpriv->irq0 = platform_get_irq(pdev, 0);\n\tif (priv->irq0 < 0) {\n\t\terr = priv->irq0;\n\t\tgoto err;\n\t}\n\tpriv->irq1 = platform_get_irq(pdev, 1);\n\tif (priv->irq1 < 0) {\n\t\terr = priv->irq1;\n\t\tgoto err;\n\t}\n\tpriv->wol_irq = platform_get_irq_optional(pdev, 2);\n\tif (priv->wol_irq == -EPROBE_DEFER) {\n\t\terr = priv->wol_irq;\n\t\tgoto err;\n\t}\n\n\tpriv->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(priv->base)) {\n\t\terr = PTR_ERR(priv->base);\n\t\tgoto err;\n\t}\n\n\tspin_lock_init(&priv->lock);\n\n\t \n\tpriv->autoneg_pause = 1;\n\tpriv->tx_pause = 1;\n\tpriv->rx_pause = 1;\n\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\tdev_set_drvdata(&pdev->dev, dev);\n\tdev->watchdog_timeo = 2 * HZ;\n\tdev->ethtool_ops = &bcmgenet_ethtool_ops;\n\tdev->netdev_ops = &bcmgenet_netdev_ops;\n\n\tpriv->msg_enable = netif_msg_init(-1, GENET_MSG_DEFAULT);\n\n\t \n\tdev->features |= NETIF_F_SG | NETIF_F_HIGHDMA | NETIF_F_HW_CSUM |\n\t\t\t NETIF_F_RXCSUM;\n\tdev->hw_features |= dev->features;\n\tdev->vlan_features |= dev->features;\n\n\t \n\tpriv->wol_irq_disabled = true;\n\tif (priv->wol_irq > 0) {\n\t\terr = devm_request_irq(&pdev->dev, priv->wol_irq,\n\t\t\t\t       bcmgenet_wol_isr, 0, dev->name, priv);\n\t\tif (!err)\n\t\t\tdevice_set_wakeup_capable(&pdev->dev, 1);\n\t}\n\n\t \n\tdev->needed_headroom += 64;\n\n\tpriv->dev = dev;\n\tpriv->pdev = pdev;\n\n\tpdata = device_get_match_data(&pdev->dev);\n\tif (pdata) {\n\t\tpriv->version = pdata->version;\n\t\tpriv->dma_max_burst_length = pdata->dma_max_burst_length;\n\t\tpriv->ephy_16nm = pdata->ephy_16nm;\n\t} else {\n\t\tpriv->version = pd->genet_version;\n\t\tpriv->dma_max_burst_length = DMA_MAX_BURST_LENGTH;\n\t}\n\n\tpriv->clk = devm_clk_get_optional(&priv->pdev->dev, \"enet\");\n\tif (IS_ERR(priv->clk)) {\n\t\tdev_dbg(&priv->pdev->dev, \"failed to get enet clock\\n\");\n\t\terr = PTR_ERR(priv->clk);\n\t\tgoto err;\n\t}\n\n\terr = clk_prepare_enable(priv->clk);\n\tif (err)\n\t\tgoto err;\n\n\tbcmgenet_set_hw_params(priv);\n\n\terr = -EIO;\n\tif (priv->hw_params->flags & GENET_HAS_40BITS)\n\t\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(40));\n\tif (err)\n\t\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\tif (err)\n\t\tgoto err_clk_disable;\n\n\t \n\tinit_waitqueue_head(&priv->wq);\n\t \n\tpriv->rx_buf_len = RX_BUF_LENGTH;\n\tINIT_WORK(&priv->bcmgenet_irq_work, bcmgenet_irq_task);\n\n\tpriv->clk_wol = devm_clk_get_optional(&priv->pdev->dev, \"enet-wol\");\n\tif (IS_ERR(priv->clk_wol)) {\n\t\tdev_dbg(&priv->pdev->dev, \"failed to get enet-wol clock\\n\");\n\t\terr = PTR_ERR(priv->clk_wol);\n\t\tgoto err_clk_disable;\n\t}\n\n\tpriv->clk_eee = devm_clk_get_optional(&priv->pdev->dev, \"enet-eee\");\n\tif (IS_ERR(priv->clk_eee)) {\n\t\tdev_dbg(&priv->pdev->dev, \"failed to get enet-eee clock\\n\");\n\t\terr = PTR_ERR(priv->clk_eee);\n\t\tgoto err_clk_disable;\n\t}\n\n\t \n\tif (device_get_phy_mode(&pdev->dev) == PHY_INTERFACE_MODE_INTERNAL)\n\t\tbcmgenet_power_up(priv, GENET_POWER_PASSIVE);\n\n\tif (pd && !IS_ERR_OR_NULL(pd->mac_address))\n\t\teth_hw_addr_set(dev, pd->mac_address);\n\telse\n\t\tif (device_get_ethdev_address(&pdev->dev, dev))\n\t\t\tif (has_acpi_companion(&pdev->dev)) {\n\t\t\t\tu8 addr[ETH_ALEN];\n\n\t\t\t\tbcmgenet_get_hw_addr(priv, addr);\n\t\t\t\teth_hw_addr_set(dev, addr);\n\t\t\t}\n\n\tif (!is_valid_ether_addr(dev->dev_addr)) {\n\t\tdev_warn(&pdev->dev, \"using random Ethernet MAC\\n\");\n\t\teth_hw_addr_random(dev);\n\t}\n\n\treset_umac(priv);\n\n\terr = bcmgenet_mii_init(dev);\n\tif (err)\n\t\tgoto err_clk_disable;\n\n\t \n\tnetif_set_real_num_tx_queues(priv->dev, priv->hw_params->tx_queues + 1);\n\tnetif_set_real_num_rx_queues(priv->dev, priv->hw_params->rx_queues + 1);\n\n\t \n\tfor (i = 0; i < priv->hw_params->rx_queues; i++)\n\t\tpriv->rx_rings[i].rx_max_coalesced_frames = 1;\n\tpriv->rx_rings[DESC_INDEX].rx_max_coalesced_frames = 1;\n\n\t \n\tnetif_carrier_off(dev);\n\n\t \n\tclk_disable_unprepare(priv->clk);\n\n\terr = register_netdev(dev);\n\tif (err) {\n\t\tbcmgenet_mii_exit(dev);\n\t\tgoto err;\n\t}\n\n\treturn err;\n\nerr_clk_disable:\n\tclk_disable_unprepare(priv->clk);\nerr:\n\tfree_netdev(dev);\n\treturn err;\n}\n\nstatic int bcmgenet_remove(struct platform_device *pdev)\n{\n\tstruct bcmgenet_priv *priv = dev_to_priv(&pdev->dev);\n\n\tdev_set_drvdata(&pdev->dev, NULL);\n\tunregister_netdev(priv->dev);\n\tbcmgenet_mii_exit(priv->dev);\n\tfree_netdev(priv->dev);\n\n\treturn 0;\n}\n\nstatic void bcmgenet_shutdown(struct platform_device *pdev)\n{\n\tbcmgenet_remove(pdev);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int bcmgenet_resume_noirq(struct device *d)\n{\n\tstruct net_device *dev = dev_get_drvdata(d);\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tint ret;\n\tu32 reg;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\t \n\tret = clk_prepare_enable(priv->clk);\n\tif (ret)\n\t\treturn ret;\n\n\tif (device_may_wakeup(d) && priv->wolopts) {\n\t\t \n\t\treg = bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_STAT);\n\t\treg = bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_STAT);\n\t\tif (reg & UMAC_IRQ_WAKE_EVENT)\n\t\t\tpm_wakeup_event(&priv->pdev->dev, 0);\n\t}\n\n\tbcmgenet_intrl2_0_writel(priv, UMAC_IRQ_WAKE_EVENT, INTRL2_CPU_CLEAR);\n\n\treturn 0;\n}\n\nstatic int bcmgenet_resume(struct device *d)\n{\n\tstruct net_device *dev = dev_get_drvdata(d);\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tstruct bcmgenet_rxnfc_rule *rule;\n\tunsigned long dma_ctrl;\n\tint ret;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\t \n\tif (device_may_wakeup(d) && priv->wolopts)\n\t\tbcmgenet_power_up(priv, GENET_POWER_WOL_MAGIC);\n\n\t \n\tif (priv->internal_phy)\n\t\tbcmgenet_power_up(priv, GENET_POWER_PASSIVE);\n\n\tbcmgenet_umac_reset(priv);\n\n\tinit_umac(priv);\n\n\tphy_init_hw(dev->phydev);\n\n\t \n\tgenphy_config_aneg(dev->phydev);\n\tbcmgenet_mii_config(priv->dev, false);\n\n\t \n\tbcmgenet_set_features(dev, dev->features);\n\n\tbcmgenet_set_hw_addr(priv, dev->dev_addr);\n\n\t \n\tbcmgenet_hfb_clear(priv);\n\tlist_for_each_entry(rule, &priv->rxnfc_list, list)\n\t\tif (rule->state != BCMGENET_RXNFC_STATE_UNUSED)\n\t\t\tbcmgenet_hfb_create_rxnfc_filter(priv, rule);\n\n\t \n\tdma_ctrl = bcmgenet_dma_disable(priv);\n\n\t \n\tret = bcmgenet_init_dma(priv);\n\tif (ret) {\n\t\tnetdev_err(dev, \"failed to initialize DMA\\n\");\n\t\tgoto out_clk_disable;\n\t}\n\n\t \n\tbcmgenet_enable_dma(priv, dma_ctrl);\n\n\tif (!device_may_wakeup(d))\n\t\tphy_resume(dev->phydev);\n\n\tbcmgenet_netif_start(dev);\n\n\tnetif_device_attach(dev);\n\n\treturn 0;\n\nout_clk_disable:\n\tif (priv->internal_phy)\n\t\tbcmgenet_power_down(priv, GENET_POWER_PASSIVE);\n\tclk_disable_unprepare(priv->clk);\n\treturn ret;\n}\n\nstatic int bcmgenet_suspend(struct device *d)\n{\n\tstruct net_device *dev = dev_get_drvdata(d);\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tnetif_device_detach(dev);\n\n\tbcmgenet_netif_stop(dev, true);\n\n\tif (!device_may_wakeup(d))\n\t\tphy_suspend(dev->phydev);\n\n\t \n\tbcmgenet_hfb_reg_writel(priv, 0, HFB_CTRL);\n\n\treturn 0;\n}\n\nstatic int bcmgenet_suspend_noirq(struct device *d)\n{\n\tstruct net_device *dev = dev_get_drvdata(d);\n\tstruct bcmgenet_priv *priv = netdev_priv(dev);\n\tint ret = 0;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\t \n\tif (device_may_wakeup(d) && priv->wolopts)\n\t\tret = bcmgenet_power_down(priv, GENET_POWER_WOL_MAGIC);\n\telse if (priv->internal_phy)\n\t\tret = bcmgenet_power_down(priv, GENET_POWER_PASSIVE);\n\n\t \n\tif (ret)\n\t\treturn ret;\n\n\t \n\tclk_disable_unprepare(priv->clk);\n\n\treturn 0;\n}\n#else\n#define bcmgenet_suspend\tNULL\n#define bcmgenet_suspend_noirq\tNULL\n#define bcmgenet_resume\t\tNULL\n#define bcmgenet_resume_noirq\tNULL\n#endif  \n\nstatic const struct dev_pm_ops bcmgenet_pm_ops = {\n\t.suspend\t= bcmgenet_suspend,\n\t.suspend_noirq\t= bcmgenet_suspend_noirq,\n\t.resume\t\t= bcmgenet_resume,\n\t.resume_noirq\t= bcmgenet_resume_noirq,\n};\n\nstatic const struct acpi_device_id genet_acpi_match[] = {\n\t{ \"BCM6E4E\", (kernel_ulong_t)&bcm2711_plat_data },\n\t{ },\n};\nMODULE_DEVICE_TABLE(acpi, genet_acpi_match);\n\nstatic struct platform_driver bcmgenet_driver = {\n\t.probe\t= bcmgenet_probe,\n\t.remove\t= bcmgenet_remove,\n\t.shutdown = bcmgenet_shutdown,\n\t.driver\t= {\n\t\t.name\t= \"bcmgenet\",\n\t\t.of_match_table = bcmgenet_match,\n\t\t.pm\t= &bcmgenet_pm_ops,\n\t\t.acpi_match_table = genet_acpi_match,\n\t},\n};\nmodule_platform_driver(bcmgenet_driver);\n\nMODULE_AUTHOR(\"Broadcom Corporation\");\nMODULE_DESCRIPTION(\"Broadcom GENET Ethernet controller driver\");\nMODULE_ALIAS(\"platform:bcmgenet\");\nMODULE_LICENSE(\"GPL\");\nMODULE_SOFTDEP(\"pre: mdio-bcm-unimac\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}