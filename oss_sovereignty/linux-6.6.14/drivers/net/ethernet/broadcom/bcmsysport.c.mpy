{
  "module_name": "bcmsysport.c",
  "hash_id": "b583459b3d28953dedebd87ce55665b67dc3909c769319904741b6bf8e488c27",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/broadcom/bcmsysport.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\tKBUILD_MODNAME \": \" fmt\n\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/netdevice.h>\n#include <linux/dsa/brcm.h>\n#include <linux/etherdevice.h>\n#include <linux/platform_device.h>\n#include <linux/of.h>\n#include <linux/of_net.h>\n#include <linux/of_mdio.h>\n#include <linux/phy.h>\n#include <linux/phy_fixed.h>\n#include <net/dsa.h>\n#include <linux/clk.h>\n#include <net/ip.h>\n#include <net/ipv6.h>\n\n#include \"bcmsysport.h\"\n\n \n#define BCM_SYSPORT_IO_MACRO(name, offset) \\\nstatic inline u32 name##_readl(struct bcm_sysport_priv *priv, u32 off)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu32 reg = readl_relaxed(priv->base + offset + off);\t\t\\\n\treturn reg;\t\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void name##_writel(struct bcm_sysport_priv *priv,\t\t\\\n\t\t\t\t  u32 val, u32 off)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\twritel_relaxed(val, priv->base + offset + off);\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\nBCM_SYSPORT_IO_MACRO(intrl2_0, SYS_PORT_INTRL2_0_OFFSET);\nBCM_SYSPORT_IO_MACRO(intrl2_1, SYS_PORT_INTRL2_1_OFFSET);\nBCM_SYSPORT_IO_MACRO(umac, SYS_PORT_UMAC_OFFSET);\nBCM_SYSPORT_IO_MACRO(gib, SYS_PORT_GIB_OFFSET);\nBCM_SYSPORT_IO_MACRO(tdma, SYS_PORT_TDMA_OFFSET);\nBCM_SYSPORT_IO_MACRO(rxchk, SYS_PORT_RXCHK_OFFSET);\nBCM_SYSPORT_IO_MACRO(txchk, SYS_PORT_TXCHK_OFFSET);\nBCM_SYSPORT_IO_MACRO(rbuf, SYS_PORT_RBUF_OFFSET);\nBCM_SYSPORT_IO_MACRO(tbuf, SYS_PORT_TBUF_OFFSET);\nBCM_SYSPORT_IO_MACRO(topctrl, SYS_PORT_TOPCTRL_OFFSET);\n\n \nstatic inline u32 rdma_readl(struct bcm_sysport_priv *priv, u32 off)\n{\n\tif (priv->is_lite && off >= RDMA_STATUS)\n\t\toff += 4;\n\treturn readl_relaxed(priv->base + SYS_PORT_RDMA_OFFSET + off);\n}\n\nstatic inline void rdma_writel(struct bcm_sysport_priv *priv, u32 val, u32 off)\n{\n\tif (priv->is_lite && off >= RDMA_STATUS)\n\t\toff += 4;\n\twritel_relaxed(val, priv->base + SYS_PORT_RDMA_OFFSET + off);\n}\n\nstatic inline u32 tdma_control_bit(struct bcm_sysport_priv *priv, u32 bit)\n{\n\tif (!priv->is_lite) {\n\t\treturn BIT(bit);\n\t} else {\n\t\tif (bit >= ACB_ALGO)\n\t\t\treturn BIT(bit + 1);\n\t\telse\n\t\t\treturn BIT(bit);\n\t}\n}\n\n \n#define BCM_SYSPORT_INTR_L2(which)\t\\\nstatic inline void intrl2_##which##_mask_clear(struct bcm_sysport_priv *priv, \\\n\t\t\t\t\t\tu32 mask)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tpriv->irq##which##_mask &= ~(mask);\t\t\t\t\\\n\tintrl2_##which##_writel(priv, mask, INTRL2_CPU_MASK_CLEAR);\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic inline void intrl2_##which##_mask_set(struct bcm_sysport_priv *priv, \\\n\t\t\t\t\t\tu32 mask)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tintrl2_## which##_writel(priv, mask, INTRL2_CPU_MASK_SET);\t\\\n\tpriv->irq##which##_mask |= (mask);\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\nBCM_SYSPORT_INTR_L2(0)\nBCM_SYSPORT_INTR_L2(1)\n\n \nstatic inline void dma_desc_set_addr(struct bcm_sysport_priv *priv,\n\t\t\t\t     void __iomem *d,\n\t\t\t\t     dma_addr_t addr)\n{\n#ifdef CONFIG_PHYS_ADDR_T_64BIT\n\twritel_relaxed(upper_32_bits(addr) & DESC_ADDR_HI_MASK,\n\t\t     d + DESC_ADDR_HI_STATUS_LEN);\n#endif\n\twritel_relaxed(lower_32_bits(addr), d + DESC_ADDR_LO);\n}\n\n \nstatic void bcm_sysport_set_rx_csum(struct net_device *dev,\n\t\t\t\t    netdev_features_t wanted)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tu32 reg;\n\n\tpriv->rx_chk_en = !!(wanted & NETIF_F_RXCSUM);\n\treg = rxchk_readl(priv, RXCHK_CONTROL);\n\t \n\treg &= ~RXCHK_L2_HDR_DIS;\n\tif (priv->rx_chk_en)\n\t\treg |= RXCHK_EN;\n\telse\n\t\treg &= ~RXCHK_EN;\n\n\t \n\tif (priv->rx_chk_en && priv->crc_fwd)\n\t\treg |= RXCHK_SKIP_FCS;\n\telse\n\t\treg &= ~RXCHK_SKIP_FCS;\n\n\t \n\tif (netdev_uses_dsa(dev))\n\t\treg |= RXCHK_BRCM_TAG_EN;\n\telse\n\t\treg &= ~RXCHK_BRCM_TAG_EN;\n\n\trxchk_writel(priv, reg, RXCHK_CONTROL);\n}\n\nstatic void bcm_sysport_set_tx_csum(struct net_device *dev,\n\t\t\t\t    netdev_features_t wanted)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tu32 reg;\n\n\t \n\tpriv->tsb_en = !!(wanted & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\t\t    NETIF_F_HW_VLAN_CTAG_TX));\n\treg = tdma_readl(priv, TDMA_CONTROL);\n\tif (priv->tsb_en)\n\t\treg |= tdma_control_bit(priv, TSB_EN);\n\telse\n\t\treg &= ~tdma_control_bit(priv, TSB_EN);\n\t \n\tif (netdev_uses_dsa(dev))\n\t\treg |= tdma_control_bit(priv, SW_BRCM_TAG);\n\telse\n\t\treg &= ~tdma_control_bit(priv, SW_BRCM_TAG);\n\ttdma_writel(priv, reg, TDMA_CONTROL);\n\n\t \n\tif (wanted & NETIF_F_HW_VLAN_CTAG_TX)\n\t\ttdma_writel(priv, ETH_P_8021Q, TDMA_TPID);\n}\n\nstatic int bcm_sysport_set_features(struct net_device *dev,\n\t\t\t\t    netdev_features_t features)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tint ret;\n\n\tret = clk_prepare_enable(priv->clk);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (!priv->is_lite)\n\t\tpriv->crc_fwd = !!(umac_readl(priv, UMAC_CMD) & CMD_CRC_FWD);\n\telse\n\t\tpriv->crc_fwd = !((gib_readl(priv, GIB_CONTROL) &\n\t\t\t\t  GIB_FCS_STRIP) >> GIB_FCS_STRIP_SHIFT);\n\n\tbcm_sysport_set_rx_csum(dev, features);\n\tbcm_sysport_set_tx_csum(dev, features);\n\n\tclk_disable_unprepare(priv->clk);\n\n\treturn 0;\n}\n\n \nstatic const struct bcm_sysport_stats bcm_sysport_gstrings_stats[] = {\n\t \n\tSTAT_NETDEV64(rx_packets),\n\tSTAT_NETDEV64(tx_packets),\n\tSTAT_NETDEV64(rx_bytes),\n\tSTAT_NETDEV64(tx_bytes),\n\tSTAT_NETDEV(rx_errors),\n\tSTAT_NETDEV(tx_errors),\n\tSTAT_NETDEV(rx_dropped),\n\tSTAT_NETDEV(tx_dropped),\n\tSTAT_NETDEV(multicast),\n\t \n\tSTAT_MIB_RX(\"rx_64_octets\", mib.rx.pkt_cnt.cnt_64),\n\tSTAT_MIB_RX(\"rx_65_127_oct\", mib.rx.pkt_cnt.cnt_127),\n\tSTAT_MIB_RX(\"rx_128_255_oct\", mib.rx.pkt_cnt.cnt_255),\n\tSTAT_MIB_RX(\"rx_256_511_oct\", mib.rx.pkt_cnt.cnt_511),\n\tSTAT_MIB_RX(\"rx_512_1023_oct\", mib.rx.pkt_cnt.cnt_1023),\n\tSTAT_MIB_RX(\"rx_1024_1518_oct\", mib.rx.pkt_cnt.cnt_1518),\n\tSTAT_MIB_RX(\"rx_vlan_1519_1522_oct\", mib.rx.pkt_cnt.cnt_mgv),\n\tSTAT_MIB_RX(\"rx_1522_2047_oct\", mib.rx.pkt_cnt.cnt_2047),\n\tSTAT_MIB_RX(\"rx_2048_4095_oct\", mib.rx.pkt_cnt.cnt_4095),\n\tSTAT_MIB_RX(\"rx_4096_9216_oct\", mib.rx.pkt_cnt.cnt_9216),\n\tSTAT_MIB_RX(\"rx_pkts\", mib.rx.pkt),\n\tSTAT_MIB_RX(\"rx_bytes\", mib.rx.bytes),\n\tSTAT_MIB_RX(\"rx_multicast\", mib.rx.mca),\n\tSTAT_MIB_RX(\"rx_broadcast\", mib.rx.bca),\n\tSTAT_MIB_RX(\"rx_fcs\", mib.rx.fcs),\n\tSTAT_MIB_RX(\"rx_control\", mib.rx.cf),\n\tSTAT_MIB_RX(\"rx_pause\", mib.rx.pf),\n\tSTAT_MIB_RX(\"rx_unknown\", mib.rx.uo),\n\tSTAT_MIB_RX(\"rx_align\", mib.rx.aln),\n\tSTAT_MIB_RX(\"rx_outrange\", mib.rx.flr),\n\tSTAT_MIB_RX(\"rx_code\", mib.rx.cde),\n\tSTAT_MIB_RX(\"rx_carrier\", mib.rx.fcr),\n\tSTAT_MIB_RX(\"rx_oversize\", mib.rx.ovr),\n\tSTAT_MIB_RX(\"rx_jabber\", mib.rx.jbr),\n\tSTAT_MIB_RX(\"rx_mtu_err\", mib.rx.mtue),\n\tSTAT_MIB_RX(\"rx_good_pkts\", mib.rx.pok),\n\tSTAT_MIB_RX(\"rx_unicast\", mib.rx.uc),\n\tSTAT_MIB_RX(\"rx_ppp\", mib.rx.ppp),\n\tSTAT_MIB_RX(\"rx_crc\", mib.rx.rcrc),\n\t \n\tSTAT_MIB_TX(\"tx_64_octets\", mib.tx.pkt_cnt.cnt_64),\n\tSTAT_MIB_TX(\"tx_65_127_oct\", mib.tx.pkt_cnt.cnt_127),\n\tSTAT_MIB_TX(\"tx_128_255_oct\", mib.tx.pkt_cnt.cnt_255),\n\tSTAT_MIB_TX(\"tx_256_511_oct\", mib.tx.pkt_cnt.cnt_511),\n\tSTAT_MIB_TX(\"tx_512_1023_oct\", mib.tx.pkt_cnt.cnt_1023),\n\tSTAT_MIB_TX(\"tx_1024_1518_oct\", mib.tx.pkt_cnt.cnt_1518),\n\tSTAT_MIB_TX(\"tx_vlan_1519_1522_oct\", mib.tx.pkt_cnt.cnt_mgv),\n\tSTAT_MIB_TX(\"tx_1522_2047_oct\", mib.tx.pkt_cnt.cnt_2047),\n\tSTAT_MIB_TX(\"tx_2048_4095_oct\", mib.tx.pkt_cnt.cnt_4095),\n\tSTAT_MIB_TX(\"tx_4096_9216_oct\", mib.tx.pkt_cnt.cnt_9216),\n\tSTAT_MIB_TX(\"tx_pkts\", mib.tx.pkts),\n\tSTAT_MIB_TX(\"tx_multicast\", mib.tx.mca),\n\tSTAT_MIB_TX(\"tx_broadcast\", mib.tx.bca),\n\tSTAT_MIB_TX(\"tx_pause\", mib.tx.pf),\n\tSTAT_MIB_TX(\"tx_control\", mib.tx.cf),\n\tSTAT_MIB_TX(\"tx_fcs_err\", mib.tx.fcs),\n\tSTAT_MIB_TX(\"tx_oversize\", mib.tx.ovr),\n\tSTAT_MIB_TX(\"tx_defer\", mib.tx.drf),\n\tSTAT_MIB_TX(\"tx_excess_defer\", mib.tx.edf),\n\tSTAT_MIB_TX(\"tx_single_col\", mib.tx.scl),\n\tSTAT_MIB_TX(\"tx_multi_col\", mib.tx.mcl),\n\tSTAT_MIB_TX(\"tx_late_col\", mib.tx.lcl),\n\tSTAT_MIB_TX(\"tx_excess_col\", mib.tx.ecl),\n\tSTAT_MIB_TX(\"tx_frags\", mib.tx.frg),\n\tSTAT_MIB_TX(\"tx_total_col\", mib.tx.ncl),\n\tSTAT_MIB_TX(\"tx_jabber\", mib.tx.jbr),\n\tSTAT_MIB_TX(\"tx_bytes\", mib.tx.bytes),\n\tSTAT_MIB_TX(\"tx_good_pkts\", mib.tx.pok),\n\tSTAT_MIB_TX(\"tx_unicast\", mib.tx.uc),\n\t \n\tSTAT_RUNT(\"rx_runt_pkts\", mib.rx_runt_cnt),\n\tSTAT_RUNT(\"rx_runt_valid_fcs\", mib.rx_runt_fcs),\n\tSTAT_RUNT(\"rx_runt_inval_fcs_align\", mib.rx_runt_fcs_align),\n\tSTAT_RUNT(\"rx_runt_bytes\", mib.rx_runt_bytes),\n\t \n\tSTAT_RXCHK(\"rxchk_bad_csum\", mib.rxchk_bad_csum, RXCHK_BAD_CSUM_CNTR),\n\tSTAT_RXCHK(\"rxchk_other_pkt_disc\", mib.rxchk_other_pkt_disc,\n\t\t   RXCHK_OTHER_DISC_CNTR),\n\t \n\tSTAT_RBUF(\"rbuf_ovflow_cnt\", mib.rbuf_ovflow_cnt, RBUF_OVFL_DISC_CNTR),\n\tSTAT_RBUF(\"rbuf_err_cnt\", mib.rbuf_err_cnt, RBUF_ERR_PKT_CNTR),\n\t \n\tSTAT_RDMA(\"rdma_ovflow_cnt\", mib.rdma_ovflow_cnt, RDMA_OVFL_DISC_CNTR),\n\tSTAT_MIB_SOFT(\"alloc_rx_buff_failed\", mib.alloc_rx_buff_failed),\n\tSTAT_MIB_SOFT(\"rx_dma_failed\", mib.rx_dma_failed),\n\tSTAT_MIB_SOFT(\"tx_dma_failed\", mib.tx_dma_failed),\n\tSTAT_MIB_SOFT(\"tx_realloc_tsb\", mib.tx_realloc_tsb),\n\tSTAT_MIB_SOFT(\"tx_realloc_tsb_failed\", mib.tx_realloc_tsb_failed),\n\t \n};\n\n#define BCM_SYSPORT_STATS_LEN\tARRAY_SIZE(bcm_sysport_gstrings_stats)\n\nstatic void bcm_sysport_get_drvinfo(struct net_device *dev,\n\t\t\t\t    struct ethtool_drvinfo *info)\n{\n\tstrscpy(info->driver, KBUILD_MODNAME, sizeof(info->driver));\n\tstrscpy(info->bus_info, \"platform\", sizeof(info->bus_info));\n}\n\nstatic u32 bcm_sysport_get_msglvl(struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\n\treturn priv->msg_enable;\n}\n\nstatic void bcm_sysport_set_msglvl(struct net_device *dev, u32 enable)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\n\tpriv->msg_enable = enable;\n}\n\nstatic inline bool bcm_sysport_lite_stat_valid(enum bcm_sysport_stat_type type)\n{\n\tswitch (type) {\n\tcase BCM_SYSPORT_STAT_NETDEV:\n\tcase BCM_SYSPORT_STAT_NETDEV64:\n\tcase BCM_SYSPORT_STAT_RXCHK:\n\tcase BCM_SYSPORT_STAT_RBUF:\n\tcase BCM_SYSPORT_STAT_RDMA:\n\tcase BCM_SYSPORT_STAT_SOFT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int bcm_sysport_get_sset_count(struct net_device *dev, int string_set)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tconst struct bcm_sysport_stats *s;\n\tunsigned int i, j;\n\n\tswitch (string_set) {\n\tcase ETH_SS_STATS:\n\t\tfor (i = 0, j = 0; i < BCM_SYSPORT_STATS_LEN; i++) {\n\t\t\ts = &bcm_sysport_gstrings_stats[i];\n\t\t\tif (priv->is_lite &&\n\t\t\t    !bcm_sysport_lite_stat_valid(s->type))\n\t\t\t\tcontinue;\n\t\t\tj++;\n\t\t}\n\t\t \n\t\treturn j + dev->num_tx_queues * NUM_SYSPORT_TXQ_STAT;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic void bcm_sysport_get_strings(struct net_device *dev,\n\t\t\t\t    u32 stringset, u8 *data)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tconst struct bcm_sysport_stats *s;\n\tchar buf[128];\n\tint i, j;\n\n\tswitch (stringset) {\n\tcase ETH_SS_STATS:\n\t\tfor (i = 0, j = 0; i < BCM_SYSPORT_STATS_LEN; i++) {\n\t\t\ts = &bcm_sysport_gstrings_stats[i];\n\t\t\tif (priv->is_lite &&\n\t\t\t    !bcm_sysport_lite_stat_valid(s->type))\n\t\t\t\tcontinue;\n\n\t\t\tmemcpy(data + j * ETH_GSTRING_LEN, s->stat_string,\n\t\t\t       ETH_GSTRING_LEN);\n\t\t\tj++;\n\t\t}\n\n\t\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\t\tsnprintf(buf, sizeof(buf), \"txq%d_packets\", i);\n\t\t\tmemcpy(data + j * ETH_GSTRING_LEN, buf,\n\t\t\t       ETH_GSTRING_LEN);\n\t\t\tj++;\n\n\t\t\tsnprintf(buf, sizeof(buf), \"txq%d_bytes\", i);\n\t\t\tmemcpy(data + j * ETH_GSTRING_LEN, buf,\n\t\t\t       ETH_GSTRING_LEN);\n\t\t\tj++;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void bcm_sysport_update_mib_counters(struct bcm_sysport_priv *priv)\n{\n\tint i, j = 0;\n\n\tfor (i = 0; i < BCM_SYSPORT_STATS_LEN; i++) {\n\t\tconst struct bcm_sysport_stats *s;\n\t\tu8 offset = 0;\n\t\tu32 val = 0;\n\t\tchar *p;\n\n\t\ts = &bcm_sysport_gstrings_stats[i];\n\t\tswitch (s->type) {\n\t\tcase BCM_SYSPORT_STAT_NETDEV:\n\t\tcase BCM_SYSPORT_STAT_NETDEV64:\n\t\tcase BCM_SYSPORT_STAT_SOFT:\n\t\t\tcontinue;\n\t\tcase BCM_SYSPORT_STAT_MIB_RX:\n\t\tcase BCM_SYSPORT_STAT_MIB_TX:\n\t\tcase BCM_SYSPORT_STAT_RUNT:\n\t\t\tif (priv->is_lite)\n\t\t\t\tcontinue;\n\n\t\t\tif (s->type != BCM_SYSPORT_STAT_MIB_RX)\n\t\t\t\toffset = UMAC_MIB_STAT_OFFSET;\n\t\t\tval = umac_readl(priv, UMAC_MIB_START + j + offset);\n\t\t\tbreak;\n\t\tcase BCM_SYSPORT_STAT_RXCHK:\n\t\t\tval = rxchk_readl(priv, s->reg_offset);\n\t\t\tif (val == ~0)\n\t\t\t\trxchk_writel(priv, 0, s->reg_offset);\n\t\t\tbreak;\n\t\tcase BCM_SYSPORT_STAT_RBUF:\n\t\t\tval = rbuf_readl(priv, s->reg_offset);\n\t\t\tif (val == ~0)\n\t\t\t\trbuf_writel(priv, 0, s->reg_offset);\n\t\t\tbreak;\n\t\tcase BCM_SYSPORT_STAT_RDMA:\n\t\t\tif (!priv->is_lite)\n\t\t\t\tcontinue;\n\n\t\t\tval = rdma_readl(priv, s->reg_offset);\n\t\t\tif (val == ~0)\n\t\t\t\trdma_writel(priv, 0, s->reg_offset);\n\t\t\tbreak;\n\t\t}\n\n\t\tj += s->stat_sizeof;\n\t\tp = (char *)priv + s->stat_offset;\n\t\t*(u32 *)p = val;\n\t}\n\n\tnetif_dbg(priv, hw, priv->netdev, \"updated MIB counters\\n\");\n}\n\nstatic void bcm_sysport_update_tx_stats(struct bcm_sysport_priv *priv,\n\t\t\t\t\tu64 *tx_bytes, u64 *tx_packets)\n{\n\tstruct bcm_sysport_tx_ring *ring;\n\tu64 bytes = 0, packets = 0;\n\tunsigned int start;\n\tunsigned int q;\n\n\tfor (q = 0; q < priv->netdev->num_tx_queues; q++) {\n\t\tring = &priv->tx_rings[q];\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&priv->syncp);\n\t\t\tbytes = ring->bytes;\n\t\t\tpackets = ring->packets;\n\t\t} while (u64_stats_fetch_retry(&priv->syncp, start));\n\n\t\t*tx_bytes += bytes;\n\t\t*tx_packets += packets;\n\t}\n}\n\nstatic void bcm_sysport_get_stats(struct net_device *dev,\n\t\t\t\t  struct ethtool_stats *stats, u64 *data)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct bcm_sysport_stats64 *stats64 = &priv->stats64;\n\tstruct u64_stats_sync *syncp = &priv->syncp;\n\tstruct bcm_sysport_tx_ring *ring;\n\tu64 tx_bytes = 0, tx_packets = 0;\n\tunsigned int start;\n\tint i, j;\n\n\tif (netif_running(dev)) {\n\t\tbcm_sysport_update_mib_counters(priv);\n\t\tbcm_sysport_update_tx_stats(priv, &tx_bytes, &tx_packets);\n\t\tstats64->tx_bytes = tx_bytes;\n\t\tstats64->tx_packets = tx_packets;\n\t}\n\n\tfor (i =  0, j = 0; i < BCM_SYSPORT_STATS_LEN; i++) {\n\t\tconst struct bcm_sysport_stats *s;\n\t\tchar *p;\n\n\t\ts = &bcm_sysport_gstrings_stats[i];\n\t\tif (s->type == BCM_SYSPORT_STAT_NETDEV)\n\t\t\tp = (char *)&dev->stats;\n\t\telse if (s->type == BCM_SYSPORT_STAT_NETDEV64)\n\t\t\tp = (char *)stats64;\n\t\telse\n\t\t\tp = (char *)priv;\n\n\t\tif (priv->is_lite && !bcm_sysport_lite_stat_valid(s->type))\n\t\t\tcontinue;\n\t\tp += s->stat_offset;\n\n\t\tif (s->stat_sizeof == sizeof(u64) &&\n\t\t    s->type == BCM_SYSPORT_STAT_NETDEV64) {\n\t\t\tdo {\n\t\t\t\tstart = u64_stats_fetch_begin(syncp);\n\t\t\t\tdata[i] = *(u64 *)p;\n\t\t\t} while (u64_stats_fetch_retry(syncp, start));\n\t\t} else\n\t\t\tdata[i] = *(u32 *)p;\n\t\tj++;\n\t}\n\n\t \n\tj = bcm_sysport_get_sset_count(dev, ETH_SS_STATS) -\n\t    dev->num_tx_queues * NUM_SYSPORT_TXQ_STAT;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tring = &priv->tx_rings[i];\n\t\tdata[j] = ring->packets;\n\t\tj++;\n\t\tdata[j] = ring->bytes;\n\t\tj++;\n\t}\n}\n\nstatic void bcm_sysport_get_wol(struct net_device *dev,\n\t\t\t\tstruct ethtool_wolinfo *wol)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\n\twol->supported = WAKE_MAGIC | WAKE_MAGICSECURE | WAKE_FILTER;\n\twol->wolopts = priv->wolopts;\n\n\tif (!(priv->wolopts & WAKE_MAGICSECURE))\n\t\treturn;\n\n\tmemcpy(wol->sopass, priv->sopass, sizeof(priv->sopass));\n}\n\nstatic int bcm_sysport_set_wol(struct net_device *dev,\n\t\t\t       struct ethtool_wolinfo *wol)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct device *kdev = &priv->pdev->dev;\n\tu32 supported = WAKE_MAGIC | WAKE_MAGICSECURE | WAKE_FILTER;\n\n\tif (!device_can_wakeup(kdev))\n\t\treturn -ENOTSUPP;\n\n\tif (wol->wolopts & ~supported)\n\t\treturn -EINVAL;\n\n\tif (wol->wolopts & WAKE_MAGICSECURE)\n\t\tmemcpy(priv->sopass, wol->sopass, sizeof(priv->sopass));\n\n\t \n\tif (wol->wolopts) {\n\t\tdevice_set_wakeup_enable(kdev, 1);\n\t\tif (priv->wol_irq_disabled)\n\t\t\tenable_irq_wake(priv->wol_irq);\n\t\tpriv->wol_irq_disabled = 0;\n\t} else {\n\t\tdevice_set_wakeup_enable(kdev, 0);\n\t\t \n\t\tif (!priv->wol_irq_disabled)\n\t\t\tdisable_irq_wake(priv->wol_irq);\n\t\tpriv->wol_irq_disabled = 1;\n\t}\n\n\tpriv->wolopts = wol->wolopts;\n\n\treturn 0;\n}\n\nstatic void bcm_sysport_set_rx_coalesce(struct bcm_sysport_priv *priv,\n\t\t\t\t\tu32 usecs, u32 pkts)\n{\n\tu32 reg;\n\n\treg = rdma_readl(priv, RDMA_MBDONE_INTR);\n\treg &= ~(RDMA_INTR_THRESH_MASK |\n\t\t RDMA_TIMEOUT_MASK << RDMA_TIMEOUT_SHIFT);\n\treg |= pkts;\n\treg |= DIV_ROUND_UP(usecs * 1000, 8192) << RDMA_TIMEOUT_SHIFT;\n\trdma_writel(priv, reg, RDMA_MBDONE_INTR);\n}\n\nstatic void bcm_sysport_set_tx_coalesce(struct bcm_sysport_tx_ring *ring,\n\t\t\t\t\tstruct ethtool_coalesce *ec)\n{\n\tstruct bcm_sysport_priv *priv = ring->priv;\n\tu32 reg;\n\n\treg = tdma_readl(priv, TDMA_DESC_RING_INTR_CONTROL(ring->index));\n\treg &= ~(RING_INTR_THRESH_MASK |\n\t\t RING_TIMEOUT_MASK << RING_TIMEOUT_SHIFT);\n\treg |= ec->tx_max_coalesced_frames;\n\treg |= DIV_ROUND_UP(ec->tx_coalesce_usecs * 1000, 8192) <<\n\t\t\t    RING_TIMEOUT_SHIFT;\n\ttdma_writel(priv, reg, TDMA_DESC_RING_INTR_CONTROL(ring->index));\n}\n\nstatic int bcm_sysport_get_coalesce(struct net_device *dev,\n\t\t\t\t    struct ethtool_coalesce *ec,\n\t\t\t\t    struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tu32 reg;\n\n\treg = tdma_readl(priv, TDMA_DESC_RING_INTR_CONTROL(0));\n\n\tec->tx_coalesce_usecs = (reg >> RING_TIMEOUT_SHIFT) * 8192 / 1000;\n\tec->tx_max_coalesced_frames = reg & RING_INTR_THRESH_MASK;\n\n\treg = rdma_readl(priv, RDMA_MBDONE_INTR);\n\n\tec->rx_coalesce_usecs = (reg >> RDMA_TIMEOUT_SHIFT) * 8192 / 1000;\n\tec->rx_max_coalesced_frames = reg & RDMA_INTR_THRESH_MASK;\n\tec->use_adaptive_rx_coalesce = priv->dim.use_dim;\n\n\treturn 0;\n}\n\nstatic int bcm_sysport_set_coalesce(struct net_device *dev,\n\t\t\t\t    struct ethtool_coalesce *ec,\n\t\t\t\t    struct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct dim_cq_moder moder;\n\tu32 usecs, pkts;\n\tunsigned int i;\n\n\t \n\tif (ec->tx_max_coalesced_frames > RING_INTR_THRESH_MASK ||\n\t    ec->tx_coalesce_usecs > (RING_TIMEOUT_MASK * 8) + 1 ||\n\t    ec->rx_max_coalesced_frames > RDMA_INTR_THRESH_MASK ||\n\t    ec->rx_coalesce_usecs > (RDMA_TIMEOUT_MASK * 8) + 1)\n\t\treturn -EINVAL;\n\n\tif ((ec->tx_coalesce_usecs == 0 && ec->tx_max_coalesced_frames == 0) ||\n\t    (ec->rx_coalesce_usecs == 0 && ec->rx_max_coalesced_frames == 0))\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tbcm_sysport_set_tx_coalesce(&priv->tx_rings[i], ec);\n\n\tpriv->rx_coalesce_usecs = ec->rx_coalesce_usecs;\n\tpriv->rx_max_coalesced_frames = ec->rx_max_coalesced_frames;\n\tusecs = priv->rx_coalesce_usecs;\n\tpkts = priv->rx_max_coalesced_frames;\n\n\tif (ec->use_adaptive_rx_coalesce && !priv->dim.use_dim) {\n\t\tmoder = net_dim_get_def_rx_moderation(priv->dim.dim.mode);\n\t\tusecs = moder.usec;\n\t\tpkts = moder.pkts;\n\t}\n\n\tpriv->dim.use_dim = ec->use_adaptive_rx_coalesce;\n\n\t \n\tbcm_sysport_set_rx_coalesce(priv, usecs, pkts);\n\n\treturn 0;\n}\n\nstatic void bcm_sysport_free_cb(struct bcm_sysport_cb *cb)\n{\n\tdev_consume_skb_any(cb->skb);\n\tcb->skb = NULL;\n\tdma_unmap_addr_set(cb, dma_addr, 0);\n}\n\nstatic struct sk_buff *bcm_sysport_rx_refill(struct bcm_sysport_priv *priv,\n\t\t\t\t\t     struct bcm_sysport_cb *cb)\n{\n\tstruct device *kdev = &priv->pdev->dev;\n\tstruct net_device *ndev = priv->netdev;\n\tstruct sk_buff *skb, *rx_skb;\n\tdma_addr_t mapping;\n\n\t \n\tskb = __netdev_alloc_skb(priv->netdev, RX_BUF_LENGTH,\n\t\t\t\t GFP_ATOMIC | __GFP_NOWARN);\n\tif (!skb) {\n\t\tpriv->mib.alloc_rx_buff_failed++;\n\t\tnetif_err(priv, rx_err, ndev, \"SKB alloc failed\\n\");\n\t\treturn NULL;\n\t}\n\n\tmapping = dma_map_single(kdev, skb->data,\n\t\t\t\t RX_BUF_LENGTH, DMA_FROM_DEVICE);\n\tif (dma_mapping_error(kdev, mapping)) {\n\t\tpriv->mib.rx_dma_failed++;\n\t\tdev_kfree_skb_any(skb);\n\t\tnetif_err(priv, rx_err, ndev, \"DMA mapping failure\\n\");\n\t\treturn NULL;\n\t}\n\n\t \n\trx_skb = cb->skb;\n\tif (likely(rx_skb))\n\t\tdma_unmap_single(kdev, dma_unmap_addr(cb, dma_addr),\n\t\t\t\t RX_BUF_LENGTH, DMA_FROM_DEVICE);\n\n\t \n\tcb->skb = skb;\n\tdma_unmap_addr_set(cb, dma_addr, mapping);\n\tdma_desc_set_addr(priv, cb->bd_addr, mapping);\n\n\tnetif_dbg(priv, rx_status, ndev, \"RX refill\\n\");\n\n\t \n\treturn rx_skb;\n}\n\nstatic int bcm_sysport_alloc_rx_bufs(struct bcm_sysport_priv *priv)\n{\n\tstruct bcm_sysport_cb *cb;\n\tstruct sk_buff *skb;\n\tunsigned int i;\n\n\tfor (i = 0; i < priv->num_rx_bds; i++) {\n\t\tcb = &priv->rx_cbs[i];\n\t\tskb = bcm_sysport_rx_refill(priv, cb);\n\t\tdev_kfree_skb(skb);\n\t\tif (!cb->skb)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic unsigned int bcm_sysport_desc_rx(struct bcm_sysport_priv *priv,\n\t\t\t\t\tunsigned int budget)\n{\n\tstruct bcm_sysport_stats64 *stats64 = &priv->stats64;\n\tstruct net_device *ndev = priv->netdev;\n\tunsigned int processed = 0, to_process;\n\tunsigned int processed_bytes = 0;\n\tstruct bcm_sysport_cb *cb;\n\tstruct sk_buff *skb;\n\tunsigned int p_index;\n\tu16 len, status;\n\tstruct bcm_rsb *rsb;\n\n\t \n\tintrl2_0_writel(priv, INTRL2_0_RDMA_MBDONE, INTRL2_CPU_CLEAR);\n\n\t \n\tif (!priv->is_lite)\n\t\tp_index = rdma_readl(priv, RDMA_PROD_INDEX);\n\telse\n\t\tp_index = rdma_readl(priv, RDMA_CONS_INDEX);\n\tp_index &= RDMA_PROD_INDEX_MASK;\n\n\tto_process = (p_index - priv->rx_c_index) & RDMA_CONS_INDEX_MASK;\n\n\tnetif_dbg(priv, rx_status, ndev,\n\t\t  \"p_index=%d rx_c_index=%d to_process=%d\\n\",\n\t\t  p_index, priv->rx_c_index, to_process);\n\n\twhile ((processed < to_process) && (processed < budget)) {\n\t\tcb = &priv->rx_cbs[priv->rx_read_ptr];\n\t\tskb = bcm_sysport_rx_refill(priv, cb);\n\n\n\t\t \n\t\tif (unlikely(!skb)) {\n\t\t\tnetif_err(priv, rx_err, ndev, \"out of memory!\\n\");\n\t\t\tndev->stats.rx_dropped++;\n\t\t\tndev->stats.rx_errors++;\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\trsb = (struct bcm_rsb *)skb->data;\n\t\tlen = (rsb->rx_status_len >> DESC_LEN_SHIFT) & DESC_LEN_MASK;\n\t\tstatus = (rsb->rx_status_len >> DESC_STATUS_SHIFT) &\n\t\t\t  DESC_STATUS_MASK;\n\n\t\tnetif_dbg(priv, rx_status, ndev,\n\t\t\t  \"p=%d, c=%d, rd_ptr=%d, len=%d, flag=0x%04x\\n\",\n\t\t\t  p_index, priv->rx_c_index, priv->rx_read_ptr,\n\t\t\t  len, status);\n\n\t\tif (unlikely(len > RX_BUF_LENGTH)) {\n\t\t\tnetif_err(priv, rx_status, ndev, \"oversized packet\\n\");\n\t\t\tndev->stats.rx_length_errors++;\n\t\t\tndev->stats.rx_errors++;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (unlikely(!(status & DESC_EOP) || !(status & DESC_SOP))) {\n\t\t\tnetif_err(priv, rx_status, ndev, \"fragmented packet!\\n\");\n\t\t\tndev->stats.rx_dropped++;\n\t\t\tndev->stats.rx_errors++;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (unlikely(status & (RX_STATUS_ERR | RX_STATUS_OVFLOW))) {\n\t\t\tnetif_err(priv, rx_err, ndev, \"error packet\\n\");\n\t\t\tif (status & RX_STATUS_OVFLOW)\n\t\t\t\tndev->stats.rx_over_errors++;\n\t\t\tndev->stats.rx_dropped++;\n\t\t\tndev->stats.rx_errors++;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto next;\n\t\t}\n\n\t\tskb_put(skb, len);\n\n\t\t \n\t\tif (likely(status & DESC_L4_CSUM))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\t \n\t\tskb_pull(skb, sizeof(*rsb) + 2);\n\t\tlen -= (sizeof(*rsb) + 2);\n\t\tprocessed_bytes += len;\n\n\t\t \n\t\tif (priv->crc_fwd) {\n\t\t\tskb_trim(skb, len - ETH_FCS_LEN);\n\t\t\tlen -= ETH_FCS_LEN;\n\t\t}\n\n\t\tskb->protocol = eth_type_trans(skb, ndev);\n\t\tndev->stats.rx_packets++;\n\t\tndev->stats.rx_bytes += len;\n\t\tu64_stats_update_begin(&priv->syncp);\n\t\tstats64->rx_packets++;\n\t\tstats64->rx_bytes += len;\n\t\tu64_stats_update_end(&priv->syncp);\n\n\t\tnapi_gro_receive(&priv->napi, skb);\nnext:\n\t\tprocessed++;\n\t\tpriv->rx_read_ptr++;\n\n\t\tif (priv->rx_read_ptr == priv->num_rx_bds)\n\t\t\tpriv->rx_read_ptr = 0;\n\t}\n\n\tpriv->dim.packets = processed;\n\tpriv->dim.bytes = processed_bytes;\n\n\treturn processed;\n}\n\nstatic void bcm_sysport_tx_reclaim_one(struct bcm_sysport_tx_ring *ring,\n\t\t\t\t       struct bcm_sysport_cb *cb,\n\t\t\t\t       unsigned int *bytes_compl,\n\t\t\t\t       unsigned int *pkts_compl)\n{\n\tstruct bcm_sysport_priv *priv = ring->priv;\n\tstruct device *kdev = &priv->pdev->dev;\n\n\tif (cb->skb) {\n\t\t*bytes_compl += cb->skb->len;\n\t\tdma_unmap_single(kdev, dma_unmap_addr(cb, dma_addr),\n\t\t\t\t dma_unmap_len(cb, dma_len),\n\t\t\t\t DMA_TO_DEVICE);\n\t\t(*pkts_compl)++;\n\t\tbcm_sysport_free_cb(cb);\n\t \n\t} else if (dma_unmap_addr(cb, dma_addr)) {\n\t\t*bytes_compl += dma_unmap_len(cb, dma_len);\n\t\tdma_unmap_page(kdev, dma_unmap_addr(cb, dma_addr),\n\t\t\t       dma_unmap_len(cb, dma_len), DMA_TO_DEVICE);\n\t\tdma_unmap_addr_set(cb, dma_addr, 0);\n\t}\n}\n\n \nstatic unsigned int __bcm_sysport_tx_reclaim(struct bcm_sysport_priv *priv,\n\t\t\t\t\t     struct bcm_sysport_tx_ring *ring)\n{\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\tstruct net_device *ndev = priv->netdev;\n\tunsigned int txbds_processed = 0;\n\tstruct bcm_sysport_cb *cb;\n\tunsigned int txbds_ready;\n\tunsigned int c_index;\n\tu32 hw_ind;\n\n\t \n\tif (!ring->priv->is_lite)\n\t\tintrl2_1_writel(ring->priv, BIT(ring->index), INTRL2_CPU_CLEAR);\n\telse\n\t\tintrl2_0_writel(ring->priv, BIT(ring->index +\n\t\t\t\tINTRL2_0_TDMA_MBDONE_SHIFT), INTRL2_CPU_CLEAR);\n\n\t \n\thw_ind = tdma_readl(priv, TDMA_DESC_RING_PROD_CONS_INDEX(ring->index));\n\tc_index = (hw_ind >> RING_CONS_INDEX_SHIFT) & RING_CONS_INDEX_MASK;\n\ttxbds_ready = (c_index - ring->c_index) & RING_CONS_INDEX_MASK;\n\n\tnetif_dbg(priv, tx_done, ndev,\n\t\t  \"ring=%d old_c_index=%u c_index=%u txbds_ready=%u\\n\",\n\t\t  ring->index, ring->c_index, c_index, txbds_ready);\n\n\twhile (txbds_processed < txbds_ready) {\n\t\tcb = &ring->cbs[ring->clean_index];\n\t\tbcm_sysport_tx_reclaim_one(ring, cb, &bytes_compl, &pkts_compl);\n\n\t\tring->desc_count++;\n\t\ttxbds_processed++;\n\n\t\tif (likely(ring->clean_index < ring->size - 1))\n\t\t\tring->clean_index++;\n\t\telse\n\t\t\tring->clean_index = 0;\n\t}\n\n\tu64_stats_update_begin(&priv->syncp);\n\tring->packets += pkts_compl;\n\tring->bytes += bytes_compl;\n\tu64_stats_update_end(&priv->syncp);\n\n\tring->c_index = c_index;\n\n\tnetif_dbg(priv, tx_done, ndev,\n\t\t  \"ring=%d c_index=%d pkts_compl=%d, bytes_compl=%d\\n\",\n\t\t  ring->index, ring->c_index, pkts_compl, bytes_compl);\n\n\treturn pkts_compl;\n}\n\n \nstatic unsigned int bcm_sysport_tx_reclaim(struct bcm_sysport_priv *priv,\n\t\t\t\t\t   struct bcm_sysport_tx_ring *ring)\n{\n\tstruct netdev_queue *txq;\n\tunsigned int released;\n\tunsigned long flags;\n\n\ttxq = netdev_get_tx_queue(priv->netdev, ring->index);\n\n\tspin_lock_irqsave(&ring->lock, flags);\n\treleased = __bcm_sysport_tx_reclaim(priv, ring);\n\tif (released)\n\t\tnetif_tx_wake_queue(txq);\n\n\tspin_unlock_irqrestore(&ring->lock, flags);\n\n\treturn released;\n}\n\n \nstatic void bcm_sysport_tx_clean(struct bcm_sysport_priv *priv,\n\t\t\t\t struct bcm_sysport_tx_ring *ring)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ring->lock, flags);\n\t__bcm_sysport_tx_reclaim(priv, ring);\n\tspin_unlock_irqrestore(&ring->lock, flags);\n}\n\nstatic int bcm_sysport_tx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct bcm_sysport_tx_ring *ring =\n\t\tcontainer_of(napi, struct bcm_sysport_tx_ring, napi);\n\tunsigned int work_done = 0;\n\n\twork_done = bcm_sysport_tx_reclaim(ring->priv, ring);\n\n\tif (work_done == 0) {\n\t\tnapi_complete(napi);\n\t\t \n\t\tif (!ring->priv->is_lite)\n\t\t\tintrl2_1_mask_clear(ring->priv, BIT(ring->index));\n\t\telse\n\t\t\tintrl2_0_mask_clear(ring->priv, BIT(ring->index +\n\t\t\t\t\t    INTRL2_0_TDMA_MBDONE_SHIFT));\n\n\t\treturn 0;\n\t}\n\n\treturn budget;\n}\n\nstatic void bcm_sysport_tx_reclaim_all(struct bcm_sysport_priv *priv)\n{\n\tunsigned int q;\n\n\tfor (q = 0; q < priv->netdev->num_tx_queues; q++)\n\t\tbcm_sysport_tx_reclaim(priv, &priv->tx_rings[q]);\n}\n\nstatic int bcm_sysport_poll(struct napi_struct *napi, int budget)\n{\n\tstruct bcm_sysport_priv *priv =\n\t\tcontainer_of(napi, struct bcm_sysport_priv, napi);\n\tstruct dim_sample dim_sample = {};\n\tunsigned int work_done = 0;\n\n\twork_done = bcm_sysport_desc_rx(priv, budget);\n\n\tpriv->rx_c_index += work_done;\n\tpriv->rx_c_index &= RDMA_CONS_INDEX_MASK;\n\n\t \n\tif (!priv->is_lite)\n\t\trdma_writel(priv, priv->rx_c_index, RDMA_CONS_INDEX);\n\telse\n\t\trdma_writel(priv, priv->rx_c_index << 16, RDMA_CONS_INDEX);\n\n\tif (work_done < budget) {\n\t\tnapi_complete_done(napi, work_done);\n\t\t \n\t\tintrl2_0_mask_clear(priv, INTRL2_0_RDMA_MBDONE);\n\t}\n\n\tif (priv->dim.use_dim) {\n\t\tdim_update_sample(priv->dim.event_ctr, priv->dim.packets,\n\t\t\t\t  priv->dim.bytes, &dim_sample);\n\t\tnet_dim(&priv->dim.dim, dim_sample);\n\t}\n\n\treturn work_done;\n}\n\nstatic void mpd_enable_set(struct bcm_sysport_priv *priv, bool enable)\n{\n\tu32 reg, bit;\n\n\treg = umac_readl(priv, UMAC_MPD_CTRL);\n\tif (enable)\n\t\treg |= MPD_EN;\n\telse\n\t\treg &= ~MPD_EN;\n\tumac_writel(priv, reg, UMAC_MPD_CTRL);\n\n\tif (priv->is_lite)\n\t\tbit = RBUF_ACPI_EN_LITE;\n\telse\n\t\tbit = RBUF_ACPI_EN;\n\n\treg = rbuf_readl(priv, RBUF_CONTROL);\n\tif (enable)\n\t\treg |= bit;\n\telse\n\t\treg &= ~bit;\n\trbuf_writel(priv, reg, RBUF_CONTROL);\n}\n\nstatic void bcm_sysport_resume_from_wol(struct bcm_sysport_priv *priv)\n{\n\tunsigned int index;\n\tu32 reg;\n\n\t \n\treg = rxchk_readl(priv, RXCHK_CONTROL);\n\treg &= ~(RXCHK_BRCM_TAG_MATCH_MASK <<\n\t\t RXCHK_BRCM_TAG_MATCH_SHIFT | RXCHK_EN | RXCHK_BRCM_TAG_EN);\n\trxchk_writel(priv, reg, RXCHK_CONTROL);\n\n\t \n\tfor_each_set_bit(index, priv->filters, RXCHK_BRCM_TAG_MAX) {\n\t\trxchk_writel(priv, priv->filters_loc[index] <<\n\t\t\t     RXCHK_BRCM_TAG_CID_SHIFT, RXCHK_BRCM_TAG(index));\n\t\trxchk_writel(priv, 0xff00ffff, RXCHK_BRCM_TAG_MASK(index));\n\t}\n\n\t \n\tmpd_enable_set(priv, false);\n\n\treg = intrl2_0_readl(priv, INTRL2_CPU_STATUS);\n\tif (reg & INTRL2_0_MPD)\n\t\tnetdev_info(priv->netdev, \"Wake-on-LAN (MPD) interrupt!\\n\");\n\n\tif (reg & INTRL2_0_BRCM_MATCH_TAG) {\n\t\treg = rxchk_readl(priv, RXCHK_BRCM_TAG_MATCH_STATUS) &\n\t\t\t\t  RXCHK_BRCM_TAG_MATCH_MASK;\n\t\tnetdev_info(priv->netdev,\n\t\t\t    \"Wake-on-LAN (filters 0x%02x) interrupt!\\n\", reg);\n\t}\n\n\tnetif_dbg(priv, wol, priv->netdev, \"resumed from WOL\\n\");\n}\n\nstatic void bcm_sysport_dim_work(struct work_struct *work)\n{\n\tstruct dim *dim = container_of(work, struct dim, work);\n\tstruct bcm_sysport_net_dim *ndim =\n\t\t\tcontainer_of(dim, struct bcm_sysport_net_dim, dim);\n\tstruct bcm_sysport_priv *priv =\n\t\t\tcontainer_of(ndim, struct bcm_sysport_priv, dim);\n\tstruct dim_cq_moder cur_profile = net_dim_get_rx_moderation(dim->mode,\n\t\t\t\t\t\t\t\t    dim->profile_ix);\n\n\tbcm_sysport_set_rx_coalesce(priv, cur_profile.usec, cur_profile.pkts);\n\tdim->state = DIM_START_MEASURE;\n}\n\n \nstatic irqreturn_t bcm_sysport_rx_isr(int irq, void *dev_id)\n{\n\tstruct net_device *dev = dev_id;\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct bcm_sysport_tx_ring *txr;\n\tunsigned int ring, ring_bit;\n\n\tpriv->irq0_stat = intrl2_0_readl(priv, INTRL2_CPU_STATUS) &\n\t\t\t  ~intrl2_0_readl(priv, INTRL2_CPU_MASK_STATUS);\n\tintrl2_0_writel(priv, priv->irq0_stat, INTRL2_CPU_CLEAR);\n\n\tif (unlikely(priv->irq0_stat == 0)) {\n\t\tnetdev_warn(priv->netdev, \"spurious RX interrupt\\n\");\n\t\treturn IRQ_NONE;\n\t}\n\n\tif (priv->irq0_stat & INTRL2_0_RDMA_MBDONE) {\n\t\tpriv->dim.event_ctr++;\n\t\tif (likely(napi_schedule_prep(&priv->napi))) {\n\t\t\t \n\t\t\tintrl2_0_mask_set(priv, INTRL2_0_RDMA_MBDONE);\n\t\t\t__napi_schedule_irqoff(&priv->napi);\n\t\t}\n\t}\n\n\t \n\tif (priv->irq0_stat & INTRL2_0_TX_RING_FULL)\n\t\tbcm_sysport_tx_reclaim_all(priv);\n\n\tif (!priv->is_lite)\n\t\tgoto out;\n\n\tfor (ring = 0; ring < dev->num_tx_queues; ring++) {\n\t\tring_bit = BIT(ring + INTRL2_0_TDMA_MBDONE_SHIFT);\n\t\tif (!(priv->irq0_stat & ring_bit))\n\t\t\tcontinue;\n\n\t\ttxr = &priv->tx_rings[ring];\n\n\t\tif (likely(napi_schedule_prep(&txr->napi))) {\n\t\t\tintrl2_0_mask_set(priv, ring_bit);\n\t\t\t__napi_schedule(&txr->napi);\n\t\t}\n\t}\nout:\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t bcm_sysport_tx_isr(int irq, void *dev_id)\n{\n\tstruct net_device *dev = dev_id;\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct bcm_sysport_tx_ring *txr;\n\tunsigned int ring;\n\n\tpriv->irq1_stat = intrl2_1_readl(priv, INTRL2_CPU_STATUS) &\n\t\t\t\t~intrl2_1_readl(priv, INTRL2_CPU_MASK_STATUS);\n\tintrl2_1_writel(priv, 0xffffffff, INTRL2_CPU_CLEAR);\n\n\tif (unlikely(priv->irq1_stat == 0)) {\n\t\tnetdev_warn(priv->netdev, \"spurious TX interrupt\\n\");\n\t\treturn IRQ_NONE;\n\t}\n\n\tfor (ring = 0; ring < dev->num_tx_queues; ring++) {\n\t\tif (!(priv->irq1_stat & BIT(ring)))\n\t\t\tcontinue;\n\n\t\ttxr = &priv->tx_rings[ring];\n\n\t\tif (likely(napi_schedule_prep(&txr->napi))) {\n\t\t\tintrl2_1_mask_set(priv, BIT(ring));\n\t\t\t__napi_schedule_irqoff(&txr->napi);\n\t\t}\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t bcm_sysport_wol_isr(int irq, void *dev_id)\n{\n\tstruct bcm_sysport_priv *priv = dev_id;\n\n\tpm_wakeup_event(&priv->pdev->dev, 0);\n\n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void bcm_sysport_poll_controller(struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\n\tdisable_irq(priv->irq0);\n\tbcm_sysport_rx_isr(priv->irq0, priv);\n\tenable_irq(priv->irq0);\n\n\tif (!priv->is_lite) {\n\t\tdisable_irq(priv->irq1);\n\t\tbcm_sysport_tx_isr(priv->irq1, priv);\n\t\tenable_irq(priv->irq1);\n\t}\n}\n#endif\n\nstatic struct sk_buff *bcm_sysport_insert_tsb(struct sk_buff *skb,\n\t\t\t\t\t      struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct sk_buff *nskb;\n\tstruct bcm_tsb *tsb;\n\tu32 csum_info;\n\tu8 ip_proto;\n\tu16 csum_start;\n\t__be16 ip_ver;\n\n\t \n\tif (unlikely(skb_headroom(skb) < sizeof(*tsb))) {\n\t\tnskb = skb_realloc_headroom(skb, sizeof(*tsb));\n\t\tif (!nskb) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tpriv->mib.tx_realloc_tsb_failed++;\n\t\t\tdev->stats.tx_errors++;\n\t\t\tdev->stats.tx_dropped++;\n\t\t\treturn NULL;\n\t\t}\n\t\tdev_consume_skb_any(skb);\n\t\tskb = nskb;\n\t\tpriv->mib.tx_realloc_tsb++;\n\t}\n\n\ttsb = skb_push(skb, sizeof(*tsb));\n\t \n\tmemset(tsb, 0, sizeof(*tsb));\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\ttsb->pcp_dei_vid = skb_vlan_tag_get_prio(skb) & PCP_DEI_MASK;\n\t\ttsb->pcp_dei_vid |= (u32)skb_vlan_tag_get_id(skb) << VID_SHIFT;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tip_ver = skb->protocol;\n\t\tswitch (ip_ver) {\n\t\tcase htons(ETH_P_IP):\n\t\t\tip_proto = ip_hdr(skb)->protocol;\n\t\t\tbreak;\n\t\tcase htons(ETH_P_IPV6):\n\t\t\tip_proto = ipv6_hdr(skb)->nexthdr;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn skb;\n\t\t}\n\n\t\t \n\t\tcsum_start = skb_checksum_start_offset(skb) - sizeof(*tsb);\n\t\t \n\t\tif (skb_vlan_tag_present(skb))\n\t\t\tcsum_start += VLAN_HLEN;\n\t\tcsum_info = (csum_start + skb->csum_offset) & L4_CSUM_PTR_MASK;\n\t\tcsum_info |= (csum_start << L4_PTR_SHIFT);\n\n\t\tif (ip_proto == IPPROTO_TCP || ip_proto == IPPROTO_UDP) {\n\t\t\tcsum_info |= L4_LENGTH_VALID;\n\t\t\tif (ip_proto == IPPROTO_UDP &&\n\t\t\t    ip_ver == htons(ETH_P_IP))\n\t\t\t\tcsum_info |= L4_UDP;\n\t\t} else {\n\t\t\tcsum_info = 0;\n\t\t}\n\n\t\ttsb->l4_ptr_dest_map = csum_info;\n\t}\n\n\treturn skb;\n}\n\nstatic netdev_tx_t bcm_sysport_xmit(struct sk_buff *skb,\n\t\t\t\t    struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct device *kdev = &priv->pdev->dev;\n\tstruct bcm_sysport_tx_ring *ring;\n\tunsigned long flags, desc_flags;\n\tstruct bcm_sysport_cb *cb;\n\tstruct netdev_queue *txq;\n\tu32 len_status, addr_lo;\n\tunsigned int skb_len;\n\tdma_addr_t mapping;\n\tu16 queue;\n\tint ret;\n\n\tqueue = skb_get_queue_mapping(skb);\n\ttxq = netdev_get_tx_queue(dev, queue);\n\tring = &priv->tx_rings[queue];\n\n\t \n\tspin_lock_irqsave(&ring->lock, flags);\n\tif (unlikely(ring->desc_count == 0)) {\n\t\tnetif_tx_stop_queue(txq);\n\t\tnetdev_err(dev, \"queue %d awake and ring full!\\n\", queue);\n\t\tret = NETDEV_TX_BUSY;\n\t\tgoto out;\n\t}\n\n\t \n\tif (priv->tsb_en) {\n\t\tskb = bcm_sysport_insert_tsb(skb, dev);\n\t\tif (!skb) {\n\t\t\tret = NETDEV_TX_OK;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tskb_len = skb->len;\n\n\tmapping = dma_map_single(kdev, skb->data, skb_len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(kdev, mapping)) {\n\t\tpriv->mib.tx_dma_failed++;\n\t\tnetif_err(priv, tx_err, dev, \"DMA map failed at %p (len=%d)\\n\",\n\t\t\t  skb->data, skb_len);\n\t\tret = NETDEV_TX_OK;\n\t\tgoto out;\n\t}\n\n\t \n\tcb = &ring->cbs[ring->curr_desc];\n\tcb->skb = skb;\n\tdma_unmap_addr_set(cb, dma_addr, mapping);\n\tdma_unmap_len_set(cb, dma_len, skb_len);\n\n\taddr_lo = lower_32_bits(mapping);\n\tlen_status = upper_32_bits(mapping) & DESC_ADDR_HI_MASK;\n\tlen_status |= (skb_len << DESC_LEN_SHIFT);\n\tlen_status |= (DESC_SOP | DESC_EOP | TX_STATUS_APP_CRC) <<\n\t\t       DESC_STATUS_SHIFT;\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tlen_status |= (DESC_L4_CSUM << DESC_STATUS_SHIFT);\n\tif (skb_vlan_tag_present(skb))\n\t\tlen_status |= (TX_STATUS_VLAN_VID_TSB << DESC_STATUS_SHIFT);\n\n\tring->curr_desc++;\n\tif (ring->curr_desc == ring->size)\n\t\tring->curr_desc = 0;\n\tring->desc_count--;\n\n\t \n\tspin_lock_irqsave(&priv->desc_lock, desc_flags);\n\ttdma_writel(priv, len_status, TDMA_WRITE_PORT_HI(ring->index));\n\ttdma_writel(priv, addr_lo, TDMA_WRITE_PORT_LO(ring->index));\n\tspin_unlock_irqrestore(&priv->desc_lock, desc_flags);\n\n\t \n\tif (ring->desc_count == 0)\n\t\tnetif_tx_stop_queue(txq);\n\n\tnetif_dbg(priv, tx_queued, dev, \"ring=%d desc_count=%d, curr_desc=%d\\n\",\n\t\t  ring->index, ring->desc_count, ring->curr_desc);\n\n\tret = NETDEV_TX_OK;\nout:\n\tspin_unlock_irqrestore(&ring->lock, flags);\n\treturn ret;\n}\n\nstatic void bcm_sysport_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tnetdev_warn(dev, \"transmit timeout!\\n\");\n\n\tnetif_trans_update(dev);\n\tdev->stats.tx_errors++;\n\n\tnetif_tx_wake_all_queues(dev);\n}\n\n \nstatic void bcm_sysport_adj_link(struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct phy_device *phydev = dev->phydev;\n\tunsigned int changed = 0;\n\tu32 cmd_bits = 0, reg;\n\n\tif (priv->old_link != phydev->link) {\n\t\tchanged = 1;\n\t\tpriv->old_link = phydev->link;\n\t}\n\n\tif (priv->old_duplex != phydev->duplex) {\n\t\tchanged = 1;\n\t\tpriv->old_duplex = phydev->duplex;\n\t}\n\n\tif (priv->is_lite)\n\t\tgoto out;\n\n\tswitch (phydev->speed) {\n\tcase SPEED_2500:\n\t\tcmd_bits = CMD_SPEED_2500;\n\t\tbreak;\n\tcase SPEED_1000:\n\t\tcmd_bits = CMD_SPEED_1000;\n\t\tbreak;\n\tcase SPEED_100:\n\t\tcmd_bits = CMD_SPEED_100;\n\t\tbreak;\n\tcase SPEED_10:\n\t\tcmd_bits = CMD_SPEED_10;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tcmd_bits <<= CMD_SPEED_SHIFT;\n\n\tif (phydev->duplex == DUPLEX_HALF)\n\t\tcmd_bits |= CMD_HD_EN;\n\n\tif (priv->old_pause != phydev->pause) {\n\t\tchanged = 1;\n\t\tpriv->old_pause = phydev->pause;\n\t}\n\n\tif (!phydev->pause)\n\t\tcmd_bits |= CMD_RX_PAUSE_IGNORE | CMD_TX_PAUSE_IGNORE;\n\n\tif (!changed)\n\t\treturn;\n\n\tif (phydev->link) {\n\t\treg = umac_readl(priv, UMAC_CMD);\n\t\treg &= ~((CMD_SPEED_MASK << CMD_SPEED_SHIFT) |\n\t\t\tCMD_HD_EN | CMD_RX_PAUSE_IGNORE |\n\t\t\tCMD_TX_PAUSE_IGNORE);\n\t\treg |= cmd_bits;\n\t\tumac_writel(priv, reg, UMAC_CMD);\n\t}\nout:\n\tif (changed)\n\t\tphy_print_status(phydev);\n}\n\nstatic void bcm_sysport_init_dim(struct bcm_sysport_priv *priv,\n\t\t\t\t void (*cb)(struct work_struct *work))\n{\n\tstruct bcm_sysport_net_dim *dim = &priv->dim;\n\n\tINIT_WORK(&dim->dim.work, cb);\n\tdim->dim.mode = DIM_CQ_PERIOD_MODE_START_FROM_EQE;\n\tdim->event_ctr = 0;\n\tdim->packets = 0;\n\tdim->bytes = 0;\n}\n\nstatic void bcm_sysport_init_rx_coalesce(struct bcm_sysport_priv *priv)\n{\n\tstruct bcm_sysport_net_dim *dim = &priv->dim;\n\tstruct dim_cq_moder moder;\n\tu32 usecs, pkts;\n\n\tusecs = priv->rx_coalesce_usecs;\n\tpkts = priv->rx_max_coalesced_frames;\n\n\t \n\tif (dim->use_dim) {\n\t\tmoder = net_dim_get_def_rx_moderation(dim->dim.mode);\n\t\tusecs = moder.usec;\n\t\tpkts = moder.pkts;\n\t}\n\n\tbcm_sysport_set_rx_coalesce(priv, usecs, pkts);\n}\n\nstatic int bcm_sysport_init_tx_ring(struct bcm_sysport_priv *priv,\n\t\t\t\t    unsigned int index)\n{\n\tstruct bcm_sysport_tx_ring *ring = &priv->tx_rings[index];\n\tsize_t size;\n\tu32 reg;\n\n\t \n\tsize = 256;\n\n\tring->cbs = kcalloc(size, sizeof(struct bcm_sysport_cb), GFP_KERNEL);\n\tif (!ring->cbs) {\n\t\tnetif_err(priv, hw, priv->netdev, \"CB allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tspin_lock_init(&ring->lock);\n\tring->priv = priv;\n\tnetif_napi_add_tx(priv->netdev, &ring->napi, bcm_sysport_tx_poll);\n\tring->index = index;\n\tring->size = size;\n\tring->clean_index = 0;\n\tring->alloc_size = ring->size;\n\tring->desc_count = ring->size;\n\tring->curr_desc = 0;\n\n\t \n\ttdma_writel(priv, RING_EN, TDMA_DESC_RING_HEAD_TAIL_PTR(index));\n\ttdma_writel(priv, 0, TDMA_DESC_RING_COUNT(index));\n\ttdma_writel(priv, 1, TDMA_DESC_RING_INTR_CONTROL(index));\n\ttdma_writel(priv, 0, TDMA_DESC_RING_PROD_CONS_INDEX(index));\n\n\t \n\treg = tdma_readl(priv, TDMA_DESC_RING_MAPPING(index));\n\treg &= ~(RING_QID_MASK | RING_PORT_ID_MASK << RING_PORT_ID_SHIFT);\n\tif (ring->inspect) {\n\t\treg |= ring->switch_queue & RING_QID_MASK;\n\t\treg |= ring->switch_port << RING_PORT_ID_SHIFT;\n\t} else {\n\t\treg |= RING_IGNORE_STATUS;\n\t}\n\ttdma_writel(priv, reg, TDMA_DESC_RING_MAPPING(index));\n\treg = 0;\n\t \n\tif (priv->netdev->features & NETIF_F_HW_VLAN_CTAG_TX)\n\t\treg = VLAN_HLEN << RING_PKT_SIZE_ADJ_SHIFT;\n\ttdma_writel(priv, reg, TDMA_DESC_RING_PCP_DEI_VID(index));\n\n\t \n\treg = tdma_readl(priv, TDMA_CONTROL);\n\treg |= tdma_control_bit(priv, ACB_ALGO);\n\ttdma_writel(priv, reg, TDMA_CONTROL);\n\n\t \n\treg = tdma_readl(priv, TDMA_CONTROL);\n\tif (priv->is_lite)\n\t\treg &= ~BIT(TSB_SWAP1);\n\t \n\tif (!IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))\n\t\treg |= tdma_control_bit(priv, TSB_SWAP0);\n\telse\n\t\treg &= ~tdma_control_bit(priv, TSB_SWAP0);\n\ttdma_writel(priv, reg, TDMA_CONTROL);\n\n\t \n\ttdma_writel(priv, ring->size |\n\t\t\t1 << RING_HYST_THRESH_SHIFT,\n\t\t\tTDMA_DESC_RING_MAX_HYST(index));\n\n\t \n\treg = tdma_readl(priv, TDMA_TIER1_ARB_0_QUEUE_EN);\n\treg |= (1 << index);\n\ttdma_writel(priv, reg, TDMA_TIER1_ARB_0_QUEUE_EN);\n\n\tnapi_enable(&ring->napi);\n\n\tnetif_dbg(priv, hw, priv->netdev,\n\t\t  \"TDMA cfg, size=%d, switch q=%d,port=%d\\n\",\n\t\t  ring->size, ring->switch_queue,\n\t\t  ring->switch_port);\n\n\treturn 0;\n}\n\nstatic void bcm_sysport_fini_tx_ring(struct bcm_sysport_priv *priv,\n\t\t\t\t     unsigned int index)\n{\n\tstruct bcm_sysport_tx_ring *ring = &priv->tx_rings[index];\n\tu32 reg;\n\n\t \n\treg = tdma_readl(priv, TDMA_STATUS);\n\tif (!(reg & TDMA_DISABLED))\n\t\tnetdev_warn(priv->netdev, \"TDMA not stopped!\\n\");\n\n\t \n\tif (!ring->cbs)\n\t\treturn;\n\n\tnapi_disable(&ring->napi);\n\tnetif_napi_del(&ring->napi);\n\n\tbcm_sysport_tx_clean(priv, ring);\n\n\tkfree(ring->cbs);\n\tring->cbs = NULL;\n\tring->size = 0;\n\tring->alloc_size = 0;\n\n\tnetif_dbg(priv, hw, priv->netdev, \"TDMA fini done\\n\");\n}\n\n \nstatic inline int rdma_enable_set(struct bcm_sysport_priv *priv,\n\t\t\t\t  unsigned int enable)\n{\n\tunsigned int timeout = 1000;\n\tu32 reg;\n\n\treg = rdma_readl(priv, RDMA_CONTROL);\n\tif (enable)\n\t\treg |= RDMA_EN;\n\telse\n\t\treg &= ~RDMA_EN;\n\trdma_writel(priv, reg, RDMA_CONTROL);\n\n\t \n\tdo {\n\t\treg = rdma_readl(priv, RDMA_STATUS);\n\t\tif (!!(reg & RDMA_DISABLED) == !enable)\n\t\t\treturn 0;\n\t\tusleep_range(1000, 2000);\n\t} while (timeout-- > 0);\n\n\tnetdev_err(priv->netdev, \"timeout waiting for RDMA to finish\\n\");\n\n\treturn -ETIMEDOUT;\n}\n\n \nstatic inline int tdma_enable_set(struct bcm_sysport_priv *priv,\n\t\t\t\t  unsigned int enable)\n{\n\tunsigned int timeout = 1000;\n\tu32 reg;\n\n\treg = tdma_readl(priv, TDMA_CONTROL);\n\tif (enable)\n\t\treg |= tdma_control_bit(priv, TDMA_EN);\n\telse\n\t\treg &= ~tdma_control_bit(priv, TDMA_EN);\n\ttdma_writel(priv, reg, TDMA_CONTROL);\n\n\t \n\tdo {\n\t\treg = tdma_readl(priv, TDMA_STATUS);\n\t\tif (!!(reg & TDMA_DISABLED) == !enable)\n\t\t\treturn 0;\n\n\t\tusleep_range(1000, 2000);\n\t} while (timeout-- > 0);\n\n\tnetdev_err(priv->netdev, \"timeout waiting for TDMA to finish\\n\");\n\n\treturn -ETIMEDOUT;\n}\n\nstatic int bcm_sysport_init_rx_ring(struct bcm_sysport_priv *priv)\n{\n\tstruct bcm_sysport_cb *cb;\n\tu32 reg;\n\tint ret;\n\tint i;\n\n\t \n\tpriv->num_rx_bds = priv->num_rx_desc_words / WORDS_PER_DESC;\n\tpriv->rx_bds = priv->base + SYS_PORT_RDMA_OFFSET;\n\tpriv->rx_c_index = 0;\n\tpriv->rx_read_ptr = 0;\n\tpriv->rx_cbs = kcalloc(priv->num_rx_bds, sizeof(struct bcm_sysport_cb),\n\t\t\t\tGFP_KERNEL);\n\tif (!priv->rx_cbs) {\n\t\tnetif_err(priv, hw, priv->netdev, \"CB allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < priv->num_rx_bds; i++) {\n\t\tcb = priv->rx_cbs + i;\n\t\tcb->bd_addr = priv->rx_bds + i * DESC_SIZE;\n\t}\n\n\tret = bcm_sysport_alloc_rx_bufs(priv);\n\tif (ret) {\n\t\tnetif_err(priv, hw, priv->netdev, \"SKB allocation failed\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\treg = rdma_readl(priv, RDMA_STATUS);\n\tif (!(reg & RDMA_DISABLED))\n\t\trdma_enable_set(priv, 0);\n\n\trdma_writel(priv, 0, RDMA_WRITE_PTR_LO);\n\trdma_writel(priv, 0, RDMA_WRITE_PTR_HI);\n\trdma_writel(priv, 0, RDMA_PROD_INDEX);\n\trdma_writel(priv, 0, RDMA_CONS_INDEX);\n\trdma_writel(priv, priv->num_rx_bds << RDMA_RING_SIZE_SHIFT |\n\t\t\t  RX_BUF_LENGTH, RDMA_RING_BUF_SIZE);\n\t \n\trdma_writel(priv, 0, RDMA_START_ADDR_HI);\n\trdma_writel(priv, 0, RDMA_START_ADDR_LO);\n\trdma_writel(priv, 0, RDMA_END_ADDR_HI);\n\trdma_writel(priv, priv->num_rx_desc_words - 1, RDMA_END_ADDR_LO);\n\n\tnetif_dbg(priv, hw, priv->netdev,\n\t\t  \"RDMA cfg, num_rx_bds=%d, rx_bds=%p\\n\",\n\t\t  priv->num_rx_bds, priv->rx_bds);\n\n\treturn 0;\n}\n\nstatic void bcm_sysport_fini_rx_ring(struct bcm_sysport_priv *priv)\n{\n\tstruct bcm_sysport_cb *cb;\n\tunsigned int i;\n\tu32 reg;\n\n\t \n\treg = rdma_readl(priv, RDMA_STATUS);\n\tif (!(reg & RDMA_DISABLED))\n\t\tnetdev_warn(priv->netdev, \"RDMA not stopped!\\n\");\n\n\tfor (i = 0; i < priv->num_rx_bds; i++) {\n\t\tcb = &priv->rx_cbs[i];\n\t\tif (dma_unmap_addr(cb, dma_addr))\n\t\t\tdma_unmap_single(&priv->pdev->dev,\n\t\t\t\t\t dma_unmap_addr(cb, dma_addr),\n\t\t\t\t\t RX_BUF_LENGTH, DMA_FROM_DEVICE);\n\t\tbcm_sysport_free_cb(cb);\n\t}\n\n\tkfree(priv->rx_cbs);\n\tpriv->rx_cbs = NULL;\n\n\tnetif_dbg(priv, hw, priv->netdev, \"RDMA fini done\\n\");\n}\n\nstatic void bcm_sysport_set_rx_mode(struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tu32 reg;\n\n\tif (priv->is_lite)\n\t\treturn;\n\n\treg = umac_readl(priv, UMAC_CMD);\n\tif (dev->flags & IFF_PROMISC)\n\t\treg |= CMD_PROMISC;\n\telse\n\t\treg &= ~CMD_PROMISC;\n\tumac_writel(priv, reg, UMAC_CMD);\n\n\t \n\tif (dev->flags & IFF_ALLMULTI)\n\t\treturn;\n}\n\nstatic inline void umac_enable_set(struct bcm_sysport_priv *priv,\n\t\t\t\t   u32 mask, unsigned int enable)\n{\n\tu32 reg;\n\n\tif (!priv->is_lite) {\n\t\treg = umac_readl(priv, UMAC_CMD);\n\t\tif (enable)\n\t\t\treg |= mask;\n\t\telse\n\t\t\treg &= ~mask;\n\t\tumac_writel(priv, reg, UMAC_CMD);\n\t} else {\n\t\treg = gib_readl(priv, GIB_CONTROL);\n\t\tif (enable)\n\t\t\treg |= mask;\n\t\telse\n\t\t\treg &= ~mask;\n\t\tgib_writel(priv, reg, GIB_CONTROL);\n\t}\n\n\t \n\tif (enable == 0)\n\t\tusleep_range(1000, 2000);\n}\n\nstatic inline void umac_reset(struct bcm_sysport_priv *priv)\n{\n\tu32 reg;\n\n\tif (priv->is_lite)\n\t\treturn;\n\n\treg = umac_readl(priv, UMAC_CMD);\n\treg |= CMD_SW_RESET;\n\tumac_writel(priv, reg, UMAC_CMD);\n\tudelay(10);\n\treg = umac_readl(priv, UMAC_CMD);\n\treg &= ~CMD_SW_RESET;\n\tumac_writel(priv, reg, UMAC_CMD);\n}\n\nstatic void umac_set_hw_addr(struct bcm_sysport_priv *priv,\n\t\t\t     const unsigned char *addr)\n{\n\tu32 mac0 = (addr[0] << 24) | (addr[1] << 16) | (addr[2] << 8) |\n\t\t    addr[3];\n\tu32 mac1 = (addr[4] << 8) | addr[5];\n\n\tif (!priv->is_lite) {\n\t\tumac_writel(priv, mac0, UMAC_MAC0);\n\t\tumac_writel(priv, mac1, UMAC_MAC1);\n\t} else {\n\t\tgib_writel(priv, mac0, GIB_MAC0);\n\t\tgib_writel(priv, mac1, GIB_MAC1);\n\t}\n}\n\nstatic void topctrl_flush(struct bcm_sysport_priv *priv)\n{\n\ttopctrl_writel(priv, RX_FLUSH, RX_FLUSH_CNTL);\n\ttopctrl_writel(priv, TX_FLUSH, TX_FLUSH_CNTL);\n\tmdelay(1);\n\ttopctrl_writel(priv, 0, RX_FLUSH_CNTL);\n\ttopctrl_writel(priv, 0, TX_FLUSH_CNTL);\n}\n\nstatic int bcm_sysport_change_mac(struct net_device *dev, void *p)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct sockaddr *addr = p;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EINVAL;\n\n\teth_hw_addr_set(dev, addr->sa_data);\n\n\t \n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tumac_set_hw_addr(priv, dev->dev_addr);\n\n\treturn 0;\n}\n\nstatic void bcm_sysport_get_stats64(struct net_device *dev,\n\t\t\t\t    struct rtnl_link_stats64 *stats)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct bcm_sysport_stats64 *stats64 = &priv->stats64;\n\tunsigned int start;\n\n\tnetdev_stats_to_stats64(stats, &dev->stats);\n\n\tbcm_sysport_update_tx_stats(priv, &stats->tx_bytes,\n\t\t\t\t    &stats->tx_packets);\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(&priv->syncp);\n\t\tstats->rx_packets = stats64->rx_packets;\n\t\tstats->rx_bytes = stats64->rx_bytes;\n\t} while (u64_stats_fetch_retry(&priv->syncp, start));\n}\n\nstatic void bcm_sysport_netif_start(struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\n\t \n\tbcm_sysport_init_dim(priv, bcm_sysport_dim_work);\n\tbcm_sysport_init_rx_coalesce(priv);\n\tnapi_enable(&priv->napi);\n\n\t \n\tintrl2_0_mask_clear(priv, INTRL2_0_RDMA_MBDONE | INTRL2_0_TX_RING_FULL);\n\n\tphy_start(dev->phydev);\n\n\t \n\tif (!priv->is_lite)\n\t\tintrl2_1_mask_clear(priv, 0xffffffff);\n\telse\n\t\tintrl2_0_mask_clear(priv, INTRL2_0_TDMA_MBDONE_MASK);\n}\n\nstatic void rbuf_init(struct bcm_sysport_priv *priv)\n{\n\tu32 reg;\n\n\treg = rbuf_readl(priv, RBUF_CONTROL);\n\treg |= RBUF_4B_ALGN | RBUF_RSB_EN;\n\t \n\tif (priv->is_lite)\n\t\treg &= ~RBUF_RSB_SWAP1;\n\n\t \n\tif (!IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))\n\t\treg |= RBUF_RSB_SWAP0;\n\telse\n\t\treg &= ~RBUF_RSB_SWAP0;\n\trbuf_writel(priv, reg, RBUF_CONTROL);\n}\n\nstatic inline void bcm_sysport_mask_all_intrs(struct bcm_sysport_priv *priv)\n{\n\tintrl2_0_mask_set(priv, 0xffffffff);\n\tintrl2_0_writel(priv, 0xffffffff, INTRL2_CPU_CLEAR);\n\tif (!priv->is_lite) {\n\t\tintrl2_1_mask_set(priv, 0xffffffff);\n\t\tintrl2_1_writel(priv, 0xffffffff, INTRL2_CPU_CLEAR);\n\t}\n}\n\nstatic inline void gib_set_pad_extension(struct bcm_sysport_priv *priv)\n{\n\tu32 reg;\n\n\treg = gib_readl(priv, GIB_CONTROL);\n\t \n\tif (netdev_uses_dsa(priv->netdev)) {\n\t\treg &= ~(GIB_PAD_EXTENSION_MASK << GIB_PAD_EXTENSION_SHIFT);\n\t\treg |= ENET_BRCM_TAG_LEN << GIB_PAD_EXTENSION_SHIFT;\n\t}\n\treg &= ~(GIB_IPG_LEN_MASK << GIB_IPG_LEN_SHIFT);\n\treg |= 12 << GIB_IPG_LEN_SHIFT;\n\tgib_writel(priv, reg, GIB_CONTROL);\n}\n\nstatic int bcm_sysport_open(struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct phy_device *phydev;\n\tunsigned int i;\n\tint ret;\n\n\tclk_prepare_enable(priv->clk);\n\n\t \n\tumac_reset(priv);\n\n\t \n\ttopctrl_flush(priv);\n\n\t \n\tumac_enable_set(priv, CMD_RX_EN | CMD_TX_EN, 0);\n\n\t \n\trbuf_init(priv);\n\n\t \n\tif (!priv->is_lite)\n\t\tumac_writel(priv, UMAC_MAX_MTU_SIZE, UMAC_MAX_FRAME_LEN);\n\telse\n\t\tgib_set_pad_extension(priv);\n\n\t \n\tbcm_sysport_set_features(dev, dev->features);\n\n\t \n\tumac_set_hw_addr(priv, dev->dev_addr);\n\n\tphydev = of_phy_connect(dev, priv->phy_dn, bcm_sysport_adj_link,\n\t\t\t\t0, priv->phy_interface);\n\tif (!phydev) {\n\t\tnetdev_err(dev, \"could not attach to PHY\\n\");\n\t\tret = -ENODEV;\n\t\tgoto out_clk_disable;\n\t}\n\n\t \n\tphydev->mac_managed_pm = true;\n\n\t \n\tpriv->old_duplex = -1;\n\tpriv->old_link = -1;\n\tpriv->old_pause = -1;\n\n\t \n\tbcm_sysport_mask_all_intrs(priv);\n\n\tret = request_irq(priv->irq0, bcm_sysport_rx_isr, 0, dev->name, dev);\n\tif (ret) {\n\t\tnetdev_err(dev, \"failed to request RX interrupt\\n\");\n\t\tgoto out_phy_disconnect;\n\t}\n\n\tif (!priv->is_lite) {\n\t\tret = request_irq(priv->irq1, bcm_sysport_tx_isr, 0,\n\t\t\t\t  dev->name, dev);\n\t\tif (ret) {\n\t\t\tnetdev_err(dev, \"failed to request TX interrupt\\n\");\n\t\t\tgoto out_free_irq0;\n\t\t}\n\t}\n\n\t \n\tspin_lock_init(&priv->desc_lock);\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tret = bcm_sysport_init_tx_ring(priv, i);\n\t\tif (ret) {\n\t\t\tnetdev_err(dev, \"failed to initialize TX ring %d\\n\",\n\t\t\t\t   i);\n\t\t\tgoto out_free_tx_ring;\n\t\t}\n\t}\n\n\t \n\ttdma_writel(priv, TDMA_LL_RAM_INIT_BUSY, TDMA_STATUS);\n\n\t \n\tret = bcm_sysport_init_rx_ring(priv);\n\tif (ret) {\n\t\tnetdev_err(dev, \"failed to initialize RX ring\\n\");\n\t\tgoto out_free_rx_ring;\n\t}\n\n\t \n\tret = rdma_enable_set(priv, 1);\n\tif (ret)\n\t\tgoto out_free_rx_ring;\n\n\t \n\tret = tdma_enable_set(priv, 1);\n\tif (ret)\n\t\tgoto out_clear_rx_int;\n\n\t \n\tumac_enable_set(priv, CMD_RX_EN | CMD_TX_EN, 1);\n\n\tbcm_sysport_netif_start(dev);\n\n\tnetif_tx_start_all_queues(dev);\n\n\treturn 0;\n\nout_clear_rx_int:\n\tintrl2_0_mask_set(priv, INTRL2_0_RDMA_MBDONE | INTRL2_0_TX_RING_FULL);\nout_free_rx_ring:\n\tbcm_sysport_fini_rx_ring(priv);\nout_free_tx_ring:\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tbcm_sysport_fini_tx_ring(priv, i);\n\tif (!priv->is_lite)\n\t\tfree_irq(priv->irq1, dev);\nout_free_irq0:\n\tfree_irq(priv->irq0, dev);\nout_phy_disconnect:\n\tphy_disconnect(phydev);\nout_clk_disable:\n\tclk_disable_unprepare(priv->clk);\n\treturn ret;\n}\n\nstatic void bcm_sysport_netif_stop(struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\n\t \n\tnetif_tx_disable(dev);\n\tnapi_disable(&priv->napi);\n\tcancel_work_sync(&priv->dim.dim.work);\n\tphy_stop(dev->phydev);\n\n\t \n\tbcm_sysport_mask_all_intrs(priv);\n}\n\nstatic int bcm_sysport_stop(struct net_device *dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tunsigned int i;\n\tint ret;\n\n\tbcm_sysport_netif_stop(dev);\n\n\t \n\tumac_enable_set(priv, CMD_RX_EN, 0);\n\n\tret = tdma_enable_set(priv, 0);\n\tif (ret) {\n\t\tnetdev_err(dev, \"timeout disabling RDMA\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tusleep_range(2000, 3000);\n\n\tret = rdma_enable_set(priv, 0);\n\tif (ret) {\n\t\tnetdev_err(dev, \"timeout disabling TDMA\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tumac_enable_set(priv, CMD_TX_EN, 0);\n\n\t \n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tbcm_sysport_fini_tx_ring(priv, i);\n\tbcm_sysport_fini_rx_ring(priv);\n\n\tfree_irq(priv->irq0, dev);\n\tif (!priv->is_lite)\n\t\tfree_irq(priv->irq1, dev);\n\n\t \n\tphy_disconnect(dev->phydev);\n\n\tclk_disable_unprepare(priv->clk);\n\n\treturn 0;\n}\n\nstatic int bcm_sysport_rule_find(struct bcm_sysport_priv *priv,\n\t\t\t\t u64 location)\n{\n\tunsigned int index;\n\tu32 reg;\n\n\tfor_each_set_bit(index, priv->filters, RXCHK_BRCM_TAG_MAX) {\n\t\treg = rxchk_readl(priv, RXCHK_BRCM_TAG(index));\n\t\treg >>= RXCHK_BRCM_TAG_CID_SHIFT;\n\t\treg &= RXCHK_BRCM_TAG_CID_MASK;\n\t\tif (reg == location)\n\t\t\treturn index;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int bcm_sysport_rule_get(struct bcm_sysport_priv *priv,\n\t\t\t\tstruct ethtool_rxnfc *nfc)\n{\n\tint index;\n\n\t \n\tindex = bcm_sysport_rule_find(priv, nfc->fs.location);\n\tif (index < 0)\n\t\treturn -EOPNOTSUPP;\n\n\tnfc->fs.ring_cookie = RX_CLS_FLOW_WAKE;\n\n\treturn 0;\n}\n\nstatic int bcm_sysport_rule_set(struct bcm_sysport_priv *priv,\n\t\t\t\tstruct ethtool_rxnfc *nfc)\n{\n\tunsigned int index;\n\tu32 reg;\n\n\t \n\tif (nfc->fs.location > RXCHK_BRCM_TAG_CID_MASK)\n\t\treturn -E2BIG;\n\n\t \n\tif (nfc->fs.ring_cookie != RX_CLS_FLOW_WAKE)\n\t\treturn -EOPNOTSUPP;\n\n\tindex = find_first_zero_bit(priv->filters, RXCHK_BRCM_TAG_MAX);\n\tif (index >= RXCHK_BRCM_TAG_MAX)\n\t\t \n\t\treturn -ENOSPC;\n\n\t \n\treg = rxchk_readl(priv, RXCHK_BRCM_TAG(index));\n\treg &= ~(RXCHK_BRCM_TAG_CID_MASK << RXCHK_BRCM_TAG_CID_SHIFT);\n\treg |= nfc->fs.location << RXCHK_BRCM_TAG_CID_SHIFT;\n\trxchk_writel(priv, reg, RXCHK_BRCM_TAG(index));\n\trxchk_writel(priv, 0xff00ffff, RXCHK_BRCM_TAG_MASK(index));\n\n\tpriv->filters_loc[index] = nfc->fs.location;\n\tset_bit(index, priv->filters);\n\n\treturn 0;\n}\n\nstatic int bcm_sysport_rule_del(struct bcm_sysport_priv *priv,\n\t\t\t\tu64 location)\n{\n\tint index;\n\n\t \n\tindex = bcm_sysport_rule_find(priv, location);\n\tif (index < 0)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tclear_bit(index, priv->filters);\n\tpriv->filters_loc[index] = 0;\n\n\treturn 0;\n}\n\nstatic int bcm_sysport_get_rxnfc(struct net_device *dev,\n\t\t\t\t struct ethtool_rxnfc *nfc, u32 *rule_locs)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tint ret = -EOPNOTSUPP;\n\n\tswitch (nfc->cmd) {\n\tcase ETHTOOL_GRXCLSRULE:\n\t\tret = bcm_sysport_rule_get(priv, nfc);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int bcm_sysport_set_rxnfc(struct net_device *dev,\n\t\t\t\t struct ethtool_rxnfc *nfc)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tint ret = -EOPNOTSUPP;\n\n\tswitch (nfc->cmd) {\n\tcase ETHTOOL_SRXCLSRLINS:\n\t\tret = bcm_sysport_rule_set(priv, nfc);\n\t\tbreak;\n\tcase ETHTOOL_SRXCLSRLDEL:\n\t\tret = bcm_sysport_rule_del(priv, nfc->fs.location);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct ethtool_ops bcm_sysport_ethtool_ops = {\n\t.supported_coalesce_params = ETHTOOL_COALESCE_USECS |\n\t\t\t\t     ETHTOOL_COALESCE_MAX_FRAMES |\n\t\t\t\t     ETHTOOL_COALESCE_USE_ADAPTIVE_RX,\n\t.get_drvinfo\t\t= bcm_sysport_get_drvinfo,\n\t.get_msglevel\t\t= bcm_sysport_get_msglvl,\n\t.set_msglevel\t\t= bcm_sysport_set_msglvl,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_strings\t\t= bcm_sysport_get_strings,\n\t.get_ethtool_stats\t= bcm_sysport_get_stats,\n\t.get_sset_count\t\t= bcm_sysport_get_sset_count,\n\t.get_wol\t\t= bcm_sysport_get_wol,\n\t.set_wol\t\t= bcm_sysport_set_wol,\n\t.get_coalesce\t\t= bcm_sysport_get_coalesce,\n\t.set_coalesce\t\t= bcm_sysport_set_coalesce,\n\t.get_link_ksettings     = phy_ethtool_get_link_ksettings,\n\t.set_link_ksettings     = phy_ethtool_set_link_ksettings,\n\t.get_rxnfc\t\t= bcm_sysport_get_rxnfc,\n\t.set_rxnfc\t\t= bcm_sysport_set_rxnfc,\n};\n\nstatic u16 bcm_sysport_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t\t\t    struct net_device *sb_dev)\n{\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tu16 queue = skb_get_queue_mapping(skb);\n\tstruct bcm_sysport_tx_ring *tx_ring;\n\tunsigned int q, port;\n\n\tif (!netdev_uses_dsa(dev))\n\t\treturn netdev_pick_tx(dev, skb, NULL);\n\n\t \n\tq = BRCM_TAG_GET_QUEUE(queue);\n\tport = BRCM_TAG_GET_PORT(queue);\n\ttx_ring = priv->ring_map[q + port * priv->per_port_num_tx_queues];\n\n\tif (unlikely(!tx_ring))\n\t\treturn netdev_pick_tx(dev, skb, NULL);\n\n\treturn tx_ring->index;\n}\n\nstatic const struct net_device_ops bcm_sysport_netdev_ops = {\n\t.ndo_start_xmit\t\t= bcm_sysport_xmit,\n\t.ndo_tx_timeout\t\t= bcm_sysport_tx_timeout,\n\t.ndo_open\t\t= bcm_sysport_open,\n\t.ndo_stop\t\t= bcm_sysport_stop,\n\t.ndo_set_features\t= bcm_sysport_set_features,\n\t.ndo_set_rx_mode\t= bcm_sysport_set_rx_mode,\n\t.ndo_set_mac_address\t= bcm_sysport_change_mac,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= bcm_sysport_poll_controller,\n#endif\n\t.ndo_get_stats64\t= bcm_sysport_get_stats64,\n\t.ndo_select_queue\t= bcm_sysport_select_queue,\n};\n\nstatic int bcm_sysport_map_queues(struct net_device *dev,\n\t\t\t\t  struct net_device *slave_dev)\n{\n\tstruct dsa_port *dp = dsa_port_from_netdev(slave_dev);\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct bcm_sysport_tx_ring *ring;\n\tunsigned int num_tx_queues;\n\tunsigned int q, qp, port;\n\n\t \n\tif (dp->ds->index)\n\t\treturn 0;\n\n\tport = dp->index;\n\n\t \n\tif (priv->is_lite)\n\t\tnetif_set_real_num_tx_queues(slave_dev,\n\t\t\t\t\t     slave_dev->num_tx_queues / 2);\n\n\tnum_tx_queues = slave_dev->real_num_tx_queues;\n\n\tif (priv->per_port_num_tx_queues &&\n\t    priv->per_port_num_tx_queues != num_tx_queues)\n\t\tnetdev_warn(slave_dev, \"asymmetric number of per-port queues\\n\");\n\n\tpriv->per_port_num_tx_queues = num_tx_queues;\n\n\tfor (q = 0, qp = 0; q < dev->num_tx_queues && qp < num_tx_queues;\n\t     q++) {\n\t\tring = &priv->tx_rings[q];\n\n\t\tif (ring->inspect)\n\t\t\tcontinue;\n\n\t\t \n\t\tring->switch_queue = qp;\n\t\tring->switch_port = port;\n\t\tring->inspect = true;\n\t\tpriv->ring_map[qp + port * num_tx_queues] = ring;\n\t\tqp++;\n\t}\n\n\treturn 0;\n}\n\nstatic int bcm_sysport_unmap_queues(struct net_device *dev,\n\t\t\t\t    struct net_device *slave_dev)\n{\n\tstruct dsa_port *dp = dsa_port_from_netdev(slave_dev);\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct bcm_sysport_tx_ring *ring;\n\tunsigned int num_tx_queues;\n\tunsigned int q, qp, port;\n\n\tport = dp->index;\n\n\tnum_tx_queues = slave_dev->real_num_tx_queues;\n\n\tfor (q = 0; q < dev->num_tx_queues; q++) {\n\t\tring = &priv->tx_rings[q];\n\n\t\tif (ring->switch_port != port)\n\t\t\tcontinue;\n\n\t\tif (!ring->inspect)\n\t\t\tcontinue;\n\n\t\tring->inspect = false;\n\t\tqp = ring->switch_queue;\n\t\tpriv->ring_map[qp + port * num_tx_queues] = NULL;\n\t}\n\n\treturn 0;\n}\n\nstatic int bcm_sysport_netdevice_event(struct notifier_block *nb,\n\t\t\t\t       unsigned long event, void *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct netdev_notifier_changeupper_info *info = ptr;\n\tstruct bcm_sysport_priv *priv;\n\tint ret = 0;\n\n\tpriv = container_of(nb, struct bcm_sysport_priv, netdev_notifier);\n\tif (priv->netdev != dev)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (event) {\n\tcase NETDEV_CHANGEUPPER:\n\t\tif (dev->netdev_ops != &bcm_sysport_netdev_ops)\n\t\t\treturn NOTIFY_DONE;\n\n\t\tif (!dsa_slave_dev_check(info->upper_dev))\n\t\t\treturn NOTIFY_DONE;\n\n\t\tif (info->linking)\n\t\t\tret = bcm_sysport_map_queues(dev, info->upper_dev);\n\t\telse\n\t\t\tret = bcm_sysport_unmap_queues(dev, info->upper_dev);\n\t\tbreak;\n\t}\n\n\treturn notifier_from_errno(ret);\n}\n\n#define REV_FMT\t\"v%2x.%02x\"\n\nstatic const struct bcm_sysport_hw_params bcm_sysport_params[] = {\n\t[SYSTEMPORT] = {\n\t\t.is_lite = false,\n\t\t.num_rx_desc_words = SP_NUM_HW_RX_DESC_WORDS,\n\t},\n\t[SYSTEMPORT_LITE] = {\n\t\t.is_lite = true,\n\t\t.num_rx_desc_words = SP_LT_NUM_HW_RX_DESC_WORDS,\n\t},\n};\n\nstatic const struct of_device_id bcm_sysport_of_match[] = {\n\t{ .compatible = \"brcm,systemportlite-v1.00\",\n\t  .data = &bcm_sysport_params[SYSTEMPORT_LITE] },\n\t{ .compatible = \"brcm,systemport-v1.00\",\n\t  .data = &bcm_sysport_params[SYSTEMPORT] },\n\t{ .compatible = \"brcm,systemport\",\n\t  .data = &bcm_sysport_params[SYSTEMPORT] },\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, bcm_sysport_of_match);\n\nstatic int bcm_sysport_probe(struct platform_device *pdev)\n{\n\tconst struct bcm_sysport_hw_params *params;\n\tconst struct of_device_id *of_id = NULL;\n\tstruct bcm_sysport_priv *priv;\n\tstruct device_node *dn;\n\tstruct net_device *dev;\n\tu32 txq, rxq;\n\tint ret;\n\n\tdn = pdev->dev.of_node;\n\tof_id = of_match_node(bcm_sysport_of_match, dn);\n\tif (!of_id || !of_id->data)\n\t\treturn -EINVAL;\n\n\tret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(40));\n\tif (ret)\n\t\tret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"unable to set DMA mask: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\t \n\tparams = of_id->data;\n\n\t \n\tif (of_property_read_u32(dn, \"systemport,num-txq\", &txq))\n\t\ttxq = TDMA_NUM_RINGS;\n\tif (of_property_read_u32(dn, \"systemport,num-rxq\", &rxq))\n\t\trxq = 1;\n\n\t \n\tif (!txq || txq > TDMA_NUM_RINGS)\n\t\treturn -EINVAL;\n\n\tdev = alloc_etherdev_mqs(sizeof(*priv), txq, rxq);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\t \n\tpriv = netdev_priv(dev);\n\n\tpriv->clk = devm_clk_get_optional(&pdev->dev, \"sw_sysport\");\n\tif (IS_ERR(priv->clk)) {\n\t\tret = PTR_ERR(priv->clk);\n\t\tgoto err_free_netdev;\n\t}\n\n\t \n\tpriv->tx_rings = devm_kcalloc(&pdev->dev, txq,\n\t\t\t\t      sizeof(struct bcm_sysport_tx_ring),\n\t\t\t\t      GFP_KERNEL);\n\tif (!priv->tx_rings) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_netdev;\n\t}\n\n\tpriv->is_lite = params->is_lite;\n\tpriv->num_rx_desc_words = params->num_rx_desc_words;\n\n\tpriv->irq0 = platform_get_irq(pdev, 0);\n\tif (!priv->is_lite) {\n\t\tpriv->irq1 = platform_get_irq(pdev, 1);\n\t\tpriv->wol_irq = platform_get_irq_optional(pdev, 2);\n\t} else {\n\t\tpriv->wol_irq = platform_get_irq_optional(pdev, 1);\n\t}\n\tif (priv->irq0 <= 0 || (priv->irq1 <= 0 && !priv->is_lite)) {\n\t\tret = -EINVAL;\n\t\tgoto err_free_netdev;\n\t}\n\n\tpriv->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(priv->base)) {\n\t\tret = PTR_ERR(priv->base);\n\t\tgoto err_free_netdev;\n\t}\n\n\tpriv->netdev = dev;\n\tpriv->pdev = pdev;\n\n\tret = of_get_phy_mode(dn, &priv->phy_interface);\n\t \n\tif (ret)\n\t\tpriv->phy_interface = PHY_INTERFACE_MODE_GMII;\n\n\t \n\tif (of_phy_is_fixed_link(dn)) {\n\t\tret = of_phy_register_fixed_link(dn);\n\t\tif (ret) {\n\t\t\tdev_err(&pdev->dev, \"failed to register fixed PHY\\n\");\n\t\t\tgoto err_free_netdev;\n\t\t}\n\n\t\tpriv->phy_dn = dn;\n\t}\n\n\t \n\tret = of_get_ethdev_address(dn, dev);\n\tif (ret) {\n\t\tdev_warn(&pdev->dev, \"using random Ethernet MAC\\n\");\n\t\teth_hw_addr_random(dev);\n\t}\n\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\tdev_set_drvdata(&pdev->dev, dev);\n\tdev->ethtool_ops = &bcm_sysport_ethtool_ops;\n\tdev->netdev_ops = &bcm_sysport_netdev_ops;\n\tnetif_napi_add(dev, &priv->napi, bcm_sysport_poll);\n\n\tdev->features |= NETIF_F_RXCSUM | NETIF_F_HIGHDMA |\n\t\t\t NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\t NETIF_F_HW_VLAN_CTAG_TX;\n\tdev->hw_features |= dev->features;\n\tdev->vlan_features |= dev->features;\n\tdev->max_mtu = UMAC_MAX_MTU_SIZE;\n\n\t \n\tpriv->wol_irq_disabled = 1;\n\tret = devm_request_irq(&pdev->dev, priv->wol_irq,\n\t\t\t       bcm_sysport_wol_isr, 0, dev->name, priv);\n\tif (!ret)\n\t\tdevice_set_wakeup_capable(&pdev->dev, 1);\n\n\tpriv->wol_clk = devm_clk_get_optional(&pdev->dev, \"sw_sysportwol\");\n\tif (IS_ERR(priv->wol_clk)) {\n\t\tret = PTR_ERR(priv->wol_clk);\n\t\tgoto err_deregister_fixed_link;\n\t}\n\n\t \n\tBUILD_BUG_ON(sizeof(struct bcm_tsb) != 8);\n\tdev->needed_headroom += sizeof(struct bcm_tsb);\n\n\t \n\tnetif_carrier_off(dev);\n\n\tpriv->rx_max_coalesced_frames = 1;\n\tu64_stats_init(&priv->syncp);\n\n\tpriv->netdev_notifier.notifier_call = bcm_sysport_netdevice_event;\n\n\tret = register_netdevice_notifier(&priv->netdev_notifier);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to register DSA notifier\\n\");\n\t\tgoto err_deregister_fixed_link;\n\t}\n\n\tret = register_netdev(dev);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"failed to register net_device\\n\");\n\t\tgoto err_deregister_notifier;\n\t}\n\n\tclk_prepare_enable(priv->clk);\n\n\tpriv->rev = topctrl_readl(priv, REV_CNTL) & REV_MASK;\n\tdev_info(&pdev->dev,\n\t\t \"Broadcom SYSTEMPORT%s \" REV_FMT\n\t\t \" (irqs: %d, %d, TXQs: %d, RXQs: %d)\\n\",\n\t\t priv->is_lite ? \" Lite\" : \"\",\n\t\t (priv->rev >> 8) & 0xff, priv->rev & 0xff,\n\t\t priv->irq0, priv->irq1, txq, rxq);\n\n\tclk_disable_unprepare(priv->clk);\n\n\treturn 0;\n\nerr_deregister_notifier:\n\tunregister_netdevice_notifier(&priv->netdev_notifier);\nerr_deregister_fixed_link:\n\tif (of_phy_is_fixed_link(dn))\n\t\tof_phy_deregister_fixed_link(dn);\nerr_free_netdev:\n\tfree_netdev(dev);\n\treturn ret;\n}\n\nstatic int bcm_sysport_remove(struct platform_device *pdev)\n{\n\tstruct net_device *dev = dev_get_drvdata(&pdev->dev);\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tstruct device_node *dn = pdev->dev.of_node;\n\n\t \n\tunregister_netdevice_notifier(&priv->netdev_notifier);\n\tunregister_netdev(dev);\n\tif (of_phy_is_fixed_link(dn))\n\t\tof_phy_deregister_fixed_link(dn);\n\tfree_netdev(dev);\n\tdev_set_drvdata(&pdev->dev, NULL);\n\n\treturn 0;\n}\n\nstatic int bcm_sysport_suspend_to_wol(struct bcm_sysport_priv *priv)\n{\n\tstruct net_device *ndev = priv->netdev;\n\tunsigned int timeout = 1000;\n\tunsigned int index, i = 0;\n\tu32 reg;\n\n\treg = umac_readl(priv, UMAC_MPD_CTRL);\n\tif (priv->wolopts & (WAKE_MAGIC | WAKE_MAGICSECURE))\n\t\treg |= MPD_EN;\n\treg &= ~PSW_EN;\n\tif (priv->wolopts & WAKE_MAGICSECURE) {\n\t\t \n\t\tumac_writel(priv, get_unaligned_be16(&priv->sopass[0]),\n\t\t\t    UMAC_PSW_MS);\n\t\tumac_writel(priv, get_unaligned_be32(&priv->sopass[2]),\n\t\t\t    UMAC_PSW_LS);\n\t\treg |= PSW_EN;\n\t}\n\tumac_writel(priv, reg, UMAC_MPD_CTRL);\n\n\tif (priv->wolopts & WAKE_FILTER) {\n\t\t \n\t\treg = rbuf_readl(priv, RBUF_CONTROL);\n\t\tif (priv->is_lite)\n\t\t\treg |= RBUF_ACPI_EN_LITE;\n\t\telse\n\t\t\treg |= RBUF_ACPI_EN;\n\t\trbuf_writel(priv, reg, RBUF_CONTROL);\n\n\t\t \n\t\treg = rxchk_readl(priv, RXCHK_CONTROL);\n\t\treg &= ~(RXCHK_BRCM_TAG_MATCH_MASK <<\n\t\t\t RXCHK_BRCM_TAG_MATCH_SHIFT);\n\t\tfor_each_set_bit(index, priv->filters, RXCHK_BRCM_TAG_MAX) {\n\t\t\treg |= BIT(RXCHK_BRCM_TAG_MATCH_SHIFT + i);\n\t\t\ti++;\n\t\t}\n\t\treg |= RXCHK_EN | RXCHK_BRCM_TAG_EN;\n\t\trxchk_writel(priv, reg, RXCHK_CONTROL);\n\t}\n\n\t \n\tdo {\n\t\treg = rbuf_readl(priv, RBUF_STATUS);\n\t\tif (reg & RBUF_WOL_MODE)\n\t\t\tbreak;\n\n\t\tudelay(10);\n\t} while (timeout-- > 0);\n\n\t \n\tif (!timeout) {\n\t\tmpd_enable_set(priv, false);\n\t\tnetif_err(priv, wol, ndev, \"failed to enter WOL mode\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\t \n\tumac_enable_set(priv, CMD_RX_EN, 1);\n\n\tnetif_dbg(priv, wol, ndev, \"entered WOL mode\\n\");\n\n\treturn 0;\n}\n\nstatic int __maybe_unused bcm_sysport_suspend(struct device *d)\n{\n\tstruct net_device *dev = dev_get_drvdata(d);\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tunsigned int i;\n\tint ret = 0;\n\tu32 reg;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tnetif_device_detach(dev);\n\n\tbcm_sysport_netif_stop(dev);\n\n\tphy_suspend(dev->phydev);\n\n\t \n\tumac_enable_set(priv, CMD_RX_EN, 0);\n\n\tret = rdma_enable_set(priv, 0);\n\tif (ret) {\n\t\tnetdev_err(dev, \"RDMA timeout!\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tif (priv->rx_chk_en) {\n\t\treg = rxchk_readl(priv, RXCHK_CONTROL);\n\t\treg &= ~RXCHK_EN;\n\t\trxchk_writel(priv, reg, RXCHK_CONTROL);\n\t}\n\n\t \n\tif (!priv->wolopts)\n\t\ttopctrl_writel(priv, RX_FLUSH, RX_FLUSH_CNTL);\n\n\tret = tdma_enable_set(priv, 0);\n\tif (ret) {\n\t\tnetdev_err(dev, \"TDMA timeout!\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tusleep_range(2000, 3000);\n\n\tumac_enable_set(priv, CMD_TX_EN, 0);\n\n\ttopctrl_writel(priv, TX_FLUSH, TX_FLUSH_CNTL);\n\n\t \n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tbcm_sysport_fini_tx_ring(priv, i);\n\tbcm_sysport_fini_rx_ring(priv);\n\n\t \n\tif (device_may_wakeup(d) && priv->wolopts) {\n\t\tclk_prepare_enable(priv->wol_clk);\n\t\tret = bcm_sysport_suspend_to_wol(priv);\n\t}\n\n\tclk_disable_unprepare(priv->clk);\n\n\treturn ret;\n}\n\nstatic int __maybe_unused bcm_sysport_resume(struct device *d)\n{\n\tstruct net_device *dev = dev_get_drvdata(d);\n\tstruct bcm_sysport_priv *priv = netdev_priv(dev);\n\tunsigned int i;\n\tint ret;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tclk_prepare_enable(priv->clk);\n\tif (priv->wolopts)\n\t\tclk_disable_unprepare(priv->wol_clk);\n\n\tumac_reset(priv);\n\n\t \n\tumac_enable_set(priv, CMD_RX_EN | CMD_TX_EN, 0);\n\n\t \n\tbcm_sysport_resume_from_wol(priv);\n\n\t \n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tret = bcm_sysport_init_tx_ring(priv, i);\n\t\tif (ret) {\n\t\t\tnetdev_err(dev, \"failed to initialize TX ring %d\\n\",\n\t\t\t\t   i);\n\t\t\tgoto out_free_tx_rings;\n\t\t}\n\t}\n\n\t \n\ttdma_writel(priv, TDMA_LL_RAM_INIT_BUSY, TDMA_STATUS);\n\n\t \n\tret = bcm_sysport_init_rx_ring(priv);\n\tif (ret) {\n\t\tnetdev_err(dev, \"failed to initialize RX ring\\n\");\n\t\tgoto out_free_rx_ring;\n\t}\n\n\t \n\ttopctrl_writel(priv, 0, RX_FLUSH_CNTL);\n\n\tret = rdma_enable_set(priv, 1);\n\tif (ret) {\n\t\tnetdev_err(dev, \"failed to enable RDMA\\n\");\n\t\tgoto out_free_rx_ring;\n\t}\n\n\t \n\tbcm_sysport_set_features(dev, dev->features);\n\n\trbuf_init(priv);\n\n\t \n\tif (!priv->is_lite)\n\t\tumac_writel(priv, UMAC_MAX_MTU_SIZE, UMAC_MAX_FRAME_LEN);\n\telse\n\t\tgib_set_pad_extension(priv);\n\n\t \n\tumac_set_hw_addr(priv, dev->dev_addr);\n\n\tumac_enable_set(priv, CMD_RX_EN, 1);\n\n\t \n\ttopctrl_writel(priv, 0, TX_FLUSH_CNTL);\n\n\tumac_enable_set(priv, CMD_TX_EN, 1);\n\n\tret = tdma_enable_set(priv, 1);\n\tif (ret) {\n\t\tnetdev_err(dev, \"TDMA timeout!\\n\");\n\t\tgoto out_free_rx_ring;\n\t}\n\n\tphy_resume(dev->phydev);\n\n\tbcm_sysport_netif_start(dev);\n\n\tnetif_device_attach(dev);\n\n\treturn 0;\n\nout_free_rx_ring:\n\tbcm_sysport_fini_rx_ring(priv);\nout_free_tx_rings:\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tbcm_sysport_fini_tx_ring(priv, i);\n\tclk_disable_unprepare(priv->clk);\n\treturn ret;\n}\n\nstatic SIMPLE_DEV_PM_OPS(bcm_sysport_pm_ops,\n\t\tbcm_sysport_suspend, bcm_sysport_resume);\n\nstatic struct platform_driver bcm_sysport_driver = {\n\t.probe\t= bcm_sysport_probe,\n\t.remove\t= bcm_sysport_remove,\n\t.driver =  {\n\t\t.name = \"brcm-systemport\",\n\t\t.of_match_table = bcm_sysport_of_match,\n\t\t.pm = &bcm_sysport_pm_ops,\n\t},\n};\nmodule_platform_driver(bcm_sysport_driver);\n\nMODULE_AUTHOR(\"Broadcom Corporation\");\nMODULE_DESCRIPTION(\"Broadcom System Port Ethernet MAC driver\");\nMODULE_ALIAS(\"platform:brcm-systemport\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}