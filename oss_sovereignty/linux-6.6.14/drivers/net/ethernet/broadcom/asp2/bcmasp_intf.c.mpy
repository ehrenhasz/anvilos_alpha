{
  "module_name": "bcmasp_intf.c",
  "hash_id": "124d036b7c2e7c8ab8bd36c0fe6325228eae86dd9ee9f114ebff97958e207ad7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/broadcom/asp2/bcmasp_intf.c",
  "human_readable_source": "\n#define pr_fmt(fmt)\t\t\t\"bcmasp_intf: \" fmt\n\n#include <asm/byteorder.h>\n#include <linux/brcmphy.h>\n#include <linux/clk.h>\n#include <linux/delay.h>\n#include <linux/etherdevice.h>\n#include <linux/netdevice.h>\n#include <linux/of_net.h>\n#include <linux/of_mdio.h>\n#include <linux/phy.h>\n#include <linux/phy_fixed.h>\n#include <linux/ptp_classify.h>\n#include <linux/platform_device.h>\n#include <net/ip.h>\n#include <net/ipv6.h>\n\n#include \"bcmasp.h\"\n#include \"bcmasp_intf_defs.h\"\n\nstatic int incr_ring(int index, int ring_count)\n{\n\tindex++;\n\tif (index == ring_count)\n\t\treturn 0;\n\n\treturn index;\n}\n\n \nstatic dma_addr_t incr_last_byte(dma_addr_t addr, dma_addr_t beg,\n\t\t\t\t int ring_count)\n{\n\tdma_addr_t end = beg + (ring_count * DESC_SIZE);\n\n\taddr += DESC_SIZE;\n\tif (addr > end)\n\t\treturn beg + DESC_SIZE - 1;\n\n\treturn addr;\n}\n\n \nstatic dma_addr_t incr_first_byte(dma_addr_t addr, dma_addr_t beg,\n\t\t\t\t  int ring_count)\n{\n\tdma_addr_t end = beg + (ring_count * DESC_SIZE);\n\n\taddr += DESC_SIZE;\n\tif (addr >= end)\n\t\treturn beg;\n\n\treturn addr;\n}\n\nstatic void bcmasp_enable_tx(struct bcmasp_intf *intf, int en)\n{\n\tif (en) {\n\t\ttx_spb_ctrl_wl(intf, TX_SPB_CTRL_ENABLE_EN, TX_SPB_CTRL_ENABLE);\n\t\ttx_epkt_core_wl(intf, (TX_EPKT_C_CFG_MISC_EN |\n\t\t\t\tTX_EPKT_C_CFG_MISC_PT |\n\t\t\t\t(intf->port << TX_EPKT_C_CFG_MISC_PS_SHIFT)),\n\t\t\t\tTX_EPKT_C_CFG_MISC);\n\t} else {\n\t\ttx_spb_ctrl_wl(intf, 0x0, TX_SPB_CTRL_ENABLE);\n\t\ttx_epkt_core_wl(intf, 0x0, TX_EPKT_C_CFG_MISC);\n\t}\n}\n\nstatic void bcmasp_enable_rx(struct bcmasp_intf *intf, int en)\n{\n\tif (en)\n\t\trx_edpkt_cfg_wl(intf, RX_EDPKT_CFG_ENABLE_EN,\n\t\t\t\tRX_EDPKT_CFG_ENABLE);\n\telse\n\t\trx_edpkt_cfg_wl(intf, 0x0, RX_EDPKT_CFG_ENABLE);\n}\n\nstatic void bcmasp_set_rx_mode(struct net_device *dev)\n{\n\tunsigned char mask[] = {0xff, 0xff, 0xff, 0xff, 0xff, 0xff};\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\tstruct netdev_hw_addr *ha;\n\tint ret;\n\n\tspin_lock_bh(&intf->parent->mda_lock);\n\n\tbcmasp_disable_all_filters(intf);\n\n\tif (dev->flags & IFF_PROMISC)\n\t\tgoto set_promisc;\n\n\tbcmasp_set_promisc(intf, 0);\n\n\tbcmasp_set_broad(intf, 1);\n\n\tbcmasp_set_oaddr(intf, dev->dev_addr, 1);\n\n\tif (dev->flags & IFF_ALLMULTI) {\n\t\tbcmasp_set_allmulti(intf, 1);\n\t} else {\n\t\tbcmasp_set_allmulti(intf, 0);\n\n\t\tnetdev_for_each_mc_addr(ha, dev) {\n\t\t\tret = bcmasp_set_en_mda_filter(intf, ha->addr, mask);\n\t\t\tif (ret) {\n\t\t\t\tintf->mib.mc_filters_full_cnt++;\n\t\t\t\tgoto set_promisc;\n\t\t\t}\n\t\t}\n\t}\n\n\tnetdev_for_each_uc_addr(ha, dev) {\n\t\tret = bcmasp_set_en_mda_filter(intf, ha->addr, mask);\n\t\tif (ret) {\n\t\t\tintf->mib.uc_filters_full_cnt++;\n\t\t\tgoto set_promisc;\n\t\t}\n\t}\n\n\tspin_unlock_bh(&intf->parent->mda_lock);\n\treturn;\n\nset_promisc:\n\tbcmasp_set_promisc(intf, 1);\n\tintf->mib.promisc_filters_cnt++;\n\n\t \n\tbcmasp_disable_all_filters(intf);\n\n\tspin_unlock_bh(&intf->parent->mda_lock);\n}\n\nstatic void bcmasp_clean_txcb(struct bcmasp_intf *intf, int index)\n{\n\tstruct bcmasp_tx_cb *txcb = &intf->tx_cbs[index];\n\n\ttxcb->skb = NULL;\n\tdma_unmap_addr_set(txcb, dma_addr, 0);\n\tdma_unmap_len_set(txcb, dma_len, 0);\n\ttxcb->last = false;\n}\n\nstatic int tx_spb_ring_full(struct bcmasp_intf *intf, int cnt)\n{\n\tint next_index, i;\n\n\t \n\tfor (i = 0; i < cnt; i++) {\n\t\tnext_index = incr_ring(intf->tx_spb_index, DESC_RING_COUNT);\n\t\tif (next_index == intf->tx_spb_clean_index)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic struct sk_buff *bcmasp_csum_offload(struct net_device *dev,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   bool *csum_hw)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\tu32 header = 0, header2 = 0, epkt = 0;\n\tstruct bcmasp_pkt_offload *offload;\n\tunsigned int header_cnt = 0;\n\tu8 ip_proto;\n\tint ret;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn skb;\n\n\tret = skb_cow_head(skb, sizeof(*offload));\n\tif (ret < 0) {\n\t\tintf->mib.tx_realloc_offload_failed++;\n\t\tgoto help;\n\t}\n\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\theader |= PKT_OFFLOAD_HDR_SIZE_2((ip_hdrlen(skb) >> 8) & 0xf);\n\t\theader2 |= PKT_OFFLOAD_HDR2_SIZE_2(ip_hdrlen(skb) & 0xff);\n\t\tepkt |= PKT_OFFLOAD_EPKT_IP(0) | PKT_OFFLOAD_EPKT_CSUM_L2;\n\t\tip_proto = ip_hdr(skb)->protocol;\n\t\theader_cnt += 2;\n\t\tbreak;\n\tcase htons(ETH_P_IPV6):\n\t\theader |= PKT_OFFLOAD_HDR_SIZE_2((IP6_HLEN >> 8) & 0xf);\n\t\theader2 |= PKT_OFFLOAD_HDR2_SIZE_2(IP6_HLEN & 0xff);\n\t\tepkt |= PKT_OFFLOAD_EPKT_IP(1) | PKT_OFFLOAD_EPKT_CSUM_L2;\n\t\tip_proto = ipv6_hdr(skb)->nexthdr;\n\t\theader_cnt += 2;\n\t\tbreak;\n\tdefault:\n\t\tgoto help;\n\t}\n\n\tswitch (ip_proto) {\n\tcase IPPROTO_TCP:\n\t\theader2 |= PKT_OFFLOAD_HDR2_SIZE_3(tcp_hdrlen(skb));\n\t\tepkt |= PKT_OFFLOAD_EPKT_TP(0) | PKT_OFFLOAD_EPKT_CSUM_L3;\n\t\theader_cnt++;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\theader2 |= PKT_OFFLOAD_HDR2_SIZE_3(UDP_HLEN);\n\t\tepkt |= PKT_OFFLOAD_EPKT_TP(1) | PKT_OFFLOAD_EPKT_CSUM_L3;\n\t\theader_cnt++;\n\t\tbreak;\n\tdefault:\n\t\tgoto help;\n\t}\n\n\toffload = (struct bcmasp_pkt_offload *)skb_push(skb, sizeof(*offload));\n\n\theader |= PKT_OFFLOAD_HDR_OP | PKT_OFFLOAD_HDR_COUNT(header_cnt) |\n\t\t  PKT_OFFLOAD_HDR_SIZE_1(ETH_HLEN);\n\tepkt |= PKT_OFFLOAD_EPKT_OP;\n\n\toffload->nop = htonl(PKT_OFFLOAD_NOP);\n\toffload->header = htonl(header);\n\toffload->header2 = htonl(header2);\n\toffload->epkt = htonl(epkt);\n\toffload->end = htonl(PKT_OFFLOAD_END_OP);\n\t*csum_hw = true;\n\n\treturn skb;\n\nhelp:\n\tskb_checksum_help(skb);\n\n\treturn skb;\n}\n\nstatic unsigned long bcmasp_rx_edpkt_dma_rq(struct bcmasp_intf *intf)\n{\n\treturn rx_edpkt_dma_rq(intf, RX_EDPKT_DMA_VALID);\n}\n\nstatic void bcmasp_rx_edpkt_cfg_wq(struct bcmasp_intf *intf, dma_addr_t addr)\n{\n\trx_edpkt_cfg_wq(intf, addr, RX_EDPKT_RING_BUFFER_READ);\n}\n\nstatic void bcmasp_rx_edpkt_dma_wq(struct bcmasp_intf *intf, dma_addr_t addr)\n{\n\trx_edpkt_dma_wq(intf, addr, RX_EDPKT_DMA_READ);\n}\n\nstatic unsigned long bcmasp_tx_spb_dma_rq(struct bcmasp_intf *intf)\n{\n\treturn tx_spb_dma_rq(intf, TX_SPB_DMA_READ);\n}\n\nstatic void bcmasp_tx_spb_dma_wq(struct bcmasp_intf *intf, dma_addr_t addr)\n{\n\ttx_spb_dma_wq(intf, addr, TX_SPB_DMA_VALID);\n}\n\nstatic const struct bcmasp_intf_ops bcmasp_intf_ops = {\n\t.rx_desc_read = bcmasp_rx_edpkt_dma_rq,\n\t.rx_buffer_write = bcmasp_rx_edpkt_cfg_wq,\n\t.rx_desc_write = bcmasp_rx_edpkt_dma_wq,\n\t.tx_read = bcmasp_tx_spb_dma_rq,\n\t.tx_write = bcmasp_tx_spb_dma_wq,\n};\n\nstatic netdev_tx_t bcmasp_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\tunsigned int total_bytes, size;\n\tint spb_index, nr_frags, i, j;\n\tstruct bcmasp_tx_cb *txcb;\n\tdma_addr_t mapping, valid;\n\tstruct bcmasp_desc *desc;\n\tbool csum_hw = false;\n\tstruct device *kdev;\n\tskb_frag_t *frag;\n\n\tkdev = &intf->parent->pdev->dev;\n\n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\tif (tx_spb_ring_full(intf, nr_frags + 1)) {\n\t\tnetif_stop_queue(dev);\n\t\tif (net_ratelimit())\n\t\t\tnetdev_err(dev, \"Tx Ring Full!\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\ttotal_bytes = skb->len;\n\tskb = bcmasp_csum_offload(dev, skb, &csum_hw);\n\tif (!skb)\n\t\treturn NETDEV_TX_OK;\n\n\tspb_index = intf->tx_spb_index;\n\tvalid = intf->tx_spb_dma_valid;\n\tfor (i = 0; i <= nr_frags; i++) {\n\t\tif (!i) {\n\t\t\tsize = skb_headlen(skb);\n\t\t\tif (!nr_frags && size < (ETH_ZLEN + ETH_FCS_LEN)) {\n\t\t\t\tif (skb_put_padto(skb, ETH_ZLEN + ETH_FCS_LEN))\n\t\t\t\t\treturn NETDEV_TX_OK;\n\t\t\t\tsize = skb->len;\n\t\t\t}\n\t\t\tmapping = dma_map_single(kdev, skb->data, size,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\t} else {\n\t\t\tfrag = &skb_shinfo(skb)->frags[i - 1];\n\t\t\tsize = skb_frag_size(frag);\n\t\t\tmapping = skb_frag_dma_map(kdev, frag, 0, size,\n\t\t\t\t\t\t   DMA_TO_DEVICE);\n\t\t}\n\n\t\tif (dma_mapping_error(kdev, mapping)) {\n\t\t\tintf->mib.tx_dma_failed++;\n\t\t\tspb_index = intf->tx_spb_index;\n\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\tbcmasp_clean_txcb(intf, spb_index);\n\t\t\t\tspb_index = incr_ring(spb_index,\n\t\t\t\t\t\t      DESC_RING_COUNT);\n\t\t\t}\n\t\t\t \n\t\t\tspb_index = intf->tx_spb_index;\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\ttxcb = &intf->tx_cbs[spb_index];\n\t\tdesc = &intf->tx_spb_cpu[spb_index];\n\t\tmemset(desc, 0, sizeof(*desc));\n\t\ttxcb->skb = skb;\n\t\ttxcb->bytes_sent = total_bytes;\n\t\tdma_unmap_addr_set(txcb, dma_addr, mapping);\n\t\tdma_unmap_len_set(txcb, dma_len, size);\n\t\tif (!i) {\n\t\t\tdesc->flags |= DESC_SOF;\n\t\t\tif (csum_hw)\n\t\t\t\tdesc->flags |= DESC_EPKT_CMD;\n\t\t}\n\n\t\tif (i == nr_frags) {\n\t\t\tdesc->flags |= DESC_EOF;\n\t\t\ttxcb->last = true;\n\t\t}\n\n\t\tdesc->buf = mapping;\n\t\tdesc->size = size;\n\t\tdesc->flags |= DESC_INT_EN;\n\n\t\tnetif_dbg(intf, tx_queued, dev,\n\t\t\t  \"%s dma_buf=%pad dma_len=0x%x flags=0x%x index=0x%x\\n\",\n\t\t\t  __func__, &mapping, desc->size, desc->flags,\n\t\t\t  spb_index);\n\n\t\tspb_index = incr_ring(spb_index, DESC_RING_COUNT);\n\t\tvalid = incr_last_byte(valid, intf->tx_spb_dma_addr,\n\t\t\t\t       DESC_RING_COUNT);\n\t}\n\n\t \n\twmb();\n\n\tintf->tx_spb_index = spb_index;\n\tintf->tx_spb_dma_valid = valid;\n\tbcmasp_intf_tx_write(intf, intf->tx_spb_dma_valid);\n\n\tif (tx_spb_ring_full(intf, MAX_SKB_FRAGS + 1))\n\t\tnetif_stop_queue(dev);\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic void bcmasp_netif_start(struct net_device *dev)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\n\tbcmasp_set_rx_mode(dev);\n\tnapi_enable(&intf->tx_napi);\n\tnapi_enable(&intf->rx_napi);\n\n\tbcmasp_enable_rx_irq(intf, 1);\n\tbcmasp_enable_tx_irq(intf, 1);\n\n\tphy_start(dev->phydev);\n}\n\nstatic void umac_reset(struct bcmasp_intf *intf)\n{\n\tumac_wl(intf, 0x0, UMC_CMD);\n\tumac_wl(intf, UMC_CMD_SW_RESET, UMC_CMD);\n\tusleep_range(10, 100);\n\tumac_wl(intf, 0x0, UMC_CMD);\n}\n\nstatic void umac_set_hw_addr(struct bcmasp_intf *intf,\n\t\t\t     const unsigned char *addr)\n{\n\tu32 mac0 = (addr[0] << 24) | (addr[1] << 16) | (addr[2] << 8) |\n\t\t    addr[3];\n\tu32 mac1 = (addr[4] << 8) | addr[5];\n\n\tumac_wl(intf, mac0, UMC_MAC0);\n\tumac_wl(intf, mac1, UMC_MAC1);\n}\n\nstatic void umac_enable_set(struct bcmasp_intf *intf, u32 mask,\n\t\t\t    unsigned int enable)\n{\n\tu32 reg;\n\n\treg = umac_rl(intf, UMC_CMD);\n\tif (enable)\n\t\treg |= mask;\n\telse\n\t\treg &= ~mask;\n\tumac_wl(intf, reg, UMC_CMD);\n\n\t \n\tif (enable == 0)\n\t\tusleep_range(1000, 2000);\n}\n\nstatic void umac_init(struct bcmasp_intf *intf)\n{\n\tumac_wl(intf, 0x800, UMC_FRM_LEN);\n\tumac_wl(intf, 0xffff, UMC_PAUSE_CNTRL);\n\tumac_wl(intf, 0x800, UMC_RX_MAX_PKT_SZ);\n\tumac_enable_set(intf, UMC_CMD_PROMISC, 1);\n}\n\nstatic int bcmasp_tx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct bcmasp_intf *intf =\n\t\tcontainer_of(napi, struct bcmasp_intf, tx_napi);\n\tstruct bcmasp_intf_stats64 *stats = &intf->stats64;\n\tstruct device *kdev = &intf->parent->pdev->dev;\n\tunsigned long read, released = 0;\n\tstruct bcmasp_tx_cb *txcb;\n\tstruct bcmasp_desc *desc;\n\tdma_addr_t mapping;\n\n\tread = bcmasp_intf_tx_read(intf);\n\twhile (intf->tx_spb_dma_read != read) {\n\t\ttxcb = &intf->tx_cbs[intf->tx_spb_clean_index];\n\t\tmapping = dma_unmap_addr(txcb, dma_addr);\n\n\t\tdma_unmap_single(kdev, mapping,\n\t\t\t\t dma_unmap_len(txcb, dma_len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\tif (txcb->last) {\n\t\t\tdev_consume_skb_any(txcb->skb);\n\n\t\t\tu64_stats_update_begin(&stats->syncp);\n\t\t\tu64_stats_inc(&stats->tx_packets);\n\t\t\tu64_stats_add(&stats->tx_bytes, txcb->bytes_sent);\n\t\t\tu64_stats_update_end(&stats->syncp);\n\t\t}\n\n\t\tdesc = &intf->tx_spb_cpu[intf->tx_spb_clean_index];\n\n\t\tnetif_dbg(intf, tx_done, intf->ndev,\n\t\t\t  \"%s dma_buf=%pad dma_len=0x%x flags=0x%x c_index=0x%x\\n\",\n\t\t\t  __func__, &mapping, desc->size, desc->flags,\n\t\t\t  intf->tx_spb_clean_index);\n\n\t\tbcmasp_clean_txcb(intf, intf->tx_spb_clean_index);\n\t\treleased++;\n\n\t\tintf->tx_spb_clean_index = incr_ring(intf->tx_spb_clean_index,\n\t\t\t\t\t\t     DESC_RING_COUNT);\n\t\tintf->tx_spb_dma_read = incr_first_byte(intf->tx_spb_dma_read,\n\t\t\t\t\t\t\tintf->tx_spb_dma_addr,\n\t\t\t\t\t\t\tDESC_RING_COUNT);\n\t}\n\n\t \n\twmb();\n\n\tnapi_complete(&intf->tx_napi);\n\n\tbcmasp_enable_tx_irq(intf, 1);\n\n\tif (released)\n\t\tnetif_wake_queue(intf->ndev);\n\n\treturn 0;\n}\n\nstatic int bcmasp_rx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct bcmasp_intf *intf =\n\t\tcontainer_of(napi, struct bcmasp_intf, rx_napi);\n\tstruct bcmasp_intf_stats64 *stats = &intf->stats64;\n\tstruct device *kdev = &intf->parent->pdev->dev;\n\tunsigned long processed = 0;\n\tstruct bcmasp_desc *desc;\n\tstruct sk_buff *skb;\n\tdma_addr_t valid;\n\tvoid *data;\n\tu64 flags;\n\tu32 len;\n\n\tvalid = bcmasp_intf_rx_desc_read(intf) + 1;\n\tif (valid == intf->rx_edpkt_dma_addr + DESC_RING_SIZE)\n\t\tvalid = intf->rx_edpkt_dma_addr;\n\n\twhile ((processed < budget) && (valid != intf->rx_edpkt_dma_read)) {\n\t\tdesc = &intf->rx_edpkt_cpu[intf->rx_edpkt_index];\n\n\t\t \n\t\trmb();\n\n\t\t \n\t\tdata = intf->rx_ring_cpu +\n\t\t\t(DESC_ADDR(desc->buf) - intf->rx_ring_dma);\n\n\t\tflags = DESC_FLAGS(desc->buf);\n\t\tif (unlikely(flags & (DESC_CRC_ERR | DESC_RX_SYM_ERR))) {\n\t\t\tif (net_ratelimit()) {\n\t\t\t\tnetif_err(intf, rx_status, intf->ndev,\n\t\t\t\t\t  \"flags=0x%llx\\n\", flags);\n\t\t\t}\n\n\t\t\tu64_stats_update_begin(&stats->syncp);\n\t\t\tif (flags & DESC_CRC_ERR)\n\t\t\t\tu64_stats_inc(&stats->rx_crc_errs);\n\t\t\tif (flags & DESC_RX_SYM_ERR)\n\t\t\t\tu64_stats_inc(&stats->rx_sym_errs);\n\t\t\tu64_stats_update_end(&stats->syncp);\n\n\t\t\tgoto next;\n\t\t}\n\n\t\tdma_sync_single_for_cpu(kdev, DESC_ADDR(desc->buf), desc->size,\n\t\t\t\t\tDMA_FROM_DEVICE);\n\n\t\tlen = desc->size;\n\n\t\tskb = napi_alloc_skb(napi, len);\n\t\tif (!skb) {\n\t\t\tu64_stats_update_begin(&stats->syncp);\n\t\t\tu64_stats_inc(&stats->rx_dropped);\n\t\t\tu64_stats_update_end(&stats->syncp);\n\t\t\tintf->mib.alloc_rx_skb_failed++;\n\n\t\t\tgoto next;\n\t\t}\n\n\t\tskb_put(skb, len);\n\t\tmemcpy(skb->data, data, len);\n\n\t\tskb_pull(skb, 2);\n\t\tlen -= 2;\n\t\tif (likely(intf->crc_fwd)) {\n\t\t\tskb_trim(skb, len - ETH_FCS_LEN);\n\t\t\tlen -= ETH_FCS_LEN;\n\t\t}\n\n\t\tif ((intf->ndev->features & NETIF_F_RXCSUM) &&\n\t\t    (desc->buf & DESC_CHKSUM))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\tskb->protocol = eth_type_trans(skb, intf->ndev);\n\n\t\tnapi_gro_receive(napi, skb);\n\n\t\tu64_stats_update_begin(&stats->syncp);\n\t\tu64_stats_inc(&stats->rx_packets);\n\t\tu64_stats_add(&stats->rx_bytes, len);\n\t\tu64_stats_update_end(&stats->syncp);\n\nnext:\n\t\tbcmasp_intf_rx_buffer_write(intf, (DESC_ADDR(desc->buf) +\n\t\t\t\t\t    desc->size));\n\n\t\tprocessed++;\n\t\tintf->rx_edpkt_dma_read =\n\t\t\tincr_first_byte(intf->rx_edpkt_dma_read,\n\t\t\t\t\tintf->rx_edpkt_dma_addr,\n\t\t\t\t\tDESC_RING_COUNT);\n\t\tintf->rx_edpkt_index = incr_ring(intf->rx_edpkt_index,\n\t\t\t\t\t\t DESC_RING_COUNT);\n\t}\n\n\tbcmasp_intf_rx_desc_write(intf, intf->rx_edpkt_dma_read);\n\n\tif (processed < budget) {\n\t\tnapi_complete_done(&intf->rx_napi, processed);\n\t\tbcmasp_enable_rx_irq(intf, 1);\n\t}\n\n\treturn processed;\n}\n\nstatic void bcmasp_adj_link(struct net_device *dev)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\tstruct phy_device *phydev = dev->phydev;\n\tu32 cmd_bits = 0, reg;\n\tint changed = 0;\n\n\tif (intf->old_link != phydev->link) {\n\t\tchanged = 1;\n\t\tintf->old_link = phydev->link;\n\t}\n\n\tif (intf->old_duplex != phydev->duplex) {\n\t\tchanged = 1;\n\t\tintf->old_duplex = phydev->duplex;\n\t}\n\n\tswitch (phydev->speed) {\n\tcase SPEED_2500:\n\t\tcmd_bits = UMC_CMD_SPEED_2500;\n\t\tbreak;\n\tcase SPEED_1000:\n\t\tcmd_bits = UMC_CMD_SPEED_1000;\n\t\tbreak;\n\tcase SPEED_100:\n\t\tcmd_bits = UMC_CMD_SPEED_100;\n\t\tbreak;\n\tcase SPEED_10:\n\t\tcmd_bits = UMC_CMD_SPEED_10;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tcmd_bits <<= UMC_CMD_SPEED_SHIFT;\n\n\tif (phydev->duplex == DUPLEX_HALF)\n\t\tcmd_bits |= UMC_CMD_HD_EN;\n\n\tif (intf->old_pause != phydev->pause) {\n\t\tchanged = 1;\n\t\tintf->old_pause = phydev->pause;\n\t}\n\n\tif (!phydev->pause)\n\t\tcmd_bits |= UMC_CMD_RX_PAUSE_IGNORE | UMC_CMD_TX_PAUSE_IGNORE;\n\n\tif (!changed)\n\t\treturn;\n\n\tif (phydev->link) {\n\t\treg = umac_rl(intf, UMC_CMD);\n\t\treg &= ~((UMC_CMD_SPEED_MASK << UMC_CMD_SPEED_SHIFT) |\n\t\t\tUMC_CMD_HD_EN | UMC_CMD_RX_PAUSE_IGNORE |\n\t\t\tUMC_CMD_TX_PAUSE_IGNORE);\n\t\treg |= cmd_bits;\n\t\tumac_wl(intf, reg, UMC_CMD);\n\n\t\tintf->eee.eee_active = phy_init_eee(phydev, 0) >= 0;\n\t\tbcmasp_eee_enable_set(intf, intf->eee.eee_active);\n\t}\n\n\treg = rgmii_rl(intf, RGMII_OOB_CNTRL);\n\tif (phydev->link)\n\t\treg |= RGMII_LINK;\n\telse\n\t\treg &= ~RGMII_LINK;\n\trgmii_wl(intf, reg, RGMII_OOB_CNTRL);\n\n\tif (changed)\n\t\tphy_print_status(phydev);\n}\n\nstatic int bcmasp_init_rx(struct bcmasp_intf *intf)\n{\n\tstruct device *kdev = &intf->parent->pdev->dev;\n\tstruct page *buffer_pg;\n\tdma_addr_t dma;\n\tvoid *p;\n\tu32 reg;\n\tint ret;\n\n\tintf->rx_buf_order = get_order(RING_BUFFER_SIZE);\n\tbuffer_pg = alloc_pages(GFP_KERNEL, intf->rx_buf_order);\n\n\tdma = dma_map_page(kdev, buffer_pg, 0, RING_BUFFER_SIZE,\n\t\t\t   DMA_FROM_DEVICE);\n\tif (dma_mapping_error(kdev, dma)) {\n\t\t__free_pages(buffer_pg, intf->rx_buf_order);\n\t\treturn -ENOMEM;\n\t}\n\tintf->rx_ring_cpu = page_to_virt(buffer_pg);\n\tintf->rx_ring_dma = dma;\n\tintf->rx_ring_dma_valid = intf->rx_ring_dma + RING_BUFFER_SIZE - 1;\n\n\tp = dma_alloc_coherent(kdev, DESC_RING_SIZE, &intf->rx_edpkt_dma_addr,\n\t\t\t       GFP_KERNEL);\n\tif (!p) {\n\t\tret = -ENOMEM;\n\t\tgoto free_rx_ring;\n\t}\n\tintf->rx_edpkt_cpu = p;\n\n\tnetif_napi_add(intf->ndev, &intf->rx_napi, bcmasp_rx_poll);\n\n\tintf->rx_edpkt_dma_read = intf->rx_edpkt_dma_addr;\n\tintf->rx_edpkt_index = 0;\n\n\t \n\trx_edpkt_cfg_wl(intf, 0x0, RX_EDPKT_CFG_ENABLE);\n\n\t \n\trx_edpkt_cfg_wq(intf, intf->rx_ring_dma, RX_EDPKT_RING_BUFFER_READ);\n\trx_edpkt_cfg_wq(intf, intf->rx_ring_dma, RX_EDPKT_RING_BUFFER_WRITE);\n\trx_edpkt_cfg_wq(intf, intf->rx_ring_dma, RX_EDPKT_RING_BUFFER_BASE);\n\trx_edpkt_cfg_wq(intf, intf->rx_ring_dma_valid,\n\t\t\tRX_EDPKT_RING_BUFFER_END);\n\trx_edpkt_cfg_wq(intf, intf->rx_ring_dma_valid,\n\t\t\tRX_EDPKT_RING_BUFFER_VALID);\n\n\t \n\trx_edpkt_cfg_wl(intf, (RX_EDPKT_CFG_CFG0_RBUF_4K <<\n\t\t\tRX_EDPKT_CFG_CFG0_DBUF_SHIFT) |\n\t\t       (RX_EDPKT_CFG_CFG0_64_ALN <<\n\t\t\tRX_EDPKT_CFG_CFG0_BALN_SHIFT) |\n\t\t       (RX_EDPKT_CFG_CFG0_EFRM_STUF),\n\t\t\tRX_EDPKT_CFG_CFG0);\n\trx_edpkt_dma_wq(intf, intf->rx_edpkt_dma_addr, RX_EDPKT_DMA_WRITE);\n\trx_edpkt_dma_wq(intf, intf->rx_edpkt_dma_addr, RX_EDPKT_DMA_READ);\n\trx_edpkt_dma_wq(intf, intf->rx_edpkt_dma_addr, RX_EDPKT_DMA_BASE);\n\trx_edpkt_dma_wq(intf, intf->rx_edpkt_dma_addr + (DESC_RING_SIZE - 1),\n\t\t\tRX_EDPKT_DMA_END);\n\trx_edpkt_dma_wq(intf, intf->rx_edpkt_dma_addr + (DESC_RING_SIZE - 1),\n\t\t\tRX_EDPKT_DMA_VALID);\n\n\treg = UMAC2FB_CFG_DEFAULT_EN |\n\t      ((intf->channel + 11) << UMAC2FB_CFG_CHID_SHIFT);\n\treg |= (0xd << UMAC2FB_CFG_OK_SEND_SHIFT);\n\tumac2fb_wl(intf, reg, UMAC2FB_CFG);\n\n\treturn 0;\n\nfree_rx_ring:\n\tdma_unmap_page(kdev, intf->rx_ring_dma, RING_BUFFER_SIZE,\n\t\t       DMA_FROM_DEVICE);\n\t__free_pages(virt_to_page(intf->rx_ring_cpu), intf->rx_buf_order);\n\n\treturn ret;\n}\n\nstatic void bcmasp_reclaim_free_all_rx(struct bcmasp_intf *intf)\n{\n\tstruct device *kdev = &intf->parent->pdev->dev;\n\n\tdma_free_coherent(kdev, DESC_RING_SIZE, intf->rx_edpkt_cpu,\n\t\t\t  intf->rx_edpkt_dma_addr);\n\tdma_unmap_page(kdev, intf->rx_ring_dma, RING_BUFFER_SIZE,\n\t\t       DMA_FROM_DEVICE);\n\t__free_pages(virt_to_page(intf->rx_ring_cpu), intf->rx_buf_order);\n}\n\nstatic int bcmasp_init_tx(struct bcmasp_intf *intf)\n{\n\tstruct device *kdev = &intf->parent->pdev->dev;\n\tvoid *p;\n\tint ret;\n\n\tp = dma_alloc_coherent(kdev, DESC_RING_SIZE, &intf->tx_spb_dma_addr,\n\t\t\t       GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tintf->tx_spb_cpu = p;\n\tintf->tx_spb_dma_valid = intf->tx_spb_dma_addr + DESC_RING_SIZE - 1;\n\tintf->tx_spb_dma_read = intf->tx_spb_dma_addr;\n\n\tintf->tx_cbs = kcalloc(DESC_RING_COUNT, sizeof(struct bcmasp_tx_cb),\n\t\t\t       GFP_KERNEL);\n\tif (!intf->tx_cbs) {\n\t\tret = -ENOMEM;\n\t\tgoto free_tx_spb;\n\t}\n\n\tintf->tx_spb_index = 0;\n\tintf->tx_spb_clean_index = 0;\n\n\tnetif_napi_add_tx(intf->ndev, &intf->tx_napi, bcmasp_tx_poll);\n\n\t \n\ttx_spb_ctrl_wl(intf, 0x0, TX_SPB_CTRL_ENABLE);\n\ttx_epkt_core_wl(intf, 0x0, TX_EPKT_C_CFG_MISC);\n\n\t \n\ttx_spb_ctrl_wl(intf, ((intf->channel + 8) << TX_SPB_CTRL_XF_BID_SHIFT),\n\t\t       TX_SPB_CTRL_XF_CTRL2);\n\ttx_pause_ctrl_wl(intf, (1 << (intf->channel + 8)), TX_PAUSE_MAP_VECTOR);\n\ttx_spb_top_wl(intf, 0x1e, TX_SPB_TOP_BLKOUT);\n\ttx_spb_top_wl(intf, 0x0, TX_SPB_TOP_SPRE_BW_CTRL);\n\n\ttx_spb_dma_wq(intf, intf->tx_spb_dma_addr, TX_SPB_DMA_READ);\n\ttx_spb_dma_wq(intf, intf->tx_spb_dma_addr, TX_SPB_DMA_BASE);\n\ttx_spb_dma_wq(intf, intf->tx_spb_dma_valid, TX_SPB_DMA_END);\n\ttx_spb_dma_wq(intf, intf->tx_spb_dma_valid, TX_SPB_DMA_VALID);\n\n\treturn 0;\n\nfree_tx_spb:\n\tdma_free_coherent(kdev, DESC_RING_SIZE, intf->tx_spb_cpu,\n\t\t\t  intf->tx_spb_dma_addr);\n\n\treturn ret;\n}\n\nstatic void bcmasp_reclaim_free_all_tx(struct bcmasp_intf *intf)\n{\n\tstruct device *kdev = &intf->parent->pdev->dev;\n\n\t \n\tdma_free_coherent(kdev, DESC_RING_SIZE, intf->tx_spb_cpu,\n\t\t\t  intf->tx_spb_dma_addr);\n\n\t \n\tkfree(intf->tx_cbs);\n}\n\nstatic void bcmasp_ephy_enable_set(struct bcmasp_intf *intf, bool enable)\n{\n\tu32 mask = RGMII_EPHY_CFG_IDDQ_BIAS | RGMII_EPHY_CFG_EXT_PWRDOWN |\n\t\t   RGMII_EPHY_CFG_IDDQ_GLOBAL;\n\tu32 reg;\n\n\treg = rgmii_rl(intf, RGMII_EPHY_CNTRL);\n\tif (enable) {\n\t\treg &= ~RGMII_EPHY_CK25_DIS;\n\t\trgmii_wl(intf, reg, RGMII_EPHY_CNTRL);\n\t\tmdelay(1);\n\n\t\treg &= ~mask;\n\t\treg |= RGMII_EPHY_RESET;\n\t\trgmii_wl(intf, reg, RGMII_EPHY_CNTRL);\n\t\tmdelay(1);\n\n\t\treg &= ~RGMII_EPHY_RESET;\n\t} else {\n\t\treg |= mask | RGMII_EPHY_RESET;\n\t\trgmii_wl(intf, reg, RGMII_EPHY_CNTRL);\n\t\tmdelay(1);\n\t\treg |= RGMII_EPHY_CK25_DIS;\n\t}\n\trgmii_wl(intf, reg, RGMII_EPHY_CNTRL);\n\tmdelay(1);\n\n\t \n\treg = rgmii_rl(intf, RGMII_SYS_LED_CNTRL);\n\tif (enable)\n\t\treg &= ~RGMII_SYS_LED_CNTRL_LINK_OVRD;\n\telse\n\t\treg |= RGMII_SYS_LED_CNTRL_LINK_OVRD;\n\trgmii_wl(intf, reg, RGMII_SYS_LED_CNTRL);\n}\n\nstatic void bcmasp_rgmii_mode_en_set(struct bcmasp_intf *intf, bool enable)\n{\n\tu32 reg;\n\n\treg = rgmii_rl(intf, RGMII_OOB_CNTRL);\n\treg &= ~RGMII_OOB_DIS;\n\tif (enable)\n\t\treg |= RGMII_MODE_EN;\n\telse\n\t\treg &= ~RGMII_MODE_EN;\n\trgmii_wl(intf, reg, RGMII_OOB_CNTRL);\n}\n\nstatic void bcmasp_netif_deinit(struct net_device *dev)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\tu32 reg, timeout = 1000;\n\n\tnapi_disable(&intf->tx_napi);\n\n\tbcmasp_enable_tx(intf, 0);\n\n\t \n\ttx_spb_dma_wl(intf, TX_SPB_DMA_FIFO_FLUSH, TX_SPB_DMA_FIFO_CTRL);\n\tdo {\n\t\treg = tx_spb_dma_rl(intf, TX_SPB_DMA_FIFO_STATUS);\n\t\tif (!(reg & TX_SPB_DMA_FIFO_FLUSH))\n\t\t\tbreak;\n\t\tusleep_range(1000, 2000);\n\t} while (timeout-- > 0);\n\ttx_spb_dma_wl(intf, 0x0, TX_SPB_DMA_FIFO_CTRL);\n\n\tumac_enable_set(intf, UMC_CMD_TX_EN, 0);\n\n\tphy_stop(dev->phydev);\n\n\tumac_enable_set(intf, UMC_CMD_RX_EN, 0);\n\n\tbcmasp_flush_rx_port(intf);\n\tusleep_range(1000, 2000);\n\tbcmasp_enable_rx(intf, 0);\n\n\tnapi_disable(&intf->rx_napi);\n\n\t \n\tbcmasp_enable_tx_irq(intf, 0);\n\tbcmasp_enable_rx_irq(intf, 0);\n\n\tnetif_napi_del(&intf->tx_napi);\n\tbcmasp_reclaim_free_all_tx(intf);\n\n\tnetif_napi_del(&intf->rx_napi);\n\tbcmasp_reclaim_free_all_rx(intf);\n}\n\nstatic int bcmasp_stop(struct net_device *dev)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\n\tnetif_dbg(intf, ifdown, dev, \"bcmasp stop\\n\");\n\n\t \n\tnetif_tx_disable(dev);\n\n\tbcmasp_netif_deinit(dev);\n\n\tphy_disconnect(dev->phydev);\n\n\t \n\tif (intf->internal_phy)\n\t\tbcmasp_ephy_enable_set(intf, false);\n\telse\n\t\tbcmasp_rgmii_mode_en_set(intf, false);\n\n\t \n\tbcmasp_core_clock_set_intf(intf, false);\n\n\tclk_disable_unprepare(intf->parent->clk);\n\n\treturn 0;\n}\n\nstatic void bcmasp_configure_port(struct bcmasp_intf *intf)\n{\n\tu32 reg, id_mode_dis = 0;\n\n\treg = rgmii_rl(intf, RGMII_PORT_CNTRL);\n\treg &= ~RGMII_PORT_MODE_MASK;\n\n\tswitch (intf->phy_interface) {\n\tcase PHY_INTERFACE_MODE_RGMII:\n\t\t \n\t\tid_mode_dis = RGMII_ID_MODE_DIS;\n\t\tfallthrough;\n\tcase PHY_INTERFACE_MODE_RGMII_TXID:\n\t\treg |= RGMII_PORT_MODE_EXT_GPHY;\n\t\tbreak;\n\tcase PHY_INTERFACE_MODE_MII:\n\t\treg |= RGMII_PORT_MODE_EXT_EPHY;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (intf->internal_phy)\n\t\treg |= RGMII_PORT_MODE_EPHY;\n\n\trgmii_wl(intf, reg, RGMII_PORT_CNTRL);\n\n\treg = rgmii_rl(intf, RGMII_OOB_CNTRL);\n\treg &= ~RGMII_ID_MODE_DIS;\n\treg |= id_mode_dis;\n\trgmii_wl(intf, reg, RGMII_OOB_CNTRL);\n}\n\nstatic int bcmasp_netif_init(struct net_device *dev, bool phy_connect)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\tphy_interface_t phy_iface = intf->phy_interface;\n\tu32 phy_flags = PHY_BRCM_AUTO_PWRDWN_ENABLE |\n\t\t\tPHY_BRCM_DIS_TXCRXC_NOENRGY |\n\t\t\tPHY_BRCM_IDDQ_SUSPEND;\n\tstruct phy_device *phydev = NULL;\n\tint ret;\n\n\t \n\tbcmasp_core_clock_set_intf(intf, true);\n\n\t \n\tif (intf->internal_phy)\n\t\tbcmasp_ephy_enable_set(intf, true);\n\telse\n\t\tbcmasp_rgmii_mode_en_set(intf, true);\n\tbcmasp_configure_port(intf);\n\n\t \n\tswitch (phy_iface) {\n\tcase PHY_INTERFACE_MODE_RGMII:\n\t\tphy_iface = PHY_INTERFACE_MODE_RGMII_ID;\n\t\tbreak;\n\tcase PHY_INTERFACE_MODE_RGMII_TXID:\n\t\tphy_iface = PHY_INTERFACE_MODE_RGMII_RXID;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (phy_connect) {\n\t\tphydev = of_phy_connect(dev, intf->phy_dn,\n\t\t\t\t\tbcmasp_adj_link, phy_flags,\n\t\t\t\t\tphy_iface);\n\t\tif (!phydev) {\n\t\t\tret = -ENODEV;\n\t\t\tnetdev_err(dev, \"could not attach to PHY\\n\");\n\t\t\tgoto err_phy_disable;\n\t\t}\n\t} else if (!intf->wolopts) {\n\t\tret = phy_resume(dev->phydev);\n\t\tif (ret)\n\t\t\tgoto err_phy_disable;\n\t}\n\n\tumac_reset(intf);\n\n\tumac_init(intf);\n\n\t \n\tumac_enable_set(intf, (UMC_CMD_RX_EN | UMC_CMD_TX_EN), 0);\n\n\tumac_set_hw_addr(intf, dev->dev_addr);\n\n\tintf->old_duplex = -1;\n\tintf->old_link = -1;\n\tintf->old_pause = -1;\n\n\tret = bcmasp_init_tx(intf);\n\tif (ret)\n\t\tgoto err_phy_disconnect;\n\n\t \n\tbcmasp_enable_tx(intf, 1);\n\n\tret = bcmasp_init_rx(intf);\n\tif (ret)\n\t\tgoto err_reclaim_tx;\n\n\tbcmasp_enable_rx(intf, 1);\n\n\t \n\tumac_enable_set(intf, (UMC_CMD_RX_EN | UMC_CMD_TX_EN), 1);\n\n\tintf->crc_fwd = !!(umac_rl(intf, UMC_CMD) & UMC_CMD_CRC_FWD);\n\n\tbcmasp_netif_start(dev);\n\n\tnetif_start_queue(dev);\n\n\treturn 0;\n\nerr_reclaim_tx:\n\tbcmasp_reclaim_free_all_tx(intf);\nerr_phy_disconnect:\n\tif (phydev)\n\t\tphy_disconnect(phydev);\nerr_phy_disable:\n\tif (intf->internal_phy)\n\t\tbcmasp_ephy_enable_set(intf, false);\n\telse\n\t\tbcmasp_rgmii_mode_en_set(intf, false);\n\treturn ret;\n}\n\nstatic int bcmasp_open(struct net_device *dev)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\tint ret;\n\n\tnetif_dbg(intf, ifup, dev, \"bcmasp open\\n\");\n\n\tret = clk_prepare_enable(intf->parent->clk);\n\tif (ret)\n\t\treturn ret;\n\n\tret = bcmasp_netif_init(dev, true);\n\tif (ret)\n\t\tclk_disable_unprepare(intf->parent->clk);\n\n\treturn ret;\n}\n\nstatic void bcmasp_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\n\tnetif_dbg(intf, tx_err, dev, \"transmit timeout!\\n\");\n\tintf->mib.tx_timeout_cnt++;\n}\n\nstatic int bcmasp_get_phys_port_name(struct net_device *dev,\n\t\t\t\t     char *name, size_t len)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\n\tif (snprintf(name, len, \"p%d\", intf->port) >= len)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void bcmasp_get_stats64(struct net_device *dev,\n\t\t\t       struct rtnl_link_stats64 *stats)\n{\n\tstruct bcmasp_intf *intf = netdev_priv(dev);\n\tstruct bcmasp_intf_stats64 *lstats;\n\tunsigned int start;\n\n\tlstats = &intf->stats64;\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(&lstats->syncp);\n\t\tstats->rx_packets = u64_stats_read(&lstats->rx_packets);\n\t\tstats->rx_bytes = u64_stats_read(&lstats->rx_bytes);\n\t\tstats->rx_dropped = u64_stats_read(&lstats->rx_dropped);\n\t\tstats->rx_crc_errors = u64_stats_read(&lstats->rx_crc_errs);\n\t\tstats->rx_frame_errors = u64_stats_read(&lstats->rx_sym_errs);\n\t\tstats->rx_errors = stats->rx_crc_errors + stats->rx_frame_errors;\n\n\t\tstats->tx_packets = u64_stats_read(&lstats->tx_packets);\n\t\tstats->tx_bytes = u64_stats_read(&lstats->tx_bytes);\n\t} while (u64_stats_fetch_retry(&lstats->syncp, start));\n}\n\nstatic const struct net_device_ops bcmasp_netdev_ops = {\n\t.ndo_open\t\t= bcmasp_open,\n\t.ndo_stop\t\t= bcmasp_stop,\n\t.ndo_start_xmit\t\t= bcmasp_xmit,\n\t.ndo_tx_timeout\t\t= bcmasp_tx_timeout,\n\t.ndo_set_rx_mode\t= bcmasp_set_rx_mode,\n\t.ndo_get_phys_port_name\t= bcmasp_get_phys_port_name,\n\t.ndo_eth_ioctl\t\t= phy_do_ioctl_running,\n\t.ndo_set_mac_address\t= eth_mac_addr,\n\t.ndo_get_stats64\t= bcmasp_get_stats64,\n};\n\nstatic void bcmasp_map_res(struct bcmasp_priv *priv, struct bcmasp_intf *intf)\n{\n\t \n\tintf->res.umac = priv->base + UMC_OFFSET(intf);\n\tintf->res.umac2fb = priv->base + (priv->hw_info->umac2fb +\n\t\t\t\t\t  (intf->port * 0x4));\n\tintf->res.rgmii = priv->base + RGMII_OFFSET(intf);\n\n\t \n\tintf->tx_spb_dma = priv->base + TX_SPB_DMA_OFFSET(intf);\n\tintf->res.tx_spb_ctrl = priv->base + TX_SPB_CTRL_OFFSET(intf);\n\tintf->res.tx_spb_top = priv->base + TX_SPB_TOP_OFFSET(intf);\n\tintf->res.tx_epkt_core = priv->base + TX_EPKT_C_OFFSET(intf);\n\tintf->res.tx_pause_ctrl = priv->base + TX_PAUSE_CTRL_OFFSET(intf);\n\n\tintf->rx_edpkt_dma = priv->base + RX_EDPKT_DMA_OFFSET(intf);\n\tintf->rx_edpkt_cfg = priv->base + RX_EDPKT_CFG_OFFSET(intf);\n}\n\n#define MAX_IRQ_STR_LEN\t\t64\nstruct bcmasp_intf *bcmasp_interface_create(struct bcmasp_priv *priv,\n\t\t\t\t\t    struct device_node *ndev_dn, int i)\n{\n\tstruct device *dev = &priv->pdev->dev;\n\tstruct bcmasp_intf *intf;\n\tstruct net_device *ndev;\n\tint ch, port, ret;\n\n\tif (of_property_read_u32(ndev_dn, \"reg\", &port)) {\n\t\tdev_warn(dev, \"%s: invalid port number\\n\", ndev_dn->name);\n\t\tgoto err;\n\t}\n\n\tif (of_property_read_u32(ndev_dn, \"brcm,channel\", &ch)) {\n\t\tdev_warn(dev, \"%s: invalid ch number\\n\", ndev_dn->name);\n\t\tgoto err;\n\t}\n\n\tndev = alloc_etherdev(sizeof(struct bcmasp_intf));\n\tif (!ndev) {\n\t\tdev_warn(dev, \"%s: unable to alloc ndev\\n\", ndev_dn->name);\n\t\tgoto err;\n\t}\n\tintf = netdev_priv(ndev);\n\n\tintf->parent = priv;\n\tintf->ndev = ndev;\n\tintf->channel = ch;\n\tintf->port = port;\n\tintf->ndev_dn = ndev_dn;\n\tintf->index = i;\n\n\tret = of_get_phy_mode(ndev_dn, &intf->phy_interface);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"invalid PHY mode property\\n\");\n\t\tgoto err_free_netdev;\n\t}\n\n\tif (intf->phy_interface == PHY_INTERFACE_MODE_INTERNAL)\n\t\tintf->internal_phy = true;\n\n\tintf->phy_dn = of_parse_phandle(ndev_dn, \"phy-handle\", 0);\n\tif (!intf->phy_dn && of_phy_is_fixed_link(ndev_dn)) {\n\t\tret = of_phy_register_fixed_link(ndev_dn);\n\t\tif (ret) {\n\t\t\tdev_warn(dev, \"%s: failed to register fixed PHY\\n\",\n\t\t\t\t ndev_dn->name);\n\t\t\tgoto err_free_netdev;\n\t\t}\n\t\tintf->phy_dn = ndev_dn;\n\t}\n\n\t \n\tbcmasp_map_res(priv, intf);\n\n\tif ((!phy_interface_mode_is_rgmii(intf->phy_interface) &&\n\t     intf->phy_interface != PHY_INTERFACE_MODE_MII &&\n\t     intf->phy_interface != PHY_INTERFACE_MODE_INTERNAL) ||\n\t    (intf->port != 1 && intf->internal_phy)) {\n\t\tnetdev_err(intf->ndev, \"invalid PHY mode: %s for port %d\\n\",\n\t\t\t   phy_modes(intf->phy_interface), intf->port);\n\t\tret = -EINVAL;\n\t\tgoto err_free_netdev;\n\t}\n\n\tret = of_get_ethdev_address(ndev_dn, ndev);\n\tif (ret) {\n\t\tnetdev_warn(ndev, \"using random Ethernet MAC\\n\");\n\t\teth_hw_addr_random(ndev);\n\t}\n\n\tSET_NETDEV_DEV(ndev, dev);\n\tintf->ops = &bcmasp_intf_ops;\n\tndev->netdev_ops = &bcmasp_netdev_ops;\n\tndev->ethtool_ops = &bcmasp_ethtool_ops;\n\tintf->msg_enable = netif_msg_init(-1, NETIF_MSG_DRV |\n\t\t\t\t\t  NETIF_MSG_PROBE |\n\t\t\t\t\t  NETIF_MSG_LINK);\n\tndev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM | NETIF_F_SG |\n\t\t\t  NETIF_F_RXCSUM;\n\tndev->hw_features |= ndev->features;\n\tndev->needed_headroom += sizeof(struct bcmasp_pkt_offload);\n\n\treturn intf;\n\nerr_free_netdev:\n\tfree_netdev(ndev);\nerr:\n\treturn NULL;\n}\n\nvoid bcmasp_interface_destroy(struct bcmasp_intf *intf)\n{\n\tif (intf->ndev->reg_state == NETREG_REGISTERED)\n\t\tunregister_netdev(intf->ndev);\n\tif (of_phy_is_fixed_link(intf->ndev_dn))\n\t\tof_phy_deregister_fixed_link(intf->ndev_dn);\n\tfree_netdev(intf->ndev);\n}\n\nstatic void bcmasp_suspend_to_wol(struct bcmasp_intf *intf)\n{\n\tstruct net_device *ndev = intf->ndev;\n\tu32 reg;\n\n\treg = umac_rl(intf, UMC_MPD_CTRL);\n\tif (intf->wolopts & (WAKE_MAGIC | WAKE_MAGICSECURE))\n\t\treg |= UMC_MPD_CTRL_MPD_EN;\n\treg &= ~UMC_MPD_CTRL_PSW_EN;\n\tif (intf->wolopts & WAKE_MAGICSECURE) {\n\t\t \n\t\tumac_wl(intf, get_unaligned_be16(&intf->sopass[0]),\n\t\t\tUMC_PSW_MS);\n\t\tumac_wl(intf, get_unaligned_be32(&intf->sopass[2]),\n\t\t\tUMC_PSW_LS);\n\t\treg |= UMC_MPD_CTRL_PSW_EN;\n\t}\n\tumac_wl(intf, reg, UMC_MPD_CTRL);\n\n\tif (intf->wolopts & WAKE_FILTER)\n\t\tbcmasp_netfilt_suspend(intf);\n\n\t \n\tumac_enable_set(intf, UMC_CMD_RX_EN, 1);\n\n\tif (intf->parent->wol_irq > 0) {\n\t\twakeup_intr2_core_wl(intf->parent, 0xffffffff,\n\t\t\t\t     ASP_WAKEUP_INTR2_MASK_CLEAR);\n\t}\n\n\tnetif_dbg(intf, wol, ndev, \"entered WOL mode\\n\");\n}\n\nint bcmasp_interface_suspend(struct bcmasp_intf *intf)\n{\n\tstruct device *kdev = &intf->parent->pdev->dev;\n\tstruct net_device *dev = intf->ndev;\n\tint ret = 0;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tnetif_device_detach(dev);\n\n\tbcmasp_netif_deinit(dev);\n\n\tif (!intf->wolopts) {\n\t\tret = phy_suspend(dev->phydev);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (intf->internal_phy)\n\t\t\tbcmasp_ephy_enable_set(intf, false);\n\t\telse\n\t\t\tbcmasp_rgmii_mode_en_set(intf, false);\n\n\t\t \n\t\tbcmasp_core_clock_set_intf(intf, false);\n\t}\n\n\tif (device_may_wakeup(kdev) && intf->wolopts)\n\t\tbcmasp_suspend_to_wol(intf);\n\n\tclk_disable_unprepare(intf->parent->clk);\n\n\treturn ret;\n\nout:\n\tbcmasp_netif_init(dev, false);\n\treturn ret;\n}\n\nstatic void bcmasp_resume_from_wol(struct bcmasp_intf *intf)\n{\n\tu32 reg;\n\n\treg = umac_rl(intf, UMC_MPD_CTRL);\n\treg &= ~UMC_MPD_CTRL_MPD_EN;\n\tumac_wl(intf, reg, UMC_MPD_CTRL);\n\n\tif (intf->parent->wol_irq > 0) {\n\t\twakeup_intr2_core_wl(intf->parent, 0xffffffff,\n\t\t\t\t     ASP_WAKEUP_INTR2_MASK_SET);\n\t}\n}\n\nint bcmasp_interface_resume(struct bcmasp_intf *intf)\n{\n\tstruct net_device *dev = intf->ndev;\n\tint ret;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tret = clk_prepare_enable(intf->parent->clk);\n\tif (ret)\n\t\treturn ret;\n\n\tret = bcmasp_netif_init(dev, false);\n\tif (ret)\n\t\tgoto out;\n\n\tbcmasp_resume_from_wol(intf);\n\n\tif (intf->eee.eee_enabled)\n\t\tbcmasp_eee_enable_set(intf, true);\n\n\tnetif_device_attach(dev);\n\n\treturn 0;\n\nout:\n\tclk_disable_unprepare(intf->parent->clk);\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}