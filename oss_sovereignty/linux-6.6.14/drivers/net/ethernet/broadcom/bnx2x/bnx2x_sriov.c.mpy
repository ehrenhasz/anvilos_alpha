{
  "module_name": "bnx2x_sriov.c",
  "hash_id": "73d45972ea3dfbbe3292a36b184e403f7cdb49ff776a99a3d3704c790a896b10",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/broadcom/bnx2x/bnx2x_sriov.c",
  "human_readable_source": " \n#include \"bnx2x.h\"\n#include \"bnx2x_init.h\"\n#include \"bnx2x_cmn.h\"\n#include \"bnx2x_sp.h\"\n#include <linux/crc32.h>\n#include <linux/if_vlan.h>\n\nstatic int bnx2x_vf_op_prep(struct bnx2x *bp, int vfidx,\n\t\t\t    struct bnx2x_virtf **vf,\n\t\t\t    struct pf_vf_bulletin_content **bulletin,\n\t\t\t    bool test_queue);\n\n \nstatic void storm_memset_vf_to_pf(struct bnx2x *bp, u16 abs_fid,\n\t\t\t\t\t u16 pf_id)\n{\n\tREG_WR8(bp, BAR_XSTRORM_INTMEM + XSTORM_VF_TO_PF_OFFSET(abs_fid),\n\t\tpf_id);\n\tREG_WR8(bp, BAR_CSTRORM_INTMEM + CSTORM_VF_TO_PF_OFFSET(abs_fid),\n\t\tpf_id);\n\tREG_WR8(bp, BAR_TSTRORM_INTMEM + TSTORM_VF_TO_PF_OFFSET(abs_fid),\n\t\tpf_id);\n\tREG_WR8(bp, BAR_USTRORM_INTMEM + USTORM_VF_TO_PF_OFFSET(abs_fid),\n\t\tpf_id);\n}\n\nstatic void storm_memset_func_en(struct bnx2x *bp, u16 abs_fid,\n\t\t\t\t\tu8 enable)\n{\n\tREG_WR8(bp, BAR_XSTRORM_INTMEM + XSTORM_FUNC_EN_OFFSET(abs_fid),\n\t\tenable);\n\tREG_WR8(bp, BAR_CSTRORM_INTMEM + CSTORM_FUNC_EN_OFFSET(abs_fid),\n\t\tenable);\n\tREG_WR8(bp, BAR_TSTRORM_INTMEM + TSTORM_FUNC_EN_OFFSET(abs_fid),\n\t\tenable);\n\tREG_WR8(bp, BAR_USTRORM_INTMEM + USTORM_FUNC_EN_OFFSET(abs_fid),\n\t\tenable);\n}\n\nint bnx2x_vf_idx_by_abs_fid(struct bnx2x *bp, u16 abs_vfid)\n{\n\tint idx;\n\n\tfor_each_vf(bp, idx)\n\t\tif (bnx2x_vf(bp, idx, abs_vfid) == abs_vfid)\n\t\t\tbreak;\n\treturn idx;\n}\n\nstatic\nstruct bnx2x_virtf *bnx2x_vf_by_abs_fid(struct bnx2x *bp, u16 abs_vfid)\n{\n\tu16 idx =  (u16)bnx2x_vf_idx_by_abs_fid(bp, abs_vfid);\n\treturn (idx < BNX2X_NR_VIRTFN(bp)) ? BP_VF(bp, idx) : NULL;\n}\n\nstatic void bnx2x_vf_igu_ack_sb(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t\tu8 igu_sb_id, u8 segment, u16 index, u8 op,\n\t\t\t\tu8 update)\n{\n\t \n\tu32 ctl;\n\tu32 igu_addr_data = IGU_REG_COMMAND_REG_32LSB_DATA;\n\tu32 igu_addr_ctl = IGU_REG_COMMAND_REG_CTRL;\n\tu32 func_encode = vf->abs_vfid;\n\tu32 addr_encode = IGU_CMD_E2_PROD_UPD_BASE + igu_sb_id;\n\tstruct igu_regular cmd_data = {0};\n\n\tcmd_data.sb_id_and_flags =\n\t\t\t((index << IGU_REGULAR_SB_INDEX_SHIFT) |\n\t\t\t (segment << IGU_REGULAR_SEGMENT_ACCESS_SHIFT) |\n\t\t\t (update << IGU_REGULAR_BUPDATE_SHIFT) |\n\t\t\t (op << IGU_REGULAR_ENABLE_INT_SHIFT));\n\n\tctl = addr_encode << IGU_CTRL_REG_ADDRESS_SHIFT\t\t|\n\t      func_encode << IGU_CTRL_REG_FID_SHIFT\t\t|\n\t      IGU_CTRL_CMD_TYPE_WR << IGU_CTRL_REG_TYPE_SHIFT;\n\n\tDP(NETIF_MSG_HW, \"write 0x%08x to IGU(via GRC) addr 0x%x\\n\",\n\t   cmd_data.sb_id_and_flags, igu_addr_data);\n\tREG_WR(bp, igu_addr_data, cmd_data.sb_id_and_flags);\n\tbarrier();\n\n\tDP(NETIF_MSG_HW, \"write 0x%08x to IGU(via GRC) addr 0x%x\\n\",\n\t   ctl, igu_addr_ctl);\n\tREG_WR(bp, igu_addr_ctl, ctl);\n\tbarrier();\n}\n\nstatic bool bnx2x_validate_vf_sp_objs(struct bnx2x *bp,\n\t\t\t\t       struct bnx2x_virtf *vf,\n\t\t\t\t       bool print_err)\n{\n\tif (!bnx2x_leading_vfq(vf, sp_initialized)) {\n\t\tif (print_err)\n\t\t\tBNX2X_ERR(\"Slowpath objects not yet initialized!\\n\");\n\t\telse\n\t\t\tDP(BNX2X_MSG_IOV, \"Slowpath objects not yet initialized!\\n\");\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nvoid bnx2x_vfop_qctor_dump_tx(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t      struct bnx2x_queue_init_params *init_params,\n\t\t\t      struct bnx2x_queue_setup_params *setup_params,\n\t\t\t      u16 q_idx, u16 sb_idx)\n{\n\tDP(BNX2X_MSG_IOV,\n\t   \"VF[%d] Q_SETUP: txq[%d]-- vfsb=%d, sb-index=%d, hc-rate=%d, flags=0x%lx, traffic-type=%d\",\n\t   vf->abs_vfid,\n\t   q_idx,\n\t   sb_idx,\n\t   init_params->tx.sb_cq_index,\n\t   init_params->tx.hc_rate,\n\t   setup_params->flags,\n\t   setup_params->txq_params.traffic_type);\n}\n\nvoid bnx2x_vfop_qctor_dump_rx(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t    struct bnx2x_queue_init_params *init_params,\n\t\t\t    struct bnx2x_queue_setup_params *setup_params,\n\t\t\t    u16 q_idx, u16 sb_idx)\n{\n\tstruct bnx2x_rxq_setup_params *rxq_params = &setup_params->rxq_params;\n\n\tDP(BNX2X_MSG_IOV, \"VF[%d] Q_SETUP: rxq[%d]-- vfsb=%d, sb-index=%d, hc-rate=%d, mtu=%d, buf-size=%d\\n\"\n\t   \"sge-size=%d, max_sge_pkt=%d, tpa-agg-size=%d, flags=0x%lx, drop-flags=0x%x, cache-log=%d\\n\",\n\t   vf->abs_vfid,\n\t   q_idx,\n\t   sb_idx,\n\t   init_params->rx.sb_cq_index,\n\t   init_params->rx.hc_rate,\n\t   setup_params->gen_params.mtu,\n\t   rxq_params->buf_sz,\n\t   rxq_params->sge_buf_sz,\n\t   rxq_params->max_sges_pkt,\n\t   rxq_params->tpa_agg_sz,\n\t   setup_params->flags,\n\t   rxq_params->drop_flags,\n\t   rxq_params->cache_line_log);\n}\n\nvoid bnx2x_vfop_qctor_prep(struct bnx2x *bp,\n\t\t\t   struct bnx2x_virtf *vf,\n\t\t\t   struct bnx2x_vf_queue *q,\n\t\t\t   struct bnx2x_vf_queue_construct_params *p,\n\t\t\t   unsigned long q_type)\n{\n\tstruct bnx2x_queue_init_params *init_p = &p->qstate.params.init;\n\tstruct bnx2x_queue_setup_params *setup_p = &p->prep_qsetup;\n\n\t \n\n\t \n\tif (test_bit(BNX2X_Q_FLG_HC, &init_p->rx.flags))\n\t\t__set_bit(BNX2X_Q_FLG_HC_EN, &init_p->rx.flags);\n\n\tif (test_bit(BNX2X_Q_FLG_HC, &init_p->tx.flags))\n\t\t__set_bit(BNX2X_Q_FLG_HC_EN, &init_p->tx.flags);\n\n\t \n\tinit_p->rx.fw_sb_id = vf_igu_sb(vf, q->sb_idx);\n\tinit_p->tx.fw_sb_id = vf_igu_sb(vf, q->sb_idx);\n\n\t \n\tinit_p->cxts[0] = q->cxt;\n\n\t \n\n\t \n\tsetup_p->gen_params.spcl_id = vf->sp_cl_id;\n\tsetup_p->gen_params.stat_id = vfq_stat_id(vf, q);\n\tsetup_p->gen_params.fp_hsi = vf->fp_hsi;\n\n\t \n\tif (test_bit(BNX2X_Q_FLG_STATS, &setup_p->flags))\n\t\t__set_bit(BNX2X_Q_FLG_ZERO_STATS, &setup_p->flags);\n\n\t \n\t__set_bit(BNX2X_Q_FLG_TX_SWITCH, &setup_p->flags);\n\t__set_bit(BNX2X_Q_FLG_TX_SEC, &setup_p->flags);\n\tif (vf->spoofchk)\n\t\t__set_bit(BNX2X_Q_FLG_ANTI_SPOOF, &setup_p->flags);\n\telse\n\t\t__clear_bit(BNX2X_Q_FLG_ANTI_SPOOF, &setup_p->flags);\n\n\t \n\tif (test_bit(BNX2X_Q_TYPE_HAS_RX, &q_type)) {\n\t\tstruct bnx2x_rxq_setup_params *rxq_p = &setup_p->rxq_params;\n\n\t\trxq_p->cl_qzone_id = vfq_qzone_id(vf, q);\n\t\trxq_p->fw_sb_id = vf_igu_sb(vf, q->sb_idx);\n\t\trxq_p->rss_engine_id = FW_VF_HANDLE(vf->abs_vfid);\n\n\t\tif (test_bit(BNX2X_Q_FLG_TPA, &setup_p->flags))\n\t\t\trxq_p->max_tpa_queues = BNX2X_VF_MAX_TPA_AGG_QUEUES;\n\t}\n\n\t \n\tif (test_bit(BNX2X_Q_TYPE_HAS_TX, &q_type)) {\n\t\tsetup_p->txq_params.tss_leading_cl_id = vf->leading_rss;\n\t\tsetup_p->txq_params.fw_sb_id = vf_igu_sb(vf, q->sb_idx);\n\t}\n}\n\nstatic int bnx2x_vf_queue_create(struct bnx2x *bp,\n\t\t\t\t struct bnx2x_virtf *vf, int qid,\n\t\t\t\t struct bnx2x_vf_queue_construct_params *qctor)\n{\n\tstruct bnx2x_queue_state_params *q_params;\n\tint rc = 0;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d:%d]\\n\", vf->abs_vfid, qid);\n\n\t \n\tq_params = &qctor->qstate;\n\tq_params->q_obj = &bnx2x_vfq(vf, qid, sp_obj);\n\tset_bit(RAMROD_COMP_WAIT, &q_params->ramrod_flags);\n\n\tif (bnx2x_get_q_logical_state(bp, q_params->q_obj) ==\n\t    BNX2X_Q_LOGICAL_STATE_ACTIVE) {\n\t\tDP(BNX2X_MSG_IOV, \"queue was already up. Aborting gracefully\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tq_params->cmd = BNX2X_Q_CMD_INIT;\n\trc = bnx2x_queue_state_change(bp, q_params);\n\tif (rc)\n\t\tgoto out;\n\n\tmemcpy(&q_params->params.setup, &qctor->prep_qsetup,\n\t       sizeof(struct bnx2x_queue_setup_params));\n\tq_params->cmd = BNX2X_Q_CMD_SETUP;\n\trc = bnx2x_queue_state_change(bp, q_params);\n\tif (rc)\n\t\tgoto out;\n\n\t \n\tbnx2x_vf_igu_ack_sb(bp, vf, vf_igu_sb(vf, bnx2x_vfq(vf, qid, sb_idx)),\n\t\t\t    USTORM_ID, 0, IGU_INT_ENABLE, 0);\nout:\n\treturn rc;\n}\n\nstatic int bnx2x_vf_queue_destroy(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t\t  int qid)\n{\n\tenum bnx2x_queue_cmd cmds[] = {BNX2X_Q_CMD_HALT,\n\t\t\t\t       BNX2X_Q_CMD_TERMINATE,\n\t\t\t\t       BNX2X_Q_CMD_CFC_DEL};\n\tstruct bnx2x_queue_state_params q_params;\n\tint rc, i;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d]\\n\", vf->abs_vfid);\n\n\t \n\tmemset(&q_params, 0, sizeof(struct bnx2x_queue_state_params));\n\tq_params.q_obj = &bnx2x_vfq(vf, qid, sp_obj);\n\tset_bit(RAMROD_COMP_WAIT, &q_params.ramrod_flags);\n\n\tif (bnx2x_get_q_logical_state(bp, q_params.q_obj) ==\n\t    BNX2X_Q_LOGICAL_STATE_STOPPED) {\n\t\tDP(BNX2X_MSG_IOV, \"queue was already stopped. Aborting gracefully\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(cmds); i++) {\n\t\tq_params.cmd = cmds[i];\n\t\trc = bnx2x_queue_state_change(bp, &q_params);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"Failed to run Queue command %d\\n\", cmds[i]);\n\t\t\treturn rc;\n\t\t}\n\t}\nout:\n\t \n\tif (bnx2x_vfq(vf, qid, cxt)) {\n\t\tbnx2x_vfq(vf, qid, cxt)->ustorm_ag_context.cdu_usage = 0;\n\t\tbnx2x_vfq(vf, qid, cxt)->xstorm_ag_context.cdu_reserved = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic void\nbnx2x_vf_set_igu_info(struct bnx2x *bp, u8 igu_sb_id, u8 abs_vfid)\n{\n\tstruct bnx2x_virtf *vf = bnx2x_vf_by_abs_fid(bp, abs_vfid);\n\tif (vf) {\n\t\t \n\t\tif (!BP_VFDB(bp)->first_vf_igu_entry)\n\t\t\tBP_VFDB(bp)->first_vf_igu_entry = igu_sb_id;\n\n\t\t \n\t\tif (!vf_sb_count(vf))\n\t\t\tvf->igu_base_id = igu_sb_id;\n\n\t\t++vf_sb_count(vf);\n\t\t++vf->sb_count;\n\t}\n\tBP_VFDB(bp)->vf_sbs_pool++;\n}\n\nstatic int bnx2x_vf_vlan_mac_clear(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t\t   int qid, bool drv_only, int type)\n{\n\tstruct bnx2x_vlan_mac_ramrod_params ramrod;\n\tint rc;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d] - deleting all %s\\n\", vf->abs_vfid,\n\t\t\t  (type == BNX2X_VF_FILTER_VLAN_MAC) ? \"VLAN-MACs\" :\n\t\t\t  (type == BNX2X_VF_FILTER_MAC) ? \"MACs\" : \"VLANs\");\n\n\t \n\tmemset(&ramrod, 0, sizeof(struct bnx2x_vlan_mac_ramrod_params));\n\tif (type == BNX2X_VF_FILTER_VLAN_MAC) {\n\t\tset_bit(BNX2X_ETH_MAC, &ramrod.user_req.vlan_mac_flags);\n\t\tramrod.vlan_mac_obj = &bnx2x_vfq(vf, qid, vlan_mac_obj);\n\t} else if (type == BNX2X_VF_FILTER_MAC) {\n\t\tset_bit(BNX2X_ETH_MAC, &ramrod.user_req.vlan_mac_flags);\n\t\tramrod.vlan_mac_obj = &bnx2x_vfq(vf, qid, mac_obj);\n\t} else {\n\t\tramrod.vlan_mac_obj = &bnx2x_vfq(vf, qid, vlan_obj);\n\t}\n\tramrod.user_req.cmd = BNX2X_VLAN_MAC_DEL;\n\n\tset_bit(RAMROD_EXEC, &ramrod.ramrod_flags);\n\tif (drv_only)\n\t\tset_bit(RAMROD_DRV_CLR_ONLY, &ramrod.ramrod_flags);\n\telse\n\t\tset_bit(RAMROD_COMP_WAIT, &ramrod.ramrod_flags);\n\n\t \n\trc = ramrod.vlan_mac_obj->delete_all(bp,\n\t\t\t\t\t     ramrod.vlan_mac_obj,\n\t\t\t\t\t     &ramrod.user_req.vlan_mac_flags,\n\t\t\t\t\t     &ramrod.ramrod_flags);\n\tif (rc) {\n\t\tBNX2X_ERR(\"Failed to delete all %s\\n\",\n\t\t\t  (type == BNX2X_VF_FILTER_VLAN_MAC) ? \"VLAN-MACs\" :\n\t\t\t  (type == BNX2X_VF_FILTER_MAC) ? \"MACs\" : \"VLANs\");\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic int bnx2x_vf_mac_vlan_config(struct bnx2x *bp,\n\t\t\t\t    struct bnx2x_virtf *vf, int qid,\n\t\t\t\t    struct bnx2x_vf_mac_vlan_filter *filter,\n\t\t\t\t    bool drv_only)\n{\n\tstruct bnx2x_vlan_mac_ramrod_params ramrod;\n\tint rc;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d] - %s a %s filter\\n\",\n\t   vf->abs_vfid, filter->add ? \"Adding\" : \"Deleting\",\n\t   (filter->type == BNX2X_VF_FILTER_VLAN_MAC) ? \"VLAN-MAC\" :\n\t   (filter->type == BNX2X_VF_FILTER_MAC) ? \"MAC\" : \"VLAN\");\n\n\t \n\tmemset(&ramrod, 0, sizeof(struct bnx2x_vlan_mac_ramrod_params));\n\tif (filter->type == BNX2X_VF_FILTER_VLAN_MAC) {\n\t\tramrod.vlan_mac_obj = &bnx2x_vfq(vf, qid, vlan_mac_obj);\n\t\tramrod.user_req.u.vlan.vlan = filter->vid;\n\t\tmemcpy(&ramrod.user_req.u.mac.mac, filter->mac, ETH_ALEN);\n\t\tset_bit(BNX2X_ETH_MAC, &ramrod.user_req.vlan_mac_flags);\n\t} else if (filter->type == BNX2X_VF_FILTER_VLAN) {\n\t\tramrod.vlan_mac_obj = &bnx2x_vfq(vf, qid, vlan_obj);\n\t\tramrod.user_req.u.vlan.vlan = filter->vid;\n\t} else {\n\t\tset_bit(BNX2X_ETH_MAC, &ramrod.user_req.vlan_mac_flags);\n\t\tramrod.vlan_mac_obj = &bnx2x_vfq(vf, qid, mac_obj);\n\t\tmemcpy(&ramrod.user_req.u.mac.mac, filter->mac, ETH_ALEN);\n\t}\n\tramrod.user_req.cmd = filter->add ? BNX2X_VLAN_MAC_ADD :\n\t\t\t\t\t    BNX2X_VLAN_MAC_DEL;\n\n\tset_bit(RAMROD_EXEC, &ramrod.ramrod_flags);\n\tif (drv_only)\n\t\tset_bit(RAMROD_DRV_CLR_ONLY, &ramrod.ramrod_flags);\n\telse\n\t\tset_bit(RAMROD_COMP_WAIT, &ramrod.ramrod_flags);\n\n\t \n\trc = bnx2x_config_vlan_mac(bp, &ramrod);\n\tif (rc == -EEXIST)\n\t\treturn 0;\n\tif (rc) {\n\t\tBNX2X_ERR(\"Failed to %s %s\\n\",\n\t\t\t  filter->add ? \"add\" : \"delete\",\n\t\t\t  (filter->type == BNX2X_VF_FILTER_VLAN_MAC) ?\n\t\t\t\t\"VLAN-MAC\" :\n\t\t\t  (filter->type == BNX2X_VF_FILTER_MAC) ?\n\t\t\t\t\"MAC\" : \"VLAN\");\n\t\treturn rc;\n\t}\n\n\tfilter->applied = true;\n\n\treturn 0;\n}\n\nint bnx2x_vf_mac_vlan_config_list(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t\t  struct bnx2x_vf_mac_vlan_filters *filters,\n\t\t\t\t  int qid, bool drv_only)\n{\n\tint rc = 0, i;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d]\\n\", vf->abs_vfid);\n\n\tif (!bnx2x_validate_vf_sp_objs(bp, vf, true))\n\t\treturn -EINVAL;\n\n\t \n\tfor (i = 0; i < filters->count; i++) {\n\t\trc = bnx2x_vf_mac_vlan_config(bp, vf, qid,\n\t\t\t\t\t      &filters->filters[i], drv_only);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (i != filters->count) {\n\t\tBNX2X_ERR(\"Managed only %d/%d filters - rolling back\\n\",\n\t\t\t  i, filters->count);\n\t\twhile (--i >= 0) {\n\t\t\tif (!filters->filters[i].applied)\n\t\t\t\tcontinue;\n\t\t\tfilters->filters[i].add = !filters->filters[i].add;\n\t\t\tbnx2x_vf_mac_vlan_config(bp, vf, qid,\n\t\t\t\t\t\t &filters->filters[i],\n\t\t\t\t\t\t drv_only);\n\t\t}\n\t}\n\n\t \n\tkfree(filters);\n\n\treturn rc;\n}\n\nint bnx2x_vf_queue_setup(struct bnx2x *bp, struct bnx2x_virtf *vf, int qid,\n\t\t\t struct bnx2x_vf_queue_construct_params *qctor)\n{\n\tint rc;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d:%d]\\n\", vf->abs_vfid, qid);\n\n\trc = bnx2x_vf_queue_create(bp, vf, qid, qctor);\n\tif (rc)\n\t\tgoto op_err;\n\n\t \n\tbnx2x_schedule_sp_rtnl(bp, BNX2X_SP_RTNL_HYPERVISOR_VLAN,\n\t\t\t       BNX2X_MSG_IOV);\n\treturn 0;\nop_err:\n\tBNX2X_ERR(\"QSETUP[%d:%d] error: rc %d\\n\", vf->abs_vfid, qid, rc);\n\treturn rc;\n}\n\nstatic int bnx2x_vf_queue_flr(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t       int qid)\n{\n\tint rc;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d:%d]\\n\", vf->abs_vfid, qid);\n\n\t \n\tif ((qid == LEADING_IDX) &&\n\t    bnx2x_validate_vf_sp_objs(bp, vf, false)) {\n\t\trc = bnx2x_vf_vlan_mac_clear(bp, vf, qid, true,\n\t\t\t\t\t     BNX2X_VF_FILTER_VLAN_MAC);\n\t\tif (rc)\n\t\t\tgoto op_err;\n\t\trc = bnx2x_vf_vlan_mac_clear(bp, vf, qid, true,\n\t\t\t\t\t     BNX2X_VF_FILTER_VLAN);\n\t\tif (rc)\n\t\t\tgoto op_err;\n\t\trc = bnx2x_vf_vlan_mac_clear(bp, vf, qid, true,\n\t\t\t\t\t     BNX2X_VF_FILTER_MAC);\n\t\tif (rc)\n\t\t\tgoto op_err;\n\t}\n\n\t \n\tif (bnx2x_vfq(vf, qid, sp_obj).state != BNX2X_Q_STATE_RESET) {\n\t\tstruct bnx2x_queue_state_params qstate;\n\n\t\tmemset(&qstate, 0, sizeof(struct bnx2x_queue_state_params));\n\t\tqstate.q_obj = &bnx2x_vfq(vf, qid, sp_obj);\n\t\tqstate.q_obj->state = BNX2X_Q_STATE_STOPPED;\n\t\tqstate.cmd = BNX2X_Q_CMD_TERMINATE;\n\t\tset_bit(RAMROD_COMP_WAIT, &qstate.ramrod_flags);\n\t\trc = bnx2x_queue_state_change(bp, &qstate);\n\t\tif (rc)\n\t\t\tgoto op_err;\n\t}\n\n\treturn 0;\nop_err:\n\tBNX2X_ERR(\"vf[%d:%d] error: rc %d\\n\", vf->abs_vfid, qid, rc);\n\treturn rc;\n}\n\nint bnx2x_vf_mcast(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t   bnx2x_mac_addr_t *mcasts, int mc_num, bool drv_only)\n{\n\tstruct bnx2x_mcast_list_elem *mc = NULL;\n\tstruct bnx2x_mcast_ramrod_params mcast;\n\tint rc, i;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d]\\n\", vf->abs_vfid);\n\n\t \n\tmemset(&mcast, 0, sizeof(struct bnx2x_mcast_ramrod_params));\n\tmcast.mcast_obj = &vf->mcast_obj;\n\tif (drv_only)\n\t\tset_bit(RAMROD_DRV_CLR_ONLY, &mcast.ramrod_flags);\n\telse\n\t\tset_bit(RAMROD_COMP_WAIT, &mcast.ramrod_flags);\n\tif (mc_num) {\n\t\tmc = kcalloc(mc_num, sizeof(struct bnx2x_mcast_list_elem),\n\t\t\t     GFP_KERNEL);\n\t\tif (!mc) {\n\t\t\tBNX2X_ERR(\"Cannot Configure multicasts due to lack of memory\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tif (mc_num) {\n\t\tINIT_LIST_HEAD(&mcast.mcast_list);\n\t\tfor (i = 0; i < mc_num; i++) {\n\t\t\tmc[i].mac = mcasts[i];\n\t\t\tlist_add_tail(&mc[i].link,\n\t\t\t\t      &mcast.mcast_list);\n\t\t}\n\n\t\t \n\t\tmcast.mcast_list_len = mc_num;\n\t\trc = bnx2x_config_mcast(bp, &mcast, BNX2X_MCAST_CMD_SET);\n\t\tif (rc)\n\t\t\tBNX2X_ERR(\"Failed to set multicasts\\n\");\n\t} else {\n\t\t \n\t\trc = bnx2x_config_mcast(bp, &mcast, BNX2X_MCAST_CMD_DEL);\n\t\tif (rc)\n\t\t\tBNX2X_ERR(\"Failed to remove multicasts\\n\");\n\t}\n\n\tkfree(mc);\n\n\treturn rc;\n}\n\nstatic void bnx2x_vf_prep_rx_mode(struct bnx2x *bp, u8 qid,\n\t\t\t\t  struct bnx2x_rx_mode_ramrod_params *ramrod,\n\t\t\t\t  struct bnx2x_virtf *vf,\n\t\t\t\t  unsigned long accept_flags)\n{\n\tstruct bnx2x_vf_queue *vfq = vfq_get(vf, qid);\n\n\tmemset(ramrod, 0, sizeof(*ramrod));\n\tramrod->cid = vfq->cid;\n\tramrod->cl_id = vfq_cl_id(vf, vfq);\n\tramrod->rx_mode_obj = &bp->rx_mode_obj;\n\tramrod->func_id = FW_VF_HANDLE(vf->abs_vfid);\n\tramrod->rx_accept_flags = accept_flags;\n\tramrod->tx_accept_flags = accept_flags;\n\tramrod->pstate = &vf->filter_state;\n\tramrod->state = BNX2X_FILTER_RX_MODE_PENDING;\n\n\tset_bit(BNX2X_FILTER_RX_MODE_PENDING, &vf->filter_state);\n\tset_bit(RAMROD_RX, &ramrod->ramrod_flags);\n\tset_bit(RAMROD_TX, &ramrod->ramrod_flags);\n\n\tramrod->rdata = bnx2x_vf_sp(bp, vf, rx_mode_rdata.e2);\n\tramrod->rdata_mapping = bnx2x_vf_sp_map(bp, vf, rx_mode_rdata.e2);\n}\n\nint bnx2x_vf_rxmode(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t    int qid, unsigned long accept_flags)\n{\n\tstruct bnx2x_rx_mode_ramrod_params ramrod;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d]\\n\", vf->abs_vfid);\n\n\tbnx2x_vf_prep_rx_mode(bp, qid, &ramrod, vf, accept_flags);\n\tset_bit(RAMROD_COMP_WAIT, &ramrod.ramrod_flags);\n\tvfq_get(vf, qid)->accept_flags = ramrod.rx_accept_flags;\n\treturn bnx2x_config_rx_mode(bp, &ramrod);\n}\n\nint bnx2x_vf_queue_teardown(struct bnx2x *bp, struct bnx2x_virtf *vf, int qid)\n{\n\tint rc;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d:%d]\\n\", vf->abs_vfid, qid);\n\n\t \n\tif (qid == LEADING_IDX) {\n\t\trc = bnx2x_vf_rxmode(bp, vf, qid, 0);\n\t\tif (rc)\n\t\t\tgoto op_err;\n\n\t\t \n\t\tif (bnx2x_validate_vf_sp_objs(bp, vf, true)) {\n\t\t\trc = bnx2x_vf_vlan_mac_clear(bp, vf, qid,\n\t\t\t\t\t\t     false,\n\t\t\t\t\t\t     BNX2X_VF_FILTER_VLAN_MAC);\n\t\t\tif (rc)\n\t\t\t\tgoto op_err;\n\t\t\trc = bnx2x_vf_vlan_mac_clear(bp, vf, qid,\n\t\t\t\t\t\t     false,\n\t\t\t\t\t\t     BNX2X_VF_FILTER_VLAN);\n\t\t\tif (rc)\n\t\t\t\tgoto op_err;\n\t\t\trc = bnx2x_vf_vlan_mac_clear(bp, vf, qid,\n\t\t\t\t\t\t     false,\n\t\t\t\t\t\t     BNX2X_VF_FILTER_MAC);\n\t\t\tif (rc)\n\t\t\t\tgoto op_err;\n\t\t\trc = bnx2x_vf_mcast(bp, vf, NULL, 0, false);\n\t\t\tif (rc)\n\t\t\t\tgoto op_err;\n\t\t}\n\t}\n\n\t \n\trc = bnx2x_vf_queue_destroy(bp, vf, qid);\n\tif (rc)\n\t\tgoto op_err;\n\treturn rc;\nop_err:\n\tBNX2X_ERR(\"vf[%d:%d] error: rc %d\\n\",\n\t\t  vf->abs_vfid, qid, rc);\n\treturn rc;\n}\n\n \n\n \nstatic void bnx2x_vf_enable_internal(struct bnx2x *bp, u8 enable)\n{\n\tREG_WR(bp, PGLUE_B_REG_INTERNAL_VFID_ENABLE, enable ? 1 : 0);\n}\n\n \nstatic void bnx2x_vf_semi_clear_err(struct bnx2x *bp, u8 abs_vfid)\n{\n\tREG_WR(bp, TSEM_REG_VFPF_ERR_NUM, abs_vfid);\n\tREG_WR(bp, USEM_REG_VFPF_ERR_NUM, abs_vfid);\n\tREG_WR(bp, CSEM_REG_VFPF_ERR_NUM, abs_vfid);\n\tREG_WR(bp, XSEM_REG_VFPF_ERR_NUM, abs_vfid);\n}\n\nstatic void bnx2x_vf_pglue_clear_err(struct bnx2x *bp, u8 abs_vfid)\n{\n\tu32 was_err_group = (2 * BP_PATH(bp) + abs_vfid) >> 5;\n\tu32 was_err_reg = 0;\n\n\tswitch (was_err_group) {\n\tcase 0:\n\t    was_err_reg = PGLUE_B_REG_WAS_ERROR_VF_31_0_CLR;\n\t    break;\n\tcase 1:\n\t    was_err_reg = PGLUE_B_REG_WAS_ERROR_VF_63_32_CLR;\n\t    break;\n\tcase 2:\n\t    was_err_reg = PGLUE_B_REG_WAS_ERROR_VF_95_64_CLR;\n\t    break;\n\tcase 3:\n\t    was_err_reg = PGLUE_B_REG_WAS_ERROR_VF_127_96_CLR;\n\t    break;\n\t}\n\tREG_WR(bp, was_err_reg, 1 << (abs_vfid & 0x1f));\n}\n\nstatic void bnx2x_vf_igu_reset(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tint i;\n\tu32 val;\n\n\t \n\tbnx2x_pretend_func(bp, HW_VF_HANDLE(bp, vf->abs_vfid));\n\n\tREG_WR(bp, IGU_REG_SB_INT_BEFORE_MASK_LSB, 0);\n\tREG_WR(bp, IGU_REG_SB_INT_BEFORE_MASK_MSB, 0);\n\tREG_WR(bp, IGU_REG_SB_MASK_LSB, 0);\n\tREG_WR(bp, IGU_REG_SB_MASK_MSB, 0);\n\tREG_WR(bp, IGU_REG_PBA_STATUS_LSB, 0);\n\tREG_WR(bp, IGU_REG_PBA_STATUS_MSB, 0);\n\n\tval = REG_RD(bp, IGU_REG_VF_CONFIGURATION);\n\tval |= (IGU_VF_CONF_FUNC_EN | IGU_VF_CONF_MSI_MSIX_EN);\n\tval &= ~IGU_VF_CONF_PARENT_MASK;\n\tval |= (BP_ABS_FUNC(bp) >> 1) << IGU_VF_CONF_PARENT_SHIFT;\n\tREG_WR(bp, IGU_REG_VF_CONFIGURATION, val);\n\n\tDP(BNX2X_MSG_IOV,\n\t   \"value in IGU_REG_VF_CONFIGURATION of vf %d after write is 0x%08x\\n\",\n\t   vf->abs_vfid, val);\n\n\tbnx2x_pretend_func(bp, BP_ABS_FUNC(bp));\n\n\t \n\tfor (i = 0; i < vf_sb_count(vf); i++) {\n\t\tu8 igu_sb_id = vf_igu_sb(vf, i);\n\n\t\t \n\t\tREG_WR(bp, IGU_REG_PROD_CONS_MEMORY + igu_sb_id * 4, 0);\n\n\t\t \n\t\tbnx2x_igu_clear_sb_gen(bp, vf->abs_vfid, igu_sb_id,\n\t\t\t\t       false  );\n\n\t\t \n\t\tbnx2x_vf_igu_ack_sb(bp, vf, igu_sb_id, USTORM_ID, 0,\n\t\t\t\t    IGU_INT_DISABLE, 1);\n\t}\n}\n\nvoid bnx2x_vf_enable_access(struct bnx2x *bp, u8 abs_vfid)\n{\n\tu16 abs_fid;\n\n\tabs_fid = FW_VF_HANDLE(abs_vfid);\n\n\t \n\tstorm_memset_vf_to_pf(bp, abs_fid, BP_FUNC(bp));\n\tstorm_memset_func_en(bp, abs_fid, 1);\n\n\t \n\tif (bp->fw_cap & FW_CAP_INVALIDATE_VF_FP_HSI)\n\t\tREG_WR8(bp, BAR_XSTRORM_INTMEM +\n\t\t\t    XSTORM_ETH_FUNCTION_INFO_FP_HSI_VALID_E2_OFFSET(abs_fid), 0);\n\n\t \n\tbnx2x_vf_semi_clear_err(bp, abs_vfid);\n\tbnx2x_vf_pglue_clear_err(bp, abs_vfid);\n\n\t \n\tbnx2x_pretend_func(bp, HW_VF_HANDLE(bp, abs_vfid));\n\tDP(BNX2X_MSG_IOV, \"enabling internal access for vf %x\\n\", abs_vfid);\n\tbnx2x_vf_enable_internal(bp, true);\n\tbnx2x_pretend_func(bp, BP_ABS_FUNC(bp));\n}\n\nstatic void bnx2x_vf_enable_traffic(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\t \n\tbnx2x_vf_igu_reset(bp, vf);\n\n\t \n\tbnx2x_pretend_func(bp, HW_VF_HANDLE(bp, vf->abs_vfid));\n\tREG_WR(bp, PBF_REG_DISABLE_VF, 0);\n\tbnx2x_pretend_func(bp, BP_ABS_FUNC(bp));\n}\n\nstatic u8 bnx2x_vf_is_pcie_pending(struct bnx2x *bp, u8 abs_vfid)\n{\n\tstruct bnx2x_virtf *vf = bnx2x_vf_by_abs_fid(bp, abs_vfid);\n\tstruct pci_dev *dev;\n\tbool pending;\n\n\tif (!vf)\n\t\treturn false;\n\n\tdev = pci_get_domain_bus_and_slot(vf->domain, vf->bus, vf->devfn);\n\tif (!dev)\n\t\treturn false;\n\tpending = bnx2x_is_pcie_pending(dev);\n\tpci_dev_put(dev);\n\n\treturn pending;\n}\n\nint bnx2x_vf_flr_clnup_epilog(struct bnx2x *bp, u8 abs_vfid)\n{\n\t \n\tif (bnx2x_vf_is_pcie_pending(bp, abs_vfid))\n\t\tBNX2X_ERR(\"PCIE Transactions still pending\\n\");\n\n\treturn 0;\n}\n\n \nstatic void\nbnx2x_iov_static_resc(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tstruct vf_pf_resc_request *resc = &vf->alloc_resc;\n\n\t \n\tresc->num_rxqs = 0;\n\tresc->num_txqs = 0;\n\n\tresc->num_mac_filters = VF_MAC_CREDIT_CNT;\n\tresc->num_vlan_filters = VF_VLAN_CREDIT_CNT;\n\n\t \n\tresc->num_mc_filters = 0;\n\n\t \n\tresc->num_sbs = vf->sb_count;\n}\n\n \nstatic void bnx2x_vf_free_resc(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\t \n\tbnx2x_iov_static_resc(bp, vf);\n\tvf->state = VF_FREE;\n}\n\nstatic void bnx2x_vf_flr_clnup_hw(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tu32 poll_cnt = bnx2x_flr_clnup_poll_count(bp);\n\n\t \n\tbnx2x_pretend_func(bp, HW_VF_HANDLE(bp, vf->abs_vfid));\n\tbnx2x_flr_clnup_poll_hw_counter(bp, DORQ_REG_VF_USAGE_CNT,\n\t\t\t\t\t\"DQ VF usage counter timed out\",\n\t\t\t\t\tpoll_cnt);\n\tbnx2x_pretend_func(bp, BP_ABS_FUNC(bp));\n\n\t \n\tif (bnx2x_send_final_clnup(bp, (u8)FW_VF_HANDLE(vf->abs_vfid),\n\t\t\t\t   poll_cnt))\n\t\tBNX2X_ERR(\"VF[%d] Final cleanup timed-out\\n\", vf->abs_vfid);\n\n\t \n\tbnx2x_tx_hw_flushed(bp, poll_cnt);\n}\n\nstatic void bnx2x_vf_flr(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tint rc, i;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d]\\n\", vf->abs_vfid);\n\n\t \n\tfor (i = 0; i < vf_rxq_count(vf); i++) {\n\t\trc = bnx2x_vf_queue_flr(bp, vf, i);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\t \n\tbnx2x_vf_mcast(bp, vf, NULL, 0, true);\n\n\t \n\tbnx2x_vf_flr_clnup_hw(bp, vf);\n\n\t \n\tbnx2x_vf_free_resc(bp, vf);\n\n\tvf->malicious = false;\n\n\t \n\tbnx2x_vf_enable_mbx(bp, vf->abs_vfid);\n\treturn;\nout:\n\tBNX2X_ERR(\"vf[%d:%d] failed flr: rc %d\\n\",\n\t\t  vf->abs_vfid, i, rc);\n}\n\nstatic void bnx2x_vf_flr_clnup(struct bnx2x *bp)\n{\n\tstruct bnx2x_virtf *vf;\n\tint i;\n\n\tfor (i = 0; i < BNX2X_NR_VIRTFN(bp); i++) {\n\t\t \n\t\tif (bnx2x_vf(bp, i, state) != VF_RESET ||\n\t\t    !bnx2x_vf(bp, i, flr_clnup_stage))\n\t\t\tcontinue;\n\n\t\tDP(BNX2X_MSG_IOV, \"next vf to cleanup: %d. Num of vfs: %d\\n\",\n\t\t   i, BNX2X_NR_VIRTFN(bp));\n\n\t\tvf = BP_VF(bp, i);\n\n\t\t \n\t\tbnx2x_lock_vf_pf_channel(bp, vf, CHANNEL_TLV_FLR);\n\n\t\t \n\t\tbnx2x_vf_flr(bp, vf);\n\n\t\t \n\t\tvf->flr_clnup_stage = false;\n\t\tbnx2x_unlock_vf_pf_channel(bp, vf, CHANNEL_TLV_FLR);\n\t}\n\n\t \n\tDP(BNX2X_MSG_MCP, \"DRV_STATUS_VF_DISABLED ACK for vfs 0x%x 0x%x\\n\",\n\t   bp->vfdb->flrd_vfs[0], bp->vfdb->flrd_vfs[1]);\n\tfor (i = 0; i < FLRD_VFS_DWORDS; i++)\n\t\tSHMEM2_WR(bp, drv_ack_vf_disabled[BP_FW_MB_IDX(bp)][i],\n\t\t\t  bp->vfdb->flrd_vfs[i]);\n\n\tbnx2x_fw_command(bp, DRV_MSG_CODE_VF_DISABLED_DONE, 0);\n\n\t \n\tfor (i = 0; i < FLRD_VFS_DWORDS; i++)\n\t\tSHMEM2_WR(bp, drv_ack_vf_disabled[BP_FW_MB_IDX(bp)][i], 0);\n}\n\nvoid bnx2x_vf_handle_flr_event(struct bnx2x *bp)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < FLRD_VFS_DWORDS; i++)\n\t\tbp->vfdb->flrd_vfs[i] = SHMEM2_RD(bp, mcp_vf_disabled[i]);\n\n\tDP(BNX2X_MSG_MCP,\n\t   \"DRV_STATUS_VF_DISABLED received for vfs 0x%x 0x%x\\n\",\n\t   bp->vfdb->flrd_vfs[0], bp->vfdb->flrd_vfs[1]);\n\n\tfor_each_vf(bp, i) {\n\t\tstruct bnx2x_virtf *vf = BP_VF(bp, i);\n\t\tu32 reset = 0;\n\n\t\tif (vf->abs_vfid < 32)\n\t\t\treset = bp->vfdb->flrd_vfs[0] & (1 << vf->abs_vfid);\n\t\telse\n\t\t\treset = bp->vfdb->flrd_vfs[1] &\n\t\t\t\t(1 << (vf->abs_vfid - 32));\n\n\t\tif (reset) {\n\t\t\t \n\t\t\tvf->state = VF_RESET;\n\t\t\tvf->flr_clnup_stage = true;\n\n\t\t\tDP(BNX2X_MSG_IOV,\n\t\t\t   \"Initiating Final cleanup for VF %d\\n\",\n\t\t\t   vf->abs_vfid);\n\t\t}\n\t}\n\n\t \n\tbnx2x_vf_flr_clnup(bp);\n}\n\n \nvoid bnx2x_iov_init_dq(struct bnx2x *bp)\n{\n\tif (!IS_SRIOV(bp))\n\t\treturn;\n\n\t \n\tREG_WR(bp, DORQ_REG_VF_NORM_VF_BASE, 0);\n\tREG_WR(bp, DORQ_REG_MAX_RVFID_SIZE, ilog2(BNX2X_MAX_NUM_OF_VFS));\n\n\t \n\tREG_WR(bp, DORQ_REG_VF_NORM_CID_BASE, BNX2X_FIRST_VF_CID);\n\n\t \n\tREG_WR(bp, DORQ_REG_VF_NORM_CID_WND_SIZE, BNX2X_VF_CID_WND);\n\n\t \n\tREG_WR(bp, DORQ_REG_VF_NORM_CID_OFST, 3);\n\n\t \n\tREG_WR(bp, DORQ_REG_VF_TYPE_MASK_0, 1);\n\tREG_WR(bp, DORQ_REG_VF_TYPE_VALUE_0, 0);\n\tREG_WR(bp, DORQ_REG_VF_TYPE_MIN_MCID_0, 0);\n\tREG_WR(bp, DORQ_REG_VF_TYPE_MAX_MCID_0, 0x1ffff);\n\n\t \n\tREG_WR(bp, DORQ_REG_VF_USAGE_CT_LIMIT, 64);\n}\n\nvoid bnx2x_iov_init_dmae(struct bnx2x *bp)\n{\n\tif (pci_find_ext_capability(bp->pdev, PCI_EXT_CAP_ID_SRIOV))\n\t\tREG_WR(bp, DMAE_REG_BACKWARD_COMP_EN, 0);\n}\n\nstatic int bnx2x_vf_domain(struct bnx2x *bp, int vfid)\n{\n\tstruct pci_dev *dev = bp->pdev;\n\n\treturn pci_domain_nr(dev->bus);\n}\n\nstatic int bnx2x_vf_bus(struct bnx2x *bp, int vfid)\n{\n\tstruct pci_dev *dev = bp->pdev;\n\tstruct bnx2x_sriov *iov = &bp->vfdb->sriov;\n\n\treturn dev->bus->number + ((dev->devfn + iov->offset +\n\t\t\t\t    iov->stride * vfid) >> 8);\n}\n\nstatic int bnx2x_vf_devfn(struct bnx2x *bp, int vfid)\n{\n\tstruct pci_dev *dev = bp->pdev;\n\tstruct bnx2x_sriov *iov = &bp->vfdb->sriov;\n\n\treturn (dev->devfn + iov->offset + iov->stride * vfid) & 0xff;\n}\n\nstatic void bnx2x_vf_set_bars(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tint i, n;\n\tstruct pci_dev *dev = bp->pdev;\n\tstruct bnx2x_sriov *iov = &bp->vfdb->sriov;\n\n\tfor (i = 0, n = 0; i < PCI_SRIOV_NUM_BARS; i += 2, n++) {\n\t\tu64 start = pci_resource_start(dev, PCI_IOV_RESOURCES + i);\n\t\tu32 size = pci_resource_len(dev, PCI_IOV_RESOURCES + i);\n\n\t\tsize /= iov->total;\n\t\tvf->bars[n].bar = start + size * vf->abs_vfid;\n\t\tvf->bars[n].size = size;\n\t}\n}\n\nstatic int\nbnx2x_get_vf_igu_cam_info(struct bnx2x *bp)\n{\n\tint sb_id;\n\tu32 val;\n\tu8 fid, current_pf = 0;\n\n\t \n\tfor (sb_id = 0; sb_id < IGU_REG_MAPPING_MEMORY_SIZE; sb_id++) {\n\t\tval = REG_RD(bp, IGU_REG_MAPPING_MEMORY + sb_id * 4);\n\t\tif (!(val & IGU_REG_MAPPING_MEMORY_VALID))\n\t\t\tcontinue;\n\t\tfid = GET_FIELD((val), IGU_REG_MAPPING_MEMORY_FID);\n\t\tif (fid & IGU_FID_ENCODE_IS_PF)\n\t\t\tcurrent_pf = fid & IGU_FID_PF_NUM_MASK;\n\t\telse if (current_pf == BP_FUNC(bp))\n\t\t\tbnx2x_vf_set_igu_info(bp, sb_id,\n\t\t\t\t\t      (fid & IGU_FID_VF_NUM_MASK));\n\t\tDP(BNX2X_MSG_IOV, \"%s[%d], igu_sb_id=%d, msix=%d\\n\",\n\t\t   ((fid & IGU_FID_ENCODE_IS_PF) ? \"PF\" : \"VF\"),\n\t\t   ((fid & IGU_FID_ENCODE_IS_PF) ? (fid & IGU_FID_PF_NUM_MASK) :\n\t\t   (fid & IGU_FID_VF_NUM_MASK)), sb_id,\n\t\t   GET_FIELD((val), IGU_REG_MAPPING_MEMORY_VECTOR));\n\t}\n\tDP(BNX2X_MSG_IOV, \"vf_sbs_pool is %d\\n\", BP_VFDB(bp)->vf_sbs_pool);\n\treturn BP_VFDB(bp)->vf_sbs_pool;\n}\n\nstatic void __bnx2x_iov_free_vfdb(struct bnx2x *bp)\n{\n\tif (bp->vfdb) {\n\t\tkfree(bp->vfdb->vfqs);\n\t\tkfree(bp->vfdb->vfs);\n\t\tkfree(bp->vfdb);\n\t}\n\tbp->vfdb = NULL;\n}\n\nstatic int bnx2x_sriov_pci_cfg_info(struct bnx2x *bp, struct bnx2x_sriov *iov)\n{\n\tint pos;\n\tstruct pci_dev *dev = bp->pdev;\n\n\tpos = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_SRIOV);\n\tif (!pos) {\n\t\tBNX2X_ERR(\"failed to find SRIOV capability in device\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tiov->pos = pos;\n\tDP(BNX2X_MSG_IOV, \"sriov ext pos %d\\n\", pos);\n\tpci_read_config_word(dev, pos + PCI_SRIOV_CTRL, &iov->ctrl);\n\tpci_read_config_word(dev, pos + PCI_SRIOV_TOTAL_VF, &iov->total);\n\tpci_read_config_word(dev, pos + PCI_SRIOV_INITIAL_VF, &iov->initial);\n\tpci_read_config_word(dev, pos + PCI_SRIOV_VF_OFFSET, &iov->offset);\n\tpci_read_config_word(dev, pos + PCI_SRIOV_VF_STRIDE, &iov->stride);\n\tpci_read_config_dword(dev, pos + PCI_SRIOV_SUP_PGSIZE, &iov->pgsz);\n\tpci_read_config_dword(dev, pos + PCI_SRIOV_CAP, &iov->cap);\n\tpci_read_config_byte(dev, pos + PCI_SRIOV_FUNC_LINK, &iov->link);\n\n\treturn 0;\n}\n\nstatic int bnx2x_sriov_info(struct bnx2x *bp, struct bnx2x_sriov *iov)\n{\n\tu32 val;\n\n\t \n\tif (bnx2x_sriov_pci_cfg_info(bp, iov))\n\t\treturn -ENODEV;\n\n\t \n\tiov->nres = 0;\n\n\t \n\tval = REG_RD(bp, PCICFG_OFFSET + GRC_CONFIG_REG_PF_INIT_VF);\n\tiov->first_vf_in_pf = ((val & GRC_CR_PF_INIT_VF_PF_FIRST_VF_NUM_MASK)\n\t\t\t       * 8) - (BNX2X_MAX_NUM_OF_VFS * BP_PATH(bp));\n\n\tDP(BNX2X_MSG_IOV,\n\t   \"IOV info[%d]: first vf %d, nres %d, cap 0x%x, ctrl 0x%x, total %d, initial %d, num vfs %d, offset %d, stride %d, page size 0x%x\\n\",\n\t   BP_FUNC(bp),\n\t   iov->first_vf_in_pf, iov->nres, iov->cap, iov->ctrl, iov->total,\n\t   iov->initial, iov->nr_virtfn, iov->offset, iov->stride, iov->pgsz);\n\n\treturn 0;\n}\n\n \nint bnx2x_iov_init_one(struct bnx2x *bp, int int_mode_param,\n\t\t       int num_vfs_param)\n{\n\tint err, i;\n\tstruct bnx2x_sriov *iov;\n\tstruct pci_dev *dev = bp->pdev;\n\n\tbp->vfdb = NULL;\n\n\t \n\tif (IS_VF(bp))\n\t\treturn 0;\n\n\t \n\tif (!pci_find_ext_capability(dev, PCI_EXT_CAP_ID_SRIOV))\n\t\treturn 0;\n\n\t \n\tif (CHIP_IS_E1x(bp))\n\t\treturn 0;\n\n\t \n\tif (!num_vfs_param)\n\t\treturn 0;\n\n\t \n\tif (BNX2X_L2_MAX_CID(bp) >= BNX2X_FIRST_VF_CID) {\n\t\tBNX2X_ERR(\"PF cids %d are overspilling into vf space (starts at %d). Abort SRIOV\\n\",\n\t\t\t  BNX2X_L2_MAX_CID(bp), BNX2X_FIRST_VF_CID);\n\t\treturn 0;\n\t}\n\n\t \n\tif (int_mode_param == BNX2X_INT_MODE_MSI ||\n\t    int_mode_param == BNX2X_INT_MODE_INTX) {\n\t\tBNX2X_ERR(\"Forced MSI/INTx mode is incompatible with SRIOV\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tif (!pci_ari_enabled(bp->pdev->bus)) {\n\t\tBNX2X_ERR(\"ARI not supported (check pci bridge ARI forwarding), SRIOV can not be enabled\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tif (CHIP_INT_MODE_IS_BC(bp)) {\n\t\tBNX2X_ERR(\"IGU not normal mode,  SRIOV can not be enabled\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tbp->vfdb = kzalloc(sizeof(*(bp->vfdb)), GFP_KERNEL);\n\tif (!bp->vfdb) {\n\t\tBNX2X_ERR(\"failed to allocate vf database\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto failed;\n\t}\n\n\t \n\tiov = &(bp->vfdb->sriov);\n\terr = bnx2x_sriov_info(bp, iov);\n\tif (err)\n\t\tgoto failed;\n\n\t \n\tif (iov->total == 0) {\n\t\terr = 0;\n\t\tgoto failed;\n\t}\n\n\tiov->nr_virtfn = min_t(u16, iov->total, num_vfs_param);\n\n\tDP(BNX2X_MSG_IOV, \"num_vfs_param was %d, nr_virtfn was %d\\n\",\n\t   num_vfs_param, iov->nr_virtfn);\n\n\t \n\tbp->vfdb->vfs = kcalloc(BNX2X_NR_VIRTFN(bp),\n\t\t\t\tsizeof(struct bnx2x_virtf),\n\t\t\t\tGFP_KERNEL);\n\tif (!bp->vfdb->vfs) {\n\t\tBNX2X_ERR(\"failed to allocate vf array\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto failed;\n\t}\n\n\t \n\tfor_each_vf(bp, i) {\n\t\tbnx2x_vf(bp, i, index) = i;\n\t\tbnx2x_vf(bp, i, abs_vfid) = iov->first_vf_in_pf + i;\n\t\tbnx2x_vf(bp, i, state) = VF_FREE;\n\t\tmutex_init(&bnx2x_vf(bp, i, op_mutex));\n\t\tbnx2x_vf(bp, i, op_current) = CHANNEL_TLV_NONE;\n\t\t \n\t\tbnx2x_vf(bp, i, spoofchk) = 1;\n\t}\n\n\t \n\tif (!bnx2x_get_vf_igu_cam_info(bp)) {\n\t\tBNX2X_ERR(\"No entries in IGU CAM for vfs\\n\");\n\t\terr = -EINVAL;\n\t\tgoto failed;\n\t}\n\n\t \n\tbp->vfdb->vfqs = kcalloc(BNX2X_MAX_NUM_VF_QUEUES,\n\t\t\t\t sizeof(struct bnx2x_vf_queue),\n\t\t\t\t GFP_KERNEL);\n\n\tif (!bp->vfdb->vfqs) {\n\t\tBNX2X_ERR(\"failed to allocate vf queue array\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto failed;\n\t}\n\n\t \n\tmutex_init(&bp->vfdb->event_mutex);\n\n\tmutex_init(&bp->vfdb->bulletin_mutex);\n\n\tif (SHMEM2_HAS(bp, sriov_switch_mode))\n\t\tSHMEM2_WR(bp, sriov_switch_mode, SRIOV_SWITCH_MODE_VEB);\n\n\treturn 0;\nfailed:\n\tDP(BNX2X_MSG_IOV, \"Failed err=%d\\n\", err);\n\t__bnx2x_iov_free_vfdb(bp);\n\treturn err;\n}\n\nvoid bnx2x_iov_remove_one(struct bnx2x *bp)\n{\n\tint vf_idx;\n\n\t \n\tif (!IS_SRIOV(bp))\n\t\treturn;\n\n\tbnx2x_disable_sriov(bp);\n\n\t \n\tfor (vf_idx = 0; vf_idx < bp->vfdb->sriov.total; vf_idx++) {\n\t\tbnx2x_pretend_func(bp,\n\t\t\t\t   HW_VF_HANDLE(bp,\n\t\t\t\t\t\tbp->vfdb->sriov.first_vf_in_pf +\n\t\t\t\t\t\tvf_idx));\n\t\tDP(BNX2X_MSG_IOV, \"disabling internal access for vf %d\\n\",\n\t\t   bp->vfdb->sriov.first_vf_in_pf + vf_idx);\n\t\tbnx2x_vf_enable_internal(bp, 0);\n\t\tbnx2x_pretend_func(bp, BP_ABS_FUNC(bp));\n\t}\n\n\t \n\t__bnx2x_iov_free_vfdb(bp);\n}\n\nvoid bnx2x_iov_free_mem(struct bnx2x *bp)\n{\n\tint i;\n\n\tif (!IS_SRIOV(bp))\n\t\treturn;\n\n\t \n\tfor (i = 0; i < BNX2X_VF_CIDS/ILT_PAGE_CIDS; i++) {\n\t\tstruct hw_dma *cxt = &bp->vfdb->context[i];\n\t\tBNX2X_PCI_FREE(cxt->addr, cxt->mapping, cxt->size);\n\t}\n\n\tBNX2X_PCI_FREE(BP_VFDB(bp)->sp_dma.addr,\n\t\t       BP_VFDB(bp)->sp_dma.mapping,\n\t\t       BP_VFDB(bp)->sp_dma.size);\n\n\tBNX2X_PCI_FREE(BP_VF_MBX_DMA(bp)->addr,\n\t\t       BP_VF_MBX_DMA(bp)->mapping,\n\t\t       BP_VF_MBX_DMA(bp)->size);\n\n\tBNX2X_PCI_FREE(BP_VF_BULLETIN_DMA(bp)->addr,\n\t\t       BP_VF_BULLETIN_DMA(bp)->mapping,\n\t\t       BP_VF_BULLETIN_DMA(bp)->size);\n}\n\nint bnx2x_iov_alloc_mem(struct bnx2x *bp)\n{\n\tsize_t tot_size;\n\tint i, rc = 0;\n\n\tif (!IS_SRIOV(bp))\n\t\treturn rc;\n\n\t \n\ttot_size = (BP_VFDB(bp)->sriov.first_vf_in_pf + BNX2X_NR_VIRTFN(bp)) *\n\t\tBNX2X_CIDS_PER_VF * sizeof(union cdu_context);\n\n\tfor (i = 0; i < BNX2X_VF_CIDS/ILT_PAGE_CIDS; i++) {\n\t\tstruct hw_dma *cxt = BP_VF_CXT_PAGE(bp, i);\n\t\tcxt->size = min_t(size_t, tot_size, CDU_ILT_PAGE_SZ);\n\n\t\tif (cxt->size) {\n\t\t\tcxt->addr = BNX2X_PCI_ALLOC(&cxt->mapping, cxt->size);\n\t\t\tif (!cxt->addr)\n\t\t\t\tgoto alloc_mem_err;\n\t\t} else {\n\t\t\tcxt->addr = NULL;\n\t\t\tcxt->mapping = 0;\n\t\t}\n\t\ttot_size -= cxt->size;\n\t}\n\n\t \n\ttot_size = BNX2X_NR_VIRTFN(bp) * sizeof(struct bnx2x_vf_sp);\n\tBP_VFDB(bp)->sp_dma.addr = BNX2X_PCI_ALLOC(&BP_VFDB(bp)->sp_dma.mapping,\n\t\t\t\t\t\t   tot_size);\n\tif (!BP_VFDB(bp)->sp_dma.addr)\n\t\tgoto alloc_mem_err;\n\tBP_VFDB(bp)->sp_dma.size = tot_size;\n\n\t \n\ttot_size = BNX2X_NR_VIRTFN(bp) * MBX_MSG_ALIGNED_SIZE;\n\tBP_VF_MBX_DMA(bp)->addr = BNX2X_PCI_ALLOC(&BP_VF_MBX_DMA(bp)->mapping,\n\t\t\t\t\t\t  tot_size);\n\tif (!BP_VF_MBX_DMA(bp)->addr)\n\t\tgoto alloc_mem_err;\n\n\tBP_VF_MBX_DMA(bp)->size = tot_size;\n\n\t \n\ttot_size = BNX2X_NR_VIRTFN(bp) * BULLETIN_CONTENT_SIZE;\n\tBP_VF_BULLETIN_DMA(bp)->addr = BNX2X_PCI_ALLOC(&BP_VF_BULLETIN_DMA(bp)->mapping,\n\t\t\t\t\t\t       tot_size);\n\tif (!BP_VF_BULLETIN_DMA(bp)->addr)\n\t\tgoto alloc_mem_err;\n\n\tBP_VF_BULLETIN_DMA(bp)->size = tot_size;\n\n\treturn 0;\n\nalloc_mem_err:\n\treturn -ENOMEM;\n}\n\nstatic void bnx2x_vfq_init(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t   struct bnx2x_vf_queue *q)\n{\n\tu8 cl_id = vfq_cl_id(vf, q);\n\tu8 func_id = FW_VF_HANDLE(vf->abs_vfid);\n\tunsigned long q_type = 0;\n\n\tset_bit(BNX2X_Q_TYPE_HAS_TX, &q_type);\n\tset_bit(BNX2X_Q_TYPE_HAS_RX, &q_type);\n\n\t \n\tbnx2x_init_queue_obj(bp, &q->sp_obj,\n\t\t\t     cl_id, &q->cid, 1, func_id,\n\t\t\t     bnx2x_vf_sp(bp, vf, q_data),\n\t\t\t     bnx2x_vf_sp_map(bp, vf, q_data),\n\t\t\t     q_type);\n\n\t \n\tq->sp_initialized = false;\n\n\tDP(BNX2X_MSG_IOV,\n\t   \"initialized vf %d's queue object. func id set to %d. cid set to 0x%x\\n\",\n\t   vf->abs_vfid, q->sp_obj.func_id, q->cid);\n}\n\nstatic int bnx2x_max_speed_cap(struct bnx2x *bp)\n{\n\tu32 supported = bp->port.supported[bnx2x_get_link_cfg_idx(bp)];\n\n\tif (supported &\n\t    (SUPPORTED_20000baseMLD2_Full | SUPPORTED_20000baseKR2_Full))\n\t\treturn 20000;\n\n\treturn 10000;  \n}\n\nint bnx2x_iov_link_update_vf(struct bnx2x *bp, int idx)\n{\n\tstruct bnx2x_link_report_data *state = &bp->last_reported_link;\n\tstruct pf_vf_bulletin_content *bulletin;\n\tstruct bnx2x_virtf *vf;\n\tbool update = true;\n\tint rc = 0;\n\n\t \n\trc = bnx2x_vf_op_prep(bp, idx, &vf, &bulletin, false);\n\tif (rc)\n\t\treturn rc;\n\n\tmutex_lock(&bp->vfdb->bulletin_mutex);\n\n\tif (vf->link_cfg == IFLA_VF_LINK_STATE_AUTO) {\n\t\tbulletin->valid_bitmap |= 1 << LINK_VALID;\n\n\t\tbulletin->link_speed = state->line_speed;\n\t\tbulletin->link_flags = 0;\n\t\tif (test_bit(BNX2X_LINK_REPORT_LINK_DOWN,\n\t\t\t     &state->link_report_flags))\n\t\t\tbulletin->link_flags |= VFPF_LINK_REPORT_LINK_DOWN;\n\t\tif (test_bit(BNX2X_LINK_REPORT_FD,\n\t\t\t     &state->link_report_flags))\n\t\t\tbulletin->link_flags |= VFPF_LINK_REPORT_FULL_DUPLEX;\n\t\tif (test_bit(BNX2X_LINK_REPORT_RX_FC_ON,\n\t\t\t     &state->link_report_flags))\n\t\t\tbulletin->link_flags |= VFPF_LINK_REPORT_RX_FC_ON;\n\t\tif (test_bit(BNX2X_LINK_REPORT_TX_FC_ON,\n\t\t\t     &state->link_report_flags))\n\t\t\tbulletin->link_flags |= VFPF_LINK_REPORT_TX_FC_ON;\n\t} else if (vf->link_cfg == IFLA_VF_LINK_STATE_DISABLE &&\n\t\t   !(bulletin->link_flags & VFPF_LINK_REPORT_LINK_DOWN)) {\n\t\tbulletin->valid_bitmap |= 1 << LINK_VALID;\n\t\tbulletin->link_flags |= VFPF_LINK_REPORT_LINK_DOWN;\n\t} else if (vf->link_cfg == IFLA_VF_LINK_STATE_ENABLE &&\n\t\t   (bulletin->link_flags & VFPF_LINK_REPORT_LINK_DOWN)) {\n\t\tbulletin->valid_bitmap |= 1 << LINK_VALID;\n\t\tbulletin->link_speed = bnx2x_max_speed_cap(bp);\n\t\tbulletin->link_flags &= ~VFPF_LINK_REPORT_LINK_DOWN;\n\t} else {\n\t\tupdate = false;\n\t}\n\n\tif (update) {\n\t\tDP(NETIF_MSG_LINK | BNX2X_MSG_IOV,\n\t\t   \"vf %d mode %u speed %d flags %x\\n\", idx,\n\t\t   vf->link_cfg, bulletin->link_speed, bulletin->link_flags);\n\n\t\t \n\t\trc = bnx2x_post_vf_bulletin(bp, idx);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"failed to update VF[%d] bulletin\\n\", idx);\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tmutex_unlock(&bp->vfdb->bulletin_mutex);\n\treturn rc;\n}\n\nint bnx2x_set_vf_link_state(struct net_device *dev, int idx, int link_state)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\tstruct bnx2x_virtf *vf = BP_VF(bp, idx);\n\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\tif (vf->link_cfg == link_state)\n\t\treturn 0;  \n\n\tvf->link_cfg = link_state;\n\n\treturn bnx2x_iov_link_update_vf(bp, idx);\n}\n\nvoid bnx2x_iov_link_update(struct bnx2x *bp)\n{\n\tint vfid;\n\n\tif (!IS_SRIOV(bp))\n\t\treturn;\n\n\tfor_each_vf(bp, vfid)\n\t\tbnx2x_iov_link_update_vf(bp, vfid);\n}\n\n \nint bnx2x_iov_nic_init(struct bnx2x *bp)\n{\n\tint vfid;\n\n\tif (!IS_SRIOV(bp)) {\n\t\tDP(BNX2X_MSG_IOV, \"vfdb was not allocated\\n\");\n\t\treturn 0;\n\t}\n\n\tDP(BNX2X_MSG_IOV, \"num of vfs: %d\\n\", (bp)->vfdb->sriov.nr_virtfn);\n\n\t \n\tmsleep(100);\n\n\t \n\tfor_each_vf(bp, vfid) {\n\t\tstruct bnx2x_virtf *vf = BP_VF(bp, vfid);\n\n\t\tint base_vf_cid = (BP_VFDB(bp)->sriov.first_vf_in_pf + vfid) *\n\t\t\tBNX2X_CIDS_PER_VF;\n\n\t\tunion cdu_context *base_cxt = (union cdu_context *)\n\t\t\tBP_VF_CXT_PAGE(bp, base_vf_cid/ILT_PAGE_CIDS)->addr +\n\t\t\t(base_vf_cid & (ILT_PAGE_CIDS-1));\n\n\t\tDP(BNX2X_MSG_IOV,\n\t\t   \"VF[%d] Max IGU SBs: %d, base vf cid 0x%x, base cid 0x%x, base cxt %p\\n\",\n\t\t   vf->abs_vfid, vf_sb_count(vf), base_vf_cid,\n\t\t   BNX2X_FIRST_VF_CID + base_vf_cid, base_cxt);\n\n\t\t \n\t\tbnx2x_iov_static_resc(bp, vf);\n\n\t\t \n\t\tvf->filter_state = 0;\n\t\tvf->sp_cl_id = bnx2x_fp(bp, 0, cl_id);\n\n\t\tbnx2x_init_credit_pool(&vf->vf_vlans_pool, 0,\n\t\t\t\t       vf_vlan_rules_cnt(vf));\n\t\tbnx2x_init_credit_pool(&vf->vf_macs_pool, 0,\n\t\t\t\t       vf_mac_rules_cnt(vf));\n\n\t\t \n\t\tbnx2x_init_mcast_obj(bp, &vf->mcast_obj, 0xFF,\n\t\t\t\t     0xFF, 0xFF, 0xFF,\n\t\t\t\t     bnx2x_vf_sp(bp, vf, mcast_rdata),\n\t\t\t\t     bnx2x_vf_sp_map(bp, vf, mcast_rdata),\n\t\t\t\t     BNX2X_FILTER_MCAST_PENDING,\n\t\t\t\t     &vf->filter_state,\n\t\t\t\t     BNX2X_OBJ_TYPE_RX_TX);\n\n\t\t \n\t\tBP_VF_MBX(bp, vfid)->msg = (struct bnx2x_vf_mbx_msg *)\n\t\t\t(((u8 *)BP_VF_MBX_DMA(bp)->addr) + vfid *\n\t\t\tMBX_MSG_ALIGNED_SIZE);\n\n\t\tBP_VF_MBX(bp, vfid)->msg_mapping = BP_VF_MBX_DMA(bp)->mapping +\n\t\t\tvfid * MBX_MSG_ALIGNED_SIZE;\n\n\t\t \n\t\tbnx2x_vf_enable_mbx(bp, vf->abs_vfid);\n\t}\n\n\t \n\tfor_each_vf(bp, vfid) {\n\t\tstruct bnx2x_virtf *vf = BP_VF(bp, vfid);\n\n\t\t \n\t\tvf->domain = bnx2x_vf_domain(bp, vfid);\n\t\tvf->bus = bnx2x_vf_bus(bp, vfid);\n\t\tvf->devfn = bnx2x_vf_devfn(bp, vfid);\n\t\tbnx2x_vf_set_bars(bp, vf);\n\n\t\tDP(BNX2X_MSG_IOV,\n\t\t   \"VF info[%d]: bus 0x%x, devfn 0x%x, bar0 [0x%x, %d], bar1 [0x%x, %d], bar2 [0x%x, %d]\\n\",\n\t\t   vf->abs_vfid, vf->bus, vf->devfn,\n\t\t   (unsigned)vf->bars[0].bar, vf->bars[0].size,\n\t\t   (unsigned)vf->bars[1].bar, vf->bars[1].size,\n\t\t   (unsigned)vf->bars[2].bar, vf->bars[2].size);\n\t}\n\n\treturn 0;\n}\n\n \nint bnx2x_iov_chip_cleanup(struct bnx2x *bp)\n{\n\tint i;\n\n\tif (!IS_SRIOV(bp))\n\t\treturn 0;\n\n\t \n\tfor_each_vf(bp, i)\n\t\tbnx2x_vf_release(bp, BP_VF(bp, i));\n\n\treturn 0;\n}\n\n \nint bnx2x_iov_init_ilt(struct bnx2x *bp, u16 line)\n{\n\tint i;\n\tstruct bnx2x_ilt *ilt = BP_ILT(bp);\n\n\tif (!IS_SRIOV(bp))\n\t\treturn line;\n\n\t \n\tfor (i = 0; i < BNX2X_VF_CIDS/ILT_PAGE_CIDS; i++) {\n\t\tstruct hw_dma *hw_cxt = BP_VF_CXT_PAGE(bp, i);\n\n\t\tilt->lines[line+i].page = hw_cxt->addr;\n\t\tilt->lines[line+i].page_mapping = hw_cxt->mapping;\n\t\tilt->lines[line+i].size = hw_cxt->size;  \n\t}\n\treturn line + i;\n}\n\nstatic u8 bnx2x_iov_is_vf_cid(struct bnx2x *bp, u16 cid)\n{\n\treturn ((cid >= BNX2X_FIRST_VF_CID) &&\n\t\t((cid - BNX2X_FIRST_VF_CID) < BNX2X_VF_CIDS));\n}\n\nstatic\nvoid bnx2x_vf_handle_classification_eqe(struct bnx2x *bp,\n\t\t\t\t\tstruct bnx2x_vf_queue *vfq,\n\t\t\t\t\tunion event_ring_elem *elem)\n{\n\tunsigned long ramrod_flags = 0;\n\tint rc = 0;\n\tu32 echo = le32_to_cpu(elem->message.data.eth_event.echo);\n\n\t \n\tset_bit(RAMROD_CONT, &ramrod_flags);\n\n\tswitch (echo >> BNX2X_SWCID_SHIFT) {\n\tcase BNX2X_FILTER_MAC_PENDING:\n\t\trc = vfq->mac_obj.complete(bp, &vfq->mac_obj, elem,\n\t\t\t\t\t   &ramrod_flags);\n\t\tbreak;\n\tcase BNX2X_FILTER_VLAN_PENDING:\n\t\trc = vfq->vlan_obj.complete(bp, &vfq->vlan_obj, elem,\n\t\t\t\t\t    &ramrod_flags);\n\t\tbreak;\n\tdefault:\n\t\tBNX2X_ERR(\"Unsupported classification command: 0x%x\\n\", echo);\n\t\treturn;\n\t}\n\tif (rc < 0)\n\t\tBNX2X_ERR(\"Failed to schedule new commands: %d\\n\", rc);\n\telse if (rc > 0)\n\t\tDP(BNX2X_MSG_IOV, \"Scheduled next pending commands...\\n\");\n}\n\nstatic\nvoid bnx2x_vf_handle_mcast_eqe(struct bnx2x *bp,\n\t\t\t       struct bnx2x_virtf *vf)\n{\n\tstruct bnx2x_mcast_ramrod_params rparam = {NULL};\n\tint rc;\n\n\trparam.mcast_obj = &vf->mcast_obj;\n\tvf->mcast_obj.raw.clear_pending(&vf->mcast_obj.raw);\n\n\t \n\tif (vf->mcast_obj.check_pending(&vf->mcast_obj)) {\n\t\trc = bnx2x_config_mcast(bp, &rparam, BNX2X_MCAST_CMD_CONT);\n\t\tif (rc < 0)\n\t\t\tBNX2X_ERR(\"Failed to send pending mcast commands: %d\\n\",\n\t\t\t\t  rc);\n\t}\n}\n\nstatic\nvoid bnx2x_vf_handle_filters_eqe(struct bnx2x *bp,\n\t\t\t\t struct bnx2x_virtf *vf)\n{\n\tsmp_mb__before_atomic();\n\tclear_bit(BNX2X_FILTER_RX_MODE_PENDING, &vf->filter_state);\n\tsmp_mb__after_atomic();\n}\n\nstatic void bnx2x_vf_handle_rss_update_eqe(struct bnx2x *bp,\n\t\t\t\t\t   struct bnx2x_virtf *vf)\n{\n\tvf->rss_conf_obj.raw.clear_pending(&vf->rss_conf_obj.raw);\n}\n\nint bnx2x_iov_eq_sp_event(struct bnx2x *bp, union event_ring_elem *elem)\n{\n\tstruct bnx2x_virtf *vf;\n\tint qidx = 0, abs_vfid;\n\tu8 opcode;\n\tu16 cid = 0xffff;\n\n\tif (!IS_SRIOV(bp))\n\t\treturn 1;\n\n\t \n\topcode = elem->message.opcode;\n\n\tswitch (opcode) {\n\tcase EVENT_RING_OPCODE_CFC_DEL:\n\t\tcid = SW_CID(elem->message.data.cfc_del_event.cid);\n\t\tDP(BNX2X_MSG_IOV, \"checking cfc-del comp cid=%d\\n\", cid);\n\t\tbreak;\n\tcase EVENT_RING_OPCODE_CLASSIFICATION_RULES:\n\tcase EVENT_RING_OPCODE_MULTICAST_RULES:\n\tcase EVENT_RING_OPCODE_FILTERS_RULES:\n\tcase EVENT_RING_OPCODE_RSS_UPDATE_RULES:\n\t\tcid = SW_CID(elem->message.data.eth_event.echo);\n\t\tDP(BNX2X_MSG_IOV, \"checking filtering comp cid=%d\\n\", cid);\n\t\tbreak;\n\tcase EVENT_RING_OPCODE_VF_FLR:\n\t\tabs_vfid = elem->message.data.vf_flr_event.vf_id;\n\t\tDP(BNX2X_MSG_IOV, \"Got VF FLR notification abs_vfid=%d\\n\",\n\t\t   abs_vfid);\n\t\tgoto get_vf;\n\tcase EVENT_RING_OPCODE_MALICIOUS_VF:\n\t\tabs_vfid = elem->message.data.malicious_vf_event.vf_id;\n\t\tBNX2X_ERR(\"Got VF MALICIOUS notification abs_vfid=%d err_id=0x%x\\n\",\n\t\t\t  abs_vfid,\n\t\t\t  elem->message.data.malicious_vf_event.err_id);\n\t\tgoto get_vf;\n\tdefault:\n\t\treturn 1;\n\t}\n\n\t \n\tif (!bnx2x_iov_is_vf_cid(bp, cid)) {\n\t\tDP(BNX2X_MSG_IOV, \"cid is outside vf range: %d\\n\", cid);\n\t\treturn 1;\n\t}\n\n\t \n\tqidx = cid & ((1 << BNX2X_VF_CID_WND)-1);\n\tabs_vfid = (cid >> BNX2X_VF_CID_WND) & (BNX2X_MAX_NUM_OF_VFS-1);\nget_vf:\n\tvf = bnx2x_vf_by_abs_fid(bp, abs_vfid);\n\n\tif (!vf) {\n\t\tBNX2X_ERR(\"EQ completion for unknown VF, cid %d, abs_vfid %d\\n\",\n\t\t\t  cid, abs_vfid);\n\t\treturn 0;\n\t}\n\n\tswitch (opcode) {\n\tcase EVENT_RING_OPCODE_CFC_DEL:\n\t\tDP(BNX2X_MSG_IOV, \"got VF [%d:%d] cfc delete ramrod\\n\",\n\t\t   vf->abs_vfid, qidx);\n\t\tvfq_get(vf, qidx)->sp_obj.complete_cmd(bp,\n\t\t\t\t\t\t       &vfq_get(vf,\n\t\t\t\t\t\t\t\tqidx)->sp_obj,\n\t\t\t\t\t\t       BNX2X_Q_CMD_CFC_DEL);\n\t\tbreak;\n\tcase EVENT_RING_OPCODE_CLASSIFICATION_RULES:\n\t\tDP(BNX2X_MSG_IOV, \"got VF [%d:%d] set mac/vlan ramrod\\n\",\n\t\t   vf->abs_vfid, qidx);\n\t\tbnx2x_vf_handle_classification_eqe(bp, vfq_get(vf, qidx), elem);\n\t\tbreak;\n\tcase EVENT_RING_OPCODE_MULTICAST_RULES:\n\t\tDP(BNX2X_MSG_IOV, \"got VF [%d:%d] set mcast ramrod\\n\",\n\t\t   vf->abs_vfid, qidx);\n\t\tbnx2x_vf_handle_mcast_eqe(bp, vf);\n\t\tbreak;\n\tcase EVENT_RING_OPCODE_FILTERS_RULES:\n\t\tDP(BNX2X_MSG_IOV, \"got VF [%d:%d] set rx-mode ramrod\\n\",\n\t\t   vf->abs_vfid, qidx);\n\t\tbnx2x_vf_handle_filters_eqe(bp, vf);\n\t\tbreak;\n\tcase EVENT_RING_OPCODE_RSS_UPDATE_RULES:\n\t\tDP(BNX2X_MSG_IOV, \"got VF [%d:%d] RSS update ramrod\\n\",\n\t\t   vf->abs_vfid, qidx);\n\t\tbnx2x_vf_handle_rss_update_eqe(bp, vf);\n\t\tfallthrough;\n\tcase EVENT_RING_OPCODE_VF_FLR:\n\t\t \n\t\treturn 0;\n\tcase EVENT_RING_OPCODE_MALICIOUS_VF:\n\t\tvf->malicious = true;\n\t\treturn 0;\n\t}\n\n\treturn 0;\n}\n\nstatic struct bnx2x_virtf *bnx2x_vf_by_cid(struct bnx2x *bp, int vf_cid)\n{\n\t \n\tint abs_vfid = (vf_cid >> BNX2X_VF_CID_WND) & (BNX2X_MAX_NUM_OF_VFS-1);\n\treturn bnx2x_vf_by_abs_fid(bp, abs_vfid);\n}\n\nvoid bnx2x_iov_set_queue_sp_obj(struct bnx2x *bp, int vf_cid,\n\t\t\t\tstruct bnx2x_queue_sp_obj **q_obj)\n{\n\tstruct bnx2x_virtf *vf;\n\n\tif (!IS_SRIOV(bp))\n\t\treturn;\n\n\tvf = bnx2x_vf_by_cid(bp, vf_cid);\n\n\tif (vf) {\n\t\t \n\t\tint q_index = vf_cid & ((1 << BNX2X_VF_CID_WND)-1);\n\t\t*q_obj = &bnx2x_vfq(vf, q_index, sp_obj);\n\t} else {\n\t\tBNX2X_ERR(\"No vf matching cid %d\\n\", vf_cid);\n\t}\n}\n\nvoid bnx2x_iov_adjust_stats_req(struct bnx2x *bp)\n{\n\tint i;\n\tint first_queue_query_index, num_queues_req;\n\tstruct stats_query_entry *cur_query_entry;\n\tu8 stats_count = 0;\n\tbool is_fcoe = false;\n\n\tif (!IS_SRIOV(bp))\n\t\treturn;\n\n\tif (!NO_FCOE(bp))\n\t\tis_fcoe = true;\n\n\t \n\tnum_queues_req = BNX2X_NUM_ETH_QUEUES(bp) + is_fcoe;\n\tfirst_queue_query_index = BNX2X_FIRST_QUEUE_QUERY_IDX -\n\t\t(is_fcoe ? 0 : 1);\n\n\tDP_AND((BNX2X_MSG_IOV | BNX2X_MSG_STATS),\n\t       \"BNX2X_NUM_ETH_QUEUES %d, is_fcoe %d, first_queue_query_index %d => determined the last non virtual statistics query index is %d. Will add queries on top of that\\n\",\n\t       BNX2X_NUM_ETH_QUEUES(bp), is_fcoe, first_queue_query_index,\n\t       first_queue_query_index + num_queues_req);\n\n\tcur_query_entry = &bp->fw_stats_req->\n\t\tquery[first_queue_query_index + num_queues_req];\n\n\tfor_each_vf(bp, i) {\n\t\tint j;\n\t\tstruct bnx2x_virtf *vf = BP_VF(bp, i);\n\n\t\tif (vf->state != VF_ENABLED) {\n\t\t\tDP_AND((BNX2X_MSG_IOV | BNX2X_MSG_STATS),\n\t\t\t       \"vf %d not enabled so no stats for it\\n\",\n\t\t\t       vf->abs_vfid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (vf->malicious) {\n\t\t\tDP_AND((BNX2X_MSG_IOV | BNX2X_MSG_STATS),\n\t\t\t       \"vf %d malicious so no stats for it\\n\",\n\t\t\t       vf->abs_vfid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tDP_AND((BNX2X_MSG_IOV | BNX2X_MSG_STATS),\n\t\t       \"add addresses for vf %d\\n\", vf->abs_vfid);\n\t\tfor_each_vfq(vf, j) {\n\t\t\tstruct bnx2x_vf_queue *rxq = vfq_get(vf, j);\n\n\t\t\tdma_addr_t q_stats_addr =\n\t\t\t\tvf->fw_stat_map + j * vf->stats_stride;\n\n\t\t\t \n\t\t\tif (bnx2x_get_q_logical_state(bp, &rxq->sp_obj) ==\n\t\t\t    BNX2X_Q_LOGICAL_STATE_STOPPED)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tcur_query_entry->kind = STATS_TYPE_QUEUE;\n\t\t\tcur_query_entry->index = vfq_stat_id(vf, rxq);\n\t\t\tcur_query_entry->funcID =\n\t\t\t\tcpu_to_le16(FW_VF_HANDLE(vf->abs_vfid));\n\t\t\tcur_query_entry->address.hi =\n\t\t\t\tcpu_to_le32(U64_HI(q_stats_addr));\n\t\t\tcur_query_entry->address.lo =\n\t\t\t\tcpu_to_le32(U64_LO(q_stats_addr));\n\t\t\tDP_AND((BNX2X_MSG_IOV | BNX2X_MSG_STATS),\n\t\t\t       \"added address %x %x for vf %d queue %d client %d\\n\",\n\t\t\t       cur_query_entry->address.hi,\n\t\t\t       cur_query_entry->address.lo,\n\t\t\t       cur_query_entry->funcID,\n\t\t\t       j, cur_query_entry->index);\n\t\t\tcur_query_entry++;\n\t\t\tstats_count++;\n\n\t\t\t \n\t\t\tif (vf->cfg_flags & VF_CFG_STATS_COALESCE)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tbp->fw_stats_req->hdr.cmd_num = bp->fw_stats_num + stats_count;\n}\n\n \nstatic void bnx2x_vf_qtbl_set_q(struct bnx2x *bp, u8 abs_vfid, u8 qid,\n\t\t\t\tu8 enable)\n{\n\tu32 reg = PXP_REG_HST_ZONE_PERMISSION_TABLE + qid * 4;\n\tu32 val = enable ? (abs_vfid | (1 << 6)) : 0;\n\n\tREG_WR(bp, reg, val);\n}\n\nstatic void bnx2x_vf_clr_qtbl(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tint i;\n\n\tfor_each_vfq(vf, i)\n\t\tbnx2x_vf_qtbl_set_q(bp, vf->abs_vfid,\n\t\t\t\t    vfq_qzone_id(vf, vfq_get(vf, i)), false);\n}\n\nstatic void bnx2x_vf_igu_disable(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tu32 val;\n\n\t \n\tbnx2x_pretend_func(bp, HW_VF_HANDLE(bp, vf->abs_vfid));\n\tval = REG_RD(bp, IGU_REG_VF_CONFIGURATION);\n\tval &= ~(IGU_VF_CONF_MSI_MSIX_EN | IGU_VF_CONF_SINGLE_ISR_EN |\n\t\t IGU_VF_CONF_FUNC_EN | IGU_VF_CONF_PARENT_MASK);\n\tREG_WR(bp, IGU_REG_VF_CONFIGURATION, val);\n\tbnx2x_pretend_func(bp, BP_ABS_FUNC(bp));\n}\n\nu8 bnx2x_vf_max_queue_cnt(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\treturn min_t(u8, min_t(u8, vf_sb_count(vf), BNX2X_CIDS_PER_VF),\n\t\t     BNX2X_VF_MAX_QUEUES);\n}\n\nstatic\nint bnx2x_vf_chk_avail_resc(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t    struct vf_pf_resc_request *req_resc)\n{\n\tu8 rxq_cnt = vf_rxq_count(vf) ? : bnx2x_vf_max_queue_cnt(bp, vf);\n\tu8 txq_cnt = vf_txq_count(vf) ? : bnx2x_vf_max_queue_cnt(bp, vf);\n\n\treturn ((req_resc->num_rxqs <= rxq_cnt) &&\n\t\t(req_resc->num_txqs <= txq_cnt) &&\n\t\t(req_resc->num_sbs <= vf_sb_count(vf))   &&\n\t\t(req_resc->num_mac_filters <= vf_mac_rules_cnt(vf)) &&\n\t\t(req_resc->num_vlan_filters <= vf_vlan_rules_cnt(vf)));\n}\n\n \nint bnx2x_vf_acquire(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t     struct vf_pf_resc_request *resc)\n{\n\tint base_vf_cid = (BP_VFDB(bp)->sriov.first_vf_in_pf + vf->index) *\n\t\tBNX2X_CIDS_PER_VF;\n\n\tunion cdu_context *base_cxt = (union cdu_context *)\n\t\tBP_VF_CXT_PAGE(bp, base_vf_cid/ILT_PAGE_CIDS)->addr +\n\t\t(base_vf_cid & (ILT_PAGE_CIDS-1));\n\tint i;\n\n\t \n\tif (vf->state == VF_ACQUIRED) {\n\t\tDP(BNX2X_MSG_IOV, \"VF[%d] Trying to re-acquire resources (VF was not released or FLR'd)\\n\",\n\t\t   vf->abs_vfid);\n\n\t\tif (!bnx2x_vf_chk_avail_resc(bp, vf, resc)) {\n\t\t\tBNX2X_ERR(\"VF[%d] When re-acquiring resources, requested numbers must be <= then previously acquired numbers\\n\",\n\t\t\t\t  vf->abs_vfid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tif (vf->state != VF_FREE && vf->state != VF_RESET) {\n\t\tBNX2X_ERR(\"VF[%d] Can not acquire a VF with state %d\\n\",\n\t\t\t  vf->abs_vfid, vf->state);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!bnx2x_vf_chk_avail_resc(bp, vf, resc)) {\n\t\tDP(BNX2X_MSG_IOV,\n\t\t   \"cannot fulfill vf resource request. Placing maximal available values in response\\n\");\n\t\t \n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tvf_sb_count(vf) = resc->num_sbs;\n\tvf_rxq_count(vf) = resc->num_rxqs ? : bnx2x_vf_max_queue_cnt(bp, vf);\n\tvf_txq_count(vf) = resc->num_txqs ? : bnx2x_vf_max_queue_cnt(bp, vf);\n\n\tDP(BNX2X_MSG_IOV,\n\t   \"Fulfilling vf request: sb count %d, tx_count %d, rx_count %d, mac_rules_count %d, vlan_rules_count %d\\n\",\n\t   vf_sb_count(vf), vf_rxq_count(vf),\n\t   vf_txq_count(vf), vf_mac_rules_cnt(vf),\n\t   vf_vlan_rules_cnt(vf));\n\n\t \n\tif (!vf->vfqs) {\n\t\tDP(BNX2X_MSG_IOV, \"vf->vfqs was not allocated\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tfor_each_vfq(vf, i) {\n\t\tstruct bnx2x_vf_queue *q = vfq_get(vf, i);\n\n\t\tif (!q) {\n\t\t\tBNX2X_ERR(\"q number %d was not allocated\\n\", i);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tq->index = i;\n\t\tq->cxt = &((base_cxt + i)->eth);\n\t\tq->cid = BNX2X_FIRST_VF_CID + base_vf_cid + i;\n\n\t\tDP(BNX2X_MSG_IOV, \"VFQ[%d:%d]: index %d, cid 0x%x, cxt %p\\n\",\n\t\t   vf->abs_vfid, i, q->index, q->cid, q->cxt);\n\n\t\t \n\t\tbnx2x_vfq_init(bp, vf, q);\n\t}\n\tvf->state = VF_ACQUIRED;\n\treturn 0;\n}\n\nint bnx2x_vf_init(struct bnx2x *bp, struct bnx2x_virtf *vf, dma_addr_t *sb_map)\n{\n\tstruct bnx2x_func_init_params func_init = {0};\n\tint i;\n\n\t \n\tfor_each_vf_sb(vf, i)\n\t\tbnx2x_init_sb(bp, (dma_addr_t)sb_map[i], vf->abs_vfid, true,\n\t\t\t      vf_igu_sb(vf, i), vf_igu_sb(vf, i));\n\n\t \n\tif (vf->state != VF_ACQUIRED) {\n\t\tDP(BNX2X_MSG_IOV, \"VF[%d] is not in VF_ACQUIRED, but %d\\n\",\n\t\t   vf->abs_vfid, vf->state);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmsleep(100);\n\n\t \n\tif (bnx2x_vf_flr_clnup_epilog(bp, vf->abs_vfid))\n\t\treturn -EBUSY;\n\n\t \n\tREG_WR(bp, IGU_REG_STATISTIC_NUM_MESSAGE_SENT + vf->abs_vfid * 4 , 0);\n\n\t \n\tfunc_init.pf_id = BP_FUNC(bp);\n\tfunc_init.func_id = FW_VF_HANDLE(vf->abs_vfid);\n\tbnx2x_func_init(bp, &func_init);\n\n\t \n\tbnx2x_vf_enable_access(bp, vf->abs_vfid);\n\tbnx2x_vf_enable_traffic(bp, vf);\n\n\t \n\tfor_each_vfq(vf, i)\n\t\tbnx2x_vf_qtbl_set_q(bp, vf->abs_vfid,\n\t\t\t\t    vfq_qzone_id(vf, vfq_get(vf, i)), true);\n\n\tvf->state = VF_ENABLED;\n\n\t \n\tbnx2x_post_vf_bulletin(bp, vf->index);\n\n\treturn 0;\n}\n\nstruct set_vf_state_cookie {\n\tstruct bnx2x_virtf *vf;\n\tu8 state;\n};\n\nstatic void bnx2x_set_vf_state(void *cookie)\n{\n\tstruct set_vf_state_cookie *p = (struct set_vf_state_cookie *)cookie;\n\n\tp->vf->state = p->state;\n}\n\nint bnx2x_vf_close(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tint rc = 0, i;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d]\\n\", vf->abs_vfid);\n\n\t \n\tfor (i = 0; i < vf_rxq_count(vf); i++) {\n\t\trc = bnx2x_vf_queue_teardown(bp, vf, i);\n\t\tif (rc)\n\t\t\tgoto op_err;\n\t}\n\n\t \n\tDP(BNX2X_MSG_IOV, \"disabling igu\\n\");\n\tbnx2x_vf_igu_disable(bp, vf);\n\n\t \n\tDP(BNX2X_MSG_IOV, \"clearing qtbl\\n\");\n\tbnx2x_vf_clr_qtbl(bp, vf);\n\n\t \n\t{\n\t\tstruct set_vf_state_cookie cookie;\n\n\t\tcookie.vf = vf;\n\t\tcookie.state = VF_ACQUIRED;\n\t\trc = bnx2x_stats_safe_exec(bp, bnx2x_set_vf_state, &cookie);\n\t\tif (rc)\n\t\t\tgoto op_err;\n\t}\n\n\tDP(BNX2X_MSG_IOV, \"set state to acquired\\n\");\n\n\treturn 0;\nop_err:\n\tBNX2X_ERR(\"vf[%d] CLOSE error: rc %d\\n\", vf->abs_vfid, rc);\n\treturn rc;\n}\n\n \nint bnx2x_vf_free(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tint rc;\n\n\tDP(BNX2X_MSG_IOV, \"VF[%d] STATE: %s\\n\", vf->abs_vfid,\n\t   vf->state == VF_FREE ? \"Free\" :\n\t   vf->state == VF_ACQUIRED ? \"Acquired\" :\n\t   vf->state == VF_ENABLED ? \"Enabled\" :\n\t   vf->state == VF_RESET ? \"Reset\" :\n\t   \"Unknown\");\n\n\tswitch (vf->state) {\n\tcase VF_ENABLED:\n\t\trc = bnx2x_vf_close(bp, vf);\n\t\tif (rc)\n\t\t\tgoto op_err;\n\t\tfallthrough;\t \n\tcase VF_ACQUIRED:\n\t\tDP(BNX2X_MSG_IOV, \"about to free resources\\n\");\n\t\tbnx2x_vf_free_resc(bp, vf);\n\t\tbreak;\n\n\tcase VF_FREE:\n\tcase VF_RESET:\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\nop_err:\n\tBNX2X_ERR(\"VF[%d] RELEASE error: rc %d\\n\", vf->abs_vfid, rc);\n\treturn rc;\n}\n\nint bnx2x_vf_rss_update(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\tstruct bnx2x_config_rss_params *rss)\n{\n\tDP(BNX2X_MSG_IOV, \"vf[%d]\\n\", vf->abs_vfid);\n\tset_bit(RAMROD_COMP_WAIT, &rss->ramrod_flags);\n\treturn bnx2x_config_rss(bp, rss);\n}\n\nint bnx2x_vf_tpa_update(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\tstruct vfpf_tpa_tlv *tlv,\n\t\t\tstruct bnx2x_queue_update_tpa_params *params)\n{\n\taligned_u64 *sge_addr = tlv->tpa_client_info.sge_addr;\n\tstruct bnx2x_queue_state_params qstate;\n\tint qid, rc = 0;\n\n\tDP(BNX2X_MSG_IOV, \"vf[%d]\\n\", vf->abs_vfid);\n\n\t \n\tmemset(&qstate, 0, sizeof(struct bnx2x_queue_state_params));\n\tmemcpy(&qstate.params.update_tpa, params,\n\t       sizeof(struct bnx2x_queue_update_tpa_params));\n\tqstate.cmd = BNX2X_Q_CMD_UPDATE_TPA;\n\tset_bit(RAMROD_COMP_WAIT, &qstate.ramrod_flags);\n\n\tfor (qid = 0; qid < vf_rxq_count(vf); qid++) {\n\t\tqstate.q_obj = &bnx2x_vfq(vf, qid, sp_obj);\n\t\tqstate.params.update_tpa.sge_map = sge_addr[qid];\n\t\tDP(BNX2X_MSG_IOV, \"sge_addr[%d:%d] %08x:%08x\\n\",\n\t\t   vf->abs_vfid, qid, U64_HI(sge_addr[qid]),\n\t\t   U64_LO(sge_addr[qid]));\n\t\trc = bnx2x_queue_state_change(bp, &qstate);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"Failed to configure sge_addr %08x:%08x for [%d:%d]\\n\",\n\t\t\t\t  U64_HI(sge_addr[qid]), U64_LO(sge_addr[qid]),\n\t\t\t\t  vf->abs_vfid, qid);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\n \nint bnx2x_vf_release(struct bnx2x *bp, struct bnx2x_virtf *vf)\n{\n\tint rc;\n\n\tDP(BNX2X_MSG_IOV, \"PF releasing vf %d\\n\", vf->abs_vfid);\n\tbnx2x_lock_vf_pf_channel(bp, vf, CHANNEL_TLV_PF_RELEASE_VF);\n\n\trc = bnx2x_vf_free(bp, vf);\n\tif (rc)\n\t\tWARN(rc,\n\t\t     \"VF[%d] Failed to allocate resources for release op- rc=%d\\n\",\n\t\t     vf->abs_vfid, rc);\n\tbnx2x_unlock_vf_pf_channel(bp, vf, CHANNEL_TLV_PF_RELEASE_VF);\n\treturn rc;\n}\n\nvoid bnx2x_lock_vf_pf_channel(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t      enum channel_tlvs tlv)\n{\n\t \n\tif (!bnx2x_tlv_supported(tlv)) {\n\t\tBNX2X_ERR(\"attempting to lock with unsupported tlv. Aborting\\n\");\n\t\treturn;\n\t}\n\n\t \n\tmutex_lock(&vf->op_mutex);\n\n\t \n\tvf->op_current = tlv;\n\n\t \n\tDP(BNX2X_MSG_IOV, \"VF[%d]: vf pf channel locked by %d\\n\",\n\t   vf->abs_vfid, tlv);\n}\n\nvoid bnx2x_unlock_vf_pf_channel(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t\tenum channel_tlvs expected_tlv)\n{\n\tenum channel_tlvs current_tlv;\n\n\tif (!vf) {\n\t\tBNX2X_ERR(\"VF was %p\\n\", vf);\n\t\treturn;\n\t}\n\n\tcurrent_tlv = vf->op_current;\n\n\t \n\tif (!bnx2x_tlv_supported(expected_tlv))\n\t\treturn;\n\n\tWARN(expected_tlv != vf->op_current,\n\t     \"lock mismatch: expected %d found %d\", expected_tlv,\n\t     vf->op_current);\n\n\t \n\tvf->op_current = CHANNEL_TLV_NONE;\n\n\t \n\tmutex_unlock(&vf->op_mutex);\n\n\t \n\tDP(BNX2X_MSG_IOV, \"VF[%d]: vf pf channel unlocked by %d\\n\",\n\t   vf->abs_vfid, current_tlv);\n}\n\nstatic int bnx2x_set_pf_tx_switching(struct bnx2x *bp, bool enable)\n{\n\tstruct bnx2x_queue_state_params q_params;\n\tu32 prev_flags;\n\tint i, rc;\n\n\t \n\tprev_flags = bp->flags;\n\tif (enable)\n\t\tbp->flags |= TX_SWITCHING;\n\telse\n\t\tbp->flags &= ~TX_SWITCHING;\n\tif (prev_flags == bp->flags)\n\t\treturn 0;\n\n\t \n\tif ((bp->state != BNX2X_STATE_OPEN) ||\n\t    (bnx2x_get_q_logical_state(bp,\n\t\t\t\t      &bnx2x_sp_obj(bp, &bp->fp[0]).q_obj) !=\n\t     BNX2X_Q_LOGICAL_STATE_ACTIVE))\n\t\treturn 0;\n\n\t \n\tmemset(&q_params, 0, sizeof(q_params));\n\t__set_bit(RAMROD_COMP_WAIT, &q_params.ramrod_flags);\n\tq_params.cmd = BNX2X_Q_CMD_UPDATE;\n\t__set_bit(BNX2X_Q_UPDATE_TX_SWITCHING_CHNG,\n\t\t  &q_params.params.update.update_flags);\n\tif (enable)\n\t\t__set_bit(BNX2X_Q_UPDATE_TX_SWITCHING,\n\t\t\t  &q_params.params.update.update_flags);\n\telse\n\t\t__clear_bit(BNX2X_Q_UPDATE_TX_SWITCHING,\n\t\t\t    &q_params.params.update.update_flags);\n\n\t \n\tfor_each_eth_queue(bp, i) {\n\t\tstruct bnx2x_fastpath *fp = &bp->fp[i];\n\t\tint tx_idx;\n\n\t\t \n\t\tq_params.q_obj = &bnx2x_sp_obj(bp, fp).q_obj;\n\n\t\tfor (tx_idx = FIRST_TX_COS_INDEX;\n\t\t     tx_idx < fp->max_cos; tx_idx++) {\n\t\t\tq_params.params.update.cid_index = tx_idx;\n\n\t\t\t \n\t\t\trc = bnx2x_queue_state_change(bp, &q_params);\n\t\t\tif (rc) {\n\t\t\t\tBNX2X_ERR(\"Failed to configure Tx switching\\n\");\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t}\n\t}\n\n\tDP(BNX2X_MSG_IOV, \"%s Tx Switching\\n\", enable ? \"Enabled\" : \"Disabled\");\n\treturn 0;\n}\n\nint bnx2x_sriov_configure(struct pci_dev *dev, int num_vfs_param)\n{\n\tstruct bnx2x *bp = netdev_priv(pci_get_drvdata(dev));\n\n\tif (!IS_SRIOV(bp)) {\n\t\tBNX2X_ERR(\"failed to configure SR-IOV since vfdb was not allocated. Check dmesg for errors in probe stage\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tDP(BNX2X_MSG_IOV, \"bnx2x_sriov_configure called with %d, BNX2X_NR_VIRTFN(bp) was %d\\n\",\n\t   num_vfs_param, BNX2X_NR_VIRTFN(bp));\n\n\t \n\tif (bp->state != BNX2X_STATE_OPEN) {\n\t\tBNX2X_ERR(\"VF num configuration via sysfs not supported while PF is down\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (num_vfs_param > BNX2X_NR_VIRTFN(bp)) {\n\t\tBNX2X_ERR(\"truncating requested number of VFs (%d) down to maximum allowed (%d)\\n\",\n\t\t\t  num_vfs_param, BNX2X_NR_VIRTFN(bp));\n\t\tnum_vfs_param = BNX2X_NR_VIRTFN(bp);\n\t}\n\n\tbp->requested_nr_virtfn = num_vfs_param;\n\tif (num_vfs_param == 0) {\n\t\tbnx2x_set_pf_tx_switching(bp, false);\n\t\tbnx2x_disable_sriov(bp);\n\t\treturn 0;\n\t} else {\n\t\treturn bnx2x_enable_sriov(bp);\n\t}\n}\n\n#define IGU_ENTRY_SIZE 4\n\nint bnx2x_enable_sriov(struct bnx2x *bp)\n{\n\tint rc = 0, req_vfs = bp->requested_nr_virtfn;\n\tint vf_idx, sb_idx, vfq_idx, qcount, first_vf;\n\tu32 igu_entry, address;\n\tu16 num_vf_queues;\n\n\tif (req_vfs == 0)\n\t\treturn 0;\n\n\tfirst_vf = bp->vfdb->sriov.first_vf_in_pf;\n\n\t \n\tnum_vf_queues = min_t(u16, BNX2X_VF_MAX_QUEUES,\n\t\t\t      BP_VFDB(bp)->vf_sbs_pool / req_vfs);\n\n\t \n\tfor (vf_idx = 0; vf_idx < req_vfs; vf_idx++) {\n\t\tstruct bnx2x_virtf *vf = BP_VF(bp, vf_idx);\n\n\t\tvf->sb_count = 0;\n\t\tvf_sb_count(BP_VF(bp, vf_idx)) = 0;\n\t}\n\tbp->vfdb->vf_sbs_pool = 0;\n\n\t \n\tsb_idx = BP_VFDB(bp)->first_vf_igu_entry;\n\taddress = IGU_REG_MAPPING_MEMORY + sb_idx * IGU_ENTRY_SIZE;\n\tfor (vf_idx = first_vf; vf_idx < first_vf + req_vfs; vf_idx++) {\n\t\tfor (vfq_idx = 0; vfq_idx < num_vf_queues; vfq_idx++) {\n\t\t\tigu_entry = vf_idx << IGU_REG_MAPPING_MEMORY_FID_SHIFT |\n\t\t\t\tvfq_idx << IGU_REG_MAPPING_MEMORY_VECTOR_SHIFT |\n\t\t\t\tIGU_REG_MAPPING_MEMORY_VALID;\n\t\t\tDP(BNX2X_MSG_IOV, \"assigning sb %d to vf %d\\n\",\n\t\t\t   sb_idx, vf_idx);\n\t\t\tREG_WR(bp, address, igu_entry);\n\t\t\tsb_idx++;\n\t\t\taddress += IGU_ENTRY_SIZE;\n\t\t}\n\t}\n\n\t \n\tbnx2x_get_vf_igu_cam_info(bp);\n\n\tDP(BNX2X_MSG_IOV, \"vf_sbs_pool %d, num_vf_queues %d\\n\",\n\t   BP_VFDB(bp)->vf_sbs_pool, num_vf_queues);\n\n\tqcount = 0;\n\tfor_each_vf(bp, vf_idx) {\n\t\tstruct bnx2x_virtf *vf = BP_VF(bp, vf_idx);\n\n\t\t \n\t\tvf->vfqs = &bp->vfdb->vfqs[qcount];\n\t\tqcount += vf_sb_count(vf);\n\t\tbnx2x_iov_static_resc(bp, vf);\n\t}\n\n\t \n\tfor (vf_idx = first_vf; vf_idx < first_vf + req_vfs; vf_idx++) {\n\t\tbnx2x_pretend_func(bp, HW_VF_HANDLE(bp, vf_idx));\n\t\tREG_WR(bp, PCICFG_OFFSET + GRC_CONFIG_REG_VF_MSIX_CONTROL,\n\t\t       num_vf_queues - 1);\n\t\tDP(BNX2X_MSG_IOV, \"set msix vec num in VF %d cfg space to %d\\n\",\n\t\t   vf_idx, num_vf_queues - 1);\n\t}\n\tbnx2x_pretend_func(bp, BP_ABS_FUNC(bp));\n\n\t \n\tDP(BNX2X_MSG_IOV, \"about to call enable sriov\\n\");\n\tbnx2x_disable_sriov(bp);\n\n\trc = bnx2x_set_pf_tx_switching(bp, true);\n\tif (rc)\n\t\treturn rc;\n\n\trc = pci_enable_sriov(bp->pdev, req_vfs);\n\tif (rc) {\n\t\tBNX2X_ERR(\"pci_enable_sriov failed with %d\\n\", rc);\n\t\treturn rc;\n\t}\n\tDP(BNX2X_MSG_IOV, \"sriov enabled (%d vfs)\\n\", req_vfs);\n\treturn req_vfs;\n}\n\nvoid bnx2x_pf_set_vfs_vlan(struct bnx2x *bp)\n{\n\tint vfidx;\n\tstruct pf_vf_bulletin_content *bulletin;\n\n\tDP(BNX2X_MSG_IOV, \"configuring vlan for VFs from sp-task\\n\");\n\tfor_each_vf(bp, vfidx) {\n\t\tbulletin = BP_VF_BULLETIN(bp, vfidx);\n\t\tif (bulletin->valid_bitmap & (1 << VLAN_VALID))\n\t\t\tbnx2x_set_vf_vlan(bp->dev, vfidx, bulletin->vlan, 0,\n\t\t\t\t\t  htons(ETH_P_8021Q));\n\t}\n}\n\nvoid bnx2x_disable_sriov(struct bnx2x *bp)\n{\n\tif (pci_vfs_assigned(bp->pdev)) {\n\t\tDP(BNX2X_MSG_IOV,\n\t\t   \"Unloading driver while VFs are assigned - VFs will not be deallocated\\n\");\n\t\treturn;\n\t}\n\n\tpci_disable_sriov(bp->pdev);\n}\n\nstatic int bnx2x_vf_op_prep(struct bnx2x *bp, int vfidx,\n\t\t\t    struct bnx2x_virtf **vf,\n\t\t\t    struct pf_vf_bulletin_content **bulletin,\n\t\t\t    bool test_queue)\n{\n\tif (bp->state != BNX2X_STATE_OPEN) {\n\t\tBNX2X_ERR(\"PF is down - can't utilize iov-related functionality\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!IS_SRIOV(bp)) {\n\t\tBNX2X_ERR(\"sriov is disabled - can't utilize iov-related functionality\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (vfidx >= BNX2X_NR_VIRTFN(bp)) {\n\t\tBNX2X_ERR(\"VF is uninitialized - can't utilize iov-related functionality. vfidx was %d BNX2X_NR_VIRTFN was %d\\n\",\n\t\t\t  vfidx, BNX2X_NR_VIRTFN(bp));\n\t\treturn -EINVAL;\n\t}\n\n\t \n\t*vf = BP_VF(bp, vfidx);\n\t*bulletin = BP_VF_BULLETIN(bp, vfidx);\n\n\tif (!*vf) {\n\t\tBNX2X_ERR(\"Unable to get VF structure for vfidx %d\\n\", vfidx);\n\t\treturn -EINVAL;\n\t}\n\n\tif (test_queue && !(*vf)->vfqs) {\n\t\tBNX2X_ERR(\"vfqs struct is null. Was this invoked before dynamically enabling SR-IOV? vfidx was %d\\n\",\n\t\t\t  vfidx);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!*bulletin) {\n\t\tBNX2X_ERR(\"Bulletin Board struct is null for vfidx %d\\n\",\n\t\t\t  vfidx);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint bnx2x_get_vf_config(struct net_device *dev, int vfidx,\n\t\t\tstruct ifla_vf_info *ivi)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\tstruct bnx2x_virtf *vf = NULL;\n\tstruct pf_vf_bulletin_content *bulletin = NULL;\n\tstruct bnx2x_vlan_mac_obj *mac_obj;\n\tstruct bnx2x_vlan_mac_obj *vlan_obj;\n\tint rc;\n\n\t \n\trc = bnx2x_vf_op_prep(bp, vfidx, &vf, &bulletin, true);\n\tif (rc)\n\t\treturn rc;\n\n\tmac_obj = &bnx2x_leading_vfq(vf, mac_obj);\n\tvlan_obj = &bnx2x_leading_vfq(vf, vlan_obj);\n\tif (!mac_obj || !vlan_obj) {\n\t\tBNX2X_ERR(\"VF partially initialized\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tivi->vf = vfidx;\n\tivi->qos = 0;\n\tivi->max_tx_rate = 10000;  \n\tivi->min_tx_rate = 0;\n\tivi->spoofchk = vf->spoofchk ? 1 : 0;\n\tivi->linkstate = vf->link_cfg;\n\tif (vf->state == VF_ENABLED) {\n\t\t \n\t\tif (bnx2x_validate_vf_sp_objs(bp, vf, false)) {\n\t\t\tmac_obj->get_n_elements(bp, mac_obj, 1, (u8 *)&ivi->mac,\n\t\t\t\t\t\t0, ETH_ALEN);\n\t\t\tvlan_obj->get_n_elements(bp, vlan_obj, 1,\n\t\t\t\t\t\t (u8 *)&ivi->vlan, 0,\n\t\t\t\t\t\t VLAN_HLEN);\n\t\t}\n\t} else {\n\t\tmutex_lock(&bp->vfdb->bulletin_mutex);\n\t\t \n\t\tif (bulletin->valid_bitmap & (1 << MAC_ADDR_VALID))\n\t\t\t \n\t\t\tmemcpy(&ivi->mac, bulletin->mac, ETH_ALEN);\n\t\telse\n\t\t\t \n\t\t\teth_zero_addr(ivi->mac);\n\n\t\t \n\t\tif (bulletin->valid_bitmap & (1 << VLAN_VALID))\n\t\t\t \n\t\t\tmemcpy(&ivi->vlan, &bulletin->vlan, VLAN_HLEN);\n\t\telse\n\t\t\t \n\t\t\tmemset(&ivi->vlan, 0, VLAN_HLEN);\n\n\t\tmutex_unlock(&bp->vfdb->bulletin_mutex);\n\t}\n\n\treturn 0;\n}\n\n \nint bnx2x_set_vf_mac(struct net_device *dev, int vfidx, u8 *mac)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\tint rc, q_logical_state;\n\tstruct bnx2x_virtf *vf = NULL;\n\tstruct pf_vf_bulletin_content *bulletin = NULL;\n\n\tif (!is_valid_ether_addr(mac)) {\n\t\tBNX2X_ERR(\"mac address invalid\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\trc = bnx2x_vf_op_prep(bp, vfidx, &vf, &bulletin, true);\n\tif (rc)\n\t\treturn rc;\n\n\tmutex_lock(&bp->vfdb->bulletin_mutex);\n\n\t \n\tbulletin->valid_bitmap |= 1 << MAC_ADDR_VALID;\n\tmemcpy(bulletin->mac, mac, ETH_ALEN);\n\n\t \n\trc = bnx2x_post_vf_bulletin(bp, vfidx);\n\n\t \n\tmutex_unlock(&bp->vfdb->bulletin_mutex);\n\n\tif (rc) {\n\t\tBNX2X_ERR(\"failed to update VF[%d] bulletin\\n\", vfidx);\n\t\treturn rc;\n\t}\n\n\tq_logical_state =\n\t\tbnx2x_get_q_logical_state(bp, &bnx2x_leading_vfq(vf, sp_obj));\n\tif (vf->state == VF_ENABLED &&\n\t    q_logical_state == BNX2X_Q_LOGICAL_STATE_ACTIVE) {\n\t\t \n\t\tunsigned long ramrod_flags = 0;\n\t\tstruct bnx2x_vlan_mac_obj *mac_obj;\n\n\t\t \n\t\tif (!bnx2x_validate_vf_sp_objs(bp, vf, true))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tbnx2x_lock_vf_pf_channel(bp, vf, CHANNEL_TLV_PF_SET_MAC);\n\n\t\t \n\t\tmac_obj = &bnx2x_leading_vfq(vf, mac_obj);\n\t\trc = bnx2x_del_all_macs(bp, mac_obj, BNX2X_ETH_MAC, true);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"failed to delete eth macs\\n\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\trc = bnx2x_del_all_macs(bp, mac_obj, BNX2X_UC_LIST_MAC, true);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"failed to delete uc_list macs\\n\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\t__set_bit(RAMROD_COMP_WAIT, &ramrod_flags);\n\t\tbnx2x_set_mac_one(bp, (u8 *)&bulletin->mac, mac_obj, true,\n\t\t\t\t  BNX2X_ETH_MAC, &ramrod_flags);\n\nout:\n\t\tbnx2x_unlock_vf_pf_channel(bp, vf, CHANNEL_TLV_PF_SET_MAC);\n\t}\n\n\treturn rc;\n}\n\nstatic void bnx2x_set_vf_vlan_acceptance(struct bnx2x *bp,\n\t\t\t\t\t struct bnx2x_virtf *vf, bool accept)\n{\n\tstruct bnx2x_rx_mode_ramrod_params rx_ramrod;\n\tunsigned long accept_flags;\n\n\t \n\taccept_flags = bnx2x_leading_vfq(vf, accept_flags);\n\tif (accept)\n\t\tset_bit(BNX2X_ACCEPT_ANY_VLAN, &accept_flags);\n\telse\n\t\tclear_bit(BNX2X_ACCEPT_ANY_VLAN, &accept_flags);\n\n\tbnx2x_vf_prep_rx_mode(bp, LEADING_IDX, &rx_ramrod, vf,\n\t\t\t      accept_flags);\n\tbnx2x_leading_vfq(vf, accept_flags) = accept_flags;\n\tbnx2x_config_rx_mode(bp, &rx_ramrod);\n}\n\nstatic int bnx2x_set_vf_vlan_filter(struct bnx2x *bp, struct bnx2x_virtf *vf,\n\t\t\t\t    u16 vlan, bool add)\n{\n\tstruct bnx2x_vlan_mac_ramrod_params ramrod_param;\n\tunsigned long ramrod_flags = 0;\n\tint rc = 0;\n\n\t \n\tmemset(&ramrod_param, 0, sizeof(ramrod_param));\n\t__set_bit(RAMROD_COMP_WAIT, &ramrod_flags);\n\tramrod_param.vlan_mac_obj = &bnx2x_leading_vfq(vf, vlan_obj);\n\tramrod_param.ramrod_flags = ramrod_flags;\n\tramrod_param.user_req.u.vlan.vlan = vlan;\n\tramrod_param.user_req.cmd = add ? BNX2X_VLAN_MAC_ADD\n\t\t\t\t\t: BNX2X_VLAN_MAC_DEL;\n\trc = bnx2x_config_vlan_mac(bp, &ramrod_param);\n\tif (rc) {\n\t\tBNX2X_ERR(\"failed to configure vlan\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint bnx2x_set_vf_vlan(struct net_device *dev, int vfidx, u16 vlan, u8 qos,\n\t\t      __be16 vlan_proto)\n{\n\tstruct pf_vf_bulletin_content *bulletin = NULL;\n\tstruct bnx2x *bp = netdev_priv(dev);\n\tstruct bnx2x_vlan_mac_obj *vlan_obj;\n\tunsigned long vlan_mac_flags = 0;\n\tunsigned long ramrod_flags = 0;\n\tstruct bnx2x_virtf *vf = NULL;\n\tint i, rc;\n\n\tif (vlan > 4095) {\n\t\tBNX2X_ERR(\"illegal vlan value %d\\n\", vlan);\n\t\treturn -EINVAL;\n\t}\n\n\tif (vlan_proto != htons(ETH_P_8021Q))\n\t\treturn -EPROTONOSUPPORT;\n\n\tDP(BNX2X_MSG_IOV, \"configuring VF %d with VLAN %d qos %d\\n\",\n\t   vfidx, vlan, 0);\n\n\t \n\trc = bnx2x_vf_op_prep(bp, vfidx, &vf, &bulletin, true);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tmutex_lock(&bp->vfdb->bulletin_mutex);\n\n\tif (vlan > 0)\n\t\tbulletin->valid_bitmap |= 1 << VLAN_VALID;\n\telse\n\t\tbulletin->valid_bitmap &= ~(1 << VLAN_VALID);\n\tbulletin->vlan = vlan;\n\n\t \n\trc = bnx2x_post_vf_bulletin(bp, vfidx);\n\tif (rc)\n\t\tBNX2X_ERR(\"failed to update VF[%d] bulletin\\n\", vfidx);\n\tmutex_unlock(&bp->vfdb->bulletin_mutex);\n\n\t \n\tif (vf->state != VF_ENABLED ||\n\t    bnx2x_get_q_logical_state(bp, &bnx2x_leading_vfq(vf, sp_obj)) !=\n\t    BNX2X_Q_LOGICAL_STATE_ACTIVE)\n\t\treturn rc;\n\n\t \n\tif (!bnx2x_validate_vf_sp_objs(bp, vf, true))\n\t\treturn -EINVAL;\n\n\t \n\tbnx2x_lock_vf_pf_channel(bp, vf, CHANNEL_TLV_PF_SET_VLAN);\n\n\t \n\t__set_bit(RAMROD_COMP_WAIT, &ramrod_flags);\n\tvlan_obj = &bnx2x_leading_vfq(vf, vlan_obj);\n\trc = vlan_obj->delete_all(bp, vlan_obj, &vlan_mac_flags,\n\t\t\t\t  &ramrod_flags);\n\tif (rc) {\n\t\tBNX2X_ERR(\"failed to delete vlans\\n\");\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (vlan || !(vf->cfg_flags & VF_CFG_VLAN_FILTER))\n\t\tbnx2x_set_vf_vlan_acceptance(bp, vf, !vlan);\n\n\trc = bnx2x_set_vf_vlan_filter(bp, vf, vlan, true);\n\tif (rc)\n\t\tgoto out;\n\n\t \n\tfor_each_vfq(vf, i) {\n\t\tstruct bnx2x_queue_state_params q_params = {NULL};\n\t\tstruct bnx2x_queue_update_params *update_params;\n\n\t\tq_params.q_obj = &bnx2x_vfq(vf, i, sp_obj);\n\n\t\t \n\t\tif (bnx2x_get_q_logical_state(bp, q_params.q_obj) !=\n\t\t    BNX2X_Q_LOGICAL_STATE_ACTIVE)\n\t\t\tcontinue;\n\n\t\t__set_bit(RAMROD_COMP_WAIT, &q_params.ramrod_flags);\n\t\tq_params.cmd = BNX2X_Q_CMD_UPDATE;\n\t\tupdate_params = &q_params.params.update;\n\t\t__set_bit(BNX2X_Q_UPDATE_DEF_VLAN_EN_CHNG,\n\t\t\t  &update_params->update_flags);\n\t\t__set_bit(BNX2X_Q_UPDATE_SILENT_VLAN_REM_CHNG,\n\t\t\t  &update_params->update_flags);\n\t\tif (vlan == 0) {\n\t\t\t \n\t\t\t__clear_bit(BNX2X_Q_UPDATE_DEF_VLAN_EN,\n\t\t\t\t    &update_params->update_flags);\n\t\t\t__clear_bit(BNX2X_Q_UPDATE_SILENT_VLAN_REM,\n\t\t\t\t    &update_params->update_flags);\n\t\t} else {\n\t\t\t \n\t\t\t__set_bit(BNX2X_Q_UPDATE_DEF_VLAN_EN,\n\t\t\t\t  &update_params->update_flags);\n\t\t\t__set_bit(BNX2X_Q_UPDATE_SILENT_VLAN_REM,\n\t\t\t\t  &update_params->update_flags);\n\t\t\tupdate_params->def_vlan = vlan;\n\t\t\tupdate_params->silent_removal_value =\n\t\t\t\tvlan & VLAN_VID_MASK;\n\t\t\tupdate_params->silent_removal_mask = VLAN_VID_MASK;\n\t\t}\n\n\t\t \n\t\trc = bnx2x_queue_state_change(bp, &q_params);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"Failed to configure default VLAN queue %d\\n\",\n\t\t\t\t  i);\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tbnx2x_unlock_vf_pf_channel(bp, vf, CHANNEL_TLV_PF_SET_VLAN);\n\n\tif (rc)\n\t\tDP(BNX2X_MSG_IOV,\n\t\t   \"updated VF[%d] vlan configuration (vlan = %d)\\n\",\n\t\t   vfidx, vlan);\n\n\treturn rc;\n}\n\nint bnx2x_set_vf_spoofchk(struct net_device *dev, int idx, bool val)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\tstruct bnx2x_virtf *vf;\n\tint i, rc = 0;\n\n\tvf = BP_VF(bp, idx);\n\tif (!vf)\n\t\treturn -EINVAL;\n\n\t \n\tif (vf->spoofchk == val)\n\t\treturn 0;\n\n\tvf->spoofchk = val ? 1 : 0;\n\n\tDP(BNX2X_MSG_IOV, \"%s spoofchk for VF %d\\n\",\n\t   val ? \"enabling\" : \"disabling\", idx);\n\n\t \n\tif (vf->state != VF_ENABLED ||\n\t    bnx2x_get_q_logical_state(bp, &bnx2x_leading_vfq(vf, sp_obj)) !=\n\t    BNX2X_Q_LOGICAL_STATE_ACTIVE)\n\t\treturn rc;\n\n\t \n\tif (!bnx2x_validate_vf_sp_objs(bp, vf, true))\n\t\treturn -EINVAL;\n\n\t \n\tfor_each_vfq(vf, i) {\n\t\tstruct bnx2x_queue_state_params q_params = {NULL};\n\t\tstruct bnx2x_queue_update_params *update_params;\n\n\t\tq_params.q_obj = &bnx2x_vfq(vf, i, sp_obj);\n\n\t\t \n\t\tif (bnx2x_get_q_logical_state(bp, q_params.q_obj) !=\n\t\t    BNX2X_Q_LOGICAL_STATE_ACTIVE)\n\t\t\tcontinue;\n\n\t\t__set_bit(RAMROD_COMP_WAIT, &q_params.ramrod_flags);\n\t\tq_params.cmd = BNX2X_Q_CMD_UPDATE;\n\t\tupdate_params = &q_params.params.update;\n\t\t__set_bit(BNX2X_Q_UPDATE_ANTI_SPOOF_CHNG,\n\t\t\t  &update_params->update_flags);\n\t\tif (val) {\n\t\t\t__set_bit(BNX2X_Q_UPDATE_ANTI_SPOOF,\n\t\t\t\t  &update_params->update_flags);\n\t\t} else {\n\t\t\t__clear_bit(BNX2X_Q_UPDATE_ANTI_SPOOF,\n\t\t\t\t    &update_params->update_flags);\n\t\t}\n\n\t\t \n\t\trc = bnx2x_queue_state_change(bp, &q_params);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"Failed to %s spoofchk on VF %d - vfq %d\\n\",\n\t\t\t\t  val ? \"enable\" : \"disable\", idx, i);\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tif (!rc)\n\t\tDP(BNX2X_MSG_IOV,\n\t\t   \"%s spoofchk for VF[%d]\\n\", val ? \"Enabled\" : \"Disabled\",\n\t\t   idx);\n\n\treturn rc;\n}\n\n \nu32 bnx2x_crc_vf_bulletin(struct pf_vf_bulletin_content *bulletin)\n{\n\treturn crc32(BULLETIN_CRC_SEED,\n\t\t ((u8 *)bulletin) + sizeof(bulletin->crc),\n\t\t bulletin->length - sizeof(bulletin->crc));\n}\n\n \nenum sample_bulletin_result bnx2x_sample_bulletin(struct bnx2x *bp)\n{\n\tstruct pf_vf_bulletin_content *bulletin;\n\tint attempts;\n\n\t \n\tfor (attempts = 0; attempts < BULLETIN_ATTEMPTS; attempts++) {\n\t\tu32 crc;\n\n\t\t \n\t\tmemcpy(&bp->shadow_bulletin, bp->pf2vf_bulletin,\n\t\t       sizeof(union pf_vf_bulletin));\n\n\t\tcrc = bnx2x_crc_vf_bulletin(&bp->shadow_bulletin.content);\n\n\t\tif (bp->shadow_bulletin.content.crc == crc)\n\t\t\tbreak;\n\n\t\tBNX2X_ERR(\"bad crc on bulletin board. Contained %x computed %x\\n\",\n\t\t\t  bp->shadow_bulletin.content.crc, crc);\n\t}\n\n\tif (attempts >= BULLETIN_ATTEMPTS) {\n\t\tBNX2X_ERR(\"pf to vf bulletin board crc was wrong %d consecutive times. Aborting\\n\",\n\t\t\t  attempts);\n\t\treturn PFVF_BULLETIN_CRC_ERR;\n\t}\n\tbulletin = &bp->shadow_bulletin.content;\n\n\t \n\tif (bp->old_bulletin.version == bulletin->version)\n\t\treturn PFVF_BULLETIN_UNCHANGED;\n\n\t \n\tif (bulletin->valid_bitmap & 1 << MAC_ADDR_VALID &&\n\t    !ether_addr_equal(bulletin->mac, bp->old_bulletin.mac)) {\n\t\t \n\t\teth_hw_addr_set(bp->dev, bulletin->mac);\n\t}\n\n\tif (bulletin->valid_bitmap & (1 << LINK_VALID)) {\n\t\tDP(BNX2X_MSG_IOV, \"link update speed %d flags %x\\n\",\n\t\t   bulletin->link_speed, bulletin->link_flags);\n\n\t\tbp->vf_link_vars.line_speed = bulletin->link_speed;\n\t\tbp->vf_link_vars.link_report_flags = 0;\n\t\t \n\t\tif (bulletin->link_flags & VFPF_LINK_REPORT_LINK_DOWN)\n\t\t\t__set_bit(BNX2X_LINK_REPORT_LINK_DOWN,\n\t\t\t\t  &bp->vf_link_vars.link_report_flags);\n\t\t \n\t\tif (bulletin->link_flags & VFPF_LINK_REPORT_FULL_DUPLEX)\n\t\t\t__set_bit(BNX2X_LINK_REPORT_FD,\n\t\t\t\t  &bp->vf_link_vars.link_report_flags);\n\t\t \n\t\tif (bulletin->link_flags & VFPF_LINK_REPORT_RX_FC_ON)\n\t\t\t__set_bit(BNX2X_LINK_REPORT_RX_FC_ON,\n\t\t\t\t  &bp->vf_link_vars.link_report_flags);\n\t\t \n\t\tif (bulletin->link_flags & VFPF_LINK_REPORT_TX_FC_ON)\n\t\t\t__set_bit(BNX2X_LINK_REPORT_TX_FC_ON,\n\t\t\t\t  &bp->vf_link_vars.link_report_flags);\n\t\t__bnx2x_link_report(bp);\n\t}\n\n\t \n\tmemcpy(&bp->old_bulletin, bulletin,\n\t       sizeof(struct pf_vf_bulletin_content));\n\n\treturn PFVF_BULLETIN_UPDATED;\n}\n\nvoid bnx2x_timer_sriov(struct bnx2x *bp)\n{\n\tbnx2x_sample_bulletin(bp);\n\n\t \n\tif (bp->old_bulletin.valid_bitmap & 1 << CHANNEL_DOWN)\n\t\tbnx2x_schedule_sp_rtnl(bp, BNX2X_SP_RTNL_VFPF_CHANNEL_DOWN,\n\t\t\t\t       BNX2X_MSG_IOV);\n}\n\nvoid __iomem *bnx2x_vf_doorbells(struct bnx2x *bp)\n{\n\t \n\treturn bp->regview + PXP_VF_ADDR_DB_START;\n}\n\nvoid bnx2x_vf_pci_dealloc(struct bnx2x *bp)\n{\n\tBNX2X_PCI_FREE(bp->vf2pf_mbox, bp->vf2pf_mbox_mapping,\n\t\t       sizeof(struct bnx2x_vf_mbx_msg));\n\tBNX2X_PCI_FREE(bp->pf2vf_bulletin, bp->pf2vf_bulletin_mapping,\n\t\t       sizeof(union pf_vf_bulletin));\n}\n\nint bnx2x_vf_pci_alloc(struct bnx2x *bp)\n{\n\tmutex_init(&bp->vf2pf_mutex);\n\n\t \n\tbp->vf2pf_mbox = BNX2X_PCI_ALLOC(&bp->vf2pf_mbox_mapping,\n\t\t\t\t\t sizeof(struct bnx2x_vf_mbx_msg));\n\tif (!bp->vf2pf_mbox)\n\t\tgoto alloc_mem_err;\n\n\t \n\tbp->pf2vf_bulletin = BNX2X_PCI_ALLOC(&bp->pf2vf_bulletin_mapping,\n\t\t\t\t\t     sizeof(union pf_vf_bulletin));\n\tif (!bp->pf2vf_bulletin)\n\t\tgoto alloc_mem_err;\n\n\tbnx2x_vf_bulletin_finalize(&bp->pf2vf_bulletin->content, true);\n\n\treturn 0;\n\nalloc_mem_err:\n\tbnx2x_vf_pci_dealloc(bp);\n\treturn -ENOMEM;\n}\n\nvoid bnx2x_iov_channel_down(struct bnx2x *bp)\n{\n\tint vf_idx;\n\tstruct pf_vf_bulletin_content *bulletin;\n\n\tif (!IS_SRIOV(bp))\n\t\treturn;\n\n\tfor_each_vf(bp, vf_idx) {\n\t\t \n\t\tbulletin = BP_VF_BULLETIN(bp, vf_idx);\n\t\tbulletin->valid_bitmap |= 1 << CHANNEL_DOWN;\n\n\t\t \n\t\tbnx2x_post_vf_bulletin(bp, vf_idx);\n\t}\n}\n\nvoid bnx2x_iov_task(struct work_struct *work)\n{\n\tstruct bnx2x *bp = container_of(work, struct bnx2x, iov_task.work);\n\n\tif (!netif_running(bp->dev))\n\t\treturn;\n\n\tif (test_and_clear_bit(BNX2X_IOV_HANDLE_FLR,\n\t\t\t       &bp->iov_task_state))\n\t\tbnx2x_vf_handle_flr_event(bp);\n\n\tif (test_and_clear_bit(BNX2X_IOV_HANDLE_VF_MSG,\n\t\t\t       &bp->iov_task_state))\n\t\tbnx2x_vf_mbx(bp);\n}\n\nvoid bnx2x_schedule_iov_task(struct bnx2x *bp, enum bnx2x_iov_flag flag)\n{\n\tsmp_mb__before_atomic();\n\tset_bit(flag, &bp->iov_task_state);\n\tsmp_mb__after_atomic();\n\tDP(BNX2X_MSG_IOV, \"Scheduling iov task [Flag: %d]\\n\", flag);\n\tqueue_delayed_work(bnx2x_iov_wq, &bp->iov_task, 0);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}