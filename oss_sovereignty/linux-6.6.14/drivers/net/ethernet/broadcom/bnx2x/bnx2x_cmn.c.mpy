{
  "module_name": "bnx2x_cmn.c",
  "hash_id": "806b3753f6472a6ab337dbc5c7d759660523468c97a791b953056984fd2ecaff",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c",
  "human_readable_source": " \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/interrupt.h>\n#include <linux/ip.h>\n#include <linux/crash_dump.h>\n#include <net/tcp.h>\n#include <net/gro.h>\n#include <net/ipv6.h>\n#include <net/ip6_checksum.h>\n#include <linux/prefetch.h>\n#include \"bnx2x_cmn.h\"\n#include \"bnx2x_init.h\"\n#include \"bnx2x_sp.h\"\n\nstatic void bnx2x_free_fp_mem_cnic(struct bnx2x *bp);\nstatic int bnx2x_alloc_fp_mem_cnic(struct bnx2x *bp);\nstatic int bnx2x_alloc_fp_mem(struct bnx2x *bp);\nstatic int bnx2x_poll(struct napi_struct *napi, int budget);\n\nstatic void bnx2x_add_all_napi_cnic(struct bnx2x *bp)\n{\n\tint i;\n\n\t \n\tfor_each_rx_queue_cnic(bp, i) {\n\t\tnetif_napi_add(bp->dev, &bnx2x_fp(bp, i, napi), bnx2x_poll);\n\t}\n}\n\nstatic void bnx2x_add_all_napi(struct bnx2x *bp)\n{\n\tint i;\n\n\t \n\tfor_each_eth_queue(bp, i) {\n\t\tnetif_napi_add(bp->dev, &bnx2x_fp(bp, i, napi), bnx2x_poll);\n\t}\n}\n\nstatic int bnx2x_calc_num_queues(struct bnx2x *bp)\n{\n\tint nq = bnx2x_num_queues ? : netif_get_num_default_rss_queues();\n\n\t \n\tif (is_kdump_kernel())\n\t\tnq = 1;\n\n\tnq = clamp(nq, 1, BNX2X_MAX_QUEUES(bp));\n\treturn nq;\n}\n\n \nstatic inline void bnx2x_move_fp(struct bnx2x *bp, int from, int to)\n{\n\tstruct bnx2x_fastpath *from_fp = &bp->fp[from];\n\tstruct bnx2x_fastpath *to_fp = &bp->fp[to];\n\tstruct bnx2x_sp_objs *from_sp_objs = &bp->sp_objs[from];\n\tstruct bnx2x_sp_objs *to_sp_objs = &bp->sp_objs[to];\n\tstruct bnx2x_fp_stats *from_fp_stats = &bp->fp_stats[from];\n\tstruct bnx2x_fp_stats *to_fp_stats = &bp->fp_stats[to];\n\tint old_max_eth_txqs, new_max_eth_txqs;\n\tint old_txdata_index = 0, new_txdata_index = 0;\n\tstruct bnx2x_agg_info *old_tpa_info = to_fp->tpa_info;\n\n\t \n\tfrom_fp->napi = to_fp->napi;\n\n\t \n\tmemcpy(to_fp, from_fp, sizeof(*to_fp));\n\tto_fp->index = to;\n\n\t \n\tto_fp->tpa_info = old_tpa_info;\n\n\t \n\tmemcpy(to_sp_objs, from_sp_objs, sizeof(*to_sp_objs));\n\n\t \n\tmemcpy(to_fp_stats, from_fp_stats, sizeof(*to_fp_stats));\n\n\t \n\n\told_max_eth_txqs = BNX2X_NUM_ETH_QUEUES(bp) * (bp)->max_cos;\n\tnew_max_eth_txqs = (BNX2X_NUM_ETH_QUEUES(bp) - from + to) *\n\t\t\t\t(bp)->max_cos;\n\tif (from == FCOE_IDX(bp)) {\n\t\told_txdata_index = old_max_eth_txqs + FCOE_TXQ_IDX_OFFSET;\n\t\tnew_txdata_index = new_max_eth_txqs + FCOE_TXQ_IDX_OFFSET;\n\t}\n\n\tmemcpy(&bp->bnx2x_txq[new_txdata_index],\n\t       &bp->bnx2x_txq[old_txdata_index],\n\t       sizeof(struct bnx2x_fp_txdata));\n\tto_fp->txdata_ptr[0] = &bp->bnx2x_txq[new_txdata_index];\n}\n\n \nvoid bnx2x_fill_fw_str(struct bnx2x *bp, char *buf, size_t buf_len)\n{\n\tif (IS_PF(bp)) {\n\t\tu8 phy_fw_ver[PHY_FW_VER_LEN];\n\n\t\tphy_fw_ver[0] = '\\0';\n\t\tbnx2x_get_ext_phy_fw_version(&bp->link_params,\n\t\t\t\t\t     phy_fw_ver, PHY_FW_VER_LEN);\n\t\tstrscpy(buf, bp->fw_ver, buf_len);\n\t\tsnprintf(buf + strlen(bp->fw_ver), 32 - strlen(bp->fw_ver),\n\t\t\t \"bc %d.%d.%d%s%s\",\n\t\t\t (bp->common.bc_ver & 0xff0000) >> 16,\n\t\t\t (bp->common.bc_ver & 0xff00) >> 8,\n\t\t\t (bp->common.bc_ver & 0xff),\n\t\t\t ((phy_fw_ver[0] != '\\0') ? \" phy \" : \"\"), phy_fw_ver);\n\t} else {\n\t\tbnx2x_vf_fill_fw_str(bp, buf, buf_len);\n\t}\n}\n\n \nstatic void bnx2x_shrink_eth_fp(struct bnx2x *bp, int delta)\n{\n\tint i, cos, old_eth_num = BNX2X_NUM_ETH_QUEUES(bp);\n\n\t \n\tfor (cos = 1; cos < bp->max_cos; cos++) {\n\t\tfor (i = 0; i < old_eth_num - delta; i++) {\n\t\t\tstruct bnx2x_fastpath *fp = &bp->fp[i];\n\t\t\tint new_idx = cos * (old_eth_num - delta) + i;\n\n\t\t\tmemcpy(&bp->bnx2x_txq[new_idx], fp->txdata_ptr[cos],\n\t\t\t       sizeof(struct bnx2x_fp_txdata));\n\t\t\tfp->txdata_ptr[cos] = &bp->bnx2x_txq[new_idx];\n\t\t}\n\t}\n}\n\nint bnx2x_load_count[2][3] = { {0} };  \n\n \nstatic u16 bnx2x_free_tx_pkt(struct bnx2x *bp, struct bnx2x_fp_txdata *txdata,\n\t\t\t     u16 idx, unsigned int *pkts_compl,\n\t\t\t     unsigned int *bytes_compl)\n{\n\tstruct sw_tx_bd *tx_buf = &txdata->tx_buf_ring[idx];\n\tstruct eth_tx_start_bd *tx_start_bd;\n\tstruct eth_tx_bd *tx_data_bd;\n\tstruct sk_buff *skb = tx_buf->skb;\n\tu16 bd_idx = TX_BD(tx_buf->first_bd), new_cons;\n\tint nbd;\n\tu16 split_bd_len = 0;\n\n\t \n\tprefetch(&skb->end);\n\n\tDP(NETIF_MSG_TX_DONE, \"fp[%d]: pkt_idx %d  buff @(%p)->skb %p\\n\",\n\t   txdata->txq_index, idx, tx_buf, skb);\n\n\ttx_start_bd = &txdata->tx_desc_ring[bd_idx].start_bd;\n\n\tnbd = le16_to_cpu(tx_start_bd->nbd) - 1;\n#ifdef BNX2X_STOP_ON_ERROR\n\tif ((nbd - 1) > (MAX_SKB_FRAGS + 2)) {\n\t\tBNX2X_ERR(\"BAD nbd!\\n\");\n\t\tbnx2x_panic();\n\t}\n#endif\n\tnew_cons = nbd + tx_buf->first_bd;\n\n\t \n\tbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\n\n\t \n\t--nbd;\n\tbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\n\n\tif (tx_buf->flags & BNX2X_HAS_SECOND_PBD) {\n\t\t \n\t\t--nbd;\n\t\tbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\n\t}\n\n\t \n\tif (tx_buf->flags & BNX2X_TSO_SPLIT_BD) {\n\t\ttx_data_bd = &txdata->tx_desc_ring[bd_idx].reg_bd;\n\t\tsplit_bd_len = BD_UNMAP_LEN(tx_data_bd);\n\t\t--nbd;\n\t\tbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\n\t}\n\n\t \n\tdma_unmap_single(&bp->pdev->dev, BD_UNMAP_ADDR(tx_start_bd),\n\t\t\t BD_UNMAP_LEN(tx_start_bd) + split_bd_len,\n\t\t\t DMA_TO_DEVICE);\n\n\t \n\twhile (nbd > 0) {\n\n\t\ttx_data_bd = &txdata->tx_desc_ring[bd_idx].reg_bd;\n\t\tdma_unmap_page(&bp->pdev->dev, BD_UNMAP_ADDR(tx_data_bd),\n\t\t\t       BD_UNMAP_LEN(tx_data_bd), DMA_TO_DEVICE);\n\t\tif (--nbd)\n\t\t\tbd_idx = TX_BD(NEXT_TX_IDX(bd_idx));\n\t}\n\n\t \n\tWARN_ON(!skb);\n\tif (likely(skb)) {\n\t\t(*pkts_compl)++;\n\t\t(*bytes_compl) += skb->len;\n\t\tdev_kfree_skb_any(skb);\n\t}\n\n\ttx_buf->first_bd = 0;\n\ttx_buf->skb = NULL;\n\n\treturn new_cons;\n}\n\nint bnx2x_tx_int(struct bnx2x *bp, struct bnx2x_fp_txdata *txdata)\n{\n\tstruct netdev_queue *txq;\n\tu16 hw_cons, sw_cons, bd_cons = txdata->tx_bd_cons;\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\n#ifdef BNX2X_STOP_ON_ERROR\n\tif (unlikely(bp->panic))\n\t\treturn -1;\n#endif\n\n\ttxq = netdev_get_tx_queue(bp->dev, txdata->txq_index);\n\thw_cons = le16_to_cpu(*txdata->tx_cons_sb);\n\tsw_cons = txdata->tx_pkt_cons;\n\n\t \n\tsmp_rmb();\n\n\twhile (sw_cons != hw_cons) {\n\t\tu16 pkt_cons;\n\n\t\tpkt_cons = TX_BD(sw_cons);\n\n\t\tDP(NETIF_MSG_TX_DONE,\n\t\t   \"queue[%d]: hw_cons %u  sw_cons %u  pkt_cons %u\\n\",\n\t\t   txdata->txq_index, hw_cons, sw_cons, pkt_cons);\n\n\t\tbd_cons = bnx2x_free_tx_pkt(bp, txdata, pkt_cons,\n\t\t\t\t\t    &pkts_compl, &bytes_compl);\n\n\t\tsw_cons++;\n\t}\n\n\tnetdev_tx_completed_queue(txq, pkts_compl, bytes_compl);\n\n\ttxdata->tx_pkt_cons = sw_cons;\n\ttxdata->tx_bd_cons = bd_cons;\n\n\t \n\tsmp_mb();\n\n\tif (unlikely(netif_tx_queue_stopped(txq))) {\n\t\t \n\n\t\t__netif_tx_lock(txq, smp_processor_id());\n\n\t\tif ((netif_tx_queue_stopped(txq)) &&\n\t\t    (bp->state == BNX2X_STATE_OPEN) &&\n\t\t    (bnx2x_tx_avail(bp, txdata) >= MAX_DESC_PER_TX_PKT))\n\t\t\tnetif_tx_wake_queue(txq);\n\n\t\t__netif_tx_unlock(txq);\n\t}\n\treturn 0;\n}\n\nstatic inline void bnx2x_update_last_max_sge(struct bnx2x_fastpath *fp,\n\t\t\t\t\t     u16 idx)\n{\n\tu16 last_max = fp->last_max_sge;\n\n\tif (SUB_S16(idx, last_max) > 0)\n\t\tfp->last_max_sge = idx;\n}\n\nstatic inline void bnx2x_update_sge_prod(struct bnx2x_fastpath *fp,\n\t\t\t\t\t u16 sge_len,\n\t\t\t\t\t struct eth_end_agg_rx_cqe *cqe)\n{\n\tstruct bnx2x *bp = fp->bp;\n\tu16 last_max, last_elem, first_elem;\n\tu16 delta = 0;\n\tu16 i;\n\n\tif (!sge_len)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < sge_len; i++)\n\t\tBIT_VEC64_CLEAR_BIT(fp->sge_mask,\n\t\t\tRX_SGE(le16_to_cpu(cqe->sgl_or_raw_data.sgl[i])));\n\n\tDP(NETIF_MSG_RX_STATUS, \"fp_cqe->sgl[%d] = %d\\n\",\n\t   sge_len - 1, le16_to_cpu(cqe->sgl_or_raw_data.sgl[sge_len - 1]));\n\n\t \n\tprefetch((void *)(fp->sge_mask));\n\tbnx2x_update_last_max_sge(fp,\n\t\tle16_to_cpu(cqe->sgl_or_raw_data.sgl[sge_len - 1]));\n\n\tlast_max = RX_SGE(fp->last_max_sge);\n\tlast_elem = last_max >> BIT_VEC64_ELEM_SHIFT;\n\tfirst_elem = RX_SGE(fp->rx_sge_prod) >> BIT_VEC64_ELEM_SHIFT;\n\n\t \n\tif (last_elem + 1 != first_elem)\n\t\tlast_elem++;\n\n\t \n\tfor (i = first_elem; i != last_elem; i = NEXT_SGE_MASK_ELEM(i)) {\n\t\tif (likely(fp->sge_mask[i]))\n\t\t\tbreak;\n\n\t\tfp->sge_mask[i] = BIT_VEC64_ELEM_ONE_MASK;\n\t\tdelta += BIT_VEC64_ELEM_SZ;\n\t}\n\n\tif (delta > 0) {\n\t\tfp->rx_sge_prod += delta;\n\t\t \n\t\tbnx2x_clear_sge_mask_next_elems(fp);\n\t}\n\n\tDP(NETIF_MSG_RX_STATUS,\n\t   \"fp->last_max_sge = %d  fp->rx_sge_prod = %d\\n\",\n\t   fp->last_max_sge, fp->rx_sge_prod);\n}\n\n \nstatic u32 bnx2x_get_rxhash(const struct bnx2x *bp,\n\t\t\t    const struct eth_fast_path_rx_cqe *cqe,\n\t\t\t    enum pkt_hash_types *rxhash_type)\n{\n\t \n\tif ((bp->dev->features & NETIF_F_RXHASH) &&\n\t    (cqe->status_flags & ETH_FAST_PATH_RX_CQE_RSS_HASH_FLG)) {\n\t\tenum eth_rss_hash_type htype;\n\n\t\thtype = cqe->status_flags & ETH_FAST_PATH_RX_CQE_RSS_HASH_TYPE;\n\t\t*rxhash_type = ((htype == TCP_IPV4_HASH_TYPE) ||\n\t\t\t\t(htype == TCP_IPV6_HASH_TYPE)) ?\n\t\t\t       PKT_HASH_TYPE_L4 : PKT_HASH_TYPE_L3;\n\n\t\treturn le32_to_cpu(cqe->rss_hash_result);\n\t}\n\t*rxhash_type = PKT_HASH_TYPE_NONE;\n\treturn 0;\n}\n\nstatic void bnx2x_tpa_start(struct bnx2x_fastpath *fp, u16 queue,\n\t\t\t    u16 cons, u16 prod,\n\t\t\t    struct eth_fast_path_rx_cqe *cqe)\n{\n\tstruct bnx2x *bp = fp->bp;\n\tstruct sw_rx_bd *cons_rx_buf = &fp->rx_buf_ring[cons];\n\tstruct sw_rx_bd *prod_rx_buf = &fp->rx_buf_ring[prod];\n\tstruct eth_rx_bd *prod_bd = &fp->rx_desc_ring[prod];\n\tdma_addr_t mapping;\n\tstruct bnx2x_agg_info *tpa_info = &fp->tpa_info[queue];\n\tstruct sw_rx_bd *first_buf = &tpa_info->first_buf;\n\n\t \n\tif (tpa_info->tpa_state != BNX2X_TPA_STOP)\n\t\tBNX2X_ERR(\"start of bin not in stop [%d]\\n\", queue);\n\n\t \n\tmapping = dma_map_single(&bp->pdev->dev,\n\t\t\t\t first_buf->data + NET_SKB_PAD,\n\t\t\t\t fp->rx_buf_size, DMA_FROM_DEVICE);\n\t \n\n\tif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\n\t\t \n\t\tbnx2x_reuse_rx_data(fp, cons, prod);\n\t\ttpa_info->tpa_state = BNX2X_TPA_ERROR;\n\t\treturn;\n\t}\n\n\t \n\tprod_rx_buf->data = first_buf->data;\n\tdma_unmap_addr_set(prod_rx_buf, mapping, mapping);\n\t \n\tprod_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\n\tprod_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\n\n\t \n\t*first_buf = *cons_rx_buf;\n\n\t \n\ttpa_info->parsing_flags =\n\t\tle16_to_cpu(cqe->pars_flags.flags);\n\ttpa_info->vlan_tag = le16_to_cpu(cqe->vlan_tag);\n\ttpa_info->tpa_state = BNX2X_TPA_START;\n\ttpa_info->len_on_bd = le16_to_cpu(cqe->len_on_bd);\n\ttpa_info->placement_offset = cqe->placement_offset;\n\ttpa_info->rxhash = bnx2x_get_rxhash(bp, cqe, &tpa_info->rxhash_type);\n\tif (fp->mode == TPA_MODE_GRO) {\n\t\tu16 gro_size = le16_to_cpu(cqe->pkt_len_or_gro_seg_len);\n\t\ttpa_info->full_page = SGE_PAGES / gro_size * gro_size;\n\t\ttpa_info->gro_size = gro_size;\n\t}\n\n#ifdef BNX2X_STOP_ON_ERROR\n\tfp->tpa_queue_used |= (1 << queue);\n\tDP(NETIF_MSG_RX_STATUS, \"fp->tpa_queue_used = 0x%llx\\n\",\n\t   fp->tpa_queue_used);\n#endif\n}\n\n \n#define TPA_TSTAMP_OPT_LEN\t12\n \nstatic void bnx2x_set_gro_params(struct sk_buff *skb, u16 parsing_flags,\n\t\t\t\t u16 len_on_bd, unsigned int pkt_len,\n\t\t\t\t u16 num_of_coalesced_segs)\n{\n\t \n\tu16 hdrs_len = ETH_HLEN + sizeof(struct tcphdr);\n\n\tif (GET_FLAG(parsing_flags, PARSING_FLAGS_OVER_ETHERNET_PROTOCOL) ==\n\t    PRS_FLAG_OVERETH_IPV6) {\n\t\thdrs_len += sizeof(struct ipv6hdr);\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\n\t} else {\n\t\thdrs_len += sizeof(struct iphdr);\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\n\t}\n\n\t \n\tif (parsing_flags & PARSING_FLAGS_TIME_STAMP_EXIST_FLAG)\n\t\thdrs_len += TPA_TSTAMP_OPT_LEN;\n\n\tskb_shinfo(skb)->gso_size = len_on_bd - hdrs_len;\n\n\t \n\tNAPI_GRO_CB(skb)->count = num_of_coalesced_segs;\n}\n\nstatic int bnx2x_alloc_rx_sge(struct bnx2x *bp, struct bnx2x_fastpath *fp,\n\t\t\t      u16 index, gfp_t gfp_mask)\n{\n\tstruct sw_rx_page *sw_buf = &fp->rx_page_ring[index];\n\tstruct eth_rx_sge *sge = &fp->rx_sge_ring[index];\n\tstruct bnx2x_alloc_pool *pool = &fp->page_pool;\n\tdma_addr_t mapping;\n\n\tif (!pool->page) {\n\t\tpool->page = alloc_pages(gfp_mask, PAGES_PER_SGE_SHIFT);\n\t\tif (unlikely(!pool->page))\n\t\t\treturn -ENOMEM;\n\n\t\tpool->offset = 0;\n\t}\n\n\tmapping = dma_map_page(&bp->pdev->dev, pool->page,\n\t\t\t       pool->offset, SGE_PAGE_SIZE, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\n\t\tBNX2X_ERR(\"Can't map sge\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tsw_buf->page = pool->page;\n\tsw_buf->offset = pool->offset;\n\n\tdma_unmap_addr_set(sw_buf, mapping, mapping);\n\n\tsge->addr_hi = cpu_to_le32(U64_HI(mapping));\n\tsge->addr_lo = cpu_to_le32(U64_LO(mapping));\n\n\tpool->offset += SGE_PAGE_SIZE;\n\tif (PAGE_SIZE - pool->offset >= SGE_PAGE_SIZE)\n\t\tget_page(pool->page);\n\telse\n\t\tpool->page = NULL;\n\treturn 0;\n}\n\nstatic int bnx2x_fill_frag_skb(struct bnx2x *bp, struct bnx2x_fastpath *fp,\n\t\t\t       struct bnx2x_agg_info *tpa_info,\n\t\t\t       u16 pages,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct eth_end_agg_rx_cqe *cqe,\n\t\t\t       u16 cqe_idx)\n{\n\tstruct sw_rx_page *rx_pg, old_rx_pg;\n\tu32 i, frag_len, frag_size;\n\tint err, j, frag_id = 0;\n\tu16 len_on_bd = tpa_info->len_on_bd;\n\tu16 full_page = 0, gro_size = 0;\n\n\tfrag_size = le16_to_cpu(cqe->pkt_len) - len_on_bd;\n\n\tif (fp->mode == TPA_MODE_GRO) {\n\t\tgro_size = tpa_info->gro_size;\n\t\tfull_page = tpa_info->full_page;\n\t}\n\n\t \n\tif (frag_size)\n\t\tbnx2x_set_gro_params(skb, tpa_info->parsing_flags, len_on_bd,\n\t\t\t\t     le16_to_cpu(cqe->pkt_len),\n\t\t\t\t     le16_to_cpu(cqe->num_of_coalesced_segs));\n\n#ifdef BNX2X_STOP_ON_ERROR\n\tif (pages > min_t(u32, 8, MAX_SKB_FRAGS) * SGE_PAGES) {\n\t\tBNX2X_ERR(\"SGL length is too long: %d. CQE index is %d\\n\",\n\t\t\t  pages, cqe_idx);\n\t\tBNX2X_ERR(\"cqe->pkt_len = %d\\n\", cqe->pkt_len);\n\t\tbnx2x_panic();\n\t\treturn -EINVAL;\n\t}\n#endif\n\n\t \n\tfor (i = 0, j = 0; i < pages; i += PAGES_PER_SGE, j++) {\n\t\tu16 sge_idx = RX_SGE(le16_to_cpu(cqe->sgl_or_raw_data.sgl[j]));\n\n\t\t \n\t\tif (fp->mode == TPA_MODE_GRO)\n\t\t\tfrag_len = min_t(u32, frag_size, (u32)full_page);\n\t\telse  \n\t\t\tfrag_len = min_t(u32, frag_size, (u32)SGE_PAGES);\n\n\t\trx_pg = &fp->rx_page_ring[sge_idx];\n\t\told_rx_pg = *rx_pg;\n\n\t\t \n\t\terr = bnx2x_alloc_rx_sge(bp, fp, sge_idx, GFP_ATOMIC);\n\t\tif (unlikely(err)) {\n\t\t\tbnx2x_fp_qstats(bp, fp)->rx_skb_alloc_failed++;\n\t\t\treturn err;\n\t\t}\n\n\t\tdma_unmap_page(&bp->pdev->dev,\n\t\t\t       dma_unmap_addr(&old_rx_pg, mapping),\n\t\t\t       SGE_PAGE_SIZE, DMA_FROM_DEVICE);\n\t\t \n\t\tif (fp->mode == TPA_MODE_LRO)\n\t\t\tskb_fill_page_desc(skb, j, old_rx_pg.page,\n\t\t\t\t\t   old_rx_pg.offset, frag_len);\n\t\telse {  \n\t\t\tint rem;\n\t\t\tint offset = 0;\n\t\t\tfor (rem = frag_len; rem > 0; rem -= gro_size) {\n\t\t\t\tint len = rem > gro_size ? gro_size : rem;\n\t\t\t\tskb_fill_page_desc(skb, frag_id++,\n\t\t\t\t\t\t   old_rx_pg.page,\n\t\t\t\t\t\t   old_rx_pg.offset + offset,\n\t\t\t\t\t\t   len);\n\t\t\t\tif (offset)\n\t\t\t\t\tget_page(old_rx_pg.page);\n\t\t\t\toffset += len;\n\t\t\t}\n\t\t}\n\n\t\tskb->data_len += frag_len;\n\t\tskb->truesize += SGE_PAGES;\n\t\tskb->len += frag_len;\n\n\t\tfrag_size -= frag_len;\n\t}\n\n\treturn 0;\n}\n\nstatic struct sk_buff *\nbnx2x_build_skb(const struct bnx2x_fastpath *fp, void *data)\n{\n\tstruct sk_buff *skb;\n\n\tif (fp->rx_frag_size)\n\t\tskb = build_skb(data, fp->rx_frag_size);\n\telse\n\t\tskb = slab_build_skb(data);\n\treturn skb;\n}\n\nstatic void bnx2x_frag_free(const struct bnx2x_fastpath *fp, void *data)\n{\n\tif (fp->rx_frag_size)\n\t\tskb_free_frag(data);\n\telse\n\t\tkfree(data);\n}\n\nstatic void *bnx2x_frag_alloc(const struct bnx2x_fastpath *fp, gfp_t gfp_mask)\n{\n\tif (fp->rx_frag_size) {\n\t\t \n\t\tif (unlikely(gfpflags_allow_blocking(gfp_mask)))\n\t\t\treturn (void *)__get_free_page(gfp_mask);\n\n\t\treturn napi_alloc_frag(fp->rx_frag_size);\n\t}\n\n\treturn kmalloc(fp->rx_buf_size + NET_SKB_PAD, gfp_mask);\n}\n\n#ifdef CONFIG_INET\nstatic void bnx2x_gro_ip_csum(struct bnx2x *bp, struct sk_buff *skb)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct tcphdr *th;\n\n\tskb_set_transport_header(skb, sizeof(struct iphdr));\n\tth = tcp_hdr(skb);\n\n\tth->check = ~tcp_v4_check(skb->len - skb_transport_offset(skb),\n\t\t\t\t  iph->saddr, iph->daddr, 0);\n}\n\nstatic void bnx2x_gro_ipv6_csum(struct bnx2x *bp, struct sk_buff *skb)\n{\n\tstruct ipv6hdr *iph = ipv6_hdr(skb);\n\tstruct tcphdr *th;\n\n\tskb_set_transport_header(skb, sizeof(struct ipv6hdr));\n\tth = tcp_hdr(skb);\n\n\tth->check = ~tcp_v6_check(skb->len - skb_transport_offset(skb),\n\t\t\t\t  &iph->saddr, &iph->daddr, 0);\n}\n\nstatic void bnx2x_gro_csum(struct bnx2x *bp, struct sk_buff *skb,\n\t\t\t    void (*gro_func)(struct bnx2x*, struct sk_buff*))\n{\n\tskb_reset_network_header(skb);\n\tgro_func(bp, skb);\n\ttcp_gro_complete(skb);\n}\n#endif\n\nstatic void bnx2x_gro_receive(struct bnx2x *bp, struct bnx2x_fastpath *fp,\n\t\t\t       struct sk_buff *skb)\n{\n#ifdef CONFIG_INET\n\tif (skb_shinfo(skb)->gso_size) {\n\t\tswitch (be16_to_cpu(skb->protocol)) {\n\t\tcase ETH_P_IP:\n\t\t\tbnx2x_gro_csum(bp, skb, bnx2x_gro_ip_csum);\n\t\t\tbreak;\n\t\tcase ETH_P_IPV6:\n\t\t\tbnx2x_gro_csum(bp, skb, bnx2x_gro_ipv6_csum);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tnetdev_WARN_ONCE(bp->dev,\n\t\t\t\t\t \"Error: FW GRO supports only IPv4/IPv6, not 0x%04x\\n\",\n\t\t\t\t\t be16_to_cpu(skb->protocol));\n\t\t}\n\t}\n#endif\n\tskb_record_rx_queue(skb, fp->rx_queue);\n\tnapi_gro_receive(&fp->napi, skb);\n}\n\nstatic void bnx2x_tpa_stop(struct bnx2x *bp, struct bnx2x_fastpath *fp,\n\t\t\t   struct bnx2x_agg_info *tpa_info,\n\t\t\t   u16 pages,\n\t\t\t   struct eth_end_agg_rx_cqe *cqe,\n\t\t\t   u16 cqe_idx)\n{\n\tstruct sw_rx_bd *rx_buf = &tpa_info->first_buf;\n\tu8 pad = tpa_info->placement_offset;\n\tu16 len = tpa_info->len_on_bd;\n\tstruct sk_buff *skb = NULL;\n\tu8 *new_data, *data = rx_buf->data;\n\tu8 old_tpa_state = tpa_info->tpa_state;\n\n\ttpa_info->tpa_state = BNX2X_TPA_STOP;\n\n\t \n\tif (old_tpa_state == BNX2X_TPA_ERROR)\n\t\tgoto drop;\n\n\t \n\tnew_data = bnx2x_frag_alloc(fp, GFP_ATOMIC);\n\t \n\tdma_unmap_single(&bp->pdev->dev, dma_unmap_addr(rx_buf, mapping),\n\t\t\t fp->rx_buf_size, DMA_FROM_DEVICE);\n\tif (likely(new_data))\n\t\tskb = bnx2x_build_skb(fp, data);\n\n\tif (likely(skb)) {\n#ifdef BNX2X_STOP_ON_ERROR\n\t\tif (pad + len > fp->rx_buf_size) {\n\t\t\tBNX2X_ERR(\"skb_put is about to fail...  pad %d  len %d  rx_buf_size %d\\n\",\n\t\t\t\t  pad, len, fp->rx_buf_size);\n\t\t\tbnx2x_panic();\n\t\t\tbnx2x_frag_free(fp, new_data);\n\t\t\treturn;\n\t\t}\n#endif\n\n\t\tskb_reserve(skb, pad + NET_SKB_PAD);\n\t\tskb_put(skb, len);\n\t\tskb_set_hash(skb, tpa_info->rxhash, tpa_info->rxhash_type);\n\n\t\tskb->protocol = eth_type_trans(skb, bp->dev);\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\tif (!bnx2x_fill_frag_skb(bp, fp, tpa_info, pages,\n\t\t\t\t\t skb, cqe, cqe_idx)) {\n\t\t\tif (tpa_info->parsing_flags & PARSING_FLAGS_VLAN)\n\t\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tpa_info->vlan_tag);\n\t\t\tbnx2x_gro_receive(bp, fp, skb);\n\t\t} else {\n\t\t\tDP(NETIF_MSG_RX_STATUS,\n\t\t\t   \"Failed to allocate new pages - dropping packet!\\n\");\n\t\t\tdev_kfree_skb_any(skb);\n\t\t}\n\n\t\t \n\t\trx_buf->data = new_data;\n\n\t\treturn;\n\t}\n\tif (new_data)\n\t\tbnx2x_frag_free(fp, new_data);\ndrop:\n\t \n\tDP(NETIF_MSG_RX_STATUS,\n\t   \"Failed to allocate or map a new skb - dropping packet!\\n\");\n\tbnx2x_fp_stats(bp, fp)->eth_q_stats.rx_skb_alloc_failed++;\n}\n\nstatic int bnx2x_alloc_rx_data(struct bnx2x *bp, struct bnx2x_fastpath *fp,\n\t\t\t       u16 index, gfp_t gfp_mask)\n{\n\tu8 *data;\n\tstruct sw_rx_bd *rx_buf = &fp->rx_buf_ring[index];\n\tstruct eth_rx_bd *rx_bd = &fp->rx_desc_ring[index];\n\tdma_addr_t mapping;\n\n\tdata = bnx2x_frag_alloc(fp, gfp_mask);\n\tif (unlikely(data == NULL))\n\t\treturn -ENOMEM;\n\n\tmapping = dma_map_single(&bp->pdev->dev, data + NET_SKB_PAD,\n\t\t\t\t fp->rx_buf_size,\n\t\t\t\t DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\n\t\tbnx2x_frag_free(fp, data);\n\t\tBNX2X_ERR(\"Can't map rx data\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trx_buf->data = data;\n\tdma_unmap_addr_set(rx_buf, mapping, mapping);\n\n\trx_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\n\trx_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\n\n\treturn 0;\n}\n\nstatic\nvoid bnx2x_csum_validate(struct sk_buff *skb, union eth_rx_cqe *cqe,\n\t\t\t\t struct bnx2x_fastpath *fp,\n\t\t\t\t struct bnx2x_eth_q_stats *qstats)\n{\n\t \n\tif (cqe->fast_path_cqe.status_flags &\n\t    ETH_FAST_PATH_RX_CQE_L4_XSUM_NO_VALIDATION_FLG)\n\t\treturn;\n\n\t \n\n\tif (cqe->fast_path_cqe.type_error_flags &\n\t    (ETH_FAST_PATH_RX_CQE_IP_BAD_XSUM_FLG |\n\t     ETH_FAST_PATH_RX_CQE_L4_BAD_XSUM_FLG))\n\t\tqstats->hw_csum_err++;\n\telse\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n}\n\nstatic int bnx2x_rx_int(struct bnx2x_fastpath *fp, int budget)\n{\n\tstruct bnx2x *bp = fp->bp;\n\tu16 bd_cons, bd_prod, bd_prod_fw, comp_ring_cons;\n\tu16 sw_comp_cons, sw_comp_prod;\n\tint rx_pkt = 0;\n\tunion eth_rx_cqe *cqe;\n\tstruct eth_fast_path_rx_cqe *cqe_fp;\n\n#ifdef BNX2X_STOP_ON_ERROR\n\tif (unlikely(bp->panic))\n\t\treturn 0;\n#endif\n\tif (budget <= 0)\n\t\treturn rx_pkt;\n\n\tbd_cons = fp->rx_bd_cons;\n\tbd_prod = fp->rx_bd_prod;\n\tbd_prod_fw = bd_prod;\n\tsw_comp_cons = fp->rx_comp_cons;\n\tsw_comp_prod = fp->rx_comp_prod;\n\n\tcomp_ring_cons = RCQ_BD(sw_comp_cons);\n\tcqe = &fp->rx_comp_ring[comp_ring_cons];\n\tcqe_fp = &cqe->fast_path_cqe;\n\n\tDP(NETIF_MSG_RX_STATUS,\n\t   \"queue[%d]: sw_comp_cons %u\\n\", fp->index, sw_comp_cons);\n\n\twhile (BNX2X_IS_CQE_COMPLETED(cqe_fp)) {\n\t\tstruct sw_rx_bd *rx_buf = NULL;\n\t\tstruct sk_buff *skb;\n\t\tu8 cqe_fp_flags;\n\t\tenum eth_rx_cqe_type cqe_fp_type;\n\t\tu16 len, pad, queue;\n\t\tu8 *data;\n\t\tu32 rxhash;\n\t\tenum pkt_hash_types rxhash_type;\n\n#ifdef BNX2X_STOP_ON_ERROR\n\t\tif (unlikely(bp->panic))\n\t\t\treturn 0;\n#endif\n\n\t\tbd_prod = RX_BD(bd_prod);\n\t\tbd_cons = RX_BD(bd_cons);\n\n\t\t \n\t\trmb();\n\n\t\tcqe_fp_flags = cqe_fp->type_error_flags;\n\t\tcqe_fp_type = cqe_fp_flags & ETH_FAST_PATH_RX_CQE_TYPE;\n\n\t\tDP(NETIF_MSG_RX_STATUS,\n\t\t   \"CQE type %x  err %x  status %x  queue %x  vlan %x  len %u\\n\",\n\t\t   CQE_TYPE(cqe_fp_flags),\n\t\t   cqe_fp_flags, cqe_fp->status_flags,\n\t\t   le32_to_cpu(cqe_fp->rss_hash_result),\n\t\t   le16_to_cpu(cqe_fp->vlan_tag),\n\t\t   le16_to_cpu(cqe_fp->pkt_len_or_gro_seg_len));\n\n\t\t \n\t\tif (unlikely(CQE_TYPE_SLOW(cqe_fp_type))) {\n\t\t\tbnx2x_sp_event(fp, cqe);\n\t\t\tgoto next_cqe;\n\t\t}\n\n\t\trx_buf = &fp->rx_buf_ring[bd_cons];\n\t\tdata = rx_buf->data;\n\n\t\tif (!CQE_TYPE_FAST(cqe_fp_type)) {\n\t\t\tstruct bnx2x_agg_info *tpa_info;\n\t\t\tu16 frag_size, pages;\n#ifdef BNX2X_STOP_ON_ERROR\n\t\t\t \n\t\t\tif (fp->mode == TPA_MODE_DISABLED &&\n\t\t\t    (CQE_TYPE_START(cqe_fp_type) ||\n\t\t\t     CQE_TYPE_STOP(cqe_fp_type)))\n\t\t\t\tBNX2X_ERR(\"START/STOP packet while TPA disabled, type %x\\n\",\n\t\t\t\t\t  CQE_TYPE(cqe_fp_type));\n#endif\n\n\t\t\tif (CQE_TYPE_START(cqe_fp_type)) {\n\t\t\t\tu16 queue = cqe_fp->queue_index;\n\t\t\t\tDP(NETIF_MSG_RX_STATUS,\n\t\t\t\t   \"calling tpa_start on queue %d\\n\",\n\t\t\t\t   queue);\n\n\t\t\t\tbnx2x_tpa_start(fp, queue,\n\t\t\t\t\t\tbd_cons, bd_prod,\n\t\t\t\t\t\tcqe_fp);\n\n\t\t\t\tgoto next_rx;\n\t\t\t}\n\t\t\tqueue = cqe->end_agg_cqe.queue_index;\n\t\t\ttpa_info = &fp->tpa_info[queue];\n\t\t\tDP(NETIF_MSG_RX_STATUS,\n\t\t\t   \"calling tpa_stop on queue %d\\n\",\n\t\t\t   queue);\n\n\t\t\tfrag_size = le16_to_cpu(cqe->end_agg_cqe.pkt_len) -\n\t\t\t\t    tpa_info->len_on_bd;\n\n\t\t\tif (fp->mode == TPA_MODE_GRO)\n\t\t\t\tpages = (frag_size + tpa_info->full_page - 1) /\n\t\t\t\t\t tpa_info->full_page;\n\t\t\telse\n\t\t\t\tpages = SGE_PAGE_ALIGN(frag_size) >>\n\t\t\t\t\tSGE_PAGE_SHIFT;\n\n\t\t\tbnx2x_tpa_stop(bp, fp, tpa_info, pages,\n\t\t\t\t       &cqe->end_agg_cqe, comp_ring_cons);\n#ifdef BNX2X_STOP_ON_ERROR\n\t\t\tif (bp->panic)\n\t\t\t\treturn 0;\n#endif\n\n\t\t\tbnx2x_update_sge_prod(fp, pages, &cqe->end_agg_cqe);\n\t\t\tgoto next_cqe;\n\t\t}\n\t\t \n\t\tlen = le16_to_cpu(cqe_fp->pkt_len_or_gro_seg_len);\n\t\tpad = cqe_fp->placement_offset;\n\t\tdma_sync_single_for_cpu(&bp->pdev->dev,\n\t\t\t\t\tdma_unmap_addr(rx_buf, mapping),\n\t\t\t\t\tpad + RX_COPY_THRESH,\n\t\t\t\t\tDMA_FROM_DEVICE);\n\t\tpad += NET_SKB_PAD;\n\t\tprefetch(data + pad);  \n\t\t \n\t\tif (unlikely(cqe_fp_flags & ETH_RX_ERROR_FALGS)) {\n\t\t\tDP(NETIF_MSG_RX_ERR | NETIF_MSG_RX_STATUS,\n\t\t\t   \"ERROR  flags %x  rx packet %u\\n\",\n\t\t\t   cqe_fp_flags, sw_comp_cons);\n\t\t\tbnx2x_fp_qstats(bp, fp)->rx_err_discard_pkt++;\n\t\t\tgoto reuse_rx;\n\t\t}\n\n\t\t \n\t\tif ((bp->dev->mtu > ETH_MAX_PACKET_SIZE) &&\n\t\t    (len <= RX_COPY_THRESH)) {\n\t\t\tskb = napi_alloc_skb(&fp->napi, len);\n\t\t\tif (skb == NULL) {\n\t\t\t\tDP(NETIF_MSG_RX_ERR | NETIF_MSG_RX_STATUS,\n\t\t\t\t   \"ERROR  packet dropped because of alloc failure\\n\");\n\t\t\t\tbnx2x_fp_qstats(bp, fp)->rx_skb_alloc_failed++;\n\t\t\t\tgoto reuse_rx;\n\t\t\t}\n\t\t\tmemcpy(skb->data, data + pad, len);\n\t\t\tbnx2x_reuse_rx_data(fp, bd_cons, bd_prod);\n\t\t} else {\n\t\t\tif (likely(bnx2x_alloc_rx_data(bp, fp, bd_prod,\n\t\t\t\t\t\t       GFP_ATOMIC) == 0)) {\n\t\t\t\tdma_unmap_single(&bp->pdev->dev,\n\t\t\t\t\t\t dma_unmap_addr(rx_buf, mapping),\n\t\t\t\t\t\t fp->rx_buf_size,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\t\t\t\tskb = bnx2x_build_skb(fp, data);\n\t\t\t\tif (unlikely(!skb)) {\n\t\t\t\t\tbnx2x_frag_free(fp, data);\n\t\t\t\t\tbnx2x_fp_qstats(bp, fp)->\n\t\t\t\t\t\t\trx_skb_alloc_failed++;\n\t\t\t\t\tgoto next_rx;\n\t\t\t\t}\n\t\t\t\tskb_reserve(skb, pad);\n\t\t\t} else {\n\t\t\t\tDP(NETIF_MSG_RX_ERR | NETIF_MSG_RX_STATUS,\n\t\t\t\t   \"ERROR  packet dropped because of alloc failure\\n\");\n\t\t\t\tbnx2x_fp_qstats(bp, fp)->rx_skb_alloc_failed++;\nreuse_rx:\n\t\t\t\tbnx2x_reuse_rx_data(fp, bd_cons, bd_prod);\n\t\t\t\tgoto next_rx;\n\t\t\t}\n\t\t}\n\n\t\tskb_put(skb, len);\n\t\tskb->protocol = eth_type_trans(skb, bp->dev);\n\n\t\t \n\t\trxhash = bnx2x_get_rxhash(bp, cqe_fp, &rxhash_type);\n\t\tskb_set_hash(skb, rxhash, rxhash_type);\n\n\t\tskb_checksum_none_assert(skb);\n\n\t\tif (bp->dev->features & NETIF_F_RXCSUM)\n\t\t\tbnx2x_csum_validate(skb, cqe, fp,\n\t\t\t\t\t    bnx2x_fp_qstats(bp, fp));\n\n\t\tskb_record_rx_queue(skb, fp->rx_queue);\n\n\t\t \n\t\tif (unlikely(cqe->fast_path_cqe.type_error_flags &\n\t\t\t     (1 << ETH_FAST_PATH_RX_CQE_PTP_PKT_SHIFT)))\n\t\t\tbnx2x_set_rx_ts(bp, skb);\n\n\t\tif (le16_to_cpu(cqe_fp->pars_flags.flags) &\n\t\t    PARSING_FLAGS_VLAN)\n\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t\t       le16_to_cpu(cqe_fp->vlan_tag));\n\n\t\tnapi_gro_receive(&fp->napi, skb);\nnext_rx:\n\t\trx_buf->data = NULL;\n\n\t\tbd_cons = NEXT_RX_IDX(bd_cons);\n\t\tbd_prod = NEXT_RX_IDX(bd_prod);\n\t\tbd_prod_fw = NEXT_RX_IDX(bd_prod_fw);\n\t\trx_pkt++;\nnext_cqe:\n\t\tsw_comp_prod = NEXT_RCQ_IDX(sw_comp_prod);\n\t\tsw_comp_cons = NEXT_RCQ_IDX(sw_comp_cons);\n\n\t\t \n\t\tBNX2X_SEED_CQE(cqe_fp);\n\n\t\tif (rx_pkt == budget)\n\t\t\tbreak;\n\n\t\tcomp_ring_cons = RCQ_BD(sw_comp_cons);\n\t\tcqe = &fp->rx_comp_ring[comp_ring_cons];\n\t\tcqe_fp = &cqe->fast_path_cqe;\n\t}  \n\n\tfp->rx_bd_cons = bd_cons;\n\tfp->rx_bd_prod = bd_prod_fw;\n\tfp->rx_comp_cons = sw_comp_cons;\n\tfp->rx_comp_prod = sw_comp_prod;\n\n\t \n\tbnx2x_update_rx_prod(bp, fp, bd_prod_fw, sw_comp_prod,\n\t\t\t     fp->rx_sge_prod);\n\n\treturn rx_pkt;\n}\n\nstatic irqreturn_t bnx2x_msix_fp_int(int irq, void *fp_cookie)\n{\n\tstruct bnx2x_fastpath *fp = fp_cookie;\n\tstruct bnx2x *bp = fp->bp;\n\tu8 cos;\n\n\tDP(NETIF_MSG_INTR,\n\t   \"got an MSI-X interrupt on IDX:SB [fp %d fw_sd %d igusb %d]\\n\",\n\t   fp->index, fp->fw_sb_id, fp->igu_sb_id);\n\n\tbnx2x_ack_sb(bp, fp->igu_sb_id, USTORM_ID, 0, IGU_INT_DISABLE, 0);\n\n#ifdef BNX2X_STOP_ON_ERROR\n\tif (unlikely(bp->panic))\n\t\treturn IRQ_HANDLED;\n#endif\n\n\t \n\tfor_each_cos_in_tx_queue(fp, cos)\n\t\tprefetch(fp->txdata_ptr[cos]->tx_cons_sb);\n\n\tprefetch(&fp->sb_running_index[SM_RX_ID]);\n\tnapi_schedule_irqoff(&bnx2x_fp(bp, fp->index, napi));\n\n\treturn IRQ_HANDLED;\n}\n\n \nvoid bnx2x_acquire_phy_lock(struct bnx2x *bp)\n{\n\tmutex_lock(&bp->port.phy_mutex);\n\n\tbnx2x_acquire_hw_lock(bp, HW_LOCK_RESOURCE_MDIO);\n}\n\nvoid bnx2x_release_phy_lock(struct bnx2x *bp)\n{\n\tbnx2x_release_hw_lock(bp, HW_LOCK_RESOURCE_MDIO);\n\n\tmutex_unlock(&bp->port.phy_mutex);\n}\n\n \nu16 bnx2x_get_mf_speed(struct bnx2x *bp)\n{\n\tu16 line_speed = bp->link_vars.line_speed;\n\tif (IS_MF(bp)) {\n\t\tu16 maxCfg = bnx2x_extract_max_cfg(bp,\n\t\t\t\t\t\t   bp->mf_config[BP_VN(bp)]);\n\n\t\t \n\t\tif (IS_MF_PERCENT_BW(bp))\n\t\t\tline_speed = (line_speed * maxCfg) / 100;\n\t\telse {  \n\t\t\tu16 vn_max_rate = maxCfg * 100;\n\n\t\t\tif (vn_max_rate < line_speed)\n\t\t\t\tline_speed = vn_max_rate;\n\t\t}\n\t}\n\n\treturn line_speed;\n}\n\n \nstatic void bnx2x_fill_report_data(struct bnx2x *bp,\n\t\t\t\t   struct bnx2x_link_report_data *data)\n{\n\tmemset(data, 0, sizeof(*data));\n\n\tif (IS_PF(bp)) {\n\t\t \n\t\tdata->line_speed = bnx2x_get_mf_speed(bp);\n\n\t\t \n\t\tif (!bp->link_vars.link_up || (bp->flags & MF_FUNC_DIS))\n\t\t\t__set_bit(BNX2X_LINK_REPORT_LINK_DOWN,\n\t\t\t\t  &data->link_report_flags);\n\n\t\tif (!BNX2X_NUM_ETH_QUEUES(bp))\n\t\t\t__set_bit(BNX2X_LINK_REPORT_LINK_DOWN,\n\t\t\t\t  &data->link_report_flags);\n\n\t\t \n\t\tif (bp->link_vars.duplex == DUPLEX_FULL)\n\t\t\t__set_bit(BNX2X_LINK_REPORT_FD,\n\t\t\t\t  &data->link_report_flags);\n\n\t\t \n\t\tif (bp->link_vars.flow_ctrl & BNX2X_FLOW_CTRL_RX)\n\t\t\t__set_bit(BNX2X_LINK_REPORT_RX_FC_ON,\n\t\t\t\t  &data->link_report_flags);\n\n\t\t \n\t\tif (bp->link_vars.flow_ctrl & BNX2X_FLOW_CTRL_TX)\n\t\t\t__set_bit(BNX2X_LINK_REPORT_TX_FC_ON,\n\t\t\t\t  &data->link_report_flags);\n\t} else {  \n\t\t*data = bp->vf_link_vars;\n\t}\n}\n\n \n\nvoid bnx2x_link_report(struct bnx2x *bp)\n{\n\tbnx2x_acquire_phy_lock(bp);\n\t__bnx2x_link_report(bp);\n\tbnx2x_release_phy_lock(bp);\n}\n\n \nvoid __bnx2x_link_report(struct bnx2x *bp)\n{\n\tstruct bnx2x_link_report_data cur_data;\n\n\tif (bp->force_link_down) {\n\t\tbp->link_vars.link_up = 0;\n\t\treturn;\n\t}\n\n\t \n\tif (IS_PF(bp) && !CHIP_IS_E1(bp))\n\t\tbnx2x_read_mf_cfg(bp);\n\n\t \n\tbnx2x_fill_report_data(bp, &cur_data);\n\n\t \n\tif (!memcmp(&cur_data, &bp->last_reported_link, sizeof(cur_data)) ||\n\t    (test_bit(BNX2X_LINK_REPORT_LINK_DOWN,\n\t\t      &bp->last_reported_link.link_report_flags) &&\n\t     test_bit(BNX2X_LINK_REPORT_LINK_DOWN,\n\t\t      &cur_data.link_report_flags)))\n\t\treturn;\n\n\tbp->link_cnt++;\n\n\t \n\tmemcpy(&bp->last_reported_link, &cur_data, sizeof(cur_data));\n\n\t \n\tif (IS_PF(bp))\n\t\tbnx2x_iov_link_update(bp);\n\n\tif (test_bit(BNX2X_LINK_REPORT_LINK_DOWN,\n\t\t     &cur_data.link_report_flags)) {\n\t\tnetif_carrier_off(bp->dev);\n\t\tnetdev_err(bp->dev, \"NIC Link is Down\\n\");\n\t\treturn;\n\t} else {\n\t\tconst char *duplex;\n\t\tconst char *flow;\n\n\t\tnetif_carrier_on(bp->dev);\n\n\t\tif (test_and_clear_bit(BNX2X_LINK_REPORT_FD,\n\t\t\t\t       &cur_data.link_report_flags))\n\t\t\tduplex = \"full\";\n\t\telse\n\t\t\tduplex = \"half\";\n\n\t\t \n\t\tif (cur_data.link_report_flags) {\n\t\t\tif (test_bit(BNX2X_LINK_REPORT_RX_FC_ON,\n\t\t\t\t     &cur_data.link_report_flags)) {\n\t\t\t\tif (test_bit(BNX2X_LINK_REPORT_TX_FC_ON,\n\t\t\t\t     &cur_data.link_report_flags))\n\t\t\t\t\tflow = \"ON - receive & transmit\";\n\t\t\t\telse\n\t\t\t\t\tflow = \"ON - receive\";\n\t\t\t} else {\n\t\t\t\tflow = \"ON - transmit\";\n\t\t\t}\n\t\t} else {\n\t\t\tflow = \"none\";\n\t\t}\n\t\tnetdev_info(bp->dev, \"NIC Link is Up, %d Mbps %s duplex, Flow control: %s\\n\",\n\t\t\t    cur_data.line_speed, duplex, flow);\n\t}\n}\n\nstatic void bnx2x_set_next_page_sgl(struct bnx2x_fastpath *fp)\n{\n\tint i;\n\n\tfor (i = 1; i <= NUM_RX_SGE_PAGES; i++) {\n\t\tstruct eth_rx_sge *sge;\n\n\t\tsge = &fp->rx_sge_ring[RX_SGE_CNT * i - 2];\n\t\tsge->addr_hi =\n\t\t\tcpu_to_le32(U64_HI(fp->rx_sge_mapping +\n\t\t\tBCM_PAGE_SIZE*(i % NUM_RX_SGE_PAGES)));\n\n\t\tsge->addr_lo =\n\t\t\tcpu_to_le32(U64_LO(fp->rx_sge_mapping +\n\t\t\tBCM_PAGE_SIZE*(i % NUM_RX_SGE_PAGES)));\n\t}\n}\n\nstatic void bnx2x_free_tpa_pool(struct bnx2x *bp,\n\t\t\t\tstruct bnx2x_fastpath *fp, int last)\n{\n\tint i;\n\n\tfor (i = 0; i < last; i++) {\n\t\tstruct bnx2x_agg_info *tpa_info = &fp->tpa_info[i];\n\t\tstruct sw_rx_bd *first_buf = &tpa_info->first_buf;\n\t\tu8 *data = first_buf->data;\n\n\t\tif (data == NULL) {\n\t\t\tDP(NETIF_MSG_IFDOWN, \"tpa bin %d empty on free\\n\", i);\n\t\t\tcontinue;\n\t\t}\n\t\tif (tpa_info->tpa_state == BNX2X_TPA_START)\n\t\t\tdma_unmap_single(&bp->pdev->dev,\n\t\t\t\t\t dma_unmap_addr(first_buf, mapping),\n\t\t\t\t\t fp->rx_buf_size, DMA_FROM_DEVICE);\n\t\tbnx2x_frag_free(fp, data);\n\t\tfirst_buf->data = NULL;\n\t}\n}\n\nvoid bnx2x_init_rx_rings_cnic(struct bnx2x *bp)\n{\n\tint j;\n\n\tfor_each_rx_queue_cnic(bp, j) {\n\t\tstruct bnx2x_fastpath *fp = &bp->fp[j];\n\n\t\tfp->rx_bd_cons = 0;\n\n\t\t \n\t\t \n\t\tbnx2x_update_rx_prod(bp, fp, fp->rx_bd_prod, fp->rx_comp_prod,\n\t\t\t\t     fp->rx_sge_prod);\n\t}\n}\n\nvoid bnx2x_init_rx_rings(struct bnx2x *bp)\n{\n\tint func = BP_FUNC(bp);\n\tu16 ring_prod;\n\tint i, j;\n\n\t \n\tfor_each_eth_queue(bp, j) {\n\t\tstruct bnx2x_fastpath *fp = &bp->fp[j];\n\n\t\tDP(NETIF_MSG_IFUP,\n\t\t   \"mtu %d  rx_buf_size %d\\n\", bp->dev->mtu, fp->rx_buf_size);\n\n\t\tif (fp->mode != TPA_MODE_DISABLED) {\n\t\t\t \n\t\t\tfor (i = 0; i < MAX_AGG_QS(bp); i++) {\n\t\t\t\tstruct bnx2x_agg_info *tpa_info =\n\t\t\t\t\t&fp->tpa_info[i];\n\t\t\t\tstruct sw_rx_bd *first_buf =\n\t\t\t\t\t&tpa_info->first_buf;\n\n\t\t\t\tfirst_buf->data =\n\t\t\t\t\tbnx2x_frag_alloc(fp, GFP_KERNEL);\n\t\t\t\tif (!first_buf->data) {\n\t\t\t\t\tBNX2X_ERR(\"Failed to allocate TPA skb pool for queue[%d] - disabling TPA on this queue!\\n\",\n\t\t\t\t\t\t  j);\n\t\t\t\t\tbnx2x_free_tpa_pool(bp, fp, i);\n\t\t\t\t\tfp->mode = TPA_MODE_DISABLED;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tdma_unmap_addr_set(first_buf, mapping, 0);\n\t\t\t\ttpa_info->tpa_state = BNX2X_TPA_STOP;\n\t\t\t}\n\n\t\t\t \n\t\t\tbnx2x_set_next_page_sgl(fp);\n\n\t\t\t \n\t\t\tbnx2x_init_sge_ring_bit_mask(fp);\n\n\t\t\t \n\t\t\tfor (i = 0, ring_prod = 0;\n\t\t\t     i < MAX_RX_SGE_CNT*NUM_RX_SGE_PAGES; i++) {\n\n\t\t\t\tif (bnx2x_alloc_rx_sge(bp, fp, ring_prod,\n\t\t\t\t\t\t       GFP_KERNEL) < 0) {\n\t\t\t\t\tBNX2X_ERR(\"was only able to allocate %d rx sges\\n\",\n\t\t\t\t\t\t  i);\n\t\t\t\t\tBNX2X_ERR(\"disabling TPA for queue[%d]\\n\",\n\t\t\t\t\t\t  j);\n\t\t\t\t\t \n\t\t\t\t\tbnx2x_free_rx_sge_range(bp, fp,\n\t\t\t\t\t\t\t\tring_prod);\n\t\t\t\t\tbnx2x_free_tpa_pool(bp, fp,\n\t\t\t\t\t\t\t    MAX_AGG_QS(bp));\n\t\t\t\t\tfp->mode = TPA_MODE_DISABLED;\n\t\t\t\t\tring_prod = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tring_prod = NEXT_SGE_IDX(ring_prod);\n\t\t\t}\n\n\t\t\tfp->rx_sge_prod = ring_prod;\n\t\t}\n\t}\n\n\tfor_each_eth_queue(bp, j) {\n\t\tstruct bnx2x_fastpath *fp = &bp->fp[j];\n\n\t\tfp->rx_bd_cons = 0;\n\n\t\t \n\t\t \n\t\tbnx2x_update_rx_prod(bp, fp, fp->rx_bd_prod, fp->rx_comp_prod,\n\t\t\t\t     fp->rx_sge_prod);\n\n\t\tif (j != 0)\n\t\t\tcontinue;\n\n\t\tif (CHIP_IS_E1(bp)) {\n\t\t\tREG_WR(bp, BAR_USTRORM_INTMEM +\n\t\t\t       USTORM_MEM_WORKAROUND_ADDRESS_OFFSET(func),\n\t\t\t       U64_LO(fp->rx_comp_mapping));\n\t\t\tREG_WR(bp, BAR_USTRORM_INTMEM +\n\t\t\t       USTORM_MEM_WORKAROUND_ADDRESS_OFFSET(func) + 4,\n\t\t\t       U64_HI(fp->rx_comp_mapping));\n\t\t}\n\t}\n}\n\nstatic void bnx2x_free_tx_skbs_queue(struct bnx2x_fastpath *fp)\n{\n\tu8 cos;\n\tstruct bnx2x *bp = fp->bp;\n\n\tfor_each_cos_in_tx_queue(fp, cos) {\n\t\tstruct bnx2x_fp_txdata *txdata = fp->txdata_ptr[cos];\n\t\tunsigned pkts_compl = 0, bytes_compl = 0;\n\n\t\tu16 sw_prod = txdata->tx_pkt_prod;\n\t\tu16 sw_cons = txdata->tx_pkt_cons;\n\n\t\twhile (sw_cons != sw_prod) {\n\t\t\tbnx2x_free_tx_pkt(bp, txdata, TX_BD(sw_cons),\n\t\t\t\t\t  &pkts_compl, &bytes_compl);\n\t\t\tsw_cons++;\n\t\t}\n\n\t\tnetdev_tx_reset_queue(\n\t\t\tnetdev_get_tx_queue(bp->dev,\n\t\t\t\t\t    txdata->txq_index));\n\t}\n}\n\nstatic void bnx2x_free_tx_skbs_cnic(struct bnx2x *bp)\n{\n\tint i;\n\n\tfor_each_tx_queue_cnic(bp, i) {\n\t\tbnx2x_free_tx_skbs_queue(&bp->fp[i]);\n\t}\n}\n\nstatic void bnx2x_free_tx_skbs(struct bnx2x *bp)\n{\n\tint i;\n\n\tfor_each_eth_queue(bp, i) {\n\t\tbnx2x_free_tx_skbs_queue(&bp->fp[i]);\n\t}\n}\n\nstatic void bnx2x_free_rx_bds(struct bnx2x_fastpath *fp)\n{\n\tstruct bnx2x *bp = fp->bp;\n\tint i;\n\n\t \n\tif (fp->rx_buf_ring == NULL)\n\t\treturn;\n\n\tfor (i = 0; i < NUM_RX_BD; i++) {\n\t\tstruct sw_rx_bd *rx_buf = &fp->rx_buf_ring[i];\n\t\tu8 *data = rx_buf->data;\n\n\t\tif (data == NULL)\n\t\t\tcontinue;\n\t\tdma_unmap_single(&bp->pdev->dev,\n\t\t\t\t dma_unmap_addr(rx_buf, mapping),\n\t\t\t\t fp->rx_buf_size, DMA_FROM_DEVICE);\n\n\t\trx_buf->data = NULL;\n\t\tbnx2x_frag_free(fp, data);\n\t}\n}\n\nstatic void bnx2x_free_rx_skbs_cnic(struct bnx2x *bp)\n{\n\tint j;\n\n\tfor_each_rx_queue_cnic(bp, j) {\n\t\tbnx2x_free_rx_bds(&bp->fp[j]);\n\t}\n}\n\nstatic void bnx2x_free_rx_skbs(struct bnx2x *bp)\n{\n\tint j;\n\n\tfor_each_eth_queue(bp, j) {\n\t\tstruct bnx2x_fastpath *fp = &bp->fp[j];\n\n\t\tbnx2x_free_rx_bds(fp);\n\n\t\tif (fp->mode != TPA_MODE_DISABLED)\n\t\t\tbnx2x_free_tpa_pool(bp, fp, MAX_AGG_QS(bp));\n\t}\n}\n\nstatic void bnx2x_free_skbs_cnic(struct bnx2x *bp)\n{\n\tbnx2x_free_tx_skbs_cnic(bp);\n\tbnx2x_free_rx_skbs_cnic(bp);\n}\n\nvoid bnx2x_free_skbs(struct bnx2x *bp)\n{\n\tbnx2x_free_tx_skbs(bp);\n\tbnx2x_free_rx_skbs(bp);\n}\n\nvoid bnx2x_update_max_mf_config(struct bnx2x *bp, u32 value)\n{\n\t \n\tu32 mf_cfg = bp->mf_config[BP_VN(bp)];\n\n\tif (value != bnx2x_extract_max_cfg(bp, mf_cfg)) {\n\t\t \n\t\tmf_cfg &= ~FUNC_MF_CFG_MAX_BW_MASK;\n\n\t\t \n\t\tmf_cfg |= (value << FUNC_MF_CFG_MAX_BW_SHIFT)\n\t\t\t\t& FUNC_MF_CFG_MAX_BW_MASK;\n\n\t\tbnx2x_fw_command(bp, DRV_MSG_CODE_SET_MF_BW, mf_cfg);\n\t}\n}\n\n \nstatic void bnx2x_free_msix_irqs(struct bnx2x *bp, int nvecs)\n{\n\tint i, offset = 0;\n\n\tif (nvecs == offset)\n\t\treturn;\n\n\t \n\tif (IS_PF(bp)) {\n\t\tfree_irq(bp->msix_table[offset].vector, bp->dev);\n\t\tDP(NETIF_MSG_IFDOWN, \"released sp irq (%d)\\n\",\n\t\t   bp->msix_table[offset].vector);\n\t\toffset++;\n\t}\n\n\tif (CNIC_SUPPORT(bp)) {\n\t\tif (nvecs == offset)\n\t\t\treturn;\n\t\toffset++;\n\t}\n\n\tfor_each_eth_queue(bp, i) {\n\t\tif (nvecs == offset)\n\t\t\treturn;\n\t\tDP(NETIF_MSG_IFDOWN, \"about to release fp #%d->%d irq\\n\",\n\t\t   i, bp->msix_table[offset].vector);\n\n\t\tfree_irq(bp->msix_table[offset++].vector, &bp->fp[i]);\n\t}\n}\n\nvoid bnx2x_free_irq(struct bnx2x *bp)\n{\n\tif (bp->flags & USING_MSIX_FLAG &&\n\t    !(bp->flags & USING_SINGLE_MSIX_FLAG)) {\n\t\tint nvecs = BNX2X_NUM_ETH_QUEUES(bp) + CNIC_SUPPORT(bp);\n\n\t\t \n\t\tif (IS_PF(bp))\n\t\t\tnvecs++;\n\n\t\tbnx2x_free_msix_irqs(bp, nvecs);\n\t} else {\n\t\tfree_irq(bp->dev->irq, bp->dev);\n\t}\n}\n\nint bnx2x_enable_msix(struct bnx2x *bp)\n{\n\tint msix_vec = 0, i, rc;\n\n\t \n\tif (IS_PF(bp)) {\n\t\tbp->msix_table[msix_vec].entry = msix_vec;\n\t\tBNX2X_DEV_INFO(\"msix_table[0].entry = %d (slowpath)\\n\",\n\t\t\t       bp->msix_table[0].entry);\n\t\tmsix_vec++;\n\t}\n\n\t \n\tif (CNIC_SUPPORT(bp)) {\n\t\tbp->msix_table[msix_vec].entry = msix_vec;\n\t\tBNX2X_DEV_INFO(\"msix_table[%d].entry = %d (CNIC)\\n\",\n\t\t\t       msix_vec, bp->msix_table[msix_vec].entry);\n\t\tmsix_vec++;\n\t}\n\n\t \n\tfor_each_eth_queue(bp, i) {\n\t\tbp->msix_table[msix_vec].entry = msix_vec;\n\t\tBNX2X_DEV_INFO(\"msix_table[%d].entry = %d (fastpath #%u)\\n\",\n\t\t\t       msix_vec, msix_vec, i);\n\t\tmsix_vec++;\n\t}\n\n\tDP(BNX2X_MSG_SP, \"about to request enable msix with %d vectors\\n\",\n\t   msix_vec);\n\n\trc = pci_enable_msix_range(bp->pdev, &bp->msix_table[0],\n\t\t\t\t   BNX2X_MIN_MSIX_VEC_CNT(bp), msix_vec);\n\t \n\tif (rc == -ENOSPC) {\n\t\t \n\t\trc = pci_enable_msix_range(bp->pdev, &bp->msix_table[0], 1, 1);\n\t\tif (rc < 0) {\n\t\t\tBNX2X_DEV_INFO(\"Single MSI-X is not attainable rc %d\\n\",\n\t\t\t\t       rc);\n\t\t\tgoto no_msix;\n\t\t}\n\n\t\tBNX2X_DEV_INFO(\"Using single MSI-X vector\\n\");\n\t\tbp->flags |= USING_SINGLE_MSIX_FLAG;\n\n\t\tBNX2X_DEV_INFO(\"set number of queues to 1\\n\");\n\t\tbp->num_ethernet_queues = 1;\n\t\tbp->num_queues = bp->num_ethernet_queues + bp->num_cnic_queues;\n\t} else if (rc < 0) {\n\t\tBNX2X_DEV_INFO(\"MSI-X is not attainable rc %d\\n\", rc);\n\t\tgoto no_msix;\n\t} else if (rc < msix_vec) {\n\t\t \n\t\tint diff = msix_vec - rc;\n\n\t\tBNX2X_DEV_INFO(\"Trying to use less MSI-X vectors: %d\\n\", rc);\n\n\t\t \n\t\tbp->num_ethernet_queues -= diff;\n\t\tbp->num_queues = bp->num_ethernet_queues + bp->num_cnic_queues;\n\n\t\tBNX2X_DEV_INFO(\"New queue configuration set: %d\\n\",\n\t\t\t       bp->num_queues);\n\t}\n\n\tbp->flags |= USING_MSIX_FLAG;\n\n\treturn 0;\n\nno_msix:\n\t \n\tif (rc == -ENOMEM)\n\t\tbp->flags |= DISABLE_MSI_FLAG;\n\n\treturn rc;\n}\n\nstatic int bnx2x_req_msix_irqs(struct bnx2x *bp)\n{\n\tint i, rc, offset = 0;\n\n\t \n\tif (IS_PF(bp)) {\n\t\trc = request_irq(bp->msix_table[offset++].vector,\n\t\t\t\t bnx2x_msix_sp_int, 0,\n\t\t\t\t bp->dev->name, bp->dev);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"request sp irq failed\\n\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\n\tif (CNIC_SUPPORT(bp))\n\t\toffset++;\n\n\tfor_each_eth_queue(bp, i) {\n\t\tstruct bnx2x_fastpath *fp = &bp->fp[i];\n\t\tsnprintf(fp->name, sizeof(fp->name), \"%s-fp-%d\",\n\t\t\t bp->dev->name, i);\n\n\t\trc = request_irq(bp->msix_table[offset].vector,\n\t\t\t\t bnx2x_msix_fp_int, 0, fp->name, fp);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"request fp #%d irq (%d) failed  rc %d\\n\", i,\n\t\t\t      bp->msix_table[offset].vector, rc);\n\t\t\tbnx2x_free_msix_irqs(bp, offset);\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\toffset++;\n\t}\n\n\ti = BNX2X_NUM_ETH_QUEUES(bp);\n\tif (IS_PF(bp)) {\n\t\toffset = 1 + CNIC_SUPPORT(bp);\n\t\tnetdev_info(bp->dev,\n\t\t\t    \"using MSI-X  IRQs: sp %d  fp[%d] %d ... fp[%d] %d\\n\",\n\t\t\t    bp->msix_table[0].vector,\n\t\t\t    0, bp->msix_table[offset].vector,\n\t\t\t    i - 1, bp->msix_table[offset + i - 1].vector);\n\t} else {\n\t\toffset = CNIC_SUPPORT(bp);\n\t\tnetdev_info(bp->dev,\n\t\t\t    \"using MSI-X  IRQs: fp[%d] %d ... fp[%d] %d\\n\",\n\t\t\t    0, bp->msix_table[offset].vector,\n\t\t\t    i - 1, bp->msix_table[offset + i - 1].vector);\n\t}\n\treturn 0;\n}\n\nint bnx2x_enable_msi(struct bnx2x *bp)\n{\n\tint rc;\n\n\trc = pci_enable_msi(bp->pdev);\n\tif (rc) {\n\t\tBNX2X_DEV_INFO(\"MSI is not attainable\\n\");\n\t\treturn -1;\n\t}\n\tbp->flags |= USING_MSI_FLAG;\n\n\treturn 0;\n}\n\nstatic int bnx2x_req_irq(struct bnx2x *bp)\n{\n\tunsigned long flags;\n\tunsigned int irq;\n\n\tif (bp->flags & (USING_MSI_FLAG | USING_MSIX_FLAG))\n\t\tflags = 0;\n\telse\n\t\tflags = IRQF_SHARED;\n\n\tif (bp->flags & USING_MSIX_FLAG)\n\t\tirq = bp->msix_table[0].vector;\n\telse\n\t\tirq = bp->pdev->irq;\n\n\treturn request_irq(irq, bnx2x_interrupt, flags, bp->dev->name, bp->dev);\n}\n\nstatic int bnx2x_setup_irqs(struct bnx2x *bp)\n{\n\tint rc = 0;\n\tif (bp->flags & USING_MSIX_FLAG &&\n\t    !(bp->flags & USING_SINGLE_MSIX_FLAG)) {\n\t\trc = bnx2x_req_msix_irqs(bp);\n\t\tif (rc)\n\t\t\treturn rc;\n\t} else {\n\t\trc = bnx2x_req_irq(bp);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"IRQ request failed  rc %d, aborting\\n\", rc);\n\t\t\treturn rc;\n\t\t}\n\t\tif (bp->flags & USING_MSI_FLAG) {\n\t\t\tbp->dev->irq = bp->pdev->irq;\n\t\t\tnetdev_info(bp->dev, \"using MSI IRQ %d\\n\",\n\t\t\t\t    bp->dev->irq);\n\t\t}\n\t\tif (bp->flags & USING_MSIX_FLAG) {\n\t\t\tbp->dev->irq = bp->msix_table[0].vector;\n\t\t\tnetdev_info(bp->dev, \"using MSIX IRQ %d\\n\",\n\t\t\t\t    bp->dev->irq);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void bnx2x_napi_enable_cnic(struct bnx2x *bp)\n{\n\tint i;\n\n\tfor_each_rx_queue_cnic(bp, i) {\n\t\tnapi_enable(&bnx2x_fp(bp, i, napi));\n\t}\n}\n\nstatic void bnx2x_napi_enable(struct bnx2x *bp)\n{\n\tint i;\n\n\tfor_each_eth_queue(bp, i) {\n\t\tnapi_enable(&bnx2x_fp(bp, i, napi));\n\t}\n}\n\nstatic void bnx2x_napi_disable_cnic(struct bnx2x *bp)\n{\n\tint i;\n\n\tfor_each_rx_queue_cnic(bp, i) {\n\t\tnapi_disable(&bnx2x_fp(bp, i, napi));\n\t}\n}\n\nstatic void bnx2x_napi_disable(struct bnx2x *bp)\n{\n\tint i;\n\n\tfor_each_eth_queue(bp, i) {\n\t\tnapi_disable(&bnx2x_fp(bp, i, napi));\n\t}\n}\n\nvoid bnx2x_netif_start(struct bnx2x *bp)\n{\n\tif (netif_running(bp->dev)) {\n\t\tbnx2x_napi_enable(bp);\n\t\tif (CNIC_LOADED(bp))\n\t\t\tbnx2x_napi_enable_cnic(bp);\n\t\tbnx2x_int_enable(bp);\n\t\tif (bp->state == BNX2X_STATE_OPEN)\n\t\t\tnetif_tx_wake_all_queues(bp->dev);\n\t}\n}\n\nvoid bnx2x_netif_stop(struct bnx2x *bp, int disable_hw)\n{\n\tbnx2x_int_disable_sync(bp, disable_hw);\n\tbnx2x_napi_disable(bp);\n\tif (CNIC_LOADED(bp))\n\t\tbnx2x_napi_disable_cnic(bp);\n}\n\nu16 bnx2x_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct net_device *sb_dev)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\n\tif (CNIC_LOADED(bp) && !NO_FCOE(bp)) {\n\t\tstruct ethhdr *hdr = (struct ethhdr *)skb->data;\n\t\tu16 ether_type = ntohs(hdr->h_proto);\n\n\t\t \n\t\tif (ether_type == ETH_P_8021Q) {\n\t\t\tstruct vlan_ethhdr *vhdr = skb_vlan_eth_hdr(skb);\n\n\t\t\tether_type = ntohs(vhdr->h_vlan_encapsulated_proto);\n\t\t}\n\n\t\t \n\t\tif ((ether_type == ETH_P_FCOE) || (ether_type == ETH_P_FIP))\n\t\t\treturn bnx2x_fcoe_tx(bp, txq_index);\n\t}\n\n\t \n\treturn netdev_pick_tx(dev, skb, NULL) %\n\t\t\t(BNX2X_NUM_ETH_QUEUES(bp) * bp->max_cos);\n}\n\nvoid bnx2x_set_num_queues(struct bnx2x *bp)\n{\n\t \n\tbp->num_ethernet_queues = bnx2x_calc_num_queues(bp);\n\n\t \n\tif (IS_MF_STORAGE_ONLY(bp))\n\t\tbp->num_ethernet_queues = 1;\n\n\t \n\tbp->num_cnic_queues = CNIC_SUPPORT(bp);  \n\tbp->num_queues = bp->num_ethernet_queues + bp->num_cnic_queues;\n\n\tBNX2X_DEV_INFO(\"set number of queues to %d\\n\", bp->num_queues);\n}\n\n \nstatic int bnx2x_set_real_num_queues(struct bnx2x *bp, int include_cnic)\n{\n\tint rc, tx, rx;\n\n\ttx = BNX2X_NUM_ETH_QUEUES(bp) * bp->max_cos;\n\trx = BNX2X_NUM_ETH_QUEUES(bp);\n\n \n\tif (include_cnic && !NO_FCOE(bp)) {\n\t\trx++;\n\t\ttx++;\n\t}\n\n\trc = netif_set_real_num_tx_queues(bp->dev, tx);\n\tif (rc) {\n\t\tBNX2X_ERR(\"Failed to set real number of Tx queues: %d\\n\", rc);\n\t\treturn rc;\n\t}\n\trc = netif_set_real_num_rx_queues(bp->dev, rx);\n\tif (rc) {\n\t\tBNX2X_ERR(\"Failed to set real number of Rx queues: %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tDP(NETIF_MSG_IFUP, \"Setting real num queues to (tx, rx) (%d, %d)\\n\",\n\t\t\t  tx, rx);\n\n\treturn rc;\n}\n\nstatic void bnx2x_set_rx_buf_size(struct bnx2x *bp)\n{\n\tint i;\n\n\tfor_each_queue(bp, i) {\n\t\tstruct bnx2x_fastpath *fp = &bp->fp[i];\n\t\tu32 mtu;\n\n\t\t \n\t\tif (IS_FCOE_IDX(i))\n\t\t\t \n\t\t\tmtu = BNX2X_FCOE_MINI_JUMBO_MTU;\n\t\telse\n\t\t\tmtu = bp->dev->mtu;\n\t\tfp->rx_buf_size = BNX2X_FW_RX_ALIGN_START +\n\t\t\t\t  IP_HEADER_ALIGNMENT_PADDING +\n\t\t\t\t  ETH_OVERHEAD +\n\t\t\t\t  mtu +\n\t\t\t\t  BNX2X_FW_RX_ALIGN_END;\n\t\tfp->rx_buf_size = SKB_DATA_ALIGN(fp->rx_buf_size);\n\t\t \n\t\tif (fp->rx_buf_size + NET_SKB_PAD <= PAGE_SIZE)\n\t\t\tfp->rx_frag_size = fp->rx_buf_size + NET_SKB_PAD;\n\t\telse\n\t\t\tfp->rx_frag_size = 0;\n\t}\n}\n\nstatic int bnx2x_init_rss(struct bnx2x *bp)\n{\n\tint i;\n\tu8 num_eth_queues = BNX2X_NUM_ETH_QUEUES(bp);\n\n\t \n\tfor (i = 0; i < sizeof(bp->rss_conf_obj.ind_table); i++)\n\t\tbp->rss_conf_obj.ind_table[i] =\n\t\t\tbp->fp->cl_id +\n\t\t\tethtool_rxfh_indir_default(i, num_eth_queues);\n\n\t \n\treturn bnx2x_config_rss_eth(bp, bp->port.pmf || !CHIP_IS_E1x(bp));\n}\n\nint bnx2x_rss(struct bnx2x *bp, struct bnx2x_rss_config_obj *rss_obj,\n\t      bool config_hash, bool enable)\n{\n\tstruct bnx2x_config_rss_params params = {NULL};\n\n\t \n\n\tparams.rss_obj = rss_obj;\n\n\t__set_bit(RAMROD_COMP_WAIT, &params.ramrod_flags);\n\n\tif (enable) {\n\t\t__set_bit(BNX2X_RSS_MODE_REGULAR, &params.rss_flags);\n\n\t\t \n\t\t__set_bit(BNX2X_RSS_IPV4, &params.rss_flags);\n\t\t__set_bit(BNX2X_RSS_IPV4_TCP, &params.rss_flags);\n\t\t__set_bit(BNX2X_RSS_IPV6, &params.rss_flags);\n\t\t__set_bit(BNX2X_RSS_IPV6_TCP, &params.rss_flags);\n\t\tif (rss_obj->udp_rss_v4)\n\t\t\t__set_bit(BNX2X_RSS_IPV4_UDP, &params.rss_flags);\n\t\tif (rss_obj->udp_rss_v6)\n\t\t\t__set_bit(BNX2X_RSS_IPV6_UDP, &params.rss_flags);\n\n\t\tif (!CHIP_IS_E1x(bp)) {\n\t\t\t \n\t\t\t__set_bit(BNX2X_RSS_IPV4_VXLAN, &params.rss_flags);\n\t\t\t__set_bit(BNX2X_RSS_IPV6_VXLAN, &params.rss_flags);\n\n\t\t\t \n\t\t\t__set_bit(BNX2X_RSS_TUNN_INNER_HDRS, &params.rss_flags);\n\t\t}\n\t} else {\n\t\t__set_bit(BNX2X_RSS_MODE_DISABLED, &params.rss_flags);\n\t}\n\n\t \n\tparams.rss_result_mask = MULTI_MASK;\n\n\tmemcpy(params.ind_table, rss_obj->ind_table, sizeof(params.ind_table));\n\n\tif (config_hash) {\n\t\t \n\t\tnetdev_rss_key_fill(params.rss_key, T_ETH_RSS_KEY * 4);\n\t\t__set_bit(BNX2X_RSS_SET_SRCH, &params.rss_flags);\n\t}\n\n\tif (IS_PF(bp))\n\t\treturn bnx2x_config_rss(bp, &params);\n\telse\n\t\treturn bnx2x_vfpf_config_rss(bp, &params);\n}\n\nstatic int bnx2x_init_hw(struct bnx2x *bp, u32 load_code)\n{\n\tstruct bnx2x_func_state_params func_params = {NULL};\n\n\t \n\t__set_bit(RAMROD_COMP_WAIT, &func_params.ramrod_flags);\n\n\tfunc_params.f_obj = &bp->func_obj;\n\tfunc_params.cmd = BNX2X_F_CMD_HW_INIT;\n\n\tfunc_params.params.hw_init.load_phase = load_code;\n\n\treturn bnx2x_func_state_change(bp, &func_params);\n}\n\n \nvoid bnx2x_squeeze_objects(struct bnx2x *bp)\n{\n\tint rc;\n\tunsigned long ramrod_flags = 0, vlan_mac_flags = 0;\n\tstruct bnx2x_mcast_ramrod_params rparam = {NULL};\n\tstruct bnx2x_vlan_mac_obj *mac_obj = &bp->sp_objs->mac_obj;\n\n\t \n\n\t \n\t__set_bit(RAMROD_COMP_WAIT, &ramrod_flags);\n\t \n\t__set_bit(RAMROD_DRV_CLR_ONLY, &ramrod_flags);\n\n\t \n\t__set_bit(BNX2X_ETH_MAC, &vlan_mac_flags);\n\trc = mac_obj->delete_all(bp, &bp->sp_objs->mac_obj, &vlan_mac_flags,\n\t\t\t\t &ramrod_flags);\n\tif (rc != 0)\n\t\tBNX2X_ERR(\"Failed to clean ETH MACs: %d\\n\", rc);\n\n\t \n\tvlan_mac_flags = 0;\n\t__set_bit(BNX2X_UC_LIST_MAC, &vlan_mac_flags);\n\trc = mac_obj->delete_all(bp, mac_obj, &vlan_mac_flags,\n\t\t\t\t &ramrod_flags);\n\tif (rc != 0)\n\t\tBNX2X_ERR(\"Failed to clean UC list MACs: %d\\n\", rc);\n\n\t \n\trparam.mcast_obj = &bp->mcast_obj;\n\t__set_bit(RAMROD_DRV_CLR_ONLY, &rparam.ramrod_flags);\n\n\t \n\tnetif_addr_lock_bh(bp->dev);\n\trc = bnx2x_config_mcast(bp, &rparam, BNX2X_MCAST_CMD_DEL);\n\tif (rc < 0)\n\t\tBNX2X_ERR(\"Failed to add a new DEL command to a multi-cast object: %d\\n\",\n\t\t\t  rc);\n\n\t \n\trc = bnx2x_config_mcast(bp, &rparam, BNX2X_MCAST_CMD_CONT);\n\twhile (rc != 0) {\n\t\tif (rc < 0) {\n\t\t\tBNX2X_ERR(\"Failed to clean multi-cast object: %d\\n\",\n\t\t\t\t  rc);\n\t\t\tnetif_addr_unlock_bh(bp->dev);\n\t\t\treturn;\n\t\t}\n\n\t\trc = bnx2x_config_mcast(bp, &rparam, BNX2X_MCAST_CMD_CONT);\n\t}\n\tnetif_addr_unlock_bh(bp->dev);\n}\n\n#ifndef BNX2X_STOP_ON_ERROR\n#define LOAD_ERROR_EXIT(bp, label) \\\n\tdo { \\\n\t\t(bp)->state = BNX2X_STATE_ERROR; \\\n\t\tgoto label; \\\n\t} while (0)\n\n#define LOAD_ERROR_EXIT_CNIC(bp, label) \\\n\tdo { \\\n\t\tbp->cnic_loaded = false; \\\n\t\tgoto label; \\\n\t} while (0)\n#else  \n#define LOAD_ERROR_EXIT(bp, label) \\\n\tdo { \\\n\t\t(bp)->state = BNX2X_STATE_ERROR; \\\n\t\t(bp)->panic = 1; \\\n\t\treturn -EBUSY; \\\n\t} while (0)\n#define LOAD_ERROR_EXIT_CNIC(bp, label) \\\n\tdo { \\\n\t\tbp->cnic_loaded = false; \\\n\t\t(bp)->panic = 1; \\\n\t\treturn -EBUSY; \\\n\t} while (0)\n#endif  \n\nstatic void bnx2x_free_fw_stats_mem(struct bnx2x *bp)\n{\n\tBNX2X_PCI_FREE(bp->fw_stats, bp->fw_stats_mapping,\n\t\t       bp->fw_stats_data_sz + bp->fw_stats_req_sz);\n\treturn;\n}\n\nstatic int bnx2x_alloc_fw_stats_mem(struct bnx2x *bp)\n{\n\tint num_groups, vf_headroom = 0;\n\tint is_fcoe_stats = NO_FCOE(bp) ? 0 : 1;\n\n\t \n\tu8 num_queue_stats = BNX2X_NUM_ETH_QUEUES(bp) + is_fcoe_stats;\n\n\t \n\tbp->fw_stats_num = 2 + is_fcoe_stats + num_queue_stats;\n\n\t \n\tif (IS_SRIOV(bp))\n\t\tvf_headroom = bnx2x_vf_headroom(bp);\n\n\t \n\tnum_groups =\n\t\t(((bp->fw_stats_num + vf_headroom) / STATS_QUERY_CMD_COUNT) +\n\t\t (((bp->fw_stats_num + vf_headroom) % STATS_QUERY_CMD_COUNT) ?\n\t\t 1 : 0));\n\n\tDP(BNX2X_MSG_SP, \"stats fw_stats_num %d, vf headroom %d, num_groups %d\\n\",\n\t   bp->fw_stats_num, vf_headroom, num_groups);\n\tbp->fw_stats_req_sz = sizeof(struct stats_query_header) +\n\t\tnum_groups * sizeof(struct stats_query_cmd_group);\n\n\t \n\tbp->fw_stats_data_sz = sizeof(struct per_port_stats) +\n\t\tsizeof(struct per_pf_stats) +\n\t\tsizeof(struct fcoe_statistics_params) +\n\t\tsizeof(struct per_queue_stats) * num_queue_stats +\n\t\tsizeof(struct stats_counter);\n\n\tbp->fw_stats = BNX2X_PCI_ALLOC(&bp->fw_stats_mapping,\n\t\t\t\t       bp->fw_stats_data_sz + bp->fw_stats_req_sz);\n\tif (!bp->fw_stats)\n\t\tgoto alloc_mem_err;\n\n\t \n\tbp->fw_stats_req = (struct bnx2x_fw_stats_req *)bp->fw_stats;\n\tbp->fw_stats_req_mapping = bp->fw_stats_mapping;\n\tbp->fw_stats_data = (struct bnx2x_fw_stats_data *)\n\t\t((u8 *)bp->fw_stats + bp->fw_stats_req_sz);\n\tbp->fw_stats_data_mapping = bp->fw_stats_mapping +\n\t\tbp->fw_stats_req_sz;\n\n\tDP(BNX2X_MSG_SP, \"statistics request base address set to %x %x\\n\",\n\t   U64_HI(bp->fw_stats_req_mapping),\n\t   U64_LO(bp->fw_stats_req_mapping));\n\tDP(BNX2X_MSG_SP, \"statistics data base address set to %x %x\\n\",\n\t   U64_HI(bp->fw_stats_data_mapping),\n\t   U64_LO(bp->fw_stats_data_mapping));\n\treturn 0;\n\nalloc_mem_err:\n\tbnx2x_free_fw_stats_mem(bp);\n\tBNX2X_ERR(\"Can't allocate FW stats memory\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int bnx2x_nic_load_request(struct bnx2x *bp, u32 *load_code)\n{\n\tu32 param;\n\n\t \n\tbp->fw_seq =\n\t\t(SHMEM_RD(bp, func_mb[BP_FW_MB_IDX(bp)].drv_mb_header) &\n\t\t DRV_MSG_SEQ_NUMBER_MASK);\n\tBNX2X_DEV_INFO(\"fw_seq 0x%08x\\n\", bp->fw_seq);\n\n\t \n\tbp->fw_drv_pulse_wr_seq =\n\t\t(SHMEM_RD(bp, func_mb[BP_FW_MB_IDX(bp)].drv_pulse_mb) &\n\t\t DRV_PULSE_SEQ_MASK);\n\tBNX2X_DEV_INFO(\"drv_pulse 0x%x\\n\", bp->fw_drv_pulse_wr_seq);\n\n\tparam = DRV_MSG_CODE_LOAD_REQ_WITH_LFA;\n\n\tif (IS_MF_SD(bp) && bnx2x_port_after_undi(bp))\n\t\tparam |= DRV_MSG_CODE_LOAD_REQ_FORCE_LFA;\n\n\t \n\t(*load_code) = bnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_REQ, param);\n\n\t \n\tif (!(*load_code)) {\n\t\tBNX2X_ERR(\"MCP response failure, aborting\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tif ((*load_code) == FW_MSG_CODE_DRV_LOAD_REFUSED) {\n\t\tBNX2X_ERR(\"MCP refused load request, aborting\\n\");\n\t\treturn -EBUSY;\n\t}\n\treturn 0;\n}\n\n \nint bnx2x_compare_fw_ver(struct bnx2x *bp, u32 load_code, bool print_err)\n{\n\t \n\tif (load_code != FW_MSG_CODE_DRV_LOAD_COMMON_CHIP &&\n\t    load_code != FW_MSG_CODE_DRV_LOAD_COMMON) {\n\t\tu8 loaded_fw_major, loaded_fw_minor, loaded_fw_rev, loaded_fw_eng;\n\t\tu32 loaded_fw;\n\n\t\t \n\t\tloaded_fw = REG_RD(bp, XSEM_REG_PRAM);\n\n\t\tloaded_fw_major = loaded_fw & 0xff;\n\t\tloaded_fw_minor = (loaded_fw >> 8) & 0xff;\n\t\tloaded_fw_rev = (loaded_fw >> 16) & 0xff;\n\t\tloaded_fw_eng = (loaded_fw >> 24) & 0xff;\n\n\t\tDP(BNX2X_MSG_SP, \"loaded fw 0x%x major 0x%x minor 0x%x rev 0x%x eng 0x%x\\n\",\n\t\t   loaded_fw, loaded_fw_major, loaded_fw_minor, loaded_fw_rev, loaded_fw_eng);\n\n\t\t \n\t\tif (loaded_fw_major != BCM_5710_FW_MAJOR_VERSION ||\n\t\t    loaded_fw_minor != BCM_5710_FW_MINOR_VERSION ||\n\t\t    loaded_fw_eng != BCM_5710_FW_ENGINEERING_VERSION ||\n\t\t    loaded_fw_rev < BCM_5710_FW_REVISION_VERSION_V15) {\n\t\t\tif (print_err)\n\t\t\t\tBNX2X_ERR(\"loaded FW incompatible. Aborting\\n\");\n\t\t\telse\n\t\t\t\tBNX2X_DEV_INFO(\"loaded FW incompatible, possibly due to MF UNDI\\n\");\n\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic int bnx2x_nic_load_no_mcp(struct bnx2x *bp, int port)\n{\n\tint path = BP_PATH(bp);\n\n\tDP(NETIF_MSG_IFUP, \"NO MCP - load counts[%d]      %d, %d, %d\\n\",\n\t   path, bnx2x_load_count[path][0], bnx2x_load_count[path][1],\n\t   bnx2x_load_count[path][2]);\n\tbnx2x_load_count[path][0]++;\n\tbnx2x_load_count[path][1 + port]++;\n\tDP(NETIF_MSG_IFUP, \"NO MCP - new load counts[%d]  %d, %d, %d\\n\",\n\t   path, bnx2x_load_count[path][0], bnx2x_load_count[path][1],\n\t   bnx2x_load_count[path][2]);\n\tif (bnx2x_load_count[path][0] == 1)\n\t\treturn FW_MSG_CODE_DRV_LOAD_COMMON;\n\telse if (bnx2x_load_count[path][1 + port] == 1)\n\t\treturn FW_MSG_CODE_DRV_LOAD_PORT;\n\telse\n\t\treturn FW_MSG_CODE_DRV_LOAD_FUNCTION;\n}\n\n \nstatic void bnx2x_nic_load_pmf(struct bnx2x *bp, u32 load_code)\n{\n\tif ((load_code == FW_MSG_CODE_DRV_LOAD_COMMON) ||\n\t    (load_code == FW_MSG_CODE_DRV_LOAD_COMMON_CHIP) ||\n\t    (load_code == FW_MSG_CODE_DRV_LOAD_PORT)) {\n\t\tbp->port.pmf = 1;\n\t\t \n\t\tsmp_mb();\n\t} else {\n\t\tbp->port.pmf = 0;\n\t}\n\n\tDP(NETIF_MSG_LINK, \"pmf %d\\n\", bp->port.pmf);\n}\n\nstatic void bnx2x_nic_load_afex_dcc(struct bnx2x *bp, int load_code)\n{\n\tif (((load_code == FW_MSG_CODE_DRV_LOAD_COMMON) ||\n\t     (load_code == FW_MSG_CODE_DRV_LOAD_COMMON_CHIP)) &&\n\t    (bp->common.shmem2_base)) {\n\t\tif (SHMEM2_HAS(bp, dcc_support))\n\t\t\tSHMEM2_WR(bp, dcc_support,\n\t\t\t\t  (SHMEM_DCC_SUPPORT_DISABLE_ENABLE_PF_TLV |\n\t\t\t\t   SHMEM_DCC_SUPPORT_BANDWIDTH_ALLOCATION_TLV));\n\t\tif (SHMEM2_HAS(bp, afex_driver_support))\n\t\t\tSHMEM2_WR(bp, afex_driver_support,\n\t\t\t\t  SHMEM_AFEX_SUPPORTED_VERSION_ONE);\n\t}\n\n\t \n\tbp->afex_def_vlan_tag = -1;\n}\n\n \nstatic void bnx2x_bz_fp(struct bnx2x *bp, int index)\n{\n\tstruct bnx2x_fastpath *fp = &bp->fp[index];\n\tint cos;\n\tstruct napi_struct orig_napi = fp->napi;\n\tstruct bnx2x_agg_info *orig_tpa_info = fp->tpa_info;\n\n\t \n\tif (fp->tpa_info)\n\t\tmemset(fp->tpa_info, 0, ETH_MAX_AGGREGATION_QUEUES_E1H_E2 *\n\t\t       sizeof(struct bnx2x_agg_info));\n\tmemset(fp, 0, sizeof(*fp));\n\n\t \n\tfp->napi = orig_napi;\n\tfp->tpa_info = orig_tpa_info;\n\tfp->bp = bp;\n\tfp->index = index;\n\tif (IS_ETH_FP(fp))\n\t\tfp->max_cos = bp->max_cos;\n\telse\n\t\t \n\t\tfp->max_cos = 1;\n\n\t \n\tif (IS_FCOE_FP(fp))\n\t\tfp->txdata_ptr[0] = &bp->bnx2x_txq[FCOE_TXQ_IDX(bp)];\n\tif (IS_ETH_FP(fp))\n\t\tfor_each_cos_in_tx_queue(fp, cos)\n\t\t\tfp->txdata_ptr[cos] = &bp->bnx2x_txq[cos *\n\t\t\t\tBNX2X_NUM_ETH_QUEUES(bp) + index];\n\n\t \n\tif (bp->dev->features & NETIF_F_LRO)\n\t\tfp->mode = TPA_MODE_LRO;\n\telse if (bp->dev->features & NETIF_F_GRO_HW)\n\t\tfp->mode = TPA_MODE_GRO;\n\telse\n\t\tfp->mode = TPA_MODE_DISABLED;\n\n\t \n\tif (bp->disable_tpa || IS_FCOE_FP(fp))\n\t\tfp->mode = TPA_MODE_DISABLED;\n}\n\nvoid bnx2x_set_os_driver_state(struct bnx2x *bp, u32 state)\n{\n\tu32 cur;\n\n\tif (!IS_MF_BD(bp) || !SHMEM2_HAS(bp, os_driver_state) || IS_VF(bp))\n\t\treturn;\n\n\tcur = SHMEM2_RD(bp, os_driver_state[BP_FW_MB_IDX(bp)]);\n\tDP(NETIF_MSG_IFUP, \"Driver state %08x-->%08x\\n\",\n\t   cur, state);\n\n\tSHMEM2_WR(bp, os_driver_state[BP_FW_MB_IDX(bp)], state);\n}\n\nint bnx2x_load_cnic(struct bnx2x *bp)\n{\n\tint i, rc, port = BP_PORT(bp);\n\n\tDP(NETIF_MSG_IFUP, \"Starting CNIC-related load\\n\");\n\n\tmutex_init(&bp->cnic_mutex);\n\n\tif (IS_PF(bp)) {\n\t\trc = bnx2x_alloc_mem_cnic(bp);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"Unable to allocate bp memory for cnic\\n\");\n\t\t\tLOAD_ERROR_EXIT_CNIC(bp, load_error_cnic0);\n\t\t}\n\t}\n\n\trc = bnx2x_alloc_fp_mem_cnic(bp);\n\tif (rc) {\n\t\tBNX2X_ERR(\"Unable to allocate memory for cnic fps\\n\");\n\t\tLOAD_ERROR_EXIT_CNIC(bp, load_error_cnic0);\n\t}\n\n\t \n\trc = bnx2x_set_real_num_queues(bp, 1);\n\tif (rc) {\n\t\tBNX2X_ERR(\"Unable to set real_num_queues including cnic\\n\");\n\t\tLOAD_ERROR_EXIT_CNIC(bp, load_error_cnic0);\n\t}\n\n\t \n\tbnx2x_add_all_napi_cnic(bp);\n\tDP(NETIF_MSG_IFUP, \"cnic napi added\\n\");\n\tbnx2x_napi_enable_cnic(bp);\n\n\trc = bnx2x_init_hw_func_cnic(bp);\n\tif (rc)\n\t\tLOAD_ERROR_EXIT_CNIC(bp, load_error_cnic1);\n\n\tbnx2x_nic_init_cnic(bp);\n\n\tif (IS_PF(bp)) {\n\t\t \n\t\tREG_WR(bp, TM_REG_EN_LINEAR0_TIMER + port*4, 1);\n\n\t\t \n\t\tfor_each_cnic_queue(bp, i) {\n\t\t\trc = bnx2x_setup_queue(bp, &bp->fp[i], 0);\n\t\t\tif (rc) {\n\t\t\t\tBNX2X_ERR(\"Queue setup failed\\n\");\n\t\t\t\tLOAD_ERROR_EXIT(bp, load_error_cnic2);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tbnx2x_set_rx_mode_inner(bp);\n\n\t \n\tbnx2x_get_iscsi_info(bp);\n\tbnx2x_setup_cnic_irq_info(bp);\n\tbnx2x_setup_cnic_info(bp);\n\tbp->cnic_loaded = true;\n\tif (bp->state == BNX2X_STATE_OPEN)\n\t\tbnx2x_cnic_notify(bp, CNIC_CTL_START_CMD);\n\n\tDP(NETIF_MSG_IFUP, \"Ending successfully CNIC-related load\\n\");\n\n\treturn 0;\n\n#ifndef BNX2X_STOP_ON_ERROR\nload_error_cnic2:\n\t \n\tREG_WR(bp, TM_REG_EN_LINEAR0_TIMER + port*4, 0);\n\nload_error_cnic1:\n\tbnx2x_napi_disable_cnic(bp);\n\t \n\tif (bnx2x_set_real_num_queues(bp, 0))\n\t\tBNX2X_ERR(\"Unable to set real_num_queues not including cnic\\n\");\nload_error_cnic0:\n\tBNX2X_ERR(\"CNIC-related load failed\\n\");\n\tbnx2x_free_fp_mem_cnic(bp);\n\tbnx2x_free_mem_cnic(bp);\n\treturn rc;\n#endif  \n}\n\n \nint bnx2x_nic_load(struct bnx2x *bp, int load_mode)\n{\n\tint port = BP_PORT(bp);\n\tint i, rc = 0, load_code = 0;\n\n\tDP(NETIF_MSG_IFUP, \"Starting NIC load\\n\");\n\tDP(NETIF_MSG_IFUP,\n\t   \"CNIC is %s\\n\", CNIC_ENABLED(bp) ? \"enabled\" : \"disabled\");\n\n#ifdef BNX2X_STOP_ON_ERROR\n\tif (unlikely(bp->panic)) {\n\t\tBNX2X_ERR(\"Can't load NIC when there is panic\\n\");\n\t\treturn -EPERM;\n\t}\n#endif\n\n\tbp->state = BNX2X_STATE_OPENING_WAIT4_LOAD;\n\n\t \n\tmemset(&bp->last_reported_link, 0, sizeof(bp->last_reported_link));\n\t__set_bit(BNX2X_LINK_REPORT_LINK_DOWN,\n\t\t&bp->last_reported_link.link_report_flags);\n\n\tif (IS_PF(bp))\n\t\t \n\t\tbnx2x_ilt_set_info(bp);\n\n\t \n\tDP(NETIF_MSG_IFUP, \"num queues: %d\", bp->num_queues);\n\tfor_each_queue(bp, i)\n\t\tbnx2x_bz_fp(bp, i);\n\tmemset(bp->bnx2x_txq, 0, (BNX2X_MAX_RSS_COUNT(bp) * BNX2X_MULTI_TX_COS +\n\t\t\t\t  bp->num_cnic_queues) *\n\t\t\t\t  sizeof(struct bnx2x_fp_txdata));\n\n\tbp->fcoe_init = false;\n\n\t \n\tbnx2x_set_rx_buf_size(bp);\n\n\tif (IS_PF(bp)) {\n\t\trc = bnx2x_alloc_mem(bp);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"Unable to allocate bp memory\\n\");\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t \n\trc = bnx2x_alloc_fp_mem(bp);\n\tif (rc) {\n\t\tBNX2X_ERR(\"Unable to allocate memory for fps\\n\");\n\t\tLOAD_ERROR_EXIT(bp, load_error0);\n\t}\n\n\t \n\trc = bnx2x_alloc_fw_stats_mem(bp);\n\tif (rc)\n\t\tLOAD_ERROR_EXIT(bp, load_error0);\n\n\t \n\tif (IS_VF(bp)) {\n\t\trc = bnx2x_vfpf_init(bp);\n\t\tif (rc)\n\t\t\tLOAD_ERROR_EXIT(bp, load_error0);\n\t}\n\n\t \n\trc = bnx2x_set_real_num_queues(bp, 0);\n\tif (rc) {\n\t\tBNX2X_ERR(\"Unable to set real_num_queues\\n\");\n\t\tLOAD_ERROR_EXIT(bp, load_error0);\n\t}\n\n\t \n\tbnx2x_setup_tc(bp->dev, bp->max_cos);\n\n\t \n\tbnx2x_add_all_napi(bp);\n\tDP(NETIF_MSG_IFUP, \"napi added\\n\");\n\tbnx2x_napi_enable(bp);\n\tbp->nic_stopped = false;\n\n\tif (IS_PF(bp)) {\n\t\t \n\t\tbnx2x_set_pf_load(bp);\n\n\t\t \n\t\tif (!BP_NOMCP(bp)) {\n\t\t\t \n\t\t\trc = bnx2x_nic_load_request(bp, &load_code);\n\t\t\tif (rc)\n\t\t\t\tLOAD_ERROR_EXIT(bp, load_error1);\n\n\t\t\t \n\t\t\trc = bnx2x_compare_fw_ver(bp, load_code, true);\n\t\t\tif (rc) {\n\t\t\t\tbnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_DONE, 0);\n\t\t\t\tLOAD_ERROR_EXIT(bp, load_error2);\n\t\t\t}\n\t\t} else {\n\t\t\tload_code = bnx2x_nic_load_no_mcp(bp, port);\n\t\t}\n\n\t\t \n\t\tbnx2x_nic_load_pmf(bp, load_code);\n\n\t\t \n\t\tbnx2x__init_func_obj(bp);\n\n\t\t \n\t\trc = bnx2x_init_hw(bp, load_code);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"HW init failed, aborting\\n\");\n\t\t\tbnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_DONE, 0);\n\t\t\tLOAD_ERROR_EXIT(bp, load_error2);\n\t\t}\n\t}\n\n\tbnx2x_pre_irq_nic_init(bp);\n\n\t \n\trc = bnx2x_setup_irqs(bp);\n\tif (rc) {\n\t\tBNX2X_ERR(\"setup irqs failed\\n\");\n\t\tif (IS_PF(bp))\n\t\t\tbnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_DONE, 0);\n\t\tLOAD_ERROR_EXIT(bp, load_error2);\n\t}\n\n\t \n\tif (IS_PF(bp)) {\n\t\t \n\t\tbnx2x_post_irq_nic_init(bp, load_code);\n\n\t\tbnx2x_init_bp_objs(bp);\n\t\tbnx2x_iov_nic_init(bp);\n\n\t\t \n\t\tbp->afex_def_vlan_tag = -1;\n\t\tbnx2x_nic_load_afex_dcc(bp, load_code);\n\t\tbp->state = BNX2X_STATE_OPENING_WAIT4_PORT;\n\t\trc = bnx2x_func_start(bp);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"Function start failed!\\n\");\n\t\t\tbnx2x_fw_command(bp, DRV_MSG_CODE_LOAD_DONE, 0);\n\n\t\t\tLOAD_ERROR_EXIT(bp, load_error3);\n\t\t}\n\n\t\t \n\t\tif (!BP_NOMCP(bp)) {\n\t\t\tload_code = bnx2x_fw_command(bp,\n\t\t\t\t\t\t     DRV_MSG_CODE_LOAD_DONE, 0);\n\t\t\tif (!load_code) {\n\t\t\t\tBNX2X_ERR(\"MCP response failure, aborting\\n\");\n\t\t\t\trc = -EBUSY;\n\t\t\t\tLOAD_ERROR_EXIT(bp, load_error3);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tbnx2x_update_coalesce(bp);\n\t}\n\n\t \n\trc = bnx2x_setup_leading(bp);\n\tif (rc) {\n\t\tBNX2X_ERR(\"Setup leading failed!\\n\");\n\t\tLOAD_ERROR_EXIT(bp, load_error3);\n\t}\n\n\t \n\tfor_each_nondefault_eth_queue(bp, i) {\n\t\tif (IS_PF(bp))\n\t\t\trc = bnx2x_setup_queue(bp, &bp->fp[i], false);\n\t\telse  \n\t\t\trc = bnx2x_vfpf_setup_q(bp, &bp->fp[i], false);\n\t\tif (rc) {\n\t\t\tBNX2X_ERR(\"Queue %d setup failed\\n\", i);\n\t\t\tLOAD_ERROR_EXIT(bp, load_error3);\n\t\t}\n\t}\n\n\t \n\trc = bnx2x_init_rss(bp);\n\tif (rc) {\n\t\tBNX2X_ERR(\"PF RSS init failed\\n\");\n\t\tLOAD_ERROR_EXIT(bp, load_error3);\n\t}\n\n\t \n\tbp->state = BNX2X_STATE_OPEN;\n\n\t \n\tif (IS_PF(bp))\n\t\trc = bnx2x_set_eth_mac(bp, true);\n\telse  \n\t\trc = bnx2x_vfpf_config_mac(bp, bp->dev->dev_addr, bp->fp->index,\n\t\t\t\t\t   true);\n\tif (rc) {\n\t\tBNX2X_ERR(\"Setting Ethernet MAC failed\\n\");\n\t\tLOAD_ERROR_EXIT(bp, load_error3);\n\t}\n\n\tif (IS_PF(bp) && bp->pending_max) {\n\t\tbnx2x_update_max_mf_config(bp, bp->pending_max);\n\t\tbp->pending_max = 0;\n\t}\n\n\tbp->force_link_down = false;\n\tif (bp->port.pmf) {\n\t\trc = bnx2x_initial_phy_init(bp, load_mode);\n\t\tif (rc)\n\t\t\tLOAD_ERROR_EXIT(bp, load_error3);\n\t}\n\tbp->link_params.feature_config_flags &= ~FEATURE_CONFIG_BOOT_FROM_SAN;\n\n\t \n\n\t \n\trc = bnx2x_vlan_reconfigure_vid(bp);\n\tif (rc)\n\t\tLOAD_ERROR_EXIT(bp, load_error3);\n\n\t \n\tbnx2x_set_rx_mode_inner(bp);\n\n\tif (bp->flags & PTP_SUPPORTED) {\n\t\tbnx2x_register_phc(bp);\n\t\tbnx2x_init_ptp(bp);\n\t\tbnx2x_configure_ptp_filters(bp);\n\t}\n\t \n\tswitch (load_mode) {\n\tcase LOAD_NORMAL:\n\t\t \n\t\tnetif_tx_wake_all_queues(bp->dev);\n\t\tbreak;\n\n\tcase LOAD_OPEN:\n\t\tnetif_tx_start_all_queues(bp->dev);\n\t\tsmp_mb__after_atomic();\n\t\tbreak;\n\n\tcase LOAD_DIAG:\n\tcase LOAD_LOOPBACK_EXT:\n\t\tbp->state = BNX2X_STATE_DIAG;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (bp->port.pmf)\n\t\tbnx2x_update_drv_flags(bp, 1 << DRV_FLAGS_PORT_MASK, 0);\n\telse\n\t\tbnx2x__link_status_update(bp);\n\n\t \n\tmod_timer(&bp->timer, jiffies + bp->current_interval);\n\n\tif (CNIC_ENABLED(bp))\n\t\tbnx2x_load_cnic(bp);\n\n\tif (IS_PF(bp))\n\t\tbnx2x_schedule_sp_rtnl(bp, BNX2X_SP_RTNL_GET_DRV_VERSION, 0);\n\n\tif (IS_PF(bp) && SHMEM2_HAS(bp, drv_capabilities_flag)) {\n\t\t \n\t\tu32 val;\n\t\tval = SHMEM2_RD(bp, drv_capabilities_flag[BP_FW_MB_IDX(bp)]);\n\t\tval &= ~DRV_FLAGS_MTU_MASK;\n\t\tval |= (bp->dev->mtu << DRV_FLAGS_MTU_SHIFT);\n\t\tSHMEM2_WR(bp, drv_capabilities_flag[BP_FW_MB_IDX(bp)],\n\t\t\t  val | DRV_FLAGS_CAPABILITIES_LOADED_SUPPORTED |\n\t\t\t  DRV_FLAGS_CAPABILITIES_LOADED_L2);\n\t}\n\n\t \n\tif (IS_PF(bp) && !bnx2x_wait_sp_comp(bp, ~0x0UL)) {\n\t\tBNX2X_ERR(\"Timeout waiting for SP elements to complete\\n\");\n\t\tbnx2x_nic_unload(bp, UNLOAD_CLOSE, false);\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tif (IS_PF(bp))\n\t\tbnx2x_update_mfw_dump(bp);\n\n\t \n\tif (bp->port.pmf && (bp->state != BNX2X_STATE_DIAG))\n\t\tbnx2x_dcbx_init(bp, false);\n\n\tif (!IS_MF_SD_STORAGE_PERSONALITY_ONLY(bp))\n\t\tbnx2x_set_os_driver_state(bp, OS_DRIVER_STATE_ACTIVE);\n\n\tDP(NETIF_MSG_IFUP, \"Ending successfully NIC load\\n\");\n\n\treturn 0;\n\n#ifndef BNX2X_STOP_ON_ERROR\nload_error3:\n\tif (IS_PF(bp)) {\n\t\tbnx2x_int_disable_sync(bp, 1);\n\n\t\t \n\t\tbnx2x_squeeze_objects(bp);\n\t}\n\n\t \n\tbnx2x_free_skbs(bp);\n\tfor_each_rx_queue(bp, i)\n\t\tbnx2x_free_rx_sge_range(bp, bp->fp + i, NUM_RX_SGE);\n\n\t \n\tbnx2x_free_irq(bp);\nload_error2:\n\tif (IS_PF(bp) && !BP_NOMCP(bp)) {\n\t\tbnx2x_fw_command(bp, DRV_MSG_CODE_UNLOAD_REQ_WOL_MCP, 0);\n\t\tbnx2x_fw_command(bp, DRV_MSG_CODE_UNLOAD_DONE, 0);\n\t}\n\n\tbp->port.pmf = 0;\nload_error1:\n\tbnx2x_napi_disable(bp);\n\tbnx2x_del_all_napi(bp);\n\tbp->nic_stopped = true;\n\n\t \n\tif (IS_PF(bp))\n\t\tbnx2x_clear_pf_load(bp);\nload_error0:\n\tbnx2x_free_fw_stats_mem(bp);\n\tbnx2x_free_fp_mem(bp);\n\tbnx2x_free_mem(bp);\n\n\treturn rc;\n#endif  \n}\n\nint bnx2x_drain_tx_queues(struct bnx2x *bp)\n{\n\tu8 rc = 0, cos, i;\n\n\t \n\tfor_each_tx_queue(bp, i) {\n\t\tstruct bnx2x_fastpath *fp = &bp->fp[i];\n\n\t\tfor_each_cos_in_tx_queue(fp, cos)\n\t\t\trc = bnx2x_clean_tx_queue(bp, fp->txdata_ptr[cos]);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\treturn 0;\n}\n\n \nint bnx2x_nic_unload(struct bnx2x *bp, int unload_mode, bool keep_link)\n{\n\tint i;\n\tbool global = false;\n\n\tDP(NETIF_MSG_IFUP, \"Starting NIC unload\\n\");\n\n\tif (!IS_MF_SD_STORAGE_PERSONALITY_ONLY(bp))\n\t\tbnx2x_set_os_driver_state(bp, OS_DRIVER_STATE_DISABLED);\n\n\t \n\tif (IS_PF(bp) && SHMEM2_HAS(bp, drv_capabilities_flag)) {\n\t\tu32 val;\n\t\tval = SHMEM2_RD(bp, drv_capabilities_flag[BP_FW_MB_IDX(bp)]);\n\t\tSHMEM2_WR(bp, drv_capabilities_flag[BP_FW_MB_IDX(bp)],\n\t\t\t  val & ~DRV_FLAGS_CAPABILITIES_LOADED_L2);\n\t}\n\n\tif (IS_PF(bp) && bp->recovery_state != BNX2X_RECOVERY_DONE &&\n\t    (bp->state == BNX2X_STATE_CLOSED ||\n\t     bp->state == BNX2X_STATE_ERROR)) {\n\t\t \n\t\tbp->recovery_state = BNX2X_RECOVERY_DONE;\n\t\tbp->is_leader = 0;\n\t\tbnx2x_release_leader_lock(bp);\n\t\tsmp_mb();\n\n\t\tDP(NETIF_MSG_IFDOWN, \"Releasing a leadership...\\n\");\n\t\tBNX2X_ERR(\"Can't unload in closed or error state\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (bp->state == BNX2X_STATE_CLOSED || bp->state == BNX2X_STATE_ERROR)\n\t\treturn 0;\n\n\t \n\tbp->state = BNX2X_STATE_CLOSING_WAIT4_HALT;\n\tsmp_mb();\n\n\t \n\tbnx2x_iov_channel_down(bp);\n\n\tif (CNIC_LOADED(bp))\n\t\tbnx2x_cnic_notify(bp, CNIC_CTL_STOP_CMD);\n\n\t \n\tbnx2x_tx_disable(bp);\n\tnetdev_reset_tc(bp->dev);\n\n\tbp->rx_mode = BNX2X_RX_MODE_NONE;\n\n\tdel_timer_sync(&bp->timer);\n\n\tif (IS_PF(bp) && !BP_NOMCP(bp)) {\n\t\t \n\t\tbp->fw_drv_pulse_wr_seq |= DRV_PULSE_ALWAYS_ALIVE;\n\t\tbnx2x_drv_pulse(bp);\n\t\tbnx2x_stats_handle(bp, STATS_EVENT_STOP);\n\t\tbnx2x_save_statistics(bp);\n\t}\n\n\t \n\tif (unload_mode != UNLOAD_RECOVERY)\n\t\tbnx2x_drain_tx_queues(bp);\n\n\t \n\tif (IS_VF(bp)) {\n\t\tbnx2x_clear_vlan_info(bp);\n\t\tbnx2x_vfpf_close_vf(bp);\n\t} else if (unload_mode != UNLOAD_RECOVERY) {\n\t\t \n\t\tbnx2x_chip_cleanup(bp, unload_mode, keep_link);\n\t} else {\n\t\t \n\t\tbnx2x_send_unload_req(bp, unload_mode);\n\n\t\t \n\t\tif (!CHIP_IS_E1x(bp))\n\t\t\tbnx2x_pf_disable(bp);\n\n\t\tif (!bp->nic_stopped) {\n\t\t\t \n\t\t\tbnx2x_netif_stop(bp, 1);\n\t\t\t \n\t\t\tbnx2x_del_all_napi(bp);\n\t\t\tif (CNIC_LOADED(bp))\n\t\t\t\tbnx2x_del_all_napi_cnic(bp);\n\t\t\t \n\t\t\tbnx2x_free_irq(bp);\n\t\t\tbp->nic_stopped = true;\n\t\t}\n\n\t\t \n\t\tbnx2x_send_unload_done(bp, false);\n\t}\n\n\t \n\tif (IS_PF(bp))\n\t\tbnx2x_squeeze_objects(bp);\n\n\t \n\tbp->sp_state = 0;\n\n\tbp->port.pmf = 0;\n\n\t \n\tbp->sp_rtnl_state = 0;\n\tsmp_mb();\n\n\t \n\tbnx2x_free_skbs(bp);\n\tif (CNIC_LOADED(bp))\n\t\tbnx2x_free_skbs_cnic(bp);\n\tfor_each_rx_queue(bp, i)\n\t\tbnx2x_free_rx_sge_range(bp, bp->fp + i, NUM_RX_SGE);\n\n\tbnx2x_free_fp_mem(bp);\n\tif (CNIC_LOADED(bp))\n\t\tbnx2x_free_fp_mem_cnic(bp);\n\n\tif (IS_PF(bp)) {\n\t\tif (CNIC_LOADED(bp))\n\t\t\tbnx2x_free_mem_cnic(bp);\n\t}\n\tbnx2x_free_mem(bp);\n\n\tbp->state = BNX2X_STATE_CLOSED;\n\tbp->cnic_loaded = false;\n\n\t \n\tif (IS_PF(bp) && !BP_NOMCP(bp))\n\t\tbnx2x_update_mng_version(bp);\n\n\t \n\tif (IS_PF(bp) && bnx2x_chk_parity_attn(bp, &global, false)) {\n\t\tbnx2x_set_reset_in_progress(bp);\n\n\t\t \n\t\tif (global)\n\t\t\tbnx2x_set_reset_global(bp);\n\t}\n\n\t \n\tif (IS_PF(bp) &&\n\t    !bnx2x_clear_pf_load(bp) &&\n\t    bnx2x_reset_is_done(bp, BP_PATH(bp)))\n\t\tbnx2x_disable_close_the_gate(bp);\n\n\tDP(NETIF_MSG_IFUP, \"Ending NIC unload\\n\");\n\n\treturn 0;\n}\n\nint bnx2x_set_power_state(struct bnx2x *bp, pci_power_t state)\n{\n\tu16 pmcsr;\n\n\t \n\tif (!bp->pdev->pm_cap) {\n\t\tBNX2X_DEV_INFO(\"No power capability. Breaking.\\n\");\n\t\treturn 0;\n\t}\n\n\tpci_read_config_word(bp->pdev, bp->pdev->pm_cap + PCI_PM_CTRL, &pmcsr);\n\n\tswitch (state) {\n\tcase PCI_D0:\n\t\tpci_write_config_word(bp->pdev, bp->pdev->pm_cap + PCI_PM_CTRL,\n\t\t\t\t      ((pmcsr & ~PCI_PM_CTRL_STATE_MASK) |\n\t\t\t\t       PCI_PM_CTRL_PME_STATUS));\n\n\t\tif (pmcsr & PCI_PM_CTRL_STATE_MASK)\n\t\t\t \n\t\t\tmsleep(20);\n\t\tbreak;\n\n\tcase PCI_D3hot:\n\t\t \n\t\tif (atomic_read(&bp->pdev->enable_cnt) != 1)\n\t\t\treturn 0;\n\t\t \n\t\tif (CHIP_REV_IS_SLOW(bp))\n\t\t\treturn 0;\n\n\t\tpmcsr &= ~PCI_PM_CTRL_STATE_MASK;\n\t\tpmcsr |= 3;\n\n\t\tif (bp->wol)\n\t\t\tpmcsr |= PCI_PM_CTRL_PME_ENABLE;\n\n\t\tpci_write_config_word(bp->pdev, bp->pdev->pm_cap + PCI_PM_CTRL,\n\t\t\t\t      pmcsr);\n\n\t\t \n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(&bp->pdev->dev, \"Can't support state = %d\\n\", state);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nstatic int bnx2x_poll(struct napi_struct *napi, int budget)\n{\n\tstruct bnx2x_fastpath *fp = container_of(napi, struct bnx2x_fastpath,\n\t\t\t\t\t\t napi);\n\tstruct bnx2x *bp = fp->bp;\n\tint rx_work_done;\n\tu8 cos;\n\n#ifdef BNX2X_STOP_ON_ERROR\n\tif (unlikely(bp->panic)) {\n\t\tnapi_complete(napi);\n\t\treturn 0;\n\t}\n#endif\n\tfor_each_cos_in_tx_queue(fp, cos)\n\t\tif (bnx2x_tx_queue_has_work(fp->txdata_ptr[cos]))\n\t\t\tbnx2x_tx_int(bp, fp->txdata_ptr[cos]);\n\n\trx_work_done = (bnx2x_has_rx_work(fp)) ? bnx2x_rx_int(fp, budget) : 0;\n\n\tif (rx_work_done < budget) {\n\t\t \n\t\tif (IS_FCOE_FP(fp)) {\n\t\t\tnapi_complete_done(napi, rx_work_done);\n\t\t} else {\n\t\t\tbnx2x_update_fpsb_idx(fp);\n\t\t\t \n\t\t\trmb();\n\n\t\t\tif (!(bnx2x_has_rx_work(fp) || bnx2x_has_tx_work(fp))) {\n\t\t\t\tif (napi_complete_done(napi, rx_work_done)) {\n\t\t\t\t\t \n\t\t\t\t\tDP(NETIF_MSG_RX_STATUS,\n\t\t\t\t\t   \"Update index to %d\\n\", fp->fp_hc_idx);\n\t\t\t\t\tbnx2x_ack_sb(bp, fp->igu_sb_id, USTORM_ID,\n\t\t\t\t\t\t     le16_to_cpu(fp->fp_hc_idx),\n\t\t\t\t\t\t     IGU_INT_ENABLE, 1);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trx_work_done = budget;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn rx_work_done;\n}\n\n \nstatic u16 bnx2x_tx_split(struct bnx2x *bp,\n\t\t\t  struct bnx2x_fp_txdata *txdata,\n\t\t\t  struct sw_tx_bd *tx_buf,\n\t\t\t  struct eth_tx_start_bd **tx_bd, u16 hlen,\n\t\t\t  u16 bd_prod)\n{\n\tstruct eth_tx_start_bd *h_tx_bd = *tx_bd;\n\tstruct eth_tx_bd *d_tx_bd;\n\tdma_addr_t mapping;\n\tint old_len = le16_to_cpu(h_tx_bd->nbytes);\n\n\t \n\th_tx_bd->nbytes = cpu_to_le16(hlen);\n\n\tDP(NETIF_MSG_TX_QUEUED,\t\"TSO split header size is %d (%x:%x)\\n\",\n\t   h_tx_bd->nbytes, h_tx_bd->addr_hi, h_tx_bd->addr_lo);\n\n\t \n\tbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\n\td_tx_bd = &txdata->tx_desc_ring[bd_prod].reg_bd;\n\n\tmapping = HILO_U64(le32_to_cpu(h_tx_bd->addr_hi),\n\t\t\t   le32_to_cpu(h_tx_bd->addr_lo)) + hlen;\n\n\td_tx_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\n\td_tx_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\n\td_tx_bd->nbytes = cpu_to_le16(old_len - hlen);\n\n\t \n\ttx_buf->flags |= BNX2X_TSO_SPLIT_BD;\n\n\tDP(NETIF_MSG_TX_QUEUED,\n\t   \"TSO split data size is %d (%x:%x)\\n\",\n\t   d_tx_bd->nbytes, d_tx_bd->addr_hi, d_tx_bd->addr_lo);\n\n\t \n\t*tx_bd = (struct eth_tx_start_bd *)d_tx_bd;\n\n\treturn bd_prod;\n}\n\n#define bswab32(b32) ((__force __le32) swab32((__force __u32) (b32)))\n#define bswab16(b16) ((__force __le16) swab16((__force __u16) (b16)))\nstatic __le16 bnx2x_csum_fix(unsigned char *t_header, u16 csum, s8 fix)\n{\n\t__sum16 tsum = (__force __sum16) csum;\n\n\tif (fix > 0)\n\t\ttsum = ~csum_fold(csum_sub((__force __wsum) csum,\n\t\t\t\t  csum_partial(t_header - fix, fix, 0)));\n\n\telse if (fix < 0)\n\t\ttsum = ~csum_fold(csum_add((__force __wsum) csum,\n\t\t\t\t  csum_partial(t_header, -fix, 0)));\n\n\treturn bswab16(tsum);\n}\n\nstatic u32 bnx2x_xmit_type(struct bnx2x *bp, struct sk_buff *skb)\n{\n\tu32 rc;\n\t__u8 prot = 0;\n\t__be16 protocol;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn XMIT_PLAIN;\n\n\tprotocol = vlan_get_protocol(skb);\n\tif (protocol == htons(ETH_P_IPV6)) {\n\t\trc = XMIT_CSUM_V6;\n\t\tprot = ipv6_hdr(skb)->nexthdr;\n\t} else {\n\t\trc = XMIT_CSUM_V4;\n\t\tprot = ip_hdr(skb)->protocol;\n\t}\n\n\tif (!CHIP_IS_E1x(bp) && skb->encapsulation) {\n\t\tif (inner_ip_hdr(skb)->version == 6) {\n\t\t\trc |= XMIT_CSUM_ENC_V6;\n\t\t\tif (inner_ipv6_hdr(skb)->nexthdr == IPPROTO_TCP)\n\t\t\t\trc |= XMIT_CSUM_TCP;\n\t\t} else {\n\t\t\trc |= XMIT_CSUM_ENC_V4;\n\t\t\tif (inner_ip_hdr(skb)->protocol == IPPROTO_TCP)\n\t\t\t\trc |= XMIT_CSUM_TCP;\n\t\t}\n\t}\n\tif (prot == IPPROTO_TCP)\n\t\trc |= XMIT_CSUM_TCP;\n\n\tif (skb_is_gso(skb)) {\n\t\tif (skb_is_gso_v6(skb)) {\n\t\t\trc |= (XMIT_GSO_V6 | XMIT_CSUM_TCP);\n\t\t\tif (rc & XMIT_CSUM_ENC)\n\t\t\t\trc |= XMIT_GSO_ENC_V6;\n\t\t} else {\n\t\t\trc |= (XMIT_GSO_V4 | XMIT_CSUM_TCP);\n\t\t\tif (rc & XMIT_CSUM_ENC)\n\t\t\t\trc |= XMIT_GSO_ENC_V4;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\n \n#define BNX2X_NUM_VXLAN_TSO_WIN_SUB_BDS         4\n\n \n#define BNX2X_NUM_TSO_WIN_SUB_BDS               3\n\n#if (MAX_SKB_FRAGS >= MAX_FETCH_BD - BDS_PER_TX_PKT)\n \nstatic int bnx2x_pkt_req_lin(struct bnx2x *bp, struct sk_buff *skb,\n\t\t\t     u32 xmit_type)\n{\n\tint first_bd_sz = 0, num_tso_win_sub = BNX2X_NUM_TSO_WIN_SUB_BDS;\n\tint to_copy = 0, hlen = 0;\n\n\tif (xmit_type & XMIT_GSO_ENC)\n\t\tnum_tso_win_sub = BNX2X_NUM_VXLAN_TSO_WIN_SUB_BDS;\n\n\tif (skb_shinfo(skb)->nr_frags >= (MAX_FETCH_BD - num_tso_win_sub)) {\n\t\tif (xmit_type & XMIT_GSO) {\n\t\t\tunsigned short lso_mss = skb_shinfo(skb)->gso_size;\n\t\t\tint wnd_size = MAX_FETCH_BD - num_tso_win_sub;\n\t\t\t \n\t\t\tint num_wnds = skb_shinfo(skb)->nr_frags - wnd_size;\n\t\t\tint wnd_idx = 0;\n\t\t\tint frag_idx = 0;\n\t\t\tu32 wnd_sum = 0;\n\n\t\t\t \n\t\t\tif (xmit_type & XMIT_GSO_ENC)\n\t\t\t\thlen = skb_inner_tcp_all_headers(skb);\n\t\t\telse\n\t\t\t\thlen = skb_tcp_all_headers(skb);\n\n\t\t\t \n\t\t\tfirst_bd_sz = skb_headlen(skb) - hlen;\n\n\t\t\twnd_sum  = first_bd_sz;\n\n\t\t\t \n\t\t\tfor (frag_idx = 0; frag_idx < wnd_size - 1; frag_idx++)\n\t\t\t\twnd_sum +=\n\t\t\t\t\tskb_frag_size(&skb_shinfo(skb)->frags[frag_idx]);\n\n\t\t\t \n\t\t\tif (first_bd_sz > 0) {\n\t\t\t\tif (unlikely(wnd_sum < lso_mss)) {\n\t\t\t\t\tto_copy = 1;\n\t\t\t\t\tgoto exit_lbl;\n\t\t\t\t}\n\n\t\t\t\twnd_sum -= first_bd_sz;\n\t\t\t}\n\n\t\t\t \n\t\t\tfor (wnd_idx = 0; wnd_idx <= num_wnds; wnd_idx++) {\n\t\t\t\twnd_sum +=\n\t\t\t  skb_frag_size(&skb_shinfo(skb)->frags[wnd_idx + wnd_size - 1]);\n\n\t\t\t\tif (unlikely(wnd_sum < lso_mss)) {\n\t\t\t\t\tto_copy = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\twnd_sum -=\n\t\t\t\t\tskb_frag_size(&skb_shinfo(skb)->frags[wnd_idx]);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tto_copy = 1;\n\t\t}\n\t}\n\nexit_lbl:\n\tif (unlikely(to_copy))\n\t\tDP(NETIF_MSG_TX_QUEUED,\n\t\t   \"Linearization IS REQUIRED for %s packet. num_frags %d  hlen %d  first_bd_sz %d\\n\",\n\t\t   (xmit_type & XMIT_GSO) ? \"LSO\" : \"non-LSO\",\n\t\t   skb_shinfo(skb)->nr_frags, hlen, first_bd_sz);\n\n\treturn to_copy;\n}\n#endif\n\n \nstatic void bnx2x_set_pbd_gso(struct sk_buff *skb,\n\t\t\t      struct eth_tx_parse_bd_e1x *pbd,\n\t\t\t      u32 xmit_type)\n{\n\tpbd->lso_mss = cpu_to_le16(skb_shinfo(skb)->gso_size);\n\tpbd->tcp_send_seq = bswab32(tcp_hdr(skb)->seq);\n\tpbd->tcp_flags = pbd_tcp_flags(tcp_hdr(skb));\n\n\tif (xmit_type & XMIT_GSO_V4) {\n\t\tpbd->ip_id = bswab16(ip_hdr(skb)->id);\n\t\tpbd->tcp_pseudo_csum =\n\t\t\tbswab16(~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t\t   0, IPPROTO_TCP, 0));\n\t} else {\n\t\tpbd->tcp_pseudo_csum =\n\t\t\tbswab16(~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t 0, IPPROTO_TCP, 0));\n\t}\n\n\tpbd->global_data |=\n\t\tcpu_to_le16(ETH_TX_PARSE_BD_E1X_PSEUDO_CS_WITHOUT_LEN);\n}\n\n \nstatic u8 bnx2x_set_pbd_csum_enc(struct bnx2x *bp, struct sk_buff *skb,\n\t\t\t\t u32 *parsing_data, u32 xmit_type)\n{\n\t*parsing_data |=\n\t\t((((u8 *)skb_inner_transport_header(skb) - skb->data) >> 1) <<\n\t\tETH_TX_PARSE_BD_E2_L4_HDR_START_OFFSET_W_SHIFT) &\n\t\tETH_TX_PARSE_BD_E2_L4_HDR_START_OFFSET_W;\n\n\tif (xmit_type & XMIT_CSUM_TCP) {\n\t\t*parsing_data |= ((inner_tcp_hdrlen(skb) / 4) <<\n\t\t\tETH_TX_PARSE_BD_E2_TCP_HDR_LENGTH_DW_SHIFT) &\n\t\t\tETH_TX_PARSE_BD_E2_TCP_HDR_LENGTH_DW;\n\n\t\treturn skb_inner_tcp_all_headers(skb);\n\t}\n\n\t \n\treturn skb_inner_transport_offset(skb) + sizeof(struct udphdr);\n}\n\n \nstatic u8 bnx2x_set_pbd_csum_e2(struct bnx2x *bp, struct sk_buff *skb,\n\t\t\t\tu32 *parsing_data, u32 xmit_type)\n{\n\t*parsing_data |=\n\t\t((((u8 *)skb_transport_header(skb) - skb->data) >> 1) <<\n\t\tETH_TX_PARSE_BD_E2_L4_HDR_START_OFFSET_W_SHIFT) &\n\t\tETH_TX_PARSE_BD_E2_L4_HDR_START_OFFSET_W;\n\n\tif (xmit_type & XMIT_CSUM_TCP) {\n\t\t*parsing_data |= ((tcp_hdrlen(skb) / 4) <<\n\t\t\tETH_TX_PARSE_BD_E2_TCP_HDR_LENGTH_DW_SHIFT) &\n\t\t\tETH_TX_PARSE_BD_E2_TCP_HDR_LENGTH_DW;\n\n\t\treturn skb_tcp_all_headers(skb);\n\t}\n\t \n\treturn skb_transport_offset(skb) + sizeof(struct udphdr);\n}\n\n \nstatic void bnx2x_set_sbd_csum(struct bnx2x *bp, struct sk_buff *skb,\n\t\t\t       struct eth_tx_start_bd *tx_start_bd,\n\t\t\t       u32 xmit_type)\n{\n\ttx_start_bd->bd_flags.as_bitfield |= ETH_TX_BD_FLAGS_L4_CSUM;\n\n\tif (xmit_type & (XMIT_CSUM_ENC_V6 | XMIT_CSUM_V6))\n\t\ttx_start_bd->bd_flags.as_bitfield |= ETH_TX_BD_FLAGS_IPV6;\n\n\tif (!(xmit_type & XMIT_CSUM_TCP))\n\t\ttx_start_bd->bd_flags.as_bitfield |= ETH_TX_BD_FLAGS_IS_UDP;\n}\n\n \nstatic u8 bnx2x_set_pbd_csum(struct bnx2x *bp, struct sk_buff *skb,\n\t\t\t     struct eth_tx_parse_bd_e1x *pbd,\n\t\t\t     u32 xmit_type)\n{\n\tu8 hlen = (skb_network_header(skb) - skb->data) >> 1;\n\n\t \n\tpbd->global_data =\n\t\tcpu_to_le16(hlen |\n\t\t\t    ((skb->protocol == cpu_to_be16(ETH_P_8021Q)) <<\n\t\t\t     ETH_TX_PARSE_BD_E1X_LLC_SNAP_EN_SHIFT));\n\n\tpbd->ip_hlen_w = (skb_transport_header(skb) -\n\t\t\tskb_network_header(skb)) >> 1;\n\n\thlen += pbd->ip_hlen_w;\n\n\t \n\tif (xmit_type & XMIT_CSUM_TCP)\n\t\thlen += tcp_hdrlen(skb) / 2;\n\telse\n\t\thlen += sizeof(struct udphdr) / 2;\n\n\tpbd->total_hlen_w = cpu_to_le16(hlen);\n\thlen = hlen*2;\n\n\tif (xmit_type & XMIT_CSUM_TCP) {\n\t\tpbd->tcp_pseudo_csum = bswab16(tcp_hdr(skb)->check);\n\n\t} else {\n\t\ts8 fix = SKB_CS_OFF(skb);  \n\n\t\tDP(NETIF_MSG_TX_QUEUED,\n\t\t   \"hlen %d  fix %d  csum before fix %x\\n\",\n\t\t   le16_to_cpu(pbd->total_hlen_w), fix, SKB_CS(skb));\n\n\t\t \n\t\tpbd->tcp_pseudo_csum =\n\t\t\tbnx2x_csum_fix(skb_transport_header(skb),\n\t\t\t\t       SKB_CS(skb), fix);\n\n\t\tDP(NETIF_MSG_TX_QUEUED, \"csum after fix %x\\n\",\n\t\t   pbd->tcp_pseudo_csum);\n\t}\n\n\treturn hlen;\n}\n\nstatic void bnx2x_update_pbds_gso_enc(struct sk_buff *skb,\n\t\t\t\t      struct eth_tx_parse_bd_e2 *pbd_e2,\n\t\t\t\t      struct eth_tx_parse_2nd_bd *pbd2,\n\t\t\t\t      u16 *global_data,\n\t\t\t\t      u32 xmit_type)\n{\n\tu16 hlen_w = 0;\n\tu8 outerip_off, outerip_len = 0;\n\n\t \n\thlen_w = (skb_inner_transport_header(skb) -\n\t\t  skb_network_header(skb)) >> 1;\n\n\t \n\thlen_w += inner_tcp_hdrlen(skb) >> 1;\n\n\tpbd2->fw_ip_hdr_to_payload_w = hlen_w;\n\n\t \n\tif (xmit_type & XMIT_CSUM_V4) {\n\t\tstruct iphdr *iph = ip_hdr(skb);\n\t\tu32 csum = (__force u32)(~iph->check) -\n\t\t\t   (__force u32)iph->tot_len -\n\t\t\t   (__force u32)iph->frag_off;\n\n\t\touterip_len = iph->ihl << 1;\n\n\t\tpbd2->fw_ip_csum_wo_len_flags_frag =\n\t\t\tbswab16(csum_fold((__force __wsum)csum));\n\t} else {\n\t\tpbd2->fw_ip_hdr_to_payload_w =\n\t\t\thlen_w - ((sizeof(struct ipv6hdr)) >> 1);\n\t\tpbd_e2->data.tunnel_data.flags |=\n\t\t\tETH_TUNNEL_DATA_IPV6_OUTER;\n\t}\n\n\tpbd2->tcp_send_seq = bswab32(inner_tcp_hdr(skb)->seq);\n\n\tpbd2->tcp_flags = pbd_tcp_flags(inner_tcp_hdr(skb));\n\n\t \n\tif (xmit_type & XMIT_CSUM_ENC_V4) {\n\t\tpbd2->hw_ip_id = bswab16(inner_ip_hdr(skb)->id);\n\n\t\tpbd_e2->data.tunnel_data.pseudo_csum =\n\t\t\tbswab16(~csum_tcpudp_magic(\n\t\t\t\t\tinner_ip_hdr(skb)->saddr,\n\t\t\t\t\tinner_ip_hdr(skb)->daddr,\n\t\t\t\t\t0, IPPROTO_TCP, 0));\n\t} else {\n\t\tpbd_e2->data.tunnel_data.pseudo_csum =\n\t\t\tbswab16(~csum_ipv6_magic(\n\t\t\t\t\t&inner_ipv6_hdr(skb)->saddr,\n\t\t\t\t\t&inner_ipv6_hdr(skb)->daddr,\n\t\t\t\t\t0, IPPROTO_TCP, 0));\n\t}\n\n\touterip_off = (skb_network_header(skb) - skb->data) >> 1;\n\n\t*global_data |=\n\t\touterip_off |\n\t\t(outerip_len <<\n\t\t\tETH_TX_PARSE_2ND_BD_IP_HDR_LEN_OUTER_W_SHIFT) |\n\t\t((skb->protocol == cpu_to_be16(ETH_P_8021Q)) <<\n\t\t\tETH_TX_PARSE_2ND_BD_LLC_SNAP_EN_SHIFT);\n\n\tif (ip_hdr(skb)->protocol == IPPROTO_UDP) {\n\t\tSET_FLAG(*global_data, ETH_TX_PARSE_2ND_BD_TUNNEL_UDP_EXIST, 1);\n\t\tpbd2->tunnel_udp_hdr_start_w = skb_transport_offset(skb) >> 1;\n\t}\n}\n\nstatic inline void bnx2x_set_ipv6_ext_e2(struct sk_buff *skb, u32 *parsing_data,\n\t\t\t\t\t u32 xmit_type)\n{\n\tstruct ipv6hdr *ipv6;\n\n\tif (!(xmit_type & (XMIT_GSO_ENC_V6 | XMIT_GSO_V6)))\n\t\treturn;\n\n\tif (xmit_type & XMIT_GSO_ENC_V6)\n\t\tipv6 = inner_ipv6_hdr(skb);\n\telse  \n\t\tipv6 = ipv6_hdr(skb);\n\n\tif (ipv6->nexthdr == NEXTHDR_IPV6)\n\t\t*parsing_data |= ETH_TX_PARSE_BD_E2_IPV6_WITH_EXT_HDR;\n}\n\n \nnetdev_tx_t bnx2x_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\n\tstruct netdev_queue *txq;\n\tstruct bnx2x_fp_txdata *txdata;\n\tstruct sw_tx_bd *tx_buf;\n\tstruct eth_tx_start_bd *tx_start_bd, *first_bd;\n\tstruct eth_tx_bd *tx_data_bd, *total_pkt_bd = NULL;\n\tstruct eth_tx_parse_bd_e1x *pbd_e1x = NULL;\n\tstruct eth_tx_parse_bd_e2 *pbd_e2 = NULL;\n\tstruct eth_tx_parse_2nd_bd *pbd2 = NULL;\n\tu32 pbd_e2_parsing_data = 0;\n\tu16 pkt_prod, bd_prod;\n\tint nbd, txq_index;\n\tdma_addr_t mapping;\n\tu32 xmit_type = bnx2x_xmit_type(bp, skb);\n\tint i;\n\tu8 hlen = 0;\n\t__le16 pkt_size = 0;\n\tstruct ethhdr *eth;\n\tu8 mac_type = UNICAST_ADDRESS;\n\n#ifdef BNX2X_STOP_ON_ERROR\n\tif (unlikely(bp->panic))\n\t\treturn NETDEV_TX_BUSY;\n#endif\n\n\ttxq_index = skb_get_queue_mapping(skb);\n\ttxq = netdev_get_tx_queue(dev, txq_index);\n\n\tBUG_ON(txq_index >= MAX_ETH_TXQ_IDX(bp) + (CNIC_LOADED(bp) ? 1 : 0));\n\n\ttxdata = &bp->bnx2x_txq[txq_index];\n\n\t \n\n\t \n\n\tif (unlikely(bnx2x_tx_avail(bp, txdata) <\n\t\t\tskb_shinfo(skb)->nr_frags +\n\t\t\tBDS_PER_TX_PKT +\n\t\t\tNEXT_CNT_PER_TX_PKT(MAX_BDS_PER_TX_PKT))) {\n\t\t \n\t\tif (txdata->tx_ring_size == 0) {\n\t\t\tstruct bnx2x_eth_q_stats *q_stats =\n\t\t\t\tbnx2x_fp_qstats(bp, txdata->parent_fp);\n\t\t\tq_stats->driver_filtered_tx_pkt++;\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\tbnx2x_fp_qstats(bp, txdata->parent_fp)->driver_xoff++;\n\t\tnetif_tx_stop_queue(txq);\n\t\tBNX2X_ERR(\"BUG! Tx ring full when queue awake!\\n\");\n\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tDP(NETIF_MSG_TX_QUEUED,\n\t   \"queue[%d]: SKB: summed %x  protocol %x protocol(%x,%x) gso type %x  xmit_type %x len %d\\n\",\n\t   txq_index, skb->ip_summed, skb->protocol, ipv6_hdr(skb)->nexthdr,\n\t   ip_hdr(skb)->protocol, skb_shinfo(skb)->gso_type, xmit_type,\n\t   skb->len);\n\n\teth = (struct ethhdr *)skb->data;\n\n\t \n\tif (unlikely(is_multicast_ether_addr(eth->h_dest))) {\n\t\tif (is_broadcast_ether_addr(eth->h_dest))\n\t\t\tmac_type = BROADCAST_ADDRESS;\n\t\telse\n\t\t\tmac_type = MULTICAST_ADDRESS;\n\t}\n\n#if (MAX_SKB_FRAGS >= MAX_FETCH_BD - BDS_PER_TX_PKT)\n\t \n\tif (bnx2x_pkt_req_lin(bp, skb, xmit_type)) {\n\t\t \n\t\tbp->lin_cnt++;\n\t\tif (skb_linearize(skb) != 0) {\n\t\t\tDP(NETIF_MSG_TX_QUEUED,\n\t\t\t   \"SKB linearization failed - silently dropping this SKB\\n\");\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t}\n#endif\n\t \n\tmapping = dma_map_single(&bp->pdev->dev, skb->data,\n\t\t\t\t skb_headlen(skb), DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\n\t\tDP(NETIF_MSG_TX_QUEUED,\n\t\t   \"SKB mapping failed - silently dropping this SKB\\n\");\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\t \n\n\t \n\tpkt_prod = txdata->tx_pkt_prod;\n\tbd_prod = TX_BD(txdata->tx_bd_prod);\n\n\t \n\ttx_buf = &txdata->tx_buf_ring[TX_BD(pkt_prod)];\n\ttx_start_bd = &txdata->tx_desc_ring[bd_prod].start_bd;\n\tfirst_bd = tx_start_bd;\n\n\ttx_start_bd->bd_flags.as_bitfield = ETH_TX_BD_FLAGS_START_BD;\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {\n\t\tif (!(bp->flags & TX_TIMESTAMPING_EN)) {\n\t\t\tbp->eth_stats.ptp_skip_tx_ts++;\n\t\t\tBNX2X_ERR(\"Tx timestamping was not enabled, this packet will not be timestamped\\n\");\n\t\t} else if (bp->ptp_tx_skb) {\n\t\t\tbp->eth_stats.ptp_skip_tx_ts++;\n\t\t\tnetdev_err_once(bp->dev,\n\t\t\t\t\t\"Device supports only a single outstanding packet to timestamp, this packet won't be timestamped\\n\");\n\t\t} else {\n\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t\t \n\t\t\tbp->ptp_tx_skb = skb_get(skb);\n\t\t\tbp->ptp_tx_start = jiffies;\n\t\t\tschedule_work(&bp->ptp_task);\n\t\t}\n\t}\n\n\t \n\ttx_start_bd->general_data = 1 << ETH_TX_START_BD_HDR_NBDS_SHIFT;\n\n\t \n\ttx_buf->first_bd = txdata->tx_bd_prod;\n\ttx_buf->skb = skb;\n\ttx_buf->flags = 0;\n\n\tDP(NETIF_MSG_TX_QUEUED,\n\t   \"sending pkt %u @%p  next_idx %u  bd %u @%p\\n\",\n\t   pkt_prod, tx_buf, txdata->tx_pkt_prod, bd_prod, tx_start_bd);\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\ttx_start_bd->vlan_or_ethertype =\n\t\t    cpu_to_le16(skb_vlan_tag_get(skb));\n\t\ttx_start_bd->bd_flags.as_bitfield |=\n\t\t    (X_ETH_OUTBAND_VLAN << ETH_TX_BD_FLAGS_VLAN_MODE_SHIFT);\n\t} else {\n\t\t \n\t\tu16 vlan_tci = 0;\n#ifndef BNX2X_STOP_ON_ERROR\n\t\tif (IS_VF(bp)) {\n#endif\n\t\t\t \n\t\t\tif (__vlan_get_tag(skb, &vlan_tci)) {\n\t\t\t\ttx_start_bd->vlan_or_ethertype =\n\t\t\t\t\tcpu_to_le16(ntohs(eth->h_proto));\n\t\t\t} else {\n\t\t\t\ttx_start_bd->bd_flags.as_bitfield |=\n\t\t\t\t\t(X_ETH_INBAND_VLAN <<\n\t\t\t\t\t ETH_TX_BD_FLAGS_VLAN_MODE_SHIFT);\n\t\t\t\ttx_start_bd->vlan_or_ethertype =\n\t\t\t\t\tcpu_to_le16(vlan_tci);\n\t\t\t}\n#ifndef BNX2X_STOP_ON_ERROR\n\t\t} else {\n\t\t\t \n\t\t\ttx_start_bd->vlan_or_ethertype = cpu_to_le16(pkt_prod);\n\t\t}\n#endif\n\t}\n\n\tnbd = 2;  \n\n\t \n\tbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\n\n\tif (xmit_type & XMIT_CSUM)\n\t\tbnx2x_set_sbd_csum(bp, skb, tx_start_bd, xmit_type);\n\n\tif (!CHIP_IS_E1x(bp)) {\n\t\tpbd_e2 = &txdata->tx_desc_ring[bd_prod].parse_bd_e2;\n\t\tmemset(pbd_e2, 0, sizeof(struct eth_tx_parse_bd_e2));\n\n\t\tif (xmit_type & XMIT_CSUM_ENC) {\n\t\t\tu16 global_data = 0;\n\n\t\t\t \n\t\t\thlen = bnx2x_set_pbd_csum_enc(bp, skb,\n\t\t\t\t\t\t      &pbd_e2_parsing_data,\n\t\t\t\t\t\t      xmit_type);\n\n\t\t\t \n\t\t\tbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\n\n\t\t\tpbd2 = &txdata->tx_desc_ring[bd_prod].parse_2nd_bd;\n\n\t\t\tmemset(pbd2, 0, sizeof(*pbd2));\n\n\t\t\tpbd_e2->data.tunnel_data.ip_hdr_start_inner_w =\n\t\t\t\t(skb_inner_network_header(skb) -\n\t\t\t\t skb->data) >> 1;\n\n\t\t\tif (xmit_type & XMIT_GSO_ENC)\n\t\t\t\tbnx2x_update_pbds_gso_enc(skb, pbd_e2, pbd2,\n\t\t\t\t\t\t\t  &global_data,\n\t\t\t\t\t\t\t  xmit_type);\n\n\t\t\tpbd2->global_data = cpu_to_le16(global_data);\n\n\t\t\t \n\t\t\tSET_FLAG(tx_start_bd->general_data,\n\t\t\t\t ETH_TX_START_BD_PARSE_NBDS, 1);\n\t\t\t \n\t\t\tSET_FLAG(tx_start_bd->general_data,\n\t\t\t\t ETH_TX_START_BD_TUNNEL_EXIST, 1);\n\n\t\t\ttx_buf->flags |= BNX2X_HAS_SECOND_PBD;\n\n\t\t\tnbd++;\n\t\t} else if (xmit_type & XMIT_CSUM) {\n\t\t\t \n\t\t\thlen = bnx2x_set_pbd_csum_e2(bp, skb,\n\t\t\t\t\t\t     &pbd_e2_parsing_data,\n\t\t\t\t\t\t     xmit_type);\n\t\t}\n\n\t\tbnx2x_set_ipv6_ext_e2(skb, &pbd_e2_parsing_data, xmit_type);\n\t\t \n\t\tif (IS_VF(bp)) {\n\t\t\t \n\t\t\tbnx2x_set_fw_mac_addr(&pbd_e2->data.mac_addr.src_hi,\n\t\t\t\t\t      &pbd_e2->data.mac_addr.src_mid,\n\t\t\t\t\t      &pbd_e2->data.mac_addr.src_lo,\n\t\t\t\t\t      eth->h_source);\n\n\t\t\tbnx2x_set_fw_mac_addr(&pbd_e2->data.mac_addr.dst_hi,\n\t\t\t\t\t      &pbd_e2->data.mac_addr.dst_mid,\n\t\t\t\t\t      &pbd_e2->data.mac_addr.dst_lo,\n\t\t\t\t\t      eth->h_dest);\n\t\t} else {\n\t\t\tif (bp->flags & TX_SWITCHING)\n\t\t\t\tbnx2x_set_fw_mac_addr(\n\t\t\t\t\t\t&pbd_e2->data.mac_addr.dst_hi,\n\t\t\t\t\t\t&pbd_e2->data.mac_addr.dst_mid,\n\t\t\t\t\t\t&pbd_e2->data.mac_addr.dst_lo,\n\t\t\t\t\t\teth->h_dest);\n#ifdef BNX2X_STOP_ON_ERROR\n\t\t\t \n\t\t\tbnx2x_set_fw_mac_addr(&pbd_e2->data.mac_addr.src_hi,\n\t\t\t\t\t      &pbd_e2->data.mac_addr.src_mid,\n\t\t\t\t\t      &pbd_e2->data.mac_addr.src_lo,\n\t\t\t\t\t      eth->h_source);\n#endif\n\t\t}\n\n\t\tSET_FLAG(pbd_e2_parsing_data,\n\t\t\t ETH_TX_PARSE_BD_E2_ETH_ADDR_TYPE, mac_type);\n\t} else {\n\t\tu16 global_data = 0;\n\t\tpbd_e1x = &txdata->tx_desc_ring[bd_prod].parse_bd_e1x;\n\t\tmemset(pbd_e1x, 0, sizeof(struct eth_tx_parse_bd_e1x));\n\t\t \n\t\tif (xmit_type & XMIT_CSUM)\n\t\t\thlen = bnx2x_set_pbd_csum(bp, skb, pbd_e1x, xmit_type);\n\n\t\tSET_FLAG(global_data,\n\t\t\t ETH_TX_PARSE_BD_E1X_ETH_ADDR_TYPE, mac_type);\n\t\tpbd_e1x->global_data |= cpu_to_le16(global_data);\n\t}\n\n\t \n\ttx_start_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\n\ttx_start_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\n\ttx_start_bd->nbytes = cpu_to_le16(skb_headlen(skb));\n\tpkt_size = tx_start_bd->nbytes;\n\n\tDP(NETIF_MSG_TX_QUEUED,\n\t   \"first bd @%p  addr (%x:%x)  nbytes %d  flags %x  vlan %x\\n\",\n\t   tx_start_bd, tx_start_bd->addr_hi, tx_start_bd->addr_lo,\n\t   le16_to_cpu(tx_start_bd->nbytes),\n\t   tx_start_bd->bd_flags.as_bitfield,\n\t   le16_to_cpu(tx_start_bd->vlan_or_ethertype));\n\n\tif (xmit_type & XMIT_GSO) {\n\n\t\tDP(NETIF_MSG_TX_QUEUED,\n\t\t   \"TSO packet len %d  hlen %d  total len %d  tso size %d\\n\",\n\t\t   skb->len, hlen, skb_headlen(skb),\n\t\t   skb_shinfo(skb)->gso_size);\n\n\t\ttx_start_bd->bd_flags.as_bitfield |= ETH_TX_BD_FLAGS_SW_LSO;\n\n\t\tif (unlikely(skb_headlen(skb) > hlen)) {\n\t\t\tnbd++;\n\t\t\tbd_prod = bnx2x_tx_split(bp, txdata, tx_buf,\n\t\t\t\t\t\t &tx_start_bd, hlen,\n\t\t\t\t\t\t bd_prod);\n\t\t}\n\t\tif (!CHIP_IS_E1x(bp))\n\t\t\tpbd_e2_parsing_data |=\n\t\t\t\t(skb_shinfo(skb)->gso_size <<\n\t\t\t\t ETH_TX_PARSE_BD_E2_LSO_MSS_SHIFT) &\n\t\t\t\t ETH_TX_PARSE_BD_E2_LSO_MSS;\n\t\telse\n\t\t\tbnx2x_set_pbd_gso(skb, pbd_e1x, xmit_type);\n\t}\n\n\t \n\tif (pbd_e2_parsing_data)\n\t\tpbd_e2->parsing_data = cpu_to_le32(pbd_e2_parsing_data);\n\n\ttx_data_bd = (struct eth_tx_bd *)tx_start_bd;\n\n\t \n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tmapping = skb_frag_dma_map(&bp->pdev->dev, frag, 0,\n\t\t\t\t\t   skb_frag_size(frag), DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(&bp->pdev->dev, mapping))) {\n\t\t\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\n\t\t\tDP(NETIF_MSG_TX_QUEUED,\n\t\t\t   \"Unable to map page - dropping packet...\\n\");\n\n\t\t\t \n\t\t\tfirst_bd->nbd = cpu_to_le16(nbd);\n\t\t\tbnx2x_free_tx_pkt(bp, txdata,\n\t\t\t\t\t  TX_BD(txdata->tx_pkt_prod),\n\t\t\t\t\t  &pkts_compl, &bytes_compl);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\tbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\n\t\ttx_data_bd = &txdata->tx_desc_ring[bd_prod].reg_bd;\n\t\tif (total_pkt_bd == NULL)\n\t\t\ttotal_pkt_bd = &txdata->tx_desc_ring[bd_prod].reg_bd;\n\n\t\ttx_data_bd->addr_hi = cpu_to_le32(U64_HI(mapping));\n\t\ttx_data_bd->addr_lo = cpu_to_le32(U64_LO(mapping));\n\t\ttx_data_bd->nbytes = cpu_to_le16(skb_frag_size(frag));\n\t\tle16_add_cpu(&pkt_size, skb_frag_size(frag));\n\t\tnbd++;\n\n\t\tDP(NETIF_MSG_TX_QUEUED,\n\t\t   \"frag %d  bd @%p  addr (%x:%x)  nbytes %d\\n\",\n\t\t   i, tx_data_bd, tx_data_bd->addr_hi, tx_data_bd->addr_lo,\n\t\t   le16_to_cpu(tx_data_bd->nbytes));\n\t}\n\n\tDP(NETIF_MSG_TX_QUEUED, \"last bd @%p\\n\", tx_data_bd);\n\n\t \n\tfirst_bd->nbd = cpu_to_le16(nbd);\n\n\tbd_prod = TX_BD(NEXT_TX_IDX(bd_prod));\n\n\t \n\tif (TX_BD_POFF(bd_prod) < nbd)\n\t\tnbd++;\n\n\t \n\tif (total_pkt_bd != NULL)\n\t\ttotal_pkt_bd->total_pkt_bytes = pkt_size;\n\n\tif (pbd_e1x)\n\t\tDP(NETIF_MSG_TX_QUEUED,\n\t\t   \"PBD (E1X) @%p  ip_data %x  ip_hlen %u  ip_id %u  lso_mss %u  tcp_flags %x  xsum %x  seq %u  hlen %u\\n\",\n\t\t   pbd_e1x, pbd_e1x->global_data, pbd_e1x->ip_hlen_w,\n\t\t   pbd_e1x->ip_id, pbd_e1x->lso_mss, pbd_e1x->tcp_flags,\n\t\t   pbd_e1x->tcp_pseudo_csum, pbd_e1x->tcp_send_seq,\n\t\t    le16_to_cpu(pbd_e1x->total_hlen_w));\n\tif (pbd_e2)\n\t\tDP(NETIF_MSG_TX_QUEUED,\n\t\t   \"PBD (E2) @%p  dst %x %x %x src %x %x %x parsing_data %x\\n\",\n\t\t   pbd_e2,\n\t\t   pbd_e2->data.mac_addr.dst_hi,\n\t\t   pbd_e2->data.mac_addr.dst_mid,\n\t\t   pbd_e2->data.mac_addr.dst_lo,\n\t\t   pbd_e2->data.mac_addr.src_hi,\n\t\t   pbd_e2->data.mac_addr.src_mid,\n\t\t   pbd_e2->data.mac_addr.src_lo,\n\t\t   pbd_e2->parsing_data);\n\tDP(NETIF_MSG_TX_QUEUED, \"doorbell: nbd %d  bd %u\\n\", nbd, bd_prod);\n\n\tnetdev_tx_sent_queue(txq, skb->len);\n\n\tskb_tx_timestamp(skb);\n\n\ttxdata->tx_pkt_prod++;\n\t \n\twmb();\n\n\ttxdata->tx_db.data.prod += nbd;\n\t \n\twmb();\n\n\tDOORBELL_RELAXED(bp, txdata->cid, txdata->tx_db.raw);\n\n\ttxdata->tx_bd_prod += nbd;\n\n\tif (unlikely(bnx2x_tx_avail(bp, txdata) < MAX_DESC_PER_TX_PKT)) {\n\t\tnetif_tx_stop_queue(txq);\n\n\t\t \n\t\tsmp_mb();\n\n\t\tbnx2x_fp_qstats(bp, txdata->parent_fp)->driver_xoff++;\n\t\tif (bnx2x_tx_avail(bp, txdata) >= MAX_DESC_PER_TX_PKT)\n\t\t\tnetif_tx_wake_queue(txq);\n\t}\n\ttxdata->tx_pkt++;\n\n\treturn NETDEV_TX_OK;\n}\n\nvoid bnx2x_get_c2s_mapping(struct bnx2x *bp, u8 *c2s_map, u8 *c2s_default)\n{\n\tint mfw_vn = BP_FW_MB_IDX(bp);\n\tu32 tmp;\n\n\t \n\tif (!IS_MF_BD(bp)) {\n\t\tint i;\n\n\t\tfor (i = 0; i < BNX2X_MAX_PRIORITY; i++)\n\t\t\tc2s_map[i] = i;\n\t\t*c2s_default = 0;\n\n\t\treturn;\n\t}\n\n\ttmp = SHMEM2_RD(bp, c2s_pcp_map_lower[mfw_vn]);\n\ttmp = (__force u32)be32_to_cpu((__force __be32)tmp);\n\tc2s_map[0] = tmp & 0xff;\n\tc2s_map[1] = (tmp >> 8) & 0xff;\n\tc2s_map[2] = (tmp >> 16) & 0xff;\n\tc2s_map[3] = (tmp >> 24) & 0xff;\n\n\ttmp = SHMEM2_RD(bp, c2s_pcp_map_upper[mfw_vn]);\n\ttmp = (__force u32)be32_to_cpu((__force __be32)tmp);\n\tc2s_map[4] = tmp & 0xff;\n\tc2s_map[5] = (tmp >> 8) & 0xff;\n\tc2s_map[6] = (tmp >> 16) & 0xff;\n\tc2s_map[7] = (tmp >> 24) & 0xff;\n\n\ttmp = SHMEM2_RD(bp, c2s_pcp_map_default[mfw_vn]);\n\ttmp = (__force u32)be32_to_cpu((__force __be32)tmp);\n\t*c2s_default = (tmp >> (8 * mfw_vn)) & 0xff;\n}\n\n \nint bnx2x_setup_tc(struct net_device *dev, u8 num_tc)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\tu8 c2s_map[BNX2X_MAX_PRIORITY], c2s_def;\n\tint cos, prio, count, offset;\n\n\t \n\tASSERT_RTNL();\n\n\t \n\tif (!num_tc) {\n\t\tnetdev_reset_tc(dev);\n\t\treturn 0;\n\t}\n\n\t \n\tif (num_tc > bp->max_cos) {\n\t\tBNX2X_ERR(\"support for too many traffic classes requested: %d. Max supported is %d\\n\",\n\t\t\t  num_tc, bp->max_cos);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (netdev_set_num_tc(dev, num_tc)) {\n\t\tBNX2X_ERR(\"failed to declare %d traffic classes\\n\", num_tc);\n\t\treturn -EINVAL;\n\t}\n\n\tbnx2x_get_c2s_mapping(bp, c2s_map, &c2s_def);\n\n\t \n\tfor (prio = 0; prio < BNX2X_MAX_PRIORITY; prio++) {\n\t\tint outer_prio = c2s_map[prio];\n\n\t\tnetdev_set_prio_tc_map(dev, prio, bp->prio_to_cos[outer_prio]);\n\t\tDP(BNX2X_MSG_SP | NETIF_MSG_IFUP,\n\t\t   \"mapping priority %d to tc %d\\n\",\n\t\t   outer_prio, bp->prio_to_cos[outer_prio]);\n\t}\n\n\t \n\n\t \n\tfor (cos = 0; cos < bp->max_cos; cos++) {\n\t\tcount = BNX2X_NUM_ETH_QUEUES(bp);\n\t\toffset = cos * BNX2X_NUM_NON_CNIC_QUEUES(bp);\n\t\tnetdev_set_tc_queue(dev, cos, count, offset);\n\t\tDP(BNX2X_MSG_SP | NETIF_MSG_IFUP,\n\t\t   \"mapping tc %d to offset %d count %d\\n\",\n\t\t   cos, offset, count);\n\t}\n\n\treturn 0;\n}\n\nint __bnx2x_setup_tc(struct net_device *dev, enum tc_setup_type type,\n\t\t     void *type_data)\n{\n\tstruct tc_mqprio_qopt *mqprio = type_data;\n\n\tif (type != TC_SETUP_QDISC_MQPRIO)\n\t\treturn -EOPNOTSUPP;\n\n\tmqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;\n\n\treturn bnx2x_setup_tc(dev, mqprio->num_tc);\n}\n\n \nint bnx2x_change_mac_addr(struct net_device *dev, void *p)\n{\n\tstruct sockaddr *addr = p;\n\tstruct bnx2x *bp = netdev_priv(dev);\n\tint rc = 0;\n\n\tif (!is_valid_ether_addr(addr->sa_data)) {\n\t\tBNX2X_ERR(\"Requested MAC address is not valid\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (IS_MF_STORAGE_ONLY(bp)) {\n\t\tBNX2X_ERR(\"Can't change address on STORAGE ONLY function\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (netif_running(dev))  {\n\t\trc = bnx2x_set_eth_mac(bp, false);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\teth_hw_addr_set(dev, addr->sa_data);\n\n\tif (netif_running(dev))\n\t\trc = bnx2x_set_eth_mac(bp, true);\n\n\tif (IS_PF(bp) && SHMEM2_HAS(bp, curr_cfg))\n\t\tSHMEM2_WR(bp, curr_cfg, CURR_CFG_MET_OS);\n\n\treturn rc;\n}\n\nstatic void bnx2x_free_fp_mem_at(struct bnx2x *bp, int fp_index)\n{\n\tunion host_hc_status_block *sb = &bnx2x_fp(bp, fp_index, status_blk);\n\tstruct bnx2x_fastpath *fp = &bp->fp[fp_index];\n\tu8 cos;\n\n\t \n\n\tif (IS_FCOE_IDX(fp_index)) {\n\t\tmemset(sb, 0, sizeof(union host_hc_status_block));\n\t\tfp->status_blk_mapping = 0;\n\t} else {\n\t\t \n\t\tif (!CHIP_IS_E1x(bp))\n\t\t\tBNX2X_PCI_FREE(sb->e2_sb,\n\t\t\t\t       bnx2x_fp(bp, fp_index,\n\t\t\t\t\t\tstatus_blk_mapping),\n\t\t\t\t       sizeof(struct host_hc_status_block_e2));\n\t\telse\n\t\t\tBNX2X_PCI_FREE(sb->e1x_sb,\n\t\t\t\t       bnx2x_fp(bp, fp_index,\n\t\t\t\t\t\tstatus_blk_mapping),\n\t\t\t\t       sizeof(struct host_hc_status_block_e1x));\n\t}\n\n\t \n\tif (!skip_rx_queue(bp, fp_index)) {\n\t\tbnx2x_free_rx_bds(fp);\n\n\t\t \n\t\tBNX2X_FREE(bnx2x_fp(bp, fp_index, rx_buf_ring));\n\t\tBNX2X_PCI_FREE(bnx2x_fp(bp, fp_index, rx_desc_ring),\n\t\t\t       bnx2x_fp(bp, fp_index, rx_desc_mapping),\n\t\t\t       sizeof(struct eth_rx_bd) * NUM_RX_BD);\n\n\t\tBNX2X_PCI_FREE(bnx2x_fp(bp, fp_index, rx_comp_ring),\n\t\t\t       bnx2x_fp(bp, fp_index, rx_comp_mapping),\n\t\t\t       sizeof(struct eth_fast_path_rx_cqe) *\n\t\t\t       NUM_RCQ_BD);\n\n\t\t \n\t\tBNX2X_FREE(bnx2x_fp(bp, fp_index, rx_page_ring));\n\t\tBNX2X_PCI_FREE(bnx2x_fp(bp, fp_index, rx_sge_ring),\n\t\t\t       bnx2x_fp(bp, fp_index, rx_sge_mapping),\n\t\t\t       BCM_PAGE_SIZE * NUM_RX_SGE_PAGES);\n\t}\n\n\t \n\tif (!skip_tx_queue(bp, fp_index)) {\n\t\t \n\t\tfor_each_cos_in_tx_queue(fp, cos) {\n\t\t\tstruct bnx2x_fp_txdata *txdata = fp->txdata_ptr[cos];\n\n\t\t\tDP(NETIF_MSG_IFDOWN,\n\t\t\t   \"freeing tx memory of fp %d cos %d cid %d\\n\",\n\t\t\t   fp_index, cos, txdata->cid);\n\n\t\t\tBNX2X_FREE(txdata->tx_buf_ring);\n\t\t\tBNX2X_PCI_FREE(txdata->tx_desc_ring,\n\t\t\t\ttxdata->tx_desc_mapping,\n\t\t\t\tsizeof(union eth_tx_bd_types) * NUM_TX_BD);\n\t\t}\n\t}\n\t \n}\n\nstatic void bnx2x_free_fp_mem_cnic(struct bnx2x *bp)\n{\n\tint i;\n\tfor_each_cnic_queue(bp, i)\n\t\tbnx2x_free_fp_mem_at(bp, i);\n}\n\nvoid bnx2x_free_fp_mem(struct bnx2x *bp)\n{\n\tint i;\n\tfor_each_eth_queue(bp, i)\n\t\tbnx2x_free_fp_mem_at(bp, i);\n}\n\nstatic void set_sb_shortcuts(struct bnx2x *bp, int index)\n{\n\tunion host_hc_status_block status_blk = bnx2x_fp(bp, index, status_blk);\n\tif (!CHIP_IS_E1x(bp)) {\n\t\tbnx2x_fp(bp, index, sb_index_values) =\n\t\t\t(__le16 *)status_blk.e2_sb->sb.index_values;\n\t\tbnx2x_fp(bp, index, sb_running_index) =\n\t\t\t(__le16 *)status_blk.e2_sb->sb.running_index;\n\t} else {\n\t\tbnx2x_fp(bp, index, sb_index_values) =\n\t\t\t(__le16 *)status_blk.e1x_sb->sb.index_values;\n\t\tbnx2x_fp(bp, index, sb_running_index) =\n\t\t\t(__le16 *)status_blk.e1x_sb->sb.running_index;\n\t}\n}\n\n \nstatic int bnx2x_alloc_rx_bds(struct bnx2x_fastpath *fp,\n\t\t\t      int rx_ring_size)\n{\n\tstruct bnx2x *bp = fp->bp;\n\tu16 ring_prod, cqe_ring_prod;\n\tint i, failure_cnt = 0;\n\n\tfp->rx_comp_cons = 0;\n\tcqe_ring_prod = ring_prod = 0;\n\n\t \n\tfor (i = 0; i < rx_ring_size; i++) {\n\t\tif (bnx2x_alloc_rx_data(bp, fp, ring_prod, GFP_KERNEL) < 0) {\n\t\t\tfailure_cnt++;\n\t\t\tcontinue;\n\t\t}\n\t\tring_prod = NEXT_RX_IDX(ring_prod);\n\t\tcqe_ring_prod = NEXT_RCQ_IDX(cqe_ring_prod);\n\t\tWARN_ON(ring_prod <= (i - failure_cnt));\n\t}\n\n\tif (failure_cnt)\n\t\tBNX2X_ERR(\"was only able to allocate %d rx skbs on queue[%d]\\n\",\n\t\t\t  i - failure_cnt, fp->index);\n\n\tfp->rx_bd_prod = ring_prod;\n\t \n\tfp->rx_comp_prod = min_t(u16, NUM_RCQ_RINGS*RCQ_DESC_CNT,\n\t\t\t       cqe_ring_prod);\n\n\tbnx2x_fp_stats(bp, fp)->eth_q_stats.rx_skb_alloc_failed += failure_cnt;\n\n\treturn i - failure_cnt;\n}\n\nstatic void bnx2x_set_next_page_rx_cq(struct bnx2x_fastpath *fp)\n{\n\tint i;\n\n\tfor (i = 1; i <= NUM_RCQ_RINGS; i++) {\n\t\tstruct eth_rx_cqe_next_page *nextpg;\n\n\t\tnextpg = (struct eth_rx_cqe_next_page *)\n\t\t\t&fp->rx_comp_ring[RCQ_DESC_CNT * i - 1];\n\t\tnextpg->addr_hi =\n\t\t\tcpu_to_le32(U64_HI(fp->rx_comp_mapping +\n\t\t\t\t   BCM_PAGE_SIZE*(i % NUM_RCQ_RINGS)));\n\t\tnextpg->addr_lo =\n\t\t\tcpu_to_le32(U64_LO(fp->rx_comp_mapping +\n\t\t\t\t   BCM_PAGE_SIZE*(i % NUM_RCQ_RINGS)));\n\t}\n}\n\nstatic int bnx2x_alloc_fp_mem_at(struct bnx2x *bp, int index)\n{\n\tunion host_hc_status_block *sb;\n\tstruct bnx2x_fastpath *fp = &bp->fp[index];\n\tint ring_size = 0;\n\tu8 cos;\n\tint rx_ring_size = 0;\n\n\tif (!bp->rx_ring_size && IS_MF_STORAGE_ONLY(bp)) {\n\t\trx_ring_size = MIN_RX_SIZE_NONTPA;\n\t\tbp->rx_ring_size = rx_ring_size;\n\t} else if (!bp->rx_ring_size) {\n\t\trx_ring_size = MAX_RX_AVAIL/BNX2X_NUM_RX_QUEUES(bp);\n\n\t\tif (CHIP_IS_E3(bp)) {\n\t\t\tu32 cfg = SHMEM_RD(bp,\n\t\t\t\t\t   dev_info.port_hw_config[BP_PORT(bp)].\n\t\t\t\t\t   default_cfg);\n\n\t\t\t \n\t\t\tif ((cfg & PORT_HW_CFG_NET_SERDES_IF_MASK) ==\n\t\t\t    PORT_HW_CFG_NET_SERDES_IF_SGMII)\n\t\t\t\trx_ring_size /= 10;\n\t\t}\n\n\t\t \n\t\trx_ring_size = max_t(int, bp->disable_tpa ? MIN_RX_SIZE_NONTPA :\n\t\t\t\t     MIN_RX_SIZE_TPA, rx_ring_size);\n\n\t\tbp->rx_ring_size = rx_ring_size;\n\t} else  \n\t\trx_ring_size = bp->rx_ring_size;\n\n\tDP(BNX2X_MSG_SP, \"calculated rx_ring_size %d\\n\", rx_ring_size);\n\n\t \n\tsb = &bnx2x_fp(bp, index, status_blk);\n\n\tif (!IS_FCOE_IDX(index)) {\n\t\t \n\t\tif (!CHIP_IS_E1x(bp)) {\n\t\t\tsb->e2_sb = BNX2X_PCI_ALLOC(&bnx2x_fp(bp, index, status_blk_mapping),\n\t\t\t\t\t\t    sizeof(struct host_hc_status_block_e2));\n\t\t\tif (!sb->e2_sb)\n\t\t\t\tgoto alloc_mem_err;\n\t\t} else {\n\t\t\tsb->e1x_sb = BNX2X_PCI_ALLOC(&bnx2x_fp(bp, index, status_blk_mapping),\n\t\t\t\t\t\t     sizeof(struct host_hc_status_block_e1x));\n\t\t\tif (!sb->e1x_sb)\n\t\t\t\tgoto alloc_mem_err;\n\t\t}\n\t}\n\n\t \n\tif (!IS_FCOE_IDX(index))\n\t\tset_sb_shortcuts(bp, index);\n\n\t \n\tif (!skip_tx_queue(bp, index)) {\n\t\t \n\t\tfor_each_cos_in_tx_queue(fp, cos) {\n\t\t\tstruct bnx2x_fp_txdata *txdata = fp->txdata_ptr[cos];\n\n\t\t\tDP(NETIF_MSG_IFUP,\n\t\t\t   \"allocating tx memory of fp %d cos %d\\n\",\n\t\t\t   index, cos);\n\n\t\t\ttxdata->tx_buf_ring = kcalloc(NUM_TX_BD,\n\t\t\t\t\t\t      sizeof(struct sw_tx_bd),\n\t\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!txdata->tx_buf_ring)\n\t\t\t\tgoto alloc_mem_err;\n\t\t\ttxdata->tx_desc_ring = BNX2X_PCI_ALLOC(&txdata->tx_desc_mapping,\n\t\t\t\t\t\t\t       sizeof(union eth_tx_bd_types) * NUM_TX_BD);\n\t\t\tif (!txdata->tx_desc_ring)\n\t\t\t\tgoto alloc_mem_err;\n\t\t}\n\t}\n\n\t \n\tif (!skip_rx_queue(bp, index)) {\n\t\t \n\t\tbnx2x_fp(bp, index, rx_buf_ring) =\n\t\t\tkcalloc(NUM_RX_BD, sizeof(struct sw_rx_bd), GFP_KERNEL);\n\t\tif (!bnx2x_fp(bp, index, rx_buf_ring))\n\t\t\tgoto alloc_mem_err;\n\t\tbnx2x_fp(bp, index, rx_desc_ring) =\n\t\t\tBNX2X_PCI_ALLOC(&bnx2x_fp(bp, index, rx_desc_mapping),\n\t\t\t\t\tsizeof(struct eth_rx_bd) * NUM_RX_BD);\n\t\tif (!bnx2x_fp(bp, index, rx_desc_ring))\n\t\t\tgoto alloc_mem_err;\n\n\t\t \n\t\tbnx2x_fp(bp, index, rx_comp_ring) =\n\t\t\tBNX2X_PCI_FALLOC(&bnx2x_fp(bp, index, rx_comp_mapping),\n\t\t\t\t\t sizeof(struct eth_fast_path_rx_cqe) * NUM_RCQ_BD);\n\t\tif (!bnx2x_fp(bp, index, rx_comp_ring))\n\t\t\tgoto alloc_mem_err;\n\n\t\t \n\t\tbnx2x_fp(bp, index, rx_page_ring) =\n\t\t\tkcalloc(NUM_RX_SGE, sizeof(struct sw_rx_page),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!bnx2x_fp(bp, index, rx_page_ring))\n\t\t\tgoto alloc_mem_err;\n\t\tbnx2x_fp(bp, index, rx_sge_ring) =\n\t\t\tBNX2X_PCI_ALLOC(&bnx2x_fp(bp, index, rx_sge_mapping),\n\t\t\t\t\tBCM_PAGE_SIZE * NUM_RX_SGE_PAGES);\n\t\tif (!bnx2x_fp(bp, index, rx_sge_ring))\n\t\t\tgoto alloc_mem_err;\n\t\t \n\t\tbnx2x_set_next_page_rx_bd(fp);\n\n\t\t \n\t\tbnx2x_set_next_page_rx_cq(fp);\n\n\t\t \n\t\tring_size = bnx2x_alloc_rx_bds(fp, rx_ring_size);\n\t\tif (ring_size < rx_ring_size)\n\t\t\tgoto alloc_mem_err;\n\t}\n\n\treturn 0;\n\n \nalloc_mem_err:\n\tBNX2X_ERR(\"Unable to allocate full memory for queue %d (size %d)\\n\",\n\t\t\t\t\t\tindex, ring_size);\n\t \n\tif (ring_size < (fp->mode == TPA_MODE_DISABLED ?\n\t\t\t\tMIN_RX_SIZE_NONTPA : MIN_RX_SIZE_TPA)) {\n\t\t\t \n\t\t\tbnx2x_free_fp_mem_at(bp, index);\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic int bnx2x_alloc_fp_mem_cnic(struct bnx2x *bp)\n{\n\tif (!NO_FCOE(bp))\n\t\t \n\t\tif (bnx2x_alloc_fp_mem_at(bp, FCOE_IDX(bp)))\n\t\t\t \n\t\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int bnx2x_alloc_fp_mem(struct bnx2x *bp)\n{\n\tint i;\n\n\t \n\n\t \n\tif (bnx2x_alloc_fp_mem_at(bp, 0))\n\t\treturn -ENOMEM;\n\n\t \n\tfor_each_nondefault_eth_queue(bp, i)\n\t\tif (bnx2x_alloc_fp_mem_at(bp, i))\n\t\t\tbreak;\n\n\t \n\tif (i != BNX2X_NUM_ETH_QUEUES(bp)) {\n\t\tint delta = BNX2X_NUM_ETH_QUEUES(bp) - i;\n\n\t\tWARN_ON(delta < 0);\n\t\tbnx2x_shrink_eth_fp(bp, delta);\n\t\tif (CNIC_SUPPORT(bp))\n\t\t\t \n\n\t\t\t \n\t\t\tbnx2x_move_fp(bp, FCOE_IDX(bp), FCOE_IDX(bp) - delta);\n\t\tbp->num_ethernet_queues -= delta;\n\t\tbp->num_queues = bp->num_ethernet_queues +\n\t\t\t\t bp->num_cnic_queues;\n\t\tBNX2X_ERR(\"Adjusted num of queues from %d to %d\\n\",\n\t\t\t  bp->num_queues + delta, bp->num_queues);\n\t}\n\n\treturn 0;\n}\n\nvoid bnx2x_free_mem_bp(struct bnx2x *bp)\n{\n\tint i;\n\n\tfor (i = 0; i < bp->fp_array_size; i++)\n\t\tkfree(bp->fp[i].tpa_info);\n\tkfree(bp->fp);\n\tkfree(bp->sp_objs);\n\tkfree(bp->fp_stats);\n\tkfree(bp->bnx2x_txq);\n\tkfree(bp->msix_table);\n\tkfree(bp->ilt);\n}\n\nint bnx2x_alloc_mem_bp(struct bnx2x *bp)\n{\n\tstruct bnx2x_fastpath *fp;\n\tstruct msix_entry *tbl;\n\tstruct bnx2x_ilt *ilt;\n\tint msix_table_size = 0;\n\tint fp_array_size, txq_array_size;\n\tint i;\n\n\t \n\tmsix_table_size = bp->igu_sb_cnt;\n\tif (IS_PF(bp))\n\t\tmsix_table_size++;\n\tBNX2X_DEV_INFO(\"msix_table_size %d\\n\", msix_table_size);\n\n\t \n\tfp_array_size = BNX2X_MAX_RSS_COUNT(bp) + CNIC_SUPPORT(bp);\n\tbp->fp_array_size = fp_array_size;\n\tBNX2X_DEV_INFO(\"fp_array_size %d\\n\", bp->fp_array_size);\n\n\tfp = kcalloc(bp->fp_array_size, sizeof(*fp), GFP_KERNEL);\n\tif (!fp)\n\t\tgoto alloc_err;\n\tfor (i = 0; i < bp->fp_array_size; i++) {\n\t\tfp[i].tpa_info =\n\t\t\tkcalloc(ETH_MAX_AGGREGATION_QUEUES_E1H_E2,\n\t\t\t\tsizeof(struct bnx2x_agg_info), GFP_KERNEL);\n\t\tif (!(fp[i].tpa_info))\n\t\t\tgoto alloc_err;\n\t}\n\n\tbp->fp = fp;\n\n\t \n\tbp->sp_objs = kcalloc(bp->fp_array_size, sizeof(struct bnx2x_sp_objs),\n\t\t\t      GFP_KERNEL);\n\tif (!bp->sp_objs)\n\t\tgoto alloc_err;\n\n\t \n\tbp->fp_stats = kcalloc(bp->fp_array_size, sizeof(struct bnx2x_fp_stats),\n\t\t\t       GFP_KERNEL);\n\tif (!bp->fp_stats)\n\t\tgoto alloc_err;\n\n\t \n\ttxq_array_size =\n\t\tBNX2X_MAX_RSS_COUNT(bp) * BNX2X_MULTI_TX_COS + CNIC_SUPPORT(bp);\n\tBNX2X_DEV_INFO(\"txq_array_size %d\", txq_array_size);\n\n\tbp->bnx2x_txq = kcalloc(txq_array_size, sizeof(struct bnx2x_fp_txdata),\n\t\t\t\tGFP_KERNEL);\n\tif (!bp->bnx2x_txq)\n\t\tgoto alloc_err;\n\n\t \n\ttbl = kcalloc(msix_table_size, sizeof(*tbl), GFP_KERNEL);\n\tif (!tbl)\n\t\tgoto alloc_err;\n\tbp->msix_table = tbl;\n\n\t \n\tilt = kzalloc(sizeof(*ilt), GFP_KERNEL);\n\tif (!ilt)\n\t\tgoto alloc_err;\n\tbp->ilt = ilt;\n\n\treturn 0;\nalloc_err:\n\tbnx2x_free_mem_bp(bp);\n\treturn -ENOMEM;\n}\n\nint bnx2x_reload_if_running(struct net_device *dev)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\n\tif (unlikely(!netif_running(dev)))\n\t\treturn 0;\n\n\tbnx2x_nic_unload(bp, UNLOAD_NORMAL, true);\n\treturn bnx2x_nic_load(bp, LOAD_NORMAL);\n}\n\nint bnx2x_get_cur_phy_idx(struct bnx2x *bp)\n{\n\tu32 sel_phy_idx = 0;\n\tif (bp->link_params.num_phys <= 1)\n\t\treturn INT_PHY;\n\n\tif (bp->link_vars.link_up) {\n\t\tsel_phy_idx = EXT_PHY1;\n\t\t \n\t\tif ((bp->link_vars.link_status & LINK_STATUS_SERDES_LINK) &&\n\t\t    (bp->link_params.phy[EXT_PHY2].supported & SUPPORTED_FIBRE))\n\t\t\tsel_phy_idx = EXT_PHY2;\n\t} else {\n\n\t\tswitch (bnx2x_phy_selection(&bp->link_params)) {\n\t\tcase PORT_HW_CFG_PHY_SELECTION_HARDWARE_DEFAULT:\n\t\tcase PORT_HW_CFG_PHY_SELECTION_FIRST_PHY:\n\t\tcase PORT_HW_CFG_PHY_SELECTION_FIRST_PHY_PRIORITY:\n\t\t       sel_phy_idx = EXT_PHY1;\n\t\t       break;\n\t\tcase PORT_HW_CFG_PHY_SELECTION_SECOND_PHY:\n\t\tcase PORT_HW_CFG_PHY_SELECTION_SECOND_PHY_PRIORITY:\n\t\t       sel_phy_idx = EXT_PHY2;\n\t\t       break;\n\t\t}\n\t}\n\n\treturn sel_phy_idx;\n}\nint bnx2x_get_link_cfg_idx(struct bnx2x *bp)\n{\n\tu32 sel_phy_idx = bnx2x_get_cur_phy_idx(bp);\n\t \n\n\tif (bp->link_params.multi_phy_config &\n\t    PORT_HW_CFG_PHY_SWAPPED_ENABLED) {\n\t\tif (sel_phy_idx == EXT_PHY1)\n\t\t\tsel_phy_idx = EXT_PHY2;\n\t\telse if (sel_phy_idx == EXT_PHY2)\n\t\t\tsel_phy_idx = EXT_PHY1;\n\t}\n\treturn LINK_CONFIG_IDX(sel_phy_idx);\n}\n\n#ifdef NETDEV_FCOE_WWNN\nint bnx2x_fcoe_get_wwn(struct net_device *dev, u64 *wwn, int type)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\tstruct cnic_eth_dev *cp = &bp->cnic_eth_dev;\n\n\tswitch (type) {\n\tcase NETDEV_FCOE_WWNN:\n\t\t*wwn = HILO_U64(cp->fcoe_wwn_node_name_hi,\n\t\t\t\tcp->fcoe_wwn_node_name_lo);\n\t\tbreak;\n\tcase NETDEV_FCOE_WWPN:\n\t\t*wwn = HILO_U64(cp->fcoe_wwn_port_name_hi,\n\t\t\t\tcp->fcoe_wwn_port_name_lo);\n\t\tbreak;\n\tdefault:\n\t\tBNX2X_ERR(\"Wrong WWN type requested - %d\\n\", type);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n#endif\n\n \nint bnx2x_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\n\tif (pci_num_vf(bp->pdev)) {\n\t\tDP(BNX2X_MSG_IOV, \"VFs are enabled, can not change MTU\\n\");\n\t\treturn -EPERM;\n\t}\n\n\tif (bp->recovery_state != BNX2X_RECOVERY_DONE) {\n\t\tBNX2X_ERR(\"Can't perform change MTU during parity recovery\\n\");\n\t\treturn -EAGAIN;\n\t}\n\n\t \n\tdev->mtu = new_mtu;\n\n\tif (!bnx2x_mtu_allows_gro(new_mtu))\n\t\tdev->features &= ~NETIF_F_GRO_HW;\n\n\tif (IS_PF(bp) && SHMEM2_HAS(bp, curr_cfg))\n\t\tSHMEM2_WR(bp, curr_cfg, CURR_CFG_MET_OS);\n\n\treturn bnx2x_reload_if_running(dev);\n}\n\nnetdev_features_t bnx2x_fix_features(struct net_device *dev,\n\t\t\t\t     netdev_features_t features)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\n\tif (pci_num_vf(bp->pdev)) {\n\t\tnetdev_features_t changed = dev->features ^ features;\n\n\t\t \n\t\tif (!(features & NETIF_F_RXCSUM) && !bp->disable_tpa) {\n\t\t\tfeatures &= ~NETIF_F_RXCSUM;\n\t\t\tfeatures |= dev->features & NETIF_F_RXCSUM;\n\t\t}\n\n\t\tif (changed & NETIF_F_LOOPBACK) {\n\t\t\tfeatures &= ~NETIF_F_LOOPBACK;\n\t\t\tfeatures |= dev->features & NETIF_F_LOOPBACK;\n\t\t}\n\t}\n\n\t \n\tif (!(features & NETIF_F_RXCSUM))\n\t\tfeatures &= ~NETIF_F_LRO;\n\n\tif (!(features & NETIF_F_GRO) || !bnx2x_mtu_allows_gro(dev->mtu))\n\t\tfeatures &= ~NETIF_F_GRO_HW;\n\tif (features & NETIF_F_GRO_HW)\n\t\tfeatures &= ~NETIF_F_LRO;\n\n\treturn features;\n}\n\nint bnx2x_set_features(struct net_device *dev, netdev_features_t features)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\tnetdev_features_t changes = features ^ dev->features;\n\tbool bnx2x_reload = false;\n\tint rc;\n\n\t \n\tif (!pci_num_vf(bp->pdev)) {\n\t\tif (features & NETIF_F_LOOPBACK) {\n\t\t\tif (bp->link_params.loopback_mode != LOOPBACK_BMAC) {\n\t\t\t\tbp->link_params.loopback_mode = LOOPBACK_BMAC;\n\t\t\t\tbnx2x_reload = true;\n\t\t\t}\n\t\t} else {\n\t\t\tif (bp->link_params.loopback_mode != LOOPBACK_NONE) {\n\t\t\t\tbp->link_params.loopback_mode = LOOPBACK_NONE;\n\t\t\t\tbnx2x_reload = true;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tchanges &= ~NETIF_F_GRO;\n\n\tif (changes)\n\t\tbnx2x_reload = true;\n\n\tif (bnx2x_reload) {\n\t\tif (bp->recovery_state == BNX2X_RECOVERY_DONE) {\n\t\t\tdev->features = features;\n\t\t\trc = bnx2x_reload_if_running(dev);\n\t\t\treturn rc ? rc : 1;\n\t\t}\n\t\t \n\t}\n\n\treturn 0;\n}\n\nvoid bnx2x_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct bnx2x *bp = netdev_priv(dev);\n\n\t \n\tif (!bp->panic)\n#ifndef BNX2X_STOP_ON_ERROR\n\t\tbnx2x_panic_dump(bp, false);\n#else\n\t\tbnx2x_panic();\n#endif\n\n\t \n\tbnx2x_schedule_sp_rtnl(bp, BNX2X_SP_RTNL_TX_TIMEOUT, 0);\n}\n\nstatic int __maybe_unused bnx2x_suspend(struct device *dev_d)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev_d);\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct bnx2x *bp;\n\n\tif (!dev) {\n\t\tdev_err(&pdev->dev, \"BAD net device from bnx2x_init_one\\n\");\n\t\treturn -ENODEV;\n\t}\n\tbp = netdev_priv(dev);\n\n\trtnl_lock();\n\n\tif (!netif_running(dev)) {\n\t\trtnl_unlock();\n\t\treturn 0;\n\t}\n\n\tnetif_device_detach(dev);\n\n\tbnx2x_nic_unload(bp, UNLOAD_CLOSE, false);\n\n\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic int __maybe_unused bnx2x_resume(struct device *dev_d)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev_d);\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct bnx2x *bp;\n\tint rc;\n\n\tif (!dev) {\n\t\tdev_err(&pdev->dev, \"BAD net device from bnx2x_init_one\\n\");\n\t\treturn -ENODEV;\n\t}\n\tbp = netdev_priv(dev);\n\n\tif (bp->recovery_state != BNX2X_RECOVERY_DONE) {\n\t\tBNX2X_ERR(\"Handling parity error recovery. Try again later\\n\");\n\t\treturn -EAGAIN;\n\t}\n\n\trtnl_lock();\n\n\tif (!netif_running(dev)) {\n\t\trtnl_unlock();\n\t\treturn 0;\n\t}\n\n\tnetif_device_attach(dev);\n\n\trc = bnx2x_nic_load(bp, LOAD_OPEN);\n\n\trtnl_unlock();\n\n\treturn rc;\n}\n\nSIMPLE_DEV_PM_OPS(bnx2x_pm_ops, bnx2x_suspend, bnx2x_resume);\n\nvoid bnx2x_set_ctx_validation(struct bnx2x *bp, struct eth_context *cxt,\n\t\t\t      u32 cid)\n{\n\tif (!cxt) {\n\t\tBNX2X_ERR(\"bad context pointer %p\\n\", cxt);\n\t\treturn;\n\t}\n\n\t \n\tcxt->ustorm_ag_context.cdu_usage =\n\t\tCDU_RSRVD_VALUE_TYPE_A(HW_CID(bp, cid),\n\t\t\tCDU_REGION_NUMBER_UCM_AG, ETH_CONNECTION_TYPE);\n\t \n\tcxt->xstorm_ag_context.cdu_reserved =\n\t\tCDU_RSRVD_VALUE_TYPE_A(HW_CID(bp, cid),\n\t\t\tCDU_REGION_NUMBER_XCM_AG, ETH_CONNECTION_TYPE);\n}\n\nstatic void storm_memset_hc_timeout(struct bnx2x *bp, u8 port,\n\t\t\t\t    u8 fw_sb_id, u8 sb_index,\n\t\t\t\t    u8 ticks)\n{\n\tu32 addr = BAR_CSTRORM_INTMEM +\n\t\t   CSTORM_STATUS_BLOCK_DATA_TIMEOUT_OFFSET(fw_sb_id, sb_index);\n\tREG_WR8(bp, addr, ticks);\n\tDP(NETIF_MSG_IFUP,\n\t   \"port %x fw_sb_id %d sb_index %d ticks %d\\n\",\n\t   port, fw_sb_id, sb_index, ticks);\n}\n\nstatic void storm_memset_hc_disable(struct bnx2x *bp, u8 port,\n\t\t\t\t    u16 fw_sb_id, u8 sb_index,\n\t\t\t\t    u8 disable)\n{\n\tu32 enable_flag = disable ? 0 : (1 << HC_INDEX_DATA_HC_ENABLED_SHIFT);\n\tu32 addr = BAR_CSTRORM_INTMEM +\n\t\t   CSTORM_STATUS_BLOCK_DATA_FLAGS_OFFSET(fw_sb_id, sb_index);\n\tu8 flags = REG_RD8(bp, addr);\n\t \n\tflags &= ~HC_INDEX_DATA_HC_ENABLED;\n\tflags |= enable_flag;\n\tREG_WR8(bp, addr, flags);\n\tDP(NETIF_MSG_IFUP,\n\t   \"port %x fw_sb_id %d sb_index %d disable %d\\n\",\n\t   port, fw_sb_id, sb_index, disable);\n}\n\nvoid bnx2x_update_coalesce_sb_index(struct bnx2x *bp, u8 fw_sb_id,\n\t\t\t\t    u8 sb_index, u8 disable, u16 usec)\n{\n\tint port = BP_PORT(bp);\n\tu8 ticks = usec / BNX2X_BTR;\n\n\tstorm_memset_hc_timeout(bp, port, fw_sb_id, sb_index, ticks);\n\n\tdisable = disable ? 1 : (usec ? 0 : 1);\n\tstorm_memset_hc_disable(bp, port, fw_sb_id, sb_index, disable);\n}\n\nvoid bnx2x_schedule_sp_rtnl(struct bnx2x *bp, enum sp_rtnl_flag flag,\n\t\t\t    u32 verbose)\n{\n\tsmp_mb__before_atomic();\n\tset_bit(flag, &bp->sp_rtnl_state);\n\tsmp_mb__after_atomic();\n\tDP((BNX2X_MSG_SP | verbose), \"Scheduling sp_rtnl task [Flag: %d]\\n\",\n\t   flag);\n\tschedule_delayed_work(&bp->sp_rtnl_task, 0);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}