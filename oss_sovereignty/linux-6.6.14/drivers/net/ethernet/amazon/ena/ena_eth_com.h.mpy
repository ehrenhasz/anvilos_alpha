{
  "module_name": "ena_eth_com.h",
  "hash_id": "cdc235ca652fb1d5103edaa2799460b5f79c51583b0e78f77cb8bfd6482a3457",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/amazon/ena/ena_eth_com.h",
  "human_readable_source": " \n \n\n#ifndef ENA_ETH_COM_H_\n#define ENA_ETH_COM_H_\n\n#include \"ena_com.h\"\n\n \n#define ENA_COMP_HEAD_THRESH 4\n \n#define ENA_LLQ_ENTRY_DESC_CHUNK_SIZE\t(2 * sizeof(struct ena_eth_io_tx_desc))\n#define ENA_LLQ_HEADER\t\t(128UL - ENA_LLQ_ENTRY_DESC_CHUNK_SIZE)\n#define ENA_LLQ_LARGE_HEADER\t(256UL - ENA_LLQ_ENTRY_DESC_CHUNK_SIZE)\n\nstruct ena_com_tx_ctx {\n\tstruct ena_com_tx_meta ena_meta;\n\tstruct ena_com_buf *ena_bufs;\n\t \n\tvoid *push_header;\n\n\tenum ena_eth_io_l3_proto_index l3_proto;\n\tenum ena_eth_io_l4_proto_index l4_proto;\n\tu16 num_bufs;\n\tu16 req_id;\n\t \n\tu16 header_len;\n\n\tu8 meta_valid;\n\tu8 tso_enable;\n\tu8 l3_csum_enable;\n\tu8 l4_csum_enable;\n\tu8 l4_csum_partial;\n\tu8 df;  \n};\n\nstruct ena_com_rx_ctx {\n\tstruct ena_com_rx_buf_info *ena_bufs;\n\tenum ena_eth_io_l3_proto_index l3_proto;\n\tenum ena_eth_io_l4_proto_index l4_proto;\n\tbool l3_csum_err;\n\tbool l4_csum_err;\n\tu8 l4_csum_checked;\n\t \n\tbool frag;\n\tu32 hash;\n\tu16 descs;\n\tint max_bufs;\n\tu8 pkt_offset;\n};\n\nint ena_com_prepare_tx(struct ena_com_io_sq *io_sq,\n\t\t       struct ena_com_tx_ctx *ena_tx_ctx,\n\t\t       int *nb_hw_desc);\n\nint ena_com_rx_pkt(struct ena_com_io_cq *io_cq,\n\t\t   struct ena_com_io_sq *io_sq,\n\t\t   struct ena_com_rx_ctx *ena_rx_ctx);\n\nint ena_com_add_single_rx_desc(struct ena_com_io_sq *io_sq,\n\t\t\t       struct ena_com_buf *ena_buf,\n\t\t\t       u16 req_id);\n\nbool ena_com_cq_empty(struct ena_com_io_cq *io_cq);\n\nstatic inline void ena_com_unmask_intr(struct ena_com_io_cq *io_cq,\n\t\t\t\t       struct ena_eth_io_intr_reg *intr_reg)\n{\n\twritel(intr_reg->intr_control, io_cq->unmask_reg);\n}\n\nstatic inline int ena_com_free_q_entries(struct ena_com_io_sq *io_sq)\n{\n\tu16 tail, next_to_comp, cnt;\n\n\tnext_to_comp = io_sq->next_to_comp;\n\ttail = io_sq->tail;\n\tcnt = tail - next_to_comp;\n\n\treturn io_sq->q_depth - 1 - cnt;\n}\n\n \nstatic inline bool ena_com_sq_have_enough_space(struct ena_com_io_sq *io_sq,\n\t\t\t\t\t\tu16 required_buffers)\n{\n\tint temp;\n\n\tif (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST)\n\t\treturn ena_com_free_q_entries(io_sq) >= required_buffers;\n\n\t \n\ttemp = required_buffers / io_sq->llq_info.descs_per_entry + 2;\n\n\treturn ena_com_free_q_entries(io_sq) > temp;\n}\n\nstatic inline bool ena_com_meta_desc_changed(struct ena_com_io_sq *io_sq,\n\t\t\t\t\t     struct ena_com_tx_ctx *ena_tx_ctx)\n{\n\tif (!ena_tx_ctx->meta_valid)\n\t\treturn false;\n\n\treturn !!memcmp(&io_sq->cached_tx_meta,\n\t\t\t&ena_tx_ctx->ena_meta,\n\t\t\tsizeof(struct ena_com_tx_meta));\n}\n\nstatic inline bool is_llq_max_tx_burst_exists(struct ena_com_io_sq *io_sq)\n{\n\treturn (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV) &&\n\t       io_sq->llq_info.max_entries_in_tx_burst > 0;\n}\n\nstatic inline bool ena_com_is_doorbell_needed(struct ena_com_io_sq *io_sq,\n\t\t\t\t\t      struct ena_com_tx_ctx *ena_tx_ctx)\n{\n\tstruct ena_com_llq_info *llq_info;\n\tint descs_after_first_entry;\n\tint num_entries_needed = 1;\n\tu16 num_descs;\n\n\tif (!is_llq_max_tx_burst_exists(io_sq))\n\t\treturn false;\n\n\tllq_info = &io_sq->llq_info;\n\tnum_descs = ena_tx_ctx->num_bufs;\n\n\tif (llq_info->disable_meta_caching ||\n\t    unlikely(ena_com_meta_desc_changed(io_sq, ena_tx_ctx)))\n\t\t++num_descs;\n\n\tif (num_descs > llq_info->descs_num_before_header) {\n\t\tdescs_after_first_entry = num_descs - llq_info->descs_num_before_header;\n\t\tnum_entries_needed += DIV_ROUND_UP(descs_after_first_entry,\n\t\t\t\t\t\t   llq_info->descs_per_entry);\n\t}\n\n\tnetdev_dbg(ena_com_io_sq_to_ena_dev(io_sq)->net_device,\n\t\t   \"Queue: %d num_descs: %d num_entries_needed: %d\\n\",\n\t\t   io_sq->qid, num_descs, num_entries_needed);\n\n\treturn num_entries_needed > io_sq->entries_in_tx_burst_left;\n}\n\nstatic inline int ena_com_write_sq_doorbell(struct ena_com_io_sq *io_sq)\n{\n\tu16 max_entries_in_tx_burst = io_sq->llq_info.max_entries_in_tx_burst;\n\tu16 tail = io_sq->tail;\n\n\tnetdev_dbg(ena_com_io_sq_to_ena_dev(io_sq)->net_device,\n\t\t   \"Write submission queue doorbell for queue: %d tail: %d\\n\",\n\t\t   io_sq->qid, tail);\n\n\twritel(tail, io_sq->db_addr);\n\n\tif (is_llq_max_tx_burst_exists(io_sq)) {\n\t\tnetdev_dbg(ena_com_io_sq_to_ena_dev(io_sq)->net_device,\n\t\t\t   \"Reset available entries in tx burst for queue %d to %d\\n\",\n\t\t\t   io_sq->qid, max_entries_in_tx_burst);\n\t\tio_sq->entries_in_tx_burst_left = max_entries_in_tx_burst;\n\t}\n\n\treturn 0;\n}\n\nstatic inline int ena_com_update_dev_comp_head(struct ena_com_io_cq *io_cq)\n{\n\tu16 unreported_comp, head;\n\tbool need_update;\n\n\tif (unlikely(io_cq->cq_head_db_reg)) {\n\t\thead = io_cq->head;\n\t\tunreported_comp = head - io_cq->last_head_update;\n\t\tneed_update = unreported_comp > (io_cq->q_depth / ENA_COMP_HEAD_THRESH);\n\n\t\tif (unlikely(need_update)) {\n\t\t\tnetdev_dbg(ena_com_io_cq_to_ena_dev(io_cq)->net_device,\n\t\t\t\t   \"Write completion queue doorbell for queue %d: head: %d\\n\",\n\t\t\t\t   io_cq->qid, head);\n\t\t\twritel(head, io_cq->cq_head_db_reg);\n\t\t\tio_cq->last_head_update = head;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic inline void ena_com_update_numa_node(struct ena_com_io_cq *io_cq,\n\t\t\t\t\t    u8 numa_node)\n{\n\tstruct ena_eth_io_numa_node_cfg_reg numa_cfg;\n\n\tif (!io_cq->numa_node_cfg_reg)\n\t\treturn;\n\n\tnuma_cfg.numa_cfg = (numa_node & ENA_ETH_IO_NUMA_NODE_CFG_REG_NUMA_MASK)\n\t\t| ENA_ETH_IO_NUMA_NODE_CFG_REG_ENABLED_MASK;\n\n\twritel(numa_cfg.numa_cfg, io_cq->numa_node_cfg_reg);\n}\n\nstatic inline void ena_com_comp_ack(struct ena_com_io_sq *io_sq, u16 elem)\n{\n\tio_sq->next_to_comp += elem;\n}\n\nstatic inline void ena_com_cq_inc_head(struct ena_com_io_cq *io_cq)\n{\n\tio_cq->head++;\n\n\t \n\tif (unlikely((io_cq->head & (io_cq->q_depth - 1)) == 0))\n\t\tio_cq->phase ^= 1;\n}\n\nstatic inline int ena_com_tx_comp_req_id_get(struct ena_com_io_cq *io_cq,\n\t\t\t\t\t     u16 *req_id)\n{\n\tu8 expected_phase, cdesc_phase;\n\tstruct ena_eth_io_tx_cdesc *cdesc;\n\tu16 masked_head;\n\n\tmasked_head = io_cq->head & (io_cq->q_depth - 1);\n\texpected_phase = io_cq->phase;\n\n\tcdesc = (struct ena_eth_io_tx_cdesc *)\n\t\t((uintptr_t)io_cq->cdesc_addr.virt_addr +\n\t\t(masked_head * io_cq->cdesc_entry_size_in_bytes));\n\n\t \n\tcdesc_phase = READ_ONCE(cdesc->flags) & ENA_ETH_IO_TX_CDESC_PHASE_MASK;\n\tif (cdesc_phase != expected_phase)\n\t\treturn -EAGAIN;\n\n\tdma_rmb();\n\n\t*req_id = READ_ONCE(cdesc->req_id);\n\tif (unlikely(*req_id >= io_cq->q_depth)) {\n\t\tnetdev_err(ena_com_io_cq_to_ena_dev(io_cq)->net_device,\n\t\t\t   \"Invalid req id %d\\n\", cdesc->req_id);\n\t\treturn -EINVAL;\n\t}\n\n\tena_com_cq_inc_head(io_cq);\n\n\treturn 0;\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}