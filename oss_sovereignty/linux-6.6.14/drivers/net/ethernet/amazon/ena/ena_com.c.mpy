{
  "module_name": "ena_com.c",
  "hash_id": "3b42b73f8296929ac6450f0df5f5bd323a6661b11341b8eda3adfc28dc54c7c0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/amazon/ena/ena_com.c",
  "human_readable_source": "\n \n\n#include \"ena_com.h\"\n\n \n \n\n \n#define ADMIN_CMD_TIMEOUT_US (3000000)\n\n#define ENA_ASYNC_QUEUE_DEPTH 16\n#define ENA_ADMIN_QUEUE_DEPTH 32\n\n\n#define ENA_CTRL_MAJOR\t\t0\n#define ENA_CTRL_MINOR\t\t0\n#define ENA_CTRL_SUB_MINOR\t1\n\n#define MIN_ENA_CTRL_VER \\\n\t(((ENA_CTRL_MAJOR) << \\\n\t(ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_SHIFT)) | \\\n\t((ENA_CTRL_MINOR) << \\\n\t(ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_SHIFT)) | \\\n\t(ENA_CTRL_SUB_MINOR))\n\n#define ENA_DMA_ADDR_TO_UINT32_LOW(x)\t((u32)((u64)(x)))\n#define ENA_DMA_ADDR_TO_UINT32_HIGH(x)\t((u32)(((u64)(x)) >> 32))\n\n#define ENA_MMIO_READ_TIMEOUT 0xFFFFFFFF\n\n#define ENA_COM_BOUNCE_BUFFER_CNTRL_CNT\t4\n\n#define ENA_REGS_ADMIN_INTR_MASK 1\n\n#define ENA_MAX_BACKOFF_DELAY_EXP 16U\n\n#define ENA_MIN_ADMIN_POLL_US 100\n\n#define ENA_MAX_ADMIN_POLL_US 5000\n\n \n \n \n\nenum ena_cmd_status {\n\tENA_CMD_SUBMITTED,\n\tENA_CMD_COMPLETED,\n\t \n\tENA_CMD_ABORTED,\n};\n\nstruct ena_comp_ctx {\n\tstruct completion wait_event;\n\tstruct ena_admin_acq_entry *user_cqe;\n\tu32 comp_size;\n\tenum ena_cmd_status status;\n\t \n\tu8 comp_status;\n\tu8 cmd_opcode;\n\tbool occupied;\n};\n\nstruct ena_com_stats_ctx {\n\tstruct ena_admin_aq_get_stats_cmd get_cmd;\n\tstruct ena_admin_acq_get_stats_resp get_resp;\n};\n\nstatic int ena_com_mem_addr_set(struct ena_com_dev *ena_dev,\n\t\t\t\t       struct ena_common_mem_addr *ena_addr,\n\t\t\t\t       dma_addr_t addr)\n{\n\tif ((addr & GENMASK_ULL(ena_dev->dma_addr_bits - 1, 0)) != addr) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"DMA address has more bits that the device supports\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tena_addr->mem_addr_low = lower_32_bits(addr);\n\tena_addr->mem_addr_high = (u16)upper_32_bits(addr);\n\n\treturn 0;\n}\n\nstatic int ena_com_admin_init_sq(struct ena_com_admin_queue *admin_queue)\n{\n\tstruct ena_com_dev *ena_dev = admin_queue->ena_dev;\n\tstruct ena_com_admin_sq *sq = &admin_queue->sq;\n\tu16 size = ADMIN_SQ_SIZE(admin_queue->q_depth);\n\n\tsq->entries = dma_alloc_coherent(admin_queue->q_dmadev, size,\n\t\t\t\t\t &sq->dma_addr, GFP_KERNEL);\n\n\tif (!sq->entries) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tsq->head = 0;\n\tsq->tail = 0;\n\tsq->phase = 1;\n\n\tsq->db_addr = NULL;\n\n\treturn 0;\n}\n\nstatic int ena_com_admin_init_cq(struct ena_com_admin_queue *admin_queue)\n{\n\tstruct ena_com_dev *ena_dev = admin_queue->ena_dev;\n\tstruct ena_com_admin_cq *cq = &admin_queue->cq;\n\tu16 size = ADMIN_CQ_SIZE(admin_queue->q_depth);\n\n\tcq->entries = dma_alloc_coherent(admin_queue->q_dmadev, size,\n\t\t\t\t\t &cq->dma_addr, GFP_KERNEL);\n\n\tif (!cq->entries) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tcq->head = 0;\n\tcq->phase = 1;\n\n\treturn 0;\n}\n\nstatic int ena_com_admin_init_aenq(struct ena_com_dev *ena_dev,\n\t\t\t\t   struct ena_aenq_handlers *aenq_handlers)\n{\n\tstruct ena_com_aenq *aenq = &ena_dev->aenq;\n\tu32 addr_low, addr_high, aenq_caps;\n\tu16 size;\n\n\tena_dev->aenq.q_depth = ENA_ASYNC_QUEUE_DEPTH;\n\tsize = ADMIN_AENQ_SIZE(ENA_ASYNC_QUEUE_DEPTH);\n\taenq->entries = dma_alloc_coherent(ena_dev->dmadev, size,\n\t\t\t\t\t   &aenq->dma_addr, GFP_KERNEL);\n\n\tif (!aenq->entries) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\taenq->head = aenq->q_depth;\n\taenq->phase = 1;\n\n\taddr_low = ENA_DMA_ADDR_TO_UINT32_LOW(aenq->dma_addr);\n\taddr_high = ENA_DMA_ADDR_TO_UINT32_HIGH(aenq->dma_addr);\n\n\twritel(addr_low, ena_dev->reg_bar + ENA_REGS_AENQ_BASE_LO_OFF);\n\twritel(addr_high, ena_dev->reg_bar + ENA_REGS_AENQ_BASE_HI_OFF);\n\n\taenq_caps = 0;\n\taenq_caps |= ena_dev->aenq.q_depth & ENA_REGS_AENQ_CAPS_AENQ_DEPTH_MASK;\n\taenq_caps |= (sizeof(struct ena_admin_aenq_entry)\n\t\t      << ENA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE_SHIFT) &\n\t\t     ENA_REGS_AENQ_CAPS_AENQ_ENTRY_SIZE_MASK;\n\twritel(aenq_caps, ena_dev->reg_bar + ENA_REGS_AENQ_CAPS_OFF);\n\n\tif (unlikely(!aenq_handlers)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"AENQ handlers pointer is NULL\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\taenq->aenq_handlers = aenq_handlers;\n\n\treturn 0;\n}\n\nstatic void comp_ctxt_release(struct ena_com_admin_queue *queue,\n\t\t\t\t     struct ena_comp_ctx *comp_ctx)\n{\n\tcomp_ctx->occupied = false;\n\tatomic_dec(&queue->outstanding_cmds);\n}\n\nstatic struct ena_comp_ctx *get_comp_ctxt(struct ena_com_admin_queue *admin_queue,\n\t\t\t\t\t  u16 command_id, bool capture)\n{\n\tif (unlikely(command_id >= admin_queue->q_depth)) {\n\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t   \"Command id is larger than the queue size. cmd_id: %u queue size %d\\n\",\n\t\t\t   command_id, admin_queue->q_depth);\n\t\treturn NULL;\n\t}\n\n\tif (unlikely(!admin_queue->comp_ctx)) {\n\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t   \"Completion context is NULL\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (unlikely(admin_queue->comp_ctx[command_id].occupied && capture)) {\n\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t   \"Completion context is occupied\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (capture) {\n\t\tatomic_inc(&admin_queue->outstanding_cmds);\n\t\tadmin_queue->comp_ctx[command_id].occupied = true;\n\t}\n\n\treturn &admin_queue->comp_ctx[command_id];\n}\n\nstatic struct ena_comp_ctx *__ena_com_submit_admin_cmd(struct ena_com_admin_queue *admin_queue,\n\t\t\t\t\t\t       struct ena_admin_aq_entry *cmd,\n\t\t\t\t\t\t       size_t cmd_size_in_bytes,\n\t\t\t\t\t\t       struct ena_admin_acq_entry *comp,\n\t\t\t\t\t\t       size_t comp_size_in_bytes)\n{\n\tstruct ena_comp_ctx *comp_ctx;\n\tu16 tail_masked, cmd_id;\n\tu16 queue_size_mask;\n\tu16 cnt;\n\n\tqueue_size_mask = admin_queue->q_depth - 1;\n\n\ttail_masked = admin_queue->sq.tail & queue_size_mask;\n\n\t \n\tcnt = (u16)atomic_read(&admin_queue->outstanding_cmds);\n\tif (cnt >= admin_queue->q_depth) {\n\t\tnetdev_dbg(admin_queue->ena_dev->net_device,\n\t\t\t   \"Admin queue is full.\\n\");\n\t\tadmin_queue->stats.out_of_space++;\n\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\n\tcmd_id = admin_queue->curr_cmd_id;\n\n\tcmd->aq_common_descriptor.flags |= admin_queue->sq.phase &\n\t\tENA_ADMIN_AQ_COMMON_DESC_PHASE_MASK;\n\n\tcmd->aq_common_descriptor.command_id |= cmd_id &\n\t\tENA_ADMIN_AQ_COMMON_DESC_COMMAND_ID_MASK;\n\n\tcomp_ctx = get_comp_ctxt(admin_queue, cmd_id, true);\n\tif (unlikely(!comp_ctx))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tcomp_ctx->status = ENA_CMD_SUBMITTED;\n\tcomp_ctx->comp_size = (u32)comp_size_in_bytes;\n\tcomp_ctx->user_cqe = comp;\n\tcomp_ctx->cmd_opcode = cmd->aq_common_descriptor.opcode;\n\n\treinit_completion(&comp_ctx->wait_event);\n\n\tmemcpy(&admin_queue->sq.entries[tail_masked], cmd, cmd_size_in_bytes);\n\n\tadmin_queue->curr_cmd_id = (admin_queue->curr_cmd_id + 1) &\n\t\tqueue_size_mask;\n\n\tadmin_queue->sq.tail++;\n\tadmin_queue->stats.submitted_cmd++;\n\n\tif (unlikely((admin_queue->sq.tail & queue_size_mask) == 0))\n\t\tadmin_queue->sq.phase = !admin_queue->sq.phase;\n\n\twritel(admin_queue->sq.tail, admin_queue->sq.db_addr);\n\n\treturn comp_ctx;\n}\n\nstatic int ena_com_init_comp_ctxt(struct ena_com_admin_queue *admin_queue)\n{\n\tstruct ena_com_dev *ena_dev = admin_queue->ena_dev;\n\tsize_t size = admin_queue->q_depth * sizeof(struct ena_comp_ctx);\n\tstruct ena_comp_ctx *comp_ctx;\n\tu16 i;\n\n\tadmin_queue->comp_ctx =\n\t\tdevm_kzalloc(admin_queue->q_dmadev, size, GFP_KERNEL);\n\tif (unlikely(!admin_queue->comp_ctx)) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < admin_queue->q_depth; i++) {\n\t\tcomp_ctx = get_comp_ctxt(admin_queue, i, false);\n\t\tif (comp_ctx)\n\t\t\tinit_completion(&comp_ctx->wait_event);\n\t}\n\n\treturn 0;\n}\n\nstatic struct ena_comp_ctx *ena_com_submit_admin_cmd(struct ena_com_admin_queue *admin_queue,\n\t\t\t\t\t\t     struct ena_admin_aq_entry *cmd,\n\t\t\t\t\t\t     size_t cmd_size_in_bytes,\n\t\t\t\t\t\t     struct ena_admin_acq_entry *comp,\n\t\t\t\t\t\t     size_t comp_size_in_bytes)\n{\n\tunsigned long flags = 0;\n\tstruct ena_comp_ctx *comp_ctx;\n\n\tspin_lock_irqsave(&admin_queue->q_lock, flags);\n\tif (unlikely(!admin_queue->running_state)) {\n\t\tspin_unlock_irqrestore(&admin_queue->q_lock, flags);\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\tcomp_ctx = __ena_com_submit_admin_cmd(admin_queue, cmd,\n\t\t\t\t\t      cmd_size_in_bytes,\n\t\t\t\t\t      comp,\n\t\t\t\t\t      comp_size_in_bytes);\n\tif (IS_ERR(comp_ctx))\n\t\tadmin_queue->running_state = false;\n\tspin_unlock_irqrestore(&admin_queue->q_lock, flags);\n\n\treturn comp_ctx;\n}\n\nstatic int ena_com_init_io_sq(struct ena_com_dev *ena_dev,\n\t\t\t      struct ena_com_create_io_ctx *ctx,\n\t\t\t      struct ena_com_io_sq *io_sq)\n{\n\tsize_t size;\n\tint dev_node = 0;\n\n\tmemset(&io_sq->desc_addr, 0x0, sizeof(io_sq->desc_addr));\n\n\tio_sq->dma_addr_bits = (u8)ena_dev->dma_addr_bits;\n\tio_sq->desc_entry_size =\n\t\t(io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?\n\t\tsizeof(struct ena_eth_io_tx_desc) :\n\t\tsizeof(struct ena_eth_io_rx_desc);\n\n\tsize = io_sq->desc_entry_size * io_sq->q_depth;\n\n\tif (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST) {\n\t\tdev_node = dev_to_node(ena_dev->dmadev);\n\t\tset_dev_node(ena_dev->dmadev, ctx->numa_node);\n\t\tio_sq->desc_addr.virt_addr =\n\t\t\tdma_alloc_coherent(ena_dev->dmadev, size,\n\t\t\t\t\t   &io_sq->desc_addr.phys_addr,\n\t\t\t\t\t   GFP_KERNEL);\n\t\tset_dev_node(ena_dev->dmadev, dev_node);\n\t\tif (!io_sq->desc_addr.virt_addr) {\n\t\t\tio_sq->desc_addr.virt_addr =\n\t\t\t\tdma_alloc_coherent(ena_dev->dmadev, size,\n\t\t\t\t\t\t   &io_sq->desc_addr.phys_addr,\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\t}\n\n\t\tif (!io_sq->desc_addr.virt_addr) {\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"Memory allocation failed\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tif (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV) {\n\t\t \n\t\tio_sq->bounce_buf_ctrl.buffer_size =\n\t\t\tena_dev->llq_info.desc_list_entry_size;\n\t\tio_sq->bounce_buf_ctrl.buffers_num =\n\t\t\tENA_COM_BOUNCE_BUFFER_CNTRL_CNT;\n\t\tio_sq->bounce_buf_ctrl.next_to_use = 0;\n\n\t\tsize = io_sq->bounce_buf_ctrl.buffer_size *\n\t\t\tio_sq->bounce_buf_ctrl.buffers_num;\n\n\t\tdev_node = dev_to_node(ena_dev->dmadev);\n\t\tset_dev_node(ena_dev->dmadev, ctx->numa_node);\n\t\tio_sq->bounce_buf_ctrl.base_buffer =\n\t\t\tdevm_kzalloc(ena_dev->dmadev, size, GFP_KERNEL);\n\t\tset_dev_node(ena_dev->dmadev, dev_node);\n\t\tif (!io_sq->bounce_buf_ctrl.base_buffer)\n\t\t\tio_sq->bounce_buf_ctrl.base_buffer =\n\t\t\t\tdevm_kzalloc(ena_dev->dmadev, size, GFP_KERNEL);\n\n\t\tif (!io_sq->bounce_buf_ctrl.base_buffer) {\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"Bounce buffer memory allocation failed\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tmemcpy(&io_sq->llq_info, &ena_dev->llq_info,\n\t\t       sizeof(io_sq->llq_info));\n\n\t\t \n\t\tio_sq->llq_buf_ctrl.curr_bounce_buf =\n\t\t\tena_com_get_next_bounce_buffer(&io_sq->bounce_buf_ctrl);\n\t\tmemset(io_sq->llq_buf_ctrl.curr_bounce_buf,\n\t\t       0x0, io_sq->llq_info.desc_list_entry_size);\n\t\tio_sq->llq_buf_ctrl.descs_left_in_line =\n\t\t\tio_sq->llq_info.descs_num_before_header;\n\t\tio_sq->disable_meta_caching =\n\t\t\tio_sq->llq_info.disable_meta_caching;\n\n\t\tif (io_sq->llq_info.max_entries_in_tx_burst > 0)\n\t\t\tio_sq->entries_in_tx_burst_left =\n\t\t\t\tio_sq->llq_info.max_entries_in_tx_burst;\n\t}\n\n\tio_sq->tail = 0;\n\tio_sq->next_to_comp = 0;\n\tio_sq->phase = 1;\n\n\treturn 0;\n}\n\nstatic int ena_com_init_io_cq(struct ena_com_dev *ena_dev,\n\t\t\t      struct ena_com_create_io_ctx *ctx,\n\t\t\t      struct ena_com_io_cq *io_cq)\n{\n\tsize_t size;\n\tint prev_node = 0;\n\n\tmemset(&io_cq->cdesc_addr, 0x0, sizeof(io_cq->cdesc_addr));\n\n\t \n\tio_cq->cdesc_entry_size_in_bytes =\n\t\t(io_cq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX) ?\n\t\tsizeof(struct ena_eth_io_tx_cdesc) :\n\t\tsizeof(struct ena_eth_io_rx_cdesc_base);\n\n\tsize = io_cq->cdesc_entry_size_in_bytes * io_cq->q_depth;\n\n\tprev_node = dev_to_node(ena_dev->dmadev);\n\tset_dev_node(ena_dev->dmadev, ctx->numa_node);\n\tio_cq->cdesc_addr.virt_addr =\n\t\tdma_alloc_coherent(ena_dev->dmadev, size,\n\t\t\t\t   &io_cq->cdesc_addr.phys_addr, GFP_KERNEL);\n\tset_dev_node(ena_dev->dmadev, prev_node);\n\tif (!io_cq->cdesc_addr.virt_addr) {\n\t\tio_cq->cdesc_addr.virt_addr =\n\t\t\tdma_alloc_coherent(ena_dev->dmadev, size,\n\t\t\t\t\t   &io_cq->cdesc_addr.phys_addr,\n\t\t\t\t\t   GFP_KERNEL);\n\t}\n\n\tif (!io_cq->cdesc_addr.virt_addr) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tio_cq->phase = 1;\n\tio_cq->head = 0;\n\n\treturn 0;\n}\n\nstatic void ena_com_handle_single_admin_completion(struct ena_com_admin_queue *admin_queue,\n\t\t\t\t\t\t   struct ena_admin_acq_entry *cqe)\n{\n\tstruct ena_comp_ctx *comp_ctx;\n\tu16 cmd_id;\n\n\tcmd_id = cqe->acq_common_descriptor.command &\n\t\tENA_ADMIN_ACQ_COMMON_DESC_COMMAND_ID_MASK;\n\n\tcomp_ctx = get_comp_ctxt(admin_queue, cmd_id, false);\n\tif (unlikely(!comp_ctx)) {\n\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t   \"comp_ctx is NULL. Changing the admin queue running state\\n\");\n\t\tadmin_queue->running_state = false;\n\t\treturn;\n\t}\n\n\tcomp_ctx->status = ENA_CMD_COMPLETED;\n\tcomp_ctx->comp_status = cqe->acq_common_descriptor.status;\n\n\tif (comp_ctx->user_cqe)\n\t\tmemcpy(comp_ctx->user_cqe, (void *)cqe, comp_ctx->comp_size);\n\n\tif (!admin_queue->polling)\n\t\tcomplete(&comp_ctx->wait_event);\n}\n\nstatic void ena_com_handle_admin_completion(struct ena_com_admin_queue *admin_queue)\n{\n\tstruct ena_admin_acq_entry *cqe = NULL;\n\tu16 comp_num = 0;\n\tu16 head_masked;\n\tu8 phase;\n\n\thead_masked = admin_queue->cq.head & (admin_queue->q_depth - 1);\n\tphase = admin_queue->cq.phase;\n\n\tcqe = &admin_queue->cq.entries[head_masked];\n\n\t \n\twhile ((READ_ONCE(cqe->acq_common_descriptor.flags) &\n\t\tENA_ADMIN_ACQ_COMMON_DESC_PHASE_MASK) == phase) {\n\t\t \n\t\tdma_rmb();\n\t\tena_com_handle_single_admin_completion(admin_queue, cqe);\n\n\t\thead_masked++;\n\t\tcomp_num++;\n\t\tif (unlikely(head_masked == admin_queue->q_depth)) {\n\t\t\thead_masked = 0;\n\t\t\tphase = !phase;\n\t\t}\n\n\t\tcqe = &admin_queue->cq.entries[head_masked];\n\t}\n\n\tadmin_queue->cq.head += comp_num;\n\tadmin_queue->cq.phase = phase;\n\tadmin_queue->sq.head += comp_num;\n\tadmin_queue->stats.completed_cmd += comp_num;\n}\n\nstatic int ena_com_comp_status_to_errno(struct ena_com_admin_queue *admin_queue,\n\t\t\t\t\tu8 comp_status)\n{\n\tif (unlikely(comp_status != 0))\n\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t   \"Admin command failed[%u]\\n\", comp_status);\n\n\tswitch (comp_status) {\n\tcase ENA_ADMIN_SUCCESS:\n\t\treturn 0;\n\tcase ENA_ADMIN_RESOURCE_ALLOCATION_FAILURE:\n\t\treturn -ENOMEM;\n\tcase ENA_ADMIN_UNSUPPORTED_OPCODE:\n\t\treturn -EOPNOTSUPP;\n\tcase ENA_ADMIN_BAD_OPCODE:\n\tcase ENA_ADMIN_MALFORMED_REQUEST:\n\tcase ENA_ADMIN_ILLEGAL_PARAMETER:\n\tcase ENA_ADMIN_UNKNOWN_ERROR:\n\t\treturn -EINVAL;\n\tcase ENA_ADMIN_RESOURCE_BUSY:\n\t\treturn -EAGAIN;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void ena_delay_exponential_backoff_us(u32 exp, u32 delay_us)\n{\n\texp = min_t(u32, exp, ENA_MAX_BACKOFF_DELAY_EXP);\n\tdelay_us = max_t(u32, ENA_MIN_ADMIN_POLL_US, delay_us);\n\tdelay_us = min_t(u32, delay_us * (1U << exp), ENA_MAX_ADMIN_POLL_US);\n\tusleep_range(delay_us, 2 * delay_us);\n}\n\nstatic int ena_com_wait_and_process_admin_cq_polling(struct ena_comp_ctx *comp_ctx,\n\t\t\t\t\t\t     struct ena_com_admin_queue *admin_queue)\n{\n\tunsigned long flags = 0;\n\tunsigned long timeout;\n\tint ret;\n\tu32 exp = 0;\n\n\ttimeout = jiffies + usecs_to_jiffies(admin_queue->completion_timeout);\n\n\twhile (1) {\n\t\tspin_lock_irqsave(&admin_queue->q_lock, flags);\n\t\tena_com_handle_admin_completion(admin_queue);\n\t\tspin_unlock_irqrestore(&admin_queue->q_lock, flags);\n\n\t\tif (comp_ctx->status != ENA_CMD_SUBMITTED)\n\t\t\tbreak;\n\n\t\tif (time_is_before_jiffies(timeout)) {\n\t\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t\t   \"Wait for completion (polling) timeout\\n\");\n\t\t\t \n\t\t\tspin_lock_irqsave(&admin_queue->q_lock, flags);\n\t\t\tadmin_queue->stats.no_completion++;\n\t\t\tadmin_queue->running_state = false;\n\t\t\tspin_unlock_irqrestore(&admin_queue->q_lock, flags);\n\n\t\t\tret = -ETIME;\n\t\t\tgoto err;\n\t\t}\n\n\t\tena_delay_exponential_backoff_us(exp++,\n\t\t\t\t\t\t admin_queue->ena_dev->ena_min_poll_delay_us);\n\t}\n\n\tif (unlikely(comp_ctx->status == ENA_CMD_ABORTED)) {\n\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t   \"Command was aborted\\n\");\n\t\tspin_lock_irqsave(&admin_queue->q_lock, flags);\n\t\tadmin_queue->stats.aborted_cmd++;\n\t\tspin_unlock_irqrestore(&admin_queue->q_lock, flags);\n\t\tret = -ENODEV;\n\t\tgoto err;\n\t}\n\n\tWARN(comp_ctx->status != ENA_CMD_COMPLETED, \"Invalid comp status %d\\n\",\n\t     comp_ctx->status);\n\n\tret = ena_com_comp_status_to_errno(admin_queue, comp_ctx->comp_status);\nerr:\n\tcomp_ctxt_release(admin_queue, comp_ctx);\n\treturn ret;\n}\n\n \nstatic int ena_com_set_llq(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_admin_queue *admin_queue;\n\tstruct ena_admin_set_feat_cmd cmd;\n\tstruct ena_admin_set_feat_resp resp;\n\tstruct ena_com_llq_info *llq_info = &ena_dev->llq_info;\n\tint ret;\n\n\tmemset(&cmd, 0x0, sizeof(cmd));\n\tadmin_queue = &ena_dev->admin_queue;\n\n\tcmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\n\tcmd.feat_common.feature_id = ENA_ADMIN_LLQ;\n\n\tcmd.u.llq.header_location_ctrl_enabled = llq_info->header_location_ctrl;\n\tcmd.u.llq.entry_size_ctrl_enabled = llq_info->desc_list_entry_size_ctrl;\n\tcmd.u.llq.desc_num_before_header_enabled = llq_info->descs_num_before_header;\n\tcmd.u.llq.descriptors_stride_ctrl_enabled = llq_info->desc_stride_ctrl;\n\n\tcmd.u.llq.accel_mode.u.set.enabled_flags =\n\t\tBIT(ENA_ADMIN_DISABLE_META_CACHING) |\n\t\tBIT(ENA_ADMIN_LIMIT_TX_BURST);\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&cmd,\n\t\t\t\t\t    sizeof(cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&resp,\n\t\t\t\t\t    sizeof(resp));\n\n\tif (unlikely(ret))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to set LLQ configurations: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int ena_com_config_llq_info(struct ena_com_dev *ena_dev,\n\t\t\t\t   struct ena_admin_feature_llq_desc *llq_features,\n\t\t\t\t   struct ena_llq_configurations *llq_default_cfg)\n{\n\tstruct ena_com_llq_info *llq_info = &ena_dev->llq_info;\n\tstruct ena_admin_accel_mode_get llq_accel_mode_get;\n\tu16 supported_feat;\n\tint rc;\n\n\tmemset(llq_info, 0, sizeof(*llq_info));\n\n\tsupported_feat = llq_features->header_location_ctrl_supported;\n\n\tif (likely(supported_feat & llq_default_cfg->llq_header_location)) {\n\t\tllq_info->header_location_ctrl =\n\t\t\tllq_default_cfg->llq_header_location;\n\t} else {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Invalid header location control, supported: 0x%x\\n\",\n\t\t\t   supported_feat);\n\t\treturn -EINVAL;\n\t}\n\n\tif (likely(llq_info->header_location_ctrl == ENA_ADMIN_INLINE_HEADER)) {\n\t\tsupported_feat = llq_features->descriptors_stride_ctrl_supported;\n\t\tif (likely(supported_feat & llq_default_cfg->llq_stride_ctrl)) {\n\t\t\tllq_info->desc_stride_ctrl = llq_default_cfg->llq_stride_ctrl;\n\t\t} else\t{\n\t\t\tif (supported_feat & ENA_ADMIN_MULTIPLE_DESCS_PER_ENTRY) {\n\t\t\t\tllq_info->desc_stride_ctrl = ENA_ADMIN_MULTIPLE_DESCS_PER_ENTRY;\n\t\t\t} else if (supported_feat & ENA_ADMIN_SINGLE_DESC_PER_ENTRY) {\n\t\t\t\tllq_info->desc_stride_ctrl = ENA_ADMIN_SINGLE_DESC_PER_ENTRY;\n\t\t\t} else {\n\t\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t\t   \"Invalid desc_stride_ctrl, supported: 0x%x\\n\",\n\t\t\t\t\t   supported_feat);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"Default llq stride ctrl is not supported, performing fallback, default: 0x%x, supported: 0x%x, used: 0x%x\\n\",\n\t\t\t\t   llq_default_cfg->llq_stride_ctrl,\n\t\t\t\t   supported_feat, llq_info->desc_stride_ctrl);\n\t\t}\n\t} else {\n\t\tllq_info->desc_stride_ctrl = 0;\n\t}\n\n\tsupported_feat = llq_features->entry_size_ctrl_supported;\n\tif (likely(supported_feat & llq_default_cfg->llq_ring_entry_size)) {\n\t\tllq_info->desc_list_entry_size_ctrl = llq_default_cfg->llq_ring_entry_size;\n\t\tllq_info->desc_list_entry_size = llq_default_cfg->llq_ring_entry_size_value;\n\t} else {\n\t\tif (supported_feat & ENA_ADMIN_LIST_ENTRY_SIZE_128B) {\n\t\t\tllq_info->desc_list_entry_size_ctrl = ENA_ADMIN_LIST_ENTRY_SIZE_128B;\n\t\t\tllq_info->desc_list_entry_size = 128;\n\t\t} else if (supported_feat & ENA_ADMIN_LIST_ENTRY_SIZE_192B) {\n\t\t\tllq_info->desc_list_entry_size_ctrl = ENA_ADMIN_LIST_ENTRY_SIZE_192B;\n\t\t\tllq_info->desc_list_entry_size = 192;\n\t\t} else if (supported_feat & ENA_ADMIN_LIST_ENTRY_SIZE_256B) {\n\t\t\tllq_info->desc_list_entry_size_ctrl = ENA_ADMIN_LIST_ENTRY_SIZE_256B;\n\t\t\tllq_info->desc_list_entry_size = 256;\n\t\t} else {\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"Invalid entry_size_ctrl, supported: 0x%x\\n\",\n\t\t\t\t   supported_feat);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Default llq ring entry size is not supported, performing fallback, default: 0x%x, supported: 0x%x, used: 0x%x\\n\",\n\t\t\t   llq_default_cfg->llq_ring_entry_size, supported_feat,\n\t\t\t   llq_info->desc_list_entry_size);\n\t}\n\tif (unlikely(llq_info->desc_list_entry_size & 0x7)) {\n\t\t \n\t\tnetdev_err(ena_dev->net_device, \"Illegal entry size %d\\n\",\n\t\t\t   llq_info->desc_list_entry_size);\n\t\treturn -EINVAL;\n\t}\n\n\tif (llq_info->desc_stride_ctrl == ENA_ADMIN_MULTIPLE_DESCS_PER_ENTRY)\n\t\tllq_info->descs_per_entry = llq_info->desc_list_entry_size /\n\t\t\tsizeof(struct ena_eth_io_tx_desc);\n\telse\n\t\tllq_info->descs_per_entry = 1;\n\n\tsupported_feat = llq_features->desc_num_before_header_supported;\n\tif (likely(supported_feat & llq_default_cfg->llq_num_decs_before_header)) {\n\t\tllq_info->descs_num_before_header = llq_default_cfg->llq_num_decs_before_header;\n\t} else {\n\t\tif (supported_feat & ENA_ADMIN_LLQ_NUM_DESCS_BEFORE_HEADER_2) {\n\t\t\tllq_info->descs_num_before_header = ENA_ADMIN_LLQ_NUM_DESCS_BEFORE_HEADER_2;\n\t\t} else if (supported_feat & ENA_ADMIN_LLQ_NUM_DESCS_BEFORE_HEADER_1) {\n\t\t\tllq_info->descs_num_before_header = ENA_ADMIN_LLQ_NUM_DESCS_BEFORE_HEADER_1;\n\t\t} else if (supported_feat & ENA_ADMIN_LLQ_NUM_DESCS_BEFORE_HEADER_4) {\n\t\t\tllq_info->descs_num_before_header = ENA_ADMIN_LLQ_NUM_DESCS_BEFORE_HEADER_4;\n\t\t} else if (supported_feat & ENA_ADMIN_LLQ_NUM_DESCS_BEFORE_HEADER_8) {\n\t\t\tllq_info->descs_num_before_header = ENA_ADMIN_LLQ_NUM_DESCS_BEFORE_HEADER_8;\n\t\t} else {\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"Invalid descs_num_before_header, supported: 0x%x\\n\",\n\t\t\t\t   supported_feat);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Default llq num descs before header is not supported, performing fallback, default: 0x%x, supported: 0x%x, used: 0x%x\\n\",\n\t\t\t   llq_default_cfg->llq_num_decs_before_header,\n\t\t\t   supported_feat, llq_info->descs_num_before_header);\n\t}\n\t \n\tllq_accel_mode_get = llq_features->accel_mode.u.get;\n\n\tllq_info->disable_meta_caching =\n\t\t!!(llq_accel_mode_get.supported_flags &\n\t\t   BIT(ENA_ADMIN_DISABLE_META_CACHING));\n\n\tif (llq_accel_mode_get.supported_flags & BIT(ENA_ADMIN_LIMIT_TX_BURST))\n\t\tllq_info->max_entries_in_tx_burst =\n\t\t\tllq_accel_mode_get.max_tx_burst_size /\n\t\t\tllq_default_cfg->llq_ring_entry_size_value;\n\n\trc = ena_com_set_llq(ena_dev);\n\tif (rc)\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Cannot set LLQ configuration: %d\\n\", rc);\n\n\treturn rc;\n}\n\nstatic int ena_com_wait_and_process_admin_cq_interrupts(struct ena_comp_ctx *comp_ctx,\n\t\t\t\t\t\t\tstruct ena_com_admin_queue *admin_queue)\n{\n\tunsigned long flags = 0;\n\tint ret;\n\n\twait_for_completion_timeout(&comp_ctx->wait_event,\n\t\t\t\t    usecs_to_jiffies(\n\t\t\t\t\t    admin_queue->completion_timeout));\n\n\t \n\tif (unlikely(comp_ctx->status == ENA_CMD_SUBMITTED)) {\n\t\tspin_lock_irqsave(&admin_queue->q_lock, flags);\n\t\tena_com_handle_admin_completion(admin_queue);\n\t\tadmin_queue->stats.no_completion++;\n\t\tspin_unlock_irqrestore(&admin_queue->q_lock, flags);\n\n\t\tif (comp_ctx->status == ENA_CMD_COMPLETED) {\n\t\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t\t   \"The ena device sent a completion but the driver didn't receive a MSI-X interrupt (cmd %d), autopolling mode is %s\\n\",\n\t\t\t\t   comp_ctx->cmd_opcode,\n\t\t\t\t   admin_queue->auto_polling ? \"ON\" : \"OFF\");\n\t\t\t \n\t\t\tif (admin_queue->auto_polling)\n\t\t\t\tadmin_queue->polling = true;\n\t\t} else {\n\t\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t\t   \"The ena device didn't send a completion for the admin cmd %d status %d\\n\",\n\t\t\t\t   comp_ctx->cmd_opcode, comp_ctx->status);\n\t\t}\n\t\t \n\t\tif (!admin_queue->polling) {\n\t\t\tadmin_queue->running_state = false;\n\t\t\tret = -ETIME;\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tret = ena_com_comp_status_to_errno(admin_queue, comp_ctx->comp_status);\nerr:\n\tcomp_ctxt_release(admin_queue, comp_ctx);\n\treturn ret;\n}\n\n \nstatic u32 ena_com_reg_bar_read32(struct ena_com_dev *ena_dev, u16 offset)\n{\n\tstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\n\tvolatile struct ena_admin_ena_mmio_req_read_less_resp *read_resp =\n\t\tmmio_read->read_resp;\n\tu32 mmio_read_reg, ret, i;\n\tunsigned long flags = 0;\n\tu32 timeout = mmio_read->reg_read_to;\n\n\tmight_sleep();\n\n\tif (timeout == 0)\n\t\ttimeout = ENA_REG_READ_TIMEOUT;\n\n\t \n\tif (!mmio_read->readless_supported)\n\t\treturn readl(ena_dev->reg_bar + offset);\n\n\tspin_lock_irqsave(&mmio_read->lock, flags);\n\tmmio_read->seq_num++;\n\n\tread_resp->req_id = mmio_read->seq_num + 0xDEAD;\n\tmmio_read_reg = (offset << ENA_REGS_MMIO_REG_READ_REG_OFF_SHIFT) &\n\t\t\tENA_REGS_MMIO_REG_READ_REG_OFF_MASK;\n\tmmio_read_reg |= mmio_read->seq_num &\n\t\t\tENA_REGS_MMIO_REG_READ_REQ_ID_MASK;\n\n\twritel(mmio_read_reg, ena_dev->reg_bar + ENA_REGS_MMIO_REG_READ_OFF);\n\n\tfor (i = 0; i < timeout; i++) {\n\t\tif (READ_ONCE(read_resp->req_id) == mmio_read->seq_num)\n\t\t\tbreak;\n\n\t\tudelay(1);\n\t}\n\n\tif (unlikely(i == timeout)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Reading reg failed for timeout. expected: req id[%u] offset[%u] actual: req id[%u] offset[%u]\\n\",\n\t\t\t   mmio_read->seq_num, offset, read_resp->req_id,\n\t\t\t   read_resp->reg_off);\n\t\tret = ENA_MMIO_READ_TIMEOUT;\n\t\tgoto err;\n\t}\n\n\tif (read_resp->reg_off != offset) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Read failure: wrong offset provided\\n\");\n\t\tret = ENA_MMIO_READ_TIMEOUT;\n\t} else {\n\t\tret = read_resp->reg_val;\n\t}\nerr:\n\tspin_unlock_irqrestore(&mmio_read->lock, flags);\n\n\treturn ret;\n}\n\n \nstatic int ena_com_wait_and_process_admin_cq(struct ena_comp_ctx *comp_ctx,\n\t\t\t\t\t     struct ena_com_admin_queue *admin_queue)\n{\n\tif (admin_queue->polling)\n\t\treturn ena_com_wait_and_process_admin_cq_polling(comp_ctx,\n\t\t\t\t\t\t\t\t admin_queue);\n\n\treturn ena_com_wait_and_process_admin_cq_interrupts(comp_ctx,\n\t\t\t\t\t\t\t    admin_queue);\n}\n\nstatic int ena_com_destroy_io_sq(struct ena_com_dev *ena_dev,\n\t\t\t\t struct ena_com_io_sq *io_sq)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tstruct ena_admin_aq_destroy_sq_cmd destroy_cmd;\n\tstruct ena_admin_acq_destroy_sq_resp_desc destroy_resp;\n\tu8 direction;\n\tint ret;\n\n\tmemset(&destroy_cmd, 0x0, sizeof(destroy_cmd));\n\n\tif (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)\n\t\tdirection = ENA_ADMIN_SQ_DIRECTION_TX;\n\telse\n\t\tdirection = ENA_ADMIN_SQ_DIRECTION_RX;\n\n\tdestroy_cmd.sq.sq_identity |= (direction <<\n\t\tENA_ADMIN_SQ_SQ_DIRECTION_SHIFT) &\n\t\tENA_ADMIN_SQ_SQ_DIRECTION_MASK;\n\n\tdestroy_cmd.sq.sq_idx = io_sq->idx;\n\tdestroy_cmd.aq_common_descriptor.opcode = ENA_ADMIN_DESTROY_SQ;\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&destroy_cmd,\n\t\t\t\t\t    sizeof(destroy_cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&destroy_resp,\n\t\t\t\t\t    sizeof(destroy_resp));\n\n\tif (unlikely(ret && (ret != -ENODEV)))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to destroy io sq error: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic void ena_com_io_queue_free(struct ena_com_dev *ena_dev,\n\t\t\t\t  struct ena_com_io_sq *io_sq,\n\t\t\t\t  struct ena_com_io_cq *io_cq)\n{\n\tsize_t size;\n\n\tif (io_cq->cdesc_addr.virt_addr) {\n\t\tsize = io_cq->cdesc_entry_size_in_bytes * io_cq->q_depth;\n\n\t\tdma_free_coherent(ena_dev->dmadev, size,\n\t\t\t\t  io_cq->cdesc_addr.virt_addr,\n\t\t\t\t  io_cq->cdesc_addr.phys_addr);\n\n\t\tio_cq->cdesc_addr.virt_addr = NULL;\n\t}\n\n\tif (io_sq->desc_addr.virt_addr) {\n\t\tsize = io_sq->desc_entry_size * io_sq->q_depth;\n\n\t\tdma_free_coherent(ena_dev->dmadev, size,\n\t\t\t\t  io_sq->desc_addr.virt_addr,\n\t\t\t\t  io_sq->desc_addr.phys_addr);\n\n\t\tio_sq->desc_addr.virt_addr = NULL;\n\t}\n\n\tif (io_sq->bounce_buf_ctrl.base_buffer) {\n\t\tdevm_kfree(ena_dev->dmadev, io_sq->bounce_buf_ctrl.base_buffer);\n\t\tio_sq->bounce_buf_ctrl.base_buffer = NULL;\n\t}\n}\n\nstatic int wait_for_reset_state(struct ena_com_dev *ena_dev, u32 timeout,\n\t\t\t\tu16 exp_state)\n{\n\tu32 val, exp = 0;\n\tunsigned long timeout_stamp;\n\n\t \n\ttimeout_stamp = jiffies + usecs_to_jiffies(100 * 1000 * timeout);\n\n\twhile (1) {\n\t\tval = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);\n\n\t\tif (unlikely(val == ENA_MMIO_READ_TIMEOUT)) {\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"Reg read timeout occurred\\n\");\n\t\t\treturn -ETIME;\n\t\t}\n\n\t\tif ((val & ENA_REGS_DEV_STS_RESET_IN_PROGRESS_MASK) ==\n\t\t\texp_state)\n\t\t\treturn 0;\n\n\t\tif (time_is_before_jiffies(timeout_stamp))\n\t\t\treturn -ETIME;\n\n\t\tena_delay_exponential_backoff_us(exp++, ena_dev->ena_min_poll_delay_us);\n\t}\n}\n\nstatic bool ena_com_check_supported_feature_id(struct ena_com_dev *ena_dev,\n\t\t\t\t\t       enum ena_admin_aq_feature_id feature_id)\n{\n\tu32 feature_mask = 1 << feature_id;\n\n\t \n\tif ((feature_id != ENA_ADMIN_DEVICE_ATTRIBUTES) &&\n\t    !(ena_dev->supported_features & feature_mask))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int ena_com_get_feature_ex(struct ena_com_dev *ena_dev,\n\t\t\t\t  struct ena_admin_get_feat_resp *get_resp,\n\t\t\t\t  enum ena_admin_aq_feature_id feature_id,\n\t\t\t\t  dma_addr_t control_buf_dma_addr,\n\t\t\t\t  u32 control_buff_size,\n\t\t\t\t  u8 feature_ver)\n{\n\tstruct ena_com_admin_queue *admin_queue;\n\tstruct ena_admin_get_feat_cmd get_cmd;\n\tint ret;\n\n\tif (!ena_com_check_supported_feature_id(ena_dev, feature_id)) {\n\t\tnetdev_dbg(ena_dev->net_device, \"Feature %d isn't supported\\n\",\n\t\t\t   feature_id);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmemset(&get_cmd, 0x0, sizeof(get_cmd));\n\tadmin_queue = &ena_dev->admin_queue;\n\n\tget_cmd.aq_common_descriptor.opcode = ENA_ADMIN_GET_FEATURE;\n\n\tif (control_buff_size)\n\t\tget_cmd.aq_common_descriptor.flags =\n\t\t\tENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;\n\telse\n\t\tget_cmd.aq_common_descriptor.flags = 0;\n\n\tret = ena_com_mem_addr_set(ena_dev,\n\t\t\t\t   &get_cmd.control_buffer.address,\n\t\t\t\t   control_buf_dma_addr);\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory address set failed\\n\");\n\t\treturn ret;\n\t}\n\n\tget_cmd.control_buffer.length = control_buff_size;\n\tget_cmd.feat_common.feature_version = feature_ver;\n\tget_cmd.feat_common.feature_id = feature_id;\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)\n\t\t\t\t\t    &get_cmd,\n\t\t\t\t\t    sizeof(get_cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)\n\t\t\t\t\t    get_resp,\n\t\t\t\t\t    sizeof(*get_resp));\n\n\tif (unlikely(ret))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to submit get_feature command %d error: %d\\n\",\n\t\t\t   feature_id, ret);\n\n\treturn ret;\n}\n\nstatic int ena_com_get_feature(struct ena_com_dev *ena_dev,\n\t\t\t       struct ena_admin_get_feat_resp *get_resp,\n\t\t\t       enum ena_admin_aq_feature_id feature_id,\n\t\t\t       u8 feature_ver)\n{\n\treturn ena_com_get_feature_ex(ena_dev,\n\t\t\t\t      get_resp,\n\t\t\t\t      feature_id,\n\t\t\t\t      0,\n\t\t\t\t      0,\n\t\t\t\t      feature_ver);\n}\n\nint ena_com_get_current_hash_function(struct ena_com_dev *ena_dev)\n{\n\treturn ena_dev->rss.hash_func;\n}\n\nstatic void ena_com_hash_key_fill_default_key(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_admin_feature_rss_flow_hash_control *hash_key =\n\t\t(ena_dev->rss).hash_key;\n\n\tnetdev_rss_key_fill(&hash_key->key, sizeof(hash_key->key));\n\t \n\thash_key->key_parts = ENA_ADMIN_RSS_KEY_PARTS;\n}\n\nstatic int ena_com_hash_key_allocate(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\n\tif (!ena_com_check_supported_feature_id(ena_dev,\n\t\t\t\t\t\tENA_ADMIN_RSS_HASH_FUNCTION))\n\t\treturn -EOPNOTSUPP;\n\n\trss->hash_key =\n\t\tdma_alloc_coherent(ena_dev->dmadev, sizeof(*rss->hash_key),\n\t\t\t\t   &rss->hash_key_dma_addr, GFP_KERNEL);\n\n\tif (unlikely(!rss->hash_key))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void ena_com_hash_key_destroy(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\n\tif (rss->hash_key)\n\t\tdma_free_coherent(ena_dev->dmadev, sizeof(*rss->hash_key),\n\t\t\t\t  rss->hash_key, rss->hash_key_dma_addr);\n\trss->hash_key = NULL;\n}\n\nstatic int ena_com_hash_ctrl_init(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\n\trss->hash_ctrl =\n\t\tdma_alloc_coherent(ena_dev->dmadev, sizeof(*rss->hash_ctrl),\n\t\t\t\t   &rss->hash_ctrl_dma_addr, GFP_KERNEL);\n\n\tif (unlikely(!rss->hash_ctrl))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void ena_com_hash_ctrl_destroy(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\n\tif (rss->hash_ctrl)\n\t\tdma_free_coherent(ena_dev->dmadev, sizeof(*rss->hash_ctrl),\n\t\t\t\t  rss->hash_ctrl, rss->hash_ctrl_dma_addr);\n\trss->hash_ctrl = NULL;\n}\n\nstatic int ena_com_indirect_table_allocate(struct ena_com_dev *ena_dev,\n\t\t\t\t\t   u16 log_size)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_admin_get_feat_resp get_resp;\n\tsize_t tbl_size;\n\tint ret;\n\n\tret = ena_com_get_feature(ena_dev, &get_resp,\n\t\t\t\t  ENA_ADMIN_RSS_INDIRECTION_TABLE_CONFIG, 0);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif ((get_resp.u.ind_table.min_size > log_size) ||\n\t    (get_resp.u.ind_table.max_size < log_size)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Indirect table size doesn't fit. requested size: %d while min is:%d and max %d\\n\",\n\t\t\t   1 << log_size, 1 << get_resp.u.ind_table.min_size,\n\t\t\t   1 << get_resp.u.ind_table.max_size);\n\t\treturn -EINVAL;\n\t}\n\n\ttbl_size = (1ULL << log_size) *\n\t\tsizeof(struct ena_admin_rss_ind_table_entry);\n\n\trss->rss_ind_tbl =\n\t\tdma_alloc_coherent(ena_dev->dmadev, tbl_size,\n\t\t\t\t   &rss->rss_ind_tbl_dma_addr, GFP_KERNEL);\n\tif (unlikely(!rss->rss_ind_tbl))\n\t\tgoto mem_err1;\n\n\ttbl_size = (1ULL << log_size) * sizeof(u16);\n\trss->host_rss_ind_tbl =\n\t\tdevm_kzalloc(ena_dev->dmadev, tbl_size, GFP_KERNEL);\n\tif (unlikely(!rss->host_rss_ind_tbl))\n\t\tgoto mem_err2;\n\n\trss->tbl_log_size = log_size;\n\n\treturn 0;\n\nmem_err2:\n\ttbl_size = (1ULL << log_size) *\n\t\tsizeof(struct ena_admin_rss_ind_table_entry);\n\n\tdma_free_coherent(ena_dev->dmadev, tbl_size, rss->rss_ind_tbl,\n\t\t\t  rss->rss_ind_tbl_dma_addr);\n\trss->rss_ind_tbl = NULL;\nmem_err1:\n\trss->tbl_log_size = 0;\n\treturn -ENOMEM;\n}\n\nstatic void ena_com_indirect_table_destroy(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tsize_t tbl_size = (1ULL << rss->tbl_log_size) *\n\t\tsizeof(struct ena_admin_rss_ind_table_entry);\n\n\tif (rss->rss_ind_tbl)\n\t\tdma_free_coherent(ena_dev->dmadev, tbl_size, rss->rss_ind_tbl,\n\t\t\t\t  rss->rss_ind_tbl_dma_addr);\n\trss->rss_ind_tbl = NULL;\n\n\tif (rss->host_rss_ind_tbl)\n\t\tdevm_kfree(ena_dev->dmadev, rss->host_rss_ind_tbl);\n\trss->host_rss_ind_tbl = NULL;\n}\n\nstatic int ena_com_create_io_sq(struct ena_com_dev *ena_dev,\n\t\t\t\tstruct ena_com_io_sq *io_sq, u16 cq_idx)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tstruct ena_admin_aq_create_sq_cmd create_cmd;\n\tstruct ena_admin_acq_create_sq_resp_desc cmd_completion;\n\tu8 direction;\n\tint ret;\n\n\tmemset(&create_cmd, 0x0, sizeof(create_cmd));\n\n\tcreate_cmd.aq_common_descriptor.opcode = ENA_ADMIN_CREATE_SQ;\n\n\tif (io_sq->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)\n\t\tdirection = ENA_ADMIN_SQ_DIRECTION_TX;\n\telse\n\t\tdirection = ENA_ADMIN_SQ_DIRECTION_RX;\n\n\tcreate_cmd.sq_identity |= (direction <<\n\t\tENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_SHIFT) &\n\t\tENA_ADMIN_AQ_CREATE_SQ_CMD_SQ_DIRECTION_MASK;\n\n\tcreate_cmd.sq_caps_2 |= io_sq->mem_queue_type &\n\t\tENA_ADMIN_AQ_CREATE_SQ_CMD_PLACEMENT_POLICY_MASK;\n\n\tcreate_cmd.sq_caps_2 |= (ENA_ADMIN_COMPLETION_POLICY_DESC <<\n\t\tENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_SHIFT) &\n\t\tENA_ADMIN_AQ_CREATE_SQ_CMD_COMPLETION_POLICY_MASK;\n\n\tcreate_cmd.sq_caps_3 |=\n\t\tENA_ADMIN_AQ_CREATE_SQ_CMD_IS_PHYSICALLY_CONTIGUOUS_MASK;\n\n\tcreate_cmd.cq_idx = cq_idx;\n\tcreate_cmd.sq_depth = io_sq->q_depth;\n\n\tif (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_HOST) {\n\t\tret = ena_com_mem_addr_set(ena_dev,\n\t\t\t\t\t   &create_cmd.sq_ba,\n\t\t\t\t\t   io_sq->desc_addr.phys_addr);\n\t\tif (unlikely(ret)) {\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"Memory address set failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&create_cmd,\n\t\t\t\t\t    sizeof(create_cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&cmd_completion,\n\t\t\t\t\t    sizeof(cmd_completion));\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to create IO SQ. error: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tio_sq->idx = cmd_completion.sq_idx;\n\n\tio_sq->db_addr = (u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\n\t\t(uintptr_t)cmd_completion.sq_doorbell_offset);\n\n\tif (io_sq->mem_queue_type == ENA_ADMIN_PLACEMENT_POLICY_DEV) {\n\t\tio_sq->header_addr = (u8 __iomem *)((uintptr_t)ena_dev->mem_bar\n\t\t\t\t+ cmd_completion.llq_headers_offset);\n\n\t\tio_sq->desc_addr.pbuf_dev_addr =\n\t\t\t(u8 __iomem *)((uintptr_t)ena_dev->mem_bar +\n\t\t\tcmd_completion.llq_descriptors_offset);\n\t}\n\n\tnetdev_dbg(ena_dev->net_device, \"Created sq[%u], depth[%u]\\n\",\n\t\t   io_sq->idx, io_sq->q_depth);\n\n\treturn ret;\n}\n\nstatic int ena_com_ind_tbl_convert_to_device(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_com_io_sq *io_sq;\n\tu16 qid;\n\tint i;\n\n\tfor (i = 0; i < 1 << rss->tbl_log_size; i++) {\n\t\tqid = rss->host_rss_ind_tbl[i];\n\t\tif (qid >= ENA_TOTAL_NUM_QUEUES)\n\t\t\treturn -EINVAL;\n\n\t\tio_sq = &ena_dev->io_sq_queues[qid];\n\n\t\tif (io_sq->direction != ENA_COM_IO_QUEUE_DIRECTION_RX)\n\t\t\treturn -EINVAL;\n\n\t\trss->rss_ind_tbl[i].cq_idx = io_sq->idx;\n\t}\n\n\treturn 0;\n}\n\nstatic void ena_com_update_intr_delay_resolution(struct ena_com_dev *ena_dev,\n\t\t\t\t\t\t u16 intr_delay_resolution)\n{\n\tu16 prev_intr_delay_resolution = ena_dev->intr_delay_resolution;\n\n\tif (unlikely(!intr_delay_resolution)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Illegal intr_delay_resolution provided. Going to use default 1 usec resolution\\n\");\n\t\tintr_delay_resolution = ENA_DEFAULT_INTR_DELAY_RESOLUTION;\n\t}\n\n\t \n\tena_dev->intr_moder_rx_interval =\n\t\tena_dev->intr_moder_rx_interval *\n\t\tprev_intr_delay_resolution /\n\t\tintr_delay_resolution;\n\n\t \n\tena_dev->intr_moder_tx_interval =\n\t\tena_dev->intr_moder_tx_interval *\n\t\tprev_intr_delay_resolution /\n\t\tintr_delay_resolution;\n\n\tena_dev->intr_delay_resolution = intr_delay_resolution;\n}\n\n \n \n \n\nint ena_com_execute_admin_command(struct ena_com_admin_queue *admin_queue,\n\t\t\t\t  struct ena_admin_aq_entry *cmd,\n\t\t\t\t  size_t cmd_size,\n\t\t\t\t  struct ena_admin_acq_entry *comp,\n\t\t\t\t  size_t comp_size)\n{\n\tstruct ena_comp_ctx *comp_ctx;\n\tint ret;\n\n\tcomp_ctx = ena_com_submit_admin_cmd(admin_queue, cmd, cmd_size,\n\t\t\t\t\t    comp, comp_size);\n\tif (IS_ERR(comp_ctx)) {\n\t\tret = PTR_ERR(comp_ctx);\n\t\tif (ret == -ENODEV)\n\t\t\tnetdev_dbg(admin_queue->ena_dev->net_device,\n\t\t\t\t   \"Failed to submit command [%d]\\n\", ret);\n\t\telse\n\t\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t\t   \"Failed to submit command [%d]\\n\", ret);\n\n\t\treturn ret;\n\t}\n\n\tret = ena_com_wait_and_process_admin_cq(comp_ctx, admin_queue);\n\tif (unlikely(ret)) {\n\t\tif (admin_queue->running_state)\n\t\t\tnetdev_err(admin_queue->ena_dev->net_device,\n\t\t\t\t   \"Failed to process command. ret = %d\\n\", ret);\n\t\telse\n\t\t\tnetdev_dbg(admin_queue->ena_dev->net_device,\n\t\t\t\t   \"Failed to process command. ret = %d\\n\", ret);\n\t}\n\treturn ret;\n}\n\nint ena_com_create_io_cq(struct ena_com_dev *ena_dev,\n\t\t\t struct ena_com_io_cq *io_cq)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tstruct ena_admin_aq_create_cq_cmd create_cmd;\n\tstruct ena_admin_acq_create_cq_resp_desc cmd_completion;\n\tint ret;\n\n\tmemset(&create_cmd, 0x0, sizeof(create_cmd));\n\n\tcreate_cmd.aq_common_descriptor.opcode = ENA_ADMIN_CREATE_CQ;\n\n\tcreate_cmd.cq_caps_2 |= (io_cq->cdesc_entry_size_in_bytes / 4) &\n\t\tENA_ADMIN_AQ_CREATE_CQ_CMD_CQ_ENTRY_SIZE_WORDS_MASK;\n\tcreate_cmd.cq_caps_1 |=\n\t\tENA_ADMIN_AQ_CREATE_CQ_CMD_INTERRUPT_MODE_ENABLED_MASK;\n\n\tcreate_cmd.msix_vector = io_cq->msix_vector;\n\tcreate_cmd.cq_depth = io_cq->q_depth;\n\n\tret = ena_com_mem_addr_set(ena_dev,\n\t\t\t\t   &create_cmd.cq_ba,\n\t\t\t\t   io_cq->cdesc_addr.phys_addr);\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory address set failed\\n\");\n\t\treturn ret;\n\t}\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&create_cmd,\n\t\t\t\t\t    sizeof(create_cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&cmd_completion,\n\t\t\t\t\t    sizeof(cmd_completion));\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to create IO CQ. error: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tio_cq->idx = cmd_completion.cq_idx;\n\n\tio_cq->unmask_reg = (u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\n\t\tcmd_completion.cq_interrupt_unmask_register_offset);\n\n\tif (cmd_completion.cq_head_db_register_offset)\n\t\tio_cq->cq_head_db_reg =\n\t\t\t(u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\n\t\t\tcmd_completion.cq_head_db_register_offset);\n\n\tif (cmd_completion.numa_node_register_offset)\n\t\tio_cq->numa_node_cfg_reg =\n\t\t\t(u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\n\t\t\tcmd_completion.numa_node_register_offset);\n\n\tnetdev_dbg(ena_dev->net_device, \"Created cq[%u], depth[%u]\\n\",\n\t\t   io_cq->idx, io_cq->q_depth);\n\n\treturn ret;\n}\n\nint ena_com_get_io_handlers(struct ena_com_dev *ena_dev, u16 qid,\n\t\t\t    struct ena_com_io_sq **io_sq,\n\t\t\t    struct ena_com_io_cq **io_cq)\n{\n\tif (qid >= ENA_TOTAL_NUM_QUEUES) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Invalid queue number %d but the max is %d\\n\", qid,\n\t\t\t   ENA_TOTAL_NUM_QUEUES);\n\t\treturn -EINVAL;\n\t}\n\n\t*io_sq = &ena_dev->io_sq_queues[qid];\n\t*io_cq = &ena_dev->io_cq_queues[qid];\n\n\treturn 0;\n}\n\nvoid ena_com_abort_admin_commands(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tstruct ena_comp_ctx *comp_ctx;\n\tu16 i;\n\n\tif (!admin_queue->comp_ctx)\n\t\treturn;\n\n\tfor (i = 0; i < admin_queue->q_depth; i++) {\n\t\tcomp_ctx = get_comp_ctxt(admin_queue, i, false);\n\t\tif (unlikely(!comp_ctx))\n\t\t\tbreak;\n\n\t\tcomp_ctx->status = ENA_CMD_ABORTED;\n\n\t\tcomplete(&comp_ctx->wait_event);\n\t}\n}\n\nvoid ena_com_wait_for_abort_completion(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tunsigned long flags = 0;\n\tu32 exp = 0;\n\n\tspin_lock_irqsave(&admin_queue->q_lock, flags);\n\twhile (atomic_read(&admin_queue->outstanding_cmds) != 0) {\n\t\tspin_unlock_irqrestore(&admin_queue->q_lock, flags);\n\t\tena_delay_exponential_backoff_us(exp++,\n\t\t\t\t\t\t ena_dev->ena_min_poll_delay_us);\n\t\tspin_lock_irqsave(&admin_queue->q_lock, flags);\n\t}\n\tspin_unlock_irqrestore(&admin_queue->q_lock, flags);\n}\n\nint ena_com_destroy_io_cq(struct ena_com_dev *ena_dev,\n\t\t\t  struct ena_com_io_cq *io_cq)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tstruct ena_admin_aq_destroy_cq_cmd destroy_cmd;\n\tstruct ena_admin_acq_destroy_cq_resp_desc destroy_resp;\n\tint ret;\n\n\tmemset(&destroy_cmd, 0x0, sizeof(destroy_cmd));\n\n\tdestroy_cmd.cq_idx = io_cq->idx;\n\tdestroy_cmd.aq_common_descriptor.opcode = ENA_ADMIN_DESTROY_CQ;\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&destroy_cmd,\n\t\t\t\t\t    sizeof(destroy_cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&destroy_resp,\n\t\t\t\t\t    sizeof(destroy_resp));\n\n\tif (unlikely(ret && (ret != -ENODEV)))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to destroy IO CQ. error: %d\\n\", ret);\n\n\treturn ret;\n}\n\nbool ena_com_get_admin_running_state(struct ena_com_dev *ena_dev)\n{\n\treturn ena_dev->admin_queue.running_state;\n}\n\nvoid ena_com_set_admin_running_state(struct ena_com_dev *ena_dev, bool state)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tunsigned long flags = 0;\n\n\tspin_lock_irqsave(&admin_queue->q_lock, flags);\n\tena_dev->admin_queue.running_state = state;\n\tspin_unlock_irqrestore(&admin_queue->q_lock, flags);\n}\n\nvoid ena_com_admin_aenq_enable(struct ena_com_dev *ena_dev)\n{\n\tu16 depth = ena_dev->aenq.q_depth;\n\n\tWARN(ena_dev->aenq.head != depth, \"Invalid AENQ state\\n\");\n\n\t \n\twritel(depth, ena_dev->reg_bar + ENA_REGS_AENQ_HEAD_DB_OFF);\n}\n\nint ena_com_set_aenq_config(struct ena_com_dev *ena_dev, u32 groups_flag)\n{\n\tstruct ena_com_admin_queue *admin_queue;\n\tstruct ena_admin_set_feat_cmd cmd;\n\tstruct ena_admin_set_feat_resp resp;\n\tstruct ena_admin_get_feat_resp get_resp;\n\tint ret;\n\n\tret = ena_com_get_feature(ena_dev, &get_resp, ENA_ADMIN_AENQ_CONFIG, 0);\n\tif (ret) {\n\t\tdev_info(ena_dev->dmadev, \"Can't get aenq configuration\\n\");\n\t\treturn ret;\n\t}\n\n\tif ((get_resp.u.aenq.supported_groups & groups_flag) != groups_flag) {\n\t\tnetdev_warn(ena_dev->net_device,\n\t\t\t    \"Trying to set unsupported aenq events. supported flag: 0x%x asked flag: 0x%x\\n\",\n\t\t\t    get_resp.u.aenq.supported_groups, groups_flag);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmemset(&cmd, 0x0, sizeof(cmd));\n\tadmin_queue = &ena_dev->admin_queue;\n\n\tcmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\n\tcmd.aq_common_descriptor.flags = 0;\n\tcmd.feat_common.feature_id = ENA_ADMIN_AENQ_CONFIG;\n\tcmd.u.aenq.enabled_groups = groups_flag;\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&cmd,\n\t\t\t\t\t    sizeof(cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&resp,\n\t\t\t\t\t    sizeof(resp));\n\n\tif (unlikely(ret))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to config AENQ ret: %d\\n\", ret);\n\n\treturn ret;\n}\n\nint ena_com_get_dma_width(struct ena_com_dev *ena_dev)\n{\n\tu32 caps = ena_com_reg_bar_read32(ena_dev, ENA_REGS_CAPS_OFF);\n\tu32 width;\n\n\tif (unlikely(caps == ENA_MMIO_READ_TIMEOUT)) {\n\t\tnetdev_err(ena_dev->net_device, \"Reg read timeout occurred\\n\");\n\t\treturn -ETIME;\n\t}\n\n\twidth = (caps & ENA_REGS_CAPS_DMA_ADDR_WIDTH_MASK) >>\n\t\tENA_REGS_CAPS_DMA_ADDR_WIDTH_SHIFT;\n\n\tnetdev_dbg(ena_dev->net_device, \"ENA dma width: %d\\n\", width);\n\n\tif ((width < 32) || width > ENA_MAX_PHYS_ADDR_SIZE_BITS) {\n\t\tnetdev_err(ena_dev->net_device, \"DMA width illegal value: %d\\n\",\n\t\t\t   width);\n\t\treturn -EINVAL;\n\t}\n\n\tena_dev->dma_addr_bits = width;\n\n\treturn width;\n}\n\nint ena_com_validate_version(struct ena_com_dev *ena_dev)\n{\n\tu32 ver;\n\tu32 ctrl_ver;\n\tu32 ctrl_ver_masked;\n\n\t \n\tver = ena_com_reg_bar_read32(ena_dev, ENA_REGS_VERSION_OFF);\n\tctrl_ver = ena_com_reg_bar_read32(ena_dev,\n\t\t\t\t\t  ENA_REGS_CONTROLLER_VERSION_OFF);\n\n\tif (unlikely((ver == ENA_MMIO_READ_TIMEOUT) ||\n\t\t     (ctrl_ver == ENA_MMIO_READ_TIMEOUT))) {\n\t\tnetdev_err(ena_dev->net_device, \"Reg read timeout occurred\\n\");\n\t\treturn -ETIME;\n\t}\n\n\tdev_info(ena_dev->dmadev, \"ENA device version: %d.%d\\n\",\n\t\t (ver & ENA_REGS_VERSION_MAJOR_VERSION_MASK) >>\n\t\t\t ENA_REGS_VERSION_MAJOR_VERSION_SHIFT,\n\t\t ver & ENA_REGS_VERSION_MINOR_VERSION_MASK);\n\n\tdev_info(ena_dev->dmadev,\n\t\t \"ENA controller version: %d.%d.%d implementation version %d\\n\",\n\t\t (ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_MASK) >>\n\t\t\t ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_SHIFT,\n\t\t (ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_MASK) >>\n\t\t\t ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_SHIFT,\n\t\t (ctrl_ver & ENA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION_MASK),\n\t\t (ctrl_ver & ENA_REGS_CONTROLLER_VERSION_IMPL_ID_MASK) >>\n\t\t\t ENA_REGS_CONTROLLER_VERSION_IMPL_ID_SHIFT);\n\n\tctrl_ver_masked =\n\t\t(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MAJOR_VERSION_MASK) |\n\t\t(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_MINOR_VERSION_MASK) |\n\t\t(ctrl_ver & ENA_REGS_CONTROLLER_VERSION_SUBMINOR_VERSION_MASK);\n\n\t \n\tif (ctrl_ver_masked < MIN_ENA_CTRL_VER) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"ENA ctrl version is lower than the minimal ctrl version the driver supports\\n\");\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic void\nena_com_free_ena_admin_queue_comp_ctx(struct ena_com_dev *ena_dev,\n\t\t\t\t      struct ena_com_admin_queue *admin_queue)\n\n{\n\tif (!admin_queue->comp_ctx)\n\t\treturn;\n\n\tdevm_kfree(ena_dev->dmadev, admin_queue->comp_ctx);\n\n\tadmin_queue->comp_ctx = NULL;\n}\n\nvoid ena_com_admin_destroy(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tstruct ena_com_admin_cq *cq = &admin_queue->cq;\n\tstruct ena_com_admin_sq *sq = &admin_queue->sq;\n\tstruct ena_com_aenq *aenq = &ena_dev->aenq;\n\tu16 size;\n\n\tena_com_free_ena_admin_queue_comp_ctx(ena_dev, admin_queue);\n\n\tsize = ADMIN_SQ_SIZE(admin_queue->q_depth);\n\tif (sq->entries)\n\t\tdma_free_coherent(ena_dev->dmadev, size, sq->entries,\n\t\t\t\t  sq->dma_addr);\n\tsq->entries = NULL;\n\n\tsize = ADMIN_CQ_SIZE(admin_queue->q_depth);\n\tif (cq->entries)\n\t\tdma_free_coherent(ena_dev->dmadev, size, cq->entries,\n\t\t\t\t  cq->dma_addr);\n\tcq->entries = NULL;\n\n\tsize = ADMIN_AENQ_SIZE(aenq->q_depth);\n\tif (ena_dev->aenq.entries)\n\t\tdma_free_coherent(ena_dev->dmadev, size, aenq->entries,\n\t\t\t\t  aenq->dma_addr);\n\taenq->entries = NULL;\n}\n\nvoid ena_com_set_admin_polling_mode(struct ena_com_dev *ena_dev, bool polling)\n{\n\tu32 mask_value = 0;\n\n\tif (polling)\n\t\tmask_value = ENA_REGS_ADMIN_INTR_MASK;\n\n\twritel(mask_value, ena_dev->reg_bar + ENA_REGS_INTR_MASK_OFF);\n\tena_dev->admin_queue.polling = polling;\n}\n\nvoid ena_com_set_admin_auto_polling_mode(struct ena_com_dev *ena_dev,\n\t\t\t\t\t bool polling)\n{\n\tena_dev->admin_queue.auto_polling = polling;\n}\n\nint ena_com_mmio_reg_read_request_init(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\n\n\tspin_lock_init(&mmio_read->lock);\n\tmmio_read->read_resp =\n\t\tdma_alloc_coherent(ena_dev->dmadev,\n\t\t\t\t   sizeof(*mmio_read->read_resp),\n\t\t\t\t   &mmio_read->read_resp_dma_addr, GFP_KERNEL);\n\tif (unlikely(!mmio_read->read_resp))\n\t\tgoto err;\n\n\tena_com_mmio_reg_read_request_write_dev_addr(ena_dev);\n\n\tmmio_read->read_resp->req_id = 0x0;\n\tmmio_read->seq_num = 0x0;\n\tmmio_read->readless_supported = true;\n\n\treturn 0;\n\nerr:\n\n\treturn -ENOMEM;\n}\n\nvoid ena_com_set_mmio_read_mode(struct ena_com_dev *ena_dev, bool readless_supported)\n{\n\tstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\n\n\tmmio_read->readless_supported = readless_supported;\n}\n\nvoid ena_com_mmio_reg_read_request_destroy(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\n\n\twritel(0x0, ena_dev->reg_bar + ENA_REGS_MMIO_RESP_LO_OFF);\n\twritel(0x0, ena_dev->reg_bar + ENA_REGS_MMIO_RESP_HI_OFF);\n\n\tdma_free_coherent(ena_dev->dmadev, sizeof(*mmio_read->read_resp),\n\t\t\t  mmio_read->read_resp, mmio_read->read_resp_dma_addr);\n\n\tmmio_read->read_resp = NULL;\n}\n\nvoid ena_com_mmio_reg_read_request_write_dev_addr(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_mmio_read *mmio_read = &ena_dev->mmio_read;\n\tu32 addr_low, addr_high;\n\n\taddr_low = ENA_DMA_ADDR_TO_UINT32_LOW(mmio_read->read_resp_dma_addr);\n\taddr_high = ENA_DMA_ADDR_TO_UINT32_HIGH(mmio_read->read_resp_dma_addr);\n\n\twritel(addr_low, ena_dev->reg_bar + ENA_REGS_MMIO_RESP_LO_OFF);\n\twritel(addr_high, ena_dev->reg_bar + ENA_REGS_MMIO_RESP_HI_OFF);\n}\n\nint ena_com_admin_init(struct ena_com_dev *ena_dev,\n\t\t       struct ena_aenq_handlers *aenq_handlers)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tu32 aq_caps, acq_caps, dev_sts, addr_low, addr_high;\n\tint ret;\n\n\tdev_sts = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);\n\n\tif (unlikely(dev_sts == ENA_MMIO_READ_TIMEOUT)) {\n\t\tnetdev_err(ena_dev->net_device, \"Reg read timeout occurred\\n\");\n\t\treturn -ETIME;\n\t}\n\n\tif (!(dev_sts & ENA_REGS_DEV_STS_READY_MASK)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Device isn't ready, abort com init\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tadmin_queue->q_depth = ENA_ADMIN_QUEUE_DEPTH;\n\n\tadmin_queue->q_dmadev = ena_dev->dmadev;\n\tadmin_queue->polling = false;\n\tadmin_queue->curr_cmd_id = 0;\n\n\tatomic_set(&admin_queue->outstanding_cmds, 0);\n\n\tspin_lock_init(&admin_queue->q_lock);\n\n\tret = ena_com_init_comp_ctxt(admin_queue);\n\tif (ret)\n\t\tgoto error;\n\n\tret = ena_com_admin_init_sq(admin_queue);\n\tif (ret)\n\t\tgoto error;\n\n\tret = ena_com_admin_init_cq(admin_queue);\n\tif (ret)\n\t\tgoto error;\n\n\tadmin_queue->sq.db_addr = (u32 __iomem *)((uintptr_t)ena_dev->reg_bar +\n\t\tENA_REGS_AQ_DB_OFF);\n\n\taddr_low = ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->sq.dma_addr);\n\taddr_high = ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->sq.dma_addr);\n\n\twritel(addr_low, ena_dev->reg_bar + ENA_REGS_AQ_BASE_LO_OFF);\n\twritel(addr_high, ena_dev->reg_bar + ENA_REGS_AQ_BASE_HI_OFF);\n\n\taddr_low = ENA_DMA_ADDR_TO_UINT32_LOW(admin_queue->cq.dma_addr);\n\taddr_high = ENA_DMA_ADDR_TO_UINT32_HIGH(admin_queue->cq.dma_addr);\n\n\twritel(addr_low, ena_dev->reg_bar + ENA_REGS_ACQ_BASE_LO_OFF);\n\twritel(addr_high, ena_dev->reg_bar + ENA_REGS_ACQ_BASE_HI_OFF);\n\n\taq_caps = 0;\n\taq_caps |= admin_queue->q_depth & ENA_REGS_AQ_CAPS_AQ_DEPTH_MASK;\n\taq_caps |= (sizeof(struct ena_admin_aq_entry) <<\n\t\t\tENA_REGS_AQ_CAPS_AQ_ENTRY_SIZE_SHIFT) &\n\t\t\tENA_REGS_AQ_CAPS_AQ_ENTRY_SIZE_MASK;\n\n\tacq_caps = 0;\n\tacq_caps |= admin_queue->q_depth & ENA_REGS_ACQ_CAPS_ACQ_DEPTH_MASK;\n\tacq_caps |= (sizeof(struct ena_admin_acq_entry) <<\n\t\tENA_REGS_ACQ_CAPS_ACQ_ENTRY_SIZE_SHIFT) &\n\t\tENA_REGS_ACQ_CAPS_ACQ_ENTRY_SIZE_MASK;\n\n\twritel(aq_caps, ena_dev->reg_bar + ENA_REGS_AQ_CAPS_OFF);\n\twritel(acq_caps, ena_dev->reg_bar + ENA_REGS_ACQ_CAPS_OFF);\n\tret = ena_com_admin_init_aenq(ena_dev, aenq_handlers);\n\tif (ret)\n\t\tgoto error;\n\n\tadmin_queue->ena_dev = ena_dev;\n\tadmin_queue->running_state = true;\n\n\treturn 0;\nerror:\n\tena_com_admin_destroy(ena_dev);\n\n\treturn ret;\n}\n\nint ena_com_create_io_queue(struct ena_com_dev *ena_dev,\n\t\t\t    struct ena_com_create_io_ctx *ctx)\n{\n\tstruct ena_com_io_sq *io_sq;\n\tstruct ena_com_io_cq *io_cq;\n\tint ret;\n\n\tif (ctx->qid >= ENA_TOTAL_NUM_QUEUES) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Qid (%d) is bigger than max num of queues (%d)\\n\",\n\t\t\t   ctx->qid, ENA_TOTAL_NUM_QUEUES);\n\t\treturn -EINVAL;\n\t}\n\n\tio_sq = &ena_dev->io_sq_queues[ctx->qid];\n\tio_cq = &ena_dev->io_cq_queues[ctx->qid];\n\n\tmemset(io_sq, 0x0, sizeof(*io_sq));\n\tmemset(io_cq, 0x0, sizeof(*io_cq));\n\n\t \n\tio_cq->q_depth = ctx->queue_size;\n\tio_cq->direction = ctx->direction;\n\tio_cq->qid = ctx->qid;\n\n\tio_cq->msix_vector = ctx->msix_vector;\n\n\tio_sq->q_depth = ctx->queue_size;\n\tio_sq->direction = ctx->direction;\n\tio_sq->qid = ctx->qid;\n\n\tio_sq->mem_queue_type = ctx->mem_queue_type;\n\n\tif (ctx->direction == ENA_COM_IO_QUEUE_DIRECTION_TX)\n\t\t \n\t\tio_sq->tx_max_header_size =\n\t\t\tmin_t(u32, ena_dev->tx_max_header_size, SZ_256);\n\n\tret = ena_com_init_io_sq(ena_dev, ctx, io_sq);\n\tif (ret)\n\t\tgoto error;\n\tret = ena_com_init_io_cq(ena_dev, ctx, io_cq);\n\tif (ret)\n\t\tgoto error;\n\n\tret = ena_com_create_io_cq(ena_dev, io_cq);\n\tif (ret)\n\t\tgoto error;\n\n\tret = ena_com_create_io_sq(ena_dev, io_sq, io_cq->idx);\n\tif (ret)\n\t\tgoto destroy_io_cq;\n\n\treturn 0;\n\ndestroy_io_cq:\n\tena_com_destroy_io_cq(ena_dev, io_cq);\nerror:\n\tena_com_io_queue_free(ena_dev, io_sq, io_cq);\n\treturn ret;\n}\n\nvoid ena_com_destroy_io_queue(struct ena_com_dev *ena_dev, u16 qid)\n{\n\tstruct ena_com_io_sq *io_sq;\n\tstruct ena_com_io_cq *io_cq;\n\n\tif (qid >= ENA_TOTAL_NUM_QUEUES) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Qid (%d) is bigger than max num of queues (%d)\\n\",\n\t\t\t   qid, ENA_TOTAL_NUM_QUEUES);\n\t\treturn;\n\t}\n\n\tio_sq = &ena_dev->io_sq_queues[qid];\n\tio_cq = &ena_dev->io_cq_queues[qid];\n\n\tena_com_destroy_io_sq(ena_dev, io_sq);\n\tena_com_destroy_io_cq(ena_dev, io_cq);\n\n\tena_com_io_queue_free(ena_dev, io_sq, io_cq);\n}\n\nint ena_com_get_link_params(struct ena_com_dev *ena_dev,\n\t\t\t    struct ena_admin_get_feat_resp *resp)\n{\n\treturn ena_com_get_feature(ena_dev, resp, ENA_ADMIN_LINK_CONFIG, 0);\n}\n\nint ena_com_get_dev_attr_feat(struct ena_com_dev *ena_dev,\n\t\t\t      struct ena_com_dev_get_features_ctx *get_feat_ctx)\n{\n\tstruct ena_admin_get_feat_resp get_resp;\n\tint rc;\n\n\trc = ena_com_get_feature(ena_dev, &get_resp,\n\t\t\t\t ENA_ADMIN_DEVICE_ATTRIBUTES, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tmemcpy(&get_feat_ctx->dev_attr, &get_resp.u.dev_attr,\n\t       sizeof(get_resp.u.dev_attr));\n\n\tena_dev->supported_features = get_resp.u.dev_attr.supported_features;\n\tena_dev->capabilities = get_resp.u.dev_attr.capabilities;\n\n\tif (ena_dev->supported_features & BIT(ENA_ADMIN_MAX_QUEUES_EXT)) {\n\t\trc = ena_com_get_feature(ena_dev, &get_resp,\n\t\t\t\t\t ENA_ADMIN_MAX_QUEUES_EXT,\n\t\t\t\t\t ENA_FEATURE_MAX_QUEUE_EXT_VER);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (get_resp.u.max_queue_ext.version !=\n\t\t    ENA_FEATURE_MAX_QUEUE_EXT_VER)\n\t\t\treturn -EINVAL;\n\n\t\tmemcpy(&get_feat_ctx->max_queue_ext, &get_resp.u.max_queue_ext,\n\t\t       sizeof(get_resp.u.max_queue_ext));\n\t\tena_dev->tx_max_header_size =\n\t\t\tget_resp.u.max_queue_ext.max_queue_ext.max_tx_header_size;\n\t} else {\n\t\trc = ena_com_get_feature(ena_dev, &get_resp,\n\t\t\t\t\t ENA_ADMIN_MAX_QUEUES_NUM, 0);\n\t\tmemcpy(&get_feat_ctx->max_queues, &get_resp.u.max_queue,\n\t\t       sizeof(get_resp.u.max_queue));\n\t\tena_dev->tx_max_header_size =\n\t\t\tget_resp.u.max_queue.max_header_size;\n\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\trc = ena_com_get_feature(ena_dev, &get_resp,\n\t\t\t\t ENA_ADMIN_AENQ_CONFIG, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tmemcpy(&get_feat_ctx->aenq, &get_resp.u.aenq,\n\t       sizeof(get_resp.u.aenq));\n\n\trc = ena_com_get_feature(ena_dev, &get_resp,\n\t\t\t\t ENA_ADMIN_STATELESS_OFFLOAD_CONFIG, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tmemcpy(&get_feat_ctx->offload, &get_resp.u.offload,\n\t       sizeof(get_resp.u.offload));\n\n\t \n\trc = ena_com_get_feature(ena_dev, &get_resp, ENA_ADMIN_HW_HINTS, 0);\n\n\tif (!rc)\n\t\tmemcpy(&get_feat_ctx->hw_hints, &get_resp.u.hw_hints,\n\t\t       sizeof(get_resp.u.hw_hints));\n\telse if (rc == -EOPNOTSUPP)\n\t\tmemset(&get_feat_ctx->hw_hints, 0x0,\n\t\t       sizeof(get_feat_ctx->hw_hints));\n\telse\n\t\treturn rc;\n\n\trc = ena_com_get_feature(ena_dev, &get_resp, ENA_ADMIN_LLQ, 0);\n\tif (!rc)\n\t\tmemcpy(&get_feat_ctx->llq, &get_resp.u.llq,\n\t\t       sizeof(get_resp.u.llq));\n\telse if (rc == -EOPNOTSUPP)\n\t\tmemset(&get_feat_ctx->llq, 0x0, sizeof(get_feat_ctx->llq));\n\telse\n\t\treturn rc;\n\n\treturn 0;\n}\n\nvoid ena_com_admin_q_comp_intr_handler(struct ena_com_dev *ena_dev)\n{\n\tena_com_handle_admin_completion(&ena_dev->admin_queue);\n}\n\n \nstatic ena_aenq_handler ena_com_get_specific_aenq_cb(struct ena_com_dev *ena_dev,\n\t\t\t\t\t\t     u16 group)\n{\n\tstruct ena_aenq_handlers *aenq_handlers = ena_dev->aenq.aenq_handlers;\n\n\tif ((group < ENA_MAX_HANDLERS) && aenq_handlers->handlers[group])\n\t\treturn aenq_handlers->handlers[group];\n\n\treturn aenq_handlers->unimplemented_handler;\n}\n\n \nvoid ena_com_aenq_intr_handler(struct ena_com_dev *ena_dev, void *data)\n{\n\tstruct ena_admin_aenq_entry *aenq_e;\n\tstruct ena_admin_aenq_common_desc *aenq_common;\n\tstruct ena_com_aenq *aenq  = &ena_dev->aenq;\n\tu64 timestamp;\n\tena_aenq_handler handler_cb;\n\tu16 masked_head, processed = 0;\n\tu8 phase;\n\n\tmasked_head = aenq->head & (aenq->q_depth - 1);\n\tphase = aenq->phase;\n\taenq_e = &aenq->entries[masked_head];  \n\taenq_common = &aenq_e->aenq_common_desc;\n\n\t \n\twhile ((READ_ONCE(aenq_common->flags) &\n\t\tENA_ADMIN_AENQ_COMMON_DESC_PHASE_MASK) == phase) {\n\t\t \n\t\tdma_rmb();\n\n\t\ttimestamp = (u64)aenq_common->timestamp_low |\n\t\t\t((u64)aenq_common->timestamp_high << 32);\n\n\t\tnetdev_dbg(ena_dev->net_device,\n\t\t\t   \"AENQ! Group[%x] Syndrome[%x] timestamp: [%llus]\\n\",\n\t\t\t   aenq_common->group, aenq_common->syndrome, timestamp);\n\n\t\t \n\t\thandler_cb = ena_com_get_specific_aenq_cb(ena_dev,\n\t\t\t\t\t\t\t  aenq_common->group);\n\t\thandler_cb(data, aenq_e);  \n\n\t\t \n\t\tmasked_head++;\n\t\tprocessed++;\n\n\t\tif (unlikely(masked_head == aenq->q_depth)) {\n\t\t\tmasked_head = 0;\n\t\t\tphase = !phase;\n\t\t}\n\t\taenq_e = &aenq->entries[masked_head];\n\t\taenq_common = &aenq_e->aenq_common_desc;\n\t}\n\n\taenq->head += processed;\n\taenq->phase = phase;\n\n\t \n\tif (!processed)\n\t\treturn;\n\n\t \n\tmb();\n\twritel_relaxed((u32)aenq->head,\n\t\t       ena_dev->reg_bar + ENA_REGS_AENQ_HEAD_DB_OFF);\n}\n\nint ena_com_dev_reset(struct ena_com_dev *ena_dev,\n\t\t      enum ena_regs_reset_reason_types reset_reason)\n{\n\tu32 stat, timeout, cap, reset_val;\n\tint rc;\n\n\tstat = ena_com_reg_bar_read32(ena_dev, ENA_REGS_DEV_STS_OFF);\n\tcap = ena_com_reg_bar_read32(ena_dev, ENA_REGS_CAPS_OFF);\n\n\tif (unlikely((stat == ENA_MMIO_READ_TIMEOUT) ||\n\t\t     (cap == ENA_MMIO_READ_TIMEOUT))) {\n\t\tnetdev_err(ena_dev->net_device, \"Reg read32 timeout occurred\\n\");\n\t\treturn -ETIME;\n\t}\n\n\tif ((stat & ENA_REGS_DEV_STS_READY_MASK) == 0) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Device isn't ready, can't reset device\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\ttimeout = (cap & ENA_REGS_CAPS_RESET_TIMEOUT_MASK) >>\n\t\t\tENA_REGS_CAPS_RESET_TIMEOUT_SHIFT;\n\tif (timeout == 0) {\n\t\tnetdev_err(ena_dev->net_device, \"Invalid timeout value\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\treset_val = ENA_REGS_DEV_CTL_DEV_RESET_MASK;\n\treset_val |= (reset_reason << ENA_REGS_DEV_CTL_RESET_REASON_SHIFT) &\n\t\t     ENA_REGS_DEV_CTL_RESET_REASON_MASK;\n\twritel(reset_val, ena_dev->reg_bar + ENA_REGS_DEV_CTL_OFF);\n\n\t \n\tena_com_mmio_reg_read_request_write_dev_addr(ena_dev);\n\n\trc = wait_for_reset_state(ena_dev, timeout,\n\t\t\t\t  ENA_REGS_DEV_STS_RESET_IN_PROGRESS_MASK);\n\tif (rc != 0) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Reset indication didn't turn on\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\twritel(0, ena_dev->reg_bar + ENA_REGS_DEV_CTL_OFF);\n\trc = wait_for_reset_state(ena_dev, timeout, 0);\n\tif (rc != 0) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Reset indication didn't turn off\\n\");\n\t\treturn rc;\n\t}\n\n\ttimeout = (cap & ENA_REGS_CAPS_ADMIN_CMD_TO_MASK) >>\n\t\tENA_REGS_CAPS_ADMIN_CMD_TO_SHIFT;\n\tif (timeout)\n\t\t \n\t\tena_dev->admin_queue.completion_timeout = timeout * 100000;\n\telse\n\t\tena_dev->admin_queue.completion_timeout = ADMIN_CMD_TIMEOUT_US;\n\n\treturn 0;\n}\n\nstatic int ena_get_dev_stats(struct ena_com_dev *ena_dev,\n\t\t\t     struct ena_com_stats_ctx *ctx,\n\t\t\t     enum ena_admin_get_stats_type type)\n{\n\tstruct ena_admin_aq_get_stats_cmd *get_cmd = &ctx->get_cmd;\n\tstruct ena_admin_acq_get_stats_resp *get_resp = &ctx->get_resp;\n\tstruct ena_com_admin_queue *admin_queue;\n\tint ret;\n\n\tadmin_queue = &ena_dev->admin_queue;\n\n\tget_cmd->aq_common_descriptor.opcode = ENA_ADMIN_GET_STATS;\n\tget_cmd->aq_common_descriptor.flags = 0;\n\tget_cmd->type = type;\n\n\tret =  ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t     (struct ena_admin_aq_entry *)get_cmd,\n\t\t\t\t\t     sizeof(*get_cmd),\n\t\t\t\t\t     (struct ena_admin_acq_entry *)get_resp,\n\t\t\t\t\t     sizeof(*get_resp));\n\n\tif (unlikely(ret))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to get stats. error: %d\\n\", ret);\n\n\treturn ret;\n}\n\nint ena_com_get_eni_stats(struct ena_com_dev *ena_dev,\n\t\t\t  struct ena_admin_eni_stats *stats)\n{\n\tstruct ena_com_stats_ctx ctx;\n\tint ret;\n\n\tif (!ena_com_get_cap(ena_dev, ENA_ADMIN_ENI_STATS)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Capability %d isn't supported\\n\",\n\t\t\t   ENA_ADMIN_ENI_STATS);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmemset(&ctx, 0x0, sizeof(ctx));\n\tret = ena_get_dev_stats(ena_dev, &ctx, ENA_ADMIN_GET_STATS_TYPE_ENI);\n\tif (likely(ret == 0))\n\t\tmemcpy(stats, &ctx.get_resp.u.eni_stats,\n\t\t       sizeof(ctx.get_resp.u.eni_stats));\n\n\treturn ret;\n}\n\nint ena_com_get_dev_basic_stats(struct ena_com_dev *ena_dev,\n\t\t\t\tstruct ena_admin_basic_stats *stats)\n{\n\tstruct ena_com_stats_ctx ctx;\n\tint ret;\n\n\tmemset(&ctx, 0x0, sizeof(ctx));\n\tret = ena_get_dev_stats(ena_dev, &ctx, ENA_ADMIN_GET_STATS_TYPE_BASIC);\n\tif (likely(ret == 0))\n\t\tmemcpy(stats, &ctx.get_resp.u.basic_stats,\n\t\t       sizeof(ctx.get_resp.u.basic_stats));\n\n\treturn ret;\n}\n\nint ena_com_set_dev_mtu(struct ena_com_dev *ena_dev, u32 mtu)\n{\n\tstruct ena_com_admin_queue *admin_queue;\n\tstruct ena_admin_set_feat_cmd cmd;\n\tstruct ena_admin_set_feat_resp resp;\n\tint ret;\n\n\tif (!ena_com_check_supported_feature_id(ena_dev, ENA_ADMIN_MTU)) {\n\t\tnetdev_dbg(ena_dev->net_device, \"Feature %d isn't supported\\n\",\n\t\t\t   ENA_ADMIN_MTU);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmemset(&cmd, 0x0, sizeof(cmd));\n\tadmin_queue = &ena_dev->admin_queue;\n\n\tcmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\n\tcmd.aq_common_descriptor.flags = 0;\n\tcmd.feat_common.feature_id = ENA_ADMIN_MTU;\n\tcmd.u.mtu.mtu = mtu;\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&cmd,\n\t\t\t\t\t    sizeof(cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&resp,\n\t\t\t\t\t    sizeof(resp));\n\n\tif (unlikely(ret))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to set mtu %d. error: %d\\n\", mtu, ret);\n\n\treturn ret;\n}\n\nint ena_com_get_offload_settings(struct ena_com_dev *ena_dev,\n\t\t\t\t struct ena_admin_feature_offload_desc *offload)\n{\n\tint ret;\n\tstruct ena_admin_get_feat_resp resp;\n\n\tret = ena_com_get_feature(ena_dev, &resp,\n\t\t\t\t  ENA_ADMIN_STATELESS_OFFLOAD_CONFIG, 0);\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to get offload capabilities %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tmemcpy(offload, &resp.u.offload, sizeof(resp.u.offload));\n\n\treturn 0;\n}\n\nint ena_com_set_hash_function(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_admin_set_feat_cmd cmd;\n\tstruct ena_admin_set_feat_resp resp;\n\tstruct ena_admin_get_feat_resp get_resp;\n\tint ret;\n\n\tif (!ena_com_check_supported_feature_id(ena_dev,\n\t\t\t\t\t\tENA_ADMIN_RSS_HASH_FUNCTION)) {\n\t\tnetdev_dbg(ena_dev->net_device, \"Feature %d isn't supported\\n\",\n\t\t\t   ENA_ADMIN_RSS_HASH_FUNCTION);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tret = ena_com_get_feature(ena_dev, &get_resp,\n\t\t\t\t  ENA_ADMIN_RSS_HASH_FUNCTION, 0);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (!(get_resp.u.flow_hash_func.supported_func & BIT(rss->hash_func))) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Func hash %d isn't supported by device, abort\\n\",\n\t\t\t   rss->hash_func);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmemset(&cmd, 0x0, sizeof(cmd));\n\n\tcmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\n\tcmd.aq_common_descriptor.flags =\n\t\tENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;\n\tcmd.feat_common.feature_id = ENA_ADMIN_RSS_HASH_FUNCTION;\n\tcmd.u.flow_hash_func.init_val = rss->hash_init_val;\n\tcmd.u.flow_hash_func.selected_func = 1 << rss->hash_func;\n\n\tret = ena_com_mem_addr_set(ena_dev,\n\t\t\t\t   &cmd.control_buffer.address,\n\t\t\t\t   rss->hash_key_dma_addr);\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory address set failed\\n\");\n\t\treturn ret;\n\t}\n\n\tcmd.control_buffer.length = sizeof(*rss->hash_key);\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&cmd,\n\t\t\t\t\t    sizeof(cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&resp,\n\t\t\t\t\t    sizeof(resp));\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to set hash function %d. error: %d\\n\",\n\t\t\t   rss->hash_func, ret);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint ena_com_fill_hash_function(struct ena_com_dev *ena_dev,\n\t\t\t       enum ena_admin_hash_functions func,\n\t\t\t       const u8 *key, u16 key_len, u32 init_val)\n{\n\tstruct ena_admin_feature_rss_flow_hash_control *hash_key;\n\tstruct ena_admin_get_feat_resp get_resp;\n\tenum ena_admin_hash_functions old_func;\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tint rc;\n\n\thash_key = rss->hash_key;\n\n\t \n\tif (unlikely(key_len & 0x3))\n\t\treturn -EINVAL;\n\n\trc = ena_com_get_feature_ex(ena_dev, &get_resp,\n\t\t\t\t    ENA_ADMIN_RSS_HASH_FUNCTION,\n\t\t\t\t    rss->hash_key_dma_addr,\n\t\t\t\t    sizeof(*rss->hash_key), 0);\n\tif (unlikely(rc))\n\t\treturn rc;\n\n\tif (!(BIT(func) & get_resp.u.flow_hash_func.supported_func)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Flow hash function %d isn't supported\\n\", func);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif ((func == ENA_ADMIN_TOEPLITZ) && key) {\n\t\tif (key_len != sizeof(hash_key->key)) {\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"key len (%u) doesn't equal the supported size (%zu)\\n\",\n\t\t\t\t   key_len, sizeof(hash_key->key));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmemcpy(hash_key->key, key, key_len);\n\t\thash_key->key_parts = key_len / sizeof(hash_key->key[0]);\n\t}\n\n\trss->hash_init_val = init_val;\n\told_func = rss->hash_func;\n\trss->hash_func = func;\n\trc = ena_com_set_hash_function(ena_dev);\n\n\t \n\tif (unlikely(rc))\n\t\trss->hash_func = old_func;\n\n\treturn rc;\n}\n\nint ena_com_get_hash_function(struct ena_com_dev *ena_dev,\n\t\t\t      enum ena_admin_hash_functions *func)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_admin_get_feat_resp get_resp;\n\tint rc;\n\n\tif (unlikely(!func))\n\t\treturn -EINVAL;\n\n\trc = ena_com_get_feature_ex(ena_dev, &get_resp,\n\t\t\t\t    ENA_ADMIN_RSS_HASH_FUNCTION,\n\t\t\t\t    rss->hash_key_dma_addr,\n\t\t\t\t    sizeof(*rss->hash_key), 0);\n\tif (unlikely(rc))\n\t\treturn rc;\n\n\t \n\trss->hash_func = ffs(get_resp.u.flow_hash_func.selected_func);\n\tif (rss->hash_func)\n\t\trss->hash_func--;\n\n\t*func = rss->hash_func;\n\n\treturn 0;\n}\n\nint ena_com_get_hash_key(struct ena_com_dev *ena_dev, u8 *key)\n{\n\tstruct ena_admin_feature_rss_flow_hash_control *hash_key =\n\t\tena_dev->rss.hash_key;\n\n\tif (key)\n\t\tmemcpy(key, hash_key->key,\n\t\t       (size_t)(hash_key->key_parts) * sizeof(hash_key->key[0]));\n\n\treturn 0;\n}\n\nint ena_com_get_hash_ctrl(struct ena_com_dev *ena_dev,\n\t\t\t  enum ena_admin_flow_hash_proto proto,\n\t\t\t  u16 *fields)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_admin_get_feat_resp get_resp;\n\tint rc;\n\n\trc = ena_com_get_feature_ex(ena_dev, &get_resp,\n\t\t\t\t    ENA_ADMIN_RSS_HASH_INPUT,\n\t\t\t\t    rss->hash_ctrl_dma_addr,\n\t\t\t\t    sizeof(*rss->hash_ctrl), 0);\n\tif (unlikely(rc))\n\t\treturn rc;\n\n\tif (fields)\n\t\t*fields = rss->hash_ctrl->selected_fields[proto].fields;\n\n\treturn 0;\n}\n\nint ena_com_set_hash_ctrl(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_admin_feature_rss_hash_control *hash_ctrl = rss->hash_ctrl;\n\tstruct ena_admin_set_feat_cmd cmd;\n\tstruct ena_admin_set_feat_resp resp;\n\tint ret;\n\n\tif (!ena_com_check_supported_feature_id(ena_dev,\n\t\t\t\t\t\tENA_ADMIN_RSS_HASH_INPUT)) {\n\t\tnetdev_dbg(ena_dev->net_device, \"Feature %d isn't supported\\n\",\n\t\t\t   ENA_ADMIN_RSS_HASH_INPUT);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmemset(&cmd, 0x0, sizeof(cmd));\n\n\tcmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\n\tcmd.aq_common_descriptor.flags =\n\t\tENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;\n\tcmd.feat_common.feature_id = ENA_ADMIN_RSS_HASH_INPUT;\n\tcmd.u.flow_hash_input.enabled_input_sort =\n\t\tENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L3_SORT_MASK |\n\t\tENA_ADMIN_FEATURE_RSS_FLOW_HASH_INPUT_L4_SORT_MASK;\n\n\tret = ena_com_mem_addr_set(ena_dev,\n\t\t\t\t   &cmd.control_buffer.address,\n\t\t\t\t   rss->hash_ctrl_dma_addr);\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory address set failed\\n\");\n\t\treturn ret;\n\t}\n\tcmd.control_buffer.length = sizeof(*hash_ctrl);\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&cmd,\n\t\t\t\t\t    sizeof(cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&resp,\n\t\t\t\t\t    sizeof(resp));\n\tif (unlikely(ret))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to set hash input. error: %d\\n\", ret);\n\n\treturn ret;\n}\n\nint ena_com_set_default_hash_ctrl(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_admin_feature_rss_hash_control *hash_ctrl =\n\t\trss->hash_ctrl;\n\tu16 available_fields = 0;\n\tint rc, i;\n\n\t \n\trc = ena_com_get_hash_ctrl(ena_dev, 0, NULL);\n\tif (unlikely(rc))\n\t\treturn rc;\n\n\thash_ctrl->selected_fields[ENA_ADMIN_RSS_TCP4].fields =\n\t\tENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |\n\t\tENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;\n\n\thash_ctrl->selected_fields[ENA_ADMIN_RSS_UDP4].fields =\n\t\tENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |\n\t\tENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;\n\n\thash_ctrl->selected_fields[ENA_ADMIN_RSS_TCP6].fields =\n\t\tENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |\n\t\tENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;\n\n\thash_ctrl->selected_fields[ENA_ADMIN_RSS_UDP6].fields =\n\t\tENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA |\n\t\tENA_ADMIN_RSS_L4_DP | ENA_ADMIN_RSS_L4_SP;\n\n\thash_ctrl->selected_fields[ENA_ADMIN_RSS_IP4].fields =\n\t\tENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA;\n\n\thash_ctrl->selected_fields[ENA_ADMIN_RSS_IP6].fields =\n\t\tENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA;\n\n\thash_ctrl->selected_fields[ENA_ADMIN_RSS_IP4_FRAG].fields =\n\t\tENA_ADMIN_RSS_L3_SA | ENA_ADMIN_RSS_L3_DA;\n\n\thash_ctrl->selected_fields[ENA_ADMIN_RSS_NOT_IP].fields =\n\t\tENA_ADMIN_RSS_L2_DA | ENA_ADMIN_RSS_L2_SA;\n\n\tfor (i = 0; i < ENA_ADMIN_RSS_PROTO_NUM; i++) {\n\t\tavailable_fields = hash_ctrl->selected_fields[i].fields &\n\t\t\t\thash_ctrl->supported_fields[i].fields;\n\t\tif (available_fields != hash_ctrl->selected_fields[i].fields) {\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"Hash control doesn't support all the desire configuration. proto %x supported %x selected %x\\n\",\n\t\t\t\t   i, hash_ctrl->supported_fields[i].fields,\n\t\t\t\t   hash_ctrl->selected_fields[i].fields);\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t}\n\n\trc = ena_com_set_hash_ctrl(ena_dev);\n\n\t \n\tif (unlikely(rc))\n\t\tena_com_get_hash_ctrl(ena_dev, 0, NULL);\n\n\treturn rc;\n}\n\nint ena_com_fill_hash_ctrl(struct ena_com_dev *ena_dev,\n\t\t\t   enum ena_admin_flow_hash_proto proto,\n\t\t\t   u16 hash_fields)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_admin_feature_rss_hash_control *hash_ctrl = rss->hash_ctrl;\n\tu16 supported_fields;\n\tint rc;\n\n\tif (proto >= ENA_ADMIN_RSS_PROTO_NUM) {\n\t\tnetdev_err(ena_dev->net_device, \"Invalid proto num (%u)\\n\",\n\t\t\t   proto);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\trc = ena_com_get_hash_ctrl(ena_dev, proto, NULL);\n\tif (unlikely(rc))\n\t\treturn rc;\n\n\t \n\tsupported_fields = hash_ctrl->supported_fields[proto].fields;\n\tif ((hash_fields & supported_fields) != hash_fields) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Proto %d doesn't support the required fields %x. supports only: %x\\n\",\n\t\t\t   proto, hash_fields, supported_fields);\n\t}\n\n\thash_ctrl->selected_fields[proto].fields = hash_fields;\n\n\trc = ena_com_set_hash_ctrl(ena_dev);\n\n\t \n\tif (unlikely(rc))\n\t\tena_com_get_hash_ctrl(ena_dev, 0, NULL);\n\n\treturn 0;\n}\n\nint ena_com_indirect_table_fill_entry(struct ena_com_dev *ena_dev,\n\t\t\t\t      u16 entry_idx, u16 entry_value)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\n\tif (unlikely(entry_idx >= (1 << rss->tbl_log_size)))\n\t\treturn -EINVAL;\n\n\tif (unlikely((entry_value > ENA_TOTAL_NUM_QUEUES)))\n\t\treturn -EINVAL;\n\n\trss->host_rss_ind_tbl[entry_idx] = entry_value;\n\n\treturn 0;\n}\n\nint ena_com_indirect_table_set(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_com_admin_queue *admin_queue = &ena_dev->admin_queue;\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_admin_set_feat_cmd cmd;\n\tstruct ena_admin_set_feat_resp resp;\n\tint ret;\n\n\tif (!ena_com_check_supported_feature_id(\n\t\t    ena_dev, ENA_ADMIN_RSS_INDIRECTION_TABLE_CONFIG)) {\n\t\tnetdev_dbg(ena_dev->net_device, \"Feature %d isn't supported\\n\",\n\t\t\t   ENA_ADMIN_RSS_INDIRECTION_TABLE_CONFIG);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tret = ena_com_ind_tbl_convert_to_device(ena_dev);\n\tif (ret) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to convert host indirection table to device table\\n\");\n\t\treturn ret;\n\t}\n\n\tmemset(&cmd, 0x0, sizeof(cmd));\n\n\tcmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\n\tcmd.aq_common_descriptor.flags =\n\t\tENA_ADMIN_AQ_COMMON_DESC_CTRL_DATA_INDIRECT_MASK;\n\tcmd.feat_common.feature_id = ENA_ADMIN_RSS_INDIRECTION_TABLE_CONFIG;\n\tcmd.u.ind_table.size = rss->tbl_log_size;\n\tcmd.u.ind_table.inline_index = 0xFFFFFFFF;\n\n\tret = ena_com_mem_addr_set(ena_dev,\n\t\t\t\t   &cmd.control_buffer.address,\n\t\t\t\t   rss->rss_ind_tbl_dma_addr);\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory address set failed\\n\");\n\t\treturn ret;\n\t}\n\n\tcmd.control_buffer.length = (1ULL << rss->tbl_log_size) *\n\t\tsizeof(struct ena_admin_rss_ind_table_entry);\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&cmd,\n\t\t\t\t\t    sizeof(cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&resp,\n\t\t\t\t\t    sizeof(resp));\n\n\tif (unlikely(ret))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to set indirect table. error: %d\\n\", ret);\n\n\treturn ret;\n}\n\nint ena_com_indirect_table_get(struct ena_com_dev *ena_dev, u32 *ind_tbl)\n{\n\tstruct ena_rss *rss = &ena_dev->rss;\n\tstruct ena_admin_get_feat_resp get_resp;\n\tu32 tbl_size;\n\tint i, rc;\n\n\ttbl_size = (1ULL << rss->tbl_log_size) *\n\t\tsizeof(struct ena_admin_rss_ind_table_entry);\n\n\trc = ena_com_get_feature_ex(ena_dev, &get_resp,\n\t\t\t\t    ENA_ADMIN_RSS_INDIRECTION_TABLE_CONFIG,\n\t\t\t\t    rss->rss_ind_tbl_dma_addr,\n\t\t\t\t    tbl_size, 0);\n\tif (unlikely(rc))\n\t\treturn rc;\n\n\tif (!ind_tbl)\n\t\treturn 0;\n\n\tfor (i = 0; i < (1 << rss->tbl_log_size); i++)\n\t\tind_tbl[i] = rss->host_rss_ind_tbl[i];\n\n\treturn 0;\n}\n\nint ena_com_rss_init(struct ena_com_dev *ena_dev, u16 indr_tbl_log_size)\n{\n\tint rc;\n\n\tmemset(&ena_dev->rss, 0x0, sizeof(ena_dev->rss));\n\n\trc = ena_com_indirect_table_allocate(ena_dev, indr_tbl_log_size);\n\tif (unlikely(rc))\n\t\tgoto err_indr_tbl;\n\n\t \n\trc = ena_com_hash_key_allocate(ena_dev);\n\tif (likely(!rc))\n\t\tena_com_hash_key_fill_default_key(ena_dev);\n\telse if (rc != -EOPNOTSUPP)\n\t\tgoto err_hash_key;\n\n\trc = ena_com_hash_ctrl_init(ena_dev);\n\tif (unlikely(rc))\n\t\tgoto err_hash_ctrl;\n\n\treturn 0;\n\nerr_hash_ctrl:\n\tena_com_hash_key_destroy(ena_dev);\nerr_hash_key:\n\tena_com_indirect_table_destroy(ena_dev);\nerr_indr_tbl:\n\n\treturn rc;\n}\n\nvoid ena_com_rss_destroy(struct ena_com_dev *ena_dev)\n{\n\tena_com_indirect_table_destroy(ena_dev);\n\tena_com_hash_key_destroy(ena_dev);\n\tena_com_hash_ctrl_destroy(ena_dev);\n\n\tmemset(&ena_dev->rss, 0x0, sizeof(ena_dev->rss));\n}\n\nint ena_com_allocate_host_info(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\n\n\thost_attr->host_info =\n\t\tdma_alloc_coherent(ena_dev->dmadev, SZ_4K,\n\t\t\t\t   &host_attr->host_info_dma_addr, GFP_KERNEL);\n\tif (unlikely(!host_attr->host_info))\n\t\treturn -ENOMEM;\n\n\thost_attr->host_info->ena_spec_version = ((ENA_COMMON_SPEC_VERSION_MAJOR <<\n\t\tENA_REGS_VERSION_MAJOR_VERSION_SHIFT) |\n\t\t(ENA_COMMON_SPEC_VERSION_MINOR));\n\n\treturn 0;\n}\n\nint ena_com_allocate_debug_area(struct ena_com_dev *ena_dev,\n\t\t\t\tu32 debug_area_size)\n{\n\tstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\n\n\thost_attr->debug_area_virt_addr =\n\t\tdma_alloc_coherent(ena_dev->dmadev, debug_area_size,\n\t\t\t\t   &host_attr->debug_area_dma_addr, GFP_KERNEL);\n\tif (unlikely(!host_attr->debug_area_virt_addr)) {\n\t\thost_attr->debug_area_size = 0;\n\t\treturn -ENOMEM;\n\t}\n\n\thost_attr->debug_area_size = debug_area_size;\n\n\treturn 0;\n}\n\nvoid ena_com_delete_host_info(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\n\n\tif (host_attr->host_info) {\n\t\tdma_free_coherent(ena_dev->dmadev, SZ_4K, host_attr->host_info,\n\t\t\t\t  host_attr->host_info_dma_addr);\n\t\thost_attr->host_info = NULL;\n\t}\n}\n\nvoid ena_com_delete_debug_area(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\n\n\tif (host_attr->debug_area_virt_addr) {\n\t\tdma_free_coherent(ena_dev->dmadev, host_attr->debug_area_size,\n\t\t\t\t  host_attr->debug_area_virt_addr,\n\t\t\t\t  host_attr->debug_area_dma_addr);\n\t\thost_attr->debug_area_virt_addr = NULL;\n\t}\n}\n\nint ena_com_set_host_attributes(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_host_attribute *host_attr = &ena_dev->host_attr;\n\tstruct ena_com_admin_queue *admin_queue;\n\tstruct ena_admin_set_feat_cmd cmd;\n\tstruct ena_admin_set_feat_resp resp;\n\n\tint ret;\n\n\t \n\n\tmemset(&cmd, 0x0, sizeof(cmd));\n\tadmin_queue = &ena_dev->admin_queue;\n\n\tcmd.aq_common_descriptor.opcode = ENA_ADMIN_SET_FEATURE;\n\tcmd.feat_common.feature_id = ENA_ADMIN_HOST_ATTR_CONFIG;\n\n\tret = ena_com_mem_addr_set(ena_dev,\n\t\t\t\t   &cmd.u.host_attr.debug_ba,\n\t\t\t\t   host_attr->debug_area_dma_addr);\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory address set failed\\n\");\n\t\treturn ret;\n\t}\n\n\tret = ena_com_mem_addr_set(ena_dev,\n\t\t\t\t   &cmd.u.host_attr.os_info_ba,\n\t\t\t\t   host_attr->host_info_dma_addr);\n\tif (unlikely(ret)) {\n\t\tnetdev_err(ena_dev->net_device, \"Memory address set failed\\n\");\n\t\treturn ret;\n\t}\n\n\tcmd.u.host_attr.debug_area_size = host_attr->debug_area_size;\n\n\tret = ena_com_execute_admin_command(admin_queue,\n\t\t\t\t\t    (struct ena_admin_aq_entry *)&cmd,\n\t\t\t\t\t    sizeof(cmd),\n\t\t\t\t\t    (struct ena_admin_acq_entry *)&resp,\n\t\t\t\t\t    sizeof(resp));\n\n\tif (unlikely(ret))\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Failed to set host attributes: %d\\n\", ret);\n\n\treturn ret;\n}\n\n \nbool ena_com_interrupt_moderation_supported(struct ena_com_dev *ena_dev)\n{\n\treturn ena_com_check_supported_feature_id(ena_dev,\n\t\t\t\t\t\t  ENA_ADMIN_INTERRUPT_MODERATION);\n}\n\nstatic int ena_com_update_nonadaptive_moderation_interval(struct ena_com_dev *ena_dev,\n\t\t\t\t\t\t\t  u32 coalesce_usecs,\n\t\t\t\t\t\t\t  u32 intr_delay_resolution,\n\t\t\t\t\t\t\t  u32 *intr_moder_interval)\n{\n\tif (!intr_delay_resolution) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"Illegal interrupt delay granularity value\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\t*intr_moder_interval = coalesce_usecs / intr_delay_resolution;\n\n\treturn 0;\n}\n\nint ena_com_update_nonadaptive_moderation_interval_tx(struct ena_com_dev *ena_dev,\n\t\t\t\t\t\t      u32 tx_coalesce_usecs)\n{\n\treturn ena_com_update_nonadaptive_moderation_interval(ena_dev,\n\t\t\t\t\t\t\t      tx_coalesce_usecs,\n\t\t\t\t\t\t\t      ena_dev->intr_delay_resolution,\n\t\t\t\t\t\t\t      &ena_dev->intr_moder_tx_interval);\n}\n\nint ena_com_update_nonadaptive_moderation_interval_rx(struct ena_com_dev *ena_dev,\n\t\t\t\t\t\t      u32 rx_coalesce_usecs)\n{\n\treturn ena_com_update_nonadaptive_moderation_interval(ena_dev,\n\t\t\t\t\t\t\t      rx_coalesce_usecs,\n\t\t\t\t\t\t\t      ena_dev->intr_delay_resolution,\n\t\t\t\t\t\t\t      &ena_dev->intr_moder_rx_interval);\n}\n\nint ena_com_init_interrupt_moderation(struct ena_com_dev *ena_dev)\n{\n\tstruct ena_admin_get_feat_resp get_resp;\n\tu16 delay_resolution;\n\tint rc;\n\n\trc = ena_com_get_feature(ena_dev, &get_resp,\n\t\t\t\t ENA_ADMIN_INTERRUPT_MODERATION, 0);\n\n\tif (rc) {\n\t\tif (rc == -EOPNOTSUPP) {\n\t\t\tnetdev_dbg(ena_dev->net_device,\n\t\t\t\t   \"Feature %d isn't supported\\n\",\n\t\t\t\t   ENA_ADMIN_INTERRUPT_MODERATION);\n\t\t\trc = 0;\n\t\t} else {\n\t\t\tnetdev_err(ena_dev->net_device,\n\t\t\t\t   \"Failed to get interrupt moderation admin cmd. rc: %d\\n\",\n\t\t\t\t   rc);\n\t\t}\n\n\t\t \n\t\tena_com_disable_adaptive_moderation(ena_dev);\n\t\treturn rc;\n\t}\n\n\t \n\tdelay_resolution = get_resp.u.intr_moderation.intr_delay_resolution;\n\tena_com_update_intr_delay_resolution(ena_dev, delay_resolution);\n\n\t \n\tena_com_disable_adaptive_moderation(ena_dev);\n\n\treturn 0;\n}\n\nunsigned int ena_com_get_nonadaptive_moderation_interval_tx(struct ena_com_dev *ena_dev)\n{\n\treturn ena_dev->intr_moder_tx_interval;\n}\n\nunsigned int ena_com_get_nonadaptive_moderation_interval_rx(struct ena_com_dev *ena_dev)\n{\n\treturn ena_dev->intr_moder_rx_interval;\n}\n\nint ena_com_config_dev_mode(struct ena_com_dev *ena_dev,\n\t\t\t    struct ena_admin_feature_llq_desc *llq_features,\n\t\t\t    struct ena_llq_configurations *llq_default_cfg)\n{\n\tstruct ena_com_llq_info *llq_info = &ena_dev->llq_info;\n\tint rc;\n\n\tif (!llq_features->max_llq_num) {\n\t\tena_dev->tx_mem_queue_type = ENA_ADMIN_PLACEMENT_POLICY_HOST;\n\t\treturn 0;\n\t}\n\n\trc = ena_com_config_llq_info(ena_dev, llq_features, llq_default_cfg);\n\tif (rc)\n\t\treturn rc;\n\n\tena_dev->tx_max_header_size = llq_info->desc_list_entry_size -\n\t\t(llq_info->descs_num_before_header * sizeof(struct ena_eth_io_tx_desc));\n\n\tif (unlikely(ena_dev->tx_max_header_size == 0)) {\n\t\tnetdev_err(ena_dev->net_device,\n\t\t\t   \"The size of the LLQ entry is smaller than needed\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tena_dev->tx_mem_queue_type = ENA_ADMIN_PLACEMENT_POLICY_DEV;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}