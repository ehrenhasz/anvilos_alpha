{
  "module_name": "enetc.c",
  "hash_id": "5eab6ae0f8e12214b00bed31f39318c4fe1a70b16b79603dd5d9551bc9d76aea",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/freescale/enetc/enetc.c",
  "human_readable_source": "\n \n\n#include \"enetc.h\"\n#include <linux/bpf_trace.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/vmalloc.h>\n#include <linux/ptp_classify.h>\n#include <net/ip6_checksum.h>\n#include <net/pkt_sched.h>\n#include <net/tso.h>\n\nu32 enetc_port_mac_rd(struct enetc_si *si, u32 reg)\n{\n\treturn enetc_port_rd(&si->hw, reg);\n}\nEXPORT_SYMBOL_GPL(enetc_port_mac_rd);\n\nvoid enetc_port_mac_wr(struct enetc_si *si, u32 reg, u32 val)\n{\n\tenetc_port_wr(&si->hw, reg, val);\n\tif (si->hw_features & ENETC_SI_F_QBU)\n\t\tenetc_port_wr(&si->hw, reg + ENETC_PMAC_OFFSET, val);\n}\nEXPORT_SYMBOL_GPL(enetc_port_mac_wr);\n\nstatic void enetc_change_preemptible_tcs(struct enetc_ndev_priv *priv,\n\t\t\t\t\t u8 preemptible_tcs)\n{\n\tpriv->preemptible_tcs = preemptible_tcs;\n\tenetc_mm_commit_preemptible_tcs(priv);\n}\n\nstatic int enetc_num_stack_tx_queues(struct enetc_ndev_priv *priv)\n{\n\tint num_tx_rings = priv->num_tx_rings;\n\n\tif (priv->xdp_prog)\n\t\treturn num_tx_rings - num_possible_cpus();\n\n\treturn num_tx_rings;\n}\n\nstatic struct enetc_bdr *enetc_rx_ring_from_xdp_tx_ring(struct enetc_ndev_priv *priv,\n\t\t\t\t\t\t\tstruct enetc_bdr *tx_ring)\n{\n\tint index = &priv->tx_ring[tx_ring->index] - priv->xdp_tx_ring;\n\n\treturn priv->rx_ring[index];\n}\n\nstatic struct sk_buff *enetc_tx_swbd_get_skb(struct enetc_tx_swbd *tx_swbd)\n{\n\tif (tx_swbd->is_xdp_tx || tx_swbd->is_xdp_redirect)\n\t\treturn NULL;\n\n\treturn tx_swbd->skb;\n}\n\nstatic struct xdp_frame *\nenetc_tx_swbd_get_xdp_frame(struct enetc_tx_swbd *tx_swbd)\n{\n\tif (tx_swbd->is_xdp_redirect)\n\t\treturn tx_swbd->xdp_frame;\n\n\treturn NULL;\n}\n\nstatic void enetc_unmap_tx_buff(struct enetc_bdr *tx_ring,\n\t\t\t\tstruct enetc_tx_swbd *tx_swbd)\n{\n\t \n\tif (tx_swbd->is_dma_page)\n\t\tdma_unmap_page(tx_ring->dev, tx_swbd->dma,\n\t\t\t       tx_swbd->is_xdp_tx ? PAGE_SIZE : tx_swbd->len,\n\t\t\t       tx_swbd->dir);\n\telse\n\t\tdma_unmap_single(tx_ring->dev, tx_swbd->dma,\n\t\t\t\t tx_swbd->len, tx_swbd->dir);\n\ttx_swbd->dma = 0;\n}\n\nstatic void enetc_free_tx_frame(struct enetc_bdr *tx_ring,\n\t\t\t\tstruct enetc_tx_swbd *tx_swbd)\n{\n\tstruct xdp_frame *xdp_frame = enetc_tx_swbd_get_xdp_frame(tx_swbd);\n\tstruct sk_buff *skb = enetc_tx_swbd_get_skb(tx_swbd);\n\n\tif (tx_swbd->dma)\n\t\tenetc_unmap_tx_buff(tx_ring, tx_swbd);\n\n\tif (xdp_frame) {\n\t\txdp_return_frame(tx_swbd->xdp_frame);\n\t\ttx_swbd->xdp_frame = NULL;\n\t} else if (skb) {\n\t\tdev_kfree_skb_any(skb);\n\t\ttx_swbd->skb = NULL;\n\t}\n}\n\n \nstatic void enetc_update_tx_ring_tail(struct enetc_bdr *tx_ring)\n{\n\t \n\tenetc_wr_reg_hot(tx_ring->tpir, tx_ring->next_to_use);\n}\n\nstatic int enetc_ptp_parse(struct sk_buff *skb, u8 *udp,\n\t\t\t   u8 *msgtype, u8 *twostep,\n\t\t\t   u16 *correction_offset, u16 *body_offset)\n{\n\tunsigned int ptp_class;\n\tstruct ptp_header *hdr;\n\tunsigned int type;\n\tu8 *base;\n\n\tptp_class = ptp_classify_raw(skb);\n\tif (ptp_class == PTP_CLASS_NONE)\n\t\treturn -EINVAL;\n\n\thdr = ptp_parse_header(skb, ptp_class);\n\tif (!hdr)\n\t\treturn -EINVAL;\n\n\ttype = ptp_class & PTP_CLASS_PMASK;\n\tif (type == PTP_CLASS_IPV4 || type == PTP_CLASS_IPV6)\n\t\t*udp = 1;\n\telse\n\t\t*udp = 0;\n\n\t*msgtype = ptp_get_msgtype(hdr, ptp_class);\n\t*twostep = hdr->flag_field[0] & 0x2;\n\n\tbase = skb_mac_header(skb);\n\t*correction_offset = (u8 *)&hdr->correction - base;\n\t*body_offset = (u8 *)hdr + sizeof(struct ptp_header) - base;\n\n\treturn 0;\n}\n\nstatic int enetc_map_tx_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb)\n{\n\tbool do_vlan, do_onestep_tstamp = false, do_twostep_tstamp = false;\n\tstruct enetc_ndev_priv *priv = netdev_priv(tx_ring->ndev);\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tstruct enetc_tx_swbd *tx_swbd;\n\tint len = skb_headlen(skb);\n\tunion enetc_tx_bd temp_bd;\n\tu8 msgtype, twostep, udp;\n\tunion enetc_tx_bd *txbd;\n\tu16 offset1, offset2;\n\tint i, count = 0;\n\tskb_frag_t *frag;\n\tunsigned int f;\n\tdma_addr_t dma;\n\tu8 flags = 0;\n\n\ti = tx_ring->next_to_use;\n\ttxbd = ENETC_TXBD(*tx_ring, i);\n\tprefetchw(txbd);\n\n\tdma = dma_map_single(tx_ring->dev, skb->data, len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(tx_ring->dev, dma)))\n\t\tgoto dma_err;\n\n\ttemp_bd.addr = cpu_to_le64(dma);\n\ttemp_bd.buf_len = cpu_to_le16(len);\n\ttemp_bd.lstatus = 0;\n\n\ttx_swbd = &tx_ring->tx_swbd[i];\n\ttx_swbd->dma = dma;\n\ttx_swbd->len = len;\n\ttx_swbd->is_dma_page = 0;\n\ttx_swbd->dir = DMA_TO_DEVICE;\n\tcount++;\n\n\tdo_vlan = skb_vlan_tag_present(skb);\n\tif (skb->cb[0] & ENETC_F_TX_ONESTEP_SYNC_TSTAMP) {\n\t\tif (enetc_ptp_parse(skb, &udp, &msgtype, &twostep, &offset1,\n\t\t\t\t    &offset2) ||\n\t\t    msgtype != PTP_MSGTYPE_SYNC || twostep)\n\t\t\tWARN_ONCE(1, \"Bad packet for one-step timestamping\\n\");\n\t\telse\n\t\t\tdo_onestep_tstamp = true;\n\t} else if (skb->cb[0] & ENETC_F_TX_TSTAMP) {\n\t\tdo_twostep_tstamp = true;\n\t}\n\n\ttx_swbd->do_twostep_tstamp = do_twostep_tstamp;\n\ttx_swbd->qbv_en = !!(priv->active_offloads & ENETC_F_QBV);\n\ttx_swbd->check_wb = tx_swbd->do_twostep_tstamp || tx_swbd->qbv_en;\n\n\tif (do_vlan || do_onestep_tstamp || do_twostep_tstamp)\n\t\tflags |= ENETC_TXBD_FLAGS_EX;\n\n\tif (tx_ring->tsd_enable)\n\t\tflags |= ENETC_TXBD_FLAGS_TSE | ENETC_TXBD_FLAGS_TXSTART;\n\n\t \n\ttemp_bd.frm_len = cpu_to_le16(skb->len);\n\ttemp_bd.flags = flags;\n\n\tif (flags & ENETC_TXBD_FLAGS_TSE)\n\t\ttemp_bd.txstart = enetc_txbd_set_tx_start(skb->skb_mstamp_ns,\n\t\t\t\t\t\t\t  flags);\n\n\tif (flags & ENETC_TXBD_FLAGS_EX) {\n\t\tu8 e_flags = 0;\n\t\t*txbd = temp_bd;\n\t\tenetc_clear_tx_bd(&temp_bd);\n\n\t\t \n\t\tflags = 0;\n\t\ttx_swbd++;\n\t\ttxbd++;\n\t\ti++;\n\t\tif (unlikely(i == tx_ring->bd_count)) {\n\t\t\ti = 0;\n\t\t\ttx_swbd = tx_ring->tx_swbd;\n\t\t\ttxbd = ENETC_TXBD(*tx_ring, 0);\n\t\t}\n\t\tprefetchw(txbd);\n\n\t\tif (do_vlan) {\n\t\t\ttemp_bd.ext.vid = cpu_to_le16(skb_vlan_tag_get(skb));\n\t\t\ttemp_bd.ext.tpid = 0;  \n\t\t\te_flags |= ENETC_TXBD_E_FLAGS_VLAN_INS;\n\t\t}\n\n\t\tif (do_onestep_tstamp) {\n\t\t\tu32 lo, hi, val;\n\t\t\tu64 sec, nsec;\n\t\t\tu8 *data;\n\n\t\t\tlo = enetc_rd_hot(hw, ENETC_SICTR0);\n\t\t\thi = enetc_rd_hot(hw, ENETC_SICTR1);\n\t\t\tsec = (u64)hi << 32 | lo;\n\t\t\tnsec = do_div(sec, 1000000000);\n\n\t\t\t \n\t\t\ttemp_bd.ext.tstamp = cpu_to_le32(lo & 0x3fffffff);\n\t\t\te_flags |= ENETC_TXBD_E_FLAGS_ONE_STEP_PTP;\n\n\t\t\t \n\t\t\tdata = skb_mac_header(skb);\n\t\t\t*(__be16 *)(data + offset2) =\n\t\t\t\thtons((sec >> 32) & 0xffff);\n\t\t\t*(__be32 *)(data + offset2 + 2) =\n\t\t\t\thtonl(sec & 0xffffffff);\n\t\t\t*(__be32 *)(data + offset2 + 6) = htonl(nsec);\n\n\t\t\t \n\t\t\tval = ENETC_PM0_SINGLE_STEP_EN;\n\t\t\tval |= ENETC_SET_SINGLE_STEP_OFFSET(offset1);\n\t\t\tif (udp)\n\t\t\t\tval |= ENETC_PM0_SINGLE_STEP_CH;\n\n\t\t\tenetc_port_mac_wr(priv->si, ENETC_PM0_SINGLE_STEP,\n\t\t\t\t\t  val);\n\t\t} else if (do_twostep_tstamp) {\n\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t\te_flags |= ENETC_TXBD_E_FLAGS_TWO_STEP_PTP;\n\t\t}\n\n\t\ttemp_bd.ext.e_flags = e_flags;\n\t\tcount++;\n\t}\n\n\tfrag = &skb_shinfo(skb)->frags[0];\n\tfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++, frag++) {\n\t\tlen = skb_frag_size(frag);\n\t\tdma = skb_frag_dma_map(tx_ring->dev, frag, 0, len,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto dma_err;\n\n\t\t*txbd = temp_bd;\n\t\tenetc_clear_tx_bd(&temp_bd);\n\n\t\tflags = 0;\n\t\ttx_swbd++;\n\t\ttxbd++;\n\t\ti++;\n\t\tif (unlikely(i == tx_ring->bd_count)) {\n\t\t\ti = 0;\n\t\t\ttx_swbd = tx_ring->tx_swbd;\n\t\t\ttxbd = ENETC_TXBD(*tx_ring, 0);\n\t\t}\n\t\tprefetchw(txbd);\n\n\t\ttemp_bd.addr = cpu_to_le64(dma);\n\t\ttemp_bd.buf_len = cpu_to_le16(len);\n\n\t\ttx_swbd->dma = dma;\n\t\ttx_swbd->len = len;\n\t\ttx_swbd->is_dma_page = 1;\n\t\ttx_swbd->dir = DMA_TO_DEVICE;\n\t\tcount++;\n\t}\n\n\t \n\tflags |= ENETC_TXBD_FLAGS_F;\n\ttemp_bd.flags = flags;\n\t*txbd = temp_bd;\n\n\ttx_ring->tx_swbd[i].is_eof = true;\n\ttx_ring->tx_swbd[i].skb = skb;\n\n\tenetc_bdr_idx_inc(tx_ring, &i);\n\ttx_ring->next_to_use = i;\n\n\tskb_tx_timestamp(skb);\n\n\tenetc_update_tx_ring_tail(tx_ring);\n\n\treturn count;\n\ndma_err:\n\tdev_err(tx_ring->dev, \"DMA map error\");\n\n\tdo {\n\t\ttx_swbd = &tx_ring->tx_swbd[i];\n\t\tenetc_free_tx_frame(tx_ring, tx_swbd);\n\t\tif (i == 0)\n\t\t\ti = tx_ring->bd_count;\n\t\ti--;\n\t} while (count--);\n\n\treturn 0;\n}\n\nstatic void enetc_map_tx_tso_hdr(struct enetc_bdr *tx_ring, struct sk_buff *skb,\n\t\t\t\t struct enetc_tx_swbd *tx_swbd,\n\t\t\t\t union enetc_tx_bd *txbd, int *i, int hdr_len,\n\t\t\t\t int data_len)\n{\n\tunion enetc_tx_bd txbd_tmp;\n\tu8 flags = 0, e_flags = 0;\n\tdma_addr_t addr;\n\n\tenetc_clear_tx_bd(&txbd_tmp);\n\taddr = tx_ring->tso_headers_dma + *i * TSO_HEADER_SIZE;\n\n\tif (skb_vlan_tag_present(skb))\n\t\tflags |= ENETC_TXBD_FLAGS_EX;\n\n\ttxbd_tmp.addr = cpu_to_le64(addr);\n\ttxbd_tmp.buf_len = cpu_to_le16(hdr_len);\n\n\t \n\ttxbd_tmp.frm_len = cpu_to_le16(hdr_len + data_len);\n\ttxbd_tmp.flags = flags;\n\n\t \n\ttx_swbd->len = hdr_len;\n\ttx_swbd->do_twostep_tstamp = false;\n\ttx_swbd->check_wb = false;\n\n\t \n\t*txbd = txbd_tmp;\n\n\t \n\tif (flags & ENETC_TXBD_FLAGS_EX) {\n\t\t \n\t\tenetc_bdr_idx_inc(tx_ring, i);\n\t\ttxbd = ENETC_TXBD(*tx_ring, *i);\n\t\ttx_swbd = &tx_ring->tx_swbd[*i];\n\t\tprefetchw(txbd);\n\n\t\t \n\t\tenetc_clear_tx_bd(&txbd_tmp);\n\t\ttxbd_tmp.ext.vid = cpu_to_le16(skb_vlan_tag_get(skb));\n\t\ttxbd_tmp.ext.tpid = 0;  \n\t\te_flags |= ENETC_TXBD_E_FLAGS_VLAN_INS;\n\n\t\t \n\t\ttxbd_tmp.ext.e_flags = e_flags;\n\t\t*txbd = txbd_tmp;\n\t}\n}\n\nstatic int enetc_map_tx_tso_data(struct enetc_bdr *tx_ring, struct sk_buff *skb,\n\t\t\t\t struct enetc_tx_swbd *tx_swbd,\n\t\t\t\t union enetc_tx_bd *txbd, char *data,\n\t\t\t\t int size, bool last_bd)\n{\n\tunion enetc_tx_bd txbd_tmp;\n\tdma_addr_t addr;\n\tu8 flags = 0;\n\n\tenetc_clear_tx_bd(&txbd_tmp);\n\n\taddr = dma_map_single(tx_ring->dev, data, size, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(tx_ring->dev, addr))) {\n\t\tnetdev_err(tx_ring->ndev, \"DMA map error\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (last_bd) {\n\t\tflags |= ENETC_TXBD_FLAGS_F;\n\t\ttx_swbd->is_eof = 1;\n\t}\n\n\ttxbd_tmp.addr = cpu_to_le64(addr);\n\ttxbd_tmp.buf_len = cpu_to_le16(size);\n\ttxbd_tmp.flags = flags;\n\n\ttx_swbd->dma = addr;\n\ttx_swbd->len = size;\n\ttx_swbd->dir = DMA_TO_DEVICE;\n\n\t*txbd = txbd_tmp;\n\n\treturn 0;\n}\n\nstatic __wsum enetc_tso_hdr_csum(struct tso_t *tso, struct sk_buff *skb,\n\t\t\t\t char *hdr, int hdr_len, int *l4_hdr_len)\n{\n\tchar *l4_hdr = hdr + skb_transport_offset(skb);\n\tint mac_hdr_len = skb_network_offset(skb);\n\n\tif (tso->tlen != sizeof(struct udphdr)) {\n\t\tstruct tcphdr *tcph = (struct tcphdr *)(l4_hdr);\n\n\t\ttcph->check = 0;\n\t} else {\n\t\tstruct udphdr *udph = (struct udphdr *)(l4_hdr);\n\n\t\tudph->check = 0;\n\t}\n\n\t \n\tif (!tso->ipv6) {\n\t\tstruct iphdr *iph = (void *)(hdr + mac_hdr_len);\n\n\t\tiph->check = 0;\n\t\tiph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);\n\t}\n\n\t \n\t*l4_hdr_len = hdr_len - skb_transport_offset(skb);\n\treturn csum_partial(l4_hdr, *l4_hdr_len, 0);\n}\n\nstatic void enetc_tso_complete_csum(struct enetc_bdr *tx_ring, struct tso_t *tso,\n\t\t\t\t    struct sk_buff *skb, char *hdr, int len,\n\t\t\t\t    __wsum sum)\n{\n\tchar *l4_hdr = hdr + skb_transport_offset(skb);\n\t__sum16 csum_final;\n\n\t \n\tif (!tso->ipv6)\n\t\tcsum_final = csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t       ip_hdr(skb)->daddr,\n\t\t\t\t\t       len, ip_hdr(skb)->protocol, sum);\n\telse\n\t\tcsum_final = csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t     &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t     len, ipv6_hdr(skb)->nexthdr, sum);\n\n\tif (tso->tlen != sizeof(struct udphdr)) {\n\t\tstruct tcphdr *tcph = (struct tcphdr *)(l4_hdr);\n\n\t\ttcph->check = csum_final;\n\t} else {\n\t\tstruct udphdr *udph = (struct udphdr *)(l4_hdr);\n\n\t\tudph->check = csum_final;\n\t}\n}\n\nstatic int enetc_map_tx_tso_buffs(struct enetc_bdr *tx_ring, struct sk_buff *skb)\n{\n\tint hdr_len, total_len, data_len;\n\tstruct enetc_tx_swbd *tx_swbd;\n\tunion enetc_tx_bd *txbd;\n\tstruct tso_t tso;\n\t__wsum csum, csum2;\n\tint count = 0, pos;\n\tint err, i, bd_data_num;\n\n\t \n\thdr_len = tso_start(skb, &tso);\n\ttotal_len = skb->len - hdr_len;\n\ti = tx_ring->next_to_use;\n\n\twhile (total_len > 0) {\n\t\tchar *hdr;\n\n\t\t \n\t\ttxbd = ENETC_TXBD(*tx_ring, i);\n\t\ttx_swbd = &tx_ring->tx_swbd[i];\n\t\tprefetchw(txbd);\n\n\t\t \n\t\tdata_len = min_t(int, skb_shinfo(skb)->gso_size, total_len);\n\t\ttotal_len -= data_len;\n\n\t\t \n\t\thdr = tx_ring->tso_headers + i * TSO_HEADER_SIZE;\n\t\ttso_build_hdr(skb, hdr, &tso, data_len, total_len == 0);\n\n\t\t \n\t\tcsum = enetc_tso_hdr_csum(&tso, skb, hdr, hdr_len, &pos);\n\t\tenetc_map_tx_tso_hdr(tx_ring, skb, tx_swbd, txbd, &i, hdr_len, data_len);\n\t\tbd_data_num = 0;\n\t\tcount++;\n\n\t\twhile (data_len > 0) {\n\t\t\tint size;\n\n\t\t\tsize = min_t(int, tso.size, data_len);\n\n\t\t\t \n\t\t\tenetc_bdr_idx_inc(tx_ring, &i);\n\t\t\ttxbd = ENETC_TXBD(*tx_ring, i);\n\t\t\ttx_swbd = &tx_ring->tx_swbd[i];\n\t\t\tprefetchw(txbd);\n\n\t\t\t \n\t\t\tcsum2 = csum_partial(tso.data, size, 0);\n\t\t\tcsum = csum_block_add(csum, csum2, pos);\n\t\t\tpos += size;\n\n\t\t\terr = enetc_map_tx_tso_data(tx_ring, skb, tx_swbd, txbd,\n\t\t\t\t\t\t    tso.data, size,\n\t\t\t\t\t\t    size == data_len);\n\t\t\tif (err)\n\t\t\t\tgoto err_map_data;\n\n\t\t\tdata_len -= size;\n\t\t\tcount++;\n\t\t\tbd_data_num++;\n\t\t\ttso_build_data(skb, &tso, size);\n\n\t\t\tif (unlikely(bd_data_num >= ENETC_MAX_SKB_FRAGS && data_len))\n\t\t\t\tgoto err_chained_bd;\n\t\t}\n\n\t\tenetc_tso_complete_csum(tx_ring, &tso, skb, hdr, pos, csum);\n\n\t\tif (total_len == 0)\n\t\t\ttx_swbd->skb = skb;\n\n\t\t \n\t\tenetc_bdr_idx_inc(tx_ring, &i);\n\t}\n\n\ttx_ring->next_to_use = i;\n\tenetc_update_tx_ring_tail(tx_ring);\n\n\treturn count;\n\nerr_map_data:\n\tdev_err(tx_ring->dev, \"DMA map error\");\n\nerr_chained_bd:\n\tdo {\n\t\ttx_swbd = &tx_ring->tx_swbd[i];\n\t\tenetc_free_tx_frame(tx_ring, tx_swbd);\n\t\tif (i == 0)\n\t\t\ti = tx_ring->bd_count;\n\t\ti--;\n\t} while (count--);\n\n\treturn 0;\n}\n\nstatic netdev_tx_t enetc_start_xmit(struct sk_buff *skb,\n\t\t\t\t    struct net_device *ndev)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct enetc_bdr *tx_ring;\n\tint count, err;\n\n\t \n\tif (skb->cb[0] & ENETC_F_TX_ONESTEP_SYNC_TSTAMP) {\n\t\tif (test_and_set_bit_lock(ENETC_TX_ONESTEP_TSTAMP_IN_PROGRESS,\n\t\t\t\t\t  &priv->flags)) {\n\t\t\tskb_queue_tail(&priv->tx_skbs, skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t}\n\n\ttx_ring = priv->tx_ring[skb->queue_mapping];\n\n\tif (skb_is_gso(skb)) {\n\t\tif (enetc_bd_unused(tx_ring) < tso_count_descs(skb)) {\n\t\t\tnetif_stop_subqueue(ndev, tx_ring->index);\n\t\t\treturn NETDEV_TX_BUSY;\n\t\t}\n\n\t\tenetc_lock_mdio();\n\t\tcount = enetc_map_tx_tso_buffs(tx_ring, skb);\n\t\tenetc_unlock_mdio();\n\t} else {\n\t\tif (unlikely(skb_shinfo(skb)->nr_frags > ENETC_MAX_SKB_FRAGS))\n\t\t\tif (unlikely(skb_linearize(skb)))\n\t\t\t\tgoto drop_packet_err;\n\n\t\tcount = skb_shinfo(skb)->nr_frags + 1;  \n\t\tif (enetc_bd_unused(tx_ring) < ENETC_TXBDS_NEEDED(count)) {\n\t\t\tnetif_stop_subqueue(ndev, tx_ring->index);\n\t\t\treturn NETDEV_TX_BUSY;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\terr = skb_checksum_help(skb);\n\t\t\tif (err)\n\t\t\t\tgoto drop_packet_err;\n\t\t}\n\t\tenetc_lock_mdio();\n\t\tcount = enetc_map_tx_buffs(tx_ring, skb);\n\t\tenetc_unlock_mdio();\n\t}\n\n\tif (unlikely(!count))\n\t\tgoto drop_packet_err;\n\n\tif (enetc_bd_unused(tx_ring) < ENETC_TXBDS_MAX_NEEDED)\n\t\tnetif_stop_subqueue(ndev, tx_ring->index);\n\n\treturn NETDEV_TX_OK;\n\ndrop_packet_err:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\nnetdev_tx_t enetc_xmit(struct sk_buff *skb, struct net_device *ndev)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tu8 udp, msgtype, twostep;\n\tu16 offset1, offset2;\n\n\t \n\tif ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\n\t    (priv->active_offloads & ENETC_F_TX_TSTAMP_MASK)) {\n\t\tskb->cb[0] = priv->active_offloads & ENETC_F_TX_TSTAMP_MASK;\n\t} else {\n\t\tskb->cb[0] = 0;\n\t}\n\n\t \n\tif (skb->cb[0] & ENETC_F_TX_ONESTEP_SYNC_TSTAMP) {\n\t\tif (enetc_ptp_parse(skb, &udp, &msgtype, &twostep,\n\t\t\t\t    &offset1, &offset2) ||\n\t\t    msgtype != PTP_MSGTYPE_SYNC || twostep != 0)\n\t\t\tskb->cb[0] = ENETC_F_TX_TSTAMP;\n\t}\n\n\treturn enetc_start_xmit(skb, ndev);\n}\nEXPORT_SYMBOL_GPL(enetc_xmit);\n\nstatic irqreturn_t enetc_msix(int irq, void *data)\n{\n\tstruct enetc_int_vector\t*v = data;\n\tint i;\n\n\tenetc_lock_mdio();\n\n\t \n\tenetc_wr_reg_hot(v->rbier, 0);\n\tenetc_wr_reg_hot(v->ricr1, v->rx_ictt);\n\n\tfor_each_set_bit(i, &v->tx_rings_map, ENETC_MAX_NUM_TXQS)\n\t\tenetc_wr_reg_hot(v->tbier_base + ENETC_BDR_OFF(i), 0);\n\n\tenetc_unlock_mdio();\n\n\tnapi_schedule(&v->napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void enetc_rx_dim_work(struct work_struct *w)\n{\n\tstruct dim *dim = container_of(w, struct dim, work);\n\tstruct dim_cq_moder moder =\n\t\tnet_dim_get_rx_moderation(dim->mode, dim->profile_ix);\n\tstruct enetc_int_vector\t*v =\n\t\tcontainer_of(dim, struct enetc_int_vector, rx_dim);\n\n\tv->rx_ictt = enetc_usecs_to_cycles(moder.usec);\n\tdim->state = DIM_START_MEASURE;\n}\n\nstatic void enetc_rx_net_dim(struct enetc_int_vector *v)\n{\n\tstruct dim_sample dim_sample = {};\n\n\tv->comp_cnt++;\n\n\tif (!v->rx_napi_work)\n\t\treturn;\n\n\tdim_update_sample(v->comp_cnt,\n\t\t\t  v->rx_ring.stats.packets,\n\t\t\t  v->rx_ring.stats.bytes,\n\t\t\t  &dim_sample);\n\tnet_dim(&v->rx_dim, dim_sample);\n}\n\nstatic int enetc_bd_ready_count(struct enetc_bdr *tx_ring, int ci)\n{\n\tint pi = enetc_rd_reg_hot(tx_ring->tcir) & ENETC_TBCIR_IDX_MASK;\n\n\treturn pi >= ci ? pi - ci : tx_ring->bd_count - ci + pi;\n}\n\nstatic bool enetc_page_reusable(struct page *page)\n{\n\treturn (!page_is_pfmemalloc(page) && page_ref_count(page) == 1);\n}\n\nstatic void enetc_reuse_page(struct enetc_bdr *rx_ring,\n\t\t\t     struct enetc_rx_swbd *old)\n{\n\tstruct enetc_rx_swbd *new;\n\n\tnew = &rx_ring->rx_swbd[rx_ring->next_to_alloc];\n\n\t \n\tenetc_bdr_idx_inc(rx_ring, &rx_ring->next_to_alloc);\n\n\t \n\t*new = *old;\n}\n\nstatic void enetc_get_tx_tstamp(struct enetc_hw *hw, union enetc_tx_bd *txbd,\n\t\t\t\tu64 *tstamp)\n{\n\tu32 lo, hi, tstamp_lo;\n\n\tlo = enetc_rd_hot(hw, ENETC_SICTR0);\n\thi = enetc_rd_hot(hw, ENETC_SICTR1);\n\ttstamp_lo = le32_to_cpu(txbd->wb.tstamp);\n\tif (lo <= tstamp_lo)\n\t\thi -= 1;\n\t*tstamp = (u64)hi << 32 | tstamp_lo;\n}\n\nstatic void enetc_tstamp_tx(struct sk_buff *skb, u64 tstamp)\n{\n\tstruct skb_shared_hwtstamps shhwtstamps;\n\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS) {\n\t\tmemset(&shhwtstamps, 0, sizeof(shhwtstamps));\n\t\tshhwtstamps.hwtstamp = ns_to_ktime(tstamp);\n\t\tskb_txtime_consumed(skb);\n\t\tskb_tstamp_tx(skb, &shhwtstamps);\n\t}\n}\n\nstatic void enetc_recycle_xdp_tx_buff(struct enetc_bdr *tx_ring,\n\t\t\t\t      struct enetc_tx_swbd *tx_swbd)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(tx_ring->ndev);\n\tstruct enetc_rx_swbd rx_swbd = {\n\t\t.dma = tx_swbd->dma,\n\t\t.page = tx_swbd->page,\n\t\t.page_offset = tx_swbd->page_offset,\n\t\t.dir = tx_swbd->dir,\n\t\t.len = tx_swbd->len,\n\t};\n\tstruct enetc_bdr *rx_ring;\n\n\trx_ring = enetc_rx_ring_from_xdp_tx_ring(priv, tx_ring);\n\n\tif (likely(enetc_swbd_unused(rx_ring))) {\n\t\tenetc_reuse_page(rx_ring, &rx_swbd);\n\n\t\t \n\t\tdma_sync_single_range_for_device(rx_ring->dev, rx_swbd.dma,\n\t\t\t\t\t\t rx_swbd.page_offset,\n\t\t\t\t\t\t ENETC_RXB_DMA_SIZE_XDP,\n\t\t\t\t\t\t rx_swbd.dir);\n\n\t\trx_ring->stats.recycles++;\n\t} else {\n\t\t \n\t\trx_ring->stats.recycle_failures++;\n\n\t\tdma_unmap_page(rx_ring->dev, rx_swbd.dma, PAGE_SIZE,\n\t\t\t       rx_swbd.dir);\n\t\t__free_page(rx_swbd.page);\n\t}\n\n\trx_ring->xdp.xdp_tx_in_flight--;\n}\n\nstatic bool enetc_clean_tx_ring(struct enetc_bdr *tx_ring, int napi_budget)\n{\n\tint tx_frm_cnt = 0, tx_byte_cnt = 0, tx_win_drop = 0;\n\tstruct net_device *ndev = tx_ring->ndev;\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct enetc_tx_swbd *tx_swbd;\n\tint i, bds_to_clean;\n\tbool do_twostep_tstamp;\n\tu64 tstamp = 0;\n\n\ti = tx_ring->next_to_clean;\n\ttx_swbd = &tx_ring->tx_swbd[i];\n\n\tbds_to_clean = enetc_bd_ready_count(tx_ring, i);\n\n\tdo_twostep_tstamp = false;\n\n\twhile (bds_to_clean && tx_frm_cnt < ENETC_DEFAULT_TX_WORK) {\n\t\tstruct xdp_frame *xdp_frame = enetc_tx_swbd_get_xdp_frame(tx_swbd);\n\t\tstruct sk_buff *skb = enetc_tx_swbd_get_skb(tx_swbd);\n\t\tbool is_eof = tx_swbd->is_eof;\n\n\t\tif (unlikely(tx_swbd->check_wb)) {\n\t\t\tunion enetc_tx_bd *txbd = ENETC_TXBD(*tx_ring, i);\n\n\t\t\tif (txbd->flags & ENETC_TXBD_FLAGS_W &&\n\t\t\t    tx_swbd->do_twostep_tstamp) {\n\t\t\t\tenetc_get_tx_tstamp(&priv->si->hw, txbd,\n\t\t\t\t\t\t    &tstamp);\n\t\t\t\tdo_twostep_tstamp = true;\n\t\t\t}\n\n\t\t\tif (tx_swbd->qbv_en &&\n\t\t\t    txbd->wb.status & ENETC_TXBD_STATS_WIN)\n\t\t\t\ttx_win_drop++;\n\t\t}\n\n\t\tif (tx_swbd->is_xdp_tx)\n\t\t\tenetc_recycle_xdp_tx_buff(tx_ring, tx_swbd);\n\t\telse if (likely(tx_swbd->dma))\n\t\t\tenetc_unmap_tx_buff(tx_ring, tx_swbd);\n\n\t\tif (xdp_frame) {\n\t\t\txdp_return_frame(xdp_frame);\n\t\t} else if (skb) {\n\t\t\tif (unlikely(skb->cb[0] & ENETC_F_TX_ONESTEP_SYNC_TSTAMP)) {\n\t\t\t\t \n\t\t\t\tschedule_work(&priv->tx_onestep_tstamp);\n\t\t\t} else if (unlikely(do_twostep_tstamp)) {\n\t\t\t\tenetc_tstamp_tx(skb, tstamp);\n\t\t\t\tdo_twostep_tstamp = false;\n\t\t\t}\n\t\t\tnapi_consume_skb(skb, napi_budget);\n\t\t}\n\n\t\ttx_byte_cnt += tx_swbd->len;\n\t\t \n\t\tmemset(tx_swbd, 0, sizeof(*tx_swbd));\n\n\t\tbds_to_clean--;\n\t\ttx_swbd++;\n\t\ti++;\n\t\tif (unlikely(i == tx_ring->bd_count)) {\n\t\t\ti = 0;\n\t\t\ttx_swbd = tx_ring->tx_swbd;\n\t\t}\n\n\t\t \n\t\tif (is_eof) {\n\t\t\ttx_frm_cnt++;\n\t\t\t \n\t\t\tenetc_wr_reg_hot(tx_ring->idr, BIT(tx_ring->index) |\n\t\t\t\t\t BIT(16 + tx_ring->index));\n\t\t}\n\n\t\tif (unlikely(!bds_to_clean))\n\t\t\tbds_to_clean = enetc_bd_ready_count(tx_ring, i);\n\t}\n\n\ttx_ring->next_to_clean = i;\n\ttx_ring->stats.packets += tx_frm_cnt;\n\ttx_ring->stats.bytes += tx_byte_cnt;\n\ttx_ring->stats.win_drop += tx_win_drop;\n\n\tif (unlikely(tx_frm_cnt && netif_carrier_ok(ndev) &&\n\t\t     __netif_subqueue_stopped(ndev, tx_ring->index) &&\n\t\t     (enetc_bd_unused(tx_ring) >= ENETC_TXBDS_MAX_NEEDED))) {\n\t\tnetif_wake_subqueue(ndev, tx_ring->index);\n\t}\n\n\treturn tx_frm_cnt != ENETC_DEFAULT_TX_WORK;\n}\n\nstatic bool enetc_new_page(struct enetc_bdr *rx_ring,\n\t\t\t   struct enetc_rx_swbd *rx_swbd)\n{\n\tbool xdp = !!(rx_ring->xdp.prog);\n\tstruct page *page;\n\tdma_addr_t addr;\n\n\tpage = dev_alloc_page();\n\tif (unlikely(!page))\n\t\treturn false;\n\n\t \n\trx_swbd->dir = xdp ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;\n\n\taddr = dma_map_page(rx_ring->dev, page, 0, PAGE_SIZE, rx_swbd->dir);\n\tif (unlikely(dma_mapping_error(rx_ring->dev, addr))) {\n\t\t__free_page(page);\n\n\t\treturn false;\n\t}\n\n\trx_swbd->dma = addr;\n\trx_swbd->page = page;\n\trx_swbd->page_offset = rx_ring->buffer_offset;\n\n\treturn true;\n}\n\nstatic int enetc_refill_rx_ring(struct enetc_bdr *rx_ring, const int buff_cnt)\n{\n\tstruct enetc_rx_swbd *rx_swbd;\n\tunion enetc_rx_bd *rxbd;\n\tint i, j;\n\n\ti = rx_ring->next_to_use;\n\trx_swbd = &rx_ring->rx_swbd[i];\n\trxbd = enetc_rxbd(rx_ring, i);\n\n\tfor (j = 0; j < buff_cnt; j++) {\n\t\t \n\t\tif (unlikely(!rx_swbd->page)) {\n\t\t\tif (unlikely(!enetc_new_page(rx_ring, rx_swbd))) {\n\t\t\t\trx_ring->stats.rx_alloc_errs++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\trxbd->w.addr = cpu_to_le64(rx_swbd->dma +\n\t\t\t\t\t   rx_swbd->page_offset);\n\t\t \n\t\trxbd->r.lstatus = 0;\n\n\t\tenetc_rxbd_next(rx_ring, &rxbd, &i);\n\t\trx_swbd = &rx_ring->rx_swbd[i];\n\t}\n\n\tif (likely(j)) {\n\t\trx_ring->next_to_alloc = i;  \n\t\trx_ring->next_to_use = i;\n\n\t\t \n\t\tenetc_wr_reg_hot(rx_ring->rcir, rx_ring->next_to_use);\n\t}\n\n\treturn j;\n}\n\n#ifdef CONFIG_FSL_ENETC_PTP_CLOCK\nstatic void enetc_get_rx_tstamp(struct net_device *ndev,\n\t\t\t\tunion enetc_rx_bd *rxbd,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tu32 lo, hi, tstamp_lo;\n\tu64 tstamp;\n\n\tif (le16_to_cpu(rxbd->r.flags) & ENETC_RXBD_FLAG_TSTMP) {\n\t\tlo = enetc_rd_reg_hot(hw->reg + ENETC_SICTR0);\n\t\thi = enetc_rd_reg_hot(hw->reg + ENETC_SICTR1);\n\t\trxbd = enetc_rxbd_ext(rxbd);\n\t\ttstamp_lo = le32_to_cpu(rxbd->ext.tstamp);\n\t\tif (lo <= tstamp_lo)\n\t\t\thi -= 1;\n\n\t\ttstamp = (u64)hi << 32 | tstamp_lo;\n\t\tmemset(shhwtstamps, 0, sizeof(*shhwtstamps));\n\t\tshhwtstamps->hwtstamp = ns_to_ktime(tstamp);\n\t}\n}\n#endif\n\nstatic void enetc_get_offloads(struct enetc_bdr *rx_ring,\n\t\t\t       union enetc_rx_bd *rxbd, struct sk_buff *skb)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(rx_ring->ndev);\n\n\t \n\tif (rx_ring->ndev->features & NETIF_F_RXCSUM) {\n\t\tu16 inet_csum = le16_to_cpu(rxbd->r.inet_csum);\n\n\t\tskb->csum = csum_unfold((__force __sum16)~htons(inet_csum));\n\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t}\n\n\tif (le16_to_cpu(rxbd->r.flags) & ENETC_RXBD_FLAG_VLAN) {\n\t\t__be16 tpid = 0;\n\n\t\tswitch (le16_to_cpu(rxbd->r.flags) & ENETC_RXBD_FLAG_TPID) {\n\t\tcase 0:\n\t\t\ttpid = htons(ETH_P_8021Q);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\ttpid = htons(ETH_P_8021AD);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\ttpid = htons(enetc_port_rd(&priv->si->hw,\n\t\t\t\t\t\t   ENETC_PCVLANR1));\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\ttpid = htons(enetc_port_rd(&priv->si->hw,\n\t\t\t\t\t\t   ENETC_PCVLANR2));\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\t__vlan_hwaccel_put_tag(skb, tpid, le16_to_cpu(rxbd->r.vlan_opt));\n\t}\n\n#ifdef CONFIG_FSL_ENETC_PTP_CLOCK\n\tif (priv->active_offloads & ENETC_F_RX_TSTAMP)\n\t\tenetc_get_rx_tstamp(rx_ring->ndev, rxbd, skb);\n#endif\n}\n\n \nstatic struct enetc_rx_swbd *enetc_get_rx_buff(struct enetc_bdr *rx_ring,\n\t\t\t\t\t       int i, u16 size)\n{\n\tstruct enetc_rx_swbd *rx_swbd = &rx_ring->rx_swbd[i];\n\n\tdma_sync_single_range_for_cpu(rx_ring->dev, rx_swbd->dma,\n\t\t\t\t      rx_swbd->page_offset,\n\t\t\t\t      size, rx_swbd->dir);\n\treturn rx_swbd;\n}\n\n \nstatic void enetc_put_rx_buff(struct enetc_bdr *rx_ring,\n\t\t\t      struct enetc_rx_swbd *rx_swbd)\n{\n\tsize_t buffer_size = ENETC_RXB_TRUESIZE - rx_ring->buffer_offset;\n\n\tenetc_reuse_page(rx_ring, rx_swbd);\n\n\tdma_sync_single_range_for_device(rx_ring->dev, rx_swbd->dma,\n\t\t\t\t\t rx_swbd->page_offset,\n\t\t\t\t\t buffer_size, rx_swbd->dir);\n\n\trx_swbd->page = NULL;\n}\n\n \nstatic void enetc_flip_rx_buff(struct enetc_bdr *rx_ring,\n\t\t\t       struct enetc_rx_swbd *rx_swbd)\n{\n\tif (likely(enetc_page_reusable(rx_swbd->page))) {\n\t\trx_swbd->page_offset ^= ENETC_RXB_TRUESIZE;\n\t\tpage_ref_inc(rx_swbd->page);\n\n\t\tenetc_put_rx_buff(rx_ring, rx_swbd);\n\t} else {\n\t\tdma_unmap_page(rx_ring->dev, rx_swbd->dma, PAGE_SIZE,\n\t\t\t       rx_swbd->dir);\n\t\trx_swbd->page = NULL;\n\t}\n}\n\nstatic struct sk_buff *enetc_map_rx_buff_to_skb(struct enetc_bdr *rx_ring,\n\t\t\t\t\t\tint i, u16 size)\n{\n\tstruct enetc_rx_swbd *rx_swbd = enetc_get_rx_buff(rx_ring, i, size);\n\tstruct sk_buff *skb;\n\tvoid *ba;\n\n\tba = page_address(rx_swbd->page) + rx_swbd->page_offset;\n\tskb = build_skb(ba - rx_ring->buffer_offset, ENETC_RXB_TRUESIZE);\n\tif (unlikely(!skb)) {\n\t\trx_ring->stats.rx_alloc_errs++;\n\t\treturn NULL;\n\t}\n\n\tskb_reserve(skb, rx_ring->buffer_offset);\n\t__skb_put(skb, size);\n\n\tenetc_flip_rx_buff(rx_ring, rx_swbd);\n\n\treturn skb;\n}\n\nstatic void enetc_add_rx_buff_to_skb(struct enetc_bdr *rx_ring, int i,\n\t\t\t\t     u16 size, struct sk_buff *skb)\n{\n\tstruct enetc_rx_swbd *rx_swbd = enetc_get_rx_buff(rx_ring, i, size);\n\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_swbd->page,\n\t\t\trx_swbd->page_offset, size, ENETC_RXB_TRUESIZE);\n\n\tenetc_flip_rx_buff(rx_ring, rx_swbd);\n}\n\nstatic bool enetc_check_bd_errors_and_consume(struct enetc_bdr *rx_ring,\n\t\t\t\t\t      u32 bd_status,\n\t\t\t\t\t      union enetc_rx_bd **rxbd, int *i)\n{\n\tif (likely(!(bd_status & ENETC_RXBD_LSTATUS(ENETC_RXBD_ERR_MASK))))\n\t\treturn false;\n\n\tenetc_put_rx_buff(rx_ring, &rx_ring->rx_swbd[*i]);\n\tenetc_rxbd_next(rx_ring, rxbd, i);\n\n\twhile (!(bd_status & ENETC_RXBD_LSTATUS_F)) {\n\t\tdma_rmb();\n\t\tbd_status = le32_to_cpu((*rxbd)->r.lstatus);\n\n\t\tenetc_put_rx_buff(rx_ring, &rx_ring->rx_swbd[*i]);\n\t\tenetc_rxbd_next(rx_ring, rxbd, i);\n\t}\n\n\trx_ring->ndev->stats.rx_dropped++;\n\trx_ring->ndev->stats.rx_errors++;\n\n\treturn true;\n}\n\nstatic struct sk_buff *enetc_build_skb(struct enetc_bdr *rx_ring,\n\t\t\t\t       u32 bd_status, union enetc_rx_bd **rxbd,\n\t\t\t\t       int *i, int *cleaned_cnt, int buffer_size)\n{\n\tstruct sk_buff *skb;\n\tu16 size;\n\n\tsize = le16_to_cpu((*rxbd)->r.buf_len);\n\tskb = enetc_map_rx_buff_to_skb(rx_ring, *i, size);\n\tif (!skb)\n\t\treturn NULL;\n\n\tenetc_get_offloads(rx_ring, *rxbd, skb);\n\n\t(*cleaned_cnt)++;\n\n\tenetc_rxbd_next(rx_ring, rxbd, i);\n\n\t \n\twhile (!(bd_status & ENETC_RXBD_LSTATUS_F)) {\n\t\tbd_status = le32_to_cpu((*rxbd)->r.lstatus);\n\t\tsize = buffer_size;\n\n\t\tif (bd_status & ENETC_RXBD_LSTATUS_F) {\n\t\t\tdma_rmb();\n\t\t\tsize = le16_to_cpu((*rxbd)->r.buf_len);\n\t\t}\n\n\t\tenetc_add_rx_buff_to_skb(rx_ring, *i, size, skb);\n\n\t\t(*cleaned_cnt)++;\n\n\t\tenetc_rxbd_next(rx_ring, rxbd, i);\n\t}\n\n\tskb_record_rx_queue(skb, rx_ring->index);\n\tskb->protocol = eth_type_trans(skb, rx_ring->ndev);\n\n\treturn skb;\n}\n\n#define ENETC_RXBD_BUNDLE 16  \n\nstatic int enetc_clean_rx_ring(struct enetc_bdr *rx_ring,\n\t\t\t       struct napi_struct *napi, int work_limit)\n{\n\tint rx_frm_cnt = 0, rx_byte_cnt = 0;\n\tint cleaned_cnt, i;\n\n\tcleaned_cnt = enetc_bd_unused(rx_ring);\n\t \n\ti = rx_ring->next_to_clean;\n\n\twhile (likely(rx_frm_cnt < work_limit)) {\n\t\tunion enetc_rx_bd *rxbd;\n\t\tstruct sk_buff *skb;\n\t\tu32 bd_status;\n\n\t\tif (cleaned_cnt >= ENETC_RXBD_BUNDLE)\n\t\t\tcleaned_cnt -= enetc_refill_rx_ring(rx_ring,\n\t\t\t\t\t\t\t    cleaned_cnt);\n\n\t\trxbd = enetc_rxbd(rx_ring, i);\n\t\tbd_status = le32_to_cpu(rxbd->r.lstatus);\n\t\tif (!bd_status)\n\t\t\tbreak;\n\n\t\tenetc_wr_reg_hot(rx_ring->idr, BIT(rx_ring->index));\n\t\tdma_rmb();  \n\n\t\tif (enetc_check_bd_errors_and_consume(rx_ring, bd_status,\n\t\t\t\t\t\t      &rxbd, &i))\n\t\t\tbreak;\n\n\t\tskb = enetc_build_skb(rx_ring, bd_status, &rxbd, &i,\n\t\t\t\t      &cleaned_cnt, ENETC_RXB_DMA_SIZE);\n\t\tif (!skb)\n\t\t\tbreak;\n\n\t\t \n\t\tif (bd_status & ENETC_RXBD_FLAG_VLAN)\n\t\t\trx_byte_cnt += VLAN_HLEN;\n\t\trx_byte_cnt += skb->len + ETH_HLEN;\n\t\trx_frm_cnt++;\n\n\t\tnapi_gro_receive(napi, skb);\n\t}\n\n\trx_ring->next_to_clean = i;\n\n\trx_ring->stats.packets += rx_frm_cnt;\n\trx_ring->stats.bytes += rx_byte_cnt;\n\n\treturn rx_frm_cnt;\n}\n\nstatic void enetc_xdp_map_tx_buff(struct enetc_bdr *tx_ring, int i,\n\t\t\t\t  struct enetc_tx_swbd *tx_swbd,\n\t\t\t\t  int frm_len)\n{\n\tunion enetc_tx_bd *txbd = ENETC_TXBD(*tx_ring, i);\n\n\tprefetchw(txbd);\n\n\tenetc_clear_tx_bd(txbd);\n\ttxbd->addr = cpu_to_le64(tx_swbd->dma + tx_swbd->page_offset);\n\ttxbd->buf_len = cpu_to_le16(tx_swbd->len);\n\ttxbd->frm_len = cpu_to_le16(frm_len);\n\n\tmemcpy(&tx_ring->tx_swbd[i], tx_swbd, sizeof(*tx_swbd));\n}\n\n \nstatic bool enetc_xdp_tx(struct enetc_bdr *tx_ring,\n\t\t\t struct enetc_tx_swbd *xdp_tx_arr, int num_tx_swbd)\n{\n\tstruct enetc_tx_swbd *tmp_tx_swbd = xdp_tx_arr;\n\tint i, k, frm_len = tmp_tx_swbd->len;\n\n\tif (unlikely(enetc_bd_unused(tx_ring) < ENETC_TXBDS_NEEDED(num_tx_swbd)))\n\t\treturn false;\n\n\twhile (unlikely(!tmp_tx_swbd->is_eof)) {\n\t\ttmp_tx_swbd++;\n\t\tfrm_len += tmp_tx_swbd->len;\n\t}\n\n\ti = tx_ring->next_to_use;\n\n\tfor (k = 0; k < num_tx_swbd; k++) {\n\t\tstruct enetc_tx_swbd *xdp_tx_swbd = &xdp_tx_arr[k];\n\n\t\tenetc_xdp_map_tx_buff(tx_ring, i, xdp_tx_swbd, frm_len);\n\n\t\t \n\t\tif (xdp_tx_swbd->is_eof) {\n\t\t\tunion enetc_tx_bd *txbd = ENETC_TXBD(*tx_ring, i);\n\n\t\t\ttxbd->flags = ENETC_TXBD_FLAGS_F;\n\t\t}\n\n\t\tenetc_bdr_idx_inc(tx_ring, &i);\n\t}\n\n\ttx_ring->next_to_use = i;\n\n\treturn true;\n}\n\nstatic int enetc_xdp_frame_to_xdp_tx_swbd(struct enetc_bdr *tx_ring,\n\t\t\t\t\t  struct enetc_tx_swbd *xdp_tx_arr,\n\t\t\t\t\t  struct xdp_frame *xdp_frame)\n{\n\tstruct enetc_tx_swbd *xdp_tx_swbd = &xdp_tx_arr[0];\n\tstruct skb_shared_info *shinfo;\n\tvoid *data = xdp_frame->data;\n\tint len = xdp_frame->len;\n\tskb_frag_t *frag;\n\tdma_addr_t dma;\n\tunsigned int f;\n\tint n = 0;\n\n\tdma = dma_map_single(tx_ring->dev, data, len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(tx_ring->dev, dma))) {\n\t\tnetdev_err(tx_ring->ndev, \"DMA map error\\n\");\n\t\treturn -1;\n\t}\n\n\txdp_tx_swbd->dma = dma;\n\txdp_tx_swbd->dir = DMA_TO_DEVICE;\n\txdp_tx_swbd->len = len;\n\txdp_tx_swbd->is_xdp_redirect = true;\n\txdp_tx_swbd->is_eof = false;\n\txdp_tx_swbd->xdp_frame = NULL;\n\n\tn++;\n\n\tif (!xdp_frame_has_frags(xdp_frame))\n\t\tgoto out;\n\n\txdp_tx_swbd = &xdp_tx_arr[n];\n\n\tshinfo = xdp_get_shared_info_from_frame(xdp_frame);\n\n\tfor (f = 0, frag = &shinfo->frags[0]; f < shinfo->nr_frags;\n\t     f++, frag++) {\n\t\tdata = skb_frag_address(frag);\n\t\tlen = skb_frag_size(frag);\n\n\t\tdma = dma_map_single(tx_ring->dev, data, len, DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(tx_ring->dev, dma))) {\n\t\t\t \n\t\t\twhile (--n >= 0)\n\t\t\t\tenetc_unmap_tx_buff(tx_ring, &xdp_tx_arr[n]);\n\n\t\t\tnetdev_err(tx_ring->ndev, \"DMA map error\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\txdp_tx_swbd->dma = dma;\n\t\txdp_tx_swbd->dir = DMA_TO_DEVICE;\n\t\txdp_tx_swbd->len = len;\n\t\txdp_tx_swbd->is_xdp_redirect = true;\n\t\txdp_tx_swbd->is_eof = false;\n\t\txdp_tx_swbd->xdp_frame = NULL;\n\n\t\tn++;\n\t\txdp_tx_swbd = &xdp_tx_arr[n];\n\t}\nout:\n\txdp_tx_arr[n - 1].is_eof = true;\n\txdp_tx_arr[n - 1].xdp_frame = xdp_frame;\n\n\treturn n;\n}\n\nint enetc_xdp_xmit(struct net_device *ndev, int num_frames,\n\t\t   struct xdp_frame **frames, u32 flags)\n{\n\tstruct enetc_tx_swbd xdp_redirect_arr[ENETC_MAX_SKB_FRAGS] = {0};\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct enetc_bdr *tx_ring;\n\tint xdp_tx_bd_cnt, i, k;\n\tint xdp_tx_frm_cnt = 0;\n\n\tenetc_lock_mdio();\n\n\ttx_ring = priv->xdp_tx_ring[smp_processor_id()];\n\n\tprefetchw(ENETC_TXBD(*tx_ring, tx_ring->next_to_use));\n\n\tfor (k = 0; k < num_frames; k++) {\n\t\txdp_tx_bd_cnt = enetc_xdp_frame_to_xdp_tx_swbd(tx_ring,\n\t\t\t\t\t\t\t       xdp_redirect_arr,\n\t\t\t\t\t\t\t       frames[k]);\n\t\tif (unlikely(xdp_tx_bd_cnt < 0))\n\t\t\tbreak;\n\n\t\tif (unlikely(!enetc_xdp_tx(tx_ring, xdp_redirect_arr,\n\t\t\t\t\t   xdp_tx_bd_cnt))) {\n\t\t\tfor (i = 0; i < xdp_tx_bd_cnt; i++)\n\t\t\t\tenetc_unmap_tx_buff(tx_ring,\n\t\t\t\t\t\t    &xdp_redirect_arr[i]);\n\t\t\ttx_ring->stats.xdp_tx_drops++;\n\t\t\tbreak;\n\t\t}\n\n\t\txdp_tx_frm_cnt++;\n\t}\n\n\tif (unlikely((flags & XDP_XMIT_FLUSH) || k != xdp_tx_frm_cnt))\n\t\tenetc_update_tx_ring_tail(tx_ring);\n\n\ttx_ring->stats.xdp_tx += xdp_tx_frm_cnt;\n\n\tenetc_unlock_mdio();\n\n\treturn xdp_tx_frm_cnt;\n}\nEXPORT_SYMBOL_GPL(enetc_xdp_xmit);\n\nstatic void enetc_map_rx_buff_to_xdp(struct enetc_bdr *rx_ring, int i,\n\t\t\t\t     struct xdp_buff *xdp_buff, u16 size)\n{\n\tstruct enetc_rx_swbd *rx_swbd = enetc_get_rx_buff(rx_ring, i, size);\n\tvoid *hard_start = page_address(rx_swbd->page) + rx_swbd->page_offset;\n\n\t \n\trx_swbd->len = size;\n\n\txdp_prepare_buff(xdp_buff, hard_start - rx_ring->buffer_offset,\n\t\t\t rx_ring->buffer_offset, size, false);\n}\n\nstatic void enetc_add_rx_buff_to_xdp(struct enetc_bdr *rx_ring, int i,\n\t\t\t\t     u16 size, struct xdp_buff *xdp_buff)\n{\n\tstruct skb_shared_info *shinfo = xdp_get_shared_info_from_buff(xdp_buff);\n\tstruct enetc_rx_swbd *rx_swbd = enetc_get_rx_buff(rx_ring, i, size);\n\tskb_frag_t *frag;\n\n\t \n\trx_swbd->len = size;\n\n\tif (!xdp_buff_has_frags(xdp_buff)) {\n\t\txdp_buff_set_frags_flag(xdp_buff);\n\t\tshinfo->xdp_frags_size = size;\n\t\tshinfo->nr_frags = 0;\n\t} else {\n\t\tshinfo->xdp_frags_size += size;\n\t}\n\n\tif (page_is_pfmemalloc(rx_swbd->page))\n\t\txdp_buff_set_frag_pfmemalloc(xdp_buff);\n\n\tfrag = &shinfo->frags[shinfo->nr_frags];\n\tskb_frag_fill_page_desc(frag, rx_swbd->page, rx_swbd->page_offset,\n\t\t\t\tsize);\n\n\tshinfo->nr_frags++;\n}\n\nstatic void enetc_build_xdp_buff(struct enetc_bdr *rx_ring, u32 bd_status,\n\t\t\t\t union enetc_rx_bd **rxbd, int *i,\n\t\t\t\t int *cleaned_cnt, struct xdp_buff *xdp_buff)\n{\n\tu16 size = le16_to_cpu((*rxbd)->r.buf_len);\n\n\txdp_init_buff(xdp_buff, ENETC_RXB_TRUESIZE, &rx_ring->xdp.rxq);\n\n\tenetc_map_rx_buff_to_xdp(rx_ring, *i, xdp_buff, size);\n\t(*cleaned_cnt)++;\n\tenetc_rxbd_next(rx_ring, rxbd, i);\n\n\t \n\twhile (!(bd_status & ENETC_RXBD_LSTATUS_F)) {\n\t\tbd_status = le32_to_cpu((*rxbd)->r.lstatus);\n\t\tsize = ENETC_RXB_DMA_SIZE_XDP;\n\n\t\tif (bd_status & ENETC_RXBD_LSTATUS_F) {\n\t\t\tdma_rmb();\n\t\t\tsize = le16_to_cpu((*rxbd)->r.buf_len);\n\t\t}\n\n\t\tenetc_add_rx_buff_to_xdp(rx_ring, *i, size, xdp_buff);\n\t\t(*cleaned_cnt)++;\n\t\tenetc_rxbd_next(rx_ring, rxbd, i);\n\t}\n}\n\n \nstatic int enetc_rx_swbd_to_xdp_tx_swbd(struct enetc_tx_swbd *xdp_tx_arr,\n\t\t\t\t\tstruct enetc_bdr *rx_ring,\n\t\t\t\t\tint rx_ring_first, int rx_ring_last)\n{\n\tint n = 0;\n\n\tfor (; rx_ring_first != rx_ring_last;\n\t     n++, enetc_bdr_idx_inc(rx_ring, &rx_ring_first)) {\n\t\tstruct enetc_rx_swbd *rx_swbd = &rx_ring->rx_swbd[rx_ring_first];\n\t\tstruct enetc_tx_swbd *tx_swbd = &xdp_tx_arr[n];\n\n\t\t \n\t\ttx_swbd->dma = rx_swbd->dma;\n\t\ttx_swbd->dir = rx_swbd->dir;\n\t\ttx_swbd->page = rx_swbd->page;\n\t\ttx_swbd->page_offset = rx_swbd->page_offset;\n\t\ttx_swbd->len = rx_swbd->len;\n\t\ttx_swbd->is_dma_page = true;\n\t\ttx_swbd->is_xdp_tx = true;\n\t\ttx_swbd->is_eof = false;\n\t}\n\n\t \n\txdp_tx_arr[n - 1].is_eof = true;\n\n\treturn n;\n}\n\nstatic void enetc_xdp_drop(struct enetc_bdr *rx_ring, int rx_ring_first,\n\t\t\t   int rx_ring_last)\n{\n\twhile (rx_ring_first != rx_ring_last) {\n\t\tenetc_put_rx_buff(rx_ring,\n\t\t\t\t  &rx_ring->rx_swbd[rx_ring_first]);\n\t\tenetc_bdr_idx_inc(rx_ring, &rx_ring_first);\n\t}\n\trx_ring->stats.xdp_drops++;\n}\n\nstatic int enetc_clean_rx_ring_xdp(struct enetc_bdr *rx_ring,\n\t\t\t\t   struct napi_struct *napi, int work_limit,\n\t\t\t\t   struct bpf_prog *prog)\n{\n\tint xdp_tx_bd_cnt, xdp_tx_frm_cnt = 0, xdp_redirect_frm_cnt = 0;\n\tstruct enetc_tx_swbd xdp_tx_arr[ENETC_MAX_SKB_FRAGS] = {0};\n\tstruct enetc_ndev_priv *priv = netdev_priv(rx_ring->ndev);\n\tint rx_frm_cnt = 0, rx_byte_cnt = 0;\n\tstruct enetc_bdr *tx_ring;\n\tint cleaned_cnt, i;\n\tu32 xdp_act;\n\n\tcleaned_cnt = enetc_bd_unused(rx_ring);\n\t \n\ti = rx_ring->next_to_clean;\n\n\twhile (likely(rx_frm_cnt < work_limit)) {\n\t\tunion enetc_rx_bd *rxbd, *orig_rxbd;\n\t\tint orig_i, orig_cleaned_cnt;\n\t\tstruct xdp_buff xdp_buff;\n\t\tstruct sk_buff *skb;\n\t\tu32 bd_status;\n\t\tint err;\n\n\t\trxbd = enetc_rxbd(rx_ring, i);\n\t\tbd_status = le32_to_cpu(rxbd->r.lstatus);\n\t\tif (!bd_status)\n\t\t\tbreak;\n\n\t\tenetc_wr_reg_hot(rx_ring->idr, BIT(rx_ring->index));\n\t\tdma_rmb();  \n\n\t\tif (enetc_check_bd_errors_and_consume(rx_ring, bd_status,\n\t\t\t\t\t\t      &rxbd, &i))\n\t\t\tbreak;\n\n\t\torig_rxbd = rxbd;\n\t\torig_cleaned_cnt = cleaned_cnt;\n\t\torig_i = i;\n\n\t\tenetc_build_xdp_buff(rx_ring, bd_status, &rxbd, &i,\n\t\t\t\t     &cleaned_cnt, &xdp_buff);\n\n\t\t \n\t\tif (bd_status & ENETC_RXBD_FLAG_VLAN)\n\t\t\trx_byte_cnt += VLAN_HLEN;\n\t\trx_byte_cnt += xdp_get_buff_len(&xdp_buff);\n\n\t\txdp_act = bpf_prog_run_xdp(prog, &xdp_buff);\n\n\t\tswitch (xdp_act) {\n\t\tdefault:\n\t\t\tbpf_warn_invalid_xdp_action(rx_ring->ndev, prog, xdp_act);\n\t\t\tfallthrough;\n\t\tcase XDP_ABORTED:\n\t\t\ttrace_xdp_exception(rx_ring->ndev, prog, xdp_act);\n\t\t\tfallthrough;\n\t\tcase XDP_DROP:\n\t\t\tenetc_xdp_drop(rx_ring, orig_i, i);\n\t\t\tbreak;\n\t\tcase XDP_PASS:\n\t\t\trxbd = orig_rxbd;\n\t\t\tcleaned_cnt = orig_cleaned_cnt;\n\t\t\ti = orig_i;\n\n\t\t\tskb = enetc_build_skb(rx_ring, bd_status, &rxbd,\n\t\t\t\t\t      &i, &cleaned_cnt,\n\t\t\t\t\t      ENETC_RXB_DMA_SIZE_XDP);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tgoto out;\n\n\t\t\tnapi_gro_receive(napi, skb);\n\t\t\tbreak;\n\t\tcase XDP_TX:\n\t\t\ttx_ring = priv->xdp_tx_ring[rx_ring->index];\n\t\t\txdp_tx_bd_cnt = enetc_rx_swbd_to_xdp_tx_swbd(xdp_tx_arr,\n\t\t\t\t\t\t\t\t     rx_ring,\n\t\t\t\t\t\t\t\t     orig_i, i);\n\n\t\t\tif (!enetc_xdp_tx(tx_ring, xdp_tx_arr, xdp_tx_bd_cnt)) {\n\t\t\t\tenetc_xdp_drop(rx_ring, orig_i, i);\n\t\t\t\ttx_ring->stats.xdp_tx_drops++;\n\t\t\t} else {\n\t\t\t\ttx_ring->stats.xdp_tx += xdp_tx_bd_cnt;\n\t\t\t\trx_ring->xdp.xdp_tx_in_flight += xdp_tx_bd_cnt;\n\t\t\t\txdp_tx_frm_cnt++;\n\t\t\t\t \n\t\t\t\twhile (orig_i != i) {\n\t\t\t\t\trx_ring->rx_swbd[orig_i].page = NULL;\n\t\t\t\t\tenetc_bdr_idx_inc(rx_ring, &orig_i);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase XDP_REDIRECT:\n\t\t\terr = xdp_do_redirect(rx_ring->ndev, &xdp_buff, prog);\n\t\t\tif (unlikely(err)) {\n\t\t\t\tenetc_xdp_drop(rx_ring, orig_i, i);\n\t\t\t\trx_ring->stats.xdp_redirect_failures++;\n\t\t\t} else {\n\t\t\t\twhile (orig_i != i) {\n\t\t\t\t\tenetc_flip_rx_buff(rx_ring,\n\t\t\t\t\t\t\t   &rx_ring->rx_swbd[orig_i]);\n\t\t\t\t\tenetc_bdr_idx_inc(rx_ring, &orig_i);\n\t\t\t\t}\n\t\t\t\txdp_redirect_frm_cnt++;\n\t\t\t\trx_ring->stats.xdp_redirect++;\n\t\t\t}\n\t\t}\n\n\t\trx_frm_cnt++;\n\t}\n\nout:\n\trx_ring->next_to_clean = i;\n\n\trx_ring->stats.packets += rx_frm_cnt;\n\trx_ring->stats.bytes += rx_byte_cnt;\n\n\tif (xdp_redirect_frm_cnt)\n\t\txdp_do_flush_map();\n\n\tif (xdp_tx_frm_cnt)\n\t\tenetc_update_tx_ring_tail(tx_ring);\n\n\tif (cleaned_cnt > rx_ring->xdp.xdp_tx_in_flight)\n\t\tenetc_refill_rx_ring(rx_ring, enetc_bd_unused(rx_ring) -\n\t\t\t\t     rx_ring->xdp.xdp_tx_in_flight);\n\n\treturn rx_frm_cnt;\n}\n\nstatic int enetc_poll(struct napi_struct *napi, int budget)\n{\n\tstruct enetc_int_vector\n\t\t*v = container_of(napi, struct enetc_int_vector, napi);\n\tstruct enetc_bdr *rx_ring = &v->rx_ring;\n\tstruct bpf_prog *prog;\n\tbool complete = true;\n\tint work_done;\n\tint i;\n\n\tenetc_lock_mdio();\n\n\tfor (i = 0; i < v->count_tx_rings; i++)\n\t\tif (!enetc_clean_tx_ring(&v->tx_ring[i], budget))\n\t\t\tcomplete = false;\n\n\tprog = rx_ring->xdp.prog;\n\tif (prog)\n\t\twork_done = enetc_clean_rx_ring_xdp(rx_ring, napi, budget, prog);\n\telse\n\t\twork_done = enetc_clean_rx_ring(rx_ring, napi, budget);\n\tif (work_done == budget)\n\t\tcomplete = false;\n\tif (work_done)\n\t\tv->rx_napi_work = true;\n\n\tif (!complete) {\n\t\tenetc_unlock_mdio();\n\t\treturn budget;\n\t}\n\n\tnapi_complete_done(napi, work_done);\n\n\tif (likely(v->rx_dim_en))\n\t\tenetc_rx_net_dim(v);\n\n\tv->rx_napi_work = false;\n\n\t \n\tenetc_wr_reg_hot(v->rbier, ENETC_RBIER_RXTIE);\n\n\tfor_each_set_bit(i, &v->tx_rings_map, ENETC_MAX_NUM_TXQS)\n\t\tenetc_wr_reg_hot(v->tbier_base + ENETC_BDR_OFF(i),\n\t\t\t\t ENETC_TBIER_TXTIE);\n\n\tenetc_unlock_mdio();\n\n\treturn work_done;\n}\n\n \n#define ENETC_MAX_RFS_SIZE 64\nvoid enetc_get_si_caps(struct enetc_si *si)\n{\n\tstruct enetc_hw *hw = &si->hw;\n\tu32 val;\n\n\t \n\tval = enetc_rd(hw, ENETC_SICAPR0);\n\tsi->num_rx_rings = (val >> 16) & 0xff;\n\tsi->num_tx_rings = val & 0xff;\n\n\tval = enetc_rd(hw, ENETC_SIRFSCAPR);\n\tsi->num_fs_entries = ENETC_SIRFSCAPR_GET_NUM_RFS(val);\n\tsi->num_fs_entries = min(si->num_fs_entries, ENETC_MAX_RFS_SIZE);\n\n\tsi->num_rss = 0;\n\tval = enetc_rd(hw, ENETC_SIPCAPR0);\n\tif (val & ENETC_SIPCAPR0_RSS) {\n\t\tu32 rss;\n\n\t\trss = enetc_rd(hw, ENETC_SIRSSCAPR);\n\t\tsi->num_rss = ENETC_SIRSSCAPR_GET_NUM_RSS(rss);\n\t}\n\n\tif (val & ENETC_SIPCAPR0_QBV)\n\t\tsi->hw_features |= ENETC_SI_F_QBV;\n\n\tif (val & ENETC_SIPCAPR0_QBU)\n\t\tsi->hw_features |= ENETC_SI_F_QBU;\n\n\tif (val & ENETC_SIPCAPR0_PSFP)\n\t\tsi->hw_features |= ENETC_SI_F_PSFP;\n}\nEXPORT_SYMBOL_GPL(enetc_get_si_caps);\n\nstatic int enetc_dma_alloc_bdr(struct enetc_bdr_resource *res)\n{\n\tsize_t bd_base_size = res->bd_count * res->bd_size;\n\n\tres->bd_base = dma_alloc_coherent(res->dev, bd_base_size,\n\t\t\t\t\t  &res->bd_dma_base, GFP_KERNEL);\n\tif (!res->bd_base)\n\t\treturn -ENOMEM;\n\n\t \n\tif (!IS_ALIGNED(res->bd_dma_base, 128)) {\n\t\tdma_free_coherent(res->dev, bd_base_size, res->bd_base,\n\t\t\t\t  res->bd_dma_base);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void enetc_dma_free_bdr(const struct enetc_bdr_resource *res)\n{\n\tsize_t bd_base_size = res->bd_count * res->bd_size;\n\n\tdma_free_coherent(res->dev, bd_base_size, res->bd_base,\n\t\t\t  res->bd_dma_base);\n}\n\nstatic int enetc_alloc_tx_resource(struct enetc_bdr_resource *res,\n\t\t\t\t   struct device *dev, size_t bd_count)\n{\n\tint err;\n\n\tres->dev = dev;\n\tres->bd_count = bd_count;\n\tres->bd_size = sizeof(union enetc_tx_bd);\n\n\tres->tx_swbd = vcalloc(bd_count, sizeof(*res->tx_swbd));\n\tif (!res->tx_swbd)\n\t\treturn -ENOMEM;\n\n\terr = enetc_dma_alloc_bdr(res);\n\tif (err)\n\t\tgoto err_alloc_bdr;\n\n\tres->tso_headers = dma_alloc_coherent(dev, bd_count * TSO_HEADER_SIZE,\n\t\t\t\t\t      &res->tso_headers_dma,\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!res->tso_headers) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_tso;\n\t}\n\n\treturn 0;\n\nerr_alloc_tso:\n\tenetc_dma_free_bdr(res);\nerr_alloc_bdr:\n\tvfree(res->tx_swbd);\n\tres->tx_swbd = NULL;\n\n\treturn err;\n}\n\nstatic void enetc_free_tx_resource(const struct enetc_bdr_resource *res)\n{\n\tdma_free_coherent(res->dev, res->bd_count * TSO_HEADER_SIZE,\n\t\t\t  res->tso_headers, res->tso_headers_dma);\n\tenetc_dma_free_bdr(res);\n\tvfree(res->tx_swbd);\n}\n\nstatic struct enetc_bdr_resource *\nenetc_alloc_tx_resources(struct enetc_ndev_priv *priv)\n{\n\tstruct enetc_bdr_resource *tx_res;\n\tint i, err;\n\n\ttx_res = kcalloc(priv->num_tx_rings, sizeof(*tx_res), GFP_KERNEL);\n\tif (!tx_res)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfor (i = 0; i < priv->num_tx_rings; i++) {\n\t\tstruct enetc_bdr *tx_ring = priv->tx_ring[i];\n\n\t\terr = enetc_alloc_tx_resource(&tx_res[i], tx_ring->dev,\n\t\t\t\t\t      tx_ring->bd_count);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\treturn tx_res;\n\nfail:\n\twhile (i-- > 0)\n\t\tenetc_free_tx_resource(&tx_res[i]);\n\n\tkfree(tx_res);\n\n\treturn ERR_PTR(err);\n}\n\nstatic void enetc_free_tx_resources(const struct enetc_bdr_resource *tx_res,\n\t\t\t\t    size_t num_resources)\n{\n\tsize_t i;\n\n\tfor (i = 0; i < num_resources; i++)\n\t\tenetc_free_tx_resource(&tx_res[i]);\n\n\tkfree(tx_res);\n}\n\nstatic int enetc_alloc_rx_resource(struct enetc_bdr_resource *res,\n\t\t\t\t   struct device *dev, size_t bd_count,\n\t\t\t\t   bool extended)\n{\n\tint err;\n\n\tres->dev = dev;\n\tres->bd_count = bd_count;\n\tres->bd_size = sizeof(union enetc_rx_bd);\n\tif (extended)\n\t\tres->bd_size *= 2;\n\n\tres->rx_swbd = vcalloc(bd_count, sizeof(struct enetc_rx_swbd));\n\tif (!res->rx_swbd)\n\t\treturn -ENOMEM;\n\n\terr = enetc_dma_alloc_bdr(res);\n\tif (err) {\n\t\tvfree(res->rx_swbd);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void enetc_free_rx_resource(const struct enetc_bdr_resource *res)\n{\n\tenetc_dma_free_bdr(res);\n\tvfree(res->rx_swbd);\n}\n\nstatic struct enetc_bdr_resource *\nenetc_alloc_rx_resources(struct enetc_ndev_priv *priv, bool extended)\n{\n\tstruct enetc_bdr_resource *rx_res;\n\tint i, err;\n\n\trx_res = kcalloc(priv->num_rx_rings, sizeof(*rx_res), GFP_KERNEL);\n\tif (!rx_res)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfor (i = 0; i < priv->num_rx_rings; i++) {\n\t\tstruct enetc_bdr *rx_ring = priv->rx_ring[i];\n\n\t\terr = enetc_alloc_rx_resource(&rx_res[i], rx_ring->dev,\n\t\t\t\t\t      rx_ring->bd_count, extended);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\treturn rx_res;\n\nfail:\n\twhile (i-- > 0)\n\t\tenetc_free_rx_resource(&rx_res[i]);\n\n\tkfree(rx_res);\n\n\treturn ERR_PTR(err);\n}\n\nstatic void enetc_free_rx_resources(const struct enetc_bdr_resource *rx_res,\n\t\t\t\t    size_t num_resources)\n{\n\tsize_t i;\n\n\tfor (i = 0; i < num_resources; i++)\n\t\tenetc_free_rx_resource(&rx_res[i]);\n\n\tkfree(rx_res);\n}\n\nstatic void enetc_assign_tx_resource(struct enetc_bdr *tx_ring,\n\t\t\t\t     const struct enetc_bdr_resource *res)\n{\n\ttx_ring->bd_base = res ? res->bd_base : NULL;\n\ttx_ring->bd_dma_base = res ? res->bd_dma_base : 0;\n\ttx_ring->tx_swbd = res ? res->tx_swbd : NULL;\n\ttx_ring->tso_headers = res ? res->tso_headers : NULL;\n\ttx_ring->tso_headers_dma = res ? res->tso_headers_dma : 0;\n}\n\nstatic void enetc_assign_rx_resource(struct enetc_bdr *rx_ring,\n\t\t\t\t     const struct enetc_bdr_resource *res)\n{\n\trx_ring->bd_base = res ? res->bd_base : NULL;\n\trx_ring->bd_dma_base = res ? res->bd_dma_base : 0;\n\trx_ring->rx_swbd = res ? res->rx_swbd : NULL;\n}\n\nstatic void enetc_assign_tx_resources(struct enetc_ndev_priv *priv,\n\t\t\t\t      const struct enetc_bdr_resource *res)\n{\n\tint i;\n\n\tif (priv->tx_res)\n\t\tenetc_free_tx_resources(priv->tx_res, priv->num_tx_rings);\n\n\tfor (i = 0; i < priv->num_tx_rings; i++) {\n\t\tenetc_assign_tx_resource(priv->tx_ring[i],\n\t\t\t\t\t res ? &res[i] : NULL);\n\t}\n\n\tpriv->tx_res = res;\n}\n\nstatic void enetc_assign_rx_resources(struct enetc_ndev_priv *priv,\n\t\t\t\t      const struct enetc_bdr_resource *res)\n{\n\tint i;\n\n\tif (priv->rx_res)\n\t\tenetc_free_rx_resources(priv->rx_res, priv->num_rx_rings);\n\n\tfor (i = 0; i < priv->num_rx_rings; i++) {\n\t\tenetc_assign_rx_resource(priv->rx_ring[i],\n\t\t\t\t\t res ? &res[i] : NULL);\n\t}\n\n\tpriv->rx_res = res;\n}\n\nstatic void enetc_free_tx_ring(struct enetc_bdr *tx_ring)\n{\n\tint i;\n\n\tfor (i = 0; i < tx_ring->bd_count; i++) {\n\t\tstruct enetc_tx_swbd *tx_swbd = &tx_ring->tx_swbd[i];\n\n\t\tenetc_free_tx_frame(tx_ring, tx_swbd);\n\t}\n}\n\nstatic void enetc_free_rx_ring(struct enetc_bdr *rx_ring)\n{\n\tint i;\n\n\tfor (i = 0; i < rx_ring->bd_count; i++) {\n\t\tstruct enetc_rx_swbd *rx_swbd = &rx_ring->rx_swbd[i];\n\n\t\tif (!rx_swbd->page)\n\t\t\tcontinue;\n\n\t\tdma_unmap_page(rx_ring->dev, rx_swbd->dma, PAGE_SIZE,\n\t\t\t       rx_swbd->dir);\n\t\t__free_page(rx_swbd->page);\n\t\trx_swbd->page = NULL;\n\t}\n}\n\nstatic void enetc_free_rxtx_rings(struct enetc_ndev_priv *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_rx_rings; i++)\n\t\tenetc_free_rx_ring(priv->rx_ring[i]);\n\n\tfor (i = 0; i < priv->num_tx_rings; i++)\n\t\tenetc_free_tx_ring(priv->tx_ring[i]);\n}\n\nstatic int enetc_setup_default_rss_table(struct enetc_si *si, int num_groups)\n{\n\tint *rss_table;\n\tint i;\n\n\trss_table = kmalloc_array(si->num_rss, sizeof(*rss_table), GFP_KERNEL);\n\tif (!rss_table)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < si->num_rss; i++)\n\t\trss_table[i] = i % num_groups;\n\n\tenetc_set_rss_table(si, rss_table, si->num_rss);\n\n\tkfree(rss_table);\n\n\treturn 0;\n}\n\nint enetc_configure_si(struct enetc_ndev_priv *priv)\n{\n\tstruct enetc_si *si = priv->si;\n\tstruct enetc_hw *hw = &si->hw;\n\tint err;\n\n\t \n\tenetc_wr(hw, ENETC_SICAR0,\n\t\t ENETC_SICAR_RD_COHERENT | ENETC_SICAR_WR_COHERENT);\n\tenetc_wr(hw, ENETC_SICAR1, ENETC_SICAR_MSI);\n\t \n\tenetc_wr(hw, ENETC_SIMR, ENETC_SIMR_EN);\n\n\tif (si->num_rss) {\n\t\terr = enetc_setup_default_rss_table(si, priv->num_rx_rings);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(enetc_configure_si);\n\nvoid enetc_init_si_rings_params(struct enetc_ndev_priv *priv)\n{\n\tstruct enetc_si *si = priv->si;\n\tint cpus = num_online_cpus();\n\n\tpriv->tx_bd_count = ENETC_TX_RING_DEFAULT_SIZE;\n\tpriv->rx_bd_count = ENETC_RX_RING_DEFAULT_SIZE;\n\n\t \n\tpriv->num_rx_rings = min_t(int, cpus, si->num_rx_rings);\n\tpriv->num_tx_rings = si->num_tx_rings;\n\tpriv->bdr_int_num = cpus;\n\tpriv->ic_mode = ENETC_IC_RX_ADAPTIVE | ENETC_IC_TX_MANUAL;\n\tpriv->tx_ictt = ENETC_TXIC_TIMETHR;\n}\nEXPORT_SYMBOL_GPL(enetc_init_si_rings_params);\n\nint enetc_alloc_si_resources(struct enetc_ndev_priv *priv)\n{\n\tstruct enetc_si *si = priv->si;\n\n\tpriv->cls_rules = kcalloc(si->num_fs_entries, sizeof(*priv->cls_rules),\n\t\t\t\t  GFP_KERNEL);\n\tif (!priv->cls_rules)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(enetc_alloc_si_resources);\n\nvoid enetc_free_si_resources(struct enetc_ndev_priv *priv)\n{\n\tkfree(priv->cls_rules);\n}\nEXPORT_SYMBOL_GPL(enetc_free_si_resources);\n\nstatic void enetc_setup_txbdr(struct enetc_hw *hw, struct enetc_bdr *tx_ring)\n{\n\tint idx = tx_ring->index;\n\tu32 tbmr;\n\n\tenetc_txbdr_wr(hw, idx, ENETC_TBBAR0,\n\t\t       lower_32_bits(tx_ring->bd_dma_base));\n\n\tenetc_txbdr_wr(hw, idx, ENETC_TBBAR1,\n\t\t       upper_32_bits(tx_ring->bd_dma_base));\n\n\tWARN_ON(!IS_ALIGNED(tx_ring->bd_count, 64));  \n\tenetc_txbdr_wr(hw, idx, ENETC_TBLENR,\n\t\t       ENETC_RTBLENR_LEN(tx_ring->bd_count));\n\n\t \n\ttx_ring->next_to_use = enetc_txbdr_rd(hw, idx, ENETC_TBPIR);\n\ttx_ring->next_to_clean = enetc_txbdr_rd(hw, idx, ENETC_TBCIR);\n\n\t \n\tenetc_txbdr_wr(hw, idx, ENETC_TBICR0, ENETC_TBICR0_ICEN | 0x1);\n\n\ttbmr = ENETC_TBMR_SET_PRIO(tx_ring->prio);\n\tif (tx_ring->ndev->features & NETIF_F_HW_VLAN_CTAG_TX)\n\t\ttbmr |= ENETC_TBMR_VIH;\n\n\t \n\tenetc_txbdr_wr(hw, idx, ENETC_TBMR, tbmr);\n\n\ttx_ring->tpir = hw->reg + ENETC_BDR(TX, idx, ENETC_TBPIR);\n\ttx_ring->tcir = hw->reg + ENETC_BDR(TX, idx, ENETC_TBCIR);\n\ttx_ring->idr = hw->reg + ENETC_SITXIDR;\n}\n\nstatic void enetc_setup_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring,\n\t\t\t      bool extended)\n{\n\tint idx = rx_ring->index;\n\tu32 rbmr = 0;\n\n\tenetc_rxbdr_wr(hw, idx, ENETC_RBBAR0,\n\t\t       lower_32_bits(rx_ring->bd_dma_base));\n\n\tenetc_rxbdr_wr(hw, idx, ENETC_RBBAR1,\n\t\t       upper_32_bits(rx_ring->bd_dma_base));\n\n\tWARN_ON(!IS_ALIGNED(rx_ring->bd_count, 64));  \n\tenetc_rxbdr_wr(hw, idx, ENETC_RBLENR,\n\t\t       ENETC_RTBLENR_LEN(rx_ring->bd_count));\n\n\tif (rx_ring->xdp.prog)\n\t\tenetc_rxbdr_wr(hw, idx, ENETC_RBBSR, ENETC_RXB_DMA_SIZE_XDP);\n\telse\n\t\tenetc_rxbdr_wr(hw, idx, ENETC_RBBSR, ENETC_RXB_DMA_SIZE);\n\n\t \n\tenetc_rxbdr_wr(hw, idx, ENETC_RBPIR, 0);\n\tenetc_rxbdr_wr(hw, idx, ENETC_RBCIR, 1);\n\n\t \n\tenetc_rxbdr_wr(hw, idx, ENETC_RBICR0, ENETC_RBICR0_ICEN | 0x1);\n\n\trx_ring->ext_en = extended;\n\tif (rx_ring->ext_en)\n\t\trbmr |= ENETC_RBMR_BDS;\n\n\tif (rx_ring->ndev->features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\trbmr |= ENETC_RBMR_VTE;\n\n\trx_ring->rcir = hw->reg + ENETC_BDR(RX, idx, ENETC_RBCIR);\n\trx_ring->idr = hw->reg + ENETC_SIRXIDR;\n\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n\trx_ring->next_to_alloc = 0;\n\n\tenetc_lock_mdio();\n\tenetc_refill_rx_ring(rx_ring, enetc_bd_unused(rx_ring));\n\tenetc_unlock_mdio();\n\n\tenetc_rxbdr_wr(hw, idx, ENETC_RBMR, rbmr);\n}\n\nstatic void enetc_setup_bdrs(struct enetc_ndev_priv *priv, bool extended)\n{\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tint i;\n\n\tfor (i = 0; i < priv->num_tx_rings; i++)\n\t\tenetc_setup_txbdr(hw, priv->tx_ring[i]);\n\n\tfor (i = 0; i < priv->num_rx_rings; i++)\n\t\tenetc_setup_rxbdr(hw, priv->rx_ring[i], extended);\n}\n\nstatic void enetc_enable_txbdr(struct enetc_hw *hw, struct enetc_bdr *tx_ring)\n{\n\tint idx = tx_ring->index;\n\tu32 tbmr;\n\n\ttbmr = enetc_txbdr_rd(hw, idx, ENETC_TBMR);\n\ttbmr |= ENETC_TBMR_EN;\n\tenetc_txbdr_wr(hw, idx, ENETC_TBMR, tbmr);\n}\n\nstatic void enetc_enable_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)\n{\n\tint idx = rx_ring->index;\n\tu32 rbmr;\n\n\trbmr = enetc_rxbdr_rd(hw, idx, ENETC_RBMR);\n\trbmr |= ENETC_RBMR_EN;\n\tenetc_rxbdr_wr(hw, idx, ENETC_RBMR, rbmr);\n}\n\nstatic void enetc_enable_bdrs(struct enetc_ndev_priv *priv)\n{\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tint i;\n\n\tfor (i = 0; i < priv->num_tx_rings; i++)\n\t\tenetc_enable_txbdr(hw, priv->tx_ring[i]);\n\n\tfor (i = 0; i < priv->num_rx_rings; i++)\n\t\tenetc_enable_rxbdr(hw, priv->rx_ring[i]);\n}\n\nstatic void enetc_disable_rxbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)\n{\n\tint idx = rx_ring->index;\n\n\t \n\tenetc_rxbdr_wr(hw, idx, ENETC_RBMR, 0);\n}\n\nstatic void enetc_disable_txbdr(struct enetc_hw *hw, struct enetc_bdr *rx_ring)\n{\n\tint idx = rx_ring->index;\n\n\t \n\tenetc_txbdr_wr(hw, idx, ENETC_TBMR, 0);\n}\n\nstatic void enetc_disable_bdrs(struct enetc_ndev_priv *priv)\n{\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tint i;\n\n\tfor (i = 0; i < priv->num_tx_rings; i++)\n\t\tenetc_disable_txbdr(hw, priv->tx_ring[i]);\n\n\tfor (i = 0; i < priv->num_rx_rings; i++)\n\t\tenetc_disable_rxbdr(hw, priv->rx_ring[i]);\n}\n\nstatic void enetc_wait_txbdr(struct enetc_hw *hw, struct enetc_bdr *tx_ring)\n{\n\tint delay = 8, timeout = 100;\n\tint idx = tx_ring->index;\n\n\t \n\twhile (delay < timeout &&\n\t       enetc_txbdr_rd(hw, idx, ENETC_TBSR) & ENETC_TBSR_BUSY) {\n\t\tmsleep(delay);\n\t\tdelay *= 2;\n\t}\n\n\tif (delay >= timeout)\n\t\tnetdev_warn(tx_ring->ndev, \"timeout for tx ring #%d clear\\n\",\n\t\t\t    idx);\n}\n\nstatic void enetc_wait_bdrs(struct enetc_ndev_priv *priv)\n{\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tint i;\n\n\tfor (i = 0; i < priv->num_tx_rings; i++)\n\t\tenetc_wait_txbdr(hw, priv->tx_ring[i]);\n}\n\nstatic int enetc_setup_irqs(struct enetc_ndev_priv *priv)\n{\n\tstruct pci_dev *pdev = priv->si->pdev;\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tint i, j, err;\n\n\tfor (i = 0; i < priv->bdr_int_num; i++) {\n\t\tint irq = pci_irq_vector(pdev, ENETC_BDR_INT_BASE_IDX + i);\n\t\tstruct enetc_int_vector *v = priv->int_vector[i];\n\t\tint entry = ENETC_BDR_INT_BASE_IDX + i;\n\n\t\tsnprintf(v->name, sizeof(v->name), \"%s-rxtx%d\",\n\t\t\t priv->ndev->name, i);\n\t\terr = request_irq(irq, enetc_msix, 0, v->name, v);\n\t\tif (err) {\n\t\t\tdev_err(priv->dev, \"request_irq() failed!\\n\");\n\t\t\tgoto irq_err;\n\t\t}\n\t\tdisable_irq(irq);\n\n\t\tv->tbier_base = hw->reg + ENETC_BDR(TX, 0, ENETC_TBIER);\n\t\tv->rbier = hw->reg + ENETC_BDR(RX, i, ENETC_RBIER);\n\t\tv->ricr1 = hw->reg + ENETC_BDR(RX, i, ENETC_RBICR1);\n\n\t\tenetc_wr(hw, ENETC_SIMSIRRV(i), entry);\n\n\t\tfor (j = 0; j < v->count_tx_rings; j++) {\n\t\t\tint idx = v->tx_ring[j].index;\n\n\t\t\tenetc_wr(hw, ENETC_SIMSITRV(idx), entry);\n\t\t}\n\t\tirq_set_affinity_hint(irq, get_cpu_mask(i % num_online_cpus()));\n\t}\n\n\treturn 0;\n\nirq_err:\n\twhile (i--) {\n\t\tint irq = pci_irq_vector(pdev, ENETC_BDR_INT_BASE_IDX + i);\n\n\t\tirq_set_affinity_hint(irq, NULL);\n\t\tfree_irq(irq, priv->int_vector[i]);\n\t}\n\n\treturn err;\n}\n\nstatic void enetc_free_irqs(struct enetc_ndev_priv *priv)\n{\n\tstruct pci_dev *pdev = priv->si->pdev;\n\tint i;\n\n\tfor (i = 0; i < priv->bdr_int_num; i++) {\n\t\tint irq = pci_irq_vector(pdev, ENETC_BDR_INT_BASE_IDX + i);\n\n\t\tirq_set_affinity_hint(irq, NULL);\n\t\tfree_irq(irq, priv->int_vector[i]);\n\t}\n}\n\nstatic void enetc_setup_interrupts(struct enetc_ndev_priv *priv)\n{\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tu32 icpt, ictt;\n\tint i;\n\n\t \n\tif (priv->ic_mode &\n\t    (ENETC_IC_RX_MANUAL | ENETC_IC_RX_ADAPTIVE)) {\n\t\ticpt = ENETC_RBICR0_SET_ICPT(ENETC_RXIC_PKTTHR);\n\t\t \n\t\tictt = 0x1;\n\t} else {\n\t\ticpt = 0x1;  \n\t\tictt = 0;\n\t}\n\n\tfor (i = 0; i < priv->num_rx_rings; i++) {\n\t\tenetc_rxbdr_wr(hw, i, ENETC_RBICR1, ictt);\n\t\tenetc_rxbdr_wr(hw, i, ENETC_RBICR0, ENETC_RBICR0_ICEN | icpt);\n\t\tenetc_rxbdr_wr(hw, i, ENETC_RBIER, ENETC_RBIER_RXTIE);\n\t}\n\n\tif (priv->ic_mode & ENETC_IC_TX_MANUAL)\n\t\ticpt = ENETC_TBICR0_SET_ICPT(ENETC_TXIC_PKTTHR);\n\telse\n\t\ticpt = 0x1;  \n\n\tfor (i = 0; i < priv->num_tx_rings; i++) {\n\t\tenetc_txbdr_wr(hw, i, ENETC_TBICR1, priv->tx_ictt);\n\t\tenetc_txbdr_wr(hw, i, ENETC_TBICR0, ENETC_TBICR0_ICEN | icpt);\n\t\tenetc_txbdr_wr(hw, i, ENETC_TBIER, ENETC_TBIER_TXTIE);\n\t}\n}\n\nstatic void enetc_clear_interrupts(struct enetc_ndev_priv *priv)\n{\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tint i;\n\n\tfor (i = 0; i < priv->num_tx_rings; i++)\n\t\tenetc_txbdr_wr(hw, i, ENETC_TBIER, 0);\n\n\tfor (i = 0; i < priv->num_rx_rings; i++)\n\t\tenetc_rxbdr_wr(hw, i, ENETC_RBIER, 0);\n}\n\nstatic int enetc_phylink_connect(struct net_device *ndev)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct ethtool_eee edata;\n\tint err;\n\n\tif (!priv->phylink) {\n\t\t \n\t\tnetif_carrier_on(ndev);\n\t\treturn 0;\n\t}\n\n\terr = phylink_of_phy_connect(priv->phylink, priv->dev->of_node, 0);\n\tif (err) {\n\t\tdev_err(&ndev->dev, \"could not attach to PHY\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tmemset(&edata, 0, sizeof(struct ethtool_eee));\n\tphylink_ethtool_set_eee(priv->phylink, &edata);\n\n\tphylink_start(priv->phylink);\n\n\treturn 0;\n}\n\nstatic void enetc_tx_onestep_tstamp(struct work_struct *work)\n{\n\tstruct enetc_ndev_priv *priv;\n\tstruct sk_buff *skb;\n\n\tpriv = container_of(work, struct enetc_ndev_priv, tx_onestep_tstamp);\n\n\tnetif_tx_lock_bh(priv->ndev);\n\n\tclear_bit_unlock(ENETC_TX_ONESTEP_TSTAMP_IN_PROGRESS, &priv->flags);\n\tskb = skb_dequeue(&priv->tx_skbs);\n\tif (skb)\n\t\tenetc_start_xmit(skb, priv->ndev);\n\n\tnetif_tx_unlock_bh(priv->ndev);\n}\n\nstatic void enetc_tx_onestep_tstamp_init(struct enetc_ndev_priv *priv)\n{\n\tINIT_WORK(&priv->tx_onestep_tstamp, enetc_tx_onestep_tstamp);\n\tskb_queue_head_init(&priv->tx_skbs);\n}\n\nvoid enetc_start(struct net_device *ndev)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tint i;\n\n\tenetc_setup_interrupts(priv);\n\n\tfor (i = 0; i < priv->bdr_int_num; i++) {\n\t\tint irq = pci_irq_vector(priv->si->pdev,\n\t\t\t\t\t ENETC_BDR_INT_BASE_IDX + i);\n\n\t\tnapi_enable(&priv->int_vector[i]->napi);\n\t\tenable_irq(irq);\n\t}\n\n\tenetc_enable_bdrs(priv);\n\n\tnetif_tx_start_all_queues(ndev);\n}\nEXPORT_SYMBOL_GPL(enetc_start);\n\nint enetc_open(struct net_device *ndev)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct enetc_bdr_resource *tx_res, *rx_res;\n\tbool extended;\n\tint err;\n\n\textended = !!(priv->active_offloads & ENETC_F_RX_TSTAMP);\n\n\terr = enetc_setup_irqs(priv);\n\tif (err)\n\t\treturn err;\n\n\terr = enetc_phylink_connect(ndev);\n\tif (err)\n\t\tgoto err_phy_connect;\n\n\ttx_res = enetc_alloc_tx_resources(priv);\n\tif (IS_ERR(tx_res)) {\n\t\terr = PTR_ERR(tx_res);\n\t\tgoto err_alloc_tx;\n\t}\n\n\trx_res = enetc_alloc_rx_resources(priv, extended);\n\tif (IS_ERR(rx_res)) {\n\t\terr = PTR_ERR(rx_res);\n\t\tgoto err_alloc_rx;\n\t}\n\n\tenetc_tx_onestep_tstamp_init(priv);\n\tenetc_assign_tx_resources(priv, tx_res);\n\tenetc_assign_rx_resources(priv, rx_res);\n\tenetc_setup_bdrs(priv, extended);\n\tenetc_start(ndev);\n\n\treturn 0;\n\nerr_alloc_rx:\n\tenetc_free_tx_resources(tx_res, priv->num_tx_rings);\nerr_alloc_tx:\n\tif (priv->phylink)\n\t\tphylink_disconnect_phy(priv->phylink);\nerr_phy_connect:\n\tenetc_free_irqs(priv);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(enetc_open);\n\nvoid enetc_stop(struct net_device *ndev)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tint i;\n\n\tnetif_tx_stop_all_queues(ndev);\n\n\tenetc_disable_bdrs(priv);\n\n\tfor (i = 0; i < priv->bdr_int_num; i++) {\n\t\tint irq = pci_irq_vector(priv->si->pdev,\n\t\t\t\t\t ENETC_BDR_INT_BASE_IDX + i);\n\n\t\tdisable_irq(irq);\n\t\tnapi_synchronize(&priv->int_vector[i]->napi);\n\t\tnapi_disable(&priv->int_vector[i]->napi);\n\t}\n\n\tenetc_wait_bdrs(priv);\n\n\tenetc_clear_interrupts(priv);\n}\nEXPORT_SYMBOL_GPL(enetc_stop);\n\nint enetc_close(struct net_device *ndev)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\n\tenetc_stop(ndev);\n\n\tif (priv->phylink) {\n\t\tphylink_stop(priv->phylink);\n\t\tphylink_disconnect_phy(priv->phylink);\n\t} else {\n\t\tnetif_carrier_off(ndev);\n\t}\n\n\tenetc_free_rxtx_rings(priv);\n\n\t \n\tenetc_assign_rx_resources(priv, NULL);\n\tenetc_assign_tx_resources(priv, NULL);\n\n\tenetc_free_irqs(priv);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(enetc_close);\n\nstatic int enetc_reconfigure(struct enetc_ndev_priv *priv, bool extended,\n\t\t\t     int (*cb)(struct enetc_ndev_priv *priv, void *ctx),\n\t\t\t     void *ctx)\n{\n\tstruct enetc_bdr_resource *tx_res, *rx_res;\n\tint err;\n\n\tASSERT_RTNL();\n\n\t \n\tif (!netif_running(priv->ndev)) {\n\t\tif (cb) {\n\t\t\terr = cb(priv, ctx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\ttx_res = enetc_alloc_tx_resources(priv);\n\tif (IS_ERR(tx_res)) {\n\t\terr = PTR_ERR(tx_res);\n\t\tgoto out;\n\t}\n\n\trx_res = enetc_alloc_rx_resources(priv, extended);\n\tif (IS_ERR(rx_res)) {\n\t\terr = PTR_ERR(rx_res);\n\t\tgoto out_free_tx_res;\n\t}\n\n\tenetc_stop(priv->ndev);\n\tenetc_free_rxtx_rings(priv);\n\n\t \n\tif (cb) {\n\t\terr = cb(priv, ctx);\n\t\tif (err)\n\t\t\tgoto out_restart;\n\t}\n\n\tenetc_assign_tx_resources(priv, tx_res);\n\tenetc_assign_rx_resources(priv, rx_res);\n\tenetc_setup_bdrs(priv, extended);\n\tenetc_start(priv->ndev);\n\n\treturn 0;\n\nout_restart:\n\tenetc_setup_bdrs(priv, extended);\n\tenetc_start(priv->ndev);\n\tenetc_free_rx_resources(rx_res, priv->num_rx_rings);\nout_free_tx_res:\n\tenetc_free_tx_resources(tx_res, priv->num_tx_rings);\nout:\n\treturn err;\n}\n\nstatic void enetc_debug_tx_ring_prios(struct enetc_ndev_priv *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_tx_rings; i++)\n\t\tnetdev_dbg(priv->ndev, \"TX ring %d prio %d\\n\", i,\n\t\t\t   priv->tx_ring[i]->prio);\n}\n\nvoid enetc_reset_tc_mqprio(struct net_device *ndev)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tstruct enetc_bdr *tx_ring;\n\tint num_stack_tx_queues;\n\tint i;\n\n\tnum_stack_tx_queues = enetc_num_stack_tx_queues(priv);\n\n\tnetdev_reset_tc(ndev);\n\tnetif_set_real_num_tx_queues(ndev, num_stack_tx_queues);\n\tpriv->min_num_stack_tx_queues = num_possible_cpus();\n\n\t \n\tfor (i = 0; i < priv->num_tx_rings; i++) {\n\t\ttx_ring = priv->tx_ring[i];\n\t\ttx_ring->prio = 0;\n\t\tenetc_set_bdr_prio(hw, tx_ring->index, tx_ring->prio);\n\t}\n\n\tenetc_debug_tx_ring_prios(priv);\n\n\tenetc_change_preemptible_tcs(priv, 0);\n}\nEXPORT_SYMBOL_GPL(enetc_reset_tc_mqprio);\n\nint enetc_setup_tc_mqprio(struct net_device *ndev, void *type_data)\n{\n\tstruct tc_mqprio_qopt_offload *mqprio = type_data;\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct tc_mqprio_qopt *qopt = &mqprio->qopt;\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tint num_stack_tx_queues = 0;\n\tstruct enetc_bdr *tx_ring;\n\tu8 num_tc = qopt->num_tc;\n\tint offset, count;\n\tint err, tc, q;\n\n\tif (!num_tc) {\n\t\tenetc_reset_tc_mqprio(ndev);\n\t\treturn 0;\n\t}\n\n\terr = netdev_set_num_tc(ndev, num_tc);\n\tif (err)\n\t\treturn err;\n\n\tfor (tc = 0; tc < num_tc; tc++) {\n\t\toffset = qopt->offset[tc];\n\t\tcount = qopt->count[tc];\n\t\tnum_stack_tx_queues += count;\n\n\t\terr = netdev_set_tc_queue(ndev, tc, count, offset);\n\t\tif (err)\n\t\t\tgoto err_reset_tc;\n\n\t\tfor (q = offset; q < offset + count; q++) {\n\t\t\ttx_ring = priv->tx_ring[q];\n\t\t\t \n\t\t\ttx_ring->prio = tc;\n\t\t\tenetc_set_bdr_prio(hw, tx_ring->index, tx_ring->prio);\n\t\t}\n\t}\n\n\terr = netif_set_real_num_tx_queues(ndev, num_stack_tx_queues);\n\tif (err)\n\t\tgoto err_reset_tc;\n\n\tpriv->min_num_stack_tx_queues = num_stack_tx_queues;\n\n\tenetc_debug_tx_ring_prios(priv);\n\n\tenetc_change_preemptible_tcs(priv, mqprio->preemptible_tcs);\n\n\treturn 0;\n\nerr_reset_tc:\n\tenetc_reset_tc_mqprio(ndev);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(enetc_setup_tc_mqprio);\n\nstatic int enetc_reconfigure_xdp_cb(struct enetc_ndev_priv *priv, void *ctx)\n{\n\tstruct bpf_prog *old_prog, *prog = ctx;\n\tint num_stack_tx_queues;\n\tint err, i;\n\n\told_prog = xchg(&priv->xdp_prog, prog);\n\n\tnum_stack_tx_queues = enetc_num_stack_tx_queues(priv);\n\terr = netif_set_real_num_tx_queues(priv->ndev, num_stack_tx_queues);\n\tif (err) {\n\t\txchg(&priv->xdp_prog, old_prog);\n\t\treturn err;\n\t}\n\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\tfor (i = 0; i < priv->num_rx_rings; i++) {\n\t\tstruct enetc_bdr *rx_ring = priv->rx_ring[i];\n\n\t\trx_ring->xdp.prog = prog;\n\n\t\tif (prog)\n\t\t\trx_ring->buffer_offset = XDP_PACKET_HEADROOM;\n\t\telse\n\t\t\trx_ring->buffer_offset = ENETC_RXB_PAD;\n\t}\n\n\treturn 0;\n}\n\nstatic int enetc_setup_xdp_prog(struct net_device *ndev, struct bpf_prog *prog,\n\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tint num_xdp_tx_queues = prog ? num_possible_cpus() : 0;\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tbool extended;\n\n\tif (priv->min_num_stack_tx_queues + num_xdp_tx_queues >\n\t    priv->num_tx_rings) {\n\t\tNL_SET_ERR_MSG_FMT_MOD(extack,\n\t\t\t\t       \"Reserving %d XDP TXQs does not leave a minimum of %d for stack (total %d)\",\n\t\t\t\t       num_xdp_tx_queues,\n\t\t\t\t       priv->min_num_stack_tx_queues,\n\t\t\t\t       priv->num_tx_rings);\n\t\treturn -EBUSY;\n\t}\n\n\textended = !!(priv->active_offloads & ENETC_F_RX_TSTAMP);\n\n\t \n\treturn enetc_reconfigure(priv, extended, enetc_reconfigure_xdp_cb, prog);\n}\n\nint enetc_setup_bpf(struct net_device *ndev, struct netdev_bpf *bpf)\n{\n\tswitch (bpf->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn enetc_setup_xdp_prog(ndev, bpf->prog, bpf->extack);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(enetc_setup_bpf);\n\nstruct net_device_stats *enetc_get_stats(struct net_device *ndev)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tunsigned long packets = 0, bytes = 0;\n\tunsigned long tx_dropped = 0;\n\tint i;\n\n\tfor (i = 0; i < priv->num_rx_rings; i++) {\n\t\tpackets += priv->rx_ring[i]->stats.packets;\n\t\tbytes\t+= priv->rx_ring[i]->stats.bytes;\n\t}\n\n\tstats->rx_packets = packets;\n\tstats->rx_bytes = bytes;\n\tbytes = 0;\n\tpackets = 0;\n\n\tfor (i = 0; i < priv->num_tx_rings; i++) {\n\t\tpackets += priv->tx_ring[i]->stats.packets;\n\t\tbytes\t+= priv->tx_ring[i]->stats.bytes;\n\t\ttx_dropped += priv->tx_ring[i]->stats.win_drop;\n\t}\n\n\tstats->tx_packets = packets;\n\tstats->tx_bytes = bytes;\n\tstats->tx_dropped = tx_dropped;\n\n\treturn stats;\n}\nEXPORT_SYMBOL_GPL(enetc_get_stats);\n\nstatic int enetc_set_rss(struct net_device *ndev, int en)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tu32 reg;\n\n\tenetc_wr(hw, ENETC_SIRBGCR, priv->num_rx_rings);\n\n\treg = enetc_rd(hw, ENETC_SIMR);\n\treg &= ~ENETC_SIMR_RSSE;\n\treg |= (en) ? ENETC_SIMR_RSSE : 0;\n\tenetc_wr(hw, ENETC_SIMR, reg);\n\n\treturn 0;\n}\n\nstatic void enetc_enable_rxvlan(struct net_device *ndev, bool en)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tint i;\n\n\tfor (i = 0; i < priv->num_rx_rings; i++)\n\t\tenetc_bdr_enable_rxvlan(hw, i, en);\n}\n\nstatic void enetc_enable_txvlan(struct net_device *ndev, bool en)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct enetc_hw *hw = &priv->si->hw;\n\tint i;\n\n\tfor (i = 0; i < priv->num_tx_rings; i++)\n\t\tenetc_bdr_enable_txvlan(hw, i, en);\n}\n\nvoid enetc_set_features(struct net_device *ndev, netdev_features_t features)\n{\n\tnetdev_features_t changed = ndev->features ^ features;\n\n\tif (changed & NETIF_F_RXHASH)\n\t\tenetc_set_rss(ndev, !!(features & NETIF_F_RXHASH));\n\n\tif (changed & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tenetc_enable_rxvlan(ndev,\n\t\t\t\t    !!(features & NETIF_F_HW_VLAN_CTAG_RX));\n\n\tif (changed & NETIF_F_HW_VLAN_CTAG_TX)\n\t\tenetc_enable_txvlan(ndev,\n\t\t\t\t    !!(features & NETIF_F_HW_VLAN_CTAG_TX));\n}\nEXPORT_SYMBOL_GPL(enetc_set_features);\n\n#ifdef CONFIG_FSL_ENETC_PTP_CLOCK\nstatic int enetc_hwtstamp_set(struct net_device *ndev, struct ifreq *ifr)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tint err, new_offloads = priv->active_offloads;\n\tstruct hwtstamp_config config;\n\n\tif (copy_from_user(&config, ifr->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\tswitch (config.tx_type) {\n\tcase HWTSTAMP_TX_OFF:\n\t\tnew_offloads &= ~ENETC_F_TX_TSTAMP_MASK;\n\t\tbreak;\n\tcase HWTSTAMP_TX_ON:\n\t\tnew_offloads &= ~ENETC_F_TX_TSTAMP_MASK;\n\t\tnew_offloads |= ENETC_F_TX_TSTAMP;\n\t\tbreak;\n\tcase HWTSTAMP_TX_ONESTEP_SYNC:\n\t\tnew_offloads &= ~ENETC_F_TX_TSTAMP_MASK;\n\t\tnew_offloads |= ENETC_F_TX_ONESTEP_SYNC_TSTAMP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tswitch (config.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\tnew_offloads &= ~ENETC_F_RX_TSTAMP;\n\t\tbreak;\n\tdefault:\n\t\tnew_offloads |= ENETC_F_RX_TSTAMP;\n\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t}\n\n\tif ((new_offloads ^ priv->active_offloads) & ENETC_F_RX_TSTAMP) {\n\t\tbool extended = !!(new_offloads & ENETC_F_RX_TSTAMP);\n\n\t\terr = enetc_reconfigure(priv, extended, NULL, NULL);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tpriv->active_offloads = new_offloads;\n\n\treturn copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?\n\t       -EFAULT : 0;\n}\n\nstatic int enetc_hwtstamp_get(struct net_device *ndev, struct ifreq *ifr)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n\tstruct hwtstamp_config config;\n\n\tconfig.flags = 0;\n\n\tif (priv->active_offloads & ENETC_F_TX_ONESTEP_SYNC_TSTAMP)\n\t\tconfig.tx_type = HWTSTAMP_TX_ONESTEP_SYNC;\n\telse if (priv->active_offloads & ENETC_F_TX_TSTAMP)\n\t\tconfig.tx_type = HWTSTAMP_TX_ON;\n\telse\n\t\tconfig.tx_type = HWTSTAMP_TX_OFF;\n\n\tconfig.rx_filter = (priv->active_offloads & ENETC_F_RX_TSTAMP) ?\n\t\t\t    HWTSTAMP_FILTER_ALL : HWTSTAMP_FILTER_NONE;\n\n\treturn copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?\n\t       -EFAULT : 0;\n}\n#endif\n\nint enetc_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd)\n{\n\tstruct enetc_ndev_priv *priv = netdev_priv(ndev);\n#ifdef CONFIG_FSL_ENETC_PTP_CLOCK\n\tif (cmd == SIOCSHWTSTAMP)\n\t\treturn enetc_hwtstamp_set(ndev, rq);\n\tif (cmd == SIOCGHWTSTAMP)\n\t\treturn enetc_hwtstamp_get(ndev, rq);\n#endif\n\n\tif (!priv->phylink)\n\t\treturn -EOPNOTSUPP;\n\n\treturn phylink_mii_ioctl(priv->phylink, rq, cmd);\n}\nEXPORT_SYMBOL_GPL(enetc_ioctl);\n\nint enetc_alloc_msix(struct enetc_ndev_priv *priv)\n{\n\tstruct pci_dev *pdev = priv->si->pdev;\n\tint num_stack_tx_queues;\n\tint first_xdp_tx_ring;\n\tint i, n, err, nvec;\n\tint v_tx_rings;\n\n\tnvec = ENETC_BDR_INT_BASE_IDX + priv->bdr_int_num;\n\t \n\tn = pci_alloc_irq_vectors(pdev, nvec, nvec, PCI_IRQ_MSIX);\n\n\tif (n < 0)\n\t\treturn n;\n\n\tif (n != nvec)\n\t\treturn -EPERM;\n\n\t \n\tv_tx_rings = priv->num_tx_rings / priv->bdr_int_num;\n\n\tfor (i = 0; i < priv->bdr_int_num; i++) {\n\t\tstruct enetc_int_vector *v;\n\t\tstruct enetc_bdr *bdr;\n\t\tint j;\n\n\t\tv = kzalloc(struct_size(v, tx_ring, v_tx_rings), GFP_KERNEL);\n\t\tif (!v) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tpriv->int_vector[i] = v;\n\n\t\tbdr = &v->rx_ring;\n\t\tbdr->index = i;\n\t\tbdr->ndev = priv->ndev;\n\t\tbdr->dev = priv->dev;\n\t\tbdr->bd_count = priv->rx_bd_count;\n\t\tbdr->buffer_offset = ENETC_RXB_PAD;\n\t\tpriv->rx_ring[i] = bdr;\n\n\t\terr = xdp_rxq_info_reg(&bdr->xdp.rxq, priv->ndev, i, 0);\n\t\tif (err) {\n\t\t\tkfree(v);\n\t\t\tgoto fail;\n\t\t}\n\n\t\terr = xdp_rxq_info_reg_mem_model(&bdr->xdp.rxq,\n\t\t\t\t\t\t MEM_TYPE_PAGE_SHARED, NULL);\n\t\tif (err) {\n\t\t\txdp_rxq_info_unreg(&bdr->xdp.rxq);\n\t\t\tkfree(v);\n\t\t\tgoto fail;\n\t\t}\n\n\t\t \n\t\tif (priv->ic_mode & ENETC_IC_RX_ADAPTIVE) {\n\t\t\tv->rx_ictt = 0x1;\n\t\t\tv->rx_dim_en = true;\n\t\t}\n\t\tINIT_WORK(&v->rx_dim.work, enetc_rx_dim_work);\n\t\tnetif_napi_add(priv->ndev, &v->napi, enetc_poll);\n\t\tv->count_tx_rings = v_tx_rings;\n\n\t\tfor (j = 0; j < v_tx_rings; j++) {\n\t\t\tint idx;\n\n\t\t\t \n\t\t\tidx = priv->bdr_int_num * j + i;\n\t\t\t__set_bit(idx, &v->tx_rings_map);\n\t\t\tbdr = &v->tx_ring[j];\n\t\t\tbdr->index = idx;\n\t\t\tbdr->ndev = priv->ndev;\n\t\t\tbdr->dev = priv->dev;\n\t\t\tbdr->bd_count = priv->tx_bd_count;\n\t\t\tpriv->tx_ring[idx] = bdr;\n\t\t}\n\t}\n\n\tnum_stack_tx_queues = enetc_num_stack_tx_queues(priv);\n\n\terr = netif_set_real_num_tx_queues(priv->ndev, num_stack_tx_queues);\n\tif (err)\n\t\tgoto fail;\n\n\terr = netif_set_real_num_rx_queues(priv->ndev, priv->num_rx_rings);\n\tif (err)\n\t\tgoto fail;\n\n\tpriv->min_num_stack_tx_queues = num_possible_cpus();\n\tfirst_xdp_tx_ring = priv->num_tx_rings - num_possible_cpus();\n\tpriv->xdp_tx_ring = &priv->tx_ring[first_xdp_tx_ring];\n\n\treturn 0;\n\nfail:\n\twhile (i--) {\n\t\tstruct enetc_int_vector *v = priv->int_vector[i];\n\t\tstruct enetc_bdr *rx_ring = &v->rx_ring;\n\n\t\txdp_rxq_info_unreg_mem_model(&rx_ring->xdp.rxq);\n\t\txdp_rxq_info_unreg(&rx_ring->xdp.rxq);\n\t\tnetif_napi_del(&v->napi);\n\t\tcancel_work_sync(&v->rx_dim.work);\n\t\tkfree(v);\n\t}\n\n\tpci_free_irq_vectors(pdev);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(enetc_alloc_msix);\n\nvoid enetc_free_msix(struct enetc_ndev_priv *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->bdr_int_num; i++) {\n\t\tstruct enetc_int_vector *v = priv->int_vector[i];\n\t\tstruct enetc_bdr *rx_ring = &v->rx_ring;\n\n\t\txdp_rxq_info_unreg_mem_model(&rx_ring->xdp.rxq);\n\t\txdp_rxq_info_unreg(&rx_ring->xdp.rxq);\n\t\tnetif_napi_del(&v->napi);\n\t\tcancel_work_sync(&v->rx_dim.work);\n\t}\n\n\tfor (i = 0; i < priv->num_rx_rings; i++)\n\t\tpriv->rx_ring[i] = NULL;\n\n\tfor (i = 0; i < priv->num_tx_rings; i++)\n\t\tpriv->tx_ring[i] = NULL;\n\n\tfor (i = 0; i < priv->bdr_int_num; i++) {\n\t\tkfree(priv->int_vector[i]);\n\t\tpriv->int_vector[i] = NULL;\n\t}\n\n\t \n\tpci_free_irq_vectors(priv->si->pdev);\n}\nEXPORT_SYMBOL_GPL(enetc_free_msix);\n\nstatic void enetc_kfree_si(struct enetc_si *si)\n{\n\tchar *p = (char *)si - si->pad;\n\n\tkfree(p);\n}\n\nstatic void enetc_detect_errata(struct enetc_si *si)\n{\n\tif (si->pdev->revision == ENETC_REV1)\n\t\tsi->errata = ENETC_ERR_VLAN_ISOL | ENETC_ERR_UCMCSWP;\n}\n\nint enetc_pci_probe(struct pci_dev *pdev, const char *name, int sizeof_priv)\n{\n\tstruct enetc_si *si, *p;\n\tstruct enetc_hw *hw;\n\tsize_t alloc_size;\n\tint err, len;\n\n\tpcie_flr(pdev);\n\terr = pci_enable_device_mem(pdev);\n\tif (err)\n\t\treturn dev_err_probe(&pdev->dev, err, \"device enable failed\\n\");\n\n\t \n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"DMA configuration failed: 0x%x\\n\", err);\n\t\tgoto err_dma;\n\t}\n\n\terr = pci_request_mem_regions(pdev, name);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"pci_request_regions failed err=%d\\n\", err);\n\t\tgoto err_pci_mem_reg;\n\t}\n\n\tpci_set_master(pdev);\n\n\talloc_size = sizeof(struct enetc_si);\n\tif (sizeof_priv) {\n\t\t \n\t\talloc_size = ALIGN(alloc_size, ENETC_SI_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t \n\talloc_size += ENETC_SI_ALIGN - 1;\n\n\tp = kzalloc(alloc_size, GFP_KERNEL);\n\tif (!p) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_si;\n\t}\n\n\tsi = PTR_ALIGN(p, ENETC_SI_ALIGN);\n\tsi->pad = (char *)si - (char *)p;\n\n\tpci_set_drvdata(pdev, si);\n\tsi->pdev = pdev;\n\thw = &si->hw;\n\n\tlen = pci_resource_len(pdev, ENETC_BAR_REGS);\n\thw->reg = ioremap(pci_resource_start(pdev, ENETC_BAR_REGS), len);\n\tif (!hw->reg) {\n\t\terr = -ENXIO;\n\t\tdev_err(&pdev->dev, \"ioremap() failed\\n\");\n\t\tgoto err_ioremap;\n\t}\n\tif (len > ENETC_PORT_BASE)\n\t\thw->port = hw->reg + ENETC_PORT_BASE;\n\tif (len > ENETC_GLOBAL_BASE)\n\t\thw->global = hw->reg + ENETC_GLOBAL_BASE;\n\n\tenetc_detect_errata(si);\n\n\treturn 0;\n\nerr_ioremap:\n\tenetc_kfree_si(si);\nerr_alloc_si:\n\tpci_release_mem_regions(pdev);\nerr_pci_mem_reg:\nerr_dma:\n\tpci_disable_device(pdev);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(enetc_pci_probe);\n\nvoid enetc_pci_remove(struct pci_dev *pdev)\n{\n\tstruct enetc_si *si = pci_get_drvdata(pdev);\n\tstruct enetc_hw *hw = &si->hw;\n\n\tiounmap(hw->reg);\n\tenetc_kfree_si(si);\n\tpci_release_mem_regions(pdev);\n\tpci_disable_device(pdev);\n}\nEXPORT_SYMBOL_GPL(enetc_pci_remove);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}