{
  "module_name": "gianfar.c",
  "hash_id": "58e901d5fe07472c5573499156b7af8553378c492bb79724c5653a9218ef415a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/freescale/gianfar.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kernel.h>\n#include <linux/platform_device.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/unistd.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/delay.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/skbuff.h>\n#include <linux/if_vlan.h>\n#include <linux/spinlock.h>\n#include <linux/mm.h>\n#include <linux/of_address.h>\n#include <linux/of_irq.h>\n#include <linux/of_mdio.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/in.h>\n#include <linux/net_tstamp.h>\n\n#include <asm/io.h>\n#ifdef CONFIG_PPC\n#include <asm/reg.h>\n#include <asm/mpc85xx.h>\n#endif\n#include <asm/irq.h>\n#include <linux/uaccess.h>\n#include <linux/module.h>\n#include <linux/dma-mapping.h>\n#include <linux/crc32.h>\n#include <linux/mii.h>\n#include <linux/phy.h>\n#include <linux/phy_fixed.h>\n#include <linux/of.h>\n#include <linux/of_net.h>\n\n#include \"gianfar.h\"\n\n#define TX_TIMEOUT      (5*HZ)\n\nMODULE_AUTHOR(\"Freescale Semiconductor, Inc\");\nMODULE_DESCRIPTION(\"Gianfar Ethernet Driver\");\nMODULE_LICENSE(\"GPL\");\n\nstatic void gfar_init_rxbdp(struct gfar_priv_rx_q *rx_queue, struct rxbd8 *bdp,\n\t\t\t    dma_addr_t buf)\n{\n\tu32 lstatus;\n\n\tbdp->bufPtr = cpu_to_be32(buf);\n\n\tlstatus = BD_LFLAG(RXBD_EMPTY | RXBD_INTERRUPT);\n\tif (bdp == rx_queue->rx_bd_base + rx_queue->rx_ring_size - 1)\n\t\tlstatus |= BD_LFLAG(RXBD_WRAP);\n\n\tgfar_wmb();\n\n\tbdp->lstatus = cpu_to_be32(lstatus);\n}\n\nstatic void gfar_init_tx_rx_base(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 __iomem *baddr;\n\tint i;\n\n\tbaddr = &regs->tbase0;\n\tfor (i = 0; i < priv->num_tx_queues; i++) {\n\t\tgfar_write(baddr, priv->tx_queue[i]->tx_bd_dma_base);\n\t\tbaddr += 2;\n\t}\n\n\tbaddr = &regs->rbase0;\n\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\tgfar_write(baddr, priv->rx_queue[i]->rx_bd_dma_base);\n\t\tbaddr += 2;\n\t}\n}\n\nstatic void gfar_init_rqprm(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 __iomem *baddr;\n\tint i;\n\n\tbaddr = &regs->rqprm0;\n\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\tgfar_write(baddr, priv->rx_queue[i]->rx_ring_size |\n\t\t\t   (DEFAULT_RX_LFC_THR << FBTHR_SHIFT));\n\t\tbaddr++;\n\t}\n}\n\nstatic void gfar_rx_offload_en(struct gfar_private *priv)\n{\n\t \n\tpriv->uses_rxfcb = 0;\n\n\tif (priv->ndev->features & (NETIF_F_RXCSUM | NETIF_F_HW_VLAN_CTAG_RX))\n\t\tpriv->uses_rxfcb = 1;\n\n\tif (priv->hwts_rx_en || priv->rx_filer_enable)\n\t\tpriv->uses_rxfcb = 1;\n}\n\nstatic void gfar_mac_rx_config(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 rctrl = 0;\n\n\tif (priv->rx_filer_enable) {\n\t\trctrl |= RCTRL_FILREN | RCTRL_PRSDEP_INIT;\n\t\t \n\t\tgfar_write(&regs->rir0, DEFAULT_2RXQ_RIR0);\n\t}\n\n\t \n\tif (priv->ndev->flags & IFF_PROMISC)\n\t\trctrl |= RCTRL_PROM;\n\n\tif (priv->ndev->features & NETIF_F_RXCSUM)\n\t\trctrl |= RCTRL_CHECKSUMMING;\n\n\tif (priv->extended_hash)\n\t\trctrl |= RCTRL_EXTHASH | RCTRL_EMEN;\n\n\tif (priv->padding) {\n\t\trctrl &= ~RCTRL_PAL_MASK;\n\t\trctrl |= RCTRL_PADDING(priv->padding);\n\t}\n\n\t \n\tif (priv->hwts_rx_en)\n\t\trctrl |= RCTRL_PRSDEP_INIT | RCTRL_TS_ENABLE;\n\n\tif (priv->ndev->features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\trctrl |= RCTRL_VLEX | RCTRL_PRSDEP_INIT;\n\n\t \n\tgfar_write(&regs->rctrl, rctrl);\n\t \n\tgfar_init_rqprm(priv);\n\tgfar_write(&regs->ptv, DEFAULT_LFC_PTVVAL);\n\trctrl |= RCTRL_LFC;\n\n\t \n\tgfar_write(&regs->rctrl, rctrl);\n}\n\nstatic void gfar_mac_tx_config(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tctrl = 0;\n\n\tif (priv->ndev->features & NETIF_F_IP_CSUM)\n\t\ttctrl |= TCTRL_INIT_CSUM;\n\n\tif (priv->prio_sched_en)\n\t\ttctrl |= TCTRL_TXSCHED_PRIO;\n\telse {\n\t\ttctrl |= TCTRL_TXSCHED_WRRS;\n\t\tgfar_write(&regs->tr03wt, DEFAULT_WRRS_WEIGHT);\n\t\tgfar_write(&regs->tr47wt, DEFAULT_WRRS_WEIGHT);\n\t}\n\n\tif (priv->ndev->features & NETIF_F_HW_VLAN_CTAG_TX)\n\t\ttctrl |= TCTRL_VLINS;\n\n\tgfar_write(&regs->tctrl, tctrl);\n}\n\nstatic void gfar_configure_coalescing(struct gfar_private *priv,\n\t\t\t       unsigned long tx_mask, unsigned long rx_mask)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 __iomem *baddr;\n\n\tif (priv->mode == MQ_MG_MODE) {\n\t\tint i = 0;\n\n\t\tbaddr = &regs->txic0;\n\t\tfor_each_set_bit(i, &tx_mask, priv->num_tx_queues) {\n\t\t\tgfar_write(baddr + i, 0);\n\t\t\tif (likely(priv->tx_queue[i]->txcoalescing))\n\t\t\t\tgfar_write(baddr + i, priv->tx_queue[i]->txic);\n\t\t}\n\n\t\tbaddr = &regs->rxic0;\n\t\tfor_each_set_bit(i, &rx_mask, priv->num_rx_queues) {\n\t\t\tgfar_write(baddr + i, 0);\n\t\t\tif (likely(priv->rx_queue[i]->rxcoalescing))\n\t\t\t\tgfar_write(baddr + i, priv->rx_queue[i]->rxic);\n\t\t}\n\t} else {\n\t\t \n\t\tgfar_write(&regs->txic, 0);\n\t\tif (likely(priv->tx_queue[0]->txcoalescing))\n\t\t\tgfar_write(&regs->txic, priv->tx_queue[0]->txic);\n\n\t\tgfar_write(&regs->rxic, 0);\n\t\tif (unlikely(priv->rx_queue[0]->rxcoalescing))\n\t\t\tgfar_write(&regs->rxic, priv->rx_queue[0]->rxic);\n\t}\n}\n\nstatic void gfar_configure_coalescing_all(struct gfar_private *priv)\n{\n\tgfar_configure_coalescing(priv, 0xFF, 0xFF);\n}\n\nstatic void gfar_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\tstats->rx_packets += priv->rx_queue[i]->stats.rx_packets;\n\t\tstats->rx_bytes   += priv->rx_queue[i]->stats.rx_bytes;\n\t\tstats->rx_dropped += priv->rx_queue[i]->stats.rx_dropped;\n\t}\n\n\tfor (i = 0; i < priv->num_tx_queues; i++) {\n\t\tstats->tx_bytes += priv->tx_queue[i]->stats.tx_bytes;\n\t\tstats->tx_packets += priv->tx_queue[i]->stats.tx_packets;\n\t}\n\n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_RMON) {\n\t\tstruct rmon_mib __iomem *rmon = &priv->gfargrp[0].regs->rmon;\n\t\tunsigned long flags;\n\t\tu32 rdrp, car, car_before;\n\t\tu64 rdrp_offset;\n\n\t\tspin_lock_irqsave(&priv->rmon_overflow.lock, flags);\n\t\tcar = gfar_read(&rmon->car1) & CAR1_C1RDR;\n\t\tdo {\n\t\t\tcar_before = car;\n\t\t\trdrp = gfar_read(&rmon->rdrp);\n\t\t\tcar = gfar_read(&rmon->car1) & CAR1_C1RDR;\n\t\t} while (car != car_before);\n\t\tif (car) {\n\t\t\tpriv->rmon_overflow.rdrp++;\n\t\t\tgfar_write(&rmon->car1, car);\n\t\t}\n\t\trdrp_offset = priv->rmon_overflow.rdrp;\n\t\tspin_unlock_irqrestore(&priv->rmon_overflow.lock, flags);\n\n\t\tstats->rx_missed_errors = rdrp + (rdrp_offset << 16);\n\t}\n}\n\n \n \nstatic void gfar_set_hash_for_addr(struct net_device *dev, u8 *addr)\n{\n\tu32 tempval;\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tu32 result = ether_crc(ETH_ALEN, addr);\n\tint width = priv->hash_width;\n\tu8 whichbit = (result >> (32 - width)) & 0x1f;\n\tu8 whichreg = result >> (32 - width + 5);\n\tu32 value = (1 << (31-whichbit));\n\n\ttempval = gfar_read(priv->hash_regs[whichreg]);\n\ttempval |= value;\n\tgfar_write(priv->hash_regs[whichreg], tempval);\n}\n\n \nstatic void gfar_set_mac_for_addr(struct net_device *dev, int num,\n\t\t\t\t  const u8 *addr)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tempval;\n\tu32 __iomem *macptr = &regs->macstnaddr1;\n\n\tmacptr += num*2;\n\n\t \n\ttempval = (addr[5] << 24) | (addr[4] << 16) |\n\t\t  (addr[3] << 8)  |  addr[2];\n\n\tgfar_write(macptr, tempval);\n\n\ttempval = (addr[1] << 24) | (addr[0] << 16);\n\n\tgfar_write(macptr+1, tempval);\n}\n\nstatic int gfar_set_mac_addr(struct net_device *dev, void *p)\n{\n\tint ret;\n\n\tret = eth_mac_addr(dev, p);\n\tif (ret)\n\t\treturn ret;\n\n\tgfar_set_mac_for_addr(dev, 0, dev->dev_addr);\n\n\treturn 0;\n}\n\nstatic void gfar_ints_disable(struct gfar_private *priv)\n{\n\tint i;\n\tfor (i = 0; i < priv->num_grps; i++) {\n\t\tstruct gfar __iomem *regs = priv->gfargrp[i].regs;\n\t\t \n\t\tgfar_write(&regs->ievent, IEVENT_INIT_CLEAR);\n\n\t\t \n\t\tgfar_write(&regs->imask, IMASK_INIT_CLEAR);\n\t}\n}\n\nstatic void gfar_ints_enable(struct gfar_private *priv)\n{\n\tint i;\n\tfor (i = 0; i < priv->num_grps; i++) {\n\t\tstruct gfar __iomem *regs = priv->gfargrp[i].regs;\n\t\t \n\t\tgfar_write(&regs->imask,\n\t\t\t   IMASK_DEFAULT | priv->rmon_overflow.imask);\n\t}\n}\n\nstatic int gfar_alloc_tx_queues(struct gfar_private *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_tx_queues; i++) {\n\t\tpriv->tx_queue[i] = kzalloc(sizeof(struct gfar_priv_tx_q),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!priv->tx_queue[i])\n\t\t\treturn -ENOMEM;\n\n\t\tpriv->tx_queue[i]->tx_skbuff = NULL;\n\t\tpriv->tx_queue[i]->qindex = i;\n\t\tpriv->tx_queue[i]->dev = priv->ndev;\n\t\tspin_lock_init(&(priv->tx_queue[i]->txlock));\n\t}\n\treturn 0;\n}\n\nstatic int gfar_alloc_rx_queues(struct gfar_private *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\tpriv->rx_queue[i] = kzalloc(sizeof(struct gfar_priv_rx_q),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!priv->rx_queue[i])\n\t\t\treturn -ENOMEM;\n\n\t\tpriv->rx_queue[i]->qindex = i;\n\t\tpriv->rx_queue[i]->ndev = priv->ndev;\n\t}\n\treturn 0;\n}\n\nstatic void gfar_free_tx_queues(struct gfar_private *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_tx_queues; i++)\n\t\tkfree(priv->tx_queue[i]);\n}\n\nstatic void gfar_free_rx_queues(struct gfar_private *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_rx_queues; i++)\n\t\tkfree(priv->rx_queue[i]);\n}\n\nstatic void unmap_group_regs(struct gfar_private *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < MAXGROUPS; i++)\n\t\tif (priv->gfargrp[i].regs)\n\t\t\tiounmap(priv->gfargrp[i].regs);\n}\n\nstatic void free_gfar_dev(struct gfar_private *priv)\n{\n\tint i, j;\n\n\tfor (i = 0; i < priv->num_grps; i++)\n\t\tfor (j = 0; j < GFAR_NUM_IRQS; j++) {\n\t\t\tkfree(priv->gfargrp[i].irqinfo[j]);\n\t\t\tpriv->gfargrp[i].irqinfo[j] = NULL;\n\t\t}\n\n\tfree_netdev(priv->ndev);\n}\n\nstatic void disable_napi(struct gfar_private *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_grps; i++) {\n\t\tnapi_disable(&priv->gfargrp[i].napi_rx);\n\t\tnapi_disable(&priv->gfargrp[i].napi_tx);\n\t}\n}\n\nstatic void enable_napi(struct gfar_private *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_grps; i++) {\n\t\tnapi_enable(&priv->gfargrp[i].napi_rx);\n\t\tnapi_enable(&priv->gfargrp[i].napi_tx);\n\t}\n}\n\nstatic int gfar_parse_group(struct device_node *np,\n\t\t\t    struct gfar_private *priv, const char *model)\n{\n\tstruct gfar_priv_grp *grp = &priv->gfargrp[priv->num_grps];\n\tint i;\n\n\tfor (i = 0; i < GFAR_NUM_IRQS; i++) {\n\t\tgrp->irqinfo[i] = kzalloc(sizeof(struct gfar_irqinfo),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!grp->irqinfo[i])\n\t\t\treturn -ENOMEM;\n\t}\n\n\tgrp->regs = of_iomap(np, 0);\n\tif (!grp->regs)\n\t\treturn -ENOMEM;\n\n\tgfar_irq(grp, TX)->irq = irq_of_parse_and_map(np, 0);\n\n\t \n\tif (model && strcasecmp(model, \"FEC\")) {\n\t\tgfar_irq(grp, RX)->irq = irq_of_parse_and_map(np, 1);\n\t\tgfar_irq(grp, ER)->irq = irq_of_parse_and_map(np, 2);\n\t\tif (!gfar_irq(grp, TX)->irq ||\n\t\t    !gfar_irq(grp, RX)->irq ||\n\t\t    !gfar_irq(grp, ER)->irq)\n\t\t\treturn -EINVAL;\n\t}\n\n\tgrp->priv = priv;\n\tspin_lock_init(&grp->grplock);\n\tif (priv->mode == MQ_MG_MODE) {\n\t\t \n\t\tgrp->rx_bit_map = (DEFAULT_MAPPING >> priv->num_grps);\n\t\tgrp->tx_bit_map = (DEFAULT_MAPPING >> priv->num_grps);\n\t} else {\n\t\tgrp->rx_bit_map = 0xFF;\n\t\tgrp->tx_bit_map = 0xFF;\n\t}\n\n\t \n\tgrp->rx_bit_map = bitrev8(grp->rx_bit_map);\n\tgrp->tx_bit_map = bitrev8(grp->tx_bit_map);\n\n\t \n\tfor_each_set_bit(i, &grp->rx_bit_map, priv->num_rx_queues) {\n\t\tif (!grp->rx_queue)\n\t\t\tgrp->rx_queue = priv->rx_queue[i];\n\t\tgrp->num_rx_queues++;\n\t\tgrp->rstat |= (RSTAT_CLEAR_RHALT >> i);\n\t\tpriv->rqueue |= ((RQUEUE_EN0 | RQUEUE_EX0) >> i);\n\t\tpriv->rx_queue[i]->grp = grp;\n\t}\n\n\tfor_each_set_bit(i, &grp->tx_bit_map, priv->num_tx_queues) {\n\t\tif (!grp->tx_queue)\n\t\t\tgrp->tx_queue = priv->tx_queue[i];\n\t\tgrp->num_tx_queues++;\n\t\tgrp->tstat |= (TSTAT_CLEAR_THALT >> i);\n\t\tpriv->tqueue |= (TQUEUE_EN0 >> i);\n\t\tpriv->tx_queue[i]->grp = grp;\n\t}\n\n\tpriv->num_grps++;\n\n\treturn 0;\n}\n\nstatic int gfar_of_group_count(struct device_node *np)\n{\n\tstruct device_node *child;\n\tint num = 0;\n\n\tfor_each_available_child_of_node(np, child)\n\t\tif (of_node_name_eq(child, \"queue-group\"))\n\t\t\tnum++;\n\n\treturn num;\n}\n\n \nstatic phy_interface_t gfar_get_interface(struct net_device *dev)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 ecntrl;\n\n\tecntrl = gfar_read(&regs->ecntrl);\n\n\tif (ecntrl & ECNTRL_SGMII_MODE)\n\t\treturn PHY_INTERFACE_MODE_SGMII;\n\n\tif (ecntrl & ECNTRL_TBI_MODE) {\n\t\tif (ecntrl & ECNTRL_REDUCED_MODE)\n\t\t\treturn PHY_INTERFACE_MODE_RTBI;\n\t\telse\n\t\t\treturn PHY_INTERFACE_MODE_TBI;\n\t}\n\n\tif (ecntrl & ECNTRL_REDUCED_MODE) {\n\t\tif (ecntrl & ECNTRL_REDUCED_MII_MODE) {\n\t\t\treturn PHY_INTERFACE_MODE_RMII;\n\t\t}\n\t\telse {\n\t\t\tphy_interface_t interface = priv->interface;\n\n\t\t\t \n\t\t\tif (interface == PHY_INTERFACE_MODE_RGMII_ID)\n\t\t\t\treturn PHY_INTERFACE_MODE_RGMII_ID;\n\n\t\t\treturn PHY_INTERFACE_MODE_RGMII;\n\t\t}\n\t}\n\n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_GIGABIT)\n\t\treturn PHY_INTERFACE_MODE_GMII;\n\n\treturn PHY_INTERFACE_MODE_MII;\n}\n\nstatic int gfar_of_init(struct platform_device *ofdev, struct net_device **pdev)\n{\n\tconst char *model;\n\tint err = 0, i;\n\tphy_interface_t interface;\n\tstruct net_device *dev = NULL;\n\tstruct gfar_private *priv = NULL;\n\tstruct device_node *np = ofdev->dev.of_node;\n\tstruct device_node *child = NULL;\n\tu32 stash_len = 0;\n\tu32 stash_idx = 0;\n\tunsigned int num_tx_qs, num_rx_qs;\n\tunsigned short mode;\n\n\tif (!np)\n\t\treturn -ENODEV;\n\n\tif (of_device_is_compatible(np, \"fsl,etsec2\"))\n\t\tmode = MQ_MG_MODE;\n\telse\n\t\tmode = SQ_SG_MODE;\n\n\tif (mode == SQ_SG_MODE) {\n\t\tnum_tx_qs = 1;\n\t\tnum_rx_qs = 1;\n\t} else {  \n\t\t \n\t\tunsigned int num_grps = gfar_of_group_count(np);\n\n\t\tif (num_grps == 0 || num_grps > MAXGROUPS) {\n\t\t\tdev_err(&ofdev->dev, \"Invalid # of int groups(%d)\\n\",\n\t\t\t\tnum_grps);\n\t\t\tpr_err(\"Cannot do alloc_etherdev, aborting\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tnum_tx_qs = num_grps;  \n\t\tnum_rx_qs = num_grps;  \n\t}\n\n\tif (num_tx_qs > MAX_TX_QS) {\n\t\tpr_err(\"num_tx_qs(=%d) greater than MAX_TX_QS(=%d)\\n\",\n\t\t       num_tx_qs, MAX_TX_QS);\n\t\tpr_err(\"Cannot do alloc_etherdev, aborting\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (num_rx_qs > MAX_RX_QS) {\n\t\tpr_err(\"num_rx_qs(=%d) greater than MAX_RX_QS(=%d)\\n\",\n\t\t       num_rx_qs, MAX_RX_QS);\n\t\tpr_err(\"Cannot do alloc_etherdev, aborting\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t*pdev = alloc_etherdev_mq(sizeof(*priv), num_tx_qs);\n\tdev = *pdev;\n\tif (NULL == dev)\n\t\treturn -ENOMEM;\n\n\tpriv = netdev_priv(dev);\n\tpriv->ndev = dev;\n\n\tpriv->mode = mode;\n\n\tpriv->num_tx_queues = num_tx_qs;\n\tnetif_set_real_num_rx_queues(dev, num_rx_qs);\n\tpriv->num_rx_queues = num_rx_qs;\n\n\terr = gfar_alloc_tx_queues(priv);\n\tif (err)\n\t\tgoto tx_alloc_failed;\n\n\terr = gfar_alloc_rx_queues(priv);\n\tif (err)\n\t\tgoto rx_alloc_failed;\n\n\terr = of_property_read_string(np, \"model\", &model);\n\tif (err) {\n\t\tpr_err(\"Device model property missing, aborting\\n\");\n\t\tgoto rx_alloc_failed;\n\t}\n\n\t \n\tINIT_LIST_HEAD(&priv->rx_list.list);\n\tpriv->rx_list.count = 0;\n\tmutex_init(&priv->rx_queue_access);\n\n\tfor (i = 0; i < MAXGROUPS; i++)\n\t\tpriv->gfargrp[i].regs = NULL;\n\n\t \n\tif (priv->mode == MQ_MG_MODE) {\n\t\tfor_each_available_child_of_node(np, child) {\n\t\t\tif (!of_node_name_eq(child, \"queue-group\"))\n\t\t\t\tcontinue;\n\n\t\t\terr = gfar_parse_group(child, priv, model);\n\t\t\tif (err) {\n\t\t\t\tof_node_put(child);\n\t\t\t\tgoto err_grp_init;\n\t\t\t}\n\t\t}\n\t} else {  \n\t\terr = gfar_parse_group(np, priv, model);\n\t\tif (err)\n\t\t\tgoto err_grp_init;\n\t}\n\n\tif (of_property_read_bool(np, \"bd-stash\")) {\n\t\tpriv->device_flags |= FSL_GIANFAR_DEV_HAS_BD_STASHING;\n\t\tpriv->bd_stash_en = 1;\n\t}\n\n\terr = of_property_read_u32(np, \"rx-stash-len\", &stash_len);\n\n\tif (err == 0)\n\t\tpriv->rx_stash_size = stash_len;\n\n\terr = of_property_read_u32(np, \"rx-stash-idx\", &stash_idx);\n\n\tif (err == 0)\n\t\tpriv->rx_stash_index = stash_idx;\n\n\tif (stash_len || stash_idx)\n\t\tpriv->device_flags |= FSL_GIANFAR_DEV_HAS_BUF_STASHING;\n\n\terr = of_get_ethdev_address(np, dev);\n\tif (err) {\n\t\teth_hw_addr_random(dev);\n\t\tdev_info(&ofdev->dev, \"Using random MAC address: %pM\\n\", dev->dev_addr);\n\t}\n\n\tif (model && !strcasecmp(model, \"TSEC\"))\n\t\tpriv->device_flags |= FSL_GIANFAR_DEV_HAS_GIGABIT |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_COALESCE |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_RMON |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_MULTI_INTR;\n\n\tif (model && !strcasecmp(model, \"eTSEC\"))\n\t\tpriv->device_flags |= FSL_GIANFAR_DEV_HAS_GIGABIT |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_COALESCE |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_RMON |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_MULTI_INTR |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_CSUM |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_VLAN |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_MAGIC_PACKET |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_EXTENDED_HASH |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_TIMER |\n\t\t\t\t     FSL_GIANFAR_DEV_HAS_RX_FILER;\n\n\t \n\terr = of_get_phy_mode(np, &interface);\n\tif (!err)\n\t\tpriv->interface = interface;\n\telse\n\t\tpriv->interface = gfar_get_interface(dev);\n\n\tif (of_property_read_bool(np, \"fsl,magic-packet\"))\n\t\tpriv->device_flags |= FSL_GIANFAR_DEV_HAS_MAGIC_PACKET;\n\n\tif (of_property_read_bool(np, \"fsl,wake-on-filer\"))\n\t\tpriv->device_flags |= FSL_GIANFAR_DEV_HAS_WAKE_ON_FILER;\n\n\tpriv->phy_node = of_parse_phandle(np, \"phy-handle\", 0);\n\n\t \n\tif (!priv->phy_node && of_phy_is_fixed_link(np)) {\n\t\terr = of_phy_register_fixed_link(np);\n\t\tif (err)\n\t\t\tgoto err_grp_init;\n\n\t\tpriv->phy_node = of_node_get(np);\n\t}\n\n\t \n\tpriv->tbi_node = of_parse_phandle(np, \"tbi-handle\", 0);\n\n\treturn 0;\n\nerr_grp_init:\n\tunmap_group_regs(priv);\nrx_alloc_failed:\n\tgfar_free_rx_queues(priv);\ntx_alloc_failed:\n\tgfar_free_tx_queues(priv);\n\tfree_gfar_dev(priv);\n\treturn err;\n}\n\nstatic u32 cluster_entry_per_class(struct gfar_private *priv, u32 rqfar,\n\t\t\t\t   u32 class)\n{\n\tu32 rqfpr = FPR_FILER_MASK;\n\tu32 rqfcr = 0x0;\n\n\trqfar--;\n\trqfcr = RQFCR_CLE | RQFCR_PID_MASK | RQFCR_CMP_EXACT;\n\tpriv->ftp_rqfpr[rqfar] = rqfpr;\n\tpriv->ftp_rqfcr[rqfar] = rqfcr;\n\tgfar_write_filer(priv, rqfar, rqfcr, rqfpr);\n\n\trqfar--;\n\trqfcr = RQFCR_CMP_NOMATCH;\n\tpriv->ftp_rqfpr[rqfar] = rqfpr;\n\tpriv->ftp_rqfcr[rqfar] = rqfcr;\n\tgfar_write_filer(priv, rqfar, rqfcr, rqfpr);\n\n\trqfar--;\n\trqfcr = RQFCR_CMP_EXACT | RQFCR_PID_PARSE | RQFCR_CLE | RQFCR_AND;\n\trqfpr = class;\n\tpriv->ftp_rqfcr[rqfar] = rqfcr;\n\tpriv->ftp_rqfpr[rqfar] = rqfpr;\n\tgfar_write_filer(priv, rqfar, rqfcr, rqfpr);\n\n\trqfar--;\n\trqfcr = RQFCR_CMP_EXACT | RQFCR_PID_MASK | RQFCR_AND;\n\trqfpr = class;\n\tpriv->ftp_rqfcr[rqfar] = rqfcr;\n\tpriv->ftp_rqfpr[rqfar] = rqfpr;\n\tgfar_write_filer(priv, rqfar, rqfcr, rqfpr);\n\n\treturn rqfar;\n}\n\nstatic void gfar_init_filer_table(struct gfar_private *priv)\n{\n\tint i = 0x0;\n\tu32 rqfar = MAX_FILER_IDX;\n\tu32 rqfcr = 0x0;\n\tu32 rqfpr = FPR_FILER_MASK;\n\n\t \n\trqfcr = RQFCR_CMP_MATCH;\n\tpriv->ftp_rqfcr[rqfar] = rqfcr;\n\tpriv->ftp_rqfpr[rqfar] = rqfpr;\n\tgfar_write_filer(priv, rqfar, rqfcr, rqfpr);\n\n\trqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV6);\n\trqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV6 | RQFPR_UDP);\n\trqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV6 | RQFPR_TCP);\n\trqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV4);\n\trqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV4 | RQFPR_UDP);\n\trqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV4 | RQFPR_TCP);\n\n\t \n\tpriv->cur_filer_idx = rqfar;\n\n\t \n\trqfcr = RQFCR_CMP_NOMATCH;\n\tfor (i = 0; i < rqfar; i++) {\n\t\tpriv->ftp_rqfcr[i] = rqfcr;\n\t\tpriv->ftp_rqfpr[i] = rqfpr;\n\t\tgfar_write_filer(priv, i, rqfcr, rqfpr);\n\t}\n}\n\n#ifdef CONFIG_PPC\nstatic void __gfar_detect_errata_83xx(struct gfar_private *priv)\n{\n\tunsigned int pvr = mfspr(SPRN_PVR);\n\tunsigned int svr = mfspr(SPRN_SVR);\n\tunsigned int mod = (svr >> 16) & 0xfff6;  \n\tunsigned int rev = svr & 0xffff;\n\n\t \n\tif ((pvr == 0x80850010 && mod == 0x80b0 && rev >= 0x0020) ||\n\t    (pvr == 0x80861010 && (mod & 0xfff9) == 0x80c0))\n\t\tpriv->errata |= GFAR_ERRATA_74;\n\n\t \n\tif ((pvr == 0x80850010 && mod == 0x80b0) ||\n\t    (pvr == 0x80861010 && (mod & 0xfff9) == 0x80c0))\n\t\tpriv->errata |= GFAR_ERRATA_76;\n\n\t \n\tif (pvr == 0x80850010 && mod == 0x80b0 && rev < 0x0020)\n\t\tpriv->errata |= GFAR_ERRATA_12;\n}\n\nstatic void __gfar_detect_errata_85xx(struct gfar_private *priv)\n{\n\tunsigned int svr = mfspr(SPRN_SVR);\n\n\tif ((SVR_SOC_VER(svr) == SVR_8548) && (SVR_REV(svr) == 0x20))\n\t\tpriv->errata |= GFAR_ERRATA_12;\n\t \n\tif (((SVR_SOC_VER(svr) == SVR_P2020) && (SVR_REV(svr) < 0x20)) ||\n\t    ((SVR_SOC_VER(svr) == SVR_P2010) && (SVR_REV(svr) < 0x20)) ||\n\t    ((SVR_SOC_VER(svr) == SVR_8548) && (SVR_REV(svr) < 0x31)))\n\t\tpriv->errata |= GFAR_ERRATA_76;  \n}\n#endif\n\nstatic void gfar_detect_errata(struct gfar_private *priv)\n{\n\tstruct device *dev = &priv->ofdev->dev;\n\n\t \n\tpriv->errata |= GFAR_ERRATA_A002;\n\n#ifdef CONFIG_PPC\n\tif (pvr_version_is(PVR_VER_E500V1) || pvr_version_is(PVR_VER_E500V2))\n\t\t__gfar_detect_errata_85xx(priv);\n\telse  \n\t\t__gfar_detect_errata_83xx(priv);\n#endif\n\n\tif (priv->errata)\n\t\tdev_info(dev, \"enabled errata workarounds, flags: 0x%x\\n\",\n\t\t\t priv->errata);\n}\n\nstatic void gfar_init_addr_hash_table(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_EXTENDED_HASH) {\n\t\tpriv->extended_hash = 1;\n\t\tpriv->hash_width = 9;\n\n\t\tpriv->hash_regs[0] = &regs->igaddr0;\n\t\tpriv->hash_regs[1] = &regs->igaddr1;\n\t\tpriv->hash_regs[2] = &regs->igaddr2;\n\t\tpriv->hash_regs[3] = &regs->igaddr3;\n\t\tpriv->hash_regs[4] = &regs->igaddr4;\n\t\tpriv->hash_regs[5] = &regs->igaddr5;\n\t\tpriv->hash_regs[6] = &regs->igaddr6;\n\t\tpriv->hash_regs[7] = &regs->igaddr7;\n\t\tpriv->hash_regs[8] = &regs->gaddr0;\n\t\tpriv->hash_regs[9] = &regs->gaddr1;\n\t\tpriv->hash_regs[10] = &regs->gaddr2;\n\t\tpriv->hash_regs[11] = &regs->gaddr3;\n\t\tpriv->hash_regs[12] = &regs->gaddr4;\n\t\tpriv->hash_regs[13] = &regs->gaddr5;\n\t\tpriv->hash_regs[14] = &regs->gaddr6;\n\t\tpriv->hash_regs[15] = &regs->gaddr7;\n\n\t} else {\n\t\tpriv->extended_hash = 0;\n\t\tpriv->hash_width = 8;\n\n\t\tpriv->hash_regs[0] = &regs->gaddr0;\n\t\tpriv->hash_regs[1] = &regs->gaddr1;\n\t\tpriv->hash_regs[2] = &regs->gaddr2;\n\t\tpriv->hash_regs[3] = &regs->gaddr3;\n\t\tpriv->hash_regs[4] = &regs->gaddr4;\n\t\tpriv->hash_regs[5] = &regs->gaddr5;\n\t\tpriv->hash_regs[6] = &regs->gaddr6;\n\t\tpriv->hash_regs[7] = &regs->gaddr7;\n\t}\n}\n\nstatic int __gfar_is_rx_idle(struct gfar_private *priv)\n{\n\tu32 res;\n\n\t \n\tif (!gfar_has_errata(priv, GFAR_ERRATA_A002))\n\t\treturn 0;\n\n\t \n\tres = gfar_read((void __iomem *)priv->gfargrp[0].regs + 0xd1c);\n\tres &= 0x7f807f80;\n\tif ((res & 0xffff) == (res >> 16))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic void gfar_halt_nodisable(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tempval;\n\tunsigned int timeout;\n\tint stopped;\n\n\tgfar_ints_disable(priv);\n\n\tif (gfar_is_dma_stopped(priv))\n\t\treturn;\n\n\t \n\ttempval = gfar_read(&regs->dmactrl);\n\ttempval |= (DMACTRL_GRS | DMACTRL_GTS);\n\tgfar_write(&regs->dmactrl, tempval);\n\nretry:\n\ttimeout = 1000;\n\twhile (!(stopped = gfar_is_dma_stopped(priv)) && timeout) {\n\t\tcpu_relax();\n\t\ttimeout--;\n\t}\n\n\tif (!timeout)\n\t\tstopped = gfar_is_dma_stopped(priv);\n\n\tif (!stopped && !gfar_is_rx_dma_stopped(priv) &&\n\t    !__gfar_is_rx_idle(priv))\n\t\tgoto retry;\n}\n\n \nstatic void gfar_halt(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tempval;\n\n\t \n\tgfar_write(&regs->rqueue, 0);\n\tgfar_write(&regs->tqueue, 0);\n\n\tmdelay(10);\n\n\tgfar_halt_nodisable(priv);\n\n\t \n\ttempval = gfar_read(&regs->maccfg1);\n\ttempval &= ~(MACCFG1_RX_EN | MACCFG1_TX_EN);\n\tgfar_write(&regs->maccfg1, tempval);\n}\n\nstatic void free_skb_tx_queue(struct gfar_priv_tx_q *tx_queue)\n{\n\tstruct txbd8 *txbdp;\n\tstruct gfar_private *priv = netdev_priv(tx_queue->dev);\n\tint i, j;\n\n\ttxbdp = tx_queue->tx_bd_base;\n\n\tfor (i = 0; i < tx_queue->tx_ring_size; i++) {\n\t\tif (!tx_queue->tx_skbuff[i])\n\t\t\tcontinue;\n\n\t\tdma_unmap_single(priv->dev, be32_to_cpu(txbdp->bufPtr),\n\t\t\t\t be16_to_cpu(txbdp->length), DMA_TO_DEVICE);\n\t\ttxbdp->lstatus = 0;\n\t\tfor (j = 0; j < skb_shinfo(tx_queue->tx_skbuff[i])->nr_frags;\n\t\t     j++) {\n\t\t\ttxbdp++;\n\t\t\tdma_unmap_page(priv->dev, be32_to_cpu(txbdp->bufPtr),\n\t\t\t\t       be16_to_cpu(txbdp->length),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\t}\n\t\ttxbdp++;\n\t\tdev_kfree_skb_any(tx_queue->tx_skbuff[i]);\n\t\ttx_queue->tx_skbuff[i] = NULL;\n\t}\n\tkfree(tx_queue->tx_skbuff);\n\ttx_queue->tx_skbuff = NULL;\n}\n\nstatic void free_skb_rx_queue(struct gfar_priv_rx_q *rx_queue)\n{\n\tint i;\n\n\tstruct rxbd8 *rxbdp = rx_queue->rx_bd_base;\n\n\tdev_kfree_skb(rx_queue->skb);\n\n\tfor (i = 0; i < rx_queue->rx_ring_size; i++) {\n\t\tstruct\tgfar_rx_buff *rxb = &rx_queue->rx_buff[i];\n\n\t\trxbdp->lstatus = 0;\n\t\trxbdp->bufPtr = 0;\n\t\trxbdp++;\n\n\t\tif (!rxb->page)\n\t\t\tcontinue;\n\n\t\tdma_unmap_page(rx_queue->dev, rxb->dma,\n\t\t\t       PAGE_SIZE, DMA_FROM_DEVICE);\n\t\t__free_page(rxb->page);\n\n\t\trxb->page = NULL;\n\t}\n\n\tkfree(rx_queue->rx_buff);\n\trx_queue->rx_buff = NULL;\n}\n\n \nstatic void free_skb_resources(struct gfar_private *priv)\n{\n\tstruct gfar_priv_tx_q *tx_queue = NULL;\n\tstruct gfar_priv_rx_q *rx_queue = NULL;\n\tint i;\n\n\t \n\tfor (i = 0; i < priv->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq;\n\n\t\ttx_queue = priv->tx_queue[i];\n\t\ttxq = netdev_get_tx_queue(tx_queue->dev, tx_queue->qindex);\n\t\tif (tx_queue->tx_skbuff)\n\t\t\tfree_skb_tx_queue(tx_queue);\n\t\tnetdev_tx_reset_queue(txq);\n\t}\n\n\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\trx_queue = priv->rx_queue[i];\n\t\tif (rx_queue->rx_buff)\n\t\t\tfree_skb_rx_queue(rx_queue);\n\t}\n\n\tdma_free_coherent(priv->dev,\n\t\t\t  sizeof(struct txbd8) * priv->total_tx_ring_size +\n\t\t\t  sizeof(struct rxbd8) * priv->total_rx_ring_size,\n\t\t\t  priv->tx_queue[0]->tx_bd_base,\n\t\t\t  priv->tx_queue[0]->tx_bd_dma_base);\n}\n\nvoid stop_gfar(struct net_device *dev)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\n\tnetif_tx_stop_all_queues(dev);\n\n\tsmp_mb__before_atomic();\n\tset_bit(GFAR_DOWN, &priv->state);\n\tsmp_mb__after_atomic();\n\n\tdisable_napi(priv);\n\n\t \n\tgfar_halt(priv);\n\n\tphy_stop(dev->phydev);\n\n\tfree_skb_resources(priv);\n}\n\nstatic void gfar_start(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tempval;\n\tint i = 0;\n\n\t \n\tgfar_write(&regs->rqueue, priv->rqueue);\n\tgfar_write(&regs->tqueue, priv->tqueue);\n\n\t \n\ttempval = gfar_read(&regs->dmactrl);\n\ttempval |= DMACTRL_INIT_SETTINGS;\n\tgfar_write(&regs->dmactrl, tempval);\n\n\t \n\ttempval = gfar_read(&regs->dmactrl);\n\ttempval &= ~(DMACTRL_GRS | DMACTRL_GTS);\n\tgfar_write(&regs->dmactrl, tempval);\n\n\tfor (i = 0; i < priv->num_grps; i++) {\n\t\tregs = priv->gfargrp[i].regs;\n\t\t \n\t\tgfar_write(&regs->tstat, priv->gfargrp[i].tstat);\n\t\tgfar_write(&regs->rstat, priv->gfargrp[i].rstat);\n\t}\n\n\t \n\ttempval = gfar_read(&regs->maccfg1);\n\ttempval |= (MACCFG1_RX_EN | MACCFG1_TX_EN);\n\tgfar_write(&regs->maccfg1, tempval);\n\n\tgfar_ints_enable(priv);\n\n\tnetif_trans_update(priv->ndev);  \n}\n\nstatic bool gfar_new_page(struct gfar_priv_rx_q *rxq, struct gfar_rx_buff *rxb)\n{\n\tstruct page *page;\n\tdma_addr_t addr;\n\n\tpage = dev_alloc_page();\n\tif (unlikely(!page))\n\t\treturn false;\n\n\taddr = dma_map_page(rxq->dev, page, 0, PAGE_SIZE, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(rxq->dev, addr))) {\n\t\t__free_page(page);\n\n\t\treturn false;\n\t}\n\n\trxb->dma = addr;\n\trxb->page = page;\n\trxb->page_offset = 0;\n\n\treturn true;\n}\n\nstatic void gfar_rx_alloc_err(struct gfar_priv_rx_q *rx_queue)\n{\n\tstruct gfar_private *priv = netdev_priv(rx_queue->ndev);\n\tstruct gfar_extra_stats *estats = &priv->extra_stats;\n\n\tnetdev_err(rx_queue->ndev, \"Can't alloc RX buffers\\n\");\n\tatomic64_inc(&estats->rx_alloc_err);\n}\n\nstatic void gfar_alloc_rx_buffs(struct gfar_priv_rx_q *rx_queue,\n\t\t\t\tint alloc_cnt)\n{\n\tstruct rxbd8 *bdp;\n\tstruct gfar_rx_buff *rxb;\n\tint i;\n\n\ti = rx_queue->next_to_use;\n\tbdp = &rx_queue->rx_bd_base[i];\n\trxb = &rx_queue->rx_buff[i];\n\n\twhile (alloc_cnt--) {\n\t\t \n\t\tif (unlikely(!rxb->page)) {\n\t\t\tif (unlikely(!gfar_new_page(rx_queue, rxb))) {\n\t\t\t\tgfar_rx_alloc_err(rx_queue);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tgfar_init_rxbdp(rx_queue, bdp,\n\t\t\t\trxb->dma + rxb->page_offset + RXBUF_ALIGNMENT);\n\n\t\t \n\t\tbdp++;\n\t\trxb++;\n\n\t\tif (unlikely(++i == rx_queue->rx_ring_size)) {\n\t\t\ti = 0;\n\t\t\tbdp = rx_queue->rx_bd_base;\n\t\t\trxb = rx_queue->rx_buff;\n\t\t}\n\t}\n\n\trx_queue->next_to_use = i;\n\trx_queue->next_to_alloc = i;\n}\n\nstatic void gfar_init_bds(struct net_device *ndev)\n{\n\tstruct gfar_private *priv = netdev_priv(ndev);\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tstruct gfar_priv_tx_q *tx_queue = NULL;\n\tstruct gfar_priv_rx_q *rx_queue = NULL;\n\tstruct txbd8 *txbdp;\n\tu32 __iomem *rfbptr;\n\tint i, j;\n\n\tfor (i = 0; i < priv->num_tx_queues; i++) {\n\t\ttx_queue = priv->tx_queue[i];\n\t\t \n\t\ttx_queue->num_txbdfree = tx_queue->tx_ring_size;\n\t\ttx_queue->dirty_tx = tx_queue->tx_bd_base;\n\t\ttx_queue->cur_tx = tx_queue->tx_bd_base;\n\t\ttx_queue->skb_curtx = 0;\n\t\ttx_queue->skb_dirtytx = 0;\n\n\t\t \n\t\ttxbdp = tx_queue->tx_bd_base;\n\t\tfor (j = 0; j < tx_queue->tx_ring_size; j++) {\n\t\t\ttxbdp->lstatus = 0;\n\t\t\ttxbdp->bufPtr = 0;\n\t\t\ttxbdp++;\n\t\t}\n\n\t\t \n\t\ttxbdp--;\n\t\ttxbdp->status = cpu_to_be16(be16_to_cpu(txbdp->status) |\n\t\t\t\t\t    TXBD_WRAP);\n\t}\n\n\trfbptr = &regs->rfbptr0;\n\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\trx_queue = priv->rx_queue[i];\n\n\t\trx_queue->next_to_clean = 0;\n\t\trx_queue->next_to_use = 0;\n\t\trx_queue->next_to_alloc = 0;\n\n\t\t \n\t\tgfar_alloc_rx_buffs(rx_queue, gfar_rxbd_unused(rx_queue));\n\n\t\trx_queue->rfbptr = rfbptr;\n\t\trfbptr += 2;\n\t}\n}\n\nstatic int gfar_alloc_skb_resources(struct net_device *ndev)\n{\n\tvoid *vaddr;\n\tdma_addr_t addr;\n\tint i, j;\n\tstruct gfar_private *priv = netdev_priv(ndev);\n\tstruct device *dev = priv->dev;\n\tstruct gfar_priv_tx_q *tx_queue = NULL;\n\tstruct gfar_priv_rx_q *rx_queue = NULL;\n\n\tpriv->total_tx_ring_size = 0;\n\tfor (i = 0; i < priv->num_tx_queues; i++)\n\t\tpriv->total_tx_ring_size += priv->tx_queue[i]->tx_ring_size;\n\n\tpriv->total_rx_ring_size = 0;\n\tfor (i = 0; i < priv->num_rx_queues; i++)\n\t\tpriv->total_rx_ring_size += priv->rx_queue[i]->rx_ring_size;\n\n\t \n\tvaddr = dma_alloc_coherent(dev,\n\t\t\t\t   (priv->total_tx_ring_size *\n\t\t\t\t    sizeof(struct txbd8)) +\n\t\t\t\t   (priv->total_rx_ring_size *\n\t\t\t\t    sizeof(struct rxbd8)),\n\t\t\t\t   &addr, GFP_KERNEL);\n\tif (!vaddr)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < priv->num_tx_queues; i++) {\n\t\ttx_queue = priv->tx_queue[i];\n\t\ttx_queue->tx_bd_base = vaddr;\n\t\ttx_queue->tx_bd_dma_base = addr;\n\t\ttx_queue->dev = ndev;\n\t\t \n\t\taddr  += sizeof(struct txbd8) * tx_queue->tx_ring_size;\n\t\tvaddr += sizeof(struct txbd8) * tx_queue->tx_ring_size;\n\t}\n\n\t \n\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\trx_queue = priv->rx_queue[i];\n\t\trx_queue->rx_bd_base = vaddr;\n\t\trx_queue->rx_bd_dma_base = addr;\n\t\trx_queue->ndev = ndev;\n\t\trx_queue->dev = dev;\n\t\taddr  += sizeof(struct rxbd8) * rx_queue->rx_ring_size;\n\t\tvaddr += sizeof(struct rxbd8) * rx_queue->rx_ring_size;\n\t}\n\n\t \n\tfor (i = 0; i < priv->num_tx_queues; i++) {\n\t\ttx_queue = priv->tx_queue[i];\n\t\ttx_queue->tx_skbuff =\n\t\t\tkmalloc_array(tx_queue->tx_ring_size,\n\t\t\t\t      sizeof(*tx_queue->tx_skbuff),\n\t\t\t\t      GFP_KERNEL);\n\t\tif (!tx_queue->tx_skbuff)\n\t\t\tgoto cleanup;\n\n\t\tfor (j = 0; j < tx_queue->tx_ring_size; j++)\n\t\t\ttx_queue->tx_skbuff[j] = NULL;\n\t}\n\n\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\trx_queue = priv->rx_queue[i];\n\t\trx_queue->rx_buff = kcalloc(rx_queue->rx_ring_size,\n\t\t\t\t\t    sizeof(*rx_queue->rx_buff),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!rx_queue->rx_buff)\n\t\t\tgoto cleanup;\n\t}\n\n\tgfar_init_bds(ndev);\n\n\treturn 0;\n\ncleanup:\n\tfree_skb_resources(priv);\n\treturn -ENOMEM;\n}\n\n \nint startup_gfar(struct net_device *ndev)\n{\n\tstruct gfar_private *priv = netdev_priv(ndev);\n\tint err;\n\n\tgfar_mac_reset(priv);\n\n\terr = gfar_alloc_skb_resources(ndev);\n\tif (err)\n\t\treturn err;\n\n\tgfar_init_tx_rx_base(priv);\n\n\tsmp_mb__before_atomic();\n\tclear_bit(GFAR_DOWN, &priv->state);\n\tsmp_mb__after_atomic();\n\n\t \n\tgfar_start(priv);\n\n\t \n\tpriv->oldlink = 0;\n\tpriv->oldspeed = 0;\n\tpriv->oldduplex = -1;\n\n\tphy_start(ndev->phydev);\n\n\tenable_napi(priv);\n\n\tnetif_tx_wake_all_queues(ndev);\n\n\treturn 0;\n}\n\nstatic u32 gfar_get_flowctrl_cfg(struct gfar_private *priv)\n{\n\tstruct net_device *ndev = priv->ndev;\n\tstruct phy_device *phydev = ndev->phydev;\n\tu32 val = 0;\n\n\tif (!phydev->duplex)\n\t\treturn val;\n\n\tif (!priv->pause_aneg_en) {\n\t\tif (priv->tx_pause_en)\n\t\t\tval |= MACCFG1_TX_FLOW;\n\t\tif (priv->rx_pause_en)\n\t\t\tval |= MACCFG1_RX_FLOW;\n\t} else {\n\t\tu16 lcl_adv, rmt_adv;\n\t\tu8 flowctrl;\n\t\t \n\t\trmt_adv = 0;\n\t\tif (phydev->pause)\n\t\t\trmt_adv = LPA_PAUSE_CAP;\n\t\tif (phydev->asym_pause)\n\t\t\trmt_adv |= LPA_PAUSE_ASYM;\n\n\t\tlcl_adv = linkmode_adv_to_lcl_adv_t(phydev->advertising);\n\t\tflowctrl = mii_resolve_flowctrl_fdx(lcl_adv, rmt_adv);\n\t\tif (flowctrl & FLOW_CTRL_TX)\n\t\t\tval |= MACCFG1_TX_FLOW;\n\t\tif (flowctrl & FLOW_CTRL_RX)\n\t\t\tval |= MACCFG1_RX_FLOW;\n\t}\n\n\treturn val;\n}\n\nstatic noinline void gfar_update_link_state(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tstruct net_device *ndev = priv->ndev;\n\tstruct phy_device *phydev = ndev->phydev;\n\tstruct gfar_priv_rx_q *rx_queue = NULL;\n\tint i;\n\n\tif (unlikely(test_bit(GFAR_RESETTING, &priv->state)))\n\t\treturn;\n\n\tif (phydev->link) {\n\t\tu32 tempval1 = gfar_read(&regs->maccfg1);\n\t\tu32 tempval = gfar_read(&regs->maccfg2);\n\t\tu32 ecntrl = gfar_read(&regs->ecntrl);\n\t\tu32 tx_flow_oldval = (tempval1 & MACCFG1_TX_FLOW);\n\n\t\tif (phydev->duplex != priv->oldduplex) {\n\t\t\tif (!(phydev->duplex))\n\t\t\t\ttempval &= ~(MACCFG2_FULL_DUPLEX);\n\t\t\telse\n\t\t\t\ttempval |= MACCFG2_FULL_DUPLEX;\n\n\t\t\tpriv->oldduplex = phydev->duplex;\n\t\t}\n\n\t\tif (phydev->speed != priv->oldspeed) {\n\t\t\tswitch (phydev->speed) {\n\t\t\tcase 1000:\n\t\t\t\ttempval =\n\t\t\t\t    ((tempval & ~(MACCFG2_IF)) | MACCFG2_GMII);\n\n\t\t\t\tecntrl &= ~(ECNTRL_R100);\n\t\t\t\tbreak;\n\t\t\tcase 100:\n\t\t\tcase 10:\n\t\t\t\ttempval =\n\t\t\t\t    ((tempval & ~(MACCFG2_IF)) | MACCFG2_MII);\n\n\t\t\t\t \n\t\t\t\tif (phydev->speed == SPEED_100)\n\t\t\t\t\tecntrl |= ECNTRL_R100;\n\t\t\t\telse\n\t\t\t\t\tecntrl &= ~(ECNTRL_R100);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tnetif_warn(priv, link, priv->ndev,\n\t\t\t\t\t   \"Ack!  Speed (%d) is not 10/100/1000!\\n\",\n\t\t\t\t\t   phydev->speed);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tpriv->oldspeed = phydev->speed;\n\t\t}\n\n\t\ttempval1 &= ~(MACCFG1_TX_FLOW | MACCFG1_RX_FLOW);\n\t\ttempval1 |= gfar_get_flowctrl_cfg(priv);\n\n\t\t \n\t\tif ((tempval1 & MACCFG1_TX_FLOW) && !tx_flow_oldval) {\n\t\t\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\t\t\tu32 bdp_dma;\n\n\t\t\t\trx_queue = priv->rx_queue[i];\n\t\t\t\tbdp_dma = gfar_rxbd_dma_lastfree(rx_queue);\n\t\t\t\tgfar_write(rx_queue->rfbptr, bdp_dma);\n\t\t\t}\n\n\t\t\tpriv->tx_actual_en = 1;\n\t\t}\n\n\t\tif (unlikely(!(tempval1 & MACCFG1_TX_FLOW) && tx_flow_oldval))\n\t\t\tpriv->tx_actual_en = 0;\n\n\t\tgfar_write(&regs->maccfg1, tempval1);\n\t\tgfar_write(&regs->maccfg2, tempval);\n\t\tgfar_write(&regs->ecntrl, ecntrl);\n\n\t\tif (!priv->oldlink)\n\t\t\tpriv->oldlink = 1;\n\n\t} else if (priv->oldlink) {\n\t\tpriv->oldlink = 0;\n\t\tpriv->oldspeed = 0;\n\t\tpriv->oldduplex = -1;\n\t}\n\n\tif (netif_msg_link(priv))\n\t\tphy_print_status(phydev);\n}\n\n \nstatic void adjust_link(struct net_device *dev)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tstruct phy_device *phydev = dev->phydev;\n\n\tif (unlikely(phydev->link != priv->oldlink ||\n\t\t     (phydev->link && (phydev->duplex != priv->oldduplex ||\n\t\t\t\t       phydev->speed != priv->oldspeed))))\n\t\tgfar_update_link_state(priv);\n}\n\n \nstatic void gfar_configure_serdes(struct net_device *dev)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tstruct phy_device *tbiphy;\n\n\tif (!priv->tbi_node) {\n\t\tdev_warn(&dev->dev, \"error: SGMII mode requires that the \"\n\t\t\t\t    \"device tree specify a tbi-handle\\n\");\n\t\treturn;\n\t}\n\n\ttbiphy = of_phy_find_device(priv->tbi_node);\n\tif (!tbiphy) {\n\t\tdev_err(&dev->dev, \"error: Could not get TBI device\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (phy_read(tbiphy, MII_BMSR) & BMSR_LSTATUS) {\n\t\tput_device(&tbiphy->mdio.dev);\n\t\treturn;\n\t}\n\n\t \n\tphy_write(tbiphy, MII_TBICON, TBICON_CLK_SELECT);\n\n\tphy_write(tbiphy, MII_ADVERTISE,\n\t\t  ADVERTISE_1000XFULL | ADVERTISE_1000XPAUSE |\n\t\t  ADVERTISE_1000XPSE_ASYM);\n\n\tphy_write(tbiphy, MII_BMCR,\n\t\t  BMCR_ANENABLE | BMCR_ANRESTART | BMCR_FULLDPLX |\n\t\t  BMCR_SPEED1000);\n\n\tput_device(&tbiphy->mdio.dev);\n}\n\n \nstatic int init_phy(struct net_device *dev)\n{\n\t__ETHTOOL_DECLARE_LINK_MODE_MASK(mask) = { 0, };\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tphy_interface_t interface = priv->interface;\n\tstruct phy_device *phydev;\n\tstruct ethtool_eee edata;\n\n\tlinkmode_set_bit_array(phy_10_100_features_array,\n\t\t\t       ARRAY_SIZE(phy_10_100_features_array),\n\t\t\t       mask);\n\tlinkmode_set_bit(ETHTOOL_LINK_MODE_Autoneg_BIT, mask);\n\tlinkmode_set_bit(ETHTOOL_LINK_MODE_MII_BIT, mask);\n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_GIGABIT)\n\t\tlinkmode_set_bit(ETHTOOL_LINK_MODE_1000baseT_Full_BIT, mask);\n\n\tpriv->oldlink = 0;\n\tpriv->oldspeed = 0;\n\tpriv->oldduplex = -1;\n\n\tphydev = of_phy_connect(dev, priv->phy_node, &adjust_link, 0,\n\t\t\t\tinterface);\n\tif (!phydev) {\n\t\tdev_err(&dev->dev, \"could not attach to PHY\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (interface == PHY_INTERFACE_MODE_SGMII)\n\t\tgfar_configure_serdes(dev);\n\n\t \n\tlinkmode_and(phydev->supported, phydev->supported, mask);\n\tlinkmode_copy(phydev->advertising, phydev->supported);\n\n\t \n\tphy_support_asym_pause(phydev);\n\n\t \n\tmemset(&edata, 0, sizeof(struct ethtool_eee));\n\tphy_ethtool_set_eee(phydev, &edata);\n\n\treturn 0;\n}\n\nstatic inline struct txfcb *gfar_add_fcb(struct sk_buff *skb)\n{\n\tstruct txfcb *fcb = skb_push(skb, GMAC_FCB_LEN);\n\n\tmemset(fcb, 0, GMAC_FCB_LEN);\n\n\treturn fcb;\n}\n\nstatic inline void gfar_tx_checksum(struct sk_buff *skb, struct txfcb *fcb,\n\t\t\t\t    int fcb_length)\n{\n\t \n\tu8 flags = TXFCB_DEFAULT;\n\n\t \n\tif (ip_hdr(skb)->protocol == IPPROTO_UDP) {\n\t\tflags |= TXFCB_UDP;\n\t\tfcb->phcs = (__force __be16)(udp_hdr(skb)->check);\n\t} else\n\t\tfcb->phcs = (__force __be16)(tcp_hdr(skb)->check);\n\n\t \n\tfcb->l3os = (u8)(skb_network_offset(skb) - fcb_length);\n\tfcb->l4os = skb_network_header_len(skb);\n\n\tfcb->flags = flags;\n}\n\nstatic inline void gfar_tx_vlan(struct sk_buff *skb, struct txfcb *fcb)\n{\n\tfcb->flags |= TXFCB_VLN;\n\tfcb->vlctl = cpu_to_be16(skb_vlan_tag_get(skb));\n}\n\nstatic inline struct txbd8 *skip_txbd(struct txbd8 *bdp, int stride,\n\t\t\t\t      struct txbd8 *base, int ring_size)\n{\n\tstruct txbd8 *new_bd = bdp + stride;\n\n\treturn (new_bd >= (base + ring_size)) ? (new_bd - ring_size) : new_bd;\n}\n\nstatic inline struct txbd8 *next_txbd(struct txbd8 *bdp, struct txbd8 *base,\n\t\t\t\t      int ring_size)\n{\n\treturn skip_txbd(bdp, 1, base, ring_size);\n}\n\n \nstatic inline bool gfar_csum_errata_12(struct gfar_private *priv,\n\t\t\t\t       unsigned long fcb_addr)\n{\n\treturn (gfar_has_errata(priv, GFAR_ERRATA_12) &&\n\t       (fcb_addr % 0x20) > 0x18);\n}\n\n \nstatic inline bool gfar_csum_errata_76(struct gfar_private *priv,\n\t\t\t\t       unsigned int len)\n{\n\treturn (gfar_has_errata(priv, GFAR_ERRATA_76) &&\n\t       (len > 2500));\n}\n\n \nstatic netdev_tx_t gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tstruct gfar_priv_tx_q *tx_queue = NULL;\n\tstruct netdev_queue *txq;\n\tstruct gfar __iomem *regs = NULL;\n\tstruct txfcb *fcb = NULL;\n\tstruct txbd8 *txbdp, *txbdp_start, *base, *txbdp_tstamp = NULL;\n\tu32 lstatus;\n\tskb_frag_t *frag;\n\tint i, rq = 0;\n\tint do_tstamp, do_csum, do_vlan;\n\tu32 bufaddr;\n\tunsigned int nr_frags, nr_txbds, bytes_sent, fcb_len = 0;\n\n\trq = skb->queue_mapping;\n\ttx_queue = priv->tx_queue[rq];\n\ttxq = netdev_get_tx_queue(dev, rq);\n\tbase = tx_queue->tx_bd_base;\n\tregs = tx_queue->grp->regs;\n\n\tdo_csum = (CHECKSUM_PARTIAL == skb->ip_summed);\n\tdo_vlan = skb_vlan_tag_present(skb);\n\tdo_tstamp = (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\n\t\t    priv->hwts_tx_en;\n\n\tif (do_csum || do_vlan)\n\t\tfcb_len = GMAC_FCB_LEN;\n\n\t \n\tif (unlikely(do_tstamp))\n\t\tfcb_len = GMAC_FCB_LEN + GMAC_TXPAL_LEN;\n\n\t \n\tif (fcb_len) {\n\t\tif (unlikely(skb_cow_head(skb, fcb_len))) {\n\t\t\tdev->stats.tx_errors++;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t}\n\n\t \n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\t \n\tif (unlikely(do_tstamp))\n\t\tnr_txbds = nr_frags + 2;\n\telse\n\t\tnr_txbds = nr_frags + 1;\n\n\t \n\tif (nr_txbds > tx_queue->num_txbdfree) {\n\t\t \n\t\tnetif_tx_stop_queue(txq);\n\t\tdev->stats.tx_fifo_errors++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\tbytes_sent = skb->len;\n\ttx_queue->stats.tx_bytes += bytes_sent;\n\t \n\tGFAR_CB(skb)->bytes_sent = bytes_sent;\n\ttx_queue->stats.tx_packets++;\n\n\ttxbdp = txbdp_start = tx_queue->cur_tx;\n\tlstatus = be32_to_cpu(txbdp->lstatus);\n\n\t \n\tif (unlikely(do_tstamp)) {\n\t\tskb_push(skb, GMAC_TXPAL_LEN);\n\t\tmemset(skb->data, 0, GMAC_TXPAL_LEN);\n\t}\n\n\t \n\tif (fcb_len) {\n\t\tfcb = gfar_add_fcb(skb);\n\t\tlstatus |= BD_LFLAG(TXBD_TOE);\n\t}\n\n\t \n\tif (do_csum) {\n\t\tgfar_tx_checksum(skb, fcb, fcb_len);\n\n\t\tif (unlikely(gfar_csum_errata_12(priv, (unsigned long)fcb)) ||\n\t\t    unlikely(gfar_csum_errata_76(priv, skb->len))) {\n\t\t\t__skb_pull(skb, GMAC_FCB_LEN);\n\t\t\tskb_checksum_help(skb);\n\t\t\tif (do_vlan || do_tstamp) {\n\t\t\t\t \n\t\t\t\tfcb = gfar_add_fcb(skb);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tlstatus &= ~(BD_LFLAG(TXBD_TOE));\n\t\t\t\tfcb = NULL;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (do_vlan)\n\t\tgfar_tx_vlan(skb, fcb);\n\n\tbufaddr = dma_map_single(priv->dev, skb->data, skb_headlen(skb),\n\t\t\t\t DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(priv->dev, bufaddr)))\n\t\tgoto dma_map_err;\n\n\ttxbdp_start->bufPtr = cpu_to_be32(bufaddr);\n\n\t \n\tif (unlikely(do_tstamp))\n\t\ttxbdp_tstamp = txbdp = next_txbd(txbdp, base,\n\t\t\t\t\t\t tx_queue->tx_ring_size);\n\n\tif (likely(!nr_frags)) {\n\t\tif (likely(!do_tstamp))\n\t\t\tlstatus |= BD_LFLAG(TXBD_LAST | TXBD_INTERRUPT);\n\t} else {\n\t\tu32 lstatus_start = lstatus;\n\n\t\t \n\t\tfrag = &skb_shinfo(skb)->frags[0];\n\t\tfor (i = 0; i < nr_frags; i++, frag++) {\n\t\t\tunsigned int size;\n\n\t\t\t \n\t\t\ttxbdp = next_txbd(txbdp, base, tx_queue->tx_ring_size);\n\n\t\t\tsize = skb_frag_size(frag);\n\n\t\t\tlstatus = be32_to_cpu(txbdp->lstatus) | size |\n\t\t\t\t  BD_LFLAG(TXBD_READY);\n\n\t\t\t \n\t\t\tif (i == nr_frags - 1)\n\t\t\t\tlstatus |= BD_LFLAG(TXBD_LAST | TXBD_INTERRUPT);\n\n\t\t\tbufaddr = skb_frag_dma_map(priv->dev, frag, 0,\n\t\t\t\t\t\t   size, DMA_TO_DEVICE);\n\t\t\tif (unlikely(dma_mapping_error(priv->dev, bufaddr)))\n\t\t\t\tgoto dma_map_err;\n\n\t\t\t \n\t\t\ttxbdp->bufPtr = cpu_to_be32(bufaddr);\n\t\t\ttxbdp->lstatus = cpu_to_be32(lstatus);\n\t\t}\n\n\t\tlstatus = lstatus_start;\n\t}\n\n\t \n\tif (unlikely(do_tstamp)) {\n\t\tu32 lstatus_ts = be32_to_cpu(txbdp_tstamp->lstatus);\n\n\t\tbufaddr = be32_to_cpu(txbdp_start->bufPtr);\n\t\tbufaddr += fcb_len;\n\n\t\tlstatus_ts |= BD_LFLAG(TXBD_READY) |\n\t\t\t      (skb_headlen(skb) - fcb_len);\n\t\tif (!nr_frags)\n\t\t\tlstatus_ts |= BD_LFLAG(TXBD_LAST | TXBD_INTERRUPT);\n\n\t\ttxbdp_tstamp->bufPtr = cpu_to_be32(bufaddr);\n\t\ttxbdp_tstamp->lstatus = cpu_to_be32(lstatus_ts);\n\t\tlstatus |= BD_LFLAG(TXBD_CRC | TXBD_READY) | GMAC_FCB_LEN;\n\n\t\t \n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\tfcb->ptp = 1;\n\t} else {\n\t\tlstatus |= BD_LFLAG(TXBD_CRC | TXBD_READY) | skb_headlen(skb);\n\t}\n\n\tskb_tx_timestamp(skb);\n\tnetdev_tx_sent_queue(txq, bytes_sent);\n\n\tgfar_wmb();\n\n\ttxbdp_start->lstatus = cpu_to_be32(lstatus);\n\n\tgfar_wmb();  \n\n\ttx_queue->tx_skbuff[tx_queue->skb_curtx] = skb;\n\n\t \n\ttx_queue->skb_curtx = (tx_queue->skb_curtx + 1) &\n\t\t\t      TX_RING_MOD_MASK(tx_queue->tx_ring_size);\n\n\ttx_queue->cur_tx = next_txbd(txbdp, base, tx_queue->tx_ring_size);\n\n\t \n\tspin_lock_bh(&tx_queue->txlock);\n\t \n\ttx_queue->num_txbdfree -= (nr_txbds);\n\tspin_unlock_bh(&tx_queue->txlock);\n\n\t \n\tif (!tx_queue->num_txbdfree) {\n\t\tnetif_tx_stop_queue(txq);\n\n\t\tdev->stats.tx_fifo_errors++;\n\t}\n\n\t \n\tgfar_write(&regs->tstat, TSTAT_CLEAR_THALT >> tx_queue->qindex);\n\n\treturn NETDEV_TX_OK;\n\ndma_map_err:\n\ttxbdp = next_txbd(txbdp_start, base, tx_queue->tx_ring_size);\n\tif (do_tstamp)\n\t\ttxbdp = next_txbd(txbdp, base, tx_queue->tx_ring_size);\n\tfor (i = 0; i < nr_frags; i++) {\n\t\tlstatus = be32_to_cpu(txbdp->lstatus);\n\t\tif (!(lstatus & BD_LFLAG(TXBD_READY)))\n\t\t\tbreak;\n\n\t\tlstatus &= ~BD_LFLAG(TXBD_READY);\n\t\ttxbdp->lstatus = cpu_to_be32(lstatus);\n\t\tbufaddr = be32_to_cpu(txbdp->bufPtr);\n\t\tdma_unmap_page(priv->dev, bufaddr, be16_to_cpu(txbdp->length),\n\t\t\t       DMA_TO_DEVICE);\n\t\ttxbdp = next_txbd(txbdp, base, tx_queue->tx_ring_size);\n\t}\n\tgfar_wmb();\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic int gfar_set_mac_address(struct net_device *dev)\n{\n\tgfar_set_mac_for_addr(dev, 0, dev->dev_addr);\n\n\treturn 0;\n}\n\nstatic int gfar_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\n\twhile (test_and_set_bit_lock(GFAR_RESETTING, &priv->state))\n\t\tcpu_relax();\n\n\tif (dev->flags & IFF_UP)\n\t\tstop_gfar(dev);\n\n\tdev->mtu = new_mtu;\n\n\tif (dev->flags & IFF_UP)\n\t\tstartup_gfar(dev);\n\n\tclear_bit_unlock(GFAR_RESETTING, &priv->state);\n\n\treturn 0;\n}\n\nstatic void reset_gfar(struct net_device *ndev)\n{\n\tstruct gfar_private *priv = netdev_priv(ndev);\n\n\twhile (test_and_set_bit_lock(GFAR_RESETTING, &priv->state))\n\t\tcpu_relax();\n\n\tstop_gfar(ndev);\n\tstartup_gfar(ndev);\n\n\tclear_bit_unlock(GFAR_RESETTING, &priv->state);\n}\n\n \nstatic void gfar_reset_task(struct work_struct *work)\n{\n\tstruct gfar_private *priv = container_of(work, struct gfar_private,\n\t\t\t\t\t\t reset_task);\n\treset_gfar(priv->ndev);\n}\n\nstatic void gfar_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\n\tdev->stats.tx_errors++;\n\tschedule_work(&priv->reset_task);\n}\n\nstatic int gfar_hwtstamp_set(struct net_device *netdev, struct ifreq *ifr)\n{\n\tstruct hwtstamp_config config;\n\tstruct gfar_private *priv = netdev_priv(netdev);\n\n\tif (copy_from_user(&config, ifr->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\tswitch (config.tx_type) {\n\tcase HWTSTAMP_TX_OFF:\n\t\tpriv->hwts_tx_en = 0;\n\t\tbreak;\n\tcase HWTSTAMP_TX_ON:\n\t\tif (!(priv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER))\n\t\t\treturn -ERANGE;\n\t\tpriv->hwts_tx_en = 1;\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tswitch (config.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\tif (priv->hwts_rx_en) {\n\t\t\tpriv->hwts_rx_en = 0;\n\t\t\treset_gfar(netdev);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tif (!(priv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER))\n\t\t\treturn -ERANGE;\n\t\tif (!priv->hwts_rx_en) {\n\t\t\tpriv->hwts_rx_en = 1;\n\t\t\treset_gfar(netdev);\n\t\t}\n\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t\tbreak;\n\t}\n\n\treturn copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?\n\t\t-EFAULT : 0;\n}\n\nstatic int gfar_hwtstamp_get(struct net_device *netdev, struct ifreq *ifr)\n{\n\tstruct hwtstamp_config config;\n\tstruct gfar_private *priv = netdev_priv(netdev);\n\n\tconfig.flags = 0;\n\tconfig.tx_type = priv->hwts_tx_en ? HWTSTAMP_TX_ON : HWTSTAMP_TX_OFF;\n\tconfig.rx_filter = (priv->hwts_rx_en ?\n\t\t\t    HWTSTAMP_FILTER_ALL : HWTSTAMP_FILTER_NONE);\n\n\treturn copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?\n\t\t-EFAULT : 0;\n}\n\nstatic int gfar_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tstruct phy_device *phydev = dev->phydev;\n\n\tif (!netif_running(dev))\n\t\treturn -EINVAL;\n\n\tif (cmd == SIOCSHWTSTAMP)\n\t\treturn gfar_hwtstamp_set(dev, rq);\n\tif (cmd == SIOCGHWTSTAMP)\n\t\treturn gfar_hwtstamp_get(dev, rq);\n\n\tif (!phydev)\n\t\treturn -ENODEV;\n\n\treturn phy_mii_ioctl(phydev, rq, cmd);\n}\n\n \nstatic void gfar_clean_tx_ring(struct gfar_priv_tx_q *tx_queue)\n{\n\tstruct net_device *dev = tx_queue->dev;\n\tstruct netdev_queue *txq;\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tstruct txbd8 *bdp, *next = NULL;\n\tstruct txbd8 *lbdp = NULL;\n\tstruct txbd8 *base = tx_queue->tx_bd_base;\n\tstruct sk_buff *skb;\n\tint skb_dirtytx;\n\tint tx_ring_size = tx_queue->tx_ring_size;\n\tint frags = 0, nr_txbds = 0;\n\tint i;\n\tint howmany = 0;\n\tint tqi = tx_queue->qindex;\n\tunsigned int bytes_sent = 0;\n\tu32 lstatus;\n\tsize_t buflen;\n\n\ttxq = netdev_get_tx_queue(dev, tqi);\n\tbdp = tx_queue->dirty_tx;\n\tskb_dirtytx = tx_queue->skb_dirtytx;\n\n\twhile ((skb = tx_queue->tx_skbuff[skb_dirtytx])) {\n\t\tbool do_tstamp;\n\n\t\tdo_tstamp = (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\n\t\t\t    priv->hwts_tx_en;\n\n\t\tfrags = skb_shinfo(skb)->nr_frags;\n\n\t\t \n\t\tif (unlikely(do_tstamp))\n\t\t\tnr_txbds = frags + 2;\n\t\telse\n\t\t\tnr_txbds = frags + 1;\n\n\t\tlbdp = skip_txbd(bdp, nr_txbds - 1, base, tx_ring_size);\n\n\t\tlstatus = be32_to_cpu(lbdp->lstatus);\n\n\t\t \n\t\tif ((lstatus & BD_LFLAG(TXBD_READY)) &&\n\t\t    (lstatus & BD_LENGTH_MASK))\n\t\t\tbreak;\n\n\t\tif (unlikely(do_tstamp)) {\n\t\t\tnext = next_txbd(bdp, base, tx_ring_size);\n\t\t\tbuflen = be16_to_cpu(next->length) +\n\t\t\t\t GMAC_FCB_LEN + GMAC_TXPAL_LEN;\n\t\t} else\n\t\t\tbuflen = be16_to_cpu(bdp->length);\n\n\t\tdma_unmap_single(priv->dev, be32_to_cpu(bdp->bufPtr),\n\t\t\t\t buflen, DMA_TO_DEVICE);\n\n\t\tif (unlikely(do_tstamp)) {\n\t\t\tstruct skb_shared_hwtstamps shhwtstamps;\n\t\t\tu64 *ns = (u64 *)(((uintptr_t)skb->data + 0x10) &\n\t\t\t\t\t  ~0x7UL);\n\n\t\t\tmemset(&shhwtstamps, 0, sizeof(shhwtstamps));\n\t\t\tshhwtstamps.hwtstamp = ns_to_ktime(be64_to_cpu(*ns));\n\t\t\tskb_pull(skb, GMAC_FCB_LEN + GMAC_TXPAL_LEN);\n\t\t\tskb_tstamp_tx(skb, &shhwtstamps);\n\t\t\tgfar_clear_txbd_status(bdp);\n\t\t\tbdp = next;\n\t\t}\n\n\t\tgfar_clear_txbd_status(bdp);\n\t\tbdp = next_txbd(bdp, base, tx_ring_size);\n\n\t\tfor (i = 0; i < frags; i++) {\n\t\t\tdma_unmap_page(priv->dev, be32_to_cpu(bdp->bufPtr),\n\t\t\t\t       be16_to_cpu(bdp->length),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\t\tgfar_clear_txbd_status(bdp);\n\t\t\tbdp = next_txbd(bdp, base, tx_ring_size);\n\t\t}\n\n\t\tbytes_sent += GFAR_CB(skb)->bytes_sent;\n\n\t\tdev_kfree_skb_any(skb);\n\n\t\ttx_queue->tx_skbuff[skb_dirtytx] = NULL;\n\n\t\tskb_dirtytx = (skb_dirtytx + 1) &\n\t\t\t      TX_RING_MOD_MASK(tx_ring_size);\n\n\t\thowmany++;\n\t\tspin_lock(&tx_queue->txlock);\n\t\ttx_queue->num_txbdfree += nr_txbds;\n\t\tspin_unlock(&tx_queue->txlock);\n\t}\n\n\t \n\tif (tx_queue->num_txbdfree &&\n\t    netif_tx_queue_stopped(txq) &&\n\t    !(test_bit(GFAR_DOWN, &priv->state)))\n\t\tnetif_wake_subqueue(priv->ndev, tqi);\n\n\t \n\ttx_queue->skb_dirtytx = skb_dirtytx;\n\ttx_queue->dirty_tx = bdp;\n\n\tnetdev_tx_completed_queue(txq, howmany, bytes_sent);\n}\n\nstatic void count_errors(u32 lstatus, struct net_device *ndev)\n{\n\tstruct gfar_private *priv = netdev_priv(ndev);\n\tstruct net_device_stats *stats = &ndev->stats;\n\tstruct gfar_extra_stats *estats = &priv->extra_stats;\n\n\t \n\tif (lstatus & BD_LFLAG(RXBD_TRUNCATED)) {\n\t\tstats->rx_length_errors++;\n\n\t\tatomic64_inc(&estats->rx_trunc);\n\n\t\treturn;\n\t}\n\t \n\tif (lstatus & BD_LFLAG(RXBD_LARGE | RXBD_SHORT)) {\n\t\tstats->rx_length_errors++;\n\n\t\tif (lstatus & BD_LFLAG(RXBD_LARGE))\n\t\t\tatomic64_inc(&estats->rx_large);\n\t\telse\n\t\t\tatomic64_inc(&estats->rx_short);\n\t}\n\tif (lstatus & BD_LFLAG(RXBD_NONOCTET)) {\n\t\tstats->rx_frame_errors++;\n\t\tatomic64_inc(&estats->rx_nonoctet);\n\t}\n\tif (lstatus & BD_LFLAG(RXBD_CRCERR)) {\n\t\tatomic64_inc(&estats->rx_crcerr);\n\t\tstats->rx_crc_errors++;\n\t}\n\tif (lstatus & BD_LFLAG(RXBD_OVERRUN)) {\n\t\tatomic64_inc(&estats->rx_overrun);\n\t\tstats->rx_over_errors++;\n\t}\n}\n\nstatic irqreturn_t gfar_receive(int irq, void *grp_id)\n{\n\tstruct gfar_priv_grp *grp = (struct gfar_priv_grp *)grp_id;\n\tunsigned long flags;\n\tu32 imask, ievent;\n\n\tievent = gfar_read(&grp->regs->ievent);\n\n\tif (unlikely(ievent & IEVENT_FGPI)) {\n\t\tgfar_write(&grp->regs->ievent, IEVENT_FGPI);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tif (likely(napi_schedule_prep(&grp->napi_rx))) {\n\t\tspin_lock_irqsave(&grp->grplock, flags);\n\t\timask = gfar_read(&grp->regs->imask);\n\t\timask &= IMASK_RX_DISABLED | grp->priv->rmon_overflow.imask;\n\t\tgfar_write(&grp->regs->imask, imask);\n\t\tspin_unlock_irqrestore(&grp->grplock, flags);\n\t\t__napi_schedule(&grp->napi_rx);\n\t} else {\n\t\t \n\t\tgfar_write(&grp->regs->ievent, IEVENT_RX_MASK);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t gfar_transmit(int irq, void *grp_id)\n{\n\tstruct gfar_priv_grp *grp = (struct gfar_priv_grp *)grp_id;\n\tunsigned long flags;\n\tu32 imask;\n\n\tif (likely(napi_schedule_prep(&grp->napi_tx))) {\n\t\tspin_lock_irqsave(&grp->grplock, flags);\n\t\timask = gfar_read(&grp->regs->imask);\n\t\timask &= IMASK_TX_DISABLED | grp->priv->rmon_overflow.imask;\n\t\tgfar_write(&grp->regs->imask, imask);\n\t\tspin_unlock_irqrestore(&grp->grplock, flags);\n\t\t__napi_schedule(&grp->napi_tx);\n\t} else {\n\t\t \n\t\tgfar_write(&grp->regs->ievent, IEVENT_TX_MASK);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic bool gfar_add_rx_frag(struct gfar_rx_buff *rxb, u32 lstatus,\n\t\t\t     struct sk_buff *skb, bool first)\n{\n\tint size = lstatus & BD_LENGTH_MASK;\n\tstruct page *page = rxb->page;\n\n\tif (likely(first)) {\n\t\tskb_put(skb, size);\n\t} else {\n\t\t \n\t\tif (lstatus & BD_LFLAG(RXBD_LAST))\n\t\t\tsize -= skb->len;\n\n\t\tWARN(size < 0, \"gianfar: rx fragment size underflow\");\n\t\tif (size < 0)\n\t\t\treturn false;\n\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,\n\t\t\t\trxb->page_offset + RXBUF_ALIGNMENT,\n\t\t\t\tsize, GFAR_RXB_TRUESIZE);\n\t}\n\n\t \n\tif (unlikely(page_count(page) != 1 || page_is_pfmemalloc(page)))\n\t\treturn false;\n\n\t \n\trxb->page_offset ^= GFAR_RXB_TRUESIZE;\n\n\tpage_ref_inc(page);\n\n\treturn true;\n}\n\nstatic void gfar_reuse_rx_page(struct gfar_priv_rx_q *rxq,\n\t\t\t       struct gfar_rx_buff *old_rxb)\n{\n\tstruct gfar_rx_buff *new_rxb;\n\tu16 nta = rxq->next_to_alloc;\n\n\tnew_rxb = &rxq->rx_buff[nta];\n\n\t \n\tnta++;\n\trxq->next_to_alloc = (nta < rxq->rx_ring_size) ? nta : 0;\n\n\t \n\t*new_rxb = *old_rxb;\n\n\t \n\tdma_sync_single_range_for_device(rxq->dev, old_rxb->dma,\n\t\t\t\t\t old_rxb->page_offset,\n\t\t\t\t\t GFAR_RXB_TRUESIZE, DMA_FROM_DEVICE);\n}\n\nstatic struct sk_buff *gfar_get_next_rxbuff(struct gfar_priv_rx_q *rx_queue,\n\t\t\t\t\t    u32 lstatus, struct sk_buff *skb)\n{\n\tstruct gfar_rx_buff *rxb = &rx_queue->rx_buff[rx_queue->next_to_clean];\n\tstruct page *page = rxb->page;\n\tbool first = false;\n\n\tif (likely(!skb)) {\n\t\tvoid *buff_addr = page_address(page) + rxb->page_offset;\n\n\t\tskb = build_skb(buff_addr, GFAR_SKBFRAG_SIZE);\n\t\tif (unlikely(!skb)) {\n\t\t\tgfar_rx_alloc_err(rx_queue);\n\t\t\treturn NULL;\n\t\t}\n\t\tskb_reserve(skb, RXBUF_ALIGNMENT);\n\t\tfirst = true;\n\t}\n\n\tdma_sync_single_range_for_cpu(rx_queue->dev, rxb->dma, rxb->page_offset,\n\t\t\t\t      GFAR_RXB_TRUESIZE, DMA_FROM_DEVICE);\n\n\tif (gfar_add_rx_frag(rxb, lstatus, skb, first)) {\n\t\t \n\t\tgfar_reuse_rx_page(rx_queue, rxb);\n\t} else {\n\t\t \n\t\tdma_unmap_page(rx_queue->dev, rxb->dma,\n\t\t\t       PAGE_SIZE, DMA_FROM_DEVICE);\n\t}\n\n\t \n\trxb->page = NULL;\n\n\treturn skb;\n}\n\nstatic inline void gfar_rx_checksum(struct sk_buff *skb, struct rxfcb *fcb)\n{\n\t \n\tif ((be16_to_cpu(fcb->flags) & RXFCB_CSUM_MASK) ==\n\t    (RXFCB_CIP | RXFCB_CTU))\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\telse\n\t\tskb_checksum_none_assert(skb);\n}\n\n \nstatic void gfar_process_frame(struct net_device *ndev, struct sk_buff *skb)\n{\n\tstruct gfar_private *priv = netdev_priv(ndev);\n\tstruct rxfcb *fcb = NULL;\n\n\t \n\tfcb = (struct rxfcb *)skb->data;\n\n\t \n\tif (priv->uses_rxfcb)\n\t\tskb_pull(skb, GMAC_FCB_LEN);\n\n\t \n\tif (priv->hwts_rx_en) {\n\t\tstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\n\t\tu64 *ns = (u64 *) skb->data;\n\n\t\tmemset(shhwtstamps, 0, sizeof(*shhwtstamps));\n\t\tshhwtstamps->hwtstamp = ns_to_ktime(be64_to_cpu(*ns));\n\t}\n\n\tif (priv->padding)\n\t\tskb_pull(skb, priv->padding);\n\n\t \n\tpskb_trim(skb, skb->len - ETH_FCS_LEN);\n\n\tif (ndev->features & NETIF_F_RXCSUM)\n\t\tgfar_rx_checksum(skb, fcb);\n\n\t \n\tif (ndev->features & NETIF_F_HW_VLAN_CTAG_RX &&\n\t    be16_to_cpu(fcb->flags) & RXFCB_VLN)\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t       be16_to_cpu(fcb->vlctl));\n}\n\n \nstatic int gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue,\n\t\t\t      int rx_work_limit)\n{\n\tstruct net_device *ndev = rx_queue->ndev;\n\tstruct gfar_private *priv = netdev_priv(ndev);\n\tstruct rxbd8 *bdp;\n\tint i, howmany = 0;\n\tstruct sk_buff *skb = rx_queue->skb;\n\tint cleaned_cnt = gfar_rxbd_unused(rx_queue);\n\tunsigned int total_bytes = 0, total_pkts = 0;\n\n\t \n\ti = rx_queue->next_to_clean;\n\n\twhile (rx_work_limit--) {\n\t\tu32 lstatus;\n\n\t\tif (cleaned_cnt >= GFAR_RX_BUFF_ALLOC) {\n\t\t\tgfar_alloc_rx_buffs(rx_queue, cleaned_cnt);\n\t\t\tcleaned_cnt = 0;\n\t\t}\n\n\t\tbdp = &rx_queue->rx_bd_base[i];\n\t\tlstatus = be32_to_cpu(bdp->lstatus);\n\t\tif (lstatus & BD_LFLAG(RXBD_EMPTY))\n\t\t\tbreak;\n\n\t\t \n\t\tif (skb &&\n\t\t    (lstatus & BD_LFLAG(RXBD_FIRST))) {\n\t\t\t \n\t\t\tdev_kfree_skb(skb);\n\t\t\tskb = NULL;\n\t\t\trx_queue->stats.rx_dropped++;\n\n\t\t\t \n\t\t}\n\n\t\t \n\t\trmb();\n\n\t\t \n\t\tskb = gfar_get_next_rxbuff(rx_queue, lstatus, skb);\n\t\tif (unlikely(!skb))\n\t\t\tbreak;\n\n\t\tcleaned_cnt++;\n\t\thowmany++;\n\n\t\tif (unlikely(++i == rx_queue->rx_ring_size))\n\t\t\ti = 0;\n\n\t\trx_queue->next_to_clean = i;\n\n\t\t \n\t\tif (!(lstatus & BD_LFLAG(RXBD_LAST)))\n\t\t\tcontinue;\n\n\t\tif (unlikely(lstatus & BD_LFLAG(RXBD_ERR))) {\n\t\t\tcount_errors(lstatus, ndev);\n\n\t\t\t \n\t\t\tdev_kfree_skb(skb);\n\t\t\tskb = NULL;\n\t\t\trx_queue->stats.rx_dropped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgfar_process_frame(ndev, skb);\n\n\t\t \n\t\ttotal_pkts++;\n\t\ttotal_bytes += skb->len;\n\n\t\tskb_record_rx_queue(skb, rx_queue->qindex);\n\n\t\tskb->protocol = eth_type_trans(skb, ndev);\n\n\t\t \n\t\tnapi_gro_receive(&rx_queue->grp->napi_rx, skb);\n\n\t\tskb = NULL;\n\t}\n\n\t \n\trx_queue->skb = skb;\n\n\trx_queue->stats.rx_packets += total_pkts;\n\trx_queue->stats.rx_bytes += total_bytes;\n\n\tif (cleaned_cnt)\n\t\tgfar_alloc_rx_buffs(rx_queue, cleaned_cnt);\n\n\t \n\tif (unlikely(priv->tx_actual_en)) {\n\t\tu32 bdp_dma = gfar_rxbd_dma_lastfree(rx_queue);\n\n\t\tgfar_write(rx_queue->rfbptr, bdp_dma);\n\t}\n\n\treturn howmany;\n}\n\nstatic int gfar_poll_rx_sq(struct napi_struct *napi, int budget)\n{\n\tstruct gfar_priv_grp *gfargrp =\n\t\tcontainer_of(napi, struct gfar_priv_grp, napi_rx);\n\tstruct gfar __iomem *regs = gfargrp->regs;\n\tstruct gfar_priv_rx_q *rx_queue = gfargrp->rx_queue;\n\tint work_done = 0;\n\n\t \n\tgfar_write(&regs->ievent, IEVENT_RX_MASK);\n\n\twork_done = gfar_clean_rx_ring(rx_queue, budget);\n\n\tif (work_done < budget) {\n\t\tu32 imask;\n\t\tnapi_complete_done(napi, work_done);\n\t\t \n\t\tgfar_write(&regs->rstat, gfargrp->rstat);\n\n\t\tspin_lock_irq(&gfargrp->grplock);\n\t\timask = gfar_read(&regs->imask);\n\t\timask |= IMASK_RX_DEFAULT;\n\t\tgfar_write(&regs->imask, imask);\n\t\tspin_unlock_irq(&gfargrp->grplock);\n\t}\n\n\treturn work_done;\n}\n\nstatic int gfar_poll_tx_sq(struct napi_struct *napi, int budget)\n{\n\tstruct gfar_priv_grp *gfargrp =\n\t\tcontainer_of(napi, struct gfar_priv_grp, napi_tx);\n\tstruct gfar __iomem *regs = gfargrp->regs;\n\tstruct gfar_priv_tx_q *tx_queue = gfargrp->tx_queue;\n\tu32 imask;\n\n\t \n\tgfar_write(&regs->ievent, IEVENT_TX_MASK);\n\n\t \n\tif (tx_queue->tx_skbuff[tx_queue->skb_dirtytx])\n\t\tgfar_clean_tx_ring(tx_queue);\n\n\tnapi_complete(napi);\n\n\tspin_lock_irq(&gfargrp->grplock);\n\timask = gfar_read(&regs->imask);\n\timask |= IMASK_TX_DEFAULT;\n\tgfar_write(&regs->imask, imask);\n\tspin_unlock_irq(&gfargrp->grplock);\n\n\treturn 0;\n}\n\n \nstatic irqreturn_t gfar_error(int irq, void *grp_id)\n{\n\tstruct gfar_priv_grp *gfargrp = grp_id;\n\tstruct gfar __iomem *regs = gfargrp->regs;\n\tstruct gfar_private *priv= gfargrp->priv;\n\tstruct net_device *dev = priv->ndev;\n\n\t \n\tu32 events = gfar_read(&regs->ievent);\n\n\t \n\tgfar_write(&regs->ievent, events & IEVENT_ERR_MASK);\n\n\t \n\tif ((priv->device_flags & FSL_GIANFAR_DEV_HAS_MAGIC_PACKET) &&\n\t    (events & IEVENT_MAG))\n\t\tevents &= ~IEVENT_MAG;\n\n\t \n\tif (netif_msg_rx_err(priv) || netif_msg_tx_err(priv))\n\t\tnetdev_dbg(dev,\n\t\t\t   \"error interrupt (ievent=0x%08x imask=0x%08x)\\n\",\n\t\t\t   events, gfar_read(&regs->imask));\n\n\t \n\tif (events & IEVENT_TXE) {\n\t\tdev->stats.tx_errors++;\n\n\t\tif (events & IEVENT_LC)\n\t\t\tdev->stats.tx_window_errors++;\n\t\tif (events & IEVENT_CRL)\n\t\t\tdev->stats.tx_aborted_errors++;\n\t\tif (events & IEVENT_XFUN) {\n\t\t\tnetif_dbg(priv, tx_err, dev,\n\t\t\t\t  \"TX FIFO underrun, packet dropped\\n\");\n\t\t\tdev->stats.tx_dropped++;\n\t\t\tatomic64_inc(&priv->extra_stats.tx_underrun);\n\n\t\t\tschedule_work(&priv->reset_task);\n\t\t}\n\t\tnetif_dbg(priv, tx_err, dev, \"Transmit Error\\n\");\n\t}\n\tif (events & IEVENT_MSRO) {\n\t\tstruct rmon_mib __iomem *rmon = &regs->rmon;\n\t\tu32 car;\n\n\t\tspin_lock(&priv->rmon_overflow.lock);\n\t\tcar = gfar_read(&rmon->car1) & CAR1_C1RDR;\n\t\tif (car) {\n\t\t\tpriv->rmon_overflow.rdrp++;\n\t\t\tgfar_write(&rmon->car1, car);\n\t\t}\n\t\tspin_unlock(&priv->rmon_overflow.lock);\n\t}\n\tif (events & IEVENT_BSY) {\n\t\tdev->stats.rx_over_errors++;\n\t\tatomic64_inc(&priv->extra_stats.rx_bsy);\n\n\t\tnetif_dbg(priv, rx_err, dev, \"busy error (rstat: %x)\\n\",\n\t\t\t  gfar_read(&regs->rstat));\n\t}\n\tif (events & IEVENT_BABR) {\n\t\tdev->stats.rx_errors++;\n\t\tatomic64_inc(&priv->extra_stats.rx_babr);\n\n\t\tnetif_dbg(priv, rx_err, dev, \"babbling RX error\\n\");\n\t}\n\tif (events & IEVENT_EBERR) {\n\t\tatomic64_inc(&priv->extra_stats.eberr);\n\t\tnetif_dbg(priv, rx_err, dev, \"bus error\\n\");\n\t}\n\tif (events & IEVENT_RXC)\n\t\tnetif_dbg(priv, rx_status, dev, \"control frame\\n\");\n\n\tif (events & IEVENT_BABT) {\n\t\tatomic64_inc(&priv->extra_stats.tx_babt);\n\t\tnetif_dbg(priv, tx_err, dev, \"babbling TX error\\n\");\n\t}\n\treturn IRQ_HANDLED;\n}\n\n \nstatic irqreturn_t gfar_interrupt(int irq, void *grp_id)\n{\n\tstruct gfar_priv_grp *gfargrp = grp_id;\n\n\t \n\tu32 events = gfar_read(&gfargrp->regs->ievent);\n\n\t \n\tif (events & IEVENT_RX_MASK)\n\t\tgfar_receive(irq, grp_id);\n\n\t \n\tif (events & IEVENT_TX_MASK)\n\t\tgfar_transmit(irq, grp_id);\n\n\t \n\tif (events & IEVENT_ERR_MASK)\n\t\tgfar_error(irq, grp_id);\n\n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\n \nstatic void gfar_netpoll(struct net_device *dev)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tint i;\n\n\t \n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MULTI_INTR) {\n\t\tfor (i = 0; i < priv->num_grps; i++) {\n\t\t\tstruct gfar_priv_grp *grp = &priv->gfargrp[i];\n\n\t\t\tdisable_irq(gfar_irq(grp, TX)->irq);\n\t\t\tdisable_irq(gfar_irq(grp, RX)->irq);\n\t\t\tdisable_irq(gfar_irq(grp, ER)->irq);\n\t\t\tgfar_interrupt(gfar_irq(grp, TX)->irq, grp);\n\t\t\tenable_irq(gfar_irq(grp, ER)->irq);\n\t\t\tenable_irq(gfar_irq(grp, RX)->irq);\n\t\t\tenable_irq(gfar_irq(grp, TX)->irq);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < priv->num_grps; i++) {\n\t\t\tstruct gfar_priv_grp *grp = &priv->gfargrp[i];\n\n\t\t\tdisable_irq(gfar_irq(grp, TX)->irq);\n\t\t\tgfar_interrupt(gfar_irq(grp, TX)->irq, grp);\n\t\t\tenable_irq(gfar_irq(grp, TX)->irq);\n\t\t}\n\t}\n}\n#endif\n\nstatic void free_grp_irqs(struct gfar_priv_grp *grp)\n{\n\tfree_irq(gfar_irq(grp, TX)->irq, grp);\n\tfree_irq(gfar_irq(grp, RX)->irq, grp);\n\tfree_irq(gfar_irq(grp, ER)->irq, grp);\n}\n\nstatic int register_grp_irqs(struct gfar_priv_grp *grp)\n{\n\tstruct gfar_private *priv = grp->priv;\n\tstruct net_device *dev = priv->ndev;\n\tint err;\n\n\t \n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MULTI_INTR) {\n\t\t \n\t\terr = request_irq(gfar_irq(grp, ER)->irq, gfar_error, 0,\n\t\t\t\t  gfar_irq(grp, ER)->name, grp);\n\t\tif (err < 0) {\n\t\t\tnetif_err(priv, intr, dev, \"Can't get IRQ %d\\n\",\n\t\t\t\t  gfar_irq(grp, ER)->irq);\n\n\t\t\tgoto err_irq_fail;\n\t\t}\n\t\tenable_irq_wake(gfar_irq(grp, ER)->irq);\n\n\t\terr = request_irq(gfar_irq(grp, TX)->irq, gfar_transmit, 0,\n\t\t\t\t  gfar_irq(grp, TX)->name, grp);\n\t\tif (err < 0) {\n\t\t\tnetif_err(priv, intr, dev, \"Can't get IRQ %d\\n\",\n\t\t\t\t  gfar_irq(grp, TX)->irq);\n\t\t\tgoto tx_irq_fail;\n\t\t}\n\t\terr = request_irq(gfar_irq(grp, RX)->irq, gfar_receive, 0,\n\t\t\t\t  gfar_irq(grp, RX)->name, grp);\n\t\tif (err < 0) {\n\t\t\tnetif_err(priv, intr, dev, \"Can't get IRQ %d\\n\",\n\t\t\t\t  gfar_irq(grp, RX)->irq);\n\t\t\tgoto rx_irq_fail;\n\t\t}\n\t\tenable_irq_wake(gfar_irq(grp, RX)->irq);\n\n\t} else {\n\t\terr = request_irq(gfar_irq(grp, TX)->irq, gfar_interrupt, 0,\n\t\t\t\t  gfar_irq(grp, TX)->name, grp);\n\t\tif (err < 0) {\n\t\t\tnetif_err(priv, intr, dev, \"Can't get IRQ %d\\n\",\n\t\t\t\t  gfar_irq(grp, TX)->irq);\n\t\t\tgoto err_irq_fail;\n\t\t}\n\t\tenable_irq_wake(gfar_irq(grp, TX)->irq);\n\t}\n\n\treturn 0;\n\nrx_irq_fail:\n\tfree_irq(gfar_irq(grp, TX)->irq, grp);\ntx_irq_fail:\n\tfree_irq(gfar_irq(grp, ER)->irq, grp);\nerr_irq_fail:\n\treturn err;\n\n}\n\nstatic void gfar_free_irq(struct gfar_private *priv)\n{\n\tint i;\n\n\t \n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MULTI_INTR) {\n\t\tfor (i = 0; i < priv->num_grps; i++)\n\t\t\tfree_grp_irqs(&priv->gfargrp[i]);\n\t} else {\n\t\tfor (i = 0; i < priv->num_grps; i++)\n\t\t\tfree_irq(gfar_irq(&priv->gfargrp[i], TX)->irq,\n\t\t\t\t &priv->gfargrp[i]);\n\t}\n}\n\nstatic int gfar_request_irq(struct gfar_private *priv)\n{\n\tint err, i, j;\n\n\tfor (i = 0; i < priv->num_grps; i++) {\n\t\terr = register_grp_irqs(&priv->gfargrp[i]);\n\t\tif (err) {\n\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\tfree_grp_irqs(&priv->gfargrp[j]);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int gfar_enet_open(struct net_device *dev)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tint err;\n\n\terr = init_phy(dev);\n\tif (err)\n\t\treturn err;\n\n\terr = gfar_request_irq(priv);\n\tif (err)\n\t\treturn err;\n\n\terr = startup_gfar(dev);\n\tif (err)\n\t\treturn err;\n\n\treturn err;\n}\n\n \nstatic int gfar_close(struct net_device *dev)\n{\n\tstruct gfar_private *priv = netdev_priv(dev);\n\n\tcancel_work_sync(&priv->reset_task);\n\tstop_gfar(dev);\n\n\t \n\tphy_disconnect(dev->phydev);\n\n\tgfar_free_irq(priv);\n\n\treturn 0;\n}\n\n \nstatic void gfar_clear_exact_match(struct net_device *dev)\n{\n\tint idx;\n\tstatic const u8 zero_arr[ETH_ALEN] = {0, 0, 0, 0, 0, 0};\n\n\tfor (idx = 1; idx < GFAR_EM_NUM + 1; idx++)\n\t\tgfar_set_mac_for_addr(dev, idx, zero_arr);\n}\n\n \nstatic void gfar_set_multi(struct net_device *dev)\n{\n\tstruct netdev_hw_addr *ha;\n\tstruct gfar_private *priv = netdev_priv(dev);\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tempval;\n\n\tif (dev->flags & IFF_PROMISC) {\n\t\t \n\t\ttempval = gfar_read(&regs->rctrl);\n\t\ttempval |= RCTRL_PROM;\n\t\tgfar_write(&regs->rctrl, tempval);\n\t} else {\n\t\t \n\t\ttempval = gfar_read(&regs->rctrl);\n\t\ttempval &= ~(RCTRL_PROM);\n\t\tgfar_write(&regs->rctrl, tempval);\n\t}\n\n\tif (dev->flags & IFF_ALLMULTI) {\n\t\t \n\t\tgfar_write(&regs->igaddr0, 0xffffffff);\n\t\tgfar_write(&regs->igaddr1, 0xffffffff);\n\t\tgfar_write(&regs->igaddr2, 0xffffffff);\n\t\tgfar_write(&regs->igaddr3, 0xffffffff);\n\t\tgfar_write(&regs->igaddr4, 0xffffffff);\n\t\tgfar_write(&regs->igaddr5, 0xffffffff);\n\t\tgfar_write(&regs->igaddr6, 0xffffffff);\n\t\tgfar_write(&regs->igaddr7, 0xffffffff);\n\t\tgfar_write(&regs->gaddr0, 0xffffffff);\n\t\tgfar_write(&regs->gaddr1, 0xffffffff);\n\t\tgfar_write(&regs->gaddr2, 0xffffffff);\n\t\tgfar_write(&regs->gaddr3, 0xffffffff);\n\t\tgfar_write(&regs->gaddr4, 0xffffffff);\n\t\tgfar_write(&regs->gaddr5, 0xffffffff);\n\t\tgfar_write(&regs->gaddr6, 0xffffffff);\n\t\tgfar_write(&regs->gaddr7, 0xffffffff);\n\t} else {\n\t\tint em_num;\n\t\tint idx;\n\n\t\t \n\t\tgfar_write(&regs->igaddr0, 0x0);\n\t\tgfar_write(&regs->igaddr1, 0x0);\n\t\tgfar_write(&regs->igaddr2, 0x0);\n\t\tgfar_write(&regs->igaddr3, 0x0);\n\t\tgfar_write(&regs->igaddr4, 0x0);\n\t\tgfar_write(&regs->igaddr5, 0x0);\n\t\tgfar_write(&regs->igaddr6, 0x0);\n\t\tgfar_write(&regs->igaddr7, 0x0);\n\t\tgfar_write(&regs->gaddr0, 0x0);\n\t\tgfar_write(&regs->gaddr1, 0x0);\n\t\tgfar_write(&regs->gaddr2, 0x0);\n\t\tgfar_write(&regs->gaddr3, 0x0);\n\t\tgfar_write(&regs->gaddr4, 0x0);\n\t\tgfar_write(&regs->gaddr5, 0x0);\n\t\tgfar_write(&regs->gaddr6, 0x0);\n\t\tgfar_write(&regs->gaddr7, 0x0);\n\n\t\t \n\t\tif (priv->extended_hash) {\n\t\t\tem_num = GFAR_EM_NUM + 1;\n\t\t\tgfar_clear_exact_match(dev);\n\t\t\tidx = 1;\n\t\t} else {\n\t\t\tidx = 0;\n\t\t\tem_num = 0;\n\t\t}\n\n\t\tif (netdev_mc_empty(dev))\n\t\t\treturn;\n\n\t\t \n\t\tnetdev_for_each_mc_addr(ha, dev) {\n\t\t\tif (idx < em_num) {\n\t\t\t\tgfar_set_mac_for_addr(dev, idx, ha->addr);\n\t\t\t\tidx++;\n\t\t\t} else\n\t\t\t\tgfar_set_hash_for_addr(dev, ha->addr);\n\t\t}\n\t}\n}\n\nvoid gfar_mac_reset(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tempval;\n\n\t \n\tgfar_write(&regs->maccfg1, MACCFG1_SOFT_RESET);\n\n\t \n\tudelay(3);\n\n\t \n\tgfar_write(&regs->maccfg1, 0);\n\n\tudelay(3);\n\n\tgfar_rx_offload_en(priv);\n\n\t \n\tgfar_write(&regs->maxfrm, GFAR_JUMBO_FRAME_SIZE);\n\tgfar_write(&regs->mrblr, GFAR_RXB_SIZE);\n\n\t \n\tgfar_write(&regs->minflr, MINFLR_INIT_SETTINGS);\n\n\t \n\ttempval = MACCFG2_INIT_SETTINGS;\n\n\t \n\tif (gfar_has_errata(priv, GFAR_ERRATA_74))\n\t\ttempval |= MACCFG2_HUGEFRAME | MACCFG2_LENGTHCHECK;\n\n\tgfar_write(&regs->maccfg2, tempval);\n\n\t \n\tgfar_write(&regs->igaddr0, 0);\n\tgfar_write(&regs->igaddr1, 0);\n\tgfar_write(&regs->igaddr2, 0);\n\tgfar_write(&regs->igaddr3, 0);\n\tgfar_write(&regs->igaddr4, 0);\n\tgfar_write(&regs->igaddr5, 0);\n\tgfar_write(&regs->igaddr6, 0);\n\tgfar_write(&regs->igaddr7, 0);\n\n\tgfar_write(&regs->gaddr0, 0);\n\tgfar_write(&regs->gaddr1, 0);\n\tgfar_write(&regs->gaddr2, 0);\n\tgfar_write(&regs->gaddr3, 0);\n\tgfar_write(&regs->gaddr4, 0);\n\tgfar_write(&regs->gaddr5, 0);\n\tgfar_write(&regs->gaddr6, 0);\n\tgfar_write(&regs->gaddr7, 0);\n\n\tif (priv->extended_hash)\n\t\tgfar_clear_exact_match(priv->ndev);\n\n\tgfar_mac_rx_config(priv);\n\n\tgfar_mac_tx_config(priv);\n\n\tgfar_set_mac_address(priv->ndev);\n\n\tgfar_set_multi(priv->ndev);\n\n\t \n\tgfar_ints_disable(priv);\n\n\t \n\tgfar_configure_coalescing_all(priv);\n}\n\nstatic void gfar_hw_init(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 attrs;\n\n\t \n\tgfar_halt(priv);\n\n\tgfar_mac_reset(priv);\n\n\t \n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_RMON) {\n\t\tmemset_io(&regs->rmon, 0, offsetof(struct rmon_mib, car1));\n\n\t\t \n\t\tgfar_write(&regs->rmon.cam1, 0xffffffff);\n\t\tgfar_write(&regs->rmon.cam2, 0xffffffff);\n\t\t \n\t\tgfar_write(&regs->rmon.car1, 0xffffffff);\n\t\tgfar_write(&regs->rmon.car2, 0xffffffff);\n\t}\n\n\t \n\tgfar_write(&regs->ecntrl, ECNTRL_INIT_SETTINGS);\n\n\t \n\tattrs = ATTRELI_EL(priv->rx_stash_size) |\n\t\tATTRELI_EI(priv->rx_stash_index);\n\n\tgfar_write(&regs->attreli, attrs);\n\n\t \n\tattrs = ATTR_INIT_SETTINGS;\n\n\tif (priv->bd_stash_en)\n\t\tattrs |= ATTR_BDSTASH;\n\n\tif (priv->rx_stash_size != 0)\n\t\tattrs |= ATTR_BUFSTASH;\n\n\tgfar_write(&regs->attr, attrs);\n\n\t \n\tgfar_write(&regs->fifo_tx_thr, DEFAULT_FIFO_TX_THR);\n\tgfar_write(&regs->fifo_tx_starve, DEFAULT_FIFO_TX_STARVE);\n\tgfar_write(&regs->fifo_tx_starve_shutoff, DEFAULT_FIFO_TX_STARVE_OFF);\n\n\t \n\tif (priv->num_grps > 1)\n\t\tgfar_write_isrg(priv);\n}\n\nstatic const struct net_device_ops gfar_netdev_ops = {\n\t.ndo_open = gfar_enet_open,\n\t.ndo_start_xmit = gfar_start_xmit,\n\t.ndo_stop = gfar_close,\n\t.ndo_change_mtu = gfar_change_mtu,\n\t.ndo_set_features = gfar_set_features,\n\t.ndo_set_rx_mode = gfar_set_multi,\n\t.ndo_tx_timeout = gfar_timeout,\n\t.ndo_eth_ioctl = gfar_ioctl,\n\t.ndo_get_stats64 = gfar_get_stats64,\n\t.ndo_change_carrier = fixed_phy_change_carrier,\n\t.ndo_set_mac_address = gfar_set_mac_addr,\n\t.ndo_validate_addr = eth_validate_addr,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller = gfar_netpoll,\n#endif\n};\n\n \nstatic int gfar_probe(struct platform_device *ofdev)\n{\n\tstruct device_node *np = ofdev->dev.of_node;\n\tstruct net_device *dev = NULL;\n\tstruct gfar_private *priv = NULL;\n\tint err = 0, i;\n\n\terr = gfar_of_init(ofdev, &dev);\n\n\tif (err)\n\t\treturn err;\n\n\tpriv = netdev_priv(dev);\n\tpriv->ndev = dev;\n\tpriv->ofdev = ofdev;\n\tpriv->dev = &ofdev->dev;\n\tSET_NETDEV_DEV(dev, &ofdev->dev);\n\n\tINIT_WORK(&priv->reset_task, gfar_reset_task);\n\n\tplatform_set_drvdata(ofdev, priv);\n\n\tgfar_detect_errata(priv);\n\n\t \n\tdev->base_addr = (unsigned long) priv->gfargrp[0].regs;\n\n\t \n\tdev->watchdog_timeo = TX_TIMEOUT;\n\t \n\tdev->mtu = 1500;\n\tdev->min_mtu = 50;\n\tdev->max_mtu = GFAR_JUMBO_FRAME_SIZE - ETH_HLEN;\n\tdev->netdev_ops = &gfar_netdev_ops;\n\tdev->ethtool_ops = &gfar_ethtool_ops;\n\n\t \n\tfor (i = 0; i < priv->num_grps; i++) {\n\t\tnetif_napi_add(dev, &priv->gfargrp[i].napi_rx,\n\t\t\t       gfar_poll_rx_sq);\n\t\tnetif_napi_add_tx_weight(dev, &priv->gfargrp[i].napi_tx,\n\t\t\t\t\t gfar_poll_tx_sq, 2);\n\t}\n\n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_CSUM) {\n\t\tdev->hw_features = NETIF_F_IP_CSUM | NETIF_F_SG |\n\t\t\t\t   NETIF_F_RXCSUM;\n\t\tdev->features |= NETIF_F_IP_CSUM | NETIF_F_SG |\n\t\t\t\t NETIF_F_RXCSUM | NETIF_F_HIGHDMA;\n\t}\n\n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_VLAN) {\n\t\tdev->hw_features |= NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t    NETIF_F_HW_VLAN_CTAG_RX;\n\t\tdev->features |= NETIF_F_HW_VLAN_CTAG_RX;\n\t}\n\n\tdev->priv_flags |= IFF_LIVE_ADDR_CHANGE;\n\n\tgfar_init_addr_hash_table(priv);\n\n\t \n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER)\n\t\tpriv->padding = 8 + DEFAULT_PADDING;\n\n\tif (dev->features & NETIF_F_IP_CSUM ||\n\t    priv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER)\n\t\tdev->needed_headroom = GMAC_FCB_LEN + GMAC_TXPAL_LEN;\n\n\t \n\tfor (i = 0; i < priv->num_tx_queues; i++) {\n\t\tpriv->tx_queue[i]->tx_ring_size = DEFAULT_TX_RING_SIZE;\n\t\tpriv->tx_queue[i]->num_txbdfree = DEFAULT_TX_RING_SIZE;\n\t\tpriv->tx_queue[i]->txcoalescing = DEFAULT_TX_COALESCE;\n\t\tpriv->tx_queue[i]->txic = DEFAULT_TXIC;\n\t}\n\n\tfor (i = 0; i < priv->num_rx_queues; i++) {\n\t\tpriv->rx_queue[i]->rx_ring_size = DEFAULT_RX_RING_SIZE;\n\t\tpriv->rx_queue[i]->rxcoalescing = DEFAULT_RX_COALESCE;\n\t\tpriv->rx_queue[i]->rxic = DEFAULT_RXIC;\n\t}\n\n\t \n\tpriv->rx_filer_enable =\n\t    (priv->device_flags & FSL_GIANFAR_DEV_HAS_RX_FILER) ? 1 : 0;\n\t \n\tpriv->msg_enable = (NETIF_MSG_IFUP << 1 ) - 1;\n\t \n\tif (priv->num_tx_queues == 1)\n\t\tpriv->prio_sched_en = 1;\n\n\tset_bit(GFAR_DOWN, &priv->state);\n\n\tgfar_hw_init(priv);\n\n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_RMON) {\n\t\tstruct rmon_mib __iomem *rmon = &priv->gfargrp[0].regs->rmon;\n\n\t\tspin_lock_init(&priv->rmon_overflow.lock);\n\t\tpriv->rmon_overflow.imask = IMASK_MSRO;\n\t\tgfar_write(&rmon->cam1, gfar_read(&rmon->cam1) & ~CAM1_M1RDR);\n\t}\n\n\t \n\tnetif_carrier_off(dev);\n\n\terr = register_netdev(dev);\n\n\tif (err) {\n\t\tpr_err(\"%s: Cannot register net device, aborting\\n\", dev->name);\n\t\tgoto register_fail;\n\t}\n\n\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MAGIC_PACKET)\n\t\tpriv->wol_supported |= GFAR_WOL_MAGIC;\n\n\tif ((priv->device_flags & FSL_GIANFAR_DEV_HAS_WAKE_ON_FILER) &&\n\t    priv->rx_filer_enable)\n\t\tpriv->wol_supported |= GFAR_WOL_FILER_UCAST;\n\n\tdevice_set_wakeup_capable(&ofdev->dev, priv->wol_supported);\n\n\t \n\tfor (i = 0; i < priv->num_grps; i++) {\n\t\tstruct gfar_priv_grp *grp = &priv->gfargrp[i];\n\t\tif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MULTI_INTR) {\n\t\t\tsprintf(gfar_irq(grp, TX)->name, \"%s%s%c%s\",\n\t\t\t\tdev->name, \"_g\", '0' + i, \"_tx\");\n\t\t\tsprintf(gfar_irq(grp, RX)->name, \"%s%s%c%s\",\n\t\t\t\tdev->name, \"_g\", '0' + i, \"_rx\");\n\t\t\tsprintf(gfar_irq(grp, ER)->name, \"%s%s%c%s\",\n\t\t\t\tdev->name, \"_g\", '0' + i, \"_er\");\n\t\t} else\n\t\t\tstrcpy(gfar_irq(grp, TX)->name, dev->name);\n\t}\n\n\t \n\tgfar_init_filer_table(priv);\n\n\t \n\tnetdev_info(dev, \"mac: %pM\\n\", dev->dev_addr);\n\n\t \n\tnetdev_info(dev, \"Running with NAPI enabled\\n\");\n\tfor (i = 0; i < priv->num_rx_queues; i++)\n\t\tnetdev_info(dev, \"RX BD ring size for Q[%d]: %d\\n\",\n\t\t\t    i, priv->rx_queue[i]->rx_ring_size);\n\tfor (i = 0; i < priv->num_tx_queues; i++)\n\t\tnetdev_info(dev, \"TX BD ring size for Q[%d]: %d\\n\",\n\t\t\t    i, priv->tx_queue[i]->tx_ring_size);\n\n\treturn 0;\n\nregister_fail:\n\tif (of_phy_is_fixed_link(np))\n\t\tof_phy_deregister_fixed_link(np);\n\tunmap_group_regs(priv);\n\tgfar_free_rx_queues(priv);\n\tgfar_free_tx_queues(priv);\n\tof_node_put(priv->phy_node);\n\tof_node_put(priv->tbi_node);\n\tfree_gfar_dev(priv);\n\treturn err;\n}\n\nstatic void gfar_remove(struct platform_device *ofdev)\n{\n\tstruct gfar_private *priv = platform_get_drvdata(ofdev);\n\tstruct device_node *np = ofdev->dev.of_node;\n\n\tof_node_put(priv->phy_node);\n\tof_node_put(priv->tbi_node);\n\n\tunregister_netdev(priv->ndev);\n\n\tif (of_phy_is_fixed_link(np))\n\t\tof_phy_deregister_fixed_link(np);\n\n\tunmap_group_regs(priv);\n\tgfar_free_rx_queues(priv);\n\tgfar_free_tx_queues(priv);\n\tfree_gfar_dev(priv);\n}\n\n#ifdef CONFIG_PM\n\nstatic void __gfar_filer_disable(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 temp;\n\n\ttemp = gfar_read(&regs->rctrl);\n\ttemp &= ~(RCTRL_FILREN | RCTRL_PRSDEP_INIT);\n\tgfar_write(&regs->rctrl, temp);\n}\n\nstatic void __gfar_filer_enable(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 temp;\n\n\ttemp = gfar_read(&regs->rctrl);\n\ttemp |= RCTRL_FILREN | RCTRL_PRSDEP_INIT;\n\tgfar_write(&regs->rctrl, temp);\n}\n\n \nstatic void gfar_filer_config_wol(struct gfar_private *priv)\n{\n\tunsigned int i;\n\tu32 rqfcr;\n\n\t__gfar_filer_disable(priv);\n\n\t \n\trqfcr = RQFCR_RJE | RQFCR_CMP_MATCH;\n\tfor (i = 0; i <= MAX_FILER_IDX; i++)\n\t\tgfar_write_filer(priv, i, rqfcr, 0);\n\n\ti = 0;\n\tif (priv->wol_opts & GFAR_WOL_FILER_UCAST) {\n\t\t \n\t\tstruct net_device *ndev = priv->ndev;\n\t\t \n\t\tu8 qindex = (u8)priv->gfargrp[0].rx_queue->qindex;\n\t\tu32 dest_mac_addr = (ndev->dev_addr[0] << 16) |\n\t\t\t\t    (ndev->dev_addr[1] << 8) |\n\t\t\t\t     ndev->dev_addr[2];\n\n\t\trqfcr = (qindex << 10) | RQFCR_AND |\n\t\t\tRQFCR_CMP_EXACT | RQFCR_PID_DAH;\n\n\t\tgfar_write_filer(priv, i++, rqfcr, dest_mac_addr);\n\n\t\tdest_mac_addr = (ndev->dev_addr[3] << 16) |\n\t\t\t\t(ndev->dev_addr[4] << 8) |\n\t\t\t\t ndev->dev_addr[5];\n\t\trqfcr = (qindex << 10) | RQFCR_GPI |\n\t\t\tRQFCR_CMP_EXACT | RQFCR_PID_DAL;\n\t\tgfar_write_filer(priv, i++, rqfcr, dest_mac_addr);\n\t}\n\n\t__gfar_filer_enable(priv);\n}\n\nstatic void gfar_filer_restore_table(struct gfar_private *priv)\n{\n\tu32 rqfcr, rqfpr;\n\tunsigned int i;\n\n\t__gfar_filer_disable(priv);\n\n\tfor (i = 0; i <= MAX_FILER_IDX; i++) {\n\t\trqfcr = priv->ftp_rqfcr[i];\n\t\trqfpr = priv->ftp_rqfpr[i];\n\t\tgfar_write_filer(priv, i, rqfcr, rqfpr);\n\t}\n\n\t__gfar_filer_enable(priv);\n}\n\n \nstatic void gfar_start_wol_filer(struct gfar_private *priv)\n{\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tempval;\n\tint i = 0;\n\n\t \n\tgfar_write(&regs->rqueue, priv->rqueue);\n\n\t \n\ttempval = gfar_read(&regs->dmactrl);\n\ttempval |= DMACTRL_INIT_SETTINGS;\n\tgfar_write(&regs->dmactrl, tempval);\n\n\t \n\ttempval = gfar_read(&regs->dmactrl);\n\ttempval &= ~DMACTRL_GRS;\n\tgfar_write(&regs->dmactrl, tempval);\n\n\tfor (i = 0; i < priv->num_grps; i++) {\n\t\tregs = priv->gfargrp[i].regs;\n\t\t \n\t\tgfar_write(&regs->rstat, priv->gfargrp[i].rstat);\n\t\t \n\t\tgfar_write(&regs->imask, IMASK_FGPI);\n\t}\n\n\t \n\ttempval = gfar_read(&regs->maccfg1);\n\ttempval |= MACCFG1_RX_EN;\n\tgfar_write(&regs->maccfg1, tempval);\n}\n\nstatic int gfar_suspend(struct device *dev)\n{\n\tstruct gfar_private *priv = dev_get_drvdata(dev);\n\tstruct net_device *ndev = priv->ndev;\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tempval;\n\tu16 wol = priv->wol_opts;\n\n\tif (!netif_running(ndev))\n\t\treturn 0;\n\n\tdisable_napi(priv);\n\tnetif_tx_lock(ndev);\n\tnetif_device_detach(ndev);\n\tnetif_tx_unlock(ndev);\n\n\tgfar_halt(priv);\n\n\tif (wol & GFAR_WOL_MAGIC) {\n\t\t \n\t\tgfar_write(&regs->imask, IMASK_MAG);\n\n\t\t \n\t\ttempval = gfar_read(&regs->maccfg2);\n\t\ttempval |= MACCFG2_MPEN;\n\t\tgfar_write(&regs->maccfg2, tempval);\n\n\t\t \n\t\ttempval = gfar_read(&regs->maccfg1);\n\t\ttempval |= MACCFG1_RX_EN;\n\t\tgfar_write(&regs->maccfg1, tempval);\n\n\t} else if (wol & GFAR_WOL_FILER_UCAST) {\n\t\tgfar_filer_config_wol(priv);\n\t\tgfar_start_wol_filer(priv);\n\n\t} else {\n\t\tphy_stop(ndev->phydev);\n\t}\n\n\treturn 0;\n}\n\nstatic int gfar_resume(struct device *dev)\n{\n\tstruct gfar_private *priv = dev_get_drvdata(dev);\n\tstruct net_device *ndev = priv->ndev;\n\tstruct gfar __iomem *regs = priv->gfargrp[0].regs;\n\tu32 tempval;\n\tu16 wol = priv->wol_opts;\n\n\tif (!netif_running(ndev))\n\t\treturn 0;\n\n\tif (wol & GFAR_WOL_MAGIC) {\n\t\t \n\t\ttempval = gfar_read(&regs->maccfg2);\n\t\ttempval &= ~MACCFG2_MPEN;\n\t\tgfar_write(&regs->maccfg2, tempval);\n\n\t} else if (wol & GFAR_WOL_FILER_UCAST) {\n\t\t \n\t\tgfar_halt(priv);\n\t\tgfar_filer_restore_table(priv);\n\n\t} else {\n\t\tphy_start(ndev->phydev);\n\t}\n\n\tgfar_start(priv);\n\n\tnetif_device_attach(ndev);\n\tenable_napi(priv);\n\n\treturn 0;\n}\n\nstatic int gfar_restore(struct device *dev)\n{\n\tstruct gfar_private *priv = dev_get_drvdata(dev);\n\tstruct net_device *ndev = priv->ndev;\n\n\tif (!netif_running(ndev)) {\n\t\tnetif_device_attach(ndev);\n\n\t\treturn 0;\n\t}\n\n\tgfar_init_bds(ndev);\n\n\tgfar_mac_reset(priv);\n\n\tgfar_init_tx_rx_base(priv);\n\n\tgfar_start(priv);\n\n\tpriv->oldlink = 0;\n\tpriv->oldspeed = 0;\n\tpriv->oldduplex = -1;\n\n\tif (ndev->phydev)\n\t\tphy_start(ndev->phydev);\n\n\tnetif_device_attach(ndev);\n\tenable_napi(priv);\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops gfar_pm_ops = {\n\t.suspend = gfar_suspend,\n\t.resume = gfar_resume,\n\t.freeze = gfar_suspend,\n\t.thaw = gfar_resume,\n\t.restore = gfar_restore,\n};\n\n#define GFAR_PM_OPS (&gfar_pm_ops)\n\n#else\n\n#define GFAR_PM_OPS NULL\n\n#endif\n\nstatic const struct of_device_id gfar_match[] =\n{\n\t{\n\t\t.type = \"network\",\n\t\t.compatible = \"gianfar\",\n\t},\n\t{\n\t\t.compatible = \"fsl,etsec2\",\n\t},\n\t{},\n};\nMODULE_DEVICE_TABLE(of, gfar_match);\n\n \nstatic struct platform_driver gfar_driver = {\n\t.driver = {\n\t\t.name = \"fsl-gianfar\",\n\t\t.pm = GFAR_PM_OPS,\n\t\t.of_match_table = gfar_match,\n\t},\n\t.probe = gfar_probe,\n\t.remove_new = gfar_remove,\n};\n\nmodule_platform_driver(gfar_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}