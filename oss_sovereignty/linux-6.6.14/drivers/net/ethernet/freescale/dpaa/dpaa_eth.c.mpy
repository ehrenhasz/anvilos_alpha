{
  "module_name": "dpaa_eth.c",
  "hash_id": "a7db493d13ccd689eed4ec8171f78452cf6cadd8fd0fe4c32a6d71a8b41cf94e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/freescale/dpaa/dpaa_eth.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/init.h>\n#include <linux/mod_devicetable.h>\n#include <linux/module.h>\n#include <linux/of_mdio.h>\n#include <linux/of_net.h>\n#include <linux/io.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/icmp.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/platform_device.h>\n#include <linux/udp.h>\n#include <linux/tcp.h>\n#include <linux/net.h>\n#include <linux/skbuff.h>\n#include <linux/etherdevice.h>\n#include <linux/if_ether.h>\n#include <linux/highmem.h>\n#include <linux/percpu.h>\n#include <linux/dma-mapping.h>\n#include <linux/sort.h>\n#include <linux/phy_fixed.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <soc/fsl/bman.h>\n#include <soc/fsl/qman.h>\n#include \"fman.h\"\n#include \"fman_port.h\"\n#include \"mac.h\"\n#include \"dpaa_eth.h\"\n\n \n#define CREATE_TRACE_POINTS\n#include \"dpaa_eth_trace.h\"\n\nstatic int debug = -1;\nmodule_param(debug, int, 0444);\nMODULE_PARM_DESC(debug, \"Module/Driver verbosity level (0=none,...,16=all)\");\n\nstatic u16 tx_timeout = 1000;\nmodule_param(tx_timeout, ushort, 0444);\nMODULE_PARM_DESC(tx_timeout, \"The Tx timeout in ms\");\n\n#define FM_FD_STAT_RX_ERRORS\t\t\t\t\t\t\\\n\t(FM_FD_ERR_DMA | FM_FD_ERR_PHYSICAL\t| \\\n\t FM_FD_ERR_SIZE | FM_FD_ERR_CLS_DISCARD | \\\n\t FM_FD_ERR_EXTRACTION | FM_FD_ERR_NO_SCHEME\t| \\\n\t FM_FD_ERR_PRS_TIMEOUT | FM_FD_ERR_PRS_ILL_INSTRUCT | \\\n\t FM_FD_ERR_PRS_HDR_ERR)\n\n#define FM_FD_STAT_TX_ERRORS \\\n\t(FM_FD_ERR_UNSUPPORTED_FORMAT | \\\n\t FM_FD_ERR_LENGTH | FM_FD_ERR_DMA)\n\n#define DPAA_MSG_DEFAULT (NETIF_MSG_DRV | NETIF_MSG_PROBE | \\\n\t\t\t  NETIF_MSG_LINK | NETIF_MSG_IFUP | \\\n\t\t\t  NETIF_MSG_IFDOWN | NETIF_MSG_HW)\n\n#define DPAA_INGRESS_CS_THRESHOLD 0x10000000\n \n\n \n#define DPAA_FQ_TD 0x200000\n\n#define DPAA_CS_THRESHOLD_1G 0x06000000\n \n\n#define DPAA_CS_THRESHOLD_10G 0x10000000\n \n\n \n#define FSL_QMAN_MAX_OAL\t127\n\n \n#ifdef CONFIG_DPAA_ERRATUM_A050385\n \n#define DPAA_FD_DATA_ALIGNMENT  (fman_has_errata_a050385() ? 64 : 16)\n \n#define DPAA_A050385_ALIGN 256\n#define DPAA_FD_RX_DATA_ALIGNMENT (fman_has_errata_a050385() ? \\\n\t\t\t\t   DPAA_A050385_ALIGN : 16)\n#else\n#define DPAA_FD_DATA_ALIGNMENT  16\n#define DPAA_FD_RX_DATA_ALIGNMENT DPAA_FD_DATA_ALIGNMENT\n#endif\n\n \n#define DPAA_SGT_SIZE 256\n\n \n \n#define FM_L3_PARSE_RESULT_IPV4\t0x8000\n \n#define FM_L3_PARSE_RESULT_IPV6\t0x4000\n \n \n#define FM_L4_PARSE_RESULT_UDP\t0x40\n \n#define FM_L4_PARSE_RESULT_TCP\t0x20\n\n \n#define FM_FD_STAT_L4CV         0x00000004\n\n#define DPAA_SGT_MAX_ENTRIES 16  \n#define DPAA_BUFF_RELEASE_MAX 8  \n\n#define FSL_DPAA_BPID_INV\t\t0xff\n#define FSL_DPAA_ETH_MAX_BUF_COUNT\t128\n#define FSL_DPAA_ETH_REFILL_THRESHOLD\t80\n\n#define DPAA_TX_PRIV_DATA_SIZE\t16\n#define DPAA_PARSE_RESULTS_SIZE sizeof(struct fman_prs_result)\n#define DPAA_TIME_STAMP_SIZE 8\n#define DPAA_HASH_RESULTS_SIZE 8\n#define DPAA_HWA_SIZE (DPAA_PARSE_RESULTS_SIZE + DPAA_TIME_STAMP_SIZE \\\n\t\t       + DPAA_HASH_RESULTS_SIZE)\n#define DPAA_RX_PRIV_DATA_DEFAULT_SIZE (DPAA_TX_PRIV_DATA_SIZE + \\\n\t\t\t\t\tXDP_PACKET_HEADROOM - DPAA_HWA_SIZE)\n#ifdef CONFIG_DPAA_ERRATUM_A050385\n#define DPAA_RX_PRIV_DATA_A050385_SIZE (DPAA_A050385_ALIGN - DPAA_HWA_SIZE)\n#define DPAA_RX_PRIV_DATA_SIZE (fman_has_errata_a050385() ? \\\n\t\t\t\tDPAA_RX_PRIV_DATA_A050385_SIZE : \\\n\t\t\t\tDPAA_RX_PRIV_DATA_DEFAULT_SIZE)\n#else\n#define DPAA_RX_PRIV_DATA_SIZE DPAA_RX_PRIV_DATA_DEFAULT_SIZE\n#endif\n\n#define DPAA_ETH_PCD_RXQ_NUM\t128\n\n#define DPAA_ENQUEUE_RETRIES\t100000\n\nenum port_type {RX, TX};\n\nstruct fm_port_fqs {\n\tstruct dpaa_fq *tx_defq;\n\tstruct dpaa_fq *tx_errq;\n\tstruct dpaa_fq *rx_defq;\n\tstruct dpaa_fq *rx_errq;\n\tstruct dpaa_fq *rx_pcdq;\n};\n\n \nstatic struct dpaa_bp *dpaa_bp_array[BM_MAX_NUM_OF_POOLS];\n\n#define DPAA_BP_RAW_SIZE 4096\n\n#ifdef CONFIG_DPAA_ERRATUM_A050385\n#define dpaa_bp_size(raw_size) (SKB_WITH_OVERHEAD(raw_size) & \\\n\t\t\t\t~(DPAA_A050385_ALIGN - 1))\n#else\n#define dpaa_bp_size(raw_size) SKB_WITH_OVERHEAD(raw_size)\n#endif\n\nstatic int dpaa_max_frm;\n\nstatic int dpaa_rx_extra_headroom;\n\n#define dpaa_get_max_mtu()\t\\\n\t(dpaa_max_frm - (VLAN_ETH_HLEN + ETH_FCS_LEN))\n\nstatic void dpaa_eth_cgr_set_speed(struct mac_device *mac_dev, int speed);\n\nstatic int dpaa_netdev_init(struct net_device *net_dev,\n\t\t\t    const struct net_device_ops *dpaa_ops,\n\t\t\t    u16 tx_timeout)\n{\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\tstruct device *dev = net_dev->dev.parent;\n\tstruct mac_device *mac_dev = priv->mac_dev;\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tconst u8 *mac_addr;\n\tint i, err;\n\n\t \n\tfor_each_possible_cpu(i) {\n\t\tpercpu_priv = per_cpu_ptr(priv->percpu_priv, i);\n\t\tpercpu_priv->net_dev = net_dev;\n\t}\n\n\tnet_dev->netdev_ops = dpaa_ops;\n\tmac_addr = mac_dev->addr;\n\n\tnet_dev->mem_start = (unsigned long)priv->mac_dev->res->start;\n\tnet_dev->mem_end = (unsigned long)priv->mac_dev->res->end;\n\n\tnet_dev->min_mtu = ETH_MIN_MTU;\n\tnet_dev->max_mtu = dpaa_get_max_mtu();\n\n\tnet_dev->hw_features |= (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\t\t NETIF_F_LLTX | NETIF_F_RXHASH);\n\n\tnet_dev->hw_features |= NETIF_F_SG | NETIF_F_HIGHDMA;\n\t \n\tnet_dev->features |= NETIF_F_GSO;\n\tnet_dev->features |= NETIF_F_RXCSUM;\n\n\tnet_dev->priv_flags |= IFF_LIVE_ADDR_CHANGE;\n\t \n\tnet_dev->priv_flags &= ~IFF_TX_SKB_SHARING;\n\n\tnet_dev->features |= net_dev->hw_features;\n\tnet_dev->vlan_features = net_dev->features;\n\n\tnet_dev->xdp_features = NETDEV_XDP_ACT_BASIC |\n\t\t\t\tNETDEV_XDP_ACT_REDIRECT |\n\t\t\t\tNETDEV_XDP_ACT_NDO_XMIT;\n\n\tif (is_valid_ether_addr(mac_addr)) {\n\t\tmemcpy(net_dev->perm_addr, mac_addr, net_dev->addr_len);\n\t\teth_hw_addr_set(net_dev, mac_addr);\n\t} else {\n\t\teth_hw_addr_random(net_dev);\n\t\terr = mac_dev->change_addr(mac_dev->fman_mac,\n\t\t\t(const enet_addr_t *)net_dev->dev_addr);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"Failed to set random MAC address\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdev_info(dev, \"Using random MAC address: %pM\\n\",\n\t\t\t net_dev->dev_addr);\n\t}\n\n\tnet_dev->ethtool_ops = &dpaa_ethtool_ops;\n\n\tnet_dev->needed_headroom = priv->tx_headroom;\n\tnet_dev->watchdog_timeo = msecs_to_jiffies(tx_timeout);\n\n\t \n\tmac_dev->phylink_config.dev = &net_dev->dev;\n\tmac_dev->phylink_config.type = PHYLINK_NETDEV;\n\tmac_dev->update_speed = dpaa_eth_cgr_set_speed;\n\tmac_dev->phylink = phylink_create(&mac_dev->phylink_config,\n\t\t\t\t\t  dev_fwnode(mac_dev->dev),\n\t\t\t\t\t  mac_dev->phy_if,\n\t\t\t\t\t  mac_dev->phylink_ops);\n\tif (IS_ERR(mac_dev->phylink)) {\n\t\terr = PTR_ERR(mac_dev->phylink);\n\t\tdev_err_probe(dev, err, \"Could not create phylink\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tnetif_carrier_off(net_dev);\n\n\terr = register_netdev(net_dev);\n\tif (err < 0) {\n\t\tdev_err(dev, \"register_netdev() = %d\\n\", err);\n\t\tphylink_destroy(mac_dev->phylink);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa_stop(struct net_device *net_dev)\n{\n\tstruct mac_device *mac_dev;\n\tstruct dpaa_priv *priv;\n\tint i, error;\n\tint err = 0;\n\n\tpriv = netdev_priv(net_dev);\n\tmac_dev = priv->mac_dev;\n\n\tnetif_tx_stop_all_queues(net_dev);\n\t \n\tmsleep(200);\n\n\tphylink_stop(mac_dev->phylink);\n\tmac_dev->disable(mac_dev->fman_mac);\n\n\tfor (i = 0; i < ARRAY_SIZE(mac_dev->port); i++) {\n\t\terror = fman_port_disable(mac_dev->port[i]);\n\t\tif (error)\n\t\t\terr = error;\n\t}\n\n\tphylink_disconnect_phy(mac_dev->phylink);\n\tnet_dev->phydev = NULL;\n\n\tmsleep(200);\n\n\treturn err;\n}\n\nstatic void dpaa_tx_timeout(struct net_device *net_dev, unsigned int txqueue)\n{\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tconst struct dpaa_priv\t*priv;\n\n\tpriv = netdev_priv(net_dev);\n\tpercpu_priv = this_cpu_ptr(priv->percpu_priv);\n\n\tnetif_crit(priv, timer, net_dev, \"Transmit timeout latency: %u ms\\n\",\n\t\t   jiffies_to_msecs(jiffies - dev_trans_start(net_dev)));\n\n\tpercpu_priv->stats.tx_errors++;\n}\n\n \nstatic void dpaa_get_stats64(struct net_device *net_dev,\n\t\t\t     struct rtnl_link_stats64 *s)\n{\n\tint numstats = sizeof(struct rtnl_link_stats64) / sizeof(u64);\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tu64 *netstats = (u64 *)s;\n\tu64 *cpustats;\n\tint i, j;\n\n\tfor_each_possible_cpu(i) {\n\t\tpercpu_priv = per_cpu_ptr(priv->percpu_priv, i);\n\n\t\tcpustats = (u64 *)&percpu_priv->stats;\n\n\t\t \n\t\tfor (j = 0; j < numstats; j++)\n\t\t\tnetstats[j] += cpustats[j];\n\t}\n}\n\nstatic int dpaa_setup_tc(struct net_device *net_dev, enum tc_setup_type type,\n\t\t\t void *type_data)\n{\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\tstruct tc_mqprio_qopt *mqprio = type_data;\n\tu8 num_tc;\n\tint i;\n\n\tif (type != TC_SETUP_QDISC_MQPRIO)\n\t\treturn -EOPNOTSUPP;\n\n\tmqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;\n\tnum_tc = mqprio->num_tc;\n\n\tif (num_tc == priv->num_tc)\n\t\treturn 0;\n\n\tif (!num_tc) {\n\t\tnetdev_reset_tc(net_dev);\n\t\tgoto out;\n\t}\n\n\tif (num_tc > DPAA_TC_NUM) {\n\t\tnetdev_err(net_dev, \"Too many traffic classes: max %d supported.\\n\",\n\t\t\t   DPAA_TC_NUM);\n\t\treturn -EINVAL;\n\t}\n\n\tnetdev_set_num_tc(net_dev, num_tc);\n\n\tfor (i = 0; i < num_tc; i++)\n\t\tnetdev_set_tc_queue(net_dev, i, DPAA_TC_TXQ_NUM,\n\t\t\t\t    i * DPAA_TC_TXQ_NUM);\n\nout:\n\tpriv->num_tc = num_tc ? : 1;\n\tnetif_set_real_num_tx_queues(net_dev, priv->num_tc * DPAA_TC_TXQ_NUM);\n\treturn 0;\n}\n\nstatic struct mac_device *dpaa_mac_dev_get(struct platform_device *pdev)\n{\n\tstruct dpaa_eth_data *eth_data;\n\tstruct device *dpaa_dev;\n\tstruct mac_device *mac_dev;\n\n\tdpaa_dev = &pdev->dev;\n\teth_data = dpaa_dev->platform_data;\n\tif (!eth_data) {\n\t\tdev_err(dpaa_dev, \"eth_data missing\\n\");\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\tmac_dev = eth_data->mac_dev;\n\tif (!mac_dev) {\n\t\tdev_err(dpaa_dev, \"mac_dev missing\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn mac_dev;\n}\n\nstatic int dpaa_set_mac_address(struct net_device *net_dev, void *addr)\n{\n\tconst struct dpaa_priv *priv;\n\tstruct mac_device *mac_dev;\n\tstruct sockaddr old_addr;\n\tint err;\n\n\tpriv = netdev_priv(net_dev);\n\n\tmemcpy(old_addr.sa_data, net_dev->dev_addr,  ETH_ALEN);\n\n\terr = eth_mac_addr(net_dev, addr);\n\tif (err < 0) {\n\t\tnetif_err(priv, drv, net_dev, \"eth_mac_addr() = %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tmac_dev = priv->mac_dev;\n\n\terr = mac_dev->change_addr(mac_dev->fman_mac,\n\t\t\t\t   (const enet_addr_t *)net_dev->dev_addr);\n\tif (err < 0) {\n\t\tnetif_err(priv, drv, net_dev, \"mac_dev->change_addr() = %d\\n\",\n\t\t\t  err);\n\t\t \n\t\teth_mac_addr(net_dev, &old_addr);\n\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void dpaa_set_rx_mode(struct net_device *net_dev)\n{\n\tconst struct dpaa_priv\t*priv;\n\tint err;\n\n\tpriv = netdev_priv(net_dev);\n\n\tif (!!(net_dev->flags & IFF_PROMISC) != priv->mac_dev->promisc) {\n\t\tpriv->mac_dev->promisc = !priv->mac_dev->promisc;\n\t\terr = priv->mac_dev->set_promisc(priv->mac_dev->fman_mac,\n\t\t\t\t\t\t priv->mac_dev->promisc);\n\t\tif (err < 0)\n\t\t\tnetif_err(priv, drv, net_dev,\n\t\t\t\t  \"mac_dev->set_promisc() = %d\\n\",\n\t\t\t\t  err);\n\t}\n\n\tif (!!(net_dev->flags & IFF_ALLMULTI) != priv->mac_dev->allmulti) {\n\t\tpriv->mac_dev->allmulti = !priv->mac_dev->allmulti;\n\t\terr = priv->mac_dev->set_allmulti(priv->mac_dev->fman_mac,\n\t\t\t\t\t\t  priv->mac_dev->allmulti);\n\t\tif (err < 0)\n\t\t\tnetif_err(priv, drv, net_dev,\n\t\t\t\t  \"mac_dev->set_allmulti() = %d\\n\",\n\t\t\t\t  err);\n\t}\n\n\terr = priv->mac_dev->set_multi(net_dev, priv->mac_dev);\n\tif (err < 0)\n\t\tnetif_err(priv, drv, net_dev, \"mac_dev->set_multi() = %d\\n\",\n\t\t\t  err);\n}\n\nstatic struct dpaa_bp *dpaa_bpid2pool(int bpid)\n{\n\tif (WARN_ON(bpid < 0 || bpid >= BM_MAX_NUM_OF_POOLS))\n\t\treturn NULL;\n\n\treturn dpaa_bp_array[bpid];\n}\n\n \nstatic bool dpaa_bpid2pool_use(int bpid)\n{\n\tif (dpaa_bpid2pool(bpid)) {\n\t\trefcount_inc(&dpaa_bp_array[bpid]->refs);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic void dpaa_bpid2pool_map(int bpid, struct dpaa_bp *dpaa_bp)\n{\n\tdpaa_bp_array[bpid] = dpaa_bp;\n\trefcount_set(&dpaa_bp->refs, 1);\n}\n\nstatic int dpaa_bp_alloc_pool(struct dpaa_bp *dpaa_bp)\n{\n\tint err;\n\n\tif (dpaa_bp->size == 0 || dpaa_bp->config_count == 0) {\n\t\tpr_err(\"%s: Buffer pool is not properly initialized! Missing size or initial number of buffers\\n\",\n\t\t       __func__);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (dpaa_bp->bpid != FSL_DPAA_BPID_INV &&\n\t    dpaa_bpid2pool_use(dpaa_bp->bpid))\n\t\treturn 0;\n\n\tif (dpaa_bp->bpid == FSL_DPAA_BPID_INV) {\n\t\tdpaa_bp->pool = bman_new_pool();\n\t\tif (!dpaa_bp->pool) {\n\t\t\tpr_err(\"%s: bman_new_pool() failed\\n\",\n\t\t\t       __func__);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tdpaa_bp->bpid = (u8)bman_get_bpid(dpaa_bp->pool);\n\t}\n\n\tif (dpaa_bp->seed_cb) {\n\t\terr = dpaa_bp->seed_cb(dpaa_bp);\n\t\tif (err)\n\t\t\tgoto pool_seed_failed;\n\t}\n\n\tdpaa_bpid2pool_map(dpaa_bp->bpid, dpaa_bp);\n\n\treturn 0;\n\npool_seed_failed:\n\tpr_err(\"%s: pool seeding failed\\n\", __func__);\n\tbman_free_pool(dpaa_bp->pool);\n\n\treturn err;\n}\n\n \nstatic void dpaa_bp_drain(struct dpaa_bp *bp)\n{\n\tu8 num = 8;\n\tint ret;\n\n\tdo {\n\t\tstruct bm_buffer bmb[8];\n\t\tint i;\n\n\t\tret = bman_acquire(bp->pool, bmb, num);\n\t\tif (ret < 0) {\n\t\t\tif (num == 8) {\n\t\t\t\t \n\t\t\t\tnum = 1;\n\t\t\t\tret = 1;\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (bp->free_buf_cb)\n\t\t\tfor (i = 0; i < num; i++)\n\t\t\t\tbp->free_buf_cb(bp, &bmb[i]);\n\t} while (ret > 0);\n}\n\nstatic void dpaa_bp_free(struct dpaa_bp *dpaa_bp)\n{\n\tstruct dpaa_bp *bp = dpaa_bpid2pool(dpaa_bp->bpid);\n\n\t \n\tif (!bp)\n\t\treturn;\n\n\tif (!refcount_dec_and_test(&bp->refs))\n\t\treturn;\n\n\tif (bp->free_buf_cb)\n\t\tdpaa_bp_drain(bp);\n\n\tdpaa_bp_array[bp->bpid] = NULL;\n\tbman_free_pool(bp->pool);\n}\n\nstatic void dpaa_bps_free(struct dpaa_priv *priv)\n{\n\tdpaa_bp_free(priv->dpaa_bp);\n}\n\n \nstatic inline void dpaa_assign_wq(struct dpaa_fq *fq, int idx)\n{\n\tswitch (fq->fq_type) {\n\tcase FQ_TYPE_TX_CONFIRM:\n\tcase FQ_TYPE_TX_CONF_MQ:\n\t\tfq->wq = 1;\n\t\tbreak;\n\tcase FQ_TYPE_RX_ERROR:\n\tcase FQ_TYPE_TX_ERROR:\n\t\tfq->wq = 5;\n\t\tbreak;\n\tcase FQ_TYPE_RX_DEFAULT:\n\tcase FQ_TYPE_RX_PCD:\n\t\tfq->wq = 6;\n\t\tbreak;\n\tcase FQ_TYPE_TX:\n\t\tswitch (idx / DPAA_TC_TXQ_NUM) {\n\t\tcase 0:\n\t\t\t \n\t\t\tfq->wq = 6;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\t \n\t\t\tfq->wq = 2;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t \n\t\t\tfq->wq = 1;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\t \n\t\t\tfq->wq = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN(1, \"Too many TX FQs: more than %d!\\n\",\n\t\t\t     DPAA_ETH_TXQ_NUM);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"Invalid FQ type %d for FQID %d!\\n\",\n\t\t     fq->fq_type, fq->fqid);\n\t}\n}\n\nstatic struct dpaa_fq *dpaa_fq_alloc(struct device *dev,\n\t\t\t\t     u32 start, u32 count,\n\t\t\t\t     struct list_head *list,\n\t\t\t\t     enum dpaa_fq_type fq_type)\n{\n\tstruct dpaa_fq *dpaa_fq;\n\tint i;\n\n\tdpaa_fq = devm_kcalloc(dev, count, sizeof(*dpaa_fq),\n\t\t\t       GFP_KERNEL);\n\tif (!dpaa_fq)\n\t\treturn NULL;\n\n\tfor (i = 0; i < count; i++) {\n\t\tdpaa_fq[i].fq_type = fq_type;\n\t\tdpaa_fq[i].fqid = start ? start + i : 0;\n\t\tlist_add_tail(&dpaa_fq[i].list, list);\n\t}\n\n\tfor (i = 0; i < count; i++)\n\t\tdpaa_assign_wq(dpaa_fq + i, i);\n\n\treturn dpaa_fq;\n}\n\nstatic int dpaa_alloc_all_fqs(struct device *dev, struct list_head *list,\n\t\t\t      struct fm_port_fqs *port_fqs)\n{\n\tstruct dpaa_fq *dpaa_fq;\n\tu32 fq_base, fq_base_aligned, i;\n\n\tdpaa_fq = dpaa_fq_alloc(dev, 0, 1, list, FQ_TYPE_RX_ERROR);\n\tif (!dpaa_fq)\n\t\tgoto fq_alloc_failed;\n\n\tport_fqs->rx_errq = &dpaa_fq[0];\n\n\tdpaa_fq = dpaa_fq_alloc(dev, 0, 1, list, FQ_TYPE_RX_DEFAULT);\n\tif (!dpaa_fq)\n\t\tgoto fq_alloc_failed;\n\n\tport_fqs->rx_defq = &dpaa_fq[0];\n\n\t \n\tif (qman_alloc_fqid_range(&fq_base, 2 * DPAA_ETH_PCD_RXQ_NUM))\n\t\tgoto fq_alloc_failed;\n\n\tfq_base_aligned = ALIGN(fq_base, DPAA_ETH_PCD_RXQ_NUM);\n\n\tfor (i = fq_base; i < fq_base_aligned; i++)\n\t\tqman_release_fqid(i);\n\n\tfor (i = fq_base_aligned + DPAA_ETH_PCD_RXQ_NUM;\n\t     i < (fq_base + 2 * DPAA_ETH_PCD_RXQ_NUM); i++)\n\t\tqman_release_fqid(i);\n\n\tdpaa_fq = dpaa_fq_alloc(dev, fq_base_aligned, DPAA_ETH_PCD_RXQ_NUM,\n\t\t\t\tlist, FQ_TYPE_RX_PCD);\n\tif (!dpaa_fq)\n\t\tgoto fq_alloc_failed;\n\n\tport_fqs->rx_pcdq = &dpaa_fq[0];\n\n\tif (!dpaa_fq_alloc(dev, 0, DPAA_ETH_TXQ_NUM, list, FQ_TYPE_TX_CONF_MQ))\n\t\tgoto fq_alloc_failed;\n\n\tdpaa_fq = dpaa_fq_alloc(dev, 0, 1, list, FQ_TYPE_TX_ERROR);\n\tif (!dpaa_fq)\n\t\tgoto fq_alloc_failed;\n\n\tport_fqs->tx_errq = &dpaa_fq[0];\n\n\tdpaa_fq = dpaa_fq_alloc(dev, 0, 1, list, FQ_TYPE_TX_CONFIRM);\n\tif (!dpaa_fq)\n\t\tgoto fq_alloc_failed;\n\n\tport_fqs->tx_defq = &dpaa_fq[0];\n\n\tif (!dpaa_fq_alloc(dev, 0, DPAA_ETH_TXQ_NUM, list, FQ_TYPE_TX))\n\t\tgoto fq_alloc_failed;\n\n\treturn 0;\n\nfq_alloc_failed:\n\tdev_err(dev, \"dpaa_fq_alloc() failed\\n\");\n\treturn -ENOMEM;\n}\n\nstatic u32 rx_pool_channel;\nstatic DEFINE_SPINLOCK(rx_pool_channel_init);\n\nstatic int dpaa_get_channel(void)\n{\n\tspin_lock(&rx_pool_channel_init);\n\tif (!rx_pool_channel) {\n\t\tu32 pool;\n\t\tint ret;\n\n\t\tret = qman_alloc_pool(&pool);\n\n\t\tif (!ret)\n\t\t\trx_pool_channel = pool;\n\t}\n\tspin_unlock(&rx_pool_channel_init);\n\tif (!rx_pool_channel)\n\t\treturn -ENOMEM;\n\treturn rx_pool_channel;\n}\n\nstatic void dpaa_release_channel(void)\n{\n\tqman_release_pool(rx_pool_channel);\n}\n\nstatic void dpaa_eth_add_channel(u16 channel, struct device *dev)\n{\n\tu32 pool = QM_SDQCR_CHANNELS_POOL_CONV(channel);\n\tconst cpumask_t *cpus = qman_affine_cpus();\n\tstruct qman_portal *portal;\n\tint cpu;\n\n\tfor_each_cpu_and(cpu, cpus, cpu_online_mask) {\n\t\tportal = qman_get_affine_portal(cpu);\n\t\tqman_p_static_dequeue_add(portal, pool);\n\t\tqman_start_using_portal(portal, dev);\n\t}\n}\n\n \nstatic void dpaa_eth_cgscn(struct qman_portal *qm, struct qman_cgr *cgr,\n\t\t\t   int congested)\n{\n\tstruct dpaa_priv *priv = (struct dpaa_priv *)container_of(cgr,\n\t\tstruct dpaa_priv, cgr_data.cgr);\n\n\tif (congested) {\n\t\tpriv->cgr_data.congestion_start_jiffies = jiffies;\n\t\tnetif_tx_stop_all_queues(priv->net_dev);\n\t\tpriv->cgr_data.cgr_congested_count++;\n\t} else {\n\t\tpriv->cgr_data.congested_jiffies +=\n\t\t\t(jiffies - priv->cgr_data.congestion_start_jiffies);\n\t\tnetif_tx_wake_all_queues(priv->net_dev);\n\t}\n}\n\nstatic int dpaa_eth_cgr_init(struct dpaa_priv *priv)\n{\n\tstruct qm_mcc_initcgr initcgr;\n\tu32 cs_th;\n\tint err;\n\n\terr = qman_alloc_cgrid(&priv->cgr_data.cgr.cgrid);\n\tif (err < 0) {\n\t\tif (netif_msg_drv(priv))\n\t\t\tpr_err(\"%s: Error %d allocating CGR ID\\n\",\n\t\t\t       __func__, err);\n\t\tgoto out_error;\n\t}\n\tpriv->cgr_data.cgr.cb = dpaa_eth_cgscn;\n\n\t \n\tmemset(&initcgr, 0, sizeof(initcgr));\n\tinitcgr.we_mask = cpu_to_be16(QM_CGR_WE_CSCN_EN | QM_CGR_WE_CS_THRES);\n\tinitcgr.cgr.cscn_en = QM_CGR_EN;\n\n\t \n\tif (priv->mac_dev->phylink_config.mac_capabilities & MAC_10000FD)\n\t\tcs_th = DPAA_CS_THRESHOLD_10G;\n\telse\n\t\tcs_th = DPAA_CS_THRESHOLD_1G;\n\tqm_cgr_cs_thres_set64(&initcgr.cgr.cs_thres, cs_th, 1);\n\n\tinitcgr.we_mask |= cpu_to_be16(QM_CGR_WE_CSTD_EN);\n\tinitcgr.cgr.cstd_en = QM_CGR_EN;\n\n\terr = qman_create_cgr(&priv->cgr_data.cgr, QMAN_CGR_FLAG_USE_INIT,\n\t\t\t      &initcgr);\n\tif (err < 0) {\n\t\tif (netif_msg_drv(priv))\n\t\t\tpr_err(\"%s: Error %d creating CGR with ID %d\\n\",\n\t\t\t       __func__, err, priv->cgr_data.cgr.cgrid);\n\t\tqman_release_cgrid(priv->cgr_data.cgr.cgrid);\n\t\tgoto out_error;\n\t}\n\tif (netif_msg_drv(priv))\n\t\tpr_debug(\"Created CGR %d for netdev with hwaddr %pM on QMan channel %d\\n\",\n\t\t\t priv->cgr_data.cgr.cgrid, priv->mac_dev->addr,\n\t\t\t priv->cgr_data.cgr.chan);\n\nout_error:\n\treturn err;\n}\n\nstatic void dpaa_eth_cgr_set_speed(struct mac_device *mac_dev, int speed)\n{\n\tstruct net_device *net_dev = to_net_dev(mac_dev->phylink_config.dev);\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\tstruct qm_mcc_initcgr opts = { };\n\tu32 cs_th;\n\tint err;\n\n\topts.we_mask = cpu_to_be16(QM_CGR_WE_CS_THRES);\n\tswitch (speed) {\n\tcase SPEED_10000:\n\t\tcs_th = DPAA_CS_THRESHOLD_10G;\n\t\tbreak;\n\tcase SPEED_1000:\n\tdefault:\n\t\tcs_th = DPAA_CS_THRESHOLD_1G;\n\t\tbreak;\n\t}\n\tqm_cgr_cs_thres_set64(&opts.cgr.cs_thres, cs_th, 1);\n\n\terr = qman_update_cgr_safe(&priv->cgr_data.cgr, &opts);\n\tif (err)\n\t\tnetdev_err(net_dev, \"could not update speed: %d\\n\", err);\n}\n\nstatic inline void dpaa_setup_ingress(const struct dpaa_priv *priv,\n\t\t\t\t      struct dpaa_fq *fq,\n\t\t\t\t      const struct qman_fq *template)\n{\n\tfq->fq_base = *template;\n\tfq->net_dev = priv->net_dev;\n\n\tfq->flags = QMAN_FQ_FLAG_NO_ENQUEUE;\n\tfq->channel = priv->channel;\n}\n\nstatic inline void dpaa_setup_egress(const struct dpaa_priv *priv,\n\t\t\t\t     struct dpaa_fq *fq,\n\t\t\t\t     struct fman_port *port,\n\t\t\t\t     const struct qman_fq *template)\n{\n\tfq->fq_base = *template;\n\tfq->net_dev = priv->net_dev;\n\n\tif (port) {\n\t\tfq->flags = QMAN_FQ_FLAG_TO_DCPORTAL;\n\t\tfq->channel = (u16)fman_port_get_qman_channel_id(port);\n\t} else {\n\t\tfq->flags = QMAN_FQ_FLAG_NO_MODIFY;\n\t}\n}\n\nstatic void dpaa_fq_setup(struct dpaa_priv *priv,\n\t\t\t  const struct dpaa_fq_cbs *fq_cbs,\n\t\t\t  struct fman_port *tx_port)\n{\n\tint egress_cnt = 0, conf_cnt = 0, num_portals = 0, portal_cnt = 0, cpu;\n\tconst cpumask_t *affine_cpus = qman_affine_cpus();\n\tu16 channels[NR_CPUS];\n\tstruct dpaa_fq *fq;\n\n\tfor_each_cpu_and(cpu, affine_cpus, cpu_online_mask)\n\t\tchannels[num_portals++] = qman_affine_channel(cpu);\n\n\tif (num_portals == 0)\n\t\tdev_err(priv->net_dev->dev.parent,\n\t\t\t\"No Qman software (affine) channels found\\n\");\n\n\t \n\tlist_for_each_entry(fq, &priv->dpaa_fq_list, list) {\n\t\tswitch (fq->fq_type) {\n\t\tcase FQ_TYPE_RX_DEFAULT:\n\t\t\tdpaa_setup_ingress(priv, fq, &fq_cbs->rx_defq);\n\t\t\tbreak;\n\t\tcase FQ_TYPE_RX_ERROR:\n\t\t\tdpaa_setup_ingress(priv, fq, &fq_cbs->rx_errq);\n\t\t\tbreak;\n\t\tcase FQ_TYPE_RX_PCD:\n\t\t\tif (!num_portals)\n\t\t\t\tcontinue;\n\t\t\tdpaa_setup_ingress(priv, fq, &fq_cbs->rx_defq);\n\t\t\tfq->channel = channels[portal_cnt++ % num_portals];\n\t\t\tbreak;\n\t\tcase FQ_TYPE_TX:\n\t\t\tdpaa_setup_egress(priv, fq, tx_port,\n\t\t\t\t\t  &fq_cbs->egress_ern);\n\t\t\t \n\t\t\tif (egress_cnt < DPAA_ETH_TXQ_NUM)\n\t\t\t\tpriv->egress_fqs[egress_cnt++] = &fq->fq_base;\n\t\t\tbreak;\n\t\tcase FQ_TYPE_TX_CONF_MQ:\n\t\t\tpriv->conf_fqs[conf_cnt++] = &fq->fq_base;\n\t\t\tfallthrough;\n\t\tcase FQ_TYPE_TX_CONFIRM:\n\t\t\tdpaa_setup_ingress(priv, fq, &fq_cbs->tx_defq);\n\t\t\tbreak;\n\t\tcase FQ_TYPE_TX_ERROR:\n\t\t\tdpaa_setup_ingress(priv, fq, &fq_cbs->tx_errq);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_warn(priv->net_dev->dev.parent,\n\t\t\t\t \"Unknown FQ type detected!\\n\");\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t  \n\twhile (egress_cnt < DPAA_ETH_TXQ_NUM) {\n\t\tlist_for_each_entry(fq, &priv->dpaa_fq_list, list) {\n\t\t\tif (fq->fq_type != FQ_TYPE_TX)\n\t\t\t\tcontinue;\n\t\t\tpriv->egress_fqs[egress_cnt++] = &fq->fq_base;\n\t\t\tif (egress_cnt == DPAA_ETH_TXQ_NUM)\n\t\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic inline int dpaa_tx_fq_to_id(const struct dpaa_priv *priv,\n\t\t\t\t   struct qman_fq *tx_fq)\n{\n\tint i;\n\n\tfor (i = 0; i < DPAA_ETH_TXQ_NUM; i++)\n\t\tif (priv->egress_fqs[i] == tx_fq)\n\t\t\treturn i;\n\n\treturn -EINVAL;\n}\n\nstatic int dpaa_fq_init(struct dpaa_fq *dpaa_fq, bool td_enable)\n{\n\tconst struct dpaa_priv\t*priv;\n\tstruct qman_fq *confq = NULL;\n\tstruct qm_mcc_initfq initfq;\n\tstruct device *dev;\n\tstruct qman_fq *fq;\n\tint queue_id;\n\tint err;\n\n\tpriv = netdev_priv(dpaa_fq->net_dev);\n\tdev = dpaa_fq->net_dev->dev.parent;\n\n\tif (dpaa_fq->fqid == 0)\n\t\tdpaa_fq->flags |= QMAN_FQ_FLAG_DYNAMIC_FQID;\n\n\tdpaa_fq->init = !(dpaa_fq->flags & QMAN_FQ_FLAG_NO_MODIFY);\n\n\terr = qman_create_fq(dpaa_fq->fqid, dpaa_fq->flags, &dpaa_fq->fq_base);\n\tif (err) {\n\t\tdev_err(dev, \"qman_create_fq() failed\\n\");\n\t\treturn err;\n\t}\n\tfq = &dpaa_fq->fq_base;\n\n\tif (dpaa_fq->init) {\n\t\tmemset(&initfq, 0, sizeof(initfq));\n\n\t\tinitfq.we_mask = cpu_to_be16(QM_INITFQ_WE_FQCTRL);\n\t\t \n\t\tinitfq.fqd.fq_ctrl = cpu_to_be16(QM_FQCTRL_PREFERINCACHE);\n\n\t\t \n\t\tif (dpaa_fq->fq_type == FQ_TYPE_TX_CONFIRM)\n\t\t\tinitfq.fqd.fq_ctrl |= cpu_to_be16(QM_FQCTRL_AVOIDBLOCK);\n\n\t\t \n\t\tinitfq.we_mask |= cpu_to_be16(QM_INITFQ_WE_DESTWQ);\n\n\t\tqm_fqd_set_destwq(&initfq.fqd, dpaa_fq->channel, dpaa_fq->wq);\n\n\t\t \n\t\tif (dpaa_fq->fq_type == FQ_TYPE_TX ||\n\t\t    dpaa_fq->fq_type == FQ_TYPE_TX_CONFIRM ||\n\t\t    dpaa_fq->fq_type == FQ_TYPE_TX_CONF_MQ) {\n\t\t\tinitfq.we_mask |= cpu_to_be16(QM_INITFQ_WE_CGID);\n\t\t\tinitfq.fqd.fq_ctrl |= cpu_to_be16(QM_FQCTRL_CGE);\n\t\t\tinitfq.fqd.cgid = (u8)priv->cgr_data.cgr.cgrid;\n\t\t\t \n\t\t\tinitfq.we_mask |= cpu_to_be16(QM_INITFQ_WE_OAC);\n\t\t\tqm_fqd_set_oac(&initfq.fqd, QM_OAC_CG);\n\t\t\tqm_fqd_set_oal(&initfq.fqd,\n\t\t\t\t       min(sizeof(struct sk_buff) +\n\t\t\t\t       priv->tx_headroom,\n\t\t\t\t       (size_t)FSL_QMAN_MAX_OAL));\n\t\t}\n\n\t\tif (td_enable) {\n\t\t\tinitfq.we_mask |= cpu_to_be16(QM_INITFQ_WE_TDTHRESH);\n\t\t\tqm_fqd_set_taildrop(&initfq.fqd, DPAA_FQ_TD, 1);\n\t\t\tinitfq.fqd.fq_ctrl = cpu_to_be16(QM_FQCTRL_TDE);\n\t\t}\n\n\t\tif (dpaa_fq->fq_type == FQ_TYPE_TX) {\n\t\t\tqueue_id = dpaa_tx_fq_to_id(priv, &dpaa_fq->fq_base);\n\t\t\tif (queue_id >= 0)\n\t\t\t\tconfq = priv->conf_fqs[queue_id];\n\t\t\tif (confq) {\n\t\t\t\tinitfq.we_mask |=\n\t\t\t\t\tcpu_to_be16(QM_INITFQ_WE_CONTEXTA);\n\t\t\t \n\t\t\t\tqm_fqd_context_a_set64(&initfq.fqd,\n\t\t\t\t\t\t       0x1e00000080000000ULL);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (priv->use_ingress_cgr &&\n\t\t    (dpaa_fq->fq_type == FQ_TYPE_RX_DEFAULT ||\n\t\t     dpaa_fq->fq_type == FQ_TYPE_RX_ERROR ||\n\t\t     dpaa_fq->fq_type == FQ_TYPE_RX_PCD)) {\n\t\t\tinitfq.we_mask |= cpu_to_be16(QM_INITFQ_WE_CGID);\n\t\t\tinitfq.fqd.fq_ctrl |= cpu_to_be16(QM_FQCTRL_CGE);\n\t\t\tinitfq.fqd.cgid = (u8)priv->ingress_cgr.cgrid;\n\t\t\t \n\t\t\tinitfq.we_mask |= cpu_to_be16(QM_INITFQ_WE_OAC);\n\t\t\tqm_fqd_set_oac(&initfq.fqd, QM_OAC_CG);\n\t\t\tqm_fqd_set_oal(&initfq.fqd,\n\t\t\t\t       min(sizeof(struct sk_buff) +\n\t\t\t\t       priv->tx_headroom,\n\t\t\t\t       (size_t)FSL_QMAN_MAX_OAL));\n\t\t}\n\n\t\t \n\t\tif (dpaa_fq->flags & QMAN_FQ_FLAG_NO_ENQUEUE) {\n\t\t\tinitfq.we_mask |= cpu_to_be16(QM_INITFQ_WE_CONTEXTA);\n\t\t\tinitfq.fqd.fq_ctrl |= cpu_to_be16(QM_FQCTRL_HOLDACTIVE |\n\t\t\t\t\t\tQM_FQCTRL_CTXASTASHING);\n\t\t\tinitfq.fqd.context_a.stashing.exclusive =\n\t\t\t\tQM_STASHING_EXCL_DATA | QM_STASHING_EXCL_CTX |\n\t\t\t\tQM_STASHING_EXCL_ANNOTATION;\n\t\t\tqm_fqd_set_stashing(&initfq.fqd, 1, 2,\n\t\t\t\t\t    DIV_ROUND_UP(sizeof(struct qman_fq),\n\t\t\t\t\t\t\t 64));\n\t\t}\n\n\t\terr = qman_init_fq(fq, QMAN_INITFQ_FLAG_SCHED, &initfq);\n\t\tif (err < 0) {\n\t\t\tdev_err(dev, \"qman_init_fq(%u) = %d\\n\",\n\t\t\t\tqman_fq_fqid(fq), err);\n\t\t\tqman_destroy_fq(fq);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tdpaa_fq->fqid = qman_fq_fqid(fq);\n\n\tif (dpaa_fq->fq_type == FQ_TYPE_RX_DEFAULT ||\n\t    dpaa_fq->fq_type == FQ_TYPE_RX_PCD) {\n\t\terr = xdp_rxq_info_reg(&dpaa_fq->xdp_rxq, dpaa_fq->net_dev,\n\t\t\t\t       dpaa_fq->fqid, 0);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"xdp_rxq_info_reg() = %d\\n\", err);\n\t\t\treturn err;\n\t\t}\n\n\t\terr = xdp_rxq_info_reg_mem_model(&dpaa_fq->xdp_rxq,\n\t\t\t\t\t\t MEM_TYPE_PAGE_ORDER0, NULL);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"xdp_rxq_info_reg_mem_model() = %d\\n\",\n\t\t\t\terr);\n\t\t\txdp_rxq_info_unreg(&dpaa_fq->xdp_rxq);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa_fq_free_entry(struct device *dev, struct qman_fq *fq)\n{\n\tconst struct dpaa_priv  *priv;\n\tstruct dpaa_fq *dpaa_fq;\n\tint err, error;\n\n\terr = 0;\n\n\tdpaa_fq = container_of(fq, struct dpaa_fq, fq_base);\n\tpriv = netdev_priv(dpaa_fq->net_dev);\n\n\tif (dpaa_fq->init) {\n\t\terr = qman_retire_fq(fq, NULL);\n\t\tif (err < 0 && netif_msg_drv(priv))\n\t\t\tdev_err(dev, \"qman_retire_fq(%u) = %d\\n\",\n\t\t\t\tqman_fq_fqid(fq), err);\n\n\t\terror = qman_oos_fq(fq);\n\t\tif (error < 0 && netif_msg_drv(priv)) {\n\t\t\tdev_err(dev, \"qman_oos_fq(%u) = %d\\n\",\n\t\t\t\tqman_fq_fqid(fq), error);\n\t\t\tif (err >= 0)\n\t\t\t\terr = error;\n\t\t}\n\t}\n\n\tif ((dpaa_fq->fq_type == FQ_TYPE_RX_DEFAULT ||\n\t     dpaa_fq->fq_type == FQ_TYPE_RX_PCD) &&\n\t    xdp_rxq_info_is_reg(&dpaa_fq->xdp_rxq))\n\t\txdp_rxq_info_unreg(&dpaa_fq->xdp_rxq);\n\n\tqman_destroy_fq(fq);\n\tlist_del(&dpaa_fq->list);\n\n\treturn err;\n}\n\nstatic int dpaa_fq_free(struct device *dev, struct list_head *list)\n{\n\tstruct dpaa_fq *dpaa_fq, *tmp;\n\tint err, error;\n\n\terr = 0;\n\tlist_for_each_entry_safe(dpaa_fq, tmp, list, list) {\n\t\terror = dpaa_fq_free_entry(dev, (struct qman_fq *)dpaa_fq);\n\t\tif (error < 0 && err >= 0)\n\t\t\terr = error;\n\t}\n\n\treturn err;\n}\n\nstatic int dpaa_eth_init_tx_port(struct fman_port *port, struct dpaa_fq *errq,\n\t\t\t\t struct dpaa_fq *defq,\n\t\t\t\t struct dpaa_buffer_layout *buf_layout)\n{\n\tstruct fman_buffer_prefix_content buf_prefix_content;\n\tstruct fman_port_params params;\n\tint err;\n\n\tmemset(&params, 0, sizeof(params));\n\tmemset(&buf_prefix_content, 0, sizeof(buf_prefix_content));\n\n\tbuf_prefix_content.priv_data_size = buf_layout->priv_data_size;\n\tbuf_prefix_content.pass_prs_result = true;\n\tbuf_prefix_content.pass_hash_result = true;\n\tbuf_prefix_content.pass_time_stamp = true;\n\tbuf_prefix_content.data_align = DPAA_FD_DATA_ALIGNMENT;\n\n\tparams.specific_params.non_rx_params.err_fqid = errq->fqid;\n\tparams.specific_params.non_rx_params.dflt_fqid = defq->fqid;\n\n\terr = fman_port_config(port, &params);\n\tif (err) {\n\t\tpr_err(\"%s: fman_port_config failed\\n\", __func__);\n\t\treturn err;\n\t}\n\n\terr = fman_port_cfg_buf_prefix_content(port, &buf_prefix_content);\n\tif (err) {\n\t\tpr_err(\"%s: fman_port_cfg_buf_prefix_content failed\\n\",\n\t\t       __func__);\n\t\treturn err;\n\t}\n\n\terr = fman_port_init(port);\n\tif (err)\n\t\tpr_err(\"%s: fm_port_init failed\\n\", __func__);\n\n\treturn err;\n}\n\nstatic int dpaa_eth_init_rx_port(struct fman_port *port, struct dpaa_bp *bp,\n\t\t\t\t struct dpaa_fq *errq,\n\t\t\t\t struct dpaa_fq *defq, struct dpaa_fq *pcdq,\n\t\t\t\t struct dpaa_buffer_layout *buf_layout)\n{\n\tstruct fman_buffer_prefix_content buf_prefix_content;\n\tstruct fman_port_rx_params *rx_p;\n\tstruct fman_port_params params;\n\tint err;\n\n\tmemset(&params, 0, sizeof(params));\n\tmemset(&buf_prefix_content, 0, sizeof(buf_prefix_content));\n\n\tbuf_prefix_content.priv_data_size = buf_layout->priv_data_size;\n\tbuf_prefix_content.pass_prs_result = true;\n\tbuf_prefix_content.pass_hash_result = true;\n\tbuf_prefix_content.pass_time_stamp = true;\n\tbuf_prefix_content.data_align = DPAA_FD_RX_DATA_ALIGNMENT;\n\n\trx_p = &params.specific_params.rx_params;\n\trx_p->err_fqid = errq->fqid;\n\trx_p->dflt_fqid = defq->fqid;\n\tif (pcdq) {\n\t\trx_p->pcd_base_fqid = pcdq->fqid;\n\t\trx_p->pcd_fqs_count = DPAA_ETH_PCD_RXQ_NUM;\n\t}\n\n\trx_p->ext_buf_pools.num_of_pools_used = 1;\n\trx_p->ext_buf_pools.ext_buf_pool[0].id =  bp->bpid;\n\trx_p->ext_buf_pools.ext_buf_pool[0].size = (u16)bp->size;\n\n\terr = fman_port_config(port, &params);\n\tif (err) {\n\t\tpr_err(\"%s: fman_port_config failed\\n\", __func__);\n\t\treturn err;\n\t}\n\n\terr = fman_port_cfg_buf_prefix_content(port, &buf_prefix_content);\n\tif (err) {\n\t\tpr_err(\"%s: fman_port_cfg_buf_prefix_content failed\\n\",\n\t\t       __func__);\n\t\treturn err;\n\t}\n\n\terr = fman_port_init(port);\n\tif (err)\n\t\tpr_err(\"%s: fm_port_init failed\\n\", __func__);\n\n\treturn err;\n}\n\nstatic int dpaa_eth_init_ports(struct mac_device *mac_dev,\n\t\t\t       struct dpaa_bp *bp,\n\t\t\t       struct fm_port_fqs *port_fqs,\n\t\t\t       struct dpaa_buffer_layout *buf_layout,\n\t\t\t       struct device *dev)\n{\n\tstruct fman_port *rxport = mac_dev->port[RX];\n\tstruct fman_port *txport = mac_dev->port[TX];\n\tint err;\n\n\terr = dpaa_eth_init_tx_port(txport, port_fqs->tx_errq,\n\t\t\t\t    port_fqs->tx_defq, &buf_layout[TX]);\n\tif (err)\n\t\treturn err;\n\n\terr = dpaa_eth_init_rx_port(rxport, bp, port_fqs->rx_errq,\n\t\t\t\t    port_fqs->rx_defq, port_fqs->rx_pcdq,\n\t\t\t\t    &buf_layout[RX]);\n\n\treturn err;\n}\n\nstatic int dpaa_bman_release(const struct dpaa_bp *dpaa_bp,\n\t\t\t     struct bm_buffer *bmb, int cnt)\n{\n\tint err;\n\n\terr = bman_release(dpaa_bp->pool, bmb, cnt);\n\t \n\tif (WARN_ON(err) && dpaa_bp->free_buf_cb)\n\t\twhile (cnt-- > 0)\n\t\t\tdpaa_bp->free_buf_cb(dpaa_bp, &bmb[cnt]);\n\n\treturn cnt;\n}\n\nstatic void dpaa_release_sgt_members(struct qm_sg_entry *sgt)\n{\n\tstruct bm_buffer bmb[DPAA_BUFF_RELEASE_MAX];\n\tstruct dpaa_bp *dpaa_bp;\n\tint i = 0, j;\n\n\tmemset(bmb, 0, sizeof(bmb));\n\n\tdo {\n\t\tdpaa_bp = dpaa_bpid2pool(sgt[i].bpid);\n\t\tif (!dpaa_bp)\n\t\t\treturn;\n\n\t\tj = 0;\n\t\tdo {\n\t\t\tWARN_ON(qm_sg_entry_is_ext(&sgt[i]));\n\n\t\t\tbm_buffer_set64(&bmb[j], qm_sg_entry_get64(&sgt[i]));\n\n\t\t\tj++; i++;\n\t\t} while (j < ARRAY_SIZE(bmb) &&\n\t\t\t\t!qm_sg_entry_is_final(&sgt[i - 1]) &&\n\t\t\t\tsgt[i - 1].bpid == sgt[i].bpid);\n\n\t\tdpaa_bman_release(dpaa_bp, bmb, j);\n\t} while (!qm_sg_entry_is_final(&sgt[i - 1]));\n}\n\nstatic void dpaa_fd_release(const struct net_device *net_dev,\n\t\t\t    const struct qm_fd *fd)\n{\n\tstruct qm_sg_entry *sgt;\n\tstruct dpaa_bp *dpaa_bp;\n\tstruct bm_buffer bmb;\n\tdma_addr_t addr;\n\tvoid *vaddr;\n\n\tbmb.data = 0;\n\tbm_buffer_set64(&bmb, qm_fd_addr(fd));\n\n\tdpaa_bp = dpaa_bpid2pool(fd->bpid);\n\tif (!dpaa_bp)\n\t\treturn;\n\n\tif (qm_fd_get_format(fd) == qm_fd_sg) {\n\t\tvaddr = phys_to_virt(qm_fd_addr(fd));\n\t\tsgt = vaddr + qm_fd_get_offset(fd);\n\n\t\tdma_unmap_page(dpaa_bp->priv->rx_dma_dev, qm_fd_addr(fd),\n\t\t\t       DPAA_BP_RAW_SIZE, DMA_FROM_DEVICE);\n\n\t\tdpaa_release_sgt_members(sgt);\n\n\t\taddr = dma_map_page(dpaa_bp->priv->rx_dma_dev,\n\t\t\t\t    virt_to_page(vaddr), 0, DPAA_BP_RAW_SIZE,\n\t\t\t\t    DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(dpaa_bp->priv->rx_dma_dev, addr)) {\n\t\t\tnetdev_err(net_dev, \"DMA mapping failed\\n\");\n\t\t\treturn;\n\t\t}\n\t\tbm_buffer_set64(&bmb, addr);\n\t}\n\n\tdpaa_bman_release(dpaa_bp, &bmb, 1);\n}\n\nstatic void count_ern(struct dpaa_percpu_priv *percpu_priv,\n\t\t      const union qm_mr_entry *msg)\n{\n\tswitch (msg->ern.rc & QM_MR_RC_MASK) {\n\tcase QM_MR_RC_CGR_TAILDROP:\n\t\tpercpu_priv->ern_cnt.cg_tdrop++;\n\t\tbreak;\n\tcase QM_MR_RC_WRED:\n\t\tpercpu_priv->ern_cnt.wred++;\n\t\tbreak;\n\tcase QM_MR_RC_ERROR:\n\t\tpercpu_priv->ern_cnt.err_cond++;\n\t\tbreak;\n\tcase QM_MR_RC_ORPWINDOW_EARLY:\n\t\tpercpu_priv->ern_cnt.early_window++;\n\t\tbreak;\n\tcase QM_MR_RC_ORPWINDOW_LATE:\n\t\tpercpu_priv->ern_cnt.late_window++;\n\t\tbreak;\n\tcase QM_MR_RC_FQ_TAILDROP:\n\t\tpercpu_priv->ern_cnt.fq_tdrop++;\n\t\tbreak;\n\tcase QM_MR_RC_ORPWINDOW_RETIRED:\n\t\tpercpu_priv->ern_cnt.fq_retired++;\n\t\tbreak;\n\tcase QM_MR_RC_ORP_ZERO:\n\t\tpercpu_priv->ern_cnt.orp_zero++;\n\t\tbreak;\n\t}\n}\n\n \nstatic int dpaa_enable_tx_csum(struct dpaa_priv *priv,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct qm_fd *fd,\n\t\t\t       void *parse_results)\n{\n\tstruct fman_prs_result *parse_result;\n\tu16 ethertype = ntohs(skb->protocol);\n\tstruct ipv6hdr *ipv6h = NULL;\n\tstruct iphdr *iph;\n\tint retval = 0;\n\tu8 l4_proto;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\t \n\n\t \n\tparse_result = (struct fman_prs_result *)parse_results;\n\n\t \n\tif (ethertype == ETH_P_8021Q)\n\t\tethertype = ntohs(skb_vlan_eth_hdr(skb)->h_vlan_encapsulated_proto);\n\n\t \n\tswitch (ethertype) {\n\tcase ETH_P_IP:\n\t\tparse_result->l3r = cpu_to_be16(FM_L3_PARSE_RESULT_IPV4);\n\t\tiph = ip_hdr(skb);\n\t\tWARN_ON(!iph);\n\t\tl4_proto = iph->protocol;\n\t\tbreak;\n\tcase ETH_P_IPV6:\n\t\tparse_result->l3r = cpu_to_be16(FM_L3_PARSE_RESULT_IPV6);\n\t\tipv6h = ipv6_hdr(skb);\n\t\tWARN_ON(!ipv6h);\n\t\tl4_proto = ipv6h->nexthdr;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tif (net_ratelimit())\n\t\t\tnetif_alert(priv, tx_err, priv->net_dev,\n\t\t\t\t    \"Can't compute HW csum for L3 proto 0x%x\\n\",\n\t\t\t\t    ntohs(skb->protocol));\n\t\tretval = -EIO;\n\t\tgoto return_error;\n\t}\n\n\t \n\tswitch (l4_proto) {\n\tcase IPPROTO_UDP:\n\t\tparse_result->l4r = FM_L4_PARSE_RESULT_UDP;\n\t\tbreak;\n\tcase IPPROTO_TCP:\n\t\tparse_result->l4r = FM_L4_PARSE_RESULT_TCP;\n\t\tbreak;\n\tdefault:\n\t\tif (net_ratelimit())\n\t\t\tnetif_alert(priv, tx_err, priv->net_dev,\n\t\t\t\t    \"Can't compute HW csum for L4 proto 0x%x\\n\",\n\t\t\t\t    l4_proto);\n\t\tretval = -EIO;\n\t\tgoto return_error;\n\t}\n\n\t \n\tparse_result->ip_off[0] = (u8)skb_network_offset(skb);\n\tparse_result->l4_off = (u8)skb_transport_offset(skb);\n\n\t \n\tfd->cmd |= cpu_to_be32(FM_FD_CMD_RPD | FM_FD_CMD_DTC);\n\n\t \n\nreturn_error:\n\treturn retval;\n}\n\nstatic int dpaa_bp_add_8_bufs(const struct dpaa_bp *dpaa_bp)\n{\n\tstruct net_device *net_dev = dpaa_bp->priv->net_dev;\n\tstruct bm_buffer bmb[8];\n\tdma_addr_t addr;\n\tstruct page *p;\n\tu8 i;\n\n\tfor (i = 0; i < 8; i++) {\n\t\tp = dev_alloc_pages(0);\n\t\tif (unlikely(!p)) {\n\t\t\tnetdev_err(net_dev, \"dev_alloc_pages() failed\\n\");\n\t\t\tgoto release_previous_buffs;\n\t\t}\n\n\t\taddr = dma_map_page(dpaa_bp->priv->rx_dma_dev, p, 0,\n\t\t\t\t    DPAA_BP_RAW_SIZE, DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dpaa_bp->priv->rx_dma_dev,\n\t\t\t\t\t       addr))) {\n\t\t\tnetdev_err(net_dev, \"DMA map failed\\n\");\n\t\t\tgoto release_previous_buffs;\n\t\t}\n\n\t\tbmb[i].data = 0;\n\t\tbm_buffer_set64(&bmb[i], addr);\n\t}\n\nrelease_bufs:\n\treturn dpaa_bman_release(dpaa_bp, bmb, i);\n\nrelease_previous_buffs:\n\tWARN_ONCE(1, \"dpaa_eth: failed to add buffers on Rx\\n\");\n\n\tbm_buffer_set64(&bmb[i], 0);\n\t \n\tif (likely(i))\n\t\tgoto release_bufs;\n\n\treturn 0;\n}\n\nstatic int dpaa_bp_seed(struct dpaa_bp *dpaa_bp)\n{\n\tint i;\n\n\t \n\tfor_each_possible_cpu(i) {\n\t\tint *count_ptr = per_cpu_ptr(dpaa_bp->percpu_count, i);\n\t\tint j;\n\n\t\t \n\t\tfor (j = 0; j < dpaa_bp->config_count; j += 8)\n\t\t\t*count_ptr += dpaa_bp_add_8_bufs(dpaa_bp);\n\t}\n\treturn 0;\n}\n\n \nstatic int dpaa_eth_refill_bpool(struct dpaa_bp *dpaa_bp, int *countptr)\n{\n\tint count = *countptr;\n\tint new_bufs;\n\n\tif (unlikely(count < FSL_DPAA_ETH_REFILL_THRESHOLD)) {\n\t\tdo {\n\t\t\tnew_bufs = dpaa_bp_add_8_bufs(dpaa_bp);\n\t\t\tif (unlikely(!new_bufs)) {\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcount += new_bufs;\n\t\t} while (count < FSL_DPAA_ETH_MAX_BUF_COUNT);\n\n\t\t*countptr = count;\n\t\tif (unlikely(count < FSL_DPAA_ETH_MAX_BUF_COUNT))\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa_eth_refill_bpools(struct dpaa_priv *priv)\n{\n\tstruct dpaa_bp *dpaa_bp;\n\tint *countptr;\n\n\tdpaa_bp = priv->dpaa_bp;\n\tif (!dpaa_bp)\n\t\treturn -EINVAL;\n\tcountptr = this_cpu_ptr(dpaa_bp->percpu_count);\n\n\treturn dpaa_eth_refill_bpool(dpaa_bp, countptr);\n}\n\n \nstatic struct sk_buff *dpaa_cleanup_tx_fd(const struct dpaa_priv *priv,\n\t\t\t\t\t  const struct qm_fd *fd, bool ts)\n{\n\tconst enum dma_data_direction dma_dir = DMA_TO_DEVICE;\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct skb_shared_hwtstamps shhwtstamps;\n\tdma_addr_t addr = qm_fd_addr(fd);\n\tvoid *vaddr = phys_to_virt(addr);\n\tconst struct qm_sg_entry *sgt;\n\tstruct dpaa_eth_swbp *swbp;\n\tstruct sk_buff *skb;\n\tu64 ns;\n\tint i;\n\n\tif (unlikely(qm_fd_get_format(fd) == qm_fd_sg)) {\n\t\tdma_unmap_page(priv->tx_dma_dev, addr,\n\t\t\t       qm_fd_get_offset(fd) + DPAA_SGT_SIZE,\n\t\t\t       dma_dir);\n\n\t\t \n\t\tsgt = vaddr + qm_fd_get_offset(fd);\n\n\t\t \n\t\tdma_unmap_single(priv->tx_dma_dev, qm_sg_addr(&sgt[0]),\n\t\t\t\t qm_sg_entry_get_len(&sgt[0]), dma_dir);\n\n\t\t \n\t\tfor (i = 1; (i < DPAA_SGT_MAX_ENTRIES) &&\n\t\t     !qm_sg_entry_is_final(&sgt[i - 1]); i++) {\n\t\t\tWARN_ON(qm_sg_entry_is_ext(&sgt[i]));\n\n\t\t\tdma_unmap_page(priv->tx_dma_dev, qm_sg_addr(&sgt[i]),\n\t\t\t\t       qm_sg_entry_get_len(&sgt[i]), dma_dir);\n\t\t}\n\t} else {\n\t\tdma_unmap_single(priv->tx_dma_dev, addr,\n\t\t\t\t qm_fd_get_offset(fd) + qm_fd_get_length(fd),\n\t\t\t\t dma_dir);\n\t}\n\n\tswbp = (struct dpaa_eth_swbp *)vaddr;\n\tskb = swbp->skb;\n\n\t \n\tif (!skb) {\n\t\txdp_return_frame(swbp->xdpf);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (ts && priv->tx_tstamp &&\n\t    skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {\n\t\tmemset(&shhwtstamps, 0, sizeof(shhwtstamps));\n\n\t\tif (!fman_port_get_tstamp(priv->mac_dev->port[TX], vaddr,\n\t\t\t\t\t  &ns)) {\n\t\t\tshhwtstamps.hwtstamp = ns_to_ktime(ns);\n\t\t\tskb_tstamp_tx(skb, &shhwtstamps);\n\t\t} else {\n\t\t\tdev_warn(dev, \"fman_port_get_tstamp failed!\\n\");\n\t\t}\n\t}\n\n\tif (qm_fd_get_format(fd) == qm_fd_sg)\n\t\t \n\t\tfree_pages((unsigned long)vaddr, 0);\n\n\treturn skb;\n}\n\nstatic u8 rx_csum_offload(const struct dpaa_priv *priv, const struct qm_fd *fd)\n{\n\t \n\tif ((priv->net_dev->features & NETIF_F_RXCSUM) &&\n\t    (be32_to_cpu(fd->status) & FM_FD_STAT_L4CV))\n\t\treturn CHECKSUM_UNNECESSARY;\n\n\t \n\treturn CHECKSUM_NONE;\n}\n\n#define PTR_IS_ALIGNED(x, a) (IS_ALIGNED((unsigned long)(x), (a)))\n\n \nstatic struct sk_buff *contig_fd_to_skb(const struct dpaa_priv *priv,\n\t\t\t\t\tconst struct qm_fd *fd)\n{\n\tssize_t fd_off = qm_fd_get_offset(fd);\n\tdma_addr_t addr = qm_fd_addr(fd);\n\tstruct dpaa_bp *dpaa_bp;\n\tstruct sk_buff *skb;\n\tvoid *vaddr;\n\n\tvaddr = phys_to_virt(addr);\n\tWARN_ON(!IS_ALIGNED((unsigned long)vaddr, SMP_CACHE_BYTES));\n\n\tdpaa_bp = dpaa_bpid2pool(fd->bpid);\n\tif (!dpaa_bp)\n\t\tgoto free_buffer;\n\n\tskb = build_skb(vaddr, dpaa_bp->size +\n\t\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info)));\n\tif (WARN_ONCE(!skb, \"Build skb failure on Rx\\n\"))\n\t\tgoto free_buffer;\n\tskb_reserve(skb, fd_off);\n\tskb_put(skb, qm_fd_get_length(fd));\n\n\tskb->ip_summed = rx_csum_offload(priv, fd);\n\n\treturn skb;\n\nfree_buffer:\n\tfree_pages((unsigned long)vaddr, 0);\n\treturn NULL;\n}\n\n \nstatic struct sk_buff *sg_fd_to_skb(const struct dpaa_priv *priv,\n\t\t\t\t    const struct qm_fd *fd)\n{\n\tssize_t fd_off = qm_fd_get_offset(fd);\n\tdma_addr_t addr = qm_fd_addr(fd);\n\tconst struct qm_sg_entry *sgt;\n\tstruct page *page, *head_page;\n\tstruct dpaa_bp *dpaa_bp;\n\tvoid *vaddr, *sg_vaddr;\n\tint frag_off, frag_len;\n\tstruct sk_buff *skb;\n\tdma_addr_t sg_addr;\n\tint page_offset;\n\tunsigned int sz;\n\tint *count_ptr;\n\tint i, j;\n\n\tvaddr = phys_to_virt(addr);\n\tWARN_ON(!IS_ALIGNED((unsigned long)vaddr, SMP_CACHE_BYTES));\n\n\t \n\tsgt = vaddr + fd_off;\n\tskb = NULL;\n\tfor (i = 0; i < DPAA_SGT_MAX_ENTRIES; i++) {\n\t\t \n\t\tWARN_ON(qm_sg_entry_is_ext(&sgt[i]));\n\n\t\tsg_addr = qm_sg_addr(&sgt[i]);\n\t\tsg_vaddr = phys_to_virt(sg_addr);\n\t\tWARN_ON(!PTR_IS_ALIGNED(sg_vaddr, SMP_CACHE_BYTES));\n\n\t\tdma_unmap_page(priv->rx_dma_dev, sg_addr,\n\t\t\t       DPAA_BP_RAW_SIZE, DMA_FROM_DEVICE);\n\n\t\t \n\t\tdpaa_bp = dpaa_bpid2pool(sgt[i].bpid);\n\t\tif (!dpaa_bp)\n\t\t\tgoto free_buffers;\n\n\t\tif (!skb) {\n\t\t\tsz = dpaa_bp->size +\n\t\t\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\t\t\tskb = build_skb(sg_vaddr, sz);\n\t\t\tif (WARN_ON(!skb))\n\t\t\t\tgoto free_buffers;\n\n\t\t\tskb->ip_summed = rx_csum_offload(priv, fd);\n\n\t\t\t \n\t\t\tWARN_ON(fd_off != priv->rx_headroom);\n\t\t\tskb_reserve(skb, fd_off);\n\t\t\tskb_put(skb, qm_sg_entry_get_len(&sgt[i]));\n\t\t} else {\n\t\t\t \n\t\t\tpage = virt_to_page(sg_vaddr);\n\t\t\thead_page = virt_to_head_page(sg_vaddr);\n\n\t\t\t \n\t\t\tpage_offset = ((unsigned long)sg_vaddr &\n\t\t\t\t\t(PAGE_SIZE - 1)) +\n\t\t\t\t(page_address(page) - page_address(head_page));\n\t\t\t \n\t\t\tfrag_off = qm_sg_entry_get_off(&sgt[i]) + page_offset;\n\t\t\tfrag_len = qm_sg_entry_get_len(&sgt[i]);\n\t\t\t \n\t\t\tskb_add_rx_frag(skb, i - 1, head_page, frag_off,\n\t\t\t\t\tfrag_len, dpaa_bp->size);\n\t\t}\n\n\t\t \n\t\tcount_ptr = this_cpu_ptr(dpaa_bp->percpu_count);\n\t\t(*count_ptr)--;\n\n\t\tif (qm_sg_entry_is_final(&sgt[i]))\n\t\t\tbreak;\n\t}\n\tWARN_ONCE(i == DPAA_SGT_MAX_ENTRIES, \"No final bit on SGT\\n\");\n\n\t \n\tfree_pages((unsigned long)vaddr, 0);\n\n\treturn skb;\n\nfree_buffers:\n\t \n\tfor (j = 0; j < DPAA_SGT_MAX_ENTRIES ; j++) {\n\t\tsg_addr = qm_sg_addr(&sgt[j]);\n\t\tsg_vaddr = phys_to_virt(sg_addr);\n\t\t \n\t\tif (j > i)\n\t\t\tdma_unmap_page(priv->rx_dma_dev, qm_sg_addr(&sgt[j]),\n\t\t\t\t       DPAA_BP_RAW_SIZE, DMA_FROM_DEVICE);\n\t\tfree_pages((unsigned long)sg_vaddr, 0);\n\t\t \n\t\tif (j >= i) {\n\t\t\tdpaa_bp = dpaa_bpid2pool(sgt[j].bpid);\n\t\t\tif (dpaa_bp) {\n\t\t\t\tcount_ptr = this_cpu_ptr(dpaa_bp->percpu_count);\n\t\t\t\t(*count_ptr)--;\n\t\t\t}\n\t\t}\n\n\t\tif (qm_sg_entry_is_final(&sgt[j]))\n\t\t\tbreak;\n\t}\n\t \n\tfree_pages((unsigned long)vaddr, 0);\n\n\treturn NULL;\n}\n\nstatic int skb_to_contig_fd(struct dpaa_priv *priv,\n\t\t\t    struct sk_buff *skb, struct qm_fd *fd,\n\t\t\t    int *offset)\n{\n\tstruct net_device *net_dev = priv->net_dev;\n\tenum dma_data_direction dma_dir;\n\tstruct dpaa_eth_swbp *swbp;\n\tunsigned char *buff_start;\n\tdma_addr_t addr;\n\tint err;\n\n\t \n\tfd->bpid = FSL_DPAA_BPID_INV;\n\tbuff_start = skb->data - priv->tx_headroom;\n\tdma_dir = DMA_TO_DEVICE;\n\n\tswbp = (struct dpaa_eth_swbp *)buff_start;\n\tswbp->skb = skb;\n\n\t \n\terr = dpaa_enable_tx_csum(priv, skb, fd,\n\t\t\t\t  buff_start + DPAA_TX_PRIV_DATA_SIZE);\n\tif (unlikely(err < 0)) {\n\t\tif (net_ratelimit())\n\t\t\tnetif_err(priv, tx_err, net_dev, \"HW csum error: %d\\n\",\n\t\t\t\t  err);\n\t\treturn err;\n\t}\n\n\t \n\tqm_fd_set_contig(fd, priv->tx_headroom, skb->len);\n\tfd->cmd |= cpu_to_be32(FM_FD_CMD_FCO);\n\n\t \n\taddr = dma_map_single(priv->tx_dma_dev, buff_start,\n\t\t\t      priv->tx_headroom + skb->len, dma_dir);\n\tif (unlikely(dma_mapping_error(priv->tx_dma_dev, addr))) {\n\t\tif (net_ratelimit())\n\t\t\tnetif_err(priv, tx_err, net_dev, \"dma_map_single() failed\\n\");\n\t\treturn -EINVAL;\n\t}\n\tqm_fd_addr_set64(fd, addr);\n\n\treturn 0;\n}\n\nstatic int skb_to_sg_fd(struct dpaa_priv *priv,\n\t\t\tstruct sk_buff *skb, struct qm_fd *fd)\n{\n\tconst enum dma_data_direction dma_dir = DMA_TO_DEVICE;\n\tconst int nr_frags = skb_shinfo(skb)->nr_frags;\n\tstruct net_device *net_dev = priv->net_dev;\n\tstruct dpaa_eth_swbp *swbp;\n\tstruct qm_sg_entry *sgt;\n\tvoid *buff_start;\n\tskb_frag_t *frag;\n\tdma_addr_t addr;\n\tsize_t frag_len;\n\tstruct page *p;\n\tint i, j, err;\n\n\t \n\tp = dev_alloc_pages(0);\n\tif (unlikely(!p)) {\n\t\tnetdev_err(net_dev, \"dev_alloc_pages() failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tbuff_start = page_address(p);\n\n\t \n\terr = dpaa_enable_tx_csum(priv, skb, fd,\n\t\t\t\t  buff_start + DPAA_TX_PRIV_DATA_SIZE);\n\tif (unlikely(err < 0)) {\n\t\tif (net_ratelimit())\n\t\t\tnetif_err(priv, tx_err, net_dev, \"HW csum error: %d\\n\",\n\t\t\t\t  err);\n\t\tgoto csum_failed;\n\t}\n\n\t \n\tsgt = (struct qm_sg_entry *)(buff_start + priv->tx_headroom);\n\tfrag_len = skb_headlen(skb);\n\tqm_sg_entry_set_len(&sgt[0], frag_len);\n\tsgt[0].bpid = FSL_DPAA_BPID_INV;\n\tsgt[0].offset = 0;\n\taddr = dma_map_single(priv->tx_dma_dev, skb->data,\n\t\t\t      skb_headlen(skb), dma_dir);\n\tif (unlikely(dma_mapping_error(priv->tx_dma_dev, addr))) {\n\t\tnetdev_err(priv->net_dev, \"DMA mapping failed\\n\");\n\t\terr = -EINVAL;\n\t\tgoto sg0_map_failed;\n\t}\n\tqm_sg_entry_set64(&sgt[0], addr);\n\n\t \n\tfor (i = 0; i < nr_frags; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\tfrag_len = skb_frag_size(frag);\n\t\tWARN_ON(!skb_frag_page(frag));\n\t\taddr = skb_frag_dma_map(priv->tx_dma_dev, frag, 0,\n\t\t\t\t\tfrag_len, dma_dir);\n\t\tif (unlikely(dma_mapping_error(priv->tx_dma_dev, addr))) {\n\t\t\tnetdev_err(priv->net_dev, \"DMA mapping failed\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto sg_map_failed;\n\t\t}\n\n\t\tqm_sg_entry_set_len(&sgt[i + 1], frag_len);\n\t\tsgt[i + 1].bpid = FSL_DPAA_BPID_INV;\n\t\tsgt[i + 1].offset = 0;\n\n\t\t \n\t\tqm_sg_entry_set64(&sgt[i + 1], addr);\n\t}\n\n\t \n\tqm_sg_entry_set_f(&sgt[nr_frags], frag_len);\n\n\t \n\tqm_fd_set_sg(fd, priv->tx_headroom, skb->len);\n\n\t \n\tswbp = (struct dpaa_eth_swbp *)buff_start;\n\tswbp->skb = skb;\n\n\taddr = dma_map_page(priv->tx_dma_dev, p, 0,\n\t\t\t    priv->tx_headroom + DPAA_SGT_SIZE, dma_dir);\n\tif (unlikely(dma_mapping_error(priv->tx_dma_dev, addr))) {\n\t\tnetdev_err(priv->net_dev, \"DMA mapping failed\\n\");\n\t\terr = -EINVAL;\n\t\tgoto sgt_map_failed;\n\t}\n\n\tfd->bpid = FSL_DPAA_BPID_INV;\n\tfd->cmd |= cpu_to_be32(FM_FD_CMD_FCO);\n\tqm_fd_addr_set64(fd, addr);\n\n\treturn 0;\n\nsgt_map_failed:\nsg_map_failed:\n\tfor (j = 0; j < i; j++)\n\t\tdma_unmap_page(priv->tx_dma_dev, qm_sg_addr(&sgt[j]),\n\t\t\t       qm_sg_entry_get_len(&sgt[j]), dma_dir);\nsg0_map_failed:\ncsum_failed:\n\tfree_pages((unsigned long)buff_start, 0);\n\n\treturn err;\n}\n\nstatic inline int dpaa_xmit(struct dpaa_priv *priv,\n\t\t\t    struct rtnl_link_stats64 *percpu_stats,\n\t\t\t    int queue,\n\t\t\t    struct qm_fd *fd)\n{\n\tstruct qman_fq *egress_fq;\n\tint err, i;\n\n\tegress_fq = priv->egress_fqs[queue];\n\tif (fd->bpid == FSL_DPAA_BPID_INV)\n\t\tfd->cmd |= cpu_to_be32(qman_fq_fqid(priv->conf_fqs[queue]));\n\n\t \n\ttrace_dpaa_tx_fd(priv->net_dev, egress_fq, fd);\n\n\tfor (i = 0; i < DPAA_ENQUEUE_RETRIES; i++) {\n\t\terr = qman_enqueue(egress_fq, fd);\n\t\tif (err != -EBUSY)\n\t\t\tbreak;\n\t}\n\n\tif (unlikely(err < 0)) {\n\t\tpercpu_stats->tx_fifo_errors++;\n\t\treturn err;\n\t}\n\n\tpercpu_stats->tx_packets++;\n\tpercpu_stats->tx_bytes += qm_fd_get_length(fd);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_DPAA_ERRATUM_A050385\nstatic int dpaa_a050385_wa_skb(struct net_device *net_dev, struct sk_buff **s)\n{\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\tstruct sk_buff *new_skb, *skb = *s;\n\tunsigned char *start, i;\n\n\t \n\tif (!PTR_IS_ALIGNED(skb->data, DPAA_A050385_ALIGN))\n\t\tgoto workaround;\n\n\t \n\tif (!skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\t \n\tif (!IS_ALIGNED(skb_headlen(skb), DPAA_A050385_ALIGN))\n\t\tgoto workaround;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t \n\t\tif (!IS_ALIGNED(skb_frag_off(frag), DPAA_A050385_ALIGN))\n\t\t\tgoto workaround;\n\n\t\t \n\t\tif (!IS_ALIGNED(skb_frag_size(frag), DPAA_A050385_ALIGN) &&\n\t\t    (i < skb_shinfo(skb)->nr_frags - 1))\n\t\t\tgoto workaround;\n\t}\n\n\treturn 0;\n\nworkaround:\n\t \n\tnew_skb = netdev_alloc_skb(net_dev, skb->len + DPAA_A050385_ALIGN - 1 +\n\t\t\t\t\t\tpriv->tx_headroom);\n\tif (!new_skb)\n\t\treturn -ENOMEM;\n\n\t \n\tskb_reserve(new_skb, priv->tx_headroom - NET_SKB_PAD);\n\n\t \n\tstart = PTR_ALIGN(new_skb->data, DPAA_A050385_ALIGN);\n\tif (start - new_skb->data)\n\t\tskb_reserve(new_skb, start - new_skb->data);\n\n\tskb_put(new_skb, skb->len);\n\tskb_copy_bits(skb, 0, new_skb->data, skb->len);\n\tskb_copy_header(new_skb, skb);\n\tnew_skb->dev = skb->dev;\n\n\t \n\tif (priv->tx_tstamp) {\n\t\tskb_shinfo(new_skb)->tx_flags = skb_shinfo(skb)->tx_flags;\n\t\tskb_shinfo(new_skb)->hwtstamps = skb_shinfo(skb)->hwtstamps;\n\t\tskb_shinfo(new_skb)->tskey = skb_shinfo(skb)->tskey;\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(new_skb, skb->sk);\n\t}\n\n\t \n\tskb_set_network_header(new_skb, skb_network_offset(skb));\n\tskb_set_transport_header(new_skb, skb_transport_offset(skb));\n\n\tdev_kfree_skb(skb);\n\t*s = new_skb;\n\n\treturn 0;\n}\n\nstatic int dpaa_a050385_wa_xdpf(struct dpaa_priv *priv,\n\t\t\t\tstruct xdp_frame **init_xdpf)\n{\n\tstruct xdp_frame *new_xdpf, *xdpf = *init_xdpf;\n\tvoid *new_buff, *aligned_data;\n\tstruct page *p;\n\tu32 data_shift;\n\tint headroom;\n\n\t \n\tif (PTR_IS_ALIGNED(xdpf->data, DPAA_FD_DATA_ALIGNMENT) &&\n\t    xdpf->headroom >= priv->tx_headroom) {\n\t\txdpf->headroom = priv->tx_headroom;\n\t\treturn 0;\n\t}\n\n\t \n\taligned_data = PTR_ALIGN_DOWN(xdpf->data, DPAA_FD_DATA_ALIGNMENT);\n\tdata_shift = xdpf->data - aligned_data;\n\n\t \n\tif (xdpf->headroom  >= data_shift + priv->tx_headroom) {\n\t\tmemmove(aligned_data, xdpf->data, xdpf->len);\n\t\txdpf->data = aligned_data;\n\t\txdpf->headroom = priv->tx_headroom;\n\t\treturn 0;\n\t}\n\n\t \n\theadroom = ALIGN(sizeof(*new_xdpf) + priv->tx_headroom,\n\t\t\t DPAA_FD_DATA_ALIGNMENT);\n\n\t \n\tif (headroom + xdpf->len > DPAA_BP_RAW_SIZE -\n\t\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\t\treturn -ENOMEM;\n\n\tp = dev_alloc_pages(0);\n\tif (unlikely(!p))\n\t\treturn -ENOMEM;\n\n\t \n\tnew_buff = page_address(p);\n\tmemcpy(new_buff + headroom, xdpf->data, xdpf->len);\n\n\t \n\tnew_xdpf = new_buff;\n\tnew_xdpf->data = new_buff + headroom;\n\tnew_xdpf->len = xdpf->len;\n\tnew_xdpf->headroom = priv->tx_headroom;\n\tnew_xdpf->frame_sz = DPAA_BP_RAW_SIZE;\n\tnew_xdpf->mem.type = MEM_TYPE_PAGE_ORDER0;\n\n\t \n\txdp_return_frame_rx_napi(xdpf);\n\n\t*init_xdpf = new_xdpf;\n\treturn 0;\n}\n#endif\n\nstatic netdev_tx_t\ndpaa_start_xmit(struct sk_buff *skb, struct net_device *net_dev)\n{\n\tconst int queue_mapping = skb_get_queue_mapping(skb);\n\tbool nonlinear = skb_is_nonlinear(skb);\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tstruct netdev_queue *txq;\n\tstruct dpaa_priv *priv;\n\tstruct qm_fd fd;\n\tint offset = 0;\n\tint err = 0;\n\n\tpriv = netdev_priv(net_dev);\n\tpercpu_priv = this_cpu_ptr(priv->percpu_priv);\n\tpercpu_stats = &percpu_priv->stats;\n\n\tqm_fd_clear_fd(&fd);\n\n\tif (!nonlinear) {\n\t\t \n\t\tif (skb_cow_head(skb, priv->tx_headroom))\n\t\t\tgoto enomem;\n\n\t\tWARN_ON(skb_is_nonlinear(skb));\n\t}\n\n\t \n\tif (unlikely(nonlinear &&\n\t\t     (skb_shinfo(skb)->nr_frags >= DPAA_SGT_MAX_ENTRIES))) {\n\t\t \n\t\tif (__skb_linearize(skb))\n\t\t\tgoto enomem;\n\n\t\tnonlinear = skb_is_nonlinear(skb);\n\t}\n\n#ifdef CONFIG_DPAA_ERRATUM_A050385\n\tif (unlikely(fman_has_errata_a050385())) {\n\t\tif (dpaa_a050385_wa_skb(net_dev, &skb))\n\t\t\tgoto enomem;\n\t\tnonlinear = skb_is_nonlinear(skb);\n\t}\n#endif\n\n\tif (nonlinear) {\n\t\t \n\t\terr = skb_to_sg_fd(priv, skb, &fd);\n\t\tpercpu_priv->tx_frag_skbuffs++;\n\t} else {\n\t\t \n\t\terr = skb_to_contig_fd(priv, skb, &fd, &offset);\n\t}\n\tif (unlikely(err < 0))\n\t\tgoto skb_to_fd_failed;\n\n\ttxq = netdev_get_tx_queue(net_dev, queue_mapping);\n\n\t \n\ttxq_trans_cond_update(txq);\n\n\tif (priv->tx_tstamp && skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {\n\t\tfd.cmd |= cpu_to_be32(FM_FD_CMD_UPD);\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t}\n\n\tif (likely(dpaa_xmit(priv, percpu_stats, queue_mapping, &fd) == 0))\n\t\treturn NETDEV_TX_OK;\n\n\tdpaa_cleanup_tx_fd(priv, &fd, false);\nskb_to_fd_failed:\nenomem:\n\tpercpu_stats->tx_errors++;\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void dpaa_rx_error(struct net_device *net_dev,\n\t\t\t  const struct dpaa_priv *priv,\n\t\t\t  struct dpaa_percpu_priv *percpu_priv,\n\t\t\t  const struct qm_fd *fd,\n\t\t\t  u32 fqid)\n{\n\tif (net_ratelimit())\n\t\tnetif_err(priv, hw, net_dev, \"Err FD status = 0x%08x\\n\",\n\t\t\t  be32_to_cpu(fd->status) & FM_FD_STAT_RX_ERRORS);\n\n\tpercpu_priv->stats.rx_errors++;\n\n\tif (be32_to_cpu(fd->status) & FM_FD_ERR_DMA)\n\t\tpercpu_priv->rx_errors.dme++;\n\tif (be32_to_cpu(fd->status) & FM_FD_ERR_PHYSICAL)\n\t\tpercpu_priv->rx_errors.fpe++;\n\tif (be32_to_cpu(fd->status) & FM_FD_ERR_SIZE)\n\t\tpercpu_priv->rx_errors.fse++;\n\tif (be32_to_cpu(fd->status) & FM_FD_ERR_PRS_HDR_ERR)\n\t\tpercpu_priv->rx_errors.phe++;\n\n\tdpaa_fd_release(net_dev, fd);\n}\n\nstatic void dpaa_tx_error(struct net_device *net_dev,\n\t\t\t  const struct dpaa_priv *priv,\n\t\t\t  struct dpaa_percpu_priv *percpu_priv,\n\t\t\t  const struct qm_fd *fd,\n\t\t\t  u32 fqid)\n{\n\tstruct sk_buff *skb;\n\n\tif (net_ratelimit())\n\t\tnetif_warn(priv, hw, net_dev, \"FD status = 0x%08x\\n\",\n\t\t\t   be32_to_cpu(fd->status) & FM_FD_STAT_TX_ERRORS);\n\n\tpercpu_priv->stats.tx_errors++;\n\n\tskb = dpaa_cleanup_tx_fd(priv, fd, false);\n\tdev_kfree_skb(skb);\n}\n\nstatic int dpaa_eth_poll(struct napi_struct *napi, int budget)\n{\n\tstruct dpaa_napi_portal *np =\n\t\t\tcontainer_of(napi, struct dpaa_napi_portal, napi);\n\tint cleaned;\n\n\tnp->xdp_act = 0;\n\n\tcleaned = qman_p_poll_dqrr(np->p, budget);\n\n\tif (np->xdp_act & XDP_REDIRECT)\n\t\txdp_do_flush();\n\n\tif (cleaned < budget) {\n\t\tnapi_complete_done(napi, cleaned);\n\t\tqman_p_irqsource_add(np->p, QM_PIRQ_DQRI);\n\t} else if (np->down) {\n\t\tqman_p_irqsource_add(np->p, QM_PIRQ_DQRI);\n\t}\n\n\treturn cleaned;\n}\n\nstatic void dpaa_tx_conf(struct net_device *net_dev,\n\t\t\t const struct dpaa_priv *priv,\n\t\t\t struct dpaa_percpu_priv *percpu_priv,\n\t\t\t const struct qm_fd *fd,\n\t\t\t u32 fqid)\n{\n\tstruct sk_buff\t*skb;\n\n\tif (unlikely(be32_to_cpu(fd->status) & FM_FD_STAT_TX_ERRORS)) {\n\t\tif (net_ratelimit())\n\t\t\tnetif_warn(priv, hw, net_dev, \"FD status = 0x%08x\\n\",\n\t\t\t\t   be32_to_cpu(fd->status) &\n\t\t\t\t   FM_FD_STAT_TX_ERRORS);\n\n\t\tpercpu_priv->stats.tx_errors++;\n\t}\n\n\tpercpu_priv->tx_confirm++;\n\n\tskb = dpaa_cleanup_tx_fd(priv, fd, true);\n\n\tconsume_skb(skb);\n}\n\nstatic inline int dpaa_eth_napi_schedule(struct dpaa_percpu_priv *percpu_priv,\n\t\t\t\t\t struct qman_portal *portal, bool sched_napi)\n{\n\tif (sched_napi) {\n\t\t \n\t\tqman_p_irqsource_remove(portal, QM_PIRQ_DQRI);\n\n\t\tpercpu_priv->np.p = portal;\n\t\tnapi_schedule(&percpu_priv->np.napi);\n\t\tpercpu_priv->in_interrupt++;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic enum qman_cb_dqrr_result rx_error_dqrr(struct qman_portal *portal,\n\t\t\t\t\t      struct qman_fq *fq,\n\t\t\t\t\t      const struct qm_dqrr_entry *dq,\n\t\t\t\t\t      bool sched_napi)\n{\n\tstruct dpaa_fq *dpaa_fq = container_of(fq, struct dpaa_fq, fq_base);\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tstruct net_device *net_dev;\n\tstruct dpaa_bp *dpaa_bp;\n\tstruct dpaa_priv *priv;\n\n\tnet_dev = dpaa_fq->net_dev;\n\tpriv = netdev_priv(net_dev);\n\tdpaa_bp = dpaa_bpid2pool(dq->fd.bpid);\n\tif (!dpaa_bp)\n\t\treturn qman_cb_dqrr_consume;\n\n\tpercpu_priv = this_cpu_ptr(priv->percpu_priv);\n\n\tif (dpaa_eth_napi_schedule(percpu_priv, portal, sched_napi))\n\t\treturn qman_cb_dqrr_stop;\n\n\tdpaa_eth_refill_bpools(priv);\n\tdpaa_rx_error(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);\n\n\treturn qman_cb_dqrr_consume;\n}\n\nstatic int dpaa_xdp_xmit_frame(struct net_device *net_dev,\n\t\t\t       struct xdp_frame *xdpf)\n{\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tstruct dpaa_eth_swbp *swbp;\n\tstruct netdev_queue *txq;\n\tvoid *buff_start;\n\tstruct qm_fd fd;\n\tdma_addr_t addr;\n\tint err;\n\n\tpercpu_priv = this_cpu_ptr(priv->percpu_priv);\n\tpercpu_stats = &percpu_priv->stats;\n\n#ifdef CONFIG_DPAA_ERRATUM_A050385\n\tif (unlikely(fman_has_errata_a050385())) {\n\t\tif (dpaa_a050385_wa_xdpf(priv, &xdpf)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_error;\n\t\t}\n\t}\n#endif\n\n\tif (xdpf->headroom < DPAA_TX_PRIV_DATA_SIZE) {\n\t\terr = -EINVAL;\n\t\tgoto out_error;\n\t}\n\n\tbuff_start = xdpf->data - xdpf->headroom;\n\n\t \n\tswbp = (struct dpaa_eth_swbp *)buff_start;\n\tswbp->skb = NULL;\n\tswbp->xdpf = xdpf;\n\n\tqm_fd_clear_fd(&fd);\n\tfd.bpid = FSL_DPAA_BPID_INV;\n\tfd.cmd |= cpu_to_be32(FM_FD_CMD_FCO);\n\tqm_fd_set_contig(&fd, xdpf->headroom, xdpf->len);\n\n\taddr = dma_map_single(priv->tx_dma_dev, buff_start,\n\t\t\t      xdpf->headroom + xdpf->len,\n\t\t\t      DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(priv->tx_dma_dev, addr))) {\n\t\terr = -EINVAL;\n\t\tgoto out_error;\n\t}\n\n\tqm_fd_addr_set64(&fd, addr);\n\n\t \n\ttxq = netdev_get_tx_queue(net_dev, smp_processor_id());\n\ttxq_trans_cond_update(txq);\n\n\terr = dpaa_xmit(priv, percpu_stats, smp_processor_id(), &fd);\n\tif (err) {\n\t\tdma_unmap_single(priv->tx_dma_dev, addr,\n\t\t\t\t qm_fd_get_offset(&fd) + qm_fd_get_length(&fd),\n\t\t\t\t DMA_TO_DEVICE);\n\t\tgoto out_error;\n\t}\n\n\treturn 0;\n\nout_error:\n\tpercpu_stats->tx_errors++;\n\treturn err;\n}\n\nstatic u32 dpaa_run_xdp(struct dpaa_priv *priv, struct qm_fd *fd, void *vaddr,\n\t\t\tstruct dpaa_fq *dpaa_fq, unsigned int *xdp_meta_len)\n{\n\tssize_t fd_off = qm_fd_get_offset(fd);\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_frame *xdpf;\n\tstruct xdp_buff xdp;\n\tu32 xdp_act;\n\tint err;\n\n\txdp_prog = READ_ONCE(priv->xdp_prog);\n\tif (!xdp_prog)\n\t\treturn XDP_PASS;\n\n\txdp_init_buff(&xdp, DPAA_BP_RAW_SIZE - DPAA_TX_PRIV_DATA_SIZE,\n\t\t      &dpaa_fq->xdp_rxq);\n\txdp_prepare_buff(&xdp, vaddr + fd_off - XDP_PACKET_HEADROOM,\n\t\t\t XDP_PACKET_HEADROOM, qm_fd_get_length(fd), true);\n\n\t \n#ifdef CONFIG_DPAA_ERRATUM_A050385\n\tif (unlikely(fman_has_errata_a050385())) {\n\t\txdp_set_data_meta_invalid(&xdp);\n\t\txdp.data_hard_start = vaddr;\n\t\txdp.frame_sz = DPAA_BP_RAW_SIZE;\n\t}\n#endif\n\n\txdp_act = bpf_prog_run_xdp(xdp_prog, &xdp);\n\n\t \n\tqm_fd_set_contig(fd, xdp.data - vaddr, xdp.data_end - xdp.data);\n\n\tswitch (xdp_act) {\n\tcase XDP_PASS:\n#ifdef CONFIG_DPAA_ERRATUM_A050385\n\t\t*xdp_meta_len = xdp_data_meta_unsupported(&xdp) ? 0 :\n\t\t\t\txdp.data - xdp.data_meta;\n#else\n\t\t*xdp_meta_len = xdp.data - xdp.data_meta;\n#endif\n\t\tbreak;\n\tcase XDP_TX:\n\t\t \n\t\txdp.data_hard_start = vaddr;\n\t\txdp.frame_sz = DPAA_BP_RAW_SIZE;\n\t\txdpf = xdp_convert_buff_to_frame(&xdp);\n\t\tif (unlikely(!xdpf)) {\n\t\t\tfree_pages((unsigned long)vaddr, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (dpaa_xdp_xmit_frame(priv->net_dev, xdpf))\n\t\t\txdp_return_frame_rx_napi(xdpf);\n\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\t \n\t\txdp.data_hard_start = vaddr;\n\t\txdp.frame_sz = DPAA_BP_RAW_SIZE;\n\n\t\terr = xdp_do_redirect(priv->net_dev, &xdp, xdp_prog);\n\t\tif (err) {\n\t\t\ttrace_xdp_exception(priv->net_dev, xdp_prog, xdp_act);\n\t\t\tfree_pages((unsigned long)vaddr, 0);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(priv->net_dev, xdp_prog, xdp_act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(priv->net_dev, xdp_prog, xdp_act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\t \n\t\tfree_pages((unsigned long)vaddr, 0);\n\t\tbreak;\n\t}\n\n\treturn xdp_act;\n}\n\nstatic enum qman_cb_dqrr_result rx_default_dqrr(struct qman_portal *portal,\n\t\t\t\t\t\tstruct qman_fq *fq,\n\t\t\t\t\t\tconst struct qm_dqrr_entry *dq,\n\t\t\t\t\t\tbool sched_napi)\n{\n\tbool ts_valid = false, hash_valid = false;\n\tstruct skb_shared_hwtstamps *shhwtstamps;\n\tunsigned int skb_len, xdp_meta_len = 0;\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tconst struct qm_fd *fd = &dq->fd;\n\tdma_addr_t addr = qm_fd_addr(fd);\n\tstruct dpaa_napi_portal *np;\n\tenum qm_fd_format fd_format;\n\tstruct net_device *net_dev;\n\tu32 fd_status, hash_offset;\n\tstruct qm_sg_entry *sgt;\n\tstruct dpaa_bp *dpaa_bp;\n\tstruct dpaa_fq *dpaa_fq;\n\tstruct dpaa_priv *priv;\n\tstruct sk_buff *skb;\n\tint *count_ptr;\n\tu32 xdp_act;\n\tvoid *vaddr;\n\tu32 hash;\n\tu64 ns;\n\n\tdpaa_fq = container_of(fq, struct dpaa_fq, fq_base);\n\tfd_status = be32_to_cpu(fd->status);\n\tfd_format = qm_fd_get_format(fd);\n\tnet_dev = dpaa_fq->net_dev;\n\tpriv = netdev_priv(net_dev);\n\tdpaa_bp = dpaa_bpid2pool(dq->fd.bpid);\n\tif (!dpaa_bp)\n\t\treturn qman_cb_dqrr_consume;\n\n\t \n\ttrace_dpaa_rx_fd(net_dev, fq, &dq->fd);\n\n\tpercpu_priv = this_cpu_ptr(priv->percpu_priv);\n\tpercpu_stats = &percpu_priv->stats;\n\tnp = &percpu_priv->np;\n\n\tif (unlikely(dpaa_eth_napi_schedule(percpu_priv, portal, sched_napi)))\n\t\treturn qman_cb_dqrr_stop;\n\n\t \n\tif (unlikely(dpaa_eth_refill_bpools(priv))) {\n\t\t \n\t\tdpaa_fd_release(net_dev, &dq->fd);\n\t\treturn qman_cb_dqrr_consume;\n\t}\n\n\tif (unlikely(fd_status & FM_FD_STAT_RX_ERRORS) != 0) {\n\t\tif (net_ratelimit())\n\t\t\tnetif_warn(priv, hw, net_dev, \"FD status = 0x%08x\\n\",\n\t\t\t\t   fd_status & FM_FD_STAT_RX_ERRORS);\n\n\t\tpercpu_stats->rx_errors++;\n\t\tdpaa_fd_release(net_dev, fd);\n\t\treturn qman_cb_dqrr_consume;\n\t}\n\n\tdma_unmap_page(dpaa_bp->priv->rx_dma_dev, addr, DPAA_BP_RAW_SIZE,\n\t\t       DMA_FROM_DEVICE);\n\n\t \n\tvaddr = phys_to_virt(addr);\n\tprefetch(vaddr + qm_fd_get_offset(fd));\n\n\t \n\tWARN_ON((fd_format != qm_fd_contig) && (fd_format != qm_fd_sg));\n\n\t \n\tcount_ptr = this_cpu_ptr(dpaa_bp->percpu_count);\n\t(*count_ptr)--;\n\n\t \n\tif (priv->rx_tstamp) {\n\t\tif (!fman_port_get_tstamp(priv->mac_dev->port[RX], vaddr, &ns))\n\t\t\tts_valid = true;\n\t\telse\n\t\t\tWARN_ONCE(1, \"fman_port_get_tstamp failed!\\n\");\n\t}\n\n\t \n\tif (net_dev->features & NETIF_F_RXHASH && priv->keygen_in_use &&\n\t    !fman_port_get_hash_result_offset(priv->mac_dev->port[RX],\n\t\t\t\t\t      &hash_offset)) {\n\t\thash = be32_to_cpu(*(u32 *)(vaddr + hash_offset));\n\t\thash_valid = true;\n\t}\n\n\tif (likely(fd_format == qm_fd_contig)) {\n\t\txdp_act = dpaa_run_xdp(priv, (struct qm_fd *)fd, vaddr,\n\t\t\t\t       dpaa_fq, &xdp_meta_len);\n\t\tnp->xdp_act |= xdp_act;\n\t\tif (xdp_act != XDP_PASS) {\n\t\t\tpercpu_stats->rx_packets++;\n\t\t\tpercpu_stats->rx_bytes += qm_fd_get_length(fd);\n\t\t\treturn qman_cb_dqrr_consume;\n\t\t}\n\t\tskb = contig_fd_to_skb(priv, fd);\n\t} else {\n\t\t \n\t\tif (READ_ONCE(priv->xdp_prog)) {\n\t\t\tWARN_ONCE(1, \"S/G frames not supported under XDP\\n\");\n\t\t\tsgt = vaddr + qm_fd_get_offset(fd);\n\t\t\tdpaa_release_sgt_members(sgt);\n\t\t\tfree_pages((unsigned long)vaddr, 0);\n\t\t\treturn qman_cb_dqrr_consume;\n\t\t}\n\t\tskb = sg_fd_to_skb(priv, fd);\n\t}\n\tif (!skb)\n\t\treturn qman_cb_dqrr_consume;\n\n\tif (xdp_meta_len)\n\t\tskb_metadata_set(skb, xdp_meta_len);\n\n\t \n\tif (ts_valid) {\n\t\tshhwtstamps = skb_hwtstamps(skb);\n\t\tmemset(shhwtstamps, 0, sizeof(*shhwtstamps));\n\t\tshhwtstamps->hwtstamp = ns_to_ktime(ns);\n\t}\n\n\tskb->protocol = eth_type_trans(skb, net_dev);\n\n\t \n\tif (hash_valid) {\n\t\tenum pkt_hash_types type;\n\n\t\t \n\t\ttype = be32_to_cpu(fd->status) & FM_FD_STAT_L4CV ?\n\t\t\tPKT_HASH_TYPE_L4 : PKT_HASH_TYPE_L3;\n\t\tskb_set_hash(skb, hash, type);\n\t}\n\n\tskb_len = skb->len;\n\n\tif (unlikely(netif_receive_skb(skb) == NET_RX_DROP)) {\n\t\tpercpu_stats->rx_dropped++;\n\t\treturn qman_cb_dqrr_consume;\n\t}\n\n\tpercpu_stats->rx_packets++;\n\tpercpu_stats->rx_bytes += skb_len;\n\n\treturn qman_cb_dqrr_consume;\n}\n\nstatic enum qman_cb_dqrr_result conf_error_dqrr(struct qman_portal *portal,\n\t\t\t\t\t\tstruct qman_fq *fq,\n\t\t\t\t\t\tconst struct qm_dqrr_entry *dq,\n\t\t\t\t\t\tbool sched_napi)\n{\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tstruct net_device *net_dev;\n\tstruct dpaa_priv *priv;\n\n\tnet_dev = ((struct dpaa_fq *)fq)->net_dev;\n\tpriv = netdev_priv(net_dev);\n\n\tpercpu_priv = this_cpu_ptr(priv->percpu_priv);\n\n\tif (dpaa_eth_napi_schedule(percpu_priv, portal, sched_napi))\n\t\treturn qman_cb_dqrr_stop;\n\n\tdpaa_tx_error(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);\n\n\treturn qman_cb_dqrr_consume;\n}\n\nstatic enum qman_cb_dqrr_result conf_dflt_dqrr(struct qman_portal *portal,\n\t\t\t\t\t       struct qman_fq *fq,\n\t\t\t\t\t       const struct qm_dqrr_entry *dq,\n\t\t\t\t\t       bool sched_napi)\n{\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tstruct net_device *net_dev;\n\tstruct dpaa_priv *priv;\n\n\tnet_dev = ((struct dpaa_fq *)fq)->net_dev;\n\tpriv = netdev_priv(net_dev);\n\n\t \n\ttrace_dpaa_tx_conf_fd(net_dev, fq, &dq->fd);\n\n\tpercpu_priv = this_cpu_ptr(priv->percpu_priv);\n\n\tif (dpaa_eth_napi_schedule(percpu_priv, portal, sched_napi))\n\t\treturn qman_cb_dqrr_stop;\n\n\tdpaa_tx_conf(net_dev, priv, percpu_priv, &dq->fd, fq->fqid);\n\n\treturn qman_cb_dqrr_consume;\n}\n\nstatic void egress_ern(struct qman_portal *portal,\n\t\t       struct qman_fq *fq,\n\t\t       const union qm_mr_entry *msg)\n{\n\tconst struct qm_fd *fd = &msg->ern.fd;\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tconst struct dpaa_priv *priv;\n\tstruct net_device *net_dev;\n\tstruct sk_buff *skb;\n\n\tnet_dev = ((struct dpaa_fq *)fq)->net_dev;\n\tpriv = netdev_priv(net_dev);\n\tpercpu_priv = this_cpu_ptr(priv->percpu_priv);\n\n\tpercpu_priv->stats.tx_dropped++;\n\tpercpu_priv->stats.tx_fifo_errors++;\n\tcount_ern(percpu_priv, msg);\n\n\tskb = dpaa_cleanup_tx_fd(priv, fd, false);\n\tdev_kfree_skb_any(skb);\n}\n\nstatic const struct dpaa_fq_cbs dpaa_fq_cbs = {\n\t.rx_defq = { .cb = { .dqrr = rx_default_dqrr } },\n\t.tx_defq = { .cb = { .dqrr = conf_dflt_dqrr } },\n\t.rx_errq = { .cb = { .dqrr = rx_error_dqrr } },\n\t.tx_errq = { .cb = { .dqrr = conf_error_dqrr } },\n\t.egress_ern = { .cb = { .ern = egress_ern } }\n};\n\nstatic void dpaa_eth_napi_enable(struct dpaa_priv *priv)\n{\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tint i;\n\n\tfor_each_online_cpu(i) {\n\t\tpercpu_priv = per_cpu_ptr(priv->percpu_priv, i);\n\n\t\tpercpu_priv->np.down = false;\n\t\tnapi_enable(&percpu_priv->np.napi);\n\t}\n}\n\nstatic void dpaa_eth_napi_disable(struct dpaa_priv *priv)\n{\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tint i;\n\n\tfor_each_online_cpu(i) {\n\t\tpercpu_priv = per_cpu_ptr(priv->percpu_priv, i);\n\n\t\tpercpu_priv->np.down = true;\n\t\tnapi_disable(&percpu_priv->np.napi);\n\t}\n}\n\nstatic int dpaa_open(struct net_device *net_dev)\n{\n\tstruct mac_device *mac_dev;\n\tstruct dpaa_priv *priv;\n\tint err, i;\n\n\tpriv = netdev_priv(net_dev);\n\tmac_dev = priv->mac_dev;\n\tdpaa_eth_napi_enable(priv);\n\n\terr = phylink_of_phy_connect(mac_dev->phylink,\n\t\t\t\t     mac_dev->dev->of_node, 0);\n\tif (err)\n\t\tgoto phy_init_failed;\n\n\tfor (i = 0; i < ARRAY_SIZE(mac_dev->port); i++) {\n\t\terr = fman_port_enable(mac_dev->port[i]);\n\t\tif (err)\n\t\t\tgoto mac_start_failed;\n\t}\n\n\terr = priv->mac_dev->enable(mac_dev->fman_mac);\n\tif (err < 0) {\n\t\tnetif_err(priv, ifup, net_dev, \"mac_dev->enable() = %d\\n\", err);\n\t\tgoto mac_start_failed;\n\t}\n\tphylink_start(mac_dev->phylink);\n\n\tnetif_tx_start_all_queues(net_dev);\n\n\treturn 0;\n\nmac_start_failed:\n\tfor (i = 0; i < ARRAY_SIZE(mac_dev->port); i++)\n\t\tfman_port_disable(mac_dev->port[i]);\n\tphylink_disconnect_phy(mac_dev->phylink);\n\nphy_init_failed:\n\tdpaa_eth_napi_disable(priv);\n\n\treturn err;\n}\n\nstatic int dpaa_eth_stop(struct net_device *net_dev)\n{\n\tstruct dpaa_priv *priv;\n\tint err;\n\n\terr = dpaa_stop(net_dev);\n\n\tpriv = netdev_priv(net_dev);\n\tdpaa_eth_napi_disable(priv);\n\n\treturn err;\n}\n\nstatic bool xdp_validate_mtu(struct dpaa_priv *priv, int mtu)\n{\n\tint max_contig_data = priv->dpaa_bp->size - priv->rx_headroom;\n\n\t \n\tif (mtu + VLAN_ETH_HLEN + ETH_FCS_LEN > max_contig_data) {\n\t\tdev_warn(priv->net_dev->dev.parent,\n\t\t\t \"The maximum MTU for XDP is %d\\n\",\n\t\t\t max_contig_data - VLAN_ETH_HLEN - ETH_FCS_LEN);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int dpaa_change_mtu(struct net_device *net_dev, int new_mtu)\n{\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\n\tif (priv->xdp_prog && !xdp_validate_mtu(priv, new_mtu))\n\t\treturn -EINVAL;\n\n\tnet_dev->mtu = new_mtu;\n\treturn 0;\n}\n\nstatic int dpaa_setup_xdp(struct net_device *net_dev, struct netdev_bpf *bpf)\n{\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\tstruct bpf_prog *old_prog;\n\tint err;\n\tbool up;\n\n\t \n\tif (bpf->prog && !xdp_validate_mtu(priv, net_dev->mtu)) {\n\t\tNL_SET_ERR_MSG_MOD(bpf->extack, \"MTU too large for XDP\");\n\t\treturn -EINVAL;\n\t}\n\n\tup = netif_running(net_dev);\n\n\tif (up)\n\t\tdpaa_eth_stop(net_dev);\n\n\told_prog = xchg(&priv->xdp_prog, bpf->prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\tif (up) {\n\t\terr = dpaa_open(net_dev);\n\t\tif (err) {\n\t\t\tNL_SET_ERR_MSG_MOD(bpf->extack, \"dpaa_open() failed\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa_xdp(struct net_device *net_dev, struct netdev_bpf *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn dpaa_setup_xdp(net_dev, xdp);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int dpaa_xdp_xmit(struct net_device *net_dev, int n,\n\t\t\t struct xdp_frame **frames, u32 flags)\n{\n\tstruct xdp_frame *xdpf;\n\tint i, nxmit = 0;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\tif (!netif_running(net_dev))\n\t\treturn -ENETDOWN;\n\n\tfor (i = 0; i < n; i++) {\n\t\txdpf = frames[i];\n\t\tif (dpaa_xdp_xmit_frame(net_dev, xdpf))\n\t\t\tbreak;\n\t\tnxmit++;\n\t}\n\n\treturn nxmit;\n}\n\nstatic int dpaa_ts_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tstruct dpaa_priv *priv = netdev_priv(dev);\n\tstruct hwtstamp_config config;\n\n\tif (copy_from_user(&config, rq->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\tswitch (config.tx_type) {\n\tcase HWTSTAMP_TX_OFF:\n\t\t \n\t\tpriv->tx_tstamp = false;\n\t\tbreak;\n\tcase HWTSTAMP_TX_ON:\n\t\tpriv->mac_dev->set_tstamp(priv->mac_dev->fman_mac, true);\n\t\tpriv->tx_tstamp = true;\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tif (config.rx_filter == HWTSTAMP_FILTER_NONE) {\n\t\t \n\t\tpriv->rx_tstamp = false;\n\t} else {\n\t\tpriv->mac_dev->set_tstamp(priv->mac_dev->fman_mac, true);\n\t\tpriv->rx_tstamp = true;\n\t\t \n\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t}\n\n\treturn copy_to_user(rq->ifr_data, &config, sizeof(config)) ?\n\t\t\t-EFAULT : 0;\n}\n\nstatic int dpaa_ioctl(struct net_device *net_dev, struct ifreq *rq, int cmd)\n{\n\tint ret = -EINVAL;\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\n\tif (cmd == SIOCGMIIREG) {\n\t\tif (net_dev->phydev)\n\t\t\treturn phylink_mii_ioctl(priv->mac_dev->phylink, rq,\n\t\t\t\t\t\t cmd);\n\t}\n\n\tif (cmd == SIOCSHWTSTAMP)\n\t\treturn dpaa_ts_ioctl(net_dev, rq, cmd);\n\n\treturn ret;\n}\n\nstatic const struct net_device_ops dpaa_ops = {\n\t.ndo_open = dpaa_open,\n\t.ndo_start_xmit = dpaa_start_xmit,\n\t.ndo_stop = dpaa_eth_stop,\n\t.ndo_tx_timeout = dpaa_tx_timeout,\n\t.ndo_get_stats64 = dpaa_get_stats64,\n\t.ndo_change_carrier = fixed_phy_change_carrier,\n\t.ndo_set_mac_address = dpaa_set_mac_address,\n\t.ndo_validate_addr = eth_validate_addr,\n\t.ndo_set_rx_mode = dpaa_set_rx_mode,\n\t.ndo_eth_ioctl = dpaa_ioctl,\n\t.ndo_setup_tc = dpaa_setup_tc,\n\t.ndo_change_mtu = dpaa_change_mtu,\n\t.ndo_bpf = dpaa_xdp,\n\t.ndo_xdp_xmit = dpaa_xdp_xmit,\n};\n\nstatic int dpaa_napi_add(struct net_device *net_dev)\n{\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tpercpu_priv = per_cpu_ptr(priv->percpu_priv, cpu);\n\n\t\tnetif_napi_add(net_dev, &percpu_priv->np.napi, dpaa_eth_poll);\n\t}\n\n\treturn 0;\n}\n\nstatic void dpaa_napi_del(struct net_device *net_dev)\n{\n\tstruct dpaa_priv *priv = netdev_priv(net_dev);\n\tstruct dpaa_percpu_priv *percpu_priv;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tpercpu_priv = per_cpu_ptr(priv->percpu_priv, cpu);\n\n\t\tnetif_napi_del(&percpu_priv->np.napi);\n\t}\n}\n\nstatic inline void dpaa_bp_free_pf(const struct dpaa_bp *bp,\n\t\t\t\t   struct bm_buffer *bmb)\n{\n\tdma_addr_t addr = bm_buf_addr(bmb);\n\n\tdma_unmap_page(bp->priv->rx_dma_dev, addr, DPAA_BP_RAW_SIZE,\n\t\t       DMA_FROM_DEVICE);\n\n\tskb_free_frag(phys_to_virt(addr));\n}\n\n \nstatic struct dpaa_bp *dpaa_bp_alloc(struct device *dev)\n{\n\tstruct dpaa_bp *dpaa_bp;\n\n\tdpaa_bp = devm_kzalloc(dev, sizeof(*dpaa_bp), GFP_KERNEL);\n\tif (!dpaa_bp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdpaa_bp->bpid = FSL_DPAA_BPID_INV;\n\tdpaa_bp->percpu_count = devm_alloc_percpu(dev, *dpaa_bp->percpu_count);\n\tif (!dpaa_bp->percpu_count)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdpaa_bp->config_count = FSL_DPAA_ETH_MAX_BUF_COUNT;\n\n\tdpaa_bp->seed_cb = dpaa_bp_seed;\n\tdpaa_bp->free_buf_cb = dpaa_bp_free_pf;\n\n\treturn dpaa_bp;\n}\n\n \nstatic int dpaa_ingress_cgr_init(struct dpaa_priv *priv)\n{\n\tstruct qm_mcc_initcgr initcgr;\n\tu32 cs_th;\n\tint err;\n\n\terr = qman_alloc_cgrid(&priv->ingress_cgr.cgrid);\n\tif (err < 0) {\n\t\tif (netif_msg_drv(priv))\n\t\t\tpr_err(\"Error %d allocating CGR ID\\n\", err);\n\t\tgoto out_error;\n\t}\n\n\t \n\tmemset(&initcgr, 0, sizeof(initcgr));\n\tinitcgr.we_mask = cpu_to_be16(QM_CGR_WE_CS_THRES);\n\tinitcgr.cgr.cscn_en = QM_CGR_EN;\n\tcs_th = DPAA_INGRESS_CS_THRESHOLD;\n\tqm_cgr_cs_thres_set64(&initcgr.cgr.cs_thres, cs_th, 1);\n\n\tinitcgr.we_mask |= cpu_to_be16(QM_CGR_WE_CSTD_EN);\n\tinitcgr.cgr.cstd_en = QM_CGR_EN;\n\n\t \n\terr = qman_create_cgr(&priv->ingress_cgr, QMAN_CGR_FLAG_USE_INIT,\n\t\t\t      &initcgr);\n\tif (err < 0) {\n\t\tif (netif_msg_drv(priv))\n\t\t\tpr_err(\"Error %d creating ingress CGR with ID %d\\n\",\n\t\t\t       err, priv->ingress_cgr.cgrid);\n\t\tqman_release_cgrid(priv->ingress_cgr.cgrid);\n\t\tgoto out_error;\n\t}\n\tif (netif_msg_drv(priv))\n\t\tpr_debug(\"Created ingress CGR %d for netdev with hwaddr %pM\\n\",\n\t\t\t priv->ingress_cgr.cgrid, priv->mac_dev->addr);\n\n\tpriv->use_ingress_cgr = true;\n\nout_error:\n\treturn err;\n}\n\nstatic u16 dpaa_get_headroom(struct dpaa_buffer_layout *bl,\n\t\t\t     enum port_type port)\n{\n\tu16 headroom;\n\n\t \n\theadroom = (u16)(bl[port].priv_data_size + DPAA_HWA_SIZE);\n\n\tif (port == RX) {\n#ifdef CONFIG_DPAA_ERRATUM_A050385\n\t\tif (unlikely(fman_has_errata_a050385()))\n\t\t\theadroom = XDP_PACKET_HEADROOM;\n#endif\n\n\t\treturn ALIGN(headroom, DPAA_FD_RX_DATA_ALIGNMENT);\n\t} else {\n\t\treturn ALIGN(headroom, DPAA_FD_DATA_ALIGNMENT);\n\t}\n}\n\nstatic int dpaa_eth_probe(struct platform_device *pdev)\n{\n\tstruct net_device *net_dev = NULL;\n\tstruct dpaa_bp *dpaa_bp = NULL;\n\tstruct dpaa_fq *dpaa_fq, *tmp;\n\tstruct dpaa_priv *priv = NULL;\n\tstruct fm_port_fqs port_fqs;\n\tstruct mac_device *mac_dev;\n\tint err = 0, channel;\n\tstruct device *dev;\n\n\tdev = &pdev->dev;\n\n\terr = bman_is_probed();\n\tif (!err)\n\t\treturn -EPROBE_DEFER;\n\tif (err < 0) {\n\t\tdev_err(dev, \"failing probe due to bman probe error\\n\");\n\t\treturn -ENODEV;\n\t}\n\terr = qman_is_probed();\n\tif (!err)\n\t\treturn -EPROBE_DEFER;\n\tif (err < 0) {\n\t\tdev_err(dev, \"failing probe due to qman probe error\\n\");\n\t\treturn -ENODEV;\n\t}\n\terr = bman_portals_probed();\n\tif (!err)\n\t\treturn -EPROBE_DEFER;\n\tif (err < 0) {\n\t\tdev_err(dev,\n\t\t\t\"failing probe due to bman portals probe error\\n\");\n\t\treturn -ENODEV;\n\t}\n\terr = qman_portals_probed();\n\tif (!err)\n\t\treturn -EPROBE_DEFER;\n\tif (err < 0) {\n\t\tdev_err(dev,\n\t\t\t\"failing probe due to qman portals probe error\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tnet_dev = alloc_etherdev_mq(sizeof(*priv), DPAA_ETH_TXQ_NUM);\n\tif (!net_dev) {\n\t\tdev_err(dev, \"alloc_etherdev_mq() failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tSET_NETDEV_DEV(net_dev, dev->parent);\n\tdev_set_drvdata(dev, net_dev);\n\n\tpriv = netdev_priv(net_dev);\n\tpriv->net_dev = net_dev;\n\n\tpriv->msg_enable = netif_msg_init(debug, DPAA_MSG_DEFAULT);\n\n\tmac_dev = dpaa_mac_dev_get(pdev);\n\tif (IS_ERR(mac_dev)) {\n\t\tnetdev_err(net_dev, \"dpaa_mac_dev_get() failed\\n\");\n\t\terr = PTR_ERR(mac_dev);\n\t\tgoto free_netdev;\n\t}\n\n\t \n\tpriv->rx_dma_dev = fman_port_get_device(mac_dev->port[RX]);\n\tpriv->tx_dma_dev = fman_port_get_device(mac_dev->port[TX]);\n\terr = dma_coerce_mask_and_coherent(priv->rx_dma_dev, DMA_BIT_MASK(40));\n\tif (!err)\n\t\terr = dma_coerce_mask_and_coherent(priv->tx_dma_dev,\n\t\t\t\t\t\t   DMA_BIT_MASK(40));\n\tif (err) {\n\t\tnetdev_err(net_dev, \"dma_coerce_mask_and_coherent() failed\\n\");\n\t\tgoto free_netdev;\n\t}\n\n\t \n\tnet_dev->mtu = min(dpaa_get_max_mtu(), ETH_DATA_LEN);\n\n\tnetdev_dbg(net_dev, \"Setting initial MTU on net device: %d\\n\",\n\t\t   net_dev->mtu);\n\n\tpriv->buf_layout[RX].priv_data_size = DPAA_RX_PRIV_DATA_SIZE;  \n\tpriv->buf_layout[TX].priv_data_size = DPAA_TX_PRIV_DATA_SIZE;  \n\n\t \n\tdpaa_bp = dpaa_bp_alloc(dev);\n\tif (IS_ERR(dpaa_bp)) {\n\t\terr = PTR_ERR(dpaa_bp);\n\t\tgoto free_dpaa_bps;\n\t}\n\t \n\tdpaa_bp->raw_size = DPAA_BP_RAW_SIZE;\n\t \n\tdpaa_bp->size = dpaa_bp_size(dpaa_bp->raw_size);\n\tdpaa_bp->priv = priv;\n\n\terr = dpaa_bp_alloc_pool(dpaa_bp);\n\tif (err < 0)\n\t\tgoto free_dpaa_bps;\n\tpriv->dpaa_bp = dpaa_bp;\n\n\tINIT_LIST_HEAD(&priv->dpaa_fq_list);\n\n\tmemset(&port_fqs, 0, sizeof(port_fqs));\n\n\terr = dpaa_alloc_all_fqs(dev, &priv->dpaa_fq_list, &port_fqs);\n\tif (err < 0) {\n\t\tdev_err(dev, \"dpaa_alloc_all_fqs() failed\\n\");\n\t\tgoto free_dpaa_bps;\n\t}\n\n\tpriv->mac_dev = mac_dev;\n\n\tchannel = dpaa_get_channel();\n\tif (channel < 0) {\n\t\tdev_err(dev, \"dpaa_get_channel() failed\\n\");\n\t\terr = channel;\n\t\tgoto free_dpaa_bps;\n\t}\n\n\tpriv->channel = (u16)channel;\n\n\t \n\tdpaa_eth_add_channel(priv->channel, &pdev->dev);\n\n\tdpaa_fq_setup(priv, &dpaa_fq_cbs, priv->mac_dev->port[TX]);\n\n\t \n\terr = dpaa_eth_cgr_init(priv);\n\tif (err < 0) {\n\t\tdev_err(dev, \"Error initializing CGR\\n\");\n\t\tgoto free_dpaa_bps;\n\t}\n\n\terr = dpaa_ingress_cgr_init(priv);\n\tif (err < 0) {\n\t\tdev_err(dev, \"Error initializing ingress CGR\\n\");\n\t\tgoto delete_egress_cgr;\n\t}\n\n\t \n\tlist_for_each_entry_safe(dpaa_fq, tmp, &priv->dpaa_fq_list, list) {\n\t\terr = dpaa_fq_init(dpaa_fq, false);\n\t\tif (err < 0)\n\t\t\tgoto free_dpaa_fqs;\n\t}\n\n\tpriv->tx_headroom = dpaa_get_headroom(priv->buf_layout, TX);\n\tpriv->rx_headroom = dpaa_get_headroom(priv->buf_layout, RX);\n\n\t \n\terr = dpaa_eth_init_ports(mac_dev, dpaa_bp, &port_fqs,\n\t\t\t\t  &priv->buf_layout[0], dev);\n\tif (err)\n\t\tgoto free_dpaa_fqs;\n\n\t \n\tpriv->keygen_in_use = true;\n\n\tpriv->percpu_priv = devm_alloc_percpu(dev, *priv->percpu_priv);\n\tif (!priv->percpu_priv) {\n\t\tdev_err(dev, \"devm_alloc_percpu() failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto free_dpaa_fqs;\n\t}\n\n\tpriv->num_tc = 1;\n\tnetif_set_real_num_tx_queues(net_dev, priv->num_tc * DPAA_TC_TXQ_NUM);\n\n\t \n\terr = dpaa_napi_add(net_dev);\n\tif (err < 0)\n\t\tgoto delete_dpaa_napi;\n\n\terr = dpaa_netdev_init(net_dev, &dpaa_ops, tx_timeout);\n\tif (err < 0)\n\t\tgoto delete_dpaa_napi;\n\n\tdpaa_eth_sysfs_init(&net_dev->dev);\n\n\tnetif_info(priv, probe, net_dev, \"Probed interface %s\\n\",\n\t\t   net_dev->name);\n\n\treturn 0;\n\ndelete_dpaa_napi:\n\tdpaa_napi_del(net_dev);\nfree_dpaa_fqs:\n\tdpaa_fq_free(dev, &priv->dpaa_fq_list);\n\tqman_delete_cgr_safe(&priv->ingress_cgr);\n\tqman_release_cgrid(priv->ingress_cgr.cgrid);\ndelete_egress_cgr:\n\tqman_delete_cgr_safe(&priv->cgr_data.cgr);\n\tqman_release_cgrid(priv->cgr_data.cgr.cgrid);\nfree_dpaa_bps:\n\tdpaa_bps_free(priv);\nfree_netdev:\n\tdev_set_drvdata(dev, NULL);\n\tfree_netdev(net_dev);\n\n\treturn err;\n}\n\nstatic void dpaa_remove(struct platform_device *pdev)\n{\n\tstruct net_device *net_dev;\n\tstruct dpaa_priv *priv;\n\tstruct device *dev;\n\tint err;\n\n\tdev = &pdev->dev;\n\tnet_dev = dev_get_drvdata(dev);\n\n\tpriv = netdev_priv(net_dev);\n\n\tdpaa_eth_sysfs_remove(dev);\n\n\tdev_set_drvdata(dev, NULL);\n\tunregister_netdev(net_dev);\n\tphylink_destroy(priv->mac_dev->phylink);\n\n\terr = dpaa_fq_free(dev, &priv->dpaa_fq_list);\n\tif (err)\n\t\tdev_err(dev, \"Failed to free FQs on remove (%pE)\\n\",\n\t\t\tERR_PTR(err));\n\n\tqman_delete_cgr_safe(&priv->ingress_cgr);\n\tqman_release_cgrid(priv->ingress_cgr.cgrid);\n\tqman_delete_cgr_safe(&priv->cgr_data.cgr);\n\tqman_release_cgrid(priv->cgr_data.cgr.cgrid);\n\n\tdpaa_napi_del(net_dev);\n\n\tdpaa_bps_free(priv);\n\n\tfree_netdev(net_dev);\n}\n\nstatic const struct platform_device_id dpaa_devtype[] = {\n\t{\n\t\t.name = \"dpaa-ethernet\",\n\t\t.driver_data = 0,\n\t}, {\n\t}\n};\nMODULE_DEVICE_TABLE(platform, dpaa_devtype);\n\nstatic struct platform_driver dpaa_driver = {\n\t.driver = {\n\t\t.name = KBUILD_MODNAME,\n\t},\n\t.id_table = dpaa_devtype,\n\t.probe = dpaa_eth_probe,\n\t.remove_new = dpaa_remove\n};\n\nstatic int __init dpaa_load(void)\n{\n\tint err;\n\n\tpr_debug(\"FSL DPAA Ethernet driver\\n\");\n\n\t \n\tdpaa_rx_extra_headroom = fman_get_rx_extra_headroom();\n\tdpaa_max_frm = fman_get_max_frm();\n\n\terr = platform_driver_register(&dpaa_driver);\n\tif (err < 0)\n\t\tpr_err(\"Error, platform_driver_register() = %d\\n\", err);\n\n\treturn err;\n}\nmodule_init(dpaa_load);\n\nstatic void __exit dpaa_unload(void)\n{\n\tplatform_driver_unregister(&dpaa_driver);\n\n\t \n\tdpaa_release_channel();\n}\nmodule_exit(dpaa_unload);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_DESCRIPTION(\"FSL DPAA Ethernet driver\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}