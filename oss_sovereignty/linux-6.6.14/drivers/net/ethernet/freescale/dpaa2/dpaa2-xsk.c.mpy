{
  "module_name": "dpaa2-xsk.c",
  "hash_id": "e5b64291708ef5df35438155ae0059b6e5e84f5a266e044b7967457933f2c80e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/freescale/dpaa2/dpaa2-xsk.c",
  "human_readable_source": "\n \n#include <linux/filter.h>\n#include <linux/compiler.h>\n#include <linux/bpf_trace.h>\n#include <net/xdp.h>\n#include <net/xdp_sock_drv.h>\n\n#include \"dpaa2-eth.h\"\n\nstatic void dpaa2_eth_setup_consume_func(struct dpaa2_eth_priv *priv,\n\t\t\t\t\t struct dpaa2_eth_channel *ch,\n\t\t\t\t\t enum dpaa2_eth_fq_type type,\n\t\t\t\t\t dpaa2_eth_consume_cb_t *consume)\n{\n\tstruct dpaa2_eth_fq *fq;\n\tint i;\n\n\tfor (i = 0; i < priv->num_fqs; i++) {\n\t\tfq = &priv->fq[i];\n\n\t\tif (fq->type != type)\n\t\t\tcontinue;\n\t\tif (fq->channel != ch)\n\t\t\tcontinue;\n\n\t\tfq->consume = consume;\n\t}\n}\n\nstatic u32 dpaa2_xsk_run_xdp(struct dpaa2_eth_priv *priv,\n\t\t\t     struct dpaa2_eth_channel *ch,\n\t\t\t     struct dpaa2_eth_fq *rx_fq,\n\t\t\t     struct dpaa2_fd *fd, void *vaddr)\n{\n\tdma_addr_t addr = dpaa2_fd_get_addr(fd);\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_buff *xdp_buff;\n\tstruct dpaa2_eth_swa *swa;\n\tu32 xdp_act = XDP_PASS;\n\tint err;\n\n\txdp_prog = READ_ONCE(ch->xdp.prog);\n\tif (!xdp_prog)\n\t\tgoto out;\n\n\tswa = (struct dpaa2_eth_swa *)(vaddr + DPAA2_ETH_RX_HWA_SIZE +\n\t\t\t\t       ch->xsk_pool->umem->headroom);\n\txdp_buff = swa->xsk.xdp_buff;\n\n\txdp_buff->data_hard_start = vaddr;\n\txdp_buff->data = vaddr + dpaa2_fd_get_offset(fd);\n\txdp_buff->data_end = xdp_buff->data + dpaa2_fd_get_len(fd);\n\txdp_set_data_meta_invalid(xdp_buff);\n\txdp_buff->rxq = &ch->xdp_rxq;\n\n\txsk_buff_dma_sync_for_cpu(xdp_buff, ch->xsk_pool);\n\txdp_act = bpf_prog_run_xdp(xdp_prog, xdp_buff);\n\n\t \n\tdpaa2_fd_set_offset(fd, xdp_buff->data - vaddr);\n\tdpaa2_fd_set_len(fd, xdp_buff->data_end - xdp_buff->data);\n\n\tif (likely(xdp_act == XDP_REDIRECT)) {\n\t\terr = xdp_do_redirect(priv->net_dev, xdp_buff, xdp_prog);\n\t\tif (unlikely(err)) {\n\t\t\tch->stats.xdp_drop++;\n\t\t\tdpaa2_eth_recycle_buf(priv, ch, addr);\n\t\t} else {\n\t\t\tch->buf_count--;\n\t\t\tch->stats.xdp_redirect++;\n\t\t}\n\n\t\tgoto xdp_redir;\n\t}\n\n\tswitch (xdp_act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\tdpaa2_eth_xdp_enqueue(priv, ch, fd, vaddr, rx_fq->flowid);\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(priv->net_dev, xdp_prog, xdp_act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(priv->net_dev, xdp_prog, xdp_act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tdpaa2_eth_recycle_buf(priv, ch, addr);\n\t\tch->stats.xdp_drop++;\n\t\tbreak;\n\t}\n\nxdp_redir:\n\tch->xdp.res |= xdp_act;\nout:\n\treturn xdp_act;\n}\n\n \nstatic void dpaa2_xsk_rx(struct dpaa2_eth_priv *priv,\n\t\t\t struct dpaa2_eth_channel *ch,\n\t\t\t const struct dpaa2_fd *fd,\n\t\t\t struct dpaa2_eth_fq *fq)\n{\n\tdma_addr_t addr = dpaa2_fd_get_addr(fd);\n\tu8 fd_format = dpaa2_fd_get_format(fd);\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tu32 fd_length = dpaa2_fd_get_len(fd);\n\tstruct sk_buff *skb;\n\tvoid *vaddr;\n\tu32 xdp_act;\n\n\ttrace_dpaa2_rx_xsk_fd(priv->net_dev, fd);\n\n\tvaddr = dpaa2_iova_to_virt(priv->iommu_domain, addr);\n\tpercpu_stats = this_cpu_ptr(priv->percpu_stats);\n\n\tif (fd_format != dpaa2_fd_single) {\n\t\tWARN_ON(priv->xdp_prog);\n\t\t \n\t\tgoto err_frame_format;\n\t}\n\n\txdp_act = dpaa2_xsk_run_xdp(priv, ch, fq, (struct dpaa2_fd *)fd, vaddr);\n\tif (xdp_act != XDP_PASS) {\n\t\tpercpu_stats->rx_packets++;\n\t\tpercpu_stats->rx_bytes += dpaa2_fd_get_len(fd);\n\t\treturn;\n\t}\n\n\t \n\tskb = dpaa2_eth_alloc_skb(priv, ch, fd, fd_length, vaddr);\n\tif (!skb)\n\t\t \n\t\tgoto err_alloc_skb;\n\n\t \n\tdpaa2_eth_receive_skb(priv, ch, fd, vaddr, fq, percpu_stats, skb);\n\n\treturn;\n\nerr_alloc_skb:\n\tdpaa2_eth_recycle_buf(priv, ch, addr);\nerr_frame_format:\n\tpercpu_stats->rx_dropped++;\n}\n\nstatic void dpaa2_xsk_set_bp_per_qdbin(struct dpaa2_eth_priv *priv,\n\t\t\t\t       struct dpni_pools_cfg *pools_params)\n{\n\tint curr_bp = 0, i, j;\n\n\tpools_params->pool_options = DPNI_POOL_ASSOC_QDBIN;\n\tfor (i = 0; i < priv->num_bps; i++) {\n\t\tfor (j = 0; j < priv->num_channels; j++)\n\t\t\tif (priv->bp[i] == priv->channel[j]->bp)\n\t\t\t\tpools_params->pools[curr_bp].priority_mask |= (1 << j);\n\t\tif (!pools_params->pools[curr_bp].priority_mask)\n\t\t\tcontinue;\n\n\t\tpools_params->pools[curr_bp].dpbp_id = priv->bp[i]->bpid;\n\t\tpools_params->pools[curr_bp].buffer_size = priv->rx_buf_size;\n\t\tpools_params->pools[curr_bp++].backup_pool = 0;\n\t}\n\tpools_params->num_dpbp = curr_bp;\n}\n\nstatic int dpaa2_xsk_disable_pool(struct net_device *dev, u16 qid)\n{\n\tstruct xsk_buff_pool *pool = xsk_get_pool_from_qid(dev, qid);\n\tstruct dpaa2_eth_priv *priv = netdev_priv(dev);\n\tstruct dpni_pools_cfg pools_params = { 0 };\n\tstruct dpaa2_eth_channel *ch;\n\tint err;\n\tbool up;\n\n\tch = priv->channel[qid];\n\tif (!ch->xsk_pool)\n\t\treturn -EINVAL;\n\n\tup = netif_running(dev);\n\tif (up)\n\t\tdev_close(dev);\n\n\txsk_pool_dma_unmap(pool, 0);\n\terr = xdp_rxq_info_reg_mem_model(&ch->xdp_rxq,\n\t\t\t\t\t MEM_TYPE_PAGE_ORDER0, NULL);\n\tif (err)\n\t\tnetdev_err(dev, \"xsk_rxq_info_reg_mem_model() failed (err = %d)\\n\",\n\t\t\t   err);\n\n\tdpaa2_eth_free_dpbp(priv, ch->bp);\n\n\tch->xsk_zc = false;\n\tch->xsk_pool = NULL;\n\tch->xsk_tx_pkts_sent = 0;\n\tch->bp = priv->bp[DPAA2_ETH_DEFAULT_BP_IDX];\n\n\tdpaa2_eth_setup_consume_func(priv, ch, DPAA2_RX_FQ, dpaa2_eth_rx);\n\n\tdpaa2_xsk_set_bp_per_qdbin(priv, &pools_params);\n\terr = dpni_set_pools(priv->mc_io, 0, priv->mc_token, &pools_params);\n\tif (err)\n\t\tnetdev_err(dev, \"dpni_set_pools() failed\\n\");\n\n\tif (up) {\n\t\terr = dev_open(dev, NULL);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_xsk_enable_pool(struct net_device *dev,\n\t\t\t\t struct xsk_buff_pool *pool,\n\t\t\t\t u16 qid)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(dev);\n\tstruct dpni_pools_cfg pools_params = { 0 };\n\tstruct dpaa2_eth_channel *ch;\n\tint err, err2;\n\tbool up;\n\n\tif (priv->dpni_attrs.wriop_version < DPAA2_WRIOP_VERSION(3, 0, 0)) {\n\t\tnetdev_err(dev, \"AF_XDP zero-copy not supported on devices <= WRIOP(3, 0, 0)\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (priv->dpni_attrs.num_queues > 8) {\n\t\tnetdev_err(dev, \"AF_XDP zero-copy not supported on DPNI with more then 8 queues\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tup = netif_running(dev);\n\tif (up)\n\t\tdev_close(dev);\n\n\terr = xsk_pool_dma_map(pool, priv->net_dev->dev.parent, 0);\n\tif (err) {\n\t\tnetdev_err(dev, \"xsk_pool_dma_map() failed (err = %d)\\n\",\n\t\t\t   err);\n\t\tgoto err_dma_unmap;\n\t}\n\n\tch = priv->channel[qid];\n\terr = xdp_rxq_info_reg_mem_model(&ch->xdp_rxq, MEM_TYPE_XSK_BUFF_POOL, NULL);\n\tif (err) {\n\t\tnetdev_err(dev, \"xdp_rxq_info_reg_mem_model() failed (err = %d)\\n\", err);\n\t\tgoto err_mem_model;\n\t}\n\txsk_pool_set_rxq_info(pool, &ch->xdp_rxq);\n\n\tpriv->bp[priv->num_bps] = dpaa2_eth_allocate_dpbp(priv);\n\tif (IS_ERR(priv->bp[priv->num_bps])) {\n\t\terr = PTR_ERR(priv->bp[priv->num_bps]);\n\t\tgoto err_bp_alloc;\n\t}\n\tch->xsk_zc = true;\n\tch->xsk_pool = pool;\n\tch->bp = priv->bp[priv->num_bps++];\n\n\tdpaa2_eth_setup_consume_func(priv, ch, DPAA2_RX_FQ, dpaa2_xsk_rx);\n\n\tdpaa2_xsk_set_bp_per_qdbin(priv, &pools_params);\n\terr = dpni_set_pools(priv->mc_io, 0, priv->mc_token, &pools_params);\n\tif (err) {\n\t\tnetdev_err(dev, \"dpni_set_pools() failed\\n\");\n\t\tgoto err_set_pools;\n\t}\n\n\tif (up) {\n\t\terr = dev_open(dev, NULL);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n\nerr_set_pools:\n\terr2 = dpaa2_xsk_disable_pool(dev, qid);\n\tif (err2)\n\t\tnetdev_err(dev, \"dpaa2_xsk_disable_pool() failed %d\\n\", err2);\nerr_bp_alloc:\n\terr2 = xdp_rxq_info_reg_mem_model(&priv->channel[qid]->xdp_rxq,\n\t\t\t\t\t  MEM_TYPE_PAGE_ORDER0, NULL);\n\tif (err2)\n\t\tnetdev_err(dev, \"xsk_rxq_info_reg_mem_model() failed with %d)\\n\", err2);\nerr_mem_model:\n\txsk_pool_dma_unmap(pool, 0);\nerr_dma_unmap:\n\tif (up)\n\t\tdev_open(dev, NULL);\n\n\treturn err;\n}\n\nint dpaa2_xsk_setup_pool(struct net_device *dev, struct xsk_buff_pool *pool, u16 qid)\n{\n\treturn pool ? dpaa2_xsk_enable_pool(dev, pool, qid) :\n\t\t      dpaa2_xsk_disable_pool(dev, qid);\n}\n\nint dpaa2_xsk_wakeup(struct net_device *dev, u32 qid, u32 flags)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(dev);\n\tstruct dpaa2_eth_channel *ch = priv->channel[qid];\n\n\tif (!priv->link_state.up)\n\t\treturn -ENETDOWN;\n\n\tif (!priv->xdp_prog)\n\t\treturn -EINVAL;\n\n\tif (!ch->xsk_zc)\n\t\treturn -EINVAL;\n\n\t \n\tif (!napi_if_scheduled_mark_missed(&ch->napi))\n\t\tnapi_schedule(&ch->napi);\n\n\treturn 0;\n}\n\nstatic int dpaa2_xsk_tx_build_fd(struct dpaa2_eth_priv *priv,\n\t\t\t\t struct dpaa2_eth_channel *ch,\n\t\t\t\t struct dpaa2_fd *fd,\n\t\t\t\t struct xdp_desc *xdp_desc)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpaa2_sg_entry *sgt;\n\tstruct dpaa2_eth_swa *swa;\n\tvoid *sgt_buf = NULL;\n\tdma_addr_t sgt_addr;\n\tint sgt_buf_size;\n\tdma_addr_t addr;\n\tint err = 0;\n\n\t \n\tsgt_buf_size = priv->tx_data_offset + sizeof(struct dpaa2_sg_entry);\n\tsgt_buf = dpaa2_eth_sgt_get(priv);\n\tif (unlikely(!sgt_buf))\n\t\treturn -ENOMEM;\n\tsgt = (struct dpaa2_sg_entry *)(sgt_buf + priv->tx_data_offset);\n\n\t \n\taddr = xsk_buff_raw_get_dma(ch->xsk_pool, xdp_desc->addr);\n\txsk_buff_raw_dma_sync_for_device(ch->xsk_pool, addr, xdp_desc->len);\n\n\t \n\tdpaa2_sg_set_addr(sgt, addr);\n\tdpaa2_sg_set_len(sgt, xdp_desc->len);\n\tdpaa2_sg_set_final(sgt, true);\n\n\t \n\tswa = (struct dpaa2_eth_swa *)sgt_buf;\n\tswa->type = DPAA2_ETH_SWA_XSK;\n\tswa->xsk.sgt_size = sgt_buf_size;\n\n\t \n\tsgt_addr = dma_map_single(dev, sgt_buf, sgt_buf_size, DMA_BIDIRECTIONAL);\n\tif (unlikely(dma_mapping_error(dev, sgt_addr))) {\n\t\terr = -ENOMEM;\n\t\tgoto sgt_map_failed;\n\t}\n\n\t \n\tmemset(fd, 0, sizeof(struct dpaa2_fd));\n\tdpaa2_fd_set_offset(fd, priv->tx_data_offset);\n\tdpaa2_fd_set_format(fd, dpaa2_fd_sg);\n\tdpaa2_fd_set_addr(fd, sgt_addr);\n\tdpaa2_fd_set_len(fd, xdp_desc->len);\n\tdpaa2_fd_set_ctrl(fd, FD_CTRL_PTA);\n\n\treturn 0;\n\nsgt_map_failed:\n\tdpaa2_eth_sgt_recycle(priv, sgt_buf);\n\n\treturn err;\n}\n\nbool dpaa2_xsk_tx(struct dpaa2_eth_priv *priv,\n\t\t  struct dpaa2_eth_channel *ch)\n{\n\tstruct xdp_desc *xdp_descs = ch->xsk_pool->tx_descs;\n\tstruct dpaa2_eth_drv_stats *percpu_extras;\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tint budget = DPAA2_ETH_TX_ZC_PER_NAPI;\n\tint total_enqueued, enqueued;\n\tint retries, max_retries;\n\tstruct dpaa2_eth_fq *fq;\n\tstruct dpaa2_fd *fds;\n\tint batch, i, err;\n\n\tpercpu_stats = this_cpu_ptr(priv->percpu_stats);\n\tpercpu_extras = this_cpu_ptr(priv->percpu_extras);\n\tfds = (this_cpu_ptr(priv->fd))->array;\n\n\t \n\tfq = &priv->fq[ch->nctx.desired_cpu];\n\n\tbatch = xsk_tx_peek_release_desc_batch(ch->xsk_pool, budget);\n\tif (!batch)\n\t\treturn false;\n\n\t \n\tfor (i = 0; i < batch; i++) {\n\t\terr = dpaa2_xsk_tx_build_fd(priv, ch, &fds[i], &xdp_descs[i]);\n\t\tif (err) {\n\t\t\tbatch = i;\n\t\t\tbreak;\n\t\t}\n\n\t\ttrace_dpaa2_tx_xsk_fd(priv->net_dev, &fds[i]);\n\t}\n\n\t \n\tmax_retries = batch * DPAA2_ETH_ENQUEUE_RETRIES;\n\ttotal_enqueued = 0;\n\tenqueued = 0;\n\tretries = 0;\n\twhile (total_enqueued < batch && retries < max_retries) {\n\t\terr = priv->enqueue(priv, fq, &fds[total_enqueued], 0,\n\t\t\t\t    batch - total_enqueued, &enqueued);\n\t\tif (err == -EBUSY) {\n\t\t\tretries++;\n\t\t\tcontinue;\n\t\t}\n\n\t\ttotal_enqueued += enqueued;\n\t}\n\tpercpu_extras->tx_portal_busy += retries;\n\n\t \n\tpercpu_stats->tx_packets += total_enqueued;\n\tfor (i = 0; i < total_enqueued; i++)\n\t\tpercpu_stats->tx_bytes += dpaa2_fd_get_len(&fds[i]);\n\tfor (i = total_enqueued; i < batch; i++) {\n\t\tdpaa2_eth_free_tx_fd(priv, ch, fq, &fds[i], false);\n\t\tpercpu_stats->tx_errors++;\n\t}\n\n\txsk_tx_release(ch->xsk_pool);\n\n\treturn total_enqueued == budget;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}