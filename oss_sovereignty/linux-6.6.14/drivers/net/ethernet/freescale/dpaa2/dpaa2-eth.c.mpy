{
  "module_name": "dpaa2-eth.c",
  "hash_id": "721e70b4eab00303f2c53646c97d1158eccb31892382caa3f3487a9483ced6cb",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/freescale/dpaa2/dpaa2-eth.c",
  "human_readable_source": "\n \n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/etherdevice.h>\n#include <linux/of_net.h>\n#include <linux/interrupt.h>\n#include <linux/kthread.h>\n#include <linux/iommu.h>\n#include <linux/fsl/mc.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <linux/fsl/ptp_qoriq.h>\n#include <linux/ptp_classify.h>\n#include <net/pkt_cls.h>\n#include <net/sock.h>\n#include <net/tso.h>\n#include <net/xdp_sock_drv.h>\n\n#include \"dpaa2-eth.h\"\n\n \n#define CREATE_TRACE_POINTS\n#include \"dpaa2-eth-trace.h\"\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_AUTHOR(\"Freescale Semiconductor, Inc\");\nMODULE_DESCRIPTION(\"Freescale DPAA2 Ethernet Driver\");\n\nstruct ptp_qoriq *dpaa2_ptp;\nEXPORT_SYMBOL(dpaa2_ptp);\n\nstatic void dpaa2_eth_detect_features(struct dpaa2_eth_priv *priv)\n{\n\tpriv->features = 0;\n\n\tif (dpaa2_eth_cmp_dpni_ver(priv, DPNI_PTP_ONESTEP_VER_MAJOR,\n\t\t\t\t   DPNI_PTP_ONESTEP_VER_MINOR) >= 0)\n\t\tpriv->features |= DPAA2_ETH_FEATURE_ONESTEP_CFG_DIRECT;\n}\n\nstatic void dpaa2_update_ptp_onestep_indirect(struct dpaa2_eth_priv *priv,\n\t\t\t\t\t      u32 offset, u8 udp)\n{\n\tstruct dpni_single_step_cfg cfg;\n\n\tcfg.en = 1;\n\tcfg.ch_update = udp;\n\tcfg.offset = offset;\n\tcfg.peer_delay = 0;\n\n\tif (dpni_set_single_step_cfg(priv->mc_io, 0, priv->mc_token, &cfg))\n\t\tWARN_ONCE(1, \"Failed to set single step register\");\n}\n\nstatic void dpaa2_update_ptp_onestep_direct(struct dpaa2_eth_priv *priv,\n\t\t\t\t\t    u32 offset, u8 udp)\n{\n\tu32 val = 0;\n\n\tval = DPAA2_PTP_SINGLE_STEP_ENABLE |\n\t       DPAA2_PTP_SINGLE_CORRECTION_OFF(offset);\n\n\tif (udp)\n\t\tval |= DPAA2_PTP_SINGLE_STEP_CH;\n\n\tif (priv->onestep_reg_base)\n\t\twritel(val, priv->onestep_reg_base);\n}\n\nstatic void dpaa2_ptp_onestep_reg_update_method(struct dpaa2_eth_priv *priv)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpni_single_step_cfg ptp_cfg;\n\n\tpriv->dpaa2_set_onestep_params_cb = dpaa2_update_ptp_onestep_indirect;\n\n\tif (!(priv->features & DPAA2_ETH_FEATURE_ONESTEP_CFG_DIRECT))\n\t\treturn;\n\n\tif (dpni_get_single_step_cfg(priv->mc_io, 0,\n\t\t\t\t     priv->mc_token, &ptp_cfg)) {\n\t\tdev_err(dev, \"dpni_get_single_step_cfg cannot retrieve onestep reg, falling back to indirect update\\n\");\n\t\treturn;\n\t}\n\n\tif (!ptp_cfg.ptp_onestep_reg_base) {\n\t\tdev_err(dev, \"1588 onestep reg not available, falling back to indirect update\\n\");\n\t\treturn;\n\t}\n\n\tpriv->onestep_reg_base = ioremap(ptp_cfg.ptp_onestep_reg_base,\n\t\t\t\t\t sizeof(u32));\n\tif (!priv->onestep_reg_base) {\n\t\tdev_err(dev, \"1588 onestep reg cannot be mapped, falling back to indirect update\\n\");\n\t\treturn;\n\t}\n\n\tpriv->dpaa2_set_onestep_params_cb = dpaa2_update_ptp_onestep_direct;\n}\n\nvoid *dpaa2_iova_to_virt(struct iommu_domain *domain,\n\t\t\t dma_addr_t iova_addr)\n{\n\tphys_addr_t phys_addr;\n\n\tphys_addr = domain ? iommu_iova_to_phys(domain, iova_addr) : iova_addr;\n\n\treturn phys_to_virt(phys_addr);\n}\n\nstatic void dpaa2_eth_validate_rx_csum(struct dpaa2_eth_priv *priv,\n\t\t\t\t       u32 fd_status,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tskb_checksum_none_assert(skb);\n\n\t \n\tif (!(priv->net_dev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\t \n\tif (!((fd_status & DPAA2_FAS_L3CV) &&\n\t      (fd_status & DPAA2_FAS_L4CV)))\n\t\treturn;\n\n\t \n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n}\n\n \nstatic void dpaa2_eth_free_rx_fd(struct dpaa2_eth_priv *priv,\n\t\t\t\t const struct dpaa2_fd *fd,\n\t\t\t\t void *vaddr)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tdma_addr_t addr = dpaa2_fd_get_addr(fd);\n\tu8 fd_format = dpaa2_fd_get_format(fd);\n\tstruct dpaa2_sg_entry *sgt;\n\tvoid *sg_vaddr;\n\tint i;\n\n\t \n\tif (fd_format == dpaa2_fd_single)\n\t\tgoto free_buf;\n\telse if (fd_format != dpaa2_fd_sg)\n\t\t \n\t\treturn;\n\n\t \n\tsgt = vaddr + dpaa2_fd_get_offset(fd);\n\tfor (i = 1; i < DPAA2_ETH_MAX_SG_ENTRIES; i++) {\n\t\taddr = dpaa2_sg_get_addr(&sgt[i]);\n\t\tsg_vaddr = dpaa2_iova_to_virt(priv->iommu_domain, addr);\n\t\tdma_unmap_page(dev, addr, priv->rx_buf_size,\n\t\t\t       DMA_BIDIRECTIONAL);\n\n\t\tfree_pages((unsigned long)sg_vaddr, 0);\n\t\tif (dpaa2_sg_is_final(&sgt[i]))\n\t\t\tbreak;\n\t}\n\nfree_buf:\n\tfree_pages((unsigned long)vaddr, 0);\n}\n\n \nstatic struct sk_buff *dpaa2_eth_build_linear_skb(struct dpaa2_eth_channel *ch,\n\t\t\t\t\t\t  const struct dpaa2_fd *fd,\n\t\t\t\t\t\t  void *fd_vaddr)\n{\n\tstruct sk_buff *skb = NULL;\n\tu16 fd_offset = dpaa2_fd_get_offset(fd);\n\tu32 fd_length = dpaa2_fd_get_len(fd);\n\n\tch->buf_count--;\n\n\tskb = build_skb(fd_vaddr, DPAA2_ETH_RX_BUF_RAW_SIZE);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tskb_reserve(skb, fd_offset);\n\tskb_put(skb, fd_length);\n\n\treturn skb;\n}\n\n \nstatic struct sk_buff *dpaa2_eth_build_frag_skb(struct dpaa2_eth_priv *priv,\n\t\t\t\t\t\tstruct dpaa2_eth_channel *ch,\n\t\t\t\t\t\tstruct dpaa2_sg_entry *sgt)\n{\n\tstruct sk_buff *skb = NULL;\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tvoid *sg_vaddr;\n\tdma_addr_t sg_addr;\n\tu16 sg_offset;\n\tu32 sg_length;\n\tstruct page *page, *head_page;\n\tint page_offset;\n\tint i;\n\n\tfor (i = 0; i < DPAA2_ETH_MAX_SG_ENTRIES; i++) {\n\t\tstruct dpaa2_sg_entry *sge = &sgt[i];\n\n\t\t \n\n\t\t \n\t\tsg_addr = dpaa2_sg_get_addr(sge);\n\t\tsg_vaddr = dpaa2_iova_to_virt(priv->iommu_domain, sg_addr);\n\t\tdma_unmap_page(dev, sg_addr, priv->rx_buf_size,\n\t\t\t       DMA_BIDIRECTIONAL);\n\n\t\tsg_length = dpaa2_sg_get_len(sge);\n\n\t\tif (i == 0) {\n\t\t\t \n\t\t\tskb = build_skb(sg_vaddr, DPAA2_ETH_RX_BUF_RAW_SIZE);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\t \n\t\t\t\tfree_pages((unsigned long)sg_vaddr, 0);\n\n\t\t\t\t \n\t\t\t\twhile (!dpaa2_sg_is_final(&sgt[i]) &&\n\t\t\t\t       i < DPAA2_ETH_MAX_SG_ENTRIES)\n\t\t\t\t\ti++;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tsg_offset = dpaa2_sg_get_offset(sge);\n\t\t\tskb_reserve(skb, sg_offset);\n\t\t\tskb_put(skb, sg_length);\n\t\t} else {\n\t\t\t \n\t\t\tpage = virt_to_page(sg_vaddr);\n\t\t\thead_page = virt_to_head_page(sg_vaddr);\n\n\t\t\t \n\t\t\tpage_offset = ((unsigned long)sg_vaddr &\n\t\t\t\t(PAGE_SIZE - 1)) +\n\t\t\t\t(page_address(page) - page_address(head_page));\n\n\t\t\tskb_add_rx_frag(skb, i - 1, head_page, page_offset,\n\t\t\t\t\tsg_length, priv->rx_buf_size);\n\t\t}\n\n\t\tif (dpaa2_sg_is_final(sge))\n\t\t\tbreak;\n\t}\n\n\tWARN_ONCE(i == DPAA2_ETH_MAX_SG_ENTRIES, \"Final bit not set in SGT\");\n\n\t \n\tch->buf_count -= i + 2;\n\n\treturn skb;\n}\n\n \nstatic void dpaa2_eth_free_bufs(struct dpaa2_eth_priv *priv, u64 *buf_array,\n\t\t\t\tint count, bool xsk_zc)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpaa2_eth_swa *swa;\n\tstruct xdp_buff *xdp_buff;\n\tvoid *vaddr;\n\tint i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tvaddr = dpaa2_iova_to_virt(priv->iommu_domain, buf_array[i]);\n\n\t\tif (!xsk_zc) {\n\t\t\tdma_unmap_page(dev, buf_array[i], priv->rx_buf_size,\n\t\t\t\t       DMA_BIDIRECTIONAL);\n\t\t\tfree_pages((unsigned long)vaddr, 0);\n\t\t} else {\n\t\t\tswa = (struct dpaa2_eth_swa *)\n\t\t\t\t(vaddr + DPAA2_ETH_RX_HWA_SIZE);\n\t\t\txdp_buff = swa->xsk.xdp_buff;\n\t\t\txsk_buff_free(xdp_buff);\n\t\t}\n\t}\n}\n\nvoid dpaa2_eth_recycle_buf(struct dpaa2_eth_priv *priv,\n\t\t\t   struct dpaa2_eth_channel *ch,\n\t\t\t   dma_addr_t addr)\n{\n\tint retries = 0;\n\tint err;\n\n\tch->recycled_bufs[ch->recycled_bufs_cnt++] = addr;\n\tif (ch->recycled_bufs_cnt < DPAA2_ETH_BUFS_PER_CMD)\n\t\treturn;\n\n\twhile ((err = dpaa2_io_service_release(ch->dpio, ch->bp->bpid,\n\t\t\t\t\t       ch->recycled_bufs,\n\t\t\t\t\t       ch->recycled_bufs_cnt)) == -EBUSY) {\n\t\tif (retries++ >= DPAA2_ETH_SWP_BUSY_RETRIES)\n\t\t\tbreak;\n\t\tcpu_relax();\n\t}\n\n\tif (err) {\n\t\tdpaa2_eth_free_bufs(priv, ch->recycled_bufs,\n\t\t\t\t    ch->recycled_bufs_cnt, ch->xsk_zc);\n\t\tch->buf_count -= ch->recycled_bufs_cnt;\n\t}\n\n\tch->recycled_bufs_cnt = 0;\n}\n\nstatic int dpaa2_eth_xdp_flush(struct dpaa2_eth_priv *priv,\n\t\t\t       struct dpaa2_eth_fq *fq,\n\t\t\t       struct dpaa2_eth_xdp_fds *xdp_fds)\n{\n\tint total_enqueued = 0, retries = 0, enqueued;\n\tstruct dpaa2_eth_drv_stats *percpu_extras;\n\tint num_fds, err, max_retries;\n\tstruct dpaa2_fd *fds;\n\n\tpercpu_extras = this_cpu_ptr(priv->percpu_extras);\n\n\t \n\tfds = xdp_fds->fds;\n\tnum_fds = xdp_fds->num;\n\tmax_retries = num_fds * DPAA2_ETH_ENQUEUE_RETRIES;\n\twhile (total_enqueued < num_fds && retries < max_retries) {\n\t\terr = priv->enqueue(priv, fq, &fds[total_enqueued],\n\t\t\t\t    0, num_fds - total_enqueued, &enqueued);\n\t\tif (err == -EBUSY) {\n\t\t\tpercpu_extras->tx_portal_busy += ++retries;\n\t\t\tcontinue;\n\t\t}\n\t\ttotal_enqueued += enqueued;\n\t}\n\txdp_fds->num = 0;\n\n\treturn total_enqueued;\n}\n\nstatic void dpaa2_eth_xdp_tx_flush(struct dpaa2_eth_priv *priv,\n\t\t\t\t   struct dpaa2_eth_channel *ch,\n\t\t\t\t   struct dpaa2_eth_fq *fq)\n{\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tstruct dpaa2_fd *fds;\n\tint enqueued, i;\n\n\tpercpu_stats = this_cpu_ptr(priv->percpu_stats);\n\n\t \n\tenqueued = dpaa2_eth_xdp_flush(priv, fq, &fq->xdp_tx_fds);\n\n\t \n\tpercpu_stats->tx_packets += enqueued;\n\tfds = fq->xdp_tx_fds.fds;\n\tfor (i = 0; i < enqueued; i++) {\n\t\tpercpu_stats->tx_bytes += dpaa2_fd_get_len(&fds[i]);\n\t\tch->stats.xdp_tx++;\n\t}\n\tfor (i = enqueued; i < fq->xdp_tx_fds.num; i++) {\n\t\tdpaa2_eth_recycle_buf(priv, ch, dpaa2_fd_get_addr(&fds[i]));\n\t\tpercpu_stats->tx_errors++;\n\t\tch->stats.xdp_tx_err++;\n\t}\n\tfq->xdp_tx_fds.num = 0;\n}\n\nvoid dpaa2_eth_xdp_enqueue(struct dpaa2_eth_priv *priv,\n\t\t\t   struct dpaa2_eth_channel *ch,\n\t\t\t   struct dpaa2_fd *fd,\n\t\t\t   void *buf_start, u16 queue_id)\n{\n\tstruct dpaa2_faead *faead;\n\tstruct dpaa2_fd *dest_fd;\n\tstruct dpaa2_eth_fq *fq;\n\tu32 ctrl, frc;\n\n\t \n\tfrc = dpaa2_fd_get_frc(fd);\n\tdpaa2_fd_set_frc(fd, frc | DPAA2_FD_FRC_FAEADV);\n\tdpaa2_fd_set_ctrl(fd, DPAA2_FD_CTRL_ASAL);\n\n\t \n\tctrl = DPAA2_FAEAD_A4V | DPAA2_FAEAD_A2V | DPAA2_FAEAD_EBDDV;\n\tfaead = dpaa2_get_faead(buf_start, false);\n\tfaead->ctrl = cpu_to_le32(ctrl);\n\tfaead->conf_fqid = 0;\n\n\tfq = &priv->fq[queue_id];\n\tdest_fd = &fq->xdp_tx_fds.fds[fq->xdp_tx_fds.num++];\n\tmemcpy(dest_fd, fd, sizeof(*dest_fd));\n\n\tif (fq->xdp_tx_fds.num < DEV_MAP_BULK_SIZE)\n\t\treturn;\n\n\tdpaa2_eth_xdp_tx_flush(priv, ch, fq);\n}\n\nstatic u32 dpaa2_eth_run_xdp(struct dpaa2_eth_priv *priv,\n\t\t\t     struct dpaa2_eth_channel *ch,\n\t\t\t     struct dpaa2_eth_fq *rx_fq,\n\t\t\t     struct dpaa2_fd *fd, void *vaddr)\n{\n\tdma_addr_t addr = dpaa2_fd_get_addr(fd);\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_buff xdp;\n\tu32 xdp_act = XDP_PASS;\n\tint err, offset;\n\n\txdp_prog = READ_ONCE(ch->xdp.prog);\n\tif (!xdp_prog)\n\t\tgoto out;\n\n\toffset = dpaa2_fd_get_offset(fd) - XDP_PACKET_HEADROOM;\n\txdp_init_buff(&xdp, DPAA2_ETH_RX_BUF_RAW_SIZE - offset, &ch->xdp_rxq);\n\txdp_prepare_buff(&xdp, vaddr + offset, XDP_PACKET_HEADROOM,\n\t\t\t dpaa2_fd_get_len(fd), false);\n\n\txdp_act = bpf_prog_run_xdp(xdp_prog, &xdp);\n\n\t \n\tdpaa2_fd_set_offset(fd, xdp.data - vaddr);\n\tdpaa2_fd_set_len(fd, xdp.data_end - xdp.data);\n\n\tswitch (xdp_act) {\n\tcase XDP_PASS:\n\t\tbreak;\n\tcase XDP_TX:\n\t\tdpaa2_eth_xdp_enqueue(priv, ch, fd, vaddr, rx_fq->flowid);\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(priv->net_dev, xdp_prog, xdp_act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(priv->net_dev, xdp_prog, xdp_act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tdpaa2_eth_recycle_buf(priv, ch, addr);\n\t\tch->stats.xdp_drop++;\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\tdma_unmap_page(priv->net_dev->dev.parent, addr,\n\t\t\t       priv->rx_buf_size, DMA_BIDIRECTIONAL);\n\t\tch->buf_count--;\n\n\t\t \n\t\txdp.data_hard_start = vaddr;\n\t\txdp.frame_sz = DPAA2_ETH_RX_BUF_RAW_SIZE;\n\n\t\terr = xdp_do_redirect(priv->net_dev, &xdp, xdp_prog);\n\t\tif (unlikely(err)) {\n\t\t\taddr = dma_map_page(priv->net_dev->dev.parent,\n\t\t\t\t\t    virt_to_page(vaddr), 0,\n\t\t\t\t\t    priv->rx_buf_size, DMA_BIDIRECTIONAL);\n\t\t\tif (unlikely(dma_mapping_error(priv->net_dev->dev.parent, addr))) {\n\t\t\t\tfree_pages((unsigned long)vaddr, 0);\n\t\t\t} else {\n\t\t\t\tch->buf_count++;\n\t\t\t\tdpaa2_eth_recycle_buf(priv, ch, addr);\n\t\t\t}\n\t\t\tch->stats.xdp_drop++;\n\t\t} else {\n\t\t\tch->stats.xdp_redirect++;\n\t\t}\n\t\tbreak;\n\t}\n\n\tch->xdp.res |= xdp_act;\nout:\n\treturn xdp_act;\n}\n\nstruct sk_buff *dpaa2_eth_alloc_skb(struct dpaa2_eth_priv *priv,\n\t\t\t\t    struct dpaa2_eth_channel *ch,\n\t\t\t\t    const struct dpaa2_fd *fd, u32 fd_length,\n\t\t\t\t    void *fd_vaddr)\n{\n\tu16 fd_offset = dpaa2_fd_get_offset(fd);\n\tstruct sk_buff *skb = NULL;\n\tunsigned int skb_len;\n\n\tskb_len = fd_length + dpaa2_eth_needed_headroom(NULL);\n\n\tskb = napi_alloc_skb(&ch->napi, skb_len);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, dpaa2_eth_needed_headroom(NULL));\n\tskb_put(skb, fd_length);\n\n\tmemcpy(skb->data, fd_vaddr + fd_offset, fd_length);\n\n\treturn skb;\n}\n\nstatic struct sk_buff *dpaa2_eth_copybreak(struct dpaa2_eth_channel *ch,\n\t\t\t\t\t   const struct dpaa2_fd *fd,\n\t\t\t\t\t   void *fd_vaddr)\n{\n\tstruct dpaa2_eth_priv *priv = ch->priv;\n\tu32 fd_length = dpaa2_fd_get_len(fd);\n\n\tif (fd_length > priv->rx_copybreak)\n\t\treturn NULL;\n\n\treturn dpaa2_eth_alloc_skb(priv, ch, fd, fd_length, fd_vaddr);\n}\n\nvoid dpaa2_eth_receive_skb(struct dpaa2_eth_priv *priv,\n\t\t\t   struct dpaa2_eth_channel *ch,\n\t\t\t   const struct dpaa2_fd *fd, void *vaddr,\n\t\t\t   struct dpaa2_eth_fq *fq,\n\t\t\t   struct rtnl_link_stats64 *percpu_stats,\n\t\t\t   struct sk_buff *skb)\n{\n\tstruct dpaa2_fas *fas;\n\tu32 status = 0;\n\n\tfas = dpaa2_get_fas(vaddr, false);\n\tprefetch(fas);\n\tprefetch(skb->data);\n\n\t \n\tif (priv->rx_tstamp) {\n\t\tstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\n\t\t__le64 *ts = dpaa2_get_ts(vaddr, false);\n\t\tu64 ns;\n\n\t\tmemset(shhwtstamps, 0, sizeof(*shhwtstamps));\n\n\t\tns = DPAA2_PTP_CLK_PERIOD_NS * le64_to_cpup(ts);\n\t\tshhwtstamps->hwtstamp = ns_to_ktime(ns);\n\t}\n\n\t \n\tif (likely(dpaa2_fd_get_frc(fd) & DPAA2_FD_FRC_FASV)) {\n\t\tstatus = le32_to_cpu(fas->status);\n\t\tdpaa2_eth_validate_rx_csum(priv, status, skb);\n\t}\n\n\tskb->protocol = eth_type_trans(skb, priv->net_dev);\n\tskb_record_rx_queue(skb, fq->flowid);\n\n\tpercpu_stats->rx_packets++;\n\tpercpu_stats->rx_bytes += dpaa2_fd_get_len(fd);\n\tch->stats.bytes_per_cdan += dpaa2_fd_get_len(fd);\n\n\tlist_add_tail(&skb->list, ch->rx_list);\n}\n\n \nvoid dpaa2_eth_rx(struct dpaa2_eth_priv *priv,\n\t\t  struct dpaa2_eth_channel *ch,\n\t\t  const struct dpaa2_fd *fd,\n\t\t  struct dpaa2_eth_fq *fq)\n{\n\tdma_addr_t addr = dpaa2_fd_get_addr(fd);\n\tu8 fd_format = dpaa2_fd_get_format(fd);\n\tvoid *vaddr;\n\tstruct sk_buff *skb;\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tstruct dpaa2_eth_drv_stats *percpu_extras;\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tbool recycle_rx_buf = false;\n\tvoid *buf_data;\n\tu32 xdp_act;\n\n\t \n\ttrace_dpaa2_rx_fd(priv->net_dev, fd);\n\n\tvaddr = dpaa2_iova_to_virt(priv->iommu_domain, addr);\n\tdma_sync_single_for_cpu(dev, addr, priv->rx_buf_size,\n\t\t\t\tDMA_BIDIRECTIONAL);\n\n\tbuf_data = vaddr + dpaa2_fd_get_offset(fd);\n\tprefetch(buf_data);\n\n\tpercpu_stats = this_cpu_ptr(priv->percpu_stats);\n\tpercpu_extras = this_cpu_ptr(priv->percpu_extras);\n\n\tif (fd_format == dpaa2_fd_single) {\n\t\txdp_act = dpaa2_eth_run_xdp(priv, ch, fq, (struct dpaa2_fd *)fd, vaddr);\n\t\tif (xdp_act != XDP_PASS) {\n\t\t\tpercpu_stats->rx_packets++;\n\t\t\tpercpu_stats->rx_bytes += dpaa2_fd_get_len(fd);\n\t\t\treturn;\n\t\t}\n\n\t\tskb = dpaa2_eth_copybreak(ch, fd, vaddr);\n\t\tif (!skb) {\n\t\t\tdma_unmap_page(dev, addr, priv->rx_buf_size,\n\t\t\t\t       DMA_BIDIRECTIONAL);\n\t\t\tskb = dpaa2_eth_build_linear_skb(ch, fd, vaddr);\n\t\t} else {\n\t\t\trecycle_rx_buf = true;\n\t\t}\n\t} else if (fd_format == dpaa2_fd_sg) {\n\t\tWARN_ON(priv->xdp_prog);\n\n\t\tdma_unmap_page(dev, addr, priv->rx_buf_size,\n\t\t\t       DMA_BIDIRECTIONAL);\n\t\tskb = dpaa2_eth_build_frag_skb(priv, ch, buf_data);\n\t\tfree_pages((unsigned long)vaddr, 0);\n\t\tpercpu_extras->rx_sg_frames++;\n\t\tpercpu_extras->rx_sg_bytes += dpaa2_fd_get_len(fd);\n\t} else {\n\t\t \n\t\tgoto err_frame_format;\n\t}\n\n\tif (unlikely(!skb))\n\t\tgoto err_build_skb;\n\n\tdpaa2_eth_receive_skb(priv, ch, fd, vaddr, fq, percpu_stats, skb);\n\n\tif (recycle_rx_buf)\n\t\tdpaa2_eth_recycle_buf(priv, ch, dpaa2_fd_get_addr(fd));\n\treturn;\n\nerr_build_skb:\n\tdpaa2_eth_free_rx_fd(priv, fd, vaddr);\nerr_frame_format:\n\tpercpu_stats->rx_dropped++;\n}\n\n \nstatic void dpaa2_eth_rx_err(struct dpaa2_eth_priv *priv,\n\t\t\t     struct dpaa2_eth_channel *ch,\n\t\t\t     const struct dpaa2_fd *fd,\n\t\t\t     struct dpaa2_eth_fq *fq __always_unused)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tdma_addr_t addr = dpaa2_fd_get_addr(fd);\n\tu8 fd_format = dpaa2_fd_get_format(fd);\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tstruct dpaa2_eth_trap_item *trap_item;\n\tstruct dpaa2_fapr *fapr;\n\tstruct sk_buff *skb;\n\tvoid *buf_data;\n\tvoid *vaddr;\n\n\tvaddr = dpaa2_iova_to_virt(priv->iommu_domain, addr);\n\tdma_sync_single_for_cpu(dev, addr, priv->rx_buf_size,\n\t\t\t\tDMA_BIDIRECTIONAL);\n\n\tbuf_data = vaddr + dpaa2_fd_get_offset(fd);\n\n\tif (fd_format == dpaa2_fd_single) {\n\t\tdma_unmap_page(dev, addr, priv->rx_buf_size,\n\t\t\t       DMA_BIDIRECTIONAL);\n\t\tskb = dpaa2_eth_build_linear_skb(ch, fd, vaddr);\n\t} else if (fd_format == dpaa2_fd_sg) {\n\t\tdma_unmap_page(dev, addr, priv->rx_buf_size,\n\t\t\t       DMA_BIDIRECTIONAL);\n\t\tskb = dpaa2_eth_build_frag_skb(priv, ch, buf_data);\n\t\tfree_pages((unsigned long)vaddr, 0);\n\t} else {\n\t\t \n\t\tdpaa2_eth_free_rx_fd(priv, fd, vaddr);\n\t\tgoto err_frame_format;\n\t}\n\n\tfapr = dpaa2_get_fapr(vaddr, false);\n\ttrap_item = dpaa2_eth_dl_get_trap(priv, fapr);\n\tif (trap_item)\n\t\tdevlink_trap_report(priv->devlink, skb, trap_item->trap_ctx,\n\t\t\t\t    &priv->devlink_port, NULL);\n\tconsume_skb(skb);\n\nerr_frame_format:\n\tpercpu_stats = this_cpu_ptr(priv->percpu_stats);\n\tpercpu_stats->rx_errors++;\n\tch->buf_count--;\n}\n\n \nstatic int dpaa2_eth_consume_frames(struct dpaa2_eth_channel *ch,\n\t\t\t\t    struct dpaa2_eth_fq **src)\n{\n\tstruct dpaa2_eth_priv *priv = ch->priv;\n\tstruct dpaa2_eth_fq *fq = NULL;\n\tstruct dpaa2_dq *dq;\n\tconst struct dpaa2_fd *fd;\n\tint cleaned = 0, retries = 0;\n\tint is_last;\n\n\tdo {\n\t\tdq = dpaa2_io_store_next(ch->store, &is_last);\n\t\tif (unlikely(!dq)) {\n\t\t\t \n\t\t\tif (retries++ >= DPAA2_ETH_SWP_BUSY_RETRIES) {\n\t\t\t\tnetdev_err_once(priv->net_dev,\n\t\t\t\t\t\t\"Unable to read a valid dequeue response\\n\");\n\t\t\t\treturn -ETIMEDOUT;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tfd = dpaa2_dq_fd(dq);\n\t\tfq = (struct dpaa2_eth_fq *)(uintptr_t)dpaa2_dq_fqd_ctx(dq);\n\n\t\tfq->consume(priv, ch, fd, fq);\n\t\tcleaned++;\n\t\tretries = 0;\n\t} while (!is_last);\n\n\tif (!cleaned)\n\t\treturn 0;\n\n\tfq->stats.frames += cleaned;\n\tch->stats.frames += cleaned;\n\tch->stats.frames_per_cdan += cleaned;\n\n\t \n\tif (src)\n\t\t*src = fq;\n\n\treturn cleaned;\n}\n\nstatic int dpaa2_eth_ptp_parse(struct sk_buff *skb,\n\t\t\t       u8 *msgtype, u8 *twostep, u8 *udp,\n\t\t\t       u16 *correction_offset,\n\t\t\t       u16 *origintimestamp_offset)\n{\n\tunsigned int ptp_class;\n\tstruct ptp_header *hdr;\n\tunsigned int type;\n\tu8 *base;\n\n\tptp_class = ptp_classify_raw(skb);\n\tif (ptp_class == PTP_CLASS_NONE)\n\t\treturn -EINVAL;\n\n\thdr = ptp_parse_header(skb, ptp_class);\n\tif (!hdr)\n\t\treturn -EINVAL;\n\n\t*msgtype = ptp_get_msgtype(hdr, ptp_class);\n\t*twostep = hdr->flag_field[0] & 0x2;\n\n\ttype = ptp_class & PTP_CLASS_PMASK;\n\tif (type == PTP_CLASS_IPV4 ||\n\t    type == PTP_CLASS_IPV6)\n\t\t*udp = 1;\n\telse\n\t\t*udp = 0;\n\n\tbase = skb_mac_header(skb);\n\t*correction_offset = (u8 *)&hdr->correction - base;\n\t*origintimestamp_offset = (u8 *)hdr + sizeof(struct ptp_header) - base;\n\n\treturn 0;\n}\n\n \nstatic void dpaa2_eth_enable_tx_tstamp(struct dpaa2_eth_priv *priv,\n\t\t\t\t       struct dpaa2_fd *fd,\n\t\t\t\t       void *buf_start,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tstruct ptp_tstamp origin_timestamp;\n\tu8 msgtype, twostep, udp;\n\tstruct dpaa2_faead *faead;\n\tstruct dpaa2_fas *fas;\n\tstruct timespec64 ts;\n\tu16 offset1, offset2;\n\tu32 ctrl, frc;\n\t__le64 *ns;\n\tu8 *data;\n\n\t \n\tfrc = dpaa2_fd_get_frc(fd);\n\tdpaa2_fd_set_frc(fd, frc | DPAA2_FD_FRC_FAEADV);\n\n\t \n\tctrl = dpaa2_fd_get_ctrl(fd);\n\tdpaa2_fd_set_ctrl(fd, ctrl | DPAA2_FD_CTRL_ASAL);\n\n\t \n\tctrl = DPAA2_FAEAD_A2V | DPAA2_FAEAD_UPDV | DPAA2_FAEAD_UPD;\n\tfaead = dpaa2_get_faead(buf_start, true);\n\tfaead->ctrl = cpu_to_le32(ctrl);\n\n\tif (skb->cb[0] == TX_TSTAMP_ONESTEP_SYNC) {\n\t\tif (dpaa2_eth_ptp_parse(skb, &msgtype, &twostep, &udp,\n\t\t\t\t\t&offset1, &offset2) ||\n\t\t    msgtype != PTP_MSGTYPE_SYNC || twostep) {\n\t\t\tWARN_ONCE(1, \"Bad packet for one-step timestamping\\n\");\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tfrc = dpaa2_fd_get_frc(fd);\n\t\tdpaa2_fd_set_frc(fd, frc | DPAA2_FD_FRC_FASV);\n\n\t\t \n\t\tfas = dpaa2_get_fas(buf_start, true);\n\t\tfas->status = cpu_to_le32(DPAA2_FAS_PTP);\n\n\t\tdpaa2_ptp->caps.gettime64(&dpaa2_ptp->caps, &ts);\n\t\tns = dpaa2_get_ts(buf_start, true);\n\t\t*ns = cpu_to_le64(timespec64_to_ns(&ts) /\n\t\t\t\t  DPAA2_PTP_CLK_PERIOD_NS);\n\n\t\t \n\t\tns_to_ptp_tstamp(&origin_timestamp, le64_to_cpup(ns));\n\t\tdata = skb_mac_header(skb);\n\t\t*(__be16 *)(data + offset2) = htons(origin_timestamp.sec_msb);\n\t\t*(__be32 *)(data + offset2 + 2) =\n\t\t\thtonl(origin_timestamp.sec_lsb);\n\t\t*(__be32 *)(data + offset2 + 6) = htonl(origin_timestamp.nsec);\n\n\t\tif (priv->ptp_correction_off == offset1)\n\t\t\treturn;\n\n\t\tpriv->dpaa2_set_onestep_params_cb(priv, offset1, udp);\n\t\tpriv->ptp_correction_off = offset1;\n\n\t}\n}\n\nvoid *dpaa2_eth_sgt_get(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_eth_sgt_cache *sgt_cache;\n\tvoid *sgt_buf = NULL;\n\tint sgt_buf_size;\n\n\tsgt_cache = this_cpu_ptr(priv->sgt_cache);\n\tsgt_buf_size = priv->tx_data_offset +\n\t\tDPAA2_ETH_SG_ENTRIES_MAX * sizeof(struct dpaa2_sg_entry);\n\n\tif (sgt_cache->count == 0)\n\t\tsgt_buf = napi_alloc_frag_align(sgt_buf_size, DPAA2_ETH_TX_BUF_ALIGN);\n\telse\n\t\tsgt_buf = sgt_cache->buf[--sgt_cache->count];\n\tif (!sgt_buf)\n\t\treturn NULL;\n\n\tmemset(sgt_buf, 0, sgt_buf_size);\n\n\treturn sgt_buf;\n}\n\nvoid dpaa2_eth_sgt_recycle(struct dpaa2_eth_priv *priv, void *sgt_buf)\n{\n\tstruct dpaa2_eth_sgt_cache *sgt_cache;\n\n\tsgt_cache = this_cpu_ptr(priv->sgt_cache);\n\tif (sgt_cache->count >= DPAA2_ETH_SGT_CACHE_SIZE)\n\t\tskb_free_frag(sgt_buf);\n\telse\n\t\tsgt_cache->buf[sgt_cache->count++] = sgt_buf;\n}\n\n \nstatic int dpaa2_eth_build_sg_fd(struct dpaa2_eth_priv *priv,\n\t\t\t\t struct sk_buff *skb,\n\t\t\t\t struct dpaa2_fd *fd,\n\t\t\t\t void **swa_addr)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tvoid *sgt_buf = NULL;\n\tdma_addr_t addr;\n\tint nr_frags = skb_shinfo(skb)->nr_frags;\n\tstruct dpaa2_sg_entry *sgt;\n\tint i, err;\n\tint sgt_buf_size;\n\tstruct scatterlist *scl, *crt_scl;\n\tint num_sg;\n\tint num_dma_bufs;\n\tstruct dpaa2_eth_swa *swa;\n\n\t \n\tif (unlikely(PAGE_SIZE / sizeof(struct scatterlist) < nr_frags + 1))\n\t\treturn -EINVAL;\n\n\tscl = kmalloc_array(nr_frags + 1, sizeof(struct scatterlist), GFP_ATOMIC);\n\tif (unlikely(!scl))\n\t\treturn -ENOMEM;\n\n\tsg_init_table(scl, nr_frags + 1);\n\tnum_sg = skb_to_sgvec(skb, scl, 0, skb->len);\n\tif (unlikely(num_sg < 0)) {\n\t\terr = -ENOMEM;\n\t\tgoto dma_map_sg_failed;\n\t}\n\tnum_dma_bufs = dma_map_sg(dev, scl, num_sg, DMA_BIDIRECTIONAL);\n\tif (unlikely(!num_dma_bufs)) {\n\t\terr = -ENOMEM;\n\t\tgoto dma_map_sg_failed;\n\t}\n\n\t \n\tsgt_buf_size = priv->tx_data_offset +\n\t\t       sizeof(struct dpaa2_sg_entry) *  num_dma_bufs;\n\tsgt_buf = dpaa2_eth_sgt_get(priv);\n\tif (unlikely(!sgt_buf)) {\n\t\terr = -ENOMEM;\n\t\tgoto sgt_buf_alloc_failed;\n\t}\n\n\tsgt = (struct dpaa2_sg_entry *)(sgt_buf + priv->tx_data_offset);\n\n\t \n\tfor_each_sg(scl, crt_scl, num_dma_bufs, i) {\n\t\tdpaa2_sg_set_addr(&sgt[i], sg_dma_address(crt_scl));\n\t\tdpaa2_sg_set_len(&sgt[i], sg_dma_len(crt_scl));\n\t}\n\tdpaa2_sg_set_final(&sgt[i - 1], true);\n\n\t \n\t*swa_addr = (void *)sgt_buf;\n\tswa = (struct dpaa2_eth_swa *)sgt_buf;\n\tswa->type = DPAA2_ETH_SWA_SG;\n\tswa->sg.skb = skb;\n\tswa->sg.scl = scl;\n\tswa->sg.num_sg = num_sg;\n\tswa->sg.sgt_size = sgt_buf_size;\n\n\t \n\taddr = dma_map_single(dev, sgt_buf, sgt_buf_size, DMA_BIDIRECTIONAL);\n\tif (unlikely(dma_mapping_error(dev, addr))) {\n\t\terr = -ENOMEM;\n\t\tgoto dma_map_single_failed;\n\t}\n\tmemset(fd, 0, sizeof(struct dpaa2_fd));\n\tdpaa2_fd_set_offset(fd, priv->tx_data_offset);\n\tdpaa2_fd_set_format(fd, dpaa2_fd_sg);\n\tdpaa2_fd_set_addr(fd, addr);\n\tdpaa2_fd_set_len(fd, skb->len);\n\tdpaa2_fd_set_ctrl(fd, FD_CTRL_PTA);\n\n\treturn 0;\n\ndma_map_single_failed:\n\tdpaa2_eth_sgt_recycle(priv, sgt_buf);\nsgt_buf_alloc_failed:\n\tdma_unmap_sg(dev, scl, num_sg, DMA_BIDIRECTIONAL);\ndma_map_sg_failed:\n\tkfree(scl);\n\treturn err;\n}\n\n \nstatic int dpaa2_eth_build_sg_fd_single_buf(struct dpaa2_eth_priv *priv,\n\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t    struct dpaa2_fd *fd,\n\t\t\t\t\t    void **swa_addr)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpaa2_sg_entry *sgt;\n\tstruct dpaa2_eth_swa *swa;\n\tdma_addr_t addr, sgt_addr;\n\tvoid *sgt_buf = NULL;\n\tint sgt_buf_size;\n\tint err;\n\n\t \n\tsgt_buf_size = priv->tx_data_offset + sizeof(struct dpaa2_sg_entry);\n\tsgt_buf = dpaa2_eth_sgt_get(priv);\n\tif (unlikely(!sgt_buf))\n\t\treturn -ENOMEM;\n\tsgt = (struct dpaa2_sg_entry *)(sgt_buf + priv->tx_data_offset);\n\n\taddr = dma_map_single(dev, skb->data, skb->len, DMA_BIDIRECTIONAL);\n\tif (unlikely(dma_mapping_error(dev, addr))) {\n\t\terr = -ENOMEM;\n\t\tgoto data_map_failed;\n\t}\n\n\t \n\tdpaa2_sg_set_addr(sgt, addr);\n\tdpaa2_sg_set_len(sgt, skb->len);\n\tdpaa2_sg_set_final(sgt, true);\n\n\t \n\t*swa_addr = (void *)sgt_buf;\n\tswa = (struct dpaa2_eth_swa *)sgt_buf;\n\tswa->type = DPAA2_ETH_SWA_SINGLE;\n\tswa->single.skb = skb;\n\tswa->single.sgt_size = sgt_buf_size;\n\n\t \n\tsgt_addr = dma_map_single(dev, sgt_buf, sgt_buf_size, DMA_BIDIRECTIONAL);\n\tif (unlikely(dma_mapping_error(dev, sgt_addr))) {\n\t\terr = -ENOMEM;\n\t\tgoto sgt_map_failed;\n\t}\n\n\tmemset(fd, 0, sizeof(struct dpaa2_fd));\n\tdpaa2_fd_set_offset(fd, priv->tx_data_offset);\n\tdpaa2_fd_set_format(fd, dpaa2_fd_sg);\n\tdpaa2_fd_set_addr(fd, sgt_addr);\n\tdpaa2_fd_set_len(fd, skb->len);\n\tdpaa2_fd_set_ctrl(fd, FD_CTRL_PTA);\n\n\treturn 0;\n\nsgt_map_failed:\n\tdma_unmap_single(dev, addr, skb->len, DMA_BIDIRECTIONAL);\ndata_map_failed:\n\tdpaa2_eth_sgt_recycle(priv, sgt_buf);\n\n\treturn err;\n}\n\n \nstatic int dpaa2_eth_build_single_fd(struct dpaa2_eth_priv *priv,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     struct dpaa2_fd *fd,\n\t\t\t\t     void **swa_addr)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tu8 *buffer_start, *aligned_start;\n\tstruct dpaa2_eth_swa *swa;\n\tdma_addr_t addr;\n\n\tbuffer_start = skb->data - dpaa2_eth_needed_headroom(skb);\n\taligned_start = PTR_ALIGN(buffer_start - DPAA2_ETH_TX_BUF_ALIGN,\n\t\t\t\t  DPAA2_ETH_TX_BUF_ALIGN);\n\tif (aligned_start >= skb->head)\n\t\tbuffer_start = aligned_start;\n\telse\n\t\treturn -ENOMEM;\n\n\t \n\t*swa_addr = (void *)buffer_start;\n\tswa = (struct dpaa2_eth_swa *)buffer_start;\n\tswa->type = DPAA2_ETH_SWA_SINGLE;\n\tswa->single.skb = skb;\n\n\taddr = dma_map_single(dev, buffer_start,\n\t\t\t      skb_tail_pointer(skb) - buffer_start,\n\t\t\t      DMA_BIDIRECTIONAL);\n\tif (unlikely(dma_mapping_error(dev, addr)))\n\t\treturn -ENOMEM;\n\n\tmemset(fd, 0, sizeof(struct dpaa2_fd));\n\tdpaa2_fd_set_addr(fd, addr);\n\tdpaa2_fd_set_offset(fd, (u16)(skb->data - buffer_start));\n\tdpaa2_fd_set_len(fd, skb->len);\n\tdpaa2_fd_set_format(fd, dpaa2_fd_single);\n\tdpaa2_fd_set_ctrl(fd, FD_CTRL_PTA);\n\n\treturn 0;\n}\n\n \nvoid dpaa2_eth_free_tx_fd(struct dpaa2_eth_priv *priv,\n\t\t\t  struct dpaa2_eth_channel *ch,\n\t\t\t  struct dpaa2_eth_fq *fq,\n\t\t\t  const struct dpaa2_fd *fd, bool in_napi)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tdma_addr_t fd_addr, sg_addr;\n\tstruct sk_buff *skb = NULL;\n\tunsigned char *buffer_start;\n\tstruct dpaa2_eth_swa *swa;\n\tu8 fd_format = dpaa2_fd_get_format(fd);\n\tu32 fd_len = dpaa2_fd_get_len(fd);\n\tstruct dpaa2_sg_entry *sgt;\n\tint should_free_skb = 1;\n\tvoid *tso_hdr;\n\tint i;\n\n\tfd_addr = dpaa2_fd_get_addr(fd);\n\tbuffer_start = dpaa2_iova_to_virt(priv->iommu_domain, fd_addr);\n\tswa = (struct dpaa2_eth_swa *)buffer_start;\n\n\tif (fd_format == dpaa2_fd_single) {\n\t\tif (swa->type == DPAA2_ETH_SWA_SINGLE) {\n\t\t\tskb = swa->single.skb;\n\t\t\t \n\t\t\tdma_unmap_single(dev, fd_addr,\n\t\t\t\t\t skb_tail_pointer(skb) - buffer_start,\n\t\t\t\t\t DMA_BIDIRECTIONAL);\n\t\t} else {\n\t\t\tWARN_ONCE(swa->type != DPAA2_ETH_SWA_XDP, \"Wrong SWA type\");\n\t\t\tdma_unmap_single(dev, fd_addr, swa->xdp.dma_size,\n\t\t\t\t\t DMA_BIDIRECTIONAL);\n\t\t}\n\t} else if (fd_format == dpaa2_fd_sg) {\n\t\tif (swa->type == DPAA2_ETH_SWA_SG) {\n\t\t\tskb = swa->sg.skb;\n\n\t\t\t \n\t\t\tdma_unmap_sg(dev, swa->sg.scl, swa->sg.num_sg,\n\t\t\t\t     DMA_BIDIRECTIONAL);\n\t\t\tkfree(swa->sg.scl);\n\n\t\t\t \n\t\t\tdma_unmap_single(dev, fd_addr, swa->sg.sgt_size,\n\t\t\t\t\t DMA_BIDIRECTIONAL);\n\t\t} else if (swa->type == DPAA2_ETH_SWA_SW_TSO) {\n\t\t\tskb = swa->tso.skb;\n\n\t\t\tsgt = (struct dpaa2_sg_entry *)(buffer_start +\n\t\t\t\t\t\t\tpriv->tx_data_offset);\n\n\t\t\t \n\t\t\tdma_unmap_single(dev, fd_addr, swa->tso.sgt_size,\n\t\t\t\t\t DMA_BIDIRECTIONAL);\n\n\t\t\t \n\t\t\ttso_hdr = dpaa2_iova_to_virt(priv->iommu_domain, dpaa2_sg_get_addr(sgt));\n\t\t\tdma_unmap_single(dev, dpaa2_sg_get_addr(sgt), TSO_HEADER_SIZE,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\tkfree(tso_hdr);\n\n\t\t\t \n\t\t\tfor (i = 1; i < swa->tso.num_sg; i++)\n\t\t\t\tdma_unmap_single(dev, dpaa2_sg_get_addr(&sgt[i]),\n\t\t\t\t\t\t dpaa2_sg_get_len(&sgt[i]), DMA_TO_DEVICE);\n\n\t\t\tif (!swa->tso.is_last_fd)\n\t\t\t\tshould_free_skb = 0;\n\t\t} else if (swa->type == DPAA2_ETH_SWA_XSK) {\n\t\t\t \n\t\t\tdma_unmap_single(dev, fd_addr, swa->xsk.sgt_size,\n\t\t\t\t\t DMA_BIDIRECTIONAL);\n\t\t} else {\n\t\t\tskb = swa->single.skb;\n\n\t\t\t \n\t\t\tdma_unmap_single(dev, fd_addr, swa->single.sgt_size,\n\t\t\t\t\t DMA_BIDIRECTIONAL);\n\n\t\t\tsgt = (struct dpaa2_sg_entry *)(buffer_start +\n\t\t\t\t\t\t\tpriv->tx_data_offset);\n\t\t\tsg_addr = dpaa2_sg_get_addr(sgt);\n\t\t\tdma_unmap_single(dev, sg_addr, skb->len, DMA_BIDIRECTIONAL);\n\t\t}\n\t} else {\n\t\tnetdev_dbg(priv->net_dev, \"Invalid FD format\\n\");\n\t\treturn;\n\t}\n\n\tif (swa->type == DPAA2_ETH_SWA_XSK) {\n\t\tch->xsk_tx_pkts_sent++;\n\t\tdpaa2_eth_sgt_recycle(priv, buffer_start);\n\t\treturn;\n\t}\n\n\tif (swa->type != DPAA2_ETH_SWA_XDP && in_napi) {\n\t\tfq->dq_frames++;\n\t\tfq->dq_bytes += fd_len;\n\t}\n\n\tif (swa->type == DPAA2_ETH_SWA_XDP) {\n\t\txdp_return_frame(swa->xdp.xdpf);\n\t\treturn;\n\t}\n\n\t \n\tif (swa->type != DPAA2_ETH_SWA_SW_TSO) {\n\t\tif (skb->cb[0] == TX_TSTAMP) {\n\t\t\tstruct skb_shared_hwtstamps shhwtstamps;\n\t\t\t__le64 *ts = dpaa2_get_ts(buffer_start, true);\n\t\t\tu64 ns;\n\n\t\t\tmemset(&shhwtstamps, 0, sizeof(shhwtstamps));\n\n\t\t\tns = DPAA2_PTP_CLK_PERIOD_NS * le64_to_cpup(ts);\n\t\t\tshhwtstamps.hwtstamp = ns_to_ktime(ns);\n\t\t\tskb_tstamp_tx(skb, &shhwtstamps);\n\t\t} else if (skb->cb[0] == TX_TSTAMP_ONESTEP_SYNC) {\n\t\t\tmutex_unlock(&priv->onestep_tstamp_lock);\n\t\t}\n\t}\n\n\t \n\tif (fd_format != dpaa2_fd_single)\n\t\tdpaa2_eth_sgt_recycle(priv, buffer_start);\n\n\t \n\tif (should_free_skb)\n\t\tnapi_consume_skb(skb, in_napi);\n}\n\nstatic int dpaa2_eth_build_gso_fd(struct dpaa2_eth_priv *priv,\n\t\t\t\t  struct sk_buff *skb, struct dpaa2_fd *fd,\n\t\t\t\t  int *num_fds, u32 *total_fds_len)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tint hdr_len, total_len, data_left, fd_len;\n\tint num_sge, err, i, sgt_buf_size;\n\tstruct dpaa2_fd *fd_start = fd;\n\tstruct dpaa2_sg_entry *sgt;\n\tstruct dpaa2_eth_swa *swa;\n\tdma_addr_t sgt_addr, addr;\n\tdma_addr_t tso_hdr_dma;\n\tunsigned int index = 0;\n\tstruct tso_t tso;\n\tchar *tso_hdr;\n\tvoid *sgt_buf;\n\n\t \n\thdr_len = tso_start(skb, &tso);\n\t*total_fds_len = 0;\n\n\ttotal_len = skb->len - hdr_len;\n\twhile (total_len > 0) {\n\t\t \n\t\tsgt_buf = dpaa2_eth_sgt_get(priv);\n\t\tif (unlikely(!sgt_buf)) {\n\t\t\tnetdev_err(priv->net_dev, \"dpaa2_eth_sgt_get() failed\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_sgt_get;\n\t\t}\n\t\tsgt = (struct dpaa2_sg_entry *)(sgt_buf + priv->tx_data_offset);\n\n\t\t \n\t\tdata_left = min_t(int, skb_shinfo(skb)->gso_size, total_len);\n\t\ttotal_len -= data_left;\n\t\tfd_len = data_left + hdr_len;\n\n\t\t \n\t\ttso_hdr = kmalloc(TSO_HEADER_SIZE, GFP_ATOMIC);\n\t\tif (!tso_hdr) {\n\t\t\terr =  -ENOMEM;\n\t\t\tgoto err_alloc_tso_hdr;\n\t\t}\n\n\t\ttso_build_hdr(skb, tso_hdr, &tso, data_left, total_len == 0);\n\t\ttso_hdr_dma = dma_map_single(dev, tso_hdr, TSO_HEADER_SIZE, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, tso_hdr_dma)) {\n\t\t\tnetdev_err(priv->net_dev, \"dma_map_single(tso_hdr) failed\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_map_tso_hdr;\n\t\t}\n\n\t\t \n\t\tdpaa2_sg_set_addr(sgt, tso_hdr_dma);\n\t\tdpaa2_sg_set_len(sgt, hdr_len);\n\t\tdpaa2_sg_set_final(sgt, data_left <= 0);\n\n\t\t \n\t\tnum_sge = 1;\n\t\twhile (data_left > 0) {\n\t\t\tint size;\n\n\t\t\t \n\t\t\tsgt++;\n\t\t\tsize = min_t(int, tso.size, data_left);\n\n\t\t\taddr = dma_map_single(dev, tso.data, size, DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(dev, addr)) {\n\t\t\t\tnetdev_err(priv->net_dev, \"dma_map_single(tso.data) failed\\n\");\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto err_map_data;\n\t\t\t}\n\t\t\tdpaa2_sg_set_addr(sgt, addr);\n\t\t\tdpaa2_sg_set_len(sgt, size);\n\t\t\tdpaa2_sg_set_final(sgt, size == data_left);\n\n\t\t\tnum_sge++;\n\n\t\t\t \n\t\t\tdata_left -= size;\n\t\t\ttso_build_data(skb, &tso, size);\n\t\t}\n\n\t\t \n\t\tsgt_buf_size = priv->tx_data_offset + num_sge * sizeof(struct dpaa2_sg_entry);\n\t\tswa = (struct dpaa2_eth_swa *)sgt_buf;\n\t\tswa->type = DPAA2_ETH_SWA_SW_TSO;\n\t\tswa->tso.skb = skb;\n\t\tswa->tso.num_sg = num_sge;\n\t\tswa->tso.sgt_size = sgt_buf_size;\n\t\tswa->tso.is_last_fd = total_len == 0 ? 1 : 0;\n\n\t\t \n\t\tsgt_addr = dma_map_single(dev, sgt_buf, sgt_buf_size, DMA_BIDIRECTIONAL);\n\t\tif (unlikely(dma_mapping_error(dev, sgt_addr))) {\n\t\t\tnetdev_err(priv->net_dev, \"dma_map_single(sgt_buf) failed\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_map_sgt;\n\t\t}\n\n\t\t \n\t\tmemset(fd, 0, sizeof(struct dpaa2_fd));\n\t\tdpaa2_fd_set_offset(fd, priv->tx_data_offset);\n\t\tdpaa2_fd_set_format(fd, dpaa2_fd_sg);\n\t\tdpaa2_fd_set_addr(fd, sgt_addr);\n\t\tdpaa2_fd_set_len(fd, fd_len);\n\t\tdpaa2_fd_set_ctrl(fd, FD_CTRL_PTA);\n\n\t\t*total_fds_len += fd_len;\n\t\t \n\t\tfd++;\n\t\tindex++;\n\t}\n\n\t*num_fds = index;\n\n\treturn 0;\n\nerr_map_sgt:\nerr_map_data:\n\t \n\tsgt = (struct dpaa2_sg_entry *)(sgt_buf + priv->tx_data_offset);\n\tfor (i = 1; i < num_sge; i++)\n\t\tdma_unmap_single(dev, dpaa2_sg_get_addr(&sgt[i]),\n\t\t\t\t dpaa2_sg_get_len(&sgt[i]), DMA_TO_DEVICE);\n\n\t \n\tdma_unmap_single(dev, tso_hdr_dma, TSO_HEADER_SIZE, DMA_TO_DEVICE);\nerr_map_tso_hdr:\n\tkfree(tso_hdr);\nerr_alloc_tso_hdr:\n\tdpaa2_eth_sgt_recycle(priv, sgt_buf);\nerr_sgt_get:\n\t \n\tfor (i = 0; i < index; i++)\n\t\tdpaa2_eth_free_tx_fd(priv, NULL, NULL, &fd_start[i], false);\n\n\treturn err;\n}\n\nstatic netdev_tx_t __dpaa2_eth_tx(struct sk_buff *skb,\n\t\t\t\t  struct net_device *net_dev)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tint total_enqueued = 0, retries = 0, enqueued;\n\tstruct dpaa2_eth_drv_stats *percpu_extras;\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tunsigned int needed_headroom;\n\tint num_fds = 1, max_retries;\n\tstruct dpaa2_eth_fq *fq;\n\tstruct netdev_queue *nq;\n\tstruct dpaa2_fd *fd;\n\tu16 queue_mapping;\n\tvoid *swa = NULL;\n\tu8 prio = 0;\n\tint err, i;\n\tu32 fd_len;\n\n\tpercpu_stats = this_cpu_ptr(priv->percpu_stats);\n\tpercpu_extras = this_cpu_ptr(priv->percpu_extras);\n\tfd = (this_cpu_ptr(priv->fd))->array;\n\n\tneeded_headroom = dpaa2_eth_needed_headroom(skb);\n\n\t \n\tskb = skb_unshare(skb, GFP_ATOMIC);\n\tif (unlikely(!skb)) {\n\t\t \n\t\tpercpu_stats->tx_dropped++;\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\t \n\n\tif (skb_is_gso(skb)) {\n\t\terr = dpaa2_eth_build_gso_fd(priv, skb, fd, &num_fds, &fd_len);\n\t\tpercpu_extras->tx_sg_frames += num_fds;\n\t\tpercpu_extras->tx_sg_bytes += fd_len;\n\t\tpercpu_extras->tx_tso_frames += num_fds;\n\t\tpercpu_extras->tx_tso_bytes += fd_len;\n\t} else if (skb_is_nonlinear(skb)) {\n\t\terr = dpaa2_eth_build_sg_fd(priv, skb, fd, &swa);\n\t\tpercpu_extras->tx_sg_frames++;\n\t\tpercpu_extras->tx_sg_bytes += skb->len;\n\t\tfd_len = dpaa2_fd_get_len(fd);\n\t} else if (skb_headroom(skb) < needed_headroom) {\n\t\terr = dpaa2_eth_build_sg_fd_single_buf(priv, skb, fd, &swa);\n\t\tpercpu_extras->tx_sg_frames++;\n\t\tpercpu_extras->tx_sg_bytes += skb->len;\n\t\tpercpu_extras->tx_converted_sg_frames++;\n\t\tpercpu_extras->tx_converted_sg_bytes += skb->len;\n\t\tfd_len = dpaa2_fd_get_len(fd);\n\t} else {\n\t\terr = dpaa2_eth_build_single_fd(priv, skb, fd, &swa);\n\t\tfd_len = dpaa2_fd_get_len(fd);\n\t}\n\n\tif (unlikely(err)) {\n\t\tpercpu_stats->tx_dropped++;\n\t\tgoto err_build_fd;\n\t}\n\n\tif (swa && skb->cb[0])\n\t\tdpaa2_eth_enable_tx_tstamp(priv, fd, swa, skb);\n\n\t \n\tfor (i = 0; i < num_fds; i++)\n\t\ttrace_dpaa2_tx_fd(net_dev, &fd[i]);\n\n\t \n\tqueue_mapping = skb_get_queue_mapping(skb);\n\n\tif (net_dev->num_tc) {\n\t\tprio = netdev_txq_to_tc(net_dev, queue_mapping);\n\t\t \n\t\tprio = net_dev->num_tc - prio - 1;\n\t\t \n\t\tqueue_mapping %= dpaa2_eth_queue_count(priv);\n\t}\n\tfq = &priv->fq[queue_mapping];\n\tnq = netdev_get_tx_queue(net_dev, queue_mapping);\n\tnetdev_tx_sent_queue(nq, fd_len);\n\n\t \n\tmax_retries = num_fds * DPAA2_ETH_ENQUEUE_RETRIES;\n\twhile (total_enqueued < num_fds && retries < max_retries) {\n\t\terr = priv->enqueue(priv, fq, &fd[total_enqueued],\n\t\t\t\t    prio, num_fds - total_enqueued, &enqueued);\n\t\tif (err == -EBUSY) {\n\t\t\tretries++;\n\t\t\tcontinue;\n\t\t}\n\n\t\ttotal_enqueued += enqueued;\n\t}\n\tpercpu_extras->tx_portal_busy += retries;\n\n\tif (unlikely(err < 0)) {\n\t\tpercpu_stats->tx_errors++;\n\t\t \n\t\tdpaa2_eth_free_tx_fd(priv, NULL, fq, fd, false);\n\t\tnetdev_tx_completed_queue(nq, 1, fd_len);\n\t} else {\n\t\tpercpu_stats->tx_packets += total_enqueued;\n\t\tpercpu_stats->tx_bytes += fd_len;\n\t}\n\n\treturn NETDEV_TX_OK;\n\nerr_build_fd:\n\tdev_kfree_skb(skb);\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic void dpaa2_eth_tx_onestep_tstamp(struct work_struct *work)\n{\n\tstruct dpaa2_eth_priv *priv = container_of(work, struct dpaa2_eth_priv,\n\t\t\t\t\t\t   tx_onestep_tstamp);\n\tstruct sk_buff *skb;\n\n\twhile (true) {\n\t\tskb = skb_dequeue(&priv->tx_skbs);\n\t\tif (!skb)\n\t\t\treturn;\n\n\t\t \n\t\tmutex_lock(&priv->onestep_tstamp_lock);\n\t\t__dpaa2_eth_tx(skb, priv->net_dev);\n\t}\n}\n\nstatic netdev_tx_t dpaa2_eth_tx(struct sk_buff *skb, struct net_device *net_dev)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tu8 msgtype, twostep, udp;\n\tu16 offset1, offset2;\n\n\t \n\tskb->cb[0] = 0;\n\n\tif ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && dpaa2_ptp) {\n\t\tif (priv->tx_tstamp_type == HWTSTAMP_TX_ON)\n\t\t\tskb->cb[0] = TX_TSTAMP;\n\t\telse if (priv->tx_tstamp_type == HWTSTAMP_TX_ONESTEP_SYNC)\n\t\t\tskb->cb[0] = TX_TSTAMP_ONESTEP_SYNC;\n\t}\n\n\t \n\tif (skb->cb[0] == TX_TSTAMP_ONESTEP_SYNC) {\n\t\tif (!dpaa2_eth_ptp_parse(skb, &msgtype, &twostep, &udp,\n\t\t\t\t\t &offset1, &offset2))\n\t\t\tif (msgtype == PTP_MSGTYPE_SYNC && twostep == 0) {\n\t\t\t\tskb_queue_tail(&priv->tx_skbs, skb);\n\t\t\t\tqueue_work(priv->dpaa2_ptp_wq,\n\t\t\t\t\t   &priv->tx_onestep_tstamp);\n\t\t\t\treturn NETDEV_TX_OK;\n\t\t\t}\n\t\t \n\t\tskb->cb[0] = TX_TSTAMP;\n\t}\n\n\t \n\treturn __dpaa2_eth_tx(skb, net_dev);\n}\n\n \nstatic void dpaa2_eth_tx_conf(struct dpaa2_eth_priv *priv,\n\t\t\t      struct dpaa2_eth_channel *ch,\n\t\t\t      const struct dpaa2_fd *fd,\n\t\t\t      struct dpaa2_eth_fq *fq)\n{\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tstruct dpaa2_eth_drv_stats *percpu_extras;\n\tu32 fd_len = dpaa2_fd_get_len(fd);\n\tu32 fd_errors;\n\n\t \n\ttrace_dpaa2_tx_conf_fd(priv->net_dev, fd);\n\n\tpercpu_extras = this_cpu_ptr(priv->percpu_extras);\n\tpercpu_extras->tx_conf_frames++;\n\tpercpu_extras->tx_conf_bytes += fd_len;\n\tch->stats.bytes_per_cdan += fd_len;\n\n\t \n\tfd_errors = dpaa2_fd_get_ctrl(fd) & DPAA2_FD_TX_ERR_MASK;\n\tdpaa2_eth_free_tx_fd(priv, ch, fq, fd, true);\n\n\tif (likely(!fd_errors))\n\t\treturn;\n\n\tif (net_ratelimit())\n\t\tnetdev_dbg(priv->net_dev, \"TX frame FD error: 0x%08x\\n\",\n\t\t\t   fd_errors);\n\n\tpercpu_stats = this_cpu_ptr(priv->percpu_stats);\n\t \n\tpercpu_stats->tx_errors++;\n}\n\nstatic int dpaa2_eth_set_rx_vlan_filtering(struct dpaa2_eth_priv *priv,\n\t\t\t\t\t   bool enable)\n{\n\tint err;\n\n\terr = dpni_enable_vlan_filter(priv->mc_io, 0, priv->mc_token, enable);\n\n\tif (err) {\n\t\tnetdev_err(priv->net_dev,\n\t\t\t   \"dpni_enable_vlan_filter failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_set_rx_csum(struct dpaa2_eth_priv *priv, bool enable)\n{\n\tint err;\n\n\terr = dpni_set_offload(priv->mc_io, 0, priv->mc_token,\n\t\t\t       DPNI_OFF_RX_L3_CSUM, enable);\n\tif (err) {\n\t\tnetdev_err(priv->net_dev,\n\t\t\t   \"dpni_set_offload(RX_L3_CSUM) failed\\n\");\n\t\treturn err;\n\t}\n\n\terr = dpni_set_offload(priv->mc_io, 0, priv->mc_token,\n\t\t\t       DPNI_OFF_RX_L4_CSUM, enable);\n\tif (err) {\n\t\tnetdev_err(priv->net_dev,\n\t\t\t   \"dpni_set_offload(RX_L4_CSUM) failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_set_tx_csum(struct dpaa2_eth_priv *priv, bool enable)\n{\n\tint err;\n\n\terr = dpni_set_offload(priv->mc_io, 0, priv->mc_token,\n\t\t\t       DPNI_OFF_TX_L3_CSUM, enable);\n\tif (err) {\n\t\tnetdev_err(priv->net_dev, \"dpni_set_offload(TX_L3_CSUM) failed\\n\");\n\t\treturn err;\n\t}\n\n\terr = dpni_set_offload(priv->mc_io, 0, priv->mc_token,\n\t\t\t       DPNI_OFF_TX_L4_CSUM, enable);\n\tif (err) {\n\t\tnetdev_err(priv->net_dev, \"dpni_set_offload(TX_L4_CSUM) failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int dpaa2_eth_add_bufs(struct dpaa2_eth_priv *priv,\n\t\t\t      struct dpaa2_eth_channel *ch)\n{\n\tstruct xdp_buff *xdp_buffs[DPAA2_ETH_BUFS_PER_CMD];\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tu64 buf_array[DPAA2_ETH_BUFS_PER_CMD];\n\tstruct dpaa2_eth_swa *swa;\n\tstruct page *page;\n\tdma_addr_t addr;\n\tint retries = 0;\n\tint i = 0, err;\n\tu32 batch;\n\n\t \n\tif (!ch->xsk_zc) {\n\t\tfor (i = 0; i < DPAA2_ETH_BUFS_PER_CMD; i++) {\n\t\t\t \n\t\t\tpage = dev_alloc_pages(0);\n\t\t\tif (!page)\n\t\t\t\tgoto err_alloc;\n\n\t\t\taddr = dma_map_page(dev, page, 0, priv->rx_buf_size,\n\t\t\t\t\t    DMA_BIDIRECTIONAL);\n\t\t\tif (unlikely(dma_mapping_error(dev, addr)))\n\t\t\t\tgoto err_map;\n\n\t\t\tbuf_array[i] = addr;\n\n\t\t\t \n\t\t\ttrace_dpaa2_eth_buf_seed(priv->net_dev,\n\t\t\t\t\t\t page_address(page),\n\t\t\t\t\t\t DPAA2_ETH_RX_BUF_RAW_SIZE,\n\t\t\t\t\t\t addr, priv->rx_buf_size,\n\t\t\t\t\t\t ch->bp->bpid);\n\t\t}\n\t} else if (xsk_buff_can_alloc(ch->xsk_pool, DPAA2_ETH_BUFS_PER_CMD)) {\n\t\t \n\t\tbatch = xsk_buff_alloc_batch(ch->xsk_pool, xdp_buffs,\n\t\t\t\t\t     DPAA2_ETH_BUFS_PER_CMD);\n\t\tif (!batch)\n\t\t\tgoto err_alloc;\n\n\t\tfor (i = 0; i < batch; i++) {\n\t\t\tswa = (struct dpaa2_eth_swa *)(xdp_buffs[i]->data_hard_start +\n\t\t\t\t\t\t       DPAA2_ETH_RX_HWA_SIZE);\n\t\t\tswa->xsk.xdp_buff = xdp_buffs[i];\n\n\t\t\taddr = xsk_buff_xdp_get_frame_dma(xdp_buffs[i]);\n\t\t\tif (unlikely(dma_mapping_error(dev, addr)))\n\t\t\t\tgoto err_map;\n\n\t\t\tbuf_array[i] = addr;\n\n\t\t\ttrace_dpaa2_xsk_buf_seed(priv->net_dev,\n\t\t\t\t\t\t xdp_buffs[i]->data_hard_start,\n\t\t\t\t\t\t DPAA2_ETH_RX_BUF_RAW_SIZE,\n\t\t\t\t\t\t addr, priv->rx_buf_size,\n\t\t\t\t\t\t ch->bp->bpid);\n\t\t}\n\t}\n\nrelease_bufs:\n\t \n\twhile ((err = dpaa2_io_service_release(ch->dpio, ch->bp->bpid,\n\t\t\t\t\t       buf_array, i)) == -EBUSY) {\n\t\tif (retries++ >= DPAA2_ETH_SWP_BUSY_RETRIES)\n\t\t\tbreak;\n\t\tcpu_relax();\n\t}\n\n\t \n\tif (err) {\n\t\tdpaa2_eth_free_bufs(priv, buf_array, i, ch->xsk_zc);\n\t\treturn 0;\n\t}\n\n\treturn i;\n\nerr_map:\n\tif (!ch->xsk_zc) {\n\t\t__free_pages(page, 0);\n\t} else {\n\t\tfor (; i < batch; i++)\n\t\t\txsk_buff_free(xdp_buffs[i]);\n\t}\nerr_alloc:\n\t \n\tif (i)\n\t\tgoto release_bufs;\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_seed_pool(struct dpaa2_eth_priv *priv,\n\t\t\t       struct dpaa2_eth_channel *ch)\n{\n\tint i;\n\tint new_count;\n\n\tfor (i = 0; i < DPAA2_ETH_NUM_BUFS; i += DPAA2_ETH_BUFS_PER_CMD) {\n\t\tnew_count = dpaa2_eth_add_bufs(priv, ch);\n\t\tch->buf_count += new_count;\n\n\t\tif (new_count < DPAA2_ETH_BUFS_PER_CMD)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void dpaa2_eth_seed_pools(struct dpaa2_eth_priv *priv)\n{\n\tstruct net_device *net_dev = priv->net_dev;\n\tstruct dpaa2_eth_channel *channel;\n\tint i, err = 0;\n\n\tfor (i = 0; i < priv->num_channels; i++) {\n\t\tchannel = priv->channel[i];\n\n\t\terr = dpaa2_eth_seed_pool(priv, channel);\n\n\t\t \n\t\tif (err)\n\t\t\tnetdev_err(net_dev, \"Buffer seeding failed for DPBP %d (bpid=%d)\\n\",\n\t\t\t\t   channel->bp->dev->obj_desc.id,\n\t\t\t\t   channel->bp->bpid);\n\t}\n}\n\n \nstatic void dpaa2_eth_drain_bufs(struct dpaa2_eth_priv *priv, int bpid,\n\t\t\t\t int count)\n{\n\tu64 buf_array[DPAA2_ETH_BUFS_PER_CMD];\n\tbool xsk_zc = false;\n\tint retries = 0;\n\tint i, ret;\n\n\tfor (i = 0; i < priv->num_channels; i++)\n\t\tif (priv->channel[i]->bp->bpid == bpid)\n\t\t\txsk_zc = priv->channel[i]->xsk_zc;\n\n\tdo {\n\t\tret = dpaa2_io_service_acquire(NULL, bpid, buf_array, count);\n\t\tif (ret < 0) {\n\t\t\tif (ret == -EBUSY &&\n\t\t\t    retries++ < DPAA2_ETH_SWP_BUSY_RETRIES)\n\t\t\t\tcontinue;\n\t\t\tnetdev_err(priv->net_dev, \"dpaa2_io_service_acquire() failed\\n\");\n\t\t\treturn;\n\t\t}\n\t\tdpaa2_eth_free_bufs(priv, buf_array, ret, xsk_zc);\n\t\tretries = 0;\n\t} while (ret);\n}\n\nstatic void dpaa2_eth_drain_pool(struct dpaa2_eth_priv *priv, int bpid)\n{\n\tint i;\n\n\t \n\tdpaa2_eth_drain_bufs(priv, bpid, DPAA2_ETH_BUFS_PER_CMD);\n\tdpaa2_eth_drain_bufs(priv, bpid, 1);\n\n\t \n\tfor (i = 0; i < priv->num_channels; i++)\n\t\tif (priv->channel[i]->bp->bpid == bpid)\n\t\t\tpriv->channel[i]->buf_count = 0;\n}\n\nstatic void dpaa2_eth_drain_pools(struct dpaa2_eth_priv *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_bps; i++)\n\t\tdpaa2_eth_drain_pool(priv, priv->bp[i]->bpid);\n}\n\n \nstatic int dpaa2_eth_refill_pool(struct dpaa2_eth_priv *priv,\n\t\t\t\t struct dpaa2_eth_channel *ch)\n{\n\tint new_count;\n\n\tif (likely(ch->buf_count >= DPAA2_ETH_REFILL_THRESH))\n\t\treturn 0;\n\n\tdo {\n\t\tnew_count = dpaa2_eth_add_bufs(priv, ch);\n\t\tif (unlikely(!new_count)) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tch->buf_count += new_count;\n\t} while (ch->buf_count < DPAA2_ETH_NUM_BUFS);\n\n\tif (unlikely(ch->buf_count < DPAA2_ETH_NUM_BUFS))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void dpaa2_eth_sgt_cache_drain(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_eth_sgt_cache *sgt_cache;\n\tu16 count;\n\tint k, i;\n\n\tfor_each_possible_cpu(k) {\n\t\tsgt_cache = per_cpu_ptr(priv->sgt_cache, k);\n\t\tcount = sgt_cache->count;\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tskb_free_frag(sgt_cache->buf[i]);\n\t\tsgt_cache->count = 0;\n\t}\n}\n\nstatic int dpaa2_eth_pull_channel(struct dpaa2_eth_channel *ch)\n{\n\tint err;\n\tint dequeues = -1;\n\n\t \n\tdo {\n\t\terr = dpaa2_io_service_pull_channel(ch->dpio, ch->ch_id,\n\t\t\t\t\t\t    ch->store);\n\t\tdequeues++;\n\t\tcpu_relax();\n\t} while (err == -EBUSY && dequeues < DPAA2_ETH_SWP_BUSY_RETRIES);\n\n\tch->stats.dequeue_portal_busy += dequeues;\n\tif (unlikely(err))\n\t\tch->stats.pull_err++;\n\n\treturn err;\n}\n\n \nstatic int dpaa2_eth_poll(struct napi_struct *napi, int budget)\n{\n\tstruct dpaa2_eth_channel *ch;\n\tstruct dpaa2_eth_priv *priv;\n\tint rx_cleaned = 0, txconf_cleaned = 0;\n\tstruct dpaa2_eth_fq *fq, *txc_fq = NULL;\n\tstruct netdev_queue *nq;\n\tint store_cleaned, work_done;\n\tbool work_done_zc = false;\n\tstruct list_head rx_list;\n\tint retries = 0;\n\tu16 flowid;\n\tint err;\n\n\tch = container_of(napi, struct dpaa2_eth_channel, napi);\n\tch->xdp.res = 0;\n\tpriv = ch->priv;\n\n\tINIT_LIST_HEAD(&rx_list);\n\tch->rx_list = &rx_list;\n\n\tif (ch->xsk_zc) {\n\t\twork_done_zc = dpaa2_xsk_tx(priv, ch);\n\t\t \n\t\tif (work_done_zc) {\n\t\t\twork_done = budget;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tdo {\n\t\terr = dpaa2_eth_pull_channel(ch);\n\t\tif (unlikely(err))\n\t\t\tbreak;\n\n\t\t \n\t\tdpaa2_eth_refill_pool(priv, ch);\n\n\t\tstore_cleaned = dpaa2_eth_consume_frames(ch, &fq);\n\t\tif (store_cleaned <= 0)\n\t\t\tbreak;\n\t\tif (fq->type == DPAA2_RX_FQ) {\n\t\t\trx_cleaned += store_cleaned;\n\t\t\tflowid = fq->flowid;\n\t\t} else {\n\t\t\ttxconf_cleaned += store_cleaned;\n\t\t\t \n\t\t\ttxc_fq = fq;\n\t\t}\n\n\t\t \n\t\tif (rx_cleaned >= budget ||\n\t\t    txconf_cleaned >= DPAA2_ETH_TXCONF_PER_NAPI) {\n\t\t\twork_done = budget;\n\t\t\tif (ch->xdp.res & XDP_REDIRECT)\n\t\t\t\txdp_do_flush();\n\t\t\tgoto out;\n\t\t}\n\t} while (store_cleaned);\n\n\tif (ch->xdp.res & XDP_REDIRECT)\n\t\txdp_do_flush();\n\n\t \n\tdpaa2_io_update_net_dim(ch->dpio, ch->stats.frames_per_cdan,\n\t\t\t\tch->stats.bytes_per_cdan);\n\tch->stats.frames_per_cdan = 0;\n\tch->stats.bytes_per_cdan = 0;\n\n\t \n\tnapi_complete_done(napi, rx_cleaned);\n\tdo {\n\t\terr = dpaa2_io_service_rearm(ch->dpio, &ch->nctx);\n\t\tcpu_relax();\n\t} while (err == -EBUSY && retries++ < DPAA2_ETH_SWP_BUSY_RETRIES);\n\tWARN_ONCE(err, \"CDAN notifications rearm failed on core %d\",\n\t\t  ch->nctx.desired_cpu);\n\n\twork_done = max(rx_cleaned, 1);\n\nout:\n\tnetif_receive_skb_list(ch->rx_list);\n\n\tif (ch->xsk_tx_pkts_sent) {\n\t\txsk_tx_completed(ch->xsk_pool, ch->xsk_tx_pkts_sent);\n\t\tch->xsk_tx_pkts_sent = 0;\n\t}\n\n\tif (txc_fq && txc_fq->dq_frames) {\n\t\tnq = netdev_get_tx_queue(priv->net_dev, txc_fq->flowid);\n\t\tnetdev_tx_completed_queue(nq, txc_fq->dq_frames,\n\t\t\t\t\t  txc_fq->dq_bytes);\n\t\ttxc_fq->dq_frames = 0;\n\t\ttxc_fq->dq_bytes = 0;\n\t}\n\n\tif (rx_cleaned && ch->xdp.res & XDP_TX)\n\t\tdpaa2_eth_xdp_tx_flush(priv, ch, &priv->fq[flowid]);\n\n\treturn work_done;\n}\n\nstatic void dpaa2_eth_enable_ch_napi(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_eth_channel *ch;\n\tint i;\n\n\tfor (i = 0; i < priv->num_channels; i++) {\n\t\tch = priv->channel[i];\n\t\tnapi_enable(&ch->napi);\n\t}\n}\n\nstatic void dpaa2_eth_disable_ch_napi(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_eth_channel *ch;\n\tint i;\n\n\tfor (i = 0; i < priv->num_channels; i++) {\n\t\tch = priv->channel[i];\n\t\tnapi_disable(&ch->napi);\n\t}\n}\n\nvoid dpaa2_eth_set_rx_taildrop(struct dpaa2_eth_priv *priv,\n\t\t\t       bool tx_pause, bool pfc)\n{\n\tstruct dpni_taildrop td = {0};\n\tstruct dpaa2_eth_fq *fq;\n\tint i, err;\n\n\t \n\ttd.enable = !tx_pause;\n\tif (priv->rx_fqtd_enabled == td.enable)\n\t\tgoto set_cgtd;\n\n\ttd.threshold = DPAA2_ETH_FQ_TAILDROP_THRESH;\n\ttd.units = DPNI_CONGESTION_UNIT_BYTES;\n\n\tfor (i = 0; i < priv->num_fqs; i++) {\n\t\tfq = &priv->fq[i];\n\t\tif (fq->type != DPAA2_RX_FQ)\n\t\t\tcontinue;\n\t\terr = dpni_set_taildrop(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\tDPNI_CP_QUEUE, DPNI_QUEUE_RX,\n\t\t\t\t\tfq->tc, fq->flowid, &td);\n\t\tif (err) {\n\t\t\tnetdev_err(priv->net_dev,\n\t\t\t\t   \"dpni_set_taildrop(FQ) failed\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\tpriv->rx_fqtd_enabled = td.enable;\n\nset_cgtd:\n\t \n\ttd.enable = !tx_pause || pfc;\n\tif (priv->rx_cgtd_enabled == td.enable)\n\t\treturn;\n\n\ttd.threshold = DPAA2_ETH_CG_TAILDROP_THRESH(priv);\n\ttd.units = DPNI_CONGESTION_UNIT_FRAMES;\n\tfor (i = 0; i < dpaa2_eth_tc_count(priv); i++) {\n\t\terr = dpni_set_taildrop(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\tDPNI_CP_GROUP, DPNI_QUEUE_RX,\n\t\t\t\t\ti, 0, &td);\n\t\tif (err) {\n\t\t\tnetdev_err(priv->net_dev,\n\t\t\t\t   \"dpni_set_taildrop(CG) failed\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\tpriv->rx_cgtd_enabled = td.enable;\n}\n\nstatic int dpaa2_eth_link_state_update(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpni_link_state state = {0};\n\tbool tx_pause;\n\tint err;\n\n\terr = dpni_get_link_state(priv->mc_io, 0, priv->mc_token, &state);\n\tif (unlikely(err)) {\n\t\tnetdev_err(priv->net_dev,\n\t\t\t   \"dpni_get_link_state() failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\ttx_pause = dpaa2_eth_tx_pause_enabled(state.options);\n\tdpaa2_eth_set_rx_taildrop(priv, tx_pause, priv->pfc_enabled);\n\n\t \n\tif (dpaa2_mac_is_type_phy(priv->mac))\n\t\tgoto out;\n\n\t \n\tif (priv->link_state.up == state.up)\n\t\tgoto out;\n\n\tif (state.up) {\n\t\tnetif_carrier_on(priv->net_dev);\n\t\tnetif_tx_start_all_queues(priv->net_dev);\n\t} else {\n\t\tnetif_tx_stop_all_queues(priv->net_dev);\n\t\tnetif_carrier_off(priv->net_dev);\n\t}\n\n\tnetdev_info(priv->net_dev, \"Link Event: state %s\\n\",\n\t\t    state.up ? \"up\" : \"down\");\n\nout:\n\tpriv->link_state = state;\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_open(struct net_device *net_dev)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tint err;\n\n\tdpaa2_eth_seed_pools(priv);\n\n\tmutex_lock(&priv->mac_lock);\n\n\tif (!dpaa2_eth_is_type_phy(priv)) {\n\t\t \n\t\tnetif_tx_stop_all_queues(net_dev);\n\n\t\t \n\t\tnetif_carrier_off(net_dev);\n\t}\n\tdpaa2_eth_enable_ch_napi(priv);\n\n\terr = dpni_enable(priv->mc_io, 0, priv->mc_token);\n\tif (err < 0) {\n\t\tmutex_unlock(&priv->mac_lock);\n\t\tnetdev_err(net_dev, \"dpni_enable() failed\\n\");\n\t\tgoto enable_err;\n\t}\n\n\tif (dpaa2_eth_is_type_phy(priv))\n\t\tdpaa2_mac_start(priv->mac);\n\n\tmutex_unlock(&priv->mac_lock);\n\n\treturn 0;\n\nenable_err:\n\tdpaa2_eth_disable_ch_napi(priv);\n\tdpaa2_eth_drain_pools(priv);\n\treturn err;\n}\n\n \nstatic u32 dpaa2_eth_ingress_fq_count(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_eth_fq *fq;\n\tu32 fcnt = 0, bcnt = 0, total = 0;\n\tint i, err;\n\n\tfor (i = 0; i < priv->num_fqs; i++) {\n\t\tfq = &priv->fq[i];\n\t\terr = dpaa2_io_query_fq_count(NULL, fq->fqid, &fcnt, &bcnt);\n\t\tif (err) {\n\t\t\tnetdev_warn(priv->net_dev, \"query_fq_count failed\");\n\t\t\tbreak;\n\t\t}\n\t\ttotal += fcnt;\n\t}\n\n\treturn total;\n}\n\nstatic void dpaa2_eth_wait_for_ingress_fq_empty(struct dpaa2_eth_priv *priv)\n{\n\tint retries = 10;\n\tu32 pending;\n\n\tdo {\n\t\tpending = dpaa2_eth_ingress_fq_count(priv);\n\t\tif (pending)\n\t\t\tmsleep(100);\n\t} while (pending && --retries);\n}\n\n#define DPNI_TX_PENDING_VER_MAJOR\t7\n#define DPNI_TX_PENDING_VER_MINOR\t13\nstatic void dpaa2_eth_wait_for_egress_fq_empty(struct dpaa2_eth_priv *priv)\n{\n\tunion dpni_statistics stats;\n\tint retries = 10;\n\tint err;\n\n\tif (dpaa2_eth_cmp_dpni_ver(priv, DPNI_TX_PENDING_VER_MAJOR,\n\t\t\t\t   DPNI_TX_PENDING_VER_MINOR) < 0)\n\t\tgoto out;\n\n\tdo {\n\t\terr = dpni_get_statistics(priv->mc_io, 0, priv->mc_token, 6,\n\t\t\t\t\t  &stats);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (stats.page_6.tx_pending_frames == 0)\n\t\t\treturn;\n\t} while (--retries);\n\nout:\n\tmsleep(500);\n}\n\nstatic int dpaa2_eth_stop(struct net_device *net_dev)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tint dpni_enabled = 0;\n\tint retries = 10;\n\n\tmutex_lock(&priv->mac_lock);\n\n\tif (dpaa2_eth_is_type_phy(priv)) {\n\t\tdpaa2_mac_stop(priv->mac);\n\t} else {\n\t\tnetif_tx_stop_all_queues(net_dev);\n\t\tnetif_carrier_off(net_dev);\n\t}\n\n\tmutex_unlock(&priv->mac_lock);\n\n\t \n\tdpaa2_eth_wait_for_egress_fq_empty(priv);\n\n\tdo {\n\t\tdpni_disable(priv->mc_io, 0, priv->mc_token);\n\t\tdpni_is_enabled(priv->mc_io, 0, priv->mc_token, &dpni_enabled);\n\t\tif (dpni_enabled)\n\t\t\t \n\t\t\tmsleep(100);\n\t} while (dpni_enabled && --retries);\n\tif (!retries) {\n\t\tnetdev_warn(net_dev, \"Retry count exceeded disabling DPNI\\n\");\n\t\t \n\t}\n\n\tdpaa2_eth_wait_for_ingress_fq_empty(priv);\n\tdpaa2_eth_disable_ch_napi(priv);\n\n\t \n\tdpaa2_eth_drain_pools(priv);\n\n\t \n\tdpaa2_eth_sgt_cache_drain(priv);\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_set_addr(struct net_device *net_dev, void *addr)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tstruct device *dev = net_dev->dev.parent;\n\tint err;\n\n\terr = eth_mac_addr(net_dev, addr);\n\tif (err < 0) {\n\t\tdev_err(dev, \"eth_mac_addr() failed (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = dpni_set_primary_mac_addr(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\tnet_dev->dev_addr);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_primary_mac_addr() failed (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void dpaa2_eth_get_stats(struct net_device *net_dev,\n\t\t\t\tstruct rtnl_link_stats64 *stats)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tu64 *cpustats;\n\tu64 *netstats = (u64 *)stats;\n\tint i, j;\n\tint num = sizeof(struct rtnl_link_stats64) / sizeof(u64);\n\n\tfor_each_possible_cpu(i) {\n\t\tpercpu_stats = per_cpu_ptr(priv->percpu_stats, i);\n\t\tcpustats = (u64 *)percpu_stats;\n\t\tfor (j = 0; j < num; j++)\n\t\t\tnetstats[j] += cpustats[j];\n\t}\n}\n\n \nstatic void dpaa2_eth_add_uc_hw_addr(const struct net_device *net_dev,\n\t\t\t\t     struct dpaa2_eth_priv *priv)\n{\n\tstruct netdev_hw_addr *ha;\n\tint err;\n\n\tnetdev_for_each_uc_addr(ha, net_dev) {\n\t\terr = dpni_add_mac_addr(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\tha->addr);\n\t\tif (err)\n\t\t\tnetdev_warn(priv->net_dev,\n\t\t\t\t    \"Could not add ucast MAC %pM to the filtering table (err %d)\\n\",\n\t\t\t\t    ha->addr, err);\n\t}\n}\n\n \nstatic void dpaa2_eth_add_mc_hw_addr(const struct net_device *net_dev,\n\t\t\t\t     struct dpaa2_eth_priv *priv)\n{\n\tstruct netdev_hw_addr *ha;\n\tint err;\n\n\tnetdev_for_each_mc_addr(ha, net_dev) {\n\t\terr = dpni_add_mac_addr(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\tha->addr);\n\t\tif (err)\n\t\t\tnetdev_warn(priv->net_dev,\n\t\t\t\t    \"Could not add mcast MAC %pM to the filtering table (err %d)\\n\",\n\t\t\t\t    ha->addr, err);\n\t}\n}\n\nstatic int dpaa2_eth_rx_add_vid(struct net_device *net_dev,\n\t\t\t\t__be16 vlan_proto, u16 vid)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tint err;\n\n\terr = dpni_add_vlan_id(priv->mc_io, 0, priv->mc_token,\n\t\t\t       vid, 0, 0, 0);\n\n\tif (err) {\n\t\tnetdev_warn(priv->net_dev,\n\t\t\t    \"Could not add the vlan id %u\\n\",\n\t\t\t    vid);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_rx_kill_vid(struct net_device *net_dev,\n\t\t\t\t __be16 vlan_proto, u16 vid)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tint err;\n\n\terr = dpni_remove_vlan_id(priv->mc_io, 0, priv->mc_token, vid);\n\n\tif (err) {\n\t\tnetdev_warn(priv->net_dev,\n\t\t\t    \"Could not remove the vlan id %u\\n\",\n\t\t\t    vid);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void dpaa2_eth_set_rx_mode(struct net_device *net_dev)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tint uc_count = netdev_uc_count(net_dev);\n\tint mc_count = netdev_mc_count(net_dev);\n\tu8 max_mac = priv->dpni_attrs.mac_filter_entries;\n\tu32 options = priv->dpni_attrs.options;\n\tu16 mc_token = priv->mc_token;\n\tstruct fsl_mc_io *mc_io = priv->mc_io;\n\tint err;\n\n\t \n\tif (options & DPNI_OPT_NO_MAC_FILTER && max_mac != 0)\n\t\tnetdev_info(net_dev,\n\t\t\t    \"mac_filter_entries=%d, DPNI_OPT_NO_MAC_FILTER option must be disabled\\n\",\n\t\t\t    max_mac);\n\n\t \n\tif (uc_count > max_mac) {\n\t\tnetdev_info(net_dev,\n\t\t\t    \"Unicast addr count reached %d, max allowed is %d; forcing promisc\\n\",\n\t\t\t    uc_count, max_mac);\n\t\tgoto force_promisc;\n\t}\n\tif (mc_count + uc_count > max_mac) {\n\t\tnetdev_info(net_dev,\n\t\t\t    \"Unicast + multicast addr count reached %d, max allowed is %d; forcing promisc\\n\",\n\t\t\t    uc_count + mc_count, max_mac);\n\t\tgoto force_mc_promisc;\n\t}\n\n\t \n\tif (net_dev->flags & IFF_PROMISC)\n\t\tgoto force_promisc;\n\tif (net_dev->flags & IFF_ALLMULTI) {\n\t\t \n\t\terr = dpni_set_unicast_promisc(mc_io, 0, mc_token, 1);\n\t\tif (err)\n\t\t\tnetdev_warn(net_dev, \"Can't set uc promisc\\n\");\n\n\t\t \n\t\terr = dpni_clear_mac_filters(mc_io, 0, mc_token, 1, 0);\n\t\tif (err)\n\t\t\tnetdev_warn(net_dev, \"Can't clear uc filters\\n\");\n\t\tdpaa2_eth_add_uc_hw_addr(net_dev, priv);\n\n\t\t \n\t\terr = dpni_set_unicast_promisc(mc_io, 0, mc_token, 0);\n\t\tif (err)\n\t\t\tnetdev_warn(net_dev, \"Can't clear uc promisc\\n\");\n\t\tgoto force_mc_promisc;\n\t}\n\n\t \n\terr = dpni_set_unicast_promisc(mc_io, 0, mc_token, 1);\n\tif (err)\n\t\tnetdev_warn(net_dev, \"Can't set uc promisc (%d)\\n\", err);\n\terr = dpni_set_multicast_promisc(mc_io, 0, mc_token, 1);\n\tif (err)\n\t\tnetdev_warn(net_dev, \"Can't set mc promisc (%d)\\n\", err);\n\n\t \n\terr = dpni_clear_mac_filters(mc_io, 0, mc_token, 1, 1);\n\tif (err)\n\t\tnetdev_warn(net_dev, \"Can't clear mac filters\\n\");\n\tdpaa2_eth_add_mc_hw_addr(net_dev, priv);\n\tdpaa2_eth_add_uc_hw_addr(net_dev, priv);\n\n\t \n\terr = dpni_set_unicast_promisc(mc_io, 0, mc_token, 0);\n\tif (err)\n\t\tnetdev_warn(net_dev, \"Can't clear ucast promisc\\n\");\n\terr = dpni_set_multicast_promisc(mc_io, 0, mc_token, 0);\n\tif (err)\n\t\tnetdev_warn(net_dev, \"Can't clear mcast promisc\\n\");\n\n\treturn;\n\nforce_promisc:\n\terr = dpni_set_unicast_promisc(mc_io, 0, mc_token, 1);\n\tif (err)\n\t\tnetdev_warn(net_dev, \"Can't set ucast promisc\\n\");\nforce_mc_promisc:\n\terr = dpni_set_multicast_promisc(mc_io, 0, mc_token, 1);\n\tif (err)\n\t\tnetdev_warn(net_dev, \"Can't set mcast promisc\\n\");\n}\n\nstatic int dpaa2_eth_set_features(struct net_device *net_dev,\n\t\t\t\t  netdev_features_t features)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tnetdev_features_t changed = features ^ net_dev->features;\n\tbool enable;\n\tint err;\n\n\tif (changed & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\tenable = !!(features & NETIF_F_HW_VLAN_CTAG_FILTER);\n\t\terr = dpaa2_eth_set_rx_vlan_filtering(priv, enable);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (changed & NETIF_F_RXCSUM) {\n\t\tenable = !!(features & NETIF_F_RXCSUM);\n\t\terr = dpaa2_eth_set_rx_csum(priv, enable);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (changed & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)) {\n\t\tenable = !!(features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM));\n\t\terr = dpaa2_eth_set_tx_csum(priv, enable);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_ts_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(dev);\n\tstruct hwtstamp_config config;\n\n\tif (!dpaa2_ptp)\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&config, rq->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\tswitch (config.tx_type) {\n\tcase HWTSTAMP_TX_OFF:\n\tcase HWTSTAMP_TX_ON:\n\tcase HWTSTAMP_TX_ONESTEP_SYNC:\n\t\tpriv->tx_tstamp_type = config.tx_type;\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tif (config.rx_filter == HWTSTAMP_FILTER_NONE) {\n\t\tpriv->rx_tstamp = false;\n\t} else {\n\t\tpriv->rx_tstamp = true;\n\t\t \n\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t}\n\n\tif (priv->tx_tstamp_type == HWTSTAMP_TX_ONESTEP_SYNC)\n\t\tdpaa2_ptp_onestep_reg_update_method(priv);\n\n\treturn copy_to_user(rq->ifr_data, &config, sizeof(config)) ?\n\t\t\t-EFAULT : 0;\n}\n\nstatic int dpaa2_eth_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(dev);\n\tint err;\n\n\tif (cmd == SIOCSHWTSTAMP)\n\t\treturn dpaa2_eth_ts_ioctl(dev, rq, cmd);\n\n\tmutex_lock(&priv->mac_lock);\n\n\tif (dpaa2_eth_is_type_phy(priv)) {\n\t\terr = phylink_mii_ioctl(priv->mac->phylink, rq, cmd);\n\t\tmutex_unlock(&priv->mac_lock);\n\t\treturn err;\n\t}\n\n\tmutex_unlock(&priv->mac_lock);\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic bool xdp_mtu_valid(struct dpaa2_eth_priv *priv, int mtu)\n{\n\tint mfl, linear_mfl;\n\n\tmfl = DPAA2_ETH_L2_MAX_FRM(mtu);\n\tlinear_mfl = priv->rx_buf_size - DPAA2_ETH_RX_HWA_SIZE -\n\t\t     dpaa2_eth_rx_head_room(priv) - XDP_PACKET_HEADROOM;\n\n\tif (mfl > linear_mfl) {\n\t\tnetdev_warn(priv->net_dev, \"Maximum MTU for XDP is %d\\n\",\n\t\t\t    linear_mfl - VLAN_ETH_HLEN);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int dpaa2_eth_set_rx_mfl(struct dpaa2_eth_priv *priv, int mtu, bool has_xdp)\n{\n\tint mfl, err;\n\n\t \n\tif (has_xdp)\n\t\tmfl = DPAA2_ETH_L2_MAX_FRM(mtu);\n\telse\n\t\tmfl = DPAA2_ETH_MFL;\n\n\terr = dpni_set_max_frame_length(priv->mc_io, 0, priv->mc_token, mfl);\n\tif (err) {\n\t\tnetdev_err(priv->net_dev, \"dpni_set_max_frame_length failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(dev);\n\tint err;\n\n\tif (!priv->xdp_prog)\n\t\tgoto out;\n\n\tif (!xdp_mtu_valid(priv, new_mtu))\n\t\treturn -EINVAL;\n\n\terr = dpaa2_eth_set_rx_mfl(priv, new_mtu, true);\n\tif (err)\n\t\treturn err;\n\nout:\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\n\nstatic int dpaa2_eth_update_rx_buffer_headroom(struct dpaa2_eth_priv *priv, bool has_xdp)\n{\n\tstruct dpni_buffer_layout buf_layout = {0};\n\tint err;\n\n\terr = dpni_get_buffer_layout(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t     DPNI_QUEUE_RX, &buf_layout);\n\tif (err) {\n\t\tnetdev_err(priv->net_dev, \"dpni_get_buffer_layout failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tbuf_layout.data_head_room = dpaa2_eth_rx_head_room(priv) +\n\t\t\t\t    (has_xdp ? XDP_PACKET_HEADROOM : 0);\n\tbuf_layout.options = DPNI_BUF_LAYOUT_OPT_DATA_HEAD_ROOM;\n\terr = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t     DPNI_QUEUE_RX, &buf_layout);\n\tif (err) {\n\t\tnetdev_err(priv->net_dev, \"dpni_set_buffer_layout failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_setup_xdp(struct net_device *dev, struct bpf_prog *prog)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(dev);\n\tstruct dpaa2_eth_channel *ch;\n\tstruct bpf_prog *old;\n\tbool up, need_update;\n\tint i, err;\n\n\tif (prog && !xdp_mtu_valid(priv, dev->mtu))\n\t\treturn -EINVAL;\n\n\tif (prog)\n\t\tbpf_prog_add(prog, priv->num_channels);\n\n\tup = netif_running(dev);\n\tneed_update = (!!priv->xdp_prog != !!prog);\n\n\tif (up)\n\t\tdev_close(dev);\n\n\t \n\tif (need_update) {\n\t\terr = dpaa2_eth_set_rx_mfl(priv, dev->mtu, !!prog);\n\t\tif (err)\n\t\t\tgoto out_err;\n\t\terr = dpaa2_eth_update_rx_buffer_headroom(priv, !!prog);\n\t\tif (err)\n\t\t\tgoto out_err;\n\t}\n\n\told = xchg(&priv->xdp_prog, prog);\n\tif (old)\n\t\tbpf_prog_put(old);\n\n\tfor (i = 0; i < priv->num_channels; i++) {\n\t\tch = priv->channel[i];\n\t\told = xchg(&ch->xdp.prog, prog);\n\t\tif (old)\n\t\t\tbpf_prog_put(old);\n\t}\n\n\tif (up) {\n\t\terr = dev_open(dev, NULL);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n\nout_err:\n\tif (prog)\n\t\tbpf_prog_sub(prog, priv->num_channels);\n\tif (up)\n\t\tdev_open(dev, NULL);\n\n\treturn err;\n}\n\nstatic int dpaa2_eth_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn dpaa2_eth_setup_xdp(dev, xdp->prog);\n\tcase XDP_SETUP_XSK_POOL:\n\t\treturn dpaa2_xsk_setup_pool(dev, xdp->xsk.pool, xdp->xsk.queue_id);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_xdp_create_fd(struct net_device *net_dev,\n\t\t\t\t   struct xdp_frame *xdpf,\n\t\t\t\t   struct dpaa2_fd *fd)\n{\n\tstruct device *dev = net_dev->dev.parent;\n\tunsigned int needed_headroom;\n\tstruct dpaa2_eth_swa *swa;\n\tvoid *buffer_start, *aligned_start;\n\tdma_addr_t addr;\n\n\t \n\tneeded_headroom = dpaa2_eth_needed_headroom(NULL);\n\tif (xdpf->headroom < needed_headroom)\n\t\treturn -EINVAL;\n\n\t \n\tmemset(fd, 0, sizeof(*fd));\n\n\t \n\tbuffer_start = xdpf->data - needed_headroom;\n\taligned_start = PTR_ALIGN(buffer_start - DPAA2_ETH_TX_BUF_ALIGN,\n\t\t\t\t  DPAA2_ETH_TX_BUF_ALIGN);\n\tif (aligned_start >= xdpf->data - xdpf->headroom)\n\t\tbuffer_start = aligned_start;\n\n\tswa = (struct dpaa2_eth_swa *)buffer_start;\n\t \n\tswa->type = DPAA2_ETH_SWA_XDP;\n\tswa->xdp.dma_size = xdpf->data + xdpf->len - buffer_start;\n\tswa->xdp.xdpf = xdpf;\n\n\taddr = dma_map_single(dev, buffer_start,\n\t\t\t      swa->xdp.dma_size,\n\t\t\t      DMA_BIDIRECTIONAL);\n\tif (unlikely(dma_mapping_error(dev, addr)))\n\t\treturn -ENOMEM;\n\n\tdpaa2_fd_set_addr(fd, addr);\n\tdpaa2_fd_set_offset(fd, xdpf->data - buffer_start);\n\tdpaa2_fd_set_len(fd, xdpf->len);\n\tdpaa2_fd_set_format(fd, dpaa2_fd_single);\n\tdpaa2_fd_set_ctrl(fd, FD_CTRL_PTA);\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_xdp_xmit(struct net_device *net_dev, int n,\n\t\t\t      struct xdp_frame **frames, u32 flags)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tstruct dpaa2_eth_xdp_fds *xdp_redirect_fds;\n\tstruct rtnl_link_stats64 *percpu_stats;\n\tstruct dpaa2_eth_fq *fq;\n\tstruct dpaa2_fd *fds;\n\tint enqueued, i, err;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\tif (!netif_running(net_dev))\n\t\treturn -ENETDOWN;\n\n\tfq = &priv->fq[smp_processor_id()];\n\txdp_redirect_fds = &fq->xdp_redirect_fds;\n\tfds = xdp_redirect_fds->fds;\n\n\tpercpu_stats = this_cpu_ptr(priv->percpu_stats);\n\n\t \n\tfor (i = 0; i < n; i++) {\n\t\terr = dpaa2_eth_xdp_create_fd(net_dev, frames[i], &fds[i]);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\txdp_redirect_fds->num = i;\n\n\t \n\tenqueued = dpaa2_eth_xdp_flush(priv, fq, xdp_redirect_fds);\n\n\t \n\tpercpu_stats->tx_packets += enqueued;\n\tfor (i = 0; i < enqueued; i++)\n\t\tpercpu_stats->tx_bytes += dpaa2_fd_get_len(&fds[i]);\n\n\treturn enqueued;\n}\n\nstatic int update_xps(struct dpaa2_eth_priv *priv)\n{\n\tstruct net_device *net_dev = priv->net_dev;\n\tstruct cpumask xps_mask;\n\tstruct dpaa2_eth_fq *fq;\n\tint i, num_queues, netdev_queues;\n\tint err = 0;\n\n\tnum_queues = dpaa2_eth_queue_count(priv);\n\tnetdev_queues = (net_dev->num_tc ? : 1) * num_queues;\n\n\t \n\tfor (i = 0; i < netdev_queues; i++) {\n\t\tfq = &priv->fq[i % num_queues];\n\n\t\tcpumask_clear(&xps_mask);\n\t\tcpumask_set_cpu(fq->target_cpu, &xps_mask);\n\n\t\terr = netif_set_xps_queue(net_dev, &xps_mask, i);\n\t\tif (err) {\n\t\t\tnetdev_warn_once(net_dev, \"Error setting XPS queue\\n\");\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic int dpaa2_eth_setup_mqprio(struct net_device *net_dev,\n\t\t\t\t  struct tc_mqprio_qopt *mqprio)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tu8 num_tc, num_queues;\n\tint i;\n\n\tmqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;\n\tnum_queues = dpaa2_eth_queue_count(priv);\n\tnum_tc = mqprio->num_tc;\n\n\tif (num_tc == net_dev->num_tc)\n\t\treturn 0;\n\n\tif (num_tc  > dpaa2_eth_tc_count(priv)) {\n\t\tnetdev_err(net_dev, \"Max %d traffic classes supported\\n\",\n\t\t\t   dpaa2_eth_tc_count(priv));\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!num_tc) {\n\t\tnetdev_reset_tc(net_dev);\n\t\tnetif_set_real_num_tx_queues(net_dev, num_queues);\n\t\tgoto out;\n\t}\n\n\tnetdev_set_num_tc(net_dev, num_tc);\n\tnetif_set_real_num_tx_queues(net_dev, num_tc * num_queues);\n\n\tfor (i = 0; i < num_tc; i++)\n\t\tnetdev_set_tc_queue(net_dev, i, num_queues, i * num_queues);\n\nout:\n\tupdate_xps(priv);\n\n\treturn 0;\n}\n\n#define bps_to_mbits(rate) (div_u64((rate), 1000000) * 8)\n\nstatic int dpaa2_eth_setup_tbf(struct net_device *net_dev, struct tc_tbf_qopt_offload *p)\n{\n\tstruct tc_tbf_qopt_offload_replace_params *cfg = &p->replace_params;\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tstruct dpni_tx_shaping_cfg tx_cr_shaper = { 0 };\n\tstruct dpni_tx_shaping_cfg tx_er_shaper = { 0 };\n\tint err;\n\n\tif (p->command == TC_TBF_STATS)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (p->parent != TC_H_ROOT)\n\t\treturn -EOPNOTSUPP;\n\n\tif (p->command == TC_TBF_REPLACE) {\n\t\tif (cfg->max_size > DPAA2_ETH_MAX_BURST_SIZE) {\n\t\t\tnetdev_err(net_dev, \"burst size cannot be greater than %d\\n\",\n\t\t\t\t   DPAA2_ETH_MAX_BURST_SIZE);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\ttx_cr_shaper.max_burst_size = cfg->max_size;\n\t\t \n\t\ttx_cr_shaper.rate_limit = bps_to_mbits(cfg->rate.rate_bytes_ps);\n\t}\n\n\terr = dpni_set_tx_shaping(priv->mc_io, 0, priv->mc_token, &tx_cr_shaper,\n\t\t\t\t  &tx_er_shaper, 0);\n\tif (err) {\n\t\tnetdev_err(net_dev, \"dpni_set_tx_shaping() = %d\\n\", err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_setup_tc(struct net_device *net_dev,\n\t\t\t      enum tc_setup_type type, void *type_data)\n{\n\tswitch (type) {\n\tcase TC_SETUP_QDISC_MQPRIO:\n\t\treturn dpaa2_eth_setup_mqprio(net_dev, type_data);\n\tcase TC_SETUP_QDISC_TBF:\n\t\treturn dpaa2_eth_setup_tbf(net_dev, type_data);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic const struct net_device_ops dpaa2_eth_ops = {\n\t.ndo_open = dpaa2_eth_open,\n\t.ndo_start_xmit = dpaa2_eth_tx,\n\t.ndo_stop = dpaa2_eth_stop,\n\t.ndo_set_mac_address = dpaa2_eth_set_addr,\n\t.ndo_get_stats64 = dpaa2_eth_get_stats,\n\t.ndo_set_rx_mode = dpaa2_eth_set_rx_mode,\n\t.ndo_set_features = dpaa2_eth_set_features,\n\t.ndo_eth_ioctl = dpaa2_eth_ioctl,\n\t.ndo_change_mtu = dpaa2_eth_change_mtu,\n\t.ndo_bpf = dpaa2_eth_xdp,\n\t.ndo_xdp_xmit = dpaa2_eth_xdp_xmit,\n\t.ndo_xsk_wakeup = dpaa2_xsk_wakeup,\n\t.ndo_setup_tc = dpaa2_eth_setup_tc,\n\t.ndo_vlan_rx_add_vid = dpaa2_eth_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid = dpaa2_eth_rx_kill_vid\n};\n\nstatic void dpaa2_eth_cdan_cb(struct dpaa2_io_notification_ctx *ctx)\n{\n\tstruct dpaa2_eth_channel *ch;\n\n\tch = container_of(ctx, struct dpaa2_eth_channel, nctx);\n\n\t \n\tch->stats.cdan++;\n\n\t \n\tif (!napi_if_scheduled_mark_missed(&ch->napi))\n\t\tnapi_schedule(&ch->napi);\n}\n\n \nstatic struct fsl_mc_device *dpaa2_eth_setup_dpcon(struct dpaa2_eth_priv *priv)\n{\n\tstruct fsl_mc_device *dpcon;\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tint err;\n\n\terr = fsl_mc_object_allocate(to_fsl_mc_device(dev),\n\t\t\t\t     FSL_MC_POOL_DPCON, &dpcon);\n\tif (err) {\n\t\tif (err == -ENXIO) {\n\t\t\tdev_dbg(dev, \"Waiting for DPCON\\n\");\n\t\t\terr = -EPROBE_DEFER;\n\t\t} else {\n\t\t\tdev_info(dev, \"Not enough DPCONs, will go on as-is\\n\");\n\t\t}\n\t\treturn ERR_PTR(err);\n\t}\n\n\terr = dpcon_open(priv->mc_io, 0, dpcon->obj_desc.id, &dpcon->mc_handle);\n\tif (err) {\n\t\tdev_err(dev, \"dpcon_open() failed\\n\");\n\t\tgoto free;\n\t}\n\n\terr = dpcon_reset(priv->mc_io, 0, dpcon->mc_handle);\n\tif (err) {\n\t\tdev_err(dev, \"dpcon_reset() failed\\n\");\n\t\tgoto close;\n\t}\n\n\terr = dpcon_enable(priv->mc_io, 0, dpcon->mc_handle);\n\tif (err) {\n\t\tdev_err(dev, \"dpcon_enable() failed\\n\");\n\t\tgoto close;\n\t}\n\n\treturn dpcon;\n\nclose:\n\tdpcon_close(priv->mc_io, 0, dpcon->mc_handle);\nfree:\n\tfsl_mc_object_free(dpcon);\n\n\treturn ERR_PTR(err);\n}\n\nstatic void dpaa2_eth_free_dpcon(struct dpaa2_eth_priv *priv,\n\t\t\t\t struct fsl_mc_device *dpcon)\n{\n\tdpcon_disable(priv->mc_io, 0, dpcon->mc_handle);\n\tdpcon_close(priv->mc_io, 0, dpcon->mc_handle);\n\tfsl_mc_object_free(dpcon);\n}\n\nstatic struct dpaa2_eth_channel *dpaa2_eth_alloc_channel(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_eth_channel *channel;\n\tstruct dpcon_attr attr;\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tint err;\n\n\tchannel = kzalloc(sizeof(*channel), GFP_KERNEL);\n\tif (!channel)\n\t\treturn NULL;\n\n\tchannel->dpcon = dpaa2_eth_setup_dpcon(priv);\n\tif (IS_ERR(channel->dpcon)) {\n\t\terr = PTR_ERR(channel->dpcon);\n\t\tgoto err_setup;\n\t}\n\n\terr = dpcon_get_attributes(priv->mc_io, 0, channel->dpcon->mc_handle,\n\t\t\t\t   &attr);\n\tif (err) {\n\t\tdev_err(dev, \"dpcon_get_attributes() failed\\n\");\n\t\tgoto err_get_attr;\n\t}\n\n\tchannel->dpcon_id = attr.id;\n\tchannel->ch_id = attr.qbman_ch_id;\n\tchannel->priv = priv;\n\n\treturn channel;\n\nerr_get_attr:\n\tdpaa2_eth_free_dpcon(priv, channel->dpcon);\nerr_setup:\n\tkfree(channel);\n\treturn ERR_PTR(err);\n}\n\nstatic void dpaa2_eth_free_channel(struct dpaa2_eth_priv *priv,\n\t\t\t\t   struct dpaa2_eth_channel *channel)\n{\n\tdpaa2_eth_free_dpcon(priv, channel->dpcon);\n\tkfree(channel);\n}\n\n \nstatic int dpaa2_eth_setup_dpio(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_io_notification_ctx *nctx;\n\tstruct dpaa2_eth_channel *channel;\n\tstruct dpcon_notification_cfg dpcon_notif_cfg;\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tint i, err;\n\n\t \n\tcpumask_clear(&priv->dpio_cpumask);\n\tfor_each_online_cpu(i) {\n\t\t \n\t\tchannel = dpaa2_eth_alloc_channel(priv);\n\t\tif (IS_ERR_OR_NULL(channel)) {\n\t\t\terr = PTR_ERR_OR_ZERO(channel);\n\t\t\tif (err == -EPROBE_DEFER)\n\t\t\t\tdev_dbg(dev, \"waiting for affine channel\\n\");\n\t\t\telse\n\t\t\t\tdev_info(dev,\n\t\t\t\t\t \"No affine channel for cpu %d and above\\n\", i);\n\t\t\tgoto err_alloc_ch;\n\t\t}\n\n\t\tpriv->channel[priv->num_channels] = channel;\n\n\t\tnctx = &channel->nctx;\n\t\tnctx->is_cdan = 1;\n\t\tnctx->cb = dpaa2_eth_cdan_cb;\n\t\tnctx->id = channel->ch_id;\n\t\tnctx->desired_cpu = i;\n\n\t\t \n\t\tchannel->dpio = dpaa2_io_service_select(i);\n\t\terr = dpaa2_io_service_register(channel->dpio, nctx, dev);\n\t\tif (err) {\n\t\t\tdev_dbg(dev, \"No affine DPIO for cpu %d\\n\", i);\n\t\t\t \n\t\t\terr = -EPROBE_DEFER;\n\t\t\tgoto err_service_reg;\n\t\t}\n\n\t\t \n\t\tdpcon_notif_cfg.dpio_id = nctx->dpio_id;\n\t\tdpcon_notif_cfg.priority = 0;\n\t\tdpcon_notif_cfg.user_ctx = nctx->qman64;\n\t\terr = dpcon_set_notification(priv->mc_io, 0,\n\t\t\t\t\t     channel->dpcon->mc_handle,\n\t\t\t\t\t     &dpcon_notif_cfg);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"dpcon_set_notification failed()\\n\");\n\t\t\tgoto err_set_cdan;\n\t\t}\n\n\t\t \n\t\tcpumask_set_cpu(i, &priv->dpio_cpumask);\n\t\tpriv->num_channels++;\n\n\t\t \n\t\tif (priv->num_channels == priv->dpni_attrs.num_queues)\n\t\t\tbreak;\n\t}\n\n\treturn 0;\n\nerr_set_cdan:\n\tdpaa2_io_service_deregister(channel->dpio, nctx, dev);\nerr_service_reg:\n\tdpaa2_eth_free_channel(priv, channel);\nerr_alloc_ch:\n\tif (err == -EPROBE_DEFER) {\n\t\tfor (i = 0; i < priv->num_channels; i++) {\n\t\t\tchannel = priv->channel[i];\n\t\t\tnctx = &channel->nctx;\n\t\t\tdpaa2_io_service_deregister(channel->dpio, nctx, dev);\n\t\t\tdpaa2_eth_free_channel(priv, channel);\n\t\t}\n\t\tpriv->num_channels = 0;\n\t\treturn err;\n\t}\n\n\tif (cpumask_empty(&priv->dpio_cpumask)) {\n\t\tdev_err(dev, \"No cpu with an affine DPIO/DPCON\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tdev_info(dev, \"Cores %*pbl available for processing ingress traffic\\n\",\n\t\t cpumask_pr_args(&priv->dpio_cpumask));\n\n\treturn 0;\n}\n\nstatic void dpaa2_eth_free_dpio(struct dpaa2_eth_priv *priv)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpaa2_eth_channel *ch;\n\tint i;\n\n\t \n\tfor (i = 0; i < priv->num_channels; i++) {\n\t\tch = priv->channel[i];\n\t\tdpaa2_io_service_deregister(ch->dpio, &ch->nctx, dev);\n\t\tdpaa2_eth_free_channel(priv, ch);\n\t}\n}\n\nstatic struct dpaa2_eth_channel *dpaa2_eth_get_affine_channel(struct dpaa2_eth_priv *priv,\n\t\t\t\t\t\t\t      int cpu)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tint i;\n\n\tfor (i = 0; i < priv->num_channels; i++)\n\t\tif (priv->channel[i]->nctx.desired_cpu == cpu)\n\t\t\treturn priv->channel[i];\n\n\t \n\tdev_warn(dev, \"No affine channel found for cpu %d\\n\", cpu);\n\n\treturn priv->channel[0];\n}\n\nstatic void dpaa2_eth_set_fq_affinity(struct dpaa2_eth_priv *priv)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpaa2_eth_fq *fq;\n\tint rx_cpu, txc_cpu;\n\tint i;\n\n\t \n\trx_cpu = txc_cpu = cpumask_first(&priv->dpio_cpumask);\n\n\tfor (i = 0; i < priv->num_fqs; i++) {\n\t\tfq = &priv->fq[i];\n\t\tswitch (fq->type) {\n\t\tcase DPAA2_RX_FQ:\n\t\tcase DPAA2_RX_ERR_FQ:\n\t\t\tfq->target_cpu = rx_cpu;\n\t\t\trx_cpu = cpumask_next(rx_cpu, &priv->dpio_cpumask);\n\t\t\tif (rx_cpu >= nr_cpu_ids)\n\t\t\t\trx_cpu = cpumask_first(&priv->dpio_cpumask);\n\t\t\tbreak;\n\t\tcase DPAA2_TX_CONF_FQ:\n\t\t\tfq->target_cpu = txc_cpu;\n\t\t\ttxc_cpu = cpumask_next(txc_cpu, &priv->dpio_cpumask);\n\t\t\tif (txc_cpu >= nr_cpu_ids)\n\t\t\t\ttxc_cpu = cpumask_first(&priv->dpio_cpumask);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(dev, \"Unknown FQ type: %d\\n\", fq->type);\n\t\t}\n\t\tfq->channel = dpaa2_eth_get_affine_channel(priv, fq->target_cpu);\n\t}\n\n\tupdate_xps(priv);\n}\n\nstatic void dpaa2_eth_setup_fqs(struct dpaa2_eth_priv *priv)\n{\n\tint i, j;\n\n\t \n\tfor (i = 0; i < dpaa2_eth_queue_count(priv); i++) {\n\t\tpriv->fq[priv->num_fqs].type = DPAA2_TX_CONF_FQ;\n\t\tpriv->fq[priv->num_fqs].consume = dpaa2_eth_tx_conf;\n\t\tpriv->fq[priv->num_fqs++].flowid = (u16)i;\n\t}\n\n\tfor (j = 0; j < dpaa2_eth_tc_count(priv); j++) {\n\t\tfor (i = 0; i < dpaa2_eth_queue_count(priv); i++) {\n\t\t\tpriv->fq[priv->num_fqs].type = DPAA2_RX_FQ;\n\t\t\tpriv->fq[priv->num_fqs].consume = dpaa2_eth_rx;\n\t\t\tpriv->fq[priv->num_fqs].tc = (u8)j;\n\t\t\tpriv->fq[priv->num_fqs++].flowid = (u16)i;\n\t\t}\n\t}\n\n\t \n\tpriv->fq[priv->num_fqs].type = DPAA2_RX_ERR_FQ;\n\tpriv->fq[priv->num_fqs++].consume = dpaa2_eth_rx_err;\n\n\t \n\tdpaa2_eth_set_fq_affinity(priv);\n}\n\n \nstruct dpaa2_eth_bp *dpaa2_eth_allocate_dpbp(struct dpaa2_eth_priv *priv)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct fsl_mc_device *dpbp_dev;\n\tstruct dpbp_attr dpbp_attrs;\n\tstruct dpaa2_eth_bp *bp;\n\tint err;\n\n\terr = fsl_mc_object_allocate(to_fsl_mc_device(dev), FSL_MC_POOL_DPBP,\n\t\t\t\t     &dpbp_dev);\n\tif (err) {\n\t\tif (err == -ENXIO)\n\t\t\terr = -EPROBE_DEFER;\n\t\telse\n\t\t\tdev_err(dev, \"DPBP device allocation failed\\n\");\n\t\treturn ERR_PTR(err);\n\t}\n\n\tbp = kzalloc(sizeof(*bp), GFP_KERNEL);\n\tif (!bp) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc;\n\t}\n\n\terr = dpbp_open(priv->mc_io, 0, dpbp_dev->obj_desc.id,\n\t\t\t&dpbp_dev->mc_handle);\n\tif (err) {\n\t\tdev_err(dev, \"dpbp_open() failed\\n\");\n\t\tgoto err_open;\n\t}\n\n\terr = dpbp_reset(priv->mc_io, 0, dpbp_dev->mc_handle);\n\tif (err) {\n\t\tdev_err(dev, \"dpbp_reset() failed\\n\");\n\t\tgoto err_reset;\n\t}\n\n\terr = dpbp_enable(priv->mc_io, 0, dpbp_dev->mc_handle);\n\tif (err) {\n\t\tdev_err(dev, \"dpbp_enable() failed\\n\");\n\t\tgoto err_enable;\n\t}\n\n\terr = dpbp_get_attributes(priv->mc_io, 0, dpbp_dev->mc_handle,\n\t\t\t\t  &dpbp_attrs);\n\tif (err) {\n\t\tdev_err(dev, \"dpbp_get_attributes() failed\\n\");\n\t\tgoto err_get_attr;\n\t}\n\n\tbp->dev = dpbp_dev;\n\tbp->bpid = dpbp_attrs.bpid;\n\n\treturn bp;\n\nerr_get_attr:\n\tdpbp_disable(priv->mc_io, 0, dpbp_dev->mc_handle);\nerr_enable:\nerr_reset:\n\tdpbp_close(priv->mc_io, 0, dpbp_dev->mc_handle);\nerr_open:\n\tkfree(bp);\nerr_alloc:\n\tfsl_mc_object_free(dpbp_dev);\n\n\treturn ERR_PTR(err);\n}\n\nstatic int dpaa2_eth_setup_default_dpbp(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_eth_bp *bp;\n\tint i;\n\n\tbp = dpaa2_eth_allocate_dpbp(priv);\n\tif (IS_ERR(bp))\n\t\treturn PTR_ERR(bp);\n\n\tpriv->bp[DPAA2_ETH_DEFAULT_BP_IDX] = bp;\n\tpriv->num_bps++;\n\n\tfor (i = 0; i < priv->num_channels; i++)\n\t\tpriv->channel[i]->bp = bp;\n\n\treturn 0;\n}\n\nvoid dpaa2_eth_free_dpbp(struct dpaa2_eth_priv *priv, struct dpaa2_eth_bp *bp)\n{\n\tint idx_bp;\n\n\t \n\tfor (idx_bp = 0; idx_bp < priv->num_bps; idx_bp++)\n\t\tif (priv->bp[idx_bp] == bp)\n\t\t\tbreak;\n\n\t \n\tdpaa2_eth_drain_pool(priv, bp->bpid);\n\tdpbp_disable(priv->mc_io, 0, bp->dev->mc_handle);\n\tdpbp_close(priv->mc_io, 0, bp->dev->mc_handle);\n\tfsl_mc_object_free(bp->dev);\n\tkfree(bp);\n\n\t \n\tpriv->bp[idx_bp] = priv->bp[priv->num_bps - 1];\n\tpriv->num_bps--;\n}\n\nstatic void dpaa2_eth_free_dpbps(struct dpaa2_eth_priv *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_bps; i++)\n\t\tdpaa2_eth_free_dpbp(priv, priv->bp[i]);\n}\n\nstatic int dpaa2_eth_set_buffer_layout(struct dpaa2_eth_priv *priv)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpni_buffer_layout buf_layout = {0};\n\tu16 rx_buf_align;\n\tint err;\n\n\t \n\tif (priv->dpni_attrs.wriop_version == DPAA2_WRIOP_VERSION(0, 0, 0) ||\n\t    priv->dpni_attrs.wriop_version == DPAA2_WRIOP_VERSION(1, 0, 0))\n\t\trx_buf_align = DPAA2_ETH_RX_BUF_ALIGN_REV1;\n\telse\n\t\trx_buf_align = DPAA2_ETH_RX_BUF_ALIGN;\n\n\t \n\tpriv->rx_buf_size = ALIGN_DOWN(DPAA2_ETH_RX_BUF_SIZE, rx_buf_align);\n\n\t \n\tbuf_layout.private_data_size = DPAA2_ETH_SWA_SIZE;\n\tbuf_layout.pass_timestamp = true;\n\tbuf_layout.pass_frame_status = true;\n\tbuf_layout.options = DPNI_BUF_LAYOUT_OPT_PRIVATE_DATA_SIZE |\n\t\t\t     DPNI_BUF_LAYOUT_OPT_TIMESTAMP |\n\t\t\t     DPNI_BUF_LAYOUT_OPT_FRAME_STATUS;\n\terr = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t     DPNI_QUEUE_TX, &buf_layout);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_buffer_layout(TX) failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tbuf_layout.options = DPNI_BUF_LAYOUT_OPT_TIMESTAMP |\n\t\t\t     DPNI_BUF_LAYOUT_OPT_FRAME_STATUS;\n\terr = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t     DPNI_QUEUE_TX_CONFIRM, &buf_layout);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_buffer_layout(TX_CONF) failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\terr = dpni_get_tx_data_offset(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t      &priv->tx_data_offset);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_tx_data_offset() failed\\n\");\n\t\treturn err;\n\t}\n\n\tif ((priv->tx_data_offset % 64) != 0)\n\t\tdev_warn(dev, \"Tx data offset (%d) not a multiple of 64B\\n\",\n\t\t\t priv->tx_data_offset);\n\n\t \n\tbuf_layout.pass_frame_status = true;\n\tbuf_layout.pass_parser_result = true;\n\tbuf_layout.data_align = rx_buf_align;\n\tbuf_layout.data_head_room = dpaa2_eth_rx_head_room(priv);\n\tbuf_layout.private_data_size = 0;\n\tbuf_layout.options = DPNI_BUF_LAYOUT_OPT_PARSER_RESULT |\n\t\t\t     DPNI_BUF_LAYOUT_OPT_FRAME_STATUS |\n\t\t\t     DPNI_BUF_LAYOUT_OPT_DATA_ALIGN |\n\t\t\t     DPNI_BUF_LAYOUT_OPT_DATA_HEAD_ROOM |\n\t\t\t     DPNI_BUF_LAYOUT_OPT_TIMESTAMP;\n\terr = dpni_set_buffer_layout(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t     DPNI_QUEUE_RX, &buf_layout);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_buffer_layout(RX) failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n#define DPNI_ENQUEUE_FQID_VER_MAJOR\t7\n#define DPNI_ENQUEUE_FQID_VER_MINOR\t9\n\nstatic inline int dpaa2_eth_enqueue_qd(struct dpaa2_eth_priv *priv,\n\t\t\t\t       struct dpaa2_eth_fq *fq,\n\t\t\t\t       struct dpaa2_fd *fd, u8 prio,\n\t\t\t\t       u32 num_frames __always_unused,\n\t\t\t\t       int *frames_enqueued)\n{\n\tint err;\n\n\terr = dpaa2_io_service_enqueue_qd(fq->channel->dpio,\n\t\t\t\t\t  priv->tx_qdid, prio,\n\t\t\t\t\t  fq->tx_qdbin, fd);\n\tif (!err && frames_enqueued)\n\t\t*frames_enqueued = 1;\n\treturn err;\n}\n\nstatic inline int dpaa2_eth_enqueue_fq_multiple(struct dpaa2_eth_priv *priv,\n\t\t\t\t\t\tstruct dpaa2_eth_fq *fq,\n\t\t\t\t\t\tstruct dpaa2_fd *fd,\n\t\t\t\t\t\tu8 prio, u32 num_frames,\n\t\t\t\t\t\tint *frames_enqueued)\n{\n\tint err;\n\n\terr = dpaa2_io_service_enqueue_multiple_fq(fq->channel->dpio,\n\t\t\t\t\t\t   fq->tx_fqid[prio],\n\t\t\t\t\t\t   fd, num_frames);\n\n\tif (err == 0)\n\t\treturn -EBUSY;\n\n\tif (frames_enqueued)\n\t\t*frames_enqueued = err;\n\treturn 0;\n}\n\nstatic void dpaa2_eth_set_enqueue_mode(struct dpaa2_eth_priv *priv)\n{\n\tif (dpaa2_eth_cmp_dpni_ver(priv, DPNI_ENQUEUE_FQID_VER_MAJOR,\n\t\t\t\t   DPNI_ENQUEUE_FQID_VER_MINOR) < 0)\n\t\tpriv->enqueue = dpaa2_eth_enqueue_qd;\n\telse\n\t\tpriv->enqueue = dpaa2_eth_enqueue_fq_multiple;\n}\n\nstatic int dpaa2_eth_set_pause(struct dpaa2_eth_priv *priv)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpni_link_cfg link_cfg = {0};\n\tint err;\n\n\t \n\terr = dpni_get_link_cfg(priv->mc_io, 0, priv->mc_token, &link_cfg);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_link_cfg() failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tlink_cfg.options |= DPNI_LINK_OPT_PAUSE;\n\tlink_cfg.options &= ~DPNI_LINK_OPT_ASYM_PAUSE;\n\terr = dpni_set_link_cfg(priv->mc_io, 0, priv->mc_token, &link_cfg);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_link_cfg() failed\\n\");\n\t\treturn err;\n\t}\n\n\tpriv->link_state.options = link_cfg.options;\n\n\treturn 0;\n}\n\nstatic void dpaa2_eth_update_tx_fqids(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpni_queue_id qid = {0};\n\tstruct dpaa2_eth_fq *fq;\n\tstruct dpni_queue queue;\n\tint i, j, err;\n\n\t \n\tif (dpaa2_eth_cmp_dpni_ver(priv, DPNI_ENQUEUE_FQID_VER_MAJOR,\n\t\t\t\t   DPNI_ENQUEUE_FQID_VER_MINOR) < 0)\n\t\treturn;\n\n\tfor (i = 0; i < priv->num_fqs; i++) {\n\t\tfq = &priv->fq[i];\n\t\tif (fq->type != DPAA2_TX_CONF_FQ)\n\t\t\tcontinue;\n\t\tfor (j = 0; j < dpaa2_eth_tc_count(priv); j++) {\n\t\t\terr = dpni_get_queue(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\t     DPNI_QUEUE_TX, j, fq->flowid,\n\t\t\t\t\t     &queue, &qid);\n\t\t\tif (err)\n\t\t\t\tgoto out_err;\n\n\t\t\tfq->tx_fqid[j] = qid.fqid;\n\t\t\tif (fq->tx_fqid[j] == 0)\n\t\t\t\tgoto out_err;\n\t\t}\n\t}\n\n\tpriv->enqueue = dpaa2_eth_enqueue_fq_multiple;\n\n\treturn;\n\nout_err:\n\tnetdev_info(priv->net_dev,\n\t\t    \"Error reading Tx FQID, fallback to QDID-based enqueue\\n\");\n\tpriv->enqueue = dpaa2_eth_enqueue_qd;\n}\n\n \nstatic int dpaa2_eth_set_vlan_qos(struct dpaa2_eth_priv *priv)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpkg_profile_cfg kg_cfg = {0};\n\tstruct dpni_qos_tbl_cfg qos_cfg = {0};\n\tstruct dpni_rule_cfg key_params;\n\tvoid *dma_mem, *key, *mask;\n\tu8 key_size = 2;\t \n\tint i, pcp, err;\n\n\t \n\tif (dpaa2_eth_tc_count(priv) == 1 || !dpaa2_eth_fs_mask_enabled(priv)) {\n\t\tdev_dbg(dev, \"VLAN-based QoS classification not supported\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tdma_mem = kzalloc(DPAA2_CLASSIFIER_DMA_SIZE, GFP_KERNEL);\n\tif (!dma_mem)\n\t\treturn -ENOMEM;\n\n\tkg_cfg.num_extracts = 1;\n\tkg_cfg.extracts[0].type = DPKG_EXTRACT_FROM_HDR;\n\tkg_cfg.extracts[0].extract.from_hdr.prot = NET_PROT_VLAN;\n\tkg_cfg.extracts[0].extract.from_hdr.type = DPKG_FULL_FIELD;\n\tkg_cfg.extracts[0].extract.from_hdr.field = NH_FLD_VLAN_TCI;\n\n\terr = dpni_prepare_key_cfg(&kg_cfg, dma_mem);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_prepare_key_cfg failed\\n\");\n\t\tgoto out_free_tbl;\n\t}\n\n\t \n\tqos_cfg.default_tc = 0;\n\tqos_cfg.discard_on_miss = 0;\n\tqos_cfg.key_cfg_iova = dma_map_single(dev, dma_mem,\n\t\t\t\t\t      DPAA2_CLASSIFIER_DMA_SIZE,\n\t\t\t\t\t      DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, qos_cfg.key_cfg_iova)) {\n\t\tdev_err(dev, \"QoS table DMA mapping failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto out_free_tbl;\n\t}\n\n\terr = dpni_set_qos_table(priv->mc_io, 0, priv->mc_token, &qos_cfg);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_qos_table failed\\n\");\n\t\tgoto out_unmap_tbl;\n\t}\n\n\t \n\tkey = kzalloc(key_size * 2, GFP_KERNEL);\n\tif (!key) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unmap_tbl;\n\t}\n\tmask = key + key_size;\n\t*(__be16 *)mask = cpu_to_be16(VLAN_PRIO_MASK);\n\n\tkey_params.key_iova = dma_map_single(dev, key, key_size * 2,\n\t\t\t\t\t     DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, key_params.key_iova)) {\n\t\tdev_err(dev, \"Qos table entry DMA mapping failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto out_free_key;\n\t}\n\n\tkey_params.mask_iova = key_params.key_iova + key_size;\n\tkey_params.key_size = key_size;\n\n\t \n\tfor (i = dpaa2_eth_tc_count(priv) - 1, pcp = 7; i >= 0; i--, pcp--) {\n\t\t*(__be16 *)key = cpu_to_be16(pcp << VLAN_PRIO_SHIFT);\n\t\tdma_sync_single_for_device(dev, key_params.key_iova,\n\t\t\t\t\t   key_size * 2, DMA_TO_DEVICE);\n\n\t\terr = dpni_add_qos_entry(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\t &key_params, i, i);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"dpni_add_qos_entry failed\\n\");\n\t\t\tdpni_clear_qos_table(priv->mc_io, 0, priv->mc_token);\n\t\t\tgoto out_unmap_key;\n\t\t}\n\t}\n\n\tpriv->vlan_cls_enabled = true;\n\n\t \nout_unmap_key:\n\tdma_unmap_single(dev, key_params.key_iova, key_size * 2, DMA_TO_DEVICE);\nout_free_key:\n\tkfree(key);\nout_unmap_tbl:\n\tdma_unmap_single(dev, qos_cfg.key_cfg_iova, DPAA2_CLASSIFIER_DMA_SIZE,\n\t\t\t DMA_TO_DEVICE);\nout_free_tbl:\n\tkfree(dma_mem);\n\n\treturn err;\n}\n\n \nstatic int dpaa2_eth_setup_dpni(struct fsl_mc_device *ls_dev)\n{\n\tstruct device *dev = &ls_dev->dev;\n\tstruct dpaa2_eth_priv *priv;\n\tstruct net_device *net_dev;\n\tint err;\n\n\tnet_dev = dev_get_drvdata(dev);\n\tpriv = netdev_priv(net_dev);\n\n\t \n\terr = dpni_open(priv->mc_io, 0, ls_dev->obj_desc.id, &priv->mc_token);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_open() failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\terr = dpni_get_api_version(priv->mc_io, 0, &priv->dpni_ver_major,\n\t\t\t\t   &priv->dpni_ver_minor);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_api_version() failed\\n\");\n\t\tgoto close;\n\t}\n\tif (dpaa2_eth_cmp_dpni_ver(priv, DPNI_VER_MAJOR, DPNI_VER_MINOR) < 0) {\n\t\tdev_err(dev, \"DPNI version %u.%u not supported, need >= %u.%u\\n\",\n\t\t\tpriv->dpni_ver_major, priv->dpni_ver_minor,\n\t\t\tDPNI_VER_MAJOR, DPNI_VER_MINOR);\n\t\terr = -EOPNOTSUPP;\n\t\tgoto close;\n\t}\n\n\tls_dev->mc_io = priv->mc_io;\n\tls_dev->mc_handle = priv->mc_token;\n\n\terr = dpni_reset(priv->mc_io, 0, priv->mc_token);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_reset() failed\\n\");\n\t\tgoto close;\n\t}\n\n\terr = dpni_get_attributes(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t  &priv->dpni_attrs);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_attributes() failed (err=%d)\\n\", err);\n\t\tgoto close;\n\t}\n\n\terr = dpaa2_eth_set_buffer_layout(priv);\n\tif (err)\n\t\tgoto close;\n\n\tdpaa2_eth_set_enqueue_mode(priv);\n\n\t \n\tif (dpaa2_eth_has_pause_support(priv)) {\n\t\terr = dpaa2_eth_set_pause(priv);\n\t\tif (err)\n\t\t\tgoto close;\n\t}\n\n\terr = dpaa2_eth_set_vlan_qos(priv);\n\tif (err && err != -EOPNOTSUPP)\n\t\tgoto close;\n\n\tpriv->cls_rules = devm_kcalloc(dev, dpaa2_eth_fs_count(priv),\n\t\t\t\t       sizeof(struct dpaa2_eth_cls_rule),\n\t\t\t\t       GFP_KERNEL);\n\tif (!priv->cls_rules) {\n\t\terr = -ENOMEM;\n\t\tgoto close;\n\t}\n\n\treturn 0;\n\nclose:\n\tdpni_close(priv->mc_io, 0, priv->mc_token);\n\n\treturn err;\n}\n\nstatic void dpaa2_eth_free_dpni(struct dpaa2_eth_priv *priv)\n{\n\tint err;\n\n\terr = dpni_reset(priv->mc_io, 0, priv->mc_token);\n\tif (err)\n\t\tnetdev_warn(priv->net_dev, \"dpni_reset() failed (err %d)\\n\",\n\t\t\t    err);\n\n\tdpni_close(priv->mc_io, 0, priv->mc_token);\n}\n\nstatic int dpaa2_eth_setup_rx_flow(struct dpaa2_eth_priv *priv,\n\t\t\t\t   struct dpaa2_eth_fq *fq)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpni_queue queue;\n\tstruct dpni_queue_id qid;\n\tint err;\n\n\terr = dpni_get_queue(priv->mc_io, 0, priv->mc_token,\n\t\t\t     DPNI_QUEUE_RX, fq->tc, fq->flowid, &queue, &qid);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_queue(RX) failed\\n\");\n\t\treturn err;\n\t}\n\n\tfq->fqid = qid.fqid;\n\n\tqueue.destination.id = fq->channel->dpcon_id;\n\tqueue.destination.type = DPNI_DEST_DPCON;\n\tqueue.destination.priority = 1;\n\tqueue.user_context = (u64)(uintptr_t)fq;\n\terr = dpni_set_queue(priv->mc_io, 0, priv->mc_token,\n\t\t\t     DPNI_QUEUE_RX, fq->tc, fq->flowid,\n\t\t\t     DPNI_QUEUE_OPT_USER_CTX | DPNI_QUEUE_OPT_DEST,\n\t\t\t     &queue);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_queue(RX) failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\t \n\tif (fq->tc > 0)\n\t\treturn 0;\n\n\terr = xdp_rxq_info_reg(&fq->channel->xdp_rxq, priv->net_dev,\n\t\t\t       fq->flowid, 0);\n\tif (err) {\n\t\tdev_err(dev, \"xdp_rxq_info_reg failed\\n\");\n\t\treturn err;\n\t}\n\n\terr = xdp_rxq_info_reg_mem_model(&fq->channel->xdp_rxq,\n\t\t\t\t\t MEM_TYPE_PAGE_ORDER0, NULL);\n\tif (err) {\n\t\tdev_err(dev, \"xdp_rxq_info_reg_mem_model failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_setup_tx_flow(struct dpaa2_eth_priv *priv,\n\t\t\t\t   struct dpaa2_eth_fq *fq)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpni_queue queue;\n\tstruct dpni_queue_id qid;\n\tint i, err;\n\n\tfor (i = 0; i < dpaa2_eth_tc_count(priv); i++) {\n\t\terr = dpni_get_queue(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t     DPNI_QUEUE_TX, i, fq->flowid,\n\t\t\t\t     &queue, &qid);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"dpni_get_queue(TX) failed\\n\");\n\t\t\treturn err;\n\t\t}\n\t\tfq->tx_fqid[i] = qid.fqid;\n\t}\n\n\t \n\tfq->tx_qdbin = qid.qdbin;\n\n\terr = dpni_get_queue(priv->mc_io, 0, priv->mc_token,\n\t\t\t     DPNI_QUEUE_TX_CONFIRM, 0, fq->flowid,\n\t\t\t     &queue, &qid);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_queue(TX_CONF) failed\\n\");\n\t\treturn err;\n\t}\n\n\tfq->fqid = qid.fqid;\n\n\tqueue.destination.id = fq->channel->dpcon_id;\n\tqueue.destination.type = DPNI_DEST_DPCON;\n\tqueue.destination.priority = 0;\n\tqueue.user_context = (u64)(uintptr_t)fq;\n\terr = dpni_set_queue(priv->mc_io, 0, priv->mc_token,\n\t\t\t     DPNI_QUEUE_TX_CONFIRM, 0, fq->flowid,\n\t\t\t     DPNI_QUEUE_OPT_USER_CTX | DPNI_QUEUE_OPT_DEST,\n\t\t\t     &queue);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_queue(TX_CONF) failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int setup_rx_err_flow(struct dpaa2_eth_priv *priv,\n\t\t\t     struct dpaa2_eth_fq *fq)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpni_queue q = { { 0 } };\n\tstruct dpni_queue_id qid;\n\tu8 q_opt = DPNI_QUEUE_OPT_USER_CTX | DPNI_QUEUE_OPT_DEST;\n\tint err;\n\n\terr = dpni_get_queue(priv->mc_io, 0, priv->mc_token,\n\t\t\t     DPNI_QUEUE_RX_ERR, 0, 0, &q, &qid);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_queue() failed (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\tfq->fqid = qid.fqid;\n\n\tq.destination.id = fq->channel->dpcon_id;\n\tq.destination.type = DPNI_DEST_DPCON;\n\tq.destination.priority = 1;\n\tq.user_context = (u64)(uintptr_t)fq;\n\terr = dpni_set_queue(priv->mc_io, 0, priv->mc_token,\n\t\t\t     DPNI_QUEUE_RX_ERR, 0, 0, q_opt, &q);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_queue() failed (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic const struct dpaa2_eth_dist_fields dist_fields[] = {\n\t{\n\t\t \n\t\t.rxnfc_field = RXH_L2DA,\n\t\t.cls_prot = NET_PROT_ETH,\n\t\t.cls_field = NH_FLD_ETH_DA,\n\t\t.id = DPAA2_ETH_DIST_ETHDST,\n\t\t.size = 6,\n\t}, {\n\t\t.cls_prot = NET_PROT_ETH,\n\t\t.cls_field = NH_FLD_ETH_SA,\n\t\t.id = DPAA2_ETH_DIST_ETHSRC,\n\t\t.size = 6,\n\t}, {\n\t\t \n\t\t.cls_prot = NET_PROT_ETH,\n\t\t.cls_field = NH_FLD_ETH_TYPE,\n\t\t.id = DPAA2_ETH_DIST_ETHTYPE,\n\t\t.size = 2,\n\t}, {\n\t\t \n\t\t.rxnfc_field = RXH_VLAN,\n\t\t.cls_prot = NET_PROT_VLAN,\n\t\t.cls_field = NH_FLD_VLAN_TCI,\n\t\t.id = DPAA2_ETH_DIST_VLAN,\n\t\t.size = 2,\n\t}, {\n\t\t \n\t\t.rxnfc_field = RXH_IP_SRC,\n\t\t.cls_prot = NET_PROT_IP,\n\t\t.cls_field = NH_FLD_IP_SRC,\n\t\t.id = DPAA2_ETH_DIST_IPSRC,\n\t\t.size = 4,\n\t}, {\n\t\t.rxnfc_field = RXH_IP_DST,\n\t\t.cls_prot = NET_PROT_IP,\n\t\t.cls_field = NH_FLD_IP_DST,\n\t\t.id = DPAA2_ETH_DIST_IPDST,\n\t\t.size = 4,\n\t}, {\n\t\t.rxnfc_field = RXH_L3_PROTO,\n\t\t.cls_prot = NET_PROT_IP,\n\t\t.cls_field = NH_FLD_IP_PROTO,\n\t\t.id = DPAA2_ETH_DIST_IPPROTO,\n\t\t.size = 1,\n\t}, {\n\t\t \n\t\t.rxnfc_field = RXH_L4_B_0_1,\n\t\t.cls_prot = NET_PROT_UDP,\n\t\t.cls_field = NH_FLD_UDP_PORT_SRC,\n\t\t.id = DPAA2_ETH_DIST_L4SRC,\n\t\t.size = 2,\n\t}, {\n\t\t.rxnfc_field = RXH_L4_B_2_3,\n\t\t.cls_prot = NET_PROT_UDP,\n\t\t.cls_field = NH_FLD_UDP_PORT_DST,\n\t\t.id = DPAA2_ETH_DIST_L4DST,\n\t\t.size = 2,\n\t},\n};\n\n \nstatic int dpaa2_eth_config_legacy_hash_key(struct dpaa2_eth_priv *priv, dma_addr_t key)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpni_rx_tc_dist_cfg dist_cfg;\n\tint i, err = 0;\n\n\tmemset(&dist_cfg, 0, sizeof(dist_cfg));\n\n\tdist_cfg.key_cfg_iova = key;\n\tdist_cfg.dist_size = dpaa2_eth_queue_count(priv);\n\tdist_cfg.dist_mode = DPNI_DIST_MODE_HASH;\n\n\tfor (i = 0; i < dpaa2_eth_tc_count(priv); i++) {\n\t\terr = dpni_set_rx_tc_dist(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\t  i, &dist_cfg);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"dpni_set_rx_tc_dist failed\\n\");\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn err;\n}\n\n \nstatic int dpaa2_eth_config_hash_key(struct dpaa2_eth_priv *priv, dma_addr_t key)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpni_rx_dist_cfg dist_cfg;\n\tint i, err = 0;\n\n\tmemset(&dist_cfg, 0, sizeof(dist_cfg));\n\n\tdist_cfg.key_cfg_iova = key;\n\tdist_cfg.dist_size = dpaa2_eth_queue_count(priv);\n\tdist_cfg.enable = 1;\n\n\tfor (i = 0; i < dpaa2_eth_tc_count(priv); i++) {\n\t\tdist_cfg.tc = i;\n\t\terr = dpni_set_rx_hash_dist(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\t    &dist_cfg);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"dpni_set_rx_hash_dist failed\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (priv->dpni_attrs.options & DPNI_OPT_SHARED_FS)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n \nstatic int dpaa2_eth_config_cls_key(struct dpaa2_eth_priv *priv, dma_addr_t key)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tstruct dpni_rx_dist_cfg dist_cfg;\n\tint i, err = 0;\n\n\tmemset(&dist_cfg, 0, sizeof(dist_cfg));\n\n\tdist_cfg.key_cfg_iova = key;\n\tdist_cfg.dist_size = dpaa2_eth_queue_count(priv);\n\tdist_cfg.enable = 1;\n\n\tfor (i = 0; i < dpaa2_eth_tc_count(priv); i++) {\n\t\tdist_cfg.tc = i;\n\t\terr = dpni_set_rx_fs_dist(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\t  &dist_cfg);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"dpni_set_rx_fs_dist failed\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (priv->dpni_attrs.options & DPNI_OPT_SHARED_FS)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n \nint dpaa2_eth_cls_key_size(u64 fields)\n{\n\tint i, size = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(dist_fields); i++) {\n\t\tif (!(fields & dist_fields[i].id))\n\t\t\tcontinue;\n\t\tsize += dist_fields[i].size;\n\t}\n\n\treturn size;\n}\n\n \nint dpaa2_eth_cls_fld_off(int prot, int field)\n{\n\tint i, off = 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(dist_fields); i++) {\n\t\tif (dist_fields[i].cls_prot == prot &&\n\t\t    dist_fields[i].cls_field == field)\n\t\t\treturn off;\n\t\toff += dist_fields[i].size;\n\t}\n\n\tWARN_ONCE(1, \"Unsupported header field used for Rx flow cls\\n\");\n\treturn 0;\n}\n\n \nvoid dpaa2_eth_cls_trim_rule(void *key_mem, u64 fields)\n{\n\tint off = 0, new_off = 0;\n\tint i, size;\n\n\tfor (i = 0; i < ARRAY_SIZE(dist_fields); i++) {\n\t\tsize = dist_fields[i].size;\n\t\tif (dist_fields[i].id & fields) {\n\t\t\tmemcpy(key_mem + new_off, key_mem + off, size);\n\t\t\tnew_off += size;\n\t\t}\n\t\toff += size;\n\t}\n}\n\n \nstatic int dpaa2_eth_set_dist_key(struct net_device *net_dev,\n\t\t\t\t  enum dpaa2_eth_rx_dist type, u64 flags)\n{\n\tstruct device *dev = net_dev->dev.parent;\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tstruct dpkg_profile_cfg cls_cfg;\n\tu32 rx_hash_fields = 0;\n\tdma_addr_t key_iova;\n\tu8 *dma_mem;\n\tint i;\n\tint err = 0;\n\n\tmemset(&cls_cfg, 0, sizeof(cls_cfg));\n\n\tfor (i = 0; i < ARRAY_SIZE(dist_fields); i++) {\n\t\tstruct dpkg_extract *key =\n\t\t\t&cls_cfg.extracts[cls_cfg.num_extracts];\n\n\t\t \n\t\tif (!(flags & dist_fields[i].id))\n\t\t\tcontinue;\n\t\tif (type == DPAA2_ETH_RX_DIST_HASH)\n\t\t\trx_hash_fields |= dist_fields[i].rxnfc_field;\n\n\t\tif (cls_cfg.num_extracts >= DPKG_MAX_NUM_OF_EXTRACTS) {\n\t\t\tdev_err(dev, \"error adding key extraction rule, too many rules?\\n\");\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\tkey->type = DPKG_EXTRACT_FROM_HDR;\n\t\tkey->extract.from_hdr.prot = dist_fields[i].cls_prot;\n\t\tkey->extract.from_hdr.type = DPKG_FULL_FIELD;\n\t\tkey->extract.from_hdr.field = dist_fields[i].cls_field;\n\t\tcls_cfg.num_extracts++;\n\t}\n\n\tdma_mem = kzalloc(DPAA2_CLASSIFIER_DMA_SIZE, GFP_KERNEL);\n\tif (!dma_mem)\n\t\treturn -ENOMEM;\n\n\terr = dpni_prepare_key_cfg(&cls_cfg, dma_mem);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_prepare_key_cfg error %d\\n\", err);\n\t\tgoto free_key;\n\t}\n\n\t \n\tkey_iova = dma_map_single(dev, dma_mem, DPAA2_CLASSIFIER_DMA_SIZE,\n\t\t\t\t  DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, key_iova)) {\n\t\tdev_err(dev, \"DMA mapping failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto free_key;\n\t}\n\n\tif (type == DPAA2_ETH_RX_DIST_HASH) {\n\t\tif (dpaa2_eth_has_legacy_dist(priv))\n\t\t\terr = dpaa2_eth_config_legacy_hash_key(priv, key_iova);\n\t\telse\n\t\t\terr = dpaa2_eth_config_hash_key(priv, key_iova);\n\t} else {\n\t\terr = dpaa2_eth_config_cls_key(priv, key_iova);\n\t}\n\n\tdma_unmap_single(dev, key_iova, DPAA2_CLASSIFIER_DMA_SIZE,\n\t\t\t DMA_TO_DEVICE);\n\tif (!err && type == DPAA2_ETH_RX_DIST_HASH)\n\t\tpriv->rx_hash_fields = rx_hash_fields;\n\nfree_key:\n\tkfree(dma_mem);\n\treturn err;\n}\n\nint dpaa2_eth_set_hash(struct net_device *net_dev, u64 flags)\n{\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tu64 key = 0;\n\tint i;\n\n\tif (!dpaa2_eth_hash_enabled(priv))\n\t\treturn -EOPNOTSUPP;\n\n\tfor (i = 0; i < ARRAY_SIZE(dist_fields); i++)\n\t\tif (dist_fields[i].rxnfc_field & flags)\n\t\t\tkey |= dist_fields[i].id;\n\n\treturn dpaa2_eth_set_dist_key(net_dev, DPAA2_ETH_RX_DIST_HASH, key);\n}\n\nint dpaa2_eth_set_cls(struct net_device *net_dev, u64 flags)\n{\n\treturn dpaa2_eth_set_dist_key(net_dev, DPAA2_ETH_RX_DIST_CLS, flags);\n}\n\nstatic int dpaa2_eth_set_default_cls(struct dpaa2_eth_priv *priv)\n{\n\tstruct device *dev = priv->net_dev->dev.parent;\n\tint err;\n\n\t \n\tif (dpaa2_eth_has_legacy_dist(priv)) {\n\t\tdev_dbg(dev, \"Rx cls not supported by current MC version\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!dpaa2_eth_fs_enabled(priv)) {\n\t\tdev_dbg(dev, \"Rx cls disabled in DPNI options\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!dpaa2_eth_hash_enabled(priv)) {\n\t\tdev_dbg(dev, \"Rx cls disabled for single queue DPNIs\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif (!dpaa2_eth_fs_mask_enabled(priv))\n\t\tgoto out;\n\n\terr = dpaa2_eth_set_cls(priv->net_dev, DPAA2_ETH_DIST_ALL);\n\tif (err)\n\t\treturn err;\n\nout:\n\tpriv->rx_cls_enabled = 1;\n\n\treturn 0;\n}\n\n \nstatic int dpaa2_eth_bind_dpni(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_eth_bp *bp = priv->bp[DPAA2_ETH_DEFAULT_BP_IDX];\n\tstruct net_device *net_dev = priv->net_dev;\n\tstruct dpni_pools_cfg pools_params = { 0 };\n\tstruct device *dev = net_dev->dev.parent;\n\tstruct dpni_error_cfg err_cfg;\n\tint err = 0;\n\tint i;\n\n\tpools_params.num_dpbp = 1;\n\tpools_params.pools[0].dpbp_id = bp->dev->obj_desc.id;\n\tpools_params.pools[0].backup_pool = 0;\n\tpools_params.pools[0].buffer_size = priv->rx_buf_size;\n\terr = dpni_set_pools(priv->mc_io, 0, priv->mc_token, &pools_params);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_pools() failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\terr = dpaa2_eth_set_hash(net_dev, DPAA2_RXH_DEFAULT);\n\tif (err && err != -EOPNOTSUPP)\n\t\tdev_err(dev, \"Failed to configure hashing\\n\");\n\n\t \n\terr = dpaa2_eth_set_default_cls(priv);\n\tif (err && err != -EOPNOTSUPP)\n\t\tdev_err(dev, \"Failed to configure Rx classification key\\n\");\n\n\t \n\terr_cfg.errors = DPAA2_FAS_RX_ERR_MASK;\n\terr_cfg.set_frame_annotation = 1;\n\terr_cfg.error_action = DPNI_ERROR_ACTION_DISCARD;\n\terr = dpni_set_errors_behavior(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t       &err_cfg);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_errors_behavior failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tfor (i = 0; i < priv->num_fqs; i++) {\n\t\tswitch (priv->fq[i].type) {\n\t\tcase DPAA2_RX_FQ:\n\t\t\terr = dpaa2_eth_setup_rx_flow(priv, &priv->fq[i]);\n\t\t\tbreak;\n\t\tcase DPAA2_TX_CONF_FQ:\n\t\t\terr = dpaa2_eth_setup_tx_flow(priv, &priv->fq[i]);\n\t\t\tbreak;\n\t\tcase DPAA2_RX_ERR_FQ:\n\t\t\terr = setup_rx_err_flow(priv, &priv->fq[i]);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(dev, \"Invalid FQ type %d\\n\", priv->fq[i].type);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = dpni_get_qdid(priv->mc_io, 0, priv->mc_token,\n\t\t\t    DPNI_QUEUE_TX, &priv->tx_qdid);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_qdid() failed\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int dpaa2_eth_alloc_rings(struct dpaa2_eth_priv *priv)\n{\n\tstruct net_device *net_dev = priv->net_dev;\n\tstruct device *dev = net_dev->dev.parent;\n\tint i;\n\n\tfor (i = 0; i < priv->num_channels; i++) {\n\t\tpriv->channel[i]->store =\n\t\t\tdpaa2_io_store_create(DPAA2_ETH_STORE_SIZE, dev);\n\t\tif (!priv->channel[i]->store) {\n\t\t\tnetdev_err(net_dev, \"dpaa2_io_store_create() failed\\n\");\n\t\t\tgoto err_ring;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_ring:\n\tfor (i = 0; i < priv->num_channels; i++) {\n\t\tif (!priv->channel[i]->store)\n\t\t\tbreak;\n\t\tdpaa2_io_store_destroy(priv->channel[i]->store);\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic void dpaa2_eth_free_rings(struct dpaa2_eth_priv *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_channels; i++)\n\t\tdpaa2_io_store_destroy(priv->channel[i]->store);\n}\n\nstatic int dpaa2_eth_set_mac_addr(struct dpaa2_eth_priv *priv)\n{\n\tstruct net_device *net_dev = priv->net_dev;\n\tstruct device *dev = net_dev->dev.parent;\n\tu8 mac_addr[ETH_ALEN], dpni_mac_addr[ETH_ALEN];\n\tint err;\n\n\t \n\terr = dpni_get_port_mac_addr(priv->mc_io, 0, priv->mc_token, mac_addr);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_port_mac_addr() failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\terr = dpni_get_primary_mac_addr(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\tdpni_mac_addr);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_get_primary_mac_addr() failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tif (!is_zero_ether_addr(mac_addr)) {\n\t\t \n\t\tif (!ether_addr_equal(mac_addr, dpni_mac_addr)) {\n\t\t\terr = dpni_set_primary_mac_addr(priv->mc_io, 0,\n\t\t\t\t\t\t\tpriv->mc_token,\n\t\t\t\t\t\t\tmac_addr);\n\t\t\tif (err) {\n\t\t\t\tdev_err(dev, \"dpni_set_primary_mac_addr() failed\\n\");\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t\teth_hw_addr_set(net_dev, mac_addr);\n\t} else if (is_zero_ether_addr(dpni_mac_addr)) {\n\t\t \n\t\teth_hw_addr_random(net_dev);\n\t\tdev_dbg_once(dev, \"device(s) have all-zero hwaddr, replaced with random\\n\");\n\n\t\terr = dpni_set_primary_mac_addr(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\t\tnet_dev->dev_addr);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"dpni_set_primary_mac_addr() failed\\n\");\n\t\t\treturn err;\n\t\t}\n\n\t\t \n\t\tnet_dev->addr_assign_type = NET_ADDR_PERM;\n\t} else {\n\t\t \n\t\teth_hw_addr_set(net_dev, dpni_mac_addr);\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_netdev_init(struct net_device *net_dev)\n{\n\tstruct device *dev = net_dev->dev.parent;\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tu32 options = priv->dpni_attrs.options;\n\tu64 supported = 0, not_supported = 0;\n\tu8 bcast_addr[ETH_ALEN];\n\tu8 num_queues;\n\tint err;\n\n\tnet_dev->netdev_ops = &dpaa2_eth_ops;\n\tnet_dev->ethtool_ops = &dpaa2_ethtool_ops;\n\n\terr = dpaa2_eth_set_mac_addr(priv);\n\tif (err)\n\t\treturn err;\n\n\t \n\teth_broadcast_addr(bcast_addr);\n\terr = dpni_add_mac_addr(priv->mc_io, 0, priv->mc_token, bcast_addr);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_add_mac_addr() failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tnet_dev->max_mtu = DPAA2_ETH_MAX_MTU;\n\terr = dpni_set_max_frame_length(priv->mc_io, 0, priv->mc_token,\n\t\t\t\t\tDPAA2_ETH_MFL);\n\tif (err) {\n\t\tdev_err(dev, \"dpni_set_max_frame_length() failed\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tnum_queues = dpaa2_eth_queue_count(priv);\n\terr = netif_set_real_num_tx_queues(net_dev, num_queues);\n\tif (err) {\n\t\tdev_err(dev, \"netif_set_real_num_tx_queues() failed\\n\");\n\t\treturn err;\n\t}\n\terr = netif_set_real_num_rx_queues(net_dev, num_queues);\n\tif (err) {\n\t\tdev_err(dev, \"netif_set_real_num_rx_queues() failed\\n\");\n\t\treturn err;\n\t}\n\n\tdpaa2_eth_detect_features(priv);\n\n\t \n\tsupported |= IFF_LIVE_ADDR_CHANGE;\n\n\tif (options & DPNI_OPT_NO_MAC_FILTER)\n\t\tnot_supported |= IFF_UNICAST_FLT;\n\telse\n\t\tsupported |= IFF_UNICAST_FLT;\n\n\tnet_dev->priv_flags |= supported;\n\tnet_dev->priv_flags &= ~not_supported;\n\n\t \n\tnet_dev->features = NETIF_F_RXCSUM |\n\t\t\t    NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\t    NETIF_F_SG | NETIF_F_HIGHDMA |\n\t\t\t    NETIF_F_LLTX | NETIF_F_HW_TC | NETIF_F_TSO;\n\tnet_dev->gso_max_segs = DPAA2_ETH_ENQUEUE_MAX_FDS;\n\tnet_dev->hw_features = net_dev->features;\n\tnet_dev->xdp_features = NETDEV_XDP_ACT_BASIC |\n\t\t\t\tNETDEV_XDP_ACT_REDIRECT |\n\t\t\t\tNETDEV_XDP_ACT_NDO_XMIT;\n\tif (priv->dpni_attrs.wriop_version >= DPAA2_WRIOP_VERSION(3, 0, 0) &&\n\t    priv->dpni_attrs.num_queues <= 8)\n\t\tnet_dev->xdp_features |= NETDEV_XDP_ACT_XSK_ZEROCOPY;\n\n\tif (priv->dpni_attrs.vlan_filter_entries)\n\t\tnet_dev->hw_features |= NETIF_F_HW_VLAN_CTAG_FILTER;\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_poll_link_state(void *arg)\n{\n\tstruct dpaa2_eth_priv *priv = (struct dpaa2_eth_priv *)arg;\n\tint err;\n\n\twhile (!kthread_should_stop()) {\n\t\terr = dpaa2_eth_link_state_update(priv);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\n\t\tmsleep(DPAA2_ETH_LINK_STATE_REFRESH);\n\t}\n\n\treturn 0;\n}\n\nstatic int dpaa2_eth_connect_mac(struct dpaa2_eth_priv *priv)\n{\n\tstruct fsl_mc_device *dpni_dev, *dpmac_dev;\n\tstruct dpaa2_mac *mac;\n\tint err;\n\n\tdpni_dev = to_fsl_mc_device(priv->net_dev->dev.parent);\n\tdpmac_dev = fsl_mc_get_endpoint(dpni_dev, 0);\n\n\tif (PTR_ERR(dpmac_dev) == -EPROBE_DEFER) {\n\t\tnetdev_dbg(priv->net_dev, \"waiting for mac\\n\");\n\t\treturn PTR_ERR(dpmac_dev);\n\t}\n\n\tif (IS_ERR(dpmac_dev) || dpmac_dev->dev.type != &fsl_mc_bus_dpmac_type)\n\t\treturn 0;\n\n\tmac = kzalloc(sizeof(struct dpaa2_mac), GFP_KERNEL);\n\tif (!mac)\n\t\treturn -ENOMEM;\n\n\tmac->mc_dev = dpmac_dev;\n\tmac->mc_io = priv->mc_io;\n\tmac->net_dev = priv->net_dev;\n\n\terr = dpaa2_mac_open(mac);\n\tif (err)\n\t\tgoto err_free_mac;\n\n\tif (dpaa2_mac_is_type_phy(mac)) {\n\t\terr = dpaa2_mac_connect(mac);\n\t\tif (err) {\n\t\t\tif (err == -EPROBE_DEFER)\n\t\t\t\tnetdev_dbg(priv->net_dev,\n\t\t\t\t\t   \"could not connect to MAC\\n\");\n\t\t\telse\n\t\t\t\tnetdev_err(priv->net_dev,\n\t\t\t\t\t   \"Error connecting to the MAC endpoint: %pe\",\n\t\t\t\t\t   ERR_PTR(err));\n\t\t\tgoto err_close_mac;\n\t\t}\n\t}\n\n\tmutex_lock(&priv->mac_lock);\n\tpriv->mac = mac;\n\tmutex_unlock(&priv->mac_lock);\n\n\treturn 0;\n\nerr_close_mac:\n\tdpaa2_mac_close(mac);\nerr_free_mac:\n\tkfree(mac);\n\treturn err;\n}\n\nstatic void dpaa2_eth_disconnect_mac(struct dpaa2_eth_priv *priv)\n{\n\tstruct dpaa2_mac *mac;\n\n\tmutex_lock(&priv->mac_lock);\n\tmac = priv->mac;\n\tpriv->mac = NULL;\n\tmutex_unlock(&priv->mac_lock);\n\n\tif (!mac)\n\t\treturn;\n\n\tif (dpaa2_mac_is_type_phy(mac))\n\t\tdpaa2_mac_disconnect(mac);\n\n\tdpaa2_mac_close(mac);\n\tkfree(mac);\n}\n\nstatic irqreturn_t dpni_irq0_handler_thread(int irq_num, void *arg)\n{\n\tu32 status = ~0;\n\tstruct device *dev = (struct device *)arg;\n\tstruct fsl_mc_device *dpni_dev = to_fsl_mc_device(dev);\n\tstruct net_device *net_dev = dev_get_drvdata(dev);\n\tstruct dpaa2_eth_priv *priv = netdev_priv(net_dev);\n\tbool had_mac;\n\tint err;\n\n\terr = dpni_get_irq_status(dpni_dev->mc_io, 0, dpni_dev->mc_handle,\n\t\t\t\t  DPNI_IRQ_INDEX, &status);\n\tif (unlikely(err)) {\n\t\tnetdev_err(net_dev, \"Can't get irq status (err %d)\\n\", err);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tif (status & DPNI_IRQ_EVENT_LINK_CHANGED)\n\t\tdpaa2_eth_link_state_update(netdev_priv(net_dev));\n\n\tif (status & DPNI_IRQ_EVENT_ENDPOINT_CHANGED) {\n\t\tdpaa2_eth_set_mac_addr(netdev_priv(net_dev));\n\t\tdpaa2_eth_update_tx_fqids(priv);\n\n\t\t \n\t\thad_mac = !!priv->mac;\n\t\tif (had_mac)\n\t\t\tdpaa2_eth_disconnect_mac(priv);\n\t\telse\n\t\t\tdpaa2_eth_connect_mac(priv);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int dpaa2_eth_setup_irqs(struct fsl_mc_device *ls_dev)\n{\n\tint err = 0;\n\tstruct fsl_mc_device_irq *irq;\n\n\terr = fsl_mc_allocate_irqs(ls_dev);\n\tif (err) {\n\t\tdev_err(&ls_dev->dev, \"MC irqs allocation failed\\n\");\n\t\treturn err;\n\t}\n\n\tirq = ls_dev->irqs[0];\n\terr = devm_request_threaded_irq(&ls_dev->dev, irq->virq,\n\t\t\t\t\tNULL, dpni_irq0_handler_thread,\n\t\t\t\t\tIRQF_NO_SUSPEND | IRQF_ONESHOT,\n\t\t\t\t\tdev_name(&ls_dev->dev), &ls_dev->dev);\n\tif (err < 0) {\n\t\tdev_err(&ls_dev->dev, \"devm_request_threaded_irq(): %d\\n\", err);\n\t\tgoto free_mc_irq;\n\t}\n\n\terr = dpni_set_irq_mask(ls_dev->mc_io, 0, ls_dev->mc_handle,\n\t\t\t\tDPNI_IRQ_INDEX, DPNI_IRQ_EVENT_LINK_CHANGED |\n\t\t\t\tDPNI_IRQ_EVENT_ENDPOINT_CHANGED);\n\tif (err < 0) {\n\t\tdev_err(&ls_dev->dev, \"dpni_set_irq_mask(): %d\\n\", err);\n\t\tgoto free_irq;\n\t}\n\n\terr = dpni_set_irq_enable(ls_dev->mc_io, 0, ls_dev->mc_handle,\n\t\t\t\t  DPNI_IRQ_INDEX, 1);\n\tif (err < 0) {\n\t\tdev_err(&ls_dev->dev, \"dpni_set_irq_enable(): %d\\n\", err);\n\t\tgoto free_irq;\n\t}\n\n\treturn 0;\n\nfree_irq:\n\tdevm_free_irq(&ls_dev->dev, irq->virq, &ls_dev->dev);\nfree_mc_irq:\n\tfsl_mc_free_irqs(ls_dev);\n\n\treturn err;\n}\n\nstatic void dpaa2_eth_add_ch_napi(struct dpaa2_eth_priv *priv)\n{\n\tint i;\n\tstruct dpaa2_eth_channel *ch;\n\n\tfor (i = 0; i < priv->num_channels; i++) {\n\t\tch = priv->channel[i];\n\t\t \n\t\tnetif_napi_add(priv->net_dev, &ch->napi, dpaa2_eth_poll);\n\t}\n}\n\nstatic void dpaa2_eth_del_ch_napi(struct dpaa2_eth_priv *priv)\n{\n\tint i;\n\tstruct dpaa2_eth_channel *ch;\n\n\tfor (i = 0; i < priv->num_channels; i++) {\n\t\tch = priv->channel[i];\n\t\tnetif_napi_del(&ch->napi);\n\t}\n}\n\nstatic int dpaa2_eth_probe(struct fsl_mc_device *dpni_dev)\n{\n\tstruct device *dev;\n\tstruct net_device *net_dev = NULL;\n\tstruct dpaa2_eth_priv *priv = NULL;\n\tint err = 0;\n\n\tdev = &dpni_dev->dev;\n\n\t \n\tnet_dev = alloc_etherdev_mq(sizeof(*priv), DPAA2_ETH_MAX_NETDEV_QUEUES);\n\tif (!net_dev) {\n\t\tdev_err(dev, \"alloc_etherdev_mq() failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tSET_NETDEV_DEV(net_dev, dev);\n\tdev_set_drvdata(dev, net_dev);\n\n\tpriv = netdev_priv(net_dev);\n\tpriv->net_dev = net_dev;\n\tSET_NETDEV_DEVLINK_PORT(net_dev, &priv->devlink_port);\n\n\tmutex_init(&priv->mac_lock);\n\n\tpriv->iommu_domain = iommu_get_domain_for_dev(dev);\n\n\tpriv->tx_tstamp_type = HWTSTAMP_TX_OFF;\n\tpriv->rx_tstamp = false;\n\n\tpriv->dpaa2_ptp_wq = alloc_workqueue(\"dpaa2_ptp_wq\", 0, 0);\n\tif (!priv->dpaa2_ptp_wq) {\n\t\terr = -ENOMEM;\n\t\tgoto err_wq_alloc;\n\t}\n\n\tINIT_WORK(&priv->tx_onestep_tstamp, dpaa2_eth_tx_onestep_tstamp);\n\tmutex_init(&priv->onestep_tstamp_lock);\n\tskb_queue_head_init(&priv->tx_skbs);\n\n\tpriv->rx_copybreak = DPAA2_ETH_DEFAULT_COPYBREAK;\n\n\t \n\terr = fsl_mc_portal_allocate(dpni_dev, FSL_MC_IO_ATOMIC_CONTEXT_PORTAL,\n\t\t\t\t     &priv->mc_io);\n\tif (err) {\n\t\tif (err == -ENXIO) {\n\t\t\tdev_dbg(dev, \"waiting for MC portal\\n\");\n\t\t\terr = -EPROBE_DEFER;\n\t\t} else {\n\t\t\tdev_err(dev, \"MC portal allocation failed\\n\");\n\t\t}\n\t\tgoto err_portal_alloc;\n\t}\n\n\t \n\terr = dpaa2_eth_setup_dpni(dpni_dev);\n\tif (err)\n\t\tgoto err_dpni_setup;\n\n\terr = dpaa2_eth_setup_dpio(priv);\n\tif (err)\n\t\tgoto err_dpio_setup;\n\n\tdpaa2_eth_setup_fqs(priv);\n\n\terr = dpaa2_eth_setup_default_dpbp(priv);\n\tif (err)\n\t\tgoto err_dpbp_setup;\n\n\terr = dpaa2_eth_bind_dpni(priv);\n\tif (err)\n\t\tgoto err_bind;\n\n\t \n\tdpaa2_eth_add_ch_napi(priv);\n\n\t \n\tpriv->percpu_stats = alloc_percpu(*priv->percpu_stats);\n\tif (!priv->percpu_stats) {\n\t\tdev_err(dev, \"alloc_percpu(percpu_stats) failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_percpu_stats;\n\t}\n\tpriv->percpu_extras = alloc_percpu(*priv->percpu_extras);\n\tif (!priv->percpu_extras) {\n\t\tdev_err(dev, \"alloc_percpu(percpu_extras) failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_percpu_extras;\n\t}\n\n\tpriv->sgt_cache = alloc_percpu(*priv->sgt_cache);\n\tif (!priv->sgt_cache) {\n\t\tdev_err(dev, \"alloc_percpu(sgt_cache) failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_sgt_cache;\n\t}\n\n\tpriv->fd = alloc_percpu(*priv->fd);\n\tif (!priv->fd) {\n\t\tdev_err(dev, \"alloc_percpu(fds) failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_fds;\n\t}\n\n\terr = dpaa2_eth_netdev_init(net_dev);\n\tif (err)\n\t\tgoto err_netdev_init;\n\n\t \n\terr = dpaa2_eth_set_rx_csum(priv, !!(net_dev->features & NETIF_F_RXCSUM));\n\tif (err)\n\t\tgoto err_csum;\n\n\terr = dpaa2_eth_set_tx_csum(priv,\n\t\t\t\t    !!(net_dev->features & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM)));\n\tif (err)\n\t\tgoto err_csum;\n\n\terr = dpaa2_eth_alloc_rings(priv);\n\tif (err)\n\t\tgoto err_alloc_rings;\n\n#ifdef CONFIG_FSL_DPAA2_ETH_DCB\n\tif (dpaa2_eth_has_pause_support(priv) && priv->vlan_cls_enabled) {\n\t\tpriv->dcbx_mode = DCB_CAP_DCBX_HOST | DCB_CAP_DCBX_VER_IEEE;\n\t\tnet_dev->dcbnl_ops = &dpaa2_eth_dcbnl_ops;\n\t} else {\n\t\tdev_dbg(dev, \"PFC not supported\\n\");\n\t}\n#endif\n\n\terr = dpaa2_eth_connect_mac(priv);\n\tif (err)\n\t\tgoto err_connect_mac;\n\n\terr = dpaa2_eth_setup_irqs(dpni_dev);\n\tif (err) {\n\t\tnetdev_warn(net_dev, \"Failed to set link interrupt, fall back to polling\\n\");\n\t\tpriv->poll_thread = kthread_run(dpaa2_eth_poll_link_state, priv,\n\t\t\t\t\t\t\"%s_poll_link\", net_dev->name);\n\t\tif (IS_ERR(priv->poll_thread)) {\n\t\t\tdev_err(dev, \"Error starting polling thread\\n\");\n\t\t\tgoto err_poll_thread;\n\t\t}\n\t\tpriv->do_link_poll = true;\n\t}\n\n\terr = dpaa2_eth_dl_alloc(priv);\n\tif (err)\n\t\tgoto err_dl_register;\n\n\terr = dpaa2_eth_dl_traps_register(priv);\n\tif (err)\n\t\tgoto err_dl_trap_register;\n\n\terr = dpaa2_eth_dl_port_add(priv);\n\tif (err)\n\t\tgoto err_dl_port_add;\n\n\tnet_dev->needed_headroom = DPAA2_ETH_SWA_SIZE + DPAA2_ETH_TX_BUF_ALIGN;\n\n\terr = register_netdev(net_dev);\n\tif (err < 0) {\n\t\tdev_err(dev, \"register_netdev() failed\\n\");\n\t\tgoto err_netdev_reg;\n\t}\n\n#ifdef CONFIG_DEBUG_FS\n\tdpaa2_dbg_add(priv);\n#endif\n\n\tdpaa2_eth_dl_register(priv);\n\tdev_info(dev, \"Probed interface %s\\n\", net_dev->name);\n\treturn 0;\n\nerr_netdev_reg:\n\tdpaa2_eth_dl_port_del(priv);\nerr_dl_port_add:\n\tdpaa2_eth_dl_traps_unregister(priv);\nerr_dl_trap_register:\n\tdpaa2_eth_dl_free(priv);\nerr_dl_register:\n\tif (priv->do_link_poll)\n\t\tkthread_stop(priv->poll_thread);\n\telse\n\t\tfsl_mc_free_irqs(dpni_dev);\nerr_poll_thread:\n\tdpaa2_eth_disconnect_mac(priv);\nerr_connect_mac:\n\tdpaa2_eth_free_rings(priv);\nerr_alloc_rings:\nerr_csum:\nerr_netdev_init:\n\tfree_percpu(priv->fd);\nerr_alloc_fds:\n\tfree_percpu(priv->sgt_cache);\nerr_alloc_sgt_cache:\n\tfree_percpu(priv->percpu_extras);\nerr_alloc_percpu_extras:\n\tfree_percpu(priv->percpu_stats);\nerr_alloc_percpu_stats:\n\tdpaa2_eth_del_ch_napi(priv);\nerr_bind:\n\tdpaa2_eth_free_dpbps(priv);\nerr_dpbp_setup:\n\tdpaa2_eth_free_dpio(priv);\nerr_dpio_setup:\n\tdpaa2_eth_free_dpni(priv);\nerr_dpni_setup:\n\tfsl_mc_portal_free(priv->mc_io);\nerr_portal_alloc:\n\tdestroy_workqueue(priv->dpaa2_ptp_wq);\nerr_wq_alloc:\n\tdev_set_drvdata(dev, NULL);\n\tfree_netdev(net_dev);\n\n\treturn err;\n}\n\nstatic void dpaa2_eth_remove(struct fsl_mc_device *ls_dev)\n{\n\tstruct device *dev;\n\tstruct net_device *net_dev;\n\tstruct dpaa2_eth_priv *priv;\n\n\tdev = &ls_dev->dev;\n\tnet_dev = dev_get_drvdata(dev);\n\tpriv = netdev_priv(net_dev);\n\n\tdpaa2_eth_dl_unregister(priv);\n\n#ifdef CONFIG_DEBUG_FS\n\tdpaa2_dbg_remove(priv);\n#endif\n\n\tunregister_netdev(net_dev);\n\n\tdpaa2_eth_dl_port_del(priv);\n\tdpaa2_eth_dl_traps_unregister(priv);\n\tdpaa2_eth_dl_free(priv);\n\n\tif (priv->do_link_poll)\n\t\tkthread_stop(priv->poll_thread);\n\telse\n\t\tfsl_mc_free_irqs(ls_dev);\n\n\tdpaa2_eth_disconnect_mac(priv);\n\tdpaa2_eth_free_rings(priv);\n\tfree_percpu(priv->fd);\n\tfree_percpu(priv->sgt_cache);\n\tfree_percpu(priv->percpu_stats);\n\tfree_percpu(priv->percpu_extras);\n\n\tdpaa2_eth_del_ch_napi(priv);\n\tdpaa2_eth_free_dpbps(priv);\n\tdpaa2_eth_free_dpio(priv);\n\tdpaa2_eth_free_dpni(priv);\n\tif (priv->onestep_reg_base)\n\t\tiounmap(priv->onestep_reg_base);\n\n\tfsl_mc_portal_free(priv->mc_io);\n\n\tdestroy_workqueue(priv->dpaa2_ptp_wq);\n\n\tdev_dbg(net_dev->dev.parent, \"Removed interface %s\\n\", net_dev->name);\n\n\tfree_netdev(net_dev);\n}\n\nstatic const struct fsl_mc_device_id dpaa2_eth_match_id_table[] = {\n\t{\n\t\t.vendor = FSL_MC_VENDOR_FREESCALE,\n\t\t.obj_type = \"dpni\",\n\t},\n\t{ .vendor = 0x0 }\n};\nMODULE_DEVICE_TABLE(fslmc, dpaa2_eth_match_id_table);\n\nstatic struct fsl_mc_driver dpaa2_eth_driver = {\n\t.driver = {\n\t\t.name = KBUILD_MODNAME,\n\t},\n\t.probe = dpaa2_eth_probe,\n\t.remove = dpaa2_eth_remove,\n\t.match_id_table = dpaa2_eth_match_id_table\n};\n\nstatic int __init dpaa2_eth_driver_init(void)\n{\n\tint err;\n\n\tdpaa2_eth_dbg_init();\n\terr = fsl_mc_driver_register(&dpaa2_eth_driver);\n\tif (err) {\n\t\tdpaa2_eth_dbg_exit();\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit dpaa2_eth_driver_exit(void)\n{\n\tdpaa2_eth_dbg_exit();\n\tfsl_mc_driver_unregister(&dpaa2_eth_driver);\n}\n\nmodule_init(dpaa2_eth_driver_init);\nmodule_exit(dpaa2_eth_driver_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}