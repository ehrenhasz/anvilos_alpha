{
  "module_name": "mlxbf_gige_tx.c",
  "hash_id": "017024224a512008e63570a3751311147eea0c2f8f31be59aa3afe17725b1703",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlxbf_gige/mlxbf_gige_tx.c",
  "human_readable_source": "\n\n \n\n#include <linux/skbuff.h>\n\n#include \"mlxbf_gige.h\"\n#include \"mlxbf_gige_regs.h\"\n\n \nint mlxbf_gige_tx_init(struct mlxbf_gige *priv)\n{\n\tsize_t size;\n\n\tsize = MLXBF_GIGE_TX_WQE_SZ * priv->tx_q_entries;\n\tpriv->tx_wqe_base = dma_alloc_coherent(priv->dev, size,\n\t\t\t\t\t       &priv->tx_wqe_base_dma,\n\t\t\t\t\t       GFP_KERNEL);\n\tif (!priv->tx_wqe_base)\n\t\treturn -ENOMEM;\n\n\tpriv->tx_wqe_next = priv->tx_wqe_base;\n\n\t \n\twriteq(priv->tx_wqe_base_dma, priv->base + MLXBF_GIGE_TX_WQ_BASE);\n\n\t \n\tpriv->tx_cc = dma_alloc_coherent(priv->dev, MLXBF_GIGE_TX_CC_SZ,\n\t\t\t\t\t &priv->tx_cc_dma, GFP_KERNEL);\n\tif (!priv->tx_cc) {\n\t\tdma_free_coherent(priv->dev, size,\n\t\t\t\t  priv->tx_wqe_base, priv->tx_wqe_base_dma);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\twriteq(priv->tx_cc_dma, priv->base + MLXBF_GIGE_TX_CI_UPDATE_ADDRESS);\n\n\twriteq(ilog2(priv->tx_q_entries),\n\t       priv->base + MLXBF_GIGE_TX_WQ_SIZE_LOG2);\n\n\tpriv->prev_tx_ci = 0;\n\tpriv->tx_pi = 0;\n\n\treturn 0;\n}\n\n \nvoid mlxbf_gige_tx_deinit(struct mlxbf_gige *priv)\n{\n\tu64 *tx_wqe_addr;\n\tsize_t size;\n\tint i;\n\n\ttx_wqe_addr = priv->tx_wqe_base;\n\n\tfor (i = 0; i < priv->tx_q_entries; i++) {\n\t\tif (priv->tx_skb[i]) {\n\t\t\tdma_unmap_single(priv->dev, *tx_wqe_addr,\n\t\t\t\t\t priv->tx_skb[i]->len, DMA_TO_DEVICE);\n\t\t\tdev_kfree_skb(priv->tx_skb[i]);\n\t\t\tpriv->tx_skb[i] = NULL;\n\t\t}\n\t\ttx_wqe_addr += 2;\n\t}\n\n\tsize = MLXBF_GIGE_TX_WQE_SZ * priv->tx_q_entries;\n\tdma_free_coherent(priv->dev, size,\n\t\t\t  priv->tx_wqe_base, priv->tx_wqe_base_dma);\n\n\tdma_free_coherent(priv->dev, MLXBF_GIGE_TX_CC_SZ,\n\t\t\t  priv->tx_cc, priv->tx_cc_dma);\n\n\tpriv->tx_wqe_base = NULL;\n\tpriv->tx_wqe_base_dma = 0;\n\tpriv->tx_cc = NULL;\n\tpriv->tx_cc_dma = 0;\n\tpriv->tx_wqe_next = NULL;\n\twriteq(0, priv->base + MLXBF_GIGE_TX_WQ_BASE);\n\twriteq(0, priv->base + MLXBF_GIGE_TX_CI_UPDATE_ADDRESS);\n}\n\n \nstatic u16 mlxbf_gige_tx_buffs_avail(struct mlxbf_gige *priv)\n{\n\tunsigned long flags;\n\tu16 avail;\n\n\tspin_lock_irqsave(&priv->lock, flags);\n\n\tif (priv->prev_tx_ci == priv->tx_pi)\n\t\tavail = priv->tx_q_entries - 1;\n\telse\n\t\tavail = ((priv->tx_q_entries + priv->prev_tx_ci - priv->tx_pi)\n\t\t\t  % priv->tx_q_entries) - 1;\n\n\tspin_unlock_irqrestore(&priv->lock, flags);\n\n\treturn avail;\n}\n\nbool mlxbf_gige_handle_tx_complete(struct mlxbf_gige *priv)\n{\n\tstruct net_device_stats *stats;\n\tu16 tx_wqe_index;\n\tu64 *tx_wqe_addr;\n\tu64 tx_status;\n\tu16 tx_ci;\n\n\ttx_status = readq(priv->base + MLXBF_GIGE_TX_STATUS);\n\tif (tx_status & MLXBF_GIGE_TX_STATUS_DATA_FIFO_FULL)\n\t\tpriv->stats.tx_fifo_full++;\n\ttx_ci = readq(priv->base + MLXBF_GIGE_TX_CONSUMER_INDEX);\n\tstats = &priv->netdev->stats;\n\n\t \n\tfor (; priv->prev_tx_ci != tx_ci; priv->prev_tx_ci++) {\n\t\ttx_wqe_index = priv->prev_tx_ci % priv->tx_q_entries;\n\t\t \n\t\ttx_wqe_addr = priv->tx_wqe_base +\n\t\t\t       (tx_wqe_index * MLXBF_GIGE_TX_WQE_SZ_QWORDS);\n\n\t\tstats->tx_packets++;\n\t\tstats->tx_bytes += MLXBF_GIGE_TX_WQE_PKT_LEN(tx_wqe_addr);\n\n\t\tdma_unmap_single(priv->dev, *tx_wqe_addr,\n\t\t\t\t priv->tx_skb[tx_wqe_index]->len, DMA_TO_DEVICE);\n\t\tdev_consume_skb_any(priv->tx_skb[tx_wqe_index]);\n\t\tpriv->tx_skb[tx_wqe_index] = NULL;\n\n\t\t \n\t\tmb();\n\t}\n\n\t \n\tif (netif_queue_stopped(priv->netdev) &&\n\t    mlxbf_gige_tx_buffs_avail(priv))\n\t\tnetif_wake_queue(priv->netdev);\n\n\treturn true;\n}\n\n \nvoid mlxbf_gige_update_tx_wqe_next(struct mlxbf_gige *priv)\n{\n\t \n\tpriv->tx_wqe_next += MLXBF_GIGE_TX_WQE_SZ_QWORDS;\n\n\t \n\t \n\tif (priv->tx_wqe_next == (priv->tx_wqe_base +\n\t\t\t\t  (priv->tx_q_entries * MLXBF_GIGE_TX_WQE_SZ_QWORDS)))\n\t\tpriv->tx_wqe_next = priv->tx_wqe_base;\n}\n\nnetdev_tx_t mlxbf_gige_start_xmit(struct sk_buff *skb,\n\t\t\t\t  struct net_device *netdev)\n{\n\tstruct mlxbf_gige *priv = netdev_priv(netdev);\n\tlong buff_addr, start_dma_page, end_dma_page;\n\tstruct sk_buff *tx_skb;\n\tdma_addr_t tx_buf_dma;\n\tunsigned long flags;\n\tu64 *tx_wqe_addr;\n\tu64 word2;\n\n\t \n\tif (skb->len > MLXBF_GIGE_DEFAULT_BUF_SZ || skb_linearize(skb)) {\n\t\tdev_kfree_skb(skb);\n\t\tnetdev->stats.tx_dropped++;\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tbuff_addr = (long)skb->data;\n\tstart_dma_page = buff_addr >> MLXBF_GIGE_DMA_PAGE_SHIFT;\n\tend_dma_page   = (buff_addr + skb->len - 1) >> MLXBF_GIGE_DMA_PAGE_SHIFT;\n\n\t \n\tif (start_dma_page != end_dma_page) {\n\t\t \n\t\ttx_skb = mlxbf_gige_alloc_skb(priv, skb->len,\n\t\t\t\t\t      &tx_buf_dma, DMA_TO_DEVICE);\n\t\tif (!tx_skb) {\n\t\t\t \n\t\t\tdev_kfree_skb(skb);\n\t\t\tnetdev->stats.tx_dropped++;\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\tskb_put_data(tx_skb, skb->data, skb->len);\n\n\t\t \n\t\tdev_kfree_skb(skb);\n\t} else {\n\t\ttx_skb = skb;\n\t\ttx_buf_dma = dma_map_single(priv->dev, skb->data,\n\t\t\t\t\t    skb->len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(priv->dev, tx_buf_dma)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\tnetdev->stats.tx_dropped++;\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t}\n\n\t \n\ttx_wqe_addr = priv->tx_wqe_next;\n\n\tmlxbf_gige_update_tx_wqe_next(priv);\n\n\t \n\t*tx_wqe_addr = tx_buf_dma;\n\n\t \n\tword2 = tx_skb->len & MLXBF_GIGE_TX_WQE_PKT_LEN_MASK;\n\n\t \n\t*(tx_wqe_addr + 1) = word2;\n\n\tspin_lock_irqsave(&priv->lock, flags);\n\tpriv->tx_skb[priv->tx_pi % priv->tx_q_entries] = tx_skb;\n\tpriv->tx_pi++;\n\tspin_unlock_irqrestore(&priv->lock, flags);\n\n\tif (!netdev_xmit_more()) {\n\t\t \n\t\twmb();\n\t\twriteq(priv->tx_pi, priv->base + MLXBF_GIGE_TX_PRODUCER_INDEX);\n\t}\n\n\t \n\tif (!mlxbf_gige_tx_buffs_avail(priv)) {\n\t\t \n\t\tnetif_stop_queue(netdev);\n\n\t\t \n\t\tnapi_schedule(&priv->napi);\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}