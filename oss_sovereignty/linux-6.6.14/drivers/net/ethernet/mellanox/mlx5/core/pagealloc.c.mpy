{
  "module_name": "pagealloc.c",
  "hash_id": "906cb690674ce5bb884c972e798e683a9bd8b0d880baa6888263ae35511f535c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/pagealloc.c",
  "human_readable_source": " \n\n#include <linux/highmem.h>\n#include <linux/kernel.h>\n#include <linux/delay.h>\n#include <linux/mlx5/driver.h>\n#include <linux/xarray.h>\n#include \"mlx5_core.h\"\n#include \"lib/eq.h\"\n#include \"lib/tout.h\"\n\nenum {\n\tMLX5_PAGES_CANT_GIVE\t= 0,\n\tMLX5_PAGES_GIVE\t\t= 1,\n\tMLX5_PAGES_TAKE\t\t= 2\n};\n\nstruct mlx5_pages_req {\n\tstruct mlx5_core_dev *dev;\n\tu16\tfunc_id;\n\tu8\tec_function;\n\ts32\tnpages;\n\tstruct work_struct work;\n\tu8\trelease_all;\n};\n\nstruct fw_page {\n\tstruct rb_node\t\trb_node;\n\tu64\t\t\taddr;\n\tstruct page\t       *page;\n\tu32\t\t\tfunction;\n\tunsigned long\t\tbitmask;\n\tstruct list_head\tlist;\n\tunsigned int free_count;\n};\n\nenum {\n\tMLX5_MAX_RECLAIM_TIME_MILI\t= 5000,\n\tMLX5_NUM_4K_IN_PAGE\t\t= PAGE_SIZE / MLX5_ADAPTER_PAGE_SIZE,\n};\n\nstatic u32 get_function(u16 func_id, bool ec_function)\n{\n\treturn (u32)func_id | (ec_function << 16);\n}\n\nstatic u16 func_id_to_type(struct mlx5_core_dev *dev, u16 func_id, bool ec_function)\n{\n\tif (!func_id)\n\t\treturn mlx5_core_is_ecpf(dev) && !ec_function ? MLX5_HOST_PF : MLX5_PF;\n\n\tif (func_id <= max(mlx5_core_max_vfs(dev), mlx5_core_max_ec_vfs(dev))) {\n\t\tif (ec_function)\n\t\t\treturn MLX5_EC_VF;\n\t\telse\n\t\t\treturn MLX5_VF;\n\t}\n\treturn MLX5_SF;\n}\n\nstatic u32 mlx5_get_ec_function(u32 function)\n{\n\treturn function >> 16;\n}\n\nstatic u32 mlx5_get_func_id(u32 function)\n{\n\treturn function & 0xffff;\n}\n\nstatic struct rb_root *page_root_per_function(struct mlx5_core_dev *dev, u32 function)\n{\n\tstruct rb_root *root;\n\tint err;\n\n\troot = xa_load(&dev->priv.page_root_xa, function);\n\tif (root)\n\t\treturn root;\n\n\troot = kzalloc(sizeof(*root), GFP_KERNEL);\n\tif (!root)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = xa_insert(&dev->priv.page_root_xa, function, root, GFP_KERNEL);\n\tif (err) {\n\t\tkfree(root);\n\t\treturn ERR_PTR(err);\n\t}\n\n\t*root = RB_ROOT;\n\n\treturn root;\n}\n\nstatic int insert_page(struct mlx5_core_dev *dev, u64 addr, struct page *page, u32 function)\n{\n\tstruct rb_node *parent = NULL;\n\tstruct rb_root *root;\n\tstruct rb_node **new;\n\tstruct fw_page *nfp;\n\tstruct fw_page *tfp;\n\tint i;\n\n\troot = page_root_per_function(dev, function);\n\tif (IS_ERR(root))\n\t\treturn PTR_ERR(root);\n\n\tnew = &root->rb_node;\n\n\twhile (*new) {\n\t\tparent = *new;\n\t\ttfp = rb_entry(parent, struct fw_page, rb_node);\n\t\tif (tfp->addr < addr)\n\t\t\tnew = &parent->rb_left;\n\t\telse if (tfp->addr > addr)\n\t\t\tnew = &parent->rb_right;\n\t\telse\n\t\t\treturn -EEXIST;\n\t}\n\n\tnfp = kzalloc(sizeof(*nfp), GFP_KERNEL);\n\tif (!nfp)\n\t\treturn -ENOMEM;\n\n\tnfp->addr = addr;\n\tnfp->page = page;\n\tnfp->function = function;\n\tnfp->free_count = MLX5_NUM_4K_IN_PAGE;\n\tfor (i = 0; i < MLX5_NUM_4K_IN_PAGE; i++)\n\t\tset_bit(i, &nfp->bitmask);\n\n\trb_link_node(&nfp->rb_node, parent, new);\n\trb_insert_color(&nfp->rb_node, root);\n\tlist_add(&nfp->list, &dev->priv.free_list);\n\n\treturn 0;\n}\n\nstatic struct fw_page *find_fw_page(struct mlx5_core_dev *dev, u64 addr,\n\t\t\t\t    u32 function)\n{\n\tstruct fw_page *result = NULL;\n\tstruct rb_root *root;\n\tstruct rb_node *tmp;\n\tstruct fw_page *tfp;\n\n\troot = xa_load(&dev->priv.page_root_xa, function);\n\tif (WARN_ON_ONCE(!root))\n\t\treturn NULL;\n\n\ttmp = root->rb_node;\n\n\twhile (tmp) {\n\t\ttfp = rb_entry(tmp, struct fw_page, rb_node);\n\t\tif (tfp->addr < addr) {\n\t\t\ttmp = tmp->rb_left;\n\t\t} else if (tfp->addr > addr) {\n\t\t\ttmp = tmp->rb_right;\n\t\t} else {\n\t\t\tresult = tfp;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn result;\n}\n\nstatic int mlx5_cmd_query_pages(struct mlx5_core_dev *dev, u16 *func_id,\n\t\t\t\ts32 *npages, int boot)\n{\n\tu32 out[MLX5_ST_SZ_DW(query_pages_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(query_pages_in)] = {};\n\tint err;\n\n\tMLX5_SET(query_pages_in, in, opcode, MLX5_CMD_OP_QUERY_PAGES);\n\tMLX5_SET(query_pages_in, in, op_mod, boot ?\n\t\t MLX5_QUERY_PAGES_IN_OP_MOD_BOOT_PAGES :\n\t\t MLX5_QUERY_PAGES_IN_OP_MOD_INIT_PAGES);\n\tMLX5_SET(query_pages_in, in, embedded_cpu_function, mlx5_core_is_ecpf(dev));\n\n\terr = mlx5_cmd_exec_inout(dev, query_pages, in, out);\n\tif (err)\n\t\treturn err;\n\n\t*npages = MLX5_GET(query_pages_out, out, num_pages);\n\t*func_id = MLX5_GET(query_pages_out, out, function_id);\n\n\treturn err;\n}\n\nstatic int alloc_4k(struct mlx5_core_dev *dev, u64 *addr, u32 function)\n{\n\tstruct fw_page *fp = NULL;\n\tstruct fw_page *iter;\n\tunsigned n;\n\n\tlist_for_each_entry(iter, &dev->priv.free_list, list) {\n\t\tif (iter->function != function)\n\t\t\tcontinue;\n\t\tfp = iter;\n\t}\n\n\tif (list_empty(&dev->priv.free_list) || !fp)\n\t\treturn -ENOMEM;\n\n\tn = find_first_bit(&fp->bitmask, 8 * sizeof(fp->bitmask));\n\tif (n >= MLX5_NUM_4K_IN_PAGE) {\n\t\tmlx5_core_warn(dev, \"alloc 4k bug: fw page = 0x%llx, n = %u, bitmask: %lu, max num of 4K pages: %d\\n\",\n\t\t\t       fp->addr, n, fp->bitmask,  MLX5_NUM_4K_IN_PAGE);\n\t\treturn -ENOENT;\n\t}\n\tclear_bit(n, &fp->bitmask);\n\tfp->free_count--;\n\tif (!fp->free_count)\n\t\tlist_del(&fp->list);\n\n\t*addr = fp->addr + n * MLX5_ADAPTER_PAGE_SIZE;\n\n\treturn 0;\n}\n\n#define MLX5_U64_4K_PAGE_MASK ((~(u64)0U) << PAGE_SHIFT)\n\nstatic void free_fwp(struct mlx5_core_dev *dev, struct fw_page *fwp,\n\t\t     bool in_free_list)\n{\n\tstruct rb_root *root;\n\n\troot = xa_load(&dev->priv.page_root_xa, fwp->function);\n\tif (WARN_ON_ONCE(!root))\n\t\treturn;\n\n\trb_erase(&fwp->rb_node, root);\n\tif (in_free_list)\n\t\tlist_del(&fwp->list);\n\tdma_unmap_page(mlx5_core_dma_dev(dev), fwp->addr & MLX5_U64_4K_PAGE_MASK,\n\t\t       PAGE_SIZE, DMA_BIDIRECTIONAL);\n\t__free_page(fwp->page);\n\tkfree(fwp);\n}\n\nstatic void free_4k(struct mlx5_core_dev *dev, u64 addr, u32 function)\n{\n\tstruct fw_page *fwp;\n\tint n;\n\n\tfwp = find_fw_page(dev, addr & MLX5_U64_4K_PAGE_MASK, function);\n\tif (!fwp) {\n\t\tmlx5_core_warn_rl(dev, \"page not found\\n\");\n\t\treturn;\n\t}\n\tn = (addr & ~MLX5_U64_4K_PAGE_MASK) >> MLX5_ADAPTER_PAGE_SHIFT;\n\tfwp->free_count++;\n\tset_bit(n, &fwp->bitmask);\n\tif (fwp->free_count == MLX5_NUM_4K_IN_PAGE)\n\t\tfree_fwp(dev, fwp, fwp->free_count != 1);\n\telse if (fwp->free_count == 1)\n\t\tlist_add(&fwp->list, &dev->priv.free_list);\n}\n\nstatic int alloc_system_page(struct mlx5_core_dev *dev, u32 function)\n{\n\tstruct device *device = mlx5_core_dma_dev(dev);\n\tint nid = dev_to_node(device);\n\tstruct page *page;\n\tu64 zero_addr = 1;\n\tu64 addr;\n\tint err;\n\n\tpage = alloc_pages_node(nid, GFP_HIGHUSER, 0);\n\tif (!page) {\n\t\tmlx5_core_warn(dev, \"failed to allocate page\\n\");\n\t\treturn -ENOMEM;\n\t}\nmap:\n\taddr = dma_map_page(device, page, 0, PAGE_SIZE, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(device, addr)) {\n\t\tmlx5_core_warn(dev, \"failed dma mapping page\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_mapping;\n\t}\n\n\t \n\tif (addr == 0) {\n\t\tzero_addr = addr;\n\t\tgoto map;\n\t}\n\n\terr = insert_page(dev, addr, page, function);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed to track allocated page\\n\");\n\t\tdma_unmap_page(device, addr, PAGE_SIZE, DMA_BIDIRECTIONAL);\n\t}\n\nerr_mapping:\n\tif (err)\n\t\t__free_page(page);\n\n\tif (zero_addr == 0)\n\t\tdma_unmap_page(device, zero_addr, PAGE_SIZE,\n\t\t\t       DMA_BIDIRECTIONAL);\n\n\treturn err;\n}\n\nstatic void page_notify_fail(struct mlx5_core_dev *dev, u16 func_id,\n\t\t\t     bool ec_function)\n{\n\tu32 in[MLX5_ST_SZ_DW(manage_pages_in)] = {};\n\tint err;\n\n\tMLX5_SET(manage_pages_in, in, opcode, MLX5_CMD_OP_MANAGE_PAGES);\n\tMLX5_SET(manage_pages_in, in, op_mod, MLX5_PAGES_CANT_GIVE);\n\tMLX5_SET(manage_pages_in, in, function_id, func_id);\n\tMLX5_SET(manage_pages_in, in, embedded_cpu_function, ec_function);\n\n\terr = mlx5_cmd_exec_in(dev, manage_pages, in);\n\tif (err)\n\t\tmlx5_core_warn(dev, \"page notify failed func_id(%d) err(%d)\\n\",\n\t\t\t       func_id, err);\n}\n\nstatic int give_pages(struct mlx5_core_dev *dev, u16 func_id, int npages,\n\t\t      int event, bool ec_function)\n{\n\tu32 function = get_function(func_id, ec_function);\n\tu32 out[MLX5_ST_SZ_DW(manage_pages_out)] = {0};\n\tint inlen = MLX5_ST_SZ_BYTES(manage_pages_in);\n\tint notify_fail = event;\n\tu16 func_type;\n\tu64 addr;\n\tint err;\n\tu32 *in;\n\tint i;\n\n\tinlen += npages * MLX5_FLD_SZ_BYTES(manage_pages_in, pas[0]);\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tmlx5_core_warn(dev, \"vzalloc failed %d\\n\", inlen);\n\t\tgoto out_free;\n\t}\n\n\tfor (i = 0; i < npages; i++) {\nretry:\n\t\terr = alloc_4k(dev, &addr, function);\n\t\tif (err) {\n\t\t\tif (err == -ENOMEM)\n\t\t\t\terr = alloc_system_page(dev, function);\n\t\t\tif (err) {\n\t\t\t\tdev->priv.fw_pages_alloc_failed += (npages - i);\n\t\t\t\tgoto out_4k;\n\t\t\t}\n\n\t\t\tgoto retry;\n\t\t}\n\t\tMLX5_ARRAY_SET64(manage_pages_in, in, pas, i, addr);\n\t}\n\n\tMLX5_SET(manage_pages_in, in, opcode, MLX5_CMD_OP_MANAGE_PAGES);\n\tMLX5_SET(manage_pages_in, in, op_mod, MLX5_PAGES_GIVE);\n\tMLX5_SET(manage_pages_in, in, function_id, func_id);\n\tMLX5_SET(manage_pages_in, in, input_num_entries, npages);\n\tMLX5_SET(manage_pages_in, in, embedded_cpu_function, ec_function);\n\n\terr = mlx5_cmd_do(dev, in, inlen, out, sizeof(out));\n\tif (err == -EREMOTEIO) {\n\t\tnotify_fail = 0;\n\t\t \n\t\tif (event) {\n\t\t\terr = 0;\n\t\t\tgoto out_dropped;\n\t\t}\n\t}\n\terr = mlx5_cmd_check(dev, err, in, out);\n\tif (err) {\n\t\tmlx5_core_warn(dev, \"func_id 0x%x, npages %d, err %d\\n\",\n\t\t\t       func_id, npages, err);\n\t\tgoto out_dropped;\n\t}\n\n\tfunc_type = func_id_to_type(dev, func_id, ec_function);\n\tdev->priv.page_counters[func_type] += npages;\n\tdev->priv.fw_pages += npages;\n\n\tmlx5_core_dbg(dev, \"npages %d, ec_function %d, func_id 0x%x, err %d\\n\",\n\t\t      npages, ec_function, func_id, err);\n\n\tkvfree(in);\n\treturn 0;\n\nout_dropped:\n\tdev->priv.give_pages_dropped += npages;\nout_4k:\n\tfor (i--; i >= 0; i--)\n\t\tfree_4k(dev, MLX5_GET64(manage_pages_in, in, pas[i]), function);\nout_free:\n\tkvfree(in);\n\tif (notify_fail)\n\t\tpage_notify_fail(dev, func_id, ec_function);\n\treturn err;\n}\n\nstatic void release_all_pages(struct mlx5_core_dev *dev, u16 func_id,\n\t\t\t      bool ec_function)\n{\n\tu32 function = get_function(func_id, ec_function);\n\tstruct rb_root *root;\n\tstruct rb_node *p;\n\tint npages = 0;\n\tu16 func_type;\n\n\troot = xa_load(&dev->priv.page_root_xa, function);\n\tif (WARN_ON_ONCE(!root))\n\t\treturn;\n\n\tp = rb_first(root);\n\twhile (p) {\n\t\tstruct fw_page *fwp = rb_entry(p, struct fw_page, rb_node);\n\n\t\tp = rb_next(p);\n\t\tnpages += (MLX5_NUM_4K_IN_PAGE - fwp->free_count);\n\t\tfree_fwp(dev, fwp, fwp->free_count);\n\t}\n\n\tfunc_type = func_id_to_type(dev, func_id, ec_function);\n\tdev->priv.page_counters[func_type] -= npages;\n\tdev->priv.fw_pages -= npages;\n\n\tmlx5_core_dbg(dev, \"npages %d, ec_function %d, func_id 0x%x\\n\",\n\t\t      npages, ec_function, func_id);\n}\n\nstatic u32 fwp_fill_manage_pages_out(struct fw_page *fwp, u32 *out, u32 index,\n\t\t\t\t     u32 npages)\n{\n\tu32 pages_set = 0;\n\tunsigned int n;\n\n\tfor_each_clear_bit(n, &fwp->bitmask, MLX5_NUM_4K_IN_PAGE) {\n\t\tMLX5_ARRAY_SET64(manage_pages_out, out, pas, index + pages_set,\n\t\t\t\t fwp->addr + (n * MLX5_ADAPTER_PAGE_SIZE));\n\t\tpages_set++;\n\n\t\tif (!--npages)\n\t\t\tbreak;\n\t}\n\n\treturn pages_set;\n}\n\nstatic int reclaim_pages_cmd(struct mlx5_core_dev *dev,\n\t\t\t     u32 *in, int in_size, u32 *out, int out_size)\n{\n\tstruct rb_root *root;\n\tstruct fw_page *fwp;\n\tstruct rb_node *p;\n\tbool ec_function;\n\tu32 func_id;\n\tu32 npages;\n\tu32 i = 0;\n\n\tif (!mlx5_cmd_is_down(dev))\n\t\treturn mlx5_cmd_do(dev, in, in_size, out, out_size);\n\n\t \n\tnpages = MLX5_GET(manage_pages_in, in, input_num_entries);\n\tfunc_id = MLX5_GET(manage_pages_in, in, function_id);\n\tec_function = MLX5_GET(manage_pages_in, in, embedded_cpu_function);\n\n\troot = xa_load(&dev->priv.page_root_xa, get_function(func_id, ec_function));\n\tif (WARN_ON_ONCE(!root))\n\t\treturn -EEXIST;\n\n\tp = rb_first(root);\n\twhile (p && i < npages) {\n\t\tfwp = rb_entry(p, struct fw_page, rb_node);\n\t\tp = rb_next(p);\n\n\t\ti += fwp_fill_manage_pages_out(fwp, out, i, npages - i);\n\t}\n\n\tMLX5_SET(manage_pages_out, out, output_num_entries, i);\n\treturn 0;\n}\n\nstatic int reclaim_pages(struct mlx5_core_dev *dev, u16 func_id, int npages,\n\t\t\t int *nclaimed, bool event, bool ec_function)\n{\n\tu32 function = get_function(func_id, ec_function);\n\tint outlen = MLX5_ST_SZ_BYTES(manage_pages_out);\n\tu32 in[MLX5_ST_SZ_DW(manage_pages_in)] = {};\n\tint num_claimed;\n\tu16 func_type;\n\tu32 *out;\n\tint err;\n\tint i;\n\n\tif (nclaimed)\n\t\t*nclaimed = 0;\n\n\toutlen += npages * MLX5_FLD_SZ_BYTES(manage_pages_out, pas[0]);\n\tout = kvzalloc(outlen, GFP_KERNEL);\n\tif (!out)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(manage_pages_in, in, opcode, MLX5_CMD_OP_MANAGE_PAGES);\n\tMLX5_SET(manage_pages_in, in, op_mod, MLX5_PAGES_TAKE);\n\tMLX5_SET(manage_pages_in, in, function_id, func_id);\n\tMLX5_SET(manage_pages_in, in, input_num_entries, npages);\n\tMLX5_SET(manage_pages_in, in, embedded_cpu_function, ec_function);\n\n\tmlx5_core_dbg(dev, \"func 0x%x, npages %d, outlen %d\\n\",\n\t\t      func_id, npages, outlen);\n\terr = reclaim_pages_cmd(dev, in, sizeof(in), out, outlen);\n\tif (err) {\n\t\tnpages = MLX5_GET(manage_pages_in, in, input_num_entries);\n\t\tdev->priv.reclaim_pages_discard += npages;\n\t}\n\t \n\tif (event && err == -EREMOTEIO) {\n\t\terr = 0;\n\t\tgoto out_free;\n\t}\n\n\terr = mlx5_cmd_check(dev, err, in, out);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed reclaiming pages: err %d\\n\", err);\n\t\tgoto out_free;\n\t}\n\n\tnum_claimed = MLX5_GET(manage_pages_out, out, output_num_entries);\n\tif (num_claimed > npages) {\n\t\tmlx5_core_warn(dev, \"fw returned %d, driver asked %d => corruption\\n\",\n\t\t\t       num_claimed, npages);\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tfor (i = 0; i < num_claimed; i++)\n\t\tfree_4k(dev, MLX5_GET64(manage_pages_out, out, pas[i]), function);\n\n\tif (nclaimed)\n\t\t*nclaimed = num_claimed;\n\n\tfunc_type = func_id_to_type(dev, func_id, ec_function);\n\tdev->priv.page_counters[func_type] -= num_claimed;\n\tdev->priv.fw_pages -= num_claimed;\n\nout_free:\n\tkvfree(out);\n\treturn err;\n}\n\nstatic void pages_work_handler(struct work_struct *work)\n{\n\tstruct mlx5_pages_req *req = container_of(work, struct mlx5_pages_req, work);\n\tstruct mlx5_core_dev *dev = req->dev;\n\tint err = 0;\n\n\tif (req->release_all)\n\t\trelease_all_pages(dev, req->func_id, req->ec_function);\n\telse if (req->npages < 0)\n\t\terr = reclaim_pages(dev, req->func_id, -1 * req->npages, NULL,\n\t\t\t\t    true, req->ec_function);\n\telse if (req->npages > 0)\n\t\terr = give_pages(dev, req->func_id, req->npages, 1, req->ec_function);\n\n\tif (err)\n\t\tmlx5_core_warn(dev, \"%s fail %d\\n\",\n\t\t\t       req->npages < 0 ? \"reclaim\" : \"give\", err);\n\n\tkfree(req);\n}\n\nenum {\n\tEC_FUNCTION_MASK = 0x8000,\n\tRELEASE_ALL_PAGES_MASK = 0x4000,\n};\n\nstatic int req_pages_handler(struct notifier_block *nb,\n\t\t\t     unsigned long type, void *data)\n{\n\tstruct mlx5_pages_req *req;\n\tstruct mlx5_core_dev *dev;\n\tstruct mlx5_priv *priv;\n\tstruct mlx5_eqe *eqe;\n\tbool ec_function;\n\tbool release_all;\n\tu16 func_id;\n\ts32 npages;\n\n\tpriv = mlx5_nb_cof(nb, struct mlx5_priv, pg_nb);\n\tdev  = container_of(priv, struct mlx5_core_dev, priv);\n\teqe  = data;\n\n\tfunc_id = be16_to_cpu(eqe->data.req_pages.func_id);\n\tnpages  = be32_to_cpu(eqe->data.req_pages.num_pages);\n\tec_function = be16_to_cpu(eqe->data.req_pages.ec_function) & EC_FUNCTION_MASK;\n\trelease_all = be16_to_cpu(eqe->data.req_pages.ec_function) &\n\t\t      RELEASE_ALL_PAGES_MASK;\n\tmlx5_core_dbg(dev, \"page request for func 0x%x, npages %d, release_all %d\\n\",\n\t\t      func_id, npages, release_all);\n\treq = kzalloc(sizeof(*req), GFP_ATOMIC);\n\tif (!req) {\n\t\tmlx5_core_warn(dev, \"failed to allocate pages request\\n\");\n\t\treturn NOTIFY_DONE;\n\t}\n\n\treq->dev = dev;\n\treq->func_id = func_id;\n\treq->npages = npages;\n\treq->ec_function = ec_function;\n\treq->release_all = release_all;\n\tINIT_WORK(&req->work, pages_work_handler);\n\tqueue_work(dev->priv.pg_wq, &req->work);\n\treturn NOTIFY_OK;\n}\n\nint mlx5_satisfy_startup_pages(struct mlx5_core_dev *dev, int boot)\n{\n\tu16 func_id;\n\ts32 npages;\n\tint err;\n\n\terr = mlx5_cmd_query_pages(dev, &func_id, &npages, boot);\n\tif (err)\n\t\treturn err;\n\n\tmlx5_core_dbg(dev, \"requested %d %s pages for func_id 0x%x\\n\",\n\t\t      npages, boot ? \"boot\" : \"init\", func_id);\n\n\treturn give_pages(dev, func_id, npages, 0, mlx5_core_is_ecpf(dev));\n}\n\nenum {\n\tMLX5_BLKS_FOR_RECLAIM_PAGES = 12\n};\n\nstatic int optimal_reclaimed_pages(void)\n{\n\tstruct mlx5_cmd_prot_block *block;\n\tstruct mlx5_cmd_layout *lay;\n\tint ret;\n\n\tret = (sizeof(lay->out) + MLX5_BLKS_FOR_RECLAIM_PAGES * sizeof(block->data) -\n\t       MLX5_ST_SZ_BYTES(manage_pages_out)) /\n\t       MLX5_FLD_SZ_BYTES(manage_pages_out, pas[0]);\n\n\treturn ret;\n}\n\nstatic int mlx5_reclaim_root_pages(struct mlx5_core_dev *dev,\n\t\t\t\t   struct rb_root *root, u32 function)\n{\n\tu64 recl_pages_to_jiffies = msecs_to_jiffies(mlx5_tout_ms(dev, RECLAIM_PAGES));\n\tunsigned long end = jiffies + recl_pages_to_jiffies;\n\n\twhile (!RB_EMPTY_ROOT(root)) {\n\t\tu32 ec_function = mlx5_get_ec_function(function);\n\t\tu32 function_id = mlx5_get_func_id(function);\n\t\tint nclaimed;\n\t\tint err;\n\n\t\terr = reclaim_pages(dev, function_id, optimal_reclaimed_pages(),\n\t\t\t\t    &nclaimed, false, ec_function);\n\t\tif (err) {\n\t\t\tmlx5_core_warn(dev, \"reclaim_pages err (%d) func_id=0x%x ec_func=0x%x\\n\",\n\t\t\t\t       err, function_id, ec_function);\n\t\t\treturn err;\n\t\t}\n\n\t\tif (nclaimed)\n\t\t\tend = jiffies + recl_pages_to_jiffies;\n\n\t\tif (time_after(jiffies, end)) {\n\t\t\tmlx5_core_warn(dev, \"FW did not return all pages. giving up...\\n\");\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint mlx5_reclaim_startup_pages(struct mlx5_core_dev *dev)\n{\n\tstruct rb_root *root;\n\tunsigned long id;\n\tvoid *entry;\n\n\txa_for_each(&dev->priv.page_root_xa, id, entry) {\n\t\troot = entry;\n\t\tmlx5_reclaim_root_pages(dev, root, id);\n\t\txa_erase(&dev->priv.page_root_xa, id);\n\t\tkfree(root);\n\t}\n\n\tWARN_ON(!xa_empty(&dev->priv.page_root_xa));\n\n\tWARN(dev->priv.fw_pages,\n\t     \"FW pages counter is %d after reclaiming all pages\\n\",\n\t     dev->priv.fw_pages);\n\tWARN(dev->priv.page_counters[MLX5_VF],\n\t     \"VFs FW pages counter is %d after reclaiming all pages\\n\",\n\t     dev->priv.page_counters[MLX5_VF]);\n\tWARN(dev->priv.page_counters[MLX5_HOST_PF],\n\t     \"External host PF FW pages counter is %d after reclaiming all pages\\n\",\n\t     dev->priv.page_counters[MLX5_HOST_PF]);\n\tWARN(dev->priv.page_counters[MLX5_EC_VF],\n\t     \"EC VFs FW pages counter is %d after reclaiming all pages\\n\",\n\t     dev->priv.page_counters[MLX5_EC_VF]);\n\n\treturn 0;\n}\n\nint mlx5_pagealloc_init(struct mlx5_core_dev *dev)\n{\n\tINIT_LIST_HEAD(&dev->priv.free_list);\n\tdev->priv.pg_wq = create_singlethread_workqueue(\"mlx5_page_allocator\");\n\tif (!dev->priv.pg_wq)\n\t\treturn -ENOMEM;\n\n\txa_init(&dev->priv.page_root_xa);\n\tmlx5_pages_debugfs_init(dev);\n\n\treturn 0;\n}\n\nvoid mlx5_pagealloc_cleanup(struct mlx5_core_dev *dev)\n{\n\tmlx5_pages_debugfs_cleanup(dev);\n\txa_destroy(&dev->priv.page_root_xa);\n\tdestroy_workqueue(dev->priv.pg_wq);\n}\n\nvoid mlx5_pagealloc_start(struct mlx5_core_dev *dev)\n{\n\tMLX5_NB_INIT(&dev->priv.pg_nb, req_pages_handler, PAGE_REQUEST);\n\tmlx5_eq_notifier_register(dev, &dev->priv.pg_nb);\n}\n\nvoid mlx5_pagealloc_stop(struct mlx5_core_dev *dev)\n{\n\tmlx5_eq_notifier_unregister(dev, &dev->priv.pg_nb);\n\tflush_workqueue(dev->priv.pg_wq);\n}\n\nint mlx5_wait_for_pages(struct mlx5_core_dev *dev, int *pages)\n{\n\tu64 recl_vf_pages_to_jiffies = msecs_to_jiffies(mlx5_tout_ms(dev, RECLAIM_VFS_PAGES));\n\tunsigned long end = jiffies + recl_vf_pages_to_jiffies;\n\tint prev_pages = *pages;\n\n\t \n\tif (dev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR) {\n\t\tmlx5_core_warn(dev, \"Skipping wait for vf pages stage\");\n\t\treturn 0;\n\t}\n\n\tmlx5_core_dbg(dev, \"Waiting for %d pages\\n\", prev_pages);\n\twhile (*pages) {\n\t\tif (time_after(jiffies, end)) {\n\t\t\tmlx5_core_warn(dev, \"aborting while there are %d pending pages\\n\", *pages);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tif (*pages < prev_pages) {\n\t\t\tend = jiffies + recl_vf_pages_to_jiffies;\n\t\t\tprev_pages = *pages;\n\t\t}\n\t\tmsleep(50);\n\t}\n\n\tmlx5_core_dbg(dev, \"All pages received\\n\");\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}