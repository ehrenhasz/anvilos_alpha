{
  "module_name": "pci_irq.c",
  "hash_id": "66da98be9aac374d501480c3940c95d03df2e6217a9c911f028f681acee8a614",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/pci_irq.c",
  "human_readable_source": "\n \n\n#include <linux/pci.h>\n#include <linux/interrupt.h>\n#include <linux/notifier.h>\n#include <linux/mlx5/driver.h>\n#include <linux/mlx5/vport.h>\n#include \"mlx5_core.h\"\n#include \"mlx5_irq.h\"\n#include \"pci_irq.h\"\n#include \"lib/sf.h\"\n#include \"lib/eq.h\"\n#ifdef CONFIG_RFS_ACCEL\n#include <linux/cpu_rmap.h>\n#endif\n\n#define MLX5_SFS_PER_CTRL_IRQ 64\n#define MLX5_IRQ_CTRL_SF_MAX 8\n \n#define MLX5_IRQ_VEC_COMP_BASE_SF 2\n\n#define MLX5_EQ_SHARE_IRQ_MAX_COMP (8)\n#define MLX5_EQ_SHARE_IRQ_MAX_CTRL (UINT_MAX)\n#define MLX5_EQ_SHARE_IRQ_MIN_COMP (1)\n#define MLX5_EQ_SHARE_IRQ_MIN_CTRL (4)\n\nstruct mlx5_irq {\n\tstruct atomic_notifier_head nh;\n\tcpumask_var_t mask;\n\tchar name[MLX5_MAX_IRQ_FORMATTED_NAME];\n\tstruct mlx5_irq_pool *pool;\n\tint refcount;\n\tstruct msi_map map;\n\tu32 pool_index;\n};\n\nstruct mlx5_irq_table {\n\tstruct mlx5_irq_pool *pcif_pool;\n\tstruct mlx5_irq_pool *sf_ctrl_pool;\n\tstruct mlx5_irq_pool *sf_comp_pool;\n};\n\nstatic int mlx5_core_func_to_vport(const struct mlx5_core_dev *dev,\n\t\t\t\t   int func,\n\t\t\t\t   bool ec_vf_func)\n{\n\tif (!ec_vf_func)\n\t\treturn func;\n\treturn mlx5_core_ec_vf_vport_base(dev) + func - 1;\n}\n\n \nint mlx5_get_default_msix_vec_count(struct mlx5_core_dev *dev, int num_vfs)\n{\n\tint num_vf_msix, min_msix, max_msix;\n\n\tnum_vf_msix = MLX5_CAP_GEN_MAX(dev, num_total_dynamic_vf_msix);\n\tif (!num_vf_msix)\n\t\treturn 0;\n\n\tmin_msix = MLX5_CAP_GEN(dev, min_dynamic_vf_msix_table_size);\n\tmax_msix = MLX5_CAP_GEN(dev, max_dynamic_vf_msix_table_size);\n\n\t \n\treturn max(min(num_vf_msix / num_vfs, max_msix / 2), min_msix);\n}\n\n \nint mlx5_set_msix_vec_count(struct mlx5_core_dev *dev, int function_id,\n\t\t\t    int msix_vec_count)\n{\n\tint query_sz = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tint set_sz = MLX5_ST_SZ_BYTES(set_hca_cap_in);\n\tvoid *hca_cap = NULL, *query_cap = NULL, *cap;\n\tint num_vf_msix, min_msix, max_msix;\n\tbool ec_vf_function;\n\tint vport;\n\tint ret;\n\n\tnum_vf_msix = MLX5_CAP_GEN_MAX(dev, num_total_dynamic_vf_msix);\n\tif (!num_vf_msix)\n\t\treturn 0;\n\n\tif (!MLX5_CAP_GEN(dev, vport_group_manager) || !mlx5_core_is_pf(dev))\n\t\treturn -EOPNOTSUPP;\n\n\tmin_msix = MLX5_CAP_GEN(dev, min_dynamic_vf_msix_table_size);\n\tmax_msix = MLX5_CAP_GEN(dev, max_dynamic_vf_msix_table_size);\n\n\tif (msix_vec_count < min_msix)\n\t\treturn -EINVAL;\n\n\tif (msix_vec_count > max_msix)\n\t\treturn -EOVERFLOW;\n\n\tquery_cap = kvzalloc(query_sz, GFP_KERNEL);\n\thca_cap = kvzalloc(set_sz, GFP_KERNEL);\n\tif (!hca_cap || !query_cap) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tec_vf_function = mlx5_core_ec_sriov_enabled(dev);\n\tvport = mlx5_core_func_to_vport(dev, function_id, ec_vf_function);\n\tret = mlx5_vport_get_other_func_general_cap(dev, vport, query_cap);\n\tif (ret)\n\t\tgoto out;\n\n\tcap = MLX5_ADDR_OF(set_hca_cap_in, hca_cap, capability);\n\tmemcpy(cap, MLX5_ADDR_OF(query_hca_cap_out, query_cap, capability),\n\t       MLX5_UN_SZ_BYTES(hca_cap_union));\n\tMLX5_SET(cmd_hca_cap, cap, dynamic_msix_table_size, msix_vec_count);\n\n\tMLX5_SET(set_hca_cap_in, hca_cap, opcode, MLX5_CMD_OP_SET_HCA_CAP);\n\tMLX5_SET(set_hca_cap_in, hca_cap, other_function, 1);\n\tMLX5_SET(set_hca_cap_in, hca_cap, ec_vf_function, ec_vf_function);\n\tMLX5_SET(set_hca_cap_in, hca_cap, function_id, function_id);\n\n\tMLX5_SET(set_hca_cap_in, hca_cap, op_mod,\n\t\t MLX5_SET_HCA_CAP_OP_MOD_GENERAL_DEVICE << 1);\n\tret = mlx5_cmd_exec_in(dev, set_hca_cap, hca_cap);\nout:\n\tkvfree(hca_cap);\n\tkvfree(query_cap);\n\treturn ret;\n}\n\n \nstatic void mlx5_system_free_irq(struct mlx5_irq *irq)\n{\n\tstruct mlx5_irq_pool *pool = irq->pool;\n#ifdef CONFIG_RFS_ACCEL\n\tstruct cpu_rmap *rmap;\n#endif\n\n\t \n\tirq_update_affinity_hint(irq->map.virq, NULL);\n#ifdef CONFIG_RFS_ACCEL\n\trmap = mlx5_eq_table_get_rmap(pool->dev);\n\tif (rmap)\n\t\tirq_cpu_rmap_remove(rmap, irq->map.virq);\n#endif\n\n\tfree_irq(irq->map.virq, &irq->nh);\n\tif (irq->map.index && pci_msix_can_alloc_dyn(pool->dev->pdev))\n\t\tpci_msix_free_irq(pool->dev->pdev, irq->map);\n}\n\nstatic void irq_release(struct mlx5_irq *irq)\n{\n\tstruct mlx5_irq_pool *pool = irq->pool;\n\n\txa_erase(&pool->irqs, irq->pool_index);\n\tmlx5_system_free_irq(irq);\n\tfree_cpumask_var(irq->mask);\n\tkfree(irq);\n}\n\nint mlx5_irq_put(struct mlx5_irq *irq)\n{\n\tstruct mlx5_irq_pool *pool = irq->pool;\n\tint ret = 0;\n\n\tmutex_lock(&pool->lock);\n\tirq->refcount--;\n\tif (!irq->refcount) {\n\t\tirq_release(irq);\n\t\tret = 1;\n\t}\n\tmutex_unlock(&pool->lock);\n\treturn ret;\n}\n\nint mlx5_irq_read_locked(struct mlx5_irq *irq)\n{\n\tlockdep_assert_held(&irq->pool->lock);\n\treturn irq->refcount;\n}\n\nint mlx5_irq_get_locked(struct mlx5_irq *irq)\n{\n\tlockdep_assert_held(&irq->pool->lock);\n\tif (WARN_ON_ONCE(!irq->refcount))\n\t\treturn 0;\n\tirq->refcount++;\n\treturn 1;\n}\n\nstatic int irq_get(struct mlx5_irq *irq)\n{\n\tint err;\n\n\tmutex_lock(&irq->pool->lock);\n\terr = mlx5_irq_get_locked(irq);\n\tmutex_unlock(&irq->pool->lock);\n\treturn err;\n}\n\nstatic irqreturn_t irq_int_handler(int irq, void *nh)\n{\n\tatomic_notifier_call_chain(nh, 0, NULL);\n\treturn IRQ_HANDLED;\n}\n\nstatic void irq_sf_set_name(struct mlx5_irq_pool *pool, char *name, int vecidx)\n{\n\tsnprintf(name, MLX5_MAX_IRQ_NAME, \"%s%d\", pool->name, vecidx);\n}\n\nstatic void irq_set_name(struct mlx5_irq_pool *pool, char *name, int vecidx)\n{\n\tif (!pool->xa_num_irqs.max) {\n\t\t \n\t\tsnprintf(name, MLX5_MAX_IRQ_NAME, \"mlx5_combined%d\", vecidx);\n\t\treturn;\n\t}\n\n\tif (!vecidx) {\n\t\tsnprintf(name, MLX5_MAX_IRQ_NAME, \"mlx5_async%d\", vecidx);\n\t\treturn;\n\t}\n\n\tsnprintf(name, MLX5_MAX_IRQ_NAME, \"mlx5_comp%d\", vecidx);\n}\n\nstruct mlx5_irq *mlx5_irq_alloc(struct mlx5_irq_pool *pool, int i,\n\t\t\t\tstruct irq_affinity_desc *af_desc,\n\t\t\t\tstruct cpu_rmap **rmap)\n{\n\tstruct mlx5_core_dev *dev = pool->dev;\n\tchar name[MLX5_MAX_IRQ_NAME];\n\tstruct mlx5_irq *irq;\n\tint err;\n\n\tirq = kzalloc(sizeof(*irq), GFP_KERNEL);\n\tif (!irq || !zalloc_cpumask_var(&irq->mask, GFP_KERNEL)) {\n\t\tkfree(irq);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tif (!i || !pci_msix_can_alloc_dyn(dev->pdev)) {\n\t\t \n\t\tirq->map.virq = pci_irq_vector(dev->pdev, i);\n\t\tirq->map.index = i;\n\t} else {\n\t\tirq->map = pci_msix_alloc_irq_at(dev->pdev, MSI_ANY_INDEX, af_desc);\n\t\tif (!irq->map.virq) {\n\t\t\terr = irq->map.index;\n\t\t\tgoto err_alloc_irq;\n\t\t}\n\t}\n\n\tif (i && rmap && *rmap) {\n#ifdef CONFIG_RFS_ACCEL\n\t\terr = irq_cpu_rmap_add(*rmap, irq->map.virq);\n\t\tif (err)\n\t\t\tgoto err_irq_rmap;\n#endif\n\t}\n\tif (!mlx5_irq_pool_is_sf_pool(pool))\n\t\tirq_set_name(pool, name, i);\n\telse\n\t\tirq_sf_set_name(pool, name, i);\n\tATOMIC_INIT_NOTIFIER_HEAD(&irq->nh);\n\tsnprintf(irq->name, MLX5_MAX_IRQ_FORMATTED_NAME,\n\t\t MLX5_IRQ_NAME_FORMAT_STR, name, pci_name(dev->pdev));\n\terr = request_irq(irq->map.virq, irq_int_handler, 0, irq->name,\n\t\t\t  &irq->nh);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to request irq. err = %d\\n\", err);\n\t\tgoto err_req_irq;\n\t}\n\n\tif (af_desc) {\n\t\tcpumask_copy(irq->mask, &af_desc->mask);\n\t\tirq_set_affinity_and_hint(irq->map.virq, irq->mask);\n\t}\n\tirq->pool = pool;\n\tirq->refcount = 1;\n\tirq->pool_index = i;\n\terr = xa_err(xa_store(&pool->irqs, irq->pool_index, irq, GFP_KERNEL));\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to alloc xa entry for irq(%u). err = %d\\n\",\n\t\t\t      irq->pool_index, err);\n\t\tgoto err_xa;\n\t}\n\treturn irq;\nerr_xa:\n\tif (af_desc)\n\t\tirq_update_affinity_hint(irq->map.virq, NULL);\n\tfree_irq(irq->map.virq, &irq->nh);\nerr_req_irq:\n#ifdef CONFIG_RFS_ACCEL\n\tif (i && rmap && *rmap) {\n\t\tfree_irq_cpu_rmap(*rmap);\n\t\t*rmap = NULL;\n\t}\nerr_irq_rmap:\n#endif\n\tif (i && pci_msix_can_alloc_dyn(dev->pdev))\n\t\tpci_msix_free_irq(dev->pdev, irq->map);\nerr_alloc_irq:\n\tfree_cpumask_var(irq->mask);\n\tkfree(irq);\n\treturn ERR_PTR(err);\n}\n\nint mlx5_irq_attach_nb(struct mlx5_irq *irq, struct notifier_block *nb)\n{\n\tint ret;\n\n\tret = irq_get(irq);\n\tif (!ret)\n\t\t \n\t\treturn -ENOENT;\n\tret = atomic_notifier_chain_register(&irq->nh, nb);\n\tif (ret)\n\t\tmlx5_irq_put(irq);\n\treturn ret;\n}\n\nint mlx5_irq_detach_nb(struct mlx5_irq *irq, struct notifier_block *nb)\n{\n\tint err = 0;\n\n\terr = atomic_notifier_chain_unregister(&irq->nh, nb);\n\tmlx5_irq_put(irq);\n\treturn err;\n}\n\nstruct cpumask *mlx5_irq_get_affinity_mask(struct mlx5_irq *irq)\n{\n\treturn irq->mask;\n}\n\nint mlx5_irq_get_index(struct mlx5_irq *irq)\n{\n\treturn irq->map.index;\n}\n\n \n\n \nstatic struct mlx5_irq *\nirq_pool_request_vector(struct mlx5_irq_pool *pool, int vecidx,\n\t\t\tstruct irq_affinity_desc *af_desc,\n\t\t\tstruct cpu_rmap **rmap)\n{\n\tstruct mlx5_irq *irq;\n\n\tmutex_lock(&pool->lock);\n\tirq = xa_load(&pool->irqs, vecidx);\n\tif (irq) {\n\t\tmlx5_irq_get_locked(irq);\n\t\tgoto unlock;\n\t}\n\tirq = mlx5_irq_alloc(pool, vecidx, af_desc, rmap);\nunlock:\n\tmutex_unlock(&pool->lock);\n\treturn irq;\n}\n\nstatic struct mlx5_irq_pool *sf_ctrl_irq_pool_get(struct mlx5_irq_table *irq_table)\n{\n\treturn irq_table->sf_ctrl_pool;\n}\n\nstatic struct mlx5_irq_pool *sf_irq_pool_get(struct mlx5_irq_table *irq_table)\n{\n\treturn irq_table->sf_comp_pool;\n}\n\nstruct mlx5_irq_pool *mlx5_irq_pool_get(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_irq_table *irq_table = mlx5_irq_table_get(dev);\n\tstruct mlx5_irq_pool *pool = NULL;\n\n\tif (mlx5_core_is_sf(dev))\n\t\tpool = sf_irq_pool_get(irq_table);\n\n\t \n\treturn pool ? pool : irq_table->pcif_pool;\n}\n\nstatic struct mlx5_irq_pool *ctrl_irq_pool_get(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_irq_table *irq_table = mlx5_irq_table_get(dev);\n\tstruct mlx5_irq_pool *pool = NULL;\n\n\tif (mlx5_core_is_sf(dev))\n\t\tpool = sf_ctrl_irq_pool_get(irq_table);\n\n\t \n\treturn pool ? pool : irq_table->pcif_pool;\n}\n\nstatic void _mlx5_irq_release(struct mlx5_irq *irq)\n{\n\tsynchronize_irq(irq->map.virq);\n\tmlx5_irq_put(irq);\n}\n\n \nvoid mlx5_ctrl_irq_release(struct mlx5_irq *ctrl_irq)\n{\n\t_mlx5_irq_release(ctrl_irq);\n}\n\n \nstruct mlx5_irq *mlx5_ctrl_irq_request(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_irq_pool *pool = ctrl_irq_pool_get(dev);\n\tstruct irq_affinity_desc af_desc;\n\tstruct mlx5_irq *irq;\n\n\tcpumask_copy(&af_desc.mask, cpu_online_mask);\n\taf_desc.is_managed = false;\n\tif (!mlx5_irq_pool_is_sf_pool(pool)) {\n\t\t \n\t\tif (!pool->xa_num_irqs.max) {\n\t\t\tcpumask_clear(&af_desc.mask);\n\t\t\t \n\t\t\tcpumask_set_cpu(cpumask_first(cpu_online_mask), &af_desc.mask);\n\t\t}\n\t\t \n\t\tirq = irq_pool_request_vector(pool, 0, &af_desc, NULL);\n\t} else {\n\t\tirq = mlx5_irq_affinity_request(pool, &af_desc);\n\t}\n\n\treturn irq;\n}\n\n \nstruct mlx5_irq *mlx5_irq_request(struct mlx5_core_dev *dev, u16 vecidx,\n\t\t\t\t  struct irq_affinity_desc *af_desc,\n\t\t\t\t  struct cpu_rmap **rmap)\n{\n\tstruct mlx5_irq_table *irq_table = mlx5_irq_table_get(dev);\n\tstruct mlx5_irq_pool *pool;\n\tstruct mlx5_irq *irq;\n\n\tpool = irq_table->pcif_pool;\n\tirq = irq_pool_request_vector(pool, vecidx, af_desc, rmap);\n\tif (IS_ERR(irq))\n\t\treturn irq;\n\tmlx5_core_dbg(dev, \"irq %u mapped to cpu %*pbl, %u EQs on this irq\\n\",\n\t\t      irq->map.virq, cpumask_pr_args(&af_desc->mask),\n\t\t      irq->refcount / MLX5_EQ_REFS_PER_IRQ);\n\treturn irq;\n}\n\n \nstruct msi_map mlx5_msix_alloc(struct mlx5_core_dev *dev,\n\t\t\t       irqreturn_t (*handler)(int, void *),\n\t\t\t       const struct irq_affinity_desc *affdesc,\n\t\t\t       const char *name)\n{\n\tstruct msi_map map;\n\tint err;\n\n\tif (!dev->pdev) {\n\t\tmap.virq = 0;\n\t\tmap.index = -EINVAL;\n\t\treturn map;\n\t}\n\n\tmap = pci_msix_alloc_irq_at(dev->pdev, MSI_ANY_INDEX, affdesc);\n\tif (!map.virq)\n\t\treturn map;\n\n\terr = request_irq(map.virq, handler, 0, name, NULL);\n\tif (err) {\n\t\tmlx5_core_warn(dev, \"err %d\\n\", err);\n\t\tpci_msix_free_irq(dev->pdev, map);\n\t\tmap.virq = 0;\n\t\tmap.index = -ENOMEM;\n\t}\n\treturn map;\n}\nEXPORT_SYMBOL(mlx5_msix_alloc);\n\n \nvoid mlx5_msix_free(struct mlx5_core_dev *dev, struct msi_map map)\n{\n\tfree_irq(map.virq, NULL);\n\tpci_msix_free_irq(dev->pdev, map);\n}\nEXPORT_SYMBOL(mlx5_msix_free);\n\n \nvoid mlx5_irq_release_vector(struct mlx5_irq *irq)\n{\n\t_mlx5_irq_release(irq);\n}\n\n \nstruct mlx5_irq *mlx5_irq_request_vector(struct mlx5_core_dev *dev, u16 cpu,\n\t\t\t\t\t u16 vecidx, struct cpu_rmap **rmap)\n{\n\tstruct mlx5_irq_table *table = mlx5_irq_table_get(dev);\n\tstruct mlx5_irq_pool *pool = table->pcif_pool;\n\tstruct irq_affinity_desc af_desc;\n\tint offset = 1;\n\n\tif (!pool->xa_num_irqs.max)\n\t\toffset = 0;\n\n\taf_desc.is_managed = false;\n\tcpumask_clear(&af_desc.mask);\n\tcpumask_set_cpu(cpu, &af_desc.mask);\n\treturn mlx5_irq_request(dev, vecidx + offset, &af_desc, rmap);\n}\n\nstatic struct mlx5_irq_pool *\nirq_pool_alloc(struct mlx5_core_dev *dev, int start, int size, char *name,\n\t       u32 min_threshold, u32 max_threshold)\n{\n\tstruct mlx5_irq_pool *pool = kvzalloc(sizeof(*pool), GFP_KERNEL);\n\n\tif (!pool)\n\t\treturn ERR_PTR(-ENOMEM);\n\tpool->dev = dev;\n\tmutex_init(&pool->lock);\n\txa_init_flags(&pool->irqs, XA_FLAGS_ALLOC);\n\tpool->xa_num_irqs.min = start;\n\tpool->xa_num_irqs.max = start + size - 1;\n\tif (name)\n\t\tsnprintf(pool->name, MLX5_MAX_IRQ_NAME - MLX5_MAX_IRQ_IDX_CHARS,\n\t\t\t \"%s\", name);\n\tpool->min_threshold = min_threshold * MLX5_EQ_REFS_PER_IRQ;\n\tpool->max_threshold = max_threshold * MLX5_EQ_REFS_PER_IRQ;\n\tmlx5_core_dbg(dev, \"pool->name = %s, pool->size = %d, pool->start = %d\",\n\t\t      name, size, start);\n\treturn pool;\n}\n\nstatic void irq_pool_free(struct mlx5_irq_pool *pool)\n{\n\tstruct mlx5_irq *irq;\n\tunsigned long index;\n\n\t \n\txa_for_each(&pool->irqs, index, irq)\n\t\tirq_release(irq);\n\txa_destroy(&pool->irqs);\n\tmutex_destroy(&pool->lock);\n\tkfree(pool->irqs_per_cpu);\n\tkvfree(pool);\n}\n\nstatic int irq_pools_init(struct mlx5_core_dev *dev, int sf_vec, int pcif_vec)\n{\n\tstruct mlx5_irq_table *table = dev->priv.irq_table;\n\tint num_sf_ctrl_by_msix;\n\tint num_sf_ctrl_by_sfs;\n\tint num_sf_ctrl;\n\tint err;\n\n\t \n\ttable->pcif_pool = irq_pool_alloc(dev, 0, pcif_vec, NULL,\n\t\t\t\t\t  MLX5_EQ_SHARE_IRQ_MIN_COMP,\n\t\t\t\t\t  MLX5_EQ_SHARE_IRQ_MAX_COMP);\n\tif (IS_ERR(table->pcif_pool))\n\t\treturn PTR_ERR(table->pcif_pool);\n\tif (!mlx5_sf_max_functions(dev))\n\t\treturn 0;\n\tif (sf_vec < MLX5_IRQ_VEC_COMP_BASE_SF) {\n\t\tmlx5_core_dbg(dev, \"Not enught IRQs for SFs. SF may run at lower performance\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tnum_sf_ctrl_by_msix = DIV_ROUND_UP(sf_vec, MLX5_COMP_EQS_PER_SF);\n\tnum_sf_ctrl_by_sfs = DIV_ROUND_UP(mlx5_sf_max_functions(dev),\n\t\t\t\t\t  MLX5_SFS_PER_CTRL_IRQ);\n\tnum_sf_ctrl = min_t(int, num_sf_ctrl_by_msix, num_sf_ctrl_by_sfs);\n\tnum_sf_ctrl = min_t(int, MLX5_IRQ_CTRL_SF_MAX, num_sf_ctrl);\n\ttable->sf_ctrl_pool = irq_pool_alloc(dev, pcif_vec, num_sf_ctrl,\n\t\t\t\t\t     \"mlx5_sf_ctrl\",\n\t\t\t\t\t     MLX5_EQ_SHARE_IRQ_MIN_CTRL,\n\t\t\t\t\t     MLX5_EQ_SHARE_IRQ_MAX_CTRL);\n\tif (IS_ERR(table->sf_ctrl_pool)) {\n\t\terr = PTR_ERR(table->sf_ctrl_pool);\n\t\tgoto err_pf;\n\t}\n\t \n\ttable->sf_comp_pool = irq_pool_alloc(dev, pcif_vec + num_sf_ctrl,\n\t\t\t\t\t     sf_vec - num_sf_ctrl, \"mlx5_sf_comp\",\n\t\t\t\t\t     MLX5_EQ_SHARE_IRQ_MIN_COMP,\n\t\t\t\t\t     MLX5_EQ_SHARE_IRQ_MAX_COMP);\n\tif (IS_ERR(table->sf_comp_pool)) {\n\t\terr = PTR_ERR(table->sf_comp_pool);\n\t\tgoto err_sf_ctrl;\n\t}\n\n\ttable->sf_comp_pool->irqs_per_cpu = kcalloc(nr_cpu_ids, sizeof(u16), GFP_KERNEL);\n\tif (!table->sf_comp_pool->irqs_per_cpu) {\n\t\terr = -ENOMEM;\n\t\tgoto err_irqs_per_cpu;\n\t}\n\n\treturn 0;\n\nerr_irqs_per_cpu:\n\tirq_pool_free(table->sf_comp_pool);\nerr_sf_ctrl:\n\tirq_pool_free(table->sf_ctrl_pool);\nerr_pf:\n\tirq_pool_free(table->pcif_pool);\n\treturn err;\n}\n\nstatic void irq_pools_destroy(struct mlx5_irq_table *table)\n{\n\tif (table->sf_ctrl_pool) {\n\t\tirq_pool_free(table->sf_comp_pool);\n\t\tirq_pool_free(table->sf_ctrl_pool);\n\t}\n\tirq_pool_free(table->pcif_pool);\n}\n\nstatic void mlx5_irq_pool_free_irqs(struct mlx5_irq_pool *pool)\n{\n\tstruct mlx5_irq *irq;\n\tunsigned long index;\n\n\txa_for_each(&pool->irqs, index, irq)\n\t\tmlx5_system_free_irq(irq);\n\n}\n\nstatic void mlx5_irq_pools_free_irqs(struct mlx5_irq_table *table)\n{\n\tif (table->sf_ctrl_pool) {\n\t\tmlx5_irq_pool_free_irqs(table->sf_comp_pool);\n\t\tmlx5_irq_pool_free_irqs(table->sf_ctrl_pool);\n\t}\n\tmlx5_irq_pool_free_irqs(table->pcif_pool);\n}\n\n \n\nint mlx5_irq_table_init(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_irq_table *irq_table;\n\n\tif (mlx5_core_is_sf(dev))\n\t\treturn 0;\n\n\tirq_table = kvzalloc_node(sizeof(*irq_table), GFP_KERNEL,\n\t\t\t\t  dev->priv.numa_node);\n\tif (!irq_table)\n\t\treturn -ENOMEM;\n\n\tdev->priv.irq_table = irq_table;\n\treturn 0;\n}\n\nvoid mlx5_irq_table_cleanup(struct mlx5_core_dev *dev)\n{\n\tif (mlx5_core_is_sf(dev))\n\t\treturn;\n\n\tkvfree(dev->priv.irq_table);\n}\n\nint mlx5_irq_table_get_num_comp(struct mlx5_irq_table *table)\n{\n\tif (!table->pcif_pool->xa_num_irqs.max)\n\t\treturn 1;\n\treturn table->pcif_pool->xa_num_irqs.max - table->pcif_pool->xa_num_irqs.min;\n}\n\nint mlx5_irq_table_create(struct mlx5_core_dev *dev)\n{\n\tint num_eqs = MLX5_CAP_GEN(dev, max_num_eqs) ?\n\t\t      MLX5_CAP_GEN(dev, max_num_eqs) :\n\t\t      1 << MLX5_CAP_GEN(dev, log_max_eq);\n\tint total_vec;\n\tint pcif_vec;\n\tint req_vec;\n\tint err;\n\tint n;\n\n\tif (mlx5_core_is_sf(dev))\n\t\treturn 0;\n\n\tpcif_vec = MLX5_CAP_GEN(dev, num_ports) * num_online_cpus() + 1;\n\tpcif_vec = min_t(int, pcif_vec, num_eqs);\n\n\ttotal_vec = pcif_vec;\n\tif (mlx5_sf_max_functions(dev))\n\t\ttotal_vec += MLX5_IRQ_CTRL_SF_MAX +\n\t\t\tMLX5_COMP_EQS_PER_SF * mlx5_sf_max_functions(dev);\n\ttotal_vec = min_t(int, total_vec, pci_msix_vec_count(dev->pdev));\n\tpcif_vec = min_t(int, pcif_vec, pci_msix_vec_count(dev->pdev));\n\n\treq_vec = pci_msix_can_alloc_dyn(dev->pdev) ? 1 : total_vec;\n\tn = pci_alloc_irq_vectors(dev->pdev, 1, req_vec, PCI_IRQ_MSIX);\n\tif (n < 0)\n\t\treturn n;\n\n\terr = irq_pools_init(dev, total_vec - pcif_vec, pcif_vec);\n\tif (err)\n\t\tpci_free_irq_vectors(dev->pdev);\n\n\treturn err;\n}\n\nvoid mlx5_irq_table_destroy(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_irq_table *table = dev->priv.irq_table;\n\n\tif (mlx5_core_is_sf(dev))\n\t\treturn;\n\n\t \n\tirq_pools_destroy(table);\n\tpci_free_irq_vectors(dev->pdev);\n}\n\nvoid mlx5_irq_table_free_irqs(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_irq_table *table = dev->priv.irq_table;\n\n\tif (mlx5_core_is_sf(dev))\n\t\treturn;\n\n\tmlx5_irq_pools_free_irqs(table);\n\tpci_free_irq_vectors(dev->pdev);\n}\n\nint mlx5_irq_table_get_sfs_vec(struct mlx5_irq_table *table)\n{\n\tif (table->sf_comp_pool)\n\t\treturn min_t(int, num_online_cpus(),\n\t\t\t     table->sf_comp_pool->xa_num_irqs.max -\n\t\t\t     table->sf_comp_pool->xa_num_irqs.min + 1);\n\telse\n\t\treturn mlx5_irq_table_get_num_comp(table);\n}\n\nstruct mlx5_irq_table *mlx5_irq_table_get(struct mlx5_core_dev *dev)\n{\n#ifdef CONFIG_MLX5_SF\n\tif (mlx5_core_is_sf(dev))\n\t\treturn dev->priv.parent_mdev->priv.irq_table;\n#endif\n\treturn dev->priv.irq_table;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}