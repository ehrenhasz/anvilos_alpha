{
  "module_name": "en_tx.c",
  "hash_id": "553a8adf70d4db76466c85af61f34d4244f3dffddcfa671b31773f57561b40d7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c",
  "human_readable_source": " \n\n#include <linux/tcp.h>\n#include <linux/if_vlan.h>\n#include <net/geneve.h>\n#include <net/dsfield.h>\n#include \"en.h\"\n#include \"en/txrx.h\"\n#include \"ipoib/ipoib.h\"\n#include \"en_accel/en_accel.h\"\n#include \"en_accel/ipsec_rxtx.h\"\n#include \"en_accel/macsec.h\"\n#include \"en/ptp.h\"\n#include <net/ipv6.h>\n\nstatic void mlx5e_dma_unmap_wqe_err(struct mlx5e_txqsq *sq, u8 num_dma)\n{\n\tint i;\n\n\tfor (i = 0; i < num_dma; i++) {\n\t\tstruct mlx5e_sq_dma *last_pushed_dma =\n\t\t\tmlx5e_dma_get(sq, --sq->dma_fifo_pc);\n\n\t\tmlx5e_tx_dma_unmap(sq->pdev, last_pushed_dma);\n\t}\n}\n\nstatic inline int mlx5e_skb_l2_header_offset(struct sk_buff *skb)\n{\n#define MLX5E_MIN_INLINE (ETH_HLEN + VLAN_HLEN)\n\n\treturn max(skb_network_offset(skb), MLX5E_MIN_INLINE);\n}\n\nstatic inline int mlx5e_skb_l3_header_offset(struct sk_buff *skb)\n{\n\tif (skb_transport_header_was_set(skb))\n\t\treturn skb_transport_offset(skb);\n\telse\n\t\treturn mlx5e_skb_l2_header_offset(skb);\n}\n\nstatic inline u16 mlx5e_calc_min_inline(enum mlx5_inline_modes mode,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tu16 hlen;\n\n\tswitch (mode) {\n\tcase MLX5_INLINE_MODE_NONE:\n\t\treturn 0;\n\tcase MLX5_INLINE_MODE_TCP_UDP:\n\t\thlen = eth_get_headlen(skb->dev, skb->data, skb_headlen(skb));\n\t\tif (hlen == ETH_HLEN && !skb_vlan_tag_present(skb))\n\t\t\thlen += VLAN_HLEN;\n\t\tbreak;\n\tcase MLX5_INLINE_MODE_IP:\n\t\thlen = mlx5e_skb_l3_header_offset(skb);\n\t\tbreak;\n\tcase MLX5_INLINE_MODE_L2:\n\tdefault:\n\t\thlen = mlx5e_skb_l2_header_offset(skb);\n\t}\n\treturn min_t(u16, hlen, skb_headlen(skb));\n}\n\n#define MLX5_UNSAFE_MEMCPY_DISCLAIMER\t\t\t\t\\\n\t\"This copy has been bounds-checked earlier in \"\t\t\\\n\t\"mlx5i_sq_calc_wqe_attr() and intentionally \"\t\t\\\n\t\"crosses a flex array boundary. Since it is \"\t\t\\\n\t\"performance sensitive, splitting the copy is \"\t\t\\\n\t\"undesirable.\"\n\nstatic inline void mlx5e_insert_vlan(void *start, struct sk_buff *skb, u16 ihs)\n{\n\tstruct vlan_ethhdr *vhdr = (struct vlan_ethhdr *)start;\n\tint cpy1_sz = 2 * ETH_ALEN;\n\tint cpy2_sz = ihs - cpy1_sz;\n\n\tmemcpy(&vhdr->addrs, skb->data, cpy1_sz);\n\tvhdr->h_vlan_proto = skb->vlan_proto;\n\tvhdr->h_vlan_TCI = cpu_to_be16(skb_vlan_tag_get(skb));\n\tunsafe_memcpy(&vhdr->h_vlan_encapsulated_proto,\n\t\t      skb->data + cpy1_sz,\n\t\t      cpy2_sz,\n\t\t      MLX5_UNSAFE_MEMCPY_DISCLAIMER);\n}\n\nstatic inline void\nmlx5e_txwqe_build_eseg_csum(struct mlx5e_txqsq *sq, struct sk_buff *skb,\n\t\t\t    struct mlx5e_accel_tx_state *accel,\n\t\t\t    struct mlx5_wqe_eth_seg *eseg)\n{\n\tif (unlikely(mlx5e_ipsec_txwqe_build_eseg_csum(sq, skb, eseg)))\n\t\treturn;\n\n\tif (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {\n\t\teseg->cs_flags = MLX5_ETH_WQE_L3_CSUM;\n\t\tif (skb->encapsulation) {\n\t\t\teseg->cs_flags |= MLX5_ETH_WQE_L3_INNER_CSUM |\n\t\t\t\t\t  MLX5_ETH_WQE_L4_INNER_CSUM;\n\t\t\tsq->stats->csum_partial_inner++;\n\t\t} else {\n\t\t\teseg->cs_flags |= MLX5_ETH_WQE_L4_CSUM;\n\t\t\tsq->stats->csum_partial++;\n\t\t}\n#ifdef CONFIG_MLX5_EN_TLS\n\t} else if (unlikely(accel && accel->tls.tls_tisn)) {\n\t\teseg->cs_flags = MLX5_ETH_WQE_L3_CSUM | MLX5_ETH_WQE_L4_CSUM;\n\t\tsq->stats->csum_partial++;\n#endif\n\t} else\n\t\tsq->stats->csum_none++;\n}\n\n \nstatic inline u16\nmlx5e_tx_get_gso_ihs(struct mlx5e_txqsq *sq, struct sk_buff *skb, int *hopbyhop)\n{\n\tstruct mlx5e_sq_stats *stats = sq->stats;\n\tu16 ihs;\n\n\t*hopbyhop = 0;\n\tif (skb->encapsulation) {\n\t\tihs = skb_inner_tcp_all_headers(skb);\n\t\tstats->tso_inner_packets++;\n\t\tstats->tso_inner_bytes += skb->len - ihs;\n\t} else {\n\t\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {\n\t\t\tihs = skb_transport_offset(skb) + sizeof(struct udphdr);\n\t\t} else {\n\t\t\tihs = skb_tcp_all_headers(skb);\n\t\t\tif (ipv6_has_hopopt_jumbo(skb)) {\n\t\t\t\t*hopbyhop = sizeof(struct hop_jumbo_hdr);\n\t\t\t\tihs -= sizeof(struct hop_jumbo_hdr);\n\t\t\t}\n\t\t}\n\t\tstats->tso_packets++;\n\t\tstats->tso_bytes += skb->len - ihs - *hopbyhop;\n\t}\n\n\treturn ihs;\n}\n\nstatic inline int\nmlx5e_txwqe_build_dsegs(struct mlx5e_txqsq *sq, struct sk_buff *skb,\n\t\t\tunsigned char *skb_data, u16 headlen,\n\t\t\tstruct mlx5_wqe_data_seg *dseg)\n{\n\tdma_addr_t dma_addr = 0;\n\tu8 num_dma          = 0;\n\tint i;\n\n\tif (headlen) {\n\t\tdma_addr = dma_map_single(sq->pdev, skb_data, headlen,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(sq->pdev, dma_addr)))\n\t\t\tgoto dma_unmap_wqe_err;\n\n\t\tdseg->addr       = cpu_to_be64(dma_addr);\n\t\tdseg->lkey       = sq->mkey_be;\n\t\tdseg->byte_count = cpu_to_be32(headlen);\n\n\t\tmlx5e_dma_push(sq, dma_addr, headlen, MLX5E_DMA_MAP_SINGLE);\n\t\tnum_dma++;\n\t\tdseg++;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tint fsz = skb_frag_size(frag);\n\n\t\tdma_addr = skb_frag_dma_map(sq->pdev, frag, 0, fsz,\n\t\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(sq->pdev, dma_addr)))\n\t\t\tgoto dma_unmap_wqe_err;\n\n\t\tdseg->addr       = cpu_to_be64(dma_addr);\n\t\tdseg->lkey       = sq->mkey_be;\n\t\tdseg->byte_count = cpu_to_be32(fsz);\n\n\t\tmlx5e_dma_push(sq, dma_addr, fsz, MLX5E_DMA_MAP_PAGE);\n\t\tnum_dma++;\n\t\tdseg++;\n\t}\n\n\treturn num_dma;\n\ndma_unmap_wqe_err:\n\tmlx5e_dma_unmap_wqe_err(sq, num_dma);\n\treturn -ENOMEM;\n}\n\nstruct mlx5e_tx_attr {\n\tu32 num_bytes;\n\tu16 headlen;\n\tu16 ihs;\n\t__be16 mss;\n\tu16 insz;\n\tu8 opcode;\n\tu8 hopbyhop;\n};\n\nstruct mlx5e_tx_wqe_attr {\n\tu16 ds_cnt;\n\tu16 ds_cnt_inl;\n\tu16 ds_cnt_ids;\n\tu8 num_wqebbs;\n};\n\nstatic u8\nmlx5e_tx_wqe_inline_mode(struct mlx5e_txqsq *sq, struct sk_buff *skb,\n\t\t\t struct mlx5e_accel_tx_state *accel)\n{\n\tu8 mode;\n\n#ifdef CONFIG_MLX5_EN_TLS\n\tif (accel && accel->tls.tls_tisn)\n\t\treturn MLX5_INLINE_MODE_TCP_UDP;\n#endif\n\n\tmode = sq->min_inline_mode;\n\n\tif (skb_vlan_tag_present(skb) &&\n\t    test_bit(MLX5E_SQ_STATE_VLAN_NEED_L2_INLINE, &sq->state))\n\t\tmode = max_t(u8, MLX5_INLINE_MODE_L2, mode);\n\n\treturn mode;\n}\n\nstatic void mlx5e_sq_xmit_prepare(struct mlx5e_txqsq *sq, struct sk_buff *skb,\n\t\t\t\t  struct mlx5e_accel_tx_state *accel,\n\t\t\t\t  struct mlx5e_tx_attr *attr)\n{\n\tstruct mlx5e_sq_stats *stats = sq->stats;\n\n\tif (skb_is_gso(skb)) {\n\t\tint hopbyhop;\n\t\tu16 ihs = mlx5e_tx_get_gso_ihs(sq, skb, &hopbyhop);\n\n\t\t*attr = (struct mlx5e_tx_attr) {\n\t\t\t.opcode    = MLX5_OPCODE_LSO,\n\t\t\t.mss       = cpu_to_be16(skb_shinfo(skb)->gso_size),\n\t\t\t.ihs       = ihs,\n\t\t\t.num_bytes = skb->len + (skb_shinfo(skb)->gso_segs - 1) * ihs,\n\t\t\t.headlen   = skb_headlen(skb) - ihs - hopbyhop,\n\t\t\t.hopbyhop  = hopbyhop,\n\t\t};\n\n\t\tstats->packets += skb_shinfo(skb)->gso_segs;\n\t} else {\n\t\tu8 mode = mlx5e_tx_wqe_inline_mode(sq, skb, accel);\n\t\tu16 ihs = mlx5e_calc_min_inline(mode, skb);\n\n\t\t*attr = (struct mlx5e_tx_attr) {\n\t\t\t.opcode    = MLX5_OPCODE_SEND,\n\t\t\t.mss       = cpu_to_be16(0),\n\t\t\t.ihs       = ihs,\n\t\t\t.num_bytes = max_t(unsigned int, skb->len, ETH_ZLEN),\n\t\t\t.headlen   = skb_headlen(skb) - ihs,\n\t\t};\n\n\t\tstats->packets++;\n\t}\n\n\tattr->insz = mlx5e_accel_tx_ids_len(sq, accel);\n\tstats->bytes += attr->num_bytes;\n}\n\nstatic void mlx5e_sq_calc_wqe_attr(struct sk_buff *skb, const struct mlx5e_tx_attr *attr,\n\t\t\t\t   struct mlx5e_tx_wqe_attr *wqe_attr)\n{\n\tu16 ds_cnt = MLX5E_TX_WQE_EMPTY_DS_COUNT;\n\tu16 ds_cnt_inl = 0;\n\tu16 ds_cnt_ids = 0;\n\n\t \n\n\tif (attr->insz)\n\t\tds_cnt_ids = DIV_ROUND_UP(sizeof(struct mlx5_wqe_inline_seg) + attr->insz,\n\t\t\t\t\t  MLX5_SEND_WQE_DS);\n\n\tds_cnt += !!attr->headlen + skb_shinfo(skb)->nr_frags + ds_cnt_ids;\n\tif (attr->ihs) {\n\t\tu16 inl = attr->ihs - INL_HDR_START_SZ;\n\n\t\tif (skb_vlan_tag_present(skb))\n\t\t\tinl += VLAN_HLEN;\n\n\t\tds_cnt_inl = DIV_ROUND_UP(inl, MLX5_SEND_WQE_DS);\n\t\tif (WARN_ON_ONCE(ds_cnt_inl > MLX5E_MAX_TX_INLINE_DS))\n\t\t\tnetdev_warn(skb->dev, \"ds_cnt_inl = %u > max %u\\n\", ds_cnt_inl,\n\t\t\t\t    (u16)MLX5E_MAX_TX_INLINE_DS);\n\t\tds_cnt += ds_cnt_inl;\n\t}\n\n\t*wqe_attr = (struct mlx5e_tx_wqe_attr) {\n\t\t.ds_cnt     = ds_cnt,\n\t\t.ds_cnt_inl = ds_cnt_inl,\n\t\t.ds_cnt_ids = ds_cnt_ids,\n\t\t.num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS),\n\t};\n}\n\nstatic void mlx5e_tx_skb_update_hwts_flags(struct sk_buff *skb)\n{\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n}\n\nstatic void mlx5e_tx_check_stop(struct mlx5e_txqsq *sq)\n{\n\tif (unlikely(!mlx5e_wqc_has_room_for(&sq->wq, sq->cc, sq->pc, sq->stop_room))) {\n\t\tnetif_tx_stop_queue(sq->txq);\n\t\tsq->stats->stopped++;\n\t}\n}\n\nstatic void mlx5e_tx_flush(struct mlx5e_txqsq *sq)\n{\n\tstruct mlx5e_tx_wqe_info *wi;\n\tstruct mlx5e_tx_wqe *wqe;\n\tu16 pi;\n\n\t \n\tmlx5e_tx_mpwqe_ensure_complete(sq);\n\n\tpi = mlx5_wq_cyc_ctr2ix(&sq->wq, sq->pc);\n\twi = &sq->db.wqe_info[pi];\n\n\t*wi = (struct mlx5e_tx_wqe_info) {\n\t\t.num_wqebbs = 1,\n\t};\n\n\twqe = mlx5e_post_nop(&sq->wq, sq->sqn, &sq->pc);\n\tmlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &wqe->ctrl);\n}\n\nstatic inline void\nmlx5e_txwqe_complete(struct mlx5e_txqsq *sq, struct sk_buff *skb,\n\t\t     const struct mlx5e_tx_attr *attr,\n\t\t     const struct mlx5e_tx_wqe_attr *wqe_attr, u8 num_dma,\n\t\t     struct mlx5e_tx_wqe_info *wi, struct mlx5_wqe_ctrl_seg *cseg,\n\t\t     struct mlx5_wqe_eth_seg *eseg, bool xmit_more)\n{\n\tstruct mlx5_wq_cyc *wq = &sq->wq;\n\tbool send_doorbell;\n\n\t*wi = (struct mlx5e_tx_wqe_info) {\n\t\t.skb = skb,\n\t\t.num_bytes = attr->num_bytes,\n\t\t.num_dma = num_dma,\n\t\t.num_wqebbs = wqe_attr->num_wqebbs,\n\t\t.num_fifo_pkts = 0,\n\t};\n\n\tcseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | attr->opcode);\n\tcseg->qpn_ds           = cpu_to_be32((sq->sqn << 8) | wqe_attr->ds_cnt);\n\n\tmlx5e_tx_skb_update_hwts_flags(skb);\n\n\tsq->pc += wi->num_wqebbs;\n\n\tmlx5e_tx_check_stop(sq);\n\n\tif (unlikely(sq->ptpsq &&\n\t\t     (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))) {\n\t\tu8 metadata_index = be32_to_cpu(eseg->flow_table_metadata);\n\n\t\tmlx5e_skb_cb_hwtstamp_init(skb);\n\t\tmlx5e_ptp_metadata_map_put(&sq->ptpsq->metadata_map, skb,\n\t\t\t\t\t   metadata_index);\n\t\tmlx5e_ptpsq_track_metadata(sq->ptpsq, metadata_index);\n\t\tif (!netif_tx_queue_stopped(sq->txq) &&\n\t\t    mlx5e_ptpsq_metadata_freelist_empty(sq->ptpsq)) {\n\t\t\tnetif_tx_stop_queue(sq->txq);\n\t\t\tsq->stats->stopped++;\n\t\t}\n\t\tskb_get(skb);\n\t}\n\n\tsend_doorbell = __netdev_tx_sent_queue(sq->txq, attr->num_bytes, xmit_more);\n\tif (send_doorbell)\n\t\tmlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);\n}\n\nstatic void\nmlx5e_sq_xmit_wqe(struct mlx5e_txqsq *sq, struct sk_buff *skb,\n\t\t  const struct mlx5e_tx_attr *attr, const struct mlx5e_tx_wqe_attr *wqe_attr,\n\t\t  struct mlx5e_tx_wqe *wqe, u16 pi, bool xmit_more)\n{\n\tstruct mlx5_wqe_ctrl_seg *cseg;\n\tstruct mlx5_wqe_eth_seg  *eseg;\n\tstruct mlx5_wqe_data_seg *dseg;\n\tstruct mlx5e_tx_wqe_info *wi;\n\tu16 ihs = attr->ihs;\n\tstruct ipv6hdr *h6;\n\tstruct mlx5e_sq_stats *stats = sq->stats;\n\tint num_dma;\n\n\tstats->xmit_more += xmit_more;\n\n\t \n\twi   = &sq->db.wqe_info[pi];\n\tcseg = &wqe->ctrl;\n\teseg = &wqe->eth;\n\tdseg =  wqe->data;\n\n\teseg->mss = attr->mss;\n\n\tif (ihs) {\n\t\tu8 *start = eseg->inline_hdr.start;\n\n\t\tif (unlikely(attr->hopbyhop)) {\n\t\t\t \n\t\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\t\tmlx5e_insert_vlan(start, skb, ETH_HLEN + sizeof(*h6));\n\t\t\t\tihs += VLAN_HLEN;\n\t\t\t\th6 = (struct ipv6hdr *)(start + sizeof(struct vlan_ethhdr));\n\t\t\t} else {\n\t\t\t\tunsafe_memcpy(start, skb->data,\n\t\t\t\t\t      ETH_HLEN + sizeof(*h6),\n\t\t\t\t\t      MLX5_UNSAFE_MEMCPY_DISCLAIMER);\n\t\t\t\th6 = (struct ipv6hdr *)(start + ETH_HLEN);\n\t\t\t}\n\t\t\th6->nexthdr = IPPROTO_TCP;\n\t\t\t \n\t\t\tmemcpy(h6 + 1,\n\t\t\t       skb->data + ETH_HLEN + sizeof(*h6) +\n\t\t\t\t\tsizeof(struct hop_jumbo_hdr),\n\t\t\t       tcp_hdrlen(skb));\n\t\t\t \n\t\t} else if (skb_vlan_tag_present(skb)) {\n\t\t\tmlx5e_insert_vlan(start, skb, ihs);\n\t\t\tihs += VLAN_HLEN;\n\t\t\tstats->added_vlan_packets++;\n\t\t} else {\n\t\t\tunsafe_memcpy(eseg->inline_hdr.start, skb->data,\n\t\t\t\t      attr->ihs,\n\t\t\t\t      MLX5_UNSAFE_MEMCPY_DISCLAIMER);\n\t\t}\n\t\teseg->inline_hdr.sz |= cpu_to_be16(ihs);\n\t\tdseg += wqe_attr->ds_cnt_inl;\n\t} else if (skb_vlan_tag_present(skb)) {\n\t\teseg->insert.type = cpu_to_be16(MLX5_ETH_WQE_INSERT_VLAN);\n\t\tif (skb->vlan_proto == cpu_to_be16(ETH_P_8021AD))\n\t\t\teseg->insert.type |= cpu_to_be16(MLX5_ETH_WQE_SVLAN);\n\t\teseg->insert.vlan_tci = cpu_to_be16(skb_vlan_tag_get(skb));\n\t\tstats->added_vlan_packets++;\n\t}\n\n\tdseg += wqe_attr->ds_cnt_ids;\n\tnum_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr->ihs + attr->hopbyhop,\n\t\t\t\t\t  attr->headlen, dseg);\n\tif (unlikely(num_dma < 0))\n\t\tgoto err_drop;\n\n\tmlx5e_txwqe_complete(sq, skb, attr, wqe_attr, num_dma, wi, cseg, eseg, xmit_more);\n\n\treturn;\n\nerr_drop:\n\tstats->dropped++;\n\tif (unlikely(sq->ptpsq && (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)))\n\t\tmlx5e_ptp_metadata_fifo_push(&sq->ptpsq->metadata_freelist,\n\t\t\t\t\t     be32_to_cpu(eseg->flow_table_metadata));\n\tdev_kfree_skb_any(skb);\n\tmlx5e_tx_flush(sq);\n}\n\nstatic bool mlx5e_tx_skb_supports_mpwqe(struct sk_buff *skb, struct mlx5e_tx_attr *attr)\n{\n\treturn !skb_is_nonlinear(skb) && !skb_vlan_tag_present(skb) && !attr->ihs &&\n\t       !attr->insz && !mlx5e_macsec_skb_is_offload(skb);\n}\n\nstatic bool mlx5e_tx_mpwqe_same_eseg(struct mlx5e_txqsq *sq, struct mlx5_wqe_eth_seg *eseg)\n{\n\tstruct mlx5e_tx_mpwqe *session = &sq->mpwqe;\n\n\t \n\treturn !memcmp(&session->wqe->eth, eseg, MLX5E_ACCEL_ESEG_LEN);\n}\n\nstatic void mlx5e_tx_mpwqe_session_start(struct mlx5e_txqsq *sq,\n\t\t\t\t\t struct mlx5_wqe_eth_seg *eseg)\n{\n\tstruct mlx5e_tx_mpwqe *session = &sq->mpwqe;\n\tstruct mlx5e_tx_wqe *wqe;\n\tu16 pi;\n\n\tpi = mlx5e_txqsq_get_next_pi(sq, sq->max_sq_mpw_wqebbs);\n\twqe = MLX5E_TX_FETCH_WQE(sq, pi);\n\tnet_prefetchw(wqe->data);\n\n\t*session = (struct mlx5e_tx_mpwqe) {\n\t\t.wqe = wqe,\n\t\t.bytes_count = 0,\n\t\t.ds_count = MLX5E_TX_WQE_EMPTY_DS_COUNT,\n\t\t.pkt_count = 0,\n\t\t.inline_on = 0,\n\t};\n\n\tmemcpy(&session->wqe->eth, eseg, MLX5E_ACCEL_ESEG_LEN);\n\n\tsq->stats->mpwqe_blks++;\n}\n\nstatic bool mlx5e_tx_mpwqe_session_is_active(struct mlx5e_txqsq *sq)\n{\n\treturn sq->mpwqe.wqe;\n}\n\nstatic void mlx5e_tx_mpwqe_add_dseg(struct mlx5e_txqsq *sq, struct mlx5e_xmit_data *txd)\n{\n\tstruct mlx5e_tx_mpwqe *session = &sq->mpwqe;\n\tstruct mlx5_wqe_data_seg *dseg;\n\n\tdseg = (struct mlx5_wqe_data_seg *)session->wqe + session->ds_count;\n\n\tsession->pkt_count++;\n\tsession->bytes_count += txd->len;\n\n\tdseg->addr = cpu_to_be64(txd->dma_addr);\n\tdseg->byte_count = cpu_to_be32(txd->len);\n\tdseg->lkey = sq->mkey_be;\n\tsession->ds_count++;\n\n\tsq->stats->mpwqe_pkts++;\n}\n\nstatic struct mlx5_wqe_ctrl_seg *mlx5e_tx_mpwqe_session_complete(struct mlx5e_txqsq *sq)\n{\n\tstruct mlx5e_tx_mpwqe *session = &sq->mpwqe;\n\tu8 ds_count = session->ds_count;\n\tstruct mlx5_wqe_ctrl_seg *cseg;\n\tstruct mlx5e_tx_wqe_info *wi;\n\tu16 pi;\n\n\tcseg = &session->wqe->ctrl;\n\tcseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_ENHANCED_MPSW);\n\tcseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_count);\n\n\tpi = mlx5_wq_cyc_ctr2ix(&sq->wq, sq->pc);\n\twi = &sq->db.wqe_info[pi];\n\t*wi = (struct mlx5e_tx_wqe_info) {\n\t\t.skb = NULL,\n\t\t.num_bytes = session->bytes_count,\n\t\t.num_wqebbs = DIV_ROUND_UP(ds_count, MLX5_SEND_WQEBB_NUM_DS),\n\t\t.num_dma = session->pkt_count,\n\t\t.num_fifo_pkts = session->pkt_count,\n\t};\n\n\tsq->pc += wi->num_wqebbs;\n\n\tsession->wqe = NULL;\n\n\tmlx5e_tx_check_stop(sq);\n\n\treturn cseg;\n}\n\nstatic void\nmlx5e_sq_xmit_mpwqe(struct mlx5e_txqsq *sq, struct sk_buff *skb,\n\t\t    struct mlx5_wqe_eth_seg *eseg, bool xmit_more)\n{\n\tstruct mlx5_wqe_ctrl_seg *cseg;\n\tstruct mlx5e_xmit_data txd;\n\n\ttxd.data = skb->data;\n\ttxd.len = skb->len;\n\n\ttxd.dma_addr = dma_map_single(sq->pdev, txd.data, txd.len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(sq->pdev, txd.dma_addr)))\n\t\tgoto err_unmap;\n\n\tif (!mlx5e_tx_mpwqe_session_is_active(sq)) {\n\t\tmlx5e_tx_mpwqe_session_start(sq, eseg);\n\t} else if (!mlx5e_tx_mpwqe_same_eseg(sq, eseg)) {\n\t\tmlx5e_tx_mpwqe_session_complete(sq);\n\t\tmlx5e_tx_mpwqe_session_start(sq, eseg);\n\t}\n\n\tsq->stats->xmit_more += xmit_more;\n\n\tmlx5e_dma_push(sq, txd.dma_addr, txd.len, MLX5E_DMA_MAP_SINGLE);\n\tmlx5e_skb_fifo_push(&sq->db.skb_fifo, skb);\n\tmlx5e_tx_mpwqe_add_dseg(sq, &txd);\n\tmlx5e_tx_skb_update_hwts_flags(skb);\n\n\tif (unlikely(mlx5e_tx_mpwqe_is_full(&sq->mpwqe, sq->max_sq_mpw_wqebbs))) {\n\t\t \n\t\tcseg = mlx5e_tx_mpwqe_session_complete(sq);\n\n\t\tif (__netdev_tx_sent_queue(sq->txq, txd.len, xmit_more))\n\t\t\tmlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);\n\t} else if (__netdev_tx_sent_queue(sq->txq, txd.len, xmit_more)) {\n\t\t \n\t\tcseg = mlx5e_tx_mpwqe_session_complete(sq);\n\n\t\tmlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);\n\t}\n\n\treturn;\n\nerr_unmap:\n\tmlx5e_dma_unmap_wqe_err(sq, 1);\n\tsq->stats->dropped++;\n\tdev_kfree_skb_any(skb);\n\tmlx5e_tx_flush(sq);\n}\n\nvoid mlx5e_tx_mpwqe_ensure_complete(struct mlx5e_txqsq *sq)\n{\n\t \n\tif (unlikely(mlx5e_tx_mpwqe_session_is_active(sq)))\n\t\tmlx5e_tx_mpwqe_session_complete(sq);\n}\n\nstatic void mlx5e_cqe_ts_id_eseg(struct mlx5e_ptpsq *ptpsq, struct sk_buff *skb,\n\t\t\t\t struct mlx5_wqe_eth_seg *eseg)\n{\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))\n\t\teseg->flow_table_metadata =\n\t\t\tcpu_to_be32(mlx5e_ptp_metadata_fifo_pop(&ptpsq->metadata_freelist));\n}\n\nstatic void mlx5e_txwqe_build_eseg(struct mlx5e_priv *priv, struct mlx5e_txqsq *sq,\n\t\t\t\t   struct sk_buff *skb, struct mlx5e_accel_tx_state *accel,\n\t\t\t\t   struct mlx5_wqe_eth_seg *eseg, u16 ihs)\n{\n\tmlx5e_accel_tx_eseg(priv, skb, eseg, ihs);\n\tmlx5e_txwqe_build_eseg_csum(sq, skb, accel, eseg);\n\tif (unlikely(sq->ptpsq))\n\t\tmlx5e_cqe_ts_id_eseg(sq->ptpsq, skb, eseg);\n}\n\nnetdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5e_accel_tx_state accel = {};\n\tstruct mlx5e_tx_wqe_attr wqe_attr;\n\tstruct mlx5e_tx_attr attr;\n\tstruct mlx5e_tx_wqe *wqe;\n\tstruct mlx5e_txqsq *sq;\n\tu16 pi;\n\n\t \n\tsq = priv->txq2sq[skb_get_queue_mapping(skb)];\n\tif (unlikely(!sq)) {\n\t\t \n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\t \n\tif (unlikely(!mlx5e_accel_tx_begin(dev, sq, skb, &accel)))\n\t\treturn NETDEV_TX_OK;\n\n\tmlx5e_sq_xmit_prepare(sq, skb, &accel, &attr);\n\n\tif (test_bit(MLX5E_SQ_STATE_MPWQE, &sq->state)) {\n\t\tif (mlx5e_tx_skb_supports_mpwqe(skb, &attr)) {\n\t\t\tstruct mlx5_wqe_eth_seg eseg = {};\n\n\t\t\tmlx5e_txwqe_build_eseg(priv, sq, skb, &accel, &eseg, attr.ihs);\n\t\t\tmlx5e_sq_xmit_mpwqe(sq, skb, &eseg, netdev_xmit_more());\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\tmlx5e_tx_mpwqe_ensure_complete(sq);\n\t}\n\n\tmlx5e_sq_calc_wqe_attr(skb, &attr, &wqe_attr);\n\tpi = mlx5e_txqsq_get_next_pi(sq, wqe_attr.num_wqebbs);\n\twqe = MLX5E_TX_FETCH_WQE(sq, pi);\n\n\t \n\tmlx5e_accel_tx_finish(sq, wqe, &accel,\n\t\t\t      (struct mlx5_wqe_inline_seg *)(wqe->data + wqe_attr.ds_cnt_inl));\n\tmlx5e_txwqe_build_eseg(priv, sq, skb, &accel, &wqe->eth, attr.ihs);\n\tmlx5e_sq_xmit_wqe(sq, skb, &attr, &wqe_attr, wqe, pi, netdev_xmit_more());\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic void mlx5e_tx_wi_dma_unmap(struct mlx5e_txqsq *sq, struct mlx5e_tx_wqe_info *wi,\n\t\t\t\t  u32 *dma_fifo_cc)\n{\n\tint i;\n\n\tfor (i = 0; i < wi->num_dma; i++) {\n\t\tstruct mlx5e_sq_dma *dma = mlx5e_dma_get(sq, (*dma_fifo_cc)++);\n\n\t\tmlx5e_tx_dma_unmap(sq->pdev, dma);\n\t}\n}\n\nstatic void mlx5e_consume_skb(struct mlx5e_txqsq *sq, struct sk_buff *skb,\n\t\t\t      struct mlx5_cqe64 *cqe, int napi_budget)\n{\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)) {\n\t\tstruct skb_shared_hwtstamps hwts = {};\n\t\tu64 ts = get_cqe_ts(cqe);\n\n\t\thwts.hwtstamp = mlx5e_cqe_ts_to_ns(sq->ptp_cyc2time, sq->clock, ts);\n\t\tif (sq->ptpsq)\n\t\t\tmlx5e_skb_cb_hwtstamp_handler(skb, MLX5E_SKB_CB_CQE_HWTSTAMP,\n\t\t\t\t\t\t      hwts.hwtstamp, sq->ptpsq->cq_stats);\n\t\telse\n\t\t\tskb_tstamp_tx(skb, &hwts);\n\t}\n\n\tnapi_consume_skb(skb, napi_budget);\n}\n\nstatic void mlx5e_tx_wi_consume_fifo_skbs(struct mlx5e_txqsq *sq, struct mlx5e_tx_wqe_info *wi,\n\t\t\t\t\t  struct mlx5_cqe64 *cqe, int napi_budget)\n{\n\tint i;\n\n\tfor (i = 0; i < wi->num_fifo_pkts; i++) {\n\t\tstruct sk_buff *skb = mlx5e_skb_fifo_pop(&sq->db.skb_fifo);\n\n\t\tmlx5e_consume_skb(sq, skb, cqe, napi_budget);\n\t}\n}\n\nvoid mlx5e_txqsq_wake(struct mlx5e_txqsq *sq)\n{\n\tif (netif_tx_queue_stopped(sq->txq) &&\n\t    mlx5e_wqc_has_room_for(&sq->wq, sq->cc, sq->pc, sq->stop_room) &&\n\t    !mlx5e_ptpsq_metadata_freelist_empty(sq->ptpsq) &&\n\t    !test_bit(MLX5E_SQ_STATE_RECOVERING, &sq->state)) {\n\t\tnetif_tx_wake_queue(sq->txq);\n\t\tsq->stats->wake++;\n\t}\n}\n\nbool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget)\n{\n\tstruct mlx5e_sq_stats *stats;\n\tstruct mlx5e_txqsq *sq;\n\tstruct mlx5_cqe64 *cqe;\n\tu32 dma_fifo_cc;\n\tu32 nbytes;\n\tu16 npkts;\n\tu16 sqcc;\n\tint i;\n\n\tsq = container_of(cq, struct mlx5e_txqsq, cq);\n\n\tif (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))\n\t\treturn false;\n\n\tcqe = mlx5_cqwq_get_cqe(&cq->wq);\n\tif (!cqe)\n\t\treturn false;\n\n\tstats = sq->stats;\n\n\tnpkts = 0;\n\tnbytes = 0;\n\n\t \n\tsqcc = sq->cc;\n\n\t \n\tdma_fifo_cc = sq->dma_fifo_cc;\n\n\ti = 0;\n\tdo {\n\t\tstruct mlx5e_tx_wqe_info *wi;\n\t\tu16 wqe_counter;\n\t\tbool last_wqe;\n\t\tu16 ci;\n\n\t\tmlx5_cqwq_pop(&cq->wq);\n\n\t\twqe_counter = be16_to_cpu(cqe->wqe_counter);\n\n\t\tdo {\n\t\t\tlast_wqe = (sqcc == wqe_counter);\n\n\t\t\tci = mlx5_wq_cyc_ctr2ix(&sq->wq, sqcc);\n\t\t\twi = &sq->db.wqe_info[ci];\n\n\t\t\tsqcc += wi->num_wqebbs;\n\n\t\t\tif (likely(wi->skb)) {\n\t\t\t\tmlx5e_tx_wi_dma_unmap(sq, wi, &dma_fifo_cc);\n\t\t\t\tmlx5e_consume_skb(sq, wi->skb, cqe, napi_budget);\n\n\t\t\t\tnpkts++;\n\t\t\t\tnbytes += wi->num_bytes;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (unlikely(mlx5e_ktls_tx_try_handle_resync_dump_comp(sq, wi,\n\t\t\t\t\t\t\t\t\t       &dma_fifo_cc)))\n\t\t\t\tcontinue;\n\n\t\t\tif (wi->num_fifo_pkts) {\n\t\t\t\tmlx5e_tx_wi_dma_unmap(sq, wi, &dma_fifo_cc);\n\t\t\t\tmlx5e_tx_wi_consume_fifo_skbs(sq, wi, cqe, napi_budget);\n\n\t\t\t\tnpkts += wi->num_fifo_pkts;\n\t\t\t\tnbytes += wi->num_bytes;\n\t\t\t}\n\t\t} while (!last_wqe);\n\n\t\tif (unlikely(get_cqe_opcode(cqe) == MLX5_CQE_REQ_ERR)) {\n\t\t\tif (!test_and_set_bit(MLX5E_SQ_STATE_RECOVERING,\n\t\t\t\t\t      &sq->state)) {\n\t\t\t\tmlx5e_dump_error_cqe(&sq->cq, sq->sqn,\n\t\t\t\t\t\t     (struct mlx5_err_cqe *)cqe);\n\t\t\t\tmlx5_wq_cyc_wqe_dump(&sq->wq, ci, wi->num_wqebbs);\n\t\t\t\tqueue_work(cq->priv->wq, &sq->recover_work);\n\t\t\t}\n\t\t\tstats->cqe_err++;\n\t\t}\n\n\t} while ((++i < MLX5E_TX_CQ_POLL_BUDGET) && (cqe = mlx5_cqwq_get_cqe(&cq->wq)));\n\n\tstats->cqes += i;\n\n\tmlx5_cqwq_update_db_record(&cq->wq);\n\n\t \n\twmb();\n\n\tsq->dma_fifo_cc = dma_fifo_cc;\n\tsq->cc = sqcc;\n\n\tnetdev_tx_completed_queue(sq->txq, npkts, nbytes);\n\n\tmlx5e_txqsq_wake(sq);\n\n\treturn (i == MLX5E_TX_CQ_POLL_BUDGET);\n}\n\nstatic void mlx5e_tx_wi_kfree_fifo_skbs(struct mlx5e_txqsq *sq, struct mlx5e_tx_wqe_info *wi)\n{\n\tint i;\n\n\tfor (i = 0; i < wi->num_fifo_pkts; i++)\n\t\tdev_kfree_skb_any(mlx5e_skb_fifo_pop(&sq->db.skb_fifo));\n}\n\nvoid mlx5e_free_txqsq_descs(struct mlx5e_txqsq *sq)\n{\n\tstruct mlx5e_tx_wqe_info *wi;\n\tu32 dma_fifo_cc, nbytes = 0;\n\tu16 ci, sqcc, npkts = 0;\n\n\tsqcc = sq->cc;\n\tdma_fifo_cc = sq->dma_fifo_cc;\n\n\twhile (sqcc != sq->pc) {\n\t\tci = mlx5_wq_cyc_ctr2ix(&sq->wq, sqcc);\n\t\twi = &sq->db.wqe_info[ci];\n\n\t\tsqcc += wi->num_wqebbs;\n\n\t\tif (likely(wi->skb)) {\n\t\t\tmlx5e_tx_wi_dma_unmap(sq, wi, &dma_fifo_cc);\n\t\t\tdev_kfree_skb_any(wi->skb);\n\n\t\t\tnpkts++;\n\t\t\tnbytes += wi->num_bytes;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (unlikely(mlx5e_ktls_tx_try_handle_resync_dump_comp(sq, wi, &dma_fifo_cc)))\n\t\t\tcontinue;\n\n\t\tif (wi->num_fifo_pkts) {\n\t\t\tmlx5e_tx_wi_dma_unmap(sq, wi, &dma_fifo_cc);\n\t\t\tmlx5e_tx_wi_kfree_fifo_skbs(sq, wi);\n\n\t\t\tnpkts += wi->num_fifo_pkts;\n\t\t\tnbytes += wi->num_bytes;\n\t\t}\n\t}\n\n\tsq->dma_fifo_cc = dma_fifo_cc;\n\tsq->cc = sqcc;\n\n\tnetdev_tx_completed_queue(sq->txq, npkts, nbytes);\n}\n\n#ifdef CONFIG_MLX5_CORE_IPOIB\nstatic inline void\nmlx5i_txwqe_build_datagram(struct mlx5_av *av, u32 dqpn, u32 dqkey,\n\t\t\t   struct mlx5_wqe_datagram_seg *dseg)\n{\n\tmemcpy(&dseg->av, av, sizeof(struct mlx5_av));\n\tdseg->av.dqp_dct = cpu_to_be32(dqpn | MLX5_EXTENDED_UD_AV);\n\tdseg->av.key.qkey.qkey = cpu_to_be32(dqkey);\n}\n\nstatic void mlx5i_sq_calc_wqe_attr(struct sk_buff *skb,\n\t\t\t\t   const struct mlx5e_tx_attr *attr,\n\t\t\t\t   struct mlx5e_tx_wqe_attr *wqe_attr)\n{\n\tu16 ds_cnt = sizeof(struct mlx5i_tx_wqe) / MLX5_SEND_WQE_DS;\n\tu16 ds_cnt_inl = 0;\n\n\tds_cnt += !!attr->headlen + skb_shinfo(skb)->nr_frags;\n\n\tif (attr->ihs) {\n\t\tu16 inl = attr->ihs - INL_HDR_START_SZ;\n\n\t\tds_cnt_inl = DIV_ROUND_UP(inl, MLX5_SEND_WQE_DS);\n\t\tds_cnt += ds_cnt_inl;\n\t}\n\n\t*wqe_attr = (struct mlx5e_tx_wqe_attr) {\n\t\t.ds_cnt     = ds_cnt,\n\t\t.ds_cnt_inl = ds_cnt_inl,\n\t\t.num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS),\n\t};\n}\n\nvoid mlx5i_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,\n\t\t   struct mlx5_av *av, u32 dqpn, u32 dqkey, bool xmit_more)\n{\n\tstruct mlx5e_tx_wqe_attr wqe_attr;\n\tstruct mlx5e_tx_attr attr;\n\tstruct mlx5i_tx_wqe *wqe;\n\n\tstruct mlx5_wqe_datagram_seg *datagram;\n\tstruct mlx5_wqe_ctrl_seg *cseg;\n\tstruct mlx5_wqe_eth_seg  *eseg;\n\tstruct mlx5_wqe_data_seg *dseg;\n\tstruct mlx5e_tx_wqe_info *wi;\n\n\tstruct mlx5e_sq_stats *stats = sq->stats;\n\tint num_dma;\n\tu16 pi;\n\n\tmlx5e_sq_xmit_prepare(sq, skb, NULL, &attr);\n\tmlx5i_sq_calc_wqe_attr(skb, &attr, &wqe_attr);\n\n\tpi = mlx5e_txqsq_get_next_pi(sq, wqe_attr.num_wqebbs);\n\twqe = MLX5I_SQ_FETCH_WQE(sq, pi);\n\n\tstats->xmit_more += xmit_more;\n\n\t \n\twi       = &sq->db.wqe_info[pi];\n\tcseg     = &wqe->ctrl;\n\tdatagram = &wqe->datagram;\n\teseg     = &wqe->eth;\n\tdseg     =  wqe->data;\n\n\tmlx5i_txwqe_build_datagram(av, dqpn, dqkey, datagram);\n\n\tmlx5e_txwqe_build_eseg_csum(sq, skb, NULL, eseg);\n\n\teseg->mss = attr.mss;\n\n\tif (attr.ihs) {\n\t\tif (unlikely(attr.hopbyhop)) {\n\t\t\tstruct ipv6hdr *h6;\n\n\t\t\t \n\t\t\tunsafe_memcpy(eseg->inline_hdr.start, skb->data,\n\t\t\t\t      ETH_HLEN + sizeof(*h6),\n\t\t\t\t      MLX5_UNSAFE_MEMCPY_DISCLAIMER);\n\t\t\th6 = (struct ipv6hdr *)((char *)eseg->inline_hdr.start + ETH_HLEN);\n\t\t\th6->nexthdr = IPPROTO_TCP;\n\t\t\t \n\t\t\tunsafe_memcpy(h6 + 1,\n\t\t\t\t      skb->data + ETH_HLEN + sizeof(*h6) +\n\t\t\t\t\t\t  sizeof(struct hop_jumbo_hdr),\n\t\t\t\t      tcp_hdrlen(skb),\n\t\t\t\t      MLX5_UNSAFE_MEMCPY_DISCLAIMER);\n\t\t\t \n\t\t} else {\n\t\t\tunsafe_memcpy(eseg->inline_hdr.start, skb->data,\n\t\t\t\t      attr.ihs,\n\t\t\t\t      MLX5_UNSAFE_MEMCPY_DISCLAIMER);\n\t\t}\n\t\teseg->inline_hdr.sz = cpu_to_be16(attr.ihs);\n\t\tdseg += wqe_attr.ds_cnt_inl;\n\t}\n\n\tnum_dma = mlx5e_txwqe_build_dsegs(sq, skb, skb->data + attr.ihs + attr.hopbyhop,\n\t\t\t\t\t  attr.headlen, dseg);\n\tif (unlikely(num_dma < 0))\n\t\tgoto err_drop;\n\n\tmlx5e_txwqe_complete(sq, skb, &attr, &wqe_attr, num_dma, wi, cseg, eseg, xmit_more);\n\n\treturn;\n\nerr_drop:\n\tstats->dropped++;\n\tdev_kfree_skb_any(skb);\n\tmlx5e_tx_flush(sq);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}