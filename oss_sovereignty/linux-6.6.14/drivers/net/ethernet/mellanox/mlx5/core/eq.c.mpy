{
  "module_name": "eq.c",
  "hash_id": "4a557f1b43fd0fafd5561f18077ca030ce0f0f58d306ea925386027e0c636414",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/eq.c",
  "human_readable_source": "\n \n\n#include <linux/interrupt.h>\n#include <linux/notifier.h>\n#include <linux/mlx5/driver.h>\n#include <linux/mlx5/vport.h>\n#include <linux/mlx5/eq.h>\n#ifdef CONFIG_RFS_ACCEL\n#include <linux/cpu_rmap.h>\n#endif\n#include \"mlx5_core.h\"\n#include \"lib/eq.h\"\n#include \"fpga/core.h\"\n#include \"eswitch.h\"\n#include \"lib/clock.h\"\n#include \"diag/fw_tracer.h\"\n#include \"mlx5_irq.h\"\n#include \"pci_irq.h\"\n#include \"devlink.h\"\n#include \"en_accel/ipsec.h\"\n\nenum {\n\tMLX5_EQE_OWNER_INIT_VAL\t= 0x1,\n};\n\nenum {\n\tMLX5_EQ_STATE_ARMED\t\t= 0x9,\n\tMLX5_EQ_STATE_FIRED\t\t= 0xa,\n\tMLX5_EQ_STATE_ALWAYS_ARMED\t= 0xb,\n};\n\nenum {\n\tMLX5_EQ_DOORBEL_OFFSET\t= 0x40,\n};\n\n \nenum {\n\tMLX5_EQ_POLLING_BUDGET\t= 128,\n};\n\nstatic_assert(MLX5_EQ_POLLING_BUDGET <= MLX5_NUM_SPARE_EQE);\n\nstruct mlx5_eq_table {\n\tstruct xarray           comp_eqs;\n\tstruct mlx5_eq_async    pages_eq;\n\tstruct mlx5_eq_async    cmd_eq;\n\tstruct mlx5_eq_async    async_eq;\n\n\tstruct atomic_notifier_head nh[MLX5_EVENT_TYPE_MAX];\n\n\t \n\tstruct mlx5_nb          cq_err_nb;\n\n\tstruct mutex            lock;  \n\tstruct mutex            comp_lock;  \n\tint\t\t\tcurr_comp_eqs;\n\tint\t\t\tmax_comp_eqs;\n\tstruct mlx5_irq_table\t*irq_table;\n\tstruct xarray           comp_irqs;\n\tstruct mlx5_irq         *ctrl_irq;\n\tstruct cpu_rmap\t\t*rmap;\n\tstruct cpumask          used_cpus;\n};\n\n#define MLX5_ASYNC_EVENT_MASK ((1ull << MLX5_EVENT_TYPE_PATH_MIG)\t    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_COMM_EST)\t    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_SQ_DRAINED)\t    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_CQ_ERROR)\t    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_WQ_CATAS_ERROR)\t    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_PATH_MIG_FAILED)    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_WQ_INVAL_REQ_ERROR) | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_WQ_ACCESS_ERROR)    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_PORT_CHANGE)\t    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_SRQ_CATAS_ERROR)    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_SRQ_LAST_WQE)\t    | \\\n\t\t\t       (1ull << MLX5_EVENT_TYPE_SRQ_RQ_LIMIT))\n\nstatic int mlx5_cmd_destroy_eq(struct mlx5_core_dev *dev, u8 eqn)\n{\n\tu32 in[MLX5_ST_SZ_DW(destroy_eq_in)] = {};\n\n\tMLX5_SET(destroy_eq_in, in, opcode, MLX5_CMD_OP_DESTROY_EQ);\n\tMLX5_SET(destroy_eq_in, in, eq_number, eqn);\n\treturn mlx5_cmd_exec_in(dev, destroy_eq, in);\n}\n\n \nstatic struct mlx5_core_cq *mlx5_eq_cq_get(struct mlx5_eq *eq, u32 cqn)\n{\n\tstruct mlx5_cq_table *table = &eq->cq_table;\n\tstruct mlx5_core_cq *cq = NULL;\n\n\trcu_read_lock();\n\tcq = radix_tree_lookup(&table->tree, cqn);\n\tif (likely(cq))\n\t\tmlx5_cq_hold(cq);\n\trcu_read_unlock();\n\n\treturn cq;\n}\n\nstatic int mlx5_eq_comp_int(struct notifier_block *nb,\n\t\t\t    __always_unused unsigned long action,\n\t\t\t    __always_unused void *data)\n{\n\tstruct mlx5_eq_comp *eq_comp =\n\t\tcontainer_of(nb, struct mlx5_eq_comp, irq_nb);\n\tstruct mlx5_eq *eq = &eq_comp->core;\n\tstruct mlx5_eqe *eqe;\n\tint num_eqes = 0;\n\tu32 cqn = -1;\n\n\teqe = next_eqe_sw(eq);\n\tif (!eqe)\n\t\tgoto out;\n\n\tdo {\n\t\tstruct mlx5_core_cq *cq;\n\n\t\t \n\t\tdma_rmb();\n\t\t \n\t\tcqn = be32_to_cpu(eqe->data.comp.cqn) & 0xffffff;\n\n\t\tcq = mlx5_eq_cq_get(eq, cqn);\n\t\tif (likely(cq)) {\n\t\t\t++cq->arm_sn;\n\t\t\tcq->comp(cq, eqe);\n\t\t\tmlx5_cq_put(cq);\n\t\t} else {\n\t\t\tdev_dbg_ratelimited(eq->dev->device,\n\t\t\t\t\t    \"Completion event for bogus CQ 0x%x\\n\", cqn);\n\t\t}\n\n\t\t++eq->cons_index;\n\n\t} while ((++num_eqes < MLX5_EQ_POLLING_BUDGET) && (eqe = next_eqe_sw(eq)));\n\nout:\n\teq_update_ci(eq, 1);\n\n\tif (cqn != -1)\n\t\ttasklet_schedule(&eq_comp->tasklet_ctx.task);\n\n\treturn 0;\n}\n\n \nu32 mlx5_eq_poll_irq_disabled(struct mlx5_eq_comp *eq)\n{\n\tu32 count_eqe;\n\n\tdisable_irq(eq->core.irqn);\n\tcount_eqe = eq->core.cons_index;\n\tmlx5_eq_comp_int(&eq->irq_nb, 0, NULL);\n\tcount_eqe = eq->core.cons_index - count_eqe;\n\tenable_irq(eq->core.irqn);\n\n\treturn count_eqe;\n}\n\nstatic void mlx5_eq_async_int_lock(struct mlx5_eq_async *eq, bool recovery,\n\t\t\t\t   unsigned long *flags)\n\t__acquires(&eq->lock)\n{\n\tif (!recovery)\n\t\tspin_lock(&eq->lock);\n\telse\n\t\tspin_lock_irqsave(&eq->lock, *flags);\n}\n\nstatic void mlx5_eq_async_int_unlock(struct mlx5_eq_async *eq, bool recovery,\n\t\t\t\t     unsigned long *flags)\n\t__releases(&eq->lock)\n{\n\tif (!recovery)\n\t\tspin_unlock(&eq->lock);\n\telse\n\t\tspin_unlock_irqrestore(&eq->lock, *flags);\n}\n\nenum async_eq_nb_action {\n\tASYNC_EQ_IRQ_HANDLER = 0,\n\tASYNC_EQ_RECOVER = 1,\n};\n\nstatic int mlx5_eq_async_int(struct notifier_block *nb,\n\t\t\t     unsigned long action, void *data)\n{\n\tstruct mlx5_eq_async *eq_async =\n\t\tcontainer_of(nb, struct mlx5_eq_async, irq_nb);\n\tstruct mlx5_eq *eq = &eq_async->core;\n\tstruct mlx5_eq_table *eqt;\n\tstruct mlx5_core_dev *dev;\n\tstruct mlx5_eqe *eqe;\n\tunsigned long flags;\n\tint num_eqes = 0;\n\tbool recovery;\n\n\tdev = eq->dev;\n\teqt = dev->priv.eq_table;\n\n\trecovery = action == ASYNC_EQ_RECOVER;\n\tmlx5_eq_async_int_lock(eq_async, recovery, &flags);\n\n\teqe = next_eqe_sw(eq);\n\tif (!eqe)\n\t\tgoto out;\n\n\tdo {\n\t\t \n\t\tdma_rmb();\n\n\t\tatomic_notifier_call_chain(&eqt->nh[eqe->type], eqe->type, eqe);\n\t\tatomic_notifier_call_chain(&eqt->nh[MLX5_EVENT_TYPE_NOTIFY_ANY], eqe->type, eqe);\n\n\t\t++eq->cons_index;\n\n\t} while ((++num_eqes < MLX5_EQ_POLLING_BUDGET) && (eqe = next_eqe_sw(eq)));\n\nout:\n\teq_update_ci(eq, 1);\n\tmlx5_eq_async_int_unlock(eq_async, recovery, &flags);\n\n\treturn unlikely(recovery) ? num_eqes : 0;\n}\n\nvoid mlx5_cmd_eq_recover(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eq_async *eq = &dev->priv.eq_table->cmd_eq;\n\tint eqes;\n\n\teqes = mlx5_eq_async_int(&eq->irq_nb, ASYNC_EQ_RECOVER, NULL);\n\tif (eqes)\n\t\tmlx5_core_warn(dev, \"Recovered %d EQEs on cmd_eq\\n\", eqes);\n}\n\nstatic void init_eq_buf(struct mlx5_eq *eq)\n{\n\tstruct mlx5_eqe *eqe;\n\tint i;\n\n\tfor (i = 0; i < eq_get_size(eq); i++) {\n\t\teqe = get_eqe(eq, i);\n\t\teqe->owner = MLX5_EQE_OWNER_INIT_VAL;\n\t}\n}\n\nstatic int\ncreate_map_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq,\n\t      struct mlx5_eq_param *param)\n{\n\tu8 log_eq_size = order_base_2(param->nent + MLX5_NUM_SPARE_EQE);\n\tstruct mlx5_cq_table *cq_table = &eq->cq_table;\n\tu32 out[MLX5_ST_SZ_DW(create_eq_out)] = {0};\n\tu8 log_eq_stride = ilog2(MLX5_EQE_SIZE);\n\tstruct mlx5_priv *priv = &dev->priv;\n\t__be64 *pas;\n\tu16 vecidx;\n\tvoid *eqc;\n\tint inlen;\n\tu32 *in;\n\tint err;\n\tint i;\n\n\t \n\tmemset(cq_table, 0, sizeof(*cq_table));\n\tspin_lock_init(&cq_table->lock);\n\tINIT_RADIX_TREE(&cq_table->tree, GFP_ATOMIC);\n\n\teq->cons_index = 0;\n\n\terr = mlx5_frag_buf_alloc_node(dev, wq_get_byte_sz(log_eq_size, log_eq_stride),\n\t\t\t\t       &eq->frag_buf, dev->priv.numa_node);\n\tif (err)\n\t\treturn err;\n\n\tmlx5_init_fbc(eq->frag_buf.frags, log_eq_stride, log_eq_size, &eq->fbc);\n\tinit_eq_buf(eq);\n\n\teq->irq = param->irq;\n\tvecidx = mlx5_irq_get_index(eq->irq);\n\n\tinlen = MLX5_ST_SZ_BYTES(create_eq_in) +\n\t\tMLX5_FLD_SZ_BYTES(create_eq_in, pas[0]) * eq->frag_buf.npages;\n\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_buf;\n\t}\n\n\tpas = (__be64 *)MLX5_ADDR_OF(create_eq_in, in, pas);\n\tmlx5_fill_page_frag_array(&eq->frag_buf, pas);\n\n\tMLX5_SET(create_eq_in, in, opcode, MLX5_CMD_OP_CREATE_EQ);\n\tif (!param->mask[0] && MLX5_CAP_GEN(dev, log_max_uctx))\n\t\tMLX5_SET(create_eq_in, in, uid, MLX5_SHARED_RESOURCE_UID);\n\n\tfor (i = 0; i < 4; i++)\n\t\tMLX5_ARRAY_SET64(create_eq_in, in, event_bitmask, i,\n\t\t\t\t param->mask[i]);\n\n\teqc = MLX5_ADDR_OF(create_eq_in, in, eq_context_entry);\n\tMLX5_SET(eqc, eqc, log_eq_size, eq->fbc.log_sz);\n\tMLX5_SET(eqc, eqc, uar_page, priv->uar->index);\n\tMLX5_SET(eqc, eqc, intr, vecidx);\n\tMLX5_SET(eqc, eqc, log_page_size,\n\t\t eq->frag_buf.page_shift - MLX5_ADAPTER_PAGE_SHIFT);\n\n\terr = mlx5_cmd_exec(dev, in, inlen, out, sizeof(out));\n\tif (err)\n\t\tgoto err_in;\n\n\teq->vecidx = vecidx;\n\teq->eqn = MLX5_GET(create_eq_out, out, eq_number);\n\teq->irqn = pci_irq_vector(dev->pdev, vecidx);\n\teq->dev = dev;\n\teq->doorbell = priv->uar->map + MLX5_EQ_DOORBEL_OFFSET;\n\n\terr = mlx5_debug_eq_add(dev, eq);\n\tif (err)\n\t\tgoto err_eq;\n\n\tkvfree(in);\n\treturn 0;\n\nerr_eq:\n\tmlx5_cmd_destroy_eq(dev, eq->eqn);\n\nerr_in:\n\tkvfree(in);\n\nerr_buf:\n\tmlx5_frag_buf_free(dev, &eq->frag_buf);\n\treturn err;\n}\n\n \nint mlx5_eq_enable(struct mlx5_core_dev *dev, struct mlx5_eq *eq,\n\t\t   struct notifier_block *nb)\n{\n\tint err;\n\n\terr = mlx5_irq_attach_nb(eq->irq, nb);\n\tif (!err)\n\t\teq_update_ci(eq, 1);\n\n\treturn err;\n}\nEXPORT_SYMBOL(mlx5_eq_enable);\n\n \nvoid mlx5_eq_disable(struct mlx5_core_dev *dev, struct mlx5_eq *eq,\n\t\t     struct notifier_block *nb)\n{\n\tmlx5_irq_detach_nb(eq->irq, nb);\n}\nEXPORT_SYMBOL(mlx5_eq_disable);\n\nstatic int destroy_unmap_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq)\n{\n\tint err;\n\n\tmlx5_debug_eq_remove(dev, eq);\n\n\terr = mlx5_cmd_destroy_eq(dev, eq->eqn);\n\tif (err)\n\t\tmlx5_core_warn(dev, \"failed to destroy a previously created eq: eqn %d\\n\",\n\t\t\t       eq->eqn);\n\n\tmlx5_frag_buf_free(dev, &eq->frag_buf);\n\treturn err;\n}\n\nint mlx5_eq_add_cq(struct mlx5_eq *eq, struct mlx5_core_cq *cq)\n{\n\tstruct mlx5_cq_table *table = &eq->cq_table;\n\tint err;\n\n\tspin_lock(&table->lock);\n\terr = radix_tree_insert(&table->tree, cq->cqn, cq);\n\tspin_unlock(&table->lock);\n\n\treturn err;\n}\n\nvoid mlx5_eq_del_cq(struct mlx5_eq *eq, struct mlx5_core_cq *cq)\n{\n\tstruct mlx5_cq_table *table = &eq->cq_table;\n\tstruct mlx5_core_cq *tmp;\n\n\tspin_lock(&table->lock);\n\ttmp = radix_tree_delete(&table->tree, cq->cqn);\n\tspin_unlock(&table->lock);\n\n\tif (!tmp) {\n\t\tmlx5_core_dbg(eq->dev, \"cq 0x%x not found in eq 0x%x tree\\n\",\n\t\t\t      eq->eqn, cq->cqn);\n\t\treturn;\n\t}\n\n\tif (tmp != cq)\n\t\tmlx5_core_dbg(eq->dev, \"corruption on cqn 0x%x in eq 0x%x\\n\",\n\t\t\t      eq->eqn, cq->cqn);\n}\n\nint mlx5_eq_table_init(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eq_table *eq_table;\n\tint i;\n\n\teq_table = kvzalloc_node(sizeof(*eq_table), GFP_KERNEL,\n\t\t\t\t dev->priv.numa_node);\n\tif (!eq_table)\n\t\treturn -ENOMEM;\n\n\tdev->priv.eq_table = eq_table;\n\n\tmlx5_eq_debugfs_init(dev);\n\n\tmutex_init(&eq_table->lock);\n\tfor (i = 0; i < MLX5_EVENT_TYPE_MAX; i++)\n\t\tATOMIC_INIT_NOTIFIER_HEAD(&eq_table->nh[i]);\n\n\teq_table->irq_table = mlx5_irq_table_get(dev);\n\tcpumask_clear(&eq_table->used_cpus);\n\txa_init(&eq_table->comp_eqs);\n\txa_init(&eq_table->comp_irqs);\n\tmutex_init(&eq_table->comp_lock);\n\teq_table->curr_comp_eqs = 0;\n\treturn 0;\n}\n\nvoid mlx5_eq_table_cleanup(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\n\tmlx5_eq_debugfs_cleanup(dev);\n\txa_destroy(&table->comp_irqs);\n\txa_destroy(&table->comp_eqs);\n\tkvfree(table);\n}\n\n \n\nstatic int create_async_eq(struct mlx5_core_dev *dev,\n\t\t\t   struct mlx5_eq *eq, struct mlx5_eq_param *param)\n{\n\tstruct mlx5_eq_table *eq_table = dev->priv.eq_table;\n\tint err;\n\n\tmutex_lock(&eq_table->lock);\n\terr = create_map_eq(dev, eq, param);\n\tmutex_unlock(&eq_table->lock);\n\treturn err;\n}\n\nstatic int destroy_async_eq(struct mlx5_core_dev *dev, struct mlx5_eq *eq)\n{\n\tstruct mlx5_eq_table *eq_table = dev->priv.eq_table;\n\tint err;\n\n\tmutex_lock(&eq_table->lock);\n\terr = destroy_unmap_eq(dev, eq);\n\tmutex_unlock(&eq_table->lock);\n\treturn err;\n}\n\nstatic int cq_err_event_notifier(struct notifier_block *nb,\n\t\t\t\t unsigned long type, void *data)\n{\n\tstruct mlx5_eq_table *eqt;\n\tstruct mlx5_core_cq *cq;\n\tstruct mlx5_eqe *eqe;\n\tstruct mlx5_eq *eq;\n\tu32 cqn;\n\n\t \n\n\teqt = mlx5_nb_cof(nb, struct mlx5_eq_table, cq_err_nb);\n\teq  = &eqt->async_eq.core;\n\teqe = data;\n\n\tcqn = be32_to_cpu(eqe->data.cq_err.cqn) & 0xffffff;\n\tmlx5_core_warn(eq->dev, \"CQ error on CQN 0x%x, syndrome 0x%x\\n\",\n\t\t       cqn, eqe->data.cq_err.syndrome);\n\n\tcq = mlx5_eq_cq_get(eq, cqn);\n\tif (unlikely(!cq)) {\n\t\tmlx5_core_warn(eq->dev, \"Async event for bogus CQ 0x%x\\n\", cqn);\n\t\treturn NOTIFY_OK;\n\t}\n\n\tif (cq->event)\n\t\tcq->event(cq, type);\n\n\tmlx5_cq_put(cq);\n\n\treturn NOTIFY_OK;\n}\n\nstatic void gather_user_async_events(struct mlx5_core_dev *dev, u64 mask[4])\n{\n\t__be64 *user_unaffiliated_events;\n\t__be64 *user_affiliated_events;\n\tint i;\n\n\tuser_affiliated_events =\n\t\tMLX5_CAP_DEV_EVENT(dev, user_affiliated_events);\n\tuser_unaffiliated_events =\n\t\tMLX5_CAP_DEV_EVENT(dev, user_unaffiliated_events);\n\n\tfor (i = 0; i < 4; i++)\n\t\tmask[i] |= be64_to_cpu(user_affiliated_events[i] |\n\t\t\t\t       user_unaffiliated_events[i]);\n}\n\nstatic void gather_async_events_mask(struct mlx5_core_dev *dev, u64 mask[4])\n{\n\tu64 async_event_mask = MLX5_ASYNC_EVENT_MASK;\n\n\tif (MLX5_VPORT_MANAGER(dev))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_NIC_VPORT_CHANGE);\n\n\tif (MLX5_CAP_GEN(dev, general_notification_event))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_GENERAL_EVENT);\n\n\tif (MLX5_CAP_GEN(dev, port_module_event))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_PORT_MODULE_EVENT);\n\telse\n\t\tmlx5_core_dbg(dev, \"port_module_event is not set\\n\");\n\n\tif (MLX5_PPS_CAP(dev))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_PPS_EVENT);\n\n\tif (MLX5_CAP_GEN(dev, fpga))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_FPGA_ERROR) |\n\t\t\t\t    (1ull << MLX5_EVENT_TYPE_FPGA_QP_ERROR);\n\tif (MLX5_CAP_GEN_MAX(dev, dct))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_DCT_DRAINED);\n\n\tif (MLX5_CAP_GEN(dev, temp_warn_event))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_TEMP_WARN_EVENT);\n\n\tif (MLX5_CAP_MCAM_REG(dev, tracer_registers))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_DEVICE_TRACER);\n\n\tif (MLX5_CAP_GEN(dev, max_num_of_monitor_counters))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_MONITOR_COUNTER);\n\n\tif (mlx5_eswitch_is_funcs_handler(dev))\n\t\tasync_event_mask |=\n\t\t\t(1ull << MLX5_EVENT_TYPE_ESW_FUNCTIONS_CHANGED);\n\n\tif (MLX5_CAP_GEN_MAX(dev, vhca_state))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_VHCA_STATE_CHANGE);\n\n\tif (MLX5_CAP_MACSEC(dev, log_max_macsec_offload))\n\t\tasync_event_mask |= (1ull << MLX5_EVENT_TYPE_OBJECT_CHANGE);\n\n\tif (mlx5_ipsec_device_caps(dev) & MLX5_IPSEC_CAP_PACKET_OFFLOAD)\n\t\tasync_event_mask |=\n\t\t\t(1ull << MLX5_EVENT_TYPE_OBJECT_CHANGE);\n\n\tmask[0] = async_event_mask;\n\n\tif (MLX5_CAP_GEN(dev, event_cap))\n\t\tgather_user_async_events(dev, mask);\n}\n\nstatic int\nsetup_async_eq(struct mlx5_core_dev *dev, struct mlx5_eq_async *eq,\n\t       struct mlx5_eq_param *param, const char *name)\n{\n\tint err;\n\n\teq->irq_nb.notifier_call = mlx5_eq_async_int;\n\tspin_lock_init(&eq->lock);\n\n\terr = create_async_eq(dev, &eq->core, param);\n\tif (err) {\n\t\tmlx5_core_warn(dev, \"failed to create %s EQ %d\\n\", name, err);\n\t\treturn err;\n\t}\n\terr = mlx5_eq_enable(dev, &eq->core, &eq->irq_nb);\n\tif (err) {\n\t\tmlx5_core_warn(dev, \"failed to enable %s EQ %d\\n\", name, err);\n\t\tdestroy_async_eq(dev, &eq->core);\n\t}\n\treturn err;\n}\n\nstatic void cleanup_async_eq(struct mlx5_core_dev *dev,\n\t\t\t     struct mlx5_eq_async *eq, const char *name)\n{\n\tint err;\n\n\tmlx5_eq_disable(dev, &eq->core, &eq->irq_nb);\n\terr = destroy_async_eq(dev, &eq->core);\n\tif (err)\n\t\tmlx5_core_err(dev, \"failed to destroy %s eq, err(%d)\\n\",\n\t\t\t      name, err);\n}\n\nstatic u16 async_eq_depth_devlink_param_get(struct mlx5_core_dev *dev)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\tunion devlink_param_value val;\n\tint err;\n\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      DEVLINK_PARAM_GENERIC_ID_EVENT_EQ_SIZE,\n\t\t\t\t\t      &val);\n\tif (!err)\n\t\treturn val.vu32;\n\tmlx5_core_dbg(dev, \"Failed to get param. using default. err = %d\\n\", err);\n\treturn MLX5_NUM_ASYNC_EQE;\n}\n\nstatic int create_async_eqs(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_eq_param param = {};\n\tint err;\n\n\t \n\ttable->ctrl_irq = mlx5_ctrl_irq_request(dev);\n\tif (IS_ERR(table->ctrl_irq))\n\t\treturn PTR_ERR(table->ctrl_irq);\n\n\tMLX5_NB_INIT(&table->cq_err_nb, cq_err_event_notifier, CQ_ERROR);\n\tmlx5_eq_notifier_register(dev, &table->cq_err_nb);\n\n\tparam = (struct mlx5_eq_param) {\n\t\t.irq = table->ctrl_irq,\n\t\t.nent = MLX5_NUM_CMD_EQE,\n\t\t.mask[0] = 1ull << MLX5_EVENT_TYPE_CMD,\n\t};\n\tmlx5_cmd_allowed_opcode(dev, MLX5_CMD_OP_CREATE_EQ);\n\terr = setup_async_eq(dev, &table->cmd_eq, &param, \"cmd\");\n\tif (err)\n\t\tgoto err1;\n\n\tmlx5_cmd_use_events(dev);\n\tmlx5_cmd_allowed_opcode(dev, CMD_ALLOWED_OPCODE_ALL);\n\n\tparam = (struct mlx5_eq_param) {\n\t\t.irq = table->ctrl_irq,\n\t\t.nent = async_eq_depth_devlink_param_get(dev),\n\t};\n\n\tgather_async_events_mask(dev, param.mask);\n\terr = setup_async_eq(dev, &table->async_eq, &param, \"async\");\n\tif (err)\n\t\tgoto err2;\n\n\tparam = (struct mlx5_eq_param) {\n\t\t.irq = table->ctrl_irq,\n\t\t.nent =   1,\n\t\t.mask[0] = 1ull << MLX5_EVENT_TYPE_PAGE_REQUEST,\n\t};\n\n\terr = setup_async_eq(dev, &table->pages_eq, &param, \"pages\");\n\tif (err)\n\t\tgoto err3;\n\n\treturn 0;\n\nerr3:\n\tcleanup_async_eq(dev, &table->async_eq, \"async\");\nerr2:\n\tmlx5_cmd_use_polling(dev);\n\tcleanup_async_eq(dev, &table->cmd_eq, \"cmd\");\nerr1:\n\tmlx5_cmd_allowed_opcode(dev, CMD_ALLOWED_OPCODE_ALL);\n\tmlx5_eq_notifier_unregister(dev, &table->cq_err_nb);\n\tmlx5_ctrl_irq_release(table->ctrl_irq);\n\treturn err;\n}\n\nstatic void destroy_async_eqs(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\n\tcleanup_async_eq(dev, &table->pages_eq, \"pages\");\n\tcleanup_async_eq(dev, &table->async_eq, \"async\");\n\tmlx5_cmd_allowed_opcode(dev, MLX5_CMD_OP_DESTROY_EQ);\n\tmlx5_cmd_use_polling(dev);\n\tcleanup_async_eq(dev, &table->cmd_eq, \"cmd\");\n\tmlx5_cmd_allowed_opcode(dev, CMD_ALLOWED_OPCODE_ALL);\n\tmlx5_eq_notifier_unregister(dev, &table->cq_err_nb);\n\tmlx5_ctrl_irq_release(table->ctrl_irq);\n}\n\nstruct mlx5_eq *mlx5_get_async_eq(struct mlx5_core_dev *dev)\n{\n\treturn &dev->priv.eq_table->async_eq.core;\n}\n\nvoid mlx5_eq_synchronize_async_irq(struct mlx5_core_dev *dev)\n{\n\tsynchronize_irq(dev->priv.eq_table->async_eq.core.irqn);\n}\n\nvoid mlx5_eq_synchronize_cmd_irq(struct mlx5_core_dev *dev)\n{\n\tsynchronize_irq(dev->priv.eq_table->cmd_eq.core.irqn);\n}\n\n \nstruct mlx5_eq *\nmlx5_eq_create_generic(struct mlx5_core_dev *dev,\n\t\t       struct mlx5_eq_param *param)\n{\n\tstruct mlx5_eq *eq = kvzalloc_node(sizeof(*eq), GFP_KERNEL,\n\t\t\t\t\t   dev->priv.numa_node);\n\tint err;\n\n\tif (!eq)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tparam->irq = dev->priv.eq_table->ctrl_irq;\n\terr = create_async_eq(dev, eq, param);\n\tif (err) {\n\t\tkvfree(eq);\n\t\teq = ERR_PTR(err);\n\t}\n\n\treturn eq;\n}\nEXPORT_SYMBOL(mlx5_eq_create_generic);\n\nint mlx5_eq_destroy_generic(struct mlx5_core_dev *dev, struct mlx5_eq *eq)\n{\n\tint err;\n\n\tif (IS_ERR(eq))\n\t\treturn -EINVAL;\n\n\terr = destroy_async_eq(dev, eq);\n\tif (err)\n\t\tgoto out;\n\n\tkvfree(eq);\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(mlx5_eq_destroy_generic);\n\nstruct mlx5_eqe *mlx5_eq_get_eqe(struct mlx5_eq *eq, u32 cc)\n{\n\tu32 ci = eq->cons_index + cc;\n\tu32 nent = eq_get_size(eq);\n\tstruct mlx5_eqe *eqe;\n\n\teqe = get_eqe(eq, ci & (nent - 1));\n\teqe = ((eqe->owner & 1) ^ !!(ci & nent)) ? NULL : eqe;\n\t \n\tif (eqe)\n\t\tdma_rmb();\n\n\treturn eqe;\n}\nEXPORT_SYMBOL(mlx5_eq_get_eqe);\n\nvoid mlx5_eq_update_ci(struct mlx5_eq *eq, u32 cc, bool arm)\n{\n\t__be32 __iomem *addr = eq->doorbell + (arm ? 0 : 2);\n\tu32 val;\n\n\teq->cons_index += cc;\n\tval = (eq->cons_index & 0xffffff) | (eq->eqn << 24);\n\n\t__raw_writel((__force u32)cpu_to_be32(val), addr);\n\t \n\twmb();\n}\nEXPORT_SYMBOL(mlx5_eq_update_ci);\n\nstatic void comp_irq_release_pci(struct mlx5_core_dev *dev, u16 vecidx)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_irq *irq;\n\n\tirq = xa_load(&table->comp_irqs, vecidx);\n\tif (!irq)\n\t\treturn;\n\n\txa_erase(&table->comp_irqs, vecidx);\n\tmlx5_irq_release_vector(irq);\n}\n\nstatic int mlx5_cpumask_default_spread(int numa_node, int index)\n{\n\tconst struct cpumask *prev = cpu_none_mask;\n\tconst struct cpumask *mask;\n\tint found_cpu = 0;\n\tint i = 0;\n\tint cpu;\n\n\trcu_read_lock();\n\tfor_each_numa_hop_mask(mask, numa_node) {\n\t\tfor_each_cpu_andnot(cpu, mask, prev) {\n\t\t\tif (i++ == index) {\n\t\t\t\tfound_cpu = cpu;\n\t\t\t\tgoto spread_done;\n\t\t\t}\n\t\t}\n\t\tprev = mask;\n\t}\n\nspread_done:\n\trcu_read_unlock();\n\treturn found_cpu;\n}\n\nstatic struct cpu_rmap *mlx5_eq_table_get_pci_rmap(struct mlx5_core_dev *dev)\n{\n#ifdef CONFIG_RFS_ACCEL\n#ifdef CONFIG_MLX5_SF\n\tif (mlx5_core_is_sf(dev))\n\t\treturn dev->priv.parent_mdev->priv.eq_table->rmap;\n#endif\n\treturn dev->priv.eq_table->rmap;\n#else\n\treturn NULL;\n#endif\n}\n\nstatic int comp_irq_request_pci(struct mlx5_core_dev *dev, u16 vecidx)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct cpu_rmap *rmap;\n\tstruct mlx5_irq *irq;\n\tint cpu;\n\n\trmap = mlx5_eq_table_get_pci_rmap(dev);\n\tcpu = mlx5_cpumask_default_spread(dev->priv.numa_node, vecidx);\n\tirq = mlx5_irq_request_vector(dev, cpu, vecidx, &rmap);\n\tif (IS_ERR(irq))\n\t\treturn PTR_ERR(irq);\n\n\treturn xa_err(xa_store(&table->comp_irqs, vecidx, irq, GFP_KERNEL));\n}\n\nstatic void comp_irq_release_sf(struct mlx5_core_dev *dev, u16 vecidx)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_irq *irq;\n\tint cpu;\n\n\tirq = xa_load(&table->comp_irqs, vecidx);\n\tif (!irq)\n\t\treturn;\n\n\tcpu = cpumask_first(mlx5_irq_get_affinity_mask(irq));\n\tcpumask_clear_cpu(cpu, &table->used_cpus);\n\txa_erase(&table->comp_irqs, vecidx);\n\tmlx5_irq_affinity_irq_release(dev, irq);\n}\n\nstatic int comp_irq_request_sf(struct mlx5_core_dev *dev, u16 vecidx)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_irq_pool *pool = mlx5_irq_pool_get(dev);\n\tstruct irq_affinity_desc af_desc = {};\n\tstruct mlx5_irq *irq;\n\n\t \n\tif (!mlx5_irq_pool_is_sf_pool(pool))\n\t\treturn comp_irq_request_pci(dev, vecidx);\n\n\taf_desc.is_managed = 1;\n\tcpumask_copy(&af_desc.mask, cpu_online_mask);\n\tcpumask_andnot(&af_desc.mask, &af_desc.mask, &table->used_cpus);\n\tirq = mlx5_irq_affinity_request(pool, &af_desc);\n\tif (IS_ERR(irq))\n\t\treturn PTR_ERR(irq);\n\n\tcpumask_or(&table->used_cpus, &table->used_cpus, mlx5_irq_get_affinity_mask(irq));\n\tmlx5_core_dbg(pool->dev, \"IRQ %u mapped to cpu %*pbl, %u EQs on this irq\\n\",\n\t\t      pci_irq_vector(dev->pdev, mlx5_irq_get_index(irq)),\n\t\t      cpumask_pr_args(mlx5_irq_get_affinity_mask(irq)),\n\t\t      mlx5_irq_read_locked(irq) / MLX5_EQ_REFS_PER_IRQ);\n\n\treturn xa_err(xa_store(&table->comp_irqs, vecidx, irq, GFP_KERNEL));\n}\n\nstatic void comp_irq_release(struct mlx5_core_dev *dev, u16 vecidx)\n{\n\tmlx5_core_is_sf(dev) ? comp_irq_release_sf(dev, vecidx) :\n\t\t\t       comp_irq_release_pci(dev, vecidx);\n}\n\nstatic int comp_irq_request(struct mlx5_core_dev *dev, u16 vecidx)\n{\n\treturn mlx5_core_is_sf(dev) ? comp_irq_request_sf(dev, vecidx) :\n\t\t\t\t      comp_irq_request_pci(dev, vecidx);\n}\n\n#ifdef CONFIG_RFS_ACCEL\nstatic int alloc_rmap(struct mlx5_core_dev *mdev)\n{\n\tstruct mlx5_eq_table *eq_table = mdev->priv.eq_table;\n\n\t \n\tif (mlx5_core_is_sf(mdev))\n\t\treturn 0;\n\n\teq_table->rmap = alloc_irq_cpu_rmap(eq_table->max_comp_eqs);\n\tif (!eq_table->rmap)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void free_rmap(struct mlx5_core_dev *mdev)\n{\n\tstruct mlx5_eq_table *eq_table = mdev->priv.eq_table;\n\n\tif (eq_table->rmap) {\n\t\tfree_irq_cpu_rmap(eq_table->rmap);\n\t\teq_table->rmap = NULL;\n\t}\n}\n#else\nstatic int alloc_rmap(struct mlx5_core_dev *mdev) { return 0; }\nstatic void free_rmap(struct mlx5_core_dev *mdev) {}\n#endif\n\nstatic void destroy_comp_eq(struct mlx5_core_dev *dev, struct mlx5_eq_comp *eq, u16 vecidx)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\n\txa_erase(&table->comp_eqs, vecidx);\n\tmlx5_eq_disable(dev, &eq->core, &eq->irq_nb);\n\tif (destroy_unmap_eq(dev, &eq->core))\n\t\tmlx5_core_warn(dev, \"failed to destroy comp EQ 0x%x\\n\",\n\t\t\t       eq->core.eqn);\n\ttasklet_disable(&eq->tasklet_ctx.task);\n\tkfree(eq);\n\tcomp_irq_release(dev, vecidx);\n\ttable->curr_comp_eqs--;\n}\n\nstatic u16 comp_eq_depth_devlink_param_get(struct mlx5_core_dev *dev)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\tunion devlink_param_value val;\n\tint err;\n\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      DEVLINK_PARAM_GENERIC_ID_IO_EQ_SIZE,\n\t\t\t\t\t      &val);\n\tif (!err)\n\t\treturn val.vu32;\n\tmlx5_core_dbg(dev, \"Failed to get param. using default. err = %d\\n\", err);\n\treturn MLX5_COMP_EQ_SIZE;\n}\n\n \nstatic int create_comp_eq(struct mlx5_core_dev *dev, u16 vecidx)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_eq_param param = {};\n\tstruct mlx5_eq_comp *eq;\n\tstruct mlx5_irq *irq;\n\tint nent;\n\tint err;\n\n\tlockdep_assert_held(&table->comp_lock);\n\tif (table->curr_comp_eqs == table->max_comp_eqs) {\n\t\tmlx5_core_err(dev, \"maximum number of vectors is allocated, %d\\n\",\n\t\t\t      table->max_comp_eqs);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = comp_irq_request(dev, vecidx);\n\tif (err)\n\t\treturn err;\n\n\tnent = comp_eq_depth_devlink_param_get(dev);\n\n\teq = kzalloc_node(sizeof(*eq), GFP_KERNEL, dev->priv.numa_node);\n\tif (!eq) {\n\t\terr = -ENOMEM;\n\t\tgoto clean_irq;\n\t}\n\n\tINIT_LIST_HEAD(&eq->tasklet_ctx.list);\n\tINIT_LIST_HEAD(&eq->tasklet_ctx.process_list);\n\tspin_lock_init(&eq->tasklet_ctx.lock);\n\ttasklet_setup(&eq->tasklet_ctx.task, mlx5_cq_tasklet_cb);\n\n\tirq = xa_load(&table->comp_irqs, vecidx);\n\teq->irq_nb.notifier_call = mlx5_eq_comp_int;\n\tparam = (struct mlx5_eq_param) {\n\t\t.irq = irq,\n\t\t.nent = nent,\n\t};\n\n\terr = create_map_eq(dev, &eq->core, &param);\n\tif (err)\n\t\tgoto clean_eq;\n\terr = mlx5_eq_enable(dev, &eq->core, &eq->irq_nb);\n\tif (err) {\n\t\tdestroy_unmap_eq(dev, &eq->core);\n\t\tgoto clean_eq;\n\t}\n\n\tmlx5_core_dbg(dev, \"allocated completion EQN %d\\n\", eq->core.eqn);\n\terr = xa_err(xa_store(&table->comp_eqs, vecidx, eq, GFP_KERNEL));\n\tif (err)\n\t\tgoto disable_eq;\n\n\ttable->curr_comp_eqs++;\n\treturn eq->core.eqn;\n\ndisable_eq:\n\tmlx5_eq_disable(dev, &eq->core, &eq->irq_nb);\nclean_eq:\n\tkfree(eq);\nclean_irq:\n\tcomp_irq_release(dev, vecidx);\n\treturn err;\n}\n\nint mlx5_comp_eqn_get(struct mlx5_core_dev *dev, u16 vecidx, int *eqn)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_eq_comp *eq;\n\tint ret = 0;\n\n\tmutex_lock(&table->comp_lock);\n\teq = xa_load(&table->comp_eqs, vecidx);\n\tif (eq) {\n\t\t*eqn = eq->core.eqn;\n\t\tgoto out;\n\t}\n\n\tret = create_comp_eq(dev, vecidx);\n\tif (ret < 0) {\n\t\tmutex_unlock(&table->comp_lock);\n\t\treturn ret;\n\t}\n\n\t*eqn = ret;\nout:\n\tmutex_unlock(&table->comp_lock);\n\treturn 0;\n}\nEXPORT_SYMBOL(mlx5_comp_eqn_get);\n\nint mlx5_comp_irqn_get(struct mlx5_core_dev *dev, int vector, unsigned int *irqn)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_eq_comp *eq;\n\tint eqn;\n\tint err;\n\n\t \n\terr = mlx5_comp_eqn_get(dev, vector, &eqn);\n\tif (err)\n\t\treturn err;\n\n\teq = xa_load(&table->comp_eqs, vector);\n\t*irqn = eq->core.irqn;\n\treturn 0;\n}\n\nunsigned int mlx5_comp_vectors_max(struct mlx5_core_dev *dev)\n{\n\treturn dev->priv.eq_table->max_comp_eqs;\n}\nEXPORT_SYMBOL(mlx5_comp_vectors_max);\n\nstatic struct cpumask *\nmlx5_comp_irq_get_affinity_mask(struct mlx5_core_dev *dev, int vector)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_eq_comp *eq;\n\n\teq = xa_load(&table->comp_eqs, vector);\n\tif (eq)\n\t\treturn mlx5_irq_get_affinity_mask(eq->core.irq);\n\n\treturn NULL;\n}\n\nint mlx5_comp_vector_get_cpu(struct mlx5_core_dev *dev, int vector)\n{\n\tstruct cpumask *mask;\n\tint cpu;\n\n\tmask = mlx5_comp_irq_get_affinity_mask(dev, vector);\n\tif (mask)\n\t\tcpu = cpumask_first(mask);\n\telse\n\t\tcpu = mlx5_cpumask_default_spread(dev->priv.numa_node, vector);\n\n\treturn cpu;\n}\nEXPORT_SYMBOL(mlx5_comp_vector_get_cpu);\n\n#ifdef CONFIG_RFS_ACCEL\nstruct cpu_rmap *mlx5_eq_table_get_rmap(struct mlx5_core_dev *dev)\n{\n\treturn dev->priv.eq_table->rmap;\n}\n#endif\n\nstruct mlx5_eq_comp *mlx5_eqn2comp_eq(struct mlx5_core_dev *dev, int eqn)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_eq_comp *eq;\n\tunsigned long index;\n\n\txa_for_each(&table->comp_eqs, index, eq)\n\t\tif (eq->core.eqn == eqn)\n\t\t\treturn eq;\n\n\treturn ERR_PTR(-ENOENT);\n}\n\n \nvoid mlx5_core_eq_free_irqs(struct mlx5_core_dev *dev)\n{\n\tmlx5_irq_table_free_irqs(dev);\n}\n\n#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING\n#define MLX5_MAX_ASYNC_EQS 4\n#else\n#define MLX5_MAX_ASYNC_EQS 3\n#endif\n\nstatic int get_num_eqs(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eq_table *eq_table = dev->priv.eq_table;\n\tint max_dev_eqs;\n\tint max_eqs_sf;\n\tint num_eqs;\n\n\t \n\tif (!mlx5_core_is_eth_enabled(dev) && mlx5_eth_supported(dev))\n\t\treturn 1;\n\n\tmax_dev_eqs = MLX5_CAP_GEN(dev, max_num_eqs) ?\n\t\t      MLX5_CAP_GEN(dev, max_num_eqs) :\n\t\t      1 << MLX5_CAP_GEN(dev, log_max_eq);\n\n\tnum_eqs = min_t(int, mlx5_irq_table_get_num_comp(eq_table->irq_table),\n\t\t\tmax_dev_eqs - MLX5_MAX_ASYNC_EQS);\n\tif (mlx5_core_is_sf(dev)) {\n\t\tmax_eqs_sf = min_t(int, MLX5_COMP_EQS_PER_SF,\n\t\t\t\t   mlx5_irq_table_get_sfs_vec(eq_table->irq_table));\n\t\tnum_eqs = min_t(int, num_eqs, max_eqs_sf);\n\t}\n\n\treturn num_eqs;\n}\n\nint mlx5_eq_table_create(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eq_table *eq_table = dev->priv.eq_table;\n\tint err;\n\n\teq_table->max_comp_eqs = get_num_eqs(dev);\n\terr = create_async_eqs(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to create async EQs\\n\");\n\t\tgoto err_async_eqs;\n\t}\n\n\terr = alloc_rmap(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to allocate rmap\\n\");\n\t\tgoto err_rmap;\n\t}\n\n\treturn 0;\n\nerr_rmap:\n\tdestroy_async_eqs(dev);\nerr_async_eqs:\n\treturn err;\n}\n\nvoid mlx5_eq_table_destroy(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eq_table *table = dev->priv.eq_table;\n\tstruct mlx5_eq_comp *eq;\n\tunsigned long index;\n\n\txa_for_each(&table->comp_eqs, index, eq)\n\t\tdestroy_comp_eq(dev, eq, index);\n\n\tfree_rmap(dev);\n\tdestroy_async_eqs(dev);\n}\n\nint mlx5_eq_notifier_register(struct mlx5_core_dev *dev, struct mlx5_nb *nb)\n{\n\tstruct mlx5_eq_table *eqt = dev->priv.eq_table;\n\n\treturn atomic_notifier_chain_register(&eqt->nh[nb->event_type], &nb->nb);\n}\nEXPORT_SYMBOL(mlx5_eq_notifier_register);\n\nint mlx5_eq_notifier_unregister(struct mlx5_core_dev *dev, struct mlx5_nb *nb)\n{\n\tstruct mlx5_eq_table *eqt = dev->priv.eq_table;\n\n\treturn atomic_notifier_chain_unregister(&eqt->nh[nb->event_type], &nb->nb);\n}\nEXPORT_SYMBOL(mlx5_eq_notifier_unregister);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}