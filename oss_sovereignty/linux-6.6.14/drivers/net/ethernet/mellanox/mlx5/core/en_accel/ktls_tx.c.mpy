{
  "module_name": "ktls_tx.c",
  "hash_id": "eb96651de9bf7b81c971ac2632198c17e9292ba6ff31850514450d76d050bf3d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_tx.c",
  "human_readable_source": "\n\n\n#include <linux/debugfs.h>\n#include \"en_accel/ktls.h\"\n#include \"en_accel/ktls_txrx.h\"\n#include \"en_accel/ktls_utils.h\"\n\nstruct mlx5e_dump_wqe {\n\tstruct mlx5_wqe_ctrl_seg ctrl;\n\tstruct mlx5_wqe_data_seg data;\n};\n\n#define MLX5E_KTLS_DUMP_WQEBBS \\\n\t(DIV_ROUND_UP(sizeof(struct mlx5e_dump_wqe), MLX5_SEND_WQE_BB))\n\nstatic u8\nmlx5e_ktls_dumps_num_wqes(struct mlx5e_params *params, unsigned int nfrags,\n\t\t\t  unsigned int sync_len)\n{\n\t \n\treturn nfrags + DIV_ROUND_UP(sync_len, MLX5E_SW2HW_MTU(params, params->sw_mtu));\n}\n\nu16 mlx5e_ktls_get_stop_room(struct mlx5_core_dev *mdev, struct mlx5e_params *params)\n{\n\tu16 num_dumps, stop_room = 0;\n\n\tif (!mlx5e_is_ktls_tx(mdev))\n\t\treturn 0;\n\n\tnum_dumps = mlx5e_ktls_dumps_num_wqes(params, MAX_SKB_FRAGS, TLS_MAX_PAYLOAD_SIZE);\n\n\tstop_room += mlx5e_stop_room_for_wqe(mdev, MLX5E_TLS_SET_STATIC_PARAMS_WQEBBS);\n\tstop_room += mlx5e_stop_room_for_wqe(mdev, MLX5E_TLS_SET_PROGRESS_PARAMS_WQEBBS);\n\tstop_room += num_dumps * mlx5e_stop_room_for_wqe(mdev, MLX5E_KTLS_DUMP_WQEBBS);\n\tstop_room += 1;  \n\n\treturn stop_room;\n}\n\nstatic void mlx5e_ktls_set_tisc(struct mlx5_core_dev *mdev, void *tisc)\n{\n\tMLX5_SET(tisc, tisc, tls_en, 1);\n\tMLX5_SET(tisc, tisc, pd, mdev->mlx5e_res.hw_objs.pdn);\n\tMLX5_SET(tisc, tisc, transport_domain, mdev->mlx5e_res.hw_objs.td.tdn);\n}\n\nstatic int mlx5e_ktls_create_tis(struct mlx5_core_dev *mdev, u32 *tisn)\n{\n\tu32 in[MLX5_ST_SZ_DW(create_tis_in)] = {};\n\n\tmlx5e_ktls_set_tisc(mdev, MLX5_ADDR_OF(create_tis_in, in, ctx));\n\n\treturn mlx5_core_create_tis(mdev, in, tisn);\n}\n\nstatic int mlx5e_ktls_create_tis_cb(struct mlx5_core_dev *mdev,\n\t\t\t\t    struct mlx5_async_ctx *async_ctx,\n\t\t\t\t    u32 *out, int outlen,\n\t\t\t\t    mlx5_async_cbk_t callback,\n\t\t\t\t    struct mlx5_async_work *context)\n{\n\tu32 in[MLX5_ST_SZ_DW(create_tis_in)] = {};\n\n\tmlx5e_ktls_set_tisc(mdev, MLX5_ADDR_OF(create_tis_in, in, ctx));\n\tMLX5_SET(create_tis_in, in, opcode, MLX5_CMD_OP_CREATE_TIS);\n\n\treturn mlx5_cmd_exec_cb(async_ctx, in, sizeof(in),\n\t\t\t\tout, outlen, callback, context);\n}\n\nstatic int mlx5e_ktls_destroy_tis_cb(struct mlx5_core_dev *mdev, u32 tisn,\n\t\t\t\t     struct mlx5_async_ctx *async_ctx,\n\t\t\t\t     u32 *out, int outlen,\n\t\t\t\t     mlx5_async_cbk_t callback,\n\t\t\t\t     struct mlx5_async_work *context)\n{\n\tu32 in[MLX5_ST_SZ_DW(destroy_tis_in)] = {};\n\n\tMLX5_SET(destroy_tis_in, in, opcode, MLX5_CMD_OP_DESTROY_TIS);\n\tMLX5_SET(destroy_tis_in, in, tisn, tisn);\n\n\treturn mlx5_cmd_exec_cb(async_ctx, in, sizeof(in),\n\t\t\t\tout, outlen, callback, context);\n}\n\nstruct mlx5e_ktls_offload_context_tx {\n\t \n\tu32 expected_seq;\n\tu32 tisn;\n\tbool ctx_post_pending;\n\t \n\tstruct list_head list_node;  \n\tunion mlx5e_crypto_info crypto_info;\n\tstruct tls_offload_context_tx *tx_ctx;\n\tstruct mlx5_core_dev *mdev;\n\tstruct mlx5e_tls_sw_stats *sw_stats;\n\tstruct mlx5_crypto_dek *dek;\n\tu8 create_err : 1;\n};\n\nstatic void\nmlx5e_set_ktls_tx_priv_ctx(struct tls_context *tls_ctx,\n\t\t\t   struct mlx5e_ktls_offload_context_tx *priv_tx)\n{\n\tstruct mlx5e_ktls_offload_context_tx **ctx =\n\t\t__tls_driver_ctx(tls_ctx, TLS_OFFLOAD_CTX_DIR_TX);\n\n\tBUILD_BUG_ON(sizeof(priv_tx) > TLS_DRIVER_STATE_SIZE_TX);\n\n\t*ctx = priv_tx;\n}\n\nstatic struct mlx5e_ktls_offload_context_tx *\nmlx5e_get_ktls_tx_priv_ctx(struct tls_context *tls_ctx)\n{\n\tstruct mlx5e_ktls_offload_context_tx **ctx =\n\t\t__tls_driver_ctx(tls_ctx, TLS_OFFLOAD_CTX_DIR_TX);\n\n\treturn *ctx;\n}\n\n \nstruct mlx5e_async_ctx {\n\tstruct mlx5_async_work context;\n\tstruct mlx5_async_ctx *async_ctx;\n\tstruct mlx5e_ktls_offload_context_tx *priv_tx;\n\tint err;\n\tunion {\n\t\tu32 out_create[MLX5_ST_SZ_DW(create_tis_out)];\n\t\tu32 out_destroy[MLX5_ST_SZ_DW(destroy_tis_out)];\n\t};\n};\n\nstruct mlx5e_bulk_async_ctx {\n\tstruct mlx5_async_ctx async_ctx;\n\tDECLARE_FLEX_ARRAY(struct mlx5e_async_ctx, arr);\n};\n\nstatic struct mlx5e_bulk_async_ctx *mlx5e_bulk_async_init(struct mlx5_core_dev *mdev, int n)\n{\n\tstruct mlx5e_bulk_async_ctx *bulk_async;\n\tint sz;\n\tint i;\n\n\tsz = struct_size(bulk_async, arr, n);\n\tbulk_async = kvzalloc(sz, GFP_KERNEL);\n\tif (!bulk_async)\n\t\treturn NULL;\n\n\tmlx5_cmd_init_async_ctx(mdev, &bulk_async->async_ctx);\n\n\tfor (i = 0; i < n; i++)\n\t\tbulk_async->arr[i].async_ctx = &bulk_async->async_ctx;\n\n\treturn bulk_async;\n}\n\nstatic void mlx5e_bulk_async_cleanup(struct mlx5e_bulk_async_ctx *bulk_async)\n{\n\tmlx5_cmd_cleanup_async_ctx(&bulk_async->async_ctx);\n\tkvfree(bulk_async);\n}\n\nstatic void create_tis_callback(int status, struct mlx5_async_work *context)\n{\n\tstruct mlx5e_async_ctx *async =\n\t\tcontainer_of(context, struct mlx5e_async_ctx, context);\n\tstruct mlx5e_ktls_offload_context_tx *priv_tx = async->priv_tx;\n\n\tif (status) {\n\t\tasync->err = status;\n\t\tpriv_tx->create_err = 1;\n\t\treturn;\n\t}\n\n\tpriv_tx->tisn = MLX5_GET(create_tis_out, async->out_create, tisn);\n}\n\nstatic void destroy_tis_callback(int status, struct mlx5_async_work *context)\n{\n\tstruct mlx5e_async_ctx *async =\n\t\tcontainer_of(context, struct mlx5e_async_ctx, context);\n\tstruct mlx5e_ktls_offload_context_tx *priv_tx = async->priv_tx;\n\n\tkfree(priv_tx);\n}\n\nstatic struct mlx5e_ktls_offload_context_tx *\nmlx5e_tls_priv_tx_init(struct mlx5_core_dev *mdev, struct mlx5e_tls_sw_stats *sw_stats,\n\t\t       struct mlx5e_async_ctx *async)\n{\n\tstruct mlx5e_ktls_offload_context_tx *priv_tx;\n\tint err;\n\n\tpriv_tx = kzalloc(sizeof(*priv_tx), GFP_KERNEL);\n\tif (!priv_tx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tpriv_tx->mdev = mdev;\n\tpriv_tx->sw_stats = sw_stats;\n\n\tif (!async) {\n\t\terr = mlx5e_ktls_create_tis(mdev, &priv_tx->tisn);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t} else {\n\t\tasync->priv_tx = priv_tx;\n\t\terr = mlx5e_ktls_create_tis_cb(mdev, async->async_ctx,\n\t\t\t\t\t       async->out_create, sizeof(async->out_create),\n\t\t\t\t\t       create_tis_callback, &async->context);\n\t\tif (err)\n\t\t\tgoto err_out;\n\t}\n\n\treturn priv_tx;\n\nerr_out:\n\tkfree(priv_tx);\n\treturn ERR_PTR(err);\n}\n\nstatic void mlx5e_tls_priv_tx_cleanup(struct mlx5e_ktls_offload_context_tx *priv_tx,\n\t\t\t\t      struct mlx5e_async_ctx *async)\n{\n\tif (priv_tx->create_err) {\n\t\tkfree(priv_tx);\n\t\treturn;\n\t}\n\tasync->priv_tx = priv_tx;\n\tmlx5e_ktls_destroy_tis_cb(priv_tx->mdev, priv_tx->tisn,\n\t\t\t\t  async->async_ctx,\n\t\t\t\t  async->out_destroy, sizeof(async->out_destroy),\n\t\t\t\t  destroy_tis_callback, &async->context);\n}\n\nstatic void mlx5e_tls_priv_tx_list_cleanup(struct mlx5_core_dev *mdev,\n\t\t\t\t\t   struct list_head *list, int size)\n{\n\tstruct mlx5e_ktls_offload_context_tx *obj, *n;\n\tstruct mlx5e_bulk_async_ctx *bulk_async;\n\tint i;\n\n\tbulk_async = mlx5e_bulk_async_init(mdev, size);\n\tif (!bulk_async)\n\t\treturn;\n\n\ti = 0;\n\tlist_for_each_entry_safe(obj, n, list, list_node) {\n\t\tmlx5e_tls_priv_tx_cleanup(obj, &bulk_async->arr[i]);\n\t\ti++;\n\t}\n\n\tmlx5e_bulk_async_cleanup(bulk_async);\n}\n\n \n\n#define MLX5E_TLS_TX_POOL_BULK (16)\n#define MLX5E_TLS_TX_POOL_HIGH (4 * 1024)\n#define MLX5E_TLS_TX_POOL_LOW (MLX5E_TLS_TX_POOL_HIGH / 4)\n\nstruct mlx5e_tls_tx_pool {\n\tstruct mlx5_core_dev *mdev;\n\tstruct mlx5e_tls_sw_stats *sw_stats;\n\tstruct mutex lock;  \n\tstruct list_head list;\n\tsize_t size;\n\n\tstruct workqueue_struct *wq;\n\tstruct work_struct create_work;\n\tstruct work_struct destroy_work;\n};\n\nstatic void create_work(struct work_struct *work)\n{\n\tstruct mlx5e_tls_tx_pool *pool =\n\t\tcontainer_of(work, struct mlx5e_tls_tx_pool, create_work);\n\tstruct mlx5e_ktls_offload_context_tx *obj;\n\tstruct mlx5e_bulk_async_ctx *bulk_async;\n\tLIST_HEAD(local_list);\n\tint i, j, err = 0;\n\n\tbulk_async = mlx5e_bulk_async_init(pool->mdev, MLX5E_TLS_TX_POOL_BULK);\n\tif (!bulk_async)\n\t\treturn;\n\n\tfor (i = 0; i < MLX5E_TLS_TX_POOL_BULK; i++) {\n\t\tobj = mlx5e_tls_priv_tx_init(pool->mdev, pool->sw_stats, &bulk_async->arr[i]);\n\t\tif (IS_ERR(obj)) {\n\t\t\terr = PTR_ERR(obj);\n\t\t\tbreak;\n\t\t}\n\t\tlist_add(&obj->list_node, &local_list);\n\t}\n\n\tfor (j = 0; j < i; j++) {\n\t\tstruct mlx5e_async_ctx *async = &bulk_async->arr[j];\n\n\t\tif (!err && async->err)\n\t\t\terr = async->err;\n\t}\n\tatomic64_add(i, &pool->sw_stats->tx_tls_pool_alloc);\n\tmlx5e_bulk_async_cleanup(bulk_async);\n\tif (err)\n\t\tgoto err_out;\n\n\tmutex_lock(&pool->lock);\n\tif (pool->size + MLX5E_TLS_TX_POOL_BULK >= MLX5E_TLS_TX_POOL_HIGH) {\n\t\tmutex_unlock(&pool->lock);\n\t\tgoto err_out;\n\t}\n\tlist_splice(&local_list, &pool->list);\n\tpool->size += MLX5E_TLS_TX_POOL_BULK;\n\tif (pool->size <= MLX5E_TLS_TX_POOL_LOW)\n\t\tqueue_work(pool->wq, work);\n\tmutex_unlock(&pool->lock);\n\treturn;\n\nerr_out:\n\tmlx5e_tls_priv_tx_list_cleanup(pool->mdev, &local_list, i);\n\tatomic64_add(i, &pool->sw_stats->tx_tls_pool_free);\n}\n\nstatic void destroy_work(struct work_struct *work)\n{\n\tstruct mlx5e_tls_tx_pool *pool =\n\t\tcontainer_of(work, struct mlx5e_tls_tx_pool, destroy_work);\n\tstruct mlx5e_ktls_offload_context_tx *obj;\n\tLIST_HEAD(local_list);\n\tint i = 0;\n\n\tmutex_lock(&pool->lock);\n\tif (pool->size < MLX5E_TLS_TX_POOL_HIGH) {\n\t\tmutex_unlock(&pool->lock);\n\t\treturn;\n\t}\n\n\tlist_for_each_entry(obj, &pool->list, list_node)\n\t\tif (++i == MLX5E_TLS_TX_POOL_BULK)\n\t\t\tbreak;\n\n\tlist_cut_position(&local_list, &pool->list, &obj->list_node);\n\tpool->size -= MLX5E_TLS_TX_POOL_BULK;\n\tif (pool->size >= MLX5E_TLS_TX_POOL_HIGH)\n\t\tqueue_work(pool->wq, work);\n\tmutex_unlock(&pool->lock);\n\n\tmlx5e_tls_priv_tx_list_cleanup(pool->mdev, &local_list, MLX5E_TLS_TX_POOL_BULK);\n\tatomic64_add(MLX5E_TLS_TX_POOL_BULK, &pool->sw_stats->tx_tls_pool_free);\n}\n\nstatic struct mlx5e_tls_tx_pool *mlx5e_tls_tx_pool_init(struct mlx5_core_dev *mdev,\n\t\t\t\t\t\t\tstruct mlx5e_tls_sw_stats *sw_stats)\n{\n\tstruct mlx5e_tls_tx_pool *pool;\n\n\tBUILD_BUG_ON(MLX5E_TLS_TX_POOL_LOW + MLX5E_TLS_TX_POOL_BULK >= MLX5E_TLS_TX_POOL_HIGH);\n\n\tpool = kvzalloc(sizeof(*pool), GFP_KERNEL);\n\tif (!pool)\n\t\treturn NULL;\n\n\tpool->wq = create_singlethread_workqueue(\"mlx5e_tls_tx_pool\");\n\tif (!pool->wq)\n\t\tgoto err_free;\n\n\tINIT_LIST_HEAD(&pool->list);\n\tmutex_init(&pool->lock);\n\n\tINIT_WORK(&pool->create_work, create_work);\n\tINIT_WORK(&pool->destroy_work, destroy_work);\n\n\tpool->mdev = mdev;\n\tpool->sw_stats = sw_stats;\n\n\treturn pool;\n\nerr_free:\n\tkvfree(pool);\n\treturn NULL;\n}\n\nstatic void mlx5e_tls_tx_pool_list_cleanup(struct mlx5e_tls_tx_pool *pool)\n{\n\twhile (pool->size > MLX5E_TLS_TX_POOL_BULK) {\n\t\tstruct mlx5e_ktls_offload_context_tx *obj;\n\t\tLIST_HEAD(local_list);\n\t\tint i = 0;\n\n\t\tlist_for_each_entry(obj, &pool->list, list_node)\n\t\t\tif (++i == MLX5E_TLS_TX_POOL_BULK)\n\t\t\t\tbreak;\n\n\t\tlist_cut_position(&local_list, &pool->list, &obj->list_node);\n\t\tmlx5e_tls_priv_tx_list_cleanup(pool->mdev, &local_list, MLX5E_TLS_TX_POOL_BULK);\n\t\tatomic64_add(MLX5E_TLS_TX_POOL_BULK, &pool->sw_stats->tx_tls_pool_free);\n\t\tpool->size -= MLX5E_TLS_TX_POOL_BULK;\n\t}\n\tif (pool->size) {\n\t\tmlx5e_tls_priv_tx_list_cleanup(pool->mdev, &pool->list, pool->size);\n\t\tatomic64_add(pool->size, &pool->sw_stats->tx_tls_pool_free);\n\t}\n}\n\nstatic void mlx5e_tls_tx_pool_cleanup(struct mlx5e_tls_tx_pool *pool)\n{\n\tmlx5e_tls_tx_pool_list_cleanup(pool);\n\tdestroy_workqueue(pool->wq);\n\tkvfree(pool);\n}\n\nstatic void pool_push(struct mlx5e_tls_tx_pool *pool, struct mlx5e_ktls_offload_context_tx *obj)\n{\n\tmutex_lock(&pool->lock);\n\tlist_add(&obj->list_node, &pool->list);\n\tif (++pool->size == MLX5E_TLS_TX_POOL_HIGH)\n\t\tqueue_work(pool->wq, &pool->destroy_work);\n\tmutex_unlock(&pool->lock);\n}\n\nstatic struct mlx5e_ktls_offload_context_tx *pool_pop(struct mlx5e_tls_tx_pool *pool)\n{\n\tstruct mlx5e_ktls_offload_context_tx *obj;\n\n\tmutex_lock(&pool->lock);\n\tif (unlikely(pool->size == 0)) {\n\t\t \n\t\tqueue_work(pool->wq, &pool->create_work);\n\t\tmutex_unlock(&pool->lock);\n\t\tobj = mlx5e_tls_priv_tx_init(pool->mdev, pool->sw_stats, NULL);\n\t\tif (!IS_ERR(obj))\n\t\t\tatomic64_inc(&pool->sw_stats->tx_tls_pool_alloc);\n\t\treturn obj;\n\t}\n\n\tobj = list_first_entry(&pool->list, struct mlx5e_ktls_offload_context_tx,\n\t\t\t       list_node);\n\tlist_del(&obj->list_node);\n\tif (--pool->size == MLX5E_TLS_TX_POOL_LOW)\n\t\tqueue_work(pool->wq, &pool->create_work);\n\tmutex_unlock(&pool->lock);\n\treturn obj;\n}\n\n \n\nint mlx5e_ktls_add_tx(struct net_device *netdev, struct sock *sk,\n\t\t      struct tls_crypto_info *crypto_info, u32 start_offload_tcp_sn)\n{\n\tstruct mlx5e_ktls_offload_context_tx *priv_tx;\n\tstruct mlx5e_tls_tx_pool *pool;\n\tstruct tls_context *tls_ctx;\n\tstruct mlx5_crypto_dek *dek;\n\tstruct mlx5e_priv *priv;\n\tint err;\n\n\ttls_ctx = tls_get_ctx(sk);\n\tpriv = netdev_priv(netdev);\n\tpool = priv->tls->tx_pool;\n\n\tpriv_tx = pool_pop(pool);\n\tif (IS_ERR(priv_tx))\n\t\treturn PTR_ERR(priv_tx);\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128:\n\t\tpriv_tx->crypto_info.crypto_info_128 =\n\t\t\t*(struct tls12_crypto_info_aes_gcm_128 *)crypto_info;\n\t\tbreak;\n\tcase TLS_CIPHER_AES_GCM_256:\n\t\tpriv_tx->crypto_info.crypto_info_256 =\n\t\t\t*(struct tls12_crypto_info_aes_gcm_256 *)crypto_info;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"Unsupported cipher type %u\\n\",\n\t\t\t  crypto_info->cipher_type);\n\t\terr = -EOPNOTSUPP;\n\t\tgoto err_pool_push;\n\t}\n\n\tdek = mlx5_ktls_create_key(priv->tls->dek_pool, crypto_info);\n\tif (IS_ERR(dek)) {\n\t\terr = PTR_ERR(dek);\n\t\tgoto err_pool_push;\n\t}\n\n\tpriv_tx->dek = dek;\n\tpriv_tx->expected_seq = start_offload_tcp_sn;\n\tpriv_tx->tx_ctx = tls_offload_ctx_tx(tls_ctx);\n\n\tmlx5e_set_ktls_tx_priv_ctx(tls_ctx, priv_tx);\n\n\tpriv_tx->ctx_post_pending = true;\n\tatomic64_inc(&priv_tx->sw_stats->tx_tls_ctx);\n\n\treturn 0;\n\nerr_pool_push:\n\tpool_push(pool, priv_tx);\n\treturn err;\n}\n\nvoid mlx5e_ktls_del_tx(struct net_device *netdev, struct tls_context *tls_ctx)\n{\n\tstruct mlx5e_ktls_offload_context_tx *priv_tx;\n\tstruct mlx5e_tls_tx_pool *pool;\n\tstruct mlx5e_priv *priv;\n\n\tpriv_tx = mlx5e_get_ktls_tx_priv_ctx(tls_ctx);\n\tpriv = netdev_priv(netdev);\n\tpool = priv->tls->tx_pool;\n\n\tatomic64_inc(&priv_tx->sw_stats->tx_tls_del);\n\tmlx5_ktls_destroy_key(priv->tls->dek_pool, priv_tx->dek);\n\tpool_push(pool, priv_tx);\n}\n\nstatic void tx_fill_wi(struct mlx5e_txqsq *sq,\n\t\t       u16 pi, u8 num_wqebbs, u32 num_bytes,\n\t\t       struct page *page)\n{\n\tstruct mlx5e_tx_wqe_info *wi = &sq->db.wqe_info[pi];\n\n\t*wi = (struct mlx5e_tx_wqe_info) {\n\t\t.num_wqebbs = num_wqebbs,\n\t\t.num_bytes  = num_bytes,\n\t\t.resync_dump_frag_page = page,\n\t};\n}\n\nstatic bool\nmlx5e_ktls_tx_offload_test_and_clear_pending(struct mlx5e_ktls_offload_context_tx *priv_tx)\n{\n\tbool ret = priv_tx->ctx_post_pending;\n\n\tpriv_tx->ctx_post_pending = false;\n\n\treturn ret;\n}\n\nstatic void\npost_static_params(struct mlx5e_txqsq *sq,\n\t\t   struct mlx5e_ktls_offload_context_tx *priv_tx,\n\t\t   bool fence)\n{\n\tstruct mlx5e_set_tls_static_params_wqe *wqe;\n\tu16 pi, num_wqebbs;\n\n\tnum_wqebbs = MLX5E_TLS_SET_STATIC_PARAMS_WQEBBS;\n\tpi = mlx5e_txqsq_get_next_pi(sq, num_wqebbs);\n\twqe = MLX5E_TLS_FETCH_SET_STATIC_PARAMS_WQE(sq, pi);\n\tmlx5e_ktls_build_static_params(wqe, sq->pc, sq->sqn, &priv_tx->crypto_info,\n\t\t\t\t       priv_tx->tisn,\n\t\t\t\t       mlx5_crypto_dek_get_id(priv_tx->dek),\n\t\t\t\t       0, fence, TLS_OFFLOAD_CTX_DIR_TX);\n\ttx_fill_wi(sq, pi, num_wqebbs, 0, NULL);\n\tsq->pc += num_wqebbs;\n}\n\nstatic void\npost_progress_params(struct mlx5e_txqsq *sq,\n\t\t     struct mlx5e_ktls_offload_context_tx *priv_tx,\n\t\t     bool fence)\n{\n\tstruct mlx5e_set_tls_progress_params_wqe *wqe;\n\tu16 pi, num_wqebbs;\n\n\tnum_wqebbs = MLX5E_TLS_SET_PROGRESS_PARAMS_WQEBBS;\n\tpi = mlx5e_txqsq_get_next_pi(sq, num_wqebbs);\n\twqe = MLX5E_TLS_FETCH_SET_PROGRESS_PARAMS_WQE(sq, pi);\n\tmlx5e_ktls_build_progress_params(wqe, sq->pc, sq->sqn, priv_tx->tisn, fence, 0,\n\t\t\t\t\t TLS_OFFLOAD_CTX_DIR_TX);\n\ttx_fill_wi(sq, pi, num_wqebbs, 0, NULL);\n\tsq->pc += num_wqebbs;\n}\n\nstatic void tx_post_fence_nop(struct mlx5e_txqsq *sq)\n{\n\tstruct mlx5_wq_cyc *wq = &sq->wq;\n\tu16 pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);\n\n\ttx_fill_wi(sq, pi, 1, 0, NULL);\n\n\tmlx5e_post_nop_fence(wq, sq->sqn, &sq->pc);\n}\n\nstatic void\nmlx5e_ktls_tx_post_param_wqes(struct mlx5e_txqsq *sq,\n\t\t\t      struct mlx5e_ktls_offload_context_tx *priv_tx,\n\t\t\t      bool skip_static_post, bool fence_first_post)\n{\n\tbool progress_fence = skip_static_post || !fence_first_post;\n\n\tif (!skip_static_post)\n\t\tpost_static_params(sq, priv_tx, fence_first_post);\n\n\tpost_progress_params(sq, priv_tx, progress_fence);\n\ttx_post_fence_nop(sq);\n}\n\nstruct tx_sync_info {\n\tu64 rcd_sn;\n\tu32 sync_len;\n\tint nr_frags;\n\tskb_frag_t frags[MAX_SKB_FRAGS];\n};\n\nenum mlx5e_ktls_sync_retval {\n\tMLX5E_KTLS_SYNC_DONE,\n\tMLX5E_KTLS_SYNC_FAIL,\n\tMLX5E_KTLS_SYNC_SKIP_NO_DATA,\n};\n\nstatic enum mlx5e_ktls_sync_retval\ntx_sync_info_get(struct mlx5e_ktls_offload_context_tx *priv_tx,\n\t\t u32 tcp_seq, int datalen, struct tx_sync_info *info)\n{\n\tstruct tls_offload_context_tx *tx_ctx = priv_tx->tx_ctx;\n\tenum mlx5e_ktls_sync_retval ret = MLX5E_KTLS_SYNC_DONE;\n\tstruct tls_record_info *record;\n\tint remaining, i = 0;\n\tunsigned long flags;\n\tbool ends_before;\n\n\tspin_lock_irqsave(&tx_ctx->lock, flags);\n\trecord = tls_get_record(tx_ctx, tcp_seq, &info->rcd_sn);\n\n\tif (unlikely(!record)) {\n\t\tret = MLX5E_KTLS_SYNC_FAIL;\n\t\tgoto out;\n\t}\n\n\t \n\tends_before = before(tcp_seq + datalen - 1, tls_record_start_seq(record));\n\n\tif (unlikely(tls_record_is_start_marker(record))) {\n\t\tret = ends_before ? MLX5E_KTLS_SYNC_SKIP_NO_DATA : MLX5E_KTLS_SYNC_FAIL;\n\t\tgoto out;\n\t} else if (ends_before) {\n\t\tret = MLX5E_KTLS_SYNC_FAIL;\n\t\tgoto out;\n\t}\n\n\tinfo->sync_len = tcp_seq - tls_record_start_seq(record);\n\tremaining = info->sync_len;\n\twhile (remaining > 0) {\n\t\tskb_frag_t *frag = &record->frags[i];\n\n\t\tget_page(skb_frag_page(frag));\n\t\tremaining -= skb_frag_size(frag);\n\t\tinfo->frags[i++] = *frag;\n\t}\n\t \n\tif (remaining < 0)\n\t\tskb_frag_size_add(&info->frags[i - 1], remaining);\n\tinfo->nr_frags = i;\nout:\n\tspin_unlock_irqrestore(&tx_ctx->lock, flags);\n\treturn ret;\n}\n\nstatic void\ntx_post_resync_params(struct mlx5e_txqsq *sq,\n\t\t      struct mlx5e_ktls_offload_context_tx *priv_tx,\n\t\t      u64 rcd_sn)\n{\n\t__be64 rn_be = cpu_to_be64(rcd_sn);\n\tbool skip_static_post;\n\tu16 rec_seq_sz;\n\tchar *rec_seq;\n\n\tswitch (priv_tx->crypto_info.crypto_info.cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *info = &priv_tx->crypto_info.crypto_info_128;\n\n\t\trec_seq = info->rec_seq;\n\t\trec_seq_sz = sizeof(info->rec_seq);\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *info = &priv_tx->crypto_info.crypto_info_256;\n\n\t\trec_seq = info->rec_seq;\n\t\trec_seq_sz = sizeof(info->rec_seq);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tWARN_ONCE(1, \"Unsupported cipher type %u\\n\",\n\t\t\t  priv_tx->crypto_info.crypto_info.cipher_type);\n\t\treturn;\n\t}\n\n\tskip_static_post = !memcmp(rec_seq, &rn_be, rec_seq_sz);\n\tif (!skip_static_post)\n\t\tmemcpy(rec_seq, &rn_be, rec_seq_sz);\n\n\tmlx5e_ktls_tx_post_param_wqes(sq, priv_tx, skip_static_post, true);\n}\n\nstatic int\ntx_post_resync_dump(struct mlx5e_txqsq *sq, skb_frag_t *frag, u32 tisn)\n{\n\tstruct mlx5_wqe_ctrl_seg *cseg;\n\tstruct mlx5_wqe_data_seg *dseg;\n\tstruct mlx5e_dump_wqe *wqe;\n\tdma_addr_t dma_addr = 0;\n\tu16 ds_cnt;\n\tint fsz;\n\tu16 pi;\n\n\tBUILD_BUG_ON(MLX5E_KTLS_DUMP_WQEBBS != 1);\n\tpi = mlx5_wq_cyc_ctr2ix(&sq->wq, sq->pc);\n\twqe = MLX5E_TLS_FETCH_DUMP_WQE(sq, pi);\n\n\tds_cnt = sizeof(*wqe) / MLX5_SEND_WQE_DS;\n\n\tcseg = &wqe->ctrl;\n\tdseg = &wqe->data;\n\n\tcseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8)  | MLX5_OPCODE_DUMP);\n\tcseg->qpn_ds           = cpu_to_be32((sq->sqn << 8) | ds_cnt);\n\tcseg->tis_tir_num      = cpu_to_be32(tisn << 8);\n\n\tfsz = skb_frag_size(frag);\n\tdma_addr = skb_frag_dma_map(sq->pdev, frag, 0, fsz,\n\t\t\t\t    DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(sq->pdev, dma_addr)))\n\t\treturn -ENOMEM;\n\n\tdseg->addr       = cpu_to_be64(dma_addr);\n\tdseg->lkey       = sq->mkey_be;\n\tdseg->byte_count = cpu_to_be32(fsz);\n\tmlx5e_dma_push(sq, dma_addr, fsz, MLX5E_DMA_MAP_PAGE);\n\n\ttx_fill_wi(sq, pi, MLX5E_KTLS_DUMP_WQEBBS, fsz, skb_frag_page(frag));\n\tsq->pc += MLX5E_KTLS_DUMP_WQEBBS;\n\n\treturn 0;\n}\n\nvoid mlx5e_ktls_tx_handle_resync_dump_comp(struct mlx5e_txqsq *sq,\n\t\t\t\t\t   struct mlx5e_tx_wqe_info *wi,\n\t\t\t\t\t   u32 *dma_fifo_cc)\n{\n\tstruct mlx5e_sq_stats *stats;\n\tstruct mlx5e_sq_dma *dma;\n\n\tdma = mlx5e_dma_get(sq, (*dma_fifo_cc)++);\n\tstats = sq->stats;\n\n\tmlx5e_tx_dma_unmap(sq->pdev, dma);\n\tput_page(wi->resync_dump_frag_page);\n\tstats->tls_dump_packets++;\n\tstats->tls_dump_bytes += wi->num_bytes;\n}\n\nstatic enum mlx5e_ktls_sync_retval\nmlx5e_ktls_tx_handle_ooo(struct mlx5e_ktls_offload_context_tx *priv_tx,\n\t\t\t struct mlx5e_txqsq *sq,\n\t\t\t int datalen,\n\t\t\t u32 seq)\n{\n\tenum mlx5e_ktls_sync_retval ret;\n\tstruct tx_sync_info info = {};\n\tint i;\n\n\tret = tx_sync_info_get(priv_tx, seq, datalen, &info);\n\tif (unlikely(ret != MLX5E_KTLS_SYNC_DONE))\n\t\t \n\t\treturn ret;\n\n\ttx_post_resync_params(sq, priv_tx, info.rcd_sn);\n\n\tfor (i = 0; i < info.nr_frags; i++) {\n\t\tunsigned int orig_fsz, frag_offset = 0, n = 0;\n\t\tskb_frag_t *f = &info.frags[i];\n\n\t\torig_fsz = skb_frag_size(f);\n\n\t\tdo {\n\t\t\tunsigned int fsz;\n\n\t\t\tn++;\n\t\t\tfsz = min_t(unsigned int, sq->hw_mtu, orig_fsz - frag_offset);\n\t\t\tskb_frag_size_set(f, fsz);\n\t\t\tif (tx_post_resync_dump(sq, f, priv_tx->tisn)) {\n\t\t\t\tpage_ref_add(skb_frag_page(f), n - 1);\n\t\t\t\tgoto err_out;\n\t\t\t}\n\n\t\t\tskb_frag_off_add(f, fsz);\n\t\t\tfrag_offset += fsz;\n\t\t} while (frag_offset < orig_fsz);\n\n\t\tpage_ref_add(skb_frag_page(f), n - 1);\n\t}\n\n\treturn MLX5E_KTLS_SYNC_DONE;\n\nerr_out:\n\tfor (; i < info.nr_frags; i++)\n\t\t \n\t\tput_page(skb_frag_page(&info.frags[i]));\n\n\treturn MLX5E_KTLS_SYNC_FAIL;\n}\n\nbool mlx5e_ktls_handle_tx_skb(struct net_device *netdev, struct mlx5e_txqsq *sq,\n\t\t\t      struct sk_buff *skb,\n\t\t\t      struct mlx5e_accel_tx_tls_state *state)\n{\n\tstruct mlx5e_ktls_offload_context_tx *priv_tx;\n\tstruct mlx5e_sq_stats *stats = sq->stats;\n\tstruct net_device *tls_netdev;\n\tstruct tls_context *tls_ctx;\n\tint datalen;\n\tu32 seq;\n\n\tdatalen = skb->len - skb_tcp_all_headers(skb);\n\tif (!datalen)\n\t\treturn true;\n\n\tmlx5e_tx_mpwqe_ensure_complete(sq);\n\n\ttls_ctx = tls_get_ctx(skb->sk);\n\ttls_netdev = rcu_dereference_bh(tls_ctx->netdev);\n\t \n\tif (WARN_ON_ONCE(tls_netdev && tls_netdev != netdev))\n\t\tgoto err_out;\n\n\tpriv_tx = mlx5e_get_ktls_tx_priv_ctx(tls_ctx);\n\n\tif (unlikely(mlx5e_ktls_tx_offload_test_and_clear_pending(priv_tx)))\n\t\tmlx5e_ktls_tx_post_param_wqes(sq, priv_tx, false, false);\n\n\tseq = ntohl(tcp_hdr(skb)->seq);\n\tif (unlikely(priv_tx->expected_seq != seq)) {\n\t\tenum mlx5e_ktls_sync_retval ret =\n\t\t\tmlx5e_ktls_tx_handle_ooo(priv_tx, sq, datalen, seq);\n\n\t\tstats->tls_ooo++;\n\n\t\tswitch (ret) {\n\t\tcase MLX5E_KTLS_SYNC_DONE:\n\t\t\tbreak;\n\t\tcase MLX5E_KTLS_SYNC_SKIP_NO_DATA:\n\t\t\tstats->tls_skip_no_sync_data++;\n\t\t\tif (likely(!skb->decrypted))\n\t\t\t\tgoto out;\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tgoto err_out;\n\t\tcase MLX5E_KTLS_SYNC_FAIL:\n\t\t\tstats->tls_drop_no_sync_data++;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tpriv_tx->expected_seq = seq + datalen;\n\n\tstate->tls_tisn = priv_tx->tisn;\n\n\tstats->tls_encrypted_packets += skb_is_gso(skb) ? skb_shinfo(skb)->gso_segs : 1;\n\tstats->tls_encrypted_bytes   += datalen;\n\nout:\n\treturn true;\n\nerr_out:\n\tdev_kfree_skb_any(skb);\n\treturn false;\n}\n\nstatic void mlx5e_tls_tx_debugfs_init(struct mlx5e_tls *tls,\n\t\t\t\t      struct dentry *dfs_root)\n{\n\tif (IS_ERR_OR_NULL(dfs_root))\n\t\treturn;\n\n\ttls->debugfs.dfs_tx = debugfs_create_dir(\"tx\", dfs_root);\n\n\tdebugfs_create_size_t(\"pool_size\", 0400, tls->debugfs.dfs_tx,\n\t\t\t      &tls->tx_pool->size);\n}\n\nint mlx5e_ktls_init_tx(struct mlx5e_priv *priv)\n{\n\tstruct mlx5_crypto_dek_pool *dek_pool;\n\tstruct mlx5e_tls *tls = priv->tls;\n\tint err;\n\n\tif (!mlx5e_is_ktls_device(priv->mdev))\n\t\treturn 0;\n\n\t \n\tdek_pool = mlx5_crypto_dek_pool_create(priv->mdev, MLX5_ACCEL_OBJ_TLS_KEY);\n\tif (IS_ERR(dek_pool))\n\t\treturn PTR_ERR(dek_pool);\n\ttls->dek_pool = dek_pool;\n\n\tif (!mlx5e_is_ktls_tx(priv->mdev))\n\t\treturn 0;\n\n\tpriv->tls->tx_pool = mlx5e_tls_tx_pool_init(priv->mdev, &priv->tls->sw_stats);\n\tif (!priv->tls->tx_pool) {\n\t\terr = -ENOMEM;\n\t\tgoto err_tx_pool_init;\n\t}\n\n\tmlx5e_tls_tx_debugfs_init(tls, tls->debugfs.dfs);\n\n\treturn 0;\n\nerr_tx_pool_init:\n\tmlx5_crypto_dek_pool_destroy(dek_pool);\n\treturn err;\n}\n\nvoid mlx5e_ktls_cleanup_tx(struct mlx5e_priv *priv)\n{\n\tif (!mlx5e_is_ktls_tx(priv->mdev))\n\t\tgoto dek_pool_destroy;\n\n\tdebugfs_remove_recursive(priv->tls->debugfs.dfs_tx);\n\tpriv->tls->debugfs.dfs_tx = NULL;\n\n\tmlx5e_tls_tx_pool_cleanup(priv->tls->tx_pool);\n\tpriv->tls->tx_pool = NULL;\n\ndek_pool_destroy:\n\tif (mlx5e_is_ktls_device(priv->mdev))\n\t\tmlx5_crypto_dek_pool_destroy(priv->tls->dek_pool);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}