{
  "module_name": "en_main.c",
  "hash_id": "535428a00c859921b88d2b50ac47527ebc21a63c15bcc9fd1baed3b8dcd251f7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en_main.c",
  "human_readable_source": " \n\n#include <net/tc_act/tc_gact.h>\n#include <linux/mlx5/fs.h>\n#include <net/vxlan.h>\n#include <net/geneve.h>\n#include <linux/bpf.h>\n#include <linux/debugfs.h>\n#include <linux/if_bridge.h>\n#include <linux/filter.h>\n#include <net/page_pool/types.h>\n#include <net/pkt_sched.h>\n#include <net/xdp_sock_drv.h>\n#include \"eswitch.h\"\n#include \"en.h\"\n#include \"en/txrx.h\"\n#include \"en_tc.h\"\n#include \"en_rep.h\"\n#include \"en_accel/ipsec.h\"\n#include \"en_accel/macsec.h\"\n#include \"en_accel/en_accel.h\"\n#include \"en_accel/ktls.h\"\n#include \"lib/vxlan.h\"\n#include \"lib/clock.h\"\n#include \"en/port.h\"\n#include \"en/xdp.h\"\n#include \"lib/eq.h\"\n#include \"en/monitor_stats.h\"\n#include \"en/health.h\"\n#include \"en/params.h\"\n#include \"en/xsk/pool.h\"\n#include \"en/xsk/setup.h\"\n#include \"en/xsk/rx.h\"\n#include \"en/xsk/tx.h\"\n#include \"en/hv_vhca_stats.h\"\n#include \"en/devlink.h\"\n#include \"lib/mlx5.h\"\n#include \"en/ptp.h\"\n#include \"en/htb.h\"\n#include \"qos.h\"\n#include \"en/trap.h\"\n\nbool mlx5e_check_fragmented_striding_rq_cap(struct mlx5_core_dev *mdev, u8 page_shift,\n\t\t\t\t\t    enum mlx5e_mpwrq_umr_mode umr_mode)\n{\n\tu16 umr_wqebbs, max_wqebbs;\n\tbool striding_rq_umr;\n\n\tstriding_rq_umr = MLX5_CAP_GEN(mdev, striding_rq) && MLX5_CAP_GEN(mdev, umr_ptr_rlky) &&\n\t\t\t  MLX5_CAP_ETH(mdev, reg_umr_sq);\n\tif (!striding_rq_umr)\n\t\treturn false;\n\n\tumr_wqebbs = mlx5e_mpwrq_umr_wqebbs(mdev, page_shift, umr_mode);\n\tmax_wqebbs = mlx5e_get_max_sq_aligned_wqebbs(mdev);\n\t \n\tif (WARN_ON(umr_wqebbs > max_wqebbs))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid mlx5e_update_carrier(struct mlx5e_priv *priv)\n{\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tu8 port_state;\n\tbool up;\n\n\tport_state = mlx5_query_vport_state(mdev,\n\t\t\t\t\t    MLX5_VPORT_STATE_OP_MOD_VNIC_VPORT,\n\t\t\t\t\t    0);\n\n\tup = port_state == VPORT_STATE_UP;\n\tif (up == netif_carrier_ok(priv->netdev))\n\t\tnetif_carrier_event(priv->netdev);\n\tif (up) {\n\t\tnetdev_info(priv->netdev, \"Link up\\n\");\n\t\tnetif_carrier_on(priv->netdev);\n\t} else {\n\t\tnetdev_info(priv->netdev, \"Link down\\n\");\n\t\tnetif_carrier_off(priv->netdev);\n\t}\n}\n\nstatic void mlx5e_update_carrier_work(struct work_struct *work)\n{\n\tstruct mlx5e_priv *priv = container_of(work, struct mlx5e_priv,\n\t\t\t\t\t       update_carrier_work);\n\n\tmutex_lock(&priv->state_lock);\n\tif (test_bit(MLX5E_STATE_OPENED, &priv->state))\n\t\tif (priv->profile->update_carrier)\n\t\t\tpriv->profile->update_carrier(priv);\n\tmutex_unlock(&priv->state_lock);\n}\n\nstatic void mlx5e_update_stats_work(struct work_struct *work)\n{\n\tstruct mlx5e_priv *priv = container_of(work, struct mlx5e_priv,\n\t\t\t\t\t       update_stats_work);\n\n\tmutex_lock(&priv->state_lock);\n\tpriv->profile->update_stats(priv);\n\tmutex_unlock(&priv->state_lock);\n}\n\nvoid mlx5e_queue_update_stats(struct mlx5e_priv *priv)\n{\n\tif (!priv->profile->update_stats)\n\t\treturn;\n\n\tif (unlikely(test_bit(MLX5E_STATE_DESTROYING, &priv->state)))\n\t\treturn;\n\n\tqueue_work(priv->wq, &priv->update_stats_work);\n}\n\nstatic int async_event(struct notifier_block *nb, unsigned long event, void *data)\n{\n\tstruct mlx5e_priv *priv = container_of(nb, struct mlx5e_priv, events_nb);\n\tstruct mlx5_eqe   *eqe = data;\n\n\tif (event != MLX5_EVENT_TYPE_PORT_CHANGE)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (eqe->sub_type) {\n\tcase MLX5_PORT_CHANGE_SUBTYPE_DOWN:\n\tcase MLX5_PORT_CHANGE_SUBTYPE_ACTIVE:\n\t\tqueue_work(priv->wq, &priv->update_carrier_work);\n\t\tbreak;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic void mlx5e_enable_async_events(struct mlx5e_priv *priv)\n{\n\tpriv->events_nb.notifier_call = async_event;\n\tmlx5_notifier_register(priv->mdev, &priv->events_nb);\n}\n\nstatic void mlx5e_disable_async_events(struct mlx5e_priv *priv)\n{\n\tmlx5_notifier_unregister(priv->mdev, &priv->events_nb);\n}\n\nstatic int blocking_event(struct notifier_block *nb, unsigned long event, void *data)\n{\n\tstruct mlx5e_priv *priv = container_of(nb, struct mlx5e_priv, blocking_events_nb);\n\tstruct mlx5_devlink_trap_event_ctx *trap_event_ctx = data;\n\tint err;\n\n\tswitch (event) {\n\tcase MLX5_DRIVER_EVENT_TYPE_TRAP:\n\t\terr = mlx5e_handle_trap_event(priv, trap_event_ctx->trap);\n\t\tif (err) {\n\t\t\ttrap_event_ctx->err = err;\n\t\t\treturn NOTIFY_BAD;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic void mlx5e_enable_blocking_events(struct mlx5e_priv *priv)\n{\n\tpriv->blocking_events_nb.notifier_call = blocking_event;\n\tmlx5_blocking_notifier_register(priv->mdev, &priv->blocking_events_nb);\n}\n\nstatic void mlx5e_disable_blocking_events(struct mlx5e_priv *priv)\n{\n\tmlx5_blocking_notifier_unregister(priv->mdev, &priv->blocking_events_nb);\n}\n\nstatic u16 mlx5e_mpwrq_umr_octowords(u32 entries, enum mlx5e_mpwrq_umr_mode umr_mode)\n{\n\tu8 umr_entry_size = mlx5e_mpwrq_umr_entry_size(umr_mode);\n\tu32 sz;\n\n\tsz = ALIGN(entries * umr_entry_size, MLX5_UMR_FLEX_ALIGNMENT);\n\n\treturn sz / MLX5_OCTWORD;\n}\n\nstatic inline void mlx5e_build_umr_wqe(struct mlx5e_rq *rq,\n\t\t\t\t       struct mlx5e_icosq *sq,\n\t\t\t\t       struct mlx5e_umr_wqe *wqe)\n{\n\tstruct mlx5_wqe_ctrl_seg      *cseg = &wqe->ctrl;\n\tstruct mlx5_wqe_umr_ctrl_seg *ucseg = &wqe->uctrl;\n\tu16 octowords;\n\tu8 ds_cnt;\n\n\tds_cnt = DIV_ROUND_UP(mlx5e_mpwrq_umr_wqe_sz(rq->mdev, rq->mpwqe.page_shift,\n\t\t\t\t\t\t     rq->mpwqe.umr_mode),\n\t\t\t      MLX5_SEND_WQE_DS);\n\n\tcseg->qpn_ds    = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT) |\n\t\t\t\t      ds_cnt);\n\tcseg->umr_mkey  = rq->mpwqe.umr_mkey_be;\n\n\tucseg->flags = MLX5_UMR_TRANSLATION_OFFSET_EN | MLX5_UMR_INLINE;\n\toctowords = mlx5e_mpwrq_umr_octowords(rq->mpwqe.pages_per_wqe, rq->mpwqe.umr_mode);\n\tucseg->xlt_octowords = cpu_to_be16(octowords);\n\tucseg->mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);\n}\n\nstatic int mlx5e_rq_shampo_hd_alloc(struct mlx5e_rq *rq, int node)\n{\n\trq->mpwqe.shampo = kvzalloc_node(sizeof(*rq->mpwqe.shampo),\n\t\t\t\t\t GFP_KERNEL, node);\n\tif (!rq->mpwqe.shampo)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void mlx5e_rq_shampo_hd_free(struct mlx5e_rq *rq)\n{\n\tkvfree(rq->mpwqe.shampo);\n}\n\nstatic int mlx5e_rq_shampo_hd_info_alloc(struct mlx5e_rq *rq, int node)\n{\n\tstruct mlx5e_shampo_hd *shampo = rq->mpwqe.shampo;\n\n\tshampo->bitmap = bitmap_zalloc_node(shampo->hd_per_wq, GFP_KERNEL,\n\t\t\t\t\t    node);\n\tshampo->info = kvzalloc_node(array_size(shampo->hd_per_wq,\n\t\t\t\t\t\tsizeof(*shampo->info)),\n\t\t\t\t     GFP_KERNEL, node);\n\tshampo->pages = kvzalloc_node(array_size(shampo->hd_per_wq,\n\t\t\t\t\t\t sizeof(*shampo->pages)),\n\t\t\t\t     GFP_KERNEL, node);\n\tif (!shampo->bitmap || !shampo->info || !shampo->pages)\n\t\tgoto err_nomem;\n\n\treturn 0;\n\nerr_nomem:\n\tkvfree(shampo->info);\n\tkvfree(shampo->bitmap);\n\tkvfree(shampo->pages);\n\n\treturn -ENOMEM;\n}\n\nstatic void mlx5e_rq_shampo_hd_info_free(struct mlx5e_rq *rq)\n{\n\tkvfree(rq->mpwqe.shampo->bitmap);\n\tkvfree(rq->mpwqe.shampo->info);\n\tkvfree(rq->mpwqe.shampo->pages);\n}\n\nstatic int mlx5e_rq_alloc_mpwqe_info(struct mlx5e_rq *rq, int node)\n{\n\tint wq_sz = mlx5_wq_ll_get_size(&rq->mpwqe.wq);\n\tsize_t alloc_size;\n\n\talloc_size = array_size(wq_sz, struct_size(rq->mpwqe.info,\n\t\t\t\t\t\t   alloc_units.frag_pages,\n\t\t\t\t\t\t   rq->mpwqe.pages_per_wqe));\n\n\trq->mpwqe.info = kvzalloc_node(alloc_size, GFP_KERNEL, node);\n\tif (!rq->mpwqe.info)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (int i = 0; i < wq_sz; i++) {\n\t\tstruct mlx5e_mpw_info *wi = mlx5e_get_mpw_info(rq, i);\n\n\t\tbitmap_fill(wi->skip_release_bitmap, rq->mpwqe.pages_per_wqe);\n\t}\n\n\tmlx5e_build_umr_wqe(rq, rq->icosq, &rq->mpwqe.umr_wqe);\n\n\treturn 0;\n}\n\n\nstatic u8 mlx5e_mpwrq_access_mode(enum mlx5e_mpwrq_umr_mode umr_mode)\n{\n\tswitch (umr_mode) {\n\tcase MLX5E_MPWRQ_UMR_MODE_ALIGNED:\n\t\treturn MLX5_MKC_ACCESS_MODE_MTT;\n\tcase MLX5E_MPWRQ_UMR_MODE_UNALIGNED:\n\t\treturn MLX5_MKC_ACCESS_MODE_KSM;\n\tcase MLX5E_MPWRQ_UMR_MODE_OVERSIZED:\n\t\treturn MLX5_MKC_ACCESS_MODE_KLMS;\n\tcase MLX5E_MPWRQ_UMR_MODE_TRIPLE:\n\t\treturn MLX5_MKC_ACCESS_MODE_KSM;\n\t}\n\tWARN_ONCE(1, \"MPWRQ UMR mode %d is not known\\n\", umr_mode);\n\treturn 0;\n}\n\nstatic int mlx5e_create_umr_mkey(struct mlx5_core_dev *mdev,\n\t\t\t\t u32 npages, u8 page_shift, u32 *umr_mkey,\n\t\t\t\t dma_addr_t filler_addr,\n\t\t\t\t enum mlx5e_mpwrq_umr_mode umr_mode,\n\t\t\t\t u32 xsk_chunk_size)\n{\n\tstruct mlx5_mtt *mtt;\n\tstruct mlx5_ksm *ksm;\n\tstruct mlx5_klm *klm;\n\tu32 octwords;\n\tint inlen;\n\tvoid *mkc;\n\tu32 *in;\n\tint err;\n\tint i;\n\n\tif ((umr_mode == MLX5E_MPWRQ_UMR_MODE_UNALIGNED ||\n\t     umr_mode == MLX5E_MPWRQ_UMR_MODE_TRIPLE) &&\n\t    !MLX5_CAP_GEN(mdev, fixed_buffer_size)) {\n\t\tmlx5_core_warn(mdev, \"Unaligned AF_XDP requires fixed_buffer_size capability\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\toctwords = mlx5e_mpwrq_umr_octowords(npages, umr_mode);\n\n\tinlen = MLX5_FLEXIBLE_INLEN(mdev, MLX5_ST_SZ_BYTES(create_mkey_in),\n\t\t\t\t    MLX5_OCTWORD, octwords);\n\tif (inlen < 0)\n\t\treturn inlen;\n\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\n\tMLX5_SET(mkc, mkc, free, 1);\n\tMLX5_SET(mkc, mkc, umr_en, 1);\n\tMLX5_SET(mkc, mkc, lw, 1);\n\tMLX5_SET(mkc, mkc, lr, 1);\n\tMLX5_SET(mkc, mkc, access_mode_1_0, mlx5e_mpwrq_access_mode(umr_mode));\n\tmlx5e_mkey_set_relaxed_ordering(mdev, mkc);\n\tMLX5_SET(mkc, mkc, qpn, 0xffffff);\n\tMLX5_SET(mkc, mkc, pd, mdev->mlx5e_res.hw_objs.pdn);\n\tMLX5_SET64(mkc, mkc, len, npages << page_shift);\n\tMLX5_SET(mkc, mkc, translations_octword_size, octwords);\n\tif (umr_mode == MLX5E_MPWRQ_UMR_MODE_TRIPLE)\n\t\tMLX5_SET(mkc, mkc, log_page_size, page_shift - 2);\n\telse if (umr_mode != MLX5E_MPWRQ_UMR_MODE_OVERSIZED)\n\t\tMLX5_SET(mkc, mkc, log_page_size, page_shift);\n\tMLX5_SET(create_mkey_in, in, translations_octword_actual_size, octwords);\n\n\t \n\tswitch (umr_mode) {\n\tcase MLX5E_MPWRQ_UMR_MODE_OVERSIZED:\n\t\tklm = MLX5_ADDR_OF(create_mkey_in, in, klm_pas_mtt);\n\t\tfor (i = 0; i < npages; i++) {\n\t\t\tklm[i << 1] = (struct mlx5_klm) {\n\t\t\t\t.va = cpu_to_be64(filler_addr),\n\t\t\t\t.bcount = cpu_to_be32(xsk_chunk_size),\n\t\t\t\t.key = cpu_to_be32(mdev->mlx5e_res.hw_objs.mkey),\n\t\t\t};\n\t\t\tklm[(i << 1) + 1] = (struct mlx5_klm) {\n\t\t\t\t.va = cpu_to_be64(filler_addr),\n\t\t\t\t.bcount = cpu_to_be32((1 << page_shift) - xsk_chunk_size),\n\t\t\t\t.key = cpu_to_be32(mdev->mlx5e_res.hw_objs.mkey),\n\t\t\t};\n\t\t}\n\t\tbreak;\n\tcase MLX5E_MPWRQ_UMR_MODE_UNALIGNED:\n\t\tksm = MLX5_ADDR_OF(create_mkey_in, in, klm_pas_mtt);\n\t\tfor (i = 0; i < npages; i++)\n\t\t\tksm[i] = (struct mlx5_ksm) {\n\t\t\t\t.key = cpu_to_be32(mdev->mlx5e_res.hw_objs.mkey),\n\t\t\t\t.va = cpu_to_be64(filler_addr),\n\t\t\t};\n\t\tbreak;\n\tcase MLX5E_MPWRQ_UMR_MODE_ALIGNED:\n\t\tmtt = MLX5_ADDR_OF(create_mkey_in, in, klm_pas_mtt);\n\t\tfor (i = 0; i < npages; i++)\n\t\t\tmtt[i] = (struct mlx5_mtt) {\n\t\t\t\t.ptag = cpu_to_be64(filler_addr),\n\t\t\t};\n\t\tbreak;\n\tcase MLX5E_MPWRQ_UMR_MODE_TRIPLE:\n\t\tksm = MLX5_ADDR_OF(create_mkey_in, in, klm_pas_mtt);\n\t\tfor (i = 0; i < npages * 4; i++) {\n\t\t\tksm[i] = (struct mlx5_ksm) {\n\t\t\t\t.key = cpu_to_be32(mdev->mlx5e_res.hw_objs.mkey),\n\t\t\t\t.va = cpu_to_be64(filler_addr),\n\t\t\t};\n\t\t}\n\t\tbreak;\n\t}\n\n\terr = mlx5_core_create_mkey(mdev, umr_mkey, in, inlen);\n\n\tkvfree(in);\n\treturn err;\n}\n\nstatic int mlx5e_create_umr_klm_mkey(struct mlx5_core_dev *mdev,\n\t\t\t\t     u64 nentries,\n\t\t\t\t     u32 *umr_mkey)\n{\n\tint inlen;\n\tvoid *mkc;\n\tu32 *in;\n\tint err;\n\n\tinlen = MLX5_ST_SZ_BYTES(create_mkey_in);\n\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\n\tMLX5_SET(mkc, mkc, free, 1);\n\tMLX5_SET(mkc, mkc, umr_en, 1);\n\tMLX5_SET(mkc, mkc, lw, 1);\n\tMLX5_SET(mkc, mkc, lr, 1);\n\tMLX5_SET(mkc, mkc, access_mode_1_0, MLX5_MKC_ACCESS_MODE_KLMS);\n\tmlx5e_mkey_set_relaxed_ordering(mdev, mkc);\n\tMLX5_SET(mkc, mkc, qpn, 0xffffff);\n\tMLX5_SET(mkc, mkc, pd, mdev->mlx5e_res.hw_objs.pdn);\n\tMLX5_SET(mkc, mkc, translations_octword_size, nentries);\n\tMLX5_SET(mkc, mkc, length64, 1);\n\terr = mlx5_core_create_mkey(mdev, umr_mkey, in, inlen);\n\n\tkvfree(in);\n\treturn err;\n}\n\nstatic int mlx5e_create_rq_umr_mkey(struct mlx5_core_dev *mdev, struct mlx5e_rq *rq)\n{\n\tu32 xsk_chunk_size = rq->xsk_pool ? rq->xsk_pool->chunk_size : 0;\n\tu32 wq_size = mlx5_wq_ll_get_size(&rq->mpwqe.wq);\n\tu32 num_entries, max_num_entries;\n\tu32 umr_mkey;\n\tint err;\n\n\tmax_num_entries = mlx5e_mpwrq_max_num_entries(mdev, rq->mpwqe.umr_mode);\n\n\t \n\tif (WARN_ON_ONCE(check_mul_overflow(wq_size, (u32)rq->mpwqe.mtts_per_wqe,\n\t\t\t\t\t    &num_entries) ||\n\t\t\t num_entries > max_num_entries))\n\t\tmlx5_core_err(mdev, \"%s: multiplication overflow: %u * %u > %u\\n\",\n\t\t\t      __func__, wq_size, rq->mpwqe.mtts_per_wqe,\n\t\t\t      max_num_entries);\n\n\terr = mlx5e_create_umr_mkey(mdev, num_entries, rq->mpwqe.page_shift,\n\t\t\t\t    &umr_mkey, rq->wqe_overflow.addr,\n\t\t\t\t    rq->mpwqe.umr_mode, xsk_chunk_size);\n\trq->mpwqe.umr_mkey_be = cpu_to_be32(umr_mkey);\n\treturn err;\n}\n\nstatic int mlx5e_create_rq_hd_umr_mkey(struct mlx5_core_dev *mdev,\n\t\t\t\t       struct mlx5e_rq *rq)\n{\n\tu32 max_klm_size = BIT(MLX5_CAP_GEN(mdev, log_max_klm_list_size));\n\n\tif (max_klm_size < rq->mpwqe.shampo->hd_per_wq) {\n\t\tmlx5_core_err(mdev, \"max klm list size 0x%x is smaller than shampo header buffer list size 0x%x\\n\",\n\t\t\t      max_klm_size, rq->mpwqe.shampo->hd_per_wq);\n\t\treturn -EINVAL;\n\t}\n\treturn mlx5e_create_umr_klm_mkey(mdev, rq->mpwqe.shampo->hd_per_wq,\n\t\t\t\t\t &rq->mpwqe.shampo->mkey);\n}\n\nstatic void mlx5e_init_frags_partition(struct mlx5e_rq *rq)\n{\n\tstruct mlx5e_wqe_frag_info next_frag = {};\n\tstruct mlx5e_wqe_frag_info *prev = NULL;\n\tint i;\n\n\tWARN_ON(rq->xsk_pool);\n\n\tnext_frag.frag_page = &rq->wqe.alloc_units->frag_pages[0];\n\n\t \n\tnext_frag.flags = BIT(MLX5E_WQE_FRAG_SKIP_RELEASE);\n\n\tfor (i = 0; i < mlx5_wq_cyc_get_size(&rq->wqe.wq); i++) {\n\t\tstruct mlx5e_rq_frag_info *frag_info = &rq->wqe.info.arr[0];\n\t\tstruct mlx5e_wqe_frag_info *frag =\n\t\t\t&rq->wqe.frags[i << rq->wqe.info.log_num_frags];\n\t\tint f;\n\n\t\tfor (f = 0; f < rq->wqe.info.num_frags; f++, frag++) {\n\t\t\tif (next_frag.offset + frag_info[f].frag_stride > PAGE_SIZE) {\n\t\t\t\t \n\t\t\t\tnext_frag.frag_page++;\n\t\t\t\tnext_frag.offset = 0;\n\t\t\t\tif (prev)\n\t\t\t\t\tprev->flags |= BIT(MLX5E_WQE_FRAG_LAST_IN_PAGE);\n\t\t\t}\n\t\t\t*frag = next_frag;\n\n\t\t\t \n\t\t\tnext_frag.offset += frag_info[f].frag_stride;\n\t\t\tprev = frag;\n\t\t}\n\t}\n\n\tif (prev)\n\t\tprev->flags |= BIT(MLX5E_WQE_FRAG_LAST_IN_PAGE);\n}\n\nstatic void mlx5e_init_xsk_buffs(struct mlx5e_rq *rq)\n{\n\tint i;\n\n\t \n\tWARN_ON(rq->wqe.info.num_frags != 1);\n\tWARN_ON(rq->wqe.info.log_num_frags != 0);\n\tWARN_ON(rq->wqe.info.arr[0].frag_stride != PAGE_SIZE);\n\n\t \n\tfor (i = 0; i < mlx5_wq_cyc_get_size(&rq->wqe.wq); i++) {\n\t\trq->wqe.frags[i].xskp = &rq->wqe.alloc_units->xsk_buffs[i];\n\n\t\t \n\t\trq->wqe.frags[i].flags |= BIT(MLX5E_WQE_FRAG_SKIP_RELEASE);\n\t}\n}\n\nstatic int mlx5e_init_wqe_alloc_info(struct mlx5e_rq *rq, int node)\n{\n\tint wq_sz = mlx5_wq_cyc_get_size(&rq->wqe.wq);\n\tint len = wq_sz << rq->wqe.info.log_num_frags;\n\tstruct mlx5e_wqe_frag_info *frags;\n\tunion mlx5e_alloc_units *aus;\n\tint aus_sz;\n\n\tif (rq->xsk_pool)\n\t\taus_sz = sizeof(*aus->xsk_buffs);\n\telse\n\t\taus_sz = sizeof(*aus->frag_pages);\n\n\taus = kvzalloc_node(array_size(len, aus_sz), GFP_KERNEL, node);\n\tif (!aus)\n\t\treturn -ENOMEM;\n\n\tfrags = kvzalloc_node(array_size(len, sizeof(*frags)), GFP_KERNEL, node);\n\tif (!frags) {\n\t\tkvfree(aus);\n\t\treturn -ENOMEM;\n\t}\n\n\trq->wqe.alloc_units = aus;\n\trq->wqe.frags = frags;\n\n\tif (rq->xsk_pool)\n\t\tmlx5e_init_xsk_buffs(rq);\n\telse\n\t\tmlx5e_init_frags_partition(rq);\n\n\treturn 0;\n}\n\nstatic void mlx5e_free_wqe_alloc_info(struct mlx5e_rq *rq)\n{\n\tkvfree(rq->wqe.frags);\n\tkvfree(rq->wqe.alloc_units);\n}\n\nstatic void mlx5e_rq_err_cqe_work(struct work_struct *recover_work)\n{\n\tstruct mlx5e_rq *rq = container_of(recover_work, struct mlx5e_rq, recover_work);\n\n\tmlx5e_reporter_rq_cqe_err(rq);\n}\n\nstatic int mlx5e_alloc_mpwqe_rq_drop_page(struct mlx5e_rq *rq)\n{\n\trq->wqe_overflow.page = alloc_page(GFP_KERNEL);\n\tif (!rq->wqe_overflow.page)\n\t\treturn -ENOMEM;\n\n\trq->wqe_overflow.addr = dma_map_page(rq->pdev, rq->wqe_overflow.page, 0,\n\t\t\t\t\t     PAGE_SIZE, rq->buff.map_dir);\n\tif (dma_mapping_error(rq->pdev, rq->wqe_overflow.addr)) {\n\t\t__free_page(rq->wqe_overflow.page);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void mlx5e_free_mpwqe_rq_drop_page(struct mlx5e_rq *rq)\n{\n\t dma_unmap_page(rq->pdev, rq->wqe_overflow.addr, PAGE_SIZE,\n\t\t\trq->buff.map_dir);\n\t __free_page(rq->wqe_overflow.page);\n}\n\nstatic int mlx5e_init_rxq_rq(struct mlx5e_channel *c, struct mlx5e_params *params,\n\t\t\t     u32 xdp_frag_size, struct mlx5e_rq *rq)\n{\n\tstruct mlx5_core_dev *mdev = c->mdev;\n\tint err;\n\n\trq->wq_type      = params->rq_wq_type;\n\trq->pdev         = c->pdev;\n\trq->netdev       = c->netdev;\n\trq->priv         = c->priv;\n\trq->tstamp       = c->tstamp;\n\trq->clock        = &mdev->clock;\n\trq->icosq        = &c->icosq;\n\trq->ix           = c->ix;\n\trq->channel      = c;\n\trq->mdev         = mdev;\n\trq->hw_mtu =\n\t\tMLX5E_SW2HW_MTU(params, params->sw_mtu) - ETH_FCS_LEN * !params->scatter_fcs_en;\n\trq->xdpsq        = &c->rq_xdpsq;\n\trq->stats        = &c->priv->channel_stats[c->ix]->rq;\n\trq->ptp_cyc2time = mlx5_rq_ts_translator(mdev);\n\terr = mlx5e_rq_set_handlers(rq, params, NULL);\n\tif (err)\n\t\treturn err;\n\n\treturn __xdp_rxq_info_reg(&rq->xdp_rxq, rq->netdev, rq->ix, c->napi.napi_id,\n\t\t\t\t  xdp_frag_size);\n}\n\nstatic int mlx5_rq_shampo_alloc(struct mlx5_core_dev *mdev,\n\t\t\t\tstruct mlx5e_params *params,\n\t\t\t\tstruct mlx5e_rq_param *rqp,\n\t\t\t\tstruct mlx5e_rq *rq,\n\t\t\t\tu32 *pool_size,\n\t\t\t\tint node)\n{\n\tvoid *wqc = MLX5_ADDR_OF(rqc, rqp->rqc, wq);\n\tint wq_size;\n\tint err;\n\n\tif (!test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state))\n\t\treturn 0;\n\terr = mlx5e_rq_shampo_hd_alloc(rq, node);\n\tif (err)\n\t\tgoto out;\n\trq->mpwqe.shampo->hd_per_wq =\n\t\tmlx5e_shampo_hd_per_wq(mdev, params, rqp);\n\terr = mlx5e_create_rq_hd_umr_mkey(mdev, rq);\n\tif (err)\n\t\tgoto err_shampo_hd;\n\terr = mlx5e_rq_shampo_hd_info_alloc(rq, node);\n\tif (err)\n\t\tgoto err_shampo_info;\n\trq->hw_gro_data = kvzalloc_node(sizeof(*rq->hw_gro_data), GFP_KERNEL, node);\n\tif (!rq->hw_gro_data) {\n\t\terr = -ENOMEM;\n\t\tgoto err_hw_gro_data;\n\t}\n\trq->mpwqe.shampo->key =\n\t\tcpu_to_be32(rq->mpwqe.shampo->mkey);\n\trq->mpwqe.shampo->hd_per_wqe =\n\t\tmlx5e_shampo_hd_per_wqe(mdev, params, rqp);\n\twq_size = BIT(MLX5_GET(wq, wqc, log_wq_sz));\n\t*pool_size += (rq->mpwqe.shampo->hd_per_wqe * wq_size) /\n\t\t     MLX5E_SHAMPO_WQ_HEADER_PER_PAGE;\n\treturn 0;\n\nerr_hw_gro_data:\n\tmlx5e_rq_shampo_hd_info_free(rq);\nerr_shampo_info:\n\tmlx5_core_destroy_mkey(mdev, rq->mpwqe.shampo->mkey);\nerr_shampo_hd:\n\tmlx5e_rq_shampo_hd_free(rq);\nout:\n\treturn err;\n}\n\nstatic void mlx5e_rq_free_shampo(struct mlx5e_rq *rq)\n{\n\tif (!test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state))\n\t\treturn;\n\n\tkvfree(rq->hw_gro_data);\n\tmlx5e_rq_shampo_hd_info_free(rq);\n\tmlx5_core_destroy_mkey(rq->mdev, rq->mpwqe.shampo->mkey);\n\tmlx5e_rq_shampo_hd_free(rq);\n}\n\nstatic int mlx5e_alloc_rq(struct mlx5e_params *params,\n\t\t\t  struct mlx5e_xsk_param *xsk,\n\t\t\t  struct mlx5e_rq_param *rqp,\n\t\t\t  int node, struct mlx5e_rq *rq)\n{\n\tstruct mlx5_core_dev *mdev = rq->mdev;\n\tvoid *rqc = rqp->rqc;\n\tvoid *rqc_wq = MLX5_ADDR_OF(rqc, rqc, wq);\n\tu32 pool_size;\n\tint wq_sz;\n\tint err;\n\tint i;\n\n\trqp->wq.db_numa_node = node;\n\tINIT_WORK(&rq->recover_work, mlx5e_rq_err_cqe_work);\n\n\tif (params->xdp_prog)\n\t\tbpf_prog_inc(params->xdp_prog);\n\tRCU_INIT_POINTER(rq->xdp_prog, params->xdp_prog);\n\n\trq->buff.map_dir = params->xdp_prog ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;\n\trq->buff.headroom = mlx5e_get_rq_headroom(mdev, params, xsk);\n\tpool_size = 1 << params->log_rq_mtu_frames;\n\n\trq->mkey_be = cpu_to_be32(mdev->mlx5e_res.hw_objs.mkey);\n\n\tswitch (rq->wq_type) {\n\tcase MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:\n\t\terr = mlx5_wq_ll_create(mdev, &rqp->wq, rqc_wq, &rq->mpwqe.wq,\n\t\t\t\t\t&rq->wq_ctrl);\n\t\tif (err)\n\t\t\tgoto err_rq_xdp_prog;\n\n\t\terr = mlx5e_alloc_mpwqe_rq_drop_page(rq);\n\t\tif (err)\n\t\t\tgoto err_rq_wq_destroy;\n\n\t\trq->mpwqe.wq.db = &rq->mpwqe.wq.db[MLX5_RCV_DBR];\n\n\t\twq_sz = mlx5_wq_ll_get_size(&rq->mpwqe.wq);\n\n\t\trq->mpwqe.page_shift = mlx5e_mpwrq_page_shift(mdev, xsk);\n\t\trq->mpwqe.umr_mode = mlx5e_mpwrq_umr_mode(mdev, xsk);\n\t\trq->mpwqe.pages_per_wqe =\n\t\t\tmlx5e_mpwrq_pages_per_wqe(mdev, rq->mpwqe.page_shift,\n\t\t\t\t\t\t  rq->mpwqe.umr_mode);\n\t\trq->mpwqe.umr_wqebbs =\n\t\t\tmlx5e_mpwrq_umr_wqebbs(mdev, rq->mpwqe.page_shift,\n\t\t\t\t\t       rq->mpwqe.umr_mode);\n\t\trq->mpwqe.mtts_per_wqe =\n\t\t\tmlx5e_mpwrq_mtts_per_wqe(mdev, rq->mpwqe.page_shift,\n\t\t\t\t\t\t rq->mpwqe.umr_mode);\n\n\t\tpool_size = rq->mpwqe.pages_per_wqe <<\n\t\t\tmlx5e_mpwqe_get_log_rq_size(mdev, params, xsk);\n\n\t\tif (!mlx5e_rx_mpwqe_is_linear_skb(mdev, params, xsk) && params->xdp_prog)\n\t\t\tpool_size *= 2;  \n\n\t\trq->mpwqe.log_stride_sz = mlx5e_mpwqe_get_log_stride_size(mdev, params, xsk);\n\t\trq->mpwqe.num_strides =\n\t\t\tBIT(mlx5e_mpwqe_get_log_num_strides(mdev, params, xsk));\n\t\trq->mpwqe.min_wqe_bulk = mlx5e_mpwqe_get_min_wqe_bulk(wq_sz);\n\n\t\trq->buff.frame0_sz = (1 << rq->mpwqe.log_stride_sz);\n\n\t\terr = mlx5e_create_rq_umr_mkey(mdev, rq);\n\t\tif (err)\n\t\t\tgoto err_rq_drop_page;\n\n\t\terr = mlx5e_rq_alloc_mpwqe_info(rq, node);\n\t\tif (err)\n\t\t\tgoto err_rq_mkey;\n\n\t\terr = mlx5_rq_shampo_alloc(mdev, params, rqp, rq, &pool_size, node);\n\t\tif (err)\n\t\t\tgoto err_free_mpwqe_info;\n\n\t\tbreak;\n\tdefault:  \n\t\terr = mlx5_wq_cyc_create(mdev, &rqp->wq, rqc_wq, &rq->wqe.wq,\n\t\t\t\t\t &rq->wq_ctrl);\n\t\tif (err)\n\t\t\tgoto err_rq_xdp_prog;\n\n\t\trq->wqe.wq.db = &rq->wqe.wq.db[MLX5_RCV_DBR];\n\n\t\twq_sz = mlx5_wq_cyc_get_size(&rq->wqe.wq);\n\n\t\trq->wqe.info = rqp->frags_info;\n\t\trq->buff.frame0_sz = rq->wqe.info.arr[0].frag_stride;\n\n\t\terr = mlx5e_init_wqe_alloc_info(rq, node);\n\t\tif (err)\n\t\t\tgoto err_rq_wq_destroy;\n\t}\n\n\tif (xsk) {\n\t\terr = xdp_rxq_info_reg_mem_model(&rq->xdp_rxq,\n\t\t\t\t\t\t MEM_TYPE_XSK_BUFF_POOL, NULL);\n\t\txsk_pool_set_rxq_info(rq->xsk_pool, &rq->xdp_rxq);\n\t} else {\n\t\t \n\t\tstruct page_pool_params pp_params = { 0 };\n\n\t\tpp_params.order     = 0;\n\t\tpp_params.flags     = PP_FLAG_DMA_MAP | PP_FLAG_DMA_SYNC_DEV | PP_FLAG_PAGE_FRAG;\n\t\tpp_params.pool_size = pool_size;\n\t\tpp_params.nid       = node;\n\t\tpp_params.dev       = rq->pdev;\n\t\tpp_params.napi      = rq->cq.napi;\n\t\tpp_params.dma_dir   = rq->buff.map_dir;\n\t\tpp_params.max_len   = PAGE_SIZE;\n\n\t\t \n\t\trq->page_pool = page_pool_create(&pp_params);\n\t\tif (IS_ERR(rq->page_pool)) {\n\t\t\terr = PTR_ERR(rq->page_pool);\n\t\t\trq->page_pool = NULL;\n\t\t\tgoto err_free_by_rq_type;\n\t\t}\n\t\tif (xdp_rxq_info_is_reg(&rq->xdp_rxq))\n\t\t\terr = xdp_rxq_info_reg_mem_model(&rq->xdp_rxq,\n\t\t\t\t\t\t\t MEM_TYPE_PAGE_POOL, rq->page_pool);\n\t}\n\tif (err)\n\t\tgoto err_destroy_page_pool;\n\n\tfor (i = 0; i < wq_sz; i++) {\n\t\tif (rq->wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ) {\n\t\t\tstruct mlx5e_rx_wqe_ll *wqe =\n\t\t\t\tmlx5_wq_ll_get_wqe(&rq->mpwqe.wq, i);\n\t\t\tu32 byte_count =\n\t\t\t\trq->mpwqe.num_strides << rq->mpwqe.log_stride_sz;\n\t\t\tu64 dma_offset = mul_u32_u32(i, rq->mpwqe.mtts_per_wqe) <<\n\t\t\t\trq->mpwqe.page_shift;\n\t\t\tu16 headroom = test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state) ?\n\t\t\t\t       0 : rq->buff.headroom;\n\n\t\t\twqe->data[0].addr = cpu_to_be64(dma_offset + headroom);\n\t\t\twqe->data[0].byte_count = cpu_to_be32(byte_count);\n\t\t\twqe->data[0].lkey = rq->mpwqe.umr_mkey_be;\n\t\t} else {\n\t\t\tstruct mlx5e_rx_wqe_cyc *wqe =\n\t\t\t\tmlx5_wq_cyc_get_wqe(&rq->wqe.wq, i);\n\t\t\tint f;\n\n\t\t\tfor (f = 0; f < rq->wqe.info.num_frags; f++) {\n\t\t\t\tu32 frag_size = rq->wqe.info.arr[f].frag_size |\n\t\t\t\t\tMLX5_HW_START_PADDING;\n\n\t\t\t\twqe->data[f].byte_count = cpu_to_be32(frag_size);\n\t\t\t\twqe->data[f].lkey = rq->mkey_be;\n\t\t\t}\n\t\t\t \n\t\t\tif (rq->wqe.info.num_frags < (1 << rq->wqe.info.log_num_frags)) {\n\t\t\t\twqe->data[f].byte_count = 0;\n\t\t\t\twqe->data[f].lkey = params->terminate_lkey_be;\n\t\t\t\twqe->data[f].addr = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tINIT_WORK(&rq->dim.work, mlx5e_rx_dim_work);\n\n\tswitch (params->rx_cq_moderation.cq_period_mode) {\n\tcase MLX5_CQ_PERIOD_MODE_START_FROM_CQE:\n\t\trq->dim.mode = DIM_CQ_PERIOD_MODE_START_FROM_CQE;\n\t\tbreak;\n\tcase MLX5_CQ_PERIOD_MODE_START_FROM_EQE:\n\tdefault:\n\t\trq->dim.mode = DIM_CQ_PERIOD_MODE_START_FROM_EQE;\n\t}\n\n\treturn 0;\n\nerr_destroy_page_pool:\n\tpage_pool_destroy(rq->page_pool);\nerr_free_by_rq_type:\n\tswitch (rq->wq_type) {\n\tcase MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:\n\t\tmlx5e_rq_free_shampo(rq);\nerr_free_mpwqe_info:\n\t\tkvfree(rq->mpwqe.info);\nerr_rq_mkey:\n\t\tmlx5_core_destroy_mkey(mdev, be32_to_cpu(rq->mpwqe.umr_mkey_be));\nerr_rq_drop_page:\n\t\tmlx5e_free_mpwqe_rq_drop_page(rq);\n\t\tbreak;\n\tdefault:  \n\t\tmlx5e_free_wqe_alloc_info(rq);\n\t}\nerr_rq_wq_destroy:\n\tmlx5_wq_destroy(&rq->wq_ctrl);\nerr_rq_xdp_prog:\n\tif (params->xdp_prog)\n\t\tbpf_prog_put(params->xdp_prog);\n\n\treturn err;\n}\n\nstatic void mlx5e_free_rq(struct mlx5e_rq *rq)\n{\n\tstruct bpf_prog *old_prog;\n\n\tif (xdp_rxq_info_is_reg(&rq->xdp_rxq)) {\n\t\told_prog = rcu_dereference_protected(rq->xdp_prog,\n\t\t\t\t\t\t     lockdep_is_held(&rq->priv->state_lock));\n\t\tif (old_prog)\n\t\t\tbpf_prog_put(old_prog);\n\t}\n\n\tswitch (rq->wq_type) {\n\tcase MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:\n\t\tkvfree(rq->mpwqe.info);\n\t\tmlx5_core_destroy_mkey(rq->mdev, be32_to_cpu(rq->mpwqe.umr_mkey_be));\n\t\tmlx5e_free_mpwqe_rq_drop_page(rq);\n\t\tmlx5e_rq_free_shampo(rq);\n\t\tbreak;\n\tdefault:  \n\t\tmlx5e_free_wqe_alloc_info(rq);\n\t}\n\n\txdp_rxq_info_unreg(&rq->xdp_rxq);\n\tpage_pool_destroy(rq->page_pool);\n\tmlx5_wq_destroy(&rq->wq_ctrl);\n}\n\nint mlx5e_create_rq(struct mlx5e_rq *rq, struct mlx5e_rq_param *param)\n{\n\tstruct mlx5_core_dev *mdev = rq->mdev;\n\tu8 ts_format;\n\tvoid *in;\n\tvoid *rqc;\n\tvoid *wq;\n\tint inlen;\n\tint err;\n\n\tinlen = MLX5_ST_SZ_BYTES(create_rq_in) +\n\t\tsizeof(u64) * rq->wq_ctrl.buf.npages;\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tts_format = mlx5_is_real_time_rq(mdev) ?\n\t\t\t    MLX5_TIMESTAMP_FORMAT_REAL_TIME :\n\t\t\t    MLX5_TIMESTAMP_FORMAT_FREE_RUNNING;\n\trqc = MLX5_ADDR_OF(create_rq_in, in, ctx);\n\twq  = MLX5_ADDR_OF(rqc, rqc, wq);\n\n\tmemcpy(rqc, param->rqc, sizeof(param->rqc));\n\n\tMLX5_SET(rqc,  rqc, cqn,\t\trq->cq.mcq.cqn);\n\tMLX5_SET(rqc,  rqc, state,\t\tMLX5_RQC_STATE_RST);\n\tMLX5_SET(rqc,  rqc, ts_format,\t\tts_format);\n\tMLX5_SET(wq,   wq,  log_wq_pg_sz,\trq->wq_ctrl.buf.page_shift -\n\t\t\t\t\t\tMLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET64(wq, wq,  dbr_addr,\t\trq->wq_ctrl.db.dma);\n\n\tif (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state)) {\n\t\tMLX5_SET(wq, wq, log_headers_buffer_entry_num,\n\t\t\t order_base_2(rq->mpwqe.shampo->hd_per_wq));\n\t\tMLX5_SET(wq, wq, headers_mkey, rq->mpwqe.shampo->mkey);\n\t}\n\n\tmlx5_fill_page_frag_array(&rq->wq_ctrl.buf,\n\t\t\t\t  (__be64 *)MLX5_ADDR_OF(wq, wq, pas));\n\n\terr = mlx5_core_create_rq(mdev, in, inlen, &rq->rqn);\n\n\tkvfree(in);\n\n\treturn err;\n}\n\nstatic int mlx5e_modify_rq_state(struct mlx5e_rq *rq, int curr_state, int next_state)\n{\n\tstruct mlx5_core_dev *mdev = rq->mdev;\n\n\tvoid *in;\n\tvoid *rqc;\n\tint inlen;\n\tint err;\n\n\tinlen = MLX5_ST_SZ_BYTES(modify_rq_in);\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tif (curr_state == MLX5_RQC_STATE_RST && next_state == MLX5_RQC_STATE_RDY)\n\t\tmlx5e_rqwq_reset(rq);\n\n\trqc = MLX5_ADDR_OF(modify_rq_in, in, ctx);\n\n\tMLX5_SET(modify_rq_in, in, rq_state, curr_state);\n\tMLX5_SET(rqc, rqc, state, next_state);\n\n\terr = mlx5_core_modify_rq(mdev, rq->rqn, in);\n\n\tkvfree(in);\n\n\treturn err;\n}\n\nstatic void mlx5e_flush_rq_cq(struct mlx5e_rq *rq)\n{\n\tstruct mlx5_cqwq *cqwq = &rq->cq.wq;\n\tstruct mlx5_cqe64 *cqe;\n\n\tif (test_bit(MLX5E_RQ_STATE_MINI_CQE_ENHANCED, &rq->state)) {\n\t\twhile ((cqe = mlx5_cqwq_get_cqe_enahnced_comp(cqwq)))\n\t\t\tmlx5_cqwq_pop(cqwq);\n\t} else {\n\t\twhile ((cqe = mlx5_cqwq_get_cqe(cqwq)))\n\t\t\tmlx5_cqwq_pop(cqwq);\n\t}\n\n\tmlx5_cqwq_update_db_record(cqwq);\n}\n\nint mlx5e_flush_rq(struct mlx5e_rq *rq, int curr_state)\n{\n\tstruct net_device *dev = rq->netdev;\n\tint err;\n\n\terr = mlx5e_modify_rq_state(rq, curr_state, MLX5_RQC_STATE_RST);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to move rq 0x%x to reset\\n\", rq->rqn);\n\t\treturn err;\n\t}\n\n\tmlx5e_free_rx_descs(rq);\n\tmlx5e_flush_rq_cq(rq);\n\n\terr = mlx5e_modify_rq_state(rq, MLX5_RQC_STATE_RST, MLX5_RQC_STATE_RDY);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed to move rq 0x%x to ready\\n\", rq->rqn);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5e_modify_rq_vsd(struct mlx5e_rq *rq, bool vsd)\n{\n\tstruct mlx5_core_dev *mdev = rq->mdev;\n\tvoid *in;\n\tvoid *rqc;\n\tint inlen;\n\tint err;\n\n\tinlen = MLX5_ST_SZ_BYTES(modify_rq_in);\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\trqc = MLX5_ADDR_OF(modify_rq_in, in, ctx);\n\n\tMLX5_SET(modify_rq_in, in, rq_state, MLX5_RQC_STATE_RDY);\n\tMLX5_SET64(modify_rq_in, in, modify_bitmask,\n\t\t   MLX5_MODIFY_RQ_IN_MODIFY_BITMASK_VSD);\n\tMLX5_SET(rqc, rqc, vsd, vsd);\n\tMLX5_SET(rqc, rqc, state, MLX5_RQC_STATE_RDY);\n\n\terr = mlx5_core_modify_rq(mdev, rq->rqn, in);\n\n\tkvfree(in);\n\n\treturn err;\n}\n\nvoid mlx5e_destroy_rq(struct mlx5e_rq *rq)\n{\n\tmlx5_core_destroy_rq(rq->mdev, rq->rqn);\n}\n\nint mlx5e_wait_for_min_rx_wqes(struct mlx5e_rq *rq, int wait_time)\n{\n\tunsigned long exp_time = jiffies + msecs_to_jiffies(wait_time);\n\n\tu16 min_wqes = mlx5_min_rx_wqes(rq->wq_type, mlx5e_rqwq_get_size(rq));\n\n\tdo {\n\t\tif (mlx5e_rqwq_get_cur_sz(rq) >= min_wqes)\n\t\t\treturn 0;\n\n\t\tmsleep(20);\n\t} while (time_before(jiffies, exp_time));\n\n\tnetdev_warn(rq->netdev, \"Failed to get min RX wqes on Channel[%d] RQN[0x%x] wq cur_sz(%d) min_rx_wqes(%d)\\n\",\n\t\t    rq->ix, rq->rqn, mlx5e_rqwq_get_cur_sz(rq), min_wqes);\n\n\tmlx5e_reporter_rx_timeout(rq);\n\treturn -ETIMEDOUT;\n}\n\nvoid mlx5e_free_rx_missing_descs(struct mlx5e_rq *rq)\n{\n\tstruct mlx5_wq_ll *wq;\n\tu16 head;\n\tint i;\n\n\tif (rq->wq_type != MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ)\n\t\treturn;\n\n\twq = &rq->mpwqe.wq;\n\thead = wq->head;\n\n\t \n\tfor (i = 0; i < mlx5_wq_ll_missing(wq) + 1; i++) {\n\t\trq->dealloc_wqe(rq, head);\n\t\thead = mlx5_wq_ll_get_wqe_next_ix(wq, head);\n\t}\n\n\tif (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state)) {\n\t\tu16 len;\n\n\t\tlen = (rq->mpwqe.shampo->pi - rq->mpwqe.shampo->ci) &\n\t\t      (rq->mpwqe.shampo->hd_per_wq - 1);\n\t\tmlx5e_shampo_dealloc_hd(rq, len, rq->mpwqe.shampo->ci, false);\n\t\trq->mpwqe.shampo->pi = rq->mpwqe.shampo->ci;\n\t}\n\n\trq->mpwqe.actual_wq_head = wq->head;\n\trq->mpwqe.umr_in_progress = 0;\n\trq->mpwqe.umr_completed = 0;\n}\n\nvoid mlx5e_free_rx_descs(struct mlx5e_rq *rq)\n{\n\t__be16 wqe_ix_be;\n\tu16 wqe_ix;\n\n\tif (rq->wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ) {\n\t\tstruct mlx5_wq_ll *wq = &rq->mpwqe.wq;\n\n\t\tmlx5e_free_rx_missing_descs(rq);\n\n\t\twhile (!mlx5_wq_ll_is_empty(wq)) {\n\t\t\tstruct mlx5e_rx_wqe_ll *wqe;\n\n\t\t\twqe_ix_be = *wq->tail_next;\n\t\t\twqe_ix    = be16_to_cpu(wqe_ix_be);\n\t\t\twqe       = mlx5_wq_ll_get_wqe(wq, wqe_ix);\n\t\t\trq->dealloc_wqe(rq, wqe_ix);\n\t\t\tmlx5_wq_ll_pop(wq, wqe_ix_be,\n\t\t\t\t       &wqe->next.next_wqe_index);\n\t\t}\n\n\t\tif (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state))\n\t\t\tmlx5e_shampo_dealloc_hd(rq, rq->mpwqe.shampo->hd_per_wq,\n\t\t\t\t\t\t0, true);\n\t} else {\n\t\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\t\tu16 missing = mlx5_wq_cyc_missing(wq);\n\t\tu16 head = mlx5_wq_cyc_get_head(wq);\n\n\t\twhile (!mlx5_wq_cyc_is_empty(wq)) {\n\t\t\twqe_ix = mlx5_wq_cyc_get_tail(wq);\n\t\t\trq->dealloc_wqe(rq, wqe_ix);\n\t\t\tmlx5_wq_cyc_pop(wq);\n\t\t}\n\t\t \n\t\twhile (missing--) {\n\t\t\twqe_ix = mlx5_wq_cyc_ctr2ix(wq, head++);\n\t\t\trq->dealloc_wqe(rq, wqe_ix);\n\t\t}\n\t}\n\n}\n\nint mlx5e_open_rq(struct mlx5e_params *params, struct mlx5e_rq_param *param,\n\t\t  struct mlx5e_xsk_param *xsk, int node,\n\t\t  struct mlx5e_rq *rq)\n{\n\tstruct mlx5_core_dev *mdev = rq->mdev;\n\tint err;\n\n\tif (params->packet_merge.type == MLX5E_PACKET_MERGE_SHAMPO)\n\t\t__set_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state);\n\n\terr = mlx5e_alloc_rq(params, xsk, param, node, rq);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_create_rq(rq, param);\n\tif (err)\n\t\tgoto err_free_rq;\n\n\terr = mlx5e_modify_rq_state(rq, MLX5_RQC_STATE_RST, MLX5_RQC_STATE_RDY);\n\tif (err)\n\t\tgoto err_destroy_rq;\n\n\tif (MLX5_CAP_ETH(mdev, cqe_checksum_full))\n\t\t__set_bit(MLX5E_RQ_STATE_CSUM_FULL, &rq->state);\n\n\tif (params->rx_dim_enabled)\n\t\t__set_bit(MLX5E_RQ_STATE_DIM, &rq->state);\n\n\t \n\tif (MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_NO_CSUM_COMPLETE) || params->xdp_prog)\n\t\t__set_bit(MLX5E_RQ_STATE_NO_CSUM_COMPLETE, &rq->state);\n\n\t \n\tif (MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_STRIDING_RQ) &&\n\t    MLX5_CAP_GEN(mdev, mini_cqe_resp_stride_index))\n\t\t__set_bit(MLX5E_RQ_STATE_MINI_CQE_HW_STRIDX, &rq->state);\n\n\t \n\tif (MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS) &&\n\t    MLX5_CAP_GEN(mdev, enhanced_cqe_compression))\n\t\t__set_bit(MLX5E_RQ_STATE_MINI_CQE_ENHANCED, &rq->state);\n\n\treturn 0;\n\nerr_destroy_rq:\n\tmlx5e_destroy_rq(rq);\nerr_free_rq:\n\tmlx5e_free_rq(rq);\n\n\treturn err;\n}\n\nvoid mlx5e_activate_rq(struct mlx5e_rq *rq)\n{\n\tset_bit(MLX5E_RQ_STATE_ENABLED, &rq->state);\n}\n\nvoid mlx5e_deactivate_rq(struct mlx5e_rq *rq)\n{\n\tclear_bit(MLX5E_RQ_STATE_ENABLED, &rq->state);\n\tsynchronize_net();  \n}\n\nvoid mlx5e_close_rq(struct mlx5e_rq *rq)\n{\n\tcancel_work_sync(&rq->dim.work);\n\tcancel_work_sync(&rq->recover_work);\n\tmlx5e_destroy_rq(rq);\n\tmlx5e_free_rx_descs(rq);\n\tmlx5e_free_rq(rq);\n}\n\nstatic void mlx5e_free_xdpsq_db(struct mlx5e_xdpsq *sq)\n{\n\tkvfree(sq->db.xdpi_fifo.xi);\n\tkvfree(sq->db.wqe_info);\n}\n\nstatic int mlx5e_alloc_xdpsq_fifo(struct mlx5e_xdpsq *sq, int numa)\n{\n\tstruct mlx5e_xdp_info_fifo *xdpi_fifo = &sq->db.xdpi_fifo;\n\tint wq_sz        = mlx5_wq_cyc_get_size(&sq->wq);\n\tint entries;\n\tsize_t size;\n\n\t \n\tentries = roundup_pow_of_two(wq_sz * MLX5_SEND_WQEBB_NUM_DS *\n\t\t\t\t     MLX5E_XDP_FIFO_ENTRIES2DS_MAX_RATIO);\n\n\tsize = array_size(sizeof(*xdpi_fifo->xi), entries);\n\txdpi_fifo->xi = kvzalloc_node(size, GFP_KERNEL, numa);\n\tif (!xdpi_fifo->xi)\n\t\treturn -ENOMEM;\n\n\txdpi_fifo->pc   = &sq->xdpi_fifo_pc;\n\txdpi_fifo->cc   = &sq->xdpi_fifo_cc;\n\txdpi_fifo->mask = entries - 1;\n\n\treturn 0;\n}\n\nstatic int mlx5e_alloc_xdpsq_db(struct mlx5e_xdpsq *sq, int numa)\n{\n\tint wq_sz = mlx5_wq_cyc_get_size(&sq->wq);\n\tsize_t size;\n\tint err;\n\n\tsize = array_size(sizeof(*sq->db.wqe_info), wq_sz);\n\tsq->db.wqe_info = kvzalloc_node(size, GFP_KERNEL, numa);\n\tif (!sq->db.wqe_info)\n\t\treturn -ENOMEM;\n\n\terr = mlx5e_alloc_xdpsq_fifo(sq, numa);\n\tif (err) {\n\t\tmlx5e_free_xdpsq_db(sq);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5e_alloc_xdpsq(struct mlx5e_channel *c,\n\t\t\t     struct mlx5e_params *params,\n\t\t\t     struct xsk_buff_pool *xsk_pool,\n\t\t\t     struct mlx5e_sq_param *param,\n\t\t\t     struct mlx5e_xdpsq *sq,\n\t\t\t     bool is_redirect)\n{\n\tvoid *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);\n\tstruct mlx5_core_dev *mdev = c->mdev;\n\tstruct mlx5_wq_cyc *wq = &sq->wq;\n\tint err;\n\n\tsq->pdev      = c->pdev;\n\tsq->mkey_be   = c->mkey_be;\n\tsq->channel   = c;\n\tsq->uar_map   = mdev->mlx5e_res.hw_objs.bfreg.map;\n\tsq->min_inline_mode = params->tx_min_inline_mode;\n\tsq->hw_mtu    = MLX5E_SW2HW_MTU(params, params->sw_mtu) - ETH_FCS_LEN;\n\tsq->xsk_pool  = xsk_pool;\n\n\tsq->stats = sq->xsk_pool ?\n\t\t&c->priv->channel_stats[c->ix]->xsksq :\n\t\tis_redirect ?\n\t\t\t&c->priv->channel_stats[c->ix]->xdpsq :\n\t\t\t&c->priv->channel_stats[c->ix]->rq_xdpsq;\n\tsq->stop_room = param->is_mpw ? mlx5e_stop_room_for_mpwqe(mdev) :\n\t\t\t\t\tmlx5e_stop_room_for_max_wqe(mdev);\n\tsq->max_sq_mpw_wqebbs = mlx5e_get_max_sq_aligned_wqebbs(mdev);\n\n\tparam->wq.db_numa_node = cpu_to_node(c->cpu);\n\terr = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, wq, &sq->wq_ctrl);\n\tif (err)\n\t\treturn err;\n\twq->db = &wq->db[MLX5_SND_DBR];\n\n\terr = mlx5e_alloc_xdpsq_db(sq, cpu_to_node(c->cpu));\n\tif (err)\n\t\tgoto err_sq_wq_destroy;\n\n\treturn 0;\n\nerr_sq_wq_destroy:\n\tmlx5_wq_destroy(&sq->wq_ctrl);\n\n\treturn err;\n}\n\nstatic void mlx5e_free_xdpsq(struct mlx5e_xdpsq *sq)\n{\n\tmlx5e_free_xdpsq_db(sq);\n\tmlx5_wq_destroy(&sq->wq_ctrl);\n}\n\nstatic void mlx5e_free_icosq_db(struct mlx5e_icosq *sq)\n{\n\tkvfree(sq->db.wqe_info);\n}\n\nstatic int mlx5e_alloc_icosq_db(struct mlx5e_icosq *sq, int numa)\n{\n\tint wq_sz = mlx5_wq_cyc_get_size(&sq->wq);\n\tsize_t size;\n\n\tsize = array_size(wq_sz, sizeof(*sq->db.wqe_info));\n\tsq->db.wqe_info = kvzalloc_node(size, GFP_KERNEL, numa);\n\tif (!sq->db.wqe_info)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void mlx5e_icosq_err_cqe_work(struct work_struct *recover_work)\n{\n\tstruct mlx5e_icosq *sq = container_of(recover_work, struct mlx5e_icosq,\n\t\t\t\t\t      recover_work);\n\n\tmlx5e_reporter_icosq_cqe_err(sq);\n}\n\nstatic void mlx5e_async_icosq_err_cqe_work(struct work_struct *recover_work)\n{\n\tstruct mlx5e_icosq *sq = container_of(recover_work, struct mlx5e_icosq,\n\t\t\t\t\t      recover_work);\n\n\t \n\n\tnetdev_warn(sq->channel->netdev, \"async_icosq recovery is not implemented\\n\");\n}\n\nstatic int mlx5e_alloc_icosq(struct mlx5e_channel *c,\n\t\t\t     struct mlx5e_sq_param *param,\n\t\t\t     struct mlx5e_icosq *sq,\n\t\t\t     work_func_t recover_work_func)\n{\n\tvoid *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);\n\tstruct mlx5_core_dev *mdev = c->mdev;\n\tstruct mlx5_wq_cyc *wq = &sq->wq;\n\tint err;\n\n\tsq->channel   = c;\n\tsq->uar_map   = mdev->mlx5e_res.hw_objs.bfreg.map;\n\tsq->reserved_room = param->stop_room;\n\n\tparam->wq.db_numa_node = cpu_to_node(c->cpu);\n\terr = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, wq, &sq->wq_ctrl);\n\tif (err)\n\t\treturn err;\n\twq->db = &wq->db[MLX5_SND_DBR];\n\n\terr = mlx5e_alloc_icosq_db(sq, cpu_to_node(c->cpu));\n\tif (err)\n\t\tgoto err_sq_wq_destroy;\n\n\tINIT_WORK(&sq->recover_work, recover_work_func);\n\n\treturn 0;\n\nerr_sq_wq_destroy:\n\tmlx5_wq_destroy(&sq->wq_ctrl);\n\n\treturn err;\n}\n\nstatic void mlx5e_free_icosq(struct mlx5e_icosq *sq)\n{\n\tmlx5e_free_icosq_db(sq);\n\tmlx5_wq_destroy(&sq->wq_ctrl);\n}\n\nvoid mlx5e_free_txqsq_db(struct mlx5e_txqsq *sq)\n{\n\tkvfree(sq->db.wqe_info);\n\tkvfree(sq->db.skb_fifo.fifo);\n\tkvfree(sq->db.dma_fifo);\n}\n\nint mlx5e_alloc_txqsq_db(struct mlx5e_txqsq *sq, int numa)\n{\n\tint wq_sz = mlx5_wq_cyc_get_size(&sq->wq);\n\tint df_sz = wq_sz * MLX5_SEND_WQEBB_NUM_DS;\n\n\tsq->db.dma_fifo = kvzalloc_node(array_size(df_sz,\n\t\t\t\t\t\t   sizeof(*sq->db.dma_fifo)),\n\t\t\t\t\tGFP_KERNEL, numa);\n\tsq->db.skb_fifo.fifo = kvzalloc_node(array_size(df_sz,\n\t\t\t\t\t\t\tsizeof(*sq->db.skb_fifo.fifo)),\n\t\t\t\t\tGFP_KERNEL, numa);\n\tsq->db.wqe_info = kvzalloc_node(array_size(wq_sz,\n\t\t\t\t\t\t   sizeof(*sq->db.wqe_info)),\n\t\t\t\t\tGFP_KERNEL, numa);\n\tif (!sq->db.dma_fifo || !sq->db.skb_fifo.fifo || !sq->db.wqe_info) {\n\t\tmlx5e_free_txqsq_db(sq);\n\t\treturn -ENOMEM;\n\t}\n\n\tsq->dma_fifo_mask = df_sz - 1;\n\n\tsq->db.skb_fifo.pc   = &sq->skb_fifo_pc;\n\tsq->db.skb_fifo.cc   = &sq->skb_fifo_cc;\n\tsq->db.skb_fifo.mask = df_sz - 1;\n\n\treturn 0;\n}\n\nstatic int mlx5e_alloc_txqsq(struct mlx5e_channel *c,\n\t\t\t     int txq_ix,\n\t\t\t     struct mlx5e_params *params,\n\t\t\t     struct mlx5e_sq_param *param,\n\t\t\t     struct mlx5e_txqsq *sq,\n\t\t\t     int tc)\n{\n\tvoid *sqc_wq               = MLX5_ADDR_OF(sqc, param->sqc, wq);\n\tstruct mlx5_core_dev *mdev = c->mdev;\n\tstruct mlx5_wq_cyc *wq = &sq->wq;\n\tint err;\n\n\tsq->pdev      = c->pdev;\n\tsq->clock     = &mdev->clock;\n\tsq->mkey_be   = c->mkey_be;\n\tsq->netdev    = c->netdev;\n\tsq->mdev      = c->mdev;\n\tsq->channel   = c;\n\tsq->priv      = c->priv;\n\tsq->ch_ix     = c->ix;\n\tsq->txq_ix    = txq_ix;\n\tsq->uar_map   = mdev->mlx5e_res.hw_objs.bfreg.map;\n\tsq->min_inline_mode = params->tx_min_inline_mode;\n\tsq->hw_mtu    = MLX5E_SW2HW_MTU(params, params->sw_mtu);\n\tsq->max_sq_mpw_wqebbs = mlx5e_get_max_sq_aligned_wqebbs(mdev);\n\tINIT_WORK(&sq->recover_work, mlx5e_tx_err_cqe_work);\n\tif (!MLX5_CAP_ETH(mdev, wqe_vlan_insert))\n\t\tset_bit(MLX5E_SQ_STATE_VLAN_NEED_L2_INLINE, &sq->state);\n\tif (mlx5_ipsec_device_caps(c->priv->mdev))\n\t\tset_bit(MLX5E_SQ_STATE_IPSEC, &sq->state);\n\tif (param->is_mpw)\n\t\tset_bit(MLX5E_SQ_STATE_MPWQE, &sq->state);\n\tsq->stop_room = param->stop_room;\n\tsq->ptp_cyc2time = mlx5_sq_ts_translator(mdev);\n\n\tparam->wq.db_numa_node = cpu_to_node(c->cpu);\n\terr = mlx5_wq_cyc_create(mdev, &param->wq, sqc_wq, wq, &sq->wq_ctrl);\n\tif (err)\n\t\treturn err;\n\twq->db    = &wq->db[MLX5_SND_DBR];\n\n\terr = mlx5e_alloc_txqsq_db(sq, cpu_to_node(c->cpu));\n\tif (err)\n\t\tgoto err_sq_wq_destroy;\n\n\tINIT_WORK(&sq->dim.work, mlx5e_tx_dim_work);\n\tsq->dim.mode = params->tx_cq_moderation.cq_period_mode;\n\n\treturn 0;\n\nerr_sq_wq_destroy:\n\tmlx5_wq_destroy(&sq->wq_ctrl);\n\n\treturn err;\n}\n\nvoid mlx5e_free_txqsq(struct mlx5e_txqsq *sq)\n{\n\tmlx5e_free_txqsq_db(sq);\n\tmlx5_wq_destroy(&sq->wq_ctrl);\n}\n\nstatic int mlx5e_create_sq(struct mlx5_core_dev *mdev,\n\t\t\t   struct mlx5e_sq_param *param,\n\t\t\t   struct mlx5e_create_sq_param *csp,\n\t\t\t   u32 *sqn)\n{\n\tu8 ts_format;\n\tvoid *in;\n\tvoid *sqc;\n\tvoid *wq;\n\tint inlen;\n\tint err;\n\n\tinlen = MLX5_ST_SZ_BYTES(create_sq_in) +\n\t\tsizeof(u64) * csp->wq_ctrl->buf.npages;\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tts_format = mlx5_is_real_time_sq(mdev) ?\n\t\t\t    MLX5_TIMESTAMP_FORMAT_REAL_TIME :\n\t\t\t    MLX5_TIMESTAMP_FORMAT_FREE_RUNNING;\n\tsqc = MLX5_ADDR_OF(create_sq_in, in, ctx);\n\twq = MLX5_ADDR_OF(sqc, sqc, wq);\n\n\tmemcpy(sqc, param->sqc, sizeof(param->sqc));\n\tMLX5_SET(sqc,  sqc, tis_lst_sz, csp->tis_lst_sz);\n\tMLX5_SET(sqc,  sqc, tis_num_0, csp->tisn);\n\tMLX5_SET(sqc,  sqc, cqn, csp->cqn);\n\tMLX5_SET(sqc,  sqc, ts_cqe_to_dest_cqn, csp->ts_cqe_to_dest_cqn);\n\tMLX5_SET(sqc,  sqc, ts_format, ts_format);\n\n\n\tif (MLX5_CAP_ETH(mdev, wqe_inline_mode) == MLX5_CAP_INLINE_MODE_VPORT_CONTEXT)\n\t\tMLX5_SET(sqc,  sqc, min_wqe_inline_mode, csp->min_inline_mode);\n\n\tMLX5_SET(sqc,  sqc, state, MLX5_SQC_STATE_RST);\n\tMLX5_SET(sqc,  sqc, flush_in_error_en, 1);\n\n\tMLX5_SET(wq,   wq, wq_type,       MLX5_WQ_TYPE_CYCLIC);\n\tMLX5_SET(wq,   wq, uar_page,      mdev->mlx5e_res.hw_objs.bfreg.index);\n\tMLX5_SET(wq,   wq, log_wq_pg_sz,  csp->wq_ctrl->buf.page_shift -\n\t\t\t\t\t  MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET64(wq, wq, dbr_addr,      csp->wq_ctrl->db.dma);\n\n\tmlx5_fill_page_frag_array(&csp->wq_ctrl->buf,\n\t\t\t\t  (__be64 *)MLX5_ADDR_OF(wq, wq, pas));\n\n\terr = mlx5_core_create_sq(mdev, in, inlen, sqn);\n\n\tkvfree(in);\n\n\treturn err;\n}\n\nint mlx5e_modify_sq(struct mlx5_core_dev *mdev, u32 sqn,\n\t\t    struct mlx5e_modify_sq_param *p)\n{\n\tu64 bitmask = 0;\n\tvoid *in;\n\tvoid *sqc;\n\tint inlen;\n\tint err;\n\n\tinlen = MLX5_ST_SZ_BYTES(modify_sq_in);\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tsqc = MLX5_ADDR_OF(modify_sq_in, in, ctx);\n\n\tMLX5_SET(modify_sq_in, in, sq_state, p->curr_state);\n\tMLX5_SET(sqc, sqc, state, p->next_state);\n\tif (p->rl_update && p->next_state == MLX5_SQC_STATE_RDY) {\n\t\tbitmask |= 1;\n\t\tMLX5_SET(sqc, sqc, packet_pacing_rate_limit_index, p->rl_index);\n\t}\n\tif (p->qos_update && p->next_state == MLX5_SQC_STATE_RDY) {\n\t\tbitmask |= 1 << 2;\n\t\tMLX5_SET(sqc, sqc, qos_queue_group_id, p->qos_queue_group_id);\n\t}\n\tMLX5_SET64(modify_sq_in, in, modify_bitmask, bitmask);\n\n\terr = mlx5_core_modify_sq(mdev, sqn, in);\n\n\tkvfree(in);\n\n\treturn err;\n}\n\nstatic void mlx5e_destroy_sq(struct mlx5_core_dev *mdev, u32 sqn)\n{\n\tmlx5_core_destroy_sq(mdev, sqn);\n}\n\nint mlx5e_create_sq_rdy(struct mlx5_core_dev *mdev,\n\t\t\tstruct mlx5e_sq_param *param,\n\t\t\tstruct mlx5e_create_sq_param *csp,\n\t\t\tu16 qos_queue_group_id,\n\t\t\tu32 *sqn)\n{\n\tstruct mlx5e_modify_sq_param msp = {0};\n\tint err;\n\n\terr = mlx5e_create_sq(mdev, param, csp, sqn);\n\tif (err)\n\t\treturn err;\n\n\tmsp.curr_state = MLX5_SQC_STATE_RST;\n\tmsp.next_state = MLX5_SQC_STATE_RDY;\n\tif (qos_queue_group_id) {\n\t\tmsp.qos_update = true;\n\t\tmsp.qos_queue_group_id = qos_queue_group_id;\n\t}\n\terr = mlx5e_modify_sq(mdev, *sqn, &msp);\n\tif (err)\n\t\tmlx5e_destroy_sq(mdev, *sqn);\n\n\treturn err;\n}\n\nstatic int mlx5e_set_sq_maxrate(struct net_device *dev,\n\t\t\t\tstruct mlx5e_txqsq *sq, u32 rate);\n\nint mlx5e_open_txqsq(struct mlx5e_channel *c, u32 tisn, int txq_ix,\n\t\t     struct mlx5e_params *params, struct mlx5e_sq_param *param,\n\t\t     struct mlx5e_txqsq *sq, int tc, u16 qos_queue_group_id,\n\t\t     struct mlx5e_sq_stats *sq_stats)\n{\n\tstruct mlx5e_create_sq_param csp = {};\n\tu32 tx_rate;\n\tint err;\n\n\terr = mlx5e_alloc_txqsq(c, txq_ix, params, param, sq, tc);\n\tif (err)\n\t\treturn err;\n\n\tsq->stats = sq_stats;\n\n\tcsp.tisn            = tisn;\n\tcsp.tis_lst_sz      = 1;\n\tcsp.cqn             = sq->cq.mcq.cqn;\n\tcsp.wq_ctrl         = &sq->wq_ctrl;\n\tcsp.min_inline_mode = sq->min_inline_mode;\n\terr = mlx5e_create_sq_rdy(c->mdev, param, &csp, qos_queue_group_id, &sq->sqn);\n\tif (err)\n\t\tgoto err_free_txqsq;\n\n\ttx_rate = c->priv->tx_rates[sq->txq_ix];\n\tif (tx_rate)\n\t\tmlx5e_set_sq_maxrate(c->netdev, sq, tx_rate);\n\n\tif (params->tx_dim_enabled)\n\t\tsq->state |= BIT(MLX5E_SQ_STATE_DIM);\n\n\treturn 0;\n\nerr_free_txqsq:\n\tmlx5e_free_txqsq(sq);\n\n\treturn err;\n}\n\nvoid mlx5e_activate_txqsq(struct mlx5e_txqsq *sq)\n{\n\tsq->txq = netdev_get_tx_queue(sq->netdev, sq->txq_ix);\n\tset_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);\n\tnetdev_tx_reset_queue(sq->txq);\n\tnetif_tx_start_queue(sq->txq);\n}\n\nvoid mlx5e_tx_disable_queue(struct netdev_queue *txq)\n{\n\t__netif_tx_lock_bh(txq);\n\tnetif_tx_stop_queue(txq);\n\t__netif_tx_unlock_bh(txq);\n}\n\nvoid mlx5e_deactivate_txqsq(struct mlx5e_txqsq *sq)\n{\n\tstruct mlx5_wq_cyc *wq = &sq->wq;\n\n\tclear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);\n\tsynchronize_net();  \n\n\tmlx5e_tx_disable_queue(sq->txq);\n\n\t \n\tif (mlx5e_wqc_has_room_for(wq, sq->cc, sq->pc, 1)) {\n\t\tu16 pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);\n\t\tstruct mlx5e_tx_wqe *nop;\n\n\t\tsq->db.wqe_info[pi] = (struct mlx5e_tx_wqe_info) {\n\t\t\t.num_wqebbs = 1,\n\t\t};\n\n\t\tnop = mlx5e_post_nop(wq, sq->sqn, &sq->pc);\n\t\tmlx5e_notify_hw(wq, sq->pc, sq->uar_map, &nop->ctrl);\n\t}\n}\n\nvoid mlx5e_close_txqsq(struct mlx5e_txqsq *sq)\n{\n\tstruct mlx5_core_dev *mdev = sq->mdev;\n\tstruct mlx5_rate_limit rl = {0};\n\n\tcancel_work_sync(&sq->dim.work);\n\tcancel_work_sync(&sq->recover_work);\n\tmlx5e_destroy_sq(mdev, sq->sqn);\n\tif (sq->rate_limit) {\n\t\trl.rate = sq->rate_limit;\n\t\tmlx5_rl_remove_rate(mdev, &rl);\n\t}\n\tmlx5e_free_txqsq_descs(sq);\n\tmlx5e_free_txqsq(sq);\n}\n\nvoid mlx5e_tx_err_cqe_work(struct work_struct *recover_work)\n{\n\tstruct mlx5e_txqsq *sq = container_of(recover_work, struct mlx5e_txqsq,\n\t\t\t\t\t      recover_work);\n\n\tmlx5e_reporter_tx_err_cqe(sq);\n}\n\nstatic int mlx5e_open_icosq(struct mlx5e_channel *c, struct mlx5e_params *params,\n\t\t\t    struct mlx5e_sq_param *param, struct mlx5e_icosq *sq,\n\t\t\t    work_func_t recover_work_func)\n{\n\tstruct mlx5e_create_sq_param csp = {};\n\tint err;\n\n\terr = mlx5e_alloc_icosq(c, param, sq, recover_work_func);\n\tif (err)\n\t\treturn err;\n\n\tcsp.cqn             = sq->cq.mcq.cqn;\n\tcsp.wq_ctrl         = &sq->wq_ctrl;\n\tcsp.min_inline_mode = params->tx_min_inline_mode;\n\terr = mlx5e_create_sq_rdy(c->mdev, param, &csp, 0, &sq->sqn);\n\tif (err)\n\t\tgoto err_free_icosq;\n\n\tif (param->is_tls) {\n\t\tsq->ktls_resync = mlx5e_ktls_rx_resync_create_resp_list();\n\t\tif (IS_ERR(sq->ktls_resync)) {\n\t\t\terr = PTR_ERR(sq->ktls_resync);\n\t\t\tgoto err_destroy_icosq;\n\t\t}\n\t}\n\treturn 0;\n\nerr_destroy_icosq:\n\tmlx5e_destroy_sq(c->mdev, sq->sqn);\nerr_free_icosq:\n\tmlx5e_free_icosq(sq);\n\n\treturn err;\n}\n\nvoid mlx5e_activate_icosq(struct mlx5e_icosq *icosq)\n{\n\tset_bit(MLX5E_SQ_STATE_ENABLED, &icosq->state);\n}\n\nvoid mlx5e_deactivate_icosq(struct mlx5e_icosq *icosq)\n{\n\tclear_bit(MLX5E_SQ_STATE_ENABLED, &icosq->state);\n\tsynchronize_net();  \n}\n\nstatic void mlx5e_close_icosq(struct mlx5e_icosq *sq)\n{\n\tstruct mlx5e_channel *c = sq->channel;\n\n\tif (sq->ktls_resync)\n\t\tmlx5e_ktls_rx_resync_destroy_resp_list(sq->ktls_resync);\n\tmlx5e_destroy_sq(c->mdev, sq->sqn);\n\tmlx5e_free_icosq_descs(sq);\n\tmlx5e_free_icosq(sq);\n}\n\nint mlx5e_open_xdpsq(struct mlx5e_channel *c, struct mlx5e_params *params,\n\t\t     struct mlx5e_sq_param *param, struct xsk_buff_pool *xsk_pool,\n\t\t     struct mlx5e_xdpsq *sq, bool is_redirect)\n{\n\tstruct mlx5e_create_sq_param csp = {};\n\tint err;\n\n\terr = mlx5e_alloc_xdpsq(c, params, xsk_pool, param, sq, is_redirect);\n\tif (err)\n\t\treturn err;\n\n\tcsp.tis_lst_sz      = 1;\n\tcsp.tisn            = c->priv->tisn[c->lag_port][0];  \n\tcsp.cqn             = sq->cq.mcq.cqn;\n\tcsp.wq_ctrl         = &sq->wq_ctrl;\n\tcsp.min_inline_mode = sq->min_inline_mode;\n\tset_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);\n\n\tif (param->is_xdp_mb)\n\t\tset_bit(MLX5E_SQ_STATE_XDP_MULTIBUF, &sq->state);\n\n\terr = mlx5e_create_sq_rdy(c->mdev, param, &csp, 0, &sq->sqn);\n\tif (err)\n\t\tgoto err_free_xdpsq;\n\n\tmlx5e_set_xmit_fp(sq, param->is_mpw);\n\n\tif (!param->is_mpw && !test_bit(MLX5E_SQ_STATE_XDP_MULTIBUF, &sq->state)) {\n\t\tunsigned int ds_cnt = MLX5E_TX_WQE_EMPTY_DS_COUNT + 1;\n\t\tunsigned int inline_hdr_sz = 0;\n\t\tint i;\n\n\t\tif (sq->min_inline_mode != MLX5_INLINE_MODE_NONE) {\n\t\t\tinline_hdr_sz = MLX5E_XDP_MIN_INLINE;\n\t\t\tds_cnt++;\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < mlx5_wq_cyc_get_size(&sq->wq); i++) {\n\t\t\tstruct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(&sq->wq, i);\n\t\t\tstruct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;\n\t\t\tstruct mlx5_wqe_eth_seg  *eseg = &wqe->eth;\n\n\t\t\tsq->db.wqe_info[i] = (struct mlx5e_xdp_wqe_info) {\n\t\t\t\t.num_wqebbs = 1,\n\t\t\t\t.num_pkts   = 1,\n\t\t\t};\n\n\t\t\tcseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_cnt);\n\t\t\teseg->inline_hdr.sz = cpu_to_be16(inline_hdr_sz);\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_free_xdpsq:\n\tclear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);\n\tmlx5e_free_xdpsq(sq);\n\n\treturn err;\n}\n\nvoid mlx5e_close_xdpsq(struct mlx5e_xdpsq *sq)\n{\n\tstruct mlx5e_channel *c = sq->channel;\n\n\tclear_bit(MLX5E_SQ_STATE_ENABLED, &sq->state);\n\tsynchronize_net();  \n\n\tmlx5e_destroy_sq(c->mdev, sq->sqn);\n\tmlx5e_free_xdpsq_descs(sq);\n\tmlx5e_free_xdpsq(sq);\n}\n\nstatic int mlx5e_alloc_cq_common(struct mlx5e_priv *priv,\n\t\t\t\t struct mlx5e_cq_param *param,\n\t\t\t\t struct mlx5e_cq *cq)\n{\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tstruct mlx5_core_cq *mcq = &cq->mcq;\n\tint err;\n\tu32 i;\n\n\terr = mlx5_cqwq_create(mdev, &param->wq, param->cqc, &cq->wq,\n\t\t\t       &cq->wq_ctrl);\n\tif (err)\n\t\treturn err;\n\n\tmcq->cqe_sz     = 64;\n\tmcq->set_ci_db  = cq->wq_ctrl.db.db;\n\tmcq->arm_db     = cq->wq_ctrl.db.db + 1;\n\t*mcq->set_ci_db = 0;\n\t*mcq->arm_db    = 0;\n\tmcq->vector     = param->eq_ix;\n\tmcq->comp       = mlx5e_completion_event;\n\tmcq->event      = mlx5e_cq_error_event;\n\n\tfor (i = 0; i < mlx5_cqwq_get_size(&cq->wq); i++) {\n\t\tstruct mlx5_cqe64 *cqe = mlx5_cqwq_get_wqe(&cq->wq, i);\n\n\t\tcqe->op_own = 0xf1;\n\t\tcqe->validity_iteration_count = 0xff;\n\t}\n\n\tcq->mdev = mdev;\n\tcq->netdev = priv->netdev;\n\tcq->priv = priv;\n\n\treturn 0;\n}\n\nstatic int mlx5e_alloc_cq(struct mlx5e_priv *priv,\n\t\t\t  struct mlx5e_cq_param *param,\n\t\t\t  struct mlx5e_create_cq_param *ccp,\n\t\t\t  struct mlx5e_cq *cq)\n{\n\tint err;\n\n\tparam->wq.buf_numa_node = ccp->node;\n\tparam->wq.db_numa_node  = ccp->node;\n\tparam->eq_ix            = ccp->ix;\n\n\terr = mlx5e_alloc_cq_common(priv, param, cq);\n\n\tcq->napi     = ccp->napi;\n\tcq->ch_stats = ccp->ch_stats;\n\n\treturn err;\n}\n\nstatic void mlx5e_free_cq(struct mlx5e_cq *cq)\n{\n\tmlx5_wq_destroy(&cq->wq_ctrl);\n}\n\nstatic int mlx5e_create_cq(struct mlx5e_cq *cq, struct mlx5e_cq_param *param)\n{\n\tu32 out[MLX5_ST_SZ_DW(create_cq_out)];\n\tstruct mlx5_core_dev *mdev = cq->mdev;\n\tstruct mlx5_core_cq *mcq = &cq->mcq;\n\n\tvoid *in;\n\tvoid *cqc;\n\tint inlen;\n\tint eqn;\n\tint err;\n\n\terr = mlx5_comp_eqn_get(mdev, param->eq_ix, &eqn);\n\tif (err)\n\t\treturn err;\n\n\tinlen = MLX5_ST_SZ_BYTES(create_cq_in) +\n\t\tsizeof(u64) * cq->wq_ctrl.buf.npages;\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tcqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);\n\n\tmemcpy(cqc, param->cqc, sizeof(param->cqc));\n\n\tmlx5_fill_page_frag_array(&cq->wq_ctrl.buf,\n\t\t\t\t  (__be64 *)MLX5_ADDR_OF(create_cq_in, in, pas));\n\n\tMLX5_SET(cqc,   cqc, cq_period_mode, param->cq_period_mode);\n\tMLX5_SET(cqc,   cqc, c_eqn_or_apu_element, eqn);\n\tMLX5_SET(cqc,   cqc, uar_page,      mdev->priv.uar->index);\n\tMLX5_SET(cqc,   cqc, log_page_size, cq->wq_ctrl.buf.page_shift -\n\t\t\t\t\t    MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET64(cqc, cqc, dbr_addr,      cq->wq_ctrl.db.dma);\n\n\terr = mlx5_core_create_cq(mdev, mcq, in, inlen, out, sizeof(out));\n\n\tkvfree(in);\n\n\tif (err)\n\t\treturn err;\n\n\tmlx5e_cq_arm(cq);\n\n\treturn 0;\n}\n\nstatic void mlx5e_destroy_cq(struct mlx5e_cq *cq)\n{\n\tmlx5_core_destroy_cq(cq->mdev, &cq->mcq);\n}\n\nint mlx5e_open_cq(struct mlx5e_priv *priv, struct dim_cq_moder moder,\n\t\t  struct mlx5e_cq_param *param, struct mlx5e_create_cq_param *ccp,\n\t\t  struct mlx5e_cq *cq)\n{\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tint err;\n\n\terr = mlx5e_alloc_cq(priv, param, ccp, cq);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_create_cq(cq, param);\n\tif (err)\n\t\tgoto err_free_cq;\n\n\tif (MLX5_CAP_GEN(mdev, cq_moderation))\n\t\tmlx5_core_modify_cq_moderation(mdev, &cq->mcq, moder.usec, moder.pkts);\n\treturn 0;\n\nerr_free_cq:\n\tmlx5e_free_cq(cq);\n\n\treturn err;\n}\n\nvoid mlx5e_close_cq(struct mlx5e_cq *cq)\n{\n\tmlx5e_destroy_cq(cq);\n\tmlx5e_free_cq(cq);\n}\n\nstatic int mlx5e_open_tx_cqs(struct mlx5e_channel *c,\n\t\t\t     struct mlx5e_params *params,\n\t\t\t     struct mlx5e_create_cq_param *ccp,\n\t\t\t     struct mlx5e_channel_param *cparam)\n{\n\tint err;\n\tint tc;\n\n\tfor (tc = 0; tc < c->num_tc; tc++) {\n\t\terr = mlx5e_open_cq(c->priv, params->tx_cq_moderation, &cparam->txq_sq.cqp,\n\t\t\t\t    ccp, &c->sq[tc].cq);\n\t\tif (err)\n\t\t\tgoto err_close_tx_cqs;\n\t}\n\n\treturn 0;\n\nerr_close_tx_cqs:\n\tfor (tc--; tc >= 0; tc--)\n\t\tmlx5e_close_cq(&c->sq[tc].cq);\n\n\treturn err;\n}\n\nstatic void mlx5e_close_tx_cqs(struct mlx5e_channel *c)\n{\n\tint tc;\n\n\tfor (tc = 0; tc < c->num_tc; tc++)\n\t\tmlx5e_close_cq(&c->sq[tc].cq);\n}\n\nstatic int mlx5e_mqprio_txq_to_tc(struct netdev_tc_txq *tc_to_txq, unsigned int txq)\n{\n\tint tc;\n\n\tfor (tc = 0; tc < TC_MAX_QUEUE; tc++)\n\t\tif (txq - tc_to_txq[tc].offset < tc_to_txq[tc].count)\n\t\t\treturn tc;\n\n\tWARN(1, \"Unexpected TCs configuration. No match found for txq %u\", txq);\n\treturn -ENOENT;\n}\n\nstatic int mlx5e_txq_get_qos_node_hw_id(struct mlx5e_params *params, int txq_ix,\n\t\t\t\t\tu32 *hw_id)\n{\n\tint tc;\n\n\tif (params->mqprio.mode != TC_MQPRIO_MODE_CHANNEL) {\n\t\t*hw_id = 0;\n\t\treturn 0;\n\t}\n\n\ttc = mlx5e_mqprio_txq_to_tc(params->mqprio.tc_to_txq, txq_ix);\n\tif (tc < 0)\n\t\treturn tc;\n\n\tif (tc >= params->mqprio.num_tc) {\n\t\tWARN(1, \"Unexpected TCs configuration. tc %d is out of range of %u\",\n\t\t     tc, params->mqprio.num_tc);\n\t\treturn -EINVAL;\n\t}\n\n\t*hw_id = params->mqprio.channel.hw_id[tc];\n\treturn 0;\n}\n\nstatic int mlx5e_open_sqs(struct mlx5e_channel *c,\n\t\t\t  struct mlx5e_params *params,\n\t\t\t  struct mlx5e_channel_param *cparam)\n{\n\tint err, tc;\n\n\tfor (tc = 0; tc < mlx5e_get_dcb_num_tc(params); tc++) {\n\t\tint txq_ix = c->ix + tc * params->num_channels;\n\t\tu32 qos_queue_group_id;\n\n\t\terr = mlx5e_txq_get_qos_node_hw_id(params, txq_ix, &qos_queue_group_id);\n\t\tif (err)\n\t\t\tgoto err_close_sqs;\n\n\t\terr = mlx5e_open_txqsq(c, c->priv->tisn[c->lag_port][tc], txq_ix,\n\t\t\t\t       params, &cparam->txq_sq, &c->sq[tc], tc,\n\t\t\t\t       qos_queue_group_id,\n\t\t\t\t       &c->priv->channel_stats[c->ix]->sq[tc]);\n\t\tif (err)\n\t\t\tgoto err_close_sqs;\n\t}\n\n\treturn 0;\n\nerr_close_sqs:\n\tfor (tc--; tc >= 0; tc--)\n\t\tmlx5e_close_txqsq(&c->sq[tc]);\n\n\treturn err;\n}\n\nstatic void mlx5e_close_sqs(struct mlx5e_channel *c)\n{\n\tint tc;\n\n\tfor (tc = 0; tc < c->num_tc; tc++)\n\t\tmlx5e_close_txqsq(&c->sq[tc]);\n}\n\nstatic int mlx5e_set_sq_maxrate(struct net_device *dev,\n\t\t\t\tstruct mlx5e_txqsq *sq, u32 rate)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tstruct mlx5e_modify_sq_param msp = {0};\n\tstruct mlx5_rate_limit rl = {0};\n\tu16 rl_index = 0;\n\tint err;\n\n\tif (rate == sq->rate_limit)\n\t\t \n\t\treturn 0;\n\n\tif (sq->rate_limit) {\n\t\trl.rate = sq->rate_limit;\n\t\t \n\t\tmlx5_rl_remove_rate(mdev, &rl);\n\t}\n\n\tsq->rate_limit = 0;\n\n\tif (rate) {\n\t\trl.rate = rate;\n\t\terr = mlx5_rl_add_rate(mdev, &rl_index, &rl);\n\t\tif (err) {\n\t\t\tnetdev_err(dev, \"Failed configuring rate %u: %d\\n\",\n\t\t\t\t   rate, err);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tmsp.curr_state = MLX5_SQC_STATE_RDY;\n\tmsp.next_state = MLX5_SQC_STATE_RDY;\n\tmsp.rl_index   = rl_index;\n\tmsp.rl_update  = true;\n\terr = mlx5e_modify_sq(mdev, sq->sqn, &msp);\n\tif (err) {\n\t\tnetdev_err(dev, \"Failed configuring rate %u: %d\\n\",\n\t\t\t   rate, err);\n\t\t \n\t\tif (rate)\n\t\t\tmlx5_rl_remove_rate(mdev, &rl);\n\t\treturn err;\n\t}\n\n\tsq->rate_limit = rate;\n\treturn 0;\n}\n\nstatic int mlx5e_set_tx_maxrate(struct net_device *dev, int index, u32 rate)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tstruct mlx5e_txqsq *sq = priv->txq2sq[index];\n\tint err = 0;\n\n\tif (!mlx5_rl_is_supported(mdev)) {\n\t\tnetdev_err(dev, \"Rate limiting is not supported on this device\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\trate = rate << 10;\n\n\t \n\tif (rate && !mlx5_rl_is_in_range(mdev, rate)) {\n\t\tnetdev_err(dev, \"TX rate %u, is not in range\\n\", rate);\n\t\treturn -ERANGE;\n\t}\n\n\tmutex_lock(&priv->state_lock);\n\tif (test_bit(MLX5E_STATE_OPENED, &priv->state))\n\t\terr = mlx5e_set_sq_maxrate(dev, sq, rate);\n\tif (!err)\n\t\tpriv->tx_rates[index] = rate;\n\tmutex_unlock(&priv->state_lock);\n\n\treturn err;\n}\n\nstatic int mlx5e_open_rxq_rq(struct mlx5e_channel *c, struct mlx5e_params *params,\n\t\t\t     struct mlx5e_rq_param *rq_params)\n{\n\tint err;\n\n\terr = mlx5e_init_rxq_rq(c, params, rq_params->xdp_frag_size, &c->rq);\n\tif (err)\n\t\treturn err;\n\n\treturn mlx5e_open_rq(params, rq_params, NULL, cpu_to_node(c->cpu), &c->rq);\n}\n\nstatic int mlx5e_open_queues(struct mlx5e_channel *c,\n\t\t\t     struct mlx5e_params *params,\n\t\t\t     struct mlx5e_channel_param *cparam)\n{\n\tstruct dim_cq_moder icocq_moder = {0, 0};\n\tstruct mlx5e_create_cq_param ccp;\n\tint err;\n\n\tmlx5e_build_create_cq_param(&ccp, c);\n\n\terr = mlx5e_open_cq(c->priv, icocq_moder, &cparam->async_icosq.cqp, &ccp,\n\t\t\t    &c->async_icosq.cq);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_open_cq(c->priv, icocq_moder, &cparam->icosq.cqp, &ccp,\n\t\t\t    &c->icosq.cq);\n\tif (err)\n\t\tgoto err_close_async_icosq_cq;\n\n\terr = mlx5e_open_tx_cqs(c, params, &ccp, cparam);\n\tif (err)\n\t\tgoto err_close_icosq_cq;\n\n\terr = mlx5e_open_cq(c->priv, params->tx_cq_moderation, &cparam->xdp_sq.cqp, &ccp,\n\t\t\t    &c->xdpsq.cq);\n\tif (err)\n\t\tgoto err_close_tx_cqs;\n\n\terr = mlx5e_open_cq(c->priv, params->rx_cq_moderation, &cparam->rq.cqp, &ccp,\n\t\t\t    &c->rq.cq);\n\tif (err)\n\t\tgoto err_close_xdp_tx_cqs;\n\n\terr = c->xdp ? mlx5e_open_cq(c->priv, params->tx_cq_moderation, &cparam->xdp_sq.cqp,\n\t\t\t\t     &ccp, &c->rq_xdpsq.cq) : 0;\n\tif (err)\n\t\tgoto err_close_rx_cq;\n\n\tspin_lock_init(&c->async_icosq_lock);\n\n\terr = mlx5e_open_icosq(c, params, &cparam->async_icosq, &c->async_icosq,\n\t\t\t       mlx5e_async_icosq_err_cqe_work);\n\tif (err)\n\t\tgoto err_close_xdpsq_cq;\n\n\tmutex_init(&c->icosq_recovery_lock);\n\n\terr = mlx5e_open_icosq(c, params, &cparam->icosq, &c->icosq,\n\t\t\t       mlx5e_icosq_err_cqe_work);\n\tif (err)\n\t\tgoto err_close_async_icosq;\n\n\terr = mlx5e_open_sqs(c, params, cparam);\n\tif (err)\n\t\tgoto err_close_icosq;\n\n\terr = mlx5e_open_rxq_rq(c, params, &cparam->rq);\n\tif (err)\n\t\tgoto err_close_sqs;\n\n\tif (c->xdp) {\n\t\terr = mlx5e_open_xdpsq(c, params, &cparam->xdp_sq, NULL,\n\t\t\t\t       &c->rq_xdpsq, false);\n\t\tif (err)\n\t\t\tgoto err_close_rq;\n\t}\n\n\terr = mlx5e_open_xdpsq(c, params, &cparam->xdp_sq, NULL, &c->xdpsq, true);\n\tif (err)\n\t\tgoto err_close_xdp_sq;\n\n\treturn 0;\n\nerr_close_xdp_sq:\n\tif (c->xdp)\n\t\tmlx5e_close_xdpsq(&c->rq_xdpsq);\n\nerr_close_rq:\n\tmlx5e_close_rq(&c->rq);\n\nerr_close_sqs:\n\tmlx5e_close_sqs(c);\n\nerr_close_icosq:\n\tmlx5e_close_icosq(&c->icosq);\n\nerr_close_async_icosq:\n\tmlx5e_close_icosq(&c->async_icosq);\n\nerr_close_xdpsq_cq:\n\tif (c->xdp)\n\t\tmlx5e_close_cq(&c->rq_xdpsq.cq);\n\nerr_close_rx_cq:\n\tmlx5e_close_cq(&c->rq.cq);\n\nerr_close_xdp_tx_cqs:\n\tmlx5e_close_cq(&c->xdpsq.cq);\n\nerr_close_tx_cqs:\n\tmlx5e_close_tx_cqs(c);\n\nerr_close_icosq_cq:\n\tmlx5e_close_cq(&c->icosq.cq);\n\nerr_close_async_icosq_cq:\n\tmlx5e_close_cq(&c->async_icosq.cq);\n\n\treturn err;\n}\n\nstatic void mlx5e_close_queues(struct mlx5e_channel *c)\n{\n\tmlx5e_close_xdpsq(&c->xdpsq);\n\tif (c->xdp)\n\t\tmlx5e_close_xdpsq(&c->rq_xdpsq);\n\t \n\tcancel_work_sync(&c->icosq.recover_work);\n\tmlx5e_close_rq(&c->rq);\n\tmlx5e_close_sqs(c);\n\tmlx5e_close_icosq(&c->icosq);\n\tmutex_destroy(&c->icosq_recovery_lock);\n\tmlx5e_close_icosq(&c->async_icosq);\n\tif (c->xdp)\n\t\tmlx5e_close_cq(&c->rq_xdpsq.cq);\n\tmlx5e_close_cq(&c->rq.cq);\n\tmlx5e_close_cq(&c->xdpsq.cq);\n\tmlx5e_close_tx_cqs(c);\n\tmlx5e_close_cq(&c->icosq.cq);\n\tmlx5e_close_cq(&c->async_icosq.cq);\n}\n\nstatic u8 mlx5e_enumerate_lag_port(struct mlx5_core_dev *mdev, int ix)\n{\n\tu16 port_aff_bias = mlx5_core_is_pf(mdev) ? 0 : MLX5_CAP_GEN(mdev, vhca_id);\n\n\treturn (ix + port_aff_bias) % mlx5e_get_num_lag_ports(mdev);\n}\n\nstatic int mlx5e_channel_stats_alloc(struct mlx5e_priv *priv, int ix, int cpu)\n{\n\tif (ix > priv->stats_nch)  {\n\t\tnetdev_warn(priv->netdev, \"Unexpected channel stats index %d > %d\\n\", ix,\n\t\t\t    priv->stats_nch);\n\t\treturn -EINVAL;\n\t}\n\n\tif (priv->channel_stats[ix])\n\t\treturn 0;\n\n\t \n\tnetdev_dbg(priv->netdev, \"Creating channel stats %d\\n\", ix);\n\tpriv->channel_stats[ix] = kvzalloc_node(sizeof(**priv->channel_stats),\n\t\t\t\t\t\tGFP_KERNEL, cpu_to_node(cpu));\n\tif (!priv->channel_stats[ix])\n\t\treturn -ENOMEM;\n\tpriv->stats_nch++;\n\n\treturn 0;\n}\n\nvoid mlx5e_trigger_napi_icosq(struct mlx5e_channel *c)\n{\n\tspin_lock_bh(&c->async_icosq_lock);\n\tmlx5e_trigger_irq(&c->async_icosq);\n\tspin_unlock_bh(&c->async_icosq_lock);\n}\n\nvoid mlx5e_trigger_napi_sched(struct napi_struct *napi)\n{\n\tlocal_bh_disable();\n\tnapi_schedule(napi);\n\tlocal_bh_enable();\n}\n\nstatic int mlx5e_open_channel(struct mlx5e_priv *priv, int ix,\n\t\t\t      struct mlx5e_params *params,\n\t\t\t      struct mlx5e_channel_param *cparam,\n\t\t\t      struct xsk_buff_pool *xsk_pool,\n\t\t\t      struct mlx5e_channel **cp)\n{\n\tint cpu = mlx5_comp_vector_get_cpu(priv->mdev, ix);\n\tstruct net_device *netdev = priv->netdev;\n\tstruct mlx5e_xsk_param xsk;\n\tstruct mlx5e_channel *c;\n\tunsigned int irq;\n\tint err;\n\n\terr = mlx5_comp_irqn_get(priv->mdev, ix, &irq);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_channel_stats_alloc(priv, ix, cpu);\n\tif (err)\n\t\treturn err;\n\n\tc = kvzalloc_node(sizeof(*c), GFP_KERNEL, cpu_to_node(cpu));\n\tif (!c)\n\t\treturn -ENOMEM;\n\n\tc->priv     = priv;\n\tc->mdev     = priv->mdev;\n\tc->tstamp   = &priv->tstamp;\n\tc->ix       = ix;\n\tc->cpu      = cpu;\n\tc->pdev     = mlx5_core_dma_dev(priv->mdev);\n\tc->netdev   = priv->netdev;\n\tc->mkey_be  = cpu_to_be32(priv->mdev->mlx5e_res.hw_objs.mkey);\n\tc->num_tc   = mlx5e_get_dcb_num_tc(params);\n\tc->xdp      = !!params->xdp_prog;\n\tc->stats    = &priv->channel_stats[ix]->ch;\n\tc->aff_mask = irq_get_effective_affinity_mask(irq);\n\tc->lag_port = mlx5e_enumerate_lag_port(priv->mdev, ix);\n\n\tnetif_napi_add(netdev, &c->napi, mlx5e_napi_poll);\n\n\terr = mlx5e_open_queues(c, params, cparam);\n\tif (unlikely(err))\n\t\tgoto err_napi_del;\n\n\tif (xsk_pool) {\n\t\tmlx5e_build_xsk_param(xsk_pool, &xsk);\n\t\terr = mlx5e_open_xsk(priv, params, &xsk, xsk_pool, c);\n\t\tif (unlikely(err))\n\t\t\tgoto err_close_queues;\n\t}\n\n\t*cp = c;\n\n\treturn 0;\n\nerr_close_queues:\n\tmlx5e_close_queues(c);\n\nerr_napi_del:\n\tnetif_napi_del(&c->napi);\n\n\tkvfree(c);\n\n\treturn err;\n}\n\nstatic void mlx5e_activate_channel(struct mlx5e_channel *c)\n{\n\tint tc;\n\n\tnapi_enable(&c->napi);\n\n\tfor (tc = 0; tc < c->num_tc; tc++)\n\t\tmlx5e_activate_txqsq(&c->sq[tc]);\n\tmlx5e_activate_icosq(&c->icosq);\n\tmlx5e_activate_icosq(&c->async_icosq);\n\n\tif (test_bit(MLX5E_CHANNEL_STATE_XSK, c->state))\n\t\tmlx5e_activate_xsk(c);\n\telse\n\t\tmlx5e_activate_rq(&c->rq);\n}\n\nstatic void mlx5e_deactivate_channel(struct mlx5e_channel *c)\n{\n\tint tc;\n\n\tif (test_bit(MLX5E_CHANNEL_STATE_XSK, c->state))\n\t\tmlx5e_deactivate_xsk(c);\n\telse\n\t\tmlx5e_deactivate_rq(&c->rq);\n\n\tmlx5e_deactivate_icosq(&c->async_icosq);\n\tmlx5e_deactivate_icosq(&c->icosq);\n\tfor (tc = 0; tc < c->num_tc; tc++)\n\t\tmlx5e_deactivate_txqsq(&c->sq[tc]);\n\tmlx5e_qos_deactivate_queues(c);\n\n\tnapi_disable(&c->napi);\n}\n\nstatic void mlx5e_close_channel(struct mlx5e_channel *c)\n{\n\tif (test_bit(MLX5E_CHANNEL_STATE_XSK, c->state))\n\t\tmlx5e_close_xsk(c);\n\tmlx5e_close_queues(c);\n\tmlx5e_qos_close_queues(c);\n\tnetif_napi_del(&c->napi);\n\n\tkvfree(c);\n}\n\nint mlx5e_open_channels(struct mlx5e_priv *priv,\n\t\t\tstruct mlx5e_channels *chs)\n{\n\tstruct mlx5e_channel_param *cparam;\n\tint err = -ENOMEM;\n\tint i;\n\n\tchs->num = chs->params.num_channels;\n\n\tchs->c = kcalloc(chs->num, sizeof(struct mlx5e_channel *), GFP_KERNEL);\n\tcparam = kvzalloc(sizeof(struct mlx5e_channel_param), GFP_KERNEL);\n\tif (!chs->c || !cparam)\n\t\tgoto err_free;\n\n\terr = mlx5e_build_channel_param(priv->mdev, &chs->params, priv->q_counter, cparam);\n\tif (err)\n\t\tgoto err_free;\n\n\tfor (i = 0; i < chs->num; i++) {\n\t\tstruct xsk_buff_pool *xsk_pool = NULL;\n\n\t\tif (chs->params.xdp_prog)\n\t\t\txsk_pool = mlx5e_xsk_get_pool(&chs->params, chs->params.xsk, i);\n\n\t\terr = mlx5e_open_channel(priv, i, &chs->params, cparam, xsk_pool, &chs->c[i]);\n\t\tif (err)\n\t\t\tgoto err_close_channels;\n\t}\n\n\tif (MLX5E_GET_PFLAG(&chs->params, MLX5E_PFLAG_TX_PORT_TS) || chs->params.ptp_rx) {\n\t\terr = mlx5e_ptp_open(priv, &chs->params, chs->c[0]->lag_port, &chs->ptp);\n\t\tif (err)\n\t\t\tgoto err_close_channels;\n\t}\n\n\tif (priv->htb) {\n\t\terr = mlx5e_qos_open_queues(priv, chs);\n\t\tif (err)\n\t\t\tgoto err_close_ptp;\n\t}\n\n\tmlx5e_health_channels_update(priv);\n\tkvfree(cparam);\n\treturn 0;\n\nerr_close_ptp:\n\tif (chs->ptp)\n\t\tmlx5e_ptp_close(chs->ptp);\n\nerr_close_channels:\n\tfor (i--; i >= 0; i--)\n\t\tmlx5e_close_channel(chs->c[i]);\n\nerr_free:\n\tkfree(chs->c);\n\tkvfree(cparam);\n\tchs->num = 0;\n\treturn err;\n}\n\nstatic void mlx5e_activate_channels(struct mlx5e_priv *priv, struct mlx5e_channels *chs)\n{\n\tint i;\n\n\tfor (i = 0; i < chs->num; i++)\n\t\tmlx5e_activate_channel(chs->c[i]);\n\n\tif (priv->htb)\n\t\tmlx5e_qos_activate_queues(priv);\n\n\tfor (i = 0; i < chs->num; i++)\n\t\tmlx5e_trigger_napi_icosq(chs->c[i]);\n\n\tif (chs->ptp)\n\t\tmlx5e_ptp_activate_channel(chs->ptp);\n}\n\nstatic int mlx5e_wait_channels_min_rx_wqes(struct mlx5e_channels *chs)\n{\n\tint err = 0;\n\tint i;\n\n\tfor (i = 0; i < chs->num; i++) {\n\t\tint timeout = err ? 0 : MLX5E_RQ_WQES_TIMEOUT;\n\t\tstruct mlx5e_channel *c = chs->c[i];\n\n\t\tif (test_bit(MLX5E_CHANNEL_STATE_XSK, c->state))\n\t\t\tcontinue;\n\n\t\terr |= mlx5e_wait_for_min_rx_wqes(&c->rq, timeout);\n\n\t\t \n\t}\n\n\treturn err ? -ETIMEDOUT : 0;\n}\n\nstatic void mlx5e_deactivate_channels(struct mlx5e_channels *chs)\n{\n\tint i;\n\n\tif (chs->ptp)\n\t\tmlx5e_ptp_deactivate_channel(chs->ptp);\n\n\tfor (i = 0; i < chs->num; i++)\n\t\tmlx5e_deactivate_channel(chs->c[i]);\n}\n\nvoid mlx5e_close_channels(struct mlx5e_channels *chs)\n{\n\tint i;\n\n\tASSERT_RTNL();\n\tif (chs->ptp) {\n\t\tmlx5e_ptp_close(chs->ptp);\n\t\tchs->ptp = NULL;\n\t}\n\tfor (i = 0; i < chs->num; i++)\n\t\tmlx5e_close_channel(chs->c[i]);\n\n\tkfree(chs->c);\n\tchs->num = 0;\n}\n\nstatic int mlx5e_modify_tirs_packet_merge(struct mlx5e_priv *priv)\n{\n\tstruct mlx5e_rx_res *res = priv->rx_res;\n\n\treturn mlx5e_rx_res_packet_merge_set_param(res, &priv->channels.params.packet_merge);\n}\n\nstatic MLX5E_DEFINE_PREACTIVATE_WRAPPER_CTX(mlx5e_modify_tirs_packet_merge);\n\nstatic int mlx5e_set_mtu(struct mlx5_core_dev *mdev,\n\t\t\t struct mlx5e_params *params, u16 mtu)\n{\n\tu16 hw_mtu = MLX5E_SW2HW_MTU(params, mtu);\n\tint err;\n\n\terr = mlx5_set_port_mtu(mdev, hw_mtu, 1);\n\tif (err)\n\t\treturn err;\n\n\t \n\tmlx5_modify_nic_vport_mtu(mdev, hw_mtu);\n\treturn 0;\n}\n\nstatic void mlx5e_query_mtu(struct mlx5_core_dev *mdev,\n\t\t\t    struct mlx5e_params *params, u16 *mtu)\n{\n\tu16 hw_mtu = 0;\n\tint err;\n\n\terr = mlx5_query_nic_vport_mtu(mdev, &hw_mtu);\n\tif (err || !hw_mtu)  \n\t\tmlx5_query_port_oper_mtu(mdev, &hw_mtu, 1);\n\n\t*mtu = MLX5E_HW2SW_MTU(params, hw_mtu);\n}\n\nint mlx5e_set_dev_port_mtu(struct mlx5e_priv *priv)\n{\n\tstruct mlx5e_params *params = &priv->channels.params;\n\tstruct net_device *netdev = priv->netdev;\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tu16 mtu;\n\tint err;\n\n\terr = mlx5e_set_mtu(mdev, params, params->sw_mtu);\n\tif (err)\n\t\treturn err;\n\n\tmlx5e_query_mtu(mdev, params, &mtu);\n\tif (mtu != params->sw_mtu)\n\t\tnetdev_warn(netdev, \"%s: VPort MTU %d is different than netdev mtu %d\\n\",\n\t\t\t    __func__, mtu, params->sw_mtu);\n\n\tparams->sw_mtu = mtu;\n\treturn 0;\n}\n\nMLX5E_DEFINE_PREACTIVATE_WRAPPER_CTX(mlx5e_set_dev_port_mtu);\n\nvoid mlx5e_set_netdev_mtu_boundaries(struct mlx5e_priv *priv)\n{\n\tstruct mlx5e_params *params = &priv->channels.params;\n\tstruct net_device *netdev   = priv->netdev;\n\tstruct mlx5_core_dev *mdev  = priv->mdev;\n\tu16 max_mtu;\n\n\t \n\tnetdev->min_mtu = ETH_MIN_MTU;\n\n\tmlx5_query_port_max_mtu(mdev, &max_mtu, 1);\n\tnetdev->max_mtu = min_t(unsigned int, MLX5E_HW2SW_MTU(params, max_mtu),\n\t\t\t\tETH_MAX_MTU);\n}\n\nstatic int mlx5e_netdev_set_tcs(struct net_device *netdev, u16 nch, u8 ntc,\n\t\t\t\tstruct netdev_tc_txq *tc_to_txq)\n{\n\tint tc, err;\n\n\tnetdev_reset_tc(netdev);\n\n\tif (ntc == 1)\n\t\treturn 0;\n\n\terr = netdev_set_num_tc(netdev, ntc);\n\tif (err) {\n\t\tnetdev_WARN(netdev, \"netdev_set_num_tc failed (%d), ntc = %d\\n\", err, ntc);\n\t\treturn err;\n\t}\n\n\tfor (tc = 0; tc < ntc; tc++) {\n\t\tu16 count, offset;\n\n\t\tcount = tc_to_txq[tc].count;\n\t\toffset = tc_to_txq[tc].offset;\n\t\tnetdev_set_tc_queue(netdev, tc, count, offset);\n\t}\n\n\treturn 0;\n}\n\nint mlx5e_update_tx_netdev_queues(struct mlx5e_priv *priv)\n{\n\tint nch, ntc, num_txqs, err;\n\tint qos_queues = 0;\n\n\tif (priv->htb)\n\t\tqos_queues = mlx5e_htb_cur_leaf_nodes(priv->htb);\n\n\tnch = priv->channels.params.num_channels;\n\tntc = mlx5e_get_dcb_num_tc(&priv->channels.params);\n\tnum_txqs = nch * ntc + qos_queues;\n\tif (MLX5E_GET_PFLAG(&priv->channels.params, MLX5E_PFLAG_TX_PORT_TS))\n\t\tnum_txqs += ntc;\n\n\tnetdev_dbg(priv->netdev, \"Setting num_txqs %d\\n\", num_txqs);\n\terr = netif_set_real_num_tx_queues(priv->netdev, num_txqs);\n\tif (err)\n\t\tnetdev_warn(priv->netdev, \"netif_set_real_num_tx_queues failed, %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int mlx5e_update_netdev_queues(struct mlx5e_priv *priv)\n{\n\tstruct netdev_tc_txq old_tc_to_txq[TC_MAX_QUEUE], *tc_to_txq;\n\tstruct net_device *netdev = priv->netdev;\n\tint old_num_txqs, old_ntc;\n\tint nch, ntc;\n\tint err;\n\tint i;\n\n\told_num_txqs = netdev->real_num_tx_queues;\n\told_ntc = netdev->num_tc ? : 1;\n\tfor (i = 0; i < ARRAY_SIZE(old_tc_to_txq); i++)\n\t\told_tc_to_txq[i] = netdev->tc_to_txq[i];\n\n\tnch = priv->channels.params.num_channels;\n\tntc = priv->channels.params.mqprio.num_tc;\n\ttc_to_txq = priv->channels.params.mqprio.tc_to_txq;\n\n\terr = mlx5e_netdev_set_tcs(netdev, nch, ntc, tc_to_txq);\n\tif (err)\n\t\tgoto err_out;\n\terr = mlx5e_update_tx_netdev_queues(priv);\n\tif (err)\n\t\tgoto err_tcs;\n\terr = netif_set_real_num_rx_queues(netdev, nch);\n\tif (err) {\n\t\tnetdev_warn(netdev, \"netif_set_real_num_rx_queues failed, %d\\n\", err);\n\t\tgoto err_txqs;\n\t}\n\n\treturn 0;\n\nerr_txqs:\n\t \n\tWARN_ON_ONCE(netif_set_real_num_tx_queues(netdev, old_num_txqs));\n\nerr_tcs:\n\tWARN_ON_ONCE(mlx5e_netdev_set_tcs(netdev, old_num_txqs / old_ntc, old_ntc,\n\t\t\t\t\t  old_tc_to_txq));\nerr_out:\n\treturn err;\n}\n\nstatic MLX5E_DEFINE_PREACTIVATE_WRAPPER_CTX(mlx5e_update_netdev_queues);\n\nstatic void mlx5e_set_default_xps_cpumasks(struct mlx5e_priv *priv,\n\t\t\t\t\t   struct mlx5e_params *params)\n{\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tint num_comp_vectors, ix, irq;\n\n\tnum_comp_vectors = mlx5_comp_vectors_max(mdev);\n\n\tfor (ix = 0; ix < params->num_channels; ix++) {\n\t\tcpumask_clear(priv->scratchpad.cpumask);\n\n\t\tfor (irq = ix; irq < num_comp_vectors; irq += params->num_channels) {\n\t\t\tint cpu = mlx5_comp_vector_get_cpu(mdev, irq);\n\n\t\t\tcpumask_set_cpu(cpu, priv->scratchpad.cpumask);\n\t\t}\n\n\t\tnetif_set_xps_queue(priv->netdev, priv->scratchpad.cpumask, ix);\n\t}\n}\n\nstatic int mlx5e_num_channels_changed(struct mlx5e_priv *priv)\n{\n\tu16 count = priv->channels.params.num_channels;\n\tint err;\n\n\terr = mlx5e_update_netdev_queues(priv);\n\tif (err)\n\t\treturn err;\n\n\tmlx5e_set_default_xps_cpumasks(priv, &priv->channels.params);\n\n\t \n\tif (!netif_is_rxfh_configured(priv->netdev) && priv->rx_res)\n\t\tmlx5e_rx_res_rss_set_indir_uniform(priv->rx_res, count);\n\n\treturn 0;\n}\n\nMLX5E_DEFINE_PREACTIVATE_WRAPPER_CTX(mlx5e_num_channels_changed);\n\nstatic void mlx5e_build_txq_maps(struct mlx5e_priv *priv)\n{\n\tint i, ch, tc, num_tc;\n\n\tch = priv->channels.num;\n\tnum_tc = mlx5e_get_dcb_num_tc(&priv->channels.params);\n\n\tfor (i = 0; i < ch; i++) {\n\t\tfor (tc = 0; tc < num_tc; tc++) {\n\t\t\tstruct mlx5e_channel *c = priv->channels.c[i];\n\t\t\tstruct mlx5e_txqsq *sq = &c->sq[tc];\n\n\t\t\tpriv->txq2sq[sq->txq_ix] = sq;\n\t\t}\n\t}\n\n\tif (!priv->channels.ptp)\n\t\tgoto out;\n\n\tif (!test_bit(MLX5E_PTP_STATE_TX, priv->channels.ptp->state))\n\t\tgoto out;\n\n\tfor (tc = 0; tc < num_tc; tc++) {\n\t\tstruct mlx5e_ptp *c = priv->channels.ptp;\n\t\tstruct mlx5e_txqsq *sq = &c->ptpsq[tc].txqsq;\n\n\t\tpriv->txq2sq[sq->txq_ix] = sq;\n\t}\n\nout:\n\t \n\tsmp_wmb();\n}\n\nvoid mlx5e_activate_priv_channels(struct mlx5e_priv *priv)\n{\n\tmlx5e_build_txq_maps(priv);\n\tmlx5e_activate_channels(priv, &priv->channels);\n\tmlx5e_xdp_tx_enable(priv);\n\n\t \n\tnetif_tx_start_all_queues(priv->netdev);\n\n\tif (mlx5e_is_vport_rep(priv))\n\t\tmlx5e_rep_activate_channels(priv);\n\n\tset_bit(MLX5E_STATE_CHANNELS_ACTIVE, &priv->state);\n\n\tmlx5e_wait_channels_min_rx_wqes(&priv->channels);\n\n\tif (priv->rx_res)\n\t\tmlx5e_rx_res_channels_activate(priv->rx_res, &priv->channels);\n}\n\nstatic void mlx5e_cancel_tx_timeout_work(struct mlx5e_priv *priv)\n{\n\tWARN_ON_ONCE(test_bit(MLX5E_STATE_CHANNELS_ACTIVE, &priv->state));\n\tif (current_work() != &priv->tx_timeout_work)\n\t\tcancel_work_sync(&priv->tx_timeout_work);\n}\n\nvoid mlx5e_deactivate_priv_channels(struct mlx5e_priv *priv)\n{\n\tif (priv->rx_res)\n\t\tmlx5e_rx_res_channels_deactivate(priv->rx_res);\n\n\tclear_bit(MLX5E_STATE_CHANNELS_ACTIVE, &priv->state);\n\tmlx5e_cancel_tx_timeout_work(priv);\n\n\tif (mlx5e_is_vport_rep(priv))\n\t\tmlx5e_rep_deactivate_channels(priv);\n\n\t \n\tnetif_tx_disable(priv->netdev);\n\n\tmlx5e_xdp_tx_disable(priv);\n\tmlx5e_deactivate_channels(&priv->channels);\n}\n\nstatic int mlx5e_switch_priv_params(struct mlx5e_priv *priv,\n\t\t\t\t    struct mlx5e_params *new_params,\n\t\t\t\t    mlx5e_fp_preactivate preactivate,\n\t\t\t\t    void *context)\n{\n\tstruct mlx5e_params old_params;\n\n\told_params = priv->channels.params;\n\tpriv->channels.params = *new_params;\n\n\tif (preactivate) {\n\t\tint err;\n\n\t\terr = preactivate(priv, context);\n\t\tif (err) {\n\t\t\tpriv->channels.params = old_params;\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5e_switch_priv_channels(struct mlx5e_priv *priv,\n\t\t\t\t      struct mlx5e_channels *new_chs,\n\t\t\t\t      mlx5e_fp_preactivate preactivate,\n\t\t\t\t      void *context)\n{\n\tstruct net_device *netdev = priv->netdev;\n\tstruct mlx5e_channels old_chs;\n\tint carrier_ok;\n\tint err = 0;\n\n\tcarrier_ok = netif_carrier_ok(netdev);\n\tnetif_carrier_off(netdev);\n\n\tmlx5e_deactivate_priv_channels(priv);\n\n\told_chs = priv->channels;\n\tpriv->channels = *new_chs;\n\n\t \n\tif (preactivate) {\n\t\terr = preactivate(priv, context);\n\t\tif (err) {\n\t\t\tpriv->channels = old_chs;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmlx5e_close_channels(&old_chs);\n\tpriv->profile->update_rx(priv);\n\n\tmlx5e_selq_apply(&priv->selq);\nout:\n\tmlx5e_activate_priv_channels(priv);\n\n\t \n\tif (carrier_ok)\n\t\tnetif_carrier_on(netdev);\n\n\treturn err;\n}\n\nint mlx5e_safe_switch_params(struct mlx5e_priv *priv,\n\t\t\t     struct mlx5e_params *params,\n\t\t\t     mlx5e_fp_preactivate preactivate,\n\t\t\t     void *context, bool reset)\n{\n\tstruct mlx5e_channels *new_chs;\n\tint err;\n\n\treset &= test_bit(MLX5E_STATE_OPENED, &priv->state);\n\tif (!reset)\n\t\treturn mlx5e_switch_priv_params(priv, params, preactivate, context);\n\n\tnew_chs = kzalloc(sizeof(*new_chs), GFP_KERNEL);\n\tif (!new_chs)\n\t\treturn -ENOMEM;\n\tnew_chs->params = *params;\n\n\tmlx5e_selq_prepare_params(&priv->selq, &new_chs->params);\n\n\terr = mlx5e_open_channels(priv, new_chs);\n\tif (err)\n\t\tgoto err_cancel_selq;\n\n\terr = mlx5e_switch_priv_channels(priv, new_chs, preactivate, context);\n\tif (err)\n\t\tgoto err_close;\n\n\tkfree(new_chs);\n\treturn 0;\n\nerr_close:\n\tmlx5e_close_channels(new_chs);\n\nerr_cancel_selq:\n\tmlx5e_selq_cancel(&priv->selq);\n\tkfree(new_chs);\n\treturn err;\n}\n\nint mlx5e_safe_reopen_channels(struct mlx5e_priv *priv)\n{\n\treturn mlx5e_safe_switch_params(priv, &priv->channels.params, NULL, NULL, true);\n}\n\nvoid mlx5e_timestamp_init(struct mlx5e_priv *priv)\n{\n\tpriv->tstamp.tx_type   = HWTSTAMP_TX_OFF;\n\tpriv->tstamp.rx_filter = HWTSTAMP_FILTER_NONE;\n}\n\nstatic void mlx5e_modify_admin_state(struct mlx5_core_dev *mdev,\n\t\t\t\t     enum mlx5_port_status state)\n{\n\tstruct mlx5_eswitch *esw = mdev->priv.eswitch;\n\tint vport_admin_state;\n\n\tmlx5_set_port_admin_status(mdev, state);\n\n\tif (mlx5_eswitch_mode(mdev) == MLX5_ESWITCH_OFFLOADS ||\n\t    !MLX5_CAP_GEN(mdev, uplink_follow))\n\t\treturn;\n\n\tif (state == MLX5_PORT_UP)\n\t\tvport_admin_state = MLX5_VPORT_ADMIN_STATE_AUTO;\n\telse\n\t\tvport_admin_state = MLX5_VPORT_ADMIN_STATE_DOWN;\n\n\tmlx5_eswitch_set_vport_state(esw, MLX5_VPORT_UPLINK, vport_admin_state);\n}\n\nint mlx5e_open_locked(struct net_device *netdev)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tint err;\n\n\tmlx5e_selq_prepare_params(&priv->selq, &priv->channels.params);\n\n\tset_bit(MLX5E_STATE_OPENED, &priv->state);\n\n\terr = mlx5e_open_channels(priv, &priv->channels);\n\tif (err)\n\t\tgoto err_clear_state_opened_flag;\n\n\terr = priv->profile->update_rx(priv);\n\tif (err)\n\t\tgoto err_close_channels;\n\n\tmlx5e_selq_apply(&priv->selq);\n\tmlx5e_activate_priv_channels(priv);\n\tmlx5e_apply_traps(priv, true);\n\tif (priv->profile->update_carrier)\n\t\tpriv->profile->update_carrier(priv);\n\n\tmlx5e_queue_update_stats(priv);\n\treturn 0;\n\nerr_close_channels:\n\tmlx5e_close_channels(&priv->channels);\nerr_clear_state_opened_flag:\n\tclear_bit(MLX5E_STATE_OPENED, &priv->state);\n\tmlx5e_selq_cancel(&priv->selq);\n\treturn err;\n}\n\nint mlx5e_open(struct net_device *netdev)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tint err;\n\n\tmutex_lock(&priv->state_lock);\n\terr = mlx5e_open_locked(netdev);\n\tif (!err)\n\t\tmlx5e_modify_admin_state(priv->mdev, MLX5_PORT_UP);\n\tmutex_unlock(&priv->state_lock);\n\n\treturn err;\n}\n\nint mlx5e_close_locked(struct net_device *netdev)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\n\t \n\tif (!test_bit(MLX5E_STATE_OPENED, &priv->state))\n\t\treturn 0;\n\n\tmlx5e_apply_traps(priv, false);\n\tclear_bit(MLX5E_STATE_OPENED, &priv->state);\n\n\tnetif_carrier_off(priv->netdev);\n\tmlx5e_deactivate_priv_channels(priv);\n\tmlx5e_close_channels(&priv->channels);\n\n\treturn 0;\n}\n\nint mlx5e_close(struct net_device *netdev)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tint err;\n\n\tif (!netif_device_present(netdev))\n\t\treturn -ENODEV;\n\n\tmutex_lock(&priv->state_lock);\n\tmlx5e_modify_admin_state(priv->mdev, MLX5_PORT_DOWN);\n\terr = mlx5e_close_locked(netdev);\n\tmutex_unlock(&priv->state_lock);\n\n\treturn err;\n}\n\nstatic void mlx5e_free_drop_rq(struct mlx5e_rq *rq)\n{\n\tmlx5_wq_destroy(&rq->wq_ctrl);\n}\n\nstatic int mlx5e_alloc_drop_rq(struct mlx5_core_dev *mdev,\n\t\t\t       struct mlx5e_rq *rq,\n\t\t\t       struct mlx5e_rq_param *param)\n{\n\tvoid *rqc = param->rqc;\n\tvoid *rqc_wq = MLX5_ADDR_OF(rqc, rqc, wq);\n\tint err;\n\n\tparam->wq.db_numa_node = param->wq.buf_numa_node;\n\n\terr = mlx5_wq_cyc_create(mdev, &param->wq, rqc_wq, &rq->wqe.wq,\n\t\t\t\t &rq->wq_ctrl);\n\tif (err)\n\t\treturn err;\n\n\t \n\txdp_rxq_info_unused(&rq->xdp_rxq);\n\n\trq->mdev = mdev;\n\n\treturn 0;\n}\n\nstatic int mlx5e_alloc_drop_cq(struct mlx5e_priv *priv,\n\t\t\t       struct mlx5e_cq *cq,\n\t\t\t       struct mlx5e_cq_param *param)\n{\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\tparam->wq.buf_numa_node = dev_to_node(mlx5_core_dma_dev(mdev));\n\tparam->wq.db_numa_node  = dev_to_node(mlx5_core_dma_dev(mdev));\n\n\treturn mlx5e_alloc_cq_common(priv, param, cq);\n}\n\nint mlx5e_open_drop_rq(struct mlx5e_priv *priv,\n\t\t       struct mlx5e_rq *drop_rq)\n{\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tstruct mlx5e_cq_param cq_param = {};\n\tstruct mlx5e_rq_param rq_param = {};\n\tstruct mlx5e_cq *cq = &drop_rq->cq;\n\tint err;\n\n\tmlx5e_build_drop_rq_param(mdev, priv->drop_rq_q_counter, &rq_param);\n\n\terr = mlx5e_alloc_drop_cq(priv, cq, &cq_param);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_create_cq(cq, &cq_param);\n\tif (err)\n\t\tgoto err_free_cq;\n\n\terr = mlx5e_alloc_drop_rq(mdev, drop_rq, &rq_param);\n\tif (err)\n\t\tgoto err_destroy_cq;\n\n\terr = mlx5e_create_rq(drop_rq, &rq_param);\n\tif (err)\n\t\tgoto err_free_rq;\n\n\terr = mlx5e_modify_rq_state(drop_rq, MLX5_RQC_STATE_RST, MLX5_RQC_STATE_RDY);\n\tif (err)\n\t\tmlx5_core_warn(priv->mdev, \"modify_rq_state failed, rx_if_down_packets won't be counted %d\\n\", err);\n\n\treturn 0;\n\nerr_free_rq:\n\tmlx5e_free_drop_rq(drop_rq);\n\nerr_destroy_cq:\n\tmlx5e_destroy_cq(cq);\n\nerr_free_cq:\n\tmlx5e_free_cq(cq);\n\n\treturn err;\n}\n\nvoid mlx5e_close_drop_rq(struct mlx5e_rq *drop_rq)\n{\n\tmlx5e_destroy_rq(drop_rq);\n\tmlx5e_free_drop_rq(drop_rq);\n\tmlx5e_destroy_cq(&drop_rq->cq);\n\tmlx5e_free_cq(&drop_rq->cq);\n}\n\nint mlx5e_create_tis(struct mlx5_core_dev *mdev, void *in, u32 *tisn)\n{\n\tvoid *tisc = MLX5_ADDR_OF(create_tis_in, in, ctx);\n\n\tMLX5_SET(tisc, tisc, transport_domain, mdev->mlx5e_res.hw_objs.td.tdn);\n\n\tif (MLX5_GET(tisc, tisc, tls_en))\n\t\tMLX5_SET(tisc, tisc, pd, mdev->mlx5e_res.hw_objs.pdn);\n\n\tif (mlx5_lag_is_lacp_owner(mdev))\n\t\tMLX5_SET(tisc, tisc, strict_lag_tx_port_affinity, 1);\n\n\treturn mlx5_core_create_tis(mdev, in, tisn);\n}\n\nvoid mlx5e_destroy_tis(struct mlx5_core_dev *mdev, u32 tisn)\n{\n\tmlx5_core_destroy_tis(mdev, tisn);\n}\n\nvoid mlx5e_destroy_tises(struct mlx5e_priv *priv)\n{\n\tint tc, i;\n\n\tfor (i = 0; i < mlx5e_get_num_lag_ports(priv->mdev); i++)\n\t\tfor (tc = 0; tc < priv->profile->max_tc; tc++)\n\t\t\tmlx5e_destroy_tis(priv->mdev, priv->tisn[i][tc]);\n}\n\nstatic bool mlx5e_lag_should_assign_affinity(struct mlx5_core_dev *mdev)\n{\n\treturn MLX5_CAP_GEN(mdev, lag_tx_port_affinity) && mlx5e_get_num_lag_ports(mdev) > 1;\n}\n\nint mlx5e_create_tises(struct mlx5e_priv *priv)\n{\n\tint tc, i;\n\tint err;\n\n\tfor (i = 0; i < mlx5e_get_num_lag_ports(priv->mdev); i++) {\n\t\tfor (tc = 0; tc < priv->profile->max_tc; tc++) {\n\t\t\tu32 in[MLX5_ST_SZ_DW(create_tis_in)] = {};\n\t\t\tvoid *tisc;\n\n\t\t\ttisc = MLX5_ADDR_OF(create_tis_in, in, ctx);\n\n\t\t\tMLX5_SET(tisc, tisc, prio, tc << 1);\n\n\t\t\tif (mlx5e_lag_should_assign_affinity(priv->mdev))\n\t\t\t\tMLX5_SET(tisc, tisc, lag_tx_port_affinity, i + 1);\n\n\t\t\terr = mlx5e_create_tis(priv->mdev, in, &priv->tisn[i][tc]);\n\t\t\tif (err)\n\t\t\t\tgoto err_close_tises;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_close_tises:\n\tfor (; i >= 0; i--) {\n\t\tfor (tc--; tc >= 0; tc--)\n\t\t\tmlx5e_destroy_tis(priv->mdev, priv->tisn[i][tc]);\n\t\ttc = priv->profile->max_tc;\n\t}\n\n\treturn err;\n}\n\nstatic void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv)\n{\n\tif (priv->mqprio_rl) {\n\t\tmlx5e_mqprio_rl_cleanup(priv->mqprio_rl);\n\t\tmlx5e_mqprio_rl_free(priv->mqprio_rl);\n\t\tpriv->mqprio_rl = NULL;\n\t}\n\tmlx5e_accel_cleanup_tx(priv);\n\tmlx5e_destroy_tises(priv);\n}\n\nstatic int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd)\n{\n\tint err;\n\tint i;\n\n\tfor (i = 0; i < chs->num; i++) {\n\t\terr = mlx5e_modify_rq_vsd(&chs->c[i]->rq, vsd);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (chs->ptp && test_bit(MLX5E_PTP_STATE_RX, chs->ptp->state))\n\t\treturn mlx5e_modify_rq_vsd(&chs->ptp->rq, vsd);\n\n\treturn 0;\n}\n\nstatic void mlx5e_mqprio_build_default_tc_to_txq(struct netdev_tc_txq *tc_to_txq,\n\t\t\t\t\t\t int ntc, int nch)\n{\n\tint tc;\n\n\tmemset(tc_to_txq, 0, sizeof(*tc_to_txq) * TC_MAX_QUEUE);\n\n\t \n\tfor (tc = 0; tc < ntc; tc++) {\n\t\ttc_to_txq[tc] = (struct netdev_tc_txq) {\n\t\t\t.count = nch,\n\t\t\t.offset = 0,\n\t\t};\n\t}\n}\n\nstatic void mlx5e_mqprio_build_tc_to_txq(struct netdev_tc_txq *tc_to_txq,\n\t\t\t\t\t struct tc_mqprio_qopt *qopt)\n{\n\tint tc;\n\n\tfor (tc = 0; tc < TC_MAX_QUEUE; tc++) {\n\t\ttc_to_txq[tc] = (struct netdev_tc_txq) {\n\t\t\t.count = qopt->count[tc],\n\t\t\t.offset = qopt->offset[tc],\n\t\t};\n\t}\n}\n\nstatic void mlx5e_params_mqprio_dcb_set(struct mlx5e_params *params, u8 num_tc)\n{\n\tparams->mqprio.mode = TC_MQPRIO_MODE_DCB;\n\tparams->mqprio.num_tc = num_tc;\n\tmlx5e_mqprio_build_default_tc_to_txq(params->mqprio.tc_to_txq, num_tc,\n\t\t\t\t\t     params->num_channels);\n}\n\nstatic void mlx5e_mqprio_rl_update_params(struct mlx5e_params *params,\n\t\t\t\t\t  struct mlx5e_mqprio_rl *rl)\n{\n\tint tc;\n\n\tfor (tc = 0; tc < TC_MAX_QUEUE; tc++) {\n\t\tu32 hw_id = 0;\n\n\t\tif (rl)\n\t\t\tmlx5e_mqprio_rl_get_node_hw_id(rl, tc, &hw_id);\n\t\tparams->mqprio.channel.hw_id[tc] = hw_id;\n\t}\n}\n\nstatic void mlx5e_params_mqprio_channel_set(struct mlx5e_params *params,\n\t\t\t\t\t    struct tc_mqprio_qopt_offload *mqprio,\n\t\t\t\t\t    struct mlx5e_mqprio_rl *rl)\n{\n\tint tc;\n\n\tparams->mqprio.mode = TC_MQPRIO_MODE_CHANNEL;\n\tparams->mqprio.num_tc = mqprio->qopt.num_tc;\n\n\tfor (tc = 0; tc < TC_MAX_QUEUE; tc++)\n\t\tparams->mqprio.channel.max_rate[tc] = mqprio->max_rate[tc];\n\n\tmlx5e_mqprio_rl_update_params(params, rl);\n\tmlx5e_mqprio_build_tc_to_txq(params->mqprio.tc_to_txq, &mqprio->qopt);\n}\n\nstatic void mlx5e_params_mqprio_reset(struct mlx5e_params *params)\n{\n\tmlx5e_params_mqprio_dcb_set(params, 1);\n}\n\nstatic int mlx5e_setup_tc_mqprio_dcb(struct mlx5e_priv *priv,\n\t\t\t\t     struct tc_mqprio_qopt *mqprio)\n{\n\tstruct mlx5e_params new_params;\n\tu8 tc = mqprio->num_tc;\n\tint err;\n\n\tmqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;\n\n\tif (tc && tc != MLX5E_MAX_NUM_TC)\n\t\treturn -EINVAL;\n\n\tnew_params = priv->channels.params;\n\tmlx5e_params_mqprio_dcb_set(&new_params, tc ? tc : 1);\n\n\terr = mlx5e_safe_switch_params(priv, &new_params,\n\t\t\t\t       mlx5e_num_channels_changed_ctx, NULL, true);\n\n\tif (!err && priv->mqprio_rl) {\n\t\tmlx5e_mqprio_rl_cleanup(priv->mqprio_rl);\n\t\tmlx5e_mqprio_rl_free(priv->mqprio_rl);\n\t\tpriv->mqprio_rl = NULL;\n\t}\n\n\tpriv->max_opened_tc = max_t(u8, priv->max_opened_tc,\n\t\t\t\t    mlx5e_get_dcb_num_tc(&priv->channels.params));\n\treturn err;\n}\n\nstatic int mlx5e_mqprio_channel_validate(struct mlx5e_priv *priv,\n\t\t\t\t\t struct tc_mqprio_qopt_offload *mqprio)\n{\n\tstruct net_device *netdev = priv->netdev;\n\tstruct mlx5e_ptp *ptp_channel;\n\tint agg_count = 0;\n\tint i;\n\n\tptp_channel = priv->channels.ptp;\n\tif (ptp_channel && test_bit(MLX5E_PTP_STATE_TX, ptp_channel->state)) {\n\t\tnetdev_err(netdev,\n\t\t\t   \"Cannot activate MQPRIO mode channel since it conflicts with TX port TS\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mqprio->qopt.offset[0] != 0 || mqprio->qopt.num_tc < 1 ||\n\t    mqprio->qopt.num_tc > MLX5E_MAX_NUM_MQPRIO_CH_TC)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < mqprio->qopt.num_tc; i++) {\n\t\tif (!mqprio->qopt.count[i]) {\n\t\t\tnetdev_err(netdev, \"Zero size for queue-group (%d) is not supported\\n\", i);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (mqprio->min_rate[i]) {\n\t\t\tnetdev_err(netdev, \"Min tx rate is not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (mqprio->max_rate[i]) {\n\t\t\tint err;\n\n\t\t\terr = mlx5e_qos_bytes_rate_check(priv->mdev, mqprio->max_rate[i]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (mqprio->qopt.offset[i] != agg_count) {\n\t\t\tnetdev_err(netdev, \"Discontinuous queues config is not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tagg_count += mqprio->qopt.count[i];\n\t}\n\n\tif (priv->channels.params.num_channels != agg_count) {\n\t\tnetdev_err(netdev, \"Num of queues (%d) does not match available (%d)\\n\",\n\t\t\t   agg_count, priv->channels.params.num_channels);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool mlx5e_mqprio_rate_limit(u8 num_tc, u64 max_rate[])\n{\n\tint tc;\n\n\tfor (tc = 0; tc < num_tc; tc++)\n\t\tif (max_rate[tc])\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic struct mlx5e_mqprio_rl *mlx5e_mqprio_rl_create(struct mlx5_core_dev *mdev,\n\t\t\t\t\t\t      u8 num_tc, u64 max_rate[])\n{\n\tstruct mlx5e_mqprio_rl *rl;\n\tint err;\n\n\tif (!mlx5e_mqprio_rate_limit(num_tc, max_rate))\n\t\treturn NULL;\n\n\trl = mlx5e_mqprio_rl_alloc();\n\tif (!rl)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = mlx5e_mqprio_rl_init(rl, mdev, num_tc, max_rate);\n\tif (err) {\n\t\tmlx5e_mqprio_rl_free(rl);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn rl;\n}\n\nstatic int mlx5e_setup_tc_mqprio_channel(struct mlx5e_priv *priv,\n\t\t\t\t\t struct tc_mqprio_qopt_offload *mqprio)\n{\n\tmlx5e_fp_preactivate preactivate;\n\tstruct mlx5e_params new_params;\n\tstruct mlx5e_mqprio_rl *rl;\n\tbool nch_changed;\n\tint err;\n\n\terr = mlx5e_mqprio_channel_validate(priv, mqprio);\n\tif (err)\n\t\treturn err;\n\n\trl = mlx5e_mqprio_rl_create(priv->mdev, mqprio->qopt.num_tc, mqprio->max_rate);\n\tif (IS_ERR(rl))\n\t\treturn PTR_ERR(rl);\n\n\tnew_params = priv->channels.params;\n\tmlx5e_params_mqprio_channel_set(&new_params, mqprio, rl);\n\n\tnch_changed = mlx5e_get_dcb_num_tc(&priv->channels.params) > 1;\n\tpreactivate = nch_changed ? mlx5e_num_channels_changed_ctx :\n\t\tmlx5e_update_netdev_queues_ctx;\n\terr = mlx5e_safe_switch_params(priv, &new_params, preactivate, NULL, true);\n\tif (err) {\n\t\tif (rl) {\n\t\t\tmlx5e_mqprio_rl_cleanup(rl);\n\t\t\tmlx5e_mqprio_rl_free(rl);\n\t\t}\n\t\treturn err;\n\t}\n\n\tif (priv->mqprio_rl) {\n\t\tmlx5e_mqprio_rl_cleanup(priv->mqprio_rl);\n\t\tmlx5e_mqprio_rl_free(priv->mqprio_rl);\n\t}\n\tpriv->mqprio_rl = rl;\n\n\treturn 0;\n}\n\nstatic int mlx5e_setup_tc_mqprio(struct mlx5e_priv *priv,\n\t\t\t\t struct tc_mqprio_qopt_offload *mqprio)\n{\n\t \n\tif (WARN_ON(mlx5e_selq_is_htb_enabled(&priv->selq)))\n\t\treturn -EINVAL;\n\n\tswitch (mqprio->mode) {\n\tcase TC_MQPRIO_MODE_DCB:\n\t\treturn mlx5e_setup_tc_mqprio_dcb(priv, &mqprio->qopt);\n\tcase TC_MQPRIO_MODE_CHANNEL:\n\t\treturn mlx5e_setup_tc_mqprio_channel(priv, mqprio);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic LIST_HEAD(mlx5e_block_cb_list);\n\nstatic int mlx5e_setup_tc(struct net_device *dev, enum tc_setup_type type,\n\t\t\t  void *type_data)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tbool tc_unbind = false;\n\tint err;\n\n\tif (type == TC_SETUP_BLOCK &&\n\t    ((struct flow_block_offload *)type_data)->command == FLOW_BLOCK_UNBIND)\n\t\ttc_unbind = true;\n\n\tif (!netif_device_present(dev) && !tc_unbind)\n\t\treturn -ENODEV;\n\n\tswitch (type) {\n\tcase TC_SETUP_BLOCK: {\n\t\tstruct flow_block_offload *f = type_data;\n\n\t\tf->unlocked_driver_cb = true;\n\t\treturn flow_block_cb_setup_simple(type_data,\n\t\t\t\t\t\t  &mlx5e_block_cb_list,\n\t\t\t\t\t\t  mlx5e_setup_tc_block_cb,\n\t\t\t\t\t\t  priv, priv, true);\n\t}\n\tcase TC_SETUP_QDISC_MQPRIO:\n\t\tmutex_lock(&priv->state_lock);\n\t\terr = mlx5e_setup_tc_mqprio(priv, type_data);\n\t\tmutex_unlock(&priv->state_lock);\n\t\treturn err;\n\tcase TC_SETUP_QDISC_HTB:\n\t\tmutex_lock(&priv->state_lock);\n\t\terr = mlx5e_htb_setup_tc(priv, type_data);\n\t\tmutex_unlock(&priv->state_lock);\n\t\treturn err;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nvoid mlx5e_fold_sw_stats64(struct mlx5e_priv *priv, struct rtnl_link_stats64 *s)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->stats_nch; i++) {\n\t\tstruct mlx5e_channel_stats *channel_stats = priv->channel_stats[i];\n\t\tstruct mlx5e_rq_stats *xskrq_stats = &channel_stats->xskrq;\n\t\tstruct mlx5e_rq_stats *rq_stats = &channel_stats->rq;\n\t\tint j;\n\n\t\ts->rx_packets   += rq_stats->packets + xskrq_stats->packets;\n\t\ts->rx_bytes     += rq_stats->bytes + xskrq_stats->bytes;\n\t\ts->multicast    += rq_stats->mcast_packets + xskrq_stats->mcast_packets;\n\n\t\tfor (j = 0; j < priv->max_opened_tc; j++) {\n\t\t\tstruct mlx5e_sq_stats *sq_stats = &channel_stats->sq[j];\n\n\t\t\ts->tx_packets    += sq_stats->packets;\n\t\t\ts->tx_bytes      += sq_stats->bytes;\n\t\t\ts->tx_dropped    += sq_stats->dropped;\n\t\t}\n\t}\n\tif (priv->tx_ptp_opened) {\n\t\tfor (i = 0; i < priv->max_opened_tc; i++) {\n\t\t\tstruct mlx5e_sq_stats *sq_stats = &priv->ptp_stats.sq[i];\n\n\t\t\ts->tx_packets    += sq_stats->packets;\n\t\t\ts->tx_bytes      += sq_stats->bytes;\n\t\t\ts->tx_dropped    += sq_stats->dropped;\n\t\t}\n\t}\n\tif (priv->rx_ptp_opened) {\n\t\tstruct mlx5e_rq_stats *rq_stats = &priv->ptp_stats.rq;\n\n\t\ts->rx_packets   += rq_stats->packets;\n\t\ts->rx_bytes     += rq_stats->bytes;\n\t\ts->multicast    += rq_stats->mcast_packets;\n\t}\n}\n\nvoid\nmlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5e_pport_stats *pstats = &priv->stats.pport;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\t \n\tif (!mlx5e_monitor_counter_supported(priv) ||\n\t    mlx5e_is_uplink_rep(priv)) {\n\t\t \n\t\tmlx5e_queue_update_stats(priv);\n\t}\n\n\tif (mlx5e_is_uplink_rep(priv)) {\n\t\tstruct mlx5e_vport_stats *vstats = &priv->stats.vport;\n\n\t\tstats->rx_packets = PPORT_802_3_GET(pstats, a_frames_received_ok);\n\t\tstats->rx_bytes   = PPORT_802_3_GET(pstats, a_octets_received_ok);\n\t\tstats->tx_packets = PPORT_802_3_GET(pstats, a_frames_transmitted_ok);\n\t\tstats->tx_bytes   = PPORT_802_3_GET(pstats, a_octets_transmitted_ok);\n\n\t\t \n\t\tstats->multicast = VPORT_COUNTER_GET(vstats, received_eth_multicast.packets);\n\t} else {\n\t\tmlx5e_fold_sw_stats64(priv, stats);\n\t}\n\n\tstats->rx_dropped = priv->stats.qcnt.rx_out_of_buffer;\n\n\tstats->rx_length_errors =\n\t\tPPORT_802_3_GET(pstats, a_in_range_length_errors) +\n\t\tPPORT_802_3_GET(pstats, a_out_of_range_length_field) +\n\t\tPPORT_802_3_GET(pstats, a_frame_too_long_errors) +\n\t\tVNIC_ENV_GET(&priv->stats.vnic, eth_wqe_too_small);\n\tstats->rx_crc_errors =\n\t\tPPORT_802_3_GET(pstats, a_frame_check_sequence_errors);\n\tstats->rx_frame_errors = PPORT_802_3_GET(pstats, a_alignment_errors);\n\tstats->tx_aborted_errors = PPORT_2863_GET(pstats, if_out_discards);\n\tstats->rx_errors = stats->rx_length_errors + stats->rx_crc_errors +\n\t\t\t   stats->rx_frame_errors;\n\tstats->tx_errors = stats->tx_aborted_errors + stats->tx_carrier_errors;\n}\n\nstatic void mlx5e_nic_set_rx_mode(struct mlx5e_priv *priv)\n{\n\tif (mlx5e_is_uplink_rep(priv))\n\t\treturn;  \n\n\tqueue_work(priv->wq, &priv->set_rx_mode_work);\n}\n\nstatic void mlx5e_set_rx_mode(struct net_device *dev)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\n\tmlx5e_nic_set_rx_mode(priv);\n}\n\nstatic int mlx5e_set_mac(struct net_device *netdev, void *addr)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct sockaddr *saddr = addr;\n\n\tif (!is_valid_ether_addr(saddr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\tnetif_addr_lock_bh(netdev);\n\teth_hw_addr_set(netdev, saddr->sa_data);\n\tnetif_addr_unlock_bh(netdev);\n\n\tmlx5e_nic_set_rx_mode(priv);\n\n\treturn 0;\n}\n\n#define MLX5E_SET_FEATURE(features, feature, enable)\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (enable)\t\t\t\t\\\n\t\t\t*features |= feature;\t\t\\\n\t\telse\t\t\t\t\t\\\n\t\t\t*features &= ~feature;\t\t\\\n\t} while (0)\n\ntypedef int (*mlx5e_feature_handler)(struct net_device *netdev, bool enable);\n\nstatic int set_feature_lro(struct net_device *netdev, bool enable)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tstruct mlx5e_params *cur_params;\n\tstruct mlx5e_params new_params;\n\tbool reset = true;\n\tint err = 0;\n\n\tmutex_lock(&priv->state_lock);\n\n\tcur_params = &priv->channels.params;\n\tnew_params = *cur_params;\n\n\tif (enable)\n\t\tnew_params.packet_merge.type = MLX5E_PACKET_MERGE_LRO;\n\telse if (new_params.packet_merge.type == MLX5E_PACKET_MERGE_LRO)\n\t\tnew_params.packet_merge.type = MLX5E_PACKET_MERGE_NONE;\n\telse\n\t\tgoto out;\n\n\tif (!(cur_params->packet_merge.type == MLX5E_PACKET_MERGE_SHAMPO &&\n\t      new_params.packet_merge.type == MLX5E_PACKET_MERGE_LRO)) {\n\t\tif (cur_params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ) {\n\t\t\tif (mlx5e_rx_mpwqe_is_linear_skb(mdev, cur_params, NULL) ==\n\t\t\t    mlx5e_rx_mpwqe_is_linear_skb(mdev, &new_params, NULL))\n\t\t\t\treset = false;\n\t\t}\n\t}\n\n\terr = mlx5e_safe_switch_params(priv, &new_params,\n\t\t\t\t       mlx5e_modify_tirs_packet_merge_ctx, NULL, reset);\nout:\n\tmutex_unlock(&priv->state_lock);\n\treturn err;\n}\n\nstatic int set_feature_hw_gro(struct net_device *netdev, bool enable)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5e_params new_params;\n\tbool reset = true;\n\tint err = 0;\n\n\tmutex_lock(&priv->state_lock);\n\tnew_params = priv->channels.params;\n\n\tif (enable) {\n\t\tnew_params.packet_merge.type = MLX5E_PACKET_MERGE_SHAMPO;\n\t\tnew_params.packet_merge.shampo.match_criteria_type =\n\t\t\tMLX5_RQC_SHAMPO_MATCH_CRITERIA_TYPE_EXTENDED;\n\t\tnew_params.packet_merge.shampo.alignment_granularity =\n\t\t\tMLX5_RQC_SHAMPO_NO_MATCH_ALIGNMENT_GRANULARITY_STRIDE;\n\t} else if (new_params.packet_merge.type == MLX5E_PACKET_MERGE_SHAMPO) {\n\t\tnew_params.packet_merge.type = MLX5E_PACKET_MERGE_NONE;\n\t} else {\n\t\tgoto out;\n\t}\n\n\terr = mlx5e_safe_switch_params(priv, &new_params, NULL, NULL, reset);\nout:\n\tmutex_unlock(&priv->state_lock);\n\treturn err;\n}\n\nstatic int set_feature_cvlan_filter(struct net_device *netdev, bool enable)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\n\tif (enable)\n\t\tmlx5e_enable_cvlan_filter(priv->fs,\n\t\t\t\t\t  !!(priv->netdev->flags & IFF_PROMISC));\n\telse\n\t\tmlx5e_disable_cvlan_filter(priv->fs,\n\t\t\t\t\t   !!(priv->netdev->flags & IFF_PROMISC));\n\n\treturn 0;\n}\n\nstatic int set_feature_hw_tc(struct net_device *netdev, bool enable)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tint err = 0;\n\n#if IS_ENABLED(CONFIG_MLX5_CLS_ACT)\n\tint tc_flag = mlx5e_is_uplink_rep(priv) ? MLX5_TC_FLAG(ESW_OFFLOAD) :\n\t\t\t\t\t\t  MLX5_TC_FLAG(NIC_OFFLOAD);\n\tif (!enable && mlx5e_tc_num_filters(priv, tc_flag)) {\n\t\tnetdev_err(netdev,\n\t\t\t   \"Active offloaded tc filters, can't turn hw_tc_offload off\\n\");\n\t\treturn -EINVAL;\n\t}\n#endif\n\n\tmutex_lock(&priv->state_lock);\n\tif (!enable && mlx5e_selq_is_htb_enabled(&priv->selq)) {\n\t\tnetdev_err(netdev, \"Active HTB offload, can't turn hw_tc_offload off\\n\");\n\t\terr = -EINVAL;\n\t}\n\tmutex_unlock(&priv->state_lock);\n\n\treturn err;\n}\n\nstatic int set_feature_rx_all(struct net_device *netdev, bool enable)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\treturn mlx5_set_port_fcs(mdev, !enable);\n}\n\nstatic int mlx5e_set_rx_port_ts(struct mlx5_core_dev *mdev, bool enable)\n{\n\tu32 in[MLX5_ST_SZ_DW(pcmr_reg)] = {};\n\tbool supported, curr_state;\n\tint err;\n\n\tif (!MLX5_CAP_GEN(mdev, ports_check))\n\t\treturn 0;\n\n\terr = mlx5_query_ports_check(mdev, in, sizeof(in));\n\tif (err)\n\t\treturn err;\n\n\tsupported = MLX5_GET(pcmr_reg, in, rx_ts_over_crc_cap);\n\tcurr_state = MLX5_GET(pcmr_reg, in, rx_ts_over_crc);\n\n\tif (!supported || enable == curr_state)\n\t\treturn 0;\n\n\tMLX5_SET(pcmr_reg, in, local_port, 1);\n\tMLX5_SET(pcmr_reg, in, rx_ts_over_crc, enable);\n\n\treturn mlx5_set_ports_check(mdev, in, sizeof(in));\n}\n\nstatic int mlx5e_set_rx_port_ts_wrap(struct mlx5e_priv *priv, void *ctx)\n{\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tbool enable = *(bool *)ctx;\n\n\treturn mlx5e_set_rx_port_ts(mdev, enable);\n}\n\nstatic int set_feature_rx_fcs(struct net_device *netdev, bool enable)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5e_channels *chs = &priv->channels;\n\tstruct mlx5e_params new_params;\n\tint err;\n\tbool rx_ts_over_crc = !enable;\n\n\tmutex_lock(&priv->state_lock);\n\n\tnew_params = chs->params;\n\tnew_params.scatter_fcs_en = enable;\n\terr = mlx5e_safe_switch_params(priv, &new_params, mlx5e_set_rx_port_ts_wrap,\n\t\t\t\t       &rx_ts_over_crc, true);\n\tmutex_unlock(&priv->state_lock);\n\treturn err;\n}\n\nstatic int set_feature_rx_vlan(struct net_device *netdev, bool enable)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tint err = 0;\n\n\tmutex_lock(&priv->state_lock);\n\n\tmlx5e_fs_set_vlan_strip_disable(priv->fs, !enable);\n\tpriv->channels.params.vlan_strip_disable = !enable;\n\n\tif (!test_bit(MLX5E_STATE_OPENED, &priv->state))\n\t\tgoto unlock;\n\n\terr = mlx5e_modify_channels_vsd(&priv->channels, !enable);\n\tif (err) {\n\t\tmlx5e_fs_set_vlan_strip_disable(priv->fs, enable);\n\t\tpriv->channels.params.vlan_strip_disable = enable;\n\t}\nunlock:\n\tmutex_unlock(&priv->state_lock);\n\n\treturn err;\n}\n\nint mlx5e_vlan_rx_add_vid(struct net_device *dev, __be16 proto, u16 vid)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5e_flow_steering *fs = priv->fs;\n\n\tif (mlx5e_is_uplink_rep(priv))\n\t\treturn 0;  \n\n\treturn mlx5e_fs_vlan_rx_add_vid(fs, dev, proto, vid);\n}\n\nint mlx5e_vlan_rx_kill_vid(struct net_device *dev, __be16 proto, u16 vid)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5e_flow_steering *fs = priv->fs;\n\n\tif (mlx5e_is_uplink_rep(priv))\n\t\treturn 0;  \n\n\treturn mlx5e_fs_vlan_rx_kill_vid(fs, dev, proto, vid);\n}\n\n#ifdef CONFIG_MLX5_EN_ARFS\nstatic int set_feature_arfs(struct net_device *netdev, bool enable)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tint err;\n\n\tif (enable)\n\t\terr = mlx5e_arfs_enable(priv->fs);\n\telse\n\t\terr = mlx5e_arfs_disable(priv->fs);\n\n\treturn err;\n}\n#endif\n\nstatic int mlx5e_handle_feature(struct net_device *netdev,\n\t\t\t\tnetdev_features_t *features,\n\t\t\t\tnetdev_features_t feature,\n\t\t\t\tmlx5e_feature_handler feature_handler)\n{\n\tnetdev_features_t changes = *features ^ netdev->features;\n\tbool enable = !!(*features & feature);\n\tint err;\n\n\tif (!(changes & feature))\n\t\treturn 0;\n\n\terr = feature_handler(netdev, enable);\n\tif (err) {\n\t\tMLX5E_SET_FEATURE(features, feature, !enable);\n\t\tnetdev_err(netdev, \"%s feature %pNF failed, err %d\\n\",\n\t\t\t   enable ? \"Enable\" : \"Disable\", &feature, err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nvoid mlx5e_set_xdp_feature(struct net_device *netdev)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5e_params *params = &priv->channels.params;\n\txdp_features_t val;\n\n\tif (params->packet_merge.type != MLX5E_PACKET_MERGE_NONE) {\n\t\txdp_clear_features_flag(netdev);\n\t\treturn;\n\t}\n\n\tval = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |\n\t      NETDEV_XDP_ACT_XSK_ZEROCOPY |\n\t      NETDEV_XDP_ACT_RX_SG |\n\t      NETDEV_XDP_ACT_NDO_XMIT |\n\t      NETDEV_XDP_ACT_NDO_XMIT_SG;\n\txdp_set_features_flag(netdev, val);\n}\n\nint mlx5e_set_features(struct net_device *netdev, netdev_features_t features)\n{\n\tnetdev_features_t oper_features = features;\n\tint err = 0;\n\n#define MLX5E_HANDLE_FEATURE(feature, handler) \\\n\tmlx5e_handle_feature(netdev, &oper_features, feature, handler)\n\n\terr |= MLX5E_HANDLE_FEATURE(NETIF_F_LRO, set_feature_lro);\n\terr |= MLX5E_HANDLE_FEATURE(NETIF_F_GRO_HW, set_feature_hw_gro);\n\terr |= MLX5E_HANDLE_FEATURE(NETIF_F_HW_VLAN_CTAG_FILTER,\n\t\t\t\t    set_feature_cvlan_filter);\n\terr |= MLX5E_HANDLE_FEATURE(NETIF_F_HW_TC, set_feature_hw_tc);\n\terr |= MLX5E_HANDLE_FEATURE(NETIF_F_RXALL, set_feature_rx_all);\n\terr |= MLX5E_HANDLE_FEATURE(NETIF_F_RXFCS, set_feature_rx_fcs);\n\terr |= MLX5E_HANDLE_FEATURE(NETIF_F_HW_VLAN_CTAG_RX, set_feature_rx_vlan);\n#ifdef CONFIG_MLX5_EN_ARFS\n\terr |= MLX5E_HANDLE_FEATURE(NETIF_F_NTUPLE, set_feature_arfs);\n#endif\n\terr |= MLX5E_HANDLE_FEATURE(NETIF_F_HW_TLS_RX, mlx5e_ktls_set_feature_rx);\n\n\tif (err) {\n\t\tnetdev->features = oper_features;\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmlx5e_set_xdp_feature(netdev);\n\n\treturn 0;\n}\n\nstatic netdev_features_t mlx5e_fix_uplink_rep_features(struct net_device *netdev,\n\t\t\t\t\t\t       netdev_features_t features)\n{\n\tfeatures &= ~NETIF_F_HW_TLS_RX;\n\tif (netdev->features & NETIF_F_HW_TLS_RX)\n\t\tnetdev_warn(netdev, \"Disabling hw_tls_rx, not supported in switchdev mode\\n\");\n\n\tfeatures &= ~NETIF_F_HW_TLS_TX;\n\tif (netdev->features & NETIF_F_HW_TLS_TX)\n\t\tnetdev_warn(netdev, \"Disabling hw_tls_tx, not supported in switchdev mode\\n\");\n\n\tfeatures &= ~NETIF_F_NTUPLE;\n\tif (netdev->features & NETIF_F_NTUPLE)\n\t\tnetdev_warn(netdev, \"Disabling ntuple, not supported in switchdev mode\\n\");\n\n\tfeatures &= ~NETIF_F_GRO_HW;\n\tif (netdev->features & NETIF_F_GRO_HW)\n\t\tnetdev_warn(netdev, \"Disabling HW_GRO, not supported in switchdev mode\\n\");\n\n\tfeatures &= ~NETIF_F_HW_VLAN_CTAG_FILTER;\n\tif (netdev->features & NETIF_F_HW_VLAN_CTAG_FILTER)\n\t\tnetdev_warn(netdev, \"Disabling HW_VLAN CTAG FILTERING, not supported in switchdev mode\\n\");\n\n\treturn features;\n}\n\nstatic netdev_features_t mlx5e_fix_features(struct net_device *netdev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5e_vlan_table *vlan;\n\tstruct mlx5e_params *params;\n\n\tif (!netif_device_present(netdev))\n\t\treturn features;\n\n\tvlan = mlx5e_fs_get_vlan(priv->fs);\n\tmutex_lock(&priv->state_lock);\n\tparams = &priv->channels.params;\n\tif (!vlan ||\n\t    !bitmap_empty(mlx5e_vlan_get_active_svlans(vlan), VLAN_N_VID)) {\n\t\t \n\t\tfeatures &= ~NETIF_F_HW_VLAN_CTAG_RX;\n\t\tif (!params->vlan_strip_disable)\n\t\t\tnetdev_warn(netdev, \"Dropping C-tag vlan stripping offload due to S-tag vlan\\n\");\n\t}\n\n\tif (!MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_STRIDING_RQ)) {\n\t\tif (features & NETIF_F_LRO) {\n\t\t\tnetdev_warn(netdev, \"Disabling LRO, not supported in legacy RQ\\n\");\n\t\t\tfeatures &= ~NETIF_F_LRO;\n\t\t}\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_warn(netdev, \"Disabling HW-GRO, not supported in legacy RQ\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif (params->xdp_prog) {\n\t\tif (features & NETIF_F_LRO) {\n\t\t\tnetdev_warn(netdev, \"LRO is incompatible with XDP\\n\");\n\t\t\tfeatures &= ~NETIF_F_LRO;\n\t\t}\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_warn(netdev, \"HW GRO is incompatible with XDP\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif (priv->xsk.refcnt) {\n\t\tif (features & NETIF_F_LRO) {\n\t\t\tnetdev_warn(netdev, \"LRO is incompatible with AF_XDP (%u XSKs are active)\\n\",\n\t\t\t\t    priv->xsk.refcnt);\n\t\t\tfeatures &= ~NETIF_F_LRO;\n\t\t}\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_warn(netdev, \"HW GRO is incompatible with AF_XDP (%u XSKs are active)\\n\",\n\t\t\t\t    priv->xsk.refcnt);\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif (MLX5E_GET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS)) {\n\t\tfeatures &= ~NETIF_F_RXHASH;\n\t\tif (netdev->features & NETIF_F_RXHASH)\n\t\t\tnetdev_warn(netdev, \"Disabling rxhash, not supported when CQE compress is active\\n\");\n\n\t\tif (features & NETIF_F_GRO_HW) {\n\t\t\tnetdev_warn(netdev, \"Disabling HW-GRO, not supported when CQE compress is active\\n\");\n\t\t\tfeatures &= ~NETIF_F_GRO_HW;\n\t\t}\n\t}\n\n\tif (mlx5e_is_uplink_rep(priv)) {\n\t\tfeatures = mlx5e_fix_uplink_rep_features(netdev, features);\n\t\tfeatures |= NETIF_F_NETNS_LOCAL;\n\t} else {\n\t\tfeatures &= ~NETIF_F_NETNS_LOCAL;\n\t}\n\n\tmutex_unlock(&priv->state_lock);\n\n\treturn features;\n}\n\nstatic bool mlx5e_xsk_validate_mtu(struct net_device *netdev,\n\t\t\t\t   struct mlx5e_channels *chs,\n\t\t\t\t   struct mlx5e_params *new_params,\n\t\t\t\t   struct mlx5_core_dev *mdev)\n{\n\tu16 ix;\n\n\tfor (ix = 0; ix < chs->params.num_channels; ix++) {\n\t\tstruct xsk_buff_pool *xsk_pool =\n\t\t\tmlx5e_xsk_get_pool(&chs->params, chs->params.xsk, ix);\n\t\tstruct mlx5e_xsk_param xsk;\n\t\tint max_xdp_mtu;\n\n\t\tif (!xsk_pool)\n\t\t\tcontinue;\n\n\t\tmlx5e_build_xsk_param(xsk_pool, &xsk);\n\t\tmax_xdp_mtu = mlx5e_xdp_max_mtu(new_params, &xsk);\n\n\t\t \n\t\tif (!mlx5e_validate_xsk_param(new_params, &xsk, mdev) ||\n\t\t    new_params->sw_mtu > max_xdp_mtu) {\n\t\t\tu32 hr = mlx5e_get_linear_rq_headroom(new_params, &xsk);\n\t\t\tint max_mtu_frame, max_mtu_page, max_mtu;\n\n\t\t\t \n\t\t\tmax_mtu_frame = MLX5E_HW2SW_MTU(new_params, xsk.chunk_size - hr);\n\t\t\tmax_mtu_page = MLX5E_HW2SW_MTU(new_params, SKB_MAX_HEAD(0));\n\t\t\tmax_mtu = min3(max_mtu_frame, max_mtu_page, max_xdp_mtu);\n\n\t\t\tnetdev_err(netdev, \"MTU %d is too big for an XSK running on channel %u or its redirection XDP program. Try MTU <= %d\\n\",\n\t\t\t\t   new_params->sw_mtu, ix, max_mtu);\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool mlx5e_params_validate_xdp(struct net_device *netdev,\n\t\t\t\t      struct mlx5_core_dev *mdev,\n\t\t\t\t      struct mlx5e_params *params)\n{\n\tbool is_linear;\n\n\t \n\tis_linear = params->rq_wq_type == MLX5_WQ_TYPE_CYCLIC ?\n\t\tmlx5e_rx_is_linear_skb(mdev, params, NULL) :\n\t\tmlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL);\n\n\tif (!is_linear) {\n\t\tif (!params->xdp_prog->aux->xdp_has_frags) {\n\t\t\tnetdev_warn(netdev, \"MTU(%d) > %d, too big for an XDP program not aware of multi buffer\\n\",\n\t\t\t\t    params->sw_mtu,\n\t\t\t\t    mlx5e_xdp_max_mtu(params, NULL));\n\t\t\treturn false;\n\t\t}\n\t\tif (params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ &&\n\t\t    !mlx5e_verify_params_rx_mpwqe_strides(mdev, params, NULL)) {\n\t\t\tnetdev_warn(netdev, \"XDP is not allowed with striding RQ and MTU(%d) > %d\\n\",\n\t\t\t\t    params->sw_mtu,\n\t\t\t\t    mlx5e_xdp_max_mtu(params, NULL));\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nint mlx5e_change_mtu(struct net_device *netdev, int new_mtu,\n\t\t     mlx5e_fp_preactivate preactivate)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5e_params new_params;\n\tstruct mlx5e_params *params;\n\tbool reset = true;\n\tint err = 0;\n\n\tmutex_lock(&priv->state_lock);\n\n\tparams = &priv->channels.params;\n\n\tnew_params = *params;\n\tnew_params.sw_mtu = new_mtu;\n\terr = mlx5e_validate_params(priv->mdev, &new_params);\n\tif (err)\n\t\tgoto out;\n\n\tif (new_params.xdp_prog && !mlx5e_params_validate_xdp(netdev, priv->mdev,\n\t\t\t\t\t\t\t      &new_params)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (priv->xsk.refcnt &&\n\t    !mlx5e_xsk_validate_mtu(netdev, &priv->channels,\n\t\t\t\t    &new_params, priv->mdev)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (params->packet_merge.type == MLX5E_PACKET_MERGE_LRO)\n\t\treset = false;\n\n\tif (params->rq_wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ &&\n\t    params->packet_merge.type != MLX5E_PACKET_MERGE_SHAMPO) {\n\t\tbool is_linear_old = mlx5e_rx_mpwqe_is_linear_skb(priv->mdev, params, NULL);\n\t\tbool is_linear_new = mlx5e_rx_mpwqe_is_linear_skb(priv->mdev,\n\t\t\t\t\t\t\t\t  &new_params, NULL);\n\t\tu8 sz_old = mlx5e_mpwqe_get_log_rq_size(priv->mdev, params, NULL);\n\t\tu8 sz_new = mlx5e_mpwqe_get_log_rq_size(priv->mdev, &new_params, NULL);\n\n\t\t \n\t\tif (!is_linear_old && !is_linear_new && !priv->xsk.refcnt &&\n\t\t    sz_old == sz_new)\n\t\t\treset = false;\n\t}\n\n\terr = mlx5e_safe_switch_params(priv, &new_params, preactivate, NULL, reset);\n\nout:\n\tnetdev->mtu = params->sw_mtu;\n\tmutex_unlock(&priv->state_lock);\n\treturn err;\n}\n\nstatic int mlx5e_change_nic_mtu(struct net_device *netdev, int new_mtu)\n{\n\treturn mlx5e_change_mtu(netdev, new_mtu, mlx5e_set_dev_port_mtu_ctx);\n}\n\nint mlx5e_ptp_rx_manage_fs_ctx(struct mlx5e_priv *priv, void *ctx)\n{\n\tbool set  = *(bool *)ctx;\n\n\treturn mlx5e_ptp_rx_manage_fs(priv, set);\n}\n\nstatic int mlx5e_hwstamp_config_no_ptp_rx(struct mlx5e_priv *priv, bool rx_filter)\n{\n\tbool rx_cqe_compress_def = priv->channels.params.rx_cqe_compress_def;\n\tint err;\n\n\tif (!rx_filter)\n\t\t \n\t\treturn mlx5e_modify_rx_cqe_compression_locked(priv, rx_cqe_compress_def, false);\n\n\tif (!MLX5E_GET_PFLAG(&priv->channels.params, MLX5E_PFLAG_RX_CQE_COMPRESS))\n\t\treturn 0;\n\n\t \n\tnetdev_warn(priv->netdev, \"Disabling RX cqe compression\\n\");\n\terr = mlx5e_modify_rx_cqe_compression_locked(priv, false, true);\n\tif (err)\n\t\tnetdev_err(priv->netdev, \"Failed disabling cqe compression err=%d\\n\", err);\n\n\treturn err;\n}\n\nstatic int mlx5e_hwstamp_config_ptp_rx(struct mlx5e_priv *priv, bool ptp_rx)\n{\n\tstruct mlx5e_params new_params;\n\n\tif (ptp_rx == priv->channels.params.ptp_rx)\n\t\treturn 0;\n\n\tnew_params = priv->channels.params;\n\tnew_params.ptp_rx = ptp_rx;\n\treturn mlx5e_safe_switch_params(priv, &new_params, mlx5e_ptp_rx_manage_fs_ctx,\n\t\t\t\t\t&new_params.ptp_rx, true);\n}\n\nint mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr)\n{\n\tstruct hwtstamp_config config;\n\tbool rx_cqe_compress_def;\n\tbool ptp_rx;\n\tint err;\n\n\tif (!MLX5_CAP_GEN(priv->mdev, device_frequency_khz) ||\n\t    (mlx5_clock_get_ptp_index(priv->mdev) == -1))\n\t\treturn -EOPNOTSUPP;\n\n\tif (copy_from_user(&config, ifr->ifr_data, sizeof(config)))\n\t\treturn -EFAULT;\n\n\t \n\tswitch (config.tx_type) {\n\tcase HWTSTAMP_TX_OFF:\n\tcase HWTSTAMP_TX_ON:\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tmutex_lock(&priv->state_lock);\n\trx_cqe_compress_def = priv->channels.params.rx_cqe_compress_def;\n\n\t \n\tswitch (config.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\tptp_rx = false;\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_ALL:\n\tcase HWTSTAMP_FILTER_SOME:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_NTP_ALL:\n\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t\t \n\t\tptp_rx = rx_cqe_compress_def;\n\t\tbreak;\n\tdefault:\n\t\terr = -ERANGE;\n\t\tgoto err_unlock;\n\t}\n\n\tif (!mlx5e_profile_feature_cap(priv->profile, PTP_RX))\n\t\terr = mlx5e_hwstamp_config_no_ptp_rx(priv,\n\t\t\t\t\t\t     config.rx_filter != HWTSTAMP_FILTER_NONE);\n\telse\n\t\terr = mlx5e_hwstamp_config_ptp_rx(priv, ptp_rx);\n\tif (err)\n\t\tgoto err_unlock;\n\n\tmemcpy(&priv->tstamp, &config, sizeof(config));\n\tmutex_unlock(&priv->state_lock);\n\n\t \n\tnetdev_update_features(priv->netdev);\n\n\treturn copy_to_user(ifr->ifr_data, &config,\n\t\t\t    sizeof(config)) ? -EFAULT : 0;\nerr_unlock:\n\tmutex_unlock(&priv->state_lock);\n\treturn err;\n}\n\nint mlx5e_hwstamp_get(struct mlx5e_priv *priv, struct ifreq *ifr)\n{\n\tstruct hwtstamp_config *cfg = &priv->tstamp;\n\n\tif (!MLX5_CAP_GEN(priv->mdev, device_frequency_khz))\n\t\treturn -EOPNOTSUPP;\n\n\treturn copy_to_user(ifr->ifr_data, cfg, sizeof(*cfg)) ? -EFAULT : 0;\n}\n\nstatic int mlx5e_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\n\tswitch (cmd) {\n\tcase SIOCSHWTSTAMP:\n\t\treturn mlx5e_hwstamp_set(priv, ifr);\n\tcase SIOCGHWTSTAMP:\n\t\treturn mlx5e_hwstamp_get(priv, ifr);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\n#ifdef CONFIG_MLX5_ESWITCH\nint mlx5e_set_vf_mac(struct net_device *dev, int vf, u8 *mac)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\treturn mlx5_eswitch_set_vport_mac(mdev->priv.eswitch, vf + 1, mac);\n}\n\nstatic int mlx5e_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos,\n\t\t\t     __be16 vlan_proto)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\tif (vlan_proto != htons(ETH_P_8021Q))\n\t\treturn -EPROTONOSUPPORT;\n\n\treturn mlx5_eswitch_set_vport_vlan(mdev->priv.eswitch, vf + 1,\n\t\t\t\t\t   vlan, qos);\n}\n\nstatic int mlx5e_set_vf_spoofchk(struct net_device *dev, int vf, bool setting)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\treturn mlx5_eswitch_set_vport_spoofchk(mdev->priv.eswitch, vf + 1, setting);\n}\n\nstatic int mlx5e_set_vf_trust(struct net_device *dev, int vf, bool setting)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\treturn mlx5_eswitch_set_vport_trust(mdev->priv.eswitch, vf + 1, setting);\n}\n\nint mlx5e_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate,\n\t\t      int max_tx_rate)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\treturn mlx5_eswitch_set_vport_rate(mdev->priv.eswitch, vf + 1,\n\t\t\t\t\t   max_tx_rate, min_tx_rate);\n}\n\nstatic int mlx5_vport_link2ifla(u8 esw_link)\n{\n\tswitch (esw_link) {\n\tcase MLX5_VPORT_ADMIN_STATE_DOWN:\n\t\treturn IFLA_VF_LINK_STATE_DISABLE;\n\tcase MLX5_VPORT_ADMIN_STATE_UP:\n\t\treturn IFLA_VF_LINK_STATE_ENABLE;\n\t}\n\treturn IFLA_VF_LINK_STATE_AUTO;\n}\n\nstatic int mlx5_ifla_link2vport(u8 ifla_link)\n{\n\tswitch (ifla_link) {\n\tcase IFLA_VF_LINK_STATE_DISABLE:\n\t\treturn MLX5_VPORT_ADMIN_STATE_DOWN;\n\tcase IFLA_VF_LINK_STATE_ENABLE:\n\t\treturn MLX5_VPORT_ADMIN_STATE_UP;\n\t}\n\treturn MLX5_VPORT_ADMIN_STATE_AUTO;\n}\n\nstatic int mlx5e_set_vf_link_state(struct net_device *dev, int vf,\n\t\t\t\t   int link_state)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\tif (mlx5e_is_uplink_rep(priv))\n\t\treturn -EOPNOTSUPP;\n\n\treturn mlx5_eswitch_set_vport_state(mdev->priv.eswitch, vf + 1,\n\t\t\t\t\t    mlx5_ifla_link2vport(link_state));\n}\n\nint mlx5e_get_vf_config(struct net_device *dev,\n\t\t\tint vf, struct ifla_vf_info *ivi)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tint err;\n\n\tif (!netif_device_present(dev))\n\t\treturn -EOPNOTSUPP;\n\n\terr = mlx5_eswitch_get_vport_config(mdev->priv.eswitch, vf + 1, ivi);\n\tif (err)\n\t\treturn err;\n\tivi->linkstate = mlx5_vport_link2ifla(ivi->linkstate);\n\treturn 0;\n}\n\nint mlx5e_get_vf_stats(struct net_device *dev,\n\t\t       int vf, struct ifla_vf_stats *vf_stats)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\treturn mlx5_eswitch_get_vport_stats(mdev->priv.eswitch, vf + 1,\n\t\t\t\t\t    vf_stats);\n}\n\nstatic bool\nmlx5e_has_offload_stats(const struct net_device *dev, int attr_id)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\n\tif (!netif_device_present(dev))\n\t\treturn false;\n\n\tif (!mlx5e_is_uplink_rep(priv))\n\t\treturn false;\n\n\treturn mlx5e_rep_has_offload_stats(dev, attr_id);\n}\n\nstatic int\nmlx5e_get_offload_stats(int attr_id, const struct net_device *dev,\n\t\t\tvoid *sp)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\n\tif (!mlx5e_is_uplink_rep(priv))\n\t\treturn -EOPNOTSUPP;\n\n\treturn mlx5e_rep_get_offload_stats(attr_id, dev, sp);\n}\n#endif\n\nstatic bool mlx5e_tunnel_proto_supported_tx(struct mlx5_core_dev *mdev, u8 proto_type)\n{\n\tswitch (proto_type) {\n\tcase IPPROTO_GRE:\n\t\treturn MLX5_CAP_ETH(mdev, tunnel_stateless_gre);\n\tcase IPPROTO_IPIP:\n\tcase IPPROTO_IPV6:\n\t\treturn (MLX5_CAP_ETH(mdev, tunnel_stateless_ip_over_ip) ||\n\t\t\tMLX5_CAP_ETH(mdev, tunnel_stateless_ip_over_ip_tx));\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic bool mlx5e_gre_tunnel_inner_proto_offload_supported(struct mlx5_core_dev *mdev,\n\t\t\t\t\t\t\t   struct sk_buff *skb)\n{\n\tswitch (skb->inner_protocol) {\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_TEB):\n\t\treturn true;\n\tcase htons(ETH_P_MPLS_UC):\n\tcase htons(ETH_P_MPLS_MC):\n\t\treturn MLX5_CAP_ETH(mdev, tunnel_stateless_mpls_over_gre);\n\t}\n\treturn false;\n}\n\nstatic netdev_features_t mlx5e_tunnel_features_check(struct mlx5e_priv *priv,\n\t\t\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t\t\t     netdev_features_t features)\n{\n\tunsigned int offset = 0;\n\tstruct udphdr *udph;\n\tu8 proto;\n\tu16 port;\n\n\tswitch (vlan_get_protocol(skb)) {\n\tcase htons(ETH_P_IP):\n\t\tproto = ip_hdr(skb)->protocol;\n\t\tbreak;\n\tcase htons(ETH_P_IPV6):\n\t\tproto = ipv6_find_hdr(skb, &offset, -1, NULL, NULL);\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (proto) {\n\tcase IPPROTO_GRE:\n\t\tif (mlx5e_gre_tunnel_inner_proto_offload_supported(priv->mdev, skb))\n\t\t\treturn features;\n\t\tbreak;\n\tcase IPPROTO_IPIP:\n\tcase IPPROTO_IPV6:\n\t\tif (mlx5e_tunnel_proto_supported_tx(priv->mdev, IPPROTO_IPIP))\n\t\t\treturn features;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tudph = udp_hdr(skb);\n\t\tport = be16_to_cpu(udph->dest);\n\n\t\t \n\t\tif (mlx5_vxlan_lookup_port(priv->mdev->vxlan, port))\n\t\t\treturn features;\n\n#if IS_ENABLED(CONFIG_GENEVE)\n\t\t \n\t\tif (port == GENEVE_UDP_PORT && mlx5_geneve_tx_allowed(priv->mdev))\n\t\t\treturn features;\n#endif\n\t\tbreak;\n#ifdef CONFIG_MLX5_EN_IPSEC\n\tcase IPPROTO_ESP:\n\t\treturn mlx5e_ipsec_feature_check(skb, features);\n#endif\n\t}\n\nout:\n\t \n\treturn features & ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n}\n\nnetdev_features_t mlx5e_features_check(struct sk_buff *skb,\n\t\t\t\t       struct net_device *netdev,\n\t\t\t\t       netdev_features_t features)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\n\tfeatures = vlan_features_check(skb, features);\n\tfeatures = vxlan_features_check(skb, features);\n\n\t \n\tif (skb->encapsulation &&\n\t    (features & NETIF_F_CSUM_MASK || features & NETIF_F_GSO_MASK))\n\t\treturn mlx5e_tunnel_features_check(priv, skb, features);\n\n\treturn features;\n}\n\nstatic void mlx5e_tx_timeout_work(struct work_struct *work)\n{\n\tstruct mlx5e_priv *priv = container_of(work, struct mlx5e_priv,\n\t\t\t\t\t       tx_timeout_work);\n\tstruct net_device *netdev = priv->netdev;\n\tint i;\n\n\t \n\twhile (!rtnl_trylock()) {\n\t\tif (!test_bit(MLX5E_STATE_CHANNELS_ACTIVE, &priv->state))\n\t\t\treturn;\n\t\tmsleep(20);\n\t}\n\n\tif (!test_bit(MLX5E_STATE_OPENED, &priv->state))\n\t\tgoto unlock;\n\n\tfor (i = 0; i < netdev->real_num_tx_queues; i++) {\n\t\tstruct netdev_queue *dev_queue =\n\t\t\tnetdev_get_tx_queue(netdev, i);\n\t\tstruct mlx5e_txqsq *sq = priv->txq2sq[i];\n\n\t\tif (!netif_xmit_stopped(dev_queue))\n\t\t\tcontinue;\n\n\t\tif (mlx5e_reporter_tx_timeout(sq))\n\t\t \n\t\t\tbreak;\n\t}\n\nunlock:\n\trtnl_unlock();\n}\n\nstatic void mlx5e_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\n\tnetdev_err(dev, \"TX timeout detected\\n\");\n\tqueue_work(priv->wq, &priv->tx_timeout_work);\n}\n\nstatic int mlx5e_xdp_allowed(struct net_device *netdev, struct mlx5_core_dev *mdev,\n\t\t\t     struct mlx5e_params *params)\n{\n\tif (params->packet_merge.type != MLX5E_PACKET_MERGE_NONE) {\n\t\tnetdev_warn(netdev, \"can't set XDP while HW-GRO/LRO is on, disable them first\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!mlx5e_params_validate_xdp(netdev, mdev, params))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void mlx5e_rq_replace_xdp_prog(struct mlx5e_rq *rq, struct bpf_prog *prog)\n{\n\tstruct bpf_prog *old_prog;\n\n\told_prog = rcu_replace_pointer(rq->xdp_prog, prog,\n\t\t\t\t       lockdep_is_held(&rq->priv->state_lock));\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n}\n\nstatic int mlx5e_xdp_set(struct net_device *netdev, struct bpf_prog *prog)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5e_params new_params;\n\tstruct bpf_prog *old_prog;\n\tint err = 0;\n\tbool reset;\n\tint i;\n\n\tmutex_lock(&priv->state_lock);\n\n\tnew_params = priv->channels.params;\n\tnew_params.xdp_prog = prog;\n\n\tif (prog) {\n\t\terr = mlx5e_xdp_allowed(netdev, priv->mdev, &new_params);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t}\n\n\t \n\treset = (!priv->channels.params.xdp_prog || !prog);\n\n\told_prog = priv->channels.params.xdp_prog;\n\n\terr = mlx5e_safe_switch_params(priv, &new_params, NULL, NULL, reset);\n\tif (err)\n\t\tgoto unlock;\n\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\tif (!test_bit(MLX5E_STATE_OPENED, &priv->state) || reset)\n\t\tgoto unlock;\n\n\t \n\tbpf_prog_add(prog, priv->channels.num);\n\tfor (i = 0; i < priv->channels.num; i++) {\n\t\tstruct mlx5e_channel *c = priv->channels.c[i];\n\n\t\tmlx5e_rq_replace_xdp_prog(&c->rq, prog);\n\t\tif (test_bit(MLX5E_CHANNEL_STATE_XSK, c->state)) {\n\t\t\tbpf_prog_inc(prog);\n\t\t\tmlx5e_rq_replace_xdp_prog(&c->xskrq, prog);\n\t\t}\n\t}\n\nunlock:\n\tmutex_unlock(&priv->state_lock);\n\n\t \n\tif (!err)\n\t\tnetdev_update_features(netdev);\n\n\treturn err;\n}\n\nstatic int mlx5e_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn mlx5e_xdp_set(dev, xdp->prog);\n\tcase XDP_SETUP_XSK_POOL:\n\t\treturn mlx5e_xsk_setup_pool(dev, xdp->xsk.pool,\n\t\t\t\t\t    xdp->xsk.queue_id);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n#ifdef CONFIG_MLX5_ESWITCH\nstatic int mlx5e_bridge_getlink(struct sk_buff *skb, u32 pid, u32 seq,\n\t\t\t\tstruct net_device *dev, u32 filter_mask,\n\t\t\t\tint nlflags)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tu8 mode, setting;\n\tint err;\n\n\terr = mlx5_eswitch_get_vepa(mdev->priv.eswitch, &setting);\n\tif (err)\n\t\treturn err;\n\tmode = setting ? BRIDGE_MODE_VEPA : BRIDGE_MODE_VEB;\n\treturn ndo_dflt_bridge_getlink(skb, pid, seq, dev,\n\t\t\t\t       mode,\n\t\t\t\t       0, 0, nlflags, filter_mask, NULL);\n}\n\nstatic int mlx5e_bridge_setlink(struct net_device *dev, struct nlmsghdr *nlh,\n\t\t\t\tu16 flags, struct netlink_ext_ack *extack)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(dev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tstruct nlattr *attr, *br_spec;\n\tu16 mode = BRIDGE_MODE_UNDEF;\n\tu8 setting;\n\tint rem;\n\n\tbr_spec = nlmsg_find_attr(nlh, sizeof(struct ifinfomsg), IFLA_AF_SPEC);\n\tif (!br_spec)\n\t\treturn -EINVAL;\n\n\tnla_for_each_nested(attr, br_spec, rem) {\n\t\tif (nla_type(attr) != IFLA_BRIDGE_MODE)\n\t\t\tcontinue;\n\n\t\tmode = nla_get_u16(attr);\n\t\tif (mode > BRIDGE_MODE_VEPA)\n\t\t\treturn -EINVAL;\n\n\t\tbreak;\n\t}\n\n\tif (mode == BRIDGE_MODE_UNDEF)\n\t\treturn -EINVAL;\n\n\tsetting = (mode == BRIDGE_MODE_VEPA) ?  1 : 0;\n\treturn mlx5_eswitch_set_vepa(mdev->priv.eswitch, setting);\n}\n#endif\n\nconst struct net_device_ops mlx5e_netdev_ops = {\n\t.ndo_open                = mlx5e_open,\n\t.ndo_stop                = mlx5e_close,\n\t.ndo_start_xmit          = mlx5e_xmit,\n\t.ndo_setup_tc            = mlx5e_setup_tc,\n\t.ndo_select_queue        = mlx5e_select_queue,\n\t.ndo_get_stats64         = mlx5e_get_stats,\n\t.ndo_set_rx_mode         = mlx5e_set_rx_mode,\n\t.ndo_set_mac_address     = mlx5e_set_mac,\n\t.ndo_vlan_rx_add_vid     = mlx5e_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid    = mlx5e_vlan_rx_kill_vid,\n\t.ndo_set_features        = mlx5e_set_features,\n\t.ndo_fix_features        = mlx5e_fix_features,\n\t.ndo_change_mtu          = mlx5e_change_nic_mtu,\n\t.ndo_eth_ioctl            = mlx5e_ioctl,\n\t.ndo_set_tx_maxrate      = mlx5e_set_tx_maxrate,\n\t.ndo_features_check      = mlx5e_features_check,\n\t.ndo_tx_timeout          = mlx5e_tx_timeout,\n\t.ndo_bpf\t\t = mlx5e_xdp,\n\t.ndo_xdp_xmit            = mlx5e_xdp_xmit,\n\t.ndo_xsk_wakeup          = mlx5e_xsk_wakeup,\n#ifdef CONFIG_MLX5_EN_ARFS\n\t.ndo_rx_flow_steer\t = mlx5e_rx_flow_steer,\n#endif\n#ifdef CONFIG_MLX5_ESWITCH\n\t.ndo_bridge_setlink      = mlx5e_bridge_setlink,\n\t.ndo_bridge_getlink      = mlx5e_bridge_getlink,\n\n\t \n\t.ndo_set_vf_mac          = mlx5e_set_vf_mac,\n\t.ndo_set_vf_vlan         = mlx5e_set_vf_vlan,\n\t.ndo_set_vf_spoofchk     = mlx5e_set_vf_spoofchk,\n\t.ndo_set_vf_trust        = mlx5e_set_vf_trust,\n\t.ndo_set_vf_rate         = mlx5e_set_vf_rate,\n\t.ndo_get_vf_config       = mlx5e_get_vf_config,\n\t.ndo_set_vf_link_state   = mlx5e_set_vf_link_state,\n\t.ndo_get_vf_stats        = mlx5e_get_vf_stats,\n\t.ndo_has_offload_stats   = mlx5e_has_offload_stats,\n\t.ndo_get_offload_stats   = mlx5e_get_offload_stats,\n#endif\n};\n\nstatic u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < MLX5E_LRO_TIMEOUT_ARR_SIZE - 1; i++)\n\t\tif (MLX5_CAP_ETH(mdev, lro_timer_supported_periods[i]) >= wanted_timeout)\n\t\t\tbreak;\n\n\treturn MLX5_CAP_ETH(mdev, lro_timer_supported_periods[i]);\n}\n\nvoid mlx5e_build_nic_params(struct mlx5e_priv *priv, struct mlx5e_xsk *xsk, u16 mtu)\n{\n\tstruct mlx5e_params *params = &priv->channels.params;\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tu8 rx_cq_period_mode;\n\n\tparams->sw_mtu = mtu;\n\tparams->hard_mtu = MLX5E_ETH_HARD_MTU;\n\tparams->num_channels = min_t(unsigned int, MLX5E_MAX_NUM_CHANNELS / 2,\n\t\t\t\t     priv->max_nch);\n\tmlx5e_params_mqprio_reset(params);\n\n\t \n\tparams->log_sq_size = is_kdump_kernel() ?\n\t\tMLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE :\n\t\tMLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;\n\tMLX5E_SET_PFLAG(params, MLX5E_PFLAG_SKB_TX_MPWQE, mlx5e_tx_mpwqe_supported(mdev));\n\n\t \n\tMLX5E_SET_PFLAG(params, MLX5E_PFLAG_XDP_TX_MPWQE, mlx5e_tx_mpwqe_supported(mdev));\n\n\t \n\tparams->rx_cqe_compress_def = false;\n\tif (MLX5_CAP_GEN(mdev, cqe_compression) &&\n\t    MLX5_CAP_GEN(mdev, vport_group_manager))\n\t\tparams->rx_cqe_compress_def = slow_pci_heuristic(mdev);\n\n\tMLX5E_SET_PFLAG(params, MLX5E_PFLAG_RX_CQE_COMPRESS, params->rx_cqe_compress_def);\n\tMLX5E_SET_PFLAG(params, MLX5E_PFLAG_RX_NO_CSUM_COMPLETE, false);\n\n\t \n\tmlx5e_build_rq_params(mdev, params);\n\n\tparams->terminate_lkey_be = mlx5_core_get_terminate_scatter_list_mkey(mdev);\n\n\tparams->packet_merge.timeout = mlx5e_choose_lro_timeout(mdev, MLX5E_DEFAULT_LRO_TIMEOUT);\n\n\t \n\trx_cq_period_mode = MLX5_CAP_GEN(mdev, cq_period_start_from_cqe) ?\n\t\t\tMLX5_CQ_PERIOD_MODE_START_FROM_CQE :\n\t\t\tMLX5_CQ_PERIOD_MODE_START_FROM_EQE;\n\tparams->rx_dim_enabled = MLX5_CAP_GEN(mdev, cq_moderation);\n\tparams->tx_dim_enabled = MLX5_CAP_GEN(mdev, cq_moderation);\n\tmlx5e_set_rx_cq_mode_params(params, rx_cq_period_mode);\n\tmlx5e_set_tx_cq_mode_params(params, MLX5_CQ_PERIOD_MODE_START_FROM_EQE);\n\n\t \n\tmlx5_query_min_inline(mdev, &params->tx_min_inline_mode);\n\n\t \n\tparams->xsk = xsk;\n\n\t \n}\n\nstatic void mlx5e_set_netdev_dev_addr(struct net_device *netdev)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tu8 addr[ETH_ALEN];\n\n\tmlx5_query_mac_address(priv->mdev, addr);\n\tif (is_zero_ether_addr(addr) &&\n\t    !MLX5_CAP_GEN(priv->mdev, vport_group_manager)) {\n\t\teth_hw_addr_random(netdev);\n\t\tmlx5_core_info(priv->mdev, \"Assigned random MAC address %pM\\n\", netdev->dev_addr);\n\t\treturn;\n\t}\n\n\teth_hw_addr_set(netdev, addr);\n}\n\nstatic int mlx5e_vxlan_set_port(struct net_device *netdev, unsigned int table,\n\t\t\t\tunsigned int entry, struct udp_tunnel_info *ti)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\n\treturn mlx5_vxlan_add_port(priv->mdev->vxlan, ntohs(ti->port));\n}\n\nstatic int mlx5e_vxlan_unset_port(struct net_device *netdev, unsigned int table,\n\t\t\t\t  unsigned int entry, struct udp_tunnel_info *ti)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\n\treturn mlx5_vxlan_del_port(priv->mdev->vxlan, ntohs(ti->port));\n}\n\nvoid mlx5e_vxlan_set_netdev_info(struct mlx5e_priv *priv)\n{\n\tif (!mlx5_vxlan_allowed(priv->mdev->vxlan))\n\t\treturn;\n\n\tpriv->nic_info.set_port = mlx5e_vxlan_set_port;\n\tpriv->nic_info.unset_port = mlx5e_vxlan_unset_port;\n\tpriv->nic_info.flags = UDP_TUNNEL_NIC_INFO_MAY_SLEEP |\n\t\t\t\tUDP_TUNNEL_NIC_INFO_STATIC_IANA_VXLAN;\n\tpriv->nic_info.tables[0].tunnel_types = UDP_TUNNEL_TYPE_VXLAN;\n\t \n\tpriv->nic_info.tables[0].n_entries =\n\t\tmlx5_vxlan_max_udp_ports(priv->mdev) - 1;\n\n\tpriv->netdev->udp_tunnel_nic_info = &priv->nic_info;\n}\n\nstatic bool mlx5e_tunnel_any_tx_proto_supported(struct mlx5_core_dev *mdev)\n{\n\tint tt;\n\n\tfor (tt = 0; tt < MLX5_NUM_TUNNEL_TT; tt++) {\n\t\tif (mlx5e_tunnel_proto_supported_tx(mdev, mlx5_get_proto_by_tunnel_type(tt)))\n\t\t\treturn true;\n\t}\n\treturn (mlx5_vxlan_allowed(mdev->vxlan) || mlx5_geneve_tx_allowed(mdev));\n}\n\nstatic void mlx5e_build_nic_netdev(struct net_device *netdev)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tbool fcs_supported;\n\tbool fcs_enabled;\n\n\tSET_NETDEV_DEV(netdev, mdev->device);\n\n\tnetdev->netdev_ops = &mlx5e_netdev_ops;\n\tnetdev->xdp_metadata_ops = &mlx5e_xdp_metadata_ops;\n\n\tmlx5e_dcbnl_build_netdev(netdev);\n\n\tnetdev->watchdog_timeo    = 15 * HZ;\n\n\tnetdev->ethtool_ops\t  = &mlx5e_ethtool_ops;\n\n\tnetdev->vlan_features    |= NETIF_F_SG;\n\tnetdev->vlan_features    |= NETIF_F_HW_CSUM;\n\tnetdev->vlan_features    |= NETIF_F_HW_MACSEC;\n\tnetdev->vlan_features    |= NETIF_F_GRO;\n\tnetdev->vlan_features    |= NETIF_F_TSO;\n\tnetdev->vlan_features    |= NETIF_F_TSO6;\n\tnetdev->vlan_features    |= NETIF_F_RXCSUM;\n\tnetdev->vlan_features    |= NETIF_F_RXHASH;\n\tnetdev->vlan_features    |= NETIF_F_GSO_PARTIAL;\n\n\tnetdev->mpls_features    |= NETIF_F_SG;\n\tnetdev->mpls_features    |= NETIF_F_HW_CSUM;\n\tnetdev->mpls_features    |= NETIF_F_TSO;\n\tnetdev->mpls_features    |= NETIF_F_TSO6;\n\n\tnetdev->hw_enc_features  |= NETIF_F_HW_VLAN_CTAG_TX;\n\tnetdev->hw_enc_features  |= NETIF_F_HW_VLAN_CTAG_RX;\n\n\t \n\tif (!!MLX5_CAP_ETH(mdev, lro_cap) &&\n\t    !MLX5_CAP_ETH(mdev, tunnel_lro_vxlan) &&\n\t    !MLX5_CAP_ETH(mdev, tunnel_lro_gre) &&\n\t    mlx5e_check_fragmented_striding_rq_cap(mdev, PAGE_SHIFT,\n\t\t\t\t\t\t   MLX5E_MPWRQ_UMR_MODE_ALIGNED))\n\t\tnetdev->vlan_features    |= NETIF_F_LRO;\n\n\tnetdev->hw_features       = netdev->vlan_features;\n\tnetdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_TX;\n\tnetdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_RX;\n\tnetdev->hw_features      |= NETIF_F_HW_VLAN_CTAG_FILTER;\n\tnetdev->hw_features      |= NETIF_F_HW_VLAN_STAG_TX;\n\n\tif (mlx5e_tunnel_any_tx_proto_supported(mdev)) {\n\t\tnetdev->hw_enc_features |= NETIF_F_HW_CSUM;\n\t\tnetdev->hw_enc_features |= NETIF_F_TSO;\n\t\tnetdev->hw_enc_features |= NETIF_F_TSO6;\n\t\tnetdev->hw_enc_features |= NETIF_F_GSO_PARTIAL;\n\t}\n\n\tif (mlx5_vxlan_allowed(mdev->vxlan) || mlx5_geneve_tx_allowed(mdev)) {\n\t\tnetdev->hw_features     |= NETIF_F_GSO_UDP_TUNNEL |\n\t\t\t\t\t   NETIF_F_GSO_UDP_TUNNEL_CSUM;\n\t\tnetdev->hw_enc_features |= NETIF_F_GSO_UDP_TUNNEL |\n\t\t\t\t\t   NETIF_F_GSO_UDP_TUNNEL_CSUM;\n\t\tnetdev->gso_partial_features = NETIF_F_GSO_UDP_TUNNEL_CSUM;\n\t\tnetdev->vlan_features |= NETIF_F_GSO_UDP_TUNNEL |\n\t\t\t\t\t NETIF_F_GSO_UDP_TUNNEL_CSUM;\n\t}\n\n\tif (mlx5e_tunnel_proto_supported_tx(mdev, IPPROTO_GRE)) {\n\t\tnetdev->hw_features     |= NETIF_F_GSO_GRE |\n\t\t\t\t\t   NETIF_F_GSO_GRE_CSUM;\n\t\tnetdev->hw_enc_features |= NETIF_F_GSO_GRE |\n\t\t\t\t\t   NETIF_F_GSO_GRE_CSUM;\n\t\tnetdev->gso_partial_features |= NETIF_F_GSO_GRE |\n\t\t\t\t\t\tNETIF_F_GSO_GRE_CSUM;\n\t}\n\n\tif (mlx5e_tunnel_proto_supported_tx(mdev, IPPROTO_IPIP)) {\n\t\tnetdev->hw_features |= NETIF_F_GSO_IPXIP4 |\n\t\t\t\t       NETIF_F_GSO_IPXIP6;\n\t\tnetdev->hw_enc_features |= NETIF_F_GSO_IPXIP4 |\n\t\t\t\t\t   NETIF_F_GSO_IPXIP6;\n\t\tnetdev->gso_partial_features |= NETIF_F_GSO_IPXIP4 |\n\t\t\t\t\t\tNETIF_F_GSO_IPXIP6;\n\t}\n\n\tnetdev->gso_partial_features             |= NETIF_F_GSO_UDP_L4;\n\tnetdev->hw_features                      |= NETIF_F_GSO_UDP_L4;\n\tnetdev->features                         |= NETIF_F_GSO_UDP_L4;\n\n\tmlx5_query_port_fcs(mdev, &fcs_supported, &fcs_enabled);\n\n\tif (fcs_supported)\n\t\tnetdev->hw_features |= NETIF_F_RXALL;\n\n\tif (MLX5_CAP_ETH(mdev, scatter_fcs))\n\t\tnetdev->hw_features |= NETIF_F_RXFCS;\n\n\tif (mlx5_qos_is_supported(mdev))\n\t\tnetdev->hw_features |= NETIF_F_HW_TC;\n\n\tnetdev->features          = netdev->hw_features;\n\n\t \n\tif (fcs_enabled)\n\t\tnetdev->features  &= ~NETIF_F_RXALL;\n\tnetdev->features  &= ~NETIF_F_LRO;\n\tnetdev->features  &= ~NETIF_F_GRO_HW;\n\tnetdev->features  &= ~NETIF_F_RXFCS;\n\n#define FT_CAP(f) MLX5_CAP_FLOWTABLE(mdev, flow_table_properties_nic_receive.f)\n\tif (FT_CAP(flow_modify_en) &&\n\t    FT_CAP(modify_root) &&\n\t    FT_CAP(identified_miss_table_mode) &&\n\t    FT_CAP(flow_table_modify)) {\n#if IS_ENABLED(CONFIG_MLX5_CLS_ACT)\n\t\tnetdev->hw_features      |= NETIF_F_HW_TC;\n#endif\n#ifdef CONFIG_MLX5_EN_ARFS\n\t\tnetdev->hw_features\t |= NETIF_F_NTUPLE;\n#endif\n\t}\n\n\tnetdev->features         |= NETIF_F_HIGHDMA;\n\tnetdev->features         |= NETIF_F_HW_VLAN_STAG_FILTER;\n\n\tnetdev->priv_flags       |= IFF_UNICAST_FLT;\n\n\tnetif_set_tso_max_size(netdev, GSO_MAX_SIZE);\n\tmlx5e_set_xdp_feature(netdev);\n\tmlx5e_set_netdev_dev_addr(netdev);\n\tmlx5e_macsec_build_netdev(priv);\n\tmlx5e_ipsec_build_netdev(priv);\n\tmlx5e_ktls_build_netdev(priv);\n}\n\nvoid mlx5e_create_q_counters(struct mlx5e_priv *priv)\n{\n\tu32 out[MLX5_ST_SZ_DW(alloc_q_counter_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(alloc_q_counter_in)] = {};\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tint err;\n\n\tMLX5_SET(alloc_q_counter_in, in, opcode, MLX5_CMD_OP_ALLOC_Q_COUNTER);\n\terr = mlx5_cmd_exec_inout(mdev, alloc_q_counter, in, out);\n\tif (!err)\n\t\tpriv->q_counter =\n\t\t\tMLX5_GET(alloc_q_counter_out, out, counter_set_id);\n\n\terr = mlx5_cmd_exec_inout(mdev, alloc_q_counter, in, out);\n\tif (!err)\n\t\tpriv->drop_rq_q_counter =\n\t\t\tMLX5_GET(alloc_q_counter_out, out, counter_set_id);\n}\n\nvoid mlx5e_destroy_q_counters(struct mlx5e_priv *priv)\n{\n\tu32 in[MLX5_ST_SZ_DW(dealloc_q_counter_in)] = {};\n\n\tMLX5_SET(dealloc_q_counter_in, in, opcode,\n\t\t MLX5_CMD_OP_DEALLOC_Q_COUNTER);\n\tif (priv->q_counter) {\n\t\tMLX5_SET(dealloc_q_counter_in, in, counter_set_id,\n\t\t\t priv->q_counter);\n\t\tmlx5_cmd_exec_in(priv->mdev, dealloc_q_counter, in);\n\t}\n\n\tif (priv->drop_rq_q_counter) {\n\t\tMLX5_SET(dealloc_q_counter_in, in, counter_set_id,\n\t\t\t priv->drop_rq_q_counter);\n\t\tmlx5_cmd_exec_in(priv->mdev, dealloc_q_counter, in);\n\t}\n}\n\nstatic int mlx5e_nic_init(struct mlx5_core_dev *mdev,\n\t\t\t  struct net_device *netdev)\n{\n\tconst bool take_rtnl = netdev->reg_state == NETREG_REGISTERED;\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5e_flow_steering *fs;\n\tint err;\n\n\tmlx5e_build_nic_params(priv, &priv->xsk, netdev->mtu);\n\tmlx5e_vxlan_set_netdev_info(priv);\n\n\tmlx5e_timestamp_init(priv);\n\n\tpriv->dfs_root = debugfs_create_dir(\"nic\",\n\t\t\t\t\t    mlx5_debugfs_get_dev_root(mdev));\n\n\tfs = mlx5e_fs_init(priv->profile, mdev,\n\t\t\t   !test_bit(MLX5E_STATE_DESTROYING, &priv->state),\n\t\t\t   priv->dfs_root);\n\tif (!fs) {\n\t\terr = -ENOMEM;\n\t\tmlx5_core_err(mdev, \"FS initialization failed, %d\\n\", err);\n\t\tdebugfs_remove_recursive(priv->dfs_root);\n\t\treturn err;\n\t}\n\tpriv->fs = fs;\n\n\terr = mlx5e_ktls_init(priv);\n\tif (err)\n\t\tmlx5_core_err(mdev, \"TLS initialization failed, %d\\n\", err);\n\n\tmlx5e_health_create_reporters(priv);\n\n\t \n\tif (take_rtnl)\n\t\trtnl_lock();\n\n\t \n\tmlx5e_set_xdp_feature(netdev);\n\n\tif (take_rtnl)\n\t\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic void mlx5e_nic_cleanup(struct mlx5e_priv *priv)\n{\n\tmlx5e_health_destroy_reporters(priv);\n\tmlx5e_ktls_cleanup(priv);\n\tmlx5e_fs_cleanup(priv->fs);\n\tdebugfs_remove_recursive(priv->dfs_root);\n\tpriv->fs = NULL;\n}\n\nstatic int mlx5e_init_nic_rx(struct mlx5e_priv *priv)\n{\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tenum mlx5e_rx_res_features features;\n\tint err;\n\n\tpriv->rx_res = mlx5e_rx_res_alloc();\n\tif (!priv->rx_res)\n\t\treturn -ENOMEM;\n\n\tmlx5e_create_q_counters(priv);\n\n\terr = mlx5e_open_drop_rq(priv, &priv->drop_rq);\n\tif (err) {\n\t\tmlx5_core_err(mdev, \"open drop rq failed, %d\\n\", err);\n\t\tgoto err_destroy_q_counters;\n\t}\n\n\tfeatures = MLX5E_RX_RES_FEATURE_PTP;\n\tif (mlx5_tunnel_inner_ft_supported(mdev))\n\t\tfeatures |= MLX5E_RX_RES_FEATURE_INNER_FT;\n\terr = mlx5e_rx_res_init(priv->rx_res, priv->mdev, features,\n\t\t\t\tpriv->max_nch, priv->drop_rq.rqn,\n\t\t\t\t&priv->channels.params.packet_merge,\n\t\t\t\tpriv->channels.params.num_channels);\n\tif (err)\n\t\tgoto err_close_drop_rq;\n\n\terr = mlx5e_create_flow_steering(priv->fs, priv->rx_res, priv->profile,\n\t\t\t\t\t priv->netdev);\n\tif (err) {\n\t\tmlx5_core_warn(mdev, \"create flow steering failed, %d\\n\", err);\n\t\tgoto err_destroy_rx_res;\n\t}\n\n\terr = mlx5e_tc_nic_init(priv);\n\tif (err)\n\t\tgoto err_destroy_flow_steering;\n\n\terr = mlx5e_accel_init_rx(priv);\n\tif (err)\n\t\tgoto err_tc_nic_cleanup;\n\n#ifdef CONFIG_MLX5_EN_ARFS\n\tpriv->netdev->rx_cpu_rmap =  mlx5_eq_table_get_rmap(priv->mdev);\n#endif\n\n\treturn 0;\n\nerr_tc_nic_cleanup:\n\tmlx5e_tc_nic_cleanup(priv);\nerr_destroy_flow_steering:\n\tmlx5e_destroy_flow_steering(priv->fs, !!(priv->netdev->hw_features & NETIF_F_NTUPLE),\n\t\t\t\t    priv->profile);\nerr_destroy_rx_res:\n\tmlx5e_rx_res_destroy(priv->rx_res);\nerr_close_drop_rq:\n\tmlx5e_close_drop_rq(&priv->drop_rq);\nerr_destroy_q_counters:\n\tmlx5e_destroy_q_counters(priv);\n\tmlx5e_rx_res_free(priv->rx_res);\n\tpriv->rx_res = NULL;\n\treturn err;\n}\n\nstatic void mlx5e_cleanup_nic_rx(struct mlx5e_priv *priv)\n{\n\tmlx5e_accel_cleanup_rx(priv);\n\tmlx5e_tc_nic_cleanup(priv);\n\tmlx5e_destroy_flow_steering(priv->fs, !!(priv->netdev->hw_features & NETIF_F_NTUPLE),\n\t\t\t\t    priv->profile);\n\tmlx5e_rx_res_destroy(priv->rx_res);\n\tmlx5e_close_drop_rq(&priv->drop_rq);\n\tmlx5e_destroy_q_counters(priv);\n\tmlx5e_rx_res_free(priv->rx_res);\n\tpriv->rx_res = NULL;\n}\n\nstatic void mlx5e_set_mqprio_rl(struct mlx5e_priv *priv)\n{\n\tstruct mlx5e_params *params;\n\tstruct mlx5e_mqprio_rl *rl;\n\n\tparams = &priv->channels.params;\n\tif (params->mqprio.mode != TC_MQPRIO_MODE_CHANNEL)\n\t\treturn;\n\n\trl = mlx5e_mqprio_rl_create(priv->mdev, params->mqprio.num_tc,\n\t\t\t\t    params->mqprio.channel.max_rate);\n\tif (IS_ERR(rl))\n\t\trl = NULL;\n\tpriv->mqprio_rl = rl;\n\tmlx5e_mqprio_rl_update_params(params, rl);\n}\n\nstatic int mlx5e_init_nic_tx(struct mlx5e_priv *priv)\n{\n\tint err;\n\n\terr = mlx5e_create_tises(priv);\n\tif (err) {\n\t\tmlx5_core_warn(priv->mdev, \"create tises failed, %d\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = mlx5e_accel_init_tx(priv);\n\tif (err)\n\t\tgoto err_destroy_tises;\n\n\tmlx5e_set_mqprio_rl(priv);\n\tmlx5e_dcbnl_initialize(priv);\n\treturn 0;\n\nerr_destroy_tises:\n\tmlx5e_destroy_tises(priv);\n\treturn err;\n}\n\nstatic void mlx5e_nic_enable(struct mlx5e_priv *priv)\n{\n\tstruct net_device *netdev = priv->netdev;\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tint err;\n\n\tmlx5e_fs_init_l2_addr(priv->fs, netdev);\n\tmlx5e_ipsec_init(priv);\n\n\terr = mlx5e_macsec_init(priv);\n\tif (err)\n\t\tmlx5_core_err(mdev, \"MACsec initialization failed, %d\\n\", err);\n\n\t \n\tif (!netif_running(netdev))\n\t\tmlx5e_modify_admin_state(mdev, MLX5_PORT_DOWN);\n\n\tmlx5e_set_netdev_mtu_boundaries(priv);\n\tmlx5e_set_dev_port_mtu(priv);\n\n\tmlx5_lag_add_netdev(mdev, netdev);\n\n\tmlx5e_enable_async_events(priv);\n\tmlx5e_enable_blocking_events(priv);\n\tif (mlx5e_monitor_counter_supported(priv))\n\t\tmlx5e_monitor_counter_init(priv);\n\n\tmlx5e_hv_vhca_stats_create(priv);\n\tif (netdev->reg_state != NETREG_REGISTERED)\n\t\treturn;\n\tmlx5e_dcbnl_init_app(priv);\n\n\tmlx5e_nic_set_rx_mode(priv);\n\n\trtnl_lock();\n\tif (netif_running(netdev))\n\t\tmlx5e_open(netdev);\n\tudp_tunnel_nic_reset_ntf(priv->netdev);\n\tnetif_device_attach(netdev);\n\trtnl_unlock();\n}\n\nstatic void mlx5e_nic_disable(struct mlx5e_priv *priv)\n{\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\tif (priv->netdev->reg_state == NETREG_REGISTERED)\n\t\tmlx5e_dcbnl_delete_app(priv);\n\n\trtnl_lock();\n\tif (netif_running(priv->netdev))\n\t\tmlx5e_close(priv->netdev);\n\tnetif_device_detach(priv->netdev);\n\trtnl_unlock();\n\n\tmlx5e_nic_set_rx_mode(priv);\n\n\tmlx5e_hv_vhca_stats_destroy(priv);\n\tif (mlx5e_monitor_counter_supported(priv))\n\t\tmlx5e_monitor_counter_cleanup(priv);\n\n\tmlx5e_disable_blocking_events(priv);\n\tif (priv->en_trap) {\n\t\tmlx5e_deactivate_trap(priv);\n\t\tmlx5e_close_trap(priv->en_trap);\n\t\tpriv->en_trap = NULL;\n\t}\n\tmlx5e_disable_async_events(priv);\n\tmlx5_lag_remove_netdev(mdev, priv->netdev);\n\tmlx5_vxlan_reset_to_default(mdev->vxlan);\n\tmlx5e_macsec_cleanup(priv);\n\tmlx5e_ipsec_cleanup(priv);\n}\n\nint mlx5e_update_nic_rx(struct mlx5e_priv *priv)\n{\n\treturn mlx5e_refresh_tirs(priv, false, false);\n}\n\nstatic const struct mlx5e_profile mlx5e_nic_profile = {\n\t.init\t\t   = mlx5e_nic_init,\n\t.cleanup\t   = mlx5e_nic_cleanup,\n\t.init_rx\t   = mlx5e_init_nic_rx,\n\t.cleanup_rx\t   = mlx5e_cleanup_nic_rx,\n\t.init_tx\t   = mlx5e_init_nic_tx,\n\t.cleanup_tx\t   = mlx5e_cleanup_nic_tx,\n\t.enable\t\t   = mlx5e_nic_enable,\n\t.disable\t   = mlx5e_nic_disable,\n\t.update_rx\t   = mlx5e_update_nic_rx,\n\t.update_stats\t   = mlx5e_stats_update_ndo_stats,\n\t.update_carrier\t   = mlx5e_update_carrier,\n\t.rx_handlers       = &mlx5e_rx_handlers_nic,\n\t.max_tc\t\t   = MLX5E_MAX_NUM_TC,\n\t.stats_grps\t   = mlx5e_nic_stats_grps,\n\t.stats_grps_num\t   = mlx5e_nic_stats_grps_num,\n\t.features          = BIT(MLX5E_PROFILE_FEATURE_PTP_RX) |\n\t\tBIT(MLX5E_PROFILE_FEATURE_PTP_TX) |\n\t\tBIT(MLX5E_PROFILE_FEATURE_QOS_HTB) |\n\t\tBIT(MLX5E_PROFILE_FEATURE_FS_VLAN) |\n\t\tBIT(MLX5E_PROFILE_FEATURE_FS_TC),\n};\n\nstatic int mlx5e_profile_max_num_channels(struct mlx5_core_dev *mdev,\n\t\t\t\t\t  const struct mlx5e_profile *profile)\n{\n\tint nch;\n\n\tnch = mlx5e_get_max_num_channels(mdev);\n\n\tif (profile->max_nch_limit)\n\t\tnch = min_t(int, nch, profile->max_nch_limit(mdev));\n\treturn nch;\n}\n\nstatic unsigned int\nmlx5e_calc_max_nch(struct mlx5_core_dev *mdev, struct net_device *netdev,\n\t\t   const struct mlx5e_profile *profile)\n\n{\n\tunsigned int max_nch, tmp;\n\n\t \n\tmax_nch = mlx5e_profile_max_num_channels(mdev, profile);\n\n\t \n\tmax_nch = min_t(unsigned int, max_nch, netdev->num_rx_queues);\n\n\t \n\ttmp = netdev->num_tx_queues;\n\tif (mlx5_qos_is_supported(mdev))\n\t\ttmp -= mlx5e_qos_max_leaf_nodes(mdev);\n\tif (MLX5_CAP_GEN(mdev, ts_cqe_to_dest_cqn))\n\t\ttmp -= profile->max_tc;\n\ttmp = tmp / profile->max_tc;\n\tmax_nch = min_t(unsigned int, max_nch, tmp);\n\n\treturn max_nch;\n}\n\nint mlx5e_get_pf_num_tirs(struct mlx5_core_dev *mdev)\n{\n\t \n\treturn 2 * MLX5E_NUM_INDIR_TIRS\n\t\t+ mlx5e_profile_max_num_channels(mdev, &mlx5e_nic_profile);\n}\n\nvoid mlx5e_set_rx_mode_work(struct work_struct *work)\n{\n\tstruct mlx5e_priv *priv = container_of(work, struct mlx5e_priv,\n\t\t\t\t\t       set_rx_mode_work);\n\n\treturn mlx5e_fs_set_rx_mode_work(priv->fs, priv->netdev);\n}\n\n \nint mlx5e_priv_init(struct mlx5e_priv *priv,\n\t\t    const struct mlx5e_profile *profile,\n\t\t    struct net_device *netdev,\n\t\t    struct mlx5_core_dev *mdev)\n{\n\tint nch, num_txqs, node;\n\tint err;\n\n\tnum_txqs = netdev->num_tx_queues;\n\tnch = mlx5e_calc_max_nch(mdev, netdev, profile);\n\tnode = dev_to_node(mlx5_core_dma_dev(mdev));\n\n\t \n\tpriv->mdev        = mdev;\n\tpriv->netdev      = netdev;\n\tpriv->max_nch     = nch;\n\tpriv->max_opened_tc = 1;\n\n\tif (!alloc_cpumask_var(&priv->scratchpad.cpumask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tmutex_init(&priv->state_lock);\n\n\terr = mlx5e_selq_init(&priv->selq, &priv->state_lock);\n\tif (err)\n\t\tgoto err_free_cpumask;\n\n\tINIT_WORK(&priv->update_carrier_work, mlx5e_update_carrier_work);\n\tINIT_WORK(&priv->set_rx_mode_work, mlx5e_set_rx_mode_work);\n\tINIT_WORK(&priv->tx_timeout_work, mlx5e_tx_timeout_work);\n\tINIT_WORK(&priv->update_stats_work, mlx5e_update_stats_work);\n\n\tpriv->wq = create_singlethread_workqueue(\"mlx5e\");\n\tif (!priv->wq)\n\t\tgoto err_free_selq;\n\n\tpriv->txq2sq = kcalloc_node(num_txqs, sizeof(*priv->txq2sq), GFP_KERNEL, node);\n\tif (!priv->txq2sq)\n\t\tgoto err_destroy_workqueue;\n\n\tpriv->tx_rates = kcalloc_node(num_txqs, sizeof(*priv->tx_rates), GFP_KERNEL, node);\n\tif (!priv->tx_rates)\n\t\tgoto err_free_txq2sq;\n\n\tpriv->channel_stats =\n\t\tkcalloc_node(nch, sizeof(*priv->channel_stats), GFP_KERNEL, node);\n\tif (!priv->channel_stats)\n\t\tgoto err_free_tx_rates;\n\n\treturn 0;\n\nerr_free_tx_rates:\n\tkfree(priv->tx_rates);\nerr_free_txq2sq:\n\tkfree(priv->txq2sq);\nerr_destroy_workqueue:\n\tdestroy_workqueue(priv->wq);\nerr_free_selq:\n\tmlx5e_selq_cleanup(&priv->selq);\nerr_free_cpumask:\n\tfree_cpumask_var(priv->scratchpad.cpumask);\n\treturn -ENOMEM;\n}\n\nvoid mlx5e_priv_cleanup(struct mlx5e_priv *priv)\n{\n\tint i;\n\n\t \n\tif (!priv->mdev)\n\t\treturn;\n\n\tfor (i = 0; i < priv->stats_nch; i++)\n\t\tkvfree(priv->channel_stats[i]);\n\tkfree(priv->channel_stats);\n\tkfree(priv->tx_rates);\n\tkfree(priv->txq2sq);\n\tdestroy_workqueue(priv->wq);\n\tmutex_lock(&priv->state_lock);\n\tmlx5e_selq_cleanup(&priv->selq);\n\tmutex_unlock(&priv->state_lock);\n\tfree_cpumask_var(priv->scratchpad.cpumask);\n\n\tfor (i = 0; i < priv->htb_max_qos_sqs; i++)\n\t\tkfree(priv->htb_qos_sq_stats[i]);\n\tkvfree(priv->htb_qos_sq_stats);\n\n\tmemset(priv, 0, sizeof(*priv));\n}\n\nstatic unsigned int mlx5e_get_max_num_txqs(struct mlx5_core_dev *mdev,\n\t\t\t\t\t   const struct mlx5e_profile *profile)\n{\n\tunsigned int nch, ptp_txqs, qos_txqs;\n\n\tnch = mlx5e_profile_max_num_channels(mdev, profile);\n\n\tptp_txqs = MLX5_CAP_GEN(mdev, ts_cqe_to_dest_cqn) &&\n\t\tmlx5e_profile_feature_cap(profile, PTP_TX) ?\n\t\tprofile->max_tc : 0;\n\n\tqos_txqs = mlx5_qos_is_supported(mdev) &&\n\t\tmlx5e_profile_feature_cap(profile, QOS_HTB) ?\n\t\tmlx5e_qos_max_leaf_nodes(mdev) : 0;\n\n\treturn nch * profile->max_tc + ptp_txqs + qos_txqs;\n}\n\nstatic unsigned int mlx5e_get_max_num_rxqs(struct mlx5_core_dev *mdev,\n\t\t\t\t\t   const struct mlx5e_profile *profile)\n{\n\treturn mlx5e_profile_max_num_channels(mdev, profile);\n}\n\nstruct net_device *\nmlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *profile)\n{\n\tstruct net_device *netdev;\n\tunsigned int txqs, rxqs;\n\tint err;\n\n\ttxqs = mlx5e_get_max_num_txqs(mdev, profile);\n\trxqs = mlx5e_get_max_num_rxqs(mdev, profile);\n\n\tnetdev = alloc_etherdev_mqs(sizeof(struct mlx5e_priv), txqs, rxqs);\n\tif (!netdev) {\n\t\tmlx5_core_err(mdev, \"alloc_etherdev_mqs() failed\\n\");\n\t\treturn NULL;\n\t}\n\n\terr = mlx5e_priv_init(netdev_priv(netdev), profile, netdev, mdev);\n\tif (err) {\n\t\tmlx5_core_err(mdev, \"mlx5e_priv_init failed, err=%d\\n\", err);\n\t\tgoto err_free_netdev;\n\t}\n\n\tnetif_carrier_off(netdev);\n\tnetif_tx_disable(netdev);\n\tdev_net_set(netdev, mlx5_core_net(mdev));\n\n\treturn netdev;\n\nerr_free_netdev:\n\tfree_netdev(netdev);\n\n\treturn NULL;\n}\n\nstatic void mlx5e_update_features(struct net_device *netdev)\n{\n\tif (netdev->reg_state != NETREG_REGISTERED)\n\t\treturn;  \n\n\trtnl_lock();\n\tnetdev_update_features(netdev);\n\trtnl_unlock();\n}\n\nstatic void mlx5e_reset_channels(struct net_device *netdev)\n{\n\tnetdev_reset_tc(netdev);\n}\n\nint mlx5e_attach_netdev(struct mlx5e_priv *priv)\n{\n\tconst bool take_rtnl = priv->netdev->reg_state == NETREG_REGISTERED;\n\tconst struct mlx5e_profile *profile = priv->profile;\n\tint max_nch;\n\tint err;\n\n\tclear_bit(MLX5E_STATE_DESTROYING, &priv->state);\n\tif (priv->fs)\n\t\tmlx5e_fs_set_state_destroy(priv->fs,\n\t\t\t\t\t   !test_bit(MLX5E_STATE_DESTROYING, &priv->state));\n\n\t \n\tif (WARN_ON_ONCE(mlx5e_get_max_sq_wqebbs(priv->mdev) < MLX5E_MAX_TX_WQEBBS)) {\n\t\tmlx5_core_warn(priv->mdev, \"MLX5E: Max SQ WQEBBs firmware capability: %u, needed %u\\n\",\n\t\t\t       mlx5e_get_max_sq_wqebbs(priv->mdev), (unsigned int)MLX5E_MAX_TX_WQEBBS);\n\t\treturn -EIO;\n\t}\n\n\t \n\tmax_nch = mlx5e_calc_max_nch(priv->mdev, priv->netdev, profile);\n\tif (priv->channels.params.num_channels > max_nch) {\n\t\tmlx5_core_warn(priv->mdev, \"MLX5E: Reducing number of channels to %d\\n\", max_nch);\n\t\t \n\t\tpriv->netdev->priv_flags &= ~IFF_RXFH_CONFIGURED;\n\t\tpriv->channels.params.num_channels = max_nch;\n\t\tif (priv->channels.params.mqprio.mode == TC_MQPRIO_MODE_CHANNEL) {\n\t\t\tmlx5_core_warn(priv->mdev, \"MLX5E: Disabling MQPRIO channel mode\\n\");\n\t\t\tmlx5e_params_mqprio_reset(&priv->channels.params);\n\t\t}\n\t}\n\tif (max_nch != priv->max_nch) {\n\t\tmlx5_core_warn(priv->mdev,\n\t\t\t       \"MLX5E: Updating max number of channels from %u to %u\\n\",\n\t\t\t       priv->max_nch, max_nch);\n\t\tpriv->max_nch = max_nch;\n\t}\n\n\t \n\tif (take_rtnl)\n\t\trtnl_lock();\n\terr = mlx5e_num_channels_changed(priv);\n\tif (take_rtnl)\n\t\trtnl_unlock();\n\tif (err)\n\t\tgoto out;\n\n\terr = profile->init_tx(priv);\n\tif (err)\n\t\tgoto out;\n\n\terr = profile->init_rx(priv);\n\tif (err)\n\t\tgoto err_cleanup_tx;\n\n\tif (profile->enable)\n\t\tprofile->enable(priv);\n\n\tmlx5e_update_features(priv->netdev);\n\n\treturn 0;\n\nerr_cleanup_tx:\n\tprofile->cleanup_tx(priv);\n\nout:\n\tmlx5e_reset_channels(priv->netdev);\n\tset_bit(MLX5E_STATE_DESTROYING, &priv->state);\n\tif (priv->fs)\n\t\tmlx5e_fs_set_state_destroy(priv->fs,\n\t\t\t\t\t   !test_bit(MLX5E_STATE_DESTROYING, &priv->state));\n\tcancel_work_sync(&priv->update_stats_work);\n\treturn err;\n}\n\nvoid mlx5e_detach_netdev(struct mlx5e_priv *priv)\n{\n\tconst struct mlx5e_profile *profile = priv->profile;\n\n\tset_bit(MLX5E_STATE_DESTROYING, &priv->state);\n\tif (priv->fs)\n\t\tmlx5e_fs_set_state_destroy(priv->fs,\n\t\t\t\t\t   !test_bit(MLX5E_STATE_DESTROYING, &priv->state));\n\n\tif (profile->disable)\n\t\tprofile->disable(priv);\n\tflush_workqueue(priv->wq);\n\n\tprofile->cleanup_rx(priv);\n\tprofile->cleanup_tx(priv);\n\tmlx5e_reset_channels(priv->netdev);\n\tcancel_work_sync(&priv->update_stats_work);\n}\n\nstatic int\nmlx5e_netdev_init_profile(struct net_device *netdev, struct mlx5_core_dev *mdev,\n\t\t\t  const struct mlx5e_profile *new_profile, void *new_ppriv)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tint err;\n\n\terr = mlx5e_priv_init(priv, new_profile, netdev, mdev);\n\tif (err) {\n\t\tmlx5_core_err(mdev, \"mlx5e_priv_init failed, err=%d\\n\", err);\n\t\treturn err;\n\t}\n\tnetif_carrier_off(netdev);\n\tpriv->profile = new_profile;\n\tpriv->ppriv = new_ppriv;\n\terr = new_profile->init(priv->mdev, priv->netdev);\n\tif (err)\n\t\tgoto priv_cleanup;\n\n\treturn 0;\n\npriv_cleanup:\n\tmlx5e_priv_cleanup(priv);\n\treturn err;\n}\n\nstatic int\nmlx5e_netdev_attach_profile(struct net_device *netdev, struct mlx5_core_dev *mdev,\n\t\t\t    const struct mlx5e_profile *new_profile, void *new_ppriv)\n{\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tint err;\n\n\terr = mlx5e_netdev_init_profile(netdev, mdev, new_profile, new_ppriv);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_attach_netdev(priv);\n\tif (err)\n\t\tgoto profile_cleanup;\n\treturn err;\n\nprofile_cleanup:\n\tnew_profile->cleanup(priv);\n\tmlx5e_priv_cleanup(priv);\n\treturn err;\n}\n\nint mlx5e_netdev_change_profile(struct mlx5e_priv *priv,\n\t\t\t\tconst struct mlx5e_profile *new_profile, void *new_ppriv)\n{\n\tconst struct mlx5e_profile *orig_profile = priv->profile;\n\tstruct net_device *netdev = priv->netdev;\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\tvoid *orig_ppriv = priv->ppriv;\n\tint err, rollback_err;\n\n\t \n\tmlx5e_detach_netdev(priv);\n\tpriv->profile->cleanup(priv);\n\tmlx5e_priv_cleanup(priv);\n\n\tif (mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR) {\n\t\tmlx5e_netdev_init_profile(netdev, mdev, new_profile, new_ppriv);\n\t\tset_bit(MLX5E_STATE_DESTROYING, &priv->state);\n\t\treturn -EIO;\n\t}\n\n\terr = mlx5e_netdev_attach_profile(netdev, mdev, new_profile, new_ppriv);\n\tif (err) {  \n\t\tnetdev_warn(netdev, \"%s: new profile init failed, %d\\n\", __func__, err);\n\t\tgoto rollback;\n\t}\n\n\treturn 0;\n\nrollback:\n\trollback_err = mlx5e_netdev_attach_profile(netdev, mdev, orig_profile, orig_ppriv);\n\tif (rollback_err)\n\t\tnetdev_err(netdev, \"%s: failed to rollback to orig profile, %d\\n\",\n\t\t\t   __func__, rollback_err);\n\treturn err;\n}\n\nvoid mlx5e_netdev_attach_nic_profile(struct mlx5e_priv *priv)\n{\n\tmlx5e_netdev_change_profile(priv, &mlx5e_nic_profile, NULL);\n}\n\nvoid mlx5e_destroy_netdev(struct mlx5e_priv *priv)\n{\n\tstruct net_device *netdev = priv->netdev;\n\n\tmlx5e_priv_cleanup(priv);\n\tfree_netdev(netdev);\n}\n\nstatic int mlx5e_resume(struct auxiliary_device *adev)\n{\n\tstruct mlx5_adev *edev = container_of(adev, struct mlx5_adev, adev);\n\tstruct mlx5e_dev *mlx5e_dev = auxiliary_get_drvdata(adev);\n\tstruct mlx5e_priv *priv = mlx5e_dev->priv;\n\tstruct net_device *netdev = priv->netdev;\n\tstruct mlx5_core_dev *mdev = edev->mdev;\n\tint err;\n\n\tif (netif_device_present(netdev))\n\t\treturn 0;\n\n\terr = mlx5e_create_mdev_resources(mdev);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_attach_netdev(priv);\n\tif (err) {\n\t\tmlx5e_destroy_mdev_resources(mdev);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5e_suspend(struct auxiliary_device *adev, pm_message_t state)\n{\n\tstruct mlx5e_dev *mlx5e_dev = auxiliary_get_drvdata(adev);\n\tstruct mlx5e_priv *priv = mlx5e_dev->priv;\n\tstruct net_device *netdev = priv->netdev;\n\tstruct mlx5_core_dev *mdev = priv->mdev;\n\n\tif (!netif_device_present(netdev)) {\n\t\tif (test_bit(MLX5E_STATE_DESTROYING, &priv->state))\n\t\t\tmlx5e_destroy_mdev_resources(mdev);\n\t\treturn -ENODEV;\n\t}\n\n\tmlx5e_detach_netdev(priv);\n\tmlx5e_destroy_mdev_resources(mdev);\n\treturn 0;\n}\n\nstatic int mlx5e_probe(struct auxiliary_device *adev,\n\t\t       const struct auxiliary_device_id *id)\n{\n\tstruct mlx5_adev *edev = container_of(adev, struct mlx5_adev, adev);\n\tconst struct mlx5e_profile *profile = &mlx5e_nic_profile;\n\tstruct mlx5_core_dev *mdev = edev->mdev;\n\tstruct mlx5e_dev *mlx5e_dev;\n\tstruct net_device *netdev;\n\tpm_message_t state = {};\n\tstruct mlx5e_priv *priv;\n\tint err;\n\n\tmlx5e_dev = mlx5e_create_devlink(&adev->dev, mdev);\n\tif (IS_ERR(mlx5e_dev))\n\t\treturn PTR_ERR(mlx5e_dev);\n\tauxiliary_set_drvdata(adev, mlx5e_dev);\n\n\terr = mlx5e_devlink_port_register(mlx5e_dev, mdev);\n\tif (err) {\n\t\tmlx5_core_err(mdev, \"mlx5e_devlink_port_register failed, %d\\n\", err);\n\t\tgoto err_devlink_unregister;\n\t}\n\n\tnetdev = mlx5e_create_netdev(mdev, profile);\n\tif (!netdev) {\n\t\tmlx5_core_err(mdev, \"mlx5e_create_netdev failed\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_devlink_port_unregister;\n\t}\n\tSET_NETDEV_DEVLINK_PORT(netdev, &mlx5e_dev->dl_port);\n\n\tmlx5e_build_nic_netdev(netdev);\n\n\tpriv = netdev_priv(netdev);\n\tmlx5e_dev->priv = priv;\n\n\tpriv->profile = profile;\n\tpriv->ppriv = NULL;\n\n\terr = profile->init(mdev, netdev);\n\tif (err) {\n\t\tmlx5_core_err(mdev, \"mlx5e_nic_profile init failed, %d\\n\", err);\n\t\tgoto err_destroy_netdev;\n\t}\n\n\terr = mlx5e_resume(adev);\n\tif (err) {\n\t\tmlx5_core_err(mdev, \"mlx5e_resume failed, %d\\n\", err);\n\t\tgoto err_profile_cleanup;\n\t}\n\n\terr = register_netdev(netdev);\n\tif (err) {\n\t\tmlx5_core_err(mdev, \"register_netdev failed, %d\\n\", err);\n\t\tgoto err_resume;\n\t}\n\n\tmlx5e_dcbnl_init_app(priv);\n\tmlx5_core_uplink_netdev_set(mdev, netdev);\n\tmlx5e_params_print_info(mdev, &priv->channels.params);\n\treturn 0;\n\nerr_resume:\n\tmlx5e_suspend(adev, state);\nerr_profile_cleanup:\n\tprofile->cleanup(priv);\nerr_destroy_netdev:\n\tmlx5e_destroy_netdev(priv);\nerr_devlink_port_unregister:\n\tmlx5e_devlink_port_unregister(mlx5e_dev);\nerr_devlink_unregister:\n\tmlx5e_destroy_devlink(mlx5e_dev);\n\treturn err;\n}\n\nstatic void mlx5e_remove(struct auxiliary_device *adev)\n{\n\tstruct mlx5e_dev *mlx5e_dev = auxiliary_get_drvdata(adev);\n\tstruct mlx5e_priv *priv = mlx5e_dev->priv;\n\tpm_message_t state = {};\n\n\tmlx5_core_uplink_netdev_set(priv->mdev, NULL);\n\tmlx5e_dcbnl_delete_app(priv);\n\tunregister_netdev(priv->netdev);\n\tmlx5e_suspend(adev, state);\n\tpriv->profile->cleanup(priv);\n\tmlx5e_destroy_netdev(priv);\n\tmlx5e_devlink_port_unregister(mlx5e_dev);\n\tmlx5e_destroy_devlink(mlx5e_dev);\n}\n\nstatic const struct auxiliary_device_id mlx5e_id_table[] = {\n\t{ .name = MLX5_ADEV_NAME \".eth\", },\n\t{},\n};\n\nMODULE_DEVICE_TABLE(auxiliary, mlx5e_id_table);\n\nstatic struct auxiliary_driver mlx5e_driver = {\n\t.name = \"eth\",\n\t.probe = mlx5e_probe,\n\t.remove = mlx5e_remove,\n\t.suspend = mlx5e_suspend,\n\t.resume = mlx5e_resume,\n\t.id_table = mlx5e_id_table,\n};\n\nint mlx5e_init(void)\n{\n\tint ret;\n\n\tmlx5e_build_ptys2ethtool_map();\n\tret = auxiliary_driver_register(&mlx5e_driver);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mlx5e_rep_init();\n\tif (ret)\n\t\tauxiliary_driver_unregister(&mlx5e_driver);\n\treturn ret;\n}\n\nvoid mlx5e_cleanup(void)\n{\n\tmlx5e_rep_cleanup();\n\tauxiliary_driver_unregister(&mlx5e_driver);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}