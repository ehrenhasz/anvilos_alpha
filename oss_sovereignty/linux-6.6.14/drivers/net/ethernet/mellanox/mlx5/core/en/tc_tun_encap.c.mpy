{
  "module_name": "tc_tun_encap.c",
  "hash_id": "73ec068d6d5d205b55fb52f17917478008d38055a0d0b0da068d29dd1c734ebe",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en/tc_tun_encap.c",
  "human_readable_source": "\n \n\n#include <net/fib_notifier.h>\n#include <net/nexthop.h>\n#include <net/ip_tunnels.h>\n#include \"tc_tun_encap.h\"\n#include \"en_tc.h\"\n#include \"tc_tun.h\"\n#include \"rep/tc.h\"\n#include \"diag/en_tc_tracepoint.h\"\n\nenum {\n\tMLX5E_ROUTE_ENTRY_VALID     = BIT(0),\n};\n\nstatic int mlx5e_set_int_port_tunnel(struct mlx5e_priv *priv,\n\t\t\t\t     struct mlx5_flow_attr *attr,\n\t\t\t\t     struct mlx5e_encap_entry *e,\n\t\t\t\t     int out_index)\n{\n\tstruct net_device *route_dev;\n\tint err = 0;\n\n\troute_dev = dev_get_by_index(dev_net(e->out_dev), e->route_dev_ifindex);\n\n\tif (!route_dev || !netif_is_ovs_master(route_dev) ||\n\t    attr->parse_attr->filter_dev == e->out_dev)\n\t\tgoto out;\n\n\terr = mlx5e_set_fwd_to_int_port_actions(priv, attr, e->route_dev_ifindex,\n\t\t\t\t\t\tMLX5E_TC_INT_PORT_EGRESS,\n\t\t\t\t\t\t&attr->action, out_index);\n\nout:\n\tif (route_dev)\n\t\tdev_put(route_dev);\n\n\treturn err;\n}\n\nstruct mlx5e_route_key {\n\tint ip_version;\n\tunion {\n\t\t__be32 v4;\n\t\tstruct in6_addr v6;\n\t} endpoint_ip;\n};\n\nstruct mlx5e_route_entry {\n\tstruct mlx5e_route_key key;\n\tstruct list_head encap_entries;\n\tstruct list_head decap_flows;\n\tu32 flags;\n\tstruct hlist_node hlist;\n\trefcount_t refcnt;\n\tint tunnel_dev_index;\n\tstruct rcu_head rcu;\n};\n\nstruct mlx5e_tc_tun_encap {\n\tstruct mlx5e_priv *priv;\n\tstruct notifier_block fib_nb;\n\tspinlock_t route_lock;  \n\tunsigned long route_tbl_last_update;\n\tDECLARE_HASHTABLE(route_tbl, 8);\n};\n\nstatic bool mlx5e_route_entry_valid(struct mlx5e_route_entry *r)\n{\n\treturn r->flags & MLX5E_ROUTE_ENTRY_VALID;\n}\n\nint mlx5e_tc_set_attr_rx_tun(struct mlx5e_tc_flow *flow,\n\t\t\t     struct mlx5_flow_spec *spec)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = flow->attr->esw_attr;\n\tstruct mlx5_rx_tun_attr *tun_attr;\n\tvoid *daddr, *saddr;\n\tu8 ip_version;\n\n\ttun_attr = kvzalloc(sizeof(*tun_attr), GFP_KERNEL);\n\tif (!tun_attr)\n\t\treturn -ENOMEM;\n\n\tesw_attr->rx_tun_attr = tun_attr;\n\tip_version = mlx5e_tc_get_ip_version(spec, true);\n\n\tif (ip_version == 4) {\n\t\tdaddr = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t     outer_headers.dst_ipv4_dst_ipv6.ipv4_layout.ipv4);\n\t\tsaddr = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t     outer_headers.src_ipv4_src_ipv6.ipv4_layout.ipv4);\n\t\ttun_attr->dst_ip.v4 = *(__be32 *)daddr;\n\t\ttun_attr->src_ip.v4 = *(__be32 *)saddr;\n\t\tif (!tun_attr->dst_ip.v4 || !tun_attr->src_ip.v4)\n\t\t\treturn 0;\n\t}\n#if IS_ENABLED(CONFIG_INET) && IS_ENABLED(CONFIG_IPV6)\n\telse if (ip_version == 6) {\n\t\tint ipv6_size = MLX5_FLD_SZ_BYTES(ipv6_layout, ipv6);\n\n\t\tdaddr = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t     outer_headers.dst_ipv4_dst_ipv6.ipv6_layout.ipv6);\n\t\tsaddr = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t     outer_headers.src_ipv4_src_ipv6.ipv6_layout.ipv6);\n\t\tmemcpy(&tun_attr->dst_ip.v6, daddr, ipv6_size);\n\t\tmemcpy(&tun_attr->src_ip.v6, saddr, ipv6_size);\n\t\tif (ipv6_addr_any(&tun_attr->dst_ip.v6) ||\n\t\t    ipv6_addr_any(&tun_attr->src_ip.v6))\n\t\t\treturn 0;\n\t}\n#endif\n\t \n\tflow_flag_set(flow, TUN_RX);\n\tflow->attr->tun_ip_version = ip_version;\n\treturn 0;\n}\n\nstatic bool mlx5e_tc_flow_all_encaps_valid(struct mlx5_esw_flow_attr *esw_attr)\n{\n\tbool all_flow_encaps_valid = true;\n\tint i;\n\n\t \n\tfor (i = 0; i < MLX5_MAX_FLOW_FWD_VPORTS; i++) {\n\t\tif (!(esw_attr->dests[i].flags & MLX5_ESW_DEST_ENCAP))\n\t\t\tcontinue;\n\t\tif (!(esw_attr->dests[i].flags & MLX5_ESW_DEST_ENCAP_VALID)) {\n\t\t\tall_flow_encaps_valid = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn all_flow_encaps_valid;\n}\n\nvoid mlx5e_tc_encap_flows_add(struct mlx5e_priv *priv,\n\t\t\t      struct mlx5e_encap_entry *e,\n\t\t\t      struct list_head *flow_list)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5_pkt_reformat_params reformat_params;\n\tstruct mlx5_esw_flow_attr *esw_attr;\n\tstruct mlx5_flow_handle *rule;\n\tstruct mlx5_flow_attr *attr;\n\tstruct mlx5_flow_spec *spec;\n\tstruct mlx5e_tc_flow *flow;\n\tint err;\n\n\tif (e->flags & MLX5_ENCAP_ENTRY_NO_ROUTE)\n\t\treturn;\n\n\tmemset(&reformat_params, 0, sizeof(reformat_params));\n\treformat_params.type = e->reformat_type;\n\treformat_params.size = e->encap_size;\n\treformat_params.data = e->encap_header;\n\te->pkt_reformat = mlx5_packet_reformat_alloc(priv->mdev,\n\t\t\t\t\t\t     &reformat_params,\n\t\t\t\t\t\t     MLX5_FLOW_NAMESPACE_FDB);\n\tif (IS_ERR(e->pkt_reformat)) {\n\t\tmlx5_core_warn(priv->mdev, \"Failed to offload cached encapsulation header, %lu\\n\",\n\t\t\t       PTR_ERR(e->pkt_reformat));\n\t\treturn;\n\t}\n\te->flags |= MLX5_ENCAP_ENTRY_VALID;\n\tmlx5e_rep_queue_neigh_stats_work(priv);\n\n\tlist_for_each_entry(flow, flow_list, tmp_list) {\n\t\tif (!mlx5e_is_offloaded_flow(flow) || !flow_flag_test(flow, SLOW))\n\t\t\tcontinue;\n\n\t\tspec = &flow->attr->parse_attr->spec;\n\n\t\tattr = mlx5e_tc_get_encap_attr(flow);\n\t\tesw_attr = attr->esw_attr;\n\t\tesw_attr->dests[flow->tmp_entry_index].pkt_reformat = e->pkt_reformat;\n\t\tesw_attr->dests[flow->tmp_entry_index].flags |= MLX5_ESW_DEST_ENCAP_VALID;\n\n\t\t \n\t\tif (!mlx5e_tc_flow_all_encaps_valid(esw_attr))\n\t\t\tcontinue;\n\n\t\terr = mlx5e_tc_offload_flow_post_acts(flow);\n\t\tif (err) {\n\t\t\tmlx5_core_warn(priv->mdev, \"Failed to update flow post acts, %d\\n\",\n\t\t\t\t       err);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\trule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, flow->attr);\n\t\tif (IS_ERR(rule)) {\n\t\t\tmlx5e_tc_unoffload_flow_post_acts(flow);\n\t\t\terr = PTR_ERR(rule);\n\t\t\tmlx5_core_warn(priv->mdev, \"Failed to update cached encapsulation flow, %d\\n\",\n\t\t\t\t       err);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmlx5e_tc_unoffload_from_slow_path(esw, flow);\n\t\tflow->rule[0] = rule;\n\t\t \n\t\tflow_flag_set(flow, OFFLOADED);\n\t}\n}\n\nvoid mlx5e_tc_encap_flows_del(struct mlx5e_priv *priv,\n\t\t\t      struct mlx5e_encap_entry *e,\n\t\t\t      struct list_head *flow_list)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5_esw_flow_attr *esw_attr;\n\tstruct mlx5_flow_handle *rule;\n\tstruct mlx5_flow_attr *attr;\n\tstruct mlx5_flow_spec *spec;\n\tstruct mlx5e_tc_flow *flow;\n\tint err;\n\n\tlist_for_each_entry(flow, flow_list, tmp_list) {\n\t\tif (!mlx5e_is_offloaded_flow(flow))\n\t\t\tcontinue;\n\n\t\tattr = mlx5e_tc_get_encap_attr(flow);\n\t\tesw_attr = attr->esw_attr;\n\t\t \n\t\tesw_attr->dests[flow->tmp_entry_index].flags &= ~MLX5_ESW_DEST_ENCAP_VALID;\n\t\tesw_attr->dests[flow->tmp_entry_index].pkt_reformat = NULL;\n\n\t\t \n\t\tif (flow_flag_test(flow, SLOW))\n\t\t\tcontinue;\n\n\t\t \n\t\tspec = &flow->attr->parse_attr->spec;\n\t\trule = mlx5e_tc_offload_to_slow_path(esw, flow, spec);\n\n\t\tif (IS_ERR(rule)) {\n\t\t\terr = PTR_ERR(rule);\n\t\t\tmlx5_core_warn(priv->mdev, \"Failed to update slow path (encap) flow, %d\\n\",\n\t\t\t\t       err);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmlx5e_tc_unoffload_fdb_rules(esw, flow, flow->attr);\n\t\tmlx5e_tc_unoffload_flow_post_acts(flow);\n\t\tflow->rule[0] = rule;\n\t\t \n\t\tflow_flag_set(flow, OFFLOADED);\n\t}\n\n\t \n\te->flags &= ~MLX5_ENCAP_ENTRY_VALID;\n\tmlx5_packet_reformat_dealloc(priv->mdev, e->pkt_reformat);\n\te->pkt_reformat = NULL;\n}\n\nstatic void mlx5e_take_tmp_flow(struct mlx5e_tc_flow *flow,\n\t\t\t\tstruct list_head *flow_list,\n\t\t\t\tint index)\n{\n\tif (IS_ERR(mlx5e_flow_get(flow))) {\n\t\t \n\t\twait_for_completion(&flow->del_hw_done);\n\t\treturn;\n\t}\n\twait_for_completion(&flow->init_done);\n\n\tflow->tmp_entry_index = index;\n\tlist_add(&flow->tmp_list, flow_list);\n}\n\n \nvoid mlx5e_take_all_encap_flows(struct mlx5e_encap_entry *e, struct list_head *flow_list)\n{\n\tstruct encap_flow_item *efi;\n\tstruct mlx5e_tc_flow *flow;\n\n\tlist_for_each_entry(efi, &e->flows, list) {\n\t\tflow = container_of(efi, struct mlx5e_tc_flow, encaps[efi->index]);\n\t\tmlx5e_take_tmp_flow(flow, flow_list, efi->index);\n\t}\n}\n\n \nstatic void mlx5e_take_all_route_decap_flows(struct mlx5e_route_entry *r,\n\t\t\t\t\t     struct list_head *flow_list)\n{\n\tstruct mlx5e_tc_flow *flow;\n\n\tlist_for_each_entry(flow, &r->decap_flows, decap_routes)\n\t\tmlx5e_take_tmp_flow(flow, flow_list, 0);\n}\n\ntypedef bool (match_cb)(struct mlx5e_encap_entry *);\n\nstatic struct mlx5e_encap_entry *\nmlx5e_get_next_matching_encap(struct mlx5e_neigh_hash_entry *nhe,\n\t\t\t      struct mlx5e_encap_entry *e,\n\t\t\t      match_cb match)\n{\n\tstruct mlx5e_encap_entry *next = NULL;\n\nretry:\n\trcu_read_lock();\n\n\t \n\tfor (next = e ?\n\t\t     list_next_or_null_rcu(&nhe->encap_list,\n\t\t\t\t\t   &e->encap_list,\n\t\t\t\t\t   struct mlx5e_encap_entry,\n\t\t\t\t\t   encap_list) :\n\t\t     list_first_or_null_rcu(&nhe->encap_list,\n\t\t\t\t\t    struct mlx5e_encap_entry,\n\t\t\t\t\t    encap_list);\n\t     next;\n\t     next = list_next_or_null_rcu(&nhe->encap_list,\n\t\t\t\t\t  &next->encap_list,\n\t\t\t\t\t  struct mlx5e_encap_entry,\n\t\t\t\t\t  encap_list))\n\t\tif (mlx5e_encap_take(next))\n\t\t\tbreak;\n\n\trcu_read_unlock();\n\n\t \n\tif (e)\n\t\tmlx5e_encap_put(netdev_priv(e->out_dev), e);\n\tif (!next)\n\t\treturn next;\n\n\t \n\twait_for_completion(&next->res_ready);\n\t \n\tif (!match(next)) {\n\t\te = next;\n\t\tgoto retry;\n\t}\n\n\treturn next;\n}\n\nstatic bool mlx5e_encap_valid(struct mlx5e_encap_entry *e)\n{\n\treturn e->flags & MLX5_ENCAP_ENTRY_VALID;\n}\n\nstatic struct mlx5e_encap_entry *\nmlx5e_get_next_valid_encap(struct mlx5e_neigh_hash_entry *nhe,\n\t\t\t   struct mlx5e_encap_entry *e)\n{\n\treturn mlx5e_get_next_matching_encap(nhe, e, mlx5e_encap_valid);\n}\n\nstatic bool mlx5e_encap_initialized(struct mlx5e_encap_entry *e)\n{\n\treturn e->compl_result >= 0;\n}\n\nstruct mlx5e_encap_entry *\nmlx5e_get_next_init_encap(struct mlx5e_neigh_hash_entry *nhe,\n\t\t\t  struct mlx5e_encap_entry *e)\n{\n\treturn mlx5e_get_next_matching_encap(nhe, e, mlx5e_encap_initialized);\n}\n\nvoid mlx5e_tc_update_neigh_used_value(struct mlx5e_neigh_hash_entry *nhe)\n{\n\tstruct mlx5e_neigh *m_neigh = &nhe->m_neigh;\n\tstruct mlx5e_encap_entry *e = NULL;\n\tstruct mlx5e_tc_flow *flow;\n\tstruct mlx5_fc *counter;\n\tstruct neigh_table *tbl;\n\tbool neigh_used = false;\n\tstruct neighbour *n;\n\tu64 lastuse;\n\n\tif (m_neigh->family == AF_INET)\n\t\ttbl = &arp_tbl;\n#if IS_ENABLED(CONFIG_IPV6)\n\telse if (m_neigh->family == AF_INET6)\n\t\ttbl = ipv6_stub->nd_tbl;\n#endif\n\telse\n\t\treturn;\n\n\t \n\twhile ((e = mlx5e_get_next_valid_encap(nhe, e)) != NULL) {\n\t\tstruct mlx5e_priv *priv = netdev_priv(e->out_dev);\n\t\tstruct encap_flow_item *efi, *tmp;\n\t\tstruct mlx5_eswitch *esw;\n\t\tLIST_HEAD(flow_list);\n\n\t\tesw = priv->mdev->priv.eswitch;\n\t\tmutex_lock(&esw->offloads.encap_tbl_lock);\n\t\tlist_for_each_entry_safe(efi, tmp, &e->flows, list) {\n\t\t\tflow = container_of(efi, struct mlx5e_tc_flow,\n\t\t\t\t\t    encaps[efi->index]);\n\t\t\tif (IS_ERR(mlx5e_flow_get(flow)))\n\t\t\t\tcontinue;\n\t\t\tlist_add(&flow->tmp_list, &flow_list);\n\n\t\t\tif (mlx5e_is_offloaded_flow(flow)) {\n\t\t\t\tcounter = mlx5e_tc_get_counter(flow);\n\t\t\t\tlastuse = mlx5_fc_query_lastuse(counter);\n\t\t\t\tif (time_after((unsigned long)lastuse, nhe->reported_lastuse)) {\n\t\t\t\t\tneigh_used = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\n\t\tmlx5e_put_flow_list(priv, &flow_list);\n\t\tif (neigh_used) {\n\t\t\t \n\t\t\tmlx5e_encap_put(priv, e);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\ttrace_mlx5e_tc_update_neigh_used_value(nhe, neigh_used);\n\n\tif (neigh_used) {\n\t\tnhe->reported_lastuse = jiffies;\n\n\t\t \n\t\tn = neigh_lookup(tbl, &m_neigh->dst_ip, READ_ONCE(nhe->neigh_dev));\n\t\tif (!n)\n\t\t\treturn;\n\n\t\tneigh_event_send(n, NULL);\n\t\tneigh_release(n);\n\t}\n}\n\nstatic void mlx5e_encap_dealloc(struct mlx5e_priv *priv, struct mlx5e_encap_entry *e)\n{\n\tWARN_ON(!list_empty(&e->flows));\n\n\tif (e->compl_result > 0) {\n\t\tmlx5e_rep_encap_entry_detach(netdev_priv(e->out_dev), e);\n\n\t\tif (e->flags & MLX5_ENCAP_ENTRY_VALID)\n\t\t\tmlx5_packet_reformat_dealloc(priv->mdev, e->pkt_reformat);\n\t}\n\n\tkfree(e->tun_info);\n\tkfree(e->encap_header);\n\tkfree_rcu(e, rcu);\n}\n\nstatic void mlx5e_decap_dealloc(struct mlx5e_priv *priv,\n\t\t\t\tstruct mlx5e_decap_entry *d)\n{\n\tWARN_ON(!list_empty(&d->flows));\n\n\tif (!d->compl_result)\n\t\tmlx5_packet_reformat_dealloc(priv->mdev, d->pkt_reformat);\n\n\tkfree_rcu(d, rcu);\n}\n\nvoid mlx5e_encap_put(struct mlx5e_priv *priv, struct mlx5e_encap_entry *e)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\n\tif (!refcount_dec_and_mutex_lock(&e->refcnt, &esw->offloads.encap_tbl_lock))\n\t\treturn;\n\tlist_del(&e->route_list);\n\thash_del_rcu(&e->encap_hlist);\n\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\n\tmlx5e_encap_dealloc(priv, e);\n}\n\nstatic void mlx5e_encap_put_locked(struct mlx5e_priv *priv, struct mlx5e_encap_entry *e)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\n\tlockdep_assert_held(&esw->offloads.encap_tbl_lock);\n\n\tif (!refcount_dec_and_test(&e->refcnt))\n\t\treturn;\n\tlist_del(&e->route_list);\n\thash_del_rcu(&e->encap_hlist);\n\tmlx5e_encap_dealloc(priv, e);\n}\n\nstatic void mlx5e_decap_put(struct mlx5e_priv *priv, struct mlx5e_decap_entry *d)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\n\tif (!refcount_dec_and_mutex_lock(&d->refcnt, &esw->offloads.decap_tbl_lock))\n\t\treturn;\n\thash_del_rcu(&d->hlist);\n\tmutex_unlock(&esw->offloads.decap_tbl_lock);\n\n\tmlx5e_decap_dealloc(priv, d);\n}\n\nstatic void mlx5e_detach_encap_route(struct mlx5e_priv *priv,\n\t\t\t\t     struct mlx5e_tc_flow *flow,\n\t\t\t\t     int out_index);\n\nvoid mlx5e_detach_encap(struct mlx5e_priv *priv,\n\t\t\tstruct mlx5e_tc_flow *flow,\n\t\t\tstruct mlx5_flow_attr *attr,\n\t\t\tint out_index)\n{\n\tstruct mlx5e_encap_entry *e = flow->encaps[out_index].e;\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\n\tif (!mlx5e_is_eswitch_flow(flow))\n\t\treturn;\n\n\tif (attr->esw_attr->dests[out_index].flags &\n\t    MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE)\n\t\tmlx5e_detach_encap_route(priv, flow, out_index);\n\n\t \n\tif (!e)\n\t\treturn;\n\n\tmutex_lock(&esw->offloads.encap_tbl_lock);\n\tlist_del(&flow->encaps[out_index].list);\n\tflow->encaps[out_index].e = NULL;\n\tif (!refcount_dec_and_test(&e->refcnt)) {\n\t\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\t\treturn;\n\t}\n\tlist_del(&e->route_list);\n\thash_del_rcu(&e->encap_hlist);\n\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\n\tmlx5e_encap_dealloc(priv, e);\n}\n\nvoid mlx5e_detach_decap(struct mlx5e_priv *priv,\n\t\t\tstruct mlx5e_tc_flow *flow)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_decap_entry *d = flow->decap_reformat;\n\n\tif (!d)\n\t\treturn;\n\n\tmutex_lock(&esw->offloads.decap_tbl_lock);\n\tlist_del(&flow->l3_to_l2_reformat);\n\tflow->decap_reformat = NULL;\n\n\tif (!refcount_dec_and_test(&d->refcnt)) {\n\t\tmutex_unlock(&esw->offloads.decap_tbl_lock);\n\t\treturn;\n\t}\n\thash_del_rcu(&d->hlist);\n\tmutex_unlock(&esw->offloads.decap_tbl_lock);\n\n\tmlx5e_decap_dealloc(priv, d);\n}\n\nbool mlx5e_tc_tun_encap_info_equal_generic(struct mlx5e_encap_key *a,\n\t\t\t\t\t   struct mlx5e_encap_key *b)\n{\n\treturn memcmp(a->ip_tun_key, b->ip_tun_key, sizeof(*a->ip_tun_key)) == 0 &&\n\t\ta->tc_tunnel->tunnel_type == b->tc_tunnel->tunnel_type;\n}\n\nbool mlx5e_tc_tun_encap_info_equal_options(struct mlx5e_encap_key *a,\n\t\t\t\t\t   struct mlx5e_encap_key *b,\n\t\t\t\t\t   __be16 tun_flags)\n{\n\tstruct ip_tunnel_info *a_info;\n\tstruct ip_tunnel_info *b_info;\n\tbool a_has_opts, b_has_opts;\n\n\tif (!mlx5e_tc_tun_encap_info_equal_generic(a, b))\n\t\treturn false;\n\n\ta_has_opts = !!(a->ip_tun_key->tun_flags & tun_flags);\n\tb_has_opts = !!(b->ip_tun_key->tun_flags & tun_flags);\n\n\t \n\tif (!a_has_opts && !b_has_opts)\n\t\treturn true;\n\n\tif (a_has_opts != b_has_opts)\n\t\treturn false;\n\n\t \n\ta_info = container_of(a->ip_tun_key, struct ip_tunnel_info, key);\n\tb_info = container_of(b->ip_tun_key, struct ip_tunnel_info, key);\n\n\treturn a_info->options_len == b_info->options_len &&\n\t       !memcmp(ip_tunnel_info_opts(a_info),\n\t\t       ip_tunnel_info_opts(b_info),\n\t\t       a_info->options_len);\n}\n\nstatic int cmp_decap_info(struct mlx5e_decap_key *a,\n\t\t\t  struct mlx5e_decap_key *b)\n{\n\treturn memcmp(&a->key, &b->key, sizeof(b->key));\n}\n\nstatic int hash_encap_info(struct mlx5e_encap_key *key)\n{\n\treturn jhash(key->ip_tun_key, sizeof(*key->ip_tun_key),\n\t\t     key->tc_tunnel->tunnel_type);\n}\n\nstatic int hash_decap_info(struct mlx5e_decap_key *key)\n{\n\treturn jhash(&key->key, sizeof(key->key), 0);\n}\n\nbool mlx5e_encap_take(struct mlx5e_encap_entry *e)\n{\n\treturn refcount_inc_not_zero(&e->refcnt);\n}\n\nstatic bool mlx5e_decap_take(struct mlx5e_decap_entry *e)\n{\n\treturn refcount_inc_not_zero(&e->refcnt);\n}\n\nstatic struct mlx5e_encap_entry *\nmlx5e_encap_get(struct mlx5e_priv *priv, struct mlx5e_encap_key *key,\n\t\tuintptr_t hash_key)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_encap_key e_key;\n\tstruct mlx5e_encap_entry *e;\n\n\thash_for_each_possible_rcu(esw->offloads.encap_tbl, e,\n\t\t\t\t   encap_hlist, hash_key) {\n\t\te_key.ip_tun_key = &e->tun_info->key;\n\t\te_key.tc_tunnel = e->tunnel;\n\t\tif (e->tunnel->encap_info_equal(&e_key, key) &&\n\t\t    mlx5e_encap_take(e))\n\t\t\treturn e;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct mlx5e_decap_entry *\nmlx5e_decap_get(struct mlx5e_priv *priv, struct mlx5e_decap_key *key,\n\t\tuintptr_t hash_key)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_decap_key r_key;\n\tstruct mlx5e_decap_entry *e;\n\n\thash_for_each_possible_rcu(esw->offloads.decap_tbl, e,\n\t\t\t\t   hlist, hash_key) {\n\t\tr_key = e->key;\n\t\tif (!cmp_decap_info(&r_key, key) &&\n\t\t    mlx5e_decap_take(e))\n\t\t\treturn e;\n\t}\n\treturn NULL;\n}\n\nstruct ip_tunnel_info *mlx5e_dup_tun_info(const struct ip_tunnel_info *tun_info)\n{\n\tsize_t tun_size = sizeof(*tun_info) + tun_info->options_len;\n\n\treturn kmemdup(tun_info, tun_size, GFP_KERNEL);\n}\n\nstatic bool is_duplicated_encap_entry(struct mlx5e_priv *priv,\n\t\t\t\t      struct mlx5e_tc_flow *flow,\n\t\t\t\t      int out_index,\n\t\t\t\t      struct mlx5e_encap_entry *e,\n\t\t\t\t      struct netlink_ext_ack *extack)\n{\n\tint i;\n\n\tfor (i = 0; i < out_index; i++) {\n\t\tif (flow->encaps[i].e != e)\n\t\t\tcontinue;\n\t\tNL_SET_ERR_MSG_MOD(extack, \"can't duplicate encap action\");\n\t\tnetdev_err(priv->netdev, \"can't duplicate encap action\\n\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int mlx5e_set_vf_tunnel(struct mlx5_eswitch *esw,\n\t\t\t       struct mlx5_flow_attr *attr,\n\t\t\t       struct mlx5e_tc_mod_hdr_acts *mod_hdr_acts,\n\t\t\t       struct net_device *out_dev,\n\t\t\t       int route_dev_ifindex,\n\t\t\t       int out_index)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tstruct net_device *route_dev;\n\tu16 vport_num;\n\tint err = 0;\n\tu32 data;\n\n\troute_dev = dev_get_by_index(dev_net(out_dev), route_dev_ifindex);\n\n\tif (!route_dev || route_dev->netdev_ops != &mlx5e_netdev_ops ||\n\t    !mlx5e_tc_is_vf_tunnel(out_dev, route_dev))\n\t\tgoto out;\n\n\terr = mlx5e_tc_query_route_vport(out_dev, route_dev, &vport_num);\n\tif (err)\n\t\tgoto out;\n\n\tattr->dest_chain = 0;\n\tattr->action |= MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;\n\tesw_attr->dests[out_index].flags |= MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE;\n\tdata = mlx5_eswitch_get_vport_metadata_for_set(esw_attr->in_mdev->priv.eswitch,\n\t\t\t\t\t\t       vport_num);\n\terr = mlx5e_tc_match_to_reg_set_and_get_id(esw->dev, mod_hdr_acts,\n\t\t\t\t\t\t   MLX5_FLOW_NAMESPACE_FDB,\n\t\t\t\t\t\t   VPORT_TO_REG, data);\n\tif (err >= 0) {\n\t\tesw_attr->dests[out_index].src_port_rewrite_act_id = err;\n\t\terr = 0;\n\t}\n\nout:\n\tif (route_dev)\n\t\tdev_put(route_dev);\n\treturn err;\n}\n\nstatic int mlx5e_update_vf_tunnel(struct mlx5_eswitch *esw,\n\t\t\t\t  struct mlx5_esw_flow_attr *attr,\n\t\t\t\t  struct mlx5e_tc_mod_hdr_acts *mod_hdr_acts,\n\t\t\t\t  struct net_device *out_dev,\n\t\t\t\t  int route_dev_ifindex,\n\t\t\t\t  int out_index)\n{\n\tint act_id = attr->dests[out_index].src_port_rewrite_act_id;\n\tstruct net_device *route_dev;\n\tu16 vport_num;\n\tint err = 0;\n\tu32 data;\n\n\troute_dev = dev_get_by_index(dev_net(out_dev), route_dev_ifindex);\n\n\tif (!route_dev || route_dev->netdev_ops != &mlx5e_netdev_ops ||\n\t    !mlx5e_tc_is_vf_tunnel(out_dev, route_dev)) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\n\terr = mlx5e_tc_query_route_vport(out_dev, route_dev, &vport_num);\n\tif (err)\n\t\tgoto out;\n\n\tdata = mlx5_eswitch_get_vport_metadata_for_set(attr->in_mdev->priv.eswitch,\n\t\t\t\t\t\t       vport_num);\n\tmlx5e_tc_match_to_reg_mod_hdr_change(esw->dev, mod_hdr_acts, VPORT_TO_REG, act_id, data);\n\nout:\n\tif (route_dev)\n\t\tdev_put(route_dev);\n\treturn err;\n}\n\nstatic unsigned int mlx5e_route_tbl_get_last_update(struct mlx5e_priv *priv)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5_rep_uplink_priv *uplink_priv;\n\tstruct mlx5e_rep_priv *uplink_rpriv;\n\tstruct mlx5e_tc_tun_encap *encap;\n\tunsigned int ret;\n\n\tuplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);\n\tuplink_priv = &uplink_rpriv->uplink_priv;\n\tencap = uplink_priv->encap;\n\n\tspin_lock_bh(&encap->route_lock);\n\tret = encap->route_tbl_last_update;\n\tspin_unlock_bh(&encap->route_lock);\n\treturn ret;\n}\n\nstatic int mlx5e_attach_encap_route(struct mlx5e_priv *priv,\n\t\t\t\t    struct mlx5e_tc_flow *flow,\n\t\t\t\t    struct mlx5_flow_attr *attr,\n\t\t\t\t    struct mlx5e_encap_entry *e,\n\t\t\t\t    bool new_encap_entry,\n\t\t\t\t    unsigned long tbl_time_before,\n\t\t\t\t    int out_index);\n\nint mlx5e_attach_encap(struct mlx5e_priv *priv,\n\t\t       struct mlx5e_tc_flow *flow,\n\t\t       struct mlx5_flow_attr *attr,\n\t\t       struct net_device *mirred_dev,\n\t\t       int out_index,\n\t\t       struct netlink_ext_ack *extack,\n\t\t       struct net_device **encap_dev)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_tc_flow_parse_attr *parse_attr;\n\tconst struct ip_tunnel_info *tun_info;\n\tconst struct mlx5e_mpls_info *mpls_info;\n\tunsigned long tbl_time_before = 0;\n\tstruct mlx5e_encap_entry *e;\n\tstruct mlx5e_encap_key key;\n\tbool entry_created = false;\n\tunsigned short family;\n\tuintptr_t hash_key;\n\tint err = 0;\n\n\tlockdep_assert_held(&esw->offloads.encap_tbl_lock);\n\n\tparse_attr = attr->parse_attr;\n\ttun_info = parse_attr->tun_info[out_index];\n\tmpls_info = &parse_attr->mpls_info[out_index];\n\tfamily = ip_tunnel_info_af(tun_info);\n\tkey.ip_tun_key = &tun_info->key;\n\tkey.tc_tunnel = mlx5e_get_tc_tun(mirred_dev);\n\tif (!key.tc_tunnel) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Unsupported tunnel\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\thash_key = hash_encap_info(&key);\n\n\te = mlx5e_encap_get(priv, &key, hash_key);\n\n\t \n\tif (e) {\n\t\t \n\t\tif (is_duplicated_encap_entry(priv, flow, out_index, e, extack)) {\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tgoto attach_flow;\n\t}\n\n\te = kzalloc(sizeof(*e), GFP_KERNEL);\n\tif (!e) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\trefcount_set(&e->refcnt, 1);\n\tinit_completion(&e->res_ready);\n\tentry_created = true;\n\tINIT_LIST_HEAD(&e->route_list);\n\n\ttun_info = mlx5e_dup_tun_info(tun_info);\n\tif (!tun_info) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err_init;\n\t}\n\te->tun_info = tun_info;\n\tmemcpy(&e->mpls_info, mpls_info, sizeof(*mpls_info));\n\terr = mlx5e_tc_tun_init_encap_attr(mirred_dev, priv, e, extack);\n\tif (err)\n\t\tgoto out_err_init;\n\n\tINIT_LIST_HEAD(&e->flows);\n\thash_add_rcu(esw->offloads.encap_tbl, &e->encap_hlist, hash_key);\n\ttbl_time_before = mlx5e_route_tbl_get_last_update(priv);\n\n\tif (family == AF_INET)\n\t\terr = mlx5e_tc_tun_create_header_ipv4(priv, mirred_dev, e);\n\telse if (family == AF_INET6)\n\t\terr = mlx5e_tc_tun_create_header_ipv6(priv, mirred_dev, e);\n\n\tcomplete_all(&e->res_ready);\n\tif (err) {\n\t\te->compl_result = err;\n\t\tgoto out_err;\n\t}\n\te->compl_result = 1;\n\nattach_flow:\n\terr = mlx5e_attach_encap_route(priv, flow, attr, e, entry_created,\n\t\t\t\t       tbl_time_before, out_index);\n\tif (err)\n\t\tgoto out_err;\n\n\terr = mlx5e_set_int_port_tunnel(priv, attr, e, out_index);\n\tif (err == -EOPNOTSUPP) {\n\t\t \n\t\tmlx5_core_dbg(priv->mdev, \"attaching int port as encap dev not supported, using uplink\\n\");\n\t\terr = 0;\n\t} else if (err) {\n\t\tgoto out_err;\n\t}\n\n\tflow->encaps[out_index].e = e;\n\tlist_add(&flow->encaps[out_index].list, &e->flows);\n\tflow->encaps[out_index].index = out_index;\n\t*encap_dev = e->out_dev;\n\tif (e->flags & MLX5_ENCAP_ENTRY_VALID) {\n\t\tattr->esw_attr->dests[out_index].pkt_reformat = e->pkt_reformat;\n\t\tattr->esw_attr->dests[out_index].flags |= MLX5_ESW_DEST_ENCAP_VALID;\n\t} else {\n\t\tflow_flag_set(flow, SLOW);\n\t}\n\n\treturn err;\n\nout_err:\n\tif (e)\n\t\tmlx5e_encap_put_locked(priv, e);\n\treturn err;\n\nout_err_init:\n\tkfree(tun_info);\n\tkfree(e);\n\treturn err;\n}\n\nint mlx5e_attach_decap(struct mlx5e_priv *priv,\n\t\t       struct mlx5e_tc_flow *flow,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5_esw_flow_attr *attr = flow->attr->esw_attr;\n\tstruct mlx5_pkt_reformat_params reformat_params;\n\tstruct mlx5e_decap_entry *d;\n\tstruct mlx5e_decap_key key;\n\tuintptr_t hash_key;\n\tint err = 0;\n\n\tif (sizeof(attr->eth) > MLX5_CAP_ESW(priv->mdev, max_encap_header_size)) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"encap header larger than max supported\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tkey.key = attr->eth;\n\thash_key = hash_decap_info(&key);\n\tmutex_lock(&esw->offloads.decap_tbl_lock);\n\td = mlx5e_decap_get(priv, &key, hash_key);\n\tif (d) {\n\t\tmutex_unlock(&esw->offloads.decap_tbl_lock);\n\t\twait_for_completion(&d->res_ready);\n\t\tmutex_lock(&esw->offloads.decap_tbl_lock);\n\t\tif (d->compl_result) {\n\t\t\terr = -EREMOTEIO;\n\t\t\tgoto out_free;\n\t\t}\n\t\tgoto found;\n\t}\n\n\td = kzalloc(sizeof(*d), GFP_KERNEL);\n\tif (!d) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\td->key = key;\n\trefcount_set(&d->refcnt, 1);\n\tinit_completion(&d->res_ready);\n\tINIT_LIST_HEAD(&d->flows);\n\thash_add_rcu(esw->offloads.decap_tbl, &d->hlist, hash_key);\n\tmutex_unlock(&esw->offloads.decap_tbl_lock);\n\n\tmemset(&reformat_params, 0, sizeof(reformat_params));\n\treformat_params.type = MLX5_REFORMAT_TYPE_L3_TUNNEL_TO_L2;\n\treformat_params.size = sizeof(attr->eth);\n\treformat_params.data = &attr->eth;\n\td->pkt_reformat = mlx5_packet_reformat_alloc(priv->mdev,\n\t\t\t\t\t\t     &reformat_params,\n\t\t\t\t\t\t     MLX5_FLOW_NAMESPACE_FDB);\n\tif (IS_ERR(d->pkt_reformat)) {\n\t\terr = PTR_ERR(d->pkt_reformat);\n\t\td->compl_result = err;\n\t}\n\tmutex_lock(&esw->offloads.decap_tbl_lock);\n\tcomplete_all(&d->res_ready);\n\tif (err)\n\t\tgoto out_free;\n\nfound:\n\tflow->decap_reformat = d;\n\tattr->decap_pkt_reformat = d->pkt_reformat;\n\tlist_add(&flow->l3_to_l2_reformat, &d->flows);\n\tmutex_unlock(&esw->offloads.decap_tbl_lock);\n\treturn 0;\n\nout_free:\n\tmutex_unlock(&esw->offloads.decap_tbl_lock);\n\tmlx5e_decap_put(priv, d);\n\treturn err;\n\nout_err:\n\tmutex_unlock(&esw->offloads.decap_tbl_lock);\n\treturn err;\n}\n\nint mlx5e_tc_tun_encap_dests_set(struct mlx5e_priv *priv,\n\t\t\t\t struct mlx5e_tc_flow *flow,\n\t\t\t\t struct mlx5_flow_attr *attr,\n\t\t\t\t struct netlink_ext_ack *extack,\n\t\t\t\t bool *vf_tun)\n{\n\tstruct mlx5e_tc_flow_parse_attr *parse_attr;\n\tstruct mlx5_esw_flow_attr *esw_attr;\n\tstruct net_device *encap_dev = NULL;\n\tstruct mlx5e_rep_priv *rpriv;\n\tstruct mlx5e_priv *out_priv;\n\tstruct mlx5_eswitch *esw;\n\tint out_index;\n\tint err = 0;\n\n\tparse_attr = attr->parse_attr;\n\tesw_attr = attr->esw_attr;\n\t*vf_tun = false;\n\n\tesw = priv->mdev->priv.eswitch;\n\tmutex_lock(&esw->offloads.encap_tbl_lock);\n\tfor (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++) {\n\t\tstruct net_device *out_dev;\n\t\tint mirred_ifindex;\n\n\t\tif (!(esw_attr->dests[out_index].flags & MLX5_ESW_DEST_ENCAP))\n\t\t\tcontinue;\n\n\t\tmirred_ifindex = parse_attr->mirred_ifindex[out_index];\n\t\tout_dev = dev_get_by_index(dev_net(priv->netdev), mirred_ifindex);\n\t\tif (!out_dev) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack, \"Requested mirred device not found\");\n\t\t\terr = -ENODEV;\n\t\t\tgoto out;\n\t\t}\n\t\terr = mlx5e_attach_encap(priv, flow, attr, out_dev, out_index,\n\t\t\t\t\t extack, &encap_dev);\n\t\tdev_put(out_dev);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (esw_attr->dests[out_index].flags &\n\t\t    MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE &&\n\t\t    !esw_attr->dest_int_port)\n\t\t\t*vf_tun = true;\n\n\t\tout_priv = netdev_priv(encap_dev);\n\t\trpriv = out_priv->ppriv;\n\t\tesw_attr->dests[out_index].vport_valid = true;\n\t\tesw_attr->dests[out_index].vport = rpriv->rep->vport;\n\t\tesw_attr->dests[out_index].mdev = out_priv->mdev;\n\t}\n\n\tif (*vf_tun && esw_attr->out_count > 1) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"VF tunnel encap with mirroring is not supported\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\nout:\n\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\treturn err;\n}\n\nvoid mlx5e_tc_tun_encap_dests_unset(struct mlx5e_priv *priv,\n\t\t\t\t    struct mlx5e_tc_flow *flow,\n\t\t\t\t    struct mlx5_flow_attr *attr)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr;\n\tint out_index;\n\n\tif (!mlx5e_is_eswitch_flow(flow))\n\t\treturn;\n\n\tesw_attr = attr->esw_attr;\n\n\tfor (out_index = 0; out_index < MLX5_MAX_FLOW_FWD_VPORTS; out_index++) {\n\t\tif (!(esw_attr->dests[out_index].flags & MLX5_ESW_DEST_ENCAP))\n\t\t\tcontinue;\n\n\t\tmlx5e_detach_encap(flow->priv, flow, attr, out_index);\n\t\tkfree(attr->parse_attr->tun_info[out_index]);\n\t}\n}\n\nstatic int cmp_route_info(struct mlx5e_route_key *a,\n\t\t\t  struct mlx5e_route_key *b)\n{\n\tif (a->ip_version == 4 && b->ip_version == 4)\n\t\treturn memcmp(&a->endpoint_ip.v4, &b->endpoint_ip.v4,\n\t\t\t      sizeof(a->endpoint_ip.v4));\n\telse if (a->ip_version == 6 && b->ip_version == 6)\n\t\treturn memcmp(&a->endpoint_ip.v6, &b->endpoint_ip.v6,\n\t\t\t      sizeof(a->endpoint_ip.v6));\n\treturn 1;\n}\n\nstatic u32 hash_route_info(struct mlx5e_route_key *key)\n{\n\tif (key->ip_version == 4)\n\t\treturn jhash(&key->endpoint_ip.v4, sizeof(key->endpoint_ip.v4), 0);\n\treturn jhash(&key->endpoint_ip.v6, sizeof(key->endpoint_ip.v6), 0);\n}\n\nstatic void mlx5e_route_dealloc(struct mlx5e_priv *priv,\n\t\t\t\tstruct mlx5e_route_entry *r)\n{\n\tWARN_ON(!list_empty(&r->decap_flows));\n\tWARN_ON(!list_empty(&r->encap_entries));\n\n\tkfree_rcu(r, rcu);\n}\n\nstatic void mlx5e_route_put(struct mlx5e_priv *priv, struct mlx5e_route_entry *r)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\n\tif (!refcount_dec_and_mutex_lock(&r->refcnt, &esw->offloads.encap_tbl_lock))\n\t\treturn;\n\n\thash_del_rcu(&r->hlist);\n\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\n\tmlx5e_route_dealloc(priv, r);\n}\n\nstatic void mlx5e_route_put_locked(struct mlx5e_priv *priv, struct mlx5e_route_entry *r)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\n\tlockdep_assert_held(&esw->offloads.encap_tbl_lock);\n\n\tif (!refcount_dec_and_test(&r->refcnt))\n\t\treturn;\n\thash_del_rcu(&r->hlist);\n\tmlx5e_route_dealloc(priv, r);\n}\n\nstatic struct mlx5e_route_entry *\nmlx5e_route_get(struct mlx5e_tc_tun_encap *encap, struct mlx5e_route_key *key,\n\t\tu32 hash_key)\n{\n\tstruct mlx5e_route_key r_key;\n\tstruct mlx5e_route_entry *r;\n\n\thash_for_each_possible(encap->route_tbl, r, hlist, hash_key) {\n\t\tr_key = r->key;\n\t\tif (!cmp_route_info(&r_key, key) &&\n\t\t    refcount_inc_not_zero(&r->refcnt))\n\t\t\treturn r;\n\t}\n\treturn NULL;\n}\n\nstatic struct mlx5e_route_entry *\nmlx5e_route_get_create(struct mlx5e_priv *priv,\n\t\t       struct mlx5e_route_key *key,\n\t\t       int tunnel_dev_index,\n\t\t       unsigned long *route_tbl_change_time)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5_rep_uplink_priv *uplink_priv;\n\tstruct mlx5e_rep_priv *uplink_rpriv;\n\tstruct mlx5e_tc_tun_encap *encap;\n\tstruct mlx5e_route_entry *r;\n\tu32 hash_key;\n\n\tuplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);\n\tuplink_priv = &uplink_rpriv->uplink_priv;\n\tencap = uplink_priv->encap;\n\n\thash_key = hash_route_info(key);\n\tspin_lock_bh(&encap->route_lock);\n\tr = mlx5e_route_get(encap, key, hash_key);\n\tspin_unlock_bh(&encap->route_lock);\n\tif (r) {\n\t\tif (!mlx5e_route_entry_valid(r)) {\n\t\t\tmlx5e_route_put_locked(priv, r);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\treturn r;\n\t}\n\n\tr = kzalloc(sizeof(*r), GFP_KERNEL);\n\tif (!r)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tr->key = *key;\n\tr->flags |= MLX5E_ROUTE_ENTRY_VALID;\n\tr->tunnel_dev_index = tunnel_dev_index;\n\trefcount_set(&r->refcnt, 1);\n\tINIT_LIST_HEAD(&r->decap_flows);\n\tINIT_LIST_HEAD(&r->encap_entries);\n\n\tspin_lock_bh(&encap->route_lock);\n\t*route_tbl_change_time = encap->route_tbl_last_update;\n\thash_add(encap->route_tbl, &r->hlist, hash_key);\n\tspin_unlock_bh(&encap->route_lock);\n\n\treturn r;\n}\n\nstatic struct mlx5e_route_entry *\nmlx5e_route_lookup_for_update(struct mlx5e_tc_tun_encap *encap, struct mlx5e_route_key *key)\n{\n\tu32 hash_key = hash_route_info(key);\n\tstruct mlx5e_route_entry *r;\n\n\tspin_lock_bh(&encap->route_lock);\n\tencap->route_tbl_last_update = jiffies;\n\tr = mlx5e_route_get(encap, key, hash_key);\n\tspin_unlock_bh(&encap->route_lock);\n\n\treturn r;\n}\n\nstruct mlx5e_tc_fib_event_data {\n\tstruct work_struct work;\n\tunsigned long event;\n\tstruct mlx5e_route_entry *r;\n\tstruct net_device *ul_dev;\n};\n\nstatic void mlx5e_tc_fib_event_work(struct work_struct *work);\nstatic struct mlx5e_tc_fib_event_data *\nmlx5e_tc_init_fib_work(unsigned long event, struct net_device *ul_dev, gfp_t flags)\n{\n\tstruct mlx5e_tc_fib_event_data *fib_work;\n\n\tfib_work = kzalloc(sizeof(*fib_work), flags);\n\tif (WARN_ON(!fib_work))\n\t\treturn NULL;\n\n\tINIT_WORK(&fib_work->work, mlx5e_tc_fib_event_work);\n\tfib_work->event = event;\n\tfib_work->ul_dev = ul_dev;\n\n\treturn fib_work;\n}\n\nstatic int\nmlx5e_route_enqueue_update(struct mlx5e_priv *priv,\n\t\t\t   struct mlx5e_route_entry *r,\n\t\t\t   unsigned long event)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_tc_fib_event_data *fib_work;\n\tstruct mlx5e_rep_priv *uplink_rpriv;\n\tstruct net_device *ul_dev;\n\n\tuplink_rpriv = mlx5_eswitch_get_uplink_priv(esw, REP_ETH);\n\tul_dev = uplink_rpriv->netdev;\n\n\tfib_work = mlx5e_tc_init_fib_work(event, ul_dev, GFP_KERNEL);\n\tif (!fib_work)\n\t\treturn -ENOMEM;\n\n\tdev_hold(ul_dev);\n\trefcount_inc(&r->refcnt);\n\tfib_work->r = r;\n\tqueue_work(priv->wq, &fib_work->work);\n\n\treturn 0;\n}\n\nint mlx5e_attach_decap_route(struct mlx5e_priv *priv,\n\t\t\t     struct mlx5e_tc_flow *flow)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tunsigned long tbl_time_before, tbl_time_after;\n\tstruct mlx5e_tc_flow_parse_attr *parse_attr;\n\tstruct mlx5_flow_attr *attr = flow->attr;\n\tstruct mlx5_esw_flow_attr *esw_attr;\n\tstruct mlx5e_route_entry *r;\n\tstruct mlx5e_route_key key;\n\tint err = 0;\n\n\tesw_attr = attr->esw_attr;\n\tparse_attr = attr->parse_attr;\n\tmutex_lock(&esw->offloads.encap_tbl_lock);\n\tif (!esw_attr->rx_tun_attr)\n\t\tgoto out;\n\n\ttbl_time_before = mlx5e_route_tbl_get_last_update(priv);\n\ttbl_time_after = tbl_time_before;\n\terr = mlx5e_tc_tun_route_lookup(priv, &parse_attr->spec, attr, parse_attr->filter_dev);\n\tif (err || !esw_attr->rx_tun_attr->decap_vport)\n\t\tgoto out;\n\n\tkey.ip_version = attr->tun_ip_version;\n\tif (key.ip_version == 4)\n\t\tkey.endpoint_ip.v4 = esw_attr->rx_tun_attr->dst_ip.v4;\n\telse\n\t\tkey.endpoint_ip.v6 = esw_attr->rx_tun_attr->dst_ip.v6;\n\n\tr = mlx5e_route_get_create(priv, &key, parse_attr->filter_dev->ifindex,\n\t\t\t\t   &tbl_time_after);\n\tif (IS_ERR(r)) {\n\t\terr = PTR_ERR(r);\n\t\tgoto out;\n\t}\n\t \n\tif (tbl_time_before != tbl_time_after) {\n\t\terr = mlx5e_route_enqueue_update(priv, r, FIB_EVENT_ENTRY_REPLACE);\n\t\tif (err) {\n\t\t\tmlx5e_route_put_locked(priv, r);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tflow->decap_route = r;\n\tlist_add(&flow->decap_routes, &r->decap_flows);\n\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\treturn 0;\n\nout:\n\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\treturn err;\n}\n\nstatic int mlx5e_attach_encap_route(struct mlx5e_priv *priv,\n\t\t\t\t    struct mlx5e_tc_flow *flow,\n\t\t\t\t    struct mlx5_flow_attr *attr,\n\t\t\t\t    struct mlx5e_encap_entry *e,\n\t\t\t\t    bool new_encap_entry,\n\t\t\t\t    unsigned long tbl_time_before,\n\t\t\t\t    int out_index)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tunsigned long tbl_time_after = tbl_time_before;\n\tstruct mlx5e_tc_flow_parse_attr *parse_attr;\n\tconst struct ip_tunnel_info *tun_info;\n\tstruct mlx5_esw_flow_attr *esw_attr;\n\tstruct mlx5e_route_entry *r;\n\tstruct mlx5e_route_key key;\n\tunsigned short family;\n\tint err = 0;\n\n\tesw_attr = attr->esw_attr;\n\tparse_attr = attr->parse_attr;\n\ttun_info = parse_attr->tun_info[out_index];\n\tfamily = ip_tunnel_info_af(tun_info);\n\n\tif (family == AF_INET) {\n\t\tkey.endpoint_ip.v4 = tun_info->key.u.ipv4.src;\n\t\tkey.ip_version = 4;\n\t} else if (family == AF_INET6) {\n\t\tkey.endpoint_ip.v6 = tun_info->key.u.ipv6.src;\n\t\tkey.ip_version = 6;\n\t}\n\n\terr = mlx5e_set_vf_tunnel(esw, attr, &parse_attr->mod_hdr_acts, e->out_dev,\n\t\t\t\t  e->route_dev_ifindex, out_index);\n\tif (err || !(esw_attr->dests[out_index].flags &\n\t\t     MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE))\n\t\treturn err;\n\n\tr = mlx5e_route_get_create(priv, &key, parse_attr->mirred_ifindex[out_index],\n\t\t\t\t   &tbl_time_after);\n\tif (IS_ERR(r))\n\t\treturn PTR_ERR(r);\n\t \n\tif (tbl_time_before != tbl_time_after) {\n\t\terr = mlx5e_route_enqueue_update(priv, r, FIB_EVENT_ENTRY_REPLACE);\n\t\tif (err) {\n\t\t\tmlx5e_route_put_locked(priv, r);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tflow->encap_routes[out_index].r = r;\n\tif (new_encap_entry)\n\t\tlist_add(&e->route_list, &r->encap_entries);\n\tflow->encap_routes[out_index].index = out_index;\n\treturn 0;\n}\n\nvoid mlx5e_detach_decap_route(struct mlx5e_priv *priv,\n\t\t\t      struct mlx5e_tc_flow *flow)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_route_entry *r = flow->decap_route;\n\n\tif (!r)\n\t\treturn;\n\n\tmutex_lock(&esw->offloads.encap_tbl_lock);\n\tlist_del(&flow->decap_routes);\n\tflow->decap_route = NULL;\n\n\tif (!refcount_dec_and_test(&r->refcnt)) {\n\t\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\t\treturn;\n\t}\n\thash_del_rcu(&r->hlist);\n\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\n\tmlx5e_route_dealloc(priv, r);\n}\n\nstatic void mlx5e_detach_encap_route(struct mlx5e_priv *priv,\n\t\t\t\t     struct mlx5e_tc_flow *flow,\n\t\t\t\t     int out_index)\n{\n\tstruct mlx5e_route_entry *r = flow->encap_routes[out_index].r;\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_encap_entry *e, *tmp;\n\n\tif (!r)\n\t\treturn;\n\n\tmutex_lock(&esw->offloads.encap_tbl_lock);\n\tflow->encap_routes[out_index].r = NULL;\n\n\tif (!refcount_dec_and_test(&r->refcnt)) {\n\t\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\t\treturn;\n\t}\n\tlist_for_each_entry_safe(e, tmp, &r->encap_entries, route_list)\n\t\tlist_del_init(&e->route_list);\n\thash_del_rcu(&r->hlist);\n\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\n\tmlx5e_route_dealloc(priv, r);\n}\n\nstatic void mlx5e_invalidate_encap(struct mlx5e_priv *priv,\n\t\t\t\t   struct mlx5e_encap_entry *e,\n\t\t\t\t   struct list_head *encap_flows)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_tc_flow *flow;\n\n\tlist_for_each_entry(flow, encap_flows, tmp_list) {\n\t\tstruct mlx5_esw_flow_attr *esw_attr;\n\t\tstruct mlx5_flow_attr *attr;\n\n\t\tif (!mlx5e_is_offloaded_flow(flow))\n\t\t\tcontinue;\n\n\t\tattr = mlx5e_tc_get_encap_attr(flow);\n\t\tesw_attr = attr->esw_attr;\n\n\t\tif (flow_flag_test(flow, SLOW)) {\n\t\t\tmlx5e_tc_unoffload_from_slow_path(esw, flow);\n\t\t} else {\n\t\t\tmlx5e_tc_unoffload_fdb_rules(esw, flow, flow->attr);\n\t\t\tmlx5e_tc_unoffload_flow_post_acts(flow);\n\t\t}\n\n\t\tmlx5e_tc_detach_mod_hdr(priv, flow, attr);\n\t\tattr->modify_hdr = NULL;\n\n\t\tesw_attr->dests[flow->tmp_entry_index].flags &=\n\t\t\t~MLX5_ESW_DEST_ENCAP_VALID;\n\t\tesw_attr->dests[flow->tmp_entry_index].pkt_reformat = NULL;\n\t}\n\n\te->flags |= MLX5_ENCAP_ENTRY_NO_ROUTE;\n\tif (e->flags & MLX5_ENCAP_ENTRY_VALID) {\n\t\te->flags &= ~MLX5_ENCAP_ENTRY_VALID;\n\t\tmlx5_packet_reformat_dealloc(priv->mdev, e->pkt_reformat);\n\t\te->pkt_reformat = NULL;\n\t}\n}\n\nstatic void mlx5e_reoffload_encap(struct mlx5e_priv *priv,\n\t\t\t\t  struct net_device *tunnel_dev,\n\t\t\t\t  struct mlx5e_encap_entry *e,\n\t\t\t\t  struct list_head *encap_flows)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_tc_flow *flow;\n\tint err;\n\n\terr = ip_tunnel_info_af(e->tun_info) == AF_INET ?\n\t\tmlx5e_tc_tun_update_header_ipv4(priv, tunnel_dev, e) :\n\t\tmlx5e_tc_tun_update_header_ipv6(priv, tunnel_dev, e);\n\tif (err)\n\t\tmlx5_core_warn(priv->mdev, \"Failed to update encap header, %d\", err);\n\te->flags &= ~MLX5_ENCAP_ENTRY_NO_ROUTE;\n\n\tlist_for_each_entry(flow, encap_flows, tmp_list) {\n\t\tstruct mlx5e_tc_flow_parse_attr *parse_attr;\n\t\tstruct mlx5_esw_flow_attr *esw_attr;\n\t\tstruct mlx5_flow_handle *rule;\n\t\tstruct mlx5_flow_attr *attr;\n\t\tstruct mlx5_flow_spec *spec;\n\n\t\tif (flow_flag_test(flow, FAILED))\n\t\t\tcontinue;\n\n\t\tspec = &flow->attr->parse_attr->spec;\n\n\t\tattr = mlx5e_tc_get_encap_attr(flow);\n\t\tesw_attr = attr->esw_attr;\n\t\tparse_attr = attr->parse_attr;\n\n\t\terr = mlx5e_update_vf_tunnel(esw, esw_attr, &parse_attr->mod_hdr_acts,\n\t\t\t\t\t     e->out_dev, e->route_dev_ifindex,\n\t\t\t\t\t     flow->tmp_entry_index);\n\t\tif (err) {\n\t\t\tmlx5_core_warn(priv->mdev, \"Failed to update VF tunnel err=%d\", err);\n\t\t\tcontinue;\n\t\t}\n\n\t\terr = mlx5e_tc_attach_mod_hdr(priv, flow, attr);\n\t\tif (err) {\n\t\t\tmlx5_core_warn(priv->mdev, \"Failed to update flow mod_hdr err=%d\",\n\t\t\t\t       err);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (e->flags & MLX5_ENCAP_ENTRY_VALID) {\n\t\t\tesw_attr->dests[flow->tmp_entry_index].pkt_reformat = e->pkt_reformat;\n\t\t\tesw_attr->dests[flow->tmp_entry_index].flags |= MLX5_ESW_DEST_ENCAP_VALID;\n\t\t\tif (!mlx5e_tc_flow_all_encaps_valid(esw_attr))\n\t\t\t\tgoto offload_to_slow_path;\n\n\t\t\terr = mlx5e_tc_offload_flow_post_acts(flow);\n\t\t\tif (err) {\n\t\t\t\tmlx5_core_warn(priv->mdev, \"Failed to update flow post acts, %d\\n\",\n\t\t\t\t\t       err);\n\t\t\t\tgoto offload_to_slow_path;\n\t\t\t}\n\n\t\t\t \n\t\t\trule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, flow->attr);\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\tmlx5e_tc_unoffload_flow_post_acts(flow);\n\t\t\t\terr = PTR_ERR(rule);\n\t\t\t\tmlx5_core_warn(priv->mdev, \"Failed to update cached encapsulation flow, %d\\n\",\n\t\t\t\t\t       err);\n\t\t\t} else {\n\t\t\t\tflow->rule[0] = rule;\n\t\t\t}\n\t\t} else {\noffload_to_slow_path:\n\t\t\trule = mlx5e_tc_offload_to_slow_path(esw, flow, spec);\n\t\t\t \n\t\t\tesw_attr->dests[flow->tmp_entry_index].flags &=\n\t\t\t\t~MLX5_ESW_DEST_ENCAP_VALID;\n\n\t\t\tif (IS_ERR(rule)) {\n\t\t\t\terr = PTR_ERR(rule);\n\t\t\t\tmlx5_core_warn(priv->mdev, \"Failed to update slow path (encap) flow, %d\\n\",\n\t\t\t\t\t       err);\n\t\t\t} else {\n\t\t\t\tflow->rule[0] = rule;\n\t\t\t}\n\t\t}\n\t\tflow_flag_set(flow, OFFLOADED);\n\t}\n}\n\nstatic int mlx5e_update_route_encaps(struct mlx5e_priv *priv,\n\t\t\t\t     struct mlx5e_route_entry *r,\n\t\t\t\t     struct list_head *flow_list,\n\t\t\t\t     bool replace)\n{\n\tstruct net_device *tunnel_dev;\n\tstruct mlx5e_encap_entry *e;\n\n\ttunnel_dev = __dev_get_by_index(dev_net(priv->netdev), r->tunnel_dev_index);\n\tif (!tunnel_dev)\n\t\treturn -ENODEV;\n\n\tlist_for_each_entry(e, &r->encap_entries, route_list) {\n\t\tLIST_HEAD(encap_flows);\n\n\t\tmlx5e_take_all_encap_flows(e, &encap_flows);\n\t\tif (list_empty(&encap_flows))\n\t\t\tcontinue;\n\n\t\tif (mlx5e_route_entry_valid(r))\n\t\t\tmlx5e_invalidate_encap(priv, e, &encap_flows);\n\n\t\tif (!replace) {\n\t\t\tlist_splice(&encap_flows, flow_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmlx5e_reoffload_encap(priv, tunnel_dev, e, &encap_flows);\n\t\tlist_splice(&encap_flows, flow_list);\n\t}\n\n\treturn 0;\n}\n\nstatic void mlx5e_unoffload_flow_list(struct mlx5e_priv *priv,\n\t\t\t\t      struct list_head *flow_list)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_tc_flow *flow;\n\n\tlist_for_each_entry(flow, flow_list, tmp_list)\n\t\tif (mlx5e_is_offloaded_flow(flow))\n\t\t\tmlx5e_tc_unoffload_fdb_rules(esw, flow, flow->attr);\n}\n\nstatic void mlx5e_reoffload_decap(struct mlx5e_priv *priv,\n\t\t\t\t  struct list_head *decap_flows)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tstruct mlx5e_tc_flow *flow;\n\n\tlist_for_each_entry(flow, decap_flows, tmp_list) {\n\t\tstruct mlx5e_tc_flow_parse_attr *parse_attr;\n\t\tstruct mlx5_flow_attr *attr = flow->attr;\n\t\tstruct mlx5_flow_handle *rule;\n\t\tstruct mlx5_flow_spec *spec;\n\t\tint err;\n\n\t\tif (flow_flag_test(flow, FAILED))\n\t\t\tcontinue;\n\n\t\tparse_attr = attr->parse_attr;\n\t\tspec = &parse_attr->spec;\n\t\terr = mlx5e_tc_tun_route_lookup(priv, spec, attr, parse_attr->filter_dev);\n\t\tif (err) {\n\t\t\tmlx5_core_warn(priv->mdev, \"Failed to lookup route for flow, %d\\n\",\n\t\t\t\t       err);\n\t\t\tcontinue;\n\t\t}\n\n\t\trule = mlx5e_tc_offload_fdb_rules(esw, flow, spec, attr);\n\t\tif (IS_ERR(rule)) {\n\t\t\terr = PTR_ERR(rule);\n\t\t\tmlx5_core_warn(priv->mdev, \"Failed to update cached decap flow, %d\\n\",\n\t\t\t\t       err);\n\t\t} else {\n\t\t\tflow->rule[0] = rule;\n\t\t\tflow_flag_set(flow, OFFLOADED);\n\t\t}\n\t}\n}\n\nstatic int mlx5e_update_route_decap_flows(struct mlx5e_priv *priv,\n\t\t\t\t\t  struct mlx5e_route_entry *r,\n\t\t\t\t\t  struct list_head *flow_list,\n\t\t\t\t\t  bool replace)\n{\n\tstruct net_device *tunnel_dev;\n\tLIST_HEAD(decap_flows);\n\n\ttunnel_dev = __dev_get_by_index(dev_net(priv->netdev), r->tunnel_dev_index);\n\tif (!tunnel_dev)\n\t\treturn -ENODEV;\n\n\tmlx5e_take_all_route_decap_flows(r, &decap_flows);\n\tif (mlx5e_route_entry_valid(r))\n\t\tmlx5e_unoffload_flow_list(priv, &decap_flows);\n\tif (replace)\n\t\tmlx5e_reoffload_decap(priv, &decap_flows);\n\n\tlist_splice(&decap_flows, flow_list);\n\n\treturn 0;\n}\n\nstatic void mlx5e_tc_fib_event_work(struct work_struct *work)\n{\n\tstruct mlx5e_tc_fib_event_data *event_data =\n\t\tcontainer_of(work, struct mlx5e_tc_fib_event_data, work);\n\tstruct net_device *ul_dev = event_data->ul_dev;\n\tstruct mlx5e_priv *priv = netdev_priv(ul_dev);\n\tstruct mlx5e_route_entry *r = event_data->r;\n\tstruct mlx5_eswitch *esw;\n\tLIST_HEAD(flow_list);\n\tbool replace;\n\tint err;\n\n\t \n\trtnl_lock();\n\tesw = priv->mdev->priv.eswitch;\n\tmutex_lock(&esw->offloads.encap_tbl_lock);\n\treplace = event_data->event == FIB_EVENT_ENTRY_REPLACE;\n\n\tif (!mlx5e_route_entry_valid(r) && !replace)\n\t\tgoto out;\n\n\terr = mlx5e_update_route_encaps(priv, r, &flow_list, replace);\n\tif (err)\n\t\tmlx5_core_warn(priv->mdev, \"Failed to update route encaps, %d\\n\",\n\t\t\t       err);\n\n\terr = mlx5e_update_route_decap_flows(priv, r, &flow_list, replace);\n\tif (err)\n\t\tmlx5_core_warn(priv->mdev, \"Failed to update route decap flows, %d\\n\",\n\t\t\t       err);\n\n\tif (replace)\n\t\tr->flags |= MLX5E_ROUTE_ENTRY_VALID;\nout:\n\tmutex_unlock(&esw->offloads.encap_tbl_lock);\n\trtnl_unlock();\n\n\tmlx5e_put_flow_list(priv, &flow_list);\n\tmlx5e_route_put(priv, event_data->r);\n\tdev_put(event_data->ul_dev);\n\tkfree(event_data);\n}\n\nstatic struct mlx5e_tc_fib_event_data *\nmlx5e_init_fib_work_ipv4(struct mlx5e_priv *priv,\n\t\t\t struct net_device *ul_dev,\n\t\t\t struct mlx5e_tc_tun_encap *encap,\n\t\t\t unsigned long event,\n\t\t\t struct fib_notifier_info *info)\n{\n\tstruct fib_entry_notifier_info *fen_info;\n\tstruct mlx5e_tc_fib_event_data *fib_work;\n\tstruct mlx5e_route_entry *r;\n\tstruct mlx5e_route_key key;\n\tstruct net_device *fib_dev;\n\n\tfen_info = container_of(info, struct fib_entry_notifier_info, info);\n\tif (fen_info->fi->nh)\n\t\treturn NULL;\n\tfib_dev = fib_info_nh(fen_info->fi, 0)->fib_nh_dev;\n\tif (!fib_dev || fib_dev->netdev_ops != &mlx5e_netdev_ops ||\n\t    fen_info->dst_len != 32)\n\t\treturn NULL;\n\n\tfib_work = mlx5e_tc_init_fib_work(event, ul_dev, GFP_ATOMIC);\n\tif (!fib_work)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tkey.endpoint_ip.v4 = htonl(fen_info->dst);\n\tkey.ip_version = 4;\n\n\t \n\tr = mlx5e_route_lookup_for_update(encap, &key);\n\tif (!r)\n\t\tgoto out;\n\tfib_work->r = r;\n\tdev_hold(ul_dev);\n\n\treturn fib_work;\n\nout:\n\tkfree(fib_work);\n\treturn NULL;\n}\n\nstatic struct mlx5e_tc_fib_event_data *\nmlx5e_init_fib_work_ipv6(struct mlx5e_priv *priv,\n\t\t\t struct net_device *ul_dev,\n\t\t\t struct mlx5e_tc_tun_encap *encap,\n\t\t\t unsigned long event,\n\t\t\t struct fib_notifier_info *info)\n{\n\tstruct fib6_entry_notifier_info *fen_info;\n\tstruct mlx5e_tc_fib_event_data *fib_work;\n\tstruct mlx5e_route_entry *r;\n\tstruct mlx5e_route_key key;\n\tstruct net_device *fib_dev;\n\n\tfen_info = container_of(info, struct fib6_entry_notifier_info, info);\n\tfib_dev = fib6_info_nh_dev(fen_info->rt);\n\tif (fib_dev->netdev_ops != &mlx5e_netdev_ops ||\n\t    fen_info->rt->fib6_dst.plen != 128)\n\t\treturn NULL;\n\n\tfib_work = mlx5e_tc_init_fib_work(event, ul_dev, GFP_ATOMIC);\n\tif (!fib_work)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmemcpy(&key.endpoint_ip.v6, &fen_info->rt->fib6_dst.addr,\n\t       sizeof(fen_info->rt->fib6_dst.addr));\n\tkey.ip_version = 6;\n\n\t \n\tr = mlx5e_route_lookup_for_update(encap, &key);\n\tif (!r)\n\t\tgoto out;\n\tfib_work->r = r;\n\tdev_hold(ul_dev);\n\n\treturn fib_work;\n\nout:\n\tkfree(fib_work);\n\treturn NULL;\n}\n\nstatic int mlx5e_tc_tun_fib_event(struct notifier_block *nb, unsigned long event, void *ptr)\n{\n\tstruct mlx5e_tc_fib_event_data *fib_work;\n\tstruct fib_notifier_info *info = ptr;\n\tstruct mlx5e_tc_tun_encap *encap;\n\tstruct net_device *ul_dev;\n\tstruct mlx5e_priv *priv;\n\n\tencap = container_of(nb, struct mlx5e_tc_tun_encap, fib_nb);\n\tpriv = encap->priv;\n\tul_dev = priv->netdev;\n\tpriv = netdev_priv(ul_dev);\n\n\tswitch (event) {\n\tcase FIB_EVENT_ENTRY_REPLACE:\n\tcase FIB_EVENT_ENTRY_DEL:\n\t\tif (info->family == AF_INET)\n\t\t\tfib_work = mlx5e_init_fib_work_ipv4(priv, ul_dev, encap, event, info);\n\t\telse if (info->family == AF_INET6)\n\t\t\tfib_work = mlx5e_init_fib_work_ipv6(priv, ul_dev, encap, event, info);\n\t\telse\n\t\t\treturn NOTIFY_DONE;\n\n\t\tif (!IS_ERR_OR_NULL(fib_work)) {\n\t\t\tqueue_work(priv->wq, &fib_work->work);\n\t\t} else if (IS_ERR(fib_work)) {\n\t\t\tNL_SET_ERR_MSG_MOD(info->extack, \"Failed to init fib work\");\n\t\t\tmlx5_core_warn(priv->mdev, \"Failed to init fib work, %ld\\n\",\n\t\t\t\t       PTR_ERR(fib_work));\n\t\t}\n\n\t\tbreak;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstruct mlx5e_tc_tun_encap *mlx5e_tc_tun_init(struct mlx5e_priv *priv)\n{\n\tstruct mlx5e_tc_tun_encap *encap;\n\tint err;\n\n\tencap = kvzalloc(sizeof(*encap), GFP_KERNEL);\n\tif (!encap)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tencap->priv = priv;\n\tencap->fib_nb.notifier_call = mlx5e_tc_tun_fib_event;\n\tspin_lock_init(&encap->route_lock);\n\thash_init(encap->route_tbl);\n\terr = register_fib_notifier(dev_net(priv->netdev), &encap->fib_nb,\n\t\t\t\t    NULL, NULL);\n\tif (err) {\n\t\tkvfree(encap);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn encap;\n}\n\nvoid mlx5e_tc_tun_cleanup(struct mlx5e_tc_tun_encap *encap)\n{\n\tif (!encap)\n\t\treturn;\n\n\tunregister_fib_notifier(dev_net(encap->priv->netdev), &encap->fib_nb);\n\tflush_workqueue(encap->priv->wq);  \n\tkvfree(encap);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}