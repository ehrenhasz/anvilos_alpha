{
  "module_name": "rx.c",
  "hash_id": "37aeb0ebc4c208dd20562b6d496162580a7102339697769cc1287335843945eb",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/rx.c",
  "human_readable_source": "\n \n\n#include \"rx.h\"\n#include \"en/xdp.h\"\n#include <net/xdp_sock_drv.h>\n#include <linux/filter.h>\n\n \n\nstatic struct mlx5e_xdp_buff *xsk_buff_to_mxbuf(struct xdp_buff *xdp)\n{\n\t \n\treturn (struct mlx5e_xdp_buff *)xdp;\n}\n\nint mlx5e_xsk_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)\n{\n\tstruct mlx5e_mpw_info *wi = mlx5e_get_mpw_info(rq, ix);\n\tstruct mlx5e_icosq *icosq = rq->icosq;\n\tstruct mlx5_wq_cyc *wq = &icosq->wq;\n\tstruct mlx5e_umr_wqe *umr_wqe;\n\tstruct xdp_buff **xsk_buffs;\n\tint batch, i;\n\tu32 offset;  \n\tu16 pi;\n\n\tif (unlikely(!xsk_buff_can_alloc(rq->xsk_pool, rq->mpwqe.pages_per_wqe)))\n\t\tgoto err;\n\n\tXSK_CHECK_PRIV_TYPE(struct mlx5e_xdp_buff);\n\txsk_buffs = (struct xdp_buff **)wi->alloc_units.xsk_buffs;\n\tbatch = xsk_buff_alloc_batch(rq->xsk_pool, xsk_buffs,\n\t\t\t\t     rq->mpwqe.pages_per_wqe);\n\n\t \n\tfor (; batch < rq->mpwqe.pages_per_wqe; batch++) {\n\t\txsk_buffs[batch] = xsk_buff_alloc(rq->xsk_pool);\n\t\tif (unlikely(!xsk_buffs[batch]))\n\t\t\tgoto err_reuse_batch;\n\t}\n\n\tpi = mlx5e_icosq_get_next_pi(icosq, rq->mpwqe.umr_wqebbs);\n\tumr_wqe = mlx5_wq_cyc_get_wqe(wq, pi);\n\tmemcpy(umr_wqe, &rq->mpwqe.umr_wqe, sizeof(struct mlx5e_umr_wqe));\n\n\tif (likely(rq->mpwqe.umr_mode == MLX5E_MPWRQ_UMR_MODE_ALIGNED)) {\n\t\tfor (i = 0; i < batch; i++) {\n\t\t\tstruct mlx5e_xdp_buff *mxbuf = xsk_buff_to_mxbuf(xsk_buffs[i]);\n\t\t\tdma_addr_t addr = xsk_buff_xdp_get_frame_dma(xsk_buffs[i]);\n\n\t\t\tumr_wqe->inline_mtts[i] = (struct mlx5_mtt) {\n\t\t\t\t.ptag = cpu_to_be64(addr | MLX5_EN_WR),\n\t\t\t};\n\t\t\tmxbuf->rq = rq;\n\t\t}\n\t} else if (unlikely(rq->mpwqe.umr_mode == MLX5E_MPWRQ_UMR_MODE_UNALIGNED)) {\n\t\tfor (i = 0; i < batch; i++) {\n\t\t\tstruct mlx5e_xdp_buff *mxbuf = xsk_buff_to_mxbuf(xsk_buffs[i]);\n\t\t\tdma_addr_t addr = xsk_buff_xdp_get_frame_dma(xsk_buffs[i]);\n\n\t\t\tumr_wqe->inline_ksms[i] = (struct mlx5_ksm) {\n\t\t\t\t.key = rq->mkey_be,\n\t\t\t\t.va = cpu_to_be64(addr),\n\t\t\t};\n\t\t\tmxbuf->rq = rq;\n\t\t}\n\t} else if (likely(rq->mpwqe.umr_mode == MLX5E_MPWRQ_UMR_MODE_TRIPLE)) {\n\t\tu32 mapping_size = 1 << (rq->mpwqe.page_shift - 2);\n\n\t\tfor (i = 0; i < batch; i++) {\n\t\t\tstruct mlx5e_xdp_buff *mxbuf = xsk_buff_to_mxbuf(xsk_buffs[i]);\n\t\t\tdma_addr_t addr = xsk_buff_xdp_get_frame_dma(xsk_buffs[i]);\n\n\t\t\tumr_wqe->inline_ksms[i << 2] = (struct mlx5_ksm) {\n\t\t\t\t.key = rq->mkey_be,\n\t\t\t\t.va = cpu_to_be64(addr),\n\t\t\t};\n\t\t\tumr_wqe->inline_ksms[(i << 2) + 1] = (struct mlx5_ksm) {\n\t\t\t\t.key = rq->mkey_be,\n\t\t\t\t.va = cpu_to_be64(addr + mapping_size),\n\t\t\t};\n\t\t\tumr_wqe->inline_ksms[(i << 2) + 2] = (struct mlx5_ksm) {\n\t\t\t\t.key = rq->mkey_be,\n\t\t\t\t.va = cpu_to_be64(addr + mapping_size * 2),\n\t\t\t};\n\t\t\tumr_wqe->inline_ksms[(i << 2) + 3] = (struct mlx5_ksm) {\n\t\t\t\t.key = rq->mkey_be,\n\t\t\t\t.va = cpu_to_be64(rq->wqe_overflow.addr),\n\t\t\t};\n\t\t\tmxbuf->rq = rq;\n\t\t}\n\t} else {\n\t\t__be32 pad_size = cpu_to_be32((1 << rq->mpwqe.page_shift) -\n\t\t\t\t\t      rq->xsk_pool->chunk_size);\n\t\t__be32 frame_size = cpu_to_be32(rq->xsk_pool->chunk_size);\n\n\t\tfor (i = 0; i < batch; i++) {\n\t\t\tstruct mlx5e_xdp_buff *mxbuf = xsk_buff_to_mxbuf(xsk_buffs[i]);\n\t\t\tdma_addr_t addr = xsk_buff_xdp_get_frame_dma(xsk_buffs[i]);\n\n\t\t\tumr_wqe->inline_klms[i << 1] = (struct mlx5_klm) {\n\t\t\t\t.key = rq->mkey_be,\n\t\t\t\t.va = cpu_to_be64(addr),\n\t\t\t\t.bcount = frame_size,\n\t\t\t};\n\t\t\tumr_wqe->inline_klms[(i << 1) + 1] = (struct mlx5_klm) {\n\t\t\t\t.key = rq->mkey_be,\n\t\t\t\t.va = cpu_to_be64(rq->wqe_overflow.addr),\n\t\t\t\t.bcount = pad_size,\n\t\t\t};\n\t\t\tmxbuf->rq = rq;\n\t\t}\n\t}\n\n\tbitmap_zero(wi->skip_release_bitmap, rq->mpwqe.pages_per_wqe);\n\twi->consumed_strides = 0;\n\n\tumr_wqe->ctrl.opmod_idx_opcode =\n\t\tcpu_to_be32((icosq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) | MLX5_OPCODE_UMR);\n\n\t \n\toffset = ix * rq->mpwqe.mtts_per_wqe;\n\tif (likely(rq->mpwqe.umr_mode == MLX5E_MPWRQ_UMR_MODE_ALIGNED))\n\t\toffset = offset * sizeof(struct mlx5_mtt) / MLX5_OCTWORD;\n\telse if (unlikely(rq->mpwqe.umr_mode == MLX5E_MPWRQ_UMR_MODE_OVERSIZED))\n\t\toffset = offset * sizeof(struct mlx5_klm) * 2 / MLX5_OCTWORD;\n\telse if (unlikely(rq->mpwqe.umr_mode == MLX5E_MPWRQ_UMR_MODE_TRIPLE))\n\t\toffset = offset * sizeof(struct mlx5_ksm) * 4 / MLX5_OCTWORD;\n\tumr_wqe->uctrl.xlt_offset = cpu_to_be16(offset);\n\n\ticosq->db.wqe_info[pi] = (struct mlx5e_icosq_wqe_info) {\n\t\t.wqe_type = MLX5E_ICOSQ_WQE_UMR_RX,\n\t\t.num_wqebbs = rq->mpwqe.umr_wqebbs,\n\t\t.umr.rq = rq,\n\t};\n\n\ticosq->pc += rq->mpwqe.umr_wqebbs;\n\n\ticosq->doorbell_cseg = &umr_wqe->ctrl;\n\n\treturn 0;\n\nerr_reuse_batch:\n\twhile (--batch >= 0)\n\t\txsk_buff_free(xsk_buffs[batch]);\n\nerr:\n\trq->stats->buff_alloc_err++;\n\treturn -ENOMEM;\n}\n\nint mlx5e_xsk_alloc_rx_wqes_batched(struct mlx5e_rq *rq, u16 ix, int wqe_bulk)\n{\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tstruct xdp_buff **buffs;\n\tu32 contig, alloc;\n\tint i;\n\n\t \n\tbuffs = rq->wqe.alloc_units->xsk_buffs;\n\tcontig = mlx5_wq_cyc_get_size(wq) - ix;\n\tif (wqe_bulk <= contig) {\n\t\talloc = xsk_buff_alloc_batch(rq->xsk_pool, buffs + ix, wqe_bulk);\n\t} else {\n\t\talloc = xsk_buff_alloc_batch(rq->xsk_pool, buffs + ix, contig);\n\t\tif (likely(alloc == contig))\n\t\t\talloc += xsk_buff_alloc_batch(rq->xsk_pool, buffs, wqe_bulk - contig);\n\t}\n\n\tfor (i = 0; i < alloc; i++) {\n\t\tint j = mlx5_wq_cyc_ctr2ix(wq, ix + i);\n\t\tstruct mlx5e_wqe_frag_info *frag;\n\t\tstruct mlx5e_rx_wqe_cyc *wqe;\n\t\tdma_addr_t addr;\n\n\t\twqe = mlx5_wq_cyc_get_wqe(wq, j);\n\t\t \n\t\tfrag = &rq->wqe.frags[j];\n\n\t\taddr = xsk_buff_xdp_get_frame_dma(*frag->xskp);\n\t\twqe->data[0].addr = cpu_to_be64(addr + rq->buff.headroom);\n\t\tfrag->flags &= ~BIT(MLX5E_WQE_FRAG_SKIP_RELEASE);\n\t}\n\n\treturn alloc;\n}\n\nint mlx5e_xsk_alloc_rx_wqes(struct mlx5e_rq *rq, u16 ix, int wqe_bulk)\n{\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tint i;\n\n\tfor (i = 0; i < wqe_bulk; i++) {\n\t\tint j = mlx5_wq_cyc_ctr2ix(wq, ix + i);\n\t\tstruct mlx5e_wqe_frag_info *frag;\n\t\tstruct mlx5e_rx_wqe_cyc *wqe;\n\t\tdma_addr_t addr;\n\n\t\twqe = mlx5_wq_cyc_get_wqe(wq, j);\n\t\t \n\t\tfrag = &rq->wqe.frags[j];\n\n\t\t*frag->xskp = xsk_buff_alloc(rq->xsk_pool);\n\t\tif (unlikely(!*frag->xskp))\n\t\t\treturn i;\n\n\t\taddr = xsk_buff_xdp_get_frame_dma(*frag->xskp);\n\t\twqe->data[0].addr = cpu_to_be64(addr + rq->buff.headroom);\n\t\tfrag->flags &= ~BIT(MLX5E_WQE_FRAG_SKIP_RELEASE);\n\t}\n\n\treturn wqe_bulk;\n}\n\nstatic struct sk_buff *mlx5e_xsk_construct_skb(struct mlx5e_rq *rq, struct xdp_buff *xdp)\n{\n\tu32 totallen = xdp->data_end - xdp->data_meta;\n\tu32 metalen = xdp->data - xdp->data_meta;\n\tstruct sk_buff *skb;\n\n\tskb = napi_alloc_skb(rq->cq.napi, totallen);\n\tif (unlikely(!skb)) {\n\t\trq->stats->buff_alloc_err++;\n\t\treturn NULL;\n\t}\n\n\tskb_put_data(skb, xdp->data_meta, totallen);\n\n\tif (metalen) {\n\t\tskb_metadata_set(skb, metalen);\n\t\t__skb_pull(skb, metalen);\n\t}\n\n\treturn skb;\n}\n\nstruct sk_buff *mlx5e_xsk_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq,\n\t\t\t\t\t\t    struct mlx5e_mpw_info *wi,\n\t\t\t\t\t\t    struct mlx5_cqe64 *cqe,\n\t\t\t\t\t\t    u16 cqe_bcnt,\n\t\t\t\t\t\t    u32 head_offset,\n\t\t\t\t\t\t    u32 page_idx)\n{\n\tstruct mlx5e_xdp_buff *mxbuf = xsk_buff_to_mxbuf(wi->alloc_units.xsk_buffs[page_idx]);\n\tstruct bpf_prog *prog;\n\n\t \n\tif (unlikely(cqe_bcnt > rq->hw_mtu)) {\n\t\trq->stats->oversize_pkts_sw_drop++;\n\t\treturn NULL;\n\t}\n\n\t \n\tWARN_ON_ONCE(head_offset);\n\n\t \n\tmxbuf->cqe = cqe;\n\txsk_buff_set_size(&mxbuf->xdp, cqe_bcnt);\n\txsk_buff_dma_sync_for_cpu(&mxbuf->xdp, rq->xsk_pool);\n\tnet_prefetch(mxbuf->xdp.data);\n\n\t \n\n\tprog = rcu_dereference(rq->xdp_prog);\n\tif (likely(prog && mlx5e_xdp_handle(rq, prog, mxbuf))) {\n\t\tif (likely(__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags)))\n\t\t\t__set_bit(page_idx, wi->skip_release_bitmap);  \n\t\treturn NULL;  \n\t}\n\n\t \n\treturn mlx5e_xsk_construct_skb(rq, &mxbuf->xdp);\n}\n\nstruct sk_buff *mlx5e_xsk_skb_from_cqe_linear(struct mlx5e_rq *rq,\n\t\t\t\t\t      struct mlx5e_wqe_frag_info *wi,\n\t\t\t\t\t      struct mlx5_cqe64 *cqe,\n\t\t\t\t\t      u32 cqe_bcnt)\n{\n\tstruct mlx5e_xdp_buff *mxbuf = xsk_buff_to_mxbuf(*wi->xskp);\n\tstruct bpf_prog *prog;\n\n\t \n\tWARN_ON_ONCE(wi->offset);\n\n\t \n\tmxbuf->cqe = cqe;\n\txsk_buff_set_size(&mxbuf->xdp, cqe_bcnt);\n\txsk_buff_dma_sync_for_cpu(&mxbuf->xdp, rq->xsk_pool);\n\tnet_prefetch(mxbuf->xdp.data);\n\n\tprog = rcu_dereference(rq->xdp_prog);\n\tif (likely(prog && mlx5e_xdp_handle(rq, prog, mxbuf))) {\n\t\tif (likely(__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags)))\n\t\t\twi->flags |= BIT(MLX5E_WQE_FRAG_SKIP_RELEASE);\n\t\treturn NULL;  \n\t}\n\n\t \n\treturn mlx5e_xsk_construct_skb(rq, &mxbuf->xdp);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}