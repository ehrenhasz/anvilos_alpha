{
  "module_name": "eswitch.c",
  "hash_id": "78855955def137c6dd99267e6de162c60d42969d3fbb9c30e77d0ceae7fce492",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/eswitch.c",
  "human_readable_source": " \n\n#include <linux/etherdevice.h>\n#include <linux/debugfs.h>\n#include <linux/mlx5/driver.h>\n#include <linux/mlx5/mlx5_ifc.h>\n#include <linux/mlx5/vport.h>\n#include <linux/mlx5/fs.h>\n#include <linux/mlx5/mpfs.h>\n#include \"esw/acl/lgcy.h\"\n#include \"esw/legacy.h\"\n#include \"esw/qos.h\"\n#include \"mlx5_core.h\"\n#include \"lib/eq.h\"\n#include \"lag/lag.h\"\n#include \"eswitch.h\"\n#include \"fs_core.h\"\n#include \"devlink.h\"\n#include \"ecpf.h\"\n#include \"en/mod_hdr.h\"\n#include \"en_accel/ipsec.h\"\n\nenum {\n\tMLX5_ACTION_NONE = 0,\n\tMLX5_ACTION_ADD  = 1,\n\tMLX5_ACTION_DEL  = 2,\n};\n\n \nstruct vport_addr {\n\tstruct l2addr_node     node;\n\tu8                     action;\n\tu16                    vport;\n\tstruct mlx5_flow_handle *flow_rule;\n\tbool mpfs;  \n\t \n\tbool mc_promisc;\n};\n\nstatic int mlx5_eswitch_check(const struct mlx5_core_dev *dev)\n{\n\tif (MLX5_CAP_GEN(dev, port_type) != MLX5_CAP_PORT_TYPE_ETH)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!MLX5_ESWITCH_MANAGER(dev))\n\t\treturn -EOPNOTSUPP;\n\n\treturn 0;\n}\n\nstatic struct mlx5_eswitch *__mlx5_devlink_eswitch_get(struct devlink *devlink, bool check)\n{\n\tstruct mlx5_core_dev *dev = devlink_priv(devlink);\n\tint err;\n\n\tif (check) {\n\t\terr = mlx5_eswitch_check(dev);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\treturn dev->priv.eswitch;\n}\n\nstruct mlx5_eswitch *__must_check\nmlx5_devlink_eswitch_get(struct devlink *devlink)\n{\n\treturn __mlx5_devlink_eswitch_get(devlink, true);\n}\n\nstruct mlx5_eswitch *mlx5_devlink_eswitch_nocheck_get(struct devlink *devlink)\n{\n\treturn __mlx5_devlink_eswitch_get(devlink, false);\n}\n\nstruct mlx5_vport *__must_check\nmlx5_eswitch_get_vport(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\tstruct mlx5_vport *vport;\n\n\tif (!esw)\n\t\treturn ERR_PTR(-EPERM);\n\n\tvport = xa_load(&esw->vports, vport_num);\n\tif (!vport) {\n\t\tesw_debug(esw->dev, \"vport out of range: num(0x%x)\\n\", vport_num);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\treturn vport;\n}\n\nstatic int arm_vport_context_events_cmd(struct mlx5_core_dev *dev, u16 vport,\n\t\t\t\t\tu32 events_mask)\n{\n\tu32 in[MLX5_ST_SZ_DW(modify_nic_vport_context_in)] = {};\n\tvoid *nic_vport_ctx;\n\n\tMLX5_SET(modify_nic_vport_context_in, in,\n\t\t opcode, MLX5_CMD_OP_MODIFY_NIC_VPORT_CONTEXT);\n\tMLX5_SET(modify_nic_vport_context_in, in, field_select.change_event, 1);\n\tMLX5_SET(modify_nic_vport_context_in, in, vport_number, vport);\n\tif (vport || mlx5_core_is_ecpf(dev))\n\t\tMLX5_SET(modify_nic_vport_context_in, in, other_vport, 1);\n\tnic_vport_ctx = MLX5_ADDR_OF(modify_nic_vport_context_in,\n\t\t\t\t     in, nic_vport_context);\n\n\tMLX5_SET(nic_vport_context, nic_vport_ctx, arm_change_event, 1);\n\n\tif (events_mask & MLX5_VPORT_UC_ADDR_CHANGE)\n\t\tMLX5_SET(nic_vport_context, nic_vport_ctx,\n\t\t\t event_on_uc_address_change, 1);\n\tif (events_mask & MLX5_VPORT_MC_ADDR_CHANGE)\n\t\tMLX5_SET(nic_vport_context, nic_vport_ctx,\n\t\t\t event_on_mc_address_change, 1);\n\tif (events_mask & MLX5_VPORT_PROMISC_CHANGE)\n\t\tMLX5_SET(nic_vport_context, nic_vport_ctx,\n\t\t\t event_on_promisc_change, 1);\n\n\treturn mlx5_cmd_exec_in(dev, modify_nic_vport_context, in);\n}\n\n \nint mlx5_eswitch_modify_esw_vport_context(struct mlx5_core_dev *dev, u16 vport,\n\t\t\t\t\t  bool other_vport, void *in)\n{\n\tMLX5_SET(modify_esw_vport_context_in, in, opcode,\n\t\t MLX5_CMD_OP_MODIFY_ESW_VPORT_CONTEXT);\n\tMLX5_SET(modify_esw_vport_context_in, in, vport_number, vport);\n\tMLX5_SET(modify_esw_vport_context_in, in, other_vport, other_vport);\n\treturn mlx5_cmd_exec_in(dev, modify_esw_vport_context, in);\n}\n\nstatic int modify_esw_vport_cvlan(struct mlx5_core_dev *dev, u16 vport,\n\t\t\t\t  u16 vlan, u8 qos, u8 set_flags)\n{\n\tu32 in[MLX5_ST_SZ_DW(modify_esw_vport_context_in)] = {};\n\n\tif (!MLX5_CAP_ESW(dev, vport_cvlan_strip) ||\n\t    !MLX5_CAP_ESW(dev, vport_cvlan_insert_if_not_exist))\n\t\treturn -EOPNOTSUPP;\n\n\tesw_debug(dev, \"Set Vport[%d] VLAN %d qos %d set=%x\\n\",\n\t\t  vport, vlan, qos, set_flags);\n\n\tif (set_flags & SET_VLAN_STRIP)\n\t\tMLX5_SET(modify_esw_vport_context_in, in,\n\t\t\t esw_vport_context.vport_cvlan_strip, 1);\n\n\tif (set_flags & SET_VLAN_INSERT) {\n\t\tif (MLX5_CAP_ESW(dev, vport_cvlan_insert_always)) {\n\t\t\t \n\t\t\tMLX5_SET(modify_esw_vport_context_in, in,\n\t\t\t\t esw_vport_context.vport_cvlan_insert,\n\t\t\t\t MLX5_VPORT_CVLAN_INSERT_ALWAYS);\n\t\t} else {\n\t\t\t \n\t\t\tMLX5_SET(modify_esw_vport_context_in, in,\n\t\t\t\t esw_vport_context.vport_cvlan_insert,\n\t\t\t\t MLX5_VPORT_CVLAN_INSERT_WHEN_NO_CVLAN);\n\t\t}\n\t\tMLX5_SET(modify_esw_vport_context_in, in,\n\t\t\t esw_vport_context.cvlan_pcp, qos);\n\t\tMLX5_SET(modify_esw_vport_context_in, in,\n\t\t\t esw_vport_context.cvlan_id, vlan);\n\t}\n\n\tMLX5_SET(modify_esw_vport_context_in, in,\n\t\t field_select.vport_cvlan_strip, 1);\n\tMLX5_SET(modify_esw_vport_context_in, in,\n\t\t field_select.vport_cvlan_insert, 1);\n\n\treturn mlx5_eswitch_modify_esw_vport_context(dev, vport, true, in);\n}\n\n \nstatic struct mlx5_flow_handle *\n__esw_fdb_set_vport_rule(struct mlx5_eswitch *esw, u16 vport, bool rx_rule,\n\t\t\t u8 mac_c[ETH_ALEN], u8 mac_v[ETH_ALEN])\n{\n\tint match_header = (is_zero_ether_addr(mac_c) ? 0 :\n\t\t\t    MLX5_MATCH_OUTER_HEADERS);\n\tstruct mlx5_flow_handle *flow_rule = NULL;\n\tstruct mlx5_flow_act flow_act = {0};\n\tstruct mlx5_flow_destination dest = {};\n\tstruct mlx5_flow_spec *spec;\n\tvoid *mv_misc = NULL;\n\tvoid *mc_misc = NULL;\n\tu8 *dmac_v = NULL;\n\tu8 *dmac_c = NULL;\n\n\tif (rx_rule)\n\t\tmatch_header |= MLX5_MATCH_MISC_PARAMETERS;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn NULL;\n\n\tdmac_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t      outer_headers.dmac_47_16);\n\tdmac_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,\n\t\t\t      outer_headers.dmac_47_16);\n\n\tif (match_header & MLX5_MATCH_OUTER_HEADERS) {\n\t\tether_addr_copy(dmac_v, mac_v);\n\t\tether_addr_copy(dmac_c, mac_c);\n\t}\n\n\tif (match_header & MLX5_MATCH_MISC_PARAMETERS) {\n\t\tmv_misc  = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t\tmisc_parameters);\n\t\tmc_misc  = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,\n\t\t\t\t\tmisc_parameters);\n\t\tMLX5_SET(fte_match_set_misc, mv_misc, source_port, MLX5_VPORT_UPLINK);\n\t\tMLX5_SET_TO_ONES(fte_match_set_misc, mc_misc, source_port);\n\t}\n\n\tdest.type = MLX5_FLOW_DESTINATION_TYPE_VPORT;\n\tdest.vport.num = vport;\n\n\tesw_debug(esw->dev,\n\t\t  \"\\tFDB add rule dmac_v(%pM) dmac_c(%pM) -> vport(%d)\\n\",\n\t\t  dmac_v, dmac_c, vport);\n\tspec->match_criteria_enable = match_header;\n\tflow_act.action =  MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;\n\tflow_rule =\n\t\tmlx5_add_flow_rules(esw->fdb_table.legacy.fdb, spec,\n\t\t\t\t    &flow_act, &dest, 1);\n\tif (IS_ERR(flow_rule)) {\n\t\tesw_warn(esw->dev,\n\t\t\t \"FDB: Failed to add flow rule: dmac_v(%pM) dmac_c(%pM) -> vport(%d), err(%ld)\\n\",\n\t\t\t dmac_v, dmac_c, vport, PTR_ERR(flow_rule));\n\t\tflow_rule = NULL;\n\t}\n\n\tkvfree(spec);\n\treturn flow_rule;\n}\n\nstatic struct mlx5_flow_handle *\nesw_fdb_set_vport_rule(struct mlx5_eswitch *esw, u8 mac[ETH_ALEN], u16 vport)\n{\n\tu8 mac_c[ETH_ALEN];\n\n\teth_broadcast_addr(mac_c);\n\treturn __esw_fdb_set_vport_rule(esw, vport, false, mac_c, mac);\n}\n\nstatic struct mlx5_flow_handle *\nesw_fdb_set_vport_allmulti_rule(struct mlx5_eswitch *esw, u16 vport)\n{\n\tu8 mac_c[ETH_ALEN];\n\tu8 mac_v[ETH_ALEN];\n\n\teth_zero_addr(mac_c);\n\teth_zero_addr(mac_v);\n\tmac_c[0] = 0x01;\n\tmac_v[0] = 0x01;\n\treturn __esw_fdb_set_vport_rule(esw, vport, false, mac_c, mac_v);\n}\n\nstatic struct mlx5_flow_handle *\nesw_fdb_set_vport_promisc_rule(struct mlx5_eswitch *esw, u16 vport)\n{\n\tu8 mac_c[ETH_ALEN];\n\tu8 mac_v[ETH_ALEN];\n\n\teth_zero_addr(mac_c);\n\teth_zero_addr(mac_v);\n\treturn __esw_fdb_set_vport_rule(esw, vport, true, mac_c, mac_v);\n}\n\n \ntypedef int (*vport_addr_action)(struct mlx5_eswitch *esw,\n\t\t\t\t struct vport_addr *vaddr);\n\nstatic int esw_add_uc_addr(struct mlx5_eswitch *esw, struct vport_addr *vaddr)\n{\n\tu8 *mac = vaddr->node.addr;\n\tu16 vport = vaddr->vport;\n\tint err;\n\n\t \n\tif (mlx5_esw_is_manager_vport(esw, vport))\n\t\tgoto fdb_add;\n\n\terr = mlx5_mpfs_add_mac(esw->dev, mac);\n\tif (err) {\n\t\tesw_warn(esw->dev,\n\t\t\t \"Failed to add L2 table mac(%pM) for vport(0x%x), err(%d)\\n\",\n\t\t\t mac, vport, err);\n\t\treturn err;\n\t}\n\tvaddr->mpfs = true;\n\nfdb_add:\n\t \n\tif (esw->fdb_table.legacy.fdb && esw->mode == MLX5_ESWITCH_LEGACY) {\n\t\tvaddr->flow_rule = esw_fdb_set_vport_rule(esw, mac, vport);\n\n\t\tesw_debug(esw->dev, \"\\tADDED UC MAC: vport[%d] %pM fr(%p)\\n\",\n\t\t\t  vport, mac, vaddr->flow_rule);\n\t}\n\n\treturn 0;\n}\n\nstatic int esw_del_uc_addr(struct mlx5_eswitch *esw, struct vport_addr *vaddr)\n{\n\tu8 *mac = vaddr->node.addr;\n\tu16 vport = vaddr->vport;\n\tint err = 0;\n\n\t \n\tif (!vaddr->mpfs || mlx5_esw_is_manager_vport(esw, vport))\n\t\tgoto fdb_del;\n\n\terr = mlx5_mpfs_del_mac(esw->dev, mac);\n\tif (err)\n\t\tesw_warn(esw->dev,\n\t\t\t \"Failed to del L2 table mac(%pM) for vport(%d), err(%d)\\n\",\n\t\t\t mac, vport, err);\n\tvaddr->mpfs = false;\n\nfdb_del:\n\tif (vaddr->flow_rule)\n\t\tmlx5_del_flow_rules(vaddr->flow_rule);\n\tvaddr->flow_rule = NULL;\n\n\treturn 0;\n}\n\nstatic void update_allmulti_vports(struct mlx5_eswitch *esw,\n\t\t\t\t   struct vport_addr *vaddr,\n\t\t\t\t   struct esw_mc_addr *esw_mc)\n{\n\tu8 *mac = vaddr->node.addr;\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\tu16 vport_num;\n\n\tmlx5_esw_for_each_vport(esw, i, vport) {\n\t\tstruct hlist_head *vport_hash = vport->mc_list;\n\t\tstruct vport_addr *iter_vaddr =\n\t\t\t\t\tl2addr_hash_find(vport_hash,\n\t\t\t\t\t\t\t mac,\n\t\t\t\t\t\t\t struct vport_addr);\n\t\tvport_num = vport->vport;\n\t\tif (IS_ERR_OR_NULL(vport->allmulti_rule) ||\n\t\t    vaddr->vport == vport_num)\n\t\t\tcontinue;\n\t\tswitch (vaddr->action) {\n\t\tcase MLX5_ACTION_ADD:\n\t\t\tif (iter_vaddr)\n\t\t\t\tcontinue;\n\t\t\titer_vaddr = l2addr_hash_add(vport_hash, mac,\n\t\t\t\t\t\t     struct vport_addr,\n\t\t\t\t\t\t     GFP_KERNEL);\n\t\t\tif (!iter_vaddr) {\n\t\t\t\tesw_warn(esw->dev,\n\t\t\t\t\t \"ALL-MULTI: Failed to add MAC(%pM) to vport[%d] DB\\n\",\n\t\t\t\t\t mac, vport_num);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\titer_vaddr->vport = vport_num;\n\t\t\titer_vaddr->flow_rule =\n\t\t\t\t\tesw_fdb_set_vport_rule(esw,\n\t\t\t\t\t\t\t       mac,\n\t\t\t\t\t\t\t       vport_num);\n\t\t\titer_vaddr->mc_promisc = true;\n\t\t\tbreak;\n\t\tcase MLX5_ACTION_DEL:\n\t\t\tif (!iter_vaddr)\n\t\t\t\tcontinue;\n\t\t\tmlx5_del_flow_rules(iter_vaddr->flow_rule);\n\t\t\tl2addr_hash_del(iter_vaddr);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int esw_add_mc_addr(struct mlx5_eswitch *esw, struct vport_addr *vaddr)\n{\n\tstruct hlist_head *hash = esw->mc_table;\n\tstruct esw_mc_addr *esw_mc;\n\tu8 *mac = vaddr->node.addr;\n\tu16 vport = vaddr->vport;\n\n\tif (!esw->fdb_table.legacy.fdb)\n\t\treturn 0;\n\n\tesw_mc = l2addr_hash_find(hash, mac, struct esw_mc_addr);\n\tif (esw_mc)\n\t\tgoto add;\n\n\tesw_mc = l2addr_hash_add(hash, mac, struct esw_mc_addr, GFP_KERNEL);\n\tif (!esw_mc)\n\t\treturn -ENOMEM;\n\n\tesw_mc->uplink_rule =  \n\t\tesw_fdb_set_vport_rule(esw, mac, MLX5_VPORT_UPLINK);\n\n\t \n\tupdate_allmulti_vports(esw, vaddr, esw_mc);\n\nadd:\n\t \n\tif (!vaddr->mc_promisc)\n\t\tesw_mc->refcnt++;\n\n\t \n\tvaddr->flow_rule = esw_fdb_set_vport_rule(esw, mac, vport);\n\tesw_debug(esw->dev,\n\t\t  \"\\tADDED MC MAC: vport[%d] %pM fr(%p) refcnt(%d) uplinkfr(%p)\\n\",\n\t\t  vport, mac, vaddr->flow_rule,\n\t\t  esw_mc->refcnt, esw_mc->uplink_rule);\n\treturn 0;\n}\n\nstatic int esw_del_mc_addr(struct mlx5_eswitch *esw, struct vport_addr *vaddr)\n{\n\tstruct hlist_head *hash = esw->mc_table;\n\tstruct esw_mc_addr *esw_mc;\n\tu8 *mac = vaddr->node.addr;\n\tu16 vport = vaddr->vport;\n\n\tif (!esw->fdb_table.legacy.fdb)\n\t\treturn 0;\n\n\tesw_mc = l2addr_hash_find(hash, mac, struct esw_mc_addr);\n\tif (!esw_mc) {\n\t\tesw_warn(esw->dev,\n\t\t\t \"Failed to find eswitch MC addr for MAC(%pM) vport(%d)\",\n\t\t\t mac, vport);\n\t\treturn -EINVAL;\n\t}\n\tesw_debug(esw->dev,\n\t\t  \"\\tDELETE MC MAC: vport[%d] %pM fr(%p) refcnt(%d) uplinkfr(%p)\\n\",\n\t\t  vport, mac, vaddr->flow_rule, esw_mc->refcnt,\n\t\t  esw_mc->uplink_rule);\n\n\tif (vaddr->flow_rule)\n\t\tmlx5_del_flow_rules(vaddr->flow_rule);\n\tvaddr->flow_rule = NULL;\n\n\t \n\tif (vaddr->mc_promisc || (--esw_mc->refcnt > 0))\n\t\treturn 0;\n\n\t \n\tupdate_allmulti_vports(esw, vaddr, esw_mc);\n\n\tif (esw_mc->uplink_rule)\n\t\tmlx5_del_flow_rules(esw_mc->uplink_rule);\n\n\tl2addr_hash_del(esw_mc);\n\treturn 0;\n}\n\n \nstatic void esw_apply_vport_addr_list(struct mlx5_eswitch *esw,\n\t\t\t\t      struct mlx5_vport *vport, int list_type)\n{\n\tbool is_uc = list_type == MLX5_NVPRT_LIST_TYPE_UC;\n\tvport_addr_action vport_addr_add;\n\tvport_addr_action vport_addr_del;\n\tstruct vport_addr *addr;\n\tstruct l2addr_node *node;\n\tstruct hlist_head *hash;\n\tstruct hlist_node *tmp;\n\tint hi;\n\n\tvport_addr_add = is_uc ? esw_add_uc_addr :\n\t\t\t\t esw_add_mc_addr;\n\tvport_addr_del = is_uc ? esw_del_uc_addr :\n\t\t\t\t esw_del_mc_addr;\n\n\thash = is_uc ? vport->uc_list : vport->mc_list;\n\tfor_each_l2hash_node(node, tmp, hash, hi) {\n\t\taddr = container_of(node, struct vport_addr, node);\n\t\tswitch (addr->action) {\n\t\tcase MLX5_ACTION_ADD:\n\t\t\tvport_addr_add(esw, addr);\n\t\t\taddr->action = MLX5_ACTION_NONE;\n\t\t\tbreak;\n\t\tcase MLX5_ACTION_DEL:\n\t\t\tvport_addr_del(esw, addr);\n\t\t\tl2addr_hash_del(addr);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic void esw_update_vport_addr_list(struct mlx5_eswitch *esw,\n\t\t\t\t       struct mlx5_vport *vport, int list_type)\n{\n\tbool is_uc = list_type == MLX5_NVPRT_LIST_TYPE_UC;\n\tu8 (*mac_list)[ETH_ALEN];\n\tstruct l2addr_node *node;\n\tstruct vport_addr *addr;\n\tstruct hlist_head *hash;\n\tstruct hlist_node *tmp;\n\tint size;\n\tint err;\n\tint hi;\n\tint i;\n\n\tsize = is_uc ? MLX5_MAX_UC_PER_VPORT(esw->dev) :\n\t\t       MLX5_MAX_MC_PER_VPORT(esw->dev);\n\n\tmac_list = kcalloc(size, ETH_ALEN, GFP_KERNEL);\n\tif (!mac_list)\n\t\treturn;\n\n\thash = is_uc ? vport->uc_list : vport->mc_list;\n\n\tfor_each_l2hash_node(node, tmp, hash, hi) {\n\t\taddr = container_of(node, struct vport_addr, node);\n\t\taddr->action = MLX5_ACTION_DEL;\n\t}\n\n\tif (!vport->enabled)\n\t\tgoto out;\n\n\terr = mlx5_query_nic_vport_mac_list(esw->dev, vport->vport, list_type,\n\t\t\t\t\t    mac_list, &size);\n\tif (err)\n\t\tgoto out;\n\tesw_debug(esw->dev, \"vport[%d] context update %s list size (%d)\\n\",\n\t\t  vport->vport, is_uc ? \"UC\" : \"MC\", size);\n\n\tfor (i = 0; i < size; i++) {\n\t\tif (is_uc && !is_valid_ether_addr(mac_list[i]))\n\t\t\tcontinue;\n\n\t\tif (!is_uc && !is_multicast_ether_addr(mac_list[i]))\n\t\t\tcontinue;\n\n\t\taddr = l2addr_hash_find(hash, mac_list[i], struct vport_addr);\n\t\tif (addr) {\n\t\t\taddr->action = MLX5_ACTION_NONE;\n\t\t\t \n\t\t\tif (addr->mc_promisc) {\n\t\t\t\tstruct esw_mc_addr *esw_mc =\n\t\t\t\t\tl2addr_hash_find(esw->mc_table,\n\t\t\t\t\t\t\t mac_list[i],\n\t\t\t\t\t\t\t struct esw_mc_addr);\n\t\t\t\tif (!esw_mc) {\n\t\t\t\t\tesw_warn(esw->dev,\n\t\t\t\t\t\t \"Failed to MAC(%pM) in mcast DB\\n\",\n\t\t\t\t\t\t mac_list[i]);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tesw_mc->refcnt++;\n\t\t\t\taddr->mc_promisc = false;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\taddr = l2addr_hash_add(hash, mac_list[i], struct vport_addr,\n\t\t\t\t       GFP_KERNEL);\n\t\tif (!addr) {\n\t\t\tesw_warn(esw->dev,\n\t\t\t\t \"Failed to add MAC(%pM) to vport[%d] DB\\n\",\n\t\t\t\t mac_list[i], vport->vport);\n\t\t\tcontinue;\n\t\t}\n\t\taddr->vport = vport->vport;\n\t\taddr->action = MLX5_ACTION_ADD;\n\t}\nout:\n\tkfree(mac_list);\n}\n\n \nstatic void esw_update_vport_mc_promisc(struct mlx5_eswitch *esw,\n\t\t\t\t\tstruct mlx5_vport *vport)\n{\n\tstruct l2addr_node *node;\n\tstruct vport_addr *addr;\n\tstruct hlist_head *hash;\n\tstruct hlist_node *tmp;\n\tint hi;\n\n\thash = vport->mc_list;\n\n\tfor_each_l2hash_node(node, tmp, esw->mc_table, hi) {\n\t\tu8 *mac = node->addr;\n\n\t\taddr = l2addr_hash_find(hash, mac, struct vport_addr);\n\t\tif (addr) {\n\t\t\tif (addr->action == MLX5_ACTION_DEL)\n\t\t\t\taddr->action = MLX5_ACTION_NONE;\n\t\t\tcontinue;\n\t\t}\n\t\taddr = l2addr_hash_add(hash, mac, struct vport_addr,\n\t\t\t\t       GFP_KERNEL);\n\t\tif (!addr) {\n\t\t\tesw_warn(esw->dev,\n\t\t\t\t \"Failed to add allmulti MAC(%pM) to vport[%d] DB\\n\",\n\t\t\t\t mac, vport->vport);\n\t\t\tcontinue;\n\t\t}\n\t\taddr->vport = vport->vport;\n\t\taddr->action = MLX5_ACTION_ADD;\n\t\taddr->mc_promisc = true;\n\t}\n}\n\n \nstatic void esw_apply_vport_rx_mode(struct mlx5_eswitch *esw,\n\t\t\t\t    struct mlx5_vport *vport,\n\t\t\t\t    bool promisc, bool mc_promisc)\n{\n\tstruct esw_mc_addr *allmulti_addr = &esw->mc_promisc;\n\n\tif (IS_ERR_OR_NULL(vport->allmulti_rule) != mc_promisc)\n\t\tgoto promisc;\n\n\tif (mc_promisc) {\n\t\tvport->allmulti_rule =\n\t\t\tesw_fdb_set_vport_allmulti_rule(esw, vport->vport);\n\t\tif (!allmulti_addr->uplink_rule)\n\t\t\tallmulti_addr->uplink_rule =\n\t\t\t\tesw_fdb_set_vport_allmulti_rule(esw,\n\t\t\t\t\t\t\t\tMLX5_VPORT_UPLINK);\n\t\tallmulti_addr->refcnt++;\n\t} else if (vport->allmulti_rule) {\n\t\tmlx5_del_flow_rules(vport->allmulti_rule);\n\t\tvport->allmulti_rule = NULL;\n\n\t\tif (--allmulti_addr->refcnt > 0)\n\t\t\tgoto promisc;\n\n\t\tif (allmulti_addr->uplink_rule)\n\t\t\tmlx5_del_flow_rules(allmulti_addr->uplink_rule);\n\t\tallmulti_addr->uplink_rule = NULL;\n\t}\n\npromisc:\n\tif (IS_ERR_OR_NULL(vport->promisc_rule) != promisc)\n\t\treturn;\n\n\tif (promisc) {\n\t\tvport->promisc_rule =\n\t\t\tesw_fdb_set_vport_promisc_rule(esw, vport->vport);\n\t} else if (vport->promisc_rule) {\n\t\tmlx5_del_flow_rules(vport->promisc_rule);\n\t\tvport->promisc_rule = NULL;\n\t}\n}\n\n \nstatic void esw_update_vport_rx_mode(struct mlx5_eswitch *esw,\n\t\t\t\t     struct mlx5_vport *vport)\n{\n\tint promisc_all = 0;\n\tint promisc_uc = 0;\n\tint promisc_mc = 0;\n\tint err;\n\n\terr = mlx5_query_nic_vport_promisc(esw->dev,\n\t\t\t\t\t   vport->vport,\n\t\t\t\t\t   &promisc_uc,\n\t\t\t\t\t   &promisc_mc,\n\t\t\t\t\t   &promisc_all);\n\tif (err)\n\t\treturn;\n\tesw_debug(esw->dev, \"vport[%d] context update rx mode promisc_all=%d, all_multi=%d\\n\",\n\t\t  vport->vport, promisc_all, promisc_mc);\n\n\tif (!vport->info.trusted || !vport->enabled) {\n\t\tpromisc_uc = 0;\n\t\tpromisc_mc = 0;\n\t\tpromisc_all = 0;\n\t}\n\n\tesw_apply_vport_rx_mode(esw, vport, promisc_all,\n\t\t\t\t(promisc_all || promisc_mc));\n}\n\nvoid esw_vport_change_handle_locked(struct mlx5_vport *vport)\n{\n\tstruct mlx5_core_dev *dev = vport->dev;\n\tstruct mlx5_eswitch *esw = dev->priv.eswitch;\n\tu8 mac[ETH_ALEN];\n\n\tif (!MLX5_CAP_GEN(dev, log_max_l2_table))\n\t\treturn;\n\n\tmlx5_query_nic_vport_mac_address(dev, vport->vport, true, mac);\n\tesw_debug(dev, \"vport[%d] Context Changed: perm mac: %pM\\n\",\n\t\t  vport->vport, mac);\n\n\tif (vport->enabled_events & MLX5_VPORT_UC_ADDR_CHANGE) {\n\t\tesw_update_vport_addr_list(esw, vport, MLX5_NVPRT_LIST_TYPE_UC);\n\t\tesw_apply_vport_addr_list(esw, vport, MLX5_NVPRT_LIST_TYPE_UC);\n\t}\n\n\tif (vport->enabled_events & MLX5_VPORT_MC_ADDR_CHANGE)\n\t\tesw_update_vport_addr_list(esw, vport, MLX5_NVPRT_LIST_TYPE_MC);\n\n\tif (vport->enabled_events & MLX5_VPORT_PROMISC_CHANGE) {\n\t\tesw_update_vport_rx_mode(esw, vport);\n\t\tif (!IS_ERR_OR_NULL(vport->allmulti_rule))\n\t\t\tesw_update_vport_mc_promisc(esw, vport);\n\t}\n\n\tif (vport->enabled_events & (MLX5_VPORT_PROMISC_CHANGE | MLX5_VPORT_MC_ADDR_CHANGE))\n\t\tesw_apply_vport_addr_list(esw, vport, MLX5_NVPRT_LIST_TYPE_MC);\n\n\tesw_debug(esw->dev, \"vport[%d] Context Changed: Done\\n\", vport->vport);\n\tif (vport->enabled)\n\t\tarm_vport_context_events_cmd(dev, vport->vport,\n\t\t\t\t\t     vport->enabled_events);\n}\n\nstatic void esw_vport_change_handler(struct work_struct *work)\n{\n\tstruct mlx5_vport *vport =\n\t\tcontainer_of(work, struct mlx5_vport, vport_change_handler);\n\tstruct mlx5_eswitch *esw = vport->dev->priv.eswitch;\n\n\tmutex_lock(&esw->state_lock);\n\tesw_vport_change_handle_locked(vport);\n\tmutex_unlock(&esw->state_lock);\n}\n\nstatic void node_guid_gen_from_mac(u64 *node_guid, const u8 *mac)\n{\n\t((u8 *)node_guid)[7] = mac[0];\n\t((u8 *)node_guid)[6] = mac[1];\n\t((u8 *)node_guid)[5] = mac[2];\n\t((u8 *)node_guid)[4] = 0xff;\n\t((u8 *)node_guid)[3] = 0xfe;\n\t((u8 *)node_guid)[2] = mac[3];\n\t((u8 *)node_guid)[1] = mac[4];\n\t((u8 *)node_guid)[0] = mac[5];\n}\n\nstatic int esw_vport_setup_acl(struct mlx5_eswitch *esw,\n\t\t\t       struct mlx5_vport *vport)\n{\n\tif (esw->mode == MLX5_ESWITCH_LEGACY)\n\t\treturn esw_legacy_vport_acl_setup(esw, vport);\n\telse\n\t\treturn esw_vport_create_offloads_acl_tables(esw, vport);\n}\n\nstatic void esw_vport_cleanup_acl(struct mlx5_eswitch *esw,\n\t\t\t\t  struct mlx5_vport *vport)\n{\n\tif (esw->mode == MLX5_ESWITCH_LEGACY)\n\t\tesw_legacy_vport_acl_cleanup(esw, vport);\n\telse\n\t\tesw_vport_destroy_offloads_acl_tables(esw, vport);\n}\n\nstatic int mlx5_esw_vport_caps_get(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tint query_out_sz = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tvoid *query_ctx;\n\tvoid *hca_caps;\n\tint err;\n\n\tif (!MLX5_CAP_GEN(esw->dev, vhca_resource_manager))\n\t\treturn 0;\n\n\tquery_ctx = kzalloc(query_out_sz, GFP_KERNEL);\n\tif (!query_ctx)\n\t\treturn -ENOMEM;\n\n\terr = mlx5_vport_get_other_func_cap(esw->dev, vport->vport, query_ctx,\n\t\t\t\t\t    MLX5_CAP_GENERAL);\n\tif (err)\n\t\tgoto out_free;\n\n\thca_caps = MLX5_ADDR_OF(query_hca_cap_out, query_ctx, capability);\n\tvport->info.roce_enabled = MLX5_GET(cmd_hca_cap, hca_caps, roce);\n\n\tif (!MLX5_CAP_GEN_MAX(esw->dev, hca_cap_2))\n\t\tgoto out_free;\n\n\tmemset(query_ctx, 0, query_out_sz);\n\terr = mlx5_vport_get_other_func_cap(esw->dev, vport->vport, query_ctx,\n\t\t\t\t\t    MLX5_CAP_GENERAL_2);\n\tif (err)\n\t\tgoto out_free;\n\n\thca_caps = MLX5_ADDR_OF(query_hca_cap_out, query_ctx, capability);\n\tvport->info.mig_enabled = MLX5_GET(cmd_hca_cap_2, hca_caps, migratable);\n\n\terr = mlx5_esw_ipsec_vf_offload_get(esw->dev, vport);\nout_free:\n\tkfree(query_ctx);\n\treturn err;\n}\n\nstatic int esw_vport_setup(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tbool vst_mode_steering = esw_vst_mode_is_steering(esw);\n\tu16 vport_num = vport->vport;\n\tint flags;\n\tint err;\n\n\terr = esw_vport_setup_acl(esw, vport);\n\tif (err)\n\t\treturn err;\n\n\tif (mlx5_esw_is_manager_vport(esw, vport_num))\n\t\treturn 0;\n\n\terr = mlx5_esw_vport_caps_get(esw, vport);\n\tif (err)\n\t\tgoto err_caps;\n\n\tmlx5_modify_vport_admin_state(esw->dev,\n\t\t\t\t      MLX5_VPORT_STATE_OP_MOD_ESW_VPORT,\n\t\t\t\t      vport_num, 1,\n\t\t\t\t      vport->info.link_state);\n\n\t \n\tif (vport_num) {\n\t\tmlx5_modify_nic_vport_mac_address(esw->dev, vport_num,\n\t\t\t\t\t\t  vport->info.mac);\n\t\tmlx5_modify_nic_vport_node_guid(esw->dev, vport_num,\n\t\t\t\t\t\tvport->info.node_guid);\n\t}\n\n\tflags = (vport->info.vlan || vport->info.qos) ?\n\t\tSET_VLAN_STRIP | SET_VLAN_INSERT : 0;\n\tif (esw->mode == MLX5_ESWITCH_OFFLOADS || !vst_mode_steering)\n\t\tmodify_esw_vport_cvlan(esw->dev, vport_num, vport->info.vlan,\n\t\t\t\t       vport->info.qos, flags);\n\n\treturn 0;\n\nerr_caps:\n\tesw_vport_cleanup_acl(esw, vport);\n\treturn err;\n}\n\n \nstatic void esw_vport_cleanup(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tu16 vport_num = vport->vport;\n\n\tif (!mlx5_esw_is_manager_vport(esw, vport_num))\n\t\tmlx5_modify_vport_admin_state(esw->dev,\n\t\t\t\t\t      MLX5_VPORT_STATE_OP_MOD_ESW_VPORT,\n\t\t\t\t\t      vport_num, 1,\n\t\t\t\t\t      MLX5_VPORT_ADMIN_STATE_DOWN);\n\n\tmlx5_esw_qos_vport_disable(esw, vport);\n\tesw_vport_cleanup_acl(esw, vport);\n}\n\nint mlx5_esw_vport_enable(struct mlx5_eswitch *esw, struct mlx5_vport *vport,\n\t\t\t  enum mlx5_eswitch_vport_event enabled_events)\n{\n\tu16 vport_num = vport->vport;\n\tint ret;\n\n\tmutex_lock(&esw->state_lock);\n\tWARN_ON(vport->enabled);\n\n\tesw_debug(esw->dev, \"Enabling VPORT(%d)\\n\", vport_num);\n\n\tret = esw_vport_setup(esw, vport);\n\tif (ret)\n\t\tgoto done;\n\n\t \n\tvport->enabled_events = enabled_events;\n\tvport->enabled = true;\n\tif (vport->vport != MLX5_VPORT_PF &&\n\t    (vport->info.ipsec_crypto_enabled || vport->info.ipsec_packet_enabled))\n\t\tesw->enabled_ipsec_vf_count++;\n\n\t \n\tif (mlx5_esw_is_manager_vport(esw, vport_num) ||\n\t    (!vport_num && mlx5_core_is_ecpf(esw->dev)))\n\t\tvport->info.trusted = true;\n\n\tif (!mlx5_esw_is_manager_vport(esw, vport_num) &&\n\t    MLX5_CAP_GEN(esw->dev, vhca_resource_manager)) {\n\t\tret = mlx5_esw_vport_vhca_id_set(esw, vport_num);\n\t\tif (ret)\n\t\t\tgoto err_vhca_mapping;\n\t}\n\n\t \n\tif (mlx5_core_is_ecpf(esw->dev) && vport_num == MLX5_VPORT_PF)\n\t\tmlx5_query_nic_vport_mac_address(esw->dev, vport_num, true, vport->info.mac);\n\n\tesw_vport_change_handle_locked(vport);\n\n\tesw->enabled_vports++;\n\tesw_debug(esw->dev, \"Enabled VPORT(%d)\\n\", vport_num);\ndone:\n\tmutex_unlock(&esw->state_lock);\n\treturn ret;\n\nerr_vhca_mapping:\n\tesw_vport_cleanup(esw, vport);\n\tmutex_unlock(&esw->state_lock);\n\treturn ret;\n}\n\nvoid mlx5_esw_vport_disable(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tu16 vport_num = vport->vport;\n\n\tmutex_lock(&esw->state_lock);\n\n\tif (!vport->enabled)\n\t\tgoto done;\n\n\tesw_debug(esw->dev, \"Disabling vport(%d)\\n\", vport_num);\n\t \n\tvport->enabled = false;\n\n\t \n\tif (MLX5_CAP_GEN(esw->dev, log_max_l2_table))\n\t\tarm_vport_context_events_cmd(esw->dev, vport_num, 0);\n\n\tif (!mlx5_esw_is_manager_vport(esw, vport_num) &&\n\t    MLX5_CAP_GEN(esw->dev, vhca_resource_manager))\n\t\tmlx5_esw_vport_vhca_id_clear(esw, vport_num);\n\n\tif (vport->vport != MLX5_VPORT_PF &&\n\t    (vport->info.ipsec_crypto_enabled || vport->info.ipsec_packet_enabled))\n\t\tesw->enabled_ipsec_vf_count--;\n\n\t \n\tesw_vport_change_handle_locked(vport);\n\tvport->enabled_events = 0;\n\tesw_apply_vport_rx_mode(esw, vport, false, false);\n\tesw_vport_cleanup(esw, vport);\n\tesw->enabled_vports--;\n\ndone:\n\tmutex_unlock(&esw->state_lock);\n}\n\nstatic int eswitch_vport_event(struct notifier_block *nb,\n\t\t\t       unsigned long type, void *data)\n{\n\tstruct mlx5_eswitch *esw = mlx5_nb_cof(nb, struct mlx5_eswitch, nb);\n\tstruct mlx5_eqe *eqe = data;\n\tstruct mlx5_vport *vport;\n\tu16 vport_num;\n\n\tvport_num = be16_to_cpu(eqe->data.vport_change.vport_num);\n\tvport = mlx5_eswitch_get_vport(esw, vport_num);\n\tif (!IS_ERR(vport))\n\t\tqueue_work(esw->work_queue, &vport->vport_change_handler);\n\treturn NOTIFY_OK;\n}\n\n \nconst u32 *mlx5_esw_query_functions(struct mlx5_core_dev *dev)\n{\n\tint outlen = MLX5_ST_SZ_BYTES(query_esw_functions_out);\n\tu32 in[MLX5_ST_SZ_DW(query_esw_functions_in)] = {};\n\tu32 *out;\n\tint err;\n\n\tout = kvzalloc(outlen, GFP_KERNEL);\n\tif (!out)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tMLX5_SET(query_esw_functions_in, in, opcode,\n\t\t MLX5_CMD_OP_QUERY_ESW_FUNCTIONS);\n\n\terr = mlx5_cmd_exec(dev, in, sizeof(in), out, outlen);\n\tif (!err)\n\t\treturn out;\n\n\tkvfree(out);\n\treturn ERR_PTR(err);\n}\n\nstatic void mlx5_eswitch_event_handler_register(struct mlx5_eswitch *esw)\n{\n\tif (esw->mode == MLX5_ESWITCH_OFFLOADS && mlx5_eswitch_is_funcs_handler(esw->dev)) {\n\t\tMLX5_NB_INIT(&esw->esw_funcs.nb, mlx5_esw_funcs_changed_handler,\n\t\t\t     ESW_FUNCTIONS_CHANGED);\n\t\tmlx5_eq_notifier_register(esw->dev, &esw->esw_funcs.nb);\n\t}\n}\n\nstatic void mlx5_eswitch_event_handler_unregister(struct mlx5_eswitch *esw)\n{\n\tif (esw->mode == MLX5_ESWITCH_OFFLOADS && mlx5_eswitch_is_funcs_handler(esw->dev))\n\t\tmlx5_eq_notifier_unregister(esw->dev, &esw->esw_funcs.nb);\n\n\tflush_workqueue(esw->work_queue);\n}\n\nstatic void mlx5_eswitch_clear_vf_vports_info(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tmlx5_esw_for_each_vf_vport(esw, i, vport, esw->esw_funcs.num_vfs) {\n\t\tmemset(&vport->qos, 0, sizeof(vport->qos));\n\t\tmemset(&vport->info, 0, sizeof(vport->info));\n\t\tvport->info.link_state = MLX5_VPORT_ADMIN_STATE_AUTO;\n\t}\n}\n\nstatic void mlx5_eswitch_clear_ec_vf_vports_info(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tmlx5_esw_for_each_ec_vf_vport(esw, i, vport, esw->esw_funcs.num_ec_vfs) {\n\t\tmemset(&vport->qos, 0, sizeof(vport->qos));\n\t\tmemset(&vport->info, 0, sizeof(vport->info));\n\t\tvport->info.link_state = MLX5_VPORT_ADMIN_STATE_AUTO;\n\t}\n}\n\nstatic int mlx5_eswitch_load_vport(struct mlx5_eswitch *esw, struct mlx5_vport *vport,\n\t\t\t\t   enum mlx5_eswitch_vport_event enabled_events)\n{\n\tint err;\n\n\terr = mlx5_esw_vport_enable(esw, vport, enabled_events);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_esw_offloads_load_rep(esw, vport);\n\tif (err)\n\t\tgoto err_rep;\n\n\treturn err;\n\nerr_rep:\n\tmlx5_esw_vport_disable(esw, vport);\n\treturn err;\n}\n\nstatic void mlx5_eswitch_unload_vport(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tmlx5_esw_offloads_unload_rep(esw, vport);\n\tmlx5_esw_vport_disable(esw, vport);\n}\n\nstatic int mlx5_eswitch_load_pf_vf_vport(struct mlx5_eswitch *esw, u16 vport_num,\n\t\t\t\t\t enum mlx5_eswitch_vport_event enabled_events)\n{\n\tstruct mlx5_vport *vport;\n\tint err;\n\n\tvport = mlx5_eswitch_get_vport(esw, vport_num);\n\tif (IS_ERR(vport))\n\t\treturn PTR_ERR(vport);\n\n\terr = mlx5_esw_offloads_init_pf_vf_rep(esw, vport);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_eswitch_load_vport(esw, vport, enabled_events);\n\tif (err)\n\t\tgoto err_load;\n\treturn 0;\n\nerr_load:\n\tmlx5_esw_offloads_cleanup_pf_vf_rep(esw, vport);\n\treturn err;\n}\n\nstatic void mlx5_eswitch_unload_pf_vf_vport(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\tstruct mlx5_vport *vport;\n\n\tvport = mlx5_eswitch_get_vport(esw, vport_num);\n\tif (IS_ERR(vport))\n\t\treturn;\n\n\tmlx5_eswitch_unload_vport(esw, vport);\n\tmlx5_esw_offloads_cleanup_pf_vf_rep(esw, vport);\n}\n\nint mlx5_eswitch_load_sf_vport(struct mlx5_eswitch *esw, u16 vport_num,\n\t\t\t       enum mlx5_eswitch_vport_event enabled_events,\n\t\t\t       struct mlx5_devlink_port *dl_port, u32 controller, u32 sfnum)\n{\n\tstruct mlx5_vport *vport;\n\tint err;\n\n\tvport = mlx5_eswitch_get_vport(esw, vport_num);\n\tif (IS_ERR(vport))\n\t\treturn PTR_ERR(vport);\n\n\terr = mlx5_esw_offloads_init_sf_rep(esw, vport, dl_port, controller, sfnum);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_eswitch_load_vport(esw, vport, enabled_events);\n\tif (err)\n\t\tgoto err_load;\n\n\treturn 0;\n\nerr_load:\n\tmlx5_esw_offloads_cleanup_sf_rep(esw, vport);\n\treturn err;\n}\n\nvoid mlx5_eswitch_unload_sf_vport(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\tstruct mlx5_vport *vport;\n\n\tvport = mlx5_eswitch_get_vport(esw, vport_num);\n\tif (IS_ERR(vport))\n\t\treturn;\n\n\tmlx5_eswitch_unload_vport(esw, vport);\n\tmlx5_esw_offloads_cleanup_sf_rep(esw, vport);\n}\n\nvoid mlx5_eswitch_unload_vf_vports(struct mlx5_eswitch *esw, u16 num_vfs)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tmlx5_esw_for_each_vf_vport(esw, i, vport, num_vfs) {\n\t\tif (!vport->enabled)\n\t\t\tcontinue;\n\t\tmlx5_eswitch_unload_pf_vf_vport(esw, vport->vport);\n\t}\n}\n\nstatic void mlx5_eswitch_unload_ec_vf_vports(struct mlx5_eswitch *esw,\n\t\t\t\t\t     u16 num_ec_vfs)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tmlx5_esw_for_each_ec_vf_vport(esw, i, vport, num_ec_vfs) {\n\t\tif (!vport->enabled)\n\t\t\tcontinue;\n\t\tmlx5_eswitch_unload_pf_vf_vport(esw, vport->vport);\n\t}\n}\n\nint mlx5_eswitch_load_vf_vports(struct mlx5_eswitch *esw, u16 num_vfs,\n\t\t\t\tenum mlx5_eswitch_vport_event enabled_events)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\tint err;\n\n\tmlx5_esw_for_each_vf_vport(esw, i, vport, num_vfs) {\n\t\terr = mlx5_eswitch_load_pf_vf_vport(esw, vport->vport, enabled_events);\n\t\tif (err)\n\t\t\tgoto vf_err;\n\t}\n\n\treturn 0;\n\nvf_err:\n\tmlx5_eswitch_unload_vf_vports(esw, num_vfs);\n\treturn err;\n}\n\nstatic int mlx5_eswitch_load_ec_vf_vports(struct mlx5_eswitch *esw, u16 num_ec_vfs,\n\t\t\t\t\t  enum mlx5_eswitch_vport_event enabled_events)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\tint err;\n\n\tmlx5_esw_for_each_ec_vf_vport(esw, i, vport, num_ec_vfs) {\n\t\terr = mlx5_eswitch_load_pf_vf_vport(esw, vport->vport, enabled_events);\n\t\tif (err)\n\t\t\tgoto vf_err;\n\t}\n\n\treturn 0;\n\nvf_err:\n\tmlx5_eswitch_unload_ec_vf_vports(esw, num_ec_vfs);\n\treturn err;\n}\n\nstatic int host_pf_enable_hca(struct mlx5_core_dev *dev)\n{\n\tif (!mlx5_core_is_ecpf(dev))\n\t\treturn 0;\n\n\t \n\treturn mlx5_cmd_host_pf_enable_hca(dev);\n}\n\nstatic void host_pf_disable_hca(struct mlx5_core_dev *dev)\n{\n\tif (!mlx5_core_is_ecpf(dev))\n\t\treturn;\n\n\tmlx5_cmd_host_pf_disable_hca(dev);\n}\n\n \nint\nmlx5_eswitch_enable_pf_vf_vports(struct mlx5_eswitch *esw,\n\t\t\t\t enum mlx5_eswitch_vport_event enabled_events)\n{\n\tbool pf_needed;\n\tint ret;\n\n\tpf_needed = mlx5_core_is_ecpf_esw_manager(esw->dev) ||\n\t\t    esw->mode == MLX5_ESWITCH_LEGACY;\n\n\t \n\tif (pf_needed) {\n\t\tret = mlx5_eswitch_load_pf_vf_vport(esw, MLX5_VPORT_PF,\n\t\t\t\t\t\t    enabled_events);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tret = host_pf_enable_hca(esw->dev);\n\tif (ret)\n\t\tgoto pf_hca_err;\n\n\t \n\tif (mlx5_ecpf_vport_exists(esw->dev)) {\n\t\tret = mlx5_eswitch_load_pf_vf_vport(esw, MLX5_VPORT_ECPF, enabled_events);\n\t\tif (ret)\n\t\t\tgoto ecpf_err;\n\t\tif (mlx5_core_ec_sriov_enabled(esw->dev)) {\n\t\t\tret = mlx5_eswitch_load_ec_vf_vports(esw, esw->esw_funcs.num_ec_vfs,\n\t\t\t\t\t\t\t     enabled_events);\n\t\t\tif (ret)\n\t\t\t\tgoto ec_vf_err;\n\t\t}\n\t}\n\n\t \n\tret = mlx5_eswitch_load_vf_vports(esw, esw->esw_funcs.num_vfs,\n\t\t\t\t\t  enabled_events);\n\tif (ret)\n\t\tgoto vf_err;\n\treturn 0;\n\nvf_err:\n\tif (mlx5_core_ec_sriov_enabled(esw->dev))\n\t\tmlx5_eswitch_unload_ec_vf_vports(esw, esw->esw_funcs.num_ec_vfs);\nec_vf_err:\n\tif (mlx5_ecpf_vport_exists(esw->dev))\n\t\tmlx5_eswitch_unload_pf_vf_vport(esw, MLX5_VPORT_ECPF);\necpf_err:\n\thost_pf_disable_hca(esw->dev);\npf_hca_err:\n\tif (pf_needed)\n\t\tmlx5_eswitch_unload_pf_vf_vport(esw, MLX5_VPORT_PF);\n\treturn ret;\n}\n\n \nvoid mlx5_eswitch_disable_pf_vf_vports(struct mlx5_eswitch *esw)\n{\n\tmlx5_eswitch_unload_vf_vports(esw, esw->esw_funcs.num_vfs);\n\n\tif (mlx5_ecpf_vport_exists(esw->dev)) {\n\t\tif (mlx5_core_ec_sriov_enabled(esw->dev))\n\t\t\tmlx5_eswitch_unload_ec_vf_vports(esw, esw->esw_funcs.num_vfs);\n\t\tmlx5_eswitch_unload_pf_vf_vport(esw, MLX5_VPORT_ECPF);\n\t}\n\n\thost_pf_disable_hca(esw->dev);\n\n\tif (mlx5_core_is_ecpf_esw_manager(esw->dev) ||\n\t    esw->mode == MLX5_ESWITCH_LEGACY)\n\t\tmlx5_eswitch_unload_pf_vf_vport(esw, MLX5_VPORT_PF);\n}\n\nstatic void mlx5_eswitch_get_devlink_param(struct mlx5_eswitch *esw)\n{\n\tstruct devlink *devlink = priv_to_devlink(esw->dev);\n\tunion devlink_param_value val;\n\tint err;\n\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      MLX5_DEVLINK_PARAM_ID_ESW_LARGE_GROUP_NUM,\n\t\t\t\t\t      &val);\n\tif (!err) {\n\t\tesw->params.large_group_num = val.vu32;\n\t} else {\n\t\tesw_warn(esw->dev,\n\t\t\t \"Devlink can't get param fdb_large_groups, uses default (%d).\\n\",\n\t\t\t ESW_OFFLOADS_DEFAULT_NUM_GROUPS);\n\t\tesw->params.large_group_num = ESW_OFFLOADS_DEFAULT_NUM_GROUPS;\n\t}\n}\n\nstatic void\nmlx5_eswitch_update_num_of_vfs(struct mlx5_eswitch *esw, int num_vfs)\n{\n\tconst u32 *out;\n\n\tif (num_vfs < 0)\n\t\treturn;\n\n\tif (!mlx5_core_is_ecpf_esw_manager(esw->dev)) {\n\t\tesw->esw_funcs.num_vfs = num_vfs;\n\t\treturn;\n\t}\n\n\tout = mlx5_esw_query_functions(esw->dev);\n\tif (IS_ERR(out))\n\t\treturn;\n\n\tesw->esw_funcs.num_vfs = MLX5_GET(query_esw_functions_out, out,\n\t\t\t\t\t  host_params_context.host_num_of_vfs);\n\tif (mlx5_core_ec_sriov_enabled(esw->dev))\n\t\tesw->esw_funcs.num_ec_vfs = num_vfs;\n\n\tkvfree(out);\n}\n\nstatic void mlx5_esw_mode_change_notify(struct mlx5_eswitch *esw, u16 mode)\n{\n\tstruct mlx5_esw_event_info info = {};\n\n\tinfo.new_mode = mode;\n\n\tblocking_notifier_call_chain(&esw->n_head, 0, &info);\n}\n\nstatic int mlx5_esw_acls_ns_init(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_core_dev *dev = esw->dev;\n\tint total_vports;\n\tint err;\n\n\tif (esw->flags & MLX5_ESWITCH_VPORT_ACL_NS_CREATED)\n\t\treturn 0;\n\n\ttotal_vports = mlx5_eswitch_get_total_vports(dev);\n\n\tif (MLX5_CAP_ESW_EGRESS_ACL(dev, ft_support)) {\n\t\terr = mlx5_fs_egress_acls_init(dev, total_vports);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\tesw_warn(dev, \"egress ACL is not supported by FW\\n\");\n\t}\n\n\tif (MLX5_CAP_ESW_INGRESS_ACL(dev, ft_support)) {\n\t\terr = mlx5_fs_ingress_acls_init(dev, total_vports);\n\t\tif (err)\n\t\t\tgoto err;\n\t} else {\n\t\tesw_warn(dev, \"ingress ACL is not supported by FW\\n\");\n\t}\n\tesw->flags |= MLX5_ESWITCH_VPORT_ACL_NS_CREATED;\n\treturn 0;\n\nerr:\n\tif (MLX5_CAP_ESW_EGRESS_ACL(dev, ft_support))\n\t\tmlx5_fs_egress_acls_cleanup(dev);\n\treturn err;\n}\n\nstatic void mlx5_esw_acls_ns_cleanup(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_core_dev *dev = esw->dev;\n\n\tesw->flags &= ~MLX5_ESWITCH_VPORT_ACL_NS_CREATED;\n\tif (MLX5_CAP_ESW_INGRESS_ACL(dev, ft_support))\n\t\tmlx5_fs_ingress_acls_cleanup(dev);\n\tif (MLX5_CAP_ESW_EGRESS_ACL(dev, ft_support))\n\t\tmlx5_fs_egress_acls_cleanup(dev);\n}\n\n \nint mlx5_eswitch_enable_locked(struct mlx5_eswitch *esw, int num_vfs)\n{\n\tint err;\n\n\tdevl_assert_locked(priv_to_devlink(esw->dev));\n\n\tif (!MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, ft_support)) {\n\t\tesw_warn(esw->dev, \"FDB is not supported, aborting ...\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmlx5_eswitch_get_devlink_param(esw);\n\n\terr = mlx5_esw_acls_ns_init(esw);\n\tif (err)\n\t\treturn err;\n\n\tmlx5_eswitch_update_num_of_vfs(esw, num_vfs);\n\n\tMLX5_NB_INIT(&esw->nb, eswitch_vport_event, NIC_VPORT_CHANGE);\n\tmlx5_eq_notifier_register(esw->dev, &esw->nb);\n\n\tif (esw->mode == MLX5_ESWITCH_LEGACY) {\n\t\terr = esw_legacy_enable(esw);\n\t} else {\n\t\tmlx5_rescan_drivers(esw->dev);\n\t\terr = esw_offloads_enable(esw);\n\t}\n\n\tif (err)\n\t\tgoto abort;\n\n\tesw->fdb_table.flags |= MLX5_ESW_FDB_CREATED;\n\n\tmlx5_eswitch_event_handler_register(esw);\n\n\tesw_info(esw->dev, \"Enable: mode(%s), nvfs(%d), necvfs(%d), active vports(%d)\\n\",\n\t\t esw->mode == MLX5_ESWITCH_LEGACY ? \"LEGACY\" : \"OFFLOADS\",\n\t\t esw->esw_funcs.num_vfs, esw->esw_funcs.num_ec_vfs, esw->enabled_vports);\n\n\tmlx5_esw_mode_change_notify(esw, esw->mode);\n\n\treturn 0;\n\nabort:\n\tmlx5_esw_acls_ns_cleanup(esw);\n\treturn err;\n}\n\n \nint mlx5_eswitch_enable(struct mlx5_eswitch *esw, int num_vfs)\n{\n\tbool toggle_lag;\n\tint ret = 0;\n\n\tif (!mlx5_esw_allowed(esw))\n\t\treturn 0;\n\n\tdevl_assert_locked(priv_to_devlink(esw->dev));\n\n\ttoggle_lag = !mlx5_esw_is_fdb_created(esw);\n\n\tif (toggle_lag)\n\t\tmlx5_lag_disable_change(esw->dev);\n\n\tif (!mlx5_esw_is_fdb_created(esw)) {\n\t\tret = mlx5_eswitch_enable_locked(esw, num_vfs);\n\t} else {\n\t\tenum mlx5_eswitch_vport_event vport_events;\n\n\t\tvport_events = (esw->mode == MLX5_ESWITCH_LEGACY) ?\n\t\t\t\t\tMLX5_LEGACY_SRIOV_VPORT_EVENTS : MLX5_VPORT_UC_ADDR_CHANGE;\n\t\t \n\t\tif (!mlx5_core_is_ecpf(esw->dev)) {\n\t\t\tret = mlx5_eswitch_load_vf_vports(esw, num_vfs, vport_events);\n\t\t\tif (!ret)\n\t\t\t\tesw->esw_funcs.num_vfs = num_vfs;\n\t\t} else if (mlx5_core_ec_sriov_enabled(esw->dev)) {\n\t\t\tret = mlx5_eswitch_load_ec_vf_vports(esw, num_vfs, vport_events);\n\t\t\tif (!ret)\n\t\t\t\tesw->esw_funcs.num_ec_vfs = num_vfs;\n\t\t}\n\t}\n\n\tif (toggle_lag)\n\t\tmlx5_lag_enable_change(esw->dev);\n\n\treturn ret;\n}\n\n \nvoid mlx5_eswitch_disable_sriov(struct mlx5_eswitch *esw, bool clear_vf)\n{\n\tif (!mlx5_esw_allowed(esw))\n\t\treturn;\n\n\tdevl_assert_locked(priv_to_devlink(esw->dev));\n\t \n\tif (!esw->esw_funcs.num_vfs && !esw->esw_funcs.num_ec_vfs && !clear_vf)\n\t\treturn;\n\n\tesw_info(esw->dev, \"Unload vfs: mode(%s), nvfs(%d), necvfs(%d), active vports(%d)\\n\",\n\t\t esw->mode == MLX5_ESWITCH_LEGACY ? \"LEGACY\" : \"OFFLOADS\",\n\t\t esw->esw_funcs.num_vfs, esw->esw_funcs.num_ec_vfs, esw->enabled_vports);\n\n\tif (!mlx5_core_is_ecpf(esw->dev)) {\n\t\tmlx5_eswitch_unload_vf_vports(esw, esw->esw_funcs.num_vfs);\n\t\tif (clear_vf)\n\t\t\tmlx5_eswitch_clear_vf_vports_info(esw);\n\t} else if (mlx5_core_ec_sriov_enabled(esw->dev)) {\n\t\tmlx5_eswitch_unload_ec_vf_vports(esw, esw->esw_funcs.num_ec_vfs);\n\t\tif (clear_vf)\n\t\t\tmlx5_eswitch_clear_ec_vf_vports_info(esw);\n\t}\n\n\tif (esw->mode == MLX5_ESWITCH_OFFLOADS) {\n\t\tstruct devlink *devlink = priv_to_devlink(esw->dev);\n\n\t\tdevl_rate_nodes_destroy(devlink);\n\t}\n\t \n\tif (esw->mode == MLX5_ESWITCH_LEGACY)\n\t\tmlx5_eswitch_disable_locked(esw);\n\n\tif (!mlx5_core_is_ecpf(esw->dev))\n\t\tesw->esw_funcs.num_vfs = 0;\n\telse\n\t\tesw->esw_funcs.num_ec_vfs = 0;\n}\n\n \nvoid mlx5_eswitch_disable_locked(struct mlx5_eswitch *esw)\n{\n\tstruct devlink *devlink = priv_to_devlink(esw->dev);\n\n\t \n\tmlx5_esw_mode_change_notify(esw, MLX5_ESWITCH_LEGACY);\n\n\tmlx5_eq_notifier_unregister(esw->dev, &esw->nb);\n\tmlx5_eswitch_event_handler_unregister(esw);\n\n\tesw_info(esw->dev, \"Disable: mode(%s), nvfs(%d), necvfs(%d), active vports(%d)\\n\",\n\t\t esw->mode == MLX5_ESWITCH_LEGACY ? \"LEGACY\" : \"OFFLOADS\",\n\t\t esw->esw_funcs.num_vfs, esw->esw_funcs.num_ec_vfs, esw->enabled_vports);\n\n\tif (esw->fdb_table.flags & MLX5_ESW_FDB_CREATED) {\n\t\tesw->fdb_table.flags &= ~MLX5_ESW_FDB_CREATED;\n\t\tif (esw->mode == MLX5_ESWITCH_OFFLOADS)\n\t\t\tesw_offloads_disable(esw);\n\t\telse if (esw->mode == MLX5_ESWITCH_LEGACY)\n\t\t\tesw_legacy_disable(esw);\n\t\tmlx5_esw_acls_ns_cleanup(esw);\n\t}\n\n\tif (esw->mode == MLX5_ESWITCH_OFFLOADS)\n\t\tdevl_rate_nodes_destroy(devlink);\n}\n\nvoid mlx5_eswitch_disable(struct mlx5_eswitch *esw)\n{\n\tif (!mlx5_esw_allowed(esw))\n\t\treturn;\n\n\tdevl_assert_locked(priv_to_devlink(esw->dev));\n\tmlx5_lag_disable_change(esw->dev);\n\tmlx5_eswitch_disable_locked(esw);\n\tesw->mode = MLX5_ESWITCH_LEGACY;\n\tmlx5_lag_enable_change(esw->dev);\n}\n\nstatic int mlx5_query_hca_cap_host_pf(struct mlx5_core_dev *dev, void *out)\n{\n\tu16 opmod = (MLX5_CAP_GENERAL << 1) | (HCA_CAP_OPMOD_GET_MAX & 0x01);\n\tu8 in[MLX5_ST_SZ_BYTES(query_hca_cap_in)] = {};\n\n\tMLX5_SET(query_hca_cap_in, in, opcode, MLX5_CMD_OP_QUERY_HCA_CAP);\n\tMLX5_SET(query_hca_cap_in, in, op_mod, opmod);\n\tMLX5_SET(query_hca_cap_in, in, function_id, MLX5_VPORT_PF);\n\tMLX5_SET(query_hca_cap_in, in, other_function, true);\n\treturn mlx5_cmd_exec_inout(dev, query_hca_cap, in, out);\n}\n\nint mlx5_esw_sf_max_hpf_functions(struct mlx5_core_dev *dev, u16 *max_sfs, u16 *sf_base_id)\n\n{\n\tint query_out_sz = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tvoid *query_ctx;\n\tvoid *hca_caps;\n\tint err;\n\n\tif (!mlx5_core_is_ecpf(dev)) {\n\t\t*max_sfs = 0;\n\t\treturn 0;\n\t}\n\n\tquery_ctx = kzalloc(query_out_sz, GFP_KERNEL);\n\tif (!query_ctx)\n\t\treturn -ENOMEM;\n\n\terr = mlx5_query_hca_cap_host_pf(dev, query_ctx);\n\tif (err)\n\t\tgoto out_free;\n\n\thca_caps = MLX5_ADDR_OF(query_hca_cap_out, query_ctx, capability);\n\t*max_sfs = MLX5_GET(cmd_hca_cap, hca_caps, max_num_sf);\n\t*sf_base_id = MLX5_GET(cmd_hca_cap, hca_caps, sf_base_id);\n\nout_free:\n\tkfree(query_ctx);\n\treturn err;\n}\n\nstatic int mlx5_esw_vport_alloc(struct mlx5_eswitch *esw,\n\t\t\t\tint index, u16 vport_num)\n{\n\tstruct mlx5_vport *vport;\n\tint err;\n\n\tvport = kzalloc(sizeof(*vport), GFP_KERNEL);\n\tif (!vport)\n\t\treturn -ENOMEM;\n\n\tvport->dev = esw->dev;\n\tvport->vport = vport_num;\n\tvport->index = index;\n\tvport->info.link_state = MLX5_VPORT_ADMIN_STATE_AUTO;\n\tINIT_WORK(&vport->vport_change_handler, esw_vport_change_handler);\n\terr = xa_insert(&esw->vports, vport_num, vport, GFP_KERNEL);\n\tif (err)\n\t\tgoto insert_err;\n\n\tesw->total_vports++;\n\treturn 0;\n\ninsert_err:\n\tkfree(vport);\n\treturn err;\n}\n\nstatic void mlx5_esw_vport_free(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\txa_erase(&esw->vports, vport->vport);\n\tkfree(vport);\n}\n\nstatic void mlx5_esw_vports_cleanup(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tmlx5_esw_for_each_vport(esw, i, vport)\n\t\tmlx5_esw_vport_free(esw, vport);\n\txa_destroy(&esw->vports);\n}\n\nstatic int mlx5_esw_vports_init(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_core_dev *dev = esw->dev;\n\tu16 max_host_pf_sfs;\n\tu16 base_sf_num;\n\tint idx = 0;\n\tint err;\n\tint i;\n\n\txa_init(&esw->vports);\n\n\terr = mlx5_esw_vport_alloc(esw, idx, MLX5_VPORT_PF);\n\tif (err)\n\t\tgoto err;\n\tif (esw->first_host_vport == MLX5_VPORT_PF)\n\t\txa_set_mark(&esw->vports, idx, MLX5_ESW_VPT_HOST_FN);\n\tidx++;\n\n\tfor (i = 0; i < mlx5_core_max_vfs(dev); i++) {\n\t\terr = mlx5_esw_vport_alloc(esw, idx, idx);\n\t\tif (err)\n\t\t\tgoto err;\n\t\txa_set_mark(&esw->vports, idx, MLX5_ESW_VPT_VF);\n\t\txa_set_mark(&esw->vports, idx, MLX5_ESW_VPT_HOST_FN);\n\t\tidx++;\n\t}\n\tbase_sf_num = mlx5_sf_start_function_id(dev);\n\tfor (i = 0; i < mlx5_sf_max_functions(dev); i++) {\n\t\terr = mlx5_esw_vport_alloc(esw, idx, base_sf_num + i);\n\t\tif (err)\n\t\t\tgoto err;\n\t\txa_set_mark(&esw->vports, base_sf_num + i, MLX5_ESW_VPT_SF);\n\t\tidx++;\n\t}\n\n\terr = mlx5_esw_sf_max_hpf_functions(dev, &max_host_pf_sfs, &base_sf_num);\n\tif (err)\n\t\tgoto err;\n\tfor (i = 0; i < max_host_pf_sfs; i++) {\n\t\terr = mlx5_esw_vport_alloc(esw, idx, base_sf_num + i);\n\t\tif (err)\n\t\t\tgoto err;\n\t\txa_set_mark(&esw->vports, base_sf_num + i, MLX5_ESW_VPT_SF);\n\t\tidx++;\n\t}\n\n\tif (mlx5_core_ec_sriov_enabled(esw->dev)) {\n\t\tint ec_vf_base_num = mlx5_core_ec_vf_vport_base(dev);\n\n\t\tfor (i = 0; i < mlx5_core_max_ec_vfs(esw->dev); i++) {\n\t\t\terr = mlx5_esw_vport_alloc(esw, idx, ec_vf_base_num + i);\n\t\t\tif (err)\n\t\t\t\tgoto err;\n\t\t\tidx++;\n\t\t}\n\t}\n\n\tif (mlx5_ecpf_vport_exists(dev) ||\n\t    mlx5_core_is_ecpf_esw_manager(dev)) {\n\t\terr = mlx5_esw_vport_alloc(esw, idx, MLX5_VPORT_ECPF);\n\t\tif (err)\n\t\t\tgoto err;\n\t\tidx++;\n\t}\n\terr = mlx5_esw_vport_alloc(esw, idx, MLX5_VPORT_UPLINK);\n\tif (err)\n\t\tgoto err;\n\treturn 0;\n\nerr:\n\tmlx5_esw_vports_cleanup(esw);\n\treturn err;\n}\n\nstatic int mlx5_devlink_esw_multiport_set(struct devlink *devlink, u32 id,\n\t\t\t\t\t  struct devlink_param_gset_ctx *ctx)\n{\n\tstruct mlx5_core_dev *dev = devlink_priv(devlink);\n\n\tif (!MLX5_ESWITCH_MANAGER(dev))\n\t\treturn -EOPNOTSUPP;\n\n\tif (ctx->val.vbool)\n\t\treturn mlx5_lag_mpesw_enable(dev);\n\n\tmlx5_lag_mpesw_disable(dev);\n\treturn 0;\n}\n\nstatic int mlx5_devlink_esw_multiport_get(struct devlink *devlink, u32 id,\n\t\t\t\t\t  struct devlink_param_gset_ctx *ctx)\n{\n\tstruct mlx5_core_dev *dev = devlink_priv(devlink);\n\n\tctx->val.vbool = mlx5_lag_is_mpesw(dev);\n\treturn 0;\n}\n\nstatic const struct devlink_param mlx5_eswitch_params[] = {\n\tDEVLINK_PARAM_DRIVER(MLX5_DEVLINK_PARAM_ID_ESW_MULTIPORT,\n\t\t\t     \"esw_multiport\", DEVLINK_PARAM_TYPE_BOOL,\n\t\t\t     BIT(DEVLINK_PARAM_CMODE_RUNTIME),\n\t\t\t     mlx5_devlink_esw_multiport_get,\n\t\t\t     mlx5_devlink_esw_multiport_set, NULL),\n};\n\nint mlx5_eswitch_init(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw;\n\tint err;\n\n\tif (!MLX5_VPORT_MANAGER(dev) && !MLX5_ESWITCH_MANAGER(dev))\n\t\treturn 0;\n\n\tesw = kzalloc(sizeof(*esw), GFP_KERNEL);\n\tif (!esw)\n\t\treturn -ENOMEM;\n\n\terr = devl_params_register(priv_to_devlink(dev), mlx5_eswitch_params,\n\t\t\t\t   ARRAY_SIZE(mlx5_eswitch_params));\n\tif (err)\n\t\tgoto free_esw;\n\n\tesw->dev = dev;\n\tesw->manager_vport = mlx5_eswitch_manager_vport(dev);\n\tesw->first_host_vport = mlx5_eswitch_first_host_vport_num(dev);\n\n\tesw->debugfs_root = debugfs_create_dir(\"esw\", mlx5_debugfs_get_dev_root(dev));\n\tesw->work_queue = create_singlethread_workqueue(\"mlx5_esw_wq\");\n\tif (!esw->work_queue) {\n\t\terr = -ENOMEM;\n\t\tgoto abort;\n\t}\n\n\terr = mlx5_esw_vports_init(esw);\n\tif (err)\n\t\tgoto abort;\n\n\terr = esw_offloads_init(esw);\n\tif (err)\n\t\tgoto reps_err;\n\n\tmutex_init(&esw->offloads.encap_tbl_lock);\n\thash_init(esw->offloads.encap_tbl);\n\tmutex_init(&esw->offloads.decap_tbl_lock);\n\thash_init(esw->offloads.decap_tbl);\n\tmlx5e_mod_hdr_tbl_init(&esw->offloads.mod_hdr);\n\tatomic64_set(&esw->offloads.num_flows, 0);\n\tida_init(&esw->offloads.vport_metadata_ida);\n\txa_init_flags(&esw->offloads.vhca_map, XA_FLAGS_ALLOC);\n\tmutex_init(&esw->state_lock);\n\tinit_rwsem(&esw->mode_lock);\n\trefcount_set(&esw->qos.refcnt, 0);\n\n\tesw->enabled_vports = 0;\n\tesw->mode = MLX5_ESWITCH_LEGACY;\n\tesw->offloads.inline_mode = MLX5_INLINE_MODE_NONE;\n\tif (MLX5_CAP_ESW_FLOWTABLE_FDB(dev, reformat) &&\n\t    MLX5_CAP_ESW_FLOWTABLE_FDB(dev, decap))\n\t\tesw->offloads.encap = DEVLINK_ESWITCH_ENCAP_MODE_BASIC;\n\telse\n\t\tesw->offloads.encap = DEVLINK_ESWITCH_ENCAP_MODE_NONE;\n\tif (MLX5_ESWITCH_MANAGER(dev) &&\n\t    mlx5_esw_vport_match_metadata_supported(esw))\n\t\tesw->flags |= MLX5_ESWITCH_VPORT_MATCH_METADATA;\n\n\tdev->priv.eswitch = esw;\n\tBLOCKING_INIT_NOTIFIER_HEAD(&esw->n_head);\n\n\tesw_info(dev,\n\t\t \"Total vports %d, per vport: max uc(%d) max mc(%d)\\n\",\n\t\t esw->total_vports,\n\t\t MLX5_MAX_UC_PER_VPORT(dev),\n\t\t MLX5_MAX_MC_PER_VPORT(dev));\n\treturn 0;\n\nreps_err:\n\tmlx5_esw_vports_cleanup(esw);\nabort:\n\tif (esw->work_queue)\n\t\tdestroy_workqueue(esw->work_queue);\n\tdebugfs_remove_recursive(esw->debugfs_root);\n\tdevl_params_unregister(priv_to_devlink(dev), mlx5_eswitch_params,\n\t\t\t       ARRAY_SIZE(mlx5_eswitch_params));\nfree_esw:\n\tkfree(esw);\n\treturn err;\n}\n\nvoid mlx5_eswitch_cleanup(struct mlx5_eswitch *esw)\n{\n\tif (!esw)\n\t\treturn;\n\n\tesw_info(esw->dev, \"cleanup\\n\");\n\n\tesw->dev->priv.eswitch = NULL;\n\tdestroy_workqueue(esw->work_queue);\n\tWARN_ON(refcount_read(&esw->qos.refcnt));\n\tmutex_destroy(&esw->state_lock);\n\tWARN_ON(!xa_empty(&esw->offloads.vhca_map));\n\txa_destroy(&esw->offloads.vhca_map);\n\tida_destroy(&esw->offloads.vport_metadata_ida);\n\tmlx5e_mod_hdr_tbl_destroy(&esw->offloads.mod_hdr);\n\tmutex_destroy(&esw->offloads.encap_tbl_lock);\n\tmutex_destroy(&esw->offloads.decap_tbl_lock);\n\tesw_offloads_cleanup(esw);\n\tmlx5_esw_vports_cleanup(esw);\n\tdebugfs_remove_recursive(esw->debugfs_root);\n\tdevl_params_unregister(priv_to_devlink(esw->dev), mlx5_eswitch_params,\n\t\t\t       ARRAY_SIZE(mlx5_eswitch_params));\n\tkfree(esw);\n}\n\n \nstatic int\nmlx5_esw_set_vport_mac_locked(struct mlx5_eswitch *esw,\n\t\t\t      struct mlx5_vport *evport, const u8 *mac)\n{\n\tu16 vport_num = evport->vport;\n\tu64 node_guid;\n\tint err = 0;\n\n\tif (is_multicast_ether_addr(mac))\n\t\treturn -EINVAL;\n\n\tif (evport->info.spoofchk && !is_valid_ether_addr(mac))\n\t\tmlx5_core_warn(esw->dev,\n\t\t\t       \"Set invalid MAC while spoofchk is on, vport(%d)\\n\",\n\t\t\t       vport_num);\n\n\terr = mlx5_modify_nic_vport_mac_address(esw->dev, vport_num, mac);\n\tif (err) {\n\t\tmlx5_core_warn(esw->dev,\n\t\t\t       \"Failed to mlx5_modify_nic_vport_mac vport(%d) err=(%d)\\n\",\n\t\t\t       vport_num, err);\n\t\treturn err;\n\t}\n\n\tnode_guid_gen_from_mac(&node_guid, mac);\n\terr = mlx5_modify_nic_vport_node_guid(esw->dev, vport_num, node_guid);\n\tif (err)\n\t\tmlx5_core_warn(esw->dev,\n\t\t\t       \"Failed to set vport %d node guid, err = %d. RDMA_CM will not function properly for this VF.\\n\",\n\t\t\t       vport_num, err);\n\n\tether_addr_copy(evport->info.mac, mac);\n\tevport->info.node_guid = node_guid;\n\tif (evport->enabled && esw->mode == MLX5_ESWITCH_LEGACY)\n\t\terr = esw_acl_ingress_lgcy_setup(esw, evport);\n\n\treturn err;\n}\n\nint mlx5_eswitch_set_vport_mac(struct mlx5_eswitch *esw,\n\t\t\t       u16 vport, const u8 *mac)\n{\n\tstruct mlx5_vport *evport = mlx5_eswitch_get_vport(esw, vport);\n\tint err = 0;\n\n\tif (IS_ERR(evport))\n\t\treturn PTR_ERR(evport);\n\n\tmutex_lock(&esw->state_lock);\n\terr = mlx5_esw_set_vport_mac_locked(esw, evport, mac);\n\tmutex_unlock(&esw->state_lock);\n\treturn err;\n}\n\nstatic bool mlx5_esw_check_port_type(struct mlx5_eswitch *esw, u16 vport_num, xa_mark_t mark)\n{\n\treturn xa_get_mark(&esw->vports, vport_num, mark);\n}\n\nbool mlx5_eswitch_is_vf_vport(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\treturn mlx5_esw_check_port_type(esw, vport_num, MLX5_ESW_VPT_VF);\n}\n\nbool mlx5_eswitch_is_pf_vf_vport(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\treturn vport_num == MLX5_VPORT_PF ||\n\t\tmlx5_eswitch_is_vf_vport(esw, vport_num);\n}\n\nbool mlx5_esw_is_sf_vport(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\treturn mlx5_esw_check_port_type(esw, vport_num, MLX5_ESW_VPT_SF);\n}\n\nint mlx5_eswitch_set_vport_state(struct mlx5_eswitch *esw,\n\t\t\t\t u16 vport, int link_state)\n{\n\tstruct mlx5_vport *evport = mlx5_eswitch_get_vport(esw, vport);\n\tint opmod = MLX5_VPORT_STATE_OP_MOD_ESW_VPORT;\n\tint other_vport = 1;\n\tint err = 0;\n\n\tif (!mlx5_esw_allowed(esw))\n\t\treturn -EPERM;\n\tif (IS_ERR(evport))\n\t\treturn PTR_ERR(evport);\n\n\tif (vport == MLX5_VPORT_UPLINK) {\n\t\topmod = MLX5_VPORT_STATE_OP_MOD_UPLINK;\n\t\tother_vport = 0;\n\t\tvport = 0;\n\t}\n\tmutex_lock(&esw->state_lock);\n\tif (esw->mode != MLX5_ESWITCH_LEGACY) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto unlock;\n\t}\n\n\terr = mlx5_modify_vport_admin_state(esw->dev, opmod, vport, other_vport, link_state);\n\tif (err) {\n\t\tmlx5_core_warn(esw->dev, \"Failed to set vport %d link state, opmod = %d, err = %d\",\n\t\t\t       vport, opmod, err);\n\t\tgoto unlock;\n\t}\n\n\tevport->info.link_state = link_state;\n\nunlock:\n\tmutex_unlock(&esw->state_lock);\n\treturn err;\n}\n\nint mlx5_eswitch_get_vport_config(struct mlx5_eswitch *esw,\n\t\t\t\t  u16 vport, struct ifla_vf_info *ivi)\n{\n\tstruct mlx5_vport *evport = mlx5_eswitch_get_vport(esw, vport);\n\n\tif (IS_ERR(evport))\n\t\treturn PTR_ERR(evport);\n\n\tmemset(ivi, 0, sizeof(*ivi));\n\tivi->vf = vport - 1;\n\n\tmutex_lock(&esw->state_lock);\n\tether_addr_copy(ivi->mac, evport->info.mac);\n\tivi->linkstate = evport->info.link_state;\n\tivi->vlan = evport->info.vlan;\n\tivi->qos = evport->info.qos;\n\tivi->spoofchk = evport->info.spoofchk;\n\tivi->trusted = evport->info.trusted;\n\tif (evport->qos.enabled) {\n\t\tivi->min_tx_rate = evport->qos.min_rate;\n\t\tivi->max_tx_rate = evport->qos.max_rate;\n\t}\n\tmutex_unlock(&esw->state_lock);\n\n\treturn 0;\n}\n\nint __mlx5_eswitch_set_vport_vlan(struct mlx5_eswitch *esw,\n\t\t\t\t  u16 vport, u16 vlan, u8 qos, u8 set_flags)\n{\n\tstruct mlx5_vport *evport = mlx5_eswitch_get_vport(esw, vport);\n\tbool vst_mode_steering = esw_vst_mode_is_steering(esw);\n\tint err = 0;\n\n\tif (IS_ERR(evport))\n\t\treturn PTR_ERR(evport);\n\tif (vlan > 4095 || qos > 7)\n\t\treturn -EINVAL;\n\n\tif (esw->mode == MLX5_ESWITCH_OFFLOADS || !vst_mode_steering) {\n\t\terr = modify_esw_vport_cvlan(esw->dev, vport, vlan, qos, set_flags);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tevport->info.vlan = vlan;\n\tevport->info.qos = qos;\n\tif (evport->enabled && esw->mode == MLX5_ESWITCH_LEGACY) {\n\t\terr = esw_acl_ingress_lgcy_setup(esw, evport);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = esw_acl_egress_lgcy_setup(esw, evport);\n\t}\n\n\treturn err;\n}\n\nint mlx5_eswitch_get_vport_stats(struct mlx5_eswitch *esw,\n\t\t\t\t u16 vport_num,\n\t\t\t\t struct ifla_vf_stats *vf_stats)\n{\n\tstruct mlx5_vport *vport = mlx5_eswitch_get_vport(esw, vport_num);\n\tint outlen = MLX5_ST_SZ_BYTES(query_vport_counter_out);\n\tu32 in[MLX5_ST_SZ_DW(query_vport_counter_in)] = {};\n\tstruct mlx5_vport_drop_stats stats = {};\n\tint err = 0;\n\tu32 *out;\n\n\tif (IS_ERR(vport))\n\t\treturn PTR_ERR(vport);\n\n\tout = kvzalloc(outlen, GFP_KERNEL);\n\tif (!out)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(query_vport_counter_in, in, opcode,\n\t\t MLX5_CMD_OP_QUERY_VPORT_COUNTER);\n\tMLX5_SET(query_vport_counter_in, in, op_mod, 0);\n\tMLX5_SET(query_vport_counter_in, in, vport_number, vport->vport);\n\tMLX5_SET(query_vport_counter_in, in, other_vport, 1);\n\n\terr = mlx5_cmd_exec_inout(esw->dev, query_vport_counter, in, out);\n\tif (err)\n\t\tgoto free_out;\n\n\t#define MLX5_GET_CTR(p, x) \\\n\t\tMLX5_GET64(query_vport_counter_out, p, x)\n\n\tmemset(vf_stats, 0, sizeof(*vf_stats));\n\tvf_stats->rx_packets =\n\t\tMLX5_GET_CTR(out, received_eth_unicast.packets) +\n\t\tMLX5_GET_CTR(out, received_ib_unicast.packets) +\n\t\tMLX5_GET_CTR(out, received_eth_multicast.packets) +\n\t\tMLX5_GET_CTR(out, received_ib_multicast.packets) +\n\t\tMLX5_GET_CTR(out, received_eth_broadcast.packets);\n\n\tvf_stats->rx_bytes =\n\t\tMLX5_GET_CTR(out, received_eth_unicast.octets) +\n\t\tMLX5_GET_CTR(out, received_ib_unicast.octets) +\n\t\tMLX5_GET_CTR(out, received_eth_multicast.octets) +\n\t\tMLX5_GET_CTR(out, received_ib_multicast.octets) +\n\t\tMLX5_GET_CTR(out, received_eth_broadcast.octets);\n\n\tvf_stats->tx_packets =\n\t\tMLX5_GET_CTR(out, transmitted_eth_unicast.packets) +\n\t\tMLX5_GET_CTR(out, transmitted_ib_unicast.packets) +\n\t\tMLX5_GET_CTR(out, transmitted_eth_multicast.packets) +\n\t\tMLX5_GET_CTR(out, transmitted_ib_multicast.packets) +\n\t\tMLX5_GET_CTR(out, transmitted_eth_broadcast.packets);\n\n\tvf_stats->tx_bytes =\n\t\tMLX5_GET_CTR(out, transmitted_eth_unicast.octets) +\n\t\tMLX5_GET_CTR(out, transmitted_ib_unicast.octets) +\n\t\tMLX5_GET_CTR(out, transmitted_eth_multicast.octets) +\n\t\tMLX5_GET_CTR(out, transmitted_ib_multicast.octets) +\n\t\tMLX5_GET_CTR(out, transmitted_eth_broadcast.octets);\n\n\tvf_stats->multicast =\n\t\tMLX5_GET_CTR(out, received_eth_multicast.packets) +\n\t\tMLX5_GET_CTR(out, received_ib_multicast.packets);\n\n\tvf_stats->broadcast =\n\t\tMLX5_GET_CTR(out, received_eth_broadcast.packets);\n\n\terr = mlx5_esw_query_vport_drop_stats(esw->dev, vport, &stats);\n\tif (err)\n\t\tgoto free_out;\n\tvf_stats->rx_dropped = stats.rx_dropped;\n\tvf_stats->tx_dropped = stats.tx_dropped;\n\nfree_out:\n\tkvfree(out);\n\treturn err;\n}\n\nu8 mlx5_eswitch_mode(const struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw = dev->priv.eswitch;\n\n\treturn mlx5_esw_allowed(esw) ? esw->mode : MLX5_ESWITCH_LEGACY;\n}\nEXPORT_SYMBOL_GPL(mlx5_eswitch_mode);\n\nenum devlink_eswitch_encap_mode\nmlx5_eswitch_get_encap_mode(const struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw;\n\n\tesw = dev->priv.eswitch;\n\treturn (mlx5_eswitch_mode(dev) == MLX5_ESWITCH_OFFLOADS)  ? esw->offloads.encap :\n\t\tDEVLINK_ESWITCH_ENCAP_MODE_NONE;\n}\nEXPORT_SYMBOL(mlx5_eswitch_get_encap_mode);\n\nbool mlx5_esw_multipath_prereq(struct mlx5_core_dev *dev0,\n\t\t\t       struct mlx5_core_dev *dev1)\n{\n\treturn (dev0->priv.eswitch->mode == MLX5_ESWITCH_OFFLOADS &&\n\t\tdev1->priv.eswitch->mode == MLX5_ESWITCH_OFFLOADS);\n}\n\nint mlx5_esw_event_notifier_register(struct mlx5_eswitch *esw, struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&esw->n_head, nb);\n}\n\nvoid mlx5_esw_event_notifier_unregister(struct mlx5_eswitch *esw, struct notifier_block *nb)\n{\n\tblocking_notifier_chain_unregister(&esw->n_head, nb);\n}\n\n \nbool mlx5_esw_hold(struct mlx5_core_dev *mdev)\n{\n\tstruct mlx5_eswitch *esw = mdev->priv.eswitch;\n\n\t \n\tif (!mlx5_esw_allowed(esw))\n\t\treturn true;\n\n\tif (down_read_trylock(&esw->mode_lock) != 0) {\n\t\tif (esw->eswitch_operation_in_progress) {\n\t\t\tup_read(&esw->mode_lock);\n\t\t\treturn false;\n\t\t}\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nvoid mlx5_esw_release(struct mlx5_core_dev *mdev)\n{\n\tstruct mlx5_eswitch *esw = mdev->priv.eswitch;\n\n\tif (mlx5_esw_allowed(esw))\n\t\tup_read(&esw->mode_lock);\n}\n\n \nvoid mlx5_esw_get(struct mlx5_core_dev *mdev)\n{\n\tstruct mlx5_eswitch *esw = mdev->priv.eswitch;\n\n\tif (mlx5_esw_allowed(esw))\n\t\tatomic64_inc(&esw->user_count);\n}\n\n \nvoid mlx5_esw_put(struct mlx5_core_dev *mdev)\n{\n\tstruct mlx5_eswitch *esw = mdev->priv.eswitch;\n\n\tif (mlx5_esw_allowed(esw))\n\t\tatomic64_dec_if_positive(&esw->user_count);\n}\n\n \nint mlx5_esw_try_lock(struct mlx5_eswitch *esw)\n{\n\tif (down_write_trylock(&esw->mode_lock) == 0)\n\t\treturn -EINVAL;\n\n\tif (esw->eswitch_operation_in_progress ||\n\t    atomic64_read(&esw->user_count) > 0) {\n\t\tup_write(&esw->mode_lock);\n\t\treturn -EBUSY;\n\t}\n\n\treturn esw->mode;\n}\n\nint mlx5_esw_lock(struct mlx5_eswitch *esw)\n{\n\tdown_write(&esw->mode_lock);\n\n\tif (esw->eswitch_operation_in_progress) {\n\t\tup_write(&esw->mode_lock);\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\n \nvoid mlx5_esw_unlock(struct mlx5_eswitch *esw)\n{\n\tup_write(&esw->mode_lock);\n}\n\n \nu16 mlx5_eswitch_get_total_vports(const struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw;\n\n\tesw = dev->priv.eswitch;\n\treturn mlx5_esw_allowed(esw) ? esw->total_vports : 0;\n}\nEXPORT_SYMBOL_GPL(mlx5_eswitch_get_total_vports);\n\n \nstruct mlx5_core_dev *mlx5_eswitch_get_core_dev(struct mlx5_eswitch *esw)\n{\n\treturn mlx5_esw_allowed(esw) ? esw->dev : NULL;\n}\nEXPORT_SYMBOL(mlx5_eswitch_get_core_dev);\n\nbool mlx5_eswitch_block_ipsec(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw = dev->priv.eswitch;\n\n\tif (!mlx5_esw_allowed(esw))\n\t\treturn true;\n\n\tmutex_lock(&esw->state_lock);\n\tif (esw->enabled_ipsec_vf_count) {\n\t\tmutex_unlock(&esw->state_lock);\n\t\treturn false;\n\t}\n\n\tdev->num_ipsec_offloads++;\n\tmutex_unlock(&esw->state_lock);\n\treturn true;\n}\n\nvoid mlx5_eswitch_unblock_ipsec(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw = dev->priv.eswitch;\n\n\tif (!mlx5_esw_allowed(esw))\n\t\t \n\t\treturn;\n\n\tmutex_lock(&esw->state_lock);\n\tdev->num_ipsec_offloads--;\n\tmutex_unlock(&esw->state_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}