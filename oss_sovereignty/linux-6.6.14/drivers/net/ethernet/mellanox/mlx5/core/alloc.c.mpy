{
  "module_name": "alloc.c",
  "hash_id": "c2e02e209a97c70be0596a10c57a651803c02e0e601bc77bb9acaf0b04f25666",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/alloc.c",
  "human_readable_source": " \n\n#include <linux/errno.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n#include <linux/bitmap.h>\n#include <linux/dma-mapping.h>\n#include <linux/vmalloc.h>\n#include <linux/mlx5/driver.h>\n\n#include \"mlx5_core.h\"\n\nstruct mlx5_db_pgdir {\n\tstruct list_head\tlist;\n\tunsigned long\t       *bitmap;\n\t__be32\t\t       *db_page;\n\tdma_addr_t\t\tdb_dma;\n};\n\n \n\nstatic void *mlx5_dma_zalloc_coherent_node(struct mlx5_core_dev *dev,\n\t\t\t\t\t   size_t size, dma_addr_t *dma_handle,\n\t\t\t\t\t   int node)\n{\n\tstruct device *device = mlx5_core_dma_dev(dev);\n\tstruct mlx5_priv *priv = &dev->priv;\n\tint original_node;\n\tvoid *cpu_handle;\n\n\tmutex_lock(&priv->alloc_mutex);\n\toriginal_node = dev_to_node(device);\n\tset_dev_node(device, node);\n\tcpu_handle = dma_alloc_coherent(device, size, dma_handle,\n\t\t\t\t\tGFP_KERNEL);\n\tset_dev_node(device, original_node);\n\tmutex_unlock(&priv->alloc_mutex);\n\treturn cpu_handle;\n}\n\nint mlx5_frag_buf_alloc_node(struct mlx5_core_dev *dev, int size,\n\t\t\t     struct mlx5_frag_buf *buf, int node)\n{\n\tint i;\n\n\tbuf->size = size;\n\tbuf->npages = DIV_ROUND_UP(size, PAGE_SIZE);\n\tbuf->page_shift = PAGE_SHIFT;\n\tbuf->frags = kcalloc(buf->npages, sizeof(struct mlx5_buf_list),\n\t\t\t     GFP_KERNEL);\n\tif (!buf->frags)\n\t\tgoto err_out;\n\n\tfor (i = 0; i < buf->npages; i++) {\n\t\tstruct mlx5_buf_list *frag = &buf->frags[i];\n\t\tint frag_sz = min_t(int, size, PAGE_SIZE);\n\n\t\tfrag->buf = mlx5_dma_zalloc_coherent_node(dev, frag_sz,\n\t\t\t\t\t\t\t  &frag->map, node);\n\t\tif (!frag->buf)\n\t\t\tgoto err_free_buf;\n\t\tif (frag->map & ((1 << buf->page_shift) - 1)) {\n\t\t\tdma_free_coherent(mlx5_core_dma_dev(dev), frag_sz,\n\t\t\t\t\t  buf->frags[i].buf, buf->frags[i].map);\n\t\t\tmlx5_core_warn(dev, \"unexpected map alignment: %pad, page_shift=%d\\n\",\n\t\t\t\t       &frag->map, buf->page_shift);\n\t\t\tgoto err_free_buf;\n\t\t}\n\t\tsize -= frag_sz;\n\t}\n\n\treturn 0;\n\nerr_free_buf:\n\twhile (i--)\n\t\tdma_free_coherent(mlx5_core_dma_dev(dev), PAGE_SIZE, buf->frags[i].buf,\n\t\t\t\t  buf->frags[i].map);\n\tkfree(buf->frags);\nerr_out:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(mlx5_frag_buf_alloc_node);\n\nvoid mlx5_frag_buf_free(struct mlx5_core_dev *dev, struct mlx5_frag_buf *buf)\n{\n\tint size = buf->size;\n\tint i;\n\n\tfor (i = 0; i < buf->npages; i++) {\n\t\tint frag_sz = min_t(int, size, PAGE_SIZE);\n\n\t\tdma_free_coherent(mlx5_core_dma_dev(dev), frag_sz, buf->frags[i].buf,\n\t\t\t\t  buf->frags[i].map);\n\t\tsize -= frag_sz;\n\t}\n\tkfree(buf->frags);\n}\nEXPORT_SYMBOL_GPL(mlx5_frag_buf_free);\n\nstatic struct mlx5_db_pgdir *mlx5_alloc_db_pgdir(struct mlx5_core_dev *dev,\n\t\t\t\t\t\t int node)\n{\n\tu32 db_per_page = PAGE_SIZE / cache_line_size();\n\tstruct mlx5_db_pgdir *pgdir;\n\n\tpgdir = kzalloc_node(sizeof(*pgdir), GFP_KERNEL, node);\n\tif (!pgdir)\n\t\treturn NULL;\n\n\tpgdir->bitmap = bitmap_zalloc_node(db_per_page, GFP_KERNEL, node);\n\tif (!pgdir->bitmap) {\n\t\tkfree(pgdir);\n\t\treturn NULL;\n\t}\n\n\tbitmap_fill(pgdir->bitmap, db_per_page);\n\n\tpgdir->db_page = mlx5_dma_zalloc_coherent_node(dev, PAGE_SIZE,\n\t\t\t\t\t\t       &pgdir->db_dma, node);\n\tif (!pgdir->db_page) {\n\t\tbitmap_free(pgdir->bitmap);\n\t\tkfree(pgdir);\n\t\treturn NULL;\n\t}\n\n\treturn pgdir;\n}\n\nstatic int mlx5_alloc_db_from_pgdir(struct mlx5_db_pgdir *pgdir,\n\t\t\t\t    struct mlx5_db *db)\n{\n\tu32 db_per_page = PAGE_SIZE / cache_line_size();\n\tint offset;\n\tint i;\n\n\ti = find_first_bit(pgdir->bitmap, db_per_page);\n\tif (i >= db_per_page)\n\t\treturn -ENOMEM;\n\n\t__clear_bit(i, pgdir->bitmap);\n\n\tdb->u.pgdir = pgdir;\n\tdb->index   = i;\n\toffset = db->index * cache_line_size();\n\tdb->db      = pgdir->db_page + offset / sizeof(*pgdir->db_page);\n\tdb->dma     = pgdir->db_dma  + offset;\n\n\tdb->db[0] = 0;\n\tdb->db[1] = 0;\n\n\treturn 0;\n}\n\nint mlx5_db_alloc_node(struct mlx5_core_dev *dev, struct mlx5_db *db, int node)\n{\n\tstruct mlx5_db_pgdir *pgdir;\n\tint ret = 0;\n\n\tmutex_lock(&dev->priv.pgdir_mutex);\n\n\tlist_for_each_entry(pgdir, &dev->priv.pgdir_list, list)\n\t\tif (!mlx5_alloc_db_from_pgdir(pgdir, db))\n\t\t\tgoto out;\n\n\tpgdir = mlx5_alloc_db_pgdir(dev, node);\n\tif (!pgdir) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlist_add(&pgdir->list, &dev->priv.pgdir_list);\n\n\t \n\tWARN_ON(mlx5_alloc_db_from_pgdir(pgdir, db));\n\nout:\n\tmutex_unlock(&dev->priv.pgdir_mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mlx5_db_alloc_node);\n\nvoid mlx5_db_free(struct mlx5_core_dev *dev, struct mlx5_db *db)\n{\n\tu32 db_per_page = PAGE_SIZE / cache_line_size();\n\n\tmutex_lock(&dev->priv.pgdir_mutex);\n\n\t__set_bit(db->index, db->u.pgdir->bitmap);\n\n\tif (bitmap_full(db->u.pgdir->bitmap, db_per_page)) {\n\t\tdma_free_coherent(mlx5_core_dma_dev(dev), PAGE_SIZE,\n\t\t\t\t  db->u.pgdir->db_page, db->u.pgdir->db_dma);\n\t\tlist_del(&db->u.pgdir->list);\n\t\tbitmap_free(db->u.pgdir->bitmap);\n\t\tkfree(db->u.pgdir);\n\t}\n\n\tmutex_unlock(&dev->priv.pgdir_mutex);\n}\nEXPORT_SYMBOL_GPL(mlx5_db_free);\n\nvoid mlx5_fill_page_frag_array_perm(struct mlx5_frag_buf *buf, __be64 *pas, u8 perm)\n{\n\tint i;\n\n\tWARN_ON(perm & 0xfc);\n\tfor (i = 0; i < buf->npages; i++)\n\t\tpas[i] = cpu_to_be64(buf->frags[i].map | perm);\n}\nEXPORT_SYMBOL_GPL(mlx5_fill_page_frag_array_perm);\n\nvoid mlx5_fill_page_frag_array(struct mlx5_frag_buf *buf, __be64 *pas)\n{\n\tmlx5_fill_page_frag_array_perm(buf, pas, 0);\n}\nEXPORT_SYMBOL_GPL(mlx5_fill_page_frag_array);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}