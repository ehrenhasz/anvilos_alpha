{
  "module_name": "eswitch_offloads.c",
  "hash_id": "ba2db8290f4bd200faed3774179a6df5ea2d9c5c2ed1eb0ee7997ccb0aaa8f23",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c",
  "human_readable_source": " \n\n#include <linux/etherdevice.h>\n#include <linux/idr.h>\n#include <linux/mlx5/driver.h>\n#include <linux/mlx5/mlx5_ifc.h>\n#include <linux/mlx5/vport.h>\n#include <linux/mlx5/fs.h>\n#include \"mlx5_core.h\"\n#include \"eswitch.h\"\n#include \"esw/indir_table.h\"\n#include \"esw/acl/ofld.h\"\n#include \"rdma.h\"\n#include \"en.h\"\n#include \"fs_core.h\"\n#include \"lib/devcom.h\"\n#include \"lib/eq.h\"\n#include \"lib/fs_chains.h\"\n#include \"en_tc.h\"\n#include \"en/mapping.h\"\n#include \"devlink.h\"\n#include \"lag/lag.h\"\n#include \"en/tc/post_meter.h\"\n\n#define mlx5_esw_for_each_rep(esw, i, rep) \\\n\txa_for_each(&((esw)->offloads.vport_reps), i, rep)\n\n \n#define MLX5_ESW_MISS_FLOWS (2)\n#define UPLINK_REP_INDEX 0\n\n#define MLX5_ESW_VPORT_TBL_SIZE 128\n#define MLX5_ESW_VPORT_TBL_NUM_GROUPS  4\n\n#define MLX5_ESW_FT_OFFLOADS_DROP_RULE (1)\n\nstatic struct esw_vport_tbl_namespace mlx5_esw_vport_tbl_mirror_ns = {\n\t.max_fte = MLX5_ESW_VPORT_TBL_SIZE,\n\t.max_num_groups = MLX5_ESW_VPORT_TBL_NUM_GROUPS,\n\t.flags = 0,\n};\n\nstatic struct mlx5_eswitch_rep *mlx5_eswitch_get_rep(struct mlx5_eswitch *esw,\n\t\t\t\t\t\t     u16 vport_num)\n{\n\treturn xa_load(&esw->offloads.vport_reps, vport_num);\n}\n\nstatic void\nmlx5_eswitch_set_rule_flow_source(struct mlx5_eswitch *esw,\n\t\t\t\t  struct mlx5_flow_spec *spec,\n\t\t\t\t  struct mlx5_esw_flow_attr *attr)\n{\n\tif (!MLX5_CAP_ESW_FLOWTABLE(esw->dev, flow_source) || !attr || !attr->in_rep)\n\t\treturn;\n\n\tif (attr->int_port) {\n\t\tspec->flow_context.flow_source = mlx5e_tc_int_port_get_flow_source(attr->int_port);\n\n\t\treturn;\n\t}\n\n\tspec->flow_context.flow_source = (attr->in_rep->vport == MLX5_VPORT_UPLINK) ?\n\t\t\t\t\t MLX5_FLOW_CONTEXT_FLOW_SOURCE_UPLINK :\n\t\t\t\t\t MLX5_FLOW_CONTEXT_FLOW_SOURCE_LOCAL_VPORT;\n}\n\n \nvoid\nmlx5_eswitch_clear_rule_source_port(struct mlx5_eswitch *esw, struct mlx5_flow_spec *spec)\n{\n\tif (mlx5_eswitch_vport_match_metadata_enabled(esw)) {\n\t\tvoid *misc2;\n\n\t\tmisc2 = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc2, metadata_reg_c_0, 0);\n\n\t\tmisc2 = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc2, metadata_reg_c_0, 0);\n\n\t\tif (!memchr_inv(misc2, 0, MLX5_ST_SZ_BYTES(fte_match_set_misc2)))\n\t\t\tspec->match_criteria_enable &= ~MLX5_MATCH_MISC_PARAMETERS_2;\n\t}\n}\n\nstatic void\nmlx5_eswitch_set_rule_source_port(struct mlx5_eswitch *esw,\n\t\t\t\t  struct mlx5_flow_spec *spec,\n\t\t\t\t  struct mlx5_flow_attr *attr,\n\t\t\t\t  struct mlx5_eswitch *src_esw,\n\t\t\t\t  u16 vport)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tu32 metadata;\n\tvoid *misc2;\n\tvoid *misc;\n\n\t \n\tif (mlx5_eswitch_vport_match_metadata_enabled(esw)) {\n\t\tif (mlx5_esw_indir_table_decap_vport(attr))\n\t\t\tvport = mlx5_esw_indir_table_decap_vport(attr);\n\n\t\tif (!attr->chain && esw_attr && esw_attr->int_port)\n\t\t\tmetadata =\n\t\t\t\tmlx5e_tc_int_port_get_metadata_for_match(esw_attr->int_port);\n\t\telse\n\t\t\tmetadata =\n\t\t\t\tmlx5_eswitch_get_vport_metadata_for_match(src_esw, vport);\n\n\t\tmisc2 = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc2, metadata_reg_c_0, metadata);\n\n\t\tmisc2 = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc2, metadata_reg_c_0,\n\t\t\t mlx5_eswitch_get_vport_metadata_mask());\n\n\t\tspec->match_criteria_enable |= MLX5_MATCH_MISC_PARAMETERS_2;\n\t} else {\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters);\n\t\tMLX5_SET(fte_match_set_misc, misc, source_port, vport);\n\n\t\tif (MLX5_CAP_ESW(esw->dev, merged_eswitch))\n\t\t\tMLX5_SET(fte_match_set_misc, misc,\n\t\t\t\t source_eswitch_owner_vhca_id,\n\t\t\t\t MLX5_CAP_GEN(src_esw->dev, vhca_id));\n\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters);\n\t\tMLX5_SET_TO_ONES(fte_match_set_misc, misc, source_port);\n\t\tif (MLX5_CAP_ESW(esw->dev, merged_eswitch))\n\t\t\tMLX5_SET_TO_ONES(fte_match_set_misc, misc,\n\t\t\t\t\t source_eswitch_owner_vhca_id);\n\n\t\tspec->match_criteria_enable |= MLX5_MATCH_MISC_PARAMETERS;\n\t}\n}\n\nstatic int\nesw_setup_decap_indir(struct mlx5_eswitch *esw,\n\t\t      struct mlx5_flow_attr *attr)\n{\n\tstruct mlx5_flow_table *ft;\n\n\tif (!(attr->flags & MLX5_ATTR_FLAG_SRC_REWRITE))\n\t\treturn -EOPNOTSUPP;\n\n\tft = mlx5_esw_indir_table_get(esw, attr,\n\t\t\t\t      mlx5_esw_indir_table_decap_vport(attr), true);\n\treturn PTR_ERR_OR_ZERO(ft);\n}\n\nstatic void\nesw_cleanup_decap_indir(struct mlx5_eswitch *esw,\n\t\t\tstruct mlx5_flow_attr *attr)\n{\n\tif (mlx5_esw_indir_table_decap_vport(attr))\n\t\tmlx5_esw_indir_table_put(esw,\n\t\t\t\t\t mlx5_esw_indir_table_decap_vport(attr),\n\t\t\t\t\t true);\n}\n\nstatic int\nesw_setup_mtu_dest(struct mlx5_flow_destination *dest,\n\t\t   struct mlx5e_meter_attr *meter,\n\t\t   int i)\n{\n\tdest[i].type = MLX5_FLOW_DESTINATION_TYPE_RANGE;\n\tdest[i].range.field = MLX5_FLOW_DEST_RANGE_FIELD_PKT_LEN;\n\tdest[i].range.min = 0;\n\tdest[i].range.max = meter->params.mtu;\n\tdest[i].range.hit_ft = mlx5e_post_meter_get_mtu_true_ft(meter->post_meter);\n\tdest[i].range.miss_ft = mlx5e_post_meter_get_mtu_false_ft(meter->post_meter);\n\n\treturn 0;\n}\n\nstatic int\nesw_setup_sampler_dest(struct mlx5_flow_destination *dest,\n\t\t       struct mlx5_flow_act *flow_act,\n\t\t       u32 sampler_id,\n\t\t       int i)\n{\n\tflow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;\n\tdest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_SAMPLER;\n\tdest[i].sampler_id = sampler_id;\n\n\treturn 0;\n}\n\nstatic int\nesw_setup_ft_dest(struct mlx5_flow_destination *dest,\n\t\t  struct mlx5_flow_act *flow_act,\n\t\t  struct mlx5_eswitch *esw,\n\t\t  struct mlx5_flow_attr *attr,\n\t\t  int i)\n{\n\tflow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;\n\tdest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\tdest[i].ft = attr->dest_ft;\n\n\tif (mlx5_esw_indir_table_decap_vport(attr))\n\t\treturn esw_setup_decap_indir(esw, attr);\n\treturn 0;\n}\n\nstatic void\nesw_setup_accept_dest(struct mlx5_flow_destination *dest, struct mlx5_flow_act *flow_act,\n\t\t      struct mlx5_fs_chains *chains, int i)\n{\n\tif (mlx5_chains_ignore_flow_level_supported(chains))\n\t\tflow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;\n\tdest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\tdest[i].ft = mlx5_chains_get_tc_end_ft(chains);\n}\n\nstatic void\nesw_setup_slow_path_dest(struct mlx5_flow_destination *dest, struct mlx5_flow_act *flow_act,\n\t\t\t struct mlx5_eswitch *esw, int i)\n{\n\tif (MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, ignore_flow_level))\n\t\tflow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;\n\tdest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\tdest[i].ft = mlx5_eswitch_get_slow_fdb(esw);\n}\n\nstatic int\nesw_setup_chain_dest(struct mlx5_flow_destination *dest,\n\t\t     struct mlx5_flow_act *flow_act,\n\t\t     struct mlx5_fs_chains *chains,\n\t\t     u32 chain, u32 prio, u32 level,\n\t\t     int i)\n{\n\tstruct mlx5_flow_table *ft;\n\n\tflow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;\n\tft = mlx5_chains_get_table(chains, chain, prio, level);\n\tif (IS_ERR(ft))\n\t\treturn PTR_ERR(ft);\n\n\tdest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\tdest[i].ft = ft;\n\treturn  0;\n}\n\nstatic void esw_put_dest_tables_loop(struct mlx5_eswitch *esw, struct mlx5_flow_attr *attr,\n\t\t\t\t     int from, int to)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tstruct mlx5_fs_chains *chains = esw_chains(esw);\n\tint i;\n\n\tfor (i = from; i < to; i++)\n\t\tif (esw_attr->dests[i].flags & MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE)\n\t\t\tmlx5_chains_put_table(chains, 0, 1, 0);\n\t\telse if (mlx5_esw_indir_table_needed(esw, attr, esw_attr->dests[i].vport,\n\t\t\t\t\t\t     esw_attr->dests[i].mdev))\n\t\t\tmlx5_esw_indir_table_put(esw, esw_attr->dests[i].vport, false);\n}\n\nstatic bool\nesw_is_chain_src_port_rewrite(struct mlx5_eswitch *esw, struct mlx5_esw_flow_attr *esw_attr)\n{\n\tint i;\n\n\tfor (i = esw_attr->split_count; i < esw_attr->out_count; i++)\n\t\tif (esw_attr->dests[i].flags & MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE)\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic int\nesw_setup_chain_src_port_rewrite(struct mlx5_flow_destination *dest,\n\t\t\t\t struct mlx5_flow_act *flow_act,\n\t\t\t\t struct mlx5_eswitch *esw,\n\t\t\t\t struct mlx5_fs_chains *chains,\n\t\t\t\t struct mlx5_flow_attr *attr,\n\t\t\t\t int *i)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tint err;\n\n\tif (!(attr->flags & MLX5_ATTR_FLAG_SRC_REWRITE))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (esw_attr->out_count - esw_attr->split_count > 1)\n\t\treturn -EOPNOTSUPP;\n\n\terr = esw_setup_chain_dest(dest, flow_act, chains, attr->dest_chain, 1, 0, *i);\n\tif (err)\n\t\treturn err;\n\n\tif (esw_attr->dests[esw_attr->split_count].pkt_reformat) {\n\t\tflow_act->action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;\n\t\tflow_act->pkt_reformat = esw_attr->dests[esw_attr->split_count].pkt_reformat;\n\t}\n\t(*i)++;\n\n\treturn 0;\n}\n\nstatic void esw_cleanup_chain_src_port_rewrite(struct mlx5_eswitch *esw,\n\t\t\t\t\t       struct mlx5_flow_attr *attr)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\n\tesw_put_dest_tables_loop(esw, attr, esw_attr->split_count, esw_attr->out_count);\n}\n\nstatic bool\nesw_is_indir_table(struct mlx5_eswitch *esw, struct mlx5_flow_attr *attr)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tbool result = false;\n\tint i;\n\n\t \n\tfor (i = esw_attr->split_count; i < esw_attr->out_count; i++) {\n\t\tif (esw_attr->dests[i].vport_valid &&\n\t\t    mlx5_esw_indir_table_needed(esw, attr, esw_attr->dests[i].vport,\n\t\t\t\t\t\tesw_attr->dests[i].mdev)) {\n\t\t\tresult = true;\n\t\t} else {\n\t\t\tresult = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn result;\n}\n\nstatic int\nesw_setup_indir_table(struct mlx5_flow_destination *dest,\n\t\t      struct mlx5_flow_act *flow_act,\n\t\t      struct mlx5_eswitch *esw,\n\t\t      struct mlx5_flow_attr *attr,\n\t\t      int *i)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tint j, err;\n\n\tif (!(attr->flags & MLX5_ATTR_FLAG_SRC_REWRITE))\n\t\treturn -EOPNOTSUPP;\n\n\tfor (j = esw_attr->split_count; j < esw_attr->out_count; j++, (*i)++) {\n\t\tflow_act->flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;\n\t\tdest[*i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\n\t\tdest[*i].ft = mlx5_esw_indir_table_get(esw, attr,\n\t\t\t\t\t\t       esw_attr->dests[j].vport, false);\n\t\tif (IS_ERR(dest[*i].ft)) {\n\t\t\terr = PTR_ERR(dest[*i].ft);\n\t\t\tgoto err_indir_tbl_get;\n\t\t}\n\t}\n\n\tif (mlx5_esw_indir_table_decap_vport(attr)) {\n\t\terr = esw_setup_decap_indir(esw, attr);\n\t\tif (err)\n\t\t\tgoto err_indir_tbl_get;\n\t}\n\n\treturn 0;\n\nerr_indir_tbl_get:\n\tesw_put_dest_tables_loop(esw, attr, esw_attr->split_count, j);\n\treturn err;\n}\n\nstatic void esw_cleanup_indir_table(struct mlx5_eswitch *esw, struct mlx5_flow_attr *attr)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\n\tesw_put_dest_tables_loop(esw, attr, esw_attr->split_count, esw_attr->out_count);\n\tesw_cleanup_decap_indir(esw, attr);\n}\n\nstatic void\nesw_cleanup_chain_dest(struct mlx5_fs_chains *chains, u32 chain, u32 prio, u32 level)\n{\n\tmlx5_chains_put_table(chains, chain, prio, level);\n}\n\nstatic bool esw_same_vhca_id(struct mlx5_core_dev *mdev1, struct mlx5_core_dev *mdev2)\n{\n\treturn MLX5_CAP_GEN(mdev1, vhca_id) == MLX5_CAP_GEN(mdev2, vhca_id);\n}\n\nstatic bool esw_setup_uplink_fwd_ipsec_needed(struct mlx5_eswitch *esw,\n\t\t\t\t\t      struct mlx5_esw_flow_attr *esw_attr,\n\t\t\t\t\t      int attr_idx)\n{\n\tif (esw->offloads.ft_ipsec_tx_pol &&\n\t    esw_attr->dests[attr_idx].vport_valid &&\n\t    esw_attr->dests[attr_idx].vport == MLX5_VPORT_UPLINK &&\n\t     \n\t    (esw_attr->dests[attr_idx].flags & MLX5_ESW_DEST_ENCAP_VALID) &&\n\t    esw_attr->dests[attr_idx].vport != esw_attr->in_rep->vport &&\n\t    esw_same_vhca_id(esw_attr->dests[attr_idx].mdev, esw->dev))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool esw_flow_dests_fwd_ipsec_check(struct mlx5_eswitch *esw,\n\t\t\t\t\t   struct mlx5_esw_flow_attr *esw_attr)\n{\n\tint i;\n\n\tif (!esw->offloads.ft_ipsec_tx_pol)\n\t\treturn true;\n\n\tfor (i = 0; i < esw_attr->split_count; i++)\n\t\tif (esw_setup_uplink_fwd_ipsec_needed(esw, esw_attr, i))\n\t\t\treturn false;\n\n\tfor (i = esw_attr->split_count; i < esw_attr->out_count; i++)\n\t\tif (esw_setup_uplink_fwd_ipsec_needed(esw, esw_attr, i) &&\n\t\t    (esw_attr->out_count - esw_attr->split_count > 1))\n\t\t\treturn false;\n\n\treturn true;\n}\n\nstatic void\nesw_setup_dest_fwd_vport(struct mlx5_flow_destination *dest, struct mlx5_flow_act *flow_act,\n\t\t\t struct mlx5_eswitch *esw, struct mlx5_esw_flow_attr *esw_attr,\n\t\t\t int attr_idx, int dest_idx, bool pkt_reformat)\n{\n\tdest[dest_idx].type = MLX5_FLOW_DESTINATION_TYPE_VPORT;\n\tdest[dest_idx].vport.num = esw_attr->dests[attr_idx].vport;\n\tif (MLX5_CAP_ESW(esw->dev, merged_eswitch)) {\n\t\tdest[dest_idx].vport.vhca_id =\n\t\t\tMLX5_CAP_GEN(esw_attr->dests[attr_idx].mdev, vhca_id);\n\t\tdest[dest_idx].vport.flags |= MLX5_FLOW_DEST_VPORT_VHCA_ID;\n\t\tif (dest[dest_idx].vport.num == MLX5_VPORT_UPLINK &&\n\t\t    mlx5_lag_is_mpesw(esw->dev))\n\t\t\tdest[dest_idx].type = MLX5_FLOW_DESTINATION_TYPE_UPLINK;\n\t}\n\tif (esw_attr->dests[attr_idx].flags & MLX5_ESW_DEST_ENCAP_VALID) {\n\t\tif (pkt_reformat) {\n\t\t\tflow_act->action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;\n\t\t\tflow_act->pkt_reformat = esw_attr->dests[attr_idx].pkt_reformat;\n\t\t}\n\t\tdest[dest_idx].vport.flags |= MLX5_FLOW_DEST_VPORT_REFORMAT_ID;\n\t\tdest[dest_idx].vport.pkt_reformat = esw_attr->dests[attr_idx].pkt_reformat;\n\t}\n}\n\nstatic void\nesw_setup_dest_fwd_ipsec(struct mlx5_flow_destination *dest, struct mlx5_flow_act *flow_act,\n\t\t\t struct mlx5_eswitch *esw, struct mlx5_esw_flow_attr *esw_attr,\n\t\t\t int attr_idx, int dest_idx, bool pkt_reformat)\n{\n\tdest[dest_idx].ft = esw->offloads.ft_ipsec_tx_pol;\n\tdest[dest_idx].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\tif (pkt_reformat &&\n\t    esw_attr->dests[attr_idx].flags & MLX5_ESW_DEST_ENCAP_VALID) {\n\t\tflow_act->action |= MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT;\n\t\tflow_act->pkt_reformat = esw_attr->dests[attr_idx].pkt_reformat;\n\t}\n}\n\nstatic void\nesw_setup_vport_dest(struct mlx5_flow_destination *dest, struct mlx5_flow_act *flow_act,\n\t\t     struct mlx5_eswitch *esw, struct mlx5_esw_flow_attr *esw_attr,\n\t\t     int attr_idx, int dest_idx, bool pkt_reformat)\n{\n\tif (esw_setup_uplink_fwd_ipsec_needed(esw, esw_attr, attr_idx))\n\t\tesw_setup_dest_fwd_ipsec(dest, flow_act, esw, esw_attr,\n\t\t\t\t\t attr_idx, dest_idx, pkt_reformat);\n\telse\n\t\tesw_setup_dest_fwd_vport(dest, flow_act, esw, esw_attr,\n\t\t\t\t\t attr_idx, dest_idx, pkt_reformat);\n}\n\nstatic int\nesw_setup_vport_dests(struct mlx5_flow_destination *dest, struct mlx5_flow_act *flow_act,\n\t\t      struct mlx5_eswitch *esw, struct mlx5_esw_flow_attr *esw_attr,\n\t\t      int i)\n{\n\tint j;\n\n\tfor (j = esw_attr->split_count; j < esw_attr->out_count; j++, i++)\n\t\tesw_setup_vport_dest(dest, flow_act, esw, esw_attr, j, i, true);\n\treturn i;\n}\n\nstatic bool\nesw_src_port_rewrite_supported(struct mlx5_eswitch *esw)\n{\n\treturn MLX5_CAP_GEN(esw->dev, reg_c_preserve) &&\n\t       mlx5_eswitch_vport_match_metadata_enabled(esw) &&\n\t       MLX5_CAP_ESW_FLOWTABLE_FDB(esw->dev, ignore_flow_level);\n}\n\nstatic bool\nesw_dests_to_vf_pf_vports(struct mlx5_flow_destination *dests, int max_dest)\n{\n\tbool vf_dest = false, pf_dest = false;\n\tint i;\n\n\tfor (i = 0; i < max_dest; i++) {\n\t\tif (dests[i].type != MLX5_FLOW_DESTINATION_TYPE_VPORT)\n\t\t\tcontinue;\n\n\t\tif (dests[i].vport.num == MLX5_VPORT_UPLINK)\n\t\t\tpf_dest = true;\n\t\telse\n\t\t\tvf_dest = true;\n\n\t\tif (vf_dest && pf_dest)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int\nesw_setup_dests(struct mlx5_flow_destination *dest,\n\t\tstruct mlx5_flow_act *flow_act,\n\t\tstruct mlx5_eswitch *esw,\n\t\tstruct mlx5_flow_attr *attr,\n\t\tstruct mlx5_flow_spec *spec,\n\t\tint *i)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tstruct mlx5_fs_chains *chains = esw_chains(esw);\n\tint err = 0;\n\n\tif (!mlx5_eswitch_termtbl_required(esw, attr, flow_act, spec) &&\n\t    esw_src_port_rewrite_supported(esw))\n\t\tattr->flags |= MLX5_ATTR_FLAG_SRC_REWRITE;\n\n\tif (attr->flags & MLX5_ATTR_FLAG_SLOW_PATH) {\n\t\tesw_setup_slow_path_dest(dest, flow_act, esw, *i);\n\t\t(*i)++;\n\t\tgoto out;\n\t}\n\n\tif (attr->flags & MLX5_ATTR_FLAG_SAMPLE) {\n\t\tesw_setup_sampler_dest(dest, flow_act, attr->sample_attr.sampler_id, *i);\n\t\t(*i)++;\n\t} else if (attr->flags & MLX5_ATTR_FLAG_ACCEPT) {\n\t\tesw_setup_accept_dest(dest, flow_act, chains, *i);\n\t\t(*i)++;\n\t} else if (attr->flags & MLX5_ATTR_FLAG_MTU) {\n\t\terr = esw_setup_mtu_dest(dest, &attr->meter_attr, *i);\n\t\t(*i)++;\n\t} else if (esw_is_indir_table(esw, attr)) {\n\t\terr = esw_setup_indir_table(dest, flow_act, esw, attr, i);\n\t} else if (esw_is_chain_src_port_rewrite(esw, esw_attr)) {\n\t\terr = esw_setup_chain_src_port_rewrite(dest, flow_act, esw, chains, attr, i);\n\t} else {\n\t\t*i = esw_setup_vport_dests(dest, flow_act, esw, esw_attr, *i);\n\n\t\tif (attr->dest_ft) {\n\t\t\terr = esw_setup_ft_dest(dest, flow_act, esw, attr, *i);\n\t\t\t(*i)++;\n\t\t} else if (attr->dest_chain) {\n\t\t\terr = esw_setup_chain_dest(dest, flow_act, chains, attr->dest_chain,\n\t\t\t\t\t\t   1, 0, *i);\n\t\t\t(*i)++;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\nstatic void\nesw_cleanup_dests(struct mlx5_eswitch *esw,\n\t\t  struct mlx5_flow_attr *attr)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tstruct mlx5_fs_chains *chains = esw_chains(esw);\n\n\tif (attr->dest_ft) {\n\t\tesw_cleanup_decap_indir(esw, attr);\n\t} else if (!mlx5e_tc_attr_flags_skip(attr->flags)) {\n\t\tif (attr->dest_chain)\n\t\t\tesw_cleanup_chain_dest(chains, attr->dest_chain, 1, 0);\n\t\telse if (esw_is_indir_table(esw, attr))\n\t\t\tesw_cleanup_indir_table(esw, attr);\n\t\telse if (esw_is_chain_src_port_rewrite(esw, esw_attr))\n\t\t\tesw_cleanup_chain_src_port_rewrite(esw, attr);\n\t}\n}\n\nstatic void\nesw_setup_meter(struct mlx5_flow_attr *attr, struct mlx5_flow_act *flow_act)\n{\n\tstruct mlx5e_flow_meter_handle *meter;\n\n\tmeter = attr->meter_attr.meter;\n\tflow_act->exe_aso.type = attr->exe_aso_type;\n\tflow_act->exe_aso.object_id = meter->obj_id;\n\tflow_act->exe_aso.flow_meter.meter_idx = meter->idx;\n\tflow_act->exe_aso.flow_meter.init_color = MLX5_FLOW_METER_COLOR_GREEN;\n\t \n\tflow_act->exe_aso.return_reg_id = 5;\n}\n\nstruct mlx5_flow_handle *\nmlx5_eswitch_add_offloaded_rule(struct mlx5_eswitch *esw,\n\t\t\t\tstruct mlx5_flow_spec *spec,\n\t\t\t\tstruct mlx5_flow_attr *attr)\n{\n\tstruct mlx5_flow_act flow_act = { .flags = FLOW_ACT_NO_APPEND, };\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tstruct mlx5_fs_chains *chains = esw_chains(esw);\n\tbool split = !!(esw_attr->split_count);\n\tstruct mlx5_vport_tbl_attr fwd_attr;\n\tstruct mlx5_flow_destination *dest;\n\tstruct mlx5_flow_handle *rule;\n\tstruct mlx5_flow_table *fdb;\n\tint i = 0;\n\n\tif (esw->mode != MLX5_ESWITCH_OFFLOADS)\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tif (!mlx5_eswitch_vlan_actions_supported(esw->dev, 1))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tif (!esw_flow_dests_fwd_ipsec_check(esw, esw_attr))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tdest = kcalloc(MLX5_MAX_FLOW_FWD_VPORTS + 1, sizeof(*dest), GFP_KERNEL);\n\tif (!dest)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tflow_act.action = attr->action;\n\n\tif (flow_act.action & MLX5_FLOW_CONTEXT_ACTION_VLAN_PUSH) {\n\t\tflow_act.vlan[0].ethtype = ntohs(esw_attr->vlan_proto[0]);\n\t\tflow_act.vlan[0].vid = esw_attr->vlan_vid[0];\n\t\tflow_act.vlan[0].prio = esw_attr->vlan_prio[0];\n\t\tif (flow_act.action & MLX5_FLOW_CONTEXT_ACTION_VLAN_PUSH_2) {\n\t\t\tflow_act.vlan[1].ethtype = ntohs(esw_attr->vlan_proto[1]);\n\t\t\tflow_act.vlan[1].vid = esw_attr->vlan_vid[1];\n\t\t\tflow_act.vlan[1].prio = esw_attr->vlan_prio[1];\n\t\t}\n\t}\n\n\tmlx5_eswitch_set_rule_flow_source(esw, spec, esw_attr);\n\n\tif (flow_act.action & MLX5_FLOW_CONTEXT_ACTION_FWD_DEST) {\n\t\tint err;\n\n\t\terr = esw_setup_dests(dest, &flow_act, esw, attr, spec, &i);\n\t\tif (err) {\n\t\t\trule = ERR_PTR(err);\n\t\t\tgoto err_create_goto_table;\n\t\t}\n\n\t\t \n\t\tif ((flow_act.action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR) &&\n\t\t    esw_dests_to_vf_pf_vports(dest, i)) {\n\t\t\tesw_warn(esw->dev,\n\t\t\t\t \"FDB: Header rewrite with forwarding to both PF and VF is not allowed\\n\");\n\t\t\trule = ERR_PTR(-EINVAL);\n\t\t\tgoto err_esw_get;\n\t\t}\n\t}\n\n\tif (esw_attr->decap_pkt_reformat)\n\t\tflow_act.pkt_reformat = esw_attr->decap_pkt_reformat;\n\n\tif (flow_act.action & MLX5_FLOW_CONTEXT_ACTION_COUNT) {\n\t\tdest[i].type = MLX5_FLOW_DESTINATION_TYPE_COUNTER;\n\t\tdest[i].counter_id = mlx5_fc_id(attr->counter);\n\t\ti++;\n\t}\n\n\tif (attr->outer_match_level != MLX5_MATCH_NONE)\n\t\tspec->match_criteria_enable |= MLX5_MATCH_OUTER_HEADERS;\n\tif (attr->inner_match_level != MLX5_MATCH_NONE)\n\t\tspec->match_criteria_enable |= MLX5_MATCH_INNER_HEADERS;\n\n\tif (flow_act.action & MLX5_FLOW_CONTEXT_ACTION_MOD_HDR)\n\t\tflow_act.modify_hdr = attr->modify_hdr;\n\n\tif ((flow_act.action & MLX5_FLOW_CONTEXT_ACTION_EXECUTE_ASO) &&\n\t    attr->exe_aso_type == MLX5_EXE_ASO_FLOW_METER)\n\t\tesw_setup_meter(attr, &flow_act);\n\n\tif (split) {\n\t\tfwd_attr.chain = attr->chain;\n\t\tfwd_attr.prio = attr->prio;\n\t\tfwd_attr.vport = esw_attr->in_rep->vport;\n\t\tfwd_attr.vport_ns = &mlx5_esw_vport_tbl_mirror_ns;\n\n\t\tfdb = mlx5_esw_vporttbl_get(esw, &fwd_attr);\n\t} else {\n\t\tif (attr->chain || attr->prio)\n\t\t\tfdb = mlx5_chains_get_table(chains, attr->chain,\n\t\t\t\t\t\t    attr->prio, 0);\n\t\telse\n\t\t\tfdb = attr->ft;\n\n\t\tif (!(attr->flags & MLX5_ATTR_FLAG_NO_IN_PORT))\n\t\t\tmlx5_eswitch_set_rule_source_port(esw, spec, attr,\n\t\t\t\t\t\t\t  esw_attr->in_mdev->priv.eswitch,\n\t\t\t\t\t\t\t  esw_attr->in_rep->vport);\n\t}\n\tif (IS_ERR(fdb)) {\n\t\trule = ERR_CAST(fdb);\n\t\tgoto err_esw_get;\n\t}\n\n\tif (!i) {\n\t\tkfree(dest);\n\t\tdest = NULL;\n\t}\n\n\tif (mlx5_eswitch_termtbl_required(esw, attr, &flow_act, spec))\n\t\trule = mlx5_eswitch_add_termtbl_rule(esw, fdb, spec, esw_attr,\n\t\t\t\t\t\t     &flow_act, dest, i);\n\telse\n\t\trule = mlx5_add_flow_rules(fdb, spec, &flow_act, dest, i);\n\tif (IS_ERR(rule))\n\t\tgoto err_add_rule;\n\telse\n\t\tatomic64_inc(&esw->offloads.num_flows);\n\n\tkfree(dest);\n\treturn rule;\n\nerr_add_rule:\n\tif (split)\n\t\tmlx5_esw_vporttbl_put(esw, &fwd_attr);\n\telse if (attr->chain || attr->prio)\n\t\tmlx5_chains_put_table(chains, attr->chain, attr->prio, 0);\nerr_esw_get:\n\tesw_cleanup_dests(esw, attr);\nerr_create_goto_table:\n\tkfree(dest);\n\treturn rule;\n}\n\nstruct mlx5_flow_handle *\nmlx5_eswitch_add_fwd_rule(struct mlx5_eswitch *esw,\n\t\t\t  struct mlx5_flow_spec *spec,\n\t\t\t  struct mlx5_flow_attr *attr)\n{\n\tstruct mlx5_flow_act flow_act = { .flags = FLOW_ACT_NO_APPEND, };\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tstruct mlx5_fs_chains *chains = esw_chains(esw);\n\tstruct mlx5_vport_tbl_attr fwd_attr;\n\tstruct mlx5_flow_destination *dest;\n\tstruct mlx5_flow_table *fast_fdb;\n\tstruct mlx5_flow_table *fwd_fdb;\n\tstruct mlx5_flow_handle *rule;\n\tint i, err = 0;\n\n\tdest = kcalloc(MLX5_MAX_FLOW_FWD_VPORTS + 1, sizeof(*dest), GFP_KERNEL);\n\tif (!dest)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfast_fdb = mlx5_chains_get_table(chains, attr->chain, attr->prio, 0);\n\tif (IS_ERR(fast_fdb)) {\n\t\trule = ERR_CAST(fast_fdb);\n\t\tgoto err_get_fast;\n\t}\n\n\tfwd_attr.chain = attr->chain;\n\tfwd_attr.prio = attr->prio;\n\tfwd_attr.vport = esw_attr->in_rep->vport;\n\tfwd_attr.vport_ns = &mlx5_esw_vport_tbl_mirror_ns;\n\tfwd_fdb = mlx5_esw_vporttbl_get(esw, &fwd_attr);\n\tif (IS_ERR(fwd_fdb)) {\n\t\trule = ERR_CAST(fwd_fdb);\n\t\tgoto err_get_fwd;\n\t}\n\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;\n\tfor (i = 0; i < esw_attr->split_count; i++) {\n\t\tif (esw_attr->dests[i].flags & MLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE)\n\t\t\t \n\t\t\terr = -EOPNOTSUPP;\n\t\telse\n\t\t\tesw_setup_vport_dest(dest, &flow_act, esw, esw_attr, i, i, false);\n\n\t\tif (err) {\n\t\t\trule = ERR_PTR(err);\n\t\t\tgoto err_chain_src_rewrite;\n\t\t}\n\t}\n\tdest[i].type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\tdest[i].ft = fwd_fdb;\n\ti++;\n\n\tmlx5_eswitch_set_rule_source_port(esw, spec, attr,\n\t\t\t\t\t  esw_attr->in_mdev->priv.eswitch,\n\t\t\t\t\t  esw_attr->in_rep->vport);\n\n\tif (attr->outer_match_level != MLX5_MATCH_NONE)\n\t\tspec->match_criteria_enable |= MLX5_MATCH_OUTER_HEADERS;\n\n\tflow_act.flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;\n\trule = mlx5_add_flow_rules(fast_fdb, spec, &flow_act, dest, i);\n\n\tif (IS_ERR(rule)) {\n\t\ti = esw_attr->split_count;\n\t\tgoto err_chain_src_rewrite;\n\t}\n\n\tatomic64_inc(&esw->offloads.num_flows);\n\n\tkfree(dest);\n\treturn rule;\nerr_chain_src_rewrite:\n\tmlx5_esw_vporttbl_put(esw, &fwd_attr);\nerr_get_fwd:\n\tmlx5_chains_put_table(chains, attr->chain, attr->prio, 0);\nerr_get_fast:\n\tkfree(dest);\n\treturn rule;\n}\n\nstatic void\n__mlx5_eswitch_del_rule(struct mlx5_eswitch *esw,\n\t\t\tstruct mlx5_flow_handle *rule,\n\t\t\tstruct mlx5_flow_attr *attr,\n\t\t\tbool fwd_rule)\n{\n\tstruct mlx5_esw_flow_attr *esw_attr = attr->esw_attr;\n\tstruct mlx5_fs_chains *chains = esw_chains(esw);\n\tbool split = (esw_attr->split_count > 0);\n\tstruct mlx5_vport_tbl_attr fwd_attr;\n\tint i;\n\n\tmlx5_del_flow_rules(rule);\n\n\tif (!mlx5e_tc_attr_flags_skip(attr->flags)) {\n\t\t \n\t\tfor (i = 0; i < MLX5_MAX_FLOW_FWD_VPORTS; i++) {\n\t\t\tif (esw_attr->dests[i].termtbl)\n\t\t\t\tmlx5_eswitch_termtbl_put(esw, esw_attr->dests[i].termtbl);\n\t\t}\n\t}\n\n\tatomic64_dec(&esw->offloads.num_flows);\n\n\tif (fwd_rule || split) {\n\t\tfwd_attr.chain = attr->chain;\n\t\tfwd_attr.prio = attr->prio;\n\t\tfwd_attr.vport = esw_attr->in_rep->vport;\n\t\tfwd_attr.vport_ns = &mlx5_esw_vport_tbl_mirror_ns;\n\t}\n\n\tif (fwd_rule)  {\n\t\tmlx5_esw_vporttbl_put(esw, &fwd_attr);\n\t\tmlx5_chains_put_table(chains, attr->chain, attr->prio, 0);\n\t} else {\n\t\tif (split)\n\t\t\tmlx5_esw_vporttbl_put(esw, &fwd_attr);\n\t\telse if (attr->chain || attr->prio)\n\t\t\tmlx5_chains_put_table(chains, attr->chain, attr->prio, 0);\n\t\tesw_cleanup_dests(esw, attr);\n\t}\n}\n\nvoid\nmlx5_eswitch_del_offloaded_rule(struct mlx5_eswitch *esw,\n\t\t\t\tstruct mlx5_flow_handle *rule,\n\t\t\t\tstruct mlx5_flow_attr *attr)\n{\n\t__mlx5_eswitch_del_rule(esw, rule, attr, false);\n}\n\nvoid\nmlx5_eswitch_del_fwd_rule(struct mlx5_eswitch *esw,\n\t\t\t  struct mlx5_flow_handle *rule,\n\t\t\t  struct mlx5_flow_attr *attr)\n{\n\t__mlx5_eswitch_del_rule(esw, rule, attr, true);\n}\n\nstruct mlx5_flow_handle *\nmlx5_eswitch_add_send_to_vport_rule(struct mlx5_eswitch *on_esw,\n\t\t\t\t    struct mlx5_eswitch *from_esw,\n\t\t\t\t    struct mlx5_eswitch_rep *rep,\n\t\t\t\t    u32 sqn)\n{\n\tstruct mlx5_flow_act flow_act = {0};\n\tstruct mlx5_flow_destination dest = {};\n\tstruct mlx5_flow_handle *flow_rule;\n\tstruct mlx5_flow_spec *spec;\n\tvoid *misc;\n\tu16 vport;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec) {\n\t\tflow_rule = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters);\n\tMLX5_SET(fte_match_set_misc, misc, source_sqn, sqn);\n\n\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters);\n\tMLX5_SET_TO_ONES(fte_match_set_misc, misc, source_sqn);\n\n\tspec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS;\n\n\t \n\tvport = from_esw->manager_vport;\n\n\tif (mlx5_eswitch_vport_match_metadata_enabled(on_esw)) {\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,\n\t\t\t mlx5_eswitch_get_vport_metadata_for_match(from_esw, vport));\n\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,\n\t\t\t mlx5_eswitch_get_vport_metadata_mask());\n\n\t\tspec->match_criteria_enable |= MLX5_MATCH_MISC_PARAMETERS_2;\n\t} else {\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters);\n\t\tMLX5_SET(fte_match_set_misc, misc, source_port, vport);\n\n\t\tif (MLX5_CAP_ESW(on_esw->dev, merged_eswitch))\n\t\t\tMLX5_SET(fte_match_set_misc, misc, source_eswitch_owner_vhca_id,\n\t\t\t\t MLX5_CAP_GEN(from_esw->dev, vhca_id));\n\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters);\n\t\tMLX5_SET_TO_ONES(fte_match_set_misc, misc, source_port);\n\n\t\tif (MLX5_CAP_ESW(on_esw->dev, merged_eswitch))\n\t\t\tMLX5_SET_TO_ONES(fte_match_set_misc, misc,\n\t\t\t\t\t source_eswitch_owner_vhca_id);\n\n\t\tspec->match_criteria_enable |= MLX5_MATCH_MISC_PARAMETERS;\n\t}\n\n\tdest.type = MLX5_FLOW_DESTINATION_TYPE_VPORT;\n\tdest.vport.num = rep->vport;\n\tdest.vport.vhca_id = MLX5_CAP_GEN(rep->esw->dev, vhca_id);\n\tdest.vport.flags |= MLX5_FLOW_DEST_VPORT_VHCA_ID;\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;\n\n\tif (rep->vport == MLX5_VPORT_UPLINK &&\n\t    on_esw == from_esw && on_esw->offloads.ft_ipsec_tx_pol) {\n\t\tdest.ft = on_esw->offloads.ft_ipsec_tx_pol;\n\t\tflow_act.flags = FLOW_ACT_IGNORE_FLOW_LEVEL;\n\t\tdest.type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\t} else {\n\t\tdest.type = MLX5_FLOW_DESTINATION_TYPE_VPORT;\n\t\tdest.vport.num = rep->vport;\n\t\tdest.vport.vhca_id = MLX5_CAP_GEN(rep->esw->dev, vhca_id);\n\t\tdest.vport.flags |= MLX5_FLOW_DEST_VPORT_VHCA_ID;\n\t}\n\n\tif (MLX5_CAP_ESW_FLOWTABLE(on_esw->dev, flow_source) &&\n\t    rep->vport == MLX5_VPORT_UPLINK)\n\t\tspec->flow_context.flow_source = MLX5_FLOW_CONTEXT_FLOW_SOURCE_LOCAL_VPORT;\n\n\tflow_rule = mlx5_add_flow_rules(mlx5_eswitch_get_slow_fdb(on_esw),\n\t\t\t\t\tspec, &flow_act, &dest, 1);\n\tif (IS_ERR(flow_rule))\n\t\tesw_warn(on_esw->dev, \"FDB: Failed to add send to vport rule err %ld\\n\",\n\t\t\t PTR_ERR(flow_rule));\nout:\n\tkvfree(spec);\n\treturn flow_rule;\n}\nEXPORT_SYMBOL(mlx5_eswitch_add_send_to_vport_rule);\n\nvoid mlx5_eswitch_del_send_to_vport_rule(struct mlx5_flow_handle *rule)\n{\n\tmlx5_del_flow_rules(rule);\n}\n\nvoid mlx5_eswitch_del_send_to_vport_meta_rule(struct mlx5_flow_handle *rule)\n{\n\tif (rule)\n\t\tmlx5_del_flow_rules(rule);\n}\n\nstruct mlx5_flow_handle *\nmlx5_eswitch_add_send_to_vport_meta_rule(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\tstruct mlx5_flow_destination dest = {};\n\tstruct mlx5_flow_act flow_act = {0};\n\tstruct mlx5_flow_handle *flow_rule;\n\tstruct mlx5_flow_spec *spec;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tMLX5_SET(fte_match_param, spec->match_criteria,\n\t\t misc_parameters_2.metadata_reg_c_0, mlx5_eswitch_get_vport_metadata_mask());\n\tMLX5_SET(fte_match_param, spec->match_criteria,\n\t\t misc_parameters_2.metadata_reg_c_1, ESW_TUN_MASK);\n\tMLX5_SET(fte_match_param, spec->match_value, misc_parameters_2.metadata_reg_c_1,\n\t\t ESW_TUN_SLOW_TABLE_GOTO_VPORT_MARK);\n\n\tspec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS_2;\n\tdest.type = MLX5_FLOW_DESTINATION_TYPE_VPORT;\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;\n\n\tMLX5_SET(fte_match_param, spec->match_value, misc_parameters_2.metadata_reg_c_0,\n\t\t mlx5_eswitch_get_vport_metadata_for_match(esw, vport_num));\n\tdest.vport.num = vport_num;\n\n\tflow_rule = mlx5_add_flow_rules(mlx5_eswitch_get_slow_fdb(esw),\n\t\t\t\t\tspec, &flow_act, &dest, 1);\n\tif (IS_ERR(flow_rule))\n\t\tesw_warn(esw->dev, \"FDB: Failed to add send to vport meta rule vport %d, err %ld\\n\",\n\t\t\t vport_num, PTR_ERR(flow_rule));\n\n\tkvfree(spec);\n\treturn flow_rule;\n}\n\nstatic bool mlx5_eswitch_reg_c1_loopback_supported(struct mlx5_eswitch *esw)\n{\n\treturn MLX5_CAP_ESW_FLOWTABLE(esw->dev, fdb_to_vport_reg_c_id) &\n\t       MLX5_FDB_TO_VPORT_REG_C_1;\n}\n\nstatic int esw_set_passing_vport_metadata(struct mlx5_eswitch *esw, bool enable)\n{\n\tu32 out[MLX5_ST_SZ_DW(query_esw_vport_context_out)] = {};\n\tu32 min[MLX5_ST_SZ_DW(modify_esw_vport_context_in)] = {};\n\tu32 in[MLX5_ST_SZ_DW(query_esw_vport_context_in)] = {};\n\tu8 curr, wanted;\n\tint err;\n\n\tif (!mlx5_eswitch_reg_c1_loopback_supported(esw) &&\n\t    !mlx5_eswitch_vport_match_metadata_enabled(esw))\n\t\treturn 0;\n\n\tMLX5_SET(query_esw_vport_context_in, in, opcode,\n\t\t MLX5_CMD_OP_QUERY_ESW_VPORT_CONTEXT);\n\terr = mlx5_cmd_exec_inout(esw->dev, query_esw_vport_context, in, out);\n\tif (err)\n\t\treturn err;\n\n\tcurr = MLX5_GET(query_esw_vport_context_out, out,\n\t\t\tesw_vport_context.fdb_to_vport_reg_c_id);\n\twanted = MLX5_FDB_TO_VPORT_REG_C_0;\n\tif (mlx5_eswitch_reg_c1_loopback_supported(esw))\n\t\twanted |= MLX5_FDB_TO_VPORT_REG_C_1;\n\n\tif (enable)\n\t\tcurr |= wanted;\n\telse\n\t\tcurr &= ~wanted;\n\n\tMLX5_SET(modify_esw_vport_context_in, min,\n\t\t esw_vport_context.fdb_to_vport_reg_c_id, curr);\n\tMLX5_SET(modify_esw_vport_context_in, min,\n\t\t field_select.fdb_to_vport_reg_c_id, 1);\n\n\terr = mlx5_eswitch_modify_esw_vport_context(esw->dev, 0, false, min);\n\tif (!err) {\n\t\tif (enable && (curr & MLX5_FDB_TO_VPORT_REG_C_1))\n\t\t\tesw->flags |= MLX5_ESWITCH_REG_C1_LOOPBACK_ENABLED;\n\t\telse\n\t\t\tesw->flags &= ~MLX5_ESWITCH_REG_C1_LOOPBACK_ENABLED;\n\t}\n\n\treturn err;\n}\n\nstatic void peer_miss_rules_setup(struct mlx5_eswitch *esw,\n\t\t\t\t  struct mlx5_core_dev *peer_dev,\n\t\t\t\t  struct mlx5_flow_spec *spec,\n\t\t\t\t  struct mlx5_flow_destination *dest)\n{\n\tvoid *misc;\n\n\tif (mlx5_eswitch_vport_match_metadata_enabled(esw)) {\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,\n\t\t\t\t    misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,\n\t\t\t mlx5_eswitch_get_vport_metadata_mask());\n\n\t\tspec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS_2;\n\t} else {\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t    misc_parameters);\n\n\t\tMLX5_SET(fte_match_set_misc, misc, source_eswitch_owner_vhca_id,\n\t\t\t MLX5_CAP_GEN(peer_dev, vhca_id));\n\n\t\tspec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS;\n\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,\n\t\t\t\t    misc_parameters);\n\t\tMLX5_SET_TO_ONES(fte_match_set_misc, misc, source_port);\n\t\tMLX5_SET_TO_ONES(fte_match_set_misc, misc,\n\t\t\t\t source_eswitch_owner_vhca_id);\n\t}\n\n\tdest->type = MLX5_FLOW_DESTINATION_TYPE_VPORT;\n\tdest->vport.num = peer_dev->priv.eswitch->manager_vport;\n\tdest->vport.vhca_id = MLX5_CAP_GEN(peer_dev, vhca_id);\n\tdest->vport.flags |= MLX5_FLOW_DEST_VPORT_VHCA_ID;\n}\n\nstatic void esw_set_peer_miss_rule_source_port(struct mlx5_eswitch *esw,\n\t\t\t\t\t       struct mlx5_eswitch *peer_esw,\n\t\t\t\t\t       struct mlx5_flow_spec *spec,\n\t\t\t\t\t       u16 vport)\n{\n\tvoid *misc;\n\n\tif (mlx5_eswitch_vport_match_metadata_enabled(esw)) {\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t    misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,\n\t\t\t mlx5_eswitch_get_vport_metadata_for_match(peer_esw,\n\t\t\t\t\t\t\t\t   vport));\n\t} else {\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t    misc_parameters);\n\t\tMLX5_SET(fte_match_set_misc, misc, source_port, vport);\n\t}\n}\n\nstatic int esw_add_fdb_peer_miss_rules(struct mlx5_eswitch *esw,\n\t\t\t\t       struct mlx5_core_dev *peer_dev)\n{\n\tstruct mlx5_flow_destination dest = {};\n\tstruct mlx5_flow_act flow_act = {0};\n\tstruct mlx5_flow_handle **flows;\n\t \n\tint nvports = esw->total_vports;\n\tstruct mlx5_flow_handle *flow;\n\tstruct mlx5_flow_spec *spec;\n\tstruct mlx5_vport *vport;\n\tint err, pfindex;\n\tunsigned long i;\n\tvoid *misc;\n\n\tif (!MLX5_VPORT_MANAGER(esw->dev) && !mlx5_core_is_ecpf_esw_manager(esw->dev))\n\t\treturn 0;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn -ENOMEM;\n\n\tpeer_miss_rules_setup(esw, peer_dev, spec, &dest);\n\n\tflows = kvcalloc(nvports, sizeof(*flows), GFP_KERNEL);\n\tif (!flows) {\n\t\terr = -ENOMEM;\n\t\tgoto alloc_flows_err;\n\t}\n\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;\n\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t    misc_parameters);\n\n\tif (mlx5_core_is_ecpf_esw_manager(esw->dev)) {\n\t\tvport = mlx5_eswitch_get_vport(esw, MLX5_VPORT_PF);\n\t\tesw_set_peer_miss_rule_source_port(esw, peer_dev->priv.eswitch,\n\t\t\t\t\t\t   spec, MLX5_VPORT_PF);\n\n\t\tflow = mlx5_add_flow_rules(mlx5_eswitch_get_slow_fdb(esw),\n\t\t\t\t\t   spec, &flow_act, &dest, 1);\n\t\tif (IS_ERR(flow)) {\n\t\t\terr = PTR_ERR(flow);\n\t\t\tgoto add_pf_flow_err;\n\t\t}\n\t\tflows[vport->index] = flow;\n\t}\n\n\tif (mlx5_ecpf_vport_exists(esw->dev)) {\n\t\tvport = mlx5_eswitch_get_vport(esw, MLX5_VPORT_ECPF);\n\t\tMLX5_SET(fte_match_set_misc, misc, source_port, MLX5_VPORT_ECPF);\n\t\tflow = mlx5_add_flow_rules(mlx5_eswitch_get_slow_fdb(esw),\n\t\t\t\t\t   spec, &flow_act, &dest, 1);\n\t\tif (IS_ERR(flow)) {\n\t\t\terr = PTR_ERR(flow);\n\t\t\tgoto add_ecpf_flow_err;\n\t\t}\n\t\tflows[vport->index] = flow;\n\t}\n\n\tmlx5_esw_for_each_vf_vport(esw, i, vport, mlx5_core_max_vfs(esw->dev)) {\n\t\tesw_set_peer_miss_rule_source_port(esw,\n\t\t\t\t\t\t   peer_dev->priv.eswitch,\n\t\t\t\t\t\t   spec, vport->vport);\n\n\t\tflow = mlx5_add_flow_rules(mlx5_eswitch_get_slow_fdb(esw),\n\t\t\t\t\t   spec, &flow_act, &dest, 1);\n\t\tif (IS_ERR(flow)) {\n\t\t\terr = PTR_ERR(flow);\n\t\t\tgoto add_vf_flow_err;\n\t\t}\n\t\tflows[vport->index] = flow;\n\t}\n\n\tif (mlx5_core_ec_sriov_enabled(esw->dev)) {\n\t\tmlx5_esw_for_each_ec_vf_vport(esw, i, vport, mlx5_core_max_ec_vfs(esw->dev)) {\n\t\t\tif (i >= mlx5_core_max_ec_vfs(peer_dev))\n\t\t\t\tbreak;\n\t\t\tesw_set_peer_miss_rule_source_port(esw, peer_dev->priv.eswitch,\n\t\t\t\t\t\t\t   spec, vport->vport);\n\t\t\tflow = mlx5_add_flow_rules(esw->fdb_table.offloads.slow_fdb,\n\t\t\t\t\t\t   spec, &flow_act, &dest, 1);\n\t\t\tif (IS_ERR(flow)) {\n\t\t\t\terr = PTR_ERR(flow);\n\t\t\t\tgoto add_ec_vf_flow_err;\n\t\t\t}\n\t\t\tflows[vport->index] = flow;\n\t\t}\n\t}\n\n\tpfindex = mlx5_get_dev_index(peer_dev);\n\tif (pfindex >= MLX5_MAX_PORTS) {\n\t\tesw_warn(esw->dev, \"Peer dev index(%d) is over the max num defined(%d)\\n\",\n\t\t\t pfindex, MLX5_MAX_PORTS);\n\t\terr = -EINVAL;\n\t\tgoto add_ec_vf_flow_err;\n\t}\n\tesw->fdb_table.offloads.peer_miss_rules[pfindex] = flows;\n\n\tkvfree(spec);\n\treturn 0;\n\nadd_ec_vf_flow_err:\n\tmlx5_esw_for_each_ec_vf_vport(esw, i, vport, mlx5_core_max_ec_vfs(esw->dev)) {\n\t\tif (!flows[vport->index])\n\t\t\tcontinue;\n\t\tmlx5_del_flow_rules(flows[vport->index]);\n\t}\nadd_vf_flow_err:\n\tmlx5_esw_for_each_vf_vport(esw, i, vport, mlx5_core_max_vfs(esw->dev)) {\n\t\tif (!flows[vport->index])\n\t\t\tcontinue;\n\t\tmlx5_del_flow_rules(flows[vport->index]);\n\t}\n\tif (mlx5_ecpf_vport_exists(esw->dev)) {\n\t\tvport = mlx5_eswitch_get_vport(esw, MLX5_VPORT_ECPF);\n\t\tmlx5_del_flow_rules(flows[vport->index]);\n\t}\nadd_ecpf_flow_err:\n\tif (mlx5_core_is_ecpf_esw_manager(esw->dev)) {\n\t\tvport = mlx5_eswitch_get_vport(esw, MLX5_VPORT_PF);\n\t\tmlx5_del_flow_rules(flows[vport->index]);\n\t}\nadd_pf_flow_err:\n\tesw_warn(esw->dev, \"FDB: Failed to add peer miss flow rule err %d\\n\", err);\n\tkvfree(flows);\nalloc_flows_err:\n\tkvfree(spec);\n\treturn err;\n}\n\nstatic void esw_del_fdb_peer_miss_rules(struct mlx5_eswitch *esw,\n\t\t\t\t\tstruct mlx5_core_dev *peer_dev)\n{\n\tu16 peer_index = mlx5_get_dev_index(peer_dev);\n\tstruct mlx5_flow_handle **flows;\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tflows = esw->fdb_table.offloads.peer_miss_rules[peer_index];\n\tif (!flows)\n\t\treturn;\n\n\tif (mlx5_core_ec_sriov_enabled(esw->dev)) {\n\t\tmlx5_esw_for_each_ec_vf_vport(esw, i, vport, mlx5_core_max_ec_vfs(esw->dev)) {\n\t\t\t \n\t\t\tif (!flows[vport->index])\n\t\t\t\tcontinue;\n\t\t\tmlx5_del_flow_rules(flows[vport->index]);\n\t\t}\n\t}\n\n\tmlx5_esw_for_each_vf_vport(esw, i, vport, mlx5_core_max_vfs(esw->dev))\n\t\tmlx5_del_flow_rules(flows[vport->index]);\n\n\tif (mlx5_ecpf_vport_exists(esw->dev)) {\n\t\tvport = mlx5_eswitch_get_vport(esw, MLX5_VPORT_ECPF);\n\t\tmlx5_del_flow_rules(flows[vport->index]);\n\t}\n\n\tif (mlx5_core_is_ecpf_esw_manager(esw->dev)) {\n\t\tvport = mlx5_eswitch_get_vport(esw, MLX5_VPORT_PF);\n\t\tmlx5_del_flow_rules(flows[vport->index]);\n\t}\n\n\tkvfree(flows);\n\tesw->fdb_table.offloads.peer_miss_rules[peer_index] = NULL;\n}\n\nstatic int esw_add_fdb_miss_rule(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_flow_act flow_act = {0};\n\tstruct mlx5_flow_destination dest = {};\n\tstruct mlx5_flow_handle *flow_rule = NULL;\n\tstruct mlx5_flow_spec *spec;\n\tvoid *headers_c;\n\tvoid *headers_v;\n\tint err = 0;\n\tu8 *dmac_c;\n\tu8 *dmac_v;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tspec->match_criteria_enable = MLX5_MATCH_OUTER_HEADERS;\n\theaders_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,\n\t\t\t\t outer_headers);\n\tdmac_c = MLX5_ADDR_OF(fte_match_param, headers_c,\n\t\t\t      outer_headers.dmac_47_16);\n\tdmac_c[0] = 0x01;\n\n\tdest.type = MLX5_FLOW_DESTINATION_TYPE_VPORT;\n\tdest.vport.num = esw->manager_vport;\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;\n\n\tflow_rule = mlx5_add_flow_rules(mlx5_eswitch_get_slow_fdb(esw),\n\t\t\t\t\tspec, &flow_act, &dest, 1);\n\tif (IS_ERR(flow_rule)) {\n\t\terr = PTR_ERR(flow_rule);\n\t\tesw_warn(esw->dev,  \"FDB: Failed to add unicast miss flow rule err %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\tesw->fdb_table.offloads.miss_rule_uni = flow_rule;\n\n\theaders_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t outer_headers);\n\tdmac_v = MLX5_ADDR_OF(fte_match_param, headers_v,\n\t\t\t      outer_headers.dmac_47_16);\n\tdmac_v[0] = 0x01;\n\tflow_rule = mlx5_add_flow_rules(mlx5_eswitch_get_slow_fdb(esw),\n\t\t\t\t\tspec, &flow_act, &dest, 1);\n\tif (IS_ERR(flow_rule)) {\n\t\terr = PTR_ERR(flow_rule);\n\t\tesw_warn(esw->dev, \"FDB: Failed to add multicast miss flow rule err %d\\n\", err);\n\t\tmlx5_del_flow_rules(esw->fdb_table.offloads.miss_rule_uni);\n\t\tgoto out;\n\t}\n\n\tesw->fdb_table.offloads.miss_rule_multi = flow_rule;\n\nout:\n\tkvfree(spec);\n\treturn err;\n}\n\nstruct mlx5_flow_handle *\nesw_add_restore_rule(struct mlx5_eswitch *esw, u32 tag)\n{\n\tstruct mlx5_flow_act flow_act = { .flags = FLOW_ACT_NO_APPEND, };\n\tstruct mlx5_flow_table *ft = esw->offloads.ft_offloads_restore;\n\tstruct mlx5_flow_context *flow_context;\n\tstruct mlx5_flow_handle *flow_rule;\n\tstruct mlx5_flow_destination dest;\n\tstruct mlx5_flow_spec *spec;\n\tvoid *misc;\n\n\tif (!mlx5_eswitch_reg_c1_loopback_supported(esw))\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,\n\t\t\t    misc_parameters_2);\n\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,\n\t\t ESW_REG_C0_USER_DATA_METADATA_MASK);\n\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t    misc_parameters_2);\n\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0, tag);\n\tspec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS_2;\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |\n\t\t\t  MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;\n\tflow_act.modify_hdr = esw->offloads.restore_copy_hdr_id;\n\n\tflow_context = &spec->flow_context;\n\tflow_context->flags |= FLOW_CONTEXT_HAS_TAG;\n\tflow_context->flow_tag = tag;\n\tdest.type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\tdest.ft = esw->offloads.ft_offloads;\n\n\tflow_rule = mlx5_add_flow_rules(ft, spec, &flow_act, &dest, 1);\n\tkvfree(spec);\n\n\tif (IS_ERR(flow_rule))\n\t\tesw_warn(esw->dev,\n\t\t\t \"Failed to create restore rule for tag: %d, err(%d)\\n\",\n\t\t\t tag, (int)PTR_ERR(flow_rule));\n\n\treturn flow_rule;\n}\n\n#define MAX_PF_SQ 256\n#define MAX_SQ_NVPORTS 32\n\nvoid\nmlx5_esw_set_flow_group_source_port(struct mlx5_eswitch *esw,\n\t\t\t\t    u32 *flow_group_in,\n\t\t\t\t    int match_params)\n{\n\tvoid *match_criteria = MLX5_ADDR_OF(create_flow_group_in,\n\t\t\t\t\t    flow_group_in,\n\t\t\t\t\t    match_criteria);\n\n\tif (mlx5_eswitch_vport_match_metadata_enabled(esw)) {\n\t\tMLX5_SET(create_flow_group_in, flow_group_in,\n\t\t\t match_criteria_enable,\n\t\t\t MLX5_MATCH_MISC_PARAMETERS_2 | match_params);\n\n\t\tMLX5_SET(fte_match_param, match_criteria,\n\t\t\t misc_parameters_2.metadata_reg_c_0,\n\t\t\t mlx5_eswitch_get_vport_metadata_mask());\n\t} else {\n\t\tMLX5_SET(create_flow_group_in, flow_group_in,\n\t\t\t match_criteria_enable,\n\t\t\t MLX5_MATCH_MISC_PARAMETERS | match_params);\n\n\t\tMLX5_SET_TO_ONES(fte_match_param, match_criteria,\n\t\t\t\t misc_parameters.source_port);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_MLX5_CLS_ACT)\nstatic void esw_vport_tbl_put(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport_tbl_attr attr;\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tattr.chain = 0;\n\tattr.prio = 1;\n\tmlx5_esw_for_each_vport(esw, i, vport) {\n\t\tattr.vport = vport->vport;\n\t\tattr.vport_ns = &mlx5_esw_vport_tbl_mirror_ns;\n\t\tmlx5_esw_vporttbl_put(esw, &attr);\n\t}\n}\n\nstatic int esw_vport_tbl_get(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport_tbl_attr attr;\n\tstruct mlx5_flow_table *fdb;\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tattr.chain = 0;\n\tattr.prio = 1;\n\tmlx5_esw_for_each_vport(esw, i, vport) {\n\t\tattr.vport = vport->vport;\n\t\tattr.vport_ns = &mlx5_esw_vport_tbl_mirror_ns;\n\t\tfdb = mlx5_esw_vporttbl_get(esw, &attr);\n\t\tif (IS_ERR(fdb))\n\t\t\tgoto out;\n\t}\n\treturn 0;\n\nout:\n\tesw_vport_tbl_put(esw);\n\treturn PTR_ERR(fdb);\n}\n\n#define fdb_modify_header_fwd_to_table_supported(esw) \\\n\t(MLX5_CAP_ESW_FLOWTABLE((esw)->dev, fdb_modify_header_fwd_to_table))\nstatic void esw_init_chains_offload_flags(struct mlx5_eswitch *esw, u32 *flags)\n{\n\tstruct mlx5_core_dev *dev = esw->dev;\n\n\tif (MLX5_CAP_ESW_FLOWTABLE_FDB(dev, ignore_flow_level))\n\t\t*flags |= MLX5_CHAINS_IGNORE_FLOW_LEVEL_SUPPORTED;\n\n\tif (!MLX5_CAP_ESW_FLOWTABLE(dev, multi_fdb_encap) &&\n\t    esw->offloads.encap != DEVLINK_ESWITCH_ENCAP_MODE_NONE) {\n\t\t*flags &= ~MLX5_CHAINS_AND_PRIOS_SUPPORTED;\n\t\tesw_warn(dev, \"Tc chains and priorities offload aren't supported, update firmware if needed\\n\");\n\t} else if (!mlx5_eswitch_reg_c1_loopback_enabled(esw)) {\n\t\t*flags &= ~MLX5_CHAINS_AND_PRIOS_SUPPORTED;\n\t\tesw_warn(dev, \"Tc chains and priorities offload aren't supported\\n\");\n\t} else if (!fdb_modify_header_fwd_to_table_supported(esw)) {\n\t\t \n\t\tesw_warn(dev,\n\t\t\t \"Tc chains and priorities offload aren't supported, check firmware version, or mlxconfig settings\\n\");\n\t\t*flags &= ~MLX5_CHAINS_AND_PRIOS_SUPPORTED;\n\t} else {\n\t\t*flags |= MLX5_CHAINS_AND_PRIOS_SUPPORTED;\n\t\tesw_info(dev, \"Supported tc chains and prios offload\\n\");\n\t}\n\n\tif (esw->offloads.encap != DEVLINK_ESWITCH_ENCAP_MODE_NONE)\n\t\t*flags |= MLX5_CHAINS_FT_TUNNEL_SUPPORTED;\n}\n\nstatic int\nesw_chains_create(struct mlx5_eswitch *esw, struct mlx5_flow_table *miss_fdb)\n{\n\tstruct mlx5_core_dev *dev = esw->dev;\n\tstruct mlx5_flow_table *nf_ft, *ft;\n\tstruct mlx5_chains_attr attr = {};\n\tstruct mlx5_fs_chains *chains;\n\tint err;\n\n\tesw_init_chains_offload_flags(esw, &attr.flags);\n\tattr.ns = MLX5_FLOW_NAMESPACE_FDB;\n\tattr.max_grp_num = esw->params.large_group_num;\n\tattr.default_ft = miss_fdb;\n\tattr.mapping = esw->offloads.reg_c0_obj_pool;\n\n\tchains = mlx5_chains_create(dev, &attr);\n\tif (IS_ERR(chains)) {\n\t\terr = PTR_ERR(chains);\n\t\tesw_warn(dev, \"Failed to create fdb chains err(%d)\\n\", err);\n\t\treturn err;\n\t}\n\tmlx5_chains_print_info(chains);\n\n\tesw->fdb_table.offloads.esw_chains_priv = chains;\n\n\t \n\tnf_ft = mlx5_chains_get_table(chains, mlx5_chains_get_nf_ft_chain(chains),\n\t\t\t\t      1, 0);\n\tif (IS_ERR(nf_ft)) {\n\t\terr = PTR_ERR(nf_ft);\n\t\tgoto nf_ft_err;\n\t}\n\n\t \n\tft = mlx5_chains_get_table(chains, 0, 1, 0);\n\tif (IS_ERR(ft)) {\n\t\terr = PTR_ERR(ft);\n\t\tgoto level_0_err;\n\t}\n\n\t \n\tif (!mlx5_chains_prios_supported(chains)) {\n\t\terr = esw_vport_tbl_get(esw);\n\t\tif (err)\n\t\t\tgoto level_1_err;\n\t}\n\n\tmlx5_chains_set_end_ft(chains, nf_ft);\n\n\treturn 0;\n\nlevel_1_err:\n\tmlx5_chains_put_table(chains, 0, 1, 0);\nlevel_0_err:\n\tmlx5_chains_put_table(chains, mlx5_chains_get_nf_ft_chain(chains), 1, 0);\nnf_ft_err:\n\tmlx5_chains_destroy(chains);\n\tesw->fdb_table.offloads.esw_chains_priv = NULL;\n\n\treturn err;\n}\n\nstatic void\nesw_chains_destroy(struct mlx5_eswitch *esw, struct mlx5_fs_chains *chains)\n{\n\tif (!mlx5_chains_prios_supported(chains))\n\t\tesw_vport_tbl_put(esw);\n\tmlx5_chains_put_table(chains, 0, 1, 0);\n\tmlx5_chains_put_table(chains, mlx5_chains_get_nf_ft_chain(chains), 1, 0);\n\tmlx5_chains_destroy(chains);\n}\n\n#else  \n\nstatic int\nesw_chains_create(struct mlx5_eswitch *esw, struct mlx5_flow_table *miss_fdb)\n{ return 0; }\n\nstatic void\nesw_chains_destroy(struct mlx5_eswitch *esw, struct mlx5_fs_chains *chains)\n{}\n\n#endif\n\nstatic int\nesw_create_send_to_vport_group(struct mlx5_eswitch *esw,\n\t\t\t       struct mlx5_flow_table *fdb,\n\t\t\t       u32 *flow_group_in,\n\t\t\t       int *ix)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_flow_group *g;\n\tvoid *match_criteria;\n\tint count, err = 0;\n\n\tmemset(flow_group_in, 0, inlen);\n\n\tmlx5_esw_set_flow_group_source_port(esw, flow_group_in, MLX5_MATCH_MISC_PARAMETERS);\n\n\tmatch_criteria = MLX5_ADDR_OF(create_flow_group_in, flow_group_in, match_criteria);\n\tMLX5_SET_TO_ONES(fte_match_param, match_criteria, misc_parameters.source_sqn);\n\n\tif (!mlx5_eswitch_vport_match_metadata_enabled(esw) &&\n\t    MLX5_CAP_ESW(esw->dev, merged_eswitch)) {\n\t\tMLX5_SET_TO_ONES(fte_match_param, match_criteria,\n\t\t\t\t misc_parameters.source_eswitch_owner_vhca_id);\n\t\tMLX5_SET(create_flow_group_in, flow_group_in,\n\t\t\t source_eswitch_owner_vhca_id_valid, 1);\n\t}\n\n\t \n\tcount = MLX5_MAX_PORTS * (esw->total_vports * MAX_SQ_NVPORTS + MAX_PF_SQ);\n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, 0);\n\tMLX5_SET(create_flow_group_in, flow_group_in, end_flow_index, *ix + count - 1);\n\t*ix += count;\n\n\tg = mlx5_create_flow_group(fdb, flow_group_in);\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tesw_warn(esw->dev, \"Failed to create send-to-vport flow group err(%d)\\n\", err);\n\t\tgoto out;\n\t}\n\tesw->fdb_table.offloads.send_to_vport_grp = g;\n\nout:\n\treturn err;\n}\n\nstatic int\nesw_create_meta_send_to_vport_group(struct mlx5_eswitch *esw,\n\t\t\t\t    struct mlx5_flow_table *fdb,\n\t\t\t\t    u32 *flow_group_in,\n\t\t\t\t    int *ix)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_flow_group *g;\n\tvoid *match_criteria;\n\tint err = 0;\n\n\tif (!esw_src_port_rewrite_supported(esw))\n\t\treturn 0;\n\n\tmemset(flow_group_in, 0, inlen);\n\n\tMLX5_SET(create_flow_group_in, flow_group_in, match_criteria_enable,\n\t\t MLX5_MATCH_MISC_PARAMETERS_2);\n\n\tmatch_criteria = MLX5_ADDR_OF(create_flow_group_in, flow_group_in, match_criteria);\n\n\tMLX5_SET(fte_match_param, match_criteria,\n\t\t misc_parameters_2.metadata_reg_c_0,\n\t\t mlx5_eswitch_get_vport_metadata_mask());\n\tMLX5_SET(fte_match_param, match_criteria,\n\t\t misc_parameters_2.metadata_reg_c_1, ESW_TUN_MASK);\n\n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, *ix);\n\tMLX5_SET(create_flow_group_in, flow_group_in,\n\t\t end_flow_index, *ix + esw->total_vports - 1);\n\t*ix += esw->total_vports;\n\n\tg = mlx5_create_flow_group(fdb, flow_group_in);\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tesw_warn(esw->dev,\n\t\t\t \"Failed to create send-to-vport meta flow group err(%d)\\n\", err);\n\t\tgoto send_vport_meta_err;\n\t}\n\tesw->fdb_table.offloads.send_to_vport_meta_grp = g;\n\n\treturn 0;\n\nsend_vport_meta_err:\n\treturn err;\n}\n\nstatic int\nesw_create_peer_esw_miss_group(struct mlx5_eswitch *esw,\n\t\t\t       struct mlx5_flow_table *fdb,\n\t\t\t       u32 *flow_group_in,\n\t\t\t       int *ix)\n{\n\tint max_peer_ports = (esw->total_vports - 1) * (MLX5_MAX_PORTS - 1);\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_flow_group *g;\n\tvoid *match_criteria;\n\tint err = 0;\n\n\tif (!MLX5_CAP_ESW(esw->dev, merged_eswitch))\n\t\treturn 0;\n\n\tmemset(flow_group_in, 0, inlen);\n\n\tmlx5_esw_set_flow_group_source_port(esw, flow_group_in, 0);\n\n\tif (!mlx5_eswitch_vport_match_metadata_enabled(esw)) {\n\t\tmatch_criteria = MLX5_ADDR_OF(create_flow_group_in,\n\t\t\t\t\t      flow_group_in,\n\t\t\t\t\t      match_criteria);\n\n\t\tMLX5_SET_TO_ONES(fte_match_param, match_criteria,\n\t\t\t\t misc_parameters.source_eswitch_owner_vhca_id);\n\n\t\tMLX5_SET(create_flow_group_in, flow_group_in,\n\t\t\t source_eswitch_owner_vhca_id_valid, 1);\n\t}\n\n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, *ix);\n\tMLX5_SET(create_flow_group_in, flow_group_in, end_flow_index,\n\t\t *ix + max_peer_ports);\n\t*ix += max_peer_ports + 1;\n\n\tg = mlx5_create_flow_group(fdb, flow_group_in);\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tesw_warn(esw->dev, \"Failed to create peer miss flow group err(%d)\\n\", err);\n\t\tgoto out;\n\t}\n\tesw->fdb_table.offloads.peer_miss_grp = g;\n\nout:\n\treturn err;\n}\n\nstatic int\nesw_create_miss_group(struct mlx5_eswitch *esw,\n\t\t      struct mlx5_flow_table *fdb,\n\t\t      u32 *flow_group_in,\n\t\t      int *ix)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_flow_group *g;\n\tvoid *match_criteria;\n\tint err = 0;\n\tu8 *dmac;\n\n\tmemset(flow_group_in, 0, inlen);\n\n\tMLX5_SET(create_flow_group_in, flow_group_in, match_criteria_enable,\n\t\t MLX5_MATCH_OUTER_HEADERS);\n\tmatch_criteria = MLX5_ADDR_OF(create_flow_group_in, flow_group_in,\n\t\t\t\t      match_criteria);\n\tdmac = MLX5_ADDR_OF(fte_match_param, match_criteria,\n\t\t\t    outer_headers.dmac_47_16);\n\tdmac[0] = 0x01;\n\n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, *ix);\n\tMLX5_SET(create_flow_group_in, flow_group_in, end_flow_index,\n\t\t *ix + MLX5_ESW_MISS_FLOWS);\n\n\tg = mlx5_create_flow_group(fdb, flow_group_in);\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tesw_warn(esw->dev, \"Failed to create miss flow group err(%d)\\n\", err);\n\t\tgoto miss_err;\n\t}\n\tesw->fdb_table.offloads.miss_grp = g;\n\n\terr = esw_add_fdb_miss_rule(esw);\n\tif (err)\n\t\tgoto miss_rule_err;\n\n\treturn 0;\n\nmiss_rule_err:\n\tmlx5_destroy_flow_group(esw->fdb_table.offloads.miss_grp);\nmiss_err:\n\treturn err;\n}\n\nstatic int esw_create_offloads_fdb_tables(struct mlx5_eswitch *esw)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_flow_table_attr ft_attr = {};\n\tstruct mlx5_core_dev *dev = esw->dev;\n\tstruct mlx5_flow_namespace *root_ns;\n\tstruct mlx5_flow_table *fdb = NULL;\n\tint table_size, ix = 0, err = 0;\n\tu32 flags = 0, *flow_group_in;\n\n\tesw_debug(esw->dev, \"Create offloads FDB Tables\\n\");\n\n\tflow_group_in = kvzalloc(inlen, GFP_KERNEL);\n\tif (!flow_group_in)\n\t\treturn -ENOMEM;\n\n\troot_ns = mlx5_get_flow_namespace(dev, MLX5_FLOW_NAMESPACE_FDB);\n\tif (!root_ns) {\n\t\tesw_warn(dev, \"Failed to get FDB flow namespace\\n\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto ns_err;\n\t}\n\tesw->fdb_table.offloads.ns = root_ns;\n\terr = mlx5_flow_namespace_set_mode(root_ns,\n\t\t\t\t\t   esw->dev->priv.steering->mode);\n\tif (err) {\n\t\tesw_warn(dev, \"Failed to set FDB namespace steering mode\\n\");\n\t\tgoto ns_err;\n\t}\n\n\t \n\ttable_size = MLX5_MAX_PORTS * (esw->total_vports * MAX_SQ_NVPORTS + MAX_PF_SQ) +\n\t\t     esw->total_vports * MLX5_MAX_PORTS + MLX5_ESW_MISS_FLOWS;\n\n\t \n\tif (esw->offloads.encap != DEVLINK_ESWITCH_ENCAP_MODE_NONE)\n\t\tflags |= (MLX5_FLOW_TABLE_TUNNEL_EN_REFORMAT |\n\t\t\t  MLX5_FLOW_TABLE_TUNNEL_EN_DECAP);\n\n\tft_attr.flags = flags;\n\tft_attr.max_fte = table_size;\n\tft_attr.prio = FDB_SLOW_PATH;\n\n\tfdb = mlx5_create_flow_table(root_ns, &ft_attr);\n\tif (IS_ERR(fdb)) {\n\t\terr = PTR_ERR(fdb);\n\t\tesw_warn(dev, \"Failed to create slow path FDB Table err %d\\n\", err);\n\t\tgoto slow_fdb_err;\n\t}\n\tesw->fdb_table.offloads.slow_fdb = fdb;\n\n\t \n\tmemset(&ft_attr, 0, sizeof(ft_attr));\n\tft_attr.prio = FDB_TC_MISS;\n\tesw->fdb_table.offloads.tc_miss_table = mlx5_create_flow_table(root_ns, &ft_attr);\n\tif (IS_ERR(esw->fdb_table.offloads.tc_miss_table)) {\n\t\terr = PTR_ERR(esw->fdb_table.offloads.tc_miss_table);\n\t\tesw_warn(dev, \"Failed to create TC miss FDB Table err %d\\n\", err);\n\t\tgoto tc_miss_table_err;\n\t}\n\n\terr = esw_chains_create(esw, esw->fdb_table.offloads.tc_miss_table);\n\tif (err) {\n\t\tesw_warn(dev, \"Failed to open fdb chains err(%d)\\n\", err);\n\t\tgoto fdb_chains_err;\n\t}\n\n\terr = esw_create_send_to_vport_group(esw, fdb, flow_group_in, &ix);\n\tif (err)\n\t\tgoto send_vport_err;\n\n\terr = esw_create_meta_send_to_vport_group(esw, fdb, flow_group_in, &ix);\n\tif (err)\n\t\tgoto send_vport_meta_err;\n\n\terr = esw_create_peer_esw_miss_group(esw, fdb, flow_group_in, &ix);\n\tif (err)\n\t\tgoto peer_miss_err;\n\n\terr = esw_create_miss_group(esw, fdb, flow_group_in, &ix);\n\tif (err)\n\t\tgoto miss_err;\n\n\tkvfree(flow_group_in);\n\treturn 0;\n\nmiss_err:\n\tif (MLX5_CAP_ESW(esw->dev, merged_eswitch))\n\t\tmlx5_destroy_flow_group(esw->fdb_table.offloads.peer_miss_grp);\npeer_miss_err:\n\tif (esw->fdb_table.offloads.send_to_vport_meta_grp)\n\t\tmlx5_destroy_flow_group(esw->fdb_table.offloads.send_to_vport_meta_grp);\nsend_vport_meta_err:\n\tmlx5_destroy_flow_group(esw->fdb_table.offloads.send_to_vport_grp);\nsend_vport_err:\n\tesw_chains_destroy(esw, esw_chains(esw));\nfdb_chains_err:\n\tmlx5_destroy_flow_table(esw->fdb_table.offloads.tc_miss_table);\ntc_miss_table_err:\n\tmlx5_destroy_flow_table(mlx5_eswitch_get_slow_fdb(esw));\nslow_fdb_err:\n\t \n\tmlx5_flow_namespace_set_mode(root_ns, MLX5_FLOW_STEERING_MODE_DMFS);\nns_err:\n\tkvfree(flow_group_in);\n\treturn err;\n}\n\nstatic void esw_destroy_offloads_fdb_tables(struct mlx5_eswitch *esw)\n{\n\tif (!mlx5_eswitch_get_slow_fdb(esw))\n\t\treturn;\n\n\tesw_debug(esw->dev, \"Destroy offloads FDB Tables\\n\");\n\tmlx5_del_flow_rules(esw->fdb_table.offloads.miss_rule_multi);\n\tmlx5_del_flow_rules(esw->fdb_table.offloads.miss_rule_uni);\n\tmlx5_destroy_flow_group(esw->fdb_table.offloads.send_to_vport_grp);\n\tif (esw->fdb_table.offloads.send_to_vport_meta_grp)\n\t\tmlx5_destroy_flow_group(esw->fdb_table.offloads.send_to_vport_meta_grp);\n\tif (MLX5_CAP_ESW(esw->dev, merged_eswitch))\n\t\tmlx5_destroy_flow_group(esw->fdb_table.offloads.peer_miss_grp);\n\tmlx5_destroy_flow_group(esw->fdb_table.offloads.miss_grp);\n\n\tesw_chains_destroy(esw, esw_chains(esw));\n\n\tmlx5_destroy_flow_table(esw->fdb_table.offloads.tc_miss_table);\n\tmlx5_destroy_flow_table(mlx5_eswitch_get_slow_fdb(esw));\n\t \n\tmlx5_flow_namespace_set_mode(esw->fdb_table.offloads.ns,\n\t\t\t\t     MLX5_FLOW_STEERING_MODE_DMFS);\n\tatomic64_set(&esw->user_count, 0);\n}\n\nstatic int esw_get_nr_ft_offloads_steering_src_ports(struct mlx5_eswitch *esw)\n{\n\tint nvports;\n\n\tnvports = esw->total_vports + MLX5_ESW_MISS_FLOWS;\n\tif (mlx5e_tc_int_port_supported(esw))\n\t\tnvports += MLX5E_TC_MAX_INT_PORT_NUM;\n\n\treturn nvports;\n}\n\nstatic int esw_create_offloads_table(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_flow_table_attr ft_attr = {};\n\tstruct mlx5_core_dev *dev = esw->dev;\n\tstruct mlx5_flow_table *ft_offloads;\n\tstruct mlx5_flow_namespace *ns;\n\tint err = 0;\n\n\tns = mlx5_get_flow_namespace(dev, MLX5_FLOW_NAMESPACE_OFFLOADS);\n\tif (!ns) {\n\t\tesw_warn(esw->dev, \"Failed to get offloads flow namespace\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tft_attr.max_fte = esw_get_nr_ft_offloads_steering_src_ports(esw) +\n\t\t\t  MLX5_ESW_FT_OFFLOADS_DROP_RULE;\n\tft_attr.prio = 1;\n\n\tft_offloads = mlx5_create_flow_table(ns, &ft_attr);\n\tif (IS_ERR(ft_offloads)) {\n\t\terr = PTR_ERR(ft_offloads);\n\t\tesw_warn(esw->dev, \"Failed to create offloads table, err %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tesw->offloads.ft_offloads = ft_offloads;\n\treturn 0;\n}\n\nstatic void esw_destroy_offloads_table(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_esw_offload *offloads = &esw->offloads;\n\n\tmlx5_destroy_flow_table(offloads->ft_offloads);\n}\n\nstatic int esw_create_vport_rx_group(struct mlx5_eswitch *esw)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_flow_group *g;\n\tu32 *flow_group_in;\n\tint nvports;\n\tint err = 0;\n\n\tnvports = esw_get_nr_ft_offloads_steering_src_ports(esw);\n\tflow_group_in = kvzalloc(inlen, GFP_KERNEL);\n\tif (!flow_group_in)\n\t\treturn -ENOMEM;\n\n\tmlx5_esw_set_flow_group_source_port(esw, flow_group_in, 0);\n\n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, 0);\n\tMLX5_SET(create_flow_group_in, flow_group_in, end_flow_index, nvports - 1);\n\n\tg = mlx5_create_flow_group(esw->offloads.ft_offloads, flow_group_in);\n\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tmlx5_core_warn(esw->dev, \"Failed to create vport rx group err %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\tesw->offloads.vport_rx_group = g;\nout:\n\tkvfree(flow_group_in);\n\treturn err;\n}\n\nstatic void esw_destroy_vport_rx_group(struct mlx5_eswitch *esw)\n{\n\tmlx5_destroy_flow_group(esw->offloads.vport_rx_group);\n}\n\nstatic int esw_create_vport_rx_drop_rule_index(struct mlx5_eswitch *esw)\n{\n\t \n\treturn esw_get_nr_ft_offloads_steering_src_ports(esw);\n}\n\nstatic int esw_create_vport_rx_drop_group(struct mlx5_eswitch *esw)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_flow_group *g;\n\tu32 *flow_group_in;\n\tint flow_index;\n\tint err = 0;\n\n\tflow_index = esw_create_vport_rx_drop_rule_index(esw);\n\n\tflow_group_in = kvzalloc(inlen, GFP_KERNEL);\n\tif (!flow_group_in)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, flow_index);\n\tMLX5_SET(create_flow_group_in, flow_group_in, end_flow_index, flow_index);\n\n\tg = mlx5_create_flow_group(esw->offloads.ft_offloads, flow_group_in);\n\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tmlx5_core_warn(esw->dev, \"Failed to create vport rx drop group err %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\tesw->offloads.vport_rx_drop_group = g;\nout:\n\tkvfree(flow_group_in);\n\treturn err;\n}\n\nstatic void esw_destroy_vport_rx_drop_group(struct mlx5_eswitch *esw)\n{\n\tif (esw->offloads.vport_rx_drop_group)\n\t\tmlx5_destroy_flow_group(esw->offloads.vport_rx_drop_group);\n}\n\nvoid\nmlx5_esw_set_spec_source_port(struct mlx5_eswitch *esw,\n\t\t\t      u16 vport,\n\t\t\t      struct mlx5_flow_spec *spec)\n{\n\tvoid *misc;\n\n\tif (mlx5_eswitch_vport_match_metadata_enabled(esw)) {\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,\n\t\t\t mlx5_eswitch_get_vport_metadata_for_match(esw, vport));\n\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters_2);\n\t\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,\n\t\t\t mlx5_eswitch_get_vport_metadata_mask());\n\n\t\tspec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS_2;\n\t} else {\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value, misc_parameters);\n\t\tMLX5_SET(fte_match_set_misc, misc, source_port, vport);\n\n\t\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters);\n\t\tMLX5_SET_TO_ONES(fte_match_set_misc, misc, source_port);\n\n\t\tspec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS;\n\t}\n}\n\nstruct mlx5_flow_handle *\nmlx5_eswitch_create_vport_rx_rule(struct mlx5_eswitch *esw, u16 vport,\n\t\t\t\t  struct mlx5_flow_destination *dest)\n{\n\tstruct mlx5_flow_act flow_act = {0};\n\tstruct mlx5_flow_handle *flow_rule;\n\tstruct mlx5_flow_spec *spec;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec) {\n\t\tflow_rule = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tmlx5_esw_set_spec_source_port(esw, vport, spec);\n\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;\n\tflow_rule = mlx5_add_flow_rules(esw->offloads.ft_offloads, spec,\n\t\t\t\t\t&flow_act, dest, 1);\n\tif (IS_ERR(flow_rule)) {\n\t\tesw_warn(esw->dev, \"fs offloads: Failed to add vport rx rule err %ld\\n\", PTR_ERR(flow_rule));\n\t\tgoto out;\n\t}\n\nout:\n\tkvfree(spec);\n\treturn flow_rule;\n}\n\nstatic int esw_create_vport_rx_drop_rule(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_flow_act flow_act = {};\n\tstruct mlx5_flow_handle *flow_rule;\n\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_DROP;\n\tflow_rule = mlx5_add_flow_rules(esw->offloads.ft_offloads, NULL,\n\t\t\t\t\t&flow_act, NULL, 0);\n\tif (IS_ERR(flow_rule)) {\n\t\tesw_warn(esw->dev,\n\t\t\t \"fs offloads: Failed to add vport rx drop rule err %ld\\n\",\n\t\t\t PTR_ERR(flow_rule));\n\t\treturn PTR_ERR(flow_rule);\n\t}\n\n\tesw->offloads.vport_rx_drop_rule = flow_rule;\n\n\treturn 0;\n}\n\nstatic void esw_destroy_vport_rx_drop_rule(struct mlx5_eswitch *esw)\n{\n\tif (esw->offloads.vport_rx_drop_rule)\n\t\tmlx5_del_flow_rules(esw->offloads.vport_rx_drop_rule);\n}\n\nstatic int mlx5_eswitch_inline_mode_get(struct mlx5_eswitch *esw, u8 *mode)\n{\n\tu8 prev_mlx5_mode, mlx5_mode = MLX5_INLINE_MODE_L2;\n\tstruct mlx5_core_dev *dev = esw->dev;\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tif (!MLX5_CAP_GEN(dev, vport_group_manager))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!mlx5_esw_is_fdb_created(esw))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (MLX5_CAP_ETH(dev, wqe_inline_mode)) {\n\tcase MLX5_CAP_INLINE_MODE_NOT_REQUIRED:\n\t\tmlx5_mode = MLX5_INLINE_MODE_NONE;\n\t\tgoto out;\n\tcase MLX5_CAP_INLINE_MODE_L2:\n\t\tmlx5_mode = MLX5_INLINE_MODE_L2;\n\t\tgoto out;\n\tcase MLX5_CAP_INLINE_MODE_VPORT_CONTEXT:\n\t\tgoto query_vports;\n\t}\n\nquery_vports:\n\tmlx5_query_nic_vport_min_inline(dev, esw->first_host_vport, &prev_mlx5_mode);\n\tmlx5_esw_for_each_host_func_vport(esw, i, vport, esw->esw_funcs.num_vfs) {\n\t\tmlx5_query_nic_vport_min_inline(dev, vport->vport, &mlx5_mode);\n\t\tif (prev_mlx5_mode != mlx5_mode)\n\t\t\treturn -EINVAL;\n\t\tprev_mlx5_mode = mlx5_mode;\n\t}\n\nout:\n\t*mode = mlx5_mode;\n\treturn 0;\n}\n\nstatic void esw_destroy_restore_table(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_esw_offload *offloads = &esw->offloads;\n\n\tif (!mlx5_eswitch_reg_c1_loopback_supported(esw))\n\t\treturn;\n\n\tmlx5_modify_header_dealloc(esw->dev, offloads->restore_copy_hdr_id);\n\tmlx5_destroy_flow_group(offloads->restore_group);\n\tmlx5_destroy_flow_table(offloads->ft_offloads_restore);\n}\n\nstatic int esw_create_restore_table(struct mlx5_eswitch *esw)\n{\n\tu8 modact[MLX5_UN_SZ_BYTES(set_add_copy_action_in_auto)] = {};\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_flow_table_attr ft_attr = {};\n\tstruct mlx5_core_dev *dev = esw->dev;\n\tstruct mlx5_flow_namespace *ns;\n\tstruct mlx5_modify_hdr *mod_hdr;\n\tvoid *match_criteria, *misc;\n\tstruct mlx5_flow_table *ft;\n\tstruct mlx5_flow_group *g;\n\tu32 *flow_group_in;\n\tint err = 0;\n\n\tif (!mlx5_eswitch_reg_c1_loopback_supported(esw))\n\t\treturn 0;\n\n\tns = mlx5_get_flow_namespace(dev, MLX5_FLOW_NAMESPACE_OFFLOADS);\n\tif (!ns) {\n\t\tesw_warn(esw->dev, \"Failed to get offloads flow namespace\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tflow_group_in = kvzalloc(inlen, GFP_KERNEL);\n\tif (!flow_group_in) {\n\t\terr = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\tft_attr.max_fte = 1 << ESW_REG_C0_USER_DATA_METADATA_BITS;\n\tft = mlx5_create_flow_table(ns, &ft_attr);\n\tif (IS_ERR(ft)) {\n\t\terr = PTR_ERR(ft);\n\t\tesw_warn(esw->dev, \"Failed to create restore table, err %d\\n\",\n\t\t\t err);\n\t\tgoto out_free;\n\t}\n\n\tmatch_criteria = MLX5_ADDR_OF(create_flow_group_in, flow_group_in,\n\t\t\t\t      match_criteria);\n\tmisc = MLX5_ADDR_OF(fte_match_param, match_criteria,\n\t\t\t    misc_parameters_2);\n\n\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_0,\n\t\t ESW_REG_C0_USER_DATA_METADATA_MASK);\n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, 0);\n\tMLX5_SET(create_flow_group_in, flow_group_in, end_flow_index,\n\t\t ft_attr.max_fte - 1);\n\tMLX5_SET(create_flow_group_in, flow_group_in, match_criteria_enable,\n\t\t MLX5_MATCH_MISC_PARAMETERS_2);\n\tg = mlx5_create_flow_group(ft, flow_group_in);\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tesw_warn(dev, \"Failed to create restore flow group, err: %d\\n\",\n\t\t\t err);\n\t\tgoto err_group;\n\t}\n\n\tMLX5_SET(copy_action_in, modact, action_type, MLX5_ACTION_TYPE_COPY);\n\tMLX5_SET(copy_action_in, modact, src_field,\n\t\t MLX5_ACTION_IN_FIELD_METADATA_REG_C_1);\n\tMLX5_SET(copy_action_in, modact, dst_field,\n\t\t MLX5_ACTION_IN_FIELD_METADATA_REG_B);\n\tmod_hdr = mlx5_modify_header_alloc(esw->dev,\n\t\t\t\t\t   MLX5_FLOW_NAMESPACE_KERNEL, 1,\n\t\t\t\t\t   modact);\n\tif (IS_ERR(mod_hdr)) {\n\t\terr = PTR_ERR(mod_hdr);\n\t\tesw_warn(dev, \"Failed to create restore mod header, err: %d\\n\",\n\t\t\t err);\n\t\tgoto err_mod_hdr;\n\t}\n\n\tesw->offloads.ft_offloads_restore = ft;\n\tesw->offloads.restore_group = g;\n\tesw->offloads.restore_copy_hdr_id = mod_hdr;\n\n\tkvfree(flow_group_in);\n\n\treturn 0;\n\nerr_mod_hdr:\n\tmlx5_destroy_flow_group(g);\nerr_group:\n\tmlx5_destroy_flow_table(ft);\nout_free:\n\tkvfree(flow_group_in);\n\n\treturn err;\n}\n\nstatic int esw_offloads_start(struct mlx5_eswitch *esw,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tint err;\n\n\tesw->mode = MLX5_ESWITCH_OFFLOADS;\n\terr = mlx5_eswitch_enable_locked(esw, esw->dev->priv.sriov.num_vfs);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Failed setting eswitch to offloads\");\n\t\tesw->mode = MLX5_ESWITCH_LEGACY;\n\t\tmlx5_rescan_drivers(esw->dev);\n\t\treturn err;\n\t}\n\tif (esw->offloads.inline_mode == MLX5_INLINE_MODE_NONE) {\n\t\tif (mlx5_eswitch_inline_mode_get(esw,\n\t\t\t\t\t\t &esw->offloads.inline_mode)) {\n\t\t\tesw->offloads.inline_mode = MLX5_INLINE_MODE_L2;\n\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t   \"Inline mode is different between vports\");\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int mlx5_esw_offloads_rep_init(struct mlx5_eswitch *esw, const struct mlx5_vport *vport)\n{\n\tstruct mlx5_eswitch_rep *rep;\n\tint rep_type;\n\tint err;\n\n\trep = kzalloc(sizeof(*rep), GFP_KERNEL);\n\tif (!rep)\n\t\treturn -ENOMEM;\n\n\trep->vport = vport->vport;\n\trep->vport_index = vport->index;\n\tfor (rep_type = 0; rep_type < NUM_REP_TYPES; rep_type++)\n\t\tatomic_set(&rep->rep_data[rep_type].state, REP_UNREGISTERED);\n\n\terr = xa_insert(&esw->offloads.vport_reps, rep->vport, rep, GFP_KERNEL);\n\tif (err)\n\t\tgoto insert_err;\n\n\treturn 0;\n\ninsert_err:\n\tkfree(rep);\n\treturn err;\n}\n\nstatic void mlx5_esw_offloads_rep_cleanup(struct mlx5_eswitch *esw,\n\t\t\t\t\t  struct mlx5_eswitch_rep *rep)\n{\n\txa_erase(&esw->offloads.vport_reps, rep->vport);\n\tkfree(rep);\n}\n\nstatic void esw_offloads_cleanup_reps(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_eswitch_rep *rep;\n\tunsigned long i;\n\n\tmlx5_esw_for_each_rep(esw, i, rep)\n\t\tmlx5_esw_offloads_rep_cleanup(esw, rep);\n\txa_destroy(&esw->offloads.vport_reps);\n}\n\nstatic int esw_offloads_init_reps(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\tint err;\n\n\txa_init(&esw->offloads.vport_reps);\n\n\tmlx5_esw_for_each_vport(esw, i, vport) {\n\t\terr = mlx5_esw_offloads_rep_init(esw, vport);\n\t\tif (err)\n\t\t\tgoto err;\n\t}\n\treturn 0;\n\nerr:\n\tesw_offloads_cleanup_reps(esw);\n\treturn err;\n}\n\nstatic int esw_port_metadata_set(struct devlink *devlink, u32 id,\n\t\t\t\t struct devlink_param_gset_ctx *ctx)\n{\n\tstruct mlx5_core_dev *dev = devlink_priv(devlink);\n\tstruct mlx5_eswitch *esw = dev->priv.eswitch;\n\tint err = 0;\n\n\tdown_write(&esw->mode_lock);\n\tif (mlx5_esw_is_fdb_created(esw)) {\n\t\terr = -EBUSY;\n\t\tgoto done;\n\t}\n\tif (!mlx5_esw_vport_match_metadata_supported(esw)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto done;\n\t}\n\tif (ctx->val.vbool)\n\t\tesw->flags |= MLX5_ESWITCH_VPORT_MATCH_METADATA;\n\telse\n\t\tesw->flags &= ~MLX5_ESWITCH_VPORT_MATCH_METADATA;\ndone:\n\tup_write(&esw->mode_lock);\n\treturn err;\n}\n\nstatic int esw_port_metadata_get(struct devlink *devlink, u32 id,\n\t\t\t\t struct devlink_param_gset_ctx *ctx)\n{\n\tstruct mlx5_core_dev *dev = devlink_priv(devlink);\n\n\tctx->val.vbool = mlx5_eswitch_vport_match_metadata_enabled(dev->priv.eswitch);\n\treturn 0;\n}\n\nstatic int esw_port_metadata_validate(struct devlink *devlink, u32 id,\n\t\t\t\t      union devlink_param_value val,\n\t\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_core_dev *dev = devlink_priv(devlink);\n\tu8 esw_mode;\n\n\tesw_mode = mlx5_eswitch_mode(dev);\n\tif (esw_mode == MLX5_ESWITCH_OFFLOADS) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"E-Switch must either disabled or non switchdev mode\");\n\t\treturn -EBUSY;\n\t}\n\treturn 0;\n}\n\nstatic const struct devlink_param esw_devlink_params[] = {\n\tDEVLINK_PARAM_DRIVER(MLX5_DEVLINK_PARAM_ID_ESW_PORT_METADATA,\n\t\t\t     \"esw_port_metadata\", DEVLINK_PARAM_TYPE_BOOL,\n\t\t\t     BIT(DEVLINK_PARAM_CMODE_RUNTIME),\n\t\t\t     esw_port_metadata_get,\n\t\t\t     esw_port_metadata_set,\n\t\t\t     esw_port_metadata_validate),\n};\n\nint esw_offloads_init(struct mlx5_eswitch *esw)\n{\n\tint err;\n\n\terr = esw_offloads_init_reps(esw);\n\tif (err)\n\t\treturn err;\n\n\terr = devl_params_register(priv_to_devlink(esw->dev),\n\t\t\t\t   esw_devlink_params,\n\t\t\t\t   ARRAY_SIZE(esw_devlink_params));\n\tif (err)\n\t\tgoto err_params;\n\n\treturn 0;\n\nerr_params:\n\tesw_offloads_cleanup_reps(esw);\n\treturn err;\n}\n\nvoid esw_offloads_cleanup(struct mlx5_eswitch *esw)\n{\n\tdevl_params_unregister(priv_to_devlink(esw->dev),\n\t\t\t       esw_devlink_params,\n\t\t\t       ARRAY_SIZE(esw_devlink_params));\n\tesw_offloads_cleanup_reps(esw);\n}\n\nstatic void __esw_offloads_unload_rep(struct mlx5_eswitch *esw,\n\t\t\t\t      struct mlx5_eswitch_rep *rep, u8 rep_type)\n{\n\tif (atomic_cmpxchg(&rep->rep_data[rep_type].state,\n\t\t\t   REP_LOADED, REP_REGISTERED) == REP_LOADED)\n\t\tesw->offloads.rep_ops[rep_type]->unload(rep);\n}\n\nstatic void __unload_reps_all_vport(struct mlx5_eswitch *esw, u8 rep_type)\n{\n\tstruct mlx5_eswitch_rep *rep;\n\tunsigned long i;\n\n\tmlx5_esw_for_each_rep(esw, i, rep)\n\t\t__esw_offloads_unload_rep(esw, rep, rep_type);\n}\n\nstatic int mlx5_esw_offloads_rep_load(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\tstruct mlx5_eswitch_rep *rep;\n\tint rep_type;\n\tint err;\n\n\trep = mlx5_eswitch_get_rep(esw, vport_num);\n\tfor (rep_type = 0; rep_type < NUM_REP_TYPES; rep_type++)\n\t\tif (atomic_cmpxchg(&rep->rep_data[rep_type].state,\n\t\t\t\t   REP_REGISTERED, REP_LOADED) == REP_REGISTERED) {\n\t\t\terr = esw->offloads.rep_ops[rep_type]->load(esw->dev, rep);\n\t\t\tif (err)\n\t\t\t\tgoto err_reps;\n\t\t}\n\n\treturn 0;\n\nerr_reps:\n\tatomic_set(&rep->rep_data[rep_type].state, REP_REGISTERED);\n\tfor (--rep_type; rep_type >= 0; rep_type--)\n\t\t__esw_offloads_unload_rep(esw, rep, rep_type);\n\treturn err;\n}\n\nstatic void mlx5_esw_offloads_rep_unload(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\tstruct mlx5_eswitch_rep *rep;\n\tint rep_type;\n\n\trep = mlx5_eswitch_get_rep(esw, vport_num);\n\tfor (rep_type = NUM_REP_TYPES - 1; rep_type >= 0; rep_type--)\n\t\t__esw_offloads_unload_rep(esw, rep, rep_type);\n}\n\nint mlx5_esw_offloads_init_pf_vf_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tif (esw->mode != MLX5_ESWITCH_OFFLOADS)\n\t\treturn 0;\n\n\treturn mlx5_esw_offloads_pf_vf_devlink_port_init(esw, vport);\n}\n\nvoid mlx5_esw_offloads_cleanup_pf_vf_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tif (esw->mode != MLX5_ESWITCH_OFFLOADS)\n\t\treturn;\n\n\tmlx5_esw_offloads_pf_vf_devlink_port_cleanup(esw, vport);\n}\n\nint mlx5_esw_offloads_init_sf_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport,\n\t\t\t\t  struct mlx5_devlink_port *dl_port,\n\t\t\t\t  u32 controller, u32 sfnum)\n{\n\treturn mlx5_esw_offloads_sf_devlink_port_init(esw, vport, dl_port, controller, sfnum);\n}\n\nvoid mlx5_esw_offloads_cleanup_sf_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tmlx5_esw_offloads_sf_devlink_port_cleanup(esw, vport);\n}\n\nint mlx5_esw_offloads_load_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tint err;\n\n\tif (esw->mode != MLX5_ESWITCH_OFFLOADS)\n\t\treturn 0;\n\n\terr = mlx5_esw_offloads_devlink_port_register(esw, vport);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_esw_offloads_rep_load(esw, vport->vport);\n\tif (err)\n\t\tgoto load_err;\n\treturn err;\n\nload_err:\n\tmlx5_esw_offloads_devlink_port_unregister(esw, vport);\n\treturn err;\n}\n\nvoid mlx5_esw_offloads_unload_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport)\n{\n\tif (esw->mode != MLX5_ESWITCH_OFFLOADS)\n\t\treturn;\n\n\tmlx5_esw_offloads_rep_unload(esw, vport->vport);\n\n\tmlx5_esw_offloads_devlink_port_unregister(esw, vport);\n}\n\nstatic int esw_set_slave_root_fdb(struct mlx5_core_dev *master,\n\t\t\t\t  struct mlx5_core_dev *slave)\n{\n\tu32 in[MLX5_ST_SZ_DW(set_flow_table_root_in)]   = {};\n\tu32 out[MLX5_ST_SZ_DW(set_flow_table_root_out)] = {};\n\tstruct mlx5_flow_root_namespace *root;\n\tstruct mlx5_flow_namespace *ns;\n\tint err;\n\n\tMLX5_SET(set_flow_table_root_in, in, opcode,\n\t\t MLX5_CMD_OP_SET_FLOW_TABLE_ROOT);\n\tMLX5_SET(set_flow_table_root_in, in, table_type,\n\t\t FS_FT_FDB);\n\n\tif (master) {\n\t\tns = mlx5_get_flow_namespace(master,\n\t\t\t\t\t     MLX5_FLOW_NAMESPACE_FDB);\n\t\troot = find_root(&ns->node);\n\t\tmutex_lock(&root->chain_lock);\n\t\tMLX5_SET(set_flow_table_root_in, in,\n\t\t\t table_eswitch_owner_vhca_id_valid, 1);\n\t\tMLX5_SET(set_flow_table_root_in, in,\n\t\t\t table_eswitch_owner_vhca_id,\n\t\t\t MLX5_CAP_GEN(master, vhca_id));\n\t\tMLX5_SET(set_flow_table_root_in, in, table_id,\n\t\t\t root->root_ft->id);\n\t} else {\n\t\tns = mlx5_get_flow_namespace(slave,\n\t\t\t\t\t     MLX5_FLOW_NAMESPACE_FDB);\n\t\troot = find_root(&ns->node);\n\t\tmutex_lock(&root->chain_lock);\n\t\tMLX5_SET(set_flow_table_root_in, in, table_id,\n\t\t\t root->root_ft->id);\n\t}\n\n\terr = mlx5_cmd_exec(slave, in, sizeof(in), out, sizeof(out));\n\tmutex_unlock(&root->chain_lock);\n\n\treturn err;\n}\n\nstatic int __esw_set_master_egress_rule(struct mlx5_core_dev *master,\n\t\t\t\t\tstruct mlx5_core_dev *slave,\n\t\t\t\t\tstruct mlx5_vport *vport,\n\t\t\t\t\tstruct mlx5_flow_table *acl)\n{\n\tu16 slave_index = MLX5_CAP_GEN(slave, vhca_id);\n\tstruct mlx5_flow_handle *flow_rule = NULL;\n\tstruct mlx5_flow_destination dest = {};\n\tstruct mlx5_flow_act flow_act = {};\n\tstruct mlx5_flow_spec *spec;\n\tint err = 0;\n\tvoid *misc;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn -ENOMEM;\n\n\tspec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS;\n\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t    misc_parameters);\n\tMLX5_SET(fte_match_set_misc, misc, source_port, MLX5_VPORT_UPLINK);\n\tMLX5_SET(fte_match_set_misc, misc, source_eswitch_owner_vhca_id, slave_index);\n\n\tmisc = MLX5_ADDR_OF(fte_match_param, spec->match_criteria, misc_parameters);\n\tMLX5_SET_TO_ONES(fte_match_set_misc, misc, source_port);\n\tMLX5_SET_TO_ONES(fte_match_set_misc, misc,\n\t\t\t source_eswitch_owner_vhca_id);\n\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST;\n\tdest.type = MLX5_FLOW_DESTINATION_TYPE_VPORT;\n\tdest.vport.num = slave->priv.eswitch->manager_vport;\n\tdest.vport.vhca_id = MLX5_CAP_GEN(slave, vhca_id);\n\tdest.vport.flags |= MLX5_FLOW_DEST_VPORT_VHCA_ID;\n\n\tflow_rule = mlx5_add_flow_rules(acl, spec, &flow_act,\n\t\t\t\t\t&dest, 1);\n\tif (IS_ERR(flow_rule)) {\n\t\terr = PTR_ERR(flow_rule);\n\t} else {\n\t\terr = xa_insert(&vport->egress.offloads.bounce_rules,\n\t\t\t\tslave_index, flow_rule, GFP_KERNEL);\n\t\tif (err)\n\t\t\tmlx5_del_flow_rules(flow_rule);\n\t}\n\n\tkvfree(spec);\n\treturn err;\n}\n\nstatic int esw_master_egress_create_resources(struct mlx5_eswitch *esw,\n\t\t\t\t\t      struct mlx5_flow_namespace *egress_ns,\n\t\t\t\t\t      struct mlx5_vport *vport, size_t count)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_flow_table_attr ft_attr = {\n\t\t.max_fte = count, .prio = 0, .level = 0,\n\t};\n\tstruct mlx5_flow_table *acl;\n\tstruct mlx5_flow_group *g;\n\tvoid *match_criteria;\n\tu32 *flow_group_in;\n\tint err;\n\n\tif (vport->egress.acl)\n\t\treturn 0;\n\n\tflow_group_in = kvzalloc(inlen, GFP_KERNEL);\n\tif (!flow_group_in)\n\t\treturn -ENOMEM;\n\n\tif (vport->vport || mlx5_core_is_ecpf(esw->dev))\n\t\tft_attr.flags = MLX5_FLOW_TABLE_OTHER_VPORT;\n\n\tacl = mlx5_create_vport_flow_table(egress_ns, &ft_attr, vport->vport);\n\tif (IS_ERR(acl)) {\n\t\terr = PTR_ERR(acl);\n\t\tgoto out;\n\t}\n\n\tmatch_criteria = MLX5_ADDR_OF(create_flow_group_in, flow_group_in,\n\t\t\t\t      match_criteria);\n\tMLX5_SET_TO_ONES(fte_match_param, match_criteria,\n\t\t\t misc_parameters.source_port);\n\tMLX5_SET_TO_ONES(fte_match_param, match_criteria,\n\t\t\t misc_parameters.source_eswitch_owner_vhca_id);\n\tMLX5_SET(create_flow_group_in, flow_group_in, match_criteria_enable,\n\t\t MLX5_MATCH_MISC_PARAMETERS);\n\n\tMLX5_SET(create_flow_group_in, flow_group_in,\n\t\t source_eswitch_owner_vhca_id_valid, 1);\n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, 0);\n\tMLX5_SET(create_flow_group_in, flow_group_in, end_flow_index, count);\n\n\tg = mlx5_create_flow_group(acl, flow_group_in);\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tgoto err_group;\n\t}\n\n\tvport->egress.acl = acl;\n\tvport->egress.offloads.bounce_grp = g;\n\tvport->egress.type = VPORT_EGRESS_ACL_TYPE_SHARED_FDB;\n\txa_init_flags(&vport->egress.offloads.bounce_rules, XA_FLAGS_ALLOC);\n\n\tkvfree(flow_group_in);\n\n\treturn 0;\n\nerr_group:\n\tmlx5_destroy_flow_table(acl);\nout:\n\tkvfree(flow_group_in);\n\treturn err;\n}\n\nstatic void esw_master_egress_destroy_resources(struct mlx5_vport *vport)\n{\n\tif (!xa_empty(&vport->egress.offloads.bounce_rules))\n\t\treturn;\n\tmlx5_destroy_flow_group(vport->egress.offloads.bounce_grp);\n\tvport->egress.offloads.bounce_grp = NULL;\n\tmlx5_destroy_flow_table(vport->egress.acl);\n\tvport->egress.acl = NULL;\n}\n\nstatic int esw_set_master_egress_rule(struct mlx5_core_dev *master,\n\t\t\t\t      struct mlx5_core_dev *slave, size_t count)\n{\n\tstruct mlx5_eswitch *esw = master->priv.eswitch;\n\tu16 slave_index = MLX5_CAP_GEN(slave, vhca_id);\n\tstruct mlx5_flow_namespace *egress_ns;\n\tstruct mlx5_vport *vport;\n\tint err;\n\n\tvport = mlx5_eswitch_get_vport(esw, esw->manager_vport);\n\tif (IS_ERR(vport))\n\t\treturn PTR_ERR(vport);\n\n\tegress_ns = mlx5_get_flow_vport_acl_namespace(master,\n\t\t\t\t\t\t      MLX5_FLOW_NAMESPACE_ESW_EGRESS,\n\t\t\t\t\t\t      vport->index);\n\tif (!egress_ns)\n\t\treturn -EINVAL;\n\n\tif (vport->egress.acl && vport->egress.type != VPORT_EGRESS_ACL_TYPE_SHARED_FDB)\n\t\treturn 0;\n\n\terr = esw_master_egress_create_resources(esw, egress_ns, vport, count);\n\tif (err)\n\t\treturn err;\n\n\tif (xa_load(&vport->egress.offloads.bounce_rules, slave_index))\n\t\treturn -EINVAL;\n\n\terr = __esw_set_master_egress_rule(master, slave, vport, vport->egress.acl);\n\tif (err)\n\t\tgoto err_rule;\n\n\treturn 0;\n\nerr_rule:\n\tesw_master_egress_destroy_resources(vport);\n\treturn err;\n}\n\nstatic void esw_unset_master_egress_rule(struct mlx5_core_dev *dev,\n\t\t\t\t\t struct mlx5_core_dev *slave_dev)\n{\n\tstruct mlx5_vport *vport;\n\n\tvport = mlx5_eswitch_get_vport(dev->priv.eswitch,\n\t\t\t\t       dev->priv.eswitch->manager_vport);\n\n\tesw_acl_egress_ofld_bounce_rule_destroy(vport, MLX5_CAP_GEN(slave_dev, vhca_id));\n\n\tif (xa_empty(&vport->egress.offloads.bounce_rules)) {\n\t\tesw_acl_egress_ofld_cleanup(vport);\n\t\txa_destroy(&vport->egress.offloads.bounce_rules);\n\t}\n}\n\nint mlx5_eswitch_offloads_single_fdb_add_one(struct mlx5_eswitch *master_esw,\n\t\t\t\t\t     struct mlx5_eswitch *slave_esw, int max_slaves)\n{\n\tint err;\n\n\terr = esw_set_slave_root_fdb(master_esw->dev,\n\t\t\t\t     slave_esw->dev);\n\tif (err)\n\t\treturn err;\n\n\terr = esw_set_master_egress_rule(master_esw->dev,\n\t\t\t\t\t slave_esw->dev, max_slaves);\n\tif (err)\n\t\tgoto err_acl;\n\n\treturn err;\n\nerr_acl:\n\tesw_set_slave_root_fdb(NULL, slave_esw->dev);\n\treturn err;\n}\n\nvoid mlx5_eswitch_offloads_single_fdb_del_one(struct mlx5_eswitch *master_esw,\n\t\t\t\t\t      struct mlx5_eswitch *slave_esw)\n{\n\tesw_set_slave_root_fdb(NULL, slave_esw->dev);\n\tesw_unset_master_egress_rule(master_esw->dev, slave_esw->dev);\n}\n\n#define ESW_OFFLOADS_DEVCOM_PAIR\t(0)\n#define ESW_OFFLOADS_DEVCOM_UNPAIR\t(1)\n\nstatic void mlx5_esw_offloads_rep_event_unpair(struct mlx5_eswitch *esw,\n\t\t\t\t\t       struct mlx5_eswitch *peer_esw)\n{\n\tconst struct mlx5_eswitch_rep_ops *ops;\n\tstruct mlx5_eswitch_rep *rep;\n\tunsigned long i;\n\tu8 rep_type;\n\n\tmlx5_esw_for_each_rep(esw, i, rep) {\n\t\trep_type = NUM_REP_TYPES;\n\t\twhile (rep_type--) {\n\t\t\tops = esw->offloads.rep_ops[rep_type];\n\t\t\tif (atomic_read(&rep->rep_data[rep_type].state) == REP_LOADED &&\n\t\t\t    ops->event)\n\t\t\t\tops->event(esw, rep, MLX5_SWITCHDEV_EVENT_UNPAIR, peer_esw);\n\t\t}\n\t}\n}\n\nstatic void mlx5_esw_offloads_unpair(struct mlx5_eswitch *esw,\n\t\t\t\t     struct mlx5_eswitch *peer_esw)\n{\n#if IS_ENABLED(CONFIG_MLX5_CLS_ACT)\n\tmlx5e_tc_clean_fdb_peer_flows(esw);\n#endif\n\tmlx5_esw_offloads_rep_event_unpair(esw, peer_esw);\n\tesw_del_fdb_peer_miss_rules(esw, peer_esw->dev);\n}\n\nstatic int mlx5_esw_offloads_pair(struct mlx5_eswitch *esw,\n\t\t\t\t  struct mlx5_eswitch *peer_esw)\n{\n\tconst struct mlx5_eswitch_rep_ops *ops;\n\tstruct mlx5_eswitch_rep *rep;\n\tunsigned long i;\n\tu8 rep_type;\n\tint err;\n\n\terr = esw_add_fdb_peer_miss_rules(esw, peer_esw->dev);\n\tif (err)\n\t\treturn err;\n\n\tmlx5_esw_for_each_rep(esw, i, rep) {\n\t\tfor (rep_type = 0; rep_type < NUM_REP_TYPES; rep_type++) {\n\t\t\tops = esw->offloads.rep_ops[rep_type];\n\t\t\tif (atomic_read(&rep->rep_data[rep_type].state) == REP_LOADED &&\n\t\t\t    ops->event) {\n\t\t\t\terr = ops->event(esw, rep, MLX5_SWITCHDEV_EVENT_PAIR, peer_esw);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_out:\n\tmlx5_esw_offloads_unpair(esw, peer_esw);\n\treturn err;\n}\n\nstatic int mlx5_esw_offloads_set_ns_peer(struct mlx5_eswitch *esw,\n\t\t\t\t\t struct mlx5_eswitch *peer_esw,\n\t\t\t\t\t bool pair)\n{\n\tu16 peer_vhca_id = MLX5_CAP_GEN(peer_esw->dev, vhca_id);\n\tu16 vhca_id = MLX5_CAP_GEN(esw->dev, vhca_id);\n\tstruct mlx5_flow_root_namespace *peer_ns;\n\tstruct mlx5_flow_root_namespace *ns;\n\tint err;\n\n\tpeer_ns = peer_esw->dev->priv.steering->fdb_root_ns;\n\tns = esw->dev->priv.steering->fdb_root_ns;\n\n\tif (pair) {\n\t\terr = mlx5_flow_namespace_set_peer(ns, peer_ns, peer_vhca_id);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = mlx5_flow_namespace_set_peer(peer_ns, ns, vhca_id);\n\t\tif (err) {\n\t\t\tmlx5_flow_namespace_set_peer(ns, NULL, peer_vhca_id);\n\t\t\treturn err;\n\t\t}\n\t} else {\n\t\tmlx5_flow_namespace_set_peer(ns, NULL, peer_vhca_id);\n\t\tmlx5_flow_namespace_set_peer(peer_ns, NULL, vhca_id);\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5_esw_offloads_devcom_event(int event,\n\t\t\t\t\t  void *my_data,\n\t\t\t\t\t  void *event_data)\n{\n\tstruct mlx5_eswitch *esw = my_data;\n\tstruct mlx5_eswitch *peer_esw = event_data;\n\tu16 esw_i, peer_esw_i;\n\tbool esw_paired;\n\tint err;\n\n\tpeer_esw_i = MLX5_CAP_GEN(peer_esw->dev, vhca_id);\n\tesw_i = MLX5_CAP_GEN(esw->dev, vhca_id);\n\tesw_paired = !!xa_load(&esw->paired, peer_esw_i);\n\n\tswitch (event) {\n\tcase ESW_OFFLOADS_DEVCOM_PAIR:\n\t\tif (mlx5_eswitch_vport_match_metadata_enabled(esw) !=\n\t\t    mlx5_eswitch_vport_match_metadata_enabled(peer_esw))\n\t\t\tbreak;\n\n\t\tif (esw_paired)\n\t\t\tbreak;\n\n\t\terr = mlx5_esw_offloads_set_ns_peer(esw, peer_esw, true);\n\t\tif (err)\n\t\t\tgoto err_out;\n\n\t\terr = mlx5_esw_offloads_pair(esw, peer_esw);\n\t\tif (err)\n\t\t\tgoto err_peer;\n\n\t\terr = mlx5_esw_offloads_pair(peer_esw, esw);\n\t\tif (err)\n\t\t\tgoto err_pair;\n\n\t\terr = xa_insert(&esw->paired, peer_esw_i, peer_esw, GFP_KERNEL);\n\t\tif (err)\n\t\t\tgoto err_xa;\n\n\t\terr = xa_insert(&peer_esw->paired, esw_i, esw, GFP_KERNEL);\n\t\tif (err)\n\t\t\tgoto err_peer_xa;\n\n\t\tesw->num_peers++;\n\t\tpeer_esw->num_peers++;\n\t\tmlx5_devcom_comp_set_ready(esw->devcom, true);\n\t\tbreak;\n\n\tcase ESW_OFFLOADS_DEVCOM_UNPAIR:\n\t\tif (!esw_paired)\n\t\t\tbreak;\n\n\t\tpeer_esw->num_peers--;\n\t\tesw->num_peers--;\n\t\tif (!esw->num_peers && !peer_esw->num_peers)\n\t\t\tmlx5_devcom_comp_set_ready(esw->devcom, false);\n\t\txa_erase(&peer_esw->paired, esw_i);\n\t\txa_erase(&esw->paired, peer_esw_i);\n\t\tmlx5_esw_offloads_unpair(peer_esw, esw);\n\t\tmlx5_esw_offloads_unpair(esw, peer_esw);\n\t\tmlx5_esw_offloads_set_ns_peer(esw, peer_esw, false);\n\t\tbreak;\n\t}\n\n\treturn 0;\n\nerr_peer_xa:\n\txa_erase(&esw->paired, peer_esw_i);\nerr_xa:\n\tmlx5_esw_offloads_unpair(peer_esw, esw);\nerr_pair:\n\tmlx5_esw_offloads_unpair(esw, peer_esw);\nerr_peer:\n\tmlx5_esw_offloads_set_ns_peer(esw, peer_esw, false);\nerr_out:\n\tmlx5_core_err(esw->dev, \"esw offloads devcom event failure, event %u err %d\",\n\t\t      event, err);\n\treturn err;\n}\n\nvoid mlx5_esw_offloads_devcom_init(struct mlx5_eswitch *esw, u64 key)\n{\n\tint i;\n\n\tfor (i = 0; i < MLX5_MAX_PORTS; i++)\n\t\tINIT_LIST_HEAD(&esw->offloads.peer_flows[i]);\n\tmutex_init(&esw->offloads.peer_mutex);\n\n\tif (!MLX5_CAP_ESW(esw->dev, merged_eswitch))\n\t\treturn;\n\n\tif ((MLX5_VPORT_MANAGER(esw->dev) || mlx5_core_is_ecpf_esw_manager(esw->dev)) &&\n\t    !mlx5_lag_is_supported(esw->dev))\n\t\treturn;\n\n\txa_init(&esw->paired);\n\tesw->num_peers = 0;\n\tesw->devcom = mlx5_devcom_register_component(esw->dev->priv.devc,\n\t\t\t\t\t\t     MLX5_DEVCOM_ESW_OFFLOADS,\n\t\t\t\t\t\t     key,\n\t\t\t\t\t\t     mlx5_esw_offloads_devcom_event,\n\t\t\t\t\t\t     esw);\n\tif (IS_ERR_OR_NULL(esw->devcom))\n\t\treturn;\n\n\tmlx5_devcom_send_event(esw->devcom,\n\t\t\t       ESW_OFFLOADS_DEVCOM_PAIR,\n\t\t\t       ESW_OFFLOADS_DEVCOM_UNPAIR,\n\t\t\t       esw);\n}\n\nvoid mlx5_esw_offloads_devcom_cleanup(struct mlx5_eswitch *esw)\n{\n\tif (IS_ERR_OR_NULL(esw->devcom))\n\t\treturn;\n\n\tmlx5_devcom_send_event(esw->devcom,\n\t\t\t       ESW_OFFLOADS_DEVCOM_UNPAIR,\n\t\t\t       ESW_OFFLOADS_DEVCOM_UNPAIR,\n\t\t\t       esw);\n\n\tmlx5_devcom_unregister_component(esw->devcom);\n\txa_destroy(&esw->paired);\n\tesw->devcom = NULL;\n}\n\nbool mlx5_esw_offloads_devcom_is_ready(struct mlx5_eswitch *esw)\n{\n\treturn mlx5_devcom_comp_is_ready(esw->devcom);\n}\n\nbool mlx5_esw_vport_match_metadata_supported(const struct mlx5_eswitch *esw)\n{\n\tif (!MLX5_CAP_ESW(esw->dev, esw_uplink_ingress_acl))\n\t\treturn false;\n\n\tif (!(MLX5_CAP_ESW_FLOWTABLE(esw->dev, fdb_to_vport_reg_c_id) &\n\t      MLX5_FDB_TO_VPORT_REG_C_0))\n\t\treturn false;\n\n\treturn true;\n}\n\n#define MLX5_ESW_METADATA_RSVD_UPLINK 1\n\n \nstatic u32 mlx5_esw_match_metadata_reserved(struct mlx5_eswitch *esw)\n{\n\treturn MLX5_ESW_METADATA_RSVD_UPLINK;\n}\n\nu32 mlx5_esw_match_metadata_alloc(struct mlx5_eswitch *esw)\n{\n\tu32 vport_end_ida = (1 << ESW_VPORT_BITS) - 1;\n\t \n\tu32 max_pf_num = (1 << ESW_PFNUM_BITS) - 2;\n\tu32 pf_num;\n\tint id;\n\n\t \n\tpf_num = mlx5_get_dev_index(esw->dev);\n\tif (pf_num > max_pf_num)\n\t\treturn 0;\n\n\t \n\t \n\tid = ida_alloc_range(&esw->offloads.vport_metadata_ida,\n\t\t\t     MLX5_ESW_METADATA_RSVD_UPLINK + 1,\n\t\t\t     vport_end_ida, GFP_KERNEL);\n\tif (id < 0)\n\t\treturn 0;\n\tid = (pf_num << ESW_VPORT_BITS) | id;\n\treturn id;\n}\n\nvoid mlx5_esw_match_metadata_free(struct mlx5_eswitch *esw, u32 metadata)\n{\n\tu32 vport_bit_mask = (1 << ESW_VPORT_BITS) - 1;\n\n\t \n\tida_free(&esw->offloads.vport_metadata_ida, metadata & vport_bit_mask);\n}\n\nstatic int esw_offloads_vport_metadata_setup(struct mlx5_eswitch *esw,\n\t\t\t\t\t     struct mlx5_vport *vport)\n{\n\tif (vport->vport == MLX5_VPORT_UPLINK)\n\t\tvport->default_metadata = mlx5_esw_match_metadata_reserved(esw);\n\telse\n\t\tvport->default_metadata = mlx5_esw_match_metadata_alloc(esw);\n\n\tvport->metadata = vport->default_metadata;\n\treturn vport->metadata ? 0 : -ENOSPC;\n}\n\nstatic void esw_offloads_vport_metadata_cleanup(struct mlx5_eswitch *esw,\n\t\t\t\t\t\tstruct mlx5_vport *vport)\n{\n\tif (!vport->default_metadata)\n\t\treturn;\n\n\tif (vport->vport == MLX5_VPORT_UPLINK)\n\t\treturn;\n\n\tWARN_ON(vport->metadata != vport->default_metadata);\n\tmlx5_esw_match_metadata_free(esw, vport->default_metadata);\n}\n\nstatic void esw_offloads_metadata_uninit(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\n\tif (!mlx5_eswitch_vport_match_metadata_enabled(esw))\n\t\treturn;\n\n\tmlx5_esw_for_each_vport(esw, i, vport)\n\t\tesw_offloads_vport_metadata_cleanup(esw, vport);\n}\n\nstatic int esw_offloads_metadata_init(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\tint err;\n\n\tif (!mlx5_eswitch_vport_match_metadata_enabled(esw))\n\t\treturn 0;\n\n\tmlx5_esw_for_each_vport(esw, i, vport) {\n\t\terr = esw_offloads_vport_metadata_setup(esw, vport);\n\t\tif (err)\n\t\t\tgoto metadata_err;\n\t}\n\n\treturn 0;\n\nmetadata_err:\n\tesw_offloads_metadata_uninit(esw);\n\treturn err;\n}\n\nint\nesw_vport_create_offloads_acl_tables(struct mlx5_eswitch *esw,\n\t\t\t\t     struct mlx5_vport *vport)\n{\n\tint err;\n\n\terr = esw_acl_ingress_ofld_setup(esw, vport);\n\tif (err)\n\t\treturn err;\n\n\terr = esw_acl_egress_ofld_setup(esw, vport);\n\tif (err)\n\t\tgoto egress_err;\n\n\treturn 0;\n\negress_err:\n\tesw_acl_ingress_ofld_cleanup(esw, vport);\n\treturn err;\n}\n\nvoid\nesw_vport_destroy_offloads_acl_tables(struct mlx5_eswitch *esw,\n\t\t\t\t      struct mlx5_vport *vport)\n{\n\tesw_acl_egress_ofld_cleanup(vport);\n\tesw_acl_ingress_ofld_cleanup(esw, vport);\n}\n\nstatic int esw_create_offloads_acl_tables(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport *uplink, *manager;\n\tint ret;\n\n\tuplink = mlx5_eswitch_get_vport(esw, MLX5_VPORT_UPLINK);\n\tif (IS_ERR(uplink))\n\t\treturn PTR_ERR(uplink);\n\n\tret = esw_vport_create_offloads_acl_tables(esw, uplink);\n\tif (ret)\n\t\treturn ret;\n\n\tmanager = mlx5_eswitch_get_vport(esw, esw->manager_vport);\n\tif (IS_ERR(manager)) {\n\t\tret = PTR_ERR(manager);\n\t\tgoto err_manager;\n\t}\n\n\tret = esw_vport_create_offloads_acl_tables(esw, manager);\n\tif (ret)\n\t\tgoto err_manager;\n\n\treturn 0;\n\nerr_manager:\n\tesw_vport_destroy_offloads_acl_tables(esw, uplink);\n\treturn ret;\n}\n\nstatic void esw_destroy_offloads_acl_tables(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_vport *vport;\n\n\tvport = mlx5_eswitch_get_vport(esw, esw->manager_vport);\n\tif (!IS_ERR(vport))\n\t\tesw_vport_destroy_offloads_acl_tables(esw, vport);\n\n\tvport = mlx5_eswitch_get_vport(esw, MLX5_VPORT_UPLINK);\n\tif (!IS_ERR(vport))\n\t\tesw_vport_destroy_offloads_acl_tables(esw, vport);\n}\n\nint mlx5_eswitch_reload_reps(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_eswitch_rep *rep;\n\tunsigned long i;\n\tint ret;\n\n\tif (!esw || esw->mode != MLX5_ESWITCH_OFFLOADS)\n\t\treturn 0;\n\n\trep = mlx5_eswitch_get_rep(esw, MLX5_VPORT_UPLINK);\n\tif (atomic_read(&rep->rep_data[REP_ETH].state) != REP_LOADED)\n\t\treturn 0;\n\n\tret = mlx5_esw_offloads_rep_load(esw, MLX5_VPORT_UPLINK);\n\tif (ret)\n\t\treturn ret;\n\n\tmlx5_esw_for_each_rep(esw, i, rep) {\n\t\tif (atomic_read(&rep->rep_data[REP_ETH].state) == REP_LOADED)\n\t\t\tmlx5_esw_offloads_rep_load(esw, rep->vport);\n\t}\n\n\treturn 0;\n}\n\nstatic int esw_offloads_steering_init(struct mlx5_eswitch *esw)\n{\n\tstruct mlx5_esw_indir_table *indir;\n\tint err;\n\n\tmemset(&esw->fdb_table.offloads, 0, sizeof(struct offloads_fdb));\n\tmutex_init(&esw->fdb_table.offloads.vports.lock);\n\thash_init(esw->fdb_table.offloads.vports.table);\n\tatomic64_set(&esw->user_count, 0);\n\n\tindir = mlx5_esw_indir_table_init();\n\tif (IS_ERR(indir)) {\n\t\terr = PTR_ERR(indir);\n\t\tgoto create_indir_err;\n\t}\n\tesw->fdb_table.offloads.indir = indir;\n\n\terr = esw_create_offloads_acl_tables(esw);\n\tif (err)\n\t\tgoto create_acl_err;\n\n\terr = esw_create_offloads_table(esw);\n\tif (err)\n\t\tgoto create_offloads_err;\n\n\terr = esw_create_restore_table(esw);\n\tif (err)\n\t\tgoto create_restore_err;\n\n\terr = esw_create_offloads_fdb_tables(esw);\n\tif (err)\n\t\tgoto create_fdb_err;\n\n\terr = esw_create_vport_rx_group(esw);\n\tif (err)\n\t\tgoto create_fg_err;\n\n\terr = esw_create_vport_rx_drop_group(esw);\n\tif (err)\n\t\tgoto create_rx_drop_fg_err;\n\n\terr = esw_create_vport_rx_drop_rule(esw);\n\tif (err)\n\t\tgoto create_rx_drop_rule_err;\n\n\treturn 0;\n\ncreate_rx_drop_rule_err:\n\tesw_destroy_vport_rx_drop_group(esw);\ncreate_rx_drop_fg_err:\n\tesw_destroy_vport_rx_group(esw);\ncreate_fg_err:\n\tesw_destroy_offloads_fdb_tables(esw);\ncreate_fdb_err:\n\tesw_destroy_restore_table(esw);\ncreate_restore_err:\n\tesw_destroy_offloads_table(esw);\ncreate_offloads_err:\n\tesw_destroy_offloads_acl_tables(esw);\ncreate_acl_err:\n\tmlx5_esw_indir_table_destroy(esw->fdb_table.offloads.indir);\ncreate_indir_err:\n\tmutex_destroy(&esw->fdb_table.offloads.vports.lock);\n\treturn err;\n}\n\nstatic void esw_offloads_steering_cleanup(struct mlx5_eswitch *esw)\n{\n\tesw_destroy_vport_rx_drop_rule(esw);\n\tesw_destroy_vport_rx_drop_group(esw);\n\tesw_destroy_vport_rx_group(esw);\n\tesw_destroy_offloads_fdb_tables(esw);\n\tesw_destroy_restore_table(esw);\n\tesw_destroy_offloads_table(esw);\n\tesw_destroy_offloads_acl_tables(esw);\n\tmlx5_esw_indir_table_destroy(esw->fdb_table.offloads.indir);\n\tmutex_destroy(&esw->fdb_table.offloads.vports.lock);\n}\n\nstatic void\nesw_vfs_changed_event_handler(struct mlx5_eswitch *esw, const u32 *out)\n{\n\tstruct devlink *devlink;\n\tbool host_pf_disabled;\n\tu16 new_num_vfs;\n\n\tnew_num_vfs = MLX5_GET(query_esw_functions_out, out,\n\t\t\t       host_params_context.host_num_of_vfs);\n\thost_pf_disabled = MLX5_GET(query_esw_functions_out, out,\n\t\t\t\t    host_params_context.host_pf_disabled);\n\n\tif (new_num_vfs == esw->esw_funcs.num_vfs || host_pf_disabled)\n\t\treturn;\n\n\tdevlink = priv_to_devlink(esw->dev);\n\tdevl_lock(devlink);\n\t \n\tif (esw->esw_funcs.num_vfs > 0) {\n\t\tmlx5_eswitch_unload_vf_vports(esw, esw->esw_funcs.num_vfs);\n\t} else {\n\t\tint err;\n\n\t\terr = mlx5_eswitch_load_vf_vports(esw, new_num_vfs,\n\t\t\t\t\t\t  MLX5_VPORT_UC_ADDR_CHANGE);\n\t\tif (err) {\n\t\t\tdevl_unlock(devlink);\n\t\t\treturn;\n\t\t}\n\t}\n\tesw->esw_funcs.num_vfs = new_num_vfs;\n\tdevl_unlock(devlink);\n}\n\nstatic void esw_functions_changed_event_handler(struct work_struct *work)\n{\n\tstruct mlx5_host_work *host_work;\n\tstruct mlx5_eswitch *esw;\n\tconst u32 *out;\n\n\thost_work = container_of(work, struct mlx5_host_work, work);\n\tesw = host_work->esw;\n\n\tout = mlx5_esw_query_functions(esw->dev);\n\tif (IS_ERR(out))\n\t\tgoto out;\n\n\tesw_vfs_changed_event_handler(esw, out);\n\tkvfree(out);\nout:\n\tkfree(host_work);\n}\n\nint mlx5_esw_funcs_changed_handler(struct notifier_block *nb, unsigned long type, void *data)\n{\n\tstruct mlx5_esw_functions *esw_funcs;\n\tstruct mlx5_host_work *host_work;\n\tstruct mlx5_eswitch *esw;\n\n\thost_work = kzalloc(sizeof(*host_work), GFP_ATOMIC);\n\tif (!host_work)\n\t\treturn NOTIFY_DONE;\n\n\tesw_funcs = mlx5_nb_cof(nb, struct mlx5_esw_functions, nb);\n\tesw = container_of(esw_funcs, struct mlx5_eswitch, esw_funcs);\n\n\thost_work->esw = esw;\n\n\tINIT_WORK(&host_work->work, esw_functions_changed_event_handler);\n\tqueue_work(esw->work_queue, &host_work->work);\n\n\treturn NOTIFY_OK;\n}\n\nstatic int mlx5_esw_host_number_init(struct mlx5_eswitch *esw)\n{\n\tconst u32 *query_host_out;\n\n\tif (!mlx5_core_is_ecpf_esw_manager(esw->dev))\n\t\treturn 0;\n\n\tquery_host_out = mlx5_esw_query_functions(esw->dev);\n\tif (IS_ERR(query_host_out))\n\t\treturn PTR_ERR(query_host_out);\n\n\t \n\tesw->offloads.host_number = MLX5_GET(query_esw_functions_out, query_host_out,\n\t\t\t\t\t     host_params_context.host_number);\n\tkvfree(query_host_out);\n\treturn 0;\n}\n\nbool mlx5_esw_offloads_controller_valid(const struct mlx5_eswitch *esw, u32 controller)\n{\n\t \n\tif (controller == 0)\n\t\treturn true;\n\n\tif (!mlx5_core_is_ecpf_esw_manager(esw->dev))\n\t\treturn false;\n\n\t \n\treturn (controller == esw->offloads.host_number + 1);\n}\n\nint esw_offloads_enable(struct mlx5_eswitch *esw)\n{\n\tstruct mapping_ctx *reg_c0_obj_pool;\n\tstruct mlx5_vport *vport;\n\tunsigned long i;\n\tu64 mapping_id;\n\tint err;\n\n\tmutex_init(&esw->offloads.termtbl_mutex);\n\tmlx5_rdma_enable_roce(esw->dev);\n\n\terr = mlx5_esw_host_number_init(esw);\n\tif (err)\n\t\tgoto err_metadata;\n\n\terr = esw_offloads_metadata_init(esw);\n\tif (err)\n\t\tgoto err_metadata;\n\n\terr = esw_set_passing_vport_metadata(esw, true);\n\tif (err)\n\t\tgoto err_vport_metadata;\n\n\tmapping_id = mlx5_query_nic_system_image_guid(esw->dev);\n\n\treg_c0_obj_pool = mapping_create_for_id(mapping_id, MAPPING_TYPE_CHAIN,\n\t\t\t\t\t\tsizeof(struct mlx5_mapped_obj),\n\t\t\t\t\t\tESW_REG_C0_USER_DATA_METADATA_MASK,\n\t\t\t\t\t\ttrue);\n\n\tif (IS_ERR(reg_c0_obj_pool)) {\n\t\terr = PTR_ERR(reg_c0_obj_pool);\n\t\tgoto err_pool;\n\t}\n\tesw->offloads.reg_c0_obj_pool = reg_c0_obj_pool;\n\n\terr = esw_offloads_steering_init(esw);\n\tif (err)\n\t\tgoto err_steering_init;\n\n\t \n\tmlx5_esw_for_each_vf_vport(esw, i, vport, esw->esw_funcs.num_vfs)\n\t\tvport->info.link_state = MLX5_VPORT_ADMIN_STATE_DOWN;\n\tif (mlx5_core_ec_sriov_enabled(esw->dev))\n\t\tmlx5_esw_for_each_ec_vf_vport(esw, i, vport, esw->esw_funcs.num_ec_vfs)\n\t\t\tvport->info.link_state = MLX5_VPORT_ADMIN_STATE_DOWN;\n\n\t \n\terr = mlx5_esw_offloads_rep_load(esw, MLX5_VPORT_UPLINK);\n\tif (err)\n\t\tgoto err_uplink;\n\n\terr = mlx5_eswitch_enable_pf_vf_vports(esw, MLX5_VPORT_UC_ADDR_CHANGE);\n\tif (err)\n\t\tgoto err_vports;\n\n\treturn 0;\n\nerr_vports:\n\tmlx5_esw_offloads_rep_unload(esw, MLX5_VPORT_UPLINK);\nerr_uplink:\n\tesw_offloads_steering_cleanup(esw);\nerr_steering_init:\n\tmapping_destroy(reg_c0_obj_pool);\nerr_pool:\n\tesw_set_passing_vport_metadata(esw, false);\nerr_vport_metadata:\n\tesw_offloads_metadata_uninit(esw);\nerr_metadata:\n\tmlx5_rdma_disable_roce(esw->dev);\n\tmutex_destroy(&esw->offloads.termtbl_mutex);\n\treturn err;\n}\n\nstatic int esw_offloads_stop(struct mlx5_eswitch *esw,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tint err;\n\n\tesw->mode = MLX5_ESWITCH_LEGACY;\n\n\t \n\tif (!mlx5_core_is_pf(esw->dev) || !mlx5_sriov_is_enabled(esw->dev))\n\t\treturn 0;\n\n\terr = mlx5_eswitch_enable_locked(esw, MLX5_ESWITCH_IGNORE_NUM_VFS);\n\tif (err)\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Failed setting eswitch to legacy\");\n\n\treturn err;\n}\n\nvoid esw_offloads_disable(struct mlx5_eswitch *esw)\n{\n\tmlx5_eswitch_disable_pf_vf_vports(esw);\n\tmlx5_esw_offloads_rep_unload(esw, MLX5_VPORT_UPLINK);\n\tesw_set_passing_vport_metadata(esw, false);\n\tesw_offloads_steering_cleanup(esw);\n\tmapping_destroy(esw->offloads.reg_c0_obj_pool);\n\tesw_offloads_metadata_uninit(esw);\n\tmlx5_rdma_disable_roce(esw->dev);\n\tmutex_destroy(&esw->offloads.termtbl_mutex);\n}\n\nstatic int esw_mode_from_devlink(u16 mode, u16 *mlx5_mode)\n{\n\tswitch (mode) {\n\tcase DEVLINK_ESWITCH_MODE_LEGACY:\n\t\t*mlx5_mode = MLX5_ESWITCH_LEGACY;\n\t\tbreak;\n\tcase DEVLINK_ESWITCH_MODE_SWITCHDEV:\n\t\t*mlx5_mode = MLX5_ESWITCH_OFFLOADS;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int esw_mode_to_devlink(u16 mlx5_mode, u16 *mode)\n{\n\tswitch (mlx5_mode) {\n\tcase MLX5_ESWITCH_LEGACY:\n\t\t*mode = DEVLINK_ESWITCH_MODE_LEGACY;\n\t\tbreak;\n\tcase MLX5_ESWITCH_OFFLOADS:\n\t\t*mode = DEVLINK_ESWITCH_MODE_SWITCHDEV;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int esw_inline_mode_from_devlink(u8 mode, u8 *mlx5_mode)\n{\n\tswitch (mode) {\n\tcase DEVLINK_ESWITCH_INLINE_MODE_NONE:\n\t\t*mlx5_mode = MLX5_INLINE_MODE_NONE;\n\t\tbreak;\n\tcase DEVLINK_ESWITCH_INLINE_MODE_LINK:\n\t\t*mlx5_mode = MLX5_INLINE_MODE_L2;\n\t\tbreak;\n\tcase DEVLINK_ESWITCH_INLINE_MODE_NETWORK:\n\t\t*mlx5_mode = MLX5_INLINE_MODE_IP;\n\t\tbreak;\n\tcase DEVLINK_ESWITCH_INLINE_MODE_TRANSPORT:\n\t\t*mlx5_mode = MLX5_INLINE_MODE_TCP_UDP;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int esw_inline_mode_to_devlink(u8 mlx5_mode, u8 *mode)\n{\n\tswitch (mlx5_mode) {\n\tcase MLX5_INLINE_MODE_NONE:\n\t\t*mode = DEVLINK_ESWITCH_INLINE_MODE_NONE;\n\t\tbreak;\n\tcase MLX5_INLINE_MODE_L2:\n\t\t*mode = DEVLINK_ESWITCH_INLINE_MODE_LINK;\n\t\tbreak;\n\tcase MLX5_INLINE_MODE_IP:\n\t\t*mode = DEVLINK_ESWITCH_INLINE_MODE_NETWORK;\n\t\tbreak;\n\tcase MLX5_INLINE_MODE_TCP_UDP:\n\t\t*mode = DEVLINK_ESWITCH_INLINE_MODE_TRANSPORT;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool esw_offloads_devlink_ns_eq_netdev_ns(struct devlink *devlink)\n{\n\tstruct mlx5_core_dev *dev = devlink_priv(devlink);\n\tstruct net *devl_net, *netdev_net;\n\tbool ret = false;\n\n\tmutex_lock(&dev->mlx5e_res.uplink_netdev_lock);\n\tif (dev->mlx5e_res.uplink_netdev) {\n\t\tnetdev_net = dev_net(dev->mlx5e_res.uplink_netdev);\n\t\tdevl_net = devlink_net(devlink);\n\t\tret = net_eq(devl_net, netdev_net);\n\t}\n\tmutex_unlock(&dev->mlx5e_res.uplink_netdev_lock);\n\treturn ret;\n}\n\nint mlx5_eswitch_block_mode(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw = dev->priv.eswitch;\n\tint err;\n\n\tif (!mlx5_esw_allowed(esw))\n\t\treturn 0;\n\n\t \n\terr = mlx5_esw_try_lock(esw);\n\tif (err < 0)\n\t\treturn err;\n\n\tesw->offloads.num_block_mode++;\n\tmlx5_esw_unlock(esw);\n\treturn 0;\n}\n\nvoid mlx5_eswitch_unblock_mode(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw = dev->priv.eswitch;\n\n\tif (!mlx5_esw_allowed(esw))\n\t\treturn;\n\n\tdown_write(&esw->mode_lock);\n\tesw->offloads.num_block_mode--;\n\tup_write(&esw->mode_lock);\n}\n\nint mlx5_devlink_eswitch_mode_set(struct devlink *devlink, u16 mode,\n\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tu16 cur_mlx5_mode, mlx5_mode = 0;\n\tstruct mlx5_eswitch *esw;\n\tint err = 0;\n\n\tesw = mlx5_devlink_eswitch_get(devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\tif (esw_mode_from_devlink(mode, &mlx5_mode))\n\t\treturn -EINVAL;\n\n\tif (mode == DEVLINK_ESWITCH_MODE_SWITCHDEV &&\n\t    !esw_offloads_devlink_ns_eq_netdev_ns(devlink)) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Can't change E-Switch mode to switchdev when netdev net namespace has diverged from the devlink's.\");\n\t\treturn -EPERM;\n\t}\n\n\tmlx5_lag_disable_change(esw->dev);\n\terr = mlx5_esw_try_lock(esw);\n\tif (err < 0) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Can't change mode, E-Switch is busy\");\n\t\tgoto enable_lag;\n\t}\n\tcur_mlx5_mode = err;\n\terr = 0;\n\n\tif (cur_mlx5_mode == mlx5_mode)\n\t\tgoto unlock;\n\n\tif (esw->offloads.num_block_mode) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Can't change eswitch mode when IPsec SA and/or policies are configured\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto unlock;\n\t}\n\n\tesw->eswitch_operation_in_progress = true;\n\tup_write(&esw->mode_lock);\n\n\tmlx5_eswitch_disable_locked(esw);\n\tif (mode == DEVLINK_ESWITCH_MODE_SWITCHDEV) {\n\t\tif (mlx5_devlink_trap_get_num_active(esw->dev)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t   \"Can't change mode while devlink traps are active\");\n\t\t\terr = -EOPNOTSUPP;\n\t\t\tgoto skip;\n\t\t}\n\t\terr = esw_offloads_start(esw, extack);\n\t} else if (mode == DEVLINK_ESWITCH_MODE_LEGACY) {\n\t\terr = esw_offloads_stop(esw, extack);\n\t\tmlx5_rescan_drivers(esw->dev);\n\t} else {\n\t\terr = -EINVAL;\n\t}\n\nskip:\n\tdown_write(&esw->mode_lock);\n\tesw->eswitch_operation_in_progress = false;\nunlock:\n\tmlx5_esw_unlock(esw);\nenable_lag:\n\tmlx5_lag_enable_change(esw->dev);\n\treturn err;\n}\n\nint mlx5_devlink_eswitch_mode_get(struct devlink *devlink, u16 *mode)\n{\n\tstruct mlx5_eswitch *esw;\n\n\tesw = mlx5_devlink_eswitch_get(devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\treturn esw_mode_to_devlink(esw->mode, mode);\n}\n\nstatic int mlx5_esw_vports_inline_set(struct mlx5_eswitch *esw, u8 mlx5_mode,\n\t\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_core_dev *dev = esw->dev;\n\tstruct mlx5_vport *vport;\n\tu16 err_vport_num = 0;\n\tunsigned long i;\n\tint err = 0;\n\n\tmlx5_esw_for_each_host_func_vport(esw, i, vport, esw->esw_funcs.num_vfs) {\n\t\terr = mlx5_modify_nic_vport_min_inline(dev, vport->vport, mlx5_mode);\n\t\tif (err) {\n\t\t\terr_vport_num = vport->vport;\n\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t   \"Failed to set min inline on vport\");\n\t\t\tgoto revert_inline_mode;\n\t\t}\n\t}\n\tif (mlx5_core_ec_sriov_enabled(esw->dev)) {\n\t\tmlx5_esw_for_each_ec_vf_vport(esw, i, vport, esw->esw_funcs.num_ec_vfs) {\n\t\t\terr = mlx5_modify_nic_vport_min_inline(dev, vport->vport, mlx5_mode);\n\t\t\tif (err) {\n\t\t\t\terr_vport_num = vport->vport;\n\t\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t\t   \"Failed to set min inline on vport\");\n\t\t\t\tgoto revert_ec_vf_inline_mode;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n\nrevert_ec_vf_inline_mode:\n\tmlx5_esw_for_each_ec_vf_vport(esw, i, vport, esw->esw_funcs.num_ec_vfs) {\n\t\tif (vport->vport == err_vport_num)\n\t\t\tbreak;\n\t\tmlx5_modify_nic_vport_min_inline(dev,\n\t\t\t\t\t\t vport->vport,\n\t\t\t\t\t\t esw->offloads.inline_mode);\n\t}\nrevert_inline_mode:\n\tmlx5_esw_for_each_host_func_vport(esw, i, vport, esw->esw_funcs.num_vfs) {\n\t\tif (vport->vport == err_vport_num)\n\t\t\tbreak;\n\t\tmlx5_modify_nic_vport_min_inline(dev,\n\t\t\t\t\t\t vport->vport,\n\t\t\t\t\t\t esw->offloads.inline_mode);\n\t}\n\treturn err;\n}\n\nint mlx5_devlink_eswitch_inline_mode_set(struct devlink *devlink, u8 mode,\n\t\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_core_dev *dev = devlink_priv(devlink);\n\tstruct mlx5_eswitch *esw;\n\tu8 mlx5_mode;\n\tint err;\n\n\tesw = mlx5_devlink_eswitch_get(devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\tdown_write(&esw->mode_lock);\n\n\tswitch (MLX5_CAP_ETH(dev, wqe_inline_mode)) {\n\tcase MLX5_CAP_INLINE_MODE_NOT_REQUIRED:\n\t\tif (mode == DEVLINK_ESWITCH_INLINE_MODE_NONE) {\n\t\t\terr = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\tfallthrough;\n\tcase MLX5_CAP_INLINE_MODE_L2:\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Inline mode can't be set\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\tcase MLX5_CAP_INLINE_MODE_VPORT_CONTEXT:\n\t\tbreak;\n\t}\n\n\tif (atomic64_read(&esw->offloads.num_flows) > 0) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Can't set inline mode when flows are configured\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\terr = esw_inline_mode_from_devlink(mode, &mlx5_mode);\n\tif (err)\n\t\tgoto out;\n\n\tesw->eswitch_operation_in_progress = true;\n\tup_write(&esw->mode_lock);\n\n\terr = mlx5_esw_vports_inline_set(esw, mlx5_mode, extack);\n\tif (!err)\n\t\tesw->offloads.inline_mode = mlx5_mode;\n\n\tdown_write(&esw->mode_lock);\n\tesw->eswitch_operation_in_progress = false;\n\tup_write(&esw->mode_lock);\n\treturn 0;\n\nout:\n\tup_write(&esw->mode_lock);\n\treturn err;\n}\n\nint mlx5_devlink_eswitch_inline_mode_get(struct devlink *devlink, u8 *mode)\n{\n\tstruct mlx5_eswitch *esw;\n\n\tesw = mlx5_devlink_eswitch_get(devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\treturn esw_inline_mode_to_devlink(esw->offloads.inline_mode, mode);\n}\n\nbool mlx5_eswitch_block_encap(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw = dev->priv.eswitch;\n\n\tif (!mlx5_esw_allowed(esw))\n\t\treturn true;\n\n\tdown_write(&esw->mode_lock);\n\tif (esw->mode != MLX5_ESWITCH_LEGACY &&\n\t    esw->offloads.encap != DEVLINK_ESWITCH_ENCAP_MODE_NONE) {\n\t\tup_write(&esw->mode_lock);\n\t\treturn false;\n\t}\n\n\tesw->offloads.num_block_encap++;\n\tup_write(&esw->mode_lock);\n\treturn true;\n}\n\nvoid mlx5_eswitch_unblock_encap(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_eswitch *esw = dev->priv.eswitch;\n\n\tif (!mlx5_esw_allowed(esw))\n\t\treturn;\n\n\tdown_write(&esw->mode_lock);\n\tesw->offloads.num_block_encap--;\n\tup_write(&esw->mode_lock);\n}\n\nint mlx5_devlink_eswitch_encap_mode_set(struct devlink *devlink,\n\t\t\t\t\tenum devlink_eswitch_encap_mode encap,\n\t\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct mlx5_core_dev *dev = devlink_priv(devlink);\n\tstruct mlx5_eswitch *esw;\n\tint err = 0;\n\n\tesw = mlx5_devlink_eswitch_get(devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\tdown_write(&esw->mode_lock);\n\n\tif (encap != DEVLINK_ESWITCH_ENCAP_MODE_NONE &&\n\t    (!MLX5_CAP_ESW_FLOWTABLE_FDB(dev, reformat) ||\n\t     !MLX5_CAP_ESW_FLOWTABLE_FDB(dev, decap))) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto unlock;\n\t}\n\n\tif (encap && encap != DEVLINK_ESWITCH_ENCAP_MODE_BASIC) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto unlock;\n\t}\n\n\tif (esw->mode == MLX5_ESWITCH_LEGACY) {\n\t\tesw->offloads.encap = encap;\n\t\tgoto unlock;\n\t}\n\n\tif (esw->offloads.encap == encap)\n\t\tgoto unlock;\n\n\tif (atomic64_read(&esw->offloads.num_flows) > 0) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Can't set encapsulation when flows are configured\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto unlock;\n\t}\n\n\tif (esw->offloads.num_block_encap) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Can't set encapsulation when IPsec SA and/or policies are configured\");\n\t\terr = -EOPNOTSUPP;\n\t\tgoto unlock;\n\t}\n\n\tesw->eswitch_operation_in_progress = true;\n\tup_write(&esw->mode_lock);\n\n\tesw_destroy_offloads_fdb_tables(esw);\n\n\tesw->offloads.encap = encap;\n\n\terr = esw_create_offloads_fdb_tables(esw);\n\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Failed re-creating fast FDB table\");\n\t\tesw->offloads.encap = !encap;\n\t\t(void)esw_create_offloads_fdb_tables(esw);\n\t}\n\n\tdown_write(&esw->mode_lock);\n\tesw->eswitch_operation_in_progress = false;\n\nunlock:\n\tup_write(&esw->mode_lock);\n\treturn err;\n}\n\nint mlx5_devlink_eswitch_encap_mode_get(struct devlink *devlink,\n\t\t\t\t\tenum devlink_eswitch_encap_mode *encap)\n{\n\tstruct mlx5_eswitch *esw;\n\n\tesw = mlx5_devlink_eswitch_get(devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\t*encap = esw->offloads.encap;\n\treturn 0;\n}\n\nstatic bool\nmlx5_eswitch_vport_has_rep(const struct mlx5_eswitch *esw, u16 vport_num)\n{\n\t \n\tif (vport_num == MLX5_VPORT_PF &&\n\t    !mlx5_core_is_ecpf_esw_manager(esw->dev))\n\t\treturn false;\n\n\tif (vport_num == MLX5_VPORT_ECPF &&\n\t    !mlx5_ecpf_vport_exists(esw->dev))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid mlx5_eswitch_register_vport_reps(struct mlx5_eswitch *esw,\n\t\t\t\t      const struct mlx5_eswitch_rep_ops *ops,\n\t\t\t\t      u8 rep_type)\n{\n\tstruct mlx5_eswitch_rep_data *rep_data;\n\tstruct mlx5_eswitch_rep *rep;\n\tunsigned long i;\n\n\tesw->offloads.rep_ops[rep_type] = ops;\n\tmlx5_esw_for_each_rep(esw, i, rep) {\n\t\tif (likely(mlx5_eswitch_vport_has_rep(esw, rep->vport))) {\n\t\t\trep->esw = esw;\n\t\t\trep_data = &rep->rep_data[rep_type];\n\t\t\tatomic_set(&rep_data->state, REP_REGISTERED);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(mlx5_eswitch_register_vport_reps);\n\nvoid mlx5_eswitch_unregister_vport_reps(struct mlx5_eswitch *esw, u8 rep_type)\n{\n\tstruct mlx5_eswitch_rep *rep;\n\tunsigned long i;\n\n\tif (esw->mode == MLX5_ESWITCH_OFFLOADS)\n\t\t__unload_reps_all_vport(esw, rep_type);\n\n\tmlx5_esw_for_each_rep(esw, i, rep)\n\t\tatomic_set(&rep->rep_data[rep_type].state, REP_UNREGISTERED);\n}\nEXPORT_SYMBOL(mlx5_eswitch_unregister_vport_reps);\n\nvoid *mlx5_eswitch_get_uplink_priv(struct mlx5_eswitch *esw, u8 rep_type)\n{\n\tstruct mlx5_eswitch_rep *rep;\n\n\trep = mlx5_eswitch_get_rep(esw, MLX5_VPORT_UPLINK);\n\treturn rep->rep_data[rep_type].priv;\n}\n\nvoid *mlx5_eswitch_get_proto_dev(struct mlx5_eswitch *esw,\n\t\t\t\t u16 vport,\n\t\t\t\t u8 rep_type)\n{\n\tstruct mlx5_eswitch_rep *rep;\n\n\trep = mlx5_eswitch_get_rep(esw, vport);\n\n\tif (atomic_read(&rep->rep_data[rep_type].state) == REP_LOADED &&\n\t    esw->offloads.rep_ops[rep_type]->get_proto_dev)\n\t\treturn esw->offloads.rep_ops[rep_type]->get_proto_dev(rep);\n\treturn NULL;\n}\nEXPORT_SYMBOL(mlx5_eswitch_get_proto_dev);\n\nvoid *mlx5_eswitch_uplink_get_proto_dev(struct mlx5_eswitch *esw, u8 rep_type)\n{\n\treturn mlx5_eswitch_get_proto_dev(esw, MLX5_VPORT_UPLINK, rep_type);\n}\nEXPORT_SYMBOL(mlx5_eswitch_uplink_get_proto_dev);\n\nstruct mlx5_eswitch_rep *mlx5_eswitch_vport_rep(struct mlx5_eswitch *esw,\n\t\t\t\t\t\tu16 vport)\n{\n\treturn mlx5_eswitch_get_rep(esw, vport);\n}\nEXPORT_SYMBOL(mlx5_eswitch_vport_rep);\n\nbool mlx5_eswitch_reg_c1_loopback_enabled(const struct mlx5_eswitch *esw)\n{\n\treturn !!(esw->flags & MLX5_ESWITCH_REG_C1_LOOPBACK_ENABLED);\n}\nEXPORT_SYMBOL(mlx5_eswitch_reg_c1_loopback_enabled);\n\nbool mlx5_eswitch_vport_match_metadata_enabled(const struct mlx5_eswitch *esw)\n{\n\treturn !!(esw->flags & MLX5_ESWITCH_VPORT_MATCH_METADATA);\n}\nEXPORT_SYMBOL(mlx5_eswitch_vport_match_metadata_enabled);\n\nu32 mlx5_eswitch_get_vport_metadata_for_match(struct mlx5_eswitch *esw,\n\t\t\t\t\t      u16 vport_num)\n{\n\tstruct mlx5_vport *vport = mlx5_eswitch_get_vport(esw, vport_num);\n\n\tif (WARN_ON_ONCE(IS_ERR(vport)))\n\t\treturn 0;\n\n\treturn vport->metadata << (32 - ESW_SOURCE_PORT_METADATA_BITS);\n}\nEXPORT_SYMBOL(mlx5_eswitch_get_vport_metadata_for_match);\n\nstatic int mlx5_esw_query_vport_vhca_id(struct mlx5_eswitch *esw, u16 vport_num, u16 *vhca_id)\n{\n\tint query_out_sz = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tvoid *query_ctx;\n\tvoid *hca_caps;\n\tint err;\n\n\t*vhca_id = 0;\n\n\tquery_ctx = kzalloc(query_out_sz, GFP_KERNEL);\n\tif (!query_ctx)\n\t\treturn -ENOMEM;\n\n\terr = mlx5_vport_get_other_func_general_cap(esw->dev, vport_num, query_ctx);\n\tif (err)\n\t\tgoto out_free;\n\n\thca_caps = MLX5_ADDR_OF(query_hca_cap_out, query_ctx, capability);\n\t*vhca_id = MLX5_GET(cmd_hca_cap, hca_caps, vhca_id);\n\nout_free:\n\tkfree(query_ctx);\n\treturn err;\n}\n\nint mlx5_esw_vport_vhca_id_set(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\tu16 *old_entry, *vhca_map_entry, vhca_id;\n\tint err;\n\n\terr = mlx5_esw_query_vport_vhca_id(esw, vport_num, &vhca_id);\n\tif (err) {\n\t\tesw_warn(esw->dev, \"Getting vhca_id for vport failed (vport=%u,err=%d)\\n\",\n\t\t\t vport_num, err);\n\t\treturn err;\n\t}\n\n\tvhca_map_entry = kmalloc(sizeof(*vhca_map_entry), GFP_KERNEL);\n\tif (!vhca_map_entry)\n\t\treturn -ENOMEM;\n\n\t*vhca_map_entry = vport_num;\n\told_entry = xa_store(&esw->offloads.vhca_map, vhca_id, vhca_map_entry, GFP_KERNEL);\n\tif (xa_is_err(old_entry)) {\n\t\tkfree(vhca_map_entry);\n\t\treturn xa_err(old_entry);\n\t}\n\tkfree(old_entry);\n\treturn 0;\n}\n\nvoid mlx5_esw_vport_vhca_id_clear(struct mlx5_eswitch *esw, u16 vport_num)\n{\n\tu16 *vhca_map_entry, vhca_id;\n\tint err;\n\n\terr = mlx5_esw_query_vport_vhca_id(esw, vport_num, &vhca_id);\n\tif (err)\n\t\tesw_warn(esw->dev, \"Getting vhca_id for vport failed (vport=%hu,err=%d)\\n\",\n\t\t\t vport_num, err);\n\n\tvhca_map_entry = xa_erase(&esw->offloads.vhca_map, vhca_id);\n\tkfree(vhca_map_entry);\n}\n\nint mlx5_eswitch_vhca_id_to_vport(struct mlx5_eswitch *esw, u16 vhca_id, u16 *vport_num)\n{\n\tu16 *res = xa_load(&esw->offloads.vhca_map, vhca_id);\n\n\tif (!res)\n\t\treturn -ENOENT;\n\n\t*vport_num = *res;\n\treturn 0;\n}\n\nu32 mlx5_eswitch_get_vport_metadata_for_set(struct mlx5_eswitch *esw,\n\t\t\t\t\t    u16 vport_num)\n{\n\tstruct mlx5_vport *vport = mlx5_eswitch_get_vport(esw, vport_num);\n\n\tif (WARN_ON_ONCE(IS_ERR(vport)))\n\t\treturn 0;\n\n\treturn vport->metadata;\n}\nEXPORT_SYMBOL(mlx5_eswitch_get_vport_metadata_for_set);\n\nint mlx5_devlink_port_fn_hw_addr_get(struct devlink_port *port,\n\t\t\t\t     u8 *hw_addr, int *hw_addr_len,\n\t\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw = mlx5_devlink_eswitch_nocheck_get(port->devlink);\n\tstruct mlx5_vport *vport = mlx5_devlink_port_vport_get(port);\n\n\tmutex_lock(&esw->state_lock);\n\tether_addr_copy(hw_addr, vport->info.mac);\n\t*hw_addr_len = ETH_ALEN;\n\tmutex_unlock(&esw->state_lock);\n\treturn 0;\n}\n\nint mlx5_devlink_port_fn_hw_addr_set(struct devlink_port *port,\n\t\t\t\t     const u8 *hw_addr, int hw_addr_len,\n\t\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw = mlx5_devlink_eswitch_nocheck_get(port->devlink);\n\tstruct mlx5_vport *vport = mlx5_devlink_port_vport_get(port);\n\n\treturn mlx5_eswitch_set_vport_mac(esw, vport->vport, hw_addr);\n}\n\nint mlx5_devlink_port_fn_migratable_get(struct devlink_port *port, bool *is_enabled,\n\t\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw = mlx5_devlink_eswitch_nocheck_get(port->devlink);\n\tstruct mlx5_vport *vport = mlx5_devlink_port_vport_get(port);\n\n\tif (!MLX5_CAP_GEN(esw->dev, migration)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Device doesn't support migration\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!MLX5_CAP_GEN(esw->dev, vhca_resource_manager)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Device doesn't support VHCA management\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmutex_lock(&esw->state_lock);\n\t*is_enabled = vport->info.mig_enabled;\n\tmutex_unlock(&esw->state_lock);\n\treturn 0;\n}\n\nint mlx5_devlink_port_fn_migratable_set(struct devlink_port *port, bool enable,\n\t\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw = mlx5_devlink_eswitch_nocheck_get(port->devlink);\n\tstruct mlx5_vport *vport = mlx5_devlink_port_vport_get(port);\n\tint query_out_sz = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tvoid *query_ctx;\n\tvoid *hca_caps;\n\tint err;\n\n\tif (!MLX5_CAP_GEN(esw->dev, migration)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Device doesn't support migration\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!MLX5_CAP_GEN(esw->dev, vhca_resource_manager)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Device doesn't support VHCA management\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmutex_lock(&esw->state_lock);\n\n\tif (vport->info.mig_enabled == enable) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tquery_ctx = kzalloc(query_out_sz, GFP_KERNEL);\n\tif (!query_ctx) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\terr = mlx5_vport_get_other_func_cap(esw->dev, vport->vport, query_ctx,\n\t\t\t\t\t    MLX5_CAP_GENERAL_2);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Failed getting HCA caps\");\n\t\tgoto out_free;\n\t}\n\n\thca_caps = MLX5_ADDR_OF(query_hca_cap_out, query_ctx, capability);\n\tMLX5_SET(cmd_hca_cap_2, hca_caps, migratable, enable);\n\n\terr = mlx5_vport_set_other_func_cap(esw->dev, hca_caps, vport->vport,\n\t\t\t\t\t    MLX5_SET_HCA_CAP_OP_MOD_GENERAL_DEVICE2);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Failed setting HCA migratable cap\");\n\t\tgoto out_free;\n\t}\n\n\tvport->info.mig_enabled = enable;\n\nout_free:\n\tkfree(query_ctx);\nout:\n\tmutex_unlock(&esw->state_lock);\n\treturn err;\n}\n\nint mlx5_devlink_port_fn_roce_get(struct devlink_port *port, bool *is_enabled,\n\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw = mlx5_devlink_eswitch_nocheck_get(port->devlink);\n\tstruct mlx5_vport *vport = mlx5_devlink_port_vport_get(port);\n\n\tif (!MLX5_CAP_GEN(esw->dev, vhca_resource_manager)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Device doesn't support VHCA management\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmutex_lock(&esw->state_lock);\n\t*is_enabled = vport->info.roce_enabled;\n\tmutex_unlock(&esw->state_lock);\n\treturn 0;\n}\n\nint mlx5_devlink_port_fn_roce_set(struct devlink_port *port, bool enable,\n\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw = mlx5_devlink_eswitch_nocheck_get(port->devlink);\n\tstruct mlx5_vport *vport = mlx5_devlink_port_vport_get(port);\n\tint query_out_sz = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tu16 vport_num = vport->vport;\n\tvoid *query_ctx;\n\tvoid *hca_caps;\n\tint err;\n\n\tif (!MLX5_CAP_GEN(esw->dev, vhca_resource_manager)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Device doesn't support VHCA management\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmutex_lock(&esw->state_lock);\n\n\tif (vport->info.roce_enabled == enable) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tquery_ctx = kzalloc(query_out_sz, GFP_KERNEL);\n\tif (!query_ctx) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\terr = mlx5_vport_get_other_func_cap(esw->dev, vport_num, query_ctx,\n\t\t\t\t\t    MLX5_CAP_GENERAL);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Failed getting HCA caps\");\n\t\tgoto out_free;\n\t}\n\n\thca_caps = MLX5_ADDR_OF(query_hca_cap_out, query_ctx, capability);\n\tMLX5_SET(cmd_hca_cap, hca_caps, roce, enable);\n\n\terr = mlx5_vport_set_other_func_cap(esw->dev, hca_caps, vport_num,\n\t\t\t\t\t    MLX5_SET_HCA_CAP_OP_MOD_GENERAL_DEVICE);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Failed setting HCA roce cap\");\n\t\tgoto out_free;\n\t}\n\n\tvport->info.roce_enabled = enable;\n\nout_free:\n\tkfree(query_ctx);\nout:\n\tmutex_unlock(&esw->state_lock);\n\treturn err;\n}\n\nint\nmlx5_eswitch_restore_ipsec_rule(struct mlx5_eswitch *esw, struct mlx5_flow_handle *rule,\n\t\t\t\tstruct mlx5_esw_flow_attr *esw_attr, int attr_idx)\n{\n\tstruct mlx5_flow_destination new_dest = {};\n\tstruct mlx5_flow_destination old_dest = {};\n\n\tif (!esw_setup_uplink_fwd_ipsec_needed(esw, esw_attr, attr_idx))\n\t\treturn 0;\n\n\tesw_setup_dest_fwd_ipsec(&old_dest, NULL, esw, esw_attr, attr_idx, 0, false);\n\tesw_setup_dest_fwd_vport(&new_dest, NULL, esw, esw_attr, attr_idx, 0, false);\n\n\treturn mlx5_modify_rule_destination(rule, &new_dest, &old_dest);\n}\n\n#ifdef CONFIG_XFRM_OFFLOAD\nint mlx5_devlink_port_fn_ipsec_crypto_get(struct devlink_port *port, bool *is_enabled,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw;\n\tstruct mlx5_vport *vport;\n\tint err = 0;\n\n\tesw = mlx5_devlink_eswitch_get(port->devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\tif (!mlx5_esw_ipsec_vf_offload_supported(esw->dev)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Device doesn't support IPSec crypto\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tvport = mlx5_devlink_port_vport_get(port);\n\n\tmutex_lock(&esw->state_lock);\n\tif (!vport->enabled) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto unlock;\n\t}\n\n\t*is_enabled = vport->info.ipsec_crypto_enabled;\nunlock:\n\tmutex_unlock(&esw->state_lock);\n\treturn err;\n}\n\nint mlx5_devlink_port_fn_ipsec_crypto_set(struct devlink_port *port, bool enable,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw;\n\tstruct mlx5_vport *vport;\n\tu16 vport_num;\n\tint err;\n\n\tesw = mlx5_devlink_eswitch_get(port->devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\tvport_num = mlx5_esw_devlink_port_index_to_vport_num(port->index);\n\terr = mlx5_esw_ipsec_vf_crypto_offload_supported(esw->dev, vport_num);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Device doesn't support IPsec crypto\");\n\t\treturn err;\n\t}\n\n\tvport = mlx5_devlink_port_vport_get(port);\n\n\tmutex_lock(&esw->state_lock);\n\tif (!vport->enabled) {\n\t\terr = -EOPNOTSUPP;\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Eswitch vport is disabled\");\n\t\tgoto unlock;\n\t}\n\n\tif (vport->info.ipsec_crypto_enabled == enable)\n\t\tgoto unlock;\n\n\tif (!esw->enabled_ipsec_vf_count && esw->dev->num_ipsec_offloads) {\n\t\terr = -EBUSY;\n\t\tgoto unlock;\n\t}\n\n\terr = mlx5_esw_ipsec_vf_crypto_offload_set(esw, vport, enable);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Failed to set IPsec crypto\");\n\t\tgoto unlock;\n\t}\n\n\tvport->info.ipsec_crypto_enabled = enable;\n\tif (enable)\n\t\tesw->enabled_ipsec_vf_count++;\n\telse\n\t\tesw->enabled_ipsec_vf_count--;\nunlock:\n\tmutex_unlock(&esw->state_lock);\n\treturn err;\n}\n\nint mlx5_devlink_port_fn_ipsec_packet_get(struct devlink_port *port, bool *is_enabled,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw;\n\tstruct mlx5_vport *vport;\n\tint err = 0;\n\n\tesw = mlx5_devlink_eswitch_get(port->devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\tif (!mlx5_esw_ipsec_vf_offload_supported(esw->dev)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Device doesn't support IPsec packet\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tvport = mlx5_devlink_port_vport_get(port);\n\n\tmutex_lock(&esw->state_lock);\n\tif (!vport->enabled) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto unlock;\n\t}\n\n\t*is_enabled = vport->info.ipsec_packet_enabled;\nunlock:\n\tmutex_unlock(&esw->state_lock);\n\treturn err;\n}\n\nint mlx5_devlink_port_fn_ipsec_packet_set(struct devlink_port *port,\n\t\t\t\t\t  bool enable,\n\t\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct mlx5_eswitch *esw;\n\tstruct mlx5_vport *vport;\n\tu16 vport_num;\n\tint err;\n\n\tesw = mlx5_devlink_eswitch_get(port->devlink);\n\tif (IS_ERR(esw))\n\t\treturn PTR_ERR(esw);\n\n\tvport_num = mlx5_esw_devlink_port_index_to_vport_num(port->index);\n\terr = mlx5_esw_ipsec_vf_packet_offload_supported(esw->dev, vport_num);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Device doesn't support IPsec packet mode\");\n\t\treturn err;\n\t}\n\n\tvport = mlx5_devlink_port_vport_get(port);\n\tmutex_lock(&esw->state_lock);\n\tif (!vport->enabled) {\n\t\terr = -EOPNOTSUPP;\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Eswitch vport is disabled\");\n\t\tgoto unlock;\n\t}\n\n\tif (vport->info.ipsec_packet_enabled == enable)\n\t\tgoto unlock;\n\n\tif (!esw->enabled_ipsec_vf_count && esw->dev->num_ipsec_offloads) {\n\t\terr = -EBUSY;\n\t\tgoto unlock;\n\t}\n\n\terr = mlx5_esw_ipsec_vf_packet_offload_set(esw, vport, enable);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Failed to set IPsec packet mode\");\n\t\tgoto unlock;\n\t}\n\n\tvport->info.ipsec_packet_enabled = enable;\n\tif (enable)\n\t\tesw->enabled_ipsec_vf_count++;\n\telse\n\t\tesw->enabled_ipsec_vf_count--;\nunlock:\n\tmutex_unlock(&esw->state_lock);\n\treturn err;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}