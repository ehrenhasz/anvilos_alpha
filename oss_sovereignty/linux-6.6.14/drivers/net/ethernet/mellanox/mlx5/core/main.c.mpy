{
  "module_name": "main.c",
  "hash_id": "62dd26898f775d969b4bff7ac25d92f4bdef025a58c0f1c68cb7a52869e2922a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/main.c",
  "human_readable_source": " \n\n#include <linux/highmem.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/pci.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/delay.h>\n#include <linux/mlx5/driver.h>\n#include <linux/mlx5/cq.h>\n#include <linux/mlx5/qp.h>\n#include <linux/debugfs.h>\n#include <linux/kmod.h>\n#include <linux/mlx5/mlx5_ifc.h>\n#include <linux/mlx5/vport.h>\n#include <linux/version.h>\n#include <net/devlink.h>\n#include \"mlx5_core.h\"\n#include \"lib/eq.h\"\n#include \"fs_core.h\"\n#include \"lib/mpfs.h\"\n#include \"eswitch.h\"\n#include \"devlink.h\"\n#include \"fw_reset.h\"\n#include \"lib/mlx5.h\"\n#include \"lib/tout.h\"\n#include \"fpga/core.h\"\n#include \"en_accel/ipsec.h\"\n#include \"lib/clock.h\"\n#include \"lib/vxlan.h\"\n#include \"lib/geneve.h\"\n#include \"lib/devcom.h\"\n#include \"lib/pci_vsc.h\"\n#include \"diag/fw_tracer.h\"\n#include \"ecpf.h\"\n#include \"lib/hv_vhca.h\"\n#include \"diag/rsc_dump.h\"\n#include \"sf/vhca_event.h\"\n#include \"sf/dev/dev.h\"\n#include \"sf/sf.h\"\n#include \"mlx5_irq.h\"\n#include \"hwmon.h\"\n\nMODULE_AUTHOR(\"Eli Cohen <eli@mellanox.com>\");\nMODULE_DESCRIPTION(\"Mellanox 5th generation network adapters (ConnectX series) core driver\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n\nunsigned int mlx5_core_debug_mask;\nmodule_param_named(debug_mask, mlx5_core_debug_mask, uint, 0644);\nMODULE_PARM_DESC(debug_mask, \"debug mask: 1 = dump cmd data, 2 = dump cmd exec time, 3 = both. Default=0\");\n\nstatic unsigned int prof_sel = MLX5_DEFAULT_PROF;\nmodule_param_named(prof_sel, prof_sel, uint, 0444);\nMODULE_PARM_DESC(prof_sel, \"profile selector. Valid range 0 - 2\");\n\nstatic u32 sw_owner_id[4];\n#define MAX_SW_VHCA_ID (BIT(__mlx5_bit_sz(cmd_hca_cap_2, sw_vhca_id)) - 1)\nstatic DEFINE_IDA(sw_vhca_ida);\n\nenum {\n\tMLX5_ATOMIC_REQ_MODE_BE = 0x0,\n\tMLX5_ATOMIC_REQ_MODE_HOST_ENDIANNESS = 0x1,\n};\n\n#define LOG_MAX_SUPPORTED_QPS 0xff\n\nstatic struct mlx5_profile profile[] = {\n\t[0] = {\n\t\t.mask           = 0,\n\t\t.num_cmd_caches = MLX5_NUM_COMMAND_CACHES,\n\t},\n\t[1] = {\n\t\t.mask\t\t= MLX5_PROF_MASK_QP_SIZE,\n\t\t.log_max_qp\t= 12,\n\t\t.num_cmd_caches = MLX5_NUM_COMMAND_CACHES,\n\n\t},\n\t[2] = {\n\t\t.mask\t\t= MLX5_PROF_MASK_QP_SIZE |\n\t\t\t\t  MLX5_PROF_MASK_MR_CACHE,\n\t\t.log_max_qp\t= LOG_MAX_SUPPORTED_QPS,\n\t\t.num_cmd_caches = MLX5_NUM_COMMAND_CACHES,\n\t\t.mr_cache[0]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[1]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[2]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[3]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[4]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[5]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[6]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[7]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[8]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[9]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[10]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[11]\t= {\n\t\t\t.size\t= 500,\n\t\t\t.limit\t= 250\n\t\t},\n\t\t.mr_cache[12]\t= {\n\t\t\t.size\t= 64,\n\t\t\t.limit\t= 32\n\t\t},\n\t\t.mr_cache[13]\t= {\n\t\t\t.size\t= 32,\n\t\t\t.limit\t= 16\n\t\t},\n\t\t.mr_cache[14]\t= {\n\t\t\t.size\t= 16,\n\t\t\t.limit\t= 8\n\t\t},\n\t\t.mr_cache[15]\t= {\n\t\t\t.size\t= 8,\n\t\t\t.limit\t= 4\n\t\t},\n\t},\n\t[3] = {\n\t\t.mask\t\t= MLX5_PROF_MASK_QP_SIZE,\n\t\t.log_max_qp\t= LOG_MAX_SUPPORTED_QPS,\n\t\t.num_cmd_caches = 0,\n\t},\n};\n\nstatic int wait_fw_init(struct mlx5_core_dev *dev, u32 max_wait_mili,\n\t\t\tu32 warn_time_mili)\n{\n\tunsigned long warn = jiffies + msecs_to_jiffies(warn_time_mili);\n\tunsigned long end = jiffies + msecs_to_jiffies(max_wait_mili);\n\tu32 fw_initializing;\n\tint err = 0;\n\n\tdo {\n\t\tfw_initializing = ioread32be(&dev->iseg->initializing);\n\t\tif (!(fw_initializing >> 31))\n\t\t\tbreak;\n\t\tif (time_after(jiffies, end) ||\n\t\t    test_bit(MLX5_BREAK_FW_WAIT, &dev->intf_state)) {\n\t\t\terr = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t\tif (warn_time_mili && time_after(jiffies, warn)) {\n\t\t\tmlx5_core_warn(dev, \"Waiting for FW initialization, timeout abort in %ds (0x%x)\\n\",\n\t\t\t\t       jiffies_to_msecs(end - warn) / 1000, fw_initializing);\n\t\t\twarn = jiffies + msecs_to_jiffies(warn_time_mili);\n\t\t}\n\t\tmsleep(mlx5_tout_ms(dev, FW_PRE_INIT_WAIT));\n\t} while (true);\n\n\treturn err;\n}\n\nstatic void mlx5_set_driver_version(struct mlx5_core_dev *dev)\n{\n\tint driver_ver_sz = MLX5_FLD_SZ_BYTES(set_driver_version_in,\n\t\t\t\t\t      driver_version);\n\tu8 in[MLX5_ST_SZ_BYTES(set_driver_version_in)] = {};\n\tint remaining_size = driver_ver_sz;\n\tchar *string;\n\n\tif (!MLX5_CAP_GEN(dev, driver_version))\n\t\treturn;\n\n\tstring = MLX5_ADDR_OF(set_driver_version_in, in, driver_version);\n\n\tstrncpy(string, \"Linux\", remaining_size);\n\n\tremaining_size = max_t(int, 0, driver_ver_sz - strlen(string));\n\tstrncat(string, \",\", remaining_size);\n\n\tremaining_size = max_t(int, 0, driver_ver_sz - strlen(string));\n\tstrncat(string, KBUILD_MODNAME, remaining_size);\n\n\tremaining_size = max_t(int, 0, driver_ver_sz - strlen(string));\n\tstrncat(string, \",\", remaining_size);\n\n\tremaining_size = max_t(int, 0, driver_ver_sz - strlen(string));\n\n\tsnprintf(string + strlen(string), remaining_size, \"%u.%u.%u\",\n\t\tLINUX_VERSION_MAJOR, LINUX_VERSION_PATCHLEVEL,\n\t\tLINUX_VERSION_SUBLEVEL);\n\n\t \n\tMLX5_SET(set_driver_version_in, in, opcode,\n\t\t MLX5_CMD_OP_SET_DRIVER_VERSION);\n\n\tmlx5_cmd_exec_in(dev, set_driver_version, in);\n}\n\nstatic int set_dma_caps(struct pci_dev *pdev)\n{\n\tint err;\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_warn(&pdev->dev, \"Warning: couldn't set 64-bit PCI DMA mask\\n\");\n\t\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev, \"Can't set PCI DMA mask, aborting\\n\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tdma_set_max_seg_size(&pdev->dev, 2u * 1024 * 1024 * 1024);\n\treturn err;\n}\n\nstatic int mlx5_pci_enable_device(struct mlx5_core_dev *dev)\n{\n\tstruct pci_dev *pdev = dev->pdev;\n\tint err = 0;\n\n\tmutex_lock(&dev->pci_status_mutex);\n\tif (dev->pci_status == MLX5_PCI_STATUS_DISABLED) {\n\t\terr = pci_enable_device(pdev);\n\t\tif (!err)\n\t\t\tdev->pci_status = MLX5_PCI_STATUS_ENABLED;\n\t}\n\tmutex_unlock(&dev->pci_status_mutex);\n\n\treturn err;\n}\n\nstatic void mlx5_pci_disable_device(struct mlx5_core_dev *dev)\n{\n\tstruct pci_dev *pdev = dev->pdev;\n\n\tmutex_lock(&dev->pci_status_mutex);\n\tif (dev->pci_status == MLX5_PCI_STATUS_ENABLED) {\n\t\tpci_disable_device(pdev);\n\t\tdev->pci_status = MLX5_PCI_STATUS_DISABLED;\n\t}\n\tmutex_unlock(&dev->pci_status_mutex);\n}\n\nstatic int request_bar(struct pci_dev *pdev)\n{\n\tint err = 0;\n\n\tif (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) {\n\t\tdev_err(&pdev->dev, \"Missing registers BAR, aborting\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\terr = pci_request_regions(pdev, KBUILD_MODNAME);\n\tif (err)\n\t\tdev_err(&pdev->dev, \"Couldn't get PCI resources, aborting\\n\");\n\n\treturn err;\n}\n\nstatic void release_bar(struct pci_dev *pdev)\n{\n\tpci_release_regions(pdev);\n}\n\nstruct mlx5_reg_host_endianness {\n\tu8\the;\n\tu8      rsvd[15];\n};\n\nstatic u16 to_fw_pkey_sz(struct mlx5_core_dev *dev, u32 size)\n{\n\tswitch (size) {\n\tcase 128:\n\t\treturn 0;\n\tcase 256:\n\t\treturn 1;\n\tcase 512:\n\t\treturn 2;\n\tcase 1024:\n\t\treturn 3;\n\tcase 2048:\n\t\treturn 4;\n\tcase 4096:\n\t\treturn 5;\n\tdefault:\n\t\tmlx5_core_warn(dev, \"invalid pkey table size %d\\n\", size);\n\t\treturn 0;\n\t}\n}\n\nvoid mlx5_core_uplink_netdev_set(struct mlx5_core_dev *dev, struct net_device *netdev)\n{\n\tmutex_lock(&dev->mlx5e_res.uplink_netdev_lock);\n\tdev->mlx5e_res.uplink_netdev = netdev;\n\tmlx5_blocking_notifier_call_chain(dev, MLX5_DRIVER_EVENT_UPLINK_NETDEV,\n\t\t\t\t\t  netdev);\n\tmutex_unlock(&dev->mlx5e_res.uplink_netdev_lock);\n}\n\nvoid mlx5_core_uplink_netdev_event_replay(struct mlx5_core_dev *dev)\n{\n\tmutex_lock(&dev->mlx5e_res.uplink_netdev_lock);\n\tmlx5_blocking_notifier_call_chain(dev, MLX5_DRIVER_EVENT_UPLINK_NETDEV,\n\t\t\t\t\t  dev->mlx5e_res.uplink_netdev);\n\tmutex_unlock(&dev->mlx5e_res.uplink_netdev_lock);\n}\nEXPORT_SYMBOL(mlx5_core_uplink_netdev_event_replay);\n\nvoid mlx5_core_mp_event_replay(struct mlx5_core_dev *dev, u32 event, void *data)\n{\n\tmlx5_blocking_notifier_call_chain(dev, event, data);\n}\nEXPORT_SYMBOL(mlx5_core_mp_event_replay);\n\nint mlx5_core_get_caps_mode(struct mlx5_core_dev *dev, enum mlx5_cap_type cap_type,\n\t\t\t    enum mlx5_cap_mode cap_mode)\n{\n\tu8 in[MLX5_ST_SZ_BYTES(query_hca_cap_in)];\n\tint out_sz = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tvoid *out, *hca_caps;\n\tu16 opmod = (cap_type << 1) | (cap_mode & 0x01);\n\tint err;\n\n\tmemset(in, 0, sizeof(in));\n\tout = kzalloc(out_sz, GFP_KERNEL);\n\tif (!out)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(query_hca_cap_in, in, opcode, MLX5_CMD_OP_QUERY_HCA_CAP);\n\tMLX5_SET(query_hca_cap_in, in, op_mod, opmod);\n\terr = mlx5_cmd_exec_inout(dev, query_hca_cap, in, out);\n\tif (err) {\n\t\tmlx5_core_warn(dev,\n\t\t\t       \"QUERY_HCA_CAP : type(%x) opmode(%x) Failed(%d)\\n\",\n\t\t\t       cap_type, cap_mode, err);\n\t\tgoto query_ex;\n\t}\n\n\thca_caps =  MLX5_ADDR_OF(query_hca_cap_out, out, capability);\n\n\tswitch (cap_mode) {\n\tcase HCA_CAP_OPMOD_GET_MAX:\n\t\tmemcpy(dev->caps.hca[cap_type]->max, hca_caps,\n\t\t       MLX5_UN_SZ_BYTES(hca_cap_union));\n\t\tbreak;\n\tcase HCA_CAP_OPMOD_GET_CUR:\n\t\tmemcpy(dev->caps.hca[cap_type]->cur, hca_caps,\n\t\t       MLX5_UN_SZ_BYTES(hca_cap_union));\n\t\tbreak;\n\tdefault:\n\t\tmlx5_core_warn(dev,\n\t\t\t       \"Tried to query dev cap type(%x) with wrong opmode(%x)\\n\",\n\t\t\t       cap_type, cap_mode);\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\nquery_ex:\n\tkfree(out);\n\treturn err;\n}\n\nint mlx5_core_get_caps(struct mlx5_core_dev *dev, enum mlx5_cap_type cap_type)\n{\n\tint ret;\n\n\tret = mlx5_core_get_caps_mode(dev, cap_type, HCA_CAP_OPMOD_GET_CUR);\n\tif (ret)\n\t\treturn ret;\n\treturn mlx5_core_get_caps_mode(dev, cap_type, HCA_CAP_OPMOD_GET_MAX);\n}\n\nstatic int set_caps(struct mlx5_core_dev *dev, void *in, int opmod)\n{\n\tMLX5_SET(set_hca_cap_in, in, opcode, MLX5_CMD_OP_SET_HCA_CAP);\n\tMLX5_SET(set_hca_cap_in, in, op_mod, opmod << 1);\n\treturn mlx5_cmd_exec_in(dev, set_hca_cap, in);\n}\n\nstatic int handle_hca_cap_atomic(struct mlx5_core_dev *dev, void *set_ctx)\n{\n\tvoid *set_hca_cap;\n\tint req_endianness;\n\tint err;\n\n\tif (!MLX5_CAP_GEN(dev, atomic))\n\t\treturn 0;\n\n\terr = mlx5_core_get_caps(dev, MLX5_CAP_ATOMIC);\n\tif (err)\n\t\treturn err;\n\n\treq_endianness =\n\t\tMLX5_CAP_ATOMIC(dev,\n\t\t\t\tsupported_atomic_req_8B_endianness_mode_1);\n\n\tif (req_endianness != MLX5_ATOMIC_REQ_MODE_HOST_ENDIANNESS)\n\t\treturn 0;\n\n\tset_hca_cap = MLX5_ADDR_OF(set_hca_cap_in, set_ctx, capability);\n\n\t \n\tMLX5_SET(atomic_caps, set_hca_cap, atomic_req_8B_endianness_mode,\n\t\t MLX5_ATOMIC_REQ_MODE_HOST_ENDIANNESS);\n\n\treturn set_caps(dev, set_ctx, MLX5_SET_HCA_CAP_OP_MOD_ATOMIC);\n}\n\nstatic int handle_hca_cap_odp(struct mlx5_core_dev *dev, void *set_ctx)\n{\n\tvoid *set_hca_cap;\n\tbool do_set = false;\n\tint err;\n\n\tif (!IS_ENABLED(CONFIG_INFINIBAND_ON_DEMAND_PAGING) ||\n\t    !MLX5_CAP_GEN(dev, pg))\n\t\treturn 0;\n\n\terr = mlx5_core_get_caps(dev, MLX5_CAP_ODP);\n\tif (err)\n\t\treturn err;\n\n\tset_hca_cap = MLX5_ADDR_OF(set_hca_cap_in, set_ctx, capability);\n\tmemcpy(set_hca_cap, dev->caps.hca[MLX5_CAP_ODP]->cur,\n\t       MLX5_ST_SZ_BYTES(odp_cap));\n\n#define ODP_CAP_SET_MAX(dev, field)                                            \\\n\tdo {                                                                   \\\n\t\tu32 _res = MLX5_CAP_ODP_MAX(dev, field);                       \\\n\t\tif (_res) {                                                    \\\n\t\t\tdo_set = true;                                         \\\n\t\t\tMLX5_SET(odp_cap, set_hca_cap, field, _res);           \\\n\t\t}                                                              \\\n\t} while (0)\n\n\tODP_CAP_SET_MAX(dev, ud_odp_caps.srq_receive);\n\tODP_CAP_SET_MAX(dev, rc_odp_caps.srq_receive);\n\tODP_CAP_SET_MAX(dev, xrc_odp_caps.srq_receive);\n\tODP_CAP_SET_MAX(dev, xrc_odp_caps.send);\n\tODP_CAP_SET_MAX(dev, xrc_odp_caps.receive);\n\tODP_CAP_SET_MAX(dev, xrc_odp_caps.write);\n\tODP_CAP_SET_MAX(dev, xrc_odp_caps.read);\n\tODP_CAP_SET_MAX(dev, xrc_odp_caps.atomic);\n\tODP_CAP_SET_MAX(dev, dc_odp_caps.srq_receive);\n\tODP_CAP_SET_MAX(dev, dc_odp_caps.send);\n\tODP_CAP_SET_MAX(dev, dc_odp_caps.receive);\n\tODP_CAP_SET_MAX(dev, dc_odp_caps.write);\n\tODP_CAP_SET_MAX(dev, dc_odp_caps.read);\n\tODP_CAP_SET_MAX(dev, dc_odp_caps.atomic);\n\n\tif (!do_set)\n\t\treturn 0;\n\n\treturn set_caps(dev, set_ctx, MLX5_SET_HCA_CAP_OP_MOD_ODP);\n}\n\nstatic int max_uc_list_get_devlink_param(struct mlx5_core_dev *dev)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\tunion devlink_param_value val;\n\tint err;\n\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      DEVLINK_PARAM_GENERIC_ID_MAX_MACS,\n\t\t\t\t\t      &val);\n\tif (!err)\n\t\treturn val.vu32;\n\tmlx5_core_dbg(dev, \"Failed to get param. err = %d\\n\", err);\n\treturn err;\n}\n\nbool mlx5_is_roce_on(struct mlx5_core_dev *dev)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\tunion devlink_param_value val;\n\tint err;\n\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      DEVLINK_PARAM_GENERIC_ID_ENABLE_ROCE,\n\t\t\t\t\t      &val);\n\n\tif (!err)\n\t\treturn val.vbool;\n\n\tmlx5_core_dbg(dev, \"Failed to get param. err = %d\\n\", err);\n\treturn MLX5_CAP_GEN(dev, roce);\n}\nEXPORT_SYMBOL(mlx5_is_roce_on);\n\nstatic int handle_hca_cap_2(struct mlx5_core_dev *dev, void *set_ctx)\n{\n\tvoid *set_hca_cap;\n\tint err;\n\n\tif (!MLX5_CAP_GEN_MAX(dev, hca_cap_2))\n\t\treturn 0;\n\n\terr = mlx5_core_get_caps(dev, MLX5_CAP_GENERAL_2);\n\tif (err)\n\t\treturn err;\n\n\tif (!MLX5_CAP_GEN_2_MAX(dev, sw_vhca_id_valid) ||\n\t    !(dev->priv.sw_vhca_id > 0))\n\t\treturn 0;\n\n\tset_hca_cap = MLX5_ADDR_OF(set_hca_cap_in, set_ctx,\n\t\t\t\t   capability);\n\tmemcpy(set_hca_cap, dev->caps.hca[MLX5_CAP_GENERAL_2]->cur,\n\t       MLX5_ST_SZ_BYTES(cmd_hca_cap_2));\n\tMLX5_SET(cmd_hca_cap_2, set_hca_cap, sw_vhca_id_valid, 1);\n\n\treturn set_caps(dev, set_ctx, MLX5_CAP_GENERAL_2);\n}\n\nstatic int handle_hca_cap(struct mlx5_core_dev *dev, void *set_ctx)\n{\n\tstruct mlx5_profile *prof = &dev->profile;\n\tvoid *set_hca_cap;\n\tint max_uc_list;\n\tint err;\n\n\terr = mlx5_core_get_caps(dev, MLX5_CAP_GENERAL);\n\tif (err)\n\t\treturn err;\n\n\tset_hca_cap = MLX5_ADDR_OF(set_hca_cap_in, set_ctx,\n\t\t\t\t   capability);\n\tmemcpy(set_hca_cap, dev->caps.hca[MLX5_CAP_GENERAL]->cur,\n\t       MLX5_ST_SZ_BYTES(cmd_hca_cap));\n\n\tmlx5_core_dbg(dev, \"Current Pkey table size %d Setting new size %d\\n\",\n\t\t      mlx5_to_sw_pkey_sz(MLX5_CAP_GEN(dev, pkey_table_size)),\n\t\t      128);\n\t \n\tMLX5_SET(cmd_hca_cap, set_hca_cap, pkey_table_size,\n\t\t to_fw_pkey_sz(dev, 128));\n\n\t \n\tif (prof->log_max_qp == LOG_MAX_SUPPORTED_QPS) {\n\t\tprof->log_max_qp = min_t(u8, 18, MLX5_CAP_GEN_MAX(dev, log_max_qp));\n\t} else if (MLX5_CAP_GEN_MAX(dev, log_max_qp) < prof->log_max_qp) {\n\t\tmlx5_core_warn(dev, \"log_max_qp value in current profile is %d, changing it to HCA capability limit (%d)\\n\",\n\t\t\t       prof->log_max_qp,\n\t\t\t       MLX5_CAP_GEN_MAX(dev, log_max_qp));\n\t\tprof->log_max_qp = MLX5_CAP_GEN_MAX(dev, log_max_qp);\n\t}\n\tif (prof->mask & MLX5_PROF_MASK_QP_SIZE)\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap, log_max_qp,\n\t\t\t prof->log_max_qp);\n\n\t \n\tMLX5_SET(cmd_hca_cap, set_hca_cap, cmdif_checksum, 0);\n\n\t \n\tif (MLX5_CAP_GEN_MAX(dev, uar_4k) && PAGE_SIZE > 4096)\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap, uar_4k, 1);\n\n\tMLX5_SET(cmd_hca_cap, set_hca_cap, log_uar_page_sz, PAGE_SHIFT - 12);\n\n\tif (MLX5_CAP_GEN_MAX(dev, cache_line_128byte))\n\t\tMLX5_SET(cmd_hca_cap,\n\t\t\t set_hca_cap,\n\t\t\t cache_line_128byte,\n\t\t\t cache_line_size() >= 128 ? 1 : 0);\n\n\tif (MLX5_CAP_GEN_MAX(dev, dct))\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap, dct, 1);\n\n\tif (MLX5_CAP_GEN_MAX(dev, pci_sync_for_fw_update_event))\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap, pci_sync_for_fw_update_event, 1);\n\tif (MLX5_CAP_GEN_MAX(dev, pci_sync_for_fw_update_with_driver_unload))\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap,\n\t\t\t pci_sync_for_fw_update_with_driver_unload, 1);\n\n\tif (MLX5_CAP_GEN_MAX(dev, num_vhca_ports))\n\t\tMLX5_SET(cmd_hca_cap,\n\t\t\t set_hca_cap,\n\t\t\t num_vhca_ports,\n\t\t\t MLX5_CAP_GEN_MAX(dev, num_vhca_ports));\n\n\tif (MLX5_CAP_GEN_MAX(dev, release_all_pages))\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap, release_all_pages, 1);\n\n\tif (MLX5_CAP_GEN_MAX(dev, mkey_by_name))\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap, mkey_by_name, 1);\n\n\tmlx5_vhca_state_cap_handle(dev, set_hca_cap);\n\n\tif (MLX5_CAP_GEN_MAX(dev, num_total_dynamic_vf_msix))\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap, num_total_dynamic_vf_msix,\n\t\t\t MLX5_CAP_GEN_MAX(dev, num_total_dynamic_vf_msix));\n\n\tif (MLX5_CAP_GEN(dev, roce_rw_supported) && MLX5_CAP_GEN_MAX(dev, roce))\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap, roce,\n\t\t\t mlx5_is_roce_on(dev));\n\n\tmax_uc_list = max_uc_list_get_devlink_param(dev);\n\tif (max_uc_list > 0)\n\t\tMLX5_SET(cmd_hca_cap, set_hca_cap, log_max_current_uc_list,\n\t\t\t ilog2(max_uc_list));\n\n\treturn set_caps(dev, set_ctx, MLX5_SET_HCA_CAP_OP_MOD_GENERAL_DEVICE);\n}\n\n \nstatic bool is_roce_fw_disabled(struct mlx5_core_dev *dev)\n{\n\treturn (MLX5_CAP_GEN(dev, roce_rw_supported) && !mlx5_is_roce_on(dev)) ||\n\t\t(!MLX5_CAP_GEN(dev, roce_rw_supported) && !MLX5_CAP_GEN(dev, roce));\n}\n\nstatic int handle_hca_cap_roce(struct mlx5_core_dev *dev, void *set_ctx)\n{\n\tvoid *set_hca_cap;\n\tint err;\n\n\tif (is_roce_fw_disabled(dev))\n\t\treturn 0;\n\n\terr = mlx5_core_get_caps(dev, MLX5_CAP_ROCE);\n\tif (err)\n\t\treturn err;\n\n\tif (MLX5_CAP_ROCE(dev, sw_r_roce_src_udp_port) ||\n\t    !MLX5_CAP_ROCE_MAX(dev, sw_r_roce_src_udp_port))\n\t\treturn 0;\n\n\tset_hca_cap = MLX5_ADDR_OF(set_hca_cap_in, set_ctx, capability);\n\tmemcpy(set_hca_cap, dev->caps.hca[MLX5_CAP_ROCE]->cur,\n\t       MLX5_ST_SZ_BYTES(roce_cap));\n\tMLX5_SET(roce_cap, set_hca_cap, sw_r_roce_src_udp_port, 1);\n\n\tif (MLX5_CAP_ROCE_MAX(dev, qp_ooo_transmit_default))\n\t\tMLX5_SET(roce_cap, set_hca_cap, qp_ooo_transmit_default, 1);\n\n\terr = set_caps(dev, set_ctx, MLX5_SET_HCA_CAP_OP_MOD_ROCE);\n\treturn err;\n}\n\nstatic int handle_hca_cap_port_selection(struct mlx5_core_dev *dev,\n\t\t\t\t\t void *set_ctx)\n{\n\tvoid *set_hca_cap;\n\tint err;\n\n\tif (!MLX5_CAP_GEN(dev, port_selection_cap))\n\t\treturn 0;\n\n\terr = mlx5_core_get_caps(dev, MLX5_CAP_PORT_SELECTION);\n\tif (err)\n\t\treturn err;\n\n\tif (MLX5_CAP_PORT_SELECTION(dev, port_select_flow_table_bypass) ||\n\t    !MLX5_CAP_PORT_SELECTION_MAX(dev, port_select_flow_table_bypass))\n\t\treturn 0;\n\n\tset_hca_cap = MLX5_ADDR_OF(set_hca_cap_in, set_ctx, capability);\n\tmemcpy(set_hca_cap, dev->caps.hca[MLX5_CAP_PORT_SELECTION]->cur,\n\t       MLX5_ST_SZ_BYTES(port_selection_cap));\n\tMLX5_SET(port_selection_cap, set_hca_cap, port_select_flow_table_bypass, 1);\n\n\terr = set_caps(dev, set_ctx, MLX5_SET_HCA_CAP_OP_MOD_PORT_SELECTION);\n\n\treturn err;\n}\n\nstatic int set_hca_cap(struct mlx5_core_dev *dev)\n{\n\tint set_sz = MLX5_ST_SZ_BYTES(set_hca_cap_in);\n\tvoid *set_ctx;\n\tint err;\n\n\tset_ctx = kzalloc(set_sz, GFP_KERNEL);\n\tif (!set_ctx)\n\t\treturn -ENOMEM;\n\n\terr = handle_hca_cap(dev, set_ctx);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"handle_hca_cap failed\\n\");\n\t\tgoto out;\n\t}\n\n\tmemset(set_ctx, 0, set_sz);\n\terr = handle_hca_cap_atomic(dev, set_ctx);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"handle_hca_cap_atomic failed\\n\");\n\t\tgoto out;\n\t}\n\n\tmemset(set_ctx, 0, set_sz);\n\terr = handle_hca_cap_odp(dev, set_ctx);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"handle_hca_cap_odp failed\\n\");\n\t\tgoto out;\n\t}\n\n\tmemset(set_ctx, 0, set_sz);\n\terr = handle_hca_cap_roce(dev, set_ctx);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"handle_hca_cap_roce failed\\n\");\n\t\tgoto out;\n\t}\n\n\tmemset(set_ctx, 0, set_sz);\n\terr = handle_hca_cap_2(dev, set_ctx);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"handle_hca_cap_2 failed\\n\");\n\t\tgoto out;\n\t}\n\n\tmemset(set_ctx, 0, set_sz);\n\terr = handle_hca_cap_port_selection(dev, set_ctx);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"handle_hca_cap_port_selection failed\\n\");\n\t\tgoto out;\n\t}\n\nout:\n\tkfree(set_ctx);\n\treturn err;\n}\n\nstatic int set_hca_ctrl(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_reg_host_endianness he_in;\n\tstruct mlx5_reg_host_endianness he_out;\n\tint err;\n\n\tif (!mlx5_core_is_pf(dev))\n\t\treturn 0;\n\n\tmemset(&he_in, 0, sizeof(he_in));\n\the_in.he = MLX5_SET_HOST_ENDIANNESS;\n\terr = mlx5_core_access_reg(dev, &he_in,  sizeof(he_in),\n\t\t\t\t\t&he_out, sizeof(he_out),\n\t\t\t\t\tMLX5_REG_HOST_ENDIANNESS, 0, 1);\n\treturn err;\n}\n\nstatic int mlx5_core_set_hca_defaults(struct mlx5_core_dev *dev)\n{\n\tint ret = 0;\n\n\t \n\tif (MLX5_CAP_GEN(dev, port_type) == MLX5_CAP_PORT_TYPE_ETH)\n\t\tret = mlx5_nic_vport_update_local_lb(dev, false);\n\n\treturn ret;\n}\n\nint mlx5_core_enable_hca(struct mlx5_core_dev *dev, u16 func_id)\n{\n\tu32 in[MLX5_ST_SZ_DW(enable_hca_in)] = {};\n\n\tMLX5_SET(enable_hca_in, in, opcode, MLX5_CMD_OP_ENABLE_HCA);\n\tMLX5_SET(enable_hca_in, in, function_id, func_id);\n\tMLX5_SET(enable_hca_in, in, embedded_cpu_function,\n\t\t dev->caps.embedded_cpu);\n\treturn mlx5_cmd_exec_in(dev, enable_hca, in);\n}\n\nint mlx5_core_disable_hca(struct mlx5_core_dev *dev, u16 func_id)\n{\n\tu32 in[MLX5_ST_SZ_DW(disable_hca_in)] = {};\n\n\tMLX5_SET(disable_hca_in, in, opcode, MLX5_CMD_OP_DISABLE_HCA);\n\tMLX5_SET(disable_hca_in, in, function_id, func_id);\n\tMLX5_SET(enable_hca_in, in, embedded_cpu_function,\n\t\t dev->caps.embedded_cpu);\n\treturn mlx5_cmd_exec_in(dev, disable_hca, in);\n}\n\nstatic int mlx5_core_set_issi(struct mlx5_core_dev *dev)\n{\n\tu32 query_out[MLX5_ST_SZ_DW(query_issi_out)] = {};\n\tu32 query_in[MLX5_ST_SZ_DW(query_issi_in)] = {};\n\tu32 sup_issi;\n\tint err;\n\n\tMLX5_SET(query_issi_in, query_in, opcode, MLX5_CMD_OP_QUERY_ISSI);\n\terr = mlx5_cmd_exec_inout(dev, query_issi, query_in, query_out);\n\tif (err) {\n\t\tu32 syndrome = MLX5_GET(query_issi_out, query_out, syndrome);\n\t\tu8 status = MLX5_GET(query_issi_out, query_out, status);\n\n\t\tif (!status || syndrome == MLX5_DRIVER_SYND) {\n\t\t\tmlx5_core_err(dev, \"Failed to query ISSI err(%d) status(%d) synd(%d)\\n\",\n\t\t\t\t      err, status, syndrome);\n\t\t\treturn err;\n\t\t}\n\n\t\tmlx5_core_warn(dev, \"Query ISSI is not supported by FW, ISSI is 0\\n\");\n\t\tdev->issi = 0;\n\t\treturn 0;\n\t}\n\n\tsup_issi = MLX5_GET(query_issi_out, query_out, supported_issi_dw0);\n\n\tif (sup_issi & (1 << 1)) {\n\t\tu32 set_in[MLX5_ST_SZ_DW(set_issi_in)] = {};\n\n\t\tMLX5_SET(set_issi_in, set_in, opcode, MLX5_CMD_OP_SET_ISSI);\n\t\tMLX5_SET(set_issi_in, set_in, current_issi, 1);\n\t\terr = mlx5_cmd_exec_in(dev, set_issi, set_in);\n\t\tif (err) {\n\t\t\tmlx5_core_err(dev, \"Failed to set ISSI to 1 err(%d)\\n\",\n\t\t\t\t      err);\n\t\t\treturn err;\n\t\t}\n\n\t\tdev->issi = 1;\n\n\t\treturn 0;\n\t} else if (sup_issi & (1 << 0) || !sup_issi) {\n\t\treturn 0;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic int mlx5_pci_init(struct mlx5_core_dev *dev, struct pci_dev *pdev,\n\t\t\t const struct pci_device_id *id)\n{\n\tint err = 0;\n\n\tmutex_init(&dev->pci_status_mutex);\n\tpci_set_drvdata(dev->pdev, dev);\n\n\tdev->bar_addr = pci_resource_start(pdev, 0);\n\n\terr = mlx5_pci_enable_device(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Cannot enable PCI device, aborting\\n\");\n\t\treturn err;\n\t}\n\n\terr = request_bar(pdev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"error requesting BARs, aborting\\n\");\n\t\tgoto err_disable;\n\t}\n\n\tpci_set_master(pdev);\n\n\terr = set_dma_caps(pdev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed setting DMA capabilities mask, aborting\\n\");\n\t\tgoto err_clr_master;\n\t}\n\n\tif (pci_enable_atomic_ops_to_root(pdev, PCI_EXP_DEVCAP2_ATOMIC_COMP32) &&\n\t    pci_enable_atomic_ops_to_root(pdev, PCI_EXP_DEVCAP2_ATOMIC_COMP64) &&\n\t    pci_enable_atomic_ops_to_root(pdev, PCI_EXP_DEVCAP2_ATOMIC_COMP128))\n\t\tmlx5_core_dbg(dev, \"Enabling pci atomics failed\\n\");\n\n\tdev->iseg_base = dev->bar_addr;\n\tdev->iseg = ioremap(dev->iseg_base, sizeof(*dev->iseg));\n\tif (!dev->iseg) {\n\t\terr = -ENOMEM;\n\t\tmlx5_core_err(dev, \"Failed mapping initialization segment, aborting\\n\");\n\t\tgoto err_clr_master;\n\t}\n\n\tmlx5_pci_vsc_init(dev);\n\treturn 0;\n\nerr_clr_master:\n\trelease_bar(dev->pdev);\nerr_disable:\n\tmlx5_pci_disable_device(dev);\n\treturn err;\n}\n\nstatic void mlx5_pci_close(struct mlx5_core_dev *dev)\n{\n\t \n\tmlx5_drain_health_wq(dev);\n\tiounmap(dev->iseg);\n\trelease_bar(dev->pdev);\n\tmlx5_pci_disable_device(dev);\n}\n\nstatic int mlx5_init_once(struct mlx5_core_dev *dev)\n{\n\tint err;\n\n\tdev->priv.devc = mlx5_devcom_register_device(dev);\n\tif (IS_ERR(dev->priv.devc))\n\t\tmlx5_core_warn(dev, \"failed to register devcom device %ld\\n\",\n\t\t\t       PTR_ERR(dev->priv.devc));\n\n\terr = mlx5_query_board_id(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"query board id failed\\n\");\n\t\tgoto err_devcom;\n\t}\n\n\terr = mlx5_irq_table_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed to initialize irq table\\n\");\n\t\tgoto err_devcom;\n\t}\n\n\terr = mlx5_eq_table_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed to initialize eq\\n\");\n\t\tgoto err_irq_cleanup;\n\t}\n\n\terr = mlx5_events_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed to initialize events\\n\");\n\t\tgoto err_eq_cleanup;\n\t}\n\n\terr = mlx5_fw_reset_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed to initialize fw reset events\\n\");\n\t\tgoto err_events_cleanup;\n\t}\n\n\tmlx5_cq_debugfs_init(dev);\n\n\tmlx5_init_reserved_gids(dev);\n\n\tmlx5_init_clock(dev);\n\n\tdev->vxlan = mlx5_vxlan_create(dev);\n\tdev->geneve = mlx5_geneve_create(dev);\n\n\terr = mlx5_init_rl_table(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init rate limiting\\n\");\n\t\tgoto err_tables_cleanup;\n\t}\n\n\terr = mlx5_mpfs_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init l2 table %d\\n\", err);\n\t\tgoto err_rl_cleanup;\n\t}\n\n\terr = mlx5_sriov_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init sriov %d\\n\", err);\n\t\tgoto err_mpfs_cleanup;\n\t}\n\n\terr = mlx5_eswitch_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init eswitch %d\\n\", err);\n\t\tgoto err_sriov_cleanup;\n\t}\n\n\terr = mlx5_fpga_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init fpga device %d\\n\", err);\n\t\tgoto err_eswitch_cleanup;\n\t}\n\n\terr = mlx5_vhca_event_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init vhca event notifier %d\\n\", err);\n\t\tgoto err_fpga_cleanup;\n\t}\n\n\terr = mlx5_sf_hw_table_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init SF HW table %d\\n\", err);\n\t\tgoto err_sf_hw_table_cleanup;\n\t}\n\n\terr = mlx5_sf_table_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init SF table %d\\n\", err);\n\t\tgoto err_sf_table_cleanup;\n\t}\n\n\terr = mlx5_fs_core_alloc(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to alloc flow steering\\n\");\n\t\tgoto err_fs;\n\t}\n\n\tdev->dm = mlx5_dm_create(dev);\n\tif (IS_ERR(dev->dm))\n\t\tmlx5_core_warn(dev, \"Failed to init device memory %ld\\n\", PTR_ERR(dev->dm));\n\n\tdev->tracer = mlx5_fw_tracer_create(dev);\n\tdev->hv_vhca = mlx5_hv_vhca_create(dev);\n\tdev->rsc_dump = mlx5_rsc_dump_create(dev);\n\n\treturn 0;\n\nerr_fs:\n\tmlx5_sf_table_cleanup(dev);\nerr_sf_table_cleanup:\n\tmlx5_sf_hw_table_cleanup(dev);\nerr_sf_hw_table_cleanup:\n\tmlx5_vhca_event_cleanup(dev);\nerr_fpga_cleanup:\n\tmlx5_fpga_cleanup(dev);\nerr_eswitch_cleanup:\n\tmlx5_eswitch_cleanup(dev->priv.eswitch);\nerr_sriov_cleanup:\n\tmlx5_sriov_cleanup(dev);\nerr_mpfs_cleanup:\n\tmlx5_mpfs_cleanup(dev);\nerr_rl_cleanup:\n\tmlx5_cleanup_rl_table(dev);\nerr_tables_cleanup:\n\tmlx5_geneve_destroy(dev->geneve);\n\tmlx5_vxlan_destroy(dev->vxlan);\n\tmlx5_cleanup_clock(dev);\n\tmlx5_cleanup_reserved_gids(dev);\n\tmlx5_cq_debugfs_cleanup(dev);\n\tmlx5_fw_reset_cleanup(dev);\nerr_events_cleanup:\n\tmlx5_events_cleanup(dev);\nerr_eq_cleanup:\n\tmlx5_eq_table_cleanup(dev);\nerr_irq_cleanup:\n\tmlx5_irq_table_cleanup(dev);\nerr_devcom:\n\tmlx5_devcom_unregister_device(dev->priv.devc);\n\n\treturn err;\n}\n\nstatic void mlx5_cleanup_once(struct mlx5_core_dev *dev)\n{\n\tmlx5_rsc_dump_destroy(dev);\n\tmlx5_hv_vhca_destroy(dev->hv_vhca);\n\tmlx5_fw_tracer_destroy(dev->tracer);\n\tmlx5_dm_cleanup(dev);\n\tmlx5_fs_core_free(dev);\n\tmlx5_sf_table_cleanup(dev);\n\tmlx5_sf_hw_table_cleanup(dev);\n\tmlx5_vhca_event_cleanup(dev);\n\tmlx5_fpga_cleanup(dev);\n\tmlx5_eswitch_cleanup(dev->priv.eswitch);\n\tmlx5_sriov_cleanup(dev);\n\tmlx5_mpfs_cleanup(dev);\n\tmlx5_cleanup_rl_table(dev);\n\tmlx5_geneve_destroy(dev->geneve);\n\tmlx5_vxlan_destroy(dev->vxlan);\n\tmlx5_cleanup_clock(dev);\n\tmlx5_cleanup_reserved_gids(dev);\n\tmlx5_cq_debugfs_cleanup(dev);\n\tmlx5_fw_reset_cleanup(dev);\n\tmlx5_events_cleanup(dev);\n\tmlx5_eq_table_cleanup(dev);\n\tmlx5_irq_table_cleanup(dev);\n\tmlx5_devcom_unregister_device(dev->priv.devc);\n}\n\nstatic int mlx5_function_enable(struct mlx5_core_dev *dev, bool boot, u64 timeout)\n{\n\tint err;\n\n\tmlx5_core_info(dev, \"firmware version: %d.%d.%d\\n\", fw_rev_maj(dev),\n\t\t       fw_rev_min(dev), fw_rev_sub(dev));\n\n\t \n\tif (mlx5_core_is_pf(dev))\n\t\tpcie_print_link_status(dev->pdev);\n\n\t \n\terr = wait_fw_init(dev, timeout,\n\t\t\t   mlx5_tout_ms(dev, FW_PRE_INIT_WARN_MESSAGE_INTERVAL));\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Firmware over %llu MS in pre-initializing state, aborting\\n\",\n\t\t\t      timeout);\n\t\treturn err;\n\t}\n\n\terr = mlx5_cmd_enable(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed initializing command interface, aborting\\n\");\n\t\treturn err;\n\t}\n\n\tmlx5_tout_query_iseg(dev);\n\n\terr = wait_fw_init(dev, mlx5_tout_ms(dev, FW_INIT), 0);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Firmware over %llu MS in initializing state, aborting\\n\",\n\t\t\t      mlx5_tout_ms(dev, FW_INIT));\n\t\tgoto err_cmd_cleanup;\n\t}\n\n\tdev->caps.embedded_cpu = mlx5_read_embedded_cpu(dev);\n\tmlx5_cmd_set_state(dev, MLX5_CMDIF_STATE_UP);\n\n\tmlx5_start_health_poll(dev);\n\n\terr = mlx5_core_enable_hca(dev, 0);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"enable hca failed\\n\");\n\t\tgoto stop_health_poll;\n\t}\n\n\terr = mlx5_core_set_issi(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed to set issi\\n\");\n\t\tgoto err_disable_hca;\n\t}\n\n\terr = mlx5_satisfy_startup_pages(dev, 1);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed to allocate boot pages\\n\");\n\t\tgoto err_disable_hca;\n\t}\n\n\terr = mlx5_tout_query_dtor(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed to read dtor\\n\");\n\t\tgoto reclaim_boot_pages;\n\t}\n\n\treturn 0;\n\nreclaim_boot_pages:\n\tmlx5_reclaim_startup_pages(dev);\nerr_disable_hca:\n\tmlx5_core_disable_hca(dev, 0);\nstop_health_poll:\n\tmlx5_stop_health_poll(dev, boot);\nerr_cmd_cleanup:\n\tmlx5_cmd_set_state(dev, MLX5_CMDIF_STATE_DOWN);\n\tmlx5_cmd_disable(dev);\n\n\treturn err;\n}\n\nstatic void mlx5_function_disable(struct mlx5_core_dev *dev, bool boot)\n{\n\tmlx5_reclaim_startup_pages(dev);\n\tmlx5_core_disable_hca(dev, 0);\n\tmlx5_stop_health_poll(dev, boot);\n\tmlx5_cmd_set_state(dev, MLX5_CMDIF_STATE_DOWN);\n\tmlx5_cmd_disable(dev);\n}\n\nstatic int mlx5_function_open(struct mlx5_core_dev *dev)\n{\n\tint err;\n\n\terr = set_hca_ctrl(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"set_hca_ctrl failed\\n\");\n\t\treturn err;\n\t}\n\n\terr = set_hca_cap(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"set_hca_cap failed\\n\");\n\t\treturn err;\n\t}\n\n\terr = mlx5_satisfy_startup_pages(dev, 0);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"failed to allocate init pages\\n\");\n\t\treturn err;\n\t}\n\n\terr = mlx5_cmd_init_hca(dev, sw_owner_id);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"init hca failed\\n\");\n\t\treturn err;\n\t}\n\n\tmlx5_set_driver_version(dev);\n\n\terr = mlx5_query_hca_caps(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"query hca failed\\n\");\n\t\treturn err;\n\t}\n\tmlx5_start_health_fw_log_up(dev);\n\treturn 0;\n}\n\nstatic int mlx5_function_close(struct mlx5_core_dev *dev)\n{\n\tint err;\n\n\terr = mlx5_cmd_teardown_hca(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"tear_down_hca failed, skip cleanup\\n\");\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5_function_setup(struct mlx5_core_dev *dev, bool boot, u64 timeout)\n{\n\tint err;\n\n\terr = mlx5_function_enable(dev, boot, timeout);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_function_open(dev);\n\tif (err)\n\t\tmlx5_function_disable(dev, boot);\n\treturn err;\n}\n\nstatic int mlx5_function_teardown(struct mlx5_core_dev *dev, bool boot)\n{\n\tint err = mlx5_function_close(dev);\n\n\tif (!err)\n\t\tmlx5_function_disable(dev, boot);\n\treturn err;\n}\n\nstatic int mlx5_load(struct mlx5_core_dev *dev)\n{\n\tint err;\n\n\tdev->priv.uar = mlx5_get_uars_page(dev);\n\tif (IS_ERR(dev->priv.uar)) {\n\t\tmlx5_core_err(dev, \"Failed allocating uar, aborting\\n\");\n\t\terr = PTR_ERR(dev->priv.uar);\n\t\treturn err;\n\t}\n\n\tmlx5_events_start(dev);\n\tmlx5_pagealloc_start(dev);\n\n\terr = mlx5_irq_table_create(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to alloc IRQs\\n\");\n\t\tgoto err_irq_table;\n\t}\n\n\terr = mlx5_eq_table_create(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to create EQs\\n\");\n\t\tgoto err_eq_table;\n\t}\n\n\terr = mlx5_fw_tracer_init(dev->tracer);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init FW tracer %d\\n\", err);\n\t\tmlx5_fw_tracer_destroy(dev->tracer);\n\t\tdev->tracer = NULL;\n\t}\n\n\tmlx5_fw_reset_events_start(dev);\n\tmlx5_hv_vhca_init(dev->hv_vhca);\n\n\terr = mlx5_rsc_dump_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init Resource dump %d\\n\", err);\n\t\tmlx5_rsc_dump_destroy(dev);\n\t\tdev->rsc_dump = NULL;\n\t}\n\n\terr = mlx5_fpga_device_start(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"fpga device start failed %d\\n\", err);\n\t\tgoto err_fpga_start;\n\t}\n\n\terr = mlx5_fs_core_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init flow steering\\n\");\n\t\tgoto err_fs;\n\t}\n\n\terr = mlx5_core_set_hca_defaults(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to set hca defaults\\n\");\n\t\tgoto err_set_hca;\n\t}\n\n\tmlx5_vhca_event_start(dev);\n\n\terr = mlx5_sf_hw_table_create(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"sf table create failed %d\\n\", err);\n\t\tgoto err_vhca;\n\t}\n\n\terr = mlx5_ec_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed to init embedded CPU\\n\");\n\t\tgoto err_ec;\n\t}\n\n\tmlx5_lag_add_mdev(dev);\n\terr = mlx5_sriov_attach(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"sriov init failed %d\\n\", err);\n\t\tgoto err_sriov;\n\t}\n\n\tmlx5_sf_dev_table_create(dev);\n\n\terr = mlx5_devlink_traps_register(priv_to_devlink(dev));\n\tif (err)\n\t\tgoto err_traps_reg;\n\n\treturn 0;\n\nerr_traps_reg:\n\tmlx5_sf_dev_table_destroy(dev);\n\tmlx5_sriov_detach(dev);\nerr_sriov:\n\tmlx5_lag_remove_mdev(dev);\n\tmlx5_ec_cleanup(dev);\nerr_ec:\n\tmlx5_sf_hw_table_destroy(dev);\nerr_vhca:\n\tmlx5_vhca_event_stop(dev);\nerr_set_hca:\n\tmlx5_fs_core_cleanup(dev);\nerr_fs:\n\tmlx5_fpga_device_stop(dev);\nerr_fpga_start:\n\tmlx5_rsc_dump_cleanup(dev);\n\tmlx5_hv_vhca_cleanup(dev->hv_vhca);\n\tmlx5_fw_reset_events_stop(dev);\n\tmlx5_fw_tracer_cleanup(dev->tracer);\n\tmlx5_eq_table_destroy(dev);\nerr_eq_table:\n\tmlx5_irq_table_destroy(dev);\nerr_irq_table:\n\tmlx5_pagealloc_stop(dev);\n\tmlx5_events_stop(dev);\n\tmlx5_put_uars_page(dev, dev->priv.uar);\n\treturn err;\n}\n\nstatic void mlx5_unload(struct mlx5_core_dev *dev)\n{\n\tmlx5_devlink_traps_unregister(priv_to_devlink(dev));\n\tmlx5_sf_dev_table_destroy(dev);\n\tmlx5_eswitch_disable(dev->priv.eswitch);\n\tmlx5_sriov_detach(dev);\n\tmlx5_lag_remove_mdev(dev);\n\tmlx5_ec_cleanup(dev);\n\tmlx5_sf_hw_table_destroy(dev);\n\tmlx5_vhca_event_stop(dev);\n\tmlx5_fs_core_cleanup(dev);\n\tmlx5_fpga_device_stop(dev);\n\tmlx5_rsc_dump_cleanup(dev);\n\tmlx5_hv_vhca_cleanup(dev->hv_vhca);\n\tmlx5_fw_reset_events_stop(dev);\n\tmlx5_fw_tracer_cleanup(dev->tracer);\n\tmlx5_eq_table_destroy(dev);\n\tmlx5_irq_table_destroy(dev);\n\tmlx5_pagealloc_stop(dev);\n\tmlx5_events_stop(dev);\n\tmlx5_put_uars_page(dev, dev->priv.uar);\n}\n\nint mlx5_init_one_devl_locked(struct mlx5_core_dev *dev)\n{\n\tbool light_probe = mlx5_dev_is_lightweight(dev);\n\tint err = 0;\n\n\tmutex_lock(&dev->intf_state_mutex);\n\tdev->state = MLX5_DEVICE_STATE_UP;\n\n\terr = mlx5_function_setup(dev, true, mlx5_tout_ms(dev, FW_PRE_INIT_TIMEOUT));\n\tif (err)\n\t\tgoto err_function;\n\n\terr = mlx5_init_once(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"sw objs init failed\\n\");\n\t\tgoto function_teardown;\n\t}\n\n\t \n\tif (!light_probe) {\n\t\terr = mlx5_devlink_params_register(priv_to_devlink(dev));\n\t\tif (err)\n\t\t\tgoto err_devlink_params_reg;\n\t}\n\n\terr = mlx5_load(dev);\n\tif (err)\n\t\tgoto err_load;\n\n\tset_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state);\n\n\terr = mlx5_register_device(dev);\n\tif (err)\n\t\tgoto err_register;\n\n\tmutex_unlock(&dev->intf_state_mutex);\n\treturn 0;\n\nerr_register:\n\tclear_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state);\n\tmlx5_unload(dev);\nerr_load:\n\tif (!light_probe)\n\t\tmlx5_devlink_params_unregister(priv_to_devlink(dev));\nerr_devlink_params_reg:\n\tmlx5_cleanup_once(dev);\nfunction_teardown:\n\tmlx5_function_teardown(dev, true);\nerr_function:\n\tdev->state = MLX5_DEVICE_STATE_INTERNAL_ERROR;\n\tmutex_unlock(&dev->intf_state_mutex);\n\treturn err;\n}\n\nint mlx5_init_one(struct mlx5_core_dev *dev)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\tint err;\n\n\tdevl_lock(devlink);\n\terr = mlx5_init_one_devl_locked(dev);\n\tdevl_unlock(devlink);\n\treturn err;\n}\n\nvoid mlx5_uninit_one(struct mlx5_core_dev *dev)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\n\tdevl_lock(devlink);\n\tmutex_lock(&dev->intf_state_mutex);\n\n\tmlx5_unregister_device(dev);\n\n\tif (!test_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state)) {\n\t\tmlx5_core_warn(dev, \"%s: interface is down, NOP\\n\",\n\t\t\t       __func__);\n\t\tmlx5_devlink_params_unregister(priv_to_devlink(dev));\n\t\tmlx5_cleanup_once(dev);\n\t\tgoto out;\n\t}\n\n\tclear_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state);\n\tmlx5_unload(dev);\n\tmlx5_devlink_params_unregister(priv_to_devlink(dev));\n\tmlx5_cleanup_once(dev);\n\tmlx5_function_teardown(dev, true);\nout:\n\tmutex_unlock(&dev->intf_state_mutex);\n\tdevl_unlock(devlink);\n}\n\nint mlx5_load_one_devl_locked(struct mlx5_core_dev *dev, bool recovery)\n{\n\tint err = 0;\n\tu64 timeout;\n\n\tdevl_assert_locked(priv_to_devlink(dev));\n\tmutex_lock(&dev->intf_state_mutex);\n\tif (test_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state)) {\n\t\tmlx5_core_warn(dev, \"interface is up, NOP\\n\");\n\t\tgoto out;\n\t}\n\t \n\tdev->state = MLX5_DEVICE_STATE_UP;\n\n\tif (recovery)\n\t\ttimeout = mlx5_tout_ms(dev, FW_PRE_INIT_ON_RECOVERY_TIMEOUT);\n\telse\n\t\ttimeout = mlx5_tout_ms(dev, FW_PRE_INIT_TIMEOUT);\n\terr = mlx5_function_setup(dev, false, timeout);\n\tif (err)\n\t\tgoto err_function;\n\n\terr = mlx5_load(dev);\n\tif (err)\n\t\tgoto err_load;\n\n\tset_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state);\n\n\terr = mlx5_attach_device(dev);\n\tif (err)\n\t\tgoto err_attach;\n\n\tmutex_unlock(&dev->intf_state_mutex);\n\treturn 0;\n\nerr_attach:\n\tclear_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state);\n\tmlx5_unload(dev);\nerr_load:\n\tmlx5_function_teardown(dev, false);\nerr_function:\n\tdev->state = MLX5_DEVICE_STATE_INTERNAL_ERROR;\nout:\n\tmutex_unlock(&dev->intf_state_mutex);\n\treturn err;\n}\n\nint mlx5_load_one(struct mlx5_core_dev *dev, bool recovery)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\tint ret;\n\n\tdevl_lock(devlink);\n\tret = mlx5_load_one_devl_locked(dev, recovery);\n\tdevl_unlock(devlink);\n\treturn ret;\n}\n\nvoid mlx5_unload_one_devl_locked(struct mlx5_core_dev *dev, bool suspend)\n{\n\tdevl_assert_locked(priv_to_devlink(dev));\n\tmutex_lock(&dev->intf_state_mutex);\n\n\tmlx5_detach_device(dev, suspend);\n\n\tif (!test_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state)) {\n\t\tmlx5_core_warn(dev, \"%s: interface is down, NOP\\n\",\n\t\t\t       __func__);\n\t\tgoto out;\n\t}\n\n\tclear_bit(MLX5_INTERFACE_STATE_UP, &dev->intf_state);\n\tmlx5_unload(dev);\n\tmlx5_function_teardown(dev, false);\nout:\n\tmutex_unlock(&dev->intf_state_mutex);\n}\n\nvoid mlx5_unload_one(struct mlx5_core_dev *dev, bool suspend)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\n\tdevl_lock(devlink);\n\tmlx5_unload_one_devl_locked(dev, suspend);\n\tdevl_unlock(devlink);\n}\n\n \nstatic int mlx5_query_hca_caps_light(struct mlx5_core_dev *dev)\n{\n\tint err;\n\n\terr = mlx5_core_get_caps(dev, MLX5_CAP_GENERAL);\n\tif (err)\n\t\treturn err;\n\n\tif (MLX5_CAP_GEN(dev, eth_net_offloads)) {\n\t\terr = mlx5_core_get_caps_mode(dev, MLX5_CAP_ETHERNET_OFFLOADS,\n\t\t\t\t\t      HCA_CAP_OPMOD_GET_CUR);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (MLX5_CAP_GEN(dev, nic_flow_table) ||\n\t    MLX5_CAP_GEN(dev, ipoib_enhanced_offloads)) {\n\t\terr = mlx5_core_get_caps_mode(dev, MLX5_CAP_FLOW_TABLE,\n\t\t\t\t\t      HCA_CAP_OPMOD_GET_CUR);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (MLX5_CAP_GEN_64(dev, general_obj_types) &\n\t\tMLX5_GENERAL_OBJ_TYPES_CAP_VIRTIO_NET_Q) {\n\t\terr = mlx5_core_get_caps_mode(dev, MLX5_CAP_VDPA_EMULATION,\n\t\t\t\t\t      HCA_CAP_OPMOD_GET_CUR);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nint mlx5_init_one_light(struct mlx5_core_dev *dev)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\tint err;\n\n\tdev->state = MLX5_DEVICE_STATE_UP;\n\terr = mlx5_function_enable(dev, true, mlx5_tout_ms(dev, FW_PRE_INIT_TIMEOUT));\n\tif (err) {\n\t\tmlx5_core_warn(dev, \"mlx5_function_enable err=%d\\n\", err);\n\t\tgoto out;\n\t}\n\n\terr = mlx5_query_hca_caps_light(dev);\n\tif (err) {\n\t\tmlx5_core_warn(dev, \"mlx5_query_hca_caps_light err=%d\\n\", err);\n\t\tgoto query_hca_caps_err;\n\t}\n\n\tdevl_lock(devlink);\n\terr = mlx5_devlink_params_register(priv_to_devlink(dev));\n\tdevl_unlock(devlink);\n\tif (err) {\n\t\tmlx5_core_warn(dev, \"mlx5_devlink_param_reg err = %d\\n\", err);\n\t\tgoto query_hca_caps_err;\n\t}\n\n\treturn 0;\n\nquery_hca_caps_err:\n\tmlx5_function_disable(dev, true);\nout:\n\tdev->state = MLX5_DEVICE_STATE_INTERNAL_ERROR;\n\treturn err;\n}\n\nvoid mlx5_uninit_one_light(struct mlx5_core_dev *dev)\n{\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\n\tdevl_lock(devlink);\n\tmlx5_devlink_params_unregister(priv_to_devlink(dev));\n\tdevl_unlock(devlink);\n\tif (dev->state != MLX5_DEVICE_STATE_UP)\n\t\treturn;\n\tmlx5_function_disable(dev, true);\n}\n\n \n\nvoid mlx5_unload_one_light(struct mlx5_core_dev *dev)\n{\n\tif (dev->state != MLX5_DEVICE_STATE_UP)\n\t\treturn;\n\tmlx5_function_disable(dev, false);\n\tdev->state = MLX5_DEVICE_STATE_INTERNAL_ERROR;\n}\n\nstatic const int types[] = {\n\tMLX5_CAP_GENERAL,\n\tMLX5_CAP_GENERAL_2,\n\tMLX5_CAP_ETHERNET_OFFLOADS,\n\tMLX5_CAP_IPOIB_ENHANCED_OFFLOADS,\n\tMLX5_CAP_ODP,\n\tMLX5_CAP_ATOMIC,\n\tMLX5_CAP_ROCE,\n\tMLX5_CAP_IPOIB_OFFLOADS,\n\tMLX5_CAP_FLOW_TABLE,\n\tMLX5_CAP_ESWITCH_FLOW_TABLE,\n\tMLX5_CAP_ESWITCH,\n\tMLX5_CAP_QOS,\n\tMLX5_CAP_DEBUG,\n\tMLX5_CAP_DEV_MEM,\n\tMLX5_CAP_DEV_EVENT,\n\tMLX5_CAP_TLS,\n\tMLX5_CAP_VDPA_EMULATION,\n\tMLX5_CAP_IPSEC,\n\tMLX5_CAP_PORT_SELECTION,\n\tMLX5_CAP_MACSEC,\n\tMLX5_CAP_ADV_VIRTUALIZATION,\n\tMLX5_CAP_CRYPTO,\n};\n\nstatic void mlx5_hca_caps_free(struct mlx5_core_dev *dev)\n{\n\tint type;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(types); i++) {\n\t\ttype = types[i];\n\t\tkfree(dev->caps.hca[type]);\n\t}\n}\n\nstatic int mlx5_hca_caps_alloc(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_hca_cap *cap;\n\tint type;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(types); i++) {\n\t\tcap = kzalloc(sizeof(*cap), GFP_KERNEL);\n\t\tif (!cap)\n\t\t\tgoto err;\n\t\ttype = types[i];\n\t\tdev->caps.hca[type] = cap;\n\t}\n\n\treturn 0;\n\nerr:\n\tmlx5_hca_caps_free(dev);\n\treturn -ENOMEM;\n}\n\nstatic int vhca_id_show(struct seq_file *file, void *priv)\n{\n\tstruct mlx5_core_dev *dev = file->private;\n\n\tseq_printf(file, \"0x%x\\n\", MLX5_CAP_GEN(dev, vhca_id));\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(vhca_id);\n\nint mlx5_mdev_init(struct mlx5_core_dev *dev, int profile_idx)\n{\n\tstruct mlx5_priv *priv = &dev->priv;\n\tint err;\n\n\tmemcpy(&dev->profile, &profile[profile_idx], sizeof(dev->profile));\n\tlockdep_register_key(&dev->lock_key);\n\tmutex_init(&dev->intf_state_mutex);\n\tlockdep_set_class(&dev->intf_state_mutex, &dev->lock_key);\n\tmutex_init(&dev->mlx5e_res.uplink_netdev_lock);\n\n\tmutex_init(&priv->bfregs.reg_head.lock);\n\tmutex_init(&priv->bfregs.wc_head.lock);\n\tINIT_LIST_HEAD(&priv->bfregs.reg_head.list);\n\tINIT_LIST_HEAD(&priv->bfregs.wc_head.list);\n\n\tmutex_init(&priv->alloc_mutex);\n\tmutex_init(&priv->pgdir_mutex);\n\tINIT_LIST_HEAD(&priv->pgdir_list);\n\n\tpriv->numa_node = dev_to_node(mlx5_core_dma_dev(dev));\n\tpriv->dbg.dbg_root = debugfs_create_dir(dev_name(dev->device),\n\t\t\t\t\t\tmlx5_debugfs_root);\n\tdebugfs_create_file(\"vhca_id\", 0400, priv->dbg.dbg_root, dev, &vhca_id_fops);\n\tINIT_LIST_HEAD(&priv->traps);\n\n\terr = mlx5_cmd_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed initializing cmdif SW structs, aborting\\n\");\n\t\tgoto err_cmd_init;\n\t}\n\n\terr = mlx5_tout_init(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"Failed initializing timeouts, aborting\\n\");\n\t\tgoto err_timeout_init;\n\t}\n\n\terr = mlx5_health_init(dev);\n\tif (err)\n\t\tgoto err_health_init;\n\n\terr = mlx5_pagealloc_init(dev);\n\tif (err)\n\t\tgoto err_pagealloc_init;\n\n\terr = mlx5_adev_init(dev);\n\tif (err)\n\t\tgoto err_adev_init;\n\n\terr = mlx5_hca_caps_alloc(dev);\n\tif (err)\n\t\tgoto err_hca_caps;\n\n\t \n\tdev->priv.sw_vhca_id = ida_alloc_range(&sw_vhca_ida, 1,\n\t\t\t\t\t       MAX_SW_VHCA_ID,\n\t\t\t\t\t       GFP_KERNEL);\n\tif (dev->priv.sw_vhca_id < 0)\n\t\tmlx5_core_err(dev, \"failed to allocate sw_vhca_id, err=%d\\n\",\n\t\t\t      dev->priv.sw_vhca_id);\n\n\treturn 0;\n\nerr_hca_caps:\n\tmlx5_adev_cleanup(dev);\nerr_adev_init:\n\tmlx5_pagealloc_cleanup(dev);\nerr_pagealloc_init:\n\tmlx5_health_cleanup(dev);\nerr_health_init:\n\tmlx5_tout_cleanup(dev);\nerr_timeout_init:\n\tmlx5_cmd_cleanup(dev);\nerr_cmd_init:\n\tdebugfs_remove(dev->priv.dbg.dbg_root);\n\tmutex_destroy(&priv->pgdir_mutex);\n\tmutex_destroy(&priv->alloc_mutex);\n\tmutex_destroy(&priv->bfregs.wc_head.lock);\n\tmutex_destroy(&priv->bfregs.reg_head.lock);\n\tmutex_destroy(&dev->intf_state_mutex);\n\tlockdep_unregister_key(&dev->lock_key);\n\treturn err;\n}\n\nvoid mlx5_mdev_uninit(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_priv *priv = &dev->priv;\n\n\tif (priv->sw_vhca_id > 0)\n\t\tida_free(&sw_vhca_ida, dev->priv.sw_vhca_id);\n\n\tmlx5_hca_caps_free(dev);\n\tmlx5_adev_cleanup(dev);\n\tmlx5_pagealloc_cleanup(dev);\n\tmlx5_health_cleanup(dev);\n\tmlx5_tout_cleanup(dev);\n\tmlx5_cmd_cleanup(dev);\n\tdebugfs_remove_recursive(dev->priv.dbg.dbg_root);\n\tmutex_destroy(&priv->pgdir_mutex);\n\tmutex_destroy(&priv->alloc_mutex);\n\tmutex_destroy(&priv->bfregs.wc_head.lock);\n\tmutex_destroy(&priv->bfregs.reg_head.lock);\n\tmutex_destroy(&dev->mlx5e_res.uplink_netdev_lock);\n\tmutex_destroy(&dev->intf_state_mutex);\n\tlockdep_unregister_key(&dev->lock_key);\n}\n\nstatic int probe_one(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct mlx5_core_dev *dev;\n\tstruct devlink *devlink;\n\tint err;\n\n\tdevlink = mlx5_devlink_alloc(&pdev->dev);\n\tif (!devlink) {\n\t\tdev_err(&pdev->dev, \"devlink alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tdev = devlink_priv(devlink);\n\tdev->device = &pdev->dev;\n\tdev->pdev = pdev;\n\n\tdev->coredev_type = id->driver_data & MLX5_PCI_DEV_IS_VF ?\n\t\t\t MLX5_COREDEV_VF : MLX5_COREDEV_PF;\n\n\tdev->priv.adev_idx = mlx5_adev_idx_alloc();\n\tif (dev->priv.adev_idx < 0) {\n\t\terr = dev->priv.adev_idx;\n\t\tgoto adev_init_err;\n\t}\n\n\terr = mlx5_mdev_init(dev, prof_sel);\n\tif (err)\n\t\tgoto mdev_init_err;\n\n\terr = mlx5_pci_init(dev, pdev, id);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"mlx5_pci_init failed with error code %d\\n\",\n\t\t\t      err);\n\t\tgoto pci_init_err;\n\t}\n\n\terr = mlx5_init_one(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"mlx5_init_one failed with error code %d\\n\",\n\t\t\t      err);\n\t\tgoto err_init_one;\n\t}\n\n\terr = mlx5_crdump_enable(dev);\n\tif (err)\n\t\tdev_err(&pdev->dev, \"mlx5_crdump_enable failed with error code %d\\n\", err);\n\n\terr = mlx5_hwmon_dev_register(dev);\n\tif (err)\n\t\tmlx5_core_err(dev, \"mlx5_hwmon_dev_register failed with error code %d\\n\", err);\n\n\tpci_save_state(pdev);\n\tdevlink_register(devlink);\n\treturn 0;\n\nerr_init_one:\n\tmlx5_pci_close(dev);\npci_init_err:\n\tmlx5_mdev_uninit(dev);\nmdev_init_err:\n\tmlx5_adev_idx_free(dev->priv.adev_idx);\nadev_init_err:\n\tmlx5_devlink_free(devlink);\n\n\treturn err;\n}\n\nstatic void remove_one(struct pci_dev *pdev)\n{\n\tstruct mlx5_core_dev *dev  = pci_get_drvdata(pdev);\n\tstruct devlink *devlink = priv_to_devlink(dev);\n\n\tset_bit(MLX5_BREAK_FW_WAIT, &dev->intf_state);\n\t \n\tmlx5_drain_fw_reset(dev);\n\tmlx5_drain_health_wq(dev);\n\tdevlink_unregister(devlink);\n\tmlx5_sriov_disable(pdev, false);\n\tmlx5_hwmon_dev_unregister(dev);\n\tmlx5_crdump_disable(dev);\n\tmlx5_uninit_one(dev);\n\tmlx5_pci_close(dev);\n\tmlx5_mdev_uninit(dev);\n\tmlx5_adev_idx_free(dev->priv.adev_idx);\n\tmlx5_devlink_free(devlink);\n}\n\n#define mlx5_pci_trace(dev, fmt, ...) ({ \\\n\tstruct mlx5_core_dev *__dev = (dev); \\\n\tmlx5_core_info(__dev, \"%s Device state = %d health sensors: %d pci_status: %d. \" fmt, \\\n\t\t       __func__, __dev->state, mlx5_health_check_fatal_sensors(__dev), \\\n\t\t       __dev->pci_status, ##__VA_ARGS__); \\\n})\n\nstatic const char *result2str(enum pci_ers_result result)\n{\n\treturn  result == PCI_ERS_RESULT_NEED_RESET ? \"need reset\" :\n\t\tresult == PCI_ERS_RESULT_DISCONNECT ? \"disconnect\" :\n\t\tresult == PCI_ERS_RESULT_RECOVERED  ? \"recovered\" :\n\t\t\"unknown\";\n}\n\nstatic pci_ers_result_t mlx5_pci_err_detected(struct pci_dev *pdev,\n\t\t\t\t\t      pci_channel_state_t state)\n{\n\tstruct mlx5_core_dev *dev = pci_get_drvdata(pdev);\n\tenum pci_ers_result res;\n\n\tmlx5_pci_trace(dev, \"Enter, pci channel state = %d\\n\", state);\n\n\tmlx5_enter_error_state(dev, false);\n\tmlx5_error_sw_reset(dev);\n\tmlx5_unload_one(dev, false);\n\tmlx5_drain_health_wq(dev);\n\tmlx5_pci_disable_device(dev);\n\n\tres = state == pci_channel_io_perm_failure ?\n\t\tPCI_ERS_RESULT_DISCONNECT : PCI_ERS_RESULT_NEED_RESET;\n\n\tmlx5_core_info(dev, \"%s Device state = %d pci_status: %d. Exit, result = %d, %s\\n\",\n\t\t       __func__, dev->state, dev->pci_status, res, result2str(res));\n\treturn res;\n}\n\n \nstatic int wait_vital(struct pci_dev *pdev)\n{\n\tstruct mlx5_core_dev *dev = pci_get_drvdata(pdev);\n\tstruct mlx5_core_health *health = &dev->priv.health;\n\tconst int niter = 100;\n\tu32 last_count = 0;\n\tu32 count;\n\tint i;\n\n\tfor (i = 0; i < niter; i++) {\n\t\tcount = ioread32be(health->health_counter);\n\t\tif (count && count != 0xffffffff) {\n\t\t\tif (last_count && last_count != count) {\n\t\t\t\tmlx5_core_info(dev,\n\t\t\t\t\t       \"wait vital counter value 0x%x after %d iterations\\n\",\n\t\t\t\t\t       count, i);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tlast_count = count;\n\t\t}\n\t\tmsleep(50);\n\t}\n\n\treturn -ETIMEDOUT;\n}\n\nstatic pci_ers_result_t mlx5_pci_slot_reset(struct pci_dev *pdev)\n{\n\tenum pci_ers_result res = PCI_ERS_RESULT_DISCONNECT;\n\tstruct mlx5_core_dev *dev = pci_get_drvdata(pdev);\n\tint err;\n\n\tmlx5_core_info(dev, \"%s Device state = %d pci_status: %d. Enter\\n\",\n\t\t       __func__, dev->state, dev->pci_status);\n\n\terr = mlx5_pci_enable_device(dev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"%s: mlx5_pci_enable_device failed with error code: %d\\n\",\n\t\t\t      __func__, err);\n\t\tgoto out;\n\t}\n\n\tpci_set_master(pdev);\n\tpci_restore_state(pdev);\n\tpci_save_state(pdev);\n\n\terr = wait_vital(pdev);\n\tif (err) {\n\t\tmlx5_core_err(dev, \"%s: wait vital failed with error code: %d\\n\",\n\t\t\t      __func__, err);\n\t\tgoto out;\n\t}\n\n\tres = PCI_ERS_RESULT_RECOVERED;\nout:\n\tmlx5_core_info(dev, \"%s Device state = %d pci_status: %d. Exit, err = %d, result = %d, %s\\n\",\n\t\t       __func__, dev->state, dev->pci_status, err, res, result2str(res));\n\treturn res;\n}\n\nstatic void mlx5_pci_resume(struct pci_dev *pdev)\n{\n\tstruct mlx5_core_dev *dev = pci_get_drvdata(pdev);\n\tint err;\n\n\tmlx5_pci_trace(dev, \"Enter, loading driver..\\n\");\n\n\terr = mlx5_load_one(dev, false);\n\n\tif (!err)\n\t\tdevlink_health_reporter_state_update(dev->priv.health.fw_fatal_reporter,\n\t\t\t\t\t\t     DEVLINK_HEALTH_REPORTER_STATE_HEALTHY);\n\n\tmlx5_pci_trace(dev, \"Done, err = %d, device %s\\n\", err,\n\t\t       !err ? \"recovered\" : \"Failed\");\n}\n\nstatic const struct pci_error_handlers mlx5_err_handler = {\n\t.error_detected = mlx5_pci_err_detected,\n\t.slot_reset\t= mlx5_pci_slot_reset,\n\t.resume\t\t= mlx5_pci_resume\n};\n\nstatic int mlx5_try_fast_unload(struct mlx5_core_dev *dev)\n{\n\tbool fast_teardown = false, force_teardown = false;\n\tint ret = 1;\n\n\tfast_teardown = MLX5_CAP_GEN(dev, fast_teardown);\n\tforce_teardown = MLX5_CAP_GEN(dev, force_teardown);\n\n\tmlx5_core_dbg(dev, \"force teardown firmware support=%d\\n\", force_teardown);\n\tmlx5_core_dbg(dev, \"fast teardown firmware support=%d\\n\", fast_teardown);\n\n\tif (!fast_teardown && !force_teardown)\n\t\treturn -EOPNOTSUPP;\n\n\tif (dev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR) {\n\t\tmlx5_core_dbg(dev, \"Device in internal error state, giving up\\n\");\n\t\treturn -EAGAIN;\n\t}\n\n\t \n\tmlx5_drain_health_wq(dev);\n\tmlx5_stop_health_poll(dev, false);\n\n\tret = mlx5_cmd_fast_teardown_hca(dev);\n\tif (!ret)\n\t\tgoto succeed;\n\n\tret = mlx5_cmd_force_teardown_hca(dev);\n\tif (!ret)\n\t\tgoto succeed;\n\n\tmlx5_core_dbg(dev, \"Firmware couldn't do fast unload error: %d\\n\", ret);\n\tmlx5_start_health_poll(dev);\n\treturn ret;\n\nsucceed:\n\tmlx5_enter_error_state(dev, true);\n\n\t \n\tmlx5_core_eq_free_irqs(dev);\n\n\treturn 0;\n}\n\nstatic void shutdown(struct pci_dev *pdev)\n{\n\tstruct mlx5_core_dev *dev  = pci_get_drvdata(pdev);\n\tint err;\n\n\tmlx5_core_info(dev, \"Shutdown was called\\n\");\n\tset_bit(MLX5_BREAK_FW_WAIT, &dev->intf_state);\n\terr = mlx5_try_fast_unload(dev);\n\tif (err)\n\t\tmlx5_unload_one(dev, false);\n\tmlx5_pci_disable_device(dev);\n}\n\nstatic int mlx5_suspend(struct pci_dev *pdev, pm_message_t state)\n{\n\tstruct mlx5_core_dev *dev = pci_get_drvdata(pdev);\n\n\tmlx5_unload_one(dev, true);\n\n\treturn 0;\n}\n\nstatic int mlx5_resume(struct pci_dev *pdev)\n{\n\tstruct mlx5_core_dev *dev = pci_get_drvdata(pdev);\n\n\treturn mlx5_load_one(dev, false);\n}\n\nstatic const struct pci_device_id mlx5_core_pci_table[] = {\n\t{ PCI_VDEVICE(MELLANOX, PCI_DEVICE_ID_MELLANOX_CONNECTIB) },\n\t{ PCI_VDEVICE(MELLANOX, 0x1012), MLX5_PCI_DEV_IS_VF},\t \n\t{ PCI_VDEVICE(MELLANOX, PCI_DEVICE_ID_MELLANOX_CONNECTX4) },\n\t{ PCI_VDEVICE(MELLANOX, 0x1014), MLX5_PCI_DEV_IS_VF},\t \n\t{ PCI_VDEVICE(MELLANOX, PCI_DEVICE_ID_MELLANOX_CONNECTX4_LX) },\n\t{ PCI_VDEVICE(MELLANOX, 0x1016), MLX5_PCI_DEV_IS_VF},\t \n\t{ PCI_VDEVICE(MELLANOX, 0x1017) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0x1018), MLX5_PCI_DEV_IS_VF},\t \n\t{ PCI_VDEVICE(MELLANOX, 0x1019) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0x101a), MLX5_PCI_DEV_IS_VF},\t \n\t{ PCI_VDEVICE(MELLANOX, 0x101b) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0x101c), MLX5_PCI_DEV_IS_VF},\t \n\t{ PCI_VDEVICE(MELLANOX, 0x101d) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0x101e), MLX5_PCI_DEV_IS_VF},\t \n\t{ PCI_VDEVICE(MELLANOX, 0x101f) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0x1021) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0x1023) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0xa2d2) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0xa2d3), MLX5_PCI_DEV_IS_VF},\t \n\t{ PCI_VDEVICE(MELLANOX, 0xa2d6) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0xa2dc) },\t\t\t \n\t{ PCI_VDEVICE(MELLANOX, 0xa2df) },\t\t\t \n\t{ 0, }\n};\n\nMODULE_DEVICE_TABLE(pci, mlx5_core_pci_table);\n\nvoid mlx5_disable_device(struct mlx5_core_dev *dev)\n{\n\tmlx5_error_sw_reset(dev);\n\tmlx5_unload_one_devl_locked(dev, false);\n}\n\nint mlx5_recover_device(struct mlx5_core_dev *dev)\n{\n\tif (!mlx5_core_is_sf(dev)) {\n\t\tmlx5_pci_disable_device(dev);\n\t\tif (mlx5_pci_slot_reset(dev->pdev) != PCI_ERS_RESULT_RECOVERED)\n\t\t\treturn -EIO;\n\t}\n\n\treturn mlx5_load_one_devl_locked(dev, true);\n}\n\nstatic struct pci_driver mlx5_core_driver = {\n\t.name           = KBUILD_MODNAME,\n\t.id_table       = mlx5_core_pci_table,\n\t.probe          = probe_one,\n\t.remove         = remove_one,\n\t.suspend        = mlx5_suspend,\n\t.resume         = mlx5_resume,\n\t.shutdown\t= shutdown,\n\t.err_handler\t= &mlx5_err_handler,\n\t.sriov_configure   = mlx5_core_sriov_configure,\n\t.sriov_get_vf_total_msix = mlx5_sriov_get_vf_total_msix,\n\t.sriov_set_msix_vec_count = mlx5_core_sriov_set_msix_vec_count,\n};\n\n \nstruct mlx5_core_dev *mlx5_vf_get_core_dev(struct pci_dev *pdev)\n{\n\tstruct mlx5_core_dev *mdev;\n\n\tmdev = pci_iov_get_pf_drvdata(pdev, &mlx5_core_driver);\n\tif (IS_ERR(mdev))\n\t\treturn NULL;\n\n\tmutex_lock(&mdev->intf_state_mutex);\n\tif (!test_bit(MLX5_INTERFACE_STATE_UP, &mdev->intf_state)) {\n\t\tmutex_unlock(&mdev->intf_state_mutex);\n\t\treturn NULL;\n\t}\n\n\treturn mdev;\n}\nEXPORT_SYMBOL(mlx5_vf_get_core_dev);\n\n \nvoid mlx5_vf_put_core_dev(struct mlx5_core_dev *mdev)\n{\n\tmutex_unlock(&mdev->intf_state_mutex);\n}\nEXPORT_SYMBOL(mlx5_vf_put_core_dev);\n\nstatic void mlx5_core_verify_params(void)\n{\n\tif (prof_sel >= ARRAY_SIZE(profile)) {\n\t\tpr_warn(\"mlx5_core: WARNING: Invalid module parameter prof_sel %d, valid range 0-%zu, changing back to default(%d)\\n\",\n\t\t\tprof_sel,\n\t\t\tARRAY_SIZE(profile) - 1,\n\t\t\tMLX5_DEFAULT_PROF);\n\t\tprof_sel = MLX5_DEFAULT_PROF;\n\t}\n}\n\nstatic int __init mlx5_init(void)\n{\n\tint err;\n\n\tWARN_ONCE(strcmp(MLX5_ADEV_NAME, KBUILD_MODNAME),\n\t\t  \"mlx5_core name not in sync with kernel module name\");\n\n\tget_random_bytes(&sw_owner_id, sizeof(sw_owner_id));\n\n\tmlx5_core_verify_params();\n\tmlx5_register_debugfs();\n\n\terr = mlx5e_init();\n\tif (err)\n\t\tgoto err_debug;\n\n\terr = mlx5_sf_driver_register();\n\tif (err)\n\t\tgoto err_sf;\n\n\terr = pci_register_driver(&mlx5_core_driver);\n\tif (err)\n\t\tgoto err_pci;\n\n\treturn 0;\n\nerr_pci:\n\tmlx5_sf_driver_unregister();\nerr_sf:\n\tmlx5e_cleanup();\nerr_debug:\n\tmlx5_unregister_debugfs();\n\treturn err;\n}\n\nstatic void __exit mlx5_cleanup(void)\n{\n\tpci_unregister_driver(&mlx5_core_driver);\n\tmlx5_sf_driver_unregister();\n\tmlx5e_cleanup();\n\tmlx5_unregister_debugfs();\n}\n\nmodule_init(mlx5_init);\nmodule_exit(mlx5_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}