{
  "module_name": "en_stats.h",
  "hash_id": "9325aead6b9b025c9d63c291c48fa031f1b4bfa6fa7f23b6670b76c81bd60e23",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en_stats.h",
  "human_readable_source": " \n\n#ifndef __MLX5_EN_STATS_H__\n#define __MLX5_EN_STATS_H__\n\n#define MLX5E_READ_CTR64_CPU(ptr, dsc, i) \\\n\t(*(u64 *)((char *)ptr + dsc[i].offset))\n#define MLX5E_READ_CTR64_BE(ptr, dsc, i) \\\n\tbe64_to_cpu(*(__be64 *)((char *)ptr + dsc[i].offset))\n#define MLX5E_READ_CTR32_CPU(ptr, dsc, i) \\\n\t(*(u32 *)((char *)ptr + dsc[i].offset))\n#define MLX5E_READ_CTR32_BE(ptr, dsc, i) \\\n\tbe32_to_cpu(*(__be32 *)((char *)ptr + dsc[i].offset))\n\n#define MLX5E_DECLARE_STAT(type, fld) #fld, offsetof(type, fld)\n#define MLX5E_DECLARE_RX_STAT(type, fld) \"rx%d_\"#fld, offsetof(type, fld)\n#define MLX5E_DECLARE_TX_STAT(type, fld) \"tx%d_\"#fld, offsetof(type, fld)\n#define MLX5E_DECLARE_XDPSQ_STAT(type, fld) \"tx%d_xdp_\"#fld, offsetof(type, fld)\n#define MLX5E_DECLARE_RQ_XDPSQ_STAT(type, fld) \"rx%d_xdp_tx_\"#fld, offsetof(type, fld)\n#define MLX5E_DECLARE_XSKRQ_STAT(type, fld) \"rx%d_xsk_\"#fld, offsetof(type, fld)\n#define MLX5E_DECLARE_XSKSQ_STAT(type, fld) \"tx%d_xsk_\"#fld, offsetof(type, fld)\n#define MLX5E_DECLARE_CH_STAT(type, fld) \"ch%d_\"#fld, offsetof(type, fld)\n\n#define MLX5E_DECLARE_PTP_TX_STAT(type, fld) \"ptp_tx%d_\"#fld, offsetof(type, fld)\n#define MLX5E_DECLARE_PTP_CH_STAT(type, fld) \"ptp_ch_\"#fld, offsetof(type, fld)\n#define MLX5E_DECLARE_PTP_CQ_STAT(type, fld) \"ptp_cq%d_\"#fld, offsetof(type, fld)\n#define MLX5E_DECLARE_PTP_RQ_STAT(type, fld) \"ptp_rq%d_\"#fld, offsetof(type, fld)\n\n#define MLX5E_DECLARE_QOS_TX_STAT(type, fld) \"qos_tx%d_\"#fld, offsetof(type, fld)\n\nstruct counter_desc {\n\tchar\t\tformat[ETH_GSTRING_LEN];\n\tsize_t\t\toffset;  \n};\n\nenum {\n\tMLX5E_NDO_UPDATE_STATS = BIT(0x1),\n};\n\nstruct mlx5e_priv;\nstruct mlx5e_stats_grp {\n\tu16 update_stats_mask;\n\tint (*get_num_stats)(struct mlx5e_priv *priv);\n\tint (*fill_strings)(struct mlx5e_priv *priv, u8 *data, int idx);\n\tint (*fill_stats)(struct mlx5e_priv *priv, u64 *data, int idx);\n\tvoid (*update_stats)(struct mlx5e_priv *priv);\n};\n\ntypedef const struct mlx5e_stats_grp *const mlx5e_stats_grp_t;\n\n#define MLX5E_STATS_GRP_OP(grp, name) mlx5e_stats_grp_ ## grp ## _ ## name\n\n#define MLX5E_DECLARE_STATS_GRP_OP_NUM_STATS(grp) \\\n\tint MLX5E_STATS_GRP_OP(grp, num_stats)(struct mlx5e_priv *priv)\n\n#define MLX5E_DECLARE_STATS_GRP_OP_UPDATE_STATS(grp) \\\n\tvoid MLX5E_STATS_GRP_OP(grp, update_stats)(struct mlx5e_priv *priv)\n\n#define MLX5E_DECLARE_STATS_GRP_OP_FILL_STRS(grp) \\\n\tint MLX5E_STATS_GRP_OP(grp, fill_strings)(struct mlx5e_priv *priv, u8 *data, int idx)\n\n#define MLX5E_DECLARE_STATS_GRP_OP_FILL_STATS(grp) \\\n\tint MLX5E_STATS_GRP_OP(grp, fill_stats)(struct mlx5e_priv *priv, u64 *data, int idx)\n\n#define MLX5E_STATS_GRP(grp) mlx5e_stats_grp_ ## grp\n\n#define MLX5E_DECLARE_STATS_GRP(grp) \\\n\tconst struct mlx5e_stats_grp MLX5E_STATS_GRP(grp)\n\n#define MLX5E_DEFINE_STATS_GRP(grp, mask) \\\nMLX5E_DECLARE_STATS_GRP(grp) = { \\\n\t.get_num_stats = MLX5E_STATS_GRP_OP(grp, num_stats), \\\n\t.fill_stats    = MLX5E_STATS_GRP_OP(grp, fill_stats), \\\n\t.fill_strings  = MLX5E_STATS_GRP_OP(grp, fill_strings), \\\n\t.update_stats  = MLX5E_STATS_GRP_OP(grp, update_stats), \\\n\t.update_stats_mask = mask, \\\n}\n\nunsigned int mlx5e_stats_total_num(struct mlx5e_priv *priv);\nvoid mlx5e_stats_update(struct mlx5e_priv *priv);\nvoid mlx5e_stats_fill(struct mlx5e_priv *priv, u64 *data, int idx);\nvoid mlx5e_stats_fill_strings(struct mlx5e_priv *priv, u8 *data);\nvoid mlx5e_stats_update_ndo_stats(struct mlx5e_priv *priv);\n\nvoid mlx5e_stats_pause_get(struct mlx5e_priv *priv,\n\t\t\t   struct ethtool_pause_stats *pause_stats);\nvoid mlx5e_stats_fec_get(struct mlx5e_priv *priv,\n\t\t\t struct ethtool_fec_stats *fec_stats);\n\nvoid mlx5e_stats_eth_phy_get(struct mlx5e_priv *priv,\n\t\t\t     struct ethtool_eth_phy_stats *phy_stats);\nvoid mlx5e_stats_eth_mac_get(struct mlx5e_priv *priv,\n\t\t\t     struct ethtool_eth_mac_stats *mac_stats);\nvoid mlx5e_stats_eth_ctrl_get(struct mlx5e_priv *priv,\n\t\t\t      struct ethtool_eth_ctrl_stats *ctrl_stats);\nvoid mlx5e_stats_rmon_get(struct mlx5e_priv *priv,\n\t\t\t  struct ethtool_rmon_stats *rmon,\n\t\t\t  const struct ethtool_rmon_hist_range **ranges);\nvoid mlx5e_get_link_ext_stats(struct net_device *dev,\n\t\t\t      struct ethtool_link_ext_stats *stats);\n\n \n\nstruct mlx5e_sw_stats {\n\tu64 rx_packets;\n\tu64 rx_bytes;\n\tu64 tx_packets;\n\tu64 tx_bytes;\n\tu64 tx_tso_packets;\n\tu64 tx_tso_bytes;\n\tu64 tx_tso_inner_packets;\n\tu64 tx_tso_inner_bytes;\n\tu64 tx_added_vlan_packets;\n\tu64 tx_nop;\n\tu64 tx_mpwqe_blks;\n\tu64 tx_mpwqe_pkts;\n\tu64 rx_lro_packets;\n\tu64 rx_lro_bytes;\n\tu64 rx_gro_packets;\n\tu64 rx_gro_bytes;\n\tu64 rx_gro_skbs;\n\tu64 rx_gro_match_packets;\n\tu64 rx_gro_large_hds;\n\tu64 rx_mcast_packets;\n\tu64 rx_ecn_mark;\n\tu64 rx_removed_vlan_packets;\n\tu64 rx_csum_unnecessary;\n\tu64 rx_csum_none;\n\tu64 rx_csum_complete;\n\tu64 rx_csum_complete_tail;\n\tu64 rx_csum_complete_tail_slow;\n\tu64 rx_csum_unnecessary_inner;\n\tu64 rx_xdp_drop;\n\tu64 rx_xdp_redirect;\n\tu64 rx_xdp_tx_xmit;\n\tu64 rx_xdp_tx_mpwqe;\n\tu64 rx_xdp_tx_inlnw;\n\tu64 rx_xdp_tx_nops;\n\tu64 rx_xdp_tx_full;\n\tu64 rx_xdp_tx_err;\n\tu64 rx_xdp_tx_cqe;\n\tu64 tx_csum_none;\n\tu64 tx_csum_partial;\n\tu64 tx_csum_partial_inner;\n\tu64 tx_queue_stopped;\n\tu64 tx_queue_dropped;\n\tu64 tx_xmit_more;\n\tu64 tx_recover;\n\tu64 tx_cqes;\n\tu64 tx_queue_wake;\n\tu64 tx_cqe_err;\n\tu64 tx_xdp_xmit;\n\tu64 tx_xdp_mpwqe;\n\tu64 tx_xdp_inlnw;\n\tu64 tx_xdp_nops;\n\tu64 tx_xdp_full;\n\tu64 tx_xdp_err;\n\tu64 tx_xdp_cqes;\n\tu64 rx_wqe_err;\n\tu64 rx_mpwqe_filler_cqes;\n\tu64 rx_mpwqe_filler_strides;\n\tu64 rx_oversize_pkts_sw_drop;\n\tu64 rx_buff_alloc_err;\n\tu64 rx_cqe_compress_blks;\n\tu64 rx_cqe_compress_pkts;\n\tu64 rx_congst_umr;\n#ifdef CONFIG_MLX5_EN_ARFS\n\tu64 rx_arfs_add;\n\tu64 rx_arfs_request_in;\n\tu64 rx_arfs_request_out;\n\tu64 rx_arfs_expired;\n\tu64 rx_arfs_err;\n#endif\n\tu64 rx_recover;\n\tu64 ch_events;\n\tu64 ch_poll;\n\tu64 ch_arm;\n\tu64 ch_aff_change;\n\tu64 ch_force_irq;\n\tu64 ch_eq_rearm;\n#ifdef CONFIG_PAGE_POOL_STATS\n\tu64 rx_pp_alloc_fast;\n\tu64 rx_pp_alloc_slow;\n\tu64 rx_pp_alloc_slow_high_order;\n\tu64 rx_pp_alloc_empty;\n\tu64 rx_pp_alloc_refill;\n\tu64 rx_pp_alloc_waive;\n\tu64 rx_pp_recycle_cached;\n\tu64 rx_pp_recycle_cache_full;\n\tu64 rx_pp_recycle_ring;\n\tu64 rx_pp_recycle_ring_full;\n\tu64 rx_pp_recycle_released_ref;\n#endif\n#ifdef CONFIG_MLX5_EN_TLS\n\tu64 tx_tls_encrypted_packets;\n\tu64 tx_tls_encrypted_bytes;\n\tu64 tx_tls_ooo;\n\tu64 tx_tls_dump_packets;\n\tu64 tx_tls_dump_bytes;\n\tu64 tx_tls_resync_bytes;\n\tu64 tx_tls_skip_no_sync_data;\n\tu64 tx_tls_drop_no_sync_data;\n\tu64 tx_tls_drop_bypass_req;\n\n\tu64 rx_tls_decrypted_packets;\n\tu64 rx_tls_decrypted_bytes;\n\tu64 rx_tls_resync_req_pkt;\n\tu64 rx_tls_resync_req_start;\n\tu64 rx_tls_resync_req_end;\n\tu64 rx_tls_resync_req_skip;\n\tu64 rx_tls_resync_res_ok;\n\tu64 rx_tls_resync_res_retry;\n\tu64 rx_tls_resync_res_skip;\n\tu64 rx_tls_err;\n#endif\n\n\tu64 rx_xsk_packets;\n\tu64 rx_xsk_bytes;\n\tu64 rx_xsk_csum_complete;\n\tu64 rx_xsk_csum_unnecessary;\n\tu64 rx_xsk_csum_unnecessary_inner;\n\tu64 rx_xsk_csum_none;\n\tu64 rx_xsk_ecn_mark;\n\tu64 rx_xsk_removed_vlan_packets;\n\tu64 rx_xsk_xdp_drop;\n\tu64 rx_xsk_xdp_redirect;\n\tu64 rx_xsk_wqe_err;\n\tu64 rx_xsk_mpwqe_filler_cqes;\n\tu64 rx_xsk_mpwqe_filler_strides;\n\tu64 rx_xsk_oversize_pkts_sw_drop;\n\tu64 rx_xsk_buff_alloc_err;\n\tu64 rx_xsk_cqe_compress_blks;\n\tu64 rx_xsk_cqe_compress_pkts;\n\tu64 rx_xsk_congst_umr;\n\tu64 tx_xsk_xmit;\n\tu64 tx_xsk_mpwqe;\n\tu64 tx_xsk_inlnw;\n\tu64 tx_xsk_full;\n\tu64 tx_xsk_err;\n\tu64 tx_xsk_cqes;\n};\n\nstruct mlx5e_qcounter_stats {\n\tu32 rx_out_of_buffer;\n\tu32 rx_if_down_packets;\n};\n\n#define VNIC_ENV_GET(vnic_env_stats, c) \\\n\tMLX5_GET(query_vnic_env_out, (vnic_env_stats)->query_vnic_env_out, \\\n\t\t vport_env.c)\n\nstruct mlx5e_vnic_env_stats {\n\t__be64 query_vnic_env_out[MLX5_ST_SZ_QW(query_vnic_env_out)];\n};\n\n#define VPORT_COUNTER_GET(vstats, c) MLX5_GET64(query_vport_counter_out, \\\n\t\t\t\t\t\tvstats->query_vport_out, c)\n\nstruct mlx5e_vport_stats {\n\t__be64 query_vport_out[MLX5_ST_SZ_QW(query_vport_counter_out)];\n};\n\n#define PPORT_802_3_GET(pstats, c) \\\n\tMLX5_GET64(ppcnt_reg, pstats->IEEE_802_3_counters, \\\n\t\t   counter_set.eth_802_3_cntrs_grp_data_layout.c##_high)\n#define PPORT_2863_GET(pstats, c) \\\n\tMLX5_GET64(ppcnt_reg, pstats->RFC_2863_counters, \\\n\t\t   counter_set.eth_2863_cntrs_grp_data_layout.c##_high)\n#define PPORT_2819_GET(pstats, c) \\\n\tMLX5_GET64(ppcnt_reg, pstats->RFC_2819_counters, \\\n\t\t   counter_set.eth_2819_cntrs_grp_data_layout.c##_high)\n#define PPORT_PHY_STATISTICAL_GET(pstats, c) \\\n\tMLX5_GET64(ppcnt_reg, (pstats)->phy_statistical_counters, \\\n\t\t   counter_set.phys_layer_statistical_cntrs.c##_high)\n#define PPORT_PER_PRIO_GET(pstats, prio, c) \\\n\tMLX5_GET64(ppcnt_reg, pstats->per_prio_counters[prio], \\\n\t\t   counter_set.eth_per_prio_grp_data_layout.c##_high)\n#define NUM_PPORT_PRIO\t\t\t\t8\n#define PPORT_ETH_EXT_GET(pstats, c) \\\n\tMLX5_GET64(ppcnt_reg, (pstats)->eth_ext_counters, \\\n\t\t   counter_set.eth_extended_cntrs_grp_data_layout.c##_high)\n\nstruct mlx5e_pport_stats {\n\t__be64 IEEE_802_3_counters[MLX5_ST_SZ_QW(ppcnt_reg)];\n\t__be64 RFC_2863_counters[MLX5_ST_SZ_QW(ppcnt_reg)];\n\t__be64 RFC_2819_counters[MLX5_ST_SZ_QW(ppcnt_reg)];\n\t__be64 per_prio_counters[NUM_PPORT_PRIO][MLX5_ST_SZ_QW(ppcnt_reg)];\n\t__be64 phy_counters[MLX5_ST_SZ_QW(ppcnt_reg)];\n\t__be64 phy_statistical_counters[MLX5_ST_SZ_QW(ppcnt_reg)];\n\t__be64 eth_ext_counters[MLX5_ST_SZ_QW(ppcnt_reg)];\n\t__be64 per_tc_prio_counters[NUM_PPORT_PRIO][MLX5_ST_SZ_QW(ppcnt_reg)];\n\t__be64 per_tc_congest_prio_counters[NUM_PPORT_PRIO][MLX5_ST_SZ_QW(ppcnt_reg)];\n};\n\n#define PCIE_PERF_GET(pcie_stats, c) \\\n\tMLX5_GET(mpcnt_reg, (pcie_stats)->pcie_perf_counters, \\\n\t\t counter_set.pcie_perf_cntrs_grp_data_layout.c)\n\n#define PCIE_PERF_GET64(pcie_stats, c) \\\n\tMLX5_GET64(mpcnt_reg, (pcie_stats)->pcie_perf_counters, \\\n\t\t   counter_set.pcie_perf_cntrs_grp_data_layout.c##_high)\n\nstruct mlx5e_pcie_stats {\n\t__be64 pcie_perf_counters[MLX5_ST_SZ_QW(mpcnt_reg)];\n};\n\nstruct mlx5e_rq_stats {\n\tu64 packets;\n\tu64 bytes;\n\tu64 csum_complete;\n\tu64 csum_complete_tail;\n\tu64 csum_complete_tail_slow;\n\tu64 csum_unnecessary;\n\tu64 csum_unnecessary_inner;\n\tu64 csum_none;\n\tu64 lro_packets;\n\tu64 lro_bytes;\n\tu64 gro_packets;\n\tu64 gro_bytes;\n\tu64 gro_skbs;\n\tu64 gro_match_packets;\n\tu64 gro_large_hds;\n\tu64 mcast_packets;\n\tu64 ecn_mark;\n\tu64 removed_vlan_packets;\n\tu64 xdp_drop;\n\tu64 xdp_redirect;\n\tu64 wqe_err;\n\tu64 mpwqe_filler_cqes;\n\tu64 mpwqe_filler_strides;\n\tu64 oversize_pkts_sw_drop;\n\tu64 buff_alloc_err;\n\tu64 cqe_compress_blks;\n\tu64 cqe_compress_pkts;\n\tu64 congst_umr;\n#ifdef CONFIG_MLX5_EN_ARFS\n\tu64 arfs_add;\n\tu64 arfs_request_in;\n\tu64 arfs_request_out;\n\tu64 arfs_expired;\n\tu64 arfs_err;\n#endif\n\tu64 recover;\n#ifdef CONFIG_PAGE_POOL_STATS\n\tu64 pp_alloc_fast;\n\tu64 pp_alloc_slow;\n\tu64 pp_alloc_slow_high_order;\n\tu64 pp_alloc_empty;\n\tu64 pp_alloc_refill;\n\tu64 pp_alloc_waive;\n\tu64 pp_recycle_cached;\n\tu64 pp_recycle_cache_full;\n\tu64 pp_recycle_ring;\n\tu64 pp_recycle_ring_full;\n\tu64 pp_recycle_released_ref;\n#endif\n#ifdef CONFIG_MLX5_EN_TLS\n\tu64 tls_decrypted_packets;\n\tu64 tls_decrypted_bytes;\n\tu64 tls_resync_req_pkt;\n\tu64 tls_resync_req_start;\n\tu64 tls_resync_req_end;\n\tu64 tls_resync_req_skip;\n\tu64 tls_resync_res_ok;\n\tu64 tls_resync_res_retry;\n\tu64 tls_resync_res_skip;\n\tu64 tls_err;\n#endif\n};\n\nstruct mlx5e_sq_stats {\n\t \n\tu64 packets;\n\tu64 bytes;\n\tu64 xmit_more;\n\tu64 tso_packets;\n\tu64 tso_bytes;\n\tu64 tso_inner_packets;\n\tu64 tso_inner_bytes;\n\tu64 csum_partial;\n\tu64 csum_partial_inner;\n\tu64 added_vlan_packets;\n\tu64 nop;\n\tu64 mpwqe_blks;\n\tu64 mpwqe_pkts;\n#ifdef CONFIG_MLX5_EN_TLS\n\tu64 tls_encrypted_packets;\n\tu64 tls_encrypted_bytes;\n\tu64 tls_ooo;\n\tu64 tls_dump_packets;\n\tu64 tls_dump_bytes;\n\tu64 tls_resync_bytes;\n\tu64 tls_skip_no_sync_data;\n\tu64 tls_drop_no_sync_data;\n\tu64 tls_drop_bypass_req;\n#endif\n\t \n\tu64 csum_none;\n\tu64 stopped;\n\tu64 dropped;\n\tu64 recover;\n\t \n\tu64 cqes ____cacheline_aligned_in_smp;\n\tu64 wake;\n\tu64 cqe_err;\n};\n\nstruct mlx5e_xdpsq_stats {\n\tu64 xmit;\n\tu64 mpwqe;\n\tu64 inlnw;\n\tu64 nops;\n\tu64 full;\n\tu64 err;\n\t \n\tu64 cqes ____cacheline_aligned_in_smp;\n};\n\nstruct mlx5e_ch_stats {\n\tu64 events;\n\tu64 poll;\n\tu64 arm;\n\tu64 aff_change;\n\tu64 force_irq;\n\tu64 eq_rearm;\n};\n\nstruct mlx5e_ptp_cq_stats {\n\tu64 cqe;\n\tu64 err_cqe;\n\tu64 abort;\n\tu64 abort_abs_diff_ns;\n\tu64 late_cqe;\n};\n\nstruct mlx5e_rep_stats {\n\tu64 vport_rx_packets;\n\tu64 vport_tx_packets;\n\tu64 vport_rx_bytes;\n\tu64 vport_tx_bytes;\n\tu64 rx_vport_rdma_unicast_packets;\n\tu64 tx_vport_rdma_unicast_packets;\n\tu64 rx_vport_rdma_unicast_bytes;\n\tu64 tx_vport_rdma_unicast_bytes;\n\tu64 rx_vport_rdma_multicast_packets;\n\tu64 tx_vport_rdma_multicast_packets;\n\tu64 rx_vport_rdma_multicast_bytes;\n\tu64 tx_vport_rdma_multicast_bytes;\n};\n\nstruct mlx5e_stats {\n\tstruct mlx5e_sw_stats sw;\n\tstruct mlx5e_qcounter_stats qcnt;\n\tstruct mlx5e_vnic_env_stats vnic;\n\tstruct mlx5e_vport_stats vport;\n\tstruct mlx5e_pport_stats pport;\n\tstruct mlx5e_pcie_stats pcie;\n\tstruct mlx5e_rep_stats rep_stats;\n};\n\nstatic inline void mlx5e_stats_copy_rep_stats(struct rtnl_link_stats64 *vf_vport,\n\t\t\t\t\t      struct mlx5e_rep_stats *rep_stats)\n{\n\tmemset(vf_vport, 0, sizeof(*vf_vport));\n\tvf_vport->rx_packets = rep_stats->vport_rx_packets;\n\tvf_vport->tx_packets = rep_stats->vport_tx_packets;\n\tvf_vport->rx_bytes = rep_stats->vport_rx_bytes;\n\tvf_vport->tx_bytes = rep_stats->vport_tx_bytes;\n}\n\nextern mlx5e_stats_grp_t mlx5e_nic_stats_grps[];\nunsigned int mlx5e_nic_stats_grps_num(struct mlx5e_priv *priv);\n\nextern MLX5E_DECLARE_STATS_GRP(sw);\nextern MLX5E_DECLARE_STATS_GRP(qcnt);\nextern MLX5E_DECLARE_STATS_GRP(vnic_env);\nextern MLX5E_DECLARE_STATS_GRP(vport);\nextern MLX5E_DECLARE_STATS_GRP(802_3);\nextern MLX5E_DECLARE_STATS_GRP(2863);\nextern MLX5E_DECLARE_STATS_GRP(2819);\nextern MLX5E_DECLARE_STATS_GRP(phy);\nextern MLX5E_DECLARE_STATS_GRP(eth_ext);\nextern MLX5E_DECLARE_STATS_GRP(pcie);\nextern MLX5E_DECLARE_STATS_GRP(per_prio);\nextern MLX5E_DECLARE_STATS_GRP(pme);\nextern MLX5E_DECLARE_STATS_GRP(channels);\nextern MLX5E_DECLARE_STATS_GRP(per_port_buff_congest);\nextern MLX5E_DECLARE_STATS_GRP(ipsec_hw);\nextern MLX5E_DECLARE_STATS_GRP(ipsec_sw);\nextern MLX5E_DECLARE_STATS_GRP(ptp);\nextern MLX5E_DECLARE_STATS_GRP(macsec_hw);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}