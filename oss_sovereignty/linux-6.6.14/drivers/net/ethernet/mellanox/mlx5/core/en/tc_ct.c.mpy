{
  "module_name": "tc_ct.c",
  "hash_id": "42952e093951eff65d0cb7837d1061255c8c55aa021b97fb58fb817d62149a82",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en/tc_ct.c",
  "human_readable_source": "\n \n\n#include <net/netfilter/nf_conntrack.h>\n#include <net/netfilter/nf_conntrack_core.h>\n#include <net/netfilter/nf_conntrack_zones.h>\n#include <net/netfilter/nf_conntrack_labels.h>\n#include <net/netfilter/nf_conntrack_helper.h>\n#include <net/netfilter/nf_conntrack_acct.h>\n#include <uapi/linux/tc_act/tc_pedit.h>\n#include <net/tc_act/tc_ct.h>\n#include <net/flow_offload.h>\n#include <net/netfilter/nf_flow_table.h>\n#include <linux/workqueue.h>\n#include <linux/refcount.h>\n#include <linux/xarray.h>\n#include <linux/if_macvlan.h>\n#include <linux/debugfs.h>\n\n#include \"lib/fs_chains.h\"\n#include \"en/tc_ct.h\"\n#include \"en/tc/ct_fs.h\"\n#include \"en/tc_priv.h\"\n#include \"en/mod_hdr.h\"\n#include \"en/mapping.h\"\n#include \"en/tc/post_act.h\"\n#include \"en.h\"\n#include \"en_tc.h\"\n#include \"en_rep.h\"\n#include \"fs_core.h\"\n\n#define MLX5_CT_STATE_ESTABLISHED_BIT BIT(1)\n#define MLX5_CT_STATE_TRK_BIT BIT(2)\n#define MLX5_CT_STATE_NAT_BIT BIT(3)\n#define MLX5_CT_STATE_REPLY_BIT BIT(4)\n#define MLX5_CT_STATE_RELATED_BIT BIT(5)\n#define MLX5_CT_STATE_INVALID_BIT BIT(6)\n#define MLX5_CT_STATE_NEW_BIT BIT(7)\n\n#define MLX5_CT_LABELS_BITS MLX5_REG_MAPPING_MBITS(LABELS_TO_REG)\n#define MLX5_CT_LABELS_MASK MLX5_REG_MAPPING_MASK(LABELS_TO_REG)\n\n \n#define MLX5_CT_MIN_MOD_ACTS 10\n\n#define ct_dbg(fmt, args...)\\\n\tnetdev_dbg(ct_priv->netdev, \"ct_debug: \" fmt \"\\n\", ##args)\n\nstruct mlx5_tc_ct_debugfs {\n\tstruct {\n\t\tatomic_t offloaded;\n\t\tatomic_t rx_dropped;\n\t} stats;\n\n\tstruct dentry *root;\n};\n\nstruct mlx5_tc_ct_priv {\n\tstruct mlx5_core_dev *dev;\n\tstruct mlx5e_priv *priv;\n\tconst struct net_device *netdev;\n\tstruct mod_hdr_tbl *mod_hdr_tbl;\n\tstruct xarray tuple_ids;\n\tstruct rhashtable zone_ht;\n\tstruct rhashtable ct_tuples_ht;\n\tstruct rhashtable ct_tuples_nat_ht;\n\tstruct mlx5_flow_table *ct;\n\tstruct mlx5_flow_table *ct_nat;\n\tstruct mlx5e_post_act *post_act;\n\tstruct mutex control_lock;  \n\tstruct mapping_ctx *zone_mapping;\n\tstruct mapping_ctx *labels_mapping;\n\tenum mlx5_flow_namespace_type ns_type;\n\tstruct mlx5_fs_chains *chains;\n\tstruct mlx5_ct_fs *fs;\n\tstruct mlx5_ct_fs_ops *fs_ops;\n\tspinlock_t ht_lock;  \n\tstruct workqueue_struct *wq;\n\n\tstruct mlx5_tc_ct_debugfs debugfs;\n};\n\nstruct mlx5_ct_zone_rule {\n\tstruct mlx5_ct_fs_rule *rule;\n\tstruct mlx5e_mod_hdr_handle *mh;\n\tstruct mlx5_flow_attr *attr;\n\tbool nat;\n};\n\nstruct mlx5_tc_ct_pre {\n\tstruct mlx5_flow_table *ft;\n\tstruct mlx5_flow_group *flow_grp;\n\tstruct mlx5_flow_group *miss_grp;\n\tstruct mlx5_flow_handle *flow_rule;\n\tstruct mlx5_flow_handle *miss_rule;\n\tstruct mlx5_modify_hdr *modify_hdr;\n};\n\nstruct mlx5_ct_ft {\n\tstruct rhash_head node;\n\tu16 zone;\n\tu32 zone_restore_id;\n\trefcount_t refcount;\n\tstruct nf_flowtable *nf_ft;\n\tstruct mlx5_tc_ct_priv *ct_priv;\n\tstruct rhashtable ct_entries_ht;\n\tstruct mlx5_tc_ct_pre pre_ct;\n\tstruct mlx5_tc_ct_pre pre_ct_nat;\n};\n\nstruct mlx5_ct_tuple {\n\tu16 addr_type;\n\t__be16 n_proto;\n\tu8 ip_proto;\n\tstruct {\n\t\tunion {\n\t\t\t__be32 src_v4;\n\t\t\tstruct in6_addr src_v6;\n\t\t};\n\t\tunion {\n\t\t\t__be32 dst_v4;\n\t\t\tstruct in6_addr dst_v6;\n\t\t};\n\t} ip;\n\tstruct {\n\t\t__be16 src;\n\t\t__be16 dst;\n\t} port;\n\n\tu16 zone;\n};\n\nstruct mlx5_ct_counter {\n\tstruct mlx5_fc *counter;\n\trefcount_t refcount;\n\tbool is_shared;\n};\n\nenum {\n\tMLX5_CT_ENTRY_FLAG_VALID,\n};\n\nstruct mlx5_ct_entry {\n\tstruct rhash_head node;\n\tstruct rhash_head tuple_node;\n\tstruct rhash_head tuple_nat_node;\n\tstruct mlx5_ct_counter *counter;\n\tunsigned long cookie;\n\tunsigned long restore_cookie;\n\tstruct mlx5_ct_tuple tuple;\n\tstruct mlx5_ct_tuple tuple_nat;\n\tstruct mlx5_ct_zone_rule zone_rules[2];\n\n\tstruct mlx5_tc_ct_priv *ct_priv;\n\tstruct work_struct work;\n\n\trefcount_t refcnt;\n\tunsigned long flags;\n};\n\nstatic void\nmlx5_tc_ct_entry_destroy_mod_hdr(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t\t struct mlx5_flow_attr *attr,\n\t\t\t\t struct mlx5e_mod_hdr_handle *mh);\n\nstatic const struct rhashtable_params cts_ht_params = {\n\t.head_offset = offsetof(struct mlx5_ct_entry, node),\n\t.key_offset = offsetof(struct mlx5_ct_entry, cookie),\n\t.key_len = sizeof(((struct mlx5_ct_entry *)0)->cookie),\n\t.automatic_shrinking = true,\n\t.min_size = 16 * 1024,\n};\n\nstatic const struct rhashtable_params zone_params = {\n\t.head_offset = offsetof(struct mlx5_ct_ft, node),\n\t.key_offset = offsetof(struct mlx5_ct_ft, zone),\n\t.key_len = sizeof(((struct mlx5_ct_ft *)0)->zone),\n\t.automatic_shrinking = true,\n};\n\nstatic const struct rhashtable_params tuples_ht_params = {\n\t.head_offset = offsetof(struct mlx5_ct_entry, tuple_node),\n\t.key_offset = offsetof(struct mlx5_ct_entry, tuple),\n\t.key_len = sizeof(((struct mlx5_ct_entry *)0)->tuple),\n\t.automatic_shrinking = true,\n\t.min_size = 16 * 1024,\n};\n\nstatic const struct rhashtable_params tuples_nat_ht_params = {\n\t.head_offset = offsetof(struct mlx5_ct_entry, tuple_nat_node),\n\t.key_offset = offsetof(struct mlx5_ct_entry, tuple_nat),\n\t.key_len = sizeof(((struct mlx5_ct_entry *)0)->tuple_nat),\n\t.automatic_shrinking = true,\n\t.min_size = 16 * 1024,\n};\n\nstatic bool\nmlx5_tc_ct_entry_has_nat(struct mlx5_ct_entry *entry)\n{\n\treturn !!(entry->tuple_nat_node.next);\n}\n\nstatic int\nmlx5_get_label_mapping(struct mlx5_tc_ct_priv *ct_priv,\n\t\t       u32 *labels, u32 *id)\n{\n\tif (!memchr_inv(labels, 0, sizeof(u32) * 4)) {\n\t\t*id = 0;\n\t\treturn 0;\n\t}\n\n\tif (mapping_add(ct_priv->labels_mapping, labels, id))\n\t\treturn -EOPNOTSUPP;\n\n\treturn 0;\n}\n\nstatic void\nmlx5_put_label_mapping(struct mlx5_tc_ct_priv *ct_priv, u32 id)\n{\n\tif (id)\n\t\tmapping_remove(ct_priv->labels_mapping, id);\n}\n\nstatic int\nmlx5_tc_ct_rule_to_tuple(struct mlx5_ct_tuple *tuple, struct flow_rule *rule)\n{\n\tstruct flow_match_control control;\n\tstruct flow_match_basic basic;\n\n\tflow_rule_match_basic(rule, &basic);\n\tflow_rule_match_control(rule, &control);\n\n\ttuple->n_proto = basic.key->n_proto;\n\ttuple->ip_proto = basic.key->ip_proto;\n\ttuple->addr_type = control.key->addr_type;\n\n\tif (tuple->addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS) {\n\t\tstruct flow_match_ipv4_addrs match;\n\n\t\tflow_rule_match_ipv4_addrs(rule, &match);\n\t\ttuple->ip.src_v4 = match.key->src;\n\t\ttuple->ip.dst_v4 = match.key->dst;\n\t} else if (tuple->addr_type == FLOW_DISSECTOR_KEY_IPV6_ADDRS) {\n\t\tstruct flow_match_ipv6_addrs match;\n\n\t\tflow_rule_match_ipv6_addrs(rule, &match);\n\t\ttuple->ip.src_v6 = match.key->src;\n\t\ttuple->ip.dst_v6 = match.key->dst;\n\t} else {\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_PORTS)) {\n\t\tstruct flow_match_ports match;\n\n\t\tflow_rule_match_ports(rule, &match);\n\t\tswitch (tuple->ip_proto) {\n\t\tcase IPPROTO_TCP:\n\t\tcase IPPROTO_UDP:\n\t\t\ttuple->port.src = match.key->src;\n\t\t\ttuple->port.dst = match.key->dst;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t} else {\n\t\tif (tuple->ip_proto != IPPROTO_GRE)\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nmlx5_tc_ct_rule_to_tuple_nat(struct mlx5_ct_tuple *tuple,\n\t\t\t     struct flow_rule *rule)\n{\n\tstruct flow_action *flow_action = &rule->action;\n\tstruct flow_action_entry *act;\n\tu32 offset, val, ip6_offset;\n\tint i;\n\n\tflow_action_for_each(i, act, flow_action) {\n\t\tif (act->id != FLOW_ACTION_MANGLE)\n\t\t\tcontinue;\n\n\t\toffset = act->mangle.offset;\n\t\tval = act->mangle.val;\n\t\tswitch (act->mangle.htype) {\n\t\tcase FLOW_ACT_MANGLE_HDR_TYPE_IP4:\n\t\t\tif (offset == offsetof(struct iphdr, saddr))\n\t\t\t\ttuple->ip.src_v4 = cpu_to_be32(val);\n\t\t\telse if (offset == offsetof(struct iphdr, daddr))\n\t\t\t\ttuple->ip.dst_v4 = cpu_to_be32(val);\n\t\t\telse\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tbreak;\n\n\t\tcase FLOW_ACT_MANGLE_HDR_TYPE_IP6:\n\t\t\tip6_offset = (offset - offsetof(struct ipv6hdr, saddr));\n\t\t\tip6_offset /= 4;\n\t\t\tif (ip6_offset < 4)\n\t\t\t\ttuple->ip.src_v6.s6_addr32[ip6_offset] = cpu_to_be32(val);\n\t\t\telse if (ip6_offset < 8)\n\t\t\t\ttuple->ip.dst_v6.s6_addr32[ip6_offset - 4] = cpu_to_be32(val);\n\t\t\telse\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tbreak;\n\n\t\tcase FLOW_ACT_MANGLE_HDR_TYPE_TCP:\n\t\t\tif (offset == offsetof(struct tcphdr, source))\n\t\t\t\ttuple->port.src = cpu_to_be16(val);\n\t\t\telse if (offset == offsetof(struct tcphdr, dest))\n\t\t\t\ttuple->port.dst = cpu_to_be16(val);\n\t\t\telse\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tbreak;\n\n\t\tcase FLOW_ACT_MANGLE_HDR_TYPE_UDP:\n\t\t\tif (offset == offsetof(struct udphdr, source))\n\t\t\t\ttuple->port.src = cpu_to_be16(val);\n\t\t\telse if (offset == offsetof(struct udphdr, dest))\n\t\t\t\ttuple->port.dst = cpu_to_be16(val);\n\t\t\telse\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int\nmlx5_tc_ct_get_flow_source_match(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t\t struct net_device *ndev)\n{\n\tstruct mlx5e_priv *other_priv = netdev_priv(ndev);\n\tstruct mlx5_core_dev *mdev = ct_priv->dev;\n\tbool vf_rep, uplink_rep;\n\n\tvf_rep = mlx5e_eswitch_vf_rep(ndev) && mlx5_same_hw_devs(mdev, other_priv->mdev);\n\tuplink_rep = mlx5e_eswitch_uplink_rep(ndev) && mlx5_same_hw_devs(mdev, other_priv->mdev);\n\n\tif (vf_rep)\n\t\treturn MLX5_FLOW_CONTEXT_FLOW_SOURCE_LOCAL_VPORT;\n\tif (uplink_rep)\n\t\treturn MLX5_FLOW_CONTEXT_FLOW_SOURCE_UPLINK;\n\tif (is_vlan_dev(ndev))\n\t\treturn mlx5_tc_ct_get_flow_source_match(ct_priv, vlan_dev_real_dev(ndev));\n\tif (netif_is_macvlan(ndev))\n\t\treturn mlx5_tc_ct_get_flow_source_match(ct_priv, macvlan_dev_real_dev(ndev));\n\tif (mlx5e_get_tc_tun(ndev) || netif_is_lag_master(ndev))\n\t\treturn MLX5_FLOW_CONTEXT_FLOW_SOURCE_UPLINK;\n\n\treturn MLX5_FLOW_CONTEXT_FLOW_SOURCE_ANY_VPORT;\n}\n\nstatic int\nmlx5_tc_ct_set_tuple_match(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t   struct mlx5_flow_spec *spec,\n\t\t\t   struct flow_rule *rule)\n{\n\tvoid *headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,\n\t\t\t\t       outer_headers);\n\tvoid *headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,\n\t\t\t\t       outer_headers);\n\tu16 addr_type = 0;\n\tu8 ip_proto = 0;\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_BASIC)) {\n\t\tstruct flow_match_basic match;\n\n\t\tflow_rule_match_basic(rule, &match);\n\n\t\tmlx5e_tc_set_ethertype(ct_priv->dev, &match, true, headers_c, headers_v);\n\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_c, ip_protocol,\n\t\t\t match.mask->ip_proto);\n\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_v, ip_protocol,\n\t\t\t match.key->ip_proto);\n\n\t\tip_proto = match.key->ip_proto;\n\t}\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_CONTROL)) {\n\t\tstruct flow_match_control match;\n\n\t\tflow_rule_match_control(rule, &match);\n\t\taddr_type = match.key->addr_type;\n\t}\n\n\tif (addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS) {\n\t\tstruct flow_match_ipv4_addrs match;\n\n\t\tflow_rule_match_ipv4_addrs(rule, &match);\n\t\tmemcpy(MLX5_ADDR_OF(fte_match_set_lyr_2_4, headers_c,\n\t\t\t\t    src_ipv4_src_ipv6.ipv4_layout.ipv4),\n\t\t       &match.mask->src, sizeof(match.mask->src));\n\t\tmemcpy(MLX5_ADDR_OF(fte_match_set_lyr_2_4, headers_v,\n\t\t\t\t    src_ipv4_src_ipv6.ipv4_layout.ipv4),\n\t\t       &match.key->src, sizeof(match.key->src));\n\t\tmemcpy(MLX5_ADDR_OF(fte_match_set_lyr_2_4, headers_c,\n\t\t\t\t    dst_ipv4_dst_ipv6.ipv4_layout.ipv4),\n\t\t       &match.mask->dst, sizeof(match.mask->dst));\n\t\tmemcpy(MLX5_ADDR_OF(fte_match_set_lyr_2_4, headers_v,\n\t\t\t\t    dst_ipv4_dst_ipv6.ipv4_layout.ipv4),\n\t\t       &match.key->dst, sizeof(match.key->dst));\n\t}\n\n\tif (addr_type == FLOW_DISSECTOR_KEY_IPV6_ADDRS) {\n\t\tstruct flow_match_ipv6_addrs match;\n\n\t\tflow_rule_match_ipv6_addrs(rule, &match);\n\t\tmemcpy(MLX5_ADDR_OF(fte_match_set_lyr_2_4, headers_c,\n\t\t\t\t    src_ipv4_src_ipv6.ipv6_layout.ipv6),\n\t\t       &match.mask->src, sizeof(match.mask->src));\n\t\tmemcpy(MLX5_ADDR_OF(fte_match_set_lyr_2_4, headers_v,\n\t\t\t\t    src_ipv4_src_ipv6.ipv6_layout.ipv6),\n\t\t       &match.key->src, sizeof(match.key->src));\n\n\t\tmemcpy(MLX5_ADDR_OF(fte_match_set_lyr_2_4, headers_c,\n\t\t\t\t    dst_ipv4_dst_ipv6.ipv6_layout.ipv6),\n\t\t       &match.mask->dst, sizeof(match.mask->dst));\n\t\tmemcpy(MLX5_ADDR_OF(fte_match_set_lyr_2_4, headers_v,\n\t\t\t\t    dst_ipv4_dst_ipv6.ipv6_layout.ipv6),\n\t\t       &match.key->dst, sizeof(match.key->dst));\n\t}\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_PORTS)) {\n\t\tstruct flow_match_ports match;\n\n\t\tflow_rule_match_ports(rule, &match);\n\t\tswitch (ip_proto) {\n\t\tcase IPPROTO_TCP:\n\t\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_c,\n\t\t\t\t tcp_sport, ntohs(match.mask->src));\n\t\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_v,\n\t\t\t\t tcp_sport, ntohs(match.key->src));\n\n\t\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_c,\n\t\t\t\t tcp_dport, ntohs(match.mask->dst));\n\t\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_v,\n\t\t\t\t tcp_dport, ntohs(match.key->dst));\n\t\t\tbreak;\n\n\t\tcase IPPROTO_UDP:\n\t\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_c,\n\t\t\t\t udp_sport, ntohs(match.mask->src));\n\t\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_v,\n\t\t\t\t udp_sport, ntohs(match.key->src));\n\n\t\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_c,\n\t\t\t\t udp_dport, ntohs(match.mask->dst));\n\t\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_v,\n\t\t\t\t udp_dport, ntohs(match.key->dst));\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_TCP)) {\n\t\tstruct flow_match_tcp match;\n\n\t\tflow_rule_match_tcp(rule, &match);\n\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_c, tcp_flags,\n\t\t\t ntohs(match.mask->flags));\n\t\tMLX5_SET(fte_match_set_lyr_2_4, headers_v, tcp_flags,\n\t\t\t ntohs(match.key->flags));\n\t}\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_META)) {\n\t\tstruct flow_match_meta match;\n\n\t\tflow_rule_match_meta(rule, &match);\n\n\t\tif (match.key->ingress_ifindex & match.mask->ingress_ifindex) {\n\t\t\tstruct net_device *dev;\n\n\t\t\tdev = dev_get_by_index(&init_net, match.key->ingress_ifindex);\n\t\t\tif (dev && MLX5_CAP_ESW_FLOWTABLE(ct_priv->dev, flow_source))\n\t\t\t\tspec->flow_context.flow_source =\n\t\t\t\t\tmlx5_tc_ct_get_flow_source_match(ct_priv, dev);\n\n\t\t\tdev_put(dev);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void\nmlx5_tc_ct_counter_put(struct mlx5_tc_ct_priv *ct_priv, struct mlx5_ct_entry *entry)\n{\n\tif (entry->counter->is_shared &&\n\t    !refcount_dec_and_test(&entry->counter->refcount))\n\t\treturn;\n\n\tmlx5_fc_destroy(ct_priv->dev, entry->counter->counter);\n\tkfree(entry->counter);\n}\n\nstatic void\nmlx5_tc_ct_entry_del_rule(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t  struct mlx5_ct_entry *entry,\n\t\t\t  bool nat)\n{\n\tstruct mlx5_ct_zone_rule *zone_rule = &entry->zone_rules[nat];\n\tstruct mlx5_flow_attr *attr = zone_rule->attr;\n\n\tct_dbg(\"Deleting ct entry rule in zone %d\", entry->tuple.zone);\n\n\tct_priv->fs_ops->ct_rule_del(ct_priv->fs, zone_rule->rule);\n\tmlx5_tc_ct_entry_destroy_mod_hdr(ct_priv, zone_rule->attr, zone_rule->mh);\n\tmlx5_put_label_mapping(ct_priv, attr->ct_attr.ct_labels_id);\n\tkfree(attr);\n}\n\nstatic void\nmlx5_tc_ct_entry_del_rules(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t   struct mlx5_ct_entry *entry)\n{\n\tmlx5_tc_ct_entry_del_rule(ct_priv, entry, true);\n\tmlx5_tc_ct_entry_del_rule(ct_priv, entry, false);\n\n\tatomic_dec(&ct_priv->debugfs.stats.offloaded);\n}\n\nstatic struct flow_action_entry *\nmlx5_tc_ct_get_ct_metadata_action(struct flow_rule *flow_rule)\n{\n\tstruct flow_action *flow_action = &flow_rule->action;\n\tstruct flow_action_entry *act;\n\tint i;\n\n\tflow_action_for_each(i, act, flow_action) {\n\t\tif (act->id == FLOW_ACTION_CT_METADATA)\n\t\t\treturn act;\n\t}\n\n\treturn NULL;\n}\n\nstatic int\nmlx5_tc_ct_entry_set_registers(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t       struct mlx5e_tc_mod_hdr_acts *mod_acts,\n\t\t\t       u8 ct_state,\n\t\t\t       u32 mark,\n\t\t\t       u32 labels_id,\n\t\t\t       u8 zone_restore_id)\n{\n\tenum mlx5_flow_namespace_type ns = ct_priv->ns_type;\n\tstruct mlx5_core_dev *dev = ct_priv->dev;\n\tint err;\n\n\terr = mlx5e_tc_match_to_reg_set(dev, mod_acts, ns,\n\t\t\t\t\tCTSTATE_TO_REG, ct_state);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_tc_match_to_reg_set(dev, mod_acts, ns,\n\t\t\t\t\tMARK_TO_REG, mark);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_tc_match_to_reg_set(dev, mod_acts, ns,\n\t\t\t\t\tLABELS_TO_REG, labels_id);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5e_tc_match_to_reg_set(dev, mod_acts, ns,\n\t\t\t\t\tZONE_RESTORE_TO_REG, zone_restore_id);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (ns != MLX5_FLOW_NAMESPACE_FDB) {\n\t\terr = mlx5e_tc_match_to_reg_set(dev, mod_acts, ns,\n\t\t\t\t\t\tNIC_ZONE_RESTORE_TO_REG, zone_restore_id);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic int\nmlx5_tc_ct_parse_mangle_to_mod_act(struct flow_action_entry *act,\n\t\t\t\t   char *modact)\n{\n\tu32 offset = act->mangle.offset, field;\n\n\tswitch (act->mangle.htype) {\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_IP4:\n\t\tMLX5_SET(set_action_in, modact, length, 0);\n\t\tif (offset == offsetof(struct iphdr, saddr))\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_SIPV4;\n\t\telse if (offset == offsetof(struct iphdr, daddr))\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_DIPV4;\n\t\telse\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_IP6:\n\t\tMLX5_SET(set_action_in, modact, length, 0);\n\t\tif (offset == offsetof(struct ipv6hdr, saddr) + 12)\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_SIPV6_31_0;\n\t\telse if (offset == offsetof(struct ipv6hdr, saddr) + 8)\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_SIPV6_63_32;\n\t\telse if (offset == offsetof(struct ipv6hdr, saddr) + 4)\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_SIPV6_95_64;\n\t\telse if (offset == offsetof(struct ipv6hdr, saddr))\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_SIPV6_127_96;\n\t\telse if (offset == offsetof(struct ipv6hdr, daddr) + 12)\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_DIPV6_31_0;\n\t\telse if (offset == offsetof(struct ipv6hdr, daddr) + 8)\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_DIPV6_63_32;\n\t\telse if (offset == offsetof(struct ipv6hdr, daddr) + 4)\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_DIPV6_95_64;\n\t\telse if (offset == offsetof(struct ipv6hdr, daddr))\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_DIPV6_127_96;\n\t\telse\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_TCP:\n\t\tMLX5_SET(set_action_in, modact, length, 16);\n\t\tif (offset == offsetof(struct tcphdr, source))\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_TCP_SPORT;\n\t\telse if (offset == offsetof(struct tcphdr, dest))\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_TCP_DPORT;\n\t\telse\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_UDP:\n\t\tMLX5_SET(set_action_in, modact, length, 16);\n\t\tif (offset == offsetof(struct udphdr, source))\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_UDP_SPORT;\n\t\telse if (offset == offsetof(struct udphdr, dest))\n\t\t\tfield = MLX5_ACTION_IN_FIELD_OUT_UDP_DPORT;\n\t\telse\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tMLX5_SET(set_action_in, modact, action_type, MLX5_ACTION_TYPE_SET);\n\tMLX5_SET(set_action_in, modact, offset, 0);\n\tMLX5_SET(set_action_in, modact, field, field);\n\tMLX5_SET(set_action_in, modact, data, act->mangle.val);\n\n\treturn 0;\n}\n\nstatic int\nmlx5_tc_ct_entry_create_nat(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t    struct flow_rule *flow_rule,\n\t\t\t    struct mlx5e_tc_mod_hdr_acts *mod_acts)\n{\n\tstruct flow_action *flow_action = &flow_rule->action;\n\tstruct mlx5_core_dev *mdev = ct_priv->dev;\n\tstruct flow_action_entry *act;\n\tchar *modact;\n\tint err, i;\n\n\tflow_action_for_each(i, act, flow_action) {\n\t\tswitch (act->id) {\n\t\tcase FLOW_ACTION_MANGLE: {\n\t\t\tmodact = mlx5e_mod_hdr_alloc(mdev, ct_priv->ns_type, mod_acts);\n\t\t\tif (IS_ERR(modact))\n\t\t\t\treturn PTR_ERR(modact);\n\n\t\t\terr = mlx5_tc_ct_parse_mangle_to_mod_act(act, modact);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tmod_acts->num_actions++;\n\t\t}\n\t\tbreak;\n\n\t\tcase FLOW_ACTION_CT_METADATA:\n\t\t\t \n\t\t\tcontinue;\n\t\tdefault:\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int\nmlx5_tc_ct_entry_create_mod_hdr(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t\tstruct mlx5_flow_attr *attr,\n\t\t\t\tstruct flow_rule *flow_rule,\n\t\t\t\tstruct mlx5e_mod_hdr_handle **mh,\n\t\t\t\tu8 zone_restore_id, bool nat_table, bool has_nat)\n{\n\tDECLARE_MOD_HDR_ACTS_ACTIONS(actions_arr, MLX5_CT_MIN_MOD_ACTS);\n\tDECLARE_MOD_HDR_ACTS(mod_acts, actions_arr);\n\tstruct flow_action_entry *meta;\n\tenum ip_conntrack_info ctinfo;\n\tu16 ct_state = 0;\n\tint err;\n\n\tmeta = mlx5_tc_ct_get_ct_metadata_action(flow_rule);\n\tif (!meta)\n\t\treturn -EOPNOTSUPP;\n\tctinfo = meta->ct_metadata.cookie & NFCT_INFOMASK;\n\n\terr = mlx5_get_label_mapping(ct_priv, meta->ct_metadata.labels,\n\t\t\t\t     &attr->ct_attr.ct_labels_id);\n\tif (err)\n\t\treturn -EOPNOTSUPP;\n\tif (nat_table) {\n\t\tif (has_nat) {\n\t\t\terr = mlx5_tc_ct_entry_create_nat(ct_priv, flow_rule, &mod_acts);\n\t\t\tif (err)\n\t\t\t\tgoto err_mapping;\n\t\t}\n\n\t\tct_state |= MLX5_CT_STATE_NAT_BIT;\n\t}\n\n\tct_state |= MLX5_CT_STATE_TRK_BIT;\n\tct_state |= ctinfo == IP_CT_NEW ? MLX5_CT_STATE_NEW_BIT : MLX5_CT_STATE_ESTABLISHED_BIT;\n\tct_state |= meta->ct_metadata.orig_dir ? 0 : MLX5_CT_STATE_REPLY_BIT;\n\terr = mlx5_tc_ct_entry_set_registers(ct_priv, &mod_acts,\n\t\t\t\t\t     ct_state,\n\t\t\t\t\t     meta->ct_metadata.mark,\n\t\t\t\t\t     attr->ct_attr.ct_labels_id,\n\t\t\t\t\t     zone_restore_id);\n\tif (err)\n\t\tgoto err_mapping;\n\n\tif (nat_table && has_nat) {\n\t\tattr->modify_hdr = mlx5_modify_header_alloc(ct_priv->dev, ct_priv->ns_type,\n\t\t\t\t\t\t\t    mod_acts.num_actions,\n\t\t\t\t\t\t\t    mod_acts.actions);\n\t\tif (IS_ERR(attr->modify_hdr)) {\n\t\t\terr = PTR_ERR(attr->modify_hdr);\n\t\t\tgoto err_mapping;\n\t\t}\n\n\t\t*mh = NULL;\n\t} else {\n\t\t*mh = mlx5e_mod_hdr_attach(ct_priv->dev,\n\t\t\t\t\t   ct_priv->mod_hdr_tbl,\n\t\t\t\t\t   ct_priv->ns_type,\n\t\t\t\t\t   &mod_acts);\n\t\tif (IS_ERR(*mh)) {\n\t\t\terr = PTR_ERR(*mh);\n\t\t\tgoto err_mapping;\n\t\t}\n\t\tattr->modify_hdr = mlx5e_mod_hdr_get(*mh);\n\t}\n\n\tmlx5e_mod_hdr_dealloc(&mod_acts);\n\treturn 0;\n\nerr_mapping:\n\tmlx5e_mod_hdr_dealloc(&mod_acts);\n\tmlx5_put_label_mapping(ct_priv, attr->ct_attr.ct_labels_id);\n\treturn err;\n}\n\nstatic void\nmlx5_tc_ct_entry_destroy_mod_hdr(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t\t struct mlx5_flow_attr *attr,\n\t\t\t\t struct mlx5e_mod_hdr_handle *mh)\n{\n\tif (mh)\n\t\tmlx5e_mod_hdr_detach(ct_priv->dev, ct_priv->mod_hdr_tbl, mh);\n\telse\n\t\tmlx5_modify_header_dealloc(ct_priv->dev, attr->modify_hdr);\n}\n\nstatic int\nmlx5_tc_ct_entry_add_rule(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t  struct flow_rule *flow_rule,\n\t\t\t  struct mlx5_ct_entry *entry,\n\t\t\t  bool nat, u8 zone_restore_id)\n{\n\tstruct mlx5_ct_zone_rule *zone_rule = &entry->zone_rules[nat];\n\tstruct mlx5e_priv *priv = netdev_priv(ct_priv->netdev);\n\tstruct mlx5_flow_spec *spec = NULL;\n\tstruct mlx5_flow_attr *attr;\n\tint err;\n\n\tzone_rule->nat = nat;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn -ENOMEM;\n\n\tattr = mlx5_alloc_flow_attr(ct_priv->ns_type);\n\tif (!attr) {\n\t\terr = -ENOMEM;\n\t\tgoto err_attr;\n\t}\n\n\terr = mlx5_tc_ct_entry_create_mod_hdr(ct_priv, attr, flow_rule,\n\t\t\t\t\t      &zone_rule->mh,\n\t\t\t\t\t      zone_restore_id,\n\t\t\t\t\t      nat,\n\t\t\t\t\t      mlx5_tc_ct_entry_has_nat(entry));\n\tif (err) {\n\t\tct_dbg(\"Failed to create ct entry mod hdr\");\n\t\tgoto err_mod_hdr;\n\t}\n\n\tattr->action = MLX5_FLOW_CONTEXT_ACTION_MOD_HDR |\n\t\t       MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |\n\t\t       MLX5_FLOW_CONTEXT_ACTION_COUNT;\n\tattr->dest_chain = 0;\n\tattr->dest_ft = mlx5e_tc_post_act_get_ft(ct_priv->post_act);\n\tattr->ft = nat ? ct_priv->ct_nat : ct_priv->ct;\n\tif (entry->tuple.ip_proto == IPPROTO_TCP ||\n\t    entry->tuple.ip_proto == IPPROTO_UDP)\n\t\tattr->outer_match_level = MLX5_MATCH_L4;\n\telse\n\t\tattr->outer_match_level = MLX5_MATCH_L3;\n\tattr->counter = entry->counter->counter;\n\tattr->flags |= MLX5_ATTR_FLAG_NO_IN_PORT;\n\tif (ct_priv->ns_type == MLX5_FLOW_NAMESPACE_FDB)\n\t\tattr->esw_attr->in_mdev = priv->mdev;\n\n\tmlx5_tc_ct_set_tuple_match(ct_priv, spec, flow_rule);\n\tmlx5e_tc_match_to_reg_match(spec, ZONE_TO_REG, entry->tuple.zone, MLX5_CT_ZONE_MASK);\n\n\tzone_rule->rule = ct_priv->fs_ops->ct_rule_add(ct_priv->fs, spec, attr, flow_rule);\n\tif (IS_ERR(zone_rule->rule)) {\n\t\terr = PTR_ERR(zone_rule->rule);\n\t\tct_dbg(\"Failed to add ct entry rule, nat: %d\", nat);\n\t\tgoto err_rule;\n\t}\n\n\tzone_rule->attr = attr;\n\n\tkvfree(spec);\n\tct_dbg(\"Offloaded ct entry rule in zone %d\", entry->tuple.zone);\n\n\treturn 0;\n\nerr_rule:\n\tmlx5_tc_ct_entry_destroy_mod_hdr(ct_priv, zone_rule->attr, zone_rule->mh);\n\tmlx5_put_label_mapping(ct_priv, attr->ct_attr.ct_labels_id);\nerr_mod_hdr:\n\tkfree(attr);\nerr_attr:\n\tkvfree(spec);\n\treturn err;\n}\n\nstatic int\nmlx5_tc_ct_entry_replace_rule(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t      struct flow_rule *flow_rule,\n\t\t\t      struct mlx5_ct_entry *entry,\n\t\t\t      bool nat, u8 zone_restore_id)\n{\n\tstruct mlx5_ct_zone_rule *zone_rule = &entry->zone_rules[nat];\n\tstruct mlx5_flow_attr *attr = zone_rule->attr, *old_attr;\n\tstruct mlx5e_mod_hdr_handle *mh;\n\tstruct mlx5_ct_fs_rule *rule;\n\tstruct mlx5_flow_spec *spec;\n\tint err;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn -ENOMEM;\n\n\told_attr = mlx5_alloc_flow_attr(ct_priv->ns_type);\n\tif (!old_attr) {\n\t\terr = -ENOMEM;\n\t\tgoto err_attr;\n\t}\n\t*old_attr = *attr;\n\n\terr = mlx5_tc_ct_entry_create_mod_hdr(ct_priv, attr, flow_rule, &mh, zone_restore_id,\n\t\t\t\t\t      nat, mlx5_tc_ct_entry_has_nat(entry));\n\tif (err) {\n\t\tct_dbg(\"Failed to create ct entry mod hdr\");\n\t\tgoto err_mod_hdr;\n\t}\n\n\tmlx5_tc_ct_set_tuple_match(ct_priv, spec, flow_rule);\n\tmlx5e_tc_match_to_reg_match(spec, ZONE_TO_REG, entry->tuple.zone, MLX5_CT_ZONE_MASK);\n\n\trule = ct_priv->fs_ops->ct_rule_add(ct_priv->fs, spec, attr, flow_rule);\n\tif (IS_ERR(rule)) {\n\t\terr = PTR_ERR(rule);\n\t\tct_dbg(\"Failed to add replacement ct entry rule, nat: %d\", nat);\n\t\tgoto err_rule;\n\t}\n\n\tct_priv->fs_ops->ct_rule_del(ct_priv->fs, zone_rule->rule);\n\tzone_rule->rule = rule;\n\tmlx5_tc_ct_entry_destroy_mod_hdr(ct_priv, old_attr, zone_rule->mh);\n\tzone_rule->mh = mh;\n\tmlx5_put_label_mapping(ct_priv, old_attr->ct_attr.ct_labels_id);\n\n\tkfree(old_attr);\n\tkvfree(spec);\n\tct_dbg(\"Replaced ct entry rule in zone %d\", entry->tuple.zone);\n\n\treturn 0;\n\nerr_rule:\n\tmlx5_tc_ct_entry_destroy_mod_hdr(ct_priv, zone_rule->attr, mh);\n\tmlx5_put_label_mapping(ct_priv, attr->ct_attr.ct_labels_id);\nerr_mod_hdr:\n\tkfree(old_attr);\nerr_attr:\n\tkvfree(spec);\n\treturn err;\n}\n\nstatic bool\nmlx5_tc_ct_entry_valid(struct mlx5_ct_entry *entry)\n{\n\treturn test_bit(MLX5_CT_ENTRY_FLAG_VALID, &entry->flags);\n}\n\nstatic struct mlx5_ct_entry *\nmlx5_tc_ct_entry_get(struct mlx5_tc_ct_priv *ct_priv, struct mlx5_ct_tuple *tuple)\n{\n\tstruct mlx5_ct_entry *entry;\n\n\tentry = rhashtable_lookup_fast(&ct_priv->ct_tuples_ht, tuple,\n\t\t\t\t       tuples_ht_params);\n\tif (entry && mlx5_tc_ct_entry_valid(entry) &&\n\t    refcount_inc_not_zero(&entry->refcnt)) {\n\t\treturn entry;\n\t} else if (!entry) {\n\t\tentry = rhashtable_lookup_fast(&ct_priv->ct_tuples_nat_ht,\n\t\t\t\t\t       tuple, tuples_nat_ht_params);\n\t\tif (entry && mlx5_tc_ct_entry_valid(entry) &&\n\t\t    refcount_inc_not_zero(&entry->refcnt))\n\t\t\treturn entry;\n\t}\n\n\treturn entry ? ERR_PTR(-EINVAL) : NULL;\n}\n\nstatic void mlx5_tc_ct_entry_remove_from_tuples(struct mlx5_ct_entry *entry)\n{\n\tstruct mlx5_tc_ct_priv *ct_priv = entry->ct_priv;\n\n\trhashtable_remove_fast(&ct_priv->ct_tuples_nat_ht,\n\t\t\t       &entry->tuple_nat_node,\n\t\t\t       tuples_nat_ht_params);\n\trhashtable_remove_fast(&ct_priv->ct_tuples_ht, &entry->tuple_node,\n\t\t\t       tuples_ht_params);\n}\n\nstatic void mlx5_tc_ct_entry_del(struct mlx5_ct_entry *entry)\n{\n\tstruct mlx5_tc_ct_priv *ct_priv = entry->ct_priv;\n\n\tmlx5_tc_ct_entry_del_rules(ct_priv, entry);\n\n\tspin_lock_bh(&ct_priv->ht_lock);\n\tmlx5_tc_ct_entry_remove_from_tuples(entry);\n\tspin_unlock_bh(&ct_priv->ht_lock);\n\n\tmlx5_tc_ct_counter_put(ct_priv, entry);\n\tkfree(entry);\n}\n\nstatic void\nmlx5_tc_ct_entry_put(struct mlx5_ct_entry *entry)\n{\n\tif (!refcount_dec_and_test(&entry->refcnt))\n\t\treturn;\n\n\tmlx5_tc_ct_entry_del(entry);\n}\n\nstatic void mlx5_tc_ct_entry_del_work(struct work_struct *work)\n{\n\tstruct mlx5_ct_entry *entry = container_of(work, struct mlx5_ct_entry, work);\n\n\tmlx5_tc_ct_entry_del(entry);\n}\n\nstatic void\n__mlx5_tc_ct_entry_put(struct mlx5_ct_entry *entry)\n{\n\tif (!refcount_dec_and_test(&entry->refcnt))\n\t\treturn;\n\n\tINIT_WORK(&entry->work, mlx5_tc_ct_entry_del_work);\n\tqueue_work(entry->ct_priv->wq, &entry->work);\n}\n\nstatic struct mlx5_ct_counter *\nmlx5_tc_ct_counter_create(struct mlx5_tc_ct_priv *ct_priv)\n{\n\tstruct mlx5_ct_counter *counter;\n\tint ret;\n\n\tcounter = kzalloc(sizeof(*counter), GFP_KERNEL);\n\tif (!counter)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcounter->is_shared = false;\n\tcounter->counter = mlx5_fc_create_ex(ct_priv->dev, true);\n\tif (IS_ERR(counter->counter)) {\n\t\tct_dbg(\"Failed to create counter for ct entry\");\n\t\tret = PTR_ERR(counter->counter);\n\t\tkfree(counter);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn counter;\n}\n\nstatic struct mlx5_ct_counter *\nmlx5_tc_ct_shared_counter_get(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t      struct mlx5_ct_entry *entry)\n{\n\tstruct mlx5_ct_tuple rev_tuple = entry->tuple;\n\tstruct mlx5_ct_counter *shared_counter;\n\tstruct mlx5_ct_entry *rev_entry;\n\n\t \n\tswap(rev_tuple.port.src, rev_tuple.port.dst);\n\n\tif (rev_tuple.addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS) {\n\t\t__be32 tmp_addr = rev_tuple.ip.src_v4;\n\n\t\trev_tuple.ip.src_v4 = rev_tuple.ip.dst_v4;\n\t\trev_tuple.ip.dst_v4 = tmp_addr;\n\t} else if (rev_tuple.addr_type == FLOW_DISSECTOR_KEY_IPV6_ADDRS) {\n\t\tstruct in6_addr tmp_addr = rev_tuple.ip.src_v6;\n\n\t\trev_tuple.ip.src_v6 = rev_tuple.ip.dst_v6;\n\t\trev_tuple.ip.dst_v6 = tmp_addr;\n\t} else {\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\t}\n\n\t \n\tspin_lock_bh(&ct_priv->ht_lock);\n\trev_entry = mlx5_tc_ct_entry_get(ct_priv, &rev_tuple);\n\n\tif (IS_ERR(rev_entry)) {\n\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\t\tgoto create_counter;\n\t}\n\n\tif (rev_entry && refcount_inc_not_zero(&rev_entry->counter->refcount)) {\n\t\tct_dbg(\"Using shared counter entry=0x%p rev=0x%p\", entry, rev_entry);\n\t\tshared_counter = rev_entry->counter;\n\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\n\t\tmlx5_tc_ct_entry_put(rev_entry);\n\t\treturn shared_counter;\n\t}\n\n\tspin_unlock_bh(&ct_priv->ht_lock);\n\ncreate_counter:\n\n\tshared_counter = mlx5_tc_ct_counter_create(ct_priv);\n\tif (IS_ERR(shared_counter))\n\t\treturn shared_counter;\n\n\tshared_counter->is_shared = true;\n\trefcount_set(&shared_counter->refcount, 1);\n\treturn shared_counter;\n}\n\nstatic int\nmlx5_tc_ct_entry_add_rules(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t   struct flow_rule *flow_rule,\n\t\t\t   struct mlx5_ct_entry *entry,\n\t\t\t   u8 zone_restore_id)\n{\n\tint err;\n\n\tif (nf_ct_acct_enabled(dev_net(ct_priv->netdev)))\n\t\tentry->counter = mlx5_tc_ct_counter_create(ct_priv);\n\telse\n\t\tentry->counter = mlx5_tc_ct_shared_counter_get(ct_priv, entry);\n\n\tif (IS_ERR(entry->counter)) {\n\t\terr = PTR_ERR(entry->counter);\n\t\treturn err;\n\t}\n\n\terr = mlx5_tc_ct_entry_add_rule(ct_priv, flow_rule, entry, false,\n\t\t\t\t\tzone_restore_id);\n\tif (err)\n\t\tgoto err_orig;\n\n\terr = mlx5_tc_ct_entry_add_rule(ct_priv, flow_rule, entry, true,\n\t\t\t\t\tzone_restore_id);\n\tif (err)\n\t\tgoto err_nat;\n\n\tatomic_inc(&ct_priv->debugfs.stats.offloaded);\n\treturn 0;\n\nerr_nat:\n\tmlx5_tc_ct_entry_del_rule(ct_priv, entry, false);\nerr_orig:\n\tmlx5_tc_ct_counter_put(ct_priv, entry);\n\treturn err;\n}\n\nstatic int\nmlx5_tc_ct_entry_replace_rules(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t       struct flow_rule *flow_rule,\n\t\t\t       struct mlx5_ct_entry *entry,\n\t\t\t       u8 zone_restore_id)\n{\n\tint err;\n\n\terr = mlx5_tc_ct_entry_replace_rule(ct_priv, flow_rule, entry, false,\n\t\t\t\t\t    zone_restore_id);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_tc_ct_entry_replace_rule(ct_priv, flow_rule, entry, true,\n\t\t\t\t\t    zone_restore_id);\n\tif (err)\n\t\tmlx5_tc_ct_entry_del_rule(ct_priv, entry, false);\n\treturn err;\n}\n\nstatic int\nmlx5_tc_ct_block_flow_offload_replace(struct mlx5_ct_ft *ft, struct flow_rule *flow_rule,\n\t\t\t\t      struct mlx5_ct_entry *entry, unsigned long cookie)\n{\n\tstruct mlx5_tc_ct_priv *ct_priv = ft->ct_priv;\n\tint err;\n\n\terr = mlx5_tc_ct_entry_replace_rules(ct_priv, flow_rule, entry, ft->zone_restore_id);\n\tif (!err)\n\t\treturn 0;\n\n\t \n\tspin_lock_bh(&ct_priv->ht_lock);\n\tentry = rhashtable_lookup_fast(&ft->ct_entries_ht, &cookie, cts_ht_params);\n\tif (entry) {\n\t\trhashtable_remove_fast(&ft->ct_entries_ht, &entry->node, cts_ht_params);\n\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\t\tmlx5_tc_ct_entry_put(entry);\n\t} else {\n\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\t}\n\treturn err;\n}\n\nstatic int\nmlx5_tc_ct_block_flow_offload_add(struct mlx5_ct_ft *ft,\n\t\t\t\t  struct flow_cls_offload *flow)\n{\n\tstruct flow_rule *flow_rule = flow_cls_offload_flow_rule(flow);\n\tstruct mlx5_tc_ct_priv *ct_priv = ft->ct_priv;\n\tstruct flow_action_entry *meta_action;\n\tunsigned long cookie = flow->cookie;\n\tstruct mlx5_ct_entry *entry;\n\tint err;\n\n\tmeta_action = mlx5_tc_ct_get_ct_metadata_action(flow_rule);\n\tif (!meta_action)\n\t\treturn -EOPNOTSUPP;\n\n\tspin_lock_bh(&ct_priv->ht_lock);\n\tentry = rhashtable_lookup_fast(&ft->ct_entries_ht, &cookie, cts_ht_params);\n\tif (entry && refcount_inc_not_zero(&entry->refcnt)) {\n\t\tif (entry->restore_cookie == meta_action->ct_metadata.cookie) {\n\t\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\t\t\tmlx5_tc_ct_entry_put(entry);\n\t\t\treturn -EEXIST;\n\t\t}\n\t\tentry->restore_cookie = meta_action->ct_metadata.cookie;\n\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\n\t\terr = mlx5_tc_ct_block_flow_offload_replace(ft, flow_rule, entry, cookie);\n\t\tmlx5_tc_ct_entry_put(entry);\n\t\treturn err;\n\t}\n\tspin_unlock_bh(&ct_priv->ht_lock);\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\tentry->tuple.zone = ft->zone;\n\tentry->cookie = flow->cookie;\n\tentry->restore_cookie = meta_action->ct_metadata.cookie;\n\trefcount_set(&entry->refcnt, 2);\n\tentry->ct_priv = ct_priv;\n\n\terr = mlx5_tc_ct_rule_to_tuple(&entry->tuple, flow_rule);\n\tif (err)\n\t\tgoto err_set;\n\n\tmemcpy(&entry->tuple_nat, &entry->tuple, sizeof(entry->tuple));\n\terr = mlx5_tc_ct_rule_to_tuple_nat(&entry->tuple_nat, flow_rule);\n\tif (err)\n\t\tgoto err_set;\n\n\tspin_lock_bh(&ct_priv->ht_lock);\n\n\terr = rhashtable_lookup_insert_fast(&ft->ct_entries_ht, &entry->node,\n\t\t\t\t\t    cts_ht_params);\n\tif (err)\n\t\tgoto err_entries;\n\n\terr = rhashtable_lookup_insert_fast(&ct_priv->ct_tuples_ht,\n\t\t\t\t\t    &entry->tuple_node,\n\t\t\t\t\t    tuples_ht_params);\n\tif (err)\n\t\tgoto err_tuple;\n\n\tif (memcmp(&entry->tuple, &entry->tuple_nat, sizeof(entry->tuple))) {\n\t\terr = rhashtable_lookup_insert_fast(&ct_priv->ct_tuples_nat_ht,\n\t\t\t\t\t\t    &entry->tuple_nat_node,\n\t\t\t\t\t\t    tuples_nat_ht_params);\n\t\tif (err)\n\t\t\tgoto err_tuple_nat;\n\t}\n\tspin_unlock_bh(&ct_priv->ht_lock);\n\n\terr = mlx5_tc_ct_entry_add_rules(ct_priv, flow_rule, entry,\n\t\t\t\t\t ft->zone_restore_id);\n\tif (err)\n\t\tgoto err_rules;\n\n\tset_bit(MLX5_CT_ENTRY_FLAG_VALID, &entry->flags);\n\tmlx5_tc_ct_entry_put(entry);  \n\n\treturn 0;\n\nerr_rules:\n\tspin_lock_bh(&ct_priv->ht_lock);\n\tif (mlx5_tc_ct_entry_has_nat(entry))\n\t\trhashtable_remove_fast(&ct_priv->ct_tuples_nat_ht,\n\t\t\t\t       &entry->tuple_nat_node, tuples_nat_ht_params);\nerr_tuple_nat:\n\trhashtable_remove_fast(&ct_priv->ct_tuples_ht,\n\t\t\t       &entry->tuple_node,\n\t\t\t       tuples_ht_params);\nerr_tuple:\n\trhashtable_remove_fast(&ft->ct_entries_ht,\n\t\t\t       &entry->node,\n\t\t\t       cts_ht_params);\nerr_entries:\n\tspin_unlock_bh(&ct_priv->ht_lock);\nerr_set:\n\tkfree(entry);\n\tif (err != -EEXIST)\n\t\tnetdev_warn(ct_priv->netdev, \"Failed to offload ct entry, err: %d\\n\", err);\n\treturn err;\n}\n\nstatic int\nmlx5_tc_ct_block_flow_offload_del(struct mlx5_ct_ft *ft,\n\t\t\t\t  struct flow_cls_offload *flow)\n{\n\tstruct mlx5_tc_ct_priv *ct_priv = ft->ct_priv;\n\tunsigned long cookie = flow->cookie;\n\tstruct mlx5_ct_entry *entry;\n\n\tspin_lock_bh(&ct_priv->ht_lock);\n\tentry = rhashtable_lookup_fast(&ft->ct_entries_ht, &cookie, cts_ht_params);\n\tif (!entry) {\n\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\t\treturn -ENOENT;\n\t}\n\n\tif (!mlx5_tc_ct_entry_valid(entry)) {\n\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\t\treturn -EINVAL;\n\t}\n\n\trhashtable_remove_fast(&ft->ct_entries_ht, &entry->node, cts_ht_params);\n\tspin_unlock_bh(&ct_priv->ht_lock);\n\n\tmlx5_tc_ct_entry_put(entry);\n\n\treturn 0;\n}\n\nstatic int\nmlx5_tc_ct_block_flow_offload_stats(struct mlx5_ct_ft *ft,\n\t\t\t\t    struct flow_cls_offload *f)\n{\n\tstruct mlx5_tc_ct_priv *ct_priv = ft->ct_priv;\n\tunsigned long cookie = f->cookie;\n\tstruct mlx5_ct_entry *entry;\n\tu64 lastuse, packets, bytes;\n\n\tspin_lock_bh(&ct_priv->ht_lock);\n\tentry = rhashtable_lookup_fast(&ft->ct_entries_ht, &cookie, cts_ht_params);\n\tif (!entry) {\n\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\t\treturn -ENOENT;\n\t}\n\n\tif (!mlx5_tc_ct_entry_valid(entry) || !refcount_inc_not_zero(&entry->refcnt)) {\n\t\tspin_unlock_bh(&ct_priv->ht_lock);\n\t\treturn -EINVAL;\n\t}\n\n\tspin_unlock_bh(&ct_priv->ht_lock);\n\n\tmlx5_fc_query_cached(entry->counter->counter, &bytes, &packets, &lastuse);\n\tflow_stats_update(&f->stats, bytes, packets, 0, lastuse,\n\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\n\tmlx5_tc_ct_entry_put(entry);\n\treturn 0;\n}\n\nstatic int\nmlx5_tc_ct_block_flow_offload(enum tc_setup_type type, void *type_data,\n\t\t\t      void *cb_priv)\n{\n\tstruct flow_cls_offload *f = type_data;\n\tstruct mlx5_ct_ft *ft = cb_priv;\n\n\tif (type != TC_SETUP_CLSFLOWER)\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (f->command) {\n\tcase FLOW_CLS_REPLACE:\n\t\treturn mlx5_tc_ct_block_flow_offload_add(ft, f);\n\tcase FLOW_CLS_DESTROY:\n\t\treturn mlx5_tc_ct_block_flow_offload_del(ft, f);\n\tcase FLOW_CLS_STATS:\n\t\treturn mlx5_tc_ct_block_flow_offload_stats(ft, f);\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic bool\nmlx5_tc_ct_skb_to_tuple(struct sk_buff *skb, struct mlx5_ct_tuple *tuple,\n\t\t\tu16 zone)\n{\n\tstruct flow_keys flow_keys;\n\n\tskb_reset_network_header(skb);\n\tskb_flow_dissect_flow_keys(skb, &flow_keys, FLOW_DISSECTOR_F_STOP_BEFORE_ENCAP);\n\n\ttuple->zone = zone;\n\n\tif (flow_keys.basic.ip_proto != IPPROTO_TCP &&\n\t    flow_keys.basic.ip_proto != IPPROTO_UDP &&\n\t    flow_keys.basic.ip_proto != IPPROTO_GRE)\n\t\treturn false;\n\n\tif (flow_keys.basic.ip_proto == IPPROTO_TCP ||\n\t    flow_keys.basic.ip_proto == IPPROTO_UDP) {\n\t\ttuple->port.src = flow_keys.ports.src;\n\t\ttuple->port.dst = flow_keys.ports.dst;\n\t}\n\ttuple->n_proto = flow_keys.basic.n_proto;\n\ttuple->ip_proto = flow_keys.basic.ip_proto;\n\n\tswitch (flow_keys.basic.n_proto) {\n\tcase htons(ETH_P_IP):\n\t\ttuple->addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n\t\ttuple->ip.src_v4 = flow_keys.addrs.v4addrs.src;\n\t\ttuple->ip.dst_v4 = flow_keys.addrs.v4addrs.dst;\n\t\tbreak;\n\n\tcase htons(ETH_P_IPV6):\n\t\ttuple->addr_type = FLOW_DISSECTOR_KEY_IPV6_ADDRS;\n\t\ttuple->ip.src_v6 = flow_keys.addrs.v6addrs.src;\n\t\ttuple->ip.dst_v6 = flow_keys.addrs.v6addrs.dst;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\treturn true;\n\nout:\n\treturn false;\n}\n\nint mlx5_tc_ct_add_no_trk_match(struct mlx5_flow_spec *spec)\n{\n\tu32 ctstate = 0, ctstate_mask = 0;\n\n\tmlx5e_tc_match_to_reg_get_match(spec, CTSTATE_TO_REG,\n\t\t\t\t\t&ctstate, &ctstate_mask);\n\n\tif ((ctstate & ctstate_mask) == MLX5_CT_STATE_TRK_BIT)\n\t\treturn -EOPNOTSUPP;\n\n\tctstate_mask |= MLX5_CT_STATE_TRK_BIT;\n\tmlx5e_tc_match_to_reg_match(spec, CTSTATE_TO_REG,\n\t\t\t\t    ctstate, ctstate_mask);\n\n\treturn 0;\n}\n\nvoid mlx5_tc_ct_match_del(struct mlx5_tc_ct_priv *priv, struct mlx5_ct_attr *ct_attr)\n{\n\tif (!priv || !ct_attr->ct_labels_id)\n\t\treturn;\n\n\tmlx5_put_label_mapping(priv, ct_attr->ct_labels_id);\n}\n\nint\nmlx5_tc_ct_match_add(struct mlx5_tc_ct_priv *priv,\n\t\t     struct mlx5_flow_spec *spec,\n\t\t     struct flow_cls_offload *f,\n\t\t     struct mlx5_ct_attr *ct_attr,\n\t\t     struct netlink_ext_ack *extack)\n{\n\tbool trk, est, untrk, unnew, unest, new, rpl, unrpl, rel, unrel, inv, uninv;\n\tstruct flow_rule *rule = flow_cls_offload_flow_rule(f);\n\tstruct flow_dissector_key_ct *mask, *key;\n\tu32 ctstate = 0, ctstate_mask = 0;\n\tu16 ct_state_on, ct_state_off;\n\tu16 ct_state, ct_state_mask;\n\tstruct flow_match_ct match;\n\tu32 ct_labels[4];\n\n\tif (!flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_CT))\n\t\treturn 0;\n\n\tif (!priv) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"offload of ct matching isn't available\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tflow_rule_match_ct(rule, &match);\n\n\tkey = match.key;\n\tmask = match.mask;\n\n\tct_state = key->ct_state;\n\tct_state_mask = mask->ct_state;\n\n\tif (ct_state_mask & ~(TCA_FLOWER_KEY_CT_FLAGS_TRACKED |\n\t\t\t      TCA_FLOWER_KEY_CT_FLAGS_ESTABLISHED |\n\t\t\t      TCA_FLOWER_KEY_CT_FLAGS_NEW |\n\t\t\t      TCA_FLOWER_KEY_CT_FLAGS_REPLY |\n\t\t\t      TCA_FLOWER_KEY_CT_FLAGS_RELATED |\n\t\t\t      TCA_FLOWER_KEY_CT_FLAGS_INVALID)) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"only ct_state trk, est, new and rpl are supported for offload\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tct_state_on = ct_state & ct_state_mask;\n\tct_state_off = (ct_state & ct_state_mask) ^ ct_state_mask;\n\ttrk = ct_state_on & TCA_FLOWER_KEY_CT_FLAGS_TRACKED;\n\tnew = ct_state_on & TCA_FLOWER_KEY_CT_FLAGS_NEW;\n\test = ct_state_on & TCA_FLOWER_KEY_CT_FLAGS_ESTABLISHED;\n\trpl = ct_state_on & TCA_FLOWER_KEY_CT_FLAGS_REPLY;\n\trel = ct_state_on & TCA_FLOWER_KEY_CT_FLAGS_RELATED;\n\tinv = ct_state_on & TCA_FLOWER_KEY_CT_FLAGS_INVALID;\n\tuntrk = ct_state_off & TCA_FLOWER_KEY_CT_FLAGS_TRACKED;\n\tunnew = ct_state_off & TCA_FLOWER_KEY_CT_FLAGS_NEW;\n\tunest = ct_state_off & TCA_FLOWER_KEY_CT_FLAGS_ESTABLISHED;\n\tunrpl = ct_state_off & TCA_FLOWER_KEY_CT_FLAGS_REPLY;\n\tunrel = ct_state_off & TCA_FLOWER_KEY_CT_FLAGS_RELATED;\n\tuninv = ct_state_off & TCA_FLOWER_KEY_CT_FLAGS_INVALID;\n\n\tctstate |= trk ? MLX5_CT_STATE_TRK_BIT : 0;\n\tctstate |= new ? MLX5_CT_STATE_NEW_BIT : 0;\n\tctstate |= est ? MLX5_CT_STATE_ESTABLISHED_BIT : 0;\n\tctstate |= rpl ? MLX5_CT_STATE_REPLY_BIT : 0;\n\tctstate_mask |= (untrk || trk) ? MLX5_CT_STATE_TRK_BIT : 0;\n\tctstate_mask |= (unnew || new) ? MLX5_CT_STATE_NEW_BIT : 0;\n\tctstate_mask |= (unest || est) ? MLX5_CT_STATE_ESTABLISHED_BIT : 0;\n\tctstate_mask |= (unrpl || rpl) ? MLX5_CT_STATE_REPLY_BIT : 0;\n\tctstate_mask |= unrel ? MLX5_CT_STATE_RELATED_BIT : 0;\n\tctstate_mask |= uninv ? MLX5_CT_STATE_INVALID_BIT : 0;\n\n\tif (rel) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"matching on ct_state +rel isn't supported\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (inv) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"matching on ct_state +inv isn't supported\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (mask->ct_zone)\n\t\tmlx5e_tc_match_to_reg_match(spec, ZONE_TO_REG,\n\t\t\t\t\t    key->ct_zone, MLX5_CT_ZONE_MASK);\n\tif (ctstate_mask)\n\t\tmlx5e_tc_match_to_reg_match(spec, CTSTATE_TO_REG,\n\t\t\t\t\t    ctstate, ctstate_mask);\n\tif (mask->ct_mark)\n\t\tmlx5e_tc_match_to_reg_match(spec, MARK_TO_REG,\n\t\t\t\t\t    key->ct_mark, mask->ct_mark);\n\tif (mask->ct_labels[0] || mask->ct_labels[1] || mask->ct_labels[2] ||\n\t    mask->ct_labels[3]) {\n\t\tct_labels[0] = key->ct_labels[0] & mask->ct_labels[0];\n\t\tct_labels[1] = key->ct_labels[1] & mask->ct_labels[1];\n\t\tct_labels[2] = key->ct_labels[2] & mask->ct_labels[2];\n\t\tct_labels[3] = key->ct_labels[3] & mask->ct_labels[3];\n\t\tif (mlx5_get_label_mapping(priv, ct_labels, &ct_attr->ct_labels_id))\n\t\t\treturn -EOPNOTSUPP;\n\t\tmlx5e_tc_match_to_reg_match(spec, LABELS_TO_REG, ct_attr->ct_labels_id,\n\t\t\t\t\t    MLX5_CT_LABELS_MASK);\n\t}\n\n\treturn 0;\n}\n\nint\nmlx5_tc_ct_parse_action(struct mlx5_tc_ct_priv *priv,\n\t\t\tstruct mlx5_flow_attr *attr,\n\t\t\tconst struct flow_action_entry *act,\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\tif (!priv) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"offload of ct action isn't available\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tattr->ct_attr.ct_action |= act->ct.action;  \n\tattr->ct_attr.zone = act->ct.zone;\n\tif (!(act->ct.action & TCA_CT_ACT_CLEAR))\n\t\tattr->ct_attr.nf_ft = act->ct.flow_table;\n\tattr->ct_attr.act_miss_cookie = act->miss_cookie;\n\n\treturn 0;\n}\n\nstatic int tc_ct_pre_ct_add_rules(struct mlx5_ct_ft *ct_ft,\n\t\t\t\t  struct mlx5_tc_ct_pre *pre_ct,\n\t\t\t\t  bool nat)\n{\n\tstruct mlx5_tc_ct_priv *ct_priv = ct_ft->ct_priv;\n\tstruct mlx5e_tc_mod_hdr_acts pre_mod_acts = {};\n\tstruct mlx5_core_dev *dev = ct_priv->dev;\n\tstruct mlx5_flow_table *ft = pre_ct->ft;\n\tstruct mlx5_flow_destination dest = {};\n\tstruct mlx5_flow_act flow_act = {};\n\tstruct mlx5_modify_hdr *mod_hdr;\n\tstruct mlx5_flow_handle *rule;\n\tstruct mlx5_flow_spec *spec;\n\tu32 ctstate;\n\tu16 zone;\n\tint err;\n\n\tspec = kvzalloc(sizeof(*spec), GFP_KERNEL);\n\tif (!spec)\n\t\treturn -ENOMEM;\n\n\tzone = ct_ft->zone & MLX5_CT_ZONE_MASK;\n\terr = mlx5e_tc_match_to_reg_set(dev, &pre_mod_acts, ct_priv->ns_type,\n\t\t\t\t\tZONE_TO_REG, zone);\n\tif (err) {\n\t\tct_dbg(\"Failed to set zone register mapping\");\n\t\tgoto err_mapping;\n\t}\n\n\tmod_hdr = mlx5_modify_header_alloc(dev, ct_priv->ns_type,\n\t\t\t\t\t   pre_mod_acts.num_actions,\n\t\t\t\t\t   pre_mod_acts.actions);\n\n\tif (IS_ERR(mod_hdr)) {\n\t\terr = PTR_ERR(mod_hdr);\n\t\tct_dbg(\"Failed to create pre ct mod hdr\");\n\t\tgoto err_mapping;\n\t}\n\tpre_ct->modify_hdr = mod_hdr;\n\n\tflow_act.action = MLX5_FLOW_CONTEXT_ACTION_FWD_DEST |\n\t\t\t  MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;\n\tflow_act.flags |= FLOW_ACT_IGNORE_FLOW_LEVEL;\n\tflow_act.modify_hdr = mod_hdr;\n\tdest.type = MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE;\n\n\t \n\tmlx5e_tc_match_to_reg_match(spec, ZONE_TO_REG,\n\t\t\t\t    zone, MLX5_CT_ZONE_MASK);\n\tctstate = MLX5_CT_STATE_TRK_BIT;\n\tif (nat)\n\t\tctstate |= MLX5_CT_STATE_NAT_BIT;\n\tmlx5e_tc_match_to_reg_match(spec, CTSTATE_TO_REG, ctstate, ctstate);\n\n\tdest.ft = mlx5e_tc_post_act_get_ft(ct_priv->post_act);\n\trule = mlx5_add_flow_rules(ft, spec, &flow_act, &dest, 1);\n\tif (IS_ERR(rule)) {\n\t\terr = PTR_ERR(rule);\n\t\tct_dbg(\"Failed to add pre ct flow rule zone %d\", zone);\n\t\tgoto err_flow_rule;\n\t}\n\tpre_ct->flow_rule = rule;\n\n\t \n\tdest.ft = nat ? ct_priv->ct_nat : ct_priv->ct;\n\trule = mlx5_add_flow_rules(ft, NULL, &flow_act, &dest, 1);\n\tif (IS_ERR(rule)) {\n\t\terr = PTR_ERR(rule);\n\t\tct_dbg(\"Failed to add pre ct miss rule zone %d\", zone);\n\t\tgoto err_miss_rule;\n\t}\n\tpre_ct->miss_rule = rule;\n\n\tmlx5e_mod_hdr_dealloc(&pre_mod_acts);\n\tkvfree(spec);\n\treturn 0;\n\nerr_miss_rule:\n\tmlx5_del_flow_rules(pre_ct->flow_rule);\nerr_flow_rule:\n\tmlx5_modify_header_dealloc(dev, pre_ct->modify_hdr);\nerr_mapping:\n\tmlx5e_mod_hdr_dealloc(&pre_mod_acts);\n\tkvfree(spec);\n\treturn err;\n}\n\nstatic void\ntc_ct_pre_ct_del_rules(struct mlx5_ct_ft *ct_ft,\n\t\t       struct mlx5_tc_ct_pre *pre_ct)\n{\n\tstruct mlx5_tc_ct_priv *ct_priv = ct_ft->ct_priv;\n\tstruct mlx5_core_dev *dev = ct_priv->dev;\n\n\tmlx5_del_flow_rules(pre_ct->flow_rule);\n\tmlx5_del_flow_rules(pre_ct->miss_rule);\n\tmlx5_modify_header_dealloc(dev, pre_ct->modify_hdr);\n}\n\nstatic int\nmlx5_tc_ct_alloc_pre_ct(struct mlx5_ct_ft *ct_ft,\n\t\t\tstruct mlx5_tc_ct_pre *pre_ct,\n\t\t\tbool nat)\n{\n\tint inlen = MLX5_ST_SZ_BYTES(create_flow_group_in);\n\tstruct mlx5_tc_ct_priv *ct_priv = ct_ft->ct_priv;\n\tstruct mlx5_core_dev *dev = ct_priv->dev;\n\tstruct mlx5_flow_table_attr ft_attr = {};\n\tstruct mlx5_flow_namespace *ns;\n\tstruct mlx5_flow_table *ft;\n\tstruct mlx5_flow_group *g;\n\tu32 metadata_reg_c_2_mask;\n\tu32 *flow_group_in;\n\tvoid *misc;\n\tint err;\n\n\tns = mlx5_get_flow_namespace(dev, ct_priv->ns_type);\n\tif (!ns) {\n\t\terr = -EOPNOTSUPP;\n\t\tct_dbg(\"Failed to get flow namespace\");\n\t\treturn err;\n\t}\n\n\tflow_group_in = kvzalloc(inlen, GFP_KERNEL);\n\tif (!flow_group_in)\n\t\treturn -ENOMEM;\n\n\tft_attr.flags = MLX5_FLOW_TABLE_UNMANAGED;\n\tft_attr.prio =  ct_priv->ns_type ==  MLX5_FLOW_NAMESPACE_FDB ?\n\t\t\tFDB_TC_OFFLOAD : MLX5E_TC_PRIO;\n\tft_attr.max_fte = 2;\n\tft_attr.level = 1;\n\tft = mlx5_create_flow_table(ns, &ft_attr);\n\tif (IS_ERR(ft)) {\n\t\terr = PTR_ERR(ft);\n\t\tct_dbg(\"Failed to create pre ct table\");\n\t\tgoto out_free;\n\t}\n\tpre_ct->ft = ft;\n\n\t \n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, 0);\n\tMLX5_SET(create_flow_group_in, flow_group_in, end_flow_index, 0);\n\tMLX5_SET(create_flow_group_in, flow_group_in, match_criteria_enable,\n\t\t MLX5_MATCH_MISC_PARAMETERS_2);\n\n\tmisc = MLX5_ADDR_OF(create_flow_group_in, flow_group_in,\n\t\t\t    match_criteria.misc_parameters_2);\n\n\tmetadata_reg_c_2_mask = MLX5_CT_ZONE_MASK;\n\tmetadata_reg_c_2_mask |= (MLX5_CT_STATE_TRK_BIT << 16);\n\tif (nat)\n\t\tmetadata_reg_c_2_mask |= (MLX5_CT_STATE_NAT_BIT << 16);\n\n\tMLX5_SET(fte_match_set_misc2, misc, metadata_reg_c_2,\n\t\t metadata_reg_c_2_mask);\n\n\tg = mlx5_create_flow_group(ft, flow_group_in);\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tct_dbg(\"Failed to create pre ct group\");\n\t\tgoto err_flow_grp;\n\t}\n\tpre_ct->flow_grp = g;\n\n\t \n\tmemset(flow_group_in, 0, inlen);\n\tMLX5_SET(create_flow_group_in, flow_group_in, start_flow_index, 1);\n\tMLX5_SET(create_flow_group_in, flow_group_in, end_flow_index, 1);\n\tg = mlx5_create_flow_group(ft, flow_group_in);\n\tif (IS_ERR(g)) {\n\t\terr = PTR_ERR(g);\n\t\tct_dbg(\"Failed to create pre ct miss group\");\n\t\tgoto err_miss_grp;\n\t}\n\tpre_ct->miss_grp = g;\n\n\terr = tc_ct_pre_ct_add_rules(ct_ft, pre_ct, nat);\n\tif (err)\n\t\tgoto err_add_rules;\n\n\tkvfree(flow_group_in);\n\treturn 0;\n\nerr_add_rules:\n\tmlx5_destroy_flow_group(pre_ct->miss_grp);\nerr_miss_grp:\n\tmlx5_destroy_flow_group(pre_ct->flow_grp);\nerr_flow_grp:\n\tmlx5_destroy_flow_table(ft);\nout_free:\n\tkvfree(flow_group_in);\n\treturn err;\n}\n\nstatic void\nmlx5_tc_ct_free_pre_ct(struct mlx5_ct_ft *ct_ft,\n\t\t       struct mlx5_tc_ct_pre *pre_ct)\n{\n\ttc_ct_pre_ct_del_rules(ct_ft, pre_ct);\n\tmlx5_destroy_flow_group(pre_ct->miss_grp);\n\tmlx5_destroy_flow_group(pre_ct->flow_grp);\n\tmlx5_destroy_flow_table(pre_ct->ft);\n}\n\nstatic int\nmlx5_tc_ct_alloc_pre_ct_tables(struct mlx5_ct_ft *ft)\n{\n\tint err;\n\n\terr = mlx5_tc_ct_alloc_pre_ct(ft, &ft->pre_ct, false);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx5_tc_ct_alloc_pre_ct(ft, &ft->pre_ct_nat, true);\n\tif (err)\n\t\tgoto err_pre_ct_nat;\n\n\treturn 0;\n\nerr_pre_ct_nat:\n\tmlx5_tc_ct_free_pre_ct(ft, &ft->pre_ct);\n\treturn err;\n}\n\nstatic void\nmlx5_tc_ct_free_pre_ct_tables(struct mlx5_ct_ft *ft)\n{\n\tmlx5_tc_ct_free_pre_ct(ft, &ft->pre_ct_nat);\n\tmlx5_tc_ct_free_pre_ct(ft, &ft->pre_ct);\n}\n\n \nstatic struct lock_class_key ct_entries_ht_lock_key;\n\nstatic struct mlx5_ct_ft *\nmlx5_tc_ct_add_ft_cb(struct mlx5_tc_ct_priv *ct_priv, u16 zone,\n\t\t     struct nf_flowtable *nf_ft)\n{\n\tstruct mlx5_ct_ft *ft;\n\tint err;\n\n\tft = rhashtable_lookup_fast(&ct_priv->zone_ht, &zone, zone_params);\n\tif (ft) {\n\t\trefcount_inc(&ft->refcount);\n\t\treturn ft;\n\t}\n\n\tft = kzalloc(sizeof(*ft), GFP_KERNEL);\n\tif (!ft)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = mapping_add(ct_priv->zone_mapping, &zone, &ft->zone_restore_id);\n\tif (err)\n\t\tgoto err_mapping;\n\n\tft->zone = zone;\n\tft->nf_ft = nf_ft;\n\tft->ct_priv = ct_priv;\n\trefcount_set(&ft->refcount, 1);\n\n\terr = mlx5_tc_ct_alloc_pre_ct_tables(ft);\n\tif (err)\n\t\tgoto err_alloc_pre_ct;\n\n\terr = rhashtable_init(&ft->ct_entries_ht, &cts_ht_params);\n\tif (err)\n\t\tgoto err_init;\n\n\tlockdep_set_class(&ft->ct_entries_ht.mutex, &ct_entries_ht_lock_key);\n\n\terr = rhashtable_insert_fast(&ct_priv->zone_ht, &ft->node,\n\t\t\t\t     zone_params);\n\tif (err)\n\t\tgoto err_insert;\n\n\terr = nf_flow_table_offload_add_cb(ft->nf_ft,\n\t\t\t\t\t   mlx5_tc_ct_block_flow_offload, ft);\n\tif (err)\n\t\tgoto err_add_cb;\n\n\treturn ft;\n\nerr_add_cb:\n\trhashtable_remove_fast(&ct_priv->zone_ht, &ft->node, zone_params);\nerr_insert:\n\trhashtable_destroy(&ft->ct_entries_ht);\nerr_init:\n\tmlx5_tc_ct_free_pre_ct_tables(ft);\nerr_alloc_pre_ct:\n\tmapping_remove(ct_priv->zone_mapping, ft->zone_restore_id);\nerr_mapping:\n\tkfree(ft);\n\treturn ERR_PTR(err);\n}\n\nstatic void\nmlx5_tc_ct_flush_ft_entry(void *ptr, void *arg)\n{\n\tstruct mlx5_ct_entry *entry = ptr;\n\n\tmlx5_tc_ct_entry_put(entry);\n}\n\nstatic void\nmlx5_tc_ct_del_ft_cb(struct mlx5_tc_ct_priv *ct_priv, struct mlx5_ct_ft *ft)\n{\n\tif (!refcount_dec_and_test(&ft->refcount))\n\t\treturn;\n\n\tflush_workqueue(ct_priv->wq);\n\tnf_flow_table_offload_del_cb(ft->nf_ft,\n\t\t\t\t     mlx5_tc_ct_block_flow_offload, ft);\n\trhashtable_remove_fast(&ct_priv->zone_ht, &ft->node, zone_params);\n\trhashtable_free_and_destroy(&ft->ct_entries_ht,\n\t\t\t\t    mlx5_tc_ct_flush_ft_entry,\n\t\t\t\t    ct_priv);\n\tmlx5_tc_ct_free_pre_ct_tables(ft);\n\tmapping_remove(ct_priv->zone_mapping, ft->zone_restore_id);\n\tkfree(ft);\n}\n\n \nstatic int\n__mlx5_tc_ct_flow_offload(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t  struct mlx5_flow_attr *attr)\n{\n\tbool nat = attr->ct_attr.ct_action & TCA_CT_ACT_NAT;\n\tstruct mlx5e_priv *priv = netdev_priv(ct_priv->netdev);\n\tint act_miss_mapping = 0, err;\n\tstruct mlx5_ct_ft *ft;\n\tu16 zone;\n\n\t \n\tft = mlx5_tc_ct_add_ft_cb(ct_priv, attr->ct_attr.zone,\n\t\t\t\t  attr->ct_attr.nf_ft);\n\tif (IS_ERR(ft)) {\n\t\terr = PTR_ERR(ft);\n\t\tct_dbg(\"Failed to register to ft callback\");\n\t\tgoto err_ft;\n\t}\n\tattr->ct_attr.ft = ft;\n\n\terr = mlx5e_tc_action_miss_mapping_get(ct_priv->priv, attr, attr->ct_attr.act_miss_cookie,\n\t\t\t\t\t       &act_miss_mapping);\n\tif (err) {\n\t\tct_dbg(\"Failed to get register mapping for act miss\");\n\t\tgoto err_get_act_miss;\n\t}\n\n\terr = mlx5e_tc_match_to_reg_set(priv->mdev, &attr->parse_attr->mod_hdr_acts,\n\t\t\t\t\tct_priv->ns_type, MAPPED_OBJ_TO_REG, act_miss_mapping);\n\tif (err) {\n\t\tct_dbg(\"Failed to set act miss register mapping\");\n\t\tgoto err_mapping;\n\t}\n\n\t \n\tif (!attr->chain) {\n\t\tzone = ft->zone & MLX5_CT_ZONE_MASK;\n\t\terr = mlx5e_tc_match_to_reg_set(priv->mdev, &attr->parse_attr->mod_hdr_acts,\n\t\t\t\t\t\tct_priv->ns_type, ZONE_TO_REG, zone);\n\t\tif (err) {\n\t\t\tct_dbg(\"Failed to set zone register mapping\");\n\t\t\tgoto err_mapping;\n\t\t}\n\n\t\tattr->dest_ft = nat ? ct_priv->ct_nat : ct_priv->ct;\n\t} else {\n\t\tattr->dest_ft = nat ? ft->pre_ct_nat.ft : ft->pre_ct.ft;\n\t}\n\n\tattr->action |= MLX5_FLOW_CONTEXT_ACTION_FWD_DEST | MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;\n\tattr->ct_attr.act_miss_mapping = act_miss_mapping;\n\n\treturn 0;\n\nerr_mapping:\n\tmlx5e_tc_action_miss_mapping_put(ct_priv->priv, attr, act_miss_mapping);\nerr_get_act_miss:\n\tmlx5_tc_ct_del_ft_cb(ct_priv, ft);\nerr_ft:\n\tnetdev_warn(priv->netdev, \"Failed to offload ct flow, err %d\\n\", err);\n\treturn err;\n}\n\nint\nmlx5_tc_ct_flow_offload(struct mlx5_tc_ct_priv *priv, struct mlx5_flow_attr *attr)\n{\n\tint err;\n\n\tif (!priv)\n\t\treturn -EOPNOTSUPP;\n\n\tif (attr->ct_attr.offloaded)\n\t\treturn 0;\n\n\tif (attr->ct_attr.ct_action & TCA_CT_ACT_CLEAR) {\n\t\terr = mlx5_tc_ct_entry_set_registers(priv, &attr->parse_attr->mod_hdr_acts,\n\t\t\t\t\t\t     0, 0, 0, 0);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tattr->action |= MLX5_FLOW_CONTEXT_ACTION_MOD_HDR;\n\t}\n\n\tif (!attr->ct_attr.nf_ft) {  \n\t\tattr->ct_attr.offloaded = true;\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&priv->control_lock);\n\terr = __mlx5_tc_ct_flow_offload(priv, attr);\n\tif (!err)\n\t\tattr->ct_attr.offloaded = true;\n\tmutex_unlock(&priv->control_lock);\n\n\treturn err;\n}\n\nstatic void\n__mlx5_tc_ct_delete_flow(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t struct mlx5_flow_attr *attr)\n{\n\tmlx5e_tc_action_miss_mapping_put(ct_priv->priv, attr, attr->ct_attr.act_miss_mapping);\n\tmlx5_tc_ct_del_ft_cb(ct_priv, attr->ct_attr.ft);\n}\n\nvoid\nmlx5_tc_ct_delete_flow(struct mlx5_tc_ct_priv *priv,\n\t\t       struct mlx5_flow_attr *attr)\n{\n\tif (!attr->ct_attr.offloaded)  \n\t\treturn;\n\tif (!attr->ct_attr.nf_ft)  \n\t\treturn;\n\n\tmutex_lock(&priv->control_lock);\n\t__mlx5_tc_ct_delete_flow(priv, attr);\n\tmutex_unlock(&priv->control_lock);\n}\n\nstatic int\nmlx5_tc_ct_fs_init(struct mlx5_tc_ct_priv *ct_priv)\n{\n\tstruct mlx5_flow_table *post_ct = mlx5e_tc_post_act_get_ft(ct_priv->post_act);\n\tstruct mlx5_ct_fs_ops *fs_ops = mlx5_ct_fs_dmfs_ops_get();\n\tint err;\n\n\tif (ct_priv->ns_type == MLX5_FLOW_NAMESPACE_FDB &&\n\t    ct_priv->dev->priv.steering->mode == MLX5_FLOW_STEERING_MODE_SMFS) {\n\t\tct_dbg(\"Using SMFS ct flow steering provider\");\n\t\tfs_ops = mlx5_ct_fs_smfs_ops_get();\n\t}\n\n\tct_priv->fs = kzalloc(sizeof(*ct_priv->fs) + fs_ops->priv_size, GFP_KERNEL);\n\tif (!ct_priv->fs)\n\t\treturn -ENOMEM;\n\n\tct_priv->fs->netdev = ct_priv->netdev;\n\tct_priv->fs->dev = ct_priv->dev;\n\tct_priv->fs_ops = fs_ops;\n\n\terr = ct_priv->fs_ops->init(ct_priv->fs, ct_priv->ct, ct_priv->ct_nat, post_ct);\n\tif (err)\n\t\tgoto err_init;\n\n\treturn 0;\n\nerr_init:\n\tkfree(ct_priv->fs);\n\treturn err;\n}\n\nstatic int\nmlx5_tc_ct_init_check_esw_support(struct mlx5_eswitch *esw,\n\t\t\t\t  const char **err_msg)\n{\n\tif (!mlx5_eswitch_vlan_actions_supported(esw->dev, 1)) {\n\t\t \n\n\t\t*err_msg = \"firmware vlan actions support is missing\";\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!MLX5_CAP_ESW_FLOWTABLE(esw->dev,\n\t\t\t\t    fdb_modify_header_fwd_to_table)) {\n\t\t \n\n\t\t*err_msg = \"firmware fwd and modify support is missing\";\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!mlx5_eswitch_reg_c1_loopback_enabled(esw)) {\n\t\t*err_msg = \"register loopback isn't supported\";\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nmlx5_tc_ct_init_check_support(struct mlx5e_priv *priv,\n\t\t\t      enum mlx5_flow_namespace_type ns_type,\n\t\t\t      struct mlx5e_post_act *post_act)\n{\n\tstruct mlx5_eswitch *esw = priv->mdev->priv.eswitch;\n\tconst char *err_msg = NULL;\n\tint err = 0;\n\n\tif (IS_ERR_OR_NULL(post_act)) {\n\t\t \n\t\tif (priv->mdev->coredev_type == MLX5_COREDEV_PF)\n\t\t\terr_msg = \"post action is missing\";\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out_err;\n\t}\n\n\tif (ns_type == MLX5_FLOW_NAMESPACE_FDB)\n\t\terr = mlx5_tc_ct_init_check_esw_support(esw, &err_msg);\n\nout_err:\n\tif (err && err_msg)\n\t\tnetdev_dbg(priv->netdev, \"tc ct offload not supported, %s\\n\", err_msg);\n\treturn err;\n}\n\nstatic void\nmlx5_ct_tc_create_dbgfs(struct mlx5_tc_ct_priv *ct_priv)\n{\n\tstruct mlx5_tc_ct_debugfs *ct_dbgfs = &ct_priv->debugfs;\n\n\tct_dbgfs->root = debugfs_create_dir(\"ct\", mlx5_debugfs_get_dev_root(ct_priv->dev));\n\tdebugfs_create_atomic_t(\"offloaded\", 0400, ct_dbgfs->root,\n\t\t\t\t&ct_dbgfs->stats.offloaded);\n\tdebugfs_create_atomic_t(\"rx_dropped\", 0400, ct_dbgfs->root,\n\t\t\t\t&ct_dbgfs->stats.rx_dropped);\n}\n\nstatic void\nmlx5_ct_tc_remove_dbgfs(struct mlx5_tc_ct_priv *ct_priv)\n{\n\tdebugfs_remove_recursive(ct_priv->debugfs.root);\n}\n\n#define INIT_ERR_PREFIX \"tc ct offload init failed\"\n\nstruct mlx5_tc_ct_priv *\nmlx5_tc_ct_init(struct mlx5e_priv *priv, struct mlx5_fs_chains *chains,\n\t\tstruct mod_hdr_tbl *mod_hdr,\n\t\tenum mlx5_flow_namespace_type ns_type,\n\t\tstruct mlx5e_post_act *post_act)\n{\n\tstruct mlx5_tc_ct_priv *ct_priv;\n\tstruct mlx5_core_dev *dev;\n\tu64 mapping_id;\n\tint err;\n\n\tdev = priv->mdev;\n\terr = mlx5_tc_ct_init_check_support(priv, ns_type, post_act);\n\tif (err)\n\t\tgoto err_support;\n\n\tct_priv = kzalloc(sizeof(*ct_priv), GFP_KERNEL);\n\tif (!ct_priv)\n\t\tgoto err_alloc;\n\n\tmapping_id = mlx5_query_nic_system_image_guid(dev);\n\n\tct_priv->zone_mapping = mapping_create_for_id(mapping_id, MAPPING_TYPE_ZONE,\n\t\t\t\t\t\t      sizeof(u16), 0, true);\n\tif (IS_ERR(ct_priv->zone_mapping)) {\n\t\terr = PTR_ERR(ct_priv->zone_mapping);\n\t\tgoto err_mapping_zone;\n\t}\n\n\tct_priv->labels_mapping = mapping_create_for_id(mapping_id, MAPPING_TYPE_LABELS,\n\t\t\t\t\t\t\tsizeof(u32) * 4, 0, true);\n\tif (IS_ERR(ct_priv->labels_mapping)) {\n\t\terr = PTR_ERR(ct_priv->labels_mapping);\n\t\tgoto err_mapping_labels;\n\t}\n\n\tspin_lock_init(&ct_priv->ht_lock);\n\tct_priv->priv = priv;\n\tct_priv->ns_type = ns_type;\n\tct_priv->chains = chains;\n\tct_priv->netdev = priv->netdev;\n\tct_priv->dev = priv->mdev;\n\tct_priv->mod_hdr_tbl = mod_hdr;\n\tct_priv->ct = mlx5_chains_create_global_table(chains);\n\tif (IS_ERR(ct_priv->ct)) {\n\t\terr = PTR_ERR(ct_priv->ct);\n\t\tmlx5_core_warn(dev,\n\t\t\t       \"%s, failed to create ct table err: %d\\n\",\n\t\t\t       INIT_ERR_PREFIX, err);\n\t\tgoto err_ct_tbl;\n\t}\n\n\tct_priv->ct_nat = mlx5_chains_create_global_table(chains);\n\tif (IS_ERR(ct_priv->ct_nat)) {\n\t\terr = PTR_ERR(ct_priv->ct_nat);\n\t\tmlx5_core_warn(dev,\n\t\t\t       \"%s, failed to create ct nat table err: %d\\n\",\n\t\t\t       INIT_ERR_PREFIX, err);\n\t\tgoto err_ct_nat_tbl;\n\t}\n\n\tct_priv->post_act = post_act;\n\tmutex_init(&ct_priv->control_lock);\n\tif (rhashtable_init(&ct_priv->zone_ht, &zone_params))\n\t\tgoto err_ct_zone_ht;\n\tif (rhashtable_init(&ct_priv->ct_tuples_ht, &tuples_ht_params))\n\t\tgoto err_ct_tuples_ht;\n\tif (rhashtable_init(&ct_priv->ct_tuples_nat_ht, &tuples_nat_ht_params))\n\t\tgoto err_ct_tuples_nat_ht;\n\n\tct_priv->wq = alloc_ordered_workqueue(\"mlx5e_ct_priv_wq\", 0);\n\tif (!ct_priv->wq) {\n\t\terr = -ENOMEM;\n\t\tgoto err_wq;\n\t}\n\n\terr = mlx5_tc_ct_fs_init(ct_priv);\n\tif (err)\n\t\tgoto err_init_fs;\n\n\tmlx5_ct_tc_create_dbgfs(ct_priv);\n\treturn ct_priv;\n\nerr_init_fs:\n\tdestroy_workqueue(ct_priv->wq);\nerr_wq:\n\trhashtable_destroy(&ct_priv->ct_tuples_nat_ht);\nerr_ct_tuples_nat_ht:\n\trhashtable_destroy(&ct_priv->ct_tuples_ht);\nerr_ct_tuples_ht:\n\trhashtable_destroy(&ct_priv->zone_ht);\nerr_ct_zone_ht:\n\tmlx5_chains_destroy_global_table(chains, ct_priv->ct_nat);\nerr_ct_nat_tbl:\n\tmlx5_chains_destroy_global_table(chains, ct_priv->ct);\nerr_ct_tbl:\n\tmapping_destroy(ct_priv->labels_mapping);\nerr_mapping_labels:\n\tmapping_destroy(ct_priv->zone_mapping);\nerr_mapping_zone:\n\tkfree(ct_priv);\nerr_alloc:\nerr_support:\n\n\treturn NULL;\n}\n\nvoid\nmlx5_tc_ct_clean(struct mlx5_tc_ct_priv *ct_priv)\n{\n\tstruct mlx5_fs_chains *chains;\n\n\tif (!ct_priv)\n\t\treturn;\n\n\tdestroy_workqueue(ct_priv->wq);\n\tmlx5_ct_tc_remove_dbgfs(ct_priv);\n\tchains = ct_priv->chains;\n\n\tct_priv->fs_ops->destroy(ct_priv->fs);\n\tkfree(ct_priv->fs);\n\n\tmlx5_chains_destroy_global_table(chains, ct_priv->ct_nat);\n\tmlx5_chains_destroy_global_table(chains, ct_priv->ct);\n\tmapping_destroy(ct_priv->zone_mapping);\n\tmapping_destroy(ct_priv->labels_mapping);\n\n\trhashtable_destroy(&ct_priv->ct_tuples_ht);\n\trhashtable_destroy(&ct_priv->ct_tuples_nat_ht);\n\trhashtable_destroy(&ct_priv->zone_ht);\n\tmutex_destroy(&ct_priv->control_lock);\n\tkfree(ct_priv);\n}\n\nbool\nmlx5e_tc_ct_restore_flow(struct mlx5_tc_ct_priv *ct_priv,\n\t\t\t struct sk_buff *skb, u8 zone_restore_id)\n{\n\tstruct mlx5_ct_tuple tuple = {};\n\tstruct mlx5_ct_entry *entry;\n\tu16 zone;\n\n\tif (!ct_priv || !zone_restore_id)\n\t\treturn true;\n\n\tif (mapping_find(ct_priv->zone_mapping, zone_restore_id, &zone))\n\t\tgoto out_inc_drop;\n\n\tif (!mlx5_tc_ct_skb_to_tuple(skb, &tuple, zone))\n\t\tgoto out_inc_drop;\n\n\tspin_lock(&ct_priv->ht_lock);\n\n\tentry = mlx5_tc_ct_entry_get(ct_priv, &tuple);\n\tif (!entry) {\n\t\tspin_unlock(&ct_priv->ht_lock);\n\t\tgoto out_inc_drop;\n\t}\n\n\tif (IS_ERR(entry)) {\n\t\tspin_unlock(&ct_priv->ht_lock);\n\t\tgoto out_inc_drop;\n\t}\n\tspin_unlock(&ct_priv->ht_lock);\n\n\ttcf_ct_flow_table_restore_skb(skb, entry->restore_cookie);\n\t__mlx5_tc_ct_entry_put(entry);\n\n\treturn true;\n\nout_inc_drop:\n\tatomic_inc(&ct_priv->debugfs.stats.rx_dropped);\n\treturn false;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}