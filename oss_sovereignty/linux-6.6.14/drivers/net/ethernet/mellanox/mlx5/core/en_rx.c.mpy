{
  "module_name": "en_rx.c",
  "hash_id": "145cfb28178cfcbe022e3a6c2a5c92506cb46ae9134fac77857ee6d496899f2a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c",
  "human_readable_source": " \n\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/tcp.h>\n#include <linux/bitmap.h>\n#include <linux/filter.h>\n#include <net/ip6_checksum.h>\n#include <net/page_pool/helpers.h>\n#include <net/inet_ecn.h>\n#include <net/gro.h>\n#include <net/udp.h>\n#include <net/tcp.h>\n#include <net/xdp_sock_drv.h>\n#include \"en.h\"\n#include \"en/txrx.h\"\n#include \"en_tc.h\"\n#include \"eswitch.h\"\n#include \"en_rep.h\"\n#include \"en/rep/tc.h\"\n#include \"ipoib/ipoib.h\"\n#include \"en_accel/ipsec.h\"\n#include \"en_accel/macsec.h\"\n#include \"en_accel/ipsec_rxtx.h\"\n#include \"en_accel/ktls_txrx.h\"\n#include \"en/xdp.h\"\n#include \"en/xsk/rx.h\"\n#include \"en/health.h\"\n#include \"en/params.h\"\n#include \"devlink.h\"\n#include \"en/devlink.h\"\n\nstatic struct sk_buff *\nmlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,\n\t\t\t\tstruct mlx5_cqe64 *cqe, u16 cqe_bcnt, u32 head_offset,\n\t\t\t\tu32 page_idx);\nstatic struct sk_buff *\nmlx5e_skb_from_cqe_mpwrq_nonlinear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,\n\t\t\t\t   struct mlx5_cqe64 *cqe, u16 cqe_bcnt, u32 head_offset,\n\t\t\t\t   u32 page_idx);\nstatic void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);\nstatic void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);\nstatic void mlx5e_handle_rx_cqe_mpwrq_shampo(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);\n\nconst struct mlx5e_rx_handlers mlx5e_rx_handlers_nic = {\n\t.handle_rx_cqe       = mlx5e_handle_rx_cqe,\n\t.handle_rx_cqe_mpwqe = mlx5e_handle_rx_cqe_mpwrq,\n\t.handle_rx_cqe_mpwqe_shampo = mlx5e_handle_rx_cqe_mpwrq_shampo,\n};\n\nstatic inline void mlx5e_read_cqe_slot(struct mlx5_cqwq *wq,\n\t\t\t\t       u32 cqcc, void *data)\n{\n\tu32 ci = mlx5_cqwq_ctr2ix(wq, cqcc);\n\n\tmemcpy(data, mlx5_cqwq_get_wqe(wq, ci), sizeof(struct mlx5_cqe64));\n}\n\nstatic void mlx5e_read_enhanced_title_slot(struct mlx5e_rq *rq,\n\t\t\t\t\t   struct mlx5_cqe64 *cqe)\n{\n\tstruct mlx5e_cq_decomp *cqd = &rq->cqd;\n\tstruct mlx5_cqe64 *title = &cqd->title;\n\n\tmemcpy(title, cqe, sizeof(struct mlx5_cqe64));\n\n\tif (likely(test_bit(MLX5E_RQ_STATE_MINI_CQE_HW_STRIDX, &rq->state)))\n\t\treturn;\n\n\tif (rq->wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ)\n\t\tcqd->wqe_counter = mpwrq_get_cqe_stride_index(title) +\n\t\t\tmpwrq_get_cqe_consumed_strides(title);\n\telse\n\t\tcqd->wqe_counter =\n\t\t\tmlx5_wq_cyc_ctr2ix(&rq->wqe.wq, be16_to_cpu(title->wqe_counter) + 1);\n}\n\nstatic inline void mlx5e_read_title_slot(struct mlx5e_rq *rq,\n\t\t\t\t\t struct mlx5_cqwq *wq,\n\t\t\t\t\t u32 cqcc)\n{\n\tstruct mlx5e_cq_decomp *cqd = &rq->cqd;\n\tstruct mlx5_cqe64 *title = &cqd->title;\n\n\tmlx5e_read_cqe_slot(wq, cqcc, title);\n\tcqd->left        = be32_to_cpu(title->byte_cnt);\n\tcqd->wqe_counter = be16_to_cpu(title->wqe_counter);\n\trq->stats->cqe_compress_blks++;\n}\n\nstatic inline void mlx5e_read_mini_arr_slot(struct mlx5_cqwq *wq,\n\t\t\t\t\t    struct mlx5e_cq_decomp *cqd,\n\t\t\t\t\t    u32 cqcc)\n{\n\tmlx5e_read_cqe_slot(wq, cqcc, cqd->mini_arr);\n\tcqd->mini_arr_idx = 0;\n}\n\nstatic inline void mlx5e_cqes_update_owner(struct mlx5_cqwq *wq, int n)\n{\n\tu32 cqcc   = wq->cc;\n\tu8  op_own = mlx5_cqwq_get_ctr_wrap_cnt(wq, cqcc) & 1;\n\tu32 ci     = mlx5_cqwq_ctr2ix(wq, cqcc);\n\tu32 wq_sz  = mlx5_cqwq_get_size(wq);\n\tu32 ci_top = min_t(u32, wq_sz, ci + n);\n\n\tfor (; ci < ci_top; ci++, n--) {\n\t\tstruct mlx5_cqe64 *cqe = mlx5_cqwq_get_wqe(wq, ci);\n\n\t\tcqe->op_own = op_own;\n\t}\n\n\tif (unlikely(ci == wq_sz)) {\n\t\top_own = !op_own;\n\t\tfor (ci = 0; ci < n; ci++) {\n\t\t\tstruct mlx5_cqe64 *cqe = mlx5_cqwq_get_wqe(wq, ci);\n\n\t\t\tcqe->op_own = op_own;\n\t\t}\n\t}\n}\n\nstatic inline void mlx5e_decompress_cqe(struct mlx5e_rq *rq,\n\t\t\t\t\tstruct mlx5_cqwq *wq,\n\t\t\t\t\tu32 cqcc)\n{\n\tstruct mlx5e_cq_decomp *cqd = &rq->cqd;\n\tstruct mlx5_mini_cqe8 *mini_cqe = &cqd->mini_arr[cqd->mini_arr_idx];\n\tstruct mlx5_cqe64 *title = &cqd->title;\n\n\ttitle->byte_cnt     = mini_cqe->byte_cnt;\n\ttitle->check_sum    = mini_cqe->checksum;\n\ttitle->op_own      &= 0xf0;\n\ttitle->op_own      |= 0x01 & (cqcc >> wq->fbc.log_sz);\n\n\t \n\tif (test_bit(MLX5E_RQ_STATE_MINI_CQE_HW_STRIDX, &rq->state)) {\n\t\ttitle->wqe_counter = mini_cqe->stridx;\n\t\treturn;\n\t}\n\n\t \n\ttitle->wqe_counter = cpu_to_be16(cqd->wqe_counter);\n\tif (rq->wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ)\n\t\tcqd->wqe_counter += mpwrq_get_cqe_consumed_strides(title);\n\telse\n\t\tcqd->wqe_counter =\n\t\t\tmlx5_wq_cyc_ctr2ix(&rq->wqe.wq, cqd->wqe_counter + 1);\n}\n\nstatic inline void mlx5e_decompress_cqe_no_hash(struct mlx5e_rq *rq,\n\t\t\t\t\t\tstruct mlx5_cqwq *wq,\n\t\t\t\t\t\tu32 cqcc)\n{\n\tstruct mlx5e_cq_decomp *cqd = &rq->cqd;\n\n\tmlx5e_decompress_cqe(rq, wq, cqcc);\n\tcqd->title.rss_hash_type   = 0;\n\tcqd->title.rss_hash_result = 0;\n}\n\nstatic u32 mlx5e_decompress_enhanced_cqe(struct mlx5e_rq *rq,\n\t\t\t\t\t struct mlx5_cqwq *wq,\n\t\t\t\t\t struct mlx5_cqe64 *cqe,\n\t\t\t\t\t int budget_rem)\n{\n\tstruct mlx5e_cq_decomp *cqd = &rq->cqd;\n\tu32 cqcc, left;\n\tu32 i;\n\n\tleft = get_cqe_enhanced_num_mini_cqes(cqe);\n\t \n\tif (unlikely(left > budget_rem))\n\t\treturn budget_rem;\n\n\tcqcc = wq->cc;\n\tcqd->mini_arr_idx = 0;\n\tmemcpy(cqd->mini_arr, cqe, sizeof(struct mlx5_cqe64));\n\tfor (i = 0; i < left; i++, cqd->mini_arr_idx++, cqcc++) {\n\t\tmlx5e_decompress_cqe_no_hash(rq, wq, cqcc);\n\t\tINDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,\n\t\t\t\tmlx5e_handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq_shampo,\n\t\t\t\trq, &cqd->title);\n\t}\n\twq->cc = cqcc;\n\trq->stats->cqe_compress_pkts += left;\n\n\treturn left;\n}\n\nstatic inline u32 mlx5e_decompress_cqes_cont(struct mlx5e_rq *rq,\n\t\t\t\t\t     struct mlx5_cqwq *wq,\n\t\t\t\t\t     int update_owner_only,\n\t\t\t\t\t     int budget_rem)\n{\n\tstruct mlx5e_cq_decomp *cqd = &rq->cqd;\n\tu32 cqcc = wq->cc + update_owner_only;\n\tu32 cqe_count;\n\tu32 i;\n\n\tcqe_count = min_t(u32, cqd->left, budget_rem);\n\n\tfor (i = update_owner_only; i < cqe_count;\n\t     i++, cqd->mini_arr_idx++, cqcc++) {\n\t\tif (cqd->mini_arr_idx == MLX5_MINI_CQE_ARRAY_SIZE)\n\t\t\tmlx5e_read_mini_arr_slot(wq, cqd, cqcc);\n\n\t\tmlx5e_decompress_cqe_no_hash(rq, wq, cqcc);\n\t\tINDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,\n\t\t\t\tmlx5e_handle_rx_cqe_mpwrq_shampo, mlx5e_handle_rx_cqe,\n\t\t\t\trq, &cqd->title);\n\t}\n\tmlx5e_cqes_update_owner(wq, cqcc - wq->cc);\n\twq->cc = cqcc;\n\tcqd->left -= cqe_count;\n\trq->stats->cqe_compress_pkts += cqe_count;\n\n\treturn cqe_count;\n}\n\nstatic inline u32 mlx5e_decompress_cqes_start(struct mlx5e_rq *rq,\n\t\t\t\t\t      struct mlx5_cqwq *wq,\n\t\t\t\t\t      int budget_rem)\n{\n\tstruct mlx5e_cq_decomp *cqd = &rq->cqd;\n\tu32 cc = wq->cc;\n\n\tmlx5e_read_title_slot(rq, wq, cc);\n\tmlx5e_read_mini_arr_slot(wq, cqd, cc + 1);\n\tmlx5e_decompress_cqe(rq, wq, cc);\n\tINDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,\n\t\t\tmlx5e_handle_rx_cqe_mpwrq_shampo, mlx5e_handle_rx_cqe,\n\t\t\trq, &cqd->title);\n\tcqd->mini_arr_idx++;\n\n\treturn mlx5e_decompress_cqes_cont(rq, wq, 1, budget_rem);\n}\n\n#define MLX5E_PAGECNT_BIAS_MAX (PAGE_SIZE / 64)\n\nstatic int mlx5e_page_alloc_fragmented(struct mlx5e_rq *rq,\n\t\t\t\t       struct mlx5e_frag_page *frag_page)\n{\n\tstruct page *page;\n\n\tpage = page_pool_dev_alloc_pages(rq->page_pool);\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\n\tpage_pool_fragment_page(page, MLX5E_PAGECNT_BIAS_MAX);\n\n\t*frag_page = (struct mlx5e_frag_page) {\n\t\t.page\t= page,\n\t\t.frags\t= 0,\n\t};\n\n\treturn 0;\n}\n\nstatic void mlx5e_page_release_fragmented(struct mlx5e_rq *rq,\n\t\t\t\t\t  struct mlx5e_frag_page *frag_page)\n{\n\tu16 drain_count = MLX5E_PAGECNT_BIAS_MAX - frag_page->frags;\n\tstruct page *page = frag_page->page;\n\n\tif (page_pool_defrag_page(page, drain_count) == 0)\n\t\tpage_pool_put_defragged_page(rq->page_pool, page, -1, true);\n}\n\nstatic inline int mlx5e_get_rx_frag(struct mlx5e_rq *rq,\n\t\t\t\t    struct mlx5e_wqe_frag_info *frag)\n{\n\tint err = 0;\n\n\tif (!frag->offset)\n\t\t \n\t\terr = mlx5e_page_alloc_fragmented(rq, frag->frag_page);\n\n\treturn err;\n}\n\nstatic bool mlx5e_frag_can_release(struct mlx5e_wqe_frag_info *frag)\n{\n#define CAN_RELEASE_MASK \\\n\t(BIT(MLX5E_WQE_FRAG_LAST_IN_PAGE) | BIT(MLX5E_WQE_FRAG_SKIP_RELEASE))\n\n#define CAN_RELEASE_VALUE BIT(MLX5E_WQE_FRAG_LAST_IN_PAGE)\n\n\treturn (frag->flags & CAN_RELEASE_MASK) == CAN_RELEASE_VALUE;\n}\n\nstatic inline void mlx5e_put_rx_frag(struct mlx5e_rq *rq,\n\t\t\t\t     struct mlx5e_wqe_frag_info *frag)\n{\n\tif (mlx5e_frag_can_release(frag))\n\t\tmlx5e_page_release_fragmented(rq, frag->frag_page);\n}\n\nstatic inline struct mlx5e_wqe_frag_info *get_frag(struct mlx5e_rq *rq, u16 ix)\n{\n\treturn &rq->wqe.frags[ix << rq->wqe.info.log_num_frags];\n}\n\nstatic int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe_cyc *wqe,\n\t\t\t      u16 ix)\n{\n\tstruct mlx5e_wqe_frag_info *frag = get_frag(rq, ix);\n\tint err;\n\tint i;\n\n\tfor (i = 0; i < rq->wqe.info.num_frags; i++, frag++) {\n\t\tdma_addr_t addr;\n\t\tu16 headroom;\n\n\t\terr = mlx5e_get_rx_frag(rq, frag);\n\t\tif (unlikely(err))\n\t\t\tgoto free_frags;\n\n\t\tfrag->flags &= ~BIT(MLX5E_WQE_FRAG_SKIP_RELEASE);\n\n\t\theadroom = i == 0 ? rq->buff.headroom : 0;\n\t\taddr = page_pool_get_dma_addr(frag->frag_page->page);\n\t\twqe->data[i].addr = cpu_to_be64(addr + frag->offset + headroom);\n\t}\n\n\treturn 0;\n\nfree_frags:\n\twhile (--i >= 0)\n\t\tmlx5e_put_rx_frag(rq, --frag);\n\n\treturn err;\n}\n\nstatic inline void mlx5e_free_rx_wqe(struct mlx5e_rq *rq,\n\t\t\t\t     struct mlx5e_wqe_frag_info *wi)\n{\n\tint i;\n\n\tfor (i = 0; i < rq->wqe.info.num_frags; i++, wi++)\n\t\tmlx5e_put_rx_frag(rq, wi);\n}\n\nstatic void mlx5e_xsk_free_rx_wqe(struct mlx5e_wqe_frag_info *wi)\n{\n\tif (!(wi->flags & BIT(MLX5E_WQE_FRAG_SKIP_RELEASE)))\n\t\txsk_buff_free(*wi->xskp);\n}\n\nstatic void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix)\n{\n\tstruct mlx5e_wqe_frag_info *wi = get_frag(rq, ix);\n\n\tif (rq->xsk_pool) {\n\t\tmlx5e_xsk_free_rx_wqe(wi);\n\t} else {\n\t\tmlx5e_free_rx_wqe(rq, wi);\n\n\t\t \n\t\tfor (int i = 0; i < rq->wqe.info.num_frags; i++, wi++)\n\t\t\twi->flags |= BIT(MLX5E_WQE_FRAG_SKIP_RELEASE);\n\t}\n}\n\nstatic void mlx5e_xsk_free_rx_wqes(struct mlx5e_rq *rq, u16 ix, int wqe_bulk)\n{\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tint i;\n\n\tfor (i = 0; i < wqe_bulk; i++) {\n\t\tint j = mlx5_wq_cyc_ctr2ix(wq, ix + i);\n\t\tstruct mlx5e_wqe_frag_info *wi;\n\n\t\twi = get_frag(rq, j);\n\t\t \n\t\tmlx5e_xsk_free_rx_wqe(wi);\n\t}\n}\n\nstatic void mlx5e_free_rx_wqes(struct mlx5e_rq *rq, u16 ix, int wqe_bulk)\n{\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tint i;\n\n\tfor (i = 0; i < wqe_bulk; i++) {\n\t\tint j = mlx5_wq_cyc_ctr2ix(wq, ix + i);\n\t\tstruct mlx5e_wqe_frag_info *wi;\n\n\t\twi = get_frag(rq, j);\n\t\tmlx5e_free_rx_wqe(rq, wi);\n\t}\n}\n\nstatic int mlx5e_alloc_rx_wqes(struct mlx5e_rq *rq, u16 ix, int wqe_bulk)\n{\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tint i;\n\n\tfor (i = 0; i < wqe_bulk; i++) {\n\t\tint j = mlx5_wq_cyc_ctr2ix(wq, ix + i);\n\t\tstruct mlx5e_rx_wqe_cyc *wqe;\n\n\t\twqe = mlx5_wq_cyc_get_wqe(wq, j);\n\n\t\tif (unlikely(mlx5e_alloc_rx_wqe(rq, wqe, j)))\n\t\t\tbreak;\n\t}\n\n\treturn i;\n}\n\nstatic int mlx5e_refill_rx_wqes(struct mlx5e_rq *rq, u16 ix, int wqe_bulk)\n{\n\tint remaining = wqe_bulk;\n\tint total_alloc = 0;\n\tint refill_alloc;\n\tint refill;\n\n\t \n\tdo {\n\t\trefill = min_t(u16, rq->wqe.info.refill_unit, remaining);\n\n\t\tmlx5e_free_rx_wqes(rq, ix + total_alloc, refill);\n\t\trefill_alloc = mlx5e_alloc_rx_wqes(rq, ix + total_alloc, refill);\n\t\tif (unlikely(refill_alloc != refill))\n\t\t\tgoto err_free;\n\n\t\ttotal_alloc += refill_alloc;\n\t\tremaining -= refill;\n\t} while (remaining);\n\n\treturn total_alloc;\n\nerr_free:\n\tmlx5e_free_rx_wqes(rq, ix, total_alloc + refill_alloc);\n\n\tfor (int i = 0; i < total_alloc + refill; i++) {\n\t\tint j = mlx5_wq_cyc_ctr2ix(&rq->wqe.wq, ix + i);\n\t\tstruct mlx5e_wqe_frag_info *frag;\n\n\t\tfrag = get_frag(rq, j);\n\t\tfor (int k = 0; k < rq->wqe.info.num_frags; k++, frag++)\n\t\t\tfrag->flags |= BIT(MLX5E_WQE_FRAG_SKIP_RELEASE);\n\t}\n\n\treturn 0;\n}\n\nstatic void\nmlx5e_add_skb_shared_info_frag(struct mlx5e_rq *rq, struct skb_shared_info *sinfo,\n\t\t\t       struct xdp_buff *xdp, struct mlx5e_frag_page *frag_page,\n\t\t\t       u32 frag_offset, u32 len)\n{\n\tskb_frag_t *frag;\n\n\tdma_addr_t addr = page_pool_get_dma_addr(frag_page->page);\n\n\tdma_sync_single_for_cpu(rq->pdev, addr + frag_offset, len, rq->buff.map_dir);\n\tif (!xdp_buff_has_frags(xdp)) {\n\t\t \n\t\tsinfo->nr_frags = 0;\n\t\tsinfo->xdp_frags_size = 0;\n\t\txdp_buff_set_frags_flag(xdp);\n\t}\n\n\tfrag = &sinfo->frags[sinfo->nr_frags++];\n\tskb_frag_fill_page_desc(frag, frag_page->page, frag_offset, len);\n\n\tif (page_is_pfmemalloc(frag_page->page))\n\t\txdp_buff_set_frag_pfmemalloc(xdp);\n\tsinfo->xdp_frags_size += len;\n}\n\nstatic inline void\nmlx5e_add_skb_frag(struct mlx5e_rq *rq, struct sk_buff *skb,\n\t\t   struct page *page, u32 frag_offset, u32 len,\n\t\t   unsigned int truesize)\n{\n\tdma_addr_t addr = page_pool_get_dma_addr(page);\n\n\tdma_sync_single_for_cpu(rq->pdev, addr + frag_offset, len,\n\t\t\t\trq->buff.map_dir);\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\n\t\t\tpage, frag_offset, len, truesize);\n}\n\nstatic inline void\nmlx5e_copy_skb_header(struct mlx5e_rq *rq, struct sk_buff *skb,\n\t\t      struct page *page, dma_addr_t addr,\n\t\t      int offset_from, int dma_offset, u32 headlen)\n{\n\tconst void *from = page_address(page) + offset_from;\n\t \n\tunsigned int len = ALIGN(headlen, sizeof(long));\n\n\tdma_sync_single_for_cpu(rq->pdev, addr + dma_offset, len,\n\t\t\t\trq->buff.map_dir);\n\tskb_copy_to_linear_data(skb, from, len);\n}\n\nstatic void\nmlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi)\n{\n\tbool no_xdp_xmit;\n\tint i;\n\n\t \n\tif (bitmap_full(wi->skip_release_bitmap, rq->mpwqe.pages_per_wqe))\n\t\treturn;\n\n\tno_xdp_xmit = bitmap_empty(wi->skip_release_bitmap, rq->mpwqe.pages_per_wqe);\n\n\tif (rq->xsk_pool) {\n\t\tstruct xdp_buff **xsk_buffs = wi->alloc_units.xsk_buffs;\n\n\t\t \n\t\tfor (i = 0; i < rq->mpwqe.pages_per_wqe; i++)\n\t\t\tif (no_xdp_xmit || !test_bit(i, wi->skip_release_bitmap))\n\t\t\t\txsk_buff_free(xsk_buffs[i]);\n\t} else {\n\t\tfor (i = 0; i < rq->mpwqe.pages_per_wqe; i++) {\n\t\t\tif (no_xdp_xmit || !test_bit(i, wi->skip_release_bitmap)) {\n\t\t\t\tstruct mlx5e_frag_page *frag_page;\n\n\t\t\t\tfrag_page = &wi->alloc_units.frag_pages[i];\n\t\t\t\tmlx5e_page_release_fragmented(rq, frag_page);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq, u8 n)\n{\n\tstruct mlx5_wq_ll *wq = &rq->mpwqe.wq;\n\n\tdo {\n\t\tu16 next_wqe_index = mlx5_wq_ll_get_wqe_next_ix(wq, wq->head);\n\n\t\tmlx5_wq_ll_push(wq, next_wqe_index);\n\t} while (--n);\n\n\t \n\tdma_wmb();\n\n\tmlx5_wq_ll_update_db_record(wq);\n}\n\n \nstatic int bitmap_find_window(unsigned long *bitmap, int len,\n\t\t\t      int bitmap_size, int first)\n{\n\tint next_one, count;\n\n\tnext_one = find_next_bit(bitmap, bitmap_size, first);\n\tif (next_one == bitmap_size) {\n\t\tif (bitmap_size - first >= len)\n\t\t\treturn len;\n\t\tnext_one = find_next_bit(bitmap, bitmap_size, 0);\n\t\tcount = next_one + bitmap_size - first;\n\t} else {\n\t\tcount = next_one - first;\n\t}\n\n\treturn min(len, count);\n}\n\nstatic void build_klm_umr(struct mlx5e_icosq *sq, struct mlx5e_umr_wqe *umr_wqe,\n\t\t\t  __be32 key, u16 offset, u16 klm_len, u16 wqe_bbs)\n{\n\tmemset(umr_wqe, 0, offsetof(struct mlx5e_umr_wqe, inline_klms));\n\tumr_wqe->ctrl.opmod_idx_opcode =\n\t\tcpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |\n\t\t\t     MLX5_OPCODE_UMR);\n\tumr_wqe->ctrl.umr_mkey = key;\n\tumr_wqe->ctrl.qpn_ds = cpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT)\n\t\t\t\t\t    | MLX5E_KLM_UMR_DS_CNT(klm_len));\n\tumr_wqe->uctrl.flags = MLX5_UMR_TRANSLATION_OFFSET_EN | MLX5_UMR_INLINE;\n\tumr_wqe->uctrl.xlt_offset = cpu_to_be16(offset);\n\tumr_wqe->uctrl.xlt_octowords = cpu_to_be16(klm_len);\n\tumr_wqe->uctrl.mkey_mask     = cpu_to_be64(MLX5_MKEY_MASK_FREE);\n}\n\nstatic int mlx5e_build_shampo_hd_umr(struct mlx5e_rq *rq,\n\t\t\t\t     struct mlx5e_icosq *sq,\n\t\t\t\t     u16 klm_entries, u16 index)\n{\n\tstruct mlx5e_shampo_hd *shampo = rq->mpwqe.shampo;\n\tu16 entries, pi, header_offset, err, wqe_bbs, new_entries;\n\tu32 lkey = rq->mdev->mlx5e_res.hw_objs.mkey;\n\tu16 page_index = shampo->curr_page_index;\n\tstruct mlx5e_frag_page *frag_page;\n\tu64 addr = shampo->last_addr;\n\tstruct mlx5e_dma_info *dma_info;\n\tstruct mlx5e_umr_wqe *umr_wqe;\n\tint headroom, i;\n\n\theadroom = rq->buff.headroom;\n\tnew_entries = klm_entries - (shampo->pi & (MLX5_UMR_KLM_NUM_ENTRIES_ALIGNMENT - 1));\n\tentries = ALIGN(klm_entries, MLX5_UMR_KLM_NUM_ENTRIES_ALIGNMENT);\n\twqe_bbs = MLX5E_KLM_UMR_WQEBBS(entries);\n\tpi = mlx5e_icosq_get_next_pi(sq, wqe_bbs);\n\tumr_wqe = mlx5_wq_cyc_get_wqe(&sq->wq, pi);\n\tbuild_klm_umr(sq, umr_wqe, shampo->key, index, entries, wqe_bbs);\n\n\tfrag_page = &shampo->pages[page_index];\n\n\tfor (i = 0; i < entries; i++, index++) {\n\t\tdma_info = &shampo->info[index];\n\t\tif (i >= klm_entries || (index < shampo->pi && shampo->pi - index <\n\t\t\t\t\t MLX5_UMR_KLM_NUM_ENTRIES_ALIGNMENT))\n\t\t\tgoto update_klm;\n\t\theader_offset = (index & (MLX5E_SHAMPO_WQ_HEADER_PER_PAGE - 1)) <<\n\t\t\tMLX5E_SHAMPO_LOG_MAX_HEADER_ENTRY_SIZE;\n\t\tif (!(header_offset & (PAGE_SIZE - 1))) {\n\t\t\tpage_index = (page_index + 1) & (shampo->hd_per_wq - 1);\n\t\t\tfrag_page = &shampo->pages[page_index];\n\n\t\t\terr = mlx5e_page_alloc_fragmented(rq, frag_page);\n\t\t\tif (unlikely(err))\n\t\t\t\tgoto err_unmap;\n\n\t\t\taddr = page_pool_get_dma_addr(frag_page->page);\n\n\t\t\tdma_info->addr = addr;\n\t\t\tdma_info->frag_page = frag_page;\n\t\t} else {\n\t\t\tdma_info->addr = addr + header_offset;\n\t\t\tdma_info->frag_page = frag_page;\n\t\t}\n\nupdate_klm:\n\t\tumr_wqe->inline_klms[i].bcount =\n\t\t\tcpu_to_be32(MLX5E_RX_MAX_HEAD);\n\t\tumr_wqe->inline_klms[i].key    = cpu_to_be32(lkey);\n\t\tumr_wqe->inline_klms[i].va     =\n\t\t\tcpu_to_be64(dma_info->addr + headroom);\n\t}\n\n\tsq->db.wqe_info[pi] = (struct mlx5e_icosq_wqe_info) {\n\t\t.wqe_type\t= MLX5E_ICOSQ_WQE_SHAMPO_HD_UMR,\n\t\t.num_wqebbs\t= wqe_bbs,\n\t\t.shampo.len\t= new_entries,\n\t};\n\n\tshampo->pi = (shampo->pi + new_entries) & (shampo->hd_per_wq - 1);\n\tshampo->curr_page_index = page_index;\n\tshampo->last_addr = addr;\n\tsq->pc += wqe_bbs;\n\tsq->doorbell_cseg = &umr_wqe->ctrl;\n\n\treturn 0;\n\nerr_unmap:\n\twhile (--i >= 0) {\n\t\tdma_info = &shampo->info[--index];\n\t\tif (!(i & (MLX5E_SHAMPO_WQ_HEADER_PER_PAGE - 1))) {\n\t\t\tdma_info->addr = ALIGN_DOWN(dma_info->addr, PAGE_SIZE);\n\t\t\tmlx5e_page_release_fragmented(rq, dma_info->frag_page);\n\t\t}\n\t}\n\trq->stats->buff_alloc_err++;\n\treturn err;\n}\n\nstatic int mlx5e_alloc_rx_hd_mpwqe(struct mlx5e_rq *rq)\n{\n\tstruct mlx5e_shampo_hd *shampo = rq->mpwqe.shampo;\n\tu16 klm_entries, num_wqe, index, entries_before;\n\tstruct mlx5e_icosq *sq = rq->icosq;\n\tint i, err, max_klm_entries, len;\n\n\tmax_klm_entries = MLX5E_MAX_KLM_PER_WQE(rq->mdev);\n\tklm_entries = bitmap_find_window(shampo->bitmap,\n\t\t\t\t\t shampo->hd_per_wqe,\n\t\t\t\t\t shampo->hd_per_wq, shampo->pi);\n\tif (!klm_entries)\n\t\treturn 0;\n\n\tklm_entries += (shampo->pi & (MLX5_UMR_KLM_NUM_ENTRIES_ALIGNMENT - 1));\n\tindex = ALIGN_DOWN(shampo->pi, MLX5_UMR_KLM_NUM_ENTRIES_ALIGNMENT);\n\tentries_before = shampo->hd_per_wq - index;\n\n\tif (unlikely(entries_before < klm_entries))\n\t\tnum_wqe = DIV_ROUND_UP(entries_before, max_klm_entries) +\n\t\t\t  DIV_ROUND_UP(klm_entries - entries_before, max_klm_entries);\n\telse\n\t\tnum_wqe = DIV_ROUND_UP(klm_entries, max_klm_entries);\n\n\tfor (i = 0; i < num_wqe; i++) {\n\t\tlen = (klm_entries > max_klm_entries) ? max_klm_entries :\n\t\t\t\t\t\t\tklm_entries;\n\t\tif (unlikely(index + len > shampo->hd_per_wq))\n\t\t\tlen = shampo->hd_per_wq - index;\n\t\terr = mlx5e_build_shampo_hd_umr(rq, sq, len, index);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t\tindex = (index + len) & (rq->mpwqe.shampo->hd_per_wq - 1);\n\t\tklm_entries -= len;\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)\n{\n\tstruct mlx5e_mpw_info *wi = mlx5e_get_mpw_info(rq, ix);\n\tstruct mlx5e_icosq *sq = rq->icosq;\n\tstruct mlx5e_frag_page *frag_page;\n\tstruct mlx5_wq_cyc *wq = &sq->wq;\n\tstruct mlx5e_umr_wqe *umr_wqe;\n\tu32 offset;  \n\tu16 pi;\n\tint err;\n\tint i;\n\n\tif (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state)) {\n\t\terr = mlx5e_alloc_rx_hd_mpwqe(rq);\n\t\tif (unlikely(err))\n\t\t\tgoto err;\n\t}\n\n\tpi = mlx5e_icosq_get_next_pi(sq, rq->mpwqe.umr_wqebbs);\n\tumr_wqe = mlx5_wq_cyc_get_wqe(wq, pi);\n\tmemcpy(umr_wqe, &rq->mpwqe.umr_wqe, sizeof(struct mlx5e_umr_wqe));\n\n\tfrag_page = &wi->alloc_units.frag_pages[0];\n\n\tfor (i = 0; i < rq->mpwqe.pages_per_wqe; i++, frag_page++) {\n\t\tdma_addr_t addr;\n\n\t\terr = mlx5e_page_alloc_fragmented(rq, frag_page);\n\t\tif (unlikely(err))\n\t\t\tgoto err_unmap;\n\t\taddr = page_pool_get_dma_addr(frag_page->page);\n\t\tumr_wqe->inline_mtts[i] = (struct mlx5_mtt) {\n\t\t\t.ptag = cpu_to_be64(addr | MLX5_EN_WR),\n\t\t};\n\t}\n\n\t \n\tif (rq->mpwqe.pages_per_wqe & (MLX5_UMR_MTT_NUM_ENTRIES_ALIGNMENT - 1)) {\n\t\tint pad = ALIGN(rq->mpwqe.pages_per_wqe, MLX5_UMR_MTT_NUM_ENTRIES_ALIGNMENT) -\n\t\t\trq->mpwqe.pages_per_wqe;\n\n\t\tmemset(&umr_wqe->inline_mtts[rq->mpwqe.pages_per_wqe], 0,\n\t\t       sizeof(*umr_wqe->inline_mtts) * pad);\n\t}\n\n\tbitmap_zero(wi->skip_release_bitmap, rq->mpwqe.pages_per_wqe);\n\twi->consumed_strides = 0;\n\n\tumr_wqe->ctrl.opmod_idx_opcode =\n\t\tcpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |\n\t\t\t    MLX5_OPCODE_UMR);\n\n\toffset = (ix * rq->mpwqe.mtts_per_wqe) * sizeof(struct mlx5_mtt) / MLX5_OCTWORD;\n\tumr_wqe->uctrl.xlt_offset = cpu_to_be16(offset);\n\n\tsq->db.wqe_info[pi] = (struct mlx5e_icosq_wqe_info) {\n\t\t.wqe_type   = MLX5E_ICOSQ_WQE_UMR_RX,\n\t\t.num_wqebbs = rq->mpwqe.umr_wqebbs,\n\t\t.umr.rq     = rq,\n\t};\n\n\tsq->pc += rq->mpwqe.umr_wqebbs;\n\n\tsq->doorbell_cseg = &umr_wqe->ctrl;\n\n\treturn 0;\n\nerr_unmap:\n\twhile (--i >= 0) {\n\t\tfrag_page--;\n\t\tmlx5e_page_release_fragmented(rq, frag_page);\n\t}\n\n\tbitmap_fill(wi->skip_release_bitmap, rq->mpwqe.pages_per_wqe);\n\nerr:\n\trq->stats->buff_alloc_err++;\n\n\treturn err;\n}\n\n \nvoid mlx5e_shampo_dealloc_hd(struct mlx5e_rq *rq, u16 len, u16 start, bool close)\n{\n\tstruct mlx5e_shampo_hd *shampo = rq->mpwqe.shampo;\n\tstruct mlx5e_frag_page *deleted_page = NULL;\n\tint hd_per_wq = shampo->hd_per_wq;\n\tstruct mlx5e_dma_info *hd_info;\n\tint i, index = start;\n\n\tfor (i = 0; i < len; i++, index++) {\n\t\tif (index == hd_per_wq)\n\t\t\tindex = 0;\n\n\t\tif (close && !test_bit(index, shampo->bitmap))\n\t\t\tcontinue;\n\n\t\thd_info = &shampo->info[index];\n\t\thd_info->addr = ALIGN_DOWN(hd_info->addr, PAGE_SIZE);\n\t\tif (hd_info->frag_page && hd_info->frag_page != deleted_page) {\n\t\t\tdeleted_page = hd_info->frag_page;\n\t\t\tmlx5e_page_release_fragmented(rq, hd_info->frag_page);\n\t\t}\n\n\t\thd_info->frag_page = NULL;\n\t}\n\n\tif (start + len > hd_per_wq) {\n\t\tlen -= hd_per_wq - start;\n\t\tbitmap_clear(shampo->bitmap, start, hd_per_wq - start);\n\t\tstart = 0;\n\t}\n\n\tbitmap_clear(shampo->bitmap, start, len);\n}\n\nstatic void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)\n{\n\tstruct mlx5e_mpw_info *wi = mlx5e_get_mpw_info(rq, ix);\n\t \n\tmlx5e_free_rx_mpwqe(rq, wi);\n\n\t \n\tbitmap_fill(wi->skip_release_bitmap, rq->mpwqe.pages_per_wqe);\n}\n\nINDIRECT_CALLABLE_SCOPE bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq)\n{\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tint wqe_bulk, count;\n\tbool busy = false;\n\tu16 head;\n\n\tif (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))\n\t\treturn false;\n\n\tif (mlx5_wq_cyc_missing(wq) < rq->wqe.info.wqe_bulk)\n\t\treturn false;\n\n\tif (rq->page_pool)\n\t\tpage_pool_nid_changed(rq->page_pool, numa_mem_id());\n\n\twqe_bulk = mlx5_wq_cyc_missing(wq);\n\thead = mlx5_wq_cyc_get_head(wq);\n\n\t \n\twqe_bulk -= (head + wqe_bulk) & rq->wqe.info.wqe_index_mask;\n\n\tif (!rq->xsk_pool) {\n\t\tcount = mlx5e_refill_rx_wqes(rq, head, wqe_bulk);\n\t} else if (likely(!rq->xsk_pool->dma_need_sync)) {\n\t\tmlx5e_xsk_free_rx_wqes(rq, head, wqe_bulk);\n\t\tcount = mlx5e_xsk_alloc_rx_wqes_batched(rq, head, wqe_bulk);\n\t} else {\n\t\tmlx5e_xsk_free_rx_wqes(rq, head, wqe_bulk);\n\t\t \n\t\tcount = mlx5e_xsk_alloc_rx_wqes(rq, head, wqe_bulk);\n\t}\n\n\tmlx5_wq_cyc_push_n(wq, count);\n\tif (unlikely(count != wqe_bulk)) {\n\t\trq->stats->buff_alloc_err++;\n\t\tbusy = true;\n\t}\n\n\t \n\tdma_wmb();\n\n\tmlx5_wq_cyc_update_db_record(wq);\n\n\treturn busy;\n}\n\nvoid mlx5e_free_icosq_descs(struct mlx5e_icosq *sq)\n{\n\tu16 sqcc;\n\n\tsqcc = sq->cc;\n\n\twhile (sqcc != sq->pc) {\n\t\tstruct mlx5e_icosq_wqe_info *wi;\n\t\tu16 ci;\n\n\t\tci = mlx5_wq_cyc_ctr2ix(&sq->wq, sqcc);\n\t\twi = &sq->db.wqe_info[ci];\n\t\tsqcc += wi->num_wqebbs;\n#ifdef CONFIG_MLX5_EN_TLS\n\t\tswitch (wi->wqe_type) {\n\t\tcase MLX5E_ICOSQ_WQE_SET_PSV_TLS:\n\t\t\tmlx5e_ktls_handle_ctx_completion(wi);\n\t\t\tbreak;\n\t\tcase MLX5E_ICOSQ_WQE_GET_PSV_TLS:\n\t\t\tmlx5e_ktls_handle_get_psv_completion(wi, sq);\n\t\t\tbreak;\n\t\t}\n#endif\n\t}\n\tsq->cc = sqcc;\n}\n\nstatic void mlx5e_handle_shampo_hd_umr(struct mlx5e_shampo_umr umr,\n\t\t\t\t       struct mlx5e_icosq *sq)\n{\n\tstruct mlx5e_channel *c = container_of(sq, struct mlx5e_channel, icosq);\n\tstruct mlx5e_shampo_hd *shampo;\n\t \n\tstruct mlx5e_rq *rq = &c->rq;\n\tint end, from, len = umr.len;\n\n\tshampo = rq->mpwqe.shampo;\n\tend = shampo->hd_per_wq;\n\tfrom = shampo->ci;\n\tif (from + len > shampo->hd_per_wq) {\n\t\tlen -= end - from;\n\t\tbitmap_set(shampo->bitmap, from, end - from);\n\t\tfrom = 0;\n\t}\n\n\tbitmap_set(shampo->bitmap, from, len);\n\tshampo->ci = (shampo->ci + umr.len) & (shampo->hd_per_wq - 1);\n}\n\nint mlx5e_poll_ico_cq(struct mlx5e_cq *cq)\n{\n\tstruct mlx5e_icosq *sq = container_of(cq, struct mlx5e_icosq, cq);\n\tstruct mlx5_cqe64 *cqe;\n\tu16 sqcc;\n\tint i;\n\n\tif (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))\n\t\treturn 0;\n\n\tcqe = mlx5_cqwq_get_cqe(&cq->wq);\n\tif (likely(!cqe))\n\t\treturn 0;\n\n\t \n\tsqcc = sq->cc;\n\n\ti = 0;\n\tdo {\n\t\tu16 wqe_counter;\n\t\tbool last_wqe;\n\n\t\tmlx5_cqwq_pop(&cq->wq);\n\n\t\twqe_counter = be16_to_cpu(cqe->wqe_counter);\n\n\t\tdo {\n\t\t\tstruct mlx5e_icosq_wqe_info *wi;\n\t\t\tu16 ci;\n\n\t\t\tlast_wqe = (sqcc == wqe_counter);\n\n\t\t\tci = mlx5_wq_cyc_ctr2ix(&sq->wq, sqcc);\n\t\t\twi = &sq->db.wqe_info[ci];\n\t\t\tsqcc += wi->num_wqebbs;\n\n\t\t\tif (last_wqe && unlikely(get_cqe_opcode(cqe) != MLX5_CQE_REQ)) {\n\t\t\t\tnetdev_WARN_ONCE(cq->netdev,\n\t\t\t\t\t\t \"Bad OP in ICOSQ CQE: 0x%x\\n\",\n\t\t\t\t\t\t get_cqe_opcode(cqe));\n\t\t\t\tmlx5e_dump_error_cqe(&sq->cq, sq->sqn,\n\t\t\t\t\t\t     (struct mlx5_err_cqe *)cqe);\n\t\t\t\tmlx5_wq_cyc_wqe_dump(&sq->wq, ci, wi->num_wqebbs);\n\t\t\t\tif (!test_and_set_bit(MLX5E_SQ_STATE_RECOVERING, &sq->state))\n\t\t\t\t\tqueue_work(cq->priv->wq, &sq->recover_work);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (wi->wqe_type) {\n\t\t\tcase MLX5E_ICOSQ_WQE_UMR_RX:\n\t\t\t\twi->umr.rq->mpwqe.umr_completed++;\n\t\t\t\tbreak;\n\t\t\tcase MLX5E_ICOSQ_WQE_NOP:\n\t\t\t\tbreak;\n\t\t\tcase MLX5E_ICOSQ_WQE_SHAMPO_HD_UMR:\n\t\t\t\tmlx5e_handle_shampo_hd_umr(wi->shampo, sq);\n\t\t\t\tbreak;\n#ifdef CONFIG_MLX5_EN_TLS\n\t\t\tcase MLX5E_ICOSQ_WQE_UMR_TLS:\n\t\t\t\tbreak;\n\t\t\tcase MLX5E_ICOSQ_WQE_SET_PSV_TLS:\n\t\t\t\tmlx5e_ktls_handle_ctx_completion(wi);\n\t\t\t\tbreak;\n\t\t\tcase MLX5E_ICOSQ_WQE_GET_PSV_TLS:\n\t\t\t\tmlx5e_ktls_handle_get_psv_completion(wi, sq);\n\t\t\t\tbreak;\n#endif\n\t\t\tdefault:\n\t\t\t\tnetdev_WARN_ONCE(cq->netdev,\n\t\t\t\t\t\t \"Bad WQE type in ICOSQ WQE info: 0x%x\\n\",\n\t\t\t\t\t\t wi->wqe_type);\n\t\t\t}\n\t\t} while (!last_wqe);\n\t} while ((++i < MLX5E_TX_CQ_POLL_BUDGET) && (cqe = mlx5_cqwq_get_cqe(&cq->wq)));\n\n\tsq->cc = sqcc;\n\n\tmlx5_cqwq_update_db_record(&cq->wq);\n\n\treturn i;\n}\n\nINDIRECT_CALLABLE_SCOPE bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq)\n{\n\tstruct mlx5_wq_ll *wq = &rq->mpwqe.wq;\n\tu8  umr_completed = rq->mpwqe.umr_completed;\n\tstruct mlx5e_icosq *sq = rq->icosq;\n\tint alloc_err = 0;\n\tu8  missing, i;\n\tu16 head;\n\n\tif (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))\n\t\treturn false;\n\n\tif (umr_completed) {\n\t\tmlx5e_post_rx_mpwqe(rq, umr_completed);\n\t\trq->mpwqe.umr_in_progress -= umr_completed;\n\t\trq->mpwqe.umr_completed = 0;\n\t}\n\n\tmissing = mlx5_wq_ll_missing(wq) - rq->mpwqe.umr_in_progress;\n\n\tif (unlikely(rq->mpwqe.umr_in_progress > rq->mpwqe.umr_last_bulk))\n\t\trq->stats->congst_umr++;\n\n\tif (likely(missing < rq->mpwqe.min_wqe_bulk))\n\t\treturn false;\n\n\tif (rq->page_pool)\n\t\tpage_pool_nid_changed(rq->page_pool, numa_mem_id());\n\n\thead = rq->mpwqe.actual_wq_head;\n\ti = missing;\n\tdo {\n\t\tstruct mlx5e_mpw_info *wi = mlx5e_get_mpw_info(rq, head);\n\n\t\t \n\t\tmlx5e_free_rx_mpwqe(rq, wi);\n\n\t\talloc_err = rq->xsk_pool ? mlx5e_xsk_alloc_rx_mpwqe(rq, head) :\n\t\t\t\t\t   mlx5e_alloc_rx_mpwqe(rq, head);\n\n\t\tif (unlikely(alloc_err))\n\t\t\tbreak;\n\t\thead = mlx5_wq_ll_get_wqe_next_ix(wq, head);\n\t} while (--i);\n\n\trq->mpwqe.umr_last_bulk    = missing - i;\n\tif (sq->doorbell_cseg) {\n\t\tmlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, sq->doorbell_cseg);\n\t\tsq->doorbell_cseg = NULL;\n\t}\n\n\trq->mpwqe.umr_in_progress += rq->mpwqe.umr_last_bulk;\n\trq->mpwqe.actual_wq_head   = head;\n\n\t \n\tif (unlikely(alloc_err == -ENOMEM && rq->xsk_pool))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void mlx5e_lro_update_tcp_hdr(struct mlx5_cqe64 *cqe, struct tcphdr *tcp)\n{\n\tu8 l4_hdr_type = get_cqe_l4_hdr_type(cqe);\n\tu8 tcp_ack     = (l4_hdr_type == CQE_L4_HDR_TYPE_TCP_ACK_NO_DATA) ||\n\t\t\t (l4_hdr_type == CQE_L4_HDR_TYPE_TCP_ACK_AND_DATA);\n\n\ttcp->check                      = 0;\n\ttcp->psh                        = get_cqe_lro_tcppsh(cqe);\n\n\tif (tcp_ack) {\n\t\ttcp->ack                = 1;\n\t\ttcp->ack_seq            = cqe->lro.ack_seq_num;\n\t\ttcp->window             = cqe->lro.tcp_win;\n\t}\n}\n\nstatic void mlx5e_lro_update_hdr(struct sk_buff *skb, struct mlx5_cqe64 *cqe,\n\t\t\t\t u32 cqe_bcnt)\n{\n\tstruct ethhdr\t*eth = (struct ethhdr *)(skb->data);\n\tstruct tcphdr\t*tcp;\n\tint network_depth = 0;\n\t__wsum check;\n\t__be16 proto;\n\tu16 tot_len;\n\tvoid *ip_p;\n\n\tproto = __vlan_get_protocol(skb, eth->h_proto, &network_depth);\n\n\ttot_len = cqe_bcnt - network_depth;\n\tip_p = skb->data + network_depth;\n\n\tif (proto == htons(ETH_P_IP)) {\n\t\tstruct iphdr *ipv4 = ip_p;\n\n\t\ttcp = ip_p + sizeof(struct iphdr);\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\n\n\t\tipv4->ttl               = cqe->lro.min_ttl;\n\t\tipv4->tot_len           = cpu_to_be16(tot_len);\n\t\tipv4->check             = 0;\n\t\tipv4->check             = ip_fast_csum((unsigned char *)ipv4,\n\t\t\t\t\t\t       ipv4->ihl);\n\n\t\tmlx5e_lro_update_tcp_hdr(cqe, tcp);\n\t\tcheck = csum_partial(tcp, tcp->doff * 4,\n\t\t\t\t     csum_unfold((__force __sum16)cqe->check_sum));\n\t\t \n\t\ttcp->check = csum_tcpudp_magic(ipv4->saddr, ipv4->daddr,\n\t\t\t\t\t       tot_len - sizeof(struct iphdr),\n\t\t\t\t\t       IPPROTO_TCP, check);\n\t} else {\n\t\tu16 payload_len = tot_len - sizeof(struct ipv6hdr);\n\t\tstruct ipv6hdr *ipv6 = ip_p;\n\n\t\ttcp = ip_p + sizeof(struct ipv6hdr);\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\n\n\t\tipv6->hop_limit         = cqe->lro.min_ttl;\n\t\tipv6->payload_len       = cpu_to_be16(payload_len);\n\n\t\tmlx5e_lro_update_tcp_hdr(cqe, tcp);\n\t\tcheck = csum_partial(tcp, tcp->doff * 4,\n\t\t\t\t     csum_unfold((__force __sum16)cqe->check_sum));\n\t\t \n\t\ttcp->check = csum_ipv6_magic(&ipv6->saddr, &ipv6->daddr, payload_len,\n\t\t\t\t\t     IPPROTO_TCP, check);\n\t}\n}\n\nstatic void *mlx5e_shampo_get_packet_hd(struct mlx5e_rq *rq, u16 header_index)\n{\n\tstruct mlx5e_dma_info *last_head = &rq->mpwqe.shampo->info[header_index];\n\tu16 head_offset = (last_head->addr & (PAGE_SIZE - 1)) + rq->buff.headroom;\n\n\treturn page_address(last_head->frag_page->page) + head_offset;\n}\n\nstatic void mlx5e_shampo_update_ipv4_udp_hdr(struct mlx5e_rq *rq, struct iphdr *ipv4)\n{\n\tint udp_off = rq->hw_gro_data->fk.control.thoff;\n\tstruct sk_buff *skb = rq->hw_gro_data->skb;\n\tstruct udphdr *uh;\n\n\tuh = (struct udphdr *)(skb->data + udp_off);\n\tuh->len = htons(skb->len - udp_off);\n\n\tif (uh->check)\n\t\tuh->check = ~udp_v4_check(skb->len - udp_off, ipv4->saddr,\n\t\t\t\t\t  ipv4->daddr, 0);\n\n\tskb->csum_start = (unsigned char *)uh - skb->head;\n\tskb->csum_offset = offsetof(struct udphdr, check);\n\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_UDP_L4;\n}\n\nstatic void mlx5e_shampo_update_ipv6_udp_hdr(struct mlx5e_rq *rq, struct ipv6hdr *ipv6)\n{\n\tint udp_off = rq->hw_gro_data->fk.control.thoff;\n\tstruct sk_buff *skb = rq->hw_gro_data->skb;\n\tstruct udphdr *uh;\n\n\tuh = (struct udphdr *)(skb->data + udp_off);\n\tuh->len = htons(skb->len - udp_off);\n\n\tif (uh->check)\n\t\tuh->check = ~udp_v6_check(skb->len - udp_off, &ipv6->saddr,\n\t\t\t\t\t  &ipv6->daddr, 0);\n\n\tskb->csum_start = (unsigned char *)uh - skb->head;\n\tskb->csum_offset = offsetof(struct udphdr, check);\n\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_UDP_L4;\n}\n\nstatic void mlx5e_shampo_update_fin_psh_flags(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,\n\t\t\t\t\t      struct tcphdr *skb_tcp_hd)\n{\n\tu16 header_index = mlx5e_shampo_get_cqe_header_index(rq, cqe);\n\tstruct tcphdr *last_tcp_hd;\n\tvoid *last_hd_addr;\n\n\tlast_hd_addr = mlx5e_shampo_get_packet_hd(rq, header_index);\n\tlast_tcp_hd =  last_hd_addr + ETH_HLEN + rq->hw_gro_data->fk.control.thoff;\n\ttcp_flag_word(skb_tcp_hd) |= tcp_flag_word(last_tcp_hd) & (TCP_FLAG_FIN | TCP_FLAG_PSH);\n}\n\nstatic void mlx5e_shampo_update_ipv4_tcp_hdr(struct mlx5e_rq *rq, struct iphdr *ipv4,\n\t\t\t\t\t     struct mlx5_cqe64 *cqe, bool match)\n{\n\tint tcp_off = rq->hw_gro_data->fk.control.thoff;\n\tstruct sk_buff *skb = rq->hw_gro_data->skb;\n\tstruct tcphdr *tcp;\n\n\ttcp = (struct tcphdr *)(skb->data + tcp_off);\n\tif (match)\n\t\tmlx5e_shampo_update_fin_psh_flags(rq, cqe, tcp);\n\n\ttcp->check = ~tcp_v4_check(skb->len - tcp_off, ipv4->saddr,\n\t\t\t\t   ipv4->daddr, 0);\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCPV4;\n\tif (ntohs(ipv4->id) == rq->hw_gro_data->second_ip_id)\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCP_FIXEDID;\n\n\tskb->csum_start = (unsigned char *)tcp - skb->head;\n\tskb->csum_offset = offsetof(struct tcphdr, check);\n\n\tif (tcp->cwr)\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCP_ECN;\n}\n\nstatic void mlx5e_shampo_update_ipv6_tcp_hdr(struct mlx5e_rq *rq, struct ipv6hdr *ipv6,\n\t\t\t\t\t     struct mlx5_cqe64 *cqe, bool match)\n{\n\tint tcp_off = rq->hw_gro_data->fk.control.thoff;\n\tstruct sk_buff *skb = rq->hw_gro_data->skb;\n\tstruct tcphdr *tcp;\n\n\ttcp = (struct tcphdr *)(skb->data + tcp_off);\n\tif (match)\n\t\tmlx5e_shampo_update_fin_psh_flags(rq, cqe, tcp);\n\n\ttcp->check = ~tcp_v6_check(skb->len - tcp_off, &ipv6->saddr,\n\t\t\t\t   &ipv6->daddr, 0);\n\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCPV6;\n\tskb->csum_start = (unsigned char *)tcp - skb->head;\n\tskb->csum_offset = offsetof(struct tcphdr, check);\n\n\tif (tcp->cwr)\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_TCP_ECN;\n}\n\nstatic void mlx5e_shampo_update_hdr(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe, bool match)\n{\n\tbool is_ipv4 = (rq->hw_gro_data->fk.basic.n_proto == htons(ETH_P_IP));\n\tstruct sk_buff *skb = rq->hw_gro_data->skb;\n\n\tskb_shinfo(skb)->gso_segs = NAPI_GRO_CB(skb)->count;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\tif (is_ipv4) {\n\t\tint nhoff = rq->hw_gro_data->fk.control.thoff - sizeof(struct iphdr);\n\t\tstruct iphdr *ipv4 = (struct iphdr *)(skb->data + nhoff);\n\t\t__be16 newlen = htons(skb->len - nhoff);\n\n\t\tcsum_replace2(&ipv4->check, ipv4->tot_len, newlen);\n\t\tipv4->tot_len = newlen;\n\n\t\tif (ipv4->protocol == IPPROTO_TCP)\n\t\t\tmlx5e_shampo_update_ipv4_tcp_hdr(rq, ipv4, cqe, match);\n\t\telse\n\t\t\tmlx5e_shampo_update_ipv4_udp_hdr(rq, ipv4);\n\t} else {\n\t\tint nhoff = rq->hw_gro_data->fk.control.thoff - sizeof(struct ipv6hdr);\n\t\tstruct ipv6hdr *ipv6 = (struct ipv6hdr *)(skb->data + nhoff);\n\n\t\tipv6->payload_len = htons(skb->len - nhoff - sizeof(*ipv6));\n\n\t\tif (ipv6->nexthdr == IPPROTO_TCP)\n\t\t\tmlx5e_shampo_update_ipv6_tcp_hdr(rq, ipv6, cqe, match);\n\t\telse\n\t\t\tmlx5e_shampo_update_ipv6_udp_hdr(rq, ipv6);\n\t}\n}\n\nstatic inline void mlx5e_skb_set_hash(struct mlx5_cqe64 *cqe,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tu8 cht = cqe->rss_hash_type;\n\tint ht = (cht & CQE_RSS_HTYPE_L4) ? PKT_HASH_TYPE_L4 :\n\t\t (cht & CQE_RSS_HTYPE_IP) ? PKT_HASH_TYPE_L3 :\n\t\t\t\t\t    PKT_HASH_TYPE_NONE;\n\tskb_set_hash(skb, be32_to_cpu(cqe->rss_hash_result), ht);\n}\n\nstatic inline bool is_last_ethertype_ip(struct sk_buff *skb, int *network_depth,\n\t\t\t\t\t__be16 *proto)\n{\n\t*proto = ((struct ethhdr *)skb->data)->h_proto;\n\t*proto = __vlan_get_protocol(skb, *proto, network_depth);\n\n\tif (*proto == htons(ETH_P_IP))\n\t\treturn pskb_may_pull(skb, *network_depth + sizeof(struct iphdr));\n\n\tif (*proto == htons(ETH_P_IPV6))\n\t\treturn pskb_may_pull(skb, *network_depth + sizeof(struct ipv6hdr));\n\n\treturn false;\n}\n\nstatic inline void mlx5e_enable_ecn(struct mlx5e_rq *rq, struct sk_buff *skb)\n{\n\tint network_depth = 0;\n\t__be16 proto;\n\tvoid *ip;\n\tint rc;\n\n\tif (unlikely(!is_last_ethertype_ip(skb, &network_depth, &proto)))\n\t\treturn;\n\n\tip = skb->data + network_depth;\n\trc = ((proto == htons(ETH_P_IP)) ? IP_ECN_set_ce((struct iphdr *)ip) :\n\t\t\t\t\t IP6_ECN_set_ce(skb, (struct ipv6hdr *)ip));\n\n\trq->stats->ecn_mark += !!rc;\n}\n\nstatic u8 get_ip_proto(struct sk_buff *skb, int network_depth, __be16 proto)\n{\n\tvoid *ip_p = skb->data + network_depth;\n\n\treturn (proto == htons(ETH_P_IP)) ? ((struct iphdr *)ip_p)->protocol :\n\t\t\t\t\t    ((struct ipv6hdr *)ip_p)->nexthdr;\n}\n\n#define short_frame(size) ((size) <= ETH_ZLEN + ETH_FCS_LEN)\n\n#define MAX_PADDING 8\n\nstatic void\ntail_padding_csum_slow(struct sk_buff *skb, int offset, int len,\n\t\t       struct mlx5e_rq_stats *stats)\n{\n\tstats->csum_complete_tail_slow++;\n\tskb->csum = csum_block_add(skb->csum,\n\t\t\t\t   skb_checksum(skb, offset, len, 0),\n\t\t\t\t   offset);\n}\n\nstatic void\ntail_padding_csum(struct sk_buff *skb, int offset,\n\t\t  struct mlx5e_rq_stats *stats)\n{\n\tu8 tail_padding[MAX_PADDING];\n\tint len = skb->len - offset;\n\tvoid *tail;\n\n\tif (unlikely(len > MAX_PADDING)) {\n\t\ttail_padding_csum_slow(skb, offset, len, stats);\n\t\treturn;\n\t}\n\n\ttail = skb_header_pointer(skb, offset, len, tail_padding);\n\tif (unlikely(!tail)) {\n\t\ttail_padding_csum_slow(skb, offset, len, stats);\n\t\treturn;\n\t}\n\n\tstats->csum_complete_tail++;\n\tskb->csum = csum_block_add(skb->csum, csum_partial(tail, len, 0), offset);\n}\n\nstatic void\nmlx5e_skb_csum_fixup(struct sk_buff *skb, int network_depth, __be16 proto,\n\t\t     struct mlx5e_rq_stats *stats)\n{\n\tstruct ipv6hdr *ip6;\n\tstruct iphdr   *ip4;\n\tint pkt_len;\n\n\t \n\tif (network_depth > ETH_HLEN)\n\t\t \n\t\tskb->csum = csum_partial(skb->data + ETH_HLEN,\n\t\t\t\t\t network_depth - ETH_HLEN,\n\t\t\t\t\t skb->csum);\n\n\t \n\tswitch (proto) {\n\tcase htons(ETH_P_IP):\n\t\tip4 = (struct iphdr *)(skb->data + network_depth);\n\t\tpkt_len = network_depth + ntohs(ip4->tot_len);\n\t\tbreak;\n\tcase htons(ETH_P_IPV6):\n\t\tip6 = (struct ipv6hdr *)(skb->data + network_depth);\n\t\tpkt_len = network_depth + sizeof(*ip6) + ntohs(ip6->payload_len);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (likely(pkt_len >= skb->len))\n\t\treturn;\n\n\ttail_padding_csum(skb, pkt_len, stats);\n}\n\nstatic inline void mlx5e_handle_csum(struct net_device *netdev,\n\t\t\t\t     struct mlx5_cqe64 *cqe,\n\t\t\t\t     struct mlx5e_rq *rq,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     bool   lro)\n{\n\tstruct mlx5e_rq_stats *stats = rq->stats;\n\tint network_depth = 0;\n\t__be16 proto;\n\n\tif (unlikely(!(netdev->features & NETIF_F_RXCSUM)))\n\t\tgoto csum_none;\n\n\tif (lro) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tstats->csum_unnecessary++;\n\t\treturn;\n\t}\n\n\t \n\tif (test_bit(MLX5E_RQ_STATE_NO_CSUM_COMPLETE, &rq->state) ||\n\t    get_cqe_tls_offload(cqe))\n\t\tgoto csum_unnecessary;\n\n\t \n\tif (short_frame(skb->len))\n\t\tgoto csum_unnecessary;\n\n\tif (likely(is_last_ethertype_ip(skb, &network_depth, &proto))) {\n\t\tif (unlikely(get_ip_proto(skb, network_depth, proto) == IPPROTO_SCTP))\n\t\t\tgoto csum_unnecessary;\n\n\t\tstats->csum_complete++;\n\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\tskb->csum = csum_unfold((__force __sum16)cqe->check_sum);\n\n\t\tif (test_bit(MLX5E_RQ_STATE_CSUM_FULL, &rq->state))\n\t\t\treturn;  \n\n\t\t \n\t\tmlx5e_skb_csum_fixup(skb, network_depth, proto, stats);\n\t\treturn;\n\t}\n\ncsum_unnecessary:\n\tif (likely((cqe->hds_ip_ext & CQE_L3_OK) &&\n\t\t   (cqe->hds_ip_ext & CQE_L4_OK))) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tif (cqe_is_tunneled(cqe)) {\n\t\t\tskb->csum_level = 1;\n\t\t\tskb->encapsulation = 1;\n\t\t\tstats->csum_unnecessary_inner++;\n\t\t\treturn;\n\t\t}\n\t\tstats->csum_unnecessary++;\n\t\treturn;\n\t}\ncsum_none:\n\tskb->ip_summed = CHECKSUM_NONE;\n\tstats->csum_none++;\n}\n\n#define MLX5E_CE_BIT_MASK 0x80\n\nstatic inline void mlx5e_build_rx_skb(struct mlx5_cqe64 *cqe,\n\t\t\t\t      u32 cqe_bcnt,\n\t\t\t\t      struct mlx5e_rq *rq,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tu8 lro_num_seg = be32_to_cpu(cqe->srqn) >> 24;\n\tstruct mlx5e_rq_stats *stats = rq->stats;\n\tstruct net_device *netdev = rq->netdev;\n\n\tskb->mac_len = ETH_HLEN;\n\n\tif (unlikely(get_cqe_tls_offload(cqe)))\n\t\tmlx5e_ktls_handle_rx_skb(rq, skb, cqe, &cqe_bcnt);\n\n\tif (unlikely(mlx5_ipsec_is_rx_flow(cqe)))\n\t\tmlx5e_ipsec_offload_handle_rx_skb(netdev, skb,\n\t\t\t\t\t\t  be32_to_cpu(cqe->ft_metadata));\n\n\tif (unlikely(mlx5e_macsec_is_rx_flow(cqe)))\n\t\tmlx5e_macsec_offload_handle_rx_skb(netdev, skb, cqe);\n\n\tif (lro_num_seg > 1) {\n\t\tmlx5e_lro_update_hdr(skb, cqe, cqe_bcnt);\n\t\tskb_shinfo(skb)->gso_size = DIV_ROUND_UP(cqe_bcnt, lro_num_seg);\n\t\t \n\t\tstats->packets += lro_num_seg - 1;\n\t\tstats->lro_packets++;\n\t\tstats->lro_bytes += cqe_bcnt;\n\t}\n\n\tif (unlikely(mlx5e_rx_hw_stamp(rq->tstamp)))\n\t\tskb_hwtstamps(skb)->hwtstamp = mlx5e_cqe_ts_to_ns(rq->ptp_cyc2time,\n\t\t\t\t\t\t\t\t  rq->clock, get_cqe_ts(cqe));\n\tskb_record_rx_queue(skb, rq->ix);\n\n\tif (likely(netdev->features & NETIF_F_RXHASH))\n\t\tmlx5e_skb_set_hash(cqe, skb);\n\n\tif (cqe_has_vlan(cqe)) {\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t       be16_to_cpu(cqe->vlan_info));\n\t\tstats->removed_vlan_packets++;\n\t}\n\n\tskb->mark = be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK;\n\n\tmlx5e_handle_csum(netdev, cqe, rq, skb, !!lro_num_seg);\n\t \n\tif (unlikely(cqe->ml_path & MLX5E_CE_BIT_MASK))\n\t\tmlx5e_enable_ecn(rq, skb);\n\n\tskb->protocol = eth_type_trans(skb, netdev);\n\n\tif (unlikely(mlx5e_skb_is_multicast(skb)))\n\t\tstats->mcast_packets++;\n}\n\nstatic void mlx5e_shampo_complete_rx_cqe(struct mlx5e_rq *rq,\n\t\t\t\t\t struct mlx5_cqe64 *cqe,\n\t\t\t\t\t u32 cqe_bcnt,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tstruct mlx5e_rq_stats *stats = rq->stats;\n\n\tstats->packets++;\n\tstats->gro_packets++;\n\tstats->bytes += cqe_bcnt;\n\tstats->gro_bytes += cqe_bcnt;\n\tif (NAPI_GRO_CB(skb)->count != 1)\n\t\treturn;\n\tmlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);\n\tskb_reset_network_header(skb);\n\tif (!skb_flow_dissect_flow_keys(skb, &rq->hw_gro_data->fk, 0)) {\n\t\tnapi_gro_receive(rq->cq.napi, skb);\n\t\trq->hw_gro_data->skb = NULL;\n\t}\n}\n\nstatic inline void mlx5e_complete_rx_cqe(struct mlx5e_rq *rq,\n\t\t\t\t\t struct mlx5_cqe64 *cqe,\n\t\t\t\t\t u32 cqe_bcnt,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tstruct mlx5e_rq_stats *stats = rq->stats;\n\n\tstats->packets++;\n\tstats->bytes += cqe_bcnt;\n\tmlx5e_build_rx_skb(cqe, cqe_bcnt, rq, skb);\n}\n\nstatic inline\nstruct sk_buff *mlx5e_build_linear_skb(struct mlx5e_rq *rq, void *va,\n\t\t\t\t       u32 frag_size, u16 headroom,\n\t\t\t\t       u32 cqe_bcnt, u32 metasize)\n{\n\tstruct sk_buff *skb = napi_build_skb(va, frag_size);\n\n\tif (unlikely(!skb)) {\n\t\trq->stats->buff_alloc_err++;\n\t\treturn NULL;\n\t}\n\n\tskb_reserve(skb, headroom);\n\tskb_put(skb, cqe_bcnt);\n\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\treturn skb;\n}\n\nstatic void mlx5e_fill_mxbuf(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,\n\t\t\t     void *va, u16 headroom, u32 frame_sz, u32 len,\n\t\t\t     struct mlx5e_xdp_buff *mxbuf)\n{\n\txdp_init_buff(&mxbuf->xdp, frame_sz, &rq->xdp_rxq);\n\txdp_prepare_buff(&mxbuf->xdp, va, headroom, len, true);\n\tmxbuf->cqe = cqe;\n\tmxbuf->rq = rq;\n}\n\nstatic struct sk_buff *\nmlx5e_skb_from_cqe_linear(struct mlx5e_rq *rq, struct mlx5e_wqe_frag_info *wi,\n\t\t\t  struct mlx5_cqe64 *cqe, u32 cqe_bcnt)\n{\n\tstruct mlx5e_frag_page *frag_page = wi->frag_page;\n\tu16 rx_headroom = rq->buff.headroom;\n\tstruct bpf_prog *prog;\n\tstruct sk_buff *skb;\n\tu32 metasize = 0;\n\tvoid *va, *data;\n\tdma_addr_t addr;\n\tu32 frag_size;\n\n\tva             = page_address(frag_page->page) + wi->offset;\n\tdata           = va + rx_headroom;\n\tfrag_size      = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);\n\n\taddr = page_pool_get_dma_addr(frag_page->page);\n\tdma_sync_single_range_for_cpu(rq->pdev, addr, wi->offset,\n\t\t\t\t      frag_size, rq->buff.map_dir);\n\tnet_prefetch(data);\n\n\tprog = rcu_dereference(rq->xdp_prog);\n\tif (prog) {\n\t\tstruct mlx5e_xdp_buff mxbuf;\n\n\t\tnet_prefetchw(va);  \n\t\tmlx5e_fill_mxbuf(rq, cqe, va, rx_headroom, rq->buff.frame0_sz,\n\t\t\t\t cqe_bcnt, &mxbuf);\n\t\tif (mlx5e_xdp_handle(rq, prog, &mxbuf))\n\t\t\treturn NULL;  \n\n\t\trx_headroom = mxbuf.xdp.data - mxbuf.xdp.data_hard_start;\n\t\tmetasize = mxbuf.xdp.data - mxbuf.xdp.data_meta;\n\t\tcqe_bcnt = mxbuf.xdp.data_end - mxbuf.xdp.data;\n\t}\n\tfrag_size = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);\n\tskb = mlx5e_build_linear_skb(rq, va, frag_size, rx_headroom, cqe_bcnt, metasize);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_mark_for_recycle(skb);\n\tfrag_page->frags++;\n\n\treturn skb;\n}\n\nstatic struct sk_buff *\nmlx5e_skb_from_cqe_nonlinear(struct mlx5e_rq *rq, struct mlx5e_wqe_frag_info *wi,\n\t\t\t     struct mlx5_cqe64 *cqe, u32 cqe_bcnt)\n{\n\tstruct mlx5e_rq_frag_info *frag_info = &rq->wqe.info.arr[0];\n\tstruct mlx5e_wqe_frag_info *head_wi = wi;\n\tu16 rx_headroom = rq->buff.headroom;\n\tstruct mlx5e_frag_page *frag_page;\n\tstruct skb_shared_info *sinfo;\n\tstruct mlx5e_xdp_buff mxbuf;\n\tu32 frag_consumed_bytes;\n\tstruct bpf_prog *prog;\n\tstruct sk_buff *skb;\n\tdma_addr_t addr;\n\tu32 truesize;\n\tvoid *va;\n\n\tfrag_page = wi->frag_page;\n\n\tva = page_address(frag_page->page) + wi->offset;\n\tfrag_consumed_bytes = min_t(u32, frag_info->frag_size, cqe_bcnt);\n\n\taddr = page_pool_get_dma_addr(frag_page->page);\n\tdma_sync_single_range_for_cpu(rq->pdev, addr, wi->offset,\n\t\t\t\t      rq->buff.frame0_sz, rq->buff.map_dir);\n\tnet_prefetchw(va);  \n\tnet_prefetch(va + rx_headroom);\n\n\tmlx5e_fill_mxbuf(rq, cqe, va, rx_headroom, rq->buff.frame0_sz,\n\t\t\t frag_consumed_bytes, &mxbuf);\n\tsinfo = xdp_get_shared_info_from_buff(&mxbuf.xdp);\n\ttruesize = 0;\n\n\tcqe_bcnt -= frag_consumed_bytes;\n\tfrag_info++;\n\twi++;\n\n\twhile (cqe_bcnt) {\n\t\tfrag_page = wi->frag_page;\n\n\t\tfrag_consumed_bytes = min_t(u32, frag_info->frag_size, cqe_bcnt);\n\n\t\tmlx5e_add_skb_shared_info_frag(rq, sinfo, &mxbuf.xdp, frag_page,\n\t\t\t\t\t       wi->offset, frag_consumed_bytes);\n\t\ttruesize += frag_info->frag_stride;\n\n\t\tcqe_bcnt -= frag_consumed_bytes;\n\t\tfrag_info++;\n\t\twi++;\n\t}\n\n\tprog = rcu_dereference(rq->xdp_prog);\n\tif (prog && mlx5e_xdp_handle(rq, prog, &mxbuf)) {\n\t\tif (__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags)) {\n\t\t\tstruct mlx5e_wqe_frag_info *pwi;\n\n\t\t\tfor (pwi = head_wi; pwi < wi; pwi++)\n\t\t\t\tpwi->frag_page->frags++;\n\t\t}\n\t\treturn NULL;  \n\t}\n\n\tskb = mlx5e_build_linear_skb(rq, mxbuf.xdp.data_hard_start, rq->buff.frame0_sz,\n\t\t\t\t     mxbuf.xdp.data - mxbuf.xdp.data_hard_start,\n\t\t\t\t     mxbuf.xdp.data_end - mxbuf.xdp.data,\n\t\t\t\t     mxbuf.xdp.data - mxbuf.xdp.data_meta);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tskb_mark_for_recycle(skb);\n\thead_wi->frag_page->frags++;\n\n\tif (xdp_buff_has_frags(&mxbuf.xdp)) {\n\t\t \n\t\txdp_update_skb_shared_info(skb, wi - head_wi - 1,\n\t\t\t\t\t   sinfo->xdp_frags_size, truesize,\n\t\t\t\t\t   xdp_buff_is_frag_pfmemalloc(&mxbuf.xdp));\n\n\t\tfor (struct mlx5e_wqe_frag_info *pwi = head_wi + 1; pwi < wi; pwi++)\n\t\t\tpwi->frag_page->frags++;\n\t}\n\n\treturn skb;\n}\n\nstatic void trigger_report(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\tstruct mlx5_err_cqe *err_cqe = (struct mlx5_err_cqe *)cqe;\n\tstruct mlx5e_priv *priv = rq->priv;\n\n\tif (cqe_syndrome_needs_recover(err_cqe->syndrome) &&\n\t    !test_and_set_bit(MLX5E_RQ_STATE_RECOVERING, &rq->state)) {\n\t\tmlx5e_dump_error_cqe(&rq->cq, rq->rqn, err_cqe);\n\t\tqueue_work(priv->wq, &rq->recover_work);\n\t}\n}\n\nstatic void mlx5e_handle_rx_err_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\ttrigger_report(rq, cqe);\n\trq->stats->wqe_err++;\n}\n\nstatic void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tstruct mlx5e_wqe_frag_info *wi;\n\tstruct sk_buff *skb;\n\tu32 cqe_bcnt;\n\tu16 ci;\n\n\tci       = mlx5_wq_cyc_ctr2ix(wq, be16_to_cpu(cqe->wqe_counter));\n\twi       = get_frag(rq, ci);\n\tcqe_bcnt = be32_to_cpu(cqe->byte_cnt);\n\n\tif (unlikely(MLX5E_RX_ERR_CQE(cqe))) {\n\t\tmlx5e_handle_rx_err_cqe(rq, cqe);\n\t\tgoto wq_cyc_pop;\n\t}\n\n\tskb = INDIRECT_CALL_3(rq->wqe.skb_from_cqe,\n\t\t\t      mlx5e_skb_from_cqe_linear,\n\t\t\t      mlx5e_skb_from_cqe_nonlinear,\n\t\t\t      mlx5e_xsk_skb_from_cqe_linear,\n\t\t\t      rq, wi, cqe, cqe_bcnt);\n\tif (!skb) {\n\t\t \n\t\tif (__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags))\n\t\t\twi->frag_page->frags++;\n\t\tgoto wq_cyc_pop;\n\t}\n\n\tmlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);\n\n\tif (mlx5e_cqe_regb_chain(cqe))\n\t\tif (!mlx5e_tc_update_skb_nic(cqe, skb)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto wq_cyc_pop;\n\t\t}\n\n\tnapi_gro_receive(rq->cq.napi, skb);\n\nwq_cyc_pop:\n\tmlx5_wq_cyc_pop(wq);\n}\n\n#ifdef CONFIG_MLX5_ESWITCH\nstatic void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\tstruct net_device *netdev = rq->netdev;\n\tstruct mlx5e_priv *priv = netdev_priv(netdev);\n\tstruct mlx5e_rep_priv *rpriv  = priv->ppriv;\n\tstruct mlx5_eswitch_rep *rep = rpriv->rep;\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tstruct mlx5e_wqe_frag_info *wi;\n\tstruct sk_buff *skb;\n\tu32 cqe_bcnt;\n\tu16 ci;\n\n\tci       = mlx5_wq_cyc_ctr2ix(wq, be16_to_cpu(cqe->wqe_counter));\n\twi       = get_frag(rq, ci);\n\tcqe_bcnt = be32_to_cpu(cqe->byte_cnt);\n\n\tif (unlikely(MLX5E_RX_ERR_CQE(cqe))) {\n\t\tmlx5e_handle_rx_err_cqe(rq, cqe);\n\t\tgoto wq_cyc_pop;\n\t}\n\n\tskb = INDIRECT_CALL_2(rq->wqe.skb_from_cqe,\n\t\t\t      mlx5e_skb_from_cqe_linear,\n\t\t\t      mlx5e_skb_from_cqe_nonlinear,\n\t\t\t      rq, wi, cqe, cqe_bcnt);\n\tif (!skb) {\n\t\t \n\t\tif (__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags))\n\t\t\twi->frag_page->frags++;\n\t\tgoto wq_cyc_pop;\n\t}\n\n\tmlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);\n\n\tif (rep->vlan && skb_vlan_tag_present(skb))\n\t\tskb_vlan_pop(skb);\n\n\tmlx5e_rep_tc_receive(cqe, rq, skb);\n\nwq_cyc_pop:\n\tmlx5_wq_cyc_pop(wq);\n}\n\nstatic void mlx5e_handle_rx_cqe_mpwrq_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\tu16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);\n\tu16 wqe_id         = be16_to_cpu(cqe->wqe_id);\n\tstruct mlx5e_mpw_info *wi = mlx5e_get_mpw_info(rq, wqe_id);\n\tu16 stride_ix      = mpwrq_get_cqe_stride_index(cqe);\n\tu32 wqe_offset     = stride_ix << rq->mpwqe.log_stride_sz;\n\tu32 head_offset    = wqe_offset & ((1 << rq->mpwqe.page_shift) - 1);\n\tu32 page_idx       = wqe_offset >> rq->mpwqe.page_shift;\n\tstruct mlx5e_rx_wqe_ll *wqe;\n\tstruct mlx5_wq_ll *wq;\n\tstruct sk_buff *skb;\n\tu16 cqe_bcnt;\n\n\twi->consumed_strides += cstrides;\n\n\tif (unlikely(MLX5E_RX_ERR_CQE(cqe))) {\n\t\tmlx5e_handle_rx_err_cqe(rq, cqe);\n\t\tgoto mpwrq_cqe_out;\n\t}\n\n\tif (unlikely(mpwrq_is_filler_cqe(cqe))) {\n\t\tstruct mlx5e_rq_stats *stats = rq->stats;\n\n\t\tstats->mpwqe_filler_cqes++;\n\t\tstats->mpwqe_filler_strides += cstrides;\n\t\tgoto mpwrq_cqe_out;\n\t}\n\n\tcqe_bcnt = mpwrq_get_cqe_byte_cnt(cqe);\n\n\tskb = INDIRECT_CALL_2(rq->mpwqe.skb_from_cqe_mpwrq,\n\t\t\t      mlx5e_skb_from_cqe_mpwrq_linear,\n\t\t\t      mlx5e_skb_from_cqe_mpwrq_nonlinear,\n\t\t\t      rq, wi, cqe, cqe_bcnt, head_offset, page_idx);\n\tif (!skb)\n\t\tgoto mpwrq_cqe_out;\n\n\tmlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);\n\n\tmlx5e_rep_tc_receive(cqe, rq, skb);\n\nmpwrq_cqe_out:\n\tif (likely(wi->consumed_strides < rq->mpwqe.num_strides))\n\t\treturn;\n\n\twq  = &rq->mpwqe.wq;\n\twqe = mlx5_wq_ll_get_wqe(wq, wqe_id);\n\tmlx5_wq_ll_pop(wq, cqe->wqe_id, &wqe->next.next_wqe_index);\n}\n\nconst struct mlx5e_rx_handlers mlx5e_rx_handlers_rep = {\n\t.handle_rx_cqe       = mlx5e_handle_rx_cqe_rep,\n\t.handle_rx_cqe_mpwqe = mlx5e_handle_rx_cqe_mpwrq_rep,\n};\n#endif\n\nstatic void\nmlx5e_fill_skb_data(struct sk_buff *skb, struct mlx5e_rq *rq,\n\t\t    struct mlx5e_frag_page *frag_page,\n\t\t    u32 data_bcnt, u32 data_offset)\n{\n\tnet_prefetchw(skb->data);\n\n\twhile (data_bcnt) {\n\t\t \n\t\tu32 pg_consumed_bytes = min_t(u32, PAGE_SIZE - data_offset, data_bcnt);\n\t\tunsigned int truesize;\n\n\t\tif (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state))\n\t\t\ttruesize = pg_consumed_bytes;\n\t\telse\n\t\t\ttruesize = ALIGN(pg_consumed_bytes, BIT(rq->mpwqe.log_stride_sz));\n\n\t\tfrag_page->frags++;\n\t\tmlx5e_add_skb_frag(rq, skb, frag_page->page, data_offset,\n\t\t\t\t   pg_consumed_bytes, truesize);\n\n\t\tdata_bcnt -= pg_consumed_bytes;\n\t\tdata_offset = 0;\n\t\tfrag_page++;\n\t}\n}\n\nstatic struct sk_buff *\nmlx5e_skb_from_cqe_mpwrq_nonlinear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,\n\t\t\t\t   struct mlx5_cqe64 *cqe, u16 cqe_bcnt, u32 head_offset,\n\t\t\t\t   u32 page_idx)\n{\n\tstruct mlx5e_frag_page *frag_page = &wi->alloc_units.frag_pages[page_idx];\n\tu16 headlen = min_t(u16, MLX5E_RX_MAX_HEAD, cqe_bcnt);\n\tstruct mlx5e_frag_page *head_page = frag_page;\n\tu32 frag_offset    = head_offset;\n\tu32 byte_cnt       = cqe_bcnt;\n\tstruct skb_shared_info *sinfo;\n\tstruct mlx5e_xdp_buff mxbuf;\n\tunsigned int truesize = 0;\n\tstruct bpf_prog *prog;\n\tstruct sk_buff *skb;\n\tu32 linear_frame_sz;\n\tu16 linear_data_len;\n\tu16 linear_hr;\n\tvoid *va;\n\n\tprog = rcu_dereference(rq->xdp_prog);\n\n\tif (prog) {\n\t\t \n\t\tnet_prefetchw(page_address(frag_page->page) + frag_offset);\n\t\tif (unlikely(mlx5e_page_alloc_fragmented(rq, &wi->linear_page))) {\n\t\t\trq->stats->buff_alloc_err++;\n\t\t\treturn NULL;\n\t\t}\n\t\tva = page_address(wi->linear_page.page);\n\t\tnet_prefetchw(va);  \n\t\tlinear_hr = XDP_PACKET_HEADROOM;\n\t\tlinear_data_len = 0;\n\t\tlinear_frame_sz = MLX5_SKB_FRAG_SZ(linear_hr + MLX5E_RX_MAX_HEAD);\n\t} else {\n\t\tskb = napi_alloc_skb(rq->cq.napi,\n\t\t\t\t     ALIGN(MLX5E_RX_MAX_HEAD, sizeof(long)));\n\t\tif (unlikely(!skb)) {\n\t\t\trq->stats->buff_alloc_err++;\n\t\t\treturn NULL;\n\t\t}\n\t\tskb_mark_for_recycle(skb);\n\t\tva = skb->head;\n\t\tnet_prefetchw(va);  \n\t\tnet_prefetchw(skb->data);\n\n\t\tfrag_offset += headlen;\n\t\tbyte_cnt -= headlen;\n\t\tlinear_hr = skb_headroom(skb);\n\t\tlinear_data_len = headlen;\n\t\tlinear_frame_sz = MLX5_SKB_FRAG_SZ(skb_end_offset(skb));\n\t\tif (unlikely(frag_offset >= PAGE_SIZE)) {\n\t\t\tfrag_page++;\n\t\t\tfrag_offset -= PAGE_SIZE;\n\t\t}\n\t}\n\n\tmlx5e_fill_mxbuf(rq, cqe, va, linear_hr, linear_frame_sz, linear_data_len, &mxbuf);\n\n\tsinfo = xdp_get_shared_info_from_buff(&mxbuf.xdp);\n\n\twhile (byte_cnt) {\n\t\t \n\t\tu32 pg_consumed_bytes = min_t(u32, PAGE_SIZE - frag_offset, byte_cnt);\n\n\t\tif (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state))\n\t\t\ttruesize += pg_consumed_bytes;\n\t\telse\n\t\t\ttruesize += ALIGN(pg_consumed_bytes, BIT(rq->mpwqe.log_stride_sz));\n\n\t\tmlx5e_add_skb_shared_info_frag(rq, sinfo, &mxbuf.xdp, frag_page, frag_offset,\n\t\t\t\t\t       pg_consumed_bytes);\n\t\tbyte_cnt -= pg_consumed_bytes;\n\t\tfrag_offset = 0;\n\t\tfrag_page++;\n\t}\n\n\tif (prog) {\n\t\tif (mlx5e_xdp_handle(rq, prog, &mxbuf)) {\n\t\t\tif (__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags)) {\n\t\t\t\tstruct mlx5e_frag_page *pfp;\n\n\t\t\t\tfor (pfp = head_page; pfp < frag_page; pfp++)\n\t\t\t\t\tpfp->frags++;\n\n\t\t\t\twi->linear_page.frags++;\n\t\t\t}\n\t\t\tmlx5e_page_release_fragmented(rq, &wi->linear_page);\n\t\t\treturn NULL;  \n\t\t}\n\n\t\tskb = mlx5e_build_linear_skb(rq, mxbuf.xdp.data_hard_start,\n\t\t\t\t\t     linear_frame_sz,\n\t\t\t\t\t     mxbuf.xdp.data - mxbuf.xdp.data_hard_start, 0,\n\t\t\t\t\t     mxbuf.xdp.data - mxbuf.xdp.data_meta);\n\t\tif (unlikely(!skb)) {\n\t\t\tmlx5e_page_release_fragmented(rq, &wi->linear_page);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tskb_mark_for_recycle(skb);\n\t\twi->linear_page.frags++;\n\t\tmlx5e_page_release_fragmented(rq, &wi->linear_page);\n\n\t\tif (xdp_buff_has_frags(&mxbuf.xdp)) {\n\t\t\tstruct mlx5e_frag_page *pagep;\n\n\t\t\t \n\t\t\txdp_update_skb_shared_info(skb, frag_page - head_page,\n\t\t\t\t\t\t   sinfo->xdp_frags_size, truesize,\n\t\t\t\t\t\t   xdp_buff_is_frag_pfmemalloc(&mxbuf.xdp));\n\n\t\t\tpagep = head_page;\n\t\t\tdo\n\t\t\t\tpagep->frags++;\n\t\t\twhile (++pagep < frag_page);\n\t\t}\n\t\t__pskb_pull_tail(skb, headlen);\n\t} else {\n\t\tdma_addr_t addr;\n\n\t\tif (xdp_buff_has_frags(&mxbuf.xdp)) {\n\t\t\tstruct mlx5e_frag_page *pagep;\n\n\t\t\txdp_update_skb_shared_info(skb, sinfo->nr_frags,\n\t\t\t\t\t\t   sinfo->xdp_frags_size, truesize,\n\t\t\t\t\t\t   xdp_buff_is_frag_pfmemalloc(&mxbuf.xdp));\n\n\t\t\tpagep = frag_page - sinfo->nr_frags;\n\t\t\tdo\n\t\t\t\tpagep->frags++;\n\t\t\twhile (++pagep < frag_page);\n\t\t}\n\t\t \n\t\taddr = page_pool_get_dma_addr(head_page->page);\n\t\tmlx5e_copy_skb_header(rq, skb, head_page->page, addr,\n\t\t\t\t      head_offset, head_offset, headlen);\n\t\t \n\t\tskb->tail += headlen;\n\t\tskb->len  += headlen;\n\t}\n\n\treturn skb;\n}\n\nstatic struct sk_buff *\nmlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,\n\t\t\t\tstruct mlx5_cqe64 *cqe, u16 cqe_bcnt, u32 head_offset,\n\t\t\t\tu32 page_idx)\n{\n\tstruct mlx5e_frag_page *frag_page = &wi->alloc_units.frag_pages[page_idx];\n\tu16 rx_headroom = rq->buff.headroom;\n\tstruct bpf_prog *prog;\n\tstruct sk_buff *skb;\n\tu32 metasize = 0;\n\tvoid *va, *data;\n\tdma_addr_t addr;\n\tu32 frag_size;\n\n\t \n\tif (unlikely(cqe_bcnt > rq->hw_mtu)) {\n\t\trq->stats->oversize_pkts_sw_drop++;\n\t\treturn NULL;\n\t}\n\n\tva             = page_address(frag_page->page) + head_offset;\n\tdata           = va + rx_headroom;\n\tfrag_size      = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);\n\n\taddr = page_pool_get_dma_addr(frag_page->page);\n\tdma_sync_single_range_for_cpu(rq->pdev, addr, head_offset,\n\t\t\t\t      frag_size, rq->buff.map_dir);\n\tnet_prefetch(data);\n\n\tprog = rcu_dereference(rq->xdp_prog);\n\tif (prog) {\n\t\tstruct mlx5e_xdp_buff mxbuf;\n\n\t\tnet_prefetchw(va);  \n\t\tmlx5e_fill_mxbuf(rq, cqe, va, rx_headroom, rq->buff.frame0_sz,\n\t\t\t\t cqe_bcnt, &mxbuf);\n\t\tif (mlx5e_xdp_handle(rq, prog, &mxbuf)) {\n\t\t\tif (__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags))\n\t\t\t\tfrag_page->frags++;\n\t\t\treturn NULL;  \n\t\t}\n\n\t\trx_headroom = mxbuf.xdp.data - mxbuf.xdp.data_hard_start;\n\t\tmetasize = mxbuf.xdp.data - mxbuf.xdp.data_meta;\n\t\tcqe_bcnt = mxbuf.xdp.data_end - mxbuf.xdp.data;\n\t}\n\tfrag_size = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);\n\tskb = mlx5e_build_linear_skb(rq, va, frag_size, rx_headroom, cqe_bcnt, metasize);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tskb_mark_for_recycle(skb);\n\tfrag_page->frags++;\n\n\treturn skb;\n}\n\nstatic struct sk_buff *\nmlx5e_skb_from_cqe_shampo(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,\n\t\t\t  struct mlx5_cqe64 *cqe, u16 header_index)\n{\n\tstruct mlx5e_dma_info *head = &rq->mpwqe.shampo->info[header_index];\n\tu16 head_offset = head->addr & (PAGE_SIZE - 1);\n\tu16 head_size = cqe->shampo.header_size;\n\tu16 rx_headroom = rq->buff.headroom;\n\tstruct sk_buff *skb = NULL;\n\tvoid *hdr, *data;\n\tu32 frag_size;\n\n\thdr\t\t= page_address(head->frag_page->page) + head_offset;\n\tdata\t\t= hdr + rx_headroom;\n\tfrag_size\t= MLX5_SKB_FRAG_SZ(rx_headroom + head_size);\n\n\tif (likely(frag_size <= BIT(MLX5E_SHAMPO_LOG_MAX_HEADER_ENTRY_SIZE))) {\n\t\t \n\t\tdma_sync_single_range_for_cpu(rq->pdev, head->addr, 0, frag_size, rq->buff.map_dir);\n\t\tprefetchw(hdr);\n\t\tprefetch(data);\n\t\tskb = mlx5e_build_linear_skb(rq, hdr, frag_size, rx_headroom, head_size, 0);\n\n\t\tif (unlikely(!skb))\n\t\t\treturn NULL;\n\n\t\thead->frag_page->frags++;\n\t} else {\n\t\t \n\t\trq->stats->gro_large_hds++;\n\t\tskb = napi_alloc_skb(rq->cq.napi,\n\t\t\t\t     ALIGN(head_size, sizeof(long)));\n\t\tif (unlikely(!skb)) {\n\t\t\trq->stats->buff_alloc_err++;\n\t\t\treturn NULL;\n\t\t}\n\n\t\tprefetchw(skb->data);\n\t\tmlx5e_copy_skb_header(rq, skb, head->frag_page->page, head->addr,\n\t\t\t\t      head_offset + rx_headroom,\n\t\t\t\t      rx_headroom, head_size);\n\t\t \n\t\tskb->tail += head_size;\n\t\tskb->len  += head_size;\n\t}\n\n\t \n\tskb_mark_for_recycle(skb);\n\n\treturn skb;\n}\n\nstatic void\nmlx5e_shampo_align_fragment(struct sk_buff *skb, u8 log_stride_sz)\n{\n\tskb_frag_t *last_frag = &skb_shinfo(skb)->frags[skb_shinfo(skb)->nr_frags - 1];\n\tunsigned int frag_size = skb_frag_size(last_frag);\n\tunsigned int frag_truesize;\n\n\tfrag_truesize = ALIGN(frag_size, BIT(log_stride_sz));\n\tskb->truesize += frag_truesize - frag_size;\n}\n\nstatic void\nmlx5e_shampo_flush_skb(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe, bool match)\n{\n\tstruct sk_buff *skb = rq->hw_gro_data->skb;\n\tstruct mlx5e_rq_stats *stats = rq->stats;\n\n\tstats->gro_skbs++;\n\tif (likely(skb_shinfo(skb)->nr_frags))\n\t\tmlx5e_shampo_align_fragment(skb, rq->mpwqe.log_stride_sz);\n\tif (NAPI_GRO_CB(skb)->count > 1)\n\t\tmlx5e_shampo_update_hdr(rq, cqe, match);\n\tnapi_gro_receive(rq->cq.napi, skb);\n\trq->hw_gro_data->skb = NULL;\n}\n\nstatic bool\nmlx5e_hw_gro_skb_has_enough_space(struct sk_buff *skb, u16 data_bcnt)\n{\n\tint nr_frags = skb_shinfo(skb)->nr_frags;\n\n\treturn PAGE_SIZE * nr_frags + data_bcnt <= GRO_LEGACY_MAX_SIZE;\n}\n\nstatic void\nmlx5e_free_rx_shampo_hd_entry(struct mlx5e_rq *rq, u16 header_index)\n{\n\tstruct mlx5e_shampo_hd *shampo = rq->mpwqe.shampo;\n\tu64 addr = shampo->info[header_index].addr;\n\n\tif (((header_index + 1) & (MLX5E_SHAMPO_WQ_HEADER_PER_PAGE - 1)) == 0) {\n\t\tstruct mlx5e_dma_info *dma_info = &shampo->info[header_index];\n\n\t\tdma_info->addr = ALIGN_DOWN(addr, PAGE_SIZE);\n\t\tmlx5e_page_release_fragmented(rq, dma_info->frag_page);\n\t}\n\tbitmap_clear(shampo->bitmap, header_index, 1);\n}\n\nstatic void mlx5e_handle_rx_cqe_mpwrq_shampo(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\tu16 data_bcnt\t\t= mpwrq_get_cqe_byte_cnt(cqe) - cqe->shampo.header_size;\n\tu16 header_index\t= mlx5e_shampo_get_cqe_header_index(rq, cqe);\n\tu32 wqe_offset\t\t= be32_to_cpu(cqe->shampo.data_offset);\n\tu16 cstrides\t\t= mpwrq_get_cqe_consumed_strides(cqe);\n\tu32 data_offset\t\t= wqe_offset & (PAGE_SIZE - 1);\n\tu32 cqe_bcnt\t\t= mpwrq_get_cqe_byte_cnt(cqe);\n\tu16 wqe_id\t\t= be16_to_cpu(cqe->wqe_id);\n\tu32 page_idx\t\t= wqe_offset >> PAGE_SHIFT;\n\tu16 head_size\t\t= cqe->shampo.header_size;\n\tstruct sk_buff **skb\t= &rq->hw_gro_data->skb;\n\tbool flush\t\t= cqe->shampo.flush;\n\tbool match\t\t= cqe->shampo.match;\n\tstruct mlx5e_rq_stats *stats = rq->stats;\n\tstruct mlx5e_rx_wqe_ll *wqe;\n\tstruct mlx5e_mpw_info *wi;\n\tstruct mlx5_wq_ll *wq;\n\n\twi = mlx5e_get_mpw_info(rq, wqe_id);\n\twi->consumed_strides += cstrides;\n\n\tif (unlikely(MLX5E_RX_ERR_CQE(cqe))) {\n\t\tmlx5e_handle_rx_err_cqe(rq, cqe);\n\t\tgoto mpwrq_cqe_out;\n\t}\n\n\tif (unlikely(mpwrq_is_filler_cqe(cqe))) {\n\t\tstats->mpwqe_filler_cqes++;\n\t\tstats->mpwqe_filler_strides += cstrides;\n\t\tgoto mpwrq_cqe_out;\n\t}\n\n\tstats->gro_match_packets += match;\n\n\tif (*skb && (!match || !(mlx5e_hw_gro_skb_has_enough_space(*skb, data_bcnt)))) {\n\t\tmatch = false;\n\t\tmlx5e_shampo_flush_skb(rq, cqe, match);\n\t}\n\n\tif (!*skb) {\n\t\tif (likely(head_size))\n\t\t\t*skb = mlx5e_skb_from_cqe_shampo(rq, wi, cqe, header_index);\n\t\telse\n\t\t\t*skb = mlx5e_skb_from_cqe_mpwrq_nonlinear(rq, wi, cqe, cqe_bcnt,\n\t\t\t\t\t\t\t\t  data_offset, page_idx);\n\t\tif (unlikely(!*skb))\n\t\t\tgoto free_hd_entry;\n\n\t\tNAPI_GRO_CB(*skb)->count = 1;\n\t\tskb_shinfo(*skb)->gso_size = cqe_bcnt - head_size;\n\t} else {\n\t\tNAPI_GRO_CB(*skb)->count++;\n\t\tif (NAPI_GRO_CB(*skb)->count == 2 &&\n\t\t    rq->hw_gro_data->fk.basic.n_proto == htons(ETH_P_IP)) {\n\t\t\tvoid *hd_addr = mlx5e_shampo_get_packet_hd(rq, header_index);\n\t\t\tint nhoff = ETH_HLEN + rq->hw_gro_data->fk.control.thoff -\n\t\t\t\t    sizeof(struct iphdr);\n\t\t\tstruct iphdr *iph = (struct iphdr *)(hd_addr + nhoff);\n\n\t\t\trq->hw_gro_data->second_ip_id = ntohs(iph->id);\n\t\t}\n\t}\n\n\tif (likely(head_size)) {\n\t\tstruct mlx5e_frag_page *frag_page;\n\n\t\tfrag_page = &wi->alloc_units.frag_pages[page_idx];\n\t\tmlx5e_fill_skb_data(*skb, rq, frag_page, data_bcnt, data_offset);\n\t}\n\n\tmlx5e_shampo_complete_rx_cqe(rq, cqe, cqe_bcnt, *skb);\n\tif (flush)\n\t\tmlx5e_shampo_flush_skb(rq, cqe, match);\nfree_hd_entry:\n\tmlx5e_free_rx_shampo_hd_entry(rq, header_index);\nmpwrq_cqe_out:\n\tif (likely(wi->consumed_strides < rq->mpwqe.num_strides))\n\t\treturn;\n\n\twq  = &rq->mpwqe.wq;\n\twqe = mlx5_wq_ll_get_wqe(wq, wqe_id);\n\tmlx5_wq_ll_pop(wq, cqe->wqe_id, &wqe->next.next_wqe_index);\n}\n\nstatic void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\tu16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);\n\tu16 wqe_id         = be16_to_cpu(cqe->wqe_id);\n\tstruct mlx5e_mpw_info *wi = mlx5e_get_mpw_info(rq, wqe_id);\n\tu16 stride_ix      = mpwrq_get_cqe_stride_index(cqe);\n\tu32 wqe_offset     = stride_ix << rq->mpwqe.log_stride_sz;\n\tu32 head_offset    = wqe_offset & ((1 << rq->mpwqe.page_shift) - 1);\n\tu32 page_idx       = wqe_offset >> rq->mpwqe.page_shift;\n\tstruct mlx5e_rx_wqe_ll *wqe;\n\tstruct mlx5_wq_ll *wq;\n\tstruct sk_buff *skb;\n\tu16 cqe_bcnt;\n\n\twi->consumed_strides += cstrides;\n\n\tif (unlikely(MLX5E_RX_ERR_CQE(cqe))) {\n\t\tmlx5e_handle_rx_err_cqe(rq, cqe);\n\t\tgoto mpwrq_cqe_out;\n\t}\n\n\tif (unlikely(mpwrq_is_filler_cqe(cqe))) {\n\t\tstruct mlx5e_rq_stats *stats = rq->stats;\n\n\t\tstats->mpwqe_filler_cqes++;\n\t\tstats->mpwqe_filler_strides += cstrides;\n\t\tgoto mpwrq_cqe_out;\n\t}\n\n\tcqe_bcnt = mpwrq_get_cqe_byte_cnt(cqe);\n\n\tskb = INDIRECT_CALL_3(rq->mpwqe.skb_from_cqe_mpwrq,\n\t\t\t      mlx5e_skb_from_cqe_mpwrq_linear,\n\t\t\t      mlx5e_skb_from_cqe_mpwrq_nonlinear,\n\t\t\t      mlx5e_xsk_skb_from_cqe_mpwrq_linear,\n\t\t\t      rq, wi, cqe, cqe_bcnt, head_offset,\n\t\t\t      page_idx);\n\tif (!skb)\n\t\tgoto mpwrq_cqe_out;\n\n\tmlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);\n\n\tif (mlx5e_cqe_regb_chain(cqe))\n\t\tif (!mlx5e_tc_update_skb_nic(cqe, skb)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto mpwrq_cqe_out;\n\t\t}\n\n\tnapi_gro_receive(rq->cq.napi, skb);\n\nmpwrq_cqe_out:\n\tif (likely(wi->consumed_strides < rq->mpwqe.num_strides))\n\t\treturn;\n\n\twq  = &rq->mpwqe.wq;\n\twqe = mlx5_wq_ll_get_wqe(wq, wqe_id);\n\tmlx5_wq_ll_pop(wq, cqe->wqe_id, &wqe->next.next_wqe_index);\n}\n\nstatic int mlx5e_rx_cq_process_enhanced_cqe_comp(struct mlx5e_rq *rq,\n\t\t\t\t\t\t struct mlx5_cqwq *cqwq,\n\t\t\t\t\t\t int budget_rem)\n{\n\tstruct mlx5_cqe64 *cqe, *title_cqe = NULL;\n\tstruct mlx5e_cq_decomp *cqd = &rq->cqd;\n\tint work_done = 0;\n\n\tcqe = mlx5_cqwq_get_cqe_enahnced_comp(cqwq);\n\tif (!cqe)\n\t\treturn work_done;\n\n\tif (cqd->last_cqe_title &&\n\t    (mlx5_get_cqe_format(cqe) == MLX5_COMPRESSED)) {\n\t\trq->stats->cqe_compress_blks++;\n\t\tcqd->last_cqe_title = false;\n\t}\n\n\tdo {\n\t\tif (mlx5_get_cqe_format(cqe) == MLX5_COMPRESSED) {\n\t\t\tif (title_cqe) {\n\t\t\t\tmlx5e_read_enhanced_title_slot(rq, title_cqe);\n\t\t\t\ttitle_cqe = NULL;\n\t\t\t\trq->stats->cqe_compress_blks++;\n\t\t\t}\n\t\t\twork_done +=\n\t\t\t\tmlx5e_decompress_enhanced_cqe(rq, cqwq, cqe,\n\t\t\t\t\t\t\t      budget_rem - work_done);\n\t\t\tcontinue;\n\t\t}\n\t\ttitle_cqe = cqe;\n\t\tmlx5_cqwq_pop(cqwq);\n\n\t\tINDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,\n\t\t\t\tmlx5e_handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq_shampo,\n\t\t\t\trq, cqe);\n\t\twork_done++;\n\t} while (work_done < budget_rem &&\n\t\t (cqe = mlx5_cqwq_get_cqe_enahnced_comp(cqwq)));\n\n\t \n\tif (title_cqe) {\n\t\tmlx5e_read_enhanced_title_slot(rq, title_cqe);\n\t\tcqd->last_cqe_title = true;\n\t}\n\n\treturn work_done;\n}\n\nstatic int mlx5e_rx_cq_process_basic_cqe_comp(struct mlx5e_rq *rq,\n\t\t\t\t\t      struct mlx5_cqwq *cqwq,\n\t\t\t\t\t      int budget_rem)\n{\n\tstruct mlx5_cqe64 *cqe;\n\tint work_done = 0;\n\n\tif (rq->cqd.left)\n\t\twork_done += mlx5e_decompress_cqes_cont(rq, cqwq, 0, budget_rem);\n\n\twhile (work_done < budget_rem && (cqe = mlx5_cqwq_get_cqe(cqwq))) {\n\t\tif (mlx5_get_cqe_format(cqe) == MLX5_COMPRESSED) {\n\t\t\twork_done +=\n\t\t\t\tmlx5e_decompress_cqes_start(rq, cqwq,\n\t\t\t\t\t\t\t    budget_rem - work_done);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmlx5_cqwq_pop(cqwq);\n\t\tINDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,\n\t\t\t\tmlx5e_handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq_shampo,\n\t\t\t\trq, cqe);\n\t\twork_done++;\n\t}\n\n\treturn work_done;\n}\n\nint mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)\n{\n\tstruct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);\n\tstruct mlx5_cqwq *cqwq = &cq->wq;\n\tint work_done;\n\n\tif (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))\n\t\treturn 0;\n\n\tif (test_bit(MLX5E_RQ_STATE_MINI_CQE_ENHANCED, &rq->state))\n\t\twork_done = mlx5e_rx_cq_process_enhanced_cqe_comp(rq, cqwq,\n\t\t\t\t\t\t\t\t  budget);\n\telse\n\t\twork_done = mlx5e_rx_cq_process_basic_cqe_comp(rq, cqwq,\n\t\t\t\t\t\t\t       budget);\n\n\tif (work_done == 0)\n\t\treturn 0;\n\n\tif (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state) && rq->hw_gro_data->skb)\n\t\tmlx5e_shampo_flush_skb(rq, NULL, false);\n\n\tif (rcu_access_pointer(rq->xdp_prog))\n\t\tmlx5e_xdp_rx_poll_complete(rq);\n\n\tmlx5_cqwq_update_db_record(cqwq);\n\n\t \n\twmb();\n\n\treturn work_done;\n}\n\n#ifdef CONFIG_MLX5_CORE_IPOIB\n\n#define MLX5_IB_GRH_SGID_OFFSET 8\n#define MLX5_IB_GRH_DGID_OFFSET 24\n#define MLX5_GID_SIZE           16\n\nstatic inline void mlx5i_complete_rx_cqe(struct mlx5e_rq *rq,\n\t\t\t\t\t struct mlx5_cqe64 *cqe,\n\t\t\t\t\t u32 cqe_bcnt,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tstruct hwtstamp_config *tstamp;\n\tstruct mlx5e_rq_stats *stats;\n\tstruct net_device *netdev;\n\tstruct mlx5e_priv *priv;\n\tchar *pseudo_header;\n\tu32 flags_rqpn;\n\tu32 qpn;\n\tu8 *dgid;\n\tu8 g;\n\n\tqpn = be32_to_cpu(cqe->sop_drop_qpn) & 0xffffff;\n\tnetdev = mlx5i_pkey_get_netdev(rq->netdev, qpn);\n\n\t \n\tif (unlikely(!netdev)) {\n\t\t \n\t\tskb->dev = NULL;\n\t\tpr_warn_once(\"Unable to map QPN %u to dev - dropping skb\\n\", qpn);\n\t\treturn;\n\t}\n\n\tpriv = mlx5i_epriv(netdev);\n\ttstamp = &priv->tstamp;\n\tstats = &priv->channel_stats[rq->ix]->rq;\n\n\tflags_rqpn = be32_to_cpu(cqe->flags_rqpn);\n\tg = (flags_rqpn >> 28) & 3;\n\tdgid = skb->data + MLX5_IB_GRH_DGID_OFFSET;\n\tif ((!g) || dgid[0] != 0xff)\n\t\tskb->pkt_type = PACKET_HOST;\n\telse if (memcmp(dgid, netdev->broadcast + 4, MLX5_GID_SIZE) == 0)\n\t\tskb->pkt_type = PACKET_BROADCAST;\n\telse\n\t\tskb->pkt_type = PACKET_MULTICAST;\n\n\t \n\tif (g && (qpn == (flags_rqpn & 0xffffff)) &&\n\t    (memcmp(netdev->dev_addr + 4, skb->data + MLX5_IB_GRH_SGID_OFFSET,\n\t\t    MLX5_GID_SIZE) == 0)) {\n\t\tskb->dev = NULL;\n\t\treturn;\n\t}\n\n\tskb_pull(skb, MLX5_IB_GRH_BYTES);\n\n\tskb->protocol = *((__be16 *)(skb->data));\n\n\tif (netdev->features & NETIF_F_RXCSUM) {\n\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\tskb->csum = csum_unfold((__force __sum16)cqe->check_sum);\n\t\tstats->csum_complete++;\n\t} else {\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tstats->csum_none++;\n\t}\n\n\tif (unlikely(mlx5e_rx_hw_stamp(tstamp)))\n\t\tskb_hwtstamps(skb)->hwtstamp = mlx5e_cqe_ts_to_ns(rq->ptp_cyc2time,\n\t\t\t\t\t\t\t\t  rq->clock, get_cqe_ts(cqe));\n\tskb_record_rx_queue(skb, rq->ix);\n\n\tif (likely(netdev->features & NETIF_F_RXHASH))\n\t\tmlx5e_skb_set_hash(cqe, skb);\n\n\t \n\tpseudo_header = skb_push(skb, MLX5_IPOIB_PSEUDO_LEN);\n\tmemset(pseudo_header, 0, MLX5_IPOIB_PSEUDO_LEN);\n\tskb_reset_mac_header(skb);\n\tskb_pull(skb, MLX5_IPOIB_HARD_LEN);\n\n\tskb->dev = netdev;\n\n\tstats->packets++;\n\tstats->bytes += cqe_bcnt;\n}\n\nstatic void mlx5i_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tstruct mlx5e_wqe_frag_info *wi;\n\tstruct sk_buff *skb;\n\tu32 cqe_bcnt;\n\tu16 ci;\n\n\tci       = mlx5_wq_cyc_ctr2ix(wq, be16_to_cpu(cqe->wqe_counter));\n\twi       = get_frag(rq, ci);\n\tcqe_bcnt = be32_to_cpu(cqe->byte_cnt);\n\n\tif (unlikely(MLX5E_RX_ERR_CQE(cqe))) {\n\t\trq->stats->wqe_err++;\n\t\tgoto wq_cyc_pop;\n\t}\n\n\tskb = INDIRECT_CALL_2(rq->wqe.skb_from_cqe,\n\t\t\t      mlx5e_skb_from_cqe_linear,\n\t\t\t      mlx5e_skb_from_cqe_nonlinear,\n\t\t\t      rq, wi, cqe, cqe_bcnt);\n\tif (!skb)\n\t\tgoto wq_cyc_pop;\n\n\tmlx5i_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);\n\tif (unlikely(!skb->dev)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tgoto wq_cyc_pop;\n\t}\n\tnapi_gro_receive(rq->cq.napi, skb);\n\nwq_cyc_pop:\n\tmlx5_wq_cyc_pop(wq);\n}\n\nconst struct mlx5e_rx_handlers mlx5i_rx_handlers = {\n\t.handle_rx_cqe       = mlx5i_handle_rx_cqe,\n\t.handle_rx_cqe_mpwqe = NULL,  \n};\n#endif  \n\nint mlx5e_rq_set_handlers(struct mlx5e_rq *rq, struct mlx5e_params *params, bool xsk)\n{\n\tstruct net_device *netdev = rq->netdev;\n\tstruct mlx5_core_dev *mdev = rq->mdev;\n\tstruct mlx5e_priv *priv = rq->priv;\n\n\tswitch (rq->wq_type) {\n\tcase MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:\n\t\trq->mpwqe.skb_from_cqe_mpwrq = xsk ?\n\t\t\tmlx5e_xsk_skb_from_cqe_mpwrq_linear :\n\t\t\tmlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL) ?\n\t\t\t\tmlx5e_skb_from_cqe_mpwrq_linear :\n\t\t\t\tmlx5e_skb_from_cqe_mpwrq_nonlinear;\n\t\trq->post_wqes = mlx5e_post_rx_mpwqes;\n\t\trq->dealloc_wqe = mlx5e_dealloc_rx_mpwqe;\n\n\t\tif (params->packet_merge.type == MLX5E_PACKET_MERGE_SHAMPO) {\n\t\t\trq->handle_rx_cqe = priv->profile->rx_handlers->handle_rx_cqe_mpwqe_shampo;\n\t\t\tif (!rq->handle_rx_cqe) {\n\t\t\t\tnetdev_err(netdev, \"RX handler of SHAMPO MPWQE RQ is not set\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\trq->handle_rx_cqe = priv->profile->rx_handlers->handle_rx_cqe_mpwqe;\n\t\t\tif (!rq->handle_rx_cqe) {\n\t\t\t\tnetdev_err(netdev, \"RX handler of MPWQE RQ is not set\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tbreak;\n\tdefault:  \n\t\trq->wqe.skb_from_cqe = xsk ?\n\t\t\tmlx5e_xsk_skb_from_cqe_linear :\n\t\t\tmlx5e_rx_is_linear_skb(mdev, params, NULL) ?\n\t\t\t\tmlx5e_skb_from_cqe_linear :\n\t\t\t\tmlx5e_skb_from_cqe_nonlinear;\n\t\trq->post_wqes = mlx5e_post_rx_wqes;\n\t\trq->dealloc_wqe = mlx5e_dealloc_rx_wqe;\n\t\trq->handle_rx_cqe = priv->profile->rx_handlers->handle_rx_cqe;\n\t\tif (!rq->handle_rx_cqe) {\n\t\t\tnetdev_err(netdev, \"RX handler of RQ is not set\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void mlx5e_trap_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\tstruct mlx5_wq_cyc *wq = &rq->wqe.wq;\n\tstruct mlx5e_wqe_frag_info *wi;\n\tstruct sk_buff *skb;\n\tu32 cqe_bcnt;\n\tu16 trap_id;\n\tu16 ci;\n\n\ttrap_id  = get_cqe_flow_tag(cqe);\n\tci       = mlx5_wq_cyc_ctr2ix(wq, be16_to_cpu(cqe->wqe_counter));\n\twi       = get_frag(rq, ci);\n\tcqe_bcnt = be32_to_cpu(cqe->byte_cnt);\n\n\tif (unlikely(MLX5E_RX_ERR_CQE(cqe))) {\n\t\trq->stats->wqe_err++;\n\t\tgoto wq_cyc_pop;\n\t}\n\n\tskb = mlx5e_skb_from_cqe_nonlinear(rq, wi, cqe, cqe_bcnt);\n\tif (!skb)\n\t\tgoto wq_cyc_pop;\n\n\tmlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);\n\tskb_push(skb, ETH_HLEN);\n\n\tmlx5_devlink_trap_report(rq->mdev, trap_id, skb,\n\t\t\t\t rq->netdev->devlink_port);\n\tdev_kfree_skb_any(skb);\n\nwq_cyc_pop:\n\tmlx5_wq_cyc_pop(wq);\n}\n\nvoid mlx5e_rq_set_trap_handlers(struct mlx5e_rq *rq, struct mlx5e_params *params)\n{\n\trq->wqe.skb_from_cqe = mlx5e_rx_is_linear_skb(rq->mdev, params, NULL) ?\n\t\t\t       mlx5e_skb_from_cqe_linear :\n\t\t\t       mlx5e_skb_from_cqe_nonlinear;\n\trq->post_wqes = mlx5e_post_rx_wqes;\n\trq->dealloc_wqe = mlx5e_dealloc_rx_wqe;\n\trq->handle_rx_cqe = mlx5e_trap_handle_rx_cqe;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}