{
  "module_name": "dr_send.c",
  "hash_id": "0b4e179ab17a694d3f9bdb9cbbf352cdf934c59c4c7b60d48fce273e25c48baa",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/steering/dr_send.c",
  "human_readable_source": "\n \n\n#include <linux/smp.h>\n#include \"dr_types.h\"\n\n#define QUEUE_SIZE 128\n#define SIGNAL_PER_DIV_QUEUE 16\n#define TH_NUMS_TO_DRAIN 2\n#define DR_SEND_INFO_POOL_SIZE 1000\n\nenum { CQ_OK = 0, CQ_EMPTY = -1, CQ_POLL_ERR = -2 };\n\nstruct dr_data_seg {\n\tu64 addr;\n\tu32 length;\n\tu32 lkey;\n\tunsigned int send_flags;\n};\n\nenum send_info_type {\n\tWRITE_ICM = 0,\n\tGTA_ARG   = 1,\n};\n\nstruct postsend_info {\n\tenum send_info_type type;\n\tstruct dr_data_seg write;\n\tstruct dr_data_seg read;\n\tu64 remote_addr;\n\tu32 rkey;\n};\n\nstruct dr_qp_rtr_attr {\n\tstruct mlx5dr_cmd_gid_attr dgid_attr;\n\tenum ib_mtu mtu;\n\tu32 qp_num;\n\tu16 port_num;\n\tu8 min_rnr_timer;\n\tu8 sgid_index;\n\tu16 udp_src_port;\n\tu8 fl:1;\n};\n\nstruct dr_qp_rts_attr {\n\tu8 timeout;\n\tu8 retry_cnt;\n\tu8 rnr_retry;\n};\n\nstruct dr_qp_init_attr {\n\tu32 cqn;\n\tu32 pdn;\n\tu32 max_send_wr;\n\tstruct mlx5_uars_page *uar;\n\tu8 isolate_vl_tc:1;\n};\n\nstruct mlx5dr_send_info_pool_obj {\n\tstruct mlx5dr_ste_send_info ste_send_info;\n\tstruct mlx5dr_send_info_pool *pool;\n\tstruct list_head list_node;\n};\n\nstruct mlx5dr_send_info_pool {\n\tstruct list_head free_list;\n};\n\nstatic int dr_send_info_pool_fill(struct mlx5dr_send_info_pool *pool)\n{\n\tstruct mlx5dr_send_info_pool_obj *pool_obj, *tmp_pool_obj;\n\tint i;\n\n\tfor (i = 0; i < DR_SEND_INFO_POOL_SIZE; i++) {\n\t\tpool_obj = kzalloc(sizeof(*pool_obj), GFP_KERNEL);\n\t\tif (!pool_obj)\n\t\t\tgoto clean_pool;\n\n\t\tpool_obj->pool = pool;\n\t\tlist_add_tail(&pool_obj->list_node, &pool->free_list);\n\t}\n\n\treturn 0;\n\nclean_pool:\n\tlist_for_each_entry_safe(pool_obj, tmp_pool_obj, &pool->free_list, list_node) {\n\t\tlist_del(&pool_obj->list_node);\n\t\tkfree(pool_obj);\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic void dr_send_info_pool_destroy(struct mlx5dr_send_info_pool *pool)\n{\n\tstruct mlx5dr_send_info_pool_obj *pool_obj, *tmp_pool_obj;\n\n\tlist_for_each_entry_safe(pool_obj, tmp_pool_obj, &pool->free_list, list_node) {\n\t\tlist_del(&pool_obj->list_node);\n\t\tkfree(pool_obj);\n\t}\n\n\tkfree(pool);\n}\n\nvoid mlx5dr_send_info_pool_destroy(struct mlx5dr_domain *dmn)\n{\n\tdr_send_info_pool_destroy(dmn->send_info_pool_tx);\n\tdr_send_info_pool_destroy(dmn->send_info_pool_rx);\n}\n\nstatic struct mlx5dr_send_info_pool *dr_send_info_pool_create(void)\n{\n\tstruct mlx5dr_send_info_pool *pool;\n\tint ret;\n\n\tpool = kzalloc(sizeof(*pool), GFP_KERNEL);\n\tif (!pool)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&pool->free_list);\n\n\tret = dr_send_info_pool_fill(pool);\n\tif (ret) {\n\t\tkfree(pool);\n\t\treturn NULL;\n\t}\n\n\treturn pool;\n}\n\nint mlx5dr_send_info_pool_create(struct mlx5dr_domain *dmn)\n{\n\tdmn->send_info_pool_rx = dr_send_info_pool_create();\n\tif (!dmn->send_info_pool_rx)\n\t\treturn -ENOMEM;\n\n\tdmn->send_info_pool_tx = dr_send_info_pool_create();\n\tif (!dmn->send_info_pool_tx) {\n\t\tdr_send_info_pool_destroy(dmn->send_info_pool_rx);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstruct mlx5dr_ste_send_info\n*mlx5dr_send_info_alloc(struct mlx5dr_domain *dmn,\n\t\t\tenum mlx5dr_domain_nic_type nic_type)\n{\n\tstruct mlx5dr_send_info_pool_obj *pool_obj;\n\tstruct mlx5dr_send_info_pool *pool;\n\tint ret;\n\n\tpool = nic_type == DR_DOMAIN_NIC_TYPE_RX ? dmn->send_info_pool_rx :\n\t\t\t\t\t\t   dmn->send_info_pool_tx;\n\n\tif (unlikely(list_empty(&pool->free_list))) {\n\t\tret = dr_send_info_pool_fill(pool);\n\t\tif (ret)\n\t\t\treturn NULL;\n\t}\n\n\tpool_obj = list_first_entry_or_null(&pool->free_list,\n\t\t\t\t\t    struct mlx5dr_send_info_pool_obj,\n\t\t\t\t\t    list_node);\n\n\tif (likely(pool_obj)) {\n\t\tlist_del_init(&pool_obj->list_node);\n\t} else {\n\t\tWARN_ONCE(!pool_obj, \"Failed getting ste send info obj from pool\");\n\t\treturn NULL;\n\t}\n\n\treturn &pool_obj->ste_send_info;\n}\n\nvoid mlx5dr_send_info_free(struct mlx5dr_ste_send_info *ste_send_info)\n{\n\tstruct mlx5dr_send_info_pool_obj *pool_obj;\n\n\tpool_obj = container_of(ste_send_info,\n\t\t\t\tstruct mlx5dr_send_info_pool_obj,\n\t\t\t\tste_send_info);\n\n\tlist_add(&pool_obj->list_node, &pool_obj->pool->free_list);\n}\n\nstatic int dr_parse_cqe(struct mlx5dr_cq *dr_cq, struct mlx5_cqe64 *cqe64)\n{\n\tunsigned int idx;\n\tu8 opcode;\n\n\topcode = get_cqe_opcode(cqe64);\n\tif (opcode == MLX5_CQE_REQ_ERR) {\n\t\tidx = be16_to_cpu(cqe64->wqe_counter) &\n\t\t\t(dr_cq->qp->sq.wqe_cnt - 1);\n\t\tdr_cq->qp->sq.cc = dr_cq->qp->sq.wqe_head[idx] + 1;\n\t} else if (opcode == MLX5_CQE_RESP_ERR) {\n\t\t++dr_cq->qp->sq.cc;\n\t} else {\n\t\tidx = be16_to_cpu(cqe64->wqe_counter) &\n\t\t\t(dr_cq->qp->sq.wqe_cnt - 1);\n\t\tdr_cq->qp->sq.cc = dr_cq->qp->sq.wqe_head[idx] + 1;\n\n\t\treturn CQ_OK;\n\t}\n\n\treturn CQ_POLL_ERR;\n}\n\nstatic int dr_cq_poll_one(struct mlx5dr_cq *dr_cq)\n{\n\tstruct mlx5_cqe64 *cqe64;\n\tint err;\n\n\tcqe64 = mlx5_cqwq_get_cqe(&dr_cq->wq);\n\tif (!cqe64) {\n\t\tif (unlikely(dr_cq->mdev->state ==\n\t\t\t     MLX5_DEVICE_STATE_INTERNAL_ERROR)) {\n\t\t\tmlx5_core_dbg_once(dr_cq->mdev,\n\t\t\t\t\t   \"Polling CQ while device is shutting down\\n\");\n\t\t\treturn CQ_POLL_ERR;\n\t\t}\n\t\treturn CQ_EMPTY;\n\t}\n\n\tmlx5_cqwq_pop(&dr_cq->wq);\n\terr = dr_parse_cqe(dr_cq, cqe64);\n\tmlx5_cqwq_update_db_record(&dr_cq->wq);\n\n\treturn err;\n}\n\nstatic int dr_poll_cq(struct mlx5dr_cq *dr_cq, int ne)\n{\n\tint npolled;\n\tint err = 0;\n\n\tfor (npolled = 0; npolled < ne; ++npolled) {\n\t\terr = dr_cq_poll_one(dr_cq);\n\t\tif (err != CQ_OK)\n\t\t\tbreak;\n\t}\n\n\treturn err == CQ_POLL_ERR ? err : npolled;\n}\n\nstatic struct mlx5dr_qp *dr_create_rc_qp(struct mlx5_core_dev *mdev,\n\t\t\t\t\t struct dr_qp_init_attr *attr)\n{\n\tu32 out[MLX5_ST_SZ_DW(create_qp_out)] = {};\n\tu32 temp_qpc[MLX5_ST_SZ_DW(qpc)] = {};\n\tstruct mlx5_wq_param wqp;\n\tstruct mlx5dr_qp *dr_qp;\n\tint inlen;\n\tvoid *qpc;\n\tvoid *in;\n\tint err;\n\n\tdr_qp = kzalloc(sizeof(*dr_qp), GFP_KERNEL);\n\tif (!dr_qp)\n\t\treturn NULL;\n\n\twqp.buf_numa_node = mdev->priv.numa_node;\n\twqp.db_numa_node = mdev->priv.numa_node;\n\n\tdr_qp->rq.pc = 0;\n\tdr_qp->rq.cc = 0;\n\tdr_qp->rq.wqe_cnt = 256;\n\tdr_qp->sq.pc = 0;\n\tdr_qp->sq.cc = 0;\n\tdr_qp->sq.head = 0;\n\tdr_qp->sq.wqe_cnt = roundup_pow_of_two(attr->max_send_wr);\n\n\tMLX5_SET(qpc, temp_qpc, log_rq_stride, ilog2(MLX5_SEND_WQE_DS) - 4);\n\tMLX5_SET(qpc, temp_qpc, log_rq_size, ilog2(dr_qp->rq.wqe_cnt));\n\tMLX5_SET(qpc, temp_qpc, log_sq_size, ilog2(dr_qp->sq.wqe_cnt));\n\terr = mlx5_wq_qp_create(mdev, &wqp, temp_qpc, &dr_qp->wq,\n\t\t\t\t&dr_qp->wq_ctrl);\n\tif (err) {\n\t\tmlx5_core_warn(mdev, \"Can't create QP WQ\\n\");\n\t\tgoto err_wq;\n\t}\n\n\tdr_qp->sq.wqe_head = kcalloc(dr_qp->sq.wqe_cnt,\n\t\t\t\t     sizeof(dr_qp->sq.wqe_head[0]),\n\t\t\t\t     GFP_KERNEL);\n\n\tif (!dr_qp->sq.wqe_head) {\n\t\tmlx5_core_warn(mdev, \"Can't allocate wqe head\\n\");\n\t\tgoto err_wqe_head;\n\t}\n\n\tinlen = MLX5_ST_SZ_BYTES(create_qp_in) +\n\t\tMLX5_FLD_SZ_BYTES(create_qp_in, pas[0]) *\n\t\tdr_qp->wq_ctrl.buf.npages;\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_in;\n\t}\n\n\tqpc = MLX5_ADDR_OF(create_qp_in, in, qpc);\n\tMLX5_SET(qpc, qpc, st, MLX5_QP_ST_RC);\n\tMLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);\n\tMLX5_SET(qpc, qpc, isolate_vl_tc, attr->isolate_vl_tc);\n\tMLX5_SET(qpc, qpc, pd, attr->pdn);\n\tMLX5_SET(qpc, qpc, uar_page, attr->uar->index);\n\tMLX5_SET(qpc, qpc, log_page_size,\n\t\t dr_qp->wq_ctrl.buf.page_shift - MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET(qpc, qpc, fre, 1);\n\tMLX5_SET(qpc, qpc, rlky, 1);\n\tMLX5_SET(qpc, qpc, cqn_snd, attr->cqn);\n\tMLX5_SET(qpc, qpc, cqn_rcv, attr->cqn);\n\tMLX5_SET(qpc, qpc, log_rq_stride, ilog2(MLX5_SEND_WQE_DS) - 4);\n\tMLX5_SET(qpc, qpc, log_rq_size, ilog2(dr_qp->rq.wqe_cnt));\n\tMLX5_SET(qpc, qpc, rq_type, MLX5_NON_ZERO_RQ);\n\tMLX5_SET(qpc, qpc, log_sq_size, ilog2(dr_qp->sq.wqe_cnt));\n\tMLX5_SET(qpc, qpc, ts_format, mlx5_get_qp_default_ts(mdev));\n\tMLX5_SET64(qpc, qpc, dbr_addr, dr_qp->wq_ctrl.db.dma);\n\tif (MLX5_CAP_GEN(mdev, cqe_version) == 1)\n\t\tMLX5_SET(qpc, qpc, user_index, 0xFFFFFF);\n\tmlx5_fill_page_frag_array(&dr_qp->wq_ctrl.buf,\n\t\t\t\t  (__be64 *)MLX5_ADDR_OF(create_qp_in,\n\t\t\t\t\t\t\t in, pas));\n\n\tMLX5_SET(create_qp_in, in, opcode, MLX5_CMD_OP_CREATE_QP);\n\terr = mlx5_cmd_exec(mdev, in, inlen, out, sizeof(out));\n\tdr_qp->qpn = MLX5_GET(create_qp_out, out, qpn);\n\tkvfree(in);\n\tif (err)\n\t\tgoto err_in;\n\tdr_qp->uar = attr->uar;\n\n\treturn dr_qp;\n\nerr_in:\n\tkfree(dr_qp->sq.wqe_head);\nerr_wqe_head:\n\tmlx5_wq_destroy(&dr_qp->wq_ctrl);\nerr_wq:\n\tkfree(dr_qp);\n\treturn NULL;\n}\n\nstatic void dr_destroy_qp(struct mlx5_core_dev *mdev,\n\t\t\t  struct mlx5dr_qp *dr_qp)\n{\n\tu32 in[MLX5_ST_SZ_DW(destroy_qp_in)] = {};\n\n\tMLX5_SET(destroy_qp_in, in, opcode, MLX5_CMD_OP_DESTROY_QP);\n\tMLX5_SET(destroy_qp_in, in, qpn, dr_qp->qpn);\n\tmlx5_cmd_exec_in(mdev, destroy_qp, in);\n\n\tkfree(dr_qp->sq.wqe_head);\n\tmlx5_wq_destroy(&dr_qp->wq_ctrl);\n\tkfree(dr_qp);\n}\n\nstatic void dr_cmd_notify_hw(struct mlx5dr_qp *dr_qp, void *ctrl)\n{\n\tdma_wmb();\n\t*dr_qp->wq.sq.db = cpu_to_be32(dr_qp->sq.pc & 0xffff);\n\n\t \n\twmb();\n\n\tmlx5_write64(ctrl, dr_qp->uar->map + MLX5_BF_OFFSET);\n}\n\nstatic void\ndr_rdma_handle_flow_access_arg_segments(struct mlx5_wqe_ctrl_seg *wq_ctrl,\n\t\t\t\t\tu32 remote_addr,\n\t\t\t\t\tstruct dr_data_seg *data_seg,\n\t\t\t\t\tint *size)\n{\n\tstruct mlx5_wqe_header_modify_argument_update_seg *wq_arg_seg;\n\tstruct mlx5_wqe_flow_update_ctrl_seg *wq_flow_seg;\n\n\twq_ctrl->general_id = cpu_to_be32(remote_addr);\n\twq_flow_seg = (void *)(wq_ctrl + 1);\n\n\t \n\tmemset(wq_flow_seg, 0, sizeof(*wq_flow_seg));\n\twq_arg_seg = (void *)(wq_flow_seg + 1);\n\n\tmemcpy(wq_arg_seg->argument_list,\n\t       (void *)(uintptr_t)data_seg->addr,\n\t       data_seg->length);\n\n\t*size = (sizeof(*wq_ctrl) +       \n\t\t sizeof(*wq_flow_seg) +   \n\t\t sizeof(*wq_arg_seg)) /   \n\t\tMLX5_SEND_WQE_DS;\n}\n\nstatic void\ndr_rdma_handle_icm_write_segments(struct mlx5_wqe_ctrl_seg *wq_ctrl,\n\t\t\t\t  u64 remote_addr,\n\t\t\t\t  u32 rkey,\n\t\t\t\t  struct dr_data_seg *data_seg,\n\t\t\t\t  unsigned int *size)\n{\n\tstruct mlx5_wqe_raddr_seg *wq_raddr;\n\tstruct mlx5_wqe_data_seg *wq_dseg;\n\n\twq_raddr = (void *)(wq_ctrl + 1);\n\n\twq_raddr->raddr = cpu_to_be64(remote_addr);\n\twq_raddr->rkey = cpu_to_be32(rkey);\n\twq_raddr->reserved = 0;\n\n\twq_dseg = (void *)(wq_raddr + 1);\n\n\twq_dseg->byte_count = cpu_to_be32(data_seg->length);\n\twq_dseg->lkey = cpu_to_be32(data_seg->lkey);\n\twq_dseg->addr = cpu_to_be64(data_seg->addr);\n\n\t*size = (sizeof(*wq_ctrl) +     \n\t\t sizeof(*wq_dseg) +     \n\t\t sizeof(*wq_raddr)) /   \n\t\tMLX5_SEND_WQE_DS;\n}\n\nstatic void dr_set_ctrl_seg(struct mlx5_wqe_ctrl_seg *wq_ctrl,\n\t\t\t    struct dr_data_seg *data_seg)\n{\n\twq_ctrl->signature = 0;\n\twq_ctrl->rsvd[0] = 0;\n\twq_ctrl->rsvd[1] = 0;\n\twq_ctrl->fm_ce_se = data_seg->send_flags & IB_SEND_SIGNALED ?\n\t\t\t\tMLX5_WQE_CTRL_CQ_UPDATE : 0;\n\twq_ctrl->imm = 0;\n}\n\nstatic void dr_rdma_segments(struct mlx5dr_qp *dr_qp, u64 remote_addr,\n\t\t\t     u32 rkey, struct dr_data_seg *data_seg,\n\t\t\t     u32 opcode, bool notify_hw)\n{\n\tstruct mlx5_wqe_ctrl_seg *wq_ctrl;\n\tint opcode_mod = 0;\n\tunsigned int size;\n\tunsigned int idx;\n\n\tidx = dr_qp->sq.pc & (dr_qp->sq.wqe_cnt - 1);\n\n\twq_ctrl = mlx5_wq_cyc_get_wqe(&dr_qp->wq.sq, idx);\n\tdr_set_ctrl_seg(wq_ctrl, data_seg);\n\n\tswitch (opcode) {\n\tcase MLX5_OPCODE_RDMA_READ:\n\tcase MLX5_OPCODE_RDMA_WRITE:\n\t\tdr_rdma_handle_icm_write_segments(wq_ctrl, remote_addr,\n\t\t\t\t\t\t  rkey, data_seg, &size);\n\t\tbreak;\n\tcase MLX5_OPCODE_FLOW_TBL_ACCESS:\n\t\topcode_mod = MLX5_CMD_OP_MOD_UPDATE_HEADER_MODIFY_ARGUMENT;\n\t\tdr_rdma_handle_flow_access_arg_segments(wq_ctrl, remote_addr,\n\t\t\t\t\t\t\tdata_seg, &size);\n\t\tbreak;\n\tdefault:\n\t\tWARN(true, \"illegal opcode %d\", opcode);\n\t\treturn;\n\t}\n\n\t \n\twq_ctrl->opmod_idx_opcode =\n\t\tcpu_to_be32((opcode_mod << 24) |\n\t\t\t    ((dr_qp->sq.pc & 0xffff) << 8) |\n\t\t\t    opcode);\n\twq_ctrl->qpn_ds = cpu_to_be32(size | dr_qp->qpn << 8);\n\n\tdr_qp->sq.pc += DIV_ROUND_UP(size * 16, MLX5_SEND_WQE_BB);\n\tdr_qp->sq.wqe_head[idx] = dr_qp->sq.head++;\n\n\tif (notify_hw)\n\t\tdr_cmd_notify_hw(dr_qp, wq_ctrl);\n}\n\nstatic void dr_post_send(struct mlx5dr_qp *dr_qp, struct postsend_info *send_info)\n{\n\tif (send_info->type == WRITE_ICM) {\n\t\tdr_rdma_segments(dr_qp, send_info->remote_addr, send_info->rkey,\n\t\t\t\t &send_info->write, MLX5_OPCODE_RDMA_WRITE, false);\n\t\tdr_rdma_segments(dr_qp, send_info->remote_addr, send_info->rkey,\n\t\t\t\t &send_info->read, MLX5_OPCODE_RDMA_READ, true);\n\t} else {  \n\t\tdr_rdma_segments(dr_qp, send_info->remote_addr, send_info->rkey,\n\t\t\t\t &send_info->write, MLX5_OPCODE_FLOW_TBL_ACCESS, true);\n\t}\n\n}\n\n \nvoid mlx5dr_send_fill_and_append_ste_send_info(struct mlx5dr_ste *ste, u16 size,\n\t\t\t\t\t       u16 offset, u8 *data,\n\t\t\t\t\t       struct mlx5dr_ste_send_info *ste_info,\n\t\t\t\t\t       struct list_head *send_list,\n\t\t\t\t\t       bool copy_data)\n{\n\tste_info->size = size;\n\tste_info->ste = ste;\n\tste_info->offset = offset;\n\n\tif (copy_data) {\n\t\tmemcpy(ste_info->data_cont, data, size);\n\t\tste_info->data = ste_info->data_cont;\n\t} else {\n\t\tste_info->data = data;\n\t}\n\n\tlist_add_tail(&ste_info->send_list, send_list);\n}\n\n \nstatic int dr_handle_pending_wc(struct mlx5dr_domain *dmn,\n\t\t\t\tstruct mlx5dr_send_ring *send_ring)\n{\n\tbool is_drain = false;\n\tint ne;\n\n\tif (send_ring->pending_wqe < send_ring->signal_th)\n\t\treturn 0;\n\n\t \n\tif (send_ring->pending_wqe >=\n\t    dmn->send_ring->signal_th * TH_NUMS_TO_DRAIN)\n\t\tis_drain = true;\n\n\tdo {\n\t\tne = dr_poll_cq(send_ring->cq, 1);\n\t\tif (unlikely(ne < 0)) {\n\t\t\tmlx5_core_warn_once(dmn->mdev, \"SMFS QPN 0x%x is disabled/limited\",\n\t\t\t\t\t    send_ring->qp->qpn);\n\t\t\tsend_ring->err_state = true;\n\t\t\treturn ne;\n\t\t} else if (ne == 1) {\n\t\t\tsend_ring->pending_wqe -= send_ring->signal_th;\n\t\t}\n\t} while (ne == 1 ||\n\t\t (is_drain && send_ring->pending_wqe  >= send_ring->signal_th));\n\n\treturn 0;\n}\n\nstatic void dr_fill_write_args_segs(struct mlx5dr_send_ring *send_ring,\n\t\t\t\t    struct postsend_info *send_info)\n{\n\tsend_ring->pending_wqe++;\n\n\tif (send_ring->pending_wqe % send_ring->signal_th == 0)\n\t\tsend_info->write.send_flags |= IB_SEND_SIGNALED;\n\telse\n\t\tsend_info->write.send_flags = 0;\n}\n\nstatic void dr_fill_write_icm_segs(struct mlx5dr_domain *dmn,\n\t\t\t\t   struct mlx5dr_send_ring *send_ring,\n\t\t\t\t   struct postsend_info *send_info)\n{\n\tu32 buff_offset;\n\n\tif (send_info->write.length > dmn->info.max_inline_size) {\n\t\tbuff_offset = (send_ring->tx_head &\n\t\t\t       (dmn->send_ring->signal_th - 1)) *\n\t\t\t      send_ring->max_post_send_size;\n\t\t \n\t\tmemcpy(send_ring->buf + buff_offset,\n\t\t       (void *)(uintptr_t)send_info->write.addr,\n\t\t       send_info->write.length);\n\t\tsend_info->write.addr = (uintptr_t)send_ring->mr->dma_addr + buff_offset;\n\t\tsend_info->write.lkey = send_ring->mr->mkey;\n\n\t\tsend_ring->tx_head++;\n\t}\n\n\tsend_ring->pending_wqe++;\n\n\tif (send_ring->pending_wqe % send_ring->signal_th == 0)\n\t\tsend_info->write.send_flags |= IB_SEND_SIGNALED;\n\n\tsend_ring->pending_wqe++;\n\tsend_info->read.length = send_info->write.length;\n\n\t \n\tsend_info->read.addr = (uintptr_t)send_ring->sync_mr->dma_addr;\n\tsend_info->read.lkey = send_ring->sync_mr->mkey;\n\n\tif (send_ring->pending_wqe % send_ring->signal_th == 0)\n\t\tsend_info->read.send_flags = IB_SEND_SIGNALED;\n\telse\n\t\tsend_info->read.send_flags = 0;\n}\n\nstatic void dr_fill_data_segs(struct mlx5dr_domain *dmn,\n\t\t\t      struct mlx5dr_send_ring *send_ring,\n\t\t\t      struct postsend_info *send_info)\n{\n\tif (send_info->type == WRITE_ICM)\n\t\tdr_fill_write_icm_segs(dmn, send_ring, send_info);\n\telse  \n\t\tdr_fill_write_args_segs(send_ring, send_info);\n}\n\nstatic int dr_postsend_icm_data(struct mlx5dr_domain *dmn,\n\t\t\t\tstruct postsend_info *send_info)\n{\n\tstruct mlx5dr_send_ring *send_ring = dmn->send_ring;\n\tint ret;\n\n\tif (unlikely(dmn->mdev->state == MLX5_DEVICE_STATE_INTERNAL_ERROR ||\n\t\t     send_ring->err_state)) {\n\t\tmlx5_core_dbg_once(dmn->mdev,\n\t\t\t\t   \"Skipping post send: QP err state: %d, device state: %d\\n\",\n\t\t\t\t   send_ring->err_state, dmn->mdev->state);\n\t\treturn 0;\n\t}\n\n\tspin_lock(&send_ring->lock);\n\n\tret = dr_handle_pending_wc(dmn, send_ring);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tdr_fill_data_segs(dmn, send_ring, send_info);\n\tdr_post_send(send_ring->qp, send_info);\n\nout_unlock:\n\tspin_unlock(&send_ring->lock);\n\treturn ret;\n}\n\nstatic int dr_get_tbl_copy_details(struct mlx5dr_domain *dmn,\n\t\t\t\t   struct mlx5dr_ste_htbl *htbl,\n\t\t\t\t   u8 **data,\n\t\t\t\t   u32 *byte_size,\n\t\t\t\t   int *iterations,\n\t\t\t\t   int *num_stes)\n{\n\tu32 chunk_byte_size = mlx5dr_icm_pool_get_chunk_byte_size(htbl->chunk);\n\tint alloc_size;\n\n\tif (chunk_byte_size > dmn->send_ring->max_post_send_size) {\n\t\t*iterations = chunk_byte_size / dmn->send_ring->max_post_send_size;\n\t\t*byte_size = dmn->send_ring->max_post_send_size;\n\t\talloc_size = *byte_size;\n\t\t*num_stes = *byte_size / DR_STE_SIZE;\n\t} else {\n\t\t*iterations = 1;\n\t\t*num_stes = mlx5dr_icm_pool_get_chunk_num_of_entries(htbl->chunk);\n\t\talloc_size = *num_stes * DR_STE_SIZE;\n\t}\n\n\t*data = kvzalloc(alloc_size, GFP_KERNEL);\n\tif (!*data)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nint mlx5dr_send_postsend_ste(struct mlx5dr_domain *dmn, struct mlx5dr_ste *ste,\n\t\t\t     u8 *data, u16 size, u16 offset)\n{\n\tstruct postsend_info send_info = {};\n\n\tmlx5dr_ste_prepare_for_postsend(dmn->ste_ctx, data, size);\n\n\tsend_info.write.addr = (uintptr_t)data;\n\tsend_info.write.length = size;\n\tsend_info.write.lkey = 0;\n\tsend_info.remote_addr = mlx5dr_ste_get_mr_addr(ste) + offset;\n\tsend_info.rkey = mlx5dr_icm_pool_get_chunk_rkey(ste->htbl->chunk);\n\n\treturn dr_postsend_icm_data(dmn, &send_info);\n}\n\nint mlx5dr_send_postsend_htbl(struct mlx5dr_domain *dmn,\n\t\t\t      struct mlx5dr_ste_htbl *htbl,\n\t\t\t      u8 *formatted_ste, u8 *mask)\n{\n\tu32 byte_size = mlx5dr_icm_pool_get_chunk_byte_size(htbl->chunk);\n\tint num_stes_per_iter;\n\tint iterations;\n\tu8 *data;\n\tint ret;\n\tint i;\n\tint j;\n\n\tret = dr_get_tbl_copy_details(dmn, htbl, &data, &byte_size,\n\t\t\t\t      &iterations, &num_stes_per_iter);\n\tif (ret)\n\t\treturn ret;\n\n\tmlx5dr_ste_prepare_for_postsend(dmn->ste_ctx, formatted_ste, DR_STE_SIZE);\n\n\t \n\tfor (i = 0; i < iterations; i++) {\n\t\tu32 ste_index = i * (byte_size / DR_STE_SIZE);\n\t\tstruct postsend_info send_info = {};\n\n\t\t \n\t\tfor (j = 0; j < num_stes_per_iter; j++) {\n\t\t\tstruct mlx5dr_ste *ste = &htbl->chunk->ste_arr[ste_index + j];\n\t\t\tu32 ste_off = j * DR_STE_SIZE;\n\n\t\t\tif (mlx5dr_ste_is_not_used(ste)) {\n\t\t\t\tmemcpy(data + ste_off,\n\t\t\t\t       formatted_ste, DR_STE_SIZE);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tmemcpy(data + ste_off,\n\t\t\t\t       htbl->chunk->hw_ste_arr +\n\t\t\t\t       DR_STE_SIZE_REDUCED * (ste_index + j),\n\t\t\t\t       DR_STE_SIZE_REDUCED);\n\t\t\t\t \n\t\t\t\tmemcpy(data + ste_off + DR_STE_SIZE_REDUCED,\n\t\t\t\t       mask, DR_STE_SIZE_MASK);\n\t\t\t\t \n\t\t\t\tmlx5dr_ste_prepare_for_postsend(dmn->ste_ctx,\n\t\t\t\t\t\t\t\tdata + (j * DR_STE_SIZE),\n\t\t\t\t\t\t\t\tDR_STE_SIZE);\n\t\t\t}\n\t\t}\n\n\t\tsend_info.write.addr = (uintptr_t)data;\n\t\tsend_info.write.length = byte_size;\n\t\tsend_info.write.lkey = 0;\n\t\tsend_info.remote_addr =\n\t\t\tmlx5dr_ste_get_mr_addr(htbl->chunk->ste_arr + ste_index);\n\t\tsend_info.rkey = mlx5dr_icm_pool_get_chunk_rkey(htbl->chunk);\n\n\t\tret = dr_postsend_icm_data(dmn, &send_info);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t}\n\nout_free:\n\tkvfree(data);\n\treturn ret;\n}\n\n \nint mlx5dr_send_postsend_formatted_htbl(struct mlx5dr_domain *dmn,\n\t\t\t\t\tstruct mlx5dr_ste_htbl *htbl,\n\t\t\t\t\tu8 *ste_init_data,\n\t\t\t\t\tbool update_hw_ste)\n{\n\tu32 byte_size = mlx5dr_icm_pool_get_chunk_byte_size(htbl->chunk);\n\tint iterations;\n\tint num_stes;\n\tu8 *copy_dst;\n\tu8 *data;\n\tint ret;\n\tint i;\n\n\tret = dr_get_tbl_copy_details(dmn, htbl, &data, &byte_size,\n\t\t\t\t      &iterations, &num_stes);\n\tif (ret)\n\t\treturn ret;\n\n\tif (update_hw_ste) {\n\t\t \n\t\tfor (i = 0; i < num_stes; i++) {\n\t\t\tcopy_dst = htbl->chunk->hw_ste_arr + i * DR_STE_SIZE_REDUCED;\n\t\t\tmemcpy(copy_dst, ste_init_data, DR_STE_SIZE_REDUCED);\n\t\t}\n\t}\n\n\tmlx5dr_ste_prepare_for_postsend(dmn->ste_ctx, ste_init_data, DR_STE_SIZE);\n\n\t \n\tfor (i = 0; i < num_stes; i++) {\n\t\tcopy_dst = data + i * DR_STE_SIZE;\n\t\tmemcpy(copy_dst, ste_init_data, DR_STE_SIZE);\n\t}\n\n\t \n\tfor (i = 0; i < iterations; i++) {\n\t\tu8 ste_index = i * (byte_size / DR_STE_SIZE);\n\t\tstruct postsend_info send_info = {};\n\n\t\tsend_info.write.addr = (uintptr_t)data;\n\t\tsend_info.write.length = byte_size;\n\t\tsend_info.write.lkey = 0;\n\t\tsend_info.remote_addr =\n\t\t\tmlx5dr_ste_get_mr_addr(htbl->chunk->ste_arr + ste_index);\n\t\tsend_info.rkey = mlx5dr_icm_pool_get_chunk_rkey(htbl->chunk);\n\n\t\tret = dr_postsend_icm_data(dmn, &send_info);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t}\n\nout_free:\n\tkvfree(data);\n\treturn ret;\n}\n\nint mlx5dr_send_postsend_action(struct mlx5dr_domain *dmn,\n\t\t\t\tstruct mlx5dr_action *action)\n{\n\tstruct postsend_info send_info = {};\n\n\tsend_info.write.addr = (uintptr_t)action->rewrite->data;\n\tsend_info.write.length = action->rewrite->num_of_actions *\n\t\t\t\t DR_MODIFY_ACTION_SIZE;\n\tsend_info.write.lkey = 0;\n\tsend_info.remote_addr =\n\t\tmlx5dr_icm_pool_get_chunk_mr_addr(action->rewrite->chunk);\n\tsend_info.rkey = mlx5dr_icm_pool_get_chunk_rkey(action->rewrite->chunk);\n\n\treturn dr_postsend_icm_data(dmn, &send_info);\n}\n\nint mlx5dr_send_postsend_pattern(struct mlx5dr_domain *dmn,\n\t\t\t\t struct mlx5dr_icm_chunk *chunk,\n\t\t\t\t u16 num_of_actions,\n\t\t\t\t u8 *data)\n{\n\tstruct postsend_info send_info = {};\n\tint ret;\n\n\tsend_info.write.addr = (uintptr_t)data;\n\tsend_info.write.length = num_of_actions * DR_MODIFY_ACTION_SIZE;\n\tsend_info.remote_addr = mlx5dr_icm_pool_get_chunk_mr_addr(chunk);\n\tsend_info.rkey = mlx5dr_icm_pool_get_chunk_rkey(chunk);\n\n\tret = dr_postsend_icm_data(dmn, &send_info);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nint mlx5dr_send_postsend_args(struct mlx5dr_domain *dmn, u64 arg_id,\n\t\t\t      u16 num_of_actions, u8 *actions_data)\n{\n\tint data_len, iter = 0, cur_sent;\n\tu64 addr;\n\tint ret;\n\n\taddr = (uintptr_t)actions_data;\n\tdata_len = num_of_actions * DR_MODIFY_ACTION_SIZE;\n\n\tdo {\n\t\tstruct postsend_info send_info = {};\n\n\t\tsend_info.type = GTA_ARG;\n\t\tsend_info.write.addr = addr;\n\t\tcur_sent = min_t(u32, data_len, DR_ACTION_CACHE_LINE_SIZE);\n\t\tsend_info.write.length = cur_sent;\n\t\tsend_info.write.lkey = 0;\n\t\tsend_info.remote_addr = arg_id + iter;\n\n\t\tret = dr_postsend_icm_data(dmn, &send_info);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\titer++;\n\t\taddr += cur_sent;\n\t\tdata_len -= cur_sent;\n\t} while (data_len > 0);\n\nout:\n\treturn ret;\n}\n\nstatic int dr_modify_qp_rst2init(struct mlx5_core_dev *mdev,\n\t\t\t\t struct mlx5dr_qp *dr_qp,\n\t\t\t\t int port)\n{\n\tu32 in[MLX5_ST_SZ_DW(rst2init_qp_in)] = {};\n\tvoid *qpc;\n\n\tqpc = MLX5_ADDR_OF(rst2init_qp_in, in, qpc);\n\n\tMLX5_SET(qpc, qpc, primary_address_path.vhca_port_num, port);\n\tMLX5_SET(qpc, qpc, pm_state, MLX5_QPC_PM_STATE_MIGRATED);\n\tMLX5_SET(qpc, qpc, rre, 1);\n\tMLX5_SET(qpc, qpc, rwe, 1);\n\n\tMLX5_SET(rst2init_qp_in, in, opcode, MLX5_CMD_OP_RST2INIT_QP);\n\tMLX5_SET(rst2init_qp_in, in, qpn, dr_qp->qpn);\n\n\treturn mlx5_cmd_exec_in(mdev, rst2init_qp, in);\n}\n\nstatic int dr_cmd_modify_qp_rtr2rts(struct mlx5_core_dev *mdev,\n\t\t\t\t    struct mlx5dr_qp *dr_qp,\n\t\t\t\t    struct dr_qp_rts_attr *attr)\n{\n\tu32 in[MLX5_ST_SZ_DW(rtr2rts_qp_in)] = {};\n\tvoid *qpc;\n\n\tqpc  = MLX5_ADDR_OF(rtr2rts_qp_in, in, qpc);\n\n\tMLX5_SET(rtr2rts_qp_in, in, qpn, dr_qp->qpn);\n\n\tMLX5_SET(qpc, qpc, retry_count, attr->retry_cnt);\n\tMLX5_SET(qpc, qpc, rnr_retry, attr->rnr_retry);\n\tMLX5_SET(qpc, qpc, primary_address_path.ack_timeout, 0x8);  \n\n\tMLX5_SET(rtr2rts_qp_in, in, opcode, MLX5_CMD_OP_RTR2RTS_QP);\n\tMLX5_SET(rtr2rts_qp_in, in, qpn, dr_qp->qpn);\n\n\treturn mlx5_cmd_exec_in(mdev, rtr2rts_qp, in);\n}\n\nstatic int dr_cmd_modify_qp_init2rtr(struct mlx5_core_dev *mdev,\n\t\t\t\t     struct mlx5dr_qp *dr_qp,\n\t\t\t\t     struct dr_qp_rtr_attr *attr)\n{\n\tu32 in[MLX5_ST_SZ_DW(init2rtr_qp_in)] = {};\n\tvoid *qpc;\n\n\tqpc = MLX5_ADDR_OF(init2rtr_qp_in, in, qpc);\n\n\tMLX5_SET(init2rtr_qp_in, in, qpn, dr_qp->qpn);\n\n\tMLX5_SET(qpc, qpc, mtu, attr->mtu);\n\tMLX5_SET(qpc, qpc, log_msg_max, DR_CHUNK_SIZE_MAX - 1);\n\tMLX5_SET(qpc, qpc, remote_qpn, attr->qp_num);\n\tmemcpy(MLX5_ADDR_OF(qpc, qpc, primary_address_path.rmac_47_32),\n\t       attr->dgid_attr.mac, sizeof(attr->dgid_attr.mac));\n\tmemcpy(MLX5_ADDR_OF(qpc, qpc, primary_address_path.rgid_rip),\n\t       attr->dgid_attr.gid, sizeof(attr->dgid_attr.gid));\n\tMLX5_SET(qpc, qpc, primary_address_path.src_addr_index,\n\t\t attr->sgid_index);\n\n\tif (attr->dgid_attr.roce_ver == MLX5_ROCE_VERSION_2)\n\t\tMLX5_SET(qpc, qpc, primary_address_path.udp_sport,\n\t\t\t attr->udp_src_port);\n\n\tMLX5_SET(qpc, qpc, primary_address_path.vhca_port_num, attr->port_num);\n\tMLX5_SET(qpc, qpc, primary_address_path.fl, attr->fl);\n\tMLX5_SET(qpc, qpc, min_rnr_nak, 1);\n\n\tMLX5_SET(init2rtr_qp_in, in, opcode, MLX5_CMD_OP_INIT2RTR_QP);\n\tMLX5_SET(init2rtr_qp_in, in, qpn, dr_qp->qpn);\n\n\treturn mlx5_cmd_exec_in(mdev, init2rtr_qp, in);\n}\n\nstatic bool dr_send_allow_fl(struct mlx5dr_cmd_caps *caps)\n{\n\t \n\treturn ((caps->roce_caps.roce_en &&\n\t\t caps->roce_caps.fl_rc_qp_when_roce_enabled) ||\n\t\t(!caps->roce_caps.roce_en &&\n\t\t caps->roce_caps.fl_rc_qp_when_roce_disabled));\n}\n\nstatic int dr_prepare_qp_to_rts(struct mlx5dr_domain *dmn)\n{\n\tstruct mlx5dr_qp *dr_qp = dmn->send_ring->qp;\n\tstruct dr_qp_rts_attr rts_attr = {};\n\tstruct dr_qp_rtr_attr rtr_attr = {};\n\tenum ib_mtu mtu = IB_MTU_1024;\n\tu16 gid_index = 0;\n\tint port = 1;\n\tint ret;\n\n\t \n\tret = dr_modify_qp_rst2init(dmn->mdev, dr_qp, port);\n\tif (ret) {\n\t\tmlx5dr_err(dmn, \"Failed modify QP rst2init\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\trtr_attr.mtu\t\t= mtu;\n\trtr_attr.qp_num\t\t= dr_qp->qpn;\n\trtr_attr.min_rnr_timer\t= 12;\n\trtr_attr.port_num\t= port;\n\trtr_attr.udp_src_port\t= dmn->info.caps.roce_min_src_udp;\n\n\t \n\trtr_attr.fl = dr_send_allow_fl(&dmn->info.caps);\n\tif (!rtr_attr.fl) {\n\t\tret = mlx5dr_cmd_query_gid(dmn->mdev, port, gid_index,\n\t\t\t\t\t   &rtr_attr.dgid_attr);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\trtr_attr.sgid_index = gid_index;\n\t}\n\n\tret = dr_cmd_modify_qp_init2rtr(dmn->mdev, dr_qp, &rtr_attr);\n\tif (ret) {\n\t\tmlx5dr_err(dmn, \"Failed modify QP init2rtr\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\trts_attr.timeout\t= 14;\n\trts_attr.retry_cnt\t= 7;\n\trts_attr.rnr_retry\t= 7;\n\n\tret = dr_cmd_modify_qp_rtr2rts(dmn->mdev, dr_qp, &rts_attr);\n\tif (ret) {\n\t\tmlx5dr_err(dmn, \"Failed modify QP rtr2rts\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void dr_cq_complete(struct mlx5_core_cq *mcq,\n\t\t\t   struct mlx5_eqe *eqe)\n{\n\tpr_err(\"CQ completion CQ: #%u\\n\", mcq->cqn);\n}\n\nstatic struct mlx5dr_cq *dr_create_cq(struct mlx5_core_dev *mdev,\n\t\t\t\t      struct mlx5_uars_page *uar,\n\t\t\t\t      size_t ncqe)\n{\n\tu32 temp_cqc[MLX5_ST_SZ_DW(cqc)] = {};\n\tu32 out[MLX5_ST_SZ_DW(create_cq_out)];\n\tstruct mlx5_wq_param wqp;\n\tstruct mlx5_cqe64 *cqe;\n\tstruct mlx5dr_cq *cq;\n\tint inlen, err, eqn;\n\tvoid *cqc, *in;\n\t__be64 *pas;\n\tint vector;\n\tu32 i;\n\n\tcq = kzalloc(sizeof(*cq), GFP_KERNEL);\n\tif (!cq)\n\t\treturn NULL;\n\n\tncqe = roundup_pow_of_two(ncqe);\n\tMLX5_SET(cqc, temp_cqc, log_cq_size, ilog2(ncqe));\n\n\twqp.buf_numa_node = mdev->priv.numa_node;\n\twqp.db_numa_node = mdev->priv.numa_node;\n\n\terr = mlx5_cqwq_create(mdev, &wqp, temp_cqc, &cq->wq,\n\t\t\t       &cq->wq_ctrl);\n\tif (err)\n\t\tgoto out;\n\n\tfor (i = 0; i < mlx5_cqwq_get_size(&cq->wq); i++) {\n\t\tcqe = mlx5_cqwq_get_wqe(&cq->wq, i);\n\t\tcqe->op_own = MLX5_CQE_INVALID << 4 | MLX5_CQE_OWNER_MASK;\n\t}\n\n\tinlen = MLX5_ST_SZ_BYTES(create_cq_in) +\n\t\tsizeof(u64) * cq->wq_ctrl.buf.npages;\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\tgoto err_cqwq;\n\n\tvector = raw_smp_processor_id() % mlx5_comp_vectors_max(mdev);\n\terr = mlx5_comp_eqn_get(mdev, vector, &eqn);\n\tif (err) {\n\t\tkvfree(in);\n\t\tgoto err_cqwq;\n\t}\n\n\tcqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);\n\tMLX5_SET(cqc, cqc, log_cq_size, ilog2(ncqe));\n\tMLX5_SET(cqc, cqc, c_eqn_or_apu_element, eqn);\n\tMLX5_SET(cqc, cqc, uar_page, uar->index);\n\tMLX5_SET(cqc, cqc, log_page_size, cq->wq_ctrl.buf.page_shift -\n\t\t MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET64(cqc, cqc, dbr_addr, cq->wq_ctrl.db.dma);\n\n\tpas = (__be64 *)MLX5_ADDR_OF(create_cq_in, in, pas);\n\tmlx5_fill_page_frag_array(&cq->wq_ctrl.buf, pas);\n\n\tcq->mcq.comp  = dr_cq_complete;\n\n\terr = mlx5_core_create_cq(mdev, &cq->mcq, in, inlen, out, sizeof(out));\n\tkvfree(in);\n\n\tif (err)\n\t\tgoto err_cqwq;\n\n\tcq->mcq.cqe_sz = 64;\n\tcq->mcq.set_ci_db = cq->wq_ctrl.db.db;\n\tcq->mcq.arm_db = cq->wq_ctrl.db.db + 1;\n\t*cq->mcq.set_ci_db = 0;\n\n\t \n\t*cq->mcq.arm_db = cpu_to_be32(2 << 28);\n\n\tcq->mcq.vector = 0;\n\tcq->mcq.uar = uar;\n\tcq->mdev = mdev;\n\n\treturn cq;\n\nerr_cqwq:\n\tmlx5_wq_destroy(&cq->wq_ctrl);\nout:\n\tkfree(cq);\n\treturn NULL;\n}\n\nstatic void dr_destroy_cq(struct mlx5_core_dev *mdev, struct mlx5dr_cq *cq)\n{\n\tmlx5_core_destroy_cq(mdev, &cq->mcq);\n\tmlx5_wq_destroy(&cq->wq_ctrl);\n\tkfree(cq);\n}\n\nstatic int dr_create_mkey(struct mlx5_core_dev *mdev, u32 pdn, u32 *mkey)\n{\n\tu32 in[MLX5_ST_SZ_DW(create_mkey_in)] = {};\n\tvoid *mkc;\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\tMLX5_SET(mkc, mkc, access_mode_1_0, MLX5_MKC_ACCESS_MODE_PA);\n\tMLX5_SET(mkc, mkc, a, 1);\n\tMLX5_SET(mkc, mkc, rw, 1);\n\tMLX5_SET(mkc, mkc, rr, 1);\n\tMLX5_SET(mkc, mkc, lw, 1);\n\tMLX5_SET(mkc, mkc, lr, 1);\n\n\tMLX5_SET(mkc, mkc, pd, pdn);\n\tMLX5_SET(mkc, mkc, length64, 1);\n\tMLX5_SET(mkc, mkc, qpn, 0xffffff);\n\n\treturn mlx5_core_create_mkey(mdev, mkey, in, sizeof(in));\n}\n\nstatic struct mlx5dr_mr *dr_reg_mr(struct mlx5_core_dev *mdev,\n\t\t\t\t   u32 pdn, void *buf, size_t size)\n{\n\tstruct mlx5dr_mr *mr = kzalloc(sizeof(*mr), GFP_KERNEL);\n\tstruct device *dma_device;\n\tdma_addr_t dma_addr;\n\tint err;\n\n\tif (!mr)\n\t\treturn NULL;\n\n\tdma_device = mlx5_core_dma_dev(mdev);\n\tdma_addr = dma_map_single(dma_device, buf, size,\n\t\t\t\t  DMA_BIDIRECTIONAL);\n\terr = dma_mapping_error(dma_device, dma_addr);\n\tif (err) {\n\t\tmlx5_core_warn(mdev, \"Can't dma buf\\n\");\n\t\tkfree(mr);\n\t\treturn NULL;\n\t}\n\n\terr = dr_create_mkey(mdev, pdn, &mr->mkey);\n\tif (err) {\n\t\tmlx5_core_warn(mdev, \"Can't create mkey\\n\");\n\t\tdma_unmap_single(dma_device, dma_addr, size,\n\t\t\t\t DMA_BIDIRECTIONAL);\n\t\tkfree(mr);\n\t\treturn NULL;\n\t}\n\n\tmr->dma_addr = dma_addr;\n\tmr->size = size;\n\tmr->addr = buf;\n\n\treturn mr;\n}\n\nstatic void dr_dereg_mr(struct mlx5_core_dev *mdev, struct mlx5dr_mr *mr)\n{\n\tmlx5_core_destroy_mkey(mdev, mr->mkey);\n\tdma_unmap_single(mlx5_core_dma_dev(mdev), mr->dma_addr, mr->size,\n\t\t\t DMA_BIDIRECTIONAL);\n\tkfree(mr);\n}\n\nint mlx5dr_send_ring_alloc(struct mlx5dr_domain *dmn)\n{\n\tstruct dr_qp_init_attr init_attr = {};\n\tint cq_size;\n\tint size;\n\tint ret;\n\n\tdmn->send_ring = kzalloc(sizeof(*dmn->send_ring), GFP_KERNEL);\n\tif (!dmn->send_ring)\n\t\treturn -ENOMEM;\n\n\tcq_size = QUEUE_SIZE + 1;\n\tdmn->send_ring->cq = dr_create_cq(dmn->mdev, dmn->uar, cq_size);\n\tif (!dmn->send_ring->cq) {\n\t\tmlx5dr_err(dmn, \"Failed creating CQ\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto free_send_ring;\n\t}\n\n\tinit_attr.cqn = dmn->send_ring->cq->mcq.cqn;\n\tinit_attr.pdn = dmn->pdn;\n\tinit_attr.uar = dmn->uar;\n\tinit_attr.max_send_wr = QUEUE_SIZE;\n\n\t \n\tif (dr_send_allow_fl(&dmn->info.caps))\n\t\tinit_attr.isolate_vl_tc = dmn->info.caps.isolate_vl_tc;\n\n\tspin_lock_init(&dmn->send_ring->lock);\n\n\tdmn->send_ring->qp = dr_create_rc_qp(dmn->mdev, &init_attr);\n\tif (!dmn->send_ring->qp)  {\n\t\tmlx5dr_err(dmn, \"Failed creating QP\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto clean_cq;\n\t}\n\n\tdmn->send_ring->cq->qp = dmn->send_ring->qp;\n\n\tdmn->info.max_send_wr = QUEUE_SIZE;\n\tdmn->info.max_inline_size = min(dmn->send_ring->qp->max_inline_data,\n\t\t\t\t\tDR_STE_SIZE);\n\n\tdmn->send_ring->signal_th = dmn->info.max_send_wr /\n\t\tSIGNAL_PER_DIV_QUEUE;\n\n\t \n\tret = dr_prepare_qp_to_rts(dmn);\n\tif (ret)\n\t\tgoto clean_qp;\n\n\tdmn->send_ring->max_post_send_size =\n\t\tmlx5dr_icm_pool_chunk_size_to_byte(DR_CHUNK_SIZE_1K,\n\t\t\t\t\t\t   DR_ICM_TYPE_STE);\n\n\t \n\tsize = dmn->send_ring->signal_th * dmn->send_ring->max_post_send_size;\n\tdmn->send_ring->buf = kzalloc(size, GFP_KERNEL);\n\tif (!dmn->send_ring->buf) {\n\t\tret = -ENOMEM;\n\t\tgoto clean_qp;\n\t}\n\n\tdmn->send_ring->buf_size = size;\n\n\tdmn->send_ring->mr = dr_reg_mr(dmn->mdev,\n\t\t\t\t       dmn->pdn, dmn->send_ring->buf, size);\n\tif (!dmn->send_ring->mr) {\n\t\tret = -ENOMEM;\n\t\tgoto free_mem;\n\t}\n\n\tdmn->send_ring->sync_buff = kzalloc(dmn->send_ring->max_post_send_size,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!dmn->send_ring->sync_buff) {\n\t\tret = -ENOMEM;\n\t\tgoto clean_mr;\n\t}\n\n\tdmn->send_ring->sync_mr = dr_reg_mr(dmn->mdev,\n\t\t\t\t\t    dmn->pdn, dmn->send_ring->sync_buff,\n\t\t\t\t\t    dmn->send_ring->max_post_send_size);\n\tif (!dmn->send_ring->sync_mr) {\n\t\tret = -ENOMEM;\n\t\tgoto free_sync_mem;\n\t}\n\n\treturn 0;\n\nfree_sync_mem:\n\tkfree(dmn->send_ring->sync_buff);\nclean_mr:\n\tdr_dereg_mr(dmn->mdev, dmn->send_ring->mr);\nfree_mem:\n\tkfree(dmn->send_ring->buf);\nclean_qp:\n\tdr_destroy_qp(dmn->mdev, dmn->send_ring->qp);\nclean_cq:\n\tdr_destroy_cq(dmn->mdev, dmn->send_ring->cq);\nfree_send_ring:\n\tkfree(dmn->send_ring);\n\n\treturn ret;\n}\n\nvoid mlx5dr_send_ring_free(struct mlx5dr_domain *dmn,\n\t\t\t   struct mlx5dr_send_ring *send_ring)\n{\n\tdr_destroy_qp(dmn->mdev, send_ring->qp);\n\tdr_destroy_cq(dmn->mdev, send_ring->cq);\n\tdr_dereg_mr(dmn->mdev, send_ring->sync_mr);\n\tdr_dereg_mr(dmn->mdev, send_ring->mr);\n\tkfree(send_ring->buf);\n\tkfree(send_ring->sync_buff);\n\tkfree(send_ring);\n}\n\nint mlx5dr_send_ring_force_drain(struct mlx5dr_domain *dmn)\n{\n\tstruct mlx5dr_send_ring *send_ring = dmn->send_ring;\n\tstruct postsend_info send_info = {};\n\tu8 data[DR_STE_SIZE];\n\tint num_of_sends_req;\n\tint ret;\n\tint i;\n\n\t \n\tnum_of_sends_req = send_ring->signal_th * TH_NUMS_TO_DRAIN / 2;\n\n\t \n\tsend_info.write.addr = (uintptr_t)data;\n\tsend_info.write.length = DR_STE_SIZE;\n\tsend_info.write.lkey = 0;\n\t \n\tsend_info.remote_addr = (uintptr_t)send_ring->sync_mr->addr;\n\tsend_info.rkey = send_ring->sync_mr->mkey;\n\n\tfor (i = 0; i < num_of_sends_req; i++) {\n\t\tret = dr_postsend_icm_data(dmn, &send_info);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tspin_lock(&send_ring->lock);\n\tret = dr_handle_pending_wc(dmn, send_ring);\n\tspin_unlock(&send_ring->lock);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}