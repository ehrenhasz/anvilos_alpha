{
  "module_name": "ktls_rx.c",
  "hash_id": "6160d546df6a17bb6fef081603a3b0e2c41fbc6e80209afa927c3cdee6fe6730",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_rx.c",
  "human_readable_source": "\n\n\n#include <net/inet6_hashtables.h>\n#include \"en_accel/en_accel.h\"\n#include \"en_accel/ktls.h\"\n#include \"en_accel/ktls_txrx.h\"\n#include \"en_accel/ktls_utils.h\"\n#include \"en_accel/fs_tcp.h\"\n\nstruct accel_rule {\n\tstruct work_struct work;\n\tstruct mlx5e_priv *priv;\n\tstruct mlx5_flow_handle *rule;\n};\n\n#define PROGRESS_PARAMS_WRITE_UNIT\t64\n#define PROGRESS_PARAMS_PADDED_SIZE\t\\\n\t\t(ALIGN(sizeof(struct mlx5_wqe_tls_progress_params_seg), \\\n\t\t       PROGRESS_PARAMS_WRITE_UNIT))\n\nstruct mlx5e_ktls_rx_resync_buf {\n\tunion {\n\t\tstruct mlx5_wqe_tls_progress_params_seg progress;\n\t\tu8 pad[PROGRESS_PARAMS_PADDED_SIZE];\n\t} ____cacheline_aligned_in_smp;\n\tdma_addr_t dma_addr;\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx;\n};\n\nenum {\n\tMLX5E_PRIV_RX_FLAG_DELETING,\n\tMLX5E_NUM_PRIV_RX_FLAGS,\n};\n\nstruct mlx5e_ktls_rx_resync_ctx {\n\tstruct tls_offload_resync_async core;\n\tstruct work_struct work;\n\tstruct mlx5e_priv *priv;\n\trefcount_t refcnt;\n\t__be64 sw_rcd_sn_be;\n\tu32 seq;\n};\n\nstruct mlx5e_ktls_offload_context_rx {\n\tunion mlx5e_crypto_info crypto_info;\n\tstruct accel_rule rule;\n\tstruct sock *sk;\n\tstruct mlx5e_rq_stats *rq_stats;\n\tstruct mlx5e_tls_sw_stats *sw_stats;\n\tstruct completion add_ctx;\n\tstruct mlx5e_tir tir;\n\tstruct mlx5_crypto_dek *dek;\n\tu32 rxq;\n\tDECLARE_BITMAP(flags, MLX5E_NUM_PRIV_RX_FLAGS);\n\n\t \n\tspinlock_t lock;  \n\tstruct mlx5e_ktls_rx_resync_ctx resync;\n\tstruct list_head list;\n};\n\nstatic bool mlx5e_ktls_priv_rx_put(struct mlx5e_ktls_offload_context_rx *priv_rx)\n{\n\tif (!refcount_dec_and_test(&priv_rx->resync.refcnt))\n\t\treturn false;\n\n\tkfree(priv_rx);\n\treturn true;\n}\n\nstatic void mlx5e_ktls_priv_rx_get(struct mlx5e_ktls_offload_context_rx *priv_rx)\n{\n\trefcount_inc(&priv_rx->resync.refcnt);\n}\n\nstruct mlx5e_ktls_resync_resp {\n\t \n\tspinlock_t lock;\n\tstruct list_head list;\n};\n\nvoid mlx5e_ktls_rx_resync_destroy_resp_list(struct mlx5e_ktls_resync_resp *resp_list)\n{\n\tkvfree(resp_list);\n}\n\nstruct mlx5e_ktls_resync_resp *\nmlx5e_ktls_rx_resync_create_resp_list(void)\n{\n\tstruct mlx5e_ktls_resync_resp *resp_list;\n\n\tresp_list = kvzalloc(sizeof(*resp_list), GFP_KERNEL);\n\tif (!resp_list)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&resp_list->list);\n\tspin_lock_init(&resp_list->lock);\n\n\treturn resp_list;\n}\n\nstatic void accel_rule_handle_work(struct work_struct *work)\n{\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx;\n\tstruct accel_rule *accel_rule;\n\tstruct mlx5_flow_handle *rule;\n\n\taccel_rule = container_of(work, struct accel_rule, work);\n\tpriv_rx = container_of(accel_rule, struct mlx5e_ktls_offload_context_rx, rule);\n\tif (unlikely(test_bit(MLX5E_PRIV_RX_FLAG_DELETING, priv_rx->flags)))\n\t\tgoto out;\n\n\trule = mlx5e_accel_fs_add_sk(accel_rule->priv->fs, priv_rx->sk,\n\t\t\t\t     mlx5e_tir_get_tirn(&priv_rx->tir),\n\t\t\t\t     MLX5_FS_DEFAULT_FLOW_TAG);\n\tif (!IS_ERR_OR_NULL(rule))\n\t\taccel_rule->rule = rule;\nout:\n\tcomplete(&priv_rx->add_ctx);\n}\n\nstatic void accel_rule_init(struct accel_rule *rule, struct mlx5e_priv *priv)\n{\n\tINIT_WORK(&rule->work, accel_rule_handle_work);\n\trule->priv = priv;\n}\n\nstatic void icosq_fill_wi(struct mlx5e_icosq *sq, u16 pi,\n\t\t\t  struct mlx5e_icosq_wqe_info *wi)\n{\n\tsq->db.wqe_info[pi] = *wi;\n}\n\nstatic struct mlx5_wqe_ctrl_seg *\npost_static_params(struct mlx5e_icosq *sq,\n\t\t   struct mlx5e_ktls_offload_context_rx *priv_rx)\n{\n\tstruct mlx5e_set_tls_static_params_wqe *wqe;\n\tstruct mlx5e_icosq_wqe_info wi;\n\tu16 pi, num_wqebbs;\n\n\tnum_wqebbs = MLX5E_TLS_SET_STATIC_PARAMS_WQEBBS;\n\tif (unlikely(!mlx5e_icosq_can_post_wqe(sq, num_wqebbs)))\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tpi = mlx5e_icosq_get_next_pi(sq, num_wqebbs);\n\twqe = MLX5E_TLS_FETCH_SET_STATIC_PARAMS_WQE(sq, pi);\n\tmlx5e_ktls_build_static_params(wqe, sq->pc, sq->sqn, &priv_rx->crypto_info,\n\t\t\t\t       mlx5e_tir_get_tirn(&priv_rx->tir),\n\t\t\t\t       mlx5_crypto_dek_get_id(priv_rx->dek),\n\t\t\t\t       priv_rx->resync.seq, false,\n\t\t\t\t       TLS_OFFLOAD_CTX_DIR_RX);\n\twi = (struct mlx5e_icosq_wqe_info) {\n\t\t.wqe_type = MLX5E_ICOSQ_WQE_UMR_TLS,\n\t\t.num_wqebbs = num_wqebbs,\n\t\t.tls_set_params.priv_rx = priv_rx,\n\t};\n\ticosq_fill_wi(sq, pi, &wi);\n\tsq->pc += num_wqebbs;\n\n\treturn &wqe->ctrl;\n}\n\nstatic struct mlx5_wqe_ctrl_seg *\npost_progress_params(struct mlx5e_icosq *sq,\n\t\t     struct mlx5e_ktls_offload_context_rx *priv_rx,\n\t\t     u32 next_record_tcp_sn)\n{\n\tstruct mlx5e_set_tls_progress_params_wqe *wqe;\n\tstruct mlx5e_icosq_wqe_info wi;\n\tu16 pi, num_wqebbs;\n\n\tnum_wqebbs = MLX5E_TLS_SET_PROGRESS_PARAMS_WQEBBS;\n\tif (unlikely(!mlx5e_icosq_can_post_wqe(sq, num_wqebbs)))\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tpi = mlx5e_icosq_get_next_pi(sq, num_wqebbs);\n\twqe = MLX5E_TLS_FETCH_SET_PROGRESS_PARAMS_WQE(sq, pi);\n\tmlx5e_ktls_build_progress_params(wqe, sq->pc, sq->sqn,\n\t\t\t\t\t mlx5e_tir_get_tirn(&priv_rx->tir),\n\t\t\t\t\t false, next_record_tcp_sn,\n\t\t\t\t\t TLS_OFFLOAD_CTX_DIR_RX);\n\twi = (struct mlx5e_icosq_wqe_info) {\n\t\t.wqe_type = MLX5E_ICOSQ_WQE_SET_PSV_TLS,\n\t\t.num_wqebbs = num_wqebbs,\n\t\t.tls_set_params.priv_rx = priv_rx,\n\t};\n\n\ticosq_fill_wi(sq, pi, &wi);\n\tsq->pc += num_wqebbs;\n\n\treturn &wqe->ctrl;\n}\n\nstatic int post_rx_param_wqes(struct mlx5e_channel *c,\n\t\t\t      struct mlx5e_ktls_offload_context_rx *priv_rx,\n\t\t\t      u32 next_record_tcp_sn)\n{\n\tstruct mlx5_wqe_ctrl_seg *cseg;\n\tstruct mlx5e_icosq *sq;\n\tint err;\n\n\terr = 0;\n\tsq = &c->async_icosq;\n\tspin_lock_bh(&c->async_icosq_lock);\n\n\tcseg = post_static_params(sq, priv_rx);\n\tif (IS_ERR(cseg))\n\t\tgoto err_out;\n\tcseg = post_progress_params(sq, priv_rx, next_record_tcp_sn);\n\tif (IS_ERR(cseg))\n\t\tgoto err_out;\n\n\tmlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);\nunlock:\n\tspin_unlock_bh(&c->async_icosq_lock);\n\n\treturn err;\n\nerr_out:\n\tpriv_rx->rq_stats->tls_resync_req_skip++;\n\terr = PTR_ERR(cseg);\n\tcomplete(&priv_rx->add_ctx);\n\tgoto unlock;\n}\n\nstatic void\nmlx5e_set_ktls_rx_priv_ctx(struct tls_context *tls_ctx,\n\t\t\t   struct mlx5e_ktls_offload_context_rx *priv_rx)\n{\n\tstruct mlx5e_ktls_offload_context_rx **ctx =\n\t\t__tls_driver_ctx(tls_ctx, TLS_OFFLOAD_CTX_DIR_RX);\n\n\tBUILD_BUG_ON(sizeof(priv_rx) > TLS_DRIVER_STATE_SIZE_RX);\n\n\t*ctx = priv_rx;\n}\n\nstatic struct mlx5e_ktls_offload_context_rx *\nmlx5e_get_ktls_rx_priv_ctx(struct tls_context *tls_ctx)\n{\n\tstruct mlx5e_ktls_offload_context_rx **ctx =\n\t\t__tls_driver_ctx(tls_ctx, TLS_OFFLOAD_CTX_DIR_RX);\n\n\treturn *ctx;\n}\n\n \n \nstatic int\nresync_post_get_progress_params(struct mlx5e_icosq *sq,\n\t\t\t\tstruct mlx5e_ktls_offload_context_rx *priv_rx)\n{\n\tstruct mlx5e_get_tls_progress_params_wqe *wqe;\n\tstruct mlx5e_ktls_rx_resync_buf *buf;\n\tstruct mlx5e_icosq_wqe_info wi;\n\tstruct mlx5_wqe_ctrl_seg *cseg;\n\tstruct mlx5_seg_get_psv *psv;\n\tstruct device *pdev;\n\tint err;\n\tu16 pi;\n\n\tbuf = kzalloc(sizeof(*buf), GFP_KERNEL);\n\tif (unlikely(!buf)) {\n\t\terr = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\n\tpdev = mlx5_core_dma_dev(sq->channel->priv->mdev);\n\tbuf->dma_addr = dma_map_single(pdev, &buf->progress,\n\t\t\t\t       PROGRESS_PARAMS_PADDED_SIZE, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(pdev, buf->dma_addr))) {\n\t\terr = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tbuf->priv_rx = priv_rx;\n\n\tspin_lock_bh(&sq->channel->async_icosq_lock);\n\n\tif (unlikely(!mlx5e_icosq_can_post_wqe(sq, MLX5E_KTLS_GET_PROGRESS_WQEBBS))) {\n\t\tspin_unlock_bh(&sq->channel->async_icosq_lock);\n\t\terr = -ENOSPC;\n\t\tgoto err_dma_unmap;\n\t}\n\n\tpi = mlx5e_icosq_get_next_pi(sq, MLX5E_KTLS_GET_PROGRESS_WQEBBS);\n\twqe = MLX5E_TLS_FETCH_GET_PROGRESS_PARAMS_WQE(sq, pi);\n\n#define GET_PSV_DS_CNT (DIV_ROUND_UP(sizeof(*wqe), MLX5_SEND_WQE_DS))\n\n\tcseg = &wqe->ctrl;\n\tcseg->opmod_idx_opcode =\n\t\tcpu_to_be32((sq->pc << 8) | MLX5_OPCODE_GET_PSV |\n\t\t\t    (MLX5_OPC_MOD_TLS_TIR_PROGRESS_PARAMS << 24));\n\tcseg->qpn_ds =\n\t\tcpu_to_be32((sq->sqn << MLX5_WQE_CTRL_QPN_SHIFT) | GET_PSV_DS_CNT);\n\n\tpsv = &wqe->psv;\n\tpsv->num_psv      = 1 << 4;\n\tpsv->l_key        = sq->channel->mkey_be;\n\tpsv->psv_index[0] = cpu_to_be32(mlx5e_tir_get_tirn(&priv_rx->tir));\n\tpsv->va           = cpu_to_be64(buf->dma_addr);\n\n\twi = (struct mlx5e_icosq_wqe_info) {\n\t\t.wqe_type = MLX5E_ICOSQ_WQE_GET_PSV_TLS,\n\t\t.num_wqebbs = MLX5E_KTLS_GET_PROGRESS_WQEBBS,\n\t\t.tls_get_params.buf = buf,\n\t};\n\ticosq_fill_wi(sq, pi, &wi);\n\tsq->pc++;\n\tmlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);\n\tspin_unlock_bh(&sq->channel->async_icosq_lock);\n\n\treturn 0;\n\nerr_dma_unmap:\n\tdma_unmap_single(pdev, buf->dma_addr, PROGRESS_PARAMS_PADDED_SIZE, DMA_FROM_DEVICE);\nerr_free:\n\tkfree(buf);\nerr_out:\n\tpriv_rx->rq_stats->tls_resync_req_skip++;\n\treturn err;\n}\n\n \nstatic void resync_handle_work(struct work_struct *work)\n{\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx;\n\tstruct mlx5e_ktls_rx_resync_ctx *resync;\n\tstruct mlx5e_channel *c;\n\tstruct mlx5e_icosq *sq;\n\n\tresync = container_of(work, struct mlx5e_ktls_rx_resync_ctx, work);\n\tpriv_rx = container_of(resync, struct mlx5e_ktls_offload_context_rx, resync);\n\n\tif (unlikely(test_bit(MLX5E_PRIV_RX_FLAG_DELETING, priv_rx->flags))) {\n\t\tmlx5e_ktls_priv_rx_put(priv_rx);\n\t\treturn;\n\t}\n\n\tc = resync->priv->channels.c[priv_rx->rxq];\n\tsq = &c->async_icosq;\n\n\tif (resync_post_get_progress_params(sq, priv_rx))\n\t\tmlx5e_ktls_priv_rx_put(priv_rx);\n}\n\nstatic void resync_init(struct mlx5e_ktls_rx_resync_ctx *resync,\n\t\t\tstruct mlx5e_priv *priv)\n{\n\tINIT_WORK(&resync->work, resync_handle_work);\n\tresync->priv = priv;\n\trefcount_set(&resync->refcnt, 1);\n}\n\n \nstatic void resync_handle_seq_match(struct mlx5e_ktls_offload_context_rx *priv_rx,\n\t\t\t\t    struct mlx5e_channel *c)\n{\n\tstruct mlx5e_ktls_resync_resp *ktls_resync;\n\tstruct mlx5e_icosq *sq;\n\tbool trigger_poll;\n\n\tsq = &c->async_icosq;\n\tktls_resync = sq->ktls_resync;\n\ttrigger_poll = false;\n\n\tspin_lock_bh(&ktls_resync->lock);\n\tspin_lock_bh(&priv_rx->lock);\n\tswitch (priv_rx->crypto_info.crypto_info.cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128: {\n\t\tstruct tls12_crypto_info_aes_gcm_128 *info =\n\t\t\t&priv_rx->crypto_info.crypto_info_128;\n\n\t\tmemcpy(info->rec_seq, &priv_rx->resync.sw_rcd_sn_be,\n\t\t       sizeof(info->rec_seq));\n\t\tbreak;\n\t}\n\tcase TLS_CIPHER_AES_GCM_256: {\n\t\tstruct tls12_crypto_info_aes_gcm_256 *info =\n\t\t\t&priv_rx->crypto_info.crypto_info_256;\n\n\t\tmemcpy(info->rec_seq, &priv_rx->resync.sw_rcd_sn_be,\n\t\t       sizeof(info->rec_seq));\n\t\tbreak;\n\t}\n\tdefault:\n\t\tWARN_ONCE(1, \"Unsupported cipher type %u\\n\",\n\t\t\t  priv_rx->crypto_info.crypto_info.cipher_type);\n\t\tspin_unlock_bh(&priv_rx->lock);\n\t\tspin_unlock_bh(&ktls_resync->lock);\n\t\treturn;\n\t}\n\n\tif (list_empty(&priv_rx->list)) {\n\t\tlist_add_tail(&priv_rx->list, &ktls_resync->list);\n\t\ttrigger_poll = !test_and_set_bit(MLX5E_SQ_STATE_PENDING_TLS_RX_RESYNC, &sq->state);\n\t}\n\tspin_unlock_bh(&priv_rx->lock);\n\tspin_unlock_bh(&ktls_resync->lock);\n\n\tif (!trigger_poll)\n\t\treturn;\n\n\tif (!napi_if_scheduled_mark_missed(&c->napi)) {\n\t\tspin_lock_bh(&c->async_icosq_lock);\n\t\tmlx5e_trigger_irq(sq);\n\t\tspin_unlock_bh(&c->async_icosq_lock);\n\t}\n}\n\n \nvoid mlx5e_ktls_handle_get_psv_completion(struct mlx5e_icosq_wqe_info *wi,\n\t\t\t\t\t  struct mlx5e_icosq *sq)\n{\n\tstruct mlx5e_ktls_rx_resync_buf *buf = wi->tls_get_params.buf;\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx;\n\tstruct mlx5e_ktls_rx_resync_ctx *resync;\n\tu8 tracker_state, auth_state, *ctx;\n\tstruct device *dev;\n\tu32 hw_seq;\n\n\tpriv_rx = buf->priv_rx;\n\tresync = &priv_rx->resync;\n\tdev = mlx5_core_dma_dev(resync->priv->mdev);\n\tif (unlikely(test_bit(MLX5E_PRIV_RX_FLAG_DELETING, priv_rx->flags)))\n\t\tgoto out;\n\n\tdma_sync_single_for_cpu(dev, buf->dma_addr, PROGRESS_PARAMS_PADDED_SIZE,\n\t\t\t\tDMA_FROM_DEVICE);\n\n\tctx = buf->progress.ctx;\n\ttracker_state = MLX5_GET(tls_progress_params, ctx, record_tracker_state);\n\tauth_state = MLX5_GET(tls_progress_params, ctx, auth_state);\n\tif (tracker_state != MLX5E_TLS_PROGRESS_PARAMS_RECORD_TRACKER_STATE_TRACKING ||\n\t    auth_state != MLX5E_TLS_PROGRESS_PARAMS_AUTH_STATE_NO_OFFLOAD) {\n\t\tpriv_rx->rq_stats->tls_resync_req_skip++;\n\t\tgoto out;\n\t}\n\n\thw_seq = MLX5_GET(tls_progress_params, ctx, hw_resync_tcp_sn);\n\ttls_offload_rx_resync_async_request_end(priv_rx->sk, cpu_to_be32(hw_seq));\n\tpriv_rx->rq_stats->tls_resync_req_end++;\nout:\n\tmlx5e_ktls_priv_rx_put(priv_rx);\n\tdma_unmap_single(dev, buf->dma_addr, PROGRESS_PARAMS_PADDED_SIZE, DMA_FROM_DEVICE);\n\tkfree(buf);\n}\n\n \nstatic bool resync_queue_get_psv(struct sock *sk)\n{\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx;\n\tstruct mlx5e_ktls_rx_resync_ctx *resync;\n\n\tpriv_rx = mlx5e_get_ktls_rx_priv_ctx(tls_get_ctx(sk));\n\tif (unlikely(!priv_rx))\n\t\treturn false;\n\n\tif (unlikely(test_bit(MLX5E_PRIV_RX_FLAG_DELETING, priv_rx->flags)))\n\t\treturn false;\n\n\tresync = &priv_rx->resync;\n\tmlx5e_ktls_priv_rx_get(priv_rx);\n\tif (unlikely(!queue_work(resync->priv->tls->rx_wq, &resync->work)))\n\t\tmlx5e_ktls_priv_rx_put(priv_rx);\n\n\treturn true;\n}\n\n \nstatic void resync_update_sn(struct mlx5e_rq *rq, struct sk_buff *skb)\n{\n\tstruct ethhdr *eth = (struct ethhdr *)(skb->data);\n\tstruct net_device *netdev = rq->netdev;\n\tstruct net *net = dev_net(netdev);\n\tstruct sock *sk = NULL;\n\tunsigned int datalen;\n\tstruct iphdr *iph;\n\tstruct tcphdr *th;\n\t__be32 seq;\n\tint depth = 0;\n\n\t__vlan_get_protocol(skb, eth->h_proto, &depth);\n\tiph = (struct iphdr *)(skb->data + depth);\n\n\tif (iph->version == 4) {\n\t\tdepth += sizeof(struct iphdr);\n\t\tth = (void *)iph + sizeof(struct iphdr);\n\n\t\tsk = inet_lookup_established(net, net->ipv4.tcp_death_row.hashinfo,\n\t\t\t\t\t     iph->saddr, th->source, iph->daddr,\n\t\t\t\t\t     th->dest, netdev->ifindex);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else {\n\t\tstruct ipv6hdr *ipv6h = (struct ipv6hdr *)iph;\n\n\t\tdepth += sizeof(struct ipv6hdr);\n\t\tth = (void *)ipv6h + sizeof(struct ipv6hdr);\n\n\t\tsk = __inet6_lookup_established(net, net->ipv4.tcp_death_row.hashinfo,\n\t\t\t\t\t\t&ipv6h->saddr, th->source,\n\t\t\t\t\t\t&ipv6h->daddr, ntohs(th->dest),\n\t\t\t\t\t\tnetdev->ifindex, 0);\n#endif\n\t}\n\n\tdepth += sizeof(struct tcphdr);\n\n\tif (unlikely(!sk))\n\t\treturn;\n\n\tif (unlikely(sk->sk_state == TCP_TIME_WAIT))\n\t\tgoto unref;\n\n\tif (unlikely(!resync_queue_get_psv(sk)))\n\t\tgoto unref;\n\n\tseq = th->seq;\n\tdatalen = skb->len - depth;\n\ttls_offload_rx_resync_async_request_start(sk, seq, datalen);\n\trq->stats->tls_resync_req_start++;\n\nunref:\n\tsock_gen_put(sk);\n}\n\nvoid mlx5e_ktls_rx_resync(struct net_device *netdev, struct sock *sk,\n\t\t\t  u32 seq, u8 *rcd_sn)\n{\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx;\n\tstruct mlx5e_ktls_rx_resync_ctx *resync;\n\tstruct mlx5e_priv *priv;\n\tstruct mlx5e_channel *c;\n\n\tpriv_rx = mlx5e_get_ktls_rx_priv_ctx(tls_get_ctx(sk));\n\tif (unlikely(!priv_rx))\n\t\treturn;\n\n\tresync = &priv_rx->resync;\n\tresync->sw_rcd_sn_be = *(__be64 *)rcd_sn;\n\tresync->seq = seq;\n\n\tpriv = netdev_priv(netdev);\n\tc = priv->channels.c[priv_rx->rxq];\n\n\tresync_handle_seq_match(priv_rx, c);\n}\n\n \n\nvoid mlx5e_ktls_handle_rx_skb(struct mlx5e_rq *rq, struct sk_buff *skb,\n\t\t\t      struct mlx5_cqe64 *cqe, u32 *cqe_bcnt)\n{\n\tstruct mlx5e_rq_stats *stats = rq->stats;\n\n\tswitch (get_cqe_tls_offload(cqe)) {\n\tcase CQE_TLS_OFFLOAD_DECRYPTED:\n\t\tskb->decrypted = 1;\n\t\tstats->tls_decrypted_packets++;\n\t\tstats->tls_decrypted_bytes += *cqe_bcnt;\n\t\tbreak;\n\tcase CQE_TLS_OFFLOAD_RESYNC:\n\t\tstats->tls_resync_req_pkt++;\n\t\tresync_update_sn(rq, skb);\n\t\tbreak;\n\tdefault:  \n\t\tstats->tls_err++;\n\t\tbreak;\n\t}\n}\n\nvoid mlx5e_ktls_handle_ctx_completion(struct mlx5e_icosq_wqe_info *wi)\n{\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx = wi->tls_set_params.priv_rx;\n\tstruct accel_rule *rule = &priv_rx->rule;\n\n\tif (unlikely(test_bit(MLX5E_PRIV_RX_FLAG_DELETING, priv_rx->flags))) {\n\t\tcomplete(&priv_rx->add_ctx);\n\t\treturn;\n\t}\n\tqueue_work(rule->priv->tls->rx_wq, &rule->work);\n}\n\nstatic int mlx5e_ktls_sk_get_rxq(struct sock *sk)\n{\n\tint rxq = sk_rx_queue_get(sk);\n\n\tif (unlikely(rxq == -1))\n\t\trxq = 0;\n\n\treturn rxq;\n}\n\nint mlx5e_ktls_add_rx(struct net_device *netdev, struct sock *sk,\n\t\t      struct tls_crypto_info *crypto_info,\n\t\t      u32 start_offload_tcp_sn)\n{\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx;\n\tstruct mlx5e_ktls_rx_resync_ctx *resync;\n\tstruct tls_context *tls_ctx;\n\tstruct mlx5_crypto_dek *dek;\n\tstruct mlx5e_priv *priv;\n\tint rxq, err;\n\n\ttls_ctx = tls_get_ctx(sk);\n\tpriv = netdev_priv(netdev);\n\tpriv_rx = kzalloc(sizeof(*priv_rx), GFP_KERNEL);\n\tif (unlikely(!priv_rx))\n\t\treturn -ENOMEM;\n\n\tswitch (crypto_info->cipher_type) {\n\tcase TLS_CIPHER_AES_GCM_128:\n\t\tpriv_rx->crypto_info.crypto_info_128 =\n\t\t\t*(struct tls12_crypto_info_aes_gcm_128 *)crypto_info;\n\t\tbreak;\n\tcase TLS_CIPHER_AES_GCM_256:\n\t\tpriv_rx->crypto_info.crypto_info_256 =\n\t\t\t*(struct tls12_crypto_info_aes_gcm_256 *)crypto_info;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"Unsupported cipher type %u\\n\",\n\t\t\t  crypto_info->cipher_type);\n\t\terr = -EOPNOTSUPP;\n\t\tgoto err_cipher_type;\n\t}\n\n\tdek = mlx5_ktls_create_key(priv->tls->dek_pool, crypto_info);\n\tif (IS_ERR(dek)) {\n\t\terr = PTR_ERR(dek);\n\t\tgoto err_cipher_type;\n\t}\n\tpriv_rx->dek = dek;\n\n\tINIT_LIST_HEAD(&priv_rx->list);\n\tspin_lock_init(&priv_rx->lock);\n\n\trxq = mlx5e_ktls_sk_get_rxq(sk);\n\tpriv_rx->rxq = rxq;\n\tpriv_rx->sk = sk;\n\n\tpriv_rx->rq_stats = &priv->channel_stats[rxq]->rq;\n\tpriv_rx->sw_stats = &priv->tls->sw_stats;\n\tmlx5e_set_ktls_rx_priv_ctx(tls_ctx, priv_rx);\n\n\terr = mlx5e_rx_res_tls_tir_create(priv->rx_res, rxq, &priv_rx->tir);\n\tif (err)\n\t\tgoto err_create_tir;\n\n\tinit_completion(&priv_rx->add_ctx);\n\n\taccel_rule_init(&priv_rx->rule, priv);\n\tresync = &priv_rx->resync;\n\tresync_init(resync, priv);\n\ttls_offload_ctx_rx(tls_ctx)->resync_async = &resync->core;\n\ttls_offload_rx_resync_set_type(sk, TLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ_ASYNC);\n\n\terr = post_rx_param_wqes(priv->channels.c[rxq], priv_rx, start_offload_tcp_sn);\n\tif (err)\n\t\tgoto err_post_wqes;\n\n\tatomic64_inc(&priv_rx->sw_stats->rx_tls_ctx);\n\n\treturn 0;\n\nerr_post_wqes:\n\tmlx5e_tir_destroy(&priv_rx->tir);\nerr_create_tir:\n\tmlx5_ktls_destroy_key(priv->tls->dek_pool, priv_rx->dek);\nerr_cipher_type:\n\tkfree(priv_rx);\n\treturn err;\n}\n\nvoid mlx5e_ktls_del_rx(struct net_device *netdev, struct tls_context *tls_ctx)\n{\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx;\n\tstruct mlx5e_ktls_rx_resync_ctx *resync;\n\tstruct mlx5e_priv *priv;\n\n\tpriv = netdev_priv(netdev);\n\n\tpriv_rx = mlx5e_get_ktls_rx_priv_ctx(tls_ctx);\n\tset_bit(MLX5E_PRIV_RX_FLAG_DELETING, priv_rx->flags);\n\tmlx5e_set_ktls_rx_priv_ctx(tls_ctx, NULL);\n\tsynchronize_net();  \n\tif (!cancel_work_sync(&priv_rx->rule.work))\n\t\t \n\t\twait_for_completion(&priv_rx->add_ctx);\n\tresync = &priv_rx->resync;\n\tif (cancel_work_sync(&resync->work))\n\t\tmlx5e_ktls_priv_rx_put(priv_rx);\n\n\tatomic64_inc(&priv_rx->sw_stats->rx_tls_del);\n\tif (priv_rx->rule.rule)\n\t\tmlx5e_accel_fs_del_sk(priv_rx->rule.rule);\n\n\tmlx5e_tir_destroy(&priv_rx->tir);\n\tmlx5_ktls_destroy_key(priv->tls->dek_pool, priv_rx->dek);\n\t \n\tmlx5e_ktls_priv_rx_put(priv_rx);\n}\n\nbool mlx5e_ktls_rx_handle_resync_list(struct mlx5e_channel *c, int budget)\n{\n\tstruct mlx5e_ktls_offload_context_rx *priv_rx, *tmp;\n\tstruct mlx5e_ktls_resync_resp *ktls_resync;\n\tstruct mlx5_wqe_ctrl_seg *db_cseg;\n\tstruct mlx5e_icosq *sq;\n\tLIST_HEAD(local_list);\n\tint i, j;\n\n\tsq = &c->async_icosq;\n\n\tif (unlikely(!test_bit(MLX5E_SQ_STATE_ENABLED, &sq->state)))\n\t\treturn false;\n\n\tktls_resync = sq->ktls_resync;\n\tdb_cseg = NULL;\n\ti = 0;\n\n\tspin_lock(&ktls_resync->lock);\n\tlist_for_each_entry_safe(priv_rx, tmp, &ktls_resync->list, list) {\n\t\tlist_move(&priv_rx->list, &local_list);\n\t\tif (++i == budget)\n\t\t\tbreak;\n\t}\n\tif (list_empty(&ktls_resync->list))\n\t\tclear_bit(MLX5E_SQ_STATE_PENDING_TLS_RX_RESYNC, &sq->state);\n\tspin_unlock(&ktls_resync->lock);\n\n\tspin_lock(&c->async_icosq_lock);\n\tfor (j = 0; j < i; j++) {\n\t\tstruct mlx5_wqe_ctrl_seg *cseg;\n\n\t\tpriv_rx = list_first_entry(&local_list,\n\t\t\t\t\t   struct mlx5e_ktls_offload_context_rx,\n\t\t\t\t\t   list);\n\t\tspin_lock(&priv_rx->lock);\n\t\tcseg = post_static_params(sq, priv_rx);\n\t\tif (IS_ERR(cseg)) {\n\t\t\tspin_unlock(&priv_rx->lock);\n\t\t\tbreak;\n\t\t}\n\t\tlist_del_init(&priv_rx->list);\n\t\tspin_unlock(&priv_rx->lock);\n\t\tdb_cseg = cseg;\n\t}\n\tif (db_cseg)\n\t\tmlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, db_cseg);\n\tspin_unlock(&c->async_icosq_lock);\n\n\tpriv_rx->rq_stats->tls_resync_res_ok += j;\n\n\tif (!list_empty(&local_list)) {\n\t\t \n\t\tspin_lock(&ktls_resync->lock);\n\t\tlist_splice(&local_list, &ktls_resync->list);\n\t\tset_bit(MLX5E_SQ_STATE_PENDING_TLS_RX_RESYNC, &sq->state);\n\t\tspin_unlock(&ktls_resync->lock);\n\t\tpriv_rx->rq_stats->tls_resync_res_retry++;\n\t}\n\n\treturn i == budget;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}