{
  "module_name": "txrx.h",
  "hash_id": "48f87e598b4d5397ab4aa481b1071b5f80525d777f3937041f14176f2fcd44d5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/en/txrx.h",
  "human_readable_source": " \n \n\n#ifndef __MLX5_EN_TXRX_H___\n#define __MLX5_EN_TXRX_H___\n\n#include \"en.h\"\n#include <linux/indirect_call_wrapper.h>\n\n#define MLX5E_TX_WQE_EMPTY_DS_COUNT (sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS)\n\n#define INL_HDR_START_SZ (sizeof(((struct mlx5_wqe_eth_seg *)NULL)->inline_hdr.start))\n\n \n#define MLX5E_MAX_TX_IPSEC_DS DIV_ROUND_UP(sizeof(struct mlx5_wqe_inline_seg) + \\\n\t\t\t\t\t   255 + 1 + 1 + 16, MLX5_SEND_WQE_DS)\n\n \n#define MLX5E_MAX_TX_INLINE_DS DIV_ROUND_UP(366 - INL_HDR_START_SZ + VLAN_HLEN, \\\n\t\t\t\t\t    MLX5_SEND_WQE_DS)\n\n \n#define MLX5E_MAX_TX_WQEBBS DIV_ROUND_UP(MLX5E_TX_WQE_EMPTY_DS_COUNT + \\\n\t\t\t\t\t MLX5E_MAX_TX_INLINE_DS + \\\n\t\t\t\t\t MLX5E_MAX_TX_IPSEC_DS + \\\n\t\t\t\t\t MAX_SKB_FRAGS + 1, \\\n\t\t\t\t\t MLX5_SEND_WQEBB_NUM_DS)\n\n#define MLX5E_RX_ERR_CQE(cqe) (get_cqe_opcode(cqe) != MLX5_CQE_RESP_SEND)\n\nstatic inline\nktime_t mlx5e_cqe_ts_to_ns(cqe_ts_to_ns func, struct mlx5_clock *clock, u64 cqe_ts)\n{\n\treturn INDIRECT_CALL_2(func, mlx5_real_time_cyc2time, mlx5_timecounter_cyc2time,\n\t\t\t       clock, cqe_ts);\n}\n\nenum mlx5e_icosq_wqe_type {\n\tMLX5E_ICOSQ_WQE_NOP,\n\tMLX5E_ICOSQ_WQE_UMR_RX,\n\tMLX5E_ICOSQ_WQE_SHAMPO_HD_UMR,\n#ifdef CONFIG_MLX5_EN_TLS\n\tMLX5E_ICOSQ_WQE_UMR_TLS,\n\tMLX5E_ICOSQ_WQE_SET_PSV_TLS,\n\tMLX5E_ICOSQ_WQE_GET_PSV_TLS,\n#endif\n};\n\n \nstatic inline bool mlx5e_skb_is_multicast(struct sk_buff *skb)\n{\n\treturn skb->pkt_type == PACKET_MULTICAST || skb->pkt_type == PACKET_BROADCAST;\n}\n\nvoid mlx5e_trigger_irq(struct mlx5e_icosq *sq);\nvoid mlx5e_completion_event(struct mlx5_core_cq *mcq, struct mlx5_eqe *eqe);\nvoid mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);\nint mlx5e_napi_poll(struct napi_struct *napi, int budget);\nint mlx5e_poll_ico_cq(struct mlx5e_cq *cq);\n\n \nINDIRECT_CALLABLE_DECLARE(bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq));\nINDIRECT_CALLABLE_DECLARE(bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq));\nint mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);\nvoid mlx5e_free_rx_descs(struct mlx5e_rq *rq);\nvoid mlx5e_free_rx_missing_descs(struct mlx5e_rq *rq);\n\nstatic inline bool mlx5e_rx_hw_stamp(struct hwtstamp_config *config)\n{\n\treturn config->rx_filter == HWTSTAMP_FILTER_ALL;\n}\n\n \nstruct mlx5e_xmit_data {\n\tdma_addr_t  dma_addr;\n\tvoid       *data;\n\tu32         len : 31;\n\tu32         has_frags : 1;\n};\n\nstruct mlx5e_xmit_data_frags {\n\tstruct mlx5e_xmit_data xd;\n\tstruct skb_shared_info *sinfo;\n\tdma_addr_t *dma_arr;\n};\n\nnetdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);\nbool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);\nvoid mlx5e_free_txqsq_descs(struct mlx5e_txqsq *sq);\n\nstatic inline bool\nmlx5e_skb_fifo_has_room(struct mlx5e_skb_fifo *fifo)\n{\n\treturn (u16)(*fifo->pc - *fifo->cc) <= fifo->mask;\n}\n\nstatic inline bool\nmlx5e_wqc_has_room_for(struct mlx5_wq_cyc *wq, u16 cc, u16 pc, u16 n)\n{\n\treturn (mlx5_wq_cyc_ctr2ix(wq, cc - pc) >= n) || (cc == pc);\n}\n\nstatic inline void *mlx5e_fetch_wqe(struct mlx5_wq_cyc *wq, u16 pi, size_t wqe_size)\n{\n\tvoid *wqe;\n\n\twqe = mlx5_wq_cyc_get_wqe(wq, pi);\n\tmemset(wqe, 0, wqe_size);\n\n\treturn wqe;\n}\n\n#define MLX5E_TX_FETCH_WQE(sq, pi) \\\n\t((struct mlx5e_tx_wqe *)mlx5e_fetch_wqe(&(sq)->wq, pi, sizeof(struct mlx5e_tx_wqe)))\n\nstatic inline struct mlx5e_tx_wqe *\nmlx5e_post_nop(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)\n{\n\tu16                         pi   = mlx5_wq_cyc_ctr2ix(wq, *pc);\n\tstruct mlx5e_tx_wqe        *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);\n\tstruct mlx5_wqe_ctrl_seg   *cseg = &wqe->ctrl;\n\n\tmemset(cseg, 0, sizeof(*cseg));\n\n\tcseg->opmod_idx_opcode = cpu_to_be32((*pc << 8) | MLX5_OPCODE_NOP);\n\tcseg->qpn_ds           = cpu_to_be32((sqn << 8) | 0x01);\n\n\t(*pc)++;\n\n\treturn wqe;\n}\n\nstatic inline struct mlx5e_tx_wqe *\nmlx5e_post_nop_fence(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)\n{\n\tu16                         pi   = mlx5_wq_cyc_ctr2ix(wq, *pc);\n\tstruct mlx5e_tx_wqe        *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);\n\tstruct mlx5_wqe_ctrl_seg   *cseg = &wqe->ctrl;\n\n\tmemset(cseg, 0, sizeof(*cseg));\n\n\tcseg->opmod_idx_opcode = cpu_to_be32((*pc << 8) | MLX5_OPCODE_NOP);\n\tcseg->qpn_ds           = cpu_to_be32((sqn << 8) | 0x01);\n\tcseg->fm_ce_se         = MLX5_FENCE_MODE_INITIATOR_SMALL;\n\n\t(*pc)++;\n\n\treturn wqe;\n}\n\nstruct mlx5e_tx_wqe_info {\n\tstruct sk_buff *skb;\n\tu32 num_bytes;\n\tu8 num_wqebbs;\n\tu8 num_dma;\n\tu8 num_fifo_pkts;\n#ifdef CONFIG_MLX5_EN_TLS\n\tstruct page *resync_dump_frag_page;\n#endif\n};\n\nstatic inline u16 mlx5e_txqsq_get_next_pi(struct mlx5e_txqsq *sq, u16 size)\n{\n\tstruct mlx5_wq_cyc *wq = &sq->wq;\n\tu16 pi, contig_wqebbs;\n\n\tpi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);\n\tcontig_wqebbs = mlx5_wq_cyc_get_contig_wqebbs(wq, pi);\n\tif (unlikely(contig_wqebbs < size)) {\n\t\tstruct mlx5e_tx_wqe_info *wi, *edge_wi;\n\n\t\twi = &sq->db.wqe_info[pi];\n\t\tedge_wi = wi + contig_wqebbs;\n\n\t\t \n\t\tfor (; wi < edge_wi; wi++) {\n\t\t\t*wi = (struct mlx5e_tx_wqe_info) {\n\t\t\t\t.num_wqebbs = 1,\n\t\t\t};\n\t\t\tmlx5e_post_nop(wq, sq->sqn, &sq->pc);\n\t\t}\n\t\tsq->stats->nop += contig_wqebbs;\n\n\t\tpi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);\n\t}\n\n\treturn pi;\n}\n\nvoid mlx5e_txqsq_wake(struct mlx5e_txqsq *sq);\n\nstatic inline u16 mlx5e_shampo_get_cqe_header_index(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)\n{\n\treturn be16_to_cpu(cqe->shampo.header_entry_index) & (rq->mpwqe.shampo->hd_per_wq - 1);\n}\n\nstruct mlx5e_shampo_umr {\n\tu16 len;\n};\n\nstruct mlx5e_icosq_wqe_info {\n\tu8 wqe_type;\n\tu8 num_wqebbs;\n\n\t \n\tunion {\n\t\tstruct {\n\t\t\tstruct mlx5e_rq *rq;\n\t\t} umr;\n\t\tstruct mlx5e_shampo_umr shampo;\n#ifdef CONFIG_MLX5_EN_TLS\n\t\tstruct {\n\t\t\tstruct mlx5e_ktls_offload_context_rx *priv_rx;\n\t\t} tls_set_params;\n\t\tstruct {\n\t\t\tstruct mlx5e_ktls_rx_resync_buf *buf;\n\t\t} tls_get_params;\n#endif\n\t};\n};\n\nvoid mlx5e_free_icosq_descs(struct mlx5e_icosq *sq);\n\nstatic inline u16 mlx5e_icosq_get_next_pi(struct mlx5e_icosq *sq, u16 size)\n{\n\tstruct mlx5_wq_cyc *wq = &sq->wq;\n\tu16 pi, contig_wqebbs;\n\n\tpi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);\n\tcontig_wqebbs = mlx5_wq_cyc_get_contig_wqebbs(wq, pi);\n\tif (unlikely(contig_wqebbs < size)) {\n\t\tstruct mlx5e_icosq_wqe_info *wi, *edge_wi;\n\n\t\twi = &sq->db.wqe_info[pi];\n\t\tedge_wi = wi + contig_wqebbs;\n\n\t\t \n\t\tfor (; wi < edge_wi; wi++) {\n\t\t\t*wi = (struct mlx5e_icosq_wqe_info) {\n\t\t\t\t.wqe_type   = MLX5E_ICOSQ_WQE_NOP,\n\t\t\t\t.num_wqebbs = 1,\n\t\t\t};\n\t\t\tmlx5e_post_nop(wq, sq->sqn, &sq->pc);\n\t\t}\n\n\t\tpi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);\n\t}\n\n\treturn pi;\n}\n\nstatic inline void\nmlx5e_notify_hw(struct mlx5_wq_cyc *wq, u16 pc, void __iomem *uar_map,\n\t\tstruct mlx5_wqe_ctrl_seg *ctrl)\n{\n\tctrl->fm_ce_se |= MLX5_WQE_CTRL_CQ_UPDATE;\n\t \n\tdma_wmb();\n\n\t*wq->db = cpu_to_be32(pc);\n\n\t \n\twmb();\n\n\tmlx5_write64((__be32 *)ctrl, uar_map);\n}\n\nstatic inline void mlx5e_cq_arm(struct mlx5e_cq *cq)\n{\n\tstruct mlx5_core_cq *mcq;\n\n\tmcq = &cq->mcq;\n\tmlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, cq->wq.cc);\n}\n\nstatic inline struct mlx5e_sq_dma *\nmlx5e_dma_get(struct mlx5e_txqsq *sq, u32 i)\n{\n\treturn &sq->db.dma_fifo[i & sq->dma_fifo_mask];\n}\n\nstatic inline void\nmlx5e_dma_push(struct mlx5e_txqsq *sq, dma_addr_t addr, u32 size,\n\t       enum mlx5e_dma_map_type map_type)\n{\n\tstruct mlx5e_sq_dma *dma = mlx5e_dma_get(sq, sq->dma_fifo_pc++);\n\n\tdma->addr = addr;\n\tdma->size = size;\n\tdma->type = map_type;\n}\n\nstatic inline\nstruct sk_buff **mlx5e_skb_fifo_get(struct mlx5e_skb_fifo *fifo, u16 i)\n{\n\treturn &fifo->fifo[i & fifo->mask];\n}\n\nstatic inline\nvoid mlx5e_skb_fifo_push(struct mlx5e_skb_fifo *fifo, struct sk_buff *skb)\n{\n\tstruct sk_buff **skb_item = mlx5e_skb_fifo_get(fifo, (*fifo->pc)++);\n\n\t*skb_item = skb;\n}\n\nstatic inline\nstruct sk_buff *mlx5e_skb_fifo_pop(struct mlx5e_skb_fifo *fifo)\n{\n\tWARN_ON_ONCE(*fifo->pc == *fifo->cc);\n\n\treturn *mlx5e_skb_fifo_get(fifo, (*fifo->cc)++);\n}\n\nstatic inline void\nmlx5e_tx_dma_unmap(struct device *pdev, struct mlx5e_sq_dma *dma)\n{\n\tswitch (dma->type) {\n\tcase MLX5E_DMA_MAP_SINGLE:\n\t\tdma_unmap_single(pdev, dma->addr, dma->size, DMA_TO_DEVICE);\n\t\tbreak;\n\tcase MLX5E_DMA_MAP_PAGE:\n\t\tdma_unmap_page(pdev, dma->addr, dma->size, DMA_TO_DEVICE);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(true, \"mlx5e_tx_dma_unmap unknown DMA type!\\n\");\n\t}\n}\n\nvoid mlx5e_tx_mpwqe_ensure_complete(struct mlx5e_txqsq *sq);\n\nstatic inline bool mlx5e_tx_mpwqe_is_full(struct mlx5e_tx_mpwqe *session, u8 max_sq_mpw_wqebbs)\n{\n\treturn session->ds_count == max_sq_mpw_wqebbs * MLX5_SEND_WQEBB_NUM_DS;\n}\n\nstatic inline void mlx5e_rqwq_reset(struct mlx5e_rq *rq)\n{\n\tif (rq->wq_type == MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ) {\n\t\tmlx5_wq_ll_reset(&rq->mpwqe.wq);\n\t\trq->mpwqe.actual_wq_head = 0;\n\t} else {\n\t\tmlx5_wq_cyc_reset(&rq->wqe.wq);\n\t}\n}\n\nstatic inline void mlx5e_dump_error_cqe(struct mlx5e_cq *cq, u32 qn,\n\t\t\t\t\tstruct mlx5_err_cqe *err_cqe)\n{\n\tstruct mlx5_cqwq *wq = &cq->wq;\n\tu32 ci;\n\n\tci = mlx5_cqwq_ctr2ix(wq, wq->cc - 1);\n\n\tnetdev_err(cq->netdev,\n\t\t   \"Error cqe on cqn 0x%x, ci 0x%x, qn 0x%x, opcode 0x%x, syndrome 0x%x, vendor syndrome 0x%x\\n\",\n\t\t   cq->mcq.cqn, ci, qn,\n\t\t   get_cqe_opcode((struct mlx5_cqe64 *)err_cqe),\n\t\t   err_cqe->syndrome, err_cqe->vendor_err_synd);\n\tmlx5_dump_err_cqe(cq->mdev, err_cqe);\n}\n\nstatic inline u32 mlx5e_rqwq_get_size(struct mlx5e_rq *rq)\n{\n\tswitch (rq->wq_type) {\n\tcase MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:\n\t\treturn mlx5_wq_ll_get_size(&rq->mpwqe.wq);\n\tdefault:\n\t\treturn mlx5_wq_cyc_get_size(&rq->wqe.wq);\n\t}\n}\n\nstatic inline u32 mlx5e_rqwq_get_cur_sz(struct mlx5e_rq *rq)\n{\n\tswitch (rq->wq_type) {\n\tcase MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:\n\t\treturn rq->mpwqe.wq.cur_sz;\n\tdefault:\n\t\treturn rq->wqe.wq.cur_sz;\n\t}\n}\n\nstatic inline u16 mlx5e_rqwq_get_head(struct mlx5e_rq *rq)\n{\n\tswitch (rq->wq_type) {\n\tcase MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:\n\t\treturn mlx5_wq_ll_get_head(&rq->mpwqe.wq);\n\tdefault:\n\t\treturn mlx5_wq_cyc_get_head(&rq->wqe.wq);\n\t}\n}\n\nstatic inline u16 mlx5e_rqwq_get_wqe_counter(struct mlx5e_rq *rq)\n{\n\tswitch (rq->wq_type) {\n\tcase MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:\n\t\treturn mlx5_wq_ll_get_counter(&rq->mpwqe.wq);\n\tdefault:\n\t\treturn mlx5_wq_cyc_get_counter(&rq->wqe.wq);\n\t}\n}\n\n \n\nstruct mlx5e_swp_spec {\n\t__be16 l3_proto;\n\tu8 l4_proto;\n\tu8 is_tun;\n\t__be16 tun_l3_proto;\n\tu8 tun_l4_proto;\n};\n\nstatic inline void mlx5e_eseg_swp_offsets_add_vlan(struct mlx5_wqe_eth_seg *eseg)\n{\n\t \n\teseg->swp_outer_l3_offset += VLAN_HLEN / 2;\n\teseg->swp_outer_l4_offset += VLAN_HLEN / 2;\n\teseg->swp_inner_l3_offset += VLAN_HLEN / 2;\n\teseg->swp_inner_l4_offset += VLAN_HLEN / 2;\n}\n\nstatic inline void\nmlx5e_set_eseg_swp(struct sk_buff *skb, struct mlx5_wqe_eth_seg *eseg,\n\t\t   struct mlx5e_swp_spec *swp_spec)\n{\n\t \n\teseg->swp_outer_l3_offset = skb_network_offset(skb) / 2;\n\tif (swp_spec->l3_proto == htons(ETH_P_IPV6))\n\t\teseg->swp_flags |= MLX5_ETH_WQE_SWP_OUTER_L3_IPV6;\n\tif (swp_spec->l4_proto) {\n\t\teseg->swp_outer_l4_offset = skb_transport_offset(skb) / 2;\n\t\tif (swp_spec->l4_proto == IPPROTO_UDP)\n\t\t\teseg->swp_flags |= MLX5_ETH_WQE_SWP_OUTER_L4_UDP;\n\t}\n\n\tif (swp_spec->is_tun) {\n\t\teseg->swp_inner_l3_offset = skb_inner_network_offset(skb) / 2;\n\t\tif (swp_spec->tun_l3_proto == htons(ETH_P_IPV6))\n\t\t\teseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;\n\t} else {  \n\t\teseg->swp_inner_l3_offset = skb_network_offset(skb) / 2;\n\t\tif (swp_spec->l3_proto == htons(ETH_P_IPV6))\n\t\t\teseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;\n\t}\n\tswitch (swp_spec->tun_l4_proto) {\n\tcase IPPROTO_UDP:\n\t\teseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L4_UDP;\n\t\tfallthrough;\n\tcase IPPROTO_TCP:\n\t\teseg->swp_inner_l4_offset = skb_inner_transport_offset(skb) / 2;\n\t\tbreak;\n\t}\n}\n\n#define MLX5E_STOP_ROOM(wqebbs) ((wqebbs) * 2 - 1)\n\nstatic inline u16 mlx5e_stop_room_for_wqe(struct mlx5_core_dev *mdev, u16 wqe_size)\n{\n\tWARN_ON_ONCE(PAGE_SIZE / MLX5_SEND_WQE_BB < (u16)mlx5e_get_max_sq_wqebbs(mdev));\n\n\t \n\tWARN_ONCE(wqe_size > mlx5e_get_max_sq_wqebbs(mdev),\n\t\t  \"wqe_size %u is greater than max SQ WQEBBs %u\",\n\t\t  wqe_size, mlx5e_get_max_sq_wqebbs(mdev));\n\n\treturn MLX5E_STOP_ROOM(wqe_size);\n}\n\nstatic inline u16 mlx5e_stop_room_for_max_wqe(struct mlx5_core_dev *mdev)\n{\n\treturn MLX5E_STOP_ROOM(mlx5e_get_max_sq_wqebbs(mdev));\n}\n\nstatic inline u16 mlx5e_stop_room_for_mpwqe(struct mlx5_core_dev *mdev)\n{\n\tu8 mpwqe_wqebbs = mlx5e_get_max_sq_aligned_wqebbs(mdev);\n\n\treturn mlx5e_stop_room_for_wqe(mdev, mpwqe_wqebbs);\n}\n\nstatic inline bool mlx5e_icosq_can_post_wqe(struct mlx5e_icosq *sq, u16 wqe_size)\n{\n\tu16 room = sq->reserved_room + MLX5E_STOP_ROOM(wqe_size);\n\n\treturn mlx5e_wqc_has_room_for(&sq->wq, sq->cc, sq->pc, room);\n}\n\nstatic inline struct mlx5e_mpw_info *mlx5e_get_mpw_info(struct mlx5e_rq *rq, int i)\n{\n\tsize_t isz = struct_size(rq->mpwqe.info, alloc_units.frag_pages, rq->mpwqe.pages_per_wqe);\n\n\treturn (struct mlx5e_mpw_info *)((char *)rq->mpwqe.info + array_size(i, isz));\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}