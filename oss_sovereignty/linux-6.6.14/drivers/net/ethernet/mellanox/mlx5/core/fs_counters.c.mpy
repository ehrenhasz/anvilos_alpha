{
  "module_name": "fs_counters.c",
  "hash_id": "d6748ed4129f6fd9e28aac7fdc6c51bb9804bed2a7a059155526dd4a417c6dad",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/fs_counters.c",
  "human_readable_source": " \n\n#include <linux/mlx5/driver.h>\n#include <linux/mlx5/fs.h>\n#include <linux/rbtree.h>\n#include \"mlx5_core.h\"\n#include \"fs_core.h\"\n#include \"fs_cmd.h\"\n\n#define MLX5_FC_STATS_PERIOD msecs_to_jiffies(1000)\n#define MLX5_FC_BULK_QUERY_ALLOC_PERIOD msecs_to_jiffies(180 * 1000)\n \n#define MLX5_SW_MAX_COUNTERS_BULK BIT(15)\n#define MLX5_INIT_COUNTERS_BULK 8\n#define MLX5_FC_POOL_MAX_THRESHOLD BIT(18)\n#define MLX5_FC_POOL_USED_BUFF_RATIO 10\n\nstruct mlx5_fc_cache {\n\tu64 packets;\n\tu64 bytes;\n\tu64 lastuse;\n};\n\nstruct mlx5_fc {\n\tstruct list_head list;\n\tstruct llist_node addlist;\n\tstruct llist_node dellist;\n\n\t \n\tu64 lastpackets;\n\tu64 lastbytes;\n\n\tstruct mlx5_fc_bulk *bulk;\n\tu32 id;\n\tbool aging;\n\n\tstruct mlx5_fc_cache cache ____cacheline_aligned_in_smp;\n};\n\nstatic void mlx5_fc_pool_init(struct mlx5_fc_pool *fc_pool, struct mlx5_core_dev *dev);\nstatic void mlx5_fc_pool_cleanup(struct mlx5_fc_pool *fc_pool);\nstatic struct mlx5_fc *mlx5_fc_pool_acquire_counter(struct mlx5_fc_pool *fc_pool);\nstatic void mlx5_fc_pool_release_counter(struct mlx5_fc_pool *fc_pool, struct mlx5_fc *fc);\n\n \n\nstatic struct list_head *mlx5_fc_counters_lookup_next(struct mlx5_core_dev *dev,\n\t\t\t\t\t\t      u32 id)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\tunsigned long next_id = (unsigned long)id + 1;\n\tstruct mlx5_fc *counter;\n\tunsigned long tmp;\n\n\trcu_read_lock();\n\t \n\tidr_for_each_entry_continue_ul(&fc_stats->counters_idr,\n\t\t\t\t       counter, tmp, next_id) {\n\t\tif (!list_empty(&counter->list))\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn counter ? &counter->list : &fc_stats->counters;\n}\n\nstatic void mlx5_fc_stats_insert(struct mlx5_core_dev *dev,\n\t\t\t\t struct mlx5_fc *counter)\n{\n\tstruct list_head *next = mlx5_fc_counters_lookup_next(dev, counter->id);\n\n\tlist_add_tail(&counter->list, next);\n}\n\nstatic void mlx5_fc_stats_remove(struct mlx5_core_dev *dev,\n\t\t\t\t struct mlx5_fc *counter)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\n\tlist_del(&counter->list);\n\n\tspin_lock(&fc_stats->counters_idr_lock);\n\tWARN_ON(!idr_remove(&fc_stats->counters_idr, counter->id));\n\tspin_unlock(&fc_stats->counters_idr_lock);\n}\n\nstatic int get_init_bulk_query_len(struct mlx5_core_dev *dev)\n{\n\treturn min_t(int, MLX5_INIT_COUNTERS_BULK,\n\t\t     (1 << MLX5_CAP_GEN(dev, log_max_flow_counter_bulk)));\n}\n\nstatic int get_max_bulk_query_len(struct mlx5_core_dev *dev)\n{\n\treturn min_t(int, MLX5_SW_MAX_COUNTERS_BULK,\n\t\t     (1 << MLX5_CAP_GEN(dev, log_max_flow_counter_bulk)));\n}\n\nstatic void update_counter_cache(int index, u32 *bulk_raw_data,\n\t\t\t\t struct mlx5_fc_cache *cache)\n{\n\tvoid *stats = MLX5_ADDR_OF(query_flow_counter_out, bulk_raw_data,\n\t\t\t     flow_statistics[index]);\n\tu64 packets = MLX5_GET64(traffic_counter, stats, packets);\n\tu64 bytes = MLX5_GET64(traffic_counter, stats, octets);\n\n\tif (cache->packets == packets)\n\t\treturn;\n\n\tcache->packets = packets;\n\tcache->bytes = bytes;\n\tcache->lastuse = jiffies;\n}\n\nstatic void mlx5_fc_stats_query_counter_range(struct mlx5_core_dev *dev,\n\t\t\t\t\t      struct mlx5_fc *first,\n\t\t\t\t\t      u32 last_id)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\tbool query_more_counters = (first->id <= last_id);\n\tint cur_bulk_len = fc_stats->bulk_query_len;\n\tu32 *data = fc_stats->bulk_query_out;\n\tstruct mlx5_fc *counter = first;\n\tu32 bulk_base_id;\n\tint bulk_len;\n\tint err;\n\n\twhile (query_more_counters) {\n\t\t \n\t\tbulk_base_id = counter->id & ~0x3;\n\n\t\t \n\t\tbulk_len = min_t(int, cur_bulk_len,\n\t\t\t\t ALIGN(last_id - bulk_base_id + 1, 4));\n\n\t\terr = mlx5_cmd_fc_bulk_query(dev, bulk_base_id, bulk_len,\n\t\t\t\t\t     data);\n\t\tif (err) {\n\t\t\tmlx5_core_err(dev, \"Error doing bulk query: %d\\n\", err);\n\t\t\treturn;\n\t\t}\n\t\tquery_more_counters = false;\n\n\t\tlist_for_each_entry_from(counter, &fc_stats->counters, list) {\n\t\t\tint counter_index = counter->id - bulk_base_id;\n\t\t\tstruct mlx5_fc_cache *cache = &counter->cache;\n\n\t\t\tif (counter->id >= bulk_base_id + bulk_len) {\n\t\t\t\tquery_more_counters = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tupdate_counter_cache(counter_index, data, cache);\n\t\t}\n\t}\n}\n\nstatic void mlx5_fc_free(struct mlx5_core_dev *dev, struct mlx5_fc *counter)\n{\n\tmlx5_cmd_fc_free(dev, counter->id);\n\tkfree(counter);\n}\n\nstatic void mlx5_fc_release(struct mlx5_core_dev *dev, struct mlx5_fc *counter)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\n\tif (counter->bulk)\n\t\tmlx5_fc_pool_release_counter(&fc_stats->fc_pool, counter);\n\telse\n\t\tmlx5_fc_free(dev, counter);\n}\n\nstatic void mlx5_fc_stats_bulk_query_size_increase(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\tint max_bulk_len = get_max_bulk_query_len(dev);\n\tunsigned long now = jiffies;\n\tu32 *bulk_query_out_tmp;\n\tint max_out_len;\n\n\tif (fc_stats->bulk_query_alloc_failed &&\n\t    time_before(now, fc_stats->next_bulk_query_alloc))\n\t\treturn;\n\n\tmax_out_len = mlx5_cmd_fc_get_bulk_query_out_len(max_bulk_len);\n\tbulk_query_out_tmp = kzalloc(max_out_len, GFP_KERNEL);\n\tif (!bulk_query_out_tmp) {\n\t\tmlx5_core_warn_once(dev,\n\t\t\t\t    \"Can't increase flow counters bulk query buffer size, insufficient memory, bulk_size(%d)\\n\",\n\t\t\t\t    max_bulk_len);\n\t\tfc_stats->bulk_query_alloc_failed = true;\n\t\tfc_stats->next_bulk_query_alloc =\n\t\t\tnow + MLX5_FC_BULK_QUERY_ALLOC_PERIOD;\n\t\treturn;\n\t}\n\n\tkfree(fc_stats->bulk_query_out);\n\tfc_stats->bulk_query_out = bulk_query_out_tmp;\n\tfc_stats->bulk_query_len = max_bulk_len;\n\tif (fc_stats->bulk_query_alloc_failed) {\n\t\tmlx5_core_info(dev,\n\t\t\t       \"Flow counters bulk query buffer size increased, bulk_size(%d)\\n\",\n\t\t\t       max_bulk_len);\n\t\tfc_stats->bulk_query_alloc_failed = false;\n\t}\n}\n\nstatic void mlx5_fc_stats_work(struct work_struct *work)\n{\n\tstruct mlx5_core_dev *dev = container_of(work, struct mlx5_core_dev,\n\t\t\t\t\t\t priv.fc_stats.work.work);\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\t \n\tstruct llist_node *dellist = llist_del_all(&fc_stats->dellist);\n\tstruct llist_node *addlist = llist_del_all(&fc_stats->addlist);\n\tstruct mlx5_fc *counter = NULL, *last = NULL, *tmp;\n\tunsigned long now = jiffies;\n\n\tif (addlist || !list_empty(&fc_stats->counters))\n\t\tqueue_delayed_work(fc_stats->wq, &fc_stats->work,\n\t\t\t\t   fc_stats->sampling_interval);\n\n\tllist_for_each_entry(counter, addlist, addlist) {\n\t\tmlx5_fc_stats_insert(dev, counter);\n\t\tfc_stats->num_counters++;\n\t}\n\n\tllist_for_each_entry_safe(counter, tmp, dellist, dellist) {\n\t\tmlx5_fc_stats_remove(dev, counter);\n\n\t\tmlx5_fc_release(dev, counter);\n\t\tfc_stats->num_counters--;\n\t}\n\n\tif (fc_stats->bulk_query_len < get_max_bulk_query_len(dev) &&\n\t    fc_stats->num_counters > get_init_bulk_query_len(dev))\n\t\tmlx5_fc_stats_bulk_query_size_increase(dev);\n\n\tif (time_before(now, fc_stats->next_query) ||\n\t    list_empty(&fc_stats->counters))\n\t\treturn;\n\tlast = list_last_entry(&fc_stats->counters, struct mlx5_fc, list);\n\n\tcounter = list_first_entry(&fc_stats->counters, struct mlx5_fc,\n\t\t\t\t   list);\n\tif (counter)\n\t\tmlx5_fc_stats_query_counter_range(dev, counter, last->id);\n\n\tfc_stats->next_query = now + fc_stats->sampling_interval;\n}\n\nstatic struct mlx5_fc *mlx5_fc_single_alloc(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_fc *counter;\n\tint err;\n\n\tcounter = kzalloc(sizeof(*counter), GFP_KERNEL);\n\tif (!counter)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = mlx5_cmd_fc_alloc(dev, &counter->id);\n\tif (err) {\n\t\tkfree(counter);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn counter;\n}\n\nstatic struct mlx5_fc *mlx5_fc_acquire(struct mlx5_core_dev *dev, bool aging)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\tstruct mlx5_fc *counter;\n\n\tif (aging && MLX5_CAP_GEN(dev, flow_counter_bulk_alloc) != 0) {\n\t\tcounter = mlx5_fc_pool_acquire_counter(&fc_stats->fc_pool);\n\t\tif (!IS_ERR(counter))\n\t\t\treturn counter;\n\t}\n\n\treturn mlx5_fc_single_alloc(dev);\n}\n\nstruct mlx5_fc *mlx5_fc_create_ex(struct mlx5_core_dev *dev, bool aging)\n{\n\tstruct mlx5_fc *counter = mlx5_fc_acquire(dev, aging);\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\tint err;\n\n\tif (IS_ERR(counter))\n\t\treturn counter;\n\n\tINIT_LIST_HEAD(&counter->list);\n\tcounter->aging = aging;\n\n\tif (aging) {\n\t\tu32 id = counter->id;\n\n\t\tcounter->cache.lastuse = jiffies;\n\t\tcounter->lastbytes = counter->cache.bytes;\n\t\tcounter->lastpackets = counter->cache.packets;\n\n\t\tidr_preload(GFP_KERNEL);\n\t\tspin_lock(&fc_stats->counters_idr_lock);\n\n\t\terr = idr_alloc_u32(&fc_stats->counters_idr, counter, &id, id,\n\t\t\t\t    GFP_NOWAIT);\n\n\t\tspin_unlock(&fc_stats->counters_idr_lock);\n\t\tidr_preload_end();\n\t\tif (err)\n\t\t\tgoto err_out_alloc;\n\n\t\tllist_add(&counter->addlist, &fc_stats->addlist);\n\t}\n\n\treturn counter;\n\nerr_out_alloc:\n\tmlx5_fc_release(dev, counter);\n\treturn ERR_PTR(err);\n}\n\nstruct mlx5_fc *mlx5_fc_create(struct mlx5_core_dev *dev, bool aging)\n{\n\tstruct mlx5_fc *counter = mlx5_fc_create_ex(dev, aging);\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\n\tif (aging)\n\t\tmod_delayed_work(fc_stats->wq, &fc_stats->work, 0);\n\treturn counter;\n}\nEXPORT_SYMBOL(mlx5_fc_create);\n\nu32 mlx5_fc_id(struct mlx5_fc *counter)\n{\n\treturn counter->id;\n}\nEXPORT_SYMBOL(mlx5_fc_id);\n\nvoid mlx5_fc_destroy(struct mlx5_core_dev *dev, struct mlx5_fc *counter)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\n\tif (!counter)\n\t\treturn;\n\n\tif (counter->aging) {\n\t\tllist_add(&counter->dellist, &fc_stats->dellist);\n\t\tmod_delayed_work(fc_stats->wq, &fc_stats->work, 0);\n\t\treturn;\n\t}\n\n\tmlx5_fc_release(dev, counter);\n}\nEXPORT_SYMBOL(mlx5_fc_destroy);\n\nint mlx5_init_fc_stats(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\tint init_bulk_len;\n\tint init_out_len;\n\n\tspin_lock_init(&fc_stats->counters_idr_lock);\n\tidr_init(&fc_stats->counters_idr);\n\tINIT_LIST_HEAD(&fc_stats->counters);\n\tinit_llist_head(&fc_stats->addlist);\n\tinit_llist_head(&fc_stats->dellist);\n\n\tinit_bulk_len = get_init_bulk_query_len(dev);\n\tinit_out_len = mlx5_cmd_fc_get_bulk_query_out_len(init_bulk_len);\n\tfc_stats->bulk_query_out = kzalloc(init_out_len, GFP_KERNEL);\n\tif (!fc_stats->bulk_query_out)\n\t\treturn -ENOMEM;\n\tfc_stats->bulk_query_len = init_bulk_len;\n\n\tfc_stats->wq = create_singlethread_workqueue(\"mlx5_fc\");\n\tif (!fc_stats->wq)\n\t\tgoto err_wq_create;\n\n\tfc_stats->sampling_interval = MLX5_FC_STATS_PERIOD;\n\tINIT_DELAYED_WORK(&fc_stats->work, mlx5_fc_stats_work);\n\n\tmlx5_fc_pool_init(&fc_stats->fc_pool, dev);\n\treturn 0;\n\nerr_wq_create:\n\tkfree(fc_stats->bulk_query_out);\n\treturn -ENOMEM;\n}\n\nvoid mlx5_cleanup_fc_stats(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\tstruct llist_node *tmplist;\n\tstruct mlx5_fc *counter;\n\tstruct mlx5_fc *tmp;\n\n\tcancel_delayed_work_sync(&dev->priv.fc_stats.work);\n\tdestroy_workqueue(dev->priv.fc_stats.wq);\n\tdev->priv.fc_stats.wq = NULL;\n\n\ttmplist = llist_del_all(&fc_stats->addlist);\n\tllist_for_each_entry_safe(counter, tmp, tmplist, addlist)\n\t\tmlx5_fc_release(dev, counter);\n\n\tlist_for_each_entry_safe(counter, tmp, &fc_stats->counters, list)\n\t\tmlx5_fc_release(dev, counter);\n\n\tmlx5_fc_pool_cleanup(&fc_stats->fc_pool);\n\tidr_destroy(&fc_stats->counters_idr);\n\tkfree(fc_stats->bulk_query_out);\n}\n\nint mlx5_fc_query(struct mlx5_core_dev *dev, struct mlx5_fc *counter,\n\t\t  u64 *packets, u64 *bytes)\n{\n\treturn mlx5_cmd_fc_query(dev, counter->id, packets, bytes);\n}\nEXPORT_SYMBOL(mlx5_fc_query);\n\nu64 mlx5_fc_query_lastuse(struct mlx5_fc *counter)\n{\n\treturn counter->cache.lastuse;\n}\n\nvoid mlx5_fc_query_cached(struct mlx5_fc *counter,\n\t\t\t  u64 *bytes, u64 *packets, u64 *lastuse)\n{\n\tstruct mlx5_fc_cache c;\n\n\tc = counter->cache;\n\n\t*bytes = c.bytes - counter->lastbytes;\n\t*packets = c.packets - counter->lastpackets;\n\t*lastuse = c.lastuse;\n\n\tcounter->lastbytes = c.bytes;\n\tcounter->lastpackets = c.packets;\n}\n\nvoid mlx5_fc_query_cached_raw(struct mlx5_fc *counter,\n\t\t\t      u64 *bytes, u64 *packets, u64 *lastuse)\n{\n\tstruct mlx5_fc_cache c = counter->cache;\n\n\t*bytes = c.bytes;\n\t*packets = c.packets;\n\t*lastuse = c.lastuse;\n}\n\nvoid mlx5_fc_queue_stats_work(struct mlx5_core_dev *dev,\n\t\t\t      struct delayed_work *dwork,\n\t\t\t      unsigned long delay)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\n\tqueue_delayed_work(fc_stats->wq, dwork, delay);\n}\n\nvoid mlx5_fc_update_sampling_interval(struct mlx5_core_dev *dev,\n\t\t\t\t      unsigned long interval)\n{\n\tstruct mlx5_fc_stats *fc_stats = &dev->priv.fc_stats;\n\n\tfc_stats->sampling_interval = min_t(unsigned long, interval,\n\t\t\t\t\t    fc_stats->sampling_interval);\n}\n\n \n\nstruct mlx5_fc_bulk {\n\tstruct list_head pool_list;\n\tu32 base_id;\n\tint bulk_len;\n\tunsigned long *bitmask;\n\tstruct mlx5_fc fcs[];\n};\n\nstatic void mlx5_fc_init(struct mlx5_fc *counter, struct mlx5_fc_bulk *bulk,\n\t\t\t u32 id)\n{\n\tcounter->bulk = bulk;\n\tcounter->id = id;\n}\n\nstatic int mlx5_fc_bulk_get_free_fcs_amount(struct mlx5_fc_bulk *bulk)\n{\n\treturn bitmap_weight(bulk->bitmask, bulk->bulk_len);\n}\n\nstatic struct mlx5_fc_bulk *mlx5_fc_bulk_create(struct mlx5_core_dev *dev)\n{\n\tenum mlx5_fc_bulk_alloc_bitmask alloc_bitmask;\n\tstruct mlx5_fc_bulk *bulk;\n\tint err = -ENOMEM;\n\tint bulk_len;\n\tu32 base_id;\n\tint i;\n\n\talloc_bitmask = MLX5_CAP_GEN(dev, flow_counter_bulk_alloc);\n\tbulk_len = alloc_bitmask > 0 ? MLX5_FC_BULK_NUM_FCS(alloc_bitmask) : 1;\n\n\tbulk = kvzalloc(struct_size(bulk, fcs, bulk_len), GFP_KERNEL);\n\tif (!bulk)\n\t\tgoto err_alloc_bulk;\n\n\tbulk->bitmask = kvcalloc(BITS_TO_LONGS(bulk_len), sizeof(unsigned long),\n\t\t\t\t GFP_KERNEL);\n\tif (!bulk->bitmask)\n\t\tgoto err_alloc_bitmask;\n\n\terr = mlx5_cmd_fc_bulk_alloc(dev, alloc_bitmask, &base_id);\n\tif (err)\n\t\tgoto err_mlx5_cmd_bulk_alloc;\n\n\tbulk->base_id = base_id;\n\tbulk->bulk_len = bulk_len;\n\tfor (i = 0; i < bulk_len; i++) {\n\t\tmlx5_fc_init(&bulk->fcs[i], bulk, base_id + i);\n\t\tset_bit(i, bulk->bitmask);\n\t}\n\n\treturn bulk;\n\nerr_mlx5_cmd_bulk_alloc:\n\tkvfree(bulk->bitmask);\nerr_alloc_bitmask:\n\tkvfree(bulk);\nerr_alloc_bulk:\n\treturn ERR_PTR(err);\n}\n\nstatic int\nmlx5_fc_bulk_destroy(struct mlx5_core_dev *dev, struct mlx5_fc_bulk *bulk)\n{\n\tif (mlx5_fc_bulk_get_free_fcs_amount(bulk) < bulk->bulk_len) {\n\t\tmlx5_core_err(dev, \"Freeing bulk before all counters were released\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tmlx5_cmd_fc_free(dev, bulk->base_id);\n\tkvfree(bulk->bitmask);\n\tkvfree(bulk);\n\n\treturn 0;\n}\n\nstatic struct mlx5_fc *mlx5_fc_bulk_acquire_fc(struct mlx5_fc_bulk *bulk)\n{\n\tint free_fc_index = find_first_bit(bulk->bitmask, bulk->bulk_len);\n\n\tif (free_fc_index >= bulk->bulk_len)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tclear_bit(free_fc_index, bulk->bitmask);\n\treturn &bulk->fcs[free_fc_index];\n}\n\nstatic int mlx5_fc_bulk_release_fc(struct mlx5_fc_bulk *bulk, struct mlx5_fc *fc)\n{\n\tint fc_index = fc->id - bulk->base_id;\n\n\tif (test_bit(fc_index, bulk->bitmask))\n\t\treturn -EINVAL;\n\n\tset_bit(fc_index, bulk->bitmask);\n\treturn 0;\n}\n\n \n\nstatic void mlx5_fc_pool_init(struct mlx5_fc_pool *fc_pool, struct mlx5_core_dev *dev)\n{\n\tfc_pool->dev = dev;\n\tmutex_init(&fc_pool->pool_lock);\n\tINIT_LIST_HEAD(&fc_pool->fully_used);\n\tINIT_LIST_HEAD(&fc_pool->partially_used);\n\tINIT_LIST_HEAD(&fc_pool->unused);\n\tfc_pool->available_fcs = 0;\n\tfc_pool->used_fcs = 0;\n\tfc_pool->threshold = 0;\n}\n\nstatic void mlx5_fc_pool_cleanup(struct mlx5_fc_pool *fc_pool)\n{\n\tstruct mlx5_core_dev *dev = fc_pool->dev;\n\tstruct mlx5_fc_bulk *bulk;\n\tstruct mlx5_fc_bulk *tmp;\n\n\tlist_for_each_entry_safe(bulk, tmp, &fc_pool->fully_used, pool_list)\n\t\tmlx5_fc_bulk_destroy(dev, bulk);\n\tlist_for_each_entry_safe(bulk, tmp, &fc_pool->partially_used, pool_list)\n\t\tmlx5_fc_bulk_destroy(dev, bulk);\n\tlist_for_each_entry_safe(bulk, tmp, &fc_pool->unused, pool_list)\n\t\tmlx5_fc_bulk_destroy(dev, bulk);\n}\n\nstatic void mlx5_fc_pool_update_threshold(struct mlx5_fc_pool *fc_pool)\n{\n\tfc_pool->threshold = min_t(int, MLX5_FC_POOL_MAX_THRESHOLD,\n\t\t\t\t   fc_pool->used_fcs / MLX5_FC_POOL_USED_BUFF_RATIO);\n}\n\nstatic struct mlx5_fc_bulk *\nmlx5_fc_pool_alloc_new_bulk(struct mlx5_fc_pool *fc_pool)\n{\n\tstruct mlx5_core_dev *dev = fc_pool->dev;\n\tstruct mlx5_fc_bulk *new_bulk;\n\n\tnew_bulk = mlx5_fc_bulk_create(dev);\n\tif (!IS_ERR(new_bulk))\n\t\tfc_pool->available_fcs += new_bulk->bulk_len;\n\tmlx5_fc_pool_update_threshold(fc_pool);\n\treturn new_bulk;\n}\n\nstatic void\nmlx5_fc_pool_free_bulk(struct mlx5_fc_pool *fc_pool, struct mlx5_fc_bulk *bulk)\n{\n\tstruct mlx5_core_dev *dev = fc_pool->dev;\n\n\tfc_pool->available_fcs -= bulk->bulk_len;\n\tmlx5_fc_bulk_destroy(dev, bulk);\n\tmlx5_fc_pool_update_threshold(fc_pool);\n}\n\nstatic struct mlx5_fc *\nmlx5_fc_pool_acquire_from_list(struct list_head *src_list,\n\t\t\t       struct list_head *next_list,\n\t\t\t       bool move_non_full_bulk)\n{\n\tstruct mlx5_fc_bulk *bulk;\n\tstruct mlx5_fc *fc;\n\n\tif (list_empty(src_list))\n\t\treturn ERR_PTR(-ENODATA);\n\n\tbulk = list_first_entry(src_list, struct mlx5_fc_bulk, pool_list);\n\tfc = mlx5_fc_bulk_acquire_fc(bulk);\n\tif (move_non_full_bulk || mlx5_fc_bulk_get_free_fcs_amount(bulk) == 0)\n\t\tlist_move(&bulk->pool_list, next_list);\n\treturn fc;\n}\n\nstatic struct mlx5_fc *\nmlx5_fc_pool_acquire_counter(struct mlx5_fc_pool *fc_pool)\n{\n\tstruct mlx5_fc_bulk *new_bulk;\n\tstruct mlx5_fc *fc;\n\n\tmutex_lock(&fc_pool->pool_lock);\n\n\tfc = mlx5_fc_pool_acquire_from_list(&fc_pool->partially_used,\n\t\t\t\t\t    &fc_pool->fully_used, false);\n\tif (IS_ERR(fc))\n\t\tfc = mlx5_fc_pool_acquire_from_list(&fc_pool->unused,\n\t\t\t\t\t\t    &fc_pool->partially_used,\n\t\t\t\t\t\t    true);\n\tif (IS_ERR(fc)) {\n\t\tnew_bulk = mlx5_fc_pool_alloc_new_bulk(fc_pool);\n\t\tif (IS_ERR(new_bulk)) {\n\t\t\tfc = ERR_CAST(new_bulk);\n\t\t\tgoto out;\n\t\t}\n\t\tfc = mlx5_fc_bulk_acquire_fc(new_bulk);\n\t\tlist_add(&new_bulk->pool_list, &fc_pool->partially_used);\n\t}\n\tfc_pool->available_fcs--;\n\tfc_pool->used_fcs++;\n\nout:\n\tmutex_unlock(&fc_pool->pool_lock);\n\treturn fc;\n}\n\nstatic void\nmlx5_fc_pool_release_counter(struct mlx5_fc_pool *fc_pool, struct mlx5_fc *fc)\n{\n\tstruct mlx5_core_dev *dev = fc_pool->dev;\n\tstruct mlx5_fc_bulk *bulk = fc->bulk;\n\tint bulk_free_fcs_amount;\n\n\tmutex_lock(&fc_pool->pool_lock);\n\n\tif (mlx5_fc_bulk_release_fc(bulk, fc)) {\n\t\tmlx5_core_warn(dev, \"Attempted to release a counter which is not acquired\\n\");\n\t\tgoto unlock;\n\t}\n\n\tfc_pool->available_fcs++;\n\tfc_pool->used_fcs--;\n\n\tbulk_free_fcs_amount = mlx5_fc_bulk_get_free_fcs_amount(bulk);\n\tif (bulk_free_fcs_amount == 1)\n\t\tlist_move_tail(&bulk->pool_list, &fc_pool->partially_used);\n\tif (bulk_free_fcs_amount == bulk->bulk_len) {\n\t\tlist_del(&bulk->pool_list);\n\t\tif (fc_pool->available_fcs > fc_pool->threshold)\n\t\t\tmlx5_fc_pool_free_bulk(fc_pool, bulk);\n\t\telse\n\t\t\tlist_add(&bulk->pool_list, &fc_pool->unused);\n\t}\n\nunlock:\n\tmutex_unlock(&fc_pool->pool_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}