{
  "module_name": "eswitch.h",
  "hash_id": "d40d8bc6a21ac3bbf0ffb8c54af20c986981b3a85377b04a62c15ece3f7b61e8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/eswitch.h",
  "human_readable_source": " \n\n#ifndef __MLX5_ESWITCH_H__\n#define __MLX5_ESWITCH_H__\n\n#include <linux/if_ether.h>\n#include <linux/if_link.h>\n#include <linux/atomic.h>\n#include <linux/xarray.h>\n#include <net/devlink.h>\n#include <linux/mlx5/device.h>\n#include <linux/mlx5/eswitch.h>\n#include <linux/mlx5/vport.h>\n#include <linux/mlx5/fs.h>\n#include \"lib/mpfs.h\"\n#include \"lib/fs_chains.h\"\n#include \"sf/sf.h\"\n#include \"en/tc_ct.h\"\n#include \"en/tc/sample.h\"\n\nenum mlx5_mapped_obj_type {\n\tMLX5_MAPPED_OBJ_CHAIN,\n\tMLX5_MAPPED_OBJ_SAMPLE,\n\tMLX5_MAPPED_OBJ_INT_PORT_METADATA,\n\tMLX5_MAPPED_OBJ_ACT_MISS,\n};\n\nstruct mlx5_mapped_obj {\n\tenum mlx5_mapped_obj_type type;\n\tunion {\n\t\tu32 chain;\n\t\tu64 act_miss_cookie;\n\t\tstruct {\n\t\t\tu32 group_id;\n\t\t\tu32 rate;\n\t\t\tu32 trunc_size;\n\t\t\tu32 tunnel_id;\n\t\t} sample;\n\t\tu32 int_port_metadata;\n\t};\n};\n\n#ifdef CONFIG_MLX5_ESWITCH\n\n#define ESW_OFFLOADS_DEFAULT_NUM_GROUPS 15\n\n#define MLX5_MAX_UC_PER_VPORT(dev) \\\n\t(1 << MLX5_CAP_GEN(dev, log_max_current_uc_list))\n\n#define MLX5_MAX_MC_PER_VPORT(dev) \\\n\t(1 << MLX5_CAP_GEN(dev, log_max_current_mc_list))\n\n#define mlx5_esw_has_fwd_fdb(dev) \\\n\tMLX5_CAP_ESW_FLOWTABLE(dev, fdb_multi_path_to_table)\n\n#define esw_chains(esw) \\\n\t((esw)->fdb_table.offloads.esw_chains_priv)\n\nenum {\n\tMAPPING_TYPE_CHAIN,\n\tMAPPING_TYPE_TUNNEL,\n\tMAPPING_TYPE_TUNNEL_ENC_OPTS,\n\tMAPPING_TYPE_LABELS,\n\tMAPPING_TYPE_ZONE,\n\tMAPPING_TYPE_INT_PORT,\n};\n\nstruct vport_ingress {\n\tstruct mlx5_flow_table *acl;\n\tstruct mlx5_flow_handle *allow_rule;\n\tstruct {\n\t\tstruct mlx5_flow_group *allow_spoofchk_only_grp;\n\t\tstruct mlx5_flow_group *allow_untagged_spoofchk_grp;\n\t\tstruct mlx5_flow_group *allow_untagged_only_grp;\n\t\tstruct mlx5_flow_group *drop_grp;\n\t\tstruct mlx5_flow_handle *drop_rule;\n\t\tstruct mlx5_fc *drop_counter;\n\t} legacy;\n\tstruct {\n\t\t \n\t\tstruct mlx5_flow_group *metadata_prio_tag_grp;\n\t\t \n\t\tstruct mlx5_flow_group *metadata_allmatch_grp;\n\t\t \n\t\tstruct mlx5_flow_group *drop_grp;\n\t\tstruct mlx5_modify_hdr *modify_metadata;\n\t\tstruct mlx5_flow_handle *modify_metadata_rule;\n\t\tstruct mlx5_flow_handle *drop_rule;\n\t} offloads;\n};\n\nenum vport_egress_acl_type {\n\tVPORT_EGRESS_ACL_TYPE_DEFAULT,\n\tVPORT_EGRESS_ACL_TYPE_SHARED_FDB,\n};\n\nstruct vport_egress {\n\tstruct mlx5_flow_table *acl;\n\tenum vport_egress_acl_type type;\n\tstruct mlx5_flow_handle  *allowed_vlan;\n\tstruct mlx5_flow_group *vlan_grp;\n\tunion {\n\t\tstruct {\n\t\t\tstruct mlx5_flow_group *drop_grp;\n\t\t\tstruct mlx5_flow_handle *drop_rule;\n\t\t\tstruct mlx5_fc *drop_counter;\n\t\t} legacy;\n\t\tstruct {\n\t\t\tstruct mlx5_flow_group *fwd_grp;\n\t\t\tstruct mlx5_flow_handle *fwd_rule;\n\t\t\tstruct xarray bounce_rules;\n\t\t\tstruct mlx5_flow_group *bounce_grp;\n\t\t} offloads;\n\t};\n};\n\nstruct mlx5_vport_drop_stats {\n\tu64 rx_dropped;\n\tu64 tx_dropped;\n};\n\nstruct mlx5_vport_info {\n\tu8                      mac[ETH_ALEN];\n\tu16                     vlan;\n\tu64                     node_guid;\n\tint                     link_state;\n\tu8                      qos;\n\tu8                      spoofchk: 1;\n\tu8                      trusted: 1;\n\tu8                      roce_enabled: 1;\n\tu8                      mig_enabled: 1;\n\tu8                      ipsec_crypto_enabled: 1;\n\tu8                      ipsec_packet_enabled: 1;\n};\n\n \nenum mlx5_eswitch_vport_event {\n\tMLX5_VPORT_UC_ADDR_CHANGE = BIT(0),\n\tMLX5_VPORT_MC_ADDR_CHANGE = BIT(1),\n\tMLX5_VPORT_PROMISC_CHANGE = BIT(3),\n};\n\nstruct mlx5_vport;\n\nstruct mlx5_devlink_port {\n\tstruct devlink_port dl_port;\n\tstruct mlx5_vport *vport;\n};\n\nstatic inline void mlx5_devlink_port_init(struct mlx5_devlink_port *dl_port,\n\t\t\t\t\t  struct mlx5_vport *vport)\n{\n\tdl_port->vport = vport;\n}\n\nstatic inline struct mlx5_devlink_port *mlx5_devlink_port_get(struct devlink_port *dl_port)\n{\n\treturn container_of(dl_port, struct mlx5_devlink_port, dl_port);\n}\n\nstatic inline struct mlx5_vport *mlx5_devlink_port_vport_get(struct devlink_port *dl_port)\n{\n\treturn mlx5_devlink_port_get(dl_port)->vport;\n}\n\nstruct mlx5_vport {\n\tstruct mlx5_core_dev    *dev;\n\tstruct hlist_head       uc_list[MLX5_L2_ADDR_HASH_SIZE];\n\tstruct hlist_head       mc_list[MLX5_L2_ADDR_HASH_SIZE];\n\tstruct mlx5_flow_handle *promisc_rule;\n\tstruct mlx5_flow_handle *allmulti_rule;\n\tstruct work_struct      vport_change_handler;\n\n\tstruct vport_ingress    ingress;\n\tstruct vport_egress     egress;\n\tu32                     default_metadata;\n\tu32                     metadata;\n\n\tstruct mlx5_vport_info  info;\n\n\tstruct {\n\t\tbool            enabled;\n\t\tu32             esw_tsar_ix;\n\t\tu32             bw_share;\n\t\tu32 min_rate;\n\t\tu32 max_rate;\n\t\tstruct mlx5_esw_rate_group *group;\n\t} qos;\n\n\tu16 vport;\n\tbool                    enabled;\n\tenum mlx5_eswitch_vport_event enabled_events;\n\tint index;\n\tstruct mlx5_devlink_port *dl_port;\n};\n\nstruct mlx5_esw_indir_table;\n\nstruct mlx5_eswitch_fdb {\n\tunion {\n\t\tstruct legacy_fdb {\n\t\t\tstruct mlx5_flow_table *fdb;\n\t\t\tstruct mlx5_flow_group *addr_grp;\n\t\t\tstruct mlx5_flow_group *allmulti_grp;\n\t\t\tstruct mlx5_flow_group *promisc_grp;\n\t\t\tstruct mlx5_flow_table *vepa_fdb;\n\t\t\tstruct mlx5_flow_handle *vepa_uplink_rule;\n\t\t\tstruct mlx5_flow_handle *vepa_star_rule;\n\t\t} legacy;\n\n\t\tstruct offloads_fdb {\n\t\t\tstruct mlx5_flow_namespace *ns;\n\t\t\tstruct mlx5_flow_table *tc_miss_table;\n\t\t\tstruct mlx5_flow_table *slow_fdb;\n\t\t\tstruct mlx5_flow_group *send_to_vport_grp;\n\t\t\tstruct mlx5_flow_group *send_to_vport_meta_grp;\n\t\t\tstruct mlx5_flow_group *peer_miss_grp;\n\t\t\tstruct mlx5_flow_handle **peer_miss_rules[MLX5_MAX_PORTS];\n\t\t\tstruct mlx5_flow_group *miss_grp;\n\t\t\tstruct mlx5_flow_handle **send_to_vport_meta_rules;\n\t\t\tstruct mlx5_flow_handle *miss_rule_uni;\n\t\t\tstruct mlx5_flow_handle *miss_rule_multi;\n\n\t\t\tstruct mlx5_fs_chains *esw_chains_priv;\n\t\t\tstruct {\n\t\t\t\tDECLARE_HASHTABLE(table, 8);\n\t\t\t\t \n\t\t\t\tstruct mutex lock;\n\t\t\t} vports;\n\n\t\t\tstruct mlx5_esw_indir_table *indir;\n\n\t\t} offloads;\n\t};\n\tu32 flags;\n};\n\nstruct mlx5_esw_offload {\n\tstruct mlx5_flow_table *ft_offloads_restore;\n\tstruct mlx5_flow_group *restore_group;\n\tstruct mlx5_modify_hdr *restore_copy_hdr_id;\n\tstruct mapping_ctx *reg_c0_obj_pool;\n\n\tstruct mlx5_flow_table *ft_offloads;\n\tstruct mlx5_flow_group *vport_rx_group;\n\tstruct mlx5_flow_group *vport_rx_drop_group;\n\tstruct mlx5_flow_handle *vport_rx_drop_rule;\n\tstruct mlx5_flow_table *ft_ipsec_tx_pol;\n\tstruct xarray vport_reps;\n\tstruct list_head peer_flows[MLX5_MAX_PORTS];\n\tstruct mutex peer_mutex;\n\tstruct mutex encap_tbl_lock;  \n\tDECLARE_HASHTABLE(encap_tbl, 8);\n\tstruct mutex decap_tbl_lock;  \n\tDECLARE_HASHTABLE(decap_tbl, 8);\n\tstruct mod_hdr_tbl mod_hdr;\n\tDECLARE_HASHTABLE(termtbl_tbl, 8);\n\tstruct mutex termtbl_mutex;  \n\tstruct xarray vhca_map;\n\tconst struct mlx5_eswitch_rep_ops *rep_ops[NUM_REP_TYPES];\n\tu8 inline_mode;\n\tatomic64_t num_flows;\n\tu64 num_block_encap;\n\tu64 num_block_mode;\n\tenum devlink_eswitch_encap_mode encap;\n\tstruct ida vport_metadata_ida;\n\tunsigned int host_number;  \n};\n\n \nstruct esw_mc_addr {  \n\tstruct l2addr_node     node;\n\tstruct mlx5_flow_handle *uplink_rule;  \n\tu32                    refcnt;\n};\n\nstruct mlx5_host_work {\n\tstruct work_struct\twork;\n\tstruct mlx5_eswitch\t*esw;\n};\n\nstruct mlx5_esw_functions {\n\tstruct mlx5_nb\t\tnb;\n\tu16\t\t\tnum_vfs;\n\tu16\t\t\tnum_ec_vfs;\n};\n\nenum {\n\tMLX5_ESWITCH_VPORT_MATCH_METADATA = BIT(0),\n\tMLX5_ESWITCH_REG_C1_LOOPBACK_ENABLED = BIT(1),\n\tMLX5_ESWITCH_VPORT_ACL_NS_CREATED = BIT(2),\n};\n\nstruct mlx5_esw_bridge_offloads;\n\nenum {\n\tMLX5_ESW_FDB_CREATED = BIT(0),\n};\n\nstruct dentry;\n\nstruct mlx5_eswitch {\n\tstruct mlx5_core_dev    *dev;\n\tstruct mlx5_nb          nb;\n\tstruct mlx5_eswitch_fdb fdb_table;\n\t \n\tstruct hlist_head       mc_table[MLX5_L2_ADDR_HASH_SIZE];\n\tstruct esw_mc_addr mc_promisc;\n\t \n\tstruct dentry *debugfs_root;\n\tstruct workqueue_struct *work_queue;\n\tstruct xarray vports;\n\tu32 flags;\n\tint                     total_vports;\n\tint                     enabled_vports;\n\t \n\tstruct mutex            state_lock;\n\n\t \n\tstruct rw_semaphore mode_lock;\n\tatomic64_t user_count;\n\n\tstruct {\n\t\tu32             root_tsar_ix;\n\t\tstruct mlx5_esw_rate_group *group0;\n\t\tstruct list_head groups;  \n\n\t\t \n\t\trefcount_t refcnt;\n\t} qos;\n\n\tstruct mlx5_esw_bridge_offloads *br_offloads;\n\tstruct mlx5_esw_offload offloads;\n\tint                     mode;\n\tu16                     manager_vport;\n\tu16                     first_host_vport;\n\tu8\t\t\tnum_peers;\n\tstruct mlx5_esw_functions esw_funcs;\n\tstruct {\n\t\tu32             large_group_num;\n\t}  params;\n\tstruct blocking_notifier_head n_head;\n\tstruct xarray paired;\n\tstruct mlx5_devcom_comp_dev *devcom;\n\tu16 enabled_ipsec_vf_count;\n\tbool eswitch_operation_in_progress;\n};\n\nvoid esw_offloads_disable(struct mlx5_eswitch *esw);\nint esw_offloads_enable(struct mlx5_eswitch *esw);\nvoid esw_offloads_cleanup(struct mlx5_eswitch *esw);\nint esw_offloads_init(struct mlx5_eswitch *esw);\n\nstruct mlx5_flow_handle *\nmlx5_eswitch_add_send_to_vport_meta_rule(struct mlx5_eswitch *esw, u16 vport_num);\nvoid mlx5_eswitch_del_send_to_vport_meta_rule(struct mlx5_flow_handle *rule);\n\nbool mlx5_esw_vport_match_metadata_supported(const struct mlx5_eswitch *esw);\nu32 mlx5_esw_match_metadata_alloc(struct mlx5_eswitch *esw);\nvoid mlx5_esw_match_metadata_free(struct mlx5_eswitch *esw, u32 metadata);\n\nint mlx5_esw_qos_modify_vport_rate(struct mlx5_eswitch *esw, u16 vport_num, u32 rate_mbps);\n\n \nint mlx5_eswitch_init(struct mlx5_core_dev *dev);\nvoid mlx5_eswitch_cleanup(struct mlx5_eswitch *esw);\n\n#define MLX5_ESWITCH_IGNORE_NUM_VFS (-1)\nint mlx5_eswitch_enable_locked(struct mlx5_eswitch *esw, int num_vfs);\nint mlx5_eswitch_enable(struct mlx5_eswitch *esw, int num_vfs);\nvoid mlx5_eswitch_disable_sriov(struct mlx5_eswitch *esw, bool clear_vf);\nvoid mlx5_eswitch_disable_locked(struct mlx5_eswitch *esw);\nvoid mlx5_eswitch_disable(struct mlx5_eswitch *esw);\nvoid mlx5_esw_offloads_devcom_init(struct mlx5_eswitch *esw, u64 key);\nvoid mlx5_esw_offloads_devcom_cleanup(struct mlx5_eswitch *esw);\nbool mlx5_esw_offloads_devcom_is_ready(struct mlx5_eswitch *esw);\nint mlx5_eswitch_set_vport_mac(struct mlx5_eswitch *esw,\n\t\t\t       u16 vport, const u8 *mac);\nint mlx5_eswitch_set_vport_state(struct mlx5_eswitch *esw,\n\t\t\t\t u16 vport, int link_state);\nint mlx5_eswitch_set_vport_vlan(struct mlx5_eswitch *esw,\n\t\t\t\tu16 vport, u16 vlan, u8 qos);\nint mlx5_eswitch_set_vport_spoofchk(struct mlx5_eswitch *esw,\n\t\t\t\t    u16 vport, bool spoofchk);\nint mlx5_eswitch_set_vport_trust(struct mlx5_eswitch *esw,\n\t\t\t\t u16 vport_num, bool setting);\nint mlx5_eswitch_set_vport_rate(struct mlx5_eswitch *esw, u16 vport,\n\t\t\t\tu32 max_rate, u32 min_rate);\nint mlx5_esw_qos_vport_update_group(struct mlx5_eswitch *esw,\n\t\t\t\t    struct mlx5_vport *vport,\n\t\t\t\t    struct mlx5_esw_rate_group *group,\n\t\t\t\t    struct netlink_ext_ack *extack);\nint mlx5_eswitch_set_vepa(struct mlx5_eswitch *esw, u8 setting);\nint mlx5_eswitch_get_vepa(struct mlx5_eswitch *esw, u8 *setting);\nint mlx5_eswitch_get_vport_config(struct mlx5_eswitch *esw,\n\t\t\t\t  u16 vport, struct ifla_vf_info *ivi);\nint mlx5_eswitch_get_vport_stats(struct mlx5_eswitch *esw,\n\t\t\t\t u16 vport,\n\t\t\t\t struct ifla_vf_stats *vf_stats);\nvoid mlx5_eswitch_del_send_to_vport_rule(struct mlx5_flow_handle *rule);\n\nint mlx5_eswitch_modify_esw_vport_context(struct mlx5_core_dev *dev, u16 vport,\n\t\t\t\t\t  bool other_vport, void *in);\n\nstruct mlx5_flow_spec;\nstruct mlx5_esw_flow_attr;\nstruct mlx5_termtbl_handle;\n\nbool\nmlx5_eswitch_termtbl_required(struct mlx5_eswitch *esw,\n\t\t\t      struct mlx5_flow_attr *attr,\n\t\t\t      struct mlx5_flow_act *flow_act,\n\t\t\t      struct mlx5_flow_spec *spec);\n\nstruct mlx5_flow_handle *\nmlx5_eswitch_add_termtbl_rule(struct mlx5_eswitch *esw,\n\t\t\t      struct mlx5_flow_table *ft,\n\t\t\t      struct mlx5_flow_spec *spec,\n\t\t\t      struct mlx5_esw_flow_attr *attr,\n\t\t\t      struct mlx5_flow_act *flow_act,\n\t\t\t      struct mlx5_flow_destination *dest,\n\t\t\t      int num_dest);\n\nvoid\nmlx5_eswitch_termtbl_put(struct mlx5_eswitch *esw,\n\t\t\t struct mlx5_termtbl_handle *tt);\n\nvoid\nmlx5_eswitch_clear_rule_source_port(struct mlx5_eswitch *esw, struct mlx5_flow_spec *spec);\n\nstruct mlx5_flow_handle *\nmlx5_eswitch_add_offloaded_rule(struct mlx5_eswitch *esw,\n\t\t\t\tstruct mlx5_flow_spec *spec,\n\t\t\t\tstruct mlx5_flow_attr *attr);\nstruct mlx5_flow_handle *\nmlx5_eswitch_add_fwd_rule(struct mlx5_eswitch *esw,\n\t\t\t  struct mlx5_flow_spec *spec,\n\t\t\t  struct mlx5_flow_attr *attr);\nvoid\nmlx5_eswitch_del_offloaded_rule(struct mlx5_eswitch *esw,\n\t\t\t\tstruct mlx5_flow_handle *rule,\n\t\t\t\tstruct mlx5_flow_attr *attr);\nvoid\nmlx5_eswitch_del_fwd_rule(struct mlx5_eswitch *esw,\n\t\t\t  struct mlx5_flow_handle *rule,\n\t\t\t  struct mlx5_flow_attr *attr);\n\nstruct mlx5_flow_handle *\nmlx5_eswitch_create_vport_rx_rule(struct mlx5_eswitch *esw, u16 vport,\n\t\t\t\t  struct mlx5_flow_destination *dest);\n\nenum {\n\tSET_VLAN_STRIP\t= BIT(0),\n\tSET_VLAN_INSERT\t= BIT(1)\n};\n\nenum mlx5_flow_match_level {\n\tMLX5_MATCH_NONE\t= MLX5_INLINE_MODE_NONE,\n\tMLX5_MATCH_L2\t= MLX5_INLINE_MODE_L2,\n\tMLX5_MATCH_L3\t= MLX5_INLINE_MODE_IP,\n\tMLX5_MATCH_L4\t= MLX5_INLINE_MODE_TCP_UDP,\n};\n\n \n#define MLX5_MAX_FLOW_FWD_VPORTS 32\n\nenum {\n\tMLX5_ESW_DEST_ENCAP         = BIT(0),\n\tMLX5_ESW_DEST_ENCAP_VALID   = BIT(1),\n\tMLX5_ESW_DEST_CHAIN_WITH_SRC_PORT_CHANGE  = BIT(2),\n};\n\nstruct mlx5_esw_flow_attr {\n\tstruct mlx5_eswitch_rep *in_rep;\n\tstruct mlx5_core_dev\t*in_mdev;\n\tstruct mlx5_core_dev    *counter_dev;\n\tstruct mlx5e_tc_int_port *dest_int_port;\n\tstruct mlx5e_tc_int_port *int_port;\n\n\tint split_count;\n\tint out_count;\n\n\t__be16\tvlan_proto[MLX5_FS_VLAN_DEPTH];\n\tu16\tvlan_vid[MLX5_FS_VLAN_DEPTH];\n\tu8\tvlan_prio[MLX5_FS_VLAN_DEPTH];\n\tu8\ttotal_vlan;\n\tstruct {\n\t\tu32 flags;\n\t\tbool vport_valid;\n\t\tu16 vport;\n\t\tstruct mlx5_pkt_reformat *pkt_reformat;\n\t\tstruct mlx5_core_dev *mdev;\n\t\tstruct mlx5_termtbl_handle *termtbl;\n\t\tint src_port_rewrite_act_id;\n\t} dests[MLX5_MAX_FLOW_FWD_VPORTS];\n\tstruct mlx5_rx_tun_attr *rx_tun_attr;\n\tstruct ethhdr eth;\n\tstruct mlx5_pkt_reformat *decap_pkt_reformat;\n};\n\nint mlx5_devlink_eswitch_mode_set(struct devlink *devlink, u16 mode,\n\t\t\t\t  struct netlink_ext_ack *extack);\nint mlx5_devlink_eswitch_mode_get(struct devlink *devlink, u16 *mode);\nint mlx5_devlink_eswitch_inline_mode_set(struct devlink *devlink, u8 mode,\n\t\t\t\t\t struct netlink_ext_ack *extack);\nint mlx5_devlink_eswitch_inline_mode_get(struct devlink *devlink, u8 *mode);\nint mlx5_devlink_eswitch_encap_mode_set(struct devlink *devlink,\n\t\t\t\t\tenum devlink_eswitch_encap_mode encap,\n\t\t\t\t\tstruct netlink_ext_ack *extack);\nint mlx5_devlink_eswitch_encap_mode_get(struct devlink *devlink,\n\t\t\t\t\tenum devlink_eswitch_encap_mode *encap);\nint mlx5_devlink_port_fn_hw_addr_get(struct devlink_port *port,\n\t\t\t\t     u8 *hw_addr, int *hw_addr_len,\n\t\t\t\t     struct netlink_ext_ack *extack);\nint mlx5_devlink_port_fn_hw_addr_set(struct devlink_port *port,\n\t\t\t\t     const u8 *hw_addr, int hw_addr_len,\n\t\t\t\t     struct netlink_ext_ack *extack);\nint mlx5_devlink_port_fn_roce_get(struct devlink_port *port, bool *is_enabled,\n\t\t\t\t  struct netlink_ext_ack *extack);\nint mlx5_devlink_port_fn_roce_set(struct devlink_port *port, bool enable,\n\t\t\t\t  struct netlink_ext_ack *extack);\nint mlx5_devlink_port_fn_migratable_get(struct devlink_port *port, bool *is_enabled,\n\t\t\t\t\tstruct netlink_ext_ack *extack);\nint mlx5_devlink_port_fn_migratable_set(struct devlink_port *port, bool enable,\n\t\t\t\t\tstruct netlink_ext_ack *extack);\n#ifdef CONFIG_XFRM_OFFLOAD\nint mlx5_devlink_port_fn_ipsec_crypto_get(struct devlink_port *port, bool *is_enabled,\n\t\t\t\t\t  struct netlink_ext_ack *extack);\nint mlx5_devlink_port_fn_ipsec_crypto_set(struct devlink_port *port, bool enable,\n\t\t\t\t\t  struct netlink_ext_ack *extack);\nint mlx5_devlink_port_fn_ipsec_packet_get(struct devlink_port *port, bool *is_enabled,\n\t\t\t\t\t  struct netlink_ext_ack *extack);\nint mlx5_devlink_port_fn_ipsec_packet_set(struct devlink_port *port, bool enable,\n\t\t\t\t\t  struct netlink_ext_ack *extack);\n#endif  \nvoid *mlx5_eswitch_get_uplink_priv(struct mlx5_eswitch *esw, u8 rep_type);\n\nint __mlx5_eswitch_set_vport_vlan(struct mlx5_eswitch *esw,\n\t\t\t\t  u16 vport, u16 vlan, u8 qos, u8 set_flags);\n\nstatic inline bool esw_vst_mode_is_steering(struct mlx5_eswitch *esw)\n{\n\treturn (MLX5_CAP_ESW_EGRESS_ACL(esw->dev, pop_vlan) &&\n\t\tMLX5_CAP_ESW_INGRESS_ACL(esw->dev, push_vlan));\n}\n\nstatic inline bool mlx5_eswitch_vlan_actions_supported(struct mlx5_core_dev *dev,\n\t\t\t\t\t\t       u8 vlan_depth)\n{\n\tbool ret = MLX5_CAP_ESW_FLOWTABLE_FDB(dev, pop_vlan) &&\n\t\t   MLX5_CAP_ESW_FLOWTABLE_FDB(dev, push_vlan);\n\n\tif (vlan_depth == 1)\n\t\treturn ret;\n\n\treturn  ret && MLX5_CAP_ESW_FLOWTABLE_FDB(dev, pop_vlan_2) &&\n\t\tMLX5_CAP_ESW_FLOWTABLE_FDB(dev, push_vlan_2);\n}\n\nbool mlx5_esw_multipath_prereq(struct mlx5_core_dev *dev0,\n\t\t\t       struct mlx5_core_dev *dev1);\n\nconst u32 *mlx5_esw_query_functions(struct mlx5_core_dev *dev);\n\n#define MLX5_DEBUG_ESWITCH_MASK BIT(3)\n\n#define esw_info(__dev, format, ...)\t\t\t\\\n\tdev_info((__dev)->device, \"E-Switch: \" format, ##__VA_ARGS__)\n\n#define esw_warn(__dev, format, ...)\t\t\t\\\n\tdev_warn((__dev)->device, \"E-Switch: \" format, ##__VA_ARGS__)\n\n#define esw_debug(dev, format, ...)\t\t\t\t\\\n\tmlx5_core_dbg_mask(dev, MLX5_DEBUG_ESWITCH_MASK, format, ##__VA_ARGS__)\n\nstatic inline bool mlx5_esw_allowed(const struct mlx5_eswitch *esw)\n{\n\treturn esw && MLX5_ESWITCH_MANAGER(esw->dev);\n}\n\n \nstatic inline u16 mlx5_eswitch_manager_vport(struct mlx5_core_dev *dev)\n{\n\treturn mlx5_core_is_ecpf_esw_manager(dev) ?\n\t\tMLX5_VPORT_ECPF : MLX5_VPORT_PF;\n}\n\nstatic inline bool\nmlx5_esw_is_manager_vport(const struct mlx5_eswitch *esw, u16 vport_num)\n{\n\treturn esw->manager_vport == vport_num;\n}\n\nstatic inline bool mlx5_esw_is_owner(struct mlx5_eswitch *esw, u16 vport_num,\n\t\t\t\t     u16 esw_owner_vhca_id)\n{\n\treturn esw_owner_vhca_id == MLX5_CAP_GEN(esw->dev, vhca_id) ||\n\t\t(vport_num == MLX5_VPORT_UPLINK && mlx5_lag_is_master(esw->dev));\n}\n\nstatic inline u16 mlx5_eswitch_first_host_vport_num(struct mlx5_core_dev *dev)\n{\n\treturn mlx5_core_is_ecpf_esw_manager(dev) ?\n\t\tMLX5_VPORT_PF : MLX5_VPORT_FIRST_VF;\n}\n\nstatic inline bool mlx5_eswitch_is_funcs_handler(const struct mlx5_core_dev *dev)\n{\n\treturn mlx5_core_is_ecpf_esw_manager(dev);\n}\n\nstatic inline unsigned int\nmlx5_esw_vport_to_devlink_port_index(const struct mlx5_core_dev *dev,\n\t\t\t\t     u16 vport_num)\n{\n\treturn (MLX5_CAP_GEN(dev, vhca_id) << 16) | vport_num;\n}\n\nstatic inline u16\nmlx5_esw_devlink_port_index_to_vport_num(unsigned int dl_port_index)\n{\n\treturn dl_port_index & 0xffff;\n}\n\nstatic inline bool mlx5_esw_is_fdb_created(struct mlx5_eswitch *esw)\n{\n\treturn esw->fdb_table.flags & MLX5_ESW_FDB_CREATED;\n}\n\n \nvoid mlx5e_tc_clean_fdb_peer_flows(struct mlx5_eswitch *esw);\n\n \n#define MLX5_ESW_VPT_HOST_FN XA_MARK_0\n#define MLX5_ESW_VPT_VF XA_MARK_1\n#define MLX5_ESW_VPT_SF XA_MARK_2\n\n \n\n#define mlx5_esw_for_each_vport(esw, index, vport) \\\n\txa_for_each(&((esw)->vports), index, vport)\n\n#define mlx5_esw_for_each_entry_marked(xa, index, entry, last, filter)\t\\\n\tfor (index = 0, entry = xa_find(xa, &index, last, filter); \\\n\t     entry; entry = xa_find_after(xa, &index, last, filter))\n\n#define mlx5_esw_for_each_vport_marked(esw, index, vport, last, filter)\t\\\n\tmlx5_esw_for_each_entry_marked(&((esw)->vports), index, vport, last, filter)\n\n#define mlx5_esw_for_each_vf_vport(esw, index, vport, last)\t\\\n\tmlx5_esw_for_each_vport_marked(esw, index, vport, last, MLX5_ESW_VPT_VF)\n\n#define mlx5_esw_for_each_host_func_vport(esw, index, vport, last)\t\\\n\tmlx5_esw_for_each_vport_marked(esw, index, vport, last, MLX5_ESW_VPT_HOST_FN)\n\n \n#define mlx5_esw_for_each_ec_vf_vport(esw, index, vport, last)\t\t\\\n\txa_for_each_range(&((esw)->vports),\t\t\t\t\\\n\t\t\t  index,\t\t\t\t\t\\\n\t\t\t  vport,\t\t\t\t\t\\\n\t\t\t  MLX5_CAP_GEN_2((esw->dev), ec_vf_vport_base),\t\\\n\t\t\t  MLX5_CAP_GEN_2((esw->dev), ec_vf_vport_base) +\\\n\t\t\t  (last) - 1)\n\nstruct mlx5_eswitch *__must_check\nmlx5_devlink_eswitch_get(struct devlink *devlink);\n\nstruct mlx5_eswitch *mlx5_devlink_eswitch_nocheck_get(struct devlink *devlink);\n\nstruct mlx5_vport *__must_check\nmlx5_eswitch_get_vport(struct mlx5_eswitch *esw, u16 vport_num);\n\nbool mlx5_eswitch_is_vf_vport(struct mlx5_eswitch *esw, u16 vport_num);\nbool mlx5_eswitch_is_pf_vf_vport(struct mlx5_eswitch *esw, u16 vport_num);\nbool mlx5_esw_is_sf_vport(struct mlx5_eswitch *esw, u16 vport_num);\n\nint mlx5_esw_funcs_changed_handler(struct notifier_block *nb, unsigned long type, void *data);\n\nint\nmlx5_eswitch_enable_pf_vf_vports(struct mlx5_eswitch *esw,\n\t\t\t\t enum mlx5_eswitch_vport_event enabled_events);\nvoid mlx5_eswitch_disable_pf_vf_vports(struct mlx5_eswitch *esw);\n\nint mlx5_esw_vport_enable(struct mlx5_eswitch *esw, struct mlx5_vport *vport,\n\t\t\t  enum mlx5_eswitch_vport_event enabled_events);\nvoid mlx5_esw_vport_disable(struct mlx5_eswitch *esw, struct mlx5_vport *vport);\n\nint\nesw_vport_create_offloads_acl_tables(struct mlx5_eswitch *esw,\n\t\t\t\t     struct mlx5_vport *vport);\nvoid\nesw_vport_destroy_offloads_acl_tables(struct mlx5_eswitch *esw,\n\t\t\t\t      struct mlx5_vport *vport);\n\nstruct esw_vport_tbl_namespace {\n\tint max_fte;\n\tint max_num_groups;\n\tu32 flags;\n};\n\nstruct mlx5_vport_tbl_attr {\n\tu32 chain;\n\tu16 prio;\n\tu16 vport;\n\tstruct esw_vport_tbl_namespace *vport_ns;\n};\n\nstruct mlx5_flow_table *\nmlx5_esw_vporttbl_get(struct mlx5_eswitch *esw, struct mlx5_vport_tbl_attr *attr);\nvoid\nmlx5_esw_vporttbl_put(struct mlx5_eswitch *esw, struct mlx5_vport_tbl_attr *attr);\n\nstruct mlx5_flow_handle *\nesw_add_restore_rule(struct mlx5_eswitch *esw, u32 tag);\n\nvoid mlx5_esw_set_flow_group_source_port(struct mlx5_eswitch *esw,\n\t\t\t\t\t u32 *flow_group_in,\n\t\t\t\t\t int match_params);\n\nvoid mlx5_esw_set_spec_source_port(struct mlx5_eswitch *esw,\n\t\t\t\t   u16 vport,\n\t\t\t\t   struct mlx5_flow_spec *spec);\n\nint mlx5_esw_offloads_init_pf_vf_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport);\nvoid mlx5_esw_offloads_cleanup_pf_vf_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport);\n\nint mlx5_esw_offloads_init_sf_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport,\n\t\t\t\t  struct mlx5_devlink_port *dl_port,\n\t\t\t\t  u32 controller, u32 sfnum);\nvoid mlx5_esw_offloads_cleanup_sf_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport);\n\nint mlx5_esw_offloads_load_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport);\nvoid mlx5_esw_offloads_unload_rep(struct mlx5_eswitch *esw, struct mlx5_vport *vport);\n\nint mlx5_eswitch_load_sf_vport(struct mlx5_eswitch *esw, u16 vport_num,\n\t\t\t       enum mlx5_eswitch_vport_event enabled_events,\n\t\t\t       struct mlx5_devlink_port *dl_port, u32 controller, u32 sfnum);\nvoid mlx5_eswitch_unload_sf_vport(struct mlx5_eswitch *esw, u16 vport_num);\n\nint mlx5_eswitch_load_vf_vports(struct mlx5_eswitch *esw, u16 num_vfs,\n\t\t\t\tenum mlx5_eswitch_vport_event enabled_events);\nvoid mlx5_eswitch_unload_vf_vports(struct mlx5_eswitch *esw, u16 num_vfs);\n\nint mlx5_esw_offloads_pf_vf_devlink_port_init(struct mlx5_eswitch *esw,\n\t\t\t\t\t      struct mlx5_vport *vport);\nvoid mlx5_esw_offloads_pf_vf_devlink_port_cleanup(struct mlx5_eswitch *esw,\n\t\t\t\t\t\t  struct mlx5_vport *vport);\n\nint mlx5_esw_offloads_sf_devlink_port_init(struct mlx5_eswitch *esw, struct mlx5_vport *vport,\n\t\t\t\t\t   struct mlx5_devlink_port *dl_port,\n\t\t\t\t\t   u32 controller, u32 sfnum);\nvoid mlx5_esw_offloads_sf_devlink_port_cleanup(struct mlx5_eswitch *esw, struct mlx5_vport *vport);\n\nint mlx5_esw_offloads_devlink_port_register(struct mlx5_eswitch *esw, struct mlx5_vport *vport);\nvoid mlx5_esw_offloads_devlink_port_unregister(struct mlx5_eswitch *esw, struct mlx5_vport *vport);\nstruct devlink_port *mlx5_esw_offloads_devlink_port(struct mlx5_eswitch *esw, u16 vport_num);\n\nint mlx5_esw_sf_max_hpf_functions(struct mlx5_core_dev *dev, u16 *max_sfs, u16 *sf_base_id);\n\nint mlx5_esw_vport_vhca_id_set(struct mlx5_eswitch *esw, u16 vport_num);\nvoid mlx5_esw_vport_vhca_id_clear(struct mlx5_eswitch *esw, u16 vport_num);\nint mlx5_eswitch_vhca_id_to_vport(struct mlx5_eswitch *esw, u16 vhca_id, u16 *vport_num);\n\n \nstruct mlx5_esw_event_info {\n\tu16 new_mode;\n};\n\nint mlx5_esw_event_notifier_register(struct mlx5_eswitch *esw, struct notifier_block *n);\nvoid mlx5_esw_event_notifier_unregister(struct mlx5_eswitch *esw, struct notifier_block *n);\n\nbool mlx5_esw_hold(struct mlx5_core_dev *dev);\nvoid mlx5_esw_release(struct mlx5_core_dev *dev);\nvoid mlx5_esw_get(struct mlx5_core_dev *dev);\nvoid mlx5_esw_put(struct mlx5_core_dev *dev);\nint mlx5_esw_try_lock(struct mlx5_eswitch *esw);\nint mlx5_esw_lock(struct mlx5_eswitch *esw);\nvoid mlx5_esw_unlock(struct mlx5_eswitch *esw);\n\nvoid esw_vport_change_handle_locked(struct mlx5_vport *vport);\n\nbool mlx5_esw_offloads_controller_valid(const struct mlx5_eswitch *esw, u32 controller);\n\nint mlx5_eswitch_offloads_single_fdb_add_one(struct mlx5_eswitch *master_esw,\n\t\t\t\t\t     struct mlx5_eswitch *slave_esw, int max_slaves);\nvoid mlx5_eswitch_offloads_single_fdb_del_one(struct mlx5_eswitch *master_esw,\n\t\t\t\t\t      struct mlx5_eswitch *slave_esw);\nint mlx5_eswitch_reload_reps(struct mlx5_eswitch *esw);\n\nbool mlx5_eswitch_block_encap(struct mlx5_core_dev *dev);\nvoid mlx5_eswitch_unblock_encap(struct mlx5_core_dev *dev);\n\nint mlx5_eswitch_block_mode(struct mlx5_core_dev *dev);\nvoid mlx5_eswitch_unblock_mode(struct mlx5_core_dev *dev);\n\nstatic inline int mlx5_eswitch_num_vfs(struct mlx5_eswitch *esw)\n{\n\tif (mlx5_esw_allowed(esw))\n\t\treturn esw->esw_funcs.num_vfs;\n\n\treturn 0;\n}\n\nstatic inline int mlx5_eswitch_get_npeers(struct mlx5_eswitch *esw)\n{\n\tif (mlx5_esw_allowed(esw))\n\t\treturn esw->num_peers;\n\treturn 0;\n}\n\nstatic inline struct mlx5_flow_table *\nmlx5_eswitch_get_slow_fdb(struct mlx5_eswitch *esw)\n{\n\treturn esw->fdb_table.offloads.slow_fdb;\n}\n\nint mlx5_eswitch_restore_ipsec_rule(struct mlx5_eswitch *esw, struct mlx5_flow_handle *rule,\n\t\t\t\t    struct mlx5_esw_flow_attr *esw_attr, int attr_idx);\nbool mlx5_eswitch_block_ipsec(struct mlx5_core_dev *dev);\nvoid mlx5_eswitch_unblock_ipsec(struct mlx5_core_dev *dev);\nbool mlx5_esw_ipsec_vf_offload_supported(struct mlx5_core_dev *dev);\nint mlx5_esw_ipsec_vf_offload_get(struct mlx5_core_dev *dev,\n\t\t\t\t  struct mlx5_vport *vport);\nint mlx5_esw_ipsec_vf_crypto_offload_supported(struct mlx5_core_dev *dev,\n\t\t\t\t\t       u16 vport_num);\nint mlx5_esw_ipsec_vf_crypto_offload_set(struct mlx5_eswitch *esw, struct mlx5_vport *vport,\n\t\t\t\t\t bool enable);\nint mlx5_esw_ipsec_vf_packet_offload_set(struct mlx5_eswitch *esw, struct mlx5_vport *vport,\n\t\t\t\t\t bool enable);\nint mlx5_esw_ipsec_vf_packet_offload_supported(struct mlx5_core_dev *dev,\n\t\t\t\t\t       u16 vport_num);\nvoid mlx5_esw_vport_ipsec_offload_enable(struct mlx5_eswitch *esw);\nvoid mlx5_esw_vport_ipsec_offload_disable(struct mlx5_eswitch *esw);\n\n#else   \n \nstatic inline int  mlx5_eswitch_init(struct mlx5_core_dev *dev) { return 0; }\nstatic inline void mlx5_eswitch_cleanup(struct mlx5_eswitch *esw) {}\nstatic inline int mlx5_eswitch_enable(struct mlx5_eswitch *esw, int num_vfs) { return 0; }\nstatic inline void mlx5_eswitch_disable_sriov(struct mlx5_eswitch *esw, bool clear_vf) {}\nstatic inline void mlx5_eswitch_disable(struct mlx5_eswitch *esw) {}\nstatic inline void mlx5_esw_offloads_devcom_init(struct mlx5_eswitch *esw, u64 key) {}\nstatic inline void mlx5_esw_offloads_devcom_cleanup(struct mlx5_eswitch *esw) {}\nstatic inline bool mlx5_esw_offloads_devcom_is_ready(struct mlx5_eswitch *esw) { return false; }\nstatic inline bool mlx5_eswitch_is_funcs_handler(struct mlx5_core_dev *dev) { return false; }\nstatic inline\nint mlx5_eswitch_set_vport_state(struct mlx5_eswitch *esw, u16 vport, int link_state) { return 0; }\nstatic inline const u32 *mlx5_esw_query_functions(struct mlx5_core_dev *dev)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\nstatic inline struct mlx5_flow_handle *\nesw_add_restore_rule(struct mlx5_eswitch *esw, u32 tag)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\nstatic inline unsigned int\nmlx5_esw_vport_to_devlink_port_index(const struct mlx5_core_dev *dev,\n\t\t\t\t     u16 vport_num)\n{\n\treturn vport_num;\n}\n\nstatic inline int\nmlx5_eswitch_offloads_single_fdb_add_one(struct mlx5_eswitch *master_esw,\n\t\t\t\t\t struct mlx5_eswitch *slave_esw, int max_slaves)\n{\n\treturn 0;\n}\n\nstatic inline void\nmlx5_eswitch_offloads_single_fdb_del_one(struct mlx5_eswitch *master_esw,\n\t\t\t\t\t struct mlx5_eswitch *slave_esw) {}\n\nstatic inline int mlx5_eswitch_get_npeers(struct mlx5_eswitch *esw) { return 0; }\n\nstatic inline int\nmlx5_eswitch_reload_reps(struct mlx5_eswitch *esw)\n{\n\treturn 0;\n}\n\nstatic inline bool mlx5_eswitch_block_encap(struct mlx5_core_dev *dev)\n{\n\treturn true;\n}\n\nstatic inline void mlx5_eswitch_unblock_encap(struct mlx5_core_dev *dev)\n{\n}\n\nstatic inline int mlx5_eswitch_block_mode(struct mlx5_core_dev *dev) { return 0; }\nstatic inline void mlx5_eswitch_unblock_mode(struct mlx5_core_dev *dev) {}\nstatic inline bool mlx5_eswitch_block_ipsec(struct mlx5_core_dev *dev)\n{\n\treturn false;\n}\n\nstatic inline void mlx5_eswitch_unblock_ipsec(struct mlx5_core_dev *dev) {}\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}