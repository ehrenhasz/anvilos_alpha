{
  "module_name": "lag.c",
  "hash_id": "ac29b44c097ef94df080572ae497a0211ea169d00a43dd2db4227b1a7c193d95",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx5/core/lag/lag.c",
  "human_readable_source": " \n\n#include <linux/netdevice.h>\n#include <net/bonding.h>\n#include <linux/mlx5/driver.h>\n#include <linux/mlx5/eswitch.h>\n#include <linux/mlx5/vport.h>\n#include \"lib/devcom.h\"\n#include \"mlx5_core.h\"\n#include \"eswitch.h\"\n#include \"esw/acl/ofld.h\"\n#include \"lag.h\"\n#include \"mp.h\"\n#include \"mpesw.h\"\n\nenum {\n\tMLX5_LAG_EGRESS_PORT_1 = 1,\n\tMLX5_LAG_EGRESS_PORT_2,\n};\n\n \nstatic DEFINE_SPINLOCK(lag_lock);\n\nstatic int get_port_sel_mode(enum mlx5_lag_mode mode, unsigned long flags)\n{\n\tif (test_bit(MLX5_LAG_MODE_FLAG_HASH_BASED, &flags))\n\t\treturn MLX5_LAG_PORT_SELECT_MODE_PORT_SELECT_FT;\n\n\tif (mode == MLX5_LAG_MODE_MPESW)\n\t\treturn MLX5_LAG_PORT_SELECT_MODE_PORT_SELECT_MPESW;\n\n\treturn MLX5_LAG_PORT_SELECT_MODE_QUEUE_AFFINITY;\n}\n\nstatic u8 lag_active_port_bits(struct mlx5_lag *ldev)\n{\n\tu8 enabled_ports[MLX5_MAX_PORTS] = {};\n\tu8 active_port = 0;\n\tint num_enabled;\n\tint idx;\n\n\tmlx5_infer_tx_enabled(&ldev->tracker, ldev->ports, enabled_ports,\n\t\t\t      &num_enabled);\n\tfor (idx = 0; idx < num_enabled; idx++)\n\t\tactive_port |= BIT_MASK(enabled_ports[idx]);\n\n\treturn active_port;\n}\n\nstatic int mlx5_cmd_create_lag(struct mlx5_core_dev *dev, u8 *ports, int mode,\n\t\t\t       unsigned long flags)\n{\n\tbool fdb_sel_mode = test_bit(MLX5_LAG_MODE_FLAG_FDB_SEL_MODE_NATIVE,\n\t\t\t\t     &flags);\n\tint port_sel_mode = get_port_sel_mode(mode, flags);\n\tu32 in[MLX5_ST_SZ_DW(create_lag_in)] = {};\n\tvoid *lag_ctx;\n\n\tlag_ctx = MLX5_ADDR_OF(create_lag_in, in, ctx);\n\tMLX5_SET(create_lag_in, in, opcode, MLX5_CMD_OP_CREATE_LAG);\n\tMLX5_SET(lagc, lag_ctx, fdb_selection_mode, fdb_sel_mode);\n\n\tswitch (port_sel_mode) {\n\tcase MLX5_LAG_PORT_SELECT_MODE_QUEUE_AFFINITY:\n\t\tMLX5_SET(lagc, lag_ctx, tx_remap_affinity_1, ports[0]);\n\t\tMLX5_SET(lagc, lag_ctx, tx_remap_affinity_2, ports[1]);\n\t\tbreak;\n\tcase MLX5_LAG_PORT_SELECT_MODE_PORT_SELECT_FT:\n\t\tif (!MLX5_CAP_PORT_SELECTION(dev, port_select_flow_table_bypass))\n\t\t\tbreak;\n\n\t\tMLX5_SET(lagc, lag_ctx, active_port,\n\t\t\t lag_active_port_bits(mlx5_lag_dev(dev)));\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tMLX5_SET(lagc, lag_ctx, port_select_mode, port_sel_mode);\n\n\treturn mlx5_cmd_exec_in(dev, create_lag, in);\n}\n\nstatic int mlx5_cmd_modify_lag(struct mlx5_core_dev *dev, u8 num_ports,\n\t\t\t       u8 *ports)\n{\n\tu32 in[MLX5_ST_SZ_DW(modify_lag_in)] = {};\n\tvoid *lag_ctx = MLX5_ADDR_OF(modify_lag_in, in, ctx);\n\n\tMLX5_SET(modify_lag_in, in, opcode, MLX5_CMD_OP_MODIFY_LAG);\n\tMLX5_SET(modify_lag_in, in, field_select, 0x1);\n\n\tMLX5_SET(lagc, lag_ctx, tx_remap_affinity_1, ports[0]);\n\tMLX5_SET(lagc, lag_ctx, tx_remap_affinity_2, ports[1]);\n\n\treturn mlx5_cmd_exec_in(dev, modify_lag, in);\n}\n\nint mlx5_cmd_create_vport_lag(struct mlx5_core_dev *dev)\n{\n\tu32 in[MLX5_ST_SZ_DW(create_vport_lag_in)] = {};\n\n\tMLX5_SET(create_vport_lag_in, in, opcode, MLX5_CMD_OP_CREATE_VPORT_LAG);\n\n\treturn mlx5_cmd_exec_in(dev, create_vport_lag, in);\n}\nEXPORT_SYMBOL(mlx5_cmd_create_vport_lag);\n\nint mlx5_cmd_destroy_vport_lag(struct mlx5_core_dev *dev)\n{\n\tu32 in[MLX5_ST_SZ_DW(destroy_vport_lag_in)] = {};\n\n\tMLX5_SET(destroy_vport_lag_in, in, opcode, MLX5_CMD_OP_DESTROY_VPORT_LAG);\n\n\treturn mlx5_cmd_exec_in(dev, destroy_vport_lag, in);\n}\nEXPORT_SYMBOL(mlx5_cmd_destroy_vport_lag);\n\nstatic void mlx5_infer_tx_disabled(struct lag_tracker *tracker, u8 num_ports,\n\t\t\t\t   u8 *ports, int *num_disabled)\n{\n\tint i;\n\n\t*num_disabled = 0;\n\tfor (i = 0; i < num_ports; i++) {\n\t\tif (!tracker->netdev_state[i].tx_enabled ||\n\t\t    !tracker->netdev_state[i].link_up)\n\t\t\tports[(*num_disabled)++] = i;\n\t}\n}\n\nvoid mlx5_infer_tx_enabled(struct lag_tracker *tracker, u8 num_ports,\n\t\t\t   u8 *ports, int *num_enabled)\n{\n\tint i;\n\n\t*num_enabled = 0;\n\tfor (i = 0; i < num_ports; i++) {\n\t\tif (tracker->netdev_state[i].tx_enabled &&\n\t\t    tracker->netdev_state[i].link_up)\n\t\t\tports[(*num_enabled)++] = i;\n\t}\n\n\tif (*num_enabled == 0)\n\t\tmlx5_infer_tx_disabled(tracker, num_ports, ports, num_enabled);\n}\n\nstatic void mlx5_lag_print_mapping(struct mlx5_core_dev *dev,\n\t\t\t\t   struct mlx5_lag *ldev,\n\t\t\t\t   struct lag_tracker *tracker,\n\t\t\t\t   unsigned long flags)\n{\n\tchar buf[MLX5_MAX_PORTS * 10 + 1] = {};\n\tu8 enabled_ports[MLX5_MAX_PORTS] = {};\n\tint written = 0;\n\tint num_enabled;\n\tint idx;\n\tint err;\n\tint i;\n\tint j;\n\n\tif (test_bit(MLX5_LAG_MODE_FLAG_HASH_BASED, &flags)) {\n\t\tmlx5_infer_tx_enabled(tracker, ldev->ports, enabled_ports,\n\t\t\t\t      &num_enabled);\n\t\tfor (i = 0; i < num_enabled; i++) {\n\t\t\terr = scnprintf(buf + written, 4, \"%d, \", enabled_ports[i] + 1);\n\t\t\tif (err != 3)\n\t\t\t\treturn;\n\t\t\twritten += err;\n\t\t}\n\t\tbuf[written - 2] = 0;\n\t\tmlx5_core_info(dev, \"lag map active ports: %s\\n\", buf);\n\t} else {\n\t\tfor (i = 0; i < ldev->ports; i++) {\n\t\t\tfor (j  = 0; j < ldev->buckets; j++) {\n\t\t\t\tidx = i * ldev->buckets + j;\n\t\t\t\terr = scnprintf(buf + written, 10,\n\t\t\t\t\t\t\" port %d:%d\", i + 1, ldev->v2p_map[idx]);\n\t\t\t\tif (err != 9)\n\t\t\t\t\treturn;\n\t\t\t\twritten += err;\n\t\t\t}\n\t\t}\n\t\tmlx5_core_info(dev, \"lag map:%s\\n\", buf);\n\t}\n}\n\nstatic int mlx5_lag_netdev_event(struct notifier_block *this,\n\t\t\t\t unsigned long event, void *ptr);\nstatic void mlx5_do_bond_work(struct work_struct *work);\n\nstatic void mlx5_ldev_free(struct kref *ref)\n{\n\tstruct mlx5_lag *ldev = container_of(ref, struct mlx5_lag, ref);\n\n\tif (ldev->nb.notifier_call)\n\t\tunregister_netdevice_notifier_net(&init_net, &ldev->nb);\n\tmlx5_lag_mp_cleanup(ldev);\n\tcancel_delayed_work_sync(&ldev->bond_work);\n\tdestroy_workqueue(ldev->wq);\n\tmutex_destroy(&ldev->lock);\n\tkfree(ldev);\n}\n\nstatic void mlx5_ldev_put(struct mlx5_lag *ldev)\n{\n\tkref_put(&ldev->ref, mlx5_ldev_free);\n}\n\nstatic void mlx5_ldev_get(struct mlx5_lag *ldev)\n{\n\tkref_get(&ldev->ref);\n}\n\nstatic struct mlx5_lag *mlx5_lag_dev_alloc(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\tint err;\n\n\tldev = kzalloc(sizeof(*ldev), GFP_KERNEL);\n\tif (!ldev)\n\t\treturn NULL;\n\n\tldev->wq = create_singlethread_workqueue(\"mlx5_lag\");\n\tif (!ldev->wq) {\n\t\tkfree(ldev);\n\t\treturn NULL;\n\t}\n\n\tkref_init(&ldev->ref);\n\tmutex_init(&ldev->lock);\n\tINIT_DELAYED_WORK(&ldev->bond_work, mlx5_do_bond_work);\n\n\tldev->nb.notifier_call = mlx5_lag_netdev_event;\n\tif (register_netdevice_notifier_net(&init_net, &ldev->nb)) {\n\t\tldev->nb.notifier_call = NULL;\n\t\tmlx5_core_err(dev, \"Failed to register LAG netdev notifier\\n\");\n\t}\n\tldev->mode = MLX5_LAG_MODE_NONE;\n\n\terr = mlx5_lag_mp_init(ldev);\n\tif (err)\n\t\tmlx5_core_err(dev, \"Failed to init multipath lag err=%d\\n\",\n\t\t\t      err);\n\n\tldev->ports = MLX5_CAP_GEN(dev, num_lag_ports);\n\tldev->buckets = 1;\n\n\treturn ldev;\n}\n\nint mlx5_lag_dev_get_netdev_idx(struct mlx5_lag *ldev,\n\t\t\t\tstruct net_device *ndev)\n{\n\tint i;\n\n\tfor (i = 0; i < ldev->ports; i++)\n\t\tif (ldev->pf[i].netdev == ndev)\n\t\t\treturn i;\n\n\treturn -ENOENT;\n}\n\nstatic bool __mlx5_lag_is_roce(struct mlx5_lag *ldev)\n{\n\treturn ldev->mode == MLX5_LAG_MODE_ROCE;\n}\n\nstatic bool __mlx5_lag_is_sriov(struct mlx5_lag *ldev)\n{\n\treturn ldev->mode == MLX5_LAG_MODE_SRIOV;\n}\n\n \nstatic void mlx5_infer_tx_affinity_mapping(struct lag_tracker *tracker,\n\t\t\t\t\t   u8 num_ports,\n\t\t\t\t\t   u8 buckets,\n\t\t\t\t\t   u8 *ports)\n{\n\tint disabled[MLX5_MAX_PORTS] = {};\n\tint enabled[MLX5_MAX_PORTS] = {};\n\tint disabled_ports_num = 0;\n\tint enabled_ports_num = 0;\n\tint idx;\n\tu32 rand;\n\tint i;\n\tint j;\n\n\tfor (i = 0; i < num_ports; i++) {\n\t\tif (tracker->netdev_state[i].tx_enabled &&\n\t\t    tracker->netdev_state[i].link_up)\n\t\t\tenabled[enabled_ports_num++] = i;\n\t\telse\n\t\t\tdisabled[disabled_ports_num++] = i;\n\t}\n\n\t \n\tfor (i = 0; i < num_ports; i++)\n\t\tfor (j = 0; j < buckets; j++) {\n\t\t\tidx = i * buckets + j;\n\t\t\tports[idx] = MLX5_LAG_EGRESS_PORT_1 + i;\n\t\t}\n\n\t \n\tif (enabled_ports_num == num_ports ||\n\t    disabled_ports_num == num_ports)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < disabled_ports_num; i++) {\n\t\tfor (j = 0; j < buckets; j++) {\n\t\t\tget_random_bytes(&rand, 4);\n\t\t\tports[disabled[i] * buckets + j] = enabled[rand % enabled_ports_num] + 1;\n\t\t}\n\t}\n}\n\nstatic bool mlx5_lag_has_drop_rule(struct mlx5_lag *ldev)\n{\n\tint i;\n\n\tfor (i = 0; i < ldev->ports; i++)\n\t\tif (ldev->pf[i].has_drop)\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic void mlx5_lag_drop_rule_cleanup(struct mlx5_lag *ldev)\n{\n\tint i;\n\n\tfor (i = 0; i < ldev->ports; i++) {\n\t\tif (!ldev->pf[i].has_drop)\n\t\t\tcontinue;\n\n\t\tmlx5_esw_acl_ingress_vport_drop_rule_destroy(ldev->pf[i].dev->priv.eswitch,\n\t\t\t\t\t\t\t     MLX5_VPORT_UPLINK);\n\t\tldev->pf[i].has_drop = false;\n\t}\n}\n\nstatic void mlx5_lag_drop_rule_setup(struct mlx5_lag *ldev,\n\t\t\t\t     struct lag_tracker *tracker)\n{\n\tu8 disabled_ports[MLX5_MAX_PORTS] = {};\n\tstruct mlx5_core_dev *dev;\n\tint disabled_index;\n\tint num_disabled;\n\tint err;\n\tint i;\n\n\t \n\tmlx5_lag_drop_rule_cleanup(ldev);\n\n\tif (!ldev->tracker.has_inactive)\n\t\treturn;\n\n\tmlx5_infer_tx_disabled(tracker, ldev->ports, disabled_ports, &num_disabled);\n\n\tfor (i = 0; i < num_disabled; i++) {\n\t\tdisabled_index = disabled_ports[i];\n\t\tdev = ldev->pf[disabled_index].dev;\n\t\terr = mlx5_esw_acl_ingress_vport_drop_rule_create(dev->priv.eswitch,\n\t\t\t\t\t\t\t\t  MLX5_VPORT_UPLINK);\n\t\tif (!err)\n\t\t\tldev->pf[disabled_index].has_drop = true;\n\t\telse\n\t\t\tmlx5_core_err(dev,\n\t\t\t\t      \"Failed to create lag drop rule, error: %d\", err);\n\t}\n}\n\nstatic int mlx5_cmd_modify_active_port(struct mlx5_core_dev *dev, u8 ports)\n{\n\tu32 in[MLX5_ST_SZ_DW(modify_lag_in)] = {};\n\tvoid *lag_ctx;\n\n\tlag_ctx = MLX5_ADDR_OF(modify_lag_in, in, ctx);\n\n\tMLX5_SET(modify_lag_in, in, opcode, MLX5_CMD_OP_MODIFY_LAG);\n\tMLX5_SET(modify_lag_in, in, field_select, 0x2);\n\n\tMLX5_SET(lagc, lag_ctx, active_port, ports);\n\n\treturn mlx5_cmd_exec_in(dev, modify_lag, in);\n}\n\nstatic int _mlx5_modify_lag(struct mlx5_lag *ldev, u8 *ports)\n{\n\tstruct mlx5_core_dev *dev0 = ldev->pf[MLX5_LAG_P1].dev;\n\tu8 active_ports;\n\tint ret;\n\n\tif (test_bit(MLX5_LAG_MODE_FLAG_HASH_BASED, &ldev->mode_flags)) {\n\t\tret = mlx5_lag_port_sel_modify(ldev, ports);\n\t\tif (ret ||\n\t\t    !MLX5_CAP_PORT_SELECTION(dev0, port_select_flow_table_bypass))\n\t\t\treturn ret;\n\n\t\tactive_ports = lag_active_port_bits(ldev);\n\n\t\treturn mlx5_cmd_modify_active_port(dev0, active_ports);\n\t}\n\treturn mlx5_cmd_modify_lag(dev0, ldev->ports, ports);\n}\n\nvoid mlx5_modify_lag(struct mlx5_lag *ldev,\n\t\t     struct lag_tracker *tracker)\n{\n\tu8 ports[MLX5_MAX_PORTS * MLX5_LAG_MAX_HASH_BUCKETS] = {};\n\tstruct mlx5_core_dev *dev0 = ldev->pf[MLX5_LAG_P1].dev;\n\tint idx;\n\tint err;\n\tint i;\n\tint j;\n\n\tmlx5_infer_tx_affinity_mapping(tracker, ldev->ports, ldev->buckets, ports);\n\n\tfor (i = 0; i < ldev->ports; i++) {\n\t\tfor (j = 0; j < ldev->buckets; j++) {\n\t\t\tidx = i * ldev->buckets + j;\n\t\t\tif (ports[idx] == ldev->v2p_map[idx])\n\t\t\t\tcontinue;\n\t\t\terr = _mlx5_modify_lag(ldev, ports);\n\t\t\tif (err) {\n\t\t\t\tmlx5_core_err(dev0,\n\t\t\t\t\t      \"Failed to modify LAG (%d)\\n\",\n\t\t\t\t\t      err);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tmemcpy(ldev->v2p_map, ports, sizeof(ports));\n\n\t\t\tmlx5_lag_print_mapping(dev0, ldev, tracker,\n\t\t\t\t\t       ldev->mode_flags);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (tracker->tx_type == NETDEV_LAG_TX_TYPE_ACTIVEBACKUP &&\n\t    !(ldev->mode == MLX5_LAG_MODE_ROCE))\n\t\tmlx5_lag_drop_rule_setup(ldev, tracker);\n}\n\nstatic int mlx5_lag_set_port_sel_mode_roce(struct mlx5_lag *ldev,\n\t\t\t\t\t   unsigned long *flags)\n{\n\tstruct mlx5_core_dev *dev0 = ldev->pf[MLX5_LAG_P1].dev;\n\n\tif (!MLX5_CAP_PORT_SELECTION(dev0, port_select_flow_table)) {\n\t\tif (ldev->ports > 2)\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\t}\n\n\tif (ldev->ports > 2)\n\t\tldev->buckets = MLX5_LAG_MAX_HASH_BUCKETS;\n\n\tset_bit(MLX5_LAG_MODE_FLAG_HASH_BASED, flags);\n\n\treturn 0;\n}\n\nstatic void mlx5_lag_set_port_sel_mode_offloads(struct mlx5_lag *ldev,\n\t\t\t\t\t\tstruct lag_tracker *tracker,\n\t\t\t\t\t\tenum mlx5_lag_mode mode,\n\t\t\t\t\t\tunsigned long *flags)\n{\n\tstruct lag_func *dev0 = &ldev->pf[MLX5_LAG_P1];\n\n\tif (mode == MLX5_LAG_MODE_MPESW)\n\t\treturn;\n\n\tif (MLX5_CAP_PORT_SELECTION(dev0->dev, port_select_flow_table) &&\n\t    tracker->tx_type == NETDEV_LAG_TX_TYPE_HASH) {\n\t\tif (ldev->ports > 2)\n\t\t\tldev->buckets = MLX5_LAG_MAX_HASH_BUCKETS;\n\t\tset_bit(MLX5_LAG_MODE_FLAG_HASH_BASED, flags);\n\t}\n}\n\nstatic int mlx5_lag_set_flags(struct mlx5_lag *ldev, enum mlx5_lag_mode mode,\n\t\t\t      struct lag_tracker *tracker, bool shared_fdb,\n\t\t\t      unsigned long *flags)\n{\n\tbool roce_lag = mode == MLX5_LAG_MODE_ROCE;\n\n\t*flags = 0;\n\tif (shared_fdb) {\n\t\tset_bit(MLX5_LAG_MODE_FLAG_SHARED_FDB, flags);\n\t\tset_bit(MLX5_LAG_MODE_FLAG_FDB_SEL_MODE_NATIVE, flags);\n\t}\n\n\tif (mode == MLX5_LAG_MODE_MPESW)\n\t\tset_bit(MLX5_LAG_MODE_FLAG_FDB_SEL_MODE_NATIVE, flags);\n\n\tif (roce_lag)\n\t\treturn mlx5_lag_set_port_sel_mode_roce(ldev, flags);\n\n\tmlx5_lag_set_port_sel_mode_offloads(ldev, tracker, mode, flags);\n\treturn 0;\n}\n\nchar *mlx5_get_str_port_sel_mode(enum mlx5_lag_mode mode, unsigned long flags)\n{\n\tint port_sel_mode = get_port_sel_mode(mode, flags);\n\n\tswitch (port_sel_mode) {\n\tcase MLX5_LAG_PORT_SELECT_MODE_QUEUE_AFFINITY: return \"queue_affinity\";\n\tcase MLX5_LAG_PORT_SELECT_MODE_PORT_SELECT_FT: return \"hash\";\n\tcase MLX5_LAG_PORT_SELECT_MODE_PORT_SELECT_MPESW: return \"mpesw\";\n\tdefault: return \"invalid\";\n\t}\n}\n\nstatic int mlx5_lag_create_single_fdb(struct mlx5_lag *ldev)\n{\n\tstruct mlx5_core_dev *dev0 = ldev->pf[MLX5_LAG_P1].dev;\n\tstruct mlx5_eswitch *master_esw = dev0->priv.eswitch;\n\tint err;\n\tint i;\n\n\tfor (i = MLX5_LAG_P1 + 1; i < ldev->ports; i++) {\n\t\tstruct mlx5_eswitch *slave_esw = ldev->pf[i].dev->priv.eswitch;\n\n\t\terr = mlx5_eswitch_offloads_single_fdb_add_one(master_esw,\n\t\t\t\t\t\t\t       slave_esw, ldev->ports);\n\t\tif (err)\n\t\t\tgoto err;\n\t}\n\treturn 0;\nerr:\n\tfor (; i > MLX5_LAG_P1; i--)\n\t\tmlx5_eswitch_offloads_single_fdb_del_one(master_esw,\n\t\t\t\t\t\t\t ldev->pf[i].dev->priv.eswitch);\n\treturn err;\n}\n\nstatic int mlx5_create_lag(struct mlx5_lag *ldev,\n\t\t\t   struct lag_tracker *tracker,\n\t\t\t   enum mlx5_lag_mode mode,\n\t\t\t   unsigned long flags)\n{\n\tbool shared_fdb = test_bit(MLX5_LAG_MODE_FLAG_SHARED_FDB, &flags);\n\tstruct mlx5_core_dev *dev0 = ldev->pf[MLX5_LAG_P1].dev;\n\tu32 in[MLX5_ST_SZ_DW(destroy_lag_in)] = {};\n\tint err;\n\n\tif (tracker)\n\t\tmlx5_lag_print_mapping(dev0, ldev, tracker, flags);\n\tmlx5_core_info(dev0, \"shared_fdb:%d mode:%s\\n\",\n\t\t       shared_fdb, mlx5_get_str_port_sel_mode(mode, flags));\n\n\terr = mlx5_cmd_create_lag(dev0, ldev->v2p_map, mode, flags);\n\tif (err) {\n\t\tmlx5_core_err(dev0,\n\t\t\t      \"Failed to create LAG (%d)\\n\",\n\t\t\t      err);\n\t\treturn err;\n\t}\n\n\tif (shared_fdb) {\n\t\terr = mlx5_lag_create_single_fdb(ldev);\n\t\tif (err)\n\t\t\tmlx5_core_err(dev0, \"Can't enable single FDB mode\\n\");\n\t\telse\n\t\t\tmlx5_core_info(dev0, \"Operation mode is single FDB\\n\");\n\t}\n\n\tif (err) {\n\t\tMLX5_SET(destroy_lag_in, in, opcode, MLX5_CMD_OP_DESTROY_LAG);\n\t\tif (mlx5_cmd_exec_in(dev0, destroy_lag, in))\n\t\t\tmlx5_core_err(dev0,\n\t\t\t\t      \"Failed to deactivate RoCE LAG; driver restart required\\n\");\n\t}\n\n\treturn err;\n}\n\nint mlx5_activate_lag(struct mlx5_lag *ldev,\n\t\t      struct lag_tracker *tracker,\n\t\t      enum mlx5_lag_mode mode,\n\t\t      bool shared_fdb)\n{\n\tbool roce_lag = mode == MLX5_LAG_MODE_ROCE;\n\tstruct mlx5_core_dev *dev0 = ldev->pf[MLX5_LAG_P1].dev;\n\tunsigned long flags = 0;\n\tint err;\n\n\terr = mlx5_lag_set_flags(ldev, mode, tracker, shared_fdb, &flags);\n\tif (err)\n\t\treturn err;\n\n\tif (mode != MLX5_LAG_MODE_MPESW) {\n\t\tmlx5_infer_tx_affinity_mapping(tracker, ldev->ports, ldev->buckets, ldev->v2p_map);\n\t\tif (test_bit(MLX5_LAG_MODE_FLAG_HASH_BASED, &flags)) {\n\t\t\terr = mlx5_lag_port_sel_create(ldev, tracker->hash_type,\n\t\t\t\t\t\t       ldev->v2p_map);\n\t\t\tif (err) {\n\t\t\t\tmlx5_core_err(dev0,\n\t\t\t\t\t      \"Failed to create LAG port selection(%d)\\n\",\n\t\t\t\t\t      err);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\terr = mlx5_create_lag(ldev, tracker, mode, flags);\n\tif (err) {\n\t\tif (test_bit(MLX5_LAG_MODE_FLAG_HASH_BASED, &flags))\n\t\t\tmlx5_lag_port_sel_destroy(ldev);\n\t\tif (roce_lag)\n\t\t\tmlx5_core_err(dev0,\n\t\t\t\t      \"Failed to activate RoCE LAG\\n\");\n\t\telse\n\t\t\tmlx5_core_err(dev0,\n\t\t\t\t      \"Failed to activate VF LAG\\n\"\n\t\t\t\t      \"Make sure all VFs are unbound prior to VF LAG activation or deactivation\\n\");\n\t\treturn err;\n\t}\n\n\tif (tracker && tracker->tx_type == NETDEV_LAG_TX_TYPE_ACTIVEBACKUP &&\n\t    !roce_lag)\n\t\tmlx5_lag_drop_rule_setup(ldev, tracker);\n\n\tldev->mode = mode;\n\tldev->mode_flags = flags;\n\treturn 0;\n}\n\nint mlx5_deactivate_lag(struct mlx5_lag *ldev)\n{\n\tstruct mlx5_core_dev *dev0 = ldev->pf[MLX5_LAG_P1].dev;\n\tstruct mlx5_eswitch *master_esw = dev0->priv.eswitch;\n\tu32 in[MLX5_ST_SZ_DW(destroy_lag_in)] = {};\n\tbool roce_lag = __mlx5_lag_is_roce(ldev);\n\tunsigned long flags = ldev->mode_flags;\n\tint err;\n\tint i;\n\n\tldev->mode = MLX5_LAG_MODE_NONE;\n\tldev->mode_flags = 0;\n\tmlx5_lag_mp_reset(ldev);\n\n\tif (test_bit(MLX5_LAG_MODE_FLAG_SHARED_FDB, &flags)) {\n\t\tfor (i = MLX5_LAG_P1 + 1; i < ldev->ports; i++)\n\t\t\tmlx5_eswitch_offloads_single_fdb_del_one(master_esw,\n\t\t\t\t\t\t\t\t ldev->pf[i].dev->priv.eswitch);\n\t\tclear_bit(MLX5_LAG_MODE_FLAG_SHARED_FDB, &flags);\n\t}\n\n\tMLX5_SET(destroy_lag_in, in, opcode, MLX5_CMD_OP_DESTROY_LAG);\n\terr = mlx5_cmd_exec_in(dev0, destroy_lag, in);\n\tif (err) {\n\t\tif (roce_lag) {\n\t\t\tmlx5_core_err(dev0,\n\t\t\t\t      \"Failed to deactivate RoCE LAG; driver restart required\\n\");\n\t\t} else {\n\t\t\tmlx5_core_err(dev0,\n\t\t\t\t      \"Failed to deactivate VF LAG; driver restart required\\n\"\n\t\t\t\t      \"Make sure all VFs are unbound prior to VF LAG activation or deactivation\\n\");\n\t\t}\n\t\treturn err;\n\t}\n\n\tif (test_bit(MLX5_LAG_MODE_FLAG_HASH_BASED, &flags))\n\t\tmlx5_lag_port_sel_destroy(ldev);\n\tif (mlx5_lag_has_drop_rule(ldev))\n\t\tmlx5_lag_drop_rule_cleanup(ldev);\n\n\treturn 0;\n}\n\n#define MLX5_LAG_OFFLOADS_SUPPORTED_PORTS 4\nbool mlx5_lag_check_prereq(struct mlx5_lag *ldev)\n{\n#ifdef CONFIG_MLX5_ESWITCH\n\tstruct mlx5_core_dev *dev;\n\tu8 mode;\n#endif\n\tint i;\n\n\tfor (i = 0; i < ldev->ports; i++)\n\t\tif (!ldev->pf[i].dev)\n\t\t\treturn false;\n\n#ifdef CONFIG_MLX5_ESWITCH\n\tfor (i = 0; i < ldev->ports; i++) {\n\t\tdev = ldev->pf[i].dev;\n\t\tif (mlx5_eswitch_num_vfs(dev->priv.eswitch) && !is_mdev_switchdev_mode(dev))\n\t\t\treturn false;\n\t}\n\n\tdev = ldev->pf[MLX5_LAG_P1].dev;\n\tmode = mlx5_eswitch_mode(dev);\n\tfor (i = 0; i < ldev->ports; i++)\n\t\tif (mlx5_eswitch_mode(ldev->pf[i].dev) != mode)\n\t\t\treturn false;\n\n\tif (mode == MLX5_ESWITCH_OFFLOADS && ldev->ports > MLX5_LAG_OFFLOADS_SUPPORTED_PORTS)\n\t\treturn false;\n#else\n\tfor (i = 0; i < ldev->ports; i++)\n\t\tif (mlx5_sriov_is_enabled(ldev->pf[i].dev))\n\t\t\treturn false;\n#endif\n\treturn true;\n}\n\nvoid mlx5_lag_add_devices(struct mlx5_lag *ldev)\n{\n\tint i;\n\n\tfor (i = 0; i < ldev->ports; i++) {\n\t\tif (!ldev->pf[i].dev)\n\t\t\tcontinue;\n\n\t\tif (ldev->pf[i].dev->priv.flags &\n\t\t    MLX5_PRIV_FLAGS_DISABLE_ALL_ADEV)\n\t\t\tcontinue;\n\n\t\tldev->pf[i].dev->priv.flags &= ~MLX5_PRIV_FLAGS_DISABLE_IB_ADEV;\n\t\tmlx5_rescan_drivers_locked(ldev->pf[i].dev);\n\t}\n}\n\nvoid mlx5_lag_remove_devices(struct mlx5_lag *ldev)\n{\n\tint i;\n\n\tfor (i = 0; i < ldev->ports; i++) {\n\t\tif (!ldev->pf[i].dev)\n\t\t\tcontinue;\n\n\t\tif (ldev->pf[i].dev->priv.flags &\n\t\t    MLX5_PRIV_FLAGS_DISABLE_ALL_ADEV)\n\t\t\tcontinue;\n\n\t\tldev->pf[i].dev->priv.flags |= MLX5_PRIV_FLAGS_DISABLE_IB_ADEV;\n\t\tmlx5_rescan_drivers_locked(ldev->pf[i].dev);\n\t}\n}\n\nvoid mlx5_disable_lag(struct mlx5_lag *ldev)\n{\n\tbool shared_fdb = test_bit(MLX5_LAG_MODE_FLAG_SHARED_FDB, &ldev->mode_flags);\n\tstruct mlx5_core_dev *dev0 = ldev->pf[MLX5_LAG_P1].dev;\n\tbool roce_lag;\n\tint err;\n\tint i;\n\n\troce_lag = __mlx5_lag_is_roce(ldev);\n\n\tif (shared_fdb) {\n\t\tmlx5_lag_remove_devices(ldev);\n\t} else if (roce_lag) {\n\t\tif (!(dev0->priv.flags & MLX5_PRIV_FLAGS_DISABLE_ALL_ADEV)) {\n\t\t\tdev0->priv.flags |= MLX5_PRIV_FLAGS_DISABLE_IB_ADEV;\n\t\t\tmlx5_rescan_drivers_locked(dev0);\n\t\t}\n\t\tfor (i = 1; i < ldev->ports; i++)\n\t\t\tmlx5_nic_vport_disable_roce(ldev->pf[i].dev);\n\t}\n\n\terr = mlx5_deactivate_lag(ldev);\n\tif (err)\n\t\treturn;\n\n\tif (shared_fdb || roce_lag)\n\t\tmlx5_lag_add_devices(ldev);\n\n\tif (shared_fdb)\n\t\tfor (i = 0; i < ldev->ports; i++)\n\t\t\tif (!(ldev->pf[i].dev->priv.flags & MLX5_PRIV_FLAGS_DISABLE_ALL_ADEV))\n\t\t\t\tmlx5_eswitch_reload_reps(ldev->pf[i].dev->priv.eswitch);\n}\n\nstatic bool mlx5_shared_fdb_supported(struct mlx5_lag *ldev)\n{\n\tstruct mlx5_core_dev *dev;\n\tint i;\n\n\tfor (i = MLX5_LAG_P1 + 1; i < ldev->ports; i++) {\n\t\tdev = ldev->pf[i].dev;\n\t\tif (is_mdev_switchdev_mode(dev) &&\n\t\t    mlx5_eswitch_vport_match_metadata_enabled(dev->priv.eswitch) &&\n\t\t    MLX5_CAP_GEN(dev, lag_native_fdb_selection) &&\n\t\t    MLX5_CAP_ESW(dev, root_ft_on_other_esw) &&\n\t\t    mlx5_eswitch_get_npeers(dev->priv.eswitch) ==\n\t\t    MLX5_CAP_GEN(dev, num_lag_ports) - 1)\n\t\t\tcontinue;\n\t\treturn false;\n\t}\n\n\tdev = ldev->pf[MLX5_LAG_P1].dev;\n\tif (is_mdev_switchdev_mode(dev) &&\n\t    mlx5_eswitch_vport_match_metadata_enabled(dev->priv.eswitch) &&\n\t    mlx5_esw_offloads_devcom_is_ready(dev->priv.eswitch) &&\n\t    MLX5_CAP_ESW(dev, esw_shared_ingress_acl) &&\n\t    mlx5_eswitch_get_npeers(dev->priv.eswitch) == MLX5_CAP_GEN(dev, num_lag_ports) - 1)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool mlx5_lag_is_roce_lag(struct mlx5_lag *ldev)\n{\n\tbool roce_lag = true;\n\tint i;\n\n\tfor (i = 0; i < ldev->ports; i++)\n\t\troce_lag = roce_lag && !mlx5_sriov_is_enabled(ldev->pf[i].dev);\n\n#ifdef CONFIG_MLX5_ESWITCH\n\tfor (i = 0; i < ldev->ports; i++)\n\t\troce_lag = roce_lag && is_mdev_legacy_mode(ldev->pf[i].dev);\n#endif\n\n\treturn roce_lag;\n}\n\nstatic bool mlx5_lag_should_modify_lag(struct mlx5_lag *ldev, bool do_bond)\n{\n\treturn do_bond && __mlx5_lag_is_active(ldev) &&\n\t       ldev->mode != MLX5_LAG_MODE_MPESW;\n}\n\nstatic bool mlx5_lag_should_disable_lag(struct mlx5_lag *ldev, bool do_bond)\n{\n\treturn !do_bond && __mlx5_lag_is_active(ldev) &&\n\t       ldev->mode != MLX5_LAG_MODE_MPESW;\n}\n\nstatic void mlx5_do_bond(struct mlx5_lag *ldev)\n{\n\tstruct mlx5_core_dev *dev0 = ldev->pf[MLX5_LAG_P1].dev;\n\tstruct lag_tracker tracker = { };\n\tbool do_bond, roce_lag;\n\tint err;\n\tint i;\n\n\tif (!mlx5_lag_is_ready(ldev)) {\n\t\tdo_bond = false;\n\t} else {\n\t\t \n\t\tif (mlx5_lag_is_multipath(dev0))\n\t\t\treturn;\n\n\t\ttracker = ldev->tracker;\n\n\t\tdo_bond = tracker.is_bonded && mlx5_lag_check_prereq(ldev);\n\t}\n\n\tif (do_bond && !__mlx5_lag_is_active(ldev)) {\n\t\tbool shared_fdb = mlx5_shared_fdb_supported(ldev);\n\n\t\troce_lag = mlx5_lag_is_roce_lag(ldev);\n\n\t\tif (shared_fdb || roce_lag)\n\t\t\tmlx5_lag_remove_devices(ldev);\n\n\t\terr = mlx5_activate_lag(ldev, &tracker,\n\t\t\t\t\troce_lag ? MLX5_LAG_MODE_ROCE :\n\t\t\t\t\t\t   MLX5_LAG_MODE_SRIOV,\n\t\t\t\t\tshared_fdb);\n\t\tif (err) {\n\t\t\tif (shared_fdb || roce_lag)\n\t\t\t\tmlx5_lag_add_devices(ldev);\n\n\t\t\treturn;\n\t\t} else if (roce_lag) {\n\t\t\tdev0->priv.flags &= ~MLX5_PRIV_FLAGS_DISABLE_IB_ADEV;\n\t\t\tmlx5_rescan_drivers_locked(dev0);\n\t\t\tfor (i = 1; i < ldev->ports; i++)\n\t\t\t\tmlx5_nic_vport_enable_roce(ldev->pf[i].dev);\n\t\t} else if (shared_fdb) {\n\t\t\tint i;\n\n\t\t\tdev0->priv.flags &= ~MLX5_PRIV_FLAGS_DISABLE_IB_ADEV;\n\t\t\tmlx5_rescan_drivers_locked(dev0);\n\n\t\t\tfor (i = 0; i < ldev->ports; i++) {\n\t\t\t\terr = mlx5_eswitch_reload_reps(ldev->pf[i].dev->priv.eswitch);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (err) {\n\t\t\t\tdev0->priv.flags |= MLX5_PRIV_FLAGS_DISABLE_IB_ADEV;\n\t\t\t\tmlx5_rescan_drivers_locked(dev0);\n\t\t\t\tmlx5_deactivate_lag(ldev);\n\t\t\t\tmlx5_lag_add_devices(ldev);\n\t\t\t\tfor (i = 0; i < ldev->ports; i++)\n\t\t\t\t\tmlx5_eswitch_reload_reps(ldev->pf[i].dev->priv.eswitch);\n\t\t\t\tmlx5_core_err(dev0, \"Failed to enable lag\\n\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t} else if (mlx5_lag_should_modify_lag(ldev, do_bond)) {\n\t\tmlx5_modify_lag(ldev, &tracker);\n\t} else if (mlx5_lag_should_disable_lag(ldev, do_bond)) {\n\t\tmlx5_disable_lag(ldev);\n\t}\n}\n\nstatic void mlx5_queue_bond_work(struct mlx5_lag *ldev, unsigned long delay)\n{\n\tqueue_delayed_work(ldev->wq, &ldev->bond_work, delay);\n}\n\nstatic void mlx5_do_bond_work(struct work_struct *work)\n{\n\tstruct delayed_work *delayed_work = to_delayed_work(work);\n\tstruct mlx5_lag *ldev = container_of(delayed_work, struct mlx5_lag,\n\t\t\t\t\t     bond_work);\n\tint status;\n\n\tstatus = mlx5_dev_list_trylock();\n\tif (!status) {\n\t\tmlx5_queue_bond_work(ldev, HZ);\n\t\treturn;\n\t}\n\n\tmutex_lock(&ldev->lock);\n\tif (ldev->mode_changes_in_progress) {\n\t\tmutex_unlock(&ldev->lock);\n\t\tmlx5_dev_list_unlock();\n\t\tmlx5_queue_bond_work(ldev, HZ);\n\t\treturn;\n\t}\n\n\tmlx5_do_bond(ldev);\n\tmutex_unlock(&ldev->lock);\n\tmlx5_dev_list_unlock();\n}\n\nstatic int mlx5_handle_changeupper_event(struct mlx5_lag *ldev,\n\t\t\t\t\t struct lag_tracker *tracker,\n\t\t\t\t\t struct netdev_notifier_changeupper_info *info)\n{\n\tstruct net_device *upper = info->upper_dev, *ndev_tmp;\n\tstruct netdev_lag_upper_info *lag_upper_info = NULL;\n\tbool is_bonded, is_in_lag, mode_supported;\n\tbool has_inactive = 0;\n\tstruct slave *slave;\n\tu8 bond_status = 0;\n\tint num_slaves = 0;\n\tint changed = 0;\n\tint idx;\n\n\tif (!netif_is_lag_master(upper))\n\t\treturn 0;\n\n\tif (info->linking)\n\t\tlag_upper_info = info->upper_info;\n\n\t \n\trcu_read_lock();\n\tfor_each_netdev_in_bond_rcu(upper, ndev_tmp) {\n\t\tidx = mlx5_lag_dev_get_netdev_idx(ldev, ndev_tmp);\n\t\tif (idx >= 0) {\n\t\t\tslave = bond_slave_get_rcu(ndev_tmp);\n\t\t\tif (slave)\n\t\t\t\thas_inactive |= bond_is_slave_inactive(slave);\n\t\t\tbond_status |= (1 << idx);\n\t\t}\n\n\t\tnum_slaves++;\n\t}\n\trcu_read_unlock();\n\n\t \n\tif (!(bond_status & GENMASK(ldev->ports - 1, 0)))\n\t\treturn 0;\n\n\tif (lag_upper_info) {\n\t\ttracker->tx_type = lag_upper_info->tx_type;\n\t\ttracker->hash_type = lag_upper_info->hash_type;\n\t}\n\n\ttracker->has_inactive = has_inactive;\n\t \n\tis_in_lag = num_slaves == ldev->ports &&\n\t\tbond_status == GENMASK(ldev->ports - 1, 0);\n\n\t \n\tmode_supported = tracker->tx_type == NETDEV_LAG_TX_TYPE_ACTIVEBACKUP ||\n\t\t\t tracker->tx_type == NETDEV_LAG_TX_TYPE_HASH;\n\n\tis_bonded = is_in_lag && mode_supported;\n\tif (tracker->is_bonded != is_bonded) {\n\t\ttracker->is_bonded = is_bonded;\n\t\tchanged = 1;\n\t}\n\n\tif (!is_in_lag)\n\t\treturn changed;\n\n\tif (!mlx5_lag_is_ready(ldev))\n\t\tNL_SET_ERR_MSG_MOD(info->info.extack,\n\t\t\t\t   \"Can't activate LAG offload, PF is configured with more than 64 VFs\");\n\telse if (!mode_supported)\n\t\tNL_SET_ERR_MSG_MOD(info->info.extack,\n\t\t\t\t   \"Can't activate LAG offload, TX type isn't supported\");\n\n\treturn changed;\n}\n\nstatic int mlx5_handle_changelowerstate_event(struct mlx5_lag *ldev,\n\t\t\t\t\t      struct lag_tracker *tracker,\n\t\t\t\t\t      struct net_device *ndev,\n\t\t\t\t\t      struct netdev_notifier_changelowerstate_info *info)\n{\n\tstruct netdev_lag_lower_state_info *lag_lower_info;\n\tint idx;\n\n\tif (!netif_is_lag_port(ndev))\n\t\treturn 0;\n\n\tidx = mlx5_lag_dev_get_netdev_idx(ldev, ndev);\n\tif (idx < 0)\n\t\treturn 0;\n\n\t \n\tlag_lower_info = info->lower_state_info;\n\tif (!lag_lower_info)\n\t\treturn 0;\n\n\ttracker->netdev_state[idx] = *lag_lower_info;\n\n\treturn 1;\n}\n\nstatic int mlx5_handle_changeinfodata_event(struct mlx5_lag *ldev,\n\t\t\t\t\t    struct lag_tracker *tracker,\n\t\t\t\t\t    struct net_device *ndev)\n{\n\tstruct net_device *ndev_tmp;\n\tstruct slave *slave;\n\tbool has_inactive = 0;\n\tint idx;\n\n\tif (!netif_is_lag_master(ndev))\n\t\treturn 0;\n\n\trcu_read_lock();\n\tfor_each_netdev_in_bond_rcu(ndev, ndev_tmp) {\n\t\tidx = mlx5_lag_dev_get_netdev_idx(ldev, ndev_tmp);\n\t\tif (idx < 0)\n\t\t\tcontinue;\n\n\t\tslave = bond_slave_get_rcu(ndev_tmp);\n\t\tif (slave)\n\t\t\thas_inactive |= bond_is_slave_inactive(slave);\n\t}\n\trcu_read_unlock();\n\n\tif (tracker->has_inactive == has_inactive)\n\t\treturn 0;\n\n\ttracker->has_inactive = has_inactive;\n\n\treturn 1;\n}\n\n \nstatic int mlx5_lag_netdev_event(struct notifier_block *this,\n\t\t\t\t unsigned long event, void *ptr)\n{\n\tstruct net_device *ndev = netdev_notifier_info_to_dev(ptr);\n\tstruct lag_tracker tracker;\n\tstruct mlx5_lag *ldev;\n\tint changed = 0;\n\n\tif (event != NETDEV_CHANGEUPPER &&\n\t    event != NETDEV_CHANGELOWERSTATE &&\n\t    event != NETDEV_CHANGEINFODATA)\n\t\treturn NOTIFY_DONE;\n\n\tldev    = container_of(this, struct mlx5_lag, nb);\n\n\ttracker = ldev->tracker;\n\n\tswitch (event) {\n\tcase NETDEV_CHANGEUPPER:\n\t\tchanged = mlx5_handle_changeupper_event(ldev, &tracker, ptr);\n\t\tbreak;\n\tcase NETDEV_CHANGELOWERSTATE:\n\t\tchanged = mlx5_handle_changelowerstate_event(ldev, &tracker,\n\t\t\t\t\t\t\t     ndev, ptr);\n\t\tbreak;\n\tcase NETDEV_CHANGEINFODATA:\n\t\tchanged = mlx5_handle_changeinfodata_event(ldev, &tracker, ndev);\n\t\tbreak;\n\t}\n\n\tldev->tracker = tracker;\n\n\tif (changed)\n\t\tmlx5_queue_bond_work(ldev, 0);\n\n\treturn NOTIFY_DONE;\n}\n\nstatic void mlx5_ldev_add_netdev(struct mlx5_lag *ldev,\n\t\t\t\t struct mlx5_core_dev *dev,\n\t\t\t\t struct net_device *netdev)\n{\n\tunsigned int fn = mlx5_get_dev_index(dev);\n\tunsigned long flags;\n\n\tif (fn >= ldev->ports)\n\t\treturn;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev->pf[fn].netdev = netdev;\n\tldev->tracker.netdev_state[fn].link_up = 0;\n\tldev->tracker.netdev_state[fn].tx_enabled = 0;\n\tspin_unlock_irqrestore(&lag_lock, flags);\n}\n\nstatic void mlx5_ldev_remove_netdev(struct mlx5_lag *ldev,\n\t\t\t\t    struct net_device *netdev)\n{\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tfor (i = 0; i < ldev->ports; i++) {\n\t\tif (ldev->pf[i].netdev == netdev) {\n\t\t\tldev->pf[i].netdev = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&lag_lock, flags);\n}\n\nstatic void mlx5_ldev_add_mdev(struct mlx5_lag *ldev,\n\t\t\t       struct mlx5_core_dev *dev)\n{\n\tunsigned int fn = mlx5_get_dev_index(dev);\n\n\tif (fn >= ldev->ports)\n\t\treturn;\n\n\tldev->pf[fn].dev = dev;\n\tdev->priv.lag = ldev;\n}\n\nstatic void mlx5_ldev_remove_mdev(struct mlx5_lag *ldev,\n\t\t\t\t  struct mlx5_core_dev *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < ldev->ports; i++)\n\t\tif (ldev->pf[i].dev == dev)\n\t\t\tbreak;\n\n\tif (i == ldev->ports)\n\t\treturn;\n\n\tldev->pf[i].dev = NULL;\n\tdev->priv.lag = NULL;\n}\n\n \nstatic int __mlx5_lag_dev_add_mdev(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev = NULL;\n\tstruct mlx5_core_dev *tmp_dev;\n\n\ttmp_dev = mlx5_get_next_phys_dev_lag(dev);\n\tif (tmp_dev)\n\t\tldev = mlx5_lag_dev(tmp_dev);\n\n\tif (!ldev) {\n\t\tldev = mlx5_lag_dev_alloc(dev);\n\t\tif (!ldev) {\n\t\t\tmlx5_core_err(dev, \"Failed to alloc lag dev\\n\");\n\t\t\treturn 0;\n\t\t}\n\t\tmlx5_ldev_add_mdev(ldev, dev);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&ldev->lock);\n\tif (ldev->mode_changes_in_progress) {\n\t\tmutex_unlock(&ldev->lock);\n\t\treturn -EAGAIN;\n\t}\n\tmlx5_ldev_get(ldev);\n\tmlx5_ldev_add_mdev(ldev, dev);\n\tmutex_unlock(&ldev->lock);\n\n\treturn 0;\n}\n\nvoid mlx5_lag_remove_mdev(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\n\tldev = mlx5_lag_dev(dev);\n\tif (!ldev)\n\t\treturn;\n\n\t \n\tmlx5_ldev_remove_debugfs(dev->priv.dbg.lag_debugfs);\nrecheck:\n\tmutex_lock(&ldev->lock);\n\tif (ldev->mode_changes_in_progress) {\n\t\tmutex_unlock(&ldev->lock);\n\t\tmsleep(100);\n\t\tgoto recheck;\n\t}\n\tmlx5_ldev_remove_mdev(ldev, dev);\n\tmutex_unlock(&ldev->lock);\n\tmlx5_ldev_put(ldev);\n}\n\nvoid mlx5_lag_add_mdev(struct mlx5_core_dev *dev)\n{\n\tint err;\n\n\tif (!mlx5_lag_is_supported(dev))\n\t\treturn;\n\nrecheck:\n\tmlx5_dev_list_lock();\n\terr = __mlx5_lag_dev_add_mdev(dev);\n\tmlx5_dev_list_unlock();\n\n\tif (err) {\n\t\tmsleep(100);\n\t\tgoto recheck;\n\t}\n\tmlx5_ldev_add_debugfs(dev);\n}\n\nvoid mlx5_lag_remove_netdev(struct mlx5_core_dev *dev,\n\t\t\t    struct net_device *netdev)\n{\n\tstruct mlx5_lag *ldev;\n\tbool lag_is_active;\n\n\tldev = mlx5_lag_dev(dev);\n\tif (!ldev)\n\t\treturn;\n\n\tmutex_lock(&ldev->lock);\n\tmlx5_ldev_remove_netdev(ldev, netdev);\n\tclear_bit(MLX5_LAG_FLAG_NDEVS_READY, &ldev->state_flags);\n\n\tlag_is_active = __mlx5_lag_is_active(ldev);\n\tmutex_unlock(&ldev->lock);\n\n\tif (lag_is_active)\n\t\tmlx5_queue_bond_work(ldev, 0);\n}\n\nvoid mlx5_lag_add_netdev(struct mlx5_core_dev *dev,\n\t\t\t struct net_device *netdev)\n{\n\tstruct mlx5_lag *ldev;\n\tint i;\n\n\tldev = mlx5_lag_dev(dev);\n\tif (!ldev)\n\t\treturn;\n\n\tmutex_lock(&ldev->lock);\n\tmlx5_ldev_add_netdev(ldev, dev, netdev);\n\n\tfor (i = 0; i < ldev->ports; i++)\n\t\tif (!ldev->pf[i].netdev)\n\t\t\tbreak;\n\n\tif (i >= ldev->ports)\n\t\tset_bit(MLX5_LAG_FLAG_NDEVS_READY, &ldev->state_flags);\n\tmutex_unlock(&ldev->lock);\n\tmlx5_queue_bond_work(ldev, 0);\n}\n\nbool mlx5_lag_is_roce(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\tres  = ldev && __mlx5_lag_is_roce(ldev);\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL(mlx5_lag_is_roce);\n\nbool mlx5_lag_is_active(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\tres  = ldev && __mlx5_lag_is_active(ldev);\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL(mlx5_lag_is_active);\n\nbool mlx5_lag_mode_is_hash(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tbool res = 0;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\tif (ldev)\n\t\tres = test_bit(MLX5_LAG_MODE_FLAG_HASH_BASED, &ldev->mode_flags);\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL(mlx5_lag_mode_is_hash);\n\nbool mlx5_lag_is_master(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\tres = ldev && __mlx5_lag_is_active(ldev) &&\n\t\tdev == ldev->pf[MLX5_LAG_P1].dev;\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL(mlx5_lag_is_master);\n\nbool mlx5_lag_is_sriov(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\tres  = ldev && __mlx5_lag_is_sriov(ldev);\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL(mlx5_lag_is_sriov);\n\nbool mlx5_lag_is_shared_fdb(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tbool res;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\tres = ldev && test_bit(MLX5_LAG_MODE_FLAG_SHARED_FDB, &ldev->mode_flags);\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\n\treturn res;\n}\nEXPORT_SYMBOL(mlx5_lag_is_shared_fdb);\n\nvoid mlx5_lag_disable_change(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\n\tldev = mlx5_lag_dev(dev);\n\tif (!ldev)\n\t\treturn;\n\n\tmlx5_dev_list_lock();\n\tmutex_lock(&ldev->lock);\n\n\tldev->mode_changes_in_progress++;\n\tif (__mlx5_lag_is_active(ldev))\n\t\tmlx5_disable_lag(ldev);\n\n\tmutex_unlock(&ldev->lock);\n\tmlx5_dev_list_unlock();\n}\n\nvoid mlx5_lag_enable_change(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\n\tldev = mlx5_lag_dev(dev);\n\tif (!ldev)\n\t\treturn;\n\n\tmutex_lock(&ldev->lock);\n\tldev->mode_changes_in_progress--;\n\tmutex_unlock(&ldev->lock);\n\tmlx5_queue_bond_work(ldev, 0);\n}\n\nstruct net_device *mlx5_lag_get_roce_netdev(struct mlx5_core_dev *dev)\n{\n\tstruct net_device *ndev = NULL;\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tint i;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\n\tif (!(ldev && __mlx5_lag_is_roce(ldev)))\n\t\tgoto unlock;\n\n\tif (ldev->tracker.tx_type == NETDEV_LAG_TX_TYPE_ACTIVEBACKUP) {\n\t\tfor (i = 0; i < ldev->ports; i++)\n\t\t\tif (ldev->tracker.netdev_state[i].tx_enabled)\n\t\t\t\tndev = ldev->pf[i].netdev;\n\t\tif (!ndev)\n\t\t\tndev = ldev->pf[ldev->ports - 1].netdev;\n\t} else {\n\t\tndev = ldev->pf[MLX5_LAG_P1].netdev;\n\t}\n\tif (ndev)\n\t\tdev_hold(ndev);\n\nunlock:\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\n\treturn ndev;\n}\nEXPORT_SYMBOL(mlx5_lag_get_roce_netdev);\n\nu8 mlx5_lag_get_slave_port(struct mlx5_core_dev *dev,\n\t\t\t   struct net_device *slave)\n{\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tu8 port = 0;\n\tint i;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\tif (!(ldev && __mlx5_lag_is_roce(ldev)))\n\t\tgoto unlock;\n\n\tfor (i = 0; i < ldev->ports; i++) {\n\t\tif (ldev->pf[MLX5_LAG_P1].netdev == slave) {\n\t\t\tport = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tport = ldev->v2p_map[port * ldev->buckets];\n\nunlock:\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\treturn port;\n}\nEXPORT_SYMBOL(mlx5_lag_get_slave_port);\n\nu8 mlx5_lag_get_num_ports(struct mlx5_core_dev *dev)\n{\n\tstruct mlx5_lag *ldev;\n\n\tldev = mlx5_lag_dev(dev);\n\tif (!ldev)\n\t\treturn 0;\n\n\treturn ldev->ports;\n}\nEXPORT_SYMBOL(mlx5_lag_get_num_ports);\n\nstruct mlx5_core_dev *mlx5_lag_get_next_peer_mdev(struct mlx5_core_dev *dev, int *i)\n{\n\tstruct mlx5_core_dev *peer_dev = NULL;\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tint idx;\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\tif (!ldev)\n\t\tgoto unlock;\n\n\tif (*i == ldev->ports)\n\t\tgoto unlock;\n\tfor (idx = *i; idx < ldev->ports; idx++)\n\t\tif (ldev->pf[idx].dev != dev)\n\t\t\tbreak;\n\n\tif (idx == ldev->ports) {\n\t\t*i = idx;\n\t\tgoto unlock;\n\t}\n\t*i = idx + 1;\n\n\tpeer_dev = ldev->pf[idx].dev;\n\nunlock:\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\treturn peer_dev;\n}\nEXPORT_SYMBOL(mlx5_lag_get_next_peer_mdev);\n\nint mlx5_lag_query_cong_counters(struct mlx5_core_dev *dev,\n\t\t\t\t u64 *values,\n\t\t\t\t int num_counters,\n\t\t\t\t size_t *offsets)\n{\n\tint outlen = MLX5_ST_SZ_BYTES(query_cong_statistics_out);\n\tstruct mlx5_core_dev **mdev;\n\tstruct mlx5_lag *ldev;\n\tunsigned long flags;\n\tint num_ports;\n\tint ret, i, j;\n\tvoid *out;\n\n\tout = kvzalloc(outlen, GFP_KERNEL);\n\tif (!out)\n\t\treturn -ENOMEM;\n\n\tmdev = kvzalloc(sizeof(mdev[0]) * MLX5_MAX_PORTS, GFP_KERNEL);\n\tif (!mdev) {\n\t\tret = -ENOMEM;\n\t\tgoto free_out;\n\t}\n\n\tmemset(values, 0, sizeof(*values) * num_counters);\n\n\tspin_lock_irqsave(&lag_lock, flags);\n\tldev = mlx5_lag_dev(dev);\n\tif (ldev && __mlx5_lag_is_active(ldev)) {\n\t\tnum_ports = ldev->ports;\n\t\tfor (i = 0; i < ldev->ports; i++)\n\t\t\tmdev[i] = ldev->pf[i].dev;\n\t} else {\n\t\tnum_ports = 1;\n\t\tmdev[MLX5_LAG_P1] = dev;\n\t}\n\tspin_unlock_irqrestore(&lag_lock, flags);\n\n\tfor (i = 0; i < num_ports; ++i) {\n\t\tu32 in[MLX5_ST_SZ_DW(query_cong_statistics_in)] = {};\n\n\t\tMLX5_SET(query_cong_statistics_in, in, opcode,\n\t\t\t MLX5_CMD_OP_QUERY_CONG_STATISTICS);\n\t\tret = mlx5_cmd_exec_inout(mdev[i], query_cong_statistics, in,\n\t\t\t\t\t  out);\n\t\tif (ret)\n\t\t\tgoto free_mdev;\n\n\t\tfor (j = 0; j < num_counters; ++j)\n\t\t\tvalues[j] += be64_to_cpup((__be64 *)(out + offsets[j]));\n\t}\n\nfree_mdev:\n\tkvfree(mdev);\nfree_out:\n\tkvfree(out);\n\treturn ret;\n}\nEXPORT_SYMBOL(mlx5_lag_query_cong_counters);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}