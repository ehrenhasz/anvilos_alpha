{
  "module_name": "main.c",
  "hash_id": "7af6476212d0810e2542d006cf4edac1db1b9fdb851ea731ff8aac0dad5b2bb6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx4/main.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/pci.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/io-mapping.h>\n#include <linux/delay.h>\n#include <linux/etherdevice.h>\n#include <net/devlink.h>\n\n#include <uapi/rdma/mlx4-abi.h>\n#include <linux/mlx4/device.h>\n#include <linux/mlx4/doorbell.h>\n\n#include \"mlx4.h\"\n#include \"fw.h\"\n#include \"icm.h\"\n\nMODULE_AUTHOR(\"Roland Dreier\");\nMODULE_DESCRIPTION(\"Mellanox ConnectX HCA low-level driver\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_VERSION(DRV_VERSION);\n\nstruct workqueue_struct *mlx4_wq;\n\n#ifdef CONFIG_MLX4_DEBUG\n\nint mlx4_debug_level;  \nmodule_param_named(debug_level, mlx4_debug_level, int, 0644);\nMODULE_PARM_DESC(debug_level, \"Enable debug tracing if > 0\");\n\n#endif  \n\n#ifdef CONFIG_PCI_MSI\n\nstatic int msi_x = 1;\nmodule_param(msi_x, int, 0444);\nMODULE_PARM_DESC(msi_x, \"0 - don't use MSI-X, 1 - use MSI-X, >1 - limit number of MSI-X irqs to msi_x\");\n\n#else  \n\n#define msi_x (0)\n\n#endif  \n\nstatic uint8_t num_vfs[3] = {0, 0, 0};\nstatic int num_vfs_argc;\nmodule_param_array(num_vfs, byte, &num_vfs_argc, 0444);\nMODULE_PARM_DESC(num_vfs, \"enable #num_vfs functions if num_vfs > 0\\n\"\n\t\t\t  \"num_vfs=port1,port2,port1+2\");\n\nstatic uint8_t probe_vf[3] = {0, 0, 0};\nstatic int probe_vfs_argc;\nmodule_param_array(probe_vf, byte, &probe_vfs_argc, 0444);\nMODULE_PARM_DESC(probe_vf, \"number of vfs to probe by pf driver (num_vfs > 0)\\n\"\n\t\t\t   \"probe_vf=port1,port2,port1+2\");\n\nstatic int mlx4_log_num_mgm_entry_size = MLX4_DEFAULT_MGM_LOG_ENTRY_SIZE;\nmodule_param_named(log_num_mgm_entry_size,\n\t\t\tmlx4_log_num_mgm_entry_size, int, 0444);\nMODULE_PARM_DESC(log_num_mgm_entry_size, \"log mgm size, that defines the num\"\n\t\t\t\t\t \" of qp per mcg, for example:\"\n\t\t\t\t\t \" 10 gives 248.range: 7 <=\"\n\t\t\t\t\t \" log_num_mgm_entry_size <= 12.\"\n\t\t\t\t\t \" To activate device managed\"\n\t\t\t\t\t \" flow steering when available, set to -1\");\n\nstatic bool enable_64b_cqe_eqe = true;\nmodule_param(enable_64b_cqe_eqe, bool, 0444);\nMODULE_PARM_DESC(enable_64b_cqe_eqe,\n\t\t \"Enable 64 byte CQEs/EQEs when the FW supports this (default: True)\");\n\nstatic bool enable_4k_uar;\nmodule_param(enable_4k_uar, bool, 0444);\nMODULE_PARM_DESC(enable_4k_uar,\n\t\t \"Enable using 4K UAR. Should not be enabled if have VFs which do not support 4K UARs (default: false)\");\n\n#define PF_CONTEXT_BEHAVIOUR_MASK\t(MLX4_FUNC_CAP_64B_EQE_CQE | \\\n\t\t\t\t\t MLX4_FUNC_CAP_EQE_CQE_STRIDE | \\\n\t\t\t\t\t MLX4_FUNC_CAP_DMFS_A0_STATIC)\n\n#define RESET_PERSIST_MASK_FLAGS\t(MLX4_FLAG_SRIOV)\n\nstatic char mlx4_version[] =\n\tDRV_NAME \": Mellanox ConnectX core driver v\"\n\tDRV_VERSION \"\\n\";\n\nstatic const struct mlx4_profile default_profile = {\n\t.num_qp\t\t= 1 << 18,\n\t.num_srq\t= 1 << 16,\n\t.rdmarc_per_qp\t= 1 << 4,\n\t.num_cq\t\t= 1 << 16,\n\t.num_mcg\t= 1 << 13,\n\t.num_mpt\t= 1 << 19,\n\t.num_mtt\t= 1 << 20,  \n};\n\nstatic const struct mlx4_profile low_mem_profile = {\n\t.num_qp\t\t= 1 << 17,\n\t.num_srq\t= 1 << 6,\n\t.rdmarc_per_qp\t= 1 << 4,\n\t.num_cq\t\t= 1 << 8,\n\t.num_mcg\t= 1 << 8,\n\t.num_mpt\t= 1 << 9,\n\t.num_mtt\t= 1 << 7,\n};\n\nstatic int log_num_mac = 7;\nmodule_param_named(log_num_mac, log_num_mac, int, 0444);\nMODULE_PARM_DESC(log_num_mac, \"Log2 max number of MACs per ETH port (1-7)\");\n\nstatic int log_num_vlan;\nmodule_param_named(log_num_vlan, log_num_vlan, int, 0444);\nMODULE_PARM_DESC(log_num_vlan, \"Log2 max number of VLANs per ETH port (0-7)\");\n \n#define MLX4_LOG_NUM_VLANS 7\n#define MLX4_MIN_LOG_NUM_VLANS 0\n#define MLX4_MIN_LOG_NUM_MAC 1\n\nstatic bool use_prio;\nmodule_param_named(use_prio, use_prio, bool, 0444);\nMODULE_PARM_DESC(use_prio, \"Enable steering by VLAN priority on ETH ports (deprecated)\");\n\nint log_mtts_per_seg = ilog2(1);\nmodule_param_named(log_mtts_per_seg, log_mtts_per_seg, int, 0444);\nMODULE_PARM_DESC(log_mtts_per_seg, \"Log2 number of MTT entries per segment \"\n\t\t \"(0-7) (default: 0)\");\n\nstatic int port_type_array[2] = {MLX4_PORT_TYPE_NONE, MLX4_PORT_TYPE_NONE};\nstatic int arr_argc = 2;\nmodule_param_array(port_type_array, int, &arr_argc, 0444);\nMODULE_PARM_DESC(port_type_array, \"Array of port types: HW_DEFAULT (0) is default \"\n\t\t\t\t\"1 for IB, 2 for Ethernet\");\n\nstruct mlx4_port_config {\n\tstruct list_head list;\n\tenum mlx4_port_type port_type[MLX4_MAX_PORTS + 1];\n\tstruct pci_dev *pdev;\n};\n\nstatic atomic_t pf_loading = ATOMIC_INIT(0);\n\nstatic int mlx4_devlink_ierr_reset_get(struct devlink *devlink, u32 id,\n\t\t\t\t       struct devlink_param_gset_ctx *ctx)\n{\n\tctx->val.vbool = !!mlx4_internal_err_reset;\n\treturn 0;\n}\n\nstatic int mlx4_devlink_ierr_reset_set(struct devlink *devlink, u32 id,\n\t\t\t\t       struct devlink_param_gset_ctx *ctx)\n{\n\tmlx4_internal_err_reset = ctx->val.vbool;\n\treturn 0;\n}\n\nstatic int mlx4_devlink_crdump_snapshot_get(struct devlink *devlink, u32 id,\n\t\t\t\t\t    struct devlink_param_gset_ctx *ctx)\n{\n\tstruct mlx4_priv *priv = devlink_priv(devlink);\n\tstruct mlx4_dev *dev = &priv->dev;\n\n\tctx->val.vbool = dev->persist->crdump.snapshot_enable;\n\treturn 0;\n}\n\nstatic int mlx4_devlink_crdump_snapshot_set(struct devlink *devlink, u32 id,\n\t\t\t\t\t    struct devlink_param_gset_ctx *ctx)\n{\n\tstruct mlx4_priv *priv = devlink_priv(devlink);\n\tstruct mlx4_dev *dev = &priv->dev;\n\n\tdev->persist->crdump.snapshot_enable = ctx->val.vbool;\n\treturn 0;\n}\n\nstatic int\nmlx4_devlink_max_macs_validate(struct devlink *devlink, u32 id,\n\t\t\t       union devlink_param_value val,\n\t\t\t       struct netlink_ext_ack *extack)\n{\n\tu32 value = val.vu32;\n\n\tif (value < 1 || value > 128)\n\t\treturn -ERANGE;\n\n\tif (!is_power_of_2(value)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"max_macs supported must be power of 2\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nenum mlx4_devlink_param_id {\n\tMLX4_DEVLINK_PARAM_ID_BASE = DEVLINK_PARAM_GENERIC_ID_MAX,\n\tMLX4_DEVLINK_PARAM_ID_ENABLE_64B_CQE_EQE,\n\tMLX4_DEVLINK_PARAM_ID_ENABLE_4K_UAR,\n};\n\nstatic const struct devlink_param mlx4_devlink_params[] = {\n\tDEVLINK_PARAM_GENERIC(INT_ERR_RESET,\n\t\t\t      BIT(DEVLINK_PARAM_CMODE_RUNTIME) |\n\t\t\t      BIT(DEVLINK_PARAM_CMODE_DRIVERINIT),\n\t\t\t      mlx4_devlink_ierr_reset_get,\n\t\t\t      mlx4_devlink_ierr_reset_set, NULL),\n\tDEVLINK_PARAM_GENERIC(MAX_MACS,\n\t\t\t      BIT(DEVLINK_PARAM_CMODE_DRIVERINIT),\n\t\t\t      NULL, NULL, mlx4_devlink_max_macs_validate),\n\tDEVLINK_PARAM_GENERIC(REGION_SNAPSHOT,\n\t\t\t      BIT(DEVLINK_PARAM_CMODE_RUNTIME) |\n\t\t\t      BIT(DEVLINK_PARAM_CMODE_DRIVERINIT),\n\t\t\t      mlx4_devlink_crdump_snapshot_get,\n\t\t\t      mlx4_devlink_crdump_snapshot_set, NULL),\n\tDEVLINK_PARAM_DRIVER(MLX4_DEVLINK_PARAM_ID_ENABLE_64B_CQE_EQE,\n\t\t\t     \"enable_64b_cqe_eqe\", DEVLINK_PARAM_TYPE_BOOL,\n\t\t\t     BIT(DEVLINK_PARAM_CMODE_DRIVERINIT),\n\t\t\t     NULL, NULL, NULL),\n\tDEVLINK_PARAM_DRIVER(MLX4_DEVLINK_PARAM_ID_ENABLE_4K_UAR,\n\t\t\t     \"enable_4k_uar\", DEVLINK_PARAM_TYPE_BOOL,\n\t\t\t     BIT(DEVLINK_PARAM_CMODE_DRIVERINIT),\n\t\t\t     NULL, NULL, NULL),\n};\n\nstatic void mlx4_devlink_set_params_init_values(struct devlink *devlink)\n{\n\tunion devlink_param_value value;\n\n\tvalue.vbool = !!mlx4_internal_err_reset;\n\tdevl_param_driverinit_value_set(devlink,\n\t\t\t\t\tDEVLINK_PARAM_GENERIC_ID_INT_ERR_RESET,\n\t\t\t\t\tvalue);\n\n\tvalue.vu32 = 1UL << log_num_mac;\n\tdevl_param_driverinit_value_set(devlink,\n\t\t\t\t\tDEVLINK_PARAM_GENERIC_ID_MAX_MACS,\n\t\t\t\t\tvalue);\n\n\tvalue.vbool = enable_64b_cqe_eqe;\n\tdevl_param_driverinit_value_set(devlink,\n\t\t\t\t\tMLX4_DEVLINK_PARAM_ID_ENABLE_64B_CQE_EQE,\n\t\t\t\t\tvalue);\n\n\tvalue.vbool = enable_4k_uar;\n\tdevl_param_driverinit_value_set(devlink,\n\t\t\t\t\tMLX4_DEVLINK_PARAM_ID_ENABLE_4K_UAR,\n\t\t\t\t\tvalue);\n\n\tvalue.vbool = false;\n\tdevl_param_driverinit_value_set(devlink,\n\t\t\t\t\tDEVLINK_PARAM_GENERIC_ID_REGION_SNAPSHOT,\n\t\t\t\t\tvalue);\n}\n\nstatic inline void mlx4_set_num_reserved_uars(struct mlx4_dev *dev,\n\t\t\t\t\t      struct mlx4_dev_cap *dev_cap)\n{\n\t \n\tdev->caps.reserved_uars\t=\n\t\tmax_t(int,\n\t\t      mlx4_get_num_reserved_uar(dev),\n\t\t      dev_cap->reserved_uars /\n\t\t\t(1 << (PAGE_SHIFT - dev->uar_page_shift)));\n}\n\nint mlx4_check_port_params(struct mlx4_dev *dev,\n\t\t\t   enum mlx4_port_type *port_type)\n{\n\tint i;\n\n\tif (!(dev->caps.flags & MLX4_DEV_CAP_FLAG_DPDP)) {\n\t\tfor (i = 0; i < dev->caps.num_ports - 1; i++) {\n\t\t\tif (port_type[i] != port_type[i + 1]) {\n\t\t\t\tmlx4_err(dev, \"Only same port types supported on this HCA, aborting\\n\");\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (i = 0; i < dev->caps.num_ports; i++) {\n\t\tif (!(port_type[i] & dev->caps.supported_type[i+1])) {\n\t\t\tmlx4_err(dev, \"Requested port type for port %d is not supported on this HCA\\n\",\n\t\t\t\t i + 1);\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void mlx4_set_port_mask(struct mlx4_dev *dev)\n{\n\tint i;\n\n\tfor (i = 1; i <= dev->caps.num_ports; ++i)\n\t\tdev->caps.port_mask[i] = dev->caps.port_type[i];\n}\n\nenum {\n\tMLX4_QUERY_FUNC_NUM_SYS_EQS = 1 << 0,\n};\n\nstatic int mlx4_query_func(struct mlx4_dev *dev, struct mlx4_dev_cap *dev_cap)\n{\n\tint err = 0;\n\tstruct mlx4_func func;\n\n\tif (dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_SYS_EQS) {\n\t\terr = mlx4_QUERY_FUNC(dev, &func, 0);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"QUERY_DEV_CAP command failed, aborting.\\n\");\n\t\t\treturn err;\n\t\t}\n\t\tdev_cap->max_eqs = func.max_eq;\n\t\tdev_cap->reserved_eqs = func.rsvd_eqs;\n\t\tdev_cap->reserved_uars = func.rsvd_uars;\n\t\terr |= MLX4_QUERY_FUNC_NUM_SYS_EQS;\n\t}\n\treturn err;\n}\n\nstatic void mlx4_enable_cqe_eqe_stride(struct mlx4_dev *dev)\n{\n\tstruct mlx4_caps *dev_cap = &dev->caps;\n\n\t \n\tif (!(dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_EQE_STRIDE) ||\n\t    !(dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_CQE_STRIDE))\n\t\treturn;\n\n\t \n\tif (!(dev_cap->flags & MLX4_DEV_CAP_FLAG_64B_EQE) ||\n\t    !(dev_cap->flags & MLX4_DEV_CAP_FLAG_64B_CQE)) {\n\t\tdev_cap->flags2 &= ~MLX4_DEV_CAP_FLAG2_CQE_STRIDE;\n\t\tdev_cap->flags2 &= ~MLX4_DEV_CAP_FLAG2_EQE_STRIDE;\n\t\treturn;\n\t}\n\n\tif (cache_line_size() == 128 || cache_line_size() == 256) {\n\t\tmlx4_dbg(dev, \"Enabling CQE stride cacheLine supported\\n\");\n\t\t \n\t\tdev_cap->flags &= ~MLX4_DEV_CAP_FLAG_64B_CQE;\n\t\tdev_cap->flags &= ~MLX4_DEV_CAP_FLAG_64B_EQE;\n\n\t\tif (mlx4_is_master(dev))\n\t\t\tdev_cap->function_caps |= MLX4_FUNC_CAP_EQE_CQE_STRIDE;\n\t} else {\n\t\tif (cache_line_size() != 32  && cache_line_size() != 64)\n\t\t\tmlx4_dbg(dev, \"Disabling CQE stride, cacheLine size unsupported\\n\");\n\t\tdev_cap->flags2 &= ~MLX4_DEV_CAP_FLAG2_CQE_STRIDE;\n\t\tdev_cap->flags2 &= ~MLX4_DEV_CAP_FLAG2_EQE_STRIDE;\n\t}\n}\n\nstatic int _mlx4_dev_port(struct mlx4_dev *dev, int port,\n\t\t\t  struct mlx4_port_cap *port_cap)\n{\n\tdev->caps.vl_cap[port]\t    = port_cap->max_vl;\n\tdev->caps.ib_mtu_cap[port]\t    = port_cap->ib_mtu;\n\tdev->phys_caps.gid_phys_table_len[port]  = port_cap->max_gids;\n\tdev->phys_caps.pkey_phys_table_len[port] = port_cap->max_pkeys;\n\t \n\tdev->caps.gid_table_len[port]  = port_cap->max_gids;\n\tdev->caps.pkey_table_len[port] = port_cap->max_pkeys;\n\tdev->caps.port_width_cap[port] = port_cap->max_port_width;\n\tdev->caps.eth_mtu_cap[port]    = port_cap->eth_mtu;\n\tdev->caps.max_tc_eth\t       = port_cap->max_tc_eth;\n\tdev->caps.def_mac[port]        = port_cap->def_mac;\n\tdev->caps.supported_type[port] = port_cap->supported_port_types;\n\tdev->caps.suggested_type[port] = port_cap->suggested_type;\n\tdev->caps.default_sense[port] = port_cap->default_sense;\n\tdev->caps.trans_type[port]\t    = port_cap->trans_type;\n\tdev->caps.vendor_oui[port]     = port_cap->vendor_oui;\n\tdev->caps.wavelength[port]     = port_cap->wavelength;\n\tdev->caps.trans_code[port]     = port_cap->trans_code;\n\n\treturn 0;\n}\n\nstatic int mlx4_dev_port(struct mlx4_dev *dev, int port,\n\t\t\t struct mlx4_port_cap *port_cap)\n{\n\tint err = 0;\n\n\terr = mlx4_QUERY_PORT(dev, port, port_cap);\n\n\tif (err)\n\t\tmlx4_err(dev, \"QUERY_PORT command failed.\\n\");\n\n\treturn err;\n}\n\nstatic inline void mlx4_enable_ignore_fcs(struct mlx4_dev *dev)\n{\n\tif (!(dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_IGNORE_FCS))\n\t\treturn;\n\n\tif (mlx4_is_mfunc(dev)) {\n\t\tmlx4_dbg(dev, \"SRIOV mode - Disabling Ignore FCS\");\n\t\tdev->caps.flags2 &= ~MLX4_DEV_CAP_FLAG2_IGNORE_FCS;\n\t\treturn;\n\t}\n\n\tif (!(dev->caps.flags & MLX4_DEV_CAP_FLAG_FCS_KEEP)) {\n\t\tmlx4_dbg(dev,\n\t\t\t \"Keep FCS is not supported - Disabling Ignore FCS\");\n\t\tdev->caps.flags2 &= ~MLX4_DEV_CAP_FLAG2_IGNORE_FCS;\n\t\treturn;\n\t}\n}\n\n#define MLX4_A0_STEERING_TABLE_SIZE\t256\nstatic int mlx4_dev_cap(struct mlx4_dev *dev, struct mlx4_dev_cap *dev_cap)\n{\n\tint err;\n\tint i;\n\n\terr = mlx4_QUERY_DEV_CAP(dev, dev_cap);\n\tif (err) {\n\t\tmlx4_err(dev, \"QUERY_DEV_CAP command failed, aborting\\n\");\n\t\treturn err;\n\t}\n\tmlx4_dev_cap_dump(dev, dev_cap);\n\n\tif (dev_cap->min_page_sz > PAGE_SIZE) {\n\t\tmlx4_err(dev, \"HCA minimum page size of %d bigger than kernel PAGE_SIZE of %ld, aborting\\n\",\n\t\t\t dev_cap->min_page_sz, PAGE_SIZE);\n\t\treturn -ENODEV;\n\t}\n\tif (dev_cap->num_ports > MLX4_MAX_PORTS) {\n\t\tmlx4_err(dev, \"HCA has %d ports, but we only support %d, aborting\\n\",\n\t\t\t dev_cap->num_ports, MLX4_MAX_PORTS);\n\t\treturn -ENODEV;\n\t}\n\n\tif (dev_cap->uar_size > pci_resource_len(dev->persist->pdev, 2)) {\n\t\tmlx4_err(dev, \"HCA reported UAR size of 0x%x bigger than PCI resource 2 size of 0x%llx, aborting\\n\",\n\t\t\t dev_cap->uar_size,\n\t\t\t (unsigned long long)\n\t\t\t pci_resource_len(dev->persist->pdev, 2));\n\t\treturn -ENODEV;\n\t}\n\n\tdev->caps.num_ports\t     = dev_cap->num_ports;\n\tdev->caps.num_sys_eqs = dev_cap->num_sys_eqs;\n\tdev->phys_caps.num_phys_eqs = dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_SYS_EQS ?\n\t\t\t\t      dev->caps.num_sys_eqs :\n\t\t\t\t      MLX4_MAX_EQ_NUM;\n\tfor (i = 1; i <= dev->caps.num_ports; ++i) {\n\t\terr = _mlx4_dev_port(dev, i, dev_cap->port_cap + i);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"QUERY_PORT command failed, aborting\\n\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tdev->caps.map_clock_to_user  = dev_cap->map_clock_to_user;\n\tdev->caps.uar_page_size\t     = PAGE_SIZE;\n\tdev->caps.num_uars\t     = dev_cap->uar_size / PAGE_SIZE;\n\tdev->caps.local_ca_ack_delay = dev_cap->local_ca_ack_delay;\n\tdev->caps.bf_reg_size\t     = dev_cap->bf_reg_size;\n\tdev->caps.bf_regs_per_page   = dev_cap->bf_regs_per_page;\n\tdev->caps.max_sq_sg\t     = dev_cap->max_sq_sg;\n\tdev->caps.max_rq_sg\t     = dev_cap->max_rq_sg;\n\tdev->caps.max_wqes\t     = dev_cap->max_qp_sz;\n\tdev->caps.max_qp_init_rdma   = dev_cap->max_requester_per_qp;\n\tdev->caps.max_srq_wqes\t     = dev_cap->max_srq_sz;\n\tdev->caps.max_srq_sge\t     = dev_cap->max_rq_sg - 1;\n\tdev->caps.reserved_srqs\t     = dev_cap->reserved_srqs;\n\tdev->caps.max_sq_desc_sz     = dev_cap->max_sq_desc_sz;\n\tdev->caps.max_rq_desc_sz     = dev_cap->max_rq_desc_sz;\n\t \n\tdev->caps.max_cqes\t     = dev_cap->max_cq_sz - 1;\n\tdev->caps.reserved_cqs\t     = dev_cap->reserved_cqs;\n\tdev->caps.reserved_eqs\t     = dev_cap->reserved_eqs;\n\tdev->caps.reserved_mtts      = dev_cap->reserved_mtts;\n\tdev->caps.reserved_mrws\t     = dev_cap->reserved_mrws;\n\n\tdev->caps.reserved_pds\t     = dev_cap->reserved_pds;\n\tdev->caps.reserved_xrcds     = (dev->caps.flags & MLX4_DEV_CAP_FLAG_XRC) ?\n\t\t\t\t\tdev_cap->reserved_xrcds : 0;\n\tdev->caps.max_xrcds          = (dev->caps.flags & MLX4_DEV_CAP_FLAG_XRC) ?\n\t\t\t\t\tdev_cap->max_xrcds : 0;\n\tdev->caps.mtt_entry_sz       = dev_cap->mtt_entry_sz;\n\n\tdev->caps.max_msg_sz         = dev_cap->max_msg_sz;\n\tdev->caps.page_size_cap\t     = ~(u32) (dev_cap->min_page_sz - 1);\n\tdev->caps.flags\t\t     = dev_cap->flags;\n\tdev->caps.flags2\t     = dev_cap->flags2;\n\tdev->caps.bmme_flags\t     = dev_cap->bmme_flags;\n\tdev->caps.reserved_lkey\t     = dev_cap->reserved_lkey;\n\tdev->caps.stat_rate_support  = dev_cap->stat_rate_support;\n\tdev->caps.max_gso_sz\t     = dev_cap->max_gso_sz;\n\tdev->caps.max_rss_tbl_sz     = dev_cap->max_rss_tbl_sz;\n\tdev->caps.wol_port[1]          = dev_cap->wol_port[1];\n\tdev->caps.wol_port[2]          = dev_cap->wol_port[2];\n\tdev->caps.health_buffer_addrs  = dev_cap->health_buffer_addrs;\n\n\t \n\tif (!mlx4_is_slave(dev)) {\n\t\t \n\t\tif (enable_4k_uar || !dev->persist->num_vfs)\n\t\t\tdev->uar_page_shift = DEFAULT_UAR_PAGE_SHIFT;\n\t\telse\n\t\t\tdev->uar_page_shift = PAGE_SHIFT;\n\n\t\tmlx4_set_num_reserved_uars(dev, dev_cap);\n\t}\n\n\tif (dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_PHV_EN) {\n\t\tstruct mlx4_init_hca_param hca_param;\n\n\t\tmemset(&hca_param, 0, sizeof(hca_param));\n\t\terr = mlx4_QUERY_HCA(dev, &hca_param);\n\t\t \n\t\tif (err || hca_param.phv_check_en)\n\t\t\tdev->caps.flags2 &= ~MLX4_DEV_CAP_FLAG2_PHV_EN;\n\t}\n\n\t \n\tif (mlx4_priv(dev)->pci_dev_data & MLX4_PCI_DEV_FORCE_SENSE_PORT)\n\t\tdev->caps.flags |= MLX4_DEV_CAP_FLAG_SENSE_SUPPORT;\n\t \n\tif (mlx4_is_mfunc(dev))\n\t\tdev->caps.flags &= ~MLX4_DEV_CAP_FLAG_SENSE_SUPPORT;\n\n\tif (mlx4_low_memory_profile()) {\n\t\tdev->caps.log_num_macs  = MLX4_MIN_LOG_NUM_MAC;\n\t\tdev->caps.log_num_vlans = MLX4_MIN_LOG_NUM_VLANS;\n\t} else {\n\t\tdev->caps.log_num_macs  = log_num_mac;\n\t\tdev->caps.log_num_vlans = MLX4_LOG_NUM_VLANS;\n\t}\n\n\tfor (i = 1; i <= dev->caps.num_ports; ++i) {\n\t\tdev->caps.port_type[i] = MLX4_PORT_TYPE_NONE;\n\t\tif (dev->caps.supported_type[i]) {\n\t\t\t \n\t\t\tif (dev->caps.supported_type[i] == MLX4_PORT_TYPE_ETH)\n\t\t\t\tdev->caps.port_type[i] = MLX4_PORT_TYPE_ETH;\n\t\t\t \n\t\t\telse if (dev->caps.supported_type[i] ==\n\t\t\t\t MLX4_PORT_TYPE_IB)\n\t\t\t\tdev->caps.port_type[i] = MLX4_PORT_TYPE_IB;\n\t\t\telse {\n\t\t\t\t \n\t\t\t\tif (port_type_array[i - 1] == MLX4_PORT_TYPE_NONE)\n\t\t\t\t\tdev->caps.port_type[i] = dev->caps.suggested_type[i] ?\n\t\t\t\t\t\tMLX4_PORT_TYPE_ETH : MLX4_PORT_TYPE_IB;\n\t\t\t\telse\n\t\t\t\t\tdev->caps.port_type[i] = port_type_array[i - 1];\n\t\t\t}\n\t\t}\n\t\t \n\t\tmlx4_priv(dev)->sense.sense_allowed[i] =\n\t\t\t((dev->caps.supported_type[i] == MLX4_PORT_TYPE_AUTO) &&\n\t\t\t (dev->caps.flags & MLX4_DEV_CAP_FLAG_DPDP) &&\n\t\t\t (dev->caps.flags & MLX4_DEV_CAP_FLAG_SENSE_SUPPORT));\n\n\t\t \n\t\tif (mlx4_priv(dev)->sense.sense_allowed[i] && dev->caps.default_sense[i]) {\n\t\t\tenum mlx4_port_type sensed_port = MLX4_PORT_TYPE_NONE;\n\t\t\tdev->caps.possible_type[i] = MLX4_PORT_TYPE_AUTO;\n\t\t\tmlx4_SENSE_PORT(dev, i, &sensed_port);\n\t\t\tif (sensed_port != MLX4_PORT_TYPE_NONE)\n\t\t\t\tdev->caps.port_type[i] = sensed_port;\n\t\t} else {\n\t\t\tdev->caps.possible_type[i] = dev->caps.port_type[i];\n\t\t}\n\n\t\tif (dev->caps.log_num_macs > dev_cap->port_cap[i].log_max_macs) {\n\t\t\tdev->caps.log_num_macs = dev_cap->port_cap[i].log_max_macs;\n\t\t\tmlx4_warn(dev, \"Requested number of MACs is too much for port %d, reducing to %d\\n\",\n\t\t\t\t  i, 1 << dev->caps.log_num_macs);\n\t\t}\n\t\tif (dev->caps.log_num_vlans > dev_cap->port_cap[i].log_max_vlans) {\n\t\t\tdev->caps.log_num_vlans = dev_cap->port_cap[i].log_max_vlans;\n\t\t\tmlx4_warn(dev, \"Requested number of VLANs is too much for port %d, reducing to %d\\n\",\n\t\t\t\t  i, 1 << dev->caps.log_num_vlans);\n\t\t}\n\t}\n\n\tif (mlx4_is_master(dev) && (dev->caps.num_ports == 2) &&\n\t    (port_type_array[0] == MLX4_PORT_TYPE_IB) &&\n\t    (port_type_array[1] == MLX4_PORT_TYPE_ETH)) {\n\t\tmlx4_warn(dev,\n\t\t\t  \"Granular QoS per VF not supported with IB/Eth configuration\\n\");\n\t\tdev->caps.flags2 &= ~MLX4_DEV_CAP_FLAG2_QOS_VPP;\n\t}\n\n\tdev->caps.max_counters = dev_cap->max_counters;\n\n\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW] = dev_cap->reserved_qps;\n\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_ETH_ADDR] =\n\t\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_FC_ADDR] =\n\t\t(1 << dev->caps.log_num_macs) *\n\t\t(1 << dev->caps.log_num_vlans) *\n\t\tdev->caps.num_ports;\n\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_FC_EXCH] = MLX4_NUM_FEXCH;\n\n\tif (dev_cap->dmfs_high_rate_qpn_base > 0 &&\n\t    dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_FS_EN)\n\t\tdev->caps.dmfs_high_rate_qpn_base = dev_cap->dmfs_high_rate_qpn_base;\n\telse\n\t\tdev->caps.dmfs_high_rate_qpn_base =\n\t\t\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW];\n\n\tif (dev_cap->dmfs_high_rate_qpn_range > 0 &&\n\t    dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_FS_EN) {\n\t\tdev->caps.dmfs_high_rate_qpn_range = dev_cap->dmfs_high_rate_qpn_range;\n\t\tdev->caps.dmfs_high_steer_mode = MLX4_STEERING_DMFS_A0_DEFAULT;\n\t\tdev->caps.flags2 |= MLX4_DEV_CAP_FLAG2_FS_A0;\n\t} else {\n\t\tdev->caps.dmfs_high_steer_mode = MLX4_STEERING_DMFS_A0_NOT_SUPPORTED;\n\t\tdev->caps.dmfs_high_rate_qpn_base =\n\t\t\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW];\n\t\tdev->caps.dmfs_high_rate_qpn_range = MLX4_A0_STEERING_TABLE_SIZE;\n\t}\n\n\tdev->caps.rl_caps = dev_cap->rl_caps;\n\n\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_RSS_RAW_ETH] =\n\t\tdev->caps.dmfs_high_rate_qpn_range;\n\n\tdev->caps.reserved_qps = dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW] +\n\t\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_ETH_ADDR] +\n\t\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_FC_ADDR] +\n\t\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_FC_EXCH];\n\n\tdev->caps.sqp_demux = (mlx4_is_master(dev)) ? MLX4_MAX_NUM_SLAVES : 0;\n\n\tif (!enable_64b_cqe_eqe && !mlx4_is_slave(dev)) {\n\t\tif (dev_cap->flags &\n\t\t    (MLX4_DEV_CAP_FLAG_64B_CQE | MLX4_DEV_CAP_FLAG_64B_EQE)) {\n\t\t\tmlx4_warn(dev, \"64B EQEs/CQEs supported by the device but not enabled\\n\");\n\t\t\tdev->caps.flags &= ~MLX4_DEV_CAP_FLAG_64B_CQE;\n\t\t\tdev->caps.flags &= ~MLX4_DEV_CAP_FLAG_64B_EQE;\n\t\t}\n\n\t\tif (dev_cap->flags2 &\n\t\t    (MLX4_DEV_CAP_FLAG2_CQE_STRIDE |\n\t\t     MLX4_DEV_CAP_FLAG2_EQE_STRIDE)) {\n\t\t\tmlx4_warn(dev, \"Disabling EQE/CQE stride per user request\\n\");\n\t\t\tdev_cap->flags2 &= ~MLX4_DEV_CAP_FLAG2_CQE_STRIDE;\n\t\t\tdev_cap->flags2 &= ~MLX4_DEV_CAP_FLAG2_EQE_STRIDE;\n\t\t}\n\t}\n\n\tif ((dev->caps.flags &\n\t    (MLX4_DEV_CAP_FLAG_64B_CQE | MLX4_DEV_CAP_FLAG_64B_EQE)) &&\n\t    mlx4_is_master(dev))\n\t\tdev->caps.function_caps |= MLX4_FUNC_CAP_64B_EQE_CQE;\n\n\tif (!mlx4_is_slave(dev)) {\n\t\tmlx4_enable_cqe_eqe_stride(dev);\n\t\tdev->caps.alloc_res_qp_mask =\n\t\t\t(dev->caps.bf_reg_size ? MLX4_RESERVE_ETH_BF_QP : 0) |\n\t\t\tMLX4_RESERVE_A0_QP;\n\n\t\tif (!(dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_ETS_CFG) &&\n\t\t    dev->caps.flags & MLX4_DEV_CAP_FLAG_SET_ETH_SCHED) {\n\t\t\tmlx4_warn(dev, \"Old device ETS support detected\\n\");\n\t\t\tmlx4_warn(dev, \"Consider upgrading device FW.\\n\");\n\t\t\tdev->caps.flags2 |= MLX4_DEV_CAP_FLAG2_ETS_CFG;\n\t\t}\n\n\t} else {\n\t\tdev->caps.alloc_res_qp_mask = 0;\n\t}\n\n\tmlx4_enable_ignore_fcs(dev);\n\n\treturn 0;\n}\n\n \nstatic int mlx4_how_many_lives_vf(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_slave_state *s_state;\n\tint i;\n\tint ret = 0;\n\n\tfor (i = 1 ; i < dev->num_slaves; ++i) {\n\t\ts_state = &priv->mfunc.master.slave_state[i];\n\t\tif (s_state->active && s_state->last_cmd !=\n\t\t    MLX4_COMM_CMD_RESET) {\n\t\t\tmlx4_warn(dev, \"%s: slave: %d is still active\\n\",\n\t\t\t\t  __func__, i);\n\t\t\tret++;\n\t\t}\n\t}\n\treturn ret;\n}\n\nint mlx4_get_parav_qkey(struct mlx4_dev *dev, u32 qpn, u32 *qkey)\n{\n\tu32 qk = MLX4_RESERVED_QKEY_BASE;\n\n\tif (qpn >= dev->phys_caps.base_tunnel_sqpn + 8 * MLX4_MFUNC_MAX ||\n\t    qpn < dev->phys_caps.base_proxy_sqpn)\n\t\treturn -EINVAL;\n\n\tif (qpn >= dev->phys_caps.base_tunnel_sqpn)\n\t\t \n\t\tqk += qpn - dev->phys_caps.base_tunnel_sqpn;\n\telse\n\t\tqk += qpn - dev->phys_caps.base_proxy_sqpn;\n\t*qkey = qk;\n\treturn 0;\n}\nEXPORT_SYMBOL(mlx4_get_parav_qkey);\n\nvoid mlx4_sync_pkey_table(struct mlx4_dev *dev, int slave, int port, int i, int val)\n{\n\tstruct mlx4_priv *priv = container_of(dev, struct mlx4_priv, dev);\n\n\tif (!mlx4_is_master(dev))\n\t\treturn;\n\n\tpriv->virt2phys_pkey[slave][port - 1][i] = val;\n}\nEXPORT_SYMBOL(mlx4_sync_pkey_table);\n\nvoid mlx4_put_slave_node_guid(struct mlx4_dev *dev, int slave, __be64 guid)\n{\n\tstruct mlx4_priv *priv = container_of(dev, struct mlx4_priv, dev);\n\n\tif (!mlx4_is_master(dev))\n\t\treturn;\n\n\tpriv->slave_node_guids[slave] = guid;\n}\nEXPORT_SYMBOL(mlx4_put_slave_node_guid);\n\n__be64 mlx4_get_slave_node_guid(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = container_of(dev, struct mlx4_priv, dev);\n\n\tif (!mlx4_is_master(dev))\n\t\treturn 0;\n\n\treturn priv->slave_node_guids[slave];\n}\nEXPORT_SYMBOL(mlx4_get_slave_node_guid);\n\nint mlx4_is_slave_active(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_slave_state *s_slave;\n\n\tif (!mlx4_is_master(dev))\n\t\treturn 0;\n\n\ts_slave = &priv->mfunc.master.slave_state[slave];\n\treturn !!s_slave->active;\n}\nEXPORT_SYMBOL(mlx4_is_slave_active);\n\nvoid mlx4_handle_eth_header_mcast_prio(struct mlx4_net_trans_rule_hw_ctrl *ctrl,\n\t\t\t\t       struct _rule_hw *eth_header)\n{\n\tif (is_multicast_ether_addr(eth_header->eth.dst_mac) ||\n\t    is_broadcast_ether_addr(eth_header->eth.dst_mac)) {\n\t\tstruct mlx4_net_trans_rule_hw_eth *eth =\n\t\t\t(struct mlx4_net_trans_rule_hw_eth *)eth_header;\n\t\tstruct _rule_hw *next_rule = (struct _rule_hw *)(eth + 1);\n\t\tbool last_rule = next_rule->size == 0 && next_rule->id == 0 &&\n\t\t\tnext_rule->rsvd == 0;\n\n\t\tif (last_rule)\n\t\t\tctrl->prio = cpu_to_be16(MLX4_DOMAIN_NIC);\n\t}\n}\nEXPORT_SYMBOL(mlx4_handle_eth_header_mcast_prio);\n\nstatic void slave_adjust_steering_mode(struct mlx4_dev *dev,\n\t\t\t\t       struct mlx4_dev_cap *dev_cap,\n\t\t\t\t       struct mlx4_init_hca_param *hca_param)\n{\n\tdev->caps.steering_mode = hca_param->steering_mode;\n\tif (dev->caps.steering_mode == MLX4_STEERING_MODE_DEVICE_MANAGED) {\n\t\tdev->caps.num_qp_per_mgm = dev_cap->fs_max_num_qp_per_entry;\n\t\tdev->caps.fs_log_max_ucast_qp_range_size =\n\t\t\tdev_cap->fs_log_max_ucast_qp_range_size;\n\t} else\n\t\tdev->caps.num_qp_per_mgm =\n\t\t\t4 * ((1 << hca_param->log_mc_entry_sz)/16 - 2);\n\n\tmlx4_dbg(dev, \"Steering mode is: %s\\n\",\n\t\t mlx4_steering_mode_str(dev->caps.steering_mode));\n}\n\nstatic void mlx4_slave_destroy_special_qp_cap(struct mlx4_dev *dev)\n{\n\tkfree(dev->caps.spec_qps);\n\tdev->caps.spec_qps = NULL;\n}\n\nstatic int mlx4_slave_special_qp_cap(struct mlx4_dev *dev)\n{\n\tstruct mlx4_func_cap *func_cap;\n\tstruct mlx4_caps *caps = &dev->caps;\n\tint i, err = 0;\n\n\tfunc_cap = kzalloc(sizeof(*func_cap), GFP_KERNEL);\n\tcaps->spec_qps = kcalloc(caps->num_ports, sizeof(*caps->spec_qps), GFP_KERNEL);\n\n\tif (!func_cap || !caps->spec_qps) {\n\t\tmlx4_err(dev, \"Failed to allocate memory for special qps cap\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_mem;\n\t}\n\n\tfor (i = 1; i <= caps->num_ports; ++i) {\n\t\terr = mlx4_QUERY_FUNC_CAP(dev, i, func_cap);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"QUERY_FUNC_CAP port command failed for port %d, aborting (%d)\\n\",\n\t\t\t\t i, err);\n\t\t\tgoto err_mem;\n\t\t}\n\t\tcaps->spec_qps[i - 1] = func_cap->spec_qps;\n\t\tcaps->port_mask[i] = caps->port_type[i];\n\t\tcaps->phys_port_id[i] = func_cap->phys_port_id;\n\t\terr = mlx4_get_slave_pkey_gid_tbl_len(dev, i,\n\t\t\t\t\t\t      &caps->gid_table_len[i],\n\t\t\t\t\t\t      &caps->pkey_table_len[i]);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"QUERY_PORT command failed for port %d, aborting (%d)\\n\",\n\t\t\t\t i, err);\n\t\t\tgoto err_mem;\n\t\t}\n\t}\n\nerr_mem:\n\tif (err)\n\t\tmlx4_slave_destroy_special_qp_cap(dev);\n\tkfree(func_cap);\n\treturn err;\n}\n\nstatic int mlx4_slave_cap(struct mlx4_dev *dev)\n{\n\tint\t\t\t   err;\n\tu32\t\t\t   page_size;\n\tstruct mlx4_dev_cap\t   *dev_cap;\n\tstruct mlx4_func_cap\t   *func_cap;\n\tstruct mlx4_init_hca_param *hca_param;\n\n\thca_param = kzalloc(sizeof(*hca_param), GFP_KERNEL);\n\tfunc_cap = kzalloc(sizeof(*func_cap), GFP_KERNEL);\n\tdev_cap = kzalloc(sizeof(*dev_cap), GFP_KERNEL);\n\tif (!hca_param || !func_cap || !dev_cap) {\n\t\tmlx4_err(dev, \"Failed to allocate memory for slave_cap\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto free_mem;\n\t}\n\n\terr = mlx4_QUERY_HCA(dev, hca_param);\n\tif (err) {\n\t\tmlx4_err(dev, \"QUERY_HCA command failed, aborting\\n\");\n\t\tgoto free_mem;\n\t}\n\n\t \n\tif (hca_param->global_caps) {\n\t\tmlx4_err(dev, \"Unknown hca global capabilities\\n\");\n\t\terr = -EINVAL;\n\t\tgoto free_mem;\n\t}\n\n\tdev->caps.hca_core_clock = hca_param->hca_core_clock;\n\n\tdev->caps.max_qp_dest_rdma = 1 << hca_param->log_rd_per_qp;\n\terr = mlx4_dev_cap(dev, dev_cap);\n\tif (err) {\n\t\tmlx4_err(dev, \"QUERY_DEV_CAP command failed, aborting\\n\");\n\t\tgoto free_mem;\n\t}\n\n\terr = mlx4_QUERY_FW(dev);\n\tif (err)\n\t\tmlx4_err(dev, \"QUERY_FW command failed: could not get FW version\\n\");\n\n\tpage_size = ~dev->caps.page_size_cap + 1;\n\tmlx4_warn(dev, \"HCA minimum page size:%d\\n\", page_size);\n\tif (page_size > PAGE_SIZE) {\n\t\tmlx4_err(dev, \"HCA minimum page size of %d bigger than kernel PAGE_SIZE of %ld, aborting\\n\",\n\t\t\t page_size, PAGE_SIZE);\n\t\terr = -ENODEV;\n\t\tgoto free_mem;\n\t}\n\n\t \n\tdev->uar_page_shift = hca_param->uar_page_sz + 12;\n\n\t \n\tif (dev->uar_page_shift > PAGE_SHIFT) {\n\t\tmlx4_err(dev,\n\t\t\t \"Invalid configuration: uar page size is larger than system page size\\n\");\n\t\terr = -ENODEV;\n\t\tgoto free_mem;\n\t}\n\n\t \n\tmlx4_set_num_reserved_uars(dev, dev_cap);\n\n\t \n\tdev->caps.uar_page_size = PAGE_SIZE;\n\n\terr = mlx4_QUERY_FUNC_CAP(dev, 0, func_cap);\n\tif (err) {\n\t\tmlx4_err(dev, \"QUERY_FUNC_CAP general command failed, aborting (%d)\\n\",\n\t\t\t err);\n\t\tgoto free_mem;\n\t}\n\n\tif ((func_cap->pf_context_behaviour | PF_CONTEXT_BEHAVIOUR_MASK) !=\n\t    PF_CONTEXT_BEHAVIOUR_MASK) {\n\t\tmlx4_err(dev, \"Unknown pf context behaviour %x known flags %x\\n\",\n\t\t\t func_cap->pf_context_behaviour,\n\t\t\t PF_CONTEXT_BEHAVIOUR_MASK);\n\t\terr = -EINVAL;\n\t\tgoto free_mem;\n\t}\n\n\tdev->caps.num_ports\t\t= func_cap->num_ports;\n\tdev->quotas.qp\t\t\t= func_cap->qp_quota;\n\tdev->quotas.srq\t\t\t= func_cap->srq_quota;\n\tdev->quotas.cq\t\t\t= func_cap->cq_quota;\n\tdev->quotas.mpt\t\t\t= func_cap->mpt_quota;\n\tdev->quotas.mtt\t\t\t= func_cap->mtt_quota;\n\tdev->caps.num_qps\t\t= 1 << hca_param->log_num_qps;\n\tdev->caps.num_srqs\t\t= 1 << hca_param->log_num_srqs;\n\tdev->caps.num_cqs\t\t= 1 << hca_param->log_num_cqs;\n\tdev->caps.num_mpts\t\t= 1 << hca_param->log_mpt_sz;\n\tdev->caps.num_eqs\t\t= func_cap->max_eq;\n\tdev->caps.reserved_eqs\t\t= func_cap->reserved_eq;\n\tdev->caps.reserved_lkey\t\t= func_cap->reserved_lkey;\n\tdev->caps.num_pds               = MLX4_NUM_PDS;\n\tdev->caps.num_mgms              = 0;\n\tdev->caps.num_amgms             = 0;\n\n\tif (dev->caps.num_ports > MLX4_MAX_PORTS) {\n\t\tmlx4_err(dev, \"HCA has %d ports, but we only support %d, aborting\\n\",\n\t\t\t dev->caps.num_ports, MLX4_MAX_PORTS);\n\t\terr = -ENODEV;\n\t\tgoto free_mem;\n\t}\n\n\tmlx4_replace_zero_macs(dev);\n\n\terr = mlx4_slave_special_qp_cap(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Set special QP caps failed. aborting\\n\");\n\t\tgoto free_mem;\n\t}\n\n\tif (dev->caps.uar_page_size * (dev->caps.num_uars -\n\t\t\t\t       dev->caps.reserved_uars) >\n\t\t\t\t       pci_resource_len(dev->persist->pdev,\n\t\t\t\t\t\t\t2)) {\n\t\tmlx4_err(dev, \"HCA reported UAR region size of 0x%x bigger than PCI resource 2 size of 0x%llx, aborting\\n\",\n\t\t\t dev->caps.uar_page_size * dev->caps.num_uars,\n\t\t\t (unsigned long long)\n\t\t\t pci_resource_len(dev->persist->pdev, 2));\n\t\terr = -ENOMEM;\n\t\tgoto err_mem;\n\t}\n\n\tif (hca_param->dev_cap_enabled & MLX4_DEV_CAP_64B_EQE_ENABLED) {\n\t\tdev->caps.eqe_size   = 64;\n\t\tdev->caps.eqe_factor = 1;\n\t} else {\n\t\tdev->caps.eqe_size   = 32;\n\t\tdev->caps.eqe_factor = 0;\n\t}\n\n\tif (hca_param->dev_cap_enabled & MLX4_DEV_CAP_64B_CQE_ENABLED) {\n\t\tdev->caps.cqe_size   = 64;\n\t\tdev->caps.userspace_caps |= MLX4_USER_DEV_CAP_LARGE_CQE;\n\t} else {\n\t\tdev->caps.cqe_size   = 32;\n\t}\n\n\tif (hca_param->dev_cap_enabled & MLX4_DEV_CAP_EQE_STRIDE_ENABLED) {\n\t\tdev->caps.eqe_size = hca_param->eqe_size;\n\t\tdev->caps.eqe_factor = 0;\n\t}\n\n\tif (hca_param->dev_cap_enabled & MLX4_DEV_CAP_CQE_STRIDE_ENABLED) {\n\t\tdev->caps.cqe_size = hca_param->cqe_size;\n\t\t \n\t\tdev->caps.userspace_caps |= MLX4_USER_DEV_CAP_LARGE_CQE;\n\t}\n\n\tdev->caps.flags2 &= ~MLX4_DEV_CAP_FLAG2_TS;\n\tmlx4_warn(dev, \"Timestamping is not supported in slave mode\\n\");\n\n\tdev->caps.flags2 &= ~MLX4_DEV_CAP_FLAG2_USER_MAC_EN;\n\tmlx4_dbg(dev, \"User MAC FW update is not supported in slave mode\\n\");\n\n\tslave_adjust_steering_mode(dev, dev_cap, hca_param);\n\tmlx4_dbg(dev, \"RSS support for IP fragments is %s\\n\",\n\t\t hca_param->rss_ip_frags ? \"on\" : \"off\");\n\n\tif (func_cap->extra_flags & MLX4_QUERY_FUNC_FLAGS_BF_RES_QP &&\n\t    dev->caps.bf_reg_size)\n\t\tdev->caps.alloc_res_qp_mask |= MLX4_RESERVE_ETH_BF_QP;\n\n\tif (func_cap->extra_flags & MLX4_QUERY_FUNC_FLAGS_A0_RES_QP)\n\t\tdev->caps.alloc_res_qp_mask |= MLX4_RESERVE_A0_QP;\n\nerr_mem:\n\tif (err)\n\t\tmlx4_slave_destroy_special_qp_cap(dev);\nfree_mem:\n\tkfree(hca_param);\n\tkfree(func_cap);\n\tkfree(dev_cap);\n\treturn err;\n}\n\n \nint mlx4_change_port_types(struct mlx4_dev *dev,\n\t\t\t   enum mlx4_port_type *port_types)\n{\n\tint err = 0;\n\tint change = 0;\n\tint port;\n\n\tfor (port = 0; port <  dev->caps.num_ports; port++) {\n\t\t \n\t\tif (port_types[port] != dev->caps.port_type[port + 1])\n\t\t\tchange = 1;\n\t}\n\tif (change) {\n\t\tmlx4_unregister_device(dev);\n\t\tfor (port = 1; port <= dev->caps.num_ports; port++) {\n\t\t\tmlx4_CLOSE_PORT(dev, port);\n\t\t\tdev->caps.port_type[port] = port_types[port - 1];\n\t\t\terr = mlx4_SET_PORT(dev, port, -1);\n\t\t\tif (err) {\n\t\t\t\tmlx4_err(dev, \"Failed to set port %d, aborting\\n\",\n\t\t\t\t\t port);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tmlx4_set_port_mask(dev);\n\t\terr = mlx4_register_device(dev);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"Failed to register device\\n\");\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\nstatic ssize_t show_port_type(struct device *dev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct mlx4_port_info *info = container_of(attr, struct mlx4_port_info,\n\t\t\t\t\t\t   port_attr);\n\tstruct mlx4_dev *mdev = info->dev;\n\tchar type[8];\n\n\tsprintf(type, \"%s\",\n\t\t(mdev->caps.port_type[info->port] == MLX4_PORT_TYPE_IB) ?\n\t\t\"ib\" : \"eth\");\n\tif (mdev->caps.possible_type[info->port] == MLX4_PORT_TYPE_AUTO)\n\t\tsprintf(buf, \"auto (%s)\\n\", type);\n\telse\n\t\tsprintf(buf, \"%s\\n\", type);\n\n\treturn strlen(buf);\n}\n\nstatic int __set_port_type(struct mlx4_port_info *info,\n\t\t\t   enum mlx4_port_type port_type)\n{\n\tstruct mlx4_dev *mdev = info->dev;\n\tstruct mlx4_priv *priv = mlx4_priv(mdev);\n\tenum mlx4_port_type types[MLX4_MAX_PORTS];\n\tenum mlx4_port_type new_types[MLX4_MAX_PORTS];\n\tint i;\n\tint err = 0;\n\n\tif ((port_type & mdev->caps.supported_type[info->port]) != port_type) {\n\t\tmlx4_err(mdev,\n\t\t\t \"Requested port type for port %d is not supported on this HCA\\n\",\n\t\t\t info->port);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmlx4_stop_sense(mdev);\n\tmutex_lock(&priv->port_mutex);\n\tinfo->tmp_type = port_type;\n\n\t \n\tmdev->caps.possible_type[info->port] = info->tmp_type;\n\n\tfor (i = 0; i < mdev->caps.num_ports; i++) {\n\t\ttypes[i] = priv->port[i+1].tmp_type ? priv->port[i+1].tmp_type :\n\t\t\t\t\tmdev->caps.possible_type[i+1];\n\t\tif (types[i] == MLX4_PORT_TYPE_AUTO)\n\t\t\ttypes[i] = mdev->caps.port_type[i+1];\n\t}\n\n\tif (!(mdev->caps.flags & MLX4_DEV_CAP_FLAG_DPDP) &&\n\t    !(mdev->caps.flags & MLX4_DEV_CAP_FLAG_SENSE_SUPPORT)) {\n\t\tfor (i = 1; i <= mdev->caps.num_ports; i++) {\n\t\t\tif (mdev->caps.possible_type[i] == MLX4_PORT_TYPE_AUTO) {\n\t\t\t\tmdev->caps.possible_type[i] = mdev->caps.port_type[i];\n\t\t\t\terr = -EOPNOTSUPP;\n\t\t\t}\n\t\t}\n\t}\n\tif (err) {\n\t\tmlx4_err(mdev, \"Auto sensing is not supported on this HCA. Set only 'eth' or 'ib' for both ports (should be the same)\\n\");\n\t\tgoto out;\n\t}\n\n\tmlx4_do_sense_ports(mdev, new_types, types);\n\n\terr = mlx4_check_port_params(mdev, new_types);\n\tif (err)\n\t\tgoto out;\n\n\t \n\tfor (i = 0; i < mdev->caps.num_ports; i++)\n\t\tpriv->port[i + 1].tmp_type = 0;\n\n\terr = mlx4_change_port_types(mdev, new_types);\n\nout:\n\tmlx4_start_sense(mdev);\n\tmutex_unlock(&priv->port_mutex);\n\n\treturn err;\n}\n\nstatic ssize_t set_port_type(struct device *dev,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\tstruct mlx4_port_info *info = container_of(attr, struct mlx4_port_info,\n\t\t\t\t\t\t   port_attr);\n\tstruct mlx4_dev *mdev = info->dev;\n\tenum mlx4_port_type port_type;\n\tstatic DEFINE_MUTEX(set_port_type_mutex);\n\tint err;\n\n\tmutex_lock(&set_port_type_mutex);\n\n\tif (!strcmp(buf, \"ib\\n\")) {\n\t\tport_type = MLX4_PORT_TYPE_IB;\n\t} else if (!strcmp(buf, \"eth\\n\")) {\n\t\tport_type = MLX4_PORT_TYPE_ETH;\n\t} else if (!strcmp(buf, \"auto\\n\")) {\n\t\tport_type = MLX4_PORT_TYPE_AUTO;\n\t} else {\n\t\tmlx4_err(mdev, \"%s is not supported port type\\n\", buf);\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\terr = __set_port_type(info, port_type);\n\nerr_out:\n\tmutex_unlock(&set_port_type_mutex);\n\n\treturn err ? err : count;\n}\n\nenum ibta_mtu {\n\tIB_MTU_256  = 1,\n\tIB_MTU_512  = 2,\n\tIB_MTU_1024 = 3,\n\tIB_MTU_2048 = 4,\n\tIB_MTU_4096 = 5\n};\n\nstatic inline int int_to_ibta_mtu(int mtu)\n{\n\tswitch (mtu) {\n\tcase 256:  return IB_MTU_256;\n\tcase 512:  return IB_MTU_512;\n\tcase 1024: return IB_MTU_1024;\n\tcase 2048: return IB_MTU_2048;\n\tcase 4096: return IB_MTU_4096;\n\tdefault: return -1;\n\t}\n}\n\nstatic inline int ibta_mtu_to_int(enum ibta_mtu mtu)\n{\n\tswitch (mtu) {\n\tcase IB_MTU_256:  return  256;\n\tcase IB_MTU_512:  return  512;\n\tcase IB_MTU_1024: return 1024;\n\tcase IB_MTU_2048: return 2048;\n\tcase IB_MTU_4096: return 4096;\n\tdefault: return -1;\n\t}\n}\n\nstatic ssize_t show_port_ib_mtu(struct device *dev,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     char *buf)\n{\n\tstruct mlx4_port_info *info = container_of(attr, struct mlx4_port_info,\n\t\t\t\t\t\t   port_mtu_attr);\n\tstruct mlx4_dev *mdev = info->dev;\n\n\tif (mdev->caps.port_type[info->port] == MLX4_PORT_TYPE_ETH)\n\t\tmlx4_warn(mdev, \"port level mtu is only used for IB ports\\n\");\n\n\tsprintf(buf, \"%d\\n\",\n\t\t\tibta_mtu_to_int(mdev->caps.port_ib_mtu[info->port]));\n\treturn strlen(buf);\n}\n\nstatic ssize_t set_port_ib_mtu(struct device *dev,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\tstruct mlx4_port_info *info = container_of(attr, struct mlx4_port_info,\n\t\t\t\t\t\t   port_mtu_attr);\n\tstruct mlx4_dev *mdev = info->dev;\n\tstruct mlx4_priv *priv = mlx4_priv(mdev);\n\tint err, port, mtu, ibta_mtu = -1;\n\n\tif (mdev->caps.port_type[info->port] == MLX4_PORT_TYPE_ETH) {\n\t\tmlx4_warn(mdev, \"port level mtu is only used for IB ports\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = kstrtoint(buf, 0, &mtu);\n\tif (!err)\n\t\tibta_mtu = int_to_ibta_mtu(mtu);\n\n\tif (err || ibta_mtu < 0) {\n\t\tmlx4_err(mdev, \"%s is invalid IBTA mtu\\n\", buf);\n\t\treturn -EINVAL;\n\t}\n\n\tmdev->caps.port_ib_mtu[info->port] = ibta_mtu;\n\n\tmlx4_stop_sense(mdev);\n\tmutex_lock(&priv->port_mutex);\n\tmlx4_unregister_device(mdev);\n\tfor (port = 1; port <= mdev->caps.num_ports; port++) {\n\t\tmlx4_CLOSE_PORT(mdev, port);\n\t\terr = mlx4_SET_PORT(mdev, port, -1);\n\t\tif (err) {\n\t\t\tmlx4_err(mdev, \"Failed to set port %d, aborting\\n\",\n\t\t\t\t port);\n\t\t\tgoto err_set_port;\n\t\t}\n\t}\n\terr = mlx4_register_device(mdev);\nerr_set_port:\n\tmutex_unlock(&priv->port_mutex);\n\tmlx4_start_sense(mdev);\n\treturn err ? err : count;\n}\n\n \n#define MAX_MF_BOND_ALLOWED_SLAVES 63\nstatic int mlx4_mf_bond(struct mlx4_dev *dev)\n{\n\tint err = 0;\n\tint nvfs;\n\tstruct mlx4_slaves_pport slaves_port1;\n\tstruct mlx4_slaves_pport slaves_port2;\n\n\tslaves_port1 = mlx4_phys_to_slaves_pport(dev, 1);\n\tslaves_port2 = mlx4_phys_to_slaves_pport(dev, 2);\n\n\t \n\tif (bitmap_weight_and(slaves_port1.slaves, slaves_port2.slaves,\n\t\t\t      dev->persist->num_vfs + 1) > 1) {\n\t\tmlx4_warn(dev, \"HA mode unsupported for dual ported VFs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tnvfs = bitmap_weight(slaves_port1.slaves, dev->persist->num_vfs + 1) +\n\t\tbitmap_weight(slaves_port2.slaves, dev->persist->num_vfs + 1) - 2;\n\n\t \n\tif (nvfs > MAX_MF_BOND_ALLOWED_SLAVES) {\n\t\tmlx4_warn(dev, \"HA mode is not supported for %d VFs (max %d are allowed)\\n\",\n\t\t\t  nvfs, MAX_MF_BOND_ALLOWED_SLAVES);\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->caps.steering_mode != MLX4_STEERING_MODE_DEVICE_MANAGED) {\n\t\tmlx4_warn(dev, \"HA mode unsupported for NON DMFS steering\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = mlx4_bond_mac_table(dev);\n\tif (err)\n\t\treturn err;\n\terr = mlx4_bond_vlan_table(dev);\n\tif (err)\n\t\tgoto err1;\n\terr = mlx4_bond_fs_rules(dev);\n\tif (err)\n\t\tgoto err2;\n\n\treturn 0;\nerr2:\n\t(void)mlx4_unbond_vlan_table(dev);\nerr1:\n\t(void)mlx4_unbond_mac_table(dev);\n\treturn err;\n}\n\nstatic int mlx4_mf_unbond(struct mlx4_dev *dev)\n{\n\tint ret, ret1;\n\n\tret = mlx4_unbond_fs_rules(dev);\n\tif (ret)\n\t\tmlx4_warn(dev, \"multifunction unbond for flow rules failed (%d)\\n\", ret);\n\tret1 = mlx4_unbond_mac_table(dev);\n\tif (ret1) {\n\t\tmlx4_warn(dev, \"multifunction unbond for MAC table failed (%d)\\n\", ret1);\n\t\tret = ret1;\n\t}\n\tret1 = mlx4_unbond_vlan_table(dev);\n\tif (ret1) {\n\t\tmlx4_warn(dev, \"multifunction unbond for VLAN table failed (%d)\\n\", ret1);\n\t\tret = ret1;\n\t}\n\treturn ret;\n}\n\nstatic int mlx4_bond(struct mlx4_dev *dev)\n{\n\tint ret = 0;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tmutex_lock(&priv->bond_mutex);\n\n\tif (!mlx4_is_bonded(dev)) {\n\t\tret = mlx4_do_bond(dev, true);\n\t\tif (ret)\n\t\t\tmlx4_err(dev, \"Failed to bond device: %d\\n\", ret);\n\t\tif (!ret && mlx4_is_master(dev)) {\n\t\t\tret = mlx4_mf_bond(dev);\n\t\t\tif (ret) {\n\t\t\t\tmlx4_err(dev, \"bond for multifunction failed\\n\");\n\t\t\t\tmlx4_do_bond(dev, false);\n\t\t\t}\n\t\t}\n\t}\n\n\tmutex_unlock(&priv->bond_mutex);\n\tif (!ret)\n\t\tmlx4_dbg(dev, \"Device is bonded\\n\");\n\n\treturn ret;\n}\n\nstatic int mlx4_unbond(struct mlx4_dev *dev)\n{\n\tint ret = 0;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tmutex_lock(&priv->bond_mutex);\n\n\tif (mlx4_is_bonded(dev)) {\n\t\tint ret2 = 0;\n\n\t\tret = mlx4_do_bond(dev, false);\n\t\tif (ret)\n\t\t\tmlx4_err(dev, \"Failed to unbond device: %d\\n\", ret);\n\t\tif (mlx4_is_master(dev))\n\t\t\tret2 = mlx4_mf_unbond(dev);\n\t\tif (ret2) {\n\t\t\tmlx4_warn(dev, \"Failed to unbond device for multifunction (%d)\\n\", ret2);\n\t\t\tret = ret2;\n\t\t}\n\t}\n\n\tmutex_unlock(&priv->bond_mutex);\n\tif (!ret)\n\t\tmlx4_dbg(dev, \"Device is unbonded\\n\");\n\n\treturn ret;\n}\n\nstatic int mlx4_port_map_set(struct mlx4_dev *dev, struct mlx4_port_map *v2p)\n{\n\tu8 port1 = v2p->port1;\n\tu8 port2 = v2p->port2;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint err;\n\n\tif (!(dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_PORT_REMAP))\n\t\treturn -EOPNOTSUPP;\n\n\tmutex_lock(&priv->bond_mutex);\n\n\t \n\tif (port1 == 0)\n\t\tport1 = priv->v2p.port1;\n\tif (port2 == 0)\n\t\tport2 = priv->v2p.port2;\n\n\tif ((port1 < 1) || (port1 > MLX4_MAX_PORTS) ||\n\t    (port2 < 1) || (port2 > MLX4_MAX_PORTS) ||\n\t    (port1 == 2 && port2 == 1)) {\n\t\t \n\t\terr = -EINVAL;\n\t} else if ((port1 == priv->v2p.port1) &&\n\t\t (port2 == priv->v2p.port2)) {\n\t\terr = 0;\n\t} else {\n\t\terr = mlx4_virt2phy_port_map(dev, port1, port2);\n\t\tif (!err) {\n\t\t\tmlx4_dbg(dev, \"port map changed: [%d][%d]\\n\",\n\t\t\t\t port1, port2);\n\t\t\tpriv->v2p.port1 = port1;\n\t\t\tpriv->v2p.port2 = port2;\n\t\t} else {\n\t\t\tmlx4_err(dev, \"Failed to change port mape: %d\\n\", err);\n\t\t}\n\t}\n\n\tmutex_unlock(&priv->bond_mutex);\n\treturn err;\n}\n\nstruct mlx4_bond {\n\tstruct work_struct work;\n\tstruct mlx4_dev *dev;\n\tint is_bonded;\n\tstruct mlx4_port_map port_map;\n};\n\nstatic void mlx4_bond_work(struct work_struct *work)\n{\n\tstruct mlx4_bond *bond = container_of(work, struct mlx4_bond, work);\n\tint err = 0;\n\n\tif (bond->is_bonded) {\n\t\tif (!mlx4_is_bonded(bond->dev)) {\n\t\t\terr = mlx4_bond(bond->dev);\n\t\t\tif (err)\n\t\t\t\tmlx4_err(bond->dev, \"Fail to bond device\\n\");\n\t\t}\n\t\tif (!err) {\n\t\t\terr = mlx4_port_map_set(bond->dev, &bond->port_map);\n\t\t\tif (err)\n\t\t\t\tmlx4_err(bond->dev,\n\t\t\t\t\t \"Fail to set port map [%d][%d]: %d\\n\",\n\t\t\t\t\t bond->port_map.port1,\n\t\t\t\t\t bond->port_map.port2, err);\n\t\t}\n\t} else if (mlx4_is_bonded(bond->dev)) {\n\t\terr = mlx4_unbond(bond->dev);\n\t\tif (err)\n\t\t\tmlx4_err(bond->dev, \"Fail to unbond device\\n\");\n\t}\n\tput_device(&bond->dev->persist->pdev->dev);\n\tkfree(bond);\n}\n\nint mlx4_queue_bond_work(struct mlx4_dev *dev, int is_bonded, u8 v2p_p1,\n\t\t\t u8 v2p_p2)\n{\n\tstruct mlx4_bond *bond;\n\n\tbond = kzalloc(sizeof(*bond), GFP_ATOMIC);\n\tif (!bond)\n\t\treturn -ENOMEM;\n\n\tINIT_WORK(&bond->work, mlx4_bond_work);\n\tget_device(&dev->persist->pdev->dev);\n\tbond->dev = dev;\n\tbond->is_bonded = is_bonded;\n\tbond->port_map.port1 = v2p_p1;\n\tbond->port_map.port2 = v2p_p2;\n\tqueue_work(mlx4_wq, &bond->work);\n\treturn 0;\n}\nEXPORT_SYMBOL(mlx4_queue_bond_work);\n\nstatic int mlx4_load_fw(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint err;\n\n\tpriv->fw.fw_icm = mlx4_alloc_icm(dev, priv->fw.fw_pages,\n\t\t\t\t\t GFP_HIGHUSER | __GFP_NOWARN, 0);\n\tif (!priv->fw.fw_icm) {\n\t\tmlx4_err(dev, \"Couldn't allocate FW area, aborting\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\terr = mlx4_MAP_FA(dev, priv->fw.fw_icm);\n\tif (err) {\n\t\tmlx4_err(dev, \"MAP_FA command failed, aborting\\n\");\n\t\tgoto err_free;\n\t}\n\n\terr = mlx4_RUN_FW(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"RUN_FW command failed, aborting\\n\");\n\t\tgoto err_unmap_fa;\n\t}\n\n\treturn 0;\n\nerr_unmap_fa:\n\tmlx4_UNMAP_FA(dev);\n\nerr_free:\n\tmlx4_free_icm(dev, priv->fw.fw_icm, 0);\n\treturn err;\n}\n\nstatic int mlx4_init_cmpt_table(struct mlx4_dev *dev, u64 cmpt_base,\n\t\t\t\tint cmpt_entry_sz)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint err;\n\tint num_eqs;\n\n\terr = mlx4_init_icm_table(dev, &priv->qp_table.cmpt_table,\n\t\t\t\t  cmpt_base +\n\t\t\t\t  ((u64) (MLX4_CMPT_TYPE_QP *\n\t\t\t\t\t  cmpt_entry_sz) << MLX4_CMPT_SHIFT),\n\t\t\t\t  cmpt_entry_sz, dev->caps.num_qps,\n\t\t\t\t  dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW],\n\t\t\t\t  0, 0);\n\tif (err)\n\t\tgoto err;\n\n\terr = mlx4_init_icm_table(dev, &priv->srq_table.cmpt_table,\n\t\t\t\t  cmpt_base +\n\t\t\t\t  ((u64) (MLX4_CMPT_TYPE_SRQ *\n\t\t\t\t\t  cmpt_entry_sz) << MLX4_CMPT_SHIFT),\n\t\t\t\t  cmpt_entry_sz, dev->caps.num_srqs,\n\t\t\t\t  dev->caps.reserved_srqs, 0, 0);\n\tif (err)\n\t\tgoto err_qp;\n\n\terr = mlx4_init_icm_table(dev, &priv->cq_table.cmpt_table,\n\t\t\t\t  cmpt_base +\n\t\t\t\t  ((u64) (MLX4_CMPT_TYPE_CQ *\n\t\t\t\t\t  cmpt_entry_sz) << MLX4_CMPT_SHIFT),\n\t\t\t\t  cmpt_entry_sz, dev->caps.num_cqs,\n\t\t\t\t  dev->caps.reserved_cqs, 0, 0);\n\tif (err)\n\t\tgoto err_srq;\n\n\tnum_eqs = dev->phys_caps.num_phys_eqs;\n\terr = mlx4_init_icm_table(dev, &priv->eq_table.cmpt_table,\n\t\t\t\t  cmpt_base +\n\t\t\t\t  ((u64) (MLX4_CMPT_TYPE_EQ *\n\t\t\t\t\t  cmpt_entry_sz) << MLX4_CMPT_SHIFT),\n\t\t\t\t  cmpt_entry_sz, num_eqs, num_eqs, 0, 0);\n\tif (err)\n\t\tgoto err_cq;\n\n\treturn 0;\n\nerr_cq:\n\tmlx4_cleanup_icm_table(dev, &priv->cq_table.cmpt_table);\n\nerr_srq:\n\tmlx4_cleanup_icm_table(dev, &priv->srq_table.cmpt_table);\n\nerr_qp:\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.cmpt_table);\n\nerr:\n\treturn err;\n}\n\nstatic int mlx4_init_icm(struct mlx4_dev *dev, struct mlx4_dev_cap *dev_cap,\n\t\t\t struct mlx4_init_hca_param *init_hca, u64 icm_size)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tu64 aux_pages;\n\tint num_eqs;\n\tint err;\n\n\terr = mlx4_SET_ICM_SIZE(dev, icm_size, &aux_pages);\n\tif (err) {\n\t\tmlx4_err(dev, \"SET_ICM_SIZE command failed, aborting\\n\");\n\t\treturn err;\n\t}\n\n\tmlx4_dbg(dev, \"%lld KB of HCA context requires %lld KB aux memory\\n\",\n\t\t (unsigned long long) icm_size >> 10,\n\t\t (unsigned long long) aux_pages << 2);\n\n\tpriv->fw.aux_icm = mlx4_alloc_icm(dev, aux_pages,\n\t\t\t\t\t  GFP_HIGHUSER | __GFP_NOWARN, 0);\n\tif (!priv->fw.aux_icm) {\n\t\tmlx4_err(dev, \"Couldn't allocate aux memory, aborting\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\terr = mlx4_MAP_ICM_AUX(dev, priv->fw.aux_icm);\n\tif (err) {\n\t\tmlx4_err(dev, \"MAP_ICM_AUX command failed, aborting\\n\");\n\t\tgoto err_free_aux;\n\t}\n\n\terr = mlx4_init_cmpt_table(dev, init_hca->cmpt_base, dev_cap->cmpt_entry_sz);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map cMPT context memory, aborting\\n\");\n\t\tgoto err_unmap_aux;\n\t}\n\n\n\tnum_eqs = dev->phys_caps.num_phys_eqs;\n\terr = mlx4_init_icm_table(dev, &priv->eq_table.table,\n\t\t\t\t  init_hca->eqc_base, dev_cap->eqc_entry_sz,\n\t\t\t\t  num_eqs, num_eqs, 0, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map EQ context memory, aborting\\n\");\n\t\tgoto err_unmap_cmpt;\n\t}\n\n\t \n\tdev->caps.reserved_mtts =\n\t\tALIGN(dev->caps.reserved_mtts * dev->caps.mtt_entry_sz,\n\t\t      dma_get_cache_alignment()) / dev->caps.mtt_entry_sz;\n\n\terr = mlx4_init_icm_table(dev, &priv->mr_table.mtt_table,\n\t\t\t\t  init_hca->mtt_base,\n\t\t\t\t  dev->caps.mtt_entry_sz,\n\t\t\t\t  dev->caps.num_mtts,\n\t\t\t\t  dev->caps.reserved_mtts, 1, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map MTT context memory, aborting\\n\");\n\t\tgoto err_unmap_eq;\n\t}\n\n\terr = mlx4_init_icm_table(dev, &priv->mr_table.dmpt_table,\n\t\t\t\t  init_hca->dmpt_base,\n\t\t\t\t  dev_cap->dmpt_entry_sz,\n\t\t\t\t  dev->caps.num_mpts,\n\t\t\t\t  dev->caps.reserved_mrws, 1, 1);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map dMPT context memory, aborting\\n\");\n\t\tgoto err_unmap_mtt;\n\t}\n\n\terr = mlx4_init_icm_table(dev, &priv->qp_table.qp_table,\n\t\t\t\t  init_hca->qpc_base,\n\t\t\t\t  dev_cap->qpc_entry_sz,\n\t\t\t\t  dev->caps.num_qps,\n\t\t\t\t  dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW],\n\t\t\t\t  0, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map QP context memory, aborting\\n\");\n\t\tgoto err_unmap_dmpt;\n\t}\n\n\terr = mlx4_init_icm_table(dev, &priv->qp_table.auxc_table,\n\t\t\t\t  init_hca->auxc_base,\n\t\t\t\t  dev_cap->aux_entry_sz,\n\t\t\t\t  dev->caps.num_qps,\n\t\t\t\t  dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW],\n\t\t\t\t  0, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map AUXC context memory, aborting\\n\");\n\t\tgoto err_unmap_qp;\n\t}\n\n\terr = mlx4_init_icm_table(dev, &priv->qp_table.altc_table,\n\t\t\t\t  init_hca->altc_base,\n\t\t\t\t  dev_cap->altc_entry_sz,\n\t\t\t\t  dev->caps.num_qps,\n\t\t\t\t  dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW],\n\t\t\t\t  0, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map ALTC context memory, aborting\\n\");\n\t\tgoto err_unmap_auxc;\n\t}\n\n\terr = mlx4_init_icm_table(dev, &priv->qp_table.rdmarc_table,\n\t\t\t\t  init_hca->rdmarc_base,\n\t\t\t\t  dev_cap->rdmarc_entry_sz << priv->qp_table.rdmarc_shift,\n\t\t\t\t  dev->caps.num_qps,\n\t\t\t\t  dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW],\n\t\t\t\t  0, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map RDMARC context memory, aborting\\n\");\n\t\tgoto err_unmap_altc;\n\t}\n\n\terr = mlx4_init_icm_table(dev, &priv->cq_table.table,\n\t\t\t\t  init_hca->cqc_base,\n\t\t\t\t  dev_cap->cqc_entry_sz,\n\t\t\t\t  dev->caps.num_cqs,\n\t\t\t\t  dev->caps.reserved_cqs, 0, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map CQ context memory, aborting\\n\");\n\t\tgoto err_unmap_rdmarc;\n\t}\n\n\terr = mlx4_init_icm_table(dev, &priv->srq_table.table,\n\t\t\t\t  init_hca->srqc_base,\n\t\t\t\t  dev_cap->srq_entry_sz,\n\t\t\t\t  dev->caps.num_srqs,\n\t\t\t\t  dev->caps.reserved_srqs, 0, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map SRQ context memory, aborting\\n\");\n\t\tgoto err_unmap_cq;\n\t}\n\n\t \n\terr = mlx4_init_icm_table(dev, &priv->mcg_table.table,\n\t\t\t\t  init_hca->mc_base,\n\t\t\t\t  mlx4_get_mgm_entry_size(dev),\n\t\t\t\t  dev->caps.num_mgms + dev->caps.num_amgms,\n\t\t\t\t  dev->caps.num_mgms + dev->caps.num_amgms,\n\t\t\t\t  0, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to map MCG context memory, aborting\\n\");\n\t\tgoto err_unmap_srq;\n\t}\n\n\treturn 0;\n\nerr_unmap_srq:\n\tmlx4_cleanup_icm_table(dev, &priv->srq_table.table);\n\nerr_unmap_cq:\n\tmlx4_cleanup_icm_table(dev, &priv->cq_table.table);\n\nerr_unmap_rdmarc:\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.rdmarc_table);\n\nerr_unmap_altc:\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.altc_table);\n\nerr_unmap_auxc:\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.auxc_table);\n\nerr_unmap_qp:\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.qp_table);\n\nerr_unmap_dmpt:\n\tmlx4_cleanup_icm_table(dev, &priv->mr_table.dmpt_table);\n\nerr_unmap_mtt:\n\tmlx4_cleanup_icm_table(dev, &priv->mr_table.mtt_table);\n\nerr_unmap_eq:\n\tmlx4_cleanup_icm_table(dev, &priv->eq_table.table);\n\nerr_unmap_cmpt:\n\tmlx4_cleanup_icm_table(dev, &priv->eq_table.cmpt_table);\n\tmlx4_cleanup_icm_table(dev, &priv->cq_table.cmpt_table);\n\tmlx4_cleanup_icm_table(dev, &priv->srq_table.cmpt_table);\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.cmpt_table);\n\nerr_unmap_aux:\n\tmlx4_UNMAP_ICM_AUX(dev);\n\nerr_free_aux:\n\tmlx4_free_icm(dev, priv->fw.aux_icm, 0);\n\n\treturn err;\n}\n\nstatic void mlx4_free_icms(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tmlx4_cleanup_icm_table(dev, &priv->mcg_table.table);\n\tmlx4_cleanup_icm_table(dev, &priv->srq_table.table);\n\tmlx4_cleanup_icm_table(dev, &priv->cq_table.table);\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.rdmarc_table);\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.altc_table);\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.auxc_table);\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.qp_table);\n\tmlx4_cleanup_icm_table(dev, &priv->mr_table.dmpt_table);\n\tmlx4_cleanup_icm_table(dev, &priv->mr_table.mtt_table);\n\tmlx4_cleanup_icm_table(dev, &priv->eq_table.table);\n\tmlx4_cleanup_icm_table(dev, &priv->eq_table.cmpt_table);\n\tmlx4_cleanup_icm_table(dev, &priv->cq_table.cmpt_table);\n\tmlx4_cleanup_icm_table(dev, &priv->srq_table.cmpt_table);\n\tmlx4_cleanup_icm_table(dev, &priv->qp_table.cmpt_table);\n\n\tmlx4_UNMAP_ICM_AUX(dev);\n\tmlx4_free_icm(dev, priv->fw.aux_icm, 0);\n}\n\nstatic void mlx4_slave_exit(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tmutex_lock(&priv->cmd.slave_cmd_mutex);\n\tif (mlx4_comm_cmd(dev, MLX4_COMM_CMD_RESET, 0, MLX4_COMM_CMD_NA_OP,\n\t\t\t  MLX4_COMM_TIME))\n\t\tmlx4_warn(dev, \"Failed to close slave function\\n\");\n\tmutex_unlock(&priv->cmd.slave_cmd_mutex);\n}\n\nstatic int map_bf_area(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tresource_size_t bf_start;\n\tresource_size_t bf_len;\n\tint err = 0;\n\n\tif (!dev->caps.bf_reg_size)\n\t\treturn -ENXIO;\n\n\tbf_start = pci_resource_start(dev->persist->pdev, 2) +\n\t\t\t(dev->caps.num_uars << PAGE_SHIFT);\n\tbf_len = pci_resource_len(dev->persist->pdev, 2) -\n\t\t\t(dev->caps.num_uars << PAGE_SHIFT);\n\tpriv->bf_mapping = io_mapping_create_wc(bf_start, bf_len);\n\tif (!priv->bf_mapping)\n\t\terr = -ENOMEM;\n\n\treturn err;\n}\n\nstatic void unmap_bf_area(struct mlx4_dev *dev)\n{\n\tif (mlx4_priv(dev)->bf_mapping)\n\t\tio_mapping_free(mlx4_priv(dev)->bf_mapping);\n}\n\nu64 mlx4_read_clock(struct mlx4_dev *dev)\n{\n\tu32 clockhi, clocklo, clockhi1;\n\tu64 cycles;\n\tint i;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tfor (i = 0; i < 10; i++) {\n\t\tclockhi = swab32(readl(priv->clock_mapping));\n\t\tclocklo = swab32(readl(priv->clock_mapping + 4));\n\t\tclockhi1 = swab32(readl(priv->clock_mapping));\n\t\tif (clockhi == clockhi1)\n\t\t\tbreak;\n\t}\n\n\tcycles = (u64) clockhi << 32 | (u64) clocklo;\n\n\treturn cycles;\n}\nEXPORT_SYMBOL_GPL(mlx4_read_clock);\n\n\nstatic int map_internal_clock(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tpriv->clock_mapping =\n\t\tioremap(pci_resource_start(dev->persist->pdev,\n\t\t\t\t\t   priv->fw.clock_bar) +\n\t\t\tpriv->fw.clock_offset, MLX4_CLOCK_SIZE);\n\n\tif (!priv->clock_mapping)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nint mlx4_get_internal_clock_params(struct mlx4_dev *dev,\n\t\t\t\t   struct mlx4_clock_params *params)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tif (mlx4_is_slave(dev))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!dev->caps.map_clock_to_user) {\n\t\tmlx4_dbg(dev, \"Map clock to user is not supported.\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!params)\n\t\treturn -EINVAL;\n\n\tparams->bar = priv->fw.clock_bar;\n\tparams->offset = priv->fw.clock_offset;\n\tparams->size = MLX4_CLOCK_SIZE;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mlx4_get_internal_clock_params);\n\nstatic void unmap_internal_clock(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tif (priv->clock_mapping)\n\t\tiounmap(priv->clock_mapping);\n}\n\nstatic void mlx4_close_hca(struct mlx4_dev *dev)\n{\n\tunmap_internal_clock(dev);\n\tunmap_bf_area(dev);\n\tif (mlx4_is_slave(dev))\n\t\tmlx4_slave_exit(dev);\n\telse {\n\t\tmlx4_CLOSE_HCA(dev, 0);\n\t\tmlx4_free_icms(dev);\n\t}\n}\n\nstatic void mlx4_close_fw(struct mlx4_dev *dev)\n{\n\tif (!mlx4_is_slave(dev)) {\n\t\tmlx4_UNMAP_FA(dev);\n\t\tmlx4_free_icm(dev, mlx4_priv(dev)->fw.fw_icm, 0);\n\t}\n}\n\nstatic int mlx4_comm_check_offline(struct mlx4_dev *dev)\n{\n#define COMM_CHAN_OFFLINE_OFFSET 0x09\n\n\tu32 comm_flags;\n\tu32 offline_bit;\n\tunsigned long end;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tend = msecs_to_jiffies(MLX4_COMM_OFFLINE_TIME_OUT) + jiffies;\n\twhile (time_before(jiffies, end)) {\n\t\tcomm_flags = swab32(readl((__iomem char *)priv->mfunc.comm +\n\t\t\t\t\t  MLX4_COMM_CHAN_FLAGS));\n\t\toffline_bit = (comm_flags &\n\t\t\t       (u32)(1 << COMM_CHAN_OFFLINE_OFFSET));\n\t\tif (!offline_bit)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (dev->persist->interface_state &\n\t\t    MLX4_INTERFACE_STATE_NOWAIT)\n\t\t\tbreak;\n\n\t\t \n\t\tmsleep(100);\n\t}\n\tmlx4_err(dev, \"Communication channel is offline.\\n\");\n\treturn -EIO;\n}\n\nstatic void mlx4_reset_vf_support(struct mlx4_dev *dev)\n{\n#define COMM_CHAN_RST_OFFSET 0x1e\n\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tu32 comm_rst;\n\tu32 comm_caps;\n\n\tcomm_caps = swab32(readl((__iomem char *)priv->mfunc.comm +\n\t\t\t\t MLX4_COMM_CHAN_CAPS));\n\tcomm_rst = (comm_caps & (u32)(1 << COMM_CHAN_RST_OFFSET));\n\n\tif (comm_rst)\n\t\tdev->caps.vf_caps |= MLX4_VF_CAP_FLAG_RESET;\n}\n\nstatic int mlx4_init_slave(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tu64 dma = (u64) priv->mfunc.vhcr_dma;\n\tint ret_from_reset = 0;\n\tu32 slave_read;\n\tu32 cmd_channel_ver;\n\n\tif (atomic_read(&pf_loading)) {\n\t\tmlx4_warn(dev, \"PF is not ready - Deferring probe\\n\");\n\t\treturn -EPROBE_DEFER;\n\t}\n\n\tmutex_lock(&priv->cmd.slave_cmd_mutex);\n\tpriv->cmd.max_cmds = 1;\n\tif (mlx4_comm_check_offline(dev)) {\n\t\tmlx4_err(dev, \"PF is not responsive, skipping initialization\\n\");\n\t\tgoto err_offline;\n\t}\n\n\tmlx4_reset_vf_support(dev);\n\tmlx4_warn(dev, \"Sending reset\\n\");\n\tret_from_reset = mlx4_comm_cmd(dev, MLX4_COMM_CMD_RESET, 0,\n\t\t\t\t       MLX4_COMM_CMD_NA_OP, MLX4_COMM_TIME);\n\t \n\tif (ret_from_reset) {\n\t\tif (MLX4_DELAY_RESET_SLAVE == ret_from_reset) {\n\t\t\tmlx4_warn(dev, \"slave is currently in the middle of FLR - Deferring probe\\n\");\n\t\t\tmutex_unlock(&priv->cmd.slave_cmd_mutex);\n\t\t\treturn -EPROBE_DEFER;\n\t\t} else\n\t\t\tgoto err;\n\t}\n\n\t \n\tslave_read = swab32(readl(&priv->mfunc.comm->slave_read));\n\tcmd_channel_ver = mlx4_comm_get_version();\n\n\tif (MLX4_COMM_GET_IF_REV(cmd_channel_ver) !=\n\t\tMLX4_COMM_GET_IF_REV(slave_read)) {\n\t\tmlx4_err(dev, \"slave driver version is not supported by the master\\n\");\n\t\tgoto err;\n\t}\n\n\tmlx4_warn(dev, \"Sending vhcr0\\n\");\n\tif (mlx4_comm_cmd(dev, MLX4_COMM_CMD_VHCR0, dma >> 48,\n\t\t\t     MLX4_COMM_CMD_NA_OP, MLX4_COMM_TIME))\n\t\tgoto err;\n\tif (mlx4_comm_cmd(dev, MLX4_COMM_CMD_VHCR1, dma >> 32,\n\t\t\t     MLX4_COMM_CMD_NA_OP, MLX4_COMM_TIME))\n\t\tgoto err;\n\tif (mlx4_comm_cmd(dev, MLX4_COMM_CMD_VHCR2, dma >> 16,\n\t\t\t     MLX4_COMM_CMD_NA_OP, MLX4_COMM_TIME))\n\t\tgoto err;\n\tif (mlx4_comm_cmd(dev, MLX4_COMM_CMD_VHCR_EN, dma,\n\t\t\t  MLX4_COMM_CMD_NA_OP, MLX4_COMM_TIME))\n\t\tgoto err;\n\n\tmutex_unlock(&priv->cmd.slave_cmd_mutex);\n\treturn 0;\n\nerr:\n\tmlx4_comm_cmd(dev, MLX4_COMM_CMD_RESET, 0, MLX4_COMM_CMD_NA_OP, 0);\nerr_offline:\n\tmutex_unlock(&priv->cmd.slave_cmd_mutex);\n\treturn -EIO;\n}\n\nstatic void mlx4_parav_master_pf_caps(struct mlx4_dev *dev)\n{\n\tint i;\n\n\tfor (i = 1; i <= dev->caps.num_ports; i++) {\n\t\tif (dev->caps.port_type[i] == MLX4_PORT_TYPE_ETH)\n\t\t\tdev->caps.gid_table_len[i] =\n\t\t\t\tmlx4_get_slave_num_gids(dev, 0, i);\n\t\telse\n\t\t\tdev->caps.gid_table_len[i] = 1;\n\t\tdev->caps.pkey_table_len[i] =\n\t\t\tdev->phys_caps.pkey_phys_table_len[i] - 1;\n\t}\n}\n\nstatic int choose_log_fs_mgm_entry_size(int qp_per_entry)\n{\n\tint i = MLX4_MIN_MGM_LOG_ENTRY_SIZE;\n\n\tfor (i = MLX4_MIN_MGM_LOG_ENTRY_SIZE; i <= MLX4_MAX_MGM_LOG_ENTRY_SIZE;\n\t      i++) {\n\t\tif (qp_per_entry <= 4 * ((1 << i) / 16 - 2))\n\t\t\tbreak;\n\t}\n\n\treturn (i <= MLX4_MAX_MGM_LOG_ENTRY_SIZE) ? i : -1;\n}\n\nstatic const char *dmfs_high_rate_steering_mode_str(int dmfs_high_steer_mode)\n{\n\tswitch (dmfs_high_steer_mode) {\n\tcase MLX4_STEERING_DMFS_A0_DEFAULT:\n\t\treturn \"default performance\";\n\n\tcase MLX4_STEERING_DMFS_A0_DYNAMIC:\n\t\treturn \"dynamic hybrid mode\";\n\n\tcase MLX4_STEERING_DMFS_A0_STATIC:\n\t\treturn \"performance optimized for limited rule configuration (static)\";\n\n\tcase MLX4_STEERING_DMFS_A0_DISABLE:\n\t\treturn \"disabled performance optimized steering\";\n\n\tcase MLX4_STEERING_DMFS_A0_NOT_SUPPORTED:\n\t\treturn \"performance optimized steering not supported\";\n\n\tdefault:\n\t\treturn \"Unrecognized mode\";\n\t}\n}\n\n#define MLX4_DMFS_A0_STEERING\t\t\t(1UL << 2)\n\nstatic void choose_steering_mode(struct mlx4_dev *dev,\n\t\t\t\t struct mlx4_dev_cap *dev_cap)\n{\n\tif (mlx4_log_num_mgm_entry_size <= 0) {\n\t\tif ((-mlx4_log_num_mgm_entry_size) & MLX4_DMFS_A0_STEERING) {\n\t\t\tif (dev->caps.dmfs_high_steer_mode ==\n\t\t\t    MLX4_STEERING_DMFS_A0_NOT_SUPPORTED)\n\t\t\t\tmlx4_err(dev, \"DMFS high rate mode not supported\\n\");\n\t\t\telse\n\t\t\t\tdev->caps.dmfs_high_steer_mode =\n\t\t\t\t\tMLX4_STEERING_DMFS_A0_STATIC;\n\t\t}\n\t}\n\n\tif (mlx4_log_num_mgm_entry_size <= 0 &&\n\t    dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_FS_EN &&\n\t    (!mlx4_is_mfunc(dev) ||\n\t     (dev_cap->fs_max_num_qp_per_entry >=\n\t     (dev->persist->num_vfs + 1))) &&\n\t    choose_log_fs_mgm_entry_size(dev_cap->fs_max_num_qp_per_entry) >=\n\t\tMLX4_MIN_MGM_LOG_ENTRY_SIZE) {\n\t\tdev->oper_log_mgm_entry_size =\n\t\t\tchoose_log_fs_mgm_entry_size(dev_cap->fs_max_num_qp_per_entry);\n\t\tdev->caps.steering_mode = MLX4_STEERING_MODE_DEVICE_MANAGED;\n\t\tdev->caps.num_qp_per_mgm = dev_cap->fs_max_num_qp_per_entry;\n\t\tdev->caps.fs_log_max_ucast_qp_range_size =\n\t\t\tdev_cap->fs_log_max_ucast_qp_range_size;\n\t} else {\n\t\tif (dev->caps.dmfs_high_steer_mode !=\n\t\t    MLX4_STEERING_DMFS_A0_NOT_SUPPORTED)\n\t\t\tdev->caps.dmfs_high_steer_mode = MLX4_STEERING_DMFS_A0_DISABLE;\n\t\tif (dev->caps.flags & MLX4_DEV_CAP_FLAG_VEP_UC_STEER &&\n\t\t    dev->caps.flags & MLX4_DEV_CAP_FLAG_VEP_MC_STEER)\n\t\t\tdev->caps.steering_mode = MLX4_STEERING_MODE_B0;\n\t\telse {\n\t\t\tdev->caps.steering_mode = MLX4_STEERING_MODE_A0;\n\n\t\t\tif (dev->caps.flags & MLX4_DEV_CAP_FLAG_VEP_UC_STEER ||\n\t\t\t    dev->caps.flags & MLX4_DEV_CAP_FLAG_VEP_MC_STEER)\n\t\t\t\tmlx4_warn(dev, \"Must have both UC_STEER and MC_STEER flags set to use B0 steering - falling back to A0 steering mode\\n\");\n\t\t}\n\t\tdev->oper_log_mgm_entry_size =\n\t\t\tmlx4_log_num_mgm_entry_size > 0 ?\n\t\t\tmlx4_log_num_mgm_entry_size :\n\t\t\tMLX4_DEFAULT_MGM_LOG_ENTRY_SIZE;\n\t\tdev->caps.num_qp_per_mgm = mlx4_get_qp_per_mgm(dev);\n\t}\n\tmlx4_dbg(dev, \"Steering mode is: %s, oper_log_mgm_entry_size = %d, modparam log_num_mgm_entry_size = %d\\n\",\n\t\t mlx4_steering_mode_str(dev->caps.steering_mode),\n\t\t dev->oper_log_mgm_entry_size,\n\t\t mlx4_log_num_mgm_entry_size);\n}\n\nstatic void choose_tunnel_offload_mode(struct mlx4_dev *dev,\n\t\t\t\t       struct mlx4_dev_cap *dev_cap)\n{\n\tif (dev->caps.steering_mode == MLX4_STEERING_MODE_DEVICE_MANAGED &&\n\t    dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_VXLAN_OFFLOADS)\n\t\tdev->caps.tunnel_offload_mode = MLX4_TUNNEL_OFFLOAD_MODE_VXLAN;\n\telse\n\t\tdev->caps.tunnel_offload_mode = MLX4_TUNNEL_OFFLOAD_MODE_NONE;\n\n\tmlx4_dbg(dev, \"Tunneling offload mode is: %s\\n\",  (dev->caps.tunnel_offload_mode\n\t\t == MLX4_TUNNEL_OFFLOAD_MODE_VXLAN) ? \"vxlan\" : \"none\");\n}\n\nstatic int mlx4_validate_optimized_steering(struct mlx4_dev *dev)\n{\n\tint i;\n\tstruct mlx4_port_cap port_cap;\n\n\tif (dev->caps.dmfs_high_steer_mode == MLX4_STEERING_DMFS_A0_NOT_SUPPORTED)\n\t\treturn -EINVAL;\n\n\tfor (i = 1; i <= dev->caps.num_ports; i++) {\n\t\tif (mlx4_dev_port(dev, i, &port_cap)) {\n\t\t\tmlx4_err(dev,\n\t\t\t\t \"QUERY_DEV_CAP command failed, can't verify DMFS high rate steering.\\n\");\n\t\t} else if ((dev->caps.dmfs_high_steer_mode !=\n\t\t\t    MLX4_STEERING_DMFS_A0_DEFAULT) &&\n\t\t\t   (port_cap.dmfs_optimized_state ==\n\t\t\t    !!(dev->caps.dmfs_high_steer_mode ==\n\t\t\t    MLX4_STEERING_DMFS_A0_DISABLE))) {\n\t\t\tmlx4_err(dev,\n\t\t\t\t \"DMFS high rate steer mode differ, driver requested %s but %s in FW.\\n\",\n\t\t\t\t dmfs_high_rate_steering_mode_str(\n\t\t\t\t\tdev->caps.dmfs_high_steer_mode),\n\t\t\t\t (port_cap.dmfs_optimized_state ?\n\t\t\t\t\t\"enabled\" : \"disabled\"));\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int mlx4_init_fw(struct mlx4_dev *dev)\n{\n\tstruct mlx4_mod_stat_cfg   mlx4_cfg;\n\tint err = 0;\n\n\tif (!mlx4_is_slave(dev)) {\n\t\terr = mlx4_QUERY_FW(dev);\n\t\tif (err) {\n\t\t\tif (err == -EACCES)\n\t\t\t\tmlx4_info(dev, \"non-primary physical function, skipping\\n\");\n\t\t\telse\n\t\t\t\tmlx4_err(dev, \"QUERY_FW command failed, aborting\\n\");\n\t\t\treturn err;\n\t\t}\n\n\t\terr = mlx4_load_fw(dev);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"Failed to start FW, aborting\\n\");\n\t\t\treturn err;\n\t\t}\n\n\t\tmlx4_cfg.log_pg_sz_m = 1;\n\t\tmlx4_cfg.log_pg_sz = 0;\n\t\terr = mlx4_MOD_STAT_CFG(dev, &mlx4_cfg);\n\t\tif (err)\n\t\t\tmlx4_warn(dev, \"Failed to override log_pg_sz parameter\\n\");\n\t}\n\n\treturn err;\n}\n\nstatic int mlx4_init_hca(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv\t  *priv = mlx4_priv(dev);\n\tstruct mlx4_init_hca_param *init_hca = NULL;\n\tstruct mlx4_dev_cap\t  *dev_cap = NULL;\n\tstruct mlx4_adapter\t   adapter;\n\tstruct mlx4_profile\t   profile;\n\tu64 icm_size;\n\tstruct mlx4_config_dev_params params;\n\tint err;\n\n\tif (!mlx4_is_slave(dev)) {\n\t\tdev_cap = kzalloc(sizeof(*dev_cap), GFP_KERNEL);\n\t\tinit_hca = kzalloc(sizeof(*init_hca), GFP_KERNEL);\n\n\t\tif (!dev_cap || !init_hca) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\terr = mlx4_dev_cap(dev, dev_cap);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"QUERY_DEV_CAP command failed, aborting\\n\");\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tchoose_steering_mode(dev, dev_cap);\n\t\tchoose_tunnel_offload_mode(dev, dev_cap);\n\n\t\tif (dev->caps.dmfs_high_steer_mode == MLX4_STEERING_DMFS_A0_STATIC &&\n\t\t    mlx4_is_master(dev))\n\t\t\tdev->caps.function_caps |= MLX4_FUNC_CAP_DMFS_A0_STATIC;\n\n\t\terr = mlx4_get_phys_port_id(dev);\n\t\tif (err)\n\t\t\tmlx4_err(dev, \"Fail to get physical port id\\n\");\n\n\t\tif (mlx4_is_master(dev))\n\t\t\tmlx4_parav_master_pf_caps(dev);\n\n\t\tif (mlx4_low_memory_profile()) {\n\t\t\tmlx4_info(dev, \"Running from within kdump kernel. Using low memory profile\\n\");\n\t\t\tprofile = low_mem_profile;\n\t\t} else {\n\t\t\tprofile = default_profile;\n\t\t}\n\t\tif (dev->caps.steering_mode ==\n\t\t    MLX4_STEERING_MODE_DEVICE_MANAGED)\n\t\t\tprofile.num_mcg = MLX4_FS_NUM_MCG;\n\n\t\ticm_size = mlx4_make_profile(dev, &profile, dev_cap,\n\t\t\t\t\t     init_hca);\n\t\tif ((long long) icm_size < 0) {\n\t\t\terr = icm_size;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tif (enable_4k_uar || !dev->persist->num_vfs) {\n\t\t\tinit_hca->log_uar_sz = ilog2(dev->caps.num_uars) +\n\t\t\t\t\t\t    PAGE_SHIFT - DEFAULT_UAR_PAGE_SHIFT;\n\t\t\tinit_hca->uar_page_sz = DEFAULT_UAR_PAGE_SHIFT - 12;\n\t\t} else {\n\t\t\tinit_hca->log_uar_sz = ilog2(dev->caps.num_uars);\n\t\t\tinit_hca->uar_page_sz = PAGE_SHIFT - 12;\n\t\t}\n\n\t\tinit_hca->mw_enabled = 0;\n\t\tif (dev->caps.flags & MLX4_DEV_CAP_FLAG_MEM_WINDOW ||\n\t\t    dev->caps.bmme_flags & MLX4_BMME_FLAG_TYPE_2_WIN)\n\t\t\tinit_hca->mw_enabled = INIT_HCA_TPT_MW_ENABLE;\n\n\t\terr = mlx4_init_icm(dev, dev_cap, init_hca, icm_size);\n\t\tif (err)\n\t\t\tgoto out_free;\n\n\t\terr = mlx4_INIT_HCA(dev, init_hca);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"INIT_HCA command failed, aborting\\n\");\n\t\t\tgoto err_free_icm;\n\t\t}\n\n\t\tif (dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_SYS_EQS) {\n\t\t\terr = mlx4_query_func(dev, dev_cap);\n\t\t\tif (err < 0) {\n\t\t\t\tmlx4_err(dev, \"QUERY_FUNC command failed, aborting.\\n\");\n\t\t\t\tgoto err_close;\n\t\t\t} else if (err & MLX4_QUERY_FUNC_NUM_SYS_EQS) {\n\t\t\t\tdev->caps.num_eqs = dev_cap->max_eqs;\n\t\t\t\tdev->caps.reserved_eqs = dev_cap->reserved_eqs;\n\t\t\t\tdev->caps.reserved_uars = dev_cap->reserved_uars;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_TS) {\n\t\t\terr = mlx4_QUERY_HCA(dev, init_hca);\n\t\t\tif (err) {\n\t\t\t\tmlx4_err(dev, \"QUERY_HCA command failed, disable timestamp\\n\");\n\t\t\t\tdev->caps.flags2 &= ~MLX4_DEV_CAP_FLAG2_TS;\n\t\t\t} else {\n\t\t\t\tdev->caps.hca_core_clock =\n\t\t\t\t\tinit_hca->hca_core_clock;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!dev->caps.hca_core_clock) {\n\t\t\t\tdev->caps.flags2 &= ~MLX4_DEV_CAP_FLAG2_TS;\n\t\t\t\tmlx4_err(dev,\n\t\t\t\t\t \"HCA frequency is 0 - timestamping is not supported\\n\");\n\t\t\t} else if (map_internal_clock(dev)) {\n\t\t\t\t \n\t\t\t\tdev->caps.flags2 &= ~MLX4_DEV_CAP_FLAG2_TS;\n\t\t\t\tmlx4_err(dev, \"Failed to map internal clock. Timestamping is not supported\\n\");\n\t\t\t}\n\t\t}\n\n\t\tif (dev->caps.dmfs_high_steer_mode !=\n\t\t    MLX4_STEERING_DMFS_A0_NOT_SUPPORTED) {\n\t\t\tif (mlx4_validate_optimized_steering(dev))\n\t\t\t\tmlx4_warn(dev, \"Optimized steering validation failed\\n\");\n\n\t\t\tif (dev->caps.dmfs_high_steer_mode ==\n\t\t\t    MLX4_STEERING_DMFS_A0_DISABLE) {\n\t\t\t\tdev->caps.dmfs_high_rate_qpn_base =\n\t\t\t\t\tdev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW];\n\t\t\t\tdev->caps.dmfs_high_rate_qpn_range =\n\t\t\t\t\tMLX4_A0_STEERING_TABLE_SIZE;\n\t\t\t}\n\n\t\t\tmlx4_info(dev, \"DMFS high rate steer mode is: %s\\n\",\n\t\t\t\t  dmfs_high_rate_steering_mode_str(\n\t\t\t\t\tdev->caps.dmfs_high_steer_mode));\n\t\t}\n\t} else {\n\t\terr = mlx4_init_slave(dev);\n\t\tif (err) {\n\t\t\tif (err != -EPROBE_DEFER)\n\t\t\t\tmlx4_err(dev, \"Failed to initialize slave\\n\");\n\t\t\treturn err;\n\t\t}\n\n\t\terr = mlx4_slave_cap(dev);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"Failed to obtain slave caps\\n\");\n\t\t\tgoto err_close;\n\t\t}\n\t}\n\n\tif (map_bf_area(dev))\n\t\tmlx4_dbg(dev, \"Failed to map blue flame area\\n\");\n\n\t \n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_set_port_mask(dev);\n\n\terr = mlx4_QUERY_ADAPTER(dev, &adapter);\n\tif (err) {\n\t\tmlx4_err(dev, \"QUERY_ADAPTER command failed, aborting\\n\");\n\t\tgoto unmap_bf;\n\t}\n\n\t \n\terr = mlx4_config_dev_retrieval(dev, &params);\n\tif (err && err != -EOPNOTSUPP) {\n\t\tmlx4_err(dev, \"Failed to query CONFIG_DEV parameters\\n\");\n\t} else if (!err) {\n\t\tdev->caps.rx_checksum_flags_port[1] = params.rx_csum_flags_port_1;\n\t\tdev->caps.rx_checksum_flags_port[2] = params.rx_csum_flags_port_2;\n\t}\n\tpriv->eq_table.inta_pin = adapter.inta_pin;\n\tmemcpy(dev->board_id, adapter.board_id, sizeof(dev->board_id));\n\n\terr = 0;\n\tgoto out_free;\n\nunmap_bf:\n\tunmap_internal_clock(dev);\n\tunmap_bf_area(dev);\n\n\tif (mlx4_is_slave(dev))\n\t\tmlx4_slave_destroy_special_qp_cap(dev);\n\nerr_close:\n\tif (mlx4_is_slave(dev))\n\t\tmlx4_slave_exit(dev);\n\telse\n\t\tmlx4_CLOSE_HCA(dev, 0);\n\nerr_free_icm:\n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_free_icms(dev);\n\nout_free:\n\tkfree(dev_cap);\n\tkfree(init_hca);\n\n\treturn err;\n}\n\nstatic int mlx4_init_counters_table(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint nent_pow2;\n\n\tif (!(dev->caps.flags & MLX4_DEV_CAP_FLAG_COUNTERS))\n\t\treturn -ENOENT;\n\n\tif (!dev->caps.max_counters)\n\t\treturn -ENOSPC;\n\n\tnent_pow2 = roundup_pow_of_two(dev->caps.max_counters);\n\t \n\treturn mlx4_bitmap_init(&priv->counters_bitmap, nent_pow2,\n\t\t\t\tnent_pow2 - 1, 0,\n\t\t\t\tnent_pow2 - dev->caps.max_counters + 1);\n}\n\nstatic void mlx4_cleanup_counters_table(struct mlx4_dev *dev)\n{\n\tif (!(dev->caps.flags & MLX4_DEV_CAP_FLAG_COUNTERS))\n\t\treturn;\n\n\tif (!dev->caps.max_counters)\n\t\treturn;\n\n\tmlx4_bitmap_cleanup(&mlx4_priv(dev)->counters_bitmap);\n}\n\nstatic void mlx4_cleanup_default_counters(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint port;\n\n\tfor (port = 0; port < dev->caps.num_ports; port++)\n\t\tif (priv->def_counter[port] != -1)\n\t\t\tmlx4_counter_free(dev,  priv->def_counter[port]);\n}\n\nstatic int mlx4_allocate_default_counters(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint port, err = 0;\n\tu32 idx;\n\n\tfor (port = 0; port < dev->caps.num_ports; port++)\n\t\tpriv->def_counter[port] = -1;\n\n\tfor (port = 0; port < dev->caps.num_ports; port++) {\n\t\terr = mlx4_counter_alloc(dev, &idx, MLX4_RES_USAGE_DRIVER);\n\n\t\tif (!err || err == -ENOSPC) {\n\t\t\tpriv->def_counter[port] = idx;\n\t\t\terr = 0;\n\t\t} else if (err == -ENOENT) {\n\t\t\terr = 0;\n\t\t\tcontinue;\n\t\t} else if (mlx4_is_slave(dev) && err == -EINVAL) {\n\t\t\tpriv->def_counter[port] = MLX4_SINK_COUNTER_INDEX(dev);\n\t\t\tmlx4_warn(dev, \"can't allocate counter from old PF driver, using index %d\\n\",\n\t\t\t\t  MLX4_SINK_COUNTER_INDEX(dev));\n\t\t\terr = 0;\n\t\t} else {\n\t\t\tmlx4_err(dev, \"%s: failed to allocate default counter port %d err %d\\n\",\n\t\t\t\t __func__, port + 1, err);\n\t\t\tmlx4_cleanup_default_counters(dev);\n\t\t\treturn err;\n\t\t}\n\n\t\tmlx4_dbg(dev, \"%s: default counter index %d for port %d\\n\",\n\t\t\t __func__, priv->def_counter[port], port + 1);\n\t}\n\n\treturn err;\n}\n\nint __mlx4_counter_alloc(struct mlx4_dev *dev, u32 *idx)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tif (!(dev->caps.flags & MLX4_DEV_CAP_FLAG_COUNTERS))\n\t\treturn -ENOENT;\n\n\t*idx = mlx4_bitmap_alloc(&priv->counters_bitmap);\n\tif (*idx == -1) {\n\t\t*idx = MLX4_SINK_COUNTER_INDEX(dev);\n\t\treturn -ENOSPC;\n\t}\n\n\treturn 0;\n}\n\nint mlx4_counter_alloc(struct mlx4_dev *dev, u32 *idx, u8 usage)\n{\n\tu32 in_modifier = RES_COUNTER | (((u32)usage & 3) << 30);\n\tu64 out_param;\n\tint err;\n\n\tif (mlx4_is_mfunc(dev)) {\n\t\terr = mlx4_cmd_imm(dev, 0, &out_param, in_modifier,\n\t\t\t\t   RES_OP_RESERVE, MLX4_CMD_ALLOC_RES,\n\t\t\t\t   MLX4_CMD_TIME_CLASS_A, MLX4_CMD_WRAPPED);\n\t\tif (!err)\n\t\t\t*idx = get_param_l(&out_param);\n\t\tif (WARN_ON(err == -ENOSPC))\n\t\t\terr = -EINVAL;\n\t\treturn err;\n\t}\n\treturn __mlx4_counter_alloc(dev, idx);\n}\nEXPORT_SYMBOL_GPL(mlx4_counter_alloc);\n\nstatic int __mlx4_clear_if_stat(struct mlx4_dev *dev,\n\t\t\t\tu8 counter_index)\n{\n\tstruct mlx4_cmd_mailbox *if_stat_mailbox;\n\tint err;\n\tu32 if_stat_in_mod = (counter_index & 0xff) | MLX4_QUERY_IF_STAT_RESET;\n\n\tif_stat_mailbox = mlx4_alloc_cmd_mailbox(dev);\n\tif (IS_ERR(if_stat_mailbox))\n\t\treturn PTR_ERR(if_stat_mailbox);\n\n\terr = mlx4_cmd_box(dev, 0, if_stat_mailbox->dma, if_stat_in_mod, 0,\n\t\t\t   MLX4_CMD_QUERY_IF_STAT, MLX4_CMD_TIME_CLASS_C,\n\t\t\t   MLX4_CMD_NATIVE);\n\n\tmlx4_free_cmd_mailbox(dev, if_stat_mailbox);\n\treturn err;\n}\n\nvoid __mlx4_counter_free(struct mlx4_dev *dev, u32 idx)\n{\n\tif (!(dev->caps.flags & MLX4_DEV_CAP_FLAG_COUNTERS))\n\t\treturn;\n\n\tif (idx == MLX4_SINK_COUNTER_INDEX(dev))\n\t\treturn;\n\n\t__mlx4_clear_if_stat(dev, idx);\n\n\tmlx4_bitmap_free(&mlx4_priv(dev)->counters_bitmap, idx, MLX4_USE_RR);\n\treturn;\n}\n\nvoid mlx4_counter_free(struct mlx4_dev *dev, u32 idx)\n{\n\tu64 in_param = 0;\n\n\tif (mlx4_is_mfunc(dev)) {\n\t\tset_param_l(&in_param, idx);\n\t\tmlx4_cmd(dev, in_param, RES_COUNTER, RES_OP_RESERVE,\n\t\t\t MLX4_CMD_FREE_RES, MLX4_CMD_TIME_CLASS_A,\n\t\t\t MLX4_CMD_WRAPPED);\n\t\treturn;\n\t}\n\t__mlx4_counter_free(dev, idx);\n}\nEXPORT_SYMBOL_GPL(mlx4_counter_free);\n\nint mlx4_get_default_counter_index(struct mlx4_dev *dev, int port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\treturn priv->def_counter[port - 1];\n}\nEXPORT_SYMBOL_GPL(mlx4_get_default_counter_index);\n\nvoid mlx4_set_admin_guid(struct mlx4_dev *dev, __be64 guid, int entry, int port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\tpriv->mfunc.master.vf_admin[entry].vport[port].guid = guid;\n}\nEXPORT_SYMBOL_GPL(mlx4_set_admin_guid);\n\n__be64 mlx4_get_admin_guid(struct mlx4_dev *dev, int entry, int port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\treturn priv->mfunc.master.vf_admin[entry].vport[port].guid;\n}\nEXPORT_SYMBOL_GPL(mlx4_get_admin_guid);\n\nvoid mlx4_set_random_admin_guid(struct mlx4_dev *dev, int entry, int port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\t__be64 guid;\n\n\t \n\tif (entry == 0)\n\t\treturn;\n\n\tget_random_bytes((char *)&guid, sizeof(guid));\n\tguid &= ~(cpu_to_be64(1ULL << 56));\n\tguid |= cpu_to_be64(1ULL << 57);\n\tpriv->mfunc.master.vf_admin[entry].vport[port].guid = guid;\n}\n\nstatic int mlx4_setup_hca(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint err;\n\tint port;\n\t__be32 ib_port_default_caps;\n\n\terr = mlx4_init_uar_table(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to initialize user access region table, aborting\\n\");\n\t\treturn err;\n\t}\n\n\terr = mlx4_uar_alloc(dev, &priv->driver_uar);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to allocate driver access region, aborting\\n\");\n\t\tgoto err_uar_table_free;\n\t}\n\n\tpriv->kar = ioremap((phys_addr_t) priv->driver_uar.pfn << PAGE_SHIFT, PAGE_SIZE);\n\tif (!priv->kar) {\n\t\tmlx4_err(dev, \"Couldn't map kernel access region, aborting\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_uar_free;\n\t}\n\n\terr = mlx4_init_pd_table(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to initialize protection domain table, aborting\\n\");\n\t\tgoto err_kar_unmap;\n\t}\n\n\terr = mlx4_init_xrcd_table(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to initialize reliable connection domain table, aborting\\n\");\n\t\tgoto err_pd_table_free;\n\t}\n\n\terr = mlx4_init_mr_table(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to initialize memory region table, aborting\\n\");\n\t\tgoto err_xrcd_table_free;\n\t}\n\n\tif (!mlx4_is_slave(dev)) {\n\t\terr = mlx4_init_mcg_table(dev);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"Failed to initialize multicast group table, aborting\\n\");\n\t\t\tgoto err_mr_table_free;\n\t\t}\n\t\terr = mlx4_config_mad_demux(dev);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"Failed in config_mad_demux, aborting\\n\");\n\t\t\tgoto err_mcg_table_free;\n\t\t}\n\t}\n\n\terr = mlx4_init_eq_table(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to initialize event queue table, aborting\\n\");\n\t\tgoto err_mcg_table_free;\n\t}\n\n\terr = mlx4_cmd_use_events(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to switch to event-driven firmware commands, aborting\\n\");\n\t\tgoto err_eq_table_free;\n\t}\n\n\terr = mlx4_NOP(dev);\n\tif (err) {\n\t\tif (dev->flags & MLX4_FLAG_MSI_X) {\n\t\t\tmlx4_warn(dev, \"NOP command failed to generate MSI-X interrupt IRQ %d)\\n\",\n\t\t\t\t  priv->eq_table.eq[MLX4_EQ_ASYNC].irq);\n\t\t\tmlx4_warn(dev, \"Trying again without MSI-X\\n\");\n\t\t} else {\n\t\t\tmlx4_err(dev, \"NOP command failed to generate interrupt (IRQ %d), aborting\\n\",\n\t\t\t\t priv->eq_table.eq[MLX4_EQ_ASYNC].irq);\n\t\t\tmlx4_err(dev, \"BIOS or ACPI interrupt routing problem?\\n\");\n\t\t}\n\n\t\tgoto err_cmd_poll;\n\t}\n\n\tmlx4_dbg(dev, \"NOP command IRQ test passed\\n\");\n\n\terr = mlx4_init_cq_table(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to initialize completion queue table, aborting\\n\");\n\t\tgoto err_cmd_poll;\n\t}\n\n\terr = mlx4_init_srq_table(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to initialize shared receive queue table, aborting\\n\");\n\t\tgoto err_cq_table_free;\n\t}\n\n\terr = mlx4_init_qp_table(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to initialize queue pair table, aborting\\n\");\n\t\tgoto err_srq_table_free;\n\t}\n\n\tif (!mlx4_is_slave(dev)) {\n\t\terr = mlx4_init_counters_table(dev);\n\t\tif (err && err != -ENOENT) {\n\t\t\tmlx4_err(dev, \"Failed to initialize counters table, aborting\\n\");\n\t\t\tgoto err_qp_table_free;\n\t\t}\n\t}\n\n\terr = mlx4_allocate_default_counters(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to allocate default counters, aborting\\n\");\n\t\tgoto err_counters_table_free;\n\t}\n\n\tif (!mlx4_is_slave(dev)) {\n\t\tfor (port = 1; port <= dev->caps.num_ports; port++) {\n\t\t\tib_port_default_caps = 0;\n\t\t\terr = mlx4_get_port_ib_caps(dev, port,\n\t\t\t\t\t\t    &ib_port_default_caps);\n\t\t\tif (err)\n\t\t\t\tmlx4_warn(dev, \"failed to get port %d default ib capabilities (%d). Continuing with caps = 0\\n\",\n\t\t\t\t\t  port, err);\n\t\t\tdev->caps.ib_port_def_cap[port] = ib_port_default_caps;\n\n\t\t\t \n\t\t\tif (mlx4_is_master(dev)) {\n\t\t\t\tint i;\n\t\t\t\tfor (i = 0; i < dev->num_slaves; i++) {\n\t\t\t\t\tif (i == mlx4_master_func_num(dev))\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tpriv->mfunc.master.slave_state[i].ib_cap_mask[port] =\n\t\t\t\t\t\tib_port_default_caps;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (mlx4_is_mfunc(dev))\n\t\t\t\tdev->caps.port_ib_mtu[port] = IB_MTU_2048;\n\t\t\telse\n\t\t\t\tdev->caps.port_ib_mtu[port] = IB_MTU_4096;\n\n\t\t\terr = mlx4_SET_PORT(dev, port, mlx4_is_master(dev) ?\n\t\t\t\t\t    dev->caps.pkey_table_len[port] : -1);\n\t\t\tif (err) {\n\t\t\t\tmlx4_err(dev, \"Failed to set port %d, aborting\\n\",\n\t\t\t\t\t port);\n\t\t\t\tgoto err_default_countes_free;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_default_countes_free:\n\tmlx4_cleanup_default_counters(dev);\n\nerr_counters_table_free:\n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_cleanup_counters_table(dev);\n\nerr_qp_table_free:\n\tmlx4_cleanup_qp_table(dev);\n\nerr_srq_table_free:\n\tmlx4_cleanup_srq_table(dev);\n\nerr_cq_table_free:\n\tmlx4_cleanup_cq_table(dev);\n\nerr_cmd_poll:\n\tmlx4_cmd_use_polling(dev);\n\nerr_eq_table_free:\n\tmlx4_cleanup_eq_table(dev);\n\nerr_mcg_table_free:\n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_cleanup_mcg_table(dev);\n\nerr_mr_table_free:\n\tmlx4_cleanup_mr_table(dev);\n\nerr_xrcd_table_free:\n\tmlx4_cleanup_xrcd_table(dev);\n\nerr_pd_table_free:\n\tmlx4_cleanup_pd_table(dev);\n\nerr_kar_unmap:\n\tiounmap(priv->kar);\n\nerr_uar_free:\n\tmlx4_uar_free(dev, &priv->driver_uar);\n\nerr_uar_table_free:\n\tmlx4_cleanup_uar_table(dev);\n\treturn err;\n}\n\nstatic int mlx4_init_affinity_hint(struct mlx4_dev *dev, int port, int eqn)\n{\n\tint requested_cpu = 0;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_eq *eq;\n\tint off = 0;\n\tint i;\n\n\tif (eqn > dev->caps.num_comp_vectors)\n\t\treturn -EINVAL;\n\n\tfor (i = 1; i < port; i++)\n\t\toff += mlx4_get_eqs_per_port(dev, i);\n\n\trequested_cpu = eqn - off - !!(eqn > MLX4_EQ_ASYNC);\n\n\t \n\tif (requested_cpu < 0)\n\t\treturn 0;\n\n\teq = &priv->eq_table.eq[eqn];\n\n\tif (!zalloc_cpumask_var(&eq->affinity_mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tcpumask_set_cpu(requested_cpu, eq->affinity_mask);\n\n\treturn 0;\n}\n\nstatic void mlx4_enable_msi_x(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct msix_entry *entries;\n\tint i;\n\tint port = 0;\n\n\tif (msi_x) {\n\t\tint nreq = min3(dev->caps.num_ports *\n\t\t\t\t(int)num_online_cpus() + 1,\n\t\t\t\tdev->caps.num_eqs - dev->caps.reserved_eqs,\n\t\t\t\tMAX_MSIX);\n\n\t\tif (msi_x > 1)\n\t\t\tnreq = min_t(int, nreq, msi_x);\n\n\t\tentries = kcalloc(nreq, sizeof(*entries), GFP_KERNEL);\n\t\tif (!entries)\n\t\t\tgoto no_msi;\n\n\t\tfor (i = 0; i < nreq; ++i)\n\t\t\tentries[i].entry = i;\n\n\t\tnreq = pci_enable_msix_range(dev->persist->pdev, entries, 2,\n\t\t\t\t\t     nreq);\n\n\t\tif (nreq < 0 || nreq < MLX4_EQ_ASYNC) {\n\t\t\tkfree(entries);\n\t\t\tgoto no_msi;\n\t\t}\n\t\t \n\t\tdev->caps.num_comp_vectors = nreq - 1;\n\n\t\tpriv->eq_table.eq[MLX4_EQ_ASYNC].irq = entries[0].vector;\n\t\tbitmap_zero(priv->eq_table.eq[MLX4_EQ_ASYNC].actv_ports.ports,\n\t\t\t    dev->caps.num_ports);\n\n\t\tfor (i = 0; i < dev->caps.num_comp_vectors + 1; i++) {\n\t\t\tif (i == MLX4_EQ_ASYNC)\n\t\t\t\tcontinue;\n\n\t\t\tpriv->eq_table.eq[i].irq =\n\t\t\t\tentries[i + 1 - !!(i > MLX4_EQ_ASYNC)].vector;\n\n\t\t\tif (MLX4_IS_LEGACY_EQ_MODE(dev->caps)) {\n\t\t\t\tbitmap_fill(priv->eq_table.eq[i].actv_ports.ports,\n\t\t\t\t\t    dev->caps.num_ports);\n\t\t\t\t \n\t\t\t} else {\n\t\t\t\tset_bit(port,\n\t\t\t\t\tpriv->eq_table.eq[i].actv_ports.ports);\n\t\t\t\tif (mlx4_init_affinity_hint(dev, port + 1, i))\n\t\t\t\t\tmlx4_warn(dev, \"Couldn't init hint cpumask for EQ %d\\n\",\n\t\t\t\t\t\t  i);\n\t\t\t}\n\t\t\t \n\t\t\tif ((dev->caps.num_comp_vectors > dev->caps.num_ports) &&\n\t\t\t    ((i + 1) %\n\t\t\t     (dev->caps.num_comp_vectors / dev->caps.num_ports)) ==\n\t\t\t    !!((i + 1) > MLX4_EQ_ASYNC))\n\t\t\t\t \n\t\t\t\tport++;\n\t\t}\n\n\t\tdev->flags |= MLX4_FLAG_MSI_X;\n\n\t\tkfree(entries);\n\t\treturn;\n\t}\n\nno_msi:\n\tdev->caps.num_comp_vectors = 1;\n\n\tBUG_ON(MLX4_EQ_ASYNC >= 2);\n\tfor (i = 0; i < 2; ++i) {\n\t\tpriv->eq_table.eq[i].irq = dev->persist->pdev->irq;\n\t\tif (i != MLX4_EQ_ASYNC) {\n\t\t\tbitmap_fill(priv->eq_table.eq[i].actv_ports.ports,\n\t\t\t\t    dev->caps.num_ports);\n\t\t}\n\t}\n}\n\nstatic int mlx4_devlink_port_type_set(struct devlink_port *devlink_port,\n\t\t\t\t      enum devlink_port_type port_type)\n{\n\tstruct mlx4_port_info *info = container_of(devlink_port,\n\t\t\t\t\t\t   struct mlx4_port_info,\n\t\t\t\t\t\t   devlink_port);\n\tenum mlx4_port_type mlx4_port_type;\n\n\tswitch (port_type) {\n\tcase DEVLINK_PORT_TYPE_AUTO:\n\t\tmlx4_port_type = MLX4_PORT_TYPE_AUTO;\n\t\tbreak;\n\tcase DEVLINK_PORT_TYPE_ETH:\n\t\tmlx4_port_type = MLX4_PORT_TYPE_ETH;\n\t\tbreak;\n\tcase DEVLINK_PORT_TYPE_IB:\n\t\tmlx4_port_type = MLX4_PORT_TYPE_IB;\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn __set_port_type(info, mlx4_port_type);\n}\n\nstatic const struct devlink_port_ops mlx4_devlink_port_ops = {\n\t.port_type_set = mlx4_devlink_port_type_set,\n};\n\nstatic int mlx4_init_port_info(struct mlx4_dev *dev, int port)\n{\n\tstruct devlink *devlink = priv_to_devlink(mlx4_priv(dev));\n\tstruct mlx4_port_info *info = &mlx4_priv(dev)->port[port];\n\tint err;\n\n\terr = devl_port_register_with_ops(devlink, &info->devlink_port, port,\n\t\t\t\t\t  &mlx4_devlink_port_ops);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (!IS_ENABLED(CONFIG_MLX4_EN) &&\n\t    dev->caps.port_type[port] == MLX4_PORT_TYPE_ETH)\n\t\tdevlink_port_type_eth_set(&info->devlink_port);\n\telse if (!IS_ENABLED(CONFIG_MLX4_INFINIBAND) &&\n\t\t dev->caps.port_type[port] == MLX4_PORT_TYPE_IB)\n\t\tdevlink_port_type_ib_set(&info->devlink_port, NULL);\n\n\tinfo->dev = dev;\n\tinfo->port = port;\n\tif (!mlx4_is_slave(dev)) {\n\t\tmlx4_init_mac_table(dev, &info->mac_table);\n\t\tmlx4_init_vlan_table(dev, &info->vlan_table);\n\t\tmlx4_init_roce_gid_table(dev, &info->gid_table);\n\t\tinfo->base_qpn = mlx4_get_base_qpn(dev, port);\n\t}\n\n\tsprintf(info->dev_name, \"mlx4_port%d\", port);\n\tinfo->port_attr.attr.name = info->dev_name;\n\tif (mlx4_is_mfunc(dev)) {\n\t\tinfo->port_attr.attr.mode = 0444;\n\t} else {\n\t\tinfo->port_attr.attr.mode = 0644;\n\t\tinfo->port_attr.store     = set_port_type;\n\t}\n\tinfo->port_attr.show      = show_port_type;\n\tsysfs_attr_init(&info->port_attr.attr);\n\n\terr = device_create_file(&dev->persist->pdev->dev, &info->port_attr);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to create file for port %d\\n\", port);\n\t\tdevlink_port_type_clear(&info->devlink_port);\n\t\tdevl_port_unregister(&info->devlink_port);\n\t\tinfo->port = -1;\n\t\treturn err;\n\t}\n\n\tsprintf(info->dev_mtu_name, \"mlx4_port%d_mtu\", port);\n\tinfo->port_mtu_attr.attr.name = info->dev_mtu_name;\n\tif (mlx4_is_mfunc(dev)) {\n\t\tinfo->port_mtu_attr.attr.mode = 0444;\n\t} else {\n\t\tinfo->port_mtu_attr.attr.mode = 0644;\n\t\tinfo->port_mtu_attr.store     = set_port_ib_mtu;\n\t}\n\tinfo->port_mtu_attr.show      = show_port_ib_mtu;\n\tsysfs_attr_init(&info->port_mtu_attr.attr);\n\n\terr = device_create_file(&dev->persist->pdev->dev,\n\t\t\t\t &info->port_mtu_attr);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to create mtu file for port %d\\n\", port);\n\t\tdevice_remove_file(&info->dev->persist->pdev->dev,\n\t\t\t\t   &info->port_attr);\n\t\tdevlink_port_type_clear(&info->devlink_port);\n\t\tdevl_port_unregister(&info->devlink_port);\n\t\tinfo->port = -1;\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void mlx4_cleanup_port_info(struct mlx4_port_info *info)\n{\n\tif (info->port < 0)\n\t\treturn;\n\n\tdevice_remove_file(&info->dev->persist->pdev->dev, &info->port_attr);\n\tdevice_remove_file(&info->dev->persist->pdev->dev,\n\t\t\t   &info->port_mtu_attr);\n\tdevlink_port_type_clear(&info->devlink_port);\n\tdevl_port_unregister(&info->devlink_port);\n\n#ifdef CONFIG_RFS_ACCEL\n\tfree_irq_cpu_rmap(info->rmap);\n\tinfo->rmap = NULL;\n#endif\n}\n\nstatic int mlx4_init_steering(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint num_entries = dev->caps.num_ports;\n\tint i, j;\n\n\tpriv->steer = kcalloc(num_entries, sizeof(struct mlx4_steer),\n\t\t\t      GFP_KERNEL);\n\tif (!priv->steer)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_entries; i++)\n\t\tfor (j = 0; j < MLX4_NUM_STEERS; j++) {\n\t\t\tINIT_LIST_HEAD(&priv->steer[i].promisc_qps[j]);\n\t\t\tINIT_LIST_HEAD(&priv->steer[i].steer_entries[j]);\n\t\t}\n\treturn 0;\n}\n\nstatic void mlx4_clear_steering(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_steer_index *entry, *tmp_entry;\n\tstruct mlx4_promisc_qp *pqp, *tmp_pqp;\n\tint num_entries = dev->caps.num_ports;\n\tint i, j;\n\n\tfor (i = 0; i < num_entries; i++) {\n\t\tfor (j = 0; j < MLX4_NUM_STEERS; j++) {\n\t\t\tlist_for_each_entry_safe(pqp, tmp_pqp,\n\t\t\t\t\t\t &priv->steer[i].promisc_qps[j],\n\t\t\t\t\t\t list) {\n\t\t\t\tlist_del(&pqp->list);\n\t\t\t\tkfree(pqp);\n\t\t\t}\n\t\t\tlist_for_each_entry_safe(entry, tmp_entry,\n\t\t\t\t\t\t &priv->steer[i].steer_entries[j],\n\t\t\t\t\t\t list) {\n\t\t\t\tlist_del(&entry->list);\n\t\t\t\tlist_for_each_entry_safe(pqp, tmp_pqp,\n\t\t\t\t\t\t\t &entry->duplicates,\n\t\t\t\t\t\t\t list) {\n\t\t\t\t\tlist_del(&pqp->list);\n\t\t\t\t\tkfree(pqp);\n\t\t\t\t}\n\t\t\t\tkfree(entry);\n\t\t\t}\n\t\t}\n\t}\n\tkfree(priv->steer);\n}\n\nstatic int extended_func_num(struct pci_dev *pdev)\n{\n\treturn PCI_SLOT(pdev->devfn) * 8 + PCI_FUNC(pdev->devfn);\n}\n\n#define MLX4_OWNER_BASE\t0x8069c\n#define MLX4_OWNER_SIZE\t4\n\nstatic int mlx4_get_ownership(struct mlx4_dev *dev)\n{\n\tvoid __iomem *owner;\n\tu32 ret;\n\n\tif (pci_channel_offline(dev->persist->pdev))\n\t\treturn -EIO;\n\n\towner = ioremap(pci_resource_start(dev->persist->pdev, 0) +\n\t\t\tMLX4_OWNER_BASE,\n\t\t\tMLX4_OWNER_SIZE);\n\tif (!owner) {\n\t\tmlx4_err(dev, \"Failed to obtain ownership bit\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tret = readl(owner);\n\tiounmap(owner);\n\treturn (int) !!ret;\n}\n\nstatic void mlx4_free_ownership(struct mlx4_dev *dev)\n{\n\tvoid __iomem *owner;\n\n\tif (pci_channel_offline(dev->persist->pdev))\n\t\treturn;\n\n\towner = ioremap(pci_resource_start(dev->persist->pdev, 0) +\n\t\t\tMLX4_OWNER_BASE,\n\t\t\tMLX4_OWNER_SIZE);\n\tif (!owner) {\n\t\tmlx4_err(dev, \"Failed to obtain ownership bit\\n\");\n\t\treturn;\n\t}\n\twritel(0, owner);\n\tmsleep(1000);\n\tiounmap(owner);\n}\n\n#define SRIOV_VALID_STATE(flags) (!!((flags) & MLX4_FLAG_SRIOV)\t==\\\n\t\t\t\t  !!((flags) & MLX4_FLAG_MASTER))\n\nstatic u64 mlx4_enable_sriov(struct mlx4_dev *dev, struct pci_dev *pdev,\n\t\t\t     u8 total_vfs, int existing_vfs, int reset_flow)\n{\n\tu64 dev_flags = dev->flags;\n\tint err = 0;\n\tint fw_enabled_sriov_vfs = min(pci_sriov_get_totalvfs(pdev),\n\t\t\t\t\tMLX4_MAX_NUM_VF);\n\n\tif (reset_flow) {\n\t\tdev->dev_vfs = kcalloc(total_vfs, sizeof(*dev->dev_vfs),\n\t\t\t\t       GFP_KERNEL);\n\t\tif (!dev->dev_vfs)\n\t\t\tgoto free_mem;\n\t\treturn dev_flags;\n\t}\n\n\tatomic_inc(&pf_loading);\n\tif (dev->flags &  MLX4_FLAG_SRIOV) {\n\t\tif (existing_vfs != total_vfs) {\n\t\t\tmlx4_err(dev, \"SR-IOV was already enabled, but with num_vfs (%d) different than requested (%d)\\n\",\n\t\t\t\t existing_vfs, total_vfs);\n\t\t\ttotal_vfs = existing_vfs;\n\t\t}\n\t}\n\n\tdev->dev_vfs = kcalloc(total_vfs, sizeof(*dev->dev_vfs), GFP_KERNEL);\n\tif (NULL == dev->dev_vfs) {\n\t\tmlx4_err(dev, \"Failed to allocate memory for VFs\\n\");\n\t\tgoto disable_sriov;\n\t}\n\n\tif (!(dev->flags &  MLX4_FLAG_SRIOV)) {\n\t\tif (total_vfs > fw_enabled_sriov_vfs) {\n\t\t\tmlx4_err(dev, \"requested vfs (%d) > available vfs (%d). Continuing without SR_IOV\\n\",\n\t\t\t\t total_vfs, fw_enabled_sriov_vfs);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto disable_sriov;\n\t\t}\n\t\tmlx4_warn(dev, \"Enabling SR-IOV with %d VFs\\n\", total_vfs);\n\t\terr = pci_enable_sriov(pdev, total_vfs);\n\t}\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to enable SR-IOV, continuing without SR-IOV (err = %d)\\n\",\n\t\t\t err);\n\t\tgoto disable_sriov;\n\t} else {\n\t\tmlx4_warn(dev, \"Running in master mode\\n\");\n\t\tdev_flags |= MLX4_FLAG_SRIOV |\n\t\t\tMLX4_FLAG_MASTER;\n\t\tdev_flags &= ~MLX4_FLAG_SLAVE;\n\t\tdev->persist->num_vfs = total_vfs;\n\t}\n\treturn dev_flags;\n\ndisable_sriov:\n\tatomic_dec(&pf_loading);\nfree_mem:\n\tdev->persist->num_vfs = 0;\n\tkfree(dev->dev_vfs);\n\tdev->dev_vfs = NULL;\n\treturn dev_flags & ~MLX4_FLAG_MASTER;\n}\n\nenum {\n\tMLX4_DEV_CAP_CHECK_NUM_VFS_ABOVE_64 = -1,\n};\n\nstatic int mlx4_check_dev_cap(struct mlx4_dev *dev, struct mlx4_dev_cap *dev_cap,\n\t\t\t      int *nvfs)\n{\n\tint requested_vfs = nvfs[0] + nvfs[1] + nvfs[2];\n\t \n\tif (!(dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_80_VFS) &&\n\t    requested_vfs >= 64) {\n\t\tmlx4_err(dev, \"Requested %d VFs, but FW does not support more than 64\\n\",\n\t\t\t requested_vfs);\n\t\treturn MLX4_DEV_CAP_CHECK_NUM_VFS_ABOVE_64;\n\t}\n\treturn 0;\n}\n\nstatic int mlx4_pci_enable_device(struct mlx4_dev *dev)\n{\n\tstruct pci_dev *pdev = dev->persist->pdev;\n\tint err = 0;\n\n\tmutex_lock(&dev->persist->pci_status_mutex);\n\tif (dev->persist->pci_status == MLX4_PCI_STATUS_DISABLED) {\n\t\terr = pci_enable_device(pdev);\n\t\tif (!err)\n\t\t\tdev->persist->pci_status = MLX4_PCI_STATUS_ENABLED;\n\t}\n\tmutex_unlock(&dev->persist->pci_status_mutex);\n\n\treturn err;\n}\n\nstatic void mlx4_pci_disable_device(struct mlx4_dev *dev)\n{\n\tstruct pci_dev *pdev = dev->persist->pdev;\n\n\tmutex_lock(&dev->persist->pci_status_mutex);\n\tif (dev->persist->pci_status == MLX4_PCI_STATUS_ENABLED) {\n\t\tpci_disable_device(pdev);\n\t\tdev->persist->pci_status = MLX4_PCI_STATUS_DISABLED;\n\t}\n\tmutex_unlock(&dev->persist->pci_status_mutex);\n}\n\nstatic int mlx4_load_one(struct pci_dev *pdev, int pci_dev_data,\n\t\t\t int total_vfs, int *nvfs, struct mlx4_priv *priv,\n\t\t\t int reset_flow)\n{\n\tstruct devlink *devlink = priv_to_devlink(priv);\n\tstruct mlx4_dev *dev;\n\tunsigned sum = 0;\n\tint err;\n\tint port;\n\tint i;\n\tstruct mlx4_dev_cap *dev_cap = NULL;\n\tint existing_vfs = 0;\n\n\tdevl_assert_locked(devlink);\n\tdev = &priv->dev;\n\n\terr = mlx4_adev_init(dev);\n\tif (err)\n\t\treturn err;\n\n\tATOMIC_INIT_NOTIFIER_HEAD(&priv->event_nh);\n\n\tmutex_init(&priv->port_mutex);\n\tmutex_init(&priv->bond_mutex);\n\n\tINIT_LIST_HEAD(&priv->pgdir_list);\n\tmutex_init(&priv->pgdir_mutex);\n\tspin_lock_init(&priv->cmd.context_lock);\n\n\tINIT_LIST_HEAD(&priv->bf_list);\n\tmutex_init(&priv->bf_mutex);\n\n\tdev->rev_id = pdev->revision;\n\tdev->numa_node = dev_to_node(&pdev->dev);\n\n\t \n\tif (pci_dev_data & MLX4_PCI_DEV_IS_VF) {\n\t\tmlx4_warn(dev, \"Detected virtual function - running in slave mode\\n\");\n\t\tdev->flags |= MLX4_FLAG_SLAVE;\n\t} else {\n\t\t \n\t\terr = mlx4_get_ownership(dev);\n\t\tif (err) {\n\t\t\tif (err < 0)\n\t\t\t\tgoto err_adev;\n\t\t\telse {\n\t\t\t\tmlx4_warn(dev, \"Multiple PFs not yet supported - Skipping PF\\n\");\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_adev;\n\t\t\t}\n\t\t}\n\n\t\tatomic_set(&priv->opreq_count, 0);\n\t\tINIT_WORK(&priv->opreq_task, mlx4_opreq_action);\n\n\t\t \n\t\terr = mlx4_reset(dev);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"Failed to reset HCA, aborting\\n\");\n\t\t\tgoto err_sriov;\n\t\t}\n\n\t\tif (total_vfs) {\n\t\t\tdev->flags = MLX4_FLAG_MASTER;\n\t\t\texisting_vfs = pci_num_vf(pdev);\n\t\t\tif (existing_vfs)\n\t\t\t\tdev->flags |= MLX4_FLAG_SRIOV;\n\t\t\tdev->persist->num_vfs = total_vfs;\n\t\t}\n\t}\n\n\t \n\tdev->persist->state = MLX4_DEVICE_STATE_UP;\n\nslave_start:\n\terr = mlx4_cmd_init(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to init command interface, aborting\\n\");\n\t\tgoto err_sriov;\n\t}\n\n\t \n\tif (mlx4_is_mfunc(dev)) {\n\t\tif (mlx4_is_master(dev)) {\n\t\t\tdev->num_slaves = MLX4_MAX_NUM_SLAVES;\n\n\t\t} else {\n\t\t\tdev->num_slaves = 0;\n\t\t\terr = mlx4_multi_func_init(dev);\n\t\t\tif (err) {\n\t\t\t\tmlx4_err(dev, \"Failed to init slave mfunc interface, aborting\\n\");\n\t\t\t\tgoto err_cmd;\n\t\t\t}\n\t\t}\n\t}\n\n\terr = mlx4_init_fw(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to init fw, aborting.\\n\");\n\t\tgoto err_mfunc;\n\t}\n\n\tif (mlx4_is_master(dev)) {\n\t\t \n\t\tif (!dev_cap) {\n\t\t\tdev_cap = kzalloc(sizeof(*dev_cap), GFP_KERNEL);\n\n\t\t\tif (!dev_cap) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto err_fw;\n\t\t\t}\n\n\t\t\terr = mlx4_QUERY_DEV_CAP(dev, dev_cap);\n\t\t\tif (err) {\n\t\t\t\tmlx4_err(dev, \"QUERY_DEV_CAP command failed, aborting.\\n\");\n\t\t\t\tgoto err_fw;\n\t\t\t}\n\n\t\t\tif (mlx4_check_dev_cap(dev, dev_cap, nvfs))\n\t\t\t\tgoto err_fw;\n\n\t\t\tif (!(dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_SYS_EQS)) {\n\t\t\t\tu64 dev_flags = mlx4_enable_sriov(dev, pdev,\n\t\t\t\t\t\t\t\t  total_vfs,\n\t\t\t\t\t\t\t\t  existing_vfs,\n\t\t\t\t\t\t\t\t  reset_flow);\n\n\t\t\t\tmlx4_close_fw(dev);\n\t\t\t\tmlx4_cmd_cleanup(dev, MLX4_CMD_CLEANUP_ALL);\n\t\t\t\tdev->flags = dev_flags;\n\t\t\t\tif (!SRIOV_VALID_STATE(dev->flags)) {\n\t\t\t\t\tmlx4_err(dev, \"Invalid SRIOV state\\n\");\n\t\t\t\t\tgoto err_sriov;\n\t\t\t\t}\n\t\t\t\terr = mlx4_reset(dev);\n\t\t\t\tif (err) {\n\t\t\t\t\tmlx4_err(dev, \"Failed to reset HCA, aborting.\\n\");\n\t\t\t\t\tgoto err_sriov;\n\t\t\t\t}\n\t\t\t\tgoto slave_start;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tmemset(dev_cap, 0, sizeof(*dev_cap));\n\t\t\terr = mlx4_QUERY_DEV_CAP(dev, dev_cap);\n\t\t\tif (err) {\n\t\t\t\tmlx4_err(dev, \"QUERY_DEV_CAP command failed, aborting.\\n\");\n\t\t\t\tgoto err_fw;\n\t\t\t}\n\n\t\t\tif (mlx4_check_dev_cap(dev, dev_cap, nvfs))\n\t\t\t\tgoto err_fw;\n\t\t}\n\t}\n\n\terr = mlx4_init_hca(dev);\n\tif (err) {\n\t\tif (err == -EACCES) {\n\t\t\t \n\t\t\tmlx4_cmd_cleanup(dev, MLX4_CMD_CLEANUP_ALL);\n\t\t\t \n\t\t\tif (dev->flags & MLX4_FLAG_SRIOV) {\n\t\t\t\tif (!existing_vfs)\n\t\t\t\t\tpci_disable_sriov(pdev);\n\t\t\t\tif (mlx4_is_master(dev) && !reset_flow)\n\t\t\t\t\tatomic_dec(&pf_loading);\n\t\t\t\tdev->flags &= ~MLX4_FLAG_SRIOV;\n\t\t\t}\n\t\t\tif (!mlx4_is_slave(dev))\n\t\t\t\tmlx4_free_ownership(dev);\n\t\t\tdev->flags |= MLX4_FLAG_SLAVE;\n\t\t\tdev->flags &= ~MLX4_FLAG_MASTER;\n\t\t\tgoto slave_start;\n\t\t} else\n\t\t\tgoto err_fw;\n\t}\n\n\tif (mlx4_is_master(dev) && (dev_cap->flags2 & MLX4_DEV_CAP_FLAG2_SYS_EQS)) {\n\t\tu64 dev_flags = mlx4_enable_sriov(dev, pdev, total_vfs,\n\t\t\t\t\t\t  existing_vfs, reset_flow);\n\n\t\tif ((dev->flags ^ dev_flags) & (MLX4_FLAG_MASTER | MLX4_FLAG_SLAVE)) {\n\t\t\tmlx4_cmd_cleanup(dev, MLX4_CMD_CLEANUP_VHCR);\n\t\t\tdev->flags = dev_flags;\n\t\t\terr = mlx4_cmd_init(dev);\n\t\t\tif (err) {\n\t\t\t\t \n\t\t\t\tmlx4_err(dev, \"Failed to init VHCR command interface, aborting\\n\");\n\t\t\t\tgoto err_close;\n\t\t\t}\n\t\t} else {\n\t\t\tdev->flags = dev_flags;\n\t\t}\n\n\t\tif (!SRIOV_VALID_STATE(dev->flags)) {\n\t\t\tmlx4_err(dev, \"Invalid SRIOV state\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_close;\n\t\t}\n\t}\n\n\t \n\tif (!mlx4_is_slave(dev))\n\t\tpcie_print_link_status(dev->persist->pdev);\n\n\t \n\tif (mlx4_is_master(dev)) {\n\t\tif (dev->caps.num_ports < 2 &&\n\t\t    num_vfs_argc > 1) {\n\t\t\terr = -EINVAL;\n\t\t\tmlx4_err(dev,\n\t\t\t\t \"Error: Trying to configure VFs on port 2, but HCA has only %d physical ports\\n\",\n\t\t\t\t dev->caps.num_ports);\n\t\t\tgoto err_close;\n\t\t}\n\t\tmemcpy(dev->persist->nvfs, nvfs, sizeof(dev->persist->nvfs));\n\n\t\tfor (i = 0;\n\t\t     i < sizeof(dev->persist->nvfs)/\n\t\t     sizeof(dev->persist->nvfs[0]); i++) {\n\t\t\tunsigned j;\n\n\t\t\tfor (j = 0; j < dev->persist->nvfs[i]; ++sum, ++j) {\n\t\t\t\tdev->dev_vfs[sum].min_port = i < 2 ? i + 1 : 1;\n\t\t\t\tdev->dev_vfs[sum].n_ports = i < 2 ? 1 :\n\t\t\t\t\tdev->caps.num_ports;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\terr = mlx4_multi_func_init(dev);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"Failed to init master mfunc interface, aborting.\\n\");\n\t\t\tgoto err_close;\n\t\t}\n\t}\n\n\terr = mlx4_alloc_eq_table(dev);\n\tif (err)\n\t\tgoto err_master_mfunc;\n\n\tbitmap_zero(priv->msix_ctl.pool_bm, MAX_MSIX);\n\tmutex_init(&priv->msix_ctl.pool_lock);\n\n\tmlx4_enable_msi_x(dev);\n\tif ((mlx4_is_mfunc(dev)) &&\n\t    !(dev->flags & MLX4_FLAG_MSI_X)) {\n\t\terr = -EOPNOTSUPP;\n\t\tmlx4_err(dev, \"INTx is not supported in multi-function mode, aborting\\n\");\n\t\tgoto err_free_eq;\n\t}\n\n\tif (!mlx4_is_slave(dev)) {\n\t\terr = mlx4_init_steering(dev);\n\t\tif (err)\n\t\t\tgoto err_disable_msix;\n\t}\n\n\tmlx4_init_quotas(dev);\n\n\terr = mlx4_setup_hca(dev);\n\tif (err == -EBUSY && (dev->flags & MLX4_FLAG_MSI_X) &&\n\t    !mlx4_is_mfunc(dev)) {\n\t\tdev->flags &= ~MLX4_FLAG_MSI_X;\n\t\tdev->caps.num_comp_vectors = 1;\n\t\tpci_disable_msix(pdev);\n\t\terr = mlx4_setup_hca(dev);\n\t}\n\n\tif (err)\n\t\tgoto err_steer;\n\n\t \n\tif (mlx4_is_master(dev)) {\n\t\terr = mlx4_ARM_COMM_CHANNEL(dev);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \" Failed to arm comm channel eq: %x\\n\",\n\t\t\t\t err);\n\t\t\tgoto err_steer;\n\t\t}\n\t}\n\n\tfor (port = 1; port <= dev->caps.num_ports; port++) {\n\t\terr = mlx4_init_port_info(dev, port);\n\t\tif (err)\n\t\t\tgoto err_port;\n\t}\n\n\tpriv->v2p.port1 = 1;\n\tpriv->v2p.port2 = 2;\n\n\terr = mlx4_register_device(dev);\n\tif (err)\n\t\tgoto err_port;\n\n\tmlx4_sense_init(dev);\n\tmlx4_start_sense(dev);\n\n\tpriv->removed = 0;\n\n\tif (mlx4_is_master(dev) && dev->persist->num_vfs && !reset_flow)\n\t\tatomic_dec(&pf_loading);\n\n\tkfree(dev_cap);\n\treturn 0;\n\nerr_port:\n\tfor (--port; port >= 1; --port)\n\t\tmlx4_cleanup_port_info(&priv->port[port]);\n\n\tmlx4_cleanup_default_counters(dev);\n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_cleanup_counters_table(dev);\n\tmlx4_cleanup_qp_table(dev);\n\tmlx4_cleanup_srq_table(dev);\n\tmlx4_cleanup_cq_table(dev);\n\tmlx4_cmd_use_polling(dev);\n\tmlx4_cleanup_eq_table(dev);\n\tmlx4_cleanup_mcg_table(dev);\n\tmlx4_cleanup_mr_table(dev);\n\tmlx4_cleanup_xrcd_table(dev);\n\tmlx4_cleanup_pd_table(dev);\n\tmlx4_cleanup_uar_table(dev);\n\nerr_steer:\n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_clear_steering(dev);\n\nerr_disable_msix:\n\tif (dev->flags & MLX4_FLAG_MSI_X)\n\t\tpci_disable_msix(pdev);\n\nerr_free_eq:\n\tmlx4_free_eq_table(dev);\n\nerr_master_mfunc:\n\tif (mlx4_is_master(dev)) {\n\t\tmlx4_free_resource_tracker(dev, RES_TR_FREE_STRUCTS_ONLY);\n\t\tmlx4_multi_func_cleanup(dev);\n\t}\n\n\tif (mlx4_is_slave(dev))\n\t\tmlx4_slave_destroy_special_qp_cap(dev);\n\nerr_close:\n\tmlx4_close_hca(dev);\n\nerr_fw:\n\tmlx4_close_fw(dev);\n\nerr_mfunc:\n\tif (mlx4_is_slave(dev))\n\t\tmlx4_multi_func_cleanup(dev);\n\nerr_cmd:\n\tmlx4_cmd_cleanup(dev, MLX4_CMD_CLEANUP_ALL);\n\nerr_sriov:\n\tif (dev->flags & MLX4_FLAG_SRIOV && !existing_vfs) {\n\t\tpci_disable_sriov(pdev);\n\t\tdev->flags &= ~MLX4_FLAG_SRIOV;\n\t}\n\n\tif (mlx4_is_master(dev) && dev->persist->num_vfs && !reset_flow)\n\t\tatomic_dec(&pf_loading);\n\n\tkfree(priv->dev.dev_vfs);\n\n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_free_ownership(dev);\n\n\tkfree(dev_cap);\n\nerr_adev:\n\tmlx4_adev_cleanup(dev);\n\treturn err;\n}\n\nstatic int __mlx4_init_one(struct pci_dev *pdev, int pci_dev_data,\n\t\t\t   struct mlx4_priv *priv)\n{\n\tint err;\n\tint nvfs[MLX4_MAX_PORTS + 1] = {0, 0, 0};\n\tint prb_vf[MLX4_MAX_PORTS + 1] = {0, 0, 0};\n\tconst int param_map[MLX4_MAX_PORTS + 1][MLX4_MAX_PORTS + 1] = {\n\t\t{2, 0, 0}, {0, 1, 2}, {0, 1, 2} };\n\tunsigned total_vfs = 0;\n\tunsigned int i;\n\n\tpr_info(DRV_NAME \": Initializing %s\\n\", pci_name(pdev));\n\n\terr = mlx4_pci_enable_device(&priv->dev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot enable PCI device, aborting\\n\");\n\t\treturn err;\n\t}\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(nvfs) && i < num_vfs_argc;\n\t     total_vfs += nvfs[param_map[num_vfs_argc - 1][i]], i++) {\n\t\tnvfs[param_map[num_vfs_argc - 1][i]] = num_vfs[i];\n\t\tif (nvfs[i] < 0) {\n\t\t\tdev_err(&pdev->dev, \"num_vfs module parameter cannot be negative\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_disable_pdev;\n\t\t}\n\t}\n\tfor (i = 0; i < ARRAY_SIZE(prb_vf) && i < probe_vfs_argc;\n\t     i++) {\n\t\tprb_vf[param_map[probe_vfs_argc - 1][i]] = probe_vf[i];\n\t\tif (prb_vf[i] < 0 || prb_vf[i] > nvfs[i]) {\n\t\t\tdev_err(&pdev->dev, \"probe_vf module parameter cannot be negative or greater than num_vfs\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_disable_pdev;\n\t\t}\n\t}\n\tif (total_vfs > MLX4_MAX_NUM_VF) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Requested more VF's (%d) than allowed by hw (%d)\\n\",\n\t\t\ttotal_vfs, MLX4_MAX_NUM_VF);\n\t\terr = -EINVAL;\n\t\tgoto err_disable_pdev;\n\t}\n\n\tfor (i = 0; i < MLX4_MAX_PORTS; i++) {\n\t\tif (nvfs[i] + nvfs[2] > MLX4_MAX_NUM_VF_P_PORT) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"Requested more VF's (%d) for port (%d) than allowed by driver (%d)\\n\",\n\t\t\t\tnvfs[i] + nvfs[2], i + 1,\n\t\t\t\tMLX4_MAX_NUM_VF_P_PORT);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_disable_pdev;\n\t\t}\n\t}\n\n\t \n\tif (!(pci_dev_data & MLX4_PCI_DEV_IS_VF) &&\n\t    !(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) {\n\t\tdev_err(&pdev->dev, \"Missing DCS, aborting (driver_data: 0x%x, pci_resource_flags(pdev, 0):0x%lx)\\n\",\n\t\t\tpci_dev_data, pci_resource_flags(pdev, 0));\n\t\terr = -ENODEV;\n\t\tgoto err_disable_pdev;\n\t}\n\tif (!(pci_resource_flags(pdev, 2) & IORESOURCE_MEM)) {\n\t\tdev_err(&pdev->dev, \"Missing UAR, aborting\\n\");\n\t\terr = -ENODEV;\n\t\tgoto err_disable_pdev;\n\t}\n\n\terr = pci_request_regions(pdev, DRV_NAME);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Couldn't get PCI resources, aborting\\n\");\n\t\tgoto err_disable_pdev;\n\t}\n\n\tpci_set_master(pdev);\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_warn(&pdev->dev, \"Warning: couldn't set 64-bit PCI DMA mask\\n\");\n\t\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev, \"Can't set PCI DMA mask, aborting\\n\");\n\t\t\tgoto err_release_regions;\n\t\t}\n\t}\n\n\t \n\tdma_set_max_seg_size(&pdev->dev, 1024 * 1024 * 1024);\n\t \n\tif (pci_dev_data & MLX4_PCI_DEV_IS_VF) {\n\t\t \n\t\tif (total_vfs) {\n\t\t\tunsigned vfs_offset = 0;\n\n\t\t\tfor (i = 0; i < ARRAY_SIZE(nvfs) &&\n\t\t\t     vfs_offset + nvfs[i] < extended_func_num(pdev);\n\t\t\t     vfs_offset += nvfs[i], i++)\n\t\t\t\t;\n\t\t\tif (i == ARRAY_SIZE(nvfs)) {\n\t\t\t\terr = -ENODEV;\n\t\t\t\tgoto err_release_regions;\n\t\t\t}\n\t\t\tif ((extended_func_num(pdev) - vfs_offset)\n\t\t\t    > prb_vf[i]) {\n\t\t\t\tdev_warn(&pdev->dev, \"Skipping virtual function:%d\\n\",\n\t\t\t\t\t extended_func_num(pdev));\n\t\t\t\terr = -ENODEV;\n\t\t\t\tgoto err_release_regions;\n\t\t\t}\n\t\t}\n\t}\n\n\terr = mlx4_crdump_init(&priv->dev);\n\tif (err)\n\t\tgoto err_release_regions;\n\n\terr = mlx4_catas_init(&priv->dev);\n\tif (err)\n\t\tgoto err_crdump;\n\n\terr = mlx4_load_one(pdev, pci_dev_data, total_vfs, nvfs, priv, 0);\n\tif (err)\n\t\tgoto err_catas;\n\n\treturn 0;\n\nerr_catas:\n\tmlx4_catas_end(&priv->dev);\n\nerr_crdump:\n\tmlx4_crdump_end(&priv->dev);\n\nerr_release_regions:\n\tpci_release_regions(pdev);\n\nerr_disable_pdev:\n\tmlx4_pci_disable_device(&priv->dev);\n\treturn err;\n}\n\nstatic void mlx4_devlink_param_load_driverinit_values(struct devlink *devlink)\n{\n\tstruct mlx4_priv *priv = devlink_priv(devlink);\n\tstruct mlx4_dev *dev = &priv->dev;\n\tstruct mlx4_fw_crdump *crdump = &dev->persist->crdump;\n\tunion devlink_param_value saved_value;\n\tint err;\n\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      DEVLINK_PARAM_GENERIC_ID_INT_ERR_RESET,\n\t\t\t\t\t      &saved_value);\n\tif (!err && mlx4_internal_err_reset != saved_value.vbool) {\n\t\tmlx4_internal_err_reset = saved_value.vbool;\n\t\t \n\t\tdevl_param_value_changed(devlink,\n\t\t\t\t\t DEVLINK_PARAM_GENERIC_ID_INT_ERR_RESET);\n\t}\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      DEVLINK_PARAM_GENERIC_ID_MAX_MACS,\n\t\t\t\t\t      &saved_value);\n\tif (!err)\n\t\tlog_num_mac = order_base_2(saved_value.vu32);\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      MLX4_DEVLINK_PARAM_ID_ENABLE_64B_CQE_EQE,\n\t\t\t\t\t      &saved_value);\n\tif (!err)\n\t\tenable_64b_cqe_eqe = saved_value.vbool;\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      MLX4_DEVLINK_PARAM_ID_ENABLE_4K_UAR,\n\t\t\t\t\t      &saved_value);\n\tif (!err)\n\t\tenable_4k_uar = saved_value.vbool;\n\terr = devl_param_driverinit_value_get(devlink,\n\t\t\t\t\t      DEVLINK_PARAM_GENERIC_ID_REGION_SNAPSHOT,\n\t\t\t\t\t      &saved_value);\n\tif (!err && crdump->snapshot_enable != saved_value.vbool) {\n\t\tcrdump->snapshot_enable = saved_value.vbool;\n\t\tdevl_param_value_changed(devlink,\n\t\t\t\t\t DEVLINK_PARAM_GENERIC_ID_REGION_SNAPSHOT);\n\t}\n}\n\nstatic void mlx4_restart_one_down(struct pci_dev *pdev);\nstatic int mlx4_restart_one_up(struct pci_dev *pdev, bool reload,\n\t\t\t       struct devlink *devlink);\n\nstatic int mlx4_devlink_reload_down(struct devlink *devlink, bool netns_change,\n\t\t\t\t    enum devlink_reload_action action,\n\t\t\t\t    enum devlink_reload_limit limit,\n\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct mlx4_priv *priv = devlink_priv(devlink);\n\tstruct mlx4_dev *dev = &priv->dev;\n\tstruct mlx4_dev_persistent *persist = dev->persist;\n\n\tif (netns_change) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Namespace change is not supported\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (persist->num_vfs)\n\t\tmlx4_warn(persist->dev, \"Reload performed on PF, will cause reset on operating Virtual Functions\\n\");\n\tmlx4_restart_one_down(persist->pdev);\n\treturn 0;\n}\n\nstatic int mlx4_devlink_reload_up(struct devlink *devlink, enum devlink_reload_action action,\n\t\t\t\t  enum devlink_reload_limit limit, u32 *actions_performed,\n\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct mlx4_priv *priv = devlink_priv(devlink);\n\tstruct mlx4_dev *dev = &priv->dev;\n\tstruct mlx4_dev_persistent *persist = dev->persist;\n\tint err;\n\n\t*actions_performed = BIT(DEVLINK_RELOAD_ACTION_DRIVER_REINIT);\n\terr = mlx4_restart_one_up(persist->pdev, true, devlink);\n\tif (err)\n\t\tmlx4_err(persist->dev, \"mlx4_restart_one_up failed, ret=%d\\n\",\n\t\t\t err);\n\n\treturn err;\n}\n\nstatic const struct devlink_ops mlx4_devlink_ops = {\n\t.reload_actions = BIT(DEVLINK_RELOAD_ACTION_DRIVER_REINIT),\n\t.reload_down\t= mlx4_devlink_reload_down,\n\t.reload_up\t= mlx4_devlink_reload_up,\n};\n\nstatic int mlx4_init_one(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct devlink *devlink;\n\tstruct mlx4_priv *priv;\n\tstruct mlx4_dev *dev;\n\tint ret;\n\n\tprintk_once(KERN_INFO \"%s\", mlx4_version);\n\n\tdevlink = devlink_alloc(&mlx4_devlink_ops, sizeof(*priv), &pdev->dev);\n\tif (!devlink)\n\t\treturn -ENOMEM;\n\tdevl_lock(devlink);\n\tpriv = devlink_priv(devlink);\n\n\tdev       = &priv->dev;\n\tdev->persist = kzalloc(sizeof(*dev->persist), GFP_KERNEL);\n\tif (!dev->persist) {\n\t\tret = -ENOMEM;\n\t\tgoto err_devlink_free;\n\t}\n\tdev->persist->pdev = pdev;\n\tdev->persist->dev = dev;\n\tpci_set_drvdata(pdev, dev->persist);\n\tpriv->pci_dev_data = id->driver_data;\n\tmutex_init(&dev->persist->device_state_mutex);\n\tmutex_init(&dev->persist->interface_state_mutex);\n\tmutex_init(&dev->persist->pci_status_mutex);\n\n\tret = devl_params_register(devlink, mlx4_devlink_params,\n\t\t\t\t   ARRAY_SIZE(mlx4_devlink_params));\n\tif (ret)\n\t\tgoto err_devlink_unregister;\n\tmlx4_devlink_set_params_init_values(devlink);\n\tret =  __mlx4_init_one(pdev, id->driver_data, priv);\n\tif (ret)\n\t\tgoto err_params_unregister;\n\n\tpci_save_state(pdev);\n\tdevl_unlock(devlink);\n\tdevlink_register(devlink);\n\treturn 0;\n\nerr_params_unregister:\n\tdevl_params_unregister(devlink, mlx4_devlink_params,\n\t\t\t       ARRAY_SIZE(mlx4_devlink_params));\nerr_devlink_unregister:\n\tkfree(dev->persist);\nerr_devlink_free:\n\tdevl_unlock(devlink);\n\tdevlink_free(devlink);\n\treturn ret;\n}\n\nstatic void mlx4_clean_dev(struct mlx4_dev *dev)\n{\n\tstruct mlx4_dev_persistent *persist = dev->persist;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tunsigned long\tflags = (dev->flags & RESET_PERSIST_MASK_FLAGS);\n\n\tmemset(priv, 0, sizeof(*priv));\n\tpriv->dev.persist = persist;\n\tpriv->dev.flags = flags;\n}\n\nstatic void mlx4_unload_one(struct pci_dev *pdev)\n{\n\tstruct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);\n\tstruct mlx4_dev  *dev  = persist->dev;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint               pci_dev_data;\n\tstruct devlink *devlink;\n\tint p, i;\n\n\tdevlink = priv_to_devlink(priv);\n\tdevl_assert_locked(devlink);\n\tif (priv->removed)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < dev->caps.num_ports; i++) {\n\t\tdev->persist->curr_port_type[i] = dev->caps.port_type[i + 1];\n\t\tdev->persist->curr_port_poss_type[i] = dev->caps.\n\t\t\t\t\t\t       possible_type[i + 1];\n\t}\n\n\tpci_dev_data = priv->pci_dev_data;\n\n\tmlx4_stop_sense(dev);\n\tmlx4_unregister_device(dev);\n\n\tfor (p = 1; p <= dev->caps.num_ports; p++) {\n\t\tmlx4_cleanup_port_info(&priv->port[p]);\n\t\tmlx4_CLOSE_PORT(dev, p);\n\t}\n\n\tif (mlx4_is_master(dev))\n\t\tmlx4_free_resource_tracker(dev,\n\t\t\t\t\t   RES_TR_FREE_SLAVES_ONLY);\n\n\tmlx4_cleanup_default_counters(dev);\n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_cleanup_counters_table(dev);\n\tmlx4_cleanup_qp_table(dev);\n\tmlx4_cleanup_srq_table(dev);\n\tmlx4_cleanup_cq_table(dev);\n\tmlx4_cmd_use_polling(dev);\n\tmlx4_cleanup_eq_table(dev);\n\tmlx4_cleanup_mcg_table(dev);\n\tmlx4_cleanup_mr_table(dev);\n\tmlx4_cleanup_xrcd_table(dev);\n\tmlx4_cleanup_pd_table(dev);\n\n\tif (mlx4_is_master(dev))\n\t\tmlx4_free_resource_tracker(dev,\n\t\t\t\t\t   RES_TR_FREE_STRUCTS_ONLY);\n\n\tiounmap(priv->kar);\n\tmlx4_uar_free(dev, &priv->driver_uar);\n\tmlx4_cleanup_uar_table(dev);\n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_clear_steering(dev);\n\tmlx4_free_eq_table(dev);\n\tif (mlx4_is_master(dev))\n\t\tmlx4_multi_func_cleanup(dev);\n\tmlx4_close_hca(dev);\n\tmlx4_close_fw(dev);\n\tif (mlx4_is_slave(dev))\n\t\tmlx4_multi_func_cleanup(dev);\n\tmlx4_cmd_cleanup(dev, MLX4_CMD_CLEANUP_ALL);\n\n\tif (dev->flags & MLX4_FLAG_MSI_X)\n\t\tpci_disable_msix(pdev);\n\n\tif (!mlx4_is_slave(dev))\n\t\tmlx4_free_ownership(dev);\n\n\tmlx4_slave_destroy_special_qp_cap(dev);\n\tkfree(dev->dev_vfs);\n\n\tmlx4_adev_cleanup(dev);\n\n\tmlx4_clean_dev(dev);\n\tpriv->pci_dev_data = pci_dev_data;\n\tpriv->removed = 1;\n}\n\nstatic void mlx4_remove_one(struct pci_dev *pdev)\n{\n\tstruct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);\n\tstruct mlx4_dev  *dev  = persist->dev;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct devlink *devlink = priv_to_devlink(priv);\n\tint active_vfs = 0;\n\n\tdevlink_unregister(devlink);\n\n\tdevl_lock(devlink);\n\tif (mlx4_is_slave(dev))\n\t\tpersist->interface_state |= MLX4_INTERFACE_STATE_NOWAIT;\n\n\tmutex_lock(&persist->interface_state_mutex);\n\tpersist->interface_state |= MLX4_INTERFACE_STATE_DELETION;\n\tmutex_unlock(&persist->interface_state_mutex);\n\n\t \n\tif (mlx4_is_master(dev) && dev->flags & MLX4_FLAG_SRIOV) {\n\t\tactive_vfs = mlx4_how_many_lives_vf(dev);\n\t\tif (active_vfs) {\n\t\t\tpr_warn(\"Removing PF when there are active VF's !!\\n\");\n\t\t\tpr_warn(\"Will not disable SR-IOV.\\n\");\n\t\t}\n\t}\n\n\t \n\tif (persist->interface_state & MLX4_INTERFACE_STATE_UP)\n\t\tmlx4_unload_one(pdev);\n\telse\n\t\tmlx4_info(dev, \"%s: interface is down\\n\", __func__);\n\tmlx4_catas_end(dev);\n\tmlx4_crdump_end(dev);\n\tif (dev->flags & MLX4_FLAG_SRIOV && !active_vfs) {\n\t\tmlx4_warn(dev, \"Disabling SR-IOV\\n\");\n\t\tpci_disable_sriov(pdev);\n\t}\n\n\tpci_release_regions(pdev);\n\tmlx4_pci_disable_device(dev);\n\tdevl_params_unregister(devlink, mlx4_devlink_params,\n\t\t\t       ARRAY_SIZE(mlx4_devlink_params));\n\tkfree(dev->persist);\n\tdevl_unlock(devlink);\n\tdevlink_free(devlink);\n}\n\nstatic int restore_current_port_types(struct mlx4_dev *dev,\n\t\t\t\t      enum mlx4_port_type *types,\n\t\t\t\t      enum mlx4_port_type *poss_types)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint err, i;\n\n\tmlx4_stop_sense(dev);\n\n\tmutex_lock(&priv->port_mutex);\n\tfor (i = 0; i < dev->caps.num_ports; i++)\n\t\tdev->caps.possible_type[i + 1] = poss_types[i];\n\terr = mlx4_change_port_types(dev, types);\n\tmlx4_start_sense(dev);\n\tmutex_unlock(&priv->port_mutex);\n\n\treturn err;\n}\n\nstatic void mlx4_restart_one_down(struct pci_dev *pdev)\n{\n\tmlx4_unload_one(pdev);\n}\n\nstatic int mlx4_restart_one_up(struct pci_dev *pdev, bool reload,\n\t\t\t       struct devlink *devlink)\n{\n\tstruct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);\n\tstruct mlx4_dev\t *dev  = persist->dev;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint nvfs[MLX4_MAX_PORTS + 1] = {0, 0, 0};\n\tint pci_dev_data, err, total_vfs;\n\n\tpci_dev_data = priv->pci_dev_data;\n\ttotal_vfs = dev->persist->num_vfs;\n\tmemcpy(nvfs, dev->persist->nvfs, sizeof(dev->persist->nvfs));\n\n\tif (reload)\n\t\tmlx4_devlink_param_load_driverinit_values(devlink);\n\terr = mlx4_load_one(pdev, pci_dev_data, total_vfs, nvfs, priv, 1);\n\tif (err) {\n\t\tmlx4_err(dev, \"%s: ERROR: mlx4_load_one failed, pci_name=%s, err=%d\\n\",\n\t\t\t __func__, pci_name(pdev), err);\n\t\treturn err;\n\t}\n\n\terr = restore_current_port_types(dev, dev->persist->curr_port_type,\n\t\t\t\t\t dev->persist->curr_port_poss_type);\n\tif (err)\n\t\tmlx4_err(dev, \"could not restore original port types (%d)\\n\",\n\t\t\t err);\n\n\treturn err;\n}\n\nint mlx4_restart_one(struct pci_dev *pdev)\n{\n\tmlx4_restart_one_down(pdev);\n\treturn mlx4_restart_one_up(pdev, false, NULL);\n}\n\n#define MLX_SP(id) { PCI_VDEVICE(MELLANOX, id), MLX4_PCI_DEV_FORCE_SENSE_PORT }\n#define MLX_VF(id) { PCI_VDEVICE(MELLANOX, id), MLX4_PCI_DEV_IS_VF }\n#define MLX_GN(id) { PCI_VDEVICE(MELLANOX, id), 0 }\n\nstatic const struct pci_device_id mlx4_pci_table[] = {\n#ifdef CONFIG_MLX4_CORE_GEN2\n\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_HERMON_SDR),\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_HERMON_DDR),\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_HERMON_QDR),\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_HERMON_DDR_GEN2),  \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_HERMON_QDR_GEN2),\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_HERMON_EN),\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_HERMON_EN_GEN2),   \n\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_CONNECTX_EN),\n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_CONNECTX_EN_T_GEN2),\t \n\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_CONNECTX_EN_GEN2),\n\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_CONNECTX_EN_5_GEN2),\n\t \n\tMLX_SP(PCI_DEVICE_ID_MELLANOX_CONNECTX2),\n\t \n\tMLX_VF(0x1002),\t\t\t\t\t \n#endif  \n\t \n\tMLX_GN(PCI_DEVICE_ID_MELLANOX_CONNECTX3),\n\tMLX_VF(0x1004),\t\t\t\t\t \n\tMLX_GN(0x1005),\t\t\t\t\t \n\tMLX_GN(0x1006),\t\t\t\t\t \n\tMLX_GN(PCI_DEVICE_ID_MELLANOX_CONNECTX3_PRO),\t \n\tMLX_GN(0x1008),\t\t\t\t\t \n\tMLX_GN(0x1009),\t\t\t\t\t \n\tMLX_GN(0x100a),\t\t\t\t\t \n\tMLX_GN(0x100b),\t\t\t\t\t \n\tMLX_GN(0x100c),\t\t\t\t\t \n\tMLX_GN(0x100d),\t\t\t\t\t \n\tMLX_GN(0x100e),\t\t\t\t\t \n\tMLX_GN(0x100f),\t\t\t\t\t \n\tMLX_GN(0x1010),\t\t\t\t\t \n\n\t \n\n\t{ 0, }\n};\n\nMODULE_DEVICE_TABLE(pci, mlx4_pci_table);\n\nstatic pci_ers_result_t mlx4_pci_err_detected(struct pci_dev *pdev,\n\t\t\t\t\t      pci_channel_state_t state)\n{\n\tstruct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);\n\tstruct mlx4_dev *dev = persist->dev;\n\tstruct devlink *devlink;\n\n\tmlx4_err(persist->dev, \"mlx4_pci_err_detected was called\\n\");\n\tmlx4_enter_error_state(persist);\n\n\tdevlink = priv_to_devlink(mlx4_priv(dev));\n\tdevl_lock(devlink);\n\tmutex_lock(&persist->interface_state_mutex);\n\tif (persist->interface_state & MLX4_INTERFACE_STATE_UP)\n\t\tmlx4_unload_one(pdev);\n\n\tmutex_unlock(&persist->interface_state_mutex);\n\tdevl_unlock(devlink);\n\tif (state == pci_channel_io_perm_failure)\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\tmlx4_pci_disable_device(persist->dev);\n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\nstatic pci_ers_result_t mlx4_pci_slot_reset(struct pci_dev *pdev)\n{\n\tstruct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);\n\tstruct mlx4_dev\t *dev  = persist->dev;\n\tint err;\n\n\tmlx4_err(dev, \"mlx4_pci_slot_reset was called\\n\");\n\terr = mlx4_pci_enable_device(dev);\n\tif (err) {\n\t\tmlx4_err(dev, \"Can not re-enable device, err=%d\\n\", err);\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\tpci_set_master(pdev);\n\tpci_restore_state(pdev);\n\tpci_save_state(pdev);\n\treturn PCI_ERS_RESULT_RECOVERED;\n}\n\nstatic void mlx4_pci_resume(struct pci_dev *pdev)\n{\n\tstruct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);\n\tstruct mlx4_dev\t *dev  = persist->dev;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint nvfs[MLX4_MAX_PORTS + 1] = {0, 0, 0};\n\tstruct devlink *devlink;\n\tint total_vfs;\n\tint err;\n\n\tmlx4_err(dev, \"%s was called\\n\", __func__);\n\ttotal_vfs = dev->persist->num_vfs;\n\tmemcpy(nvfs, dev->persist->nvfs, sizeof(dev->persist->nvfs));\n\n\tdevlink = priv_to_devlink(priv);\n\tdevl_lock(devlink);\n\tmutex_lock(&persist->interface_state_mutex);\n\tif (!(persist->interface_state & MLX4_INTERFACE_STATE_UP)) {\n\t\terr = mlx4_load_one(pdev, priv->pci_dev_data, total_vfs, nvfs,\n\t\t\t\t    priv, 1);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"%s: mlx4_load_one failed, err=%d\\n\",\n\t\t\t\t __func__,  err);\n\t\t\tgoto end;\n\t\t}\n\n\t\terr = restore_current_port_types(dev, dev->persist->\n\t\t\t\t\t\t curr_port_type, dev->persist->\n\t\t\t\t\t\t curr_port_poss_type);\n\t\tif (err)\n\t\t\tmlx4_err(dev, \"could not restore original port types (%d)\\n\", err);\n\t}\nend:\n\tmutex_unlock(&persist->interface_state_mutex);\n\tdevl_unlock(devlink);\n}\n\nstatic void mlx4_shutdown(struct pci_dev *pdev)\n{\n\tstruct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);\n\tstruct mlx4_dev *dev = persist->dev;\n\tstruct devlink *devlink;\n\n\tmlx4_info(persist->dev, \"mlx4_shutdown was called\\n\");\n\tdevlink = priv_to_devlink(mlx4_priv(dev));\n\tdevl_lock(devlink);\n\tmutex_lock(&persist->interface_state_mutex);\n\tif (persist->interface_state & MLX4_INTERFACE_STATE_UP)\n\t\tmlx4_unload_one(pdev);\n\tmutex_unlock(&persist->interface_state_mutex);\n\tdevl_unlock(devlink);\n\tmlx4_pci_disable_device(dev);\n}\n\nstatic const struct pci_error_handlers mlx4_err_handler = {\n\t.error_detected = mlx4_pci_err_detected,\n\t.slot_reset     = mlx4_pci_slot_reset,\n\t.resume\t\t= mlx4_pci_resume,\n};\n\nstatic int __maybe_unused mlx4_suspend(struct device *dev_d)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev_d);\n\tstruct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);\n\tstruct mlx4_dev\t*dev = persist->dev;\n\tstruct devlink *devlink;\n\n\tmlx4_err(dev, \"suspend was called\\n\");\n\tdevlink = priv_to_devlink(mlx4_priv(dev));\n\tdevl_lock(devlink);\n\tmutex_lock(&persist->interface_state_mutex);\n\tif (persist->interface_state & MLX4_INTERFACE_STATE_UP)\n\t\tmlx4_unload_one(pdev);\n\tmutex_unlock(&persist->interface_state_mutex);\n\tdevl_unlock(devlink);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused mlx4_resume(struct device *dev_d)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev_d);\n\tstruct mlx4_dev_persistent *persist = pci_get_drvdata(pdev);\n\tstruct mlx4_dev\t*dev = persist->dev;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint nvfs[MLX4_MAX_PORTS + 1] = {0, 0, 0};\n\tstruct devlink *devlink;\n\tint total_vfs;\n\tint ret = 0;\n\n\tmlx4_err(dev, \"resume was called\\n\");\n\ttotal_vfs = dev->persist->num_vfs;\n\tmemcpy(nvfs, dev->persist->nvfs, sizeof(dev->persist->nvfs));\n\n\tdevlink = priv_to_devlink(priv);\n\tdevl_lock(devlink);\n\tmutex_lock(&persist->interface_state_mutex);\n\tif (!(persist->interface_state & MLX4_INTERFACE_STATE_UP)) {\n\t\tret = mlx4_load_one(pdev, priv->pci_dev_data, total_vfs,\n\t\t\t\t    nvfs, priv, 1);\n\t\tif (!ret) {\n\t\t\tret = restore_current_port_types(dev,\n\t\t\t\t\tdev->persist->curr_port_type,\n\t\t\t\t\tdev->persist->curr_port_poss_type);\n\t\t\tif (ret)\n\t\t\t\tmlx4_err(dev, \"resume: could not restore original port types (%d)\\n\", ret);\n\t\t}\n\t}\n\tmutex_unlock(&persist->interface_state_mutex);\n\tdevl_unlock(devlink);\n\n\treturn ret;\n}\n\nstatic SIMPLE_DEV_PM_OPS(mlx4_pm_ops, mlx4_suspend, mlx4_resume);\n\nstatic struct pci_driver mlx4_driver = {\n\t.name\t\t= DRV_NAME,\n\t.id_table\t= mlx4_pci_table,\n\t.probe\t\t= mlx4_init_one,\n\t.shutdown\t= mlx4_shutdown,\n\t.remove\t\t= mlx4_remove_one,\n\t.driver.pm\t= &mlx4_pm_ops,\n\t.err_handler    = &mlx4_err_handler,\n};\n\nstatic int __init mlx4_verify_params(void)\n{\n\tif (msi_x < 0) {\n\t\tpr_warn(\"mlx4_core: bad msi_x: %d\\n\", msi_x);\n\t\treturn -1;\n\t}\n\n\tif ((log_num_mac < 0) || (log_num_mac > 7)) {\n\t\tpr_warn(\"mlx4_core: bad num_mac: %d\\n\", log_num_mac);\n\t\treturn -1;\n\t}\n\n\tif (log_num_vlan != 0)\n\t\tpr_warn(\"mlx4_core: log_num_vlan - obsolete module param, using %d\\n\",\n\t\t\tMLX4_LOG_NUM_VLANS);\n\n\tif (use_prio != 0)\n\t\tpr_warn(\"mlx4_core: use_prio - obsolete module param, ignored\\n\");\n\n\tif ((log_mtts_per_seg < 0) || (log_mtts_per_seg > 7)) {\n\t\tpr_warn(\"mlx4_core: bad log_mtts_per_seg: %d\\n\",\n\t\t\tlog_mtts_per_seg);\n\t\treturn -1;\n\t}\n\n\t \n\tif (port_type_array[0] == false && port_type_array[1] == true) {\n\t\tpr_warn(\"Module parameter configuration ETH/IB is not supported. Switching to default configuration IB/IB\\n\");\n\t\tport_type_array[0] = true;\n\t}\n\n\tif (mlx4_log_num_mgm_entry_size < -7 ||\n\t    (mlx4_log_num_mgm_entry_size > 0 &&\n\t     (mlx4_log_num_mgm_entry_size < MLX4_MIN_MGM_LOG_ENTRY_SIZE ||\n\t      mlx4_log_num_mgm_entry_size > MLX4_MAX_MGM_LOG_ENTRY_SIZE))) {\n\t\tpr_warn(\"mlx4_core: mlx4_log_num_mgm_entry_size (%d) not in legal range (-7..0 or %d..%d)\\n\",\n\t\t\tmlx4_log_num_mgm_entry_size,\n\t\t\tMLX4_MIN_MGM_LOG_ENTRY_SIZE,\n\t\t\tMLX4_MAX_MGM_LOG_ENTRY_SIZE);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic int __init mlx4_init(void)\n{\n\tint ret;\n\n\tWARN_ONCE(strcmp(MLX4_ADEV_NAME, KBUILD_MODNAME),\n\t\t  \"mlx4_core name not in sync with kernel module name\");\n\n\tif (mlx4_verify_params())\n\t\treturn -EINVAL;\n\n\n\tmlx4_wq = create_singlethread_workqueue(\"mlx4\");\n\tif (!mlx4_wq)\n\t\treturn -ENOMEM;\n\n\tret = pci_register_driver(&mlx4_driver);\n\tif (ret < 0)\n\t\tdestroy_workqueue(mlx4_wq);\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic void __exit mlx4_cleanup(void)\n{\n\tpci_unregister_driver(&mlx4_driver);\n\tdestroy_workqueue(mlx4_wq);\n}\n\nmodule_init(mlx4_init);\nmodule_exit(mlx4_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}