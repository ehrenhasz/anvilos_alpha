{
  "module_name": "alloc.c",
  "hash_id": "106d35fb9c5591bdcfe6a4ded51d410b51f21fa685f3373bee205502a83a452d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx4/alloc.c",
  "human_readable_source": " \n\n#include <linux/errno.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/export.h>\n#include <linux/bitmap.h>\n#include <linux/dma-mapping.h>\n#include <linux/vmalloc.h>\n\n#include \"mlx4.h\"\n\nu32 mlx4_bitmap_alloc(struct mlx4_bitmap *bitmap)\n{\n\tu32 obj;\n\n\tspin_lock(&bitmap->lock);\n\n\tobj = find_next_zero_bit(bitmap->table, bitmap->max, bitmap->last);\n\tif (obj >= bitmap->max) {\n\t\tbitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)\n\t\t\t\t& bitmap->mask;\n\t\tobj = find_first_zero_bit(bitmap->table, bitmap->max);\n\t}\n\n\tif (obj < bitmap->max) {\n\t\tset_bit(obj, bitmap->table);\n\t\tbitmap->last = (obj + 1);\n\t\tif (bitmap->last == bitmap->max)\n\t\t\tbitmap->last = 0;\n\t\tobj |= bitmap->top;\n\t} else\n\t\tobj = -1;\n\n\tif (obj != -1)\n\t\t--bitmap->avail;\n\n\tspin_unlock(&bitmap->lock);\n\n\treturn obj;\n}\n\nvoid mlx4_bitmap_free(struct mlx4_bitmap *bitmap, u32 obj, int use_rr)\n{\n\tmlx4_bitmap_free_range(bitmap, obj, 1, use_rr);\n}\n\nstatic unsigned long find_aligned_range(unsigned long *bitmap,\n\t\t\t\t\tu32 start, u32 nbits,\n\t\t\t\t\tint len, int align, u32 skip_mask)\n{\n\tunsigned long end, i;\n\nagain:\n\tstart = ALIGN(start, align);\n\n\twhile ((start < nbits) && (test_bit(start, bitmap) ||\n\t\t\t\t   (start & skip_mask)))\n\t\tstart += align;\n\n\tif (start >= nbits)\n\t\treturn -1;\n\n\tend = start+len;\n\tif (end > nbits)\n\t\treturn -1;\n\n\tfor (i = start + 1; i < end; i++) {\n\t\tif (test_bit(i, bitmap) || ((u32)i & skip_mask)) {\n\t\t\tstart = i + 1;\n\t\t\tgoto again;\n\t\t}\n\t}\n\n\treturn start;\n}\n\nu32 mlx4_bitmap_alloc_range(struct mlx4_bitmap *bitmap, int cnt,\n\t\t\t    int align, u32 skip_mask)\n{\n\tu32 obj;\n\n\tif (likely(cnt == 1 && align == 1 && !skip_mask))\n\t\treturn mlx4_bitmap_alloc(bitmap);\n\n\tspin_lock(&bitmap->lock);\n\n\tobj = find_aligned_range(bitmap->table, bitmap->last,\n\t\t\t\t bitmap->max, cnt, align, skip_mask);\n\tif (obj >= bitmap->max) {\n\t\tbitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)\n\t\t\t\t& bitmap->mask;\n\t\tobj = find_aligned_range(bitmap->table, 0, bitmap->max,\n\t\t\t\t\t cnt, align, skip_mask);\n\t}\n\n\tif (obj < bitmap->max) {\n\t\tbitmap_set(bitmap->table, obj, cnt);\n\t\tif (obj == bitmap->last) {\n\t\t\tbitmap->last = (obj + cnt);\n\t\t\tif (bitmap->last >= bitmap->max)\n\t\t\t\tbitmap->last = 0;\n\t\t}\n\t\tobj |= bitmap->top;\n\t} else\n\t\tobj = -1;\n\n\tif (obj != -1)\n\t\tbitmap->avail -= cnt;\n\n\tspin_unlock(&bitmap->lock);\n\n\treturn obj;\n}\n\nu32 mlx4_bitmap_avail(struct mlx4_bitmap *bitmap)\n{\n\treturn bitmap->avail;\n}\n\nstatic u32 mlx4_bitmap_masked_value(struct mlx4_bitmap *bitmap, u32 obj)\n{\n\treturn obj & (bitmap->max + bitmap->reserved_top - 1);\n}\n\nvoid mlx4_bitmap_free_range(struct mlx4_bitmap *bitmap, u32 obj, int cnt,\n\t\t\t    int use_rr)\n{\n\tobj &= bitmap->max + bitmap->reserved_top - 1;\n\n\tspin_lock(&bitmap->lock);\n\tif (!use_rr) {\n\t\tbitmap->last = min(bitmap->last, obj);\n\t\tbitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)\n\t\t\t\t& bitmap->mask;\n\t}\n\tbitmap_clear(bitmap->table, obj, cnt);\n\tbitmap->avail += cnt;\n\tspin_unlock(&bitmap->lock);\n}\n\nint mlx4_bitmap_init(struct mlx4_bitmap *bitmap, u32 num, u32 mask,\n\t\t     u32 reserved_bot, u32 reserved_top)\n{\n\t \n\tif (num != roundup_pow_of_two(num))\n\t\treturn -EINVAL;\n\n\tbitmap->last = 0;\n\tbitmap->top  = 0;\n\tbitmap->max  = num - reserved_top;\n\tbitmap->mask = mask;\n\tbitmap->reserved_top = reserved_top;\n\tbitmap->avail = num - reserved_top - reserved_bot;\n\tbitmap->effective_len = bitmap->avail;\n\tspin_lock_init(&bitmap->lock);\n\tbitmap->table = bitmap_zalloc(bitmap->max, GFP_KERNEL);\n\tif (!bitmap->table)\n\t\treturn -ENOMEM;\n\n\tbitmap_set(bitmap->table, 0, reserved_bot);\n\n\treturn 0;\n}\n\nvoid mlx4_bitmap_cleanup(struct mlx4_bitmap *bitmap)\n{\n\tbitmap_free(bitmap->table);\n}\n\nstruct mlx4_zone_allocator {\n\tstruct list_head\t\tentries;\n\tstruct list_head\t\tprios;\n\tu32\t\t\t\tlast_uid;\n\tu32\t\t\t\tmask;\n\t \n\tspinlock_t\t\t\tlock;\n\tenum mlx4_zone_alloc_flags\tflags;\n};\n\nstruct mlx4_zone_entry {\n\tstruct list_head\t\tlist;\n\tstruct list_head\t\tprio_list;\n\tu32\t\t\t\tuid;\n\tstruct mlx4_zone_allocator\t*allocator;\n\tstruct mlx4_bitmap\t\t*bitmap;\n\tint\t\t\t\tuse_rr;\n\tint\t\t\t\tpriority;\n\tint\t\t\t\toffset;\n\tenum mlx4_zone_flags\t\tflags;\n};\n\nstruct mlx4_zone_allocator *mlx4_zone_allocator_create(enum mlx4_zone_alloc_flags flags)\n{\n\tstruct mlx4_zone_allocator *zones = kmalloc(sizeof(*zones), GFP_KERNEL);\n\n\tif (NULL == zones)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&zones->entries);\n\tINIT_LIST_HEAD(&zones->prios);\n\tspin_lock_init(&zones->lock);\n\tzones->last_uid = 0;\n\tzones->mask = 0;\n\tzones->flags = flags;\n\n\treturn zones;\n}\n\nint mlx4_zone_add_one(struct mlx4_zone_allocator *zone_alloc,\n\t\t      struct mlx4_bitmap *bitmap,\n\t\t      u32 flags,\n\t\t      int priority,\n\t\t      int offset,\n\t\t      u32 *puid)\n{\n\tu32 mask = mlx4_bitmap_masked_value(bitmap, (u32)-1);\n\tstruct mlx4_zone_entry *it;\n\tstruct mlx4_zone_entry *zone = kmalloc(sizeof(*zone), GFP_KERNEL);\n\n\tif (NULL == zone)\n\t\treturn -ENOMEM;\n\n\tzone->flags = flags;\n\tzone->bitmap = bitmap;\n\tzone->use_rr = (flags & MLX4_ZONE_USE_RR) ? MLX4_USE_RR : 0;\n\tzone->priority = priority;\n\tzone->offset = offset;\n\n\tspin_lock(&zone_alloc->lock);\n\n\tzone->uid = zone_alloc->last_uid++;\n\tzone->allocator = zone_alloc;\n\n\tif (zone_alloc->mask < mask)\n\t\tzone_alloc->mask = mask;\n\n\tlist_for_each_entry(it, &zone_alloc->prios, prio_list)\n\t\tif (it->priority >= priority)\n\t\t\tbreak;\n\n\tif (&it->prio_list == &zone_alloc->prios || it->priority > priority)\n\t\tlist_add_tail(&zone->prio_list, &it->prio_list);\n\tlist_add_tail(&zone->list, &it->list);\n\n\tspin_unlock(&zone_alloc->lock);\n\n\t*puid = zone->uid;\n\n\treturn 0;\n}\n\n \nstatic void __mlx4_zone_remove_one_entry(struct mlx4_zone_entry *entry)\n{\n\tstruct mlx4_zone_allocator *zone_alloc = entry->allocator;\n\n\tif (!list_empty(&entry->prio_list)) {\n\t\t \n\t\tif (!list_is_last(&entry->list, &zone_alloc->entries)) {\n\t\t\tstruct mlx4_zone_entry *next = list_first_entry(&entry->list,\n\t\t\t\t\t\t\t\t\ttypeof(*next),\n\t\t\t\t\t\t\t\t\tlist);\n\n\t\t\tif (next->priority == entry->priority)\n\t\t\t\tlist_add_tail(&next->prio_list, &entry->prio_list);\n\t\t}\n\n\t\tlist_del(&entry->prio_list);\n\t}\n\n\tlist_del(&entry->list);\n\n\tif (zone_alloc->flags & MLX4_ZONE_ALLOC_FLAGS_NO_OVERLAP) {\n\t\tu32 mask = 0;\n\t\tstruct mlx4_zone_entry *it;\n\n\t\tlist_for_each_entry(it, &zone_alloc->prios, prio_list) {\n\t\t\tu32 cur_mask = mlx4_bitmap_masked_value(it->bitmap, (u32)-1);\n\n\t\t\tif (mask < cur_mask)\n\t\t\t\tmask = cur_mask;\n\t\t}\n\t\tzone_alloc->mask = mask;\n\t}\n}\n\nvoid mlx4_zone_allocator_destroy(struct mlx4_zone_allocator *zone_alloc)\n{\n\tstruct mlx4_zone_entry *zone, *tmp;\n\n\tspin_lock(&zone_alloc->lock);\n\n\tlist_for_each_entry_safe(zone, tmp, &zone_alloc->entries, list) {\n\t\tlist_del(&zone->list);\n\t\tlist_del(&zone->prio_list);\n\t\tkfree(zone);\n\t}\n\n\tspin_unlock(&zone_alloc->lock);\n\tkfree(zone_alloc);\n}\n\n \nstatic u32 __mlx4_alloc_from_zone(struct mlx4_zone_entry *zone, int count,\n\t\t\t\t  int align, u32 skip_mask, u32 *puid)\n{\n\tu32 uid = 0;\n\tu32 res;\n\tstruct mlx4_zone_allocator *zone_alloc = zone->allocator;\n\tstruct mlx4_zone_entry *curr_node;\n\n\tres = mlx4_bitmap_alloc_range(zone->bitmap, count,\n\t\t\t\t      align, skip_mask);\n\n\tif (res != (u32)-1) {\n\t\tres += zone->offset;\n\t\tuid = zone->uid;\n\t\tgoto out;\n\t}\n\n\tlist_for_each_entry(curr_node, &zone_alloc->prios, prio_list) {\n\t\tif (unlikely(curr_node->priority == zone->priority))\n\t\t\tbreak;\n\t}\n\n\tif (zone->flags & MLX4_ZONE_ALLOW_ALLOC_FROM_LOWER_PRIO) {\n\t\tstruct mlx4_zone_entry *it = curr_node;\n\n\t\tlist_for_each_entry_continue_reverse(it, &zone_alloc->entries, list) {\n\t\t\tres = mlx4_bitmap_alloc_range(it->bitmap, count,\n\t\t\t\t\t\t      align, skip_mask);\n\t\t\tif (res != (u32)-1) {\n\t\t\t\tres += it->offset;\n\t\t\t\tuid = it->uid;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (zone->flags & MLX4_ZONE_ALLOW_ALLOC_FROM_EQ_PRIO) {\n\t\tstruct mlx4_zone_entry *it = curr_node;\n\n\t\tlist_for_each_entry_from(it, &zone_alloc->entries, list) {\n\t\t\tif (unlikely(it == zone))\n\t\t\t\tcontinue;\n\n\t\t\tif (unlikely(it->priority != curr_node->priority))\n\t\t\t\tbreak;\n\n\t\t\tres = mlx4_bitmap_alloc_range(it->bitmap, count,\n\t\t\t\t\t\t      align, skip_mask);\n\t\t\tif (res != (u32)-1) {\n\t\t\t\tres += it->offset;\n\t\t\t\tuid = it->uid;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (zone->flags & MLX4_ZONE_FALLBACK_TO_HIGHER_PRIO) {\n\t\tif (list_is_last(&curr_node->prio_list, &zone_alloc->prios))\n\t\t\tgoto out;\n\n\t\tcurr_node = list_first_entry(&curr_node->prio_list,\n\t\t\t\t\t     typeof(*curr_node),\n\t\t\t\t\t     prio_list);\n\n\t\tlist_for_each_entry_from(curr_node, &zone_alloc->entries, list) {\n\t\t\tres = mlx4_bitmap_alloc_range(curr_node->bitmap, count,\n\t\t\t\t\t\t      align, skip_mask);\n\t\t\tif (res != (u32)-1) {\n\t\t\t\tres += curr_node->offset;\n\t\t\t\tuid = curr_node->uid;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\nout:\n\tif (NULL != puid && res != (u32)-1)\n\t\t*puid = uid;\n\treturn res;\n}\n\n \nstatic void __mlx4_free_from_zone(struct mlx4_zone_entry *zone, u32 obj,\n\t\t\t\t  u32 count)\n{\n\tmlx4_bitmap_free_range(zone->bitmap, obj - zone->offset, count, zone->use_rr);\n}\n\n \nstatic struct mlx4_zone_entry *__mlx4_find_zone_by_uid(\n\t\tstruct mlx4_zone_allocator *zones, u32 uid)\n{\n\tstruct mlx4_zone_entry *zone;\n\n\tlist_for_each_entry(zone, &zones->entries, list) {\n\t\tif (zone->uid == uid)\n\t\t\treturn zone;\n\t}\n\n\treturn NULL;\n}\n\nstruct mlx4_bitmap *mlx4_zone_get_bitmap(struct mlx4_zone_allocator *zones, u32 uid)\n{\n\tstruct mlx4_zone_entry *zone;\n\tstruct mlx4_bitmap *bitmap;\n\n\tspin_lock(&zones->lock);\n\n\tzone = __mlx4_find_zone_by_uid(zones, uid);\n\n\tbitmap = zone == NULL ? NULL : zone->bitmap;\n\n\tspin_unlock(&zones->lock);\n\n\treturn bitmap;\n}\n\nint mlx4_zone_remove_one(struct mlx4_zone_allocator *zones, u32 uid)\n{\n\tstruct mlx4_zone_entry *zone;\n\tint res = 0;\n\n\tspin_lock(&zones->lock);\n\n\tzone = __mlx4_find_zone_by_uid(zones, uid);\n\n\tif (NULL == zone) {\n\t\tres = -1;\n\t\tgoto out;\n\t}\n\n\t__mlx4_zone_remove_one_entry(zone);\n\nout:\n\tspin_unlock(&zones->lock);\n\tkfree(zone);\n\n\treturn res;\n}\n\n \nstatic struct mlx4_zone_entry *__mlx4_find_zone_by_uid_unique(\n\t\tstruct mlx4_zone_allocator *zones, u32 obj)\n{\n\tstruct mlx4_zone_entry *zone, *zone_candidate = NULL;\n\tu32 dist = (u32)-1;\n\n\t \n\tlist_for_each_entry(zone, &zones->entries, list) {\n\t\tif (obj >= zone->offset) {\n\t\t\tu32 mobj = (obj - zone->offset) & zones->mask;\n\n\t\t\tif (mobj < zone->bitmap->max) {\n\t\t\t\tu32 curr_dist = zone->bitmap->effective_len;\n\n\t\t\t\tif (curr_dist < dist) {\n\t\t\t\t\tdist = curr_dist;\n\t\t\t\t\tzone_candidate = zone;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn zone_candidate;\n}\n\nu32 mlx4_zone_alloc_entries(struct mlx4_zone_allocator *zones, u32 uid, int count,\n\t\t\t    int align, u32 skip_mask, u32 *puid)\n{\n\tstruct mlx4_zone_entry *zone;\n\tint res = -1;\n\n\tspin_lock(&zones->lock);\n\n\tzone = __mlx4_find_zone_by_uid(zones, uid);\n\n\tif (NULL == zone)\n\t\tgoto out;\n\n\tres = __mlx4_alloc_from_zone(zone, count, align, skip_mask, puid);\n\nout:\n\tspin_unlock(&zones->lock);\n\n\treturn res;\n}\n\nu32 mlx4_zone_free_entries(struct mlx4_zone_allocator *zones, u32 uid, u32 obj, u32 count)\n{\n\tstruct mlx4_zone_entry *zone;\n\tint res = 0;\n\n\tspin_lock(&zones->lock);\n\n\tzone = __mlx4_find_zone_by_uid(zones, uid);\n\n\tif (NULL == zone) {\n\t\tres = -1;\n\t\tgoto out;\n\t}\n\n\t__mlx4_free_from_zone(zone, obj, count);\n\nout:\n\tspin_unlock(&zones->lock);\n\n\treturn res;\n}\n\nu32 mlx4_zone_free_entries_unique(struct mlx4_zone_allocator *zones, u32 obj, u32 count)\n{\n\tstruct mlx4_zone_entry *zone;\n\tint res;\n\n\tif (!(zones->flags & MLX4_ZONE_ALLOC_FLAGS_NO_OVERLAP))\n\t\treturn -EFAULT;\n\n\tspin_lock(&zones->lock);\n\n\tzone = __mlx4_find_zone_by_uid_unique(zones, obj);\n\n\tif (NULL == zone) {\n\t\tres = -1;\n\t\tgoto out;\n\t}\n\n\t__mlx4_free_from_zone(zone, obj, count);\n\tres = 0;\n\nout:\n\tspin_unlock(&zones->lock);\n\n\treturn res;\n}\n\nstatic int mlx4_buf_direct_alloc(struct mlx4_dev *dev, int size,\n\t\t\t\t struct mlx4_buf *buf)\n{\n\tdma_addr_t t;\n\n\tbuf->nbufs        = 1;\n\tbuf->npages       = 1;\n\tbuf->page_shift   = get_order(size) + PAGE_SHIFT;\n\tbuf->direct.buf   =\n\t\tdma_alloc_coherent(&dev->persist->pdev->dev, size, &t,\n\t\t\t\t   GFP_KERNEL);\n\tif (!buf->direct.buf)\n\t\treturn -ENOMEM;\n\n\tbuf->direct.map = t;\n\n\twhile (t & ((1 << buf->page_shift) - 1)) {\n\t\t--buf->page_shift;\n\t\tbuf->npages *= 2;\n\t}\n\n\treturn 0;\n}\n\n \nint mlx4_buf_alloc(struct mlx4_dev *dev, int size, int max_direct,\n\t\t   struct mlx4_buf *buf)\n{\n\tif (size <= max_direct) {\n\t\treturn mlx4_buf_direct_alloc(dev, size, buf);\n\t} else {\n\t\tdma_addr_t t;\n\t\tint i;\n\n\t\tbuf->direct.buf = NULL;\n\t\tbuf->nbufs      = DIV_ROUND_UP(size, PAGE_SIZE);\n\t\tbuf->npages\t= buf->nbufs;\n\t\tbuf->page_shift  = PAGE_SHIFT;\n\t\tbuf->page_list   = kcalloc(buf->nbufs, sizeof(*buf->page_list),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!buf->page_list)\n\t\t\treturn -ENOMEM;\n\n\t\tfor (i = 0; i < buf->nbufs; ++i) {\n\t\t\tbuf->page_list[i].buf =\n\t\t\t\tdma_alloc_coherent(&dev->persist->pdev->dev,\n\t\t\t\t\t\t   PAGE_SIZE, &t, GFP_KERNEL);\n\t\t\tif (!buf->page_list[i].buf)\n\t\t\t\tgoto err_free;\n\n\t\t\tbuf->page_list[i].map = t;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_free:\n\tmlx4_buf_free(dev, size, buf);\n\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(mlx4_buf_alloc);\n\nvoid mlx4_buf_free(struct mlx4_dev *dev, int size, struct mlx4_buf *buf)\n{\n\tif (buf->nbufs == 1) {\n\t\tdma_free_coherent(&dev->persist->pdev->dev, size,\n\t\t\t\t  buf->direct.buf, buf->direct.map);\n\t} else {\n\t\tint i;\n\n\t\tfor (i = 0; i < buf->nbufs; ++i)\n\t\t\tif (buf->page_list[i].buf)\n\t\t\t\tdma_free_coherent(&dev->persist->pdev->dev,\n\t\t\t\t\t\t  PAGE_SIZE,\n\t\t\t\t\t\t  buf->page_list[i].buf,\n\t\t\t\t\t\t  buf->page_list[i].map);\n\t\tkfree(buf->page_list);\n\t}\n}\nEXPORT_SYMBOL_GPL(mlx4_buf_free);\n\nstatic struct mlx4_db_pgdir *mlx4_alloc_db_pgdir(struct device *dma_device)\n{\n\tstruct mlx4_db_pgdir *pgdir;\n\n\tpgdir = kzalloc(sizeof(*pgdir), GFP_KERNEL);\n\tif (!pgdir)\n\t\treturn NULL;\n\n\tbitmap_fill(pgdir->order1, MLX4_DB_PER_PAGE / 2);\n\tpgdir->bits[0] = pgdir->order0;\n\tpgdir->bits[1] = pgdir->order1;\n\tpgdir->db_page = dma_alloc_coherent(dma_device, PAGE_SIZE,\n\t\t\t\t\t    &pgdir->db_dma, GFP_KERNEL);\n\tif (!pgdir->db_page) {\n\t\tkfree(pgdir);\n\t\treturn NULL;\n\t}\n\n\treturn pgdir;\n}\n\nstatic int mlx4_alloc_db_from_pgdir(struct mlx4_db_pgdir *pgdir,\n\t\t\t\t    struct mlx4_db *db, int order)\n{\n\tint o;\n\tint i;\n\n\tfor (o = order; o <= 1; ++o) {\n\t\ti = find_first_bit(pgdir->bits[o], MLX4_DB_PER_PAGE >> o);\n\t\tif (i < MLX4_DB_PER_PAGE >> o)\n\t\t\tgoto found;\n\t}\n\n\treturn -ENOMEM;\n\nfound:\n\tclear_bit(i, pgdir->bits[o]);\n\n\ti <<= o;\n\n\tif (o > order)\n\t\tset_bit(i ^ 1, pgdir->bits[order]);\n\n\tdb->u.pgdir = pgdir;\n\tdb->index   = i;\n\tdb->db      = pgdir->db_page + db->index;\n\tdb->dma     = pgdir->db_dma  + db->index * 4;\n\tdb->order   = order;\n\n\treturn 0;\n}\n\nint mlx4_db_alloc(struct mlx4_dev *dev, struct mlx4_db *db, int order)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_db_pgdir *pgdir;\n\tint ret = 0;\n\n\tmutex_lock(&priv->pgdir_mutex);\n\n\tlist_for_each_entry(pgdir, &priv->pgdir_list, list)\n\t\tif (!mlx4_alloc_db_from_pgdir(pgdir, db, order))\n\t\t\tgoto out;\n\n\tpgdir = mlx4_alloc_db_pgdir(&dev->persist->pdev->dev);\n\tif (!pgdir) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tlist_add(&pgdir->list, &priv->pgdir_list);\n\n\t \n\tWARN_ON(mlx4_alloc_db_from_pgdir(pgdir, db, order));\n\nout:\n\tmutex_unlock(&priv->pgdir_mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mlx4_db_alloc);\n\nvoid mlx4_db_free(struct mlx4_dev *dev, struct mlx4_db *db)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint o;\n\tint i;\n\n\tmutex_lock(&priv->pgdir_mutex);\n\n\to = db->order;\n\ti = db->index;\n\n\tif (db->order == 0 && test_bit(i ^ 1, db->u.pgdir->order0)) {\n\t\tclear_bit(i ^ 1, db->u.pgdir->order0);\n\t\t++o;\n\t}\n\ti >>= o;\n\tset_bit(i, db->u.pgdir->bits[o]);\n\n\tif (bitmap_full(db->u.pgdir->order1, MLX4_DB_PER_PAGE / 2)) {\n\t\tdma_free_coherent(&dev->persist->pdev->dev, PAGE_SIZE,\n\t\t\t\t  db->u.pgdir->db_page, db->u.pgdir->db_dma);\n\t\tlist_del(&db->u.pgdir->list);\n\t\tkfree(db->u.pgdir);\n\t}\n\n\tmutex_unlock(&priv->pgdir_mutex);\n}\nEXPORT_SYMBOL_GPL(mlx4_db_free);\n\nint mlx4_alloc_hwq_res(struct mlx4_dev *dev, struct mlx4_hwq_resources *wqres,\n\t\t       int size)\n{\n\tint err;\n\n\terr = mlx4_db_alloc(dev, &wqres->db, 1);\n\tif (err)\n\t\treturn err;\n\n\t*wqres->db.db = 0;\n\n\terr = mlx4_buf_direct_alloc(dev, size, &wqres->buf);\n\tif (err)\n\t\tgoto err_db;\n\n\terr = mlx4_mtt_init(dev, wqres->buf.npages, wqres->buf.page_shift,\n\t\t\t    &wqres->mtt);\n\tif (err)\n\t\tgoto err_buf;\n\n\terr = mlx4_buf_write_mtt(dev, &wqres->mtt, &wqres->buf);\n\tif (err)\n\t\tgoto err_mtt;\n\n\treturn 0;\n\nerr_mtt:\n\tmlx4_mtt_cleanup(dev, &wqres->mtt);\nerr_buf:\n\tmlx4_buf_free(dev, size, &wqres->buf);\nerr_db:\n\tmlx4_db_free(dev, &wqres->db);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(mlx4_alloc_hwq_res);\n\nvoid mlx4_free_hwq_res(struct mlx4_dev *dev, struct mlx4_hwq_resources *wqres,\n\t\t       int size)\n{\n\tmlx4_mtt_cleanup(dev, &wqres->mtt);\n\tmlx4_buf_free(dev, size, &wqres->buf);\n\tmlx4_db_free(dev, &wqres->db);\n}\nEXPORT_SYMBOL_GPL(mlx4_free_hwq_res);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}