{
  "module_name": "en_tx.c",
  "hash_id": "40a8e623de6b8f5efa3f2a6ca2f532df38f192c1a787e26959104f33b92d256a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx4/en_tx.c",
  "human_readable_source": " \n\n#include <asm/page.h>\n#include <linux/mlx4/cq.h>\n#include <linux/slab.h>\n#include <linux/mlx4/qp.h>\n#include <linux/skbuff.h>\n#include <linux/if_vlan.h>\n#include <linux/prefetch.h>\n#include <linux/vmalloc.h>\n#include <linux/tcp.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/indirect_call_wrapper.h>\n#include <net/ipv6.h>\n\n#include \"mlx4_en.h\"\n\nint mlx4_en_create_tx_ring(struct mlx4_en_priv *priv,\n\t\t\t   struct mlx4_en_tx_ring **pring, u32 size,\n\t\t\t   u16 stride, int node, int queue_index)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\tstruct mlx4_en_tx_ring *ring;\n\tint tmp;\n\tint err;\n\n\tring = kzalloc_node(sizeof(*ring), GFP_KERNEL, node);\n\tif (!ring) {\n\t\ten_err(priv, \"Failed allocating TX ring\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tring->size = size;\n\tring->size_mask = size - 1;\n\tring->sp_stride = stride;\n\tring->full_size = ring->size - HEADROOM - MLX4_MAX_DESC_TXBBS;\n\n\ttmp = size * sizeof(struct mlx4_en_tx_info);\n\tring->tx_info = kvmalloc_node(tmp, GFP_KERNEL, node);\n\tif (!ring->tx_info) {\n\t\terr = -ENOMEM;\n\t\tgoto err_ring;\n\t}\n\n\ten_dbg(DRV, priv, \"Allocated tx_info ring at addr:%p size:%d\\n\",\n\t\t ring->tx_info, tmp);\n\n\tring->bounce_buf = kmalloc_node(MLX4_TX_BOUNCE_BUFFER_SIZE,\n\t\t\t\t\tGFP_KERNEL, node);\n\tif (!ring->bounce_buf) {\n\t\tring->bounce_buf = kmalloc(MLX4_TX_BOUNCE_BUFFER_SIZE,\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!ring->bounce_buf) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_info;\n\t\t}\n\t}\n\tring->buf_size = ALIGN(size * ring->sp_stride, MLX4_EN_PAGE_SIZE);\n\n\t \n\tset_dev_node(&mdev->dev->persist->pdev->dev, node);\n\terr = mlx4_alloc_hwq_res(mdev->dev, &ring->sp_wqres, ring->buf_size);\n\tset_dev_node(&mdev->dev->persist->pdev->dev, mdev->dev->numa_node);\n\tif (err) {\n\t\ten_err(priv, \"Failed allocating hwq resources\\n\");\n\t\tgoto err_bounce;\n\t}\n\n\tring->buf = ring->sp_wqres.buf.direct.buf;\n\n\ten_dbg(DRV, priv, \"Allocated TX ring (addr:%p) - buf:%p size:%d buf_size:%d dma:%llx\\n\",\n\t       ring, ring->buf, ring->size, ring->buf_size,\n\t       (unsigned long long) ring->sp_wqres.buf.direct.map);\n\n\terr = mlx4_qp_reserve_range(mdev->dev, 1, 1, &ring->qpn,\n\t\t\t\t    MLX4_RESERVE_ETH_BF_QP,\n\t\t\t\t    MLX4_RES_USAGE_DRIVER);\n\tif (err) {\n\t\ten_err(priv, \"failed reserving qp for TX ring\\n\");\n\t\tgoto err_hwq_res;\n\t}\n\n\terr = mlx4_qp_alloc(mdev->dev, ring->qpn, &ring->sp_qp);\n\tif (err) {\n\t\ten_err(priv, \"Failed allocating qp %d\\n\", ring->qpn);\n\t\tgoto err_reserve;\n\t}\n\tring->sp_qp.event = mlx4_en_sqp_event;\n\n\terr = mlx4_bf_alloc(mdev->dev, &ring->bf, node);\n\tif (err) {\n\t\ten_dbg(DRV, priv, \"working without blueflame (%d)\\n\", err);\n\t\tring->bf.uar = &mdev->priv_uar;\n\t\tring->bf.uar->map = mdev->uar_map;\n\t\tring->bf_enabled = false;\n\t\tring->bf_alloced = false;\n\t\tpriv->pflags &= ~MLX4_EN_PRIV_FLAGS_BLUEFLAME;\n\t} else {\n\t\tring->bf_alloced = true;\n\t\tring->bf_enabled = !!(priv->pflags &\n\t\t\t\t      MLX4_EN_PRIV_FLAGS_BLUEFLAME);\n\t}\n\tring->doorbell_address = ring->bf.uar->map + MLX4_SEND_DOORBELL;\n\n\tring->hwtstamp_tx_type = priv->hwtstamp_config.tx_type;\n\tring->queue_index = queue_index;\n\n\tif (queue_index < priv->num_tx_rings_p_up)\n\t\tcpumask_set_cpu(cpumask_local_spread(queue_index,\n\t\t\t\t\t\t     priv->mdev->dev->numa_node),\n\t\t\t\t&ring->sp_affinity_mask);\n\n\t*pring = ring;\n\treturn 0;\n\nerr_reserve:\n\tmlx4_qp_release_range(mdev->dev, ring->qpn, 1);\nerr_hwq_res:\n\tmlx4_free_hwq_res(mdev->dev, &ring->sp_wqres, ring->buf_size);\nerr_bounce:\n\tkfree(ring->bounce_buf);\n\tring->bounce_buf = NULL;\nerr_info:\n\tkvfree(ring->tx_info);\n\tring->tx_info = NULL;\nerr_ring:\n\tkfree(ring);\n\t*pring = NULL;\n\treturn err;\n}\n\nvoid mlx4_en_destroy_tx_ring(struct mlx4_en_priv *priv,\n\t\t\t     struct mlx4_en_tx_ring **pring)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\tstruct mlx4_en_tx_ring *ring = *pring;\n\ten_dbg(DRV, priv, \"Destroying tx ring, qpn: %d\\n\", ring->qpn);\n\n\tif (ring->bf_alloced)\n\t\tmlx4_bf_free(mdev->dev, &ring->bf);\n\tmlx4_qp_remove(mdev->dev, &ring->sp_qp);\n\tmlx4_qp_free(mdev->dev, &ring->sp_qp);\n\tmlx4_qp_release_range(priv->mdev->dev, ring->qpn, 1);\n\tmlx4_free_hwq_res(mdev->dev, &ring->sp_wqres, ring->buf_size);\n\tkfree(ring->bounce_buf);\n\tring->bounce_buf = NULL;\n\tkvfree(ring->tx_info);\n\tring->tx_info = NULL;\n\tkfree(ring);\n\t*pring = NULL;\n}\n\nint mlx4_en_activate_tx_ring(struct mlx4_en_priv *priv,\n\t\t\t     struct mlx4_en_tx_ring *ring,\n\t\t\t     int cq, int user_prio)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\tint err;\n\n\tring->sp_cqn = cq;\n\tring->prod = 0;\n\tring->cons = 0xffffffff;\n\tring->last_nr_txbb = 1;\n\tmemset(ring->tx_info, 0, ring->size * sizeof(struct mlx4_en_tx_info));\n\tmemset(ring->buf, 0, ring->buf_size);\n\tring->free_tx_desc = mlx4_en_free_tx_desc;\n\n\tring->sp_qp_state = MLX4_QP_STATE_RST;\n\tring->doorbell_qpn = cpu_to_be32(ring->sp_qp.qpn << 8);\n\tring->mr_key = cpu_to_be32(mdev->mr.key);\n\n\tmlx4_en_fill_qp_context(priv, ring->size, ring->sp_stride, 1, 0, ring->qpn,\n\t\t\t\tring->sp_cqn, user_prio, &ring->sp_context);\n\tif (ring->bf_alloced)\n\t\tring->sp_context.usr_page =\n\t\t\tcpu_to_be32(mlx4_to_hw_uar_index(mdev->dev,\n\t\t\t\t\t\t\t ring->bf.uar->index));\n\n\terr = mlx4_qp_to_ready(mdev->dev, &ring->sp_wqres.mtt, &ring->sp_context,\n\t\t\t       &ring->sp_qp, &ring->sp_qp_state);\n\tif (!cpumask_empty(&ring->sp_affinity_mask))\n\t\tnetif_set_xps_queue(priv->dev, &ring->sp_affinity_mask,\n\t\t\t\t    ring->queue_index);\n\n\treturn err;\n}\n\nvoid mlx4_en_deactivate_tx_ring(struct mlx4_en_priv *priv,\n\t\t\t\tstruct mlx4_en_tx_ring *ring)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\n\tmlx4_qp_modify(mdev->dev, NULL, ring->sp_qp_state,\n\t\t       MLX4_QP_STATE_RST, NULL, 0, 0, &ring->sp_qp);\n}\n\nstatic inline bool mlx4_en_is_tx_ring_full(struct mlx4_en_tx_ring *ring)\n{\n\tu32 used = READ_ONCE(ring->prod) - READ_ONCE(ring->cons);\n\n\treturn used > ring->full_size;\n}\n\nstatic void mlx4_en_stamp_wqe(struct mlx4_en_priv *priv,\n\t\t\t      struct mlx4_en_tx_ring *ring, int index,\n\t\t\t      u8 owner)\n{\n\t__be32 stamp = cpu_to_be32(STAMP_VAL | (!!owner << STAMP_SHIFT));\n\tstruct mlx4_en_tx_desc *tx_desc = ring->buf + (index << LOG_TXBB_SIZE);\n\tstruct mlx4_en_tx_info *tx_info = &ring->tx_info[index];\n\tvoid *end = ring->buf + ring->buf_size;\n\t__be32 *ptr = (__be32 *)tx_desc;\n\tint i;\n\n\t \n\tif (likely((void *)tx_desc +\n\t\t   (tx_info->nr_txbb << LOG_TXBB_SIZE) <= end)) {\n\t\t \n\t\tfor (i = 0; i < tx_info->nr_txbb << LOG_TXBB_SIZE;\n\t\t     i += STAMP_STRIDE) {\n\t\t\t*ptr = stamp;\n\t\t\tptr += STAMP_DWORDS;\n\t\t}\n\t} else {\n\t\t \n\t\tfor (i = 0; i < tx_info->nr_txbb << LOG_TXBB_SIZE;\n\t\t     i += STAMP_STRIDE) {\n\t\t\t*ptr = stamp;\n\t\t\tptr += STAMP_DWORDS;\n\t\t\tif ((void *)ptr >= end) {\n\t\t\t\tptr = ring->buf;\n\t\t\t\tstamp ^= cpu_to_be32(0x80000000);\n\t\t\t}\n\t\t}\n\t}\n}\n\nINDIRECT_CALLABLE_DECLARE(u32 mlx4_en_free_tx_desc(struct mlx4_en_priv *priv,\n\t\t\t\t\t\t   struct mlx4_en_tx_ring *ring,\n\t\t\t\t\t\t   int index, u64 timestamp,\n\t\t\t\t\t\t   int napi_mode));\n\nu32 mlx4_en_free_tx_desc(struct mlx4_en_priv *priv,\n\t\t\t struct mlx4_en_tx_ring *ring,\n\t\t\t int index, u64 timestamp,\n\t\t\t int napi_mode)\n{\n\tstruct mlx4_en_tx_info *tx_info = &ring->tx_info[index];\n\tstruct mlx4_en_tx_desc *tx_desc = ring->buf + (index << LOG_TXBB_SIZE);\n\tstruct mlx4_wqe_data_seg *data = (void *) tx_desc + tx_info->data_offset;\n\tvoid *end = ring->buf + ring->buf_size;\n\tstruct sk_buff *skb = tx_info->skb;\n\tint nr_maps = tx_info->nr_maps;\n\tint i;\n\n\t \n\tprefetchw(&skb->users);\n\n\tif (unlikely(timestamp)) {\n\t\tstruct skb_shared_hwtstamps hwts;\n\n\t\tmlx4_en_fill_hwtstamps(priv->mdev, &hwts, timestamp);\n\t\tskb_tstamp_tx(skb, &hwts);\n\t}\n\n\tif (!tx_info->inl) {\n\t\tif (tx_info->linear)\n\t\t\tdma_unmap_single(priv->ddev,\n\t\t\t\t\t tx_info->map0_dma,\n\t\t\t\t\t tx_info->map0_byte_count,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_page(priv->ddev,\n\t\t\t\t       tx_info->map0_dma,\n\t\t\t\t       tx_info->map0_byte_count,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\t \n\t\tif (likely((void *)tx_desc +\n\t\t\t   (tx_info->nr_txbb << LOG_TXBB_SIZE) <= end)) {\n\t\t\tfor (i = 1; i < nr_maps; i++) {\n\t\t\t\tdata++;\n\t\t\t\tdma_unmap_page(priv->ddev,\n\t\t\t\t\t(dma_addr_t)be64_to_cpu(data->addr),\n\t\t\t\t\tbe32_to_cpu(data->byte_count),\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\t}\n\t\t} else {\n\t\t\tif ((void *)data >= end)\n\t\t\t\tdata = ring->buf + ((void *)data - end);\n\n\t\t\tfor (i = 1; i < nr_maps; i++) {\n\t\t\t\tdata++;\n\t\t\t\t \n\t\t\t\tif ((void *) data >= end)\n\t\t\t\t\tdata = ring->buf;\n\t\t\t\tdma_unmap_page(priv->ddev,\n\t\t\t\t\t(dma_addr_t)be64_to_cpu(data->addr),\n\t\t\t\t\tbe32_to_cpu(data->byte_count),\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\t}\n\t\t}\n\t}\n\tnapi_consume_skb(skb, napi_mode);\n\n\treturn tx_info->nr_txbb;\n}\n\nINDIRECT_CALLABLE_DECLARE(u32 mlx4_en_recycle_tx_desc(struct mlx4_en_priv *priv,\n\t\t\t\t\t\t      struct mlx4_en_tx_ring *ring,\n\t\t\t\t\t\t      int index, u64 timestamp,\n\t\t\t\t\t\t      int napi_mode));\n\nu32 mlx4_en_recycle_tx_desc(struct mlx4_en_priv *priv,\n\t\t\t    struct mlx4_en_tx_ring *ring,\n\t\t\t    int index, u64 timestamp,\n\t\t\t    int napi_mode)\n{\n\tstruct mlx4_en_tx_info *tx_info = &ring->tx_info[index];\n\tstruct mlx4_en_rx_alloc frame = {\n\t\t.page = tx_info->page,\n\t\t.dma = tx_info->map0_dma,\n\t};\n\n\tif (!napi_mode || !mlx4_en_rx_recycle(ring->recycle_ring, &frame)) {\n\t\tdma_unmap_page(priv->ddev, tx_info->map0_dma,\n\t\t\t       PAGE_SIZE, priv->dma_dir);\n\t\tput_page(tx_info->page);\n\t}\n\n\treturn tx_info->nr_txbb;\n}\n\nint mlx4_en_free_tx_buf(struct net_device *dev, struct mlx4_en_tx_ring *ring)\n{\n\tstruct mlx4_en_priv *priv = netdev_priv(dev);\n\tint cnt = 0;\n\n\t \n\tring->cons += ring->last_nr_txbb;\n\ten_dbg(DRV, priv, \"Freeing Tx buf - cons:0x%x prod:0x%x\\n\",\n\t\t ring->cons, ring->prod);\n\n\tif ((u32) (ring->prod - ring->cons) > ring->size) {\n\t\tif (netif_msg_tx_err(priv))\n\t\t\ten_warn(priv, \"Tx consumer passed producer!\\n\");\n\t\treturn 0;\n\t}\n\n\twhile (ring->cons != ring->prod) {\n\t\tring->last_nr_txbb = ring->free_tx_desc(priv, ring,\n\t\t\t\t\t\tring->cons & ring->size_mask,\n\t\t\t\t\t\t0, 0  );\n\t\tring->cons += ring->last_nr_txbb;\n\t\tcnt++;\n\t}\n\n\tif (ring->tx_queue)\n\t\tnetdev_tx_reset_queue(ring->tx_queue);\n\n\tif (cnt)\n\t\ten_dbg(DRV, priv, \"Freed %d uncompleted tx descriptors\\n\", cnt);\n\n\treturn cnt;\n}\n\nstatic void mlx4_en_handle_err_cqe(struct mlx4_en_priv *priv, struct mlx4_err_cqe *err_cqe,\n\t\t\t\t   u16 cqe_index, struct mlx4_en_tx_ring *ring)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\tstruct mlx4_en_tx_info *tx_info;\n\tstruct mlx4_en_tx_desc *tx_desc;\n\tu16 wqe_index;\n\tint desc_size;\n\n\ten_err(priv, \"CQE error - cqn 0x%x, ci 0x%x, vendor syndrome: 0x%x syndrome: 0x%x\\n\",\n\t       ring->sp_cqn, cqe_index, err_cqe->vendor_err_syndrome, err_cqe->syndrome);\n\tprint_hex_dump(KERN_WARNING, \"\", DUMP_PREFIX_OFFSET, 16, 1, err_cqe, sizeof(*err_cqe),\n\t\t       false);\n\n\twqe_index = be16_to_cpu(err_cqe->wqe_index) & ring->size_mask;\n\ttx_info = &ring->tx_info[wqe_index];\n\tdesc_size = tx_info->nr_txbb << LOG_TXBB_SIZE;\n\ten_err(priv, \"Related WQE - qpn 0x%x, wqe index 0x%x, wqe size 0x%x\\n\", ring->qpn,\n\t       wqe_index, desc_size);\n\ttx_desc = ring->buf + (wqe_index << LOG_TXBB_SIZE);\n\tprint_hex_dump(KERN_WARNING, \"\", DUMP_PREFIX_OFFSET, 16, 1, tx_desc, desc_size, false);\n\n\tif (test_and_set_bit(MLX4_EN_STATE_FLAG_RESTARTING, &priv->state))\n\t\treturn;\n\n\ten_err(priv, \"Scheduling port restart\\n\");\n\tqueue_work(mdev->workqueue, &priv->restart_task);\n}\n\nint mlx4_en_process_tx_cq(struct net_device *dev,\n\t\t\t  struct mlx4_en_cq *cq, int napi_budget)\n{\n\tstruct mlx4_en_priv *priv = netdev_priv(dev);\n\tstruct mlx4_cq *mcq = &cq->mcq;\n\tstruct mlx4_en_tx_ring *ring = priv->tx_ring[cq->type][cq->ring];\n\tstruct mlx4_cqe *cqe;\n\tu16 index, ring_index, stamp_index;\n\tu32 txbbs_skipped = 0;\n\tu32 txbbs_stamp = 0;\n\tu32 cons_index = mcq->cons_index;\n\tint size = cq->size;\n\tu32 size_mask = ring->size_mask;\n\tstruct mlx4_cqe *buf = cq->buf;\n\tu32 packets = 0;\n\tu32 bytes = 0;\n\tint factor = priv->cqe_factor;\n\tint done = 0;\n\tint budget = priv->tx_work_limit;\n\tu32 last_nr_txbb;\n\tu32 ring_cons;\n\n\tif (unlikely(!priv->port_up))\n\t\treturn 0;\n\n\tnetdev_txq_bql_complete_prefetchw(ring->tx_queue);\n\n\tindex = cons_index & size_mask;\n\tcqe = mlx4_en_get_cqe(buf, index, priv->cqe_size) + factor;\n\tlast_nr_txbb = READ_ONCE(ring->last_nr_txbb);\n\tring_cons = READ_ONCE(ring->cons);\n\tring_index = ring_cons & size_mask;\n\tstamp_index = ring_index;\n\n\t \n\twhile (XNOR(cqe->owner_sr_opcode & MLX4_CQE_OWNER_MASK,\n\t\t\tcons_index & size) && (done < budget)) {\n\t\tu16 new_index;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tif (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==\n\t\t\t     MLX4_CQE_OPCODE_ERROR))\n\t\t\tif (!test_and_set_bit(MLX4_EN_TX_RING_STATE_RECOVERING, &ring->state))\n\t\t\t\tmlx4_en_handle_err_cqe(priv, (struct mlx4_err_cqe *)cqe, index,\n\t\t\t\t\t\t       ring);\n\n\t\t \n\t\tnew_index = be16_to_cpu(cqe->wqe_index) & size_mask;\n\n\t\tdo {\n\t\t\tu64 timestamp = 0;\n\n\t\t\ttxbbs_skipped += last_nr_txbb;\n\t\t\tring_index = (ring_index + last_nr_txbb) & size_mask;\n\n\t\t\tif (unlikely(ring->tx_info[ring_index].ts_requested))\n\t\t\t\ttimestamp = mlx4_en_get_cqe_ts(cqe);\n\n\t\t\t \n\t\t\tlast_nr_txbb = INDIRECT_CALL_2(ring->free_tx_desc,\n\t\t\t\t\t\t       mlx4_en_free_tx_desc,\n\t\t\t\t\t\t       mlx4_en_recycle_tx_desc,\n\t\t\t\t\tpriv, ring, ring_index,\n\t\t\t\t\ttimestamp, napi_budget);\n\n\t\t\tmlx4_en_stamp_wqe(priv, ring, stamp_index,\n\t\t\t\t\t  !!((ring_cons + txbbs_stamp) &\n\t\t\t\t\t\tring->size));\n\t\t\tstamp_index = ring_index;\n\t\t\ttxbbs_stamp = txbbs_skipped;\n\t\t\tpackets++;\n\t\t\tbytes += ring->tx_info[ring_index].nr_bytes;\n\t\t} while ((++done < budget) && (ring_index != new_index));\n\n\t\t++cons_index;\n\t\tindex = cons_index & size_mask;\n\t\tcqe = mlx4_en_get_cqe(buf, index, priv->cqe_size) + factor;\n\t}\n\n\t \n\tmcq->cons_index = cons_index;\n\tmlx4_cq_set_ci(mcq);\n\twmb();\n\n\t \n\tWRITE_ONCE(ring->last_nr_txbb, last_nr_txbb);\n\tWRITE_ONCE(ring->cons, ring_cons + txbbs_skipped);\n\n\tif (cq->type == TX_XDP)\n\t\treturn done;\n\n\tnetdev_tx_completed_queue(ring->tx_queue, packets, bytes);\n\n\t \n\tif (netif_tx_queue_stopped(ring->tx_queue) &&\n\t    !mlx4_en_is_tx_ring_full(ring)) {\n\t\tnetif_tx_wake_queue(ring->tx_queue);\n\t\tring->wake_queue++;\n\t}\n\n\treturn done;\n}\n\nvoid mlx4_en_tx_irq(struct mlx4_cq *mcq)\n{\n\tstruct mlx4_en_cq *cq = container_of(mcq, struct mlx4_en_cq, mcq);\n\tstruct mlx4_en_priv *priv = netdev_priv(cq->dev);\n\n\tif (likely(priv->port_up))\n\t\tnapi_schedule_irqoff(&cq->napi);\n\telse\n\t\tmlx4_en_arm_cq(priv, cq);\n}\n\n \nint mlx4_en_poll_tx_cq(struct napi_struct *napi, int budget)\n{\n\tstruct mlx4_en_cq *cq = container_of(napi, struct mlx4_en_cq, napi);\n\tstruct net_device *dev = cq->dev;\n\tstruct mlx4_en_priv *priv = netdev_priv(dev);\n\tint work_done;\n\n\twork_done = mlx4_en_process_tx_cq(dev, cq, budget);\n\tif (work_done >= budget)\n\t\treturn budget;\n\n\tif (napi_complete_done(napi, work_done))\n\t\tmlx4_en_arm_cq(priv, cq);\n\n\treturn 0;\n}\n\nstatic struct mlx4_en_tx_desc *mlx4_en_bounce_to_desc(struct mlx4_en_priv *priv,\n\t\t\t\t\t\t      struct mlx4_en_tx_ring *ring,\n\t\t\t\t\t\t      u32 index,\n\t\t\t\t\t\t      unsigned int desc_size)\n{\n\tu32 copy = (ring->size - index) << LOG_TXBB_SIZE;\n\tint i;\n\n\tfor (i = desc_size - copy - 4; i >= 0; i -= 4) {\n\t\tif ((i & (TXBB_SIZE - 1)) == 0)\n\t\t\twmb();\n\n\t\t*((u32 *) (ring->buf + i)) =\n\t\t\t*((u32 *) (ring->bounce_buf + copy + i));\n\t}\n\n\tfor (i = copy - 4; i >= 4 ; i -= 4) {\n\t\tif ((i & (TXBB_SIZE - 1)) == 0)\n\t\t\twmb();\n\n\t\t*((u32 *)(ring->buf + (index << LOG_TXBB_SIZE) + i)) =\n\t\t\t*((u32 *) (ring->bounce_buf + i));\n\t}\n\n\t \n\treturn ring->buf + (index << LOG_TXBB_SIZE);\n}\n\n \nstatic bool is_inline(int inline_thold, const struct sk_buff *skb,\n\t\t      const struct skb_shared_info *shinfo,\n\t\t      void **pfrag)\n{\n\tvoid *ptr;\n\n\tif (skb->len > inline_thold || !inline_thold)\n\t\treturn false;\n\n\tif (shinfo->nr_frags == 1) {\n\t\tptr = skb_frag_address_safe(&shinfo->frags[0]);\n\t\tif (unlikely(!ptr))\n\t\t\treturn false;\n\t\t*pfrag = ptr;\n\t\treturn true;\n\t}\n\tif (shinfo->nr_frags)\n\t\treturn false;\n\treturn true;\n}\n\nstatic int inline_size(const struct sk_buff *skb)\n{\n\tif (skb->len + CTRL_SIZE + sizeof(struct mlx4_wqe_inline_seg)\n\t    <= MLX4_INLINE_ALIGN)\n\t\treturn ALIGN(skb->len + CTRL_SIZE +\n\t\t\t     sizeof(struct mlx4_wqe_inline_seg), 16);\n\telse\n\t\treturn ALIGN(skb->len + CTRL_SIZE + 2 *\n\t\t\t     sizeof(struct mlx4_wqe_inline_seg), 16);\n}\n\nstatic int get_real_size(const struct sk_buff *skb,\n\t\t\t const struct skb_shared_info *shinfo,\n\t\t\t struct net_device *dev,\n\t\t\t int *lso_header_size,\n\t\t\t bool *inline_ok,\n\t\t\t void **pfrag,\n\t\t\t int *hopbyhop)\n{\n\tstruct mlx4_en_priv *priv = netdev_priv(dev);\n\tint real_size;\n\n\tif (shinfo->gso_size) {\n\t\t*inline_ok = false;\n\t\t*hopbyhop = 0;\n\t\tif (skb->encapsulation) {\n\t\t\t*lso_header_size = skb_inner_tcp_all_headers(skb);\n\t\t} else {\n\t\t\t \n\t\t\tif (ipv6_has_hopopt_jumbo(skb))\n\t\t\t\t*hopbyhop = sizeof(struct hop_jumbo_hdr);\n\t\t\t*lso_header_size = skb_tcp_all_headers(skb);\n\t\t}\n\t\treal_size = CTRL_SIZE + shinfo->nr_frags * DS_SIZE +\n\t\t\tALIGN(*lso_header_size - *hopbyhop + 4, DS_SIZE);\n\t\tif (unlikely(*lso_header_size != skb_headlen(skb))) {\n\t\t\t \n\t\t\tif (*lso_header_size < skb_headlen(skb))\n\t\t\t\treal_size += DS_SIZE;\n\t\t\telse {\n\t\t\t\tif (netif_msg_tx_err(priv))\n\t\t\t\t\ten_warn(priv, \"Non-linear headers\\n\");\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t*lso_header_size = 0;\n\t\t*inline_ok = is_inline(priv->prof->inline_thold, skb,\n\t\t\t\t       shinfo, pfrag);\n\n\t\tif (*inline_ok)\n\t\t\treal_size = inline_size(skb);\n\t\telse\n\t\t\treal_size = CTRL_SIZE +\n\t\t\t\t    (shinfo->nr_frags + 1) * DS_SIZE;\n\t}\n\n\treturn real_size;\n}\n\nstatic void build_inline_wqe(struct mlx4_en_tx_desc *tx_desc,\n\t\t\t     const struct sk_buff *skb,\n\t\t\t     const struct skb_shared_info *shinfo,\n\t\t\t     void *fragptr)\n{\n\tstruct mlx4_wqe_inline_seg *inl = &tx_desc->inl;\n\tint spc = MLX4_INLINE_ALIGN - CTRL_SIZE - sizeof(*inl);\n\tunsigned int hlen = skb_headlen(skb);\n\n\tif (skb->len <= spc) {\n\t\tif (likely(skb->len >= MIN_PKT_LEN)) {\n\t\t\tinl->byte_count = cpu_to_be32(1 << 31 | skb->len);\n\t\t} else {\n\t\t\tinl->byte_count = cpu_to_be32(1 << 31 | MIN_PKT_LEN);\n\t\t\tmemset(inl->data + skb->len, 0,\n\t\t\t       MIN_PKT_LEN - skb->len);\n\t\t}\n\t\tskb_copy_from_linear_data(skb, inl->data, hlen);\n\t\tif (shinfo->nr_frags)\n\t\t\tmemcpy(inl->data + hlen, fragptr,\n\t\t\t       skb_frag_size(&shinfo->frags[0]));\n\n\t} else {\n\t\tinl->byte_count = cpu_to_be32(1 << 31 | spc);\n\t\tif (hlen <= spc) {\n\t\t\tskb_copy_from_linear_data(skb, inl->data, hlen);\n\t\t\tif (hlen < spc) {\n\t\t\t\tmemcpy(inl->data + hlen,\n\t\t\t\t       fragptr, spc - hlen);\n\t\t\t\tfragptr +=  spc - hlen;\n\t\t\t}\n\t\t\tinl = (void *)inl->data + spc;\n\t\t\tmemcpy(inl->data, fragptr, skb->len - spc);\n\t\t} else {\n\t\t\tskb_copy_from_linear_data(skb, inl->data, spc);\n\t\t\tinl = (void *)inl->data + spc;\n\t\t\tskb_copy_from_linear_data_offset(skb, spc, inl->data,\n\t\t\t\t\t\t\t hlen - spc);\n\t\t\tif (shinfo->nr_frags)\n\t\t\t\tmemcpy(inl->data + hlen - spc,\n\t\t\t\t       fragptr,\n\t\t\t\t       skb_frag_size(&shinfo->frags[0]));\n\t\t}\n\n\t\tdma_wmb();\n\t\tinl->byte_count = cpu_to_be32(1 << 31 | (skb->len - spc));\n\t}\n}\n\nu16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t\t struct net_device *sb_dev)\n{\n\tstruct mlx4_en_priv *priv = netdev_priv(dev);\n\tu16 rings_p_up = priv->num_tx_rings_p_up;\n\n\tif (netdev_get_num_tc(dev))\n\t\treturn netdev_pick_tx(dev, skb, NULL);\n\n\treturn netdev_pick_tx(dev, skb, NULL) % rings_p_up;\n}\n\nstatic void mlx4_bf_copy(void __iomem *dst, const void *src,\n\t\t\t unsigned int bytecnt)\n{\n\t__iowrite64_copy(dst, src, bytecnt / 8);\n}\n\nvoid mlx4_en_xmit_doorbell(struct mlx4_en_tx_ring *ring)\n{\n\twmb();\n\t \n#if defined(__LITTLE_ENDIAN)\n\tiowrite32(\n#else\n\tiowrite32be(\n#endif\n\t\t  (__force u32)ring->doorbell_qpn, ring->doorbell_address);\n}\n\nstatic void mlx4_en_tx_write_desc(struct mlx4_en_tx_ring *ring,\n\t\t\t\t  struct mlx4_en_tx_desc *tx_desc,\n\t\t\t\t  union mlx4_wqe_qpn_vlan qpn_vlan,\n\t\t\t\t  int desc_size, int bf_index,\n\t\t\t\t  __be32 op_own, bool bf_ok,\n\t\t\t\t  bool send_doorbell)\n{\n\ttx_desc->ctrl.qpn_vlan = qpn_vlan;\n\n\tif (bf_ok) {\n\t\top_own |= htonl((bf_index & 0xffff) << 8);\n\t\t \n\t\tdma_wmb();\n\t\ttx_desc->ctrl.owner_opcode = op_own;\n\n\t\twmb();\n\n\t\tmlx4_bf_copy(ring->bf.reg + ring->bf.offset, &tx_desc->ctrl,\n\t\t\t     desc_size);\n\n\t\twmb();\n\n\t\tring->bf.offset ^= ring->bf.buf_size;\n\t} else {\n\t\t \n\t\tdma_wmb();\n\t\ttx_desc->ctrl.owner_opcode = op_own;\n\t\tif (send_doorbell)\n\t\t\tmlx4_en_xmit_doorbell(ring);\n\t\telse\n\t\t\tring->xmit_more++;\n\t}\n}\n\nstatic bool mlx4_en_build_dma_wqe(struct mlx4_en_priv *priv,\n\t\t\t\t  struct skb_shared_info *shinfo,\n\t\t\t\t  struct mlx4_wqe_data_seg *data,\n\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t  int lso_header_size,\n\t\t\t\t  __be32 mr_key,\n\t\t\t\t  struct mlx4_en_tx_info *tx_info)\n{\n\tstruct device *ddev = priv->ddev;\n\tdma_addr_t dma = 0;\n\tu32 byte_count = 0;\n\tint i_frag;\n\n\t \n\tfor (i_frag = shinfo->nr_frags - 1; i_frag >= 0; i_frag--) {\n\t\tconst skb_frag_t *frag = &shinfo->frags[i_frag];\n\t\tbyte_count = skb_frag_size(frag);\n\t\tdma = skb_frag_dma_map(ddev, frag,\n\t\t\t\t       0, byte_count,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(ddev, dma))\n\t\t\tgoto tx_drop_unmap;\n\n\t\tdata->addr = cpu_to_be64(dma);\n\t\tdata->lkey = mr_key;\n\t\tdma_wmb();\n\t\tdata->byte_count = cpu_to_be32(byte_count);\n\t\t--data;\n\t}\n\n\t \n\tif (tx_info->linear) {\n\t\tbyte_count = skb_headlen(skb) - lso_header_size;\n\n\t\tdma = dma_map_single(ddev, skb->data +\n\t\t\t\t     lso_header_size, byte_count,\n\t\t\t\t     DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(ddev, dma))\n\t\t\tgoto tx_drop_unmap;\n\n\t\tdata->addr = cpu_to_be64(dma);\n\t\tdata->lkey = mr_key;\n\t\tdma_wmb();\n\t\tdata->byte_count = cpu_to_be32(byte_count);\n\t}\n\t \n\ttx_info->map0_dma = dma;\n\ttx_info->map0_byte_count = byte_count;\n\n\treturn true;\n\ntx_drop_unmap:\n\ten_err(priv, \"DMA mapping error\\n\");\n\n\twhile (++i_frag < shinfo->nr_frags) {\n\t\t++data;\n\t\tdma_unmap_page(ddev, (dma_addr_t)be64_to_cpu(data->addr),\n\t\t\t       be32_to_cpu(data->byte_count),\n\t\t\t       DMA_TO_DEVICE);\n\t}\n\n\treturn false;\n}\n\nnetdev_tx_t mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tstruct mlx4_en_priv *priv = netdev_priv(dev);\n\tunion mlx4_wqe_qpn_vlan\tqpn_vlan = {};\n\tstruct mlx4_en_tx_ring *ring;\n\tstruct mlx4_en_tx_desc *tx_desc;\n\tstruct mlx4_wqe_data_seg *data;\n\tstruct mlx4_en_tx_info *tx_info;\n\tu32 __maybe_unused ring_cons;\n\tint tx_ind;\n\tint nr_txbb;\n\tint desc_size;\n\tint real_size;\n\tu32 index, bf_index;\n\tstruct ipv6hdr *h6;\n\t__be32 op_own;\n\tint lso_header_size;\n\tvoid *fragptr = NULL;\n\tbool bounce = false;\n\tbool send_doorbell;\n\tbool stop_queue;\n\tbool inline_ok;\n\tu8 data_offset;\n\tint hopbyhop;\n\tbool bf_ok;\n\n\ttx_ind = skb_get_queue_mapping(skb);\n\tring = priv->tx_ring[TX][tx_ind];\n\n\tif (unlikely(!priv->port_up))\n\t\tgoto tx_drop;\n\n\treal_size = get_real_size(skb, shinfo, dev, &lso_header_size,\n\t\t\t\t  &inline_ok, &fragptr, &hopbyhop);\n\tif (unlikely(!real_size))\n\t\tgoto tx_drop_count;\n\n\t \n\tdesc_size = ALIGN(real_size, TXBB_SIZE);\n\tnr_txbb = desc_size >> LOG_TXBB_SIZE;\n\n\tbf_ok = ring->bf_enabled;\n\tif (skb_vlan_tag_present(skb)) {\n\t\tu16 vlan_proto;\n\n\t\tqpn_vlan.vlan_tag = cpu_to_be16(skb_vlan_tag_get(skb));\n\t\tvlan_proto = be16_to_cpu(skb->vlan_proto);\n\t\tif (vlan_proto == ETH_P_8021AD)\n\t\t\tqpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_SVLAN;\n\t\telse if (vlan_proto == ETH_P_8021Q)\n\t\t\tqpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_CVLAN;\n\t\telse\n\t\t\tqpn_vlan.ins_vlan = 0;\n\t\tbf_ok = false;\n\t}\n\n\tnetdev_txq_bql_enqueue_prefetchw(ring->tx_queue);\n\n\t \n\tindex = ring->prod & ring->size_mask;\n\tbf_index = ring->prod;\n\n\t \n\tif (likely(index + nr_txbb <= ring->size))\n\t\ttx_desc = ring->buf + (index << LOG_TXBB_SIZE);\n\telse {\n\t\tif (unlikely(nr_txbb > MLX4_MAX_DESC_TXBBS)) {\n\t\t\tif (netif_msg_tx_err(priv))\n\t\t\t\ten_warn(priv, \"Oversized header or SG list\\n\");\n\t\t\tgoto tx_drop_count;\n\t\t}\n\t\ttx_desc = (struct mlx4_en_tx_desc *) ring->bounce_buf;\n\t\tbounce = true;\n\t\tbf_ok = false;\n\t}\n\n\t \n\ttx_info = &ring->tx_info[index];\n\ttx_info->skb = skb;\n\ttx_info->nr_txbb = nr_txbb;\n\n\tif (!lso_header_size) {\n\t\tdata = &tx_desc->data;\n\t\tdata_offset = offsetof(struct mlx4_en_tx_desc, data);\n\t} else {\n\t\tint lso_align = ALIGN(lso_header_size - hopbyhop + 4, DS_SIZE);\n\n\t\tdata = (void *)&tx_desc->lso + lso_align;\n\t\tdata_offset = offsetof(struct mlx4_en_tx_desc, lso) + lso_align;\n\t}\n\n\t \n\ttx_info->data_offset = data_offset;\n\n\ttx_info->inl = inline_ok;\n\n\ttx_info->linear = lso_header_size < skb_headlen(skb) && !inline_ok;\n\n\ttx_info->nr_maps = shinfo->nr_frags + tx_info->linear;\n\tdata += tx_info->nr_maps - 1;\n\n\tif (!tx_info->inl)\n\t\tif (!mlx4_en_build_dma_wqe(priv, shinfo, data, skb,\n\t\t\t\t\t   lso_header_size, ring->mr_key,\n\t\t\t\t\t   tx_info))\n\t\t\tgoto tx_drop_count;\n\n\t \n\ttx_info->ts_requested = 0;\n\tif (unlikely(ring->hwtstamp_tx_type == HWTSTAMP_TX_ON &&\n\t\t     shinfo->tx_flags & SKBTX_HW_TSTAMP)) {\n\t\tshinfo->tx_flags |= SKBTX_IN_PROGRESS;\n\t\ttx_info->ts_requested = 1;\n\t}\n\n\t \n\ttx_desc->ctrl.srcrb_flags = priv->ctrl_flags;\n\tif (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {\n\t\tif (!skb->encapsulation)\n\t\t\ttx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM |\n\t\t\t\t\t\t\t\t MLX4_WQE_CTRL_TCP_UDP_CSUM);\n\t\telse\n\t\t\ttx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM);\n\t\tring->tx_csum++;\n\t}\n\n\tif (priv->flags & MLX4_EN_FLAG_ENABLE_HW_LOOPBACK) {\n\t\tstruct ethhdr *ethh;\n\n\t\t \n\t\tethh = (struct ethhdr *)skb->data;\n\t\ttx_desc->ctrl.srcrb_flags16[0] = get_unaligned((__be16 *)ethh->h_dest);\n\t\ttx_desc->ctrl.imm = get_unaligned((__be32 *)(ethh->h_dest + 2));\n\t}\n\n\t \n\tif (lso_header_size) {\n\t\tint i;\n\n\t\t \n\t\top_own = cpu_to_be32(MLX4_OPCODE_LSO | (1 << 6)) |\n\t\t\t((ring->prod & ring->size) ?\n\t\t\t\tcpu_to_be32(MLX4_EN_BIT_DESC_OWN) : 0);\n\n\t\tlso_header_size -= hopbyhop;\n\t\t \n\t\ttx_desc->lso.mss_hdr_size = cpu_to_be32(\n\t\t\tshinfo->gso_size << 16 | lso_header_size);\n\n\n\t\tif (unlikely(hopbyhop)) {\n\t\t\t \n\t\t\tmemcpy(tx_desc->lso.header, skb->data, ETH_HLEN + sizeof(*h6));\n\t\t\th6 = (struct ipv6hdr *)((char *)tx_desc->lso.header + ETH_HLEN);\n\t\t\th6->nexthdr = IPPROTO_TCP;\n\t\t\t \n\t\t\tmemcpy(h6 + 1,\n\t\t\t       skb->data + ETH_HLEN + sizeof(*h6) +\n\t\t\t\t\tsizeof(struct hop_jumbo_hdr),\n\t\t\t       tcp_hdrlen(skb));\n\t\t\t \n\t\t} else {\n\t\t\t \n\t\t\tmemcpy(tx_desc->lso.header, skb->data, lso_header_size);\n\t\t}\n\t\tring->tso_packets++;\n\n\t\ti = shinfo->gso_segs;\n\t\ttx_info->nr_bytes = skb->len + (i - 1) * lso_header_size;\n\t\tring->packets += i;\n\t} else {\n\t\t \n\t\top_own = cpu_to_be32(MLX4_OPCODE_SEND) |\n\t\t\t((ring->prod & ring->size) ?\n\t\t\t cpu_to_be32(MLX4_EN_BIT_DESC_OWN) : 0);\n\t\ttx_info->nr_bytes = max_t(unsigned int, skb->len, ETH_ZLEN);\n\t\tring->packets++;\n\t}\n\tring->bytes += tx_info->nr_bytes;\n\n\tif (tx_info->inl)\n\t\tbuild_inline_wqe(tx_desc, skb, shinfo, fragptr);\n\n\tif (skb->encapsulation) {\n\t\tunion {\n\t\t\tstruct iphdr *v4;\n\t\t\tstruct ipv6hdr *v6;\n\t\t\tunsigned char *hdr;\n\t\t} ip;\n\t\tu8 proto;\n\n\t\tip.hdr = skb_inner_network_header(skb);\n\t\tproto = (ip.v4->version == 4) ? ip.v4->protocol :\n\t\t\t\t\t\tip.v6->nexthdr;\n\n\t\tif (proto == IPPROTO_TCP || proto == IPPROTO_UDP)\n\t\t\top_own |= cpu_to_be32(MLX4_WQE_CTRL_IIP | MLX4_WQE_CTRL_ILP);\n\t\telse\n\t\t\top_own |= cpu_to_be32(MLX4_WQE_CTRL_IIP);\n\t}\n\n\tWRITE_ONCE(ring->prod, ring->prod + nr_txbb);\n\n\t \n\tif (unlikely(bounce))\n\t\ttx_desc = mlx4_en_bounce_to_desc(priv, ring, index, desc_size);\n\n\tskb_tx_timestamp(skb);\n\n\t \n\tstop_queue = mlx4_en_is_tx_ring_full(ring);\n\tif (unlikely(stop_queue)) {\n\t\tnetif_tx_stop_queue(ring->tx_queue);\n\t\tring->queue_stopped++;\n\t}\n\n\tsend_doorbell = __netdev_tx_sent_queue(ring->tx_queue,\n\t\t\t\t\t       tx_info->nr_bytes,\n\t\t\t\t\t       netdev_xmit_more());\n\n\treal_size = (real_size / 16) & 0x3f;\n\n\tbf_ok &= desc_size <= MAX_BF && send_doorbell;\n\n\tif (bf_ok)\n\t\tqpn_vlan.bf_qpn = ring->doorbell_qpn | cpu_to_be32(real_size);\n\telse\n\t\tqpn_vlan.fence_size = real_size;\n\n\tmlx4_en_tx_write_desc(ring, tx_desc, qpn_vlan, desc_size, bf_index,\n\t\t\t      op_own, bf_ok, send_doorbell);\n\n\tif (unlikely(stop_queue)) {\n\t\t \n\t\tsmp_rmb();\n\n\t\tif (unlikely(!mlx4_en_is_tx_ring_full(ring))) {\n\t\t\tnetif_tx_wake_queue(ring->tx_queue);\n\t\t\tring->wake_queue++;\n\t\t}\n\t}\n\treturn NETDEV_TX_OK;\n\ntx_drop_count:\n\tring->tx_dropped++;\ntx_drop:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\n#define MLX4_EN_XDP_TX_NRTXBB  1\n#define MLX4_EN_XDP_TX_REAL_SZ (((CTRL_SIZE + MLX4_EN_XDP_TX_NRTXBB * DS_SIZE) \\\n\t\t\t\t / 16) & 0x3f)\n\nvoid mlx4_en_init_tx_xdp_ring_descs(struct mlx4_en_priv *priv,\n\t\t\t\t    struct mlx4_en_tx_ring *ring)\n{\n\tint i;\n\n\tfor (i = 0; i < ring->size; i++) {\n\t\tstruct mlx4_en_tx_info *tx_info = &ring->tx_info[i];\n\t\tstruct mlx4_en_tx_desc *tx_desc = ring->buf +\n\t\t\t(i << LOG_TXBB_SIZE);\n\n\t\ttx_info->map0_byte_count = PAGE_SIZE;\n\t\ttx_info->nr_txbb = MLX4_EN_XDP_TX_NRTXBB;\n\t\ttx_info->data_offset = offsetof(struct mlx4_en_tx_desc, data);\n\t\ttx_info->ts_requested = 0;\n\t\ttx_info->nr_maps = 1;\n\t\ttx_info->linear = 1;\n\t\ttx_info->inl = 0;\n\n\t\ttx_desc->data.lkey = ring->mr_key;\n\t\ttx_desc->ctrl.qpn_vlan.fence_size = MLX4_EN_XDP_TX_REAL_SZ;\n\t\ttx_desc->ctrl.srcrb_flags = priv->ctrl_flags;\n\t}\n}\n\nnetdev_tx_t mlx4_en_xmit_frame(struct mlx4_en_rx_ring *rx_ring,\n\t\t\t       struct mlx4_en_rx_alloc *frame,\n\t\t\t       struct mlx4_en_priv *priv, unsigned int length,\n\t\t\t       int tx_ind, bool *doorbell_pending)\n{\n\tstruct mlx4_en_tx_desc *tx_desc;\n\tstruct mlx4_en_tx_info *tx_info;\n\tstruct mlx4_wqe_data_seg *data;\n\tstruct mlx4_en_tx_ring *ring;\n\tdma_addr_t dma;\n\t__be32 op_own;\n\tint index;\n\n\tif (unlikely(!priv->port_up))\n\t\tgoto tx_drop;\n\n\tring = priv->tx_ring[TX_XDP][tx_ind];\n\n\tif (unlikely(mlx4_en_is_tx_ring_full(ring)))\n\t\tgoto tx_drop_count;\n\n\tindex = ring->prod & ring->size_mask;\n\ttx_info = &ring->tx_info[index];\n\n\ttx_desc = ring->buf + (index << LOG_TXBB_SIZE);\n\tdata = &tx_desc->data;\n\n\tdma = frame->dma;\n\n\ttx_info->page = frame->page;\n\tframe->page = NULL;\n\ttx_info->map0_dma = dma;\n\ttx_info->nr_bytes = max_t(unsigned int, length, ETH_ZLEN);\n\n\tdma_sync_single_range_for_device(priv->ddev, dma, frame->page_offset,\n\t\t\t\t\t length, DMA_TO_DEVICE);\n\n\tdata->addr = cpu_to_be64(dma + frame->page_offset);\n\tdma_wmb();\n\tdata->byte_count = cpu_to_be32(length);\n\n\t \n\n\top_own = cpu_to_be32(MLX4_OPCODE_SEND) |\n\t\t((ring->prod & ring->size) ?\n\t\t cpu_to_be32(MLX4_EN_BIT_DESC_OWN) : 0);\n\n\trx_ring->xdp_tx++;\n\n\tWRITE_ONCE(ring->prod, ring->prod + MLX4_EN_XDP_TX_NRTXBB);\n\n\t \n\tdma_wmb();\n\ttx_desc->ctrl.owner_opcode = op_own;\n\tring->xmit_more++;\n\n\t*doorbell_pending = true;\n\n\treturn NETDEV_TX_OK;\n\ntx_drop_count:\n\trx_ring->xdp_tx_full++;\n\t*doorbell_pending = true;\ntx_drop:\n\treturn NETDEV_TX_BUSY;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}