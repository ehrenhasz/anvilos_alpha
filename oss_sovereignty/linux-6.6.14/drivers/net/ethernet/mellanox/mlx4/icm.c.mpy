{
  "module_name": "icm.c",
  "hash_id": "030ae989237d4f870e0379c853cd62dd31acacf7ac4878a81bbf8a1fbfe0f29c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx4/icm.c",
  "human_readable_source": " \n\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n\n#include <linux/mlx4/cmd.h>\n\n#include \"mlx4.h\"\n#include \"icm.h\"\n#include \"fw.h\"\n\n \nenum {\n\tMLX4_ICM_ALLOC_SIZE\t= 1 << 18,\n\tMLX4_TABLE_CHUNK_SIZE\t= 1 << 18,\n};\n\nstatic void mlx4_free_icm_pages(struct mlx4_dev *dev, struct mlx4_icm_chunk *chunk)\n{\n\tint i;\n\n\tif (chunk->nsg > 0)\n\t\tdma_unmap_sg(&dev->persist->pdev->dev, chunk->sg, chunk->npages,\n\t\t\t     DMA_BIDIRECTIONAL);\n\n\tfor (i = 0; i < chunk->npages; ++i)\n\t\t__free_pages(sg_page(&chunk->sg[i]),\n\t\t\t     get_order(chunk->sg[i].length));\n}\n\nstatic void mlx4_free_icm_coherent(struct mlx4_dev *dev, struct mlx4_icm_chunk *chunk)\n{\n\tint i;\n\n\tfor (i = 0; i < chunk->npages; ++i)\n\t\tdma_free_coherent(&dev->persist->pdev->dev,\n\t\t\t\t  chunk->buf[i].size,\n\t\t\t\t  chunk->buf[i].addr,\n\t\t\t\t  chunk->buf[i].dma_addr);\n}\n\nvoid mlx4_free_icm(struct mlx4_dev *dev, struct mlx4_icm *icm, int coherent)\n{\n\tstruct mlx4_icm_chunk *chunk, *tmp;\n\n\tif (!icm)\n\t\treturn;\n\n\tlist_for_each_entry_safe(chunk, tmp, &icm->chunk_list, list) {\n\t\tif (coherent)\n\t\t\tmlx4_free_icm_coherent(dev, chunk);\n\t\telse\n\t\t\tmlx4_free_icm_pages(dev, chunk);\n\n\t\tkfree(chunk);\n\t}\n\n\tkfree(icm);\n}\n\nstatic int mlx4_alloc_icm_pages(struct scatterlist *mem, int order,\n\t\t\t\tgfp_t gfp_mask, int node)\n{\n\tstruct page *page;\n\n\tpage = alloc_pages_node(node, gfp_mask, order);\n\tif (!page) {\n\t\tpage = alloc_pages(gfp_mask, order);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tsg_set_page(mem, page, PAGE_SIZE << order, 0);\n\treturn 0;\n}\n\nstatic int mlx4_alloc_icm_coherent(struct device *dev, struct mlx4_icm_buf *buf,\n\t\t\t\t   int order, gfp_t gfp_mask)\n{\n\tbuf->addr = dma_alloc_coherent(dev, PAGE_SIZE << order,\n\t\t\t\t       &buf->dma_addr, gfp_mask);\n\tif (!buf->addr)\n\t\treturn -ENOMEM;\n\n\tif (offset_in_page(buf->addr)) {\n\t\tdma_free_coherent(dev, PAGE_SIZE << order, buf->addr,\n\t\t\t\t  buf->dma_addr);\n\t\treturn -ENOMEM;\n\t}\n\n\tbuf->size = PAGE_SIZE << order;\n\treturn 0;\n}\n\nstruct mlx4_icm *mlx4_alloc_icm(struct mlx4_dev *dev, int npages,\n\t\t\t\tgfp_t gfp_mask, int coherent)\n{\n\tstruct mlx4_icm *icm;\n\tstruct mlx4_icm_chunk *chunk = NULL;\n\tint cur_order;\n\tgfp_t mask;\n\tint ret;\n\n\t \n\tBUG_ON(coherent && (gfp_mask & __GFP_HIGHMEM));\n\n\ticm = kmalloc_node(sizeof(*icm),\n\t\t\t   gfp_mask & ~(__GFP_HIGHMEM | __GFP_NOWARN),\n\t\t\t   dev->numa_node);\n\tif (!icm) {\n\t\ticm = kmalloc(sizeof(*icm),\n\t\t\t      gfp_mask & ~(__GFP_HIGHMEM | __GFP_NOWARN));\n\t\tif (!icm)\n\t\t\treturn NULL;\n\t}\n\n\ticm->refcount = 0;\n\tINIT_LIST_HEAD(&icm->chunk_list);\n\n\tcur_order = get_order(MLX4_ICM_ALLOC_SIZE);\n\n\twhile (npages > 0) {\n\t\tif (!chunk) {\n\t\t\tchunk = kzalloc_node(sizeof(*chunk),\n\t\t\t\t\t     gfp_mask & ~(__GFP_HIGHMEM |\n\t\t\t\t\t\t\t  __GFP_NOWARN),\n\t\t\t\t\t     dev->numa_node);\n\t\t\tif (!chunk) {\n\t\t\t\tchunk = kzalloc(sizeof(*chunk),\n\t\t\t\t\t\tgfp_mask & ~(__GFP_HIGHMEM |\n\t\t\t\t\t\t\t     __GFP_NOWARN));\n\t\t\t\tif (!chunk)\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\tchunk->coherent = coherent;\n\n\t\t\tif (!coherent)\n\t\t\t\tsg_init_table(chunk->sg, MLX4_ICM_CHUNK_LEN);\n\t\t\tlist_add_tail(&chunk->list, &icm->chunk_list);\n\t\t}\n\n\t\twhile (1 << cur_order > npages)\n\t\t\t--cur_order;\n\n\t\tmask = gfp_mask;\n\t\tif (cur_order)\n\t\t\tmask &= ~__GFP_DIRECT_RECLAIM;\n\n\t\tif (coherent)\n\t\t\tret = mlx4_alloc_icm_coherent(&dev->persist->pdev->dev,\n\t\t\t\t\t\t&chunk->buf[chunk->npages],\n\t\t\t\t\t\tcur_order, mask);\n\t\telse\n\t\t\tret = mlx4_alloc_icm_pages(&chunk->sg[chunk->npages],\n\t\t\t\t\t\t   cur_order, mask,\n\t\t\t\t\t\t   dev->numa_node);\n\n\t\tif (ret) {\n\t\t\tif (--cur_order < 0)\n\t\t\t\tgoto fail;\n\t\t\telse\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t++chunk->npages;\n\n\t\tif (coherent)\n\t\t\t++chunk->nsg;\n\t\telse if (chunk->npages == MLX4_ICM_CHUNK_LEN) {\n\t\t\tchunk->nsg = dma_map_sg(&dev->persist->pdev->dev,\n\t\t\t\t\t\tchunk->sg, chunk->npages,\n\t\t\t\t\t\tDMA_BIDIRECTIONAL);\n\n\t\t\tif (!chunk->nsg)\n\t\t\t\tgoto fail;\n\t\t}\n\n\t\tif (chunk->npages == MLX4_ICM_CHUNK_LEN)\n\t\t\tchunk = NULL;\n\n\t\tnpages -= 1 << cur_order;\n\t}\n\n\tif (!coherent && chunk) {\n\t\tchunk->nsg = dma_map_sg(&dev->persist->pdev->dev, chunk->sg,\n\t\t\t\t\tchunk->npages, DMA_BIDIRECTIONAL);\n\n\t\tif (!chunk->nsg)\n\t\t\tgoto fail;\n\t}\n\n\treturn icm;\n\nfail:\n\tmlx4_free_icm(dev, icm, coherent);\n\treturn NULL;\n}\n\nstatic int mlx4_MAP_ICM(struct mlx4_dev *dev, struct mlx4_icm *icm, u64 virt)\n{\n\treturn mlx4_map_cmd(dev, MLX4_CMD_MAP_ICM, icm, virt);\n}\n\nstatic int mlx4_UNMAP_ICM(struct mlx4_dev *dev, u64 virt, u32 page_count)\n{\n\treturn mlx4_cmd(dev, virt, page_count, 0, MLX4_CMD_UNMAP_ICM,\n\t\t\tMLX4_CMD_TIME_CLASS_B, MLX4_CMD_NATIVE);\n}\n\nint mlx4_MAP_ICM_AUX(struct mlx4_dev *dev, struct mlx4_icm *icm)\n{\n\treturn mlx4_map_cmd(dev, MLX4_CMD_MAP_ICM_AUX, icm, -1);\n}\n\nint mlx4_UNMAP_ICM_AUX(struct mlx4_dev *dev)\n{\n\treturn mlx4_cmd(dev, 0, 0, 0, MLX4_CMD_UNMAP_ICM_AUX,\n\t\t\tMLX4_CMD_TIME_CLASS_B, MLX4_CMD_NATIVE);\n}\n\nint mlx4_table_get(struct mlx4_dev *dev, struct mlx4_icm_table *table, u32 obj)\n{\n\tu32 i = (obj & (table->num_obj - 1)) /\n\t\t\t(MLX4_TABLE_CHUNK_SIZE / table->obj_size);\n\tint ret = 0;\n\n\tmutex_lock(&table->mutex);\n\n\tif (table->icm[i]) {\n\t\t++table->icm[i]->refcount;\n\t\tgoto out;\n\t}\n\n\ttable->icm[i] = mlx4_alloc_icm(dev, MLX4_TABLE_CHUNK_SIZE >> PAGE_SHIFT,\n\t\t\t\t       (table->lowmem ? GFP_KERNEL : GFP_HIGHUSER) |\n\t\t\t\t       __GFP_NOWARN, table->coherent);\n\tif (!table->icm[i]) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (mlx4_MAP_ICM(dev, table->icm[i], table->virt +\n\t\t\t (u64) i * MLX4_TABLE_CHUNK_SIZE)) {\n\t\tmlx4_free_icm(dev, table->icm[i], table->coherent);\n\t\ttable->icm[i] = NULL;\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t++table->icm[i]->refcount;\n\nout:\n\tmutex_unlock(&table->mutex);\n\treturn ret;\n}\n\nvoid mlx4_table_put(struct mlx4_dev *dev, struct mlx4_icm_table *table, u32 obj)\n{\n\tu32 i;\n\tu64 offset;\n\n\ti = (obj & (table->num_obj - 1)) / (MLX4_TABLE_CHUNK_SIZE / table->obj_size);\n\n\tmutex_lock(&table->mutex);\n\n\tif (--table->icm[i]->refcount == 0) {\n\t\toffset = (u64) i * MLX4_TABLE_CHUNK_SIZE;\n\t\tmlx4_UNMAP_ICM(dev, table->virt + offset,\n\t\t\t       MLX4_TABLE_CHUNK_SIZE / MLX4_ICM_PAGE_SIZE);\n\t\tmlx4_free_icm(dev, table->icm[i], table->coherent);\n\t\ttable->icm[i] = NULL;\n\t}\n\n\tmutex_unlock(&table->mutex);\n}\n\nvoid *mlx4_table_find(struct mlx4_icm_table *table, u32 obj,\n\t\t\tdma_addr_t *dma_handle)\n{\n\tint offset, dma_offset, i;\n\tu64 idx;\n\tstruct mlx4_icm_chunk *chunk;\n\tstruct mlx4_icm *icm;\n\tvoid *addr = NULL;\n\n\tif (!table->lowmem)\n\t\treturn NULL;\n\n\tmutex_lock(&table->mutex);\n\n\tidx = (u64) (obj & (table->num_obj - 1)) * table->obj_size;\n\ticm = table->icm[idx / MLX4_TABLE_CHUNK_SIZE];\n\tdma_offset = offset = idx % MLX4_TABLE_CHUNK_SIZE;\n\n\tif (!icm)\n\t\tgoto out;\n\n\tlist_for_each_entry(chunk, &icm->chunk_list, list) {\n\t\tfor (i = 0; i < chunk->npages; ++i) {\n\t\t\tdma_addr_t dma_addr;\n\t\t\tsize_t len;\n\n\t\t\tif (table->coherent) {\n\t\t\t\tlen = chunk->buf[i].size;\n\t\t\t\tdma_addr = chunk->buf[i].dma_addr;\n\t\t\t\taddr = chunk->buf[i].addr;\n\t\t\t} else {\n\t\t\t\tstruct page *page;\n\n\t\t\t\tlen = sg_dma_len(&chunk->sg[i]);\n\t\t\t\tdma_addr = sg_dma_address(&chunk->sg[i]);\n\n\t\t\t\t \n\t\t\t\tpage = sg_page(&chunk->sg[i]);\n\t\t\t\taddr = lowmem_page_address(page);\n\t\t\t}\n\n\t\t\tif (dma_handle && dma_offset >= 0) {\n\t\t\t\tif (len > dma_offset)\n\t\t\t\t\t*dma_handle = dma_addr + dma_offset;\n\t\t\t\tdma_offset -= len;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (len > offset)\n\t\t\t\tgoto out;\n\t\t\toffset -= len;\n\t\t}\n\t}\n\n\taddr = NULL;\nout:\n\tmutex_unlock(&table->mutex);\n\treturn addr ? addr + offset : NULL;\n}\n\nint mlx4_table_get_range(struct mlx4_dev *dev, struct mlx4_icm_table *table,\n\t\t\t u32 start, u32 end)\n{\n\tint inc = MLX4_TABLE_CHUNK_SIZE / table->obj_size;\n\tint err;\n\tu32 i;\n\n\tfor (i = start; i <= end; i += inc) {\n\t\terr = mlx4_table_get(dev, table, i);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n\treturn 0;\n\nfail:\n\twhile (i > start) {\n\t\ti -= inc;\n\t\tmlx4_table_put(dev, table, i);\n\t}\n\n\treturn err;\n}\n\nvoid mlx4_table_put_range(struct mlx4_dev *dev, struct mlx4_icm_table *table,\n\t\t\t  u32 start, u32 end)\n{\n\tu32 i;\n\n\tfor (i = start; i <= end; i += MLX4_TABLE_CHUNK_SIZE / table->obj_size)\n\t\tmlx4_table_put(dev, table, i);\n}\n\nint mlx4_init_icm_table(struct mlx4_dev *dev, struct mlx4_icm_table *table,\n\t\t\tu64 virt, int obj_size,\tu32 nobj, int reserved,\n\t\t\tint use_lowmem, int use_coherent)\n{\n\tint obj_per_chunk;\n\tint num_icm;\n\tunsigned chunk_size;\n\tint i;\n\tu64 size;\n\n\tobj_per_chunk = MLX4_TABLE_CHUNK_SIZE / obj_size;\n\tif (WARN_ON(!obj_per_chunk))\n\t\treturn -EINVAL;\n\tnum_icm = DIV_ROUND_UP(nobj, obj_per_chunk);\n\n\ttable->icm      = kvcalloc(num_icm, sizeof(*table->icm), GFP_KERNEL);\n\tif (!table->icm)\n\t\treturn -ENOMEM;\n\ttable->virt     = virt;\n\ttable->num_icm  = num_icm;\n\ttable->num_obj  = nobj;\n\ttable->obj_size = obj_size;\n\ttable->lowmem   = use_lowmem;\n\ttable->coherent = use_coherent;\n\tmutex_init(&table->mutex);\n\n\tsize = (u64) nobj * obj_size;\n\tfor (i = 0; i * MLX4_TABLE_CHUNK_SIZE < reserved * obj_size; ++i) {\n\t\tchunk_size = MLX4_TABLE_CHUNK_SIZE;\n\t\tif ((i + 1) * MLX4_TABLE_CHUNK_SIZE > size)\n\t\t\tchunk_size = PAGE_ALIGN(size -\n\t\t\t\t\ti * MLX4_TABLE_CHUNK_SIZE);\n\n\t\ttable->icm[i] = mlx4_alloc_icm(dev, chunk_size >> PAGE_SHIFT,\n\t\t\t\t\t       (use_lowmem ? GFP_KERNEL : GFP_HIGHUSER) |\n\t\t\t\t\t       __GFP_NOWARN, use_coherent);\n\t\tif (!table->icm[i])\n\t\t\tgoto err;\n\t\tif (mlx4_MAP_ICM(dev, table->icm[i], virt + i * MLX4_TABLE_CHUNK_SIZE)) {\n\t\t\tmlx4_free_icm(dev, table->icm[i], use_coherent);\n\t\t\ttable->icm[i] = NULL;\n\t\t\tgoto err;\n\t\t}\n\n\t\t \n\t\t++table->icm[i]->refcount;\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (i = 0; i < num_icm; ++i)\n\t\tif (table->icm[i]) {\n\t\t\tmlx4_UNMAP_ICM(dev, virt + i * MLX4_TABLE_CHUNK_SIZE,\n\t\t\t\t       MLX4_TABLE_CHUNK_SIZE / MLX4_ICM_PAGE_SIZE);\n\t\t\tmlx4_free_icm(dev, table->icm[i], use_coherent);\n\t\t}\n\n\tkvfree(table->icm);\n\n\treturn -ENOMEM;\n}\n\nvoid mlx4_cleanup_icm_table(struct mlx4_dev *dev, struct mlx4_icm_table *table)\n{\n\tint i;\n\n\tfor (i = 0; i < table->num_icm; ++i)\n\t\tif (table->icm[i]) {\n\t\t\tmlx4_UNMAP_ICM(dev, table->virt + i * MLX4_TABLE_CHUNK_SIZE,\n\t\t\t\t       MLX4_TABLE_CHUNK_SIZE / MLX4_ICM_PAGE_SIZE);\n\t\t\tmlx4_free_icm(dev, table->icm[i], table->coherent);\n\t\t}\n\n\tkvfree(table->icm);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}