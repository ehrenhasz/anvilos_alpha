{
  "module_name": "resource_tracker.c",
  "hash_id": "883fd158b325c1fd1fcafe60097cd8a04f6edcda25cd5bcc208fe83cde835b2f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx4/resource_tracker.c",
  "human_readable_source": " \n\n#include <linux/sched.h>\n#include <linux/pci.h>\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/io.h>\n#include <linux/slab.h>\n#include <linux/mlx4/cmd.h>\n#include <linux/mlx4/qp.h>\n#include <linux/if_ether.h>\n#include <linux/etherdevice.h>\n\n#include \"mlx4.h\"\n#include \"fw.h\"\n#include \"mlx4_stats.h\"\n\n#define MLX4_MAC_VALID\t\t(1ull << 63)\n#define MLX4_PF_COUNTERS_PER_PORT\t2\n#define MLX4_VF_COUNTERS_PER_PORT\t1\n\nstruct mac_res {\n\tstruct list_head list;\n\tu64 mac;\n\tint ref_count;\n\tu8 smac_index;\n\tu8 port;\n};\n\nstruct vlan_res {\n\tstruct list_head list;\n\tu16 vlan;\n\tint ref_count;\n\tint vlan_index;\n\tu8 port;\n};\n\nstruct res_common {\n\tstruct list_head\tlist;\n\tstruct rb_node\t\tnode;\n\tu64\t\t        res_id;\n\tint\t\t\towner;\n\tint\t\t\tstate;\n\tint\t\t\tfrom_state;\n\tint\t\t\tto_state;\n\tint\t\t\tremoving;\n\tconst char\t\t*func_name;\n};\n\nenum {\n\tRES_ANY_BUSY = 1\n};\n\nstruct res_gid {\n\tstruct list_head\tlist;\n\tu8\t\t\tgid[16];\n\tenum mlx4_protocol\tprot;\n\tenum mlx4_steer_type\tsteer;\n\tu64\t\t\treg_id;\n};\n\nenum res_qp_states {\n\tRES_QP_BUSY = RES_ANY_BUSY,\n\n\t \n\tRES_QP_RESERVED,\n\n\t \n\tRES_QP_MAPPED,\n\n\t \n\tRES_QP_HW\n};\n\nstruct res_qp {\n\tstruct res_common\tcom;\n\tstruct res_mtt\t       *mtt;\n\tstruct res_cq\t       *rcq;\n\tstruct res_cq\t       *scq;\n\tstruct res_srq\t       *srq;\n\tstruct list_head\tmcg_list;\n\tspinlock_t\t\tmcg_spl;\n\tint\t\t\tlocal_qpn;\n\tatomic_t\t\tref_count;\n\tu32\t\t\tqpc_flags;\n\t \n\tu8\t\t\tsched_queue;\n\t__be32\t\t\tparam3;\n\tu8\t\t\tvlan_control;\n\tu8\t\t\tfvl_rx;\n\tu8\t\t\tpri_path_fl;\n\tu8\t\t\tvlan_index;\n\tu8\t\t\tfeup;\n};\n\nenum res_mtt_states {\n\tRES_MTT_BUSY = RES_ANY_BUSY,\n\tRES_MTT_ALLOCATED,\n};\n\nstatic inline const char *mtt_states_str(enum res_mtt_states state)\n{\n\tswitch (state) {\n\tcase RES_MTT_BUSY: return \"RES_MTT_BUSY\";\n\tcase RES_MTT_ALLOCATED: return \"RES_MTT_ALLOCATED\";\n\tdefault: return \"Unknown\";\n\t}\n}\n\nstruct res_mtt {\n\tstruct res_common\tcom;\n\tint\t\t\torder;\n\tatomic_t\t\tref_count;\n};\n\nenum res_mpt_states {\n\tRES_MPT_BUSY = RES_ANY_BUSY,\n\tRES_MPT_RESERVED,\n\tRES_MPT_MAPPED,\n\tRES_MPT_HW,\n};\n\nstruct res_mpt {\n\tstruct res_common\tcom;\n\tstruct res_mtt\t       *mtt;\n\tint\t\t\tkey;\n};\n\nenum res_eq_states {\n\tRES_EQ_BUSY = RES_ANY_BUSY,\n\tRES_EQ_RESERVED,\n\tRES_EQ_HW,\n};\n\nstruct res_eq {\n\tstruct res_common\tcom;\n\tstruct res_mtt\t       *mtt;\n};\n\nenum res_cq_states {\n\tRES_CQ_BUSY = RES_ANY_BUSY,\n\tRES_CQ_ALLOCATED,\n\tRES_CQ_HW,\n};\n\nstruct res_cq {\n\tstruct res_common\tcom;\n\tstruct res_mtt\t       *mtt;\n\tatomic_t\t\tref_count;\n};\n\nenum res_srq_states {\n\tRES_SRQ_BUSY = RES_ANY_BUSY,\n\tRES_SRQ_ALLOCATED,\n\tRES_SRQ_HW,\n};\n\nstruct res_srq {\n\tstruct res_common\tcom;\n\tstruct res_mtt\t       *mtt;\n\tstruct res_cq\t       *cq;\n\tatomic_t\t\tref_count;\n};\n\nenum res_counter_states {\n\tRES_COUNTER_BUSY = RES_ANY_BUSY,\n\tRES_COUNTER_ALLOCATED,\n};\n\nstruct res_counter {\n\tstruct res_common\tcom;\n\tint\t\t\tport;\n};\n\nenum res_xrcdn_states {\n\tRES_XRCD_BUSY = RES_ANY_BUSY,\n\tRES_XRCD_ALLOCATED,\n};\n\nstruct res_xrcdn {\n\tstruct res_common\tcom;\n\tint\t\t\tport;\n};\n\nenum res_fs_rule_states {\n\tRES_FS_RULE_BUSY = RES_ANY_BUSY,\n\tRES_FS_RULE_ALLOCATED,\n};\n\nstruct res_fs_rule {\n\tstruct res_common\tcom;\n\tint\t\t\tqpn;\n\t \n\tvoid\t\t\t*mirr_mbox;\n\t \n\t \n\tu32\t\t\tmirr_mbox_size;\n\tstruct list_head\tmirr_list;\n\tu64\t\t\tmirr_rule_id;\n};\n\nstatic void *res_tracker_lookup(struct rb_root *root, u64 res_id)\n{\n\tstruct rb_node *node = root->rb_node;\n\n\twhile (node) {\n\t\tstruct res_common *res = rb_entry(node, struct res_common,\n\t\t\t\t\t\t  node);\n\n\t\tif (res_id < res->res_id)\n\t\t\tnode = node->rb_left;\n\t\telse if (res_id > res->res_id)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn res;\n\t}\n\treturn NULL;\n}\n\nstatic int res_tracker_insert(struct rb_root *root, struct res_common *res)\n{\n\tstruct rb_node **new = &(root->rb_node), *parent = NULL;\n\n\t \n\twhile (*new) {\n\t\tstruct res_common *this = rb_entry(*new, struct res_common,\n\t\t\t\t\t\t   node);\n\n\t\tparent = *new;\n\t\tif (res->res_id < this->res_id)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse if (res->res_id > this->res_id)\n\t\t\tnew = &((*new)->rb_right);\n\t\telse\n\t\t\treturn -EEXIST;\n\t}\n\n\t \n\trb_link_node(&res->node, parent, new);\n\trb_insert_color(&res->node, root);\n\n\treturn 0;\n}\n\nenum qp_transition {\n\tQP_TRANS_INIT2RTR,\n\tQP_TRANS_RTR2RTS,\n\tQP_TRANS_RTS2RTS,\n\tQP_TRANS_SQERR2RTS,\n\tQP_TRANS_SQD2SQD,\n\tQP_TRANS_SQD2RTS\n};\n\n \nstatic const char *resource_str(enum mlx4_resource rt)\n{\n\tswitch (rt) {\n\tcase RES_QP: return \"RES_QP\";\n\tcase RES_CQ: return \"RES_CQ\";\n\tcase RES_SRQ: return \"RES_SRQ\";\n\tcase RES_MPT: return \"RES_MPT\";\n\tcase RES_MTT: return \"RES_MTT\";\n\tcase RES_MAC: return  \"RES_MAC\";\n\tcase RES_VLAN: return  \"RES_VLAN\";\n\tcase RES_EQ: return \"RES_EQ\";\n\tcase RES_COUNTER: return \"RES_COUNTER\";\n\tcase RES_FS_RULE: return \"RES_FS_RULE\";\n\tcase RES_XRCD: return \"RES_XRCD\";\n\tdefault: return \"Unknown resource type !!!\";\n\t}\n}\n\nstatic void rem_slave_vlans(struct mlx4_dev *dev, int slave);\nstatic inline int mlx4_grant_resource(struct mlx4_dev *dev, int slave,\n\t\t\t\t      enum mlx4_resource res_type, int count,\n\t\t\t\t      int port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct resource_allocator *res_alloc =\n\t\t&priv->mfunc.master.res_tracker.res_alloc[res_type];\n\tint err = -EDQUOT;\n\tint allocated, free, reserved, guaranteed, from_free;\n\tint from_rsvd;\n\n\tif (slave > dev->persist->num_vfs)\n\t\treturn -EINVAL;\n\n\tspin_lock(&res_alloc->alloc_lock);\n\tallocated = (port > 0) ?\n\t\tres_alloc->allocated[(port - 1) *\n\t\t(dev->persist->num_vfs + 1) + slave] :\n\t\tres_alloc->allocated[slave];\n\tfree = (port > 0) ? res_alloc->res_port_free[port - 1] :\n\t\tres_alloc->res_free;\n\treserved = (port > 0) ? res_alloc->res_port_rsvd[port - 1] :\n\t\tres_alloc->res_reserved;\n\tguaranteed = res_alloc->guaranteed[slave];\n\n\tif (allocated + count > res_alloc->quota[slave]) {\n\t\tmlx4_warn(dev, \"VF %d port %d res %s: quota exceeded, count %d alloc %d quota %d\\n\",\n\t\t\t  slave, port, resource_str(res_type), count,\n\t\t\t  allocated, res_alloc->quota[slave]);\n\t\tgoto out;\n\t}\n\n\tif (allocated + count <= guaranteed) {\n\t\terr = 0;\n\t\tfrom_rsvd = count;\n\t} else {\n\t\t \n\t\tif (guaranteed - allocated > 0)\n\t\t\tfrom_free = count - (guaranteed - allocated);\n\t\telse\n\t\t\tfrom_free = count;\n\n\t\tfrom_rsvd = count - from_free;\n\n\t\tif (free - from_free >= reserved)\n\t\t\terr = 0;\n\t\telse\n\t\t\tmlx4_warn(dev, \"VF %d port %d res %s: free pool empty, free %d from_free %d rsvd %d\\n\",\n\t\t\t\t  slave, port, resource_str(res_type), free,\n\t\t\t\t  from_free, reserved);\n\t}\n\n\tif (!err) {\n\t\t \n\t\tif (port > 0) {\n\t\t\tres_alloc->allocated[(port - 1) *\n\t\t\t(dev->persist->num_vfs + 1) + slave] += count;\n\t\t\tres_alloc->res_port_free[port - 1] -= count;\n\t\t\tres_alloc->res_port_rsvd[port - 1] -= from_rsvd;\n\t\t} else {\n\t\t\tres_alloc->allocated[slave] += count;\n\t\t\tres_alloc->res_free -= count;\n\t\t\tres_alloc->res_reserved -= from_rsvd;\n\t\t}\n\t}\n\nout:\n\tspin_unlock(&res_alloc->alloc_lock);\n\treturn err;\n}\n\nstatic inline void mlx4_release_resource(struct mlx4_dev *dev, int slave,\n\t\t\t\t    enum mlx4_resource res_type, int count,\n\t\t\t\t    int port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct resource_allocator *res_alloc =\n\t\t&priv->mfunc.master.res_tracker.res_alloc[res_type];\n\tint allocated, guaranteed, from_rsvd;\n\n\tif (slave > dev->persist->num_vfs)\n\t\treturn;\n\n\tspin_lock(&res_alloc->alloc_lock);\n\n\tallocated = (port > 0) ?\n\t\tres_alloc->allocated[(port - 1) *\n\t\t(dev->persist->num_vfs + 1) + slave] :\n\t\tres_alloc->allocated[slave];\n\tguaranteed = res_alloc->guaranteed[slave];\n\n\tif (allocated - count >= guaranteed) {\n\t\tfrom_rsvd = 0;\n\t} else {\n\t\t \n\t\tif (allocated - guaranteed > 0)\n\t\t\tfrom_rsvd = count - (allocated - guaranteed);\n\t\telse\n\t\t\tfrom_rsvd = count;\n\t}\n\n\tif (port > 0) {\n\t\tres_alloc->allocated[(port - 1) *\n\t\t(dev->persist->num_vfs + 1) + slave] -= count;\n\t\tres_alloc->res_port_free[port - 1] += count;\n\t\tres_alloc->res_port_rsvd[port - 1] += from_rsvd;\n\t} else {\n\t\tres_alloc->allocated[slave] -= count;\n\t\tres_alloc->res_free += count;\n\t\tres_alloc->res_reserved += from_rsvd;\n\t}\n\n\tspin_unlock(&res_alloc->alloc_lock);\n\treturn;\n}\n\nstatic inline void initialize_res_quotas(struct mlx4_dev *dev,\n\t\t\t\t\t struct resource_allocator *res_alloc,\n\t\t\t\t\t enum mlx4_resource res_type,\n\t\t\t\t\t int vf, int num_instances)\n{\n\tres_alloc->guaranteed[vf] = num_instances /\n\t\t\t\t    (2 * (dev->persist->num_vfs + 1));\n\tres_alloc->quota[vf] = (num_instances / 2) + res_alloc->guaranteed[vf];\n\tif (vf == mlx4_master_func_num(dev)) {\n\t\tres_alloc->res_free = num_instances;\n\t\tif (res_type == RES_MTT) {\n\t\t\t \n\t\t\tres_alloc->res_free += dev->caps.reserved_mtts;\n\t\t\tres_alloc->guaranteed[vf] += dev->caps.reserved_mtts;\n\t\t\tres_alloc->quota[vf] += dev->caps.reserved_mtts;\n\t\t}\n\t}\n}\n\nvoid mlx4_init_quotas(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint pf;\n\n\t \n\tif (mlx4_is_slave(dev))\n\t\treturn;\n\n\tif (!mlx4_is_mfunc(dev)) {\n\t\tdev->quotas.qp = dev->caps.num_qps - dev->caps.reserved_qps -\n\t\t\tmlx4_num_reserved_sqps(dev);\n\t\tdev->quotas.cq = dev->caps.num_cqs - dev->caps.reserved_cqs;\n\t\tdev->quotas.srq = dev->caps.num_srqs - dev->caps.reserved_srqs;\n\t\tdev->quotas.mtt = dev->caps.num_mtts - dev->caps.reserved_mtts;\n\t\tdev->quotas.mpt = dev->caps.num_mpts - dev->caps.reserved_mrws;\n\t\treturn;\n\t}\n\n\tpf = mlx4_master_func_num(dev);\n\tdev->quotas.qp =\n\t\tpriv->mfunc.master.res_tracker.res_alloc[RES_QP].quota[pf];\n\tdev->quotas.cq =\n\t\tpriv->mfunc.master.res_tracker.res_alloc[RES_CQ].quota[pf];\n\tdev->quotas.srq =\n\t\tpriv->mfunc.master.res_tracker.res_alloc[RES_SRQ].quota[pf];\n\tdev->quotas.mtt =\n\t\tpriv->mfunc.master.res_tracker.res_alloc[RES_MTT].quota[pf];\n\tdev->quotas.mpt =\n\t\tpriv->mfunc.master.res_tracker.res_alloc[RES_MPT].quota[pf];\n}\n\nstatic int\nmlx4_calc_res_counter_guaranteed(struct mlx4_dev *dev,\n\t\t\t\t struct resource_allocator *res_alloc,\n\t\t\t\t int vf)\n{\n\tstruct mlx4_active_ports actv_ports;\n\tint ports, counters_guaranteed;\n\n\t \n\tif (vf == mlx4_master_func_num(dev))\n\t\treturn MLX4_PF_COUNTERS_PER_PORT * dev->caps.num_ports;\n\n\t \n\tactv_ports = mlx4_get_active_ports(dev, vf);\n\tports = bitmap_weight(actv_ports.ports, dev->caps.num_ports);\n\tcounters_guaranteed = ports * MLX4_VF_COUNTERS_PER_PORT;\n\n\t \n\tif ((res_alloc->res_reserved + counters_guaranteed) >\n\t    (dev->caps.max_counters - 1))\n\t\treturn 0;\n\n\treturn counters_guaranteed;\n}\n\nint mlx4_init_resource_tracker(struct mlx4_dev *dev)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint i, j;\n\tint t;\n\n\tpriv->mfunc.master.res_tracker.slave_list =\n\t\tkcalloc(dev->num_slaves, sizeof(struct slave_list),\n\t\t\tGFP_KERNEL);\n\tif (!priv->mfunc.master.res_tracker.slave_list)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0 ; i < dev->num_slaves; i++) {\n\t\tfor (t = 0; t < MLX4_NUM_OF_RESOURCE_TYPE; ++t)\n\t\t\tINIT_LIST_HEAD(&priv->mfunc.master.res_tracker.\n\t\t\t\t       slave_list[i].res_list[t]);\n\t\tmutex_init(&priv->mfunc.master.res_tracker.slave_list[i].mutex);\n\t}\n\n\tmlx4_dbg(dev, \"Started init_resource_tracker: %ld slaves\\n\",\n\t\t dev->num_slaves);\n\tfor (i = 0 ; i < MLX4_NUM_OF_RESOURCE_TYPE; i++)\n\t\tpriv->mfunc.master.res_tracker.res_tree[i] = RB_ROOT;\n\n\tfor (i = 0; i < MLX4_NUM_OF_RESOURCE_TYPE; i++) {\n\t\tstruct resource_allocator *res_alloc =\n\t\t\t&priv->mfunc.master.res_tracker.res_alloc[i];\n\t\tres_alloc->quota = kmalloc_array(dev->persist->num_vfs + 1,\n\t\t\t\t\t\t sizeof(int),\n\t\t\t\t\t\t GFP_KERNEL);\n\t\tres_alloc->guaranteed = kmalloc_array(dev->persist->num_vfs + 1,\n\t\t\t\t\t\t      sizeof(int),\n\t\t\t\t\t\t      GFP_KERNEL);\n\t\tif (i == RES_MAC || i == RES_VLAN)\n\t\t\tres_alloc->allocated =\n\t\t\t\tkcalloc(MLX4_MAX_PORTS *\n\t\t\t\t\t\t(dev->persist->num_vfs + 1),\n\t\t\t\t\tsizeof(int), GFP_KERNEL);\n\t\telse\n\t\t\tres_alloc->allocated =\n\t\t\t\tkcalloc(dev->persist->num_vfs + 1,\n\t\t\t\t\tsizeof(int), GFP_KERNEL);\n\t\t \n\t\tif (i == RES_COUNTER)\n\t\t\tres_alloc->res_free = dev->caps.max_counters - 1;\n\n\t\tif (!res_alloc->quota || !res_alloc->guaranteed ||\n\t\t    !res_alloc->allocated)\n\t\t\tgoto no_mem_err;\n\n\t\tspin_lock_init(&res_alloc->alloc_lock);\n\t\tfor (t = 0; t < dev->persist->num_vfs + 1; t++) {\n\t\t\tstruct mlx4_active_ports actv_ports =\n\t\t\t\tmlx4_get_active_ports(dev, t);\n\t\t\tswitch (i) {\n\t\t\tcase RES_QP:\n\t\t\t\tinitialize_res_quotas(dev, res_alloc, RES_QP,\n\t\t\t\t\t\t      t, dev->caps.num_qps -\n\t\t\t\t\t\t      dev->caps.reserved_qps -\n\t\t\t\t\t\t      mlx4_num_reserved_sqps(dev));\n\t\t\t\tbreak;\n\t\t\tcase RES_CQ:\n\t\t\t\tinitialize_res_quotas(dev, res_alloc, RES_CQ,\n\t\t\t\t\t\t      t, dev->caps.num_cqs -\n\t\t\t\t\t\t      dev->caps.reserved_cqs);\n\t\t\t\tbreak;\n\t\t\tcase RES_SRQ:\n\t\t\t\tinitialize_res_quotas(dev, res_alloc, RES_SRQ,\n\t\t\t\t\t\t      t, dev->caps.num_srqs -\n\t\t\t\t\t\t      dev->caps.reserved_srqs);\n\t\t\t\tbreak;\n\t\t\tcase RES_MPT:\n\t\t\t\tinitialize_res_quotas(dev, res_alloc, RES_MPT,\n\t\t\t\t\t\t      t, dev->caps.num_mpts -\n\t\t\t\t\t\t      dev->caps.reserved_mrws);\n\t\t\t\tbreak;\n\t\t\tcase RES_MTT:\n\t\t\t\tinitialize_res_quotas(dev, res_alloc, RES_MTT,\n\t\t\t\t\t\t      t, dev->caps.num_mtts -\n\t\t\t\t\t\t      dev->caps.reserved_mtts);\n\t\t\t\tbreak;\n\t\t\tcase RES_MAC:\n\t\t\t\tif (t == mlx4_master_func_num(dev)) {\n\t\t\t\t\tint max_vfs_pport = 0;\n\t\t\t\t\t \n\t\t\t\t\t \n\t\t\t\t\tfor (j = 0; j < dev->caps.num_ports;\n\t\t\t\t\t     j++) {\n\t\t\t\t\t\tstruct mlx4_slaves_pport slaves_pport =\n\t\t\t\t\t\t\tmlx4_phys_to_slaves_pport(dev, j + 1);\n\t\t\t\t\t\tunsigned current_slaves =\n\t\t\t\t\t\t\tbitmap_weight(slaves_pport.slaves,\n\t\t\t\t\t\t\t\t      dev->caps.num_ports) - 1;\n\t\t\t\t\t\tif (max_vfs_pport < current_slaves)\n\t\t\t\t\t\t\tmax_vfs_pport =\n\t\t\t\t\t\t\t\tcurrent_slaves;\n\t\t\t\t\t}\n\t\t\t\t\tres_alloc->quota[t] =\n\t\t\t\t\t\tMLX4_MAX_MAC_NUM -\n\t\t\t\t\t\t2 * max_vfs_pport;\n\t\t\t\t\tres_alloc->guaranteed[t] = 2;\n\t\t\t\t\tfor (j = 0; j < MLX4_MAX_PORTS; j++)\n\t\t\t\t\t\tres_alloc->res_port_free[j] =\n\t\t\t\t\t\t\tMLX4_MAX_MAC_NUM;\n\t\t\t\t} else {\n\t\t\t\t\tres_alloc->quota[t] = MLX4_MAX_MAC_NUM;\n\t\t\t\t\tres_alloc->guaranteed[t] = 2;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase RES_VLAN:\n\t\t\t\tif (t == mlx4_master_func_num(dev)) {\n\t\t\t\t\tres_alloc->quota[t] = MLX4_MAX_VLAN_NUM;\n\t\t\t\t\tres_alloc->guaranteed[t] = MLX4_MAX_VLAN_NUM / 2;\n\t\t\t\t\tfor (j = 0; j < MLX4_MAX_PORTS; j++)\n\t\t\t\t\t\tres_alloc->res_port_free[j] =\n\t\t\t\t\t\t\tres_alloc->quota[t];\n\t\t\t\t} else {\n\t\t\t\t\tres_alloc->quota[t] = MLX4_MAX_VLAN_NUM / 2;\n\t\t\t\t\tres_alloc->guaranteed[t] = 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase RES_COUNTER:\n\t\t\t\tres_alloc->quota[t] = dev->caps.max_counters;\n\t\t\t\tres_alloc->guaranteed[t] =\n\t\t\t\t\tmlx4_calc_res_counter_guaranteed(dev, res_alloc, t);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (i == RES_MAC || i == RES_VLAN) {\n\t\t\t\tfor (j = 0; j < dev->caps.num_ports; j++)\n\t\t\t\t\tif (test_bit(j, actv_ports.ports))\n\t\t\t\t\t\tres_alloc->res_port_rsvd[j] +=\n\t\t\t\t\t\t\tres_alloc->guaranteed[t];\n\t\t\t} else {\n\t\t\t\tres_alloc->res_reserved += res_alloc->guaranteed[t];\n\t\t\t}\n\t\t}\n\t}\n\tspin_lock_init(&priv->mfunc.master.res_tracker.lock);\n\treturn 0;\n\nno_mem_err:\n\tfor (i = 0; i < MLX4_NUM_OF_RESOURCE_TYPE; i++) {\n\t\tkfree(priv->mfunc.master.res_tracker.res_alloc[i].allocated);\n\t\tpriv->mfunc.master.res_tracker.res_alloc[i].allocated = NULL;\n\t\tkfree(priv->mfunc.master.res_tracker.res_alloc[i].guaranteed);\n\t\tpriv->mfunc.master.res_tracker.res_alloc[i].guaranteed = NULL;\n\t\tkfree(priv->mfunc.master.res_tracker.res_alloc[i].quota);\n\t\tpriv->mfunc.master.res_tracker.res_alloc[i].quota = NULL;\n\t}\n\treturn -ENOMEM;\n}\n\nvoid mlx4_free_resource_tracker(struct mlx4_dev *dev,\n\t\t\t\tenum mlx4_res_tracker_free_type type)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint i;\n\n\tif (priv->mfunc.master.res_tracker.slave_list) {\n\t\tif (type != RES_TR_FREE_STRUCTS_ONLY) {\n\t\t\tfor (i = 0; i < dev->num_slaves; i++) {\n\t\t\t\tif (type == RES_TR_FREE_ALL ||\n\t\t\t\t    dev->caps.function != i)\n\t\t\t\t\tmlx4_delete_all_resources_for_slave(dev, i);\n\t\t\t}\n\t\t\t \n\t\t\ti = dev->caps.function;\n\t\t\tmlx4_reset_roce_gids(dev, i);\n\t\t\tmutex_lock(&priv->mfunc.master.res_tracker.slave_list[i].mutex);\n\t\t\trem_slave_vlans(dev, i);\n\t\t\tmutex_unlock(&priv->mfunc.master.res_tracker.slave_list[i].mutex);\n\t\t}\n\n\t\tif (type != RES_TR_FREE_SLAVES_ONLY) {\n\t\t\tfor (i = 0; i < MLX4_NUM_OF_RESOURCE_TYPE; i++) {\n\t\t\t\tkfree(priv->mfunc.master.res_tracker.res_alloc[i].allocated);\n\t\t\t\tpriv->mfunc.master.res_tracker.res_alloc[i].allocated = NULL;\n\t\t\t\tkfree(priv->mfunc.master.res_tracker.res_alloc[i].guaranteed);\n\t\t\t\tpriv->mfunc.master.res_tracker.res_alloc[i].guaranteed = NULL;\n\t\t\t\tkfree(priv->mfunc.master.res_tracker.res_alloc[i].quota);\n\t\t\t\tpriv->mfunc.master.res_tracker.res_alloc[i].quota = NULL;\n\t\t\t}\n\t\t\tkfree(priv->mfunc.master.res_tracker.slave_list);\n\t\t\tpriv->mfunc.master.res_tracker.slave_list = NULL;\n\t\t}\n\t}\n}\n\nstatic void update_pkey_index(struct mlx4_dev *dev, int slave,\n\t\t\t      struct mlx4_cmd_mailbox *inbox)\n{\n\tu8 sched = *(u8 *)(inbox->buf + 64);\n\tu8 orig_index = *(u8 *)(inbox->buf + 35);\n\tu8 new_index;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tint port;\n\n\tport = (sched >> 6 & 1) + 1;\n\n\tnew_index = priv->virt2phys_pkey[slave][port - 1][orig_index];\n\t*(u8 *)(inbox->buf + 35) = new_index;\n}\n\nstatic void update_gid(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *inbox,\n\t\t       u8 slave)\n{\n\tstruct mlx4_qp_context\t*qp_ctx = inbox->buf + 8;\n\tenum mlx4_qp_optpar\toptpar = be32_to_cpu(*(__be32 *) inbox->buf);\n\tu32\t\t\tts = (be32_to_cpu(qp_ctx->flags) >> 16) & 0xff;\n\tint port;\n\n\tif (MLX4_QP_ST_UD == ts) {\n\t\tport = (qp_ctx->pri_path.sched_queue >> 6 & 1) + 1;\n\t\tif (mlx4_is_eth(dev, port))\n\t\t\tqp_ctx->pri_path.mgid_index =\n\t\t\t\tmlx4_get_base_gid_ix(dev, slave, port) | 0x80;\n\t\telse\n\t\t\tqp_ctx->pri_path.mgid_index = slave | 0x80;\n\n\t} else if (MLX4_QP_ST_RC == ts || MLX4_QP_ST_XRC == ts || MLX4_QP_ST_UC == ts) {\n\t\tif (optpar & MLX4_QP_OPTPAR_PRIMARY_ADDR_PATH) {\n\t\t\tport = (qp_ctx->pri_path.sched_queue >> 6 & 1) + 1;\n\t\t\tif (mlx4_is_eth(dev, port)) {\n\t\t\t\tqp_ctx->pri_path.mgid_index +=\n\t\t\t\t\tmlx4_get_base_gid_ix(dev, slave, port);\n\t\t\t\tqp_ctx->pri_path.mgid_index &= 0x7f;\n\t\t\t} else {\n\t\t\t\tqp_ctx->pri_path.mgid_index = slave & 0x7F;\n\t\t\t}\n\t\t}\n\t\tif (optpar & MLX4_QP_OPTPAR_ALT_ADDR_PATH) {\n\t\t\tport = (qp_ctx->alt_path.sched_queue >> 6 & 1) + 1;\n\t\t\tif (mlx4_is_eth(dev, port)) {\n\t\t\t\tqp_ctx->alt_path.mgid_index +=\n\t\t\t\t\tmlx4_get_base_gid_ix(dev, slave, port);\n\t\t\t\tqp_ctx->alt_path.mgid_index &= 0x7f;\n\t\t\t} else {\n\t\t\t\tqp_ctx->alt_path.mgid_index = slave & 0x7F;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int handle_counter(struct mlx4_dev *dev, struct mlx4_qp_context *qpc,\n\t\t\t  u8 slave, int port);\n\nstatic int update_vport_qp_param(struct mlx4_dev *dev,\n\t\t\t\t struct mlx4_cmd_mailbox *inbox,\n\t\t\t\t u8 slave, u32 qpn)\n{\n\tstruct mlx4_qp_context\t*qpc = inbox->buf + 8;\n\tstruct mlx4_vport_oper_state *vp_oper;\n\tstruct mlx4_priv *priv;\n\tu32 qp_type;\n\tint port, err = 0;\n\n\tport = (qpc->pri_path.sched_queue & 0x40) ? 2 : 1;\n\tpriv = mlx4_priv(dev);\n\tvp_oper = &priv->mfunc.master.vf_oper[slave].vport[port];\n\tqp_type\t= (be32_to_cpu(qpc->flags) >> 16) & 0xff;\n\n\terr = handle_counter(dev, qpc, slave, port);\n\tif (err)\n\t\tgoto out;\n\n\tif (MLX4_VGT != vp_oper->state.default_vlan) {\n\t\t \n\t\tif (mlx4_is_qp_reserved(dev, qpn))\n\t\t\treturn 0;\n\n\t\t \n\t\tif (qp_type == MLX4_QP_ST_UD ||\n\t\t    (qp_type == MLX4_QP_ST_MLX && mlx4_is_eth(dev, port))) {\n\t\t\tif (dev->caps.bmme_flags & MLX4_BMME_FLAG_VSD_INIT2RTR) {\n\t\t\t\t*(__be32 *)inbox->buf =\n\t\t\t\t\tcpu_to_be32(be32_to_cpu(*(__be32 *)inbox->buf) |\n\t\t\t\t\tMLX4_QP_OPTPAR_VLAN_STRIPPING);\n\t\t\t\tqpc->param3 &= ~cpu_to_be32(MLX4_STRIP_VLAN);\n\t\t\t} else {\n\t\t\t\tstruct mlx4_update_qp_params params = {.flags = 0};\n\n\t\t\t\terr = mlx4_update_qp(dev, qpn, MLX4_UPDATE_QP_VSD, &params);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tqpc->pri_path.vlan_control &=\n\t\t\tMLX4_CTRL_ETH_SRC_CHECK_IF_COUNTER;\n\t\tif (vp_oper->state.link_state == IFLA_VF_LINK_STATE_DISABLE &&\n\t\t    dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_UPDATE_QP) {\n\t\t\tqpc->pri_path.vlan_control |=\n\t\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |\n\t\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_PRIO_TAGGED |\n\t\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_UNTAGGED |\n\t\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |\n\t\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED |\n\t\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_TAGGED;\n\t\t} else if (0 != vp_oper->state.default_vlan) {\n\t\t\tif (vp_oper->state.vlan_proto == htons(ETH_P_8021AD)) {\n\t\t\t\t \n\t\t\t\tqpc->pri_path.vlan_control |=\n\t\t\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_PRIO_TAGGED |\n\t\t\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |\n\t\t\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |\n\t\t\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED;\n\t\t\t} else {  \n\t\t\t\tqpc->pri_path.vlan_control |=\n\t\t\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |\n\t\t\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |\n\t\t\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED;\n\t\t\t}\n\t\t} else {  \n\t\t\tqpc->pri_path.vlan_control |=\n\t\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |\n\t\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_TAGGED;\n\t\t}\n\n\t\tqpc->pri_path.fvl_rx |= MLX4_FVL_RX_FORCE_ETH_VLAN;\n\t\tqpc->pri_path.vlan_index = vp_oper->vlan_idx;\n\t\tqpc->pri_path.fl |= MLX4_FL_ETH_HIDE_CQE_VLAN;\n\t\tif (vp_oper->state.vlan_proto == htons(ETH_P_8021AD))\n\t\t\tqpc->pri_path.fl |= MLX4_FL_SV;\n\t\telse\n\t\t\tqpc->pri_path.fl |= MLX4_FL_CV;\n\t\tqpc->pri_path.feup |= MLX4_FEUP_FORCE_ETH_UP | MLX4_FVL_FORCE_ETH_VLAN;\n\t\tqpc->pri_path.sched_queue &= 0xC7;\n\t\tqpc->pri_path.sched_queue |= (vp_oper->state.default_qos) << 3;\n\t\tqpc->qos_vport = vp_oper->state.qos_vport;\n\t}\n\tif (vp_oper->state.spoofchk) {\n\t\tqpc->pri_path.feup |= MLX4_FSM_FORCE_ETH_SRC_MAC;\n\t\tqpc->pri_path.grh_mylmc = (0x80 & qpc->pri_path.grh_mylmc) + vp_oper->mac_idx;\n\t}\nout:\n\treturn err;\n}\n\nstatic int mpt_mask(struct mlx4_dev *dev)\n{\n\treturn dev->caps.num_mpts - 1;\n}\n\nstatic const char *mlx4_resource_type_to_str(enum mlx4_resource t)\n{\n\tswitch (t) {\n\tcase RES_QP:\n\t\treturn \"QP\";\n\tcase RES_CQ:\n\t\treturn \"CQ\";\n\tcase RES_SRQ:\n\t\treturn \"SRQ\";\n\tcase RES_XRCD:\n\t\treturn \"XRCD\";\n\tcase RES_MPT:\n\t\treturn \"MPT\";\n\tcase RES_MTT:\n\t\treturn \"MTT\";\n\tcase RES_MAC:\n\t\treturn \"MAC\";\n\tcase RES_VLAN:\n\t\treturn \"VLAN\";\n\tcase RES_COUNTER:\n\t\treturn \"COUNTER\";\n\tcase RES_FS_RULE:\n\t\treturn \"FS_RULE\";\n\tcase RES_EQ:\n\t\treturn \"EQ\";\n\tdefault:\n\t\treturn \"INVALID RESOURCE\";\n\t}\n}\n\nstatic void *find_res(struct mlx4_dev *dev, u64 res_id,\n\t\t      enum mlx4_resource type)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\n\treturn res_tracker_lookup(&priv->mfunc.master.res_tracker.res_tree[type],\n\t\t\t\t  res_id);\n}\n\nstatic int _get_res(struct mlx4_dev *dev, int slave, u64 res_id,\n\t\t    enum mlx4_resource type,\n\t\t    void *res, const char *func_name)\n{\n\tstruct res_common *r;\n\tint err = 0;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = find_res(dev, res_id, type);\n\tif (!r) {\n\t\terr = -ENONET;\n\t\tgoto exit;\n\t}\n\n\tif (r->state == RES_ANY_BUSY) {\n\t\tmlx4_warn(dev,\n\t\t\t  \"%s(%d) trying to get resource %llx of type %s, but it's already taken by %s\\n\",\n\t\t\t  func_name, slave, res_id, mlx4_resource_type_to_str(type),\n\t\t\t  r->func_name);\n\t\terr = -EBUSY;\n\t\tgoto exit;\n\t}\n\n\tif (r->owner != slave) {\n\t\terr = -EPERM;\n\t\tgoto exit;\n\t}\n\n\tr->from_state = r->state;\n\tr->state = RES_ANY_BUSY;\n\tr->func_name = func_name;\n\n\tif (res)\n\t\t*((struct res_common **)res) = r;\n\nexit:\n\tspin_unlock_irq(mlx4_tlock(dev));\n\treturn err;\n}\n\n#define get_res(dev, slave, res_id, type, res) \\\n\t_get_res((dev), (slave), (res_id), (type), (res), __func__)\n\nint mlx4_get_slave_from_resource_id(struct mlx4_dev *dev,\n\t\t\t\t    enum mlx4_resource type,\n\t\t\t\t    u64 res_id, int *slave)\n{\n\n\tstruct res_common *r;\n\tint err = -ENOENT;\n\tint id = res_id;\n\n\tif (type == RES_QP)\n\t\tid &= 0x7fffff;\n\tspin_lock(mlx4_tlock(dev));\n\n\tr = find_res(dev, id, type);\n\tif (r) {\n\t\t*slave = r->owner;\n\t\terr = 0;\n\t}\n\tspin_unlock(mlx4_tlock(dev));\n\n\treturn err;\n}\n\nstatic void put_res(struct mlx4_dev *dev, int slave, u64 res_id,\n\t\t    enum mlx4_resource type)\n{\n\tstruct res_common *r;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = find_res(dev, res_id, type);\n\tif (r) {\n\t\tr->state = r->from_state;\n\t\tr->func_name = \"\";\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic int counter_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t     u64 in_param, u64 *out_param, int port);\n\nstatic int handle_existing_counter(struct mlx4_dev *dev, u8 slave, int port,\n\t\t\t\t   int counter_index)\n{\n\tstruct res_common *r;\n\tstruct res_counter *counter;\n\tint ret = 0;\n\n\tif (counter_index == MLX4_SINK_COUNTER_INDEX(dev))\n\t\treturn ret;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = find_res(dev, counter_index, RES_COUNTER);\n\tif (!r || r->owner != slave) {\n\t\tret = -EINVAL;\n\t} else {\n\t\tcounter = container_of(r, struct res_counter, com);\n\t\tif (!counter->port)\n\t\t\tcounter->port = port;\n\t}\n\n\tspin_unlock_irq(mlx4_tlock(dev));\n\treturn ret;\n}\n\nstatic int handle_unexisting_counter(struct mlx4_dev *dev,\n\t\t\t\t     struct mlx4_qp_context *qpc, u8 slave,\n\t\t\t\t     int port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_common *tmp;\n\tstruct res_counter *counter;\n\tu64 counter_idx = MLX4_SINK_COUNTER_INDEX(dev);\n\tint err = 0;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry(tmp,\n\t\t\t    &tracker->slave_list[slave].res_list[RES_COUNTER],\n\t\t\t    list) {\n\t\tcounter = container_of(tmp, struct res_counter, com);\n\t\tif (port == counter->port) {\n\t\t\tqpc->pri_path.counter_index  = counter->com.res_id;\n\t\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\t\treturn 0;\n\t\t}\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\t \n\terr = counter_alloc_res(dev, slave, RES_OP_RESERVE, 0, 0, &counter_idx,\n\t\t\t\tport);\n\tif (err == -ENOENT) {\n\t\terr = 0;\n\t} else if (err && err != -ENOSPC) {\n\t\tmlx4_err(dev, \"%s: failed to create new counter for slave %d err %d\\n\",\n\t\t\t __func__, slave, err);\n\t} else {\n\t\tqpc->pri_path.counter_index = counter_idx;\n\t\tmlx4_dbg(dev, \"%s: alloc new counter for slave %d index %d\\n\",\n\t\t\t __func__, slave, qpc->pri_path.counter_index);\n\t\terr = 0;\n\t}\n\n\treturn err;\n}\n\nstatic int handle_counter(struct mlx4_dev *dev, struct mlx4_qp_context *qpc,\n\t\t\t  u8 slave, int port)\n{\n\tif (qpc->pri_path.counter_index != MLX4_SINK_COUNTER_INDEX(dev))\n\t\treturn handle_existing_counter(dev, slave, port,\n\t\t\t\t\t       qpc->pri_path.counter_index);\n\n\treturn handle_unexisting_counter(dev, qpc, slave, port);\n}\n\nstatic struct res_common *alloc_qp_tr(int id)\n{\n\tstruct res_qp *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tret->com.res_id = id;\n\tret->com.state = RES_QP_RESERVED;\n\tret->local_qpn = id;\n\tINIT_LIST_HEAD(&ret->mcg_list);\n\tspin_lock_init(&ret->mcg_spl);\n\tatomic_set(&ret->ref_count, 0);\n\n\treturn &ret->com;\n}\n\nstatic struct res_common *alloc_mtt_tr(int id, int order)\n{\n\tstruct res_mtt *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tret->com.res_id = id;\n\tret->order = order;\n\tret->com.state = RES_MTT_ALLOCATED;\n\tatomic_set(&ret->ref_count, 0);\n\n\treturn &ret->com;\n}\n\nstatic struct res_common *alloc_mpt_tr(int id, int key)\n{\n\tstruct res_mpt *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tret->com.res_id = id;\n\tret->com.state = RES_MPT_RESERVED;\n\tret->key = key;\n\n\treturn &ret->com;\n}\n\nstatic struct res_common *alloc_eq_tr(int id)\n{\n\tstruct res_eq *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tret->com.res_id = id;\n\tret->com.state = RES_EQ_RESERVED;\n\n\treturn &ret->com;\n}\n\nstatic struct res_common *alloc_cq_tr(int id)\n{\n\tstruct res_cq *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tret->com.res_id = id;\n\tret->com.state = RES_CQ_ALLOCATED;\n\tatomic_set(&ret->ref_count, 0);\n\n\treturn &ret->com;\n}\n\nstatic struct res_common *alloc_srq_tr(int id)\n{\n\tstruct res_srq *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tret->com.res_id = id;\n\tret->com.state = RES_SRQ_ALLOCATED;\n\tatomic_set(&ret->ref_count, 0);\n\n\treturn &ret->com;\n}\n\nstatic struct res_common *alloc_counter_tr(int id, int port)\n{\n\tstruct res_counter *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tret->com.res_id = id;\n\tret->com.state = RES_COUNTER_ALLOCATED;\n\tret->port = port;\n\n\treturn &ret->com;\n}\n\nstatic struct res_common *alloc_xrcdn_tr(int id)\n{\n\tstruct res_xrcdn *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tret->com.res_id = id;\n\tret->com.state = RES_XRCD_ALLOCATED;\n\n\treturn &ret->com;\n}\n\nstatic struct res_common *alloc_fs_rule_tr(u64 id, int qpn)\n{\n\tstruct res_fs_rule *ret;\n\n\tret = kzalloc(sizeof(*ret), GFP_KERNEL);\n\tif (!ret)\n\t\treturn NULL;\n\n\tret->com.res_id = id;\n\tret->com.state = RES_FS_RULE_ALLOCATED;\n\tret->qpn = qpn;\n\treturn &ret->com;\n}\n\nstatic struct res_common *alloc_tr(u64 id, enum mlx4_resource type, int slave,\n\t\t\t\t   int extra)\n{\n\tstruct res_common *ret;\n\n\tswitch (type) {\n\tcase RES_QP:\n\t\tret = alloc_qp_tr(id);\n\t\tbreak;\n\tcase RES_MPT:\n\t\tret = alloc_mpt_tr(id, extra);\n\t\tbreak;\n\tcase RES_MTT:\n\t\tret = alloc_mtt_tr(id, extra);\n\t\tbreak;\n\tcase RES_EQ:\n\t\tret = alloc_eq_tr(id);\n\t\tbreak;\n\tcase RES_CQ:\n\t\tret = alloc_cq_tr(id);\n\t\tbreak;\n\tcase RES_SRQ:\n\t\tret = alloc_srq_tr(id);\n\t\tbreak;\n\tcase RES_MAC:\n\t\tpr_err(\"implementation missing\\n\");\n\t\treturn NULL;\n\tcase RES_COUNTER:\n\t\tret = alloc_counter_tr(id, extra);\n\t\tbreak;\n\tcase RES_XRCD:\n\t\tret = alloc_xrcdn_tr(id);\n\t\tbreak;\n\tcase RES_FS_RULE:\n\t\tret = alloc_fs_rule_tr(id, extra);\n\t\tbreak;\n\tdefault:\n\t\treturn NULL;\n\t}\n\tif (ret)\n\t\tret->owner = slave;\n\n\treturn ret;\n}\n\nint mlx4_calc_vf_counters(struct mlx4_dev *dev, int slave, int port,\n\t\t\t  struct mlx4_counter *data)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_common *tmp;\n\tstruct res_counter *counter;\n\tint *counters_arr;\n\tint i = 0, err = 0;\n\n\tmemset(data, 0, sizeof(*data));\n\n\tcounters_arr = kmalloc_array(dev->caps.max_counters,\n\t\t\t\t     sizeof(*counters_arr), GFP_KERNEL);\n\tif (!counters_arr)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry(tmp,\n\t\t\t    &tracker->slave_list[slave].res_list[RES_COUNTER],\n\t\t\t    list) {\n\t\tcounter = container_of(tmp, struct res_counter, com);\n\t\tif (counter->port == port) {\n\t\t\tcounters_arr[i] = (int)tmp->res_id;\n\t\t\ti++;\n\t\t}\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n\tcounters_arr[i] = -1;\n\n\ti = 0;\n\n\twhile (counters_arr[i] != -1) {\n\t\terr = mlx4_get_counter_stats(dev, counters_arr[i], data,\n\t\t\t\t\t     0);\n\t\tif (err) {\n\t\t\tmemset(data, 0, sizeof(*data));\n\t\t\tgoto table_changed;\n\t\t}\n\t\ti++;\n\t}\n\ntable_changed:\n\tkfree(counters_arr);\n\treturn 0;\n}\n\nstatic int add_res_range(struct mlx4_dev *dev, int slave, u64 base, int count,\n\t\t\t enum mlx4_resource type, int extra)\n{\n\tint i;\n\tint err;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct res_common **res_arr;\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct rb_root *root = &tracker->res_tree[type];\n\n\tres_arr = kcalloc(count, sizeof(*res_arr), GFP_KERNEL);\n\tif (!res_arr)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < count; ++i) {\n\t\tres_arr[i] = alloc_tr(base + i, type, slave, extra);\n\t\tif (!res_arr[i]) {\n\t\t\tfor (--i; i >= 0; --i)\n\t\t\t\tkfree(res_arr[i]);\n\n\t\t\tkfree(res_arr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tfor (i = 0; i < count; ++i) {\n\t\tif (find_res(dev, base + i, type)) {\n\t\t\terr = -EEXIST;\n\t\t\tgoto undo;\n\t\t}\n\t\terr = res_tracker_insert(root, res_arr[i]);\n\t\tif (err)\n\t\t\tgoto undo;\n\t\tlist_add_tail(&res_arr[i]->list,\n\t\t\t      &tracker->slave_list[slave].res_list[type]);\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n\tkfree(res_arr);\n\n\treturn 0;\n\nundo:\n\tfor (--i; i >= 0; --i) {\n\t\trb_erase(&res_arr[i]->node, root);\n\t\tlist_del_init(&res_arr[i]->list);\n\t}\n\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\tfor (i = 0; i < count; ++i)\n\t\tkfree(res_arr[i]);\n\n\tkfree(res_arr);\n\n\treturn err;\n}\n\nstatic int remove_qp_ok(struct res_qp *res)\n{\n\tif (res->com.state == RES_QP_BUSY || atomic_read(&res->ref_count) ||\n\t    !list_empty(&res->mcg_list)) {\n\t\tpr_err(\"resource tracker: fail to remove qp, state %d, ref_count %d\\n\",\n\t\t       res->com.state, atomic_read(&res->ref_count));\n\t\treturn -EBUSY;\n\t} else if (res->com.state != RES_QP_RESERVED) {\n\t\treturn -EPERM;\n\t}\n\n\treturn 0;\n}\n\nstatic int remove_mtt_ok(struct res_mtt *res, int order)\n{\n\tif (res->com.state == RES_MTT_BUSY ||\n\t    atomic_read(&res->ref_count)) {\n\t\tpr_devel(\"%s-%d: state %s, ref_count %d\\n\",\n\t\t\t __func__, __LINE__,\n\t\t\t mtt_states_str(res->com.state),\n\t\t\t atomic_read(&res->ref_count));\n\t\treturn -EBUSY;\n\t} else if (res->com.state != RES_MTT_ALLOCATED)\n\t\treturn -EPERM;\n\telse if (res->order != order)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int remove_mpt_ok(struct res_mpt *res)\n{\n\tif (res->com.state == RES_MPT_BUSY)\n\t\treturn -EBUSY;\n\telse if (res->com.state != RES_MPT_RESERVED)\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int remove_eq_ok(struct res_eq *res)\n{\n\tif (res->com.state == RES_MPT_BUSY)\n\t\treturn -EBUSY;\n\telse if (res->com.state != RES_MPT_RESERVED)\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int remove_counter_ok(struct res_counter *res)\n{\n\tif (res->com.state == RES_COUNTER_BUSY)\n\t\treturn -EBUSY;\n\telse if (res->com.state != RES_COUNTER_ALLOCATED)\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int remove_xrcdn_ok(struct res_xrcdn *res)\n{\n\tif (res->com.state == RES_XRCD_BUSY)\n\t\treturn -EBUSY;\n\telse if (res->com.state != RES_XRCD_ALLOCATED)\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int remove_fs_rule_ok(struct res_fs_rule *res)\n{\n\tif (res->com.state == RES_FS_RULE_BUSY)\n\t\treturn -EBUSY;\n\telse if (res->com.state != RES_FS_RULE_ALLOCATED)\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int remove_cq_ok(struct res_cq *res)\n{\n\tif (res->com.state == RES_CQ_BUSY)\n\t\treturn -EBUSY;\n\telse if (res->com.state != RES_CQ_ALLOCATED)\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int remove_srq_ok(struct res_srq *res)\n{\n\tif (res->com.state == RES_SRQ_BUSY)\n\t\treturn -EBUSY;\n\telse if (res->com.state != RES_SRQ_ALLOCATED)\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nstatic int remove_ok(struct res_common *res, enum mlx4_resource type, int extra)\n{\n\tswitch (type) {\n\tcase RES_QP:\n\t\treturn remove_qp_ok((struct res_qp *)res);\n\tcase RES_CQ:\n\t\treturn remove_cq_ok((struct res_cq *)res);\n\tcase RES_SRQ:\n\t\treturn remove_srq_ok((struct res_srq *)res);\n\tcase RES_MPT:\n\t\treturn remove_mpt_ok((struct res_mpt *)res);\n\tcase RES_MTT:\n\t\treturn remove_mtt_ok((struct res_mtt *)res, extra);\n\tcase RES_MAC:\n\t\treturn -EOPNOTSUPP;\n\tcase RES_EQ:\n\t\treturn remove_eq_ok((struct res_eq *)res);\n\tcase RES_COUNTER:\n\t\treturn remove_counter_ok((struct res_counter *)res);\n\tcase RES_XRCD:\n\t\treturn remove_xrcdn_ok((struct res_xrcdn *)res);\n\tcase RES_FS_RULE:\n\t\treturn remove_fs_rule_ok((struct res_fs_rule *)res);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int rem_res_range(struct mlx4_dev *dev, int slave, u64 base, int count,\n\t\t\t enum mlx4_resource type, int extra)\n{\n\tu64 i;\n\tint err;\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_common *r;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tfor (i = base; i < base + count; ++i) {\n\t\tr = res_tracker_lookup(&tracker->res_tree[type], i);\n\t\tif (!r) {\n\t\t\terr = -ENOENT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (r->owner != slave) {\n\t\t\terr = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t\terr = remove_ok(r, type, extra);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (i = base; i < base + count; ++i) {\n\t\tr = res_tracker_lookup(&tracker->res_tree[type], i);\n\t\trb_erase(&r->node, &tracker->res_tree[type]);\n\t\tlist_del(&r->list);\n\t\tkfree(r);\n\t}\n\terr = 0;\n\nout:\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\treturn err;\n}\n\nstatic int qp_res_start_move_to(struct mlx4_dev *dev, int slave, int qpn,\n\t\t\t\tenum res_qp_states state, struct res_qp **qp,\n\t\t\t\tint alloc)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_qp *r;\n\tint err = 0;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = res_tracker_lookup(&tracker->res_tree[RES_QP], qpn);\n\tif (!r)\n\t\terr = -ENOENT;\n\telse if (r->com.owner != slave)\n\t\terr = -EPERM;\n\telse {\n\t\tswitch (state) {\n\t\tcase RES_QP_BUSY:\n\t\t\tmlx4_dbg(dev, \"%s: failed RES_QP, 0x%llx\\n\",\n\t\t\t\t __func__, r->com.res_id);\n\t\t\terr = -EBUSY;\n\t\t\tbreak;\n\n\t\tcase RES_QP_RESERVED:\n\t\t\tif (r->com.state == RES_QP_MAPPED && !alloc)\n\t\t\t\tbreak;\n\n\t\t\tmlx4_dbg(dev, \"failed RES_QP, 0x%llx\\n\", r->com.res_id);\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\n\t\tcase RES_QP_MAPPED:\n\t\t\tif ((r->com.state == RES_QP_RESERVED && alloc) ||\n\t\t\t    r->com.state == RES_QP_HW)\n\t\t\t\tbreak;\n\t\t\telse {\n\t\t\t\tmlx4_dbg(dev, \"failed RES_QP, 0x%llx\\n\",\n\t\t\t\t\t  r->com.res_id);\n\t\t\t\terr = -EINVAL;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase RES_QP_HW:\n\t\t\tif (r->com.state != RES_QP_MAPPED)\n\t\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t}\n\n\t\tif (!err) {\n\t\t\tr->com.from_state = r->com.state;\n\t\t\tr->com.to_state = state;\n\t\t\tr->com.state = RES_QP_BUSY;\n\t\t\tif (qp)\n\t\t\t\t*qp = r;\n\t\t}\n\t}\n\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\treturn err;\n}\n\nstatic int mr_res_start_move_to(struct mlx4_dev *dev, int slave, int index,\n\t\t\t\tenum res_mpt_states state, struct res_mpt **mpt)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_mpt *r;\n\tint err = 0;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = res_tracker_lookup(&tracker->res_tree[RES_MPT], index);\n\tif (!r)\n\t\terr = -ENOENT;\n\telse if (r->com.owner != slave)\n\t\terr = -EPERM;\n\telse {\n\t\tswitch (state) {\n\t\tcase RES_MPT_BUSY:\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\n\t\tcase RES_MPT_RESERVED:\n\t\t\tif (r->com.state != RES_MPT_MAPPED)\n\t\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\n\t\tcase RES_MPT_MAPPED:\n\t\t\tif (r->com.state != RES_MPT_RESERVED &&\n\t\t\t    r->com.state != RES_MPT_HW)\n\t\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\n\t\tcase RES_MPT_HW:\n\t\t\tif (r->com.state != RES_MPT_MAPPED)\n\t\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t}\n\n\t\tif (!err) {\n\t\t\tr->com.from_state = r->com.state;\n\t\t\tr->com.to_state = state;\n\t\t\tr->com.state = RES_MPT_BUSY;\n\t\t\tif (mpt)\n\t\t\t\t*mpt = r;\n\t\t}\n\t}\n\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\treturn err;\n}\n\nstatic int eq_res_start_move_to(struct mlx4_dev *dev, int slave, int index,\n\t\t\t\tenum res_eq_states state, struct res_eq **eq)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_eq *r;\n\tint err = 0;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = res_tracker_lookup(&tracker->res_tree[RES_EQ], index);\n\tif (!r)\n\t\terr = -ENOENT;\n\telse if (r->com.owner != slave)\n\t\terr = -EPERM;\n\telse {\n\t\tswitch (state) {\n\t\tcase RES_EQ_BUSY:\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\n\t\tcase RES_EQ_RESERVED:\n\t\t\tif (r->com.state != RES_EQ_HW)\n\t\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\n\t\tcase RES_EQ_HW:\n\t\t\tif (r->com.state != RES_EQ_RESERVED)\n\t\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t}\n\n\t\tif (!err) {\n\t\t\tr->com.from_state = r->com.state;\n\t\t\tr->com.to_state = state;\n\t\t\tr->com.state = RES_EQ_BUSY;\n\t\t}\n\t}\n\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\tif (!err && eq)\n\t\t*eq = r;\n\n\treturn err;\n}\n\nstatic int cq_res_start_move_to(struct mlx4_dev *dev, int slave, int cqn,\n\t\t\t\tenum res_cq_states state, struct res_cq **cq)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_cq *r;\n\tint err;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = res_tracker_lookup(&tracker->res_tree[RES_CQ], cqn);\n\tif (!r) {\n\t\terr = -ENOENT;\n\t} else if (r->com.owner != slave) {\n\t\terr = -EPERM;\n\t} else if (state == RES_CQ_ALLOCATED) {\n\t\tif (r->com.state != RES_CQ_HW)\n\t\t\terr = -EINVAL;\n\t\telse if (atomic_read(&r->ref_count))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\terr = 0;\n\t} else if (state != RES_CQ_HW || r->com.state != RES_CQ_ALLOCATED) {\n\t\terr = -EINVAL;\n\t} else {\n\t\terr = 0;\n\t}\n\n\tif (!err) {\n\t\tr->com.from_state = r->com.state;\n\t\tr->com.to_state = state;\n\t\tr->com.state = RES_CQ_BUSY;\n\t\tif (cq)\n\t\t\t*cq = r;\n\t}\n\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\treturn err;\n}\n\nstatic int srq_res_start_move_to(struct mlx4_dev *dev, int slave, int index,\n\t\t\t\t enum res_srq_states state, struct res_srq **srq)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_srq *r;\n\tint err = 0;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = res_tracker_lookup(&tracker->res_tree[RES_SRQ], index);\n\tif (!r) {\n\t\terr = -ENOENT;\n\t} else if (r->com.owner != slave) {\n\t\terr = -EPERM;\n\t} else if (state == RES_SRQ_ALLOCATED) {\n\t\tif (r->com.state != RES_SRQ_HW)\n\t\t\terr = -EINVAL;\n\t\telse if (atomic_read(&r->ref_count))\n\t\t\terr = -EBUSY;\n\t} else if (state != RES_SRQ_HW || r->com.state != RES_SRQ_ALLOCATED) {\n\t\terr = -EINVAL;\n\t}\n\n\tif (!err) {\n\t\tr->com.from_state = r->com.state;\n\t\tr->com.to_state = state;\n\t\tr->com.state = RES_SRQ_BUSY;\n\t\tif (srq)\n\t\t\t*srq = r;\n\t}\n\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\treturn err;\n}\n\nstatic void res_abort_move(struct mlx4_dev *dev, int slave,\n\t\t\t   enum mlx4_resource type, int id)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_common *r;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = res_tracker_lookup(&tracker->res_tree[type], id);\n\tif (r && (r->owner == slave))\n\t\tr->state = r->from_state;\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic void res_end_move(struct mlx4_dev *dev, int slave,\n\t\t\t enum mlx4_resource type, int id)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_common *r;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tr = res_tracker_lookup(&tracker->res_tree[type], id);\n\tif (r && (r->owner == slave))\n\t\tr->state = r->to_state;\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic int valid_reserved(struct mlx4_dev *dev, int slave, int qpn)\n{\n\treturn mlx4_is_qp_reserved(dev, qpn) &&\n\t\t(mlx4_is_master(dev) || mlx4_is_guest_proxy(dev, slave, qpn));\n}\n\nstatic int fw_reserved(struct mlx4_dev *dev, int qpn)\n{\n\treturn qpn < dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW];\n}\n\nstatic int qp_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\tu64 in_param, u64 *out_param)\n{\n\tint err;\n\tint count;\n\tint align;\n\tint base;\n\tint qpn;\n\tu8 flags;\n\n\tswitch (op) {\n\tcase RES_OP_RESERVE:\n\t\tcount = get_param_l(&in_param) & 0xffffff;\n\t\t \n\t\tflags = (get_param_l(&in_param) >> 24) & dev->caps.alloc_res_qp_mask;\n\t\talign = get_param_h(&in_param);\n\t\terr = mlx4_grant_resource(dev, slave, RES_QP, count, 0);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = __mlx4_qp_reserve_range(dev, count, align, &base, flags);\n\t\tif (err) {\n\t\t\tmlx4_release_resource(dev, slave, RES_QP, count, 0);\n\t\t\treturn err;\n\t\t}\n\n\t\terr = add_res_range(dev, slave, base, count, RES_QP, 0);\n\t\tif (err) {\n\t\t\tmlx4_release_resource(dev, slave, RES_QP, count, 0);\n\t\t\t__mlx4_qp_release_range(dev, base, count);\n\t\t\treturn err;\n\t\t}\n\t\tset_param_l(out_param, base);\n\t\tbreak;\n\tcase RES_OP_MAP_ICM:\n\t\tqpn = get_param_l(&in_param) & 0x7fffff;\n\t\tif (valid_reserved(dev, slave, qpn)) {\n\t\t\terr = add_res_range(dev, slave, qpn, 1, RES_QP, 0);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\terr = qp_res_start_move_to(dev, slave, qpn, RES_QP_MAPPED,\n\t\t\t\t\t   NULL, 1);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!fw_reserved(dev, qpn)) {\n\t\t\terr = __mlx4_qp_alloc_icm(dev, qpn);\n\t\t\tif (err) {\n\t\t\t\tres_abort_move(dev, slave, RES_QP, qpn);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\tres_end_move(dev, slave, RES_QP, qpn);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn err;\n}\n\nstatic int mtt_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t u64 in_param, u64 *out_param)\n{\n\tint err = -EINVAL;\n\tint base;\n\tint order;\n\n\tif (op != RES_OP_RESERVE_AND_MAP)\n\t\treturn err;\n\n\torder = get_param_l(&in_param);\n\n\terr = mlx4_grant_resource(dev, slave, RES_MTT, 1 << order, 0);\n\tif (err)\n\t\treturn err;\n\n\tbase = __mlx4_alloc_mtt_range(dev, order);\n\tif (base == -1) {\n\t\tmlx4_release_resource(dev, slave, RES_MTT, 1 << order, 0);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = add_res_range(dev, slave, base, 1, RES_MTT, order);\n\tif (err) {\n\t\tmlx4_release_resource(dev, slave, RES_MTT, 1 << order, 0);\n\t\t__mlx4_free_mtt_range(dev, base, order);\n\t} else {\n\t\tset_param_l(out_param, base);\n\t}\n\n\treturn err;\n}\n\nstatic int mpt_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t u64 in_param, u64 *out_param)\n{\n\tint err = -EINVAL;\n\tint index;\n\tint id;\n\tstruct res_mpt *mpt;\n\n\tswitch (op) {\n\tcase RES_OP_RESERVE:\n\t\terr = mlx4_grant_resource(dev, slave, RES_MPT, 1, 0);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tindex = __mlx4_mpt_reserve(dev);\n\t\tif (index == -1) {\n\t\t\tmlx4_release_resource(dev, slave, RES_MPT, 1, 0);\n\t\t\tbreak;\n\t\t}\n\t\tid = index & mpt_mask(dev);\n\n\t\terr = add_res_range(dev, slave, id, 1, RES_MPT, index);\n\t\tif (err) {\n\t\t\tmlx4_release_resource(dev, slave, RES_MPT, 1, 0);\n\t\t\t__mlx4_mpt_release(dev, index);\n\t\t\tbreak;\n\t\t}\n\t\tset_param_l(out_param, index);\n\t\tbreak;\n\tcase RES_OP_MAP_ICM:\n\t\tindex = get_param_l(&in_param);\n\t\tid = index & mpt_mask(dev);\n\t\terr = mr_res_start_move_to(dev, slave, id,\n\t\t\t\t\t   RES_MPT_MAPPED, &mpt);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = __mlx4_mpt_alloc_icm(dev, mpt->key);\n\t\tif (err) {\n\t\t\tres_abort_move(dev, slave, RES_MPT, id);\n\t\t\treturn err;\n\t\t}\n\n\t\tres_end_move(dev, slave, RES_MPT, id);\n\t\tbreak;\n\t}\n\treturn err;\n}\n\nstatic int cq_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\tu64 in_param, u64 *out_param)\n{\n\tint cqn;\n\tint err;\n\n\tswitch (op) {\n\tcase RES_OP_RESERVE_AND_MAP:\n\t\terr = mlx4_grant_resource(dev, slave, RES_CQ, 1, 0);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\terr = __mlx4_cq_alloc_icm(dev, &cqn);\n\t\tif (err) {\n\t\t\tmlx4_release_resource(dev, slave, RES_CQ, 1, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\terr = add_res_range(dev, slave, cqn, 1, RES_CQ, 0);\n\t\tif (err) {\n\t\t\tmlx4_release_resource(dev, slave, RES_CQ, 1, 0);\n\t\t\t__mlx4_cq_free_icm(dev, cqn);\n\t\t\tbreak;\n\t\t}\n\n\t\tset_param_l(out_param, cqn);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}\n\nstatic int srq_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t u64 in_param, u64 *out_param)\n{\n\tint srqn;\n\tint err;\n\n\tswitch (op) {\n\tcase RES_OP_RESERVE_AND_MAP:\n\t\terr = mlx4_grant_resource(dev, slave, RES_SRQ, 1, 0);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\terr = __mlx4_srq_alloc_icm(dev, &srqn);\n\t\tif (err) {\n\t\t\tmlx4_release_resource(dev, slave, RES_SRQ, 1, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\terr = add_res_range(dev, slave, srqn, 1, RES_SRQ, 0);\n\t\tif (err) {\n\t\t\tmlx4_release_resource(dev, slave, RES_SRQ, 1, 0);\n\t\t\t__mlx4_srq_free_icm(dev, srqn);\n\t\t\tbreak;\n\t\t}\n\n\t\tset_param_l(out_param, srqn);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t}\n\n\treturn err;\n}\n\nstatic int mac_find_smac_ix_in_slave(struct mlx4_dev *dev, int slave, int port,\n\t\t\t\t     u8 smac_index, u64 *mac)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *mac_list =\n\t\t&tracker->slave_list[slave].res_list[RES_MAC];\n\tstruct mac_res *res, *tmp;\n\n\tlist_for_each_entry_safe(res, tmp, mac_list, list) {\n\t\tif (res->smac_index == smac_index && res->port == (u8) port) {\n\t\t\t*mac = res->mac;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOENT;\n}\n\nstatic int mac_add_to_slave(struct mlx4_dev *dev, int slave, u64 mac, int port, u8 smac_index)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *mac_list =\n\t\t&tracker->slave_list[slave].res_list[RES_MAC];\n\tstruct mac_res *res, *tmp;\n\n\tlist_for_each_entry_safe(res, tmp, mac_list, list) {\n\t\tif (res->mac == mac && res->port == (u8) port) {\n\t\t\t \n\t\t\t++res->ref_count;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (mlx4_grant_resource(dev, slave, RES_MAC, 1, port))\n\t\treturn -EINVAL;\n\tres = kzalloc(sizeof(*res), GFP_KERNEL);\n\tif (!res) {\n\t\tmlx4_release_resource(dev, slave, RES_MAC, 1, port);\n\t\treturn -ENOMEM;\n\t}\n\tres->mac = mac;\n\tres->port = (u8) port;\n\tres->smac_index = smac_index;\n\tres->ref_count = 1;\n\tlist_add_tail(&res->list,\n\t\t      &tracker->slave_list[slave].res_list[RES_MAC]);\n\treturn 0;\n}\n\nstatic void mac_del_from_slave(struct mlx4_dev *dev, int slave, u64 mac,\n\t\t\t       int port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *mac_list =\n\t\t&tracker->slave_list[slave].res_list[RES_MAC];\n\tstruct mac_res *res, *tmp;\n\n\tlist_for_each_entry_safe(res, tmp, mac_list, list) {\n\t\tif (res->mac == mac && res->port == (u8) port) {\n\t\t\tif (!--res->ref_count) {\n\t\t\t\tlist_del(&res->list);\n\t\t\t\tmlx4_release_resource(dev, slave, RES_MAC, 1, port);\n\t\t\t\tkfree(res);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void rem_slave_macs(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *mac_list =\n\t\t&tracker->slave_list[slave].res_list[RES_MAC];\n\tstruct mac_res *res, *tmp;\n\tint i;\n\n\tlist_for_each_entry_safe(res, tmp, mac_list, list) {\n\t\tlist_del(&res->list);\n\t\t \n\t\tfor (i = 0; i < res->ref_count; i++)\n\t\t\t__mlx4_unregister_mac(dev, res->port, res->mac);\n\t\tmlx4_release_resource(dev, slave, RES_MAC, 1, res->port);\n\t\tkfree(res);\n\t}\n}\n\nstatic int mac_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t u64 in_param, u64 *out_param, int in_port)\n{\n\tint err = -EINVAL;\n\tint port;\n\tu64 mac;\n\tu8 smac_index;\n\n\tif (op != RES_OP_RESERVE_AND_MAP)\n\t\treturn err;\n\n\tport = !in_port ? get_param_l(out_param) : in_port;\n\tport = mlx4_slave_convert_port(\n\t\t\tdev, slave, port);\n\n\tif (port < 0)\n\t\treturn -EINVAL;\n\tmac = in_param;\n\n\terr = __mlx4_register_mac(dev, port, mac);\n\tif (err >= 0) {\n\t\tsmac_index = err;\n\t\tset_param_l(out_param, err);\n\t\terr = 0;\n\t}\n\n\tif (!err) {\n\t\terr = mac_add_to_slave(dev, slave, mac, port, smac_index);\n\t\tif (err)\n\t\t\t__mlx4_unregister_mac(dev, port, mac);\n\t}\n\treturn err;\n}\n\nstatic int vlan_add_to_slave(struct mlx4_dev *dev, int slave, u16 vlan,\n\t\t\t     int port, int vlan_index)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *vlan_list =\n\t\t&tracker->slave_list[slave].res_list[RES_VLAN];\n\tstruct vlan_res *res, *tmp;\n\n\tlist_for_each_entry_safe(res, tmp, vlan_list, list) {\n\t\tif (res->vlan == vlan && res->port == (u8) port) {\n\t\t\t \n\t\t\t++res->ref_count;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (mlx4_grant_resource(dev, slave, RES_VLAN, 1, port))\n\t\treturn -EINVAL;\n\tres = kzalloc(sizeof(*res), GFP_KERNEL);\n\tif (!res) {\n\t\tmlx4_release_resource(dev, slave, RES_VLAN, 1, port);\n\t\treturn -ENOMEM;\n\t}\n\tres->vlan = vlan;\n\tres->port = (u8) port;\n\tres->vlan_index = vlan_index;\n\tres->ref_count = 1;\n\tlist_add_tail(&res->list,\n\t\t      &tracker->slave_list[slave].res_list[RES_VLAN]);\n\treturn 0;\n}\n\n\nstatic void vlan_del_from_slave(struct mlx4_dev *dev, int slave, u16 vlan,\n\t\t\t\tint port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *vlan_list =\n\t\t&tracker->slave_list[slave].res_list[RES_VLAN];\n\tstruct vlan_res *res, *tmp;\n\n\tlist_for_each_entry_safe(res, tmp, vlan_list, list) {\n\t\tif (res->vlan == vlan && res->port == (u8) port) {\n\t\t\tif (!--res->ref_count) {\n\t\t\t\tlist_del(&res->list);\n\t\t\t\tmlx4_release_resource(dev, slave, RES_VLAN,\n\t\t\t\t\t\t      1, port);\n\t\t\t\tkfree(res);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void rem_slave_vlans(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *vlan_list =\n\t\t&tracker->slave_list[slave].res_list[RES_VLAN];\n\tstruct vlan_res *res, *tmp;\n\tint i;\n\n\tlist_for_each_entry_safe(res, tmp, vlan_list, list) {\n\t\tlist_del(&res->list);\n\t\t \n\t\tfor (i = 0; i < res->ref_count; i++)\n\t\t\t__mlx4_unregister_vlan(dev, res->port, res->vlan);\n\t\tmlx4_release_resource(dev, slave, RES_VLAN, 1, res->port);\n\t\tkfree(res);\n\t}\n}\n\nstatic int vlan_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t  u64 in_param, u64 *out_param, int in_port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_slave_state *slave_state = priv->mfunc.master.slave_state;\n\tint err;\n\tu16 vlan;\n\tint vlan_index;\n\tint port;\n\n\tport = !in_port ? get_param_l(out_param) : in_port;\n\n\tif (!port || op != RES_OP_RESERVE_AND_MAP)\n\t\treturn -EINVAL;\n\n\tport = mlx4_slave_convert_port(\n\t\t\tdev, slave, port);\n\n\tif (port < 0)\n\t\treturn -EINVAL;\n\t \n\tif (!in_port && port > 0 && port <= dev->caps.num_ports) {\n\t\tslave_state[slave].old_vlan_api = true;\n\t\treturn 0;\n\t}\n\n\tvlan = (u16) in_param;\n\n\terr = __mlx4_register_vlan(dev, port, vlan, &vlan_index);\n\tif (!err) {\n\t\tset_param_l(out_param, (u32) vlan_index);\n\t\terr = vlan_add_to_slave(dev, slave, vlan, port, vlan_index);\n\t\tif (err)\n\t\t\t__mlx4_unregister_vlan(dev, port, vlan);\n\t}\n\treturn err;\n}\n\nstatic int counter_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t     u64 in_param, u64 *out_param, int port)\n{\n\tu32 index;\n\tint err;\n\n\tif (op != RES_OP_RESERVE)\n\t\treturn -EINVAL;\n\n\terr = mlx4_grant_resource(dev, slave, RES_COUNTER, 1, 0);\n\tif (err)\n\t\treturn err;\n\n\terr = __mlx4_counter_alloc(dev, &index);\n\tif (err) {\n\t\tmlx4_release_resource(dev, slave, RES_COUNTER, 1, 0);\n\t\treturn err;\n\t}\n\n\terr = add_res_range(dev, slave, index, 1, RES_COUNTER, port);\n\tif (err) {\n\t\t__mlx4_counter_free(dev, index);\n\t\tmlx4_release_resource(dev, slave, RES_COUNTER, 1, 0);\n\t} else {\n\t\tset_param_l(out_param, index);\n\t}\n\n\treturn err;\n}\n\nstatic int xrcdn_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t   u64 in_param, u64 *out_param)\n{\n\tu32 xrcdn;\n\tint err;\n\n\tif (op != RES_OP_RESERVE)\n\t\treturn -EINVAL;\n\n\terr = __mlx4_xrcd_alloc(dev, &xrcdn);\n\tif (err)\n\t\treturn err;\n\n\terr = add_res_range(dev, slave, xrcdn, 1, RES_XRCD, 0);\n\tif (err)\n\t\t__mlx4_xrcd_free(dev, xrcdn);\n\telse\n\t\tset_param_l(out_param, xrcdn);\n\n\treturn err;\n}\n\nint mlx4_ALLOC_RES_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint alop = vhcr->op_modifier;\n\n\tswitch (vhcr->in_modifier & 0xFF) {\n\tcase RES_QP:\n\t\terr = qp_alloc_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t   vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tcase RES_MTT:\n\t\terr = mtt_alloc_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t    vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tcase RES_MPT:\n\t\terr = mpt_alloc_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t    vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tcase RES_CQ:\n\t\terr = cq_alloc_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t   vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tcase RES_SRQ:\n\t\terr = srq_alloc_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t    vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tcase RES_MAC:\n\t\terr = mac_alloc_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t    vhcr->in_param, &vhcr->out_param,\n\t\t\t\t    (vhcr->in_modifier >> 8) & 0xFF);\n\t\tbreak;\n\n\tcase RES_VLAN:\n\t\terr = vlan_alloc_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t     vhcr->in_param, &vhcr->out_param,\n\t\t\t\t     (vhcr->in_modifier >> 8) & 0xFF);\n\t\tbreak;\n\n\tcase RES_COUNTER:\n\t\terr = counter_alloc_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t\tvhcr->in_param, &vhcr->out_param, 0);\n\t\tbreak;\n\n\tcase RES_XRCD:\n\t\terr = xrcdn_alloc_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t      vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic int qp_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t       u64 in_param)\n{\n\tint err;\n\tint count;\n\tint base;\n\tint qpn;\n\n\tswitch (op) {\n\tcase RES_OP_RESERVE:\n\t\tbase = get_param_l(&in_param) & 0x7fffff;\n\t\tcount = get_param_h(&in_param);\n\t\terr = rem_res_range(dev, slave, base, count, RES_QP, 0);\n\t\tif (err)\n\t\t\tbreak;\n\t\tmlx4_release_resource(dev, slave, RES_QP, count, 0);\n\t\t__mlx4_qp_release_range(dev, base, count);\n\t\tbreak;\n\tcase RES_OP_MAP_ICM:\n\t\tqpn = get_param_l(&in_param) & 0x7fffff;\n\t\terr = qp_res_start_move_to(dev, slave, qpn, RES_QP_RESERVED,\n\t\t\t\t\t   NULL, 0);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!fw_reserved(dev, qpn))\n\t\t\t__mlx4_qp_free_icm(dev, qpn);\n\n\t\tres_end_move(dev, slave, RES_QP, qpn);\n\n\t\tif (valid_reserved(dev, slave, qpn))\n\t\t\terr = rem_res_range(dev, slave, qpn, 1, RES_QP, 0);\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn err;\n}\n\nstatic int mtt_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\tu64 in_param, u64 *out_param)\n{\n\tint err = -EINVAL;\n\tint base;\n\tint order;\n\n\tif (op != RES_OP_RESERVE_AND_MAP)\n\t\treturn err;\n\n\tbase = get_param_l(&in_param);\n\torder = get_param_h(&in_param);\n\terr = rem_res_range(dev, slave, base, 1, RES_MTT, order);\n\tif (!err) {\n\t\tmlx4_release_resource(dev, slave, RES_MTT, 1 << order, 0);\n\t\t__mlx4_free_mtt_range(dev, base, order);\n\t}\n\treturn err;\n}\n\nstatic int mpt_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\tu64 in_param)\n{\n\tint err = -EINVAL;\n\tint index;\n\tint id;\n\tstruct res_mpt *mpt;\n\n\tswitch (op) {\n\tcase RES_OP_RESERVE:\n\t\tindex = get_param_l(&in_param);\n\t\tid = index & mpt_mask(dev);\n\t\terr = get_res(dev, slave, id, RES_MPT, &mpt);\n\t\tif (err)\n\t\t\tbreak;\n\t\tindex = mpt->key;\n\t\tput_res(dev, slave, id, RES_MPT);\n\n\t\terr = rem_res_range(dev, slave, id, 1, RES_MPT, 0);\n\t\tif (err)\n\t\t\tbreak;\n\t\tmlx4_release_resource(dev, slave, RES_MPT, 1, 0);\n\t\t__mlx4_mpt_release(dev, index);\n\t\tbreak;\n\tcase RES_OP_MAP_ICM:\n\t\tindex = get_param_l(&in_param);\n\t\tid = index & mpt_mask(dev);\n\t\terr = mr_res_start_move_to(dev, slave, id,\n\t\t\t\t\t   RES_MPT_RESERVED, &mpt);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t__mlx4_mpt_free_icm(dev, mpt->key);\n\t\tres_end_move(dev, slave, RES_MPT, id);\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\treturn err;\n}\n\nstatic int cq_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t       u64 in_param, u64 *out_param)\n{\n\tint cqn;\n\tint err;\n\n\tswitch (op) {\n\tcase RES_OP_RESERVE_AND_MAP:\n\t\tcqn = get_param_l(&in_param);\n\t\terr = rem_res_range(dev, slave, cqn, 1, RES_CQ, 0);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tmlx4_release_resource(dev, slave, RES_CQ, 1, 0);\n\t\t__mlx4_cq_free_icm(dev, cqn);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic int srq_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\tu64 in_param, u64 *out_param)\n{\n\tint srqn;\n\tint err;\n\n\tswitch (op) {\n\tcase RES_OP_RESERVE_AND_MAP:\n\t\tsrqn = get_param_l(&in_param);\n\t\terr = rem_res_range(dev, slave, srqn, 1, RES_SRQ, 0);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tmlx4_release_resource(dev, slave, RES_SRQ, 1, 0);\n\t\t__mlx4_srq_free_icm(dev, srqn);\n\t\tbreak;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic int mac_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t    u64 in_param, u64 *out_param, int in_port)\n{\n\tint port;\n\tint err = 0;\n\n\tswitch (op) {\n\tcase RES_OP_RESERVE_AND_MAP:\n\t\tport = !in_port ? get_param_l(out_param) : in_port;\n\t\tport = mlx4_slave_convert_port(\n\t\t\t\tdev, slave, port);\n\n\t\tif (port < 0)\n\t\t\treturn -EINVAL;\n\t\tmac_del_from_slave(dev, slave, in_param, port);\n\t\t__mlx4_unregister_mac(dev, port, in_param);\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn err;\n\n}\n\nstatic int vlan_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t    u64 in_param, u64 *out_param, int port)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_slave_state *slave_state = priv->mfunc.master.slave_state;\n\tint err = 0;\n\n\tport = mlx4_slave_convert_port(\n\t\t\tdev, slave, port);\n\n\tif (port < 0)\n\t\treturn -EINVAL;\n\tswitch (op) {\n\tcase RES_OP_RESERVE_AND_MAP:\n\t\tif (slave_state[slave].old_vlan_api)\n\t\t\treturn 0;\n\t\tif (!port)\n\t\t\treturn -EINVAL;\n\t\tvlan_del_from_slave(dev, slave, in_param, port);\n\t\t__mlx4_unregister_vlan(dev, port, in_param);\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic int counter_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t    u64 in_param, u64 *out_param)\n{\n\tint index;\n\tint err;\n\n\tif (op != RES_OP_RESERVE)\n\t\treturn -EINVAL;\n\n\tindex = get_param_l(&in_param);\n\tif (index == MLX4_SINK_COUNTER_INDEX(dev))\n\t\treturn 0;\n\n\terr = rem_res_range(dev, slave, index, 1, RES_COUNTER, 0);\n\tif (err)\n\t\treturn err;\n\n\t__mlx4_counter_free(dev, index);\n\tmlx4_release_resource(dev, slave, RES_COUNTER, 1, 0);\n\n\treturn err;\n}\n\nstatic int xrcdn_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\n\t\t\t  u64 in_param, u64 *out_param)\n{\n\tint xrcdn;\n\tint err;\n\n\tif (op != RES_OP_RESERVE)\n\t\treturn -EINVAL;\n\n\txrcdn = get_param_l(&in_param);\n\terr = rem_res_range(dev, slave, xrcdn, 1, RES_XRCD, 0);\n\tif (err)\n\t\treturn err;\n\n\t__mlx4_xrcd_free(dev, xrcdn);\n\n\treturn err;\n}\n\nint mlx4_FREE_RES_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t  struct mlx4_vhcr *vhcr,\n\t\t\t  struct mlx4_cmd_mailbox *inbox,\n\t\t\t  struct mlx4_cmd_mailbox *outbox,\n\t\t\t  struct mlx4_cmd_info *cmd)\n{\n\tint err = -EINVAL;\n\tint alop = vhcr->op_modifier;\n\n\tswitch (vhcr->in_modifier & 0xFF) {\n\tcase RES_QP:\n\t\terr = qp_free_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t  vhcr->in_param);\n\t\tbreak;\n\n\tcase RES_MTT:\n\t\terr = mtt_free_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t   vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tcase RES_MPT:\n\t\terr = mpt_free_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t   vhcr->in_param);\n\t\tbreak;\n\n\tcase RES_CQ:\n\t\terr = cq_free_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t  vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tcase RES_SRQ:\n\t\terr = srq_free_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t   vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tcase RES_MAC:\n\t\terr = mac_free_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t   vhcr->in_param, &vhcr->out_param,\n\t\t\t\t   (vhcr->in_modifier >> 8) & 0xFF);\n\t\tbreak;\n\n\tcase RES_VLAN:\n\t\terr = vlan_free_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t    vhcr->in_param, &vhcr->out_param,\n\t\t\t\t    (vhcr->in_modifier >> 8) & 0xFF);\n\t\tbreak;\n\n\tcase RES_COUNTER:\n\t\terr = counter_free_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t       vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tcase RES_XRCD:\n\t\terr = xrcdn_free_res(dev, slave, vhcr->op_modifier, alop,\n\t\t\t\t     vhcr->in_param, &vhcr->out_param);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\treturn err;\n}\n\n \nstatic int mr_phys_mpt(struct mlx4_mpt_entry *mpt)\n{\n\treturn (be32_to_cpu(mpt->flags) >> 9) & 1;\n}\n\nstatic int mr_get_mtt_addr(struct mlx4_mpt_entry *mpt)\n{\n\treturn (int)be64_to_cpu(mpt->mtt_addr) & 0xfffffff8;\n}\n\nstatic int mr_get_mtt_size(struct mlx4_mpt_entry *mpt)\n{\n\treturn be32_to_cpu(mpt->mtt_sz);\n}\n\nstatic u32 mr_get_pd(struct mlx4_mpt_entry *mpt)\n{\n\treturn be32_to_cpu(mpt->pd_flags) & 0x00ffffff;\n}\n\nstatic int mr_is_fmr(struct mlx4_mpt_entry *mpt)\n{\n\treturn be32_to_cpu(mpt->pd_flags) & MLX4_MPT_PD_FLAG_FAST_REG;\n}\n\nstatic int mr_is_bind_enabled(struct mlx4_mpt_entry *mpt)\n{\n\treturn be32_to_cpu(mpt->flags) & MLX4_MPT_FLAG_BIND_ENABLE;\n}\n\nstatic int mr_is_region(struct mlx4_mpt_entry *mpt)\n{\n\treturn be32_to_cpu(mpt->flags) & MLX4_MPT_FLAG_REGION;\n}\n\nstatic int qp_get_mtt_addr(struct mlx4_qp_context *qpc)\n{\n\treturn be32_to_cpu(qpc->mtt_base_addr_l) & 0xfffffff8;\n}\n\nstatic int srq_get_mtt_addr(struct mlx4_srq_context *srqc)\n{\n\treturn be32_to_cpu(srqc->mtt_base_addr_l) & 0xfffffff8;\n}\n\nstatic int qp_get_mtt_size(struct mlx4_qp_context *qpc)\n{\n\tint page_shift = (qpc->log_page_size & 0x3f) + 12;\n\tint log_sq_size = (qpc->sq_size_stride >> 3) & 0xf;\n\tint log_sq_sride = qpc->sq_size_stride & 7;\n\tint log_rq_size = (qpc->rq_size_stride >> 3) & 0xf;\n\tint log_rq_stride = qpc->rq_size_stride & 7;\n\tint srq = (be32_to_cpu(qpc->srqn) >> 24) & 1;\n\tint rss = (be32_to_cpu(qpc->flags) >> 13) & 1;\n\tu32 ts = (be32_to_cpu(qpc->flags) >> 16) & 0xff;\n\tint xrc = (ts == MLX4_QP_ST_XRC) ? 1 : 0;\n\tint sq_size;\n\tint rq_size;\n\tint total_pages;\n\tint total_mem;\n\tint page_offset = (be32_to_cpu(qpc->params2) >> 6) & 0x3f;\n\tint tot;\n\n\tsq_size = 1 << (log_sq_size + log_sq_sride + 4);\n\trq_size = (srq|rss|xrc) ? 0 : (1 << (log_rq_size + log_rq_stride + 4));\n\ttotal_mem = sq_size + rq_size;\n\ttot = (total_mem + (page_offset << 6)) >> page_shift;\n\ttotal_pages = !tot ? 1 : roundup_pow_of_two(tot);\n\n\treturn total_pages;\n}\n\nstatic int check_mtt_range(struct mlx4_dev *dev, int slave, int start,\n\t\t\t   int size, struct res_mtt *mtt)\n{\n\tint res_start = mtt->com.res_id;\n\tint res_size = (1 << mtt->order);\n\n\tif (start < res_start || start + size > res_start + res_size)\n\t\treturn -EPERM;\n\treturn 0;\n}\n\nint mlx4_SW2HW_MPT_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint index = vhcr->in_modifier;\n\tstruct res_mtt *mtt;\n\tstruct res_mpt *mpt = NULL;\n\tint mtt_base = mr_get_mtt_addr(inbox->buf) / dev->caps.mtt_entry_sz;\n\tint phys;\n\tint id;\n\tu32 pd;\n\tint pd_slave;\n\n\tid = index & mpt_mask(dev);\n\terr = mr_res_start_move_to(dev, slave, id, RES_MPT_HW, &mpt);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (!mr_is_region(inbox->buf)) {\n\t\terr = -EPERM;\n\t\tgoto ex_abort;\n\t}\n\n\t \n\tpd = mr_get_pd(inbox->buf);\n\tpd_slave = (pd >> 17) & 0x7f;\n\tif (pd_slave != 0 && --pd_slave != slave) {\n\t\terr = -EPERM;\n\t\tgoto ex_abort;\n\t}\n\n\tif (mr_is_fmr(inbox->buf)) {\n\t\t \n\t\tif (mr_is_bind_enabled(inbox->buf)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto ex_abort;\n\t\t}\n\t\t \n\t\tif (!mr_is_region(inbox->buf)) {\n\t\t\terr = -EPERM;\n\t\t\tgoto ex_abort;\n\t\t}\n\t}\n\n\tphys = mr_phys_mpt(inbox->buf);\n\tif (!phys) {\n\t\terr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\n\t\tif (err)\n\t\t\tgoto ex_abort;\n\n\t\terr = check_mtt_range(dev, slave, mtt_base,\n\t\t\t\t      mr_get_mtt_size(inbox->buf), mtt);\n\t\tif (err)\n\t\t\tgoto ex_put;\n\n\t\tmpt->mtt = mtt;\n\t}\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto ex_put;\n\n\tif (!phys) {\n\t\tatomic_inc(&mtt->ref_count);\n\t\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\n\t}\n\n\tres_end_move(dev, slave, RES_MPT, id);\n\treturn 0;\n\nex_put:\n\tif (!phys)\n\t\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\nex_abort:\n\tres_abort_move(dev, slave, RES_MPT, id);\n\n\treturn err;\n}\n\nint mlx4_HW2SW_MPT_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint index = vhcr->in_modifier;\n\tstruct res_mpt *mpt;\n\tint id;\n\n\tid = index & mpt_mask(dev);\n\terr = mr_res_start_move_to(dev, slave, id, RES_MPT_MAPPED, &mpt);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto ex_abort;\n\n\tif (mpt->mtt)\n\t\tatomic_dec(&mpt->mtt->ref_count);\n\n\tres_end_move(dev, slave, RES_MPT, id);\n\treturn 0;\n\nex_abort:\n\tres_abort_move(dev, slave, RES_MPT, id);\n\n\treturn err;\n}\n\nint mlx4_QUERY_MPT_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint index = vhcr->in_modifier;\n\tstruct res_mpt *mpt;\n\tint id;\n\n\tid = index & mpt_mask(dev);\n\terr = get_res(dev, slave, id, RES_MPT, &mpt);\n\tif (err)\n\t\treturn err;\n\n\tif (mpt->com.from_state == RES_MPT_MAPPED) {\n\t\t \n\t\tstruct mlx4_mpt_entry *mpt_entry = mlx4_table_find(\n\t\t\t\t\t&mlx4_priv(dev)->mr_table.dmpt_table,\n\t\t\t\t\tmpt->key, NULL);\n\n\t\tif (NULL == mpt_entry || NULL == outbox->buf) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmemcpy(outbox->buf, mpt_entry, sizeof(*mpt_entry));\n\n\t\terr = 0;\n\t} else if (mpt->com.from_state == RES_MPT_HW) {\n\t\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\t} else {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\nout:\n\tput_res(dev, slave, id, RES_MPT);\n\treturn err;\n}\n\nstatic int qp_get_rcqn(struct mlx4_qp_context *qpc)\n{\n\treturn be32_to_cpu(qpc->cqn_recv) & 0xffffff;\n}\n\nstatic int qp_get_scqn(struct mlx4_qp_context *qpc)\n{\n\treturn be32_to_cpu(qpc->cqn_send) & 0xffffff;\n}\n\nstatic u32 qp_get_srqn(struct mlx4_qp_context *qpc)\n{\n\treturn be32_to_cpu(qpc->srqn) & 0x1ffffff;\n}\n\nstatic void adjust_proxy_tun_qkey(struct mlx4_dev *dev, struct mlx4_vhcr *vhcr,\n\t\t\t\t  struct mlx4_qp_context *context)\n{\n\tu32 qpn = vhcr->in_modifier & 0xffffff;\n\tu32 qkey = 0;\n\n\tif (mlx4_get_parav_qkey(dev, qpn, &qkey))\n\t\treturn;\n\n\t \n\tcontext->qkey = cpu_to_be32(qkey);\n}\n\nstatic int adjust_qp_sched_queue(struct mlx4_dev *dev, int slave,\n\t\t\t\t struct mlx4_qp_context *qpc,\n\t\t\t\t struct mlx4_cmd_mailbox *inbox);\n\nint mlx4_RST2INIT_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t     struct mlx4_vhcr *vhcr,\n\t\t\t     struct mlx4_cmd_mailbox *inbox,\n\t\t\t     struct mlx4_cmd_mailbox *outbox,\n\t\t\t     struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint qpn = vhcr->in_modifier & 0x7fffff;\n\tstruct res_mtt *mtt;\n\tstruct res_qp *qp;\n\tstruct mlx4_qp_context *qpc = inbox->buf + 8;\n\tint mtt_base = qp_get_mtt_addr(qpc) / dev->caps.mtt_entry_sz;\n\tint mtt_size = qp_get_mtt_size(qpc);\n\tstruct res_cq *rcq;\n\tstruct res_cq *scq;\n\tint rcqn = qp_get_rcqn(qpc);\n\tint scqn = qp_get_scqn(qpc);\n\tu32 srqn = qp_get_srqn(qpc) & 0xffffff;\n\tint use_srq = (qp_get_srqn(qpc) >> 24) & 1;\n\tstruct res_srq *srq;\n\tint local_qpn = vhcr->in_modifier & 0xffffff;\n\n\terr = adjust_qp_sched_queue(dev, slave, qpc, inbox);\n\tif (err)\n\t\treturn err;\n\n\terr = qp_res_start_move_to(dev, slave, qpn, RES_QP_HW, &qp, 0);\n\tif (err)\n\t\treturn err;\n\tqp->local_qpn = local_qpn;\n\tqp->sched_queue = 0;\n\tqp->param3 = 0;\n\tqp->vlan_control = 0;\n\tqp->fvl_rx = 0;\n\tqp->pri_path_fl = 0;\n\tqp->vlan_index = 0;\n\tqp->feup = 0;\n\tqp->qpc_flags = be32_to_cpu(qpc->flags);\n\n\terr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\n\tif (err)\n\t\tgoto ex_abort;\n\n\terr = check_mtt_range(dev, slave, mtt_base, mtt_size, mtt);\n\tif (err)\n\t\tgoto ex_put_mtt;\n\n\terr = get_res(dev, slave, rcqn, RES_CQ, &rcq);\n\tif (err)\n\t\tgoto ex_put_mtt;\n\n\tif (scqn != rcqn) {\n\t\terr = get_res(dev, slave, scqn, RES_CQ, &scq);\n\t\tif (err)\n\t\t\tgoto ex_put_rcq;\n\t} else\n\t\tscq = rcq;\n\n\tif (use_srq) {\n\t\terr = get_res(dev, slave, srqn, RES_SRQ, &srq);\n\t\tif (err)\n\t\t\tgoto ex_put_scq;\n\t}\n\n\tadjust_proxy_tun_qkey(dev, vhcr, qpc);\n\tupdate_pkey_index(dev, slave, inbox);\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto ex_put_srq;\n\tatomic_inc(&mtt->ref_count);\n\tqp->mtt = mtt;\n\tatomic_inc(&rcq->ref_count);\n\tqp->rcq = rcq;\n\tatomic_inc(&scq->ref_count);\n\tqp->scq = scq;\n\n\tif (scqn != rcqn)\n\t\tput_res(dev, slave, scqn, RES_CQ);\n\n\tif (use_srq) {\n\t\tatomic_inc(&srq->ref_count);\n\t\tput_res(dev, slave, srqn, RES_SRQ);\n\t\tqp->srq = srq;\n\t}\n\n\t \n\tqp->param3 = qpc->param3;\n\tput_res(dev, slave, rcqn, RES_CQ);\n\tput_res(dev, slave, mtt_base, RES_MTT);\n\tres_end_move(dev, slave, RES_QP, qpn);\n\n\treturn 0;\n\nex_put_srq:\n\tif (use_srq)\n\t\tput_res(dev, slave, srqn, RES_SRQ);\nex_put_scq:\n\tif (scqn != rcqn)\n\t\tput_res(dev, slave, scqn, RES_CQ);\nex_put_rcq:\n\tput_res(dev, slave, rcqn, RES_CQ);\nex_put_mtt:\n\tput_res(dev, slave, mtt_base, RES_MTT);\nex_abort:\n\tres_abort_move(dev, slave, RES_QP, qpn);\n\n\treturn err;\n}\n\nstatic int eq_get_mtt_addr(struct mlx4_eq_context *eqc)\n{\n\treturn be32_to_cpu(eqc->mtt_base_addr_l) & 0xfffffff8;\n}\n\nstatic int eq_get_mtt_size(struct mlx4_eq_context *eqc)\n{\n\tint log_eq_size = eqc->log_eq_size & 0x1f;\n\tint page_shift = (eqc->log_page_size & 0x3f) + 12;\n\n\tif (log_eq_size + 5 < page_shift)\n\t\treturn 1;\n\n\treturn 1 << (log_eq_size + 5 - page_shift);\n}\n\nstatic int cq_get_mtt_addr(struct mlx4_cq_context *cqc)\n{\n\treturn be32_to_cpu(cqc->mtt_base_addr_l) & 0xfffffff8;\n}\n\nstatic int cq_get_mtt_size(struct mlx4_cq_context *cqc)\n{\n\tint log_cq_size = (be32_to_cpu(cqc->logsize_usrpage) >> 24) & 0x1f;\n\tint page_shift = (cqc->log_page_size & 0x3f) + 12;\n\n\tif (log_cq_size + 5 < page_shift)\n\t\treturn 1;\n\n\treturn 1 << (log_cq_size + 5 - page_shift);\n}\n\nint mlx4_SW2HW_EQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t  struct mlx4_vhcr *vhcr,\n\t\t\t  struct mlx4_cmd_mailbox *inbox,\n\t\t\t  struct mlx4_cmd_mailbox *outbox,\n\t\t\t  struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint eqn = vhcr->in_modifier;\n\tint res_id = (slave << 10) | eqn;\n\tstruct mlx4_eq_context *eqc = inbox->buf;\n\tint mtt_base = eq_get_mtt_addr(eqc) / dev->caps.mtt_entry_sz;\n\tint mtt_size = eq_get_mtt_size(eqc);\n\tstruct res_eq *eq;\n\tstruct res_mtt *mtt;\n\n\terr = add_res_range(dev, slave, res_id, 1, RES_EQ, 0);\n\tif (err)\n\t\treturn err;\n\terr = eq_res_start_move_to(dev, slave, res_id, RES_EQ_HW, &eq);\n\tif (err)\n\t\tgoto out_add;\n\n\terr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\n\tif (err)\n\t\tgoto out_move;\n\n\terr = check_mtt_range(dev, slave, mtt_base, mtt_size, mtt);\n\tif (err)\n\t\tgoto out_put;\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto out_put;\n\n\tatomic_inc(&mtt->ref_count);\n\teq->mtt = mtt;\n\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\n\tres_end_move(dev, slave, RES_EQ, res_id);\n\treturn 0;\n\nout_put:\n\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\nout_move:\n\tres_abort_move(dev, slave, RES_EQ, res_id);\nout_add:\n\trem_res_range(dev, slave, res_id, 1, RES_EQ, 0);\n\treturn err;\n}\n\nint mlx4_CONFIG_DEV_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t    struct mlx4_vhcr *vhcr,\n\t\t\t    struct mlx4_cmd_mailbox *inbox,\n\t\t\t    struct mlx4_cmd_mailbox *outbox,\n\t\t\t    struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tu8 get = vhcr->op_modifier;\n\n\tif (get != 1)\n\t\treturn -EPERM;\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\n\treturn err;\n}\n\nstatic int get_containing_mtt(struct mlx4_dev *dev, int slave, int start,\n\t\t\t      int len, struct res_mtt **res)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct res_mtt *mtt;\n\tint err = -EINVAL;\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry(mtt, &tracker->slave_list[slave].res_list[RES_MTT],\n\t\t\t    com.list) {\n\t\tif (!check_mtt_range(dev, slave, start, len, mtt)) {\n\t\t\t*res = mtt;\n\t\t\tmtt->com.from_state = mtt->com.state;\n\t\t\tmtt->com.state = RES_MTT_BUSY;\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\treturn err;\n}\n\nstatic int verify_qp_parameters(struct mlx4_dev *dev,\n\t\t\t\tstruct mlx4_vhcr *vhcr,\n\t\t\t\tstruct mlx4_cmd_mailbox *inbox,\n\t\t\t\tenum qp_transition transition, u8 slave)\n{\n\tu32\t\t\tqp_type;\n\tu32\t\t\tqpn;\n\tstruct mlx4_qp_context\t*qp_ctx;\n\tenum mlx4_qp_optpar\toptpar;\n\tint port;\n\tint num_gids;\n\n\tqp_ctx  = inbox->buf + 8;\n\tqp_type\t= (be32_to_cpu(qp_ctx->flags) >> 16) & 0xff;\n\toptpar\t= be32_to_cpu(*(__be32 *) inbox->buf);\n\n\tif (slave != mlx4_master_func_num(dev)) {\n\t\tqp_ctx->params2 &= ~cpu_to_be32(MLX4_QP_BIT_FPP);\n\t\t \n\t\tif (qp_ctx->rate_limit_params)\n\t\t\treturn -EPERM;\n\t}\n\n\tswitch (qp_type) {\n\tcase MLX4_QP_ST_RC:\n\tcase MLX4_QP_ST_XRC:\n\tcase MLX4_QP_ST_UC:\n\t\tswitch (transition) {\n\t\tcase QP_TRANS_INIT2RTR:\n\t\tcase QP_TRANS_RTR2RTS:\n\t\tcase QP_TRANS_RTS2RTS:\n\t\tcase QP_TRANS_SQD2SQD:\n\t\tcase QP_TRANS_SQD2RTS:\n\t\t\tif (slave != mlx4_master_func_num(dev)) {\n\t\t\t\tif (optpar & MLX4_QP_OPTPAR_PRIMARY_ADDR_PATH) {\n\t\t\t\t\tport = (qp_ctx->pri_path.sched_queue >> 6 & 1) + 1;\n\t\t\t\t\tif (dev->caps.port_mask[port] != MLX4_PORT_TYPE_IB)\n\t\t\t\t\t\tnum_gids = mlx4_get_slave_num_gids(dev, slave, port);\n\t\t\t\t\telse\n\t\t\t\t\t\tnum_gids = 1;\n\t\t\t\t\tif (qp_ctx->pri_path.mgid_index >= num_gids)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (optpar & MLX4_QP_OPTPAR_ALT_ADDR_PATH) {\n\t\t\t\t\tport = (qp_ctx->alt_path.sched_queue >> 6 & 1) + 1;\n\t\t\t\t\tif (dev->caps.port_mask[port] != MLX4_PORT_TYPE_IB)\n\t\t\t\t\t\tnum_gids = mlx4_get_slave_num_gids(dev, slave, port);\n\t\t\t\t\telse\n\t\t\t\t\t\tnum_gids = 1;\n\t\t\t\t\tif (qp_ctx->alt_path.mgid_index >= num_gids)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase MLX4_QP_ST_MLX:\n\t\tqpn = vhcr->in_modifier & 0x7fffff;\n\t\tport = (qp_ctx->pri_path.sched_queue >> 6 & 1) + 1;\n\t\tif (transition == QP_TRANS_INIT2RTR &&\n\t\t    slave != mlx4_master_func_num(dev) &&\n\t\t    mlx4_is_qp_reserved(dev, qpn) &&\n\t\t    !mlx4_vf_smi_enabled(dev, slave, port)) {\n\t\t\t \n\t\t\tmlx4_err(dev, \"%s: unprivileged slave %d attempting to create an MLX proxy special QP on port %d\\n\",\n\t\t\t\t __func__, slave, port);\n\t\t\treturn -EPERM;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nint mlx4_WRITE_MTT_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd)\n{\n\tstruct mlx4_mtt mtt;\n\t__be64 *page_list = inbox->buf;\n\tu64 *pg_list = (u64 *)page_list;\n\tint i;\n\tstruct res_mtt *rmtt = NULL;\n\tint start = be64_to_cpu(page_list[0]);\n\tint npages = vhcr->in_modifier;\n\tint err;\n\n\terr = get_containing_mtt(dev, slave, start, npages, &rmtt);\n\tif (err)\n\t\treturn err;\n\n\t \n\tmtt.offset = 0;   \n\tmtt.order = 0;\n\tmtt.page_shift = 0;\n\tfor (i = 0; i < npages; ++i)\n\t\tpg_list[i + 2] = (be64_to_cpu(page_list[i + 2]) & ~1ULL);\n\n\terr = __mlx4_write_mtt(dev, &mtt, be64_to_cpu(page_list[0]), npages,\n\t\t\t       ((u64 *)page_list + 2));\n\n\tif (rmtt)\n\t\tput_res(dev, slave, rmtt->com.res_id, RES_MTT);\n\n\treturn err;\n}\n\nint mlx4_HW2SW_EQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t  struct mlx4_vhcr *vhcr,\n\t\t\t  struct mlx4_cmd_mailbox *inbox,\n\t\t\t  struct mlx4_cmd_mailbox *outbox,\n\t\t\t  struct mlx4_cmd_info *cmd)\n{\n\tint eqn = vhcr->in_modifier;\n\tint res_id = eqn | (slave << 10);\n\tstruct res_eq *eq;\n\tint err;\n\n\terr = eq_res_start_move_to(dev, slave, res_id, RES_EQ_RESERVED, &eq);\n\tif (err)\n\t\treturn err;\n\n\terr = get_res(dev, slave, eq->mtt->com.res_id, RES_MTT, NULL);\n\tif (err)\n\t\tgoto ex_abort;\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto ex_put;\n\n\tatomic_dec(&eq->mtt->ref_count);\n\tput_res(dev, slave, eq->mtt->com.res_id, RES_MTT);\n\tres_end_move(dev, slave, RES_EQ, res_id);\n\trem_res_range(dev, slave, res_id, 1, RES_EQ, 0);\n\n\treturn 0;\n\nex_put:\n\tput_res(dev, slave, eq->mtt->com.res_id, RES_MTT);\nex_abort:\n\tres_abort_move(dev, slave, RES_EQ, res_id);\n\n\treturn err;\n}\n\nint mlx4_GEN_EQE(struct mlx4_dev *dev, int slave, struct mlx4_eqe *eqe)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_slave_event_eq_info *event_eq;\n\tstruct mlx4_cmd_mailbox *mailbox;\n\tu32 in_modifier = 0;\n\tint err;\n\tint res_id;\n\tstruct res_eq *req;\n\n\tif (!priv->mfunc.master.slave_state)\n\t\treturn -EINVAL;\n\n\t \n\tif (slave < 0 || slave > dev->persist->num_vfs ||\n\t    slave == dev->caps.function ||\n\t    !priv->mfunc.master.slave_state[slave].active)\n\t\treturn 0;\n\n\tevent_eq = &priv->mfunc.master.slave_state[slave].event_eq[eqe->type];\n\n\t \n\tif (event_eq->eqn < 0)\n\t\treturn 0;\n\n\tmutex_lock(&priv->mfunc.master.gen_eqe_mutex[slave]);\n\tres_id = (slave << 10) | event_eq->eqn;\n\terr = get_res(dev, slave, res_id, RES_EQ, &req);\n\tif (err)\n\t\tgoto unlock;\n\n\tif (req->com.from_state != RES_EQ_HW) {\n\t\terr = -EINVAL;\n\t\tgoto put;\n\t}\n\n\tmailbox = mlx4_alloc_cmd_mailbox(dev);\n\tif (IS_ERR(mailbox)) {\n\t\terr = PTR_ERR(mailbox);\n\t\tgoto put;\n\t}\n\n\tif (eqe->type == MLX4_EVENT_TYPE_CMD) {\n\t\t++event_eq->token;\n\t\teqe->event.cmd.token = cpu_to_be16(event_eq->token);\n\t}\n\n\tmemcpy(mailbox->buf, (u8 *) eqe, 28);\n\n\tin_modifier = (slave & 0xff) | ((event_eq->eqn & 0x3ff) << 16);\n\n\terr = mlx4_cmd(dev, mailbox->dma, in_modifier, 0,\n\t\t       MLX4_CMD_GEN_EQE, MLX4_CMD_TIME_CLASS_B,\n\t\t       MLX4_CMD_NATIVE);\n\n\tput_res(dev, slave, res_id, RES_EQ);\n\tmutex_unlock(&priv->mfunc.master.gen_eqe_mutex[slave]);\n\tmlx4_free_cmd_mailbox(dev, mailbox);\n\treturn err;\n\nput:\n\tput_res(dev, slave, res_id, RES_EQ);\n\nunlock:\n\tmutex_unlock(&priv->mfunc.master.gen_eqe_mutex[slave]);\n\treturn err;\n}\n\nint mlx4_QUERY_EQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t  struct mlx4_vhcr *vhcr,\n\t\t\t  struct mlx4_cmd_mailbox *inbox,\n\t\t\t  struct mlx4_cmd_mailbox *outbox,\n\t\t\t  struct mlx4_cmd_info *cmd)\n{\n\tint eqn = vhcr->in_modifier;\n\tint res_id = eqn | (slave << 10);\n\tstruct res_eq *eq;\n\tint err;\n\n\terr = get_res(dev, slave, res_id, RES_EQ, &eq);\n\tif (err)\n\t\treturn err;\n\n\tif (eq->com.from_state != RES_EQ_HW) {\n\t\terr = -EINVAL;\n\t\tgoto ex_put;\n\t}\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\nex_put:\n\tput_res(dev, slave, res_id, RES_EQ);\n\treturn err;\n}\n\nint mlx4_SW2HW_CQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t  struct mlx4_vhcr *vhcr,\n\t\t\t  struct mlx4_cmd_mailbox *inbox,\n\t\t\t  struct mlx4_cmd_mailbox *outbox,\n\t\t\t  struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint cqn = vhcr->in_modifier;\n\tstruct mlx4_cq_context *cqc = inbox->buf;\n\tint mtt_base = cq_get_mtt_addr(cqc) / dev->caps.mtt_entry_sz;\n\tstruct res_cq *cq = NULL;\n\tstruct res_mtt *mtt;\n\n\terr = cq_res_start_move_to(dev, slave, cqn, RES_CQ_HW, &cq);\n\tif (err)\n\t\treturn err;\n\terr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\n\tif (err)\n\t\tgoto out_move;\n\terr = check_mtt_range(dev, slave, mtt_base, cq_get_mtt_size(cqc), mtt);\n\tif (err)\n\t\tgoto out_put;\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto out_put;\n\tatomic_inc(&mtt->ref_count);\n\tcq->mtt = mtt;\n\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\n\tres_end_move(dev, slave, RES_CQ, cqn);\n\treturn 0;\n\nout_put:\n\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\nout_move:\n\tres_abort_move(dev, slave, RES_CQ, cqn);\n\treturn err;\n}\n\nint mlx4_HW2SW_CQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t  struct mlx4_vhcr *vhcr,\n\t\t\t  struct mlx4_cmd_mailbox *inbox,\n\t\t\t  struct mlx4_cmd_mailbox *outbox,\n\t\t\t  struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint cqn = vhcr->in_modifier;\n\tstruct res_cq *cq = NULL;\n\n\terr = cq_res_start_move_to(dev, slave, cqn, RES_CQ_ALLOCATED, &cq);\n\tif (err)\n\t\treturn err;\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto out_move;\n\tatomic_dec(&cq->mtt->ref_count);\n\tres_end_move(dev, slave, RES_CQ, cqn);\n\treturn 0;\n\nout_move:\n\tres_abort_move(dev, slave, RES_CQ, cqn);\n\treturn err;\n}\n\nint mlx4_QUERY_CQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t  struct mlx4_vhcr *vhcr,\n\t\t\t  struct mlx4_cmd_mailbox *inbox,\n\t\t\t  struct mlx4_cmd_mailbox *outbox,\n\t\t\t  struct mlx4_cmd_info *cmd)\n{\n\tint cqn = vhcr->in_modifier;\n\tstruct res_cq *cq;\n\tint err;\n\n\terr = get_res(dev, slave, cqn, RES_CQ, &cq);\n\tif (err)\n\t\treturn err;\n\n\tif (cq->com.from_state != RES_CQ_HW)\n\t\tgoto ex_put;\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\nex_put:\n\tput_res(dev, slave, cqn, RES_CQ);\n\n\treturn err;\n}\n\nstatic int handle_resize(struct mlx4_dev *dev, int slave,\n\t\t\t struct mlx4_vhcr *vhcr,\n\t\t\t struct mlx4_cmd_mailbox *inbox,\n\t\t\t struct mlx4_cmd_mailbox *outbox,\n\t\t\t struct mlx4_cmd_info *cmd,\n\t\t\t struct res_cq *cq)\n{\n\tint err;\n\tstruct res_mtt *orig_mtt;\n\tstruct res_mtt *mtt;\n\tstruct mlx4_cq_context *cqc = inbox->buf;\n\tint mtt_base = cq_get_mtt_addr(cqc) / dev->caps.mtt_entry_sz;\n\n\terr = get_res(dev, slave, cq->mtt->com.res_id, RES_MTT, &orig_mtt);\n\tif (err)\n\t\treturn err;\n\n\tif (orig_mtt != cq->mtt) {\n\t\terr = -EINVAL;\n\t\tgoto ex_put;\n\t}\n\n\terr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\n\tif (err)\n\t\tgoto ex_put;\n\n\terr = check_mtt_range(dev, slave, mtt_base, cq_get_mtt_size(cqc), mtt);\n\tif (err)\n\t\tgoto ex_put1;\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto ex_put1;\n\tatomic_dec(&orig_mtt->ref_count);\n\tput_res(dev, slave, orig_mtt->com.res_id, RES_MTT);\n\tatomic_inc(&mtt->ref_count);\n\tcq->mtt = mtt;\n\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\n\treturn 0;\n\nex_put1:\n\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\nex_put:\n\tput_res(dev, slave, orig_mtt->com.res_id, RES_MTT);\n\n\treturn err;\n\n}\n\nint mlx4_MODIFY_CQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd)\n{\n\tint cqn = vhcr->in_modifier;\n\tstruct res_cq *cq;\n\tint err;\n\n\terr = get_res(dev, slave, cqn, RES_CQ, &cq);\n\tif (err)\n\t\treturn err;\n\n\tif (cq->com.from_state != RES_CQ_HW)\n\t\tgoto ex_put;\n\n\tif (vhcr->op_modifier == 0) {\n\t\terr = handle_resize(dev, slave, vhcr, inbox, outbox, cmd, cq);\n\t\tgoto ex_put;\n\t}\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\nex_put:\n\tput_res(dev, slave, cqn, RES_CQ);\n\n\treturn err;\n}\n\nstatic int srq_get_mtt_size(struct mlx4_srq_context *srqc)\n{\n\tint log_srq_size = (be32_to_cpu(srqc->state_logsize_srqn) >> 24) & 0xf;\n\tint log_rq_stride = srqc->logstride & 7;\n\tint page_shift = (srqc->log_page_size & 0x3f) + 12;\n\n\tif (log_srq_size + log_rq_stride + 4 < page_shift)\n\t\treturn 1;\n\n\treturn 1 << (log_srq_size + log_rq_stride + 4 - page_shift);\n}\n\nint mlx4_SW2HW_SRQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint srqn = vhcr->in_modifier;\n\tstruct res_mtt *mtt;\n\tstruct res_srq *srq = NULL;\n\tstruct mlx4_srq_context *srqc = inbox->buf;\n\tint mtt_base = srq_get_mtt_addr(srqc) / dev->caps.mtt_entry_sz;\n\n\tif (srqn != (be32_to_cpu(srqc->state_logsize_srqn) & 0xffffff))\n\t\treturn -EINVAL;\n\n\terr = srq_res_start_move_to(dev, slave, srqn, RES_SRQ_HW, &srq);\n\tif (err)\n\t\treturn err;\n\terr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\n\tif (err)\n\t\tgoto ex_abort;\n\terr = check_mtt_range(dev, slave, mtt_base, srq_get_mtt_size(srqc),\n\t\t\t      mtt);\n\tif (err)\n\t\tgoto ex_put_mtt;\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto ex_put_mtt;\n\n\tatomic_inc(&mtt->ref_count);\n\tsrq->mtt = mtt;\n\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\n\tres_end_move(dev, slave, RES_SRQ, srqn);\n\treturn 0;\n\nex_put_mtt:\n\tput_res(dev, slave, mtt->com.res_id, RES_MTT);\nex_abort:\n\tres_abort_move(dev, slave, RES_SRQ, srqn);\n\n\treturn err;\n}\n\nint mlx4_HW2SW_SRQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint srqn = vhcr->in_modifier;\n\tstruct res_srq *srq = NULL;\n\n\terr = srq_res_start_move_to(dev, slave, srqn, RES_SRQ_ALLOCATED, &srq);\n\tif (err)\n\t\treturn err;\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto ex_abort;\n\tatomic_dec(&srq->mtt->ref_count);\n\tif (srq->cq)\n\t\tatomic_dec(&srq->cq->ref_count);\n\tres_end_move(dev, slave, RES_SRQ, srqn);\n\n\treturn 0;\n\nex_abort:\n\tres_abort_move(dev, slave, RES_SRQ, srqn);\n\n\treturn err;\n}\n\nint mlx4_QUERY_SRQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint srqn = vhcr->in_modifier;\n\tstruct res_srq *srq;\n\n\terr = get_res(dev, slave, srqn, RES_SRQ, &srq);\n\tif (err)\n\t\treturn err;\n\tif (srq->com.from_state != RES_SRQ_HW) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\nout:\n\tput_res(dev, slave, srqn, RES_SRQ);\n\treturn err;\n}\n\nint mlx4_ARM_SRQ_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t struct mlx4_vhcr *vhcr,\n\t\t\t struct mlx4_cmd_mailbox *inbox,\n\t\t\t struct mlx4_cmd_mailbox *outbox,\n\t\t\t struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint srqn = vhcr->in_modifier;\n\tstruct res_srq *srq;\n\n\terr = get_res(dev, slave, srqn, RES_SRQ, &srq);\n\tif (err)\n\t\treturn err;\n\n\tif (srq->com.from_state != RES_SRQ_HW) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\nout:\n\tput_res(dev, slave, srqn, RES_SRQ);\n\treturn err;\n}\n\nint mlx4_GEN_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\tstruct mlx4_vhcr *vhcr,\n\t\t\tstruct mlx4_cmd_mailbox *inbox,\n\t\t\tstruct mlx4_cmd_mailbox *outbox,\n\t\t\tstruct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint qpn = vhcr->in_modifier & 0x7fffff;\n\tstruct res_qp *qp;\n\n\terr = get_res(dev, slave, qpn, RES_QP, &qp);\n\tif (err)\n\t\treturn err;\n\tif (qp->com.from_state != RES_QP_HW) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\nout:\n\tput_res(dev, slave, qpn, RES_QP);\n\treturn err;\n}\n\nint mlx4_INIT2INIT_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t      struct mlx4_vhcr *vhcr,\n\t\t\t      struct mlx4_cmd_mailbox *inbox,\n\t\t\t      struct mlx4_cmd_mailbox *outbox,\n\t\t\t      struct mlx4_cmd_info *cmd)\n{\n\tstruct mlx4_qp_context *context = inbox->buf + 8;\n\tadjust_proxy_tun_qkey(dev, vhcr, context);\n\tupdate_pkey_index(dev, slave, inbox);\n\treturn mlx4_GEN_QP_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n}\n\nstatic int adjust_qp_sched_queue(struct mlx4_dev *dev, int slave,\n\t\t\t\t  struct mlx4_qp_context *qpc,\n\t\t\t\t  struct mlx4_cmd_mailbox *inbox)\n{\n\tenum mlx4_qp_optpar optpar = be32_to_cpu(*(__be32 *)inbox->buf);\n\tu8 pri_sched_queue;\n\tint port = mlx4_slave_convert_port(\n\t\t   dev, slave, (qpc->pri_path.sched_queue >> 6 & 1) + 1) - 1;\n\n\tif (port < 0)\n\t\treturn -EINVAL;\n\n\tpri_sched_queue = (qpc->pri_path.sched_queue & ~(1 << 6)) |\n\t\t\t  ((port & 1) << 6);\n\n\tif (optpar & (MLX4_QP_OPTPAR_PRIMARY_ADDR_PATH | MLX4_QP_OPTPAR_SCHED_QUEUE) ||\n\t    qpc->pri_path.sched_queue || mlx4_is_eth(dev, port + 1)) {\n\t\tqpc->pri_path.sched_queue = pri_sched_queue;\n\t}\n\n\tif (optpar & MLX4_QP_OPTPAR_ALT_ADDR_PATH) {\n\t\tport = mlx4_slave_convert_port(\n\t\t\t\tdev, slave, (qpc->alt_path.sched_queue >> 6 & 1)\n\t\t\t\t+ 1) - 1;\n\t\tif (port < 0)\n\t\t\treturn -EINVAL;\n\t\tqpc->alt_path.sched_queue =\n\t\t\t(qpc->alt_path.sched_queue & ~(1 << 6)) |\n\t\t\t(port & 1) << 6;\n\t}\n\treturn 0;\n}\n\nstatic int roce_verify_mac(struct mlx4_dev *dev, int slave,\n\t\t\t\tstruct mlx4_qp_context *qpc,\n\t\t\t\tstruct mlx4_cmd_mailbox *inbox)\n{\n\tu64 mac;\n\tint port;\n\tu32 ts = (be32_to_cpu(qpc->flags) >> 16) & 0xff;\n\tu8 sched = *(u8 *)(inbox->buf + 64);\n\tu8 smac_ix;\n\n\tport = (sched >> 6 & 1) + 1;\n\tif (mlx4_is_eth(dev, port) && (ts != MLX4_QP_ST_MLX)) {\n\t\tsmac_ix = qpc->pri_path.grh_mylmc & 0x7f;\n\t\tif (mac_find_smac_ix_in_slave(dev, slave, port, smac_ix, &mac))\n\t\t\treturn -ENOENT;\n\t}\n\treturn 0;\n}\n\nint mlx4_INIT2RTR_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t     struct mlx4_vhcr *vhcr,\n\t\t\t     struct mlx4_cmd_mailbox *inbox,\n\t\t\t     struct mlx4_cmd_mailbox *outbox,\n\t\t\t     struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tstruct mlx4_qp_context *qpc = inbox->buf + 8;\n\tint qpn = vhcr->in_modifier & 0x7fffff;\n\tstruct res_qp *qp;\n\tu8 orig_sched_queue;\n\tu8 orig_vlan_control = qpc->pri_path.vlan_control;\n\tu8 orig_fvl_rx = qpc->pri_path.fvl_rx;\n\tu8 orig_pri_path_fl = qpc->pri_path.fl;\n\tu8 orig_vlan_index = qpc->pri_path.vlan_index;\n\tu8 orig_feup = qpc->pri_path.feup;\n\n\terr = adjust_qp_sched_queue(dev, slave, qpc, inbox);\n\tif (err)\n\t\treturn err;\n\terr = verify_qp_parameters(dev, vhcr, inbox, QP_TRANS_INIT2RTR, slave);\n\tif (err)\n\t\treturn err;\n\n\tif (roce_verify_mac(dev, slave, qpc, inbox))\n\t\treturn -EINVAL;\n\n\tupdate_pkey_index(dev, slave, inbox);\n\tupdate_gid(dev, inbox, (u8)slave);\n\tadjust_proxy_tun_qkey(dev, vhcr, qpc);\n\torig_sched_queue = qpc->pri_path.sched_queue;\n\n\terr = get_res(dev, slave, qpn, RES_QP, &qp);\n\tif (err)\n\t\treturn err;\n\tif (qp->com.from_state != RES_QP_HW) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\terr = update_vport_qp_param(dev, inbox, slave, qpn);\n\tif (err)\n\t\tgoto out;\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\nout:\n\t \n\tif (!err) {\n\t\tqp->sched_queue = orig_sched_queue;\n\t\tqp->vlan_control = orig_vlan_control;\n\t\tqp->fvl_rx\t=  orig_fvl_rx;\n\t\tqp->pri_path_fl = orig_pri_path_fl;\n\t\tqp->vlan_index  = orig_vlan_index;\n\t\tqp->feup\t= orig_feup;\n\t}\n\tput_res(dev, slave, qpn, RES_QP);\n\treturn err;\n}\n\nint mlx4_RTR2RTS_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t    struct mlx4_vhcr *vhcr,\n\t\t\t    struct mlx4_cmd_mailbox *inbox,\n\t\t\t    struct mlx4_cmd_mailbox *outbox,\n\t\t\t    struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tstruct mlx4_qp_context *context = inbox->buf + 8;\n\n\terr = adjust_qp_sched_queue(dev, slave, context, inbox);\n\tif (err)\n\t\treturn err;\n\terr = verify_qp_parameters(dev, vhcr, inbox, QP_TRANS_RTR2RTS, slave);\n\tif (err)\n\t\treturn err;\n\n\tupdate_pkey_index(dev, slave, inbox);\n\tupdate_gid(dev, inbox, (u8)slave);\n\tadjust_proxy_tun_qkey(dev, vhcr, context);\n\treturn mlx4_GEN_QP_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n}\n\nint mlx4_RTS2RTS_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t    struct mlx4_vhcr *vhcr,\n\t\t\t    struct mlx4_cmd_mailbox *inbox,\n\t\t\t    struct mlx4_cmd_mailbox *outbox,\n\t\t\t    struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tstruct mlx4_qp_context *context = inbox->buf + 8;\n\n\terr = adjust_qp_sched_queue(dev, slave, context, inbox);\n\tif (err)\n\t\treturn err;\n\terr = verify_qp_parameters(dev, vhcr, inbox, QP_TRANS_RTS2RTS, slave);\n\tif (err)\n\t\treturn err;\n\n\tupdate_pkey_index(dev, slave, inbox);\n\tupdate_gid(dev, inbox, (u8)slave);\n\tadjust_proxy_tun_qkey(dev, vhcr, context);\n\treturn mlx4_GEN_QP_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n}\n\n\nint mlx4_SQERR2RTS_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t      struct mlx4_vhcr *vhcr,\n\t\t\t      struct mlx4_cmd_mailbox *inbox,\n\t\t\t      struct mlx4_cmd_mailbox *outbox,\n\t\t\t      struct mlx4_cmd_info *cmd)\n{\n\tstruct mlx4_qp_context *context = inbox->buf + 8;\n\tint err = adjust_qp_sched_queue(dev, slave, context, inbox);\n\tif (err)\n\t\treturn err;\n\tadjust_proxy_tun_qkey(dev, vhcr, context);\n\treturn mlx4_GEN_QP_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n}\n\nint mlx4_SQD2SQD_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t    struct mlx4_vhcr *vhcr,\n\t\t\t    struct mlx4_cmd_mailbox *inbox,\n\t\t\t    struct mlx4_cmd_mailbox *outbox,\n\t\t\t    struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tstruct mlx4_qp_context *context = inbox->buf + 8;\n\n\terr = adjust_qp_sched_queue(dev, slave, context, inbox);\n\tif (err)\n\t\treturn err;\n\terr = verify_qp_parameters(dev, vhcr, inbox, QP_TRANS_SQD2SQD, slave);\n\tif (err)\n\t\treturn err;\n\n\tadjust_proxy_tun_qkey(dev, vhcr, context);\n\tupdate_gid(dev, inbox, (u8)slave);\n\tupdate_pkey_index(dev, slave, inbox);\n\treturn mlx4_GEN_QP_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n}\n\nint mlx4_SQD2RTS_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t    struct mlx4_vhcr *vhcr,\n\t\t\t    struct mlx4_cmd_mailbox *inbox,\n\t\t\t    struct mlx4_cmd_mailbox *outbox,\n\t\t\t    struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tstruct mlx4_qp_context *context = inbox->buf + 8;\n\n\terr = adjust_qp_sched_queue(dev, slave, context, inbox);\n\tif (err)\n\t\treturn err;\n\terr = verify_qp_parameters(dev, vhcr, inbox, QP_TRANS_SQD2RTS, slave);\n\tif (err)\n\t\treturn err;\n\n\tadjust_proxy_tun_qkey(dev, vhcr, context);\n\tupdate_gid(dev, inbox, (u8)slave);\n\tupdate_pkey_index(dev, slave, inbox);\n\treturn mlx4_GEN_QP_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n}\n\nint mlx4_2RST_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t struct mlx4_vhcr *vhcr,\n\t\t\t struct mlx4_cmd_mailbox *inbox,\n\t\t\t struct mlx4_cmd_mailbox *outbox,\n\t\t\t struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint qpn = vhcr->in_modifier & 0x7fffff;\n\tstruct res_qp *qp;\n\n\terr = qp_res_start_move_to(dev, slave, qpn, RES_QP_MAPPED, &qp, 0);\n\tif (err)\n\t\treturn err;\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tif (err)\n\t\tgoto ex_abort;\n\n\tatomic_dec(&qp->mtt->ref_count);\n\tatomic_dec(&qp->rcq->ref_count);\n\tatomic_dec(&qp->scq->ref_count);\n\tif (qp->srq)\n\t\tatomic_dec(&qp->srq->ref_count);\n\tres_end_move(dev, slave, RES_QP, qpn);\n\treturn 0;\n\nex_abort:\n\tres_abort_move(dev, slave, RES_QP, qpn);\n\n\treturn err;\n}\n\nstatic struct res_gid *find_gid(struct mlx4_dev *dev, int slave,\n\t\t\t\tstruct res_qp *rqp, u8 *gid)\n{\n\tstruct res_gid *res;\n\n\tlist_for_each_entry(res, &rqp->mcg_list, list) {\n\t\tif (!memcmp(res->gid, gid, 16))\n\t\t\treturn res;\n\t}\n\treturn NULL;\n}\n\nstatic int add_mcg_res(struct mlx4_dev *dev, int slave, struct res_qp *rqp,\n\t\t       u8 *gid, enum mlx4_protocol prot,\n\t\t       enum mlx4_steer_type steer, u64 reg_id)\n{\n\tstruct res_gid *res;\n\tint err;\n\n\tres = kzalloc(sizeof(*res), GFP_KERNEL);\n\tif (!res)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irq(&rqp->mcg_spl);\n\tif (find_gid(dev, slave, rqp, gid)) {\n\t\tkfree(res);\n\t\terr = -EEXIST;\n\t} else {\n\t\tmemcpy(res->gid, gid, 16);\n\t\tres->prot = prot;\n\t\tres->steer = steer;\n\t\tres->reg_id = reg_id;\n\t\tlist_add_tail(&res->list, &rqp->mcg_list);\n\t\terr = 0;\n\t}\n\tspin_unlock_irq(&rqp->mcg_spl);\n\n\treturn err;\n}\n\nstatic int rem_mcg_res(struct mlx4_dev *dev, int slave, struct res_qp *rqp,\n\t\t       u8 *gid, enum mlx4_protocol prot,\n\t\t       enum mlx4_steer_type steer, u64 *reg_id)\n{\n\tstruct res_gid *res;\n\tint err;\n\n\tspin_lock_irq(&rqp->mcg_spl);\n\tres = find_gid(dev, slave, rqp, gid);\n\tif (!res || res->prot != prot || res->steer != steer)\n\t\terr = -EINVAL;\n\telse {\n\t\t*reg_id = res->reg_id;\n\t\tlist_del(&res->list);\n\t\tkfree(res);\n\t\terr = 0;\n\t}\n\tspin_unlock_irq(&rqp->mcg_spl);\n\n\treturn err;\n}\n\nstatic int qp_attach(struct mlx4_dev *dev, int slave, struct mlx4_qp *qp,\n\t\t     u8 gid[16], int block_loopback, enum mlx4_protocol prot,\n\t\t     enum mlx4_steer_type type, u64 *reg_id)\n{\n\tswitch (dev->caps.steering_mode) {\n\tcase MLX4_STEERING_MODE_DEVICE_MANAGED: {\n\t\tint port = mlx4_slave_convert_port(dev, slave, gid[5]);\n\t\tif (port < 0)\n\t\t\treturn port;\n\t\treturn mlx4_trans_to_dmfs_attach(dev, qp, gid, port,\n\t\t\t\t\t\tblock_loopback, prot,\n\t\t\t\t\t\treg_id);\n\t}\n\tcase MLX4_STEERING_MODE_B0:\n\t\tif (prot == MLX4_PROT_ETH) {\n\t\t\tint port = mlx4_slave_convert_port(dev, slave, gid[5]);\n\t\t\tif (port < 0)\n\t\t\t\treturn port;\n\t\t\tgid[5] = port;\n\t\t}\n\t\treturn mlx4_qp_attach_common(dev, qp, gid,\n\t\t\t\t\t    block_loopback, prot, type);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int qp_detach(struct mlx4_dev *dev, struct mlx4_qp *qp,\n\t\t     u8 gid[16], enum mlx4_protocol prot,\n\t\t     enum mlx4_steer_type type, u64 reg_id)\n{\n\tswitch (dev->caps.steering_mode) {\n\tcase MLX4_STEERING_MODE_DEVICE_MANAGED:\n\t\treturn mlx4_flow_detach(dev, reg_id);\n\tcase MLX4_STEERING_MODE_B0:\n\t\treturn mlx4_qp_detach_common(dev, qp, gid, prot, type);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int mlx4_adjust_port(struct mlx4_dev *dev, int slave,\n\t\t\t    u8 *gid, enum mlx4_protocol prot)\n{\n\tint real_port;\n\n\tif (prot != MLX4_PROT_ETH)\n\t\treturn 0;\n\n\tif (dev->caps.steering_mode == MLX4_STEERING_MODE_B0 ||\n\t    dev->caps.steering_mode == MLX4_STEERING_MODE_DEVICE_MANAGED) {\n\t\treal_port = mlx4_slave_convert_port(dev, slave, gid[5]);\n\t\tif (real_port < 0)\n\t\t\treturn -EINVAL;\n\t\tgid[5] = real_port;\n\t}\n\n\treturn 0;\n}\n\nint mlx4_QP_ATTACH_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t       struct mlx4_vhcr *vhcr,\n\t\t\t       struct mlx4_cmd_mailbox *inbox,\n\t\t\t       struct mlx4_cmd_mailbox *outbox,\n\t\t\t       struct mlx4_cmd_info *cmd)\n{\n\tstruct mlx4_qp qp;  \n\tu8 *gid = inbox->buf;\n\tenum mlx4_protocol prot = (vhcr->in_modifier >> 28) & 0x7;\n\tint err;\n\tint qpn;\n\tstruct res_qp *rqp;\n\tu64 reg_id = 0;\n\tint attach = vhcr->op_modifier;\n\tint block_loopback = vhcr->in_modifier >> 31;\n\tu8 steer_type_mask = 2;\n\tenum mlx4_steer_type type = (gid[7] & steer_type_mask) >> 1;\n\n\tqpn = vhcr->in_modifier & 0xffffff;\n\terr = get_res(dev, slave, qpn, RES_QP, &rqp);\n\tif (err)\n\t\treturn err;\n\n\tqp.qpn = qpn;\n\tif (attach) {\n\t\terr = qp_attach(dev, slave, &qp, gid, block_loopback, prot,\n\t\t\t\ttype, &reg_id);\n\t\tif (err) {\n\t\t\tpr_err(\"Fail to attach rule to qp 0x%x\\n\", qpn);\n\t\t\tgoto ex_put;\n\t\t}\n\t\terr = add_mcg_res(dev, slave, rqp, gid, prot, type, reg_id);\n\t\tif (err)\n\t\t\tgoto ex_detach;\n\t} else {\n\t\terr = mlx4_adjust_port(dev, slave, gid, prot);\n\t\tif (err)\n\t\t\tgoto ex_put;\n\n\t\terr = rem_mcg_res(dev, slave, rqp, gid, prot, type, &reg_id);\n\t\tif (err)\n\t\t\tgoto ex_put;\n\n\t\terr = qp_detach(dev, &qp, gid, prot, type, reg_id);\n\t\tif (err)\n\t\t\tpr_err(\"Fail to detach rule from qp 0x%x reg_id = 0x%llx\\n\",\n\t\t\t       qpn, reg_id);\n\t}\n\tput_res(dev, slave, qpn, RES_QP);\n\treturn err;\n\nex_detach:\n\tqp_detach(dev, &qp, gid, prot, type, reg_id);\nex_put:\n\tput_res(dev, slave, qpn, RES_QP);\n\treturn err;\n}\n\n \nstatic int validate_eth_header_mac(int slave, struct _rule_hw *eth_header,\n\t\t\t\t   struct list_head *rlist)\n{\n\tstruct mac_res *res, *tmp;\n\t__be64 be_mac;\n\n\t \n\tif (!is_multicast_ether_addr(eth_header->eth.dst_mac) &&\n\t    !is_broadcast_ether_addr(eth_header->eth.dst_mac)) {\n\t\tlist_for_each_entry_safe(res, tmp, rlist, list) {\n\t\t\tbe_mac = cpu_to_be64(res->mac << 16);\n\t\t\tif (ether_addr_equal((u8 *)&be_mac, eth_header->eth.dst_mac))\n\t\t\t\treturn 0;\n\t\t}\n\t\tpr_err(\"MAC %pM doesn't belong to VF %d, Steering rule rejected\\n\",\n\t\t       eth_header->eth.dst_mac, slave);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nstatic int add_eth_header(struct mlx4_dev *dev, int slave,\n\t\t\t  struct mlx4_cmd_mailbox *inbox,\n\t\t\t  struct list_head *rlist, int header_id)\n{\n\tstruct mac_res *res, *tmp;\n\tu8 port;\n\tstruct mlx4_net_trans_rule_hw_ctrl *ctrl;\n\tstruct mlx4_net_trans_rule_hw_eth *eth_header;\n\tstruct mlx4_net_trans_rule_hw_ipv4 *ip_header;\n\tstruct mlx4_net_trans_rule_hw_tcp_udp *l4_header;\n\t__be64 be_mac = 0;\n\t__be64 mac_msk = cpu_to_be64(MLX4_MAC_MASK << 16);\n\n\tctrl = (struct mlx4_net_trans_rule_hw_ctrl *)inbox->buf;\n\tport = ctrl->port;\n\teth_header = (struct mlx4_net_trans_rule_hw_eth *)(ctrl + 1);\n\n\t \n\tswitch (header_id) {\n\tcase MLX4_NET_TRANS_RULE_ID_IPV4:\n\t\tip_header =\n\t\t\t(struct mlx4_net_trans_rule_hw_ipv4 *)(eth_header + 1);\n\t\tmemmove(ip_header, eth_header,\n\t\t\tsizeof(*ip_header) + sizeof(*l4_header));\n\t\tbreak;\n\tcase MLX4_NET_TRANS_RULE_ID_TCP:\n\tcase MLX4_NET_TRANS_RULE_ID_UDP:\n\t\tl4_header = (struct mlx4_net_trans_rule_hw_tcp_udp *)\n\t\t\t    (eth_header + 1);\n\t\tmemmove(l4_header, eth_header, sizeof(*l4_header));\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\tlist_for_each_entry_safe(res, tmp, rlist, list) {\n\t\tif (port == res->port) {\n\t\t\tbe_mac = cpu_to_be64(res->mac << 16);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!be_mac) {\n\t\tpr_err(\"Failed adding eth header to FS rule, Can't find matching MAC for port %d\\n\",\n\t\t       port);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(eth_header, 0, sizeof(*eth_header));\n\teth_header->size = sizeof(*eth_header) >> 2;\n\teth_header->id = cpu_to_be16(__sw_id_hw[MLX4_NET_TRANS_RULE_ID_ETH]);\n\tmemcpy(eth_header->dst_mac, &be_mac, ETH_ALEN);\n\tmemcpy(eth_header->dst_mac_msk, &mac_msk, ETH_ALEN);\n\n\treturn 0;\n\n}\n\n#define MLX4_UPD_QP_PATH_MASK_SUPPORTED      (                                \\\n\t1ULL << MLX4_UPD_QP_PATH_MASK_MAC_INDEX                     |\\\n\t1ULL << MLX4_UPD_QP_PATH_MASK_ETH_SRC_CHECK_MC_LB)\nint mlx4_UPDATE_QP_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t   struct mlx4_vhcr *vhcr,\n\t\t\t   struct mlx4_cmd_mailbox *inbox,\n\t\t\t   struct mlx4_cmd_mailbox *outbox,\n\t\t\t   struct mlx4_cmd_info *cmd_info)\n{\n\tint err;\n\tu32 qpn = vhcr->in_modifier & 0xffffff;\n\tstruct res_qp *rqp;\n\tu64 mac;\n\tunsigned port;\n\tu64 pri_addr_path_mask;\n\tstruct mlx4_update_qp_context *cmd;\n\tint smac_index;\n\n\tcmd = (struct mlx4_update_qp_context *)inbox->buf;\n\n\tpri_addr_path_mask = be64_to_cpu(cmd->primary_addr_path_mask);\n\tif (cmd->qp_mask || cmd->secondary_addr_path_mask ||\n\t    (pri_addr_path_mask & ~MLX4_UPD_QP_PATH_MASK_SUPPORTED))\n\t\treturn -EPERM;\n\n\tif ((pri_addr_path_mask &\n\t     (1ULL << MLX4_UPD_QP_PATH_MASK_ETH_SRC_CHECK_MC_LB)) &&\n\t\t!(dev->caps.flags2 &\n\t\t  MLX4_DEV_CAP_FLAG2_UPDATE_QP_SRC_CHECK_LB)) {\n\t\tmlx4_warn(dev, \"Src check LB for slave %d isn't supported\\n\",\n\t\t\t  slave);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\terr = get_res(dev, slave, qpn, RES_QP, &rqp);\n\tif (err) {\n\t\tmlx4_err(dev, \"Updating qpn 0x%x for slave %d rejected\\n\", qpn, slave);\n\t\treturn err;\n\t}\n\n\tport = (rqp->sched_queue >> 6 & 1) + 1;\n\n\tif (pri_addr_path_mask & (1ULL << MLX4_UPD_QP_PATH_MASK_MAC_INDEX)) {\n\t\tsmac_index = cmd->qp_context.pri_path.grh_mylmc;\n\t\terr = mac_find_smac_ix_in_slave(dev, slave, port,\n\t\t\t\t\t\tsmac_index, &mac);\n\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"Failed to update qpn 0x%x, MAC is invalid. smac_ix: %d\\n\",\n\t\t\t\t qpn, smac_index);\n\t\t\tgoto err_mac;\n\t\t}\n\t}\n\n\terr = mlx4_cmd(dev, inbox->dma,\n\t\t       vhcr->in_modifier, 0,\n\t\t       MLX4_CMD_UPDATE_QP, MLX4_CMD_TIME_CLASS_A,\n\t\t       MLX4_CMD_NATIVE);\n\tif (err) {\n\t\tmlx4_err(dev, \"Failed to update qpn on qpn 0x%x, command failed\\n\", qpn);\n\t\tgoto err_mac;\n\t}\n\nerr_mac:\n\tput_res(dev, slave, qpn, RES_QP);\n\treturn err;\n}\n\nstatic u32 qp_attach_mbox_size(void *mbox)\n{\n\tu32 size = sizeof(struct mlx4_net_trans_rule_hw_ctrl);\n\tstruct _rule_hw  *rule_header;\n\n\trule_header = (struct _rule_hw *)(mbox + size);\n\n\twhile (rule_header->size) {\n\t\tsize += rule_header->size * sizeof(u32);\n\t\trule_header += 1;\n\t}\n\treturn size;\n}\n\nstatic int mlx4_do_mirror_rule(struct mlx4_dev *dev, struct res_fs_rule *fs_rule);\n\nint mlx4_QP_FLOW_STEERING_ATTACH_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t\t\t struct mlx4_vhcr *vhcr,\n\t\t\t\t\t struct mlx4_cmd_mailbox *inbox,\n\t\t\t\t\t struct mlx4_cmd_mailbox *outbox,\n\t\t\t\t\t struct mlx4_cmd_info *cmd)\n{\n\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *rlist = &tracker->slave_list[slave].res_list[RES_MAC];\n\tint err;\n\tint qpn;\n\tstruct res_qp *rqp;\n\tstruct mlx4_net_trans_rule_hw_ctrl *ctrl;\n\tstruct _rule_hw  *rule_header;\n\tint header_id;\n\tstruct res_fs_rule *rrule;\n\tu32 mbox_size;\n\n\tif (dev->caps.steering_mode !=\n\t    MLX4_STEERING_MODE_DEVICE_MANAGED)\n\t\treturn -EOPNOTSUPP;\n\n\tctrl = (struct mlx4_net_trans_rule_hw_ctrl *)inbox->buf;\n\terr = mlx4_slave_convert_port(dev, slave, ctrl->port);\n\tif (err <= 0)\n\t\treturn -EINVAL;\n\tctrl->port = err;\n\tqpn = be32_to_cpu(ctrl->qpn) & 0xffffff;\n\terr = get_res(dev, slave, qpn, RES_QP, &rqp);\n\tif (err) {\n\t\tpr_err(\"Steering rule with qpn 0x%x rejected\\n\", qpn);\n\t\treturn err;\n\t}\n\trule_header = (struct _rule_hw *)(ctrl + 1);\n\theader_id = map_hw_to_sw_id(be16_to_cpu(rule_header->id));\n\n\tif (header_id == MLX4_NET_TRANS_RULE_ID_ETH)\n\t\tmlx4_handle_eth_header_mcast_prio(ctrl, rule_header);\n\n\tswitch (header_id) {\n\tcase MLX4_NET_TRANS_RULE_ID_ETH:\n\t\tif (validate_eth_header_mac(slave, rule_header, rlist)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_put_qp;\n\t\t}\n\t\tbreak;\n\tcase MLX4_NET_TRANS_RULE_ID_IB:\n\t\tbreak;\n\tcase MLX4_NET_TRANS_RULE_ID_IPV4:\n\tcase MLX4_NET_TRANS_RULE_ID_TCP:\n\tcase MLX4_NET_TRANS_RULE_ID_UDP:\n\t\tpr_warn(\"Can't attach FS rule without L2 headers, adding L2 header\\n\");\n\t\tif (add_eth_header(dev, slave, inbox, rlist, header_id)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_put_qp;\n\t\t}\n\t\tvhcr->in_modifier +=\n\t\t\tsizeof(struct mlx4_net_trans_rule_hw_eth) >> 2;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Corrupted mailbox\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_put_qp;\n\t}\n\n\terr = mlx4_cmd_imm(dev, inbox->dma, &vhcr->out_param,\n\t\t\t   vhcr->in_modifier, 0,\n\t\t\t   MLX4_QP_FLOW_STEERING_ATTACH, MLX4_CMD_TIME_CLASS_A,\n\t\t\t   MLX4_CMD_NATIVE);\n\tif (err)\n\t\tgoto err_put_qp;\n\n\n\terr = add_res_range(dev, slave, vhcr->out_param, 1, RES_FS_RULE, qpn);\n\tif (err) {\n\t\tmlx4_err(dev, \"Fail to add flow steering resources\\n\");\n\t\tgoto err_detach;\n\t}\n\n\terr = get_res(dev, slave, vhcr->out_param, RES_FS_RULE, &rrule);\n\tif (err)\n\t\tgoto err_detach;\n\n\tmbox_size = qp_attach_mbox_size(inbox->buf);\n\trrule->mirr_mbox = kmalloc(mbox_size, GFP_KERNEL);\n\tif (!rrule->mirr_mbox) {\n\t\terr = -ENOMEM;\n\t\tgoto err_put_rule;\n\t}\n\trrule->mirr_mbox_size = mbox_size;\n\trrule->mirr_rule_id = 0;\n\tmemcpy(rrule->mirr_mbox, inbox->buf, mbox_size);\n\n\t \n\tctrl = (struct mlx4_net_trans_rule_hw_ctrl *)rrule->mirr_mbox;\n\tif (ctrl->port == 1)\n\t\tctrl->port = 2;\n\telse\n\t\tctrl->port = 1;\n\n\tif (mlx4_is_bonded(dev))\n\t\tmlx4_do_mirror_rule(dev, rrule);\n\n\tatomic_inc(&rqp->ref_count);\n\nerr_put_rule:\n\tput_res(dev, slave, vhcr->out_param, RES_FS_RULE);\nerr_detach:\n\t \n\tif (err)\n\t\tmlx4_cmd(dev, vhcr->out_param, 0, 0,\n\t\t\t MLX4_QP_FLOW_STEERING_DETACH, MLX4_CMD_TIME_CLASS_A,\n\t\t\t MLX4_CMD_NATIVE);\nerr_put_qp:\n\tput_res(dev, slave, qpn, RES_QP);\n\treturn err;\n}\n\nstatic int mlx4_undo_mirror_rule(struct mlx4_dev *dev, struct res_fs_rule *fs_rule)\n{\n\tint err;\n\n\terr = rem_res_range(dev, fs_rule->com.owner, fs_rule->com.res_id, 1, RES_FS_RULE, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Fail to remove flow steering resources\\n\");\n\t\treturn err;\n\t}\n\n\tmlx4_cmd(dev, fs_rule->com.res_id, 0, 0, MLX4_QP_FLOW_STEERING_DETACH,\n\t\t MLX4_CMD_TIME_CLASS_A, MLX4_CMD_NATIVE);\n\treturn 0;\n}\n\nint mlx4_QP_FLOW_STEERING_DETACH_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t\t\t struct mlx4_vhcr *vhcr,\n\t\t\t\t\t struct mlx4_cmd_mailbox *inbox,\n\t\t\t\t\t struct mlx4_cmd_mailbox *outbox,\n\t\t\t\t\t struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tstruct res_qp *rqp;\n\tstruct res_fs_rule *rrule;\n\tu64 mirr_reg_id;\n\tint qpn;\n\n\tif (dev->caps.steering_mode !=\n\t    MLX4_STEERING_MODE_DEVICE_MANAGED)\n\t\treturn -EOPNOTSUPP;\n\n\terr = get_res(dev, slave, vhcr->in_param, RES_FS_RULE, &rrule);\n\tif (err)\n\t\treturn err;\n\n\tif (!rrule->mirr_mbox) {\n\t\tmlx4_err(dev, \"Mirror rules cannot be removed explicitly\\n\");\n\t\tput_res(dev, slave, vhcr->in_param, RES_FS_RULE);\n\t\treturn -EINVAL;\n\t}\n\tmirr_reg_id = rrule->mirr_rule_id;\n\tkfree(rrule->mirr_mbox);\n\tqpn = rrule->qpn;\n\n\t \n\tput_res(dev, slave, vhcr->in_param, RES_FS_RULE);\n\terr = get_res(dev, slave, qpn, RES_QP, &rqp);\n\tif (err)\n\t\treturn err;\n\n\tif (mirr_reg_id && mlx4_is_bonded(dev)) {\n\t\terr = get_res(dev, slave, mirr_reg_id, RES_FS_RULE, &rrule);\n\t\tif (err) {\n\t\t\tmlx4_err(dev, \"Fail to get resource of mirror rule\\n\");\n\t\t} else {\n\t\t\tput_res(dev, slave, mirr_reg_id, RES_FS_RULE);\n\t\t\tmlx4_undo_mirror_rule(dev, rrule);\n\t\t}\n\t}\n\terr = rem_res_range(dev, slave, vhcr->in_param, 1, RES_FS_RULE, 0);\n\tif (err) {\n\t\tmlx4_err(dev, \"Fail to remove flow steering resources\\n\");\n\t\tgoto out;\n\t}\n\n\terr = mlx4_cmd(dev, vhcr->in_param, 0, 0,\n\t\t       MLX4_QP_FLOW_STEERING_DETACH, MLX4_CMD_TIME_CLASS_A,\n\t\t       MLX4_CMD_NATIVE);\n\tif (!err)\n\t\tatomic_dec(&rqp->ref_count);\nout:\n\tput_res(dev, slave, qpn, RES_QP);\n\treturn err;\n}\n\nenum {\n\tBUSY_MAX_RETRIES = 10\n};\n\nint mlx4_QUERY_IF_STAT_wrapper(struct mlx4_dev *dev, int slave,\n\t\t\t       struct mlx4_vhcr *vhcr,\n\t\t\t       struct mlx4_cmd_mailbox *inbox,\n\t\t\t       struct mlx4_cmd_mailbox *outbox,\n\t\t\t       struct mlx4_cmd_info *cmd)\n{\n\tint err;\n\tint index = vhcr->in_modifier & 0xffff;\n\n\terr = get_res(dev, slave, index, RES_COUNTER, NULL);\n\tif (err)\n\t\treturn err;\n\n\terr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\n\tput_res(dev, slave, index, RES_COUNTER);\n\treturn err;\n}\n\nstatic void detach_qp(struct mlx4_dev *dev, int slave, struct res_qp *rqp)\n{\n\tstruct res_gid *rgid;\n\tstruct res_gid *tmp;\n\tstruct mlx4_qp qp;  \n\n\tlist_for_each_entry_safe(rgid, tmp, &rqp->mcg_list, list) {\n\t\tswitch (dev->caps.steering_mode) {\n\t\tcase MLX4_STEERING_MODE_DEVICE_MANAGED:\n\t\t\tmlx4_flow_detach(dev, rgid->reg_id);\n\t\t\tbreak;\n\t\tcase MLX4_STEERING_MODE_B0:\n\t\t\tqp.qpn = rqp->local_qpn;\n\t\t\t(void) mlx4_qp_detach_common(dev, &qp, rgid->gid,\n\t\t\t\t\t\t     rgid->prot, rgid->steer);\n\t\t\tbreak;\n\t\t}\n\t\tlist_del(&rgid->list);\n\t\tkfree(rgid);\n\t}\n}\n\nstatic int _move_all_busy(struct mlx4_dev *dev, int slave,\n\t\t\t  enum mlx4_resource type, int print)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker =\n\t\t&priv->mfunc.master.res_tracker;\n\tstruct list_head *rlist = &tracker->slave_list[slave].res_list[type];\n\tstruct res_common *r;\n\tstruct res_common *tmp;\n\tint busy;\n\n\tbusy = 0;\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(r, tmp, rlist, list) {\n\t\tif (r->owner == slave) {\n\t\t\tif (!r->removing) {\n\t\t\t\tif (r->state == RES_ANY_BUSY) {\n\t\t\t\t\tif (print)\n\t\t\t\t\t\tmlx4_dbg(dev,\n\t\t\t\t\t\t\t \"%s id 0x%llx is busy\\n\",\n\t\t\t\t\t\t\t  resource_str(type),\n\t\t\t\t\t\t\t  r->res_id);\n\t\t\t\t\t++busy;\n\t\t\t\t} else {\n\t\t\t\t\tr->from_state = r->state;\n\t\t\t\t\tr->state = RES_ANY_BUSY;\n\t\t\t\t\tr->removing = 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n\n\treturn busy;\n}\n\nstatic int move_all_busy(struct mlx4_dev *dev, int slave,\n\t\t\t enum mlx4_resource type)\n{\n\tunsigned long begin;\n\tint busy;\n\n\tbegin = jiffies;\n\tdo {\n\t\tbusy = _move_all_busy(dev, slave, type, 0);\n\t\tif (time_after(jiffies, begin + 5 * HZ))\n\t\t\tbreak;\n\t\tif (busy)\n\t\t\tcond_resched();\n\t} while (busy);\n\n\tif (busy)\n\t\tbusy = _move_all_busy(dev, slave, type, 1);\n\n\treturn busy;\n}\nstatic void rem_slave_qps(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *qp_list =\n\t\t&tracker->slave_list[slave].res_list[RES_QP];\n\tstruct res_qp *qp;\n\tstruct res_qp *tmp;\n\tint state;\n\tu64 in_param;\n\tint qpn;\n\tint err;\n\n\terr = move_all_busy(dev, slave, RES_QP);\n\tif (err)\n\t\tmlx4_warn(dev, \"rem_slave_qps: Could not move all qps to busy for slave %d\\n\",\n\t\t\t  slave);\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(qp, tmp, qp_list, com.list) {\n\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\tif (qp->com.owner == slave) {\n\t\t\tqpn = qp->com.res_id;\n\t\t\tdetach_qp(dev, slave, qp);\n\t\t\tstate = qp->com.from_state;\n\t\t\twhile (state != 0) {\n\t\t\t\tswitch (state) {\n\t\t\t\tcase RES_QP_RESERVED:\n\t\t\t\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\t\t\t\trb_erase(&qp->com.node,\n\t\t\t\t\t\t &tracker->res_tree[RES_QP]);\n\t\t\t\t\tlist_del(&qp->com.list);\n\t\t\t\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\t\t\t\tif (!valid_reserved(dev, slave, qpn)) {\n\t\t\t\t\t\t__mlx4_qp_release_range(dev, qpn, 1);\n\t\t\t\t\t\tmlx4_release_resource(dev, slave,\n\t\t\t\t\t\t\t\t      RES_QP, 1, 0);\n\t\t\t\t\t}\n\t\t\t\t\tkfree(qp);\n\t\t\t\t\tstate = 0;\n\t\t\t\t\tbreak;\n\t\t\t\tcase RES_QP_MAPPED:\n\t\t\t\t\tif (!valid_reserved(dev, slave, qpn))\n\t\t\t\t\t\t__mlx4_qp_free_icm(dev, qpn);\n\t\t\t\t\tstate = RES_QP_RESERVED;\n\t\t\t\t\tbreak;\n\t\t\t\tcase RES_QP_HW:\n\t\t\t\t\tin_param = slave;\n\t\t\t\t\terr = mlx4_cmd(dev, in_param,\n\t\t\t\t\t\t       qp->local_qpn, 2,\n\t\t\t\t\t\t       MLX4_CMD_2RST_QP,\n\t\t\t\t\t\t       MLX4_CMD_TIME_CLASS_A,\n\t\t\t\t\t\t       MLX4_CMD_NATIVE);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\tmlx4_dbg(dev, \"rem_slave_qps: failed to move slave %d qpn %d to reset\\n\",\n\t\t\t\t\t\t\t slave, qp->local_qpn);\n\t\t\t\t\tatomic_dec(&qp->rcq->ref_count);\n\t\t\t\t\tatomic_dec(&qp->scq->ref_count);\n\t\t\t\t\tatomic_dec(&qp->mtt->ref_count);\n\t\t\t\t\tif (qp->srq)\n\t\t\t\t\t\tatomic_dec(&qp->srq->ref_count);\n\t\t\t\t\tstate = RES_QP_MAPPED;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tstate = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(mlx4_tlock(dev));\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic void rem_slave_srqs(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *srq_list =\n\t\t&tracker->slave_list[slave].res_list[RES_SRQ];\n\tstruct res_srq *srq;\n\tstruct res_srq *tmp;\n\tint state;\n\tu64 in_param;\n\tint srqn;\n\tint err;\n\n\terr = move_all_busy(dev, slave, RES_SRQ);\n\tif (err)\n\t\tmlx4_warn(dev, \"rem_slave_srqs: Could not move all srqs - too busy for slave %d\\n\",\n\t\t\t  slave);\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(srq, tmp, srq_list, com.list) {\n\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\tif (srq->com.owner == slave) {\n\t\t\tsrqn = srq->com.res_id;\n\t\t\tstate = srq->com.from_state;\n\t\t\twhile (state != 0) {\n\t\t\t\tswitch (state) {\n\t\t\t\tcase RES_SRQ_ALLOCATED:\n\t\t\t\t\t__mlx4_srq_free_icm(dev, srqn);\n\t\t\t\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\t\t\t\trb_erase(&srq->com.node,\n\t\t\t\t\t\t &tracker->res_tree[RES_SRQ]);\n\t\t\t\t\tlist_del(&srq->com.list);\n\t\t\t\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\t\t\t\tmlx4_release_resource(dev, slave,\n\t\t\t\t\t\t\t      RES_SRQ, 1, 0);\n\t\t\t\t\tkfree(srq);\n\t\t\t\t\tstate = 0;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase RES_SRQ_HW:\n\t\t\t\t\tin_param = slave;\n\t\t\t\t\terr = mlx4_cmd(dev, in_param, srqn, 1,\n\t\t\t\t\t\t       MLX4_CMD_HW2SW_SRQ,\n\t\t\t\t\t\t       MLX4_CMD_TIME_CLASS_A,\n\t\t\t\t\t\t       MLX4_CMD_NATIVE);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\tmlx4_dbg(dev, \"rem_slave_srqs: failed to move slave %d srq %d to SW ownership\\n\",\n\t\t\t\t\t\t\t slave, srqn);\n\n\t\t\t\t\tatomic_dec(&srq->mtt->ref_count);\n\t\t\t\t\tif (srq->cq)\n\t\t\t\t\t\tatomic_dec(&srq->cq->ref_count);\n\t\t\t\t\tstate = RES_SRQ_ALLOCATED;\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tstate = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(mlx4_tlock(dev));\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic void rem_slave_cqs(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *cq_list =\n\t\t&tracker->slave_list[slave].res_list[RES_CQ];\n\tstruct res_cq *cq;\n\tstruct res_cq *tmp;\n\tint state;\n\tu64 in_param;\n\tint cqn;\n\tint err;\n\n\terr = move_all_busy(dev, slave, RES_CQ);\n\tif (err)\n\t\tmlx4_warn(dev, \"rem_slave_cqs: Could not move all cqs - too busy for slave %d\\n\",\n\t\t\t  slave);\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(cq, tmp, cq_list, com.list) {\n\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\tif (cq->com.owner == slave && !atomic_read(&cq->ref_count)) {\n\t\t\tcqn = cq->com.res_id;\n\t\t\tstate = cq->com.from_state;\n\t\t\twhile (state != 0) {\n\t\t\t\tswitch (state) {\n\t\t\t\tcase RES_CQ_ALLOCATED:\n\t\t\t\t\t__mlx4_cq_free_icm(dev, cqn);\n\t\t\t\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\t\t\t\trb_erase(&cq->com.node,\n\t\t\t\t\t\t &tracker->res_tree[RES_CQ]);\n\t\t\t\t\tlist_del(&cq->com.list);\n\t\t\t\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\t\t\t\tmlx4_release_resource(dev, slave,\n\t\t\t\t\t\t\t      RES_CQ, 1, 0);\n\t\t\t\t\tkfree(cq);\n\t\t\t\t\tstate = 0;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase RES_CQ_HW:\n\t\t\t\t\tin_param = slave;\n\t\t\t\t\terr = mlx4_cmd(dev, in_param, cqn, 1,\n\t\t\t\t\t\t       MLX4_CMD_HW2SW_CQ,\n\t\t\t\t\t\t       MLX4_CMD_TIME_CLASS_A,\n\t\t\t\t\t\t       MLX4_CMD_NATIVE);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\tmlx4_dbg(dev, \"rem_slave_cqs: failed to move slave %d cq %d to SW ownership\\n\",\n\t\t\t\t\t\t\t slave, cqn);\n\t\t\t\t\tatomic_dec(&cq->mtt->ref_count);\n\t\t\t\t\tstate = RES_CQ_ALLOCATED;\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tstate = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(mlx4_tlock(dev));\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic void rem_slave_mrs(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *mpt_list =\n\t\t&tracker->slave_list[slave].res_list[RES_MPT];\n\tstruct res_mpt *mpt;\n\tstruct res_mpt *tmp;\n\tint state;\n\tu64 in_param;\n\tint mptn;\n\tint err;\n\n\terr = move_all_busy(dev, slave, RES_MPT);\n\tif (err)\n\t\tmlx4_warn(dev, \"rem_slave_mrs: Could not move all mpts - too busy for slave %d\\n\",\n\t\t\t  slave);\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(mpt, tmp, mpt_list, com.list) {\n\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\tif (mpt->com.owner == slave) {\n\t\t\tmptn = mpt->com.res_id;\n\t\t\tstate = mpt->com.from_state;\n\t\t\twhile (state != 0) {\n\t\t\t\tswitch (state) {\n\t\t\t\tcase RES_MPT_RESERVED:\n\t\t\t\t\t__mlx4_mpt_release(dev, mpt->key);\n\t\t\t\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\t\t\t\trb_erase(&mpt->com.node,\n\t\t\t\t\t\t &tracker->res_tree[RES_MPT]);\n\t\t\t\t\tlist_del(&mpt->com.list);\n\t\t\t\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\t\t\t\tmlx4_release_resource(dev, slave,\n\t\t\t\t\t\t\t      RES_MPT, 1, 0);\n\t\t\t\t\tkfree(mpt);\n\t\t\t\t\tstate = 0;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase RES_MPT_MAPPED:\n\t\t\t\t\t__mlx4_mpt_free_icm(dev, mpt->key);\n\t\t\t\t\tstate = RES_MPT_RESERVED;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase RES_MPT_HW:\n\t\t\t\t\tin_param = slave;\n\t\t\t\t\terr = mlx4_cmd(dev, in_param, mptn, 0,\n\t\t\t\t\t\t     MLX4_CMD_HW2SW_MPT,\n\t\t\t\t\t\t     MLX4_CMD_TIME_CLASS_A,\n\t\t\t\t\t\t     MLX4_CMD_NATIVE);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\tmlx4_dbg(dev, \"rem_slave_mrs: failed to move slave %d mpt %d to SW ownership\\n\",\n\t\t\t\t\t\t\t slave, mptn);\n\t\t\t\t\tif (mpt->mtt)\n\t\t\t\t\t\tatomic_dec(&mpt->mtt->ref_count);\n\t\t\t\t\tstate = RES_MPT_MAPPED;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tstate = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(mlx4_tlock(dev));\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic void rem_slave_mtts(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker =\n\t\t&priv->mfunc.master.res_tracker;\n\tstruct list_head *mtt_list =\n\t\t&tracker->slave_list[slave].res_list[RES_MTT];\n\tstruct res_mtt *mtt;\n\tstruct res_mtt *tmp;\n\tint state;\n\tint base;\n\tint err;\n\n\terr = move_all_busy(dev, slave, RES_MTT);\n\tif (err)\n\t\tmlx4_warn(dev, \"rem_slave_mtts: Could not move all mtts  - too busy for slave %d\\n\",\n\t\t\t  slave);\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(mtt, tmp, mtt_list, com.list) {\n\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\tif (mtt->com.owner == slave) {\n\t\t\tbase = mtt->com.res_id;\n\t\t\tstate = mtt->com.from_state;\n\t\t\twhile (state != 0) {\n\t\t\t\tswitch (state) {\n\t\t\t\tcase RES_MTT_ALLOCATED:\n\t\t\t\t\t__mlx4_free_mtt_range(dev, base,\n\t\t\t\t\t\t\t      mtt->order);\n\t\t\t\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\t\t\t\trb_erase(&mtt->com.node,\n\t\t\t\t\t\t &tracker->res_tree[RES_MTT]);\n\t\t\t\t\tlist_del(&mtt->com.list);\n\t\t\t\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\t\t\t\tmlx4_release_resource(dev, slave, RES_MTT,\n\t\t\t\t\t\t\t      1 << mtt->order, 0);\n\t\t\t\t\tkfree(mtt);\n\t\t\t\t\tstate = 0;\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tstate = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(mlx4_tlock(dev));\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic int mlx4_do_mirror_rule(struct mlx4_dev *dev, struct res_fs_rule *fs_rule)\n{\n\tstruct mlx4_cmd_mailbox *mailbox;\n\tint err;\n\tstruct res_fs_rule *mirr_rule;\n\tu64 reg_id;\n\n\tmailbox = mlx4_alloc_cmd_mailbox(dev);\n\tif (IS_ERR(mailbox))\n\t\treturn PTR_ERR(mailbox);\n\n\tif (!fs_rule->mirr_mbox) {\n\t\tmlx4_err(dev, \"rule mirroring mailbox is null\\n\");\n\t\tmlx4_free_cmd_mailbox(dev, mailbox);\n\t\treturn -EINVAL;\n\t}\n\tmemcpy(mailbox->buf, fs_rule->mirr_mbox, fs_rule->mirr_mbox_size);\n\terr = mlx4_cmd_imm(dev, mailbox->dma, &reg_id, fs_rule->mirr_mbox_size >> 2, 0,\n\t\t\t   MLX4_QP_FLOW_STEERING_ATTACH, MLX4_CMD_TIME_CLASS_A,\n\t\t\t   MLX4_CMD_NATIVE);\n\tmlx4_free_cmd_mailbox(dev, mailbox);\n\n\tif (err)\n\t\tgoto err;\n\n\terr = add_res_range(dev, fs_rule->com.owner, reg_id, 1, RES_FS_RULE, fs_rule->qpn);\n\tif (err)\n\t\tgoto err_detach;\n\n\terr = get_res(dev, fs_rule->com.owner, reg_id, RES_FS_RULE, &mirr_rule);\n\tif (err)\n\t\tgoto err_rem;\n\n\tfs_rule->mirr_rule_id = reg_id;\n\tmirr_rule->mirr_rule_id = 0;\n\tmirr_rule->mirr_mbox_size = 0;\n\tmirr_rule->mirr_mbox = NULL;\n\tput_res(dev, fs_rule->com.owner, reg_id, RES_FS_RULE);\n\n\treturn 0;\nerr_rem:\n\trem_res_range(dev, fs_rule->com.owner, reg_id, 1, RES_FS_RULE, 0);\nerr_detach:\n\tmlx4_cmd(dev, reg_id, 0, 0, MLX4_QP_FLOW_STEERING_DETACH,\n\t\t MLX4_CMD_TIME_CLASS_A, MLX4_CMD_NATIVE);\nerr:\n\treturn err;\n}\n\nstatic int mlx4_mirror_fs_rules(struct mlx4_dev *dev, bool bond)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker =\n\t\t&priv->mfunc.master.res_tracker;\n\tstruct rb_root *root = &tracker->res_tree[RES_FS_RULE];\n\tstruct rb_node *p;\n\tstruct res_fs_rule *fs_rule;\n\tint err = 0;\n\tLIST_HEAD(mirr_list);\n\n\tfor (p = rb_first(root); p; p = rb_next(p)) {\n\t\tfs_rule = rb_entry(p, struct res_fs_rule, com.node);\n\t\tif ((bond && fs_rule->mirr_mbox_size) ||\n\t\t    (!bond && !fs_rule->mirr_mbox_size))\n\t\t\tlist_add_tail(&fs_rule->mirr_list, &mirr_list);\n\t}\n\n\tlist_for_each_entry(fs_rule, &mirr_list, mirr_list) {\n\t\tif (bond)\n\t\t\terr += mlx4_do_mirror_rule(dev, fs_rule);\n\t\telse\n\t\t\terr += mlx4_undo_mirror_rule(dev, fs_rule);\n\t}\n\treturn err;\n}\n\nint mlx4_bond_fs_rules(struct mlx4_dev *dev)\n{\n\treturn mlx4_mirror_fs_rules(dev, true);\n}\n\nint mlx4_unbond_fs_rules(struct mlx4_dev *dev)\n{\n\treturn mlx4_mirror_fs_rules(dev, false);\n}\n\nstatic void rem_slave_fs_rule(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker =\n\t\t&priv->mfunc.master.res_tracker;\n\tstruct list_head *fs_rule_list =\n\t\t&tracker->slave_list[slave].res_list[RES_FS_RULE];\n\tstruct res_fs_rule *fs_rule;\n\tstruct res_fs_rule *tmp;\n\tint state;\n\tu64 base;\n\tint err;\n\n\terr = move_all_busy(dev, slave, RES_FS_RULE);\n\tif (err)\n\t\tmlx4_warn(dev, \"rem_slave_fs_rule: Could not move all mtts to busy for slave %d\\n\",\n\t\t\t  slave);\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(fs_rule, tmp, fs_rule_list, com.list) {\n\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\tif (fs_rule->com.owner == slave) {\n\t\t\tbase = fs_rule->com.res_id;\n\t\t\tstate = fs_rule->com.from_state;\n\t\t\twhile (state != 0) {\n\t\t\t\tswitch (state) {\n\t\t\t\tcase RES_FS_RULE_ALLOCATED:\n\t\t\t\t\t \n\t\t\t\t\terr = mlx4_cmd(dev, base, 0, 0,\n\t\t\t\t\t\t       MLX4_QP_FLOW_STEERING_DETACH,\n\t\t\t\t\t\t       MLX4_CMD_TIME_CLASS_A,\n\t\t\t\t\t\t       MLX4_CMD_NATIVE);\n\n\t\t\t\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\t\t\t\trb_erase(&fs_rule->com.node,\n\t\t\t\t\t\t &tracker->res_tree[RES_FS_RULE]);\n\t\t\t\t\tlist_del(&fs_rule->com.list);\n\t\t\t\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\t\t\t\tkfree(fs_rule->mirr_mbox);\n\t\t\t\t\tkfree(fs_rule);\n\t\t\t\t\tstate = 0;\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tstate = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(mlx4_tlock(dev));\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic void rem_slave_eqs(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *eq_list =\n\t\t&tracker->slave_list[slave].res_list[RES_EQ];\n\tstruct res_eq *eq;\n\tstruct res_eq *tmp;\n\tint err;\n\tint state;\n\tint eqn;\n\n\terr = move_all_busy(dev, slave, RES_EQ);\n\tif (err)\n\t\tmlx4_warn(dev, \"rem_slave_eqs: Could not move all eqs - too busy for slave %d\\n\",\n\t\t\t  slave);\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(eq, tmp, eq_list, com.list) {\n\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\tif (eq->com.owner == slave) {\n\t\t\teqn = eq->com.res_id;\n\t\t\tstate = eq->com.from_state;\n\t\t\twhile (state != 0) {\n\t\t\t\tswitch (state) {\n\t\t\t\tcase RES_EQ_RESERVED:\n\t\t\t\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\t\t\t\trb_erase(&eq->com.node,\n\t\t\t\t\t\t &tracker->res_tree[RES_EQ]);\n\t\t\t\t\tlist_del(&eq->com.list);\n\t\t\t\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\t\t\t\tkfree(eq);\n\t\t\t\t\tstate = 0;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase RES_EQ_HW:\n\t\t\t\t\terr = mlx4_cmd(dev, slave, eqn & 0x3ff,\n\t\t\t\t\t\t       1, MLX4_CMD_HW2SW_EQ,\n\t\t\t\t\t\t       MLX4_CMD_TIME_CLASS_A,\n\t\t\t\t\t\t       MLX4_CMD_NATIVE);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\tmlx4_dbg(dev, \"rem_slave_eqs: failed to move slave %d eqs %d to SW ownership\\n\",\n\t\t\t\t\t\t\t slave, eqn & 0x3ff);\n\t\t\t\t\tatomic_dec(&eq->mtt->ref_count);\n\t\t\t\t\tstate = RES_EQ_RESERVED;\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tstate = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(mlx4_tlock(dev));\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nstatic void rem_slave_counters(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *counter_list =\n\t\t&tracker->slave_list[slave].res_list[RES_COUNTER];\n\tstruct res_counter *counter;\n\tstruct res_counter *tmp;\n\tint err;\n\tint *counters_arr = NULL;\n\tint i, j;\n\n\terr = move_all_busy(dev, slave, RES_COUNTER);\n\tif (err)\n\t\tmlx4_warn(dev, \"rem_slave_counters: Could not move all counters - too busy for slave %d\\n\",\n\t\t\t  slave);\n\n\tcounters_arr = kmalloc_array(dev->caps.max_counters,\n\t\t\t\t     sizeof(*counters_arr), GFP_KERNEL);\n\tif (!counters_arr)\n\t\treturn;\n\n\tdo {\n\t\ti = 0;\n\t\tj = 0;\n\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\tlist_for_each_entry_safe(counter, tmp, counter_list, com.list) {\n\t\t\tif (counter->com.owner == slave) {\n\t\t\t\tcounters_arr[i++] = counter->com.res_id;\n\t\t\t\trb_erase(&counter->com.node,\n\t\t\t\t\t &tracker->res_tree[RES_COUNTER]);\n\t\t\t\tlist_del(&counter->com.list);\n\t\t\t\tkfree(counter);\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(mlx4_tlock(dev));\n\n\t\twhile (j < i) {\n\t\t\t__mlx4_counter_free(dev, counters_arr[j++]);\n\t\t\tmlx4_release_resource(dev, slave, RES_COUNTER, 1, 0);\n\t\t}\n\t} while (i);\n\n\tkfree(counters_arr);\n}\n\nstatic void rem_slave_xrcdns(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\n\tstruct list_head *xrcdn_list =\n\t\t&tracker->slave_list[slave].res_list[RES_XRCD];\n\tstruct res_xrcdn *xrcd;\n\tstruct res_xrcdn *tmp;\n\tint err;\n\tint xrcdn;\n\n\terr = move_all_busy(dev, slave, RES_XRCD);\n\tif (err)\n\t\tmlx4_warn(dev, \"rem_slave_xrcdns: Could not move all xrcdns - too busy for slave %d\\n\",\n\t\t\t  slave);\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(xrcd, tmp, xrcdn_list, com.list) {\n\t\tif (xrcd->com.owner == slave) {\n\t\t\txrcdn = xrcd->com.res_id;\n\t\t\trb_erase(&xrcd->com.node, &tracker->res_tree[RES_XRCD]);\n\t\t\tlist_del(&xrcd->com.list);\n\t\t\tkfree(xrcd);\n\t\t\t__mlx4_xrcd_free(dev, xrcdn);\n\t\t}\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n}\n\nvoid mlx4_delete_all_resources_for_slave(struct mlx4_dev *dev, int slave)\n{\n\tstruct mlx4_priv *priv = mlx4_priv(dev);\n\tmlx4_reset_roce_gids(dev, slave);\n\tmutex_lock(&priv->mfunc.master.res_tracker.slave_list[slave].mutex);\n\trem_slave_vlans(dev, slave);\n\trem_slave_macs(dev, slave);\n\trem_slave_fs_rule(dev, slave);\n\trem_slave_qps(dev, slave);\n\trem_slave_srqs(dev, slave);\n\trem_slave_cqs(dev, slave);\n\trem_slave_mrs(dev, slave);\n\trem_slave_eqs(dev, slave);\n\trem_slave_mtts(dev, slave);\n\trem_slave_counters(dev, slave);\n\trem_slave_xrcdns(dev, slave);\n\tmutex_unlock(&priv->mfunc.master.res_tracker.slave_list[slave].mutex);\n}\n\nstatic void update_qos_vpp(struct mlx4_update_qp_context *ctx,\n\t\t\t   struct mlx4_vf_immed_vlan_work *work)\n{\n\tctx->qp_mask |= cpu_to_be64(1ULL << MLX4_UPD_QP_MASK_QOS_VPP);\n\tctx->qp_context.qos_vport = work->qos_vport;\n}\n\nvoid mlx4_vf_immed_vlan_work_handler(struct work_struct *_work)\n{\n\tstruct mlx4_vf_immed_vlan_work *work =\n\t\tcontainer_of(_work, struct mlx4_vf_immed_vlan_work, work);\n\tstruct mlx4_cmd_mailbox *mailbox;\n\tstruct mlx4_update_qp_context *upd_context;\n\tstruct mlx4_dev *dev = &work->priv->dev;\n\tstruct mlx4_resource_tracker *tracker =\n\t\t&work->priv->mfunc.master.res_tracker;\n\tstruct list_head *qp_list =\n\t\t&tracker->slave_list[work->slave].res_list[RES_QP];\n\tstruct res_qp *qp;\n\tstruct res_qp *tmp;\n\tu64 qp_path_mask_vlan_ctrl =\n\t\t       ((1ULL << MLX4_UPD_QP_PATH_MASK_ETH_TX_BLOCK_UNTAGGED) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_ETH_TX_BLOCK_1P) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_ETH_TX_BLOCK_TAGGED) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_ETH_RX_BLOCK_UNTAGGED) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_ETH_RX_BLOCK_1P) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_ETH_RX_BLOCK_TAGGED));\n\n\tu64 qp_path_mask = ((1ULL << MLX4_UPD_QP_PATH_MASK_VLAN_INDEX) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_FVL) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_CV) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_SV) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_ETH_HIDE_CQE_VLAN) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_FEUP) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_FVL_RX) |\n\t\t       (1ULL << MLX4_UPD_QP_PATH_MASK_SCHED_QUEUE));\n\n\tint err;\n\tint port, errors = 0;\n\tu8 vlan_control;\n\n\tif (mlx4_is_slave(dev)) {\n\t\tmlx4_warn(dev, \"Trying to update-qp in slave %d\\n\",\n\t\t\t  work->slave);\n\t\tgoto out;\n\t}\n\n\tmailbox = mlx4_alloc_cmd_mailbox(dev);\n\tif (IS_ERR(mailbox))\n\t\tgoto out;\n\tif (work->flags & MLX4_VF_IMMED_VLAN_FLAG_LINK_DISABLE)  \n\t\tvlan_control = MLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_PRIO_TAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_UNTAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_TAGGED;\n\telse if (!work->vlan_id)\n\t\tvlan_control = MLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_TAGGED;\n\telse if (work->vlan_proto == htons(ETH_P_8021AD))\n\t\tvlan_control = MLX4_VLAN_CTRL_ETH_TX_BLOCK_PRIO_TAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED;\n\telse   \n\t\tvlan_control = MLX4_VLAN_CTRL_ETH_TX_BLOCK_TAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_PRIO_TAGGED |\n\t\t\tMLX4_VLAN_CTRL_ETH_RX_BLOCK_UNTAGGED;\n\n\tupd_context = mailbox->buf;\n\tupd_context->qp_mask = cpu_to_be64(1ULL << MLX4_UPD_QP_MASK_VSD);\n\n\tspin_lock_irq(mlx4_tlock(dev));\n\tlist_for_each_entry_safe(qp, tmp, qp_list, com.list) {\n\t\tspin_unlock_irq(mlx4_tlock(dev));\n\t\tif (qp->com.owner == work->slave) {\n\t\t\tif (qp->com.from_state != RES_QP_HW ||\n\t\t\t    !qp->sched_queue ||   \n\t\t\t    mlx4_is_qp_reserved(dev, qp->local_qpn) ||\n\t\t\t    qp->qpc_flags & (1 << MLX4_RSS_QPC_FLAG_OFFSET)) {\n\t\t\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tport = (qp->sched_queue >> 6 & 1) + 1;\n\t\t\tif (port != work->port) {\n\t\t\t\tspin_lock_irq(mlx4_tlock(dev));\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (MLX4_QP_ST_RC == ((qp->qpc_flags >> 16) & 0xff))\n\t\t\t\tupd_context->primary_addr_path_mask = cpu_to_be64(qp_path_mask);\n\t\t\telse\n\t\t\t\tupd_context->primary_addr_path_mask =\n\t\t\t\t\tcpu_to_be64(qp_path_mask | qp_path_mask_vlan_ctrl);\n\t\t\tif (work->vlan_id == MLX4_VGT) {\n\t\t\t\tupd_context->qp_context.param3 = qp->param3;\n\t\t\t\tupd_context->qp_context.pri_path.vlan_control = qp->vlan_control;\n\t\t\t\tupd_context->qp_context.pri_path.fvl_rx = qp->fvl_rx;\n\t\t\t\tupd_context->qp_context.pri_path.vlan_index = qp->vlan_index;\n\t\t\t\tupd_context->qp_context.pri_path.fl = qp->pri_path_fl;\n\t\t\t\tupd_context->qp_context.pri_path.feup = qp->feup;\n\t\t\t\tupd_context->qp_context.pri_path.sched_queue =\n\t\t\t\t\tqp->sched_queue;\n\t\t\t} else {\n\t\t\t\tupd_context->qp_context.param3 = qp->param3 & ~cpu_to_be32(MLX4_STRIP_VLAN);\n\t\t\t\tupd_context->qp_context.pri_path.vlan_control = vlan_control;\n\t\t\t\tupd_context->qp_context.pri_path.vlan_index = work->vlan_ix;\n\t\t\t\tupd_context->qp_context.pri_path.fvl_rx =\n\t\t\t\t\tqp->fvl_rx | MLX4_FVL_RX_FORCE_ETH_VLAN;\n\t\t\t\tupd_context->qp_context.pri_path.fl =\n\t\t\t\t\tqp->pri_path_fl | MLX4_FL_ETH_HIDE_CQE_VLAN;\n\t\t\t\tif (work->vlan_proto == htons(ETH_P_8021AD))\n\t\t\t\t\tupd_context->qp_context.pri_path.fl |= MLX4_FL_SV;\n\t\t\t\telse\n\t\t\t\t\tupd_context->qp_context.pri_path.fl |= MLX4_FL_CV;\n\t\t\t\tupd_context->qp_context.pri_path.feup =\n\t\t\t\t\tqp->feup | MLX4_FEUP_FORCE_ETH_UP | MLX4_FVL_FORCE_ETH_VLAN;\n\t\t\t\tupd_context->qp_context.pri_path.sched_queue =\n\t\t\t\t\tqp->sched_queue & 0xC7;\n\t\t\t\tupd_context->qp_context.pri_path.sched_queue |=\n\t\t\t\t\t((work->qos & 0x7) << 3);\n\n\t\t\t\tif (dev->caps.flags2 &\n\t\t\t\t    MLX4_DEV_CAP_FLAG2_QOS_VPP)\n\t\t\t\t\tupdate_qos_vpp(upd_context, work);\n\t\t\t}\n\n\t\t\terr = mlx4_cmd(dev, mailbox->dma,\n\t\t\t\t       qp->local_qpn & 0xffffff,\n\t\t\t\t       0, MLX4_CMD_UPDATE_QP,\n\t\t\t\t       MLX4_CMD_TIME_CLASS_C, MLX4_CMD_NATIVE);\n\t\t\tif (err) {\n\t\t\t\tmlx4_info(dev, \"UPDATE_QP failed for slave %d, port %d, qpn %d (%d)\\n\",\n\t\t\t\t\t  work->slave, port, qp->local_qpn, err);\n\t\t\t\terrors++;\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(mlx4_tlock(dev));\n\t}\n\tspin_unlock_irq(mlx4_tlock(dev));\n\tmlx4_free_cmd_mailbox(dev, mailbox);\n\n\tif (errors)\n\t\tmlx4_err(dev, \"%d UPDATE_QP failures for slave %d, port %d\\n\",\n\t\t\t errors, work->slave, work->port);\n\n\t \n\tif (work->flags & MLX4_VF_IMMED_VLAN_FLAG_VLAN && !errors &&\n\t    NO_INDX != work->orig_vlan_ix)\n\t\t__mlx4_unregister_vlan(&work->priv->dev, work->port,\n\t\t\t\t       work->orig_vlan_id);\nout:\n\tkfree(work);\n\treturn;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}