{
  "module_name": "en_rx.c",
  "hash_id": "dcf40d1401b4820449341d5a3676d63bb2de5ce97695468f56d69483c5832d67",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mellanox/mlx4/en_rx.c",
  "human_readable_source": " \n\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <linux/mlx4/cq.h>\n#include <linux/slab.h>\n#include <linux/mlx4/qp.h>\n#include <linux/skbuff.h>\n#include <linux/rculist.h>\n#include <linux/if_ether.h>\n#include <linux/if_vlan.h>\n#include <linux/vmalloc.h>\n#include <linux/irq.h>\n\n#include <net/ip.h>\n#if IS_ENABLED(CONFIG_IPV6)\n#include <net/ip6_checksum.h>\n#endif\n\n#include \"mlx4_en.h\"\n\nstatic int mlx4_alloc_page(struct mlx4_en_priv *priv,\n\t\t\t   struct mlx4_en_rx_alloc *frag,\n\t\t\t   gfp_t gfp)\n{\n\tstruct page *page;\n\tdma_addr_t dma;\n\n\tpage = alloc_page(gfp);\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\tdma = dma_map_page(priv->ddev, page, 0, PAGE_SIZE, priv->dma_dir);\n\tif (unlikely(dma_mapping_error(priv->ddev, dma))) {\n\t\t__free_page(page);\n\t\treturn -ENOMEM;\n\t}\n\tfrag->page = page;\n\tfrag->dma = dma;\n\tfrag->page_offset = priv->rx_headroom;\n\treturn 0;\n}\n\nstatic int mlx4_en_alloc_frags(struct mlx4_en_priv *priv,\n\t\t\t       struct mlx4_en_rx_ring *ring,\n\t\t\t       struct mlx4_en_rx_desc *rx_desc,\n\t\t\t       struct mlx4_en_rx_alloc *frags,\n\t\t\t       gfp_t gfp)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->num_frags; i++, frags++) {\n\t\tif (!frags->page) {\n\t\t\tif (mlx4_alloc_page(priv, frags, gfp))\n\t\t\t\treturn -ENOMEM;\n\t\t\tring->rx_alloc_pages++;\n\t\t}\n\t\trx_desc->data[i].addr = cpu_to_be64(frags->dma +\n\t\t\t\t\t\t    frags->page_offset);\n\t}\n\treturn 0;\n}\n\nstatic void mlx4_en_free_frag(const struct mlx4_en_priv *priv,\n\t\t\t      struct mlx4_en_rx_alloc *frag)\n{\n\tif (frag->page) {\n\t\tdma_unmap_page(priv->ddev, frag->dma,\n\t\t\t       PAGE_SIZE, priv->dma_dir);\n\t\t__free_page(frag->page);\n\t}\n\t \n\tmemset(frag, 0, sizeof(*frag));\n}\n\nstatic void mlx4_en_init_rx_desc(const struct mlx4_en_priv *priv,\n\t\t\t\t struct mlx4_en_rx_ring *ring, int index)\n{\n\tstruct mlx4_en_rx_desc *rx_desc = ring->buf + ring->stride * index;\n\tint possible_frags;\n\tint i;\n\n\t \n\tfor (i = 0; i < priv->num_frags; i++) {\n\t\trx_desc->data[i].byte_count =\n\t\t\tcpu_to_be32(priv->frag_info[i].frag_size);\n\t\trx_desc->data[i].lkey = cpu_to_be32(priv->mdev->mr.key);\n\t}\n\n\t \n\tpossible_frags = (ring->stride - sizeof(struct mlx4_en_rx_desc)) / DS_SIZE;\n\tfor (i = priv->num_frags; i < possible_frags; i++) {\n\t\trx_desc->data[i].byte_count = 0;\n\t\trx_desc->data[i].lkey = cpu_to_be32(MLX4_EN_MEMTYPE_PAD);\n\t\trx_desc->data[i].addr = 0;\n\t}\n}\n\nstatic int mlx4_en_prepare_rx_desc(struct mlx4_en_priv *priv,\n\t\t\t\t   struct mlx4_en_rx_ring *ring, int index,\n\t\t\t\t   gfp_t gfp)\n{\n\tstruct mlx4_en_rx_desc *rx_desc = ring->buf +\n\t\t(index << ring->log_stride);\n\tstruct mlx4_en_rx_alloc *frags = ring->rx_info +\n\t\t\t\t\t(index << priv->log_rx_info);\n\tif (likely(ring->page_cache.index > 0)) {\n\t\t \n\t\tif (!frags->page) {\n\t\t\tring->page_cache.index--;\n\t\t\tfrags->page = ring->page_cache.buf[ring->page_cache.index].page;\n\t\t\tfrags->dma  = ring->page_cache.buf[ring->page_cache.index].dma;\n\t\t}\n\t\tfrags->page_offset = XDP_PACKET_HEADROOM;\n\t\trx_desc->data[0].addr = cpu_to_be64(frags->dma +\n\t\t\t\t\t\t    XDP_PACKET_HEADROOM);\n\t\treturn 0;\n\t}\n\n\treturn mlx4_en_alloc_frags(priv, ring, rx_desc, frags, gfp);\n}\n\nstatic bool mlx4_en_is_ring_empty(const struct mlx4_en_rx_ring *ring)\n{\n\treturn ring->prod == ring->cons;\n}\n\nstatic inline void mlx4_en_update_rx_prod_db(struct mlx4_en_rx_ring *ring)\n{\n\t*ring->wqres.db.db = cpu_to_be32(ring->prod & 0xffff);\n}\n\n \nstatic void mlx4_en_free_rx_desc(const struct mlx4_en_priv *priv,\n\t\t\t\t struct mlx4_en_rx_ring *ring,\n\t\t\t\t int index)\n{\n\tstruct mlx4_en_rx_alloc *frags;\n\tint nr;\n\n\tfrags = ring->rx_info + (index << priv->log_rx_info);\n\tfor (nr = 0; nr < priv->num_frags; nr++) {\n\t\ten_dbg(DRV, priv, \"Freeing fragment:%d\\n\", nr);\n\t\tmlx4_en_free_frag(priv, frags + nr);\n\t}\n}\n\n \nstatic int mlx4_en_fill_rx_buffers(struct mlx4_en_priv *priv)\n{\n\tstruct mlx4_en_rx_ring *ring;\n\tint ring_ind;\n\tint buf_ind;\n\tint new_size;\n\n\tfor (buf_ind = 0; buf_ind < priv->prof->rx_ring_size; buf_ind++) {\n\t\tfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {\n\t\t\tring = priv->rx_ring[ring_ind];\n\n\t\t\tif (mlx4_en_prepare_rx_desc(priv, ring,\n\t\t\t\t\t\t    ring->actual_size,\n\t\t\t\t\t\t    GFP_KERNEL)) {\n\t\t\t\tif (ring->actual_size < MLX4_EN_MIN_RX_SIZE) {\n\t\t\t\t\ten_err(priv, \"Failed to allocate enough rx buffers\\n\");\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\t} else {\n\t\t\t\t\tnew_size = rounddown_pow_of_two(ring->actual_size);\n\t\t\t\t\ten_warn(priv, \"Only %d buffers allocated reducing ring size to %d\\n\",\n\t\t\t\t\t\tring->actual_size, new_size);\n\t\t\t\t\tgoto reduce_rings;\n\t\t\t\t}\n\t\t\t}\n\t\t\tring->actual_size++;\n\t\t\tring->prod++;\n\t\t}\n\t}\n\treturn 0;\n\nreduce_rings:\n\tfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {\n\t\tring = priv->rx_ring[ring_ind];\n\t\twhile (ring->actual_size > new_size) {\n\t\t\tring->actual_size--;\n\t\t\tring->prod--;\n\t\t\tmlx4_en_free_rx_desc(priv, ring, ring->actual_size);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void mlx4_en_free_rx_buf(struct mlx4_en_priv *priv,\n\t\t\t\tstruct mlx4_en_rx_ring *ring)\n{\n\tint index;\n\n\ten_dbg(DRV, priv, \"Freeing Rx buf - cons:%d prod:%d\\n\",\n\t       ring->cons, ring->prod);\n\n\t \n\tfor (index = 0; index < ring->size; index++) {\n\t\ten_dbg(DRV, priv, \"Processing descriptor:%d\\n\", index);\n\t\tmlx4_en_free_rx_desc(priv, ring, index);\n\t}\n\tring->cons = 0;\n\tring->prod = 0;\n}\n\nvoid mlx4_en_set_num_rx_rings(struct mlx4_en_dev *mdev)\n{\n\tint i;\n\tint num_of_eqs;\n\tint num_rx_rings;\n\tstruct mlx4_dev *dev = mdev->dev;\n\n\tmlx4_foreach_port(i, dev, MLX4_PORT_TYPE_ETH) {\n\t\tnum_of_eqs = max_t(int, MIN_RX_RINGS,\n\t\t\t\t   min_t(int,\n\t\t\t\t\t mlx4_get_eqs_per_port(mdev->dev, i),\n\t\t\t\t\t DEF_RX_RINGS));\n\n\t\tnum_rx_rings = mlx4_low_memory_profile() ? MIN_RX_RINGS :\n\t\t\tmin_t(int, num_of_eqs, num_online_cpus());\n\t\tmdev->profile.prof[i].rx_ring_num =\n\t\t\trounddown_pow_of_two(num_rx_rings);\n\t}\n}\n\nint mlx4_en_create_rx_ring(struct mlx4_en_priv *priv,\n\t\t\t   struct mlx4_en_rx_ring **pring,\n\t\t\t   u32 size, u16 stride, int node, int queue_index)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\tstruct mlx4_en_rx_ring *ring;\n\tint err = -ENOMEM;\n\tint tmp;\n\n\tring = kzalloc_node(sizeof(*ring), GFP_KERNEL, node);\n\tif (!ring) {\n\t\ten_err(priv, \"Failed to allocate RX ring structure\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tring->prod = 0;\n\tring->cons = 0;\n\tring->size = size;\n\tring->size_mask = size - 1;\n\tring->stride = stride;\n\tring->log_stride = ffs(ring->stride) - 1;\n\tring->buf_size = ring->size * ring->stride + TXBB_SIZE;\n\n\tif (xdp_rxq_info_reg(&ring->xdp_rxq, priv->dev, queue_index, 0) < 0)\n\t\tgoto err_ring;\n\n\ttmp = size * roundup_pow_of_two(MLX4_EN_MAX_RX_FRAGS *\n\t\t\t\t\tsizeof(struct mlx4_en_rx_alloc));\n\tring->rx_info = kvzalloc_node(tmp, GFP_KERNEL, node);\n\tif (!ring->rx_info) {\n\t\terr = -ENOMEM;\n\t\tgoto err_xdp_info;\n\t}\n\n\ten_dbg(DRV, priv, \"Allocated rx_info ring at addr:%p size:%d\\n\",\n\t\t ring->rx_info, tmp);\n\n\t \n\tset_dev_node(&mdev->dev->persist->pdev->dev, node);\n\terr = mlx4_alloc_hwq_res(mdev->dev, &ring->wqres, ring->buf_size);\n\tset_dev_node(&mdev->dev->persist->pdev->dev, mdev->dev->numa_node);\n\tif (err)\n\t\tgoto err_info;\n\n\tring->buf = ring->wqres.buf.direct.buf;\n\n\tring->hwtstamp_rx_filter = priv->hwtstamp_config.rx_filter;\n\n\t*pring = ring;\n\treturn 0;\n\nerr_info:\n\tkvfree(ring->rx_info);\n\tring->rx_info = NULL;\nerr_xdp_info:\n\txdp_rxq_info_unreg(&ring->xdp_rxq);\nerr_ring:\n\tkfree(ring);\n\t*pring = NULL;\n\n\treturn err;\n}\n\nint mlx4_en_activate_rx_rings(struct mlx4_en_priv *priv)\n{\n\tstruct mlx4_en_rx_ring *ring;\n\tint i;\n\tint ring_ind;\n\tint err;\n\tint stride = roundup_pow_of_two(sizeof(struct mlx4_en_rx_desc) +\n\t\t\t\t\tDS_SIZE * priv->num_frags);\n\n\tfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {\n\t\tring = priv->rx_ring[ring_ind];\n\n\t\tring->prod = 0;\n\t\tring->cons = 0;\n\t\tring->actual_size = 0;\n\t\tring->cqn = priv->rx_cq[ring_ind]->mcq.cqn;\n\n\t\tring->stride = stride;\n\t\tif (ring->stride <= TXBB_SIZE) {\n\t\t\t \n\t\t\t__be32 *ptr = (__be32 *)ring->buf;\n\t\t\t__be32 stamp = cpu_to_be32(1 << STAMP_SHIFT);\n\t\t\t*ptr = stamp;\n\t\t\t \n\t\t\tring->buf += TXBB_SIZE;\n\t\t}\n\n\t\tring->log_stride = ffs(ring->stride) - 1;\n\t\tring->buf_size = ring->size * ring->stride;\n\n\t\tmemset(ring->buf, 0, ring->buf_size);\n\t\tmlx4_en_update_rx_prod_db(ring);\n\n\t\t \n\t\tfor (i = 0; i < ring->size; i++)\n\t\t\tmlx4_en_init_rx_desc(priv, ring, i);\n\t}\n\terr = mlx4_en_fill_rx_buffers(priv);\n\tif (err)\n\t\tgoto err_buffers;\n\n\tfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {\n\t\tring = priv->rx_ring[ring_ind];\n\n\t\tring->size_mask = ring->actual_size - 1;\n\t\tmlx4_en_update_rx_prod_db(ring);\n\t}\n\n\treturn 0;\n\nerr_buffers:\n\tfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++)\n\t\tmlx4_en_free_rx_buf(priv, priv->rx_ring[ring_ind]);\n\n\tring_ind = priv->rx_ring_num - 1;\n\twhile (ring_ind >= 0) {\n\t\tif (priv->rx_ring[ring_ind]->stride <= TXBB_SIZE)\n\t\t\tpriv->rx_ring[ring_ind]->buf -= TXBB_SIZE;\n\t\tring_ind--;\n\t}\n\treturn err;\n}\n\n \nvoid mlx4_en_recover_from_oom(struct mlx4_en_priv *priv)\n{\n\tint ring;\n\n\tif (!priv->port_up)\n\t\treturn;\n\n\tfor (ring = 0; ring < priv->rx_ring_num; ring++) {\n\t\tif (mlx4_en_is_ring_empty(priv->rx_ring[ring])) {\n\t\t\tlocal_bh_disable();\n\t\t\tnapi_reschedule(&priv->rx_cq[ring]->napi);\n\t\t\tlocal_bh_enable();\n\t\t}\n\t}\n}\n\n \nbool mlx4_en_rx_recycle(struct mlx4_en_rx_ring *ring,\n\t\t\tstruct mlx4_en_rx_alloc *frame)\n{\n\tstruct mlx4_en_page_cache *cache = &ring->page_cache;\n\n\tif (cache->index >= MLX4_EN_CACHE_SIZE)\n\t\treturn false;\n\n\tcache->buf[cache->index].page = frame->page;\n\tcache->buf[cache->index].dma = frame->dma;\n\tcache->index++;\n\treturn true;\n}\n\nvoid mlx4_en_destroy_rx_ring(struct mlx4_en_priv *priv,\n\t\t\t     struct mlx4_en_rx_ring **pring,\n\t\t\t     u32 size, u16 stride)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\tstruct mlx4_en_rx_ring *ring = *pring;\n\tstruct bpf_prog *old_prog;\n\n\told_prog = rcu_dereference_protected(\n\t\t\t\t\tring->xdp_prog,\n\t\t\t\t\tlockdep_is_held(&mdev->state_lock));\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\txdp_rxq_info_unreg(&ring->xdp_rxq);\n\tmlx4_free_hwq_res(mdev->dev, &ring->wqres, size * stride + TXBB_SIZE);\n\tkvfree(ring->rx_info);\n\tring->rx_info = NULL;\n\tkfree(ring);\n\t*pring = NULL;\n}\n\nvoid mlx4_en_deactivate_rx_ring(struct mlx4_en_priv *priv,\n\t\t\t\tstruct mlx4_en_rx_ring *ring)\n{\n\tint i;\n\n\tfor (i = 0; i < ring->page_cache.index; i++) {\n\t\tdma_unmap_page(priv->ddev, ring->page_cache.buf[i].dma,\n\t\t\t       PAGE_SIZE, priv->dma_dir);\n\t\tput_page(ring->page_cache.buf[i].page);\n\t}\n\tring->page_cache.index = 0;\n\tmlx4_en_free_rx_buf(priv, ring);\n\tif (ring->stride <= TXBB_SIZE)\n\t\tring->buf -= TXBB_SIZE;\n}\n\n\nstatic int mlx4_en_complete_rx_desc(struct mlx4_en_priv *priv,\n\t\t\t\t    struct mlx4_en_rx_alloc *frags,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    int length)\n{\n\tconst struct mlx4_en_frag_info *frag_info = priv->frag_info;\n\tunsigned int truesize = 0;\n\tbool release = true;\n\tint nr, frag_size;\n\tstruct page *page;\n\tdma_addr_t dma;\n\n\t \n\tfor (nr = 0;; frags++) {\n\t\tfrag_size = min_t(int, length, frag_info->frag_size);\n\n\t\tpage = frags->page;\n\t\tif (unlikely(!page))\n\t\t\tgoto fail;\n\n\t\tdma = frags->dma;\n\t\tdma_sync_single_range_for_cpu(priv->ddev, dma, frags->page_offset,\n\t\t\t\t\t      frag_size, priv->dma_dir);\n\n\t\t__skb_fill_page_desc(skb, nr, page, frags->page_offset,\n\t\t\t\t     frag_size);\n\n\t\ttruesize += frag_info->frag_stride;\n\t\tif (frag_info->frag_stride == PAGE_SIZE / 2) {\n\t\t\tfrags->page_offset ^= PAGE_SIZE / 2;\n\t\t\trelease = page_count(page) != 1 ||\n\t\t\t\t  page_is_pfmemalloc(page) ||\n\t\t\t\t  page_to_nid(page) != numa_mem_id();\n\t\t} else if (!priv->rx_headroom) {\n\t\t\t \n\t\t\tu32 sz_align = ALIGN(frag_size, SMP_CACHE_BYTES);\n\n\t\t\tfrags->page_offset += sz_align;\n\t\t\trelease = frags->page_offset + frag_info->frag_size > PAGE_SIZE;\n\t\t}\n\t\tif (release) {\n\t\t\tdma_unmap_page(priv->ddev, dma, PAGE_SIZE, priv->dma_dir);\n\t\t\tfrags->page = NULL;\n\t\t} else {\n\t\t\tpage_ref_inc(page);\n\t\t}\n\n\t\tnr++;\n\t\tlength -= frag_size;\n\t\tif (!length)\n\t\t\tbreak;\n\t\tfrag_info++;\n\t}\n\tskb->truesize += truesize;\n\treturn nr;\n\nfail:\n\twhile (nr > 0) {\n\t\tnr--;\n\t\t__skb_frag_unref(skb_shinfo(skb)->frags + nr, false);\n\t}\n\treturn 0;\n}\n\nstatic void validate_loopback(struct mlx4_en_priv *priv, void *va)\n{\n\tconst unsigned char *data = va + ETH_HLEN;\n\tint i;\n\n\tfor (i = 0; i < MLX4_LOOPBACK_TEST_PAYLOAD; i++) {\n\t\tif (data[i] != (unsigned char)i)\n\t\t\treturn;\n\t}\n\t \n\tpriv->loopback_ok = 1;\n}\n\nstatic void mlx4_en_refill_rx_buffers(struct mlx4_en_priv *priv,\n\t\t\t\t      struct mlx4_en_rx_ring *ring)\n{\n\tu32 missing = ring->actual_size - (ring->prod - ring->cons);\n\n\t \n\tif (missing < 8)\n\t\treturn;\n\tdo {\n\t\tif (mlx4_en_prepare_rx_desc(priv, ring,\n\t\t\t\t\t    ring->prod & ring->size_mask,\n\t\t\t\t\t    GFP_ATOMIC | __GFP_MEMALLOC))\n\t\t\tbreak;\n\t\tring->prod++;\n\t} while (likely(--missing));\n\n\tmlx4_en_update_rx_prod_db(ring);\n}\n\n \nstatic inline __wsum get_fixed_vlan_csum(__wsum hw_checksum,\n\t\t\t\t\t struct vlan_hdr *vlanh)\n{\n\treturn csum_add(hw_checksum, *(__wsum *)vlanh);\n}\n\n \nstatic int get_fixed_ipv4_csum(__wsum hw_checksum, struct sk_buff *skb,\n\t\t\t       struct iphdr *iph)\n{\n\t__u16 length_for_csum = 0;\n\t__wsum csum_pseudo_header = 0;\n\t__u8 ipproto = iph->protocol;\n\n\tif (unlikely(ipproto == IPPROTO_SCTP))\n\t\treturn -1;\n\n\tlength_for_csum = (be16_to_cpu(iph->tot_len) - (iph->ihl << 2));\n\tcsum_pseudo_header = csum_tcpudp_nofold(iph->saddr, iph->daddr,\n\t\t\t\t\t\tlength_for_csum, ipproto, 0);\n\tskb->csum = csum_sub(hw_checksum, csum_pseudo_header);\n\treturn 0;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n \nstatic int get_fixed_ipv6_csum(__wsum hw_checksum, struct sk_buff *skb,\n\t\t\t       struct ipv6hdr *ipv6h)\n{\n\t__u8 nexthdr = ipv6h->nexthdr;\n\t__wsum temp;\n\n\tif (unlikely(nexthdr == IPPROTO_FRAGMENT ||\n\t\t     nexthdr == IPPROTO_HOPOPTS ||\n\t\t     nexthdr == IPPROTO_SCTP))\n\t\treturn -1;\n\n\t \n\ttemp = csum_add(hw_checksum, *(__wsum *)ipv6h);\n\t \n\tskb->csum = csum_add(temp, (__force __wsum)*(__be16 *)&ipv6h->nexthdr);\n\treturn 0;\n}\n#endif\n\n#define short_frame(size) ((size) <= ETH_ZLEN + ETH_FCS_LEN)\n\n \nstatic int check_csum(struct mlx4_cqe *cqe, struct sk_buff *skb, void *va,\n\t\t      netdev_features_t dev_features)\n{\n\t__wsum hw_checksum = 0;\n\tvoid *hdr;\n\n\t \n\tif (short_frame(skb->len))\n\t\treturn -EINVAL;\n\n\thdr = (u8 *)va + sizeof(struct ethhdr);\n\thw_checksum = csum_unfold((__force __sum16)cqe->checksum);\n\n\tif (cqe->vlan_my_qpn & cpu_to_be32(MLX4_CQE_CVLAN_PRESENT_MASK) &&\n\t    !(dev_features & NETIF_F_HW_VLAN_CTAG_RX)) {\n\t\thw_checksum = get_fixed_vlan_csum(hw_checksum, hdr);\n\t\thdr += sizeof(struct vlan_hdr);\n\t}\n\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (cqe->status & cpu_to_be16(MLX4_CQE_STATUS_IPV6))\n\t\treturn get_fixed_ipv6_csum(hw_checksum, skb, hdr);\n#endif\n\treturn get_fixed_ipv4_csum(hw_checksum, skb, hdr);\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n#define MLX4_CQE_STATUS_IP_ANY (MLX4_CQE_STATUS_IPV4 | MLX4_CQE_STATUS_IPV6)\n#else\n#define MLX4_CQE_STATUS_IP_ANY (MLX4_CQE_STATUS_IPV4)\n#endif\n\nstruct mlx4_en_xdp_buff {\n\tstruct xdp_buff xdp;\n\tstruct mlx4_cqe *cqe;\n\tstruct mlx4_en_dev *mdev;\n\tstruct mlx4_en_rx_ring *ring;\n\tstruct net_device *dev;\n};\n\nint mlx4_en_xdp_rx_timestamp(const struct xdp_md *ctx, u64 *timestamp)\n{\n\tstruct mlx4_en_xdp_buff *_ctx = (void *)ctx;\n\n\tif (unlikely(_ctx->ring->hwtstamp_rx_filter != HWTSTAMP_FILTER_ALL))\n\t\treturn -ENODATA;\n\n\t*timestamp = mlx4_en_get_hwtstamp(_ctx->mdev,\n\t\t\t\t\t  mlx4_en_get_cqe_ts(_ctx->cqe));\n\treturn 0;\n}\n\nint mlx4_en_xdp_rx_hash(const struct xdp_md *ctx, u32 *hash,\n\t\t\tenum xdp_rss_hash_type *rss_type)\n{\n\tstruct mlx4_en_xdp_buff *_ctx = (void *)ctx;\n\tstruct mlx4_cqe *cqe = _ctx->cqe;\n\tenum xdp_rss_hash_type xht = 0;\n\t__be16 status;\n\n\tif (unlikely(!(_ctx->dev->features & NETIF_F_RXHASH)))\n\t\treturn -ENODATA;\n\n\t*hash = be32_to_cpu(cqe->immed_rss_invalid);\n\tstatus = cqe->status;\n\tif (status & cpu_to_be16(MLX4_CQE_STATUS_TCP))\n\t\txht = XDP_RSS_L4_TCP;\n\tif (status & cpu_to_be16(MLX4_CQE_STATUS_UDP))\n\t\txht = XDP_RSS_L4_UDP;\n\tif (status & cpu_to_be16(MLX4_CQE_STATUS_IPV4 | MLX4_CQE_STATUS_IPV4F))\n\t\txht |= XDP_RSS_L3_IPV4;\n\tif (status & cpu_to_be16(MLX4_CQE_STATUS_IPV6)) {\n\t\txht |= XDP_RSS_L3_IPV6;\n\t\tif (cqe->ipv6_ext_mask)\n\t\t\txht |= XDP_RSS_L3_DYNHDR;\n\t}\n\t*rss_type = xht;\n\n\treturn 0;\n}\n\nint mlx4_en_process_rx_cq(struct net_device *dev, struct mlx4_en_cq *cq, int budget)\n{\n\tstruct mlx4_en_priv *priv = netdev_priv(dev);\n\tstruct mlx4_en_xdp_buff mxbuf = {};\n\tint factor = priv->cqe_factor;\n\tstruct mlx4_en_rx_ring *ring;\n\tstruct bpf_prog *xdp_prog;\n\tint cq_ring = cq->ring;\n\tbool doorbell_pending;\n\tbool xdp_redir_flush;\n\tstruct mlx4_cqe *cqe;\n\tint polled = 0;\n\tint index;\n\n\tif (unlikely(!priv->port_up || budget <= 0))\n\t\treturn 0;\n\n\tring = priv->rx_ring[cq_ring];\n\n\txdp_prog = rcu_dereference_bh(ring->xdp_prog);\n\txdp_init_buff(&mxbuf.xdp, priv->frag_info[0].frag_stride, &ring->xdp_rxq);\n\tdoorbell_pending = false;\n\txdp_redir_flush = false;\n\n\t \n\tindex = cq->mcq.cons_index & ring->size_mask;\n\tcqe = mlx4_en_get_cqe(cq->buf, index, priv->cqe_size) + factor;\n\n\t \n\twhile (XNOR(cqe->owner_sr_opcode & MLX4_CQE_OWNER_MASK,\n\t\t    cq->mcq.cons_index & cq->size)) {\n\t\tstruct mlx4_en_rx_alloc *frags;\n\t\tenum pkt_hash_types hash_type;\n\t\tstruct sk_buff *skb;\n\t\tunsigned int length;\n\t\tint ip_summed;\n\t\tvoid *va;\n\t\tint nr;\n\n\t\tfrags = ring->rx_info + (index << priv->log_rx_info);\n\t\tva = page_address(frags[0].page) + frags[0].page_offset;\n\t\tnet_prefetchw(va);\n\t\t \n\t\tdma_rmb();\n\n\t\t \n\t\tif (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==\n\t\t\t\t\t\tMLX4_CQE_OPCODE_ERROR)) {\n\t\t\ten_err(priv, \"CQE completed in error - vendor syndrom:%d syndrom:%d\\n\",\n\t\t\t       ((struct mlx4_err_cqe *)cqe)->vendor_err_syndrome,\n\t\t\t       ((struct mlx4_err_cqe *)cqe)->syndrome);\n\t\t\tgoto next;\n\t\t}\n\t\tif (unlikely(cqe->badfcs_enc & MLX4_CQE_BAD_FCS)) {\n\t\t\ten_dbg(RX_ERR, priv, \"Accepted frame with bad FCS\\n\");\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (priv->flags & MLX4_EN_FLAG_RX_FILTER_NEEDED) {\n\t\t\tconst struct ethhdr *ethh = va;\n\t\t\tdma_addr_t dma;\n\t\t\t \n\t\t\tdma = frags[0].dma + frags[0].page_offset;\n\t\t\tdma_sync_single_for_cpu(priv->ddev, dma, sizeof(*ethh),\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\n\t\t\tif (is_multicast_ether_addr(ethh->h_dest)) {\n\t\t\t\tstruct mlx4_mac_entry *entry;\n\t\t\t\tstruct hlist_head *bucket;\n\t\t\t\tunsigned int mac_hash;\n\n\t\t\t\t \n\t\t\t\tmac_hash = ethh->h_source[MLX4_EN_MAC_HASH_IDX];\n\t\t\t\tbucket = &priv->mac_hash[mac_hash];\n\t\t\t\thlist_for_each_entry_rcu_bh(entry, bucket, hlist) {\n\t\t\t\t\tif (ether_addr_equal_64bits(entry->mac,\n\t\t\t\t\t\t\t\t    ethh->h_source))\n\t\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(priv->validate_loopback)) {\n\t\t\tvalidate_loopback(priv, va);\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tlength = be32_to_cpu(cqe->byte_cnt);\n\t\tlength -= ring->fcs_del;\n\n\t\t \n\t\tif (xdp_prog) {\n\t\t\tdma_addr_t dma;\n\t\t\tvoid *orig_data;\n\t\t\tu32 act;\n\n\t\t\tdma = frags[0].dma + frags[0].page_offset;\n\t\t\tdma_sync_single_for_cpu(priv->ddev, dma,\n\t\t\t\t\t\tpriv->frag_info[0].frag_size,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\n\t\t\txdp_prepare_buff(&mxbuf.xdp, va - frags[0].page_offset,\n\t\t\t\t\t frags[0].page_offset, length, true);\n\t\t\torig_data = mxbuf.xdp.data;\n\t\t\tmxbuf.cqe = cqe;\n\t\t\tmxbuf.mdev = priv->mdev;\n\t\t\tmxbuf.ring = ring;\n\t\t\tmxbuf.dev = dev;\n\n\t\t\tact = bpf_prog_run_xdp(xdp_prog, &mxbuf.xdp);\n\n\t\t\tlength = mxbuf.xdp.data_end - mxbuf.xdp.data;\n\t\t\tif (mxbuf.xdp.data != orig_data) {\n\t\t\t\tfrags[0].page_offset = mxbuf.xdp.data -\n\t\t\t\t\tmxbuf.xdp.data_hard_start;\n\t\t\t\tva = mxbuf.xdp.data;\n\t\t\t}\n\n\t\t\tswitch (act) {\n\t\t\tcase XDP_PASS:\n\t\t\t\tbreak;\n\t\t\tcase XDP_REDIRECT:\n\t\t\t\tif (likely(!xdp_do_redirect(dev, &mxbuf.xdp, xdp_prog))) {\n\t\t\t\t\tring->xdp_redirect++;\n\t\t\t\t\txdp_redir_flush = true;\n\t\t\t\t\tfrags[0].page = NULL;\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t\tring->xdp_redirect_fail++;\n\t\t\t\ttrace_xdp_exception(dev, xdp_prog, act);\n\t\t\t\tgoto xdp_drop_no_cnt;\n\t\t\tcase XDP_TX:\n\t\t\t\tif (likely(!mlx4_en_xmit_frame(ring, frags, priv,\n\t\t\t\t\t\t\tlength, cq_ring,\n\t\t\t\t\t\t\t&doorbell_pending))) {\n\t\t\t\t\tfrags[0].page = NULL;\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t\ttrace_xdp_exception(dev, xdp_prog, act);\n\t\t\t\tgoto xdp_drop_no_cnt;  \n\t\t\tdefault:\n\t\t\t\tbpf_warn_invalid_xdp_action(dev, xdp_prog, act);\n\t\t\t\tfallthrough;\n\t\t\tcase XDP_ABORTED:\n\t\t\t\ttrace_xdp_exception(dev, xdp_prog, act);\n\t\t\t\tfallthrough;\n\t\t\tcase XDP_DROP:\n\t\t\t\tring->xdp_drop++;\nxdp_drop_no_cnt:\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t}\n\n\t\tring->bytes += length;\n\t\tring->packets++;\n\n\t\tskb = napi_get_frags(&cq->napi);\n\t\tif (unlikely(!skb))\n\t\t\tgoto next;\n\n\t\tif (unlikely(ring->hwtstamp_rx_filter == HWTSTAMP_FILTER_ALL)) {\n\t\t\tu64 timestamp = mlx4_en_get_cqe_ts(cqe);\n\n\t\t\tmlx4_en_fill_hwtstamps(priv->mdev, skb_hwtstamps(skb),\n\t\t\t\t\t       timestamp);\n\t\t}\n\t\tskb_record_rx_queue(skb, cq_ring);\n\n\t\tif (likely(dev->features & NETIF_F_RXCSUM)) {\n\t\t\t \n\t\t\tif ((cqe->status & cpu_to_be16(MLX4_CQE_STATUS_TCP |\n\t\t\t\t\t\t       MLX4_CQE_STATUS_UDP)) &&\n\t\t\t    (cqe->status & cpu_to_be16(MLX4_CQE_STATUS_IPOK)) &&\n\t\t\t    cqe->checksum == cpu_to_be16(0xffff)) {\n\t\t\t\tbool l2_tunnel;\n\n\t\t\t\tl2_tunnel = (dev->hw_enc_features & NETIF_F_RXCSUM) &&\n\t\t\t\t\t(cqe->vlan_my_qpn & cpu_to_be32(MLX4_CQE_L2_TUNNEL));\n\t\t\t\tip_summed = CHECKSUM_UNNECESSARY;\n\t\t\t\thash_type = PKT_HASH_TYPE_L4;\n\t\t\t\tif (l2_tunnel)\n\t\t\t\t\tskb->csum_level = 1;\n\t\t\t\tring->csum_ok++;\n\t\t\t} else {\n\t\t\t\tif (!(priv->flags & MLX4_EN_FLAG_RX_CSUM_NON_TCP_UDP &&\n\t\t\t\t      (cqe->status & cpu_to_be16(MLX4_CQE_STATUS_IP_ANY))))\n\t\t\t\t\tgoto csum_none;\n\t\t\t\tif (check_csum(cqe, skb, va, dev->features))\n\t\t\t\t\tgoto csum_none;\n\t\t\t\tip_summed = CHECKSUM_COMPLETE;\n\t\t\t\thash_type = PKT_HASH_TYPE_L3;\n\t\t\t\tring->csum_complete++;\n\t\t\t}\n\t\t} else {\ncsum_none:\n\t\t\tip_summed = CHECKSUM_NONE;\n\t\t\thash_type = PKT_HASH_TYPE_L3;\n\t\t\tring->csum_none++;\n\t\t}\n\t\tskb->ip_summed = ip_summed;\n\t\tif (dev->features & NETIF_F_RXHASH)\n\t\t\tskb_set_hash(skb,\n\t\t\t\t     be32_to_cpu(cqe->immed_rss_invalid),\n\t\t\t\t     hash_type);\n\n\t\tif ((cqe->vlan_my_qpn &\n\t\t     cpu_to_be32(MLX4_CQE_CVLAN_PRESENT_MASK)) &&\n\t\t    (dev->features & NETIF_F_HW_VLAN_CTAG_RX))\n\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t\t       be16_to_cpu(cqe->sl_vid));\n\t\telse if ((cqe->vlan_my_qpn &\n\t\t\t  cpu_to_be32(MLX4_CQE_SVLAN_PRESENT_MASK)) &&\n\t\t\t (dev->features & NETIF_F_HW_VLAN_STAG_RX))\n\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021AD),\n\t\t\t\t\t       be16_to_cpu(cqe->sl_vid));\n\n\t\tnr = mlx4_en_complete_rx_desc(priv, frags, skb, length);\n\t\tif (likely(nr)) {\n\t\t\tskb_shinfo(skb)->nr_frags = nr;\n\t\t\tskb->len = length;\n\t\t\tskb->data_len = length;\n\t\t\tnapi_gro_frags(&cq->napi);\n\t\t} else {\n\t\t\t__vlan_hwaccel_clear_tag(skb);\n\t\t\tskb_clear_hash(skb);\n\t\t}\nnext:\n\t\t++cq->mcq.cons_index;\n\t\tindex = (cq->mcq.cons_index) & ring->size_mask;\n\t\tcqe = mlx4_en_get_cqe(cq->buf, index, priv->cqe_size) + factor;\n\t\tif (unlikely(++polled == budget))\n\t\t\tbreak;\n\t}\n\n\tif (xdp_redir_flush)\n\t\txdp_do_flush();\n\n\tif (likely(polled)) {\n\t\tif (doorbell_pending) {\n\t\t\tpriv->tx_cq[TX_XDP][cq_ring]->xdp_busy = true;\n\t\t\tmlx4_en_xmit_doorbell(priv->tx_ring[TX_XDP][cq_ring]);\n\t\t}\n\n\t\tmlx4_cq_set_ci(&cq->mcq);\n\t\twmb();  \n\t\tring->cons = cq->mcq.cons_index;\n\t}\n\n\tmlx4_en_refill_rx_buffers(priv, ring);\n\n\treturn polled;\n}\n\n\nvoid mlx4_en_rx_irq(struct mlx4_cq *mcq)\n{\n\tstruct mlx4_en_cq *cq = container_of(mcq, struct mlx4_en_cq, mcq);\n\tstruct mlx4_en_priv *priv = netdev_priv(cq->dev);\n\n\tif (likely(priv->port_up))\n\t\tnapi_schedule_irqoff(&cq->napi);\n\telse\n\t\tmlx4_en_arm_cq(priv, cq);\n}\n\n \nint mlx4_en_poll_rx_cq(struct napi_struct *napi, int budget)\n{\n\tstruct mlx4_en_cq *cq = container_of(napi, struct mlx4_en_cq, napi);\n\tstruct net_device *dev = cq->dev;\n\tstruct mlx4_en_priv *priv = netdev_priv(dev);\n\tstruct mlx4_en_cq *xdp_tx_cq = NULL;\n\tbool clean_complete = true;\n\tint done;\n\n\tif (!budget)\n\t\treturn 0;\n\n\tif (priv->tx_ring_num[TX_XDP]) {\n\t\txdp_tx_cq = priv->tx_cq[TX_XDP][cq->ring];\n\t\tif (xdp_tx_cq->xdp_busy) {\n\t\t\tclean_complete = mlx4_en_process_tx_cq(dev, xdp_tx_cq,\n\t\t\t\t\t\t\t       budget) < budget;\n\t\t\txdp_tx_cq->xdp_busy = !clean_complete;\n\t\t}\n\t}\n\n\tdone = mlx4_en_process_rx_cq(dev, cq, budget);\n\n\t \n\tif (done == budget || !clean_complete) {\n\t\tint cpu_curr;\n\n\t\t \n\t\tdone = budget;\n\n\t\tcpu_curr = smp_processor_id();\n\n\t\tif (likely(cpumask_test_cpu(cpu_curr, cq->aff_mask)))\n\t\t\treturn budget;\n\n\t\t \n\t\tif (done)\n\t\t\tdone--;\n\t}\n\t \n\tif (likely(napi_complete_done(napi, done)))\n\t\tmlx4_en_arm_cq(priv, cq);\n\treturn done;\n}\n\nvoid mlx4_en_calc_rx_buf(struct net_device *dev)\n{\n\tstruct mlx4_en_priv *priv = netdev_priv(dev);\n\tint eff_mtu = MLX4_EN_EFF_MTU(dev->mtu);\n\tint i = 0;\n\n\t \n\tif (priv->tx_ring_num[TX_XDP]) {\n\t\tpriv->frag_info[0].frag_size = eff_mtu;\n\t\t \n\t\tpriv->frag_info[0].frag_stride = PAGE_SIZE;\n\t\tpriv->dma_dir = DMA_BIDIRECTIONAL;\n\t\tpriv->rx_headroom = XDP_PACKET_HEADROOM;\n\t\ti = 1;\n\t} else {\n\t\tint frag_size_max = 2048, buf_size = 0;\n\n\t\t \n\t\tif (eff_mtu > PAGE_SIZE + (MLX4_EN_MAX_RX_FRAGS - 1) * 2048)\n\t\t\tfrag_size_max = PAGE_SIZE;\n\n\t\twhile (buf_size < eff_mtu) {\n\t\t\tint frag_stride, frag_size = eff_mtu - buf_size;\n\t\t\tint pad, nb;\n\n\t\t\tif (i < MLX4_EN_MAX_RX_FRAGS - 1)\n\t\t\t\tfrag_size = min(frag_size, frag_size_max);\n\n\t\t\tpriv->frag_info[i].frag_size = frag_size;\n\t\t\tfrag_stride = ALIGN(frag_size, SMP_CACHE_BYTES);\n\t\t\t \n\t\t\tnb = PAGE_SIZE / frag_stride;\n\t\t\tpad = (PAGE_SIZE - nb * frag_stride) / nb;\n\t\t\tpad &= ~(SMP_CACHE_BYTES - 1);\n\t\t\tpriv->frag_info[i].frag_stride = frag_stride + pad;\n\n\t\t\tbuf_size += frag_size;\n\t\t\ti++;\n\t\t}\n\t\tpriv->dma_dir = DMA_FROM_DEVICE;\n\t\tpriv->rx_headroom = 0;\n\t}\n\n\tpriv->num_frags = i;\n\tpriv->rx_skb_size = eff_mtu;\n\tpriv->log_rx_info = ROUNDUP_LOG2(i * sizeof(struct mlx4_en_rx_alloc));\n\n\ten_dbg(DRV, priv, \"Rx buffer scatter-list (effective-mtu:%d num_frags:%d):\\n\",\n\t       eff_mtu, priv->num_frags);\n\tfor (i = 0; i < priv->num_frags; i++) {\n\t\ten_dbg(DRV,\n\t\t       priv,\n\t\t       \"  frag:%d - size:%d stride:%d\\n\",\n\t\t       i,\n\t\t       priv->frag_info[i].frag_size,\n\t\t       priv->frag_info[i].frag_stride);\n\t}\n}\n\n \n\nstatic int mlx4_en_config_rss_qp(struct mlx4_en_priv *priv, int qpn,\n\t\t\t\t struct mlx4_en_rx_ring *ring,\n\t\t\t\t enum mlx4_qp_state *state,\n\t\t\t\t struct mlx4_qp *qp)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\tstruct mlx4_qp_context *context;\n\tint err = 0;\n\n\tcontext = kzalloc(sizeof(*context), GFP_KERNEL);\n\tif (!context)\n\t\treturn -ENOMEM;\n\n\terr = mlx4_qp_alloc(mdev->dev, qpn, qp);\n\tif (err) {\n\t\ten_err(priv, \"Failed to allocate qp #%x\\n\", qpn);\n\t\tgoto out;\n\t}\n\tqp->event = mlx4_en_sqp_event;\n\n\tmlx4_en_fill_qp_context(priv, ring->actual_size, ring->stride, 0, 0,\n\t\t\t\tqpn, ring->cqn, -1, context);\n\tcontext->db_rec_addr = cpu_to_be64(ring->wqres.db.dma);\n\n\t \n\tif (mdev->dev->caps.flags & MLX4_DEV_CAP_FLAG_FCS_KEEP) {\n\t\tcontext->param3 |= cpu_to_be32(1 << 29);\n\t\tif (priv->dev->features & NETIF_F_RXFCS)\n\t\t\tring->fcs_del = 0;\n\t\telse\n\t\t\tring->fcs_del = ETH_FCS_LEN;\n\t} else\n\t\tring->fcs_del = 0;\n\n\terr = mlx4_qp_to_ready(mdev->dev, &ring->wqres.mtt, context, qp, state);\n\tif (err) {\n\t\tmlx4_qp_remove(mdev->dev, qp);\n\t\tmlx4_qp_free(mdev->dev, qp);\n\t}\n\tmlx4_en_update_rx_prod_db(ring);\nout:\n\tkfree(context);\n\treturn err;\n}\n\nint mlx4_en_create_drop_qp(struct mlx4_en_priv *priv)\n{\n\tint err;\n\tu32 qpn;\n\n\terr = mlx4_qp_reserve_range(priv->mdev->dev, 1, 1, &qpn,\n\t\t\t\t    MLX4_RESERVE_A0_QP,\n\t\t\t\t    MLX4_RES_USAGE_DRIVER);\n\tif (err) {\n\t\ten_err(priv, \"Failed reserving drop qpn\\n\");\n\t\treturn err;\n\t}\n\terr = mlx4_qp_alloc(priv->mdev->dev, qpn, &priv->drop_qp);\n\tif (err) {\n\t\ten_err(priv, \"Failed allocating drop qp\\n\");\n\t\tmlx4_qp_release_range(priv->mdev->dev, qpn, 1);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nvoid mlx4_en_destroy_drop_qp(struct mlx4_en_priv *priv)\n{\n\tu32 qpn;\n\n\tqpn = priv->drop_qp.qpn;\n\tmlx4_qp_remove(priv->mdev->dev, &priv->drop_qp);\n\tmlx4_qp_free(priv->mdev->dev, &priv->drop_qp);\n\tmlx4_qp_release_range(priv->mdev->dev, qpn, 1);\n}\n\n \nint mlx4_en_config_rss_steer(struct mlx4_en_priv *priv)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\tstruct mlx4_en_rss_map *rss_map = &priv->rss_map;\n\tstruct mlx4_qp_context context;\n\tstruct mlx4_rss_context *rss_context;\n\tint rss_rings;\n\tvoid *ptr;\n\tu8 rss_mask = (MLX4_RSS_IPV4 | MLX4_RSS_TCP_IPV4 | MLX4_RSS_IPV6 |\n\t\t\tMLX4_RSS_TCP_IPV6);\n\tint i, qpn;\n\tint err = 0;\n\tint good_qps = 0;\n\tu8 flags;\n\n\ten_dbg(DRV, priv, \"Configuring rss steering\\n\");\n\n\tflags = priv->rx_ring_num == 1 ? MLX4_RESERVE_A0_QP : 0;\n\terr = mlx4_qp_reserve_range(mdev->dev, priv->rx_ring_num,\n\t\t\t\t    priv->rx_ring_num,\n\t\t\t\t    &rss_map->base_qpn, flags,\n\t\t\t\t    MLX4_RES_USAGE_DRIVER);\n\tif (err) {\n\t\ten_err(priv, \"Failed reserving %d qps\\n\", priv->rx_ring_num);\n\t\treturn err;\n\t}\n\n\tfor (i = 0; i < priv->rx_ring_num; i++) {\n\t\tqpn = rss_map->base_qpn + i;\n\t\terr = mlx4_en_config_rss_qp(priv, qpn, priv->rx_ring[i],\n\t\t\t\t\t    &rss_map->state[i],\n\t\t\t\t\t    &rss_map->qps[i]);\n\t\tif (err)\n\t\t\tgoto rss_err;\n\n\t\t++good_qps;\n\t}\n\n\tif (priv->rx_ring_num == 1) {\n\t\trss_map->indir_qp = &rss_map->qps[0];\n\t\tpriv->base_qpn = rss_map->indir_qp->qpn;\n\t\ten_info(priv, \"Optimized Non-RSS steering\\n\");\n\t\treturn 0;\n\t}\n\n\trss_map->indir_qp = kzalloc(sizeof(*rss_map->indir_qp), GFP_KERNEL);\n\tif (!rss_map->indir_qp) {\n\t\terr = -ENOMEM;\n\t\tgoto rss_err;\n\t}\n\n\t \n\terr = mlx4_qp_alloc(mdev->dev, priv->base_qpn, rss_map->indir_qp);\n\tif (err) {\n\t\ten_err(priv, \"Failed to allocate RSS indirection QP\\n\");\n\t\tgoto qp_alloc_err;\n\t}\n\n\trss_map->indir_qp->event = mlx4_en_sqp_event;\n\tmlx4_en_fill_qp_context(priv, 0, 0, 0, 1, priv->base_qpn,\n\t\t\t\tpriv->rx_ring[0]->cqn, -1, &context);\n\n\tif (!priv->prof->rss_rings || priv->prof->rss_rings > priv->rx_ring_num)\n\t\trss_rings = priv->rx_ring_num;\n\telse\n\t\trss_rings = priv->prof->rss_rings;\n\n\tptr = ((void *) &context) + offsetof(struct mlx4_qp_context, pri_path)\n\t\t\t\t\t+ MLX4_RSS_OFFSET_IN_QPC_PRI_PATH;\n\trss_context = ptr;\n\trss_context->base_qpn = cpu_to_be32(ilog2(rss_rings) << 24 |\n\t\t\t\t\t    (rss_map->base_qpn));\n\trss_context->default_qpn = cpu_to_be32(rss_map->base_qpn);\n\tif (priv->mdev->profile.udp_rss) {\n\t\trss_mask |=  MLX4_RSS_UDP_IPV4 | MLX4_RSS_UDP_IPV6;\n\t\trss_context->base_qpn_udp = rss_context->default_qpn;\n\t}\n\n\tif (mdev->dev->caps.tunnel_offload_mode == MLX4_TUNNEL_OFFLOAD_MODE_VXLAN) {\n\t\ten_info(priv, \"Setting RSS context tunnel type to RSS on inner headers\\n\");\n\t\trss_mask |= MLX4_RSS_BY_INNER_HEADERS;\n\t}\n\n\trss_context->flags = rss_mask;\n\trss_context->hash_fn = MLX4_RSS_HASH_TOP;\n\tif (priv->rss_hash_fn == ETH_RSS_HASH_XOR) {\n\t\trss_context->hash_fn = MLX4_RSS_HASH_XOR;\n\t} else if (priv->rss_hash_fn == ETH_RSS_HASH_TOP) {\n\t\trss_context->hash_fn = MLX4_RSS_HASH_TOP;\n\t\tmemcpy(rss_context->rss_key, priv->rss_key,\n\t\t       MLX4_EN_RSS_KEY_SIZE);\n\t} else {\n\t\ten_err(priv, \"Unknown RSS hash function requested\\n\");\n\t\terr = -EINVAL;\n\t\tgoto indir_err;\n\t}\n\n\terr = mlx4_qp_to_ready(mdev->dev, &priv->res.mtt, &context,\n\t\t\t       rss_map->indir_qp, &rss_map->indir_state);\n\tif (err)\n\t\tgoto indir_err;\n\n\treturn 0;\n\nindir_err:\n\tmlx4_qp_modify(mdev->dev, NULL, rss_map->indir_state,\n\t\t       MLX4_QP_STATE_RST, NULL, 0, 0, rss_map->indir_qp);\n\tmlx4_qp_remove(mdev->dev, rss_map->indir_qp);\n\tmlx4_qp_free(mdev->dev, rss_map->indir_qp);\nqp_alloc_err:\n\tkfree(rss_map->indir_qp);\n\trss_map->indir_qp = NULL;\nrss_err:\n\tfor (i = 0; i < good_qps; i++) {\n\t\tmlx4_qp_modify(mdev->dev, NULL, rss_map->state[i],\n\t\t\t       MLX4_QP_STATE_RST, NULL, 0, 0, &rss_map->qps[i]);\n\t\tmlx4_qp_remove(mdev->dev, &rss_map->qps[i]);\n\t\tmlx4_qp_free(mdev->dev, &rss_map->qps[i]);\n\t}\n\tmlx4_qp_release_range(mdev->dev, rss_map->base_qpn, priv->rx_ring_num);\n\treturn err;\n}\n\nvoid mlx4_en_release_rss_steer(struct mlx4_en_priv *priv)\n{\n\tstruct mlx4_en_dev *mdev = priv->mdev;\n\tstruct mlx4_en_rss_map *rss_map = &priv->rss_map;\n\tint i;\n\n\tif (priv->rx_ring_num > 1) {\n\t\tmlx4_qp_modify(mdev->dev, NULL, rss_map->indir_state,\n\t\t\t       MLX4_QP_STATE_RST, NULL, 0, 0,\n\t\t\t       rss_map->indir_qp);\n\t\tmlx4_qp_remove(mdev->dev, rss_map->indir_qp);\n\t\tmlx4_qp_free(mdev->dev, rss_map->indir_qp);\n\t\tkfree(rss_map->indir_qp);\n\t\trss_map->indir_qp = NULL;\n\t}\n\n\tfor (i = 0; i < priv->rx_ring_num; i++) {\n\t\tmlx4_qp_modify(mdev->dev, NULL, rss_map->state[i],\n\t\t\t       MLX4_QP_STATE_RST, NULL, 0, 0, &rss_map->qps[i]);\n\t\tmlx4_qp_remove(mdev->dev, &rss_map->qps[i]);\n\t\tmlx4_qp_free(mdev->dev, &rss_map->qps[i]);\n\t}\n\tmlx4_qp_release_range(mdev->dev, rss_map->base_qpn, priv->rx_ring_num);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}