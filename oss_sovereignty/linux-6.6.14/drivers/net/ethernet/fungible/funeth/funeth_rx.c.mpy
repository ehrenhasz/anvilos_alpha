{
  "module_name": "funeth_rx.c",
  "hash_id": "c6992447f9459b7e42b47c2e887f729292f06d686036ab57899b05a05d33d21e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/fungible/funeth/funeth_rx.c",
  "human_readable_source": "\n\n#include <linux/bpf_trace.h>\n#include <linux/dma-mapping.h>\n#include <linux/etherdevice.h>\n#include <linux/filter.h>\n#include <linux/irq.h>\n#include <linux/pci.h>\n#include <linux/skbuff.h>\n#include \"funeth_txrx.h\"\n#include \"funeth.h\"\n#include \"fun_queue.h\"\n\n#define CREATE_TRACE_POINTS\n#include \"funeth_trace.h\"\n\n \n#define RX_MAX_FRAGS 4\n\n \n#define FUN_RX_HEADROOM (NET_SKB_PAD + NET_IP_ALIGN)\n\n \n#define EXTRA_PAGE_REFS 1000000\n#define MIN_PAGE_REFS 1000\n\nenum {\n\tFUN_XDP_FLUSH_REDIR = 1,\n\tFUN_XDP_FLUSH_TX = 2,\n};\n\n \nstatic void refresh_refs(struct funeth_rxbuf *buf)\n{\n\tif (unlikely(buf->pg_refs < MIN_PAGE_REFS)) {\n\t\tbuf->pg_refs += EXTRA_PAGE_REFS;\n\t\tpage_ref_add(buf->page, EXTRA_PAGE_REFS);\n\t}\n}\n\n \nstatic void cache_offer(struct funeth_rxq *q, const struct funeth_rxbuf *buf)\n{\n\tstruct funeth_rx_cache *c = &q->cache;\n\n\tif (c->prod_cnt - c->cons_cnt <= c->mask && buf->node == numa_mem_id()) {\n\t\tc->bufs[c->prod_cnt & c->mask] = *buf;\n\t\tc->prod_cnt++;\n\t} else {\n\t\tdma_unmap_page_attrs(q->dma_dev, buf->dma_addr, PAGE_SIZE,\n\t\t\t\t     DMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\n\t\t__page_frag_cache_drain(buf->page, buf->pg_refs);\n\t}\n}\n\n \nstatic bool cache_get(struct funeth_rxq *q, struct funeth_rxbuf *rb)\n{\n\tstruct funeth_rx_cache *c = &q->cache;\n\tstruct funeth_rxbuf *buf;\n\n\tif (c->prod_cnt == c->cons_cnt)\n\t\treturn false;              \n\n\tbuf = &c->bufs[c->cons_cnt & c->mask];\n\tif (page_ref_count(buf->page) == buf->pg_refs) {\n\t\tdma_sync_single_for_device(q->dma_dev, buf->dma_addr,\n\t\t\t\t\t   PAGE_SIZE, DMA_FROM_DEVICE);\n\t\t*rb = *buf;\n\t\tbuf->page = NULL;\n\t\trefresh_refs(rb);\n\t\tc->cons_cnt++;\n\t\treturn true;\n\t}\n\n\t \n\tif (c->prod_cnt - c->cons_cnt > c->mask) {\n\t\tdma_unmap_page_attrs(q->dma_dev, buf->dma_addr, PAGE_SIZE,\n\t\t\t\t     DMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\n\t\t__page_frag_cache_drain(buf->page, buf->pg_refs);\n\t\tbuf->page = NULL;\n\t\tc->cons_cnt++;\n\t}\n\treturn false;\n}\n\n \nstatic int funeth_alloc_page(struct funeth_rxq *q, struct funeth_rxbuf *rb,\n\t\t\t     int node, gfp_t gfp)\n{\n\tstruct page *p;\n\n\tif (cache_get(q, rb))\n\t\treturn 0;\n\n\tp = __alloc_pages_node(node, gfp | __GFP_NOWARN, 0);\n\tif (unlikely(!p))\n\t\treturn -ENOMEM;\n\n\trb->dma_addr = dma_map_page(q->dma_dev, p, 0, PAGE_SIZE,\n\t\t\t\t    DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(q->dma_dev, rb->dma_addr))) {\n\t\tFUN_QSTAT_INC(q, rx_map_err);\n\t\t__free_page(p);\n\t\treturn -ENOMEM;\n\t}\n\n\tFUN_QSTAT_INC(q, rx_page_alloc);\n\n\trb->page = p;\n\trb->pg_refs = 1;\n\trefresh_refs(rb);\n\trb->node = page_is_pfmemalloc(p) ? -1 : page_to_nid(p);\n\treturn 0;\n}\n\nstatic void funeth_free_page(struct funeth_rxq *q, struct funeth_rxbuf *rb)\n{\n\tif (rb->page) {\n\t\tdma_unmap_page(q->dma_dev, rb->dma_addr, PAGE_SIZE,\n\t\t\t       DMA_FROM_DEVICE);\n\t\t__page_frag_cache_drain(rb->page, rb->pg_refs);\n\t\trb->page = NULL;\n\t}\n}\n\n \nstatic void *fun_run_xdp(struct funeth_rxq *q, skb_frag_t *frags, void *buf_va,\n\t\t\t int ref_ok, struct funeth_txq *xdp_q)\n{\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_frame *xdpf;\n\tstruct xdp_buff xdp;\n\tu32 act;\n\n\t \n\txdp_init_buff(&xdp, ALIGN(skb_frag_size(frags), FUN_EPRQ_PKT_ALIGN),\n\t\t      &q->xdp_rxq);\n\txdp_prepare_buff(&xdp, buf_va, FUN_XDP_HEADROOM, skb_frag_size(frags) -\n\t\t\t (FUN_RX_TAILROOM + FUN_XDP_HEADROOM), false);\n\n\txdp_prog = READ_ONCE(q->xdp_prog);\n\tact = bpf_prog_run_xdp(xdp_prog, &xdp);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\t \n\t\tskb_frag_size_set(frags, xdp.data_end - xdp.data);\n\t\tskb_frag_off_add(frags, xdp.data - xdp.data_hard_start);\n\t\tgoto pass;\n\tcase XDP_TX:\n\t\tif (unlikely(!ref_ok))\n\t\t\tgoto pass;\n\n\t\txdpf = xdp_convert_buff_to_frame(&xdp);\n\t\tif (!xdpf || !fun_xdp_tx(xdp_q, xdpf))\n\t\t\tgoto xdp_error;\n\t\tFUN_QSTAT_INC(q, xdp_tx);\n\t\tq->xdp_flush |= FUN_XDP_FLUSH_TX;\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\tif (unlikely(!ref_ok))\n\t\t\tgoto pass;\n\t\tif (unlikely(xdp_do_redirect(q->netdev, &xdp, xdp_prog)))\n\t\t\tgoto xdp_error;\n\t\tFUN_QSTAT_INC(q, xdp_redir);\n\t\tq->xdp_flush |= FUN_XDP_FLUSH_REDIR;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(q->netdev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(q->netdev, xdp_prog, act);\nxdp_error:\n\t\tq->cur_buf->pg_refs++;  \n\t\tFUN_QSTAT_INC(q, xdp_err);\n\t\tbreak;\n\tcase XDP_DROP:\n\t\tq->cur_buf->pg_refs++;\n\t\tFUN_QSTAT_INC(q, xdp_drops);\n\t\tbreak;\n\t}\n\treturn NULL;\n\npass:\n\treturn xdp.data;\n}\n\n \nstatic const void *cqe_to_info(const void *cqe)\n{\n\treturn cqe + FUNETH_CQE_INFO_OFFSET;\n}\n\n \nstatic const void *info_to_cqe(const void *cqe_info)\n{\n\treturn cqe_info - FUNETH_CQE_INFO_OFFSET;\n}\n\n \nstatic enum pkt_hash_types cqe_to_pkt_hash_type(u16 pkt_parse)\n{\n\tstatic const enum pkt_hash_types htype_map[] = {\n\t\tPKT_HASH_TYPE_NONE, PKT_HASH_TYPE_L3,\n\t\tPKT_HASH_TYPE_NONE, PKT_HASH_TYPE_L4,\n\t\tPKT_HASH_TYPE_NONE, PKT_HASH_TYPE_L3,\n\t\tPKT_HASH_TYPE_NONE, PKT_HASH_TYPE_L3\n\t};\n\tu16 key;\n\n\t \n\tkey = ((pkt_parse >> FUN_ETH_RX_CV_OL4_PROT_S) & 6) |\n\t      ((pkt_parse >> (FUN_ETH_RX_CV_OL3_PROT_S + 1)) & 1);\n\n\treturn htype_map[key];\n}\n\n \nstatic struct funeth_rxbuf *\nget_buf(struct funeth_rxq *q, struct funeth_rxbuf *buf, unsigned int len)\n{\n\tif (q->buf_offset + len <= PAGE_SIZE || !q->buf_offset)\n\t\treturn buf;             \n\n\t \n\tif ((page_ref_count(buf->page) == buf->pg_refs &&\n\t     buf->node == numa_mem_id()) || !q->spare_buf.page) {\n\t\tdma_sync_single_for_device(q->dma_dev, buf->dma_addr,\n\t\t\t\t\t   PAGE_SIZE, DMA_FROM_DEVICE);\n\t\trefresh_refs(buf);\n\t} else {\n\t\tcache_offer(q, buf);\n\t\t*buf = q->spare_buf;\n\t\tq->spare_buf.page = NULL;\n\t\tq->rqes[q->rq_cons & q->rq_mask] =\n\t\t\tFUN_EPRQ_RQBUF_INIT(buf->dma_addr);\n\t}\n\tq->buf_offset = 0;\n\tq->rq_cons++;\n\treturn &q->bufs[q->rq_cons & q->rq_mask];\n}\n\n \nstatic int fun_gather_pkt(struct funeth_rxq *q, unsigned int tot_len,\n\t\t\t  skb_frag_t *frags)\n{\n\tstruct funeth_rxbuf *buf = q->cur_buf;\n\tunsigned int frag_len;\n\tint ref_ok = 1;\n\n\tfor (;;) {\n\t\tbuf = get_buf(q, buf, tot_len);\n\n\t\t \n\t\tif (!q->spare_buf.page &&\n\t\t    funeth_alloc_page(q, &q->spare_buf, numa_mem_id(),\n\t\t\t\t      GFP_ATOMIC | __GFP_MEMALLOC))\n\t\t\tref_ok = 0;\n\n\t\tfrag_len = min_t(unsigned int, tot_len,\n\t\t\t\t PAGE_SIZE - q->buf_offset);\n\t\tdma_sync_single_for_cpu(q->dma_dev,\n\t\t\t\t\tbuf->dma_addr + q->buf_offset,\n\t\t\t\t\tfrag_len, DMA_FROM_DEVICE);\n\t\tbuf->pg_refs--;\n\t\tif (ref_ok)\n\t\t\tref_ok |= buf->node;\n\n\t\tskb_frag_fill_page_desc(frags++, buf->page, q->buf_offset,\n\t\t\t\t\tfrag_len);\n\n\t\ttot_len -= frag_len;\n\t\tif (!tot_len)\n\t\t\tbreak;\n\n\t\tq->buf_offset = PAGE_SIZE;\n\t}\n\tq->buf_offset = ALIGN(q->buf_offset + frag_len, FUN_EPRQ_PKT_ALIGN);\n\tq->cur_buf = buf;\n\treturn ref_ok;\n}\n\nstatic bool rx_hwtstamp_enabled(const struct net_device *dev)\n{\n\tconst struct funeth_priv *d = netdev_priv(dev);\n\n\treturn d->hwtstamp_cfg.rx_filter == HWTSTAMP_FILTER_ALL;\n}\n\n \nstatic void advance_cq(struct funeth_rxq *q)\n{\n\tif (unlikely(q->cq_head == q->cq_mask)) {\n\t\tq->cq_head = 0;\n\t\tq->phase ^= 1;\n\t\tq->next_cqe_info = cqe_to_info(q->cqes);\n\t} else {\n\t\tq->cq_head++;\n\t\tq->next_cqe_info += FUNETH_CQE_SIZE;\n\t}\n\tprefetch(q->next_cqe_info);\n}\n\n \nstatic void fun_handle_cqe_pkt(struct funeth_rxq *q, struct funeth_txq *xdp_q)\n{\n\tconst struct fun_eth_cqe *rxreq = info_to_cqe(q->next_cqe_info);\n\tunsigned int i, tot_len, pkt_len = be32_to_cpu(rxreq->pkt_len);\n\tstruct net_device *ndev = q->netdev;\n\tskb_frag_t frags[RX_MAX_FRAGS];\n\tstruct skb_shared_info *si;\n\tunsigned int headroom;\n\tgro_result_t gro_res;\n\tstruct sk_buff *skb;\n\tint ref_ok;\n\tvoid *va;\n\tu16 cv;\n\n\tu64_stats_update_begin(&q->syncp);\n\tq->stats.rx_pkts++;\n\tq->stats.rx_bytes += pkt_len;\n\tu64_stats_update_end(&q->syncp);\n\n\tadvance_cq(q);\n\n\t \n\ttot_len = pkt_len;\n\theadroom = be16_to_cpu(rxreq->headroom);\n\tif (likely(headroom))\n\t\ttot_len += FUN_RX_TAILROOM + headroom;\n\n\tref_ok = fun_gather_pkt(q, tot_len, frags);\n\tva = skb_frag_address(frags);\n\tif (xdp_q && headroom == FUN_XDP_HEADROOM) {\n\t\tva = fun_run_xdp(q, frags, va, ref_ok, xdp_q);\n\t\tif (!va)\n\t\t\treturn;\n\t\theadroom = 0;    \n\t}\n\tif (unlikely(!ref_ok))\n\t\tgoto no_mem;\n\n\tif (likely(headroom)) {\n\t\t \n\t\tprefetch(va + headroom);\n\t\tskb = napi_build_skb(va, ALIGN(tot_len, FUN_EPRQ_PKT_ALIGN));\n\t\tif (unlikely(!skb))\n\t\t\tgoto no_mem;\n\n\t\tskb_reserve(skb, headroom);\n\t\t__skb_put(skb, pkt_len);\n\t\tskb->protocol = eth_type_trans(skb, ndev);\n\t} else {\n\t\tprefetch(va);\n\t\tskb = napi_get_frags(q->napi);\n\t\tif (unlikely(!skb))\n\t\t\tgoto no_mem;\n\n\t\tif (ref_ok < 0)\n\t\t\tskb->pfmemalloc = 1;\n\n\t\tsi = skb_shinfo(skb);\n\t\tsi->nr_frags = rxreq->nsgl;\n\t\tfor (i = 0; i < si->nr_frags; i++)\n\t\t\tsi->frags[i] = frags[i];\n\n\t\tskb->len = pkt_len;\n\t\tskb->data_len = pkt_len;\n\t\tskb->truesize += round_up(pkt_len, FUN_EPRQ_PKT_ALIGN);\n\t}\n\n\tskb_record_rx_queue(skb, q->qidx);\n\tcv = be16_to_cpu(rxreq->pkt_cv);\n\tif (likely((q->netdev->features & NETIF_F_RXHASH) && rxreq->hash))\n\t\tskb_set_hash(skb, be32_to_cpu(rxreq->hash),\n\t\t\t     cqe_to_pkt_hash_type(cv));\n\tif (likely((q->netdev->features & NETIF_F_RXCSUM) && rxreq->csum)) {\n\t\tFUN_QSTAT_INC(q, rx_cso);\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tskb->csum_level = be16_to_cpu(rxreq->csum) - 1;\n\t}\n\tif (unlikely(rx_hwtstamp_enabled(q->netdev)))\n\t\tskb_hwtstamps(skb)->hwtstamp = be64_to_cpu(rxreq->timestamp);\n\n\ttrace_funeth_rx(q, rxreq->nsgl, pkt_len, skb->hash, cv);\n\n\tgro_res = skb->data_len ? napi_gro_frags(q->napi) :\n\t\t\t\t  napi_gro_receive(q->napi, skb);\n\tif (gro_res == GRO_MERGED || gro_res == GRO_MERGED_FREE)\n\t\tFUN_QSTAT_INC(q, gro_merged);\n\telse if (gro_res == GRO_HELD)\n\t\tFUN_QSTAT_INC(q, gro_pkts);\n\treturn;\n\nno_mem:\n\tFUN_QSTAT_INC(q, rx_mem_drops);\n\n\t \n\tq->cur_buf->pg_refs++;\n\tfor (i = 0; i < rxreq->nsgl - 1; i++)\n\t\t__free_page(skb_frag_page(frags + i));\n}\n\n \nstatic u16 cqe_phase_mismatch(const struct fun_cqe_info *ci, u16 phase)\n{\n\tu16 sf_p = be16_to_cpu(ci->sf_p);\n\n\treturn (sf_p & 1) ^ phase;\n}\n\n \nstatic int fun_process_cqes(struct funeth_rxq *q, int budget)\n{\n\tstruct funeth_priv *fp = netdev_priv(q->netdev);\n\tstruct funeth_txq **xdpqs, *xdp_q = NULL;\n\n\txdpqs = rcu_dereference_bh(fp->xdpqs);\n\tif (xdpqs)\n\t\txdp_q = xdpqs[smp_processor_id()];\n\n\twhile (budget && !cqe_phase_mismatch(q->next_cqe_info, q->phase)) {\n\t\t \n\t\tdma_rmb();\n\n\t\tfun_handle_cqe_pkt(q, xdp_q);\n\t\tbudget--;\n\t}\n\n\tif (unlikely(q->xdp_flush)) {\n\t\tif (q->xdp_flush & FUN_XDP_FLUSH_TX)\n\t\t\tfun_txq_wr_db(xdp_q);\n\t\tif (q->xdp_flush & FUN_XDP_FLUSH_REDIR)\n\t\t\txdp_do_flush();\n\t\tq->xdp_flush = 0;\n\t}\n\n\treturn budget;\n}\n\n \nint fun_rxq_napi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct fun_irq *irq = container_of(napi, struct fun_irq, napi);\n\tstruct funeth_rxq *q = irq->rxq;\n\tint work_done = budget - fun_process_cqes(q, budget);\n\tu32 cq_db_val = q->cq_head;\n\n\tif (unlikely(work_done >= budget))\n\t\tFUN_QSTAT_INC(q, rx_budget);\n\telse if (napi_complete_done(napi, work_done))\n\t\tcq_db_val |= q->irq_db_val;\n\n\t \n\tif (q->rq_cons - q->rq_cons_db >= q->rq_db_thres) {\n\t\tu64_stats_update_begin(&q->syncp);\n\t\tq->stats.rx_bufs += q->rq_cons - q->rq_cons_db;\n\t\tu64_stats_update_end(&q->syncp);\n\t\tq->rq_cons_db = q->rq_cons;\n\t\twritel((q->rq_cons - 1) & q->rq_mask, q->rq_db);\n\t}\n\n\twritel(cq_db_val, q->cq_db);\n\treturn work_done;\n}\n\n \nstatic void fun_rxq_free_bufs(struct funeth_rxq *q)\n{\n\tstruct funeth_rxbuf *b = q->bufs;\n\tunsigned int i;\n\n\tfor (i = 0; i <= q->rq_mask; i++, b++)\n\t\tfuneth_free_page(q, b);\n\n\tfuneth_free_page(q, &q->spare_buf);\n\tq->cur_buf = NULL;\n}\n\n \nstatic int fun_rxq_alloc_bufs(struct funeth_rxq *q, int node)\n{\n\tstruct funeth_rxbuf *b = q->bufs;\n\tunsigned int i;\n\n\tfor (i = 0; i <= q->rq_mask; i++, b++) {\n\t\tif (funeth_alloc_page(q, b, node, GFP_KERNEL)) {\n\t\t\tfun_rxq_free_bufs(q);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tq->rqes[i] = FUN_EPRQ_RQBUF_INIT(b->dma_addr);\n\t}\n\tq->cur_buf = q->bufs;\n\treturn 0;\n}\n\n \nstatic int fun_rxq_init_cache(struct funeth_rx_cache *c, unsigned int depth,\n\t\t\t      int node)\n{\n\tc->mask = depth - 1;\n\tc->bufs = kvzalloc_node(depth * sizeof(*c->bufs), GFP_KERNEL, node);\n\treturn c->bufs ? 0 : -ENOMEM;\n}\n\n \nstatic void fun_rxq_free_cache(struct funeth_rxq *q)\n{\n\tstruct funeth_rxbuf *b = q->cache.bufs;\n\tunsigned int i;\n\n\tfor (i = 0; i <= q->cache.mask; i++, b++)\n\t\tfuneth_free_page(q, b);\n\n\tkvfree(q->cache.bufs);\n\tq->cache.bufs = NULL;\n}\n\nint fun_rxq_set_bpf(struct funeth_rxq *q, struct bpf_prog *prog)\n{\n\tstruct funeth_priv *fp = netdev_priv(q->netdev);\n\tstruct fun_admin_epcq_req cmd;\n\tu16 headroom;\n\tint err;\n\n\theadroom = prog ? FUN_XDP_HEADROOM : FUN_RX_HEADROOM;\n\tif (headroom != q->headroom) {\n\t\tcmd.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_EPCQ,\n\t\t\t\t\t\t\tsizeof(cmd));\n\t\tcmd.u.modify =\n\t\t\tFUN_ADMIN_EPCQ_MODIFY_REQ_INIT(FUN_ADMIN_SUBOP_MODIFY,\n\t\t\t\t\t\t       0, q->hw_cqid, headroom);\n\t\terr = fun_submit_admin_sync_cmd(fp->fdev, &cmd.common, NULL, 0,\n\t\t\t\t\t\t0);\n\t\tif (err)\n\t\t\treturn err;\n\t\tq->headroom = headroom;\n\t}\n\n\tWRITE_ONCE(q->xdp_prog, prog);\n\treturn 0;\n}\n\n \nstatic struct funeth_rxq *fun_rxq_create_sw(struct net_device *dev,\n\t\t\t\t\t    unsigned int qidx,\n\t\t\t\t\t    unsigned int ncqe,\n\t\t\t\t\t    unsigned int nrqe,\n\t\t\t\t\t    struct fun_irq *irq)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct funeth_rxq *q;\n\tint err = -ENOMEM;\n\tint numa_node;\n\n\tnuma_node = fun_irq_node(irq);\n\tq = kzalloc_node(sizeof(*q), GFP_KERNEL, numa_node);\n\tif (!q)\n\t\tgoto err;\n\n\tq->qidx = qidx;\n\tq->netdev = dev;\n\tq->cq_mask = ncqe - 1;\n\tq->rq_mask = nrqe - 1;\n\tq->numa_node = numa_node;\n\tq->rq_db_thres = nrqe / 4;\n\tu64_stats_init(&q->syncp);\n\tq->dma_dev = &fp->pdev->dev;\n\n\tq->rqes = fun_alloc_ring_mem(q->dma_dev, nrqe, sizeof(*q->rqes),\n\t\t\t\t     sizeof(*q->bufs), false, numa_node,\n\t\t\t\t     &q->rq_dma_addr, (void **)&q->bufs, NULL);\n\tif (!q->rqes)\n\t\tgoto free_q;\n\n\tq->cqes = fun_alloc_ring_mem(q->dma_dev, ncqe, FUNETH_CQE_SIZE, 0,\n\t\t\t\t     false, numa_node, &q->cq_dma_addr, NULL,\n\t\t\t\t     NULL);\n\tif (!q->cqes)\n\t\tgoto free_rqes;\n\n\terr = fun_rxq_init_cache(&q->cache, nrqe, numa_node);\n\tif (err)\n\t\tgoto free_cqes;\n\n\terr = fun_rxq_alloc_bufs(q, numa_node);\n\tif (err)\n\t\tgoto free_cache;\n\n\tq->stats.rx_bufs = q->rq_mask;\n\tq->init_state = FUN_QSTATE_INIT_SW;\n\treturn q;\n\nfree_cache:\n\tfun_rxq_free_cache(q);\nfree_cqes:\n\tdma_free_coherent(q->dma_dev, ncqe * FUNETH_CQE_SIZE, q->cqes,\n\t\t\t  q->cq_dma_addr);\nfree_rqes:\n\tfun_free_ring_mem(q->dma_dev, nrqe, sizeof(*q->rqes), false, q->rqes,\n\t\t\t  q->rq_dma_addr, q->bufs);\nfree_q:\n\tkfree(q);\nerr:\n\tnetdev_err(dev, \"Unable to allocate memory for Rx queue %u\\n\", qidx);\n\treturn ERR_PTR(err);\n}\n\nstatic void fun_rxq_free_sw(struct funeth_rxq *q)\n{\n\tstruct funeth_priv *fp = netdev_priv(q->netdev);\n\n\tfun_rxq_free_cache(q);\n\tfun_rxq_free_bufs(q);\n\tfun_free_ring_mem(q->dma_dev, q->rq_mask + 1, sizeof(*q->rqes), false,\n\t\t\t  q->rqes, q->rq_dma_addr, q->bufs);\n\tdma_free_coherent(q->dma_dev, (q->cq_mask + 1) * FUNETH_CQE_SIZE,\n\t\t\t  q->cqes, q->cq_dma_addr);\n\n\t \n\tfp->rx_packets += q->stats.rx_pkts;\n\tfp->rx_bytes   += q->stats.rx_bytes;\n\tfp->rx_dropped += q->stats.rx_map_err + q->stats.rx_mem_drops;\n\n\tkfree(q);\n}\n\n \nint fun_rxq_create_dev(struct funeth_rxq *q, struct fun_irq *irq)\n{\n\tstruct funeth_priv *fp = netdev_priv(q->netdev);\n\tunsigned int ncqe = q->cq_mask + 1;\n\tunsigned int nrqe = q->rq_mask + 1;\n\tint err;\n\n\terr = xdp_rxq_info_reg(&q->xdp_rxq, q->netdev, q->qidx,\n\t\t\t       irq->napi.napi_id);\n\tif (err)\n\t\tgoto out;\n\n\terr = xdp_rxq_info_reg_mem_model(&q->xdp_rxq, MEM_TYPE_PAGE_SHARED,\n\t\t\t\t\t NULL);\n\tif (err)\n\t\tgoto xdp_unreg;\n\n\tq->phase = 1;\n\tq->irq_cnt = 0;\n\tq->cq_head = 0;\n\tq->rq_cons = 0;\n\tq->rq_cons_db = 0;\n\tq->buf_offset = 0;\n\tq->napi = &irq->napi;\n\tq->irq_db_val = fp->cq_irq_db;\n\tq->next_cqe_info = cqe_to_info(q->cqes);\n\n\tq->xdp_prog = fp->xdp_prog;\n\tq->headroom = fp->xdp_prog ? FUN_XDP_HEADROOM : FUN_RX_HEADROOM;\n\n\terr = fun_sq_create(fp->fdev, FUN_ADMIN_RES_CREATE_FLAG_ALLOCATOR |\n\t\t\t    FUN_ADMIN_EPSQ_CREATE_FLAG_RQ, 0,\n\t\t\t    FUN_HCI_ID_INVALID, 0, nrqe, q->rq_dma_addr, 0, 0,\n\t\t\t    0, 0, fp->fdev->kern_end_qid, PAGE_SHIFT,\n\t\t\t    &q->hw_sqid, &q->rq_db);\n\tif (err)\n\t\tgoto xdp_unreg;\n\n\terr = fun_cq_create(fp->fdev, FUN_ADMIN_RES_CREATE_FLAG_ALLOCATOR |\n\t\t\t    FUN_ADMIN_EPCQ_CREATE_FLAG_RQ, 0,\n\t\t\t    q->hw_sqid, ilog2(FUNETH_CQE_SIZE), ncqe,\n\t\t\t    q->cq_dma_addr, q->headroom, FUN_RX_TAILROOM, 0, 0,\n\t\t\t    irq->irq_idx, 0, fp->fdev->kern_end_qid,\n\t\t\t    &q->hw_cqid, &q->cq_db);\n\tif (err)\n\t\tgoto free_rq;\n\n\tirq->rxq = q;\n\twritel(q->rq_mask, q->rq_db);\n\tq->init_state = FUN_QSTATE_INIT_FULL;\n\n\tnetif_info(fp, ifup, q->netdev,\n\t\t   \"Rx queue %u, depth %u/%u, HW qid %u/%u, IRQ idx %u, node %d, headroom %u\\n\",\n\t\t   q->qidx, ncqe, nrqe, q->hw_cqid, q->hw_sqid, irq->irq_idx,\n\t\t   q->numa_node, q->headroom);\n\treturn 0;\n\nfree_rq:\n\tfun_destroy_sq(fp->fdev, q->hw_sqid);\nxdp_unreg:\n\txdp_rxq_info_unreg(&q->xdp_rxq);\nout:\n\tnetdev_err(q->netdev,\n\t\t   \"Failed to create Rx queue %u on device, error %d\\n\",\n\t\t   q->qidx, err);\n\treturn err;\n}\n\nstatic void fun_rxq_free_dev(struct funeth_rxq *q)\n{\n\tstruct funeth_priv *fp = netdev_priv(q->netdev);\n\tstruct fun_irq *irq;\n\n\tif (q->init_state < FUN_QSTATE_INIT_FULL)\n\t\treturn;\n\n\tirq = container_of(q->napi, struct fun_irq, napi);\n\tnetif_info(fp, ifdown, q->netdev,\n\t\t   \"Freeing Rx queue %u (id %u/%u), IRQ %u\\n\",\n\t\t   q->qidx, q->hw_cqid, q->hw_sqid, irq->irq_idx);\n\n\tirq->rxq = NULL;\n\txdp_rxq_info_unreg(&q->xdp_rxq);\n\tfun_destroy_sq(fp->fdev, q->hw_sqid);\n\tfun_destroy_cq(fp->fdev, q->hw_cqid);\n\tq->init_state = FUN_QSTATE_INIT_SW;\n}\n\n \nint funeth_rxq_create(struct net_device *dev, unsigned int qidx,\n\t\t      unsigned int ncqe, unsigned int nrqe, struct fun_irq *irq,\n\t\t      int state, struct funeth_rxq **qp)\n{\n\tstruct funeth_rxq *q = *qp;\n\tint err;\n\n\tif (!q) {\n\t\tq = fun_rxq_create_sw(dev, qidx, ncqe, nrqe, irq);\n\t\tif (IS_ERR(q))\n\t\t\treturn PTR_ERR(q);\n\t}\n\n\tif (q->init_state >= state)\n\t\tgoto out;\n\n\terr = fun_rxq_create_dev(q, irq);\n\tif (err) {\n\t\tif (!*qp)\n\t\t\tfun_rxq_free_sw(q);\n\t\treturn err;\n\t}\n\nout:\n\t*qp = q;\n\treturn 0;\n}\n\n \nstruct funeth_rxq *funeth_rxq_free(struct funeth_rxq *q, int state)\n{\n\tif (state < FUN_QSTATE_INIT_FULL)\n\t\tfun_rxq_free_dev(q);\n\n\tif (state == FUN_QSTATE_DESTROYED) {\n\t\tfun_rxq_free_sw(q);\n\t\tq = NULL;\n\t}\n\n\treturn q;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}