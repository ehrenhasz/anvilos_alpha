{
  "module_name": "funeth_main.c",
  "hash_id": "a81ca6d3cb0bddb7208b65abb095cbbced3ea7039c15706afb93cdd9a7979957",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/fungible/funeth/funeth_main.c",
  "human_readable_source": "\n\n#include <linux/bpf.h>\n#include <linux/crash_dump.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/filter.h>\n#include <linux/idr.h>\n#include <linux/if_vlan.h>\n#include <linux/module.h>\n#include <linux/netdevice.h>\n#include <linux/pci.h>\n#include <linux/rtnetlink.h>\n#include <linux/inetdevice.h>\n\n#include \"funeth.h\"\n#include \"funeth_devlink.h\"\n#include \"funeth_ktls.h\"\n#include \"fun_port.h\"\n#include \"fun_queue.h\"\n#include \"funeth_txrx.h\"\n\n#define ADMIN_SQ_DEPTH 32\n#define ADMIN_CQ_DEPTH 64\n#define ADMIN_RQ_DEPTH 16\n\n \n#define FUN_DFLT_QUEUES 16U\n\nenum {\n\tFUN_SERV_RES_CHANGE = FUN_SERV_FIRST_AVAIL,\n\tFUN_SERV_DEL_PORTS,\n};\n\nstatic const struct pci_device_id funeth_id_table[] = {\n\t{ PCI_VDEVICE(FUNGIBLE, 0x0101) },\n\t{ PCI_VDEVICE(FUNGIBLE, 0x0181) },\n\t{ 0, }\n};\n\n \nstatic int fun_port_write_cmds(struct funeth_priv *fp, unsigned int n,\n\t\t\t       const int *keys, const u64 *data)\n{\n\tunsigned int cmd_size, i;\n\tunion {\n\t\tstruct fun_admin_port_req req;\n\t\tstruct fun_admin_port_rsp rsp;\n\t\tu8 v[ADMIN_SQE_SIZE];\n\t} cmd;\n\n\tcmd_size = offsetof(struct fun_admin_port_req, u.write.write48) +\n\t\tn * sizeof(struct fun_admin_write48_req);\n\tif (cmd_size > sizeof(cmd) || cmd_size > ADMIN_RSP_MAX_LEN)\n\t\treturn -EINVAL;\n\n\tcmd.req.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_PORT,\n\t\t\t\t\t\t    cmd_size);\n\tcmd.req.u.write =\n\t\tFUN_ADMIN_PORT_WRITE_REQ_INIT(FUN_ADMIN_SUBOP_WRITE, 0,\n\t\t\t\t\t      fp->netdev->dev_port);\n\tfor (i = 0; i < n; i++)\n\t\tcmd.req.u.write.write48[i] =\n\t\t\tFUN_ADMIN_WRITE48_REQ_INIT(keys[i], data[i]);\n\n\treturn fun_submit_admin_sync_cmd(fp->fdev, &cmd.req.common,\n\t\t\t\t\t &cmd.rsp, cmd_size, 0);\n}\n\nint fun_port_write_cmd(struct funeth_priv *fp, int key, u64 data)\n{\n\treturn fun_port_write_cmds(fp, 1, &key, &data);\n}\n\n \nstatic int fun_port_read_cmds(struct funeth_priv *fp, unsigned int n,\n\t\t\t      const int *keys, u64 *data)\n{\n\tconst struct fun_admin_read48_rsp *r48rsp;\n\tunsigned int cmd_size, i;\n\tint rc;\n\tunion {\n\t\tstruct fun_admin_port_req req;\n\t\tstruct fun_admin_port_rsp rsp;\n\t\tu8 v[ADMIN_SQE_SIZE];\n\t} cmd;\n\n\tcmd_size = offsetof(struct fun_admin_port_req, u.read.read48) +\n\t\tn * sizeof(struct fun_admin_read48_req);\n\tif (cmd_size > sizeof(cmd) || cmd_size > ADMIN_RSP_MAX_LEN)\n\t\treturn -EINVAL;\n\n\tcmd.req.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_PORT,\n\t\t\t\t\t\t    cmd_size);\n\tcmd.req.u.read =\n\t\tFUN_ADMIN_PORT_READ_REQ_INIT(FUN_ADMIN_SUBOP_READ, 0,\n\t\t\t\t\t     fp->netdev->dev_port);\n\tfor (i = 0; i < n; i++)\n\t\tcmd.req.u.read.read48[i] = FUN_ADMIN_READ48_REQ_INIT(keys[i]);\n\n\trc = fun_submit_admin_sync_cmd(fp->fdev, &cmd.req.common,\n\t\t\t\t       &cmd.rsp, cmd_size, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tfor (r48rsp = cmd.rsp.u.read.read48, i = 0; i < n; i++, r48rsp++) {\n\t\tdata[i] = FUN_ADMIN_READ48_RSP_DATA_G(r48rsp->key_to_data);\n\t\tdev_dbg(fp->fdev->dev,\n\t\t\t\"port_read_rsp lport=%u (key_to_data=0x%llx) key=%d data:%lld retval:%lld\",\n\t\t\tfp->lport, r48rsp->key_to_data, keys[i], data[i],\n\t\t\tFUN_ADMIN_READ48_RSP_RET_G(r48rsp->key_to_data));\n\t}\n\treturn 0;\n}\n\nint fun_port_read_cmd(struct funeth_priv *fp, int key, u64 *data)\n{\n\treturn fun_port_read_cmds(fp, 1, &key, data);\n}\n\nstatic void fun_report_link(struct net_device *netdev)\n{\n\tif (netif_carrier_ok(netdev)) {\n\t\tconst struct funeth_priv *fp = netdev_priv(netdev);\n\t\tconst char *fec = \"\", *pause = \"\";\n\t\tint speed = fp->link_speed;\n\t\tchar unit = 'M';\n\n\t\tif (fp->link_speed >= SPEED_1000) {\n\t\t\tspeed /= 1000;\n\t\t\tunit = 'G';\n\t\t}\n\n\t\tif (fp->active_fec & FUN_PORT_FEC_RS)\n\t\t\tfec = \", RS-FEC\";\n\t\telse if (fp->active_fec & FUN_PORT_FEC_FC)\n\t\t\tfec = \", BASER-FEC\";\n\n\t\tif ((fp->active_fc & FUN_PORT_CAP_PAUSE_MASK) == FUN_PORT_CAP_PAUSE_MASK)\n\t\t\tpause = \", Tx/Rx PAUSE\";\n\t\telse if (fp->active_fc & FUN_PORT_CAP_RX_PAUSE)\n\t\t\tpause = \", Rx PAUSE\";\n\t\telse if (fp->active_fc & FUN_PORT_CAP_TX_PAUSE)\n\t\t\tpause = \", Tx PAUSE\";\n\n\t\tnetdev_info(netdev, \"Link up at %d %cb/s full-duplex%s%s\\n\",\n\t\t\t    speed, unit, pause, fec);\n\t} else {\n\t\tnetdev_info(netdev, \"Link down\\n\");\n\t}\n}\n\nstatic int fun_adi_write(struct fun_dev *fdev, enum fun_admin_adi_attr attr,\n\t\t\t unsigned int adi_id, const struct fun_adi_param *param)\n{\n\tstruct fun_admin_adi_req req = {\n\t\t.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_ADI,\n\t\t\t\t\t\t     sizeof(req)),\n\t\t.u.write.subop = FUN_ADMIN_SUBOP_WRITE,\n\t\t.u.write.attribute = attr,\n\t\t.u.write.id = cpu_to_be32(adi_id),\n\t\t.u.write.param = *param\n\t};\n\n\treturn fun_submit_admin_sync_cmd(fdev, &req.common, NULL, 0, 0);\n}\n\n \nint fun_config_rss(struct net_device *dev, int algo, const u8 *key,\n\t\t   const u32 *qtable, u8 op)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tunsigned int table_len = fp->indir_table_nentries;\n\tunsigned int len = FUN_ETH_RSS_MAX_KEY_SIZE + sizeof(u32) * table_len;\n\tstruct funeth_rxq **rxqs = rtnl_dereference(fp->rxqs);\n\tunion {\n\t\tstruct {\n\t\t\tstruct fun_admin_rss_req req;\n\t\t\tstruct fun_dataop_gl gl;\n\t\t};\n\t\tstruct fun_admin_generic_create_rsp rsp;\n\t} cmd;\n\t__be32 *indir_tab;\n\tu16 flags;\n\tint rc;\n\n\tif (op != FUN_ADMIN_SUBOP_CREATE && fp->rss_hw_id == FUN_HCI_ID_INVALID)\n\t\treturn -EINVAL;\n\n\tflags = op == FUN_ADMIN_SUBOP_CREATE ?\n\t\t\tFUN_ADMIN_RES_CREATE_FLAG_ALLOCATOR : 0;\n\tcmd.req.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_RSS,\n\t\t\t\t\t\t    sizeof(cmd));\n\tcmd.req.u.create =\n\t\tFUN_ADMIN_RSS_CREATE_REQ_INIT(op, flags, fp->rss_hw_id,\n\t\t\t\t\t      dev->dev_port, algo,\n\t\t\t\t\t      FUN_ETH_RSS_MAX_KEY_SIZE,\n\t\t\t\t\t      table_len, 0,\n\t\t\t\t\t      FUN_ETH_RSS_MAX_KEY_SIZE);\n\tcmd.req.u.create.dataop = FUN_DATAOP_HDR_INIT(1, 0, 1, 0, len);\n\tfun_dataop_gl_init(&cmd.gl, 0, 0, len, fp->rss_dma_addr);\n\n\t \n\tmemcpy(fp->rss_cfg, key, FUN_ETH_RSS_MAX_KEY_SIZE);\n\tindir_tab = fp->rss_cfg + FUN_ETH_RSS_MAX_KEY_SIZE;\n\tfor (rc = 0; rc < table_len; rc++)\n\t\t*indir_tab++ = cpu_to_be32(rxqs[*qtable++]->hw_cqid);\n\n\trc = fun_submit_admin_sync_cmd(fp->fdev, &cmd.req.common,\n\t\t\t\t       &cmd.rsp, sizeof(cmd.rsp), 0);\n\tif (!rc && op == FUN_ADMIN_SUBOP_CREATE)\n\t\tfp->rss_hw_id = be32_to_cpu(cmd.rsp.id);\n\treturn rc;\n}\n\n \nstatic void fun_destroy_rss(struct funeth_priv *fp)\n{\n\tif (fp->rss_hw_id != FUN_HCI_ID_INVALID) {\n\t\tfun_res_destroy(fp->fdev, FUN_ADMIN_OP_RSS, 0, fp->rss_hw_id);\n\t\tfp->rss_hw_id = FUN_HCI_ID_INVALID;\n\t}\n}\n\nstatic void fun_irq_aff_notify(struct irq_affinity_notify *notify,\n\t\t\t       const cpumask_t *mask)\n{\n\tstruct fun_irq *p = container_of(notify, struct fun_irq, aff_notify);\n\n\tcpumask_copy(&p->affinity_mask, mask);\n}\n\nstatic void fun_irq_aff_release(struct kref __always_unused *ref)\n{\n}\n\n \nstatic struct fun_irq *fun_alloc_qirq(struct funeth_priv *fp, unsigned int idx,\n\t\t\t\t      int node, unsigned int xa_idx_offset)\n{\n\tstruct fun_irq *irq;\n\tint cpu, res;\n\n\tcpu = cpumask_local_spread(idx, node);\n\tnode = cpu_to_mem(cpu);\n\n\tirq = kzalloc_node(sizeof(*irq), GFP_KERNEL, node);\n\tif (!irq)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tres = fun_reserve_irqs(fp->fdev, 1, &irq->irq_idx);\n\tif (res != 1)\n\t\tgoto free_irq;\n\n\tres = xa_insert(&fp->irqs, idx + xa_idx_offset, irq, GFP_KERNEL);\n\tif (res)\n\t\tgoto release_irq;\n\n\tirq->irq = pci_irq_vector(fp->pdev, irq->irq_idx);\n\tcpumask_set_cpu(cpu, &irq->affinity_mask);\n\tirq->aff_notify.notify = fun_irq_aff_notify;\n\tirq->aff_notify.release = fun_irq_aff_release;\n\tirq->state = FUN_IRQ_INIT;\n\treturn irq;\n\nrelease_irq:\n\tfun_release_irqs(fp->fdev, 1, &irq->irq_idx);\nfree_irq:\n\tkfree(irq);\n\treturn ERR_PTR(res);\n}\n\nstatic void fun_free_qirq(struct funeth_priv *fp, struct fun_irq *irq)\n{\n\tnetif_napi_del(&irq->napi);\n\tfun_release_irqs(fp->fdev, 1, &irq->irq_idx);\n\tkfree(irq);\n}\n\n \nstatic void fun_prune_queue_irqs(struct net_device *dev)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tunsigned int nreleased = 0;\n\tstruct fun_irq *irq;\n\tunsigned long idx;\n\n\txa_for_each(&fp->irqs, idx, irq) {\n\t\tif (irq->txq || irq->rxq)   \n\t\t\tcontinue;\n\n\t\txa_erase(&fp->irqs, idx);\n\t\tfun_free_qirq(fp, irq);\n\t\tnreleased++;\n\t\tif (idx < fp->rx_irq_ofst)\n\t\t\tfp->num_tx_irqs--;\n\t\telse\n\t\t\tfp->num_rx_irqs--;\n\t}\n\tnetif_info(fp, intr, dev, \"Released %u queue IRQs\\n\", nreleased);\n}\n\n \nstatic int fun_alloc_queue_irqs(struct net_device *dev, unsigned int ntx,\n\t\t\t\tunsigned int nrx)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tint node = dev_to_node(&fp->pdev->dev);\n\tstruct fun_irq *irq;\n\tunsigned int i;\n\n\tfor (i = fp->num_tx_irqs; i < ntx; i++) {\n\t\tirq = fun_alloc_qirq(fp, i, node, 0);\n\t\tif (IS_ERR(irq))\n\t\t\treturn PTR_ERR(irq);\n\n\t\tfp->num_tx_irqs++;\n\t\tnetif_napi_add_tx(dev, &irq->napi, fun_txq_napi_poll);\n\t}\n\n\tfor (i = fp->num_rx_irqs; i < nrx; i++) {\n\t\tirq = fun_alloc_qirq(fp, i, node, fp->rx_irq_ofst);\n\t\tif (IS_ERR(irq))\n\t\t\treturn PTR_ERR(irq);\n\n\t\tfp->num_rx_irqs++;\n\t\tnetif_napi_add(dev, &irq->napi, fun_rxq_napi_poll);\n\t}\n\n\tnetif_info(fp, intr, dev, \"Reserved %u/%u IRQs for Tx/Rx queues\\n\",\n\t\t   ntx, nrx);\n\treturn 0;\n}\n\nstatic void free_txqs(struct funeth_txq **txqs, unsigned int nqs,\n\t\t      unsigned int start, int state)\n{\n\tunsigned int i;\n\n\tfor (i = start; i < nqs && txqs[i]; i++)\n\t\ttxqs[i] = funeth_txq_free(txqs[i], state);\n}\n\nstatic int alloc_txqs(struct net_device *dev, struct funeth_txq **txqs,\n\t\t      unsigned int nqs, unsigned int depth, unsigned int start,\n\t\t      int state)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tunsigned int i;\n\tint err;\n\n\tfor (i = start; i < nqs; i++) {\n\t\terr = funeth_txq_create(dev, i, depth, xa_load(&fp->irqs, i),\n\t\t\t\t\tstate, &txqs[i]);\n\t\tif (err) {\n\t\t\tfree_txqs(txqs, nqs, start, FUN_QSTATE_DESTROYED);\n\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void free_rxqs(struct funeth_rxq **rxqs, unsigned int nqs,\n\t\t      unsigned int start, int state)\n{\n\tunsigned int i;\n\n\tfor (i = start; i < nqs && rxqs[i]; i++)\n\t\trxqs[i] = funeth_rxq_free(rxqs[i], state);\n}\n\nstatic int alloc_rxqs(struct net_device *dev, struct funeth_rxq **rxqs,\n\t\t      unsigned int nqs, unsigned int ncqe, unsigned int nrqe,\n\t\t      unsigned int start, int state)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tunsigned int i;\n\tint err;\n\n\tfor (i = start; i < nqs; i++) {\n\t\terr = funeth_rxq_create(dev, i, ncqe, nrqe,\n\t\t\t\t\txa_load(&fp->irqs, i + fp->rx_irq_ofst),\n\t\t\t\t\tstate, &rxqs[i]);\n\t\tif (err) {\n\t\t\tfree_rxqs(rxqs, nqs, start, FUN_QSTATE_DESTROYED);\n\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void free_xdpqs(struct funeth_txq **xdpqs, unsigned int nqs,\n\t\t       unsigned int start, int state)\n{\n\tunsigned int i;\n\n\tfor (i = start; i < nqs && xdpqs[i]; i++)\n\t\txdpqs[i] = funeth_txq_free(xdpqs[i], state);\n\n\tif (state == FUN_QSTATE_DESTROYED)\n\t\tkfree(xdpqs);\n}\n\nstatic struct funeth_txq **alloc_xdpqs(struct net_device *dev, unsigned int nqs,\n\t\t\t\t       unsigned int depth, unsigned int start,\n\t\t\t\t       int state)\n{\n\tstruct funeth_txq **xdpqs;\n\tunsigned int i;\n\tint err;\n\n\txdpqs = kcalloc(nqs, sizeof(*xdpqs), GFP_KERNEL);\n\tif (!xdpqs)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfor (i = start; i < nqs; i++) {\n\t\terr = funeth_txq_create(dev, i, depth, NULL, state, &xdpqs[i]);\n\t\tif (err) {\n\t\t\tfree_xdpqs(xdpqs, nqs, start, FUN_QSTATE_DESTROYED);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\treturn xdpqs;\n}\n\nstatic void fun_free_rings(struct net_device *netdev, struct fun_qset *qset)\n{\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\tstruct funeth_txq **xdpqs = qset->xdpqs;\n\tstruct funeth_rxq **rxqs = qset->rxqs;\n\n\t \n\tif (!rxqs) {\n\t\trxqs = rtnl_dereference(fp->rxqs);\n\t\txdpqs = rtnl_dereference(fp->xdpqs);\n\t\tqset->txqs = fp->txqs;\n\t\tqset->nrxqs = netdev->real_num_rx_queues;\n\t\tqset->ntxqs = netdev->real_num_tx_queues;\n\t\tqset->nxdpqs = fp->num_xdpqs;\n\t}\n\tif (!rxqs)\n\t\treturn;\n\n\tif (rxqs == rtnl_dereference(fp->rxqs)) {\n\t\trcu_assign_pointer(fp->rxqs, NULL);\n\t\trcu_assign_pointer(fp->xdpqs, NULL);\n\t\tsynchronize_net();\n\t\tfp->txqs = NULL;\n\t}\n\n\tfree_rxqs(rxqs, qset->nrxqs, qset->rxq_start, qset->state);\n\tfree_txqs(qset->txqs, qset->ntxqs, qset->txq_start, qset->state);\n\tfree_xdpqs(xdpqs, qset->nxdpqs, qset->xdpq_start, qset->state);\n\tif (qset->state == FUN_QSTATE_DESTROYED)\n\t\tkfree(rxqs);\n\n\t \n\tqset->rxqs = rxqs;\n\tqset->xdpqs = xdpqs;\n}\n\nstatic int fun_alloc_rings(struct net_device *netdev, struct fun_qset *qset)\n{\n\tstruct funeth_txq **xdpqs = NULL, **txqs;\n\tstruct funeth_rxq **rxqs;\n\tint err;\n\n\terr = fun_alloc_queue_irqs(netdev, qset->ntxqs, qset->nrxqs);\n\tif (err)\n\t\treturn err;\n\n\trxqs = kcalloc(qset->ntxqs + qset->nrxqs, sizeof(*rxqs), GFP_KERNEL);\n\tif (!rxqs)\n\t\treturn -ENOMEM;\n\n\tif (qset->nxdpqs) {\n\t\txdpqs = alloc_xdpqs(netdev, qset->nxdpqs, qset->sq_depth,\n\t\t\t\t    qset->xdpq_start, qset->state);\n\t\tif (IS_ERR(xdpqs)) {\n\t\t\terr = PTR_ERR(xdpqs);\n\t\t\tgoto free_qvec;\n\t\t}\n\t}\n\n\ttxqs = (struct funeth_txq **)&rxqs[qset->nrxqs];\n\terr = alloc_txqs(netdev, txqs, qset->ntxqs, qset->sq_depth,\n\t\t\t qset->txq_start, qset->state);\n\tif (err)\n\t\tgoto free_xdpqs;\n\n\terr = alloc_rxqs(netdev, rxqs, qset->nrxqs, qset->cq_depth,\n\t\t\t qset->rq_depth, qset->rxq_start, qset->state);\n\tif (err)\n\t\tgoto free_txqs;\n\n\tqset->rxqs = rxqs;\n\tqset->txqs = txqs;\n\tqset->xdpqs = xdpqs;\n\treturn 0;\n\nfree_txqs:\n\tfree_txqs(txqs, qset->ntxqs, qset->txq_start, FUN_QSTATE_DESTROYED);\nfree_xdpqs:\n\tfree_xdpqs(xdpqs, qset->nxdpqs, qset->xdpq_start, FUN_QSTATE_DESTROYED);\nfree_qvec:\n\tkfree(rxqs);\n\treturn err;\n}\n\n \nstatic int fun_advance_ring_state(struct net_device *dev, struct fun_qset *qset)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tint i, err;\n\n\tfor (i = 0; i < qset->nrxqs; i++) {\n\t\terr = fun_rxq_create_dev(qset->rxqs[i],\n\t\t\t\t\t xa_load(&fp->irqs,\n\t\t\t\t\t\t i + fp->rx_irq_ofst));\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < qset->ntxqs; i++) {\n\t\terr = fun_txq_create_dev(qset->txqs[i], xa_load(&fp->irqs, i));\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < qset->nxdpqs; i++) {\n\t\terr = fun_txq_create_dev(qset->xdpqs[i], NULL);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\treturn 0;\n\nout:\n\tfun_free_rings(dev, qset);\n\treturn err;\n}\n\nstatic int fun_port_create(struct net_device *netdev)\n{\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\tunion {\n\t\tstruct fun_admin_port_req req;\n\t\tstruct fun_admin_port_rsp rsp;\n\t} cmd;\n\tint rc;\n\n\tif (fp->lport != INVALID_LPORT)\n\t\treturn 0;\n\n\tcmd.req.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_PORT,\n\t\t\t\t\t\t    sizeof(cmd.req));\n\tcmd.req.u.create =\n\t\tFUN_ADMIN_PORT_CREATE_REQ_INIT(FUN_ADMIN_SUBOP_CREATE, 0,\n\t\t\t\t\t       netdev->dev_port);\n\n\trc = fun_submit_admin_sync_cmd(fp->fdev, &cmd.req.common, &cmd.rsp,\n\t\t\t\t       sizeof(cmd.rsp), 0);\n\n\tif (!rc)\n\t\tfp->lport = be16_to_cpu(cmd.rsp.u.create.lport);\n\treturn rc;\n}\n\nstatic int fun_port_destroy(struct net_device *netdev)\n{\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\n\tif (fp->lport == INVALID_LPORT)\n\t\treturn 0;\n\n\tfp->lport = INVALID_LPORT;\n\treturn fun_res_destroy(fp->fdev, FUN_ADMIN_OP_PORT, 0,\n\t\t\t       netdev->dev_port);\n}\n\nstatic int fun_eth_create(struct funeth_priv *fp)\n{\n\tunion {\n\t\tstruct fun_admin_eth_req req;\n\t\tstruct fun_admin_generic_create_rsp rsp;\n\t} cmd;\n\tint rc;\n\n\tcmd.req.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_ETH,\n\t\t\t\t\t\t    sizeof(cmd.req));\n\tcmd.req.u.create = FUN_ADMIN_ETH_CREATE_REQ_INIT(\n\t\t\t\tFUN_ADMIN_SUBOP_CREATE,\n\t\t\t\tFUN_ADMIN_RES_CREATE_FLAG_ALLOCATOR,\n\t\t\t\t0, fp->netdev->dev_port);\n\n\trc = fun_submit_admin_sync_cmd(fp->fdev, &cmd.req.common, &cmd.rsp,\n\t\t\t\t       sizeof(cmd.rsp), 0);\n\treturn rc ? rc : be32_to_cpu(cmd.rsp.id);\n}\n\nstatic int fun_vi_create(struct funeth_priv *fp)\n{\n\tstruct fun_admin_vi_req req = {\n\t\t.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_VI,\n\t\t\t\t\t\t     sizeof(req)),\n\t\t.u.create = FUN_ADMIN_VI_CREATE_REQ_INIT(FUN_ADMIN_SUBOP_CREATE,\n\t\t\t\t\t\t\t 0,\n\t\t\t\t\t\t\t fp->netdev->dev_port,\n\t\t\t\t\t\t\t fp->netdev->dev_port)\n\t};\n\n\treturn fun_submit_admin_sync_cmd(fp->fdev, &req.common, NULL, 0, 0);\n}\n\n \nint fun_create_and_bind_tx(struct funeth_priv *fp, u32 sqid)\n{\n\tint rc, ethid;\n\n\tethid = fun_eth_create(fp);\n\tif (ethid >= 0) {\n\t\trc = fun_bind(fp->fdev, FUN_ADMIN_BIND_TYPE_EPSQ, sqid,\n\t\t\t      FUN_ADMIN_BIND_TYPE_ETH, ethid);\n\t\tif (rc) {\n\t\t\tfun_res_destroy(fp->fdev, FUN_ADMIN_OP_ETH, 0, ethid);\n\t\t\tethid = rc;\n\t\t}\n\t}\n\treturn ethid;\n}\n\nstatic irqreturn_t fun_queue_irq_handler(int irq, void *data)\n{\n\tstruct fun_irq *p = data;\n\n\tif (p->rxq) {\n\t\tprefetch(p->rxq->next_cqe_info);\n\t\tp->rxq->irq_cnt++;\n\t}\n\tnapi_schedule_irqoff(&p->napi);\n\treturn IRQ_HANDLED;\n}\n\nstatic int fun_enable_irqs(struct net_device *dev)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tunsigned long idx, last;\n\tunsigned int qidx;\n\tstruct fun_irq *p;\n\tconst char *qtype;\n\tint err;\n\n\txa_for_each(&fp->irqs, idx, p) {\n\t\tif (p->txq) {\n\t\t\tqtype = \"tx\";\n\t\t\tqidx = p->txq->qidx;\n\t\t} else if (p->rxq) {\n\t\t\tqtype = \"rx\";\n\t\t\tqidx = p->rxq->qidx;\n\t\t} else {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (p->state != FUN_IRQ_INIT)\n\t\t\tcontinue;\n\n\t\tsnprintf(p->name, sizeof(p->name) - 1, \"%s-%s-%u\", dev->name,\n\t\t\t qtype, qidx);\n\t\terr = request_irq(p->irq, fun_queue_irq_handler, 0, p->name, p);\n\t\tif (err) {\n\t\t\tnetdev_err(dev, \"Failed to allocate IRQ %u, err %d\\n\",\n\t\t\t\t   p->irq, err);\n\t\t\tgoto unroll;\n\t\t}\n\t\tp->state = FUN_IRQ_REQUESTED;\n\t}\n\n\txa_for_each(&fp->irqs, idx, p) {\n\t\tif (p->state != FUN_IRQ_REQUESTED)\n\t\t\tcontinue;\n\t\tirq_set_affinity_notifier(p->irq, &p->aff_notify);\n\t\tirq_set_affinity_and_hint(p->irq, &p->affinity_mask);\n\t\tnapi_enable(&p->napi);\n\t\tp->state = FUN_IRQ_ENABLED;\n\t}\n\n\treturn 0;\n\nunroll:\n\tlast = idx - 1;\n\txa_for_each_range(&fp->irqs, idx, p, 0, last)\n\t\tif (p->state == FUN_IRQ_REQUESTED) {\n\t\t\tfree_irq(p->irq, p);\n\t\t\tp->state = FUN_IRQ_INIT;\n\t\t}\n\n\treturn err;\n}\n\nstatic void fun_disable_one_irq(struct fun_irq *irq)\n{\n\tnapi_disable(&irq->napi);\n\tirq_set_affinity_notifier(irq->irq, NULL);\n\tirq_update_affinity_hint(irq->irq, NULL);\n\tfree_irq(irq->irq, irq);\n\tirq->state = FUN_IRQ_INIT;\n}\n\nstatic void fun_disable_irqs(struct net_device *dev)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct fun_irq *p;\n\tunsigned long idx;\n\n\txa_for_each(&fp->irqs, idx, p)\n\t\tif (p->state == FUN_IRQ_ENABLED)\n\t\t\tfun_disable_one_irq(p);\n}\n\nstatic void fun_down(struct net_device *dev, struct fun_qset *qset)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\n\t \n\tif (!rcu_access_pointer(fp->rxqs))\n\t\treturn;\n\n\t \n\tif (fp->txqs[0]->init_state >= FUN_QSTATE_INIT_FULL) {\n\t\tnetif_info(fp, ifdown, dev,\n\t\t\t   \"Tearing down data path on device\\n\");\n\t\tfun_port_write_cmd(fp, FUN_ADMIN_PORT_KEY_DISABLE, 0);\n\n\t\tnetif_carrier_off(dev);\n\t\tnetif_tx_disable(dev);\n\n\t\tfun_destroy_rss(fp);\n\t\tfun_res_destroy(fp->fdev, FUN_ADMIN_OP_VI, 0, dev->dev_port);\n\t\tfun_disable_irqs(dev);\n\t}\n\n\tfun_free_rings(dev, qset);\n}\n\nstatic int fun_up(struct net_device *dev, struct fun_qset *qset)\n{\n\tstatic const int port_keys[] = {\n\t\tFUN_ADMIN_PORT_KEY_STATS_DMA_LOW,\n\t\tFUN_ADMIN_PORT_KEY_STATS_DMA_HIGH,\n\t\tFUN_ADMIN_PORT_KEY_ENABLE\n\t};\n\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tu64 vals[] = {\n\t\tlower_32_bits(fp->stats_dma_addr),\n\t\tupper_32_bits(fp->stats_dma_addr),\n\t\tFUN_PORT_FLAG_ENABLE_NOTIFY\n\t};\n\tint err;\n\n\tnetif_info(fp, ifup, dev, \"Setting up data path on device\\n\");\n\n\tif (qset->rxqs[0]->init_state < FUN_QSTATE_INIT_FULL) {\n\t\terr = fun_advance_ring_state(dev, qset);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = fun_vi_create(fp);\n\tif (err)\n\t\tgoto free_queues;\n\n\tfp->txqs = qset->txqs;\n\trcu_assign_pointer(fp->rxqs, qset->rxqs);\n\trcu_assign_pointer(fp->xdpqs, qset->xdpqs);\n\n\terr = fun_enable_irqs(dev);\n\tif (err)\n\t\tgoto destroy_vi;\n\n\tif (fp->rss_cfg) {\n\t\terr = fun_config_rss(dev, fp->hash_algo, fp->rss_key,\n\t\t\t\t     fp->indir_table, FUN_ADMIN_SUBOP_CREATE);\n\t} else {\n\t\t \n\t\terr = fun_bind(fp->fdev, FUN_ADMIN_BIND_TYPE_VI, dev->dev_port,\n\t\t\t       FUN_ADMIN_BIND_TYPE_EPCQ,\n\t\t\t       qset->rxqs[0]->hw_cqid);\n\t}\n\tif (err)\n\t\tgoto disable_irqs;\n\n\terr = fun_port_write_cmds(fp, 3, port_keys, vals);\n\tif (err)\n\t\tgoto free_rss;\n\n\tnetif_tx_start_all_queues(dev);\n\treturn 0;\n\nfree_rss:\n\tfun_destroy_rss(fp);\ndisable_irqs:\n\tfun_disable_irqs(dev);\ndestroy_vi:\n\tfun_res_destroy(fp->fdev, FUN_ADMIN_OP_VI, 0, dev->dev_port);\nfree_queues:\n\tfun_free_rings(dev, qset);\n\treturn err;\n}\n\nstatic int funeth_open(struct net_device *netdev)\n{\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\tstruct fun_qset qset = {\n\t\t.nrxqs = netdev->real_num_rx_queues,\n\t\t.ntxqs = netdev->real_num_tx_queues,\n\t\t.nxdpqs = fp->num_xdpqs,\n\t\t.cq_depth = fp->cq_depth,\n\t\t.rq_depth = fp->rq_depth,\n\t\t.sq_depth = fp->sq_depth,\n\t\t.state = FUN_QSTATE_INIT_FULL,\n\t};\n\tint rc;\n\n\trc = fun_alloc_rings(netdev, &qset);\n\tif (rc)\n\t\treturn rc;\n\n\trc = fun_up(netdev, &qset);\n\tif (rc) {\n\t\tqset.state = FUN_QSTATE_DESTROYED;\n\t\tfun_free_rings(netdev, &qset);\n\t}\n\n\treturn rc;\n}\n\nstatic int funeth_close(struct net_device *netdev)\n{\n\tstruct fun_qset qset = { .state = FUN_QSTATE_DESTROYED };\n\n\tfun_down(netdev, &qset);\n\treturn 0;\n}\n\nstatic void fun_get_stats64(struct net_device *netdev,\n\t\t\t    struct rtnl_link_stats64 *stats)\n{\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\tstruct funeth_txq **xdpqs;\n\tstruct funeth_rxq **rxqs;\n\tunsigned int i, start;\n\n\tstats->tx_packets = fp->tx_packets;\n\tstats->tx_bytes   = fp->tx_bytes;\n\tstats->tx_dropped = fp->tx_dropped;\n\n\tstats->rx_packets = fp->rx_packets;\n\tstats->rx_bytes   = fp->rx_bytes;\n\tstats->rx_dropped = fp->rx_dropped;\n\n\trcu_read_lock();\n\trxqs = rcu_dereference(fp->rxqs);\n\tif (!rxqs)\n\t\tgoto unlock;\n\n\tfor (i = 0; i < netdev->real_num_tx_queues; i++) {\n\t\tstruct funeth_txq_stats txs;\n\n\t\tFUN_QSTAT_READ(fp->txqs[i], start, txs);\n\t\tstats->tx_packets += txs.tx_pkts;\n\t\tstats->tx_bytes   += txs.tx_bytes;\n\t\tstats->tx_dropped += txs.tx_map_err;\n\t}\n\n\tfor (i = 0; i < netdev->real_num_rx_queues; i++) {\n\t\tstruct funeth_rxq_stats rxs;\n\n\t\tFUN_QSTAT_READ(rxqs[i], start, rxs);\n\t\tstats->rx_packets += rxs.rx_pkts;\n\t\tstats->rx_bytes   += rxs.rx_bytes;\n\t\tstats->rx_dropped += rxs.rx_map_err + rxs.rx_mem_drops;\n\t}\n\n\txdpqs = rcu_dereference(fp->xdpqs);\n\tif (!xdpqs)\n\t\tgoto unlock;\n\n\tfor (i = 0; i < fp->num_xdpqs; i++) {\n\t\tstruct funeth_txq_stats txs;\n\n\t\tFUN_QSTAT_READ(xdpqs[i], start, txs);\n\t\tstats->tx_packets += txs.tx_pkts;\n\t\tstats->tx_bytes   += txs.tx_bytes;\n\t}\nunlock:\n\trcu_read_unlock();\n}\n\nstatic int fun_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\tint rc;\n\n\trc = fun_port_write_cmd(fp, FUN_ADMIN_PORT_KEY_MTU, new_mtu);\n\tif (!rc)\n\t\tnetdev->mtu = new_mtu;\n\treturn rc;\n}\n\nstatic int fun_set_macaddr(struct net_device *netdev, void *addr)\n{\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\tstruct sockaddr *saddr = addr;\n\tint rc;\n\n\tif (!is_valid_ether_addr(saddr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\tif (ether_addr_equal(netdev->dev_addr, saddr->sa_data))\n\t\treturn 0;\n\n\trc = fun_port_write_cmd(fp, FUN_ADMIN_PORT_KEY_MACADDR,\n\t\t\t\tether_addr_to_u64(saddr->sa_data));\n\tif (!rc)\n\t\teth_hw_addr_set(netdev, saddr->sa_data);\n\treturn rc;\n}\n\nstatic int fun_get_port_attributes(struct net_device *netdev)\n{\n\tstatic const int keys[] = {\n\t\tFUN_ADMIN_PORT_KEY_MACADDR, FUN_ADMIN_PORT_KEY_CAPABILITIES,\n\t\tFUN_ADMIN_PORT_KEY_ADVERT, FUN_ADMIN_PORT_KEY_MTU\n\t};\n\tstatic const int phys_keys[] = {\n\t\tFUN_ADMIN_PORT_KEY_LANE_ATTRS,\n\t};\n\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\tu64 data[ARRAY_SIZE(keys)];\n\tu8 mac[ETH_ALEN];\n\tint i, rc;\n\n\trc = fun_port_read_cmds(fp, ARRAY_SIZE(keys), keys, data);\n\tif (rc)\n\t\treturn rc;\n\n\tfor (i = 0; i < ARRAY_SIZE(keys); i++) {\n\t\tswitch (keys[i]) {\n\t\tcase FUN_ADMIN_PORT_KEY_MACADDR:\n\t\t\tu64_to_ether_addr(data[i], mac);\n\t\t\tif (is_zero_ether_addr(mac)) {\n\t\t\t\teth_hw_addr_random(netdev);\n\t\t\t} else if (is_valid_ether_addr(mac)) {\n\t\t\t\teth_hw_addr_set(netdev, mac);\n\t\t\t} else {\n\t\t\t\tnetdev_err(netdev,\n\t\t\t\t\t   \"device provided a bad MAC address %pM\\n\",\n\t\t\t\t\t   mac);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase FUN_ADMIN_PORT_KEY_CAPABILITIES:\n\t\t\tfp->port_caps = data[i];\n\t\t\tbreak;\n\n\t\tcase FUN_ADMIN_PORT_KEY_ADVERT:\n\t\t\tfp->advertising = data[i];\n\t\t\tbreak;\n\n\t\tcase FUN_ADMIN_PORT_KEY_MTU:\n\t\t\tnetdev->mtu = data[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!(fp->port_caps & FUN_PORT_CAP_VPORT)) {\n\t\trc = fun_port_read_cmds(fp, ARRAY_SIZE(phys_keys), phys_keys,\n\t\t\t\t\tdata);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tfp->lane_attrs = data[0];\n\t}\n\n\tif (netdev->addr_assign_type == NET_ADDR_RANDOM)\n\t\treturn fun_port_write_cmd(fp, FUN_ADMIN_PORT_KEY_MACADDR,\n\t\t\t\t\t  ether_addr_to_u64(netdev->dev_addr));\n\treturn 0;\n}\n\nstatic int fun_hwtstamp_get(struct net_device *dev, struct ifreq *ifr)\n{\n\tconst struct funeth_priv *fp = netdev_priv(dev);\n\n\treturn copy_to_user(ifr->ifr_data, &fp->hwtstamp_cfg,\n\t\t\t    sizeof(fp->hwtstamp_cfg)) ? -EFAULT : 0;\n}\n\nstatic int fun_hwtstamp_set(struct net_device *dev, struct ifreq *ifr)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct hwtstamp_config cfg;\n\n\tif (copy_from_user(&cfg, ifr->ifr_data, sizeof(cfg)))\n\t\treturn -EFAULT;\n\n\t \n\tcfg.tx_type = HWTSTAMP_TX_OFF;\n\n\tswitch (cfg.rx_filter) {\n\tcase HWTSTAMP_FILTER_NONE:\n\t\tbreak;\n\tcase HWTSTAMP_FILTER_ALL:\n\tcase HWTSTAMP_FILTER_SOME:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_PTP_V2_EVENT:\n\tcase HWTSTAMP_FILTER_PTP_V2_SYNC:\n\tcase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\n\tcase HWTSTAMP_FILTER_NTP_ALL:\n\t\tcfg.rx_filter = HWTSTAMP_FILTER_ALL;\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\tfp->hwtstamp_cfg = cfg;\n\treturn copy_to_user(ifr->ifr_data, &cfg, sizeof(cfg)) ? -EFAULT : 0;\n}\n\nstatic int fun_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tswitch (cmd) {\n\tcase SIOCSHWTSTAMP:\n\t\treturn fun_hwtstamp_set(dev, ifr);\n\tcase SIOCGHWTSTAMP:\n\t\treturn fun_hwtstamp_get(dev, ifr);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\n \nstatic int fun_enter_xdp(struct net_device *dev, struct bpf_prog *prog)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tunsigned int i, nqs = num_online_cpus();\n\tstruct funeth_txq **xdpqs;\n\tstruct funeth_rxq **rxqs;\n\tint err;\n\n\txdpqs = alloc_xdpqs(dev, nqs, fp->sq_depth, 0, FUN_QSTATE_INIT_FULL);\n\tif (IS_ERR(xdpqs))\n\t\treturn PTR_ERR(xdpqs);\n\n\trxqs = rtnl_dereference(fp->rxqs);\n\tfor (i = 0; i < dev->real_num_rx_queues; i++) {\n\t\terr = fun_rxq_set_bpf(rxqs[i], prog);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tfp->num_xdpqs = nqs;\n\trcu_assign_pointer(fp->xdpqs, xdpqs);\n\treturn 0;\nout:\n\twhile (i--)\n\t\tfun_rxq_set_bpf(rxqs[i], NULL);\n\n\tfree_xdpqs(xdpqs, nqs, 0, FUN_QSTATE_DESTROYED);\n\treturn err;\n}\n\n \nstatic void fun_end_xdp(struct net_device *dev)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct funeth_txq **xdpqs;\n\tstruct funeth_rxq **rxqs;\n\tunsigned int i;\n\n\txdpqs = rtnl_dereference(fp->xdpqs);\n\trcu_assign_pointer(fp->xdpqs, NULL);\n\tsynchronize_net();\n\t \n\n\tfree_xdpqs(xdpqs, fp->num_xdpqs, 0, FUN_QSTATE_DESTROYED);\n\tfp->num_xdpqs = 0;\n\n\trxqs = rtnl_dereference(fp->rxqs);\n\tfor (i = 0; i < dev->real_num_rx_queues; i++)\n\t\tfun_rxq_set_bpf(rxqs[i], NULL);\n}\n\n#define XDP_MAX_MTU \\\n\t(PAGE_SIZE - FUN_XDP_HEADROOM - VLAN_ETH_HLEN - FUN_RX_TAILROOM)\n\nstatic int fun_xdp_setup(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct bpf_prog *old_prog, *prog = xdp->prog;\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tint i, err;\n\n\t \n\tif (prog && dev->mtu > XDP_MAX_MTU) {\n\t\tnetdev_err(dev, \"device MTU %u too large for XDP\\n\", dev->mtu);\n\t\tNL_SET_ERR_MSG_MOD(xdp->extack,\n\t\t\t\t   \"Device MTU too large for XDP\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!netif_running(dev)) {\n\t\tfp->num_xdpqs = prog ? num_online_cpus() : 0;\n\t} else if (prog && !fp->xdp_prog) {\n\t\terr = fun_enter_xdp(dev, prog);\n\t\tif (err) {\n\t\t\tNL_SET_ERR_MSG_MOD(xdp->extack,\n\t\t\t\t\t   \"Failed to set queues for XDP.\");\n\t\t\treturn err;\n\t\t}\n\t} else if (!prog && fp->xdp_prog) {\n\t\tfun_end_xdp(dev);\n\t} else {\n\t\tstruct funeth_rxq **rxqs = rtnl_dereference(fp->rxqs);\n\n\t\tfor (i = 0; i < dev->real_num_rx_queues; i++)\n\t\t\tWRITE_ONCE(rxqs[i]->xdp_prog, prog);\n\t}\n\n\tif (prog)\n\t\txdp_features_set_redirect_target(dev, true);\n\telse\n\t\txdp_features_clear_redirect_target(dev);\n\n\tdev->max_mtu = prog ? XDP_MAX_MTU : FUN_MAX_MTU;\n\told_prog = xchg(&fp->xdp_prog, prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\treturn 0;\n}\n\nstatic int fun_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn fun_xdp_setup(dev, xdp);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int fun_init_vports(struct fun_ethdev *ed, unsigned int n)\n{\n\tif (ed->num_vports)\n\t\treturn -EINVAL;\n\n\ted->vport_info = kvcalloc(n, sizeof(*ed->vport_info), GFP_KERNEL);\n\tif (!ed->vport_info)\n\t\treturn -ENOMEM;\n\ted->num_vports = n;\n\treturn 0;\n}\n\nstatic void fun_free_vports(struct fun_ethdev *ed)\n{\n\tkvfree(ed->vport_info);\n\ted->vport_info = NULL;\n\ted->num_vports = 0;\n}\n\nstatic struct fun_vport_info *fun_get_vport(struct fun_ethdev *ed,\n\t\t\t\t\t    unsigned int vport)\n{\n\tif (!ed->vport_info || vport >= ed->num_vports)\n\t\treturn NULL;\n\n\treturn ed->vport_info + vport;\n}\n\nstatic int fun_set_vf_mac(struct net_device *dev, int vf, u8 *mac)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct fun_adi_param mac_param = {};\n\tstruct fun_dev *fdev = fp->fdev;\n\tstruct fun_ethdev *ed = to_fun_ethdev(fdev);\n\tstruct fun_vport_info *vi;\n\tint rc = -EINVAL;\n\n\tif (is_multicast_ether_addr(mac))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&ed->state_mutex);\n\tvi = fun_get_vport(ed, vf);\n\tif (!vi)\n\t\tgoto unlock;\n\n\tmac_param.u.mac = FUN_ADI_MAC_INIT(ether_addr_to_u64(mac));\n\trc = fun_adi_write(fdev, FUN_ADMIN_ADI_ATTR_MACADDR, vf + 1,\n\t\t\t   &mac_param);\n\tif (!rc)\n\t\tether_addr_copy(vi->mac, mac);\nunlock:\n\tmutex_unlock(&ed->state_mutex);\n\treturn rc;\n}\n\nstatic int fun_set_vf_vlan(struct net_device *dev, int vf, u16 vlan, u8 qos,\n\t\t\t   __be16 vlan_proto)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct fun_adi_param vlan_param = {};\n\tstruct fun_dev *fdev = fp->fdev;\n\tstruct fun_ethdev *ed = to_fun_ethdev(fdev);\n\tstruct fun_vport_info *vi;\n\tint rc = -EINVAL;\n\n\tif (vlan > 4095 || qos > 7)\n\t\treturn -EINVAL;\n\tif (vlan_proto && vlan_proto != htons(ETH_P_8021Q) &&\n\t    vlan_proto != htons(ETH_P_8021AD))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&ed->state_mutex);\n\tvi = fun_get_vport(ed, vf);\n\tif (!vi)\n\t\tgoto unlock;\n\n\tvlan_param.u.vlan = FUN_ADI_VLAN_INIT(be16_to_cpu(vlan_proto),\n\t\t\t\t\t      ((u16)qos << VLAN_PRIO_SHIFT) | vlan);\n\trc = fun_adi_write(fdev, FUN_ADMIN_ADI_ATTR_VLAN, vf + 1, &vlan_param);\n\tif (!rc) {\n\t\tvi->vlan = vlan;\n\t\tvi->qos = qos;\n\t\tvi->vlan_proto = vlan_proto;\n\t}\nunlock:\n\tmutex_unlock(&ed->state_mutex);\n\treturn rc;\n}\n\nstatic int fun_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate,\n\t\t\t   int max_tx_rate)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct fun_adi_param rate_param = {};\n\tstruct fun_dev *fdev = fp->fdev;\n\tstruct fun_ethdev *ed = to_fun_ethdev(fdev);\n\tstruct fun_vport_info *vi;\n\tint rc = -EINVAL;\n\n\tif (min_tx_rate)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&ed->state_mutex);\n\tvi = fun_get_vport(ed, vf);\n\tif (!vi)\n\t\tgoto unlock;\n\n\trate_param.u.rate = FUN_ADI_RATE_INIT(max_tx_rate);\n\trc = fun_adi_write(fdev, FUN_ADMIN_ADI_ATTR_RATE, vf + 1, &rate_param);\n\tif (!rc)\n\t\tvi->max_rate = max_tx_rate;\nunlock:\n\tmutex_unlock(&ed->state_mutex);\n\treturn rc;\n}\n\nstatic int fun_get_vf_config(struct net_device *dev, int vf,\n\t\t\t     struct ifla_vf_info *ivi)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct fun_ethdev *ed = to_fun_ethdev(fp->fdev);\n\tconst struct fun_vport_info *vi;\n\n\tmutex_lock(&ed->state_mutex);\n\tvi = fun_get_vport(ed, vf);\n\tif (!vi)\n\t\tgoto unlock;\n\n\tmemset(ivi, 0, sizeof(*ivi));\n\tivi->vf = vf;\n\tether_addr_copy(ivi->mac, vi->mac);\n\tivi->vlan = vi->vlan;\n\tivi->qos = vi->qos;\n\tivi->vlan_proto = vi->vlan_proto;\n\tivi->max_tx_rate = vi->max_rate;\n\tivi->spoofchk = vi->spoofchk;\nunlock:\n\tmutex_unlock(&ed->state_mutex);\n\treturn vi ? 0 : -EINVAL;\n}\n\nstatic void fun_uninit(struct net_device *dev)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\n\tfun_prune_queue_irqs(dev);\n\txa_destroy(&fp->irqs);\n}\n\nstatic const struct net_device_ops fun_netdev_ops = {\n\t.ndo_open\t\t= funeth_open,\n\t.ndo_stop\t\t= funeth_close,\n\t.ndo_start_xmit\t\t= fun_start_xmit,\n\t.ndo_get_stats64\t= fun_get_stats64,\n\t.ndo_change_mtu\t\t= fun_change_mtu,\n\t.ndo_set_mac_address\t= fun_set_macaddr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_eth_ioctl\t\t= fun_ioctl,\n\t.ndo_uninit\t\t= fun_uninit,\n\t.ndo_bpf\t\t= fun_xdp,\n\t.ndo_xdp_xmit\t\t= fun_xdp_xmit_frames,\n\t.ndo_set_vf_mac\t\t= fun_set_vf_mac,\n\t.ndo_set_vf_vlan\t= fun_set_vf_vlan,\n\t.ndo_set_vf_rate\t= fun_set_vf_rate,\n\t.ndo_get_vf_config\t= fun_get_vf_config,\n};\n\n#define GSO_ENCAP_FLAGS (NETIF_F_GSO_GRE | NETIF_F_GSO_IPXIP4 | \\\n\t\t\t NETIF_F_GSO_IPXIP6 | NETIF_F_GSO_UDP_TUNNEL | \\\n\t\t\t NETIF_F_GSO_UDP_TUNNEL_CSUM)\n#define TSO_FLAGS (NETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_TSO_ECN | \\\n\t\t   NETIF_F_GSO_UDP_L4)\n#define VLAN_FEAT (NETIF_F_SG | NETIF_F_HW_CSUM | TSO_FLAGS | \\\n\t\t   GSO_ENCAP_FLAGS | NETIF_F_HIGHDMA)\n\nstatic void fun_dflt_rss_indir(struct funeth_priv *fp, unsigned int nrx)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < fp->indir_table_nentries; i++)\n\t\tfp->indir_table[i] = ethtool_rxfh_indir_default(i, nrx);\n}\n\n \nstatic void fun_reset_rss_indir(struct net_device *dev, unsigned int nrx)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\n\tif (!fp->rss_cfg)\n\t\treturn;\n\n\t \n\tfp->indir_table_nentries = rounddown(FUN_ETH_RSS_MAX_INDIR_ENT, nrx);\n\tfun_dflt_rss_indir(fp, nrx);\n}\n\n \nstatic int fun_rss_set_qnum(struct net_device *dev, unsigned int nrx,\n\t\t\t    bool only_if_needed)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tu32 old_lut[FUN_ETH_RSS_MAX_INDIR_ENT];\n\tunsigned int i, oldsz;\n\tint err;\n\n\tif (!fp->rss_cfg)\n\t\treturn 0;\n\n\tif (only_if_needed) {\n\t\tfor (i = 0; i < fp->indir_table_nentries; i++)\n\t\t\tif (fp->indir_table[i] >= nrx)\n\t\t\t\tbreak;\n\n\t\tif (i >= fp->indir_table_nentries)\n\t\t\treturn 0;\n\t}\n\n\tmemcpy(old_lut, fp->indir_table, sizeof(old_lut));\n\toldsz = fp->indir_table_nentries;\n\tfun_reset_rss_indir(dev, nrx);\n\n\terr = fun_config_rss(dev, fp->hash_algo, fp->rss_key,\n\t\t\t     fp->indir_table, FUN_ADMIN_SUBOP_MODIFY);\n\tif (!err)\n\t\treturn 0;\n\n\tmemcpy(fp->indir_table, old_lut, sizeof(old_lut));\n\tfp->indir_table_nentries = oldsz;\n\treturn err;\n}\n\n \nstatic int fun_init_rss(struct net_device *dev)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tsize_t size = sizeof(fp->rss_key) + sizeof(fp->indir_table);\n\n\tfp->rss_hw_id = FUN_HCI_ID_INVALID;\n\tif (!(fp->port_caps & FUN_PORT_CAP_OFFLOADS))\n\t\treturn 0;\n\n\tfp->rss_cfg = dma_alloc_coherent(&fp->pdev->dev, size,\n\t\t\t\t\t &fp->rss_dma_addr, GFP_KERNEL);\n\tif (!fp->rss_cfg)\n\t\treturn -ENOMEM;\n\n\tfp->hash_algo = FUN_ETH_RSS_ALG_TOEPLITZ;\n\tnetdev_rss_key_fill(fp->rss_key, sizeof(fp->rss_key));\n\tfun_reset_rss_indir(dev, dev->real_num_rx_queues);\n\treturn 0;\n}\n\nstatic void fun_free_rss(struct funeth_priv *fp)\n{\n\tif (fp->rss_cfg) {\n\t\tdma_free_coherent(&fp->pdev->dev,\n\t\t\t\t  sizeof(fp->rss_key) + sizeof(fp->indir_table),\n\t\t\t\t  fp->rss_cfg, fp->rss_dma_addr);\n\t\tfp->rss_cfg = NULL;\n\t}\n}\n\nvoid fun_set_ring_count(struct net_device *netdev, unsigned int ntx,\n\t\t\tunsigned int nrx)\n{\n\tnetif_set_real_num_tx_queues(netdev, ntx);\n\tif (nrx != netdev->real_num_rx_queues) {\n\t\tnetif_set_real_num_rx_queues(netdev, nrx);\n\t\tfun_reset_rss_indir(netdev, nrx);\n\t}\n}\n\nstatic int fun_init_stats_area(struct funeth_priv *fp)\n{\n\tunsigned int nstats;\n\n\tif (!(fp->port_caps & FUN_PORT_CAP_STATS))\n\t\treturn 0;\n\n\tnstats = PORT_MAC_RX_STATS_MAX + PORT_MAC_TX_STATS_MAX +\n\t\t PORT_MAC_FEC_STATS_MAX;\n\n\tfp->stats = dma_alloc_coherent(&fp->pdev->dev, nstats * sizeof(u64),\n\t\t\t\t       &fp->stats_dma_addr, GFP_KERNEL);\n\tif (!fp->stats)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void fun_free_stats_area(struct funeth_priv *fp)\n{\n\tunsigned int nstats;\n\n\tif (fp->stats) {\n\t\tnstats = PORT_MAC_RX_STATS_MAX + PORT_MAC_TX_STATS_MAX;\n\t\tdma_free_coherent(&fp->pdev->dev, nstats * sizeof(u64),\n\t\t\t\t  fp->stats, fp->stats_dma_addr);\n\t\tfp->stats = NULL;\n\t}\n}\n\nstatic int fun_dl_port_register(struct net_device *netdev)\n{\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\tstruct devlink *dl = priv_to_devlink(fp->fdev);\n\tstruct devlink_port_attrs attrs = {};\n\tunsigned int idx;\n\n\tif (fp->port_caps & FUN_PORT_CAP_VPORT) {\n\t\tattrs.flavour = DEVLINK_PORT_FLAVOUR_VIRTUAL;\n\t\tidx = fp->lport;\n\t} else {\n\t\tidx = netdev->dev_port;\n\t\tattrs.flavour = DEVLINK_PORT_FLAVOUR_PHYSICAL;\n\t\tattrs.lanes = fp->lane_attrs & 7;\n\t\tif (fp->lane_attrs & FUN_PORT_LANE_SPLIT) {\n\t\t\tattrs.split = 1;\n\t\t\tattrs.phys.port_number = fp->lport & ~3;\n\t\t\tattrs.phys.split_subport_number = fp->lport & 3;\n\t\t} else {\n\t\t\tattrs.phys.port_number = fp->lport;\n\t\t}\n\t}\n\n\tdevlink_port_attrs_set(&fp->dl_port, &attrs);\n\n\treturn devlink_port_register(dl, &fp->dl_port, idx);\n}\n\n \nstatic int fun_max_qs(struct fun_ethdev *ed, unsigned int *ntx,\n\t\t      unsigned int *nrx)\n{\n\tint neth;\n\n\tif (ed->num_ports > 1 || is_kdump_kernel()) {\n\t\t*ntx = 1;\n\t\t*nrx = 1;\n\t\treturn 0;\n\t}\n\n\tneth = fun_get_res_count(&ed->fdev, FUN_ADMIN_OP_ETH);\n\tif (neth < 0)\n\t\treturn neth;\n\n\t \n\t*ntx = min(ed->nsqs_per_port - 1, num_online_cpus());\n\t*nrx = *ntx;\n\tif (*ntx > neth)\n\t\t*ntx = neth;\n\tif (*nrx > FUN_ETH_RSS_MAX_INDIR_ENT)\n\t\t*nrx = FUN_ETH_RSS_MAX_INDIR_ENT;\n\treturn 0;\n}\n\nstatic void fun_queue_defaults(struct net_device *dev, unsigned int nsqs)\n{\n\tunsigned int ntx, nrx;\n\n\tntx = min(dev->num_tx_queues, FUN_DFLT_QUEUES);\n\tnrx = min(dev->num_rx_queues, FUN_DFLT_QUEUES);\n\tif (ntx <= nrx) {\n\t\tntx = min(ntx, nsqs / 2);\n\t\tnrx = min(nrx, nsqs - ntx);\n\t} else {\n\t\tnrx = min(nrx, nsqs / 2);\n\t\tntx = min(ntx, nsqs - nrx);\n\t}\n\n\tnetif_set_real_num_tx_queues(dev, ntx);\n\tnetif_set_real_num_rx_queues(dev, nrx);\n}\n\n \nint fun_replace_queues(struct net_device *dev, struct fun_qset *newqs,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct fun_qset oldqs = { .state = FUN_QSTATE_DESTROYED };\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tint err;\n\n\tnewqs->nrxqs = dev->real_num_rx_queues;\n\tnewqs->ntxqs = dev->real_num_tx_queues;\n\tnewqs->nxdpqs = fp->num_xdpqs;\n\tnewqs->state = FUN_QSTATE_INIT_SW;\n\terr = fun_alloc_rings(dev, newqs);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"Unable to allocate memory for new queues, keeping current settings\");\n\t\treturn err;\n\t}\n\n\tfun_down(dev, &oldqs);\n\n\terr = fun_up(dev, newqs);\n\tif (!err)\n\t\treturn 0;\n\n\t \n\tnewqs->state = FUN_QSTATE_DESTROYED;\n\tfun_free_rings(dev, newqs);\n\tNL_SET_ERR_MSG_MOD(extack, \"Unable to restore the data path with the new queues.\");\n\treturn err;\n}\n\n \nint fun_change_num_queues(struct net_device *dev, unsigned int ntx,\n\t\t\t  unsigned int nrx)\n{\n\tunsigned int keep_tx = min(dev->real_num_tx_queues, ntx);\n\tunsigned int keep_rx = min(dev->real_num_rx_queues, nrx);\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct fun_qset oldqs = {\n\t\t.rxqs = rtnl_dereference(fp->rxqs),\n\t\t.txqs = fp->txqs,\n\t\t.nrxqs = dev->real_num_rx_queues,\n\t\t.ntxqs = dev->real_num_tx_queues,\n\t\t.rxq_start = keep_rx,\n\t\t.txq_start = keep_tx,\n\t\t.state = FUN_QSTATE_DESTROYED\n\t};\n\tstruct fun_qset newqs = {\n\t\t.nrxqs = nrx,\n\t\t.ntxqs = ntx,\n\t\t.rxq_start = keep_rx,\n\t\t.txq_start = keep_tx,\n\t\t.cq_depth = fp->cq_depth,\n\t\t.rq_depth = fp->rq_depth,\n\t\t.sq_depth = fp->sq_depth,\n\t\t.state = FUN_QSTATE_INIT_FULL\n\t};\n\tint i, err;\n\n\terr = fun_alloc_rings(dev, &newqs);\n\tif (err)\n\t\tgoto free_irqs;\n\n\terr = fun_enable_irqs(dev);  \n\tif (err)\n\t\tgoto free_rings;\n\n\t \n\tmemcpy(newqs.rxqs, oldqs.rxqs, keep_rx * sizeof(*oldqs.rxqs));\n\tmemcpy(newqs.txqs, fp->txqs, keep_tx * sizeof(*fp->txqs));\n\n\tif (nrx < dev->real_num_rx_queues) {\n\t\terr = fun_rss_set_qnum(dev, nrx, true);\n\t\tif (err)\n\t\t\tgoto disable_tx_irqs;\n\n\t\tfor (i = nrx; i < dev->real_num_rx_queues; i++)\n\t\t\tfun_disable_one_irq(container_of(oldqs.rxqs[i]->napi,\n\t\t\t\t\t\t\t struct fun_irq, napi));\n\n\t\tnetif_set_real_num_rx_queues(dev, nrx);\n\t}\n\n\tif (ntx < dev->real_num_tx_queues)\n\t\tnetif_set_real_num_tx_queues(dev, ntx);\n\n\trcu_assign_pointer(fp->rxqs, newqs.rxqs);\n\tfp->txqs = newqs.txqs;\n\tsynchronize_net();\n\n\tif (ntx > dev->real_num_tx_queues)\n\t\tnetif_set_real_num_tx_queues(dev, ntx);\n\n\tif (nrx > dev->real_num_rx_queues) {\n\t\tnetif_set_real_num_rx_queues(dev, nrx);\n\t\tfun_rss_set_qnum(dev, nrx, false);\n\t}\n\n\t \n\tfor (i = keep_tx; i < oldqs.ntxqs; i++)\n\t\tfun_disable_one_irq(oldqs.txqs[i]->irq);\n\n\tfun_free_rings(dev, &oldqs);\n\tfun_prune_queue_irqs(dev);\n\treturn 0;\n\ndisable_tx_irqs:\n\tfor (i = oldqs.ntxqs; i < ntx; i++)\n\t\tfun_disable_one_irq(newqs.txqs[i]->irq);\nfree_rings:\n\tnewqs.state = FUN_QSTATE_DESTROYED;\n\tfun_free_rings(dev, &newqs);\nfree_irqs:\n\tfun_prune_queue_irqs(dev);\n\treturn err;\n}\n\nstatic int fun_create_netdev(struct fun_ethdev *ed, unsigned int portid)\n{\n\tstruct fun_dev *fdev = &ed->fdev;\n\tstruct net_device *netdev;\n\tstruct funeth_priv *fp;\n\tunsigned int ntx, nrx;\n\tint rc;\n\n\trc = fun_max_qs(ed, &ntx, &nrx);\n\tif (rc)\n\t\treturn rc;\n\n\tnetdev = alloc_etherdev_mqs(sizeof(*fp), ntx, nrx);\n\tif (!netdev) {\n\t\trc = -ENOMEM;\n\t\tgoto done;\n\t}\n\n\tnetdev->dev_port = portid;\n\tfun_queue_defaults(netdev, ed->nsqs_per_port);\n\n\tfp = netdev_priv(netdev);\n\tfp->fdev = fdev;\n\tfp->pdev = to_pci_dev(fdev->dev);\n\tfp->netdev = netdev;\n\txa_init(&fp->irqs);\n\tfp->rx_irq_ofst = ntx;\n\tseqcount_init(&fp->link_seq);\n\n\tfp->lport = INVALID_LPORT;\n\trc = fun_port_create(netdev);\n\tif (rc)\n\t\tgoto free_netdev;\n\n\t \n\trc = fun_bind(fdev, FUN_ADMIN_BIND_TYPE_PORT, portid,\n\t\t      FUN_ADMIN_BIND_TYPE_EPCQ, 0);\n\tif (rc)\n\t\tgoto destroy_port;\n\n\trc = fun_get_port_attributes(netdev);\n\tif (rc)\n\t\tgoto destroy_port;\n\n\trc = fun_init_rss(netdev);\n\tif (rc)\n\t\tgoto destroy_port;\n\n\trc = fun_init_stats_area(fp);\n\tif (rc)\n\t\tgoto free_rss;\n\n\tSET_NETDEV_DEV(netdev, fdev->dev);\n\tSET_NETDEV_DEVLINK_PORT(netdev, &fp->dl_port);\n\tnetdev->netdev_ops = &fun_netdev_ops;\n\n\tnetdev->hw_features = NETIF_F_SG | NETIF_F_RXHASH | NETIF_F_RXCSUM;\n\tif (fp->port_caps & FUN_PORT_CAP_OFFLOADS)\n\t\tnetdev->hw_features |= NETIF_F_HW_CSUM | TSO_FLAGS;\n\tif (fp->port_caps & FUN_PORT_CAP_ENCAP_OFFLOADS)\n\t\tnetdev->hw_features |= GSO_ENCAP_FLAGS;\n\n\tnetdev->features |= netdev->hw_features | NETIF_F_HIGHDMA;\n\tnetdev->vlan_features = netdev->features & VLAN_FEAT;\n\tnetdev->mpls_features = netdev->vlan_features;\n\tnetdev->hw_enc_features = netdev->hw_features;\n\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT;\n\n\tnetdev->min_mtu = ETH_MIN_MTU;\n\tnetdev->max_mtu = FUN_MAX_MTU;\n\n\tfun_set_ethtool_ops(netdev);\n\n\t \n\tfp->sq_depth = min(SQ_DEPTH, fdev->q_depth);\n\tfp->cq_depth = min(CQ_DEPTH, fdev->q_depth);\n\tfp->rq_depth = min_t(unsigned int, RQ_DEPTH, fdev->q_depth);\n\tfp->rx_coal_usec  = CQ_INTCOAL_USEC;\n\tfp->rx_coal_count = CQ_INTCOAL_NPKT;\n\tfp->tx_coal_usec  = SQ_INTCOAL_USEC;\n\tfp->tx_coal_count = SQ_INTCOAL_NPKT;\n\tfp->cq_irq_db = FUN_IRQ_CQ_DB(fp->rx_coal_usec, fp->rx_coal_count);\n\n\trc = fun_dl_port_register(netdev);\n\tif (rc)\n\t\tgoto free_stats;\n\n\tfp->ktls_id = FUN_HCI_ID_INVALID;\n\tfun_ktls_init(netdev);             \n\n\tnetif_carrier_off(netdev);\n\ted->netdevs[portid] = netdev;\n\trc = register_netdev(netdev);\n\tif (rc)\n\t\tgoto unreg_devlink;\n\treturn 0;\n\nunreg_devlink:\n\ted->netdevs[portid] = NULL;\n\tfun_ktls_cleanup(fp);\n\tdevlink_port_unregister(&fp->dl_port);\nfree_stats:\n\tfun_free_stats_area(fp);\nfree_rss:\n\tfun_free_rss(fp);\ndestroy_port:\n\tfun_port_destroy(netdev);\nfree_netdev:\n\tfree_netdev(netdev);\ndone:\n\tdev_err(fdev->dev, \"couldn't allocate port %u, error %d\", portid, rc);\n\treturn rc;\n}\n\nstatic void fun_destroy_netdev(struct net_device *netdev)\n{\n\tstruct funeth_priv *fp;\n\n\tfp = netdev_priv(netdev);\n\tunregister_netdev(netdev);\n\tdevlink_port_unregister(&fp->dl_port);\n\tfun_ktls_cleanup(fp);\n\tfun_free_stats_area(fp);\n\tfun_free_rss(fp);\n\tfun_port_destroy(netdev);\n\tfree_netdev(netdev);\n}\n\nstatic int fun_create_ports(struct fun_ethdev *ed, unsigned int nports)\n{\n\tstruct fun_dev *fd = &ed->fdev;\n\tint i, rc;\n\n\t \n\ted->nsqs_per_port = min(fd->num_irqs - 1,\n\t\t\t\tfd->kern_end_qid - 2) / nports;\n\tif (ed->nsqs_per_port < 2) {\n\t\tdev_err(fd->dev, \"Too few SQs for %u ports\", nports);\n\t\treturn -EINVAL;\n\t}\n\n\ted->netdevs = kcalloc(nports, sizeof(*ed->netdevs), GFP_KERNEL);\n\tif (!ed->netdevs)\n\t\treturn -ENOMEM;\n\n\ted->num_ports = nports;\n\tfor (i = 0; i < nports; i++) {\n\t\trc = fun_create_netdev(ed, i);\n\t\tif (rc)\n\t\t\tgoto free_netdevs;\n\t}\n\n\treturn 0;\n\nfree_netdevs:\n\twhile (i)\n\t\tfun_destroy_netdev(ed->netdevs[--i]);\n\tkfree(ed->netdevs);\n\ted->netdevs = NULL;\n\ted->num_ports = 0;\n\treturn rc;\n}\n\nstatic void fun_destroy_ports(struct fun_ethdev *ed)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ed->num_ports; i++)\n\t\tfun_destroy_netdev(ed->netdevs[i]);\n\n\tkfree(ed->netdevs);\n\ted->netdevs = NULL;\n\ted->num_ports = 0;\n}\n\nstatic void fun_update_link_state(const struct fun_ethdev *ed,\n\t\t\t\t  const struct fun_admin_port_notif *notif)\n{\n\tunsigned int port_idx = be16_to_cpu(notif->id);\n\tstruct net_device *netdev;\n\tstruct funeth_priv *fp;\n\n\tif (port_idx >= ed->num_ports)\n\t\treturn;\n\n\tnetdev = ed->netdevs[port_idx];\n\tfp = netdev_priv(netdev);\n\n\twrite_seqcount_begin(&fp->link_seq);\n\tfp->link_speed = be32_to_cpu(notif->speed) * 10;   \n\tfp->active_fc = notif->flow_ctrl;\n\tfp->active_fec = notif->fec;\n\tfp->xcvr_type = notif->xcvr_type;\n\tfp->link_down_reason = notif->link_down_reason;\n\tfp->lp_advertising = be64_to_cpu(notif->lp_advertising);\n\n\tif ((notif->link_state | notif->missed_events) & FUN_PORT_FLAG_MAC_DOWN)\n\t\tnetif_carrier_off(netdev);\n\tif (notif->link_state & FUN_PORT_FLAG_MAC_UP)\n\t\tnetif_carrier_on(netdev);\n\n\twrite_seqcount_end(&fp->link_seq);\n\tfun_report_link(netdev);\n}\n\n \nstatic void fun_event_cb(struct fun_dev *fdev, void *entry)\n{\n\tu8 op = ((struct fun_admin_rsp_common *)entry)->op;\n\n\tif (op == FUN_ADMIN_OP_PORT) {\n\t\tconst struct fun_admin_port_notif *rsp = entry;\n\n\t\tif (rsp->subop == FUN_ADMIN_SUBOP_NOTIFY) {\n\t\t\tfun_update_link_state(to_fun_ethdev(fdev), rsp);\n\t\t} else if (rsp->subop == FUN_ADMIN_SUBOP_RES_COUNT) {\n\t\t\tconst struct fun_admin_res_count_rsp *r = entry;\n\n\t\t\tif (r->count.data)\n\t\t\t\tset_bit(FUN_SERV_RES_CHANGE, &fdev->service_flags);\n\t\t\telse\n\t\t\t\tset_bit(FUN_SERV_DEL_PORTS, &fdev->service_flags);\n\t\t\tfun_serv_sched(fdev);\n\t\t} else {\n\t\t\tdev_info(fdev->dev, \"adminq event unexpected op %u subop %u\",\n\t\t\t\t op, rsp->subop);\n\t\t}\n\t} else {\n\t\tdev_info(fdev->dev, \"adminq event unexpected op %u\", op);\n\t}\n}\n\n \nstatic void fun_service_cb(struct fun_dev *fdev)\n{\n\tstruct fun_ethdev *ed = to_fun_ethdev(fdev);\n\tint rc;\n\n\tif (test_and_clear_bit(FUN_SERV_DEL_PORTS, &fdev->service_flags))\n\t\tfun_destroy_ports(ed);\n\n\tif (!test_and_clear_bit(FUN_SERV_RES_CHANGE, &fdev->service_flags))\n\t\treturn;\n\n\trc = fun_get_res_count(fdev, FUN_ADMIN_OP_PORT);\n\tif (rc < 0 || rc == ed->num_ports)\n\t\treturn;\n\n\tif (ed->num_ports)\n\t\tfun_destroy_ports(ed);\n\tif (rc)\n\t\tfun_create_ports(ed, rc);\n}\n\nstatic int funeth_sriov_configure(struct pci_dev *pdev, int nvfs)\n{\n\tstruct fun_dev *fdev = pci_get_drvdata(pdev);\n\tstruct fun_ethdev *ed = to_fun_ethdev(fdev);\n\tint rc;\n\n\tif (nvfs == 0) {\n\t\tif (pci_vfs_assigned(pdev)) {\n\t\t\tdev_warn(&pdev->dev,\n\t\t\t\t \"Cannot disable SR-IOV while VFs are assigned\\n\");\n\t\t\treturn -EPERM;\n\t\t}\n\n\t\tmutex_lock(&ed->state_mutex);\n\t\tfun_free_vports(ed);\n\t\tmutex_unlock(&ed->state_mutex);\n\t\tpci_disable_sriov(pdev);\n\t\treturn 0;\n\t}\n\n\trc = pci_enable_sriov(pdev, nvfs);\n\tif (rc)\n\t\treturn rc;\n\n\tmutex_lock(&ed->state_mutex);\n\trc = fun_init_vports(ed, nvfs);\n\tmutex_unlock(&ed->state_mutex);\n\tif (rc) {\n\t\tpci_disable_sriov(pdev);\n\t\treturn rc;\n\t}\n\n\treturn nvfs;\n}\n\nstatic int funeth_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct fun_dev_params aqreq = {\n\t\t.cqe_size_log2 = ilog2(ADMIN_CQE_SIZE),\n\t\t.sqe_size_log2 = ilog2(ADMIN_SQE_SIZE),\n\t\t.cq_depth      = ADMIN_CQ_DEPTH,\n\t\t.sq_depth      = ADMIN_SQ_DEPTH,\n\t\t.rq_depth      = ADMIN_RQ_DEPTH,\n\t\t.min_msix      = 2,               \n\t\t.event_cb      = fun_event_cb,\n\t\t.serv_cb       = fun_service_cb,\n\t};\n\tstruct devlink *devlink;\n\tstruct fun_ethdev *ed;\n\tstruct fun_dev *fdev;\n\tint rc;\n\n\tdevlink = fun_devlink_alloc(&pdev->dev);\n\tif (!devlink) {\n\t\tdev_err(&pdev->dev, \"devlink alloc failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\ted = devlink_priv(devlink);\n\tmutex_init(&ed->state_mutex);\n\n\tfdev = &ed->fdev;\n\trc = fun_dev_enable(fdev, pdev, &aqreq, KBUILD_MODNAME);\n\tif (rc)\n\t\tgoto free_devlink;\n\n\trc = fun_get_res_count(fdev, FUN_ADMIN_OP_PORT);\n\tif (rc > 0)\n\t\trc = fun_create_ports(ed, rc);\n\tif (rc < 0)\n\t\tgoto disable_dev;\n\n\tfun_serv_restart(fdev);\n\tfun_devlink_register(devlink);\n\treturn 0;\n\ndisable_dev:\n\tfun_dev_disable(fdev);\nfree_devlink:\n\tmutex_destroy(&ed->state_mutex);\n\tfun_devlink_free(devlink);\n\treturn rc;\n}\n\nstatic void funeth_remove(struct pci_dev *pdev)\n{\n\tstruct fun_dev *fdev = pci_get_drvdata(pdev);\n\tstruct devlink *devlink;\n\tstruct fun_ethdev *ed;\n\n\ted = to_fun_ethdev(fdev);\n\tdevlink = priv_to_devlink(ed);\n\tfun_devlink_unregister(devlink);\n\n#ifdef CONFIG_PCI_IOV\n\tfuneth_sriov_configure(pdev, 0);\n#endif\n\n\tfun_serv_stop(fdev);\n\tfun_destroy_ports(ed);\n\tfun_dev_disable(fdev);\n\tmutex_destroy(&ed->state_mutex);\n\n\tfun_devlink_free(devlink);\n}\n\nstatic struct pci_driver funeth_driver = {\n\t.name\t\t = KBUILD_MODNAME,\n\t.id_table\t = funeth_id_table,\n\t.probe\t\t = funeth_probe,\n\t.remove\t\t = funeth_remove,\n\t.shutdown\t = funeth_remove,\n\t.sriov_configure = funeth_sriov_configure,\n};\n\nmodule_pci_driver(funeth_driver);\n\nMODULE_AUTHOR(\"Dimitris Michailidis <dmichail@fungible.com>\");\nMODULE_DESCRIPTION(\"Fungible Ethernet Network Driver\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_DEVICE_TABLE(pci, funeth_id_table);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}