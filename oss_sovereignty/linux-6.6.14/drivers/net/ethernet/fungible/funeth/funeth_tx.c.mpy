{
  "module_name": "funeth_tx.c",
  "hash_id": "b3410191d0738f1fd46ba868935bd931621e848c35529f1c90b195631944151b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/fungible/funeth/funeth_tx.c",
  "human_readable_source": "\n\n#include <linux/dma-mapping.h>\n#include <linux/ip.h>\n#include <linux/pci.h>\n#include <linux/skbuff.h>\n#include <linux/tcp.h>\n#include <uapi/linux/udp.h>\n#include \"funeth.h\"\n#include \"funeth_ktls.h\"\n#include \"funeth_txrx.h\"\n#include \"funeth_trace.h\"\n#include \"fun_queue.h\"\n\n#define FUN_XDP_CLEAN_THRES 32\n#define FUN_XDP_CLEAN_BATCH 16\n\n \nstatic int fun_map_pkt(struct device *dev, const struct skb_shared_info *si,\n\t\t       void *data, unsigned int data_len,\n\t\t       dma_addr_t *addr, unsigned int *len)\n{\n\tconst skb_frag_t *fp, *end;\n\n\t*len = data_len;\n\t*addr = dma_map_single(dev, data, *len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, *addr))\n\t\treturn -ENOMEM;\n\n\tif (!si)\n\t\treturn 0;\n\n\tfor (fp = si->frags, end = fp + si->nr_frags; fp < end; fp++) {\n\t\t*++len = skb_frag_size(fp);\n\t\t*++addr = skb_frag_dma_map(dev, fp, 0, *len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, *addr))\n\t\t\tgoto unwind;\n\t}\n\treturn 0;\n\nunwind:\n\twhile (fp-- > si->frags)\n\t\tdma_unmap_page(dev, *--addr, skb_frag_size(fp), DMA_TO_DEVICE);\n\n\tdma_unmap_single(dev, addr[-1], data_len, DMA_TO_DEVICE);\n\treturn -ENOMEM;\n}\n\n \nstatic void *txq_end(const struct funeth_txq *q)\n{\n\treturn (void *)q->hw_wb;\n}\n\n \nstatic unsigned int txq_to_end(const struct funeth_txq *q, void *p)\n{\n\treturn txq_end(q) - p;\n}\n\n \nstatic unsigned int tx_req_ndesc(const struct fun_eth_tx_req *req)\n{\n\treturn DIV_ROUND_UP(req->len8, FUNETH_SQE_SIZE / 8);\n}\n\n \nstatic struct fun_dataop_gl *fun_write_gl(const struct funeth_txq *q,\n\t\t\t\t\t  struct fun_eth_tx_req *req,\n\t\t\t\t\t  const dma_addr_t *addrs,\n\t\t\t\t\t  const unsigned int *lens,\n\t\t\t\t\t  unsigned int ngle)\n{\n\tstruct fun_dataop_gl *gle;\n\tunsigned int i;\n\n\treq->len8 = (sizeof(*req) + ngle * sizeof(*gle)) / 8;\n\n\tfor (i = 0, gle = (struct fun_dataop_gl *)req->dataop.imm;\n\t     i < ngle && txq_to_end(q, gle); i++, gle++)\n\t\tfun_dataop_gl_init(gle, 0, 0, lens[i], addrs[i]);\n\n\tif (txq_to_end(q, gle) == 0) {\n\t\tgle = (struct fun_dataop_gl *)q->desc;\n\t\tfor ( ; i < ngle; i++, gle++)\n\t\t\tfun_dataop_gl_init(gle, 0, 0, lens[i], addrs[i]);\n\t}\n\n\treturn gle;\n}\n\nstatic __be16 tcp_hdr_doff_flags(const struct tcphdr *th)\n{\n\treturn *(__be16 *)&tcp_flag_word(th);\n}\n\nstatic struct sk_buff *fun_tls_tx(struct sk_buff *skb, struct funeth_txq *q,\n\t\t\t\t  unsigned int *tls_len)\n{\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\n\tconst struct fun_ktls_tx_ctx *tls_ctx;\n\tu32 datalen, seq;\n\n\tdatalen = skb->len - skb_tcp_all_headers(skb);\n\tif (!datalen)\n\t\treturn skb;\n\n\tif (likely(!tls_offload_tx_resync_pending(skb->sk))) {\n\t\tseq = ntohl(tcp_hdr(skb)->seq);\n\t\ttls_ctx = tls_driver_ctx(skb->sk, TLS_OFFLOAD_CTX_DIR_TX);\n\n\t\tif (likely(tls_ctx->next_seq == seq)) {\n\t\t\t*tls_len = datalen;\n\t\t\treturn skb;\n\t\t}\n\t\tif (seq - tls_ctx->next_seq < U32_MAX / 4) {\n\t\t\ttls_offload_tx_resync_request(skb->sk, seq,\n\t\t\t\t\t\t      tls_ctx->next_seq);\n\t\t}\n\t}\n\n\tFUN_QSTAT_INC(q, tx_tls_fallback);\n\tskb = tls_encrypt_skb(skb);\n\tif (!skb)\n\t\tFUN_QSTAT_INC(q, tx_tls_drops);\n\n\treturn skb;\n#else\n\treturn NULL;\n#endif\n}\n\n \nstatic unsigned int write_pkt_desc(struct sk_buff *skb, struct funeth_txq *q,\n\t\t\t\t   unsigned int tls_len)\n{\n\tunsigned int extra_bytes = 0, extra_pkts = 0;\n\tunsigned int idx = q->prod_cnt & q->mask;\n\tconst struct skb_shared_info *shinfo;\n\tunsigned int lens[MAX_SKB_FRAGS + 1];\n\tdma_addr_t addrs[MAX_SKB_FRAGS + 1];\n\tstruct fun_eth_tx_req *req;\n\tstruct fun_dataop_gl *gle;\n\tconst struct tcphdr *th;\n\tunsigned int l4_hlen;\n\tunsigned int ngle;\n\tu16 flags;\n\n\tshinfo = skb_shinfo(skb);\n\tif (unlikely(fun_map_pkt(q->dma_dev, shinfo, skb->data,\n\t\t\t\t skb_headlen(skb), addrs, lens))) {\n\t\tFUN_QSTAT_INC(q, tx_map_err);\n\t\treturn 0;\n\t}\n\n\treq = fun_tx_desc_addr(q, idx);\n\treq->op = FUN_ETH_OP_TX;\n\treq->len8 = 0;\n\treq->flags = 0;\n\treq->suboff8 = offsetof(struct fun_eth_tx_req, dataop);\n\treq->repr_idn = 0;\n\treq->encap_proto = 0;\n\n\tif (likely(shinfo->gso_size)) {\n\t\tif (skb->encapsulation) {\n\t\t\tu16 ol4_ofst;\n\n\t\t\tflags = FUN_ETH_OUTER_EN | FUN_ETH_INNER_LSO |\n\t\t\t\tFUN_ETH_UPDATE_INNER_L4_CKSUM |\n\t\t\t\tFUN_ETH_UPDATE_OUTER_L3_LEN;\n\t\t\tif (shinfo->gso_type & (SKB_GSO_UDP_TUNNEL |\n\t\t\t\t\t\tSKB_GSO_UDP_TUNNEL_CSUM)) {\n\t\t\t\tflags |= FUN_ETH_UPDATE_OUTER_L4_LEN |\n\t\t\t\t\t FUN_ETH_OUTER_UDP;\n\t\t\t\tif (shinfo->gso_type & SKB_GSO_UDP_TUNNEL_CSUM)\n\t\t\t\t\tflags |= FUN_ETH_UPDATE_OUTER_L4_CKSUM;\n\t\t\t\tol4_ofst = skb_transport_offset(skb);\n\t\t\t} else {\n\t\t\t\tol4_ofst = skb_inner_network_offset(skb);\n\t\t\t}\n\n\t\t\tif (ip_hdr(skb)->version == 4)\n\t\t\t\tflags |= FUN_ETH_UPDATE_OUTER_L3_CKSUM;\n\t\t\telse\n\t\t\t\tflags |= FUN_ETH_OUTER_IPV6;\n\n\t\t\tif (skb->inner_network_header) {\n\t\t\t\tif (inner_ip_hdr(skb)->version == 4)\n\t\t\t\t\tflags |= FUN_ETH_UPDATE_INNER_L3_CKSUM |\n\t\t\t\t\t\t FUN_ETH_UPDATE_INNER_L3_LEN;\n\t\t\t\telse\n\t\t\t\t\tflags |= FUN_ETH_INNER_IPV6 |\n\t\t\t\t\t\t FUN_ETH_UPDATE_INNER_L3_LEN;\n\t\t\t}\n\t\t\tth = inner_tcp_hdr(skb);\n\t\t\tl4_hlen = __tcp_hdrlen(th);\n\t\t\tfun_eth_offload_init(&req->offload, flags,\n\t\t\t\t\t     shinfo->gso_size,\n\t\t\t\t\t     tcp_hdr_doff_flags(th), 0,\n\t\t\t\t\t     skb_inner_network_offset(skb),\n\t\t\t\t\t     skb_inner_transport_offset(skb),\n\t\t\t\t\t     skb_network_offset(skb), ol4_ofst);\n\t\t\tFUN_QSTAT_INC(q, tx_encap_tso);\n\t\t} else if (shinfo->gso_type & SKB_GSO_UDP_L4) {\n\t\t\tflags = FUN_ETH_INNER_LSO | FUN_ETH_INNER_UDP |\n\t\t\t\tFUN_ETH_UPDATE_INNER_L4_CKSUM |\n\t\t\t\tFUN_ETH_UPDATE_INNER_L4_LEN |\n\t\t\t\tFUN_ETH_UPDATE_INNER_L3_LEN;\n\n\t\t\tif (ip_hdr(skb)->version == 4)\n\t\t\t\tflags |= FUN_ETH_UPDATE_INNER_L3_CKSUM;\n\t\t\telse\n\t\t\t\tflags |= FUN_ETH_INNER_IPV6;\n\n\t\t\tl4_hlen = sizeof(struct udphdr);\n\t\t\tfun_eth_offload_init(&req->offload, flags,\n\t\t\t\t\t     shinfo->gso_size,\n\t\t\t\t\t     cpu_to_be16(l4_hlen << 10), 0,\n\t\t\t\t\t     skb_network_offset(skb),\n\t\t\t\t\t     skb_transport_offset(skb), 0, 0);\n\t\t\tFUN_QSTAT_INC(q, tx_uso);\n\t\t} else {\n\t\t\t \n\t\t\tflags = FUN_ETH_INNER_LSO |\n\t\t\t\tFUN_ETH_UPDATE_INNER_L4_CKSUM |\n\t\t\t\tFUN_ETH_UPDATE_INNER_L3_LEN;\n\t\t\tif (shinfo->gso_type & SKB_GSO_TCPV6)\n\t\t\t\tflags |= FUN_ETH_INNER_IPV6;\n\t\t\telse\n\t\t\t\tflags |= FUN_ETH_UPDATE_INNER_L3_CKSUM;\n\t\t\tth = tcp_hdr(skb);\n\t\t\tl4_hlen = __tcp_hdrlen(th);\n\t\t\tfun_eth_offload_init(&req->offload, flags,\n\t\t\t\t\t     shinfo->gso_size,\n\t\t\t\t\t     tcp_hdr_doff_flags(th), 0,\n\t\t\t\t\t     skb_network_offset(skb),\n\t\t\t\t\t     skb_transport_offset(skb), 0, 0);\n\t\t\tFUN_QSTAT_INC(q, tx_tso);\n\t\t}\n\n\t\tu64_stats_update_begin(&q->syncp);\n\t\tq->stats.tx_cso += shinfo->gso_segs;\n\t\tu64_stats_update_end(&q->syncp);\n\n\t\textra_pkts = shinfo->gso_segs - 1;\n\t\textra_bytes = (be16_to_cpu(req->offload.inner_l4_off) +\n\t\t\t       l4_hlen) * extra_pkts;\n\t} else if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {\n\t\tflags = FUN_ETH_UPDATE_INNER_L4_CKSUM;\n\t\tif (skb->csum_offset == offsetof(struct udphdr, check))\n\t\t\tflags |= FUN_ETH_INNER_UDP;\n\t\tfun_eth_offload_init(&req->offload, flags, 0, 0, 0, 0,\n\t\t\t\t     skb_checksum_start_offset(skb), 0, 0);\n\t\tFUN_QSTAT_INC(q, tx_cso);\n\t} else {\n\t\tfun_eth_offload_init(&req->offload, 0, 0, 0, 0, 0, 0, 0, 0);\n\t}\n\n\tngle = shinfo->nr_frags + 1;\n\treq->dataop = FUN_DATAOP_HDR_INIT(ngle, 0, ngle, 0, skb->len);\n\n\tgle = fun_write_gl(q, req, addrs, lens, ngle);\n\n\tif (IS_ENABLED(CONFIG_TLS_DEVICE) && unlikely(tls_len)) {\n\t\tstruct fun_eth_tls *tls = (struct fun_eth_tls *)gle;\n\t\tstruct fun_ktls_tx_ctx *tls_ctx;\n\n\t\treq->len8 += FUNETH_TLS_SZ / 8;\n\t\treq->flags = cpu_to_be16(FUN_ETH_TX_TLS);\n\n\t\ttls_ctx = tls_driver_ctx(skb->sk, TLS_OFFLOAD_CTX_DIR_TX);\n\t\ttls->tlsid = tls_ctx->tlsid;\n\t\ttls_ctx->next_seq += tls_len;\n\n\t\tu64_stats_update_begin(&q->syncp);\n\t\tq->stats.tx_tls_bytes += tls_len;\n\t\tq->stats.tx_tls_pkts += 1 + extra_pkts;\n\t\tu64_stats_update_end(&q->syncp);\n\t}\n\n\tu64_stats_update_begin(&q->syncp);\n\tq->stats.tx_bytes += skb->len + extra_bytes;\n\tq->stats.tx_pkts += 1 + extra_pkts;\n\tu64_stats_update_end(&q->syncp);\n\n\tq->info[idx].skb = skb;\n\n\ttrace_funeth_tx(q, skb->len, idx, req->dataop.ngather);\n\treturn tx_req_ndesc(req);\n}\n\n \nstatic unsigned int fun_txq_avail(const struct funeth_txq *q)\n{\n\treturn q->mask - q->prod_cnt + q->cons_cnt;\n}\n\n \nstatic void fun_tx_check_stop(struct funeth_txq *q)\n{\n\tif (likely(fun_txq_avail(q) >= FUNETH_MAX_PKT_DESC))\n\t\treturn;\n\n\tnetif_tx_stop_queue(q->ndq);\n\n\t \n\tsmp_mb();\n\tif (likely(fun_txq_avail(q) < FUNETH_MAX_PKT_DESC))\n\t\tFUN_QSTAT_INC(q, tx_nstops);\n\telse\n\t\tnetif_tx_start_queue(q->ndq);\n}\n\n \nstatic bool fun_txq_may_restart(struct funeth_txq *q)\n{\n\treturn fun_txq_avail(q) >= q->mask / 4;\n}\n\nnetdev_tx_t fun_start_xmit(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct funeth_priv *fp = netdev_priv(netdev);\n\tunsigned int qid = skb_get_queue_mapping(skb);\n\tstruct funeth_txq *q = fp->txqs[qid];\n\tunsigned int tls_len = 0;\n\tunsigned int ndesc;\n\n\tif (tls_is_skb_tx_device_offloaded(skb)) {\n\t\tskb = fun_tls_tx(skb, q, &tls_len);\n\t\tif (unlikely(!skb))\n\t\t\tgoto dropped;\n\t}\n\n\tndesc = write_pkt_desc(skb, q, tls_len);\n\tif (unlikely(!ndesc)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tgoto dropped;\n\t}\n\n\tq->prod_cnt += ndesc;\n\tfun_tx_check_stop(q);\n\n\tskb_tx_timestamp(skb);\n\n\tif (__netdev_tx_sent_queue(q->ndq, skb->len, netdev_xmit_more()))\n\t\tfun_txq_wr_db(q);\n\telse\n\t\tFUN_QSTAT_INC(q, tx_more);\n\n\treturn NETDEV_TX_OK;\n\ndropped:\n\t \n\tif (!netdev_xmit_more())\n\t\tfun_txq_wr_db(q);\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic u16 txq_hw_head(const struct funeth_txq *q)\n{\n\treturn (u16)be64_to_cpu(*q->hw_wb);\n}\n\n \nstatic unsigned int fun_unmap_pkt(const struct funeth_txq *q, unsigned int idx)\n{\n\tconst struct fun_eth_tx_req *req = fun_tx_desc_addr(q, idx);\n\tunsigned int ngle = req->dataop.ngather;\n\tstruct fun_dataop_gl *gle;\n\n\tif (ngle) {\n\t\tgle = (struct fun_dataop_gl *)req->dataop.imm;\n\t\tdma_unmap_single(q->dma_dev, be64_to_cpu(gle->sgl_data),\n\t\t\t\t be32_to_cpu(gle->sgl_len), DMA_TO_DEVICE);\n\n\t\tfor (gle++; --ngle && txq_to_end(q, gle); gle++)\n\t\t\tdma_unmap_page(q->dma_dev, be64_to_cpu(gle->sgl_data),\n\t\t\t\t       be32_to_cpu(gle->sgl_len),\n\t\t\t\t       DMA_TO_DEVICE);\n\n\t\tfor (gle = (struct fun_dataop_gl *)q->desc; ngle; ngle--, gle++)\n\t\t\tdma_unmap_page(q->dma_dev, be64_to_cpu(gle->sgl_data),\n\t\t\t\t       be32_to_cpu(gle->sgl_len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t}\n\n\treturn tx_req_ndesc(req);\n}\n\n \nstatic bool fun_txq_reclaim(struct funeth_txq *q, int budget)\n{\n\tunsigned int npkts = 0, nbytes = 0, ndesc = 0;\n\tunsigned int head, limit, reclaim_idx;\n\n\t \n\tlimit = budget ? budget : UINT_MAX;\n\n\tfor (head = txq_hw_head(q), reclaim_idx = q->cons_cnt & q->mask;\n\t     head != reclaim_idx && npkts < limit; head = txq_hw_head(q)) {\n\t\t \n\t\trmb();\n\n\t\tdo {\n\t\t\tunsigned int pkt_desc = fun_unmap_pkt(q, reclaim_idx);\n\t\t\tstruct sk_buff *skb = q->info[reclaim_idx].skb;\n\n\t\t\ttrace_funeth_tx_free(q, reclaim_idx, pkt_desc, head);\n\n\t\t\tnbytes += skb->len;\n\t\t\tnapi_consume_skb(skb, budget);\n\t\t\tndesc += pkt_desc;\n\t\t\treclaim_idx = (reclaim_idx + pkt_desc) & q->mask;\n\t\t\tnpkts++;\n\t\t} while (reclaim_idx != head && npkts < limit);\n\t}\n\n\tq->cons_cnt += ndesc;\n\tnetdev_tx_completed_queue(q->ndq, npkts, nbytes);\n\tsmp_mb();  \n\n\tif (unlikely(netif_tx_queue_stopped(q->ndq) &&\n\t\t     fun_txq_may_restart(q))) {\n\t\tnetif_tx_wake_queue(q->ndq);\n\t\tFUN_QSTAT_INC(q, tx_nrestarts);\n\t}\n\n\treturn reclaim_idx != head;\n}\n\n \nint fun_txq_napi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct fun_irq *irq = container_of(napi, struct fun_irq, napi);\n\tstruct funeth_txq *q = irq->txq;\n\tunsigned int db_val;\n\n\tif (fun_txq_reclaim(q, budget))\n\t\treturn budget;                \n\n\tnapi_complete(napi);                  \n\tdb_val = READ_ONCE(q->irq_db_val) | (q->cons_cnt & q->mask);\n\twritel(db_val, q->db);\n\treturn 0;\n}\n\n \nstatic unsigned int fun_xdpq_clean(struct funeth_txq *q, unsigned int budget)\n{\n\tunsigned int npkts = 0, ndesc = 0, head, reclaim_idx;\n\n\tfor (head = txq_hw_head(q), reclaim_idx = q->cons_cnt & q->mask;\n\t     head != reclaim_idx && npkts < budget; head = txq_hw_head(q)) {\n\t\t \n\t\trmb();\n\n\t\tdo {\n\t\t\tunsigned int pkt_desc = fun_unmap_pkt(q, reclaim_idx);\n\n\t\t\txdp_return_frame(q->info[reclaim_idx].xdpf);\n\n\t\t\ttrace_funeth_tx_free(q, reclaim_idx, pkt_desc, head);\n\n\t\t\treclaim_idx = (reclaim_idx + pkt_desc) & q->mask;\n\t\t\tndesc += pkt_desc;\n\t\t\tnpkts++;\n\t\t} while (reclaim_idx != head && npkts < budget);\n\t}\n\n\tq->cons_cnt += ndesc;\n\treturn npkts;\n}\n\nbool fun_xdp_tx(struct funeth_txq *q, struct xdp_frame *xdpf)\n{\n\tunsigned int idx, nfrags = 1, ndesc = 1, tot_len = xdpf->len;\n\tconst struct skb_shared_info *si = NULL;\n\tunsigned int lens[MAX_SKB_FRAGS + 1];\n\tdma_addr_t dma[MAX_SKB_FRAGS + 1];\n\tstruct fun_eth_tx_req *req;\n\n\tif (fun_txq_avail(q) < FUN_XDP_CLEAN_THRES)\n\t\tfun_xdpq_clean(q, FUN_XDP_CLEAN_BATCH);\n\n\tif (unlikely(xdp_frame_has_frags(xdpf))) {\n\t\tsi = xdp_get_shared_info_from_frame(xdpf);\n\t\ttot_len = xdp_get_frame_len(xdpf);\n\t\tnfrags += si->nr_frags;\n\t\tndesc = DIV_ROUND_UP((sizeof(*req) + nfrags *\n\t\t\t\t      sizeof(struct fun_dataop_gl)),\n\t\t\t\t     FUNETH_SQE_SIZE);\n\t}\n\n\tif (unlikely(fun_txq_avail(q) < ndesc)) {\n\t\tFUN_QSTAT_INC(q, tx_xdp_full);\n\t\treturn false;\n\t}\n\n\tif (unlikely(fun_map_pkt(q->dma_dev, si, xdpf->data, xdpf->len, dma,\n\t\t\t\t lens))) {\n\t\tFUN_QSTAT_INC(q, tx_map_err);\n\t\treturn false;\n\t}\n\n\tidx = q->prod_cnt & q->mask;\n\treq = fun_tx_desc_addr(q, idx);\n\treq->op = FUN_ETH_OP_TX;\n\treq->len8 = 0;\n\treq->flags = 0;\n\treq->suboff8 = offsetof(struct fun_eth_tx_req, dataop);\n\treq->repr_idn = 0;\n\treq->encap_proto = 0;\n\tfun_eth_offload_init(&req->offload, 0, 0, 0, 0, 0, 0, 0, 0);\n\treq->dataop = FUN_DATAOP_HDR_INIT(nfrags, 0, nfrags, 0, tot_len);\n\n\tfun_write_gl(q, req, dma, lens, nfrags);\n\n\tq->info[idx].xdpf = xdpf;\n\n\tu64_stats_update_begin(&q->syncp);\n\tq->stats.tx_bytes += tot_len;\n\tq->stats.tx_pkts++;\n\tu64_stats_update_end(&q->syncp);\n\n\ttrace_funeth_tx(q, tot_len, idx, nfrags);\n\tq->prod_cnt += ndesc;\n\n\treturn true;\n}\n\nint fun_xdp_xmit_frames(struct net_device *dev, int n,\n\t\t\tstruct xdp_frame **frames, u32 flags)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct funeth_txq *q, **xdpqs;\n\tint i, q_idx;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\txdpqs = rcu_dereference_bh(fp->xdpqs);\n\tif (unlikely(!xdpqs))\n\t\treturn -ENETDOWN;\n\n\tq_idx = smp_processor_id();\n\tif (unlikely(q_idx >= fp->num_xdpqs))\n\t\treturn -ENXIO;\n\n\tfor (q = xdpqs[q_idx], i = 0; i < n; i++)\n\t\tif (!fun_xdp_tx(q, frames[i]))\n\t\t\tbreak;\n\n\tif (unlikely(flags & XDP_XMIT_FLUSH))\n\t\tfun_txq_wr_db(q);\n\treturn i;\n}\n\n \nstatic void fun_txq_purge(struct funeth_txq *q)\n{\n\twhile (q->cons_cnt != q->prod_cnt) {\n\t\tunsigned int idx = q->cons_cnt & q->mask;\n\n\t\tq->cons_cnt += fun_unmap_pkt(q, idx);\n\t\tdev_kfree_skb_any(q->info[idx].skb);\n\t}\n\tnetdev_tx_reset_queue(q->ndq);\n}\n\nstatic void fun_xdpq_purge(struct funeth_txq *q)\n{\n\twhile (q->cons_cnt != q->prod_cnt) {\n\t\tunsigned int idx = q->cons_cnt & q->mask;\n\n\t\tq->cons_cnt += fun_unmap_pkt(q, idx);\n\t\txdp_return_frame(q->info[idx].xdpf);\n\t}\n}\n\n \nstatic struct funeth_txq *fun_txq_create_sw(struct net_device *dev,\n\t\t\t\t\t    unsigned int qidx,\n\t\t\t\t\t    unsigned int ndesc,\n\t\t\t\t\t    struct fun_irq *irq)\n{\n\tstruct funeth_priv *fp = netdev_priv(dev);\n\tstruct funeth_txq *q;\n\tint numa_node;\n\n\tif (irq)\n\t\tnuma_node = fun_irq_node(irq);  \n\telse\n\t\tnuma_node = cpu_to_node(qidx);  \n\n\tq = kzalloc_node(sizeof(*q), GFP_KERNEL, numa_node);\n\tif (!q)\n\t\tgoto err;\n\n\tq->dma_dev = &fp->pdev->dev;\n\tq->desc = fun_alloc_ring_mem(q->dma_dev, ndesc, FUNETH_SQE_SIZE,\n\t\t\t\t     sizeof(*q->info), true, numa_node,\n\t\t\t\t     &q->dma_addr, (void **)&q->info,\n\t\t\t\t     &q->hw_wb);\n\tif (!q->desc)\n\t\tgoto free_q;\n\n\tq->netdev = dev;\n\tq->mask = ndesc - 1;\n\tq->qidx = qidx;\n\tq->numa_node = numa_node;\n\tu64_stats_init(&q->syncp);\n\tq->init_state = FUN_QSTATE_INIT_SW;\n\treturn q;\n\nfree_q:\n\tkfree(q);\nerr:\n\tnetdev_err(dev, \"Can't allocate memory for %s queue %u\\n\",\n\t\t   irq ? \"Tx\" : \"XDP\", qidx);\n\treturn NULL;\n}\n\nstatic void fun_txq_free_sw(struct funeth_txq *q)\n{\n\tstruct funeth_priv *fp = netdev_priv(q->netdev);\n\n\tfun_free_ring_mem(q->dma_dev, q->mask + 1, FUNETH_SQE_SIZE, true,\n\t\t\t  q->desc, q->dma_addr, q->info);\n\n\tfp->tx_packets += q->stats.tx_pkts;\n\tfp->tx_bytes   += q->stats.tx_bytes;\n\tfp->tx_dropped += q->stats.tx_map_err;\n\n\tkfree(q);\n}\n\n \nint fun_txq_create_dev(struct funeth_txq *q, struct fun_irq *irq)\n{\n\tstruct funeth_priv *fp = netdev_priv(q->netdev);\n\tunsigned int irq_idx, ndesc = q->mask + 1;\n\tint err;\n\n\tq->irq = irq;\n\t*q->hw_wb = 0;\n\tq->prod_cnt = 0;\n\tq->cons_cnt = 0;\n\tirq_idx = irq ? irq->irq_idx : 0;\n\n\terr = fun_sq_create(fp->fdev,\n\t\t\t    FUN_ADMIN_EPSQ_CREATE_FLAG_HEAD_WB_ADDRESS |\n\t\t\t    FUN_ADMIN_RES_CREATE_FLAG_ALLOCATOR, 0,\n\t\t\t    FUN_HCI_ID_INVALID, ilog2(FUNETH_SQE_SIZE), ndesc,\n\t\t\t    q->dma_addr, fp->tx_coal_count, fp->tx_coal_usec,\n\t\t\t    irq_idx, 0, fp->fdev->kern_end_qid, 0,\n\t\t\t    &q->hw_qid, &q->db);\n\tif (err)\n\t\tgoto out;\n\n\terr = fun_create_and_bind_tx(fp, q->hw_qid);\n\tif (err < 0)\n\t\tgoto free_devq;\n\tq->ethid = err;\n\n\tif (irq) {\n\t\tirq->txq = q;\n\t\tq->ndq = netdev_get_tx_queue(q->netdev, q->qidx);\n\t\tq->irq_db_val = FUN_IRQ_SQ_DB(fp->tx_coal_usec,\n\t\t\t\t\t      fp->tx_coal_count);\n\t\twritel(q->irq_db_val, q->db);\n\t}\n\n\tq->init_state = FUN_QSTATE_INIT_FULL;\n\tnetif_info(fp, ifup, q->netdev,\n\t\t   \"%s queue %u, depth %u, HW qid %u, IRQ idx %u, eth id %u, node %d\\n\",\n\t\t   irq ? \"Tx\" : \"XDP\", q->qidx, ndesc, q->hw_qid, irq_idx,\n\t\t   q->ethid, q->numa_node);\n\treturn 0;\n\nfree_devq:\n\tfun_destroy_sq(fp->fdev, q->hw_qid);\nout:\n\tnetdev_err(q->netdev,\n\t\t   \"Failed to create %s queue %u on device, error %d\\n\",\n\t\t   irq ? \"Tx\" : \"XDP\", q->qidx, err);\n\treturn err;\n}\n\nstatic void fun_txq_free_dev(struct funeth_txq *q)\n{\n\tstruct funeth_priv *fp = netdev_priv(q->netdev);\n\n\tif (q->init_state < FUN_QSTATE_INIT_FULL)\n\t\treturn;\n\n\tnetif_info(fp, ifdown, q->netdev,\n\t\t   \"Freeing %s queue %u (id %u), IRQ %u, ethid %u\\n\",\n\t\t   q->irq ? \"Tx\" : \"XDP\", q->qidx, q->hw_qid,\n\t\t   q->irq ? q->irq->irq_idx : 0, q->ethid);\n\n\tfun_destroy_sq(fp->fdev, q->hw_qid);\n\tfun_res_destroy(fp->fdev, FUN_ADMIN_OP_ETH, 0, q->ethid);\n\n\tif (q->irq) {\n\t\tq->irq->txq = NULL;\n\t\tfun_txq_purge(q);\n\t} else {\n\t\tfun_xdpq_purge(q);\n\t}\n\n\tq->init_state = FUN_QSTATE_INIT_SW;\n}\n\n \nint funeth_txq_create(struct net_device *dev, unsigned int qidx,\n\t\t      unsigned int ndesc, struct fun_irq *irq, int state,\n\t\t      struct funeth_txq **qp)\n{\n\tstruct funeth_txq *q = *qp;\n\tint err;\n\n\tif (!q)\n\t\tq = fun_txq_create_sw(dev, qidx, ndesc, irq);\n\tif (!q)\n\t\treturn -ENOMEM;\n\n\tif (q->init_state >= state)\n\t\tgoto out;\n\n\terr = fun_txq_create_dev(q, irq);\n\tif (err) {\n\t\tif (!*qp)\n\t\t\tfun_txq_free_sw(q);\n\t\treturn err;\n\t}\n\nout:\n\t*qp = q;\n\treturn 0;\n}\n\n \nstruct funeth_txq *funeth_txq_free(struct funeth_txq *q, int state)\n{\n\tif (state < FUN_QSTATE_INIT_FULL)\n\t\tfun_txq_free_dev(q);\n\n\tif (state == FUN_QSTATE_DESTROYED) {\n\t\tfun_txq_free_sw(q);\n\t\tq = NULL;\n\t}\n\n\treturn q;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}