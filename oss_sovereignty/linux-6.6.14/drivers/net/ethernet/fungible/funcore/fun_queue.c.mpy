{
  "module_name": "fun_queue.c",
  "hash_id": "82ca062ba8be8ffa1cb5f714d9a72edb7be562e341e8b87c8625f74e278dccec",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/fungible/funcore/fun_queue.c",
  "human_readable_source": "\n\n#include <linux/dma-mapping.h>\n#include <linux/interrupt.h>\n#include <linux/log2.h>\n#include <linux/mm.h>\n#include <linux/netdevice.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n\n#include \"fun_dev.h\"\n#include \"fun_queue.h\"\n\n \nvoid *fun_alloc_ring_mem(struct device *dma_dev, size_t depth,\n\t\t\t size_t hw_desc_sz, size_t sw_desc_sz, bool wb,\n\t\t\t int numa_node, dma_addr_t *dma_addr, void **sw_va,\n\t\t\t volatile __be64 **wb_va)\n{\n\tint dev_node = dev_to_node(dma_dev);\n\tsize_t dma_sz;\n\tvoid *va;\n\n\tif (numa_node == NUMA_NO_NODE)\n\t\tnuma_node = dev_node;\n\n\t \n\tdma_sz = hw_desc_sz * depth;\n\tif (wb)\n\t\tdma_sz += sizeof(u64);\n\n\tset_dev_node(dma_dev, numa_node);\n\tva = dma_alloc_coherent(dma_dev, dma_sz, dma_addr, GFP_KERNEL);\n\tset_dev_node(dma_dev, dev_node);\n\tif (!va)\n\t\treturn NULL;\n\n\tif (sw_desc_sz) {\n\t\t*sw_va = kvzalloc_node(sw_desc_sz * depth, GFP_KERNEL,\n\t\t\t\t       numa_node);\n\t\tif (!*sw_va) {\n\t\t\tdma_free_coherent(dma_dev, dma_sz, va, *dma_addr);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (wb)\n\t\t*wb_va = va + dma_sz - sizeof(u64);\n\treturn va;\n}\nEXPORT_SYMBOL_GPL(fun_alloc_ring_mem);\n\nvoid fun_free_ring_mem(struct device *dma_dev, size_t depth, size_t hw_desc_sz,\n\t\t       bool wb, void *hw_va, dma_addr_t dma_addr, void *sw_va)\n{\n\tif (hw_va) {\n\t\tsize_t sz = depth * hw_desc_sz;\n\n\t\tif (wb)\n\t\t\tsz += sizeof(u64);\n\t\tdma_free_coherent(dma_dev, sz, hw_va, dma_addr);\n\t}\n\tkvfree(sw_va);\n}\nEXPORT_SYMBOL_GPL(fun_free_ring_mem);\n\n \nint fun_sq_create(struct fun_dev *fdev, u16 flags, u32 sqid, u32 cqid,\n\t\t  u8 sqe_size_log2, u32 sq_depth, dma_addr_t dma_addr,\n\t\t  u8 coal_nentries, u8 coal_usec, u32 irq_num,\n\t\t  u32 scan_start_id, u32 scan_end_id,\n\t\t  u32 rq_buf_size_log2, u32 *sqidp, u32 __iomem **dbp)\n{\n\tunion {\n\t\tstruct fun_admin_epsq_req req;\n\t\tstruct fun_admin_generic_create_rsp rsp;\n\t} cmd;\n\tdma_addr_t wb_addr;\n\tu32 hw_qid;\n\tint rc;\n\n\tif (sq_depth > fdev->q_depth)\n\t\treturn -EINVAL;\n\tif (flags & FUN_ADMIN_EPSQ_CREATE_FLAG_RQ)\n\t\tsqe_size_log2 = ilog2(sizeof(struct fun_eprq_rqbuf));\n\n\twb_addr = dma_addr + (sq_depth << sqe_size_log2);\n\n\tcmd.req.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_EPSQ,\n\t\t\t\t\t\t    sizeof(cmd.req));\n\tcmd.req.u.create =\n\t\tFUN_ADMIN_EPSQ_CREATE_REQ_INIT(FUN_ADMIN_SUBOP_CREATE, flags,\n\t\t\t\t\t       sqid, cqid, sqe_size_log2,\n\t\t\t\t\t       sq_depth - 1, dma_addr, 0,\n\t\t\t\t\t       coal_nentries, coal_usec,\n\t\t\t\t\t       irq_num, scan_start_id,\n\t\t\t\t\t       scan_end_id, 0,\n\t\t\t\t\t       rq_buf_size_log2,\n\t\t\t\t\t       ilog2(sizeof(u64)), wb_addr);\n\n\trc = fun_submit_admin_sync_cmd(fdev, &cmd.req.common,\n\t\t\t\t       &cmd.rsp, sizeof(cmd.rsp), 0);\n\tif (rc)\n\t\treturn rc;\n\n\thw_qid = be32_to_cpu(cmd.rsp.id);\n\t*dbp = fun_sq_db_addr(fdev, hw_qid);\n\tif (flags & FUN_ADMIN_RES_CREATE_FLAG_ALLOCATOR)\n\t\t*sqidp = hw_qid;\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(fun_sq_create);\n\n \nint fun_cq_create(struct fun_dev *fdev, u16 flags, u32 cqid, u32 rqid,\n\t\t  u8 cqe_size_log2, u32 cq_depth, dma_addr_t dma_addr,\n\t\t  u16 headroom, u16 tailroom, u8 coal_nentries, u8 coal_usec,\n\t\t  u32 irq_num, u32 scan_start_id, u32 scan_end_id, u32 *cqidp,\n\t\t  u32 __iomem **dbp)\n{\n\tunion {\n\t\tstruct fun_admin_epcq_req req;\n\t\tstruct fun_admin_generic_create_rsp rsp;\n\t} cmd;\n\tu32 hw_qid;\n\tint rc;\n\n\tif (cq_depth > fdev->q_depth)\n\t\treturn -EINVAL;\n\n\tcmd.req.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_EPCQ,\n\t\t\t\t\t\t    sizeof(cmd.req));\n\tcmd.req.u.create =\n\t\tFUN_ADMIN_EPCQ_CREATE_REQ_INIT(FUN_ADMIN_SUBOP_CREATE, flags,\n\t\t\t\t\t       cqid, rqid, cqe_size_log2,\n\t\t\t\t\t       cq_depth - 1, dma_addr, tailroom,\n\t\t\t\t\t       headroom / 2, 0, coal_nentries,\n\t\t\t\t\t       coal_usec, irq_num,\n\t\t\t\t\t       scan_start_id, scan_end_id, 0);\n\n\trc = fun_submit_admin_sync_cmd(fdev, &cmd.req.common,\n\t\t\t\t       &cmd.rsp, sizeof(cmd.rsp), 0);\n\tif (rc)\n\t\treturn rc;\n\n\thw_qid = be32_to_cpu(cmd.rsp.id);\n\t*dbp = fun_cq_db_addr(fdev, hw_qid);\n\tif (flags & FUN_ADMIN_RES_CREATE_FLAG_ALLOCATOR)\n\t\t*cqidp = hw_qid;\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(fun_cq_create);\n\nstatic bool fun_sq_is_head_wb(const struct fun_queue *funq)\n{\n\treturn funq->sq_flags & FUN_ADMIN_EPSQ_CREATE_FLAG_HEAD_WB_ADDRESS;\n}\n\nstatic void fun_clean_rq(struct fun_queue *funq)\n{\n\tstruct fun_dev *fdev = funq->fdev;\n\tstruct fun_rq_info *rqinfo;\n\tunsigned int i;\n\n\tfor (i = 0; i < funq->rq_depth; i++) {\n\t\trqinfo = &funq->rq_info[i];\n\t\tif (rqinfo->page) {\n\t\t\tdma_unmap_page(fdev->dev, rqinfo->dma, PAGE_SIZE,\n\t\t\t\t       DMA_FROM_DEVICE);\n\t\t\tput_page(rqinfo->page);\n\t\t\trqinfo->page = NULL;\n\t\t}\n\t}\n}\n\nstatic int fun_fill_rq(struct fun_queue *funq)\n{\n\tstruct device *dev = funq->fdev->dev;\n\tint i, node = dev_to_node(dev);\n\tstruct fun_rq_info *rqinfo;\n\n\tfor (i = 0; i < funq->rq_depth; i++) {\n\t\trqinfo = &funq->rq_info[i];\n\t\trqinfo->page = alloc_pages_node(node, GFP_KERNEL, 0);\n\t\tif (unlikely(!rqinfo->page))\n\t\t\treturn -ENOMEM;\n\n\t\trqinfo->dma = dma_map_page(dev, rqinfo->page, 0,\n\t\t\t\t\t   PAGE_SIZE, DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dev, rqinfo->dma))) {\n\t\t\tput_page(rqinfo->page);\n\t\t\trqinfo->page = NULL;\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfunq->rqes[i] = FUN_EPRQ_RQBUF_INIT(rqinfo->dma);\n\t}\n\n\tfunq->rq_tail = funq->rq_depth - 1;\n\treturn 0;\n}\n\nstatic void fun_rq_update_pos(struct fun_queue *funq, int buf_offset)\n{\n\tif (buf_offset <= funq->rq_buf_offset) {\n\t\tstruct fun_rq_info *rqinfo = &funq->rq_info[funq->rq_buf_idx];\n\t\tstruct device *dev = funq->fdev->dev;\n\n\t\tdma_sync_single_for_device(dev, rqinfo->dma, PAGE_SIZE,\n\t\t\t\t\t   DMA_FROM_DEVICE);\n\t\tfunq->num_rqe_to_fill++;\n\t\tif (++funq->rq_buf_idx == funq->rq_depth)\n\t\t\tfunq->rq_buf_idx = 0;\n\t}\n\tfunq->rq_buf_offset = buf_offset;\n}\n\n \nstatic void *fun_data_from_rq(struct fun_queue *funq,\n\t\t\t      const struct fun_rsp_common *rsp, bool *need_free)\n{\n\tu32 bufoff, total_len, remaining, fragsize, dataoff;\n\tstruct device *dma_dev = funq->fdev->dev;\n\tconst struct fun_dataop_rqbuf *databuf;\n\tconst struct fun_dataop_hdr *dataop;\n\tconst struct fun_rq_info *rqinfo;\n\tvoid *data;\n\n\tdataop = (void *)rsp + rsp->suboff8 * 8;\n\ttotal_len = be32_to_cpu(dataop->total_len);\n\n\tif (likely(dataop->nsgl == 1)) {\n\t\tdatabuf = (struct fun_dataop_rqbuf *)dataop->imm;\n\t\tbufoff = be32_to_cpu(databuf->bufoff);\n\t\tfun_rq_update_pos(funq, bufoff);\n\t\trqinfo = &funq->rq_info[funq->rq_buf_idx];\n\t\tdma_sync_single_for_cpu(dma_dev, rqinfo->dma + bufoff,\n\t\t\t\t\ttotal_len, DMA_FROM_DEVICE);\n\t\t*need_free = false;\n\t\treturn page_address(rqinfo->page) + bufoff;\n\t}\n\n\t \n\n\tdata = kmalloc(total_len, GFP_ATOMIC);\n\t \n\tif (likely(data))\n\t\t*need_free = true;\n\n\tdataoff = 0;\n\tfor (remaining = total_len; remaining; remaining -= fragsize) {\n\t\tfun_rq_update_pos(funq, 0);\n\t\tfragsize = min_t(unsigned int, PAGE_SIZE, remaining);\n\t\tif (data) {\n\t\t\trqinfo = &funq->rq_info[funq->rq_buf_idx];\n\t\t\tdma_sync_single_for_cpu(dma_dev, rqinfo->dma, fragsize,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\tmemcpy(data + dataoff, page_address(rqinfo->page),\n\t\t\t       fragsize);\n\t\t\tdataoff += fragsize;\n\t\t}\n\t}\n\treturn data;\n}\n\nunsigned int __fun_process_cq(struct fun_queue *funq, unsigned int max)\n{\n\tconst struct fun_cqe_info *info;\n\tstruct fun_rsp_common *rsp;\n\tunsigned int new_cqes;\n\tu16 sf_p, flags;\n\tbool need_free;\n\tvoid *cqe;\n\n\tif (!max)\n\t\tmax = funq->cq_depth - 1;\n\n\tfor (new_cqes = 0; new_cqes < max; new_cqes++) {\n\t\tcqe = funq->cqes + (funq->cq_head << funq->cqe_size_log2);\n\t\tinfo = funq_cqe_info(funq, cqe);\n\t\tsf_p = be16_to_cpu(info->sf_p);\n\n\t\tif ((sf_p & 1) != funq->cq_phase)\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tif (++funq->cq_head == funq->cq_depth) {\n\t\t\tfunq->cq_head = 0;\n\t\t\tfunq->cq_phase = !funq->cq_phase;\n\t\t}\n\n\t\trsp = cqe;\n\t\tflags = be16_to_cpu(rsp->flags);\n\n\t\tneed_free = false;\n\t\tif (unlikely(flags & FUN_REQ_COMMON_FLAG_CQE_IN_RQBUF)) {\n\t\t\trsp = fun_data_from_rq(funq, rsp, &need_free);\n\t\t\tif (!rsp) {\n\t\t\t\trsp = cqe;\n\t\t\t\trsp->len8 = 1;\n\t\t\t\tif (rsp->ret == 0)\n\t\t\t\t\trsp->ret = ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tif (funq->cq_cb)\n\t\t\tfunq->cq_cb(funq, funq->cb_data, rsp, info);\n\t\tif (need_free)\n\t\t\tkfree(rsp);\n\t}\n\n\tdev_dbg(funq->fdev->dev, \"CQ %u, new CQEs %u/%u, head %u, phase %u\\n\",\n\t\tfunq->cqid, new_cqes, max, funq->cq_head, funq->cq_phase);\n\treturn new_cqes;\n}\n\nunsigned int fun_process_cq(struct fun_queue *funq, unsigned int max)\n{\n\tunsigned int processed;\n\tu32 db;\n\n\tprocessed = __fun_process_cq(funq, max);\n\n\tif (funq->num_rqe_to_fill) {\n\t\tfunq->rq_tail = (funq->rq_tail + funq->num_rqe_to_fill) %\n\t\t\t\tfunq->rq_depth;\n\t\tfunq->num_rqe_to_fill = 0;\n\t\twritel(funq->rq_tail, funq->rq_db);\n\t}\n\n\tdb = funq->cq_head | FUN_DB_IRQ_ARM_F;\n\twritel(db, funq->cq_db);\n\treturn processed;\n}\n\nstatic int fun_alloc_sqes(struct fun_queue *funq)\n{\n\tfunq->sq_cmds = fun_alloc_ring_mem(funq->fdev->dev, funq->sq_depth,\n\t\t\t\t\t   1 << funq->sqe_size_log2, 0,\n\t\t\t\t\t   fun_sq_is_head_wb(funq),\n\t\t\t\t\t   NUMA_NO_NODE, &funq->sq_dma_addr,\n\t\t\t\t\t   NULL, &funq->sq_head);\n\treturn funq->sq_cmds ? 0 : -ENOMEM;\n}\n\nstatic int fun_alloc_cqes(struct fun_queue *funq)\n{\n\tfunq->cqes = fun_alloc_ring_mem(funq->fdev->dev, funq->cq_depth,\n\t\t\t\t\t1 << funq->cqe_size_log2, 0, false,\n\t\t\t\t\tNUMA_NO_NODE, &funq->cq_dma_addr, NULL,\n\t\t\t\t\tNULL);\n\treturn funq->cqes ? 0 : -ENOMEM;\n}\n\nstatic int fun_alloc_rqes(struct fun_queue *funq)\n{\n\tfunq->rqes = fun_alloc_ring_mem(funq->fdev->dev, funq->rq_depth,\n\t\t\t\t\tsizeof(*funq->rqes),\n\t\t\t\t\tsizeof(*funq->rq_info), false,\n\t\t\t\t\tNUMA_NO_NODE, &funq->rq_dma_addr,\n\t\t\t\t\t(void **)&funq->rq_info, NULL);\n\treturn funq->rqes ? 0 : -ENOMEM;\n}\n\n \nvoid fun_free_queue(struct fun_queue *funq)\n{\n\tstruct device *dev = funq->fdev->dev;\n\n\tfun_free_ring_mem(dev, funq->cq_depth, 1 << funq->cqe_size_log2, false,\n\t\t\t  funq->cqes, funq->cq_dma_addr, NULL);\n\tfun_free_ring_mem(dev, funq->sq_depth, 1 << funq->sqe_size_log2,\n\t\t\t  fun_sq_is_head_wb(funq), funq->sq_cmds,\n\t\t\t  funq->sq_dma_addr, NULL);\n\n\tif (funq->rqes) {\n\t\tfun_clean_rq(funq);\n\t\tfun_free_ring_mem(dev, funq->rq_depth, sizeof(*funq->rqes),\n\t\t\t\t  false, funq->rqes, funq->rq_dma_addr,\n\t\t\t\t  funq->rq_info);\n\t}\n\n\tkfree(funq);\n}\n\n \nstruct fun_queue *fun_alloc_queue(struct fun_dev *fdev, int qid,\n\t\t\t\t  const struct fun_queue_alloc_req *req)\n{\n\tstruct fun_queue *funq = kzalloc(sizeof(*funq), GFP_KERNEL);\n\n\tif (!funq)\n\t\treturn NULL;\n\n\tfunq->fdev = fdev;\n\tspin_lock_init(&funq->sq_lock);\n\n\tfunq->qid = qid;\n\n\t \n\tif (req->rq_depth) {\n\t\tfunq->cqid = 2 * qid;\n\t\tif (funq->qid) {\n\t\t\t \n\t\t\tfunq->rqid = funq->cqid;\n\t\t\tfunq->sqid = funq->rqid + 1;\n\t\t} else {\n\t\t\t \n\t\t\tfunq->sqid = 0;\n\t\t\tfunq->rqid = 1;\n\t\t}\n\t} else {\n\t\tfunq->cqid = qid;\n\t\tfunq->sqid = qid;\n\t}\n\n\tfunq->cq_flags = req->cq_flags;\n\tfunq->sq_flags = req->sq_flags;\n\n\tfunq->cqe_size_log2 = req->cqe_size_log2;\n\tfunq->sqe_size_log2 = req->sqe_size_log2;\n\n\tfunq->cq_depth = req->cq_depth;\n\tfunq->sq_depth = req->sq_depth;\n\n\tfunq->cq_intcoal_nentries = req->cq_intcoal_nentries;\n\tfunq->cq_intcoal_usec = req->cq_intcoal_usec;\n\n\tfunq->sq_intcoal_nentries = req->sq_intcoal_nentries;\n\tfunq->sq_intcoal_usec = req->sq_intcoal_usec;\n\n\tif (fun_alloc_cqes(funq))\n\t\tgoto free_funq;\n\n\tfunq->cq_phase = 1;\n\n\tif (fun_alloc_sqes(funq))\n\t\tgoto free_funq;\n\n\tif (req->rq_depth) {\n\t\tfunq->rq_flags = req->rq_flags | FUN_ADMIN_EPSQ_CREATE_FLAG_RQ;\n\t\tfunq->rq_depth = req->rq_depth;\n\t\tfunq->rq_buf_offset = -1;\n\n\t\tif (fun_alloc_rqes(funq) || fun_fill_rq(funq))\n\t\t\tgoto free_funq;\n\t}\n\n\tfunq->cq_vector = -1;\n\tfunq->cqe_info_offset = (1 << funq->cqe_size_log2) - sizeof(struct fun_cqe_info);\n\n\t \n\tif (funq->sqid == 0)\n\t\tfunq->sq_db = fun_sq_db_addr(fdev, 0);\n\tif (funq->cqid == 0)\n\t\tfunq->cq_db = fun_cq_db_addr(fdev, 0);\n\n\treturn funq;\n\nfree_funq:\n\tfun_free_queue(funq);\n\treturn NULL;\n}\n\n \nstatic int fun_create_cq(struct fun_queue *funq)\n{\n\tstruct fun_dev *fdev = funq->fdev;\n\tunsigned int rqid;\n\tint rc;\n\n\trqid = funq->cq_flags & FUN_ADMIN_EPCQ_CREATE_FLAG_RQ ?\n\t\tfunq->rqid : FUN_HCI_ID_INVALID;\n\trc = fun_cq_create(fdev, funq->cq_flags, funq->cqid, rqid,\n\t\t\t   funq->cqe_size_log2, funq->cq_depth,\n\t\t\t   funq->cq_dma_addr, 0, 0, funq->cq_intcoal_nentries,\n\t\t\t   funq->cq_intcoal_usec, funq->cq_vector, 0, 0,\n\t\t\t   &funq->cqid, &funq->cq_db);\n\tif (!rc)\n\t\tdev_dbg(fdev->dev, \"created CQ %u\\n\", funq->cqid);\n\n\treturn rc;\n}\n\n \nstatic int fun_create_sq(struct fun_queue *funq)\n{\n\tstruct fun_dev *fdev = funq->fdev;\n\tint rc;\n\n\trc = fun_sq_create(fdev, funq->sq_flags, funq->sqid, funq->cqid,\n\t\t\t   funq->sqe_size_log2, funq->sq_depth,\n\t\t\t   funq->sq_dma_addr, funq->sq_intcoal_nentries,\n\t\t\t   funq->sq_intcoal_usec, funq->cq_vector, 0, 0,\n\t\t\t   0, &funq->sqid, &funq->sq_db);\n\tif (!rc)\n\t\tdev_dbg(fdev->dev, \"created SQ %u\\n\", funq->sqid);\n\n\treturn rc;\n}\n\n \nint fun_create_rq(struct fun_queue *funq)\n{\n\tstruct fun_dev *fdev = funq->fdev;\n\tint rc;\n\n\trc = fun_sq_create(fdev, funq->rq_flags, funq->rqid, funq->cqid, 0,\n\t\t\t   funq->rq_depth, funq->rq_dma_addr, 0, 0,\n\t\t\t   funq->cq_vector, 0, 0, PAGE_SHIFT, &funq->rqid,\n\t\t\t   &funq->rq_db);\n\tif (!rc)\n\t\tdev_dbg(fdev->dev, \"created RQ %u\\n\", funq->rqid);\n\n\treturn rc;\n}\n\nstatic unsigned int funq_irq(struct fun_queue *funq)\n{\n\treturn pci_irq_vector(to_pci_dev(funq->fdev->dev), funq->cq_vector);\n}\n\nint fun_request_irq(struct fun_queue *funq, const char *devname,\n\t\t    irq_handler_t handler, void *data)\n{\n\tint rc;\n\n\tif (funq->cq_vector < 0)\n\t\treturn -EINVAL;\n\n\tfunq->irq_handler = handler;\n\tfunq->irq_data = data;\n\n\tsnprintf(funq->irqname, sizeof(funq->irqname),\n\t\t funq->qid ? \"%s-q[%d]\" : \"%s-adminq\", devname, funq->qid);\n\n\trc = request_irq(funq_irq(funq), handler, 0, funq->irqname, data);\n\tif (rc)\n\t\tfunq->irq_handler = NULL;\n\n\treturn rc;\n}\n\n \nint fun_create_queue(struct fun_queue *funq)\n{\n\tint rc;\n\n\trc = fun_create_cq(funq);\n\tif (rc)\n\t\treturn rc;\n\n\tif (funq->rq_depth) {\n\t\trc = fun_create_rq(funq);\n\t\tif (rc)\n\t\t\tgoto release_cq;\n\t}\n\n\trc = fun_create_sq(funq);\n\tif (rc)\n\t\tgoto release_rq;\n\n\treturn 0;\n\nrelease_rq:\n\tfun_destroy_sq(funq->fdev, funq->rqid);\nrelease_cq:\n\tfun_destroy_cq(funq->fdev, funq->cqid);\n\treturn rc;\n}\n\nvoid fun_free_irq(struct fun_queue *funq)\n{\n\tif (funq->irq_handler) {\n\t\tunsigned int vector = funq_irq(funq);\n\n\t\tfree_irq(vector, funq->irq_data);\n\t\tfunq->irq_handler = NULL;\n\t\tfunq->irq_data = NULL;\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}