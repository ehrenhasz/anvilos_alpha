{
  "module_name": "fun_dev.c",
  "hash_id": "733b51eddd15666bf1035416b82287dd291394acb8021a21c1a14e5f4112063b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/fungible/funcore/fun_dev.c",
  "human_readable_source": "\n\n#include <linux/bitmap.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/nvme.h>\n#include <linux/pci.h>\n#include <linux/wait.h>\n#include <linux/sched/signal.h>\n\n#include \"fun_queue.h\"\n#include \"fun_dev.h\"\n\n#define FUN_ADMIN_CMD_TO_MS 3000\n\nenum {\n\tAQA_ASQS_SHIFT = 0,\n\tAQA_ACQS_SHIFT = 16,\n\tAQA_MIN_QUEUE_SIZE = 2,\n\tAQA_MAX_QUEUE_SIZE = 4096\n};\n\n \nstruct fun_cmd_ctx {\n\tfun_admin_callback_t cb;   \n\tvoid *cb_data;             \n\tint cpu;                   \n};\n\n \nstruct fun_sync_cmd_ctx {\n\tstruct completion compl;\n\tu8 *rsp_buf;               \n\tunsigned int rsp_len;      \n\tu8 rsp_status;             \n};\n\n \nstatic int fun_wait_ready(struct fun_dev *fdev, bool enabled)\n{\n\tunsigned int cap_to = NVME_CAP_TIMEOUT(fdev->cap_reg);\n\tu32 bit = enabled ? NVME_CSTS_RDY : 0;\n\tunsigned long deadline;\n\n\tdeadline = ((cap_to + 1) * HZ / 2) + jiffies;  \n\n\tfor (;;) {\n\t\tu32 csts = readl(fdev->bar + NVME_REG_CSTS);\n\n\t\tif (csts == ~0) {\n\t\t\tdev_err(fdev->dev, \"CSTS register read %#x\\n\", csts);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif ((csts & NVME_CSTS_RDY) == bit)\n\t\t\treturn 0;\n\n\t\tif (time_is_before_jiffies(deadline))\n\t\t\tbreak;\n\n\t\tmsleep(100);\n\t}\n\n\tdev_err(fdev->dev,\n\t\t\"Timed out waiting for device to indicate RDY %u; aborting %s\\n\",\n\t\tenabled, enabled ? \"initialization\" : \"reset\");\n\treturn -ETIMEDOUT;\n}\n\n \nstatic int fun_check_csts_rdy(struct fun_dev *fdev, unsigned int expected_rdy)\n{\n\tu32 csts = readl(fdev->bar + NVME_REG_CSTS);\n\tu32 actual_rdy = csts & NVME_CSTS_RDY;\n\n\tif (csts == ~0) {\n\t\tdev_err(fdev->dev, \"CSTS register read %#x\\n\", csts);\n\t\treturn -EIO;\n\t}\n\tif (actual_rdy != expected_rdy) {\n\t\tdev_err(fdev->dev, \"Unexpected CSTS RDY %u\\n\", actual_rdy);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nstatic int fun_update_cc_enable(struct fun_dev *fdev, unsigned int initial_rdy)\n{\n\tint rc = fun_check_csts_rdy(fdev, initial_rdy);\n\n\tif (rc)\n\t\treturn rc;\n\twritel(fdev->cc_reg, fdev->bar + NVME_REG_CC);\n\treturn fun_wait_ready(fdev, !!(fdev->cc_reg & NVME_CC_ENABLE));\n}\n\nstatic int fun_disable_ctrl(struct fun_dev *fdev)\n{\n\tfdev->cc_reg &= ~(NVME_CC_SHN_MASK | NVME_CC_ENABLE);\n\treturn fun_update_cc_enable(fdev, 1);\n}\n\nstatic int fun_enable_ctrl(struct fun_dev *fdev, u32 admin_cqesz_log2,\n\t\t\t   u32 admin_sqesz_log2)\n{\n\tfdev->cc_reg = (admin_cqesz_log2 << NVME_CC_IOCQES_SHIFT) |\n\t\t       (admin_sqesz_log2 << NVME_CC_IOSQES_SHIFT) |\n\t\t       ((PAGE_SHIFT - 12) << NVME_CC_MPS_SHIFT) |\n\t\t       NVME_CC_ENABLE;\n\n\treturn fun_update_cc_enable(fdev, 0);\n}\n\nstatic int fun_map_bars(struct fun_dev *fdev, const char *name)\n{\n\tstruct pci_dev *pdev = to_pci_dev(fdev->dev);\n\tint err;\n\n\terr = pci_request_mem_regions(pdev, name);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Couldn't get PCI memory resources, err %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tfdev->bar = pci_ioremap_bar(pdev, 0);\n\tif (!fdev->bar) {\n\t\tdev_err(&pdev->dev, \"Couldn't map BAR 0\\n\");\n\t\tpci_release_mem_regions(pdev);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void fun_unmap_bars(struct fun_dev *fdev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(fdev->dev);\n\n\tif (fdev->bar) {\n\t\tiounmap(fdev->bar);\n\t\tfdev->bar = NULL;\n\t\tpci_release_mem_regions(pdev);\n\t}\n}\n\nstatic int fun_set_dma_masks(struct device *dev)\n{\n\tint err;\n\n\terr = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));\n\tif (err)\n\t\tdev_err(dev, \"DMA mask configuration failed, err %d\\n\", err);\n\treturn err;\n}\n\nstatic irqreturn_t fun_admin_irq(int irq, void *data)\n{\n\tstruct fun_queue *funq = data;\n\n\treturn fun_process_cq(funq, 0) ? IRQ_HANDLED : IRQ_NONE;\n}\n\nstatic void fun_complete_admin_cmd(struct fun_queue *funq, void *data,\n\t\t\t\t   void *entry, const struct fun_cqe_info *info)\n{\n\tconst struct fun_admin_rsp_common *rsp_common = entry;\n\tstruct fun_dev *fdev = funq->fdev;\n\tstruct fun_cmd_ctx *cmd_ctx;\n\tint cpu;\n\tu16 cid;\n\n\tif (info->sqhd == cpu_to_be16(0xffff)) {\n\t\tdev_dbg(fdev->dev, \"adminq event\");\n\t\tif (fdev->adminq_cb)\n\t\t\tfdev->adminq_cb(fdev, entry);\n\t\treturn;\n\t}\n\n\tcid = be16_to_cpu(rsp_common->cid);\n\tdev_dbg(fdev->dev, \"admin CQE cid %u, op %u, ret %u\\n\", cid,\n\t\trsp_common->op, rsp_common->ret);\n\n\tcmd_ctx = &fdev->cmd_ctx[cid];\n\tif (cmd_ctx->cpu < 0) {\n\t\tdev_err(fdev->dev,\n\t\t\t\"admin CQE with CID=%u, op=%u does not match a pending command\\n\",\n\t\t\tcid, rsp_common->op);\n\t\treturn;\n\t}\n\n\tif (cmd_ctx->cb)\n\t\tcmd_ctx->cb(fdev, entry, xchg(&cmd_ctx->cb_data, NULL));\n\n\tcpu = cmd_ctx->cpu;\n\tcmd_ctx->cpu = -1;\n\tsbitmap_queue_clear(&fdev->admin_sbq, cid, cpu);\n}\n\nstatic int fun_init_cmd_ctx(struct fun_dev *fdev, unsigned int ntags)\n{\n\tunsigned int i;\n\n\tfdev->cmd_ctx = kvcalloc(ntags, sizeof(*fdev->cmd_ctx), GFP_KERNEL);\n\tif (!fdev->cmd_ctx)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < ntags; i++)\n\t\tfdev->cmd_ctx[i].cpu = -1;\n\n\treturn 0;\n}\n\n \nstatic int fun_enable_admin_queue(struct fun_dev *fdev,\n\t\t\t\t  const struct fun_dev_params *areq)\n{\n\tstruct fun_queue_alloc_req qreq = {\n\t\t.cqe_size_log2 = areq->cqe_size_log2,\n\t\t.sqe_size_log2 = areq->sqe_size_log2,\n\t\t.cq_depth = areq->cq_depth,\n\t\t.sq_depth = areq->sq_depth,\n\t\t.rq_depth = areq->rq_depth,\n\t};\n\tunsigned int ntags = areq->sq_depth - 1;\n\tstruct fun_queue *funq;\n\tint rc;\n\n\tif (fdev->admin_q)\n\t\treturn -EEXIST;\n\n\tif (areq->sq_depth < AQA_MIN_QUEUE_SIZE ||\n\t    areq->sq_depth > AQA_MAX_QUEUE_SIZE ||\n\t    areq->cq_depth < AQA_MIN_QUEUE_SIZE ||\n\t    areq->cq_depth > AQA_MAX_QUEUE_SIZE)\n\t\treturn -EINVAL;\n\n\tfdev->admin_q = fun_alloc_queue(fdev, 0, &qreq);\n\tif (!fdev->admin_q)\n\t\treturn -ENOMEM;\n\n\trc = fun_init_cmd_ctx(fdev, ntags);\n\tif (rc)\n\t\tgoto free_q;\n\n\trc = sbitmap_queue_init_node(&fdev->admin_sbq, ntags, -1, false,\n\t\t\t\t     GFP_KERNEL, dev_to_node(fdev->dev));\n\tif (rc)\n\t\tgoto free_cmd_ctx;\n\n\tfunq = fdev->admin_q;\n\tfunq->cq_vector = 0;\n\trc = fun_request_irq(funq, dev_name(fdev->dev), fun_admin_irq, funq);\n\tif (rc)\n\t\tgoto free_sbq;\n\n\tfun_set_cq_callback(funq, fun_complete_admin_cmd, NULL);\n\tfdev->adminq_cb = areq->event_cb;\n\n\twritel((funq->sq_depth - 1) << AQA_ASQS_SHIFT |\n\t       (funq->cq_depth - 1) << AQA_ACQS_SHIFT,\n\t       fdev->bar + NVME_REG_AQA);\n\n\twriteq(funq->sq_dma_addr, fdev->bar + NVME_REG_ASQ);\n\twriteq(funq->cq_dma_addr, fdev->bar + NVME_REG_ACQ);\n\n\trc = fun_enable_ctrl(fdev, areq->cqe_size_log2, areq->sqe_size_log2);\n\tif (rc)\n\t\tgoto free_irq;\n\n\tif (areq->rq_depth) {\n\t\trc = fun_create_rq(funq);\n\t\tif (rc)\n\t\t\tgoto disable_ctrl;\n\n\t\tfunq_rq_post(funq);\n\t}\n\n\treturn 0;\n\ndisable_ctrl:\n\tfun_disable_ctrl(fdev);\nfree_irq:\n\tfun_free_irq(funq);\nfree_sbq:\n\tsbitmap_queue_free(&fdev->admin_sbq);\nfree_cmd_ctx:\n\tkvfree(fdev->cmd_ctx);\n\tfdev->cmd_ctx = NULL;\nfree_q:\n\tfun_free_queue(fdev->admin_q);\n\tfdev->admin_q = NULL;\n\treturn rc;\n}\n\nstatic void fun_disable_admin_queue(struct fun_dev *fdev)\n{\n\tstruct fun_queue *admq = fdev->admin_q;\n\n\tif (!admq)\n\t\treturn;\n\n\tfun_disable_ctrl(fdev);\n\n\tfun_free_irq(admq);\n\t__fun_process_cq(admq, 0);\n\n\tsbitmap_queue_free(&fdev->admin_sbq);\n\n\tkvfree(fdev->cmd_ctx);\n\tfdev->cmd_ctx = NULL;\n\n\tfun_free_queue(admq);\n\tfdev->admin_q = NULL;\n}\n\n \nstatic bool fun_adminq_stopped(struct fun_dev *fdev)\n{\n\tu32 csts = readl(fdev->bar + NVME_REG_CSTS);\n\n\treturn (csts & (NVME_CSTS_CFS | NVME_CSTS_RDY)) != NVME_CSTS_RDY;\n}\n\nstatic int fun_wait_for_tag(struct fun_dev *fdev, int *cpup)\n{\n\tstruct sbitmap_queue *sbq = &fdev->admin_sbq;\n\tstruct sbq_wait_state *ws = &sbq->ws[0];\n\tDEFINE_SBQ_WAIT(wait);\n\tint tag;\n\n\tfor (;;) {\n\t\tsbitmap_prepare_to_wait(sbq, ws, &wait, TASK_UNINTERRUPTIBLE);\n\t\tif (fdev->suppress_cmds) {\n\t\t\ttag = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\ttag = sbitmap_queue_get(sbq, cpup);\n\t\tif (tag >= 0)\n\t\t\tbreak;\n\t\tschedule();\n\t}\n\n\tsbitmap_finish_wait(sbq, ws, &wait);\n\treturn tag;\n}\n\n \nint fun_submit_admin_cmd(struct fun_dev *fdev, struct fun_admin_req_common *cmd,\n\t\t\t fun_admin_callback_t cb, void *cb_data, bool wait_ok)\n{\n\tstruct fun_queue *funq = fdev->admin_q;\n\tunsigned int cmdsize = cmd->len8 * 8;\n\tstruct fun_cmd_ctx *cmd_ctx;\n\tint tag, cpu, rc = 0;\n\n\tif (WARN_ON(cmdsize > (1 << funq->sqe_size_log2)))\n\t\treturn -EMSGSIZE;\n\n\ttag = sbitmap_queue_get(&fdev->admin_sbq, &cpu);\n\tif (tag < 0) {\n\t\tif (!wait_ok)\n\t\t\treturn -EAGAIN;\n\t\ttag = fun_wait_for_tag(fdev, &cpu);\n\t\tif (tag < 0)\n\t\t\treturn tag;\n\t}\n\n\tcmd->cid = cpu_to_be16(tag);\n\n\tcmd_ctx = &fdev->cmd_ctx[tag];\n\tcmd_ctx->cb = cb;\n\tcmd_ctx->cb_data = cb_data;\n\n\tspin_lock(&funq->sq_lock);\n\n\tif (unlikely(fdev->suppress_cmds)) {\n\t\trc = -ESHUTDOWN;\n\t\tsbitmap_queue_clear(&fdev->admin_sbq, tag, cpu);\n\t} else {\n\t\tcmd_ctx->cpu = cpu;\n\t\tmemcpy(fun_sqe_at(funq, funq->sq_tail), cmd, cmdsize);\n\n\t\tdev_dbg(fdev->dev, \"admin cmd @ %u: %8ph\\n\", funq->sq_tail,\n\t\t\tcmd);\n\n\t\tif (++funq->sq_tail == funq->sq_depth)\n\t\t\tfunq->sq_tail = 0;\n\t\twritel(funq->sq_tail, funq->sq_db);\n\t}\n\tspin_unlock(&funq->sq_lock);\n\treturn rc;\n}\n\n \nstatic bool fun_abandon_admin_cmd(struct fun_dev *fd,\n\t\t\t\t  const struct fun_admin_req_common *cmd,\n\t\t\t\t  void *cb_data)\n{\n\tu16 cid = be16_to_cpu(cmd->cid);\n\tstruct fun_cmd_ctx *cmd_ctx = &fd->cmd_ctx[cid];\n\n\treturn cmpxchg(&cmd_ctx->cb_data, cb_data, NULL) == cb_data;\n}\n\n \nstatic void fun_admin_stop(struct fun_dev *fdev)\n{\n\tspin_lock(&fdev->admin_q->sq_lock);\n\tfdev->suppress_cmds = true;\n\tspin_unlock(&fdev->admin_q->sq_lock);\n\tsbitmap_queue_wake_all(&fdev->admin_sbq);\n}\n\n \nstatic void fun_admin_cmd_sync_cb(struct fun_dev *fd, void *rsp, void *cb_data)\n{\n\tconst struct fun_admin_rsp_common *rsp_common = rsp;\n\tstruct fun_sync_cmd_ctx *ctx = cb_data;\n\n\tif (!ctx)\n\t\treturn;          \n\tif (ctx->rsp_buf) {\n\t\tunsigned int rsp_len = rsp_common->len8 * 8;\n\n\t\tif (unlikely(rsp_len > ctx->rsp_len)) {\n\t\t\tdev_err(fd->dev,\n\t\t\t\t\"response for op %u is %uB > response buffer %uB\\n\",\n\t\t\t\trsp_common->op, rsp_len, ctx->rsp_len);\n\t\t\trsp_len = ctx->rsp_len;\n\t\t}\n\t\tmemcpy(ctx->rsp_buf, rsp, rsp_len);\n\t}\n\tctx->rsp_status = rsp_common->ret;\n\tcomplete(&ctx->compl);\n}\n\n \nint fun_submit_admin_sync_cmd(struct fun_dev *fdev,\n\t\t\t      struct fun_admin_req_common *cmd, void *rsp,\n\t\t\t      size_t rspsize, unsigned int timeout)\n{\n\tstruct fun_sync_cmd_ctx ctx = {\n\t\t.compl = COMPLETION_INITIALIZER_ONSTACK(ctx.compl),\n\t\t.rsp_buf = rsp,\n\t\t.rsp_len = rspsize,\n\t};\n\tunsigned int cmdlen = cmd->len8 * 8;\n\tunsigned long jiffies_left;\n\tint ret;\n\n\tret = fun_submit_admin_cmd(fdev, cmd, fun_admin_cmd_sync_cb, &ctx,\n\t\t\t\t   true);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!timeout)\n\t\ttimeout = FUN_ADMIN_CMD_TO_MS;\n\n\tjiffies_left = wait_for_completion_timeout(&ctx.compl,\n\t\t\t\t\t\t   msecs_to_jiffies(timeout));\n\tif (!jiffies_left) {\n\t\t \n\t\tif (fun_abandon_admin_cmd(fdev, cmd, &ctx)) {\n\t\t\tdev_err(fdev->dev, \"admin command timed out: %*ph\\n\",\n\t\t\t\tcmdlen, cmd);\n\t\t\tfun_admin_stop(fdev);\n\t\t\t \n\t\t\tif (fun_adminq_stopped(fdev))\n\t\t\t\tdev_err(fdev->dev,\n\t\t\t\t\t\"device does not accept admin commands\\n\");\n\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\twait_for_completion(&ctx.compl);\n\t}\n\n\tif (ctx.rsp_status) {\n\t\tdev_err(fdev->dev, \"admin command failed, err %d: %*ph\\n\",\n\t\t\tctx.rsp_status, cmdlen, cmd);\n\t}\n\n\treturn -ctx.rsp_status;\n}\nEXPORT_SYMBOL_GPL(fun_submit_admin_sync_cmd);\n\n \nint fun_get_res_count(struct fun_dev *fdev, enum fun_admin_op res)\n{\n\tunion {\n\t\tstruct fun_admin_res_count_req req;\n\t\tstruct fun_admin_res_count_rsp rsp;\n\t} cmd;\n\tint rc;\n\n\tcmd.req.common = FUN_ADMIN_REQ_COMMON_INIT2(res, sizeof(cmd.req));\n\tcmd.req.count = FUN_ADMIN_SIMPLE_SUBOP_INIT(FUN_ADMIN_SUBOP_RES_COUNT,\n\t\t\t\t\t\t    0, 0);\n\n\trc = fun_submit_admin_sync_cmd(fdev, &cmd.req.common, &cmd.rsp,\n\t\t\t\t       sizeof(cmd), 0);\n\treturn rc ? rc : be32_to_cpu(cmd.rsp.count.data);\n}\nEXPORT_SYMBOL_GPL(fun_get_res_count);\n\n \nint fun_res_destroy(struct fun_dev *fdev, enum fun_admin_op res,\n\t\t    unsigned int flags, u32 id)\n{\n\tstruct fun_admin_generic_destroy_req req = {\n\t\t.common = FUN_ADMIN_REQ_COMMON_INIT2(res, sizeof(req)),\n\t\t.destroy = FUN_ADMIN_SIMPLE_SUBOP_INIT(FUN_ADMIN_SUBOP_DESTROY,\n\t\t\t\t\t\t       flags, id)\n\t};\n\n\treturn fun_submit_admin_sync_cmd(fdev, &req.common, NULL, 0, 0);\n}\nEXPORT_SYMBOL_GPL(fun_res_destroy);\n\n \nint fun_bind(struct fun_dev *fdev, enum fun_admin_bind_type type0,\n\t     unsigned int id0, enum fun_admin_bind_type type1,\n\t     unsigned int id1)\n{\n\tstruct {\n\t\tstruct fun_admin_bind_req req;\n\t\tstruct fun_admin_bind_entry entry[2];\n\t} cmd = {\n\t\t.req.common = FUN_ADMIN_REQ_COMMON_INIT2(FUN_ADMIN_OP_BIND,\n\t\t\t\t\t\t\t sizeof(cmd)),\n\t\t.entry[0] = FUN_ADMIN_BIND_ENTRY_INIT(type0, id0),\n\t\t.entry[1] = FUN_ADMIN_BIND_ENTRY_INIT(type1, id1),\n\t};\n\n\treturn fun_submit_admin_sync_cmd(fdev, &cmd.req.common, NULL, 0, 0);\n}\nEXPORT_SYMBOL_GPL(fun_bind);\n\nstatic int fun_get_dev_limits(struct fun_dev *fdev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(fdev->dev);\n\tunsigned int cq_count, sq_count, num_dbs;\n\tint rc;\n\n\trc = fun_get_res_count(fdev, FUN_ADMIN_OP_EPCQ);\n\tif (rc < 0)\n\t\treturn rc;\n\tcq_count = rc;\n\n\trc = fun_get_res_count(fdev, FUN_ADMIN_OP_EPSQ);\n\tif (rc < 0)\n\t\treturn rc;\n\tsq_count = rc;\n\n\t \n\tif (cq_count < 2 || sq_count < 2 + !!fdev->admin_q->rq_depth)\n\t\treturn -EINVAL;\n\n\t \n\tnum_dbs = (pci_resource_len(pdev, 0) - NVME_REG_DBS) >>\n\t\t  (2 + NVME_CAP_STRIDE(fdev->cap_reg));\n\tfdev->max_qid = min3(cq_count, sq_count, num_dbs / 2) - 1;\n\tfdev->kern_end_qid = fdev->max_qid + 1;\n\treturn 0;\n}\n\n \nstatic int fun_alloc_irqs(struct pci_dev *pdev, unsigned int min_vecs)\n{\n\tint vecs, num_msix = pci_msix_vec_count(pdev);\n\n\tif (num_msix < 0)\n\t\treturn num_msix;\n\tif (min_vecs > num_msix)\n\t\treturn -ERANGE;\n\n\tvecs = pci_alloc_irq_vectors(pdev, min_vecs, num_msix, PCI_IRQ_MSIX);\n\tif (vecs > 0) {\n\t\tdev_info(&pdev->dev,\n\t\t\t \"Allocated %d IRQ vectors of %d requested\\n\",\n\t\t\t vecs, num_msix);\n\t} else {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Unable to allocate at least %u IRQ vectors\\n\",\n\t\t\tmin_vecs);\n\t}\n\treturn vecs;\n}\n\n \nstatic int fun_alloc_irq_mgr(struct fun_dev *fdev)\n{\n\tfdev->irq_map = bitmap_zalloc(fdev->num_irqs, GFP_KERNEL);\n\tif (!fdev->irq_map)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&fdev->irqmgr_lock);\n\t \n\t__set_bit(0, fdev->irq_map);\n\tfdev->irqs_avail = fdev->num_irqs - 1;\n\treturn 0;\n}\n\n \nint fun_reserve_irqs(struct fun_dev *fdev, unsigned int nirqs, u16 *irq_indices)\n{\n\tunsigned int b, n = 0;\n\tint err = -ENOSPC;\n\n\tif (!nirqs)\n\t\treturn 0;\n\n\tspin_lock(&fdev->irqmgr_lock);\n\tif (nirqs > fdev->irqs_avail)\n\t\tgoto unlock;\n\n\tfor_each_clear_bit(b, fdev->irq_map, fdev->num_irqs) {\n\t\t__set_bit(b, fdev->irq_map);\n\t\tirq_indices[n++] = b;\n\t\tif (n >= nirqs)\n\t\t\tbreak;\n\t}\n\n\tWARN_ON(n < nirqs);\n\tfdev->irqs_avail -= n;\n\terr = n;\nunlock:\n\tspin_unlock(&fdev->irqmgr_lock);\n\treturn err;\n}\nEXPORT_SYMBOL(fun_reserve_irqs);\n\n \nvoid fun_release_irqs(struct fun_dev *fdev, unsigned int nirqs,\n\t\t      u16 *irq_indices)\n{\n\tunsigned int i;\n\n\tspin_lock(&fdev->irqmgr_lock);\n\tfor (i = 0; i < nirqs; i++)\n\t\t__clear_bit(irq_indices[i], fdev->irq_map);\n\tfdev->irqs_avail += nirqs;\n\tspin_unlock(&fdev->irqmgr_lock);\n}\nEXPORT_SYMBOL(fun_release_irqs);\n\nstatic void fun_serv_handler(struct work_struct *work)\n{\n\tstruct fun_dev *fd = container_of(work, struct fun_dev, service_task);\n\n\tif (test_bit(FUN_SERV_DISABLED, &fd->service_flags))\n\t\treturn;\n\tif (fd->serv_cb)\n\t\tfd->serv_cb(fd);\n}\n\nvoid fun_serv_stop(struct fun_dev *fd)\n{\n\tset_bit(FUN_SERV_DISABLED, &fd->service_flags);\n\tcancel_work_sync(&fd->service_task);\n}\nEXPORT_SYMBOL_GPL(fun_serv_stop);\n\nvoid fun_serv_restart(struct fun_dev *fd)\n{\n\tclear_bit(FUN_SERV_DISABLED, &fd->service_flags);\n\tif (fd->service_flags)\n\t\tschedule_work(&fd->service_task);\n}\nEXPORT_SYMBOL_GPL(fun_serv_restart);\n\nvoid fun_serv_sched(struct fun_dev *fd)\n{\n\tif (!test_bit(FUN_SERV_DISABLED, &fd->service_flags))\n\t\tschedule_work(&fd->service_task);\n}\nEXPORT_SYMBOL_GPL(fun_serv_sched);\n\n \nstatic int sanitize_dev(struct fun_dev *fdev)\n{\n\tint rc;\n\n\tfdev->cap_reg = readq(fdev->bar + NVME_REG_CAP);\n\tfdev->cc_reg = readl(fdev->bar + NVME_REG_CC);\n\n\t \n\trc = fun_wait_ready(fdev, fdev->cc_reg & NVME_CC_ENABLE);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (fdev->cc_reg & NVME_CC_ENABLE)\n\t\trc = fun_disable_ctrl(fdev);\n\n\treturn rc;\n}\n\n \nvoid fun_dev_disable(struct fun_dev *fdev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(fdev->dev);\n\n\tpci_set_drvdata(pdev, NULL);\n\n\tif (fdev->fw_handle != FUN_HCI_ID_INVALID) {\n\t\tfun_res_destroy(fdev, FUN_ADMIN_OP_SWUPGRADE, 0,\n\t\t\t\tfdev->fw_handle);\n\t\tfdev->fw_handle = FUN_HCI_ID_INVALID;\n\t}\n\n\tfun_disable_admin_queue(fdev);\n\n\tbitmap_free(fdev->irq_map);\n\tpci_free_irq_vectors(pdev);\n\n\tpci_disable_device(pdev);\n\n\tfun_unmap_bars(fdev);\n}\nEXPORT_SYMBOL(fun_dev_disable);\n\n \nint fun_dev_enable(struct fun_dev *fdev, struct pci_dev *pdev,\n\t\t   const struct fun_dev_params *areq, const char *name)\n{\n\tint rc;\n\n\tfdev->dev = &pdev->dev;\n\trc = fun_map_bars(fdev, name);\n\tif (rc)\n\t\treturn rc;\n\n\trc = fun_set_dma_masks(fdev->dev);\n\tif (rc)\n\t\tgoto unmap;\n\n\trc = pci_enable_device_mem(pdev);\n\tif (rc) {\n\t\tdev_err(&pdev->dev, \"Couldn't enable device, err %d\\n\", rc);\n\t\tgoto unmap;\n\t}\n\n\trc = sanitize_dev(fdev);\n\tif (rc)\n\t\tgoto disable_dev;\n\n\tfdev->fw_handle = FUN_HCI_ID_INVALID;\n\tfdev->q_depth = NVME_CAP_MQES(fdev->cap_reg) + 1;\n\tfdev->db_stride = 1 << NVME_CAP_STRIDE(fdev->cap_reg);\n\tfdev->dbs = fdev->bar + NVME_REG_DBS;\n\n\tINIT_WORK(&fdev->service_task, fun_serv_handler);\n\tfdev->service_flags = FUN_SERV_DISABLED;\n\tfdev->serv_cb = areq->serv_cb;\n\n\trc = fun_alloc_irqs(pdev, areq->min_msix + 1);  \n\tif (rc < 0)\n\t\tgoto disable_dev;\n\tfdev->num_irqs = rc;\n\n\trc = fun_alloc_irq_mgr(fdev);\n\tif (rc)\n\t\tgoto free_irqs;\n\n\tpci_set_master(pdev);\n\trc = fun_enable_admin_queue(fdev, areq);\n\tif (rc)\n\t\tgoto free_irq_mgr;\n\n\trc = fun_get_dev_limits(fdev);\n\tif (rc < 0)\n\t\tgoto disable_admin;\n\n\tpci_save_state(pdev);\n\tpci_set_drvdata(pdev, fdev);\n\tpcie_print_link_status(pdev);\n\tdev_dbg(fdev->dev, \"q_depth %u, db_stride %u, max qid %d kern_end_qid %d\\n\",\n\t\tfdev->q_depth, fdev->db_stride, fdev->max_qid,\n\t\tfdev->kern_end_qid);\n\treturn 0;\n\ndisable_admin:\n\tfun_disable_admin_queue(fdev);\nfree_irq_mgr:\n\tbitmap_free(fdev->irq_map);\nfree_irqs:\n\tpci_free_irq_vectors(pdev);\ndisable_dev:\n\tpci_disable_device(pdev);\nunmap:\n\tfun_unmap_bars(fdev);\n\treturn rc;\n}\nEXPORT_SYMBOL(fun_dev_enable);\n\nMODULE_AUTHOR(\"Dimitris Michailidis <dmichail@fungible.com>\");\nMODULE_DESCRIPTION(\"Core services driver for Fungible devices\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}