{
  "module_name": "gve.h",
  "hash_id": "551c94ffcb8dec898d4ea67d7347237706dc3771fc610ae1a8cac2d66d21d431",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/google/gve/gve.h",
  "human_readable_source": " \n\n#ifndef _GVE_H_\n#define _GVE_H_\n\n#include <linux/dma-mapping.h>\n#include <linux/netdevice.h>\n#include <linux/pci.h>\n#include <linux/u64_stats_sync.h>\n#include <net/xdp.h>\n\n#include \"gve_desc.h\"\n#include \"gve_desc_dqo.h\"\n\n#ifndef PCI_VENDOR_ID_GOOGLE\n#define PCI_VENDOR_ID_GOOGLE\t0x1ae0\n#endif\n\n#define PCI_DEV_ID_GVNIC\t0x0042\n\n#define GVE_REGISTER_BAR\t0\n#define GVE_DOORBELL_BAR\t2\n\n \n#define GVE_TX_MAX_IOVEC\t4\n \n#define GVE_MIN_MSIX 3\n\n \n#define GVE_TX_STATS_REPORT_NUM\t6\n#define GVE_RX_STATS_REPORT_NUM\t2\n\n \n#define GVE_STATS_REPORT_TIMER_PERIOD\t20000\n\n \n#define NIC_TX_STATS_REPORT_NUM\t0\n#define NIC_RX_STATS_REPORT_NUM\t4\n\n#define GVE_DATA_SLOT_ADDR_PAGE_MASK (~(PAGE_SIZE - 1))\n\n \n#define GVE_NUM_PTYPES\t1024\n\n#define GVE_RX_BUFFER_SIZE_DQO 2048\n\n#define GVE_XDP_ACTIONS 5\n\n#define GVE_GQ_TX_MIN_PKT_DESC_BYTES 182\n\n#define DQO_QPL_DEFAULT_TX_PAGES 512\n#define DQO_QPL_DEFAULT_RX_PAGES 2048\n\n \n#define GVE_DQO_TX_MAX\t0x3FFFF\n\n#define GVE_TX_BUF_SHIFT_DQO 11\n\n \n#define GVE_TX_BUF_SIZE_DQO BIT(GVE_TX_BUF_SHIFT_DQO)\n#define GVE_TX_BUFS_PER_PAGE_DQO (PAGE_SIZE >> GVE_TX_BUF_SHIFT_DQO)\n#define GVE_MAX_TX_BUFS_PER_PKT (DIV_ROUND_UP(GVE_DQO_TX_MAX, GVE_TX_BUF_SIZE_DQO))\n\n \n#define GVE_DQO_QPL_ONDEMAND_ALLOC_THRESHOLD 96\n\n \nstruct gve_rx_desc_queue {\n\tstruct gve_rx_desc *desc_ring;  \n\tdma_addr_t bus;  \n\tu8 seqno;  \n};\n\n \nstruct gve_rx_slot_page_info {\n\tstruct page *page;\n\tvoid *page_address;\n\tu32 page_offset;  \n\tint pagecnt_bias;  \n\tu16 pad;  \n\tu8 can_flip;  \n};\n\n \nstruct gve_queue_page_list {\n\tu32 id;  \n\tu32 num_entries;\n\tstruct page **pages;  \n\tdma_addr_t *page_buses;  \n};\n\n \nstruct gve_rx_data_queue {\n\tunion gve_rx_data_slot *data_ring;  \n\tdma_addr_t data_bus;  \n\tstruct gve_rx_slot_page_info *page_info;  \n\tstruct gve_queue_page_list *qpl;  \n\tu8 raw_addressing;  \n};\n\nstruct gve_priv;\n\n \nstruct gve_rx_buf_queue_dqo {\n\tstruct gve_rx_desc_dqo *desc_ring;\n\tdma_addr_t bus;\n\tu32 head;  \n\tu32 tail;  \n\tu32 mask;  \n};\n\n \nstruct gve_rx_compl_queue_dqo {\n\tstruct gve_rx_compl_desc_dqo *desc_ring;\n\tdma_addr_t bus;\n\n\t \n\tint num_free_slots;\n\n\t \n\tu8 cur_gen_bit;\n\n\t \n\tu32 head;\n\tu32 mask;  \n};\n\n \nstruct gve_rx_buf_state_dqo {\n\t \n\tstruct gve_rx_slot_page_info page_info;\n\n\t \n\tdma_addr_t addr;\n\n\t \n\tu32 last_single_ref_offset;\n\n\t \n\ts16 next;\n};\n\n \nstruct gve_index_list {\n\ts16 head;\n\ts16 tail;\n};\n\n \nstruct gve_rx_ctx {\n\t \n\tstruct sk_buff *skb_head;\n\tstruct sk_buff *skb_tail;\n\tu32 total_size;\n\tu8 frag_cnt;\n\tbool drop_pkt;\n};\n\nstruct gve_rx_cnts {\n\tu32 ok_pkt_bytes;\n\tu16 ok_pkt_cnt;\n\tu16 total_pkt_cnt;\n\tu16 cont_pkt_cnt;\n\tu16 desc_err_pkt_cnt;\n};\n\n \nstruct gve_rx_ring {\n\tstruct gve_priv *gve;\n\tunion {\n\t\t \n\t\tstruct {\n\t\t\tstruct gve_rx_desc_queue desc;\n\t\t\tstruct gve_rx_data_queue data;\n\n\t\t\t \n\t\t\tu32 db_threshold;\n\t\t\tu16 packet_buffer_size;\n\n\t\t\tu32 qpl_copy_pool_mask;\n\t\t\tu32 qpl_copy_pool_head;\n\t\t\tstruct gve_rx_slot_page_info *qpl_copy_pool;\n\t\t};\n\n\t\t \n\t\tstruct {\n\t\t\tstruct gve_rx_buf_queue_dqo bufq;\n\t\t\tstruct gve_rx_compl_queue_dqo complq;\n\n\t\t\tstruct gve_rx_buf_state_dqo *buf_states;\n\t\t\tu16 num_buf_states;\n\n\t\t\t \n\t\t\ts16 free_buf_states;\n\n\t\t\t \n\t\t\tstruct gve_index_list recycled_buf_states;\n\n\t\t\t \n\t\t\tstruct gve_index_list used_buf_states;\n\n\t\t\t \n\t\t\tstruct gve_queue_page_list *qpl;\n\n\t\t\t \n\t\t\tu32 next_qpl_page_idx;\n\n\t\t\t \n\t\t\tu16 used_buf_states_cnt;\n\t\t} dqo;\n\t};\n\n\tu64 rbytes;  \n\tu64 rpackets;  \n\tu32 cnt;  \n\tu32 fill_cnt;  \n\tu32 mask;  \n\tu64 rx_copybreak_pkt;  \n\tu64 rx_copied_pkt;  \n\tu64 rx_skb_alloc_fail;  \n\tu64 rx_buf_alloc_fail;  \n\tu64 rx_desc_err_dropped_pkt;  \n\tu64 rx_cont_packet_cnt;  \n\tu64 rx_frag_flip_cnt;  \n\tu64 rx_frag_copy_cnt;  \n\tu64 rx_frag_alloc_cnt;  \n\tu64 xdp_tx_errors;\n\tu64 xdp_redirect_errors;\n\tu64 xdp_alloc_fails;\n\tu64 xdp_actions[GVE_XDP_ACTIONS];\n\tu32 q_num;  \n\tu32 ntfy_id;  \n\tstruct gve_queue_resources *q_resources;  \n\tdma_addr_t q_resources_bus;  \n\tstruct u64_stats_sync statss;  \n\n\tstruct gve_rx_ctx ctx;  \n\n\t \n\tstruct xdp_rxq_info xdp_rxq;\n\tstruct xdp_rxq_info xsk_rxq;\n\tstruct xsk_buff_pool *xsk_pool;\n\tstruct page_frag_cache page_cache;  \n};\n\n \nunion gve_tx_desc {\n\tstruct gve_tx_pkt_desc pkt;  \n\tstruct gve_tx_mtd_desc mtd;  \n\tstruct gve_tx_seg_desc seg;  \n};\n\n \nstruct gve_tx_iovec {\n\tu32 iov_offset;  \n\tu32 iov_len;  \n\tu32 iov_padding;  \n};\n\n \nstruct gve_tx_buffer_state {\n\tunion {\n\t\tstruct sk_buff *skb;  \n\t\tstruct xdp_frame *xdp_frame;  \n\t};\n\tstruct {\n\t\tu16 size;  \n\t\tu8 is_xsk;  \n\t} xdp;\n\tunion {\n\t\tstruct gve_tx_iovec iov[GVE_TX_MAX_IOVEC];  \n\t\tstruct {\n\t\t\tDEFINE_DMA_UNMAP_ADDR(dma);\n\t\t\tDEFINE_DMA_UNMAP_LEN(len);\n\t\t};\n\t};\n};\n\n \nstruct gve_tx_fifo {\n\tvoid *base;  \n\tu32 size;  \n\tatomic_t available;  \n\tu32 head;  \n\tstruct gve_queue_page_list *qpl;  \n};\n\n \nunion gve_tx_desc_dqo {\n\tstruct gve_tx_pkt_desc_dqo pkt;\n\tstruct gve_tx_tso_context_desc_dqo tso_ctx;\n\tstruct gve_tx_general_context_desc_dqo general_ctx;\n};\n\nenum gve_packet_state {\n\t \n\tGVE_PACKET_STATE_UNALLOCATED,\n\t \n\tGVE_PACKET_STATE_PENDING_DATA_COMPL,\n\t \n\tGVE_PACKET_STATE_PENDING_REINJECT_COMPL,\n\t \n\tGVE_PACKET_STATE_TIMED_OUT_COMPL,\n};\n\nstruct gve_tx_pending_packet_dqo {\n\tstruct sk_buff *skb;  \n\n\t \n\tunion {\n\t\tstruct {\n\t\t\tDEFINE_DMA_UNMAP_ADDR(dma[MAX_SKB_FRAGS + 1]);\n\t\t\tDEFINE_DMA_UNMAP_LEN(len[MAX_SKB_FRAGS + 1]);\n\t\t};\n\t\ts16 tx_qpl_buf_ids[GVE_MAX_TX_BUFS_PER_PKT];\n\t};\n\n\tu16 num_bufs;\n\n\t \n\ts16 next;\n\n\t \n\ts16 prev;\n\n\t \n\tu8 state;\n\n\t \n\tunsigned long timeout_jiffies;\n};\n\n \nstruct gve_tx_ring {\n\t \n\tunion {\n\t\t \n\t\tstruct {\n\t\t\tstruct gve_tx_fifo tx_fifo;\n\t\t\tu32 req;  \n\t\t\tu32 done;  \n\t\t};\n\n\t\t \n\t\tstruct {\n\t\t\t \n\t\t\ts16 free_pending_packets;\n\n\t\t\t \n\t\t\tu32 head;\n\t\t\tu32 tail;  \n\n\t\t\t \n\t\t\tu32 last_re_idx;\n\n\t\t\t \n\t\t\tu16 posted_packet_desc_cnt;\n\t\t\t \n\t\t\tu16 completed_packet_desc_cnt;\n\n\t\t\t \n\t\t\tstruct {\n\t\t\t        \n\t\t\t\ts16 free_tx_qpl_buf_head;\n\n\t\t\t        \n\t\t\t\tu32 alloc_tx_qpl_buf_cnt;\n\n\t\t\t\t \n\t\t\t\tu32 free_tx_qpl_buf_cnt;\n\t\t\t};\n\t\t} dqo_tx;\n\t};\n\n\t \n\tunion {\n\t\t \n\t\tstruct {\n\t\t\t \n\t\t\tspinlock_t clean_lock;\n\t\t\t \n\t\t\tspinlock_t xdp_lock;\n\t\t};\n\n\t\t \n\t\tstruct {\n\t\t\tu32 head;  \n\n\t\t\t \n\t\t\tu8 cur_gen_bit;\n\n\t\t\t \n\t\t\tatomic_t free_pending_packets;\n\n\t\t\t \n\t\t\tatomic_t hw_tx_head;\n\n\t\t\t \n\t\t\tstruct gve_index_list miss_completions;\n\n\t\t\t \n\t\t\tstruct gve_index_list timed_out_completions;\n\n\t\t\t \n\t\t\tstruct {\n\t\t\t\t \n\t\t\t\tatomic_t free_tx_qpl_buf_head;\n\n\t\t\t\t \n\t\t\t\tatomic_t free_tx_qpl_buf_cnt;\n\t\t\t};\n\t\t} dqo_compl;\n\t} ____cacheline_aligned;\n\tu64 pkt_done;  \n\tu64 bytes_done;  \n\tu64 dropped_pkt;  \n\tu64 dma_mapping_error;  \n\n\t \n\tunion {\n\t\t \n\t\tstruct {\n\t\t\tunion gve_tx_desc *desc;\n\n\t\t\t \n\t\t\tstruct gve_tx_buffer_state *info;\n\t\t};\n\n\t\t \n\t\tstruct {\n\t\t\tunion gve_tx_desc_dqo *tx_ring;\n\t\t\tstruct gve_tx_compl_desc *compl_ring;\n\n\t\t\tstruct gve_tx_pending_packet_dqo *pending_packets;\n\t\t\ts16 num_pending_packets;\n\n\t\t\tu32 complq_mask;  \n\n\t\t\t \n\t\t\tstruct {\n\t\t\t\t \n\t\t\t\tstruct gve_queue_page_list *qpl;\n\n\t\t\t\t \n\t\t\t\ts16 *tx_qpl_buf_next;\n\t\t\t\tu32 num_tx_qpl_bufs;\n\t\t\t};\n\t\t} dqo;\n\t} ____cacheline_aligned;\n\tstruct netdev_queue *netdev_txq;\n\tstruct gve_queue_resources *q_resources;  \n\tstruct device *dev;\n\tu32 mask;  \n\tu8 raw_addressing;  \n\n\t \n\tu32 q_num ____cacheline_aligned;  \n\tu32 stop_queue;  \n\tu32 wake_queue;  \n\tu32 queue_timeout;  \n\tu32 ntfy_id;  \n\tu32 last_kick_msec;  \n\tdma_addr_t bus;  \n\tdma_addr_t q_resources_bus;  \n\tdma_addr_t complq_bus_dqo;  \n\tstruct u64_stats_sync statss;  \n\tstruct xsk_buff_pool *xsk_pool;\n\tu32 xdp_xsk_wakeup;\n\tu32 xdp_xsk_done;\n\tu64 xdp_xsk_sent;\n\tu64 xdp_xmit;\n\tu64 xdp_xmit_errors;\n} ____cacheline_aligned;\n\n \nstruct gve_notify_block {\n\t__be32 *irq_db_index;  \n\tchar name[IFNAMSIZ + 16];  \n\tstruct napi_struct napi;  \n\tstruct gve_priv *priv;\n\tstruct gve_tx_ring *tx;  \n\tstruct gve_rx_ring *rx;  \n};\n\n \nstruct gve_queue_config {\n\tu16 max_queues;\n\tu16 num_queues;  \n};\n\n \nstruct gve_qpl_config {\n\tu32 qpl_map_size;  \n\tunsigned long *qpl_id_map;  \n};\n\nstruct gve_options_dqo_rda {\n\tu16 tx_comp_ring_entries;  \n\tu16 rx_buff_ring_entries;  \n};\n\nstruct gve_irq_db {\n\t__be32 index;\n} ____cacheline_aligned;\n\nstruct gve_ptype {\n\tu8 l3_type;   \n\tu8 l4_type;   \n};\n\nstruct gve_ptype_lut {\n\tstruct gve_ptype ptypes[GVE_NUM_PTYPES];\n};\n\n \nenum gve_queue_format {\n\tGVE_QUEUE_FORMAT_UNSPECIFIED\t= 0x0,\n\tGVE_GQI_RDA_FORMAT\t\t= 0x1,\n\tGVE_GQI_QPL_FORMAT\t\t= 0x2,\n\tGVE_DQO_RDA_FORMAT\t\t= 0x3,\n\tGVE_DQO_QPL_FORMAT\t\t= 0x4,\n};\n\nstruct gve_priv {\n\tstruct net_device *dev;\n\tstruct gve_tx_ring *tx;  \n\tstruct gve_rx_ring *rx;  \n\tstruct gve_queue_page_list *qpls;  \n\tstruct gve_notify_block *ntfy_blocks;  \n\tstruct gve_irq_db *irq_db_indices;  \n\tdma_addr_t irq_db_indices_bus;\n\tstruct msix_entry *msix_vectors;  \n\tchar mgmt_msix_name[IFNAMSIZ + 16];\n\tu32 mgmt_msix_idx;\n\t__be32 *counter_array;  \n\tdma_addr_t counter_array_bus;\n\n\tu16 num_event_counters;\n\tu16 tx_desc_cnt;  \n\tu16 rx_desc_cnt;  \n\tu16 tx_pages_per_qpl;  \n\tu16 rx_pages_per_qpl;  \n\tu16 rx_data_slot_cnt;  \n\tu64 max_registered_pages;\n\tu64 num_registered_pages;  \n\tstruct bpf_prog *xdp_prog;  \n\tu32 rx_copybreak;  \n\tu16 default_num_queues;  \n\n\tu16 num_xdp_queues;\n\tstruct gve_queue_config tx_cfg;\n\tstruct gve_queue_config rx_cfg;\n\tstruct gve_qpl_config qpl_cfg;  \n\tu32 num_ntfy_blks;  \n\n\tstruct gve_registers __iomem *reg_bar0;  \n\t__be32 __iomem *db_bar2;  \n\tu32 msg_enable;\t \n\tstruct pci_dev *pdev;\n\n\t \n\tu32 tx_timeo_cnt;\n\n\t \n\tunion gve_adminq_command *adminq;\n\tdma_addr_t adminq_bus_addr;\n\tu32 adminq_mask;  \n\tu32 adminq_prod_cnt;  \n\tu32 adminq_cmd_fail;  \n\tu32 adminq_timeouts;  \n\t \n\tu32 adminq_describe_device_cnt;\n\tu32 adminq_cfg_device_resources_cnt;\n\tu32 adminq_register_page_list_cnt;\n\tu32 adminq_unregister_page_list_cnt;\n\tu32 adminq_create_tx_queue_cnt;\n\tu32 adminq_create_rx_queue_cnt;\n\tu32 adminq_destroy_tx_queue_cnt;\n\tu32 adminq_destroy_rx_queue_cnt;\n\tu32 adminq_dcfg_device_resources_cnt;\n\tu32 adminq_set_driver_parameter_cnt;\n\tu32 adminq_report_stats_cnt;\n\tu32 adminq_report_link_speed_cnt;\n\tu32 adminq_get_ptype_map_cnt;\n\tu32 adminq_verify_driver_compatibility_cnt;\n\n\t \n\tu32 interface_up_cnt;  \n\tu32 interface_down_cnt;  \n\tu32 reset_cnt;  \n\tu32 page_alloc_fail;  \n\tu32 dma_mapping_error;  \n\tu32 stats_report_trigger_cnt;  \n\tu32 suspend_cnt;  \n\tu32 resume_cnt;  \n\tstruct workqueue_struct *gve_wq;\n\tstruct work_struct service_task;\n\tstruct work_struct stats_report_task;\n\tunsigned long service_task_flags;\n\tunsigned long state_flags;\n\n\tstruct gve_stats_report *stats_report;\n\tu64 stats_report_len;\n\tdma_addr_t stats_report_bus;  \n\tunsigned long ethtool_flags;\n\n\tunsigned long stats_report_timer_period;\n\tstruct timer_list stats_report_timer;\n\n\t \n\tu64 link_speed;\n\tbool up_before_suspend;  \n\n\tstruct gve_options_dqo_rda options_dqo_rda;\n\tstruct gve_ptype_lut *ptype_lut_dqo;\n\n\t \n\tint data_buffer_size_dqo;\n\n\tenum gve_queue_format queue_format;\n\n\t \n\tu32 tx_coalesce_usecs;\n\tu32 rx_coalesce_usecs;\n};\n\nenum gve_service_task_flags_bit {\n\tGVE_PRIV_FLAGS_DO_RESET\t\t\t= 1,\n\tGVE_PRIV_FLAGS_RESET_IN_PROGRESS\t= 2,\n\tGVE_PRIV_FLAGS_PROBE_IN_PROGRESS\t= 3,\n\tGVE_PRIV_FLAGS_DO_REPORT_STATS = 4,\n};\n\nenum gve_state_flags_bit {\n\tGVE_PRIV_FLAGS_ADMIN_QUEUE_OK\t\t= 1,\n\tGVE_PRIV_FLAGS_DEVICE_RESOURCES_OK\t= 2,\n\tGVE_PRIV_FLAGS_DEVICE_RINGS_OK\t\t= 3,\n\tGVE_PRIV_FLAGS_NAPI_ENABLED\t\t= 4,\n};\n\nenum gve_ethtool_flags_bit {\n\tGVE_PRIV_FLAGS_REPORT_STATS\t\t= 0,\n};\n\nstatic inline bool gve_get_do_reset(struct gve_priv *priv)\n{\n\treturn test_bit(GVE_PRIV_FLAGS_DO_RESET, &priv->service_task_flags);\n}\n\nstatic inline void gve_set_do_reset(struct gve_priv *priv)\n{\n\tset_bit(GVE_PRIV_FLAGS_DO_RESET, &priv->service_task_flags);\n}\n\nstatic inline void gve_clear_do_reset(struct gve_priv *priv)\n{\n\tclear_bit(GVE_PRIV_FLAGS_DO_RESET, &priv->service_task_flags);\n}\n\nstatic inline bool gve_get_reset_in_progress(struct gve_priv *priv)\n{\n\treturn test_bit(GVE_PRIV_FLAGS_RESET_IN_PROGRESS,\n\t\t\t&priv->service_task_flags);\n}\n\nstatic inline void gve_set_reset_in_progress(struct gve_priv *priv)\n{\n\tset_bit(GVE_PRIV_FLAGS_RESET_IN_PROGRESS, &priv->service_task_flags);\n}\n\nstatic inline void gve_clear_reset_in_progress(struct gve_priv *priv)\n{\n\tclear_bit(GVE_PRIV_FLAGS_RESET_IN_PROGRESS, &priv->service_task_flags);\n}\n\nstatic inline bool gve_get_probe_in_progress(struct gve_priv *priv)\n{\n\treturn test_bit(GVE_PRIV_FLAGS_PROBE_IN_PROGRESS,\n\t\t\t&priv->service_task_flags);\n}\n\nstatic inline void gve_set_probe_in_progress(struct gve_priv *priv)\n{\n\tset_bit(GVE_PRIV_FLAGS_PROBE_IN_PROGRESS, &priv->service_task_flags);\n}\n\nstatic inline void gve_clear_probe_in_progress(struct gve_priv *priv)\n{\n\tclear_bit(GVE_PRIV_FLAGS_PROBE_IN_PROGRESS, &priv->service_task_flags);\n}\n\nstatic inline bool gve_get_do_report_stats(struct gve_priv *priv)\n{\n\treturn test_bit(GVE_PRIV_FLAGS_DO_REPORT_STATS,\n\t\t\t&priv->service_task_flags);\n}\n\nstatic inline void gve_set_do_report_stats(struct gve_priv *priv)\n{\n\tset_bit(GVE_PRIV_FLAGS_DO_REPORT_STATS, &priv->service_task_flags);\n}\n\nstatic inline void gve_clear_do_report_stats(struct gve_priv *priv)\n{\n\tclear_bit(GVE_PRIV_FLAGS_DO_REPORT_STATS, &priv->service_task_flags);\n}\n\nstatic inline bool gve_get_admin_queue_ok(struct gve_priv *priv)\n{\n\treturn test_bit(GVE_PRIV_FLAGS_ADMIN_QUEUE_OK, &priv->state_flags);\n}\n\nstatic inline void gve_set_admin_queue_ok(struct gve_priv *priv)\n{\n\tset_bit(GVE_PRIV_FLAGS_ADMIN_QUEUE_OK, &priv->state_flags);\n}\n\nstatic inline void gve_clear_admin_queue_ok(struct gve_priv *priv)\n{\n\tclear_bit(GVE_PRIV_FLAGS_ADMIN_QUEUE_OK, &priv->state_flags);\n}\n\nstatic inline bool gve_get_device_resources_ok(struct gve_priv *priv)\n{\n\treturn test_bit(GVE_PRIV_FLAGS_DEVICE_RESOURCES_OK, &priv->state_flags);\n}\n\nstatic inline void gve_set_device_resources_ok(struct gve_priv *priv)\n{\n\tset_bit(GVE_PRIV_FLAGS_DEVICE_RESOURCES_OK, &priv->state_flags);\n}\n\nstatic inline void gve_clear_device_resources_ok(struct gve_priv *priv)\n{\n\tclear_bit(GVE_PRIV_FLAGS_DEVICE_RESOURCES_OK, &priv->state_flags);\n}\n\nstatic inline bool gve_get_device_rings_ok(struct gve_priv *priv)\n{\n\treturn test_bit(GVE_PRIV_FLAGS_DEVICE_RINGS_OK, &priv->state_flags);\n}\n\nstatic inline void gve_set_device_rings_ok(struct gve_priv *priv)\n{\n\tset_bit(GVE_PRIV_FLAGS_DEVICE_RINGS_OK, &priv->state_flags);\n}\n\nstatic inline void gve_clear_device_rings_ok(struct gve_priv *priv)\n{\n\tclear_bit(GVE_PRIV_FLAGS_DEVICE_RINGS_OK, &priv->state_flags);\n}\n\nstatic inline bool gve_get_napi_enabled(struct gve_priv *priv)\n{\n\treturn test_bit(GVE_PRIV_FLAGS_NAPI_ENABLED, &priv->state_flags);\n}\n\nstatic inline void gve_set_napi_enabled(struct gve_priv *priv)\n{\n\tset_bit(GVE_PRIV_FLAGS_NAPI_ENABLED, &priv->state_flags);\n}\n\nstatic inline void gve_clear_napi_enabled(struct gve_priv *priv)\n{\n\tclear_bit(GVE_PRIV_FLAGS_NAPI_ENABLED, &priv->state_flags);\n}\n\nstatic inline bool gve_get_report_stats(struct gve_priv *priv)\n{\n\treturn test_bit(GVE_PRIV_FLAGS_REPORT_STATS, &priv->ethtool_flags);\n}\n\nstatic inline void gve_clear_report_stats(struct gve_priv *priv)\n{\n\tclear_bit(GVE_PRIV_FLAGS_REPORT_STATS, &priv->ethtool_flags);\n}\n\n \nstatic inline __be32 __iomem *gve_irq_doorbell(struct gve_priv *priv,\n\t\t\t\t\t       struct gve_notify_block *block)\n{\n\treturn &priv->db_bar2[be32_to_cpu(*block->irq_db_index)];\n}\n\n \nstatic inline u32 gve_tx_idx_to_ntfy(struct gve_priv *priv, u32 queue_idx)\n{\n\treturn queue_idx;\n}\n\n \nstatic inline u32 gve_rx_idx_to_ntfy(struct gve_priv *priv, u32 queue_idx)\n{\n\treturn (priv->num_ntfy_blks / 2) + queue_idx;\n}\n\nstatic inline bool gve_is_qpl(struct gve_priv *priv)\n{\n\treturn priv->queue_format == GVE_GQI_QPL_FORMAT ||\n\t\tpriv->queue_format == GVE_DQO_QPL_FORMAT;\n}\n\n \nstatic inline u32 gve_num_tx_qpls(struct gve_priv *priv)\n{\n\tif (!gve_is_qpl(priv))\n\t\treturn 0;\n\n\treturn priv->tx_cfg.num_queues + priv->num_xdp_queues;\n}\n\n \nstatic inline u32 gve_num_xdp_qpls(struct gve_priv *priv)\n{\n\tif (priv->queue_format != GVE_GQI_QPL_FORMAT)\n\t\treturn 0;\n\n\treturn priv->num_xdp_queues;\n}\n\n \nstatic inline u32 gve_num_rx_qpls(struct gve_priv *priv)\n{\n\tif (!gve_is_qpl(priv))\n\t\treturn 0;\n\n\treturn priv->rx_cfg.num_queues;\n}\n\nstatic inline u32 gve_tx_qpl_id(struct gve_priv *priv, int tx_qid)\n{\n\treturn tx_qid;\n}\n\nstatic inline u32 gve_rx_qpl_id(struct gve_priv *priv, int rx_qid)\n{\n\treturn priv->tx_cfg.max_queues + rx_qid;\n}\n\nstatic inline u32 gve_tx_start_qpl_id(struct gve_priv *priv)\n{\n\treturn gve_tx_qpl_id(priv, 0);\n}\n\nstatic inline u32 gve_rx_start_qpl_id(struct gve_priv *priv)\n{\n\treturn gve_rx_qpl_id(priv, 0);\n}\n\n \nstatic inline\nstruct gve_queue_page_list *gve_assign_tx_qpl(struct gve_priv *priv, int tx_qid)\n{\n\tint id = gve_tx_qpl_id(priv, tx_qid);\n\n\t \n\tif (test_bit(id, priv->qpl_cfg.qpl_id_map))\n\t\treturn NULL;\n\n\tset_bit(id, priv->qpl_cfg.qpl_id_map);\n\treturn &priv->qpls[id];\n}\n\n \nstatic inline\nstruct gve_queue_page_list *gve_assign_rx_qpl(struct gve_priv *priv, int rx_qid)\n{\n\tint id = gve_rx_qpl_id(priv, rx_qid);\n\n\t \n\tif (test_bit(id, priv->qpl_cfg.qpl_id_map))\n\t\treturn NULL;\n\n\tset_bit(id, priv->qpl_cfg.qpl_id_map);\n\treturn &priv->qpls[id];\n}\n\n \nstatic inline void gve_unassign_qpl(struct gve_priv *priv, int id)\n{\n\tclear_bit(id, priv->qpl_cfg.qpl_id_map);\n}\n\n \nstatic inline enum dma_data_direction gve_qpl_dma_dir(struct gve_priv *priv,\n\t\t\t\t\t\t      int id)\n{\n\tif (id < gve_rx_start_qpl_id(priv))\n\t\treturn DMA_TO_DEVICE;\n\telse\n\t\treturn DMA_FROM_DEVICE;\n}\n\nstatic inline bool gve_is_gqi(struct gve_priv *priv)\n{\n\treturn priv->queue_format == GVE_GQI_RDA_FORMAT ||\n\t\tpriv->queue_format == GVE_GQI_QPL_FORMAT;\n}\n\nstatic inline u32 gve_num_tx_queues(struct gve_priv *priv)\n{\n\treturn priv->tx_cfg.num_queues + priv->num_xdp_queues;\n}\n\nstatic inline u32 gve_xdp_tx_queue_id(struct gve_priv *priv, u32 queue_id)\n{\n\treturn priv->tx_cfg.num_queues + queue_id;\n}\n\nstatic inline u32 gve_xdp_tx_start_queue_id(struct gve_priv *priv)\n{\n\treturn gve_xdp_tx_queue_id(priv, 0);\n}\n\n \nint gve_alloc_page(struct gve_priv *priv, struct device *dev,\n\t\t   struct page **page, dma_addr_t *dma,\n\t\t   enum dma_data_direction, gfp_t gfp_flags);\nvoid gve_free_page(struct device *dev, struct page *page, dma_addr_t dma,\n\t\t   enum dma_data_direction);\n \nnetdev_tx_t gve_tx(struct sk_buff *skb, struct net_device *dev);\nint gve_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,\n\t\t u32 flags);\nint gve_xdp_xmit_one(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t     void *data, int len, void *frame_p);\nvoid gve_xdp_tx_flush(struct gve_priv *priv, u32 xdp_qid);\nbool gve_tx_poll(struct gve_notify_block *block, int budget);\nbool gve_xdp_poll(struct gve_notify_block *block, int budget);\nint gve_tx_alloc_rings(struct gve_priv *priv, int start_id, int num_rings);\nvoid gve_tx_free_rings_gqi(struct gve_priv *priv, int start_id, int num_rings);\nu32 gve_tx_load_event_counter(struct gve_priv *priv,\n\t\t\t      struct gve_tx_ring *tx);\nbool gve_tx_clean_pending(struct gve_priv *priv, struct gve_tx_ring *tx);\n \nvoid gve_rx_write_doorbell(struct gve_priv *priv, struct gve_rx_ring *rx);\nint gve_rx_poll(struct gve_notify_block *block, int budget);\nbool gve_rx_work_pending(struct gve_rx_ring *rx);\nint gve_rx_alloc_rings(struct gve_priv *priv);\nvoid gve_rx_free_rings_gqi(struct gve_priv *priv);\n \nvoid gve_schedule_reset(struct gve_priv *priv);\nint gve_reset(struct gve_priv *priv, bool attempt_teardown);\nint gve_adjust_queues(struct gve_priv *priv,\n\t\t      struct gve_queue_config new_rx_config,\n\t\t      struct gve_queue_config new_tx_config);\n \nvoid gve_handle_report_stats(struct gve_priv *priv);\n \nextern const struct ethtool_ops gve_ethtool_ops;\n \nextern char gve_driver_name[];\nextern const char gve_version_str[];\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}