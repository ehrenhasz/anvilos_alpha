{
  "module_name": "gve_main.c",
  "hash_id": "827341e756ea0e414b677000665bcdcb93883831249e2d13f9c2e3493c26cef0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/google/gve/gve_main.c",
  "human_readable_source": "\n \n\n#include <linux/bpf.h>\n#include <linux/cpumask.h>\n#include <linux/etherdevice.h>\n#include <linux/filter.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/workqueue.h>\n#include <linux/utsname.h>\n#include <linux/version.h>\n#include <net/sch_generic.h>\n#include <net/xdp_sock_drv.h>\n#include \"gve.h\"\n#include \"gve_dqo.h\"\n#include \"gve_adminq.h\"\n#include \"gve_register.h\"\n\n#define GVE_DEFAULT_RX_COPYBREAK\t(256)\n\n#define DEFAULT_MSG_LEVEL\t(NETIF_MSG_DRV | NETIF_MSG_LINK)\n#define GVE_VERSION\t\t\"1.0.0\"\n#define GVE_VERSION_PREFIX\t\"GVE-\"\n\n\n#define MIN_TX_TIMEOUT_GAP (1000 * 10)\n\nchar gve_driver_name[] = \"gve\";\nconst char gve_version_str[] = GVE_VERSION;\nstatic const char gve_version_prefix[] = GVE_VERSION_PREFIX;\n\nstatic int gve_verify_driver_compatibility(struct gve_priv *priv)\n{\n\tint err;\n\tstruct gve_driver_info *driver_info;\n\tdma_addr_t driver_info_bus;\n\n\tdriver_info = dma_alloc_coherent(&priv->pdev->dev,\n\t\t\t\t\t sizeof(struct gve_driver_info),\n\t\t\t\t\t &driver_info_bus, GFP_KERNEL);\n\tif (!driver_info)\n\t\treturn -ENOMEM;\n\n\t*driver_info = (struct gve_driver_info) {\n\t\t.os_type = 1,  \n\t\t.os_version_major = cpu_to_be32(LINUX_VERSION_MAJOR),\n\t\t.os_version_minor = cpu_to_be32(LINUX_VERSION_SUBLEVEL),\n\t\t.os_version_sub = cpu_to_be32(LINUX_VERSION_PATCHLEVEL),\n\t\t.driver_capability_flags = {\n\t\t\tcpu_to_be64(GVE_DRIVER_CAPABILITY_FLAGS1),\n\t\t\tcpu_to_be64(GVE_DRIVER_CAPABILITY_FLAGS2),\n\t\t\tcpu_to_be64(GVE_DRIVER_CAPABILITY_FLAGS3),\n\t\t\tcpu_to_be64(GVE_DRIVER_CAPABILITY_FLAGS4),\n\t\t},\n\t};\n\tstrscpy(driver_info->os_version_str1, utsname()->release,\n\t\tsizeof(driver_info->os_version_str1));\n\tstrscpy(driver_info->os_version_str2, utsname()->version,\n\t\tsizeof(driver_info->os_version_str2));\n\n\terr = gve_adminq_verify_driver_compatibility(priv,\n\t\t\t\t\t\t     sizeof(struct gve_driver_info),\n\t\t\t\t\t\t     driver_info_bus);\n\n\t \n\tif (err == -EOPNOTSUPP)\n\t\terr = 0;\n\n\tdma_free_coherent(&priv->pdev->dev,\n\t\t\t  sizeof(struct gve_driver_info),\n\t\t\t  driver_info, driver_info_bus);\n\treturn err;\n}\n\nstatic netdev_tx_t gve_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\n\tif (gve_is_gqi(priv))\n\t\treturn gve_tx(skb, dev);\n\telse\n\t\treturn gve_tx_dqo(skb, dev);\n}\n\nstatic void gve_get_stats(struct net_device *dev, struct rtnl_link_stats64 *s)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tunsigned int start;\n\tu64 packets, bytes;\n\tint num_tx_queues;\n\tint ring;\n\n\tnum_tx_queues = gve_num_tx_queues(priv);\n\tif (priv->rx) {\n\t\tfor (ring = 0; ring < priv->rx_cfg.num_queues; ring++) {\n\t\t\tdo {\n\t\t\t\tstart =\n\t\t\t\t  u64_stats_fetch_begin(&priv->rx[ring].statss);\n\t\t\t\tpackets = priv->rx[ring].rpackets;\n\t\t\t\tbytes = priv->rx[ring].rbytes;\n\t\t\t} while (u64_stats_fetch_retry(&priv->rx[ring].statss,\n\t\t\t\t\t\t       start));\n\t\t\ts->rx_packets += packets;\n\t\t\ts->rx_bytes += bytes;\n\t\t}\n\t}\n\tif (priv->tx) {\n\t\tfor (ring = 0; ring < num_tx_queues; ring++) {\n\t\t\tdo {\n\t\t\t\tstart =\n\t\t\t\t  u64_stats_fetch_begin(&priv->tx[ring].statss);\n\t\t\t\tpackets = priv->tx[ring].pkt_done;\n\t\t\t\tbytes = priv->tx[ring].bytes_done;\n\t\t\t} while (u64_stats_fetch_retry(&priv->tx[ring].statss,\n\t\t\t\t\t\t       start));\n\t\t\ts->tx_packets += packets;\n\t\t\ts->tx_bytes += bytes;\n\t\t}\n\t}\n}\n\nstatic int gve_alloc_counter_array(struct gve_priv *priv)\n{\n\tpriv->counter_array =\n\t\tdma_alloc_coherent(&priv->pdev->dev,\n\t\t\t\t   priv->num_event_counters *\n\t\t\t\t   sizeof(*priv->counter_array),\n\t\t\t\t   &priv->counter_array_bus, GFP_KERNEL);\n\tif (!priv->counter_array)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void gve_free_counter_array(struct gve_priv *priv)\n{\n\tif (!priv->counter_array)\n\t\treturn;\n\n\tdma_free_coherent(&priv->pdev->dev,\n\t\t\t  priv->num_event_counters *\n\t\t\t  sizeof(*priv->counter_array),\n\t\t\t  priv->counter_array, priv->counter_array_bus);\n\tpriv->counter_array = NULL;\n}\n\n \nstatic void gve_stats_report_task(struct work_struct *work)\n{\n\tstruct gve_priv *priv = container_of(work, struct gve_priv,\n\t\t\t\t\t     stats_report_task);\n\tif (gve_get_do_report_stats(priv)) {\n\t\tgve_handle_report_stats(priv);\n\t\tgve_clear_do_report_stats(priv);\n\t}\n}\n\nstatic void gve_stats_report_schedule(struct gve_priv *priv)\n{\n\tif (!gve_get_probe_in_progress(priv) &&\n\t    !gve_get_reset_in_progress(priv)) {\n\t\tgve_set_do_report_stats(priv);\n\t\tqueue_work(priv->gve_wq, &priv->stats_report_task);\n\t}\n}\n\nstatic void gve_stats_report_timer(struct timer_list *t)\n{\n\tstruct gve_priv *priv = from_timer(priv, t, stats_report_timer);\n\n\tmod_timer(&priv->stats_report_timer,\n\t\t  round_jiffies(jiffies +\n\t\t  msecs_to_jiffies(priv->stats_report_timer_period)));\n\tgve_stats_report_schedule(priv);\n}\n\nstatic int gve_alloc_stats_report(struct gve_priv *priv)\n{\n\tint tx_stats_num, rx_stats_num;\n\n\ttx_stats_num = (GVE_TX_STATS_REPORT_NUM + NIC_TX_STATS_REPORT_NUM) *\n\t\t       gve_num_tx_queues(priv);\n\trx_stats_num = (GVE_RX_STATS_REPORT_NUM + NIC_RX_STATS_REPORT_NUM) *\n\t\t       priv->rx_cfg.num_queues;\n\tpriv->stats_report_len = struct_size(priv->stats_report, stats,\n\t\t\t\t\t     size_add(tx_stats_num, rx_stats_num));\n\tpriv->stats_report =\n\t\tdma_alloc_coherent(&priv->pdev->dev, priv->stats_report_len,\n\t\t\t\t   &priv->stats_report_bus, GFP_KERNEL);\n\tif (!priv->stats_report)\n\t\treturn -ENOMEM;\n\t \n\ttimer_setup(&priv->stats_report_timer, gve_stats_report_timer, 0);\n\tpriv->stats_report_timer_period = GVE_STATS_REPORT_TIMER_PERIOD;\n\treturn 0;\n}\n\nstatic void gve_free_stats_report(struct gve_priv *priv)\n{\n\tif (!priv->stats_report)\n\t\treturn;\n\n\tdel_timer_sync(&priv->stats_report_timer);\n\tdma_free_coherent(&priv->pdev->dev, priv->stats_report_len,\n\t\t\t  priv->stats_report, priv->stats_report_bus);\n\tpriv->stats_report = NULL;\n}\n\nstatic irqreturn_t gve_mgmnt_intr(int irq, void *arg)\n{\n\tstruct gve_priv *priv = arg;\n\n\tqueue_work(priv->gve_wq, &priv->service_task);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t gve_intr(int irq, void *arg)\n{\n\tstruct gve_notify_block *block = arg;\n\tstruct gve_priv *priv = block->priv;\n\n\tiowrite32be(GVE_IRQ_MASK, gve_irq_doorbell(priv, block));\n\tnapi_schedule_irqoff(&block->napi);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t gve_intr_dqo(int irq, void *arg)\n{\n\tstruct gve_notify_block *block = arg;\n\n\t \n\tnapi_schedule_irqoff(&block->napi);\n\treturn IRQ_HANDLED;\n}\n\nstatic int gve_napi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct gve_notify_block *block;\n\t__be32 __iomem *irq_doorbell;\n\tbool reschedule = false;\n\tstruct gve_priv *priv;\n\tint work_done = 0;\n\n\tblock = container_of(napi, struct gve_notify_block, napi);\n\tpriv = block->priv;\n\n\tif (block->tx) {\n\t\tif (block->tx->q_num < priv->tx_cfg.num_queues)\n\t\t\treschedule |= gve_tx_poll(block, budget);\n\t\telse if (budget)\n\t\t\treschedule |= gve_xdp_poll(block, budget);\n\t}\n\n\tif (!budget)\n\t\treturn 0;\n\n\tif (block->rx) {\n\t\twork_done = gve_rx_poll(block, budget);\n\t\treschedule |= work_done == budget;\n\t}\n\n\tif (reschedule)\n\t\treturn budget;\n\n        \n\tif (likely(napi_complete_done(napi, work_done))) {\n\t\tirq_doorbell = gve_irq_doorbell(priv, block);\n\t\tiowrite32be(GVE_IRQ_ACK | GVE_IRQ_EVENT, irq_doorbell);\n\n\t\t \n\t\tmb();\n\n\t\tif (block->tx)\n\t\t\treschedule |= gve_tx_clean_pending(priv, block->tx);\n\t\tif (block->rx)\n\t\t\treschedule |= gve_rx_work_pending(block->rx);\n\n\t\tif (reschedule && napi_reschedule(napi))\n\t\t\tiowrite32be(GVE_IRQ_MASK, irq_doorbell);\n\t}\n\treturn work_done;\n}\n\nstatic int gve_napi_poll_dqo(struct napi_struct *napi, int budget)\n{\n\tstruct gve_notify_block *block =\n\t\tcontainer_of(napi, struct gve_notify_block, napi);\n\tstruct gve_priv *priv = block->priv;\n\tbool reschedule = false;\n\tint work_done = 0;\n\n\tif (block->tx)\n\t\treschedule |= gve_tx_poll_dqo(block,  true);\n\n\tif (!budget)\n\t\treturn 0;\n\n\tif (block->rx) {\n\t\twork_done = gve_rx_poll_dqo(block, budget);\n\t\treschedule |= work_done == budget;\n\t}\n\n\tif (reschedule)\n\t\treturn budget;\n\n\tif (likely(napi_complete_done(napi, work_done))) {\n\t\t \n\t\tgve_write_irq_doorbell_dqo(priv, block,\n\t\t\t\t\t   GVE_ITR_NO_UPDATE_DQO | GVE_ITR_ENABLE_BIT_DQO);\n\t}\n\n\treturn work_done;\n}\n\nstatic int gve_alloc_notify_blocks(struct gve_priv *priv)\n{\n\tint num_vecs_requested = priv->num_ntfy_blks + 1;\n\tunsigned int active_cpus;\n\tint vecs_enabled;\n\tint i, j;\n\tint err;\n\n\tpriv->msix_vectors = kvcalloc(num_vecs_requested,\n\t\t\t\t      sizeof(*priv->msix_vectors), GFP_KERNEL);\n\tif (!priv->msix_vectors)\n\t\treturn -ENOMEM;\n\tfor (i = 0; i < num_vecs_requested; i++)\n\t\tpriv->msix_vectors[i].entry = i;\n\tvecs_enabled = pci_enable_msix_range(priv->pdev, priv->msix_vectors,\n\t\t\t\t\t     GVE_MIN_MSIX, num_vecs_requested);\n\tif (vecs_enabled < 0) {\n\t\tdev_err(&priv->pdev->dev, \"Could not enable min msix %d/%d\\n\",\n\t\t\tGVE_MIN_MSIX, vecs_enabled);\n\t\terr = vecs_enabled;\n\t\tgoto abort_with_msix_vectors;\n\t}\n\tif (vecs_enabled != num_vecs_requested) {\n\t\tint new_num_ntfy_blks = (vecs_enabled - 1) & ~0x1;\n\t\tint vecs_per_type = new_num_ntfy_blks / 2;\n\t\tint vecs_left = new_num_ntfy_blks % 2;\n\n\t\tpriv->num_ntfy_blks = new_num_ntfy_blks;\n\t\tpriv->mgmt_msix_idx = priv->num_ntfy_blks;\n\t\tpriv->tx_cfg.max_queues = min_t(int, priv->tx_cfg.max_queues,\n\t\t\t\t\t\tvecs_per_type);\n\t\tpriv->rx_cfg.max_queues = min_t(int, priv->rx_cfg.max_queues,\n\t\t\t\t\t\tvecs_per_type + vecs_left);\n\t\tdev_err(&priv->pdev->dev,\n\t\t\t\"Could not enable desired msix, only enabled %d, adjusting tx max queues to %d, and rx max queues to %d\\n\",\n\t\t\tvecs_enabled, priv->tx_cfg.max_queues,\n\t\t\tpriv->rx_cfg.max_queues);\n\t\tif (priv->tx_cfg.num_queues > priv->tx_cfg.max_queues)\n\t\t\tpriv->tx_cfg.num_queues = priv->tx_cfg.max_queues;\n\t\tif (priv->rx_cfg.num_queues > priv->rx_cfg.max_queues)\n\t\t\tpriv->rx_cfg.num_queues = priv->rx_cfg.max_queues;\n\t}\n\t \n\tactive_cpus = min_t(int, priv->num_ntfy_blks / 2, num_online_cpus());\n\n\t \n\tsnprintf(priv->mgmt_msix_name, sizeof(priv->mgmt_msix_name), \"gve-mgmnt@pci:%s\",\n\t\t pci_name(priv->pdev));\n\terr = request_irq(priv->msix_vectors[priv->mgmt_msix_idx].vector,\n\t\t\t  gve_mgmnt_intr, 0, priv->mgmt_msix_name, priv);\n\tif (err) {\n\t\tdev_err(&priv->pdev->dev, \"Did not receive management vector.\\n\");\n\t\tgoto abort_with_msix_enabled;\n\t}\n\tpriv->irq_db_indices =\n\t\tdma_alloc_coherent(&priv->pdev->dev,\n\t\t\t\t   priv->num_ntfy_blks *\n\t\t\t\t   sizeof(*priv->irq_db_indices),\n\t\t\t\t   &priv->irq_db_indices_bus, GFP_KERNEL);\n\tif (!priv->irq_db_indices) {\n\t\terr = -ENOMEM;\n\t\tgoto abort_with_mgmt_vector;\n\t}\n\n\tpriv->ntfy_blocks = kvzalloc(priv->num_ntfy_blks *\n\t\t\t\t     sizeof(*priv->ntfy_blocks), GFP_KERNEL);\n\tif (!priv->ntfy_blocks) {\n\t\terr = -ENOMEM;\n\t\tgoto abort_with_irq_db_indices;\n\t}\n\n\t \n\tfor (i = 0; i < priv->num_ntfy_blks; i++) {\n\t\tstruct gve_notify_block *block = &priv->ntfy_blocks[i];\n\t\tint msix_idx = i;\n\n\t\tsnprintf(block->name, sizeof(block->name), \"gve-ntfy-blk%d@pci:%s\",\n\t\t\t i, pci_name(priv->pdev));\n\t\tblock->priv = priv;\n\t\terr = request_irq(priv->msix_vectors[msix_idx].vector,\n\t\t\t\t  gve_is_gqi(priv) ? gve_intr : gve_intr_dqo,\n\t\t\t\t  0, block->name, block);\n\t\tif (err) {\n\t\t\tdev_err(&priv->pdev->dev,\n\t\t\t\t\"Failed to receive msix vector %d\\n\", i);\n\t\t\tgoto abort_with_some_ntfy_blocks;\n\t\t}\n\t\tirq_set_affinity_hint(priv->msix_vectors[msix_idx].vector,\n\t\t\t\t      get_cpu_mask(i % active_cpus));\n\t\tblock->irq_db_index = &priv->irq_db_indices[i].index;\n\t}\n\treturn 0;\nabort_with_some_ntfy_blocks:\n\tfor (j = 0; j < i; j++) {\n\t\tstruct gve_notify_block *block = &priv->ntfy_blocks[j];\n\t\tint msix_idx = j;\n\n\t\tirq_set_affinity_hint(priv->msix_vectors[msix_idx].vector,\n\t\t\t\t      NULL);\n\t\tfree_irq(priv->msix_vectors[msix_idx].vector, block);\n\t}\n\tkvfree(priv->ntfy_blocks);\n\tpriv->ntfy_blocks = NULL;\nabort_with_irq_db_indices:\n\tdma_free_coherent(&priv->pdev->dev, priv->num_ntfy_blks *\n\t\t\t  sizeof(*priv->irq_db_indices),\n\t\t\t  priv->irq_db_indices, priv->irq_db_indices_bus);\n\tpriv->irq_db_indices = NULL;\nabort_with_mgmt_vector:\n\tfree_irq(priv->msix_vectors[priv->mgmt_msix_idx].vector, priv);\nabort_with_msix_enabled:\n\tpci_disable_msix(priv->pdev);\nabort_with_msix_vectors:\n\tkvfree(priv->msix_vectors);\n\tpriv->msix_vectors = NULL;\n\treturn err;\n}\n\nstatic void gve_free_notify_blocks(struct gve_priv *priv)\n{\n\tint i;\n\n\tif (!priv->msix_vectors)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < priv->num_ntfy_blks; i++) {\n\t\tstruct gve_notify_block *block = &priv->ntfy_blocks[i];\n\t\tint msix_idx = i;\n\n\t\tirq_set_affinity_hint(priv->msix_vectors[msix_idx].vector,\n\t\t\t\t      NULL);\n\t\tfree_irq(priv->msix_vectors[msix_idx].vector, block);\n\t}\n\tfree_irq(priv->msix_vectors[priv->mgmt_msix_idx].vector, priv);\n\tkvfree(priv->ntfy_blocks);\n\tpriv->ntfy_blocks = NULL;\n\tdma_free_coherent(&priv->pdev->dev, priv->num_ntfy_blks *\n\t\t\t  sizeof(*priv->irq_db_indices),\n\t\t\t  priv->irq_db_indices, priv->irq_db_indices_bus);\n\tpriv->irq_db_indices = NULL;\n\tpci_disable_msix(priv->pdev);\n\tkvfree(priv->msix_vectors);\n\tpriv->msix_vectors = NULL;\n}\n\nstatic int gve_setup_device_resources(struct gve_priv *priv)\n{\n\tint err;\n\n\terr = gve_alloc_counter_array(priv);\n\tif (err)\n\t\treturn err;\n\terr = gve_alloc_notify_blocks(priv);\n\tif (err)\n\t\tgoto abort_with_counter;\n\terr = gve_alloc_stats_report(priv);\n\tif (err)\n\t\tgoto abort_with_ntfy_blocks;\n\terr = gve_adminq_configure_device_resources(priv,\n\t\t\t\t\t\t    priv->counter_array_bus,\n\t\t\t\t\t\t    priv->num_event_counters,\n\t\t\t\t\t\t    priv->irq_db_indices_bus,\n\t\t\t\t\t\t    priv->num_ntfy_blks);\n\tif (unlikely(err)) {\n\t\tdev_err(&priv->pdev->dev,\n\t\t\t\"could not setup device_resources: err=%d\\n\", err);\n\t\terr = -ENXIO;\n\t\tgoto abort_with_stats_report;\n\t}\n\n\tif (!gve_is_gqi(priv)) {\n\t\tpriv->ptype_lut_dqo = kvzalloc(sizeof(*priv->ptype_lut_dqo),\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!priv->ptype_lut_dqo) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto abort_with_stats_report;\n\t\t}\n\t\terr = gve_adminq_get_ptype_map_dqo(priv, priv->ptype_lut_dqo);\n\t\tif (err) {\n\t\t\tdev_err(&priv->pdev->dev,\n\t\t\t\t\"Failed to get ptype map: err=%d\\n\", err);\n\t\t\tgoto abort_with_ptype_lut;\n\t\t}\n\t}\n\n\terr = gve_adminq_report_stats(priv, priv->stats_report_len,\n\t\t\t\t      priv->stats_report_bus,\n\t\t\t\t      GVE_STATS_REPORT_TIMER_PERIOD);\n\tif (err)\n\t\tdev_err(&priv->pdev->dev,\n\t\t\t\"Failed to report stats: err=%d\\n\", err);\n\tgve_set_device_resources_ok(priv);\n\treturn 0;\n\nabort_with_ptype_lut:\n\tkvfree(priv->ptype_lut_dqo);\n\tpriv->ptype_lut_dqo = NULL;\nabort_with_stats_report:\n\tgve_free_stats_report(priv);\nabort_with_ntfy_blocks:\n\tgve_free_notify_blocks(priv);\nabort_with_counter:\n\tgve_free_counter_array(priv);\n\n\treturn err;\n}\n\nstatic void gve_trigger_reset(struct gve_priv *priv);\n\nstatic void gve_teardown_device_resources(struct gve_priv *priv)\n{\n\tint err;\n\n\t \n\tif (gve_get_device_resources_ok(priv)) {\n\t\t \n\t\terr = gve_adminq_report_stats(priv, 0, 0x0, GVE_STATS_REPORT_TIMER_PERIOD);\n\t\tif (err) {\n\t\t\tdev_err(&priv->pdev->dev,\n\t\t\t\t\"Failed to detach stats report: err=%d\\n\", err);\n\t\t\tgve_trigger_reset(priv);\n\t\t}\n\t\terr = gve_adminq_deconfigure_device_resources(priv);\n\t\tif (err) {\n\t\t\tdev_err(&priv->pdev->dev,\n\t\t\t\t\"Could not deconfigure device resources: err=%d\\n\",\n\t\t\t\terr);\n\t\t\tgve_trigger_reset(priv);\n\t\t}\n\t}\n\n\tkvfree(priv->ptype_lut_dqo);\n\tpriv->ptype_lut_dqo = NULL;\n\n\tgve_free_counter_array(priv);\n\tgve_free_notify_blocks(priv);\n\tgve_free_stats_report(priv);\n\tgve_clear_device_resources_ok(priv);\n}\n\nstatic void gve_add_napi(struct gve_priv *priv, int ntfy_idx,\n\t\t\t int (*gve_poll)(struct napi_struct *, int))\n{\n\tstruct gve_notify_block *block = &priv->ntfy_blocks[ntfy_idx];\n\n\tnetif_napi_add(priv->dev, &block->napi, gve_poll);\n}\n\nstatic void gve_remove_napi(struct gve_priv *priv, int ntfy_idx)\n{\n\tstruct gve_notify_block *block = &priv->ntfy_blocks[ntfy_idx];\n\n\tnetif_napi_del(&block->napi);\n}\n\nstatic int gve_register_xdp_qpls(struct gve_priv *priv)\n{\n\tint start_id;\n\tint err;\n\tint i;\n\n\tstart_id = gve_tx_qpl_id(priv, gve_xdp_tx_start_queue_id(priv));\n\tfor (i = start_id; i < start_id + gve_num_xdp_qpls(priv); i++) {\n\t\terr = gve_adminq_register_page_list(priv, &priv->qpls[i]);\n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"failed to register queue page list %d\\n\",\n\t\t\t\t  priv->qpls[i].id);\n\t\t\t \n\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int gve_register_qpls(struct gve_priv *priv)\n{\n\tint start_id;\n\tint err;\n\tint i;\n\n\tstart_id = gve_tx_start_qpl_id(priv);\n\tfor (i = start_id; i < start_id + gve_num_tx_qpls(priv); i++) {\n\t\terr = gve_adminq_register_page_list(priv, &priv->qpls[i]);\n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"failed to register queue page list %d\\n\",\n\t\t\t\t  priv->qpls[i].id);\n\t\t\t \n\t\t\treturn err;\n\t\t}\n\t}\n\n\tstart_id = gve_rx_start_qpl_id(priv);\n\tfor (i = start_id; i < start_id + gve_num_rx_qpls(priv); i++) {\n\t\terr = gve_adminq_register_page_list(priv, &priv->qpls[i]);\n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"failed to register queue page list %d\\n\",\n\t\t\t\t  priv->qpls[i].id);\n\t\t\t \n\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int gve_unregister_xdp_qpls(struct gve_priv *priv)\n{\n\tint start_id;\n\tint err;\n\tint i;\n\n\tstart_id = gve_tx_qpl_id(priv, gve_xdp_tx_start_queue_id(priv));\n\tfor (i = start_id; i < start_id + gve_num_xdp_qpls(priv); i++) {\n\t\terr = gve_adminq_unregister_page_list(priv, priv->qpls[i].id);\n\t\t \n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"Failed to unregister queue page list %d\\n\",\n\t\t\t\t  priv->qpls[i].id);\n\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int gve_unregister_qpls(struct gve_priv *priv)\n{\n\tint start_id;\n\tint err;\n\tint i;\n\n\tstart_id = gve_tx_start_qpl_id(priv);\n\tfor (i = start_id; i < start_id + gve_num_tx_qpls(priv); i++) {\n\t\terr = gve_adminq_unregister_page_list(priv, priv->qpls[i].id);\n\t\t \n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"Failed to unregister queue page list %d\\n\",\n\t\t\t\t  priv->qpls[i].id);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tstart_id = gve_rx_start_qpl_id(priv);\n\tfor (i = start_id; i < start_id + gve_num_rx_qpls(priv); i++) {\n\t\terr = gve_adminq_unregister_page_list(priv, priv->qpls[i].id);\n\t\t \n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"Failed to unregister queue page list %d\\n\",\n\t\t\t\t  priv->qpls[i].id);\n\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int gve_create_xdp_rings(struct gve_priv *priv)\n{\n\tint err;\n\n\terr = gve_adminq_create_tx_queues(priv,\n\t\t\t\t\t  gve_xdp_tx_start_queue_id(priv),\n\t\t\t\t\t  priv->num_xdp_queues);\n\tif (err) {\n\t\tnetif_err(priv, drv, priv->dev, \"failed to create %d XDP tx queues\\n\",\n\t\t\t  priv->num_xdp_queues);\n\t\t \n\t\treturn err;\n\t}\n\tnetif_dbg(priv, drv, priv->dev, \"created %d XDP tx queues\\n\",\n\t\t  priv->num_xdp_queues);\n\n\treturn 0;\n}\n\nstatic int gve_create_rings(struct gve_priv *priv)\n{\n\tint num_tx_queues = gve_num_tx_queues(priv);\n\tint err;\n\tint i;\n\n\terr = gve_adminq_create_tx_queues(priv, 0, num_tx_queues);\n\tif (err) {\n\t\tnetif_err(priv, drv, priv->dev, \"failed to create %d tx queues\\n\",\n\t\t\t  num_tx_queues);\n\t\t \n\t\treturn err;\n\t}\n\tnetif_dbg(priv, drv, priv->dev, \"created %d tx queues\\n\",\n\t\t  num_tx_queues);\n\n\terr = gve_adminq_create_rx_queues(priv, priv->rx_cfg.num_queues);\n\tif (err) {\n\t\tnetif_err(priv, drv, priv->dev, \"failed to create %d rx queues\\n\",\n\t\t\t  priv->rx_cfg.num_queues);\n\t\t \n\t\treturn err;\n\t}\n\tnetif_dbg(priv, drv, priv->dev, \"created %d rx queues\\n\",\n\t\t  priv->rx_cfg.num_queues);\n\n\tif (gve_is_gqi(priv)) {\n\t\t \n\t\tfor (i = 0; i < priv->rx_cfg.num_queues; i++)\n\t\t\tgve_rx_write_doorbell(priv, &priv->rx[i]);\n\t} else {\n\t\tfor (i = 0; i < priv->rx_cfg.num_queues; i++) {\n\t\t\t \n\t\t\tgve_rx_post_buffers_dqo(&priv->rx[i]);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void add_napi_init_xdp_sync_stats(struct gve_priv *priv,\n\t\t\t\t\t int (*napi_poll)(struct napi_struct *napi,\n\t\t\t\t\t\t\t  int budget))\n{\n\tint start_id = gve_xdp_tx_start_queue_id(priv);\n\tint i;\n\n\t \n\tfor (i = start_id; i < start_id + priv->num_xdp_queues; i++) {\n\t\tint ntfy_idx = gve_tx_idx_to_ntfy(priv, i);\n\n\t\tu64_stats_init(&priv->tx[i].statss);\n\t\tpriv->tx[i].ntfy_id = ntfy_idx;\n\t\tgve_add_napi(priv, ntfy_idx, napi_poll);\n\t}\n}\n\nstatic void add_napi_init_sync_stats(struct gve_priv *priv,\n\t\t\t\t     int (*napi_poll)(struct napi_struct *napi,\n\t\t\t\t\t\t      int budget))\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < gve_num_tx_queues(priv); i++) {\n\t\tint ntfy_idx = gve_tx_idx_to_ntfy(priv, i);\n\n\t\tu64_stats_init(&priv->tx[i].statss);\n\t\tpriv->tx[i].ntfy_id = ntfy_idx;\n\t\tgve_add_napi(priv, ntfy_idx, napi_poll);\n\t}\n\t \n\tfor (i = 0; i < priv->rx_cfg.num_queues; i++) {\n\t\tint ntfy_idx = gve_rx_idx_to_ntfy(priv, i);\n\n\t\tu64_stats_init(&priv->rx[i].statss);\n\t\tpriv->rx[i].ntfy_id = ntfy_idx;\n\t\tgve_add_napi(priv, ntfy_idx, napi_poll);\n\t}\n}\n\nstatic void gve_tx_free_rings(struct gve_priv *priv, int start_id, int num_rings)\n{\n\tif (gve_is_gqi(priv)) {\n\t\tgve_tx_free_rings_gqi(priv, start_id, num_rings);\n\t} else {\n\t\tgve_tx_free_rings_dqo(priv);\n\t}\n}\n\nstatic int gve_alloc_xdp_rings(struct gve_priv *priv)\n{\n\tint start_id;\n\tint err = 0;\n\n\tif (!priv->num_xdp_queues)\n\t\treturn 0;\n\n\tstart_id = gve_xdp_tx_start_queue_id(priv);\n\terr = gve_tx_alloc_rings(priv, start_id, priv->num_xdp_queues);\n\tif (err)\n\t\treturn err;\n\tadd_napi_init_xdp_sync_stats(priv, gve_napi_poll);\n\n\treturn 0;\n}\n\nstatic int gve_alloc_rings(struct gve_priv *priv)\n{\n\tint err;\n\n\t \n\tpriv->tx = kvcalloc(priv->tx_cfg.max_queues, sizeof(*priv->tx),\n\t\t\t    GFP_KERNEL);\n\tif (!priv->tx)\n\t\treturn -ENOMEM;\n\n\tif (gve_is_gqi(priv))\n\t\terr = gve_tx_alloc_rings(priv, 0, gve_num_tx_queues(priv));\n\telse\n\t\terr = gve_tx_alloc_rings_dqo(priv);\n\tif (err)\n\t\tgoto free_tx;\n\n\t \n\tpriv->rx = kvcalloc(priv->rx_cfg.max_queues, sizeof(*priv->rx),\n\t\t\t    GFP_KERNEL);\n\tif (!priv->rx) {\n\t\terr = -ENOMEM;\n\t\tgoto free_tx_queue;\n\t}\n\n\tif (gve_is_gqi(priv))\n\t\terr = gve_rx_alloc_rings(priv);\n\telse\n\t\terr = gve_rx_alloc_rings_dqo(priv);\n\tif (err)\n\t\tgoto free_rx;\n\n\tif (gve_is_gqi(priv))\n\t\tadd_napi_init_sync_stats(priv, gve_napi_poll);\n\telse\n\t\tadd_napi_init_sync_stats(priv, gve_napi_poll_dqo);\n\n\treturn 0;\n\nfree_rx:\n\tkvfree(priv->rx);\n\tpriv->rx = NULL;\nfree_tx_queue:\n\tgve_tx_free_rings(priv, 0, gve_num_tx_queues(priv));\nfree_tx:\n\tkvfree(priv->tx);\n\tpriv->tx = NULL;\n\treturn err;\n}\n\nstatic int gve_destroy_xdp_rings(struct gve_priv *priv)\n{\n\tint start_id;\n\tint err;\n\n\tstart_id = gve_xdp_tx_start_queue_id(priv);\n\terr = gve_adminq_destroy_tx_queues(priv,\n\t\t\t\t\t   start_id,\n\t\t\t\t\t   priv->num_xdp_queues);\n\tif (err) {\n\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t  \"failed to destroy XDP queues\\n\");\n\t\t \n\t\treturn err;\n\t}\n\tnetif_dbg(priv, drv, priv->dev, \"destroyed XDP queues\\n\");\n\n\treturn 0;\n}\n\nstatic int gve_destroy_rings(struct gve_priv *priv)\n{\n\tint num_tx_queues = gve_num_tx_queues(priv);\n\tint err;\n\n\terr = gve_adminq_destroy_tx_queues(priv, 0, num_tx_queues);\n\tif (err) {\n\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t  \"failed to destroy tx queues\\n\");\n\t\t \n\t\treturn err;\n\t}\n\tnetif_dbg(priv, drv, priv->dev, \"destroyed tx queues\\n\");\n\terr = gve_adminq_destroy_rx_queues(priv, priv->rx_cfg.num_queues);\n\tif (err) {\n\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t  \"failed to destroy rx queues\\n\");\n\t\t \n\t\treturn err;\n\t}\n\tnetif_dbg(priv, drv, priv->dev, \"destroyed rx queues\\n\");\n\treturn 0;\n}\n\nstatic void gve_rx_free_rings(struct gve_priv *priv)\n{\n\tif (gve_is_gqi(priv))\n\t\tgve_rx_free_rings_gqi(priv);\n\telse\n\t\tgve_rx_free_rings_dqo(priv);\n}\n\nstatic void gve_free_xdp_rings(struct gve_priv *priv)\n{\n\tint ntfy_idx, start_id;\n\tint i;\n\n\tstart_id = gve_xdp_tx_start_queue_id(priv);\n\tif (priv->tx) {\n\t\tfor (i = start_id; i <  start_id + priv->num_xdp_queues; i++) {\n\t\t\tntfy_idx = gve_tx_idx_to_ntfy(priv, i);\n\t\t\tgve_remove_napi(priv, ntfy_idx);\n\t\t}\n\t\tgve_tx_free_rings(priv, start_id, priv->num_xdp_queues);\n\t}\n}\n\nstatic void gve_free_rings(struct gve_priv *priv)\n{\n\tint num_tx_queues = gve_num_tx_queues(priv);\n\tint ntfy_idx;\n\tint i;\n\n\tif (priv->tx) {\n\t\tfor (i = 0; i < num_tx_queues; i++) {\n\t\t\tntfy_idx = gve_tx_idx_to_ntfy(priv, i);\n\t\t\tgve_remove_napi(priv, ntfy_idx);\n\t\t}\n\t\tgve_tx_free_rings(priv, 0, num_tx_queues);\n\t\tkvfree(priv->tx);\n\t\tpriv->tx = NULL;\n\t}\n\tif (priv->rx) {\n\t\tfor (i = 0; i < priv->rx_cfg.num_queues; i++) {\n\t\t\tntfy_idx = gve_rx_idx_to_ntfy(priv, i);\n\t\t\tgve_remove_napi(priv, ntfy_idx);\n\t\t}\n\t\tgve_rx_free_rings(priv);\n\t\tkvfree(priv->rx);\n\t\tpriv->rx = NULL;\n\t}\n}\n\nint gve_alloc_page(struct gve_priv *priv, struct device *dev,\n\t\t   struct page **page, dma_addr_t *dma,\n\t\t   enum dma_data_direction dir, gfp_t gfp_flags)\n{\n\t*page = alloc_page(gfp_flags);\n\tif (!*page) {\n\t\tpriv->page_alloc_fail++;\n\t\treturn -ENOMEM;\n\t}\n\t*dma = dma_map_page(dev, *page, 0, PAGE_SIZE, dir);\n\tif (dma_mapping_error(dev, *dma)) {\n\t\tpriv->dma_mapping_error++;\n\t\tput_page(*page);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic int gve_alloc_queue_page_list(struct gve_priv *priv, u32 id,\n\t\t\t\t     int pages)\n{\n\tstruct gve_queue_page_list *qpl = &priv->qpls[id];\n\tint err;\n\tint i;\n\n\tif (pages + priv->num_registered_pages > priv->max_registered_pages) {\n\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t  \"Reached max number of registered pages %llu > %llu\\n\",\n\t\t\t  pages + priv->num_registered_pages,\n\t\t\t  priv->max_registered_pages);\n\t\treturn -EINVAL;\n\t}\n\n\tqpl->id = id;\n\tqpl->num_entries = 0;\n\tqpl->pages = kvcalloc(pages, sizeof(*qpl->pages), GFP_KERNEL);\n\t \n\tif (!qpl->pages)\n\t\treturn -ENOMEM;\n\tqpl->page_buses = kvcalloc(pages, sizeof(*qpl->page_buses), GFP_KERNEL);\n\t \n\tif (!qpl->page_buses)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < pages; i++) {\n\t\terr = gve_alloc_page(priv, &priv->pdev->dev, &qpl->pages[i],\n\t\t\t\t     &qpl->page_buses[i],\n\t\t\t\t     gve_qpl_dma_dir(priv, id), GFP_KERNEL);\n\t\t \n\t\tif (err)\n\t\t\treturn -ENOMEM;\n\t\tqpl->num_entries++;\n\t}\n\tpriv->num_registered_pages += pages;\n\n\treturn 0;\n}\n\nvoid gve_free_page(struct device *dev, struct page *page, dma_addr_t dma,\n\t\t   enum dma_data_direction dir)\n{\n\tif (!dma_mapping_error(dev, dma))\n\t\tdma_unmap_page(dev, dma, PAGE_SIZE, dir);\n\tif (page)\n\t\tput_page(page);\n}\n\nstatic void gve_free_queue_page_list(struct gve_priv *priv, u32 id)\n{\n\tstruct gve_queue_page_list *qpl = &priv->qpls[id];\n\tint i;\n\n\tif (!qpl->pages)\n\t\treturn;\n\tif (!qpl->page_buses)\n\t\tgoto free_pages;\n\n\tfor (i = 0; i < qpl->num_entries; i++)\n\t\tgve_free_page(&priv->pdev->dev, qpl->pages[i],\n\t\t\t      qpl->page_buses[i], gve_qpl_dma_dir(priv, id));\n\n\tkvfree(qpl->page_buses);\n\tqpl->page_buses = NULL;\nfree_pages:\n\tkvfree(qpl->pages);\n\tqpl->pages = NULL;\n\tpriv->num_registered_pages -= qpl->num_entries;\n}\n\nstatic int gve_alloc_xdp_qpls(struct gve_priv *priv)\n{\n\tint start_id;\n\tint i, j;\n\tint err;\n\n\tstart_id = gve_tx_qpl_id(priv, gve_xdp_tx_start_queue_id(priv));\n\tfor (i = start_id; i < start_id + gve_num_xdp_qpls(priv); i++) {\n\t\terr = gve_alloc_queue_page_list(priv, i,\n\t\t\t\t\t\tpriv->tx_pages_per_qpl);\n\t\tif (err)\n\t\t\tgoto free_qpls;\n\t}\n\n\treturn 0;\n\nfree_qpls:\n\tfor (j = start_id; j <= i; j++)\n\t\tgve_free_queue_page_list(priv, j);\n\treturn err;\n}\n\nstatic int gve_alloc_qpls(struct gve_priv *priv)\n{\n\tint max_queues = priv->tx_cfg.max_queues + priv->rx_cfg.max_queues;\n\tint page_count;\n\tint start_id;\n\tint i, j;\n\tint err;\n\n\tif (!gve_is_qpl(priv))\n\t\treturn 0;\n\n\tpriv->qpls = kvcalloc(max_queues, sizeof(*priv->qpls), GFP_KERNEL);\n\tif (!priv->qpls)\n\t\treturn -ENOMEM;\n\n\tstart_id = gve_tx_start_qpl_id(priv);\n\tpage_count = priv->tx_pages_per_qpl;\n\tfor (i = start_id; i < start_id + gve_num_tx_qpls(priv); i++) {\n\t\terr = gve_alloc_queue_page_list(priv, i,\n\t\t\t\t\t\tpage_count);\n\t\tif (err)\n\t\t\tgoto free_qpls;\n\t}\n\n\tstart_id = gve_rx_start_qpl_id(priv);\n\n\t \n\tpage_count = priv->queue_format == GVE_GQI_QPL_FORMAT ?\n\t\tpriv->rx_data_slot_cnt : priv->rx_pages_per_qpl;\n\tfor (i = start_id; i < start_id + gve_num_rx_qpls(priv); i++) {\n\t\terr = gve_alloc_queue_page_list(priv, i,\n\t\t\t\t\t\tpage_count);\n\t\tif (err)\n\t\t\tgoto free_qpls;\n\t}\n\n\tpriv->qpl_cfg.qpl_map_size = BITS_TO_LONGS(max_queues) *\n\t\t\t\t     sizeof(unsigned long) * BITS_PER_BYTE;\n\tpriv->qpl_cfg.qpl_id_map = kvcalloc(BITS_TO_LONGS(max_queues),\n\t\t\t\t\t    sizeof(unsigned long), GFP_KERNEL);\n\tif (!priv->qpl_cfg.qpl_id_map) {\n\t\terr = -ENOMEM;\n\t\tgoto free_qpls;\n\t}\n\n\treturn 0;\n\nfree_qpls:\n\tfor (j = 0; j <= i; j++)\n\t\tgve_free_queue_page_list(priv, j);\n\tkvfree(priv->qpls);\n\tpriv->qpls = NULL;\n\treturn err;\n}\n\nstatic void gve_free_xdp_qpls(struct gve_priv *priv)\n{\n\tint start_id;\n\tint i;\n\n\tstart_id = gve_tx_qpl_id(priv, gve_xdp_tx_start_queue_id(priv));\n\tfor (i = start_id; i < start_id + gve_num_xdp_qpls(priv); i++)\n\t\tgve_free_queue_page_list(priv, i);\n}\n\nstatic void gve_free_qpls(struct gve_priv *priv)\n{\n\tint max_queues = priv->tx_cfg.max_queues + priv->rx_cfg.max_queues;\n\tint i;\n\n\tif (!priv->qpls)\n\t\treturn;\n\n\tkvfree(priv->qpl_cfg.qpl_id_map);\n\tpriv->qpl_cfg.qpl_id_map = NULL;\n\n\tfor (i = 0; i < max_queues; i++)\n\t\tgve_free_queue_page_list(priv, i);\n\n\tkvfree(priv->qpls);\n\tpriv->qpls = NULL;\n}\n\n \nvoid gve_schedule_reset(struct gve_priv *priv)\n{\n\tgve_set_do_reset(priv);\n\tqueue_work(priv->gve_wq, &priv->service_task);\n}\n\nstatic void gve_reset_and_teardown(struct gve_priv *priv, bool was_up);\nstatic int gve_reset_recovery(struct gve_priv *priv, bool was_up);\nstatic void gve_turndown(struct gve_priv *priv);\nstatic void gve_turnup(struct gve_priv *priv);\n\nstatic int gve_reg_xdp_info(struct gve_priv *priv, struct net_device *dev)\n{\n\tstruct napi_struct *napi;\n\tstruct gve_rx_ring *rx;\n\tint err = 0;\n\tint i, j;\n\tu32 tx_qid;\n\n\tif (!priv->num_xdp_queues)\n\t\treturn 0;\n\n\tfor (i = 0; i < priv->rx_cfg.num_queues; i++) {\n\t\trx = &priv->rx[i];\n\t\tnapi = &priv->ntfy_blocks[rx->ntfy_id].napi;\n\n\t\terr = xdp_rxq_info_reg(&rx->xdp_rxq, dev, i,\n\t\t\t\t       napi->napi_id);\n\t\tif (err)\n\t\t\tgoto err;\n\t\terr = xdp_rxq_info_reg_mem_model(&rx->xdp_rxq,\n\t\t\t\t\t\t MEM_TYPE_PAGE_SHARED, NULL);\n\t\tif (err)\n\t\t\tgoto err;\n\t\trx->xsk_pool = xsk_get_pool_from_qid(dev, i);\n\t\tif (rx->xsk_pool) {\n\t\t\terr = xdp_rxq_info_reg(&rx->xsk_rxq, dev, i,\n\t\t\t\t\t       napi->napi_id);\n\t\t\tif (err)\n\t\t\t\tgoto err;\n\t\t\terr = xdp_rxq_info_reg_mem_model(&rx->xsk_rxq,\n\t\t\t\t\t\t\t MEM_TYPE_XSK_BUFF_POOL, NULL);\n\t\t\tif (err)\n\t\t\t\tgoto err;\n\t\t\txsk_pool_set_rxq_info(rx->xsk_pool,\n\t\t\t\t\t      &rx->xsk_rxq);\n\t\t}\n\t}\n\n\tfor (i = 0; i < priv->num_xdp_queues; i++) {\n\t\ttx_qid = gve_xdp_tx_queue_id(priv, i);\n\t\tpriv->tx[tx_qid].xsk_pool = xsk_get_pool_from_qid(dev, i);\n\t}\n\treturn 0;\n\nerr:\n\tfor (j = i; j >= 0; j--) {\n\t\trx = &priv->rx[j];\n\t\tif (xdp_rxq_info_is_reg(&rx->xdp_rxq))\n\t\t\txdp_rxq_info_unreg(&rx->xdp_rxq);\n\t\tif (xdp_rxq_info_is_reg(&rx->xsk_rxq))\n\t\t\txdp_rxq_info_unreg(&rx->xsk_rxq);\n\t}\n\treturn err;\n}\n\nstatic void gve_unreg_xdp_info(struct gve_priv *priv)\n{\n\tint i, tx_qid;\n\n\tif (!priv->num_xdp_queues)\n\t\treturn;\n\n\tfor (i = 0; i < priv->rx_cfg.num_queues; i++) {\n\t\tstruct gve_rx_ring *rx = &priv->rx[i];\n\n\t\txdp_rxq_info_unreg(&rx->xdp_rxq);\n\t\tif (rx->xsk_pool) {\n\t\t\txdp_rxq_info_unreg(&rx->xsk_rxq);\n\t\t\trx->xsk_pool = NULL;\n\t\t}\n\t}\n\n\tfor (i = 0; i < priv->num_xdp_queues; i++) {\n\t\ttx_qid = gve_xdp_tx_queue_id(priv, i);\n\t\tpriv->tx[tx_qid].xsk_pool = NULL;\n\t}\n}\n\nstatic void gve_drain_page_cache(struct gve_priv *priv)\n{\n\tstruct page_frag_cache *nc;\n\tint i;\n\n\tfor (i = 0; i < priv->rx_cfg.num_queues; i++) {\n\t\tnc = &priv->rx[i].page_cache;\n\t\tif (nc->va) {\n\t\t\t__page_frag_cache_drain(virt_to_page(nc->va),\n\t\t\t\t\t\tnc->pagecnt_bias);\n\t\t\tnc->va = NULL;\n\t\t}\n\t}\n}\n\nstatic int gve_open(struct net_device *dev)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tint err;\n\n\tif (priv->xdp_prog)\n\t\tpriv->num_xdp_queues = priv->rx_cfg.num_queues;\n\telse\n\t\tpriv->num_xdp_queues = 0;\n\n\terr = gve_alloc_qpls(priv);\n\tif (err)\n\t\treturn err;\n\n\terr = gve_alloc_rings(priv);\n\tif (err)\n\t\tgoto free_qpls;\n\n\terr = netif_set_real_num_tx_queues(dev, priv->tx_cfg.num_queues);\n\tif (err)\n\t\tgoto free_rings;\n\terr = netif_set_real_num_rx_queues(dev, priv->rx_cfg.num_queues);\n\tif (err)\n\t\tgoto free_rings;\n\n\terr = gve_reg_xdp_info(priv, dev);\n\tif (err)\n\t\tgoto free_rings;\n\n\terr = gve_register_qpls(priv);\n\tif (err)\n\t\tgoto reset;\n\n\tif (!gve_is_gqi(priv)) {\n\t\t \n\t\tpriv->data_buffer_size_dqo = GVE_RX_BUFFER_SIZE_DQO;\n\t}\n\terr = gve_create_rings(priv);\n\tif (err)\n\t\tgoto reset;\n\n\tgve_set_device_rings_ok(priv);\n\n\tif (gve_get_report_stats(priv))\n\t\tmod_timer(&priv->stats_report_timer,\n\t\t\t  round_jiffies(jiffies +\n\t\t\t\tmsecs_to_jiffies(priv->stats_report_timer_period)));\n\n\tgve_turnup(priv);\n\tqueue_work(priv->gve_wq, &priv->service_task);\n\tpriv->interface_up_cnt++;\n\treturn 0;\n\nfree_rings:\n\tgve_free_rings(priv);\nfree_qpls:\n\tgve_free_qpls(priv);\n\treturn err;\n\nreset:\n\t \n\tif (gve_get_reset_in_progress(priv))\n\t\treturn err;\n\t \n\tgve_reset_and_teardown(priv, true);\n\t \n\tgve_reset_recovery(priv, false);\n\t \n\treturn err;\n}\n\nstatic int gve_close(struct net_device *dev)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tint err;\n\n\tnetif_carrier_off(dev);\n\tif (gve_get_device_rings_ok(priv)) {\n\t\tgve_turndown(priv);\n\t\tgve_drain_page_cache(priv);\n\t\terr = gve_destroy_rings(priv);\n\t\tif (err)\n\t\t\tgoto err;\n\t\terr = gve_unregister_qpls(priv);\n\t\tif (err)\n\t\t\tgoto err;\n\t\tgve_clear_device_rings_ok(priv);\n\t}\n\tdel_timer_sync(&priv->stats_report_timer);\n\n\tgve_unreg_xdp_info(priv);\n\tgve_free_rings(priv);\n\tgve_free_qpls(priv);\n\tpriv->interface_down_cnt++;\n\treturn 0;\n\nerr:\n\t \n\tif (gve_get_reset_in_progress(priv))\n\t\treturn err;\n\t \n\tgve_reset_and_teardown(priv, true);\n\treturn gve_reset_recovery(priv, false);\n}\n\nstatic int gve_remove_xdp_queues(struct gve_priv *priv)\n{\n\tint err;\n\n\terr = gve_destroy_xdp_rings(priv);\n\tif (err)\n\t\treturn err;\n\n\terr = gve_unregister_xdp_qpls(priv);\n\tif (err)\n\t\treturn err;\n\n\tgve_unreg_xdp_info(priv);\n\tgve_free_xdp_rings(priv);\n\tgve_free_xdp_qpls(priv);\n\tpriv->num_xdp_queues = 0;\n\treturn 0;\n}\n\nstatic int gve_add_xdp_queues(struct gve_priv *priv)\n{\n\tint err;\n\n\tpriv->num_xdp_queues = priv->tx_cfg.num_queues;\n\n\terr = gve_alloc_xdp_qpls(priv);\n\tif (err)\n\t\tgoto err;\n\n\terr = gve_alloc_xdp_rings(priv);\n\tif (err)\n\t\tgoto free_xdp_qpls;\n\n\terr = gve_reg_xdp_info(priv, priv->dev);\n\tif (err)\n\t\tgoto free_xdp_rings;\n\n\terr = gve_register_xdp_qpls(priv);\n\tif (err)\n\t\tgoto free_xdp_rings;\n\n\terr = gve_create_xdp_rings(priv);\n\tif (err)\n\t\tgoto free_xdp_rings;\n\n\treturn 0;\n\nfree_xdp_rings:\n\tgve_free_xdp_rings(priv);\nfree_xdp_qpls:\n\tgve_free_xdp_qpls(priv);\nerr:\n\tpriv->num_xdp_queues = 0;\n\treturn err;\n}\n\nstatic void gve_handle_link_status(struct gve_priv *priv, bool link_status)\n{\n\tif (!gve_get_napi_enabled(priv))\n\t\treturn;\n\n\tif (link_status == netif_carrier_ok(priv->dev))\n\t\treturn;\n\n\tif (link_status) {\n\t\tnetdev_info(priv->dev, \"Device link is up.\\n\");\n\t\tnetif_carrier_on(priv->dev);\n\t} else {\n\t\tnetdev_info(priv->dev, \"Device link is down.\\n\");\n\t\tnetif_carrier_off(priv->dev);\n\t}\n}\n\nstatic int gve_set_xdp(struct gve_priv *priv, struct bpf_prog *prog,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct bpf_prog *old_prog;\n\tint err = 0;\n\tu32 status;\n\n\told_prog = READ_ONCE(priv->xdp_prog);\n\tif (!netif_carrier_ok(priv->dev)) {\n\t\tWRITE_ONCE(priv->xdp_prog, prog);\n\t\tif (old_prog)\n\t\t\tbpf_prog_put(old_prog);\n\t\treturn 0;\n\t}\n\n\tgve_turndown(priv);\n\tif (!old_prog && prog) {\n\t\t\n\t\t\n\t\terr = gve_add_xdp_queues(priv);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else if (old_prog && !prog) {\n\t\t\n\t\t\n\t\terr = gve_remove_xdp_queues(priv);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\tWRITE_ONCE(priv->xdp_prog, prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\nout:\n\tgve_turnup(priv);\n\tstatus = ioread32be(&priv->reg_bar0->device_status);\n\tgve_handle_link_status(priv, GVE_DEVICE_STATUS_LINK_STATUS_MASK & status);\n\treturn err;\n}\n\nstatic int gve_xsk_pool_enable(struct net_device *dev,\n\t\t\t       struct xsk_buff_pool *pool,\n\t\t\t       u16 qid)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tstruct napi_struct *napi;\n\tstruct gve_rx_ring *rx;\n\tint tx_qid;\n\tint err;\n\n\tif (qid >= priv->rx_cfg.num_queues) {\n\t\tdev_err(&priv->pdev->dev, \"xsk pool invalid qid %d\", qid);\n\t\treturn -EINVAL;\n\t}\n\tif (xsk_pool_get_rx_frame_size(pool) <\n\t     priv->dev->max_mtu + sizeof(struct ethhdr)) {\n\t\tdev_err(&priv->pdev->dev, \"xsk pool frame_len too small\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = xsk_pool_dma_map(pool, &priv->pdev->dev,\n\t\t\t       DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (!priv->xdp_prog)\n\t\treturn 0;\n\n\trx = &priv->rx[qid];\n\tnapi = &priv->ntfy_blocks[rx->ntfy_id].napi;\n\terr = xdp_rxq_info_reg(&rx->xsk_rxq, dev, qid, napi->napi_id);\n\tif (err)\n\t\tgoto err;\n\n\terr = xdp_rxq_info_reg_mem_model(&rx->xsk_rxq,\n\t\t\t\t\t MEM_TYPE_XSK_BUFF_POOL, NULL);\n\tif (err)\n\t\tgoto err;\n\n\txsk_pool_set_rxq_info(pool, &rx->xsk_rxq);\n\trx->xsk_pool = pool;\n\n\ttx_qid = gve_xdp_tx_queue_id(priv, qid);\n\tpriv->tx[tx_qid].xsk_pool = pool;\n\n\treturn 0;\nerr:\n\tif (xdp_rxq_info_is_reg(&rx->xsk_rxq))\n\t\txdp_rxq_info_unreg(&rx->xsk_rxq);\n\n\txsk_pool_dma_unmap(pool,\n\t\t\t   DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING);\n\treturn err;\n}\n\nstatic int gve_xsk_pool_disable(struct net_device *dev,\n\t\t\t\tu16 qid)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tstruct napi_struct *napi_rx;\n\tstruct napi_struct *napi_tx;\n\tstruct xsk_buff_pool *pool;\n\tint tx_qid;\n\n\tpool = xsk_get_pool_from_qid(dev, qid);\n\tif (!pool)\n\t\treturn -EINVAL;\n\tif (qid >= priv->rx_cfg.num_queues)\n\t\treturn -EINVAL;\n\n\t \n\tif (!priv->xdp_prog)\n\t\tgoto done;\n\n\ttx_qid = gve_xdp_tx_queue_id(priv, qid);\n\tif (!netif_running(dev)) {\n\t\tpriv->rx[qid].xsk_pool = NULL;\n\t\txdp_rxq_info_unreg(&priv->rx[qid].xsk_rxq);\n\t\tpriv->tx[tx_qid].xsk_pool = NULL;\n\t\tgoto done;\n\t}\n\n\tnapi_rx = &priv->ntfy_blocks[priv->rx[qid].ntfy_id].napi;\n\tnapi_disable(napi_rx);  \n\n\tnapi_tx = &priv->ntfy_blocks[priv->tx[tx_qid].ntfy_id].napi;\n\tnapi_disable(napi_tx);  \n\n\tpriv->rx[qid].xsk_pool = NULL;\n\txdp_rxq_info_unreg(&priv->rx[qid].xsk_rxq);\n\tpriv->tx[tx_qid].xsk_pool = NULL;\n\tsmp_mb();  \n\n\tnapi_enable(napi_rx);\n\tif (gve_rx_work_pending(&priv->rx[qid]))\n\t\tnapi_schedule(napi_rx);\n\n\tnapi_enable(napi_tx);\n\tif (gve_tx_clean_pending(priv, &priv->tx[tx_qid]))\n\t\tnapi_schedule(napi_tx);\n\ndone:\n\txsk_pool_dma_unmap(pool,\n\t\t\t   DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING);\n\treturn 0;\n}\n\nstatic int gve_xsk_wakeup(struct net_device *dev, u32 queue_id, u32 flags)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tint tx_queue_id = gve_xdp_tx_queue_id(priv, queue_id);\n\n\tif (queue_id >= priv->rx_cfg.num_queues || !priv->xdp_prog)\n\t\treturn -EINVAL;\n\n\tif (flags & XDP_WAKEUP_TX) {\n\t\tstruct gve_tx_ring *tx = &priv->tx[tx_queue_id];\n\t\tstruct napi_struct *napi =\n\t\t\t&priv->ntfy_blocks[tx->ntfy_id].napi;\n\n\t\tif (!napi_if_scheduled_mark_missed(napi)) {\n\t\t\t \n\t\t\tlocal_bh_disable();\n\t\t\tnapi_schedule(napi);\n\t\t\tlocal_bh_enable();\n\t\t}\n\n\t\ttx->xdp_xsk_wakeup++;\n\t}\n\n\treturn 0;\n}\n\nstatic int verify_xdp_configuration(struct net_device *dev)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\n\tif (dev->features & NETIF_F_LRO) {\n\t\tnetdev_warn(dev, \"XDP is not supported when LRO is on.\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (priv->queue_format != GVE_GQI_QPL_FORMAT) {\n\t\tnetdev_warn(dev, \"XDP is not supported in mode %d.\\n\",\n\t\t\t    priv->queue_format);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (dev->mtu > (PAGE_SIZE / 2) - sizeof(struct ethhdr) - GVE_RX_PAD) {\n\t\tnetdev_warn(dev, \"XDP is not supported for mtu %d.\\n\",\n\t\t\t    dev->mtu);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (priv->rx_cfg.num_queues != priv->tx_cfg.num_queues ||\n\t    (2 * priv->tx_cfg.num_queues > priv->tx_cfg.max_queues)) {\n\t\tnetdev_warn(dev, \"XDP load failed: The number of configured RX queues %d should be equal to the number of configured TX queues %d and the number of configured RX/TX queues should be less than or equal to half the maximum number of RX/TX queues %d\",\n\t\t\t    priv->rx_cfg.num_queues,\n\t\t\t    priv->tx_cfg.num_queues,\n\t\t\t    priv->tx_cfg.max_queues);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int gve_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tint err;\n\n\terr = verify_xdp_configuration(dev);\n\tif (err)\n\t\treturn err;\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn gve_set_xdp(priv, xdp->prog, xdp->extack);\n\tcase XDP_SETUP_XSK_POOL:\n\t\tif (xdp->xsk.pool)\n\t\t\treturn gve_xsk_pool_enable(dev, xdp->xsk.pool, xdp->xsk.queue_id);\n\t\telse\n\t\t\treturn gve_xsk_pool_disable(dev, xdp->xsk.queue_id);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nint gve_adjust_queues(struct gve_priv *priv,\n\t\t      struct gve_queue_config new_rx_config,\n\t\t      struct gve_queue_config new_tx_config)\n{\n\tint err;\n\n\tif (netif_carrier_ok(priv->dev)) {\n\t\t \n\t\terr = gve_close(priv->dev);\n\t\t \n\t\tif (err)\n\t\t\treturn err;\n\t\tpriv->tx_cfg = new_tx_config;\n\t\tpriv->rx_cfg = new_rx_config;\n\n\t\terr = gve_open(priv->dev);\n\t\tif (err)\n\t\t\tgoto err;\n\n\t\treturn 0;\n\t}\n\t \n\tpriv->tx_cfg = new_tx_config;\n\tpriv->rx_cfg = new_rx_config;\n\n\treturn 0;\nerr:\n\tnetif_err(priv, drv, priv->dev,\n\t\t  \"Adjust queues failed! !!! DISABLING ALL QUEUES !!!\\n\");\n\tgve_turndown(priv);\n\treturn err;\n}\n\nstatic void gve_turndown(struct gve_priv *priv)\n{\n\tint idx;\n\n\tif (netif_carrier_ok(priv->dev))\n\t\tnetif_carrier_off(priv->dev);\n\n\tif (!gve_get_napi_enabled(priv))\n\t\treturn;\n\n\t \n\tfor (idx = 0; idx < gve_num_tx_queues(priv); idx++) {\n\t\tint ntfy_idx = gve_tx_idx_to_ntfy(priv, idx);\n\t\tstruct gve_notify_block *block = &priv->ntfy_blocks[ntfy_idx];\n\n\t\tnapi_disable(&block->napi);\n\t}\n\tfor (idx = 0; idx < priv->rx_cfg.num_queues; idx++) {\n\t\tint ntfy_idx = gve_rx_idx_to_ntfy(priv, idx);\n\t\tstruct gve_notify_block *block = &priv->ntfy_blocks[ntfy_idx];\n\n\t\tnapi_disable(&block->napi);\n\t}\n\n\t \n\tnetif_tx_disable(priv->dev);\n\n\tgve_clear_napi_enabled(priv);\n\tgve_clear_report_stats(priv);\n}\n\nstatic void gve_turnup(struct gve_priv *priv)\n{\n\tint idx;\n\n\t \n\tnetif_tx_start_all_queues(priv->dev);\n\n\t \n\tfor (idx = 0; idx < gve_num_tx_queues(priv); idx++) {\n\t\tint ntfy_idx = gve_tx_idx_to_ntfy(priv, idx);\n\t\tstruct gve_notify_block *block = &priv->ntfy_blocks[ntfy_idx];\n\n\t\tnapi_enable(&block->napi);\n\t\tif (gve_is_gqi(priv)) {\n\t\t\tiowrite32be(0, gve_irq_doorbell(priv, block));\n\t\t} else {\n\t\t\tgve_set_itr_coalesce_usecs_dqo(priv, block,\n\t\t\t\t\t\t       priv->tx_coalesce_usecs);\n\t\t}\n\t}\n\tfor (idx = 0; idx < priv->rx_cfg.num_queues; idx++) {\n\t\tint ntfy_idx = gve_rx_idx_to_ntfy(priv, idx);\n\t\tstruct gve_notify_block *block = &priv->ntfy_blocks[ntfy_idx];\n\n\t\tnapi_enable(&block->napi);\n\t\tif (gve_is_gqi(priv)) {\n\t\t\tiowrite32be(0, gve_irq_doorbell(priv, block));\n\t\t} else {\n\t\t\tgve_set_itr_coalesce_usecs_dqo(priv, block,\n\t\t\t\t\t\t       priv->rx_coalesce_usecs);\n\t\t}\n\t}\n\n\tgve_set_napi_enabled(priv);\n}\n\nstatic void gve_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct gve_notify_block *block;\n\tstruct gve_tx_ring *tx = NULL;\n\tstruct gve_priv *priv;\n\tu32 last_nic_done;\n\tu32 current_time;\n\tu32 ntfy_idx;\n\n\tnetdev_info(dev, \"Timeout on tx queue, %d\", txqueue);\n\tpriv = netdev_priv(dev);\n\tif (txqueue > priv->tx_cfg.num_queues)\n\t\tgoto reset;\n\n\tntfy_idx = gve_tx_idx_to_ntfy(priv, txqueue);\n\tif (ntfy_idx >= priv->num_ntfy_blks)\n\t\tgoto reset;\n\n\tblock = &priv->ntfy_blocks[ntfy_idx];\n\ttx = block->tx;\n\n\tcurrent_time = jiffies_to_msecs(jiffies);\n\tif (tx->last_kick_msec + MIN_TX_TIMEOUT_GAP > current_time)\n\t\tgoto reset;\n\n\t \n\tlast_nic_done = gve_tx_load_event_counter(priv, tx);\n\tif (last_nic_done - tx->done) {\n\t\tnetdev_info(dev, \"Kicking queue %d\", txqueue);\n\t\tiowrite32be(GVE_IRQ_MASK, gve_irq_doorbell(priv, block));\n\t\tnapi_schedule(&block->napi);\n\t\ttx->last_kick_msec = current_time;\n\t\tgoto out;\n\t} \n\nreset:\n\tgve_schedule_reset(priv);\n\nout:\n\tif (tx)\n\t\ttx->queue_timeout++;\n\tpriv->tx_timeo_cnt++;\n}\n\nstatic int gve_set_features(struct net_device *netdev,\n\t\t\t    netdev_features_t features)\n{\n\tconst netdev_features_t orig_features = netdev->features;\n\tstruct gve_priv *priv = netdev_priv(netdev);\n\tint err;\n\n\tif ((netdev->features & NETIF_F_LRO) != (features & NETIF_F_LRO)) {\n\t\tnetdev->features ^= NETIF_F_LRO;\n\t\tif (netif_carrier_ok(netdev)) {\n\t\t\t \n\t\t\terr = gve_close(netdev);\n\t\t\t \n\t\t\tif (err)\n\t\t\t\tgoto err;\n\n\t\t\terr = gve_open(netdev);\n\t\t\tif (err)\n\t\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn 0;\nerr:\n\t \n\tnetdev->features = orig_features;\n\tnetif_err(priv, drv, netdev,\n\t\t  \"Set features failed! !!! DISABLING ALL QUEUES !!!\\n\");\n\treturn err;\n}\n\nstatic const struct net_device_ops gve_netdev_ops = {\n\t.ndo_start_xmit\t\t=\tgve_start_xmit,\n\t.ndo_open\t\t=\tgve_open,\n\t.ndo_stop\t\t=\tgve_close,\n\t.ndo_get_stats64\t=\tgve_get_stats,\n\t.ndo_tx_timeout         =       gve_tx_timeout,\n\t.ndo_set_features\t=\tgve_set_features,\n\t.ndo_bpf\t\t=\tgve_xdp,\n\t.ndo_xdp_xmit\t\t=\tgve_xdp_xmit,\n\t.ndo_xsk_wakeup\t\t=\tgve_xsk_wakeup,\n};\n\nstatic void gve_handle_status(struct gve_priv *priv, u32 status)\n{\n\tif (GVE_DEVICE_STATUS_RESET_MASK & status) {\n\t\tdev_info(&priv->pdev->dev, \"Device requested reset.\\n\");\n\t\tgve_set_do_reset(priv);\n\t}\n\tif (GVE_DEVICE_STATUS_REPORT_STATS_MASK & status) {\n\t\tpriv->stats_report_trigger_cnt++;\n\t\tgve_set_do_report_stats(priv);\n\t}\n}\n\nstatic void gve_handle_reset(struct gve_priv *priv)\n{\n\t \n\tif (gve_get_probe_in_progress(priv))\n\t\treturn;\n\n\tif (gve_get_do_reset(priv)) {\n\t\trtnl_lock();\n\t\tgve_reset(priv, false);\n\t\trtnl_unlock();\n\t}\n}\n\nvoid gve_handle_report_stats(struct gve_priv *priv)\n{\n\tstruct stats *stats = priv->stats_report->stats;\n\tint idx, stats_idx = 0;\n\tunsigned int start = 0;\n\tu64 tx_bytes;\n\n\tif (!gve_get_report_stats(priv))\n\t\treturn;\n\n\tbe64_add_cpu(&priv->stats_report->written_count, 1);\n\t \n\tif (priv->tx) {\n\t\tfor (idx = 0; idx < gve_num_tx_queues(priv); idx++) {\n\t\t\tu32 last_completion = 0;\n\t\t\tu32 tx_frames = 0;\n\n\t\t\t \n\t\t\tif (gve_is_gqi(priv)) {\n\t\t\t\tlast_completion = priv->tx[idx].done;\n\t\t\t\ttx_frames = priv->tx[idx].req;\n\t\t\t}\n\n\t\t\tdo {\n\t\t\t\tstart = u64_stats_fetch_begin(&priv->tx[idx].statss);\n\t\t\t\ttx_bytes = priv->tx[idx].bytes_done;\n\t\t\t} while (u64_stats_fetch_retry(&priv->tx[idx].statss, start));\n\t\t\tstats[stats_idx++] = (struct stats) {\n\t\t\t\t.stat_name = cpu_to_be32(TX_WAKE_CNT),\n\t\t\t\t.value = cpu_to_be64(priv->tx[idx].wake_queue),\n\t\t\t\t.queue_id = cpu_to_be32(idx),\n\t\t\t};\n\t\t\tstats[stats_idx++] = (struct stats) {\n\t\t\t\t.stat_name = cpu_to_be32(TX_STOP_CNT),\n\t\t\t\t.value = cpu_to_be64(priv->tx[idx].stop_queue),\n\t\t\t\t.queue_id = cpu_to_be32(idx),\n\t\t\t};\n\t\t\tstats[stats_idx++] = (struct stats) {\n\t\t\t\t.stat_name = cpu_to_be32(TX_FRAMES_SENT),\n\t\t\t\t.value = cpu_to_be64(tx_frames),\n\t\t\t\t.queue_id = cpu_to_be32(idx),\n\t\t\t};\n\t\t\tstats[stats_idx++] = (struct stats) {\n\t\t\t\t.stat_name = cpu_to_be32(TX_BYTES_SENT),\n\t\t\t\t.value = cpu_to_be64(tx_bytes),\n\t\t\t\t.queue_id = cpu_to_be32(idx),\n\t\t\t};\n\t\t\tstats[stats_idx++] = (struct stats) {\n\t\t\t\t.stat_name = cpu_to_be32(TX_LAST_COMPLETION_PROCESSED),\n\t\t\t\t.value = cpu_to_be64(last_completion),\n\t\t\t\t.queue_id = cpu_to_be32(idx),\n\t\t\t};\n\t\t\tstats[stats_idx++] = (struct stats) {\n\t\t\t\t.stat_name = cpu_to_be32(TX_TIMEOUT_CNT),\n\t\t\t\t.value = cpu_to_be64(priv->tx[idx].queue_timeout),\n\t\t\t\t.queue_id = cpu_to_be32(idx),\n\t\t\t};\n\t\t}\n\t}\n\t \n\tif (priv->rx) {\n\t\tfor (idx = 0; idx < priv->rx_cfg.num_queues; idx++) {\n\t\t\tstats[stats_idx++] = (struct stats) {\n\t\t\t\t.stat_name = cpu_to_be32(RX_NEXT_EXPECTED_SEQUENCE),\n\t\t\t\t.value = cpu_to_be64(priv->rx[idx].desc.seqno),\n\t\t\t\t.queue_id = cpu_to_be32(idx),\n\t\t\t};\n\t\t\tstats[stats_idx++] = (struct stats) {\n\t\t\t\t.stat_name = cpu_to_be32(RX_BUFFERS_POSTED),\n\t\t\t\t.value = cpu_to_be64(priv->rx[0].fill_cnt),\n\t\t\t\t.queue_id = cpu_to_be32(idx),\n\t\t\t};\n\t\t}\n\t}\n}\n\n \nstatic void gve_service_task(struct work_struct *work)\n{\n\tstruct gve_priv *priv = container_of(work, struct gve_priv,\n\t\t\t\t\t     service_task);\n\tu32 status = ioread32be(&priv->reg_bar0->device_status);\n\n\tgve_handle_status(priv, status);\n\n\tgve_handle_reset(priv);\n\tgve_handle_link_status(priv, GVE_DEVICE_STATUS_LINK_STATUS_MASK & status);\n}\n\nstatic void gve_set_netdev_xdp_features(struct gve_priv *priv)\n{\n\tif (priv->queue_format == GVE_GQI_QPL_FORMAT) {\n\t\tpriv->dev->xdp_features = NETDEV_XDP_ACT_BASIC;\n\t\tpriv->dev->xdp_features |= NETDEV_XDP_ACT_REDIRECT;\n\t\tpriv->dev->xdp_features |= NETDEV_XDP_ACT_NDO_XMIT;\n\t\tpriv->dev->xdp_features |= NETDEV_XDP_ACT_XSK_ZEROCOPY;\n\t} else {\n\t\tpriv->dev->xdp_features = 0;\n\t}\n}\n\nstatic int gve_init_priv(struct gve_priv *priv, bool skip_describe_device)\n{\n\tint num_ntfy;\n\tint err;\n\n\t \n\terr = gve_adminq_alloc(&priv->pdev->dev, priv);\n\tif (err) {\n\t\tdev_err(&priv->pdev->dev,\n\t\t\t\"Failed to alloc admin queue: err=%d\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = gve_verify_driver_compatibility(priv);\n\tif (err) {\n\t\tdev_err(&priv->pdev->dev,\n\t\t\t\"Could not verify driver compatibility: err=%d\\n\", err);\n\t\tgoto err;\n\t}\n\n\tif (skip_describe_device)\n\t\tgoto setup_device;\n\n\tpriv->queue_format = GVE_QUEUE_FORMAT_UNSPECIFIED;\n\t \n\terr = gve_adminq_describe_device(priv);\n\tif (err) {\n\t\tdev_err(&priv->pdev->dev,\n\t\t\t\"Could not get device information: err=%d\\n\", err);\n\t\tgoto err;\n\t}\n\tpriv->dev->mtu = priv->dev->max_mtu;\n\tnum_ntfy = pci_msix_vec_count(priv->pdev);\n\tif (num_ntfy <= 0) {\n\t\tdev_err(&priv->pdev->dev,\n\t\t\t\"could not count MSI-x vectors: err=%d\\n\", num_ntfy);\n\t\terr = num_ntfy;\n\t\tgoto err;\n\t} else if (num_ntfy < GVE_MIN_MSIX) {\n\t\tdev_err(&priv->pdev->dev, \"gve needs at least %d MSI-x vectors, but only has %d\\n\",\n\t\t\tGVE_MIN_MSIX, num_ntfy);\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t \n\tif (!gve_is_gqi(priv))\n\t\tnetif_set_tso_max_size(priv->dev, GVE_DQO_TX_MAX);\n\n\tpriv->num_registered_pages = 0;\n\tpriv->rx_copybreak = GVE_DEFAULT_RX_COPYBREAK;\n\t \n\tpriv->num_ntfy_blks = (num_ntfy - 1) & ~0x1;\n\tpriv->mgmt_msix_idx = priv->num_ntfy_blks;\n\n\tpriv->tx_cfg.max_queues =\n\t\tmin_t(int, priv->tx_cfg.max_queues, priv->num_ntfy_blks / 2);\n\tpriv->rx_cfg.max_queues =\n\t\tmin_t(int, priv->rx_cfg.max_queues, priv->num_ntfy_blks / 2);\n\n\tpriv->tx_cfg.num_queues = priv->tx_cfg.max_queues;\n\tpriv->rx_cfg.num_queues = priv->rx_cfg.max_queues;\n\tif (priv->default_num_queues > 0) {\n\t\tpriv->tx_cfg.num_queues = min_t(int, priv->default_num_queues,\n\t\t\t\t\t\tpriv->tx_cfg.num_queues);\n\t\tpriv->rx_cfg.num_queues = min_t(int, priv->default_num_queues,\n\t\t\t\t\t\tpriv->rx_cfg.num_queues);\n\t}\n\n\tdev_info(&priv->pdev->dev, \"TX queues %d, RX queues %d\\n\",\n\t\t priv->tx_cfg.num_queues, priv->rx_cfg.num_queues);\n\tdev_info(&priv->pdev->dev, \"Max TX queues %d, Max RX queues %d\\n\",\n\t\t priv->tx_cfg.max_queues, priv->rx_cfg.max_queues);\n\n\tif (!gve_is_gqi(priv)) {\n\t\tpriv->tx_coalesce_usecs = GVE_TX_IRQ_RATELIMIT_US_DQO;\n\t\tpriv->rx_coalesce_usecs = GVE_RX_IRQ_RATELIMIT_US_DQO;\n\t}\n\nsetup_device:\n\tgve_set_netdev_xdp_features(priv);\n\terr = gve_setup_device_resources(priv);\n\tif (!err)\n\t\treturn 0;\nerr:\n\tgve_adminq_free(&priv->pdev->dev, priv);\n\treturn err;\n}\n\nstatic void gve_teardown_priv_resources(struct gve_priv *priv)\n{\n\tgve_teardown_device_resources(priv);\n\tgve_adminq_free(&priv->pdev->dev, priv);\n}\n\nstatic void gve_trigger_reset(struct gve_priv *priv)\n{\n\t \n\tgve_adminq_release(priv);\n}\n\nstatic void gve_reset_and_teardown(struct gve_priv *priv, bool was_up)\n{\n\tgve_trigger_reset(priv);\n\t \n\tif (was_up)\n\t\tgve_close(priv->dev);\n\tgve_teardown_priv_resources(priv);\n}\n\nstatic int gve_reset_recovery(struct gve_priv *priv, bool was_up)\n{\n\tint err;\n\n\terr = gve_init_priv(priv, true);\n\tif (err)\n\t\tgoto err;\n\tif (was_up) {\n\t\terr = gve_open(priv->dev);\n\t\tif (err)\n\t\t\tgoto err;\n\t}\n\treturn 0;\nerr:\n\tdev_err(&priv->pdev->dev, \"Reset failed! !!! DISABLING ALL QUEUES !!!\\n\");\n\tgve_turndown(priv);\n\treturn err;\n}\n\nint gve_reset(struct gve_priv *priv, bool attempt_teardown)\n{\n\tbool was_up = netif_carrier_ok(priv->dev);\n\tint err;\n\n\tdev_info(&priv->pdev->dev, \"Performing reset\\n\");\n\tgve_clear_do_reset(priv);\n\tgve_set_reset_in_progress(priv);\n\t \n\tif (!attempt_teardown) {\n\t\tgve_turndown(priv);\n\t\tgve_reset_and_teardown(priv, was_up);\n\t} else {\n\t\t \n\t\tif (was_up) {\n\t\t\terr = gve_close(priv->dev);\n\t\t\t \n\t\t\tif (err)\n\t\t\t\tgve_reset_and_teardown(priv, was_up);\n\t\t}\n\t\t \n\t\tgve_teardown_priv_resources(priv);\n\t}\n\n\t \n\terr = gve_reset_recovery(priv, was_up);\n\tgve_clear_reset_in_progress(priv);\n\tpriv->reset_cnt++;\n\tpriv->interface_up_cnt = 0;\n\tpriv->interface_down_cnt = 0;\n\tpriv->stats_report_trigger_cnt = 0;\n\treturn err;\n}\n\nstatic void gve_write_version(u8 __iomem *driver_version_register)\n{\n\tconst char *c = gve_version_prefix;\n\n\twhile (*c) {\n\t\twriteb(*c, driver_version_register);\n\t\tc++;\n\t}\n\n\tc = gve_version_str;\n\twhile (*c) {\n\t\twriteb(*c, driver_version_register);\n\t\tc++;\n\t}\n\twriteb('\\n', driver_version_register);\n}\n\nstatic int gve_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tint max_tx_queues, max_rx_queues;\n\tstruct net_device *dev;\n\t__be32 __iomem *db_bar;\n\tstruct gve_registers __iomem *reg_bar;\n\tstruct gve_priv *priv;\n\tint err;\n\n\terr = pci_enable_device(pdev);\n\tif (err)\n\t\treturn err;\n\n\terr = pci_request_regions(pdev, gve_driver_name);\n\tif (err)\n\t\tgoto abort_with_enabled;\n\n\tpci_set_master(pdev);\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to set dma mask: err=%d\\n\", err);\n\t\tgoto abort_with_pci_region;\n\t}\n\n\treg_bar = pci_iomap(pdev, GVE_REGISTER_BAR, 0);\n\tif (!reg_bar) {\n\t\tdev_err(&pdev->dev, \"Failed to map pci bar!\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto abort_with_pci_region;\n\t}\n\n\tdb_bar = pci_iomap(pdev, GVE_DOORBELL_BAR, 0);\n\tif (!db_bar) {\n\t\tdev_err(&pdev->dev, \"Failed to map doorbell bar!\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto abort_with_reg_bar;\n\t}\n\n\tgve_write_version(&reg_bar->driver_version);\n\t \n\tmax_tx_queues = ioread32be(&reg_bar->max_tx_queues);\n\tmax_rx_queues = ioread32be(&reg_bar->max_rx_queues);\n\t \n\tdev = alloc_etherdev_mqs(sizeof(*priv), max_tx_queues, max_rx_queues);\n\tif (!dev) {\n\t\tdev_err(&pdev->dev, \"could not allocate netdev\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto abort_with_db_bar;\n\t}\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\tpci_set_drvdata(pdev, dev);\n\tdev->ethtool_ops = &gve_ethtool_ops;\n\tdev->netdev_ops = &gve_netdev_ops;\n\n\t \n\tdev->hw_features = NETIF_F_HIGHDMA;\n\tdev->hw_features |= NETIF_F_SG;\n\tdev->hw_features |= NETIF_F_HW_CSUM;\n\tdev->hw_features |= NETIF_F_TSO;\n\tdev->hw_features |= NETIF_F_TSO6;\n\tdev->hw_features |= NETIF_F_TSO_ECN;\n\tdev->hw_features |= NETIF_F_RXCSUM;\n\tdev->hw_features |= NETIF_F_RXHASH;\n\tdev->features = dev->hw_features;\n\tdev->watchdog_timeo = 5 * HZ;\n\tdev->min_mtu = ETH_MIN_MTU;\n\tnetif_carrier_off(dev);\n\n\tpriv = netdev_priv(dev);\n\tpriv->dev = dev;\n\tpriv->pdev = pdev;\n\tpriv->msg_enable = DEFAULT_MSG_LEVEL;\n\tpriv->reg_bar0 = reg_bar;\n\tpriv->db_bar2 = db_bar;\n\tpriv->service_task_flags = 0x0;\n\tpriv->state_flags = 0x0;\n\tpriv->ethtool_flags = 0x0;\n\n\tgve_set_probe_in_progress(priv);\n\tpriv->gve_wq = alloc_ordered_workqueue(\"gve\", 0);\n\tif (!priv->gve_wq) {\n\t\tdev_err(&pdev->dev, \"Could not allocate workqueue\");\n\t\terr = -ENOMEM;\n\t\tgoto abort_with_netdev;\n\t}\n\tINIT_WORK(&priv->service_task, gve_service_task);\n\tINIT_WORK(&priv->stats_report_task, gve_stats_report_task);\n\tpriv->tx_cfg.max_queues = max_tx_queues;\n\tpriv->rx_cfg.max_queues = max_rx_queues;\n\n\terr = gve_init_priv(priv, false);\n\tif (err)\n\t\tgoto abort_with_wq;\n\n\terr = register_netdev(dev);\n\tif (err)\n\t\tgoto abort_with_gve_init;\n\n\tdev_info(&pdev->dev, \"GVE version %s\\n\", gve_version_str);\n\tdev_info(&pdev->dev, \"GVE queue format %d\\n\", (int)priv->queue_format);\n\tgve_clear_probe_in_progress(priv);\n\tqueue_work(priv->gve_wq, &priv->service_task);\n\treturn 0;\n\nabort_with_gve_init:\n\tgve_teardown_priv_resources(priv);\n\nabort_with_wq:\n\tdestroy_workqueue(priv->gve_wq);\n\nabort_with_netdev:\n\tfree_netdev(dev);\n\nabort_with_db_bar:\n\tpci_iounmap(pdev, db_bar);\n\nabort_with_reg_bar:\n\tpci_iounmap(pdev, reg_bar);\n\nabort_with_pci_region:\n\tpci_release_regions(pdev);\n\nabort_with_enabled:\n\tpci_disable_device(pdev);\n\treturn err;\n}\n\nstatic void gve_remove(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct gve_priv *priv = netdev_priv(netdev);\n\t__be32 __iomem *db_bar = priv->db_bar2;\n\tvoid __iomem *reg_bar = priv->reg_bar0;\n\n\tunregister_netdev(netdev);\n\tgve_teardown_priv_resources(priv);\n\tdestroy_workqueue(priv->gve_wq);\n\tfree_netdev(netdev);\n\tpci_iounmap(pdev, db_bar);\n\tpci_iounmap(pdev, reg_bar);\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n}\n\nstatic void gve_shutdown(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct gve_priv *priv = netdev_priv(netdev);\n\tbool was_up = netif_carrier_ok(priv->dev);\n\n\trtnl_lock();\n\tif (was_up && gve_close(priv->dev)) {\n\t\t \n\t\tgve_reset_and_teardown(priv, was_up);\n\t} else {\n\t\t \n\t\tgve_teardown_priv_resources(priv);\n\t}\n\trtnl_unlock();\n}\n\n#ifdef CONFIG_PM\nstatic int gve_suspend(struct pci_dev *pdev, pm_message_t state)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct gve_priv *priv = netdev_priv(netdev);\n\tbool was_up = netif_carrier_ok(priv->dev);\n\n\tpriv->suspend_cnt++;\n\trtnl_lock();\n\tif (was_up && gve_close(priv->dev)) {\n\t\t \n\t\tgve_reset_and_teardown(priv, was_up);\n\t} else {\n\t\t \n\t\tgve_teardown_priv_resources(priv);\n\t}\n\tpriv->up_before_suspend = was_up;\n\trtnl_unlock();\n\treturn 0;\n}\n\nstatic int gve_resume(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct gve_priv *priv = netdev_priv(netdev);\n\tint err;\n\n\tpriv->resume_cnt++;\n\trtnl_lock();\n\terr = gve_reset_recovery(priv, priv->up_before_suspend);\n\trtnl_unlock();\n\treturn err;\n}\n#endif  \n\nstatic const struct pci_device_id gve_id_table[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_GOOGLE, PCI_DEV_ID_GVNIC) },\n\t{ }\n};\n\nstatic struct pci_driver gve_driver = {\n\t.name\t\t= gve_driver_name,\n\t.id_table\t= gve_id_table,\n\t.probe\t\t= gve_probe,\n\t.remove\t\t= gve_remove,\n\t.shutdown\t= gve_shutdown,\n#ifdef CONFIG_PM\n\t.suspend        = gve_suspend,\n\t.resume         = gve_resume,\n#endif\n};\n\nmodule_pci_driver(gve_driver);\n\nMODULE_DEVICE_TABLE(pci, gve_id_table);\nMODULE_AUTHOR(\"Google, Inc.\");\nMODULE_DESCRIPTION(\"Google Virtual NIC Driver\");\nMODULE_LICENSE(\"Dual MIT/GPL\");\nMODULE_VERSION(GVE_VERSION);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}