{
  "module_name": "gve_rx_dqo.c",
  "hash_id": "fffdfd44884a60acf96e494736253fb0571e9013755fe53b3e4b85395b14716c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/google/gve/gve_rx_dqo.c",
  "human_readable_source": "\n \n\n#include \"gve.h\"\n#include \"gve_dqo.h\"\n#include \"gve_adminq.h\"\n#include \"gve_utils.h\"\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <net/ip6_checksum.h>\n#include <net/ipv6.h>\n#include <net/tcp.h>\n\nstatic int gve_buf_ref_cnt(struct gve_rx_buf_state_dqo *bs)\n{\n\treturn page_count(bs->page_info.page) - bs->page_info.pagecnt_bias;\n}\n\nstatic void gve_free_page_dqo(struct gve_priv *priv,\n\t\t\t      struct gve_rx_buf_state_dqo *bs,\n\t\t\t      bool free_page)\n{\n\tpage_ref_sub(bs->page_info.page, bs->page_info.pagecnt_bias - 1);\n\tif (free_page)\n\t\tgve_free_page(&priv->pdev->dev, bs->page_info.page, bs->addr,\n\t\t\t      DMA_FROM_DEVICE);\n\tbs->page_info.page = NULL;\n}\n\nstatic struct gve_rx_buf_state_dqo *gve_alloc_buf_state(struct gve_rx_ring *rx)\n{\n\tstruct gve_rx_buf_state_dqo *buf_state;\n\ts16 buffer_id;\n\n\tbuffer_id = rx->dqo.free_buf_states;\n\tif (unlikely(buffer_id == -1))\n\t\treturn NULL;\n\n\tbuf_state = &rx->dqo.buf_states[buffer_id];\n\n\t \n\trx->dqo.free_buf_states = buf_state->next;\n\n\t \n\tbuf_state->next = buffer_id;\n\n\treturn buf_state;\n}\n\nstatic bool gve_buf_state_is_allocated(struct gve_rx_ring *rx,\n\t\t\t\t       struct gve_rx_buf_state_dqo *buf_state)\n{\n\ts16 buffer_id = buf_state - rx->dqo.buf_states;\n\n\treturn buf_state->next == buffer_id;\n}\n\nstatic void gve_free_buf_state(struct gve_rx_ring *rx,\n\t\t\t       struct gve_rx_buf_state_dqo *buf_state)\n{\n\ts16 buffer_id = buf_state - rx->dqo.buf_states;\n\n\tbuf_state->next = rx->dqo.free_buf_states;\n\trx->dqo.free_buf_states = buffer_id;\n}\n\nstatic struct gve_rx_buf_state_dqo *\ngve_dequeue_buf_state(struct gve_rx_ring *rx, struct gve_index_list *list)\n{\n\tstruct gve_rx_buf_state_dqo *buf_state;\n\ts16 buffer_id;\n\n\tbuffer_id = list->head;\n\tif (unlikely(buffer_id == -1))\n\t\treturn NULL;\n\n\tbuf_state = &rx->dqo.buf_states[buffer_id];\n\n\t \n\tlist->head = buf_state->next;\n\tif (buf_state->next == -1)\n\t\tlist->tail = -1;\n\n\t \n\tbuf_state->next = buffer_id;\n\n\treturn buf_state;\n}\n\nstatic void gve_enqueue_buf_state(struct gve_rx_ring *rx,\n\t\t\t\t  struct gve_index_list *list,\n\t\t\t\t  struct gve_rx_buf_state_dqo *buf_state)\n{\n\ts16 buffer_id = buf_state - rx->dqo.buf_states;\n\n\tbuf_state->next = -1;\n\n\tif (list->head == -1) {\n\t\tlist->head = buffer_id;\n\t\tlist->tail = buffer_id;\n\t} else {\n\t\tint tail = list->tail;\n\n\t\trx->dqo.buf_states[tail].next = buffer_id;\n\t\tlist->tail = buffer_id;\n\t}\n}\n\nstatic struct gve_rx_buf_state_dqo *\ngve_get_recycled_buf_state(struct gve_rx_ring *rx)\n{\n\tstruct gve_rx_buf_state_dqo *buf_state;\n\tint i;\n\n\t \n\tbuf_state = gve_dequeue_buf_state(rx, &rx->dqo.recycled_buf_states);\n\tif (likely(buf_state))\n\t\treturn buf_state;\n\n\tif (unlikely(rx->dqo.used_buf_states.head == -1))\n\t\treturn NULL;\n\n\t \n\tfor (i = 0; i < 5; i++) {\n\t\tbuf_state = gve_dequeue_buf_state(rx, &rx->dqo.used_buf_states);\n\t\tif (gve_buf_ref_cnt(buf_state) == 0) {\n\t\t\trx->dqo.used_buf_states_cnt--;\n\t\t\treturn buf_state;\n\t\t}\n\n\t\tgve_enqueue_buf_state(rx, &rx->dqo.used_buf_states, buf_state);\n\t}\n\n\t \n\tif (rx->dqo.qpl)\n\t\treturn NULL;\n\n\t \n\tif (unlikely(rx->dqo.free_buf_states == -1)) {\n\t\tbuf_state = gve_dequeue_buf_state(rx, &rx->dqo.used_buf_states);\n\t\tif (gve_buf_ref_cnt(buf_state) == 0)\n\t\t\treturn buf_state;\n\n\t\tgve_free_page_dqo(rx->gve, buf_state, true);\n\t\tgve_free_buf_state(rx, buf_state);\n\t}\n\n\treturn NULL;\n}\n\nstatic int gve_alloc_page_dqo(struct gve_rx_ring *rx,\n\t\t\t      struct gve_rx_buf_state_dqo *buf_state)\n{\n\tstruct gve_priv *priv = rx->gve;\n\tu32 idx;\n\n\tif (!rx->dqo.qpl) {\n\t\tint err;\n\n\t\terr = gve_alloc_page(priv, &priv->pdev->dev,\n\t\t\t\t     &buf_state->page_info.page,\n\t\t\t\t     &buf_state->addr,\n\t\t\t\t     DMA_FROM_DEVICE, GFP_ATOMIC);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\tidx = rx->dqo.next_qpl_page_idx;\n\t\tif (idx >= priv->rx_pages_per_qpl) {\n\t\t\tnet_err_ratelimited(\"%s: Out of QPL pages\\n\",\n\t\t\t\t\t    priv->dev->name);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tbuf_state->page_info.page = rx->dqo.qpl->pages[idx];\n\t\tbuf_state->addr = rx->dqo.qpl->page_buses[idx];\n\t\trx->dqo.next_qpl_page_idx++;\n\t}\n\tbuf_state->page_info.page_offset = 0;\n\tbuf_state->page_info.page_address =\n\t\tpage_address(buf_state->page_info.page);\n\tbuf_state->last_single_ref_offset = 0;\n\n\t \n\tpage_ref_add(buf_state->page_info.page, INT_MAX - 1);\n\tbuf_state->page_info.pagecnt_bias = INT_MAX;\n\n\treturn 0;\n}\n\nstatic void gve_rx_free_ring_dqo(struct gve_priv *priv, int idx)\n{\n\tstruct gve_rx_ring *rx = &priv->rx[idx];\n\tstruct device *hdev = &priv->pdev->dev;\n\tsize_t completion_queue_slots;\n\tsize_t buffer_queue_slots;\n\tsize_t size;\n\tint i;\n\n\tcompletion_queue_slots = rx->dqo.complq.mask + 1;\n\tbuffer_queue_slots = rx->dqo.bufq.mask + 1;\n\n\tgve_rx_remove_from_block(priv, idx);\n\n\tif (rx->q_resources) {\n\t\tdma_free_coherent(hdev, sizeof(*rx->q_resources),\n\t\t\t\t  rx->q_resources, rx->q_resources_bus);\n\t\trx->q_resources = NULL;\n\t}\n\n\tfor (i = 0; i < rx->dqo.num_buf_states; i++) {\n\t\tstruct gve_rx_buf_state_dqo *bs = &rx->dqo.buf_states[i];\n\t\t \n\t\tif (bs->page_info.page)\n\t\t\tgve_free_page_dqo(priv, bs, !rx->dqo.qpl);\n\t}\n\tif (rx->dqo.qpl) {\n\t\tgve_unassign_qpl(priv, rx->dqo.qpl->id);\n\t\trx->dqo.qpl = NULL;\n\t}\n\n\tif (rx->dqo.bufq.desc_ring) {\n\t\tsize = sizeof(rx->dqo.bufq.desc_ring[0]) * buffer_queue_slots;\n\t\tdma_free_coherent(hdev, size, rx->dqo.bufq.desc_ring,\n\t\t\t\t  rx->dqo.bufq.bus);\n\t\trx->dqo.bufq.desc_ring = NULL;\n\t}\n\n\tif (rx->dqo.complq.desc_ring) {\n\t\tsize = sizeof(rx->dqo.complq.desc_ring[0]) *\n\t\t\tcompletion_queue_slots;\n\t\tdma_free_coherent(hdev, size, rx->dqo.complq.desc_ring,\n\t\t\t\t  rx->dqo.complq.bus);\n\t\trx->dqo.complq.desc_ring = NULL;\n\t}\n\n\tkvfree(rx->dqo.buf_states);\n\trx->dqo.buf_states = NULL;\n\n\tnetif_dbg(priv, drv, priv->dev, \"freed rx ring %d\\n\", idx);\n}\n\nstatic int gve_rx_alloc_ring_dqo(struct gve_priv *priv, int idx)\n{\n\tstruct gve_rx_ring *rx = &priv->rx[idx];\n\tstruct device *hdev = &priv->pdev->dev;\n\tsize_t size;\n\tint i;\n\n\tconst u32 buffer_queue_slots =\n\t\tpriv->queue_format == GVE_DQO_RDA_FORMAT ?\n\t\tpriv->options_dqo_rda.rx_buff_ring_entries : priv->rx_desc_cnt;\n\tconst u32 completion_queue_slots = priv->rx_desc_cnt;\n\n\tnetif_dbg(priv, drv, priv->dev, \"allocating rx ring DQO\\n\");\n\n\tmemset(rx, 0, sizeof(*rx));\n\trx->gve = priv;\n\trx->q_num = idx;\n\trx->dqo.bufq.mask = buffer_queue_slots - 1;\n\trx->dqo.complq.num_free_slots = completion_queue_slots;\n\trx->dqo.complq.mask = completion_queue_slots - 1;\n\trx->ctx.skb_head = NULL;\n\trx->ctx.skb_tail = NULL;\n\n\trx->dqo.num_buf_states = priv->queue_format == GVE_DQO_RDA_FORMAT ?\n\t\tmin_t(s16, S16_MAX, buffer_queue_slots * 4) :\n\t\tpriv->rx_pages_per_qpl;\n\trx->dqo.buf_states = kvcalloc(rx->dqo.num_buf_states,\n\t\t\t\t      sizeof(rx->dqo.buf_states[0]),\n\t\t\t\t      GFP_KERNEL);\n\tif (!rx->dqo.buf_states)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < rx->dqo.num_buf_states - 1; i++)\n\t\trx->dqo.buf_states[i].next = i + 1;\n\n\trx->dqo.buf_states[rx->dqo.num_buf_states - 1].next = -1;\n\trx->dqo.recycled_buf_states.head = -1;\n\trx->dqo.recycled_buf_states.tail = -1;\n\trx->dqo.used_buf_states.head = -1;\n\trx->dqo.used_buf_states.tail = -1;\n\n\t \n\tsize = sizeof(rx->dqo.complq.desc_ring[0]) *\n\t\tcompletion_queue_slots;\n\trx->dqo.complq.desc_ring =\n\t\tdma_alloc_coherent(hdev, size, &rx->dqo.complq.bus, GFP_KERNEL);\n\tif (!rx->dqo.complq.desc_ring)\n\t\tgoto err;\n\n\t \n\tsize = sizeof(rx->dqo.bufq.desc_ring[0]) * buffer_queue_slots;\n\trx->dqo.bufq.desc_ring =\n\t\tdma_alloc_coherent(hdev, size, &rx->dqo.bufq.bus, GFP_KERNEL);\n\tif (!rx->dqo.bufq.desc_ring)\n\t\tgoto err;\n\n\tif (priv->queue_format != GVE_DQO_RDA_FORMAT) {\n\t\trx->dqo.qpl = gve_assign_rx_qpl(priv, rx->q_num);\n\t\tif (!rx->dqo.qpl)\n\t\t\tgoto err;\n\t\trx->dqo.next_qpl_page_idx = 0;\n\t}\n\n\trx->q_resources = dma_alloc_coherent(hdev, sizeof(*rx->q_resources),\n\t\t\t\t\t     &rx->q_resources_bus, GFP_KERNEL);\n\tif (!rx->q_resources)\n\t\tgoto err;\n\n\tgve_rx_add_to_block(priv, idx);\n\n\treturn 0;\n\nerr:\n\tgve_rx_free_ring_dqo(priv, idx);\n\treturn -ENOMEM;\n}\n\nvoid gve_rx_write_doorbell_dqo(const struct gve_priv *priv, int queue_idx)\n{\n\tconst struct gve_rx_ring *rx = &priv->rx[queue_idx];\n\tu64 index = be32_to_cpu(rx->q_resources->db_index);\n\n\tiowrite32(rx->dqo.bufq.tail, &priv->db_bar2[index]);\n}\n\nint gve_rx_alloc_rings_dqo(struct gve_priv *priv)\n{\n\tint err = 0;\n\tint i;\n\n\tfor (i = 0; i < priv->rx_cfg.num_queues; i++) {\n\t\terr = gve_rx_alloc_ring_dqo(priv, i);\n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"Failed to alloc rx ring=%d: err=%d\\n\",\n\t\t\t\t  i, err);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (i--; i >= 0; i--)\n\t\tgve_rx_free_ring_dqo(priv, i);\n\n\treturn err;\n}\n\nvoid gve_rx_free_rings_dqo(struct gve_priv *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->rx_cfg.num_queues; i++)\n\t\tgve_rx_free_ring_dqo(priv, i);\n}\n\nvoid gve_rx_post_buffers_dqo(struct gve_rx_ring *rx)\n{\n\tstruct gve_rx_compl_queue_dqo *complq = &rx->dqo.complq;\n\tstruct gve_rx_buf_queue_dqo *bufq = &rx->dqo.bufq;\n\tstruct gve_priv *priv = rx->gve;\n\tu32 num_avail_slots;\n\tu32 num_full_slots;\n\tu32 num_posted = 0;\n\n\tnum_full_slots = (bufq->tail - bufq->head) & bufq->mask;\n\tnum_avail_slots = bufq->mask - num_full_slots;\n\n\tnum_avail_slots = min_t(u32, num_avail_slots, complq->num_free_slots);\n\twhile (num_posted < num_avail_slots) {\n\t\tstruct gve_rx_desc_dqo *desc = &bufq->desc_ring[bufq->tail];\n\t\tstruct gve_rx_buf_state_dqo *buf_state;\n\n\t\tbuf_state = gve_get_recycled_buf_state(rx);\n\t\tif (unlikely(!buf_state)) {\n\t\t\tbuf_state = gve_alloc_buf_state(rx);\n\t\t\tif (unlikely(!buf_state))\n\t\t\t\tbreak;\n\n\t\t\tif (unlikely(gve_alloc_page_dqo(rx, buf_state))) {\n\t\t\t\tu64_stats_update_begin(&rx->statss);\n\t\t\t\trx->rx_buf_alloc_fail++;\n\t\t\t\tu64_stats_update_end(&rx->statss);\n\t\t\t\tgve_free_buf_state(rx, buf_state);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tdesc->buf_id = cpu_to_le16(buf_state - rx->dqo.buf_states);\n\t\tdesc->buf_addr = cpu_to_le64(buf_state->addr +\n\t\t\t\t\t     buf_state->page_info.page_offset);\n\n\t\tbufq->tail = (bufq->tail + 1) & bufq->mask;\n\t\tcomplq->num_free_slots--;\n\t\tnum_posted++;\n\n\t\tif ((bufq->tail & (GVE_RX_BUF_THRESH_DQO - 1)) == 0)\n\t\t\tgve_rx_write_doorbell_dqo(priv, rx->q_num);\n\t}\n\n\trx->fill_cnt += num_posted;\n}\n\nstatic void gve_try_recycle_buf(struct gve_priv *priv, struct gve_rx_ring *rx,\n\t\t\t\tstruct gve_rx_buf_state_dqo *buf_state)\n{\n\tconst int data_buffer_size = priv->data_buffer_size_dqo;\n\tint pagecount;\n\n\t \n\tif (data_buffer_size * 2 > PAGE_SIZE)\n\t\tgoto mark_used;\n\n\tpagecount = gve_buf_ref_cnt(buf_state);\n\n\t \n\tif (pagecount == 1) {\n\t\tbuf_state->last_single_ref_offset =\n\t\t\tbuf_state->page_info.page_offset;\n\t}\n\n\t \n\tbuf_state->page_info.page_offset += data_buffer_size;\n\tbuf_state->page_info.page_offset &= (PAGE_SIZE - 1);\n\n\t \n\tif (buf_state->page_info.page_offset ==\n\t    buf_state->last_single_ref_offset) {\n\t\tgoto mark_used;\n\t}\n\n\tgve_enqueue_buf_state(rx, &rx->dqo.recycled_buf_states, buf_state);\n\treturn;\n\nmark_used:\n\tgve_enqueue_buf_state(rx, &rx->dqo.used_buf_states, buf_state);\n\trx->dqo.used_buf_states_cnt++;\n}\n\nstatic void gve_rx_skb_csum(struct sk_buff *skb,\n\t\t\t    const struct gve_rx_compl_desc_dqo *desc,\n\t\t\t    struct gve_ptype ptype)\n{\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\t \n\tif (unlikely(!desc->l3_l4_processed))\n\t\treturn;\n\n\tif (ptype.l3_type == GVE_L3_TYPE_IPV4) {\n\t\tif (unlikely(desc->csum_ip_err || desc->csum_external_ip_err))\n\t\t\treturn;\n\t} else if (ptype.l3_type == GVE_L3_TYPE_IPV6) {\n\t\t \n\t\tif (unlikely(desc->ipv6_ex_add))\n\t\t\treturn;\n\t}\n\n\tif (unlikely(desc->csum_l4_err))\n\t\treturn;\n\n\tswitch (ptype.l4_type) {\n\tcase GVE_L4_TYPE_TCP:\n\tcase GVE_L4_TYPE_UDP:\n\tcase GVE_L4_TYPE_ICMP:\n\tcase GVE_L4_TYPE_SCTP:\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void gve_rx_skb_hash(struct sk_buff *skb,\n\t\t\t    const struct gve_rx_compl_desc_dqo *compl_desc,\n\t\t\t    struct gve_ptype ptype)\n{\n\tenum pkt_hash_types hash_type = PKT_HASH_TYPE_L2;\n\n\tif (ptype.l4_type != GVE_L4_TYPE_UNKNOWN)\n\t\thash_type = PKT_HASH_TYPE_L4;\n\telse if (ptype.l3_type != GVE_L3_TYPE_UNKNOWN)\n\t\thash_type = PKT_HASH_TYPE_L3;\n\n\tskb_set_hash(skb, le32_to_cpu(compl_desc->hash), hash_type);\n}\n\nstatic void gve_rx_free_skb(struct gve_rx_ring *rx)\n{\n\tif (!rx->ctx.skb_head)\n\t\treturn;\n\n\tdev_kfree_skb_any(rx->ctx.skb_head);\n\trx->ctx.skb_head = NULL;\n\trx->ctx.skb_tail = NULL;\n}\n\nstatic bool gve_rx_should_trigger_copy_ondemand(struct gve_rx_ring *rx)\n{\n\tif (!rx->dqo.qpl)\n\t\treturn false;\n\tif (rx->dqo.used_buf_states_cnt <\n\t\t     (rx->dqo.num_buf_states -\n\t\t     GVE_DQO_QPL_ONDEMAND_ALLOC_THRESHOLD))\n\t\treturn false;\n\treturn true;\n}\n\nstatic int gve_rx_copy_ondemand(struct gve_rx_ring *rx,\n\t\t\t\tstruct gve_rx_buf_state_dqo *buf_state,\n\t\t\t\tu16 buf_len)\n{\n\tstruct page *page = alloc_page(GFP_ATOMIC);\n\tint num_frags;\n\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tmemcpy(page_address(page),\n\t       buf_state->page_info.page_address +\n\t       buf_state->page_info.page_offset,\n\t       buf_len);\n\tnum_frags = skb_shinfo(rx->ctx.skb_tail)->nr_frags;\n\tskb_add_rx_frag(rx->ctx.skb_tail, num_frags, page,\n\t\t\t0, buf_len, PAGE_SIZE);\n\n\tu64_stats_update_begin(&rx->statss);\n\trx->rx_frag_alloc_cnt++;\n\tu64_stats_update_end(&rx->statss);\n\t \n\tgve_enqueue_buf_state(rx, &rx->dqo.recycled_buf_states, buf_state);\n\treturn 0;\n}\n\n \nstatic int gve_rx_append_frags(struct napi_struct *napi,\n\t\t\t       struct gve_rx_buf_state_dqo *buf_state,\n\t\t\t       u16 buf_len, struct gve_rx_ring *rx,\n\t\t\t       struct gve_priv *priv)\n{\n\tint num_frags = skb_shinfo(rx->ctx.skb_tail)->nr_frags;\n\n\tif (unlikely(num_frags == MAX_SKB_FRAGS)) {\n\t\tstruct sk_buff *skb;\n\n\t\tskb = napi_alloc_skb(napi, 0);\n\t\tif (!skb)\n\t\t\treturn -1;\n\n\t\tif (rx->ctx.skb_tail == rx->ctx.skb_head)\n\t\t\tskb_shinfo(rx->ctx.skb_head)->frag_list = skb;\n\t\telse\n\t\t\trx->ctx.skb_tail->next = skb;\n\t\trx->ctx.skb_tail = skb;\n\t\tnum_frags = 0;\n\t}\n\tif (rx->ctx.skb_tail != rx->ctx.skb_head) {\n\t\trx->ctx.skb_head->len += buf_len;\n\t\trx->ctx.skb_head->data_len += buf_len;\n\t\trx->ctx.skb_head->truesize += priv->data_buffer_size_dqo;\n\t}\n\n\t \n\tif (gve_rx_should_trigger_copy_ondemand(rx))\n\t\treturn gve_rx_copy_ondemand(rx, buf_state, buf_len);\n\n\tskb_add_rx_frag(rx->ctx.skb_tail, num_frags,\n\t\t\tbuf_state->page_info.page,\n\t\t\tbuf_state->page_info.page_offset,\n\t\t\tbuf_len, priv->data_buffer_size_dqo);\n\tgve_dec_pagecnt_bias(&buf_state->page_info);\n\n\t \n\tgve_try_recycle_buf(priv, rx, buf_state);\n\treturn 0;\n}\n\n \nstatic int gve_rx_dqo(struct napi_struct *napi, struct gve_rx_ring *rx,\n\t\t      const struct gve_rx_compl_desc_dqo *compl_desc,\n\t\t      int queue_idx)\n{\n\tconst u16 buffer_id = le16_to_cpu(compl_desc->buf_id);\n\tconst bool eop = compl_desc->end_of_packet != 0;\n\tstruct gve_rx_buf_state_dqo *buf_state;\n\tstruct gve_priv *priv = rx->gve;\n\tu16 buf_len;\n\n\tif (unlikely(buffer_id >= rx->dqo.num_buf_states)) {\n\t\tnet_err_ratelimited(\"%s: Invalid RX buffer_id=%u\\n\",\n\t\t\t\t    priv->dev->name, buffer_id);\n\t\treturn -EINVAL;\n\t}\n\tbuf_state = &rx->dqo.buf_states[buffer_id];\n\tif (unlikely(!gve_buf_state_is_allocated(rx, buf_state))) {\n\t\tnet_err_ratelimited(\"%s: RX buffer_id is not allocated: %u\\n\",\n\t\t\t\t    priv->dev->name, buffer_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(compl_desc->rx_error)) {\n\t\tgve_enqueue_buf_state(rx, &rx->dqo.recycled_buf_states,\n\t\t\t\t      buf_state);\n\t\treturn -EINVAL;\n\t}\n\n\tbuf_len = compl_desc->packet_len;\n\n\t \n\tprefetch(buf_state->page_info.page);\n\n\t \n\tdma_sync_single_range_for_cpu(&priv->pdev->dev, buf_state->addr,\n\t\t\t\t      buf_state->page_info.page_offset,\n\t\t\t\t      buf_len, DMA_FROM_DEVICE);\n\n\t \n\tif (rx->ctx.skb_head) {\n\t\tif (unlikely(gve_rx_append_frags(napi, buf_state, buf_len, rx,\n\t\t\t\t\t\t priv)) != 0) {\n\t\t\tgoto error;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (eop && buf_len <= priv->rx_copybreak) {\n\t\trx->ctx.skb_head = gve_rx_copy(priv->dev, napi,\n\t\t\t\t\t       &buf_state->page_info, buf_len);\n\t\tif (unlikely(!rx->ctx.skb_head))\n\t\t\tgoto error;\n\t\trx->ctx.skb_tail = rx->ctx.skb_head;\n\n\t\tu64_stats_update_begin(&rx->statss);\n\t\trx->rx_copied_pkt++;\n\t\trx->rx_copybreak_pkt++;\n\t\tu64_stats_update_end(&rx->statss);\n\n\t\tgve_enqueue_buf_state(rx, &rx->dqo.recycled_buf_states,\n\t\t\t\t      buf_state);\n\t\treturn 0;\n\t}\n\n\trx->ctx.skb_head = napi_get_frags(napi);\n\tif (unlikely(!rx->ctx.skb_head))\n\t\tgoto error;\n\trx->ctx.skb_tail = rx->ctx.skb_head;\n\n\tif (gve_rx_should_trigger_copy_ondemand(rx)) {\n\t\tif (gve_rx_copy_ondemand(rx, buf_state, buf_len) < 0)\n\t\t\tgoto error;\n\t\treturn 0;\n\t}\n\n\tskb_add_rx_frag(rx->ctx.skb_head, 0, buf_state->page_info.page,\n\t\t\tbuf_state->page_info.page_offset, buf_len,\n\t\t\tpriv->data_buffer_size_dqo);\n\tgve_dec_pagecnt_bias(&buf_state->page_info);\n\n\tgve_try_recycle_buf(priv, rx, buf_state);\n\treturn 0;\n\nerror:\n\tgve_enqueue_buf_state(rx, &rx->dqo.recycled_buf_states, buf_state);\n\treturn -ENOMEM;\n}\n\nstatic int gve_rx_complete_rsc(struct sk_buff *skb,\n\t\t\t       const struct gve_rx_compl_desc_dqo *desc,\n\t\t\t       struct gve_ptype ptype)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t \n\tif (ptype.l4_type != GVE_L4_TYPE_TCP)\n\t\treturn -EINVAL;\n\n\tswitch (ptype.l3_type) {\n\tcase GVE_L3_TYPE_IPV4:\n\t\tshinfo->gso_type = SKB_GSO_TCPV4;\n\t\tbreak;\n\tcase GVE_L3_TYPE_IPV6:\n\t\tshinfo->gso_type = SKB_GSO_TCPV6;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tshinfo->gso_size = le16_to_cpu(desc->rsc_seg_len);\n\treturn 0;\n}\n\n \nstatic int gve_rx_complete_skb(struct gve_rx_ring *rx, struct napi_struct *napi,\n\t\t\t       const struct gve_rx_compl_desc_dqo *desc,\n\t\t\t       netdev_features_t feat)\n{\n\tstruct gve_ptype ptype =\n\t\trx->gve->ptype_lut_dqo->ptypes[desc->packet_type];\n\tint err;\n\n\tskb_record_rx_queue(rx->ctx.skb_head, rx->q_num);\n\n\tif (feat & NETIF_F_RXHASH)\n\t\tgve_rx_skb_hash(rx->ctx.skb_head, desc, ptype);\n\n\tif (feat & NETIF_F_RXCSUM)\n\t\tgve_rx_skb_csum(rx->ctx.skb_head, desc, ptype);\n\n\t \n\tif (desc->rsc) {\n\t\terr = gve_rx_complete_rsc(rx->ctx.skb_head, desc, ptype);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tif (skb_headlen(rx->ctx.skb_head) == 0)\n\t\tnapi_gro_frags(napi);\n\telse\n\t\tnapi_gro_receive(napi, rx->ctx.skb_head);\n\n\treturn 0;\n}\n\nint gve_rx_poll_dqo(struct gve_notify_block *block, int budget)\n{\n\tstruct napi_struct *napi = &block->napi;\n\tnetdev_features_t feat = napi->dev->features;\n\n\tstruct gve_rx_ring *rx = block->rx;\n\tstruct gve_rx_compl_queue_dqo *complq = &rx->dqo.complq;\n\n\tu32 work_done = 0;\n\tu64 bytes = 0;\n\tint err;\n\n\twhile (work_done < budget) {\n\t\tstruct gve_rx_compl_desc_dqo *compl_desc =\n\t\t\t&complq->desc_ring[complq->head];\n\t\tu32 pkt_bytes;\n\n\t\t \n\t\tif (compl_desc->generation == complq->cur_gen_bit)\n\t\t\tbreak;\n\n\t\t \n\t\tprefetch(&complq->desc_ring[(complq->head + 1) & complq->mask]);\n\t\tprefetch(&complq->desc_ring[(complq->head + 2) & complq->mask]);\n\n\t\t \n\t\tdma_rmb();\n\n\t\terr = gve_rx_dqo(napi, rx, compl_desc, rx->q_num);\n\t\tif (err < 0) {\n\t\t\tgve_rx_free_skb(rx);\n\t\t\tu64_stats_update_begin(&rx->statss);\n\t\t\tif (err == -ENOMEM)\n\t\t\t\trx->rx_skb_alloc_fail++;\n\t\t\telse if (err == -EINVAL)\n\t\t\t\trx->rx_desc_err_dropped_pkt++;\n\t\t\tu64_stats_update_end(&rx->statss);\n\t\t}\n\n\t\tcomplq->head = (complq->head + 1) & complq->mask;\n\t\tcomplq->num_free_slots++;\n\n\t\t \n\t\tcomplq->cur_gen_bit ^= (complq->head == 0);\n\n\t\t \n\t\t{\n\t\t\tstruct gve_rx_buf_queue_dqo *bufq = &rx->dqo.bufq;\n\n\t\t\tbufq->head = (bufq->head + 1) & bufq->mask;\n\t\t}\n\n\t\t \n\t\trx->cnt++;\n\n\t\tif (!rx->ctx.skb_head)\n\t\t\tcontinue;\n\n\t\tif (!compl_desc->end_of_packet)\n\t\t\tcontinue;\n\n\t\twork_done++;\n\t\tpkt_bytes = rx->ctx.skb_head->len;\n\t\t \n\t\tif (skb_headlen(rx->ctx.skb_head))\n\t\t\tpkt_bytes += ETH_HLEN;\n\n\t\t \n\t\tif (gve_rx_complete_skb(rx, napi, compl_desc, feat) != 0) {\n\t\t\tgve_rx_free_skb(rx);\n\t\t\tu64_stats_update_begin(&rx->statss);\n\t\t\trx->rx_desc_err_dropped_pkt++;\n\t\t\tu64_stats_update_end(&rx->statss);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbytes += pkt_bytes;\n\t\trx->ctx.skb_head = NULL;\n\t\trx->ctx.skb_tail = NULL;\n\t}\n\n\tgve_rx_post_buffers_dqo(rx);\n\n\tu64_stats_update_begin(&rx->statss);\n\trx->rpackets += work_done;\n\trx->rbytes += bytes;\n\tu64_stats_update_end(&rx->statss);\n\n\treturn work_done;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}