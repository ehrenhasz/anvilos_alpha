{
  "module_name": "gve_rx.c",
  "hash_id": "b21ffc0bc528671c6bc54fb13d4b72b805bfad121475a9477d5786be077f921a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/google/gve/gve_rx.c",
  "human_readable_source": "\n \n\n#include \"gve.h\"\n#include \"gve_adminq.h\"\n#include \"gve_utils.h\"\n#include <linux/etherdevice.h>\n#include <linux/filter.h>\n#include <net/xdp.h>\n#include <net/xdp_sock_drv.h>\n\nstatic void gve_rx_free_buffer(struct device *dev,\n\t\t\t       struct gve_rx_slot_page_info *page_info,\n\t\t\t       union gve_rx_data_slot *data_slot)\n{\n\tdma_addr_t dma = (dma_addr_t)(be64_to_cpu(data_slot->addr) &\n\t\t\t\t      GVE_DATA_SLOT_ADDR_PAGE_MASK);\n\n\tpage_ref_sub(page_info->page, page_info->pagecnt_bias - 1);\n\tgve_free_page(dev, page_info->page, dma, DMA_FROM_DEVICE);\n}\n\nstatic void gve_rx_unfill_pages(struct gve_priv *priv, struct gve_rx_ring *rx)\n{\n\tu32 slots = rx->mask + 1;\n\tint i;\n\n\tif (rx->data.raw_addressing) {\n\t\tfor (i = 0; i < slots; i++)\n\t\t\tgve_rx_free_buffer(&priv->pdev->dev, &rx->data.page_info[i],\n\t\t\t\t\t   &rx->data.data_ring[i]);\n\t} else {\n\t\tfor (i = 0; i < slots; i++)\n\t\t\tpage_ref_sub(rx->data.page_info[i].page,\n\t\t\t\t     rx->data.page_info[i].pagecnt_bias - 1);\n\t\tgve_unassign_qpl(priv, rx->data.qpl->id);\n\t\trx->data.qpl = NULL;\n\n\t\tfor (i = 0; i < rx->qpl_copy_pool_mask + 1; i++) {\n\t\t\tpage_ref_sub(rx->qpl_copy_pool[i].page,\n\t\t\t\t     rx->qpl_copy_pool[i].pagecnt_bias - 1);\n\t\t\tput_page(rx->qpl_copy_pool[i].page);\n\t\t}\n\t}\n\tkvfree(rx->data.page_info);\n\trx->data.page_info = NULL;\n}\n\nstatic void gve_rx_free_ring(struct gve_priv *priv, int idx)\n{\n\tstruct gve_rx_ring *rx = &priv->rx[idx];\n\tstruct device *dev = &priv->pdev->dev;\n\tu32 slots = rx->mask + 1;\n\tsize_t bytes;\n\n\tgve_rx_remove_from_block(priv, idx);\n\n\tbytes = sizeof(struct gve_rx_desc) * priv->rx_desc_cnt;\n\tdma_free_coherent(dev, bytes, rx->desc.desc_ring, rx->desc.bus);\n\trx->desc.desc_ring = NULL;\n\n\tdma_free_coherent(dev, sizeof(*rx->q_resources),\n\t\t\t  rx->q_resources, rx->q_resources_bus);\n\trx->q_resources = NULL;\n\n\tgve_rx_unfill_pages(priv, rx);\n\n\tbytes = sizeof(*rx->data.data_ring) * slots;\n\tdma_free_coherent(dev, bytes, rx->data.data_ring,\n\t\t\t  rx->data.data_bus);\n\trx->data.data_ring = NULL;\n\n\tkvfree(rx->qpl_copy_pool);\n\trx->qpl_copy_pool = NULL;\n\n\tnetif_dbg(priv, drv, priv->dev, \"freed rx ring %d\\n\", idx);\n}\n\nstatic void gve_setup_rx_buffer(struct gve_rx_slot_page_info *page_info,\n\t\t\t     dma_addr_t addr, struct page *page, __be64 *slot_addr)\n{\n\tpage_info->page = page;\n\tpage_info->page_offset = 0;\n\tpage_info->page_address = page_address(page);\n\t*slot_addr = cpu_to_be64(addr);\n\t \n\tpage_ref_add(page, INT_MAX - 1);\n\tpage_info->pagecnt_bias = INT_MAX;\n}\n\nstatic int gve_rx_alloc_buffer(struct gve_priv *priv, struct device *dev,\n\t\t\t       struct gve_rx_slot_page_info *page_info,\n\t\t\t       union gve_rx_data_slot *data_slot)\n{\n\tstruct page *page;\n\tdma_addr_t dma;\n\tint err;\n\n\terr = gve_alloc_page(priv, dev, &page, &dma, DMA_FROM_DEVICE,\n\t\t\t     GFP_ATOMIC);\n\tif (err)\n\t\treturn err;\n\n\tgve_setup_rx_buffer(page_info, dma, page, &data_slot->addr);\n\treturn 0;\n}\n\nstatic int gve_prefill_rx_pages(struct gve_rx_ring *rx)\n{\n\tstruct gve_priv *priv = rx->gve;\n\tu32 slots;\n\tint err;\n\tint i;\n\tint j;\n\n\t \n\tslots = rx->mask + 1;\n\n\trx->data.page_info = kvzalloc(slots *\n\t\t\t\t      sizeof(*rx->data.page_info), GFP_KERNEL);\n\tif (!rx->data.page_info)\n\t\treturn -ENOMEM;\n\n\tif (!rx->data.raw_addressing) {\n\t\trx->data.qpl = gve_assign_rx_qpl(priv, rx->q_num);\n\t\tif (!rx->data.qpl) {\n\t\t\tkvfree(rx->data.page_info);\n\t\t\trx->data.page_info = NULL;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\tfor (i = 0; i < slots; i++) {\n\t\tif (!rx->data.raw_addressing) {\n\t\t\tstruct page *page = rx->data.qpl->pages[i];\n\t\t\tdma_addr_t addr = i * PAGE_SIZE;\n\n\t\t\tgve_setup_rx_buffer(&rx->data.page_info[i], addr, page,\n\t\t\t\t\t    &rx->data.data_ring[i].qpl_offset);\n\t\t\tcontinue;\n\t\t}\n\t\terr = gve_rx_alloc_buffer(priv, &priv->pdev->dev, &rx->data.page_info[i],\n\t\t\t\t\t  &rx->data.data_ring[i]);\n\t\tif (err)\n\t\t\tgoto alloc_err_rda;\n\t}\n\n\tif (!rx->data.raw_addressing) {\n\t\tfor (j = 0; j < rx->qpl_copy_pool_mask + 1; j++) {\n\t\t\tstruct page *page = alloc_page(GFP_KERNEL);\n\n\t\t\tif (!page) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto alloc_err_qpl;\n\t\t\t}\n\n\t\t\trx->qpl_copy_pool[j].page = page;\n\t\t\trx->qpl_copy_pool[j].page_offset = 0;\n\t\t\trx->qpl_copy_pool[j].page_address = page_address(page);\n\n\t\t\t \n\t\t\tpage_ref_add(page, INT_MAX - 1);\n\t\t\trx->qpl_copy_pool[j].pagecnt_bias = INT_MAX;\n\t\t}\n\t}\n\n\treturn slots;\n\nalloc_err_qpl:\n\t \n\twhile (j--) {\n\t\tpage_ref_sub(rx->qpl_copy_pool[j].page,\n\t\t\t     rx->qpl_copy_pool[j].pagecnt_bias - 1);\n\t\tput_page(rx->qpl_copy_pool[j].page);\n\t}\n\n\t \n\twhile (i--)\n\t\tpage_ref_sub(rx->data.page_info[i].page,\n\t\t\t     rx->data.page_info[i].pagecnt_bias - 1);\n\n\tgve_unassign_qpl(priv, rx->data.qpl->id);\n\trx->data.qpl = NULL;\n\n\treturn err;\n\nalloc_err_rda:\n\twhile (i--)\n\t\tgve_rx_free_buffer(&priv->pdev->dev,\n\t\t\t\t   &rx->data.page_info[i],\n\t\t\t\t   &rx->data.data_ring[i]);\n\treturn err;\n}\n\nstatic void gve_rx_ctx_clear(struct gve_rx_ctx *ctx)\n{\n\tctx->skb_head = NULL;\n\tctx->skb_tail = NULL;\n\tctx->total_size = 0;\n\tctx->frag_cnt = 0;\n\tctx->drop_pkt = false;\n}\n\nstatic int gve_rx_alloc_ring(struct gve_priv *priv, int idx)\n{\n\tstruct gve_rx_ring *rx = &priv->rx[idx];\n\tstruct device *hdev = &priv->pdev->dev;\n\tu32 slots, npages;\n\tint filled_pages;\n\tsize_t bytes;\n\tint err;\n\n\tnetif_dbg(priv, drv, priv->dev, \"allocating rx ring\\n\");\n\t \n\tmemset(rx, 0, sizeof(*rx));\n\n\trx->gve = priv;\n\trx->q_num = idx;\n\n\tslots = priv->rx_data_slot_cnt;\n\trx->mask = slots - 1;\n\trx->data.raw_addressing = priv->queue_format == GVE_GQI_RDA_FORMAT;\n\n\t \n\tbytes = sizeof(*rx->data.data_ring) * slots;\n\trx->data.data_ring = dma_alloc_coherent(hdev, bytes,\n\t\t\t\t\t\t&rx->data.data_bus,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!rx->data.data_ring)\n\t\treturn -ENOMEM;\n\n\trx->qpl_copy_pool_mask = min_t(u32, U32_MAX, slots * 2) - 1;\n\trx->qpl_copy_pool_head = 0;\n\trx->qpl_copy_pool = kvcalloc(rx->qpl_copy_pool_mask + 1,\n\t\t\t\t     sizeof(rx->qpl_copy_pool[0]),\n\t\t\t\t     GFP_KERNEL);\n\n\tif (!rx->qpl_copy_pool) {\n\t\terr = -ENOMEM;\n\t\tgoto abort_with_slots;\n\t}\n\n\tfilled_pages = gve_prefill_rx_pages(rx);\n\tif (filled_pages < 0) {\n\t\terr = -ENOMEM;\n\t\tgoto abort_with_copy_pool;\n\t}\n\trx->fill_cnt = filled_pages;\n\t \n\tdma_wmb();\n\n\t \n\trx->q_resources =\n\t\tdma_alloc_coherent(hdev,\n\t\t\t\t   sizeof(*rx->q_resources),\n\t\t\t\t   &rx->q_resources_bus,\n\t\t\t\t   GFP_KERNEL);\n\tif (!rx->q_resources) {\n\t\terr = -ENOMEM;\n\t\tgoto abort_filled;\n\t}\n\tnetif_dbg(priv, drv, priv->dev, \"rx[%d]->data.data_bus=%lx\\n\", idx,\n\t\t  (unsigned long)rx->data.data_bus);\n\n\t \n\tbytes = sizeof(struct gve_rx_desc) * priv->rx_desc_cnt;\n\tnpages = bytes / PAGE_SIZE;\n\tif (npages * PAGE_SIZE != bytes) {\n\t\terr = -EIO;\n\t\tgoto abort_with_q_resources;\n\t}\n\n\trx->desc.desc_ring = dma_alloc_coherent(hdev, bytes, &rx->desc.bus,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!rx->desc.desc_ring) {\n\t\terr = -ENOMEM;\n\t\tgoto abort_with_q_resources;\n\t}\n\trx->cnt = 0;\n\trx->db_threshold = priv->rx_desc_cnt / 2;\n\trx->desc.seqno = 1;\n\n\t \n\trx->packet_buffer_size = PAGE_SIZE / 2;\n\tgve_rx_ctx_clear(&rx->ctx);\n\tgve_rx_add_to_block(priv, idx);\n\n\treturn 0;\n\nabort_with_q_resources:\n\tdma_free_coherent(hdev, sizeof(*rx->q_resources),\n\t\t\t  rx->q_resources, rx->q_resources_bus);\n\trx->q_resources = NULL;\nabort_filled:\n\tgve_rx_unfill_pages(priv, rx);\nabort_with_copy_pool:\n\tkvfree(rx->qpl_copy_pool);\n\trx->qpl_copy_pool = NULL;\nabort_with_slots:\n\tbytes = sizeof(*rx->data.data_ring) * slots;\n\tdma_free_coherent(hdev, bytes, rx->data.data_ring, rx->data.data_bus);\n\trx->data.data_ring = NULL;\n\n\treturn err;\n}\n\nint gve_rx_alloc_rings(struct gve_priv *priv)\n{\n\tint err = 0;\n\tint i;\n\n\tfor (i = 0; i < priv->rx_cfg.num_queues; i++) {\n\t\terr = gve_rx_alloc_ring(priv, i);\n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"Failed to alloc rx ring=%d: err=%d\\n\",\n\t\t\t\t  i, err);\n\t\t\tbreak;\n\t\t}\n\t}\n\t \n\tif (err) {\n\t\tint j;\n\n\t\tfor (j = 0; j < i; j++)\n\t\t\tgve_rx_free_ring(priv, j);\n\t}\n\treturn err;\n}\n\nvoid gve_rx_free_rings_gqi(struct gve_priv *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->rx_cfg.num_queues; i++)\n\t\tgve_rx_free_ring(priv, i);\n}\n\nvoid gve_rx_write_doorbell(struct gve_priv *priv, struct gve_rx_ring *rx)\n{\n\tu32 db_idx = be32_to_cpu(rx->q_resources->db_index);\n\n\tiowrite32be(rx->fill_cnt, &priv->db_bar2[db_idx]);\n}\n\nstatic enum pkt_hash_types gve_rss_type(__be16 pkt_flags)\n{\n\tif (likely(pkt_flags & (GVE_RXF_TCP | GVE_RXF_UDP)))\n\t\treturn PKT_HASH_TYPE_L4;\n\tif (pkt_flags & (GVE_RXF_IPV4 | GVE_RXF_IPV6))\n\t\treturn PKT_HASH_TYPE_L3;\n\treturn PKT_HASH_TYPE_L2;\n}\n\nstatic struct sk_buff *gve_rx_add_frags(struct napi_struct *napi,\n\t\t\t\t\tstruct gve_rx_slot_page_info *page_info,\n\t\t\t\t\tu16 packet_buffer_size, u16 len,\n\t\t\t\t\tstruct gve_rx_ctx *ctx)\n{\n\tu32 offset = page_info->page_offset + page_info->pad;\n\tstruct sk_buff *skb = ctx->skb_tail;\n\tint num_frags = 0;\n\n\tif (!skb) {\n\t\tskb = napi_get_frags(napi);\n\t\tif (unlikely(!skb))\n\t\t\treturn NULL;\n\n\t\tctx->skb_head = skb;\n\t\tctx->skb_tail = skb;\n\t} else {\n\t\tnum_frags = skb_shinfo(ctx->skb_tail)->nr_frags;\n\t\tif (num_frags == MAX_SKB_FRAGS) {\n\t\t\tskb = napi_alloc_skb(napi, 0);\n\t\t\tif (!skb)\n\t\t\t\treturn NULL;\n\n\t\t\t\n\t\t\t\n\t\t\tskb_shinfo(ctx->skb_tail)->frag_list = skb;\n\n\t\t\tctx->skb_tail = skb;\n\t\t\tnum_frags = 0;\n\t\t}\n\t}\n\n\tif (skb != ctx->skb_head) {\n\t\tctx->skb_head->len += len;\n\t\tctx->skb_head->data_len += len;\n\t\tctx->skb_head->truesize += packet_buffer_size;\n\t}\n\tskb_add_rx_frag(skb, num_frags, page_info->page,\n\t\t\toffset, len, packet_buffer_size);\n\n\treturn ctx->skb_head;\n}\n\nstatic void gve_rx_flip_buff(struct gve_rx_slot_page_info *page_info, __be64 *slot_addr)\n{\n\tconst __be64 offset = cpu_to_be64(PAGE_SIZE / 2);\n\n\t \n\tpage_info->page_offset ^= PAGE_SIZE / 2;\n\t*(slot_addr) ^= offset;\n}\n\nstatic int gve_rx_can_recycle_buffer(struct gve_rx_slot_page_info *page_info)\n{\n\tint pagecount = page_count(page_info->page);\n\n\t \n\tif (pagecount == page_info->pagecnt_bias)\n\t\treturn 1;\n\t \n\telse if (pagecount > page_info->pagecnt_bias)\n\t\treturn 0;\n\tWARN(pagecount < page_info->pagecnt_bias,\n\t     \"Pagecount should never be less than the bias.\");\n\treturn -1;\n}\n\nstatic struct sk_buff *\ngve_rx_raw_addressing(struct device *dev, struct net_device *netdev,\n\t\t      struct gve_rx_slot_page_info *page_info, u16 len,\n\t\t      struct napi_struct *napi,\n\t\t      union gve_rx_data_slot *data_slot,\n\t\t      u16 packet_buffer_size, struct gve_rx_ctx *ctx)\n{\n\tstruct sk_buff *skb = gve_rx_add_frags(napi, page_info, packet_buffer_size, len, ctx);\n\n\tif (!skb)\n\t\treturn NULL;\n\n\t \n\tgve_dec_pagecnt_bias(page_info);\n\n\treturn skb;\n}\n\nstatic struct sk_buff *gve_rx_copy_to_pool(struct gve_rx_ring *rx,\n\t\t\t\t\t   struct gve_rx_slot_page_info *page_info,\n\t\t\t\t\t   u16 len, struct napi_struct *napi)\n{\n\tu32 pool_idx = rx->qpl_copy_pool_head & rx->qpl_copy_pool_mask;\n\tvoid *src = page_info->page_address + page_info->page_offset;\n\tstruct gve_rx_slot_page_info *copy_page_info;\n\tstruct gve_rx_ctx *ctx = &rx->ctx;\n\tbool alloc_page = false;\n\tstruct sk_buff *skb;\n\tvoid *dst;\n\n\tcopy_page_info = &rx->qpl_copy_pool[pool_idx];\n\tif (!copy_page_info->can_flip) {\n\t\tint recycle = gve_rx_can_recycle_buffer(copy_page_info);\n\n\t\tif (unlikely(recycle < 0)) {\n\t\t\tgve_schedule_reset(rx->gve);\n\t\t\treturn NULL;\n\t\t}\n\t\talloc_page = !recycle;\n\t}\n\n\tif (alloc_page) {\n\t\tstruct gve_rx_slot_page_info alloc_page_info;\n\t\tstruct page *page;\n\n\t\t \n\t\trx->qpl_copy_pool_head++;\n\n\t\tpage = alloc_page(GFP_ATOMIC);\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\talloc_page_info.page = page;\n\t\talloc_page_info.page_offset = 0;\n\t\talloc_page_info.page_address = page_address(page);\n\t\talloc_page_info.pad = page_info->pad;\n\n\t\tmemcpy(alloc_page_info.page_address, src, page_info->pad + len);\n\t\tskb = gve_rx_add_frags(napi, &alloc_page_info,\n\t\t\t\t       rx->packet_buffer_size,\n\t\t\t\t       len, ctx);\n\n\t\tu64_stats_update_begin(&rx->statss);\n\t\trx->rx_frag_copy_cnt++;\n\t\trx->rx_frag_alloc_cnt++;\n\t\tu64_stats_update_end(&rx->statss);\n\n\t\treturn skb;\n\t}\n\n\tdst = copy_page_info->page_address + copy_page_info->page_offset;\n\tmemcpy(dst, src, page_info->pad + len);\n\tcopy_page_info->pad = page_info->pad;\n\n\tskb = gve_rx_add_frags(napi, copy_page_info,\n\t\t\t       rx->packet_buffer_size, len, ctx);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tgve_dec_pagecnt_bias(copy_page_info);\n\tcopy_page_info->page_offset += rx->packet_buffer_size;\n\tcopy_page_info->page_offset &= (PAGE_SIZE - 1);\n\n\tif (copy_page_info->can_flip) {\n\t\t \n\t\tcopy_page_info->can_flip = false;\n\t\trx->qpl_copy_pool_head++;\n\t\tprefetch(rx->qpl_copy_pool[rx->qpl_copy_pool_head & rx->qpl_copy_pool_mask].page);\n\t} else {\n\t\tcopy_page_info->can_flip = true;\n\t}\n\n\tu64_stats_update_begin(&rx->statss);\n\trx->rx_frag_copy_cnt++;\n\tu64_stats_update_end(&rx->statss);\n\n\treturn skb;\n}\n\nstatic struct sk_buff *\ngve_rx_qpl(struct device *dev, struct net_device *netdev,\n\t   struct gve_rx_ring *rx, struct gve_rx_slot_page_info *page_info,\n\t   u16 len, struct napi_struct *napi,\n\t   union gve_rx_data_slot *data_slot)\n{\n\tstruct gve_rx_ctx *ctx = &rx->ctx;\n\tstruct sk_buff *skb;\n\n\t \n\tif (page_info->can_flip) {\n\t\tskb = gve_rx_add_frags(napi, page_info, rx->packet_buffer_size, len, ctx);\n\t\t \n\t\tif (skb) {\n\t\t\t \n\t\t\tgve_dec_pagecnt_bias(page_info);\n\t\t\tgve_rx_flip_buff(page_info, &data_slot->qpl_offset);\n\t\t}\n\t} else {\n\t\tskb = gve_rx_copy_to_pool(rx, page_info, len, napi);\n\t}\n\treturn skb;\n}\n\nstatic struct sk_buff *gve_rx_skb(struct gve_priv *priv, struct gve_rx_ring *rx,\n\t\t\t\t  struct gve_rx_slot_page_info *page_info, struct napi_struct *napi,\n\t\t\t\t  u16 len, union gve_rx_data_slot *data_slot,\n\t\t\t\t  bool is_only_frag)\n{\n\tstruct net_device *netdev = priv->dev;\n\tstruct gve_rx_ctx *ctx = &rx->ctx;\n\tstruct sk_buff *skb = NULL;\n\n\tif (len <= priv->rx_copybreak && is_only_frag)  {\n\t\t \n\t\tskb = gve_rx_copy(netdev, napi, page_info, len);\n\t\tif (skb) {\n\t\t\tu64_stats_update_begin(&rx->statss);\n\t\t\trx->rx_copied_pkt++;\n\t\t\trx->rx_frag_copy_cnt++;\n\t\t\trx->rx_copybreak_pkt++;\n\t\t\tu64_stats_update_end(&rx->statss);\n\t\t}\n\t} else {\n\t\tint recycle = gve_rx_can_recycle_buffer(page_info);\n\n\t\tif (unlikely(recycle < 0)) {\n\t\t\tgve_schedule_reset(priv);\n\t\t\treturn NULL;\n\t\t}\n\t\tpage_info->can_flip = recycle;\n\t\tif (page_info->can_flip) {\n\t\t\tu64_stats_update_begin(&rx->statss);\n\t\t\trx->rx_frag_flip_cnt++;\n\t\t\tu64_stats_update_end(&rx->statss);\n\t\t}\n\n\t\tif (rx->data.raw_addressing) {\n\t\t\tskb = gve_rx_raw_addressing(&priv->pdev->dev, netdev,\n\t\t\t\t\t\t    page_info, len, napi,\n\t\t\t\t\t\t    data_slot,\n\t\t\t\t\t\t    rx->packet_buffer_size, ctx);\n\t\t} else {\n\t\t\tskb = gve_rx_qpl(&priv->pdev->dev, netdev, rx,\n\t\t\t\t\t page_info, len, napi, data_slot);\n\t\t}\n\t}\n\treturn skb;\n}\n\nstatic int gve_xsk_pool_redirect(struct net_device *dev,\n\t\t\t\t struct gve_rx_ring *rx,\n\t\t\t\t void *data, int len,\n\t\t\t\t struct bpf_prog *xdp_prog)\n{\n\tstruct xdp_buff *xdp;\n\tint err;\n\n\tif (rx->xsk_pool->frame_len < len)\n\t\treturn -E2BIG;\n\txdp = xsk_buff_alloc(rx->xsk_pool);\n\tif (!xdp) {\n\t\tu64_stats_update_begin(&rx->statss);\n\t\trx->xdp_alloc_fails++;\n\t\tu64_stats_update_end(&rx->statss);\n\t\treturn -ENOMEM;\n\t}\n\txdp->data_end = xdp->data + len;\n\tmemcpy(xdp->data, data, len);\n\terr = xdp_do_redirect(dev, xdp, xdp_prog);\n\tif (err)\n\t\txsk_buff_free(xdp);\n\treturn err;\n}\n\nstatic int gve_xdp_redirect(struct net_device *dev, struct gve_rx_ring *rx,\n\t\t\t    struct xdp_buff *orig, struct bpf_prog *xdp_prog)\n{\n\tint total_len, len = orig->data_end - orig->data;\n\tint headroom = XDP_PACKET_HEADROOM;\n\tstruct xdp_buff new;\n\tvoid *frame;\n\tint err;\n\n\tif (rx->xsk_pool)\n\t\treturn gve_xsk_pool_redirect(dev, rx, orig->data,\n\t\t\t\t\t     len, xdp_prog);\n\n\ttotal_len = headroom + SKB_DATA_ALIGN(len) +\n\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tframe = page_frag_alloc(&rx->page_cache, total_len, GFP_ATOMIC);\n\tif (!frame) {\n\t\tu64_stats_update_begin(&rx->statss);\n\t\trx->xdp_alloc_fails++;\n\t\tu64_stats_update_end(&rx->statss);\n\t\treturn -ENOMEM;\n\t}\n\txdp_init_buff(&new, total_len, &rx->xdp_rxq);\n\txdp_prepare_buff(&new, frame, headroom, len, false);\n\tmemcpy(new.data, orig->data, len);\n\n\terr = xdp_do_redirect(dev, &new, xdp_prog);\n\tif (err)\n\t\tpage_frag_free(frame);\n\n\treturn err;\n}\n\nstatic void gve_xdp_done(struct gve_priv *priv, struct gve_rx_ring *rx,\n\t\t\t struct xdp_buff *xdp, struct bpf_prog *xprog,\n\t\t\t int xdp_act)\n{\n\tstruct gve_tx_ring *tx;\n\tint tx_qid;\n\tint err;\n\n\tswitch (xdp_act) {\n\tcase XDP_ABORTED:\n\tcase XDP_DROP:\n\tdefault:\n\t\tbreak;\n\tcase XDP_TX:\n\t\ttx_qid = gve_xdp_tx_queue_id(priv, rx->q_num);\n\t\ttx = &priv->tx[tx_qid];\n\t\tspin_lock(&tx->xdp_lock);\n\t\terr = gve_xdp_xmit_one(priv, tx, xdp->data,\n\t\t\t\t       xdp->data_end - xdp->data, NULL);\n\t\tspin_unlock(&tx->xdp_lock);\n\n\t\tif (unlikely(err)) {\n\t\t\tu64_stats_update_begin(&rx->statss);\n\t\t\trx->xdp_tx_errors++;\n\t\t\tu64_stats_update_end(&rx->statss);\n\t\t}\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\terr = gve_xdp_redirect(priv->dev, rx, xdp, xprog);\n\n\t\tif (unlikely(err)) {\n\t\t\tu64_stats_update_begin(&rx->statss);\n\t\t\trx->xdp_redirect_errors++;\n\t\t\tu64_stats_update_end(&rx->statss);\n\t\t}\n\t\tbreak;\n\t}\n\tu64_stats_update_begin(&rx->statss);\n\tif ((u32)xdp_act < GVE_XDP_ACTIONS)\n\t\trx->xdp_actions[xdp_act]++;\n\tu64_stats_update_end(&rx->statss);\n}\n\n#define GVE_PKTCONT_BIT_IS_SET(x) (GVE_RXF_PKT_CONT & (x))\nstatic void gve_rx(struct gve_rx_ring *rx, netdev_features_t feat,\n\t\t   struct gve_rx_desc *desc, u32 idx,\n\t\t   struct gve_rx_cnts *cnts)\n{\n\tbool is_last_frag = !GVE_PKTCONT_BIT_IS_SET(desc->flags_seq);\n\tstruct gve_rx_slot_page_info *page_info;\n\tu16 frag_size = be16_to_cpu(desc->len);\n\tstruct gve_rx_ctx *ctx = &rx->ctx;\n\tunion gve_rx_data_slot *data_slot;\n\tstruct gve_priv *priv = rx->gve;\n\tstruct sk_buff *skb = NULL;\n\tstruct bpf_prog *xprog;\n\tstruct xdp_buff xdp;\n\tdma_addr_t page_bus;\n\tvoid *va;\n\n\tu16 len = frag_size;\n\tstruct napi_struct *napi = &priv->ntfy_blocks[rx->ntfy_id].napi;\n\tbool is_first_frag = ctx->frag_cnt == 0;\n\n\tbool is_only_frag = is_first_frag && is_last_frag;\n\n\tif (unlikely(ctx->drop_pkt))\n\t\tgoto finish_frag;\n\n\tif (desc->flags_seq & GVE_RXF_ERR) {\n\t\tctx->drop_pkt = true;\n\t\tcnts->desc_err_pkt_cnt++;\n\t\tnapi_free_frags(napi);\n\t\tgoto finish_frag;\n\t}\n\n\tif (unlikely(frag_size > rx->packet_buffer_size)) {\n\t\tnetdev_warn(priv->dev, \"Unexpected frag size %d, can't exceed %d, scheduling reset\",\n\t\t\t    frag_size, rx->packet_buffer_size);\n\t\tctx->drop_pkt = true;\n\t\tnapi_free_frags(napi);\n\t\tgve_schedule_reset(rx->gve);\n\t\tgoto finish_frag;\n\t}\n\n\t \n\tpage_info = &rx->data.page_info[(idx + 2) & rx->mask];\n\tva = page_info->page_address + page_info->page_offset;\n\tprefetch(page_info->page);  \n\tprefetch(va);               \n\tprefetch(va + 64);          \n\n\tpage_info = &rx->data.page_info[idx];\n\tdata_slot = &rx->data.data_ring[idx];\n\tpage_bus = (rx->data.raw_addressing) ?\n\t\tbe64_to_cpu(data_slot->addr) - page_info->page_offset :\n\t\trx->data.qpl->page_buses[idx];\n\tdma_sync_single_for_cpu(&priv->pdev->dev, page_bus,\n\t\t\t\tPAGE_SIZE, DMA_FROM_DEVICE);\n\tpage_info->pad = is_first_frag ? GVE_RX_PAD : 0;\n\tlen -= page_info->pad;\n\tfrag_size -= page_info->pad;\n\n\txprog = READ_ONCE(priv->xdp_prog);\n\tif (xprog && is_only_frag) {\n\t\tvoid *old_data;\n\t\tint xdp_act;\n\n\t\txdp_init_buff(&xdp, rx->packet_buffer_size, &rx->xdp_rxq);\n\t\txdp_prepare_buff(&xdp, page_info->page_address +\n\t\t\t\t page_info->page_offset, GVE_RX_PAD,\n\t\t\t\t len, false);\n\t\told_data = xdp.data;\n\t\txdp_act = bpf_prog_run_xdp(xprog, &xdp);\n\t\tif (xdp_act != XDP_PASS) {\n\t\t\tgve_xdp_done(priv, rx, &xdp, xprog, xdp_act);\n\t\t\tctx->total_size += frag_size;\n\t\t\tgoto finish_ok_pkt;\n\t\t}\n\n\t\tpage_info->pad += xdp.data - old_data;\n\t\tlen = xdp.data_end - xdp.data;\n\n\t\tu64_stats_update_begin(&rx->statss);\n\t\trx->xdp_actions[XDP_PASS]++;\n\t\tu64_stats_update_end(&rx->statss);\n\t}\n\n\tskb = gve_rx_skb(priv, rx, page_info, napi, len,\n\t\t\t data_slot, is_only_frag);\n\tif (!skb) {\n\t\tu64_stats_update_begin(&rx->statss);\n\t\trx->rx_skb_alloc_fail++;\n\t\tu64_stats_update_end(&rx->statss);\n\n\t\tnapi_free_frags(napi);\n\t\tctx->drop_pkt = true;\n\t\tgoto finish_frag;\n\t}\n\tctx->total_size += frag_size;\n\n\tif (is_first_frag) {\n\t\tif (likely(feat & NETIF_F_RXCSUM)) {\n\t\t\t \n\t\t\tif (desc->csum)\n\t\t\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\t\telse\n\t\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\t\tskb->csum = csum_unfold(desc->csum);\n\t\t}\n\n\t\t \n\t\tif (likely(feat & NETIF_F_RXHASH) &&\n\t\t    gve_needs_rss(desc->flags_seq))\n\t\t\tskb_set_hash(skb, be32_to_cpu(desc->rss_hash),\n\t\t\t\t     gve_rss_type(desc->flags_seq));\n\t}\n\n\tif (is_last_frag) {\n\t\tskb_record_rx_queue(skb, rx->q_num);\n\t\tif (skb_is_nonlinear(skb))\n\t\t\tnapi_gro_frags(napi);\n\t\telse\n\t\t\tnapi_gro_receive(napi, skb);\n\t\tgoto finish_ok_pkt;\n\t}\n\n\tgoto finish_frag;\n\nfinish_ok_pkt:\n\tcnts->ok_pkt_bytes += ctx->total_size;\n\tcnts->ok_pkt_cnt++;\nfinish_frag:\n\tctx->frag_cnt++;\n\tif (is_last_frag) {\n\t\tcnts->total_pkt_cnt++;\n\t\tcnts->cont_pkt_cnt += (ctx->frag_cnt > 1);\n\t\tgve_rx_ctx_clear(ctx);\n\t}\n}\n\nbool gve_rx_work_pending(struct gve_rx_ring *rx)\n{\n\tstruct gve_rx_desc *desc;\n\t__be16 flags_seq;\n\tu32 next_idx;\n\n\tnext_idx = rx->cnt & rx->mask;\n\tdesc = rx->desc.desc_ring + next_idx;\n\n\tflags_seq = desc->flags_seq;\n\n\treturn (GVE_SEQNO(flags_seq) == rx->desc.seqno);\n}\n\nstatic bool gve_rx_refill_buffers(struct gve_priv *priv, struct gve_rx_ring *rx)\n{\n\tint refill_target = rx->mask + 1;\n\tu32 fill_cnt = rx->fill_cnt;\n\n\twhile (fill_cnt - rx->cnt < refill_target) {\n\t\tstruct gve_rx_slot_page_info *page_info;\n\t\tu32 idx = fill_cnt & rx->mask;\n\n\t\tpage_info = &rx->data.page_info[idx];\n\t\tif (page_info->can_flip) {\n\t\t\t \n\t\t\tunion gve_rx_data_slot *data_slot =\n\t\t\t\t\t\t&rx->data.data_ring[idx];\n\n\t\t\tgve_rx_flip_buff(page_info, &data_slot->addr);\n\t\t\tpage_info->can_flip = 0;\n\t\t} else {\n\t\t\t \n\t\t\tint recycle = gve_rx_can_recycle_buffer(page_info);\n\n\t\t\tif (recycle < 0) {\n\t\t\t\tif (!rx->data.raw_addressing)\n\t\t\t\t\tgve_schedule_reset(priv);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tif (!recycle) {\n\t\t\t\t \n\t\t\t\tunion gve_rx_data_slot *data_slot =\n\t\t\t\t\t\t&rx->data.data_ring[idx];\n\t\t\t\tstruct device *dev = &priv->pdev->dev;\n\t\t\t\tgve_rx_free_buffer(dev, page_info, data_slot);\n\t\t\t\tpage_info->page = NULL;\n\t\t\t\tif (gve_rx_alloc_buffer(priv, dev, page_info,\n\t\t\t\t\t\t\tdata_slot)) {\n\t\t\t\t\tu64_stats_update_begin(&rx->statss);\n\t\t\t\t\trx->rx_buf_alloc_fail++;\n\t\t\t\t\tu64_stats_update_end(&rx->statss);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfill_cnt++;\n\t}\n\trx->fill_cnt = fill_cnt;\n\treturn true;\n}\n\nstatic int gve_clean_rx_done(struct gve_rx_ring *rx, int budget,\n\t\t\t     netdev_features_t feat)\n{\n\tu64 xdp_redirects = rx->xdp_actions[XDP_REDIRECT];\n\tu64 xdp_txs = rx->xdp_actions[XDP_TX];\n\tstruct gve_rx_ctx *ctx = &rx->ctx;\n\tstruct gve_priv *priv = rx->gve;\n\tstruct gve_rx_cnts cnts = {0};\n\tstruct gve_rx_desc *next_desc;\n\tu32 idx = rx->cnt & rx->mask;\n\tu32 work_done = 0;\n\n\tstruct gve_rx_desc *desc = &rx->desc.desc_ring[idx];\n\n\t \n\twhile ((GVE_SEQNO(desc->flags_seq) == rx->desc.seqno) &&\n\t       (work_done < budget || ctx->frag_cnt)) {\n\t\tnext_desc = &rx->desc.desc_ring[(idx + 1) & rx->mask];\n\t\tprefetch(next_desc);\n\n\t\tgve_rx(rx, feat, desc, idx, &cnts);\n\n\t\trx->cnt++;\n\t\tidx = rx->cnt & rx->mask;\n\t\tdesc = &rx->desc.desc_ring[idx];\n\t\trx->desc.seqno = gve_next_seqno(rx->desc.seqno);\n\t\twork_done++;\n\t}\n\n\t \n\tif (unlikely(ctx->frag_cnt)) {\n\t\tstruct napi_struct *napi = &priv->ntfy_blocks[rx->ntfy_id].napi;\n\n\t\tnapi_free_frags(napi);\n\t\tgve_rx_ctx_clear(&rx->ctx);\n\t\tnetdev_warn(priv->dev, \"Unexpected seq number %d with incomplete packet, expected %d, scheduling reset\",\n\t\t\t    GVE_SEQNO(desc->flags_seq), rx->desc.seqno);\n\t\tgve_schedule_reset(rx->gve);\n\t}\n\n\tif (!work_done && rx->fill_cnt - rx->cnt > rx->db_threshold)\n\t\treturn 0;\n\n\tif (work_done) {\n\t\tu64_stats_update_begin(&rx->statss);\n\t\trx->rpackets += cnts.ok_pkt_cnt;\n\t\trx->rbytes += cnts.ok_pkt_bytes;\n\t\trx->rx_cont_packet_cnt += cnts.cont_pkt_cnt;\n\t\trx->rx_desc_err_dropped_pkt += cnts.desc_err_pkt_cnt;\n\t\tu64_stats_update_end(&rx->statss);\n\t}\n\n\tif (xdp_txs != rx->xdp_actions[XDP_TX])\n\t\tgve_xdp_tx_flush(priv, rx->q_num);\n\n\tif (xdp_redirects != rx->xdp_actions[XDP_REDIRECT])\n\t\txdp_do_flush();\n\n\t \n\tif (!rx->data.raw_addressing) {\n\t\t \n\t\trx->fill_cnt += work_done;\n\t} else if (rx->fill_cnt - rx->cnt <= rx->db_threshold) {\n\t\t \n\t\tif (!gve_rx_refill_buffers(priv, rx))\n\t\t\treturn 0;\n\n\t\t \n\t\tif (rx->fill_cnt - rx->cnt <= rx->db_threshold) {\n\t\t\tgve_rx_write_doorbell(priv, rx);\n\t\t\treturn budget;\n\t\t}\n\t}\n\n\tgve_rx_write_doorbell(priv, rx);\n\treturn cnts.total_pkt_cnt;\n}\n\nint gve_rx_poll(struct gve_notify_block *block, int budget)\n{\n\tstruct gve_rx_ring *rx = block->rx;\n\tnetdev_features_t feat;\n\tint work_done = 0;\n\n\tfeat = block->napi.dev->features;\n\n\tif (budget > 0)\n\t\twork_done = gve_clean_rx_done(rx, budget, feat);\n\n\treturn work_done;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}