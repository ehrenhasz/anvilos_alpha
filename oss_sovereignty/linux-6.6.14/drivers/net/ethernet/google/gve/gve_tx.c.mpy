{
  "module_name": "gve_tx.c",
  "hash_id": "e78169b52a1ef97199ae2546d468b16f5245818d8912041eb3ab77bd7887c4b0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/google/gve/gve_tx.c",
  "human_readable_source": "\n \n\n#include \"gve.h\"\n#include \"gve_adminq.h\"\n#include \"gve_utils.h\"\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/vmalloc.h>\n#include <linux/skbuff.h>\n#include <net/xdp_sock_drv.h>\n\nstatic inline void gve_tx_put_doorbell(struct gve_priv *priv,\n\t\t\t\t       struct gve_queue_resources *q_resources,\n\t\t\t\t       u32 val)\n{\n\tiowrite32be(val, &priv->db_bar2[be32_to_cpu(q_resources->db_index)]);\n}\n\nvoid gve_xdp_tx_flush(struct gve_priv *priv, u32 xdp_qid)\n{\n\tu32 tx_qid = gve_xdp_tx_queue_id(priv, xdp_qid);\n\tstruct gve_tx_ring *tx = &priv->tx[tx_qid];\n\n\tgve_tx_put_doorbell(priv, tx->q_resources, tx->req);\n}\n\n \n\nstatic int gve_tx_fifo_init(struct gve_priv *priv, struct gve_tx_fifo *fifo)\n{\n\tfifo->base = vmap(fifo->qpl->pages, fifo->qpl->num_entries, VM_MAP,\n\t\t\t  PAGE_KERNEL);\n\tif (unlikely(!fifo->base)) {\n\t\tnetif_err(priv, drv, priv->dev, \"Failed to vmap fifo, qpl_id = %d\\n\",\n\t\t\t  fifo->qpl->id);\n\t\treturn -ENOMEM;\n\t}\n\n\tfifo->size = fifo->qpl->num_entries * PAGE_SIZE;\n\tatomic_set(&fifo->available, fifo->size);\n\tfifo->head = 0;\n\treturn 0;\n}\n\nstatic void gve_tx_fifo_release(struct gve_priv *priv, struct gve_tx_fifo *fifo)\n{\n\tWARN(atomic_read(&fifo->available) != fifo->size,\n\t     \"Releasing non-empty fifo\");\n\n\tvunmap(fifo->base);\n}\n\nstatic int gve_tx_fifo_pad_alloc_one_frag(struct gve_tx_fifo *fifo,\n\t\t\t\t\t  size_t bytes)\n{\n\treturn (fifo->head + bytes < fifo->size) ? 0 : fifo->size - fifo->head;\n}\n\nstatic bool gve_tx_fifo_can_alloc(struct gve_tx_fifo *fifo, size_t bytes)\n{\n\treturn (atomic_read(&fifo->available) <= bytes) ? false : true;\n}\n\n \nstatic int gve_tx_alloc_fifo(struct gve_tx_fifo *fifo, size_t bytes,\n\t\t\t     struct gve_tx_iovec iov[2])\n{\n\tsize_t overflow, padding;\n\tu32 aligned_head;\n\tint nfrags = 0;\n\n\tif (!bytes)\n\t\treturn 0;\n\n\t \n\tWARN(!gve_tx_fifo_can_alloc(fifo, bytes),\n\t     \"Reached %s when there's not enough space in the fifo\", __func__);\n\n\tnfrags++;\n\n\tiov[0].iov_offset = fifo->head;\n\tiov[0].iov_len = bytes;\n\tfifo->head += bytes;\n\n\tif (fifo->head > fifo->size) {\n\t\t \n\t\tnfrags++;\n\t\toverflow = fifo->head - fifo->size;\n\t\tiov[0].iov_len -= overflow;\n\t\tiov[1].iov_offset = 0;\t \n\t\tiov[1].iov_len = overflow;\n\n\t\tfifo->head = overflow;\n\t}\n\n\t \n\taligned_head = L1_CACHE_ALIGN(fifo->head);\n\tpadding = aligned_head - fifo->head;\n\tiov[nfrags - 1].iov_padding = padding;\n\tatomic_sub(bytes + padding, &fifo->available);\n\tfifo->head = aligned_head;\n\n\tif (fifo->head == fifo->size)\n\t\tfifo->head = 0;\n\n\treturn nfrags;\n}\n\n \nstatic void gve_tx_free_fifo(struct gve_tx_fifo *fifo, size_t bytes)\n{\n\tatomic_add(bytes, &fifo->available);\n}\n\nstatic size_t gve_tx_clear_buffer_state(struct gve_tx_buffer_state *info)\n{\n\tsize_t space_freed = 0;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(info->iov); i++) {\n\t\tspace_freed += info->iov[i].iov_len + info->iov[i].iov_padding;\n\t\tinfo->iov[i].iov_len = 0;\n\t\tinfo->iov[i].iov_padding = 0;\n\t}\n\treturn space_freed;\n}\n\nstatic int gve_clean_xdp_done(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t\t      u32 to_do)\n{\n\tstruct gve_tx_buffer_state *info;\n\tu32 clean_end = tx->done + to_do;\n\tu64 pkts = 0, bytes = 0;\n\tsize_t space_freed = 0;\n\tu32 xsk_complete = 0;\n\tu32 idx;\n\n\tfor (; tx->done < clean_end; tx->done++) {\n\t\tidx = tx->done & tx->mask;\n\t\tinfo = &tx->info[idx];\n\n\t\tif (unlikely(!info->xdp.size))\n\t\t\tcontinue;\n\n\t\tbytes += info->xdp.size;\n\t\tpkts++;\n\t\txsk_complete += info->xdp.is_xsk;\n\n\t\tinfo->xdp.size = 0;\n\t\tif (info->xdp_frame) {\n\t\t\txdp_return_frame(info->xdp_frame);\n\t\t\tinfo->xdp_frame = NULL;\n\t\t}\n\t\tspace_freed += gve_tx_clear_buffer_state(info);\n\t}\n\n\tgve_tx_free_fifo(&tx->tx_fifo, space_freed);\n\tif (xsk_complete > 0 && tx->xsk_pool)\n\t\txsk_tx_completed(tx->xsk_pool, xsk_complete);\n\tu64_stats_update_begin(&tx->statss);\n\ttx->bytes_done += bytes;\n\ttx->pkt_done += pkts;\n\tu64_stats_update_end(&tx->statss);\n\treturn pkts;\n}\n\nstatic int gve_clean_tx_done(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t\t     u32 to_do, bool try_to_wake);\n\nstatic void gve_tx_free_ring(struct gve_priv *priv, int idx)\n{\n\tstruct gve_tx_ring *tx = &priv->tx[idx];\n\tstruct device *hdev = &priv->pdev->dev;\n\tsize_t bytes;\n\tu32 slots;\n\n\tgve_tx_remove_from_block(priv, idx);\n\tslots = tx->mask + 1;\n\tif (tx->q_num < priv->tx_cfg.num_queues) {\n\t\tgve_clean_tx_done(priv, tx, priv->tx_desc_cnt, false);\n\t\tnetdev_tx_reset_queue(tx->netdev_txq);\n\t} else {\n\t\tgve_clean_xdp_done(priv, tx, priv->tx_desc_cnt);\n\t}\n\n\tdma_free_coherent(hdev, sizeof(*tx->q_resources),\n\t\t\t  tx->q_resources, tx->q_resources_bus);\n\ttx->q_resources = NULL;\n\n\tif (!tx->raw_addressing) {\n\t\tgve_tx_fifo_release(priv, &tx->tx_fifo);\n\t\tgve_unassign_qpl(priv, tx->tx_fifo.qpl->id);\n\t\ttx->tx_fifo.qpl = NULL;\n\t}\n\n\tbytes = sizeof(*tx->desc) * slots;\n\tdma_free_coherent(hdev, bytes, tx->desc, tx->bus);\n\ttx->desc = NULL;\n\n\tvfree(tx->info);\n\ttx->info = NULL;\n\n\tnetif_dbg(priv, drv, priv->dev, \"freed tx queue %d\\n\", idx);\n}\n\nstatic int gve_tx_alloc_ring(struct gve_priv *priv, int idx)\n{\n\tstruct gve_tx_ring *tx = &priv->tx[idx];\n\tstruct device *hdev = &priv->pdev->dev;\n\tu32 slots = priv->tx_desc_cnt;\n\tsize_t bytes;\n\n\t \n\tmemset(tx, 0, sizeof(*tx));\n\tspin_lock_init(&tx->clean_lock);\n\tspin_lock_init(&tx->xdp_lock);\n\ttx->q_num = idx;\n\n\ttx->mask = slots - 1;\n\n\t \n\ttx->info = vcalloc(slots, sizeof(*tx->info));\n\tif (!tx->info)\n\t\treturn -ENOMEM;\n\n\t \n\tbytes = sizeof(*tx->desc) * slots;\n\ttx->desc = dma_alloc_coherent(hdev, bytes, &tx->bus, GFP_KERNEL);\n\tif (!tx->desc)\n\t\tgoto abort_with_info;\n\n\ttx->raw_addressing = priv->queue_format == GVE_GQI_RDA_FORMAT;\n\ttx->dev = &priv->pdev->dev;\n\tif (!tx->raw_addressing) {\n\t\ttx->tx_fifo.qpl = gve_assign_tx_qpl(priv, idx);\n\t\tif (!tx->tx_fifo.qpl)\n\t\t\tgoto abort_with_desc;\n\t\t \n\t\tif (gve_tx_fifo_init(priv, &tx->tx_fifo))\n\t\t\tgoto abort_with_qpl;\n\t}\n\n\ttx->q_resources =\n\t\tdma_alloc_coherent(hdev,\n\t\t\t\t   sizeof(*tx->q_resources),\n\t\t\t\t   &tx->q_resources_bus,\n\t\t\t\t   GFP_KERNEL);\n\tif (!tx->q_resources)\n\t\tgoto abort_with_fifo;\n\n\tnetif_dbg(priv, drv, priv->dev, \"tx[%d]->bus=%lx\\n\", idx,\n\t\t  (unsigned long)tx->bus);\n\tif (idx < priv->tx_cfg.num_queues)\n\t\ttx->netdev_txq = netdev_get_tx_queue(priv->dev, idx);\n\tgve_tx_add_to_block(priv, idx);\n\n\treturn 0;\n\nabort_with_fifo:\n\tif (!tx->raw_addressing)\n\t\tgve_tx_fifo_release(priv, &tx->tx_fifo);\nabort_with_qpl:\n\tif (!tx->raw_addressing)\n\t\tgve_unassign_qpl(priv, tx->tx_fifo.qpl->id);\nabort_with_desc:\n\tdma_free_coherent(hdev, bytes, tx->desc, tx->bus);\n\ttx->desc = NULL;\nabort_with_info:\n\tvfree(tx->info);\n\ttx->info = NULL;\n\treturn -ENOMEM;\n}\n\nint gve_tx_alloc_rings(struct gve_priv *priv, int start_id, int num_rings)\n{\n\tint err = 0;\n\tint i;\n\n\tfor (i = start_id; i < start_id + num_rings; i++) {\n\t\terr = gve_tx_alloc_ring(priv, i);\n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"Failed to alloc tx ring=%d: err=%d\\n\",\n\t\t\t\t  i, err);\n\t\t\tbreak;\n\t\t}\n\t}\n\t \n\tif (err) {\n\t\tint j;\n\n\t\tfor (j = start_id; j < i; j++)\n\t\t\tgve_tx_free_ring(priv, j);\n\t}\n\treturn err;\n}\n\nvoid gve_tx_free_rings_gqi(struct gve_priv *priv, int start_id, int num_rings)\n{\n\tint i;\n\n\tfor (i = start_id; i < start_id + num_rings; i++)\n\t\tgve_tx_free_ring(priv, i);\n}\n\n \nstatic inline u32 gve_tx_avail(struct gve_tx_ring *tx)\n{\n\treturn tx->mask + 1 - (tx->req - tx->done);\n}\n\nstatic inline int gve_skb_fifo_bytes_required(struct gve_tx_ring *tx,\n\t\t\t\t\t      struct sk_buff *skb)\n{\n\tint pad_bytes, align_hdr_pad;\n\tint bytes;\n\tint hlen;\n\n\thlen = skb_is_gso(skb) ? skb_checksum_start_offset(skb) + tcp_hdrlen(skb) :\n\t\t\t\t min_t(int, GVE_GQ_TX_MIN_PKT_DESC_BYTES, skb->len);\n\n\tpad_bytes = gve_tx_fifo_pad_alloc_one_frag(&tx->tx_fifo,\n\t\t\t\t\t\t   hlen);\n\t \n\talign_hdr_pad = L1_CACHE_ALIGN(hlen) - hlen;\n\tbytes = align_hdr_pad + pad_bytes + skb->len;\n\n\treturn bytes;\n}\n\n \n#define MAX_TX_DESC_NEEDED\t(MAX_SKB_FRAGS + 4)\nstatic void gve_tx_unmap_buf(struct device *dev, struct gve_tx_buffer_state *info)\n{\n\tif (info->skb) {\n\t\tdma_unmap_single(dev, dma_unmap_addr(info, dma),\n\t\t\t\t dma_unmap_len(info, len),\n\t\t\t\t DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(info, len, 0);\n\t} else {\n\t\tdma_unmap_page(dev, dma_unmap_addr(info, dma),\n\t\t\t       dma_unmap_len(info, len),\n\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(info, len, 0);\n\t}\n}\n\n \nstatic inline bool gve_can_tx(struct gve_tx_ring *tx, int bytes_required)\n{\n\tbool can_alloc = true;\n\n\tif (!tx->raw_addressing)\n\t\tcan_alloc = gve_tx_fifo_can_alloc(&tx->tx_fifo, bytes_required);\n\n\treturn (gve_tx_avail(tx) >= MAX_TX_DESC_NEEDED && can_alloc);\n}\n\nstatic_assert(NAPI_POLL_WEIGHT >= MAX_TX_DESC_NEEDED);\n\n \nstatic int gve_maybe_stop_tx(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t\t     struct sk_buff *skb)\n{\n\tint bytes_required = 0;\n\tu32 nic_done;\n\tu32 to_do;\n\tint ret;\n\n\tif (!tx->raw_addressing)\n\t\tbytes_required = gve_skb_fifo_bytes_required(tx, skb);\n\n\tif (likely(gve_can_tx(tx, bytes_required)))\n\t\treturn 0;\n\n\tret = -EBUSY;\n\tspin_lock(&tx->clean_lock);\n\tnic_done = gve_tx_load_event_counter(priv, tx);\n\tto_do = nic_done - tx->done;\n\n\t \n\tif (to_do + gve_tx_avail(tx) >= MAX_TX_DESC_NEEDED) {\n\t\tif (to_do > 0) {\n\t\t\tto_do = min_t(u32, to_do, NAPI_POLL_WEIGHT);\n\t\t\tgve_clean_tx_done(priv, tx, to_do, false);\n\t\t}\n\t\tif (likely(gve_can_tx(tx, bytes_required)))\n\t\t\tret = 0;\n\t}\n\tif (ret) {\n\t\t \n\t\ttx->stop_queue++;\n\t\tnetif_tx_stop_queue(tx->netdev_txq);\n\t}\n\tspin_unlock(&tx->clean_lock);\n\n\treturn ret;\n}\n\nstatic void gve_tx_fill_pkt_desc(union gve_tx_desc *pkt_desc,\n\t\t\t\t u16 csum_offset, u8 ip_summed, bool is_gso,\n\t\t\t\t int l4_hdr_offset, u32 desc_cnt,\n\t\t\t\t u16 hlen, u64 addr, u16 pkt_len)\n{\n\t \n\tif (is_gso) {\n\t\tpkt_desc->pkt.type_flags = GVE_TXD_TSO | GVE_TXF_L4CSUM;\n\t\tpkt_desc->pkt.l4_csum_offset = csum_offset >> 1;\n\t\tpkt_desc->pkt.l4_hdr_offset = l4_hdr_offset >> 1;\n\t} else if (likely(ip_summed == CHECKSUM_PARTIAL)) {\n\t\tpkt_desc->pkt.type_flags = GVE_TXD_STD | GVE_TXF_L4CSUM;\n\t\tpkt_desc->pkt.l4_csum_offset = csum_offset >> 1;\n\t\tpkt_desc->pkt.l4_hdr_offset = l4_hdr_offset >> 1;\n\t} else {\n\t\tpkt_desc->pkt.type_flags = GVE_TXD_STD;\n\t\tpkt_desc->pkt.l4_csum_offset = 0;\n\t\tpkt_desc->pkt.l4_hdr_offset = 0;\n\t}\n\tpkt_desc->pkt.desc_cnt = desc_cnt;\n\tpkt_desc->pkt.len = cpu_to_be16(pkt_len);\n\tpkt_desc->pkt.seg_len = cpu_to_be16(hlen);\n\tpkt_desc->pkt.seg_addr = cpu_to_be64(addr);\n}\n\nstatic void gve_tx_fill_mtd_desc(union gve_tx_desc *mtd_desc,\n\t\t\t\t struct sk_buff *skb)\n{\n\tBUILD_BUG_ON(sizeof(mtd_desc->mtd) != sizeof(mtd_desc->pkt));\n\n\tmtd_desc->mtd.type_flags = GVE_TXD_MTD | GVE_MTD_SUBTYPE_PATH;\n\tmtd_desc->mtd.path_state = GVE_MTD_PATH_STATE_DEFAULT |\n\t\t\t\t   GVE_MTD_PATH_HASH_L4;\n\tmtd_desc->mtd.path_hash = cpu_to_be32(skb->hash);\n\tmtd_desc->mtd.reserved0 = 0;\n\tmtd_desc->mtd.reserved1 = 0;\n}\n\nstatic void gve_tx_fill_seg_desc(union gve_tx_desc *seg_desc,\n\t\t\t\t u16 l3_offset, u16 gso_size,\n\t\t\t\t bool is_gso_v6, bool is_gso,\n\t\t\t\t u16 len, u64 addr)\n{\n\tseg_desc->seg.type_flags = GVE_TXD_SEG;\n\tif (is_gso) {\n\t\tif (is_gso_v6)\n\t\t\tseg_desc->seg.type_flags |= GVE_TXSF_IPV6;\n\t\tseg_desc->seg.l3_offset = l3_offset >> 1;\n\t\tseg_desc->seg.mss = cpu_to_be16(gso_size);\n\t}\n\tseg_desc->seg.seg_len = cpu_to_be16(len);\n\tseg_desc->seg.seg_addr = cpu_to_be64(addr);\n}\n\nstatic void gve_dma_sync_for_device(struct device *dev, dma_addr_t *page_buses,\n\t\t\t\t    u64 iov_offset, u64 iov_len)\n{\n\tu64 last_page = (iov_offset + iov_len - 1) / PAGE_SIZE;\n\tu64 first_page = iov_offset / PAGE_SIZE;\n\tu64 page;\n\n\tfor (page = first_page; page <= last_page; page++)\n\t\tdma_sync_single_for_device(dev, page_buses[page], PAGE_SIZE, DMA_TO_DEVICE);\n}\n\nstatic int gve_tx_add_skb_copy(struct gve_priv *priv, struct gve_tx_ring *tx, struct sk_buff *skb)\n{\n\tint pad_bytes, hlen, hdr_nfrags, payload_nfrags, l4_hdr_offset;\n\tunion gve_tx_desc *pkt_desc, *seg_desc;\n\tstruct gve_tx_buffer_state *info;\n\tint mtd_desc_nr = !!skb->l4_hash;\n\tbool is_gso = skb_is_gso(skb);\n\tu32 idx = tx->req & tx->mask;\n\tint payload_iov = 2;\n\tint copy_offset;\n\tu32 next_idx;\n\tint i;\n\n\tinfo = &tx->info[idx];\n\tpkt_desc = &tx->desc[idx];\n\n\tl4_hdr_offset = skb_checksum_start_offset(skb);\n\t \n\thlen = is_gso ? l4_hdr_offset + tcp_hdrlen(skb) :\n\t\t\tmin_t(int, GVE_GQ_TX_MIN_PKT_DESC_BYTES, skb->len);\n\n\tinfo->skb =  skb;\n\t \n\tpad_bytes = gve_tx_fifo_pad_alloc_one_frag(&tx->tx_fifo, hlen);\n\thdr_nfrags = gve_tx_alloc_fifo(&tx->tx_fifo, hlen + pad_bytes,\n\t\t\t\t       &info->iov[0]);\n\tWARN(!hdr_nfrags, \"hdr_nfrags should never be 0!\");\n\tpayload_nfrags = gve_tx_alloc_fifo(&tx->tx_fifo, skb->len - hlen,\n\t\t\t\t\t   &info->iov[payload_iov]);\n\n\tgve_tx_fill_pkt_desc(pkt_desc, skb->csum_offset, skb->ip_summed,\n\t\t\t     is_gso, l4_hdr_offset,\n\t\t\t     1 + mtd_desc_nr + payload_nfrags, hlen,\n\t\t\t     info->iov[hdr_nfrags - 1].iov_offset, skb->len);\n\n\tskb_copy_bits(skb, 0,\n\t\t      tx->tx_fifo.base + info->iov[hdr_nfrags - 1].iov_offset,\n\t\t      hlen);\n\tgve_dma_sync_for_device(&priv->pdev->dev, tx->tx_fifo.qpl->page_buses,\n\t\t\t\tinfo->iov[hdr_nfrags - 1].iov_offset,\n\t\t\t\tinfo->iov[hdr_nfrags - 1].iov_len);\n\tcopy_offset = hlen;\n\n\tif (mtd_desc_nr) {\n\t\tnext_idx = (tx->req + 1) & tx->mask;\n\t\tgve_tx_fill_mtd_desc(&tx->desc[next_idx], skb);\n\t}\n\n\tfor (i = payload_iov; i < payload_nfrags + payload_iov; i++) {\n\t\tnext_idx = (tx->req + 1 + mtd_desc_nr + i - payload_iov) & tx->mask;\n\t\tseg_desc = &tx->desc[next_idx];\n\n\t\tgve_tx_fill_seg_desc(seg_desc, skb_network_offset(skb),\n\t\t\t\t     skb_shinfo(skb)->gso_size,\n\t\t\t\t     skb_is_gso_v6(skb), is_gso,\n\t\t\t\t     info->iov[i].iov_len,\n\t\t\t\t     info->iov[i].iov_offset);\n\n\t\tskb_copy_bits(skb, copy_offset,\n\t\t\t      tx->tx_fifo.base + info->iov[i].iov_offset,\n\t\t\t      info->iov[i].iov_len);\n\t\tgve_dma_sync_for_device(&priv->pdev->dev, tx->tx_fifo.qpl->page_buses,\n\t\t\t\t\tinfo->iov[i].iov_offset,\n\t\t\t\t\tinfo->iov[i].iov_len);\n\t\tcopy_offset += info->iov[i].iov_len;\n\t}\n\n\treturn 1 + mtd_desc_nr + payload_nfrags;\n}\n\nstatic int gve_tx_add_skb_no_copy(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint hlen, num_descriptors, l4_hdr_offset;\n\tunion gve_tx_desc *pkt_desc, *mtd_desc, *seg_desc;\n\tstruct gve_tx_buffer_state *info;\n\tint mtd_desc_nr = !!skb->l4_hash;\n\tbool is_gso = skb_is_gso(skb);\n\tu32 idx = tx->req & tx->mask;\n\tu64 addr;\n\tu32 len;\n\tint i;\n\n\tinfo = &tx->info[idx];\n\tpkt_desc = &tx->desc[idx];\n\n\tl4_hdr_offset = skb_checksum_start_offset(skb);\n\t \n\thlen = is_gso ? l4_hdr_offset + tcp_hdrlen(skb) : skb_headlen(skb);\n\tlen = skb_headlen(skb);\n\n\tinfo->skb =  skb;\n\n\taddr = dma_map_single(tx->dev, skb->data, len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(tx->dev, addr))) {\n\t\ttx->dma_mapping_error++;\n\t\tgoto drop;\n\t}\n\tdma_unmap_len_set(info, len, len);\n\tdma_unmap_addr_set(info, dma, addr);\n\n\tnum_descriptors = 1 + shinfo->nr_frags;\n\tif (hlen < len)\n\t\tnum_descriptors++;\n\tif (mtd_desc_nr)\n\t\tnum_descriptors++;\n\n\tgve_tx_fill_pkt_desc(pkt_desc, skb->csum_offset, skb->ip_summed,\n\t\t\t     is_gso, l4_hdr_offset,\n\t\t\t     num_descriptors, hlen, addr, skb->len);\n\n\tif (mtd_desc_nr) {\n\t\tidx = (idx + 1) & tx->mask;\n\t\tmtd_desc = &tx->desc[idx];\n\t\tgve_tx_fill_mtd_desc(mtd_desc, skb);\n\t}\n\n\tif (hlen < len) {\n\t\t \n\t\tlen -= hlen;\n\t\taddr += hlen;\n\t\tidx = (idx + 1) & tx->mask;\n\t\tseg_desc = &tx->desc[idx];\n\t\tgve_tx_fill_seg_desc(seg_desc, skb_network_offset(skb),\n\t\t\t\t     skb_shinfo(skb)->gso_size,\n\t\t\t\t     skb_is_gso_v6(skb), is_gso, len, addr);\n\t}\n\n\tfor (i = 0; i < shinfo->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &shinfo->frags[i];\n\n\t\tidx = (idx + 1) & tx->mask;\n\t\tseg_desc = &tx->desc[idx];\n\t\tlen = skb_frag_size(frag);\n\t\taddr = skb_frag_dma_map(tx->dev, frag, 0, len, DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(tx->dev, addr))) {\n\t\t\ttx->dma_mapping_error++;\n\t\t\tgoto unmap_drop;\n\t\t}\n\t\ttx->info[idx].skb = NULL;\n\t\tdma_unmap_len_set(&tx->info[idx], len, len);\n\t\tdma_unmap_addr_set(&tx->info[idx], dma, addr);\n\n\t\tgve_tx_fill_seg_desc(seg_desc, skb_network_offset(skb),\n\t\t\t\t     skb_shinfo(skb)->gso_size,\n\t\t\t\t     skb_is_gso_v6(skb), is_gso, len, addr);\n\t}\n\n\treturn num_descriptors;\n\nunmap_drop:\n\ti += num_descriptors - shinfo->nr_frags;\n\twhile (i--) {\n\t\t \n\t\tif (i == 1 && mtd_desc_nr == 1)\n\t\t\tcontinue;\n\t\tidx--;\n\t\tgve_tx_unmap_buf(tx->dev, &tx->info[idx & tx->mask]);\n\t}\ndrop:\n\ttx->dropped_pkt++;\n\treturn 0;\n}\n\nnetdev_tx_t gve_tx(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tstruct gve_tx_ring *tx;\n\tint nsegs;\n\n\tWARN(skb_get_queue_mapping(skb) >= priv->tx_cfg.num_queues,\n\t     \"skb queue index out of range\");\n\ttx = &priv->tx[skb_get_queue_mapping(skb)];\n\tif (unlikely(gve_maybe_stop_tx(priv, tx, skb))) {\n\t\t \n\n\t\tgve_tx_put_doorbell(priv, tx->q_resources, tx->req);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\tif (tx->raw_addressing)\n\t\tnsegs = gve_tx_add_skb_no_copy(priv, tx, skb);\n\telse\n\t\tnsegs = gve_tx_add_skb_copy(priv, tx, skb);\n\n\t \n\tif (nsegs) {\n\t\tnetdev_tx_sent_queue(tx->netdev_txq, skb->len);\n\t\tskb_tx_timestamp(skb);\n\t\ttx->req += nsegs;\n\t} else {\n\t\tdev_kfree_skb_any(skb);\n\t}\n\n\tif (!netif_xmit_stopped(tx->netdev_txq) && netdev_xmit_more())\n\t\treturn NETDEV_TX_OK;\n\n\t \n\tgve_tx_put_doorbell(priv, tx->q_resources, tx->req);\n\treturn NETDEV_TX_OK;\n}\n\nstatic int gve_tx_fill_xdp(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t\t   void *data, int len, void *frame_p, bool is_xsk)\n{\n\tint pad, nfrags, ndescs, iovi, offset;\n\tstruct gve_tx_buffer_state *info;\n\tu32 reqi = tx->req;\n\n\tpad = gve_tx_fifo_pad_alloc_one_frag(&tx->tx_fifo, len);\n\tif (pad >= GVE_GQ_TX_MIN_PKT_DESC_BYTES)\n\t\tpad = 0;\n\tinfo = &tx->info[reqi & tx->mask];\n\tinfo->xdp_frame = frame_p;\n\tinfo->xdp.size = len;\n\tinfo->xdp.is_xsk = is_xsk;\n\n\tnfrags = gve_tx_alloc_fifo(&tx->tx_fifo, pad + len,\n\t\t\t\t   &info->iov[0]);\n\tiovi = pad > 0;\n\tndescs = nfrags - iovi;\n\toffset = 0;\n\n\twhile (iovi < nfrags) {\n\t\tif (!offset)\n\t\t\tgve_tx_fill_pkt_desc(&tx->desc[reqi & tx->mask], 0,\n\t\t\t\t\t     CHECKSUM_NONE, false, 0, ndescs,\n\t\t\t\t\t     info->iov[iovi].iov_len,\n\t\t\t\t\t     info->iov[iovi].iov_offset, len);\n\t\telse\n\t\t\tgve_tx_fill_seg_desc(&tx->desc[reqi & tx->mask],\n\t\t\t\t\t     0, 0, false, false,\n\t\t\t\t\t     info->iov[iovi].iov_len,\n\t\t\t\t\t     info->iov[iovi].iov_offset);\n\n\t\tmemcpy(tx->tx_fifo.base + info->iov[iovi].iov_offset,\n\t\t       data + offset, info->iov[iovi].iov_len);\n\t\tgve_dma_sync_for_device(&priv->pdev->dev,\n\t\t\t\t\ttx->tx_fifo.qpl->page_buses,\n\t\t\t\t\tinfo->iov[iovi].iov_offset,\n\t\t\t\t\tinfo->iov[iovi].iov_len);\n\t\toffset += info->iov[iovi].iov_len;\n\t\tiovi++;\n\t\treqi++;\n\t}\n\n\treturn ndescs;\n}\n\nint gve_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,\n\t\t u32 flags)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tstruct gve_tx_ring *tx;\n\tint i, err = 0, qid;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\tqid = gve_xdp_tx_queue_id(priv,\n\t\t\t\t  smp_processor_id() % priv->num_xdp_queues);\n\n\ttx = &priv->tx[qid];\n\n\tspin_lock(&tx->xdp_lock);\n\tfor (i = 0; i < n; i++) {\n\t\terr = gve_xdp_xmit_one(priv, tx, frames[i]->data,\n\t\t\t\t       frames[i]->len, frames[i]);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\tif (flags & XDP_XMIT_FLUSH)\n\t\tgve_tx_put_doorbell(priv, tx->q_resources, tx->req);\n\n\tspin_unlock(&tx->xdp_lock);\n\n\tu64_stats_update_begin(&tx->statss);\n\ttx->xdp_xmit += n;\n\ttx->xdp_xmit_errors += n - i;\n\tu64_stats_update_end(&tx->statss);\n\n\treturn i ? i : err;\n}\n\nint gve_xdp_xmit_one(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t     void *data, int len, void *frame_p)\n{\n\tint nsegs;\n\n\tif (!gve_can_tx(tx, len + GVE_GQ_TX_MIN_PKT_DESC_BYTES - 1))\n\t\treturn -EBUSY;\n\n\tnsegs = gve_tx_fill_xdp(priv, tx, data, len, frame_p, false);\n\ttx->req += nsegs;\n\n\treturn 0;\n}\n\n#define GVE_TX_START_THRESH\tPAGE_SIZE\n\nstatic int gve_clean_tx_done(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t\t     u32 to_do, bool try_to_wake)\n{\n\tstruct gve_tx_buffer_state *info;\n\tu64 pkts = 0, bytes = 0;\n\tsize_t space_freed = 0;\n\tstruct sk_buff *skb;\n\tu32 idx;\n\tint j;\n\n\tfor (j = 0; j < to_do; j++) {\n\t\tidx = tx->done & tx->mask;\n\t\tnetif_info(priv, tx_done, priv->dev,\n\t\t\t   \"[%d] %s: idx=%d (req=%u done=%u)\\n\",\n\t\t\t   tx->q_num, __func__, idx, tx->req, tx->done);\n\t\tinfo = &tx->info[idx];\n\t\tskb = info->skb;\n\n\t\t \n\t\tif (tx->raw_addressing)\n\t\t\tgve_tx_unmap_buf(tx->dev, info);\n\t\ttx->done++;\n\t\t \n\t\tif (skb) {\n\t\t\tinfo->skb = NULL;\n\t\t\tbytes += skb->len;\n\t\t\tpkts++;\n\t\t\tdev_consume_skb_any(skb);\n\t\t\tif (tx->raw_addressing)\n\t\t\t\tcontinue;\n\t\t\tspace_freed += gve_tx_clear_buffer_state(info);\n\t\t}\n\t}\n\n\tif (!tx->raw_addressing)\n\t\tgve_tx_free_fifo(&tx->tx_fifo, space_freed);\n\tu64_stats_update_begin(&tx->statss);\n\ttx->bytes_done += bytes;\n\ttx->pkt_done += pkts;\n\tu64_stats_update_end(&tx->statss);\n\tnetdev_tx_completed_queue(tx->netdev_txq, pkts, bytes);\n\n\t \n#ifndef CONFIG_BQL\n\t \n\tsmp_mb();\n#endif\n\tif (try_to_wake && netif_tx_queue_stopped(tx->netdev_txq) &&\n\t    likely(gve_can_tx(tx, GVE_TX_START_THRESH))) {\n\t\ttx->wake_queue++;\n\t\tnetif_tx_wake_queue(tx->netdev_txq);\n\t}\n\n\treturn pkts;\n}\n\nu32 gve_tx_load_event_counter(struct gve_priv *priv,\n\t\t\t      struct gve_tx_ring *tx)\n{\n\tu32 counter_index = be32_to_cpu(tx->q_resources->counter_index);\n\t__be32 counter = READ_ONCE(priv->counter_array[counter_index]);\n\n\treturn be32_to_cpu(counter);\n}\n\nstatic int gve_xsk_tx(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t      int budget)\n{\n\tstruct xdp_desc desc;\n\tint sent = 0, nsegs;\n\tvoid *data;\n\n\tspin_lock(&tx->xdp_lock);\n\twhile (sent < budget) {\n\t\tif (!gve_can_tx(tx, GVE_TX_START_THRESH))\n\t\t\tgoto out;\n\n\t\tif (!xsk_tx_peek_desc(tx->xsk_pool, &desc)) {\n\t\t\ttx->xdp_xsk_done = tx->xdp_xsk_wakeup;\n\t\t\tgoto out;\n\t\t}\n\n\t\tdata = xsk_buff_raw_get_data(tx->xsk_pool, desc.addr);\n\t\tnsegs = gve_tx_fill_xdp(priv, tx, data, desc.len, NULL, true);\n\t\ttx->req += nsegs;\n\t\tsent++;\n\t}\nout:\n\tif (sent > 0) {\n\t\tgve_tx_put_doorbell(priv, tx->q_resources, tx->req);\n\t\txsk_tx_release(tx->xsk_pool);\n\t}\n\tspin_unlock(&tx->xdp_lock);\n\treturn sent;\n}\n\nbool gve_xdp_poll(struct gve_notify_block *block, int budget)\n{\n\tstruct gve_priv *priv = block->priv;\n\tstruct gve_tx_ring *tx = block->tx;\n\tu32 nic_done;\n\tbool repoll;\n\tu32 to_do;\n\n\t \n\tnic_done = gve_tx_load_event_counter(priv, tx);\n\tto_do = min_t(u32, (nic_done - tx->done), budget);\n\tgve_clean_xdp_done(priv, tx, to_do);\n\trepoll = nic_done != tx->done;\n\n\tif (tx->xsk_pool) {\n\t\tint sent = gve_xsk_tx(priv, tx, budget);\n\n\t\tu64_stats_update_begin(&tx->statss);\n\t\ttx->xdp_xsk_sent += sent;\n\t\tu64_stats_update_end(&tx->statss);\n\t\trepoll |= (sent == budget);\n\t\tif (xsk_uses_need_wakeup(tx->xsk_pool))\n\t\t\txsk_set_tx_need_wakeup(tx->xsk_pool);\n\t}\n\n\t \n\treturn repoll;\n}\n\nbool gve_tx_poll(struct gve_notify_block *block, int budget)\n{\n\tstruct gve_priv *priv = block->priv;\n\tstruct gve_tx_ring *tx = block->tx;\n\tu32 nic_done;\n\tu32 to_do;\n\n\t \n\tif (budget == 0)\n\t\tbudget = INT_MAX;\n\n\t \n\tspin_lock(&tx->clean_lock);\n\t \n\tnic_done = gve_tx_load_event_counter(priv, tx);\n\tto_do = min_t(u32, (nic_done - tx->done), budget);\n\tgve_clean_tx_done(priv, tx, to_do, true);\n\tspin_unlock(&tx->clean_lock);\n\t \n\treturn nic_done != tx->done;\n}\n\nbool gve_tx_clean_pending(struct gve_priv *priv, struct gve_tx_ring *tx)\n{\n\tu32 nic_done = gve_tx_load_event_counter(priv, tx);\n\n\treturn nic_done != tx->done;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}