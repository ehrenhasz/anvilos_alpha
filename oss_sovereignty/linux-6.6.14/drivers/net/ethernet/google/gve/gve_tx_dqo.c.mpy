{
  "module_name": "gve_tx_dqo.c",
  "hash_id": "7fa56847bf63902f9d199aff86e9a114524853df32777b4a4fe43b90b951162e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/google/gve/gve_tx_dqo.c",
  "human_readable_source": "\n \n\n#include \"gve.h\"\n#include \"gve_adminq.h\"\n#include \"gve_utils.h\"\n#include \"gve_dqo.h\"\n#include <net/ip.h>\n#include <linux/tcp.h>\n#include <linux/slab.h>\n#include <linux/skbuff.h>\n\n \nstatic bool gve_has_free_tx_qpl_bufs(struct gve_tx_ring *tx, int count)\n{\n\tint num_avail;\n\n\tif (!tx->dqo.qpl)\n\t\treturn true;\n\n\tnum_avail = tx->dqo.num_tx_qpl_bufs -\n\t\t(tx->dqo_tx.alloc_tx_qpl_buf_cnt -\n\t\t tx->dqo_tx.free_tx_qpl_buf_cnt);\n\n\tif (count <= num_avail)\n\t\treturn true;\n\n\t \n\ttx->dqo_tx.free_tx_qpl_buf_cnt =\n\t\tatomic_read_acquire(&tx->dqo_compl.free_tx_qpl_buf_cnt);\n\n\tnum_avail = tx->dqo.num_tx_qpl_bufs -\n\t\t(tx->dqo_tx.alloc_tx_qpl_buf_cnt -\n\t\t tx->dqo_tx.free_tx_qpl_buf_cnt);\n\n\treturn count <= num_avail;\n}\n\nstatic s16\ngve_alloc_tx_qpl_buf(struct gve_tx_ring *tx)\n{\n\ts16 index;\n\n\tindex = tx->dqo_tx.free_tx_qpl_buf_head;\n\n\t \n\tif (unlikely(index == -1)) {\n\t\ttx->dqo_tx.free_tx_qpl_buf_head =\n\t\t\tatomic_xchg(&tx->dqo_compl.free_tx_qpl_buf_head, -1);\n\t\tindex = tx->dqo_tx.free_tx_qpl_buf_head;\n\n\t\tif (unlikely(index == -1))\n\t\t\treturn index;\n\t}\n\n\t \n\ttx->dqo_tx.free_tx_qpl_buf_head = tx->dqo.tx_qpl_buf_next[index];\n\n\treturn index;\n}\n\nstatic void\ngve_free_tx_qpl_bufs(struct gve_tx_ring *tx,\n\t\t     struct gve_tx_pending_packet_dqo *pkt)\n{\n\ts16 index;\n\tint i;\n\n\tif (!pkt->num_bufs)\n\t\treturn;\n\n\tindex = pkt->tx_qpl_buf_ids[0];\n\t \n\tfor (i = 1; i < pkt->num_bufs; i++) {\n\t\ttx->dqo.tx_qpl_buf_next[index] = pkt->tx_qpl_buf_ids[i];\n\t\tindex = pkt->tx_qpl_buf_ids[i];\n\t}\n\n\twhile (true) {\n\t\ts16 old_head = atomic_read_acquire(&tx->dqo_compl.free_tx_qpl_buf_head);\n\n\t\ttx->dqo.tx_qpl_buf_next[index] = old_head;\n\t\tif (atomic_cmpxchg(&tx->dqo_compl.free_tx_qpl_buf_head,\n\t\t\t\t   old_head,\n\t\t\t\t   pkt->tx_qpl_buf_ids[0]) == old_head) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tatomic_add(pkt->num_bufs, &tx->dqo_compl.free_tx_qpl_buf_cnt);\n\tpkt->num_bufs = 0;\n}\n\n \nstatic bool gve_has_pending_packet(struct gve_tx_ring *tx)\n{\n\t \n\tif (tx->dqo_tx.free_pending_packets != -1)\n\t\treturn true;\n\n\t \n\tif (atomic_read_acquire(&tx->dqo_compl.free_pending_packets) != -1)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic struct gve_tx_pending_packet_dqo *\ngve_alloc_pending_packet(struct gve_tx_ring *tx)\n{\n\tstruct gve_tx_pending_packet_dqo *pending_packet;\n\ts16 index;\n\n\tindex = tx->dqo_tx.free_pending_packets;\n\n\t \n\tif (unlikely(index == -1)) {\n\t\ttx->dqo_tx.free_pending_packets =\n\t\t\tatomic_xchg(&tx->dqo_compl.free_pending_packets, -1);\n\t\tindex = tx->dqo_tx.free_pending_packets;\n\n\t\tif (unlikely(index == -1))\n\t\t\treturn NULL;\n\t}\n\n\tpending_packet = &tx->dqo.pending_packets[index];\n\n\t \n\ttx->dqo_tx.free_pending_packets = pending_packet->next;\n\tpending_packet->state = GVE_PACKET_STATE_PENDING_DATA_COMPL;\n\n\treturn pending_packet;\n}\n\nstatic void\ngve_free_pending_packet(struct gve_tx_ring *tx,\n\t\t\tstruct gve_tx_pending_packet_dqo *pending_packet)\n{\n\ts16 index = pending_packet - tx->dqo.pending_packets;\n\n\tpending_packet->state = GVE_PACKET_STATE_UNALLOCATED;\n\twhile (true) {\n\t\ts16 old_head = atomic_read_acquire(&tx->dqo_compl.free_pending_packets);\n\n\t\tpending_packet->next = old_head;\n\t\tif (atomic_cmpxchg(&tx->dqo_compl.free_pending_packets,\n\t\t\t\t   old_head, index) == old_head) {\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic void gve_tx_clean_pending_packets(struct gve_tx_ring *tx)\n{\n\tint i;\n\n\tfor (i = 0; i < tx->dqo.num_pending_packets; i++) {\n\t\tstruct gve_tx_pending_packet_dqo *cur_state =\n\t\t\t&tx->dqo.pending_packets[i];\n\t\tint j;\n\n\t\tfor (j = 0; j < cur_state->num_bufs; j++) {\n\t\t\tif (j == 0) {\n\t\t\t\tdma_unmap_single(tx->dev,\n\t\t\t\t\tdma_unmap_addr(cur_state, dma[j]),\n\t\t\t\t\tdma_unmap_len(cur_state, len[j]),\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\t} else {\n\t\t\t\tdma_unmap_page(tx->dev,\n\t\t\t\t\tdma_unmap_addr(cur_state, dma[j]),\n\t\t\t\t\tdma_unmap_len(cur_state, len[j]),\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\t}\n\t\t}\n\t\tif (cur_state->skb) {\n\t\t\tdev_consume_skb_any(cur_state->skb);\n\t\t\tcur_state->skb = NULL;\n\t\t}\n\t}\n}\n\nstatic void gve_tx_free_ring_dqo(struct gve_priv *priv, int idx)\n{\n\tstruct gve_tx_ring *tx = &priv->tx[idx];\n\tstruct device *hdev = &priv->pdev->dev;\n\tsize_t bytes;\n\n\tgve_tx_remove_from_block(priv, idx);\n\n\tif (tx->q_resources) {\n\t\tdma_free_coherent(hdev, sizeof(*tx->q_resources),\n\t\t\t\t  tx->q_resources, tx->q_resources_bus);\n\t\ttx->q_resources = NULL;\n\t}\n\n\tif (tx->dqo.compl_ring) {\n\t\tbytes = sizeof(tx->dqo.compl_ring[0]) *\n\t\t\t(tx->dqo.complq_mask + 1);\n\t\tdma_free_coherent(hdev, bytes, tx->dqo.compl_ring,\n\t\t\t\t  tx->complq_bus_dqo);\n\t\ttx->dqo.compl_ring = NULL;\n\t}\n\n\tif (tx->dqo.tx_ring) {\n\t\tbytes = sizeof(tx->dqo.tx_ring[0]) * (tx->mask + 1);\n\t\tdma_free_coherent(hdev, bytes, tx->dqo.tx_ring, tx->bus);\n\t\ttx->dqo.tx_ring = NULL;\n\t}\n\n\tkvfree(tx->dqo.pending_packets);\n\ttx->dqo.pending_packets = NULL;\n\n\tkvfree(tx->dqo.tx_qpl_buf_next);\n\ttx->dqo.tx_qpl_buf_next = NULL;\n\n\tif (tx->dqo.qpl) {\n\t\tgve_unassign_qpl(priv, tx->dqo.qpl->id);\n\t\ttx->dqo.qpl = NULL;\n\t}\n\n\tnetif_dbg(priv, drv, priv->dev, \"freed tx queue %d\\n\", idx);\n}\n\nstatic int gve_tx_qpl_buf_init(struct gve_tx_ring *tx)\n{\n\tint num_tx_qpl_bufs = GVE_TX_BUFS_PER_PAGE_DQO *\n\t\ttx->dqo.qpl->num_entries;\n\tint i;\n\n\ttx->dqo.tx_qpl_buf_next = kvcalloc(num_tx_qpl_bufs,\n\t\t\t\t\t   sizeof(tx->dqo.tx_qpl_buf_next[0]),\n\t\t\t\t\t   GFP_KERNEL);\n\tif (!tx->dqo.tx_qpl_buf_next)\n\t\treturn -ENOMEM;\n\n\ttx->dqo.num_tx_qpl_bufs = num_tx_qpl_bufs;\n\n\t \n\tfor (i = 0; i < num_tx_qpl_bufs - 1; i++)\n\t\ttx->dqo.tx_qpl_buf_next[i] = i + 1;\n\ttx->dqo.tx_qpl_buf_next[num_tx_qpl_bufs - 1] = -1;\n\n\tatomic_set_release(&tx->dqo_compl.free_tx_qpl_buf_head, -1);\n\treturn 0;\n}\n\nstatic int gve_tx_alloc_ring_dqo(struct gve_priv *priv, int idx)\n{\n\tstruct gve_tx_ring *tx = &priv->tx[idx];\n\tstruct device *hdev = &priv->pdev->dev;\n\tint num_pending_packets;\n\tsize_t bytes;\n\tint i;\n\n\tmemset(tx, 0, sizeof(*tx));\n\ttx->q_num = idx;\n\ttx->dev = &priv->pdev->dev;\n\ttx->netdev_txq = netdev_get_tx_queue(priv->dev, idx);\n\tatomic_set_release(&tx->dqo_compl.hw_tx_head, 0);\n\n\t \n\ttx->mask = priv->tx_desc_cnt - 1;\n\ttx->dqo.complq_mask = priv->queue_format == GVE_DQO_RDA_FORMAT ?\n\t\tpriv->options_dqo_rda.tx_comp_ring_entries - 1 :\n\t\ttx->mask;\n\n\t \n\tnum_pending_packets = tx->dqo.complq_mask + 1;\n\n\t \n\tnum_pending_packets -=\n\t\t(tx->dqo.complq_mask + 1) / GVE_TX_MIN_RE_INTERVAL;\n\n\t \n\tnum_pending_packets /= 2;\n\n\ttx->dqo.num_pending_packets = min_t(int, num_pending_packets, S16_MAX);\n\ttx->dqo.pending_packets = kvcalloc(tx->dqo.num_pending_packets,\n\t\t\t\t\t   sizeof(tx->dqo.pending_packets[0]),\n\t\t\t\t\t   GFP_KERNEL);\n\tif (!tx->dqo.pending_packets)\n\t\tgoto err;\n\n\t \n\tfor (i = 0; i < tx->dqo.num_pending_packets - 1; i++)\n\t\ttx->dqo.pending_packets[i].next = i + 1;\n\n\ttx->dqo.pending_packets[tx->dqo.num_pending_packets - 1].next = -1;\n\tatomic_set_release(&tx->dqo_compl.free_pending_packets, -1);\n\ttx->dqo_compl.miss_completions.head = -1;\n\ttx->dqo_compl.miss_completions.tail = -1;\n\ttx->dqo_compl.timed_out_completions.head = -1;\n\ttx->dqo_compl.timed_out_completions.tail = -1;\n\n\tbytes = sizeof(tx->dqo.tx_ring[0]) * (tx->mask + 1);\n\ttx->dqo.tx_ring = dma_alloc_coherent(hdev, bytes, &tx->bus, GFP_KERNEL);\n\tif (!tx->dqo.tx_ring)\n\t\tgoto err;\n\n\tbytes = sizeof(tx->dqo.compl_ring[0]) * (tx->dqo.complq_mask + 1);\n\ttx->dqo.compl_ring = dma_alloc_coherent(hdev, bytes,\n\t\t\t\t\t\t&tx->complq_bus_dqo,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!tx->dqo.compl_ring)\n\t\tgoto err;\n\n\ttx->q_resources = dma_alloc_coherent(hdev, sizeof(*tx->q_resources),\n\t\t\t\t\t     &tx->q_resources_bus, GFP_KERNEL);\n\tif (!tx->q_resources)\n\t\tgoto err;\n\n\tif (gve_is_qpl(priv)) {\n\t\ttx->dqo.qpl = gve_assign_tx_qpl(priv, idx);\n\t\tif (!tx->dqo.qpl)\n\t\t\tgoto err;\n\n\t\tif (gve_tx_qpl_buf_init(tx))\n\t\t\tgoto err;\n\t}\n\n\tgve_tx_add_to_block(priv, idx);\n\n\treturn 0;\n\nerr:\n\tgve_tx_free_ring_dqo(priv, idx);\n\treturn -ENOMEM;\n}\n\nint gve_tx_alloc_rings_dqo(struct gve_priv *priv)\n{\n\tint err = 0;\n\tint i;\n\n\tfor (i = 0; i < priv->tx_cfg.num_queues; i++) {\n\t\terr = gve_tx_alloc_ring_dqo(priv, i);\n\t\tif (err) {\n\t\t\tnetif_err(priv, drv, priv->dev,\n\t\t\t\t  \"Failed to alloc tx ring=%d: err=%d\\n\",\n\t\t\t\t  i, err);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (i--; i >= 0; i--)\n\t\tgve_tx_free_ring_dqo(priv, i);\n\n\treturn err;\n}\n\nvoid gve_tx_free_rings_dqo(struct gve_priv *priv)\n{\n\tint i;\n\n\tfor (i = 0; i < priv->tx_cfg.num_queues; i++) {\n\t\tstruct gve_tx_ring *tx = &priv->tx[i];\n\n\t\tgve_clean_tx_done_dqo(priv, tx,  NULL);\n\t\tnetdev_tx_reset_queue(tx->netdev_txq);\n\t\tgve_tx_clean_pending_packets(tx);\n\n\t\tgve_tx_free_ring_dqo(priv, i);\n\t}\n}\n\n \nstatic u32 num_avail_tx_slots(const struct gve_tx_ring *tx)\n{\n\tu32 num_used = (tx->dqo_tx.tail - tx->dqo_tx.head) & tx->mask;\n\n\treturn tx->mask - num_used;\n}\n\nstatic bool gve_has_avail_slots_tx_dqo(struct gve_tx_ring *tx,\n\t\t\t\t       int desc_count, int buf_count)\n{\n\treturn gve_has_pending_packet(tx) &&\n\t\t   num_avail_tx_slots(tx) >= desc_count &&\n\t\t   gve_has_free_tx_qpl_bufs(tx, buf_count);\n}\n\n \nstatic int gve_maybe_stop_tx_dqo(struct gve_tx_ring *tx,\n\t\t\t\t int desc_count, int buf_count)\n{\n\tif (likely(gve_has_avail_slots_tx_dqo(tx, desc_count, buf_count)))\n\t\treturn 0;\n\n\t \n\ttx->dqo_tx.head = atomic_read_acquire(&tx->dqo_compl.hw_tx_head);\n\n\tif (likely(gve_has_avail_slots_tx_dqo(tx, desc_count, buf_count)))\n\t\treturn 0;\n\n\t \n\ttx->stop_queue++;\n\tnetif_tx_stop_queue(tx->netdev_txq);\n\n\t \n\tmb();\n\n\t \n\ttx->dqo_tx.head = atomic_read_acquire(&tx->dqo_compl.hw_tx_head);\n\n\tif (likely(!gve_has_avail_slots_tx_dqo(tx, desc_count, buf_count)))\n\t\treturn -EBUSY;\n\n\tnetif_tx_start_queue(tx->netdev_txq);\n\ttx->wake_queue++;\n\treturn 0;\n}\n\nstatic void gve_extract_tx_metadata_dqo(const struct sk_buff *skb,\n\t\t\t\t\tstruct gve_tx_metadata_dqo *metadata)\n{\n\tmemset(metadata, 0, sizeof(*metadata));\n\tmetadata->version = GVE_TX_METADATA_VERSION_DQO;\n\n\tif (skb->l4_hash) {\n\t\tu16 path_hash = skb->hash ^ (skb->hash >> 16);\n\n\t\tpath_hash &= (1 << 15) - 1;\n\t\tif (unlikely(path_hash == 0))\n\t\t\tpath_hash = ~path_hash;\n\n\t\tmetadata->path_hash = path_hash;\n\t}\n}\n\nstatic void gve_tx_fill_pkt_desc_dqo(struct gve_tx_ring *tx, u32 *desc_idx,\n\t\t\t\t     struct sk_buff *skb, u32 len, u64 addr,\n\t\t\t\t     s16 compl_tag, bool eop, bool is_gso)\n{\n\tconst bool checksum_offload_en = skb->ip_summed == CHECKSUM_PARTIAL;\n\n\twhile (len > 0) {\n\t\tstruct gve_tx_pkt_desc_dqo *desc =\n\t\t\t&tx->dqo.tx_ring[*desc_idx].pkt;\n\t\tu32 cur_len = min_t(u32, len, GVE_TX_MAX_BUF_SIZE_DQO);\n\t\tbool cur_eop = eop && cur_len == len;\n\n\t\t*desc = (struct gve_tx_pkt_desc_dqo){\n\t\t\t.buf_addr = cpu_to_le64(addr),\n\t\t\t.dtype = GVE_TX_PKT_DESC_DTYPE_DQO,\n\t\t\t.end_of_packet = cur_eop,\n\t\t\t.checksum_offload_enable = checksum_offload_en,\n\t\t\t.compl_tag = cpu_to_le16(compl_tag),\n\t\t\t.buf_size = cur_len,\n\t\t};\n\n\t\taddr += cur_len;\n\t\tlen -= cur_len;\n\t\t*desc_idx = (*desc_idx + 1) & tx->mask;\n\t}\n}\n\n \nstatic int gve_prep_tso(struct sk_buff *skb)\n{\n\tstruct tcphdr *tcp;\n\tint header_len;\n\tu32 paylen;\n\tint err;\n\n\t \n\n\tif (unlikely(skb_shinfo(skb)->gso_size < GVE_TX_MIN_TSO_MSS_DQO))\n\t\treturn -1;\n\n\t \n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\ttcp = tcp_hdr(skb);\n\n\t \n\tpaylen = skb->len - skb_transport_offset(skb);\n\n\tswitch (skb_shinfo(skb)->gso_type) {\n\tcase SKB_GSO_TCPV4:\n\tcase SKB_GSO_TCPV6:\n\t\tcsum_replace_by_diff(&tcp->check,\n\t\t\t\t     (__force __wsum)htonl(paylen));\n\n\t\t \n\t\theader_len = skb_tcp_all_headers(skb);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely(header_len > GVE_TX_MAX_HDR_SIZE_DQO))\n\t\treturn -EINVAL;\n\n\treturn header_len;\n}\n\nstatic void gve_tx_fill_tso_ctx_desc(struct gve_tx_tso_context_desc_dqo *desc,\n\t\t\t\t     const struct sk_buff *skb,\n\t\t\t\t     const struct gve_tx_metadata_dqo *metadata,\n\t\t\t\t     int header_len)\n{\n\t*desc = (struct gve_tx_tso_context_desc_dqo){\n\t\t.header_len = header_len,\n\t\t.cmd_dtype = {\n\t\t\t.dtype = GVE_TX_TSO_CTX_DESC_DTYPE_DQO,\n\t\t\t.tso = 1,\n\t\t},\n\t\t.flex0 = metadata->bytes[0],\n\t\t.flex5 = metadata->bytes[5],\n\t\t.flex6 = metadata->bytes[6],\n\t\t.flex7 = metadata->bytes[7],\n\t\t.flex8 = metadata->bytes[8],\n\t\t.flex9 = metadata->bytes[9],\n\t\t.flex10 = metadata->bytes[10],\n\t\t.flex11 = metadata->bytes[11],\n\t};\n\tdesc->tso_total_len = skb->len - header_len;\n\tdesc->mss = skb_shinfo(skb)->gso_size;\n}\n\nstatic void\ngve_tx_fill_general_ctx_desc(struct gve_tx_general_context_desc_dqo *desc,\n\t\t\t     const struct gve_tx_metadata_dqo *metadata)\n{\n\t*desc = (struct gve_tx_general_context_desc_dqo){\n\t\t.flex0 = metadata->bytes[0],\n\t\t.flex1 = metadata->bytes[1],\n\t\t.flex2 = metadata->bytes[2],\n\t\t.flex3 = metadata->bytes[3],\n\t\t.flex4 = metadata->bytes[4],\n\t\t.flex5 = metadata->bytes[5],\n\t\t.flex6 = metadata->bytes[6],\n\t\t.flex7 = metadata->bytes[7],\n\t\t.flex8 = metadata->bytes[8],\n\t\t.flex9 = metadata->bytes[9],\n\t\t.flex10 = metadata->bytes[10],\n\t\t.flex11 = metadata->bytes[11],\n\t\t.cmd_dtype = {.dtype = GVE_TX_GENERAL_CTX_DESC_DTYPE_DQO},\n\t};\n}\n\nstatic int gve_tx_add_skb_no_copy_dqo(struct gve_tx_ring *tx,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      struct gve_tx_pending_packet_dqo *pkt,\n\t\t\t\t      s16 completion_tag,\n\t\t\t\t      u32 *desc_idx,\n\t\t\t\t      bool is_gso)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint i;\n\n\t \n\n\tpkt->num_bufs = 0;\n\t \n\t{\n\t\tu32 len = skb_headlen(skb);\n\t\tdma_addr_t addr;\n\n\t\taddr = dma_map_single(tx->dev, skb->data, len, DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(tx->dev, addr)))\n\t\t\tgoto err;\n\n\t\tdma_unmap_len_set(pkt, len[pkt->num_bufs], len);\n\t\tdma_unmap_addr_set(pkt, dma[pkt->num_bufs], addr);\n\t\t++pkt->num_bufs;\n\n\t\tgve_tx_fill_pkt_desc_dqo(tx, desc_idx, skb, len, addr,\n\t\t\t\t\t completion_tag,\n\t\t\t\t\t  shinfo->nr_frags == 0, is_gso);\n\t}\n\n\tfor (i = 0; i < shinfo->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &shinfo->frags[i];\n\t\tbool is_eop = i == (shinfo->nr_frags - 1);\n\t\tu32 len = skb_frag_size(frag);\n\t\tdma_addr_t addr;\n\n\t\taddr = skb_frag_dma_map(tx->dev, frag, 0, len, DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(tx->dev, addr)))\n\t\t\tgoto err;\n\n\t\tdma_unmap_len_set(pkt, len[pkt->num_bufs], len);\n\t\tdma_unmap_addr_set(pkt, dma[pkt->num_bufs], addr);\n\t\t++pkt->num_bufs;\n\n\t\tgve_tx_fill_pkt_desc_dqo(tx, desc_idx, skb, len, addr,\n\t\t\t\t\t completion_tag, is_eop, is_gso);\n\t}\n\n\treturn 0;\nerr:\n\tfor (i = 0; i < pkt->num_bufs; i++) {\n\t\tif (i == 0) {\n\t\t\tdma_unmap_single(tx->dev,\n\t\t\t\t\t dma_unmap_addr(pkt, dma[i]),\n\t\t\t\t\t dma_unmap_len(pkt, len[i]),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t} else {\n\t\t\tdma_unmap_page(tx->dev,\n\t\t\t\t       dma_unmap_addr(pkt, dma[i]),\n\t\t\t\t       dma_unmap_len(pkt, len[i]),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\t}\n\t}\n\tpkt->num_bufs = 0;\n\treturn -1;\n}\n\n \nstatic void gve_tx_buf_get_addr(struct gve_tx_ring *tx,\n\t\t\t\ts16 index,\n\t\t\t\tvoid **va, dma_addr_t *dma_addr)\n{\n\tint page_id = index >> (PAGE_SHIFT - GVE_TX_BUF_SHIFT_DQO);\n\tint offset = (index & (GVE_TX_BUFS_PER_PAGE_DQO - 1)) << GVE_TX_BUF_SHIFT_DQO;\n\n\t*va = page_address(tx->dqo.qpl->pages[page_id]) + offset;\n\t*dma_addr = tx->dqo.qpl->page_buses[page_id] + offset;\n}\n\nstatic int gve_tx_add_skb_copy_dqo(struct gve_tx_ring *tx,\n\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t   struct gve_tx_pending_packet_dqo *pkt,\n\t\t\t\t   s16 completion_tag,\n\t\t\t\t   u32 *desc_idx,\n\t\t\t\t   bool is_gso)\n{\n\tu32 copy_offset = 0;\n\tdma_addr_t dma_addr;\n\tu32 copy_len;\n\ts16 index;\n\tvoid *va;\n\n\t \n\tpkt->num_bufs = 0;\n\twhile (copy_offset < skb->len) {\n\t\tindex = gve_alloc_tx_qpl_buf(tx);\n\t\tif (unlikely(index == -1))\n\t\t\tgoto err;\n\n\t\tgve_tx_buf_get_addr(tx, index, &va, &dma_addr);\n\t\tcopy_len = min_t(u32, GVE_TX_BUF_SIZE_DQO,\n\t\t\t\t skb->len - copy_offset);\n\t\tskb_copy_bits(skb, copy_offset, va, copy_len);\n\n\t\tcopy_offset += copy_len;\n\t\tdma_sync_single_for_device(tx->dev, dma_addr,\n\t\t\t\t\t   copy_len, DMA_TO_DEVICE);\n\t\tgve_tx_fill_pkt_desc_dqo(tx, desc_idx, skb,\n\t\t\t\t\t copy_len,\n\t\t\t\t\t dma_addr,\n\t\t\t\t\t completion_tag,\n\t\t\t\t\t copy_offset == skb->len,\n\t\t\t\t\t is_gso);\n\n\t\tpkt->tx_qpl_buf_ids[pkt->num_bufs] = index;\n\t\t++tx->dqo_tx.alloc_tx_qpl_buf_cnt;\n\t\t++pkt->num_bufs;\n\t}\n\n\treturn 0;\nerr:\n\t \n\tgve_free_tx_qpl_bufs(tx, pkt);\n\treturn -ENOMEM;\n}\n\n \nstatic int gve_tx_add_skb_dqo(struct gve_tx_ring *tx,\n\t\t\t      struct sk_buff *skb)\n{\n\tconst bool is_gso = skb_is_gso(skb);\n\tu32 desc_idx = tx->dqo_tx.tail;\n\tstruct gve_tx_pending_packet_dqo *pkt;\n\tstruct gve_tx_metadata_dqo metadata;\n\ts16 completion_tag;\n\n\tpkt = gve_alloc_pending_packet(tx);\n\tpkt->skb = skb;\n\tcompletion_tag = pkt - tx->dqo.pending_packets;\n\n\tgve_extract_tx_metadata_dqo(skb, &metadata);\n\tif (is_gso) {\n\t\tint header_len = gve_prep_tso(skb);\n\n\t\tif (unlikely(header_len < 0))\n\t\t\tgoto err;\n\n\t\tgve_tx_fill_tso_ctx_desc(&tx->dqo.tx_ring[desc_idx].tso_ctx,\n\t\t\t\t\t skb, &metadata, header_len);\n\t\tdesc_idx = (desc_idx + 1) & tx->mask;\n\t}\n\n\tgve_tx_fill_general_ctx_desc(&tx->dqo.tx_ring[desc_idx].general_ctx,\n\t\t\t\t     &metadata);\n\tdesc_idx = (desc_idx + 1) & tx->mask;\n\n\tif (tx->dqo.qpl) {\n\t\tif (gve_tx_add_skb_copy_dqo(tx, skb, pkt,\n\t\t\t\t\t    completion_tag,\n\t\t\t\t\t    &desc_idx, is_gso))\n\t\t\tgoto err;\n\t}  else {\n\t\tif (gve_tx_add_skb_no_copy_dqo(tx, skb, pkt,\n\t\t\t\t\t       completion_tag,\n\t\t\t\t\t       &desc_idx, is_gso))\n\t\t\tgoto err;\n\t}\n\n\ttx->dqo_tx.posted_packet_desc_cnt += pkt->num_bufs;\n\n\t \n\ttx->dqo_tx.tail = desc_idx;\n\n\t \n\t{\n\t\tu32 last_desc_idx = (desc_idx - 1) & tx->mask;\n\t\tu32 last_report_event_interval =\n\t\t\t(last_desc_idx - tx->dqo_tx.last_re_idx) & tx->mask;\n\n\t\tif (unlikely(last_report_event_interval >=\n\t\t\t     GVE_TX_MIN_RE_INTERVAL)) {\n\t\t\ttx->dqo.tx_ring[last_desc_idx].pkt.report_event = true;\n\t\t\ttx->dqo_tx.last_re_idx = last_desc_idx;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tpkt->skb = NULL;\n\tgve_free_pending_packet(tx, pkt);\n\n\treturn -1;\n}\n\nstatic int gve_num_descs_per_buf(size_t size)\n{\n\treturn DIV_ROUND_UP(size, GVE_TX_MAX_BUF_SIZE_DQO);\n}\n\nstatic int gve_num_buffer_descs_needed(const struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint num_descs;\n\tint i;\n\n\tnum_descs = gve_num_descs_per_buf(skb_headlen(skb));\n\n\tfor (i = 0; i < shinfo->nr_frags; i++) {\n\t\tunsigned int frag_size = skb_frag_size(&shinfo->frags[i]);\n\n\t\tnum_descs += gve_num_descs_per_buf(frag_size);\n\t}\n\n\treturn num_descs;\n}\n\n \nstatic bool gve_can_send_tso(const struct sk_buff *skb)\n{\n\tconst int max_bufs_per_seg = GVE_TX_MAX_DATA_DESCS - 1;\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\tconst int header_len = skb_tcp_all_headers(skb);\n\tconst int gso_size = shinfo->gso_size;\n\tint cur_seg_num_bufs;\n\tint cur_seg_size;\n\tint i;\n\n\tcur_seg_size = skb_headlen(skb) - header_len;\n\tcur_seg_num_bufs = cur_seg_size > 0;\n\n\tfor (i = 0; i < shinfo->nr_frags; i++) {\n\t\tif (cur_seg_size >= gso_size) {\n\t\t\tcur_seg_size %= gso_size;\n\t\t\tcur_seg_num_bufs = cur_seg_size > 0;\n\t\t}\n\n\t\tif (unlikely(++cur_seg_num_bufs > max_bufs_per_seg))\n\t\t\treturn false;\n\n\t\tcur_seg_size += skb_frag_size(&shinfo->frags[i]);\n\t}\n\n\treturn true;\n}\n\n \nstatic int gve_try_tx_skb(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t\t  struct sk_buff *skb)\n{\n\tint num_buffer_descs;\n\tint total_num_descs;\n\n\tif (tx->dqo.qpl) {\n\t\tif (skb_is_gso(skb))\n\t\t\tif (unlikely(ipv6_hopopt_jumbo_remove(skb)))\n\t\t\t\tgoto drop;\n\n\t\t \n\t\tnum_buffer_descs = DIV_ROUND_UP(skb->len, GVE_TX_BUF_SIZE_DQO);\n\t} else {\n\t\tif (skb_is_gso(skb)) {\n\t\t\t \n\t\t\tif (unlikely(!gve_can_send_tso(skb) &&\n\t\t\t\t     skb_linearize(skb) < 0)) {\n\t\t\t\tnet_err_ratelimited(\"%s: Failed to transmit TSO packet\\n\",\n\t\t\t\t\t\t    priv->dev->name);\n\t\t\t\tgoto drop;\n\t\t\t}\n\n\t\t\tif (unlikely(ipv6_hopopt_jumbo_remove(skb)))\n\t\t\t\tgoto drop;\n\n\t\t\tnum_buffer_descs = gve_num_buffer_descs_needed(skb);\n\t\t} else {\n\t\t\tnum_buffer_descs = gve_num_buffer_descs_needed(skb);\n\n\t\t\tif (unlikely(num_buffer_descs > GVE_TX_MAX_DATA_DESCS)) {\n\t\t\t\tif (unlikely(skb_linearize(skb) < 0))\n\t\t\t\t\tgoto drop;\n\n\t\t\t\tnum_buffer_descs = 1;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\ttotal_num_descs = 1 + skb_is_gso(skb) + num_buffer_descs;\n\tif (unlikely(gve_maybe_stop_tx_dqo(tx, total_num_descs +\n\t\t\tGVE_TX_MIN_DESC_PREVENT_CACHE_OVERLAP,\n\t\t\tnum_buffer_descs))) {\n\t\treturn -1;\n\t}\n\n\tif (unlikely(gve_tx_add_skb_dqo(tx, skb) < 0))\n\t\tgoto drop;\n\n\tnetdev_tx_sent_queue(tx->netdev_txq, skb->len);\n\tskb_tx_timestamp(skb);\n\treturn 0;\n\ndrop:\n\ttx->dropped_pkt++;\n\tdev_kfree_skb_any(skb);\n\treturn 0;\n}\n\n \nnetdev_tx_t gve_tx_dqo(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct gve_priv *priv = netdev_priv(dev);\n\tstruct gve_tx_ring *tx;\n\n\ttx = &priv->tx[skb_get_queue_mapping(skb)];\n\tif (unlikely(gve_try_tx_skb(priv, tx, skb) < 0)) {\n\t\t \n\t\tgve_tx_put_doorbell_dqo(priv, tx->q_resources, tx->dqo_tx.tail);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (!netif_xmit_stopped(tx->netdev_txq) && netdev_xmit_more())\n\t\treturn NETDEV_TX_OK;\n\n\tgve_tx_put_doorbell_dqo(priv, tx->q_resources, tx->dqo_tx.tail);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void add_to_list(struct gve_tx_ring *tx, struct gve_index_list *list,\n\t\t\tstruct gve_tx_pending_packet_dqo *pending_packet)\n{\n\ts16 old_tail, index;\n\n\tindex = pending_packet - tx->dqo.pending_packets;\n\told_tail = list->tail;\n\tlist->tail = index;\n\tif (old_tail == -1)\n\t\tlist->head = index;\n\telse\n\t\ttx->dqo.pending_packets[old_tail].next = index;\n\n\tpending_packet->next = -1;\n\tpending_packet->prev = old_tail;\n}\n\nstatic void remove_from_list(struct gve_tx_ring *tx,\n\t\t\t     struct gve_index_list *list,\n\t\t\t     struct gve_tx_pending_packet_dqo *pkt)\n{\n\ts16 prev_index, next_index;\n\n\tprev_index = pkt->prev;\n\tnext_index = pkt->next;\n\n\tif (prev_index == -1) {\n\t\t \n\t\tlist->head = next_index;\n\t} else {\n\t\ttx->dqo.pending_packets[prev_index].next = next_index;\n\t}\n\tif (next_index == -1) {\n\t\t \n\t\tlist->tail = prev_index;\n\t} else {\n\t\ttx->dqo.pending_packets[next_index].prev = prev_index;\n\t}\n}\n\nstatic void gve_unmap_packet(struct device *dev,\n\t\t\t     struct gve_tx_pending_packet_dqo *pkt)\n{\n\tint i;\n\n\t \n\tdma_unmap_single(dev, dma_unmap_addr(pkt, dma[0]),\n\t\t\t dma_unmap_len(pkt, len[0]), DMA_TO_DEVICE);\n\tfor (i = 1; i < pkt->num_bufs; i++) {\n\t\tdma_unmap_page(dev, dma_unmap_addr(pkt, dma[i]),\n\t\t\t       dma_unmap_len(pkt, len[i]), DMA_TO_DEVICE);\n\t}\n\tpkt->num_bufs = 0;\n}\n\n \nstatic void gve_handle_packet_completion(struct gve_priv *priv,\n\t\t\t\t\t struct gve_tx_ring *tx, bool is_napi,\n\t\t\t\t\t u16 compl_tag, u64 *bytes, u64 *pkts,\n\t\t\t\t\t bool is_reinjection)\n{\n\tstruct gve_tx_pending_packet_dqo *pending_packet;\n\n\tif (unlikely(compl_tag >= tx->dqo.num_pending_packets)) {\n\t\tnet_err_ratelimited(\"%s: Invalid TX completion tag: %d\\n\",\n\t\t\t\t    priv->dev->name, (int)compl_tag);\n\t\treturn;\n\t}\n\n\tpending_packet = &tx->dqo.pending_packets[compl_tag];\n\n\tif (unlikely(is_reinjection)) {\n\t\tif (unlikely(pending_packet->state ==\n\t\t\t     GVE_PACKET_STATE_TIMED_OUT_COMPL)) {\n\t\t\tnet_err_ratelimited(\"%s: Re-injection completion: %d received after timeout.\\n\",\n\t\t\t\t\t    priv->dev->name, (int)compl_tag);\n\t\t\t \n\t\t\tremove_from_list(tx,\n\t\t\t\t\t &tx->dqo_compl.timed_out_completions,\n\t\t\t\t\t pending_packet);\n\t\t\tgve_free_pending_packet(tx, pending_packet);\n\t\t\treturn;\n\t\t}\n\t\tif (unlikely(pending_packet->state !=\n\t\t\t     GVE_PACKET_STATE_PENDING_REINJECT_COMPL)) {\n\t\t\t \n\t\t\tnet_err_ratelimited(\"%s: Re-injection completion received without corresponding miss completion: %d\\n\",\n\t\t\t\t\t    priv->dev->name, (int)compl_tag);\n\t\t\treturn;\n\t\t}\n\t\tremove_from_list(tx, &tx->dqo_compl.miss_completions,\n\t\t\t\t pending_packet);\n\t} else {\n\t\t \n\t\tif (unlikely(pending_packet->state !=\n\t\t\t     GVE_PACKET_STATE_PENDING_DATA_COMPL)) {\n\t\t\tnet_err_ratelimited(\"%s: No pending data completion: %d\\n\",\n\t\t\t\t\t    priv->dev->name, (int)compl_tag);\n\t\t\treturn;\n\t\t}\n\t}\n\ttx->dqo_tx.completed_packet_desc_cnt += pending_packet->num_bufs;\n\tif (tx->dqo.qpl)\n\t\tgve_free_tx_qpl_bufs(tx, pending_packet);\n\telse\n\t\tgve_unmap_packet(tx->dev, pending_packet);\n\n\t*bytes += pending_packet->skb->len;\n\t(*pkts)++;\n\tnapi_consume_skb(pending_packet->skb, is_napi);\n\tpending_packet->skb = NULL;\n\tgve_free_pending_packet(tx, pending_packet);\n}\n\nstatic void gve_handle_miss_completion(struct gve_priv *priv,\n\t\t\t\t       struct gve_tx_ring *tx, u16 compl_tag,\n\t\t\t\t       u64 *bytes, u64 *pkts)\n{\n\tstruct gve_tx_pending_packet_dqo *pending_packet;\n\n\tif (unlikely(compl_tag >= tx->dqo.num_pending_packets)) {\n\t\tnet_err_ratelimited(\"%s: Invalid TX completion tag: %d\\n\",\n\t\t\t\t    priv->dev->name, (int)compl_tag);\n\t\treturn;\n\t}\n\n\tpending_packet = &tx->dqo.pending_packets[compl_tag];\n\tif (unlikely(pending_packet->state !=\n\t\t\t\tGVE_PACKET_STATE_PENDING_DATA_COMPL)) {\n\t\tnet_err_ratelimited(\"%s: Unexpected packet state: %d for completion tag : %d\\n\",\n\t\t\t\t    priv->dev->name, (int)pending_packet->state,\n\t\t\t\t    (int)compl_tag);\n\t\treturn;\n\t}\n\n\tpending_packet->state = GVE_PACKET_STATE_PENDING_REINJECT_COMPL;\n\t \n\tpending_packet->timeout_jiffies =\n\t\t\tjiffies +\n\t\t\tmsecs_to_jiffies(GVE_REINJECT_COMPL_TIMEOUT *\n\t\t\t\t\t MSEC_PER_SEC);\n\tadd_to_list(tx, &tx->dqo_compl.miss_completions, pending_packet);\n\n\t*bytes += pending_packet->skb->len;\n\t(*pkts)++;\n}\n\nstatic void remove_miss_completions(struct gve_priv *priv,\n\t\t\t\t    struct gve_tx_ring *tx)\n{\n\tstruct gve_tx_pending_packet_dqo *pending_packet;\n\ts16 next_index;\n\n\tnext_index = tx->dqo_compl.miss_completions.head;\n\twhile (next_index != -1) {\n\t\tpending_packet = &tx->dqo.pending_packets[next_index];\n\t\tnext_index = pending_packet->next;\n\t\t \n\t\tif (time_is_after_jiffies(pending_packet->timeout_jiffies))\n\t\t\tbreak;\n\n\t\tremove_from_list(tx, &tx->dqo_compl.miss_completions,\n\t\t\t\t pending_packet);\n\t\t \n\t\tif (tx->dqo.qpl)\n\t\t\tgve_free_tx_qpl_bufs(tx, pending_packet);\n\t\telse\n\t\t\tgve_unmap_packet(tx->dev, pending_packet);\n\n\t\t \n\t\tdev_kfree_skb_any(pending_packet->skb);\n\t\tpending_packet->skb = NULL;\n\t\ttx->dropped_pkt++;\n\t\tnet_err_ratelimited(\"%s: No reinjection completion was received for: %d.\\n\",\n\t\t\t\t    priv->dev->name,\n\t\t\t\t    (int)(pending_packet - tx->dqo.pending_packets));\n\n\t\tpending_packet->state = GVE_PACKET_STATE_TIMED_OUT_COMPL;\n\t\tpending_packet->timeout_jiffies =\n\t\t\t\tjiffies +\n\t\t\t\tmsecs_to_jiffies(GVE_DEALLOCATE_COMPL_TIMEOUT *\n\t\t\t\t\t\t MSEC_PER_SEC);\n\t\t \n\t\tadd_to_list(tx, &tx->dqo_compl.timed_out_completions,\n\t\t\t    pending_packet);\n\t}\n}\n\nstatic void remove_timed_out_completions(struct gve_priv *priv,\n\t\t\t\t\t struct gve_tx_ring *tx)\n{\n\tstruct gve_tx_pending_packet_dqo *pending_packet;\n\ts16 next_index;\n\n\tnext_index = tx->dqo_compl.timed_out_completions.head;\n\twhile (next_index != -1) {\n\t\tpending_packet = &tx->dqo.pending_packets[next_index];\n\t\tnext_index = pending_packet->next;\n\t\t \n\t\tif (time_is_after_jiffies(pending_packet->timeout_jiffies))\n\t\t\tbreak;\n\n\t\tremove_from_list(tx, &tx->dqo_compl.timed_out_completions,\n\t\t\t\t pending_packet);\n\t\tgve_free_pending_packet(tx, pending_packet);\n\t}\n}\n\nint gve_clean_tx_done_dqo(struct gve_priv *priv, struct gve_tx_ring *tx,\n\t\t\t  struct napi_struct *napi)\n{\n\tu64 reinject_compl_bytes = 0;\n\tu64 reinject_compl_pkts = 0;\n\tint num_descs_cleaned = 0;\n\tu64 miss_compl_bytes = 0;\n\tu64 miss_compl_pkts = 0;\n\tu64 pkt_compl_bytes = 0;\n\tu64 pkt_compl_pkts = 0;\n\n\t \n\twhile (!napi || pkt_compl_pkts < napi->weight) {\n\t\tstruct gve_tx_compl_desc *compl_desc =\n\t\t\t&tx->dqo.compl_ring[tx->dqo_compl.head];\n\t\tu16 type;\n\n\t\tif (compl_desc->generation == tx->dqo_compl.cur_gen_bit)\n\t\t\tbreak;\n\n\t\t \n\t\tprefetch(&tx->dqo.compl_ring[(tx->dqo_compl.head + 1) &\n\t\t\t\ttx->dqo.complq_mask]);\n\n\t\t \n\t\tdma_rmb();\n\t\ttype = compl_desc->type;\n\n\t\tif (type == GVE_COMPL_TYPE_DQO_DESC) {\n\t\t\t \n\t\t\tu16 tx_head = le16_to_cpu(compl_desc->tx_head);\n\n\t\t\tatomic_set_release(&tx->dqo_compl.hw_tx_head, tx_head);\n\t\t} else if (type == GVE_COMPL_TYPE_DQO_PKT) {\n\t\t\tu16 compl_tag = le16_to_cpu(compl_desc->completion_tag);\n\t\t\tif (compl_tag & GVE_ALT_MISS_COMPL_BIT) {\n\t\t\t\tcompl_tag &= ~GVE_ALT_MISS_COMPL_BIT;\n\t\t\t\tgve_handle_miss_completion(priv, tx, compl_tag,\n\t\t\t\t\t\t\t   &miss_compl_bytes,\n\t\t\t\t\t\t\t   &miss_compl_pkts);\n\t\t\t} else {\n\t\t\t\tgve_handle_packet_completion(priv, tx, !!napi,\n\t\t\t\t\t\t\t     compl_tag,\n\t\t\t\t\t\t\t     &pkt_compl_bytes,\n\t\t\t\t\t\t\t     &pkt_compl_pkts,\n\t\t\t\t\t\t\t     false);\n\t\t\t}\n\t\t} else if (type == GVE_COMPL_TYPE_DQO_MISS) {\n\t\t\tu16 compl_tag = le16_to_cpu(compl_desc->completion_tag);\n\n\t\t\tgve_handle_miss_completion(priv, tx, compl_tag,\n\t\t\t\t\t\t   &miss_compl_bytes,\n\t\t\t\t\t\t   &miss_compl_pkts);\n\t\t} else if (type == GVE_COMPL_TYPE_DQO_REINJECTION) {\n\t\t\tu16 compl_tag = le16_to_cpu(compl_desc->completion_tag);\n\n\t\t\tgve_handle_packet_completion(priv, tx, !!napi,\n\t\t\t\t\t\t     compl_tag,\n\t\t\t\t\t\t     &reinject_compl_bytes,\n\t\t\t\t\t\t     &reinject_compl_pkts,\n\t\t\t\t\t\t     true);\n\t\t}\n\n\t\ttx->dqo_compl.head =\n\t\t\t(tx->dqo_compl.head + 1) & tx->dqo.complq_mask;\n\t\t \n\t\ttx->dqo_compl.cur_gen_bit ^= tx->dqo_compl.head == 0;\n\t\tnum_descs_cleaned++;\n\t}\n\n\tnetdev_tx_completed_queue(tx->netdev_txq,\n\t\t\t\t  pkt_compl_pkts + miss_compl_pkts,\n\t\t\t\t  pkt_compl_bytes + miss_compl_bytes);\n\n\tremove_miss_completions(priv, tx);\n\tremove_timed_out_completions(priv, tx);\n\n\tu64_stats_update_begin(&tx->statss);\n\ttx->bytes_done += pkt_compl_bytes + reinject_compl_bytes;\n\ttx->pkt_done += pkt_compl_pkts + reinject_compl_pkts;\n\tu64_stats_update_end(&tx->statss);\n\treturn num_descs_cleaned;\n}\n\nbool gve_tx_poll_dqo(struct gve_notify_block *block, bool do_clean)\n{\n\tstruct gve_tx_compl_desc *compl_desc;\n\tstruct gve_tx_ring *tx = block->tx;\n\tstruct gve_priv *priv = block->priv;\n\n\tif (do_clean) {\n\t\tint num_descs_cleaned = gve_clean_tx_done_dqo(priv, tx,\n\t\t\t\t\t\t\t      &block->napi);\n\n\t\t \n\t\tmb();\n\n\t\tif (netif_tx_queue_stopped(tx->netdev_txq) &&\n\t\t    num_descs_cleaned > 0) {\n\t\t\ttx->wake_queue++;\n\t\t\tnetif_tx_wake_queue(tx->netdev_txq);\n\t\t}\n\t}\n\n\t \n\tcompl_desc = &tx->dqo.compl_ring[tx->dqo_compl.head];\n\treturn compl_desc->generation != tx->dqo_compl.cur_gen_bit;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}