{
  "module_name": "ef100_tx.c",
  "hash_id": "a7c2ad4d2c7883803b4dd6e375dc9a98e7115eddceb52c8c691bf4b76c7d4de9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/ef100_tx.c",
  "human_readable_source": "\n \n\n#include <net/ip6_checksum.h>\n\n#include \"net_driver.h\"\n#include \"tx_common.h\"\n#include \"nic_common.h\"\n#include \"mcdi_functions.h\"\n#include \"ef100_regs.h\"\n#include \"io.h\"\n#include \"ef100_tx.h\"\n#include \"ef100_nic.h\"\n\nint ef100_tx_probe(struct efx_tx_queue *tx_queue)\n{\n\t \n\treturn efx_nic_alloc_buffer(tx_queue->efx, &tx_queue->txd,\n\t\t\t\t    (tx_queue->ptr_mask + 2) *\n\t\t\t\t    sizeof(efx_oword_t),\n\t\t\t\t    GFP_KERNEL);\n}\n\nvoid ef100_tx_init(struct efx_tx_queue *tx_queue)\n{\n\t \n\ttx_queue->core_txq =\n\t\tnetdev_get_tx_queue(tx_queue->efx->net_dev,\n\t\t\t\t    tx_queue->channel->channel -\n\t\t\t\t    tx_queue->efx->tx_channel_offset);\n\n\t \n\ttx_queue->tso_version = 3;\n\tif (efx_mcdi_tx_init(tx_queue))\n\t\tnetdev_WARN(tx_queue->efx->net_dev,\n\t\t\t    \"failed to initialise TXQ %d\\n\", tx_queue->queue);\n}\n\nstatic bool ef100_tx_can_tso(struct efx_tx_queue *tx_queue, struct sk_buff *skb)\n{\n\tstruct efx_nic *efx = tx_queue->efx;\n\tstruct ef100_nic_data *nic_data;\n\tstruct efx_tx_buffer *buffer;\n\tsize_t header_len;\n\tu32 mss;\n\n\tnic_data = efx->nic_data;\n\n\tif (!skb_is_gso_tcp(skb))\n\t\treturn false;\n\tif (!(efx->net_dev->features & NETIF_F_TSO))\n\t\treturn false;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(mss < 4)) {\n\t\tWARN_ONCE(1, \"MSS of %u is too small for TSO\\n\", mss);\n\t\treturn false;\n\t}\n\n\theader_len = efx_tx_tso_header_length(skb);\n\tif (header_len > nic_data->tso_max_hdr_len)\n\t\treturn false;\n\n\tif (skb_shinfo(skb)->gso_segs > nic_data->tso_max_payload_num_segs) {\n\t\t \n\t\tWARN_ON_ONCE(1);\n\t\treturn false;\n\t}\n\n\tif (skb->data_len / mss > nic_data->tso_max_frames)\n\t\treturn false;\n\n\t \n\tif (WARN_ON_ONCE(skb->data_len > nic_data->tso_max_payload_len))\n\t\treturn false;\n\n\t \n\tbuffer = efx_tx_queue_get_insert_buffer(tx_queue);\n\tbuffer->flags = EFX_TX_BUF_TSO_V3 | EFX_TX_BUF_CONT;\n\tbuffer->len = header_len;\n\tbuffer->unmap_len = 0;\n\tbuffer->skb = skb;\n\t++tx_queue->insert_count;\n\treturn true;\n}\n\nstatic efx_oword_t *ef100_tx_desc(struct efx_tx_queue *tx_queue, unsigned int index)\n{\n\tif (likely(tx_queue->txd.addr))\n\t\treturn ((efx_oword_t *)tx_queue->txd.addr) + index;\n\telse\n\t\treturn NULL;\n}\n\nstatic void ef100_notify_tx_desc(struct efx_tx_queue *tx_queue)\n{\n\tunsigned int write_ptr;\n\tefx_dword_t reg;\n\n\ttx_queue->xmit_pending = false;\n\n\tif (unlikely(tx_queue->notify_count == tx_queue->write_count))\n\t\treturn;\n\n\twrite_ptr = tx_queue->write_count & tx_queue->ptr_mask;\n\t \n\tEFX_POPULATE_DWORD_1(reg, ERF_GZ_TX_RING_PIDX, write_ptr);\n\tefx_writed_page(tx_queue->efx, &reg,\n\t\t\tER_GZ_TX_RING_DOORBELL, tx_queue->queue);\n\ttx_queue->notify_count = tx_queue->write_count;\n}\n\nstatic void ef100_tx_push_buffers(struct efx_tx_queue *tx_queue)\n{\n\tef100_notify_tx_desc(tx_queue);\n\t++tx_queue->pushes;\n}\n\nstatic void ef100_set_tx_csum_partial(const struct sk_buff *skb,\n\t\t\t\t      struct efx_tx_buffer *buffer, efx_oword_t *txd)\n{\n\tefx_oword_t csum;\n\tint csum_start;\n\n\tif (!skb || skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn;\n\n\t \n\tcsum_start = skb_checksum_start_offset(skb);\n\tEFX_POPULATE_OWORD_3(csum,\n\t\t\t     ESF_GZ_TX_SEND_CSO_PARTIAL_EN, 1,\n\t\t\t     ESF_GZ_TX_SEND_CSO_PARTIAL_START_W,\n\t\t\t     csum_start >> 1,\n\t\t\t     ESF_GZ_TX_SEND_CSO_PARTIAL_CSUM_W,\n\t\t\t     skb->csum_offset >> 1);\n\tEFX_OR_OWORD(*txd, *txd, csum);\n}\n\nstatic void ef100_set_tx_hw_vlan(const struct sk_buff *skb, efx_oword_t *txd)\n{\n\tu16 vlan_tci = skb_vlan_tag_get(skb);\n\tefx_oword_t vlan;\n\n\tEFX_POPULATE_OWORD_2(vlan,\n\t\t\t     ESF_GZ_TX_SEND_VLAN_INSERT_EN, 1,\n\t\t\t     ESF_GZ_TX_SEND_VLAN_INSERT_TCI, vlan_tci);\n\tEFX_OR_OWORD(*txd, *txd, vlan);\n}\n\nstatic void ef100_make_send_desc(struct efx_nic *efx,\n\t\t\t\t const struct sk_buff *skb,\n\t\t\t\t struct efx_tx_buffer *buffer, efx_oword_t *txd,\n\t\t\t\t unsigned int segment_count)\n{\n\t \n\tEFX_POPULATE_OWORD_3(*txd,\n\t\t\t     ESF_GZ_TX_SEND_NUM_SEGS, segment_count,\n\t\t\t     ESF_GZ_TX_SEND_LEN, buffer->len,\n\t\t\t     ESF_GZ_TX_SEND_ADDR, buffer->dma_addr);\n\n\tif (likely(efx->net_dev->features & NETIF_F_HW_CSUM))\n\t\tef100_set_tx_csum_partial(skb, buffer, txd);\n\tif (efx->net_dev->features & NETIF_F_HW_VLAN_CTAG_TX &&\n\t    skb && skb_vlan_tag_present(skb))\n\t\tef100_set_tx_hw_vlan(skb, txd);\n}\n\nstatic void ef100_make_tso_desc(struct efx_nic *efx,\n\t\t\t\tconst struct sk_buff *skb,\n\t\t\t\tstruct efx_tx_buffer *buffer, efx_oword_t *txd,\n\t\t\t\tunsigned int segment_count)\n{\n\tbool gso_partial = skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL;\n\tunsigned int len, ip_offset, tcp_offset, payload_segs;\n\tu32 mangleid = ESE_GZ_TX_DESC_IP4_ID_INC_MOD16;\n\tunsigned int outer_ip_offset, outer_l4_offset;\n\tu16 vlan_tci = skb_vlan_tag_get(skb);\n\tu32 mss = skb_shinfo(skb)->gso_size;\n\tbool encap = skb->encapsulation;\n\tbool udp_encap = false;\n\tu16 vlan_enable = 0;\n\tstruct tcphdr *tcp;\n\tbool outer_csum;\n\tu32 paylen;\n\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCP_FIXEDID)\n\t\tmangleid = ESE_GZ_TX_DESC_IP4_ID_NO_OP;\n\tif (efx->net_dev->features & NETIF_F_HW_VLAN_CTAG_TX)\n\t\tvlan_enable = skb_vlan_tag_present(skb);\n\n\tlen = skb->len - buffer->len;\n\t \n\tpayload_segs = segment_count - 2;\n\tif (encap) {\n\t\touter_ip_offset = skb_network_offset(skb);\n\t\touter_l4_offset = skb_transport_offset(skb);\n\t\tip_offset = skb_inner_network_offset(skb);\n\t\ttcp_offset = skb_inner_transport_offset(skb);\n\t\tif (skb_shinfo(skb)->gso_type &\n\t\t    (SKB_GSO_UDP_TUNNEL | SKB_GSO_UDP_TUNNEL_CSUM))\n\t\t\tudp_encap = true;\n\t} else {\n\t\tip_offset =  skb_network_offset(skb);\n\t\ttcp_offset = skb_transport_offset(skb);\n\t\touter_ip_offset = outer_l4_offset = 0;\n\t}\n\touter_csum = skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM;\n\n\t \n\ttcp = (void *)skb->data + tcp_offset;\n\tpaylen = skb->len - tcp_offset;\n\tcsum_replace_by_diff(&tcp->check, (__force __wsum)htonl(paylen));\n\n\tEFX_POPULATE_OWORD_19(*txd,\n\t\t\t      ESF_GZ_TX_DESC_TYPE, ESE_GZ_TX_DESC_TYPE_TSO,\n\t\t\t      ESF_GZ_TX_TSO_MSS, mss,\n\t\t\t      ESF_GZ_TX_TSO_HDR_NUM_SEGS, 1,\n\t\t\t      ESF_GZ_TX_TSO_PAYLOAD_NUM_SEGS, payload_segs,\n\t\t\t      ESF_GZ_TX_TSO_HDR_LEN_W, buffer->len >> 1,\n\t\t\t      ESF_GZ_TX_TSO_PAYLOAD_LEN, len,\n\t\t\t      ESF_GZ_TX_TSO_CSO_OUTER_L4, outer_csum,\n\t\t\t      ESF_GZ_TX_TSO_CSO_INNER_L4, 1,\n\t\t\t      ESF_GZ_TX_TSO_INNER_L3_OFF_W, ip_offset >> 1,\n\t\t\t      ESF_GZ_TX_TSO_INNER_L4_OFF_W, tcp_offset >> 1,\n\t\t\t      ESF_GZ_TX_TSO_ED_INNER_IP4_ID, mangleid,\n\t\t\t      ESF_GZ_TX_TSO_ED_INNER_IP_LEN, 1,\n\t\t\t      ESF_GZ_TX_TSO_OUTER_L3_OFF_W, outer_ip_offset >> 1,\n\t\t\t      ESF_GZ_TX_TSO_OUTER_L4_OFF_W, outer_l4_offset >> 1,\n\t\t\t      ESF_GZ_TX_TSO_ED_OUTER_UDP_LEN, udp_encap && !gso_partial,\n\t\t\t      ESF_GZ_TX_TSO_ED_OUTER_IP_LEN, encap && !gso_partial,\n\t\t\t      ESF_GZ_TX_TSO_ED_OUTER_IP4_ID, encap ? mangleid :\n\t\t\t\t\t\t\t\t     ESE_GZ_TX_DESC_IP4_ID_NO_OP,\n\t\t\t      ESF_GZ_TX_TSO_VLAN_INSERT_EN, vlan_enable,\n\t\t\t      ESF_GZ_TX_TSO_VLAN_INSERT_TCI, vlan_tci\n\t\t);\n}\n\nstatic void ef100_tx_make_descriptors(struct efx_tx_queue *tx_queue,\n\t\t\t\t      const struct sk_buff *skb,\n\t\t\t\t      unsigned int segment_count,\n\t\t\t\t      struct efx_rep *efv)\n{\n\tunsigned int old_write_count = tx_queue->write_count;\n\tunsigned int new_write_count = old_write_count;\n\tstruct efx_tx_buffer *buffer;\n\tunsigned int next_desc_type;\n\tunsigned int write_ptr;\n\tefx_oword_t *txd;\n\tunsigned int nr_descs = tx_queue->insert_count - old_write_count;\n\n\tif (unlikely(nr_descs == 0))\n\t\treturn;\n\n\tif (segment_count)\n\t\tnext_desc_type = ESE_GZ_TX_DESC_TYPE_TSO;\n\telse\n\t\tnext_desc_type = ESE_GZ_TX_DESC_TYPE_SEND;\n\n\tif (unlikely(efv)) {\n\t\t \n\t\twrite_ptr = new_write_count & tx_queue->ptr_mask;\n\t\ttxd = ef100_tx_desc(tx_queue, write_ptr);\n\t\t++new_write_count;\n\n\t\ttx_queue->packet_write_count = new_write_count;\n\t\tEFX_POPULATE_OWORD_3(*txd,\n\t\t\t\t     ESF_GZ_TX_DESC_TYPE, ESE_GZ_TX_DESC_TYPE_PREFIX,\n\t\t\t\t     ESF_GZ_TX_PREFIX_EGRESS_MPORT, efv->mport,\n\t\t\t\t     ESF_GZ_TX_PREFIX_EGRESS_MPORT_EN, 1);\n\t\tnr_descs--;\n\t}\n\n\t \n\tif (!skb)\n\t\tnr_descs = 1;\n\n\tdo {\n\t\twrite_ptr = new_write_count & tx_queue->ptr_mask;\n\t\tbuffer = &tx_queue->buffer[write_ptr];\n\t\ttxd = ef100_tx_desc(tx_queue, write_ptr);\n\t\t++new_write_count;\n\n\t\t \n\t\ttx_queue->packet_write_count = new_write_count;\n\n\t\tswitch (next_desc_type) {\n\t\tcase ESE_GZ_TX_DESC_TYPE_SEND:\n\t\t\tef100_make_send_desc(tx_queue->efx, skb,\n\t\t\t\t\t     buffer, txd, nr_descs);\n\t\t\tbreak;\n\t\tcase ESE_GZ_TX_DESC_TYPE_TSO:\n\t\t\t \n\t\t\tWARN_ON_ONCE(!(buffer->flags & EFX_TX_BUF_TSO_V3));\n\t\t\tef100_make_tso_desc(tx_queue->efx, skb,\n\t\t\t\t\t    buffer, txd, nr_descs);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tEFX_POPULATE_OWORD_3(*txd,\n\t\t\t\t\t     ESF_GZ_TX_DESC_TYPE, ESE_GZ_TX_DESC_TYPE_SEG,\n\t\t\t\t\t     ESF_GZ_TX_SEG_LEN, buffer->len,\n\t\t\t\t\t     ESF_GZ_TX_SEG_ADDR, buffer->dma_addr);\n\t\t}\n\t\t \n\t\tnext_desc_type = skb ? ESE_GZ_TX_DESC_TYPE_SEG :\n\t\t\t\t       ESE_GZ_TX_DESC_TYPE_SEND;\n\t\t \n\t\tif (unlikely(efv))\n\t\t\tbuffer->flags |= EFX_TX_BUF_EFV;\n\n\t} while (new_write_count != tx_queue->insert_count);\n\n\twmb();  \n\n\ttx_queue->write_count = new_write_count;\n\n\t \n\tsmp_mb();\n}\n\nvoid ef100_tx_write(struct efx_tx_queue *tx_queue)\n{\n\tef100_tx_make_descriptors(tx_queue, NULL, 0, NULL);\n\tef100_tx_push_buffers(tx_queue);\n}\n\nint ef100_ev_tx(struct efx_channel *channel, const efx_qword_t *p_event)\n{\n\tunsigned int tx_done =\n\t\tEFX_QWORD_FIELD(*p_event, ESF_GZ_EV_TXCMPL_NUM_DESC);\n\tunsigned int qlabel =\n\t\tEFX_QWORD_FIELD(*p_event, ESF_GZ_EV_TXCMPL_Q_LABEL);\n\tstruct efx_tx_queue *tx_queue =\n\t\tefx_channel_get_tx_queue(channel, qlabel);\n\tunsigned int tx_index = (tx_queue->read_count + tx_done - 1) &\n\t\t\t\ttx_queue->ptr_mask;\n\n\treturn efx_xmit_done(tx_queue, tx_index);\n}\n\n \nnetdev_tx_t ef100_enqueue_skb(struct efx_tx_queue *tx_queue,\n\t\t\t      struct sk_buff *skb)\n{\n\treturn __ef100_enqueue_skb(tx_queue, skb, NULL);\n}\n\nint __ef100_enqueue_skb(struct efx_tx_queue *tx_queue, struct sk_buff *skb,\n\t\t\tstruct efx_rep *efv)\n{\n\tunsigned int old_insert_count = tx_queue->insert_count;\n\tstruct efx_nic *efx = tx_queue->efx;\n\tbool xmit_more = netdev_xmit_more();\n\tunsigned int fill_level;\n\tunsigned int segments;\n\tint rc;\n\n\tif (!tx_queue->buffer || !tx_queue->ptr_mask) {\n\t\tnetif_stop_queue(efx->net_dev);\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -ENODEV;\n\t}\n\n\tsegments = skb_is_gso(skb) ? skb_shinfo(skb)->gso_segs : 0;\n\tif (segments == 1)\n\t\tsegments = 0;\t \n\tif (segments && !ef100_tx_can_tso(tx_queue, skb)) {\n\t\trc = efx_tx_tso_fallback(tx_queue, skb);\n\t\ttx_queue->tso_fallbacks++;\n\t\tif (rc)\n\t\t\tgoto err;\n\t\telse\n\t\t\treturn 0;\n\t}\n\n\tif (unlikely(efv)) {\n\t\tstruct efx_tx_buffer *buffer = __efx_tx_queue_get_insert_buffer(tx_queue);\n\n\t\t \n\t\tif (netif_tx_queue_stopped(tx_queue->core_txq) ||\n\t\t    unlikely(efx_tx_buffer_in_use(buffer))) {\n\t\t\tatomic64_inc(&efv->stats.tx_errors);\n\t\t\trc = -ENOSPC;\n\t\t\tgoto err;\n\t\t}\n\n\t\t \n\t\tfill_level = efx_channel_tx_old_fill_level(tx_queue->channel);\n\t\tfill_level += efx_tx_max_skb_descs(efx);\n\t\tif (fill_level > efx->txq_stop_thresh) {\n\t\t\tstruct efx_tx_queue *txq2;\n\n\t\t\t \n\t\t\tefx_for_each_channel_tx_queue(txq2, tx_queue->channel)\n\t\t\t\ttxq2->old_read_count = READ_ONCE(txq2->read_count);\n\n\t\t\tfill_level = efx_channel_tx_old_fill_level(tx_queue->channel);\n\t\t\tfill_level += efx_tx_max_skb_descs(efx);\n\t\t\tif (fill_level > efx->txq_stop_thresh) {\n\t\t\t\tatomic64_inc(&efv->stats.tx_errors);\n\t\t\t\trc = -ENOSPC;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\tbuffer->flags = EFX_TX_BUF_OPTION | EFX_TX_BUF_EFV;\n\t\ttx_queue->insert_count++;\n\t}\n\n\t \n\trc = efx_tx_map_data(tx_queue, skb, segments);\n\tif (rc)\n\t\tgoto err;\n\tef100_tx_make_descriptors(tx_queue, skb, segments, efv);\n\n\tfill_level = efx_channel_tx_old_fill_level(tx_queue->channel);\n\tif (fill_level > efx->txq_stop_thresh) {\n\t\tstruct efx_tx_queue *txq2;\n\n\t\t \n\t\tWARN_ON(efv);\n\n\t\tnetif_tx_stop_queue(tx_queue->core_txq);\n\t\t \n\t\tsmp_mb();\n\t\tefx_for_each_channel_tx_queue(txq2, tx_queue->channel)\n\t\t\ttxq2->old_read_count = READ_ONCE(txq2->read_count);\n\t\tfill_level = efx_channel_tx_old_fill_level(tx_queue->channel);\n\t\tif (fill_level < efx->txq_stop_thresh)\n\t\t\tnetif_tx_start_queue(tx_queue->core_txq);\n\t}\n\n\ttx_queue->xmit_pending = true;\n\n\t \n\tif (unlikely(efv) ||\n\t    __netdev_tx_sent_queue(tx_queue->core_txq, skb->len, xmit_more) ||\n\t    tx_queue->write_count - tx_queue->notify_count > 255)\n\t\tef100_tx_push_buffers(tx_queue);\n\n\tif (segments) {\n\t\ttx_queue->tso_bursts++;\n\t\ttx_queue->tso_packets += segments;\n\t\ttx_queue->tx_packets  += segments;\n\t} else {\n\t\ttx_queue->tx_packets++;\n\t}\n\treturn 0;\n\nerr:\n\tefx_enqueue_unwind(tx_queue, old_insert_count);\n\tif (!IS_ERR_OR_NULL(skb))\n\t\tdev_kfree_skb_any(skb);\n\n\t \n\tif (tx_queue->xmit_pending && !xmit_more)\n\t\tef100_tx_push_buffers(tx_queue);\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}