{
  "module_name": "tx.c",
  "hash_id": "189c709f20f736513cb24c8bfd5feb30809495c5794cac830d92466a1e7b8d15",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/tx.c",
  "human_readable_source": "\n \n\n#include <linux/pci.h>\n#include <linux/tcp.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/ipv6.h>\n#include <linux/slab.h>\n#include <net/ipv6.h>\n#include <linux/if_ether.h>\n#include <linux/highmem.h>\n#include <linux/cache.h>\n#include \"net_driver.h\"\n#include \"efx.h\"\n#include \"io.h\"\n#include \"nic.h\"\n#include \"tx.h\"\n#include \"tx_common.h\"\n#include \"workarounds.h\"\n#include \"ef10_regs.h\"\n\n#ifdef EFX_USE_PIO\n\n#define EFX_PIOBUF_SIZE_DEF ALIGN(256, L1_CACHE_BYTES)\nunsigned int efx_piobuf_size __read_mostly = EFX_PIOBUF_SIZE_DEF;\n\n#endif  \n\nstatic inline u8 *efx_tx_get_copy_buffer(struct efx_tx_queue *tx_queue,\n\t\t\t\t\t struct efx_tx_buffer *buffer)\n{\n\tunsigned int index = efx_tx_queue_get_insert_index(tx_queue);\n\tstruct efx_buffer *page_buf =\n\t\t&tx_queue->cb_page[index >> (PAGE_SHIFT - EFX_TX_CB_ORDER)];\n\tunsigned int offset =\n\t\t((index << EFX_TX_CB_ORDER) + NET_IP_ALIGN) & (PAGE_SIZE - 1);\n\n\tif (unlikely(!page_buf->addr) &&\n\t    efx_nic_alloc_buffer(tx_queue->efx, page_buf, PAGE_SIZE,\n\t\t\t\t GFP_ATOMIC))\n\t\treturn NULL;\n\tbuffer->dma_addr = page_buf->dma_addr + offset;\n\tbuffer->unmap_len = 0;\n\treturn (u8 *)page_buf->addr + offset;\n}\n\nu8 *efx_tx_get_copy_buffer_limited(struct efx_tx_queue *tx_queue,\n\t\t\t\t   struct efx_tx_buffer *buffer, size_t len)\n{\n\tif (len > EFX_TX_CB_SIZE)\n\t\treturn NULL;\n\treturn efx_tx_get_copy_buffer(tx_queue, buffer);\n}\n\nstatic void efx_tx_maybe_stop_queue(struct efx_tx_queue *txq1)\n{\n\t \n\tstruct efx_nic *efx = txq1->efx;\n\tstruct efx_tx_queue *txq2;\n\tunsigned int fill_level;\n\n\tfill_level = efx_channel_tx_old_fill_level(txq1->channel);\n\tif (likely(fill_level < efx->txq_stop_thresh))\n\t\treturn;\n\n\t \n\tnetif_tx_stop_queue(txq1->core_txq);\n\tsmp_mb();\n\tefx_for_each_channel_tx_queue(txq2, txq1->channel)\n\t\ttxq2->old_read_count = READ_ONCE(txq2->read_count);\n\n\tfill_level = efx_channel_tx_old_fill_level(txq1->channel);\n\tEFX_WARN_ON_ONCE_PARANOID(fill_level >= efx->txq_entries);\n\tif (likely(fill_level < efx->txq_stop_thresh)) {\n\t\tsmp_mb();\n\t\tif (likely(!efx->loopback_selftest))\n\t\t\tnetif_tx_start_queue(txq1->core_txq);\n\t}\n}\n\nstatic int efx_enqueue_skb_copy(struct efx_tx_queue *tx_queue,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tunsigned int copy_len = skb->len;\n\tstruct efx_tx_buffer *buffer;\n\tu8 *copy_buffer;\n\tint rc;\n\n\tEFX_WARN_ON_ONCE_PARANOID(copy_len > EFX_TX_CB_SIZE);\n\n\tbuffer = efx_tx_queue_get_insert_buffer(tx_queue);\n\n\tcopy_buffer = efx_tx_get_copy_buffer(tx_queue, buffer);\n\tif (unlikely(!copy_buffer))\n\t\treturn -ENOMEM;\n\n\trc = skb_copy_bits(skb, 0, copy_buffer, copy_len);\n\tEFX_WARN_ON_PARANOID(rc);\n\tbuffer->len = copy_len;\n\n\tbuffer->skb = skb;\n\tbuffer->flags = EFX_TX_BUF_SKB;\n\n\t++tx_queue->insert_count;\n\treturn rc;\n}\n\n#ifdef EFX_USE_PIO\n\nstruct efx_short_copy_buffer {\n\tint used;\n\tu8 buf[L1_CACHE_BYTES];\n};\n\n \nstatic void efx_memcpy_toio_aligned(struct efx_nic *efx, u8 __iomem **piobuf,\n\t\t\t\t    u8 *data, int len,\n\t\t\t\t    struct efx_short_copy_buffer *copy_buf)\n{\n\tint block_len = len & ~(sizeof(copy_buf->buf) - 1);\n\n\t__iowrite64_copy(*piobuf, data, block_len >> 3);\n\t*piobuf += block_len;\n\tlen -= block_len;\n\n\tif (len) {\n\t\tdata += block_len;\n\t\tBUG_ON(copy_buf->used);\n\t\tBUG_ON(len > sizeof(copy_buf->buf));\n\t\tmemcpy(copy_buf->buf, data, len);\n\t\tcopy_buf->used = len;\n\t}\n}\n\n \nstatic void efx_memcpy_toio_aligned_cb(struct efx_nic *efx, u8 __iomem **piobuf,\n\t\t\t\t       u8 *data, int len,\n\t\t\t\t       struct efx_short_copy_buffer *copy_buf)\n{\n\tif (copy_buf->used) {\n\t\t \n\t\tint copy_to_buf =\n\t\t\tmin_t(int, sizeof(copy_buf->buf) - copy_buf->used, len);\n\n\t\tmemcpy(copy_buf->buf + copy_buf->used, data, copy_to_buf);\n\t\tcopy_buf->used += copy_to_buf;\n\n\t\t \n\t\tif (copy_buf->used < sizeof(copy_buf->buf))\n\t\t\treturn;\n\n\t\t__iowrite64_copy(*piobuf, copy_buf->buf,\n\t\t\t\t sizeof(copy_buf->buf) >> 3);\n\t\t*piobuf += sizeof(copy_buf->buf);\n\t\tdata += copy_to_buf;\n\t\tlen -= copy_to_buf;\n\t\tcopy_buf->used = 0;\n\t}\n\n\tefx_memcpy_toio_aligned(efx, piobuf, data, len, copy_buf);\n}\n\nstatic void efx_flush_copy_buffer(struct efx_nic *efx, u8 __iomem *piobuf,\n\t\t\t\t  struct efx_short_copy_buffer *copy_buf)\n{\n\t \n\tif (copy_buf->used)\n\t\t__iowrite64_copy(piobuf, copy_buf->buf,\n\t\t\t\t sizeof(copy_buf->buf) >> 3);\n}\n\n \nstatic void efx_skb_copy_bits_to_pio(struct efx_nic *efx, struct sk_buff *skb,\n\t\t\t\t     u8 __iomem **piobuf,\n\t\t\t\t     struct efx_short_copy_buffer *copy_buf)\n{\n\tint i;\n\n\tefx_memcpy_toio_aligned(efx, piobuf, skb->data, skb_headlen(skb),\n\t\t\t\tcopy_buf);\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; ++i) {\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\t\tu8 *vaddr;\n\n\t\tvaddr = kmap_local_page(skb_frag_page(f));\n\n\t\tefx_memcpy_toio_aligned_cb(efx, piobuf, vaddr + skb_frag_off(f),\n\t\t\t\t\t   skb_frag_size(f), copy_buf);\n\t\tkunmap_local(vaddr);\n\t}\n\n\tEFX_WARN_ON_ONCE_PARANOID(skb_shinfo(skb)->frag_list);\n}\n\nstatic int efx_enqueue_skb_pio(struct efx_tx_queue *tx_queue,\n\t\t\t       struct sk_buff *skb)\n{\n\tstruct efx_tx_buffer *buffer =\n\t\tefx_tx_queue_get_insert_buffer(tx_queue);\n\tu8 __iomem *piobuf = tx_queue->piobuf;\n\n\t \n\n\tif (skb_shinfo(skb)->nr_frags) {\n\t\t \n\t\tstruct efx_short_copy_buffer copy_buf;\n\n\t\tcopy_buf.used = 0;\n\n\t\tefx_skb_copy_bits_to_pio(tx_queue->efx, skb,\n\t\t\t\t\t &piobuf, &copy_buf);\n\t\tefx_flush_copy_buffer(tx_queue->efx, piobuf, &copy_buf);\n\t} else {\n\t\t \n\t\tBUILD_BUG_ON(L1_CACHE_BYTES >\n\t\t\t     SKB_DATA_ALIGN(sizeof(struct skb_shared_info)));\n\t\t__iowrite64_copy(tx_queue->piobuf, skb->data,\n\t\t\t\t ALIGN(skb->len, L1_CACHE_BYTES) >> 3);\n\t}\n\n\tbuffer->skb = skb;\n\tbuffer->flags = EFX_TX_BUF_SKB | EFX_TX_BUF_OPTION;\n\n\tEFX_POPULATE_QWORD_5(buffer->option,\n\t\t\t     ESF_DZ_TX_DESC_IS_OPT, 1,\n\t\t\t     ESF_DZ_TX_OPTION_TYPE, ESE_DZ_TX_OPTION_DESC_PIO,\n\t\t\t     ESF_DZ_TX_PIO_CONT, 0,\n\t\t\t     ESF_DZ_TX_PIO_BYTE_CNT, skb->len,\n\t\t\t     ESF_DZ_TX_PIO_BUF_ADDR,\n\t\t\t     tx_queue->piobuf_offset);\n\t++tx_queue->insert_count;\n\treturn 0;\n}\n\n \nstatic bool efx_tx_may_pio(struct efx_tx_queue *tx_queue)\n{\n\tstruct efx_channel *channel = tx_queue->channel;\n\n\tif (!tx_queue->piobuf)\n\t\treturn false;\n\n\tEFX_WARN_ON_ONCE_PARANOID(!channel->efx->type->option_descriptors);\n\n\tefx_for_each_channel_tx_queue(tx_queue, channel)\n\t\tif (!efx_nic_tx_is_empty(tx_queue, tx_queue->packet_write_count))\n\t\t\treturn false;\n\n\treturn true;\n}\n#endif  \n\n \nstatic void efx_tx_send_pending(struct efx_channel *channel)\n{\n\tstruct efx_tx_queue *q;\n\n\tefx_for_each_channel_tx_queue(q, channel) {\n\t\tif (q->xmit_pending)\n\t\t\tefx_nic_push_buffers(q);\n\t}\n}\n\n \nnetdev_tx_t __efx_enqueue_skb(struct efx_tx_queue *tx_queue, struct sk_buff *skb)\n{\n\tunsigned int old_insert_count = tx_queue->insert_count;\n\tbool xmit_more = netdev_xmit_more();\n\tbool data_mapped = false;\n\tunsigned int segments;\n\tunsigned int skb_len;\n\tint rc;\n\n\tskb_len = skb->len;\n\tsegments = skb_is_gso(skb) ? skb_shinfo(skb)->gso_segs : 0;\n\tif (segments == 1)\n\t\tsegments = 0;  \n\n\t \n\tif (segments) {\n\t\tswitch (tx_queue->tso_version) {\n\t\tcase 1:\n\t\t\trc = efx_enqueue_skb_tso(tx_queue, skb, &data_mapped);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\trc = efx_ef10_tx_tso_desc(tx_queue, skb, &data_mapped);\n\t\t\tbreak;\n\t\tcase 0:  \n\t\tdefault:\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (rc == -EINVAL) {\n\t\t\trc = efx_tx_tso_fallback(tx_queue, skb);\n\t\t\ttx_queue->tso_fallbacks++;\n\t\t\tif (rc == 0)\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (rc)\n\t\t\tgoto err;\n#ifdef EFX_USE_PIO\n\t} else if (skb_len <= efx_piobuf_size && !xmit_more &&\n\t\t   efx_tx_may_pio(tx_queue)) {\n\t\t \n\t\tif (efx_enqueue_skb_pio(tx_queue, skb))\n\t\t\tgoto err;\n\t\ttx_queue->pio_packets++;\n\t\tdata_mapped = true;\n#endif\n\t} else if (skb->data_len && skb_len <= EFX_TX_CB_SIZE) {\n\t\t \n\t\tif (efx_enqueue_skb_copy(tx_queue, skb))\n\t\t\tgoto err;\n\t\ttx_queue->cb_packets++;\n\t\tdata_mapped = true;\n\t}\n\n\t \n\tif (!data_mapped && (efx_tx_map_data(tx_queue, skb, segments)))\n\t\tgoto err;\n\n\tefx_tx_maybe_stop_queue(tx_queue);\n\n\ttx_queue->xmit_pending = true;\n\n\t \n\tif (__netdev_tx_sent_queue(tx_queue->core_txq, skb_len, xmit_more))\n\t\tefx_tx_send_pending(tx_queue->channel);\n\n\tif (segments) {\n\t\ttx_queue->tso_bursts++;\n\t\ttx_queue->tso_packets += segments;\n\t\ttx_queue->tx_packets  += segments;\n\t} else {\n\t\ttx_queue->tx_packets++;\n\t}\n\n\treturn NETDEV_TX_OK;\n\n\nerr:\n\tefx_enqueue_unwind(tx_queue, old_insert_count);\n\tdev_kfree_skb_any(skb);\n\n\t \n\tif (!xmit_more)\n\t\tefx_tx_send_pending(tx_queue->channel);\n\n\treturn NETDEV_TX_OK;\n}\n\n \nint efx_xdp_tx_buffers(struct efx_nic *efx, int n, struct xdp_frame **xdpfs,\n\t\t       bool flush)\n{\n\tstruct efx_tx_buffer *tx_buffer;\n\tstruct efx_tx_queue *tx_queue;\n\tstruct xdp_frame *xdpf;\n\tdma_addr_t dma_addr;\n\tunsigned int len;\n\tint space;\n\tint cpu;\n\tint i = 0;\n\n\tif (unlikely(n && !xdpfs))\n\t\treturn -EINVAL;\n\tif (unlikely(!n))\n\t\treturn 0;\n\n\tcpu = raw_smp_processor_id();\n\tif (unlikely(cpu >= efx->xdp_tx_queue_count))\n\t\treturn -EINVAL;\n\n\ttx_queue = efx->xdp_tx_queues[cpu];\n\tif (unlikely(!tx_queue))\n\t\treturn -EINVAL;\n\n\tif (!tx_queue->initialised)\n\t\treturn -EINVAL;\n\n\tif (efx->xdp_txq_queues_mode != EFX_XDP_TX_QUEUES_DEDICATED)\n\t\tHARD_TX_LOCK(efx->net_dev, tx_queue->core_txq, cpu);\n\n\t \n\tif (efx->xdp_txq_queues_mode == EFX_XDP_TX_QUEUES_BORROWED) {\n\t\tif (netif_tx_queue_stopped(tx_queue->core_txq))\n\t\t\tgoto unlock;\n\t\tefx_tx_maybe_stop_queue(tx_queue);\n\t}\n\n\t \n\tspace = efx->txq_entries +\n\t\ttx_queue->read_count - tx_queue->insert_count;\n\n\tfor (i = 0; i < n; i++) {\n\t\txdpf = xdpfs[i];\n\n\t\tif (i >= space)\n\t\t\tbreak;\n\n\t\t \n\t\tprefetchw(__efx_tx_queue_get_insert_buffer(tx_queue));\n\n\t\tlen = xdpf->len;\n\n\t\t \n\t\tdma_addr = dma_map_single(&efx->pci_dev->dev,\n\t\t\t\t\t  xdpf->data, len,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&efx->pci_dev->dev, dma_addr))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buffer = efx_tx_map_chunk(tx_queue, dma_addr, len);\n\t\ttx_buffer->xdpf = xdpf;\n\t\ttx_buffer->flags = EFX_TX_BUF_XDP |\n\t\t\t\t   EFX_TX_BUF_MAP_SINGLE;\n\t\ttx_buffer->dma_offset = 0;\n\t\ttx_buffer->unmap_len = len;\n\t\ttx_queue->tx_packets++;\n\t}\n\n\t \n\tif (flush && i > 0)\n\t\tefx_nic_push_buffers(tx_queue);\n\nunlock:\n\tif (efx->xdp_txq_queues_mode != EFX_XDP_TX_QUEUES_DEDICATED)\n\t\tHARD_TX_UNLOCK(efx->net_dev, tx_queue->core_txq);\n\n\treturn i == 0 ? -EIO : i;\n}\n\n \nnetdev_tx_t efx_hard_start_xmit(struct sk_buff *skb,\n\t\t\t\tstruct net_device *net_dev)\n{\n\tstruct efx_nic *efx = efx_netdev_priv(net_dev);\n\tstruct efx_tx_queue *tx_queue;\n\tunsigned index, type;\n\n\tEFX_WARN_ON_PARANOID(!netif_device_present(net_dev));\n\tindex = skb_get_queue_mapping(skb);\n\ttype = efx_tx_csum_type_skb(skb);\n\n\t \n\tif (unlikely(efx_xmit_with_hwtstamp(skb)) &&\n\t    ((efx_ptp_use_mac_tx_timestamps(efx) && efx->ptp_data) ||\n\t    unlikely(efx_ptp_is_ptp_tx(efx, skb)))) {\n\t\t \n\t\tefx_tx_send_pending(efx_get_tx_channel(efx, index));\n\t\treturn efx_ptp_tx(efx, skb);\n\t}\n\n\ttx_queue = efx_get_tx_queue(efx, index, type);\n\tif (WARN_ON_ONCE(!tx_queue)) {\n\t\t \n\t\tdev_kfree_skb_any(skb);\n\t\t \n\t\tif (!netdev_xmit_more())\n\t\t\tefx_tx_send_pending(efx_get_tx_channel(efx, index));\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\treturn __efx_enqueue_skb(tx_queue, skb);\n}\n\nvoid efx_xmit_done_single(struct efx_tx_queue *tx_queue)\n{\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\tunsigned int efv_pkts_compl = 0;\n\tunsigned int read_ptr;\n\tbool finished = false;\n\n\tread_ptr = tx_queue->read_count & tx_queue->ptr_mask;\n\n\twhile (!finished) {\n\t\tstruct efx_tx_buffer *buffer = &tx_queue->buffer[read_ptr];\n\n\t\tif (!efx_tx_buffer_in_use(buffer)) {\n\t\t\tstruct efx_nic *efx = tx_queue->efx;\n\n\t\t\tnetif_err(efx, hw, efx->net_dev,\n\t\t\t\t  \"TX queue %d spurious single TX completion\\n\",\n\t\t\t\t  tx_queue->queue);\n\t\t\tefx_schedule_reset(efx, RESET_TYPE_TX_SKIP);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tif (buffer->flags & EFX_TX_BUF_SKB)\n\t\t\tfinished = true;\n\t\tefx_dequeue_buffer(tx_queue, buffer, &pkts_compl, &bytes_compl,\n\t\t\t\t   &efv_pkts_compl);\n\n\t\t++tx_queue->read_count;\n\t\tread_ptr = tx_queue->read_count & tx_queue->ptr_mask;\n\t}\n\n\ttx_queue->pkts_compl += pkts_compl;\n\ttx_queue->bytes_compl += bytes_compl;\n\n\tEFX_WARN_ON_PARANOID(pkts_compl + efv_pkts_compl != 1);\n\n\tefx_xmit_done_check_empty(tx_queue);\n}\n\nvoid efx_init_tx_queue_core_txq(struct efx_tx_queue *tx_queue)\n{\n\tstruct efx_nic *efx = tx_queue->efx;\n\n\t \n\ttx_queue->core_txq =\n\t\tnetdev_get_tx_queue(efx->net_dev,\n\t\t\t\t    tx_queue->channel->channel);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}