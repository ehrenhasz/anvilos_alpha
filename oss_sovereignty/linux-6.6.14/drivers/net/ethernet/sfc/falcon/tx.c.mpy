{
  "module_name": "tx.c",
  "hash_id": "9face377911e0f1916e7a7465138cc4ee0ab9cb99f725108e73111ae0a593f71",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/falcon/tx.c",
  "human_readable_source": "\n \n\n#include <linux/pci.h>\n#include <linux/tcp.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/ipv6.h>\n#include <linux/slab.h>\n#include <net/ipv6.h>\n#include <linux/if_ether.h>\n#include <linux/highmem.h>\n#include <linux/cache.h>\n#include \"net_driver.h\"\n#include \"efx.h\"\n#include \"io.h\"\n#include \"nic.h\"\n#include \"tx.h\"\n#include \"workarounds.h\"\n\nstatic inline u8 *ef4_tx_get_copy_buffer(struct ef4_tx_queue *tx_queue,\n\t\t\t\t\t struct ef4_tx_buffer *buffer)\n{\n\tunsigned int index = ef4_tx_queue_get_insert_index(tx_queue);\n\tstruct ef4_buffer *page_buf =\n\t\t&tx_queue->cb_page[index >> (PAGE_SHIFT - EF4_TX_CB_ORDER)];\n\tunsigned int offset =\n\t\t((index << EF4_TX_CB_ORDER) + NET_IP_ALIGN) & (PAGE_SIZE - 1);\n\n\tif (unlikely(!page_buf->addr) &&\n\t    ef4_nic_alloc_buffer(tx_queue->efx, page_buf, PAGE_SIZE,\n\t\t\t\t GFP_ATOMIC))\n\t\treturn NULL;\n\tbuffer->dma_addr = page_buf->dma_addr + offset;\n\tbuffer->unmap_len = 0;\n\treturn (u8 *)page_buf->addr + offset;\n}\n\nu8 *ef4_tx_get_copy_buffer_limited(struct ef4_tx_queue *tx_queue,\n\t\t\t\t   struct ef4_tx_buffer *buffer, size_t len)\n{\n\tif (len > EF4_TX_CB_SIZE)\n\t\treturn NULL;\n\treturn ef4_tx_get_copy_buffer(tx_queue, buffer);\n}\n\nstatic void ef4_dequeue_buffer(struct ef4_tx_queue *tx_queue,\n\t\t\t       struct ef4_tx_buffer *buffer,\n\t\t\t       unsigned int *pkts_compl,\n\t\t\t       unsigned int *bytes_compl)\n{\n\tif (buffer->unmap_len) {\n\t\tstruct device *dma_dev = &tx_queue->efx->pci_dev->dev;\n\t\tdma_addr_t unmap_addr = buffer->dma_addr - buffer->dma_offset;\n\t\tif (buffer->flags & EF4_TX_BUF_MAP_SINGLE)\n\t\t\tdma_unmap_single(dma_dev, unmap_addr, buffer->unmap_len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_page(dma_dev, unmap_addr, buffer->unmap_len,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tbuffer->unmap_len = 0;\n\t}\n\n\tif (buffer->flags & EF4_TX_BUF_SKB) {\n\t\t(*pkts_compl)++;\n\t\t(*bytes_compl) += buffer->skb->len;\n\t\tdev_consume_skb_any((struct sk_buff *)buffer->skb);\n\t\tnetif_vdbg(tx_queue->efx, tx_done, tx_queue->efx->net_dev,\n\t\t\t   \"TX queue %d transmission id %x complete\\n\",\n\t\t\t   tx_queue->queue, tx_queue->read_count);\n\t}\n\n\tbuffer->len = 0;\n\tbuffer->flags = 0;\n}\n\nunsigned int ef4_tx_max_skb_descs(struct ef4_nic *efx)\n{\n\t \n\t \n\tunsigned int max_descs = EF4_TSO_MAX_SEGS * 2 + MAX_SKB_FRAGS;\n\n\t \n\tif (EF4_WORKAROUND_5391(efx))\n\t\tmax_descs += EF4_TSO_MAX_SEGS;\n\n\t \n\tif (PAGE_SIZE > EF4_PAGE_SIZE)\n\t\tmax_descs += max_t(unsigned int, MAX_SKB_FRAGS,\n\t\t\t\t   DIV_ROUND_UP(GSO_LEGACY_MAX_SIZE,\n\t\t\t\t\t\tEF4_PAGE_SIZE));\n\n\treturn max_descs;\n}\n\nstatic void ef4_tx_maybe_stop_queue(struct ef4_tx_queue *txq1)\n{\n\t \n\tstruct ef4_tx_queue *txq2 = ef4_tx_queue_partner(txq1);\n\tstruct ef4_nic *efx = txq1->efx;\n\tunsigned int fill_level;\n\n\tfill_level = max(txq1->insert_count - txq1->old_read_count,\n\t\t\t txq2->insert_count - txq2->old_read_count);\n\tif (likely(fill_level < efx->txq_stop_thresh))\n\t\treturn;\n\n\t \n\tnetif_tx_stop_queue(txq1->core_txq);\n\tsmp_mb();\n\ttxq1->old_read_count = READ_ONCE(txq1->read_count);\n\ttxq2->old_read_count = READ_ONCE(txq2->read_count);\n\n\tfill_level = max(txq1->insert_count - txq1->old_read_count,\n\t\t\t txq2->insert_count - txq2->old_read_count);\n\tEF4_BUG_ON_PARANOID(fill_level >= efx->txq_entries);\n\tif (likely(fill_level < efx->txq_stop_thresh)) {\n\t\tsmp_mb();\n\t\tif (likely(!efx->loopback_selftest))\n\t\t\tnetif_tx_start_queue(txq1->core_txq);\n\t}\n}\n\nstatic int ef4_enqueue_skb_copy(struct ef4_tx_queue *tx_queue,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tunsigned int min_len = tx_queue->tx_min_size;\n\tunsigned int copy_len = skb->len;\n\tstruct ef4_tx_buffer *buffer;\n\tu8 *copy_buffer;\n\tint rc;\n\n\tEF4_BUG_ON_PARANOID(copy_len > EF4_TX_CB_SIZE);\n\n\tbuffer = ef4_tx_queue_get_insert_buffer(tx_queue);\n\n\tcopy_buffer = ef4_tx_get_copy_buffer(tx_queue, buffer);\n\tif (unlikely(!copy_buffer))\n\t\treturn -ENOMEM;\n\n\trc = skb_copy_bits(skb, 0, copy_buffer, copy_len);\n\tEF4_WARN_ON_PARANOID(rc);\n\tif (unlikely(copy_len < min_len)) {\n\t\tmemset(copy_buffer + copy_len, 0, min_len - copy_len);\n\t\tbuffer->len = min_len;\n\t} else {\n\t\tbuffer->len = copy_len;\n\t}\n\n\tbuffer->skb = skb;\n\tbuffer->flags = EF4_TX_BUF_SKB;\n\n\t++tx_queue->insert_count;\n\treturn rc;\n}\n\nstatic struct ef4_tx_buffer *ef4_tx_map_chunk(struct ef4_tx_queue *tx_queue,\n\t\t\t\t\t      dma_addr_t dma_addr,\n\t\t\t\t\t      size_t len)\n{\n\tconst struct ef4_nic_type *nic_type = tx_queue->efx->type;\n\tstruct ef4_tx_buffer *buffer;\n\tunsigned int dma_len;\n\n\t \n\tdo {\n\t\tbuffer = ef4_tx_queue_get_insert_buffer(tx_queue);\n\t\tdma_len = nic_type->tx_limit_len(tx_queue, dma_addr, len);\n\n\t\tbuffer->len = dma_len;\n\t\tbuffer->dma_addr = dma_addr;\n\t\tbuffer->flags = EF4_TX_BUF_CONT;\n\t\tlen -= dma_len;\n\t\tdma_addr += dma_len;\n\t\t++tx_queue->insert_count;\n\t} while (len);\n\n\treturn buffer;\n}\n\n \nstatic int ef4_tx_map_data(struct ef4_tx_queue *tx_queue, struct sk_buff *skb)\n{\n\tstruct ef4_nic *efx = tx_queue->efx;\n\tstruct device *dma_dev = &efx->pci_dev->dev;\n\tunsigned int frag_index, nr_frags;\n\tdma_addr_t dma_addr, unmap_addr;\n\tunsigned short dma_flags;\n\tsize_t len, unmap_len;\n\n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\tfrag_index = 0;\n\n\t \n\tlen = skb_headlen(skb);\n\tdma_addr = dma_map_single(dma_dev, skb->data, len, DMA_TO_DEVICE);\n\tdma_flags = EF4_TX_BUF_MAP_SINGLE;\n\tunmap_len = len;\n\tunmap_addr = dma_addr;\n\n\tif (unlikely(dma_mapping_error(dma_dev, dma_addr)))\n\t\treturn -EIO;\n\n\t \n\tdo {\n\t\tstruct ef4_tx_buffer *buffer;\n\t\tskb_frag_t *fragment;\n\n\t\tbuffer = ef4_tx_map_chunk(tx_queue, dma_addr, len);\n\n\t\t \n\t\tbuffer->flags = EF4_TX_BUF_CONT | dma_flags;\n\t\tbuffer->unmap_len = unmap_len;\n\t\tbuffer->dma_offset = buffer->dma_addr - unmap_addr;\n\n\t\tif (frag_index >= nr_frags) {\n\t\t\t \n\t\t\tbuffer->skb = skb;\n\t\t\tbuffer->flags = EF4_TX_BUF_SKB | dma_flags;\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tfragment = &skb_shinfo(skb)->frags[frag_index++];\n\t\tlen = skb_frag_size(fragment);\n\t\tdma_addr = skb_frag_dma_map(dma_dev, fragment,\n\t\t\t\t0, len, DMA_TO_DEVICE);\n\t\tdma_flags = 0;\n\t\tunmap_len = len;\n\t\tunmap_addr = dma_addr;\n\n\t\tif (unlikely(dma_mapping_error(dma_dev, dma_addr)))\n\t\t\treturn -EIO;\n\t} while (1);\n}\n\n \nstatic void ef4_enqueue_unwind(struct ef4_tx_queue *tx_queue)\n{\n\tstruct ef4_tx_buffer *buffer;\n\n\t \n\twhile (tx_queue->insert_count != tx_queue->write_count) {\n\t\t--tx_queue->insert_count;\n\t\tbuffer = __ef4_tx_queue_get_insert_buffer(tx_queue);\n\t\tef4_dequeue_buffer(tx_queue, buffer, NULL, NULL);\n\t}\n}\n\n \nnetdev_tx_t ef4_enqueue_skb(struct ef4_tx_queue *tx_queue, struct sk_buff *skb)\n{\n\tbool data_mapped = false;\n\tunsigned int skb_len;\n\n\tskb_len = skb->len;\n\tEF4_WARN_ON_PARANOID(skb_is_gso(skb));\n\n\tif (skb_len < tx_queue->tx_min_size ||\n\t\t\t(skb->data_len && skb_len <= EF4_TX_CB_SIZE)) {\n\t\t \n\t\tif (ef4_enqueue_skb_copy(tx_queue, skb))\n\t\t\tgoto err;\n\t\ttx_queue->cb_packets++;\n\t\tdata_mapped = true;\n\t}\n\n\t \n\tif (!data_mapped && (ef4_tx_map_data(tx_queue, skb)))\n\t\tgoto err;\n\n\t \n\tnetdev_tx_sent_queue(tx_queue->core_txq, skb_len);\n\n\t \n\tif (!netdev_xmit_more() || netif_xmit_stopped(tx_queue->core_txq)) {\n\t\tstruct ef4_tx_queue *txq2 = ef4_tx_queue_partner(tx_queue);\n\n\t\t \n\t\tif (txq2->xmit_more_available)\n\t\t\tef4_nic_push_buffers(txq2);\n\n\t\tef4_nic_push_buffers(tx_queue);\n\t} else {\n\t\ttx_queue->xmit_more_available = netdev_xmit_more();\n\t}\n\n\ttx_queue->tx_packets++;\n\n\tef4_tx_maybe_stop_queue(tx_queue);\n\n\treturn NETDEV_TX_OK;\n\n\nerr:\n\tef4_enqueue_unwind(tx_queue);\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic void ef4_dequeue_buffers(struct ef4_tx_queue *tx_queue,\n\t\t\t\tunsigned int index,\n\t\t\t\tunsigned int *pkts_compl,\n\t\t\t\tunsigned int *bytes_compl)\n{\n\tstruct ef4_nic *efx = tx_queue->efx;\n\tunsigned int stop_index, read_ptr;\n\n\tstop_index = (index + 1) & tx_queue->ptr_mask;\n\tread_ptr = tx_queue->read_count & tx_queue->ptr_mask;\n\n\twhile (read_ptr != stop_index) {\n\t\tstruct ef4_tx_buffer *buffer = &tx_queue->buffer[read_ptr];\n\n\t\tif (!(buffer->flags & EF4_TX_BUF_OPTION) &&\n\t\t    unlikely(buffer->len == 0)) {\n\t\t\tnetif_err(efx, tx_err, efx->net_dev,\n\t\t\t\t  \"TX queue %d spurious TX completion id %x\\n\",\n\t\t\t\t  tx_queue->queue, read_ptr);\n\t\t\tef4_schedule_reset(efx, RESET_TYPE_TX_SKIP);\n\t\t\treturn;\n\t\t}\n\n\t\tef4_dequeue_buffer(tx_queue, buffer, pkts_compl, bytes_compl);\n\n\t\t++tx_queue->read_count;\n\t\tread_ptr = tx_queue->read_count & tx_queue->ptr_mask;\n\t}\n}\n\n \nnetdev_tx_t ef4_hard_start_xmit(struct sk_buff *skb,\n\t\t\t\tstruct net_device *net_dev)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\tstruct ef4_tx_queue *tx_queue;\n\tunsigned index, type;\n\n\tEF4_WARN_ON_PARANOID(!netif_device_present(net_dev));\n\n\tindex = skb_get_queue_mapping(skb);\n\ttype = skb->ip_summed == CHECKSUM_PARTIAL ? EF4_TXQ_TYPE_OFFLOAD : 0;\n\tif (index >= efx->n_tx_channels) {\n\t\tindex -= efx->n_tx_channels;\n\t\ttype |= EF4_TXQ_TYPE_HIGHPRI;\n\t}\n\ttx_queue = ef4_get_tx_queue(efx, index, type);\n\n\treturn ef4_enqueue_skb(tx_queue, skb);\n}\n\nvoid ef4_init_tx_queue_core_txq(struct ef4_tx_queue *tx_queue)\n{\n\tstruct ef4_nic *efx = tx_queue->efx;\n\n\t \n\ttx_queue->core_txq =\n\t\tnetdev_get_tx_queue(efx->net_dev,\n\t\t\t\t    tx_queue->queue / EF4_TXQ_TYPES +\n\t\t\t\t    ((tx_queue->queue & EF4_TXQ_TYPE_HIGHPRI) ?\n\t\t\t\t     efx->n_tx_channels : 0));\n}\n\nint ef4_setup_tc(struct net_device *net_dev, enum tc_setup_type type,\n\t\t void *type_data)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\tstruct tc_mqprio_qopt *mqprio = type_data;\n\tstruct ef4_channel *channel;\n\tstruct ef4_tx_queue *tx_queue;\n\tunsigned tc, num_tc;\n\tint rc;\n\n\tif (type != TC_SETUP_QDISC_MQPRIO)\n\t\treturn -EOPNOTSUPP;\n\n\tnum_tc = mqprio->num_tc;\n\n\tif (ef4_nic_rev(efx) < EF4_REV_FALCON_B0 || num_tc > EF4_MAX_TX_TC)\n\t\treturn -EINVAL;\n\n\tmqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;\n\n\tif (num_tc == net_dev->num_tc)\n\t\treturn 0;\n\n\tfor (tc = 0; tc < num_tc; tc++) {\n\t\tnet_dev->tc_to_txq[tc].offset = tc * efx->n_tx_channels;\n\t\tnet_dev->tc_to_txq[tc].count = efx->n_tx_channels;\n\t}\n\n\tif (num_tc > net_dev->num_tc) {\n\t\t \n\t\tef4_for_each_channel(channel, efx) {\n\t\t\tef4_for_each_possible_channel_tx_queue(tx_queue,\n\t\t\t\t\t\t\t       channel) {\n\t\t\t\tif (!(tx_queue->queue & EF4_TXQ_TYPE_HIGHPRI))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!tx_queue->buffer) {\n\t\t\t\t\trc = ef4_probe_tx_queue(tx_queue);\n\t\t\t\t\tif (rc)\n\t\t\t\t\t\treturn rc;\n\t\t\t\t}\n\t\t\t\tif (!tx_queue->initialised)\n\t\t\t\t\tef4_init_tx_queue(tx_queue);\n\t\t\t\tef4_init_tx_queue_core_txq(tx_queue);\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tnet_dev->num_tc = num_tc;\n\t}\n\n\trc = netif_set_real_num_tx_queues(net_dev,\n\t\t\t\t\t  max_t(int, num_tc, 1) *\n\t\t\t\t\t  efx->n_tx_channels);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\n\tnet_dev->num_tc = num_tc;\n\treturn 0;\n}\n\nvoid ef4_xmit_done(struct ef4_tx_queue *tx_queue, unsigned int index)\n{\n\tunsigned fill_level;\n\tstruct ef4_nic *efx = tx_queue->efx;\n\tstruct ef4_tx_queue *txq2;\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\n\tEF4_BUG_ON_PARANOID(index > tx_queue->ptr_mask);\n\n\tef4_dequeue_buffers(tx_queue, index, &pkts_compl, &bytes_compl);\n\ttx_queue->pkts_compl += pkts_compl;\n\ttx_queue->bytes_compl += bytes_compl;\n\n\tif (pkts_compl > 1)\n\t\t++tx_queue->merge_events;\n\n\t \n\tsmp_mb();\n\tif (unlikely(netif_tx_queue_stopped(tx_queue->core_txq)) &&\n\t    likely(efx->port_enabled) &&\n\t    likely(netif_device_present(efx->net_dev))) {\n\t\ttxq2 = ef4_tx_queue_partner(tx_queue);\n\t\tfill_level = max(tx_queue->insert_count - tx_queue->read_count,\n\t\t\t\t txq2->insert_count - txq2->read_count);\n\t\tif (fill_level <= efx->txq_wake_thresh)\n\t\t\tnetif_tx_wake_queue(tx_queue->core_txq);\n\t}\n\n\t \n\tif ((int)(tx_queue->read_count - tx_queue->old_write_count) >= 0) {\n\t\ttx_queue->old_write_count = READ_ONCE(tx_queue->write_count);\n\t\tif (tx_queue->read_count == tx_queue->old_write_count) {\n\t\t\tsmp_mb();\n\t\t\ttx_queue->empty_read_count =\n\t\t\t\ttx_queue->read_count | EF4_EMPTY_COUNT_VALID;\n\t\t}\n\t}\n}\n\nstatic unsigned int ef4_tx_cb_page_count(struct ef4_tx_queue *tx_queue)\n{\n\treturn DIV_ROUND_UP(tx_queue->ptr_mask + 1, PAGE_SIZE >> EF4_TX_CB_ORDER);\n}\n\nint ef4_probe_tx_queue(struct ef4_tx_queue *tx_queue)\n{\n\tstruct ef4_nic *efx = tx_queue->efx;\n\tunsigned int entries;\n\tint rc;\n\n\t \n\tentries = max(roundup_pow_of_two(efx->txq_entries), EF4_MIN_DMAQ_SIZE);\n\tEF4_BUG_ON_PARANOID(entries > EF4_MAX_DMAQ_SIZE);\n\ttx_queue->ptr_mask = entries - 1;\n\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"creating TX queue %d size %#x mask %#x\\n\",\n\t\t  tx_queue->queue, efx->txq_entries, tx_queue->ptr_mask);\n\n\t \n\ttx_queue->buffer = kcalloc(entries, sizeof(*tx_queue->buffer),\n\t\t\t\t   GFP_KERNEL);\n\tif (!tx_queue->buffer)\n\t\treturn -ENOMEM;\n\n\ttx_queue->cb_page = kcalloc(ef4_tx_cb_page_count(tx_queue),\n\t\t\t\t    sizeof(tx_queue->cb_page[0]), GFP_KERNEL);\n\tif (!tx_queue->cb_page) {\n\t\trc = -ENOMEM;\n\t\tgoto fail1;\n\t}\n\n\t \n\trc = ef4_nic_probe_tx(tx_queue);\n\tif (rc)\n\t\tgoto fail2;\n\n\treturn 0;\n\nfail2:\n\tkfree(tx_queue->cb_page);\n\ttx_queue->cb_page = NULL;\nfail1:\n\tkfree(tx_queue->buffer);\n\ttx_queue->buffer = NULL;\n\treturn rc;\n}\n\nvoid ef4_init_tx_queue(struct ef4_tx_queue *tx_queue)\n{\n\tstruct ef4_nic *efx = tx_queue->efx;\n\n\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t  \"initialising TX queue %d\\n\", tx_queue->queue);\n\n\ttx_queue->insert_count = 0;\n\ttx_queue->write_count = 0;\n\ttx_queue->old_write_count = 0;\n\ttx_queue->read_count = 0;\n\ttx_queue->old_read_count = 0;\n\ttx_queue->empty_read_count = 0 | EF4_EMPTY_COUNT_VALID;\n\ttx_queue->xmit_more_available = false;\n\n\t \n\ttx_queue->tx_min_size = EF4_WORKAROUND_15592(efx) ? 33 : 0;\n\n\t \n\tef4_nic_init_tx(tx_queue);\n\n\ttx_queue->initialised = true;\n}\n\nvoid ef4_fini_tx_queue(struct ef4_tx_queue *tx_queue)\n{\n\tstruct ef4_tx_buffer *buffer;\n\n\tnetif_dbg(tx_queue->efx, drv, tx_queue->efx->net_dev,\n\t\t  \"shutting down TX queue %d\\n\", tx_queue->queue);\n\n\tif (!tx_queue->buffer)\n\t\treturn;\n\n\t \n\twhile (tx_queue->read_count != tx_queue->write_count) {\n\t\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\t\tbuffer = &tx_queue->buffer[tx_queue->read_count & tx_queue->ptr_mask];\n\t\tef4_dequeue_buffer(tx_queue, buffer, &pkts_compl, &bytes_compl);\n\n\t\t++tx_queue->read_count;\n\t}\n\ttx_queue->xmit_more_available = false;\n\tnetdev_tx_reset_queue(tx_queue->core_txq);\n}\n\nvoid ef4_remove_tx_queue(struct ef4_tx_queue *tx_queue)\n{\n\tint i;\n\n\tif (!tx_queue->buffer)\n\t\treturn;\n\n\tnetif_dbg(tx_queue->efx, drv, tx_queue->efx->net_dev,\n\t\t  \"destroying TX queue %d\\n\", tx_queue->queue);\n\tef4_nic_remove_tx(tx_queue);\n\n\tif (tx_queue->cb_page) {\n\t\tfor (i = 0; i < ef4_tx_cb_page_count(tx_queue); i++)\n\t\t\tef4_nic_free_buffer(tx_queue->efx,\n\t\t\t\t\t    &tx_queue->cb_page[i]);\n\t\tkfree(tx_queue->cb_page);\n\t\ttx_queue->cb_page = NULL;\n\t}\n\n\tkfree(tx_queue->buffer);\n\ttx_queue->buffer = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}