{
  "module_name": "efx.c",
  "hash_id": "e299807233384866102045b40aca2faf00f6e7eedb94f8c1f2713266a70516e3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/falcon/efx.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/delay.h>\n#include <linux/notifier.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/in.h>\n#include <linux/ethtool.h>\n#include <linux/topology.h>\n#include <linux/gfp.h>\n#include <linux/interrupt.h>\n#include \"net_driver.h\"\n#include \"efx.h\"\n#include \"nic.h\"\n#include \"selftest.h\"\n\n#include \"workarounds.h\"\n\n \n\n \nconst unsigned int ef4_loopback_mode_max = LOOPBACK_MAX;\nconst char *const ef4_loopback_mode_names[] = {\n\t[LOOPBACK_NONE]\t\t= \"NONE\",\n\t[LOOPBACK_DATA]\t\t= \"DATAPATH\",\n\t[LOOPBACK_GMAC]\t\t= \"GMAC\",\n\t[LOOPBACK_XGMII]\t= \"XGMII\",\n\t[LOOPBACK_XGXS]\t\t= \"XGXS\",\n\t[LOOPBACK_XAUI]\t\t= \"XAUI\",\n\t[LOOPBACK_GMII]\t\t= \"GMII\",\n\t[LOOPBACK_SGMII]\t= \"SGMII\",\n\t[LOOPBACK_XGBR]\t\t= \"XGBR\",\n\t[LOOPBACK_XFI]\t\t= \"XFI\",\n\t[LOOPBACK_XAUI_FAR]\t= \"XAUI_FAR\",\n\t[LOOPBACK_GMII_FAR]\t= \"GMII_FAR\",\n\t[LOOPBACK_SGMII_FAR]\t= \"SGMII_FAR\",\n\t[LOOPBACK_XFI_FAR]\t= \"XFI_FAR\",\n\t[LOOPBACK_GPHY]\t\t= \"GPHY\",\n\t[LOOPBACK_PHYXS]\t= \"PHYXS\",\n\t[LOOPBACK_PCS]\t\t= \"PCS\",\n\t[LOOPBACK_PMAPMD]\t= \"PMA/PMD\",\n\t[LOOPBACK_XPORT]\t= \"XPORT\",\n\t[LOOPBACK_XGMII_WS]\t= \"XGMII_WS\",\n\t[LOOPBACK_XAUI_WS]\t= \"XAUI_WS\",\n\t[LOOPBACK_XAUI_WS_FAR]  = \"XAUI_WS_FAR\",\n\t[LOOPBACK_XAUI_WS_NEAR] = \"XAUI_WS_NEAR\",\n\t[LOOPBACK_GMII_WS]\t= \"GMII_WS\",\n\t[LOOPBACK_XFI_WS]\t= \"XFI_WS\",\n\t[LOOPBACK_XFI_WS_FAR]\t= \"XFI_WS_FAR\",\n\t[LOOPBACK_PHYXS_WS]\t= \"PHYXS_WS\",\n};\n\nconst unsigned int ef4_reset_type_max = RESET_TYPE_MAX;\nconst char *const ef4_reset_type_names[] = {\n\t[RESET_TYPE_INVISIBLE]          = \"INVISIBLE\",\n\t[RESET_TYPE_ALL]                = \"ALL\",\n\t[RESET_TYPE_RECOVER_OR_ALL]     = \"RECOVER_OR_ALL\",\n\t[RESET_TYPE_WORLD]              = \"WORLD\",\n\t[RESET_TYPE_RECOVER_OR_DISABLE] = \"RECOVER_OR_DISABLE\",\n\t[RESET_TYPE_DATAPATH]           = \"DATAPATH\",\n\t[RESET_TYPE_DISABLE]            = \"DISABLE\",\n\t[RESET_TYPE_TX_WATCHDOG]        = \"TX_WATCHDOG\",\n\t[RESET_TYPE_INT_ERROR]          = \"INT_ERROR\",\n\t[RESET_TYPE_RX_RECOVERY]        = \"RX_RECOVERY\",\n\t[RESET_TYPE_DMA_ERROR]          = \"DMA_ERROR\",\n\t[RESET_TYPE_TX_SKIP]            = \"TX_SKIP\",\n};\n\n \nstatic struct workqueue_struct *reset_workqueue;\n\n \n#define BIST_WAIT_DELAY_MS\t100\n#define BIST_WAIT_DELAY_COUNT\t100\n\n \n\n \nbool ef4_separate_tx_channels;\nmodule_param(ef4_separate_tx_channels, bool, 0444);\nMODULE_PARM_DESC(ef4_separate_tx_channels,\n\t\t \"Use separate channels for TX and RX\");\n\n \nstatic unsigned int ef4_monitor_interval = 1 * HZ;\n\n \nstatic unsigned int rx_irq_mod_usec = 60;\n\n \nstatic unsigned int tx_irq_mod_usec = 150;\n\n \nstatic unsigned int interrupt_mode;\n\n \nstatic unsigned int rss_cpus;\nmodule_param(rss_cpus, uint, 0444);\nMODULE_PARM_DESC(rss_cpus, \"Number of CPUs to use for Receive-Side Scaling\");\n\nstatic bool phy_flash_cfg;\nmodule_param(phy_flash_cfg, bool, 0644);\nMODULE_PARM_DESC(phy_flash_cfg, \"Set PHYs into reflash mode initially\");\n\nstatic unsigned irq_adapt_low_thresh = 8000;\nmodule_param(irq_adapt_low_thresh, uint, 0644);\nMODULE_PARM_DESC(irq_adapt_low_thresh,\n\t\t \"Threshold score for reducing IRQ moderation\");\n\nstatic unsigned irq_adapt_high_thresh = 16000;\nmodule_param(irq_adapt_high_thresh, uint, 0644);\nMODULE_PARM_DESC(irq_adapt_high_thresh,\n\t\t \"Threshold score for increasing IRQ moderation\");\n\nstatic unsigned debug = (NETIF_MSG_DRV | NETIF_MSG_PROBE |\n\t\t\t NETIF_MSG_LINK | NETIF_MSG_IFDOWN |\n\t\t\t NETIF_MSG_IFUP | NETIF_MSG_RX_ERR |\n\t\t\t NETIF_MSG_TX_ERR | NETIF_MSG_HW);\nmodule_param(debug, uint, 0);\nMODULE_PARM_DESC(debug, \"Bitmapped debugging message enable value\");\n\n \n\nstatic int ef4_soft_enable_interrupts(struct ef4_nic *efx);\nstatic void ef4_soft_disable_interrupts(struct ef4_nic *efx);\nstatic void ef4_remove_channel(struct ef4_channel *channel);\nstatic void ef4_remove_channels(struct ef4_nic *efx);\nstatic const struct ef4_channel_type ef4_default_channel_type;\nstatic void ef4_remove_port(struct ef4_nic *efx);\nstatic void ef4_init_napi_channel(struct ef4_channel *channel);\nstatic void ef4_fini_napi(struct ef4_nic *efx);\nstatic void ef4_fini_napi_channel(struct ef4_channel *channel);\nstatic void ef4_fini_struct(struct ef4_nic *efx);\nstatic void ef4_start_all(struct ef4_nic *efx);\nstatic void ef4_stop_all(struct ef4_nic *efx);\n\n#define EF4_ASSERT_RESET_SERIALISED(efx)\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif ((efx->state == STATE_READY) ||\t\\\n\t\t    (efx->state == STATE_RECOVERY) ||\t\\\n\t\t    (efx->state == STATE_DISABLED))\t\\\n\t\t\tASSERT_RTNL();\t\t\t\\\n\t} while (0)\n\nstatic int ef4_check_disabled(struct ef4_nic *efx)\n{\n\tif (efx->state == STATE_DISABLED || efx->state == STATE_RECOVERY) {\n\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t  \"device is disabled due to earlier errors\\n\");\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\n \n\n \nstatic int ef4_process_channel(struct ef4_channel *channel, int budget)\n{\n\tstruct ef4_tx_queue *tx_queue;\n\tint spent;\n\n\tif (unlikely(!channel->enabled))\n\t\treturn 0;\n\n\tef4_for_each_channel_tx_queue(tx_queue, channel) {\n\t\ttx_queue->pkts_compl = 0;\n\t\ttx_queue->bytes_compl = 0;\n\t}\n\n\tspent = ef4_nic_process_eventq(channel, budget);\n\tif (spent && ef4_channel_has_rx_queue(channel)) {\n\t\tstruct ef4_rx_queue *rx_queue =\n\t\t\tef4_channel_get_rx_queue(channel);\n\n\t\tef4_rx_flush_packet(channel);\n\t\tef4_fast_push_rx_descriptors(rx_queue, true);\n\t}\n\n\t \n\tef4_for_each_channel_tx_queue(tx_queue, channel) {\n\t\tif (tx_queue->bytes_compl) {\n\t\t\tnetdev_tx_completed_queue(tx_queue->core_txq,\n\t\t\t\ttx_queue->pkts_compl, tx_queue->bytes_compl);\n\t\t}\n\t}\n\n\treturn spent;\n}\n\n \nstatic void ef4_update_irq_mod(struct ef4_nic *efx, struct ef4_channel *channel)\n{\n\tint step = efx->irq_mod_step_us;\n\n\tif (channel->irq_mod_score < irq_adapt_low_thresh) {\n\t\tif (channel->irq_moderation_us > step) {\n\t\t\tchannel->irq_moderation_us -= step;\n\t\t\tefx->type->push_irq_moderation(channel);\n\t\t}\n\t} else if (channel->irq_mod_score > irq_adapt_high_thresh) {\n\t\tif (channel->irq_moderation_us <\n\t\t    efx->irq_rx_moderation_us) {\n\t\t\tchannel->irq_moderation_us += step;\n\t\t\tefx->type->push_irq_moderation(channel);\n\t\t}\n\t}\n\n\tchannel->irq_count = 0;\n\tchannel->irq_mod_score = 0;\n}\n\nstatic int ef4_poll(struct napi_struct *napi, int budget)\n{\n\tstruct ef4_channel *channel =\n\t\tcontainer_of(napi, struct ef4_channel, napi_str);\n\tstruct ef4_nic *efx = channel->efx;\n\tint spent;\n\n\tnetif_vdbg(efx, intr, efx->net_dev,\n\t\t   \"channel %d NAPI poll executing on CPU %d\\n\",\n\t\t   channel->channel, raw_smp_processor_id());\n\n\tspent = ef4_process_channel(channel, budget);\n\n\tif (spent < budget) {\n\t\tif (ef4_channel_has_rx_queue(channel) &&\n\t\t    efx->irq_rx_adaptive &&\n\t\t    unlikely(++channel->irq_count == 1000)) {\n\t\t\tef4_update_irq_mod(efx, channel);\n\t\t}\n\n\t\tef4_filter_rfs_expire(channel);\n\n\t\t \n\t\tnapi_complete_done(napi, spent);\n\t\tef4_nic_eventq_read_ack(channel);\n\t}\n\n\treturn spent;\n}\n\n \nstatic int ef4_probe_eventq(struct ef4_channel *channel)\n{\n\tstruct ef4_nic *efx = channel->efx;\n\tunsigned long entries;\n\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"chan %d create event queue\\n\", channel->channel);\n\n\t \n\tentries = roundup_pow_of_two(efx->rxq_entries + efx->txq_entries + 128);\n\tEF4_BUG_ON_PARANOID(entries > EF4_MAX_EVQ_SIZE);\n\tchannel->eventq_mask = max(entries, EF4_MIN_EVQ_SIZE) - 1;\n\n\treturn ef4_nic_probe_eventq(channel);\n}\n\n \nstatic int ef4_init_eventq(struct ef4_channel *channel)\n{\n\tstruct ef4_nic *efx = channel->efx;\n\tint rc;\n\n\tEF4_WARN_ON_PARANOID(channel->eventq_init);\n\n\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t  \"chan %d init event queue\\n\", channel->channel);\n\n\trc = ef4_nic_init_eventq(channel);\n\tif (rc == 0) {\n\t\tefx->type->push_irq_moderation(channel);\n\t\tchannel->eventq_read_ptr = 0;\n\t\tchannel->eventq_init = true;\n\t}\n\treturn rc;\n}\n\n \nvoid ef4_start_eventq(struct ef4_channel *channel)\n{\n\tnetif_dbg(channel->efx, ifup, channel->efx->net_dev,\n\t\t  \"chan %d start event queue\\n\", channel->channel);\n\n\t \n\tchannel->enabled = true;\n\tsmp_wmb();\n\n\tnapi_enable(&channel->napi_str);\n\tef4_nic_eventq_read_ack(channel);\n}\n\n \nvoid ef4_stop_eventq(struct ef4_channel *channel)\n{\n\tif (!channel->enabled)\n\t\treturn;\n\n\tnapi_disable(&channel->napi_str);\n\tchannel->enabled = false;\n}\n\nstatic void ef4_fini_eventq(struct ef4_channel *channel)\n{\n\tif (!channel->eventq_init)\n\t\treturn;\n\n\tnetif_dbg(channel->efx, drv, channel->efx->net_dev,\n\t\t  \"chan %d fini event queue\\n\", channel->channel);\n\n\tef4_nic_fini_eventq(channel);\n\tchannel->eventq_init = false;\n}\n\nstatic void ef4_remove_eventq(struct ef4_channel *channel)\n{\n\tnetif_dbg(channel->efx, drv, channel->efx->net_dev,\n\t\t  \"chan %d remove event queue\\n\", channel->channel);\n\n\tef4_nic_remove_eventq(channel);\n}\n\n \n\n \nstatic struct ef4_channel *\nef4_alloc_channel(struct ef4_nic *efx, int i, struct ef4_channel *old_channel)\n{\n\tstruct ef4_channel *channel;\n\tstruct ef4_rx_queue *rx_queue;\n\tstruct ef4_tx_queue *tx_queue;\n\tint j;\n\n\tchannel = kzalloc(sizeof(*channel), GFP_KERNEL);\n\tif (!channel)\n\t\treturn NULL;\n\n\tchannel->efx = efx;\n\tchannel->channel = i;\n\tchannel->type = &ef4_default_channel_type;\n\n\tfor (j = 0; j < EF4_TXQ_TYPES; j++) {\n\t\ttx_queue = &channel->tx_queue[j];\n\t\ttx_queue->efx = efx;\n\t\ttx_queue->queue = i * EF4_TXQ_TYPES + j;\n\t\ttx_queue->channel = channel;\n\t}\n\n\trx_queue = &channel->rx_queue;\n\trx_queue->efx = efx;\n\ttimer_setup(&rx_queue->slow_fill, ef4_rx_slow_fill, 0);\n\n\treturn channel;\n}\n\n \nstatic struct ef4_channel *\nef4_copy_channel(const struct ef4_channel *old_channel)\n{\n\tstruct ef4_channel *channel;\n\tstruct ef4_rx_queue *rx_queue;\n\tstruct ef4_tx_queue *tx_queue;\n\tint j;\n\n\tchannel = kmalloc(sizeof(*channel), GFP_KERNEL);\n\tif (!channel)\n\t\treturn NULL;\n\n\t*channel = *old_channel;\n\n\tchannel->napi_dev = NULL;\n\tINIT_HLIST_NODE(&channel->napi_str.napi_hash_node);\n\tchannel->napi_str.napi_id = 0;\n\tchannel->napi_str.state = 0;\n\tmemset(&channel->eventq, 0, sizeof(channel->eventq));\n\n\tfor (j = 0; j < EF4_TXQ_TYPES; j++) {\n\t\ttx_queue = &channel->tx_queue[j];\n\t\tif (tx_queue->channel)\n\t\t\ttx_queue->channel = channel;\n\t\ttx_queue->buffer = NULL;\n\t\tmemset(&tx_queue->txd, 0, sizeof(tx_queue->txd));\n\t}\n\n\trx_queue = &channel->rx_queue;\n\trx_queue->buffer = NULL;\n\tmemset(&rx_queue->rxd, 0, sizeof(rx_queue->rxd));\n\ttimer_setup(&rx_queue->slow_fill, ef4_rx_slow_fill, 0);\n\n\treturn channel;\n}\n\nstatic int ef4_probe_channel(struct ef4_channel *channel)\n{\n\tstruct ef4_tx_queue *tx_queue;\n\tstruct ef4_rx_queue *rx_queue;\n\tint rc;\n\n\tnetif_dbg(channel->efx, probe, channel->efx->net_dev,\n\t\t  \"creating channel %d\\n\", channel->channel);\n\n\trc = channel->type->pre_probe(channel);\n\tif (rc)\n\t\tgoto fail;\n\n\trc = ef4_probe_eventq(channel);\n\tif (rc)\n\t\tgoto fail;\n\n\tef4_for_each_channel_tx_queue(tx_queue, channel) {\n\t\trc = ef4_probe_tx_queue(tx_queue);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\tef4_for_each_channel_rx_queue(rx_queue, channel) {\n\t\trc = ef4_probe_rx_queue(rx_queue);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\treturn 0;\n\nfail:\n\tef4_remove_channel(channel);\n\treturn rc;\n}\n\nstatic void\nef4_get_channel_name(struct ef4_channel *channel, char *buf, size_t len)\n{\n\tstruct ef4_nic *efx = channel->efx;\n\tconst char *type;\n\tint number;\n\n\tnumber = channel->channel;\n\tif (efx->tx_channel_offset == 0) {\n\t\ttype = \"\";\n\t} else if (channel->channel < efx->tx_channel_offset) {\n\t\ttype = \"-rx\";\n\t} else {\n\t\ttype = \"-tx\";\n\t\tnumber -= efx->tx_channel_offset;\n\t}\n\tsnprintf(buf, len, \"%s%s-%d\", efx->name, type, number);\n}\n\nstatic void ef4_set_channel_names(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\n\tef4_for_each_channel(channel, efx)\n\t\tchannel->type->get_name(channel,\n\t\t\t\t\tefx->msi_context[channel->channel].name,\n\t\t\t\t\tsizeof(efx->msi_context[0].name));\n}\n\nstatic int ef4_probe_channels(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\tint rc;\n\n\t \n\tefx->next_buffer_table = 0;\n\n\t \n\tef4_for_each_channel_rev(channel, efx) {\n\t\trc = ef4_probe_channel(channel);\n\t\tif (rc) {\n\t\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t\t  \"failed to create channel %d\\n\",\n\t\t\t\t  channel->channel);\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tef4_set_channel_names(efx);\n\n\treturn 0;\n\nfail:\n\tef4_remove_channels(efx);\n\treturn rc;\n}\n\n \nstatic void ef4_start_datapath(struct ef4_nic *efx)\n{\n\tnetdev_features_t old_features = efx->net_dev->features;\n\tbool old_rx_scatter = efx->rx_scatter;\n\tstruct ef4_tx_queue *tx_queue;\n\tstruct ef4_rx_queue *rx_queue;\n\tstruct ef4_channel *channel;\n\tsize_t rx_buf_len;\n\n\t \n\tefx->rx_dma_len = (efx->rx_prefix_size +\n\t\t\t   EF4_MAX_FRAME_LEN(efx->net_dev->mtu) +\n\t\t\t   efx->type->rx_buffer_padding);\n\trx_buf_len = (sizeof(struct ef4_rx_page_state) +\n\t\t      efx->rx_ip_align + efx->rx_dma_len);\n\tif (rx_buf_len <= PAGE_SIZE) {\n\t\tefx->rx_scatter = efx->type->always_rx_scatter;\n\t\tefx->rx_buffer_order = 0;\n\t} else if (efx->type->can_rx_scatter) {\n\t\tBUILD_BUG_ON(EF4_RX_USR_BUF_SIZE % L1_CACHE_BYTES);\n\t\tBUILD_BUG_ON(sizeof(struct ef4_rx_page_state) +\n\t\t\t     2 * ALIGN(NET_IP_ALIGN + EF4_RX_USR_BUF_SIZE,\n\t\t\t\t       EF4_RX_BUF_ALIGNMENT) >\n\t\t\t     PAGE_SIZE);\n\t\tefx->rx_scatter = true;\n\t\tefx->rx_dma_len = EF4_RX_USR_BUF_SIZE;\n\t\tefx->rx_buffer_order = 0;\n\t} else {\n\t\tefx->rx_scatter = false;\n\t\tefx->rx_buffer_order = get_order(rx_buf_len);\n\t}\n\n\tef4_rx_config_page_split(efx);\n\tif (efx->rx_buffer_order)\n\t\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t\t  \"RX buf len=%u; page order=%u batch=%u\\n\",\n\t\t\t  efx->rx_dma_len, efx->rx_buffer_order,\n\t\t\t  efx->rx_pages_per_batch);\n\telse\n\t\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t\t  \"RX buf len=%u step=%u bpp=%u; page batch=%u\\n\",\n\t\t\t  efx->rx_dma_len, efx->rx_page_buf_step,\n\t\t\t  efx->rx_bufs_per_page, efx->rx_pages_per_batch);\n\n\t \n\tefx->net_dev->hw_features |= efx->net_dev->features;\n\tefx->net_dev->hw_features &= ~efx->fixed_features;\n\tefx->net_dev->features |= efx->fixed_features;\n\tif (efx->net_dev->features != old_features)\n\t\tnetdev_features_change(efx->net_dev);\n\n\t \n\tif (efx->rx_scatter != old_rx_scatter)\n\t\tefx->type->filter_update_rx_scatter(efx);\n\n\t \n\tefx->txq_stop_thresh = efx->txq_entries - ef4_tx_max_skb_descs(efx);\n\tefx->txq_wake_thresh = efx->txq_stop_thresh / 2;\n\n\t \n\tef4_for_each_channel(channel, efx) {\n\t\tef4_for_each_channel_tx_queue(tx_queue, channel) {\n\t\t\tef4_init_tx_queue(tx_queue);\n\t\t\tatomic_inc(&efx->active_queues);\n\t\t}\n\n\t\tef4_for_each_channel_rx_queue(rx_queue, channel) {\n\t\t\tef4_init_rx_queue(rx_queue);\n\t\t\tatomic_inc(&efx->active_queues);\n\t\t\tef4_stop_eventq(channel);\n\t\t\tef4_fast_push_rx_descriptors(rx_queue, false);\n\t\t\tef4_start_eventq(channel);\n\t\t}\n\n\t\tWARN_ON(channel->rx_pkt_n_frags);\n\t}\n\n\tif (netif_device_present(efx->net_dev))\n\t\tnetif_tx_wake_all_queues(efx->net_dev);\n}\n\nstatic void ef4_stop_datapath(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\tstruct ef4_tx_queue *tx_queue;\n\tstruct ef4_rx_queue *rx_queue;\n\tint rc;\n\n\tEF4_ASSERT_RESET_SERIALISED(efx);\n\tBUG_ON(efx->port_enabled);\n\n\t \n\tef4_for_each_channel(channel, efx) {\n\t\tef4_for_each_channel_rx_queue(rx_queue, channel)\n\t\t\trx_queue->refill_enabled = false;\n\t}\n\n\tef4_for_each_channel(channel, efx) {\n\t\t \n\t\tif (ef4_channel_has_rx_queue(channel)) {\n\t\t\tef4_stop_eventq(channel);\n\t\t\tef4_start_eventq(channel);\n\t\t}\n\t}\n\n\trc = efx->type->fini_dmaq(efx);\n\tif (rc && EF4_WORKAROUND_7803(efx)) {\n\t\t \n\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t  \"Resetting to recover from flush failure\\n\");\n\t\tef4_schedule_reset(efx, RESET_TYPE_ALL);\n\t} else if (rc) {\n\t\tnetif_err(efx, drv, efx->net_dev, \"failed to flush queues\\n\");\n\t} else {\n\t\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t\t  \"successfully flushed all queues\\n\");\n\t}\n\n\tef4_for_each_channel(channel, efx) {\n\t\tef4_for_each_channel_rx_queue(rx_queue, channel)\n\t\t\tef4_fini_rx_queue(rx_queue);\n\t\tef4_for_each_possible_channel_tx_queue(tx_queue, channel)\n\t\t\tef4_fini_tx_queue(tx_queue);\n\t}\n}\n\nstatic void ef4_remove_channel(struct ef4_channel *channel)\n{\n\tstruct ef4_tx_queue *tx_queue;\n\tstruct ef4_rx_queue *rx_queue;\n\n\tnetif_dbg(channel->efx, drv, channel->efx->net_dev,\n\t\t  \"destroy chan %d\\n\", channel->channel);\n\n\tef4_for_each_channel_rx_queue(rx_queue, channel)\n\t\tef4_remove_rx_queue(rx_queue);\n\tef4_for_each_possible_channel_tx_queue(tx_queue, channel)\n\t\tef4_remove_tx_queue(tx_queue);\n\tef4_remove_eventq(channel);\n\tchannel->type->post_remove(channel);\n}\n\nstatic void ef4_remove_channels(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\n\tef4_for_each_channel(channel, efx)\n\t\tef4_remove_channel(channel);\n}\n\nint\nef4_realloc_channels(struct ef4_nic *efx, u32 rxq_entries, u32 txq_entries)\n{\n\tstruct ef4_channel *other_channel[EF4_MAX_CHANNELS], *channel;\n\tu32 old_rxq_entries, old_txq_entries;\n\tunsigned i, next_buffer_table = 0;\n\tint rc, rc2;\n\n\trc = ef4_check_disabled(efx);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tef4_for_each_channel(channel, efx) {\n\t\tstruct ef4_rx_queue *rx_queue;\n\t\tstruct ef4_tx_queue *tx_queue;\n\n\t\tif (channel->type->copy)\n\t\t\tcontinue;\n\t\tnext_buffer_table = max(next_buffer_table,\n\t\t\t\t\tchannel->eventq.index +\n\t\t\t\t\tchannel->eventq.entries);\n\t\tef4_for_each_channel_rx_queue(rx_queue, channel)\n\t\t\tnext_buffer_table = max(next_buffer_table,\n\t\t\t\t\t\trx_queue->rxd.index +\n\t\t\t\t\t\trx_queue->rxd.entries);\n\t\tef4_for_each_channel_tx_queue(tx_queue, channel)\n\t\t\tnext_buffer_table = max(next_buffer_table,\n\t\t\t\t\t\ttx_queue->txd.index +\n\t\t\t\t\t\ttx_queue->txd.entries);\n\t}\n\n\tef4_device_detach_sync(efx);\n\tef4_stop_all(efx);\n\tef4_soft_disable_interrupts(efx);\n\n\t \n\tmemset(other_channel, 0, sizeof(other_channel));\n\tfor (i = 0; i < efx->n_channels; i++) {\n\t\tchannel = efx->channel[i];\n\t\tif (channel->type->copy)\n\t\t\tchannel = channel->type->copy(channel);\n\t\tif (!channel) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tother_channel[i] = channel;\n\t}\n\n\t \n\told_rxq_entries = efx->rxq_entries;\n\told_txq_entries = efx->txq_entries;\n\tefx->rxq_entries = rxq_entries;\n\tefx->txq_entries = txq_entries;\n\tfor (i = 0; i < efx->n_channels; i++) {\n\t\tswap(efx->channel[i], other_channel[i]);\n\t}\n\n\t \n\tefx->next_buffer_table = next_buffer_table;\n\n\tfor (i = 0; i < efx->n_channels; i++) {\n\t\tchannel = efx->channel[i];\n\t\tif (!channel->type->copy)\n\t\t\tcontinue;\n\t\trc = ef4_probe_channel(channel);\n\t\tif (rc)\n\t\t\tgoto rollback;\n\t\tef4_init_napi_channel(efx->channel[i]);\n\t}\n\nout:\n\t \n\tfor (i = 0; i < efx->n_channels; i++) {\n\t\tchannel = other_channel[i];\n\t\tif (channel && channel->type->copy) {\n\t\t\tef4_fini_napi_channel(channel);\n\t\t\tef4_remove_channel(channel);\n\t\t\tkfree(channel);\n\t\t}\n\t}\n\n\trc2 = ef4_soft_enable_interrupts(efx);\n\tif (rc2) {\n\t\trc = rc ? rc : rc2;\n\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t  \"unable to restart interrupts on channel reallocation\\n\");\n\t\tef4_schedule_reset(efx, RESET_TYPE_DISABLE);\n\t} else {\n\t\tef4_start_all(efx);\n\t\tnetif_device_attach(efx->net_dev);\n\t}\n\treturn rc;\n\nrollback:\n\t \n\tefx->rxq_entries = old_rxq_entries;\n\tefx->txq_entries = old_txq_entries;\n\tfor (i = 0; i < efx->n_channels; i++) {\n\t\tswap(efx->channel[i], other_channel[i]);\n\t}\n\tgoto out;\n}\n\nvoid ef4_schedule_slow_fill(struct ef4_rx_queue *rx_queue)\n{\n\tmod_timer(&rx_queue->slow_fill, jiffies + msecs_to_jiffies(100));\n}\n\nstatic const struct ef4_channel_type ef4_default_channel_type = {\n\t.pre_probe\t\t= ef4_channel_dummy_op_int,\n\t.post_remove\t\t= ef4_channel_dummy_op_void,\n\t.get_name\t\t= ef4_get_channel_name,\n\t.copy\t\t\t= ef4_copy_channel,\n\t.keep_eventq\t\t= false,\n};\n\nint ef4_channel_dummy_op_int(struct ef4_channel *channel)\n{\n\treturn 0;\n}\n\nvoid ef4_channel_dummy_op_void(struct ef4_channel *channel)\n{\n}\n\n \n\n \nvoid ef4_link_status_changed(struct ef4_nic *efx)\n{\n\tstruct ef4_link_state *link_state = &efx->link_state;\n\n\t \n\tif (!netif_running(efx->net_dev))\n\t\treturn;\n\n\tif (link_state->up != netif_carrier_ok(efx->net_dev)) {\n\t\tefx->n_link_state_changes++;\n\n\t\tif (link_state->up)\n\t\t\tnetif_carrier_on(efx->net_dev);\n\t\telse\n\t\t\tnetif_carrier_off(efx->net_dev);\n\t}\n\n\t \n\tif (link_state->up)\n\t\tnetif_info(efx, link, efx->net_dev,\n\t\t\t   \"link up at %uMbps %s-duplex (MTU %d)\\n\",\n\t\t\t   link_state->speed, link_state->fd ? \"full\" : \"half\",\n\t\t\t   efx->net_dev->mtu);\n\telse\n\t\tnetif_info(efx, link, efx->net_dev, \"link down\\n\");\n}\n\nvoid ef4_link_set_advertising(struct ef4_nic *efx, u32 advertising)\n{\n\tefx->link_advertising = advertising;\n\tif (advertising) {\n\t\tif (advertising & ADVERTISED_Pause)\n\t\t\tefx->wanted_fc |= (EF4_FC_TX | EF4_FC_RX);\n\t\telse\n\t\t\tefx->wanted_fc &= ~(EF4_FC_TX | EF4_FC_RX);\n\t\tif (advertising & ADVERTISED_Asym_Pause)\n\t\t\tefx->wanted_fc ^= EF4_FC_TX;\n\t}\n}\n\nvoid ef4_link_set_wanted_fc(struct ef4_nic *efx, u8 wanted_fc)\n{\n\tefx->wanted_fc = wanted_fc;\n\tif (efx->link_advertising) {\n\t\tif (wanted_fc & EF4_FC_RX)\n\t\t\tefx->link_advertising |= (ADVERTISED_Pause |\n\t\t\t\t\t\t  ADVERTISED_Asym_Pause);\n\t\telse\n\t\t\tefx->link_advertising &= ~(ADVERTISED_Pause |\n\t\t\t\t\t\t   ADVERTISED_Asym_Pause);\n\t\tif (wanted_fc & EF4_FC_TX)\n\t\t\tefx->link_advertising ^= ADVERTISED_Asym_Pause;\n\t}\n}\n\nstatic void ef4_fini_port(struct ef4_nic *efx);\n\n \nvoid ef4_mac_reconfigure(struct ef4_nic *efx)\n{\n\tdown_read(&efx->filter_sem);\n\tefx->type->reconfigure_mac(efx);\n\tup_read(&efx->filter_sem);\n}\n\n \nint __ef4_reconfigure_port(struct ef4_nic *efx)\n{\n\tenum ef4_phy_mode phy_mode;\n\tint rc;\n\n\tWARN_ON(!mutex_is_locked(&efx->mac_lock));\n\n\t \n\tphy_mode = efx->phy_mode;\n\tif (LOOPBACK_INTERNAL(efx))\n\t\tefx->phy_mode |= PHY_MODE_TX_DISABLED;\n\telse\n\t\tefx->phy_mode &= ~PHY_MODE_TX_DISABLED;\n\n\trc = efx->type->reconfigure_port(efx);\n\n\tif (rc)\n\t\tefx->phy_mode = phy_mode;\n\n\treturn rc;\n}\n\n \nint ef4_reconfigure_port(struct ef4_nic *efx)\n{\n\tint rc;\n\n\tEF4_ASSERT_RESET_SERIALISED(efx);\n\n\tmutex_lock(&efx->mac_lock);\n\trc = __ef4_reconfigure_port(efx);\n\tmutex_unlock(&efx->mac_lock);\n\n\treturn rc;\n}\n\n \nstatic void ef4_mac_work(struct work_struct *data)\n{\n\tstruct ef4_nic *efx = container_of(data, struct ef4_nic, mac_work);\n\n\tmutex_lock(&efx->mac_lock);\n\tif (efx->port_enabled)\n\t\tef4_mac_reconfigure(efx);\n\tmutex_unlock(&efx->mac_lock);\n}\n\nstatic int ef4_probe_port(struct ef4_nic *efx)\n{\n\tint rc;\n\n\tnetif_dbg(efx, probe, efx->net_dev, \"create port\\n\");\n\n\tif (phy_flash_cfg)\n\t\tefx->phy_mode = PHY_MODE_SPECIAL;\n\n\t \n\trc = efx->type->probe_port(efx);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\teth_hw_addr_set(efx->net_dev, efx->net_dev->perm_addr);\n\n\treturn 0;\n}\n\nstatic int ef4_init_port(struct ef4_nic *efx)\n{\n\tint rc;\n\n\tnetif_dbg(efx, drv, efx->net_dev, \"init port\\n\");\n\n\tmutex_lock(&efx->mac_lock);\n\n\trc = efx->phy_op->init(efx);\n\tif (rc)\n\t\tgoto fail1;\n\n\tefx->port_initialized = true;\n\n\t \n\tef4_mac_reconfigure(efx);\n\n\t \n\trc = efx->phy_op->reconfigure(efx);\n\tif (rc && rc != -EPERM)\n\t\tgoto fail2;\n\n\tmutex_unlock(&efx->mac_lock);\n\treturn 0;\n\nfail2:\n\tefx->phy_op->fini(efx);\nfail1:\n\tmutex_unlock(&efx->mac_lock);\n\treturn rc;\n}\n\nstatic void ef4_start_port(struct ef4_nic *efx)\n{\n\tnetif_dbg(efx, ifup, efx->net_dev, \"start port\\n\");\n\tBUG_ON(efx->port_enabled);\n\n\tmutex_lock(&efx->mac_lock);\n\tefx->port_enabled = true;\n\n\t \n\tef4_mac_reconfigure(efx);\n\n\tmutex_unlock(&efx->mac_lock);\n}\n\n \nstatic void ef4_stop_port(struct ef4_nic *efx)\n{\n\tnetif_dbg(efx, ifdown, efx->net_dev, \"stop port\\n\");\n\n\tEF4_ASSERT_RESET_SERIALISED(efx);\n\n\tmutex_lock(&efx->mac_lock);\n\tefx->port_enabled = false;\n\tmutex_unlock(&efx->mac_lock);\n\n\t \n\tnetif_addr_lock_bh(efx->net_dev);\n\tnetif_addr_unlock_bh(efx->net_dev);\n\n\tcancel_delayed_work_sync(&efx->monitor_work);\n\tef4_selftest_async_cancel(efx);\n\tcancel_work_sync(&efx->mac_work);\n}\n\nstatic void ef4_fini_port(struct ef4_nic *efx)\n{\n\tnetif_dbg(efx, drv, efx->net_dev, \"shut down port\\n\");\n\n\tif (!efx->port_initialized)\n\t\treturn;\n\n\tefx->phy_op->fini(efx);\n\tefx->port_initialized = false;\n\n\tefx->link_state.up = false;\n\tef4_link_status_changed(efx);\n}\n\nstatic void ef4_remove_port(struct ef4_nic *efx)\n{\n\tnetif_dbg(efx, drv, efx->net_dev, \"destroying port\\n\");\n\n\tefx->type->remove_port(efx);\n}\n\n \n\nstatic LIST_HEAD(ef4_primary_list);\nstatic LIST_HEAD(ef4_unassociated_list);\n\nstatic bool ef4_same_controller(struct ef4_nic *left, struct ef4_nic *right)\n{\n\treturn left->type == right->type &&\n\t\tleft->vpd_sn && right->vpd_sn &&\n\t\t!strcmp(left->vpd_sn, right->vpd_sn);\n}\n\nstatic void ef4_associate(struct ef4_nic *efx)\n{\n\tstruct ef4_nic *other, *next;\n\n\tif (efx->primary == efx) {\n\t\t \n\n\t\tnetif_dbg(efx, probe, efx->net_dev, \"adding to primary list\\n\");\n\t\tlist_add_tail(&efx->node, &ef4_primary_list);\n\n\t\tlist_for_each_entry_safe(other, next, &ef4_unassociated_list,\n\t\t\t\t\t node) {\n\t\t\tif (ef4_same_controller(efx, other)) {\n\t\t\t\tlist_del(&other->node);\n\t\t\t\tnetif_dbg(other, probe, other->net_dev,\n\t\t\t\t\t  \"moving to secondary list of %s %s\\n\",\n\t\t\t\t\t  pci_name(efx->pci_dev),\n\t\t\t\t\t  efx->net_dev->name);\n\t\t\t\tlist_add_tail(&other->node,\n\t\t\t\t\t      &efx->secondary_list);\n\t\t\t\tother->primary = efx;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\n\t\tlist_for_each_entry(other, &ef4_primary_list, node) {\n\t\t\tif (ef4_same_controller(efx, other)) {\n\t\t\t\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t\t\t\t  \"adding to secondary list of %s %s\\n\",\n\t\t\t\t\t  pci_name(other->pci_dev),\n\t\t\t\t\t  other->net_dev->name);\n\t\t\t\tlist_add_tail(&efx->node,\n\t\t\t\t\t      &other->secondary_list);\n\t\t\t\tefx->primary = other;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t\t  \"adding to unassociated list\\n\");\n\t\tlist_add_tail(&efx->node, &ef4_unassociated_list);\n\t}\n}\n\nstatic void ef4_dissociate(struct ef4_nic *efx)\n{\n\tstruct ef4_nic *other, *next;\n\n\tlist_del(&efx->node);\n\tefx->primary = NULL;\n\n\tlist_for_each_entry_safe(other, next, &efx->secondary_list, node) {\n\t\tlist_del(&other->node);\n\t\tnetif_dbg(other, probe, other->net_dev,\n\t\t\t  \"moving to unassociated list\\n\");\n\t\tlist_add_tail(&other->node, &ef4_unassociated_list);\n\t\tother->primary = NULL;\n\t}\n}\n\n \nstatic int ef4_init_io(struct ef4_nic *efx)\n{\n\tstruct pci_dev *pci_dev = efx->pci_dev;\n\tdma_addr_t dma_mask = efx->type->max_dma_mask;\n\tunsigned int mem_map_size = efx->type->mem_map_size(efx);\n\tint rc, bar;\n\n\tnetif_dbg(efx, probe, efx->net_dev, \"initialising I/O\\n\");\n\n\tbar = efx->type->mem_bar;\n\n\trc = pci_enable_device(pci_dev);\n\tif (rc) {\n\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t  \"failed to enable PCI device\\n\");\n\t\tgoto fail1;\n\t}\n\n\tpci_set_master(pci_dev);\n\n\t \n\twhile (dma_mask > 0x7fffffffUL) {\n\t\trc = dma_set_mask_and_coherent(&pci_dev->dev, dma_mask);\n\t\tif (rc == 0)\n\t\t\tbreak;\n\t\tdma_mask >>= 1;\n\t}\n\tif (rc) {\n\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t  \"could not find a suitable DMA mask\\n\");\n\t\tgoto fail2;\n\t}\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"using DMA mask %llx\\n\", (unsigned long long) dma_mask);\n\n\tefx->membase_phys = pci_resource_start(efx->pci_dev, bar);\n\trc = pci_request_region(pci_dev, bar, \"sfc\");\n\tif (rc) {\n\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t  \"request for memory BAR failed\\n\");\n\t\trc = -EIO;\n\t\tgoto fail3;\n\t}\n\tefx->membase = ioremap(efx->membase_phys, mem_map_size);\n\tif (!efx->membase) {\n\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t  \"could not map memory BAR at %llx+%x\\n\",\n\t\t\t  (unsigned long long)efx->membase_phys, mem_map_size);\n\t\trc = -ENOMEM;\n\t\tgoto fail4;\n\t}\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"memory BAR at %llx+%x (virtual %p)\\n\",\n\t\t  (unsigned long long)efx->membase_phys, mem_map_size,\n\t\t  efx->membase);\n\n\treturn 0;\n\n fail4:\n\tpci_release_region(efx->pci_dev, bar);\n fail3:\n\tefx->membase_phys = 0;\n fail2:\n\tpci_disable_device(efx->pci_dev);\n fail1:\n\treturn rc;\n}\n\nstatic void ef4_fini_io(struct ef4_nic *efx)\n{\n\tint bar;\n\n\tnetif_dbg(efx, drv, efx->net_dev, \"shutting down I/O\\n\");\n\n\tif (efx->membase) {\n\t\tiounmap(efx->membase);\n\t\tefx->membase = NULL;\n\t}\n\n\tif (efx->membase_phys) {\n\t\tbar = efx->type->mem_bar;\n\t\tpci_release_region(efx->pci_dev, bar);\n\t\tefx->membase_phys = 0;\n\t}\n\n\t \n\tif (!pci_vfs_assigned(efx->pci_dev))\n\t\tpci_disable_device(efx->pci_dev);\n}\n\nvoid ef4_set_default_rx_indir_table(struct ef4_nic *efx)\n{\n\tsize_t i;\n\n\tfor (i = 0; i < ARRAY_SIZE(efx->rx_indir_table); i++)\n\t\tefx->rx_indir_table[i] =\n\t\t\tethtool_rxfh_indir_default(i, efx->rss_spread);\n}\n\nstatic unsigned int ef4_wanted_parallelism(struct ef4_nic *efx)\n{\n\tcpumask_var_t thread_mask;\n\tunsigned int count;\n\tint cpu;\n\n\tif (rss_cpus) {\n\t\tcount = rss_cpus;\n\t} else {\n\t\tif (unlikely(!zalloc_cpumask_var(&thread_mask, GFP_KERNEL))) {\n\t\t\tnetif_warn(efx, probe, efx->net_dev,\n\t\t\t\t   \"RSS disabled due to allocation failure\\n\");\n\t\t\treturn 1;\n\t\t}\n\n\t\tcount = 0;\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tif (!cpumask_test_cpu(cpu, thread_mask)) {\n\t\t\t\t++count;\n\t\t\t\tcpumask_or(thread_mask, thread_mask,\n\t\t\t\t\t   topology_sibling_cpumask(cpu));\n\t\t\t}\n\t\t}\n\n\t\tfree_cpumask_var(thread_mask);\n\t}\n\n\tif (count > EF4_MAX_RX_QUEUES) {\n\t\tnetif_cond_dbg(efx, probe, efx->net_dev, !rss_cpus, warn,\n\t\t\t       \"Reducing number of rx queues from %u to %u.\\n\",\n\t\t\t       count, EF4_MAX_RX_QUEUES);\n\t\tcount = EF4_MAX_RX_QUEUES;\n\t}\n\n\treturn count;\n}\n\n \nstatic int ef4_probe_interrupts(struct ef4_nic *efx)\n{\n\tunsigned int extra_channels = 0;\n\tunsigned int i, j;\n\tint rc;\n\n\tfor (i = 0; i < EF4_MAX_EXTRA_CHANNELS; i++)\n\t\tif (efx->extra_channel_type[i])\n\t\t\t++extra_channels;\n\n\tif (efx->interrupt_mode == EF4_INT_MODE_MSIX) {\n\t\tstruct msix_entry xentries[EF4_MAX_CHANNELS];\n\t\tunsigned int n_channels;\n\n\t\tn_channels = ef4_wanted_parallelism(efx);\n\t\tif (ef4_separate_tx_channels)\n\t\t\tn_channels *= 2;\n\t\tn_channels += extra_channels;\n\t\tn_channels = min(n_channels, efx->max_channels);\n\n\t\tfor (i = 0; i < n_channels; i++)\n\t\t\txentries[i].entry = i;\n\t\trc = pci_enable_msix_range(efx->pci_dev,\n\t\t\t\t\t   xentries, 1, n_channels);\n\t\tif (rc < 0) {\n\t\t\t \n\t\t\tefx->interrupt_mode = EF4_INT_MODE_MSI;\n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"could not enable MSI-X\\n\");\n\t\t} else if (rc < n_channels) {\n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"WARNING: Insufficient MSI-X vectors\"\n\t\t\t\t  \" available (%d < %u).\\n\", rc, n_channels);\n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"WARNING: Performance may be reduced.\\n\");\n\t\t\tn_channels = rc;\n\t\t}\n\n\t\tif (rc > 0) {\n\t\t\tefx->n_channels = n_channels;\n\t\t\tif (n_channels > extra_channels)\n\t\t\t\tn_channels -= extra_channels;\n\t\t\tif (ef4_separate_tx_channels) {\n\t\t\t\tefx->n_tx_channels = min(max(n_channels / 2,\n\t\t\t\t\t\t\t     1U),\n\t\t\t\t\t\t\t efx->max_tx_channels);\n\t\t\t\tefx->n_rx_channels = max(n_channels -\n\t\t\t\t\t\t\t efx->n_tx_channels,\n\t\t\t\t\t\t\t 1U);\n\t\t\t} else {\n\t\t\t\tefx->n_tx_channels = min(n_channels,\n\t\t\t\t\t\t\t efx->max_tx_channels);\n\t\t\t\tefx->n_rx_channels = n_channels;\n\t\t\t}\n\t\t\tfor (i = 0; i < efx->n_channels; i++)\n\t\t\t\tef4_get_channel(efx, i)->irq =\n\t\t\t\t\txentries[i].vector;\n\t\t}\n\t}\n\n\t \n\tif (efx->interrupt_mode == EF4_INT_MODE_MSI) {\n\t\tefx->n_channels = 1;\n\t\tefx->n_rx_channels = 1;\n\t\tefx->n_tx_channels = 1;\n\t\trc = pci_enable_msi(efx->pci_dev);\n\t\tif (rc == 0) {\n\t\t\tef4_get_channel(efx, 0)->irq = efx->pci_dev->irq;\n\t\t} else {\n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"could not enable MSI\\n\");\n\t\t\tefx->interrupt_mode = EF4_INT_MODE_LEGACY;\n\t\t}\n\t}\n\n\t \n\tif (efx->interrupt_mode == EF4_INT_MODE_LEGACY) {\n\t\tefx->n_channels = 1 + (ef4_separate_tx_channels ? 1 : 0);\n\t\tefx->n_rx_channels = 1;\n\t\tefx->n_tx_channels = 1;\n\t\tefx->legacy_irq = efx->pci_dev->irq;\n\t}\n\n\t \n\tj = efx->n_channels;\n\tfor (i = 0; i < EF4_MAX_EXTRA_CHANNELS; i++) {\n\t\tif (!efx->extra_channel_type[i])\n\t\t\tcontinue;\n\t\tif (efx->interrupt_mode != EF4_INT_MODE_MSIX ||\n\t\t    efx->n_channels <= extra_channels) {\n\t\t\tefx->extra_channel_type[i]->handle_no_channel(efx);\n\t\t} else {\n\t\t\t--j;\n\t\t\tef4_get_channel(efx, j)->type =\n\t\t\t\tefx->extra_channel_type[i];\n\t\t}\n\t}\n\n\tefx->rss_spread = efx->n_rx_channels;\n\n\treturn 0;\n}\n\nstatic int ef4_soft_enable_interrupts(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel, *end_channel;\n\tint rc;\n\n\tBUG_ON(efx->state == STATE_DISABLED);\n\n\tefx->irq_soft_enabled = true;\n\tsmp_wmb();\n\n\tef4_for_each_channel(channel, efx) {\n\t\tif (!channel->type->keep_eventq) {\n\t\t\trc = ef4_init_eventq(channel);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t}\n\t\tef4_start_eventq(channel);\n\t}\n\n\treturn 0;\nfail:\n\tend_channel = channel;\n\tef4_for_each_channel(channel, efx) {\n\t\tif (channel == end_channel)\n\t\t\tbreak;\n\t\tef4_stop_eventq(channel);\n\t\tif (!channel->type->keep_eventq)\n\t\t\tef4_fini_eventq(channel);\n\t}\n\n\treturn rc;\n}\n\nstatic void ef4_soft_disable_interrupts(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\n\tif (efx->state == STATE_DISABLED)\n\t\treturn;\n\n\tefx->irq_soft_enabled = false;\n\tsmp_wmb();\n\n\tif (efx->legacy_irq)\n\t\tsynchronize_irq(efx->legacy_irq);\n\n\tef4_for_each_channel(channel, efx) {\n\t\tif (channel->irq)\n\t\t\tsynchronize_irq(channel->irq);\n\n\t\tef4_stop_eventq(channel);\n\t\tif (!channel->type->keep_eventq)\n\t\t\tef4_fini_eventq(channel);\n\t}\n}\n\nstatic int ef4_enable_interrupts(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel, *end_channel;\n\tint rc;\n\n\tBUG_ON(efx->state == STATE_DISABLED);\n\n\tif (efx->eeh_disabled_legacy_irq) {\n\t\tenable_irq(efx->legacy_irq);\n\t\tefx->eeh_disabled_legacy_irq = false;\n\t}\n\n\tefx->type->irq_enable_master(efx);\n\n\tef4_for_each_channel(channel, efx) {\n\t\tif (channel->type->keep_eventq) {\n\t\t\trc = ef4_init_eventq(channel);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\n\trc = ef4_soft_enable_interrupts(efx);\n\tif (rc)\n\t\tgoto fail;\n\n\treturn 0;\n\nfail:\n\tend_channel = channel;\n\tef4_for_each_channel(channel, efx) {\n\t\tif (channel == end_channel)\n\t\t\tbreak;\n\t\tif (channel->type->keep_eventq)\n\t\t\tef4_fini_eventq(channel);\n\t}\n\n\tefx->type->irq_disable_non_ev(efx);\n\n\treturn rc;\n}\n\nstatic void ef4_disable_interrupts(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\n\tef4_soft_disable_interrupts(efx);\n\n\tef4_for_each_channel(channel, efx) {\n\t\tif (channel->type->keep_eventq)\n\t\t\tef4_fini_eventq(channel);\n\t}\n\n\tefx->type->irq_disable_non_ev(efx);\n}\n\nstatic void ef4_remove_interrupts(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\n\t \n\tef4_for_each_channel(channel, efx)\n\t\tchannel->irq = 0;\n\tpci_disable_msi(efx->pci_dev);\n\tpci_disable_msix(efx->pci_dev);\n\n\t \n\tefx->legacy_irq = 0;\n}\n\nstatic void ef4_set_channels(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\tstruct ef4_tx_queue *tx_queue;\n\n\tefx->tx_channel_offset =\n\t\tef4_separate_tx_channels ?\n\t\tefx->n_channels - efx->n_tx_channels : 0;\n\n\t \n\tef4_for_each_channel(channel, efx) {\n\t\tif (channel->channel < efx->n_rx_channels)\n\t\t\tchannel->rx_queue.core_index = channel->channel;\n\t\telse\n\t\t\tchannel->rx_queue.core_index = -1;\n\n\t\tef4_for_each_channel_tx_queue(tx_queue, channel)\n\t\t\ttx_queue->queue -= (efx->tx_channel_offset *\n\t\t\t\t\t    EF4_TXQ_TYPES);\n\t}\n}\n\nstatic int ef4_probe_nic(struct ef4_nic *efx)\n{\n\tint rc;\n\n\tnetif_dbg(efx, probe, efx->net_dev, \"creating NIC\\n\");\n\n\t \n\trc = efx->type->probe(efx);\n\tif (rc)\n\t\treturn rc;\n\n\tdo {\n\t\tif (!efx->max_channels || !efx->max_tx_channels) {\n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"Insufficient resources to allocate\"\n\t\t\t\t  \" any channels\\n\");\n\t\t\trc = -ENOSPC;\n\t\t\tgoto fail1;\n\t\t}\n\n\t\t \n\t\trc = ef4_probe_interrupts(efx);\n\t\tif (rc)\n\t\t\tgoto fail1;\n\n\t\tef4_set_channels(efx);\n\n\t\t \n\t\trc = efx->type->dimension_resources(efx);\n\t\tif (rc != 0 && rc != -EAGAIN)\n\t\t\tgoto fail2;\n\n\t\tif (rc == -EAGAIN)\n\t\t\t \n\t\t\tef4_remove_interrupts(efx);\n\n\t} while (rc == -EAGAIN);\n\n\tif (efx->n_channels > 1)\n\t\tnetdev_rss_key_fill(&efx->rx_hash_key,\n\t\t\t\t    sizeof(efx->rx_hash_key));\n\tef4_set_default_rx_indir_table(efx);\n\n\tnetif_set_real_num_tx_queues(efx->net_dev, efx->n_tx_channels);\n\tnetif_set_real_num_rx_queues(efx->net_dev, efx->n_rx_channels);\n\n\t \n\tefx->irq_mod_step_us = DIV_ROUND_UP(efx->timer_quantum_ns, 1000);\n\tef4_init_irq_moderation(efx, tx_irq_mod_usec, rx_irq_mod_usec, true,\n\t\t\t\ttrue);\n\n\treturn 0;\n\nfail2:\n\tef4_remove_interrupts(efx);\nfail1:\n\tefx->type->remove(efx);\n\treturn rc;\n}\n\nstatic void ef4_remove_nic(struct ef4_nic *efx)\n{\n\tnetif_dbg(efx, drv, efx->net_dev, \"destroying NIC\\n\");\n\n\tef4_remove_interrupts(efx);\n\tefx->type->remove(efx);\n}\n\nstatic int ef4_probe_filters(struct ef4_nic *efx)\n{\n\tint rc;\n\n\tspin_lock_init(&efx->filter_lock);\n\tinit_rwsem(&efx->filter_sem);\n\tmutex_lock(&efx->mac_lock);\n\tdown_write(&efx->filter_sem);\n\trc = efx->type->filter_table_probe(efx);\n\tif (rc)\n\t\tgoto out_unlock;\n\n#ifdef CONFIG_RFS_ACCEL\n\tif (efx->type->offload_features & NETIF_F_NTUPLE) {\n\t\tstruct ef4_channel *channel;\n\t\tint i, success = 1;\n\n\t\tef4_for_each_channel(channel, efx) {\n\t\t\tchannel->rps_flow_id =\n\t\t\t\tkcalloc(efx->type->max_rx_ip_filters,\n\t\t\t\t\tsizeof(*channel->rps_flow_id),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!channel->rps_flow_id)\n\t\t\t\tsuccess = 0;\n\t\t\telse\n\t\t\t\tfor (i = 0;\n\t\t\t\t     i < efx->type->max_rx_ip_filters;\n\t\t\t\t     ++i)\n\t\t\t\t\tchannel->rps_flow_id[i] =\n\t\t\t\t\t\tRPS_FLOW_ID_INVALID;\n\t\t}\n\n\t\tif (!success) {\n\t\t\tef4_for_each_channel(channel, efx)\n\t\t\t\tkfree(channel->rps_flow_id);\n\t\t\tefx->type->filter_table_remove(efx);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tefx->rps_expire_index = efx->rps_expire_channel = 0;\n\t}\n#endif\nout_unlock:\n\tup_write(&efx->filter_sem);\n\tmutex_unlock(&efx->mac_lock);\n\treturn rc;\n}\n\nstatic void ef4_remove_filters(struct ef4_nic *efx)\n{\n#ifdef CONFIG_RFS_ACCEL\n\tstruct ef4_channel *channel;\n\n\tef4_for_each_channel(channel, efx)\n\t\tkfree(channel->rps_flow_id);\n#endif\n\tdown_write(&efx->filter_sem);\n\tefx->type->filter_table_remove(efx);\n\tup_write(&efx->filter_sem);\n}\n\nstatic void ef4_restore_filters(struct ef4_nic *efx)\n{\n\tdown_read(&efx->filter_sem);\n\tefx->type->filter_table_restore(efx);\n\tup_read(&efx->filter_sem);\n}\n\n \n\nstatic int ef4_probe_all(struct ef4_nic *efx)\n{\n\tint rc;\n\n\trc = ef4_probe_nic(efx);\n\tif (rc) {\n\t\tnetif_err(efx, probe, efx->net_dev, \"failed to create NIC\\n\");\n\t\tgoto fail1;\n\t}\n\n\trc = ef4_probe_port(efx);\n\tif (rc) {\n\t\tnetif_err(efx, probe, efx->net_dev, \"failed to create port\\n\");\n\t\tgoto fail2;\n\t}\n\n\tBUILD_BUG_ON(EF4_DEFAULT_DMAQ_SIZE < EF4_RXQ_MIN_ENT);\n\tif (WARN_ON(EF4_DEFAULT_DMAQ_SIZE < EF4_TXQ_MIN_ENT(efx))) {\n\t\trc = -EINVAL;\n\t\tgoto fail3;\n\t}\n\tefx->rxq_entries = efx->txq_entries = EF4_DEFAULT_DMAQ_SIZE;\n\n\trc = ef4_probe_filters(efx);\n\tif (rc) {\n\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t  \"failed to create filter tables\\n\");\n\t\tgoto fail4;\n\t}\n\n\trc = ef4_probe_channels(efx);\n\tif (rc)\n\t\tgoto fail5;\n\n\treturn 0;\n\n fail5:\n\tef4_remove_filters(efx);\n fail4:\n fail3:\n\tef4_remove_port(efx);\n fail2:\n\tef4_remove_nic(efx);\n fail1:\n\treturn rc;\n}\n\n \nstatic void ef4_start_all(struct ef4_nic *efx)\n{\n\tEF4_ASSERT_RESET_SERIALISED(efx);\n\tBUG_ON(efx->state == STATE_DISABLED);\n\n\t \n\tif (efx->port_enabled || !netif_running(efx->net_dev) ||\n\t    efx->reset_pending)\n\t\treturn;\n\n\tef4_start_port(efx);\n\tef4_start_datapath(efx);\n\n\t \n\tif (efx->type->monitor != NULL)\n\t\tqueue_delayed_work(efx->workqueue, &efx->monitor_work,\n\t\t\t\t   ef4_monitor_interval);\n\n\tefx->type->start_stats(efx);\n\tefx->type->pull_stats(efx);\n\tspin_lock_bh(&efx->stats_lock);\n\tefx->type->update_stats(efx, NULL, NULL);\n\tspin_unlock_bh(&efx->stats_lock);\n}\n\n \nstatic void ef4_stop_all(struct ef4_nic *efx)\n{\n\tEF4_ASSERT_RESET_SERIALISED(efx);\n\n\t \n\tif (!efx->port_enabled)\n\t\treturn;\n\n\t \n\tefx->type->pull_stats(efx);\n\tspin_lock_bh(&efx->stats_lock);\n\tefx->type->update_stats(efx, NULL, NULL);\n\tspin_unlock_bh(&efx->stats_lock);\n\tefx->type->stop_stats(efx);\n\tef4_stop_port(efx);\n\n\t \n\tWARN_ON(netif_running(efx->net_dev) &&\n\t\tnetif_device_present(efx->net_dev));\n\tnetif_tx_disable(efx->net_dev);\n\n\tef4_stop_datapath(efx);\n}\n\nstatic void ef4_remove_all(struct ef4_nic *efx)\n{\n\tef4_remove_channels(efx);\n\tef4_remove_filters(efx);\n\tef4_remove_port(efx);\n\tef4_remove_nic(efx);\n}\n\n \nunsigned int ef4_usecs_to_ticks(struct ef4_nic *efx, unsigned int usecs)\n{\n\tif (usecs == 0)\n\t\treturn 0;\n\tif (usecs * 1000 < efx->timer_quantum_ns)\n\t\treturn 1;  \n\treturn usecs * 1000 / efx->timer_quantum_ns;\n}\n\nunsigned int ef4_ticks_to_usecs(struct ef4_nic *efx, unsigned int ticks)\n{\n\t \n\treturn DIV_ROUND_UP(ticks * efx->timer_quantum_ns, 1000);\n}\n\n \nint ef4_init_irq_moderation(struct ef4_nic *efx, unsigned int tx_usecs,\n\t\t\t    unsigned int rx_usecs, bool rx_adaptive,\n\t\t\t    bool rx_may_override_tx)\n{\n\tstruct ef4_channel *channel;\n\tunsigned int timer_max_us;\n\n\tEF4_ASSERT_RESET_SERIALISED(efx);\n\n\ttimer_max_us = efx->timer_max_ns / 1000;\n\n\tif (tx_usecs > timer_max_us || rx_usecs > timer_max_us)\n\t\treturn -EINVAL;\n\n\tif (tx_usecs != rx_usecs && efx->tx_channel_offset == 0 &&\n\t    !rx_may_override_tx) {\n\t\tnetif_err(efx, drv, efx->net_dev, \"Channels are shared. \"\n\t\t\t  \"RX and TX IRQ moderation must be equal\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tefx->irq_rx_adaptive = rx_adaptive;\n\tefx->irq_rx_moderation_us = rx_usecs;\n\tef4_for_each_channel(channel, efx) {\n\t\tif (ef4_channel_has_rx_queue(channel))\n\t\t\tchannel->irq_moderation_us = rx_usecs;\n\t\telse if (ef4_channel_has_tx_queues(channel))\n\t\t\tchannel->irq_moderation_us = tx_usecs;\n\t}\n\n\treturn 0;\n}\n\nvoid ef4_get_irq_moderation(struct ef4_nic *efx, unsigned int *tx_usecs,\n\t\t\t    unsigned int *rx_usecs, bool *rx_adaptive)\n{\n\t*rx_adaptive = efx->irq_rx_adaptive;\n\t*rx_usecs = efx->irq_rx_moderation_us;\n\n\t \n\tif (efx->tx_channel_offset == 0) {\n\t\t*tx_usecs = *rx_usecs;\n\t} else {\n\t\tstruct ef4_channel *tx_channel;\n\n\t\ttx_channel = efx->channel[efx->tx_channel_offset];\n\t\t*tx_usecs = tx_channel->irq_moderation_us;\n\t}\n}\n\n \n\n \nstatic void ef4_monitor(struct work_struct *data)\n{\n\tstruct ef4_nic *efx = container_of(data, struct ef4_nic,\n\t\t\t\t\t   monitor_work.work);\n\n\tnetif_vdbg(efx, timer, efx->net_dev,\n\t\t   \"hardware monitor executing on CPU %d\\n\",\n\t\t   raw_smp_processor_id());\n\tBUG_ON(efx->type->monitor == NULL);\n\n\t \n\tif (mutex_trylock(&efx->mac_lock)) {\n\t\tif (efx->port_enabled)\n\t\t\tefx->type->monitor(efx);\n\t\tmutex_unlock(&efx->mac_lock);\n\t}\n\n\tqueue_delayed_work(efx->workqueue, &efx->monitor_work,\n\t\t\t   ef4_monitor_interval);\n}\n\n \n\n \nstatic int ef4_ioctl(struct net_device *net_dev, struct ifreq *ifr, int cmd)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\tstruct mii_ioctl_data *data = if_mii(ifr);\n\n\t \n\tif ((cmd == SIOCGMIIREG || cmd == SIOCSMIIREG) &&\n\t    (data->phy_id & 0xfc00) == 0x0400)\n\t\tdata->phy_id ^= MDIO_PHY_ID_C45 | 0x0400;\n\n\treturn mdio_mii_ioctl(&efx->mdio, data, cmd);\n}\n\n \n\nstatic void ef4_init_napi_channel(struct ef4_channel *channel)\n{\n\tstruct ef4_nic *efx = channel->efx;\n\n\tchannel->napi_dev = efx->net_dev;\n\tnetif_napi_add(channel->napi_dev, &channel->napi_str, ef4_poll);\n}\n\nstatic void ef4_init_napi(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\n\tef4_for_each_channel(channel, efx)\n\t\tef4_init_napi_channel(channel);\n}\n\nstatic void ef4_fini_napi_channel(struct ef4_channel *channel)\n{\n\tif (channel->napi_dev)\n\t\tnetif_napi_del(&channel->napi_str);\n\n\tchannel->napi_dev = NULL;\n}\n\nstatic void ef4_fini_napi(struct ef4_nic *efx)\n{\n\tstruct ef4_channel *channel;\n\n\tef4_for_each_channel(channel, efx)\n\t\tef4_fini_napi_channel(channel);\n}\n\n \n\n \nint ef4_net_open(struct net_device *net_dev)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\tint rc;\n\n\tnetif_dbg(efx, ifup, efx->net_dev, \"opening device on CPU %d\\n\",\n\t\t  raw_smp_processor_id());\n\n\trc = ef4_check_disabled(efx);\n\tif (rc)\n\t\treturn rc;\n\tif (efx->phy_mode & PHY_MODE_SPECIAL)\n\t\treturn -EBUSY;\n\n\t \n\tef4_link_status_changed(efx);\n\n\tef4_start_all(efx);\n\tef4_selftest_async_start(efx);\n\treturn 0;\n}\n\n \nint ef4_net_stop(struct net_device *net_dev)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\n\tnetif_dbg(efx, ifdown, efx->net_dev, \"closing on CPU %d\\n\",\n\t\t  raw_smp_processor_id());\n\n\t \n\tef4_stop_all(efx);\n\n\treturn 0;\n}\n\n \nstatic void ef4_net_stats(struct net_device *net_dev,\n\t\t\t  struct rtnl_link_stats64 *stats)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\n\tspin_lock_bh(&efx->stats_lock);\n\tefx->type->update_stats(efx, NULL, stats);\n\tspin_unlock_bh(&efx->stats_lock);\n}\n\n \nstatic void ef4_watchdog(struct net_device *net_dev, unsigned int txqueue)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\n\tnetif_err(efx, tx_err, efx->net_dev,\n\t\t  \"TX stuck with port_enabled=%d: resetting channels\\n\",\n\t\t  efx->port_enabled);\n\n\tef4_schedule_reset(efx, RESET_TYPE_TX_WATCHDOG);\n}\n\n\n \nstatic int ef4_change_mtu(struct net_device *net_dev, int new_mtu)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\tint rc;\n\n\trc = ef4_check_disabled(efx);\n\tif (rc)\n\t\treturn rc;\n\n\tnetif_dbg(efx, drv, efx->net_dev, \"changing MTU to %d\\n\", new_mtu);\n\n\tef4_device_detach_sync(efx);\n\tef4_stop_all(efx);\n\n\tmutex_lock(&efx->mac_lock);\n\tnet_dev->mtu = new_mtu;\n\tef4_mac_reconfigure(efx);\n\tmutex_unlock(&efx->mac_lock);\n\n\tef4_start_all(efx);\n\tnetif_device_attach(efx->net_dev);\n\treturn 0;\n}\n\nstatic int ef4_set_mac_address(struct net_device *net_dev, void *data)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\tstruct sockaddr *addr = data;\n\tu8 *new_addr = addr->sa_data;\n\tu8 old_addr[6];\n\tint rc;\n\n\tif (!is_valid_ether_addr(new_addr)) {\n\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t  \"invalid ethernet MAC address requested: %pM\\n\",\n\t\t\t  new_addr);\n\t\treturn -EADDRNOTAVAIL;\n\t}\n\n\t \n\tether_addr_copy(old_addr, net_dev->dev_addr);\n\teth_hw_addr_set(net_dev, new_addr);\n\tif (efx->type->set_mac_address) {\n\t\trc = efx->type->set_mac_address(efx);\n\t\tif (rc) {\n\t\t\teth_hw_addr_set(net_dev, old_addr);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t \n\tmutex_lock(&efx->mac_lock);\n\tef4_mac_reconfigure(efx);\n\tmutex_unlock(&efx->mac_lock);\n\n\treturn 0;\n}\n\n \nstatic void ef4_set_rx_mode(struct net_device *net_dev)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\n\tif (efx->port_enabled)\n\t\tqueue_work(efx->workqueue, &efx->mac_work);\n\t \n}\n\nstatic int ef4_set_features(struct net_device *net_dev, netdev_features_t data)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\tint rc;\n\n\t \n\tif (net_dev->features & ~data & NETIF_F_NTUPLE) {\n\t\trc = efx->type->filter_clear_rx(efx, EF4_FILTER_PRI_MANUAL);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\t \n\tif ((net_dev->features ^ data) & NETIF_F_HW_VLAN_CTAG_FILTER) {\n\t\t \n\t\tef4_set_rx_mode(net_dev);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct net_device_ops ef4_netdev_ops = {\n\t.ndo_open\t\t= ef4_net_open,\n\t.ndo_stop\t\t= ef4_net_stop,\n\t.ndo_get_stats64\t= ef4_net_stats,\n\t.ndo_tx_timeout\t\t= ef4_watchdog,\n\t.ndo_start_xmit\t\t= ef4_hard_start_xmit,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_eth_ioctl\t\t= ef4_ioctl,\n\t.ndo_change_mtu\t\t= ef4_change_mtu,\n\t.ndo_set_mac_address\t= ef4_set_mac_address,\n\t.ndo_set_rx_mode\t= ef4_set_rx_mode,\n\t.ndo_set_features\t= ef4_set_features,\n\t.ndo_setup_tc\t\t= ef4_setup_tc,\n#ifdef CONFIG_RFS_ACCEL\n\t.ndo_rx_flow_steer\t= ef4_filter_rfs,\n#endif\n};\n\nstatic void ef4_update_name(struct ef4_nic *efx)\n{\n\tstrcpy(efx->name, efx->net_dev->name);\n\tef4_mtd_rename(efx);\n\tef4_set_channel_names(efx);\n}\n\nstatic int ef4_netdev_event(struct notifier_block *this,\n\t\t\t    unsigned long event, void *ptr)\n{\n\tstruct net_device *net_dev = netdev_notifier_info_to_dev(ptr);\n\n\tif ((net_dev->netdev_ops == &ef4_netdev_ops) &&\n\t    event == NETDEV_CHANGENAME)\n\t\tef4_update_name(netdev_priv(net_dev));\n\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block ef4_netdev_notifier = {\n\t.notifier_call = ef4_netdev_event,\n};\n\nstatic ssize_t\nphy_type_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct ef4_nic *efx = dev_get_drvdata(dev);\n\treturn sprintf(buf, \"%d\\n\", efx->phy_type);\n}\nstatic DEVICE_ATTR_RO(phy_type);\n\nstatic int ef4_register_netdev(struct ef4_nic *efx)\n{\n\tstruct net_device *net_dev = efx->net_dev;\n\tstruct ef4_channel *channel;\n\tint rc;\n\n\tnet_dev->watchdog_timeo = 5 * HZ;\n\tnet_dev->irq = efx->pci_dev->irq;\n\tnet_dev->netdev_ops = &ef4_netdev_ops;\n\tnet_dev->ethtool_ops = &ef4_ethtool_ops;\n\tnetif_set_tso_max_segs(net_dev, EF4_TSO_MAX_SEGS);\n\tnet_dev->min_mtu = EF4_MIN_MTU;\n\tnet_dev->max_mtu = EF4_MAX_MTU;\n\n\trtnl_lock();\n\n\t \n\tefx->state = STATE_READY;\n\tsmp_mb();  \n\tif (efx->reset_pending) {\n\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t  \"aborting probe due to scheduled reset\\n\");\n\t\trc = -EIO;\n\t\tgoto fail_locked;\n\t}\n\n\trc = dev_alloc_name(net_dev, net_dev->name);\n\tif (rc < 0)\n\t\tgoto fail_locked;\n\tef4_update_name(efx);\n\n\t \n\tnetif_carrier_off(net_dev);\n\n\trc = register_netdevice(net_dev);\n\tif (rc)\n\t\tgoto fail_locked;\n\n\tef4_for_each_channel(channel, efx) {\n\t\tstruct ef4_tx_queue *tx_queue;\n\t\tef4_for_each_channel_tx_queue(tx_queue, channel)\n\t\t\tef4_init_tx_queue_core_txq(tx_queue);\n\t}\n\n\tef4_associate(efx);\n\n\trtnl_unlock();\n\n\trc = device_create_file(&efx->pci_dev->dev, &dev_attr_phy_type);\n\tif (rc) {\n\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t  \"failed to init net dev attributes\\n\");\n\t\tgoto fail_registered;\n\t}\n\treturn 0;\n\nfail_registered:\n\trtnl_lock();\n\tef4_dissociate(efx);\n\tunregister_netdevice(net_dev);\nfail_locked:\n\tefx->state = STATE_UNINIT;\n\trtnl_unlock();\n\tnetif_err(efx, drv, efx->net_dev, \"could not register net dev\\n\");\n\treturn rc;\n}\n\nstatic void ef4_unregister_netdev(struct ef4_nic *efx)\n{\n\tif (!efx->net_dev)\n\t\treturn;\n\n\tBUG_ON(netdev_priv(efx->net_dev) != efx);\n\n\tif (ef4_dev_registered(efx)) {\n\t\tstrscpy(efx->name, pci_name(efx->pci_dev), sizeof(efx->name));\n\t\tdevice_remove_file(&efx->pci_dev->dev, &dev_attr_phy_type);\n\t\tunregister_netdev(efx->net_dev);\n\t}\n}\n\n \n\n \nvoid ef4_reset_down(struct ef4_nic *efx, enum reset_type method)\n{\n\tEF4_ASSERT_RESET_SERIALISED(efx);\n\n\tef4_stop_all(efx);\n\tef4_disable_interrupts(efx);\n\n\tmutex_lock(&efx->mac_lock);\n\tif (efx->port_initialized && method != RESET_TYPE_INVISIBLE &&\n\t    method != RESET_TYPE_DATAPATH)\n\t\tefx->phy_op->fini(efx);\n\tefx->type->fini(efx);\n}\n\n \nint ef4_reset_up(struct ef4_nic *efx, enum reset_type method, bool ok)\n{\n\tint rc;\n\n\tEF4_ASSERT_RESET_SERIALISED(efx);\n\n\t \n\trc = efx->type->init(efx);\n\tif (rc) {\n\t\tnetif_err(efx, drv, efx->net_dev, \"failed to initialise NIC\\n\");\n\t\tgoto fail;\n\t}\n\n\tif (!ok)\n\t\tgoto fail;\n\n\tif (efx->port_initialized && method != RESET_TYPE_INVISIBLE &&\n\t    method != RESET_TYPE_DATAPATH) {\n\t\trc = efx->phy_op->init(efx);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t\trc = efx->phy_op->reconfigure(efx);\n\t\tif (rc && rc != -EPERM)\n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"could not restore PHY settings\\n\");\n\t}\n\n\trc = ef4_enable_interrupts(efx);\n\tif (rc)\n\t\tgoto fail;\n\n\tdown_read(&efx->filter_sem);\n\tef4_restore_filters(efx);\n\tup_read(&efx->filter_sem);\n\n\tmutex_unlock(&efx->mac_lock);\n\n\tef4_start_all(efx);\n\n\treturn 0;\n\nfail:\n\tefx->port_initialized = false;\n\n\tmutex_unlock(&efx->mac_lock);\n\n\treturn rc;\n}\n\n \nint ef4_reset(struct ef4_nic *efx, enum reset_type method)\n{\n\tint rc, rc2;\n\tbool disabled;\n\n\tnetif_info(efx, drv, efx->net_dev, \"resetting (%s)\\n\",\n\t\t   RESET_TYPE(method));\n\n\tef4_device_detach_sync(efx);\n\tef4_reset_down(efx, method);\n\n\trc = efx->type->reset(efx, method);\n\tif (rc) {\n\t\tnetif_err(efx, drv, efx->net_dev, \"failed to reset hardware\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tif (method < RESET_TYPE_MAX_METHOD)\n\t\tefx->reset_pending &= -(1 << (method + 1));\n\telse  \n\t\t__clear_bit(method, &efx->reset_pending);\n\n\t \n\tpci_set_master(efx->pci_dev);\n\nout:\n\t \n\tdisabled = rc ||\n\t\tmethod == RESET_TYPE_DISABLE ||\n\t\tmethod == RESET_TYPE_RECOVER_OR_DISABLE;\n\trc2 = ef4_reset_up(efx, method, !disabled);\n\tif (rc2) {\n\t\tdisabled = true;\n\t\tif (!rc)\n\t\t\trc = rc2;\n\t}\n\n\tif (disabled) {\n\t\tdev_close(efx->net_dev);\n\t\tnetif_err(efx, drv, efx->net_dev, \"has been disabled\\n\");\n\t\tefx->state = STATE_DISABLED;\n\t} else {\n\t\tnetif_dbg(efx, drv, efx->net_dev, \"reset complete\\n\");\n\t\tnetif_device_attach(efx->net_dev);\n\t}\n\treturn rc;\n}\n\n \nint ef4_try_recovery(struct ef4_nic *efx)\n{\n#ifdef CONFIG_EEH\n\t \n\tstruct eeh_dev *eehdev = pci_dev_to_eeh_dev(efx->pci_dev);\n\tif (eeh_dev_check_failure(eehdev)) {\n\t\t \n\t\treturn 1;\n\t}\n#endif\n\treturn 0;\n}\n\n \nstatic void ef4_reset_work(struct work_struct *data)\n{\n\tstruct ef4_nic *efx = container_of(data, struct ef4_nic, reset_work);\n\tunsigned long pending;\n\tenum reset_type method;\n\n\tpending = READ_ONCE(efx->reset_pending);\n\tmethod = fls(pending) - 1;\n\n\tif ((method == RESET_TYPE_RECOVER_OR_DISABLE ||\n\t     method == RESET_TYPE_RECOVER_OR_ALL) &&\n\t    ef4_try_recovery(efx))\n\t\treturn;\n\n\tif (!pending)\n\t\treturn;\n\n\trtnl_lock();\n\n\t \n\tif (efx->state == STATE_READY)\n\t\t(void)ef4_reset(efx, method);\n\n\trtnl_unlock();\n}\n\nvoid ef4_schedule_reset(struct ef4_nic *efx, enum reset_type type)\n{\n\tenum reset_type method;\n\n\tif (efx->state == STATE_RECOVERY) {\n\t\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t\t  \"recovering: skip scheduling %s reset\\n\",\n\t\t\t  RESET_TYPE(type));\n\t\treturn;\n\t}\n\n\tswitch (type) {\n\tcase RESET_TYPE_INVISIBLE:\n\tcase RESET_TYPE_ALL:\n\tcase RESET_TYPE_RECOVER_OR_ALL:\n\tcase RESET_TYPE_WORLD:\n\tcase RESET_TYPE_DISABLE:\n\tcase RESET_TYPE_RECOVER_OR_DISABLE:\n\tcase RESET_TYPE_DATAPATH:\n\t\tmethod = type;\n\t\tnetif_dbg(efx, drv, efx->net_dev, \"scheduling %s reset\\n\",\n\t\t\t  RESET_TYPE(method));\n\t\tbreak;\n\tdefault:\n\t\tmethod = efx->type->map_reset_reason(type);\n\t\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t\t  \"scheduling %s reset for %s\\n\",\n\t\t\t  RESET_TYPE(method), RESET_TYPE(type));\n\t\tbreak;\n\t}\n\n\tset_bit(method, &efx->reset_pending);\n\tsmp_mb();  \n\n\t \n\tif (READ_ONCE(efx->state) != STATE_READY)\n\t\treturn;\n\n\tqueue_work(reset_workqueue, &efx->reset_work);\n}\n\n \n\n \nstatic const struct pci_device_id ef4_pci_table[] = {\n\t{PCI_DEVICE(PCI_VENDOR_ID_SOLARFLARE,\n\t\t    PCI_DEVICE_ID_SOLARFLARE_SFC4000A_0),\n\t .driver_data = (unsigned long) &falcon_a1_nic_type},\n\t{PCI_DEVICE(PCI_VENDOR_ID_SOLARFLARE,\n\t\t    PCI_DEVICE_ID_SOLARFLARE_SFC4000B),\n\t .driver_data = (unsigned long) &falcon_b0_nic_type},\n\t{0}\t\t\t \n};\n\n \nint ef4_port_dummy_op_int(struct ef4_nic *efx)\n{\n\treturn 0;\n}\nvoid ef4_port_dummy_op_void(struct ef4_nic *efx) {}\n\nstatic bool ef4_port_dummy_op_poll(struct ef4_nic *efx)\n{\n\treturn false;\n}\n\nstatic const struct ef4_phy_operations ef4_dummy_phy_operations = {\n\t.init\t\t = ef4_port_dummy_op_int,\n\t.reconfigure\t = ef4_port_dummy_op_int,\n\t.poll\t\t = ef4_port_dummy_op_poll,\n\t.fini\t\t = ef4_port_dummy_op_void,\n};\n\n \n\n \nstatic int ef4_init_struct(struct ef4_nic *efx,\n\t\t\t   struct pci_dev *pci_dev, struct net_device *net_dev)\n{\n\tint i;\n\n\t \n\tINIT_LIST_HEAD(&efx->node);\n\tINIT_LIST_HEAD(&efx->secondary_list);\n\tspin_lock_init(&efx->biu_lock);\n#ifdef CONFIG_SFC_FALCON_MTD\n\tINIT_LIST_HEAD(&efx->mtd_list);\n#endif\n\tINIT_WORK(&efx->reset_work, ef4_reset_work);\n\tINIT_DELAYED_WORK(&efx->monitor_work, ef4_monitor);\n\tINIT_DELAYED_WORK(&efx->selftest_work, ef4_selftest_async_work);\n\tefx->pci_dev = pci_dev;\n\tefx->msg_enable = debug;\n\tefx->state = STATE_UNINIT;\n\tstrscpy(efx->name, pci_name(pci_dev), sizeof(efx->name));\n\n\tefx->net_dev = net_dev;\n\tefx->rx_prefix_size = efx->type->rx_prefix_size;\n\tefx->rx_ip_align =\n\t\tNET_IP_ALIGN ? (efx->rx_prefix_size + NET_IP_ALIGN) % 4 : 0;\n\tefx->rx_packet_hash_offset =\n\t\tefx->type->rx_hash_offset - efx->type->rx_prefix_size;\n\tefx->rx_packet_ts_offset =\n\t\tefx->type->rx_ts_offset - efx->type->rx_prefix_size;\n\tspin_lock_init(&efx->stats_lock);\n\tmutex_init(&efx->mac_lock);\n\tefx->phy_op = &ef4_dummy_phy_operations;\n\tefx->mdio.dev = net_dev;\n\tINIT_WORK(&efx->mac_work, ef4_mac_work);\n\tinit_waitqueue_head(&efx->flush_wq);\n\n\tfor (i = 0; i < EF4_MAX_CHANNELS; i++) {\n\t\tefx->channel[i] = ef4_alloc_channel(efx, i, NULL);\n\t\tif (!efx->channel[i])\n\t\t\tgoto fail;\n\t\tefx->msi_context[i].efx = efx;\n\t\tefx->msi_context[i].index = i;\n\t}\n\n\t \n\tefx->interrupt_mode = max(efx->type->max_interrupt_mode,\n\t\t\t\t  interrupt_mode);\n\n\t \n\tsnprintf(efx->workqueue_name, sizeof(efx->workqueue_name), \"sfc%s\",\n\t\t pci_name(pci_dev));\n\tefx->workqueue = create_singlethread_workqueue(efx->workqueue_name);\n\tif (!efx->workqueue)\n\t\tgoto fail;\n\n\treturn 0;\n\nfail:\n\tef4_fini_struct(efx);\n\treturn -ENOMEM;\n}\n\nstatic void ef4_fini_struct(struct ef4_nic *efx)\n{\n\tint i;\n\n\tfor (i = 0; i < EF4_MAX_CHANNELS; i++)\n\t\tkfree(efx->channel[i]);\n\n\tkfree(efx->vpd_sn);\n\n\tif (efx->workqueue) {\n\t\tdestroy_workqueue(efx->workqueue);\n\t\tefx->workqueue = NULL;\n\t}\n}\n\nvoid ef4_update_sw_stats(struct ef4_nic *efx, u64 *stats)\n{\n\tu64 n_rx_nodesc_trunc = 0;\n\tstruct ef4_channel *channel;\n\n\tef4_for_each_channel(channel, efx)\n\t\tn_rx_nodesc_trunc += channel->n_rx_nodesc_trunc;\n\tstats[GENERIC_STAT_rx_nodesc_trunc] = n_rx_nodesc_trunc;\n\tstats[GENERIC_STAT_rx_noskb_drops] = atomic_read(&efx->n_rx_noskb_drops);\n}\n\n \n\n \nstatic void ef4_pci_remove_main(struct ef4_nic *efx)\n{\n\t \n\tBUG_ON(efx->state == STATE_READY);\n\tcancel_work_sync(&efx->reset_work);\n\n\tef4_disable_interrupts(efx);\n\tef4_nic_fini_interrupt(efx);\n\tef4_fini_port(efx);\n\tefx->type->fini(efx);\n\tef4_fini_napi(efx);\n\tef4_remove_all(efx);\n}\n\n \nstatic void ef4_pci_remove(struct pci_dev *pci_dev)\n{\n\tstruct ef4_nic *efx;\n\n\tefx = pci_get_drvdata(pci_dev);\n\tif (!efx)\n\t\treturn;\n\n\t \n\trtnl_lock();\n\tef4_dissociate(efx);\n\tdev_close(efx->net_dev);\n\tef4_disable_interrupts(efx);\n\tefx->state = STATE_UNINIT;\n\trtnl_unlock();\n\n\tef4_unregister_netdev(efx);\n\n\tef4_mtd_remove(efx);\n\n\tef4_pci_remove_main(efx);\n\n\tef4_fini_io(efx);\n\tnetif_dbg(efx, drv, efx->net_dev, \"shutdown successful\\n\");\n\n\tef4_fini_struct(efx);\n\tfree_netdev(efx->net_dev);\n};\n\n \nstatic void ef4_probe_vpd_strings(struct ef4_nic *efx)\n{\n\tstruct pci_dev *dev = efx->pci_dev;\n\tunsigned int vpd_size, kw_len;\n\tu8 *vpd_data;\n\tint start;\n\n\tvpd_data = pci_vpd_alloc(dev, &vpd_size);\n\tif (IS_ERR(vpd_data)) {\n\t\tpci_warn(dev, \"Unable to read VPD\\n\");\n\t\treturn;\n\t}\n\n\tstart = pci_vpd_find_ro_info_keyword(vpd_data, vpd_size,\n\t\t\t\t\t     PCI_VPD_RO_KEYWORD_PARTNO, &kw_len);\n\tif (start < 0)\n\t\tpci_warn(dev, \"Part number not found or incomplete\\n\");\n\telse\n\t\tpci_info(dev, \"Part Number : %.*s\\n\", kw_len, vpd_data + start);\n\n\tstart = pci_vpd_find_ro_info_keyword(vpd_data, vpd_size,\n\t\t\t\t\t     PCI_VPD_RO_KEYWORD_SERIALNO, &kw_len);\n\tif (start < 0)\n\t\tpci_warn(dev, \"Serial number not found or incomplete\\n\");\n\telse\n\t\tefx->vpd_sn = kmemdup_nul(vpd_data + start, kw_len, GFP_KERNEL);\n\n\tkfree(vpd_data);\n}\n\n\n \nstatic int ef4_pci_probe_main(struct ef4_nic *efx)\n{\n\tint rc;\n\n\t \n\trc = ef4_probe_all(efx);\n\tif (rc)\n\t\tgoto fail1;\n\n\tef4_init_napi(efx);\n\n\trc = efx->type->init(efx);\n\tif (rc) {\n\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t  \"failed to initialise NIC\\n\");\n\t\tgoto fail3;\n\t}\n\n\trc = ef4_init_port(efx);\n\tif (rc) {\n\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t  \"failed to initialise port\\n\");\n\t\tgoto fail4;\n\t}\n\n\trc = ef4_nic_init_interrupt(efx);\n\tif (rc)\n\t\tgoto fail5;\n\trc = ef4_enable_interrupts(efx);\n\tif (rc)\n\t\tgoto fail6;\n\n\treturn 0;\n\n fail6:\n\tef4_nic_fini_interrupt(efx);\n fail5:\n\tef4_fini_port(efx);\n fail4:\n\tefx->type->fini(efx);\n fail3:\n\tef4_fini_napi(efx);\n\tef4_remove_all(efx);\n fail1:\n\treturn rc;\n}\n\n \nstatic int ef4_pci_probe(struct pci_dev *pci_dev,\n\t\t\t const struct pci_device_id *entry)\n{\n\tstruct net_device *net_dev;\n\tstruct ef4_nic *efx;\n\tint rc;\n\n\t \n\tnet_dev = alloc_etherdev_mqs(sizeof(*efx), EF4_MAX_CORE_TX_QUEUES,\n\t\t\t\t     EF4_MAX_RX_QUEUES);\n\tif (!net_dev)\n\t\treturn -ENOMEM;\n\tefx = netdev_priv(net_dev);\n\tefx->type = (const struct ef4_nic_type *) entry->driver_data;\n\tefx->fixed_features |= NETIF_F_HIGHDMA;\n\n\tpci_set_drvdata(pci_dev, efx);\n\tSET_NETDEV_DEV(net_dev, &pci_dev->dev);\n\trc = ef4_init_struct(efx, pci_dev, net_dev);\n\tif (rc)\n\t\tgoto fail1;\n\n\tnetif_info(efx, probe, efx->net_dev,\n\t\t   \"Solarflare NIC detected\\n\");\n\n\tef4_probe_vpd_strings(efx);\n\n\t \n\trc = ef4_init_io(efx);\n\tif (rc)\n\t\tgoto fail2;\n\n\trc = ef4_pci_probe_main(efx);\n\tif (rc)\n\t\tgoto fail3;\n\n\tnet_dev->features |= (efx->type->offload_features | NETIF_F_SG |\n\t\t\t      NETIF_F_RXCSUM);\n\t \n\tnet_dev->vlan_features |= (NETIF_F_HW_CSUM | NETIF_F_SG |\n\t\t\t\t   NETIF_F_HIGHDMA | NETIF_F_RXCSUM);\n\n\tnet_dev->hw_features = net_dev->features & ~efx->fixed_features;\n\n\t \n\tnet_dev->features &= ~NETIF_F_HW_VLAN_CTAG_FILTER;\n\tnet_dev->features |= efx->fixed_features;\n\n\trc = ef4_register_netdev(efx);\n\tif (rc)\n\t\tgoto fail4;\n\n\tnetif_dbg(efx, probe, efx->net_dev, \"initialisation successful\\n\");\n\n\t \n\trtnl_lock();\n\trc = ef4_mtd_probe(efx);\n\trtnl_unlock();\n\tif (rc && rc != -EPERM)\n\t\tnetif_warn(efx, probe, efx->net_dev,\n\t\t\t   \"failed to create MTDs (%d)\\n\", rc);\n\n\treturn 0;\n\n fail4:\n\tef4_pci_remove_main(efx);\n fail3:\n\tef4_fini_io(efx);\n fail2:\n\tef4_fini_struct(efx);\n fail1:\n\tWARN_ON(rc > 0);\n\tnetif_dbg(efx, drv, efx->net_dev, \"initialisation failed. rc=%d\\n\", rc);\n\tfree_netdev(net_dev);\n\treturn rc;\n}\n\nstatic int ef4_pm_freeze(struct device *dev)\n{\n\tstruct ef4_nic *efx = dev_get_drvdata(dev);\n\n\trtnl_lock();\n\n\tif (efx->state != STATE_DISABLED) {\n\t\tefx->state = STATE_UNINIT;\n\n\t\tef4_device_detach_sync(efx);\n\n\t\tef4_stop_all(efx);\n\t\tef4_disable_interrupts(efx);\n\t}\n\n\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic int ef4_pm_thaw(struct device *dev)\n{\n\tint rc;\n\tstruct ef4_nic *efx = dev_get_drvdata(dev);\n\n\trtnl_lock();\n\n\tif (efx->state != STATE_DISABLED) {\n\t\trc = ef4_enable_interrupts(efx);\n\t\tif (rc)\n\t\t\tgoto fail;\n\n\t\tmutex_lock(&efx->mac_lock);\n\t\tefx->phy_op->reconfigure(efx);\n\t\tmutex_unlock(&efx->mac_lock);\n\n\t\tef4_start_all(efx);\n\n\t\tnetif_device_attach(efx->net_dev);\n\n\t\tefx->state = STATE_READY;\n\n\t\tefx->type->resume_wol(efx);\n\t}\n\n\trtnl_unlock();\n\n\t \n\tqueue_work(reset_workqueue, &efx->reset_work);\n\n\treturn 0;\n\nfail:\n\trtnl_unlock();\n\n\treturn rc;\n}\n\nstatic int ef4_pm_poweroff(struct device *dev)\n{\n\tstruct pci_dev *pci_dev = to_pci_dev(dev);\n\tstruct ef4_nic *efx = pci_get_drvdata(pci_dev);\n\n\tefx->type->fini(efx);\n\n\tefx->reset_pending = 0;\n\n\tpci_save_state(pci_dev);\n\treturn pci_set_power_state(pci_dev, PCI_D3hot);\n}\n\n \nstatic int ef4_pm_resume(struct device *dev)\n{\n\tstruct pci_dev *pci_dev = to_pci_dev(dev);\n\tstruct ef4_nic *efx = pci_get_drvdata(pci_dev);\n\tint rc;\n\n\trc = pci_set_power_state(pci_dev, PCI_D0);\n\tif (rc)\n\t\treturn rc;\n\tpci_restore_state(pci_dev);\n\trc = pci_enable_device(pci_dev);\n\tif (rc)\n\t\treturn rc;\n\tpci_set_master(efx->pci_dev);\n\trc = efx->type->reset(efx, RESET_TYPE_ALL);\n\tif (rc)\n\t\treturn rc;\n\trc = efx->type->init(efx);\n\tif (rc)\n\t\treturn rc;\n\trc = ef4_pm_thaw(dev);\n\treturn rc;\n}\n\nstatic int ef4_pm_suspend(struct device *dev)\n{\n\tint rc;\n\n\tef4_pm_freeze(dev);\n\trc = ef4_pm_poweroff(dev);\n\tif (rc)\n\t\tef4_pm_resume(dev);\n\treturn rc;\n}\n\nstatic const struct dev_pm_ops ef4_pm_ops = {\n\t.suspend\t= ef4_pm_suspend,\n\t.resume\t\t= ef4_pm_resume,\n\t.freeze\t\t= ef4_pm_freeze,\n\t.thaw\t\t= ef4_pm_thaw,\n\t.poweroff\t= ef4_pm_poweroff,\n\t.restore\t= ef4_pm_resume,\n};\n\n \nstatic pci_ers_result_t ef4_io_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t      pci_channel_state_t state)\n{\n\tpci_ers_result_t status = PCI_ERS_RESULT_RECOVERED;\n\tstruct ef4_nic *efx = pci_get_drvdata(pdev);\n\n\tif (state == pci_channel_io_perm_failure)\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\trtnl_lock();\n\n\tif (efx->state != STATE_DISABLED) {\n\t\tefx->state = STATE_RECOVERY;\n\t\tefx->reset_pending = 0;\n\n\t\tef4_device_detach_sync(efx);\n\n\t\tef4_stop_all(efx);\n\t\tef4_disable_interrupts(efx);\n\n\t\tstatus = PCI_ERS_RESULT_NEED_RESET;\n\t} else {\n\t\t \n\t\tstatus = PCI_ERS_RESULT_RECOVERED;\n\t}\n\n\trtnl_unlock();\n\n\tpci_disable_device(pdev);\n\n\treturn status;\n}\n\n \nstatic pci_ers_result_t ef4_io_slot_reset(struct pci_dev *pdev)\n{\n\tstruct ef4_nic *efx = pci_get_drvdata(pdev);\n\tpci_ers_result_t status = PCI_ERS_RESULT_RECOVERED;\n\n\tif (pci_enable_device(pdev)) {\n\t\tnetif_err(efx, hw, efx->net_dev,\n\t\t\t  \"Cannot re-enable PCI device after reset.\\n\");\n\t\tstatus =  PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\treturn status;\n}\n\n \nstatic void ef4_io_resume(struct pci_dev *pdev)\n{\n\tstruct ef4_nic *efx = pci_get_drvdata(pdev);\n\tint rc;\n\n\trtnl_lock();\n\n\tif (efx->state == STATE_DISABLED)\n\t\tgoto out;\n\n\trc = ef4_reset(efx, RESET_TYPE_ALL);\n\tif (rc) {\n\t\tnetif_err(efx, hw, efx->net_dev,\n\t\t\t  \"ef4_reset failed after PCI error (%d)\\n\", rc);\n\t} else {\n\t\tefx->state = STATE_READY;\n\t\tnetif_dbg(efx, hw, efx->net_dev,\n\t\t\t  \"Done resetting and resuming IO after PCI error.\\n\");\n\t}\n\nout:\n\trtnl_unlock();\n}\n\n \nstatic const struct pci_error_handlers ef4_err_handlers = {\n\t.error_detected = ef4_io_error_detected,\n\t.slot_reset\t= ef4_io_slot_reset,\n\t.resume\t\t= ef4_io_resume,\n};\n\nstatic struct pci_driver ef4_pci_driver = {\n\t.name\t\t= KBUILD_MODNAME,\n\t.id_table\t= ef4_pci_table,\n\t.probe\t\t= ef4_pci_probe,\n\t.remove\t\t= ef4_pci_remove,\n\t.driver.pm\t= &ef4_pm_ops,\n\t.err_handler\t= &ef4_err_handlers,\n};\n\n \n\nmodule_param(interrupt_mode, uint, 0444);\nMODULE_PARM_DESC(interrupt_mode,\n\t\t \"Interrupt mode (0=>MSIX 1=>MSI 2=>legacy)\");\n\nstatic int __init ef4_init_module(void)\n{\n\tint rc;\n\n\tprintk(KERN_INFO \"Solarflare Falcon driver v\" EF4_DRIVER_VERSION \"\\n\");\n\n\trc = register_netdevice_notifier(&ef4_netdev_notifier);\n\tif (rc)\n\t\tgoto err_notifier;\n\n\treset_workqueue = create_singlethread_workqueue(\"sfc_reset\");\n\tif (!reset_workqueue) {\n\t\trc = -ENOMEM;\n\t\tgoto err_reset;\n\t}\n\n\trc = pci_register_driver(&ef4_pci_driver);\n\tif (rc < 0)\n\t\tgoto err_pci;\n\n\treturn 0;\n\n err_pci:\n\tdestroy_workqueue(reset_workqueue);\n err_reset:\n\tunregister_netdevice_notifier(&ef4_netdev_notifier);\n err_notifier:\n\treturn rc;\n}\n\nstatic void __exit ef4_exit_module(void)\n{\n\tprintk(KERN_INFO \"Solarflare Falcon driver unloading\\n\");\n\n\tpci_unregister_driver(&ef4_pci_driver);\n\tdestroy_workqueue(reset_workqueue);\n\tunregister_netdevice_notifier(&ef4_netdev_notifier);\n\n}\n\nmodule_init(ef4_init_module);\nmodule_exit(ef4_exit_module);\n\nMODULE_AUTHOR(\"Solarflare Communications and \"\n\t      \"Michael Brown <mbrown@fensystems.co.uk>\");\nMODULE_DESCRIPTION(\"Solarflare Falcon network driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DEVICE_TABLE(pci, ef4_pci_table);\nMODULE_VERSION(EF4_DRIVER_VERSION);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}