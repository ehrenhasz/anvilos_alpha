{
  "module_name": "rx.c",
  "hash_id": "5811c21dec396cfcc01c679169792c01f72734877caa11ed2c3ed47a4fb9f604",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/falcon/rx.c",
  "human_readable_source": "\n \n\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/slab.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/prefetch.h>\n#include <linux/moduleparam.h>\n#include <linux/iommu.h>\n#include <net/ip.h>\n#include <net/checksum.h>\n#include \"net_driver.h\"\n#include \"efx.h\"\n#include \"filter.h\"\n#include \"nic.h\"\n#include \"selftest.h\"\n#include \"workarounds.h\"\n\n \n#define EF4_RX_PREFERRED_BATCH 8U\n\n \n#define EF4_RECYCLE_RING_SIZE_IOMMU 4096\n#define EF4_RECYCLE_RING_SIZE_NOIOMMU (2 * EF4_RX_PREFERRED_BATCH)\n\n \n#define EF4_SKB_HEADERS  128u\n\n \nstatic unsigned int rx_refill_threshold;\n\n \n#define EF4_RX_MAX_FRAGS DIV_ROUND_UP(EF4_MAX_FRAME_LEN(EF4_MAX_MTU), \\\n\t\t\t\t      EF4_RX_USR_BUF_SIZE)\n\n \n#define EF4_RXD_HEAD_ROOM (1 + EF4_RX_MAX_FRAGS)\n\nstatic inline u8 *ef4_rx_buf_va(struct ef4_rx_buffer *buf)\n{\n\treturn page_address(buf->page) + buf->page_offset;\n}\n\nstatic inline u32 ef4_rx_buf_hash(struct ef4_nic *efx, const u8 *eh)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\treturn __le32_to_cpup((const __le32 *)(eh + efx->rx_packet_hash_offset));\n#else\n\tconst u8 *data = eh + efx->rx_packet_hash_offset;\n\treturn (u32)data[0]\t  |\n\t       (u32)data[1] << 8  |\n\t       (u32)data[2] << 16 |\n\t       (u32)data[3] << 24;\n#endif\n}\n\nstatic inline struct ef4_rx_buffer *\nef4_rx_buf_next(struct ef4_rx_queue *rx_queue, struct ef4_rx_buffer *rx_buf)\n{\n\tif (unlikely(rx_buf == ef4_rx_buffer(rx_queue, rx_queue->ptr_mask)))\n\t\treturn ef4_rx_buffer(rx_queue, 0);\n\telse\n\t\treturn rx_buf + 1;\n}\n\nstatic inline void ef4_sync_rx_buffer(struct ef4_nic *efx,\n\t\t\t\t      struct ef4_rx_buffer *rx_buf,\n\t\t\t\t      unsigned int len)\n{\n\tdma_sync_single_for_cpu(&efx->pci_dev->dev, rx_buf->dma_addr, len,\n\t\t\t\tDMA_FROM_DEVICE);\n}\n\nvoid ef4_rx_config_page_split(struct ef4_nic *efx)\n{\n\tefx->rx_page_buf_step = ALIGN(efx->rx_dma_len + efx->rx_ip_align,\n\t\t\t\t      EF4_RX_BUF_ALIGNMENT);\n\tefx->rx_bufs_per_page = efx->rx_buffer_order ? 1 :\n\t\t((PAGE_SIZE - sizeof(struct ef4_rx_page_state)) /\n\t\t efx->rx_page_buf_step);\n\tefx->rx_buffer_truesize = (PAGE_SIZE << efx->rx_buffer_order) /\n\t\tefx->rx_bufs_per_page;\n\tefx->rx_pages_per_batch = DIV_ROUND_UP(EF4_RX_PREFERRED_BATCH,\n\t\t\t\t\t       efx->rx_bufs_per_page);\n}\n\n \nstatic struct page *ef4_reuse_page(struct ef4_rx_queue *rx_queue)\n{\n\tstruct ef4_nic *efx = rx_queue->efx;\n\tstruct page *page;\n\tstruct ef4_rx_page_state *state;\n\tunsigned index;\n\n\tif (unlikely(!rx_queue->page_ring))\n\t\treturn NULL;\n\tindex = rx_queue->page_remove & rx_queue->page_ptr_mask;\n\tpage = rx_queue->page_ring[index];\n\tif (page == NULL)\n\t\treturn NULL;\n\n\trx_queue->page_ring[index] = NULL;\n\t \n\tif (rx_queue->page_remove != rx_queue->page_add)\n\t\t++rx_queue->page_remove;\n\n\t \n\tif (page_count(page) == 1) {\n\t\t++rx_queue->page_recycle_count;\n\t\treturn page;\n\t} else {\n\t\tstate = page_address(page);\n\t\tdma_unmap_page(&efx->pci_dev->dev, state->dma_addr,\n\t\t\t       PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(page);\n\t\t++rx_queue->page_recycle_failed;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic int ef4_init_rx_buffers(struct ef4_rx_queue *rx_queue, bool atomic)\n{\n\tstruct ef4_nic *efx = rx_queue->efx;\n\tstruct ef4_rx_buffer *rx_buf;\n\tstruct page *page;\n\tunsigned int page_offset;\n\tstruct ef4_rx_page_state *state;\n\tdma_addr_t dma_addr;\n\tunsigned index, count;\n\n\tcount = 0;\n\tdo {\n\t\tpage = ef4_reuse_page(rx_queue);\n\t\tif (page == NULL) {\n\t\t\tpage = alloc_pages(__GFP_COMP |\n\t\t\t\t\t   (atomic ? GFP_ATOMIC : GFP_KERNEL),\n\t\t\t\t\t   efx->rx_buffer_order);\n\t\t\tif (unlikely(page == NULL))\n\t\t\t\treturn -ENOMEM;\n\t\t\tdma_addr =\n\t\t\t\tdma_map_page(&efx->pci_dev->dev, page, 0,\n\t\t\t\t\t     PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t\t\t     DMA_FROM_DEVICE);\n\t\t\tif (unlikely(dma_mapping_error(&efx->pci_dev->dev,\n\t\t\t\t\t\t       dma_addr))) {\n\t\t\t\t__free_pages(page, efx->rx_buffer_order);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t\tstate = page_address(page);\n\t\t\tstate->dma_addr = dma_addr;\n\t\t} else {\n\t\t\tstate = page_address(page);\n\t\t\tdma_addr = state->dma_addr;\n\t\t}\n\n\t\tdma_addr += sizeof(struct ef4_rx_page_state);\n\t\tpage_offset = sizeof(struct ef4_rx_page_state);\n\n\t\tdo {\n\t\t\tindex = rx_queue->added_count & rx_queue->ptr_mask;\n\t\t\trx_buf = ef4_rx_buffer(rx_queue, index);\n\t\t\trx_buf->dma_addr = dma_addr + efx->rx_ip_align;\n\t\t\trx_buf->page = page;\n\t\t\trx_buf->page_offset = page_offset + efx->rx_ip_align;\n\t\t\trx_buf->len = efx->rx_dma_len;\n\t\t\trx_buf->flags = 0;\n\t\t\t++rx_queue->added_count;\n\t\t\tget_page(page);\n\t\t\tdma_addr += efx->rx_page_buf_step;\n\t\t\tpage_offset += efx->rx_page_buf_step;\n\t\t} while (page_offset + efx->rx_page_buf_step <= PAGE_SIZE);\n\n\t\trx_buf->flags = EF4_RX_BUF_LAST_IN_PAGE;\n\t} while (++count < efx->rx_pages_per_batch);\n\n\treturn 0;\n}\n\n \nstatic void ef4_unmap_rx_buffer(struct ef4_nic *efx,\n\t\t\t\tstruct ef4_rx_buffer *rx_buf)\n{\n\tstruct page *page = rx_buf->page;\n\n\tif (page) {\n\t\tstruct ef4_rx_page_state *state = page_address(page);\n\t\tdma_unmap_page(&efx->pci_dev->dev,\n\t\t\t       state->dma_addr,\n\t\t\t       PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t       DMA_FROM_DEVICE);\n\t}\n}\n\nstatic void ef4_free_rx_buffers(struct ef4_rx_queue *rx_queue,\n\t\t\t\tstruct ef4_rx_buffer *rx_buf,\n\t\t\t\tunsigned int num_bufs)\n{\n\tdo {\n\t\tif (rx_buf->page) {\n\t\t\tput_page(rx_buf->page);\n\t\t\trx_buf->page = NULL;\n\t\t}\n\t\trx_buf = ef4_rx_buf_next(rx_queue, rx_buf);\n\t} while (--num_bufs);\n}\n\n \nstatic void ef4_recycle_rx_page(struct ef4_channel *channel,\n\t\t\t\tstruct ef4_rx_buffer *rx_buf)\n{\n\tstruct page *page = rx_buf->page;\n\tstruct ef4_rx_queue *rx_queue = ef4_channel_get_rx_queue(channel);\n\tstruct ef4_nic *efx = rx_queue->efx;\n\tunsigned index;\n\n\t \n\tif (!(rx_buf->flags & EF4_RX_BUF_LAST_IN_PAGE))\n\t\treturn;\n\n\tindex = rx_queue->page_add & rx_queue->page_ptr_mask;\n\tif (rx_queue->page_ring[index] == NULL) {\n\t\tunsigned read_index = rx_queue->page_remove &\n\t\t\trx_queue->page_ptr_mask;\n\n\t\t \n\t\tif (read_index == index)\n\t\t\t++rx_queue->page_remove;\n\t\trx_queue->page_ring[index] = page;\n\t\t++rx_queue->page_add;\n\t\treturn;\n\t}\n\t++rx_queue->page_recycle_full;\n\tef4_unmap_rx_buffer(efx, rx_buf);\n\tput_page(rx_buf->page);\n}\n\nstatic void ef4_fini_rx_buffer(struct ef4_rx_queue *rx_queue,\n\t\t\t       struct ef4_rx_buffer *rx_buf)\n{\n\t \n\tif (rx_buf->page)\n\t\tput_page(rx_buf->page);\n\n\t \n\tif (rx_buf->flags & EF4_RX_BUF_LAST_IN_PAGE) {\n\t\tef4_unmap_rx_buffer(rx_queue->efx, rx_buf);\n\t\tef4_free_rx_buffers(rx_queue, rx_buf, 1);\n\t}\n\trx_buf->page = NULL;\n}\n\n \nstatic void ef4_recycle_rx_pages(struct ef4_channel *channel,\n\t\t\t\t struct ef4_rx_buffer *rx_buf,\n\t\t\t\t unsigned int n_frags)\n{\n\tstruct ef4_rx_queue *rx_queue = ef4_channel_get_rx_queue(channel);\n\n\tif (unlikely(!rx_queue->page_ring))\n\t\treturn;\n\n\tdo {\n\t\tef4_recycle_rx_page(channel, rx_buf);\n\t\trx_buf = ef4_rx_buf_next(rx_queue, rx_buf);\n\t} while (--n_frags);\n}\n\nstatic void ef4_discard_rx_packet(struct ef4_channel *channel,\n\t\t\t\t  struct ef4_rx_buffer *rx_buf,\n\t\t\t\t  unsigned int n_frags)\n{\n\tstruct ef4_rx_queue *rx_queue = ef4_channel_get_rx_queue(channel);\n\n\tef4_recycle_rx_pages(channel, rx_buf, n_frags);\n\n\tef4_free_rx_buffers(rx_queue, rx_buf, n_frags);\n}\n\n \nvoid ef4_fast_push_rx_descriptors(struct ef4_rx_queue *rx_queue, bool atomic)\n{\n\tstruct ef4_nic *efx = rx_queue->efx;\n\tunsigned int fill_level, batch_size;\n\tint space, rc = 0;\n\n\tif (!rx_queue->refill_enabled)\n\t\treturn;\n\n\t \n\tfill_level = (rx_queue->added_count - rx_queue->removed_count);\n\tEF4_BUG_ON_PARANOID(fill_level > rx_queue->efx->rxq_entries);\n\tif (fill_level >= rx_queue->fast_fill_trigger)\n\t\tgoto out;\n\n\t \n\tif (unlikely(fill_level < rx_queue->min_fill)) {\n\t\tif (fill_level)\n\t\t\trx_queue->min_fill = fill_level;\n\t}\n\n\tbatch_size = efx->rx_pages_per_batch * efx->rx_bufs_per_page;\n\tspace = rx_queue->max_fill - fill_level;\n\tEF4_BUG_ON_PARANOID(space < batch_size);\n\n\tnetif_vdbg(rx_queue->efx, rx_status, rx_queue->efx->net_dev,\n\t\t   \"RX queue %d fast-filling descriptor ring from\"\n\t\t   \" level %d to level %d\\n\",\n\t\t   ef4_rx_queue_index(rx_queue), fill_level,\n\t\t   rx_queue->max_fill);\n\n\n\tdo {\n\t\trc = ef4_init_rx_buffers(rx_queue, atomic);\n\t\tif (unlikely(rc)) {\n\t\t\t \n\t\t\tif (rx_queue->added_count == rx_queue->removed_count)\n\t\t\t\tef4_schedule_slow_fill(rx_queue);\n\t\t\tgoto out;\n\t\t}\n\t} while ((space -= batch_size) >= batch_size);\n\n\tnetif_vdbg(rx_queue->efx, rx_status, rx_queue->efx->net_dev,\n\t\t   \"RX queue %d fast-filled descriptor ring \"\n\t\t   \"to level %d\\n\", ef4_rx_queue_index(rx_queue),\n\t\t   rx_queue->added_count - rx_queue->removed_count);\n\n out:\n\tif (rx_queue->notified_count != rx_queue->added_count)\n\t\tef4_nic_notify_rx_desc(rx_queue);\n}\n\nvoid ef4_rx_slow_fill(struct timer_list *t)\n{\n\tstruct ef4_rx_queue *rx_queue = from_timer(rx_queue, t, slow_fill);\n\n\t \n\tef4_nic_generate_fill_event(rx_queue);\n\t++rx_queue->slow_fill_count;\n}\n\nstatic void ef4_rx_packet__check_len(struct ef4_rx_queue *rx_queue,\n\t\t\t\t     struct ef4_rx_buffer *rx_buf,\n\t\t\t\t     int len)\n{\n\tstruct ef4_nic *efx = rx_queue->efx;\n\tunsigned max_len = rx_buf->len - efx->type->rx_buffer_padding;\n\n\tif (likely(len <= max_len))\n\t\treturn;\n\n\t \n\trx_buf->flags |= EF4_RX_PKT_DISCARD;\n\n\tif ((len > rx_buf->len) && EF4_WORKAROUND_8071(efx)) {\n\t\tif (net_ratelimit())\n\t\t\tnetif_err(efx, rx_err, efx->net_dev,\n\t\t\t\t  \" RX queue %d seriously overlength \"\n\t\t\t\t  \"RX event (0x%x > 0x%x+0x%x). Leaking\\n\",\n\t\t\t\t  ef4_rx_queue_index(rx_queue), len, max_len,\n\t\t\t\t  efx->type->rx_buffer_padding);\n\t\tef4_schedule_reset(efx, RESET_TYPE_RX_RECOVERY);\n\t} else {\n\t\tif (net_ratelimit())\n\t\t\tnetif_err(efx, rx_err, efx->net_dev,\n\t\t\t\t  \" RX queue %d overlength RX event \"\n\t\t\t\t  \"(0x%x > 0x%x)\\n\",\n\t\t\t\t  ef4_rx_queue_index(rx_queue), len, max_len);\n\t}\n\n\tef4_rx_queue_channel(rx_queue)->n_rx_overlength++;\n}\n\n \nstatic void\nef4_rx_packet_gro(struct ef4_channel *channel, struct ef4_rx_buffer *rx_buf,\n\t\t  unsigned int n_frags, u8 *eh)\n{\n\tstruct napi_struct *napi = &channel->napi_str;\n\tstruct ef4_nic *efx = channel->efx;\n\tstruct sk_buff *skb;\n\n\tskb = napi_get_frags(napi);\n\tif (unlikely(!skb)) {\n\t\tstruct ef4_rx_queue *rx_queue;\n\n\t\trx_queue = ef4_channel_get_rx_queue(channel);\n\t\tef4_free_rx_buffers(rx_queue, rx_buf, n_frags);\n\t\treturn;\n\t}\n\n\tif (efx->net_dev->features & NETIF_F_RXHASH)\n\t\tskb_set_hash(skb, ef4_rx_buf_hash(efx, eh),\n\t\t\t     PKT_HASH_TYPE_L3);\n\tskb->ip_summed = ((rx_buf->flags & EF4_RX_PKT_CSUMMED) ?\n\t\t\t  CHECKSUM_UNNECESSARY : CHECKSUM_NONE);\n\n\tfor (;;) {\n\t\tskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t   rx_buf->page, rx_buf->page_offset,\n\t\t\t\t   rx_buf->len);\n\t\trx_buf->page = NULL;\n\t\tskb->len += rx_buf->len;\n\t\tif (skb_shinfo(skb)->nr_frags == n_frags)\n\t\t\tbreak;\n\n\t\trx_buf = ef4_rx_buf_next(&channel->rx_queue, rx_buf);\n\t}\n\n\tskb->data_len = skb->len;\n\tskb->truesize += n_frags * efx->rx_buffer_truesize;\n\n\tskb_record_rx_queue(skb, channel->rx_queue.core_index);\n\n\tnapi_gro_frags(napi);\n}\n\n \nstatic struct sk_buff *ef4_rx_mk_skb(struct ef4_channel *channel,\n\t\t\t\t     struct ef4_rx_buffer *rx_buf,\n\t\t\t\t     unsigned int n_frags,\n\t\t\t\t     u8 *eh, int hdr_len)\n{\n\tstruct ef4_nic *efx = channel->efx;\n\tstruct sk_buff *skb;\n\n\t \n\tskb = netdev_alloc_skb(efx->net_dev,\n\t\t\t       efx->rx_ip_align + efx->rx_prefix_size +\n\t\t\t       hdr_len);\n\tif (unlikely(skb == NULL)) {\n\t\tatomic_inc(&efx->n_rx_noskb_drops);\n\t\treturn NULL;\n\t}\n\n\tEF4_BUG_ON_PARANOID(rx_buf->len < hdr_len);\n\n\tmemcpy(skb->data + efx->rx_ip_align, eh - efx->rx_prefix_size,\n\t       efx->rx_prefix_size + hdr_len);\n\tskb_reserve(skb, efx->rx_ip_align + efx->rx_prefix_size);\n\t__skb_put(skb, hdr_len);\n\n\t \n\tif (rx_buf->len > hdr_len) {\n\t\trx_buf->page_offset += hdr_len;\n\t\trx_buf->len -= hdr_len;\n\n\t\tfor (;;) {\n\t\t\tskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t\t   rx_buf->page, rx_buf->page_offset,\n\t\t\t\t\t   rx_buf->len);\n\t\t\trx_buf->page = NULL;\n\t\t\tskb->len += rx_buf->len;\n\t\t\tskb->data_len += rx_buf->len;\n\t\t\tif (skb_shinfo(skb)->nr_frags == n_frags)\n\t\t\t\tbreak;\n\n\t\t\trx_buf = ef4_rx_buf_next(&channel->rx_queue, rx_buf);\n\t\t}\n\t} else {\n\t\t__free_pages(rx_buf->page, efx->rx_buffer_order);\n\t\trx_buf->page = NULL;\n\t\tn_frags = 0;\n\t}\n\n\tskb->truesize += n_frags * efx->rx_buffer_truesize;\n\n\t \n\tskb->protocol = eth_type_trans(skb, efx->net_dev);\n\n\tskb_mark_napi_id(skb, &channel->napi_str);\n\n\treturn skb;\n}\n\nvoid ef4_rx_packet(struct ef4_rx_queue *rx_queue, unsigned int index,\n\t\t   unsigned int n_frags, unsigned int len, u16 flags)\n{\n\tstruct ef4_nic *efx = rx_queue->efx;\n\tstruct ef4_channel *channel = ef4_rx_queue_channel(rx_queue);\n\tstruct ef4_rx_buffer *rx_buf;\n\n\trx_queue->rx_packets++;\n\n\trx_buf = ef4_rx_buffer(rx_queue, index);\n\trx_buf->flags |= flags;\n\n\t \n\tif (n_frags == 1) {\n\t\tif (!(flags & EF4_RX_PKT_PREFIX_LEN))\n\t\t\tef4_rx_packet__check_len(rx_queue, rx_buf, len);\n\t} else if (unlikely(n_frags > EF4_RX_MAX_FRAGS) ||\n\t\t   unlikely(len <= (n_frags - 1) * efx->rx_dma_len) ||\n\t\t   unlikely(len > n_frags * efx->rx_dma_len) ||\n\t\t   unlikely(!efx->rx_scatter)) {\n\t\t \n\t\tWARN_ON(!(len == 0 && rx_buf->flags & EF4_RX_PKT_DISCARD));\n\t\trx_buf->flags |= EF4_RX_PKT_DISCARD;\n\t}\n\n\tnetif_vdbg(efx, rx_status, efx->net_dev,\n\t\t   \"RX queue %d received ids %x-%x len %d %s%s\\n\",\n\t\t   ef4_rx_queue_index(rx_queue), index,\n\t\t   (index + n_frags - 1) & rx_queue->ptr_mask, len,\n\t\t   (rx_buf->flags & EF4_RX_PKT_CSUMMED) ? \" [SUMMED]\" : \"\",\n\t\t   (rx_buf->flags & EF4_RX_PKT_DISCARD) ? \" [DISCARD]\" : \"\");\n\n\t \n\tif (unlikely(rx_buf->flags & EF4_RX_PKT_DISCARD)) {\n\t\tef4_rx_flush_packet(channel);\n\t\tef4_discard_rx_packet(channel, rx_buf, n_frags);\n\t\treturn;\n\t}\n\n\tif (n_frags == 1 && !(flags & EF4_RX_PKT_PREFIX_LEN))\n\t\trx_buf->len = len;\n\n\t \n\tef4_sync_rx_buffer(efx, rx_buf, rx_buf->len);\n\n\t \n\tprefetch(ef4_rx_buf_va(rx_buf));\n\n\trx_buf->page_offset += efx->rx_prefix_size;\n\trx_buf->len -= efx->rx_prefix_size;\n\n\tif (n_frags > 1) {\n\t\t \n\t\tunsigned int tail_frags = n_frags - 1;\n\n\t\tfor (;;) {\n\t\t\trx_buf = ef4_rx_buf_next(rx_queue, rx_buf);\n\t\t\tif (--tail_frags == 0)\n\t\t\t\tbreak;\n\t\t\tef4_sync_rx_buffer(efx, rx_buf, efx->rx_dma_len);\n\t\t}\n\t\trx_buf->len = len - (n_frags - 1) * efx->rx_dma_len;\n\t\tef4_sync_rx_buffer(efx, rx_buf, rx_buf->len);\n\t}\n\n\t \n\trx_buf = ef4_rx_buffer(rx_queue, index);\n\tef4_recycle_rx_pages(channel, rx_buf, n_frags);\n\n\t \n\tef4_rx_flush_packet(channel);\n\tchannel->rx_pkt_n_frags = n_frags;\n\tchannel->rx_pkt_index = index;\n}\n\nstatic void ef4_rx_deliver(struct ef4_channel *channel, u8 *eh,\n\t\t\t   struct ef4_rx_buffer *rx_buf,\n\t\t\t   unsigned int n_frags)\n{\n\tstruct sk_buff *skb;\n\tu16 hdr_len = min_t(u16, rx_buf->len, EF4_SKB_HEADERS);\n\n\tskb = ef4_rx_mk_skb(channel, rx_buf, n_frags, eh, hdr_len);\n\tif (unlikely(skb == NULL)) {\n\t\tstruct ef4_rx_queue *rx_queue;\n\n\t\trx_queue = ef4_channel_get_rx_queue(channel);\n\t\tef4_free_rx_buffers(rx_queue, rx_buf, n_frags);\n\t\treturn;\n\t}\n\tskb_record_rx_queue(skb, channel->rx_queue.core_index);\n\n\t \n\tskb_checksum_none_assert(skb);\n\tif (likely(rx_buf->flags & EF4_RX_PKT_CSUMMED))\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tif (channel->type->receive_skb)\n\t\tif (channel->type->receive_skb(channel, skb))\n\t\t\treturn;\n\n\t \n\tnetif_receive_skb(skb);\n}\n\n \nvoid __ef4_rx_packet(struct ef4_channel *channel)\n{\n\tstruct ef4_nic *efx = channel->efx;\n\tstruct ef4_rx_buffer *rx_buf =\n\t\tef4_rx_buffer(&channel->rx_queue, channel->rx_pkt_index);\n\tu8 *eh = ef4_rx_buf_va(rx_buf);\n\n\t \n\tif (rx_buf->flags & EF4_RX_PKT_PREFIX_LEN)\n\t\trx_buf->len = le16_to_cpup((__le16 *)\n\t\t\t\t\t   (eh + efx->rx_packet_len_offset));\n\n\t \n\tif (unlikely(efx->loopback_selftest)) {\n\t\tstruct ef4_rx_queue *rx_queue;\n\n\t\tef4_loopback_rx_packet(efx, eh, rx_buf->len);\n\t\trx_queue = ef4_channel_get_rx_queue(channel);\n\t\tef4_free_rx_buffers(rx_queue, rx_buf,\n\t\t\t\t    channel->rx_pkt_n_frags);\n\t\tgoto out;\n\t}\n\n\tif (unlikely(!(efx->net_dev->features & NETIF_F_RXCSUM)))\n\t\trx_buf->flags &= ~EF4_RX_PKT_CSUMMED;\n\n\tif ((rx_buf->flags & EF4_RX_PKT_TCP) && !channel->type->receive_skb)\n\t\tef4_rx_packet_gro(channel, rx_buf, channel->rx_pkt_n_frags, eh);\n\telse\n\t\tef4_rx_deliver(channel, eh, rx_buf, channel->rx_pkt_n_frags);\nout:\n\tchannel->rx_pkt_n_frags = 0;\n}\n\nint ef4_probe_rx_queue(struct ef4_rx_queue *rx_queue)\n{\n\tstruct ef4_nic *efx = rx_queue->efx;\n\tunsigned int entries;\n\tint rc;\n\n\t \n\tentries = max(roundup_pow_of_two(efx->rxq_entries), EF4_MIN_DMAQ_SIZE);\n\tEF4_BUG_ON_PARANOID(entries > EF4_MAX_DMAQ_SIZE);\n\trx_queue->ptr_mask = entries - 1;\n\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"creating RX queue %d size %#x mask %#x\\n\",\n\t\t  ef4_rx_queue_index(rx_queue), efx->rxq_entries,\n\t\t  rx_queue->ptr_mask);\n\n\t \n\trx_queue->buffer = kcalloc(entries, sizeof(*rx_queue->buffer),\n\t\t\t\t   GFP_KERNEL);\n\tif (!rx_queue->buffer)\n\t\treturn -ENOMEM;\n\n\trc = ef4_nic_probe_rx(rx_queue);\n\tif (rc) {\n\t\tkfree(rx_queue->buffer);\n\t\trx_queue->buffer = NULL;\n\t}\n\n\treturn rc;\n}\n\nstatic void ef4_init_rx_recycle_ring(struct ef4_nic *efx,\n\t\t\t\t     struct ef4_rx_queue *rx_queue)\n{\n\tunsigned int bufs_in_recycle_ring, page_ring_size;\n\tstruct iommu_domain __maybe_unused *domain;\n\n\t \n#ifdef CONFIG_PPC64\n\tbufs_in_recycle_ring = EF4_RECYCLE_RING_SIZE_IOMMU;\n#else\n\tdomain = iommu_get_domain_for_dev(&efx->pci_dev->dev);\n\tif (domain && domain->type != IOMMU_DOMAIN_IDENTITY)\n\t\tbufs_in_recycle_ring = EF4_RECYCLE_RING_SIZE_IOMMU;\n\telse\n\t\tbufs_in_recycle_ring = EF4_RECYCLE_RING_SIZE_NOIOMMU;\n#endif  \n\n\tpage_ring_size = roundup_pow_of_two(bufs_in_recycle_ring /\n\t\t\t\t\t    efx->rx_bufs_per_page);\n\trx_queue->page_ring = kcalloc(page_ring_size,\n\t\t\t\t      sizeof(*rx_queue->page_ring), GFP_KERNEL);\n\tif (!rx_queue->page_ring)\n\t\trx_queue->page_ptr_mask = 0;\n\telse\n\t\trx_queue->page_ptr_mask = page_ring_size - 1;\n}\n\nvoid ef4_init_rx_queue(struct ef4_rx_queue *rx_queue)\n{\n\tstruct ef4_nic *efx = rx_queue->efx;\n\tunsigned int max_fill, trigger, max_trigger;\n\n\tnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\n\t\t  \"initialising RX queue %d\\n\", ef4_rx_queue_index(rx_queue));\n\n\t \n\trx_queue->added_count = 0;\n\trx_queue->notified_count = 0;\n\trx_queue->removed_count = 0;\n\trx_queue->min_fill = -1U;\n\tef4_init_rx_recycle_ring(efx, rx_queue);\n\n\trx_queue->page_remove = 0;\n\trx_queue->page_add = rx_queue->page_ptr_mask + 1;\n\trx_queue->page_recycle_count = 0;\n\trx_queue->page_recycle_failed = 0;\n\trx_queue->page_recycle_full = 0;\n\n\t \n\tmax_fill = efx->rxq_entries - EF4_RXD_HEAD_ROOM;\n\tmax_trigger =\n\t\tmax_fill - efx->rx_pages_per_batch * efx->rx_bufs_per_page;\n\tif (rx_refill_threshold != 0) {\n\t\ttrigger = max_fill * min(rx_refill_threshold, 100U) / 100U;\n\t\tif (trigger > max_trigger)\n\t\t\ttrigger = max_trigger;\n\t} else {\n\t\ttrigger = max_trigger;\n\t}\n\n\trx_queue->max_fill = max_fill;\n\trx_queue->fast_fill_trigger = trigger;\n\trx_queue->refill_enabled = true;\n\n\t \n\tef4_nic_init_rx(rx_queue);\n}\n\nvoid ef4_fini_rx_queue(struct ef4_rx_queue *rx_queue)\n{\n\tint i;\n\tstruct ef4_nic *efx = rx_queue->efx;\n\tstruct ef4_rx_buffer *rx_buf;\n\n\tnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\n\t\t  \"shutting down RX queue %d\\n\", ef4_rx_queue_index(rx_queue));\n\n\tdel_timer_sync(&rx_queue->slow_fill);\n\n\t \n\tif (rx_queue->buffer) {\n\t\tfor (i = rx_queue->removed_count; i < rx_queue->added_count;\n\t\t     i++) {\n\t\t\tunsigned index = i & rx_queue->ptr_mask;\n\t\t\trx_buf = ef4_rx_buffer(rx_queue, index);\n\t\t\tef4_fini_rx_buffer(rx_queue, rx_buf);\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i <= rx_queue->page_ptr_mask; i++) {\n\t\tstruct page *page = rx_queue->page_ring[i];\n\t\tstruct ef4_rx_page_state *state;\n\n\t\tif (page == NULL)\n\t\t\tcontinue;\n\n\t\tstate = page_address(page);\n\t\tdma_unmap_page(&efx->pci_dev->dev, state->dma_addr,\n\t\t\t       PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(page);\n\t}\n\tkfree(rx_queue->page_ring);\n\trx_queue->page_ring = NULL;\n}\n\nvoid ef4_remove_rx_queue(struct ef4_rx_queue *rx_queue)\n{\n\tnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\n\t\t  \"destroying RX queue %d\\n\", ef4_rx_queue_index(rx_queue));\n\n\tef4_nic_remove_rx(rx_queue);\n\n\tkfree(rx_queue->buffer);\n\trx_queue->buffer = NULL;\n}\n\n\nmodule_param(rx_refill_threshold, uint, 0444);\nMODULE_PARM_DESC(rx_refill_threshold,\n\t\t \"RX descriptor ring refill threshold (%)\");\n\n#ifdef CONFIG_RFS_ACCEL\n\nint ef4_filter_rfs(struct net_device *net_dev, const struct sk_buff *skb,\n\t\t   u16 rxq_index, u32 flow_id)\n{\n\tstruct ef4_nic *efx = netdev_priv(net_dev);\n\tstruct ef4_channel *channel;\n\tstruct ef4_filter_spec spec;\n\tstruct flow_keys fk;\n\tint rc;\n\n\tif (flow_id == RPS_FLOW_ID_INVALID)\n\t\treturn -EINVAL;\n\n\tif (!skb_flow_dissect_flow_keys(skb, &fk, 0))\n\t\treturn -EPROTONOSUPPORT;\n\n\tif (fk.basic.n_proto != htons(ETH_P_IP) && fk.basic.n_proto != htons(ETH_P_IPV6))\n\t\treturn -EPROTONOSUPPORT;\n\tif (fk.control.flags & FLOW_DIS_IS_FRAGMENT)\n\t\treturn -EPROTONOSUPPORT;\n\n\tef4_filter_init_rx(&spec, EF4_FILTER_PRI_HINT,\n\t\t\t   efx->rx_scatter ? EF4_FILTER_FLAG_RX_SCATTER : 0,\n\t\t\t   rxq_index);\n\tspec.match_flags =\n\t\tEF4_FILTER_MATCH_ETHER_TYPE | EF4_FILTER_MATCH_IP_PROTO |\n\t\tEF4_FILTER_MATCH_LOC_HOST | EF4_FILTER_MATCH_LOC_PORT |\n\t\tEF4_FILTER_MATCH_REM_HOST | EF4_FILTER_MATCH_REM_PORT;\n\tspec.ether_type = fk.basic.n_proto;\n\tspec.ip_proto = fk.basic.ip_proto;\n\n\tif (fk.basic.n_proto == htons(ETH_P_IP)) {\n\t\tspec.rem_host[0] = fk.addrs.v4addrs.src;\n\t\tspec.loc_host[0] = fk.addrs.v4addrs.dst;\n\t} else {\n\t\tmemcpy(spec.rem_host, &fk.addrs.v6addrs.src, sizeof(struct in6_addr));\n\t\tmemcpy(spec.loc_host, &fk.addrs.v6addrs.dst, sizeof(struct in6_addr));\n\t}\n\n\tspec.rem_port = fk.ports.src;\n\tspec.loc_port = fk.ports.dst;\n\n\trc = efx->type->filter_rfs_insert(efx, &spec);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tchannel = ef4_get_channel(efx, rxq_index);\n\tchannel->rps_flow_id[rc] = flow_id;\n\t++channel->rfs_filters_added;\n\n\tif (spec.ether_type == htons(ETH_P_IP))\n\t\tnetif_info(efx, rx_status, efx->net_dev,\n\t\t\t   \"steering %s %pI4:%u:%pI4:%u to queue %u [flow %u filter %d]\\n\",\n\t\t\t   (spec.ip_proto == IPPROTO_TCP) ? \"TCP\" : \"UDP\",\n\t\t\t   spec.rem_host, ntohs(spec.rem_port), spec.loc_host,\n\t\t\t   ntohs(spec.loc_port), rxq_index, flow_id, rc);\n\telse\n\t\tnetif_info(efx, rx_status, efx->net_dev,\n\t\t\t   \"steering %s [%pI6]:%u:[%pI6]:%u to queue %u [flow %u filter %d]\\n\",\n\t\t\t   (spec.ip_proto == IPPROTO_TCP) ? \"TCP\" : \"UDP\",\n\t\t\t   spec.rem_host, ntohs(spec.rem_port), spec.loc_host,\n\t\t\t   ntohs(spec.loc_port), rxq_index, flow_id, rc);\n\n\treturn rc;\n}\n\nbool __ef4_filter_rfs_expire(struct ef4_nic *efx, unsigned int quota)\n{\n\tbool (*expire_one)(struct ef4_nic *efx, u32 flow_id, unsigned int index);\n\tunsigned int channel_idx, index, size;\n\tu32 flow_id;\n\n\tif (!spin_trylock_bh(&efx->filter_lock))\n\t\treturn false;\n\n\texpire_one = efx->type->filter_rfs_expire_one;\n\tchannel_idx = efx->rps_expire_channel;\n\tindex = efx->rps_expire_index;\n\tsize = efx->type->max_rx_ip_filters;\n\twhile (quota--) {\n\t\tstruct ef4_channel *channel = ef4_get_channel(efx, channel_idx);\n\t\tflow_id = channel->rps_flow_id[index];\n\n\t\tif (flow_id != RPS_FLOW_ID_INVALID &&\n\t\t    expire_one(efx, flow_id, index)) {\n\t\t\tnetif_info(efx, rx_status, efx->net_dev,\n\t\t\t\t   \"expired filter %d [queue %u flow %u]\\n\",\n\t\t\t\t   index, channel_idx, flow_id);\n\t\t\tchannel->rps_flow_id[index] = RPS_FLOW_ID_INVALID;\n\t\t}\n\t\tif (++index == size) {\n\t\t\tif (++channel_idx == efx->n_channels)\n\t\t\t\tchannel_idx = 0;\n\t\t\tindex = 0;\n\t\t}\n\t}\n\tefx->rps_expire_channel = channel_idx;\n\tefx->rps_expire_index = index;\n\n\tspin_unlock_bh(&efx->filter_lock);\n\treturn true;\n}\n\n#endif  \n\n \nbool ef4_filter_is_mc_recipient(const struct ef4_filter_spec *spec)\n{\n\tif (!(spec->flags & EF4_FILTER_FLAG_RX) ||\n\t    spec->dmaq_id == EF4_FILTER_RX_DMAQ_ID_DROP)\n\t\treturn false;\n\n\tif (spec->match_flags &\n\t    (EF4_FILTER_MATCH_LOC_MAC | EF4_FILTER_MATCH_LOC_MAC_IG) &&\n\t    is_multicast_ether_addr(spec->loc_mac))\n\t\treturn true;\n\n\tif ((spec->match_flags &\n\t     (EF4_FILTER_MATCH_ETHER_TYPE | EF4_FILTER_MATCH_LOC_HOST)) ==\n\t    (EF4_FILTER_MATCH_ETHER_TYPE | EF4_FILTER_MATCH_LOC_HOST)) {\n\t\tif (spec->ether_type == htons(ETH_P_IP) &&\n\t\t    ipv4_is_multicast(spec->loc_host[0]))\n\t\t\treturn true;\n\t\tif (spec->ether_type == htons(ETH_P_IPV6) &&\n\t\t    ((const u8 *)spec->loc_host)[0] == 0xff)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}