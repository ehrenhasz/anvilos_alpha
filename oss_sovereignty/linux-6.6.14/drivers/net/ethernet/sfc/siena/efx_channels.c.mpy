{
  "module_name": "efx_channels.c",
  "hash_id": "14232d9037c80c8029bd6bc49dd1e84120158ec2a7647c8b01701a510a0443e2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/siena/efx_channels.c",
  "human_readable_source": "\n \n\n#include \"net_driver.h\"\n#include <linux/module.h>\n#include <linux/filter.h>\n#include \"efx_channels.h\"\n#include \"efx.h\"\n#include \"efx_common.h\"\n#include \"tx_common.h\"\n#include \"rx_common.h\"\n#include \"nic.h\"\n#include \"sriov.h\"\n#include \"workarounds.h\"\n\n \nunsigned int efx_siena_interrupt_mode = EFX_INT_MODE_MSIX;\n\n \nunsigned int efx_siena_rss_cpus;\n\nstatic unsigned int irq_adapt_low_thresh = 8000;\nmodule_param(irq_adapt_low_thresh, uint, 0644);\nMODULE_PARM_DESC(irq_adapt_low_thresh,\n\t\t \"Threshold score for reducing IRQ moderation\");\n\nstatic unsigned int irq_adapt_high_thresh = 16000;\nmodule_param(irq_adapt_high_thresh, uint, 0644);\nMODULE_PARM_DESC(irq_adapt_high_thresh,\n\t\t \"Threshold score for increasing IRQ moderation\");\n\nstatic const struct efx_channel_type efx_default_channel_type;\n\n \n\nstatic unsigned int count_online_cores(struct efx_nic *efx, bool local_node)\n{\n\tcpumask_var_t filter_mask;\n\tunsigned int count;\n\tint cpu;\n\n\tif (unlikely(!zalloc_cpumask_var(&filter_mask, GFP_KERNEL))) {\n\t\tnetif_warn(efx, probe, efx->net_dev,\n\t\t\t   \"RSS disabled due to allocation failure\\n\");\n\t\treturn 1;\n\t}\n\n\tcpumask_copy(filter_mask, cpu_online_mask);\n\tif (local_node)\n\t\tcpumask_and(filter_mask, filter_mask,\n\t\t\t    cpumask_of_pcibus(efx->pci_dev->bus));\n\n\tcount = 0;\n\tfor_each_cpu(cpu, filter_mask) {\n\t\t++count;\n\t\tcpumask_andnot(filter_mask, filter_mask, topology_sibling_cpumask(cpu));\n\t}\n\n\tfree_cpumask_var(filter_mask);\n\n\treturn count;\n}\n\nstatic unsigned int efx_wanted_parallelism(struct efx_nic *efx)\n{\n\tunsigned int count;\n\n\tif (efx_siena_rss_cpus) {\n\t\tcount = efx_siena_rss_cpus;\n\t} else {\n\t\tcount = count_online_cores(efx, true);\n\n\t\t \n\t\tif (count == 0)\n\t\t\tcount = count_online_cores(efx, false);\n\t}\n\n\tif (count > EFX_MAX_RX_QUEUES) {\n\t\tnetif_cond_dbg(efx, probe, efx->net_dev, !efx_siena_rss_cpus,\n\t\t\t       warn,\n\t\t\t       \"Reducing number of rx queues from %u to %u.\\n\",\n\t\t\t       count, EFX_MAX_RX_QUEUES);\n\t\tcount = EFX_MAX_RX_QUEUES;\n\t}\n\n\t \n#ifdef CONFIG_SFC_SIENA_SRIOV\n\tif (efx->type->sriov_wanted) {\n\t\tif (efx->type->sriov_wanted(efx) && efx_vf_size(efx) > 1 &&\n\t\t    count > efx_vf_size(efx)) {\n\t\t\tnetif_warn(efx, probe, efx->net_dev,\n\t\t\t\t   \"Reducing number of RSS channels from %u to %u for \"\n\t\t\t\t   \"VF support. Increase vf-msix-limit to use more \"\n\t\t\t\t   \"channels on the PF.\\n\",\n\t\t\t\t   count, efx_vf_size(efx));\n\t\t\tcount = efx_vf_size(efx);\n\t\t}\n\t}\n#endif\n\n\treturn count;\n}\n\nstatic int efx_allocate_msix_channels(struct efx_nic *efx,\n\t\t\t\t      unsigned int max_channels,\n\t\t\t\t      unsigned int extra_channels,\n\t\t\t\t      unsigned int parallelism)\n{\n\tunsigned int n_channels = parallelism;\n\tint vec_count;\n\tint tx_per_ev;\n\tint n_xdp_tx;\n\tint n_xdp_ev;\n\n\tif (efx_siena_separate_tx_channels)\n\t\tn_channels *= 2;\n\tn_channels += extra_channels;\n\n\t \n\ttx_per_ev = EFX_MAX_EVQ_SIZE / EFX_TXQ_MAX_ENT(efx);\n\ttx_per_ev = min(tx_per_ev, EFX_MAX_TXQ_PER_CHANNEL);\n\tn_xdp_tx = num_possible_cpus();\n\tn_xdp_ev = DIV_ROUND_UP(n_xdp_tx, tx_per_ev);\n\n\tvec_count = pci_msix_vec_count(efx->pci_dev);\n\tif (vec_count < 0)\n\t\treturn vec_count;\n\n\tmax_channels = min_t(unsigned int, vec_count, max_channels);\n\n\t \n\tif (n_channels >= max_channels) {\n\t\tefx->xdp_txq_queues_mode = EFX_XDP_TX_QUEUES_BORROWED;\n\t\tnetif_warn(efx, drv, efx->net_dev,\n\t\t\t   \"Insufficient resources for %d XDP event queues (%d other channels, max %d)\\n\",\n\t\t\t   n_xdp_ev, n_channels, max_channels);\n\t\tnetif_warn(efx, drv, efx->net_dev,\n\t\t\t   \"XDP_TX and XDP_REDIRECT might decrease device's performance\\n\");\n\t} else if (n_channels + n_xdp_tx > efx->max_vis) {\n\t\tefx->xdp_txq_queues_mode = EFX_XDP_TX_QUEUES_BORROWED;\n\t\tnetif_warn(efx, drv, efx->net_dev,\n\t\t\t   \"Insufficient resources for %d XDP TX queues (%d other channels, max VIs %d)\\n\",\n\t\t\t   n_xdp_tx, n_channels, efx->max_vis);\n\t\tnetif_warn(efx, drv, efx->net_dev,\n\t\t\t   \"XDP_TX and XDP_REDIRECT might decrease device's performance\\n\");\n\t} else if (n_channels + n_xdp_ev > max_channels) {\n\t\tefx->xdp_txq_queues_mode = EFX_XDP_TX_QUEUES_SHARED;\n\t\tnetif_warn(efx, drv, efx->net_dev,\n\t\t\t   \"Insufficient resources for %d XDP event queues (%d other channels, max %d)\\n\",\n\t\t\t   n_xdp_ev, n_channels, max_channels);\n\n\t\tn_xdp_ev = max_channels - n_channels;\n\t\tnetif_warn(efx, drv, efx->net_dev,\n\t\t\t   \"XDP_TX and XDP_REDIRECT will work with reduced performance (%d cpus/tx_queue)\\n\",\n\t\t\t   DIV_ROUND_UP(n_xdp_tx, tx_per_ev * n_xdp_ev));\n\t} else {\n\t\tefx->xdp_txq_queues_mode = EFX_XDP_TX_QUEUES_DEDICATED;\n\t}\n\n\tif (efx->xdp_txq_queues_mode != EFX_XDP_TX_QUEUES_BORROWED) {\n\t\tefx->n_xdp_channels = n_xdp_ev;\n\t\tefx->xdp_tx_per_channel = tx_per_ev;\n\t\tefx->xdp_tx_queue_count = n_xdp_tx;\n\t\tn_channels += n_xdp_ev;\n\t\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t\t  \"Allocating %d TX and %d event queues for XDP\\n\",\n\t\t\t  n_xdp_ev * tx_per_ev, n_xdp_ev);\n\t} else {\n\t\tefx->n_xdp_channels = 0;\n\t\tefx->xdp_tx_per_channel = 0;\n\t\tefx->xdp_tx_queue_count = n_xdp_tx;\n\t}\n\n\tif (vec_count < n_channels) {\n\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t  \"WARNING: Insufficient MSI-X vectors available (%d < %u).\\n\",\n\t\t\t  vec_count, n_channels);\n\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t  \"WARNING: Performance may be reduced.\\n\");\n\t\tn_channels = vec_count;\n\t}\n\n\tn_channels = min(n_channels, max_channels);\n\n\tefx->n_channels = n_channels;\n\n\t \n\tn_channels -= efx->n_xdp_channels;\n\n\tif (efx_siena_separate_tx_channels) {\n\t\tefx->n_tx_channels =\n\t\t\tmin(max(n_channels / 2, 1U),\n\t\t\t    efx->max_tx_channels);\n\t\tefx->tx_channel_offset =\n\t\t\tn_channels - efx->n_tx_channels;\n\t\tefx->n_rx_channels =\n\t\t\tmax(n_channels -\n\t\t\t    efx->n_tx_channels, 1U);\n\t} else {\n\t\tefx->n_tx_channels = min(n_channels, efx->max_tx_channels);\n\t\tefx->tx_channel_offset = 0;\n\t\tefx->n_rx_channels = n_channels;\n\t}\n\n\tefx->n_rx_channels = min(efx->n_rx_channels, parallelism);\n\tefx->n_tx_channels = min(efx->n_tx_channels, parallelism);\n\n\tefx->xdp_channel_offset = n_channels;\n\n\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t  \"Allocating %u RX channels\\n\",\n\t\t  efx->n_rx_channels);\n\n\treturn efx->n_channels;\n}\n\n \nint efx_siena_probe_interrupts(struct efx_nic *efx)\n{\n\tunsigned int extra_channels = 0;\n\tunsigned int rss_spread;\n\tunsigned int i, j;\n\tint rc;\n\n\tfor (i = 0; i < EFX_MAX_EXTRA_CHANNELS; i++)\n\t\tif (efx->extra_channel_type[i])\n\t\t\t++extra_channels;\n\n\tif (efx->interrupt_mode == EFX_INT_MODE_MSIX) {\n\t\tunsigned int parallelism = efx_wanted_parallelism(efx);\n\t\tstruct msix_entry xentries[EFX_MAX_CHANNELS];\n\t\tunsigned int n_channels;\n\n\t\trc = efx_allocate_msix_channels(efx, efx->max_channels,\n\t\t\t\t\t\textra_channels, parallelism);\n\t\tif (rc >= 0) {\n\t\t\tn_channels = rc;\n\t\t\tfor (i = 0; i < n_channels; i++)\n\t\t\t\txentries[i].entry = i;\n\t\t\trc = pci_enable_msix_range(efx->pci_dev, xentries, 1,\n\t\t\t\t\t\t   n_channels);\n\t\t}\n\t\tif (rc < 0) {\n\t\t\t \n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"could not enable MSI-X\\n\");\n\t\t\tif (efx->type->min_interrupt_mode >= EFX_INT_MODE_MSI)\n\t\t\t\tefx->interrupt_mode = EFX_INT_MODE_MSI;\n\t\t\telse\n\t\t\t\treturn rc;\n\t\t} else if (rc < n_channels) {\n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"WARNING: Insufficient MSI-X vectors\"\n\t\t\t\t  \" available (%d < %u).\\n\", rc, n_channels);\n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"WARNING: Performance may be reduced.\\n\");\n\t\t\tn_channels = rc;\n\t\t}\n\n\t\tif (rc > 0) {\n\t\t\tfor (i = 0; i < efx->n_channels; i++)\n\t\t\t\tefx_get_channel(efx, i)->irq =\n\t\t\t\t\txentries[i].vector;\n\t\t}\n\t}\n\n\t \n\tif (efx->interrupt_mode == EFX_INT_MODE_MSI) {\n\t\tefx->n_channels = 1;\n\t\tefx->n_rx_channels = 1;\n\t\tefx->n_tx_channels = 1;\n\t\tefx->tx_channel_offset = 0;\n\t\tefx->n_xdp_channels = 0;\n\t\tefx->xdp_channel_offset = efx->n_channels;\n\t\tefx->xdp_txq_queues_mode = EFX_XDP_TX_QUEUES_BORROWED;\n\t\trc = pci_enable_msi(efx->pci_dev);\n\t\tif (rc == 0) {\n\t\t\tefx_get_channel(efx, 0)->irq = efx->pci_dev->irq;\n\t\t} else {\n\t\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t\t  \"could not enable MSI\\n\");\n\t\t\tif (efx->type->min_interrupt_mode >= EFX_INT_MODE_LEGACY)\n\t\t\t\tefx->interrupt_mode = EFX_INT_MODE_LEGACY;\n\t\t\telse\n\t\t\t\treturn rc;\n\t\t}\n\t}\n\n\t \n\tif (efx->interrupt_mode == EFX_INT_MODE_LEGACY) {\n\t\tefx->n_channels = 1 + (efx_siena_separate_tx_channels ? 1 : 0);\n\t\tefx->n_rx_channels = 1;\n\t\tefx->n_tx_channels = 1;\n\t\tefx->tx_channel_offset = efx_siena_separate_tx_channels ? 1 : 0;\n\t\tefx->n_xdp_channels = 0;\n\t\tefx->xdp_channel_offset = efx->n_channels;\n\t\tefx->xdp_txq_queues_mode = EFX_XDP_TX_QUEUES_BORROWED;\n\t\tefx->legacy_irq = efx->pci_dev->irq;\n\t}\n\n\t \n\tefx->n_extra_tx_channels = 0;\n\tj = efx->xdp_channel_offset;\n\tfor (i = 0; i < EFX_MAX_EXTRA_CHANNELS; i++) {\n\t\tif (!efx->extra_channel_type[i])\n\t\t\tcontinue;\n\t\tif (j <= efx->tx_channel_offset + efx->n_tx_channels) {\n\t\t\tefx->extra_channel_type[i]->handle_no_channel(efx);\n\t\t} else {\n\t\t\t--j;\n\t\t\tefx_get_channel(efx, j)->type =\n\t\t\t\tefx->extra_channel_type[i];\n\t\t\tif (efx_channel_has_tx_queues(efx_get_channel(efx, j)))\n\t\t\t\tefx->n_extra_tx_channels++;\n\t\t}\n\t}\n\n\trss_spread = efx->n_rx_channels;\n\t \n#ifdef CONFIG_SFC_SIENA_SRIOV\n\tif (efx->type->sriov_wanted) {\n\t\tefx->rss_spread = ((rss_spread > 1 ||\n\t\t\t\t    !efx->type->sriov_wanted(efx)) ?\n\t\t\t\t   rss_spread : efx_vf_size(efx));\n\t\treturn 0;\n\t}\n#endif\n\tefx->rss_spread = rss_spread;\n\n\treturn 0;\n}\n\n#if defined(CONFIG_SMP)\nvoid efx_siena_set_interrupt_affinity(struct efx_nic *efx)\n{\n\tconst struct cpumask *numa_mask = cpumask_of_pcibus(efx->pci_dev->bus);\n\tstruct efx_channel *channel;\n\tunsigned int cpu;\n\n\t \n\tif (cpumask_first_and(cpu_online_mask, numa_mask) >= nr_cpu_ids)\n\t\tnuma_mask = cpu_online_mask;\n\n\tcpu = -1;\n\tefx_for_each_channel(channel, efx) {\n\t\tcpu = cpumask_next_and(cpu, cpu_online_mask, numa_mask);\n\t\tif (cpu >= nr_cpu_ids)\n\t\t\tcpu = cpumask_first_and(cpu_online_mask, numa_mask);\n\t\tirq_set_affinity_hint(channel->irq, cpumask_of(cpu));\n\t}\n}\n\nvoid efx_siena_clear_interrupt_affinity(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\n\tefx_for_each_channel(channel, efx)\n\t\tirq_set_affinity_hint(channel->irq, NULL);\n}\n#else\nvoid\nefx_siena_set_interrupt_affinity(struct efx_nic *efx __always_unused)\n{\n}\n\nvoid\nefx_siena_clear_interrupt_affinity(struct efx_nic *efx __always_unused)\n{\n}\n#endif  \n\nvoid efx_siena_remove_interrupts(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\n\t \n\tefx_for_each_channel(channel, efx)\n\t\tchannel->irq = 0;\n\tpci_disable_msi(efx->pci_dev);\n\tpci_disable_msix(efx->pci_dev);\n\n\t \n\tefx->legacy_irq = 0;\n}\n\n \n\n \nstatic int efx_probe_eventq(struct efx_channel *channel)\n{\n\tstruct efx_nic *efx = channel->efx;\n\tunsigned long entries;\n\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"chan %d create event queue\\n\", channel->channel);\n\n\t \n\tentries = roundup_pow_of_two(efx->rxq_entries + efx->txq_entries + 128);\n\tEFX_WARN_ON_PARANOID(entries > EFX_MAX_EVQ_SIZE);\n\tchannel->eventq_mask = max(entries, EFX_MIN_EVQ_SIZE) - 1;\n\n\treturn efx_nic_probe_eventq(channel);\n}\n\n \nstatic int efx_init_eventq(struct efx_channel *channel)\n{\n\tstruct efx_nic *efx = channel->efx;\n\tint rc;\n\n\tEFX_WARN_ON_PARANOID(channel->eventq_init);\n\n\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t  \"chan %d init event queue\\n\", channel->channel);\n\n\trc = efx_nic_init_eventq(channel);\n\tif (rc == 0) {\n\t\tefx->type->push_irq_moderation(channel);\n\t\tchannel->eventq_read_ptr = 0;\n\t\tchannel->eventq_init = true;\n\t}\n\treturn rc;\n}\n\n \nvoid efx_siena_start_eventq(struct efx_channel *channel)\n{\n\tnetif_dbg(channel->efx, ifup, channel->efx->net_dev,\n\t\t  \"chan %d start event queue\\n\", channel->channel);\n\n\t \n\tchannel->enabled = true;\n\tsmp_wmb();\n\n\tnapi_enable(&channel->napi_str);\n\tefx_nic_eventq_read_ack(channel);\n}\n\n \nvoid efx_siena_stop_eventq(struct efx_channel *channel)\n{\n\tif (!channel->enabled)\n\t\treturn;\n\n\tnapi_disable(&channel->napi_str);\n\tchannel->enabled = false;\n}\n\nstatic void efx_fini_eventq(struct efx_channel *channel)\n{\n\tif (!channel->eventq_init)\n\t\treturn;\n\n\tnetif_dbg(channel->efx, drv, channel->efx->net_dev,\n\t\t  \"chan %d fini event queue\\n\", channel->channel);\n\n\tefx_nic_fini_eventq(channel);\n\tchannel->eventq_init = false;\n}\n\nstatic void efx_remove_eventq(struct efx_channel *channel)\n{\n\tnetif_dbg(channel->efx, drv, channel->efx->net_dev,\n\t\t  \"chan %d remove event queue\\n\", channel->channel);\n\n\tefx_nic_remove_eventq(channel);\n}\n\n \n\n#ifdef CONFIG_RFS_ACCEL\nstatic void efx_filter_rfs_expire(struct work_struct *data)\n{\n\tstruct delayed_work *dwork = to_delayed_work(data);\n\tstruct efx_channel *channel;\n\tunsigned int time, quota;\n\n\tchannel = container_of(dwork, struct efx_channel, filter_work);\n\ttime = jiffies - channel->rfs_last_expiry;\n\tquota = channel->rfs_filter_count * time / (30 * HZ);\n\tif (quota >= 20 && __efx_siena_filter_rfs_expire(channel,\n\t\t\t\t\tmin(channel->rfs_filter_count, quota)))\n\t\tchannel->rfs_last_expiry += time;\n\t \n\tschedule_delayed_work(dwork, 30 * HZ);\n}\n#endif\n\n \nstatic struct efx_channel *efx_alloc_channel(struct efx_nic *efx, int i)\n{\n\tstruct efx_rx_queue *rx_queue;\n\tstruct efx_tx_queue *tx_queue;\n\tstruct efx_channel *channel;\n\tint j;\n\n\tchannel = kzalloc(sizeof(*channel), GFP_KERNEL);\n\tif (!channel)\n\t\treturn NULL;\n\n\tchannel->efx = efx;\n\tchannel->channel = i;\n\tchannel->type = &efx_default_channel_type;\n\n\tfor (j = 0; j < EFX_MAX_TXQ_PER_CHANNEL; j++) {\n\t\ttx_queue = &channel->tx_queue[j];\n\t\ttx_queue->efx = efx;\n\t\ttx_queue->queue = -1;\n\t\ttx_queue->label = j;\n\t\ttx_queue->channel = channel;\n\t}\n\n#ifdef CONFIG_RFS_ACCEL\n\tINIT_DELAYED_WORK(&channel->filter_work, efx_filter_rfs_expire);\n#endif\n\n\trx_queue = &channel->rx_queue;\n\trx_queue->efx = efx;\n\ttimer_setup(&rx_queue->slow_fill, efx_siena_rx_slow_fill, 0);\n\n\treturn channel;\n}\n\nint efx_siena_init_channels(struct efx_nic *efx)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < EFX_MAX_CHANNELS; i++) {\n\t\tefx->channel[i] = efx_alloc_channel(efx, i);\n\t\tif (!efx->channel[i])\n\t\t\treturn -ENOMEM;\n\t\tefx->msi_context[i].efx = efx;\n\t\tefx->msi_context[i].index = i;\n\t}\n\n\t \n\tefx->interrupt_mode = min(efx->type->min_interrupt_mode,\n\t\t\t\t  efx_siena_interrupt_mode);\n\n\tefx->max_channels = EFX_MAX_CHANNELS;\n\tefx->max_tx_channels = EFX_MAX_CHANNELS;\n\n\treturn 0;\n}\n\nvoid efx_siena_fini_channels(struct efx_nic *efx)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < EFX_MAX_CHANNELS; i++)\n\t\tif (efx->channel[i]) {\n\t\t\tkfree(efx->channel[i]);\n\t\t\tefx->channel[i] = NULL;\n\t\t}\n}\n\n \nstatic\nstruct efx_channel *efx_copy_channel(const struct efx_channel *old_channel)\n{\n\tstruct efx_rx_queue *rx_queue;\n\tstruct efx_tx_queue *tx_queue;\n\tstruct efx_channel *channel;\n\tint j;\n\n\tchannel = kmalloc(sizeof(*channel), GFP_KERNEL);\n\tif (!channel)\n\t\treturn NULL;\n\n\t*channel = *old_channel;\n\n\tchannel->napi_dev = NULL;\n\tINIT_HLIST_NODE(&channel->napi_str.napi_hash_node);\n\tchannel->napi_str.napi_id = 0;\n\tchannel->napi_str.state = 0;\n\tmemset(&channel->eventq, 0, sizeof(channel->eventq));\n\n\tfor (j = 0; j < EFX_MAX_TXQ_PER_CHANNEL; j++) {\n\t\ttx_queue = &channel->tx_queue[j];\n\t\tif (tx_queue->channel)\n\t\t\ttx_queue->channel = channel;\n\t\ttx_queue->buffer = NULL;\n\t\ttx_queue->cb_page = NULL;\n\t\tmemset(&tx_queue->txd, 0, sizeof(tx_queue->txd));\n\t}\n\n\trx_queue = &channel->rx_queue;\n\trx_queue->buffer = NULL;\n\tmemset(&rx_queue->rxd, 0, sizeof(rx_queue->rxd));\n\ttimer_setup(&rx_queue->slow_fill, efx_siena_rx_slow_fill, 0);\n#ifdef CONFIG_RFS_ACCEL\n\tINIT_DELAYED_WORK(&channel->filter_work, efx_filter_rfs_expire);\n#endif\n\n\treturn channel;\n}\n\nstatic int efx_probe_channel(struct efx_channel *channel)\n{\n\tstruct efx_tx_queue *tx_queue;\n\tstruct efx_rx_queue *rx_queue;\n\tint rc;\n\n\tnetif_dbg(channel->efx, probe, channel->efx->net_dev,\n\t\t  \"creating channel %d\\n\", channel->channel);\n\n\trc = channel->type->pre_probe(channel);\n\tif (rc)\n\t\tgoto fail;\n\n\trc = efx_probe_eventq(channel);\n\tif (rc)\n\t\tgoto fail;\n\n\tefx_for_each_channel_tx_queue(tx_queue, channel) {\n\t\trc = efx_siena_probe_tx_queue(tx_queue);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\tefx_for_each_channel_rx_queue(rx_queue, channel) {\n\t\trc = efx_siena_probe_rx_queue(rx_queue);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\tchannel->rx_list = NULL;\n\n\treturn 0;\n\nfail:\n\tefx_siena_remove_channel(channel);\n\treturn rc;\n}\n\nstatic void efx_get_channel_name(struct efx_channel *channel, char *buf,\n\t\t\t\t size_t len)\n{\n\tstruct efx_nic *efx = channel->efx;\n\tconst char *type;\n\tint number;\n\n\tnumber = channel->channel;\n\n\tif (number >= efx->xdp_channel_offset &&\n\t    !WARN_ON_ONCE(!efx->n_xdp_channels)) {\n\t\ttype = \"-xdp\";\n\t\tnumber -= efx->xdp_channel_offset;\n\t} else if (efx->tx_channel_offset == 0) {\n\t\ttype = \"\";\n\t} else if (number < efx->tx_channel_offset) {\n\t\ttype = \"-rx\";\n\t} else {\n\t\ttype = \"-tx\";\n\t\tnumber -= efx->tx_channel_offset;\n\t}\n\tsnprintf(buf, len, \"%s%s-%d\", efx->name, type, number);\n}\n\nvoid efx_siena_set_channel_names(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\n\tefx_for_each_channel(channel, efx)\n\t\tchannel->type->get_name(channel,\n\t\t\t\t\tefx->msi_context[channel->channel].name,\n\t\t\t\t\tsizeof(efx->msi_context[0].name));\n}\n\nint efx_siena_probe_channels(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\tint rc;\n\n\t \n\tefx->next_buffer_table = 0;\n\n\t \n\tefx_for_each_channel_rev(channel, efx) {\n\t\trc = efx_probe_channel(channel);\n\t\tif (rc) {\n\t\t\tnetif_err(efx, probe, efx->net_dev,\n\t\t\t\t  \"failed to create channel %d\\n\",\n\t\t\t\t  channel->channel);\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tefx_siena_set_channel_names(efx);\n\n\treturn 0;\n\nfail:\n\tefx_siena_remove_channels(efx);\n\treturn rc;\n}\n\nvoid efx_siena_remove_channel(struct efx_channel *channel)\n{\n\tstruct efx_tx_queue *tx_queue;\n\tstruct efx_rx_queue *rx_queue;\n\n\tnetif_dbg(channel->efx, drv, channel->efx->net_dev,\n\t\t  \"destroy chan %d\\n\", channel->channel);\n\n\tefx_for_each_channel_rx_queue(rx_queue, channel)\n\t\tefx_siena_remove_rx_queue(rx_queue);\n\tefx_for_each_channel_tx_queue(tx_queue, channel)\n\t\tefx_siena_remove_tx_queue(tx_queue);\n\tefx_remove_eventq(channel);\n\tchannel->type->post_remove(channel);\n}\n\nvoid efx_siena_remove_channels(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\n\tefx_for_each_channel(channel, efx)\n\t\tefx_siena_remove_channel(channel);\n\n\tkfree(efx->xdp_tx_queues);\n}\n\nstatic int efx_set_xdp_tx_queue(struct efx_nic *efx, int xdp_queue_number,\n\t\t\t\tstruct efx_tx_queue *tx_queue)\n{\n\tif (xdp_queue_number >= efx->xdp_tx_queue_count)\n\t\treturn -EINVAL;\n\n\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t  \"Channel %u TXQ %u is XDP %u, HW %u\\n\",\n\t\t  tx_queue->channel->channel, tx_queue->label,\n\t\t  xdp_queue_number, tx_queue->queue);\n\tefx->xdp_tx_queues[xdp_queue_number] = tx_queue;\n\treturn 0;\n}\n\nstatic void efx_set_xdp_channels(struct efx_nic *efx)\n{\n\tstruct efx_tx_queue *tx_queue;\n\tstruct efx_channel *channel;\n\tunsigned int next_queue = 0;\n\tint xdp_queue_number = 0;\n\tint rc;\n\n\t \n\tefx_for_each_channel(channel, efx) {\n\t\tif (channel->channel < efx->tx_channel_offset)\n\t\t\tcontinue;\n\n\t\tif (efx_channel_is_xdp_tx(channel)) {\n\t\t\tefx_for_each_channel_tx_queue(tx_queue, channel) {\n\t\t\t\ttx_queue->queue = next_queue++;\n\t\t\t\trc = efx_set_xdp_tx_queue(efx, xdp_queue_number,\n\t\t\t\t\t\t\t  tx_queue);\n\t\t\t\tif (rc == 0)\n\t\t\t\t\txdp_queue_number++;\n\t\t\t}\n\t\t} else {\n\t\t\tefx_for_each_channel_tx_queue(tx_queue, channel) {\n\t\t\t\ttx_queue->queue = next_queue++;\n\t\t\t\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t\t\t\t  \"Channel %u TXQ %u is HW %u\\n\",\n\t\t\t\t\t  channel->channel, tx_queue->label,\n\t\t\t\t\t  tx_queue->queue);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (efx->xdp_txq_queues_mode ==\n\t\t\t    EFX_XDP_TX_QUEUES_BORROWED) {\n\t\t\t\ttx_queue = &channel->tx_queue[0];\n\t\t\t\trc = efx_set_xdp_tx_queue(efx, xdp_queue_number,\n\t\t\t\t\t\t\t  tx_queue);\n\t\t\t\tif (rc == 0)\n\t\t\t\t\txdp_queue_number++;\n\t\t\t}\n\t\t}\n\t}\n\tWARN_ON(efx->xdp_txq_queues_mode == EFX_XDP_TX_QUEUES_DEDICATED &&\n\t\txdp_queue_number != efx->xdp_tx_queue_count);\n\tWARN_ON(efx->xdp_txq_queues_mode != EFX_XDP_TX_QUEUES_DEDICATED &&\n\t\txdp_queue_number > efx->xdp_tx_queue_count);\n\n\t \n\tnext_queue = 0;\n\twhile (xdp_queue_number < efx->xdp_tx_queue_count) {\n\t\ttx_queue = efx->xdp_tx_queues[next_queue++];\n\t\trc = efx_set_xdp_tx_queue(efx, xdp_queue_number, tx_queue);\n\t\tif (rc == 0)\n\t\t\txdp_queue_number++;\n\t}\n}\n\nstatic int efx_soft_enable_interrupts(struct efx_nic *efx);\nstatic void efx_soft_disable_interrupts(struct efx_nic *efx);\nstatic void efx_init_napi_channel(struct efx_channel *channel);\nstatic void efx_fini_napi_channel(struct efx_channel *channel);\n\nint efx_siena_realloc_channels(struct efx_nic *efx, u32 rxq_entries,\n\t\t\t       u32 txq_entries)\n{\n\tstruct efx_channel *other_channel[EFX_MAX_CHANNELS], *channel;\n\tunsigned int i, next_buffer_table = 0;\n\tu32 old_rxq_entries, old_txq_entries;\n\tint rc, rc2;\n\n\trc = efx_check_disabled(efx);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tefx_for_each_channel(channel, efx) {\n\t\tstruct efx_rx_queue *rx_queue;\n\t\tstruct efx_tx_queue *tx_queue;\n\n\t\tif (channel->type->copy)\n\t\t\tcontinue;\n\t\tnext_buffer_table = max(next_buffer_table,\n\t\t\t\t\tchannel->eventq.index +\n\t\t\t\t\tchannel->eventq.entries);\n\t\tefx_for_each_channel_rx_queue(rx_queue, channel)\n\t\t\tnext_buffer_table = max(next_buffer_table,\n\t\t\t\t\t\trx_queue->rxd.index +\n\t\t\t\t\t\trx_queue->rxd.entries);\n\t\tefx_for_each_channel_tx_queue(tx_queue, channel)\n\t\t\tnext_buffer_table = max(next_buffer_table,\n\t\t\t\t\t\ttx_queue->txd.index +\n\t\t\t\t\t\ttx_queue->txd.entries);\n\t}\n\n\tefx_device_detach_sync(efx);\n\tefx_siena_stop_all(efx);\n\tefx_soft_disable_interrupts(efx);\n\n\t \n\tmemset(other_channel, 0, sizeof(other_channel));\n\tfor (i = 0; i < efx->n_channels; i++) {\n\t\tchannel = efx->channel[i];\n\t\tif (channel->type->copy)\n\t\t\tchannel = channel->type->copy(channel);\n\t\tif (!channel) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tother_channel[i] = channel;\n\t}\n\n\t \n\told_rxq_entries = efx->rxq_entries;\n\told_txq_entries = efx->txq_entries;\n\tefx->rxq_entries = rxq_entries;\n\tefx->txq_entries = txq_entries;\n\tfor (i = 0; i < efx->n_channels; i++)\n\t\tswap(efx->channel[i], other_channel[i]);\n\n\t \n\tefx->next_buffer_table = next_buffer_table;\n\n\tfor (i = 0; i < efx->n_channels; i++) {\n\t\tchannel = efx->channel[i];\n\t\tif (!channel->type->copy)\n\t\t\tcontinue;\n\t\trc = efx_probe_channel(channel);\n\t\tif (rc)\n\t\t\tgoto rollback;\n\t\tefx_init_napi_channel(efx->channel[i]);\n\t}\n\n\tefx_set_xdp_channels(efx);\nout:\n\t \n\tfor (i = 0; i < efx->n_channels; i++) {\n\t\tchannel = other_channel[i];\n\t\tif (channel && channel->type->copy) {\n\t\t\tefx_fini_napi_channel(channel);\n\t\t\tefx_siena_remove_channel(channel);\n\t\t\tkfree(channel);\n\t\t}\n\t}\n\n\trc2 = efx_soft_enable_interrupts(efx);\n\tif (rc2) {\n\t\trc = rc ? rc : rc2;\n\t\tnetif_err(efx, drv, efx->net_dev,\n\t\t\t  \"unable to restart interrupts on channel reallocation\\n\");\n\t\tefx_siena_schedule_reset(efx, RESET_TYPE_DISABLE);\n\t} else {\n\t\tefx_siena_start_all(efx);\n\t\tefx_device_attach_if_not_resetting(efx);\n\t}\n\treturn rc;\n\nrollback:\n\t \n\tefx->rxq_entries = old_rxq_entries;\n\tefx->txq_entries = old_txq_entries;\n\tfor (i = 0; i < efx->n_channels; i++)\n\t\tswap(efx->channel[i], other_channel[i]);\n\tgoto out;\n}\n\nint efx_siena_set_channels(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\tint rc;\n\n\tif (efx->xdp_tx_queue_count) {\n\t\tEFX_WARN_ON_PARANOID(efx->xdp_tx_queues);\n\n\t\t \n\t\tefx->xdp_tx_queues = kcalloc(efx->xdp_tx_queue_count,\n\t\t\t\t\t     sizeof(*efx->xdp_tx_queues),\n\t\t\t\t\t     GFP_KERNEL);\n\t\tif (!efx->xdp_tx_queues)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tefx_for_each_channel(channel, efx) {\n\t\tif (channel->channel < efx->n_rx_channels)\n\t\t\tchannel->rx_queue.core_index = channel->channel;\n\t\telse\n\t\t\tchannel->rx_queue.core_index = -1;\n\t}\n\n\tefx_set_xdp_channels(efx);\n\n\trc = netif_set_real_num_tx_queues(efx->net_dev, efx->n_tx_channels);\n\tif (rc)\n\t\treturn rc;\n\treturn netif_set_real_num_rx_queues(efx->net_dev, efx->n_rx_channels);\n}\n\nstatic bool efx_default_channel_want_txqs(struct efx_channel *channel)\n{\n\treturn channel->channel - channel->efx->tx_channel_offset <\n\t\tchannel->efx->n_tx_channels;\n}\n\n \n\nstatic int efx_soft_enable_interrupts(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel, *end_channel;\n\tint rc;\n\n\tBUG_ON(efx->state == STATE_DISABLED);\n\n\tefx->irq_soft_enabled = true;\n\tsmp_wmb();\n\n\tefx_for_each_channel(channel, efx) {\n\t\tif (!channel->type->keep_eventq) {\n\t\t\trc = efx_init_eventq(channel);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t}\n\t\tefx_siena_start_eventq(channel);\n\t}\n\n\tefx_siena_mcdi_mode_event(efx);\n\n\treturn 0;\nfail:\n\tend_channel = channel;\n\tefx_for_each_channel(channel, efx) {\n\t\tif (channel == end_channel)\n\t\t\tbreak;\n\t\tefx_siena_stop_eventq(channel);\n\t\tif (!channel->type->keep_eventq)\n\t\t\tefx_fini_eventq(channel);\n\t}\n\n\treturn rc;\n}\n\nstatic void efx_soft_disable_interrupts(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\n\tif (efx->state == STATE_DISABLED)\n\t\treturn;\n\n\tefx_siena_mcdi_mode_poll(efx);\n\n\tefx->irq_soft_enabled = false;\n\tsmp_wmb();\n\n\tif (efx->legacy_irq)\n\t\tsynchronize_irq(efx->legacy_irq);\n\n\tefx_for_each_channel(channel, efx) {\n\t\tif (channel->irq)\n\t\t\tsynchronize_irq(channel->irq);\n\n\t\tefx_siena_stop_eventq(channel);\n\t\tif (!channel->type->keep_eventq)\n\t\t\tefx_fini_eventq(channel);\n\t}\n\n\t \n\tefx_siena_mcdi_flush_async(efx);\n}\n\nint efx_siena_enable_interrupts(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel, *end_channel;\n\tint rc;\n\n\t \n\tBUG_ON(efx->state == STATE_DISABLED);\n\n\tif (efx->eeh_disabled_legacy_irq) {\n\t\tenable_irq(efx->legacy_irq);\n\t\tefx->eeh_disabled_legacy_irq = false;\n\t}\n\n\tefx->type->irq_enable_master(efx);\n\n\tefx_for_each_channel(channel, efx) {\n\t\tif (channel->type->keep_eventq) {\n\t\t\trc = efx_init_eventq(channel);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\n\trc = efx_soft_enable_interrupts(efx);\n\tif (rc)\n\t\tgoto fail;\n\n\treturn 0;\n\nfail:\n\tend_channel = channel;\n\tefx_for_each_channel(channel, efx) {\n\t\tif (channel == end_channel)\n\t\t\tbreak;\n\t\tif (channel->type->keep_eventq)\n\t\t\tefx_fini_eventq(channel);\n\t}\n\n\tefx->type->irq_disable_non_ev(efx);\n\n\treturn rc;\n}\n\nvoid efx_siena_disable_interrupts(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\n\tefx_soft_disable_interrupts(efx);\n\n\tefx_for_each_channel(channel, efx) {\n\t\tif (channel->type->keep_eventq)\n\t\t\tefx_fini_eventq(channel);\n\t}\n\n\tefx->type->irq_disable_non_ev(efx);\n}\n\nvoid efx_siena_start_channels(struct efx_nic *efx)\n{\n\tstruct efx_tx_queue *tx_queue;\n\tstruct efx_rx_queue *rx_queue;\n\tstruct efx_channel *channel;\n\n\tefx_for_each_channel_rev(channel, efx) {\n\t\tefx_for_each_channel_tx_queue(tx_queue, channel) {\n\t\t\tefx_siena_init_tx_queue(tx_queue);\n\t\t\tatomic_inc(&efx->active_queues);\n\t\t}\n\n\t\tefx_for_each_channel_rx_queue(rx_queue, channel) {\n\t\t\tefx_siena_init_rx_queue(rx_queue);\n\t\t\tatomic_inc(&efx->active_queues);\n\t\t\tefx_siena_stop_eventq(channel);\n\t\t\tefx_siena_fast_push_rx_descriptors(rx_queue, false);\n\t\t\tefx_siena_start_eventq(channel);\n\t\t}\n\n\t\tWARN_ON(channel->rx_pkt_n_frags);\n\t}\n}\n\nvoid efx_siena_stop_channels(struct efx_nic *efx)\n{\n\tstruct efx_tx_queue *tx_queue;\n\tstruct efx_rx_queue *rx_queue;\n\tstruct efx_channel *channel;\n\tint rc = 0;\n\n\t \n\tefx_for_each_channel(channel, efx) {\n\t\tefx_for_each_channel_rx_queue(rx_queue, channel)\n\t\t\trx_queue->refill_enabled = false;\n\t}\n\n\tefx_for_each_channel(channel, efx) {\n\t\t \n\t\tif (efx_channel_has_rx_queue(channel)) {\n\t\t\tefx_siena_stop_eventq(channel);\n\t\t\tefx_siena_start_eventq(channel);\n\t\t}\n\t}\n\n\tif (efx->type->fini_dmaq)\n\t\trc = efx->type->fini_dmaq(efx);\n\n\tif (rc) {\n\t\tnetif_err(efx, drv, efx->net_dev, \"failed to flush queues\\n\");\n\t} else {\n\t\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t\t  \"successfully flushed all queues\\n\");\n\t}\n\n\tefx_for_each_channel(channel, efx) {\n\t\tefx_for_each_channel_rx_queue(rx_queue, channel)\n\t\t\tefx_siena_fini_rx_queue(rx_queue);\n\t\tefx_for_each_channel_tx_queue(tx_queue, channel)\n\t\t\tefx_siena_fini_tx_queue(tx_queue);\n\t}\n}\n\n \n\n \nstatic int efx_process_channel(struct efx_channel *channel, int budget)\n{\n\tstruct efx_tx_queue *tx_queue;\n\tstruct list_head rx_list;\n\tint spent;\n\n\tif (unlikely(!channel->enabled))\n\t\treturn 0;\n\n\t \n\tEFX_WARN_ON_PARANOID(channel->rx_list != NULL);\n\tINIT_LIST_HEAD(&rx_list);\n\tchannel->rx_list = &rx_list;\n\n\tefx_for_each_channel_tx_queue(tx_queue, channel) {\n\t\ttx_queue->pkts_compl = 0;\n\t\ttx_queue->bytes_compl = 0;\n\t}\n\n\tspent = efx_nic_process_eventq(channel, budget);\n\tif (spent && efx_channel_has_rx_queue(channel)) {\n\t\tstruct efx_rx_queue *rx_queue =\n\t\t\tefx_channel_get_rx_queue(channel);\n\n\t\tefx_rx_flush_packet(channel);\n\t\tefx_siena_fast_push_rx_descriptors(rx_queue, true);\n\t}\n\n\t \n\tefx_for_each_channel_tx_queue(tx_queue, channel) {\n\t\tif (tx_queue->bytes_compl) {\n\t\t\tnetdev_tx_completed_queue(tx_queue->core_txq,\n\t\t\t\t\t\t  tx_queue->pkts_compl,\n\t\t\t\t\t\t  tx_queue->bytes_compl);\n\t\t}\n\t}\n\n\t \n\tnetif_receive_skb_list(channel->rx_list);\n\tchannel->rx_list = NULL;\n\n\treturn spent;\n}\n\nstatic void efx_update_irq_mod(struct efx_nic *efx, struct efx_channel *channel)\n{\n\tint step = efx->irq_mod_step_us;\n\n\tif (channel->irq_mod_score < irq_adapt_low_thresh) {\n\t\tif (channel->irq_moderation_us > step) {\n\t\t\tchannel->irq_moderation_us -= step;\n\t\t\tefx->type->push_irq_moderation(channel);\n\t\t}\n\t} else if (channel->irq_mod_score > irq_adapt_high_thresh) {\n\t\tif (channel->irq_moderation_us <\n\t\t    efx->irq_rx_moderation_us) {\n\t\t\tchannel->irq_moderation_us += step;\n\t\t\tefx->type->push_irq_moderation(channel);\n\t\t}\n\t}\n\n\tchannel->irq_count = 0;\n\tchannel->irq_mod_score = 0;\n}\n\n \nstatic int efx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct efx_channel *channel =\n\t\tcontainer_of(napi, struct efx_channel, napi_str);\n\tstruct efx_nic *efx = channel->efx;\n#ifdef CONFIG_RFS_ACCEL\n\tunsigned int time;\n#endif\n\tint spent;\n\n\tnetif_vdbg(efx, intr, efx->net_dev,\n\t\t   \"channel %d NAPI poll executing on CPU %d\\n\",\n\t\t   channel->channel, raw_smp_processor_id());\n\n\tspent = efx_process_channel(channel, budget);\n\n\txdp_do_flush_map();\n\n\tif (spent < budget) {\n\t\tif (efx_channel_has_rx_queue(channel) &&\n\t\t    efx->irq_rx_adaptive &&\n\t\t    unlikely(++channel->irq_count == 1000)) {\n\t\t\tefx_update_irq_mod(efx, channel);\n\t\t}\n\n#ifdef CONFIG_RFS_ACCEL\n\t\t \n\t\ttime = jiffies - channel->rfs_last_expiry;\n\t\t \n\t\tif (channel->rfs_filter_count * time >= 600 * HZ)\n\t\t\tmod_delayed_work(system_wq, &channel->filter_work, 0);\n#endif\n\n\t\t \n\t\tif (napi_complete_done(napi, spent))\n\t\t\tefx_nic_eventq_read_ack(channel);\n\t}\n\n\treturn spent;\n}\n\nstatic void efx_init_napi_channel(struct efx_channel *channel)\n{\n\tstruct efx_nic *efx = channel->efx;\n\n\tchannel->napi_dev = efx->net_dev;\n\tnetif_napi_add(channel->napi_dev, &channel->napi_str, efx_poll);\n}\n\nvoid efx_siena_init_napi(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\n\tefx_for_each_channel(channel, efx)\n\t\tefx_init_napi_channel(channel);\n}\n\nstatic void efx_fini_napi_channel(struct efx_channel *channel)\n{\n\tif (channel->napi_dev)\n\t\tnetif_napi_del(&channel->napi_str);\n\n\tchannel->napi_dev = NULL;\n}\n\nvoid efx_siena_fini_napi(struct efx_nic *efx)\n{\n\tstruct efx_channel *channel;\n\n\tefx_for_each_channel(channel, efx)\n\t\tefx_fini_napi_channel(channel);\n}\n\n \n\nstatic int efx_channel_dummy_op_int(struct efx_channel *channel)\n{\n\treturn 0;\n}\n\nvoid efx_siena_channel_dummy_op_void(struct efx_channel *channel)\n{\n}\n\nstatic const struct efx_channel_type efx_default_channel_type = {\n\t.pre_probe\t\t= efx_channel_dummy_op_int,\n\t.post_remove\t\t= efx_siena_channel_dummy_op_void,\n\t.get_name\t\t= efx_get_channel_name,\n\t.copy\t\t\t= efx_copy_channel,\n\t.want_txqs\t\t= efx_default_channel_want_txqs,\n\t.keep_eventq\t\t= false,\n\t.want_pio\t\t= true,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}