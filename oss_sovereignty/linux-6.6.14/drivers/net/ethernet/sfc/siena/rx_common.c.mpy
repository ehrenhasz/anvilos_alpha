{
  "module_name": "rx_common.c",
  "hash_id": "c0887d6a075f775f8fd81497fe8b42945c91be6c70053abbff083121c591c295",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/siena/rx_common.c",
  "human_readable_source": "\n \n\n#include \"net_driver.h\"\n#include <linux/module.h>\n#include <linux/iommu.h>\n#include \"efx.h\"\n#include \"nic.h\"\n#include \"rx_common.h\"\n\n \nstatic unsigned int rx_refill_threshold;\nmodule_param(rx_refill_threshold, uint, 0444);\nMODULE_PARM_DESC(rx_refill_threshold,\n\t\t \"RX descriptor ring refill threshold (%)\");\n\n \n#define EFX_RXD_HEAD_ROOM (1 + EFX_RX_MAX_FRAGS)\n\nstatic void efx_unmap_rx_buffer(struct efx_nic *efx,\n\t\t\t\tstruct efx_rx_buffer *rx_buf);\n\n \nstatic struct page *efx_reuse_page(struct efx_rx_queue *rx_queue)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tstruct efx_rx_page_state *state;\n\tunsigned int index;\n\tstruct page *page;\n\n\tif (unlikely(!rx_queue->page_ring))\n\t\treturn NULL;\n\tindex = rx_queue->page_remove & rx_queue->page_ptr_mask;\n\tpage = rx_queue->page_ring[index];\n\tif (page == NULL)\n\t\treturn NULL;\n\n\trx_queue->page_ring[index] = NULL;\n\t \n\tif (rx_queue->page_remove != rx_queue->page_add)\n\t\t++rx_queue->page_remove;\n\n\t \n\tif (page_count(page) == 1) {\n\t\t++rx_queue->page_recycle_count;\n\t\treturn page;\n\t} else {\n\t\tstate = page_address(page);\n\t\tdma_unmap_page(&efx->pci_dev->dev, state->dma_addr,\n\t\t\t       PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(page);\n\t\t++rx_queue->page_recycle_failed;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void efx_recycle_rx_page(struct efx_channel *channel,\n\t\t\t\tstruct efx_rx_buffer *rx_buf)\n{\n\tstruct efx_rx_queue *rx_queue = efx_channel_get_rx_queue(channel);\n\tstruct efx_nic *efx = rx_queue->efx;\n\tstruct page *page = rx_buf->page;\n\tunsigned int index;\n\n\t \n\tif (!(rx_buf->flags & EFX_RX_BUF_LAST_IN_PAGE))\n\t\treturn;\n\n\tindex = rx_queue->page_add & rx_queue->page_ptr_mask;\n\tif (rx_queue->page_ring[index] == NULL) {\n\t\tunsigned int read_index = rx_queue->page_remove &\n\t\t\trx_queue->page_ptr_mask;\n\n\t\t \n\t\tif (read_index == index)\n\t\t\t++rx_queue->page_remove;\n\t\trx_queue->page_ring[index] = page;\n\t\t++rx_queue->page_add;\n\t\treturn;\n\t}\n\t++rx_queue->page_recycle_full;\n\tefx_unmap_rx_buffer(efx, rx_buf);\n\tput_page(rx_buf->page);\n}\n\n \nvoid efx_siena_recycle_rx_pages(struct efx_channel *channel,\n\t\t\t\tstruct efx_rx_buffer *rx_buf,\n\t\t\t\tunsigned int n_frags)\n{\n\tstruct efx_rx_queue *rx_queue = efx_channel_get_rx_queue(channel);\n\n\tif (unlikely(!rx_queue->page_ring))\n\t\treturn;\n\n\tdo {\n\t\tefx_recycle_rx_page(channel, rx_buf);\n\t\trx_buf = efx_rx_buf_next(rx_queue, rx_buf);\n\t} while (--n_frags);\n}\n\nvoid efx_siena_discard_rx_packet(struct efx_channel *channel,\n\t\t\t\t struct efx_rx_buffer *rx_buf,\n\t\t\t\t unsigned int n_frags)\n{\n\tstruct efx_rx_queue *rx_queue = efx_channel_get_rx_queue(channel);\n\n\tefx_siena_recycle_rx_pages(channel, rx_buf, n_frags);\n\n\tefx_siena_free_rx_buffers(rx_queue, rx_buf, n_frags);\n}\n\nstatic void efx_init_rx_recycle_ring(struct efx_rx_queue *rx_queue)\n{\n\tunsigned int bufs_in_recycle_ring, page_ring_size;\n\tstruct efx_nic *efx = rx_queue->efx;\n\n\tbufs_in_recycle_ring = efx_rx_recycle_ring_size(efx);\n\tpage_ring_size = roundup_pow_of_two(bufs_in_recycle_ring /\n\t\t\t\t\t    efx->rx_bufs_per_page);\n\trx_queue->page_ring = kcalloc(page_ring_size,\n\t\t\t\t      sizeof(*rx_queue->page_ring), GFP_KERNEL);\n\tif (!rx_queue->page_ring)\n\t\trx_queue->page_ptr_mask = 0;\n\telse\n\t\trx_queue->page_ptr_mask = page_ring_size - 1;\n}\n\nstatic void efx_fini_rx_recycle_ring(struct efx_rx_queue *rx_queue)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tint i;\n\n\tif (unlikely(!rx_queue->page_ring))\n\t\treturn;\n\n\t \n\tfor (i = 0; i <= rx_queue->page_ptr_mask; i++) {\n\t\tstruct page *page = rx_queue->page_ring[i];\n\t\tstruct efx_rx_page_state *state;\n\n\t\tif (page == NULL)\n\t\t\tcontinue;\n\n\t\tstate = page_address(page);\n\t\tdma_unmap_page(&efx->pci_dev->dev, state->dma_addr,\n\t\t\t       PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t       DMA_FROM_DEVICE);\n\t\tput_page(page);\n\t}\n\tkfree(rx_queue->page_ring);\n\trx_queue->page_ring = NULL;\n}\n\nstatic void efx_fini_rx_buffer(struct efx_rx_queue *rx_queue,\n\t\t\t       struct efx_rx_buffer *rx_buf)\n{\n\t \n\tif (rx_buf->page)\n\t\tput_page(rx_buf->page);\n\n\t \n\tif (rx_buf->flags & EFX_RX_BUF_LAST_IN_PAGE) {\n\t\tefx_unmap_rx_buffer(rx_queue->efx, rx_buf);\n\t\tefx_siena_free_rx_buffers(rx_queue, rx_buf, 1);\n\t}\n\trx_buf->page = NULL;\n}\n\nint efx_siena_probe_rx_queue(struct efx_rx_queue *rx_queue)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tunsigned int entries;\n\tint rc;\n\n\t \n\tentries = max(roundup_pow_of_two(efx->rxq_entries), EFX_MIN_DMAQ_SIZE);\n\tEFX_WARN_ON_PARANOID(entries > EFX_MAX_DMAQ_SIZE);\n\trx_queue->ptr_mask = entries - 1;\n\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"creating RX queue %d size %#x mask %#x\\n\",\n\t\t  efx_rx_queue_index(rx_queue), efx->rxq_entries,\n\t\t  rx_queue->ptr_mask);\n\n\t \n\trx_queue->buffer = kcalloc(entries, sizeof(*rx_queue->buffer),\n\t\t\t\t   GFP_KERNEL);\n\tif (!rx_queue->buffer)\n\t\treturn -ENOMEM;\n\n\trc = efx_nic_probe_rx(rx_queue);\n\tif (rc) {\n\t\tkfree(rx_queue->buffer);\n\t\trx_queue->buffer = NULL;\n\t}\n\n\treturn rc;\n}\n\nvoid efx_siena_init_rx_queue(struct efx_rx_queue *rx_queue)\n{\n\tunsigned int max_fill, trigger, max_trigger;\n\tstruct efx_nic *efx = rx_queue->efx;\n\tint rc = 0;\n\n\tnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\n\t\t  \"initialising RX queue %d\\n\", efx_rx_queue_index(rx_queue));\n\n\t \n\trx_queue->added_count = 0;\n\trx_queue->notified_count = 0;\n\trx_queue->removed_count = 0;\n\trx_queue->min_fill = -1U;\n\tefx_init_rx_recycle_ring(rx_queue);\n\n\trx_queue->page_remove = 0;\n\trx_queue->page_add = rx_queue->page_ptr_mask + 1;\n\trx_queue->page_recycle_count = 0;\n\trx_queue->page_recycle_failed = 0;\n\trx_queue->page_recycle_full = 0;\n\n\t \n\tmax_fill = efx->rxq_entries - EFX_RXD_HEAD_ROOM;\n\tmax_trigger =\n\t\tmax_fill - efx->rx_pages_per_batch * efx->rx_bufs_per_page;\n\tif (rx_refill_threshold != 0) {\n\t\ttrigger = max_fill * min(rx_refill_threshold, 100U) / 100U;\n\t\tif (trigger > max_trigger)\n\t\t\ttrigger = max_trigger;\n\t} else {\n\t\ttrigger = max_trigger;\n\t}\n\n\trx_queue->max_fill = max_fill;\n\trx_queue->fast_fill_trigger = trigger;\n\trx_queue->refill_enabled = true;\n\n\t \n\trc = xdp_rxq_info_reg(&rx_queue->xdp_rxq_info, efx->net_dev,\n\t\t\t      rx_queue->core_index, 0);\n\n\tif (rc) {\n\t\tnetif_err(efx, rx_err, efx->net_dev,\n\t\t\t  \"Failure to initialise XDP queue information rc=%d\\n\",\n\t\t\t  rc);\n\t\tefx->xdp_rxq_info_failed = true;\n\t} else {\n\t\trx_queue->xdp_rxq_info_valid = true;\n\t}\n\n\t \n\tefx_nic_init_rx(rx_queue);\n}\n\nvoid efx_siena_fini_rx_queue(struct efx_rx_queue *rx_queue)\n{\n\tstruct efx_rx_buffer *rx_buf;\n\tint i;\n\n\tnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\n\t\t  \"shutting down RX queue %d\\n\", efx_rx_queue_index(rx_queue));\n\n\tdel_timer_sync(&rx_queue->slow_fill);\n\n\t \n\tif (rx_queue->buffer) {\n\t\tfor (i = rx_queue->removed_count; i < rx_queue->added_count;\n\t\t     i++) {\n\t\t\tunsigned int index = i & rx_queue->ptr_mask;\n\n\t\t\trx_buf = efx_rx_buffer(rx_queue, index);\n\t\t\tefx_fini_rx_buffer(rx_queue, rx_buf);\n\t\t}\n\t}\n\n\tefx_fini_rx_recycle_ring(rx_queue);\n\n\tif (rx_queue->xdp_rxq_info_valid)\n\t\txdp_rxq_info_unreg(&rx_queue->xdp_rxq_info);\n\n\trx_queue->xdp_rxq_info_valid = false;\n}\n\nvoid efx_siena_remove_rx_queue(struct efx_rx_queue *rx_queue)\n{\n\tnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\n\t\t  \"destroying RX queue %d\\n\", efx_rx_queue_index(rx_queue));\n\n\tefx_nic_remove_rx(rx_queue);\n\n\tkfree(rx_queue->buffer);\n\trx_queue->buffer = NULL;\n}\n\n \nstatic void efx_unmap_rx_buffer(struct efx_nic *efx,\n\t\t\t\tstruct efx_rx_buffer *rx_buf)\n{\n\tstruct page *page = rx_buf->page;\n\n\tif (page) {\n\t\tstruct efx_rx_page_state *state = page_address(page);\n\n\t\tdma_unmap_page(&efx->pci_dev->dev,\n\t\t\t       state->dma_addr,\n\t\t\t       PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t       DMA_FROM_DEVICE);\n\t}\n}\n\nvoid efx_siena_free_rx_buffers(struct efx_rx_queue *rx_queue,\n\t\t\t       struct efx_rx_buffer *rx_buf,\n\t\t\t       unsigned int num_bufs)\n{\n\tdo {\n\t\tif (rx_buf->page) {\n\t\t\tput_page(rx_buf->page);\n\t\t\trx_buf->page = NULL;\n\t\t}\n\t\trx_buf = efx_rx_buf_next(rx_queue, rx_buf);\n\t} while (--num_bufs);\n}\n\nvoid efx_siena_rx_slow_fill(struct timer_list *t)\n{\n\tstruct efx_rx_queue *rx_queue = from_timer(rx_queue, t, slow_fill);\n\n\t \n\tefx_nic_generate_fill_event(rx_queue);\n\t++rx_queue->slow_fill_count;\n}\n\nstatic void efx_schedule_slow_fill(struct efx_rx_queue *rx_queue)\n{\n\tmod_timer(&rx_queue->slow_fill, jiffies + msecs_to_jiffies(10));\n}\n\n \nstatic int efx_init_rx_buffers(struct efx_rx_queue *rx_queue, bool atomic)\n{\n\tunsigned int page_offset, index, count;\n\tstruct efx_nic *efx = rx_queue->efx;\n\tstruct efx_rx_page_state *state;\n\tstruct efx_rx_buffer *rx_buf;\n\tdma_addr_t dma_addr;\n\tstruct page *page;\n\n\tcount = 0;\n\tdo {\n\t\tpage = efx_reuse_page(rx_queue);\n\t\tif (page == NULL) {\n\t\t\tpage = alloc_pages(__GFP_COMP |\n\t\t\t\t\t   (atomic ? GFP_ATOMIC : GFP_KERNEL),\n\t\t\t\t\t   efx->rx_buffer_order);\n\t\t\tif (unlikely(page == NULL))\n\t\t\t\treturn -ENOMEM;\n\t\t\tdma_addr =\n\t\t\t\tdma_map_page(&efx->pci_dev->dev, page, 0,\n\t\t\t\t\t     PAGE_SIZE << efx->rx_buffer_order,\n\t\t\t\t\t     DMA_FROM_DEVICE);\n\t\t\tif (unlikely(dma_mapping_error(&efx->pci_dev->dev,\n\t\t\t\t\t\t       dma_addr))) {\n\t\t\t\t__free_pages(page, efx->rx_buffer_order);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t\tstate = page_address(page);\n\t\t\tstate->dma_addr = dma_addr;\n\t\t} else {\n\t\t\tstate = page_address(page);\n\t\t\tdma_addr = state->dma_addr;\n\t\t}\n\n\t\tdma_addr += sizeof(struct efx_rx_page_state);\n\t\tpage_offset = sizeof(struct efx_rx_page_state);\n\n\t\tdo {\n\t\t\tindex = rx_queue->added_count & rx_queue->ptr_mask;\n\t\t\trx_buf = efx_rx_buffer(rx_queue, index);\n\t\t\trx_buf->dma_addr = dma_addr + efx->rx_ip_align +\n\t\t\t\t\t   EFX_XDP_HEADROOM;\n\t\t\trx_buf->page = page;\n\t\t\trx_buf->page_offset = page_offset + efx->rx_ip_align +\n\t\t\t\t\t      EFX_XDP_HEADROOM;\n\t\t\trx_buf->len = efx->rx_dma_len;\n\t\t\trx_buf->flags = 0;\n\t\t\t++rx_queue->added_count;\n\t\t\tget_page(page);\n\t\t\tdma_addr += efx->rx_page_buf_step;\n\t\t\tpage_offset += efx->rx_page_buf_step;\n\t\t} while (page_offset + efx->rx_page_buf_step <= PAGE_SIZE);\n\n\t\trx_buf->flags = EFX_RX_BUF_LAST_IN_PAGE;\n\t} while (++count < efx->rx_pages_per_batch);\n\n\treturn 0;\n}\n\nvoid efx_siena_rx_config_page_split(struct efx_nic *efx)\n{\n\tefx->rx_page_buf_step = ALIGN(efx->rx_dma_len + efx->rx_ip_align +\n\t\t\t\t      EFX_XDP_HEADROOM + EFX_XDP_TAILROOM,\n\t\t\t\t      EFX_RX_BUF_ALIGNMENT);\n\tefx->rx_bufs_per_page = efx->rx_buffer_order ? 1 :\n\t\t((PAGE_SIZE - sizeof(struct efx_rx_page_state)) /\n\t\tefx->rx_page_buf_step);\n\tefx->rx_buffer_truesize = (PAGE_SIZE << efx->rx_buffer_order) /\n\t\tefx->rx_bufs_per_page;\n\tefx->rx_pages_per_batch = DIV_ROUND_UP(EFX_RX_PREFERRED_BATCH,\n\t\t\t\t\t       efx->rx_bufs_per_page);\n}\n\n \nvoid efx_siena_fast_push_rx_descriptors(struct efx_rx_queue *rx_queue,\n\t\t\t\t\tbool atomic)\n{\n\tstruct efx_nic *efx = rx_queue->efx;\n\tunsigned int fill_level, batch_size;\n\tint space, rc = 0;\n\n\tif (!rx_queue->refill_enabled)\n\t\treturn;\n\n\t \n\tfill_level = (rx_queue->added_count - rx_queue->removed_count);\n\tEFX_WARN_ON_ONCE_PARANOID(fill_level > rx_queue->efx->rxq_entries);\n\tif (fill_level >= rx_queue->fast_fill_trigger)\n\t\tgoto out;\n\n\t \n\tif (unlikely(fill_level < rx_queue->min_fill)) {\n\t\tif (fill_level)\n\t\t\trx_queue->min_fill = fill_level;\n\t}\n\n\tbatch_size = efx->rx_pages_per_batch * efx->rx_bufs_per_page;\n\tspace = rx_queue->max_fill - fill_level;\n\tEFX_WARN_ON_ONCE_PARANOID(space < batch_size);\n\n\tnetif_vdbg(rx_queue->efx, rx_status, rx_queue->efx->net_dev,\n\t\t   \"RX queue %d fast-filling descriptor ring from\"\n\t\t   \" level %d to level %d\\n\",\n\t\t   efx_rx_queue_index(rx_queue), fill_level,\n\t\t   rx_queue->max_fill);\n\n\tdo {\n\t\trc = efx_init_rx_buffers(rx_queue, atomic);\n\t\tif (unlikely(rc)) {\n\t\t\t \n\t\t\tefx_schedule_slow_fill(rx_queue);\n\t\t\tgoto out;\n\t\t}\n\t} while ((space -= batch_size) >= batch_size);\n\n\tnetif_vdbg(rx_queue->efx, rx_status, rx_queue->efx->net_dev,\n\t\t   \"RX queue %d fast-filled descriptor ring \"\n\t\t   \"to level %d\\n\", efx_rx_queue_index(rx_queue),\n\t\t   rx_queue->added_count - rx_queue->removed_count);\n\n out:\n\tif (rx_queue->notified_count != rx_queue->added_count)\n\t\tefx_nic_notify_rx_desc(rx_queue);\n}\n\n \nvoid\nefx_siena_rx_packet_gro(struct efx_channel *channel,\n\t\t\tstruct efx_rx_buffer *rx_buf,\n\t\t\tunsigned int n_frags, u8 *eh, __wsum csum)\n{\n\tstruct napi_struct *napi = &channel->napi_str;\n\tstruct efx_nic *efx = channel->efx;\n\tstruct sk_buff *skb;\n\n\tskb = napi_get_frags(napi);\n\tif (unlikely(!skb)) {\n\t\tstruct efx_rx_queue *rx_queue;\n\n\t\trx_queue = efx_channel_get_rx_queue(channel);\n\t\tefx_siena_free_rx_buffers(rx_queue, rx_buf, n_frags);\n\t\treturn;\n\t}\n\n\tif (efx->net_dev->features & NETIF_F_RXHASH)\n\t\tskb_set_hash(skb, efx_rx_buf_hash(efx, eh),\n\t\t\t     PKT_HASH_TYPE_L3);\n\tif (csum) {\n\t\tskb->csum = csum;\n\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t} else {\n\t\tskb->ip_summed = ((rx_buf->flags & EFX_RX_PKT_CSUMMED) ?\n\t\t\t\t  CHECKSUM_UNNECESSARY : CHECKSUM_NONE);\n\t}\n\tskb->csum_level = !!(rx_buf->flags & EFX_RX_PKT_CSUM_LEVEL);\n\n\tfor (;;) {\n\t\tskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t   rx_buf->page, rx_buf->page_offset,\n\t\t\t\t   rx_buf->len);\n\t\trx_buf->page = NULL;\n\t\tskb->len += rx_buf->len;\n\t\tif (skb_shinfo(skb)->nr_frags == n_frags)\n\t\t\tbreak;\n\n\t\trx_buf = efx_rx_buf_next(&channel->rx_queue, rx_buf);\n\t}\n\n\tskb->data_len = skb->len;\n\tskb->truesize += n_frags * efx->rx_buffer_truesize;\n\n\tskb_record_rx_queue(skb, channel->rx_queue.core_index);\n\n\tnapi_gro_frags(napi);\n}\n\n \nstruct efx_rss_context *efx_siena_alloc_rss_context_entry(struct efx_nic *efx)\n{\n\tstruct list_head *head = &efx->rss_context.list;\n\tstruct efx_rss_context *ctx, *new;\n\tu32 id = 1;  \n\n\tWARN_ON(!mutex_is_locked(&efx->rss_lock));\n\n\t \n\tlist_for_each_entry(ctx, head, list) {\n\t\tif (ctx->user_id != id)\n\t\t\tbreak;\n\t\tid++;\n\t\t \n\t\tif (WARN_ON_ONCE(!id))\n\t\t\treturn NULL;\n\t}\n\n\t \n\tnew = kmalloc(sizeof(*new), GFP_KERNEL);\n\tif (!new)\n\t\treturn NULL;\n\tnew->context_id = EFX_MCDI_RSS_CONTEXT_INVALID;\n\tnew->rx_hash_udp_4tuple = false;\n\n\t \n\tnew->user_id = id;\n\tlist_add_tail(&new->list, &ctx->list);\n\treturn new;\n}\n\nstruct efx_rss_context *efx_siena_find_rss_context_entry(struct efx_nic *efx,\n\t\t\t\t\t\t\t u32 id)\n{\n\tstruct list_head *head = &efx->rss_context.list;\n\tstruct efx_rss_context *ctx;\n\n\tWARN_ON(!mutex_is_locked(&efx->rss_lock));\n\n\tlist_for_each_entry(ctx, head, list)\n\t\tif (ctx->user_id == id)\n\t\t\treturn ctx;\n\treturn NULL;\n}\n\nvoid efx_siena_free_rss_context_entry(struct efx_rss_context *ctx)\n{\n\tlist_del(&ctx->list);\n\tkfree(ctx);\n}\n\nvoid efx_siena_set_default_rx_indir_table(struct efx_nic *efx,\n\t\t\t\t\t  struct efx_rss_context *ctx)\n{\n\tsize_t i;\n\n\tfor (i = 0; i < ARRAY_SIZE(ctx->rx_indir_table); i++)\n\t\tctx->rx_indir_table[i] =\n\t\t\tethtool_rxfh_indir_default(i, efx->rss_spread);\n}\n\n \nbool efx_siena_filter_is_mc_recipient(const struct efx_filter_spec *spec)\n{\n\tif (!(spec->flags & EFX_FILTER_FLAG_RX) ||\n\t    spec->dmaq_id == EFX_FILTER_RX_DMAQ_ID_DROP)\n\t\treturn false;\n\n\tif (spec->match_flags &\n\t    (EFX_FILTER_MATCH_LOC_MAC | EFX_FILTER_MATCH_LOC_MAC_IG) &&\n\t    is_multicast_ether_addr(spec->loc_mac))\n\t\treturn true;\n\n\tif ((spec->match_flags &\n\t     (EFX_FILTER_MATCH_ETHER_TYPE | EFX_FILTER_MATCH_LOC_HOST)) ==\n\t    (EFX_FILTER_MATCH_ETHER_TYPE | EFX_FILTER_MATCH_LOC_HOST)) {\n\t\tif (spec->ether_type == htons(ETH_P_IP) &&\n\t\t    ipv4_is_multicast(spec->loc_host[0]))\n\t\t\treturn true;\n\t\tif (spec->ether_type == htons(ETH_P_IPV6) &&\n\t\t    ((const u8 *)spec->loc_host)[0] == 0xff)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nbool efx_siena_filter_spec_equal(const struct efx_filter_spec *left,\n\t\t\t\t const struct efx_filter_spec *right)\n{\n\tif ((left->match_flags ^ right->match_flags) |\n\t    ((left->flags ^ right->flags) &\n\t     (EFX_FILTER_FLAG_RX | EFX_FILTER_FLAG_TX)))\n\t\treturn false;\n\n\treturn memcmp(&left->outer_vid, &right->outer_vid,\n\t\t      sizeof(struct efx_filter_spec) -\n\t\t      offsetof(struct efx_filter_spec, outer_vid)) == 0;\n}\n\nu32 efx_siena_filter_spec_hash(const struct efx_filter_spec *spec)\n{\n\tBUILD_BUG_ON(offsetof(struct efx_filter_spec, outer_vid) & 3);\n\treturn jhash2((const u32 *)&spec->outer_vid,\n\t\t      (sizeof(struct efx_filter_spec) -\n\t\t       offsetof(struct efx_filter_spec, outer_vid)) / 4,\n\t\t      0);\n}\n\n#ifdef CONFIG_RFS_ACCEL\nbool efx_siena_rps_check_rule(struct efx_arfs_rule *rule,\n\t\t\t      unsigned int filter_idx, bool *force)\n{\n\tif (rule->filter_id == EFX_ARFS_FILTER_ID_PENDING) {\n\t\t \n\t\treturn false;\n\t}\n\tif (rule->filter_id == EFX_ARFS_FILTER_ID_ERROR) {\n\t\t \n\t\trule->filter_id = EFX_ARFS_FILTER_ID_REMOVING;\n\t\t*force = true;\n\t\treturn true;\n\t} else if (WARN_ON(rule->filter_id != filter_idx)) {  \n\t\t \n\t\t*force = true;\n\t\treturn true;\n\t}\n\t \n\treturn true;\n}\n\nstatic\nstruct hlist_head *efx_rps_hash_bucket(struct efx_nic *efx,\n\t\t\t\t       const struct efx_filter_spec *spec)\n{\n\tu32 hash = efx_siena_filter_spec_hash(spec);\n\n\tlockdep_assert_held(&efx->rps_hash_lock);\n\tif (!efx->rps_hash_table)\n\t\treturn NULL;\n\treturn &efx->rps_hash_table[hash % EFX_ARFS_HASH_TABLE_SIZE];\n}\n\nstruct efx_arfs_rule *efx_siena_rps_hash_find(struct efx_nic *efx,\n\t\t\t\t\tconst struct efx_filter_spec *spec)\n{\n\tstruct efx_arfs_rule *rule;\n\tstruct hlist_head *head;\n\tstruct hlist_node *node;\n\n\thead = efx_rps_hash_bucket(efx, spec);\n\tif (!head)\n\t\treturn NULL;\n\thlist_for_each(node, head) {\n\t\trule = container_of(node, struct efx_arfs_rule, node);\n\t\tif (efx_siena_filter_spec_equal(spec, &rule->spec))\n\t\t\treturn rule;\n\t}\n\treturn NULL;\n}\n\nstatic struct efx_arfs_rule *efx_rps_hash_add(struct efx_nic *efx,\n\t\t\t\t\tconst struct efx_filter_spec *spec,\n\t\t\t\t\tbool *new)\n{\n\tstruct efx_arfs_rule *rule;\n\tstruct hlist_head *head;\n\tstruct hlist_node *node;\n\n\thead = efx_rps_hash_bucket(efx, spec);\n\tif (!head)\n\t\treturn NULL;\n\thlist_for_each(node, head) {\n\t\trule = container_of(node, struct efx_arfs_rule, node);\n\t\tif (efx_siena_filter_spec_equal(spec, &rule->spec)) {\n\t\t\t*new = false;\n\t\t\treturn rule;\n\t\t}\n\t}\n\trule = kmalloc(sizeof(*rule), GFP_ATOMIC);\n\t*new = true;\n\tif (rule) {\n\t\tmemcpy(&rule->spec, spec, sizeof(rule->spec));\n\t\thlist_add_head(&rule->node, head);\n\t}\n\treturn rule;\n}\n\nvoid efx_siena_rps_hash_del(struct efx_nic *efx,\n\t\t\t    const struct efx_filter_spec *spec)\n{\n\tstruct efx_arfs_rule *rule;\n\tstruct hlist_head *head;\n\tstruct hlist_node *node;\n\n\thead = efx_rps_hash_bucket(efx, spec);\n\tif (WARN_ON(!head))\n\t\treturn;\n\thlist_for_each(node, head) {\n\t\trule = container_of(node, struct efx_arfs_rule, node);\n\t\tif (efx_siena_filter_spec_equal(spec, &rule->spec)) {\n\t\t\t \n\t\t\tif (rule->filter_id != EFX_ARFS_FILTER_ID_REMOVING)\n\t\t\t\treturn;\n\t\t\thlist_del(node);\n\t\t\tkfree(rule);\n\t\t\treturn;\n\t\t}\n\t}\n\t \n\tWARN_ON(1);\n}\n#endif\n\nint efx_siena_probe_filters(struct efx_nic *efx)\n{\n\tint rc;\n\n\tmutex_lock(&efx->mac_lock);\n\tdown_write(&efx->filter_sem);\n\trc = efx->type->filter_table_probe(efx);\n\tif (rc)\n\t\tgoto out_unlock;\n\n#ifdef CONFIG_RFS_ACCEL\n\tif (efx->type->offload_features & NETIF_F_NTUPLE) {\n\t\tstruct efx_channel *channel;\n\t\tint i, success = 1;\n\n\t\tefx_for_each_channel(channel, efx) {\n\t\t\tchannel->rps_flow_id =\n\t\t\t\tkcalloc(efx->type->max_rx_ip_filters,\n\t\t\t\t\tsizeof(*channel->rps_flow_id),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!channel->rps_flow_id)\n\t\t\t\tsuccess = 0;\n\t\t\telse\n\t\t\t\tfor (i = 0;\n\t\t\t\t     i < efx->type->max_rx_ip_filters;\n\t\t\t\t     ++i)\n\t\t\t\t\tchannel->rps_flow_id[i] =\n\t\t\t\t\t\tRPS_FLOW_ID_INVALID;\n\t\t\tchannel->rfs_expire_index = 0;\n\t\t\tchannel->rfs_filter_count = 0;\n\t\t}\n\n\t\tif (!success) {\n\t\t\tefx_for_each_channel(channel, efx)\n\t\t\t\tkfree(channel->rps_flow_id);\n\t\t\tefx->type->filter_table_remove(efx);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n#endif\nout_unlock:\n\tup_write(&efx->filter_sem);\n\tmutex_unlock(&efx->mac_lock);\n\treturn rc;\n}\n\nvoid efx_siena_remove_filters(struct efx_nic *efx)\n{\n#ifdef CONFIG_RFS_ACCEL\n\tstruct efx_channel *channel;\n\n\tefx_for_each_channel(channel, efx) {\n\t\tcancel_delayed_work_sync(&channel->filter_work);\n\t\tkfree(channel->rps_flow_id);\n\t\tchannel->rps_flow_id = NULL;\n\t}\n#endif\n\tdown_write(&efx->filter_sem);\n\tefx->type->filter_table_remove(efx);\n\tup_write(&efx->filter_sem);\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\nstatic void efx_filter_rfs_work(struct work_struct *data)\n{\n\tstruct efx_async_filter_insertion *req = container_of(data, struct efx_async_filter_insertion,\n\t\t\t\t\t\t\t      work);\n\tstruct efx_nic *efx = netdev_priv(req->net_dev);\n\tstruct efx_channel *channel = efx_get_channel(efx, req->rxq_index);\n\tint slot_idx = req - efx->rps_slot;\n\tstruct efx_arfs_rule *rule;\n\tu16 arfs_id = 0;\n\tint rc;\n\n\trc = efx->type->filter_insert(efx, &req->spec, true);\n\tif (rc >= 0)\n\t\t \n\t\trc %= efx->type->max_rx_ip_filters;\n\tif (efx->rps_hash_table) {\n\t\tspin_lock_bh(&efx->rps_hash_lock);\n\t\trule = efx_siena_rps_hash_find(efx, &req->spec);\n\t\t \n\t\tif (rule) {\n\t\t\tif (rc < 0)\n\t\t\t\trule->filter_id = EFX_ARFS_FILTER_ID_ERROR;\n\t\t\telse\n\t\t\t\trule->filter_id = rc;\n\t\t\tarfs_id = rule->arfs_id;\n\t\t}\n\t\tspin_unlock_bh(&efx->rps_hash_lock);\n\t}\n\tif (rc >= 0) {\n\t\t \n\t\tmutex_lock(&efx->rps_mutex);\n\t\tif (channel->rps_flow_id[rc] == RPS_FLOW_ID_INVALID)\n\t\t\tchannel->rfs_filter_count++;\n\t\tchannel->rps_flow_id[rc] = req->flow_id;\n\t\tmutex_unlock(&efx->rps_mutex);\n\n\t\tif (req->spec.ether_type == htons(ETH_P_IP))\n\t\t\tnetif_info(efx, rx_status, efx->net_dev,\n\t\t\t\t   \"steering %s %pI4:%u:%pI4:%u to queue %u [flow %u filter %d id %u]\\n\",\n\t\t\t\t   (req->spec.ip_proto == IPPROTO_TCP) ? \"TCP\" : \"UDP\",\n\t\t\t\t   req->spec.rem_host, ntohs(req->spec.rem_port),\n\t\t\t\t   req->spec.loc_host, ntohs(req->spec.loc_port),\n\t\t\t\t   req->rxq_index, req->flow_id, rc, arfs_id);\n\t\telse\n\t\t\tnetif_info(efx, rx_status, efx->net_dev,\n\t\t\t\t   \"steering %s [%pI6]:%u:[%pI6]:%u to queue %u [flow %u filter %d id %u]\\n\",\n\t\t\t\t   (req->spec.ip_proto == IPPROTO_TCP) ? \"TCP\" : \"UDP\",\n\t\t\t\t   req->spec.rem_host, ntohs(req->spec.rem_port),\n\t\t\t\t   req->spec.loc_host, ntohs(req->spec.loc_port),\n\t\t\t\t   req->rxq_index, req->flow_id, rc, arfs_id);\n\t\tchannel->n_rfs_succeeded++;\n\t} else {\n\t\tif (req->spec.ether_type == htons(ETH_P_IP))\n\t\t\tnetif_dbg(efx, rx_status, efx->net_dev,\n\t\t\t\t  \"failed to steer %s %pI4:%u:%pI4:%u to queue %u [flow %u rc %d id %u]\\n\",\n\t\t\t\t  (req->spec.ip_proto == IPPROTO_TCP) ? \"TCP\" : \"UDP\",\n\t\t\t\t  req->spec.rem_host, ntohs(req->spec.rem_port),\n\t\t\t\t  req->spec.loc_host, ntohs(req->spec.loc_port),\n\t\t\t\t  req->rxq_index, req->flow_id, rc, arfs_id);\n\t\telse\n\t\t\tnetif_dbg(efx, rx_status, efx->net_dev,\n\t\t\t\t  \"failed to steer %s [%pI6]:%u:[%pI6]:%u to queue %u [flow %u rc %d id %u]\\n\",\n\t\t\t\t  (req->spec.ip_proto == IPPROTO_TCP) ? \"TCP\" : \"UDP\",\n\t\t\t\t  req->spec.rem_host, ntohs(req->spec.rem_port),\n\t\t\t\t  req->spec.loc_host, ntohs(req->spec.loc_port),\n\t\t\t\t  req->rxq_index, req->flow_id, rc, arfs_id);\n\t\tchannel->n_rfs_failed++;\n\t\t \n\t\t__efx_siena_filter_rfs_expire(channel,\n\t\t\t\t\t      min(channel->rfs_filter_count,\n\t\t\t\t\t\t  100u));\n\t}\n\n\t \n\tclear_bit(slot_idx, &efx->rps_slot_map);\n\tdev_put(req->net_dev);\n}\n\nint efx_siena_filter_rfs(struct net_device *net_dev, const struct sk_buff *skb,\n\t\t\t u16 rxq_index, u32 flow_id)\n{\n\tstruct efx_nic *efx = netdev_priv(net_dev);\n\tstruct efx_async_filter_insertion *req;\n\tstruct efx_arfs_rule *rule;\n\tstruct flow_keys fk;\n\tint slot_idx;\n\tbool new;\n\tint rc;\n\n\t \n\tfor (slot_idx = 0; slot_idx < EFX_RPS_MAX_IN_FLIGHT; slot_idx++)\n\t\tif (!test_and_set_bit(slot_idx, &efx->rps_slot_map))\n\t\t\tbreak;\n\tif (slot_idx >= EFX_RPS_MAX_IN_FLIGHT)\n\t\treturn -EBUSY;\n\n\tif (flow_id == RPS_FLOW_ID_INVALID) {\n\t\trc = -EINVAL;\n\t\tgoto out_clear;\n\t}\n\n\tif (!skb_flow_dissect_flow_keys(skb, &fk, 0)) {\n\t\trc = -EPROTONOSUPPORT;\n\t\tgoto out_clear;\n\t}\n\n\tif (fk.basic.n_proto != htons(ETH_P_IP) && fk.basic.n_proto != htons(ETH_P_IPV6)) {\n\t\trc = -EPROTONOSUPPORT;\n\t\tgoto out_clear;\n\t}\n\tif (fk.control.flags & FLOW_DIS_IS_FRAGMENT) {\n\t\trc = -EPROTONOSUPPORT;\n\t\tgoto out_clear;\n\t}\n\n\treq = efx->rps_slot + slot_idx;\n\tefx_filter_init_rx(&req->spec, EFX_FILTER_PRI_HINT,\n\t\t\t   efx->rx_scatter ? EFX_FILTER_FLAG_RX_SCATTER : 0,\n\t\t\t   rxq_index);\n\treq->spec.match_flags =\n\t\tEFX_FILTER_MATCH_ETHER_TYPE | EFX_FILTER_MATCH_IP_PROTO |\n\t\tEFX_FILTER_MATCH_LOC_HOST | EFX_FILTER_MATCH_LOC_PORT |\n\t\tEFX_FILTER_MATCH_REM_HOST | EFX_FILTER_MATCH_REM_PORT;\n\treq->spec.ether_type = fk.basic.n_proto;\n\treq->spec.ip_proto = fk.basic.ip_proto;\n\n\tif (fk.basic.n_proto == htons(ETH_P_IP)) {\n\t\treq->spec.rem_host[0] = fk.addrs.v4addrs.src;\n\t\treq->spec.loc_host[0] = fk.addrs.v4addrs.dst;\n\t} else {\n\t\tmemcpy(req->spec.rem_host, &fk.addrs.v6addrs.src,\n\t\t       sizeof(struct in6_addr));\n\t\tmemcpy(req->spec.loc_host, &fk.addrs.v6addrs.dst,\n\t\t       sizeof(struct in6_addr));\n\t}\n\n\treq->spec.rem_port = fk.ports.src;\n\treq->spec.loc_port = fk.ports.dst;\n\n\tif (efx->rps_hash_table) {\n\t\t \n\t\tspin_lock(&efx->rps_hash_lock);\n\t\trule = efx_rps_hash_add(efx, &req->spec, &new);\n\t\tif (!rule) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (new)\n\t\t\trule->arfs_id = efx->rps_next_id++ % RPS_NO_FILTER;\n\t\trc = rule->arfs_id;\n\t\t \n\t\tif (!new && rule->rxq_index == rxq_index &&\n\t\t    rule->filter_id >= EFX_ARFS_FILTER_ID_PENDING)\n\t\t\tgoto out_unlock;\n\t\trule->rxq_index = rxq_index;\n\t\trule->filter_id = EFX_ARFS_FILTER_ID_PENDING;\n\t\tspin_unlock(&efx->rps_hash_lock);\n\t} else {\n\t\t \n\t\trc = 0;\n\t}\n\n\t \n\tdev_hold(req->net_dev = net_dev);\n\tINIT_WORK(&req->work, efx_filter_rfs_work);\n\treq->rxq_index = rxq_index;\n\treq->flow_id = flow_id;\n\tschedule_work(&req->work);\n\treturn rc;\nout_unlock:\n\tspin_unlock(&efx->rps_hash_lock);\nout_clear:\n\tclear_bit(slot_idx, &efx->rps_slot_map);\n\treturn rc;\n}\n\nbool __efx_siena_filter_rfs_expire(struct efx_channel *channel,\n\t\t\t\t   unsigned int quota)\n{\n\tbool (*expire_one)(struct efx_nic *efx, u32 flow_id, unsigned int index);\n\tstruct efx_nic *efx = channel->efx;\n\tunsigned int index, size, start;\n\tu32 flow_id;\n\n\tif (!mutex_trylock(&efx->rps_mutex))\n\t\treturn false;\n\texpire_one = efx->type->filter_rfs_expire_one;\n\tindex = channel->rfs_expire_index;\n\tstart = index;\n\tsize = efx->type->max_rx_ip_filters;\n\twhile (quota) {\n\t\tflow_id = channel->rps_flow_id[index];\n\n\t\tif (flow_id != RPS_FLOW_ID_INVALID) {\n\t\t\tquota--;\n\t\t\tif (expire_one(efx, flow_id, index)) {\n\t\t\t\tnetif_info(efx, rx_status, efx->net_dev,\n\t\t\t\t\t   \"expired filter %d [channel %u flow %u]\\n\",\n\t\t\t\t\t   index, channel->channel, flow_id);\n\t\t\t\tchannel->rps_flow_id[index] = RPS_FLOW_ID_INVALID;\n\t\t\t\tchannel->rfs_filter_count--;\n\t\t\t}\n\t\t}\n\t\tif (++index == size)\n\t\t\tindex = 0;\n\t\t \n\t\tif (index == start)\n\t\t\tbreak;\n\t}\n\n\tchannel->rfs_expire_index = index;\n\tmutex_unlock(&efx->rps_mutex);\n\treturn true;\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}