{
  "module_name": "tx.c",
  "hash_id": "c71ac5938ae1a1e59ff3eea7799c58eca3a936d165f513ff36b43abd87980320",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/siena/tx.c",
  "human_readable_source": "\n \n\n#include <linux/pci.h>\n#include <linux/tcp.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/ipv6.h>\n#include <linux/slab.h>\n#include <net/ipv6.h>\n#include <linux/if_ether.h>\n#include <linux/highmem.h>\n#include <linux/cache.h>\n#include \"net_driver.h\"\n#include \"efx.h\"\n#include \"io.h\"\n#include \"nic.h\"\n#include \"tx.h\"\n#include \"tx_common.h\"\n#include \"workarounds.h\"\n\nstatic inline u8 *efx_tx_get_copy_buffer(struct efx_tx_queue *tx_queue,\n\t\t\t\t\t struct efx_tx_buffer *buffer)\n{\n\tunsigned int index = efx_tx_queue_get_insert_index(tx_queue);\n\tstruct efx_buffer *page_buf =\n\t\t&tx_queue->cb_page[index >> (PAGE_SHIFT - EFX_TX_CB_ORDER)];\n\tunsigned int offset =\n\t\t((index << EFX_TX_CB_ORDER) + NET_IP_ALIGN) & (PAGE_SIZE - 1);\n\n\tif (unlikely(!page_buf->addr) &&\n\t    efx_siena_alloc_buffer(tx_queue->efx, page_buf, PAGE_SIZE,\n\t\t\t\t   GFP_ATOMIC))\n\t\treturn NULL;\n\tbuffer->dma_addr = page_buf->dma_addr + offset;\n\tbuffer->unmap_len = 0;\n\treturn (u8 *)page_buf->addr + offset;\n}\n\nstatic void efx_tx_maybe_stop_queue(struct efx_tx_queue *txq1)\n{\n\t \n\tstruct efx_nic *efx = txq1->efx;\n\tstruct efx_tx_queue *txq2;\n\tunsigned int fill_level;\n\n\tfill_level = efx_channel_tx_old_fill_level(txq1->channel);\n\tif (likely(fill_level < efx->txq_stop_thresh))\n\t\treturn;\n\n\t \n\tnetif_tx_stop_queue(txq1->core_txq);\n\tsmp_mb();\n\tefx_for_each_channel_tx_queue(txq2, txq1->channel)\n\t\ttxq2->old_read_count = READ_ONCE(txq2->read_count);\n\n\tfill_level = efx_channel_tx_old_fill_level(txq1->channel);\n\tEFX_WARN_ON_ONCE_PARANOID(fill_level >= efx->txq_entries);\n\tif (likely(fill_level < efx->txq_stop_thresh)) {\n\t\tsmp_mb();\n\t\tif (likely(!efx->loopback_selftest))\n\t\t\tnetif_tx_start_queue(txq1->core_txq);\n\t}\n}\n\nstatic int efx_enqueue_skb_copy(struct efx_tx_queue *tx_queue,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tunsigned int copy_len = skb->len;\n\tstruct efx_tx_buffer *buffer;\n\tu8 *copy_buffer;\n\tint rc;\n\n\tEFX_WARN_ON_ONCE_PARANOID(copy_len > EFX_TX_CB_SIZE);\n\n\tbuffer = efx_tx_queue_get_insert_buffer(tx_queue);\n\n\tcopy_buffer = efx_tx_get_copy_buffer(tx_queue, buffer);\n\tif (unlikely(!copy_buffer))\n\t\treturn -ENOMEM;\n\n\trc = skb_copy_bits(skb, 0, copy_buffer, copy_len);\n\tEFX_WARN_ON_PARANOID(rc);\n\tbuffer->len = copy_len;\n\n\tbuffer->skb = skb;\n\tbuffer->flags = EFX_TX_BUF_SKB;\n\n\t++tx_queue->insert_count;\n\treturn rc;\n}\n\n \nstatic void efx_tx_send_pending(struct efx_channel *channel)\n{\n\tstruct efx_tx_queue *q;\n\n\tefx_for_each_channel_tx_queue(q, channel) {\n\t\tif (q->xmit_pending)\n\t\t\tefx_nic_push_buffers(q);\n\t}\n}\n\n \nnetdev_tx_t __efx_siena_enqueue_skb(struct efx_tx_queue *tx_queue,\n\t\t\t\t    struct sk_buff *skb)\n{\n\tunsigned int old_insert_count = tx_queue->insert_count;\n\tbool xmit_more = netdev_xmit_more();\n\tbool data_mapped = false;\n\tunsigned int segments;\n\tunsigned int skb_len;\n\tint rc;\n\n\tskb_len = skb->len;\n\tsegments = skb_is_gso(skb) ? skb_shinfo(skb)->gso_segs : 0;\n\tif (segments == 1)\n\t\tsegments = 0;  \n\n\t \n\tif (segments) {\n\t\trc = efx_siena_tx_tso_fallback(tx_queue, skb);\n\t\ttx_queue->tso_fallbacks++;\n\t\tif (rc == 0)\n\t\t\treturn 0;\n\t\tgoto err;\n\t} else if (skb->data_len && skb_len <= EFX_TX_CB_SIZE) {\n\t\t \n\t\tif (efx_enqueue_skb_copy(tx_queue, skb))\n\t\t\tgoto err;\n\t\ttx_queue->cb_packets++;\n\t\tdata_mapped = true;\n\t}\n\n\t \n\tif (!data_mapped && (efx_siena_tx_map_data(tx_queue, skb, segments)))\n\t\tgoto err;\n\n\tefx_tx_maybe_stop_queue(tx_queue);\n\n\ttx_queue->xmit_pending = true;\n\n\t \n\tif (__netdev_tx_sent_queue(tx_queue->core_txq, skb_len, xmit_more))\n\t\tefx_tx_send_pending(tx_queue->channel);\n\n\ttx_queue->tx_packets++;\n\treturn NETDEV_TX_OK;\n\n\nerr:\n\tefx_siena_enqueue_unwind(tx_queue, old_insert_count);\n\tdev_kfree_skb_any(skb);\n\n\t \n\tif (!xmit_more)\n\t\tefx_tx_send_pending(tx_queue->channel);\n\n\treturn NETDEV_TX_OK;\n}\n\n \nint efx_siena_xdp_tx_buffers(struct efx_nic *efx, int n, struct xdp_frame **xdpfs,\n\t\t\t     bool flush)\n{\n\tstruct efx_tx_buffer *tx_buffer;\n\tstruct efx_tx_queue *tx_queue;\n\tstruct xdp_frame *xdpf;\n\tdma_addr_t dma_addr;\n\tunsigned int len;\n\tint space;\n\tint cpu;\n\tint i = 0;\n\n\tif (unlikely(n && !xdpfs))\n\t\treturn -EINVAL;\n\tif (unlikely(!n))\n\t\treturn 0;\n\n\tcpu = raw_smp_processor_id();\n\tif (unlikely(cpu >= efx->xdp_tx_queue_count))\n\t\treturn -EINVAL;\n\n\ttx_queue = efx->xdp_tx_queues[cpu];\n\tif (unlikely(!tx_queue))\n\t\treturn -EINVAL;\n\n\tif (!tx_queue->initialised)\n\t\treturn -EINVAL;\n\n\tif (efx->xdp_txq_queues_mode != EFX_XDP_TX_QUEUES_DEDICATED)\n\t\tHARD_TX_LOCK(efx->net_dev, tx_queue->core_txq, cpu);\n\n\t \n\tif (efx->xdp_txq_queues_mode == EFX_XDP_TX_QUEUES_BORROWED) {\n\t\tif (netif_tx_queue_stopped(tx_queue->core_txq))\n\t\t\tgoto unlock;\n\t\tefx_tx_maybe_stop_queue(tx_queue);\n\t}\n\n\t \n\tspace = efx->txq_entries +\n\t\ttx_queue->read_count - tx_queue->insert_count;\n\n\tfor (i = 0; i < n; i++) {\n\t\txdpf = xdpfs[i];\n\n\t\tif (i >= space)\n\t\t\tbreak;\n\n\t\t \n\t\tprefetchw(__efx_tx_queue_get_insert_buffer(tx_queue));\n\n\t\tlen = xdpf->len;\n\n\t\t \n\t\tdma_addr = dma_map_single(&efx->pci_dev->dev,\n\t\t\t\t\t  xdpf->data, len,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&efx->pci_dev->dev, dma_addr))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buffer = efx_siena_tx_map_chunk(tx_queue, dma_addr, len);\n\t\ttx_buffer->xdpf = xdpf;\n\t\ttx_buffer->flags = EFX_TX_BUF_XDP |\n\t\t\t\t   EFX_TX_BUF_MAP_SINGLE;\n\t\ttx_buffer->dma_offset = 0;\n\t\ttx_buffer->unmap_len = len;\n\t\ttx_queue->tx_packets++;\n\t}\n\n\t \n\tif (flush && i > 0)\n\t\tefx_nic_push_buffers(tx_queue);\n\nunlock:\n\tif (efx->xdp_txq_queues_mode != EFX_XDP_TX_QUEUES_DEDICATED)\n\t\tHARD_TX_UNLOCK(efx->net_dev, tx_queue->core_txq);\n\n\treturn i == 0 ? -EIO : i;\n}\n\n \nnetdev_tx_t efx_siena_hard_start_xmit(struct sk_buff *skb,\n\t\t\t\t      struct net_device *net_dev)\n{\n\tstruct efx_nic *efx = netdev_priv(net_dev);\n\tstruct efx_tx_queue *tx_queue;\n\tunsigned index, type;\n\n\tEFX_WARN_ON_PARANOID(!netif_device_present(net_dev));\n\n\tindex = skb_get_queue_mapping(skb);\n\ttype = efx_tx_csum_type_skb(skb);\n\tif (index >= efx->n_tx_channels) {\n\t\tindex -= efx->n_tx_channels;\n\t\ttype |= EFX_TXQ_TYPE_HIGHPRI;\n\t}\n\n\t \n\tif (unlikely(efx_xmit_with_hwtstamp(skb)) &&\n\t    ((efx_siena_ptp_use_mac_tx_timestamps(efx) && efx->ptp_data) ||\n\t     unlikely(efx_siena_ptp_is_ptp_tx(efx, skb)))) {\n\t\t \n\t\tefx_tx_send_pending(efx_get_tx_channel(efx, index));\n\t\treturn efx_siena_ptp_tx(efx, skb);\n\t}\n\n\ttx_queue = efx_get_tx_queue(efx, index, type);\n\tif (WARN_ON_ONCE(!tx_queue)) {\n\t\t \n\t\tdev_kfree_skb_any(skb);\n\t\t \n\t\tif (!netdev_xmit_more())\n\t\t\tefx_tx_send_pending(efx_get_tx_channel(efx, index));\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\treturn __efx_siena_enqueue_skb(tx_queue, skb);\n}\n\nvoid efx_siena_init_tx_queue_core_txq(struct efx_tx_queue *tx_queue)\n{\n\tstruct efx_nic *efx = tx_queue->efx;\n\n\t \n\ttx_queue->core_txq =\n\t\tnetdev_get_tx_queue(efx->net_dev,\n\t\t\t\t    tx_queue->channel->channel +\n\t\t\t\t    ((tx_queue->type & EFX_TXQ_TYPE_HIGHPRI) ?\n\t\t\t\t     efx->n_tx_channels : 0));\n}\n\nint efx_siena_setup_tc(struct net_device *net_dev, enum tc_setup_type type,\n\t\t       void *type_data)\n{\n\tstruct efx_nic *efx = netdev_priv(net_dev);\n\tstruct tc_mqprio_qopt *mqprio = type_data;\n\tunsigned tc, num_tc;\n\n\tif (type != TC_SETUP_QDISC_MQPRIO)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (efx_nic_rev(efx) > EFX_REV_SIENA_A0)\n\t\treturn -EOPNOTSUPP;\n\n\tnum_tc = mqprio->num_tc;\n\n\tif (num_tc > EFX_MAX_TX_TC)\n\t\treturn -EINVAL;\n\n\tmqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;\n\n\tif (num_tc == net_dev->num_tc)\n\t\treturn 0;\n\n\tfor (tc = 0; tc < num_tc; tc++) {\n\t\tnet_dev->tc_to_txq[tc].offset = tc * efx->n_tx_channels;\n\t\tnet_dev->tc_to_txq[tc].count = efx->n_tx_channels;\n\t}\n\n\tnet_dev->num_tc = num_tc;\n\n\treturn netif_set_real_num_tx_queues(net_dev,\n\t\t\t\t\t    max_t(int, num_tc, 1) *\n\t\t\t\t\t    efx->n_tx_channels);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}