{
  "module_name": "tx_common.c",
  "hash_id": "7e3edcacb7d65eeda4132de37eb479ab1f3d73b929d80759c2e64ea5440db074",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/siena/tx_common.c",
  "human_readable_source": "\n \n\n#include \"net_driver.h\"\n#include \"efx.h\"\n#include \"nic_common.h\"\n#include \"tx_common.h\"\n#include <net/gso.h>\n\nstatic unsigned int efx_tx_cb_page_count(struct efx_tx_queue *tx_queue)\n{\n\treturn DIV_ROUND_UP(tx_queue->ptr_mask + 1,\n\t\t\t    PAGE_SIZE >> EFX_TX_CB_ORDER);\n}\n\nint efx_siena_probe_tx_queue(struct efx_tx_queue *tx_queue)\n{\n\tstruct efx_nic *efx = tx_queue->efx;\n\tunsigned int entries;\n\tint rc;\n\n\t \n\tentries = max(roundup_pow_of_two(efx->txq_entries), EFX_MIN_DMAQ_SIZE);\n\tEFX_WARN_ON_PARANOID(entries > EFX_MAX_DMAQ_SIZE);\n\ttx_queue->ptr_mask = entries - 1;\n\n\tnetif_dbg(efx, probe, efx->net_dev,\n\t\t  \"creating TX queue %d size %#x mask %#x\\n\",\n\t\t  tx_queue->queue, efx->txq_entries, tx_queue->ptr_mask);\n\n\t \n\ttx_queue->buffer = kcalloc(entries, sizeof(*tx_queue->buffer),\n\t\t\t\t   GFP_KERNEL);\n\tif (!tx_queue->buffer)\n\t\treturn -ENOMEM;\n\n\ttx_queue->cb_page = kcalloc(efx_tx_cb_page_count(tx_queue),\n\t\t\t\t    sizeof(tx_queue->cb_page[0]), GFP_KERNEL);\n\tif (!tx_queue->cb_page) {\n\t\trc = -ENOMEM;\n\t\tgoto fail1;\n\t}\n\n\t \n\trc = efx_nic_probe_tx(tx_queue);\n\tif (rc)\n\t\tgoto fail2;\n\n\ttx_queue->channel->tx_queue_by_type[tx_queue->type] = tx_queue;\n\treturn 0;\n\nfail2:\n\tkfree(tx_queue->cb_page);\n\ttx_queue->cb_page = NULL;\nfail1:\n\tkfree(tx_queue->buffer);\n\ttx_queue->buffer = NULL;\n\treturn rc;\n}\n\nvoid efx_siena_init_tx_queue(struct efx_tx_queue *tx_queue)\n{\n\tstruct efx_nic *efx = tx_queue->efx;\n\n\tnetif_dbg(efx, drv, efx->net_dev,\n\t\t  \"initialising TX queue %d\\n\", tx_queue->queue);\n\n\ttx_queue->insert_count = 0;\n\ttx_queue->notify_count = 0;\n\ttx_queue->write_count = 0;\n\ttx_queue->packet_write_count = 0;\n\ttx_queue->old_write_count = 0;\n\ttx_queue->read_count = 0;\n\ttx_queue->old_read_count = 0;\n\ttx_queue->empty_read_count = 0 | EFX_EMPTY_COUNT_VALID;\n\ttx_queue->xmit_pending = false;\n\ttx_queue->timestamping = (efx_siena_ptp_use_mac_tx_timestamps(efx) &&\n\t\t\t\t  tx_queue->channel == efx_siena_ptp_channel(efx));\n\ttx_queue->completed_timestamp_major = 0;\n\ttx_queue->completed_timestamp_minor = 0;\n\n\ttx_queue->xdp_tx = efx_channel_is_xdp_tx(tx_queue->channel);\n\ttx_queue->tso_version = 0;\n\n\t \n\tefx_nic_init_tx(tx_queue);\n\n\ttx_queue->initialised = true;\n}\n\nvoid efx_siena_remove_tx_queue(struct efx_tx_queue *tx_queue)\n{\n\tint i;\n\n\tif (!tx_queue->buffer)\n\t\treturn;\n\n\tnetif_dbg(tx_queue->efx, drv, tx_queue->efx->net_dev,\n\t\t  \"destroying TX queue %d\\n\", tx_queue->queue);\n\tefx_nic_remove_tx(tx_queue);\n\n\tif (tx_queue->cb_page) {\n\t\tfor (i = 0; i < efx_tx_cb_page_count(tx_queue); i++)\n\t\t\tefx_siena_free_buffer(tx_queue->efx,\n\t\t\t\t\t      &tx_queue->cb_page[i]);\n\t\tkfree(tx_queue->cb_page);\n\t\ttx_queue->cb_page = NULL;\n\t}\n\n\tkfree(tx_queue->buffer);\n\ttx_queue->buffer = NULL;\n\ttx_queue->channel->tx_queue_by_type[tx_queue->type] = NULL;\n}\n\nstatic void efx_dequeue_buffer(struct efx_tx_queue *tx_queue,\n\t\t\t       struct efx_tx_buffer *buffer,\n\t\t\t       unsigned int *pkts_compl,\n\t\t\t       unsigned int *bytes_compl)\n{\n\tif (buffer->unmap_len) {\n\t\tstruct device *dma_dev = &tx_queue->efx->pci_dev->dev;\n\t\tdma_addr_t unmap_addr = buffer->dma_addr - buffer->dma_offset;\n\n\t\tif (buffer->flags & EFX_TX_BUF_MAP_SINGLE)\n\t\t\tdma_unmap_single(dma_dev, unmap_addr, buffer->unmap_len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_page(dma_dev, unmap_addr, buffer->unmap_len,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tbuffer->unmap_len = 0;\n\t}\n\n\tif (buffer->flags & EFX_TX_BUF_SKB) {\n\t\tstruct sk_buff *skb = (struct sk_buff *)buffer->skb;\n\n\t\tEFX_WARN_ON_PARANOID(!pkts_compl || !bytes_compl);\n\t\t(*pkts_compl)++;\n\t\t(*bytes_compl) += skb->len;\n\t\tif (tx_queue->timestamping &&\n\t\t    (tx_queue->completed_timestamp_major ||\n\t\t     tx_queue->completed_timestamp_minor)) {\n\t\t\tstruct skb_shared_hwtstamps hwtstamp;\n\n\t\t\thwtstamp.hwtstamp =\n\t\t\t\tefx_siena_ptp_nic_to_kernel_time(tx_queue);\n\t\t\tskb_tstamp_tx(skb, &hwtstamp);\n\n\t\t\ttx_queue->completed_timestamp_major = 0;\n\t\t\ttx_queue->completed_timestamp_minor = 0;\n\t\t}\n\t\tdev_consume_skb_any((struct sk_buff *)buffer->skb);\n\t\tnetif_vdbg(tx_queue->efx, tx_done, tx_queue->efx->net_dev,\n\t\t\t   \"TX queue %d transmission id %x complete\\n\",\n\t\t\t   tx_queue->queue, tx_queue->read_count);\n\t} else if (buffer->flags & EFX_TX_BUF_XDP) {\n\t\txdp_return_frame_rx_napi(buffer->xdpf);\n\t}\n\n\tbuffer->len = 0;\n\tbuffer->flags = 0;\n}\n\nvoid efx_siena_fini_tx_queue(struct efx_tx_queue *tx_queue)\n{\n\tstruct efx_tx_buffer *buffer;\n\n\tnetif_dbg(tx_queue->efx, drv, tx_queue->efx->net_dev,\n\t\t  \"shutting down TX queue %d\\n\", tx_queue->queue);\n\n\tif (!tx_queue->buffer)\n\t\treturn;\n\n\t \n\twhile (tx_queue->read_count != tx_queue->write_count) {\n\t\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\n\t\tbuffer = &tx_queue->buffer[tx_queue->read_count & tx_queue->ptr_mask];\n\t\tefx_dequeue_buffer(tx_queue, buffer, &pkts_compl, &bytes_compl);\n\n\t\t++tx_queue->read_count;\n\t}\n\ttx_queue->xmit_pending = false;\n\tnetdev_tx_reset_queue(tx_queue->core_txq);\n}\n\n \nstatic void efx_dequeue_buffers(struct efx_tx_queue *tx_queue,\n\t\t\t\tunsigned int index,\n\t\t\t\tunsigned int *pkts_compl,\n\t\t\t\tunsigned int *bytes_compl)\n{\n\tstruct efx_nic *efx = tx_queue->efx;\n\tunsigned int stop_index, read_ptr;\n\n\tstop_index = (index + 1) & tx_queue->ptr_mask;\n\tread_ptr = tx_queue->read_count & tx_queue->ptr_mask;\n\n\twhile (read_ptr != stop_index) {\n\t\tstruct efx_tx_buffer *buffer = &tx_queue->buffer[read_ptr];\n\n\t\tif (!efx_tx_buffer_in_use(buffer)) {\n\t\t\tnetif_err(efx, tx_err, efx->net_dev,\n\t\t\t\t  \"TX queue %d spurious TX completion id %d\\n\",\n\t\t\t\t  tx_queue->queue, read_ptr);\n\t\t\tefx_siena_schedule_reset(efx, RESET_TYPE_TX_SKIP);\n\t\t\treturn;\n\t\t}\n\n\t\tefx_dequeue_buffer(tx_queue, buffer, pkts_compl, bytes_compl);\n\n\t\t++tx_queue->read_count;\n\t\tread_ptr = tx_queue->read_count & tx_queue->ptr_mask;\n\t}\n}\n\nvoid efx_siena_xmit_done_check_empty(struct efx_tx_queue *tx_queue)\n{\n\tif ((int)(tx_queue->read_count - tx_queue->old_write_count) >= 0) {\n\t\ttx_queue->old_write_count = READ_ONCE(tx_queue->write_count);\n\t\tif (tx_queue->read_count == tx_queue->old_write_count) {\n\t\t\t \n\t\t\tsmp_mb();\n\t\t\ttx_queue->empty_read_count =\n\t\t\t\ttx_queue->read_count | EFX_EMPTY_COUNT_VALID;\n\t\t}\n\t}\n}\n\nvoid efx_siena_xmit_done(struct efx_tx_queue *tx_queue, unsigned int index)\n{\n\tunsigned int fill_level, pkts_compl = 0, bytes_compl = 0;\n\tstruct efx_nic *efx = tx_queue->efx;\n\n\tEFX_WARN_ON_ONCE_PARANOID(index > tx_queue->ptr_mask);\n\n\tefx_dequeue_buffers(tx_queue, index, &pkts_compl, &bytes_compl);\n\ttx_queue->pkts_compl += pkts_compl;\n\ttx_queue->bytes_compl += bytes_compl;\n\n\tif (pkts_compl > 1)\n\t\t++tx_queue->merge_events;\n\n\t \n\tsmp_mb();\n\tif (unlikely(netif_tx_queue_stopped(tx_queue->core_txq)) &&\n\t    likely(efx->port_enabled) &&\n\t    likely(netif_device_present(efx->net_dev))) {\n\t\tfill_level = efx_channel_tx_fill_level(tx_queue->channel);\n\t\tif (fill_level <= efx->txq_wake_thresh)\n\t\t\tnetif_tx_wake_queue(tx_queue->core_txq);\n\t}\n\n\tefx_siena_xmit_done_check_empty(tx_queue);\n}\n\n \nvoid efx_siena_enqueue_unwind(struct efx_tx_queue *tx_queue,\n\t\t\t      unsigned int insert_count)\n{\n\tstruct efx_tx_buffer *buffer;\n\tunsigned int bytes_compl = 0;\n\tunsigned int pkts_compl = 0;\n\n\t \n\twhile (tx_queue->insert_count != insert_count) {\n\t\t--tx_queue->insert_count;\n\t\tbuffer = __efx_tx_queue_get_insert_buffer(tx_queue);\n\t\tefx_dequeue_buffer(tx_queue, buffer, &pkts_compl, &bytes_compl);\n\t}\n}\n\nstruct efx_tx_buffer *efx_siena_tx_map_chunk(struct efx_tx_queue *tx_queue,\n\t\t\t\t\t     dma_addr_t dma_addr, size_t len)\n{\n\tconst struct efx_nic_type *nic_type = tx_queue->efx->type;\n\tstruct efx_tx_buffer *buffer;\n\tunsigned int dma_len;\n\n\t \n\tdo {\n\t\tbuffer = efx_tx_queue_get_insert_buffer(tx_queue);\n\n\t\tif (nic_type->tx_limit_len)\n\t\t\tdma_len = nic_type->tx_limit_len(tx_queue, dma_addr, len);\n\t\telse\n\t\t\tdma_len = len;\n\n\t\tbuffer->len = dma_len;\n\t\tbuffer->dma_addr = dma_addr;\n\t\tbuffer->flags = EFX_TX_BUF_CONT;\n\t\tlen -= dma_len;\n\t\tdma_addr += dma_len;\n\t\t++tx_queue->insert_count;\n\t} while (len);\n\n\treturn buffer;\n}\n\nstatic int efx_tx_tso_header_length(struct sk_buff *skb)\n{\n\tsize_t header_len;\n\n\tif (skb->encapsulation)\n\t\theader_len = skb_inner_transport_header(skb) -\n\t\t\t\tskb->data +\n\t\t\t\t(inner_tcp_hdr(skb)->doff << 2u);\n\telse\n\t\theader_len = skb_transport_header(skb) - skb->data +\n\t\t\t\t(tcp_hdr(skb)->doff << 2u);\n\treturn header_len;\n}\n\n \nint efx_siena_tx_map_data(struct efx_tx_queue *tx_queue, struct sk_buff *skb,\n\t\t\t  unsigned int segment_count)\n{\n\tstruct efx_nic *efx = tx_queue->efx;\n\tstruct device *dma_dev = &efx->pci_dev->dev;\n\tunsigned int frag_index, nr_frags;\n\tdma_addr_t dma_addr, unmap_addr;\n\tunsigned short dma_flags;\n\tsize_t len, unmap_len;\n\n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\tfrag_index = 0;\n\n\t \n\tlen = skb_headlen(skb);\n\tdma_addr = dma_map_single(dma_dev, skb->data, len, DMA_TO_DEVICE);\n\tdma_flags = EFX_TX_BUF_MAP_SINGLE;\n\tunmap_len = len;\n\tunmap_addr = dma_addr;\n\n\tif (unlikely(dma_mapping_error(dma_dev, dma_addr)))\n\t\treturn -EIO;\n\n\tif (segment_count) {\n\t\t \n\t\tsize_t header_len = efx_tx_tso_header_length(skb);\n\n\t\tif (header_len != len) {\n\t\t\ttx_queue->tso_long_headers++;\n\t\t\tefx_siena_tx_map_chunk(tx_queue, dma_addr, header_len);\n\t\t\tlen -= header_len;\n\t\t\tdma_addr += header_len;\n\t\t}\n\t}\n\n\t \n\tdo {\n\t\tstruct efx_tx_buffer *buffer;\n\t\tskb_frag_t *fragment;\n\n\t\tbuffer = efx_siena_tx_map_chunk(tx_queue, dma_addr, len);\n\n\t\t \n\t\tbuffer->flags = EFX_TX_BUF_CONT | dma_flags;\n\t\tbuffer->unmap_len = unmap_len;\n\t\tbuffer->dma_offset = buffer->dma_addr - unmap_addr;\n\n\t\tif (frag_index >= nr_frags) {\n\t\t\t \n\t\t\tbuffer->skb = skb;\n\t\t\tbuffer->flags = EFX_TX_BUF_SKB | dma_flags;\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tfragment = &skb_shinfo(skb)->frags[frag_index++];\n\t\tlen = skb_frag_size(fragment);\n\t\tdma_addr = skb_frag_dma_map(dma_dev, fragment, 0, len,\n\t\t\t\t\t    DMA_TO_DEVICE);\n\t\tdma_flags = 0;\n\t\tunmap_len = len;\n\t\tunmap_addr = dma_addr;\n\n\t\tif (unlikely(dma_mapping_error(dma_dev, dma_addr)))\n\t\t\treturn -EIO;\n\t} while (1);\n}\n\nunsigned int efx_siena_tx_max_skb_descs(struct efx_nic *efx)\n{\n\t \n\tunsigned int max_descs = EFX_TSO_MAX_SEGS * 2 + MAX_SKB_FRAGS;\n\n\t \n\tif (efx_nic_rev(efx) >= EFX_REV_HUNT_A0)\n\t\tmax_descs += EFX_TSO_MAX_SEGS;\n\n\t \n\tif (PAGE_SIZE > EFX_PAGE_SIZE)\n\t\tmax_descs += max_t(unsigned int, MAX_SKB_FRAGS,\n\t\t\t\t   DIV_ROUND_UP(GSO_MAX_SIZE, EFX_PAGE_SIZE));\n\n\treturn max_descs;\n}\n\n \nint efx_siena_tx_tso_fallback(struct efx_tx_queue *tx_queue,\n\t\t\t      struct sk_buff *skb)\n{\n\tstruct sk_buff *segments, *next;\n\n\tsegments = skb_gso_segment(skb, 0);\n\tif (IS_ERR(segments))\n\t\treturn PTR_ERR(segments);\n\n\tdev_consume_skb_any(skb);\n\n\tskb_list_walk_safe(segments, skb, next) {\n\t\tskb_mark_not_on_list(skb);\n\t\tefx_enqueue_skb(tx_queue, skb);\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}