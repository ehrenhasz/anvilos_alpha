{
  "module_name": "tx_tso.c",
  "hash_id": "4a60f1616bfb6ec0af709b9f08ce3e2eaaf5905e603df80a9f0fe8477e804214",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/tx_tso.c",
  "human_readable_source": "\n \n\n#include <linux/pci.h>\n#include <linux/tcp.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/ipv6.h>\n#include <linux/slab.h>\n#include <net/ipv6.h>\n#include <linux/if_ether.h>\n#include <linux/highmem.h>\n#include <linux/moduleparam.h>\n#include <linux/cache.h>\n#include \"net_driver.h\"\n#include \"efx.h\"\n#include \"io.h\"\n#include \"nic.h\"\n#include \"tx.h\"\n#include \"workarounds.h\"\n#include \"ef10_regs.h\"\n\n \n\n#define PTR_DIFF(p1, p2)  ((u8 *)(p1) - (u8 *)(p2))\n\n \nstruct tso_state {\n\t \n\tunsigned int out_len;\n\tunsigned int seqnum;\n\tu16 ipv4_id;\n\tunsigned int packet_space;\n\n\t \n\tdma_addr_t dma_addr;\n\tunsigned int in_len;\n\tunsigned int unmap_len;\n\tdma_addr_t unmap_addr;\n\n\t__be16 protocol;\n\tunsigned int ip_off;\n\tunsigned int tcp_off;\n\tunsigned int header_len;\n\tunsigned int ip_base_len;\n\tdma_addr_t header_dma_addr;\n\tunsigned int header_unmap_len;\n};\n\nstatic inline void prefetch_ptr(struct efx_tx_queue *tx_queue)\n{\n\tunsigned int insert_ptr = efx_tx_queue_get_insert_index(tx_queue);\n\tchar *ptr;\n\n\tptr = (char *) (tx_queue->buffer + insert_ptr);\n\tprefetch(ptr);\n\tprefetch(ptr + 0x80);\n\n\tptr = (char *)(((efx_qword_t *)tx_queue->txd.addr) + insert_ptr);\n\tprefetch(ptr);\n\tprefetch(ptr + 0x80);\n}\n\n \nstatic void efx_tx_queue_insert(struct efx_tx_queue *tx_queue,\n\t\t\t\tdma_addr_t dma_addr, unsigned int len,\n\t\t\t\tstruct efx_tx_buffer **final_buffer)\n{\n\tstruct efx_tx_buffer *buffer;\n\tunsigned int dma_len;\n\n\tEFX_WARN_ON_ONCE_PARANOID(len <= 0);\n\n\twhile (1) {\n\t\tbuffer = efx_tx_queue_get_insert_buffer(tx_queue);\n\t\t++tx_queue->insert_count;\n\n\t\tEFX_WARN_ON_ONCE_PARANOID(tx_queue->insert_count -\n\t\t\t\t\t  tx_queue->read_count >=\n\t\t\t\t\t  tx_queue->efx->txq_entries);\n\n\t\tbuffer->dma_addr = dma_addr;\n\n\t\tdma_len = tx_queue->efx->type->tx_limit_len(tx_queue,\n\t\t\t\tdma_addr, len);\n\n\t\t \n\t\tif (dma_len >= len)\n\t\t\tbreak;\n\n\t\tbuffer->len = dma_len;\n\t\tbuffer->flags = EFX_TX_BUF_CONT;\n\t\tdma_addr += dma_len;\n\t\tlen -= dma_len;\n\t}\n\n\tEFX_WARN_ON_ONCE_PARANOID(!len);\n\tbuffer->len = len;\n\t*final_buffer = buffer;\n}\n\n \nstatic __be16 efx_tso_check_protocol(struct sk_buff *skb)\n{\n\t__be16 protocol = skb->protocol;\n\n\tEFX_WARN_ON_ONCE_PARANOID(((struct ethhdr *)skb->data)->h_proto !=\n\t\t\t\t  protocol);\n\tif (protocol == htons(ETH_P_8021Q)) {\n\t\tstruct vlan_ethhdr *veh = skb_vlan_eth_hdr(skb);\n\n\t\tprotocol = veh->h_vlan_encapsulated_proto;\n\t}\n\n\tif (protocol == htons(ETH_P_IP)) {\n\t\tEFX_WARN_ON_ONCE_PARANOID(ip_hdr(skb)->protocol != IPPROTO_TCP);\n\t} else {\n\t\tEFX_WARN_ON_ONCE_PARANOID(protocol != htons(ETH_P_IPV6));\n\t\tEFX_WARN_ON_ONCE_PARANOID(ipv6_hdr(skb)->nexthdr != NEXTHDR_TCP);\n\t}\n\tEFX_WARN_ON_ONCE_PARANOID((PTR_DIFF(tcp_hdr(skb), skb->data) +\n\t\t\t\t   (tcp_hdr(skb)->doff << 2u)) >\n\t\t\t\t  skb_headlen(skb));\n\n\treturn protocol;\n}\n\n \nstatic int tso_start(struct tso_state *st, struct efx_nic *efx,\n\t\t     struct efx_tx_queue *tx_queue,\n\t\t     const struct sk_buff *skb)\n{\n\tstruct device *dma_dev = &efx->pci_dev->dev;\n\tunsigned int header_len, in_len;\n\tdma_addr_t dma_addr;\n\n\tst->ip_off = skb_network_header(skb) - skb->data;\n\tst->tcp_off = skb_transport_header(skb) - skb->data;\n\theader_len = st->tcp_off + (tcp_hdr(skb)->doff << 2u);\n\tin_len = skb_headlen(skb) - header_len;\n\tst->header_len = header_len;\n\tst->in_len = in_len;\n\tif (st->protocol == htons(ETH_P_IP)) {\n\t\tst->ip_base_len = st->header_len - st->ip_off;\n\t\tst->ipv4_id = ntohs(ip_hdr(skb)->id);\n\t} else {\n\t\tst->ip_base_len = st->header_len - st->tcp_off;\n\t\tst->ipv4_id = 0;\n\t}\n\tst->seqnum = ntohl(tcp_hdr(skb)->seq);\n\n\tEFX_WARN_ON_ONCE_PARANOID(tcp_hdr(skb)->urg);\n\tEFX_WARN_ON_ONCE_PARANOID(tcp_hdr(skb)->syn);\n\tEFX_WARN_ON_ONCE_PARANOID(tcp_hdr(skb)->rst);\n\n\tst->out_len = skb->len - header_len;\n\n\tdma_addr = dma_map_single(dma_dev, skb->data,\n\t\t\t\t  skb_headlen(skb), DMA_TO_DEVICE);\n\tst->header_dma_addr = dma_addr;\n\tst->header_unmap_len = skb_headlen(skb);\n\tst->dma_addr = dma_addr + header_len;\n\tst->unmap_len = 0;\n\n\treturn unlikely(dma_mapping_error(dma_dev, dma_addr)) ? -ENOMEM : 0;\n}\n\nstatic int tso_get_fragment(struct tso_state *st, struct efx_nic *efx,\n\t\t\t    skb_frag_t *frag)\n{\n\tst->unmap_addr = skb_frag_dma_map(&efx->pci_dev->dev, frag, 0,\n\t\t\t\t\t  skb_frag_size(frag), DMA_TO_DEVICE);\n\tif (likely(!dma_mapping_error(&efx->pci_dev->dev, st->unmap_addr))) {\n\t\tst->unmap_len = skb_frag_size(frag);\n\t\tst->in_len = skb_frag_size(frag);\n\t\tst->dma_addr = st->unmap_addr;\n\t\treturn 0;\n\t}\n\treturn -ENOMEM;\n}\n\n\n \nstatic void tso_fill_packet_with_fragment(struct efx_tx_queue *tx_queue,\n\t\t\t\t\t  const struct sk_buff *skb,\n\t\t\t\t\t  struct tso_state *st)\n{\n\tstruct efx_tx_buffer *buffer;\n\tint n;\n\n\tif (st->in_len == 0)\n\t\treturn;\n\tif (st->packet_space == 0)\n\t\treturn;\n\n\tEFX_WARN_ON_ONCE_PARANOID(st->in_len <= 0);\n\tEFX_WARN_ON_ONCE_PARANOID(st->packet_space <= 0);\n\n\tn = min(st->in_len, st->packet_space);\n\n\tst->packet_space -= n;\n\tst->out_len -= n;\n\tst->in_len -= n;\n\n\tefx_tx_queue_insert(tx_queue, st->dma_addr, n, &buffer);\n\n\tif (st->out_len == 0) {\n\t\t \n\t\tbuffer->skb = skb;\n\t\tbuffer->flags = EFX_TX_BUF_SKB;\n\t} else if (st->packet_space != 0) {\n\t\tbuffer->flags = EFX_TX_BUF_CONT;\n\t}\n\n\tif (st->in_len == 0) {\n\t\t \n\t\tbuffer->unmap_len = st->unmap_len;\n\t\tbuffer->dma_offset = buffer->unmap_len - buffer->len;\n\t\tst->unmap_len = 0;\n\t}\n\n\tst->dma_addr += n;\n}\n\n\n#define TCP_FLAGS_OFFSET 13\n\n \nstatic int tso_start_new_packet(struct efx_tx_queue *tx_queue,\n\t\t\t\tconst struct sk_buff *skb,\n\t\t\t\tstruct tso_state *st)\n{\n\tstruct efx_tx_buffer *buffer =\n\t\tefx_tx_queue_get_insert_buffer(tx_queue);\n\tbool is_last = st->out_len <= skb_shinfo(skb)->gso_size;\n\tu8 tcp_flags_mask, tcp_flags;\n\n\tif (!is_last) {\n\t\tst->packet_space = skb_shinfo(skb)->gso_size;\n\t\ttcp_flags_mask = 0x09;  \n\t} else {\n\t\tst->packet_space = st->out_len;\n\t\ttcp_flags_mask = 0x00;\n\t}\n\n\tif (WARN_ON(!st->header_unmap_len))\n\t\treturn -EINVAL;\n\t \n\ttcp_flags = ((u8 *)tcp_hdr(skb))[TCP_FLAGS_OFFSET] & ~tcp_flags_mask;\n\n\tbuffer->flags = EFX_TX_BUF_OPTION;\n\tbuffer->len = 0;\n\tbuffer->unmap_len = 0;\n\tEFX_POPULATE_QWORD_5(buffer->option,\n\t\t\t     ESF_DZ_TX_DESC_IS_OPT, 1,\n\t\t\t     ESF_DZ_TX_OPTION_TYPE,\n\t\t\t     ESE_DZ_TX_OPTION_DESC_TSO,\n\t\t\t     ESF_DZ_TX_TSO_TCP_FLAGS, tcp_flags,\n\t\t\t     ESF_DZ_TX_TSO_IP_ID, st->ipv4_id,\n\t\t\t     ESF_DZ_TX_TSO_TCP_SEQNO, st->seqnum);\n\t++tx_queue->insert_count;\n\n\t \n\tbuffer = efx_tx_queue_get_insert_buffer(tx_queue);\n\tbuffer->dma_addr = st->header_dma_addr;\n\tbuffer->len = st->header_len;\n\tif (is_last) {\n\t\tbuffer->flags = EFX_TX_BUF_CONT | EFX_TX_BUF_MAP_SINGLE;\n\t\tbuffer->unmap_len = st->header_unmap_len;\n\t\tbuffer->dma_offset = 0;\n\t\t \n\t\tst->header_unmap_len = 0;\n\t} else {\n\t\tbuffer->flags = EFX_TX_BUF_CONT;\n\t\tbuffer->unmap_len = 0;\n\t}\n\t++tx_queue->insert_count;\n\n\tst->seqnum += skb_shinfo(skb)->gso_size;\n\n\t \n\t++st->ipv4_id;\n\n\treturn 0;\n}\n\n \nint efx_enqueue_skb_tso(struct efx_tx_queue *tx_queue,\n\t\t\tstruct sk_buff *skb,\n\t\t\tbool *data_mapped)\n{\n\tstruct efx_nic *efx = tx_queue->efx;\n\tint frag_i, rc;\n\tstruct tso_state state;\n\n\tif (tx_queue->tso_version != 1)\n\t\treturn -EINVAL;\n\n\tprefetch(skb->data);\n\n\t \n\tstate.protocol = efx_tso_check_protocol(skb);\n\n\tEFX_WARN_ON_ONCE_PARANOID(tx_queue->write_count != tx_queue->insert_count);\n\n\trc = tso_start(&state, efx, tx_queue, skb);\n\tif (rc)\n\t\tgoto fail;\n\n\tif (likely(state.in_len == 0)) {\n\t\t \n\t\tEFX_WARN_ON_ONCE_PARANOID(skb_shinfo(skb)->nr_frags < 1);\n\t\tfrag_i = 0;\n\t\trc = tso_get_fragment(&state, efx,\n\t\t\t\t      skb_shinfo(skb)->frags + frag_i);\n\t\tif (rc)\n\t\t\tgoto fail;\n\t} else {\n\t\t \n\t\tfrag_i = -1;\n\t}\n\n\trc = tso_start_new_packet(tx_queue, skb, &state);\n\tif (rc)\n\t\tgoto fail;\n\n\tprefetch_ptr(tx_queue);\n\n\twhile (1) {\n\t\ttso_fill_packet_with_fragment(tx_queue, skb, &state);\n\n\t\t \n\t\tif (state.in_len == 0) {\n\t\t\tif (++frag_i >= skb_shinfo(skb)->nr_frags)\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\trc = tso_get_fragment(&state, efx,\n\t\t\t\t\t      skb_shinfo(skb)->frags + frag_i);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t}\n\n\t\t \n\t\tif (state.packet_space == 0) {\n\t\t\trc = tso_start_new_packet(tx_queue, skb, &state);\n\t\t\tif (rc)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\n\t*data_mapped = true;\n\n\treturn 0;\n\nfail:\n\tif (rc == -ENOMEM)\n\t\tnetif_err(efx, tx_err, efx->net_dev,\n\t\t\t  \"Out of memory for TSO headers, or DMA mapping error\\n\");\n\telse\n\t\tnetif_err(efx, tx_err, efx->net_dev, \"TSO failed, rc = %d\\n\", rc);\n\n\t \n\tif (state.unmap_len) {\n\t\tdma_unmap_page(&efx->pci_dev->dev, state.unmap_addr,\n\t\t\t       state.unmap_len, DMA_TO_DEVICE);\n\t}\n\n\t \n\tif (state.header_unmap_len)\n\t\tdma_unmap_single(&efx->pci_dev->dev, state.header_dma_addr,\n\t\t\t\t state.header_unmap_len, DMA_TO_DEVICE);\n\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}