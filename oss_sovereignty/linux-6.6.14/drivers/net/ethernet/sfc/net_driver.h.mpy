{
  "module_name": "net_driver.h",
  "hash_id": "b0b89406773281593c869f204caabbe4c364dedc5affee316dd2ce5f8c9e76ad",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sfc/net_driver.h",
  "human_readable_source": " \n \n\n \n\n#ifndef EFX_NET_DRIVER_H\n#define EFX_NET_DRIVER_H\n\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/if_vlan.h>\n#include <linux/timer.h>\n#include <linux/mdio.h>\n#include <linux/list.h>\n#include <linux/pci.h>\n#include <linux/device.h>\n#include <linux/highmem.h>\n#include <linux/workqueue.h>\n#include <linux/mutex.h>\n#include <linux/rwsem.h>\n#include <linux/vmalloc.h>\n#include <linux/mtd/mtd.h>\n#include <net/busy_poll.h>\n#include <net/xdp.h>\n#include <net/netevent.h>\n\n#include \"enum.h\"\n#include \"bitfield.h\"\n#include \"filter.h\"\n\n \n\n#ifdef DEBUG\n#define EFX_WARN_ON_ONCE_PARANOID(x) WARN_ON_ONCE(x)\n#define EFX_WARN_ON_PARANOID(x) WARN_ON(x)\n#else\n#define EFX_WARN_ON_ONCE_PARANOID(x) do {} while (0)\n#define EFX_WARN_ON_PARANOID(x) do {} while (0)\n#endif\n\n \n\n#define EFX_MAX_CHANNELS 32U\n#define EFX_MAX_RX_QUEUES EFX_MAX_CHANNELS\n#define EFX_EXTRA_CHANNEL_IOV\t0\n#define EFX_EXTRA_CHANNEL_PTP\t1\n#define EFX_EXTRA_CHANNEL_TC\t2\n#define EFX_MAX_EXTRA_CHANNELS\t3U\n\n \n#define EFX_MAX_TX_TC\t\t2\n#define EFX_MAX_CORE_TX_QUEUES\t(EFX_MAX_TX_TC * EFX_MAX_CHANNELS)\n#define EFX_TXQ_TYPE_OUTER_CSUM\t1\t \n#define EFX_TXQ_TYPE_INNER_CSUM\t2\t \n#define EFX_TXQ_TYPES\t\t4\n#define EFX_MAX_TXQ_PER_CHANNEL\t4\n#define EFX_MAX_TX_QUEUES\t(EFX_MAX_TXQ_PER_CHANNEL * EFX_MAX_CHANNELS)\n\n \n#define EFX_MAX_MTU (9 * 1024)\n\n \n#define EFX_MIN_MTU 68\n\n \n#define EFX_TSO2_MAX_HDRLEN\t208\n\n \n#define EFX_RX_USR_BUF_SIZE\t(2048 - 256)\n\n \n#if NET_IP_ALIGN == 0\n#define EFX_RX_BUF_ALIGNMENT\tL1_CACHE_BYTES\n#else\n#define EFX_RX_BUF_ALIGNMENT\t4\n#endif\n\n \n#define EFX_XDP_HEADROOM\t128\n#define EFX_XDP_TAILROOM\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info))\n\n \nstruct efx_ptp_data;\nstruct hwtstamp_config;\n\nstruct efx_self_tests;\n\n \nstruct efx_buffer {\n\tvoid *addr;\n\tdma_addr_t dma_addr;\n\tunsigned int len;\n};\n\n \nstruct efx_tx_buffer {\n\tunion {\n\t\tconst struct sk_buff *skb;\n\t\tstruct xdp_frame *xdpf;\n\t};\n\tunion {\n\t\tefx_qword_t option;     \n\t\tdma_addr_t dma_addr;\n\t};\n\tunsigned short flags;\n\tunsigned short len;\n\tunsigned short unmap_len;\n\tunsigned short dma_offset;\n};\n#define EFX_TX_BUF_CONT\t\t1\t \n#define EFX_TX_BUF_SKB\t\t2\t \n#define EFX_TX_BUF_MAP_SINGLE\t8\t \n#define EFX_TX_BUF_OPTION\t0x10\t \n#define EFX_TX_BUF_XDP\t\t0x20\t \n#define EFX_TX_BUF_TSO_V3\t0x40\t \n#define EFX_TX_BUF_EFV\t\t0x100\t \n\n \nstruct efx_tx_queue {\n\t \n\tstruct efx_nic *efx ____cacheline_aligned_in_smp;\n\tunsigned int queue;\n\tunsigned int label;\n\tunsigned int type;\n\tunsigned int tso_version;\n\tbool tso_encap;\n\tstruct efx_channel *channel;\n\tstruct netdev_queue *core_txq;\n\tstruct efx_tx_buffer *buffer;\n\tstruct efx_buffer *cb_page;\n\tstruct efx_buffer txd;\n\tunsigned int ptr_mask;\n\tvoid __iomem *piobuf;\n\tunsigned int piobuf_offset;\n\tbool initialised;\n\tbool timestamping;\n\tbool xdp_tx;\n\n\t \n\tunsigned int read_count ____cacheline_aligned_in_smp;\n\tunsigned int old_write_count;\n\tunsigned int merge_events;\n\tunsigned int bytes_compl;\n\tunsigned int pkts_compl;\n\tu32 completed_timestamp_major;\n\tu32 completed_timestamp_minor;\n\n\t \n\tunsigned int insert_count ____cacheline_aligned_in_smp;\n\tunsigned int write_count;\n\tunsigned int packet_write_count;\n\tunsigned int old_read_count;\n\tunsigned int tso_bursts;\n\tunsigned int tso_long_headers;\n\tunsigned int tso_packets;\n\tunsigned int tso_fallbacks;\n\tunsigned int pushes;\n\tunsigned int pio_packets;\n\tbool xmit_pending;\n\tunsigned int cb_packets;\n\tunsigned int notify_count;\n\t \n\tunsigned long tx_packets;\n\n\t \n\tunsigned int empty_read_count ____cacheline_aligned_in_smp;\n#define EFX_EMPTY_COUNT_VALID 0x80000000\n\tatomic_t flush_outstanding;\n};\n\n#define EFX_TX_CB_ORDER\t7\n#define EFX_TX_CB_SIZE\t(1 << EFX_TX_CB_ORDER) - NET_IP_ALIGN\n\n \nstruct efx_rx_buffer {\n\tdma_addr_t dma_addr;\n\tstruct page *page;\n\tu16 page_offset;\n\tu16 len;\n\tu16 flags;\n};\n#define EFX_RX_BUF_LAST_IN_PAGE\t0x0001\n#define EFX_RX_PKT_CSUMMED\t0x0002\n#define EFX_RX_PKT_DISCARD\t0x0004\n#define EFX_RX_PKT_TCP\t\t0x0040\n#define EFX_RX_PKT_PREFIX_LEN\t0x0080\t \n#define EFX_RX_PKT_CSUM_LEVEL\t0x0200\n\n \nstruct efx_rx_page_state {\n\tdma_addr_t dma_addr;\n\n\tunsigned int __pad[] ____cacheline_aligned;\n};\n\n \nstruct efx_rx_queue {\n\tstruct efx_nic *efx;\n\tint core_index;\n\tstruct efx_rx_buffer *buffer;\n\tstruct efx_buffer rxd;\n\tunsigned int ptr_mask;\n\tbool refill_enabled;\n\tbool flush_pending;\n\tbool grant_credits;\n\n\tunsigned int added_count;\n\tunsigned int notified_count;\n\tunsigned int granted_count;\n\tunsigned int removed_count;\n\tunsigned int scatter_n;\n\tunsigned int scatter_len;\n\tstruct page **page_ring;\n\tunsigned int page_add;\n\tunsigned int page_remove;\n\tunsigned int page_recycle_count;\n\tunsigned int page_recycle_failed;\n\tunsigned int page_recycle_full;\n\tunsigned int page_ptr_mask;\n\tunsigned int max_fill;\n\tunsigned int fast_fill_trigger;\n\tunsigned int min_fill;\n\tunsigned int min_overfill;\n\tunsigned int recycle_count;\n\tstruct timer_list slow_fill;\n\tunsigned int slow_fill_count;\n\tstruct work_struct grant_work;\n\t \n\tunsigned long rx_packets;\n\tstruct xdp_rxq_info xdp_rxq_info;\n\tbool xdp_rxq_info_valid;\n};\n\nenum efx_sync_events_state {\n\tSYNC_EVENTS_DISABLED = 0,\n\tSYNC_EVENTS_QUIESCENT,\n\tSYNC_EVENTS_REQUESTED,\n\tSYNC_EVENTS_VALID,\n};\n\n \nstruct efx_channel {\n\tstruct efx_nic *efx;\n\tint channel;\n\tconst struct efx_channel_type *type;\n\tbool eventq_init;\n\tbool enabled;\n\tint irq;\n\tunsigned int irq_moderation_us;\n\tstruct net_device *napi_dev;\n\tstruct napi_struct napi_str;\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tunsigned long busy_poll_state;\n#endif\n\tstruct efx_buffer eventq;\n\tunsigned int eventq_mask;\n\tunsigned int eventq_read_ptr;\n\tint event_test_cpu;\n\n\tunsigned int irq_count;\n\tunsigned int irq_mod_score;\n#ifdef CONFIG_RFS_ACCEL\n\tunsigned int rfs_filter_count;\n\tunsigned int rfs_last_expiry;\n\tunsigned int rfs_expire_index;\n\tunsigned int n_rfs_succeeded;\n\tunsigned int n_rfs_failed;\n\tstruct delayed_work filter_work;\n#define RPS_FLOW_ID_INVALID 0xFFFFFFFF\n\tu32 *rps_flow_id;\n#endif\n\n\tunsigned int n_rx_tobe_disc;\n\tunsigned int n_rx_ip_hdr_chksum_err;\n\tunsigned int n_rx_tcp_udp_chksum_err;\n\tunsigned int n_rx_outer_ip_hdr_chksum_err;\n\tunsigned int n_rx_outer_tcp_udp_chksum_err;\n\tunsigned int n_rx_inner_ip_hdr_chksum_err;\n\tunsigned int n_rx_inner_tcp_udp_chksum_err;\n\tunsigned int n_rx_eth_crc_err;\n\tunsigned int n_rx_mcast_mismatch;\n\tunsigned int n_rx_frm_trunc;\n\tunsigned int n_rx_overlength;\n\tunsigned int n_skbuff_leaks;\n\tunsigned int n_rx_nodesc_trunc;\n\tunsigned int n_rx_merge_events;\n\tunsigned int n_rx_merge_packets;\n\tunsigned int n_rx_xdp_drops;\n\tunsigned int n_rx_xdp_bad_drops;\n\tunsigned int n_rx_xdp_tx;\n\tunsigned int n_rx_xdp_redirect;\n\tunsigned int n_rx_mport_bad;\n\n\tunsigned int rx_pkt_n_frags;\n\tunsigned int rx_pkt_index;\n\n\tstruct list_head *rx_list;\n\n\tstruct efx_rx_queue rx_queue;\n\tstruct efx_tx_queue tx_queue[EFX_MAX_TXQ_PER_CHANNEL];\n\tstruct efx_tx_queue *tx_queue_by_type[EFX_TXQ_TYPES];\n\n\tenum efx_sync_events_state sync_events_state;\n\tu32 sync_timestamp_major;\n\tu32 sync_timestamp_minor;\n};\n\n \nstruct efx_msi_context {\n\tstruct efx_nic *efx;\n\tunsigned int index;\n\tchar name[IFNAMSIZ + 6];\n};\n\n \nstruct efx_channel_type {\n\tvoid (*handle_no_channel)(struct efx_nic *);\n\tint (*pre_probe)(struct efx_channel *);\n\tint (*start)(struct efx_channel *);\n\tvoid (*stop)(struct efx_channel *);\n\tvoid (*post_remove)(struct efx_channel *);\n\tvoid (*get_name)(struct efx_channel *, char *buf, size_t len);\n\tstruct efx_channel *(*copy)(const struct efx_channel *);\n\tbool (*receive_skb)(struct efx_channel *, struct sk_buff *);\n\tbool (*receive_raw)(struct efx_rx_queue *, u32);\n\tbool (*want_txqs)(struct efx_channel *);\n\tbool keep_eventq;\n\tbool want_pio;\n};\n\nenum efx_led_mode {\n\tEFX_LED_OFF\t= 0,\n\tEFX_LED_ON\t= 1,\n\tEFX_LED_DEFAULT\t= 2\n};\n\n#define STRING_TABLE_LOOKUP(val, member) \\\n\t((val) < member ## _max) ? member ## _names[val] : \"(invalid)\"\n\nextern const char *const efx_loopback_mode_names[];\nextern const unsigned int efx_loopback_mode_max;\n#define LOOPBACK_MODE(efx) \\\n\tSTRING_TABLE_LOOKUP((efx)->loopback_mode, efx_loopback_mode)\n\nenum efx_int_mode {\n\t \n\tEFX_INT_MODE_MSIX = 0,\n\tEFX_INT_MODE_MSI = 1,\n\tEFX_INT_MODE_LEGACY = 2,\n\tEFX_INT_MODE_MAX\t \n};\n#define EFX_INT_MODE_USE_MSI(x) (((x)->interrupt_mode) <= EFX_INT_MODE_MSI)\n\nenum nic_state {\n\tSTATE_UNINIT = 0,\t \n\tSTATE_PROBED,\t\t \n\tSTATE_NET_DOWN,\t\t \n\tSTATE_NET_UP,\t\t \n\tSTATE_DISABLED,\t\t \n\n\tSTATE_RECOVERY = 0x100, \n\tSTATE_FROZEN = 0x200,\t \n};\n\nstatic inline bool efx_net_active(enum nic_state state)\n{\n\treturn state == STATE_NET_DOWN || state == STATE_NET_UP;\n}\n\nstatic inline bool efx_frozen(enum nic_state state)\n{\n\treturn state & STATE_FROZEN;\n}\n\nstatic inline bool efx_recovering(enum nic_state state)\n{\n\treturn state & STATE_RECOVERY;\n}\n\nstatic inline enum nic_state efx_freeze(enum nic_state state)\n{\n\tWARN_ON(!efx_net_active(state));\n\treturn state | STATE_FROZEN;\n}\n\nstatic inline enum nic_state efx_thaw(enum nic_state state)\n{\n\tWARN_ON(!efx_frozen(state));\n\treturn state & ~STATE_FROZEN;\n}\n\nstatic inline enum nic_state efx_recover(enum nic_state state)\n{\n\tWARN_ON(!efx_net_active(state));\n\treturn state | STATE_RECOVERY;\n}\n\nstatic inline enum nic_state efx_recovered(enum nic_state state)\n{\n\tWARN_ON(!efx_recovering(state));\n\treturn state & ~STATE_RECOVERY;\n}\n\n \nstruct efx_nic;\n\n \n#define EFX_FC_RX\tFLOW_CTRL_RX\n#define EFX_FC_TX\tFLOW_CTRL_TX\n#define EFX_FC_AUTO\t4\n\n \nstruct efx_link_state {\n\tbool up;\n\tbool fd;\n\tu8 fc;\n\tunsigned int speed;\n};\n\nstatic inline bool efx_link_state_equal(const struct efx_link_state *left,\n\t\t\t\t\tconst struct efx_link_state *right)\n{\n\treturn left->up == right->up && left->fd == right->fd &&\n\t\tleft->fc == right->fc && left->speed == right->speed;\n}\n\n \nenum efx_phy_mode {\n\tPHY_MODE_NORMAL\t\t= 0,\n\tPHY_MODE_TX_DISABLED\t= 1,\n\tPHY_MODE_LOW_POWER\t= 2,\n\tPHY_MODE_OFF\t\t= 4,\n\tPHY_MODE_SPECIAL\t= 8,\n};\n\nstatic inline bool efx_phy_mode_disabled(enum efx_phy_mode mode)\n{\n\treturn !!(mode & ~PHY_MODE_TX_DISABLED);\n}\n\n \nstruct efx_hw_stat_desc {\n\tconst char *name;\n\tu16 dma_width;\n\tu16 offset;\n};\n\nstruct vfdi_status;\n\n \n#define EFX_MCDI_RSS_CONTEXT_INVALID\t0xffffffff\n \nstruct efx_rss_context {\n\tstruct list_head list;\n\tu32 context_id;\n\tu32 user_id;\n\tbool rx_hash_udp_4tuple;\n\tu8 rx_hash_key[40];\n\tu32 rx_indir_table[128];\n};\n\n#ifdef CONFIG_RFS_ACCEL\n \n#define EFX_ARFS_FILTER_ID_PENDING\t-1\n#define EFX_ARFS_FILTER_ID_ERROR\t-2\n#define EFX_ARFS_FILTER_ID_REMOVING\t-3\n \nstruct efx_arfs_rule {\n\tstruct hlist_node node;\n\tstruct efx_filter_spec spec;\n\tu16 rxq_index;\n\tu16 arfs_id;\n\ts32 filter_id;\n};\n\n \n#define EFX_ARFS_HASH_TABLE_SIZE\t512\n\n \nstruct efx_async_filter_insertion {\n\tstruct net_device *net_dev;\n\tstruct efx_filter_spec spec;\n\tstruct work_struct work;\n\tu16 rxq_index;\n\tu32 flow_id;\n};\n\n \n#define EFX_RPS_MAX_IN_FLIGHT\t8\n#endif  \n\nenum efx_xdp_tx_queues_mode {\n\tEFX_XDP_TX_QUEUES_DEDICATED,\t \n\tEFX_XDP_TX_QUEUES_SHARED,\t \n\tEFX_XDP_TX_QUEUES_BORROWED\t \n};\n\nstruct efx_mae;\n\n \nstruct efx_nic {\n\t \n\n\tchar name[IFNAMSIZ];\n\tstruct list_head node;\n\tstruct efx_nic *primary;\n\tstruct list_head secondary_list;\n\tstruct pci_dev *pci_dev;\n\tunsigned int port_num;\n\tconst struct efx_nic_type *type;\n\tint legacy_irq;\n\tbool eeh_disabled_legacy_irq;\n\tstruct workqueue_struct *workqueue;\n\tchar workqueue_name[16];\n\tstruct work_struct reset_work;\n\tresource_size_t membase_phys;\n\tvoid __iomem *membase;\n\n\tunsigned int vi_stride;\n\n\tenum efx_int_mode interrupt_mode;\n\tunsigned int timer_quantum_ns;\n\tunsigned int timer_max_ns;\n\tbool irq_rx_adaptive;\n\tbool irqs_hooked;\n\tunsigned int irq_mod_step_us;\n\tunsigned int irq_rx_moderation_us;\n\tu32 msg_enable;\n\n\tenum nic_state state;\n\tunsigned long reset_pending;\n\n\tstruct efx_channel *channel[EFX_MAX_CHANNELS];\n\tstruct efx_msi_context msi_context[EFX_MAX_CHANNELS];\n\tconst struct efx_channel_type *\n\textra_channel_type[EFX_MAX_EXTRA_CHANNELS];\n\tstruct efx_mae *mae;\n\n\tunsigned int xdp_tx_queue_count;\n\tstruct efx_tx_queue **xdp_tx_queues;\n\tenum efx_xdp_tx_queues_mode xdp_txq_queues_mode;\n\n\tunsigned rxq_entries;\n\tunsigned txq_entries;\n\tunsigned int txq_stop_thresh;\n\tunsigned int txq_wake_thresh;\n\n\tunsigned tx_dc_base;\n\tunsigned rx_dc_base;\n\tunsigned sram_lim_qw;\n\n\tunsigned int max_channels;\n\tunsigned int max_vis;\n\tunsigned int max_tx_channels;\n\tunsigned n_channels;\n\tunsigned n_rx_channels;\n\tunsigned rss_spread;\n\tunsigned tx_channel_offset;\n\tunsigned n_tx_channels;\n\tunsigned n_extra_tx_channels;\n\tunsigned int tx_queues_per_channel;\n\tunsigned int n_xdp_channels;\n\tunsigned int xdp_channel_offset;\n\tunsigned int xdp_tx_per_channel;\n\tunsigned int rx_ip_align;\n\tunsigned int rx_dma_len;\n\tunsigned int rx_buffer_order;\n\tunsigned int rx_buffer_truesize;\n\tunsigned int rx_page_buf_step;\n\tunsigned int rx_bufs_per_page;\n\tunsigned int rx_pages_per_batch;\n\tunsigned int rx_prefix_size;\n\tint rx_packet_hash_offset;\n\tint rx_packet_len_offset;\n\tint rx_packet_ts_offset;\n\tbool rx_scatter;\n\tstruct efx_rss_context rss_context;\n\tstruct mutex rss_lock;\n\tu32 vport_id;\n\n\tunsigned int_error_count;\n\tunsigned long int_error_expire;\n\n\tbool must_realloc_vis;\n\tbool irq_soft_enabled;\n\tstruct efx_buffer irq_status;\n\tunsigned irq_zero_count;\n\tunsigned irq_level;\n\tstruct delayed_work selftest_work;\n\n#ifdef CONFIG_SFC_MTD\n\tstruct list_head mtd_list;\n#endif\n\n\tvoid *nic_data;\n\tstruct efx_mcdi_data *mcdi;\n\n\tstruct mutex mac_lock;\n\tstruct work_struct mac_work;\n\tbool port_enabled;\n\n\tbool mc_bist_for_other_fn;\n\tbool port_initialized;\n\tstruct net_device *net_dev;\n\n\tnetdev_features_t fixed_features;\n\n\tu16 num_mac_stats;\n\tstruct efx_buffer stats_buffer;\n\tu64 rx_nodesc_drops_total;\n\tu64 rx_nodesc_drops_while_down;\n\tbool rx_nodesc_drops_prev_state;\n\n\tunsigned int phy_type;\n\tvoid *phy_data;\n\tstruct mdio_if_info mdio;\n\tunsigned int mdio_bus;\n\tenum efx_phy_mode phy_mode;\n\n\t__ETHTOOL_DECLARE_LINK_MODE_MASK(link_advertising);\n\tu32 fec_config;\n\tstruct efx_link_state link_state;\n\tunsigned int n_link_state_changes;\n\n\tu8 wanted_fc;\n\tunsigned fc_disable;\n\n\tatomic_t rx_reset;\n\tenum efx_loopback_mode loopback_mode;\n\tu64 loopback_modes;\n\n\tvoid *loopback_selftest;\n\t \n\tstruct bpf_prog __rcu *xdp_prog;\n\n\tstruct rw_semaphore filter_sem;\n\tvoid *filter_state;\n#ifdef CONFIG_RFS_ACCEL\n\tstruct mutex rps_mutex;\n\tunsigned long rps_slot_map;\n\tstruct efx_async_filter_insertion rps_slot[EFX_RPS_MAX_IN_FLIGHT];\n\tspinlock_t rps_hash_lock;\n\tstruct hlist_head *rps_hash_table;\n\tu32 rps_next_id;\n#endif\n\n\tatomic_t active_queues;\n\tatomic_t rxq_flush_pending;\n\tatomic_t rxq_flush_outstanding;\n\twait_queue_head_t flush_wq;\n\n#ifdef CONFIG_SFC_SRIOV\n\tunsigned vf_count;\n\tunsigned vf_init_count;\n\tunsigned vi_scale;\n#endif\n\tspinlock_t vf_reps_lock;\n\tstruct list_head vf_reps;\n\n\tstruct efx_ptp_data *ptp_data;\n\tbool ptp_warned;\n\n\tchar *vpd_sn;\n\tbool xdp_rxq_info_failed;\n\n\tstruct notifier_block netdev_notifier;\n\tstruct notifier_block netevent_notifier;\n\tstruct efx_tc_state *tc;\n\n\tstruct devlink *devlink;\n\tstruct devlink_port *dl_port;\n\tunsigned int mem_bar;\n\tu32 reg_base;\n\n\t \n\n\tstruct delayed_work monitor_work ____cacheline_aligned_in_smp;\n\tspinlock_t biu_lock;\n\tint last_irq_cpu;\n\tspinlock_t stats_lock;\n\tatomic_t n_rx_noskb_drops;\n};\n\n \nstruct efx_probe_data {\n\tstruct pci_dev *pci_dev;\n\tstruct efx_nic efx;\n};\n\nstatic inline struct efx_nic *efx_netdev_priv(struct net_device *dev)\n{\n\tstruct efx_probe_data **probe_ptr = netdev_priv(dev);\n\tstruct efx_probe_data *probe_data = *probe_ptr;\n\n\treturn &probe_data->efx;\n}\n\nstatic inline int efx_dev_registered(struct efx_nic *efx)\n{\n\treturn efx->net_dev->reg_state == NETREG_REGISTERED;\n}\n\nstatic inline unsigned int efx_port_num(struct efx_nic *efx)\n{\n\treturn efx->port_num;\n}\n\nstruct efx_mtd_partition {\n\tstruct list_head node;\n\tstruct mtd_info mtd;\n\tconst char *dev_type_name;\n\tconst char *type_name;\n\tchar name[IFNAMSIZ + 20];\n};\n\nstruct efx_udp_tunnel {\n#define TUNNEL_ENCAP_UDP_PORT_ENTRY_INVALID\t0xffff\n\tu16 type;  \n\t__be16 port;\n};\n\n \nstruct efx_nic_type {\n\tbool is_vf;\n\tunsigned int (*mem_bar)(struct efx_nic *efx);\n\tunsigned int (*mem_map_size)(struct efx_nic *efx);\n\tint (*probe)(struct efx_nic *efx);\n\tvoid (*remove)(struct efx_nic *efx);\n\tint (*init)(struct efx_nic *efx);\n\tint (*dimension_resources)(struct efx_nic *efx);\n\tvoid (*fini)(struct efx_nic *efx);\n\tvoid (*monitor)(struct efx_nic *efx);\n\tenum reset_type (*map_reset_reason)(enum reset_type reason);\n\tint (*map_reset_flags)(u32 *flags);\n\tint (*reset)(struct efx_nic *efx, enum reset_type method);\n\tint (*probe_port)(struct efx_nic *efx);\n\tvoid (*remove_port)(struct efx_nic *efx);\n\tbool (*handle_global_event)(struct efx_channel *channel, efx_qword_t *);\n\tint (*fini_dmaq)(struct efx_nic *efx);\n\tvoid (*prepare_flr)(struct efx_nic *efx);\n\tvoid (*finish_flr)(struct efx_nic *efx);\n\tsize_t (*describe_stats)(struct efx_nic *efx, u8 *names);\n\tsize_t (*update_stats)(struct efx_nic *efx, u64 *full_stats,\n\t\t\t       struct rtnl_link_stats64 *core_stats);\n\tsize_t (*update_stats_atomic)(struct efx_nic *efx, u64 *full_stats,\n\t\t\t\t      struct rtnl_link_stats64 *core_stats);\n\tvoid (*start_stats)(struct efx_nic *efx);\n\tvoid (*pull_stats)(struct efx_nic *efx);\n\tvoid (*stop_stats)(struct efx_nic *efx);\n\tvoid (*push_irq_moderation)(struct efx_channel *channel);\n\tint (*reconfigure_port)(struct efx_nic *efx);\n\tvoid (*prepare_enable_fc_tx)(struct efx_nic *efx);\n\tint (*reconfigure_mac)(struct efx_nic *efx, bool mtu_only);\n\tbool (*check_mac_fault)(struct efx_nic *efx);\n\tvoid (*get_wol)(struct efx_nic *efx, struct ethtool_wolinfo *wol);\n\tint (*set_wol)(struct efx_nic *efx, u32 type);\n\tvoid (*resume_wol)(struct efx_nic *efx);\n\tvoid (*get_fec_stats)(struct efx_nic *efx,\n\t\t\t      struct ethtool_fec_stats *fec_stats);\n\tunsigned int (*check_caps)(const struct efx_nic *efx,\n\t\t\t\t   u8 flag,\n\t\t\t\t   u32 offset);\n\tint (*test_chip)(struct efx_nic *efx, struct efx_self_tests *tests);\n\tint (*test_nvram)(struct efx_nic *efx);\n\tvoid (*mcdi_request)(struct efx_nic *efx,\n\t\t\t     const efx_dword_t *hdr, size_t hdr_len,\n\t\t\t     const efx_dword_t *sdu, size_t sdu_len);\n\tbool (*mcdi_poll_response)(struct efx_nic *efx);\n\tvoid (*mcdi_read_response)(struct efx_nic *efx, efx_dword_t *pdu,\n\t\t\t\t   size_t pdu_offset, size_t pdu_len);\n\tint (*mcdi_poll_reboot)(struct efx_nic *efx);\n\tvoid (*mcdi_reboot_detected)(struct efx_nic *efx);\n\tvoid (*irq_enable_master)(struct efx_nic *efx);\n\tint (*irq_test_generate)(struct efx_nic *efx);\n\tvoid (*irq_disable_non_ev)(struct efx_nic *efx);\n\tirqreturn_t (*irq_handle_msi)(int irq, void *dev_id);\n\tirqreturn_t (*irq_handle_legacy)(int irq, void *dev_id);\n\tint (*tx_probe)(struct efx_tx_queue *tx_queue);\n\tvoid (*tx_init)(struct efx_tx_queue *tx_queue);\n\tvoid (*tx_remove)(struct efx_tx_queue *tx_queue);\n\tvoid (*tx_write)(struct efx_tx_queue *tx_queue);\n\tnetdev_tx_t (*tx_enqueue)(struct efx_tx_queue *tx_queue, struct sk_buff *skb);\n\tunsigned int (*tx_limit_len)(struct efx_tx_queue *tx_queue,\n\t\t\t\t     dma_addr_t dma_addr, unsigned int len);\n\tint (*rx_push_rss_config)(struct efx_nic *efx, bool user,\n\t\t\t\t  const u32 *rx_indir_table, const u8 *key);\n\tint (*rx_pull_rss_config)(struct efx_nic *efx);\n\tint (*rx_push_rss_context_config)(struct efx_nic *efx,\n\t\t\t\t\t  struct efx_rss_context *ctx,\n\t\t\t\t\t  const u32 *rx_indir_table,\n\t\t\t\t\t  const u8 *key);\n\tint (*rx_pull_rss_context_config)(struct efx_nic *efx,\n\t\t\t\t\t  struct efx_rss_context *ctx);\n\tvoid (*rx_restore_rss_contexts)(struct efx_nic *efx);\n\tint (*rx_probe)(struct efx_rx_queue *rx_queue);\n\tvoid (*rx_init)(struct efx_rx_queue *rx_queue);\n\tvoid (*rx_remove)(struct efx_rx_queue *rx_queue);\n\tvoid (*rx_write)(struct efx_rx_queue *rx_queue);\n\tvoid (*rx_defer_refill)(struct efx_rx_queue *rx_queue);\n\tvoid (*rx_packet)(struct efx_channel *channel);\n\tbool (*rx_buf_hash_valid)(const u8 *prefix);\n\tint (*ev_probe)(struct efx_channel *channel);\n\tint (*ev_init)(struct efx_channel *channel);\n\tvoid (*ev_fini)(struct efx_channel *channel);\n\tvoid (*ev_remove)(struct efx_channel *channel);\n\tint (*ev_process)(struct efx_channel *channel, int quota);\n\tvoid (*ev_read_ack)(struct efx_channel *channel);\n\tvoid (*ev_test_generate)(struct efx_channel *channel);\n\tint (*filter_table_probe)(struct efx_nic *efx);\n\tvoid (*filter_table_restore)(struct efx_nic *efx);\n\tvoid (*filter_table_remove)(struct efx_nic *efx);\n\tvoid (*filter_update_rx_scatter)(struct efx_nic *efx);\n\ts32 (*filter_insert)(struct efx_nic *efx,\n\t\t\t     struct efx_filter_spec *spec, bool replace);\n\tint (*filter_remove_safe)(struct efx_nic *efx,\n\t\t\t\t  enum efx_filter_priority priority,\n\t\t\t\t  u32 filter_id);\n\tint (*filter_get_safe)(struct efx_nic *efx,\n\t\t\t       enum efx_filter_priority priority,\n\t\t\t       u32 filter_id, struct efx_filter_spec *);\n\tint (*filter_clear_rx)(struct efx_nic *efx,\n\t\t\t       enum efx_filter_priority priority);\n\tu32 (*filter_count_rx_used)(struct efx_nic *efx,\n\t\t\t\t    enum efx_filter_priority priority);\n\tu32 (*filter_get_rx_id_limit)(struct efx_nic *efx);\n\ts32 (*filter_get_rx_ids)(struct efx_nic *efx,\n\t\t\t\t enum efx_filter_priority priority,\n\t\t\t\t u32 *buf, u32 size);\n#ifdef CONFIG_RFS_ACCEL\n\tbool (*filter_rfs_expire_one)(struct efx_nic *efx, u32 flow_id,\n\t\t\t\t      unsigned int index);\n#endif\n#ifdef CONFIG_SFC_MTD\n\tint (*mtd_probe)(struct efx_nic *efx);\n\tvoid (*mtd_rename)(struct efx_mtd_partition *part);\n\tint (*mtd_read)(struct mtd_info *mtd, loff_t start, size_t len,\n\t\t\tsize_t *retlen, u8 *buffer);\n\tint (*mtd_erase)(struct mtd_info *mtd, loff_t start, size_t len);\n\tint (*mtd_write)(struct mtd_info *mtd, loff_t start, size_t len,\n\t\t\t size_t *retlen, const u8 *buffer);\n\tint (*mtd_sync)(struct mtd_info *mtd);\n#endif\n\tvoid (*ptp_write_host_time)(struct efx_nic *efx, u32 host_time);\n\tint (*ptp_set_ts_sync_events)(struct efx_nic *efx, bool en, bool temp);\n\tint (*ptp_set_ts_config)(struct efx_nic *efx,\n\t\t\t\t struct hwtstamp_config *init);\n\tint (*sriov_configure)(struct efx_nic *efx, int num_vfs);\n\tint (*vlan_rx_add_vid)(struct efx_nic *efx, __be16 proto, u16 vid);\n\tint (*vlan_rx_kill_vid)(struct efx_nic *efx, __be16 proto, u16 vid);\n\tint (*get_phys_port_id)(struct efx_nic *efx,\n\t\t\t\tstruct netdev_phys_item_id *ppid);\n\tint (*sriov_init)(struct efx_nic *efx);\n\tvoid (*sriov_fini)(struct efx_nic *efx);\n\tbool (*sriov_wanted)(struct efx_nic *efx);\n\tint (*sriov_set_vf_mac)(struct efx_nic *efx, int vf_i, const u8 *mac);\n\tint (*sriov_set_vf_vlan)(struct efx_nic *efx, int vf_i, u16 vlan,\n\t\t\t\t u8 qos);\n\tint (*sriov_set_vf_spoofchk)(struct efx_nic *efx, int vf_i,\n\t\t\t\t     bool spoofchk);\n\tint (*sriov_get_vf_config)(struct efx_nic *efx, int vf_i,\n\t\t\t\t   struct ifla_vf_info *ivi);\n\tint (*sriov_set_vf_link_state)(struct efx_nic *efx, int vf_i,\n\t\t\t\t       int link_state);\n\tint (*vswitching_probe)(struct efx_nic *efx);\n\tint (*vswitching_restore)(struct efx_nic *efx);\n\tvoid (*vswitching_remove)(struct efx_nic *efx);\n\tint (*get_mac_address)(struct efx_nic *efx, unsigned char *perm_addr);\n\tint (*set_mac_address)(struct efx_nic *efx);\n\tu32 (*tso_versions)(struct efx_nic *efx);\n\tint (*udp_tnl_push_ports)(struct efx_nic *efx);\n\tbool (*udp_tnl_has_port)(struct efx_nic *efx, __be16 port);\n\tsize_t (*print_additional_fwver)(struct efx_nic *efx, char *buf,\n\t\t\t\t\t size_t len);\n\tvoid (*sensor_event)(struct efx_nic *efx, efx_qword_t *ev);\n\tunsigned int (*rx_recycle_ring_size)(const struct efx_nic *efx);\n\n\tint revision;\n\tunsigned int txd_ptr_tbl_base;\n\tunsigned int rxd_ptr_tbl_base;\n\tunsigned int buf_tbl_base;\n\tunsigned int evq_ptr_tbl_base;\n\tunsigned int evq_rptr_tbl_base;\n\tu64 max_dma_mask;\n\tunsigned int rx_prefix_size;\n\tunsigned int rx_hash_offset;\n\tunsigned int rx_ts_offset;\n\tunsigned int rx_buffer_padding;\n\tbool can_rx_scatter;\n\tbool always_rx_scatter;\n\tbool option_descriptors;\n\tunsigned int min_interrupt_mode;\n\tunsigned int timer_period_max;\n\tnetdev_features_t offload_features;\n\tint mcdi_max_ver;\n\tunsigned int max_rx_ip_filters;\n\tu32 hwtstamp_filters;\n\tunsigned int rx_hash_key_size;\n};\n\n \n\nstatic inline struct efx_channel *\nefx_get_channel(struct efx_nic *efx, unsigned index)\n{\n\tEFX_WARN_ON_ONCE_PARANOID(index >= efx->n_channels);\n\treturn efx->channel[index];\n}\n\n \n#define efx_for_each_channel(_channel, _efx)\t\t\t\t\\\n\tfor (_channel = (_efx)->channel[0];\t\t\t\t\\\n\t     _channel;\t\t\t\t\t\t\t\\\n\t     _channel = (_channel->channel + 1 < (_efx)->n_channels) ?\t\\\n\t\t     (_efx)->channel[_channel->channel + 1] : NULL)\n\n \n#define efx_for_each_channel_rev(_channel, _efx)\t\t\t\\\n\tfor (_channel = (_efx)->channel[(_efx)->n_channels - 1];\t\\\n\t     _channel;\t\t\t\t\t\t\t\\\n\t     _channel = _channel->channel ?\t\t\t\t\\\n\t\t     (_efx)->channel[_channel->channel - 1] : NULL)\n\nstatic inline struct efx_channel *\nefx_get_tx_channel(struct efx_nic *efx, unsigned int index)\n{\n\tEFX_WARN_ON_ONCE_PARANOID(index >= efx->n_tx_channels);\n\treturn efx->channel[efx->tx_channel_offset + index];\n}\n\nstatic inline struct efx_channel *\nefx_get_xdp_channel(struct efx_nic *efx, unsigned int index)\n{\n\tEFX_WARN_ON_ONCE_PARANOID(index >= efx->n_xdp_channels);\n\treturn efx->channel[efx->xdp_channel_offset + index];\n}\n\nstatic inline bool efx_channel_is_xdp_tx(struct efx_channel *channel)\n{\n\treturn channel->channel - channel->efx->xdp_channel_offset <\n\t       channel->efx->n_xdp_channels;\n}\n\nstatic inline bool efx_channel_has_tx_queues(struct efx_channel *channel)\n{\n\treturn channel && channel->channel >= channel->efx->tx_channel_offset;\n}\n\nstatic inline unsigned int efx_channel_num_tx_queues(struct efx_channel *channel)\n{\n\tif (efx_channel_is_xdp_tx(channel))\n\t\treturn channel->efx->xdp_tx_per_channel;\n\treturn channel->efx->tx_queues_per_channel;\n}\n\nstatic inline struct efx_tx_queue *\nefx_channel_get_tx_queue(struct efx_channel *channel, unsigned int type)\n{\n\tEFX_WARN_ON_ONCE_PARANOID(type >= EFX_TXQ_TYPES);\n\treturn channel->tx_queue_by_type[type];\n}\n\nstatic inline struct efx_tx_queue *\nefx_get_tx_queue(struct efx_nic *efx, unsigned int index, unsigned int type)\n{\n\tstruct efx_channel *channel = efx_get_tx_channel(efx, index);\n\n\treturn efx_channel_get_tx_queue(channel, type);\n}\n\n \n#define efx_for_each_channel_tx_queue(_tx_queue, _channel)\t\t\\\n\tif (!efx_channel_has_tx_queues(_channel))\t\t\t\\\n\t\t;\t\t\t\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tfor (_tx_queue = (_channel)->tx_queue;\t\t\t\\\n\t\t     _tx_queue < (_channel)->tx_queue +\t\t\t\\\n\t\t\t\t efx_channel_num_tx_queues(_channel);\t\t\\\n\t\t     _tx_queue++)\n\nstatic inline bool efx_channel_has_rx_queue(struct efx_channel *channel)\n{\n\treturn channel->rx_queue.core_index >= 0;\n}\n\nstatic inline struct efx_rx_queue *\nefx_channel_get_rx_queue(struct efx_channel *channel)\n{\n\tEFX_WARN_ON_ONCE_PARANOID(!efx_channel_has_rx_queue(channel));\n\treturn &channel->rx_queue;\n}\n\n \n#define efx_for_each_channel_rx_queue(_rx_queue, _channel)\t\t\\\n\tif (!efx_channel_has_rx_queue(_channel))\t\t\t\\\n\t\t;\t\t\t\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tfor (_rx_queue = &(_channel)->rx_queue;\t\t\t\\\n\t\t     _rx_queue;\t\t\t\t\t\t\\\n\t\t     _rx_queue = NULL)\n\nstatic inline struct efx_channel *\nefx_rx_queue_channel(struct efx_rx_queue *rx_queue)\n{\n\treturn container_of(rx_queue, struct efx_channel, rx_queue);\n}\n\nstatic inline int efx_rx_queue_index(struct efx_rx_queue *rx_queue)\n{\n\treturn efx_rx_queue_channel(rx_queue)->channel;\n}\n\n \nstatic inline struct efx_rx_buffer *efx_rx_buffer(struct efx_rx_queue *rx_queue,\n\t\t\t\t\t\t  unsigned int index)\n{\n\treturn &rx_queue->buffer[index];\n}\n\nstatic inline struct efx_rx_buffer *\nefx_rx_buf_next(struct efx_rx_queue *rx_queue, struct efx_rx_buffer *rx_buf)\n{\n\tif (unlikely(rx_buf == efx_rx_buffer(rx_queue, rx_queue->ptr_mask)))\n\t\treturn efx_rx_buffer(rx_queue, 0);\n\telse\n\t\treturn rx_buf + 1;\n}\n\n \n#define EFX_FRAME_PAD\t16\n#define EFX_MAX_FRAME_LEN(mtu) \\\n\t(ALIGN(((mtu) + ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN + EFX_FRAME_PAD), 8))\n\nstatic inline bool efx_xmit_with_hwtstamp(struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP;\n}\nstatic inline void efx_xmit_hwtstamp_pending(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n}\n\n \nstatic inline unsigned int\nefx_channel_tx_fill_level(struct efx_channel *channel)\n{\n\tstruct efx_tx_queue *tx_queue;\n\tunsigned int fill_level = 0;\n\n\tefx_for_each_channel_tx_queue(tx_queue, channel)\n\t\tfill_level = max(fill_level,\n\t\t\t\t tx_queue->insert_count - tx_queue->read_count);\n\n\treturn fill_level;\n}\n\n \nstatic inline unsigned int\nefx_channel_tx_old_fill_level(struct efx_channel *channel)\n{\n\tstruct efx_tx_queue *tx_queue;\n\tunsigned int fill_level = 0;\n\n\tefx_for_each_channel_tx_queue(tx_queue, channel)\n\t\tfill_level = max(fill_level,\n\t\t\t\t tx_queue->insert_count - tx_queue->old_read_count);\n\n\treturn fill_level;\n}\n\n \nstatic inline netdev_features_t efx_supported_features(const struct efx_nic *efx)\n{\n\tconst struct net_device *net_dev = efx->net_dev;\n\n\treturn net_dev->features | net_dev->hw_features;\n}\n\n \nstatic inline unsigned int\nefx_tx_queue_get_insert_index(const struct efx_tx_queue *tx_queue)\n{\n\treturn tx_queue->insert_count & tx_queue->ptr_mask;\n}\n\n \nstatic inline struct efx_tx_buffer *\n__efx_tx_queue_get_insert_buffer(const struct efx_tx_queue *tx_queue)\n{\n\treturn &tx_queue->buffer[efx_tx_queue_get_insert_index(tx_queue)];\n}\n\n \nstatic inline struct efx_tx_buffer *\nefx_tx_queue_get_insert_buffer(const struct efx_tx_queue *tx_queue)\n{\n\tstruct efx_tx_buffer *buffer =\n\t\t__efx_tx_queue_get_insert_buffer(tx_queue);\n\n\tEFX_WARN_ON_ONCE_PARANOID(buffer->len);\n\tEFX_WARN_ON_ONCE_PARANOID(buffer->flags);\n\tEFX_WARN_ON_ONCE_PARANOID(buffer->unmap_len);\n\n\treturn buffer;\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}