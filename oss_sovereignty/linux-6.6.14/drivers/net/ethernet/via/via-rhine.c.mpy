{
  "module_name": "via-rhine.c",
  "hash_id": "456cecfd328513015674efc42ec292390158051b192725a90bfbef6d6f68bf7d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/via/via-rhine.c",
  "human_readable_source": " \n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#define DRV_NAME\t\"via-rhine\"\n\n#include <linux/types.h>\n\n \nstatic int debug = 0;\n#define RHINE_MSG_DEFAULT \\\n        (0x0000)\n\n \n#if defined(__alpha__) || defined(__arm__) || defined(__hppa__) || \\\n\tdefined(CONFIG_SPARC) || defined(__ia64__) ||\t\t   \\\n\tdefined(__sh__) || defined(__mips__)\nstatic int rx_copybreak = 1518;\n#else\nstatic int rx_copybreak;\n#endif\n\n \nstatic bool avoid_D3;\n\n \n\n \nstatic const int multicast_filter_limit = 32;\n\n\n \n\n \n#define TX_RING_SIZE\t64\n#define TX_QUEUE_LEN\t(TX_RING_SIZE - 6)\t \n#define RX_RING_SIZE\t64\n\n \n\n \n#define TX_TIMEOUT\t(2*HZ)\n\n#define PKT_BUF_SZ\t1536\t \n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/timer.h>\n#include <linux/errno.h>\n#include <linux/ioport.h>\n#include <linux/interrupt.h>\n#include <linux/pci.h>\n#include <linux/of.h>\n#include <linux/of_irq.h>\n#include <linux/platform_device.h>\n#include <linux/dma-mapping.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/skbuff.h>\n#include <linux/init.h>\n#include <linux/delay.h>\n#include <linux/mii.h>\n#include <linux/ethtool.h>\n#include <linux/crc32.h>\n#include <linux/if_vlan.h>\n#include <linux/bitops.h>\n#include <linux/workqueue.h>\n#include <asm/processor.h>\t \n#include <asm/io.h>\n#include <asm/irq.h>\n#include <linux/uaccess.h>\n#include <linux/dmi.h>\n\nMODULE_AUTHOR(\"Donald Becker <becker@scyld.com>\");\nMODULE_DESCRIPTION(\"VIA Rhine PCI Fast Ethernet driver\");\nMODULE_LICENSE(\"GPL\");\n\nmodule_param(debug, int, 0);\nmodule_param(rx_copybreak, int, 0);\nmodule_param(avoid_D3, bool, 0);\nMODULE_PARM_DESC(debug, \"VIA Rhine debug message flags\");\nMODULE_PARM_DESC(rx_copybreak, \"VIA Rhine copy breakpoint for copy-only-tiny-frames\");\nMODULE_PARM_DESC(avoid_D3, \"Avoid power state D3 (work-around for broken BIOSes)\");\n\n#define MCAM_SIZE\t32\n#define VCAM_SIZE\t32\n\n \n\n\n \n\nenum rhine_revs {\n\tVT86C100A\t= 0x00,\n\tVTunknown0\t= 0x20,\n\tVT6102\t\t= 0x40,\n\tVT8231\t\t= 0x50,\t \n\tVT8233\t\t= 0x60,\t \n\tVT8235\t\t= 0x74,\t \n\tVT8237\t\t= 0x78,\t \n\tVT8251\t\t= 0x7C,\t \n\tVT6105\t\t= 0x80,\n\tVT6105_B0\t= 0x83,\n\tVT6105L\t\t= 0x8A,\n\tVT6107\t\t= 0x8C,\n\tVTunknown2\t= 0x8E,\n\tVT6105M\t\t= 0x90,\t \n};\n\nenum rhine_quirks {\n\trqWOL\t\t= 0x0001,\t \n\trqForceReset\t= 0x0002,\n\trq6patterns\t= 0x0040,\t \n\trqStatusWBRace\t= 0x0080,\t \n\trqRhineI\t= 0x0100,\t \n\trqIntPHY\t= 0x0200,\t \n\trqMgmt\t\t= 0x0400,\t \n\trqNeedEnMMIO\t= 0x0800,\t \n};\n \n\n \n#define IOSYNC\tdo { ioread8(ioaddr + StationAddr); } while (0)\n\nstatic const struct pci_device_id rhine_pci_tbl[] = {\n\t{ 0x1106, 0x3043, PCI_ANY_ID, PCI_ANY_ID, },\t \n\t{ 0x1106, 0x3065, PCI_ANY_ID, PCI_ANY_ID, },\t \n\t{ 0x1106, 0x3106, PCI_ANY_ID, PCI_ANY_ID, },\t \n\t{ 0x1106, 0x3053, PCI_ANY_ID, PCI_ANY_ID, },\t \n\t{ }\t \n};\nMODULE_DEVICE_TABLE(pci, rhine_pci_tbl);\n\n \nstatic u32 vt8500_quirks = rqWOL | rqForceReset | rq6patterns;\nstatic const struct of_device_id rhine_of_tbl[] = {\n\t{ .compatible = \"via,vt8500-rhine\", .data = &vt8500_quirks },\n\t{ }\t \n};\nMODULE_DEVICE_TABLE(of, rhine_of_tbl);\n\n \nenum register_offsets {\n\tStationAddr=0x00, RxConfig=0x06, TxConfig=0x07, ChipCmd=0x08,\n\tChipCmd1=0x09, TQWake=0x0A,\n\tIntrStatus=0x0C, IntrEnable=0x0E,\n\tMulticastFilter0=0x10, MulticastFilter1=0x14,\n\tRxRingPtr=0x18, TxRingPtr=0x1C, GFIFOTest=0x54,\n\tMIIPhyAddr=0x6C, MIIStatus=0x6D, PCIBusConfig=0x6E, PCIBusConfig1=0x6F,\n\tMIICmd=0x70, MIIRegAddr=0x71, MIIData=0x72, MACRegEEcsr=0x74,\n\tConfigA=0x78, ConfigB=0x79, ConfigC=0x7A, ConfigD=0x7B,\n\tRxMissed=0x7C, RxCRCErrs=0x7E, MiscCmd=0x81,\n\tStickyHW=0x83, IntrStatus2=0x84,\n\tCamMask=0x88, CamCon=0x92, CamAddr=0x93,\n\tWOLcrSet=0xA0, PwcfgSet=0xA1, WOLcgSet=0xA3, WOLcrClr=0xA4,\n\tWOLcrClr1=0xA6, WOLcgClr=0xA7,\n\tPwrcsrSet=0xA8, PwrcsrSet1=0xA9, PwrcsrClr=0xAC, PwrcsrClr1=0xAD,\n};\n\n \nenum backoff_bits {\n\tBackOptional=0x01, BackModify=0x02,\n\tBackCaptureEffect=0x04, BackRandom=0x08\n};\n\n \nenum tcr_bits {\n\tTCR_PQEN=0x01,\n\tTCR_LB0=0x02,\t\t \n\tTCR_LB1=0x04,\t\t \n\tTCR_OFSET=0x08,\n\tTCR_RTGOPT=0x10,\n\tTCR_RTFT0=0x20,\n\tTCR_RTFT1=0x40,\n\tTCR_RTSF=0x80,\n};\n\n \nenum camcon_bits {\n\tCAMC_CAMEN=0x01,\n\tCAMC_VCAMSL=0x02,\n\tCAMC_CAMWR=0x04,\n\tCAMC_CAMRD=0x08,\n};\n\n \nenum bcr1_bits {\n\tBCR1_POT0=0x01,\n\tBCR1_POT1=0x02,\n\tBCR1_POT2=0x04,\n\tBCR1_CTFT0=0x08,\n\tBCR1_CTFT1=0x10,\n\tBCR1_CTSF=0x20,\n\tBCR1_TXQNOBK=0x40,\t \n\tBCR1_VIDFR=0x80,\t \n\tBCR1_MED0=0x40,\t\t \n\tBCR1_MED1=0x80,\t\t \n};\n\n \nstatic const int mmio_verify_registers[] = {\n\tRxConfig, TxConfig, IntrEnable, ConfigA, ConfigB, ConfigC, ConfigD,\n\t0\n};\n\n \nenum intr_status_bits {\n\tIntrRxDone\t= 0x0001,\n\tIntrTxDone\t= 0x0002,\n\tIntrRxErr\t= 0x0004,\n\tIntrTxError\t= 0x0008,\n\tIntrRxEmpty\t= 0x0020,\n\tIntrPCIErr\t= 0x0040,\n\tIntrStatsMax\t= 0x0080,\n\tIntrRxEarly\t= 0x0100,\n\tIntrTxUnderrun\t= 0x0210,\n\tIntrRxOverflow\t= 0x0400,\n\tIntrRxDropped\t= 0x0800,\n\tIntrRxNoBuf\t= 0x1000,\n\tIntrTxAborted\t= 0x2000,\n\tIntrLinkChange\t= 0x4000,\n\tIntrRxWakeUp\t= 0x8000,\n\tIntrTxDescRace\t\t= 0x080000,\t \n\tIntrNormalSummary\t= IntrRxDone | IntrTxDone,\n\tIntrTxErrSummary\t= IntrTxDescRace | IntrTxAborted | IntrTxError |\n\t\t\t\t  IntrTxUnderrun,\n};\n\n \nenum wol_bits {\n\tWOLucast\t= 0x10,\n\tWOLmagic\t= 0x20,\n\tWOLbmcast\t= 0x30,\n\tWOLlnkon\t= 0x40,\n\tWOLlnkoff\t= 0x80,\n};\n\n \nstruct rx_desc {\n\t__le32 rx_status;\n\t__le32 desc_length;  \n\t__le32 addr;\n\t__le32 next_desc;\n};\nstruct tx_desc {\n\t__le32 tx_status;\n\t__le32 desc_length;  \n\t__le32 addr;\n\t__le32 next_desc;\n};\n\n \n#define TXDESC\t\t0x00e08000\n\nenum rx_status_bits {\n\tRxOK=0x8000, RxWholePkt=0x0300, RxErr=0x008F\n};\n\n \nenum desc_status_bits {\n\tDescOwn=0x80000000\n};\n\n \nenum desc_length_bits {\n\tDescTag=0x00010000\n};\n\n \nenum chip_cmd_bits {\n\tCmdInit=0x01, CmdStart=0x02, CmdStop=0x04, CmdRxOn=0x08,\n\tCmdTxOn=0x10, Cmd1TxDemand=0x20, CmdRxDemand=0x40,\n\tCmd1EarlyRx=0x01, Cmd1EarlyTx=0x02, Cmd1FDuplex=0x04,\n\tCmd1NoTxPoll=0x08, Cmd1Reset=0x80,\n};\n\nstruct rhine_stats {\n\tu64\t\tpackets;\n\tu64\t\tbytes;\n\tstruct u64_stats_sync syncp;\n};\n\nstruct rhine_private {\n\t \n\tunsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];\n\n\t \n\tstruct rx_desc *rx_ring;\n\tstruct tx_desc *tx_ring;\n\tdma_addr_t rx_ring_dma;\n\tdma_addr_t tx_ring_dma;\n\n\t \n\tstruct sk_buff *rx_skbuff[RX_RING_SIZE];\n\tdma_addr_t rx_skbuff_dma[RX_RING_SIZE];\n\n\t \n\tstruct sk_buff *tx_skbuff[TX_RING_SIZE];\n\tdma_addr_t tx_skbuff_dma[TX_RING_SIZE];\n\n\t \n\tunsigned char *tx_buf[TX_RING_SIZE];\n\tunsigned char *tx_bufs;\n\tdma_addr_t tx_bufs_dma;\n\n\tint irq;\n\tlong pioaddr;\n\tstruct net_device *dev;\n\tstruct napi_struct napi;\n\tspinlock_t lock;\n\tstruct mutex task_lock;\n\tbool task_enable;\n\tstruct work_struct slow_event_task;\n\tstruct work_struct reset_task;\n\n\tu32 msg_enable;\n\n\t \n\tu32 quirks;\n\tunsigned int cur_rx;\n\tunsigned int cur_tx, dirty_tx;\n\tunsigned int rx_buf_sz;\t\t \n\tstruct rhine_stats rx_stats;\n\tstruct rhine_stats tx_stats;\n\tu8 wolopts;\n\n\tu8 tx_thresh, rx_thresh;\n\n\tstruct mii_if_info mii_if;\n\tvoid __iomem *base;\n};\n\n#define BYTE_REG_BITS_ON(x, p)      do { iowrite8((ioread8((p))|(x)), (p)); } while (0)\n#define WORD_REG_BITS_ON(x, p)      do { iowrite16((ioread16((p))|(x)), (p)); } while (0)\n#define DWORD_REG_BITS_ON(x, p)     do { iowrite32((ioread32((p))|(x)), (p)); } while (0)\n\n#define BYTE_REG_BITS_IS_ON(x, p)   (ioread8((p)) & (x))\n#define WORD_REG_BITS_IS_ON(x, p)   (ioread16((p)) & (x))\n#define DWORD_REG_BITS_IS_ON(x, p)  (ioread32((p)) & (x))\n\n#define BYTE_REG_BITS_OFF(x, p)     do { iowrite8(ioread8((p)) & (~(x)), (p)); } while (0)\n#define WORD_REG_BITS_OFF(x, p)     do { iowrite16(ioread16((p)) & (~(x)), (p)); } while (0)\n#define DWORD_REG_BITS_OFF(x, p)    do { iowrite32(ioread32((p)) & (~(x)), (p)); } while (0)\n\n#define BYTE_REG_BITS_SET(x, m, p)   do { iowrite8((ioread8((p)) & (~(m)))|(x), (p)); } while (0)\n#define WORD_REG_BITS_SET(x, m, p)   do { iowrite16((ioread16((p)) & (~(m)))|(x), (p)); } while (0)\n#define DWORD_REG_BITS_SET(x, m, p)  do { iowrite32((ioread32((p)) & (~(m)))|(x), (p)); } while (0)\n\n\nstatic int  mdio_read(struct net_device *dev, int phy_id, int location);\nstatic void mdio_write(struct net_device *dev, int phy_id, int location, int value);\nstatic int  rhine_open(struct net_device *dev);\nstatic void rhine_reset_task(struct work_struct *work);\nstatic void rhine_slow_event_task(struct work_struct *work);\nstatic void rhine_tx_timeout(struct net_device *dev, unsigned int txqueue);\nstatic netdev_tx_t rhine_start_tx(struct sk_buff *skb,\n\t\t\t\t  struct net_device *dev);\nstatic irqreturn_t rhine_interrupt(int irq, void *dev_instance);\nstatic void rhine_tx(struct net_device *dev);\nstatic int rhine_rx(struct net_device *dev, int limit);\nstatic void rhine_set_rx_mode(struct net_device *dev);\nstatic void rhine_get_stats64(struct net_device *dev,\n\t\t\t      struct rtnl_link_stats64 *stats);\nstatic int netdev_ioctl(struct net_device *dev, struct ifreq *rq, int cmd);\nstatic const struct ethtool_ops netdev_ethtool_ops;\nstatic int  rhine_close(struct net_device *dev);\nstatic int rhine_vlan_rx_add_vid(struct net_device *dev,\n\t\t\t\t __be16 proto, u16 vid);\nstatic int rhine_vlan_rx_kill_vid(struct net_device *dev,\n\t\t\t\t  __be16 proto, u16 vid);\nstatic void rhine_restart_tx(struct net_device *dev);\n\nstatic void rhine_wait_bit(struct rhine_private *rp, u8 reg, u8 mask, bool low)\n{\n\tvoid __iomem *ioaddr = rp->base;\n\tint i;\n\n\tfor (i = 0; i < 1024; i++) {\n\t\tbool has_mask_bits = !!(ioread8(ioaddr + reg) & mask);\n\n\t\tif (low ^ has_mask_bits)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\tif (i > 64) {\n\t\tnetif_dbg(rp, hw, rp->dev, \"%s bit wait (%02x/%02x) cycle \"\n\t\t\t  \"count: %04d\\n\", low ? \"low\" : \"high\", reg, mask, i);\n\t}\n}\n\nstatic void rhine_wait_bit_high(struct rhine_private *rp, u8 reg, u8 mask)\n{\n\trhine_wait_bit(rp, reg, mask, false);\n}\n\nstatic void rhine_wait_bit_low(struct rhine_private *rp, u8 reg, u8 mask)\n{\n\trhine_wait_bit(rp, reg, mask, true);\n}\n\nstatic u32 rhine_get_events(struct rhine_private *rp)\n{\n\tvoid __iomem *ioaddr = rp->base;\n\tu32 intr_status;\n\n\tintr_status = ioread16(ioaddr + IntrStatus);\n\t \n\tif (rp->quirks & rqStatusWBRace)\n\t\tintr_status |= ioread8(ioaddr + IntrStatus2) << 16;\n\treturn intr_status;\n}\n\nstatic void rhine_ack_events(struct rhine_private *rp, u32 mask)\n{\n\tvoid __iomem *ioaddr = rp->base;\n\n\tif (rp->quirks & rqStatusWBRace)\n\t\tiowrite8(mask >> 16, ioaddr + IntrStatus2);\n\tiowrite16(mask, ioaddr + IntrStatus);\n}\n\n \nstatic void rhine_power_init(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\tu16 wolstat;\n\n\tif (rp->quirks & rqWOL) {\n\t\t \n\t\tiowrite8(ioread8(ioaddr + StickyHW) & 0xFC, ioaddr + StickyHW);\n\n\t\t \n\t\tiowrite8(0x80, ioaddr + WOLcgClr);\n\n\t\t \n\t\tiowrite8(0xFF, ioaddr + WOLcrClr);\n\t\t \n\t\tif (rp->quirks & rq6patterns)\n\t\t\tiowrite8(0x03, ioaddr + WOLcrClr1);\n\n\t\t \n\t\twolstat = ioread8(ioaddr + PwrcsrSet);\n\t\tif (rp->quirks & rq6patterns)\n\t\t\twolstat |= (ioread8(ioaddr + PwrcsrSet1) & 0x03) << 8;\n\n\t\t \n\t\tiowrite8(0xFF, ioaddr + PwrcsrClr);\n\t\tif (rp->quirks & rq6patterns)\n\t\t\tiowrite8(0x03, ioaddr + PwrcsrClr1);\n\n\t\tif (wolstat) {\n\t\t\tchar *reason;\n\t\t\tswitch (wolstat) {\n\t\t\tcase WOLmagic:\n\t\t\t\treason = \"Magic packet\";\n\t\t\t\tbreak;\n\t\t\tcase WOLlnkon:\n\t\t\t\treason = \"Link went up\";\n\t\t\t\tbreak;\n\t\t\tcase WOLlnkoff:\n\t\t\t\treason = \"Link went down\";\n\t\t\t\tbreak;\n\t\t\tcase WOLucast:\n\t\t\t\treason = \"Unicast packet\";\n\t\t\t\tbreak;\n\t\t\tcase WOLbmcast:\n\t\t\t\treason = \"Multicast/broadcast packet\";\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\treason = \"Unknown\";\n\t\t\t}\n\t\t\tnetdev_info(dev, \"Woke system up. Reason: %s\\n\",\n\t\t\t\t    reason);\n\t\t}\n\t}\n}\n\nstatic void rhine_chip_reset(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\tu8 cmd1;\n\n\tiowrite8(Cmd1Reset, ioaddr + ChipCmd1);\n\tIOSYNC;\n\n\tif (ioread8(ioaddr + ChipCmd1) & Cmd1Reset) {\n\t\tnetdev_info(dev, \"Reset not complete yet. Trying harder.\\n\");\n\n\t\t \n\t\tif (rp->quirks & rqForceReset)\n\t\t\tiowrite8(0x40, ioaddr + MiscCmd);\n\n\t\t \n\t\trhine_wait_bit_low(rp, ChipCmd1, Cmd1Reset);\n\t}\n\n\tcmd1 = ioread8(ioaddr + ChipCmd1);\n\tnetif_info(rp, hw, dev, \"Reset %s\\n\", (cmd1 & Cmd1Reset) ?\n\t\t   \"failed\" : \"succeeded\");\n}\n\nstatic void enable_mmio(long pioaddr, u32 quirks)\n{\n\tint n;\n\n\tif (quirks & rqNeedEnMMIO) {\n\t\tif (quirks & rqRhineI) {\n\t\t\t \n\t\t\tn = inb(pioaddr + ConfigA) | 0x20;\n\t\t\toutb(n, pioaddr + ConfigA);\n\t\t} else {\n\t\t\tn = inb(pioaddr + ConfigD) | 0x80;\n\t\t\toutb(n, pioaddr + ConfigD);\n\t\t}\n\t}\n}\n\nstatic inline int verify_mmio(struct device *hwdev,\n\t\t\t      long pioaddr,\n\t\t\t      void __iomem *ioaddr,\n\t\t\t      u32 quirks)\n{\n\tif (quirks & rqNeedEnMMIO) {\n\t\tint i = 0;\n\n\t\t \n\t\twhile (mmio_verify_registers[i]) {\n\t\t\tint reg = mmio_verify_registers[i++];\n\t\t\tunsigned char a = inb(pioaddr+reg);\n\t\t\tunsigned char b = readb(ioaddr+reg);\n\n\t\t\tif (a != b) {\n\t\t\t\tdev_err(hwdev,\n\t\t\t\t\t\"MMIO do not match PIO [%02x] (%02x != %02x)\\n\",\n\t\t\t\t\treg, a, b);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic void rhine_reload_eeprom(long pioaddr, struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\tint i;\n\n\toutb(0x20, pioaddr + MACRegEEcsr);\n\tfor (i = 0; i < 1024; i++) {\n\t\tif (!(inb(pioaddr + MACRegEEcsr) & 0x20))\n\t\t\tbreak;\n\t}\n\tif (i > 512)\n\t\tpr_info(\"%4d cycles used @ %s:%d\\n\", i, __func__, __LINE__);\n\n\t \n\tenable_mmio(pioaddr, rp->quirks);\n\n\t \n\tif (rp->quirks & rqWOL)\n\t\tiowrite8(ioread8(ioaddr + ConfigA) & 0xFC, ioaddr + ConfigA);\n\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void rhine_poll(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tconst int irq = rp->irq;\n\n\tdisable_irq(irq);\n\trhine_interrupt(irq, dev);\n\tenable_irq(irq);\n}\n#endif\n\nstatic void rhine_kick_tx_threshold(struct rhine_private *rp)\n{\n\tif (rp->tx_thresh < 0xe0) {\n\t\tvoid __iomem *ioaddr = rp->base;\n\n\t\trp->tx_thresh += 0x20;\n\t\tBYTE_REG_BITS_SET(rp->tx_thresh, 0x80, ioaddr + TxConfig);\n\t}\n}\n\nstatic void rhine_tx_err(struct rhine_private *rp, u32 status)\n{\n\tstruct net_device *dev = rp->dev;\n\n\tif (status & IntrTxAborted) {\n\t\tnetif_info(rp, tx_err, dev,\n\t\t\t   \"Abort %08x, frame dropped\\n\", status);\n\t}\n\n\tif (status & IntrTxUnderrun) {\n\t\trhine_kick_tx_threshold(rp);\n\t\tnetif_info(rp, tx_err ,dev, \"Transmitter underrun, \"\n\t\t\t   \"Tx threshold now %02x\\n\", rp->tx_thresh);\n\t}\n\n\tif (status & IntrTxDescRace)\n\t\tnetif_info(rp, tx_err, dev, \"Tx descriptor write-back race\\n\");\n\n\tif ((status & IntrTxError) &&\n\t    (status & (IntrTxAborted | IntrTxUnderrun | IntrTxDescRace)) == 0) {\n\t\trhine_kick_tx_threshold(rp);\n\t\tnetif_info(rp, tx_err, dev, \"Unspecified error. \"\n\t\t\t   \"Tx threshold now %02x\\n\", rp->tx_thresh);\n\t}\n\n\trhine_restart_tx(dev);\n}\n\nstatic void rhine_update_rx_crc_and_missed_errord(struct rhine_private *rp)\n{\n\tvoid __iomem *ioaddr = rp->base;\n\tstruct net_device_stats *stats = &rp->dev->stats;\n\n\tstats->rx_crc_errors    += ioread16(ioaddr + RxCRCErrs);\n\tstats->rx_missed_errors += ioread16(ioaddr + RxMissed);\n\n\t \n\tiowrite32(0, ioaddr + RxMissed);\n\tioread16(ioaddr + RxCRCErrs);\n\tioread16(ioaddr + RxMissed);\n}\n\n#define RHINE_EVENT_NAPI_RX\t(IntrRxDone | \\\n\t\t\t\t IntrRxErr | \\\n\t\t\t\t IntrRxEmpty | \\\n\t\t\t\t IntrRxOverflow\t| \\\n\t\t\t\t IntrRxDropped | \\\n\t\t\t\t IntrRxNoBuf | \\\n\t\t\t\t IntrRxWakeUp)\n\n#define RHINE_EVENT_NAPI_TX_ERR\t(IntrTxError | \\\n\t\t\t\t IntrTxAborted | \\\n\t\t\t\t IntrTxUnderrun | \\\n\t\t\t\t IntrTxDescRace)\n#define RHINE_EVENT_NAPI_TX\t(IntrTxDone | RHINE_EVENT_NAPI_TX_ERR)\n\n#define RHINE_EVENT_NAPI\t(RHINE_EVENT_NAPI_RX | \\\n\t\t\t\t RHINE_EVENT_NAPI_TX | \\\n\t\t\t\t IntrStatsMax)\n#define RHINE_EVENT_SLOW\t(IntrPCIErr | IntrLinkChange)\n#define RHINE_EVENT\t\t(RHINE_EVENT_NAPI | RHINE_EVENT_SLOW)\n\nstatic int rhine_napipoll(struct napi_struct *napi, int budget)\n{\n\tstruct rhine_private *rp = container_of(napi, struct rhine_private, napi);\n\tstruct net_device *dev = rp->dev;\n\tvoid __iomem *ioaddr = rp->base;\n\tu16 enable_mask = RHINE_EVENT & 0xffff;\n\tint work_done = 0;\n\tu32 status;\n\n\tstatus = rhine_get_events(rp);\n\trhine_ack_events(rp, status & ~RHINE_EVENT_SLOW);\n\n\tif (status & RHINE_EVENT_NAPI_RX)\n\t\twork_done += rhine_rx(dev, budget);\n\n\tif (status & RHINE_EVENT_NAPI_TX) {\n\t\tif (status & RHINE_EVENT_NAPI_TX_ERR) {\n\t\t\t \n\t\t\trhine_wait_bit_low(rp, ChipCmd, CmdTxOn);\n\t\t\tif (ioread8(ioaddr + ChipCmd) & CmdTxOn)\n\t\t\t\tnetif_warn(rp, tx_err, dev, \"Tx still on\\n\");\n\t\t}\n\n\t\trhine_tx(dev);\n\n\t\tif (status & RHINE_EVENT_NAPI_TX_ERR)\n\t\t\trhine_tx_err(rp, status);\n\t}\n\n\tif (status & IntrStatsMax) {\n\t\tspin_lock(&rp->lock);\n\t\trhine_update_rx_crc_and_missed_errord(rp);\n\t\tspin_unlock(&rp->lock);\n\t}\n\n\tif (status & RHINE_EVENT_SLOW) {\n\t\tenable_mask &= ~RHINE_EVENT_SLOW;\n\t\tschedule_work(&rp->slow_event_task);\n\t}\n\n\tif (work_done < budget) {\n\t\tnapi_complete_done(napi, work_done);\n\t\tiowrite16(enable_mask, ioaddr + IntrEnable);\n\t}\n\treturn work_done;\n}\n\nstatic void rhine_hw_init(struct net_device *dev, long pioaddr)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\t \n\trhine_chip_reset(dev);\n\n\t \n\tif (rp->quirks & rqRhineI)\n\t\tmsleep(5);\n\n\t \n\tif (dev_is_pci(dev->dev.parent))\n\t\trhine_reload_eeprom(pioaddr, dev);\n}\n\nstatic const struct net_device_ops rhine_netdev_ops = {\n\t.ndo_open\t\t = rhine_open,\n\t.ndo_stop\t\t = rhine_close,\n\t.ndo_start_xmit\t\t = rhine_start_tx,\n\t.ndo_get_stats64\t = rhine_get_stats64,\n\t.ndo_set_rx_mode\t = rhine_set_rx_mode,\n\t.ndo_validate_addr\t = eth_validate_addr,\n\t.ndo_set_mac_address \t = eth_mac_addr,\n\t.ndo_eth_ioctl\t\t = netdev_ioctl,\n\t.ndo_tx_timeout \t = rhine_tx_timeout,\n\t.ndo_vlan_rx_add_vid\t = rhine_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t = rhine_vlan_rx_kill_vid,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t = rhine_poll,\n#endif\n};\n\nstatic int rhine_init_one_common(struct device *hwdev, u32 quirks,\n\t\t\t\t long pioaddr, void __iomem *ioaddr, int irq)\n{\n\tstruct net_device *dev;\n\tstruct rhine_private *rp;\n\tint i, rc, phy_id;\n\tu8 addr[ETH_ALEN];\n\tconst char *name;\n\n\t \n\trc = dma_set_mask(hwdev, DMA_BIT_MASK(32));\n\tif (rc) {\n\t\tdev_err(hwdev, \"32-bit DMA addresses not supported by the card!?\\n\");\n\t\tgoto err_out;\n\t}\n\n\tdev = alloc_etherdev(sizeof(struct rhine_private));\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\tSET_NETDEV_DEV(dev, hwdev);\n\n\trp = netdev_priv(dev);\n\trp->dev = dev;\n\trp->quirks = quirks;\n\trp->pioaddr = pioaddr;\n\trp->base = ioaddr;\n\trp->irq = irq;\n\trp->msg_enable = netif_msg_init(debug, RHINE_MSG_DEFAULT);\n\n\tphy_id = rp->quirks & rqIntPHY ? 1 : 0;\n\n\tu64_stats_init(&rp->tx_stats.syncp);\n\tu64_stats_init(&rp->rx_stats.syncp);\n\n\t \n\trhine_power_init(dev);\n\trhine_hw_init(dev, pioaddr);\n\n\tfor (i = 0; i < 6; i++)\n\t\taddr[i] = ioread8(ioaddr + StationAddr + i);\n\teth_hw_addr_set(dev, addr);\n\n\tif (!is_valid_ether_addr(dev->dev_addr)) {\n\t\t \n\t\tnetdev_err(dev, \"Invalid MAC address: %pM\\n\", dev->dev_addr);\n\t\teth_hw_addr_random(dev);\n\t\tnetdev_info(dev, \"Using random MAC address: %pM\\n\",\n\t\t\t    dev->dev_addr);\n\t}\n\n\t \n\tif (!phy_id)\n\t\tphy_id = ioread8(ioaddr + 0x6C);\n\n\tspin_lock_init(&rp->lock);\n\tmutex_init(&rp->task_lock);\n\tINIT_WORK(&rp->reset_task, rhine_reset_task);\n\tINIT_WORK(&rp->slow_event_task, rhine_slow_event_task);\n\n\trp->mii_if.dev = dev;\n\trp->mii_if.mdio_read = mdio_read;\n\trp->mii_if.mdio_write = mdio_write;\n\trp->mii_if.phy_id_mask = 0x1f;\n\trp->mii_if.reg_num_mask = 0x1f;\n\n\t \n\tdev->netdev_ops = &rhine_netdev_ops;\n\tdev->ethtool_ops = &netdev_ethtool_ops;\n\tdev->watchdog_timeo = TX_TIMEOUT;\n\n\tnetif_napi_add(dev, &rp->napi, rhine_napipoll);\n\n\tif (rp->quirks & rqRhineI)\n\t\tdev->features |= NETIF_F_SG|NETIF_F_HW_CSUM;\n\n\tif (rp->quirks & rqMgmt)\n\t\tdev->features |= NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t NETIF_F_HW_VLAN_CTAG_RX |\n\t\t\t\t NETIF_F_HW_VLAN_CTAG_FILTER;\n\n\t \n\trc = register_netdev(dev);\n\tif (rc)\n\t\tgoto err_out_free_netdev;\n\n\tif (rp->quirks & rqRhineI)\n\t\tname = \"Rhine\";\n\telse if (rp->quirks & rqStatusWBRace)\n\t\tname = \"Rhine II\";\n\telse if (rp->quirks & rqMgmt)\n\t\tname = \"Rhine III (Management Adapter)\";\n\telse\n\t\tname = \"Rhine III\";\n\n\tnetdev_info(dev, \"VIA %s at %p, %pM, IRQ %d\\n\",\n\t\t    name, ioaddr, dev->dev_addr, rp->irq);\n\n\tdev_set_drvdata(hwdev, dev);\n\n\t{\n\t\tu16 mii_cmd;\n\t\tint mii_status = mdio_read(dev, phy_id, 1);\n\t\tmii_cmd = mdio_read(dev, phy_id, MII_BMCR) & ~BMCR_ISOLATE;\n\t\tmdio_write(dev, phy_id, MII_BMCR, mii_cmd);\n\t\tif (mii_status != 0xffff && mii_status != 0x0000) {\n\t\t\trp->mii_if.advertising = mdio_read(dev, phy_id, 4);\n\t\t\tnetdev_info(dev,\n\t\t\t\t    \"MII PHY found at address %d, status 0x%04x advertising %04x Link %04x\\n\",\n\t\t\t\t    phy_id,\n\t\t\t\t    mii_status, rp->mii_if.advertising,\n\t\t\t\t    mdio_read(dev, phy_id, 5));\n\n\t\t\t \n\t\t\tif (mii_status & BMSR_LSTATUS)\n\t\t\t\tnetif_carrier_on(dev);\n\t\t\telse\n\t\t\t\tnetif_carrier_off(dev);\n\n\t\t}\n\t}\n\trp->mii_if.phy_id = phy_id;\n\tif (avoid_D3)\n\t\tnetif_info(rp, probe, dev, \"No D3 power state at shutdown\\n\");\n\n\treturn 0;\n\nerr_out_free_netdev:\n\tfree_netdev(dev);\nerr_out:\n\treturn rc;\n}\n\nstatic int rhine_init_one_pci(struct pci_dev *pdev,\n\t\t\t      const struct pci_device_id *ent)\n{\n\tstruct device *hwdev = &pdev->dev;\n\tint rc;\n\tlong pioaddr, memaddr;\n\tvoid __iomem *ioaddr;\n\tint io_size = pdev->revision < VTunknown0 ? 128 : 256;\n\n \n#ifdef CONFIG_VIA_RHINE_MMIO\n\tu32 quirks = rqNeedEnMMIO;\n#else\n\tu32 quirks = 0;\n#endif\n\n\trc = pci_enable_device(pdev);\n\tif (rc)\n\t\tgoto err_out;\n\n\tif (pdev->revision < VTunknown0) {\n\t\tquirks |= rqRhineI;\n\t} else if (pdev->revision >= VT6102) {\n\t\tquirks |= rqWOL | rqForceReset;\n\t\tif (pdev->revision < VT6105) {\n\t\t\tquirks |= rqStatusWBRace;\n\t\t} else {\n\t\t\tquirks |= rqIntPHY;\n\t\t\tif (pdev->revision >= VT6105_B0)\n\t\t\t\tquirks |= rq6patterns;\n\t\t\tif (pdev->revision >= VT6105M)\n\t\t\t\tquirks |= rqMgmt;\n\t\t}\n\t}\n\n\t \n\tif ((pci_resource_len(pdev, 0) < io_size) ||\n\t    (pci_resource_len(pdev, 1) < io_size)) {\n\t\trc = -EIO;\n\t\tdev_err(hwdev, \"Insufficient PCI resources, aborting\\n\");\n\t\tgoto err_out_pci_disable;\n\t}\n\n\tpioaddr = pci_resource_start(pdev, 0);\n\tmemaddr = pci_resource_start(pdev, 1);\n\n\tpci_set_master(pdev);\n\n\trc = pci_request_regions(pdev, DRV_NAME);\n\tif (rc)\n\t\tgoto err_out_pci_disable;\n\n\tioaddr = pci_iomap(pdev, (quirks & rqNeedEnMMIO ? 1 : 0), io_size);\n\tif (!ioaddr) {\n\t\trc = -EIO;\n\t\tdev_err(hwdev,\n\t\t\t\"ioremap failed for device %s, region 0x%X @ 0x%lX\\n\",\n\t\t\tdev_name(hwdev), io_size, memaddr);\n\t\tgoto err_out_free_res;\n\t}\n\n\tenable_mmio(pioaddr, quirks);\n\n\trc = verify_mmio(hwdev, pioaddr, ioaddr, quirks);\n\tif (rc)\n\t\tgoto err_out_unmap;\n\n\trc = rhine_init_one_common(&pdev->dev, quirks,\n\t\t\t\t   pioaddr, ioaddr, pdev->irq);\n\tif (!rc)\n\t\treturn 0;\n\nerr_out_unmap:\n\tpci_iounmap(pdev, ioaddr);\nerr_out_free_res:\n\tpci_release_regions(pdev);\nerr_out_pci_disable:\n\tpci_disable_device(pdev);\nerr_out:\n\treturn rc;\n}\n\nstatic int rhine_init_one_platform(struct platform_device *pdev)\n{\n\tconst u32 *quirks;\n\tint irq;\n\tvoid __iomem *ioaddr;\n\n\tquirks = of_device_get_match_data(&pdev->dev);\n\tif (!quirks)\n\t\treturn -EINVAL;\n\n\tioaddr = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(ioaddr))\n\t\treturn PTR_ERR(ioaddr);\n\n\tirq = irq_of_parse_and_map(pdev->dev.of_node, 0);\n\tif (!irq)\n\t\treturn -EINVAL;\n\n\treturn rhine_init_one_common(&pdev->dev, *quirks,\n\t\t\t\t     (long)ioaddr, ioaddr, irq);\n}\n\nstatic int alloc_ring(struct net_device* dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tstruct device *hwdev = dev->dev.parent;\n\tvoid *ring;\n\tdma_addr_t ring_dma;\n\n\tring = dma_alloc_coherent(hwdev,\n\t\t\t\t  RX_RING_SIZE * sizeof(struct rx_desc) +\n\t\t\t\t  TX_RING_SIZE * sizeof(struct tx_desc),\n\t\t\t\t  &ring_dma,\n\t\t\t\t  GFP_ATOMIC);\n\tif (!ring) {\n\t\tnetdev_err(dev, \"Could not allocate DMA memory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tif (rp->quirks & rqRhineI) {\n\t\trp->tx_bufs = dma_alloc_coherent(hwdev,\n\t\t\t\t\t\t PKT_BUF_SZ * TX_RING_SIZE,\n\t\t\t\t\t\t &rp->tx_bufs_dma,\n\t\t\t\t\t\t GFP_ATOMIC);\n\t\tif (rp->tx_bufs == NULL) {\n\t\t\tdma_free_coherent(hwdev,\n\t\t\t\t\t  RX_RING_SIZE * sizeof(struct rx_desc) +\n\t\t\t\t\t  TX_RING_SIZE * sizeof(struct tx_desc),\n\t\t\t\t\t  ring, ring_dma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\trp->rx_ring = ring;\n\trp->tx_ring = ring + RX_RING_SIZE * sizeof(struct rx_desc);\n\trp->rx_ring_dma = ring_dma;\n\trp->tx_ring_dma = ring_dma + RX_RING_SIZE * sizeof(struct rx_desc);\n\n\treturn 0;\n}\n\nstatic void free_ring(struct net_device* dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tstruct device *hwdev = dev->dev.parent;\n\n\tdma_free_coherent(hwdev,\n\t\t\t  RX_RING_SIZE * sizeof(struct rx_desc) +\n\t\t\t  TX_RING_SIZE * sizeof(struct tx_desc),\n\t\t\t  rp->rx_ring, rp->rx_ring_dma);\n\trp->tx_ring = NULL;\n\n\tif (rp->tx_bufs)\n\t\tdma_free_coherent(hwdev, PKT_BUF_SZ * TX_RING_SIZE,\n\t\t\t\t  rp->tx_bufs, rp->tx_bufs_dma);\n\n\trp->tx_bufs = NULL;\n\n}\n\nstruct rhine_skb_dma {\n\tstruct sk_buff *skb;\n\tdma_addr_t dma;\n};\n\nstatic inline int rhine_skb_dma_init(struct net_device *dev,\n\t\t\t\t     struct rhine_skb_dma *sd)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tstruct device *hwdev = dev->dev.parent;\n\tconst int size = rp->rx_buf_sz;\n\n\tsd->skb = netdev_alloc_skb(dev, size);\n\tif (!sd->skb)\n\t\treturn -ENOMEM;\n\n\tsd->dma = dma_map_single(hwdev, sd->skb->data, size, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(hwdev, sd->dma))) {\n\t\tnetif_err(rp, drv, dev, \"Rx DMA mapping failure\\n\");\n\t\tdev_kfree_skb_any(sd->skb);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic void rhine_reset_rbufs(struct rhine_private *rp)\n{\n\tint i;\n\n\trp->cur_rx = 0;\n\n\tfor (i = 0; i < RX_RING_SIZE; i++)\n\t\trp->rx_ring[i].rx_status = cpu_to_le32(DescOwn);\n}\n\nstatic inline void rhine_skb_dma_nic_store(struct rhine_private *rp,\n\t\t\t\t\t   struct rhine_skb_dma *sd, int entry)\n{\n\trp->rx_skbuff_dma[entry] = sd->dma;\n\trp->rx_skbuff[entry] = sd->skb;\n\n\trp->rx_ring[entry].addr = cpu_to_le32(sd->dma);\n\tdma_wmb();\n}\n\nstatic void free_rbufs(struct net_device* dev);\n\nstatic int alloc_rbufs(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tdma_addr_t next;\n\tint rc, i;\n\n\trp->rx_buf_sz = (dev->mtu <= 1500 ? PKT_BUF_SZ : dev->mtu + 32);\n\tnext = rp->rx_ring_dma;\n\n\t \n\tfor (i = 0; i < RX_RING_SIZE; i++) {\n\t\trp->rx_ring[i].rx_status = 0;\n\t\trp->rx_ring[i].desc_length = cpu_to_le32(rp->rx_buf_sz);\n\t\tnext += sizeof(struct rx_desc);\n\t\trp->rx_ring[i].next_desc = cpu_to_le32(next);\n\t\trp->rx_skbuff[i] = NULL;\n\t}\n\t \n\trp->rx_ring[i-1].next_desc = cpu_to_le32(rp->rx_ring_dma);\n\n\t \n\tfor (i = 0; i < RX_RING_SIZE; i++) {\n\t\tstruct rhine_skb_dma sd;\n\n\t\trc = rhine_skb_dma_init(dev, &sd);\n\t\tif (rc < 0) {\n\t\t\tfree_rbufs(dev);\n\t\t\tgoto out;\n\t\t}\n\n\t\trhine_skb_dma_nic_store(rp, &sd, i);\n\t}\n\n\trhine_reset_rbufs(rp);\nout:\n\treturn rc;\n}\n\nstatic void free_rbufs(struct net_device* dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tstruct device *hwdev = dev->dev.parent;\n\tint i;\n\n\t \n\tfor (i = 0; i < RX_RING_SIZE; i++) {\n\t\trp->rx_ring[i].rx_status = 0;\n\t\trp->rx_ring[i].addr = cpu_to_le32(0xBADF00D0);  \n\t\tif (rp->rx_skbuff[i]) {\n\t\t\tdma_unmap_single(hwdev,\n\t\t\t\t\t rp->rx_skbuff_dma[i],\n\t\t\t\t\t rp->rx_buf_sz, DMA_FROM_DEVICE);\n\t\t\tdev_kfree_skb(rp->rx_skbuff[i]);\n\t\t}\n\t\trp->rx_skbuff[i] = NULL;\n\t}\n}\n\nstatic void alloc_tbufs(struct net_device* dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tdma_addr_t next;\n\tint i;\n\n\trp->dirty_tx = rp->cur_tx = 0;\n\tnext = rp->tx_ring_dma;\n\tfor (i = 0; i < TX_RING_SIZE; i++) {\n\t\trp->tx_skbuff[i] = NULL;\n\t\trp->tx_ring[i].tx_status = 0;\n\t\trp->tx_ring[i].desc_length = cpu_to_le32(TXDESC);\n\t\tnext += sizeof(struct tx_desc);\n\t\trp->tx_ring[i].next_desc = cpu_to_le32(next);\n\t\tif (rp->quirks & rqRhineI)\n\t\t\trp->tx_buf[i] = &rp->tx_bufs[i * PKT_BUF_SZ];\n\t}\n\trp->tx_ring[i-1].next_desc = cpu_to_le32(rp->tx_ring_dma);\n\n\tnetdev_reset_queue(dev);\n}\n\nstatic void free_tbufs(struct net_device* dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tstruct device *hwdev = dev->dev.parent;\n\tint i;\n\n\tfor (i = 0; i < TX_RING_SIZE; i++) {\n\t\trp->tx_ring[i].tx_status = 0;\n\t\trp->tx_ring[i].desc_length = cpu_to_le32(TXDESC);\n\t\trp->tx_ring[i].addr = cpu_to_le32(0xBADF00D0);  \n\t\tif (rp->tx_skbuff[i]) {\n\t\t\tif (rp->tx_skbuff_dma[i]) {\n\t\t\t\tdma_unmap_single(hwdev,\n\t\t\t\t\t\t rp->tx_skbuff_dma[i],\n\t\t\t\t\t\t rp->tx_skbuff[i]->len,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\t}\n\t\t\tdev_kfree_skb(rp->tx_skbuff[i]);\n\t\t}\n\t\trp->tx_skbuff[i] = NULL;\n\t\trp->tx_buf[i] = NULL;\n\t}\n}\n\nstatic void rhine_check_media(struct net_device *dev, unsigned int init_media)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\n\tif (!rp->mii_if.force_media)\n\t\tmii_check_media(&rp->mii_if, netif_msg_link(rp), init_media);\n\n\tif (rp->mii_if.full_duplex)\n\t    iowrite8(ioread8(ioaddr + ChipCmd1) | Cmd1FDuplex,\n\t\t   ioaddr + ChipCmd1);\n\telse\n\t    iowrite8(ioread8(ioaddr + ChipCmd1) & ~Cmd1FDuplex,\n\t\t   ioaddr + ChipCmd1);\n\n\tnetif_info(rp, link, dev, \"force_media %d, carrier %d\\n\",\n\t\t   rp->mii_if.force_media, netif_carrier_ok(dev));\n}\n\n \nstatic void rhine_set_carrier(struct mii_if_info *mii)\n{\n\tstruct net_device *dev = mii->dev;\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\tif (mii->force_media) {\n\t\t \n\t\tif (!netif_carrier_ok(dev))\n\t\t\tnetif_carrier_on(dev);\n\t}\n\n\trhine_check_media(dev, 0);\n\n\tnetif_info(rp, link, dev, \"force_media %d, carrier %d\\n\",\n\t\t   mii->force_media, netif_carrier_ok(dev));\n}\n\n \nstatic void rhine_set_cam(void __iomem *ioaddr, int idx, u8 *addr)\n{\n\tint i;\n\n\tiowrite8(CAMC_CAMEN, ioaddr + CamCon);\n\twmb();\n\n\t \n\tidx &= (MCAM_SIZE - 1);\n\n\tiowrite8((u8) idx, ioaddr + CamAddr);\n\n\tfor (i = 0; i < 6; i++, addr++)\n\t\tiowrite8(*addr, ioaddr + MulticastFilter0 + i);\n\tudelay(10);\n\twmb();\n\n\tiowrite8(CAMC_CAMWR | CAMC_CAMEN, ioaddr + CamCon);\n\tudelay(10);\n\n\tiowrite8(0, ioaddr + CamCon);\n}\n\n \nstatic void rhine_set_vlan_cam(void __iomem *ioaddr, int idx, u8 *addr)\n{\n\tiowrite8(CAMC_CAMEN | CAMC_VCAMSL, ioaddr + CamCon);\n\twmb();\n\n\t \n\tidx &= (VCAM_SIZE - 1);\n\n\tiowrite8((u8) idx, ioaddr + CamAddr);\n\n\tiowrite16(*((u16 *) addr), ioaddr + MulticastFilter0 + 6);\n\tudelay(10);\n\twmb();\n\n\tiowrite8(CAMC_CAMWR | CAMC_CAMEN, ioaddr + CamCon);\n\tudelay(10);\n\n\tiowrite8(0, ioaddr + CamCon);\n}\n\n \nstatic void rhine_set_cam_mask(void __iomem *ioaddr, u32 mask)\n{\n\tiowrite8(CAMC_CAMEN, ioaddr + CamCon);\n\twmb();\n\n\t \n\tiowrite32(mask, ioaddr + CamMask);\n\n\t \n\tiowrite8(0, ioaddr + CamCon);\n}\n\n \nstatic void rhine_set_vlan_cam_mask(void __iomem *ioaddr, u32 mask)\n{\n\tiowrite8(CAMC_CAMEN | CAMC_VCAMSL, ioaddr + CamCon);\n\twmb();\n\n\t \n\tiowrite32(mask, ioaddr + CamMask);\n\n\t \n\tiowrite8(0, ioaddr + CamCon);\n}\n\n \nstatic void rhine_init_cam_filter(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\n\t \n\trhine_set_vlan_cam_mask(ioaddr, 0);\n\trhine_set_cam_mask(ioaddr, 0);\n\n\t \n\tBYTE_REG_BITS_ON(TCR_PQEN, ioaddr + TxConfig);\n\tBYTE_REG_BITS_OFF(BCR1_VIDFR, ioaddr + PCIBusConfig1);\n}\n\n \nstatic void rhine_update_vcam(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\tu16 vid;\n\tu32 vCAMmask = 0;\t \n\tunsigned int i = 0;\n\n\tfor_each_set_bit(vid, rp->active_vlans, VLAN_N_VID) {\n\t\trhine_set_vlan_cam(ioaddr, i, (u8 *)&vid);\n\t\tvCAMmask |= 1 << i;\n\t\tif (++i >= VCAM_SIZE)\n\t\t\tbreak;\n\t}\n\trhine_set_vlan_cam_mask(ioaddr, vCAMmask);\n}\n\nstatic int rhine_vlan_rx_add_vid(struct net_device *dev, __be16 proto, u16 vid)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\tspin_lock_bh(&rp->lock);\n\tset_bit(vid, rp->active_vlans);\n\trhine_update_vcam(dev);\n\tspin_unlock_bh(&rp->lock);\n\treturn 0;\n}\n\nstatic int rhine_vlan_rx_kill_vid(struct net_device *dev, __be16 proto, u16 vid)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\tspin_lock_bh(&rp->lock);\n\tclear_bit(vid, rp->active_vlans);\n\trhine_update_vcam(dev);\n\tspin_unlock_bh(&rp->lock);\n\treturn 0;\n}\n\nstatic void init_registers(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\tint i;\n\n\tfor (i = 0; i < 6; i++)\n\t\tiowrite8(dev->dev_addr[i], ioaddr + StationAddr + i);\n\n\t \n\tiowrite16(0x0006, ioaddr + PCIBusConfig);\t \n\t \n\tiowrite8(0x20, ioaddr + TxConfig);\n\trp->tx_thresh = 0x20;\n\trp->rx_thresh = 0x60;\t\t \n\n\tiowrite32(rp->rx_ring_dma, ioaddr + RxRingPtr);\n\tiowrite32(rp->tx_ring_dma, ioaddr + TxRingPtr);\n\n\trhine_set_rx_mode(dev);\n\n\tif (rp->quirks & rqMgmt)\n\t\trhine_init_cam_filter(dev);\n\n\tnapi_enable(&rp->napi);\n\n\tiowrite16(RHINE_EVENT & 0xffff, ioaddr + IntrEnable);\n\n\tiowrite16(CmdStart | CmdTxOn | CmdRxOn | (Cmd1NoTxPoll << 8),\n\t       ioaddr + ChipCmd);\n\trhine_check_media(dev, 1);\n}\n\n \nstatic void rhine_enable_linkmon(struct rhine_private *rp)\n{\n\tvoid __iomem *ioaddr = rp->base;\n\n\tiowrite8(0, ioaddr + MIICmd);\n\tiowrite8(MII_BMSR, ioaddr + MIIRegAddr);\n\tiowrite8(0x80, ioaddr + MIICmd);\n\n\trhine_wait_bit_high(rp, MIIRegAddr, 0x20);\n\n\tiowrite8(MII_BMSR | 0x40, ioaddr + MIIRegAddr);\n}\n\n \nstatic void rhine_disable_linkmon(struct rhine_private *rp)\n{\n\tvoid __iomem *ioaddr = rp->base;\n\n\tiowrite8(0, ioaddr + MIICmd);\n\n\tif (rp->quirks & rqRhineI) {\n\t\tiowrite8(0x01, ioaddr + MIIRegAddr);\t\n\n\t\t \n\t\tmdelay(1);\n\n\t\t \n\t\tiowrite8(0x80, ioaddr + MIICmd);\n\n\t\trhine_wait_bit_high(rp, MIIRegAddr, 0x20);\n\n\t\t \n\t\tiowrite8(0, ioaddr + MIICmd);\n\t}\n\telse\n\t\trhine_wait_bit_high(rp, MIIRegAddr, 0x80);\n}\n\n \n\nstatic int mdio_read(struct net_device *dev, int phy_id, int regnum)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\tint result;\n\n\trhine_disable_linkmon(rp);\n\n\t \n\tiowrite8(phy_id, ioaddr + MIIPhyAddr);\n\tiowrite8(regnum, ioaddr + MIIRegAddr);\n\tiowrite8(0x40, ioaddr + MIICmd);\t\t \n\trhine_wait_bit_low(rp, MIICmd, 0x40);\n\tresult = ioread16(ioaddr + MIIData);\n\n\trhine_enable_linkmon(rp);\n\treturn result;\n}\n\nstatic void mdio_write(struct net_device *dev, int phy_id, int regnum, int value)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\n\trhine_disable_linkmon(rp);\n\n\t \n\tiowrite8(phy_id, ioaddr + MIIPhyAddr);\n\tiowrite8(regnum, ioaddr + MIIRegAddr);\n\tiowrite16(value, ioaddr + MIIData);\n\tiowrite8(0x20, ioaddr + MIICmd);\t\t \n\trhine_wait_bit_low(rp, MIICmd, 0x20);\n\n\trhine_enable_linkmon(rp);\n}\n\nstatic void rhine_task_disable(struct rhine_private *rp)\n{\n\tmutex_lock(&rp->task_lock);\n\trp->task_enable = false;\n\tmutex_unlock(&rp->task_lock);\n\n\tcancel_work_sync(&rp->slow_event_task);\n\tcancel_work_sync(&rp->reset_task);\n}\n\nstatic void rhine_task_enable(struct rhine_private *rp)\n{\n\tmutex_lock(&rp->task_lock);\n\trp->task_enable = true;\n\tmutex_unlock(&rp->task_lock);\n}\n\nstatic int rhine_open(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\tint rc;\n\n\trc = request_irq(rp->irq, rhine_interrupt, IRQF_SHARED, dev->name, dev);\n\tif (rc)\n\t\tgoto out;\n\n\tnetif_dbg(rp, ifup, dev, \"%s() irq %d\\n\", __func__, rp->irq);\n\n\trc = alloc_ring(dev);\n\tif (rc < 0)\n\t\tgoto out_free_irq;\n\n\trc = alloc_rbufs(dev);\n\tif (rc < 0)\n\t\tgoto out_free_ring;\n\n\talloc_tbufs(dev);\n\tenable_mmio(rp->pioaddr, rp->quirks);\n\trhine_power_init(dev);\n\trhine_chip_reset(dev);\n\trhine_task_enable(rp);\n\tinit_registers(dev);\n\n\tnetif_dbg(rp, ifup, dev, \"%s() Done - status %04x MII status: %04x\\n\",\n\t\t  __func__, ioread16(ioaddr + ChipCmd),\n\t\t  mdio_read(dev, rp->mii_if.phy_id, MII_BMSR));\n\n\tnetif_start_queue(dev);\n\nout:\n\treturn rc;\n\nout_free_ring:\n\tfree_ring(dev);\nout_free_irq:\n\tfree_irq(rp->irq, dev);\n\tgoto out;\n}\n\nstatic void rhine_reset_task(struct work_struct *work)\n{\n\tstruct rhine_private *rp = container_of(work, struct rhine_private,\n\t\t\t\t\t\treset_task);\n\tstruct net_device *dev = rp->dev;\n\n\tmutex_lock(&rp->task_lock);\n\n\tif (!rp->task_enable)\n\t\tgoto out_unlock;\n\n\tnapi_disable(&rp->napi);\n\tnetif_tx_disable(dev);\n\tspin_lock_bh(&rp->lock);\n\n\t \n\tfree_tbufs(dev);\n\talloc_tbufs(dev);\n\n\trhine_reset_rbufs(rp);\n\n\t \n\trhine_chip_reset(dev);\n\tinit_registers(dev);\n\n\tspin_unlock_bh(&rp->lock);\n\n\tnetif_trans_update(dev);  \n\tdev->stats.tx_errors++;\n\tnetif_wake_queue(dev);\n\nout_unlock:\n\tmutex_unlock(&rp->task_lock);\n}\n\nstatic void rhine_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\n\tnetdev_warn(dev, \"Transmit timed out, status %04x, PHY status %04x, resetting...\\n\",\n\t\t    ioread16(ioaddr + IntrStatus),\n\t\t    mdio_read(dev, rp->mii_if.phy_id, MII_BMSR));\n\n\tschedule_work(&rp->reset_task);\n}\n\nstatic inline bool rhine_tx_queue_full(struct rhine_private *rp)\n{\n\treturn (rp->cur_tx - rp->dirty_tx) >= TX_QUEUE_LEN;\n}\n\nstatic netdev_tx_t rhine_start_tx(struct sk_buff *skb,\n\t\t\t\t  struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tstruct device *hwdev = dev->dev.parent;\n\tvoid __iomem *ioaddr = rp->base;\n\tunsigned entry;\n\n\t \n\n\t \n\tentry = rp->cur_tx % TX_RING_SIZE;\n\n\tif (skb_padto(skb, ETH_ZLEN))\n\t\treturn NETDEV_TX_OK;\n\n\trp->tx_skbuff[entry] = skb;\n\n\tif ((rp->quirks & rqRhineI) &&\n\t    (((unsigned long)skb->data & 3) || skb_shinfo(skb)->nr_frags != 0 || skb->ip_summed == CHECKSUM_PARTIAL)) {\n\t\t \n\t\tif (skb->len > PKT_BUF_SZ) {\n\t\t\t \n\t\t\tdev_kfree_skb_any(skb);\n\t\t\trp->tx_skbuff[entry] = NULL;\n\t\t\tdev->stats.tx_dropped++;\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\t \n\t\tskb_copy_and_csum_dev(skb, rp->tx_buf[entry]);\n\t\tif (skb->len < ETH_ZLEN)\n\t\t\tmemset(rp->tx_buf[entry] + skb->len, 0,\n\t\t\t       ETH_ZLEN - skb->len);\n\t\trp->tx_skbuff_dma[entry] = 0;\n\t\trp->tx_ring[entry].addr = cpu_to_le32(rp->tx_bufs_dma +\n\t\t\t\t\t\t      (rp->tx_buf[entry] -\n\t\t\t\t\t\t       rp->tx_bufs));\n\t} else {\n\t\trp->tx_skbuff_dma[entry] =\n\t\t\tdma_map_single(hwdev, skb->data, skb->len,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(hwdev, rp->tx_skbuff_dma[entry])) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\trp->tx_skbuff_dma[entry] = 0;\n\t\t\tdev->stats.tx_dropped++;\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t\trp->tx_ring[entry].addr = cpu_to_le32(rp->tx_skbuff_dma[entry]);\n\t}\n\n\trp->tx_ring[entry].desc_length =\n\t\tcpu_to_le32(TXDESC | (skb->len >= ETH_ZLEN ? skb->len : ETH_ZLEN));\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\tu16 vid_pcp = skb_vlan_tag_get(skb);\n\n\t\t \n\t\tvid_pcp = (vid_pcp & VLAN_VID_MASK) |\n\t\t\t  ((vid_pcp & VLAN_PRIO_MASK) >> 1);\n\t\trp->tx_ring[entry].tx_status = cpu_to_le32((vid_pcp) << 16);\n\t\t \n\t\trp->tx_ring[entry].desc_length |= cpu_to_le32(0x020000);\n\t}\n\telse\n\t\trp->tx_ring[entry].tx_status = 0;\n\n\tnetdev_sent_queue(dev, skb->len);\n\t \n\tdma_wmb();\n\trp->tx_ring[entry].tx_status |= cpu_to_le32(DescOwn);\n\twmb();\n\n\trp->cur_tx++;\n\t \n\tsmp_wmb();\n\n\t \n\n\tif (skb_vlan_tag_present(skb))\n\t\t \n\t\tBYTE_REG_BITS_ON(1 << 7, ioaddr + TQWake);\n\n\t \n\tiowrite8(ioread8(ioaddr + ChipCmd1) | Cmd1TxDemand,\n\t       ioaddr + ChipCmd1);\n\tIOSYNC;\n\n\t \n\tif (rhine_tx_queue_full(rp)) {\n\t\tnetif_stop_queue(dev);\n\t\tsmp_rmb();\n\t\t \n\t\tif (!rhine_tx_queue_full(rp))\n\t\t\tnetif_wake_queue(dev);\n\t}\n\n\tnetif_dbg(rp, tx_queued, dev, \"Transmit frame #%d queued in slot %d\\n\",\n\t\t  rp->cur_tx - 1, entry);\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic void rhine_irq_disable(struct rhine_private *rp)\n{\n\tiowrite16(0x0000, rp->base + IntrEnable);\n}\n\n \nstatic irqreturn_t rhine_interrupt(int irq, void *dev_instance)\n{\n\tstruct net_device *dev = dev_instance;\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tu32 status;\n\tint handled = 0;\n\n\tstatus = rhine_get_events(rp);\n\n\tnetif_dbg(rp, intr, dev, \"Interrupt, status %08x\\n\", status);\n\n\tif (status & RHINE_EVENT) {\n\t\thandled = 1;\n\n\t\trhine_irq_disable(rp);\n\t\tnapi_schedule(&rp->napi);\n\t}\n\n\tif (status & ~(IntrLinkChange | IntrStatsMax | RHINE_EVENT_NAPI)) {\n\t\tnetif_err(rp, intr, dev, \"Something Wicked happened! %08x\\n\",\n\t\t\t  status);\n\t}\n\n\treturn IRQ_RETVAL(handled);\n}\n\n \nstatic void rhine_tx(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tstruct device *hwdev = dev->dev.parent;\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\tunsigned int dirty_tx = rp->dirty_tx;\n\tunsigned int cur_tx;\n\tstruct sk_buff *skb;\n\n\t \n\tsmp_rmb();\n\tcur_tx = rp->cur_tx;\n\t \n\twhile (dirty_tx != cur_tx) {\n\t\tunsigned int entry = dirty_tx % TX_RING_SIZE;\n\t\tu32 txstatus = le32_to_cpu(rp->tx_ring[entry].tx_status);\n\n\t\tnetif_dbg(rp, tx_done, dev, \"Tx scavenge %d status %08x\\n\",\n\t\t\t  entry, txstatus);\n\t\tif (txstatus & DescOwn)\n\t\t\tbreak;\n\t\tskb = rp->tx_skbuff[entry];\n\t\tif (txstatus & 0x8000) {\n\t\t\tnetif_dbg(rp, tx_done, dev,\n\t\t\t\t  \"Transmit error, Tx status %08x\\n\", txstatus);\n\t\t\tdev->stats.tx_errors++;\n\t\t\tif (txstatus & 0x0400)\n\t\t\t\tdev->stats.tx_carrier_errors++;\n\t\t\tif (txstatus & 0x0200)\n\t\t\t\tdev->stats.tx_window_errors++;\n\t\t\tif (txstatus & 0x0100)\n\t\t\t\tdev->stats.tx_aborted_errors++;\n\t\t\tif (txstatus & 0x0080)\n\t\t\t\tdev->stats.tx_heartbeat_errors++;\n\t\t\tif (((rp->quirks & rqRhineI) && txstatus & 0x0002) ||\n\t\t\t    (txstatus & 0x0800) || (txstatus & 0x1000)) {\n\t\t\t\tdev->stats.tx_fifo_errors++;\n\t\t\t\trp->tx_ring[entry].tx_status = cpu_to_le32(DescOwn);\n\t\t\t\tbreak;  \n\t\t\t}\n\t\t\t \n\t\t} else {\n\t\t\tif (rp->quirks & rqRhineI)\n\t\t\t\tdev->stats.collisions += (txstatus >> 3) & 0x0F;\n\t\t\telse\n\t\t\t\tdev->stats.collisions += txstatus & 0x0F;\n\t\t\tnetif_dbg(rp, tx_done, dev, \"collisions: %1.1x:%1.1x\\n\",\n\t\t\t\t  (txstatus >> 3) & 0xF, txstatus & 0xF);\n\n\t\t\tu64_stats_update_begin(&rp->tx_stats.syncp);\n\t\t\trp->tx_stats.bytes += skb->len;\n\t\t\trp->tx_stats.packets++;\n\t\t\tu64_stats_update_end(&rp->tx_stats.syncp);\n\t\t}\n\t\t \n\t\tif (rp->tx_skbuff_dma[entry]) {\n\t\t\tdma_unmap_single(hwdev,\n\t\t\t\t\t rp->tx_skbuff_dma[entry],\n\t\t\t\t\t skb->len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t}\n\t\tbytes_compl += skb->len;\n\t\tpkts_compl++;\n\t\tdev_consume_skb_any(skb);\n\t\trp->tx_skbuff[entry] = NULL;\n\t\tdirty_tx++;\n\t}\n\n\trp->dirty_tx = dirty_tx;\n\t \n\tsmp_wmb();\n\n\tnetdev_completed_queue(dev, pkts_compl, bytes_compl);\n\n\t \n\tif (!rhine_tx_queue_full(rp) && netif_queue_stopped(dev)) {\n\t\tnetif_wake_queue(dev);\n\t\tsmp_rmb();\n\t\t \n\t\tif (rhine_tx_queue_full(rp))\n\t\t\tnetif_stop_queue(dev);\n\t}\n}\n\n \nstatic inline u16 rhine_get_vlan_tci(struct sk_buff *skb, int data_size)\n{\n\tu8 *trailer = (u8 *)skb->data + ((data_size + 3) & ~3) + 2;\n\treturn be16_to_cpup((__be16 *)trailer);\n}\n\nstatic inline void rhine_rx_vlan_tag(struct sk_buff *skb, struct rx_desc *desc,\n\t\t\t\t     int data_size)\n{\n\tdma_rmb();\n\tif (unlikely(desc->desc_length & cpu_to_le32(DescTag))) {\n\t\tu16 vlan_tci;\n\n\t\tvlan_tci = rhine_get_vlan_tci(skb, data_size);\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tci);\n\t}\n}\n\n \nstatic int rhine_rx(struct net_device *dev, int limit)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tstruct device *hwdev = dev->dev.parent;\n\tint entry = rp->cur_rx % RX_RING_SIZE;\n\tint count;\n\n\tnetif_dbg(rp, rx_status, dev, \"%s(), entry %d status %08x\\n\", __func__,\n\t\t  entry, le32_to_cpu(rp->rx_ring[entry].rx_status));\n\n\t \n\tfor (count = 0; count < limit; ++count) {\n\t\tstruct rx_desc *desc = rp->rx_ring + entry;\n\t\tu32 desc_status = le32_to_cpu(desc->rx_status);\n\t\tint data_size = desc_status >> 16;\n\n\t\tif (desc_status & DescOwn)\n\t\t\tbreak;\n\n\t\tnetif_dbg(rp, rx_status, dev, \"%s() status %08x\\n\", __func__,\n\t\t\t  desc_status);\n\n\t\tif ((desc_status & (RxWholePkt | RxErr)) != RxWholePkt) {\n\t\t\tif ((desc_status & RxWholePkt) != RxWholePkt) {\n\t\t\t\tnetdev_warn(dev,\n\t\"Oversized Ethernet frame spanned multiple buffers, \"\n\t\"entry %#x length %d status %08x!\\n\",\n\t\t\t\t\t    entry, data_size,\n\t\t\t\t\t    desc_status);\n\t\t\t\tdev->stats.rx_length_errors++;\n\t\t\t} else if (desc_status & RxErr) {\n\t\t\t\t \n\t\t\t\tnetif_dbg(rp, rx_err, dev,\n\t\t\t\t\t  \"%s() Rx error %08x\\n\", __func__,\n\t\t\t\t\t  desc_status);\n\t\t\t\tdev->stats.rx_errors++;\n\t\t\t\tif (desc_status & 0x0030)\n\t\t\t\t\tdev->stats.rx_length_errors++;\n\t\t\t\tif (desc_status & 0x0048)\n\t\t\t\t\tdev->stats.rx_fifo_errors++;\n\t\t\t\tif (desc_status & 0x0004)\n\t\t\t\t\tdev->stats.rx_frame_errors++;\n\t\t\t\tif (desc_status & 0x0002) {\n\t\t\t\t\t \n\t\t\t\t\tspin_lock(&rp->lock);\n\t\t\t\t\tdev->stats.rx_crc_errors++;\n\t\t\t\t\tspin_unlock(&rp->lock);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tint pkt_len = data_size - 4;\n\t\t\tstruct sk_buff *skb;\n\n\t\t\t \n\t\t\tif (pkt_len < rx_copybreak) {\n\t\t\t\tskb = netdev_alloc_skb_ip_align(dev, pkt_len);\n\t\t\t\tif (unlikely(!skb))\n\t\t\t\t\tgoto drop;\n\n\t\t\t\tdma_sync_single_for_cpu(hwdev,\n\t\t\t\t\t\t\trp->rx_skbuff_dma[entry],\n\t\t\t\t\t\t\trp->rx_buf_sz,\n\t\t\t\t\t\t\tDMA_FROM_DEVICE);\n\n\t\t\t\tskb_copy_to_linear_data(skb,\n\t\t\t\t\t\t rp->rx_skbuff[entry]->data,\n\t\t\t\t\t\t pkt_len);\n\n\t\t\t\tdma_sync_single_for_device(hwdev,\n\t\t\t\t\t\t\t   rp->rx_skbuff_dma[entry],\n\t\t\t\t\t\t\t   rp->rx_buf_sz,\n\t\t\t\t\t\t\t   DMA_FROM_DEVICE);\n\t\t\t} else {\n\t\t\t\tstruct rhine_skb_dma sd;\n\n\t\t\t\tif (unlikely(rhine_skb_dma_init(dev, &sd) < 0))\n\t\t\t\t\tgoto drop;\n\n\t\t\t\tskb = rp->rx_skbuff[entry];\n\n\t\t\t\tdma_unmap_single(hwdev,\n\t\t\t\t\t\t rp->rx_skbuff_dma[entry],\n\t\t\t\t\t\t rp->rx_buf_sz,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\t\t\t\trhine_skb_dma_nic_store(rp, &sd, entry);\n\t\t\t}\n\n\t\t\tskb_put(skb, pkt_len);\n\n\t\t\trhine_rx_vlan_tag(skb, desc, data_size);\n\n\t\t\tskb->protocol = eth_type_trans(skb, dev);\n\n\t\t\tnetif_receive_skb(skb);\n\n\t\t\tu64_stats_update_begin(&rp->rx_stats.syncp);\n\t\t\trp->rx_stats.bytes += pkt_len;\n\t\t\trp->rx_stats.packets++;\n\t\t\tu64_stats_update_end(&rp->rx_stats.syncp);\n\t\t}\ngive_descriptor_to_nic:\n\t\tdesc->rx_status = cpu_to_le32(DescOwn);\n\t\tentry = (++rp->cur_rx) % RX_RING_SIZE;\n\t}\n\n\treturn count;\n\ndrop:\n\tdev->stats.rx_dropped++;\n\tgoto give_descriptor_to_nic;\n}\n\nstatic void rhine_restart_tx(struct net_device *dev) {\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\tint entry = rp->dirty_tx % TX_RING_SIZE;\n\tu32 intr_status;\n\n\t \n\tintr_status = rhine_get_events(rp);\n\n\tif ((intr_status & IntrTxErrSummary) == 0) {\n\n\t\t \n\t\tiowrite32(rp->tx_ring_dma + entry * sizeof(struct tx_desc),\n\t\t       ioaddr + TxRingPtr);\n\n\t\tiowrite8(ioread8(ioaddr + ChipCmd) | CmdTxOn,\n\t\t       ioaddr + ChipCmd);\n\n\t\tif (rp->tx_ring[entry].desc_length & cpu_to_le32(0x020000))\n\t\t\t \n\t\t\tBYTE_REG_BITS_ON(1 << 7, ioaddr + TQWake);\n\n\t\tiowrite8(ioread8(ioaddr + ChipCmd1) | Cmd1TxDemand,\n\t\t       ioaddr + ChipCmd1);\n\t\tIOSYNC;\n\t}\n\telse {\n\t\t \n\t\tnetif_warn(rp, tx_err, dev, \"another error occurred %08x\\n\",\n\t\t\t   intr_status);\n\t}\n\n}\n\nstatic void rhine_slow_event_task(struct work_struct *work)\n{\n\tstruct rhine_private *rp =\n\t\tcontainer_of(work, struct rhine_private, slow_event_task);\n\tstruct net_device *dev = rp->dev;\n\tu32 intr_status;\n\n\tmutex_lock(&rp->task_lock);\n\n\tif (!rp->task_enable)\n\t\tgoto out_unlock;\n\n\tintr_status = rhine_get_events(rp);\n\trhine_ack_events(rp, intr_status & RHINE_EVENT_SLOW);\n\n\tif (intr_status & IntrLinkChange)\n\t\trhine_check_media(dev, 0);\n\n\tif (intr_status & IntrPCIErr)\n\t\tnetif_warn(rp, hw, dev, \"PCI error\\n\");\n\n\tiowrite16(RHINE_EVENT & 0xffff, rp->base + IntrEnable);\n\nout_unlock:\n\tmutex_unlock(&rp->task_lock);\n}\n\nstatic void\nrhine_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tunsigned int start;\n\n\tspin_lock_bh(&rp->lock);\n\trhine_update_rx_crc_and_missed_errord(rp);\n\tspin_unlock_bh(&rp->lock);\n\n\tnetdev_stats_to_stats64(stats, &dev->stats);\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(&rp->rx_stats.syncp);\n\t\tstats->rx_packets = rp->rx_stats.packets;\n\t\tstats->rx_bytes = rp->rx_stats.bytes;\n\t} while (u64_stats_fetch_retry(&rp->rx_stats.syncp, start));\n\n\tdo {\n\t\tstart = u64_stats_fetch_begin(&rp->tx_stats.syncp);\n\t\tstats->tx_packets = rp->tx_stats.packets;\n\t\tstats->tx_bytes = rp->tx_stats.bytes;\n\t} while (u64_stats_fetch_retry(&rp->tx_stats.syncp, start));\n}\n\nstatic void rhine_set_rx_mode(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\tu32 mc_filter[2];\t \n\tu8 rx_mode = 0x0C;\t \n\tstruct netdev_hw_addr *ha;\n\n\tif (dev->flags & IFF_PROMISC) {\t\t \n\t\trx_mode = 0x1C;\n\t\tiowrite32(0xffffffff, ioaddr + MulticastFilter0);\n\t\tiowrite32(0xffffffff, ioaddr + MulticastFilter1);\n\t} else if ((netdev_mc_count(dev) > multicast_filter_limit) ||\n\t\t   (dev->flags & IFF_ALLMULTI)) {\n\t\t \n\t\tiowrite32(0xffffffff, ioaddr + MulticastFilter0);\n\t\tiowrite32(0xffffffff, ioaddr + MulticastFilter1);\n\t} else if (rp->quirks & rqMgmt) {\n\t\tint i = 0;\n\t\tu32 mCAMmask = 0;\t \n\t\tnetdev_for_each_mc_addr(ha, dev) {\n\t\t\tif (i == MCAM_SIZE)\n\t\t\t\tbreak;\n\t\t\trhine_set_cam(ioaddr, i, ha->addr);\n\t\t\tmCAMmask |= 1 << i;\n\t\t\ti++;\n\t\t}\n\t\trhine_set_cam_mask(ioaddr, mCAMmask);\n\t} else {\n\t\tmemset(mc_filter, 0, sizeof(mc_filter));\n\t\tnetdev_for_each_mc_addr(ha, dev) {\n\t\t\tint bit_nr = ether_crc(ETH_ALEN, ha->addr) >> 26;\n\n\t\t\tmc_filter[bit_nr >> 5] |= 1 << (bit_nr & 31);\n\t\t}\n\t\tiowrite32(mc_filter[0], ioaddr + MulticastFilter0);\n\t\tiowrite32(mc_filter[1], ioaddr + MulticastFilter1);\n\t}\n\t \n\tif (rp->quirks & rqMgmt) {\n\t\tif (dev->flags & IFF_PROMISC)\n\t\t\tBYTE_REG_BITS_OFF(BCR1_VIDFR, ioaddr + PCIBusConfig1);\n\t\telse\n\t\t\tBYTE_REG_BITS_ON(BCR1_VIDFR, ioaddr + PCIBusConfig1);\n\t}\n\tBYTE_REG_BITS_ON(rx_mode, ioaddr + RxConfig);\n}\n\nstatic void netdev_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\n{\n\tstruct device *hwdev = dev->dev.parent;\n\n\tstrscpy(info->driver, DRV_NAME, sizeof(info->driver));\n\tstrscpy(info->bus_info, dev_name(hwdev), sizeof(info->bus_info));\n}\n\nstatic int netdev_get_link_ksettings(struct net_device *dev,\n\t\t\t\t     struct ethtool_link_ksettings *cmd)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\tmutex_lock(&rp->task_lock);\n\tmii_ethtool_get_link_ksettings(&rp->mii_if, cmd);\n\tmutex_unlock(&rp->task_lock);\n\n\treturn 0;\n}\n\nstatic int netdev_set_link_ksettings(struct net_device *dev,\n\t\t\t\t     const struct ethtool_link_ksettings *cmd)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tint rc;\n\n\tmutex_lock(&rp->task_lock);\n\trc = mii_ethtool_set_link_ksettings(&rp->mii_if, cmd);\n\trhine_set_carrier(&rp->mii_if);\n\tmutex_unlock(&rp->task_lock);\n\n\treturn rc;\n}\n\nstatic int netdev_nway_reset(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\treturn mii_nway_restart(&rp->mii_if);\n}\n\nstatic u32 netdev_get_link(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\treturn mii_link_ok(&rp->mii_if);\n}\n\nstatic u32 netdev_get_msglevel(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\treturn rp->msg_enable;\n}\n\nstatic void netdev_set_msglevel(struct net_device *dev, u32 value)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\trp->msg_enable = value;\n}\n\nstatic void rhine_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\tif (!(rp->quirks & rqWOL))\n\t\treturn;\n\n\tspin_lock_irq(&rp->lock);\n\twol->supported = WAKE_PHY | WAKE_MAGIC |\n\t\t\t WAKE_UCAST | WAKE_MCAST | WAKE_BCAST;\t \n\twol->wolopts = rp->wolopts;\n\tspin_unlock_irq(&rp->lock);\n}\n\nstatic int rhine_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tu32 support = WAKE_PHY | WAKE_MAGIC |\n\t\t      WAKE_UCAST | WAKE_MCAST | WAKE_BCAST;\t \n\n\tif (!(rp->quirks & rqWOL))\n\t\treturn -EINVAL;\n\n\tif (wol->wolopts & ~support)\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&rp->lock);\n\trp->wolopts = wol->wolopts;\n\tspin_unlock_irq(&rp->lock);\n\n\treturn 0;\n}\n\nstatic const struct ethtool_ops netdev_ethtool_ops = {\n\t.get_drvinfo\t\t= netdev_get_drvinfo,\n\t.nway_reset\t\t= netdev_nway_reset,\n\t.get_link\t\t= netdev_get_link,\n\t.get_msglevel\t\t= netdev_get_msglevel,\n\t.set_msglevel\t\t= netdev_set_msglevel,\n\t.get_wol\t\t= rhine_get_wol,\n\t.set_wol\t\t= rhine_set_wol,\n\t.get_link_ksettings\t= netdev_get_link_ksettings,\n\t.set_link_ksettings\t= netdev_set_link_ksettings,\n};\n\nstatic int netdev_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tint rc;\n\n\tif (!netif_running(dev))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rp->task_lock);\n\trc = generic_mii_ioctl(&rp->mii_if, if_mii(rq), cmd, NULL);\n\trhine_set_carrier(&rp->mii_if);\n\tmutex_unlock(&rp->task_lock);\n\n\treturn rc;\n}\n\nstatic int rhine_close(struct net_device *dev)\n{\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\n\trhine_task_disable(rp);\n\tnapi_disable(&rp->napi);\n\tnetif_stop_queue(dev);\n\n\tnetif_dbg(rp, ifdown, dev, \"Shutting down ethercard, status was %04x\\n\",\n\t\t  ioread16(ioaddr + ChipCmd));\n\n\t \n\tiowrite8(rp->tx_thresh | 0x02, ioaddr + TxConfig);\n\n\trhine_irq_disable(rp);\n\n\t \n\tiowrite16(CmdStop, ioaddr + ChipCmd);\n\n\tfree_irq(rp->irq, dev);\n\tfree_rbufs(dev);\n\tfree_tbufs(dev);\n\tfree_ring(dev);\n\n\treturn 0;\n}\n\n\nstatic void rhine_remove_one_pci(struct pci_dev *pdev)\n{\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\tunregister_netdev(dev);\n\n\tpci_iounmap(pdev, rp->base);\n\tpci_release_regions(pdev);\n\n\tfree_netdev(dev);\n\tpci_disable_device(pdev);\n}\n\nstatic int rhine_remove_one_platform(struct platform_device *pdev)\n{\n\tstruct net_device *dev = platform_get_drvdata(pdev);\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\tunregister_netdev(dev);\n\n\tiounmap(rp->base);\n\n\tfree_netdev(dev);\n\n\treturn 0;\n}\n\nstatic void rhine_shutdown_pci(struct pci_dev *pdev)\n{\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct rhine_private *rp = netdev_priv(dev);\n\tvoid __iomem *ioaddr = rp->base;\n\n\tif (!(rp->quirks & rqWOL))\n\t\treturn;  \n\n\trhine_power_init(dev);\n\n\t \n\tif (rp->quirks & rq6patterns)\n\t\tiowrite8(0x04, ioaddr + WOLcgClr);\n\n\tspin_lock(&rp->lock);\n\n\tif (rp->wolopts & WAKE_MAGIC) {\n\t\tiowrite8(WOLmagic, ioaddr + WOLcrSet);\n\t\t \n\t\tiowrite8(ioread8(ioaddr + ConfigA) | 0x03, ioaddr + ConfigA);\n\t}\n\n\tif (rp->wolopts & (WAKE_BCAST|WAKE_MCAST))\n\t\tiowrite8(WOLbmcast, ioaddr + WOLcgSet);\n\n\tif (rp->wolopts & WAKE_PHY)\n\t\tiowrite8(WOLlnkon | WOLlnkoff, ioaddr + WOLcrSet);\n\n\tif (rp->wolopts & WAKE_UCAST)\n\t\tiowrite8(WOLucast, ioaddr + WOLcrSet);\n\n\tif (rp->wolopts) {\n\t\t \n\t\tiowrite8(0x01, ioaddr + PwcfgSet);\n\t\tiowrite8(ioread8(ioaddr + StickyHW) | 0x04, ioaddr + StickyHW);\n\t}\n\n\tspin_unlock(&rp->lock);\n\n\tif (system_state == SYSTEM_POWER_OFF && !avoid_D3) {\n\t\tiowrite8(ioread8(ioaddr + StickyHW) | 0x03, ioaddr + StickyHW);\n\n\t\tpci_wake_from_d3(pdev, true);\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n\t}\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int rhine_suspend(struct device *device)\n{\n\tstruct net_device *dev = dev_get_drvdata(device);\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\trhine_task_disable(rp);\n\trhine_irq_disable(rp);\n\tnapi_disable(&rp->napi);\n\n\tnetif_device_detach(dev);\n\n\tif (dev_is_pci(device))\n\t\trhine_shutdown_pci(to_pci_dev(device));\n\n\treturn 0;\n}\n\nstatic int rhine_resume(struct device *device)\n{\n\tstruct net_device *dev = dev_get_drvdata(device);\n\tstruct rhine_private *rp = netdev_priv(dev);\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tenable_mmio(rp->pioaddr, rp->quirks);\n\trhine_power_init(dev);\n\tfree_tbufs(dev);\n\talloc_tbufs(dev);\n\trhine_reset_rbufs(rp);\n\trhine_task_enable(rp);\n\tspin_lock_bh(&rp->lock);\n\tinit_registers(dev);\n\tspin_unlock_bh(&rp->lock);\n\n\tnetif_device_attach(dev);\n\n\treturn 0;\n}\n\nstatic SIMPLE_DEV_PM_OPS(rhine_pm_ops, rhine_suspend, rhine_resume);\n#define RHINE_PM_OPS\t(&rhine_pm_ops)\n\n#else\n\n#define RHINE_PM_OPS\tNULL\n\n#endif  \n\nstatic struct pci_driver rhine_driver_pci = {\n\t.name\t\t= DRV_NAME,\n\t.id_table\t= rhine_pci_tbl,\n\t.probe\t\t= rhine_init_one_pci,\n\t.remove\t\t= rhine_remove_one_pci,\n\t.shutdown\t= rhine_shutdown_pci,\n\t.driver.pm\t= RHINE_PM_OPS,\n};\n\nstatic struct platform_driver rhine_driver_platform = {\n\t.probe\t\t= rhine_init_one_platform,\n\t.remove\t\t= rhine_remove_one_platform,\n\t.driver = {\n\t\t.name\t= DRV_NAME,\n\t\t.of_match_table\t= rhine_of_tbl,\n\t\t.pm\t\t= RHINE_PM_OPS,\n\t}\n};\n\nstatic const struct dmi_system_id rhine_dmi_table[] __initconst = {\n\t{\n\t\t.ident = \"EPIA-M\",\n\t\t.matches = {\n\t\t\tDMI_MATCH(DMI_BIOS_VENDOR, \"Award Software International, Inc.\"),\n\t\t\tDMI_MATCH(DMI_BIOS_VERSION, \"6.00 PG\"),\n\t\t},\n\t},\n\t{\n\t\t.ident = \"KV7\",\n\t\t.matches = {\n\t\t\tDMI_MATCH(DMI_BIOS_VENDOR, \"Phoenix Technologies, LTD\"),\n\t\t\tDMI_MATCH(DMI_BIOS_VERSION, \"6.00 PG\"),\n\t\t},\n\t},\n\t{ NULL }\n};\n\nstatic int __init rhine_init(void)\n{\n\tint ret_pci, ret_platform;\n\n \n\tif (dmi_check_system(rhine_dmi_table)) {\n\t\t \n\t\tavoid_D3 = true;\n\t\tpr_warn(\"Broken BIOS detected, avoid_D3 enabled\\n\");\n\t}\n\telse if (avoid_D3)\n\t\tpr_info(\"avoid_D3 set\\n\");\n\n\tret_pci = pci_register_driver(&rhine_driver_pci);\n\tret_platform = platform_driver_register(&rhine_driver_platform);\n\tif ((ret_pci < 0) && (ret_platform < 0))\n\t\treturn ret_pci;\n\n\treturn 0;\n}\n\n\nstatic void __exit rhine_cleanup(void)\n{\n\tplatform_driver_unregister(&rhine_driver_platform);\n\tpci_unregister_driver(&rhine_driver_pci);\n}\n\n\nmodule_init(rhine_init);\nmodule_exit(rhine_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}