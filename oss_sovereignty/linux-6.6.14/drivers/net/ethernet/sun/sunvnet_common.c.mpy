{
  "module_name": "sunvnet_common.c",
  "hash_id": "f671ec845dfb80a9347480c2469cbea9db8430e60fab0c477a3bd60254bc1248",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sun/sunvnet_common.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/init.h>\n#include <linux/netdevice.h>\n#include <linux/ethtool.h>\n#include <linux/etherdevice.h>\n#include <linux/mutex.h>\n#include <linux/highmem.h>\n#include <linux/if_vlan.h>\n#define CREATE_TRACE_POINTS\n#include <trace/events/sunvnet.h>\n\n#if IS_ENABLED(CONFIG_IPV6)\n#include <linux/icmpv6.h>\n#endif\n\n#include <net/ip.h>\n#include <net/gso.h>\n#include <net/icmp.h>\n#include <net/route.h>\n\n#include <asm/vio.h>\n#include <asm/ldc.h>\n\n#include \"sunvnet_common.h\"\n\n \n#define\tVNET_MAX_RETRIES\t10\n\nMODULE_AUTHOR(\"David S. Miller (davem@davemloft.net)\");\nMODULE_DESCRIPTION(\"Sun LDOM virtual network support library\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(\"1.1\");\n\nstatic int __vnet_tx_trigger(struct vnet_port *port, u32 start);\n\nstatic inline u32 vnet_tx_dring_avail(struct vio_dring_state *dr)\n{\n\treturn vio_dring_avail(dr, VNET_TX_RING_SIZE);\n}\n\nstatic int vnet_handle_unknown(struct vnet_port *port, void *arg)\n{\n\tstruct vio_msg_tag *pkt = arg;\n\n\tpr_err(\"Received unknown msg [%02x:%02x:%04x:%08x]\\n\",\n\t       pkt->type, pkt->stype, pkt->stype_env, pkt->sid);\n\tpr_err(\"Resetting connection\\n\");\n\n\tldc_disconnect(port->vio.lp);\n\n\treturn -ECONNRESET;\n}\n\nstatic int vnet_port_alloc_tx_ring(struct vnet_port *port);\n\nint sunvnet_send_attr_common(struct vio_driver_state *vio)\n{\n\tstruct vnet_port *port = to_vnet_port(vio);\n\tstruct net_device *dev = VNET_PORT_TO_NET_DEVICE(port);\n\tstruct vio_net_attr_info pkt;\n\tint framelen = ETH_FRAME_LEN;\n\tint i, err;\n\n\terr = vnet_port_alloc_tx_ring(to_vnet_port(vio));\n\tif (err)\n\t\treturn err;\n\n\tmemset(&pkt, 0, sizeof(pkt));\n\tpkt.tag.type = VIO_TYPE_CTRL;\n\tpkt.tag.stype = VIO_SUBTYPE_INFO;\n\tpkt.tag.stype_env = VIO_ATTR_INFO;\n\tpkt.tag.sid = vio_send_sid(vio);\n\tif (vio_version_before(vio, 1, 2))\n\t\tpkt.xfer_mode = VIO_DRING_MODE;\n\telse\n\t\tpkt.xfer_mode = VIO_NEW_DRING_MODE;\n\tpkt.addr_type = VNET_ADDR_ETHERMAC;\n\tpkt.ack_freq = 0;\n\tfor (i = 0; i < 6; i++)\n\t\tpkt.addr |= (u64)dev->dev_addr[i] << ((5 - i) * 8);\n\tif (vio_version_after(vio, 1, 3)) {\n\t\tif (port->rmtu) {\n\t\t\tport->rmtu = min(VNET_MAXPACKET, port->rmtu);\n\t\t\tpkt.mtu = port->rmtu;\n\t\t} else {\n\t\t\tport->rmtu = VNET_MAXPACKET;\n\t\t\tpkt.mtu = port->rmtu;\n\t\t}\n\t\tif (vio_version_after_eq(vio, 1, 6))\n\t\t\tpkt.options = VIO_TX_DRING;\n\t} else if (vio_version_before(vio, 1, 3)) {\n\t\tpkt.mtu = framelen;\n\t} else {  \n\t\tpkt.mtu = framelen + VLAN_HLEN;\n\t}\n\n\tpkt.cflags = 0;\n\tif (vio_version_after_eq(vio, 1, 7) && port->tso) {\n\t\tpkt.cflags |= VNET_LSO_IPV4_CAPAB;\n\t\tif (!port->tsolen)\n\t\t\tport->tsolen = VNET_MAXTSO;\n\t\tpkt.ipv4_lso_maxlen = port->tsolen;\n\t}\n\n\tpkt.plnk_updt = PHYSLINK_UPDATE_NONE;\n\n\tviodbg(HS, \"SEND NET ATTR xmode[0x%x] atype[0x%x] addr[%llx] \"\n\t       \"ackfreq[%u] plnk_updt[0x%02x] opts[0x%02x] mtu[%llu] \"\n\t       \"cflags[0x%04x] lso_max[%u]\\n\",\n\t       pkt.xfer_mode, pkt.addr_type,\n\t       (unsigned long long)pkt.addr,\n\t       pkt.ack_freq, pkt.plnk_updt, pkt.options,\n\t       (unsigned long long)pkt.mtu, pkt.cflags, pkt.ipv4_lso_maxlen);\n\n\treturn vio_ldc_send(vio, &pkt, sizeof(pkt));\n}\nEXPORT_SYMBOL_GPL(sunvnet_send_attr_common);\n\nstatic int handle_attr_info(struct vio_driver_state *vio,\n\t\t\t    struct vio_net_attr_info *pkt)\n{\n\tstruct vnet_port *port = to_vnet_port(vio);\n\tu64\tlocalmtu;\n\tu8\txfer_mode;\n\n\tviodbg(HS, \"GOT NET ATTR xmode[0x%x] atype[0x%x] addr[%llx] \"\n\t       \"ackfreq[%u] plnk_updt[0x%02x] opts[0x%02x] mtu[%llu] \"\n\t       \" (rmtu[%llu]) cflags[0x%04x] lso_max[%u]\\n\",\n\t       pkt->xfer_mode, pkt->addr_type,\n\t       (unsigned long long)pkt->addr,\n\t       pkt->ack_freq, pkt->plnk_updt, pkt->options,\n\t       (unsigned long long)pkt->mtu, port->rmtu, pkt->cflags,\n\t       pkt->ipv4_lso_maxlen);\n\n\tpkt->tag.sid = vio_send_sid(vio);\n\n\txfer_mode = pkt->xfer_mode;\n\t \n\tif (vio_version_before(vio, 1, 2) && xfer_mode == VIO_DRING_MODE)\n\t\txfer_mode = VIO_NEW_DRING_MODE;\n\n\t \n\tif (vio_version_before(vio, 1, 3)) {\n\t\tlocalmtu = ETH_FRAME_LEN;\n\t} else if (vio_version_after(vio, 1, 3)) {\n\t\tlocalmtu = port->rmtu ? port->rmtu : VNET_MAXPACKET;\n\t\tlocalmtu = min(pkt->mtu, localmtu);\n\t\tpkt->mtu = localmtu;\n\t} else {  \n\t\tlocalmtu = ETH_FRAME_LEN + VLAN_HLEN;\n\t}\n\tport->rmtu = localmtu;\n\n\t \n\tif (vio_version_after_eq(vio, 1, 7))\n\t\tport->tso &= !!(pkt->cflags & VNET_LSO_IPV4_CAPAB);\n\telse\n\t\tport->tso = false;\n\tif (port->tso) {\n\t\tif (!port->tsolen)\n\t\t\tport->tsolen = VNET_MAXTSO;\n\t\tport->tsolen = min(port->tsolen, pkt->ipv4_lso_maxlen);\n\t\tif (port->tsolen < VNET_MINTSO) {\n\t\t\tport->tso = false;\n\t\t\tport->tsolen = 0;\n\t\t\tpkt->cflags &= ~VNET_LSO_IPV4_CAPAB;\n\t\t}\n\t\tpkt->ipv4_lso_maxlen = port->tsolen;\n\t} else {\n\t\tpkt->cflags &= ~VNET_LSO_IPV4_CAPAB;\n\t\tpkt->ipv4_lso_maxlen = 0;\n\t\tport->tsolen = 0;\n\t}\n\n\t \n\tif (vio_version_after_eq(vio, 1, 6)) {\n\t\tpkt->xfer_mode = VIO_NEW_DRING_MODE;\n\t\tpkt->options = VIO_TX_DRING;\n\t}\n\n\tif (!(xfer_mode | VIO_NEW_DRING_MODE) ||\n\t    pkt->addr_type != VNET_ADDR_ETHERMAC ||\n\t    pkt->mtu != localmtu) {\n\t\tviodbg(HS, \"SEND NET ATTR NACK\\n\");\n\n\t\tpkt->tag.stype = VIO_SUBTYPE_NACK;\n\n\t\t(void)vio_ldc_send(vio, pkt, sizeof(*pkt));\n\n\t\treturn -ECONNRESET;\n\t}\n\n\tviodbg(HS, \"SEND NET ATTR ACK xmode[0x%x] atype[0x%x] \"\n\t       \"addr[%llx] ackfreq[%u] plnk_updt[0x%02x] opts[0x%02x] \"\n\t       \"mtu[%llu] (rmtu[%llu]) cflags[0x%04x] lso_max[%u]\\n\",\n\t       pkt->xfer_mode, pkt->addr_type,\n\t       (unsigned long long)pkt->addr,\n\t       pkt->ack_freq, pkt->plnk_updt, pkt->options,\n\t       (unsigned long long)pkt->mtu, port->rmtu, pkt->cflags,\n\t       pkt->ipv4_lso_maxlen);\n\n\tpkt->tag.stype = VIO_SUBTYPE_ACK;\n\n\treturn vio_ldc_send(vio, pkt, sizeof(*pkt));\n}\n\nstatic int handle_attr_ack(struct vio_driver_state *vio,\n\t\t\t   struct vio_net_attr_info *pkt)\n{\n\tviodbg(HS, \"GOT NET ATTR ACK\\n\");\n\n\treturn 0;\n}\n\nstatic int handle_attr_nack(struct vio_driver_state *vio,\n\t\t\t    struct vio_net_attr_info *pkt)\n{\n\tviodbg(HS, \"GOT NET ATTR NACK\\n\");\n\n\treturn -ECONNRESET;\n}\n\nint sunvnet_handle_attr_common(struct vio_driver_state *vio, void *arg)\n{\n\tstruct vio_net_attr_info *pkt = arg;\n\n\tswitch (pkt->tag.stype) {\n\tcase VIO_SUBTYPE_INFO:\n\t\treturn handle_attr_info(vio, pkt);\n\n\tcase VIO_SUBTYPE_ACK:\n\t\treturn handle_attr_ack(vio, pkt);\n\n\tcase VIO_SUBTYPE_NACK:\n\t\treturn handle_attr_nack(vio, pkt);\n\n\tdefault:\n\t\treturn -ECONNRESET;\n\t}\n}\nEXPORT_SYMBOL_GPL(sunvnet_handle_attr_common);\n\nvoid sunvnet_handshake_complete_common(struct vio_driver_state *vio)\n{\n\tstruct vio_dring_state *dr;\n\n\tdr = &vio->drings[VIO_DRIVER_RX_RING];\n\tdr->rcv_nxt = 1;\n\tdr->snd_nxt = 1;\n\n\tdr = &vio->drings[VIO_DRIVER_TX_RING];\n\tdr->rcv_nxt = 1;\n\tdr->snd_nxt = 1;\n}\nEXPORT_SYMBOL_GPL(sunvnet_handshake_complete_common);\n\n \nstatic struct sk_buff *alloc_and_align_skb(struct net_device *dev,\n\t\t\t\t\t   unsigned int len)\n{\n\tstruct sk_buff *skb;\n\tunsigned long addr, off;\n\n\tskb = netdev_alloc_skb(dev, len + VNET_PACKET_SKIP + 8 + 8);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\taddr = (unsigned long)skb->data;\n\toff = ((addr + 7UL) & ~7UL) - addr;\n\tif (off)\n\t\tskb_reserve(skb, off);\n\n\treturn skb;\n}\n\nstatic inline void vnet_fullcsum_ipv4(struct sk_buff *skb)\n{\n\tstruct iphdr *iph = ip_hdr(skb);\n\tint offset = skb_transport_offset(skb);\n\n\tif (skb->protocol != htons(ETH_P_IP))\n\t\treturn;\n\tif (iph->protocol != IPPROTO_TCP &&\n\t    iph->protocol != IPPROTO_UDP)\n\t\treturn;\n\tskb->ip_summed = CHECKSUM_NONE;\n\tskb->csum_level = 1;\n\tskb->csum = 0;\n\tif (iph->protocol == IPPROTO_TCP) {\n\t\tstruct tcphdr *ptcp = tcp_hdr(skb);\n\n\t\tptcp->check = 0;\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\t\tptcp->check = csum_tcpudp_magic(iph->saddr, iph->daddr,\n\t\t\t\t\t\tskb->len - offset, IPPROTO_TCP,\n\t\t\t\t\t\tskb->csum);\n\t} else if (iph->protocol == IPPROTO_UDP) {\n\t\tstruct udphdr *pudp = udp_hdr(skb);\n\n\t\tpudp->check = 0;\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\t\tpudp->check = csum_tcpudp_magic(iph->saddr, iph->daddr,\n\t\t\t\t\t\tskb->len - offset, IPPROTO_UDP,\n\t\t\t\t\t\tskb->csum);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic inline void vnet_fullcsum_ipv6(struct sk_buff *skb)\n{\n\tstruct ipv6hdr *ip6h = ipv6_hdr(skb);\n\tint offset = skb_transport_offset(skb);\n\n\tif (skb->protocol != htons(ETH_P_IPV6))\n\t\treturn;\n\tif (ip6h->nexthdr != IPPROTO_TCP &&\n\t    ip6h->nexthdr != IPPROTO_UDP)\n\t\treturn;\n\tskb->ip_summed = CHECKSUM_NONE;\n\tskb->csum_level = 1;\n\tskb->csum = 0;\n\tif (ip6h->nexthdr == IPPROTO_TCP) {\n\t\tstruct tcphdr *ptcp = tcp_hdr(skb);\n\n\t\tptcp->check = 0;\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\t\tptcp->check = csum_ipv6_magic(&ip6h->saddr, &ip6h->daddr,\n\t\t\t\t\t      skb->len - offset, IPPROTO_TCP,\n\t\t\t\t\t      skb->csum);\n\t} else if (ip6h->nexthdr == IPPROTO_UDP) {\n\t\tstruct udphdr *pudp = udp_hdr(skb);\n\n\t\tpudp->check = 0;\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\t\tpudp->check = csum_ipv6_magic(&ip6h->saddr, &ip6h->daddr,\n\t\t\t\t\t      skb->len - offset, IPPROTO_UDP,\n\t\t\t\t\t      skb->csum);\n\t}\n}\n#endif\n\nstatic int vnet_rx_one(struct vnet_port *port, struct vio_net_desc *desc)\n{\n\tstruct net_device *dev = VNET_PORT_TO_NET_DEVICE(port);\n\tunsigned int len = desc->size;\n\tunsigned int copy_len;\n\tstruct sk_buff *skb;\n\tint maxlen;\n\tint err;\n\n\terr = -EMSGSIZE;\n\tif (port->tso && port->tsolen > port->rmtu)\n\t\tmaxlen = port->tsolen;\n\telse\n\t\tmaxlen = port->rmtu;\n\tif (unlikely(len < ETH_ZLEN || len > maxlen)) {\n\t\tdev->stats.rx_length_errors++;\n\t\tgoto out_dropped;\n\t}\n\n\tskb = alloc_and_align_skb(dev, len);\n\terr = -ENOMEM;\n\tif (unlikely(!skb)) {\n\t\tdev->stats.rx_missed_errors++;\n\t\tgoto out_dropped;\n\t}\n\n\tcopy_len = (len + VNET_PACKET_SKIP + 7U) & ~7U;\n\tskb_put(skb, copy_len);\n\terr = ldc_copy(port->vio.lp, LDC_COPY_IN,\n\t\t       skb->data, copy_len, 0,\n\t\t       desc->cookies, desc->ncookies);\n\tif (unlikely(err < 0)) {\n\t\tdev->stats.rx_frame_errors++;\n\t\tgoto out_free_skb;\n\t}\n\n\tskb_pull(skb, VNET_PACKET_SKIP);\n\tskb_trim(skb, len);\n\tskb->protocol = eth_type_trans(skb, dev);\n\n\tif (vio_version_after_eq(&port->vio, 1, 8)) {\n\t\tstruct vio_net_dext *dext = vio_net_ext(desc);\n\n\t\tskb_reset_network_header(skb);\n\n\t\tif (dext->flags & VNET_PKT_HCK_IPV4_HDRCKSUM) {\n\t\t\tif (skb->protocol == ETH_P_IP) {\n\t\t\t\tstruct iphdr *iph = ip_hdr(skb);\n\n\t\t\t\tiph->check = 0;\n\t\t\t\tip_send_check(iph);\n\t\t\t}\n\t\t}\n\t\tif ((dext->flags & VNET_PKT_HCK_FULLCKSUM) &&\n\t\t    skb->ip_summed == CHECKSUM_NONE) {\n\t\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\t\tstruct iphdr *iph = ip_hdr(skb);\n\t\t\t\tint ihl = iph->ihl * 4;\n\n\t\t\t\tskb_set_transport_header(skb, ihl);\n\t\t\t\tvnet_fullcsum_ipv4(skb);\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t sizeof(struct ipv6hdr));\n\t\t\t\tvnet_fullcsum_ipv6(skb);\n#endif\n\t\t\t}\n\t\t}\n\t\tif (dext->flags & VNET_PKT_HCK_IPV4_HDRCKSUM_OK) {\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\t\tskb->csum_level = 0;\n\t\t\tif (dext->flags & VNET_PKT_HCK_FULLCKSUM_OK)\n\t\t\t\tskb->csum_level = 1;\n\t\t}\n\t}\n\n\tskb->ip_summed = port->switch_port ? CHECKSUM_NONE : CHECKSUM_PARTIAL;\n\n\tif (unlikely(is_multicast_ether_addr(eth_hdr(skb)->h_dest)))\n\t\tdev->stats.multicast++;\n\tdev->stats.rx_packets++;\n\tdev->stats.rx_bytes += len;\n\tport->stats.rx_packets++;\n\tport->stats.rx_bytes += len;\n\tnapi_gro_receive(&port->napi, skb);\n\treturn 0;\n\nout_free_skb:\n\tkfree_skb(skb);\n\nout_dropped:\n\tdev->stats.rx_dropped++;\n\treturn err;\n}\n\nstatic int vnet_send_ack(struct vnet_port *port, struct vio_dring_state *dr,\n\t\t\t u32 start, u32 end, u8 vio_dring_state)\n{\n\tstruct vio_dring_data hdr = {\n\t\t.tag = {\n\t\t\t.type\t\t= VIO_TYPE_DATA,\n\t\t\t.stype\t\t= VIO_SUBTYPE_ACK,\n\t\t\t.stype_env\t= VIO_DRING_DATA,\n\t\t\t.sid\t\t= vio_send_sid(&port->vio),\n\t\t},\n\t\t.dring_ident\t\t= dr->ident,\n\t\t.start_idx\t\t= start,\n\t\t.end_idx\t\t= end,\n\t\t.state\t\t\t= vio_dring_state,\n\t};\n\tint err, delay;\n\tint retries = 0;\n\n\thdr.seq = dr->snd_nxt;\n\tdelay = 1;\n\tdo {\n\t\terr = vio_ldc_send(&port->vio, &hdr, sizeof(hdr));\n\t\tif (err > 0) {\n\t\t\tdr->snd_nxt++;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(delay);\n\t\tif ((delay <<= 1) > 128)\n\t\t\tdelay = 128;\n\t\tif (retries++ > VNET_MAX_RETRIES) {\n\t\t\tpr_info(\"ECONNRESET %x:%x:%x:%x:%x:%x\\n\",\n\t\t\t\tport->raddr[0], port->raddr[1],\n\t\t\t\tport->raddr[2], port->raddr[3],\n\t\t\t\tport->raddr[4], port->raddr[5]);\n\t\t\tbreak;\n\t\t}\n\t} while (err == -EAGAIN);\n\n\tif (err <= 0 && vio_dring_state == VIO_DRING_STOPPED) {\n\t\tport->stop_rx_idx = end;\n\t\tport->stop_rx = true;\n\t} else {\n\t\tport->stop_rx_idx = 0;\n\t\tport->stop_rx = false;\n\t}\n\n\treturn err;\n}\n\nstatic struct vio_net_desc *get_rx_desc(struct vnet_port *port,\n\t\t\t\t\tstruct vio_dring_state *dr,\n\t\t\t\t\tu32 index)\n{\n\tstruct vio_net_desc *desc = port->vio.desc_buf;\n\tint err;\n\n\terr = ldc_get_dring_entry(port->vio.lp, desc, dr->entry_size,\n\t\t\t\t  (index * dr->entry_size),\n\t\t\t\t  dr->cookies, dr->ncookies);\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\n\treturn desc;\n}\n\nstatic int put_rx_desc(struct vnet_port *port,\n\t\t       struct vio_dring_state *dr,\n\t\t       struct vio_net_desc *desc,\n\t\t       u32 index)\n{\n\tint err;\n\n\terr = ldc_put_dring_entry(port->vio.lp, desc, dr->entry_size,\n\t\t\t\t  (index * dr->entry_size),\n\t\t\t\t  dr->cookies, dr->ncookies);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic int vnet_walk_rx_one(struct vnet_port *port,\n\t\t\t    struct vio_dring_state *dr,\n\t\t\t    u32 index, int *needs_ack)\n{\n\tstruct vio_net_desc *desc = get_rx_desc(port, dr, index);\n\tstruct vio_driver_state *vio = &port->vio;\n\tint err;\n\n\tBUG_ON(!desc);\n\tif (IS_ERR(desc))\n\t\treturn PTR_ERR(desc);\n\n\tif (desc->hdr.state != VIO_DESC_READY)\n\t\treturn 1;\n\n\tdma_rmb();\n\n\tviodbg(DATA, \"vio_walk_rx_one desc[%02x:%02x:%08x:%08x:%llx:%llx]\\n\",\n\t       desc->hdr.state, desc->hdr.ack,\n\t       desc->size, desc->ncookies,\n\t       desc->cookies[0].cookie_addr,\n\t       desc->cookies[0].cookie_size);\n\n\terr = vnet_rx_one(port, desc);\n\tif (err == -ECONNRESET)\n\t\treturn err;\n\ttrace_vnet_rx_one(port->vio._local_sid, port->vio._peer_sid,\n\t\t\t  index, desc->hdr.ack);\n\tdesc->hdr.state = VIO_DESC_DONE;\n\terr = put_rx_desc(port, dr, desc, index);\n\tif (err < 0)\n\t\treturn err;\n\t*needs_ack = desc->hdr.ack;\n\treturn 0;\n}\n\nstatic int vnet_walk_rx(struct vnet_port *port, struct vio_dring_state *dr,\n\t\t\tu32 start, u32 end, int *npkts, int budget)\n{\n\tstruct vio_driver_state *vio = &port->vio;\n\tint ack_start = -1, ack_end = -1;\n\tbool send_ack = true;\n\n\tend = (end == (u32)-1) ? vio_dring_prev(dr, start)\n\t\t\t       : vio_dring_next(dr, end);\n\n\tviodbg(DATA, \"vnet_walk_rx start[%08x] end[%08x]\\n\", start, end);\n\n\twhile (start != end) {\n\t\tint ack = 0, err = vnet_walk_rx_one(port, dr, start, &ack);\n\n\t\tif (err == -ECONNRESET)\n\t\t\treturn err;\n\t\tif (err != 0)\n\t\t\tbreak;\n\t\t(*npkts)++;\n\t\tif (ack_start == -1)\n\t\t\tack_start = start;\n\t\tack_end = start;\n\t\tstart = vio_dring_next(dr, start);\n\t\tif (ack && start != end) {\n\t\t\terr = vnet_send_ack(port, dr, ack_start, ack_end,\n\t\t\t\t\t    VIO_DRING_ACTIVE);\n\t\t\tif (err == -ECONNRESET)\n\t\t\t\treturn err;\n\t\t\tack_start = -1;\n\t\t}\n\t\tif ((*npkts) >= budget) {\n\t\t\tsend_ack = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (unlikely(ack_start == -1)) {\n\t\tack_end = vio_dring_prev(dr, start);\n\t\tack_start = ack_end;\n\t}\n\tif (send_ack) {\n\t\tport->napi_resume = false;\n\t\ttrace_vnet_tx_send_stopped_ack(port->vio._local_sid,\n\t\t\t\t\t       port->vio._peer_sid,\n\t\t\t\t\t       ack_end, *npkts);\n\t\treturn vnet_send_ack(port, dr, ack_start, ack_end,\n\t\t\t\t     VIO_DRING_STOPPED);\n\t} else  {\n\t\ttrace_vnet_tx_defer_stopped_ack(port->vio._local_sid,\n\t\t\t\t\t\tport->vio._peer_sid,\n\t\t\t\t\t\tack_end, *npkts);\n\t\tport->napi_resume = true;\n\t\tport->napi_stop_idx = ack_end;\n\t\treturn 1;\n\t}\n}\n\nstatic int vnet_rx(struct vnet_port *port, void *msgbuf, int *npkts,\n\t\t   int budget)\n{\n\tstruct vio_dring_data *pkt = msgbuf;\n\tstruct vio_dring_state *dr = &port->vio.drings[VIO_DRIVER_RX_RING];\n\tstruct vio_driver_state *vio = &port->vio;\n\n\tviodbg(DATA, \"vnet_rx stype_env[%04x] seq[%016llx] rcv_nxt[%016llx]\\n\",\n\t       pkt->tag.stype_env, pkt->seq, dr->rcv_nxt);\n\n\tif (unlikely(pkt->tag.stype_env != VIO_DRING_DATA))\n\t\treturn 0;\n\tif (unlikely(pkt->seq != dr->rcv_nxt)) {\n\t\tpr_err(\"RX out of sequence seq[0x%llx] rcv_nxt[0x%llx]\\n\",\n\t\t       pkt->seq, dr->rcv_nxt);\n\t\treturn 0;\n\t}\n\n\tif (!port->napi_resume)\n\t\tdr->rcv_nxt++;\n\n\t \n\n\treturn vnet_walk_rx(port, dr, pkt->start_idx, pkt->end_idx,\n\t\t\t    npkts, budget);\n}\n\nstatic int idx_is_pending(struct vio_dring_state *dr, u32 end)\n{\n\tu32 idx = dr->cons;\n\tint found = 0;\n\n\twhile (idx != dr->prod) {\n\t\tif (idx == end) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t\tidx = vio_dring_next(dr, idx);\n\t}\n\treturn found;\n}\n\nstatic int vnet_ack(struct vnet_port *port, void *msgbuf)\n{\n\tstruct vio_dring_state *dr = &port->vio.drings[VIO_DRIVER_TX_RING];\n\tstruct vio_dring_data *pkt = msgbuf;\n\tstruct net_device *dev;\n\tu32 end;\n\tstruct vio_net_desc *desc;\n\tstruct netdev_queue *txq;\n\n\tif (unlikely(pkt->tag.stype_env != VIO_DRING_DATA))\n\t\treturn 0;\n\n\tend = pkt->end_idx;\n\tdev = VNET_PORT_TO_NET_DEVICE(port);\n\tnetif_tx_lock(dev);\n\tif (unlikely(!idx_is_pending(dr, end))) {\n\t\tnetif_tx_unlock(dev);\n\t\treturn 0;\n\t}\n\n\t \n\ttrace_vnet_rx_stopped_ack(port->vio._local_sid,\n\t\t\t\t  port->vio._peer_sid, end);\n\tdr->cons = vio_dring_next(dr, end);\n\tdesc = vio_dring_entry(dr, dr->cons);\n\tif (desc->hdr.state == VIO_DESC_READY && !port->start_cons) {\n\t\t \n\t\tif (__vnet_tx_trigger(port, dr->cons) > 0)\n\t\t\tport->start_cons = false;\n\t\telse\n\t\t\tport->start_cons = true;\n\t} else {\n\t\tport->start_cons = true;\n\t}\n\tnetif_tx_unlock(dev);\n\n\ttxq = netdev_get_tx_queue(dev, port->q_index);\n\tif (unlikely(netif_tx_queue_stopped(txq) &&\n\t\t     vnet_tx_dring_avail(dr) >= VNET_TX_WAKEUP_THRESH(dr)))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int vnet_nack(struct vnet_port *port, void *msgbuf)\n{\n\t \n\treturn 0;\n}\n\nstatic int handle_mcast(struct vnet_port *port, void *msgbuf)\n{\n\tstruct vio_net_mcast_info *pkt = msgbuf;\n\tstruct net_device *dev = VNET_PORT_TO_NET_DEVICE(port);\n\n\tif (pkt->tag.stype != VIO_SUBTYPE_ACK)\n\t\tpr_err(\"%s: Got unexpected MCAST reply [%02x:%02x:%04x:%08x]\\n\",\n\t\t       dev->name,\n\t\t       pkt->tag.type,\n\t\t       pkt->tag.stype,\n\t\t       pkt->tag.stype_env,\n\t\t       pkt->tag.sid);\n\n\treturn 0;\n}\n\n \nstatic void maybe_tx_wakeup(struct vnet_port *port)\n{\n\tstruct netdev_queue *txq;\n\n\ttxq = netdev_get_tx_queue(VNET_PORT_TO_NET_DEVICE(port),\n\t\t\t\t  port->q_index);\n\t__netif_tx_lock(txq, smp_processor_id());\n\tif (likely(netif_tx_queue_stopped(txq)))\n\t\tnetif_tx_wake_queue(txq);\n\t__netif_tx_unlock(txq);\n}\n\nbool sunvnet_port_is_up_common(struct vnet_port *vnet)\n{\n\tstruct vio_driver_state *vio = &vnet->vio;\n\n\treturn !!(vio->hs_state & VIO_HS_COMPLETE);\n}\nEXPORT_SYMBOL_GPL(sunvnet_port_is_up_common);\n\nstatic int vnet_event_napi(struct vnet_port *port, int budget)\n{\n\tstruct net_device *dev = VNET_PORT_TO_NET_DEVICE(port);\n\tstruct vio_driver_state *vio = &port->vio;\n\tint tx_wakeup, err;\n\tint npkts = 0;\n\n\t \n\tBUG_ON(port->rx_event & ~(LDC_EVENT_DATA_READY |\n\t\t\t\t  LDC_EVENT_RESET |\n\t\t\t\t  LDC_EVENT_UP));\n\n\t \n\tif (port->rx_event & LDC_EVENT_RESET) {\n\t\t \n\n\t\tif (port->vsw == 1) {\n\t\t\tnetif_tx_stop_all_queues(dev);\n\t\t\tnetif_carrier_off(dev);\n\t\t}\n\n\t\tvio_link_state_change(vio, LDC_EVENT_RESET);\n\t\tvnet_port_reset(port);\n\t\tvio_port_up(vio);\n\n\t\t \n\t\tif (netif_running(dev))\n\t\t\tmaybe_tx_wakeup(port);\n\n\t\tport->rx_event = 0;\n\t\tport->stats.event_reset++;\n\t\treturn 0;\n\t}\n\n\tif (port->rx_event & LDC_EVENT_UP) {\n\t\t \n\n\t\tif (port->vsw == 1) {\n\t\t\tnetif_carrier_on(port->dev);\n\t\t\tnetif_tx_start_all_queues(port->dev);\n\t\t}\n\n\t\tvio_link_state_change(vio, LDC_EVENT_UP);\n\t\tport->rx_event = 0;\n\t\tport->stats.event_up++;\n\t\treturn 0;\n\t}\n\n\terr = 0;\n\ttx_wakeup = 0;\n\twhile (1) {\n\t\tunion {\n\t\t\tstruct vio_msg_tag tag;\n\t\t\tu64 raw[8];\n\t\t} msgbuf;\n\n\t\tif (port->napi_resume) {\n\t\t\tstruct vio_dring_data *pkt =\n\t\t\t\t(struct vio_dring_data *)&msgbuf;\n\t\t\tstruct vio_dring_state *dr =\n\t\t\t\t&port->vio.drings[VIO_DRIVER_RX_RING];\n\n\t\t\tpkt->tag.type = VIO_TYPE_DATA;\n\t\t\tpkt->tag.stype = VIO_SUBTYPE_INFO;\n\t\t\tpkt->tag.stype_env = VIO_DRING_DATA;\n\t\t\tpkt->seq = dr->rcv_nxt;\n\t\t\tpkt->start_idx = vio_dring_next(dr,\n\t\t\t\t\t\t\tport->napi_stop_idx);\n\t\t\tpkt->end_idx = -1;\n\t\t} else {\n\t\t\terr = ldc_read(vio->lp, &msgbuf, sizeof(msgbuf));\n\t\t\tif (unlikely(err < 0)) {\n\t\t\t\tif (err == -ECONNRESET)\n\t\t\t\t\tvio_conn_reset(vio);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (err == 0)\n\t\t\t\tbreak;\n\t\t\tviodbg(DATA, \"TAG [%02x:%02x:%04x:%08x]\\n\",\n\t\t\t       msgbuf.tag.type,\n\t\t\t       msgbuf.tag.stype,\n\t\t\t       msgbuf.tag.stype_env,\n\t\t\t       msgbuf.tag.sid);\n\t\t\terr = vio_validate_sid(vio, &msgbuf.tag);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (likely(msgbuf.tag.type == VIO_TYPE_DATA)) {\n\t\t\tif (msgbuf.tag.stype == VIO_SUBTYPE_INFO) {\n\t\t\t\tif (!sunvnet_port_is_up_common(port)) {\n\t\t\t\t\t \n\t\t\t\t\terr = -ECONNRESET;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\terr = vnet_rx(port, &msgbuf, &npkts, budget);\n\t\t\t\tif (npkts >= budget)\n\t\t\t\t\tbreak;\n\t\t\t\tif (npkts == 0)\n\t\t\t\t\tbreak;\n\t\t\t} else if (msgbuf.tag.stype == VIO_SUBTYPE_ACK) {\n\t\t\t\terr = vnet_ack(port, &msgbuf);\n\t\t\t\tif (err > 0)\n\t\t\t\t\ttx_wakeup |= err;\n\t\t\t} else if (msgbuf.tag.stype == VIO_SUBTYPE_NACK) {\n\t\t\t\terr = vnet_nack(port, &msgbuf);\n\t\t\t}\n\t\t} else if (msgbuf.tag.type == VIO_TYPE_CTRL) {\n\t\t\tif (msgbuf.tag.stype_env == VNET_MCAST_INFO)\n\t\t\t\terr = handle_mcast(port, &msgbuf);\n\t\t\telse\n\t\t\t\terr = vio_control_pkt_engine(vio, &msgbuf);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\terr = vnet_handle_unknown(port, &msgbuf);\n\t\t}\n\t\tif (err == -ECONNRESET)\n\t\t\tbreak;\n\t}\n\tif (unlikely(tx_wakeup && err != -ECONNRESET))\n\t\tmaybe_tx_wakeup(port);\n\treturn npkts;\n}\n\nint sunvnet_poll_common(struct napi_struct *napi, int budget)\n{\n\tstruct vnet_port *port = container_of(napi, struct vnet_port, napi);\n\tstruct vio_driver_state *vio = &port->vio;\n\tint processed = vnet_event_napi(port, budget);\n\n\tif (processed < budget) {\n\t\tnapi_complete_done(napi, processed);\n\t\tport->rx_event &= ~LDC_EVENT_DATA_READY;\n\t\tvio_set_intr(vio->vdev->rx_ino, HV_INTR_ENABLED);\n\t}\n\treturn processed;\n}\nEXPORT_SYMBOL_GPL(sunvnet_poll_common);\n\nvoid sunvnet_event_common(void *arg, int event)\n{\n\tstruct vnet_port *port = arg;\n\tstruct vio_driver_state *vio = &port->vio;\n\n\tport->rx_event |= event;\n\tvio_set_intr(vio->vdev->rx_ino, HV_INTR_DISABLED);\n\tnapi_schedule(&port->napi);\n}\nEXPORT_SYMBOL_GPL(sunvnet_event_common);\n\nstatic int __vnet_tx_trigger(struct vnet_port *port, u32 start)\n{\n\tstruct vio_dring_state *dr = &port->vio.drings[VIO_DRIVER_TX_RING];\n\tstruct vio_dring_data hdr = {\n\t\t.tag = {\n\t\t\t.type\t\t= VIO_TYPE_DATA,\n\t\t\t.stype\t\t= VIO_SUBTYPE_INFO,\n\t\t\t.stype_env\t= VIO_DRING_DATA,\n\t\t\t.sid\t\t= vio_send_sid(&port->vio),\n\t\t},\n\t\t.dring_ident\t\t= dr->ident,\n\t\t.start_idx\t\t= start,\n\t\t.end_idx\t\t= (u32)-1,\n\t};\n\tint err, delay;\n\tint retries = 0;\n\n\tif (port->stop_rx) {\n\t\ttrace_vnet_tx_pending_stopped_ack(port->vio._local_sid,\n\t\t\t\t\t\t  port->vio._peer_sid,\n\t\t\t\t\t\t  port->stop_rx_idx, -1);\n\t\terr = vnet_send_ack(port,\n\t\t\t\t    &port->vio.drings[VIO_DRIVER_RX_RING],\n\t\t\t\t    port->stop_rx_idx, -1,\n\t\t\t\t    VIO_DRING_STOPPED);\n\t\tif (err <= 0)\n\t\t\treturn err;\n\t}\n\n\thdr.seq = dr->snd_nxt;\n\tdelay = 1;\n\tdo {\n\t\terr = vio_ldc_send(&port->vio, &hdr, sizeof(hdr));\n\t\tif (err > 0) {\n\t\t\tdr->snd_nxt++;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(delay);\n\t\tif ((delay <<= 1) > 128)\n\t\t\tdelay = 128;\n\t\tif (retries++ > VNET_MAX_RETRIES)\n\t\t\tbreak;\n\t} while (err == -EAGAIN);\n\ttrace_vnet_tx_trigger(port->vio._local_sid,\n\t\t\t      port->vio._peer_sid, start, err);\n\n\treturn err;\n}\n\nstatic struct sk_buff *vnet_clean_tx_ring(struct vnet_port *port,\n\t\t\t\t\t  unsigned *pending)\n{\n\tstruct vio_dring_state *dr = &port->vio.drings[VIO_DRIVER_TX_RING];\n\tstruct sk_buff *skb = NULL;\n\tint i, txi;\n\n\t*pending = 0;\n\n\ttxi = dr->prod;\n\tfor (i = 0; i < VNET_TX_RING_SIZE; ++i) {\n\t\tstruct vio_net_desc *d;\n\n\t\t--txi;\n\t\tif (txi < 0)\n\t\t\ttxi = VNET_TX_RING_SIZE - 1;\n\n\t\td = vio_dring_entry(dr, txi);\n\n\t\tif (d->hdr.state == VIO_DESC_READY) {\n\t\t\t(*pending)++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (port->tx_bufs[txi].skb) {\n\t\t\tif (d->hdr.state != VIO_DESC_DONE)\n\t\t\t\tpr_notice(\"invalid ring buffer state %d\\n\",\n\t\t\t\t\t  d->hdr.state);\n\t\t\tBUG_ON(port->tx_bufs[txi].skb->next);\n\n\t\t\tport->tx_bufs[txi].skb->next = skb;\n\t\t\tskb = port->tx_bufs[txi].skb;\n\t\t\tport->tx_bufs[txi].skb = NULL;\n\n\t\t\tldc_unmap(port->vio.lp,\n\t\t\t\t  port->tx_bufs[txi].cookies,\n\t\t\t\t  port->tx_bufs[txi].ncookies);\n\t\t} else if (d->hdr.state == VIO_DESC_FREE) {\n\t\t\tbreak;\n\t\t}\n\t\td->hdr.state = VIO_DESC_FREE;\n\t}\n\treturn skb;\n}\n\nstatic inline void vnet_free_skbs(struct sk_buff *skb)\n{\n\tstruct sk_buff *next;\n\n\twhile (skb) {\n\t\tnext = skb->next;\n\t\tskb->next = NULL;\n\t\tdev_kfree_skb(skb);\n\t\tskb = next;\n\t}\n}\n\nvoid sunvnet_clean_timer_expire_common(struct timer_list *t)\n{\n\tstruct vnet_port *port = from_timer(port, t, clean_timer);\n\tstruct sk_buff *freeskbs;\n\tunsigned pending;\n\n\tnetif_tx_lock(VNET_PORT_TO_NET_DEVICE(port));\n\tfreeskbs = vnet_clean_tx_ring(port, &pending);\n\tnetif_tx_unlock(VNET_PORT_TO_NET_DEVICE(port));\n\n\tvnet_free_skbs(freeskbs);\n\n\tif (pending)\n\t\t(void)mod_timer(&port->clean_timer,\n\t\t\t\tjiffies + VNET_CLEAN_TIMEOUT);\n\t else\n\t\tdel_timer(&port->clean_timer);\n}\nEXPORT_SYMBOL_GPL(sunvnet_clean_timer_expire_common);\n\nstatic inline int vnet_skb_map(struct ldc_channel *lp, struct sk_buff *skb,\n\t\t\t       struct ldc_trans_cookie *cookies, int ncookies,\n\t\t\t       unsigned int map_perm)\n{\n\tint i, nc, err, blen;\n\n\t \n\tblen = skb_headlen(skb);\n\tif (blen < ETH_ZLEN)\n\t\tblen = ETH_ZLEN;\n\tblen += VNET_PACKET_SKIP;\n\tblen += 8 - (blen & 7);\n\n\terr = ldc_map_single(lp, skb->data - VNET_PACKET_SKIP, blen, cookies,\n\t\t\t     ncookies, map_perm);\n\tif (err < 0)\n\t\treturn err;\n\tnc = err;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\t\tu8 *vaddr;\n\n\t\tif (nc < ncookies) {\n\t\t\tvaddr = kmap_local_page(skb_frag_page(f));\n\t\t\tblen = skb_frag_size(f);\n\t\t\tblen += 8 - (blen & 7);\n\t\t\terr = ldc_map_single(lp, vaddr + skb_frag_off(f),\n\t\t\t\t\t     blen, cookies + nc, ncookies - nc,\n\t\t\t\t\t     map_perm);\n\t\t\tkunmap_local(vaddr);\n\t\t} else {\n\t\t\terr = -EMSGSIZE;\n\t\t}\n\n\t\tif (err < 0) {\n\t\t\tldc_unmap(lp, cookies, nc);\n\t\t\treturn err;\n\t\t}\n\t\tnc += err;\n\t}\n\treturn nc;\n}\n\nstatic inline struct sk_buff *vnet_skb_shape(struct sk_buff *skb, int ncookies)\n{\n\tstruct sk_buff *nskb;\n\tint i, len, pad, docopy;\n\n\tlen = skb->len;\n\tpad = 0;\n\tif (len < ETH_ZLEN) {\n\t\tpad += ETH_ZLEN - skb->len;\n\t\tlen += pad;\n\t}\n\tlen += VNET_PACKET_SKIP;\n\tpad += 8 - (len & 7);\n\n\t \n\tdocopy = skb_shinfo(skb)->nr_frags >= ncookies;\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *f = &skb_shinfo(skb)->frags[i];\n\n\t\tdocopy |= skb_frag_off(f) & 7;\n\t}\n\tif (((unsigned long)skb->data & 7) != VNET_PACKET_SKIP ||\n\t    skb_tailroom(skb) < pad ||\n\t    skb_headroom(skb) < VNET_PACKET_SKIP || docopy) {\n\t\tint start = 0, offset;\n\t\t__wsum csum;\n\n\t\tlen = skb->len > ETH_ZLEN ? skb->len : ETH_ZLEN;\n\t\tnskb = alloc_and_align_skb(skb->dev, len);\n\t\tif (!nskb) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NULL;\n\t\t}\n\t\tskb_reserve(nskb, VNET_PACKET_SKIP);\n\n\t\tnskb->protocol = skb->protocol;\n\t\toffset = skb_mac_header(skb) - skb->data;\n\t\tskb_set_mac_header(nskb, offset);\n\t\toffset = skb_network_header(skb) - skb->data;\n\t\tskb_set_network_header(nskb, offset);\n\t\toffset = skb_transport_header(skb) - skb->data;\n\t\tskb_set_transport_header(nskb, offset);\n\n\t\toffset = 0;\n\t\tnskb->csum_offset = skb->csum_offset;\n\t\tnskb->ip_summed = skb->ip_summed;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\tstart = skb_checksum_start_offset(skb);\n\t\tif (start) {\n\t\t\tint offset = start + nskb->csum_offset;\n\n\t\t\t \n\t\t\tif (skb_copy_bits(skb, 0, nskb->data, start)) {\n\t\t\t\tdev_kfree_skb(nskb);\n\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\treturn NULL;\n\t\t\t}\n\n\t\t\t \n\t\t\t*(__sum16 *)(skb->data + offset) = 0;\n\t\t\tcsum = skb_copy_and_csum_bits(skb, start,\n\t\t\t\t\t\t      nskb->data + start,\n\t\t\t\t\t\t      skb->len - start);\n\n\t\t\t \n\t\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\t\tstruct iphdr *iph = ip_hdr(nskb);\n\n\t\t\t\tif (iph->protocol == IPPROTO_TCP ||\n\t\t\t\t    iph->protocol == IPPROTO_UDP) {\n\t\t\t\t\tcsum = csum_tcpudp_magic(iph->saddr,\n\t\t\t\t\t\t\t\t iph->daddr,\n\t\t\t\t\t\t\t\t skb->len - start,\n\t\t\t\t\t\t\t\t iph->protocol,\n\t\t\t\t\t\t\t\t csum);\n\t\t\t\t}\n\t\t\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\t\tstruct ipv6hdr *ip6h = ipv6_hdr(nskb);\n\n\t\t\t\tif (ip6h->nexthdr == IPPROTO_TCP ||\n\t\t\t\t    ip6h->nexthdr == IPPROTO_UDP) {\n\t\t\t\t\tcsum = csum_ipv6_magic(&ip6h->saddr,\n\t\t\t\t\t\t\t       &ip6h->daddr,\n\t\t\t\t\t\t\t       skb->len - start,\n\t\t\t\t\t\t\t       ip6h->nexthdr,\n\t\t\t\t\t\t\t       csum);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\t*(__sum16 *)(nskb->data + offset) = csum;\n\n\t\t\tnskb->ip_summed = CHECKSUM_NONE;\n\t\t} else if (skb_copy_bits(skb, 0, nskb->data, skb->len)) {\n\t\t\tdev_kfree_skb(nskb);\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NULL;\n\t\t}\n\t\t(void)skb_put(nskb, skb->len);\n\t\tif (skb_is_gso(skb)) {\n\t\t\tskb_shinfo(nskb)->gso_size = skb_shinfo(skb)->gso_size;\n\t\t\tskb_shinfo(nskb)->gso_type = skb_shinfo(skb)->gso_type;\n\t\t}\n\t\tnskb->queue_mapping = skb->queue_mapping;\n\t\tdev_kfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\nstatic netdev_tx_t\nvnet_handle_offloads(struct vnet_port *port, struct sk_buff *skb,\n\t\t     struct vnet_port *(*vnet_tx_port)\n\t\t     (struct sk_buff *, struct net_device *))\n{\n\tstruct net_device *dev = VNET_PORT_TO_NET_DEVICE(port);\n\tstruct vio_dring_state *dr = &port->vio.drings[VIO_DRIVER_TX_RING];\n\tstruct sk_buff *segs, *curr, *next;\n\tint maclen, datalen;\n\tint status;\n\tint gso_size, gso_type, gso_segs;\n\tint hlen = skb_transport_header(skb) - skb_mac_header(skb);\n\tint proto = IPPROTO_IP;\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\tproto = ip_hdr(skb)->protocol;\n\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\tproto = ipv6_hdr(skb)->nexthdr;\n\n\tif (proto == IPPROTO_TCP) {\n\t\thlen += tcp_hdr(skb)->doff * 4;\n\t} else if (proto == IPPROTO_UDP) {\n\t\thlen += sizeof(struct udphdr);\n\t} else {\n\t\tpr_err(\"vnet_handle_offloads GSO with unknown transport \"\n\t\t       \"protocol %d tproto %d\\n\", skb->protocol, proto);\n\t\thlen = 128;  \n\t}\n\tdatalen = port->tsolen - hlen;\n\n\tgso_size = skb_shinfo(skb)->gso_size;\n\tgso_type = skb_shinfo(skb)->gso_type;\n\tgso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (port->tso && gso_size < datalen)\n\t\tgso_segs = DIV_ROUND_UP(skb->len - hlen, datalen);\n\n\tif (unlikely(vnet_tx_dring_avail(dr) < gso_segs)) {\n\t\tstruct netdev_queue *txq;\n\n\t\ttxq  = netdev_get_tx_queue(dev, port->q_index);\n\t\tnetif_tx_stop_queue(txq);\n\t\tif (vnet_tx_dring_avail(dr) < skb_shinfo(skb)->gso_segs)\n\t\t\treturn NETDEV_TX_BUSY;\n\t\tnetif_tx_wake_queue(txq);\n\t}\n\n\tmaclen = skb_network_header(skb) - skb_mac_header(skb);\n\tskb_pull(skb, maclen);\n\n\tif (port->tso && gso_size < datalen) {\n\t\tif (skb_unclone(skb, GFP_ATOMIC))\n\t\t\tgoto out_dropped;\n\n\t\t \n\t\tskb_shinfo(skb)->gso_size = datalen;\n\t\tskb_shinfo(skb)->gso_segs = gso_segs;\n\t}\n\tsegs = skb_gso_segment(skb, dev->features & ~NETIF_F_TSO);\n\tif (IS_ERR(segs))\n\t\tgoto out_dropped;\n\n\tskb_push(skb, maclen);\n\tskb_reset_mac_header(skb);\n\n\tstatus = 0;\n\tskb_list_walk_safe(segs, curr, next) {\n\t\tskb_mark_not_on_list(curr);\n\t\tif (port->tso && curr->len > dev->mtu) {\n\t\t\tskb_shinfo(curr)->gso_size = gso_size;\n\t\t\tskb_shinfo(curr)->gso_type = gso_type;\n\t\t\tskb_shinfo(curr)->gso_segs =\n\t\t\t\tDIV_ROUND_UP(curr->len - hlen, gso_size);\n\t\t} else {\n\t\t\tskb_shinfo(curr)->gso_size = 0;\n\t\t}\n\n\t\tskb_push(curr, maclen);\n\t\tskb_reset_mac_header(curr);\n\t\tmemcpy(skb_mac_header(curr), skb_mac_header(skb),\n\t\t       maclen);\n\t\tcurr->csum_start = skb_transport_header(curr) - curr->head;\n\t\tif (ip_hdr(curr)->protocol == IPPROTO_TCP)\n\t\t\tcurr->csum_offset = offsetof(struct tcphdr, check);\n\t\telse if (ip_hdr(curr)->protocol == IPPROTO_UDP)\n\t\t\tcurr->csum_offset = offsetof(struct udphdr, check);\n\n\t\tif (!(status & NETDEV_TX_MASK))\n\t\t\tstatus = sunvnet_start_xmit_common(curr, dev,\n\t\t\t\t\t\t\t   vnet_tx_port);\n\t\tif (status & NETDEV_TX_MASK)\n\t\t\tdev_kfree_skb_any(curr);\n\t}\n\n\tif (!(status & NETDEV_TX_MASK))\n\t\tdev_kfree_skb_any(skb);\n\treturn status;\nout_dropped:\n\tdev->stats.tx_dropped++;\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\nnetdev_tx_t\nsunvnet_start_xmit_common(struct sk_buff *skb, struct net_device *dev,\n\t\t\t  struct vnet_port *(*vnet_tx_port)\n\t\t\t  (struct sk_buff *, struct net_device *))\n{\n\tstruct vnet_port *port = NULL;\n\tstruct vio_dring_state *dr;\n\tstruct vio_net_desc *d;\n\tunsigned int len;\n\tstruct sk_buff *freeskbs = NULL;\n\tint i, err, txi;\n\tunsigned pending = 0;\n\tstruct netdev_queue *txq;\n\n\trcu_read_lock();\n\tport = vnet_tx_port(skb, dev);\n\tif (unlikely(!port))\n\t\tgoto out_dropped;\n\n\tif (skb_is_gso(skb) && skb->len > port->tsolen) {\n\t\terr = vnet_handle_offloads(port, skb, vnet_tx_port);\n\t\trcu_read_unlock();\n\t\treturn err;\n\t}\n\n\tif (!skb_is_gso(skb) && skb->len > port->rmtu) {\n\t\tunsigned long localmtu = port->rmtu - ETH_HLEN;\n\n\t\tif (vio_version_after_eq(&port->vio, 1, 3))\n\t\t\tlocalmtu -= VLAN_HLEN;\n\n\t\tif (skb->protocol == htons(ETH_P_IP))\n\t\t\ticmp_ndo_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED,\n\t\t\t\t      htonl(localmtu));\n#if IS_ENABLED(CONFIG_IPV6)\n\t\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\t\ticmpv6_ndo_send(skb, ICMPV6_PKT_TOOBIG, 0, localmtu);\n#endif\n\t\tgoto out_dropped;\n\t}\n\n\tskb = vnet_skb_shape(skb, 2);\n\n\tif (unlikely(!skb))\n\t\tgoto out_dropped;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tif (skb->protocol == htons(ETH_P_IP))\n\t\t\tvnet_fullcsum_ipv4(skb);\n#if IS_ENABLED(CONFIG_IPV6)\n\t\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\t\tvnet_fullcsum_ipv6(skb);\n#endif\n\t}\n\n\tdr = &port->vio.drings[VIO_DRIVER_TX_RING];\n\ti = skb_get_queue_mapping(skb);\n\ttxq = netdev_get_tx_queue(dev, i);\n\tif (unlikely(vnet_tx_dring_avail(dr) < 1)) {\n\t\tif (!netif_tx_queue_stopped(txq)) {\n\t\t\tnetif_tx_stop_queue(txq);\n\n\t\t\t \n\t\t\tnetdev_err(dev, \"BUG! Tx Ring full when queue awake!\\n\");\n\t\t\tdev->stats.tx_errors++;\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\td = vio_dring_cur(dr);\n\n\ttxi = dr->prod;\n\n\tfreeskbs = vnet_clean_tx_ring(port, &pending);\n\n\tBUG_ON(port->tx_bufs[txi].skb);\n\n\tlen = skb->len;\n\tif (len < ETH_ZLEN)\n\t\tlen = ETH_ZLEN;\n\n\terr = vnet_skb_map(port->vio.lp, skb, port->tx_bufs[txi].cookies, 2,\n\t\t\t   (LDC_MAP_SHADOW | LDC_MAP_DIRECT | LDC_MAP_RW));\n\tif (err < 0) {\n\t\tnetdev_info(dev, \"tx buffer map error %d\\n\", err);\n\t\tgoto out_dropped;\n\t}\n\n\tport->tx_bufs[txi].skb = skb;\n\tskb = NULL;\n\tport->tx_bufs[txi].ncookies = err;\n\n\t \n\td->hdr.ack = VIO_ACK_DISABLE;\n\td->size = len;\n\td->ncookies = port->tx_bufs[txi].ncookies;\n\tfor (i = 0; i < d->ncookies; i++)\n\t\td->cookies[i] = port->tx_bufs[txi].cookies[i];\n\tif (vio_version_after_eq(&port->vio, 1, 7)) {\n\t\tstruct vio_net_dext *dext = vio_net_ext(d);\n\n\t\tmemset(dext, 0, sizeof(*dext));\n\t\tif (skb_is_gso(port->tx_bufs[txi].skb)) {\n\t\t\tdext->ipv4_lso_mss = skb_shinfo(port->tx_bufs[txi].skb)\n\t\t\t\t\t     ->gso_size;\n\t\t\tdext->flags |= VNET_PKT_IPV4_LSO;\n\t\t}\n\t\tif (vio_version_after_eq(&port->vio, 1, 8) &&\n\t\t    !port->switch_port) {\n\t\t\tdext->flags |= VNET_PKT_HCK_IPV4_HDRCKSUM_OK;\n\t\t\tdext->flags |= VNET_PKT_HCK_FULLCKSUM_OK;\n\t\t}\n\t}\n\n\t \n\tdma_wmb();\n\n\td->hdr.state = VIO_DESC_READY;\n\n\t \n\tif (!port->start_cons) {  \n\t\ttrace_vnet_skip_tx_trigger(port->vio._local_sid,\n\t\t\t\t\t   port->vio._peer_sid, dr->cons);\n\t\tgoto ldc_start_done;\n\t}\n\n\terr = __vnet_tx_trigger(port, dr->cons);\n\tif (unlikely(err < 0)) {\n\t\tnetdev_info(dev, \"TX trigger error %d\\n\", err);\n\t\td->hdr.state = VIO_DESC_FREE;\n\t\tskb = port->tx_bufs[txi].skb;\n\t\tport->tx_bufs[txi].skb = NULL;\n\t\tdev->stats.tx_carrier_errors++;\n\t\tgoto out_dropped;\n\t}\n\nldc_start_done:\n\tport->start_cons = false;\n\n\tdev->stats.tx_packets++;\n\tdev->stats.tx_bytes += port->tx_bufs[txi].skb->len;\n\tport->stats.tx_packets++;\n\tport->stats.tx_bytes += port->tx_bufs[txi].skb->len;\n\n\tdr->prod = (dr->prod + 1) & (VNET_TX_RING_SIZE - 1);\n\tif (unlikely(vnet_tx_dring_avail(dr) < 1)) {\n\t\tnetif_tx_stop_queue(txq);\n\t\tsmp_rmb();\n\t\tif (vnet_tx_dring_avail(dr) > VNET_TX_WAKEUP_THRESH(dr))\n\t\t\tnetif_tx_wake_queue(txq);\n\t}\n\n\t(void)mod_timer(&port->clean_timer, jiffies + VNET_CLEAN_TIMEOUT);\n\trcu_read_unlock();\n\n\tvnet_free_skbs(freeskbs);\n\n\treturn NETDEV_TX_OK;\n\nout_dropped:\n\tif (pending)\n\t\t(void)mod_timer(&port->clean_timer,\n\t\t\t\tjiffies + VNET_CLEAN_TIMEOUT);\n\telse if (port)\n\t\tdel_timer(&port->clean_timer);\n\trcu_read_unlock();\n\tdev_kfree_skb(skb);\n\tvnet_free_skbs(freeskbs);\n\tdev->stats.tx_dropped++;\n\treturn NETDEV_TX_OK;\n}\nEXPORT_SYMBOL_GPL(sunvnet_start_xmit_common);\n\nvoid sunvnet_tx_timeout_common(struct net_device *dev, unsigned int txqueue)\n{\n\t \n}\nEXPORT_SYMBOL_GPL(sunvnet_tx_timeout_common);\n\nint sunvnet_open_common(struct net_device *dev)\n{\n\tnetif_carrier_on(dev);\n\tnetif_tx_start_all_queues(dev);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sunvnet_open_common);\n\nint sunvnet_close_common(struct net_device *dev)\n{\n\tnetif_tx_stop_all_queues(dev);\n\tnetif_carrier_off(dev);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sunvnet_close_common);\n\nstatic struct vnet_mcast_entry *__vnet_mc_find(struct vnet *vp, u8 *addr)\n{\n\tstruct vnet_mcast_entry *m;\n\n\tfor (m = vp->mcast_list; m; m = m->next) {\n\t\tif (ether_addr_equal(m->addr, addr))\n\t\t\treturn m;\n\t}\n\treturn NULL;\n}\n\nstatic void __update_mc_list(struct vnet *vp, struct net_device *dev)\n{\n\tstruct netdev_hw_addr *ha;\n\n\tnetdev_for_each_mc_addr(ha, dev) {\n\t\tstruct vnet_mcast_entry *m;\n\n\t\tm = __vnet_mc_find(vp, ha->addr);\n\t\tif (m) {\n\t\t\tm->hit = 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!m) {\n\t\t\tm = kzalloc(sizeof(*m), GFP_ATOMIC);\n\t\t\tif (!m)\n\t\t\t\tcontinue;\n\t\t\tmemcpy(m->addr, ha->addr, ETH_ALEN);\n\t\t\tm->hit = 1;\n\n\t\t\tm->next = vp->mcast_list;\n\t\t\tvp->mcast_list = m;\n\t\t}\n\t}\n}\n\nstatic void __send_mc_list(struct vnet *vp, struct vnet_port *port)\n{\n\tstruct vio_net_mcast_info info;\n\tstruct vnet_mcast_entry *m, **pp;\n\tint n_addrs;\n\n\tmemset(&info, 0, sizeof(info));\n\n\tinfo.tag.type = VIO_TYPE_CTRL;\n\tinfo.tag.stype = VIO_SUBTYPE_INFO;\n\tinfo.tag.stype_env = VNET_MCAST_INFO;\n\tinfo.tag.sid = vio_send_sid(&port->vio);\n\tinfo.set = 1;\n\n\tn_addrs = 0;\n\tfor (m = vp->mcast_list; m; m = m->next) {\n\t\tif (m->sent)\n\t\t\tcontinue;\n\t\tm->sent = 1;\n\t\tmemcpy(&info.mcast_addr[n_addrs * ETH_ALEN],\n\t\t       m->addr, ETH_ALEN);\n\t\tif (++n_addrs == VNET_NUM_MCAST) {\n\t\t\tinfo.count = n_addrs;\n\n\t\t\t(void)vio_ldc_send(&port->vio, &info,\n\t\t\t\t\t   sizeof(info));\n\t\t\tn_addrs = 0;\n\t\t}\n\t}\n\tif (n_addrs) {\n\t\tinfo.count = n_addrs;\n\t\t(void)vio_ldc_send(&port->vio, &info, sizeof(info));\n\t}\n\n\tinfo.set = 0;\n\n\tn_addrs = 0;\n\tpp = &vp->mcast_list;\n\twhile ((m = *pp) != NULL) {\n\t\tif (m->hit) {\n\t\t\tm->hit = 0;\n\t\t\tpp = &m->next;\n\t\t\tcontinue;\n\t\t}\n\n\t\tmemcpy(&info.mcast_addr[n_addrs * ETH_ALEN],\n\t\t       m->addr, ETH_ALEN);\n\t\tif (++n_addrs == VNET_NUM_MCAST) {\n\t\t\tinfo.count = n_addrs;\n\t\t\t(void)vio_ldc_send(&port->vio, &info,\n\t\t\t\t\t   sizeof(info));\n\t\t\tn_addrs = 0;\n\t\t}\n\n\t\t*pp = m->next;\n\t\tkfree(m);\n\t}\n\tif (n_addrs) {\n\t\tinfo.count = n_addrs;\n\t\t(void)vio_ldc_send(&port->vio, &info, sizeof(info));\n\t}\n}\n\nvoid sunvnet_set_rx_mode_common(struct net_device *dev, struct vnet *vp)\n{\n\tstruct vnet_port *port;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(port, &vp->port_list, list) {\n\t\tif (port->switch_port) {\n\t\t\t__update_mc_list(vp, dev);\n\t\t\t__send_mc_list(vp, port);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(sunvnet_set_rx_mode_common);\n\nint sunvnet_set_mac_addr_common(struct net_device *dev, void *p)\n{\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL_GPL(sunvnet_set_mac_addr_common);\n\nvoid sunvnet_port_free_tx_bufs_common(struct vnet_port *port)\n{\n\tstruct vio_dring_state *dr;\n\tint i;\n\n\tdr = &port->vio.drings[VIO_DRIVER_TX_RING];\n\n\tif (!dr->base)\n\t\treturn;\n\n\tfor (i = 0; i < VNET_TX_RING_SIZE; i++) {\n\t\tstruct vio_net_desc *d;\n\t\tvoid *skb = port->tx_bufs[i].skb;\n\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\td = vio_dring_entry(dr, i);\n\n\t\tldc_unmap(port->vio.lp,\n\t\t\t  port->tx_bufs[i].cookies,\n\t\t\t  port->tx_bufs[i].ncookies);\n\t\tdev_kfree_skb(skb);\n\t\tport->tx_bufs[i].skb = NULL;\n\t\td->hdr.state = VIO_DESC_FREE;\n\t}\n\tldc_free_exp_dring(port->vio.lp, dr->base,\n\t\t\t   (dr->entry_size * dr->num_entries),\n\t\t\t   dr->cookies, dr->ncookies);\n\tdr->base = NULL;\n\tdr->entry_size = 0;\n\tdr->num_entries = 0;\n\tdr->pending = 0;\n\tdr->ncookies = 0;\n}\nEXPORT_SYMBOL_GPL(sunvnet_port_free_tx_bufs_common);\n\nvoid vnet_port_reset(struct vnet_port *port)\n{\n\tdel_timer(&port->clean_timer);\n\tsunvnet_port_free_tx_bufs_common(port);\n\tport->rmtu = 0;\n\tport->tso = (port->vsw == 0);   \n\tport->tsolen = 0;\n}\nEXPORT_SYMBOL_GPL(vnet_port_reset);\n\nstatic int vnet_port_alloc_tx_ring(struct vnet_port *port)\n{\n\tstruct vio_dring_state *dr;\n\tunsigned long len, elen;\n\tint i, err, ncookies;\n\tvoid *dring;\n\n\tdr = &port->vio.drings[VIO_DRIVER_TX_RING];\n\n\telen = sizeof(struct vio_net_desc) +\n\t       sizeof(struct ldc_trans_cookie) * 2;\n\tif (vio_version_after_eq(&port->vio, 1, 7))\n\t\telen += sizeof(struct vio_net_dext);\n\tlen = VNET_TX_RING_SIZE * elen;\n\n\tncookies = VIO_MAX_RING_COOKIES;\n\tdring = ldc_alloc_exp_dring(port->vio.lp, len,\n\t\t\t\t    dr->cookies, &ncookies,\n\t\t\t\t    (LDC_MAP_SHADOW |\n\t\t\t\t     LDC_MAP_DIRECT |\n\t\t\t\t     LDC_MAP_RW));\n\tif (IS_ERR(dring)) {\n\t\terr = PTR_ERR(dring);\n\t\tgoto err_out;\n\t}\n\n\tdr->base = dring;\n\tdr->entry_size = elen;\n\tdr->num_entries = VNET_TX_RING_SIZE;\n\tdr->prod = 0;\n\tdr->cons = 0;\n\tport->start_cons  = true;  \n\tdr->pending = VNET_TX_RING_SIZE;\n\tdr->ncookies = ncookies;\n\n\tfor (i = 0; i < VNET_TX_RING_SIZE; ++i) {\n\t\tstruct vio_net_desc *d;\n\n\t\td = vio_dring_entry(dr, i);\n\t\td->hdr.state = VIO_DESC_FREE;\n\t}\n\treturn 0;\n\nerr_out:\n\tsunvnet_port_free_tx_bufs_common(port);\n\n\treturn err;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nvoid sunvnet_poll_controller_common(struct net_device *dev, struct vnet *vp)\n{\n\tstruct vnet_port *port;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&vp->lock, flags);\n\tif (!list_empty(&vp->port_list)) {\n\t\tport = list_entry(vp->port_list.next, struct vnet_port, list);\n\t\tnapi_schedule(&port->napi);\n\t}\n\tspin_unlock_irqrestore(&vp->lock, flags);\n}\nEXPORT_SYMBOL_GPL(sunvnet_poll_controller_common);\n#endif\n\nvoid sunvnet_port_add_txq_common(struct vnet_port *port)\n{\n\tstruct vnet *vp = port->vp;\n\tint smallest = 0;\n\tint i;\n\n\t \n\tfor (i = 0; i < VNET_MAX_TXQS; i++) {\n\t\tif (vp->q_used[i] == 0) {\n\t\t\tsmallest = i;\n\t\t\tbreak;\n\t\t}\n\t\tif (vp->q_used[i] < vp->q_used[smallest])\n\t\t\tsmallest = i;\n\t}\n\n\tvp->nports++;\n\tvp->q_used[smallest]++;\n\tport->q_index = smallest;\n}\nEXPORT_SYMBOL_GPL(sunvnet_port_add_txq_common);\n\nvoid sunvnet_port_rm_txq_common(struct vnet_port *port)\n{\n\tport->vp->nports--;\n\tport->vp->q_used[port->q_index]--;\n\tport->q_index = 0;\n}\nEXPORT_SYMBOL_GPL(sunvnet_port_rm_txq_common);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}