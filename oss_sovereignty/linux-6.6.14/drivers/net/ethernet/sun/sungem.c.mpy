{
  "module_name": "sungem.c",
  "hash_id": "0d3e8d5e349b58f178e9b236fb8d7c6b6b4ef9c232ab87ba34ee3bc91e3a10be",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/sun/sungem.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/interrupt.h>\n#include <linux/ioport.h>\n#include <linux/in.h>\n#include <linux/sched.h>\n#include <linux/string.h>\n#include <linux/delay.h>\n#include <linux/errno.h>\n#include <linux/pci.h>\n#include <linux/dma-mapping.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/skbuff.h>\n#include <linux/mii.h>\n#include <linux/ethtool.h>\n#include <linux/crc32.h>\n#include <linux/random.h>\n#include <linux/workqueue.h>\n#include <linux/if_vlan.h>\n#include <linux/bitops.h>\n#include <linux/mm.h>\n#include <linux/gfp.h>\n#include <linux/of.h>\n\n#include <asm/io.h>\n#include <asm/byteorder.h>\n#include <linux/uaccess.h>\n#include <asm/irq.h>\n\n#ifdef CONFIG_SPARC\n#include <asm/idprom.h>\n#include <asm/prom.h>\n#endif\n\n#ifdef CONFIG_PPC_PMAC\n#include <asm/machdep.h>\n#include <asm/pmac_feature.h>\n#endif\n\n#include <linux/sungem_phy.h>\n#include \"sungem.h\"\n\n#define STRIP_FCS\n\n#define DEFAULT_MSG\t(NETIF_MSG_DRV\t\t| \\\n\t\t\t NETIF_MSG_PROBE\t| \\\n\t\t\t NETIF_MSG_LINK)\n\n#define ADVERTISE_MASK\t(SUPPORTED_10baseT_Half | SUPPORTED_10baseT_Full | \\\n\t\t\t SUPPORTED_100baseT_Half | SUPPORTED_100baseT_Full | \\\n\t\t\t SUPPORTED_1000baseT_Half | SUPPORTED_1000baseT_Full | \\\n\t\t\t SUPPORTED_Pause | SUPPORTED_Autoneg)\n\n#define DRV_NAME\t\"sungem\"\n#define DRV_VERSION\t\"1.0\"\n#define DRV_AUTHOR\t\"David S. Miller <davem@redhat.com>\"\n\nstatic char version[] =\n        DRV_NAME \".c:v\" DRV_VERSION \" \" DRV_AUTHOR \"\\n\";\n\nMODULE_AUTHOR(DRV_AUTHOR);\nMODULE_DESCRIPTION(\"Sun GEM Gbit ethernet driver\");\nMODULE_LICENSE(\"GPL\");\n\n#define GEM_MODULE_NAME\t\"gem\"\n\nstatic const struct pci_device_id gem_pci_tbl[] = {\n\t{ PCI_VENDOR_ID_SUN, PCI_DEVICE_ID_SUN_GEM,\n\t  PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0UL },\n\n\t \n\t{ PCI_VENDOR_ID_SUN, PCI_DEVICE_ID_SUN_RIO_GEM,\n\t  PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0UL },\n\t{ PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_UNI_N_GMAC,\n\t  PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0UL },\n\t{ PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_UNI_N_GMACP,\n\t  PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0UL },\n\t{ PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_UNI_N_GMAC2,\n\t  PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0UL },\n\t{ PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_K2_GMAC,\n\t  PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0UL },\n\t{ PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_SH_SUNGEM,\n\t  PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0UL },\n\t{ PCI_VENDOR_ID_APPLE, PCI_DEVICE_ID_APPLE_IPID2_GMAC,\n\t  PCI_ANY_ID, PCI_ANY_ID, 0, 0, 0UL },\n\t{0, }\n};\n\nMODULE_DEVICE_TABLE(pci, gem_pci_tbl);\n\nstatic u16 __sungem_phy_read(struct gem *gp, int phy_addr, int reg)\n{\n\tu32 cmd;\n\tint limit = 10000;\n\n\tcmd  = (1 << 30);\n\tcmd |= (2 << 28);\n\tcmd |= (phy_addr << 23) & MIF_FRAME_PHYAD;\n\tcmd |= (reg << 18) & MIF_FRAME_REGAD;\n\tcmd |= (MIF_FRAME_TAMSB);\n\twritel(cmd, gp->regs + MIF_FRAME);\n\n\twhile (--limit) {\n\t\tcmd = readl(gp->regs + MIF_FRAME);\n\t\tif (cmd & MIF_FRAME_TALSB)\n\t\t\tbreak;\n\n\t\tudelay(10);\n\t}\n\n\tif (!limit)\n\t\tcmd = 0xffff;\n\n\treturn cmd & MIF_FRAME_DATA;\n}\n\nstatic inline int _sungem_phy_read(struct net_device *dev, int mii_id, int reg)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\treturn __sungem_phy_read(gp, mii_id, reg);\n}\n\nstatic inline u16 sungem_phy_read(struct gem *gp, int reg)\n{\n\treturn __sungem_phy_read(gp, gp->mii_phy_addr, reg);\n}\n\nstatic void __sungem_phy_write(struct gem *gp, int phy_addr, int reg, u16 val)\n{\n\tu32 cmd;\n\tint limit = 10000;\n\n\tcmd  = (1 << 30);\n\tcmd |= (1 << 28);\n\tcmd |= (phy_addr << 23) & MIF_FRAME_PHYAD;\n\tcmd |= (reg << 18) & MIF_FRAME_REGAD;\n\tcmd |= (MIF_FRAME_TAMSB);\n\tcmd |= (val & MIF_FRAME_DATA);\n\twritel(cmd, gp->regs + MIF_FRAME);\n\n\twhile (limit--) {\n\t\tcmd = readl(gp->regs + MIF_FRAME);\n\t\tif (cmd & MIF_FRAME_TALSB)\n\t\t\tbreak;\n\n\t\tudelay(10);\n\t}\n}\n\nstatic inline void _sungem_phy_write(struct net_device *dev, int mii_id, int reg, int val)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\t__sungem_phy_write(gp, mii_id, reg, val & 0xffff);\n}\n\nstatic inline void sungem_phy_write(struct gem *gp, int reg, u16 val)\n{\n\t__sungem_phy_write(gp, gp->mii_phy_addr, reg, val);\n}\n\nstatic inline void gem_enable_ints(struct gem *gp)\n{\n\t \n\twritel(GREG_STAT_TXDONE, gp->regs + GREG_IMASK);\n}\n\nstatic inline void gem_disable_ints(struct gem *gp)\n{\n\t \n\twritel(GREG_STAT_NAPI | GREG_STAT_TXDONE, gp->regs + GREG_IMASK);\n\t(void)readl(gp->regs + GREG_IMASK);  \n}\n\nstatic void gem_get_cell(struct gem *gp)\n{\n\tBUG_ON(gp->cell_enabled < 0);\n\tgp->cell_enabled++;\n#ifdef CONFIG_PPC_PMAC\n\tif (gp->cell_enabled == 1) {\n\t\tmb();\n\t\tpmac_call_feature(PMAC_FTR_GMAC_ENABLE, gp->of_node, 0, 1);\n\t\tudelay(10);\n\t}\n#endif  \n}\n\n \nstatic void gem_put_cell(struct gem *gp)\n{\n\tBUG_ON(gp->cell_enabled <= 0);\n\tgp->cell_enabled--;\n#ifdef CONFIG_PPC_PMAC\n\tif (gp->cell_enabled == 0) {\n\t\tmb();\n\t\tpmac_call_feature(PMAC_FTR_GMAC_ENABLE, gp->of_node, 0, 0);\n\t\tudelay(10);\n\t}\n#endif  \n}\n\nstatic inline void gem_netif_stop(struct gem *gp)\n{\n\tnetif_trans_update(gp->dev);\t \n\tnapi_disable(&gp->napi);\n\tnetif_tx_disable(gp->dev);\n}\n\nstatic inline void gem_netif_start(struct gem *gp)\n{\n\t \n\tnetif_wake_queue(gp->dev);\n\tnapi_enable(&gp->napi);\n}\n\nstatic void gem_schedule_reset(struct gem *gp)\n{\n\tgp->reset_task_pending = 1;\n\tschedule_work(&gp->reset_task);\n}\n\nstatic void gem_handle_mif_event(struct gem *gp, u32 reg_val, u32 changed_bits)\n{\n\tif (netif_msg_intr(gp))\n\t\tprintk(KERN_DEBUG \"%s: mif interrupt\\n\", gp->dev->name);\n}\n\nstatic int gem_pcs_interrupt(struct net_device *dev, struct gem *gp, u32 gem_status)\n{\n\tu32 pcs_istat = readl(gp->regs + PCS_ISTAT);\n\tu32 pcs_miistat;\n\n\tif (netif_msg_intr(gp))\n\t\tprintk(KERN_DEBUG \"%s: pcs interrupt, pcs_istat: 0x%x\\n\",\n\t\t\tgp->dev->name, pcs_istat);\n\n\tif (!(pcs_istat & PCS_ISTAT_LSC)) {\n\t\tnetdev_err(dev, \"PCS irq but no link status change???\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tpcs_miistat = readl(gp->regs + PCS_MIISTAT);\n\tif (!(pcs_miistat & PCS_MIISTAT_LS))\n\t\tpcs_miistat |=\n\t\t\t(readl(gp->regs + PCS_MIISTAT) &\n\t\t\t PCS_MIISTAT_LS);\n\n\tif (pcs_miistat & PCS_MIISTAT_ANC) {\n\t\t \n\t\tif (pcs_miistat & PCS_MIISTAT_RF)\n\t\t\tnetdev_info(dev, \"PCS AutoNEG complete, RemoteFault\\n\");\n\t\telse\n\t\t\tnetdev_info(dev, \"PCS AutoNEG complete\\n\");\n\t}\n\n\tif (pcs_miistat & PCS_MIISTAT_LS) {\n\t\tnetdev_info(dev, \"PCS link is now up\\n\");\n\t\tnetif_carrier_on(gp->dev);\n\t} else {\n\t\tnetdev_info(dev, \"PCS link is now down\\n\");\n\t\tnetif_carrier_off(gp->dev);\n\t\t \n\t\tif (!timer_pending(&gp->link_timer))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int gem_txmac_interrupt(struct net_device *dev, struct gem *gp, u32 gem_status)\n{\n\tu32 txmac_stat = readl(gp->regs + MAC_TXSTAT);\n\n\tif (netif_msg_intr(gp))\n\t\tprintk(KERN_DEBUG \"%s: txmac interrupt, txmac_stat: 0x%x\\n\",\n\t\t\tgp->dev->name, txmac_stat);\n\n\t \n\tif ((txmac_stat & MAC_TXSTAT_DTE) &&\n\t    !(txmac_stat & ~MAC_TXSTAT_DTE))\n\t\treturn 0;\n\n\tif (txmac_stat & MAC_TXSTAT_URUN) {\n\t\tnetdev_err(dev, \"TX MAC xmit underrun\\n\");\n\t\tdev->stats.tx_fifo_errors++;\n\t}\n\n\tif (txmac_stat & MAC_TXSTAT_MPE) {\n\t\tnetdev_err(dev, \"TX MAC max packet size error\\n\");\n\t\tdev->stats.tx_errors++;\n\t}\n\n\t \n\tif (txmac_stat & MAC_TXSTAT_NCE)\n\t\tdev->stats.collisions += 0x10000;\n\n\tif (txmac_stat & MAC_TXSTAT_ECE) {\n\t\tdev->stats.tx_aborted_errors += 0x10000;\n\t\tdev->stats.collisions += 0x10000;\n\t}\n\n\tif (txmac_stat & MAC_TXSTAT_LCE) {\n\t\tdev->stats.tx_aborted_errors += 0x10000;\n\t\tdev->stats.collisions += 0x10000;\n\t}\n\n\t \n\treturn 0;\n}\n\n \nstatic int gem_rxmac_reset(struct gem *gp)\n{\n\tstruct net_device *dev = gp->dev;\n\tint limit, i;\n\tu64 desc_dma;\n\tu32 val;\n\n\t \n\twritel(MAC_RXRST_CMD, gp->regs + MAC_RXRST);\n\tfor (limit = 0; limit < 5000; limit++) {\n\t\tif (!(readl(gp->regs + MAC_RXRST) & MAC_RXRST_CMD))\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\tif (limit == 5000) {\n\t\tnetdev_err(dev, \"RX MAC will not reset, resetting whole chip\\n\");\n\t\treturn 1;\n\t}\n\n\twritel(gp->mac_rx_cfg & ~MAC_RXCFG_ENAB,\n\t       gp->regs + MAC_RXCFG);\n\tfor (limit = 0; limit < 5000; limit++) {\n\t\tif (!(readl(gp->regs + MAC_RXCFG) & MAC_RXCFG_ENAB))\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\tif (limit == 5000) {\n\t\tnetdev_err(dev, \"RX MAC will not disable, resetting whole chip\\n\");\n\t\treturn 1;\n\t}\n\n\t \n\twritel(0, gp->regs + RXDMA_CFG);\n\tfor (limit = 0; limit < 5000; limit++) {\n\t\tif (!(readl(gp->regs + RXDMA_CFG) & RXDMA_CFG_ENABLE))\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\tif (limit == 5000) {\n\t\tnetdev_err(dev, \"RX DMA will not disable, resetting whole chip\\n\");\n\t\treturn 1;\n\t}\n\n\tmdelay(5);\n\n\t \n\twritel(gp->swrst_base | GREG_SWRST_RXRST,\n\t       gp->regs + GREG_SWRST);\n\tfor (limit = 0; limit < 5000; limit++) {\n\t\tif (!(readl(gp->regs + GREG_SWRST) & GREG_SWRST_RXRST))\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\tif (limit == 5000) {\n\t\tnetdev_err(dev, \"RX reset command will not execute, resetting whole chip\\n\");\n\t\treturn 1;\n\t}\n\n\t \n\tfor (i = 0; i < RX_RING_SIZE; i++) {\n\t\tstruct gem_rxd *rxd = &gp->init_block->rxd[i];\n\n\t\tif (gp->rx_skbs[i] == NULL) {\n\t\t\tnetdev_err(dev, \"Parts of RX ring empty, resetting whole chip\\n\");\n\t\t\treturn 1;\n\t\t}\n\n\t\trxd->status_word = cpu_to_le64(RXDCTRL_FRESH(gp));\n\t}\n\tgp->rx_new = gp->rx_old = 0;\n\n\t \n\tdesc_dma = (u64) gp->gblock_dvma;\n\tdesc_dma += (INIT_BLOCK_TX_RING_SIZE * sizeof(struct gem_txd));\n\twritel(desc_dma >> 32, gp->regs + RXDMA_DBHI);\n\twritel(desc_dma & 0xffffffff, gp->regs + RXDMA_DBLOW);\n\twritel(RX_RING_SIZE - 4, gp->regs + RXDMA_KICK);\n\tval = (RXDMA_CFG_BASE | (RX_OFFSET << 10) |\n\t       (ETH_HLEN << 13) | RXDMA_CFG_FTHRESH_128);\n\twritel(val, gp->regs + RXDMA_CFG);\n\tif (readl(gp->regs + GREG_BIFCFG) & GREG_BIFCFG_M66EN)\n\t\twritel(((5 & RXDMA_BLANK_IPKTS) |\n\t\t\t((8 << 12) & RXDMA_BLANK_ITIME)),\n\t\t       gp->regs + RXDMA_BLANK);\n\telse\n\t\twritel(((5 & RXDMA_BLANK_IPKTS) |\n\t\t\t((4 << 12) & RXDMA_BLANK_ITIME)),\n\t\t       gp->regs + RXDMA_BLANK);\n\tval  = (((gp->rx_pause_off / 64) << 0) & RXDMA_PTHRESH_OFF);\n\tval |= (((gp->rx_pause_on / 64) << 12) & RXDMA_PTHRESH_ON);\n\twritel(val, gp->regs + RXDMA_PTHRESH);\n\tval = readl(gp->regs + RXDMA_CFG);\n\twritel(val | RXDMA_CFG_ENABLE, gp->regs + RXDMA_CFG);\n\twritel(MAC_RXSTAT_RCV, gp->regs + MAC_RXMASK);\n\tval = readl(gp->regs + MAC_RXCFG);\n\twritel(val | MAC_RXCFG_ENAB, gp->regs + MAC_RXCFG);\n\n\treturn 0;\n}\n\nstatic int gem_rxmac_interrupt(struct net_device *dev, struct gem *gp, u32 gem_status)\n{\n\tu32 rxmac_stat = readl(gp->regs + MAC_RXSTAT);\n\tint ret = 0;\n\n\tif (netif_msg_intr(gp))\n\t\tprintk(KERN_DEBUG \"%s: rxmac interrupt, rxmac_stat: 0x%x\\n\",\n\t\t\tgp->dev->name, rxmac_stat);\n\n\tif (rxmac_stat & MAC_RXSTAT_OFLW) {\n\t\tu32 smac = readl(gp->regs + MAC_SMACHINE);\n\n\t\tnetdev_err(dev, \"RX MAC fifo overflow smac[%08x]\\n\", smac);\n\t\tdev->stats.rx_over_errors++;\n\t\tdev->stats.rx_fifo_errors++;\n\n\t\tret = gem_rxmac_reset(gp);\n\t}\n\n\tif (rxmac_stat & MAC_RXSTAT_ACE)\n\t\tdev->stats.rx_frame_errors += 0x10000;\n\n\tif (rxmac_stat & MAC_RXSTAT_CCE)\n\t\tdev->stats.rx_crc_errors += 0x10000;\n\n\tif (rxmac_stat & MAC_RXSTAT_LCE)\n\t\tdev->stats.rx_length_errors += 0x10000;\n\n\t \n\treturn ret;\n}\n\nstatic int gem_mac_interrupt(struct net_device *dev, struct gem *gp, u32 gem_status)\n{\n\tu32 mac_cstat = readl(gp->regs + MAC_CSTAT);\n\n\tif (netif_msg_intr(gp))\n\t\tprintk(KERN_DEBUG \"%s: mac interrupt, mac_cstat: 0x%x\\n\",\n\t\t\tgp->dev->name, mac_cstat);\n\n\t \n\tif (mac_cstat & MAC_CSTAT_PS)\n\t\tgp->pause_entered++;\n\n\tif (mac_cstat & MAC_CSTAT_PRCV)\n\t\tgp->pause_last_time_recvd = (mac_cstat >> 16);\n\n\treturn 0;\n}\n\nstatic int gem_mif_interrupt(struct net_device *dev, struct gem *gp, u32 gem_status)\n{\n\tu32 mif_status = readl(gp->regs + MIF_STATUS);\n\tu32 reg_val, changed_bits;\n\n\treg_val = (mif_status & MIF_STATUS_DATA) >> 16;\n\tchanged_bits = (mif_status & MIF_STATUS_STAT);\n\n\tgem_handle_mif_event(gp, reg_val, changed_bits);\n\n\treturn 0;\n}\n\nstatic int gem_pci_interrupt(struct net_device *dev, struct gem *gp, u32 gem_status)\n{\n\tu32 pci_estat = readl(gp->regs + GREG_PCIESTAT);\n\n\tif (gp->pdev->vendor == PCI_VENDOR_ID_SUN &&\n\t    gp->pdev->device == PCI_DEVICE_ID_SUN_GEM) {\n\t\tnetdev_err(dev, \"PCI error [%04x]\", pci_estat);\n\n\t\tif (pci_estat & GREG_PCIESTAT_BADACK)\n\t\t\tpr_cont(\" <No ACK64# during ABS64 cycle>\");\n\t\tif (pci_estat & GREG_PCIESTAT_DTRTO)\n\t\t\tpr_cont(\" <Delayed transaction timeout>\");\n\t\tif (pci_estat & GREG_PCIESTAT_OTHER)\n\t\t\tpr_cont(\" <other>\");\n\t\tpr_cont(\"\\n\");\n\t} else {\n\t\tpci_estat |= GREG_PCIESTAT_OTHER;\n\t\tnetdev_err(dev, \"PCI error\\n\");\n\t}\n\n\tif (pci_estat & GREG_PCIESTAT_OTHER) {\n\t\tint pci_errs;\n\n\t\t \n\t\tpci_errs = pci_status_get_and_clear_errors(gp->pdev);\n\t\tnetdev_err(dev, \"PCI status errors[%04x]\\n\", pci_errs);\n\t\tif (pci_errs & PCI_STATUS_PARITY)\n\t\t\tnetdev_err(dev, \"PCI parity error detected\\n\");\n\t\tif (pci_errs & PCI_STATUS_SIG_TARGET_ABORT)\n\t\t\tnetdev_err(dev, \"PCI target abort\\n\");\n\t\tif (pci_errs & PCI_STATUS_REC_TARGET_ABORT)\n\t\t\tnetdev_err(dev, \"PCI master acks target abort\\n\");\n\t\tif (pci_errs & PCI_STATUS_REC_MASTER_ABORT)\n\t\t\tnetdev_err(dev, \"PCI master abort\\n\");\n\t\tif (pci_errs & PCI_STATUS_SIG_SYSTEM_ERROR)\n\t\t\tnetdev_err(dev, \"PCI system error SERR#\\n\");\n\t\tif (pci_errs & PCI_STATUS_DETECTED_PARITY)\n\t\t\tnetdev_err(dev, \"PCI parity error\\n\");\n\t}\n\n\t \n\treturn 1;\n}\n\n \nstatic int gem_abnormal_irq(struct net_device *dev, struct gem *gp, u32 gem_status)\n{\n\tif (gem_status & GREG_STAT_RXNOBUF) {\n\t\t \n\t\tif (netif_msg_rx_err(gp))\n\t\t\tprintk(KERN_DEBUG \"%s: no buffer for rx frame\\n\",\n\t\t\t\tgp->dev->name);\n\t\tdev->stats.rx_dropped++;\n\t}\n\n\tif (gem_status & GREG_STAT_RXTAGERR) {\n\t\t \n\t\tif (netif_msg_rx_err(gp))\n\t\t\tprintk(KERN_DEBUG \"%s: corrupt rx tag framing\\n\",\n\t\t\t\tgp->dev->name);\n\t\tdev->stats.rx_errors++;\n\n\t\treturn 1;\n\t}\n\n\tif (gem_status & GREG_STAT_PCS) {\n\t\tif (gem_pcs_interrupt(dev, gp, gem_status))\n\t\t\treturn 1;\n\t}\n\n\tif (gem_status & GREG_STAT_TXMAC) {\n\t\tif (gem_txmac_interrupt(dev, gp, gem_status))\n\t\t\treturn 1;\n\t}\n\n\tif (gem_status & GREG_STAT_RXMAC) {\n\t\tif (gem_rxmac_interrupt(dev, gp, gem_status))\n\t\t\treturn 1;\n\t}\n\n\tif (gem_status & GREG_STAT_MAC) {\n\t\tif (gem_mac_interrupt(dev, gp, gem_status))\n\t\t\treturn 1;\n\t}\n\n\tif (gem_status & GREG_STAT_MIF) {\n\t\tif (gem_mif_interrupt(dev, gp, gem_status))\n\t\t\treturn 1;\n\t}\n\n\tif (gem_status & GREG_STAT_PCIERR) {\n\t\tif (gem_pci_interrupt(dev, gp, gem_status))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic __inline__ void gem_tx(struct net_device *dev, struct gem *gp, u32 gem_status)\n{\n\tint entry, limit;\n\n\tentry = gp->tx_old;\n\tlimit = ((gem_status & GREG_STAT_TXNR) >> GREG_STAT_TXNR_SHIFT);\n\twhile (entry != limit) {\n\t\tstruct sk_buff *skb;\n\t\tstruct gem_txd *txd;\n\t\tdma_addr_t dma_addr;\n\t\tu32 dma_len;\n\t\tint frag;\n\n\t\tif (netif_msg_tx_done(gp))\n\t\t\tprintk(KERN_DEBUG \"%s: tx done, slot %d\\n\",\n\t\t\t\tgp->dev->name, entry);\n\t\tskb = gp->tx_skbs[entry];\n\t\tif (skb_shinfo(skb)->nr_frags) {\n\t\t\tint last = entry + skb_shinfo(skb)->nr_frags;\n\t\t\tint walk = entry;\n\t\t\tint incomplete = 0;\n\n\t\t\tlast &= (TX_RING_SIZE - 1);\n\t\t\tfor (;;) {\n\t\t\t\twalk = NEXT_TX(walk);\n\t\t\t\tif (walk == limit)\n\t\t\t\t\tincomplete = 1;\n\t\t\t\tif (walk == last)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (incomplete)\n\t\t\t\tbreak;\n\t\t}\n\t\tgp->tx_skbs[entry] = NULL;\n\t\tdev->stats.tx_bytes += skb->len;\n\n\t\tfor (frag = 0; frag <= skb_shinfo(skb)->nr_frags; frag++) {\n\t\t\ttxd = &gp->init_block->txd[entry];\n\n\t\t\tdma_addr = le64_to_cpu(txd->buffer);\n\t\t\tdma_len = le64_to_cpu(txd->control_word) & TXDCTRL_BUFSZ;\n\n\t\t\tdma_unmap_page(&gp->pdev->dev, dma_addr, dma_len,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\t\tentry = NEXT_TX(entry);\n\t\t}\n\n\t\tdev->stats.tx_packets++;\n\t\tdev_consume_skb_any(skb);\n\t}\n\tgp->tx_old = entry;\n\n\t \n\tsmp_mb();\n\n\tif (unlikely(netif_queue_stopped(dev) &&\n\t\t     TX_BUFFS_AVAIL(gp) > (MAX_SKB_FRAGS + 1))) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, 0);\n\n\t\t__netif_tx_lock(txq, smp_processor_id());\n\t\tif (netif_queue_stopped(dev) &&\n\t\t    TX_BUFFS_AVAIL(gp) > (MAX_SKB_FRAGS + 1))\n\t\t\tnetif_wake_queue(dev);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic __inline__ void gem_post_rxds(struct gem *gp, int limit)\n{\n\tint cluster_start, curr, count, kick;\n\n\tcluster_start = curr = (gp->rx_new & ~(4 - 1));\n\tcount = 0;\n\tkick = -1;\n\tdma_wmb();\n\twhile (curr != limit) {\n\t\tcurr = NEXT_RX(curr);\n\t\tif (++count == 4) {\n\t\t\tstruct gem_rxd *rxd =\n\t\t\t\t&gp->init_block->rxd[cluster_start];\n\t\t\tfor (;;) {\n\t\t\t\trxd->status_word = cpu_to_le64(RXDCTRL_FRESH(gp));\n\t\t\t\trxd++;\n\t\t\t\tcluster_start = NEXT_RX(cluster_start);\n\t\t\t\tif (cluster_start == curr)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tkick = curr;\n\t\t\tcount = 0;\n\t\t}\n\t}\n\tif (kick >= 0) {\n\t\tmb();\n\t\twritel(kick, gp->regs + RXDMA_KICK);\n\t}\n}\n\n#define ALIGNED_RX_SKB_ADDR(addr) \\\n        ((((unsigned long)(addr) + (64UL - 1UL)) & ~(64UL - 1UL)) - (unsigned long)(addr))\nstatic __inline__ struct sk_buff *gem_alloc_skb(struct net_device *dev, int size,\n\t\t\t\t\t\tgfp_t gfp_flags)\n{\n\tstruct sk_buff *skb = alloc_skb(size + 64, gfp_flags);\n\n\tif (likely(skb)) {\n\t\tunsigned long offset = ALIGNED_RX_SKB_ADDR(skb->data);\n\t\tskb_reserve(skb, offset);\n\t}\n\treturn skb;\n}\n\nstatic int gem_rx(struct gem *gp, int work_to_do)\n{\n\tstruct net_device *dev = gp->dev;\n\tint entry, drops, work_done = 0;\n\tu32 done;\n\n\tif (netif_msg_rx_status(gp))\n\t\tprintk(KERN_DEBUG \"%s: rx interrupt, done: %d, rx_new: %d\\n\",\n\t\t\tgp->dev->name, readl(gp->regs + RXDMA_DONE), gp->rx_new);\n\n\tentry = gp->rx_new;\n\tdrops = 0;\n\tdone = readl(gp->regs + RXDMA_DONE);\n\tfor (;;) {\n\t\tstruct gem_rxd *rxd = &gp->init_block->rxd[entry];\n\t\tstruct sk_buff *skb;\n\t\tu64 status = le64_to_cpu(rxd->status_word);\n\t\tdma_addr_t dma_addr;\n\t\tint len;\n\n\t\tif ((status & RXDCTRL_OWN) != 0)\n\t\t\tbreak;\n\n\t\tif (work_done >= RX_RING_SIZE || work_done >= work_to_do)\n\t\t\tbreak;\n\n\t\t \n\t\tif (entry == done) {\n\t\t\tdone = readl(gp->regs + RXDMA_DONE);\n\t\t\tif (entry == done)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\twork_done++;\n\n\t\tskb = gp->rx_skbs[entry];\n\n\t\tlen = (status & RXDCTRL_BUFSZ) >> 16;\n\t\tif ((len < ETH_ZLEN) || (status & RXDCTRL_BAD)) {\n\t\t\tdev->stats.rx_errors++;\n\t\t\tif (len < ETH_ZLEN)\n\t\t\t\tdev->stats.rx_length_errors++;\n\t\t\tif (len & RXDCTRL_BAD)\n\t\t\t\tdev->stats.rx_crc_errors++;\n\n\t\t\t \n\t\tdrop_it:\n\t\t\tdev->stats.rx_dropped++;\n\t\t\tgoto next;\n\t\t}\n\n\t\tdma_addr = le64_to_cpu(rxd->buffer);\n\t\tif (len > RX_COPY_THRESHOLD) {\n\t\t\tstruct sk_buff *new_skb;\n\n\t\t\tnew_skb = gem_alloc_skb(dev, RX_BUF_ALLOC_SIZE(gp), GFP_ATOMIC);\n\t\t\tif (new_skb == NULL) {\n\t\t\t\tdrops++;\n\t\t\t\tgoto drop_it;\n\t\t\t}\n\t\t\tdma_unmap_page(&gp->pdev->dev, dma_addr,\n\t\t\t\t       RX_BUF_ALLOC_SIZE(gp), DMA_FROM_DEVICE);\n\t\t\tgp->rx_skbs[entry] = new_skb;\n\t\t\tskb_put(new_skb, (gp->rx_buf_sz + RX_OFFSET));\n\t\t\trxd->buffer = cpu_to_le64(dma_map_page(&gp->pdev->dev,\n\t\t\t\t\t\t\t       virt_to_page(new_skb->data),\n\t\t\t\t\t\t\t       offset_in_page(new_skb->data),\n\t\t\t\t\t\t\t       RX_BUF_ALLOC_SIZE(gp),\n\t\t\t\t\t\t\t       DMA_FROM_DEVICE));\n\t\t\tskb_reserve(new_skb, RX_OFFSET);\n\n\t\t\t \n\t\t\tskb_trim(skb, len);\n\t\t} else {\n\t\t\tstruct sk_buff *copy_skb = netdev_alloc_skb(dev, len + 2);\n\n\t\t\tif (copy_skb == NULL) {\n\t\t\t\tdrops++;\n\t\t\t\tgoto drop_it;\n\t\t\t}\n\n\t\t\tskb_reserve(copy_skb, 2);\n\t\t\tskb_put(copy_skb, len);\n\t\t\tdma_sync_single_for_cpu(&gp->pdev->dev, dma_addr, len,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\tskb_copy_from_linear_data(skb, copy_skb->data, len);\n\t\t\tdma_sync_single_for_device(&gp->pdev->dev, dma_addr,\n\t\t\t\t\t\t   len, DMA_FROM_DEVICE);\n\n\t\t\t \n\t\t\tskb = copy_skb;\n\t\t}\n\n\t\tif (likely(dev->features & NETIF_F_RXCSUM)) {\n\t\t\t__sum16 csum;\n\n\t\t\tcsum = (__force __sum16)htons((status & RXDCTRL_TCPCSUM) ^ 0xffff);\n\t\t\tskb->csum = csum_unfold(csum);\n\t\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\t}\n\t\tskb->protocol = eth_type_trans(skb, gp->dev);\n\n\t\tnapi_gro_receive(&gp->napi, skb);\n\n\t\tdev->stats.rx_packets++;\n\t\tdev->stats.rx_bytes += len;\n\n\tnext:\n\t\tentry = NEXT_RX(entry);\n\t}\n\n\tgem_post_rxds(gp, entry);\n\n\tgp->rx_new = entry;\n\n\tif (drops)\n\t\tnetdev_info(gp->dev, \"Memory squeeze, deferring packet\\n\");\n\n\treturn work_done;\n}\n\nstatic int gem_poll(struct napi_struct *napi, int budget)\n{\n\tstruct gem *gp = container_of(napi, struct gem, napi);\n\tstruct net_device *dev = gp->dev;\n\tint work_done;\n\n\twork_done = 0;\n\tdo {\n\t\t \n\t\tif (unlikely(gp->status & GREG_STAT_ABNORMAL)) {\n\t\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, 0);\n\t\t\tint reset;\n\n\t\t\t \n\t\t\t__netif_tx_lock(txq, smp_processor_id());\n\t\t\treset = gem_abnormal_irq(dev, gp, gp->status);\n\t\t\t__netif_tx_unlock(txq);\n\t\t\tif (reset) {\n\t\t\t\tgem_schedule_reset(gp);\n\t\t\t\tnapi_complete(napi);\n\t\t\t\treturn work_done;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tgem_tx(dev, gp, gp->status);\n\n\t\t \n\t\twork_done += gem_rx(gp, budget - work_done);\n\n\t\tif (work_done >= budget)\n\t\t\treturn work_done;\n\n\t\tgp->status = readl(gp->regs + GREG_STAT);\n\t} while (gp->status & GREG_STAT_NAPI);\n\n\tnapi_complete_done(napi, work_done);\n\tgem_enable_ints(gp);\n\n\treturn work_done;\n}\n\nstatic irqreturn_t gem_interrupt(int irq, void *dev_id)\n{\n\tstruct net_device *dev = dev_id;\n\tstruct gem *gp = netdev_priv(dev);\n\n\tif (napi_schedule_prep(&gp->napi)) {\n\t\tu32 gem_status = readl(gp->regs + GREG_STAT);\n\n\t\tif (unlikely(gem_status == 0)) {\n\t\t\tnapi_enable(&gp->napi);\n\t\t\treturn IRQ_NONE;\n\t\t}\n\t\tif (netif_msg_intr(gp))\n\t\t\tprintk(KERN_DEBUG \"%s: gem_interrupt() gem_status: 0x%x\\n\",\n\t\t\t       gp->dev->name, gem_status);\n\n\t\tgp->status = gem_status;\n\t\tgem_disable_ints(gp);\n\t\t__napi_schedule(&gp->napi);\n\t}\n\n\t \n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void gem_poll_controller(struct net_device *dev)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\tdisable_irq(gp->pdev->irq);\n\tgem_interrupt(gp->pdev->irq, dev);\n\tenable_irq(gp->pdev->irq);\n}\n#endif\n\nstatic void gem_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\tnetdev_err(dev, \"transmit timed out, resetting\\n\");\n\n\tnetdev_err(dev, \"TX_STATE[%08x:%08x:%08x]\\n\",\n\t\t   readl(gp->regs + TXDMA_CFG),\n\t\t   readl(gp->regs + MAC_TXSTAT),\n\t\t   readl(gp->regs + MAC_TXCFG));\n\tnetdev_err(dev, \"RX_STATE[%08x:%08x:%08x]\\n\",\n\t\t   readl(gp->regs + RXDMA_CFG),\n\t\t   readl(gp->regs + MAC_RXSTAT),\n\t\t   readl(gp->regs + MAC_RXCFG));\n\n\tgem_schedule_reset(gp);\n}\n\nstatic __inline__ int gem_intme(int entry)\n{\n\t \n\tif (!(entry & ((TX_RING_SIZE>>1)-1)))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic netdev_tx_t gem_start_xmit(struct sk_buff *skb,\n\t\t\t\t  struct net_device *dev)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\tint entry;\n\tu64 ctrl;\n\n\tctrl = 0;\n\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tconst u64 csum_start_off = skb_checksum_start_offset(skb);\n\t\tconst u64 csum_stuff_off = csum_start_off + skb->csum_offset;\n\n\t\tctrl = (TXDCTRL_CENAB |\n\t\t\t(csum_start_off << 15) |\n\t\t\t(csum_stuff_off << 21));\n\t}\n\n\tif (unlikely(TX_BUFFS_AVAIL(gp) <= (skb_shinfo(skb)->nr_frags + 1))) {\n\t\t \n\t\tif (!netif_queue_stopped(dev)) {\n\t\t\tnetif_stop_queue(dev);\n\t\t\tnetdev_err(dev, \"BUG! Tx Ring full when queue awake!\\n\");\n\t\t}\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tentry = gp->tx_new;\n\tgp->tx_skbs[entry] = skb;\n\n\tif (skb_shinfo(skb)->nr_frags == 0) {\n\t\tstruct gem_txd *txd = &gp->init_block->txd[entry];\n\t\tdma_addr_t mapping;\n\t\tu32 len;\n\n\t\tlen = skb->len;\n\t\tmapping = dma_map_page(&gp->pdev->dev,\n\t\t\t\t       virt_to_page(skb->data),\n\t\t\t\t       offset_in_page(skb->data),\n\t\t\t\t       len, DMA_TO_DEVICE);\n\t\tctrl |= TXDCTRL_SOF | TXDCTRL_EOF | len;\n\t\tif (gem_intme(entry))\n\t\t\tctrl |= TXDCTRL_INTME;\n\t\ttxd->buffer = cpu_to_le64(mapping);\n\t\tdma_wmb();\n\t\ttxd->control_word = cpu_to_le64(ctrl);\n\t\tentry = NEXT_TX(entry);\n\t} else {\n\t\tstruct gem_txd *txd;\n\t\tu32 first_len;\n\t\tu64 intme;\n\t\tdma_addr_t first_mapping;\n\t\tint frag, first_entry = entry;\n\n\t\tintme = 0;\n\t\tif (gem_intme(entry))\n\t\t\tintme |= TXDCTRL_INTME;\n\n\t\t \n\t\tfirst_len = skb_headlen(skb);\n\t\tfirst_mapping = dma_map_page(&gp->pdev->dev,\n\t\t\t\t\t     virt_to_page(skb->data),\n\t\t\t\t\t     offset_in_page(skb->data),\n\t\t\t\t\t     first_len, DMA_TO_DEVICE);\n\t\tentry = NEXT_TX(entry);\n\n\t\tfor (frag = 0; frag < skb_shinfo(skb)->nr_frags; frag++) {\n\t\t\tconst skb_frag_t *this_frag = &skb_shinfo(skb)->frags[frag];\n\t\t\tu32 len;\n\t\t\tdma_addr_t mapping;\n\t\t\tu64 this_ctrl;\n\n\t\t\tlen = skb_frag_size(this_frag);\n\t\t\tmapping = skb_frag_dma_map(&gp->pdev->dev, this_frag,\n\t\t\t\t\t\t   0, len, DMA_TO_DEVICE);\n\t\t\tthis_ctrl = ctrl;\n\t\t\tif (frag == skb_shinfo(skb)->nr_frags - 1)\n\t\t\t\tthis_ctrl |= TXDCTRL_EOF;\n\n\t\t\ttxd = &gp->init_block->txd[entry];\n\t\t\ttxd->buffer = cpu_to_le64(mapping);\n\t\t\tdma_wmb();\n\t\t\ttxd->control_word = cpu_to_le64(this_ctrl | len);\n\n\t\t\tif (gem_intme(entry))\n\t\t\t\tintme |= TXDCTRL_INTME;\n\n\t\t\tentry = NEXT_TX(entry);\n\t\t}\n\t\ttxd = &gp->init_block->txd[first_entry];\n\t\ttxd->buffer = cpu_to_le64(first_mapping);\n\t\tdma_wmb();\n\t\ttxd->control_word =\n\t\t\tcpu_to_le64(ctrl | TXDCTRL_SOF | intme | first_len);\n\t}\n\n\tgp->tx_new = entry;\n\tif (unlikely(TX_BUFFS_AVAIL(gp) <= (MAX_SKB_FRAGS + 1))) {\n\t\tnetif_stop_queue(dev);\n\n\t\t \n\t\tsmp_mb();\n\t\tif (TX_BUFFS_AVAIL(gp) > (MAX_SKB_FRAGS + 1))\n\t\t\tnetif_wake_queue(dev);\n\t}\n\tif (netif_msg_tx_queued(gp))\n\t\tprintk(KERN_DEBUG \"%s: tx queued, slot %d, skblen %d\\n\",\n\t\t       dev->name, entry, skb->len);\n\tmb();\n\twritel(gp->tx_new, gp->regs + TXDMA_KICK);\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic void gem_pcs_reset(struct gem *gp)\n{\n\tint limit;\n\tu32 val;\n\n\t \n\tval = readl(gp->regs + PCS_MIICTRL);\n\tval |= PCS_MIICTRL_RST;\n\twritel(val, gp->regs + PCS_MIICTRL);\n\n\tlimit = 32;\n\twhile (readl(gp->regs + PCS_MIICTRL) & PCS_MIICTRL_RST) {\n\t\tudelay(100);\n\t\tif (limit-- <= 0)\n\t\t\tbreak;\n\t}\n\tif (limit < 0)\n\t\tnetdev_warn(gp->dev, \"PCS reset bit would not clear\\n\");\n}\n\nstatic void gem_pcs_reinit_adv(struct gem *gp)\n{\n\tu32 val;\n\n\t \n\tval = readl(gp->regs + PCS_CFG);\n\tval &= ~(PCS_CFG_ENABLE | PCS_CFG_TO);\n\twritel(val, gp->regs + PCS_CFG);\n\n\t \n\tval = readl(gp->regs + PCS_MIIADV);\n\tval |= (PCS_MIIADV_FD | PCS_MIIADV_HD |\n\t\tPCS_MIIADV_SP | PCS_MIIADV_AP);\n\twritel(val, gp->regs + PCS_MIIADV);\n\n\t \n\tval = readl(gp->regs + PCS_MIICTRL);\n\tval |= (PCS_MIICTRL_RAN | PCS_MIICTRL_ANE);\n\tval &= ~PCS_MIICTRL_WB;\n\twritel(val, gp->regs + PCS_MIICTRL);\n\n\tval = readl(gp->regs + PCS_CFG);\n\tval |= PCS_CFG_ENABLE;\n\twritel(val, gp->regs + PCS_CFG);\n\n\t \n\tval = readl(gp->regs + PCS_SCTRL);\n\tif (gp->phy_type == phy_serialink)\n\t\tval &= ~PCS_SCTRL_LOOP;\n\telse\n\t\tval |= PCS_SCTRL_LOOP;\n\twritel(val, gp->regs + PCS_SCTRL);\n}\n\n#define STOP_TRIES 32\n\nstatic void gem_reset(struct gem *gp)\n{\n\tint limit;\n\tu32 val;\n\n\t \n\twritel(0xffffffff, gp->regs + GREG_IMASK);\n\n\t \n\twritel(gp->swrst_base | GREG_SWRST_TXRST | GREG_SWRST_RXRST,\n\t       gp->regs + GREG_SWRST);\n\n\tlimit = STOP_TRIES;\n\n\tdo {\n\t\tudelay(20);\n\t\tval = readl(gp->regs + GREG_SWRST);\n\t\tif (limit-- <= 0)\n\t\t\tbreak;\n\t} while (val & (GREG_SWRST_TXRST | GREG_SWRST_RXRST));\n\n\tif (limit < 0)\n\t\tnetdev_err(gp->dev, \"SW reset is ghetto\\n\");\n\n\tif (gp->phy_type == phy_serialink || gp->phy_type == phy_serdes)\n\t\tgem_pcs_reinit_adv(gp);\n}\n\nstatic void gem_start_dma(struct gem *gp)\n{\n\tu32 val;\n\n\t \n\tval = readl(gp->regs + TXDMA_CFG);\n\twritel(val | TXDMA_CFG_ENABLE, gp->regs + TXDMA_CFG);\n\tval = readl(gp->regs + RXDMA_CFG);\n\twritel(val | RXDMA_CFG_ENABLE, gp->regs + RXDMA_CFG);\n\tval = readl(gp->regs + MAC_TXCFG);\n\twritel(val | MAC_TXCFG_ENAB, gp->regs + MAC_TXCFG);\n\tval = readl(gp->regs + MAC_RXCFG);\n\twritel(val | MAC_RXCFG_ENAB, gp->regs + MAC_RXCFG);\n\n\t(void) readl(gp->regs + MAC_RXCFG);\n\tudelay(100);\n\n\tgem_enable_ints(gp);\n\n\twritel(RX_RING_SIZE - 4, gp->regs + RXDMA_KICK);\n}\n\n \nstatic void gem_stop_dma(struct gem *gp)\n{\n\tu32 val;\n\n\t \n\tval = readl(gp->regs + TXDMA_CFG);\n\twritel(val & ~TXDMA_CFG_ENABLE, gp->regs + TXDMA_CFG);\n\tval = readl(gp->regs + RXDMA_CFG);\n\twritel(val & ~RXDMA_CFG_ENABLE, gp->regs + RXDMA_CFG);\n\tval = readl(gp->regs + MAC_TXCFG);\n\twritel(val & ~MAC_TXCFG_ENAB, gp->regs + MAC_TXCFG);\n\tval = readl(gp->regs + MAC_RXCFG);\n\twritel(val & ~MAC_RXCFG_ENAB, gp->regs + MAC_RXCFG);\n\n\t(void) readl(gp->regs + MAC_RXCFG);\n\n\t \n}\n\n\n\nstatic void gem_begin_auto_negotiation(struct gem *gp,\n\t\t\t\t       const struct ethtool_link_ksettings *ep)\n{\n\tu32 advertise, features;\n\tint autoneg;\n\tint speed;\n\tint duplex;\n\tu32 advertising;\n\n\tif (ep)\n\t\tethtool_convert_link_mode_to_legacy_u32(\n\t\t\t&advertising, ep->link_modes.advertising);\n\n\tif (gp->phy_type != phy_mii_mdio0 &&\n\t    gp->phy_type != phy_mii_mdio1)\n\t\tgoto non_mii;\n\n\t \n\tif (found_mii_phy(gp))\n\t\tfeatures = gp->phy_mii.def->features;\n\telse\n\t\tfeatures = 0;\n\n\tadvertise = features & ADVERTISE_MASK;\n\tif (gp->phy_mii.advertising != 0)\n\t\tadvertise &= gp->phy_mii.advertising;\n\n\tautoneg = gp->want_autoneg;\n\tspeed = gp->phy_mii.speed;\n\tduplex = gp->phy_mii.duplex;\n\n\t \n\tif (!ep)\n\t\tgoto start_aneg;\n\tif (ep->base.autoneg == AUTONEG_ENABLE) {\n\t\tadvertise = advertising;\n\t\tautoneg = 1;\n\t} else {\n\t\tautoneg = 0;\n\t\tspeed = ep->base.speed;\n\t\tduplex = ep->base.duplex;\n\t}\n\nstart_aneg:\n\t \n\tif ((features & SUPPORTED_Autoneg) == 0)\n\t\tautoneg = 0;\n\tif (speed == SPEED_1000 &&\n\t    !(features & (SUPPORTED_1000baseT_Half | SUPPORTED_1000baseT_Full)))\n\t\tspeed = SPEED_100;\n\tif (speed == SPEED_100 &&\n\t    !(features & (SUPPORTED_100baseT_Half | SUPPORTED_100baseT_Full)))\n\t\tspeed = SPEED_10;\n\tif (duplex == DUPLEX_FULL &&\n\t    !(features & (SUPPORTED_1000baseT_Full |\n\t    \t\t  SUPPORTED_100baseT_Full |\n\t    \t\t  SUPPORTED_10baseT_Full)))\n\t    \tduplex = DUPLEX_HALF;\n\tif (speed == 0)\n\t\tspeed = SPEED_10;\n\n\t \n\tif (!netif_device_present(gp->dev)) {\n\t\tgp->phy_mii.autoneg = gp->want_autoneg = autoneg;\n\t\tgp->phy_mii.speed = speed;\n\t\tgp->phy_mii.duplex = duplex;\n\t\treturn;\n\t}\n\n\t \n\tgp->want_autoneg = autoneg;\n\tif (autoneg) {\n\t\tif (found_mii_phy(gp))\n\t\t\tgp->phy_mii.def->ops->setup_aneg(&gp->phy_mii, advertise);\n\t\tgp->lstate = link_aneg;\n\t} else {\n\t\tif (found_mii_phy(gp))\n\t\t\tgp->phy_mii.def->ops->setup_forced(&gp->phy_mii, speed, duplex);\n\t\tgp->lstate = link_force_ok;\n\t}\n\nnon_mii:\n\tgp->timer_ticks = 0;\n\tmod_timer(&gp->link_timer, jiffies + ((12 * HZ) / 10));\n}\n\n \nstatic int gem_set_link_modes(struct gem *gp)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(gp->dev, 0);\n\tint full_duplex, speed, pause;\n\tu32 val;\n\n\tfull_duplex = 0;\n\tspeed = SPEED_10;\n\tpause = 0;\n\n\tif (found_mii_phy(gp)) {\n\t    \tif (gp->phy_mii.def->ops->read_link(&gp->phy_mii))\n\t    \t\treturn 1;\n\t\tfull_duplex = (gp->phy_mii.duplex == DUPLEX_FULL);\n\t\tspeed = gp->phy_mii.speed;\n\t\tpause = gp->phy_mii.pause;\n\t} else if (gp->phy_type == phy_serialink ||\n\t    \t   gp->phy_type == phy_serdes) {\n\t\tu32 pcs_lpa = readl(gp->regs + PCS_MIILP);\n\n\t\tif ((pcs_lpa & PCS_MIIADV_FD) || gp->phy_type == phy_serdes)\n\t\t\tfull_duplex = 1;\n\t\tspeed = SPEED_1000;\n\t}\n\n\tnetif_info(gp, link, gp->dev, \"Link is up at %d Mbps, %s-duplex\\n\",\n\t\t   speed, (full_duplex ? \"full\" : \"half\"));\n\n\n\t \n\t__netif_tx_lock(txq, smp_processor_id());\n\n\tval = (MAC_TXCFG_EIPG0 | MAC_TXCFG_NGU);\n\tif (full_duplex) {\n\t\tval |= (MAC_TXCFG_ICS | MAC_TXCFG_ICOLL);\n\t} else {\n\t\t \n\t}\n\twritel(val, gp->regs + MAC_TXCFG);\n\n\tval = (MAC_XIFCFG_OE | MAC_XIFCFG_LLED);\n\tif (!full_duplex &&\n\t    (gp->phy_type == phy_mii_mdio0 ||\n\t     gp->phy_type == phy_mii_mdio1)) {\n\t\tval |= MAC_XIFCFG_DISE;\n\t} else if (full_duplex) {\n\t\tval |= MAC_XIFCFG_FLED;\n\t}\n\n\tif (speed == SPEED_1000)\n\t\tval |= (MAC_XIFCFG_GMII);\n\n\twritel(val, gp->regs + MAC_XIFCFG);\n\n\t \n\tif (speed == SPEED_1000 && !full_duplex) {\n\t\tval = readl(gp->regs + MAC_TXCFG);\n\t\twritel(val | MAC_TXCFG_TCE, gp->regs + MAC_TXCFG);\n\n\t\tval = readl(gp->regs + MAC_RXCFG);\n\t\twritel(val | MAC_RXCFG_RCE, gp->regs + MAC_RXCFG);\n\t} else {\n\t\tval = readl(gp->regs + MAC_TXCFG);\n\t\twritel(val & ~MAC_TXCFG_TCE, gp->regs + MAC_TXCFG);\n\n\t\tval = readl(gp->regs + MAC_RXCFG);\n\t\twritel(val & ~MAC_RXCFG_RCE, gp->regs + MAC_RXCFG);\n\t}\n\n\tif (gp->phy_type == phy_serialink ||\n\t    gp->phy_type == phy_serdes) {\n\t\tu32 pcs_lpa = readl(gp->regs + PCS_MIILP);\n\n\t\tif (pcs_lpa & (PCS_MIIADV_SP | PCS_MIIADV_AP))\n\t\t\tpause = 1;\n\t}\n\n\tif (!full_duplex)\n\t\twritel(512, gp->regs + MAC_STIME);\n\telse\n\t\twritel(64, gp->regs + MAC_STIME);\n\tval = readl(gp->regs + MAC_MCCFG);\n\tif (pause)\n\t\tval |= (MAC_MCCFG_SPE | MAC_MCCFG_RPE);\n\telse\n\t\tval &= ~(MAC_MCCFG_SPE | MAC_MCCFG_RPE);\n\twritel(val, gp->regs + MAC_MCCFG);\n\n\tgem_start_dma(gp);\n\n\t__netif_tx_unlock(txq);\n\n\tif (netif_msg_link(gp)) {\n\t\tif (pause) {\n\t\t\tnetdev_info(gp->dev,\n\t\t\t\t    \"Pause is enabled (rxfifo: %d off: %d on: %d)\\n\",\n\t\t\t\t    gp->rx_fifo_sz,\n\t\t\t\t    gp->rx_pause_off,\n\t\t\t\t    gp->rx_pause_on);\n\t\t} else {\n\t\t\tnetdev_info(gp->dev, \"Pause is disabled\\n\");\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gem_mdio_link_not_up(struct gem *gp)\n{\n\tswitch (gp->lstate) {\n\tcase link_force_ret:\n\t\tnetif_info(gp, link, gp->dev,\n\t\t\t   \"Autoneg failed again, keeping forced mode\\n\");\n\t\tgp->phy_mii.def->ops->setup_forced(&gp->phy_mii,\n\t\t\tgp->last_forced_speed, DUPLEX_HALF);\n\t\tgp->timer_ticks = 5;\n\t\tgp->lstate = link_force_ok;\n\t\treturn 0;\n\tcase link_aneg:\n\t\t \n\t\tif (gp->phy_mii.def->magic_aneg)\n\t\t\treturn 1;\n\t\tnetif_info(gp, link, gp->dev, \"switching to forced 100bt\\n\");\n\t\t \n\t\tgp->phy_mii.def->ops->setup_forced(&gp->phy_mii, SPEED_100,\n\t\t\tDUPLEX_HALF);\n\t\tgp->timer_ticks = 5;\n\t\tgp->lstate = link_force_try;\n\t\treturn 0;\n\tcase link_force_try:\n\t\t \n\t\tif (gp->phy_mii.speed == SPEED_100) {\n\t\t\tgp->phy_mii.def->ops->setup_forced(&gp->phy_mii, SPEED_10,\n\t\t\t\tDUPLEX_HALF);\n\t\t\tgp->timer_ticks = 5;\n\t\t\tnetif_info(gp, link, gp->dev,\n\t\t\t\t   \"switching to forced 10bt\\n\");\n\t\t\treturn 0;\n\t\t} else\n\t\t\treturn 1;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic void gem_link_timer(struct timer_list *t)\n{\n\tstruct gem *gp = from_timer(gp, t, link_timer);\n\tstruct net_device *dev = gp->dev;\n\tint restart_aneg = 0;\n\n\t \n\tif (gp->reset_task_pending)\n\t\treturn;\n\n\tif (gp->phy_type == phy_serialink ||\n\t    gp->phy_type == phy_serdes) {\n\t\tu32 val = readl(gp->regs + PCS_MIISTAT);\n\n\t\tif (!(val & PCS_MIISTAT_LS))\n\t\t\tval = readl(gp->regs + PCS_MIISTAT);\n\n\t\tif ((val & PCS_MIISTAT_LS) != 0) {\n\t\t\tif (gp->lstate == link_up)\n\t\t\t\tgoto restart;\n\n\t\t\tgp->lstate = link_up;\n\t\t\tnetif_carrier_on(dev);\n\t\t\t(void)gem_set_link_modes(gp);\n\t\t}\n\t\tgoto restart;\n\t}\n\tif (found_mii_phy(gp) && gp->phy_mii.def->ops->poll_link(&gp->phy_mii)) {\n\t\t \n\t\tif (gp->lstate == link_force_try && gp->want_autoneg) {\n\t\t\tgp->lstate = link_force_ret;\n\t\t\tgp->last_forced_speed = gp->phy_mii.speed;\n\t\t\tgp->timer_ticks = 5;\n\t\t\tif (netif_msg_link(gp))\n\t\t\t\tnetdev_info(dev,\n\t\t\t\t\t    \"Got link after fallback, retrying autoneg once...\\n\");\n\t\t\tgp->phy_mii.def->ops->setup_aneg(&gp->phy_mii, gp->phy_mii.advertising);\n\t\t} else if (gp->lstate != link_up) {\n\t\t\tgp->lstate = link_up;\n\t\t\tnetif_carrier_on(dev);\n\t\t\tif (gem_set_link_modes(gp))\n\t\t\t\trestart_aneg = 1;\n\t\t}\n\t} else {\n\t\t \n\t\tif (gp->lstate == link_up) {\n\t\t\tgp->lstate = link_down;\n\t\t\tnetif_info(gp, link, dev, \"Link down\\n\");\n\t\t\tnetif_carrier_off(dev);\n\t\t\tgem_schedule_reset(gp);\n\t\t\t \n\t\t\treturn;\n\t\t} else if (++gp->timer_ticks > 10) {\n\t\t\tif (found_mii_phy(gp))\n\t\t\t\trestart_aneg = gem_mdio_link_not_up(gp);\n\t\t\telse\n\t\t\t\trestart_aneg = 1;\n\t\t}\n\t}\n\tif (restart_aneg) {\n\t\tgem_begin_auto_negotiation(gp, NULL);\n\t\treturn;\n\t}\nrestart:\n\tmod_timer(&gp->link_timer, jiffies + ((12 * HZ) / 10));\n}\n\nstatic void gem_clean_rings(struct gem *gp)\n{\n\tstruct gem_init_block *gb = gp->init_block;\n\tstruct sk_buff *skb;\n\tint i;\n\tdma_addr_t dma_addr;\n\n\tfor (i = 0; i < RX_RING_SIZE; i++) {\n\t\tstruct gem_rxd *rxd;\n\n\t\trxd = &gb->rxd[i];\n\t\tif (gp->rx_skbs[i] != NULL) {\n\t\t\tskb = gp->rx_skbs[i];\n\t\t\tdma_addr = le64_to_cpu(rxd->buffer);\n\t\t\tdma_unmap_page(&gp->pdev->dev, dma_addr,\n\t\t\t\t       RX_BUF_ALLOC_SIZE(gp),\n\t\t\t\t       DMA_FROM_DEVICE);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgp->rx_skbs[i] = NULL;\n\t\t}\n\t\trxd->status_word = 0;\n\t\tdma_wmb();\n\t\trxd->buffer = 0;\n\t}\n\n\tfor (i = 0; i < TX_RING_SIZE; i++) {\n\t\tif (gp->tx_skbs[i] != NULL) {\n\t\t\tstruct gem_txd *txd;\n\t\t\tint frag;\n\n\t\t\tskb = gp->tx_skbs[i];\n\t\t\tgp->tx_skbs[i] = NULL;\n\n\t\t\tfor (frag = 0; frag <= skb_shinfo(skb)->nr_frags; frag++) {\n\t\t\t\tint ent = i & (TX_RING_SIZE - 1);\n\n\t\t\t\ttxd = &gb->txd[ent];\n\t\t\t\tdma_addr = le64_to_cpu(txd->buffer);\n\t\t\t\tdma_unmap_page(&gp->pdev->dev, dma_addr,\n\t\t\t\t\t       le64_to_cpu(txd->control_word) &\n\t\t\t\t\t       TXDCTRL_BUFSZ, DMA_TO_DEVICE);\n\n\t\t\t\tif (frag != skb_shinfo(skb)->nr_frags)\n\t\t\t\t\ti++;\n\t\t\t}\n\t\t\tdev_kfree_skb_any(skb);\n\t\t}\n\t}\n}\n\nstatic void gem_init_rings(struct gem *gp)\n{\n\tstruct gem_init_block *gb = gp->init_block;\n\tstruct net_device *dev = gp->dev;\n\tint i;\n\tdma_addr_t dma_addr;\n\n\tgp->rx_new = gp->rx_old = gp->tx_new = gp->tx_old = 0;\n\n\tgem_clean_rings(gp);\n\n\tgp->rx_buf_sz = max(dev->mtu + ETH_HLEN + VLAN_HLEN,\n\t\t\t    (unsigned)VLAN_ETH_FRAME_LEN);\n\n\tfor (i = 0; i < RX_RING_SIZE; i++) {\n\t\tstruct sk_buff *skb;\n\t\tstruct gem_rxd *rxd = &gb->rxd[i];\n\n\t\tskb = gem_alloc_skb(dev, RX_BUF_ALLOC_SIZE(gp), GFP_KERNEL);\n\t\tif (!skb) {\n\t\t\trxd->buffer = 0;\n\t\t\trxd->status_word = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tgp->rx_skbs[i] = skb;\n\t\tskb_put(skb, (gp->rx_buf_sz + RX_OFFSET));\n\t\tdma_addr = dma_map_page(&gp->pdev->dev,\n\t\t\t\t\tvirt_to_page(skb->data),\n\t\t\t\t\toffset_in_page(skb->data),\n\t\t\t\t\tRX_BUF_ALLOC_SIZE(gp),\n\t\t\t\t\tDMA_FROM_DEVICE);\n\t\trxd->buffer = cpu_to_le64(dma_addr);\n\t\tdma_wmb();\n\t\trxd->status_word = cpu_to_le64(RXDCTRL_FRESH(gp));\n\t\tskb_reserve(skb, RX_OFFSET);\n\t}\n\n\tfor (i = 0; i < TX_RING_SIZE; i++) {\n\t\tstruct gem_txd *txd = &gb->txd[i];\n\n\t\ttxd->control_word = 0;\n\t\tdma_wmb();\n\t\ttxd->buffer = 0;\n\t}\n\twmb();\n}\n\n \nstatic void gem_init_phy(struct gem *gp)\n{\n\tu32 mifcfg;\n\n\t \n\tmifcfg = readl(gp->regs + MIF_CFG);\n\tmifcfg &= ~MIF_CFG_BBMODE;\n\twritel(mifcfg, gp->regs + MIF_CFG);\n\n\tif (gp->pdev->vendor == PCI_VENDOR_ID_APPLE) {\n\t\tint i;\n\n\t\t \n\t\tfor (i = 0; i < 3; i++) {\n#ifdef CONFIG_PPC_PMAC\n\t\t\tpmac_call_feature(PMAC_FTR_GMAC_PHY_RESET, gp->of_node, 0, 0);\n\t\t\tmsleep(20);\n#endif\n\t\t\t \n\t\t\tsungem_phy_write(gp, MII_BMCR, BMCR_RESET);\n\t\t\tmsleep(20);\n\t\t\tif (sungem_phy_read(gp, MII_BMCR) != 0xffff)\n\t\t\t\tbreak;\n\t\t\tif (i == 2)\n\t\t\t\tnetdev_warn(gp->dev, \"GMAC PHY not responding !\\n\");\n\t\t}\n\t}\n\n\tif (gp->pdev->vendor == PCI_VENDOR_ID_SUN &&\n\t    gp->pdev->device == PCI_DEVICE_ID_SUN_GEM) {\n\t\tu32 val;\n\n\t\t \n\t\tif (gp->phy_type == phy_mii_mdio0 ||\n\t\t    gp->phy_type == phy_mii_mdio1) {\n\t\t\tval = PCS_DMODE_MGM;\n\t\t} else if (gp->phy_type == phy_serialink) {\n\t\t\tval = PCS_DMODE_SM | PCS_DMODE_GMOE;\n\t\t} else {\n\t\t\tval = PCS_DMODE_ESM;\n\t\t}\n\n\t\twritel(val, gp->regs + PCS_DMODE);\n\t}\n\n\tif (gp->phy_type == phy_mii_mdio0 ||\n\t    gp->phy_type == phy_mii_mdio1) {\n\t\t \n\t\tsungem_phy_probe(&gp->phy_mii, gp->mii_phy_addr);\n\n\t\t \n\t\tif (gp->phy_mii.def && gp->phy_mii.def->ops->init)\n\t\t\tgp->phy_mii.def->ops->init(&gp->phy_mii);\n\t} else {\n\t\tgem_pcs_reset(gp);\n\t\tgem_pcs_reinit_adv(gp);\n\t}\n\n\t \n\tgp->timer_ticks = 0;\n\tgp->lstate = link_down;\n\tnetif_carrier_off(gp->dev);\n\n\t \n\tif (gp->phy_type == phy_mii_mdio0 ||\n\t    gp->phy_type == phy_mii_mdio1)\n\t\tnetdev_info(gp->dev, \"Found %s PHY\\n\",\n\t\t\t    gp->phy_mii.def ? gp->phy_mii.def->name : \"no\");\n\n\tgem_begin_auto_negotiation(gp, NULL);\n}\n\nstatic void gem_init_dma(struct gem *gp)\n{\n\tu64 desc_dma = (u64) gp->gblock_dvma;\n\tu32 val;\n\n\tval = (TXDMA_CFG_BASE | (0x7ff << 10) | TXDMA_CFG_PMODE);\n\twritel(val, gp->regs + TXDMA_CFG);\n\n\twritel(desc_dma >> 32, gp->regs + TXDMA_DBHI);\n\twritel(desc_dma & 0xffffffff, gp->regs + TXDMA_DBLOW);\n\tdesc_dma += (INIT_BLOCK_TX_RING_SIZE * sizeof(struct gem_txd));\n\n\twritel(0, gp->regs + TXDMA_KICK);\n\n\tval = (RXDMA_CFG_BASE | (RX_OFFSET << 10) |\n\t       (ETH_HLEN << 13) | RXDMA_CFG_FTHRESH_128);\n\twritel(val, gp->regs + RXDMA_CFG);\n\n\twritel(desc_dma >> 32, gp->regs + RXDMA_DBHI);\n\twritel(desc_dma & 0xffffffff, gp->regs + RXDMA_DBLOW);\n\n\twritel(RX_RING_SIZE - 4, gp->regs + RXDMA_KICK);\n\n\tval  = (((gp->rx_pause_off / 64) << 0) & RXDMA_PTHRESH_OFF);\n\tval |= (((gp->rx_pause_on / 64) << 12) & RXDMA_PTHRESH_ON);\n\twritel(val, gp->regs + RXDMA_PTHRESH);\n\n\tif (readl(gp->regs + GREG_BIFCFG) & GREG_BIFCFG_M66EN)\n\t\twritel(((5 & RXDMA_BLANK_IPKTS) |\n\t\t\t((8 << 12) & RXDMA_BLANK_ITIME)),\n\t\t       gp->regs + RXDMA_BLANK);\n\telse\n\t\twritel(((5 & RXDMA_BLANK_IPKTS) |\n\t\t\t((4 << 12) & RXDMA_BLANK_ITIME)),\n\t\t       gp->regs + RXDMA_BLANK);\n}\n\nstatic u32 gem_setup_multicast(struct gem *gp)\n{\n\tu32 rxcfg = 0;\n\tint i;\n\n\tif ((gp->dev->flags & IFF_ALLMULTI) ||\n\t    (netdev_mc_count(gp->dev) > 256)) {\n\t    \tfor (i=0; i<16; i++)\n\t\t\twritel(0xffff, gp->regs + MAC_HASH0 + (i << 2));\n\t\trxcfg |= MAC_RXCFG_HFE;\n\t} else if (gp->dev->flags & IFF_PROMISC) {\n\t\trxcfg |= MAC_RXCFG_PROM;\n\t} else {\n\t\tu16 hash_table[16];\n\t\tu32 crc;\n\t\tstruct netdev_hw_addr *ha;\n\t\tint i;\n\n\t\tmemset(hash_table, 0, sizeof(hash_table));\n\t\tnetdev_for_each_mc_addr(ha, gp->dev) {\n\t\t\tcrc = ether_crc_le(6, ha->addr);\n\t\t\tcrc >>= 24;\n\t\t\thash_table[crc >> 4] |= 1 << (15 - (crc & 0xf));\n\t\t}\n\t    \tfor (i=0; i<16; i++)\n\t\t\twritel(hash_table[i], gp->regs + MAC_HASH0 + (i << 2));\n\t\trxcfg |= MAC_RXCFG_HFE;\n\t}\n\n\treturn rxcfg;\n}\n\nstatic void gem_init_mac(struct gem *gp)\n{\n\tconst unsigned char *e = &gp->dev->dev_addr[0];\n\n\twritel(0x1bf0, gp->regs + MAC_SNDPAUSE);\n\n\twritel(0x00, gp->regs + MAC_IPG0);\n\twritel(0x08, gp->regs + MAC_IPG1);\n\twritel(0x04, gp->regs + MAC_IPG2);\n\twritel(0x40, gp->regs + MAC_STIME);\n\twritel(0x40, gp->regs + MAC_MINFSZ);\n\n\t \n\twritel(0x20000000 | (gp->rx_buf_sz + 4), gp->regs + MAC_MAXFSZ);\n\n\twritel(0x07, gp->regs + MAC_PASIZE);\n\twritel(0x04, gp->regs + MAC_JAMSIZE);\n\twritel(0x10, gp->regs + MAC_ATTLIM);\n\twritel(0x8808, gp->regs + MAC_MCTYPE);\n\n\twritel((e[5] | (e[4] << 8)) & 0x3ff, gp->regs + MAC_RANDSEED);\n\n\twritel((e[4] << 8) | e[5], gp->regs + MAC_ADDR0);\n\twritel((e[2] << 8) | e[3], gp->regs + MAC_ADDR1);\n\twritel((e[0] << 8) | e[1], gp->regs + MAC_ADDR2);\n\n\twritel(0, gp->regs + MAC_ADDR3);\n\twritel(0, gp->regs + MAC_ADDR4);\n\twritel(0, gp->regs + MAC_ADDR5);\n\n\twritel(0x0001, gp->regs + MAC_ADDR6);\n\twritel(0xc200, gp->regs + MAC_ADDR7);\n\twritel(0x0180, gp->regs + MAC_ADDR8);\n\n\twritel(0, gp->regs + MAC_AFILT0);\n\twritel(0, gp->regs + MAC_AFILT1);\n\twritel(0, gp->regs + MAC_AFILT2);\n\twritel(0, gp->regs + MAC_AF21MSK);\n\twritel(0, gp->regs + MAC_AF0MSK);\n\n\tgp->mac_rx_cfg = gem_setup_multicast(gp);\n#ifdef STRIP_FCS\n\tgp->mac_rx_cfg |= MAC_RXCFG_SFCS;\n#endif\n\twritel(0, gp->regs + MAC_NCOLL);\n\twritel(0, gp->regs + MAC_FASUCC);\n\twritel(0, gp->regs + MAC_ECOLL);\n\twritel(0, gp->regs + MAC_LCOLL);\n\twritel(0, gp->regs + MAC_DTIMER);\n\twritel(0, gp->regs + MAC_PATMPS);\n\twritel(0, gp->regs + MAC_RFCTR);\n\twritel(0, gp->regs + MAC_LERR);\n\twritel(0, gp->regs + MAC_AERR);\n\twritel(0, gp->regs + MAC_FCSERR);\n\twritel(0, gp->regs + MAC_RXCVERR);\n\n\t \n\twritel(0, gp->regs + MAC_TXCFG);\n\twritel(gp->mac_rx_cfg, gp->regs + MAC_RXCFG);\n\twritel(0, gp->regs + MAC_MCCFG);\n\twritel(0, gp->regs + MAC_XIFCFG);\n\n\t \n\twritel(MAC_TXSTAT_XMIT, gp->regs + MAC_TXMASK);\n\twritel(MAC_RXSTAT_RCV, gp->regs + MAC_RXMASK);\n\n\t \n\twritel(0xffffffff, gp->regs + MAC_MCMASK);\n\n\t \n\tif (gp->has_wol)\n\t\twritel(0, gp->regs + WOL_WAKECSR);\n}\n\nstatic void gem_init_pause_thresholds(struct gem *gp)\n{\n\tu32 cfg;\n\n\t \n\tif (gp->rx_fifo_sz <= (2 * 1024)) {\n\t\tgp->rx_pause_off = gp->rx_pause_on = gp->rx_fifo_sz;\n\t} else {\n\t\tint max_frame = (gp->rx_buf_sz + 4 + 64) & ~63;\n\t\tint off = (gp->rx_fifo_sz - (max_frame * 2));\n\t\tint on = off - max_frame;\n\n\t\tgp->rx_pause_off = off;\n\t\tgp->rx_pause_on = on;\n\t}\n\n\n\t \n\tcfg  = 0;\n\tif (gp->pdev->vendor == PCI_VENDOR_ID_APPLE)\n\t\tcfg |= GREG_CFG_RONPAULBIT | GREG_CFG_ENBUG2FIX;\n#if !defined(CONFIG_SPARC64) && !defined(CONFIG_ALPHA)\n\tcfg |= GREG_CFG_IBURST;\n#endif\n\tcfg |= ((31 << 1) & GREG_CFG_TXDMALIM);\n\tcfg |= ((31 << 6) & GREG_CFG_RXDMALIM);\n\twritel(cfg, gp->regs + GREG_CFG);\n\n\t \n\tif (!(readl(gp->regs + GREG_CFG) & GREG_CFG_IBURST)) {\n\t\tcfg = ((2 << 1) & GREG_CFG_TXDMALIM);\n\t\tcfg |= ((8 << 6) & GREG_CFG_RXDMALIM);\n\t\twritel(cfg, gp->regs + GREG_CFG);\n\t}\n}\n\nstatic int gem_check_invariants(struct gem *gp)\n{\n\tstruct pci_dev *pdev = gp->pdev;\n\tu32 mif_cfg;\n\n\t \n\tif (pdev->vendor == PCI_VENDOR_ID_APPLE) {\n\t\tgp->phy_type = phy_mii_mdio0;\n\t\tgp->tx_fifo_sz = readl(gp->regs + TXDMA_FSZ) * 64;\n\t\tgp->rx_fifo_sz = readl(gp->regs + RXDMA_FSZ) * 64;\n\t\tgp->swrst_base = 0;\n\n\t\tmif_cfg = readl(gp->regs + MIF_CFG);\n\t\tmif_cfg &= ~(MIF_CFG_PSELECT|MIF_CFG_POLL|MIF_CFG_BBMODE|MIF_CFG_MDI1);\n\t\tmif_cfg |= MIF_CFG_MDI0;\n\t\twritel(mif_cfg, gp->regs + MIF_CFG);\n\t\twritel(PCS_DMODE_MGM, gp->regs + PCS_DMODE);\n\t\twritel(MAC_XIFCFG_OE, gp->regs + MAC_XIFCFG);\n\n\t\t \n\t\tif (gp->pdev->device == PCI_DEVICE_ID_APPLE_K2_GMAC)\n\t\t\tgp->mii_phy_addr = 1;\n\t\telse\n\t\t\tgp->mii_phy_addr = 0;\n\n\t\treturn 0;\n\t}\n\n\tmif_cfg = readl(gp->regs + MIF_CFG);\n\n\tif (pdev->vendor == PCI_VENDOR_ID_SUN &&\n\t    pdev->device == PCI_DEVICE_ID_SUN_RIO_GEM) {\n\t\t \n\t\tif ((mif_cfg & (MIF_CFG_MDI0 | MIF_CFG_MDI1)) == 0) {\n\t\t\tpr_err(\"RIO GEM lacks MII phy, mif_cfg[%08x]\\n\",\n\t\t\t       mif_cfg);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\t \n\n\tif (mif_cfg & MIF_CFG_MDI1) {\n\t\tgp->phy_type = phy_mii_mdio1;\n\t\tmif_cfg |= MIF_CFG_PSELECT;\n\t\twritel(mif_cfg, gp->regs + MIF_CFG);\n\t} else if (mif_cfg & MIF_CFG_MDI0) {\n\t\tgp->phy_type = phy_mii_mdio0;\n\t\tmif_cfg &= ~MIF_CFG_PSELECT;\n\t\twritel(mif_cfg, gp->regs + MIF_CFG);\n\t} else {\n#ifdef CONFIG_SPARC\n\t\tconst char *p;\n\n\t\tp = of_get_property(gp->of_node, \"shared-pins\", NULL);\n\t\tif (p && !strcmp(p, \"serdes\"))\n\t\t\tgp->phy_type = phy_serdes;\n\t\telse\n#endif\n\t\t\tgp->phy_type = phy_serialink;\n\t}\n\tif (gp->phy_type == phy_mii_mdio1 ||\n\t    gp->phy_type == phy_mii_mdio0) {\n\t\tint i;\n\n\t\tfor (i = 0; i < 32; i++) {\n\t\t\tgp->mii_phy_addr = i;\n\t\t\tif (sungem_phy_read(gp, MII_BMCR) != 0xffff)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (i == 32) {\n\t\t\tif (pdev->device != PCI_DEVICE_ID_SUN_GEM) {\n\t\t\t\tpr_err(\"RIO MII phy will not respond\\n\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tgp->phy_type = phy_serdes;\n\t\t}\n\t}\n\n\t \n\tgp->tx_fifo_sz = readl(gp->regs + TXDMA_FSZ) * 64;\n\tgp->rx_fifo_sz = readl(gp->regs + RXDMA_FSZ) * 64;\n\n\tif (pdev->vendor == PCI_VENDOR_ID_SUN) {\n\t\tif (pdev->device == PCI_DEVICE_ID_SUN_GEM) {\n\t\t\tif (gp->tx_fifo_sz != (9 * 1024) ||\n\t\t\t    gp->rx_fifo_sz != (20 * 1024)) {\n\t\t\t\tpr_err(\"GEM has bogus fifo sizes tx(%d) rx(%d)\\n\",\n\t\t\t\t       gp->tx_fifo_sz, gp->rx_fifo_sz);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tgp->swrst_base = 0;\n\t\t} else {\n\t\t\tif (gp->tx_fifo_sz != (2 * 1024) ||\n\t\t\t    gp->rx_fifo_sz != (2 * 1024)) {\n\t\t\t\tpr_err(\"RIO GEM has bogus fifo sizes tx(%d) rx(%d)\\n\",\n\t\t\t\t       gp->tx_fifo_sz, gp->rx_fifo_sz);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tgp->swrst_base = (64 / 4) << GREG_SWRST_CACHE_SHIFT;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void gem_reinit_chip(struct gem *gp)\n{\n\t \n\tgem_reset(gp);\n\n\t \n\tgem_disable_ints(gp);\n\n\t \n\tgem_init_rings(gp);\n\n\t \n\tgem_init_pause_thresholds(gp);\n\n\t \n\tgem_init_dma(gp);\n\tgem_init_mac(gp);\n}\n\n\nstatic void gem_stop_phy(struct gem *gp, int wol)\n{\n\tu32 mifcfg;\n\n\t \n\tmsleep(10);\n\n\t \n\tmifcfg = readl(gp->regs + MIF_CFG);\n\tmifcfg &= ~MIF_CFG_POLL;\n\twritel(mifcfg, gp->regs + MIF_CFG);\n\n\tif (wol && gp->has_wol) {\n\t\tconst unsigned char *e = &gp->dev->dev_addr[0];\n\t\tu32 csr;\n\n\t\t \n\t\twritel(MAC_RXCFG_HFE | MAC_RXCFG_SFCS | MAC_RXCFG_ENAB,\n\t\t       gp->regs + MAC_RXCFG);\n\t\twritel((e[4] << 8) | e[5], gp->regs + WOL_MATCH0);\n\t\twritel((e[2] << 8) | e[3], gp->regs + WOL_MATCH1);\n\t\twritel((e[0] << 8) | e[1], gp->regs + WOL_MATCH2);\n\n\t\twritel(WOL_MCOUNT_N | WOL_MCOUNT_M, gp->regs + WOL_MCOUNT);\n\t\tcsr = WOL_WAKECSR_ENABLE;\n\t\tif ((readl(gp->regs + MAC_XIFCFG) & MAC_XIFCFG_GMII) == 0)\n\t\t\tcsr |= WOL_WAKECSR_MII;\n\t\twritel(csr, gp->regs + WOL_WAKECSR);\n\t} else {\n\t\twritel(0, gp->regs + MAC_RXCFG);\n\t\t(void)readl(gp->regs + MAC_RXCFG);\n\t\t \n\t\tmsleep(10);\n\t}\n\n\twritel(0, gp->regs + MAC_TXCFG);\n\twritel(0, gp->regs + MAC_XIFCFG);\n\twritel(0, gp->regs + TXDMA_CFG);\n\twritel(0, gp->regs + RXDMA_CFG);\n\n\tif (!wol) {\n\t\tgem_reset(gp);\n\t\twritel(MAC_TXRST_CMD, gp->regs + MAC_TXRST);\n\t\twritel(MAC_RXRST_CMD, gp->regs + MAC_RXRST);\n\n\t\tif (found_mii_phy(gp) && gp->phy_mii.def->ops->suspend)\n\t\t\tgp->phy_mii.def->ops->suspend(&gp->phy_mii);\n\n\t\t \n\t\twritel(mifcfg | MIF_CFG_BBMODE, gp->regs + MIF_CFG);\n\t\twritel(0, gp->regs + MIF_BBCLK);\n\t\twritel(0, gp->regs + MIF_BBDATA);\n\t\twritel(0, gp->regs + MIF_BBOENAB);\n\t\twritel(MAC_XIFCFG_GMII | MAC_XIFCFG_LBCK, gp->regs + MAC_XIFCFG);\n\t\t(void) readl(gp->regs + MAC_XIFCFG);\n\t}\n}\n\nstatic int gem_do_start(struct net_device *dev)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\tint rc;\n\n\tpci_set_master(gp->pdev);\n\n\t \n\tgem_reinit_chip(gp);\n\n\t \n\trc = request_irq(gp->pdev->irq, gem_interrupt,\n\t\t\t IRQF_SHARED, dev->name, (void *)dev);\n\tif (rc) {\n\t\tnetdev_err(dev, \"failed to request irq !\\n\");\n\n\t\tgem_reset(gp);\n\t\tgem_clean_rings(gp);\n\t\tgem_put_cell(gp);\n\t\treturn rc;\n\t}\n\n\t \n\tnetif_device_attach(dev);\n\n\t \n\tgem_netif_start(gp);\n\n\t \n\tgem_init_phy(gp);\n\n\treturn 0;\n}\n\nstatic void gem_do_stop(struct net_device *dev, int wol)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\t \n\tgem_netif_stop(gp);\n\n\t \n\tgem_disable_ints(gp);\n\n\t \n\tdel_timer_sync(&gp->link_timer);\n\n\t \n\tgp->reset_task_pending = 0;\n\n\t \n\tgem_stop_dma(gp);\n\tmsleep(10);\n\tif (!wol)\n\t\tgem_reset(gp);\n\tmsleep(10);\n\n\t \n\tgem_clean_rings(gp);\n\n\t \n\tfree_irq(gp->pdev->irq, (void *) dev);\n\n\t \n\tgem_stop_phy(gp, wol);\n}\n\nstatic void gem_reset_task(struct work_struct *work)\n{\n\tstruct gem *gp = container_of(work, struct gem, reset_task);\n\n\t \n\trtnl_lock();\n\n\t \n\tif (!netif_device_present(gp->dev) ||\n\t    !netif_running(gp->dev) ||\n\t    !gp->reset_task_pending) {\n\t\trtnl_unlock();\n\t\treturn;\n\t}\n\n\t \n\tdel_timer_sync(&gp->link_timer);\n\n\t \n\tgem_netif_stop(gp);\n\n\t \n\tgem_reinit_chip(gp);\n\tif (gp->lstate == link_up)\n\t\tgem_set_link_modes(gp);\n\n\t \n\tgem_netif_start(gp);\n\n\t \n\tgp->reset_task_pending = 0;\n\n\t \n\tif (gp->lstate != link_up)\n\t\tgem_begin_auto_negotiation(gp, NULL);\n\telse\n\t\tmod_timer(&gp->link_timer, jiffies + ((12 * HZ) / 10));\n\n\trtnl_unlock();\n}\n\nstatic int gem_open(struct net_device *dev)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\tint rc;\n\n\t \n\tif (netif_device_present(dev)) {\n\t\t \n\t\tgem_get_cell(gp);\n\n\t\t \n\t\trc = pci_enable_device(gp->pdev);\n\t\tif (rc) {\n\t\t\tnetdev_err(dev, \"Failed to enable chip on PCI bus !\\n\");\n\n\t\t\t \n\t\t\tgem_put_cell(gp);\n\t\t\treturn -ENXIO;\n\t\t}\n\t\treturn gem_do_start(dev);\n\t}\n\n\treturn 0;\n}\n\nstatic int gem_close(struct net_device *dev)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\tif (netif_device_present(dev)) {\n\t\tgem_do_stop(dev, 0);\n\n\t\t \n\t\tpci_disable_device(gp->pdev);\n\n\t\t \n\t\tif (!gp->asleep_wol)\n\t\t\tgem_put_cell(gp);\n\t}\n\treturn 0;\n}\n\nstatic int __maybe_unused gem_suspend(struct device *dev_d)\n{\n\tstruct net_device *dev = dev_get_drvdata(dev_d);\n\tstruct gem *gp = netdev_priv(dev);\n\n\t \n\trtnl_lock();\n\n\t \n\tif (!netif_running(dev)) {\n\t\tnetif_device_detach(dev);\n\t\trtnl_unlock();\n\t\treturn 0;\n\t}\n\tnetdev_info(dev, \"suspending, WakeOnLan %s\\n\",\n\t\t    (gp->wake_on_lan && netif_running(dev)) ?\n\t\t    \"enabled\" : \"disabled\");\n\n\t \n\tnetif_device_detach(dev);\n\n\t \n\tgp->asleep_wol = !!gp->wake_on_lan;\n\tgem_do_stop(dev, gp->asleep_wol);\n\n\t \n\tif (!gp->asleep_wol)\n\t\tgem_put_cell(gp);\n\n\t \n\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic int __maybe_unused gem_resume(struct device *dev_d)\n{\n\tstruct net_device *dev = dev_get_drvdata(dev_d);\n\tstruct gem *gp = netdev_priv(dev);\n\n\t \n\trtnl_lock();\n\n\t \n\tif (!netif_running(dev)) {\n\t\tnetif_device_attach(dev);\n\t\trtnl_unlock();\n\t\treturn 0;\n\t}\n\n\t \n\tgem_get_cell(gp);\n\n\t \n\tgem_do_start(dev);\n\n\t \n\tif (gp->asleep_wol)\n\t\tgem_put_cell(gp);\n\n\t \n\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic struct net_device_stats *gem_get_stats(struct net_device *dev)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\t \n\tif (!netif_device_present(dev) || !netif_running(dev))\n\t\tgoto bail;\n\n\t \n\tif (WARN_ON(!gp->cell_enabled))\n\t\tgoto bail;\n\n\tdev->stats.rx_crc_errors += readl(gp->regs + MAC_FCSERR);\n\twritel(0, gp->regs + MAC_FCSERR);\n\n\tdev->stats.rx_frame_errors += readl(gp->regs + MAC_AERR);\n\twritel(0, gp->regs + MAC_AERR);\n\n\tdev->stats.rx_length_errors += readl(gp->regs + MAC_LERR);\n\twritel(0, gp->regs + MAC_LERR);\n\n\tdev->stats.tx_aborted_errors += readl(gp->regs + MAC_ECOLL);\n\tdev->stats.collisions +=\n\t\t(readl(gp->regs + MAC_ECOLL) + readl(gp->regs + MAC_LCOLL));\n\twritel(0, gp->regs + MAC_ECOLL);\n\twritel(0, gp->regs + MAC_LCOLL);\n bail:\n\treturn &dev->stats;\n}\n\nstatic int gem_set_mac_address(struct net_device *dev, void *addr)\n{\n\tstruct sockaddr *macaddr = (struct sockaddr *) addr;\n\tconst unsigned char *e = &dev->dev_addr[0];\n\tstruct gem *gp = netdev_priv(dev);\n\n\tif (!is_valid_ether_addr(macaddr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\teth_hw_addr_set(dev, macaddr->sa_data);\n\n\t \n\tif (!netif_running(dev) || !netif_device_present(dev))\n\t\treturn 0;\n\n\t \n\tif (WARN_ON(!gp->cell_enabled))\n\t\treturn 0;\n\n\twritel((e[4] << 8) | e[5], gp->regs + MAC_ADDR0);\n\twritel((e[2] << 8) | e[3], gp->regs + MAC_ADDR1);\n\twritel((e[0] << 8) | e[1], gp->regs + MAC_ADDR2);\n\n\treturn 0;\n}\n\nstatic void gem_set_multicast(struct net_device *dev)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\tu32 rxcfg, rxcfg_new;\n\tint limit = 10000;\n\n\tif (!netif_running(dev) || !netif_device_present(dev))\n\t\treturn;\n\n\t \n\tif (gp->reset_task_pending || WARN_ON(!gp->cell_enabled))\n\t\treturn;\n\n\trxcfg = readl(gp->regs + MAC_RXCFG);\n\trxcfg_new = gem_setup_multicast(gp);\n#ifdef STRIP_FCS\n\trxcfg_new |= MAC_RXCFG_SFCS;\n#endif\n\tgp->mac_rx_cfg = rxcfg_new;\n\n\twritel(rxcfg & ~MAC_RXCFG_ENAB, gp->regs + MAC_RXCFG);\n\twhile (readl(gp->regs + MAC_RXCFG) & MAC_RXCFG_ENAB) {\n\t\tif (!limit--)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\n\trxcfg &= ~(MAC_RXCFG_PROM | MAC_RXCFG_HFE);\n\trxcfg |= rxcfg_new;\n\n\twritel(rxcfg, gp->regs + MAC_RXCFG);\n}\n\n \n#define GEM_MIN_MTU\tETH_MIN_MTU\n#if 1\n#define GEM_MAX_MTU\tETH_DATA_LEN\n#else\n#define GEM_MAX_MTU\t9000\n#endif\n\nstatic int gem_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\tdev->mtu = new_mtu;\n\n\t \n\tif (!netif_running(dev) || !netif_device_present(dev))\n\t\treturn 0;\n\n\t \n\tif (WARN_ON(!gp->cell_enabled))\n\t\treturn 0;\n\n\tgem_netif_stop(gp);\n\tgem_reinit_chip(gp);\n\tif (gp->lstate == link_up)\n\t\tgem_set_link_modes(gp);\n\tgem_netif_start(gp);\n\n\treturn 0;\n}\n\nstatic void gem_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\tstrscpy(info->driver, DRV_NAME, sizeof(info->driver));\n\tstrscpy(info->version, DRV_VERSION, sizeof(info->version));\n\tstrscpy(info->bus_info, pci_name(gp->pdev), sizeof(info->bus_info));\n}\n\nstatic int gem_get_link_ksettings(struct net_device *dev,\n\t\t\t\t  struct ethtool_link_ksettings *cmd)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\tu32 supported, advertising;\n\n\tif (gp->phy_type == phy_mii_mdio0 ||\n\t    gp->phy_type == phy_mii_mdio1) {\n\t\tif (gp->phy_mii.def)\n\t\t\tsupported = gp->phy_mii.def->features;\n\t\telse\n\t\t\tsupported = (SUPPORTED_10baseT_Half |\n\t\t\t\t\t  SUPPORTED_10baseT_Full);\n\n\t\t \n\t\tcmd->base.port = PORT_MII;\n\t\tcmd->base.phy_address = 0;  \n\n\t\t \n\t\tcmd->base.autoneg = gp->want_autoneg;\n\t\tcmd->base.speed = gp->phy_mii.speed;\n\t\tcmd->base.duplex = gp->phy_mii.duplex;\n\t\tadvertising = gp->phy_mii.advertising;\n\n\t\t \n\t\tif (advertising == 0)\n\t\t\tadvertising = supported;\n\t} else {  \n\t\tsupported =\n\t\t\t(SUPPORTED_10baseT_Half | SUPPORTED_10baseT_Full |\n\t\t\t SUPPORTED_100baseT_Half | SUPPORTED_100baseT_Full |\n\t\t\t SUPPORTED_Autoneg);\n\t\tadvertising = supported;\n\t\tcmd->base.speed = 0;\n\t\tcmd->base.duplex = 0;\n\t\tcmd->base.port = 0;\n\t\tcmd->base.phy_address = 0;\n\t\tcmd->base.autoneg = 0;\n\n\t\t \n\t\tif (gp->phy_type == phy_serdes) {\n\t\t\tcmd->base.port = PORT_FIBRE;\n\t\t\tsupported = (SUPPORTED_1000baseT_Half |\n\t\t\t\tSUPPORTED_1000baseT_Full |\n\t\t\t\tSUPPORTED_FIBRE | SUPPORTED_Autoneg |\n\t\t\t\tSUPPORTED_Pause | SUPPORTED_Asym_Pause);\n\t\t\tadvertising = supported;\n\t\t\tif (gp->lstate == link_up)\n\t\t\t\tcmd->base.speed = SPEED_1000;\n\t\t\tcmd->base.duplex = DUPLEX_FULL;\n\t\t\tcmd->base.autoneg = 1;\n\t\t}\n\t}\n\n\tethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.supported,\n\t\t\t\t\t\tsupported);\n\tethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.advertising,\n\t\t\t\t\t\tadvertising);\n\n\treturn 0;\n}\n\nstatic int gem_set_link_ksettings(struct net_device *dev,\n\t\t\t\t  const struct ethtool_link_ksettings *cmd)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\tu32 speed = cmd->base.speed;\n\tu32 advertising;\n\n\tethtool_convert_link_mode_to_legacy_u32(&advertising,\n\t\t\t\t\t\tcmd->link_modes.advertising);\n\n\t \n\tif (cmd->base.autoneg != AUTONEG_ENABLE &&\n\t    cmd->base.autoneg != AUTONEG_DISABLE)\n\t\treturn -EINVAL;\n\n\tif (cmd->base.autoneg == AUTONEG_ENABLE &&\n\t    advertising == 0)\n\t\treturn -EINVAL;\n\n\tif (cmd->base.autoneg == AUTONEG_DISABLE &&\n\t    ((speed != SPEED_1000 &&\n\t      speed != SPEED_100 &&\n\t      speed != SPEED_10) ||\n\t     (cmd->base.duplex != DUPLEX_HALF &&\n\t      cmd->base.duplex != DUPLEX_FULL)))\n\t\treturn -EINVAL;\n\n\t \n\tif (netif_device_present(gp->dev)) {\n\t\tdel_timer_sync(&gp->link_timer);\n\t\tgem_begin_auto_negotiation(gp, cmd);\n\t}\n\n\treturn 0;\n}\n\nstatic int gem_nway_reset(struct net_device *dev)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\tif (!gp->want_autoneg)\n\t\treturn -EINVAL;\n\n\t \n\tif (netif_device_present(gp->dev)) {\n\t\tdel_timer_sync(&gp->link_timer);\n\t\tgem_begin_auto_negotiation(gp, NULL);\n\t}\n\n\treturn 0;\n}\n\nstatic u32 gem_get_msglevel(struct net_device *dev)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\treturn gp->msg_enable;\n}\n\nstatic void gem_set_msglevel(struct net_device *dev, u32 value)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\tgp->msg_enable = value;\n}\n\n\n \n \n\n#define WOL_SUPPORTED_MASK\t(WAKE_MAGIC)\n\nstatic void gem_get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\t \n\tif (gp->has_wol) {\n\t\twol->supported = WOL_SUPPORTED_MASK;\n\t\twol->wolopts = gp->wake_on_lan;\n\t} else {\n\t\twol->supported = 0;\n\t\twol->wolopts = 0;\n\t}\n}\n\nstatic int gem_set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\n\tif (!gp->has_wol)\n\t\treturn -EOPNOTSUPP;\n\tgp->wake_on_lan = wol->wolopts & WOL_SUPPORTED_MASK;\n\treturn 0;\n}\n\nstatic const struct ethtool_ops gem_ethtool_ops = {\n\t.get_drvinfo\t\t= gem_get_drvinfo,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.nway_reset\t\t= gem_nway_reset,\n\t.get_msglevel\t\t= gem_get_msglevel,\n\t.set_msglevel\t\t= gem_set_msglevel,\n\t.get_wol\t\t= gem_get_wol,\n\t.set_wol\t\t= gem_set_wol,\n\t.get_link_ksettings\t= gem_get_link_ksettings,\n\t.set_link_ksettings\t= gem_set_link_ksettings,\n};\n\nstatic int gem_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tstruct gem *gp = netdev_priv(dev);\n\tstruct mii_ioctl_data *data = if_mii(ifr);\n\tint rc = -EOPNOTSUPP;\n\n\t \n\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\t\t \n\t\tdata->phy_id = gp->mii_phy_addr;\n\t\tfallthrough;\n\n\tcase SIOCGMIIREG:\t\t \n\t\tdata->val_out = __sungem_phy_read(gp, data->phy_id & 0x1f,\n\t\t\t\t\t   data->reg_num & 0x1f);\n\t\trc = 0;\n\t\tbreak;\n\n\tcase SIOCSMIIREG:\t\t \n\t\t__sungem_phy_write(gp, data->phy_id & 0x1f, data->reg_num & 0x1f,\n\t\t\t    data->val_in);\n\t\trc = 0;\n\t\tbreak;\n\t}\n\treturn rc;\n}\n\n#if (!defined(CONFIG_SPARC) && !defined(CONFIG_PPC_PMAC))\n \nstatic int find_eth_addr_in_vpd(void __iomem *rom_base, int len, unsigned char *dev_addr)\n{\n\tint this_offset;\n\n\tfor (this_offset = 0x20; this_offset < len; this_offset++) {\n\t\tvoid __iomem *p = rom_base + this_offset;\n\t\tint i;\n\n\t\tif (readb(p + 0) != 0x90 ||\n\t\t    readb(p + 1) != 0x00 ||\n\t\t    readb(p + 2) != 0x09 ||\n\t\t    readb(p + 3) != 0x4e ||\n\t\t    readb(p + 4) != 0x41 ||\n\t\t    readb(p + 5) != 0x06)\n\t\t\tcontinue;\n\n\t\tthis_offset += 6;\n\t\tp += 6;\n\n\t\tfor (i = 0; i < 6; i++)\n\t\t\tdev_addr[i] = readb(p + i);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void get_gem_mac_nonobp(struct pci_dev *pdev, unsigned char *dev_addr)\n{\n\tsize_t size;\n\tvoid __iomem *p = pci_map_rom(pdev, &size);\n\n\tif (p) {\n\t\tint found;\n\n\t\tfound = readb(p) == 0x55 &&\n\t\t\treadb(p + 1) == 0xaa &&\n\t\t\tfind_eth_addr_in_vpd(p, (64 * 1024), dev_addr);\n\t\tpci_unmap_rom(pdev, p);\n\t\tif (found)\n\t\t\treturn;\n\t}\n\n\t \n\tdev_addr[0] = 0x08;\n\tdev_addr[1] = 0x00;\n\tdev_addr[2] = 0x20;\n\tget_random_bytes(dev_addr + 3, 3);\n}\n#endif  \n\nstatic int gem_get_device_address(struct gem *gp)\n{\n#if defined(CONFIG_SPARC) || defined(CONFIG_PPC_PMAC)\n\tstruct net_device *dev = gp->dev;\n\tconst unsigned char *addr;\n\n\taddr = of_get_property(gp->of_node, \"local-mac-address\", NULL);\n\tif (addr == NULL) {\n#ifdef CONFIG_SPARC\n\t\taddr = idprom->id_ethaddr;\n#else\n\t\tprintk(\"\\n\");\n\t\tpr_err(\"%s: can't get mac-address\\n\", dev->name);\n\t\treturn -1;\n#endif\n\t}\n\teth_hw_addr_set(dev, addr);\n#else\n\tu8 addr[ETH_ALEN];\n\n\tget_gem_mac_nonobp(gp->pdev, addr);\n\teth_hw_addr_set(gp->dev, addr);\n#endif\n\treturn 0;\n}\n\nstatic void gem_remove_one(struct pci_dev *pdev)\n{\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\n\tif (dev) {\n\t\tstruct gem *gp = netdev_priv(dev);\n\n\t\tunregister_netdev(dev);\n\n\t\t \n\t\tcancel_work_sync(&gp->reset_task);\n\n\t\t \n\t\tdma_free_coherent(&pdev->dev, sizeof(struct gem_init_block),\n\t\t\t\t  gp->init_block, gp->gblock_dvma);\n\t\tiounmap(gp->regs);\n\t\tpci_release_regions(pdev);\n\t\tfree_netdev(dev);\n\t}\n}\n\nstatic const struct net_device_ops gem_netdev_ops = {\n\t.ndo_open\t\t= gem_open,\n\t.ndo_stop\t\t= gem_close,\n\t.ndo_start_xmit\t\t= gem_start_xmit,\n\t.ndo_get_stats\t\t= gem_get_stats,\n\t.ndo_set_rx_mode\t= gem_set_multicast,\n\t.ndo_eth_ioctl\t\t= gem_ioctl,\n\t.ndo_tx_timeout\t\t= gem_tx_timeout,\n\t.ndo_change_mtu\t\t= gem_change_mtu,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_mac_address    = gem_set_mac_address,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller    = gem_poll_controller,\n#endif\n};\n\nstatic int gem_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tunsigned long gemreg_base, gemreg_len;\n\tstruct net_device *dev;\n\tstruct gem *gp;\n\tint err, pci_using_dac;\n\n\tprintk_once(KERN_INFO \"%s\", version);\n\n\t \n\terr = pci_enable_device(pdev);\n\tif (err) {\n\t\tpr_err(\"Cannot enable MMIO operation, aborting\\n\");\n\t\treturn err;\n\t}\n\tpci_set_master(pdev);\n\n\t \n\n\t \n\tif (pdev->vendor == PCI_VENDOR_ID_SUN &&\n\t    pdev->device == PCI_DEVICE_ID_SUN_GEM &&\n\t    !dma_set_mask(&pdev->dev, DMA_BIT_MASK(64))) {\n\t\tpci_using_dac = 1;\n\t} else {\n\t\terr = dma_set_mask(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (err) {\n\t\t\tpr_err(\"No usable DMA configuration, aborting\\n\");\n\t\t\tgoto err_disable_device;\n\t\t}\n\t\tpci_using_dac = 0;\n\t}\n\n\tgemreg_base = pci_resource_start(pdev, 0);\n\tgemreg_len = pci_resource_len(pdev, 0);\n\n\tif ((pci_resource_flags(pdev, 0) & IORESOURCE_IO) != 0) {\n\t\tpr_err(\"Cannot find proper PCI device base address, aborting\\n\");\n\t\terr = -ENODEV;\n\t\tgoto err_disable_device;\n\t}\n\n\tdev = alloc_etherdev(sizeof(*gp));\n\tif (!dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_disable_device;\n\t}\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\n\tgp = netdev_priv(dev);\n\n\terr = pci_request_regions(pdev, DRV_NAME);\n\tif (err) {\n\t\tpr_err(\"Cannot obtain PCI resources, aborting\\n\");\n\t\tgoto err_out_free_netdev;\n\t}\n\n\tgp->pdev = pdev;\n\tgp->dev = dev;\n\n\tgp->msg_enable = DEFAULT_MSG;\n\n\ttimer_setup(&gp->link_timer, gem_link_timer, 0);\n\n\tINIT_WORK(&gp->reset_task, gem_reset_task);\n\n\tgp->lstate = link_down;\n\tgp->timer_ticks = 0;\n\tnetif_carrier_off(dev);\n\n\tgp->regs = ioremap(gemreg_base, gemreg_len);\n\tif (!gp->regs) {\n\t\tpr_err(\"Cannot map device registers, aborting\\n\");\n\t\terr = -EIO;\n\t\tgoto err_out_free_res;\n\t}\n\n\t \n#if defined(CONFIG_PPC_PMAC) || defined(CONFIG_SPARC)\n\tgp->of_node = pci_device_to_OF_node(pdev);\n#endif\n\n\t \n\tif (pdev->vendor == PCI_VENDOR_ID_APPLE)\n\t\tgp->has_wol = 1;\n\n\t \n\tgem_get_cell(gp);\n\n\t \n\tgem_reset(gp);\n\n\t \n\tgp->phy_mii.dev = dev;\n\tgp->phy_mii.mdio_read = _sungem_phy_read;\n\tgp->phy_mii.mdio_write = _sungem_phy_write;\n#ifdef CONFIG_PPC_PMAC\n\tgp->phy_mii.platform_data = gp->of_node;\n#endif\n\t \n\tgp->want_autoneg = 1;\n\n\t \n\tif (gem_check_invariants(gp)) {\n\t\terr = -ENODEV;\n\t\tgoto err_out_iounmap;\n\t}\n\n\t \n\tgp->init_block = dma_alloc_coherent(&pdev->dev, sizeof(struct gem_init_block),\n\t\t\t\t\t    &gp->gblock_dvma, GFP_KERNEL);\n\tif (!gp->init_block) {\n\t\tpr_err(\"Cannot allocate init block, aborting\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_out_iounmap;\n\t}\n\n\terr = gem_get_device_address(gp);\n\tif (err)\n\t\tgoto err_out_free_consistent;\n\n\tdev->netdev_ops = &gem_netdev_ops;\n\tnetif_napi_add(dev, &gp->napi, gem_poll);\n\tdev->ethtool_ops = &gem_ethtool_ops;\n\tdev->watchdog_timeo = 5 * HZ;\n\tdev->dma = 0;\n\n\t \n\tpci_set_drvdata(pdev, dev);\n\n\t \n\tdev->hw_features = NETIF_F_SG | NETIF_F_HW_CSUM | NETIF_F_RXCSUM;\n\tdev->features = dev->hw_features;\n\tif (pci_using_dac)\n\t\tdev->features |= NETIF_F_HIGHDMA;\n\n\t \n\tdev->min_mtu = GEM_MIN_MTU;\n\tdev->max_mtu = GEM_MAX_MTU;\n\n\t \n\tif (register_netdev(dev)) {\n\t\tpr_err(\"Cannot register net device, aborting\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_out_free_consistent;\n\t}\n\n\t \n\trtnl_lock();\n\tgem_put_cell(gp);\n\trtnl_unlock();\n\n\tnetdev_info(dev, \"Sun GEM (PCI) 10/100/1000BaseT Ethernet %pM\\n\",\n\t\t    dev->dev_addr);\n\treturn 0;\n\nerr_out_free_consistent:\n\tgem_remove_one(pdev);\nerr_out_iounmap:\n\tgem_put_cell(gp);\n\tiounmap(gp->regs);\n\nerr_out_free_res:\n\tpci_release_regions(pdev);\n\nerr_out_free_netdev:\n\tfree_netdev(dev);\nerr_disable_device:\n\tpci_disable_device(pdev);\n\treturn err;\n\n}\n\nstatic SIMPLE_DEV_PM_OPS(gem_pm_ops, gem_suspend, gem_resume);\n\nstatic struct pci_driver gem_driver = {\n\t.name\t\t= GEM_MODULE_NAME,\n\t.id_table\t= gem_pci_tbl,\n\t.probe\t\t= gem_init_one,\n\t.remove\t\t= gem_remove_one,\n\t.driver.pm\t= &gem_pm_ops,\n};\n\nmodule_pci_driver(gem_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}