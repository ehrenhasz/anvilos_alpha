{
  "module_name": "lan966x_fdma.c",
  "hash_id": "8ef33049976e8e0f07119bafc2460a6aab26d604acf7d6c5feefc67a014318c0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/microchip/lan966x/lan966x_fdma.c",
  "human_readable_source": "\n\n#include <linux/bpf.h>\n#include <linux/filter.h>\n#include <net/page_pool/helpers.h>\n\n#include \"lan966x_main.h\"\n\nstatic int lan966x_fdma_channel_active(struct lan966x *lan966x)\n{\n\treturn lan_rd(lan966x, FDMA_CH_ACTIVE);\n}\n\nstatic struct page *lan966x_fdma_rx_alloc_page(struct lan966x_rx *rx,\n\t\t\t\t\t       struct lan966x_db *db)\n{\n\tstruct page *page;\n\n\tpage = page_pool_dev_alloc_pages(rx->page_pool);\n\tif (unlikely(!page))\n\t\treturn NULL;\n\n\tdb->dataptr = page_pool_get_dma_addr(page) + XDP_PACKET_HEADROOM;\n\n\treturn page;\n}\n\nstatic void lan966x_fdma_rx_free_pages(struct lan966x_rx *rx)\n{\n\tint i, j;\n\n\tfor (i = 0; i < FDMA_DCB_MAX; ++i) {\n\t\tfor (j = 0; j < FDMA_RX_DCB_MAX_DBS; ++j)\n\t\t\tpage_pool_put_full_page(rx->page_pool,\n\t\t\t\t\t\trx->page[i][j], false);\n\t}\n}\n\nstatic void lan966x_fdma_rx_free_page(struct lan966x_rx *rx)\n{\n\tstruct page *page;\n\n\tpage = rx->page[rx->dcb_index][rx->db_index];\n\tif (unlikely(!page))\n\t\treturn;\n\n\tpage_pool_recycle_direct(rx->page_pool, page);\n}\n\nstatic void lan966x_fdma_rx_add_dcb(struct lan966x_rx *rx,\n\t\t\t\t    struct lan966x_rx_dcb *dcb,\n\t\t\t\t    u64 nextptr)\n{\n\tstruct lan966x_db *db;\n\tint i;\n\n\tfor (i = 0; i < FDMA_RX_DCB_MAX_DBS; ++i) {\n\t\tdb = &dcb->db[i];\n\t\tdb->status = FDMA_DCB_STATUS_INTR;\n\t}\n\n\tdcb->nextptr = FDMA_DCB_INVALID_DATA;\n\tdcb->info = FDMA_DCB_INFO_DATAL(PAGE_SIZE << rx->page_order);\n\n\trx->last_entry->nextptr = nextptr;\n\trx->last_entry = dcb;\n}\n\nstatic int lan966x_fdma_rx_alloc_page_pool(struct lan966x_rx *rx)\n{\n\tstruct lan966x *lan966x = rx->lan966x;\n\tstruct page_pool_params pp_params = {\n\t\t.order = rx->page_order,\n\t\t.flags = PP_FLAG_DMA_MAP | PP_FLAG_DMA_SYNC_DEV,\n\t\t.pool_size = FDMA_DCB_MAX,\n\t\t.nid = NUMA_NO_NODE,\n\t\t.dev = lan966x->dev,\n\t\t.dma_dir = DMA_FROM_DEVICE,\n\t\t.offset = XDP_PACKET_HEADROOM,\n\t\t.max_len = rx->max_mtu -\n\t\t\t   SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t};\n\n\tif (lan966x_xdp_present(lan966x))\n\t\tpp_params.dma_dir = DMA_BIDIRECTIONAL;\n\n\trx->page_pool = page_pool_create(&pp_params);\n\n\tfor (int i = 0; i < lan966x->num_phys_ports; ++i) {\n\t\tstruct lan966x_port *port;\n\n\t\tif (!lan966x->ports[i])\n\t\t\tcontinue;\n\n\t\tport = lan966x->ports[i];\n\t\txdp_rxq_info_unreg_mem_model(&port->xdp_rxq);\n\t\txdp_rxq_info_reg_mem_model(&port->xdp_rxq, MEM_TYPE_PAGE_POOL,\n\t\t\t\t\t   rx->page_pool);\n\t}\n\n\treturn PTR_ERR_OR_ZERO(rx->page_pool);\n}\n\nstatic int lan966x_fdma_rx_alloc(struct lan966x_rx *rx)\n{\n\tstruct lan966x *lan966x = rx->lan966x;\n\tstruct lan966x_rx_dcb *dcb;\n\tstruct lan966x_db *db;\n\tstruct page *page;\n\tint i, j;\n\tint size;\n\n\tif (lan966x_fdma_rx_alloc_page_pool(rx))\n\t\treturn PTR_ERR(rx->page_pool);\n\n\t \n\tsize = sizeof(struct lan966x_rx_dcb) * FDMA_DCB_MAX;\n\tsize = ALIGN(size, PAGE_SIZE);\n\n\trx->dcbs = dma_alloc_coherent(lan966x->dev, size, &rx->dma, GFP_KERNEL);\n\tif (!rx->dcbs)\n\t\treturn -ENOMEM;\n\n\trx->last_entry = rx->dcbs;\n\trx->db_index = 0;\n\trx->dcb_index = 0;\n\n\t \n\tfor (i = 0; i < FDMA_DCB_MAX; ++i) {\n\t\tdcb = &rx->dcbs[i];\n\t\tdcb->info = 0;\n\n\t\t \n\t\tfor (j = 0; j < FDMA_RX_DCB_MAX_DBS; ++j) {\n\t\t\tdb = &dcb->db[j];\n\t\t\tpage = lan966x_fdma_rx_alloc_page(rx, db);\n\t\t\tif (!page)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdb->status = 0;\n\t\t\trx->page[i][j] = page;\n\t\t}\n\n\t\tlan966x_fdma_rx_add_dcb(rx, dcb, rx->dma + sizeof(*dcb) * i);\n\t}\n\n\treturn 0;\n}\n\nstatic void lan966x_fdma_rx_advance_dcb(struct lan966x_rx *rx)\n{\n\trx->dcb_index++;\n\trx->dcb_index &= FDMA_DCB_MAX - 1;\n}\n\nstatic void lan966x_fdma_rx_free(struct lan966x_rx *rx)\n{\n\tstruct lan966x *lan966x = rx->lan966x;\n\tu32 size;\n\n\t \n\tsize = sizeof(struct lan966x_tx_dcb) * FDMA_DCB_MAX;\n\tsize = ALIGN(size, PAGE_SIZE);\n\tdma_free_coherent(lan966x->dev, size, rx->dcbs, rx->dma);\n}\n\nstatic void lan966x_fdma_rx_start(struct lan966x_rx *rx)\n{\n\tstruct lan966x *lan966x = rx->lan966x;\n\tu32 mask;\n\n\t \n\tlan_wr(lower_32_bits((u64)rx->dma), lan966x,\n\t       FDMA_DCB_LLP(rx->channel_id));\n\tlan_wr(upper_32_bits((u64)rx->dma), lan966x,\n\t       FDMA_DCB_LLP1(rx->channel_id));\n\n\tlan_wr(FDMA_CH_CFG_CH_DCB_DB_CNT_SET(FDMA_RX_DCB_MAX_DBS) |\n\t       FDMA_CH_CFG_CH_INTR_DB_EOF_ONLY_SET(1) |\n\t       FDMA_CH_CFG_CH_INJ_PORT_SET(0) |\n\t       FDMA_CH_CFG_CH_MEM_SET(1),\n\t       lan966x, FDMA_CH_CFG(rx->channel_id));\n\n\t \n\tlan_rmw(FDMA_PORT_CTRL_XTR_STOP_SET(0),\n\t\tFDMA_PORT_CTRL_XTR_STOP,\n\t\tlan966x, FDMA_PORT_CTRL(0));\n\n\t \n\tmask = lan_rd(lan966x, FDMA_INTR_DB_ENA);\n\tmask = FDMA_INTR_DB_ENA_INTR_DB_ENA_GET(mask);\n\tmask |= BIT(rx->channel_id);\n\tlan_rmw(FDMA_INTR_DB_ENA_INTR_DB_ENA_SET(mask),\n\t\tFDMA_INTR_DB_ENA_INTR_DB_ENA,\n\t\tlan966x, FDMA_INTR_DB_ENA);\n\n\t \n\tlan_rmw(FDMA_CH_ACTIVATE_CH_ACTIVATE_SET(BIT(rx->channel_id)),\n\t\tFDMA_CH_ACTIVATE_CH_ACTIVATE,\n\t\tlan966x, FDMA_CH_ACTIVATE);\n}\n\nstatic void lan966x_fdma_rx_disable(struct lan966x_rx *rx)\n{\n\tstruct lan966x *lan966x = rx->lan966x;\n\tu32 val;\n\n\t \n\tlan_rmw(FDMA_CH_DISABLE_CH_DISABLE_SET(BIT(rx->channel_id)),\n\t\tFDMA_CH_DISABLE_CH_DISABLE,\n\t\tlan966x, FDMA_CH_DISABLE);\n\n\treadx_poll_timeout_atomic(lan966x_fdma_channel_active, lan966x,\n\t\t\t\t  val, !(val & BIT(rx->channel_id)),\n\t\t\t\t  READL_SLEEP_US, READL_TIMEOUT_US);\n\n\tlan_rmw(FDMA_CH_DB_DISCARD_DB_DISCARD_SET(BIT(rx->channel_id)),\n\t\tFDMA_CH_DB_DISCARD_DB_DISCARD,\n\t\tlan966x, FDMA_CH_DB_DISCARD);\n}\n\nstatic void lan966x_fdma_rx_reload(struct lan966x_rx *rx)\n{\n\tstruct lan966x *lan966x = rx->lan966x;\n\n\tlan_rmw(FDMA_CH_RELOAD_CH_RELOAD_SET(BIT(rx->channel_id)),\n\t\tFDMA_CH_RELOAD_CH_RELOAD,\n\t\tlan966x, FDMA_CH_RELOAD);\n}\n\nstatic void lan966x_fdma_tx_add_dcb(struct lan966x_tx *tx,\n\t\t\t\t    struct lan966x_tx_dcb *dcb)\n{\n\tdcb->nextptr = FDMA_DCB_INVALID_DATA;\n\tdcb->info = 0;\n}\n\nstatic int lan966x_fdma_tx_alloc(struct lan966x_tx *tx)\n{\n\tstruct lan966x *lan966x = tx->lan966x;\n\tstruct lan966x_tx_dcb *dcb;\n\tstruct lan966x_db *db;\n\tint size;\n\tint i, j;\n\n\ttx->dcbs_buf = kcalloc(FDMA_DCB_MAX, sizeof(struct lan966x_tx_dcb_buf),\n\t\t\t       GFP_KERNEL);\n\tif (!tx->dcbs_buf)\n\t\treturn -ENOMEM;\n\n\t \n\tsize = sizeof(struct lan966x_tx_dcb) * FDMA_DCB_MAX;\n\tsize = ALIGN(size, PAGE_SIZE);\n\ttx->dcbs = dma_alloc_coherent(lan966x->dev, size, &tx->dma, GFP_KERNEL);\n\tif (!tx->dcbs)\n\t\tgoto out;\n\n\t \n\tfor (i = 0; i < FDMA_DCB_MAX; ++i) {\n\t\tdcb = &tx->dcbs[i];\n\n\t\tfor (j = 0; j < FDMA_TX_DCB_MAX_DBS; ++j) {\n\t\t\tdb = &dcb->db[j];\n\t\t\tdb->dataptr = 0;\n\t\t\tdb->status = 0;\n\t\t}\n\n\t\tlan966x_fdma_tx_add_dcb(tx, dcb);\n\t}\n\n\treturn 0;\n\nout:\n\tkfree(tx->dcbs_buf);\n\treturn -ENOMEM;\n}\n\nstatic void lan966x_fdma_tx_free(struct lan966x_tx *tx)\n{\n\tstruct lan966x *lan966x = tx->lan966x;\n\tint size;\n\n\tkfree(tx->dcbs_buf);\n\n\tsize = sizeof(struct lan966x_tx_dcb) * FDMA_DCB_MAX;\n\tsize = ALIGN(size, PAGE_SIZE);\n\tdma_free_coherent(lan966x->dev, size, tx->dcbs, tx->dma);\n}\n\nstatic void lan966x_fdma_tx_activate(struct lan966x_tx *tx)\n{\n\tstruct lan966x *lan966x = tx->lan966x;\n\tu32 mask;\n\n\t \n\tlan_wr(lower_32_bits((u64)tx->dma), lan966x,\n\t       FDMA_DCB_LLP(tx->channel_id));\n\tlan_wr(upper_32_bits((u64)tx->dma), lan966x,\n\t       FDMA_DCB_LLP1(tx->channel_id));\n\n\tlan_wr(FDMA_CH_CFG_CH_DCB_DB_CNT_SET(FDMA_TX_DCB_MAX_DBS) |\n\t       FDMA_CH_CFG_CH_INTR_DB_EOF_ONLY_SET(1) |\n\t       FDMA_CH_CFG_CH_INJ_PORT_SET(0) |\n\t       FDMA_CH_CFG_CH_MEM_SET(1),\n\t       lan966x, FDMA_CH_CFG(tx->channel_id));\n\n\t \n\tlan_rmw(FDMA_PORT_CTRL_INJ_STOP_SET(0),\n\t\tFDMA_PORT_CTRL_INJ_STOP,\n\t\tlan966x, FDMA_PORT_CTRL(0));\n\n\t \n\tmask = lan_rd(lan966x, FDMA_INTR_DB_ENA);\n\tmask = FDMA_INTR_DB_ENA_INTR_DB_ENA_GET(mask);\n\tmask |= BIT(tx->channel_id);\n\tlan_rmw(FDMA_INTR_DB_ENA_INTR_DB_ENA_SET(mask),\n\t\tFDMA_INTR_DB_ENA_INTR_DB_ENA,\n\t\tlan966x, FDMA_INTR_DB_ENA);\n\n\t \n\tlan_rmw(FDMA_CH_ACTIVATE_CH_ACTIVATE_SET(BIT(tx->channel_id)),\n\t\tFDMA_CH_ACTIVATE_CH_ACTIVATE,\n\t\tlan966x, FDMA_CH_ACTIVATE);\n}\n\nstatic void lan966x_fdma_tx_disable(struct lan966x_tx *tx)\n{\n\tstruct lan966x *lan966x = tx->lan966x;\n\tu32 val;\n\n\t \n\tlan_rmw(FDMA_CH_DISABLE_CH_DISABLE_SET(BIT(tx->channel_id)),\n\t\tFDMA_CH_DISABLE_CH_DISABLE,\n\t\tlan966x, FDMA_CH_DISABLE);\n\n\treadx_poll_timeout_atomic(lan966x_fdma_channel_active, lan966x,\n\t\t\t\t  val, !(val & BIT(tx->channel_id)),\n\t\t\t\t  READL_SLEEP_US, READL_TIMEOUT_US);\n\n\tlan_rmw(FDMA_CH_DB_DISCARD_DB_DISCARD_SET(BIT(tx->channel_id)),\n\t\tFDMA_CH_DB_DISCARD_DB_DISCARD,\n\t\tlan966x, FDMA_CH_DB_DISCARD);\n\n\ttx->activated = false;\n\ttx->last_in_use = -1;\n}\n\nstatic void lan966x_fdma_tx_reload(struct lan966x_tx *tx)\n{\n\tstruct lan966x *lan966x = tx->lan966x;\n\n\t \n\tlan_rmw(FDMA_CH_RELOAD_CH_RELOAD_SET(BIT(tx->channel_id)),\n\t\tFDMA_CH_RELOAD_CH_RELOAD,\n\t\tlan966x, FDMA_CH_RELOAD);\n}\n\nstatic void lan966x_fdma_wakeup_netdev(struct lan966x *lan966x)\n{\n\tstruct lan966x_port *port;\n\tint i;\n\n\tfor (i = 0; i < lan966x->num_phys_ports; ++i) {\n\t\tport = lan966x->ports[i];\n\t\tif (!port)\n\t\t\tcontinue;\n\n\t\tif (netif_queue_stopped(port->dev))\n\t\t\tnetif_wake_queue(port->dev);\n\t}\n}\n\nstatic void lan966x_fdma_stop_netdev(struct lan966x *lan966x)\n{\n\tstruct lan966x_port *port;\n\tint i;\n\n\tfor (i = 0; i < lan966x->num_phys_ports; ++i) {\n\t\tport = lan966x->ports[i];\n\t\tif (!port)\n\t\t\tcontinue;\n\n\t\tnetif_stop_queue(port->dev);\n\t}\n}\n\nstatic void lan966x_fdma_tx_clear_buf(struct lan966x *lan966x, int weight)\n{\n\tstruct lan966x_tx *tx = &lan966x->tx;\n\tstruct lan966x_rx *rx = &lan966x->rx;\n\tstruct lan966x_tx_dcb_buf *dcb_buf;\n\tstruct xdp_frame_bulk bq;\n\tstruct lan966x_db *db;\n\tunsigned long flags;\n\tbool clear = false;\n\tint i;\n\n\txdp_frame_bulk_init(&bq);\n\n\tspin_lock_irqsave(&lan966x->tx_lock, flags);\n\tfor (i = 0; i < FDMA_DCB_MAX; ++i) {\n\t\tdcb_buf = &tx->dcbs_buf[i];\n\n\t\tif (!dcb_buf->used)\n\t\t\tcontinue;\n\n\t\tdb = &tx->dcbs[i].db[0];\n\t\tif (!(db->status & FDMA_DCB_STATUS_DONE))\n\t\t\tcontinue;\n\n\t\tdcb_buf->dev->stats.tx_packets++;\n\t\tdcb_buf->dev->stats.tx_bytes += dcb_buf->len;\n\n\t\tdcb_buf->used = false;\n\t\tif (dcb_buf->use_skb) {\n\t\t\tdma_unmap_single(lan966x->dev,\n\t\t\t\t\t dcb_buf->dma_addr,\n\t\t\t\t\t dcb_buf->len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\n\t\t\tif (!dcb_buf->ptp)\n\t\t\t\tnapi_consume_skb(dcb_buf->data.skb, weight);\n\t\t} else {\n\t\t\tif (dcb_buf->xdp_ndo)\n\t\t\t\tdma_unmap_single(lan966x->dev,\n\t\t\t\t\t\t dcb_buf->dma_addr,\n\t\t\t\t\t\t dcb_buf->len,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\n\t\t\tif (dcb_buf->xdp_ndo)\n\t\t\t\txdp_return_frame_bulk(dcb_buf->data.xdpf, &bq);\n\t\t\telse\n\t\t\t\tpage_pool_recycle_direct(rx->page_pool,\n\t\t\t\t\t\t\t dcb_buf->data.page);\n\t\t}\n\n\t\tclear = true;\n\t}\n\n\txdp_flush_frame_bulk(&bq);\n\n\tif (clear)\n\t\tlan966x_fdma_wakeup_netdev(lan966x);\n\n\tspin_unlock_irqrestore(&lan966x->tx_lock, flags);\n}\n\nstatic bool lan966x_fdma_rx_more_frames(struct lan966x_rx *rx)\n{\n\tstruct lan966x_db *db;\n\n\t \n\tdb = &rx->dcbs[rx->dcb_index].db[rx->db_index];\n\tif (unlikely(!(db->status & FDMA_DCB_STATUS_DONE)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int lan966x_fdma_rx_check_frame(struct lan966x_rx *rx, u64 *src_port)\n{\n\tstruct lan966x *lan966x = rx->lan966x;\n\tstruct lan966x_port *port;\n\tstruct lan966x_db *db;\n\tstruct page *page;\n\n\tdb = &rx->dcbs[rx->dcb_index].db[rx->db_index];\n\tpage = rx->page[rx->dcb_index][rx->db_index];\n\tif (unlikely(!page))\n\t\treturn FDMA_ERROR;\n\n\tdma_sync_single_for_cpu(lan966x->dev,\n\t\t\t\t(dma_addr_t)db->dataptr + XDP_PACKET_HEADROOM,\n\t\t\t\tFDMA_DCB_STATUS_BLOCKL(db->status),\n\t\t\t\tDMA_FROM_DEVICE);\n\n\tlan966x_ifh_get_src_port(page_address(page) + XDP_PACKET_HEADROOM,\n\t\t\t\t src_port);\n\tif (WARN_ON(*src_port >= lan966x->num_phys_ports))\n\t\treturn FDMA_ERROR;\n\n\tport = lan966x->ports[*src_port];\n\tif (!lan966x_xdp_port_present(port))\n\t\treturn FDMA_PASS;\n\n\treturn lan966x_xdp_run(port, page, FDMA_DCB_STATUS_BLOCKL(db->status));\n}\n\nstatic struct sk_buff *lan966x_fdma_rx_get_frame(struct lan966x_rx *rx,\n\t\t\t\t\t\t u64 src_port)\n{\n\tstruct lan966x *lan966x = rx->lan966x;\n\tstruct lan966x_db *db;\n\tstruct sk_buff *skb;\n\tstruct page *page;\n\tu64 timestamp;\n\n\t \n\tdb = &rx->dcbs[rx->dcb_index].db[rx->db_index];\n\tpage = rx->page[rx->dcb_index][rx->db_index];\n\n\tskb = build_skb(page_address(page), PAGE_SIZE << rx->page_order);\n\tif (unlikely(!skb))\n\t\tgoto free_page;\n\n\tskb_mark_for_recycle(skb);\n\n\tskb_reserve(skb, XDP_PACKET_HEADROOM);\n\tskb_put(skb, FDMA_DCB_STATUS_BLOCKL(db->status));\n\n\tlan966x_ifh_get_timestamp(skb->data, &timestamp);\n\n\tskb->dev = lan966x->ports[src_port]->dev;\n\tskb_pull(skb, IFH_LEN_BYTES);\n\n\tif (likely(!(skb->dev->features & NETIF_F_RXFCS)))\n\t\tskb_trim(skb, skb->len - ETH_FCS_LEN);\n\n\tlan966x_ptp_rxtstamp(lan966x, skb, src_port, timestamp);\n\tskb->protocol = eth_type_trans(skb, skb->dev);\n\n\tif (lan966x->bridge_mask & BIT(src_port)) {\n\t\tskb->offload_fwd_mark = 1;\n\n\t\tskb_reset_network_header(skb);\n\t\tif (!lan966x_hw_offload(lan966x, src_port, skb))\n\t\t\tskb->offload_fwd_mark = 0;\n\t}\n\n\tskb->dev->stats.rx_bytes += skb->len;\n\tskb->dev->stats.rx_packets++;\n\n\treturn skb;\n\nfree_page:\n\tpage_pool_recycle_direct(rx->page_pool, page);\n\n\treturn NULL;\n}\n\nstatic int lan966x_fdma_napi_poll(struct napi_struct *napi, int weight)\n{\n\tstruct lan966x *lan966x = container_of(napi, struct lan966x, napi);\n\tstruct lan966x_rx *rx = &lan966x->rx;\n\tint dcb_reload = rx->dcb_index;\n\tstruct lan966x_rx_dcb *old_dcb;\n\tstruct lan966x_db *db;\n\tbool redirect = false;\n\tstruct sk_buff *skb;\n\tstruct page *page;\n\tint counter = 0;\n\tu64 src_port;\n\tu64 nextptr;\n\n\tlan966x_fdma_tx_clear_buf(lan966x, weight);\n\n\t \n\twhile (counter < weight) {\n\t\tif (!lan966x_fdma_rx_more_frames(rx))\n\t\t\tbreak;\n\n\t\tcounter++;\n\n\t\tswitch (lan966x_fdma_rx_check_frame(rx, &src_port)) {\n\t\tcase FDMA_PASS:\n\t\t\tbreak;\n\t\tcase FDMA_ERROR:\n\t\t\tlan966x_fdma_rx_free_page(rx);\n\t\t\tlan966x_fdma_rx_advance_dcb(rx);\n\t\t\tgoto allocate_new;\n\t\tcase FDMA_REDIRECT:\n\t\t\tredirect = true;\n\t\t\tfallthrough;\n\t\tcase FDMA_TX:\n\t\t\tlan966x_fdma_rx_advance_dcb(rx);\n\t\t\tcontinue;\n\t\tcase FDMA_DROP:\n\t\t\tlan966x_fdma_rx_free_page(rx);\n\t\t\tlan966x_fdma_rx_advance_dcb(rx);\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb = lan966x_fdma_rx_get_frame(rx, src_port);\n\t\tlan966x_fdma_rx_advance_dcb(rx);\n\t\tif (!skb)\n\t\t\tgoto allocate_new;\n\n\t\tnapi_gro_receive(&lan966x->napi, skb);\n\t}\n\nallocate_new:\n\t \n\twhile (dcb_reload != rx->dcb_index) {\n\t\tdb = &rx->dcbs[dcb_reload].db[rx->db_index];\n\t\tpage = lan966x_fdma_rx_alloc_page(rx, db);\n\t\tif (unlikely(!page))\n\t\t\tbreak;\n\t\trx->page[dcb_reload][rx->db_index] = page;\n\n\t\told_dcb = &rx->dcbs[dcb_reload];\n\t\tdcb_reload++;\n\t\tdcb_reload &= FDMA_DCB_MAX - 1;\n\n\t\tnextptr = rx->dma + ((unsigned long)old_dcb -\n\t\t\t\t     (unsigned long)rx->dcbs);\n\t\tlan966x_fdma_rx_add_dcb(rx, old_dcb, nextptr);\n\t\tlan966x_fdma_rx_reload(rx);\n\t}\n\n\tif (redirect)\n\t\txdp_do_flush();\n\n\tif (counter < weight && napi_complete_done(napi, counter))\n\t\tlan_wr(0xff, lan966x, FDMA_INTR_DB_ENA);\n\n\treturn counter;\n}\n\nirqreturn_t lan966x_fdma_irq_handler(int irq, void *args)\n{\n\tstruct lan966x *lan966x = args;\n\tu32 db, err, err_type;\n\n\tdb = lan_rd(lan966x, FDMA_INTR_DB);\n\terr = lan_rd(lan966x, FDMA_INTR_ERR);\n\n\tif (db) {\n\t\tlan_wr(0, lan966x, FDMA_INTR_DB_ENA);\n\t\tlan_wr(db, lan966x, FDMA_INTR_DB);\n\n\t\tnapi_schedule(&lan966x->napi);\n\t}\n\n\tif (err) {\n\t\terr_type = lan_rd(lan966x, FDMA_ERRORS);\n\n\t\tWARN(1, \"Unexpected error: %d, error_type: %d\\n\", err, err_type);\n\n\t\tlan_wr(err, lan966x, FDMA_INTR_ERR);\n\t\tlan_wr(err_type, lan966x, FDMA_ERRORS);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int lan966x_fdma_get_next_dcb(struct lan966x_tx *tx)\n{\n\tstruct lan966x_tx_dcb_buf *dcb_buf;\n\tint i;\n\n\tfor (i = 0; i < FDMA_DCB_MAX; ++i) {\n\t\tdcb_buf = &tx->dcbs_buf[i];\n\t\tif (!dcb_buf->used && i != tx->last_in_use)\n\t\t\treturn i;\n\t}\n\n\treturn -1;\n}\n\nstatic void lan966x_fdma_tx_setup_dcb(struct lan966x_tx *tx,\n\t\t\t\t      int next_to_use, int len,\n\t\t\t\t      dma_addr_t dma_addr)\n{\n\tstruct lan966x_tx_dcb *next_dcb;\n\tstruct lan966x_db *next_db;\n\n\tnext_dcb = &tx->dcbs[next_to_use];\n\tnext_dcb->nextptr = FDMA_DCB_INVALID_DATA;\n\n\tnext_db = &next_dcb->db[0];\n\tnext_db->dataptr = dma_addr;\n\tnext_db->status = FDMA_DCB_STATUS_SOF |\n\t\t\t  FDMA_DCB_STATUS_EOF |\n\t\t\t  FDMA_DCB_STATUS_INTR |\n\t\t\t  FDMA_DCB_STATUS_BLOCKO(0) |\n\t\t\t  FDMA_DCB_STATUS_BLOCKL(len);\n}\n\nstatic void lan966x_fdma_tx_start(struct lan966x_tx *tx, int next_to_use)\n{\n\tstruct lan966x *lan966x = tx->lan966x;\n\tstruct lan966x_tx_dcb *dcb;\n\n\tif (likely(lan966x->tx.activated)) {\n\t\t \n\t\tdcb = &tx->dcbs[tx->last_in_use];\n\t\tdcb->nextptr = tx->dma + (next_to_use *\n\t\t\t\t\t  sizeof(struct lan966x_tx_dcb));\n\n\t\tlan966x_fdma_tx_reload(tx);\n\t} else {\n\t\t \n\t\tlan966x->tx.activated = true;\n\t\tlan966x_fdma_tx_activate(tx);\n\t}\n\n\t \n\ttx->last_in_use = next_to_use;\n}\n\nint lan966x_fdma_xmit_xdpf(struct lan966x_port *port, void *ptr, u32 len)\n{\n\tstruct lan966x *lan966x = port->lan966x;\n\tstruct lan966x_tx_dcb_buf *next_dcb_buf;\n\tstruct lan966x_tx *tx = &lan966x->tx;\n\tstruct xdp_frame *xdpf;\n\tdma_addr_t dma_addr;\n\tstruct page *page;\n\tint next_to_use;\n\t__be32 *ifh;\n\tint ret = 0;\n\n\tspin_lock(&lan966x->tx_lock);\n\n\t \n\tnext_to_use = lan966x_fdma_get_next_dcb(tx);\n\tif (next_to_use < 0) {\n\t\tnetif_stop_queue(port->dev);\n\t\tret = NETDEV_TX_BUSY;\n\t\tgoto out;\n\t}\n\n\t \n\tnext_dcb_buf = &tx->dcbs_buf[next_to_use];\n\n\t \n\tif (!len) {\n\t\txdpf = ptr;\n\n\t\tif (xdpf->headroom < IFH_LEN_BYTES) {\n\t\t\tret = NETDEV_TX_OK;\n\t\t\tgoto out;\n\t\t}\n\n\t\tifh = xdpf->data - IFH_LEN_BYTES;\n\t\tmemset(ifh, 0x0, sizeof(__be32) * IFH_LEN);\n\t\tlan966x_ifh_set_bypass(ifh, 1);\n\t\tlan966x_ifh_set_port(ifh, BIT_ULL(port->chip_port));\n\n\t\tdma_addr = dma_map_single(lan966x->dev,\n\t\t\t\t\t  xdpf->data - IFH_LEN_BYTES,\n\t\t\t\t\t  xdpf->len + IFH_LEN_BYTES,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(lan966x->dev, dma_addr)) {\n\t\t\tret = NETDEV_TX_OK;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnext_dcb_buf->data.xdpf = xdpf;\n\t\tnext_dcb_buf->len = xdpf->len + IFH_LEN_BYTES;\n\n\t\t \n\t\tlan966x_fdma_tx_setup_dcb(tx, next_to_use,\n\t\t\t\t\t  xdpf->len + IFH_LEN_BYTES,\n\t\t\t\t\t  dma_addr);\n\t} else {\n\t\tpage = ptr;\n\n\t\tifh = page_address(page) + XDP_PACKET_HEADROOM;\n\t\tmemset(ifh, 0x0, sizeof(__be32) * IFH_LEN);\n\t\tlan966x_ifh_set_bypass(ifh, 1);\n\t\tlan966x_ifh_set_port(ifh, BIT_ULL(port->chip_port));\n\n\t\tdma_addr = page_pool_get_dma_addr(page);\n\t\tdma_sync_single_for_device(lan966x->dev,\n\t\t\t\t\t   dma_addr + XDP_PACKET_HEADROOM,\n\t\t\t\t\t   len + IFH_LEN_BYTES,\n\t\t\t\t\t   DMA_TO_DEVICE);\n\n\t\tnext_dcb_buf->data.page = page;\n\t\tnext_dcb_buf->len = len + IFH_LEN_BYTES;\n\n\t\t \n\t\tlan966x_fdma_tx_setup_dcb(tx, next_to_use,\n\t\t\t\t\t  len + IFH_LEN_BYTES,\n\t\t\t\t\t  dma_addr + XDP_PACKET_HEADROOM);\n\t}\n\n\t \n\tnext_dcb_buf->use_skb = false;\n\tnext_dcb_buf->xdp_ndo = !len;\n\tnext_dcb_buf->dma_addr = dma_addr;\n\tnext_dcb_buf->used = true;\n\tnext_dcb_buf->ptp = false;\n\tnext_dcb_buf->dev = port->dev;\n\n\t \n\tlan966x_fdma_tx_start(tx, next_to_use);\n\nout:\n\tspin_unlock(&lan966x->tx_lock);\n\n\treturn ret;\n}\n\nint lan966x_fdma_xmit(struct sk_buff *skb, __be32 *ifh, struct net_device *dev)\n{\n\tstruct lan966x_port *port = netdev_priv(dev);\n\tstruct lan966x *lan966x = port->lan966x;\n\tstruct lan966x_tx_dcb_buf *next_dcb_buf;\n\tstruct lan966x_tx *tx = &lan966x->tx;\n\tint needed_headroom;\n\tint needed_tailroom;\n\tdma_addr_t dma_addr;\n\tint next_to_use;\n\tint err;\n\n\t \n\tnext_to_use = lan966x_fdma_get_next_dcb(tx);\n\tif (next_to_use < 0) {\n\t\tnetif_stop_queue(dev);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (skb_put_padto(skb, ETH_ZLEN)) {\n\t\tdev->stats.tx_dropped++;\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\t \n\tneeded_headroom = max_t(int, IFH_LEN_BYTES - skb_headroom(skb), 0);\n\tneeded_tailroom = max_t(int, ETH_FCS_LEN - skb_tailroom(skb), 0);\n\tif (needed_headroom || needed_tailroom || skb_header_cloned(skb)) {\n\t\terr = pskb_expand_head(skb, needed_headroom, needed_tailroom,\n\t\t\t\t       GFP_ATOMIC);\n\t\tif (unlikely(err)) {\n\t\t\tdev->stats.tx_dropped++;\n\t\t\terr = NETDEV_TX_OK;\n\t\t\tgoto release;\n\t\t}\n\t}\n\n\tskb_tx_timestamp(skb);\n\tskb_push(skb, IFH_LEN_BYTES);\n\tmemcpy(skb->data, ifh, IFH_LEN_BYTES);\n\tskb_put(skb, 4);\n\n\tdma_addr = dma_map_single(lan966x->dev, skb->data, skb->len,\n\t\t\t\t  DMA_TO_DEVICE);\n\tif (dma_mapping_error(lan966x->dev, dma_addr)) {\n\t\tdev->stats.tx_dropped++;\n\t\terr = NETDEV_TX_OK;\n\t\tgoto release;\n\t}\n\n\t \n\tlan966x_fdma_tx_setup_dcb(tx, next_to_use, skb->len, dma_addr);\n\n\t \n\tnext_dcb_buf = &tx->dcbs_buf[next_to_use];\n\tnext_dcb_buf->use_skb = true;\n\tnext_dcb_buf->data.skb = skb;\n\tnext_dcb_buf->xdp_ndo = false;\n\tnext_dcb_buf->len = skb->len;\n\tnext_dcb_buf->dma_addr = dma_addr;\n\tnext_dcb_buf->used = true;\n\tnext_dcb_buf->ptp = false;\n\tnext_dcb_buf->dev = dev;\n\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP &&\n\t    LAN966X_SKB_CB(skb)->rew_op == IFH_REW_OP_TWO_STEP_PTP)\n\t\tnext_dcb_buf->ptp = true;\n\n\t \n\tlan966x_fdma_tx_start(tx, next_to_use);\n\n\treturn NETDEV_TX_OK;\n\nrelease:\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP &&\n\t    LAN966X_SKB_CB(skb)->rew_op == IFH_REW_OP_TWO_STEP_PTP)\n\t\tlan966x_ptp_txtstamp_release(port, skb);\n\n\tdev_kfree_skb_any(skb);\n\treturn err;\n}\n\nstatic int lan966x_fdma_get_max_mtu(struct lan966x *lan966x)\n{\n\tint max_mtu = 0;\n\tint i;\n\n\tfor (i = 0; i < lan966x->num_phys_ports; ++i) {\n\t\tstruct lan966x_port *port;\n\t\tint mtu;\n\n\t\tport = lan966x->ports[i];\n\t\tif (!port)\n\t\t\tcontinue;\n\n\t\tmtu = lan_rd(lan966x, DEV_MAC_MAXLEN_CFG(port->chip_port));\n\t\tif (mtu > max_mtu)\n\t\t\tmax_mtu = mtu;\n\t}\n\n\treturn max_mtu;\n}\n\nstatic int lan966x_qsys_sw_status(struct lan966x *lan966x)\n{\n\treturn lan_rd(lan966x, QSYS_SW_STATUS(CPU_PORT));\n}\n\nstatic int lan966x_fdma_reload(struct lan966x *lan966x, int new_mtu)\n{\n\tstruct page_pool *page_pool;\n\tdma_addr_t rx_dma;\n\tvoid *rx_dcbs;\n\tu32 size;\n\tint err;\n\n\t \n\trx_dma = lan966x->rx.dma;\n\trx_dcbs = lan966x->rx.dcbs;\n\tpage_pool = lan966x->rx.page_pool;\n\n\tnapi_synchronize(&lan966x->napi);\n\tnapi_disable(&lan966x->napi);\n\tlan966x_fdma_stop_netdev(lan966x);\n\n\tlan966x_fdma_rx_disable(&lan966x->rx);\n\tlan966x_fdma_rx_free_pages(&lan966x->rx);\n\tlan966x->rx.page_order = round_up(new_mtu, PAGE_SIZE) / PAGE_SIZE - 1;\n\tlan966x->rx.max_mtu = new_mtu;\n\terr = lan966x_fdma_rx_alloc(&lan966x->rx);\n\tif (err)\n\t\tgoto restore;\n\tlan966x_fdma_rx_start(&lan966x->rx);\n\n\tsize = sizeof(struct lan966x_rx_dcb) * FDMA_DCB_MAX;\n\tsize = ALIGN(size, PAGE_SIZE);\n\tdma_free_coherent(lan966x->dev, size, rx_dcbs, rx_dma);\n\n\tpage_pool_destroy(page_pool);\n\n\tlan966x_fdma_wakeup_netdev(lan966x);\n\tnapi_enable(&lan966x->napi);\n\n\treturn err;\nrestore:\n\tlan966x->rx.page_pool = page_pool;\n\tlan966x->rx.dma = rx_dma;\n\tlan966x->rx.dcbs = rx_dcbs;\n\tlan966x_fdma_rx_start(&lan966x->rx);\n\n\treturn err;\n}\n\nstatic int lan966x_fdma_get_max_frame(struct lan966x *lan966x)\n{\n\treturn lan966x_fdma_get_max_mtu(lan966x) +\n\t       IFH_LEN_BYTES +\n\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) +\n\t       VLAN_HLEN * 2 +\n\t       XDP_PACKET_HEADROOM;\n}\n\nstatic int __lan966x_fdma_reload(struct lan966x *lan966x, int max_mtu)\n{\n\tint err;\n\tu32 val;\n\n\t \n\tlan_rmw(QSYS_SW_PORT_MODE_PORT_ENA_SET(0),\n\t\tQSYS_SW_PORT_MODE_PORT_ENA,\n\t\tlan966x, QSYS_SW_PORT_MODE(CPU_PORT));\n\n\t \n\treadx_poll_timeout(lan966x_qsys_sw_status, lan966x,\n\t\t\t   val, !(QSYS_SW_STATUS_EQ_AVAIL_GET(val)),\n\t\t\t   READL_SLEEP_US, READL_TIMEOUT_US);\n\n\t \n\tusleep_range(1000, 2000);\n\n\terr = lan966x_fdma_reload(lan966x, max_mtu);\n\n\t \n\tlan_rmw(QSYS_SW_PORT_MODE_PORT_ENA_SET(1),\n\t\tQSYS_SW_PORT_MODE_PORT_ENA,\n\t\tlan966x,  QSYS_SW_PORT_MODE(CPU_PORT));\n\n\treturn err;\n}\n\nint lan966x_fdma_change_mtu(struct lan966x *lan966x)\n{\n\tint max_mtu;\n\n\tmax_mtu = lan966x_fdma_get_max_frame(lan966x);\n\tif (max_mtu == lan966x->rx.max_mtu)\n\t\treturn 0;\n\n\treturn __lan966x_fdma_reload(lan966x, max_mtu);\n}\n\nint lan966x_fdma_reload_page_pool(struct lan966x *lan966x)\n{\n\tint max_mtu;\n\n\tmax_mtu = lan966x_fdma_get_max_frame(lan966x);\n\treturn __lan966x_fdma_reload(lan966x, max_mtu);\n}\n\nvoid lan966x_fdma_netdev_init(struct lan966x *lan966x, struct net_device *dev)\n{\n\tif (lan966x->fdma_ndev)\n\t\treturn;\n\n\tlan966x->fdma_ndev = dev;\n\tnetif_napi_add(dev, &lan966x->napi, lan966x_fdma_napi_poll);\n\tnapi_enable(&lan966x->napi);\n}\n\nvoid lan966x_fdma_netdev_deinit(struct lan966x *lan966x, struct net_device *dev)\n{\n\tif (lan966x->fdma_ndev == dev) {\n\t\tnetif_napi_del(&lan966x->napi);\n\t\tlan966x->fdma_ndev = NULL;\n\t}\n}\n\nint lan966x_fdma_init(struct lan966x *lan966x)\n{\n\tint err;\n\n\tif (!lan966x->fdma)\n\t\treturn 0;\n\n\tlan966x->rx.lan966x = lan966x;\n\tlan966x->rx.channel_id = FDMA_XTR_CHANNEL;\n\tlan966x->rx.max_mtu = lan966x_fdma_get_max_frame(lan966x);\n\tlan966x->tx.lan966x = lan966x;\n\tlan966x->tx.channel_id = FDMA_INJ_CHANNEL;\n\tlan966x->tx.last_in_use = -1;\n\n\terr = lan966x_fdma_rx_alloc(&lan966x->rx);\n\tif (err)\n\t\treturn err;\n\n\terr = lan966x_fdma_tx_alloc(&lan966x->tx);\n\tif (err) {\n\t\tlan966x_fdma_rx_free(&lan966x->rx);\n\t\treturn err;\n\t}\n\n\tlan966x_fdma_rx_start(&lan966x->rx);\n\n\treturn 0;\n}\n\nvoid lan966x_fdma_deinit(struct lan966x *lan966x)\n{\n\tif (!lan966x->fdma)\n\t\treturn;\n\n\tlan966x_fdma_rx_disable(&lan966x->rx);\n\tlan966x_fdma_tx_disable(&lan966x->tx);\n\n\tnapi_synchronize(&lan966x->napi);\n\tnapi_disable(&lan966x->napi);\n\n\tlan966x_fdma_rx_free_pages(&lan966x->rx);\n\tlan966x_fdma_rx_free(&lan966x->rx);\n\tpage_pool_destroy(lan966x->rx.page_pool);\n\tlan966x_fdma_tx_free(&lan966x->tx);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}