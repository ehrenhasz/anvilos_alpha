{
  "module_name": "sparx5_fdma.c",
  "hash_id": "313444b3fc4fba6f4ef5f728d538a1511113c6100d69079d7f59c312c3904b69",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/microchip/sparx5/sparx5_fdma.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/interrupt.h>\n#include <linux/ip.h>\n#include <linux/dma-mapping.h>\n\n#include \"sparx5_main_regs.h\"\n#include \"sparx5_main.h\"\n#include \"sparx5_port.h\"\n\n#define FDMA_XTR_CHANNEL\t\t6\n#define FDMA_INJ_CHANNEL\t\t0\n\n#define FDMA_DCB_INFO_DATAL(x)\t\t((x) & GENMASK(15, 0))\n#define FDMA_DCB_INFO_TOKEN\t\tBIT(17)\n#define FDMA_DCB_INFO_INTR\t\tBIT(18)\n#define FDMA_DCB_INFO_SW(x)\t\t(((x) << 24) & GENMASK(31, 24))\n\n#define FDMA_DCB_STATUS_BLOCKL(x)\t((x) & GENMASK(15, 0))\n#define FDMA_DCB_STATUS_SOF\t\tBIT(16)\n#define FDMA_DCB_STATUS_EOF\t\tBIT(17)\n#define FDMA_DCB_STATUS_INTR\t\tBIT(18)\n#define FDMA_DCB_STATUS_DONE\t\tBIT(19)\n#define FDMA_DCB_STATUS_BLOCKO(x)\t(((x) << 20) & GENMASK(31, 20))\n#define FDMA_DCB_INVALID_DATA\t\t0x1\n\n#define FDMA_XTR_BUFFER_SIZE\t\t2048\n#define FDMA_WEIGHT\t\t\t4\n\n \n\n \nstruct sparx5_db {\n\tstruct list_head list;\n\tvoid *cpu_addr;\n};\n\nstatic void sparx5_fdma_rx_add_dcb(struct sparx5_rx *rx,\n\t\t\t\t   struct sparx5_rx_dcb_hw *dcb,\n\t\t\t\t   u64 nextptr)\n{\n\tint idx = 0;\n\n\t \n\tfor (idx = 0; idx < FDMA_RX_DCB_MAX_DBS; ++idx) {\n\t\tstruct sparx5_db_hw *db = &dcb->db[idx];\n\n\t\tdb->status = FDMA_DCB_STATUS_INTR;\n\t}\n\tdcb->nextptr = FDMA_DCB_INVALID_DATA;\n\tdcb->info = FDMA_DCB_INFO_DATAL(FDMA_XTR_BUFFER_SIZE);\n\trx->last_entry->nextptr = nextptr;\n\trx->last_entry = dcb;\n}\n\nstatic void sparx5_fdma_tx_add_dcb(struct sparx5_tx *tx,\n\t\t\t\t   struct sparx5_tx_dcb_hw *dcb,\n\t\t\t\t   u64 nextptr)\n{\n\tint idx = 0;\n\n\t \n\tfor (idx = 0; idx < FDMA_TX_DCB_MAX_DBS; ++idx) {\n\t\tstruct sparx5_db_hw *db = &dcb->db[idx];\n\n\t\tdb->status = FDMA_DCB_STATUS_DONE;\n\t}\n\tdcb->nextptr = FDMA_DCB_INVALID_DATA;\n\tdcb->info = FDMA_DCB_INFO_DATAL(FDMA_XTR_BUFFER_SIZE);\n}\n\nstatic void sparx5_fdma_rx_activate(struct sparx5 *sparx5, struct sparx5_rx *rx)\n{\n\t \n\tspx5_wr(((u64)rx->dma) & GENMASK(31, 0), sparx5,\n\t\tFDMA_DCB_LLP(rx->channel_id));\n\tspx5_wr(((u64)rx->dma) >> 32, sparx5, FDMA_DCB_LLP1(rx->channel_id));\n\n\t \n\tspx5_wr(FDMA_CH_CFG_CH_DCB_DB_CNT_SET(FDMA_RX_DCB_MAX_DBS) |\n\t\tFDMA_CH_CFG_CH_INTR_DB_EOF_ONLY_SET(1) |\n\t\tFDMA_CH_CFG_CH_INJ_PORT_SET(XTR_QUEUE),\n\t\tsparx5, FDMA_CH_CFG(rx->channel_id));\n\n\t \n\tspx5_rmw(FDMA_XTR_CFG_XTR_FIFO_WM_SET(31), FDMA_XTR_CFG_XTR_FIFO_WM,\n\t\t sparx5,\n\t\t FDMA_XTR_CFG);\n\n\t \n\tspx5_rmw(FDMA_PORT_CTRL_XTR_STOP_SET(0), FDMA_PORT_CTRL_XTR_STOP,\n\t\t sparx5, FDMA_PORT_CTRL(0));\n\n\t \n\tspx5_rmw(BIT(rx->channel_id),\n\t\t BIT(rx->channel_id) & FDMA_INTR_DB_ENA_INTR_DB_ENA,\n\t\t sparx5, FDMA_INTR_DB_ENA);\n\n\t \n\tspx5_wr(BIT(rx->channel_id), sparx5, FDMA_CH_ACTIVATE);\n}\n\nstatic void sparx5_fdma_rx_deactivate(struct sparx5 *sparx5, struct sparx5_rx *rx)\n{\n\t \n\tspx5_rmw(0, BIT(rx->channel_id) & FDMA_CH_ACTIVATE_CH_ACTIVATE,\n\t\t sparx5, FDMA_CH_ACTIVATE);\n\n\t \n\tspx5_rmw(0, BIT(rx->channel_id) & FDMA_INTR_DB_ENA_INTR_DB_ENA,\n\t\t sparx5, FDMA_INTR_DB_ENA);\n\n\t \n\tspx5_rmw(FDMA_PORT_CTRL_XTR_STOP_SET(1), FDMA_PORT_CTRL_XTR_STOP,\n\t\t sparx5, FDMA_PORT_CTRL(0));\n}\n\nstatic void sparx5_fdma_tx_activate(struct sparx5 *sparx5, struct sparx5_tx *tx)\n{\n\t \n\tspx5_wr(((u64)tx->dma) & GENMASK(31, 0), sparx5,\n\t\tFDMA_DCB_LLP(tx->channel_id));\n\tspx5_wr(((u64)tx->dma) >> 32, sparx5, FDMA_DCB_LLP1(tx->channel_id));\n\n\t \n\tspx5_wr(FDMA_CH_CFG_CH_DCB_DB_CNT_SET(FDMA_TX_DCB_MAX_DBS) |\n\t\tFDMA_CH_CFG_CH_INTR_DB_EOF_ONLY_SET(1) |\n\t\tFDMA_CH_CFG_CH_INJ_PORT_SET(INJ_QUEUE),\n\t\tsparx5, FDMA_CH_CFG(tx->channel_id));\n\n\t \n\tspx5_rmw(FDMA_PORT_CTRL_INJ_STOP_SET(0), FDMA_PORT_CTRL_INJ_STOP,\n\t\t sparx5, FDMA_PORT_CTRL(0));\n\n\t \n\tspx5_wr(BIT(tx->channel_id), sparx5, FDMA_CH_ACTIVATE);\n}\n\nstatic void sparx5_fdma_tx_deactivate(struct sparx5 *sparx5, struct sparx5_tx *tx)\n{\n\t \n\tspx5_rmw(0, BIT(tx->channel_id) & FDMA_CH_ACTIVATE_CH_ACTIVATE,\n\t\t sparx5, FDMA_CH_ACTIVATE);\n}\n\nstatic void sparx5_fdma_rx_reload(struct sparx5 *sparx5, struct sparx5_rx *rx)\n{\n\t \n\tspx5_wr(BIT(rx->channel_id), sparx5, FDMA_CH_RELOAD);\n}\n\nstatic void sparx5_fdma_tx_reload(struct sparx5 *sparx5, struct sparx5_tx *tx)\n{\n\t \n\tspx5_wr(BIT(tx->channel_id), sparx5, FDMA_CH_RELOAD);\n}\n\nstatic struct sk_buff *sparx5_fdma_rx_alloc_skb(struct sparx5_rx *rx)\n{\n\treturn __netdev_alloc_skb(rx->ndev, FDMA_XTR_BUFFER_SIZE,\n\t\t\t\t  GFP_ATOMIC);\n}\n\nstatic bool sparx5_fdma_rx_get_frame(struct sparx5 *sparx5, struct sparx5_rx *rx)\n{\n\tstruct sparx5_db_hw *db_hw;\n\tunsigned int packet_size;\n\tstruct sparx5_port *port;\n\tstruct sk_buff *new_skb;\n\tstruct frame_info fi;\n\tstruct sk_buff *skb;\n\tdma_addr_t dma_addr;\n\n\t \n\tdb_hw = &rx->dcb_entries[rx->dcb_index].db[rx->db_index];\n\tif (unlikely(!(db_hw->status & FDMA_DCB_STATUS_DONE)))\n\t\treturn false;\n\tskb = rx->skb[rx->dcb_index][rx->db_index];\n\t \n\tnew_skb = sparx5_fdma_rx_alloc_skb(rx);\n\tif (unlikely(!new_skb))\n\t\treturn false;\n\t \n\tdma_addr = virt_to_phys(new_skb->data);\n\trx->skb[rx->dcb_index][rx->db_index] = new_skb;\n\tdb_hw->dataptr = dma_addr;\n\tpacket_size = FDMA_DCB_STATUS_BLOCKL(db_hw->status);\n\tskb_put(skb, packet_size);\n\t \n\tsparx5_ifh_parse((u32 *)skb->data, &fi);\n\t \n\tport = fi.src_port < SPX5_PORTS ?  sparx5->ports[fi.src_port] : NULL;\n\tif (!port || !port->ndev) {\n\t\tdev_err(sparx5->dev, \"Data on inactive port %d\\n\", fi.src_port);\n\t\tsparx5_xtr_flush(sparx5, XTR_QUEUE);\n\t\treturn false;\n\t}\n\tskb->dev = port->ndev;\n\tskb_pull(skb, IFH_LEN * sizeof(u32));\n\tif (likely(!(skb->dev->features & NETIF_F_RXFCS)))\n\t\tskb_trim(skb, skb->len - ETH_FCS_LEN);\n\n\tsparx5_ptp_rxtstamp(sparx5, skb, fi.timestamp);\n\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t \n\tif (test_bit(port->portno, sparx5->bridge_mask))\n\t\tskb->offload_fwd_mark = 1;\n\tskb->dev->stats.rx_bytes += skb->len;\n\tskb->dev->stats.rx_packets++;\n\trx->packets++;\n\tnetif_receive_skb(skb);\n\treturn true;\n}\n\nstatic int sparx5_fdma_napi_callback(struct napi_struct *napi, int weight)\n{\n\tstruct sparx5_rx *rx = container_of(napi, struct sparx5_rx, napi);\n\tstruct sparx5 *sparx5 = container_of(rx, struct sparx5, rx);\n\tint counter = 0;\n\n\twhile (counter < weight && sparx5_fdma_rx_get_frame(sparx5, rx)) {\n\t\tstruct sparx5_rx_dcb_hw *old_dcb;\n\n\t\trx->db_index++;\n\t\tcounter++;\n\t\t \n\t\tif (rx->db_index != FDMA_RX_DCB_MAX_DBS)\n\t\t\tcontinue;\n\t\t \n\t\trx->db_index = 0;\n\t\told_dcb = &rx->dcb_entries[rx->dcb_index];\n\t\trx->dcb_index++;\n\t\trx->dcb_index &= FDMA_DCB_MAX - 1;\n\t\tsparx5_fdma_rx_add_dcb(rx, old_dcb,\n\t\t\t\t       rx->dma +\n\t\t\t\t       ((unsigned long)old_dcb -\n\t\t\t\t\t(unsigned long)rx->dcb_entries));\n\t}\n\tif (counter < weight) {\n\t\tnapi_complete_done(&rx->napi, counter);\n\t\tspx5_rmw(BIT(rx->channel_id),\n\t\t\t BIT(rx->channel_id) & FDMA_INTR_DB_ENA_INTR_DB_ENA,\n\t\t\t sparx5, FDMA_INTR_DB_ENA);\n\t}\n\tif (counter)\n\t\tsparx5_fdma_rx_reload(sparx5, rx);\n\treturn counter;\n}\n\nstatic struct sparx5_tx_dcb_hw *sparx5_fdma_next_dcb(struct sparx5_tx *tx,\n\t\t\t\t\t\t     struct sparx5_tx_dcb_hw *dcb)\n{\n\tstruct sparx5_tx_dcb_hw *next_dcb;\n\n\tnext_dcb = dcb;\n\tnext_dcb++;\n\t \n\tif ((unsigned long)next_dcb >=\n\t    ((unsigned long)tx->first_entry + FDMA_DCB_MAX * sizeof(*dcb)))\n\t\tnext_dcb = tx->first_entry;\n\treturn next_dcb;\n}\n\nint sparx5_fdma_xmit(struct sparx5 *sparx5, u32 *ifh, struct sk_buff *skb)\n{\n\tstruct sparx5_tx_dcb_hw *next_dcb_hw;\n\tstruct sparx5_tx *tx = &sparx5->tx;\n\tstatic bool first_time = true;\n\tstruct sparx5_db_hw *db_hw;\n\tstruct sparx5_db *db;\n\n\tnext_dcb_hw = sparx5_fdma_next_dcb(tx, tx->curr_entry);\n\tdb_hw = &next_dcb_hw->db[0];\n\tif (!(db_hw->status & FDMA_DCB_STATUS_DONE))\n\t\treturn -EINVAL;\n\tdb = list_first_entry(&tx->db_list, struct sparx5_db, list);\n\tlist_move_tail(&db->list, &tx->db_list);\n\tnext_dcb_hw->nextptr = FDMA_DCB_INVALID_DATA;\n\ttx->curr_entry->nextptr = tx->dma +\n\t\t((unsigned long)next_dcb_hw -\n\t\t (unsigned long)tx->first_entry);\n\ttx->curr_entry = next_dcb_hw;\n\tmemset(db->cpu_addr, 0, FDMA_XTR_BUFFER_SIZE);\n\tmemcpy(db->cpu_addr, ifh, IFH_LEN * 4);\n\tmemcpy(db->cpu_addr + IFH_LEN * 4, skb->data, skb->len);\n\tdb_hw->status = FDMA_DCB_STATUS_SOF |\n\t\t\tFDMA_DCB_STATUS_EOF |\n\t\t\tFDMA_DCB_STATUS_BLOCKO(0) |\n\t\t\tFDMA_DCB_STATUS_BLOCKL(skb->len + IFH_LEN * 4 + 4);\n\tif (first_time) {\n\t\tsparx5_fdma_tx_activate(sparx5, tx);\n\t\tfirst_time = false;\n\t} else {\n\t\tsparx5_fdma_tx_reload(sparx5, tx);\n\t}\n\treturn NETDEV_TX_OK;\n}\n\nstatic int sparx5_fdma_rx_alloc(struct sparx5 *sparx5)\n{\n\tstruct sparx5_rx *rx = &sparx5->rx;\n\tstruct sparx5_rx_dcb_hw *dcb;\n\tint idx, jdx;\n\tint size;\n\n\tsize = sizeof(struct sparx5_rx_dcb_hw) * FDMA_DCB_MAX;\n\tsize = ALIGN(size, PAGE_SIZE);\n\trx->dcb_entries = devm_kzalloc(sparx5->dev, size, GFP_KERNEL);\n\tif (!rx->dcb_entries)\n\t\treturn -ENOMEM;\n\trx->dma = virt_to_phys(rx->dcb_entries);\n\trx->last_entry = rx->dcb_entries;\n\trx->db_index = 0;\n\trx->dcb_index = 0;\n\t \n\tfor (idx = 0; idx < FDMA_DCB_MAX; ++idx) {\n\t\tdcb = &rx->dcb_entries[idx];\n\t\tdcb->info = 0;\n\t\t \n\t\tfor (jdx = 0; jdx < FDMA_RX_DCB_MAX_DBS; ++jdx) {\n\t\t\tstruct sparx5_db_hw *db_hw = &dcb->db[jdx];\n\t\t\tdma_addr_t dma_addr;\n\t\t\tstruct sk_buff *skb;\n\n\t\t\tskb = sparx5_fdma_rx_alloc_skb(rx);\n\t\t\tif (!skb)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdma_addr = virt_to_phys(skb->data);\n\t\t\tdb_hw->dataptr = dma_addr;\n\t\t\tdb_hw->status = 0;\n\t\t\trx->skb[idx][jdx] = skb;\n\t\t}\n\t\tsparx5_fdma_rx_add_dcb(rx, dcb, rx->dma + sizeof(*dcb) * idx);\n\t}\n\tnetif_napi_add_weight(rx->ndev, &rx->napi, sparx5_fdma_napi_callback,\n\t\t\t      FDMA_WEIGHT);\n\tnapi_enable(&rx->napi);\n\tsparx5_fdma_rx_activate(sparx5, rx);\n\treturn 0;\n}\n\nstatic int sparx5_fdma_tx_alloc(struct sparx5 *sparx5)\n{\n\tstruct sparx5_tx *tx = &sparx5->tx;\n\tstruct sparx5_tx_dcb_hw *dcb;\n\tint idx, jdx;\n\tint size;\n\n\tsize = sizeof(struct sparx5_tx_dcb_hw) * FDMA_DCB_MAX;\n\tsize = ALIGN(size, PAGE_SIZE);\n\ttx->curr_entry = devm_kzalloc(sparx5->dev, size, GFP_KERNEL);\n\tif (!tx->curr_entry)\n\t\treturn -ENOMEM;\n\ttx->dma = virt_to_phys(tx->curr_entry);\n\ttx->first_entry = tx->curr_entry;\n\tINIT_LIST_HEAD(&tx->db_list);\n\t \n\tfor (idx = 0; idx < FDMA_DCB_MAX; ++idx) {\n\t\tdcb = &tx->curr_entry[idx];\n\t\tdcb->info = 0;\n\t\t \n\t\tfor (jdx = 0; jdx < FDMA_TX_DCB_MAX_DBS; ++jdx) {\n\t\t\tstruct sparx5_db_hw *db_hw = &dcb->db[jdx];\n\t\t\tstruct sparx5_db *db;\n\t\t\tdma_addr_t phys;\n\t\t\tvoid *cpu_addr;\n\n\t\t\tcpu_addr = devm_kzalloc(sparx5->dev,\n\t\t\t\t\t\tFDMA_XTR_BUFFER_SIZE,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!cpu_addr)\n\t\t\t\treturn -ENOMEM;\n\t\t\tphys = virt_to_phys(cpu_addr);\n\t\t\tdb_hw->dataptr = phys;\n\t\t\tdb_hw->status = 0;\n\t\t\tdb = devm_kzalloc(sparx5->dev, sizeof(*db), GFP_KERNEL);\n\t\t\tif (!db)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdb->cpu_addr = cpu_addr;\n\t\t\tlist_add_tail(&db->list, &tx->db_list);\n\t\t}\n\t\tsparx5_fdma_tx_add_dcb(tx, dcb, tx->dma + sizeof(*dcb) * idx);\n\t\t \n\t\tif (idx == FDMA_DCB_MAX - 1)\n\t\t\ttx->curr_entry = dcb;\n\t}\n\treturn 0;\n}\n\nstatic void sparx5_fdma_rx_init(struct sparx5 *sparx5,\n\t\t\t\tstruct sparx5_rx *rx, int channel)\n{\n\tint idx;\n\n\trx->channel_id = channel;\n\t \n\tfor (idx = 0; idx < SPX5_PORTS; ++idx) {\n\t\tstruct sparx5_port *port = sparx5->ports[idx];\n\n\t\tif (port && port->ndev) {\n\t\t\trx->ndev = port->ndev;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void sparx5_fdma_tx_init(struct sparx5 *sparx5,\n\t\t\t\tstruct sparx5_tx *tx, int channel)\n{\n\ttx->channel_id = channel;\n}\n\nirqreturn_t sparx5_fdma_handler(int irq, void *args)\n{\n\tstruct sparx5 *sparx5 = args;\n\tu32 db = 0, err = 0;\n\n\tdb = spx5_rd(sparx5, FDMA_INTR_DB);\n\terr = spx5_rd(sparx5, FDMA_INTR_ERR);\n\t \n\tif (db) {\n\t\tspx5_wr(0, sparx5, FDMA_INTR_DB_ENA);\n\t\tspx5_wr(db, sparx5, FDMA_INTR_DB);\n\t\tnapi_schedule(&sparx5->rx.napi);\n\t}\n\tif (err) {\n\t\tu32 err_type = spx5_rd(sparx5, FDMA_ERRORS);\n\n\t\tdev_err_ratelimited(sparx5->dev,\n\t\t\t\t    \"ERR: int: %#x, type: %#x\\n\",\n\t\t\t\t    err, err_type);\n\t\tspx5_wr(err, sparx5, FDMA_INTR_ERR);\n\t\tspx5_wr(err_type, sparx5, FDMA_ERRORS);\n\t}\n\treturn IRQ_HANDLED;\n}\n\nstatic void sparx5_fdma_injection_mode(struct sparx5 *sparx5)\n{\n\tconst int byte_swap = 1;\n\tint portno;\n\tint urgency;\n\n\t \n\tspx5_wr(QS_XTR_GRP_CFG_MODE_SET(2) |\n\t\tQS_XTR_GRP_CFG_STATUS_WORD_POS_SET(1) |\n\t\tQS_XTR_GRP_CFG_BYTE_SWAP_SET(byte_swap),\n\t\tsparx5, QS_XTR_GRP_CFG(XTR_QUEUE));\n\tspx5_wr(QS_INJ_GRP_CFG_MODE_SET(2) |\n\t\tQS_INJ_GRP_CFG_BYTE_SWAP_SET(byte_swap),\n\t\tsparx5, QS_INJ_GRP_CFG(INJ_QUEUE));\n\n\t \n\tfor (portno = SPX5_PORT_CPU_0; portno <= SPX5_PORT_CPU_1; portno++) {\n\t\t \n\t\tspx5_wr(ASM_PORT_CFG_PAD_ENA_SET(1) |\n\t\t\tASM_PORT_CFG_NO_PREAMBLE_ENA_SET(1) |\n\t\t\tASM_PORT_CFG_INJ_FORMAT_CFG_SET(1),  \n\t\t\tsparx5, ASM_PORT_CFG(portno));\n\n\t\t \n\t\tspx5_rmw(DSM_DEV_TX_STOP_WM_CFG_DEV_TX_CNT_CLR_SET(1),\n\t\t\t DSM_DEV_TX_STOP_WM_CFG_DEV_TX_CNT_CLR,\n\t\t\t sparx5,\n\t\t\t DSM_DEV_TX_STOP_WM_CFG(portno));\n\n\t\t \n\t\tspx5_rmw(DSM_DEV_TX_STOP_WM_CFG_DEV_TX_STOP_WM_SET(100),\n\t\t\t DSM_DEV_TX_STOP_WM_CFG_DEV_TX_STOP_WM,\n\t\t\t sparx5,\n\t\t\t DSM_DEV_TX_STOP_WM_CFG(portno));\n\n\t\t \n\t\turgency = sparx5_port_fwd_urg(sparx5, SPEED_2500);\n\t\tspx5_rmw(QFWD_SWITCH_PORT_MODE_PORT_ENA_SET(1) |\n\t\t\t QFWD_SWITCH_PORT_MODE_FWD_URGENCY_SET(urgency),\n\t\t\t QFWD_SWITCH_PORT_MODE_PORT_ENA |\n\t\t\t QFWD_SWITCH_PORT_MODE_FWD_URGENCY,\n\t\t\t sparx5,\n\t\t\t QFWD_SWITCH_PORT_MODE(portno));\n\n\t\t \n\t\tspx5_rmw(DSM_BUF_CFG_UNDERFLOW_WATCHDOG_DIS_SET(1),\n\t\t\t DSM_BUF_CFG_UNDERFLOW_WATCHDOG_DIS,\n\t\t\t sparx5,\n\t\t\t DSM_BUF_CFG(portno));\n\n\t\t \n\t\tspx5_rmw(HSCH_PORT_MODE_AGE_DIS_SET(1),\n\t\t\t HSCH_PORT_MODE_AGE_DIS,\n\t\t\t sparx5,\n\t\t\t HSCH_PORT_MODE(portno));\n\t}\n}\n\nint sparx5_fdma_start(struct sparx5 *sparx5)\n{\n\tint err;\n\n\t \n\tspx5_wr(FDMA_CTRL_NRESET_SET(0), sparx5, FDMA_CTRL);\n\tspx5_wr(FDMA_CTRL_NRESET_SET(1), sparx5, FDMA_CTRL);\n\n\t \n\tspx5_rmw(CPU_PROC_CTRL_ACP_CACHE_FORCE_ENA_SET(1) |\n\t\t CPU_PROC_CTRL_ACP_AWCACHE_SET(0) |\n\t\t CPU_PROC_CTRL_ACP_ARCACHE_SET(0),\n\t\t CPU_PROC_CTRL_ACP_CACHE_FORCE_ENA |\n\t\t CPU_PROC_CTRL_ACP_AWCACHE |\n\t\t CPU_PROC_CTRL_ACP_ARCACHE,\n\t\t sparx5, CPU_PROC_CTRL);\n\n\tsparx5_fdma_injection_mode(sparx5);\n\tsparx5_fdma_rx_init(sparx5, &sparx5->rx, FDMA_XTR_CHANNEL);\n\tsparx5_fdma_tx_init(sparx5, &sparx5->tx, FDMA_INJ_CHANNEL);\n\terr = sparx5_fdma_rx_alloc(sparx5);\n\tif (err) {\n\t\tdev_err(sparx5->dev, \"Could not allocate RX buffers: %d\\n\", err);\n\t\treturn err;\n\t}\n\terr = sparx5_fdma_tx_alloc(sparx5);\n\tif (err) {\n\t\tdev_err(sparx5->dev, \"Could not allocate TX buffers: %d\\n\", err);\n\t\treturn err;\n\t}\n\treturn err;\n}\n\nstatic u32 sparx5_fdma_port_ctrl(struct sparx5 *sparx5)\n{\n\treturn spx5_rd(sparx5, FDMA_PORT_CTRL(0));\n}\n\nint sparx5_fdma_stop(struct sparx5 *sparx5)\n{\n\tu32 val;\n\n\tnapi_disable(&sparx5->rx.napi);\n\t \n\tsparx5_fdma_rx_deactivate(sparx5, &sparx5->rx);\n\tsparx5_fdma_tx_deactivate(sparx5, &sparx5->tx);\n\t \n\tread_poll_timeout(sparx5_fdma_port_ctrl, val,\n\t\t\t  FDMA_PORT_CTRL_XTR_BUF_IS_EMPTY_GET(val) == 0,\n\t\t\t  500, 10000, 0, sparx5);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}