{
  "module_name": "owl-emac.c",
  "hash_id": "d5fcaa4af482d5c856c70da65234637f1be1f93900900fa3819aaac566b55d6a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/actions/owl-emac.c",
  "human_readable_source": "\n \n\n#include <linux/circ_buf.h>\n#include <linux/clk.h>\n#include <linux/dma-mapping.h>\n#include <linux/etherdevice.h>\n#include <linux/of_mdio.h>\n#include <linux/of_net.h>\n#include <linux/platform_device.h>\n#include <linux/pm.h>\n#include <linux/reset.h>\n\n#include \"owl-emac.h\"\n\n#define OWL_EMAC_DEFAULT_MSG_ENABLE\t(NETIF_MSG_DRV | \\\n\t\t\t\t\t NETIF_MSG_PROBE | \\\n\t\t\t\t\t NETIF_MSG_LINK)\n\nstatic u32 owl_emac_reg_read(struct owl_emac_priv *priv, u32 reg)\n{\n\treturn readl(priv->base + reg);\n}\n\nstatic void owl_emac_reg_write(struct owl_emac_priv *priv, u32 reg, u32 data)\n{\n\twritel(data, priv->base + reg);\n}\n\nstatic u32 owl_emac_reg_update(struct owl_emac_priv *priv,\n\t\t\t       u32 reg, u32 mask, u32 val)\n{\n\tu32 data, old_val;\n\n\tdata = owl_emac_reg_read(priv, reg);\n\told_val = data & mask;\n\n\tdata &= ~mask;\n\tdata |= val & mask;\n\n\towl_emac_reg_write(priv, reg, data);\n\n\treturn old_val;\n}\n\nstatic void owl_emac_reg_set(struct owl_emac_priv *priv, u32 reg, u32 bits)\n{\n\towl_emac_reg_update(priv, reg, bits, bits);\n}\n\nstatic void owl_emac_reg_clear(struct owl_emac_priv *priv, u32 reg, u32 bits)\n{\n\towl_emac_reg_update(priv, reg, bits, 0);\n}\n\nstatic struct device *owl_emac_get_dev(struct owl_emac_priv *priv)\n{\n\treturn priv->netdev->dev.parent;\n}\n\nstatic void owl_emac_irq_enable(struct owl_emac_priv *priv)\n{\n\t \n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR7,\n\t\t\t   OWL_EMAC_BIT_MAC_CSR7_NIE |\n\t\t\t   OWL_EMAC_BIT_MAC_CSR7_AIE |\n\t\t\t   OWL_EMAC_BIT_MAC_CSR7_ALL_NOT_TUE);\n}\n\nstatic void owl_emac_irq_disable(struct owl_emac_priv *priv)\n{\n\t \n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR7,\n\t\t\t   OWL_EMAC_BIT_MAC_CSR7_ALL_NOT_TUE);\n}\n\nstatic u32 owl_emac_irq_status(struct owl_emac_priv *priv)\n{\n\treturn owl_emac_reg_read(priv, OWL_EMAC_REG_MAC_CSR5);\n}\n\nstatic u32 owl_emac_irq_clear(struct owl_emac_priv *priv)\n{\n\tu32 val = owl_emac_irq_status(priv);\n\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR5, val);\n\n\treturn val;\n}\n\nstatic dma_addr_t owl_emac_dma_map_rx(struct owl_emac_priv *priv,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tstruct device *dev = owl_emac_get_dev(priv);\n\n\t \n\treturn dma_map_single(dev, skb_tail_pointer(skb),\n\t\t\t      skb_tailroom(skb), DMA_FROM_DEVICE);\n}\n\nstatic void owl_emac_dma_unmap_rx(struct owl_emac_priv *priv,\n\t\t\t\t  struct sk_buff *skb, dma_addr_t dma_addr)\n{\n\tstruct device *dev = owl_emac_get_dev(priv);\n\n\tdma_unmap_single(dev, dma_addr, skb_tailroom(skb), DMA_FROM_DEVICE);\n}\n\nstatic dma_addr_t owl_emac_dma_map_tx(struct owl_emac_priv *priv,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tstruct device *dev = owl_emac_get_dev(priv);\n\n\treturn dma_map_single(dev, skb->data, skb_headlen(skb), DMA_TO_DEVICE);\n}\n\nstatic void owl_emac_dma_unmap_tx(struct owl_emac_priv *priv,\n\t\t\t\t  struct sk_buff *skb, dma_addr_t dma_addr)\n{\n\tstruct device *dev = owl_emac_get_dev(priv);\n\n\tdma_unmap_single(dev, dma_addr, skb_headlen(skb), DMA_TO_DEVICE);\n}\n\nstatic unsigned int owl_emac_ring_num_unused(struct owl_emac_ring *ring)\n{\n\treturn CIRC_SPACE(ring->head, ring->tail, ring->size);\n}\n\nstatic unsigned int owl_emac_ring_get_next(struct owl_emac_ring *ring,\n\t\t\t\t\t   unsigned int cur)\n{\n\treturn (cur + 1) & (ring->size - 1);\n}\n\nstatic void owl_emac_ring_push_head(struct owl_emac_ring *ring)\n{\n\tring->head = owl_emac_ring_get_next(ring, ring->head);\n}\n\nstatic void owl_emac_ring_pop_tail(struct owl_emac_ring *ring)\n{\n\tring->tail = owl_emac_ring_get_next(ring, ring->tail);\n}\n\nstatic struct sk_buff *owl_emac_alloc_skb(struct net_device *netdev)\n{\n\tstruct sk_buff *skb;\n\tint offset;\n\n\tskb = netdev_alloc_skb(netdev, OWL_EMAC_RX_FRAME_MAX_LEN +\n\t\t\t       OWL_EMAC_SKB_RESERVE);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\toffset = ((uintptr_t)skb->data) & (OWL_EMAC_SKB_ALIGN - 1);\n\tif (unlikely(offset))\n\t\tskb_reserve(skb, OWL_EMAC_SKB_ALIGN - offset);\n\n\treturn skb;\n}\n\nstatic int owl_emac_ring_prepare_rx(struct owl_emac_priv *priv)\n{\n\tstruct owl_emac_ring *ring = &priv->rx_ring;\n\tstruct device *dev = owl_emac_get_dev(priv);\n\tstruct net_device *netdev = priv->netdev;\n\tstruct owl_emac_ring_desc *desc;\n\tstruct sk_buff *skb;\n\tdma_addr_t dma_addr;\n\tint i;\n\n\tfor (i = 0; i < ring->size; i++) {\n\t\tskb = owl_emac_alloc_skb(netdev);\n\t\tif (!skb)\n\t\t\treturn -ENOMEM;\n\n\t\tdma_addr = owl_emac_dma_map_rx(priv, skb);\n\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdesc = &ring->descs[i];\n\t\tdesc->status = OWL_EMAC_BIT_RDES0_OWN;\n\t\tdesc->control = skb_tailroom(skb) & OWL_EMAC_MSK_RDES1_RBS1;\n\t\tdesc->buf_addr = dma_addr;\n\t\tdesc->reserved = 0;\n\n\t\tring->skbs[i] = skb;\n\t\tring->skbs_dma[i] = dma_addr;\n\t}\n\n\tdesc->control |= OWL_EMAC_BIT_RDES1_RER;\n\n\tring->head = 0;\n\tring->tail = 0;\n\n\treturn 0;\n}\n\nstatic void owl_emac_ring_prepare_tx(struct owl_emac_priv *priv)\n{\n\tstruct owl_emac_ring *ring = &priv->tx_ring;\n\tstruct owl_emac_ring_desc *desc;\n\tint i;\n\n\tfor (i = 0; i < ring->size; i++) {\n\t\tdesc = &ring->descs[i];\n\n\t\tdesc->status = 0;\n\t\tdesc->control = OWL_EMAC_BIT_TDES1_IC;\n\t\tdesc->buf_addr = 0;\n\t\tdesc->reserved = 0;\n\t}\n\n\tdesc->control |= OWL_EMAC_BIT_TDES1_TER;\n\n\tmemset(ring->skbs_dma, 0, sizeof(dma_addr_t) * ring->size);\n\n\tring->head = 0;\n\tring->tail = 0;\n}\n\nstatic void owl_emac_ring_unprepare_rx(struct owl_emac_priv *priv)\n{\n\tstruct owl_emac_ring *ring = &priv->rx_ring;\n\tint i;\n\n\tfor (i = 0; i < ring->size; i++) {\n\t\tring->descs[i].status = 0;\n\n\t\tif (!ring->skbs_dma[i])\n\t\t\tcontinue;\n\n\t\towl_emac_dma_unmap_rx(priv, ring->skbs[i], ring->skbs_dma[i]);\n\t\tring->skbs_dma[i] = 0;\n\n\t\tdev_kfree_skb(ring->skbs[i]);\n\t\tring->skbs[i] = NULL;\n\t}\n}\n\nstatic void owl_emac_ring_unprepare_tx(struct owl_emac_priv *priv)\n{\n\tstruct owl_emac_ring *ring = &priv->tx_ring;\n\tint i;\n\n\tfor (i = 0; i < ring->size; i++) {\n\t\tring->descs[i].status = 0;\n\n\t\tif (!ring->skbs_dma[i])\n\t\t\tcontinue;\n\n\t\towl_emac_dma_unmap_tx(priv, ring->skbs[i], ring->skbs_dma[i]);\n\t\tring->skbs_dma[i] = 0;\n\n\t\tdev_kfree_skb(ring->skbs[i]);\n\t\tring->skbs[i] = NULL;\n\t}\n}\n\nstatic int owl_emac_ring_alloc(struct device *dev, struct owl_emac_ring *ring,\n\t\t\t       unsigned int size)\n{\n\tring->descs = dmam_alloc_coherent(dev,\n\t\t\t\t\t  sizeof(struct owl_emac_ring_desc) * size,\n\t\t\t\t\t  &ring->descs_dma, GFP_KERNEL);\n\tif (!ring->descs)\n\t\treturn -ENOMEM;\n\n\tring->skbs = devm_kcalloc(dev, size, sizeof(struct sk_buff *),\n\t\t\t\t  GFP_KERNEL);\n\tif (!ring->skbs)\n\t\treturn -ENOMEM;\n\n\tring->skbs_dma = devm_kcalloc(dev, size, sizeof(dma_addr_t),\n\t\t\t\t      GFP_KERNEL);\n\tif (!ring->skbs_dma)\n\t\treturn -ENOMEM;\n\n\tring->size = size;\n\n\treturn 0;\n}\n\nstatic void owl_emac_dma_cmd_resume_rx(struct owl_emac_priv *priv)\n{\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR2,\n\t\t\t   OWL_EMAC_VAL_MAC_CSR2_RPD);\n}\n\nstatic void owl_emac_dma_cmd_resume_tx(struct owl_emac_priv *priv)\n{\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR1,\n\t\t\t   OWL_EMAC_VAL_MAC_CSR1_TPD);\n}\n\nstatic u32 owl_emac_dma_cmd_set_tx(struct owl_emac_priv *priv, u32 status)\n{\n\treturn owl_emac_reg_update(priv, OWL_EMAC_REG_MAC_CSR6,\n\t\t\t\t   OWL_EMAC_BIT_MAC_CSR6_ST, status);\n}\n\nstatic u32 owl_emac_dma_cmd_start_tx(struct owl_emac_priv *priv)\n{\n\treturn owl_emac_dma_cmd_set_tx(priv, ~0);\n}\n\nstatic u32 owl_emac_dma_cmd_set(struct owl_emac_priv *priv, u32 status)\n{\n\treturn owl_emac_reg_update(priv, OWL_EMAC_REG_MAC_CSR6,\n\t\t\t\t   OWL_EMAC_MSK_MAC_CSR6_STSR, status);\n}\n\nstatic u32 owl_emac_dma_cmd_start(struct owl_emac_priv *priv)\n{\n\treturn owl_emac_dma_cmd_set(priv, ~0);\n}\n\nstatic u32 owl_emac_dma_cmd_stop(struct owl_emac_priv *priv)\n{\n\treturn owl_emac_dma_cmd_set(priv, 0);\n}\n\nstatic void owl_emac_set_hw_mac_addr(struct net_device *netdev)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\tconst u8 *mac_addr = netdev->dev_addr;\n\tu32 addr_high, addr_low;\n\n\taddr_high = mac_addr[0] << 8 | mac_addr[1];\n\taddr_low = mac_addr[2] << 24 | mac_addr[3] << 16 |\n\t\t   mac_addr[4] << 8 | mac_addr[5];\n\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR17, addr_high);\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR16, addr_low);\n}\n\nstatic void owl_emac_update_link_state(struct owl_emac_priv *priv)\n{\n\tu32 val, status;\n\n\tif (priv->pause) {\n\t\tval = OWL_EMAC_BIT_MAC_CSR20_FCE | OWL_EMAC_BIT_MAC_CSR20_TUE;\n\t\tval |= OWL_EMAC_BIT_MAC_CSR20_TPE | OWL_EMAC_BIT_MAC_CSR20_RPE;\n\t\tval |= OWL_EMAC_BIT_MAC_CSR20_BPE;\n\t} else {\n\t\tval = 0;\n\t}\n\n\t \n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR20, val);\n\n\tval = (priv->speed == SPEED_100) ? OWL_EMAC_VAL_MAC_CSR6_SPEED_100M :\n\t\t\t\t\t   OWL_EMAC_VAL_MAC_CSR6_SPEED_10M;\n\tval <<= OWL_EMAC_OFF_MAC_CSR6_SPEED;\n\n\tif (priv->duplex == DUPLEX_FULL)\n\t\tval |= OWL_EMAC_BIT_MAC_CSR6_FD;\n\n\tspin_lock_bh(&priv->lock);\n\n\t \n\tstatus = owl_emac_dma_cmd_stop(priv);\n\n\t \n\towl_emac_reg_update(priv, OWL_EMAC_REG_MAC_CSR6,\n\t\t\t    OWL_EMAC_MSK_MAC_CSR6_SPEED |\n\t\t\t    OWL_EMAC_BIT_MAC_CSR6_FD, val);\n\n\t \n\towl_emac_dma_cmd_set(priv, status);\n\n\tspin_unlock_bh(&priv->lock);\n}\n\nstatic void owl_emac_adjust_link(struct net_device *netdev)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\tstruct phy_device *phydev = netdev->phydev;\n\tbool state_changed = false;\n\n\tif (phydev->link) {\n\t\tif (!priv->link) {\n\t\t\tpriv->link = phydev->link;\n\t\t\tstate_changed = true;\n\t\t}\n\n\t\tif (priv->speed != phydev->speed) {\n\t\t\tpriv->speed = phydev->speed;\n\t\t\tstate_changed = true;\n\t\t}\n\n\t\tif (priv->duplex != phydev->duplex) {\n\t\t\tpriv->duplex = phydev->duplex;\n\t\t\tstate_changed = true;\n\t\t}\n\n\t\tif (priv->pause != phydev->pause) {\n\t\t\tpriv->pause = phydev->pause;\n\t\t\tstate_changed = true;\n\t\t}\n\t} else {\n\t\tif (priv->link) {\n\t\t\tpriv->link = phydev->link;\n\t\t\tstate_changed = true;\n\t\t}\n\t}\n\n\tif (state_changed) {\n\t\tif (phydev->link)\n\t\t\towl_emac_update_link_state(priv);\n\n\t\tif (netif_msg_link(priv))\n\t\t\tphy_print_status(phydev);\n\t}\n}\n\nstatic irqreturn_t owl_emac_handle_irq(int irq, void *data)\n{\n\tstruct net_device *netdev = data;\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\n\tif (netif_running(netdev)) {\n\t\towl_emac_irq_disable(priv);\n\t\tnapi_schedule(&priv->napi);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void owl_emac_ether_addr_push(u8 **dst, const u8 *src)\n{\n\tu32 *a = (u32 *)(*dst);\n\tconst u16 *b = (const u16 *)src;\n\n\ta[0] = b[0];\n\ta[1] = b[1];\n\ta[2] = b[2];\n\n\t*dst += 12;\n}\n\nstatic void\nowl_emac_setup_frame_prepare(struct owl_emac_priv *priv, struct sk_buff *skb)\n{\n\tconst u8 bcast_addr[] = { 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF };\n\tconst u8 *mac_addr = priv->netdev->dev_addr;\n\tu8 *frame;\n\tint i;\n\n\tskb_put(skb, OWL_EMAC_SETUP_FRAME_LEN);\n\n\tframe = skb->data;\n\tmemset(frame, 0, skb->len);\n\n\towl_emac_ether_addr_push(&frame, mac_addr);\n\towl_emac_ether_addr_push(&frame, bcast_addr);\n\n\t \n\tWARN_ON(priv->mcaddr_list.count >= OWL_EMAC_MAX_MULTICAST_ADDRS);\n\tfor (i = 0; i < priv->mcaddr_list.count; i++) {\n\t\tmac_addr = priv->mcaddr_list.addrs[i];\n\t\towl_emac_ether_addr_push(&frame, mac_addr);\n\t}\n}\n\n \nstatic int owl_emac_setup_frame_xmit(struct owl_emac_priv *priv)\n{\n\tstruct owl_emac_ring *ring = &priv->tx_ring;\n\tstruct net_device *netdev = priv->netdev;\n\tstruct owl_emac_ring_desc *desc;\n\tstruct sk_buff *skb;\n\tunsigned int tx_head;\n\tu32 status, control;\n\tdma_addr_t dma_addr;\n\tint ret;\n\n\tskb = owl_emac_alloc_skb(netdev);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\towl_emac_setup_frame_prepare(priv, skb);\n\n\tdma_addr = owl_emac_dma_map_tx(priv, skb);\n\tif (dma_mapping_error(owl_emac_get_dev(priv), dma_addr)) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_skb;\n\t}\n\n\tspin_lock_bh(&priv->lock);\n\n\ttx_head = ring->head;\n\tdesc = &ring->descs[tx_head];\n\n\tstatus = READ_ONCE(desc->status);\n\tcontrol = READ_ONCE(desc->control);\n\tdma_rmb();  \n\n\tif (unlikely(status & OWL_EMAC_BIT_TDES0_OWN) ||\n\t    !owl_emac_ring_num_unused(ring)) {\n\t\tspin_unlock_bh(&priv->lock);\n\t\towl_emac_dma_unmap_tx(priv, skb, dma_addr);\n\t\tret = -EBUSY;\n\t\tgoto err_free_skb;\n\t}\n\n\tring->skbs[tx_head] = skb;\n\tring->skbs_dma[tx_head] = dma_addr;\n\n\tcontrol &= OWL_EMAC_BIT_TDES1_IC | OWL_EMAC_BIT_TDES1_TER;  \n\tcontrol |= OWL_EMAC_BIT_TDES1_SET;\n\tcontrol |= OWL_EMAC_MSK_TDES1_TBS1 & skb->len;\n\n\tWRITE_ONCE(desc->control, control);\n\tWRITE_ONCE(desc->buf_addr, dma_addr);\n\tdma_wmb();  \n\tWRITE_ONCE(desc->status, OWL_EMAC_BIT_TDES0_OWN);\n\n\towl_emac_ring_push_head(ring);\n\n\t \n\tstatus = owl_emac_dma_cmd_start_tx(priv);\n\n\t \n\towl_emac_dma_cmd_resume_tx(priv);\n\n\t \n\towl_emac_dma_cmd_set_tx(priv, status);\n\n\t \n\tnetif_stop_queue(netdev);\n\n\tspin_unlock_bh(&priv->lock);\n\n\treturn 0;\n\nerr_free_skb:\n\tdev_kfree_skb(skb);\n\treturn ret;\n}\n\nstatic netdev_tx_t owl_emac_ndo_start_xmit(struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *netdev)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\tstruct device *dev = owl_emac_get_dev(priv);\n\tstruct owl_emac_ring *ring = &priv->tx_ring;\n\tstruct owl_emac_ring_desc *desc;\n\tunsigned int tx_head;\n\tu32 status, control;\n\tdma_addr_t dma_addr;\n\n\tdma_addr = owl_emac_dma_map_tx(priv, skb);\n\tif (dma_mapping_error(dev, dma_addr)) {\n\t\tdev_err_ratelimited(&netdev->dev, \"TX DMA mapping failed\\n\");\n\t\tdev_kfree_skb(skb);\n\t\tnetdev->stats.tx_dropped++;\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tspin_lock_bh(&priv->lock);\n\n\ttx_head = ring->head;\n\tdesc = &ring->descs[tx_head];\n\n\tstatus = READ_ONCE(desc->status);\n\tcontrol = READ_ONCE(desc->control);\n\tdma_rmb();  \n\n\tif (!owl_emac_ring_num_unused(ring) ||\n\t    unlikely(status & OWL_EMAC_BIT_TDES0_OWN)) {\n\t\tnetif_stop_queue(netdev);\n\t\tspin_unlock_bh(&priv->lock);\n\n\t\tdev_dbg_ratelimited(&netdev->dev, \"TX buffer full, status=0x%08x\\n\",\n\t\t\t\t    owl_emac_irq_status(priv));\n\t\towl_emac_dma_unmap_tx(priv, skb, dma_addr);\n\t\tnetdev->stats.tx_dropped++;\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tring->skbs[tx_head] = skb;\n\tring->skbs_dma[tx_head] = dma_addr;\n\n\tcontrol &= OWL_EMAC_BIT_TDES1_IC | OWL_EMAC_BIT_TDES1_TER;  \n\tcontrol |= OWL_EMAC_BIT_TDES1_FS | OWL_EMAC_BIT_TDES1_LS;\n\tcontrol |= OWL_EMAC_MSK_TDES1_TBS1 & skb->len;\n\n\tWRITE_ONCE(desc->control, control);\n\tWRITE_ONCE(desc->buf_addr, dma_addr);\n\tdma_wmb();  \n\tWRITE_ONCE(desc->status, OWL_EMAC_BIT_TDES0_OWN);\n\n\towl_emac_dma_cmd_resume_tx(priv);\n\towl_emac_ring_push_head(ring);\n\n\t \n\tnetif_stop_queue(netdev);\n\n\tspin_unlock_bh(&priv->lock);\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic bool owl_emac_tx_complete_tail(struct owl_emac_priv *priv)\n{\n\tstruct owl_emac_ring *ring = &priv->tx_ring;\n\tstruct net_device *netdev = priv->netdev;\n\tstruct owl_emac_ring_desc *desc;\n\tstruct sk_buff *skb;\n\tunsigned int tx_tail;\n\tu32 status;\n\n\ttx_tail = ring->tail;\n\tdesc = &ring->descs[tx_tail];\n\n\tstatus = READ_ONCE(desc->status);\n\tdma_rmb();  \n\n\tif (status & OWL_EMAC_BIT_TDES0_OWN)\n\t\treturn false;\n\n\t \n\tif (status & OWL_EMAC_BIT_TDES0_ES) {\n\t\tdev_dbg_ratelimited(&netdev->dev,\n\t\t\t\t    \"TX complete error status: 0x%08x\\n\",\n\t\t\t\t    status);\n\n\t\tnetdev->stats.tx_errors++;\n\n\t\tif (status & OWL_EMAC_BIT_TDES0_UF)\n\t\t\tnetdev->stats.tx_fifo_errors++;\n\n\t\tif (status & OWL_EMAC_BIT_TDES0_EC)\n\t\t\tnetdev->stats.tx_aborted_errors++;\n\n\t\tif (status & OWL_EMAC_BIT_TDES0_LC)\n\t\t\tnetdev->stats.tx_window_errors++;\n\n\t\tif (status & OWL_EMAC_BIT_TDES0_NC)\n\t\t\tnetdev->stats.tx_heartbeat_errors++;\n\n\t\tif (status & OWL_EMAC_BIT_TDES0_LO)\n\t\t\tnetdev->stats.tx_carrier_errors++;\n\t} else {\n\t\tnetdev->stats.tx_packets++;\n\t\tnetdev->stats.tx_bytes += ring->skbs[tx_tail]->len;\n\t}\n\n\t \n\tif (status & OWL_EMAC_BIT_TDES0_DE)\n\t\tnetdev->stats.collisions++;\n\n\tskb = ring->skbs[tx_tail];\n\towl_emac_dma_unmap_tx(priv, skb, ring->skbs_dma[tx_tail]);\n\tdev_kfree_skb(skb);\n\n\tring->skbs[tx_tail] = NULL;\n\tring->skbs_dma[tx_tail] = 0;\n\n\towl_emac_ring_pop_tail(ring);\n\n\tif (unlikely(netif_queue_stopped(netdev)))\n\t\tnetif_wake_queue(netdev);\n\n\treturn true;\n}\n\nstatic void owl_emac_tx_complete(struct owl_emac_priv *priv)\n{\n\tstruct owl_emac_ring *ring = &priv->tx_ring;\n\tstruct net_device *netdev = priv->netdev;\n\tunsigned int tx_next;\n\tu32 status;\n\n\tspin_lock(&priv->lock);\n\n\twhile (ring->tail != ring->head) {\n\t\tif (!owl_emac_tx_complete_tail(priv))\n\t\t\tbreak;\n\t}\n\n\t \n\tif (unlikely(!owl_emac_ring_num_unused(ring))) {\n\t\ttx_next = ring->tail;\n\n\t\twhile ((tx_next = owl_emac_ring_get_next(ring, tx_next)) != ring->head) {\n\t\t\tstatus = READ_ONCE(ring->descs[tx_next].status);\n\t\t\tdma_rmb();  \n\n\t\t\tif (status & OWL_EMAC_BIT_TDES0_OWN)\n\t\t\t\tcontinue;\n\n\t\t\tnetdev_dbg(netdev, \"Found uncleared TX desc OWN bit\\n\");\n\n\t\t\tstatus = READ_ONCE(ring->descs[ring->tail].status);\n\t\t\tdma_rmb();  \n\t\t\tstatus &= ~OWL_EMAC_BIT_TDES0_OWN;\n\t\t\tWRITE_ONCE(ring->descs[ring->tail].status, status);\n\n\t\t\towl_emac_tx_complete_tail(priv);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock(&priv->lock);\n}\n\nstatic int owl_emac_rx_process(struct owl_emac_priv *priv, int budget)\n{\n\tstruct owl_emac_ring *ring = &priv->rx_ring;\n\tstruct device *dev = owl_emac_get_dev(priv);\n\tstruct net_device *netdev = priv->netdev;\n\tstruct owl_emac_ring_desc *desc;\n\tstruct sk_buff *curr_skb, *new_skb;\n\tdma_addr_t curr_dma, new_dma;\n\tunsigned int rx_tail, len;\n\tu32 status;\n\tint recv = 0;\n\n\twhile (recv < budget) {\n\t\tspin_lock(&priv->lock);\n\n\t\trx_tail = ring->tail;\n\t\tdesc = &ring->descs[rx_tail];\n\n\t\tstatus = READ_ONCE(desc->status);\n\t\tdma_rmb();  \n\n\t\tif (status & OWL_EMAC_BIT_RDES0_OWN) {\n\t\t\tspin_unlock(&priv->lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tcurr_skb = ring->skbs[rx_tail];\n\t\tcurr_dma = ring->skbs_dma[rx_tail];\n\t\towl_emac_ring_pop_tail(ring);\n\n\t\tspin_unlock(&priv->lock);\n\n\t\tif (status & (OWL_EMAC_BIT_RDES0_DE | OWL_EMAC_BIT_RDES0_RF |\n\t\t    OWL_EMAC_BIT_RDES0_TL | OWL_EMAC_BIT_RDES0_CS |\n\t\t    OWL_EMAC_BIT_RDES0_DB | OWL_EMAC_BIT_RDES0_CE |\n\t\t    OWL_EMAC_BIT_RDES0_ZERO)) {\n\t\t\tdev_dbg_ratelimited(&netdev->dev,\n\t\t\t\t\t    \"RX desc error status: 0x%08x\\n\",\n\t\t\t\t\t    status);\n\n\t\t\tif (status & OWL_EMAC_BIT_RDES0_DE)\n\t\t\t\tnetdev->stats.rx_over_errors++;\n\n\t\t\tif (status & (OWL_EMAC_BIT_RDES0_RF | OWL_EMAC_BIT_RDES0_DB))\n\t\t\t\tnetdev->stats.rx_frame_errors++;\n\n\t\t\tif (status & OWL_EMAC_BIT_RDES0_TL)\n\t\t\t\tnetdev->stats.rx_length_errors++;\n\n\t\t\tif (status & OWL_EMAC_BIT_RDES0_CS)\n\t\t\t\tnetdev->stats.collisions++;\n\n\t\t\tif (status & OWL_EMAC_BIT_RDES0_CE)\n\t\t\t\tnetdev->stats.rx_crc_errors++;\n\n\t\t\tif (status & OWL_EMAC_BIT_RDES0_ZERO)\n\t\t\t\tnetdev->stats.rx_fifo_errors++;\n\n\t\t\tgoto drop_skb;\n\t\t}\n\n\t\tlen = (status & OWL_EMAC_MSK_RDES0_FL) >> OWL_EMAC_OFF_RDES0_FL;\n\t\tif (unlikely(len > OWL_EMAC_RX_FRAME_MAX_LEN)) {\n\t\t\tnetdev->stats.rx_length_errors++;\n\t\t\tnetdev_err(netdev, \"invalid RX frame len: %u\\n\", len);\n\t\t\tgoto drop_skb;\n\t\t}\n\n\t\t \n\t\tnew_skb = owl_emac_alloc_skb(netdev);\n\t\tif (unlikely(!new_skb))\n\t\t\tgoto drop_skb;\n\n\t\tnew_dma = owl_emac_dma_map_rx(priv, new_skb);\n\t\tif (dma_mapping_error(dev, new_dma)) {\n\t\t\tdev_kfree_skb(new_skb);\n\t\t\tnetdev_err(netdev, \"RX DMA mapping failed\\n\");\n\t\t\tgoto drop_skb;\n\t\t}\n\n\t\towl_emac_dma_unmap_rx(priv, curr_skb, curr_dma);\n\n\t\tskb_put(curr_skb, len - ETH_FCS_LEN);\n\t\tcurr_skb->ip_summed = CHECKSUM_NONE;\n\t\tcurr_skb->protocol = eth_type_trans(curr_skb, netdev);\n\t\tcurr_skb->dev = netdev;\n\n\t\tnetif_receive_skb(curr_skb);\n\n\t\tnetdev->stats.rx_packets++;\n\t\tnetdev->stats.rx_bytes += len;\n\t\trecv++;\n\t\tgoto push_skb;\n\ndrop_skb:\n\t\tnetdev->stats.rx_dropped++;\n\t\tnetdev->stats.rx_errors++;\n\t\t \n\t\tnew_skb = curr_skb;\n\t\tnew_dma = curr_dma;\n\npush_skb:\n\t\tspin_lock(&priv->lock);\n\n\t\tring->skbs[ring->head] = new_skb;\n\t\tring->skbs_dma[ring->head] = new_dma;\n\n\t\tWRITE_ONCE(desc->buf_addr, new_dma);\n\t\tdma_wmb();  \n\t\tWRITE_ONCE(desc->status, OWL_EMAC_BIT_RDES0_OWN);\n\n\t\towl_emac_ring_push_head(ring);\n\n\t\tspin_unlock(&priv->lock);\n\t}\n\n\treturn recv;\n}\n\nstatic int owl_emac_poll(struct napi_struct *napi, int budget)\n{\n\tint work_done = 0, ru_cnt = 0, recv;\n\tstatic int tx_err_cnt, rx_err_cnt;\n\tstruct owl_emac_priv *priv;\n\tu32 status, proc_status;\n\n\tpriv = container_of(napi, struct owl_emac_priv, napi);\n\n\twhile ((status = owl_emac_irq_clear(priv)) &\n\t       (OWL_EMAC_BIT_MAC_CSR5_NIS | OWL_EMAC_BIT_MAC_CSR5_AIS)) {\n\t\trecv = 0;\n\n\t\t \n\t\tif (status & (OWL_EMAC_BIT_MAC_CSR5_TI | OWL_EMAC_BIT_MAC_CSR5_ETI)) {\n\t\t\towl_emac_tx_complete(priv);\n\t\t\ttx_err_cnt = 0;\n\n\t\t\t \n\t\t\tproc_status = status & OWL_EMAC_MSK_MAC_CSR5_RS;\n\t\t\tproc_status >>= OWL_EMAC_OFF_MAC_CSR5_RS;\n\t\t\tif (proc_status == OWL_EMAC_VAL_MAC_CSR5_RS_DATA ||\n\t\t\t    proc_status == OWL_EMAC_VAL_MAC_CSR5_RS_CDES ||\n\t\t\t    proc_status == OWL_EMAC_VAL_MAC_CSR5_RS_FDES)\n\t\t\t\trx_err_cnt++;\n\t\t}\n\n\t\tif (status & OWL_EMAC_BIT_MAC_CSR5_RI) {\n\t\t\trecv = owl_emac_rx_process(priv, budget - work_done);\n\t\t\trx_err_cnt = 0;\n\n\t\t\t \n\t\t\tproc_status = status & OWL_EMAC_MSK_MAC_CSR5_TS;\n\t\t\tproc_status >>= OWL_EMAC_OFF_MAC_CSR5_TS;\n\t\t\tif (proc_status == OWL_EMAC_VAL_MAC_CSR5_TS_DATA ||\n\t\t\t    proc_status == OWL_EMAC_VAL_MAC_CSR5_TS_CDES)\n\t\t\t\ttx_err_cnt++;\n\t\t} else if (status & OWL_EMAC_BIT_MAC_CSR5_RU) {\n\t\t\t \n\t\t\tif (++ru_cnt == 2)\n\t\t\t\towl_emac_dma_cmd_resume_rx(priv);\n\n\t\t\trecv = owl_emac_rx_process(priv, budget - work_done);\n\n\t\t\t \n\t\t\tif (ru_cnt > 3)\n\t\t\t\tbreak;\n\t\t}\n\n\t\twork_done += recv;\n\t\tif (work_done >= budget)\n\t\t\tbreak;\n\t}\n\n\tif (work_done < budget) {\n\t\tnapi_complete_done(napi, work_done);\n\t\towl_emac_irq_enable(priv);\n\t}\n\n\t \n\tif (tx_err_cnt > 10 || rx_err_cnt > 10) {\n\t\tnetdev_dbg(priv->netdev, \"%s error status: 0x%08x\\n\",\n\t\t\t   tx_err_cnt > 10 ? \"TX\" : \"RX\", status);\n\t\trx_err_cnt = 0;\n\t\ttx_err_cnt = 0;\n\t\tschedule_work(&priv->mac_reset_task);\n\t}\n\n\treturn work_done;\n}\n\nstatic void owl_emac_mdio_clock_enable(struct owl_emac_priv *priv)\n{\n\tu32 val;\n\n\t \n\tval = owl_emac_reg_read(priv, OWL_EMAC_REG_MAC_CSR10);\n\tval &= OWL_EMAC_MSK_MAC_CSR10_CLKDIV;\n\tval |= OWL_EMAC_VAL_MAC_CSR10_CLKDIV_128 << OWL_EMAC_OFF_MAC_CSR10_CLKDIV;\n\n\tval |= OWL_EMAC_BIT_MAC_CSR10_SB;\n\tval |= OWL_EMAC_VAL_MAC_CSR10_OPCODE_CDS << OWL_EMAC_OFF_MAC_CSR10_OPCODE;\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR10, val);\n}\n\nstatic void owl_emac_core_hw_reset(struct owl_emac_priv *priv)\n{\n\t \n\treset_control_assert(priv->reset);\n\tusleep_range(10, 20);\n\treset_control_deassert(priv->reset);\n\tusleep_range(100, 200);\n}\n\nstatic int owl_emac_core_sw_reset(struct owl_emac_priv *priv)\n{\n\tu32 val;\n\tint ret;\n\n\t \n\towl_emac_reg_set(priv, OWL_EMAC_REG_MAC_CSR0, OWL_EMAC_BIT_MAC_CSR0_SWR);\n\tret = readl_poll_timeout(priv->base + OWL_EMAC_REG_MAC_CSR0,\n\t\t\t\t val, !(val & OWL_EMAC_BIT_MAC_CSR0_SWR),\n\t\t\t\t OWL_EMAC_POLL_DELAY_USEC,\n\t\t\t\t OWL_EMAC_RESET_POLL_TIMEOUT_USEC);\n\tif (ret)\n\t\treturn ret;\n\n\tif (priv->phy_mode == PHY_INTERFACE_MODE_RMII) {\n\t\t \n\t\tval = 0;\n\t} else {\n\t\t \n\t\tval = 0x04 << OWL_EMAC_OFF_MAC_CTRL_SSDC;\n\t\tval |= OWL_EMAC_BIT_MAC_CTRL_RSIS;\n\t}\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CTRL, val);\n\n\t \n\towl_emac_mdio_clock_enable(priv);\n\n\t \n\tval = 0x40 << OWL_EMAC_OFF_MAC_CSR19_FPTL;\n\tval |= 0x10 << OWL_EMAC_OFF_MAC_CSR19_FRTL;\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR19, val);\n\n\t \n\tval = 0x4FFF << OWL_EMAC_OFF_MAC_CSR18_PQT;\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR18, val);\n\n\t \n\tval = 7 << OWL_EMAC_OFF_MAC_CSR11_NRP;\n\tval |= 4 << OWL_EMAC_OFF_MAC_CSR11_RT;\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR11, val);\n\n\t \n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR3,\n\t\t\t   (u32)(priv->rx_ring.descs_dma));\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR4,\n\t\t\t   (u32)(priv->tx_ring.descs_dma));\n\n\t \n\tval = OWL_EMAC_VAL_MAC_CSR6_SPEED_100M << OWL_EMAC_OFF_MAC_CSR6_SPEED;\n\tval |= OWL_EMAC_BIT_MAC_CSR6_FD;\n\towl_emac_reg_update(priv, OWL_EMAC_REG_MAC_CSR6,\n\t\t\t    OWL_EMAC_MSK_MAC_CSR6_SPEED |\n\t\t\t    OWL_EMAC_BIT_MAC_CSR6_FD, val);\n\towl_emac_reg_clear(priv, OWL_EMAC_REG_MAC_CSR6,\n\t\t\t   OWL_EMAC_BIT_MAC_CSR6_PR | OWL_EMAC_BIT_MAC_CSR6_PM);\n\n\tpriv->link = 0;\n\tpriv->speed = SPEED_UNKNOWN;\n\tpriv->duplex = DUPLEX_UNKNOWN;\n\tpriv->pause = 0;\n\tpriv->mcaddr_list.count = 0;\n\n\treturn 0;\n}\n\nstatic int owl_emac_enable(struct net_device *netdev, bool start_phy)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\tint ret;\n\n\towl_emac_dma_cmd_stop(priv);\n\towl_emac_irq_disable(priv);\n\towl_emac_irq_clear(priv);\n\n\towl_emac_ring_prepare_tx(priv);\n\tret = owl_emac_ring_prepare_rx(priv);\n\tif (ret)\n\t\tgoto err_unprep;\n\n\tret = owl_emac_core_sw_reset(priv);\n\tif (ret) {\n\t\tnetdev_err(netdev, \"failed to soft reset MAC core: %d\\n\", ret);\n\t\tgoto err_unprep;\n\t}\n\n\towl_emac_set_hw_mac_addr(netdev);\n\towl_emac_setup_frame_xmit(priv);\n\n\tnetdev_reset_queue(netdev);\n\tnapi_enable(&priv->napi);\n\n\towl_emac_irq_enable(priv);\n\towl_emac_dma_cmd_start(priv);\n\n\tif (start_phy)\n\t\tphy_start(netdev->phydev);\n\n\tnetif_start_queue(netdev);\n\n\treturn 0;\n\nerr_unprep:\n\towl_emac_ring_unprepare_rx(priv);\n\towl_emac_ring_unprepare_tx(priv);\n\n\treturn ret;\n}\n\nstatic void owl_emac_disable(struct net_device *netdev, bool stop_phy)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\n\towl_emac_dma_cmd_stop(priv);\n\towl_emac_irq_disable(priv);\n\n\tnetif_stop_queue(netdev);\n\tnapi_disable(&priv->napi);\n\n\tif (stop_phy)\n\t\tphy_stop(netdev->phydev);\n\n\towl_emac_ring_unprepare_rx(priv);\n\towl_emac_ring_unprepare_tx(priv);\n}\n\nstatic int owl_emac_ndo_open(struct net_device *netdev)\n{\n\treturn owl_emac_enable(netdev, true);\n}\n\nstatic int owl_emac_ndo_stop(struct net_device *netdev)\n{\n\towl_emac_disable(netdev, true);\n\n\treturn 0;\n}\n\nstatic void owl_emac_set_multicast(struct net_device *netdev, int count)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\tstruct netdev_hw_addr *ha;\n\tint index = 0;\n\n\tif (count <= 0) {\n\t\tpriv->mcaddr_list.count = 0;\n\t\treturn;\n\t}\n\n\tnetdev_for_each_mc_addr(ha, netdev) {\n\t\tif (!is_multicast_ether_addr(ha->addr))\n\t\t\tcontinue;\n\n\t\tWARN_ON(index >= OWL_EMAC_MAX_MULTICAST_ADDRS);\n\t\tether_addr_copy(priv->mcaddr_list.addrs[index++], ha->addr);\n\t}\n\n\tpriv->mcaddr_list.count = index;\n\n\towl_emac_setup_frame_xmit(priv);\n}\n\nstatic void owl_emac_ndo_set_rx_mode(struct net_device *netdev)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\tu32 status, val = 0;\n\tint mcast_count = 0;\n\n\tif (netdev->flags & IFF_PROMISC) {\n\t\tval = OWL_EMAC_BIT_MAC_CSR6_PR;\n\t} else if (netdev->flags & IFF_ALLMULTI) {\n\t\tval = OWL_EMAC_BIT_MAC_CSR6_PM;\n\t} else if (netdev->flags & IFF_MULTICAST) {\n\t\tmcast_count = netdev_mc_count(netdev);\n\n\t\tif (mcast_count > OWL_EMAC_MAX_MULTICAST_ADDRS) {\n\t\t\tval = OWL_EMAC_BIT_MAC_CSR6_PM;\n\t\t\tmcast_count = 0;\n\t\t}\n\t}\n\n\tspin_lock_bh(&priv->lock);\n\n\t \n\tstatus = owl_emac_dma_cmd_stop(priv);\n\n\t \n\towl_emac_reg_update(priv, OWL_EMAC_REG_MAC_CSR6,\n\t\t\t    OWL_EMAC_BIT_MAC_CSR6_PR | OWL_EMAC_BIT_MAC_CSR6_PM,\n\t\t\t    val);\n\n\t \n\towl_emac_dma_cmd_set(priv, status);\n\n\tspin_unlock_bh(&priv->lock);\n\n\t \n\towl_emac_set_multicast(netdev, mcast_count);\n}\n\nstatic int owl_emac_ndo_set_mac_addr(struct net_device *netdev, void *addr)\n{\n\tstruct sockaddr *skaddr = addr;\n\n\tif (!is_valid_ether_addr(skaddr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\tif (netif_running(netdev))\n\t\treturn -EBUSY;\n\n\teth_hw_addr_set(netdev, skaddr->sa_data);\n\towl_emac_set_hw_mac_addr(netdev);\n\n\treturn owl_emac_setup_frame_xmit(netdev_priv(netdev));\n}\n\nstatic int owl_emac_ndo_eth_ioctl(struct net_device *netdev,\n\t\t\t\t  struct ifreq *req, int cmd)\n{\n\tif (!netif_running(netdev))\n\t\treturn -EINVAL;\n\n\treturn phy_mii_ioctl(netdev->phydev, req, cmd);\n}\n\nstatic void owl_emac_ndo_tx_timeout(struct net_device *netdev,\n\t\t\t\t    unsigned int txqueue)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\n\tschedule_work(&priv->mac_reset_task);\n}\n\nstatic void owl_emac_reset_task(struct work_struct *work)\n{\n\tstruct owl_emac_priv *priv;\n\n\tpriv = container_of(work, struct owl_emac_priv, mac_reset_task);\n\n\tnetdev_dbg(priv->netdev, \"resetting MAC\\n\");\n\towl_emac_disable(priv->netdev, false);\n\towl_emac_enable(priv->netdev, false);\n}\n\nstatic struct net_device_stats *\nowl_emac_ndo_get_stats(struct net_device *netdev)\n{\n\t \n\n\treturn &netdev->stats;\n}\n\nstatic const struct net_device_ops owl_emac_netdev_ops = {\n\t.ndo_open\t\t= owl_emac_ndo_open,\n\t.ndo_stop\t\t= owl_emac_ndo_stop,\n\t.ndo_start_xmit\t\t= owl_emac_ndo_start_xmit,\n\t.ndo_set_rx_mode\t= owl_emac_ndo_set_rx_mode,\n\t.ndo_set_mac_address\t= owl_emac_ndo_set_mac_addr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_eth_ioctl\t\t= owl_emac_ndo_eth_ioctl,\n\t.ndo_tx_timeout         = owl_emac_ndo_tx_timeout,\n\t.ndo_get_stats\t\t= owl_emac_ndo_get_stats,\n};\n\nstatic void owl_emac_ethtool_get_drvinfo(struct net_device *dev,\n\t\t\t\t\t struct ethtool_drvinfo *info)\n{\n\tstrscpy(info->driver, OWL_EMAC_DRVNAME, sizeof(info->driver));\n}\n\nstatic u32 owl_emac_ethtool_get_msglevel(struct net_device *netdev)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\n\treturn priv->msg_enable;\n}\n\nstatic void owl_emac_ethtool_set_msglevel(struct net_device *ndev, u32 val)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(ndev);\n\n\tpriv->msg_enable = val;\n}\n\nstatic const struct ethtool_ops owl_emac_ethtool_ops = {\n\t.get_drvinfo\t\t= owl_emac_ethtool_get_drvinfo,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_link_ksettings\t= phy_ethtool_get_link_ksettings,\n\t.set_link_ksettings\t= phy_ethtool_set_link_ksettings,\n\t.get_msglevel\t\t= owl_emac_ethtool_get_msglevel,\n\t.set_msglevel\t\t= owl_emac_ethtool_set_msglevel,\n};\n\nstatic int owl_emac_mdio_wait(struct owl_emac_priv *priv)\n{\n\tu32 val;\n\n\t \n\treturn readl_poll_timeout(priv->base + OWL_EMAC_REG_MAC_CSR10,\n\t\t\t\t  val, !(val & OWL_EMAC_BIT_MAC_CSR10_SB),\n\t\t\t\t  OWL_EMAC_POLL_DELAY_USEC,\n\t\t\t\t  OWL_EMAC_MDIO_POLL_TIMEOUT_USEC);\n}\n\nstatic int owl_emac_mdio_read(struct mii_bus *bus, int addr, int regnum)\n{\n\tstruct owl_emac_priv *priv = bus->priv;\n\tu32 data, tmp;\n\tint ret;\n\n\tdata = OWL_EMAC_BIT_MAC_CSR10_SB;\n\tdata |= OWL_EMAC_VAL_MAC_CSR10_OPCODE_RD << OWL_EMAC_OFF_MAC_CSR10_OPCODE;\n\n\ttmp = addr << OWL_EMAC_OFF_MAC_CSR10_PHYADD;\n\tdata |= tmp & OWL_EMAC_MSK_MAC_CSR10_PHYADD;\n\n\ttmp = regnum << OWL_EMAC_OFF_MAC_CSR10_REGADD;\n\tdata |= tmp & OWL_EMAC_MSK_MAC_CSR10_REGADD;\n\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR10, data);\n\n\tret = owl_emac_mdio_wait(priv);\n\tif (ret)\n\t\treturn ret;\n\n\tdata = owl_emac_reg_read(priv, OWL_EMAC_REG_MAC_CSR10);\n\tdata &= OWL_EMAC_MSK_MAC_CSR10_DATA;\n\n\treturn data;\n}\n\nstatic int\nowl_emac_mdio_write(struct mii_bus *bus, int addr, int regnum, u16 val)\n{\n\tstruct owl_emac_priv *priv = bus->priv;\n\tu32 data, tmp;\n\n\tdata = OWL_EMAC_BIT_MAC_CSR10_SB;\n\tdata |= OWL_EMAC_VAL_MAC_CSR10_OPCODE_WR << OWL_EMAC_OFF_MAC_CSR10_OPCODE;\n\n\ttmp = addr << OWL_EMAC_OFF_MAC_CSR10_PHYADD;\n\tdata |= tmp & OWL_EMAC_MSK_MAC_CSR10_PHYADD;\n\n\ttmp = regnum << OWL_EMAC_OFF_MAC_CSR10_REGADD;\n\tdata |= tmp & OWL_EMAC_MSK_MAC_CSR10_REGADD;\n\n\tdata |= val & OWL_EMAC_MSK_MAC_CSR10_DATA;\n\n\towl_emac_reg_write(priv, OWL_EMAC_REG_MAC_CSR10, data);\n\n\treturn owl_emac_mdio_wait(priv);\n}\n\nstatic int owl_emac_mdio_init(struct net_device *netdev)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\tstruct device *dev = owl_emac_get_dev(priv);\n\tstruct device_node *mdio_node;\n\tint ret;\n\n\tmdio_node = of_get_child_by_name(dev->of_node, \"mdio\");\n\tif (!mdio_node)\n\t\treturn -ENODEV;\n\n\tif (!of_device_is_available(mdio_node)) {\n\t\tret = -ENODEV;\n\t\tgoto err_put_node;\n\t}\n\n\tpriv->mii = devm_mdiobus_alloc(dev);\n\tif (!priv->mii) {\n\t\tret = -ENOMEM;\n\t\tgoto err_put_node;\n\t}\n\n\tsnprintf(priv->mii->id, MII_BUS_ID_SIZE, \"%s\", dev_name(dev));\n\tpriv->mii->name = \"owl-emac-mdio\";\n\tpriv->mii->parent = dev;\n\tpriv->mii->read = owl_emac_mdio_read;\n\tpriv->mii->write = owl_emac_mdio_write;\n\tpriv->mii->phy_mask = ~0;  \n\tpriv->mii->priv = priv;\n\n\tret = devm_of_mdiobus_register(dev, priv->mii, mdio_node);\n\nerr_put_node:\n\tof_node_put(mdio_node);\n\treturn ret;\n}\n\nstatic int owl_emac_phy_init(struct net_device *netdev)\n{\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\tstruct device *dev = owl_emac_get_dev(priv);\n\tstruct phy_device *phy;\n\n\tphy = of_phy_get_and_connect(netdev, dev->of_node,\n\t\t\t\t     owl_emac_adjust_link);\n\tif (!phy)\n\t\treturn -ENODEV;\n\n\tphy_set_sym_pause(phy, true, true, true);\n\n\tif (netif_msg_link(priv))\n\t\tphy_attached_info(phy);\n\n\treturn 0;\n}\n\nstatic void owl_emac_get_mac_addr(struct net_device *netdev)\n{\n\tstruct device *dev = netdev->dev.parent;\n\tint ret;\n\n\tret = platform_get_ethdev_address(dev, netdev);\n\tif (!ret && is_valid_ether_addr(netdev->dev_addr))\n\t\treturn;\n\n\teth_hw_addr_random(netdev);\n\tdev_warn(dev, \"using random MAC address %pM\\n\", netdev->dev_addr);\n}\n\nstatic __maybe_unused int owl_emac_suspend(struct device *dev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev);\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\n\tdisable_irq(netdev->irq);\n\n\tif (netif_running(netdev)) {\n\t\towl_emac_disable(netdev, true);\n\t\tnetif_device_detach(netdev);\n\t}\n\n\tclk_bulk_disable_unprepare(OWL_EMAC_NCLKS, priv->clks);\n\n\treturn 0;\n}\n\nstatic __maybe_unused int owl_emac_resume(struct device *dev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev);\n\tstruct owl_emac_priv *priv = netdev_priv(netdev);\n\tint ret;\n\n\tret = clk_bulk_prepare_enable(OWL_EMAC_NCLKS, priv->clks);\n\tif (ret)\n\t\treturn ret;\n\n\tif (netif_running(netdev)) {\n\t\towl_emac_core_hw_reset(priv);\n\t\towl_emac_core_sw_reset(priv);\n\n\t\tret = owl_emac_enable(netdev, true);\n\t\tif (ret) {\n\t\t\tclk_bulk_disable_unprepare(OWL_EMAC_NCLKS, priv->clks);\n\t\t\treturn ret;\n\t\t}\n\n\t\tnetif_device_attach(netdev);\n\t}\n\n\tenable_irq(netdev->irq);\n\n\treturn 0;\n}\n\nstatic void owl_emac_clk_disable_unprepare(void *data)\n{\n\tstruct owl_emac_priv *priv = data;\n\n\tclk_bulk_disable_unprepare(OWL_EMAC_NCLKS, priv->clks);\n}\n\nstatic int owl_emac_clk_set_rate(struct owl_emac_priv *priv)\n{\n\tstruct device *dev = owl_emac_get_dev(priv);\n\tunsigned long rate;\n\tint ret;\n\n\tswitch (priv->phy_mode) {\n\tcase PHY_INTERFACE_MODE_RMII:\n\t\trate = 50000000;\n\t\tbreak;\n\n\tcase PHY_INTERFACE_MODE_SMII:\n\t\trate = 125000000;\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(dev, \"unsupported phy interface mode %d\\n\",\n\t\t\tpriv->phy_mode);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tret = clk_set_rate(priv->clks[OWL_EMAC_CLK_RMII].clk, rate);\n\tif (ret)\n\t\tdev_err(dev, \"failed to set RMII clock rate: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int owl_emac_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct net_device *netdev;\n\tstruct owl_emac_priv *priv;\n\tint ret, i;\n\n\tnetdev = devm_alloc_etherdev(dev, sizeof(*priv));\n\tif (!netdev)\n\t\treturn -ENOMEM;\n\n\tplatform_set_drvdata(pdev, netdev);\n\tSET_NETDEV_DEV(netdev, dev);\n\n\tpriv = netdev_priv(netdev);\n\tpriv->netdev = netdev;\n\tpriv->msg_enable = netif_msg_init(-1, OWL_EMAC_DEFAULT_MSG_ENABLE);\n\n\tret = of_get_phy_mode(dev->of_node, &priv->phy_mode);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to get phy mode: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tspin_lock_init(&priv->lock);\n\n\tret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32));\n\tif (ret) {\n\t\tdev_err(dev, \"unsupported DMA mask\\n\");\n\t\treturn ret;\n\t}\n\n\tret = owl_emac_ring_alloc(dev, &priv->rx_ring, OWL_EMAC_RX_RING_SIZE);\n\tif (ret)\n\t\treturn ret;\n\n\tret = owl_emac_ring_alloc(dev, &priv->tx_ring, OWL_EMAC_TX_RING_SIZE);\n\tif (ret)\n\t\treturn ret;\n\n\tpriv->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(priv->base))\n\t\treturn PTR_ERR(priv->base);\n\n\tnetdev->irq = platform_get_irq(pdev, 0);\n\tif (netdev->irq < 0)\n\t\treturn netdev->irq;\n\n\tret = devm_request_irq(dev, netdev->irq, owl_emac_handle_irq,\n\t\t\t       IRQF_SHARED, netdev->name, netdev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to request irq: %d\\n\", netdev->irq);\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < OWL_EMAC_NCLKS; i++)\n\t\tpriv->clks[i].id = owl_emac_clk_names[i];\n\n\tret = devm_clk_bulk_get(dev, OWL_EMAC_NCLKS, priv->clks);\n\tif (ret)\n\t\treturn ret;\n\n\tret = clk_bulk_prepare_enable(OWL_EMAC_NCLKS, priv->clks);\n\tif (ret)\n\t\treturn ret;\n\n\tret = devm_add_action_or_reset(dev, owl_emac_clk_disable_unprepare, priv);\n\tif (ret)\n\t\treturn ret;\n\n\tret = owl_emac_clk_set_rate(priv);\n\tif (ret)\n\t\treturn ret;\n\n\tpriv->reset = devm_reset_control_get_exclusive(dev, NULL);\n\tif (IS_ERR(priv->reset))\n\t\treturn dev_err_probe(dev, PTR_ERR(priv->reset),\n\t\t\t\t     \"failed to get reset control\");\n\n\towl_emac_get_mac_addr(netdev);\n\n\towl_emac_core_hw_reset(priv);\n\towl_emac_mdio_clock_enable(priv);\n\n\tret = owl_emac_mdio_init(netdev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to initialize MDIO bus\\n\");\n\t\treturn ret;\n\t}\n\n\tret = owl_emac_phy_init(netdev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to initialize PHY\\n\");\n\t\treturn ret;\n\t}\n\n\tINIT_WORK(&priv->mac_reset_task, owl_emac_reset_task);\n\n\tnetdev->min_mtu = OWL_EMAC_MTU_MIN;\n\tnetdev->max_mtu = OWL_EMAC_MTU_MAX;\n\tnetdev->watchdog_timeo = OWL_EMAC_TX_TIMEOUT;\n\tnetdev->netdev_ops = &owl_emac_netdev_ops;\n\tnetdev->ethtool_ops = &owl_emac_ethtool_ops;\n\tnetif_napi_add(netdev, &priv->napi, owl_emac_poll);\n\n\tret = devm_register_netdev(dev, netdev);\n\tif (ret) {\n\t\tnetif_napi_del(&priv->napi);\n\t\tphy_disconnect(netdev->phydev);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int owl_emac_remove(struct platform_device *pdev)\n{\n\tstruct owl_emac_priv *priv = platform_get_drvdata(pdev);\n\n\tnetif_napi_del(&priv->napi);\n\tphy_disconnect(priv->netdev->phydev);\n\tcancel_work_sync(&priv->mac_reset_task);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id owl_emac_of_match[] = {\n\t{ .compatible = \"actions,owl-emac\", },\n\t{ }\n};\nMODULE_DEVICE_TABLE(of, owl_emac_of_match);\n\nstatic SIMPLE_DEV_PM_OPS(owl_emac_pm_ops,\n\t\t\t owl_emac_suspend, owl_emac_resume);\n\nstatic struct platform_driver owl_emac_driver = {\n\t.driver = {\n\t\t.name = OWL_EMAC_DRVNAME,\n\t\t.of_match_table = owl_emac_of_match,\n\t\t.pm = &owl_emac_pm_ops,\n\t},\n\t.probe = owl_emac_probe,\n\t.remove = owl_emac_remove,\n};\nmodule_platform_driver(owl_emac_driver);\n\nMODULE_DESCRIPTION(\"Actions Semi Owl SoCs Ethernet MAC Driver\");\nMODULE_AUTHOR(\"Actions Semi Inc.\");\nMODULE_AUTHOR(\"Cristian Ciocaltea <cristian.ciocaltea@gmail.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}