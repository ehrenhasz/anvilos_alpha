{
  "module_name": "sxgbe_dma.c",
  "hash_id": "485f1ad8c1b339747932e825cd4f29f5856bdf79904da92c753419e6baa51502",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/samsung/sxgbe/sxgbe_dma.c",
  "human_readable_source": "\n \n#include <linux/delay.h>\n#include <linux/export.h>\n#include <linux/io.h>\n#include <linux/netdevice.h>\n#include <linux/phy.h>\n\n#include \"sxgbe_common.h\"\n#include \"sxgbe_dma.h\"\n#include \"sxgbe_reg.h\"\n#include \"sxgbe_desc.h\"\n\n \nstatic int sxgbe_dma_init(void __iomem *ioaddr, int fix_burst, int burst_map)\n{\n\tu32 reg_val;\n\n\treg_val = readl(ioaddr + SXGBE_DMA_SYSBUS_MODE_REG);\n\n\t \n\tif (!fix_burst)\n\t\treg_val |= SXGBE_DMA_AXI_UNDEF_BURST;\n\n\t \n\treg_val |= (burst_map << SXGBE_DMA_BLENMAP_LSHIFT);\n\n\twritel(reg_val,\tioaddr + SXGBE_DMA_SYSBUS_MODE_REG);\n\n\treturn 0;\n}\n\nstatic void sxgbe_dma_channel_init(void __iomem *ioaddr, int cha_num,\n\t\t\t\t   int fix_burst, int pbl, dma_addr_t dma_tx,\n\t\t\t\t   dma_addr_t dma_rx, int t_rsize, int r_rsize)\n{\n\tu32 reg_val;\n\tdma_addr_t dma_addr;\n\n\treg_val = readl(ioaddr + SXGBE_DMA_CHA_CTL_REG(cha_num));\n\t \n\tif (fix_burst) {\n\t\treg_val |= SXGBE_DMA_PBL_X8MODE;\n\t\twritel(reg_val, ioaddr + SXGBE_DMA_CHA_CTL_REG(cha_num));\n\t\t \n\t\treg_val = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cha_num));\n\t\treg_val |= (pbl << SXGBE_DMA_TXPBL_LSHIFT);\n\t\twritel(reg_val, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cha_num));\n\t\t \n\t\treg_val = readl(ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cha_num));\n\t\treg_val |= (pbl << SXGBE_DMA_RXPBL_LSHIFT);\n\t\twritel(reg_val, ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cha_num));\n\t}\n\n\t \n\twritel(upper_32_bits(dma_tx),\n\t       ioaddr + SXGBE_DMA_CHA_TXDESC_HADD_REG(cha_num));\n\twritel(lower_32_bits(dma_tx),\n\t       ioaddr + SXGBE_DMA_CHA_TXDESC_LADD_REG(cha_num));\n\n\twritel(upper_32_bits(dma_rx),\n\t       ioaddr + SXGBE_DMA_CHA_RXDESC_HADD_REG(cha_num));\n\twritel(lower_32_bits(dma_rx),\n\t       ioaddr + SXGBE_DMA_CHA_RXDESC_LADD_REG(cha_num));\n\n\t \n\t \n\tdma_addr = dma_tx + ((t_rsize - 1) * SXGBE_DESC_SIZE_BYTES);\n\twritel(lower_32_bits(dma_addr),\n\t       ioaddr + SXGBE_DMA_CHA_TXDESC_TAILPTR_REG(cha_num));\n\n\tdma_addr = dma_rx + ((r_rsize - 1) * SXGBE_DESC_SIZE_BYTES);\n\twritel(lower_32_bits(dma_addr),\n\t       ioaddr + SXGBE_DMA_CHA_RXDESC_LADD_REG(cha_num));\n\t \n\twritel(t_rsize - 1, ioaddr + SXGBE_DMA_CHA_TXDESC_RINGLEN_REG(cha_num));\n\twritel(r_rsize - 1, ioaddr + SXGBE_DMA_CHA_RXDESC_RINGLEN_REG(cha_num));\n\n\t \n\twritel(SXGBE_DMA_ENA_INT,\n\t       ioaddr + SXGBE_DMA_CHA_INT_ENABLE_REG(cha_num));\n}\n\nstatic void sxgbe_enable_dma_transmission(void __iomem *ioaddr, int cha_num)\n{\n\tu32 tx_config;\n\n\ttx_config = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cha_num));\n\ttx_config |= SXGBE_TX_START_DMA;\n\twritel(tx_config, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cha_num));\n}\n\nstatic void sxgbe_enable_dma_irq(void __iomem *ioaddr, int dma_cnum)\n{\n\t \n\twritel(SXGBE_DMA_ENA_INT,\n\t       ioaddr + SXGBE_DMA_CHA_INT_ENABLE_REG(dma_cnum));\n}\n\nstatic void sxgbe_disable_dma_irq(void __iomem *ioaddr, int dma_cnum)\n{\n\t \n\twritel(0, ioaddr + SXGBE_DMA_CHA_INT_ENABLE_REG(dma_cnum));\n}\n\nstatic void sxgbe_dma_start_tx(void __iomem *ioaddr, int tchannels)\n{\n\tint cnum;\n\tu32 tx_ctl_reg;\n\n\tfor (cnum = 0; cnum < tchannels; cnum++) {\n\t\ttx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cnum));\n\t\ttx_ctl_reg |= SXGBE_TX_ENABLE;\n\t\twritel(tx_ctl_reg,\n\t\t       ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cnum));\n\t}\n}\n\nstatic void sxgbe_dma_start_tx_queue(void __iomem *ioaddr, int dma_cnum)\n{\n\tu32 tx_ctl_reg;\n\n\ttx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(dma_cnum));\n\ttx_ctl_reg |= SXGBE_TX_ENABLE;\n\twritel(tx_ctl_reg, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(dma_cnum));\n}\n\nstatic void sxgbe_dma_stop_tx_queue(void __iomem *ioaddr, int dma_cnum)\n{\n\tu32 tx_ctl_reg;\n\n\ttx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(dma_cnum));\n\ttx_ctl_reg &= ~(SXGBE_TX_ENABLE);\n\twritel(tx_ctl_reg, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(dma_cnum));\n}\n\nstatic void sxgbe_dma_stop_tx(void __iomem *ioaddr, int tchannels)\n{\n\tint cnum;\n\tu32 tx_ctl_reg;\n\n\tfor (cnum = 0; cnum < tchannels; cnum++) {\n\t\ttx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cnum));\n\t\ttx_ctl_reg &= ~(SXGBE_TX_ENABLE);\n\t\twritel(tx_ctl_reg, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(cnum));\n\t}\n}\n\nstatic void sxgbe_dma_start_rx(void __iomem *ioaddr, int rchannels)\n{\n\tint cnum;\n\tu32 rx_ctl_reg;\n\n\tfor (cnum = 0; cnum < rchannels; cnum++) {\n\t\trx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cnum));\n\t\trx_ctl_reg |= SXGBE_RX_ENABLE;\n\t\twritel(rx_ctl_reg,\n\t\t       ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cnum));\n\t}\n}\n\nstatic void sxgbe_dma_stop_rx(void __iomem *ioaddr, int rchannels)\n{\n\tint cnum;\n\tu32 rx_ctl_reg;\n\n\tfor (cnum = 0; cnum < rchannels; cnum++) {\n\t\trx_ctl_reg = readl(ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cnum));\n\t\trx_ctl_reg &= ~(SXGBE_RX_ENABLE);\n\t\twritel(rx_ctl_reg, ioaddr + SXGBE_DMA_CHA_RXCTL_REG(cnum));\n\t}\n}\n\nstatic int sxgbe_tx_dma_int_status(void __iomem *ioaddr, int channel_no,\n\t\t\t\t   struct sxgbe_extra_stats *x)\n{\n\tu32 int_status = readl(ioaddr + SXGBE_DMA_CHA_STATUS_REG(channel_no));\n\tu32 clear_val = 0;\n\tu32 ret_val = 0;\n\n\t \n\tif (likely(int_status & SXGBE_DMA_INT_STATUS_NIS)) {\n\t\tx->normal_irq_n++;\n\t\tif (int_status & SXGBE_DMA_INT_STATUS_TI) {\n\t\t\tret_val |= handle_tx;\n\t\t\tx->tx_normal_irq_n++;\n\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_TI;\n\t\t}\n\n\t\tif (int_status & SXGBE_DMA_INT_STATUS_TBU) {\n\t\t\tx->tx_underflow_irq++;\n\t\t\tret_val |= tx_bump_tc;\n\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_TBU;\n\t\t}\n\t} else if (unlikely(int_status & SXGBE_DMA_INT_STATUS_AIS)) {\n\t\t \n\t\tif (int_status & SXGBE_DMA_INT_STATUS_TPS) {\n\t\t\tret_val |= tx_hard_error;\n\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_TPS;\n\t\t\tx->tx_process_stopped_irq++;\n\t\t}\n\n\t\tif (int_status & SXGBE_DMA_INT_STATUS_FBE) {\n\t\t\tret_val |= tx_hard_error;\n\t\t\tx->fatal_bus_error_irq++;\n\n\t\t\t \n\n\t\t\t \n\t\t\tif (int_status & SXGBE_DMA_INT_STATUS_TEB0) {\n\t\t\t\tx->tx_read_transfer_err++;\n\t\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_TEB0;\n\t\t\t} else {\n\t\t\t\tx->tx_write_transfer_err++;\n\t\t\t}\n\n\t\t\tif (int_status & SXGBE_DMA_INT_STATUS_TEB1) {\n\t\t\t\tx->tx_desc_access_err++;\n\t\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_TEB1;\n\t\t\t} else {\n\t\t\t\tx->tx_buffer_access_err++;\n\t\t\t}\n\n\t\t\tif (int_status & SXGBE_DMA_INT_STATUS_TEB2) {\n\t\t\t\tx->tx_data_transfer_err++;\n\t\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_TEB2;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (int_status & SXGBE_DMA_INT_STATUS_CTXTERR) {\n\t\t\tx->tx_ctxt_desc_err++;\n\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_CTXTERR;\n\t\t}\n\t}\n\n\t \n\twritel(clear_val, ioaddr + SXGBE_DMA_CHA_STATUS_REG(channel_no));\n\n\treturn ret_val;\n}\n\nstatic int sxgbe_rx_dma_int_status(void __iomem *ioaddr, int channel_no,\n\t\t\t\t   struct sxgbe_extra_stats *x)\n{\n\tu32 int_status = readl(ioaddr + SXGBE_DMA_CHA_STATUS_REG(channel_no));\n\tu32 clear_val = 0;\n\tu32 ret_val = 0;\n\n\t \n\tif (likely(int_status & SXGBE_DMA_INT_STATUS_NIS)) {\n\t\tx->normal_irq_n++;\n\t\tif (int_status & SXGBE_DMA_INT_STATUS_RI) {\n\t\t\tret_val |= handle_rx;\n\t\t\tx->rx_normal_irq_n++;\n\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_RI;\n\t\t}\n\t} else if (unlikely(int_status & SXGBE_DMA_INT_STATUS_AIS)) {\n\t\t \n\t\tif (int_status & SXGBE_DMA_INT_STATUS_RBU) {\n\t\t\tret_val |= rx_bump_tc;\n\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_RBU;\n\t\t\tx->rx_underflow_irq++;\n\t\t}\n\n\t\tif (int_status & SXGBE_DMA_INT_STATUS_RPS) {\n\t\t\tret_val |= rx_hard_error;\n\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_RPS;\n\t\t\tx->rx_process_stopped_irq++;\n\t\t}\n\n\t\tif (int_status & SXGBE_DMA_INT_STATUS_FBE) {\n\t\t\tret_val |= rx_hard_error;\n\t\t\tx->fatal_bus_error_irq++;\n\n\t\t\t \n\n\t\t\t \n\t\t\tif (int_status & SXGBE_DMA_INT_STATUS_REB0) {\n\t\t\t\tx->rx_read_transfer_err++;\n\t\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_REB0;\n\t\t\t} else {\n\t\t\t\tx->rx_write_transfer_err++;\n\t\t\t}\n\n\t\t\tif (int_status & SXGBE_DMA_INT_STATUS_REB1) {\n\t\t\t\tx->rx_desc_access_err++;\n\t\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_REB1;\n\t\t\t} else {\n\t\t\t\tx->rx_buffer_access_err++;\n\t\t\t}\n\n\t\t\tif (int_status & SXGBE_DMA_INT_STATUS_REB2) {\n\t\t\t\tx->rx_data_transfer_err++;\n\t\t\t\tclear_val |= SXGBE_DMA_INT_STATUS_REB2;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\twritel(clear_val, ioaddr + SXGBE_DMA_CHA_STATUS_REG(channel_no));\n\n\treturn ret_val;\n}\n\n \nstatic void sxgbe_dma_rx_watchdog(void __iomem *ioaddr, u32 riwt)\n{\n\tu32 que_num;\n\n\tSXGBE_FOR_EACH_QUEUE(SXGBE_RX_QUEUES, que_num) {\n\t\twritel(riwt,\n\t\t       ioaddr + SXGBE_DMA_CHA_INT_RXWATCHTMR_REG(que_num));\n\t}\n}\n\nstatic void sxgbe_enable_tso(void __iomem *ioaddr, u8 chan_num)\n{\n\tu32 ctrl;\n\n\tctrl = readl(ioaddr + SXGBE_DMA_CHA_TXCTL_REG(chan_num));\n\tctrl |= SXGBE_DMA_CHA_TXCTL_TSE_ENABLE;\n\twritel(ctrl, ioaddr + SXGBE_DMA_CHA_TXCTL_REG(chan_num));\n}\n\nstatic const struct sxgbe_dma_ops sxgbe_dma_ops = {\n\t.init\t\t\t\t= sxgbe_dma_init,\n\t.cha_init\t\t\t= sxgbe_dma_channel_init,\n\t.enable_dma_transmission\t= sxgbe_enable_dma_transmission,\n\t.enable_dma_irq\t\t\t= sxgbe_enable_dma_irq,\n\t.disable_dma_irq\t\t= sxgbe_disable_dma_irq,\n\t.start_tx\t\t\t= sxgbe_dma_start_tx,\n\t.start_tx_queue\t\t\t= sxgbe_dma_start_tx_queue,\n\t.stop_tx\t\t\t= sxgbe_dma_stop_tx,\n\t.stop_tx_queue\t\t\t= sxgbe_dma_stop_tx_queue,\n\t.start_rx\t\t\t= sxgbe_dma_start_rx,\n\t.stop_rx\t\t\t= sxgbe_dma_stop_rx,\n\t.tx_dma_int_status\t\t= sxgbe_tx_dma_int_status,\n\t.rx_dma_int_status\t\t= sxgbe_rx_dma_int_status,\n\t.rx_watchdog\t\t\t= sxgbe_dma_rx_watchdog,\n\t.enable_tso\t\t\t= sxgbe_enable_tso,\n};\n\nconst struct sxgbe_dma_ops *sxgbe_get_dma_ops(void)\n{\n\treturn &sxgbe_dma_ops;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}