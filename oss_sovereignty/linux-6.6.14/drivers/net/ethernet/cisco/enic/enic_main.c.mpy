{
  "module_name": "enic_main.c",
  "hash_id": "ea4f855dfec98f42cacfd7917475f378437823a5b89492439432f5c31be5d8dd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/cisco/enic/enic_main.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/workqueue.h>\n#include <linux/pci.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/if.h>\n#include <linux/if_ether.h>\n#include <linux/if_vlan.h>\n#include <linux/in.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/tcp.h>\n#include <linux/rtnetlink.h>\n#include <linux/prefetch.h>\n#include <net/ip6_checksum.h>\n#include <linux/ktime.h>\n#include <linux/numa.h>\n#ifdef CONFIG_RFS_ACCEL\n#include <linux/cpu_rmap.h>\n#endif\n#include <linux/crash_dump.h>\n#include <net/busy_poll.h>\n#include <net/vxlan.h>\n\n#include \"cq_enet_desc.h\"\n#include \"vnic_dev.h\"\n#include \"vnic_intr.h\"\n#include \"vnic_stats.h\"\n#include \"vnic_vic.h\"\n#include \"enic_res.h\"\n#include \"enic.h\"\n#include \"enic_dev.h\"\n#include \"enic_pp.h\"\n#include \"enic_clsf.h\"\n\n#define ENIC_NOTIFY_TIMER_PERIOD\t(2 * HZ)\n#define WQ_ENET_MAX_DESC_LEN\t\t(1 << WQ_ENET_LEN_BITS)\n#define MAX_TSO\t\t\t\t(1 << 16)\n#define ENIC_DESC_MAX_SPLITS\t\t(MAX_TSO / WQ_ENET_MAX_DESC_LEN + 1)\n\n#define PCI_DEVICE_ID_CISCO_VIC_ENET         0x0043   \n#define PCI_DEVICE_ID_CISCO_VIC_ENET_DYN     0x0044   \n#define PCI_DEVICE_ID_CISCO_VIC_ENET_VF      0x0071   \n\n#define RX_COPYBREAK_DEFAULT\t\t256\n\n \nstatic const struct pci_device_id enic_id_table[] = {\n\t{ PCI_VDEVICE(CISCO, PCI_DEVICE_ID_CISCO_VIC_ENET) },\n\t{ PCI_VDEVICE(CISCO, PCI_DEVICE_ID_CISCO_VIC_ENET_DYN) },\n\t{ PCI_VDEVICE(CISCO, PCI_DEVICE_ID_CISCO_VIC_ENET_VF) },\n\t{ 0, }\t \n};\n\nMODULE_DESCRIPTION(DRV_DESCRIPTION);\nMODULE_AUTHOR(\"Scott Feldman <scofeldm@cisco.com>\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DEVICE_TABLE(pci, enic_id_table);\n\n#define ENIC_LARGE_PKT_THRESHOLD\t\t1000\n#define ENIC_MAX_COALESCE_TIMERS\t\t10\n \nstatic struct enic_intr_mod_table mod_table[ENIC_MAX_COALESCE_TIMERS + 1] = {\n\t{4000,  0},\n\t{4400, 10},\n\t{5060, 20},\n\t{5230, 30},\n\t{5540, 40},\n\t{5820, 50},\n\t{6120, 60},\n\t{6435, 70},\n\t{6745, 80},\n\t{7000, 90},\n\t{0xFFFFFFFF, 100}\n};\n\n \nstatic struct enic_intr_mod_range mod_range[ENIC_MAX_LINK_SPEEDS] = {\n\t{0,  0},  \n\t{0,  3},  \n\t{3,  6},  \n};\n\nstatic void enic_init_affinity_hint(struct enic *enic)\n{\n\tint numa_node = dev_to_node(&enic->pdev->dev);\n\tint i;\n\n\tfor (i = 0; i < enic->intr_count; i++) {\n\t\tif (enic_is_err_intr(enic, i) || enic_is_notify_intr(enic, i) ||\n\t\t    (cpumask_available(enic->msix[i].affinity_mask) &&\n\t\t     !cpumask_empty(enic->msix[i].affinity_mask)))\n\t\t\tcontinue;\n\t\tif (zalloc_cpumask_var(&enic->msix[i].affinity_mask,\n\t\t\t\t       GFP_KERNEL))\n\t\t\tcpumask_set_cpu(cpumask_local_spread(i, numa_node),\n\t\t\t\t\tenic->msix[i].affinity_mask);\n\t}\n}\n\nstatic void enic_free_affinity_hint(struct enic *enic)\n{\n\tint i;\n\n\tfor (i = 0; i < enic->intr_count; i++) {\n\t\tif (enic_is_err_intr(enic, i) || enic_is_notify_intr(enic, i))\n\t\t\tcontinue;\n\t\tfree_cpumask_var(enic->msix[i].affinity_mask);\n\t}\n}\n\nstatic void enic_set_affinity_hint(struct enic *enic)\n{\n\tint i;\n\tint err;\n\n\tfor (i = 0; i < enic->intr_count; i++) {\n\t\tif (enic_is_err_intr(enic, i)\t\t||\n\t\t    enic_is_notify_intr(enic, i)\t||\n\t\t    !cpumask_available(enic->msix[i].affinity_mask) ||\n\t\t    cpumask_empty(enic->msix[i].affinity_mask))\n\t\t\tcontinue;\n\t\terr = irq_update_affinity_hint(enic->msix_entry[i].vector,\n\t\t\t\t\t       enic->msix[i].affinity_mask);\n\t\tif (err)\n\t\t\tnetdev_warn(enic->netdev, \"irq_update_affinity_hint failed, err %d\\n\",\n\t\t\t\t    err);\n\t}\n\n\tfor (i = 0; i < enic->wq_count; i++) {\n\t\tint wq_intr = enic_msix_wq_intr(enic, i);\n\n\t\tif (cpumask_available(enic->msix[wq_intr].affinity_mask) &&\n\t\t    !cpumask_empty(enic->msix[wq_intr].affinity_mask))\n\t\t\tnetif_set_xps_queue(enic->netdev,\n\t\t\t\t\t    enic->msix[wq_intr].affinity_mask,\n\t\t\t\t\t    i);\n\t}\n}\n\nstatic void enic_unset_affinity_hint(struct enic *enic)\n{\n\tint i;\n\n\tfor (i = 0; i < enic->intr_count; i++)\n\t\tirq_update_affinity_hint(enic->msix_entry[i].vector, NULL);\n}\n\nstatic int enic_udp_tunnel_set_port(struct net_device *netdev,\n\t\t\t\t    unsigned int table, unsigned int entry,\n\t\t\t\t    struct udp_tunnel_info *ti)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tint err;\n\n\tspin_lock_bh(&enic->devcmd_lock);\n\n\terr = vnic_dev_overlay_offload_cfg(enic->vdev,\n\t\t\t\t\t   OVERLAY_CFG_VXLAN_PORT_UPDATE,\n\t\t\t\t\t   ntohs(ti->port));\n\tif (err)\n\t\tgoto error;\n\n\terr = vnic_dev_overlay_offload_ctrl(enic->vdev, OVERLAY_FEATURE_VXLAN,\n\t\t\t\t\t    enic->vxlan.patch_level);\n\tif (err)\n\t\tgoto error;\n\n\tenic->vxlan.vxlan_udp_port_number = ntohs(ti->port);\nerror:\n\tspin_unlock_bh(&enic->devcmd_lock);\n\n\treturn err;\n}\n\nstatic int enic_udp_tunnel_unset_port(struct net_device *netdev,\n\t\t\t\t      unsigned int table, unsigned int entry,\n\t\t\t\t      struct udp_tunnel_info *ti)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tint err;\n\n\tspin_lock_bh(&enic->devcmd_lock);\n\n\terr = vnic_dev_overlay_offload_ctrl(enic->vdev, OVERLAY_FEATURE_VXLAN,\n\t\t\t\t\t    OVERLAY_OFFLOAD_DISABLE);\n\tif (err)\n\t\tgoto unlock;\n\n\tenic->vxlan.vxlan_udp_port_number = 0;\n\nunlock:\n\tspin_unlock_bh(&enic->devcmd_lock);\n\n\treturn err;\n}\n\nstatic const struct udp_tunnel_nic_info enic_udp_tunnels = {\n\t.set_port\t= enic_udp_tunnel_set_port,\n\t.unset_port\t= enic_udp_tunnel_unset_port,\n\t.tables\t\t= {\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_VXLAN, },\n\t},\n}, enic_udp_tunnels_v4 = {\n\t.set_port\t= enic_udp_tunnel_set_port,\n\t.unset_port\t= enic_udp_tunnel_unset_port,\n\t.flags\t\t= UDP_TUNNEL_NIC_INFO_IPV4_ONLY,\n\t.tables\t\t= {\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_VXLAN, },\n\t},\n};\n\nstatic netdev_features_t enic_features_check(struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\tconst struct ethhdr *eth = (struct ethhdr *)skb_inner_mac_header(skb);\n\tstruct enic *enic = netdev_priv(dev);\n\tstruct udphdr *udph;\n\tu16 port = 0;\n\tu8 proto;\n\n\tif (!skb->encapsulation)\n\t\treturn features;\n\n\tfeatures = vxlan_features_check(skb, features);\n\n\tswitch (vlan_get_protocol(skb)) {\n\tcase htons(ETH_P_IPV6):\n\t\tif (!(enic->vxlan.flags & ENIC_VXLAN_OUTER_IPV6))\n\t\t\tgoto out;\n\t\tproto = ipv6_hdr(skb)->nexthdr;\n\t\tbreak;\n\tcase htons(ETH_P_IP):\n\t\tproto = ip_hdr(skb)->protocol;\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tswitch (eth->h_proto) {\n\tcase ntohs(ETH_P_IPV6):\n\t\tif (!(enic->vxlan.flags & ENIC_VXLAN_INNER_IPV6))\n\t\t\tgoto out;\n\t\tfallthrough;\n\tcase ntohs(ETH_P_IP):\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\n\tif (proto == IPPROTO_UDP) {\n\t\tudph = udp_hdr(skb);\n\t\tport = be16_to_cpu(udph->dest);\n\t}\n\n\t \n\tif (port  != enic->vxlan.vxlan_udp_port_number)\n\t\tgoto out;\n\n\treturn features;\n\nout:\n\treturn features & ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n}\n\nint enic_is_dynamic(struct enic *enic)\n{\n\treturn enic->pdev->device == PCI_DEVICE_ID_CISCO_VIC_ENET_DYN;\n}\n\nint enic_sriov_enabled(struct enic *enic)\n{\n\treturn (enic->priv_flags & ENIC_SRIOV_ENABLED) ? 1 : 0;\n}\n\nstatic int enic_is_sriov_vf(struct enic *enic)\n{\n\treturn enic->pdev->device == PCI_DEVICE_ID_CISCO_VIC_ENET_VF;\n}\n\nint enic_is_valid_vf(struct enic *enic, int vf)\n{\n#ifdef CONFIG_PCI_IOV\n\treturn vf >= 0 && vf < enic->num_vfs;\n#else\n\treturn 0;\n#endif\n}\n\nstatic void enic_free_wq_buf(struct vnic_wq *wq, struct vnic_wq_buf *buf)\n{\n\tstruct enic *enic = vnic_dev_priv(wq->vdev);\n\n\tif (buf->sop)\n\t\tdma_unmap_single(&enic->pdev->dev, buf->dma_addr, buf->len,\n\t\t\t\t DMA_TO_DEVICE);\n\telse\n\t\tdma_unmap_page(&enic->pdev->dev, buf->dma_addr, buf->len,\n\t\t\t       DMA_TO_DEVICE);\n\n\tif (buf->os_buf)\n\t\tdev_kfree_skb_any(buf->os_buf);\n}\n\nstatic void enic_wq_free_buf(struct vnic_wq *wq,\n\tstruct cq_desc *cq_desc, struct vnic_wq_buf *buf, void *opaque)\n{\n\tenic_free_wq_buf(wq, buf);\n}\n\nstatic int enic_wq_service(struct vnic_dev *vdev, struct cq_desc *cq_desc,\n\tu8 type, u16 q_number, u16 completed_index, void *opaque)\n{\n\tstruct enic *enic = vnic_dev_priv(vdev);\n\n\tspin_lock(&enic->wq_lock[q_number]);\n\n\tvnic_wq_service(&enic->wq[q_number], cq_desc,\n\t\tcompleted_index, enic_wq_free_buf,\n\t\topaque);\n\n\tif (netif_tx_queue_stopped(netdev_get_tx_queue(enic->netdev, q_number)) &&\n\t    vnic_wq_desc_avail(&enic->wq[q_number]) >=\n\t    (MAX_SKB_FRAGS + ENIC_DESC_MAX_SPLITS))\n\t\tnetif_wake_subqueue(enic->netdev, q_number);\n\n\tspin_unlock(&enic->wq_lock[q_number]);\n\n\treturn 0;\n}\n\nstatic bool enic_log_q_error(struct enic *enic)\n{\n\tunsigned int i;\n\tu32 error_status;\n\tbool err = false;\n\n\tfor (i = 0; i < enic->wq_count; i++) {\n\t\terror_status = vnic_wq_error_status(&enic->wq[i]);\n\t\terr |= error_status;\n\t\tif (error_status)\n\t\t\tnetdev_err(enic->netdev, \"WQ[%d] error_status %d\\n\",\n\t\t\t\ti, error_status);\n\t}\n\n\tfor (i = 0; i < enic->rq_count; i++) {\n\t\terror_status = vnic_rq_error_status(&enic->rq[i]);\n\t\terr |= error_status;\n\t\tif (error_status)\n\t\t\tnetdev_err(enic->netdev, \"RQ[%d] error_status %d\\n\",\n\t\t\t\ti, error_status);\n\t}\n\n\treturn err;\n}\n\nstatic void enic_msglvl_check(struct enic *enic)\n{\n\tu32 msg_enable = vnic_dev_msg_lvl(enic->vdev);\n\n\tif (msg_enable != enic->msg_enable) {\n\t\tnetdev_info(enic->netdev, \"msg lvl changed from 0x%x to 0x%x\\n\",\n\t\t\tenic->msg_enable, msg_enable);\n\t\tenic->msg_enable = msg_enable;\n\t}\n}\n\nstatic void enic_mtu_check(struct enic *enic)\n{\n\tu32 mtu = vnic_dev_mtu(enic->vdev);\n\tstruct net_device *netdev = enic->netdev;\n\n\tif (mtu && mtu != enic->port_mtu) {\n\t\tenic->port_mtu = mtu;\n\t\tif (enic_is_dynamic(enic) || enic_is_sriov_vf(enic)) {\n\t\t\tmtu = max_t(int, ENIC_MIN_MTU,\n\t\t\t\tmin_t(int, ENIC_MAX_MTU, mtu));\n\t\t\tif (mtu != netdev->mtu)\n\t\t\t\tschedule_work(&enic->change_mtu_work);\n\t\t} else {\n\t\t\tif (mtu < netdev->mtu)\n\t\t\t\tnetdev_warn(netdev,\n\t\t\t\t\t\"interface MTU (%d) set higher \"\n\t\t\t\t\t\"than switch port MTU (%d)\\n\",\n\t\t\t\t\tnetdev->mtu, mtu);\n\t\t}\n\t}\n}\n\nstatic void enic_link_check(struct enic *enic)\n{\n\tint link_status = vnic_dev_link_status(enic->vdev);\n\tint carrier_ok = netif_carrier_ok(enic->netdev);\n\n\tif (link_status && !carrier_ok) {\n\t\tnetdev_info(enic->netdev, \"Link UP\\n\");\n\t\tnetif_carrier_on(enic->netdev);\n\t} else if (!link_status && carrier_ok) {\n\t\tnetdev_info(enic->netdev, \"Link DOWN\\n\");\n\t\tnetif_carrier_off(enic->netdev);\n\t}\n}\n\nstatic void enic_notify_check(struct enic *enic)\n{\n\tenic_msglvl_check(enic);\n\tenic_mtu_check(enic);\n\tenic_link_check(enic);\n}\n\n#define ENIC_TEST_INTR(pba, i) (pba & (1 << i))\n\nstatic irqreturn_t enic_isr_legacy(int irq, void *data)\n{\n\tstruct net_device *netdev = data;\n\tstruct enic *enic = netdev_priv(netdev);\n\tunsigned int io_intr = ENIC_LEGACY_IO_INTR;\n\tunsigned int err_intr = ENIC_LEGACY_ERR_INTR;\n\tunsigned int notify_intr = ENIC_LEGACY_NOTIFY_INTR;\n\tu32 pba;\n\n\tvnic_intr_mask(&enic->intr[io_intr]);\n\n\tpba = vnic_intr_legacy_pba(enic->legacy_pba);\n\tif (!pba) {\n\t\tvnic_intr_unmask(&enic->intr[io_intr]);\n\t\treturn IRQ_NONE;\t \n\t}\n\n\tif (ENIC_TEST_INTR(pba, notify_intr)) {\n\t\tenic_notify_check(enic);\n\t\tvnic_intr_return_all_credits(&enic->intr[notify_intr]);\n\t}\n\n\tif (ENIC_TEST_INTR(pba, err_intr)) {\n\t\tvnic_intr_return_all_credits(&enic->intr[err_intr]);\n\t\tenic_log_q_error(enic);\n\t\t \n\t\tschedule_work(&enic->reset);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tif (ENIC_TEST_INTR(pba, io_intr))\n\t\tnapi_schedule_irqoff(&enic->napi[0]);\n\telse\n\t\tvnic_intr_unmask(&enic->intr[io_intr]);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t enic_isr_msi(int irq, void *data)\n{\n\tstruct enic *enic = data;\n\n\t \n\n\tnapi_schedule_irqoff(&enic->napi[0]);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t enic_isr_msix(int irq, void *data)\n{\n\tstruct napi_struct *napi = data;\n\n\tnapi_schedule_irqoff(napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t enic_isr_msix_err(int irq, void *data)\n{\n\tstruct enic *enic = data;\n\tunsigned int intr = enic_msix_err_intr(enic);\n\n\tvnic_intr_return_all_credits(&enic->intr[intr]);\n\n\tif (enic_log_q_error(enic))\n\t\t \n\t\tschedule_work(&enic->reset);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t enic_isr_msix_notify(int irq, void *data)\n{\n\tstruct enic *enic = data;\n\tunsigned int intr = enic_msix_notify_intr(enic);\n\n\tenic_notify_check(enic);\n\tvnic_intr_return_all_credits(&enic->intr[intr]);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int enic_queue_wq_skb_cont(struct enic *enic, struct vnic_wq *wq,\n\t\t\t\t  struct sk_buff *skb, unsigned int len_left,\n\t\t\t\t  int loopback)\n{\n\tconst skb_frag_t *frag;\n\tdma_addr_t dma_addr;\n\n\t \n\tfor (frag = skb_shinfo(skb)->frags; len_left; frag++) {\n\t\tlen_left -= skb_frag_size(frag);\n\t\tdma_addr = skb_frag_dma_map(&enic->pdev->dev, frag, 0,\n\t\t\t\t\t    skb_frag_size(frag),\n\t\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (unlikely(enic_dma_map_check(enic, dma_addr)))\n\t\t\treturn -ENOMEM;\n\t\tenic_queue_wq_desc_cont(wq, skb, dma_addr, skb_frag_size(frag),\n\t\t\t\t\t(len_left == 0),\t \n\t\t\t\t\tloopback);\n\t}\n\n\treturn 0;\n}\n\nstatic int enic_queue_wq_skb_vlan(struct enic *enic, struct vnic_wq *wq,\n\t\t\t\t  struct sk_buff *skb, int vlan_tag_insert,\n\t\t\t\t  unsigned int vlan_tag, int loopback)\n{\n\tunsigned int head_len = skb_headlen(skb);\n\tunsigned int len_left = skb->len - head_len;\n\tint eop = (len_left == 0);\n\tdma_addr_t dma_addr;\n\tint err = 0;\n\n\tdma_addr = dma_map_single(&enic->pdev->dev, skb->data, head_len,\n\t\t\t\t  DMA_TO_DEVICE);\n\tif (unlikely(enic_dma_map_check(enic, dma_addr)))\n\t\treturn -ENOMEM;\n\n\t \n\tenic_queue_wq_desc(wq, skb, dma_addr, head_len,\tvlan_tag_insert,\n\t\t\t   vlan_tag, eop, loopback);\n\n\tif (!eop)\n\t\terr = enic_queue_wq_skb_cont(enic, wq, skb, len_left, loopback);\n\n\treturn err;\n}\n\nstatic int enic_queue_wq_skb_csum_l4(struct enic *enic, struct vnic_wq *wq,\n\t\t\t\t     struct sk_buff *skb, int vlan_tag_insert,\n\t\t\t\t     unsigned int vlan_tag, int loopback)\n{\n\tunsigned int head_len = skb_headlen(skb);\n\tunsigned int len_left = skb->len - head_len;\n\tunsigned int hdr_len = skb_checksum_start_offset(skb);\n\tunsigned int csum_offset = hdr_len + skb->csum_offset;\n\tint eop = (len_left == 0);\n\tdma_addr_t dma_addr;\n\tint err = 0;\n\n\tdma_addr = dma_map_single(&enic->pdev->dev, skb->data, head_len,\n\t\t\t\t  DMA_TO_DEVICE);\n\tif (unlikely(enic_dma_map_check(enic, dma_addr)))\n\t\treturn -ENOMEM;\n\n\t \n\tenic_queue_wq_desc_csum_l4(wq, skb, dma_addr, head_len,\tcsum_offset,\n\t\t\t\t   hdr_len, vlan_tag_insert, vlan_tag, eop,\n\t\t\t\t   loopback);\n\n\tif (!eop)\n\t\terr = enic_queue_wq_skb_cont(enic, wq, skb, len_left, loopback);\n\n\treturn err;\n}\n\nstatic void enic_preload_tcp_csum_encap(struct sk_buff *skb)\n{\n\tconst struct ethhdr *eth = (struct ethhdr *)skb_inner_mac_header(skb);\n\n\tswitch (eth->h_proto) {\n\tcase ntohs(ETH_P_IP):\n\t\tinner_ip_hdr(skb)->check = 0;\n\t\tinner_tcp_hdr(skb)->check =\n\t\t\t~csum_tcpudp_magic(inner_ip_hdr(skb)->saddr,\n\t\t\t\t\t   inner_ip_hdr(skb)->daddr, 0,\n\t\t\t\t\t   IPPROTO_TCP, 0);\n\t\tbreak;\n\tcase ntohs(ETH_P_IPV6):\n\t\tinner_tcp_hdr(skb)->check =\n\t\t\t~csum_ipv6_magic(&inner_ipv6_hdr(skb)->saddr,\n\t\t\t\t\t &inner_ipv6_hdr(skb)->daddr, 0,\n\t\t\t\t\t IPPROTO_TCP, 0);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"Non ipv4/ipv6 inner pkt for encap offload\");\n\t\tbreak;\n\t}\n}\n\nstatic void enic_preload_tcp_csum(struct sk_buff *skb)\n{\n\t \n\n\tif (skb->protocol == cpu_to_be16(ETH_P_IP)) {\n\t\tip_hdr(skb)->check = 0;\n\t\ttcp_hdr(skb)->check = ~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\tip_hdr(skb)->daddr, 0, IPPROTO_TCP, 0);\n\t} else if (skb->protocol == cpu_to_be16(ETH_P_IPV6)) {\n\t\ttcp_v6_gso_csum_prep(skb);\n\t}\n}\n\nstatic int enic_queue_wq_skb_tso(struct enic *enic, struct vnic_wq *wq,\n\t\t\t\t struct sk_buff *skb, unsigned int mss,\n\t\t\t\t int vlan_tag_insert, unsigned int vlan_tag,\n\t\t\t\t int loopback)\n{\n\tunsigned int frag_len_left = skb_headlen(skb);\n\tunsigned int len_left = skb->len - frag_len_left;\n\tint eop = (len_left == 0);\n\tunsigned int offset = 0;\n\tunsigned int hdr_len;\n\tdma_addr_t dma_addr;\n\tunsigned int len;\n\tskb_frag_t *frag;\n\n\tif (skb->encapsulation) {\n\t\thdr_len = skb_inner_tcp_all_headers(skb);\n\t\tenic_preload_tcp_csum_encap(skb);\n\t} else {\n\t\thdr_len = skb_tcp_all_headers(skb);\n\t\tenic_preload_tcp_csum(skb);\n\t}\n\n\t \n\twhile (frag_len_left) {\n\t\tlen = min(frag_len_left, (unsigned int)WQ_ENET_MAX_DESC_LEN);\n\t\tdma_addr = dma_map_single(&enic->pdev->dev,\n\t\t\t\t\t  skb->data + offset, len,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tif (unlikely(enic_dma_map_check(enic, dma_addr)))\n\t\t\treturn -ENOMEM;\n\t\tenic_queue_wq_desc_tso(wq, skb, dma_addr, len, mss, hdr_len,\n\t\t\t\t       vlan_tag_insert, vlan_tag,\n\t\t\t\t       eop && (len == frag_len_left), loopback);\n\t\tfrag_len_left -= len;\n\t\toffset += len;\n\t}\n\n\tif (eop)\n\t\treturn 0;\n\n\t \n\tfor (frag = skb_shinfo(skb)->frags; len_left; frag++) {\n\t\tlen_left -= skb_frag_size(frag);\n\t\tfrag_len_left = skb_frag_size(frag);\n\t\toffset = 0;\n\n\t\twhile (frag_len_left) {\n\t\t\tlen = min(frag_len_left,\n\t\t\t\t(unsigned int)WQ_ENET_MAX_DESC_LEN);\n\t\t\tdma_addr = skb_frag_dma_map(&enic->pdev->dev, frag,\n\t\t\t\t\t\t    offset, len,\n\t\t\t\t\t\t    DMA_TO_DEVICE);\n\t\t\tif (unlikely(enic_dma_map_check(enic, dma_addr)))\n\t\t\t\treturn -ENOMEM;\n\t\t\tenic_queue_wq_desc_cont(wq, skb, dma_addr, len,\n\t\t\t\t\t\t(len_left == 0) &&\n\t\t\t\t\t\t (len == frag_len_left), \n\t\t\t\t\t\tloopback);\n\t\t\tfrag_len_left -= len;\n\t\t\toffset += len;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic inline int enic_queue_wq_skb_encap(struct enic *enic, struct vnic_wq *wq,\n\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t  int vlan_tag_insert,\n\t\t\t\t\t  unsigned int vlan_tag, int loopback)\n{\n\tunsigned int head_len = skb_headlen(skb);\n\tunsigned int len_left = skb->len - head_len;\n\t \n\tunsigned int mss_or_csum = 7;\n\tint eop = (len_left == 0);\n\tdma_addr_t dma_addr;\n\tint err = 0;\n\n\tdma_addr = dma_map_single(&enic->pdev->dev, skb->data, head_len,\n\t\t\t\t  DMA_TO_DEVICE);\n\tif (unlikely(enic_dma_map_check(enic, dma_addr)))\n\t\treturn -ENOMEM;\n\n\tenic_queue_wq_desc_ex(wq, skb, dma_addr, head_len, mss_or_csum, 0,\n\t\t\t      vlan_tag_insert, vlan_tag,\n\t\t\t      WQ_ENET_OFFLOAD_MODE_CSUM, eop, 1  , eop,\n\t\t\t      loopback);\n\tif (!eop)\n\t\terr = enic_queue_wq_skb_cont(enic, wq, skb, len_left, loopback);\n\n\treturn err;\n}\n\nstatic inline int enic_queue_wq_skb(struct enic *enic,\n\tstruct vnic_wq *wq, struct sk_buff *skb)\n{\n\tunsigned int mss = skb_shinfo(skb)->gso_size;\n\tunsigned int vlan_tag = 0;\n\tint vlan_tag_insert = 0;\n\tint loopback = 0;\n\tint err;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\t \n\t\tvlan_tag_insert = 1;\n\t\tvlan_tag = skb_vlan_tag_get(skb);\n\t} else if (enic->loop_enable) {\n\t\tvlan_tag = enic->loop_tag;\n\t\tloopback = 1;\n\t}\n\n\tif (mss)\n\t\terr = enic_queue_wq_skb_tso(enic, wq, skb, mss,\n\t\t\t\t\t    vlan_tag_insert, vlan_tag,\n\t\t\t\t\t    loopback);\n\telse if (skb->encapsulation)\n\t\terr = enic_queue_wq_skb_encap(enic, wq, skb, vlan_tag_insert,\n\t\t\t\t\t      vlan_tag, loopback);\n\telse if\t(skb->ip_summed == CHECKSUM_PARTIAL)\n\t\terr = enic_queue_wq_skb_csum_l4(enic, wq, skb, vlan_tag_insert,\n\t\t\t\t\t\tvlan_tag, loopback);\n\telse\n\t\terr = enic_queue_wq_skb_vlan(enic, wq, skb, vlan_tag_insert,\n\t\t\t\t\t     vlan_tag, loopback);\n\tif (unlikely(err)) {\n\t\tstruct vnic_wq_buf *buf;\n\n\t\tbuf = wq->to_use->prev;\n\t\t \n\t\twhile (!buf->os_buf && (buf->next != wq->to_clean)) {\n\t\t\tenic_free_wq_buf(wq, buf);\n\t\t\twq->ring.desc_avail++;\n\t\t\tbuf = buf->prev;\n\t\t}\n\t\twq->to_use = buf->next;\n\t\tdev_kfree_skb(skb);\n\t}\n\treturn err;\n}\n\n \nstatic netdev_tx_t enic_hard_start_xmit(struct sk_buff *skb,\n\tstruct net_device *netdev)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tstruct vnic_wq *wq;\n\tunsigned int txq_map;\n\tstruct netdev_queue *txq;\n\n\tif (skb->len <= 0) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\ttxq_map = skb_get_queue_mapping(skb) % enic->wq_count;\n\twq = &enic->wq[txq_map];\n\ttxq = netdev_get_tx_queue(netdev, txq_map);\n\n\t \n\n\tif (skb_shinfo(skb)->gso_size == 0 &&\n\t    skb_shinfo(skb)->nr_frags + 1 > ENIC_NON_TSO_MAX_DESC &&\n\t    skb_linearize(skb)) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tspin_lock(&enic->wq_lock[txq_map]);\n\n\tif (vnic_wq_desc_avail(wq) <\n\t    skb_shinfo(skb)->nr_frags + ENIC_DESC_MAX_SPLITS) {\n\t\tnetif_tx_stop_queue(txq);\n\t\t \n\t\tnetdev_err(netdev, \"BUG! Tx ring full when queue awake!\\n\");\n\t\tspin_unlock(&enic->wq_lock[txq_map]);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tif (enic_queue_wq_skb(enic, wq, skb))\n\t\tgoto error;\n\n\tif (vnic_wq_desc_avail(wq) < MAX_SKB_FRAGS + ENIC_DESC_MAX_SPLITS)\n\t\tnetif_tx_stop_queue(txq);\n\tskb_tx_timestamp(skb);\n\tif (!netdev_xmit_more() || netif_xmit_stopped(txq))\n\t\tvnic_wq_doorbell(wq);\n\nerror:\n\tspin_unlock(&enic->wq_lock[txq_map]);\n\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic void enic_get_stats(struct net_device *netdev,\n\t\t\t   struct rtnl_link_stats64 *net_stats)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tstruct vnic_stats *stats;\n\tint err;\n\n\terr = enic_dev_stats_dump(enic, &stats);\n\t \n\tif (err == -ENOMEM)\n\t\treturn;\n\n\tnet_stats->tx_packets = stats->tx.tx_frames_ok;\n\tnet_stats->tx_bytes = stats->tx.tx_bytes_ok;\n\tnet_stats->tx_errors = stats->tx.tx_errors;\n\tnet_stats->tx_dropped = stats->tx.tx_drops;\n\n\tnet_stats->rx_packets = stats->rx.rx_frames_ok;\n\tnet_stats->rx_bytes = stats->rx.rx_bytes_ok;\n\tnet_stats->rx_errors = stats->rx.rx_errors;\n\tnet_stats->multicast = stats->rx.rx_multicast_frames_ok;\n\tnet_stats->rx_over_errors = enic->rq_truncated_pkts;\n\tnet_stats->rx_crc_errors = enic->rq_bad_fcs;\n\tnet_stats->rx_dropped = stats->rx.rx_no_bufs + stats->rx.rx_drop;\n}\n\nstatic int enic_mc_sync(struct net_device *netdev, const u8 *mc_addr)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\n\tif (enic->mc_count == ENIC_MULTICAST_PERFECT_FILTERS) {\n\t\tunsigned int mc_count = netdev_mc_count(netdev);\n\n\t\tnetdev_warn(netdev, \"Registering only %d out of %d multicast addresses\\n\",\n\t\t\t    ENIC_MULTICAST_PERFECT_FILTERS, mc_count);\n\n\t\treturn -ENOSPC;\n\t}\n\n\tenic_dev_add_addr(enic, mc_addr);\n\tenic->mc_count++;\n\n\treturn 0;\n}\n\nstatic int enic_mc_unsync(struct net_device *netdev, const u8 *mc_addr)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\n\tenic_dev_del_addr(enic, mc_addr);\n\tenic->mc_count--;\n\n\treturn 0;\n}\n\nstatic int enic_uc_sync(struct net_device *netdev, const u8 *uc_addr)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\n\tif (enic->uc_count == ENIC_UNICAST_PERFECT_FILTERS) {\n\t\tunsigned int uc_count = netdev_uc_count(netdev);\n\n\t\tnetdev_warn(netdev, \"Registering only %d out of %d unicast addresses\\n\",\n\t\t\t    ENIC_UNICAST_PERFECT_FILTERS, uc_count);\n\n\t\treturn -ENOSPC;\n\t}\n\n\tenic_dev_add_addr(enic, uc_addr);\n\tenic->uc_count++;\n\n\treturn 0;\n}\n\nstatic int enic_uc_unsync(struct net_device *netdev, const u8 *uc_addr)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\n\tenic_dev_del_addr(enic, uc_addr);\n\tenic->uc_count--;\n\n\treturn 0;\n}\n\nvoid enic_reset_addr_lists(struct enic *enic)\n{\n\tstruct net_device *netdev = enic->netdev;\n\n\t__dev_uc_unsync(netdev, NULL);\n\t__dev_mc_unsync(netdev, NULL);\n\n\tenic->mc_count = 0;\n\tenic->uc_count = 0;\n\tenic->flags = 0;\n}\n\nstatic int enic_set_mac_addr(struct net_device *netdev, char *addr)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\n\tif (enic_is_dynamic(enic) || enic_is_sriov_vf(enic)) {\n\t\tif (!is_valid_ether_addr(addr) && !is_zero_ether_addr(addr))\n\t\t\treturn -EADDRNOTAVAIL;\n\t} else {\n\t\tif (!is_valid_ether_addr(addr))\n\t\t\treturn -EADDRNOTAVAIL;\n\t}\n\n\teth_hw_addr_set(netdev, addr);\n\n\treturn 0;\n}\n\nstatic int enic_set_mac_address_dynamic(struct net_device *netdev, void *p)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tstruct sockaddr *saddr = p;\n\tchar *addr = saddr->sa_data;\n\tint err;\n\n\tif (netif_running(enic->netdev)) {\n\t\terr = enic_dev_del_station_addr(enic);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = enic_set_mac_addr(netdev, addr);\n\tif (err)\n\t\treturn err;\n\n\tif (netif_running(enic->netdev)) {\n\t\terr = enic_dev_add_station_addr(enic);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn err;\n}\n\nstatic int enic_set_mac_address(struct net_device *netdev, void *p)\n{\n\tstruct sockaddr *saddr = p;\n\tchar *addr = saddr->sa_data;\n\tstruct enic *enic = netdev_priv(netdev);\n\tint err;\n\n\terr = enic_dev_del_station_addr(enic);\n\tif (err)\n\t\treturn err;\n\n\terr = enic_set_mac_addr(netdev, addr);\n\tif (err)\n\t\treturn err;\n\n\treturn enic_dev_add_station_addr(enic);\n}\n\n \nstatic void enic_set_rx_mode(struct net_device *netdev)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tint directed = 1;\n\tint multicast = (netdev->flags & IFF_MULTICAST) ? 1 : 0;\n\tint broadcast = (netdev->flags & IFF_BROADCAST) ? 1 : 0;\n\tint promisc = (netdev->flags & IFF_PROMISC) ||\n\t\tnetdev_uc_count(netdev) > ENIC_UNICAST_PERFECT_FILTERS;\n\tint allmulti = (netdev->flags & IFF_ALLMULTI) ||\n\t\tnetdev_mc_count(netdev) > ENIC_MULTICAST_PERFECT_FILTERS;\n\tunsigned int flags = netdev->flags |\n\t\t(allmulti ? IFF_ALLMULTI : 0) |\n\t\t(promisc ? IFF_PROMISC : 0);\n\n\tif (enic->flags != flags) {\n\t\tenic->flags = flags;\n\t\tenic_dev_packet_filter(enic, directed,\n\t\t\tmulticast, broadcast, promisc, allmulti);\n\t}\n\n\tif (!promisc) {\n\t\t__dev_uc_sync(netdev, enic_uc_sync, enic_uc_unsync);\n\t\tif (!allmulti)\n\t\t\t__dev_mc_sync(netdev, enic_mc_sync, enic_mc_unsync);\n\t}\n}\n\n \nstatic void enic_tx_timeout(struct net_device *netdev, unsigned int txqueue)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tschedule_work(&enic->tx_hang_reset);\n}\n\nstatic int enic_set_vf_mac(struct net_device *netdev, int vf, u8 *mac)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tstruct enic_port_profile *pp;\n\tint err;\n\n\tENIC_PP_BY_INDEX(enic, vf, pp, &err);\n\tif (err)\n\t\treturn err;\n\n\tif (is_valid_ether_addr(mac) || is_zero_ether_addr(mac)) {\n\t\tif (vf == PORT_SELF_VF) {\n\t\t\tmemcpy(pp->vf_mac, mac, ETH_ALEN);\n\t\t\treturn 0;\n\t\t} else {\n\t\t\t \n\t\t\tENIC_DEVCMD_PROXY_BY_INDEX(vf, err, enic,\n\t\t\t\tvnic_dev_set_mac_addr, mac);\n\t\t\treturn enic_dev_status_to_errno(err);\n\t\t}\n\t} else\n\t\treturn -EINVAL;\n}\n\nstatic int enic_set_vf_port(struct net_device *netdev, int vf,\n\tstruct nlattr *port[])\n{\n\tstatic const u8 zero_addr[ETH_ALEN] = {};\n\tstruct enic *enic = netdev_priv(netdev);\n\tstruct enic_port_profile prev_pp;\n\tstruct enic_port_profile *pp;\n\tint err = 0, restore_pp = 1;\n\n\tENIC_PP_BY_INDEX(enic, vf, pp, &err);\n\tif (err)\n\t\treturn err;\n\n\tif (!port[IFLA_PORT_REQUEST])\n\t\treturn -EOPNOTSUPP;\n\n\tmemcpy(&prev_pp, pp, sizeof(*enic->pp));\n\tmemset(pp, 0, sizeof(*enic->pp));\n\n\tpp->set |= ENIC_SET_REQUEST;\n\tpp->request = nla_get_u8(port[IFLA_PORT_REQUEST]);\n\n\tif (port[IFLA_PORT_PROFILE]) {\n\t\tpp->set |= ENIC_SET_NAME;\n\t\tmemcpy(pp->name, nla_data(port[IFLA_PORT_PROFILE]),\n\t\t\tPORT_PROFILE_MAX);\n\t}\n\n\tif (port[IFLA_PORT_INSTANCE_UUID]) {\n\t\tpp->set |= ENIC_SET_INSTANCE;\n\t\tmemcpy(pp->instance_uuid,\n\t\t\tnla_data(port[IFLA_PORT_INSTANCE_UUID]), PORT_UUID_MAX);\n\t}\n\n\tif (port[IFLA_PORT_HOST_UUID]) {\n\t\tpp->set |= ENIC_SET_HOST;\n\t\tmemcpy(pp->host_uuid,\n\t\t\tnla_data(port[IFLA_PORT_HOST_UUID]), PORT_UUID_MAX);\n\t}\n\n\tif (vf == PORT_SELF_VF) {\n\t\t \n\t\tif (!is_zero_ether_addr(prev_pp.vf_mac))\n\t\t\tmemcpy(pp->mac_addr, prev_pp.vf_mac, ETH_ALEN);\n\n\t\tif (is_zero_ether_addr(netdev->dev_addr))\n\t\t\teth_hw_addr_random(netdev);\n\t} else {\n\t\t \n\t\tENIC_DEVCMD_PROXY_BY_INDEX(vf, err, enic,\n\t\t\tvnic_dev_get_mac_addr, pp->mac_addr);\n\t\tif (err) {\n\t\t\tnetdev_err(netdev, \"Error getting mac for vf %d\\n\", vf);\n\t\t\tmemcpy(pp, &prev_pp, sizeof(*pp));\n\t\t\treturn enic_dev_status_to_errno(err);\n\t\t}\n\t}\n\n\terr = enic_process_set_pp_request(enic, vf, &prev_pp, &restore_pp);\n\tif (err) {\n\t\tif (restore_pp) {\n\t\t\t \n\t\t\tmemcpy(pp, &prev_pp, sizeof(*pp));\n\t\t} else {\n\t\t\tmemset(pp, 0, sizeof(*pp));\n\t\t\tif (vf == PORT_SELF_VF)\n\t\t\t\teth_hw_addr_set(netdev, zero_addr);\n\t\t}\n\t} else {\n\t\t \n\t\tpp->set |= ENIC_PORT_REQUEST_APPLIED;\n\n\t\t \n\t\tif (pp->request == PORT_REQUEST_DISASSOCIATE) {\n\t\t\teth_zero_addr(pp->mac_addr);\n\t\t\tif (vf == PORT_SELF_VF)\n\t\t\t\teth_hw_addr_set(netdev, zero_addr);\n\t\t}\n\t}\n\n\tif (vf == PORT_SELF_VF)\n\t\teth_zero_addr(pp->vf_mac);\n\n\treturn err;\n}\n\nstatic int enic_get_vf_port(struct net_device *netdev, int vf,\n\tstruct sk_buff *skb)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tu16 response = PORT_PROFILE_RESPONSE_SUCCESS;\n\tstruct enic_port_profile *pp;\n\tint err;\n\n\tENIC_PP_BY_INDEX(enic, vf, pp, &err);\n\tif (err)\n\t\treturn err;\n\n\tif (!(pp->set & ENIC_PORT_REQUEST_APPLIED))\n\t\treturn -ENODATA;\n\n\terr = enic_process_get_pp_request(enic, vf, pp->request, &response);\n\tif (err)\n\t\treturn err;\n\n\tif (nla_put_u16(skb, IFLA_PORT_REQUEST, pp->request) ||\n\t    nla_put_u16(skb, IFLA_PORT_RESPONSE, response) ||\n\t    ((pp->set & ENIC_SET_NAME) &&\n\t     nla_put(skb, IFLA_PORT_PROFILE, PORT_PROFILE_MAX, pp->name)) ||\n\t    ((pp->set & ENIC_SET_INSTANCE) &&\n\t     nla_put(skb, IFLA_PORT_INSTANCE_UUID, PORT_UUID_MAX,\n\t\t     pp->instance_uuid)) ||\n\t    ((pp->set & ENIC_SET_HOST) &&\n\t     nla_put(skb, IFLA_PORT_HOST_UUID, PORT_UUID_MAX, pp->host_uuid)))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}\n\nstatic void enic_free_rq_buf(struct vnic_rq *rq, struct vnic_rq_buf *buf)\n{\n\tstruct enic *enic = vnic_dev_priv(rq->vdev);\n\n\tif (!buf->os_buf)\n\t\treturn;\n\n\tdma_unmap_single(&enic->pdev->dev, buf->dma_addr, buf->len,\n\t\t\t DMA_FROM_DEVICE);\n\tdev_kfree_skb_any(buf->os_buf);\n\tbuf->os_buf = NULL;\n}\n\nstatic int enic_rq_alloc_buf(struct vnic_rq *rq)\n{\n\tstruct enic *enic = vnic_dev_priv(rq->vdev);\n\tstruct net_device *netdev = enic->netdev;\n\tstruct sk_buff *skb;\n\tunsigned int len = netdev->mtu + VLAN_ETH_HLEN;\n\tunsigned int os_buf_index = 0;\n\tdma_addr_t dma_addr;\n\tstruct vnic_rq_buf *buf = rq->to_use;\n\n\tif (buf->os_buf) {\n\t\tenic_queue_rq_desc(rq, buf->os_buf, os_buf_index, buf->dma_addr,\n\t\t\t\t   buf->len);\n\n\t\treturn 0;\n\t}\n\tskb = netdev_alloc_skb_ip_align(netdev, len);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tdma_addr = dma_map_single(&enic->pdev->dev, skb->data, len,\n\t\t\t\t  DMA_FROM_DEVICE);\n\tif (unlikely(enic_dma_map_check(enic, dma_addr))) {\n\t\tdev_kfree_skb(skb);\n\t\treturn -ENOMEM;\n\t}\n\n\tenic_queue_rq_desc(rq, skb, os_buf_index,\n\t\tdma_addr, len);\n\n\treturn 0;\n}\n\nstatic void enic_intr_update_pkt_size(struct vnic_rx_bytes_counter *pkt_size,\n\t\t\t\t      u32 pkt_len)\n{\n\tif (ENIC_LARGE_PKT_THRESHOLD <= pkt_len)\n\t\tpkt_size->large_pkt_bytes_cnt += pkt_len;\n\telse\n\t\tpkt_size->small_pkt_bytes_cnt += pkt_len;\n}\n\nstatic bool enic_rxcopybreak(struct net_device *netdev, struct sk_buff **skb,\n\t\t\t     struct vnic_rq_buf *buf, u16 len)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tstruct sk_buff *new_skb;\n\n\tif (len > enic->rx_copybreak)\n\t\treturn false;\n\tnew_skb = netdev_alloc_skb_ip_align(netdev, len);\n\tif (!new_skb)\n\t\treturn false;\n\tdma_sync_single_for_cpu(&enic->pdev->dev, buf->dma_addr, len,\n\t\t\t\tDMA_FROM_DEVICE);\n\tmemcpy(new_skb->data, (*skb)->data, len);\n\t*skb = new_skb;\n\n\treturn true;\n}\n\nstatic void enic_rq_indicate_buf(struct vnic_rq *rq,\n\tstruct cq_desc *cq_desc, struct vnic_rq_buf *buf,\n\tint skipped, void *opaque)\n{\n\tstruct enic *enic = vnic_dev_priv(rq->vdev);\n\tstruct net_device *netdev = enic->netdev;\n\tstruct sk_buff *skb;\n\tstruct vnic_cq *cq = &enic->cq[enic_cq_rq(enic, rq->index)];\n\n\tu8 type, color, eop, sop, ingress_port, vlan_stripped;\n\tu8 fcoe, fcoe_sof, fcoe_fc_crc_ok, fcoe_enc_error, fcoe_eof;\n\tu8 tcp_udp_csum_ok, udp, tcp, ipv4_csum_ok;\n\tu8 ipv6, ipv4, ipv4_fragment, fcs_ok, rss_type, csum_not_calc;\n\tu8 packet_error;\n\tu16 q_number, completed_index, bytes_written, vlan_tci, checksum;\n\tu32 rss_hash;\n\tbool outer_csum_ok = true, encap = false;\n\n\tif (skipped)\n\t\treturn;\n\n\tskb = buf->os_buf;\n\n\tcq_enet_rq_desc_dec((struct cq_enet_rq_desc *)cq_desc,\n\t\t&type, &color, &q_number, &completed_index,\n\t\t&ingress_port, &fcoe, &eop, &sop, &rss_type,\n\t\t&csum_not_calc, &rss_hash, &bytes_written,\n\t\t&packet_error, &vlan_stripped, &vlan_tci, &checksum,\n\t\t&fcoe_sof, &fcoe_fc_crc_ok, &fcoe_enc_error,\n\t\t&fcoe_eof, &tcp_udp_csum_ok, &udp, &tcp,\n\t\t&ipv4_csum_ok, &ipv6, &ipv4, &ipv4_fragment,\n\t\t&fcs_ok);\n\n\tif (packet_error) {\n\n\t\tif (!fcs_ok) {\n\t\t\tif (bytes_written > 0)\n\t\t\t\tenic->rq_bad_fcs++;\n\t\t\telse if (bytes_written == 0)\n\t\t\t\tenic->rq_truncated_pkts++;\n\t\t}\n\n\t\tdma_unmap_single(&enic->pdev->dev, buf->dma_addr, buf->len,\n\t\t\t\t DMA_FROM_DEVICE);\n\t\tdev_kfree_skb_any(skb);\n\t\tbuf->os_buf = NULL;\n\n\t\treturn;\n\t}\n\n\tif (eop && bytes_written > 0) {\n\n\t\t \n\n\t\tif (!enic_rxcopybreak(netdev, &skb, buf, bytes_written)) {\n\t\t\tbuf->os_buf = NULL;\n\t\t\tdma_unmap_single(&enic->pdev->dev, buf->dma_addr,\n\t\t\t\t\t buf->len, DMA_FROM_DEVICE);\n\t\t}\n\t\tprefetch(skb->data - NET_IP_ALIGN);\n\n\t\tskb_put(skb, bytes_written);\n\t\tskb->protocol = eth_type_trans(skb, netdev);\n\t\tskb_record_rx_queue(skb, q_number);\n\t\tif ((netdev->features & NETIF_F_RXHASH) && rss_hash &&\n\t\t    (type == 3)) {\n\t\t\tswitch (rss_type) {\n\t\t\tcase CQ_ENET_RQ_DESC_RSS_TYPE_TCP_IPv4:\n\t\t\tcase CQ_ENET_RQ_DESC_RSS_TYPE_TCP_IPv6:\n\t\t\tcase CQ_ENET_RQ_DESC_RSS_TYPE_TCP_IPv6_EX:\n\t\t\t\tskb_set_hash(skb, rss_hash, PKT_HASH_TYPE_L4);\n\t\t\t\tbreak;\n\t\t\tcase CQ_ENET_RQ_DESC_RSS_TYPE_IPv4:\n\t\t\tcase CQ_ENET_RQ_DESC_RSS_TYPE_IPv6:\n\t\t\tcase CQ_ENET_RQ_DESC_RSS_TYPE_IPv6_EX:\n\t\t\t\tskb_set_hash(skb, rss_hash, PKT_HASH_TYPE_L3);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (enic->vxlan.vxlan_udp_port_number) {\n\t\t\tswitch (enic->vxlan.patch_level) {\n\t\t\tcase 0:\n\t\t\t\tif (fcoe) {\n\t\t\t\t\tencap = true;\n\t\t\t\t\touter_csum_ok = fcoe_fc_crc_ok;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase 2:\n\t\t\t\tif ((type == 7) &&\n\t\t\t\t    (rss_hash & BIT(0))) {\n\t\t\t\t\tencap = true;\n\t\t\t\t\touter_csum_ok = (rss_hash & BIT(1)) &&\n\t\t\t\t\t\t\t(rss_hash & BIT(2));\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif ((netdev->features & NETIF_F_RXCSUM) && !csum_not_calc &&\n\t\t    tcp_udp_csum_ok && outer_csum_ok &&\n\t\t    (ipv4_csum_ok || ipv6)) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tskb->csum_level = encap;\n\t\t}\n\n\t\tif (vlan_stripped)\n\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tci);\n\n\t\tskb_mark_napi_id(skb, &enic->napi[rq->index]);\n\t\tif (!(netdev->features & NETIF_F_GRO))\n\t\t\tnetif_receive_skb(skb);\n\t\telse\n\t\t\tnapi_gro_receive(&enic->napi[q_number], skb);\n\t\tif (enic->rx_coalesce_setting.use_adaptive_rx_coalesce)\n\t\t\tenic_intr_update_pkt_size(&cq->pkt_size_counter,\n\t\t\t\t\t\t  bytes_written);\n\t} else {\n\n\t\t \n\n\t\tdma_unmap_single(&enic->pdev->dev, buf->dma_addr, buf->len,\n\t\t\t\t DMA_FROM_DEVICE);\n\t\tdev_kfree_skb_any(skb);\n\t\tbuf->os_buf = NULL;\n\t}\n}\n\nstatic int enic_rq_service(struct vnic_dev *vdev, struct cq_desc *cq_desc,\n\tu8 type, u16 q_number, u16 completed_index, void *opaque)\n{\n\tstruct enic *enic = vnic_dev_priv(vdev);\n\n\tvnic_rq_service(&enic->rq[q_number], cq_desc,\n\t\tcompleted_index, VNIC_RQ_RETURN_DESC,\n\t\tenic_rq_indicate_buf, opaque);\n\n\treturn 0;\n}\n\nstatic void enic_set_int_moderation(struct enic *enic, struct vnic_rq *rq)\n{\n\tunsigned int intr = enic_msix_rq_intr(enic, rq->index);\n\tstruct vnic_cq *cq = &enic->cq[enic_cq_rq(enic, rq->index)];\n\tu32 timer = cq->tobe_rx_coal_timeval;\n\n\tif (cq->tobe_rx_coal_timeval != cq->cur_rx_coal_timeval) {\n\t\tvnic_intr_coalescing_timer_set(&enic->intr[intr], timer);\n\t\tcq->cur_rx_coal_timeval = cq->tobe_rx_coal_timeval;\n\t}\n}\n\nstatic void enic_calc_int_moderation(struct enic *enic, struct vnic_rq *rq)\n{\n\tstruct enic_rx_coal *rx_coal = &enic->rx_coalesce_setting;\n\tstruct vnic_cq *cq = &enic->cq[enic_cq_rq(enic, rq->index)];\n\tstruct vnic_rx_bytes_counter *pkt_size_counter = &cq->pkt_size_counter;\n\tint index;\n\tu32 timer;\n\tu32 range_start;\n\tu32 traffic;\n\tu64 delta;\n\tktime_t now = ktime_get();\n\n\tdelta = ktime_us_delta(now, cq->prev_ts);\n\tif (delta < ENIC_AIC_TS_BREAK)\n\t\treturn;\n\tcq->prev_ts = now;\n\n\ttraffic = pkt_size_counter->large_pkt_bytes_cnt +\n\t\t  pkt_size_counter->small_pkt_bytes_cnt;\n\t \n\n\ttraffic <<= 3;\n\ttraffic = delta > UINT_MAX ? 0 : traffic / (u32)delta;\n\n\tfor (index = 0; index < ENIC_MAX_COALESCE_TIMERS; index++)\n\t\tif (traffic < mod_table[index].rx_rate)\n\t\t\tbreak;\n\trange_start = (pkt_size_counter->small_pkt_bytes_cnt >\n\t\t       pkt_size_counter->large_pkt_bytes_cnt << 1) ?\n\t\t      rx_coal->small_pkt_range_start :\n\t\t      rx_coal->large_pkt_range_start;\n\ttimer = range_start + ((rx_coal->range_end - range_start) *\n\t\t\t       mod_table[index].range_percent / 100);\n\t \n\tcq->tobe_rx_coal_timeval = (timer + cq->tobe_rx_coal_timeval) >> 1;\n\n\tpkt_size_counter->large_pkt_bytes_cnt = 0;\n\tpkt_size_counter->small_pkt_bytes_cnt = 0;\n}\n\nstatic int enic_poll(struct napi_struct *napi, int budget)\n{\n\tstruct net_device *netdev = napi->dev;\n\tstruct enic *enic = netdev_priv(netdev);\n\tunsigned int cq_rq = enic_cq_rq(enic, 0);\n\tunsigned int cq_wq = enic_cq_wq(enic, 0);\n\tunsigned int intr = ENIC_LEGACY_IO_INTR;\n\tunsigned int rq_work_to_do = budget;\n\tunsigned int wq_work_to_do = ENIC_WQ_NAPI_BUDGET;\n\tunsigned int  work_done, rq_work_done = 0, wq_work_done;\n\tint err;\n\n\twq_work_done = vnic_cq_service(&enic->cq[cq_wq], wq_work_to_do,\n\t\t\t\t       enic_wq_service, NULL);\n\n\tif (budget > 0)\n\t\trq_work_done = vnic_cq_service(&enic->cq[cq_rq],\n\t\t\trq_work_to_do, enic_rq_service, NULL);\n\n\t \n\n\twork_done = rq_work_done + wq_work_done;\n\n\tif (work_done > 0)\n\t\tvnic_intr_return_credits(&enic->intr[intr],\n\t\t\twork_done,\n\t\t\t0  ,\n\t\t\t0  );\n\n\terr = vnic_rq_fill(&enic->rq[0], enic_rq_alloc_buf);\n\n\t \n\n\tif (err)\n\t\trq_work_done = rq_work_to_do;\n\tif (enic->rx_coalesce_setting.use_adaptive_rx_coalesce)\n\t\t \n\t\tenic_calc_int_moderation(enic, &enic->rq[0]);\n\n\tif ((rq_work_done < budget) && napi_complete_done(napi, rq_work_done)) {\n\n\t\t \n\n\t\tif (enic->rx_coalesce_setting.use_adaptive_rx_coalesce)\n\t\t\tenic_set_int_moderation(enic, &enic->rq[0]);\n\t\tvnic_intr_unmask(&enic->intr[intr]);\n\t}\n\n\treturn rq_work_done;\n}\n\n#ifdef CONFIG_RFS_ACCEL\nstatic void enic_free_rx_cpu_rmap(struct enic *enic)\n{\n\tfree_irq_cpu_rmap(enic->netdev->rx_cpu_rmap);\n\tenic->netdev->rx_cpu_rmap = NULL;\n}\n\nstatic void enic_set_rx_cpu_rmap(struct enic *enic)\n{\n\tint i, res;\n\n\tif (vnic_dev_get_intr_mode(enic->vdev) == VNIC_DEV_INTR_MODE_MSIX) {\n\t\tenic->netdev->rx_cpu_rmap = alloc_irq_cpu_rmap(enic->rq_count);\n\t\tif (unlikely(!enic->netdev->rx_cpu_rmap))\n\t\t\treturn;\n\t\tfor (i = 0; i < enic->rq_count; i++) {\n\t\t\tres = irq_cpu_rmap_add(enic->netdev->rx_cpu_rmap,\n\t\t\t\t\t       enic->msix_entry[i].vector);\n\t\t\tif (unlikely(res)) {\n\t\t\t\tenic_free_rx_cpu_rmap(enic);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}\n\n#else\n\nstatic void enic_free_rx_cpu_rmap(struct enic *enic)\n{\n}\n\nstatic void enic_set_rx_cpu_rmap(struct enic *enic)\n{\n}\n\n#endif  \n\nstatic int enic_poll_msix_wq(struct napi_struct *napi, int budget)\n{\n\tstruct net_device *netdev = napi->dev;\n\tstruct enic *enic = netdev_priv(netdev);\n\tunsigned int wq_index = (napi - &enic->napi[0]) - enic->rq_count;\n\tstruct vnic_wq *wq = &enic->wq[wq_index];\n\tunsigned int cq;\n\tunsigned int intr;\n\tunsigned int wq_work_to_do = ENIC_WQ_NAPI_BUDGET;\n\tunsigned int wq_work_done;\n\tunsigned int wq_irq;\n\n\twq_irq = wq->index;\n\tcq = enic_cq_wq(enic, wq_irq);\n\tintr = enic_msix_wq_intr(enic, wq_irq);\n\twq_work_done = vnic_cq_service(&enic->cq[cq], wq_work_to_do,\n\t\t\t\t       enic_wq_service, NULL);\n\n\tvnic_intr_return_credits(&enic->intr[intr], wq_work_done,\n\t\t\t\t 0  ,\n\t\t\t\t 1  );\n\tif (!wq_work_done) {\n\t\tnapi_complete(napi);\n\t\tvnic_intr_unmask(&enic->intr[intr]);\n\t\treturn 0;\n\t}\n\n\treturn budget;\n}\n\nstatic int enic_poll_msix_rq(struct napi_struct *napi, int budget)\n{\n\tstruct net_device *netdev = napi->dev;\n\tstruct enic *enic = netdev_priv(netdev);\n\tunsigned int rq = (napi - &enic->napi[0]);\n\tunsigned int cq = enic_cq_rq(enic, rq);\n\tunsigned int intr = enic_msix_rq_intr(enic, rq);\n\tunsigned int work_to_do = budget;\n\tunsigned int work_done = 0;\n\tint err;\n\n\t \n\n\tif (budget > 0)\n\t\twork_done = vnic_cq_service(&enic->cq[cq],\n\t\t\twork_to_do, enic_rq_service, NULL);\n\n\t \n\n\tif (work_done > 0)\n\t\tvnic_intr_return_credits(&enic->intr[intr],\n\t\t\twork_done,\n\t\t\t0  ,\n\t\t\t0  );\n\n\terr = vnic_rq_fill(&enic->rq[rq], enic_rq_alloc_buf);\n\n\t \n\n\tif (err)\n\t\twork_done = work_to_do;\n\tif (enic->rx_coalesce_setting.use_adaptive_rx_coalesce)\n\t\t \n\t\tenic_calc_int_moderation(enic, &enic->rq[rq]);\n\n\tif ((work_done < budget) && napi_complete_done(napi, work_done)) {\n\n\t\t \n\n\t\tif (enic->rx_coalesce_setting.use_adaptive_rx_coalesce)\n\t\t\tenic_set_int_moderation(enic, &enic->rq[rq]);\n\t\tvnic_intr_unmask(&enic->intr[intr]);\n\t}\n\n\treturn work_done;\n}\n\nstatic void enic_notify_timer(struct timer_list *t)\n{\n\tstruct enic *enic = from_timer(enic, t, notify_timer);\n\n\tenic_notify_check(enic);\n\n\tmod_timer(&enic->notify_timer,\n\t\tround_jiffies(jiffies + ENIC_NOTIFY_TIMER_PERIOD));\n}\n\nstatic void enic_free_intr(struct enic *enic)\n{\n\tstruct net_device *netdev = enic->netdev;\n\tunsigned int i;\n\n\tenic_free_rx_cpu_rmap(enic);\n\tswitch (vnic_dev_get_intr_mode(enic->vdev)) {\n\tcase VNIC_DEV_INTR_MODE_INTX:\n\t\tfree_irq(enic->pdev->irq, netdev);\n\t\tbreak;\n\tcase VNIC_DEV_INTR_MODE_MSI:\n\t\tfree_irq(enic->pdev->irq, enic);\n\t\tbreak;\n\tcase VNIC_DEV_INTR_MODE_MSIX:\n\t\tfor (i = 0; i < ARRAY_SIZE(enic->msix); i++)\n\t\t\tif (enic->msix[i].requested)\n\t\t\t\tfree_irq(enic->msix_entry[i].vector,\n\t\t\t\t\tenic->msix[i].devid);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic int enic_request_intr(struct enic *enic)\n{\n\tstruct net_device *netdev = enic->netdev;\n\tunsigned int i, intr;\n\tint err = 0;\n\n\tenic_set_rx_cpu_rmap(enic);\n\tswitch (vnic_dev_get_intr_mode(enic->vdev)) {\n\n\tcase VNIC_DEV_INTR_MODE_INTX:\n\n\t\terr = request_irq(enic->pdev->irq, enic_isr_legacy,\n\t\t\tIRQF_SHARED, netdev->name, netdev);\n\t\tbreak;\n\n\tcase VNIC_DEV_INTR_MODE_MSI:\n\n\t\terr = request_irq(enic->pdev->irq, enic_isr_msi,\n\t\t\t0, netdev->name, enic);\n\t\tbreak;\n\n\tcase VNIC_DEV_INTR_MODE_MSIX:\n\n\t\tfor (i = 0; i < enic->rq_count; i++) {\n\t\t\tintr = enic_msix_rq_intr(enic, i);\n\t\t\tsnprintf(enic->msix[intr].devname,\n\t\t\t\tsizeof(enic->msix[intr].devname),\n\t\t\t\t\"%s-rx-%u\", netdev->name, i);\n\t\t\tenic->msix[intr].isr = enic_isr_msix;\n\t\t\tenic->msix[intr].devid = &enic->napi[i];\n\t\t}\n\n\t\tfor (i = 0; i < enic->wq_count; i++) {\n\t\t\tint wq = enic_cq_wq(enic, i);\n\n\t\t\tintr = enic_msix_wq_intr(enic, i);\n\t\t\tsnprintf(enic->msix[intr].devname,\n\t\t\t\tsizeof(enic->msix[intr].devname),\n\t\t\t\t\"%s-tx-%u\", netdev->name, i);\n\t\t\tenic->msix[intr].isr = enic_isr_msix;\n\t\t\tenic->msix[intr].devid = &enic->napi[wq];\n\t\t}\n\n\t\tintr = enic_msix_err_intr(enic);\n\t\tsnprintf(enic->msix[intr].devname,\n\t\t\tsizeof(enic->msix[intr].devname),\n\t\t\t\"%s-err\", netdev->name);\n\t\tenic->msix[intr].isr = enic_isr_msix_err;\n\t\tenic->msix[intr].devid = enic;\n\n\t\tintr = enic_msix_notify_intr(enic);\n\t\tsnprintf(enic->msix[intr].devname,\n\t\t\tsizeof(enic->msix[intr].devname),\n\t\t\t\"%s-notify\", netdev->name);\n\t\tenic->msix[intr].isr = enic_isr_msix_notify;\n\t\tenic->msix[intr].devid = enic;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(enic->msix); i++)\n\t\t\tenic->msix[i].requested = 0;\n\n\t\tfor (i = 0; i < enic->intr_count; i++) {\n\t\t\terr = request_irq(enic->msix_entry[i].vector,\n\t\t\t\tenic->msix[i].isr, 0,\n\t\t\t\tenic->msix[i].devname,\n\t\t\t\tenic->msix[i].devid);\n\t\t\tif (err) {\n\t\t\t\tenic_free_intr(enic);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tenic->msix[i].requested = 1;\n\t\t}\n\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic void enic_synchronize_irqs(struct enic *enic)\n{\n\tunsigned int i;\n\n\tswitch (vnic_dev_get_intr_mode(enic->vdev)) {\n\tcase VNIC_DEV_INTR_MODE_INTX:\n\tcase VNIC_DEV_INTR_MODE_MSI:\n\t\tsynchronize_irq(enic->pdev->irq);\n\t\tbreak;\n\tcase VNIC_DEV_INTR_MODE_MSIX:\n\t\tfor (i = 0; i < enic->intr_count; i++)\n\t\t\tsynchronize_irq(enic->msix_entry[i].vector);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void enic_set_rx_coal_setting(struct enic *enic)\n{\n\tunsigned int speed;\n\tint index = -1;\n\tstruct enic_rx_coal *rx_coal = &enic->rx_coalesce_setting;\n\n\t \n\tspeed = vnic_dev_port_speed(enic->vdev);\n\tif (ENIC_LINK_SPEED_10G < speed)\n\t\tindex = ENIC_LINK_40G_INDEX;\n\telse if (ENIC_LINK_SPEED_4G < speed)\n\t\tindex = ENIC_LINK_10G_INDEX;\n\telse\n\t\tindex = ENIC_LINK_4G_INDEX;\n\n\trx_coal->small_pkt_range_start = mod_range[index].small_pkt_range_start;\n\trx_coal->large_pkt_range_start = mod_range[index].large_pkt_range_start;\n\trx_coal->range_end = ENIC_RX_COALESCE_RANGE_END;\n\n\t \n\tfor (index = 0; index < enic->rq_count; index++)\n\t\tenic->cq[index].cur_rx_coal_timeval =\n\t\t\t\tenic->config.intr_timer_usec;\n\n\trx_coal->use_adaptive_rx_coalesce = 1;\n}\n\nstatic int enic_dev_notify_set(struct enic *enic)\n{\n\tint err;\n\n\tspin_lock_bh(&enic->devcmd_lock);\n\tswitch (vnic_dev_get_intr_mode(enic->vdev)) {\n\tcase VNIC_DEV_INTR_MODE_INTX:\n\t\terr = vnic_dev_notify_set(enic->vdev, ENIC_LEGACY_NOTIFY_INTR);\n\t\tbreak;\n\tcase VNIC_DEV_INTR_MODE_MSIX:\n\t\terr = vnic_dev_notify_set(enic->vdev,\n\t\t\tenic_msix_notify_intr(enic));\n\t\tbreak;\n\tdefault:\n\t\terr = vnic_dev_notify_set(enic->vdev, -1  );\n\t\tbreak;\n\t}\n\tspin_unlock_bh(&enic->devcmd_lock);\n\n\treturn err;\n}\n\nstatic void enic_notify_timer_start(struct enic *enic)\n{\n\tswitch (vnic_dev_get_intr_mode(enic->vdev)) {\n\tcase VNIC_DEV_INTR_MODE_MSI:\n\t\tmod_timer(&enic->notify_timer, jiffies);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n}\n\n \nstatic int enic_open(struct net_device *netdev)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tunsigned int i;\n\tint err, ret;\n\n\terr = enic_request_intr(enic);\n\tif (err) {\n\t\tnetdev_err(netdev, \"Unable to request irq.\\n\");\n\t\treturn err;\n\t}\n\tenic_init_affinity_hint(enic);\n\tenic_set_affinity_hint(enic);\n\n\terr = enic_dev_notify_set(enic);\n\tif (err) {\n\t\tnetdev_err(netdev,\n\t\t\t\"Failed to alloc notify buffer, aborting.\\n\");\n\t\tgoto err_out_free_intr;\n\t}\n\n\tfor (i = 0; i < enic->rq_count; i++) {\n\t\t \n\t\tvnic_rq_enable(&enic->rq[i]);\n\t\tvnic_rq_fill(&enic->rq[i], enic_rq_alloc_buf);\n\t\t \n\t\tif (vnic_rq_desc_used(&enic->rq[i]) == 0) {\n\t\t\tnetdev_err(netdev, \"Unable to alloc receive buffers\\n\");\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_out_free_rq;\n\t\t}\n\t}\n\n\tfor (i = 0; i < enic->wq_count; i++)\n\t\tvnic_wq_enable(&enic->wq[i]);\n\n\tif (!enic_is_dynamic(enic) && !enic_is_sriov_vf(enic))\n\t\tenic_dev_add_station_addr(enic);\n\n\tenic_set_rx_mode(netdev);\n\n\tnetif_tx_wake_all_queues(netdev);\n\n\tfor (i = 0; i < enic->rq_count; i++)\n\t\tnapi_enable(&enic->napi[i]);\n\n\tif (vnic_dev_get_intr_mode(enic->vdev) == VNIC_DEV_INTR_MODE_MSIX)\n\t\tfor (i = 0; i < enic->wq_count; i++)\n\t\t\tnapi_enable(&enic->napi[enic_cq_wq(enic, i)]);\n\tenic_dev_enable(enic);\n\n\tfor (i = 0; i < enic->intr_count; i++)\n\t\tvnic_intr_unmask(&enic->intr[i]);\n\n\tenic_notify_timer_start(enic);\n\tenic_rfs_timer_start(enic);\n\n\treturn 0;\n\nerr_out_free_rq:\n\tfor (i = 0; i < enic->rq_count; i++) {\n\t\tret = vnic_rq_disable(&enic->rq[i]);\n\t\tif (!ret)\n\t\t\tvnic_rq_clean(&enic->rq[i], enic_free_rq_buf);\n\t}\n\tenic_dev_notify_unset(enic);\nerr_out_free_intr:\n\tenic_unset_affinity_hint(enic);\n\tenic_free_intr(enic);\n\n\treturn err;\n}\n\n \nstatic int enic_stop(struct net_device *netdev)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tunsigned int i;\n\tint err;\n\n\tfor (i = 0; i < enic->intr_count; i++) {\n\t\tvnic_intr_mask(&enic->intr[i]);\n\t\t(void)vnic_intr_masked(&enic->intr[i]);  \n\t}\n\n\tenic_synchronize_irqs(enic);\n\n\tdel_timer_sync(&enic->notify_timer);\n\tenic_rfs_flw_tbl_free(enic);\n\n\tenic_dev_disable(enic);\n\n\tfor (i = 0; i < enic->rq_count; i++)\n\t\tnapi_disable(&enic->napi[i]);\n\n\tnetif_carrier_off(netdev);\n\tif (vnic_dev_get_intr_mode(enic->vdev) == VNIC_DEV_INTR_MODE_MSIX)\n\t\tfor (i = 0; i < enic->wq_count; i++)\n\t\t\tnapi_disable(&enic->napi[enic_cq_wq(enic, i)]);\n\tnetif_tx_disable(netdev);\n\n\tif (!enic_is_dynamic(enic) && !enic_is_sriov_vf(enic))\n\t\tenic_dev_del_station_addr(enic);\n\n\tfor (i = 0; i < enic->wq_count; i++) {\n\t\terr = vnic_wq_disable(&enic->wq[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tfor (i = 0; i < enic->rq_count; i++) {\n\t\terr = vnic_rq_disable(&enic->rq[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tenic_dev_notify_unset(enic);\n\tenic_unset_affinity_hint(enic);\n\tenic_free_intr(enic);\n\n\tfor (i = 0; i < enic->wq_count; i++)\n\t\tvnic_wq_clean(&enic->wq[i], enic_free_wq_buf);\n\tfor (i = 0; i < enic->rq_count; i++)\n\t\tvnic_rq_clean(&enic->rq[i], enic_free_rq_buf);\n\tfor (i = 0; i < enic->cq_count; i++)\n\t\tvnic_cq_clean(&enic->cq[i]);\n\tfor (i = 0; i < enic->intr_count; i++)\n\t\tvnic_intr_clean(&enic->intr[i]);\n\n\treturn 0;\n}\n\nstatic int _enic_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tbool running = netif_running(netdev);\n\tint err = 0;\n\n\tASSERT_RTNL();\n\tif (running) {\n\t\terr = enic_stop(netdev);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tnetdev->mtu = new_mtu;\n\n\tif (running) {\n\t\terr = enic_open(netdev);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int enic_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\n\tif (enic_is_dynamic(enic) || enic_is_sriov_vf(enic))\n\t\treturn -EOPNOTSUPP;\n\n\tif (netdev->mtu > enic->port_mtu)\n\t\tnetdev_warn(netdev,\n\t\t\t    \"interface MTU (%d) set higher than port MTU (%d)\\n\",\n\t\t\t    netdev->mtu, enic->port_mtu);\n\n\treturn _enic_change_mtu(netdev, new_mtu);\n}\n\nstatic void enic_change_mtu_work(struct work_struct *work)\n{\n\tstruct enic *enic = container_of(work, struct enic, change_mtu_work);\n\tstruct net_device *netdev = enic->netdev;\n\tint new_mtu = vnic_dev_mtu(enic->vdev);\n\n\trtnl_lock();\n\t(void)_enic_change_mtu(netdev, new_mtu);\n\trtnl_unlock();\n\n\tnetdev_info(netdev, \"interface MTU set as %d\\n\", netdev->mtu);\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void enic_poll_controller(struct net_device *netdev)\n{\n\tstruct enic *enic = netdev_priv(netdev);\n\tstruct vnic_dev *vdev = enic->vdev;\n\tunsigned int i, intr;\n\n\tswitch (vnic_dev_get_intr_mode(vdev)) {\n\tcase VNIC_DEV_INTR_MODE_MSIX:\n\t\tfor (i = 0; i < enic->rq_count; i++) {\n\t\t\tintr = enic_msix_rq_intr(enic, i);\n\t\t\tenic_isr_msix(enic->msix_entry[intr].vector,\n\t\t\t\t      &enic->napi[i]);\n\t\t}\n\n\t\tfor (i = 0; i < enic->wq_count; i++) {\n\t\t\tintr = enic_msix_wq_intr(enic, i);\n\t\t\tenic_isr_msix(enic->msix_entry[intr].vector,\n\t\t\t\t      &enic->napi[enic_cq_wq(enic, i)]);\n\t\t}\n\n\t\tbreak;\n\tcase VNIC_DEV_INTR_MODE_MSI:\n\t\tenic_isr_msi(enic->pdev->irq, enic);\n\t\tbreak;\n\tcase VNIC_DEV_INTR_MODE_INTX:\n\t\tenic_isr_legacy(enic->pdev->irq, netdev);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n#endif\n\nstatic int enic_dev_wait(struct vnic_dev *vdev,\n\tint (*start)(struct vnic_dev *, int),\n\tint (*finished)(struct vnic_dev *, int *),\n\tint arg)\n{\n\tunsigned long time;\n\tint done;\n\tint err;\n\n\terr = start(vdev, arg);\n\tif (err)\n\t\treturn err;\n\n\t \n\n\ttime = jiffies + (HZ * 2);\n\tdo {\n\n\t\terr = finished(vdev, &done);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (done)\n\t\t\treturn 0;\n\n\t\tschedule_timeout_uninterruptible(HZ / 10);\n\n\t} while (time_after(time, jiffies));\n\n\treturn -ETIMEDOUT;\n}\n\nstatic int enic_dev_open(struct enic *enic)\n{\n\tint err;\n\tu32 flags = CMD_OPENF_IG_DESCCACHE;\n\n\terr = enic_dev_wait(enic->vdev, vnic_dev_open,\n\t\tvnic_dev_open_done, flags);\n\tif (err)\n\t\tdev_err(enic_get_dev(enic), \"vNIC device open failed, err %d\\n\",\n\t\t\terr);\n\n\treturn err;\n}\n\nstatic int enic_dev_soft_reset(struct enic *enic)\n{\n\tint err;\n\n\terr = enic_dev_wait(enic->vdev, vnic_dev_soft_reset,\n\t\t\t    vnic_dev_soft_reset_done, 0);\n\tif (err)\n\t\tnetdev_err(enic->netdev, \"vNIC soft reset failed, err %d\\n\",\n\t\t\t   err);\n\n\treturn err;\n}\n\nstatic int enic_dev_hang_reset(struct enic *enic)\n{\n\tint err;\n\n\terr = enic_dev_wait(enic->vdev, vnic_dev_hang_reset,\n\t\tvnic_dev_hang_reset_done, 0);\n\tif (err)\n\t\tnetdev_err(enic->netdev, \"vNIC hang reset failed, err %d\\n\",\n\t\t\terr);\n\n\treturn err;\n}\n\nint __enic_set_rsskey(struct enic *enic)\n{\n\tunion vnic_rss_key *rss_key_buf_va;\n\tdma_addr_t rss_key_buf_pa;\n\tint i, kidx, bidx, err;\n\n\trss_key_buf_va = dma_alloc_coherent(&enic->pdev->dev,\n\t\t\t\t\t    sizeof(union vnic_rss_key),\n\t\t\t\t\t    &rss_key_buf_pa, GFP_ATOMIC);\n\tif (!rss_key_buf_va)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < ENIC_RSS_LEN; i++) {\n\t\tkidx = i / ENIC_RSS_BYTES_PER_KEY;\n\t\tbidx = i % ENIC_RSS_BYTES_PER_KEY;\n\t\trss_key_buf_va->key[kidx].b[bidx] = enic->rss_key[i];\n\t}\n\tspin_lock_bh(&enic->devcmd_lock);\n\terr = enic_set_rss_key(enic,\n\t\trss_key_buf_pa,\n\t\tsizeof(union vnic_rss_key));\n\tspin_unlock_bh(&enic->devcmd_lock);\n\n\tdma_free_coherent(&enic->pdev->dev, sizeof(union vnic_rss_key),\n\t\t\t  rss_key_buf_va, rss_key_buf_pa);\n\n\treturn err;\n}\n\nstatic int enic_set_rsskey(struct enic *enic)\n{\n\tnetdev_rss_key_fill(enic->rss_key, ENIC_RSS_LEN);\n\n\treturn __enic_set_rsskey(enic);\n}\n\nstatic int enic_set_rsscpu(struct enic *enic, u8 rss_hash_bits)\n{\n\tdma_addr_t rss_cpu_buf_pa;\n\tunion vnic_rss_cpu *rss_cpu_buf_va = NULL;\n\tunsigned int i;\n\tint err;\n\n\trss_cpu_buf_va = dma_alloc_coherent(&enic->pdev->dev,\n\t\t\t\t\t    sizeof(union vnic_rss_cpu),\n\t\t\t\t\t    &rss_cpu_buf_pa, GFP_ATOMIC);\n\tif (!rss_cpu_buf_va)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < (1 << rss_hash_bits); i++)\n\t\t(*rss_cpu_buf_va).cpu[i/4].b[i%4] = i % enic->rq_count;\n\n\tspin_lock_bh(&enic->devcmd_lock);\n\terr = enic_set_rss_cpu(enic,\n\t\trss_cpu_buf_pa,\n\t\tsizeof(union vnic_rss_cpu));\n\tspin_unlock_bh(&enic->devcmd_lock);\n\n\tdma_free_coherent(&enic->pdev->dev, sizeof(union vnic_rss_cpu),\n\t\t\t  rss_cpu_buf_va, rss_cpu_buf_pa);\n\n\treturn err;\n}\n\nstatic int enic_set_niccfg(struct enic *enic, u8 rss_default_cpu,\n\tu8 rss_hash_type, u8 rss_hash_bits, u8 rss_base_cpu, u8 rss_enable)\n{\n\tconst u8 tso_ipid_split_en = 0;\n\tconst u8 ig_vlan_strip_en = 1;\n\tint err;\n\n\t \n\n\tspin_lock_bh(&enic->devcmd_lock);\n\terr = enic_set_nic_cfg(enic,\n\t\trss_default_cpu, rss_hash_type,\n\t\trss_hash_bits, rss_base_cpu,\n\t\trss_enable, tso_ipid_split_en,\n\t\tig_vlan_strip_en);\n\tspin_unlock_bh(&enic->devcmd_lock);\n\n\treturn err;\n}\n\nstatic int enic_set_rss_nic_cfg(struct enic *enic)\n{\n\tstruct device *dev = enic_get_dev(enic);\n\tconst u8 rss_default_cpu = 0;\n\tconst u8 rss_hash_bits = 7;\n\tconst u8 rss_base_cpu = 0;\n\tu8 rss_hash_type;\n\tint res;\n\tu8 rss_enable = ENIC_SETTING(enic, RSS) && (enic->rq_count > 1);\n\n\tspin_lock_bh(&enic->devcmd_lock);\n\tres = vnic_dev_capable_rss_hash_type(enic->vdev, &rss_hash_type);\n\tspin_unlock_bh(&enic->devcmd_lock);\n\tif (res) {\n\t\t \n\t\trss_hash_type = NIC_CFG_RSS_HASH_TYPE_IPV4\t|\n\t\t\t\tNIC_CFG_RSS_HASH_TYPE_TCP_IPV4\t|\n\t\t\t\tNIC_CFG_RSS_HASH_TYPE_IPV6\t|\n\t\t\t\tNIC_CFG_RSS_HASH_TYPE_TCP_IPV6;\n\t}\n\n\tif (rss_enable) {\n\t\tif (!enic_set_rsskey(enic)) {\n\t\t\tif (enic_set_rsscpu(enic, rss_hash_bits)) {\n\t\t\t\trss_enable = 0;\n\t\t\t\tdev_warn(dev, \"RSS disabled, \"\n\t\t\t\t\t\"Failed to set RSS cpu indirection table.\");\n\t\t\t}\n\t\t} else {\n\t\t\trss_enable = 0;\n\t\t\tdev_warn(dev, \"RSS disabled, Failed to set RSS key.\\n\");\n\t\t}\n\t}\n\n\treturn enic_set_niccfg(enic, rss_default_cpu, rss_hash_type,\n\t\trss_hash_bits, rss_base_cpu, rss_enable);\n}\n\nstatic void enic_set_api_busy(struct enic *enic, bool busy)\n{\n\tspin_lock(&enic->enic_api_lock);\n\tenic->enic_api_busy = busy;\n\tspin_unlock(&enic->enic_api_lock);\n}\n\nstatic void enic_reset(struct work_struct *work)\n{\n\tstruct enic *enic = container_of(work, struct enic, reset);\n\n\tif (!netif_running(enic->netdev))\n\t\treturn;\n\n\trtnl_lock();\n\n\t \n\tenic_set_api_busy(enic, true);\n\n\tenic_stop(enic->netdev);\n\tenic_dev_soft_reset(enic);\n\tenic_reset_addr_lists(enic);\n\tenic_init_vnic_resources(enic);\n\tenic_set_rss_nic_cfg(enic);\n\tenic_dev_set_ig_vlan_rewrite_mode(enic);\n\tenic_open(enic->netdev);\n\n\t \n\tenic_set_api_busy(enic, false);\n\n\tcall_netdevice_notifiers(NETDEV_REBOOT, enic->netdev);\n\n\trtnl_unlock();\n}\n\nstatic void enic_tx_hang_reset(struct work_struct *work)\n{\n\tstruct enic *enic = container_of(work, struct enic, tx_hang_reset);\n\n\trtnl_lock();\n\n\t \n\tenic_set_api_busy(enic, true);\n\n\tenic_dev_hang_notify(enic);\n\tenic_stop(enic->netdev);\n\tenic_dev_hang_reset(enic);\n\tenic_reset_addr_lists(enic);\n\tenic_init_vnic_resources(enic);\n\tenic_set_rss_nic_cfg(enic);\n\tenic_dev_set_ig_vlan_rewrite_mode(enic);\n\tenic_open(enic->netdev);\n\n\t \n\tenic_set_api_busy(enic, false);\n\n\tcall_netdevice_notifiers(NETDEV_REBOOT, enic->netdev);\n\n\trtnl_unlock();\n}\n\nstatic int enic_set_intr_mode(struct enic *enic)\n{\n\tunsigned int n = min_t(unsigned int, enic->rq_count, ENIC_RQ_MAX);\n\tunsigned int m = min_t(unsigned int, enic->wq_count, ENIC_WQ_MAX);\n\tunsigned int i;\n\n\t \n\n\tBUG_ON(ARRAY_SIZE(enic->msix_entry) < n + m + 2);\n\tfor (i = 0; i < n + m + 2; i++)\n\t\tenic->msix_entry[i].entry = i;\n\n\t \n\n\tif (ENIC_SETTING(enic, RSS) &&\n\t    enic->config.intr_mode < 1 &&\n\t    enic->rq_count >= n &&\n\t    enic->wq_count >= m &&\n\t    enic->cq_count >= n + m &&\n\t    enic->intr_count >= n + m + 2) {\n\n\t\tif (pci_enable_msix_range(enic->pdev, enic->msix_entry,\n\t\t\t\t\t  n + m + 2, n + m + 2) > 0) {\n\n\t\t\tenic->rq_count = n;\n\t\t\tenic->wq_count = m;\n\t\t\tenic->cq_count = n + m;\n\t\t\tenic->intr_count = n + m + 2;\n\n\t\t\tvnic_dev_set_intr_mode(enic->vdev,\n\t\t\t\tVNIC_DEV_INTR_MODE_MSIX);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (enic->config.intr_mode < 1 &&\n\t    enic->rq_count >= 1 &&\n\t    enic->wq_count >= m &&\n\t    enic->cq_count >= 1 + m &&\n\t    enic->intr_count >= 1 + m + 2) {\n\t\tif (pci_enable_msix_range(enic->pdev, enic->msix_entry,\n\t\t\t\t\t  1 + m + 2, 1 + m + 2) > 0) {\n\n\t\t\tenic->rq_count = 1;\n\t\t\tenic->wq_count = m;\n\t\t\tenic->cq_count = 1 + m;\n\t\t\tenic->intr_count = 1 + m + 2;\n\n\t\t\tvnic_dev_set_intr_mode(enic->vdev,\n\t\t\t\tVNIC_DEV_INTR_MODE_MSIX);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\n\tif (enic->config.intr_mode < 2 &&\n\t    enic->rq_count >= 1 &&\n\t    enic->wq_count >= 1 &&\n\t    enic->cq_count >= 2 &&\n\t    enic->intr_count >= 1 &&\n\t    !pci_enable_msi(enic->pdev)) {\n\n\t\tenic->rq_count = 1;\n\t\tenic->wq_count = 1;\n\t\tenic->cq_count = 2;\n\t\tenic->intr_count = 1;\n\n\t\tvnic_dev_set_intr_mode(enic->vdev, VNIC_DEV_INTR_MODE_MSI);\n\n\t\treturn 0;\n\t}\n\n\t \n\n\tif (enic->config.intr_mode < 3 &&\n\t    enic->rq_count >= 1 &&\n\t    enic->wq_count >= 1 &&\n\t    enic->cq_count >= 2 &&\n\t    enic->intr_count >= 3) {\n\n\t\tenic->rq_count = 1;\n\t\tenic->wq_count = 1;\n\t\tenic->cq_count = 2;\n\t\tenic->intr_count = 3;\n\n\t\tvnic_dev_set_intr_mode(enic->vdev, VNIC_DEV_INTR_MODE_INTX);\n\n\t\treturn 0;\n\t}\n\n\tvnic_dev_set_intr_mode(enic->vdev, VNIC_DEV_INTR_MODE_UNKNOWN);\n\n\treturn -EINVAL;\n}\n\nstatic void enic_clear_intr_mode(struct enic *enic)\n{\n\tswitch (vnic_dev_get_intr_mode(enic->vdev)) {\n\tcase VNIC_DEV_INTR_MODE_MSIX:\n\t\tpci_disable_msix(enic->pdev);\n\t\tbreak;\n\tcase VNIC_DEV_INTR_MODE_MSI:\n\t\tpci_disable_msi(enic->pdev);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tvnic_dev_set_intr_mode(enic->vdev, VNIC_DEV_INTR_MODE_UNKNOWN);\n}\n\nstatic const struct net_device_ops enic_netdev_dynamic_ops = {\n\t.ndo_open\t\t= enic_open,\n\t.ndo_stop\t\t= enic_stop,\n\t.ndo_start_xmit\t\t= enic_hard_start_xmit,\n\t.ndo_get_stats64\t= enic_get_stats,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_rx_mode\t= enic_set_rx_mode,\n\t.ndo_set_mac_address\t= enic_set_mac_address_dynamic,\n\t.ndo_change_mtu\t\t= enic_change_mtu,\n\t.ndo_vlan_rx_add_vid\t= enic_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= enic_vlan_rx_kill_vid,\n\t.ndo_tx_timeout\t\t= enic_tx_timeout,\n\t.ndo_set_vf_port\t= enic_set_vf_port,\n\t.ndo_get_vf_port\t= enic_get_vf_port,\n\t.ndo_set_vf_mac\t\t= enic_set_vf_mac,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= enic_poll_controller,\n#endif\n#ifdef CONFIG_RFS_ACCEL\n\t.ndo_rx_flow_steer\t= enic_rx_flow_steer,\n#endif\n\t.ndo_features_check\t= enic_features_check,\n};\n\nstatic const struct net_device_ops enic_netdev_ops = {\n\t.ndo_open\t\t= enic_open,\n\t.ndo_stop\t\t= enic_stop,\n\t.ndo_start_xmit\t\t= enic_hard_start_xmit,\n\t.ndo_get_stats64\t= enic_get_stats,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_mac_address\t= enic_set_mac_address,\n\t.ndo_set_rx_mode\t= enic_set_rx_mode,\n\t.ndo_change_mtu\t\t= enic_change_mtu,\n\t.ndo_vlan_rx_add_vid\t= enic_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= enic_vlan_rx_kill_vid,\n\t.ndo_tx_timeout\t\t= enic_tx_timeout,\n\t.ndo_set_vf_port\t= enic_set_vf_port,\n\t.ndo_get_vf_port\t= enic_get_vf_port,\n\t.ndo_set_vf_mac\t\t= enic_set_vf_mac,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= enic_poll_controller,\n#endif\n#ifdef CONFIG_RFS_ACCEL\n\t.ndo_rx_flow_steer\t= enic_rx_flow_steer,\n#endif\n\t.ndo_features_check\t= enic_features_check,\n};\n\nstatic void enic_dev_deinit(struct enic *enic)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < enic->rq_count; i++)\n\t\t__netif_napi_del(&enic->napi[i]);\n\n\tif (vnic_dev_get_intr_mode(enic->vdev) == VNIC_DEV_INTR_MODE_MSIX)\n\t\tfor (i = 0; i < enic->wq_count; i++)\n\t\t\t__netif_napi_del(&enic->napi[enic_cq_wq(enic, i)]);\n\n\t \n\tsynchronize_net();\n\n\tenic_free_vnic_resources(enic);\n\tenic_clear_intr_mode(enic);\n\tenic_free_affinity_hint(enic);\n}\n\nstatic void enic_kdump_kernel_config(struct enic *enic)\n{\n\tif (is_kdump_kernel()) {\n\t\tdev_info(enic_get_dev(enic), \"Running from within kdump kernel. Using minimal resources\\n\");\n\t\tenic->rq_count = 1;\n\t\tenic->wq_count = 1;\n\t\tenic->config.rq_desc_count = ENIC_MIN_RQ_DESCS;\n\t\tenic->config.wq_desc_count = ENIC_MIN_WQ_DESCS;\n\t\tenic->config.mtu = min_t(u16, 1500, enic->config.mtu);\n\t}\n}\n\nstatic int enic_dev_init(struct enic *enic)\n{\n\tstruct device *dev = enic_get_dev(enic);\n\tstruct net_device *netdev = enic->netdev;\n\tunsigned int i;\n\tint err;\n\n\t \n\terr = enic_dev_intr_coal_timer_info(enic);\n\tif (err) {\n\t\tdev_warn(dev, \"Using default conversion factor for \"\n\t\t\t\"interrupt coalesce timer\\n\");\n\t\tvnic_dev_intr_coal_timer_info_default(enic->vdev);\n\t}\n\n\t \n\n\terr = enic_get_vnic_config(enic);\n\tif (err) {\n\t\tdev_err(dev, \"Get vNIC configuration failed, aborting\\n\");\n\t\treturn err;\n\t}\n\n\t \n\n\tenic_get_res_counts(enic);\n\n\t \n\tenic_kdump_kernel_config(enic);\n\n\t \n\n\terr = enic_set_intr_mode(enic);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to set intr mode based on resource \"\n\t\t\t\"counts and system capabilities, aborting\\n\");\n\t\treturn err;\n\t}\n\n\t \n\n\terr = enic_alloc_vnic_resources(enic);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to alloc vNIC resources, aborting\\n\");\n\t\tgoto err_out_free_vnic_resources;\n\t}\n\n\tenic_init_vnic_resources(enic);\n\n\terr = enic_set_rss_nic_cfg(enic);\n\tif (err) {\n\t\tdev_err(dev, \"Failed to config nic, aborting\\n\");\n\t\tgoto err_out_free_vnic_resources;\n\t}\n\n\tswitch (vnic_dev_get_intr_mode(enic->vdev)) {\n\tdefault:\n\t\tnetif_napi_add(netdev, &enic->napi[0], enic_poll);\n\t\tbreak;\n\tcase VNIC_DEV_INTR_MODE_MSIX:\n\t\tfor (i = 0; i < enic->rq_count; i++) {\n\t\t\tnetif_napi_add(netdev, &enic->napi[i],\n\t\t\t\t       enic_poll_msix_rq);\n\t\t}\n\t\tfor (i = 0; i < enic->wq_count; i++)\n\t\t\tnetif_napi_add(netdev,\n\t\t\t\t       &enic->napi[enic_cq_wq(enic, i)],\n\t\t\t\t       enic_poll_msix_wq);\n\t\tbreak;\n\t}\n\n\treturn 0;\n\nerr_out_free_vnic_resources:\n\tenic_free_affinity_hint(enic);\n\tenic_clear_intr_mode(enic);\n\tenic_free_vnic_resources(enic);\n\n\treturn err;\n}\n\nstatic void enic_iounmap(struct enic *enic)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(enic->bar); i++)\n\t\tif (enic->bar[i].vaddr)\n\t\t\tiounmap(enic->bar[i].vaddr);\n}\n\nstatic int enic_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct net_device *netdev;\n\tstruct enic *enic;\n\tint using_dac = 0;\n\tunsigned int i;\n\tint err;\n#ifdef CONFIG_PCI_IOV\n\tint pos = 0;\n#endif\n\tint num_pps = 1;\n\n\t \n\n\tnetdev = alloc_etherdev_mqs(sizeof(struct enic),\n\t\t\t\t    ENIC_RQ_MAX, ENIC_WQ_MAX);\n\tif (!netdev)\n\t\treturn -ENOMEM;\n\n\tpci_set_drvdata(pdev, netdev);\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\n\tenic = netdev_priv(netdev);\n\tenic->netdev = netdev;\n\tenic->pdev = pdev;\n\n\t \n\n\terr = pci_enable_device_mem(pdev);\n\tif (err) {\n\t\tdev_err(dev, \"Cannot enable PCI device, aborting\\n\");\n\t\tgoto err_out_free_netdev;\n\t}\n\n\terr = pci_request_regions(pdev, DRV_NAME);\n\tif (err) {\n\t\tdev_err(dev, \"Cannot request PCI regions, aborting\\n\");\n\t\tgoto err_out_disable_device;\n\t}\n\n\tpci_set_master(pdev);\n\n\t \n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(47));\n\tif (err) {\n\t\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (err) {\n\t\t\tdev_err(dev, \"No usable DMA configuration, aborting\\n\");\n\t\t\tgoto err_out_release_regions;\n\t\t}\n\t} else {\n\t\tusing_dac = 1;\n\t}\n\n\t \n\n\tfor (i = 0; i < ARRAY_SIZE(enic->bar); i++) {\n\t\tif (!(pci_resource_flags(pdev, i) & IORESOURCE_MEM))\n\t\t\tcontinue;\n\t\tenic->bar[i].len = pci_resource_len(pdev, i);\n\t\tenic->bar[i].vaddr = pci_iomap(pdev, i, enic->bar[i].len);\n\t\tif (!enic->bar[i].vaddr) {\n\t\t\tdev_err(dev, \"Cannot memory-map BAR %d, aborting\\n\", i);\n\t\t\terr = -ENODEV;\n\t\t\tgoto err_out_iounmap;\n\t\t}\n\t\tenic->bar[i].bus_addr = pci_resource_start(pdev, i);\n\t}\n\n\t \n\n\tenic->vdev = vnic_dev_register(NULL, enic, pdev, enic->bar,\n\t\tARRAY_SIZE(enic->bar));\n\tif (!enic->vdev) {\n\t\tdev_err(dev, \"vNIC registration failed, aborting\\n\");\n\t\terr = -ENODEV;\n\t\tgoto err_out_iounmap;\n\t}\n\n\terr = vnic_devcmd_init(enic->vdev);\n\n\tif (err)\n\t\tgoto err_out_vnic_unregister;\n\n#ifdef CONFIG_PCI_IOV\n\t \n\tpos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_SRIOV);\n\tif (pos) {\n\t\tpci_read_config_word(pdev, pos + PCI_SRIOV_TOTAL_VF,\n\t\t\t&enic->num_vfs);\n\t\tif (enic->num_vfs) {\n\t\t\terr = pci_enable_sriov(pdev, enic->num_vfs);\n\t\t\tif (err) {\n\t\t\t\tdev_err(dev, \"SRIOV enable failed, aborting.\"\n\t\t\t\t\t\" pci_enable_sriov() returned %d\\n\",\n\t\t\t\t\terr);\n\t\t\t\tgoto err_out_vnic_unregister;\n\t\t\t}\n\t\t\tenic->priv_flags |= ENIC_SRIOV_ENABLED;\n\t\t\tnum_pps = enic->num_vfs;\n\t\t}\n\t}\n#endif\n\n\t \n\tenic->pp = kcalloc(num_pps, sizeof(*enic->pp), GFP_KERNEL);\n\tif (!enic->pp) {\n\t\terr = -ENOMEM;\n\t\tgoto err_out_disable_sriov_pp;\n\t}\n\n\t \n\n\terr = enic_dev_open(enic);\n\tif (err) {\n\t\tdev_err(dev, \"vNIC dev open failed, aborting\\n\");\n\t\tgoto err_out_disable_sriov;\n\t}\n\n\t \n\n\tspin_lock_init(&enic->devcmd_lock);\n\tspin_lock_init(&enic->enic_api_lock);\n\n\t \n\n\terr = enic_dev_set_ig_vlan_rewrite_mode(enic);\n\tif (err) {\n\t\tdev_err(dev,\n\t\t\t\"Failed to set ingress vlan rewrite mode, aborting.\\n\");\n\t\tgoto err_out_dev_close;\n\t}\n\n\t \n\n\tnetif_carrier_off(netdev);\n\n\t \n\n\tif (!enic_is_dynamic(enic)) {\n\t\terr = vnic_dev_init(enic->vdev, 0);\n\t\tif (err) {\n\t\t\tdev_err(dev, \"vNIC dev init failed, aborting\\n\");\n\t\t\tgoto err_out_dev_close;\n\t\t}\n\t}\n\n\terr = enic_dev_init(enic);\n\tif (err) {\n\t\tdev_err(dev, \"Device initialization failed, aborting\\n\");\n\t\tgoto err_out_dev_close;\n\t}\n\n\tnetif_set_real_num_tx_queues(netdev, enic->wq_count);\n\tnetif_set_real_num_rx_queues(netdev, enic->rq_count);\n\n\t \n\n\ttimer_setup(&enic->notify_timer, enic_notify_timer, 0);\n\n\tenic_rfs_flw_tbl_init(enic);\n\tenic_set_rx_coal_setting(enic);\n\tINIT_WORK(&enic->reset, enic_reset);\n\tINIT_WORK(&enic->tx_hang_reset, enic_tx_hang_reset);\n\tINIT_WORK(&enic->change_mtu_work, enic_change_mtu_work);\n\n\tfor (i = 0; i < enic->wq_count; i++)\n\t\tspin_lock_init(&enic->wq_lock[i]);\n\n\t \n\n\tenic->port_mtu = enic->config.mtu;\n\n\terr = enic_set_mac_addr(netdev, enic->mac_addr);\n\tif (err) {\n\t\tdev_err(dev, \"Invalid MAC address, aborting\\n\");\n\t\tgoto err_out_dev_deinit;\n\t}\n\n\tenic->tx_coalesce_usecs = enic->config.intr_timer_usec;\n\t \n\tenic->rx_coalesce_usecs = enic->tx_coalesce_usecs;\n\n\tif (enic_is_dynamic(enic) || enic_is_sriov_vf(enic))\n\t\tnetdev->netdev_ops = &enic_netdev_dynamic_ops;\n\telse\n\t\tnetdev->netdev_ops = &enic_netdev_ops;\n\n\tnetdev->watchdog_timeo = 2 * HZ;\n\tenic_set_ethtool_ops(netdev);\n\n\tnetdev->features |= NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;\n\tif (ENIC_SETTING(enic, LOOP)) {\n\t\tnetdev->features &= ~NETIF_F_HW_VLAN_CTAG_TX;\n\t\tenic->loop_enable = 1;\n\t\tenic->loop_tag = enic->config.loop_tag;\n\t\tdev_info(dev, \"loopback tag=0x%04x\\n\", enic->loop_tag);\n\t}\n\tif (ENIC_SETTING(enic, TXCSUM))\n\t\tnetdev->hw_features |= NETIF_F_SG | NETIF_F_HW_CSUM;\n\tif (ENIC_SETTING(enic, TSO))\n\t\tnetdev->hw_features |= NETIF_F_TSO |\n\t\t\tNETIF_F_TSO6 | NETIF_F_TSO_ECN;\n\tif (ENIC_SETTING(enic, RSS))\n\t\tnetdev->hw_features |= NETIF_F_RXHASH;\n\tif (ENIC_SETTING(enic, RXCSUM))\n\t\tnetdev->hw_features |= NETIF_F_RXCSUM;\n\tif (ENIC_SETTING(enic, VXLAN)) {\n\t\tu64 patch_level;\n\t\tu64 a1 = 0;\n\n\t\tnetdev->hw_enc_features |= NETIF_F_RXCSUM\t\t|\n\t\t\t\t\t   NETIF_F_TSO\t\t\t|\n\t\t\t\t\t   NETIF_F_TSO6\t\t\t|\n\t\t\t\t\t   NETIF_F_TSO_ECN\t\t|\n\t\t\t\t\t   NETIF_F_GSO_UDP_TUNNEL\t|\n\t\t\t\t\t   NETIF_F_HW_CSUM\t\t|\n\t\t\t\t\t   NETIF_F_GSO_UDP_TUNNEL_CSUM;\n\t\tnetdev->hw_features |= netdev->hw_enc_features;\n\t\t \n\t\terr = vnic_dev_get_supported_feature_ver(enic->vdev,\n\t\t\t\t\t\t\t VIC_FEATURE_VXLAN,\n\t\t\t\t\t\t\t &patch_level, &a1);\n\t\tif (err)\n\t\t\tpatch_level = 0;\n\t\tenic->vxlan.flags = (u8)a1;\n\t\t \n\t\tpatch_level &= BIT_ULL(0) | BIT_ULL(2);\n\t\tpatch_level = fls(patch_level);\n\t\tpatch_level = patch_level ? patch_level - 1 : 0;\n\t\tenic->vxlan.patch_level = patch_level;\n\n\t\tif (vnic_dev_get_res_count(enic->vdev, RES_TYPE_WQ) == 1 ||\n\t\t    enic->vxlan.flags & ENIC_VXLAN_MULTI_WQ) {\n\t\t\tnetdev->udp_tunnel_nic_info = &enic_udp_tunnels_v4;\n\t\t\tif (enic->vxlan.flags & ENIC_VXLAN_OUTER_IPV6)\n\t\t\t\tnetdev->udp_tunnel_nic_info = &enic_udp_tunnels;\n\t\t}\n\t}\n\n\tnetdev->features |= netdev->hw_features;\n\tnetdev->vlan_features |= netdev->features;\n\n#ifdef CONFIG_RFS_ACCEL\n\tnetdev->hw_features |= NETIF_F_NTUPLE;\n#endif\n\n\tif (using_dac)\n\t\tnetdev->features |= NETIF_F_HIGHDMA;\n\n\tnetdev->priv_flags |= IFF_UNICAST_FLT;\n\n\t \n\tnetdev->min_mtu = ENIC_MIN_MTU;\n\tnetdev->max_mtu = ENIC_MAX_MTU;\n\tnetdev->mtu\t= enic->port_mtu;\n\n\terr = register_netdev(netdev);\n\tif (err) {\n\t\tdev_err(dev, \"Cannot register net device, aborting\\n\");\n\t\tgoto err_out_dev_deinit;\n\t}\n\tenic->rx_copybreak = RX_COPYBREAK_DEFAULT;\n\n\treturn 0;\n\nerr_out_dev_deinit:\n\tenic_dev_deinit(enic);\nerr_out_dev_close:\n\tvnic_dev_close(enic->vdev);\nerr_out_disable_sriov:\n\tkfree(enic->pp);\nerr_out_disable_sriov_pp:\n#ifdef CONFIG_PCI_IOV\n\tif (enic_sriov_enabled(enic)) {\n\t\tpci_disable_sriov(pdev);\n\t\tenic->priv_flags &= ~ENIC_SRIOV_ENABLED;\n\t}\n#endif\nerr_out_vnic_unregister:\n\tvnic_dev_unregister(enic->vdev);\nerr_out_iounmap:\n\tenic_iounmap(enic);\nerr_out_release_regions:\n\tpci_release_regions(pdev);\nerr_out_disable_device:\n\tpci_disable_device(pdev);\nerr_out_free_netdev:\n\tfree_netdev(netdev);\n\n\treturn err;\n}\n\nstatic void enic_remove(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\n\tif (netdev) {\n\t\tstruct enic *enic = netdev_priv(netdev);\n\n\t\tcancel_work_sync(&enic->reset);\n\t\tcancel_work_sync(&enic->change_mtu_work);\n\t\tunregister_netdev(netdev);\n\t\tenic_dev_deinit(enic);\n\t\tvnic_dev_close(enic->vdev);\n#ifdef CONFIG_PCI_IOV\n\t\tif (enic_sriov_enabled(enic)) {\n\t\t\tpci_disable_sriov(pdev);\n\t\t\tenic->priv_flags &= ~ENIC_SRIOV_ENABLED;\n\t\t}\n#endif\n\t\tkfree(enic->pp);\n\t\tvnic_dev_unregister(enic->vdev);\n\t\tenic_iounmap(enic);\n\t\tpci_release_regions(pdev);\n\t\tpci_disable_device(pdev);\n\t\tfree_netdev(netdev);\n\t}\n}\n\nstatic struct pci_driver enic_driver = {\n\t.name = DRV_NAME,\n\t.id_table = enic_id_table,\n\t.probe = enic_probe,\n\t.remove = enic_remove,\n};\n\nmodule_pci_driver(enic_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}