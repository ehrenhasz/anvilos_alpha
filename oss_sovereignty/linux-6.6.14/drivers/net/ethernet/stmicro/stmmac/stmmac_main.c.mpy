{
  "module_name": "stmmac_main.c",
  "hash_id": "0f2f5268501c20e33eff66d42740f00caf59d10b1c33a8b09ea7b3d2748e047e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c",
  "human_readable_source": "\n \n\n#include <linux/clk.h>\n#include <linux/kernel.h>\n#include <linux/interrupt.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/skbuff.h>\n#include <linux/ethtool.h>\n#include <linux/if_ether.h>\n#include <linux/crc32.h>\n#include <linux/mii.h>\n#include <linux/if.h>\n#include <linux/if_vlan.h>\n#include <linux/dma-mapping.h>\n#include <linux/slab.h>\n#include <linux/pm_runtime.h>\n#include <linux/prefetch.h>\n#include <linux/pinctrl/consumer.h>\n#ifdef CONFIG_DEBUG_FS\n#include <linux/debugfs.h>\n#include <linux/seq_file.h>\n#endif  \n#include <linux/net_tstamp.h>\n#include <linux/phylink.h>\n#include <linux/udp.h>\n#include <linux/bpf_trace.h>\n#include <net/page_pool/helpers.h>\n#include <net/pkt_cls.h>\n#include <net/xdp_sock_drv.h>\n#include \"stmmac_ptp.h\"\n#include \"stmmac.h\"\n#include \"stmmac_xdp.h\"\n#include <linux/reset.h>\n#include <linux/of_mdio.h>\n#include \"dwmac1000.h\"\n#include \"dwxgmac2.h\"\n#include \"hwif.h\"\n\n \n#define STMMAC_HWTS_ACTIVE\t(PTP_TCR_TSENA | PTP_TCR_TSCFUPDT | \\\n\t\t\t\t PTP_TCR_TSCTRLSSR)\n\n#define\tSTMMAC_ALIGN(x)\t\tALIGN(ALIGN(x, SMP_CACHE_BYTES), 16)\n#define\tTSO_MAX_BUFF_SIZE\t(SZ_16K - 1)\n\n \n#define TX_TIMEO\t5000\nstatic int watchdog = TX_TIMEO;\nmodule_param(watchdog, int, 0644);\nMODULE_PARM_DESC(watchdog, \"Transmit timeout in milliseconds (default 5s)\");\n\nstatic int debug = -1;\nmodule_param(debug, int, 0644);\nMODULE_PARM_DESC(debug, \"Message Level (-1: default, 0: no output, 16: all)\");\n\nstatic int phyaddr = -1;\nmodule_param(phyaddr, int, 0444);\nMODULE_PARM_DESC(phyaddr, \"Physical device address\");\n\n#define STMMAC_TX_THRESH(x)\t((x)->dma_conf.dma_tx_size / 4)\n#define STMMAC_RX_THRESH(x)\t((x)->dma_conf.dma_rx_size / 4)\n\n \n#define STMMAC_XSK_TX_BUDGET_MAX\t256\n#define STMMAC_TX_XSK_AVAIL\t\t16\n#define STMMAC_RX_FILL_BATCH\t\t16\n\n#define STMMAC_XDP_PASS\t\t0\n#define STMMAC_XDP_CONSUMED\tBIT(0)\n#define STMMAC_XDP_TX\t\tBIT(1)\n#define STMMAC_XDP_REDIRECT\tBIT(2)\n\nstatic int flow_ctrl = FLOW_AUTO;\nmodule_param(flow_ctrl, int, 0644);\nMODULE_PARM_DESC(flow_ctrl, \"Flow control ability [on/off]\");\n\nstatic int pause = PAUSE_TIME;\nmodule_param(pause, int, 0644);\nMODULE_PARM_DESC(pause, \"Flow Control Pause Time\");\n\n#define TC_DEFAULT 64\nstatic int tc = TC_DEFAULT;\nmodule_param(tc, int, 0644);\nMODULE_PARM_DESC(tc, \"DMA threshold control value\");\n\n#define\tDEFAULT_BUFSIZE\t1536\nstatic int buf_sz = DEFAULT_BUFSIZE;\nmodule_param(buf_sz, int, 0644);\nMODULE_PARM_DESC(buf_sz, \"DMA buffer size\");\n\n#define\tSTMMAC_RX_COPYBREAK\t256\n\nstatic const u32 default_msg_level = (NETIF_MSG_DRV | NETIF_MSG_PROBE |\n\t\t\t\t      NETIF_MSG_LINK | NETIF_MSG_IFUP |\n\t\t\t\t      NETIF_MSG_IFDOWN | NETIF_MSG_TIMER);\n\n#define STMMAC_DEFAULT_LPI_TIMER\t1000\nstatic int eee_timer = STMMAC_DEFAULT_LPI_TIMER;\nmodule_param(eee_timer, int, 0644);\nMODULE_PARM_DESC(eee_timer, \"LPI tx expiration time in msec\");\n#define STMMAC_LPI_T(x) (jiffies + usecs_to_jiffies(x))\n\n \nstatic unsigned int chain_mode;\nmodule_param(chain_mode, int, 0444);\nMODULE_PARM_DESC(chain_mode, \"To use chain instead of ring mode\");\n\nstatic irqreturn_t stmmac_interrupt(int irq, void *dev_id);\n \nstatic irqreturn_t stmmac_mac_interrupt(int irq, void *dev_id);\nstatic irqreturn_t stmmac_safety_interrupt(int irq, void *dev_id);\nstatic irqreturn_t stmmac_msi_intr_tx(int irq, void *data);\nstatic irqreturn_t stmmac_msi_intr_rx(int irq, void *data);\nstatic void stmmac_reset_rx_queue(struct stmmac_priv *priv, u32 queue);\nstatic void stmmac_reset_tx_queue(struct stmmac_priv *priv, u32 queue);\nstatic void stmmac_reset_queues_param(struct stmmac_priv *priv);\nstatic void stmmac_tx_timer_arm(struct stmmac_priv *priv, u32 queue);\nstatic void stmmac_flush_tx_descriptors(struct stmmac_priv *priv, int queue);\nstatic void stmmac_set_dma_operation_mode(struct stmmac_priv *priv, u32 txmode,\n\t\t\t\t\t  u32 rxmode, u32 chan);\n\n#ifdef CONFIG_DEBUG_FS\nstatic const struct net_device_ops stmmac_netdev_ops;\nstatic void stmmac_init_fs(struct net_device *dev);\nstatic void stmmac_exit_fs(struct net_device *dev);\n#endif\n\n#define STMMAC_COAL_TIMER(x) (ns_to_ktime((x) * NSEC_PER_USEC))\n\nint stmmac_bus_clks_config(struct stmmac_priv *priv, bool enabled)\n{\n\tint ret = 0;\n\n\tif (enabled) {\n\t\tret = clk_prepare_enable(priv->plat->stmmac_clk);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = clk_prepare_enable(priv->plat->pclk);\n\t\tif (ret) {\n\t\t\tclk_disable_unprepare(priv->plat->stmmac_clk);\n\t\t\treturn ret;\n\t\t}\n\t\tif (priv->plat->clks_config) {\n\t\t\tret = priv->plat->clks_config(priv->plat->bsp_priv, enabled);\n\t\t\tif (ret) {\n\t\t\t\tclk_disable_unprepare(priv->plat->stmmac_clk);\n\t\t\t\tclk_disable_unprepare(priv->plat->pclk);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tclk_disable_unprepare(priv->plat->stmmac_clk);\n\t\tclk_disable_unprepare(priv->plat->pclk);\n\t\tif (priv->plat->clks_config)\n\t\t\tpriv->plat->clks_config(priv->plat->bsp_priv, enabled);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(stmmac_bus_clks_config);\n\n \nstatic void stmmac_verify_args(void)\n{\n\tif (unlikely(watchdog < 0))\n\t\twatchdog = TX_TIMEO;\n\tif (unlikely((buf_sz < DEFAULT_BUFSIZE) || (buf_sz > BUF_SIZE_16KiB)))\n\t\tbuf_sz = DEFAULT_BUFSIZE;\n\tif (unlikely(flow_ctrl > 1))\n\t\tflow_ctrl = FLOW_AUTO;\n\telse if (likely(flow_ctrl < 0))\n\t\tflow_ctrl = FLOW_OFF;\n\tif (unlikely((pause < 0) || (pause > 0xffff)))\n\t\tpause = PAUSE_TIME;\n\tif (eee_timer < 0)\n\t\teee_timer = STMMAC_DEFAULT_LPI_TIMER;\n}\n\nstatic void __stmmac_disable_all_queues(struct stmmac_priv *priv)\n{\n\tu32 rx_queues_cnt = priv->plat->rx_queues_to_use;\n\tu32 tx_queues_cnt = priv->plat->tx_queues_to_use;\n\tu32 maxq = max(rx_queues_cnt, tx_queues_cnt);\n\tu32 queue;\n\n\tfor (queue = 0; queue < maxq; queue++) {\n\t\tstruct stmmac_channel *ch = &priv->channel[queue];\n\n\t\tif (stmmac_xdp_is_enabled(priv) &&\n\t\t    test_bit(queue, priv->af_xdp_zc_qps)) {\n\t\t\tnapi_disable(&ch->rxtx_napi);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (queue < rx_queues_cnt)\n\t\t\tnapi_disable(&ch->rx_napi);\n\t\tif (queue < tx_queues_cnt)\n\t\t\tnapi_disable(&ch->tx_napi);\n\t}\n}\n\n \nstatic void stmmac_disable_all_queues(struct stmmac_priv *priv)\n{\n\tu32 rx_queues_cnt = priv->plat->rx_queues_to_use;\n\tstruct stmmac_rx_queue *rx_q;\n\tu32 queue;\n\n\t \n\tfor (queue = 0; queue < rx_queues_cnt; queue++) {\n\t\trx_q = &priv->dma_conf.rx_queue[queue];\n\t\tif (rx_q->xsk_pool) {\n\t\t\tsynchronize_rcu();\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t__stmmac_disable_all_queues(priv);\n}\n\n \nstatic void stmmac_enable_all_queues(struct stmmac_priv *priv)\n{\n\tu32 rx_queues_cnt = priv->plat->rx_queues_to_use;\n\tu32 tx_queues_cnt = priv->plat->tx_queues_to_use;\n\tu32 maxq = max(rx_queues_cnt, tx_queues_cnt);\n\tu32 queue;\n\n\tfor (queue = 0; queue < maxq; queue++) {\n\t\tstruct stmmac_channel *ch = &priv->channel[queue];\n\n\t\tif (stmmac_xdp_is_enabled(priv) &&\n\t\t    test_bit(queue, priv->af_xdp_zc_qps)) {\n\t\t\tnapi_enable(&ch->rxtx_napi);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (queue < rx_queues_cnt)\n\t\t\tnapi_enable(&ch->rx_napi);\n\t\tif (queue < tx_queues_cnt)\n\t\t\tnapi_enable(&ch->tx_napi);\n\t}\n}\n\nstatic void stmmac_service_event_schedule(struct stmmac_priv *priv)\n{\n\tif (!test_bit(STMMAC_DOWN, &priv->state) &&\n\t    !test_and_set_bit(STMMAC_SERVICE_SCHED, &priv->state))\n\t\tqueue_work(priv->wq, &priv->service_task);\n}\n\nstatic void stmmac_global_err(struct stmmac_priv *priv)\n{\n\tnetif_carrier_off(priv->dev);\n\tset_bit(STMMAC_RESET_REQUESTED, &priv->state);\n\tstmmac_service_event_schedule(priv);\n}\n\n \nstatic void stmmac_clk_csr_set(struct stmmac_priv *priv)\n{\n\tu32 clk_rate;\n\n\tclk_rate = clk_get_rate(priv->plat->stmmac_clk);\n\n\t \n\tif (!(priv->clk_csr & MAC_CSR_H_FRQ_MASK)) {\n\t\tif (clk_rate < CSR_F_35M)\n\t\t\tpriv->clk_csr = STMMAC_CSR_20_35M;\n\t\telse if ((clk_rate >= CSR_F_35M) && (clk_rate < CSR_F_60M))\n\t\t\tpriv->clk_csr = STMMAC_CSR_35_60M;\n\t\telse if ((clk_rate >= CSR_F_60M) && (clk_rate < CSR_F_100M))\n\t\t\tpriv->clk_csr = STMMAC_CSR_60_100M;\n\t\telse if ((clk_rate >= CSR_F_100M) && (clk_rate < CSR_F_150M))\n\t\t\tpriv->clk_csr = STMMAC_CSR_100_150M;\n\t\telse if ((clk_rate >= CSR_F_150M) && (clk_rate < CSR_F_250M))\n\t\t\tpriv->clk_csr = STMMAC_CSR_150_250M;\n\t\telse if ((clk_rate >= CSR_F_250M) && (clk_rate <= CSR_F_300M))\n\t\t\tpriv->clk_csr = STMMAC_CSR_250_300M;\n\t}\n\n\tif (priv->plat->flags & STMMAC_FLAG_HAS_SUN8I) {\n\t\tif (clk_rate > 160000000)\n\t\t\tpriv->clk_csr = 0x03;\n\t\telse if (clk_rate > 80000000)\n\t\t\tpriv->clk_csr = 0x02;\n\t\telse if (clk_rate > 40000000)\n\t\t\tpriv->clk_csr = 0x01;\n\t\telse\n\t\t\tpriv->clk_csr = 0;\n\t}\n\n\tif (priv->plat->has_xgmac) {\n\t\tif (clk_rate > 400000000)\n\t\t\tpriv->clk_csr = 0x5;\n\t\telse if (clk_rate > 350000000)\n\t\t\tpriv->clk_csr = 0x4;\n\t\telse if (clk_rate > 300000000)\n\t\t\tpriv->clk_csr = 0x3;\n\t\telse if (clk_rate > 250000000)\n\t\t\tpriv->clk_csr = 0x2;\n\t\telse if (clk_rate > 150000000)\n\t\t\tpriv->clk_csr = 0x1;\n\t\telse\n\t\t\tpriv->clk_csr = 0x0;\n\t}\n}\n\nstatic void print_pkt(unsigned char *buf, int len)\n{\n\tpr_debug(\"len = %d byte, buf addr: 0x%p\\n\", len, buf);\n\tprint_hex_dump_bytes(\"\", DUMP_PREFIX_OFFSET, buf, len);\n}\n\nstatic inline u32 stmmac_tx_avail(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\tu32 avail;\n\n\tif (tx_q->dirty_tx > tx_q->cur_tx)\n\t\tavail = tx_q->dirty_tx - tx_q->cur_tx - 1;\n\telse\n\t\tavail = priv->dma_conf.dma_tx_size - tx_q->cur_tx + tx_q->dirty_tx - 1;\n\n\treturn avail;\n}\n\n \nstatic inline u32 stmmac_rx_dirty(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];\n\tu32 dirty;\n\n\tif (rx_q->dirty_rx <= rx_q->cur_rx)\n\t\tdirty = rx_q->cur_rx - rx_q->dirty_rx;\n\telse\n\t\tdirty = priv->dma_conf.dma_rx_size - rx_q->dirty_rx + rx_q->cur_rx;\n\n\treturn dirty;\n}\n\nstatic void stmmac_lpi_entry_timer_config(struct stmmac_priv *priv, bool en)\n{\n\tint tx_lpi_timer;\n\n\t \n\tpriv->eee_sw_timer_en = en ? 0 : 1;\n\ttx_lpi_timer  = en ? priv->tx_lpi_timer : 0;\n\tstmmac_set_eee_lpi_timer(priv, priv->hw, tx_lpi_timer);\n}\n\n \nstatic int stmmac_enable_eee_mode(struct stmmac_priv *priv)\n{\n\tu32 tx_cnt = priv->plat->tx_queues_to_use;\n\tu32 queue;\n\n\t \n\tfor (queue = 0; queue < tx_cnt; queue++) {\n\t\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\n\t\tif (tx_q->dirty_tx != tx_q->cur_tx)\n\t\t\treturn -EBUSY;  \n\t}\n\n\t \n\tif (!priv->tx_path_in_lpi_mode)\n\t\tstmmac_set_eee_mode(priv, priv->hw,\n\t\t\tpriv->plat->flags & STMMAC_FLAG_EN_TX_LPI_CLOCKGATING);\n\treturn 0;\n}\n\n \nvoid stmmac_disable_eee_mode(struct stmmac_priv *priv)\n{\n\tif (!priv->eee_sw_timer_en) {\n\t\tstmmac_lpi_entry_timer_config(priv, 0);\n\t\treturn;\n\t}\n\n\tstmmac_reset_eee_mode(priv, priv->hw);\n\tdel_timer_sync(&priv->eee_ctrl_timer);\n\tpriv->tx_path_in_lpi_mode = false;\n}\n\n \nstatic void stmmac_eee_ctrl_timer(struct timer_list *t)\n{\n\tstruct stmmac_priv *priv = from_timer(priv, t, eee_ctrl_timer);\n\n\tif (stmmac_enable_eee_mode(priv))\n\t\tmod_timer(&priv->eee_ctrl_timer, STMMAC_LPI_T(priv->tx_lpi_timer));\n}\n\n \nbool stmmac_eee_init(struct stmmac_priv *priv)\n{\n\tint eee_tw_timer = priv->eee_tw_timer;\n\n\t \n\tif (priv->hw->pcs == STMMAC_PCS_TBI ||\n\t    priv->hw->pcs == STMMAC_PCS_RTBI)\n\t\treturn false;\n\n\t \n\tif (!priv->dma_cap.eee)\n\t\treturn false;\n\n\tmutex_lock(&priv->lock);\n\n\t \n\tif (!priv->eee_active) {\n\t\tif (priv->eee_enabled) {\n\t\t\tnetdev_dbg(priv->dev, \"disable EEE\\n\");\n\t\t\tstmmac_lpi_entry_timer_config(priv, 0);\n\t\t\tdel_timer_sync(&priv->eee_ctrl_timer);\n\t\t\tstmmac_set_eee_timer(priv, priv->hw, 0, eee_tw_timer);\n\t\t\tif (priv->hw->xpcs)\n\t\t\t\txpcs_config_eee(priv->hw->xpcs,\n\t\t\t\t\t\tpriv->plat->mult_fact_100ns,\n\t\t\t\t\t\tfalse);\n\t\t}\n\t\tmutex_unlock(&priv->lock);\n\t\treturn false;\n\t}\n\n\tif (priv->eee_active && !priv->eee_enabled) {\n\t\ttimer_setup(&priv->eee_ctrl_timer, stmmac_eee_ctrl_timer, 0);\n\t\tstmmac_set_eee_timer(priv, priv->hw, STMMAC_DEFAULT_LIT_LS,\n\t\t\t\t     eee_tw_timer);\n\t\tif (priv->hw->xpcs)\n\t\t\txpcs_config_eee(priv->hw->xpcs,\n\t\t\t\t\tpriv->plat->mult_fact_100ns,\n\t\t\t\t\ttrue);\n\t}\n\n\tif (priv->plat->has_gmac4 && priv->tx_lpi_timer <= STMMAC_ET_MAX) {\n\t\tdel_timer_sync(&priv->eee_ctrl_timer);\n\t\tpriv->tx_path_in_lpi_mode = false;\n\t\tstmmac_lpi_entry_timer_config(priv, 1);\n\t} else {\n\t\tstmmac_lpi_entry_timer_config(priv, 0);\n\t\tmod_timer(&priv->eee_ctrl_timer,\n\t\t\t  STMMAC_LPI_T(priv->tx_lpi_timer));\n\t}\n\n\tmutex_unlock(&priv->lock);\n\tnetdev_dbg(priv->dev, \"Energy-Efficient Ethernet initialized\\n\");\n\treturn true;\n}\n\n \nstatic void stmmac_get_tx_hwtstamp(struct stmmac_priv *priv,\n\t\t\t\t   struct dma_desc *p, struct sk_buff *skb)\n{\n\tstruct skb_shared_hwtstamps shhwtstamp;\n\tbool found = false;\n\tu64 ns = 0;\n\n\tif (!priv->hwts_tx_en)\n\t\treturn;\n\n\t \n\tif (likely(!skb || !(skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS)))\n\t\treturn;\n\n\t \n\tif (stmmac_get_tx_timestamp_status(priv, p)) {\n\t\tstmmac_get_timestamp(priv, p, priv->adv_ts, &ns);\n\t\tfound = true;\n\t} else if (!stmmac_get_mac_tx_timestamp(priv, priv->hw, &ns)) {\n\t\tfound = true;\n\t}\n\n\tif (found) {\n\t\tns -= priv->plat->cdc_error_adj;\n\n\t\tmemset(&shhwtstamp, 0, sizeof(struct skb_shared_hwtstamps));\n\t\tshhwtstamp.hwtstamp = ns_to_ktime(ns);\n\n\t\tnetdev_dbg(priv->dev, \"get valid TX hw timestamp %llu\\n\", ns);\n\t\t \n\t\tskb_tstamp_tx(skb, &shhwtstamp);\n\t}\n}\n\n \nstatic void stmmac_get_rx_hwtstamp(struct stmmac_priv *priv, struct dma_desc *p,\n\t\t\t\t   struct dma_desc *np, struct sk_buff *skb)\n{\n\tstruct skb_shared_hwtstamps *shhwtstamp = NULL;\n\tstruct dma_desc *desc = p;\n\tu64 ns = 0;\n\n\tif (!priv->hwts_rx_en)\n\t\treturn;\n\t \n\tif (priv->plat->has_gmac4 || priv->plat->has_xgmac)\n\t\tdesc = np;\n\n\t \n\tif (stmmac_get_rx_timestamp_status(priv, p, np, priv->adv_ts)) {\n\t\tstmmac_get_timestamp(priv, desc, priv->adv_ts, &ns);\n\n\t\tns -= priv->plat->cdc_error_adj;\n\n\t\tnetdev_dbg(priv->dev, \"get valid RX hw timestamp %llu\\n\", ns);\n\t\tshhwtstamp = skb_hwtstamps(skb);\n\t\tmemset(shhwtstamp, 0, sizeof(struct skb_shared_hwtstamps));\n\t\tshhwtstamp->hwtstamp = ns_to_ktime(ns);\n\t} else  {\n\t\tnetdev_dbg(priv->dev, \"cannot get RX hw timestamp\\n\");\n\t}\n}\n\n \nstatic int stmmac_hwtstamp_set(struct net_device *dev, struct ifreq *ifr)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tstruct hwtstamp_config config;\n\tu32 ptp_v2 = 0;\n\tu32 tstamp_all = 0;\n\tu32 ptp_over_ipv4_udp = 0;\n\tu32 ptp_over_ipv6_udp = 0;\n\tu32 ptp_over_ethernet = 0;\n\tu32 snap_type_sel = 0;\n\tu32 ts_master_en = 0;\n\tu32 ts_event_en = 0;\n\n\tif (!(priv->dma_cap.time_stamp || priv->adv_ts)) {\n\t\tnetdev_alert(priv->dev, \"No support for HW time stamping\\n\");\n\t\tpriv->hwts_tx_en = 0;\n\t\tpriv->hwts_rx_en = 0;\n\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (copy_from_user(&config, ifr->ifr_data,\n\t\t\t   sizeof(config)))\n\t\treturn -EFAULT;\n\n\tnetdev_dbg(priv->dev, \"%s config flags:0x%x, tx_type:0x%x, rx_filter:0x%x\\n\",\n\t\t   __func__, config.flags, config.tx_type, config.rx_filter);\n\n\tif (config.tx_type != HWTSTAMP_TX_OFF &&\n\t    config.tx_type != HWTSTAMP_TX_ON)\n\t\treturn -ERANGE;\n\n\tif (priv->adv_ts) {\n\t\tswitch (config.rx_filter) {\n\t\tcase HWTSTAMP_FILTER_NONE:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_NONE;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_EVENT;\n\t\t\t \n\t\t\tsnap_type_sel = PTP_TCR_SNAPTYPSEL_1;\n\t\t\tptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;\n\t\t\tptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_SYNC;\n\t\t\t \n\t\t\tts_event_en = PTP_TCR_TSEVNTENA;\n\n\t\t\tptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;\n\t\t\tptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ;\n\t\t\t \n\t\t\tts_master_en = PTP_TCR_TSMSTRENA;\n\t\t\tts_event_en = PTP_TCR_TSEVNTENA;\n\n\t\t\tptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;\n\t\t\tptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_EVENT;\n\t\t\tptp_v2 = PTP_TCR_TSVER2ENA;\n\t\t\t \n\t\t\tsnap_type_sel = PTP_TCR_SNAPTYPSEL_1;\n\n\t\t\tptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;\n\t\t\tptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_SYNC;\n\t\t\tptp_v2 = PTP_TCR_TSVER2ENA;\n\t\t\t \n\t\t\tts_event_en = PTP_TCR_TSEVNTENA;\n\n\t\t\tptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;\n\t\t\tptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ;\n\t\t\tptp_v2 = PTP_TCR_TSVER2ENA;\n\t\t\t \n\t\t\tts_master_en = PTP_TCR_TSMSTRENA;\n\t\t\tts_event_en = PTP_TCR_TSEVNTENA;\n\n\t\t\tptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;\n\t\t\tptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_PTP_V2_EVENT:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V2_EVENT;\n\t\t\tptp_v2 = PTP_TCR_TSVER2ENA;\n\t\t\tsnap_type_sel = PTP_TCR_SNAPTYPSEL_1;\n\t\t\tif (priv->synopsys_id < DWMAC_CORE_4_10)\n\t\t\t\tts_event_en = PTP_TCR_TSEVNTENA;\n\t\t\tptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;\n\t\t\tptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;\n\t\t\tptp_over_ethernet = PTP_TCR_TSIPENA;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_PTP_V2_SYNC:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V2_SYNC;\n\t\t\tptp_v2 = PTP_TCR_TSVER2ENA;\n\t\t\t \n\t\t\tts_event_en = PTP_TCR_TSEVNTENA;\n\n\t\t\tptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;\n\t\t\tptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;\n\t\t\tptp_over_ethernet = PTP_TCR_TSIPENA;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V2_DELAY_REQ;\n\t\t\tptp_v2 = PTP_TCR_TSVER2ENA;\n\t\t\t \n\t\t\tts_master_en = PTP_TCR_TSMSTRENA;\n\t\t\tts_event_en = PTP_TCR_TSEVNTENA;\n\n\t\t\tptp_over_ipv4_udp = PTP_TCR_TSIPV4ENA;\n\t\t\tptp_over_ipv6_udp = PTP_TCR_TSIPV6ENA;\n\t\t\tptp_over_ethernet = PTP_TCR_TSIPENA;\n\t\t\tbreak;\n\n\t\tcase HWTSTAMP_FILTER_NTP_ALL:\n\t\tcase HWTSTAMP_FILTER_ALL:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_ALL;\n\t\t\ttstamp_all = PTP_TCR_TSENALL;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\treturn -ERANGE;\n\t\t}\n\t} else {\n\t\tswitch (config.rx_filter) {\n\t\tcase HWTSTAMP_FILTER_NONE:\n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_NONE;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tconfig.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_EVENT;\n\t\t\tbreak;\n\t\t}\n\t}\n\tpriv->hwts_rx_en = ((config.rx_filter == HWTSTAMP_FILTER_NONE) ? 0 : 1);\n\tpriv->hwts_tx_en = config.tx_type == HWTSTAMP_TX_ON;\n\n\tpriv->systime_flags = STMMAC_HWTS_ACTIVE;\n\n\tif (priv->hwts_tx_en || priv->hwts_rx_en) {\n\t\tpriv->systime_flags |= tstamp_all | ptp_v2 |\n\t\t\t\t       ptp_over_ethernet | ptp_over_ipv6_udp |\n\t\t\t\t       ptp_over_ipv4_udp | ts_event_en |\n\t\t\t\t       ts_master_en | snap_type_sel;\n\t}\n\n\tstmmac_config_hw_tstamping(priv, priv->ptpaddr, priv->systime_flags);\n\n\tmemcpy(&priv->tstamp_config, &config, sizeof(config));\n\n\treturn copy_to_user(ifr->ifr_data, &config,\n\t\t\t    sizeof(config)) ? -EFAULT : 0;\n}\n\n \nstatic int stmmac_hwtstamp_get(struct net_device *dev, struct ifreq *ifr)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tstruct hwtstamp_config *config = &priv->tstamp_config;\n\n\tif (!(priv->dma_cap.time_stamp || priv->dma_cap.atime_stamp))\n\t\treturn -EOPNOTSUPP;\n\n\treturn copy_to_user(ifr->ifr_data, config,\n\t\t\t    sizeof(*config)) ? -EFAULT : 0;\n}\n\n \nint stmmac_init_tstamp_counter(struct stmmac_priv *priv, u32 systime_flags)\n{\n\tbool xmac = priv->plat->has_gmac4 || priv->plat->has_xgmac;\n\tstruct timespec64 now;\n\tu32 sec_inc = 0;\n\tu64 temp = 0;\n\n\tif (!(priv->dma_cap.time_stamp || priv->dma_cap.atime_stamp))\n\t\treturn -EOPNOTSUPP;\n\n\tstmmac_config_hw_tstamping(priv, priv->ptpaddr, systime_flags);\n\tpriv->systime_flags = systime_flags;\n\n\t \n\tstmmac_config_sub_second_increment(priv, priv->ptpaddr,\n\t\t\t\t\t   priv->plat->clk_ptp_rate,\n\t\t\t\t\t   xmac, &sec_inc);\n\ttemp = div_u64(1000000000ULL, sec_inc);\n\n\t \n\tpriv->sub_second_inc = sec_inc;\n\n\t \n\ttemp = (u64)(temp << 32);\n\tpriv->default_addend = div_u64(temp, priv->plat->clk_ptp_rate);\n\tstmmac_config_addend(priv, priv->ptpaddr, priv->default_addend);\n\n\t \n\tktime_get_real_ts64(&now);\n\n\t \n\tstmmac_init_systime(priv, priv->ptpaddr, (u32)now.tv_sec, now.tv_nsec);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(stmmac_init_tstamp_counter);\n\n \nstatic int stmmac_init_ptp(struct stmmac_priv *priv)\n{\n\tbool xmac = priv->plat->has_gmac4 || priv->plat->has_xgmac;\n\tint ret;\n\n\tif (priv->plat->ptp_clk_freq_config)\n\t\tpriv->plat->ptp_clk_freq_config(priv);\n\n\tret = stmmac_init_tstamp_counter(priv, STMMAC_HWTS_ACTIVE);\n\tif (ret)\n\t\treturn ret;\n\n\tpriv->adv_ts = 0;\n\t \n\tif (xmac && priv->dma_cap.atime_stamp)\n\t\tpriv->adv_ts = 1;\n\t \n\telse if (priv->extend_desc && priv->dma_cap.atime_stamp)\n\t\tpriv->adv_ts = 1;\n\n\tif (priv->dma_cap.time_stamp)\n\t\tnetdev_info(priv->dev, \"IEEE 1588-2002 Timestamp supported\\n\");\n\n\tif (priv->adv_ts)\n\t\tnetdev_info(priv->dev,\n\t\t\t    \"IEEE 1588-2008 Advanced Timestamp supported\\n\");\n\n\tpriv->hwts_tx_en = 0;\n\tpriv->hwts_rx_en = 0;\n\n\tif (priv->plat->flags & STMMAC_FLAG_HWTSTAMP_CORRECT_LATENCY)\n\t\tstmmac_hwtstamp_correct_latency(priv, priv);\n\n\treturn 0;\n}\n\nstatic void stmmac_release_ptp(struct stmmac_priv *priv)\n{\n\tclk_disable_unprepare(priv->plat->clk_ptp_ref);\n\tstmmac_ptp_unregister(priv);\n}\n\n \nstatic void stmmac_mac_flow_ctrl(struct stmmac_priv *priv, u32 duplex)\n{\n\tu32 tx_cnt = priv->plat->tx_queues_to_use;\n\n\tstmmac_flow_ctrl(priv, priv->hw, duplex, priv->flow_ctrl,\n\t\t\tpriv->pause, tx_cnt);\n}\n\nstatic struct phylink_pcs *stmmac_mac_select_pcs(struct phylink_config *config,\n\t\t\t\t\t\t phy_interface_t interface)\n{\n\tstruct stmmac_priv *priv = netdev_priv(to_net_dev(config->dev));\n\n\tif (priv->hw->xpcs)\n\t\treturn &priv->hw->xpcs->pcs;\n\n\tif (priv->hw->lynx_pcs)\n\t\treturn priv->hw->lynx_pcs;\n\n\treturn NULL;\n}\n\nstatic void stmmac_mac_config(struct phylink_config *config, unsigned int mode,\n\t\t\t      const struct phylink_link_state *state)\n{\n\t \n}\n\nstatic void stmmac_fpe_link_state_handle(struct stmmac_priv *priv, bool is_up)\n{\n\tstruct stmmac_fpe_cfg *fpe_cfg = priv->plat->fpe_cfg;\n\tenum stmmac_fpe_state *lo_state = &fpe_cfg->lo_fpe_state;\n\tenum stmmac_fpe_state *lp_state = &fpe_cfg->lp_fpe_state;\n\tbool *hs_enable = &fpe_cfg->hs_enable;\n\n\tif (is_up && *hs_enable) {\n\t\tstmmac_fpe_send_mpacket(priv, priv->ioaddr, fpe_cfg,\n\t\t\t\t\tMPACKET_VERIFY);\n\t} else {\n\t\t*lo_state = FPE_STATE_OFF;\n\t\t*lp_state = FPE_STATE_OFF;\n\t}\n}\n\nstatic void stmmac_mac_link_down(struct phylink_config *config,\n\t\t\t\t unsigned int mode, phy_interface_t interface)\n{\n\tstruct stmmac_priv *priv = netdev_priv(to_net_dev(config->dev));\n\n\tstmmac_mac_set(priv, priv->ioaddr, false);\n\tpriv->eee_active = false;\n\tpriv->tx_lpi_enabled = false;\n\tpriv->eee_enabled = stmmac_eee_init(priv);\n\tstmmac_set_eee_pls(priv, priv->hw, false);\n\n\tif (priv->dma_cap.fpesel)\n\t\tstmmac_fpe_link_state_handle(priv, false);\n}\n\nstatic void stmmac_mac_link_up(struct phylink_config *config,\n\t\t\t       struct phy_device *phy,\n\t\t\t       unsigned int mode, phy_interface_t interface,\n\t\t\t       int speed, int duplex,\n\t\t\t       bool tx_pause, bool rx_pause)\n{\n\tstruct stmmac_priv *priv = netdev_priv(to_net_dev(config->dev));\n\tu32 old_ctrl, ctrl;\n\n\tif ((priv->plat->flags & STMMAC_FLAG_SERDES_UP_AFTER_PHY_LINKUP) &&\n\t    priv->plat->serdes_powerup)\n\t\tpriv->plat->serdes_powerup(priv->dev, priv->plat->bsp_priv);\n\n\told_ctrl = readl(priv->ioaddr + MAC_CTRL_REG);\n\tctrl = old_ctrl & ~priv->hw->link.speed_mask;\n\n\tif (interface == PHY_INTERFACE_MODE_USXGMII) {\n\t\tswitch (speed) {\n\t\tcase SPEED_10000:\n\t\t\tctrl |= priv->hw->link.xgmii.speed10000;\n\t\t\tbreak;\n\t\tcase SPEED_5000:\n\t\t\tctrl |= priv->hw->link.xgmii.speed5000;\n\t\t\tbreak;\n\t\tcase SPEED_2500:\n\t\t\tctrl |= priv->hw->link.xgmii.speed2500;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t} else if (interface == PHY_INTERFACE_MODE_XLGMII) {\n\t\tswitch (speed) {\n\t\tcase SPEED_100000:\n\t\t\tctrl |= priv->hw->link.xlgmii.speed100000;\n\t\t\tbreak;\n\t\tcase SPEED_50000:\n\t\t\tctrl |= priv->hw->link.xlgmii.speed50000;\n\t\t\tbreak;\n\t\tcase SPEED_40000:\n\t\t\tctrl |= priv->hw->link.xlgmii.speed40000;\n\t\t\tbreak;\n\t\tcase SPEED_25000:\n\t\t\tctrl |= priv->hw->link.xlgmii.speed25000;\n\t\t\tbreak;\n\t\tcase SPEED_10000:\n\t\t\tctrl |= priv->hw->link.xgmii.speed10000;\n\t\t\tbreak;\n\t\tcase SPEED_2500:\n\t\t\tctrl |= priv->hw->link.speed2500;\n\t\t\tbreak;\n\t\tcase SPEED_1000:\n\t\t\tctrl |= priv->hw->link.speed1000;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tswitch (speed) {\n\t\tcase SPEED_2500:\n\t\t\tctrl |= priv->hw->link.speed2500;\n\t\t\tbreak;\n\t\tcase SPEED_1000:\n\t\t\tctrl |= priv->hw->link.speed1000;\n\t\t\tbreak;\n\t\tcase SPEED_100:\n\t\t\tctrl |= priv->hw->link.speed100;\n\t\t\tbreak;\n\t\tcase SPEED_10:\n\t\t\tctrl |= priv->hw->link.speed10;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t}\n\n\tpriv->speed = speed;\n\n\tif (priv->plat->fix_mac_speed)\n\t\tpriv->plat->fix_mac_speed(priv->plat->bsp_priv, speed, mode);\n\n\tif (!duplex)\n\t\tctrl &= ~priv->hw->link.duplex;\n\telse\n\t\tctrl |= priv->hw->link.duplex;\n\n\t \n\tif (rx_pause && tx_pause)\n\t\tpriv->flow_ctrl = FLOW_AUTO;\n\telse if (rx_pause && !tx_pause)\n\t\tpriv->flow_ctrl = FLOW_RX;\n\telse if (!rx_pause && tx_pause)\n\t\tpriv->flow_ctrl = FLOW_TX;\n\telse\n\t\tpriv->flow_ctrl = FLOW_OFF;\n\n\tstmmac_mac_flow_ctrl(priv, duplex);\n\n\tif (ctrl != old_ctrl)\n\t\twritel(ctrl, priv->ioaddr + MAC_CTRL_REG);\n\n\tstmmac_mac_set(priv, priv->ioaddr, true);\n\tif (phy && priv->dma_cap.eee) {\n\t\tpriv->eee_active =\n\t\t\tphy_init_eee(phy, !(priv->plat->flags &\n\t\t\t\tSTMMAC_FLAG_RX_CLK_RUNS_IN_LPI)) >= 0;\n\t\tpriv->eee_enabled = stmmac_eee_init(priv);\n\t\tpriv->tx_lpi_enabled = priv->eee_enabled;\n\t\tstmmac_set_eee_pls(priv, priv->hw, true);\n\t}\n\n\tif (priv->dma_cap.fpesel)\n\t\tstmmac_fpe_link_state_handle(priv, true);\n\n\tif (priv->plat->flags & STMMAC_FLAG_HWTSTAMP_CORRECT_LATENCY)\n\t\tstmmac_hwtstamp_correct_latency(priv, priv);\n}\n\nstatic const struct phylink_mac_ops stmmac_phylink_mac_ops = {\n\t.mac_select_pcs = stmmac_mac_select_pcs,\n\t.mac_config = stmmac_mac_config,\n\t.mac_link_down = stmmac_mac_link_down,\n\t.mac_link_up = stmmac_mac_link_up,\n};\n\n \nstatic void stmmac_check_pcs_mode(struct stmmac_priv *priv)\n{\n\tint interface = priv->plat->mac_interface;\n\n\tif (priv->dma_cap.pcs) {\n\t\tif ((interface == PHY_INTERFACE_MODE_RGMII) ||\n\t\t    (interface == PHY_INTERFACE_MODE_RGMII_ID) ||\n\t\t    (interface == PHY_INTERFACE_MODE_RGMII_RXID) ||\n\t\t    (interface == PHY_INTERFACE_MODE_RGMII_TXID)) {\n\t\t\tnetdev_dbg(priv->dev, \"PCS RGMII support enabled\\n\");\n\t\t\tpriv->hw->pcs = STMMAC_PCS_RGMII;\n\t\t} else if (interface == PHY_INTERFACE_MODE_SGMII) {\n\t\t\tnetdev_dbg(priv->dev, \"PCS SGMII support enabled\\n\");\n\t\t\tpriv->hw->pcs = STMMAC_PCS_SGMII;\n\t\t}\n\t}\n}\n\n \nstatic int stmmac_init_phy(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tstruct fwnode_handle *phy_fwnode;\n\tstruct fwnode_handle *fwnode;\n\tint ret;\n\n\tif (!phylink_expects_phy(priv->phylink))\n\t\treturn 0;\n\n\tfwnode = priv->plat->port_node;\n\tif (!fwnode)\n\t\tfwnode = dev_fwnode(priv->device);\n\n\tif (fwnode)\n\t\tphy_fwnode = fwnode_get_phy_node(fwnode);\n\telse\n\t\tphy_fwnode = NULL;\n\n\t \n\tif (!phy_fwnode || IS_ERR(phy_fwnode)) {\n\t\tint addr = priv->plat->phy_addr;\n\t\tstruct phy_device *phydev;\n\n\t\tif (addr < 0) {\n\t\t\tnetdev_err(priv->dev, \"no phy found\\n\");\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tphydev = mdiobus_get_phy(priv->mii, addr);\n\t\tif (!phydev) {\n\t\t\tnetdev_err(priv->dev, \"no phy at addr %d\\n\", addr);\n\t\t\treturn -ENODEV;\n\t\t}\n\n\t\tret = phylink_connect_phy(priv->phylink, phydev);\n\t} else {\n\t\tfwnode_handle_put(phy_fwnode);\n\t\tret = phylink_fwnode_phy_connect(priv->phylink, fwnode, 0);\n\t}\n\n\tif (!priv->plat->pmt) {\n\t\tstruct ethtool_wolinfo wol = { .cmd = ETHTOOL_GWOL };\n\n\t\tphylink_ethtool_get_wol(priv->phylink, &wol);\n\t\tdevice_set_wakeup_capable(priv->device, !!wol.supported);\n\t\tdevice_set_wakeup_enable(priv->device, !!wol.wolopts);\n\t}\n\n\treturn ret;\n}\n\nstatic void stmmac_set_half_duplex(struct stmmac_priv *priv)\n{\n\t \n\tif (priv->plat->tx_queues_to_use > 1)\n\t\tpriv->phylink_config.mac_capabilities &=\n\t\t\t~(MAC_10HD | MAC_100HD | MAC_1000HD);\n\telse\n\t\tpriv->phylink_config.mac_capabilities |=\n\t\t\t(MAC_10HD | MAC_100HD | MAC_1000HD);\n}\n\nstatic int stmmac_phy_setup(struct stmmac_priv *priv)\n{\n\tstruct stmmac_mdio_bus_data *mdio_bus_data;\n\tint mode = priv->plat->phy_interface;\n\tstruct fwnode_handle *fwnode;\n\tstruct phylink *phylink;\n\tint max_speed;\n\n\tpriv->phylink_config.dev = &priv->dev->dev;\n\tpriv->phylink_config.type = PHYLINK_NETDEV;\n\tpriv->phylink_config.mac_managed_pm = true;\n\n\tmdio_bus_data = priv->plat->mdio_bus_data;\n\tif (mdio_bus_data)\n\t\tpriv->phylink_config.ovr_an_inband =\n\t\t\tmdio_bus_data->xpcs_an_inband;\n\n\t \n\t__set_bit(mode, priv->phylink_config.supported_interfaces);\n\n\t \n\tif (priv->hw->xpcs)\n\t\txpcs_get_interfaces(priv->hw->xpcs,\n\t\t\t\t    priv->phylink_config.supported_interfaces);\n\n\tpriv->phylink_config.mac_capabilities = MAC_ASYM_PAUSE | MAC_SYM_PAUSE |\n\t\t\t\t\t\tMAC_10FD | MAC_100FD |\n\t\t\t\t\t\tMAC_1000FD;\n\n\tstmmac_set_half_duplex(priv);\n\n\t \n\tstmmac_mac_phylink_get_caps(priv);\n\n\tmax_speed = priv->plat->max_speed;\n\tif (max_speed)\n\t\tphylink_limit_mac_speed(&priv->phylink_config, max_speed);\n\n\tfwnode = priv->plat->port_node;\n\tif (!fwnode)\n\t\tfwnode = dev_fwnode(priv->device);\n\n\tphylink = phylink_create(&priv->phylink_config, fwnode,\n\t\t\t\t mode, &stmmac_phylink_mac_ops);\n\tif (IS_ERR(phylink))\n\t\treturn PTR_ERR(phylink);\n\n\tpriv->phylink = phylink;\n\treturn 0;\n}\n\nstatic void stmmac_display_rx_rings(struct stmmac_priv *priv,\n\t\t\t\t    struct stmmac_dma_conf *dma_conf)\n{\n\tu32 rx_cnt = priv->plat->rx_queues_to_use;\n\tunsigned int desc_size;\n\tvoid *head_rx;\n\tu32 queue;\n\n\t \n\tfor (queue = 0; queue < rx_cnt; queue++) {\n\t\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\n\t\tpr_info(\"\\tRX Queue %u rings\\n\", queue);\n\n\t\tif (priv->extend_desc) {\n\t\t\thead_rx = (void *)rx_q->dma_erx;\n\t\t\tdesc_size = sizeof(struct dma_extended_desc);\n\t\t} else {\n\t\t\thead_rx = (void *)rx_q->dma_rx;\n\t\t\tdesc_size = sizeof(struct dma_desc);\n\t\t}\n\n\t\t \n\t\tstmmac_display_ring(priv, head_rx, dma_conf->dma_rx_size, true,\n\t\t\t\t    rx_q->dma_rx_phy, desc_size);\n\t}\n}\n\nstatic void stmmac_display_tx_rings(struct stmmac_priv *priv,\n\t\t\t\t    struct stmmac_dma_conf *dma_conf)\n{\n\tu32 tx_cnt = priv->plat->tx_queues_to_use;\n\tunsigned int desc_size;\n\tvoid *head_tx;\n\tu32 queue;\n\n\t \n\tfor (queue = 0; queue < tx_cnt; queue++) {\n\t\tstruct stmmac_tx_queue *tx_q = &dma_conf->tx_queue[queue];\n\n\t\tpr_info(\"\\tTX Queue %d rings\\n\", queue);\n\n\t\tif (priv->extend_desc) {\n\t\t\thead_tx = (void *)tx_q->dma_etx;\n\t\t\tdesc_size = sizeof(struct dma_extended_desc);\n\t\t} else if (tx_q->tbs & STMMAC_TBS_AVAIL) {\n\t\t\thead_tx = (void *)tx_q->dma_entx;\n\t\t\tdesc_size = sizeof(struct dma_edesc);\n\t\t} else {\n\t\t\thead_tx = (void *)tx_q->dma_tx;\n\t\t\tdesc_size = sizeof(struct dma_desc);\n\t\t}\n\n\t\tstmmac_display_ring(priv, head_tx, dma_conf->dma_tx_size, false,\n\t\t\t\t    tx_q->dma_tx_phy, desc_size);\n\t}\n}\n\nstatic void stmmac_display_rings(struct stmmac_priv *priv,\n\t\t\t\t struct stmmac_dma_conf *dma_conf)\n{\n\t \n\tstmmac_display_rx_rings(priv, dma_conf);\n\n\t \n\tstmmac_display_tx_rings(priv, dma_conf);\n}\n\nstatic int stmmac_set_bfsize(int mtu, int bufsize)\n{\n\tint ret = bufsize;\n\n\tif (mtu >= BUF_SIZE_8KiB)\n\t\tret = BUF_SIZE_16KiB;\n\telse if (mtu >= BUF_SIZE_4KiB)\n\t\tret = BUF_SIZE_8KiB;\n\telse if (mtu >= BUF_SIZE_2KiB)\n\t\tret = BUF_SIZE_4KiB;\n\telse if (mtu > DEFAULT_BUFSIZE)\n\t\tret = BUF_SIZE_2KiB;\n\telse\n\t\tret = DEFAULT_BUFSIZE;\n\n\treturn ret;\n}\n\n \nstatic void stmmac_clear_rx_descriptors(struct stmmac_priv *priv,\n\t\t\t\t\tstruct stmmac_dma_conf *dma_conf,\n\t\t\t\t\tu32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\tint i;\n\n\t \n\tfor (i = 0; i < dma_conf->dma_rx_size; i++)\n\t\tif (priv->extend_desc)\n\t\t\tstmmac_init_rx_desc(priv, &rx_q->dma_erx[i].basic,\n\t\t\t\t\tpriv->use_riwt, priv->mode,\n\t\t\t\t\t(i == dma_conf->dma_rx_size - 1),\n\t\t\t\t\tdma_conf->dma_buf_sz);\n\t\telse\n\t\t\tstmmac_init_rx_desc(priv, &rx_q->dma_rx[i],\n\t\t\t\t\tpriv->use_riwt, priv->mode,\n\t\t\t\t\t(i == dma_conf->dma_rx_size - 1),\n\t\t\t\t\tdma_conf->dma_buf_sz);\n}\n\n \nstatic void stmmac_clear_tx_descriptors(struct stmmac_priv *priv,\n\t\t\t\t\tstruct stmmac_dma_conf *dma_conf,\n\t\t\t\t\tu32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &dma_conf->tx_queue[queue];\n\tint i;\n\n\t \n\tfor (i = 0; i < dma_conf->dma_tx_size; i++) {\n\t\tint last = (i == (dma_conf->dma_tx_size - 1));\n\t\tstruct dma_desc *p;\n\n\t\tif (priv->extend_desc)\n\t\t\tp = &tx_q->dma_etx[i].basic;\n\t\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\tp = &tx_q->dma_entx[i].basic;\n\t\telse\n\t\t\tp = &tx_q->dma_tx[i];\n\n\t\tstmmac_init_tx_desc(priv, p, priv->mode, last);\n\t}\n}\n\n \nstatic void stmmac_clear_descriptors(struct stmmac_priv *priv,\n\t\t\t\t     struct stmmac_dma_conf *dma_conf)\n{\n\tu32 rx_queue_cnt = priv->plat->rx_queues_to_use;\n\tu32 tx_queue_cnt = priv->plat->tx_queues_to_use;\n\tu32 queue;\n\n\t \n\tfor (queue = 0; queue < rx_queue_cnt; queue++)\n\t\tstmmac_clear_rx_descriptors(priv, dma_conf, queue);\n\n\t \n\tfor (queue = 0; queue < tx_queue_cnt; queue++)\n\t\tstmmac_clear_tx_descriptors(priv, dma_conf, queue);\n}\n\n \nstatic int stmmac_init_rx_buffers(struct stmmac_priv *priv,\n\t\t\t\t  struct stmmac_dma_conf *dma_conf,\n\t\t\t\t  struct dma_desc *p,\n\t\t\t\t  int i, gfp_t flags, u32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\tstruct stmmac_rx_buffer *buf = &rx_q->buf_pool[i];\n\tgfp_t gfp = (GFP_ATOMIC | __GFP_NOWARN);\n\n\tif (priv->dma_cap.host_dma_width <= 32)\n\t\tgfp |= GFP_DMA32;\n\n\tif (!buf->page) {\n\t\tbuf->page = page_pool_alloc_pages(rx_q->page_pool, gfp);\n\t\tif (!buf->page)\n\t\t\treturn -ENOMEM;\n\t\tbuf->page_offset = stmmac_rx_offset(priv);\n\t}\n\n\tif (priv->sph && !buf->sec_page) {\n\t\tbuf->sec_page = page_pool_alloc_pages(rx_q->page_pool, gfp);\n\t\tif (!buf->sec_page)\n\t\t\treturn -ENOMEM;\n\n\t\tbuf->sec_addr = page_pool_get_dma_addr(buf->sec_page);\n\t\tstmmac_set_desc_sec_addr(priv, p, buf->sec_addr, true);\n\t} else {\n\t\tbuf->sec_page = NULL;\n\t\tstmmac_set_desc_sec_addr(priv, p, buf->sec_addr, false);\n\t}\n\n\tbuf->addr = page_pool_get_dma_addr(buf->page) + buf->page_offset;\n\n\tstmmac_set_desc_addr(priv, p, buf->addr);\n\tif (dma_conf->dma_buf_sz == BUF_SIZE_16KiB)\n\t\tstmmac_init_desc3(priv, p);\n\n\treturn 0;\n}\n\n \nstatic void stmmac_free_rx_buffer(struct stmmac_priv *priv,\n\t\t\t\t  struct stmmac_rx_queue *rx_q,\n\t\t\t\t  int i)\n{\n\tstruct stmmac_rx_buffer *buf = &rx_q->buf_pool[i];\n\n\tif (buf->page)\n\t\tpage_pool_put_full_page(rx_q->page_pool, buf->page, false);\n\tbuf->page = NULL;\n\n\tif (buf->sec_page)\n\t\tpage_pool_put_full_page(rx_q->page_pool, buf->sec_page, false);\n\tbuf->sec_page = NULL;\n}\n\n \nstatic void stmmac_free_tx_buffer(struct stmmac_priv *priv,\n\t\t\t\t  struct stmmac_dma_conf *dma_conf,\n\t\t\t\t  u32 queue, int i)\n{\n\tstruct stmmac_tx_queue *tx_q = &dma_conf->tx_queue[queue];\n\n\tif (tx_q->tx_skbuff_dma[i].buf &&\n\t    tx_q->tx_skbuff_dma[i].buf_type != STMMAC_TXBUF_T_XDP_TX) {\n\t\tif (tx_q->tx_skbuff_dma[i].map_as_page)\n\t\t\tdma_unmap_page(priv->device,\n\t\t\t\t       tx_q->tx_skbuff_dma[i].buf,\n\t\t\t\t       tx_q->tx_skbuff_dma[i].len,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(priv->device,\n\t\t\t\t\t tx_q->tx_skbuff_dma[i].buf,\n\t\t\t\t\t tx_q->tx_skbuff_dma[i].len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\tif (tx_q->xdpf[i] &&\n\t    (tx_q->tx_skbuff_dma[i].buf_type == STMMAC_TXBUF_T_XDP_TX ||\n\t     tx_q->tx_skbuff_dma[i].buf_type == STMMAC_TXBUF_T_XDP_NDO)) {\n\t\txdp_return_frame(tx_q->xdpf[i]);\n\t\ttx_q->xdpf[i] = NULL;\n\t}\n\n\tif (tx_q->tx_skbuff_dma[i].buf_type == STMMAC_TXBUF_T_XSK_TX)\n\t\ttx_q->xsk_frames_done++;\n\n\tif (tx_q->tx_skbuff[i] &&\n\t    tx_q->tx_skbuff_dma[i].buf_type == STMMAC_TXBUF_T_SKB) {\n\t\tdev_kfree_skb_any(tx_q->tx_skbuff[i]);\n\t\ttx_q->tx_skbuff[i] = NULL;\n\t}\n\n\ttx_q->tx_skbuff_dma[i].buf = 0;\n\ttx_q->tx_skbuff_dma[i].map_as_page = false;\n}\n\n \nstatic void dma_free_rx_skbufs(struct stmmac_priv *priv,\n\t\t\t       struct stmmac_dma_conf *dma_conf,\n\t\t\t       u32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\tint i;\n\n\tfor (i = 0; i < dma_conf->dma_rx_size; i++)\n\t\tstmmac_free_rx_buffer(priv, rx_q, i);\n}\n\nstatic int stmmac_alloc_rx_buffers(struct stmmac_priv *priv,\n\t\t\t\t   struct stmmac_dma_conf *dma_conf,\n\t\t\t\t   u32 queue, gfp_t flags)\n{\n\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\tint i;\n\n\tfor (i = 0; i < dma_conf->dma_rx_size; i++) {\n\t\tstruct dma_desc *p;\n\t\tint ret;\n\n\t\tif (priv->extend_desc)\n\t\t\tp = &((rx_q->dma_erx + i)->basic);\n\t\telse\n\t\t\tp = rx_q->dma_rx + i;\n\n\t\tret = stmmac_init_rx_buffers(priv, dma_conf, p, i, flags,\n\t\t\t\t\t     queue);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\trx_q->buf_alloc_num++;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void dma_free_rx_xskbufs(struct stmmac_priv *priv,\n\t\t\t\tstruct stmmac_dma_conf *dma_conf,\n\t\t\t\tu32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\tint i;\n\n\tfor (i = 0; i < dma_conf->dma_rx_size; i++) {\n\t\tstruct stmmac_rx_buffer *buf = &rx_q->buf_pool[i];\n\n\t\tif (!buf->xdp)\n\t\t\tcontinue;\n\n\t\txsk_buff_free(buf->xdp);\n\t\tbuf->xdp = NULL;\n\t}\n}\n\nstatic int stmmac_alloc_rx_buffers_zc(struct stmmac_priv *priv,\n\t\t\t\t      struct stmmac_dma_conf *dma_conf,\n\t\t\t\t      u32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\tint i;\n\n\t \n\tXSK_CHECK_PRIV_TYPE(struct stmmac_xdp_buff);\n\n\tfor (i = 0; i < dma_conf->dma_rx_size; i++) {\n\t\tstruct stmmac_rx_buffer *buf;\n\t\tdma_addr_t dma_addr;\n\t\tstruct dma_desc *p;\n\n\t\tif (priv->extend_desc)\n\t\t\tp = (struct dma_desc *)(rx_q->dma_erx + i);\n\t\telse\n\t\t\tp = rx_q->dma_rx + i;\n\n\t\tbuf = &rx_q->buf_pool[i];\n\n\t\tbuf->xdp = xsk_buff_alloc(rx_q->xsk_pool);\n\t\tif (!buf->xdp)\n\t\t\treturn -ENOMEM;\n\n\t\tdma_addr = xsk_buff_xdp_get_dma(buf->xdp);\n\t\tstmmac_set_desc_addr(priv, p, dma_addr);\n\t\trx_q->buf_alloc_num++;\n\t}\n\n\treturn 0;\n}\n\nstatic struct xsk_buff_pool *stmmac_get_xsk_pool(struct stmmac_priv *priv, u32 queue)\n{\n\tif (!stmmac_xdp_is_enabled(priv) || !test_bit(queue, priv->af_xdp_zc_qps))\n\t\treturn NULL;\n\n\treturn xsk_get_pool_from_qid(priv->dev, queue);\n}\n\n \nstatic int __init_dma_rx_desc_rings(struct stmmac_priv *priv,\n\t\t\t\t    struct stmmac_dma_conf *dma_conf,\n\t\t\t\t    u32 queue, gfp_t flags)\n{\n\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\tint ret;\n\n\tnetif_dbg(priv, probe, priv->dev,\n\t\t  \"(%s) dma_rx_phy=0x%08x\\n\", __func__,\n\t\t  (u32)rx_q->dma_rx_phy);\n\n\tstmmac_clear_rx_descriptors(priv, dma_conf, queue);\n\n\txdp_rxq_info_unreg_mem_model(&rx_q->xdp_rxq);\n\n\trx_q->xsk_pool = stmmac_get_xsk_pool(priv, queue);\n\n\tif (rx_q->xsk_pool) {\n\t\tWARN_ON(xdp_rxq_info_reg_mem_model(&rx_q->xdp_rxq,\n\t\t\t\t\t\t   MEM_TYPE_XSK_BUFF_POOL,\n\t\t\t\t\t\t   NULL));\n\t\tnetdev_info(priv->dev,\n\t\t\t    \"Register MEM_TYPE_XSK_BUFF_POOL RxQ-%d\\n\",\n\t\t\t    rx_q->queue_index);\n\t\txsk_pool_set_rxq_info(rx_q->xsk_pool, &rx_q->xdp_rxq);\n\t} else {\n\t\tWARN_ON(xdp_rxq_info_reg_mem_model(&rx_q->xdp_rxq,\n\t\t\t\t\t\t   MEM_TYPE_PAGE_POOL,\n\t\t\t\t\t\t   rx_q->page_pool));\n\t\tnetdev_info(priv->dev,\n\t\t\t    \"Register MEM_TYPE_PAGE_POOL RxQ-%d\\n\",\n\t\t\t    rx_q->queue_index);\n\t}\n\n\tif (rx_q->xsk_pool) {\n\t\t \n\t\tstmmac_alloc_rx_buffers_zc(priv, dma_conf, queue);\n\t} else {\n\t\tret = stmmac_alloc_rx_buffers(priv, dma_conf, queue, flags);\n\t\tif (ret < 0)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (priv->mode == STMMAC_CHAIN_MODE) {\n\t\tif (priv->extend_desc)\n\t\t\tstmmac_mode_init(priv, rx_q->dma_erx,\n\t\t\t\t\t rx_q->dma_rx_phy,\n\t\t\t\t\t dma_conf->dma_rx_size, 1);\n\t\telse\n\t\t\tstmmac_mode_init(priv, rx_q->dma_rx,\n\t\t\t\t\t rx_q->dma_rx_phy,\n\t\t\t\t\t dma_conf->dma_rx_size, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic int init_dma_rx_desc_rings(struct net_device *dev,\n\t\t\t\t  struct stmmac_dma_conf *dma_conf,\n\t\t\t\t  gfp_t flags)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 rx_count = priv->plat->rx_queues_to_use;\n\tint queue;\n\tint ret;\n\n\t \n\tnetif_dbg(priv, probe, priv->dev,\n\t\t  \"SKB addresses:\\nskb\\t\\tskb data\\tdma data\\n\");\n\n\tfor (queue = 0; queue < rx_count; queue++) {\n\t\tret = __init_dma_rx_desc_rings(priv, dma_conf, queue, flags);\n\t\tif (ret)\n\t\t\tgoto err_init_rx_buffers;\n\t}\n\n\treturn 0;\n\nerr_init_rx_buffers:\n\twhile (queue >= 0) {\n\t\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\n\t\tif (rx_q->xsk_pool)\n\t\t\tdma_free_rx_xskbufs(priv, dma_conf, queue);\n\t\telse\n\t\t\tdma_free_rx_skbufs(priv, dma_conf, queue);\n\n\t\trx_q->buf_alloc_num = 0;\n\t\trx_q->xsk_pool = NULL;\n\n\t\tqueue--;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int __init_dma_tx_desc_rings(struct stmmac_priv *priv,\n\t\t\t\t    struct stmmac_dma_conf *dma_conf,\n\t\t\t\t    u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &dma_conf->tx_queue[queue];\n\tint i;\n\n\tnetif_dbg(priv, probe, priv->dev,\n\t\t  \"(%s) dma_tx_phy=0x%08x\\n\", __func__,\n\t\t  (u32)tx_q->dma_tx_phy);\n\n\t \n\tif (priv->mode == STMMAC_CHAIN_MODE) {\n\t\tif (priv->extend_desc)\n\t\t\tstmmac_mode_init(priv, tx_q->dma_etx,\n\t\t\t\t\t tx_q->dma_tx_phy,\n\t\t\t\t\t dma_conf->dma_tx_size, 1);\n\t\telse if (!(tx_q->tbs & STMMAC_TBS_AVAIL))\n\t\t\tstmmac_mode_init(priv, tx_q->dma_tx,\n\t\t\t\t\t tx_q->dma_tx_phy,\n\t\t\t\t\t dma_conf->dma_tx_size, 0);\n\t}\n\n\ttx_q->xsk_pool = stmmac_get_xsk_pool(priv, queue);\n\n\tfor (i = 0; i < dma_conf->dma_tx_size; i++) {\n\t\tstruct dma_desc *p;\n\n\t\tif (priv->extend_desc)\n\t\t\tp = &((tx_q->dma_etx + i)->basic);\n\t\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\tp = &((tx_q->dma_entx + i)->basic);\n\t\telse\n\t\t\tp = tx_q->dma_tx + i;\n\n\t\tstmmac_clear_desc(priv, p);\n\n\t\ttx_q->tx_skbuff_dma[i].buf = 0;\n\t\ttx_q->tx_skbuff_dma[i].map_as_page = false;\n\t\ttx_q->tx_skbuff_dma[i].len = 0;\n\t\ttx_q->tx_skbuff_dma[i].last_segment = false;\n\t\ttx_q->tx_skbuff[i] = NULL;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_dma_tx_desc_rings(struct net_device *dev,\n\t\t\t\t  struct stmmac_dma_conf *dma_conf)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 tx_queue_cnt;\n\tu32 queue;\n\n\ttx_queue_cnt = priv->plat->tx_queues_to_use;\n\n\tfor (queue = 0; queue < tx_queue_cnt; queue++)\n\t\t__init_dma_tx_desc_rings(priv, dma_conf, queue);\n\n\treturn 0;\n}\n\n \nstatic int init_dma_desc_rings(struct net_device *dev,\n\t\t\t       struct stmmac_dma_conf *dma_conf,\n\t\t\t       gfp_t flags)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tint ret;\n\n\tret = init_dma_rx_desc_rings(dev, dma_conf, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = init_dma_tx_desc_rings(dev, dma_conf);\n\n\tstmmac_clear_descriptors(priv, dma_conf);\n\n\tif (netif_msg_hw(priv))\n\t\tstmmac_display_rings(priv, dma_conf);\n\n\treturn ret;\n}\n\n \nstatic void dma_free_tx_skbufs(struct stmmac_priv *priv,\n\t\t\t       struct stmmac_dma_conf *dma_conf,\n\t\t\t       u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &dma_conf->tx_queue[queue];\n\tint i;\n\n\ttx_q->xsk_frames_done = 0;\n\n\tfor (i = 0; i < dma_conf->dma_tx_size; i++)\n\t\tstmmac_free_tx_buffer(priv, dma_conf, queue, i);\n\n\tif (tx_q->xsk_pool && tx_q->xsk_frames_done) {\n\t\txsk_tx_completed(tx_q->xsk_pool, tx_q->xsk_frames_done);\n\t\ttx_q->xsk_frames_done = 0;\n\t\ttx_q->xsk_pool = NULL;\n\t}\n}\n\n \nstatic void stmmac_free_tx_skbufs(struct stmmac_priv *priv)\n{\n\tu32 tx_queue_cnt = priv->plat->tx_queues_to_use;\n\tu32 queue;\n\n\tfor (queue = 0; queue < tx_queue_cnt; queue++)\n\t\tdma_free_tx_skbufs(priv, &priv->dma_conf, queue);\n}\n\n \nstatic void __free_dma_rx_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t\t struct stmmac_dma_conf *dma_conf,\n\t\t\t\t\t u32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\n\t \n\tif (rx_q->xsk_pool)\n\t\tdma_free_rx_xskbufs(priv, dma_conf, queue);\n\telse\n\t\tdma_free_rx_skbufs(priv, dma_conf, queue);\n\n\trx_q->buf_alloc_num = 0;\n\trx_q->xsk_pool = NULL;\n\n\t \n\tif (!priv->extend_desc)\n\t\tdma_free_coherent(priv->device, dma_conf->dma_rx_size *\n\t\t\t\t  sizeof(struct dma_desc),\n\t\t\t\t  rx_q->dma_rx, rx_q->dma_rx_phy);\n\telse\n\t\tdma_free_coherent(priv->device, dma_conf->dma_rx_size *\n\t\t\t\t  sizeof(struct dma_extended_desc),\n\t\t\t\t  rx_q->dma_erx, rx_q->dma_rx_phy);\n\n\tif (xdp_rxq_info_is_reg(&rx_q->xdp_rxq))\n\t\txdp_rxq_info_unreg(&rx_q->xdp_rxq);\n\n\tkfree(rx_q->buf_pool);\n\tif (rx_q->page_pool)\n\t\tpage_pool_destroy(rx_q->page_pool);\n}\n\nstatic void free_dma_rx_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t       struct stmmac_dma_conf *dma_conf)\n{\n\tu32 rx_count = priv->plat->rx_queues_to_use;\n\tu32 queue;\n\n\t \n\tfor (queue = 0; queue < rx_count; queue++)\n\t\t__free_dma_rx_desc_resources(priv, dma_conf, queue);\n}\n\n \nstatic void __free_dma_tx_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t\t struct stmmac_dma_conf *dma_conf,\n\t\t\t\t\t u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &dma_conf->tx_queue[queue];\n\tsize_t size;\n\tvoid *addr;\n\n\t \n\tdma_free_tx_skbufs(priv, dma_conf, queue);\n\n\tif (priv->extend_desc) {\n\t\tsize = sizeof(struct dma_extended_desc);\n\t\taddr = tx_q->dma_etx;\n\t} else if (tx_q->tbs & STMMAC_TBS_AVAIL) {\n\t\tsize = sizeof(struct dma_edesc);\n\t\taddr = tx_q->dma_entx;\n\t} else {\n\t\tsize = sizeof(struct dma_desc);\n\t\taddr = tx_q->dma_tx;\n\t}\n\n\tsize *= dma_conf->dma_tx_size;\n\n\tdma_free_coherent(priv->device, size, addr, tx_q->dma_tx_phy);\n\n\tkfree(tx_q->tx_skbuff_dma);\n\tkfree(tx_q->tx_skbuff);\n}\n\nstatic void free_dma_tx_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t       struct stmmac_dma_conf *dma_conf)\n{\n\tu32 tx_count = priv->plat->tx_queues_to_use;\n\tu32 queue;\n\n\t \n\tfor (queue = 0; queue < tx_count; queue++)\n\t\t__free_dma_tx_desc_resources(priv, dma_conf, queue);\n}\n\n \nstatic int __alloc_dma_rx_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t\t struct stmmac_dma_conf *dma_conf,\n\t\t\t\t\t u32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &dma_conf->rx_queue[queue];\n\tstruct stmmac_channel *ch = &priv->channel[queue];\n\tbool xdp_prog = stmmac_xdp_is_enabled(priv);\n\tstruct page_pool_params pp_params = { 0 };\n\tunsigned int num_pages;\n\tunsigned int napi_id;\n\tint ret;\n\n\trx_q->queue_index = queue;\n\trx_q->priv_data = priv;\n\n\tpp_params.flags = PP_FLAG_DMA_MAP | PP_FLAG_DMA_SYNC_DEV;\n\tpp_params.pool_size = dma_conf->dma_rx_size;\n\tnum_pages = DIV_ROUND_UP(dma_conf->dma_buf_sz, PAGE_SIZE);\n\tpp_params.order = ilog2(num_pages);\n\tpp_params.nid = dev_to_node(priv->device);\n\tpp_params.dev = priv->device;\n\tpp_params.dma_dir = xdp_prog ? DMA_BIDIRECTIONAL : DMA_FROM_DEVICE;\n\tpp_params.offset = stmmac_rx_offset(priv);\n\tpp_params.max_len = STMMAC_MAX_RX_BUF_SIZE(num_pages);\n\n\trx_q->page_pool = page_pool_create(&pp_params);\n\tif (IS_ERR(rx_q->page_pool)) {\n\t\tret = PTR_ERR(rx_q->page_pool);\n\t\trx_q->page_pool = NULL;\n\t\treturn ret;\n\t}\n\n\trx_q->buf_pool = kcalloc(dma_conf->dma_rx_size,\n\t\t\t\t sizeof(*rx_q->buf_pool),\n\t\t\t\t GFP_KERNEL);\n\tif (!rx_q->buf_pool)\n\t\treturn -ENOMEM;\n\n\tif (priv->extend_desc) {\n\t\trx_q->dma_erx = dma_alloc_coherent(priv->device,\n\t\t\t\t\t\t   dma_conf->dma_rx_size *\n\t\t\t\t\t\t   sizeof(struct dma_extended_desc),\n\t\t\t\t\t\t   &rx_q->dma_rx_phy,\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!rx_q->dma_erx)\n\t\t\treturn -ENOMEM;\n\n\t} else {\n\t\trx_q->dma_rx = dma_alloc_coherent(priv->device,\n\t\t\t\t\t\t  dma_conf->dma_rx_size *\n\t\t\t\t\t\t  sizeof(struct dma_desc),\n\t\t\t\t\t\t  &rx_q->dma_rx_phy,\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!rx_q->dma_rx)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (stmmac_xdp_is_enabled(priv) &&\n\t    test_bit(queue, priv->af_xdp_zc_qps))\n\t\tnapi_id = ch->rxtx_napi.napi_id;\n\telse\n\t\tnapi_id = ch->rx_napi.napi_id;\n\n\tret = xdp_rxq_info_reg(&rx_q->xdp_rxq, priv->dev,\n\t\t\t       rx_q->queue_index,\n\t\t\t       napi_id);\n\tif (ret) {\n\t\tnetdev_err(priv->dev, \"Failed to register xdp rxq info\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int alloc_dma_rx_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t       struct stmmac_dma_conf *dma_conf)\n{\n\tu32 rx_count = priv->plat->rx_queues_to_use;\n\tu32 queue;\n\tint ret;\n\n\t \n\tfor (queue = 0; queue < rx_count; queue++) {\n\t\tret = __alloc_dma_rx_desc_resources(priv, dma_conf, queue);\n\t\tif (ret)\n\t\t\tgoto err_dma;\n\t}\n\n\treturn 0;\n\nerr_dma:\n\tfree_dma_rx_desc_resources(priv, dma_conf);\n\n\treturn ret;\n}\n\n \nstatic int __alloc_dma_tx_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t\t struct stmmac_dma_conf *dma_conf,\n\t\t\t\t\t u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &dma_conf->tx_queue[queue];\n\tsize_t size;\n\tvoid *addr;\n\n\ttx_q->queue_index = queue;\n\ttx_q->priv_data = priv;\n\n\ttx_q->tx_skbuff_dma = kcalloc(dma_conf->dma_tx_size,\n\t\t\t\t      sizeof(*tx_q->tx_skbuff_dma),\n\t\t\t\t      GFP_KERNEL);\n\tif (!tx_q->tx_skbuff_dma)\n\t\treturn -ENOMEM;\n\n\ttx_q->tx_skbuff = kcalloc(dma_conf->dma_tx_size,\n\t\t\t\t  sizeof(struct sk_buff *),\n\t\t\t\t  GFP_KERNEL);\n\tif (!tx_q->tx_skbuff)\n\t\treturn -ENOMEM;\n\n\tif (priv->extend_desc)\n\t\tsize = sizeof(struct dma_extended_desc);\n\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\tsize = sizeof(struct dma_edesc);\n\telse\n\t\tsize = sizeof(struct dma_desc);\n\n\tsize *= dma_conf->dma_tx_size;\n\n\taddr = dma_alloc_coherent(priv->device, size,\n\t\t\t\t  &tx_q->dma_tx_phy, GFP_KERNEL);\n\tif (!addr)\n\t\treturn -ENOMEM;\n\n\tif (priv->extend_desc)\n\t\ttx_q->dma_etx = addr;\n\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\ttx_q->dma_entx = addr;\n\telse\n\t\ttx_q->dma_tx = addr;\n\n\treturn 0;\n}\n\nstatic int alloc_dma_tx_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t       struct stmmac_dma_conf *dma_conf)\n{\n\tu32 tx_count = priv->plat->tx_queues_to_use;\n\tu32 queue;\n\tint ret;\n\n\t \n\tfor (queue = 0; queue < tx_count; queue++) {\n\t\tret = __alloc_dma_tx_desc_resources(priv, dma_conf, queue);\n\t\tif (ret)\n\t\t\tgoto err_dma;\n\t}\n\n\treturn 0;\n\nerr_dma:\n\tfree_dma_tx_desc_resources(priv, dma_conf);\n\treturn ret;\n}\n\n \nstatic int alloc_dma_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t    struct stmmac_dma_conf *dma_conf)\n{\n\t \n\tint ret = alloc_dma_rx_desc_resources(priv, dma_conf);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = alloc_dma_tx_desc_resources(priv, dma_conf);\n\n\treturn ret;\n}\n\n \nstatic void free_dma_desc_resources(struct stmmac_priv *priv,\n\t\t\t\t    struct stmmac_dma_conf *dma_conf)\n{\n\t \n\tfree_dma_tx_desc_resources(priv, dma_conf);\n\n\t \n\tfree_dma_rx_desc_resources(priv, dma_conf);\n}\n\n \nstatic void stmmac_mac_enable_rx_queues(struct stmmac_priv *priv)\n{\n\tu32 rx_queues_count = priv->plat->rx_queues_to_use;\n\tint queue;\n\tu8 mode;\n\n\tfor (queue = 0; queue < rx_queues_count; queue++) {\n\t\tmode = priv->plat->rx_queues_cfg[queue].mode_to_use;\n\t\tstmmac_rx_queue_enable(priv, priv->hw, mode, queue);\n\t}\n}\n\n \nstatic void stmmac_start_rx_dma(struct stmmac_priv *priv, u32 chan)\n{\n\tnetdev_dbg(priv->dev, \"DMA RX processes started in channel %d\\n\", chan);\n\tstmmac_start_rx(priv, priv->ioaddr, chan);\n}\n\n \nstatic void stmmac_start_tx_dma(struct stmmac_priv *priv, u32 chan)\n{\n\tnetdev_dbg(priv->dev, \"DMA TX processes started in channel %d\\n\", chan);\n\tstmmac_start_tx(priv, priv->ioaddr, chan);\n}\n\n \nstatic void stmmac_stop_rx_dma(struct stmmac_priv *priv, u32 chan)\n{\n\tnetdev_dbg(priv->dev, \"DMA RX processes stopped in channel %d\\n\", chan);\n\tstmmac_stop_rx(priv, priv->ioaddr, chan);\n}\n\n \nstatic void stmmac_stop_tx_dma(struct stmmac_priv *priv, u32 chan)\n{\n\tnetdev_dbg(priv->dev, \"DMA TX processes stopped in channel %d\\n\", chan);\n\tstmmac_stop_tx(priv, priv->ioaddr, chan);\n}\n\nstatic void stmmac_enable_all_dma_irq(struct stmmac_priv *priv)\n{\n\tu32 rx_channels_count = priv->plat->rx_queues_to_use;\n\tu32 tx_channels_count = priv->plat->tx_queues_to_use;\n\tu32 dma_csr_ch = max(rx_channels_count, tx_channels_count);\n\tu32 chan;\n\n\tfor (chan = 0; chan < dma_csr_ch; chan++) {\n\t\tstruct stmmac_channel *ch = &priv->channel[chan];\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ch->lock, flags);\n\t\tstmmac_enable_dma_irq(priv, priv->ioaddr, chan, 1, 1);\n\t\tspin_unlock_irqrestore(&ch->lock, flags);\n\t}\n}\n\n \nstatic void stmmac_start_all_dma(struct stmmac_priv *priv)\n{\n\tu32 rx_channels_count = priv->plat->rx_queues_to_use;\n\tu32 tx_channels_count = priv->plat->tx_queues_to_use;\n\tu32 chan = 0;\n\n\tfor (chan = 0; chan < rx_channels_count; chan++)\n\t\tstmmac_start_rx_dma(priv, chan);\n\n\tfor (chan = 0; chan < tx_channels_count; chan++)\n\t\tstmmac_start_tx_dma(priv, chan);\n}\n\n \nstatic void stmmac_stop_all_dma(struct stmmac_priv *priv)\n{\n\tu32 rx_channels_count = priv->plat->rx_queues_to_use;\n\tu32 tx_channels_count = priv->plat->tx_queues_to_use;\n\tu32 chan = 0;\n\n\tfor (chan = 0; chan < rx_channels_count; chan++)\n\t\tstmmac_stop_rx_dma(priv, chan);\n\n\tfor (chan = 0; chan < tx_channels_count; chan++)\n\t\tstmmac_stop_tx_dma(priv, chan);\n}\n\n \nstatic void stmmac_dma_operation_mode(struct stmmac_priv *priv)\n{\n\tu32 rx_channels_count = priv->plat->rx_queues_to_use;\n\tu32 tx_channels_count = priv->plat->tx_queues_to_use;\n\tint rxfifosz = priv->plat->rx_fifo_size;\n\tint txfifosz = priv->plat->tx_fifo_size;\n\tu32 txmode = 0;\n\tu32 rxmode = 0;\n\tu32 chan = 0;\n\tu8 qmode = 0;\n\n\tif (rxfifosz == 0)\n\t\trxfifosz = priv->dma_cap.rx_fifo_size;\n\tif (txfifosz == 0)\n\t\ttxfifosz = priv->dma_cap.tx_fifo_size;\n\n\t \n\trxfifosz /= rx_channels_count;\n\ttxfifosz /= tx_channels_count;\n\n\tif (priv->plat->force_thresh_dma_mode) {\n\t\ttxmode = tc;\n\t\trxmode = tc;\n\t} else if (priv->plat->force_sf_dma_mode || priv->plat->tx_coe) {\n\t\t \n\t\ttxmode = SF_DMA_MODE;\n\t\trxmode = SF_DMA_MODE;\n\t\tpriv->xstats.threshold = SF_DMA_MODE;\n\t} else {\n\t\ttxmode = tc;\n\t\trxmode = SF_DMA_MODE;\n\t}\n\n\t \n\tfor (chan = 0; chan < rx_channels_count; chan++) {\n\t\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[chan];\n\t\tu32 buf_size;\n\n\t\tqmode = priv->plat->rx_queues_cfg[chan].mode_to_use;\n\n\t\tstmmac_dma_rx_mode(priv, priv->ioaddr, rxmode, chan,\n\t\t\t\trxfifosz, qmode);\n\n\t\tif (rx_q->xsk_pool) {\n\t\t\tbuf_size = xsk_pool_get_rx_frame_size(rx_q->xsk_pool);\n\t\t\tstmmac_set_dma_bfsize(priv, priv->ioaddr,\n\t\t\t\t\t      buf_size,\n\t\t\t\t\t      chan);\n\t\t} else {\n\t\t\tstmmac_set_dma_bfsize(priv, priv->ioaddr,\n\t\t\t\t\t      priv->dma_conf.dma_buf_sz,\n\t\t\t\t\t      chan);\n\t\t}\n\t}\n\n\tfor (chan = 0; chan < tx_channels_count; chan++) {\n\t\tqmode = priv->plat->tx_queues_cfg[chan].mode_to_use;\n\n\t\tstmmac_dma_tx_mode(priv, priv->ioaddr, txmode, chan,\n\t\t\t\ttxfifosz, qmode);\n\t}\n}\n\nstatic bool stmmac_xdp_xmit_zc(struct stmmac_priv *priv, u32 queue, u32 budget)\n{\n\tstruct netdev_queue *nq = netdev_get_tx_queue(priv->dev, queue);\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\tstruct stmmac_txq_stats *txq_stats = &priv->xstats.txq_stats[queue];\n\tstruct xsk_buff_pool *pool = tx_q->xsk_pool;\n\tunsigned int entry = tx_q->cur_tx;\n\tstruct dma_desc *tx_desc = NULL;\n\tstruct xdp_desc xdp_desc;\n\tbool work_done = true;\n\tu32 tx_set_ic_bit = 0;\n\tunsigned long flags;\n\n\t \n\ttxq_trans_cond_update(nq);\n\n\tbudget = min(budget, stmmac_tx_avail(priv, queue));\n\n\twhile (budget-- > 0) {\n\t\tdma_addr_t dma_addr;\n\t\tbool set_ic;\n\n\t\t \n\t\tif (unlikely(stmmac_tx_avail(priv, queue) < STMMAC_TX_XSK_AVAIL) ||\n\t\t    !netif_carrier_ok(priv->dev)) {\n\t\t\twork_done = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!xsk_tx_peek_desc(pool, &xdp_desc))\n\t\t\tbreak;\n\n\t\tif (likely(priv->extend_desc))\n\t\t\ttx_desc = (struct dma_desc *)(tx_q->dma_etx + entry);\n\t\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\ttx_desc = &tx_q->dma_entx[entry].basic;\n\t\telse\n\t\t\ttx_desc = tx_q->dma_tx + entry;\n\n\t\tdma_addr = xsk_buff_raw_get_dma(pool, xdp_desc.addr);\n\t\txsk_buff_raw_dma_sync_for_device(pool, dma_addr, xdp_desc.len);\n\n\t\ttx_q->tx_skbuff_dma[entry].buf_type = STMMAC_TXBUF_T_XSK_TX;\n\n\t\t \n\t\ttx_q->tx_skbuff_dma[entry].buf = 0;\n\t\ttx_q->xdpf[entry] = NULL;\n\n\t\ttx_q->tx_skbuff_dma[entry].map_as_page = false;\n\t\ttx_q->tx_skbuff_dma[entry].len = xdp_desc.len;\n\t\ttx_q->tx_skbuff_dma[entry].last_segment = true;\n\t\ttx_q->tx_skbuff_dma[entry].is_jumbo = false;\n\n\t\tstmmac_set_desc_addr(priv, tx_desc, dma_addr);\n\n\t\ttx_q->tx_count_frames++;\n\n\t\tif (!priv->tx_coal_frames[queue])\n\t\t\tset_ic = false;\n\t\telse if (tx_q->tx_count_frames % priv->tx_coal_frames[queue] == 0)\n\t\t\tset_ic = true;\n\t\telse\n\t\t\tset_ic = false;\n\n\t\tif (set_ic) {\n\t\t\ttx_q->tx_count_frames = 0;\n\t\t\tstmmac_set_tx_ic(priv, tx_desc);\n\t\t\ttx_set_ic_bit++;\n\t\t}\n\n\t\tstmmac_prepare_tx_desc(priv, tx_desc, 1, xdp_desc.len,\n\t\t\t\t       true, priv->mode, true, true,\n\t\t\t\t       xdp_desc.len);\n\n\t\tstmmac_enable_dma_transmission(priv, priv->ioaddr);\n\n\t\ttx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size);\n\t\tentry = tx_q->cur_tx;\n\t}\n\tflags = u64_stats_update_begin_irqsave(&txq_stats->syncp);\n\ttxq_stats->tx_set_ic_bit += tx_set_ic_bit;\n\tu64_stats_update_end_irqrestore(&txq_stats->syncp, flags);\n\n\tif (tx_desc) {\n\t\tstmmac_flush_tx_descriptors(priv, queue);\n\t\txsk_tx_release(pool);\n\t}\n\n\t \n\treturn !!budget && work_done;\n}\n\nstatic void stmmac_bump_dma_threshold(struct stmmac_priv *priv, u32 chan)\n{\n\tif (unlikely(priv->xstats.threshold != SF_DMA_MODE) && tc <= 256) {\n\t\ttc += 64;\n\n\t\tif (priv->plat->force_thresh_dma_mode)\n\t\t\tstmmac_set_dma_operation_mode(priv, tc, tc, chan);\n\t\telse\n\t\t\tstmmac_set_dma_operation_mode(priv, tc, SF_DMA_MODE,\n\t\t\t\t\t\t      chan);\n\n\t\tpriv->xstats.threshold = tc;\n\t}\n}\n\n \nstatic int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\tstruct stmmac_txq_stats *txq_stats = &priv->xstats.txq_stats[queue];\n\tunsigned int bytes_compl = 0, pkts_compl = 0;\n\tunsigned int entry, xmits = 0, count = 0;\n\tu32 tx_packets = 0, tx_errors = 0;\n\tunsigned long flags;\n\n\t__netif_tx_lock_bh(netdev_get_tx_queue(priv->dev, queue));\n\n\ttx_q->xsk_frames_done = 0;\n\n\tentry = tx_q->dirty_tx;\n\n\t \n\twhile ((entry != tx_q->cur_tx) && count < priv->dma_conf.dma_tx_size) {\n\t\tstruct xdp_frame *xdpf;\n\t\tstruct sk_buff *skb;\n\t\tstruct dma_desc *p;\n\t\tint status;\n\n\t\tif (tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XDP_TX ||\n\t\t    tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XDP_NDO) {\n\t\t\txdpf = tx_q->xdpf[entry];\n\t\t\tskb = NULL;\n\t\t} else if (tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_SKB) {\n\t\t\txdpf = NULL;\n\t\t\tskb = tx_q->tx_skbuff[entry];\n\t\t} else {\n\t\t\txdpf = NULL;\n\t\t\tskb = NULL;\n\t\t}\n\n\t\tif (priv->extend_desc)\n\t\t\tp = (struct dma_desc *)(tx_q->dma_etx + entry);\n\t\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\tp = &tx_q->dma_entx[entry].basic;\n\t\telse\n\t\t\tp = tx_q->dma_tx + entry;\n\n\t\tstatus = stmmac_tx_status(priv,\t&priv->xstats, p, priv->ioaddr);\n\t\t \n\t\tif (unlikely(status & tx_dma_own))\n\t\t\tbreak;\n\n\t\tcount++;\n\n\t\t \n\t\tdma_rmb();\n\n\t\t \n\t\tif (likely(!(status & tx_not_ls))) {\n\t\t\t \n\t\t\tif (unlikely(status & tx_err)) {\n\t\t\t\ttx_errors++;\n\t\t\t\tif (unlikely(status & tx_err_bump_tc))\n\t\t\t\t\tstmmac_bump_dma_threshold(priv, queue);\n\t\t\t} else {\n\t\t\t\ttx_packets++;\n\t\t\t}\n\t\t\tif (skb)\n\t\t\t\tstmmac_get_tx_hwtstamp(priv, p, skb);\n\t\t}\n\n\t\tif (likely(tx_q->tx_skbuff_dma[entry].buf &&\n\t\t\t   tx_q->tx_skbuff_dma[entry].buf_type != STMMAC_TXBUF_T_XDP_TX)) {\n\t\t\tif (tx_q->tx_skbuff_dma[entry].map_as_page)\n\t\t\t\tdma_unmap_page(priv->device,\n\t\t\t\t\t       tx_q->tx_skbuff_dma[entry].buf,\n\t\t\t\t\t       tx_q->tx_skbuff_dma[entry].len,\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\telse\n\t\t\t\tdma_unmap_single(priv->device,\n\t\t\t\t\t\t tx_q->tx_skbuff_dma[entry].buf,\n\t\t\t\t\t\t tx_q->tx_skbuff_dma[entry].len,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\ttx_q->tx_skbuff_dma[entry].buf = 0;\n\t\t\ttx_q->tx_skbuff_dma[entry].len = 0;\n\t\t\ttx_q->tx_skbuff_dma[entry].map_as_page = false;\n\t\t}\n\n\t\tstmmac_clean_desc3(priv, tx_q, p);\n\n\t\ttx_q->tx_skbuff_dma[entry].last_segment = false;\n\t\ttx_q->tx_skbuff_dma[entry].is_jumbo = false;\n\n\t\tif (xdpf &&\n\t\t    tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XDP_TX) {\n\t\t\txdp_return_frame_rx_napi(xdpf);\n\t\t\ttx_q->xdpf[entry] = NULL;\n\t\t}\n\n\t\tif (xdpf &&\n\t\t    tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XDP_NDO) {\n\t\t\txdp_return_frame(xdpf);\n\t\t\ttx_q->xdpf[entry] = NULL;\n\t\t}\n\n\t\tif (tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_XSK_TX)\n\t\t\ttx_q->xsk_frames_done++;\n\n\t\tif (tx_q->tx_skbuff_dma[entry].buf_type == STMMAC_TXBUF_T_SKB) {\n\t\t\tif (likely(skb)) {\n\t\t\t\tpkts_compl++;\n\t\t\t\tbytes_compl += skb->len;\n\t\t\t\tdev_consume_skb_any(skb);\n\t\t\t\ttx_q->tx_skbuff[entry] = NULL;\n\t\t\t}\n\t\t}\n\n\t\tstmmac_release_tx_desc(priv, p, priv->mode);\n\n\t\tentry = STMMAC_GET_ENTRY(entry, priv->dma_conf.dma_tx_size);\n\t}\n\ttx_q->dirty_tx = entry;\n\n\tnetdev_tx_completed_queue(netdev_get_tx_queue(priv->dev, queue),\n\t\t\t\t  pkts_compl, bytes_compl);\n\n\tif (unlikely(netif_tx_queue_stopped(netdev_get_tx_queue(priv->dev,\n\t\t\t\t\t\t\t\tqueue))) &&\n\t    stmmac_tx_avail(priv, queue) > STMMAC_TX_THRESH(priv)) {\n\n\t\tnetif_dbg(priv, tx_done, priv->dev,\n\t\t\t  \"%s: restart transmit\\n\", __func__);\n\t\tnetif_tx_wake_queue(netdev_get_tx_queue(priv->dev, queue));\n\t}\n\n\tif (tx_q->xsk_pool) {\n\t\tbool work_done;\n\n\t\tif (tx_q->xsk_frames_done)\n\t\t\txsk_tx_completed(tx_q->xsk_pool, tx_q->xsk_frames_done);\n\n\t\tif (xsk_uses_need_wakeup(tx_q->xsk_pool))\n\t\t\txsk_set_tx_need_wakeup(tx_q->xsk_pool);\n\n\t\t \n\t\twork_done = stmmac_xdp_xmit_zc(priv, queue,\n\t\t\t\t\t       STMMAC_XSK_TX_BUDGET_MAX);\n\t\tif (work_done)\n\t\t\txmits = budget - 1;\n\t\telse\n\t\t\txmits = budget;\n\t}\n\n\tif (priv->eee_enabled && !priv->tx_path_in_lpi_mode &&\n\t    priv->eee_sw_timer_en) {\n\t\tif (stmmac_enable_eee_mode(priv))\n\t\t\tmod_timer(&priv->eee_ctrl_timer, STMMAC_LPI_T(priv->tx_lpi_timer));\n\t}\n\n\t \n\tif (tx_q->dirty_tx != tx_q->cur_tx)\n\t\tstmmac_tx_timer_arm(priv, queue);\n\n\tflags = u64_stats_update_begin_irqsave(&txq_stats->syncp);\n\ttxq_stats->tx_packets += tx_packets;\n\ttxq_stats->tx_pkt_n += tx_packets;\n\ttxq_stats->tx_clean++;\n\tu64_stats_update_end_irqrestore(&txq_stats->syncp, flags);\n\n\tpriv->xstats.tx_errors += tx_errors;\n\n\t__netif_tx_unlock_bh(netdev_get_tx_queue(priv->dev, queue));\n\n\t \n\treturn max(count, xmits);\n}\n\n \nstatic void stmmac_tx_err(struct stmmac_priv *priv, u32 chan)\n{\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[chan];\n\n\tnetif_tx_stop_queue(netdev_get_tx_queue(priv->dev, chan));\n\n\tstmmac_stop_tx_dma(priv, chan);\n\tdma_free_tx_skbufs(priv, &priv->dma_conf, chan);\n\tstmmac_clear_tx_descriptors(priv, &priv->dma_conf, chan);\n\tstmmac_reset_tx_queue(priv, chan);\n\tstmmac_init_tx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,\n\t\t\t    tx_q->dma_tx_phy, chan);\n\tstmmac_start_tx_dma(priv, chan);\n\n\tpriv->xstats.tx_errors++;\n\tnetif_tx_wake_queue(netdev_get_tx_queue(priv->dev, chan));\n}\n\n \nstatic void stmmac_set_dma_operation_mode(struct stmmac_priv *priv, u32 txmode,\n\t\t\t\t\t  u32 rxmode, u32 chan)\n{\n\tu8 rxqmode = priv->plat->rx_queues_cfg[chan].mode_to_use;\n\tu8 txqmode = priv->plat->tx_queues_cfg[chan].mode_to_use;\n\tu32 rx_channels_count = priv->plat->rx_queues_to_use;\n\tu32 tx_channels_count = priv->plat->tx_queues_to_use;\n\tint rxfifosz = priv->plat->rx_fifo_size;\n\tint txfifosz = priv->plat->tx_fifo_size;\n\n\tif (rxfifosz == 0)\n\t\trxfifosz = priv->dma_cap.rx_fifo_size;\n\tif (txfifosz == 0)\n\t\ttxfifosz = priv->dma_cap.tx_fifo_size;\n\n\t \n\trxfifosz /= rx_channels_count;\n\ttxfifosz /= tx_channels_count;\n\n\tstmmac_dma_rx_mode(priv, priv->ioaddr, rxmode, chan, rxfifosz, rxqmode);\n\tstmmac_dma_tx_mode(priv, priv->ioaddr, txmode, chan, txfifosz, txqmode);\n}\n\nstatic bool stmmac_safety_feat_interrupt(struct stmmac_priv *priv)\n{\n\tint ret;\n\n\tret = stmmac_safety_feat_irq_status(priv, priv->dev,\n\t\t\tpriv->ioaddr, priv->dma_cap.asp, &priv->sstats);\n\tif (ret && (ret != -EINVAL)) {\n\t\tstmmac_global_err(priv);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int stmmac_napi_check(struct stmmac_priv *priv, u32 chan, u32 dir)\n{\n\tint status = stmmac_dma_interrupt_status(priv, priv->ioaddr,\n\t\t\t\t\t\t &priv->xstats, chan, dir);\n\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[chan];\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[chan];\n\tstruct stmmac_channel *ch = &priv->channel[chan];\n\tstruct napi_struct *rx_napi;\n\tstruct napi_struct *tx_napi;\n\tunsigned long flags;\n\n\trx_napi = rx_q->xsk_pool ? &ch->rxtx_napi : &ch->rx_napi;\n\ttx_napi = tx_q->xsk_pool ? &ch->rxtx_napi : &ch->tx_napi;\n\n\tif ((status & handle_rx) && (chan < priv->plat->rx_queues_to_use)) {\n\t\tif (napi_schedule_prep(rx_napi)) {\n\t\t\tspin_lock_irqsave(&ch->lock, flags);\n\t\t\tstmmac_disable_dma_irq(priv, priv->ioaddr, chan, 1, 0);\n\t\t\tspin_unlock_irqrestore(&ch->lock, flags);\n\t\t\t__napi_schedule(rx_napi);\n\t\t}\n\t}\n\n\tif ((status & handle_tx) && (chan < priv->plat->tx_queues_to_use)) {\n\t\tif (napi_schedule_prep(tx_napi)) {\n\t\t\tspin_lock_irqsave(&ch->lock, flags);\n\t\t\tstmmac_disable_dma_irq(priv, priv->ioaddr, chan, 0, 1);\n\t\t\tspin_unlock_irqrestore(&ch->lock, flags);\n\t\t\t__napi_schedule(tx_napi);\n\t\t}\n\t}\n\n\treturn status;\n}\n\n \nstatic void stmmac_dma_interrupt(struct stmmac_priv *priv)\n{\n\tu32 tx_channel_count = priv->plat->tx_queues_to_use;\n\tu32 rx_channel_count = priv->plat->rx_queues_to_use;\n\tu32 channels_to_check = tx_channel_count > rx_channel_count ?\n\t\t\t\ttx_channel_count : rx_channel_count;\n\tu32 chan;\n\tint status[max_t(u32, MTL_MAX_TX_QUEUES, MTL_MAX_RX_QUEUES)];\n\n\t \n\tif (WARN_ON_ONCE(channels_to_check > ARRAY_SIZE(status)))\n\t\tchannels_to_check = ARRAY_SIZE(status);\n\n\tfor (chan = 0; chan < channels_to_check; chan++)\n\t\tstatus[chan] = stmmac_napi_check(priv, chan,\n\t\t\t\t\t\t DMA_DIR_RXTX);\n\n\tfor (chan = 0; chan < tx_channel_count; chan++) {\n\t\tif (unlikely(status[chan] & tx_hard_error_bump_tc)) {\n\t\t\t \n\t\t\tstmmac_bump_dma_threshold(priv, chan);\n\t\t} else if (unlikely(status[chan] == tx_hard_error)) {\n\t\t\tstmmac_tx_err(priv, chan);\n\t\t}\n\t}\n}\n\n \nstatic void stmmac_mmc_setup(struct stmmac_priv *priv)\n{\n\tunsigned int mode = MMC_CNTRL_RESET_ON_READ | MMC_CNTRL_COUNTER_RESET |\n\t\t\t    MMC_CNTRL_PRESET | MMC_CNTRL_FULL_HALF_PRESET;\n\n\tstmmac_mmc_intr_all_mask(priv, priv->mmcaddr);\n\n\tif (priv->dma_cap.rmon) {\n\t\tstmmac_mmc_ctrl(priv, priv->mmcaddr, mode);\n\t\tmemset(&priv->mmc, 0, sizeof(struct stmmac_counters));\n\t} else\n\t\tnetdev_info(priv->dev, \"No MAC Management Counters available\\n\");\n}\n\n \nstatic int stmmac_get_hw_features(struct stmmac_priv *priv)\n{\n\treturn stmmac_get_hw_feature(priv, priv->ioaddr, &priv->dma_cap) == 0;\n}\n\n \nstatic void stmmac_check_ether_addr(struct stmmac_priv *priv)\n{\n\tu8 addr[ETH_ALEN];\n\n\tif (!is_valid_ether_addr(priv->dev->dev_addr)) {\n\t\tstmmac_get_umac_addr(priv, priv->hw, addr, 0);\n\t\tif (is_valid_ether_addr(addr))\n\t\t\teth_hw_addr_set(priv->dev, addr);\n\t\telse\n\t\t\teth_hw_addr_random(priv->dev);\n\t\tdev_info(priv->device, \"device MAC address %pM\\n\",\n\t\t\t priv->dev->dev_addr);\n\t}\n}\n\n \nstatic int stmmac_init_dma_engine(struct stmmac_priv *priv)\n{\n\tu32 rx_channels_count = priv->plat->rx_queues_to_use;\n\tu32 tx_channels_count = priv->plat->tx_queues_to_use;\n\tu32 dma_csr_ch = max(rx_channels_count, tx_channels_count);\n\tstruct stmmac_rx_queue *rx_q;\n\tstruct stmmac_tx_queue *tx_q;\n\tu32 chan = 0;\n\tint atds = 0;\n\tint ret = 0;\n\n\tif (!priv->plat->dma_cfg || !priv->plat->dma_cfg->pbl) {\n\t\tdev_err(priv->device, \"Invalid DMA configuration\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (priv->extend_desc && (priv->mode == STMMAC_RING_MODE))\n\t\tatds = 1;\n\n\tret = stmmac_reset(priv, priv->ioaddr);\n\tif (ret) {\n\t\tdev_err(priv->device, \"Failed to reset the dma\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tstmmac_dma_init(priv, priv->ioaddr, priv->plat->dma_cfg, atds);\n\n\tif (priv->plat->axi)\n\t\tstmmac_axi(priv, priv->ioaddr, priv->plat->axi);\n\n\t \n\tfor (chan = 0; chan < dma_csr_ch; chan++) {\n\t\tstmmac_init_chan(priv, priv->ioaddr, priv->plat->dma_cfg, chan);\n\t\tstmmac_disable_dma_irq(priv, priv->ioaddr, chan, 1, 1);\n\t}\n\n\t \n\tfor (chan = 0; chan < rx_channels_count; chan++) {\n\t\trx_q = &priv->dma_conf.rx_queue[chan];\n\n\t\tstmmac_init_rx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,\n\t\t\t\t    rx_q->dma_rx_phy, chan);\n\n\t\trx_q->rx_tail_addr = rx_q->dma_rx_phy +\n\t\t\t\t     (rx_q->buf_alloc_num *\n\t\t\t\t      sizeof(struct dma_desc));\n\t\tstmmac_set_rx_tail_ptr(priv, priv->ioaddr,\n\t\t\t\t       rx_q->rx_tail_addr, chan);\n\t}\n\n\t \n\tfor (chan = 0; chan < tx_channels_count; chan++) {\n\t\ttx_q = &priv->dma_conf.tx_queue[chan];\n\n\t\tstmmac_init_tx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,\n\t\t\t\t    tx_q->dma_tx_phy, chan);\n\n\t\ttx_q->tx_tail_addr = tx_q->dma_tx_phy;\n\t\tstmmac_set_tx_tail_ptr(priv, priv->ioaddr,\n\t\t\t\t       tx_q->tx_tail_addr, chan);\n\t}\n\n\treturn ret;\n}\n\nstatic void stmmac_tx_timer_arm(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\tu32 tx_coal_timer = priv->tx_coal_timer[queue];\n\n\tif (!tx_coal_timer)\n\t\treturn;\n\n\thrtimer_start(&tx_q->txtimer,\n\t\t      STMMAC_COAL_TIMER(tx_coal_timer),\n\t\t      HRTIMER_MODE_REL);\n}\n\n \nstatic enum hrtimer_restart stmmac_tx_timer(struct hrtimer *t)\n{\n\tstruct stmmac_tx_queue *tx_q = container_of(t, struct stmmac_tx_queue, txtimer);\n\tstruct stmmac_priv *priv = tx_q->priv_data;\n\tstruct stmmac_channel *ch;\n\tstruct napi_struct *napi;\n\n\tch = &priv->channel[tx_q->queue_index];\n\tnapi = tx_q->xsk_pool ? &ch->rxtx_napi : &ch->tx_napi;\n\n\tif (likely(napi_schedule_prep(napi))) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ch->lock, flags);\n\t\tstmmac_disable_dma_irq(priv, priv->ioaddr, ch->index, 0, 1);\n\t\tspin_unlock_irqrestore(&ch->lock, flags);\n\t\t__napi_schedule(napi);\n\t}\n\n\treturn HRTIMER_NORESTART;\n}\n\n \nstatic void stmmac_init_coalesce(struct stmmac_priv *priv)\n{\n\tu32 tx_channel_count = priv->plat->tx_queues_to_use;\n\tu32 rx_channel_count = priv->plat->rx_queues_to_use;\n\tu32 chan;\n\n\tfor (chan = 0; chan < tx_channel_count; chan++) {\n\t\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[chan];\n\n\t\tpriv->tx_coal_frames[chan] = STMMAC_TX_FRAMES;\n\t\tpriv->tx_coal_timer[chan] = STMMAC_COAL_TX_TIMER;\n\n\t\thrtimer_init(&tx_q->txtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\t\ttx_q->txtimer.function = stmmac_tx_timer;\n\t}\n\n\tfor (chan = 0; chan < rx_channel_count; chan++)\n\t\tpriv->rx_coal_frames[chan] = STMMAC_RX_FRAMES;\n}\n\nstatic void stmmac_set_rings_length(struct stmmac_priv *priv)\n{\n\tu32 rx_channels_count = priv->plat->rx_queues_to_use;\n\tu32 tx_channels_count = priv->plat->tx_queues_to_use;\n\tu32 chan;\n\n\t \n\tfor (chan = 0; chan < tx_channels_count; chan++)\n\t\tstmmac_set_tx_ring_len(priv, priv->ioaddr,\n\t\t\t\t       (priv->dma_conf.dma_tx_size - 1), chan);\n\n\t \n\tfor (chan = 0; chan < rx_channels_count; chan++)\n\t\tstmmac_set_rx_ring_len(priv, priv->ioaddr,\n\t\t\t\t       (priv->dma_conf.dma_rx_size - 1), chan);\n}\n\n \nstatic void stmmac_set_tx_queue_weight(struct stmmac_priv *priv)\n{\n\tu32 tx_queues_count = priv->plat->tx_queues_to_use;\n\tu32 weight;\n\tu32 queue;\n\n\tfor (queue = 0; queue < tx_queues_count; queue++) {\n\t\tweight = priv->plat->tx_queues_cfg[queue].weight;\n\t\tstmmac_set_mtl_tx_queue_weight(priv, priv->hw, weight, queue);\n\t}\n}\n\n \nstatic void stmmac_configure_cbs(struct stmmac_priv *priv)\n{\n\tu32 tx_queues_count = priv->plat->tx_queues_to_use;\n\tu32 mode_to_use;\n\tu32 queue;\n\n\t \n\tfor (queue = 1; queue < tx_queues_count; queue++) {\n\t\tmode_to_use = priv->plat->tx_queues_cfg[queue].mode_to_use;\n\t\tif (mode_to_use == MTL_QUEUE_DCB)\n\t\t\tcontinue;\n\n\t\tstmmac_config_cbs(priv, priv->hw,\n\t\t\t\tpriv->plat->tx_queues_cfg[queue].send_slope,\n\t\t\t\tpriv->plat->tx_queues_cfg[queue].idle_slope,\n\t\t\t\tpriv->plat->tx_queues_cfg[queue].high_credit,\n\t\t\t\tpriv->plat->tx_queues_cfg[queue].low_credit,\n\t\t\t\tqueue);\n\t}\n}\n\n \nstatic void stmmac_rx_queue_dma_chan_map(struct stmmac_priv *priv)\n{\n\tu32 rx_queues_count = priv->plat->rx_queues_to_use;\n\tu32 queue;\n\tu32 chan;\n\n\tfor (queue = 0; queue < rx_queues_count; queue++) {\n\t\tchan = priv->plat->rx_queues_cfg[queue].chan;\n\t\tstmmac_map_mtl_to_dma(priv, priv->hw, queue, chan);\n\t}\n}\n\n \nstatic void stmmac_mac_config_rx_queues_prio(struct stmmac_priv *priv)\n{\n\tu32 rx_queues_count = priv->plat->rx_queues_to_use;\n\tu32 queue;\n\tu32 prio;\n\n\tfor (queue = 0; queue < rx_queues_count; queue++) {\n\t\tif (!priv->plat->rx_queues_cfg[queue].use_prio)\n\t\t\tcontinue;\n\n\t\tprio = priv->plat->rx_queues_cfg[queue].prio;\n\t\tstmmac_rx_queue_prio(priv, priv->hw, prio, queue);\n\t}\n}\n\n \nstatic void stmmac_mac_config_tx_queues_prio(struct stmmac_priv *priv)\n{\n\tu32 tx_queues_count = priv->plat->tx_queues_to_use;\n\tu32 queue;\n\tu32 prio;\n\n\tfor (queue = 0; queue < tx_queues_count; queue++) {\n\t\tif (!priv->plat->tx_queues_cfg[queue].use_prio)\n\t\t\tcontinue;\n\n\t\tprio = priv->plat->tx_queues_cfg[queue].prio;\n\t\tstmmac_tx_queue_prio(priv, priv->hw, prio, queue);\n\t}\n}\n\n \nstatic void stmmac_mac_config_rx_queues_routing(struct stmmac_priv *priv)\n{\n\tu32 rx_queues_count = priv->plat->rx_queues_to_use;\n\tu32 queue;\n\tu8 packet;\n\n\tfor (queue = 0; queue < rx_queues_count; queue++) {\n\t\t \n\t\tif (priv->plat->rx_queues_cfg[queue].pkt_route == 0x0)\n\t\t\tcontinue;\n\n\t\tpacket = priv->plat->rx_queues_cfg[queue].pkt_route;\n\t\tstmmac_rx_queue_routing(priv, priv->hw, packet, queue);\n\t}\n}\n\nstatic void stmmac_mac_config_rss(struct stmmac_priv *priv)\n{\n\tif (!priv->dma_cap.rssen || !priv->plat->rss_en) {\n\t\tpriv->rss.enable = false;\n\t\treturn;\n\t}\n\n\tif (priv->dev->features & NETIF_F_RXHASH)\n\t\tpriv->rss.enable = true;\n\telse\n\t\tpriv->rss.enable = false;\n\n\tstmmac_rss_configure(priv, priv->hw, &priv->rss,\n\t\t\t     priv->plat->rx_queues_to_use);\n}\n\n \nstatic void stmmac_mtl_configuration(struct stmmac_priv *priv)\n{\n\tu32 rx_queues_count = priv->plat->rx_queues_to_use;\n\tu32 tx_queues_count = priv->plat->tx_queues_to_use;\n\n\tif (tx_queues_count > 1)\n\t\tstmmac_set_tx_queue_weight(priv);\n\n\t \n\tif (rx_queues_count > 1)\n\t\tstmmac_prog_mtl_rx_algorithms(priv, priv->hw,\n\t\t\t\tpriv->plat->rx_sched_algorithm);\n\n\t \n\tif (tx_queues_count > 1)\n\t\tstmmac_prog_mtl_tx_algorithms(priv, priv->hw,\n\t\t\t\tpriv->plat->tx_sched_algorithm);\n\n\t \n\tif (tx_queues_count > 1)\n\t\tstmmac_configure_cbs(priv);\n\n\t \n\tstmmac_rx_queue_dma_chan_map(priv);\n\n\t \n\tstmmac_mac_enable_rx_queues(priv);\n\n\t \n\tif (rx_queues_count > 1)\n\t\tstmmac_mac_config_rx_queues_prio(priv);\n\n\t \n\tif (tx_queues_count > 1)\n\t\tstmmac_mac_config_tx_queues_prio(priv);\n\n\t \n\tif (rx_queues_count > 1)\n\t\tstmmac_mac_config_rx_queues_routing(priv);\n\n\t \n\tif (rx_queues_count > 1)\n\t\tstmmac_mac_config_rss(priv);\n}\n\nstatic void stmmac_safety_feat_configuration(struct stmmac_priv *priv)\n{\n\tif (priv->dma_cap.asp) {\n\t\tnetdev_info(priv->dev, \"Enabling Safety Features\\n\");\n\t\tstmmac_safety_feat_config(priv, priv->ioaddr, priv->dma_cap.asp,\n\t\t\t\t\t  priv->plat->safety_feat_cfg);\n\t} else {\n\t\tnetdev_info(priv->dev, \"No Safety Features support found\\n\");\n\t}\n}\n\nstatic int stmmac_fpe_start_wq(struct stmmac_priv *priv)\n{\n\tchar *name;\n\n\tclear_bit(__FPE_TASK_SCHED, &priv->fpe_task_state);\n\tclear_bit(__FPE_REMOVING,  &priv->fpe_task_state);\n\n\tname = priv->wq_name;\n\tsprintf(name, \"%s-fpe\", priv->dev->name);\n\n\tpriv->fpe_wq = create_singlethread_workqueue(name);\n\tif (!priv->fpe_wq) {\n\t\tnetdev_err(priv->dev, \"%s: Failed to create workqueue\\n\", name);\n\n\t\treturn -ENOMEM;\n\t}\n\tnetdev_info(priv->dev, \"FPE workqueue start\");\n\n\treturn 0;\n}\n\n \nstatic int stmmac_hw_setup(struct net_device *dev, bool ptp_register)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 rx_cnt = priv->plat->rx_queues_to_use;\n\tu32 tx_cnt = priv->plat->tx_queues_to_use;\n\tbool sph_en;\n\tu32 chan;\n\tint ret;\n\n\t \n\tret = stmmac_init_dma_engine(priv);\n\tif (ret < 0) {\n\t\tnetdev_err(priv->dev, \"%s: DMA engine initialization failed\\n\",\n\t\t\t   __func__);\n\t\treturn ret;\n\t}\n\n\t \n\tstmmac_set_umac_addr(priv, priv->hw, dev->dev_addr, 0);\n\n\t \n\tif (priv->hw->pcs) {\n\t\tint speed = priv->plat->mac_port_sel_speed;\n\n\t\tif ((speed == SPEED_10) || (speed == SPEED_100) ||\n\t\t    (speed == SPEED_1000)) {\n\t\t\tpriv->hw->ps = speed;\n\t\t} else {\n\t\t\tdev_warn(priv->device, \"invalid port speed\\n\");\n\t\t\tpriv->hw->ps = 0;\n\t\t}\n\t}\n\n\t \n\tstmmac_core_init(priv, priv->hw, dev);\n\n\t \n\tstmmac_mtl_configuration(priv);\n\n\t \n\tstmmac_safety_feat_configuration(priv);\n\n\tret = stmmac_rx_ipc(priv, priv->hw);\n\tif (!ret) {\n\t\tnetdev_warn(priv->dev, \"RX IPC Checksum Offload disabled\\n\");\n\t\tpriv->plat->rx_coe = STMMAC_RX_COE_NONE;\n\t\tpriv->hw->rx_csum = 0;\n\t}\n\n\t \n\tstmmac_mac_set(priv, priv->ioaddr, true);\n\n\t \n\tstmmac_dma_operation_mode(priv);\n\n\tstmmac_mmc_setup(priv);\n\n\tif (ptp_register) {\n\t\tret = clk_prepare_enable(priv->plat->clk_ptp_ref);\n\t\tif (ret < 0)\n\t\t\tnetdev_warn(priv->dev,\n\t\t\t\t    \"failed to enable PTP reference clock: %pe\\n\",\n\t\t\t\t    ERR_PTR(ret));\n\t}\n\n\tret = stmmac_init_ptp(priv);\n\tif (ret == -EOPNOTSUPP)\n\t\tnetdev_info(priv->dev, \"PTP not supported by HW\\n\");\n\telse if (ret)\n\t\tnetdev_warn(priv->dev, \"PTP init failed\\n\");\n\telse if (ptp_register)\n\t\tstmmac_ptp_register(priv);\n\n\tpriv->eee_tw_timer = STMMAC_DEFAULT_TWT_LS;\n\n\t \n\tif (!priv->tx_lpi_timer)\n\t\tpriv->tx_lpi_timer = eee_timer * 1000;\n\n\tif (priv->use_riwt) {\n\t\tu32 queue;\n\n\t\tfor (queue = 0; queue < rx_cnt; queue++) {\n\t\t\tif (!priv->rx_riwt[queue])\n\t\t\t\tpriv->rx_riwt[queue] = DEF_DMA_RIWT;\n\n\t\t\tstmmac_rx_watchdog(priv, priv->ioaddr,\n\t\t\t\t\t   priv->rx_riwt[queue], queue);\n\t\t}\n\t}\n\n\tif (priv->hw->pcs)\n\t\tstmmac_pcs_ctrl_ane(priv, priv->ioaddr, 1, priv->hw->ps, 0);\n\n\t \n\tstmmac_set_rings_length(priv);\n\n\t \n\tif (priv->tso) {\n\t\tfor (chan = 0; chan < tx_cnt; chan++) {\n\t\t\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[chan];\n\n\t\t\t \n\t\t\tif (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\t\tcontinue;\n\n\t\t\tstmmac_enable_tso(priv, priv->ioaddr, 1, chan);\n\t\t}\n\t}\n\n\t \n\tsph_en = (priv->hw->rx_csum > 0) && priv->sph;\n\tfor (chan = 0; chan < rx_cnt; chan++)\n\t\tstmmac_enable_sph(priv, priv->ioaddr, sph_en, chan);\n\n\n\t \n\tif (priv->dma_cap.vlins)\n\t\tstmmac_enable_vlan(priv, priv->hw, STMMAC_VLAN_INSERT);\n\n\t \n\tfor (chan = 0; chan < tx_cnt; chan++) {\n\t\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[chan];\n\t\tint enable = tx_q->tbs & STMMAC_TBS_AVAIL;\n\n\t\tstmmac_enable_tbs(priv, priv->ioaddr, enable, chan);\n\t}\n\n\t \n\tnetif_set_real_num_rx_queues(dev, priv->plat->rx_queues_to_use);\n\tnetif_set_real_num_tx_queues(dev, priv->plat->tx_queues_to_use);\n\n\t \n\tstmmac_start_all_dma(priv);\n\n\tif (priv->dma_cap.fpesel) {\n\t\tstmmac_fpe_start_wq(priv);\n\n\t\tif (priv->plat->fpe_cfg->enable)\n\t\t\tstmmac_fpe_handshake(priv, true);\n\t}\n\n\treturn 0;\n}\n\nstatic void stmmac_hw_teardown(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tclk_disable_unprepare(priv->plat->clk_ptp_ref);\n}\n\nstatic void stmmac_free_irq(struct net_device *dev,\n\t\t\t    enum request_irq_err irq_err, int irq_idx)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tint j;\n\n\tswitch (irq_err) {\n\tcase REQ_IRQ_ERR_ALL:\n\t\tirq_idx = priv->plat->tx_queues_to_use;\n\t\tfallthrough;\n\tcase REQ_IRQ_ERR_TX:\n\t\tfor (j = irq_idx - 1; j >= 0; j--) {\n\t\t\tif (priv->tx_irq[j] > 0) {\n\t\t\t\tirq_set_affinity_hint(priv->tx_irq[j], NULL);\n\t\t\t\tfree_irq(priv->tx_irq[j], &priv->dma_conf.tx_queue[j]);\n\t\t\t}\n\t\t}\n\t\tirq_idx = priv->plat->rx_queues_to_use;\n\t\tfallthrough;\n\tcase REQ_IRQ_ERR_RX:\n\t\tfor (j = irq_idx - 1; j >= 0; j--) {\n\t\t\tif (priv->rx_irq[j] > 0) {\n\t\t\t\tirq_set_affinity_hint(priv->rx_irq[j], NULL);\n\t\t\t\tfree_irq(priv->rx_irq[j], &priv->dma_conf.rx_queue[j]);\n\t\t\t}\n\t\t}\n\n\t\tif (priv->sfty_ue_irq > 0 && priv->sfty_ue_irq != dev->irq)\n\t\t\tfree_irq(priv->sfty_ue_irq, dev);\n\t\tfallthrough;\n\tcase REQ_IRQ_ERR_SFTY_UE:\n\t\tif (priv->sfty_ce_irq > 0 && priv->sfty_ce_irq != dev->irq)\n\t\t\tfree_irq(priv->sfty_ce_irq, dev);\n\t\tfallthrough;\n\tcase REQ_IRQ_ERR_SFTY_CE:\n\t\tif (priv->lpi_irq > 0 && priv->lpi_irq != dev->irq)\n\t\t\tfree_irq(priv->lpi_irq, dev);\n\t\tfallthrough;\n\tcase REQ_IRQ_ERR_LPI:\n\t\tif (priv->wol_irq > 0 && priv->wol_irq != dev->irq)\n\t\t\tfree_irq(priv->wol_irq, dev);\n\t\tfallthrough;\n\tcase REQ_IRQ_ERR_WOL:\n\t\tfree_irq(dev->irq, dev);\n\t\tfallthrough;\n\tcase REQ_IRQ_ERR_MAC:\n\tcase REQ_IRQ_ERR_NO:\n\t\t \n\t\tbreak;\n\t}\n}\n\nstatic int stmmac_request_irq_multi_msi(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tenum request_irq_err irq_err;\n\tcpumask_t cpu_mask;\n\tint irq_idx = 0;\n\tchar *int_name;\n\tint ret;\n\tint i;\n\n\t \n\tint_name = priv->int_name_mac;\n\tsprintf(int_name, \"%s:%s\", dev->name, \"mac\");\n\tret = request_irq(dev->irq, stmmac_mac_interrupt,\n\t\t\t  0, int_name, dev);\n\tif (unlikely(ret < 0)) {\n\t\tnetdev_err(priv->dev,\n\t\t\t   \"%s: alloc mac MSI %d (error: %d)\\n\",\n\t\t\t   __func__, dev->irq, ret);\n\t\tirq_err = REQ_IRQ_ERR_MAC;\n\t\tgoto irq_error;\n\t}\n\n\t \n\tpriv->wol_irq_disabled = true;\n\tif (priv->wol_irq > 0 && priv->wol_irq != dev->irq) {\n\t\tint_name = priv->int_name_wol;\n\t\tsprintf(int_name, \"%s:%s\", dev->name, \"wol\");\n\t\tret = request_irq(priv->wol_irq,\n\t\t\t\t  stmmac_mac_interrupt,\n\t\t\t\t  0, int_name, dev);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: alloc wol MSI %d (error: %d)\\n\",\n\t\t\t\t   __func__, priv->wol_irq, ret);\n\t\t\tirq_err = REQ_IRQ_ERR_WOL;\n\t\t\tgoto irq_error;\n\t\t}\n\t}\n\n\t \n\tif (priv->lpi_irq > 0 && priv->lpi_irq != dev->irq) {\n\t\tint_name = priv->int_name_lpi;\n\t\tsprintf(int_name, \"%s:%s\", dev->name, \"lpi\");\n\t\tret = request_irq(priv->lpi_irq,\n\t\t\t\t  stmmac_mac_interrupt,\n\t\t\t\t  0, int_name, dev);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: alloc lpi MSI %d (error: %d)\\n\",\n\t\t\t\t   __func__, priv->lpi_irq, ret);\n\t\t\tirq_err = REQ_IRQ_ERR_LPI;\n\t\t\tgoto irq_error;\n\t\t}\n\t}\n\n\t \n\tif (priv->sfty_ce_irq > 0 && priv->sfty_ce_irq != dev->irq) {\n\t\tint_name = priv->int_name_sfty_ce;\n\t\tsprintf(int_name, \"%s:%s\", dev->name, \"safety-ce\");\n\t\tret = request_irq(priv->sfty_ce_irq,\n\t\t\t\t  stmmac_safety_interrupt,\n\t\t\t\t  0, int_name, dev);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: alloc sfty ce MSI %d (error: %d)\\n\",\n\t\t\t\t   __func__, priv->sfty_ce_irq, ret);\n\t\t\tirq_err = REQ_IRQ_ERR_SFTY_CE;\n\t\t\tgoto irq_error;\n\t\t}\n\t}\n\n\t \n\tif (priv->sfty_ue_irq > 0 && priv->sfty_ue_irq != dev->irq) {\n\t\tint_name = priv->int_name_sfty_ue;\n\t\tsprintf(int_name, \"%s:%s\", dev->name, \"safety-ue\");\n\t\tret = request_irq(priv->sfty_ue_irq,\n\t\t\t\t  stmmac_safety_interrupt,\n\t\t\t\t  0, int_name, dev);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: alloc sfty ue MSI %d (error: %d)\\n\",\n\t\t\t\t   __func__, priv->sfty_ue_irq, ret);\n\t\t\tirq_err = REQ_IRQ_ERR_SFTY_UE;\n\t\t\tgoto irq_error;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < priv->plat->rx_queues_to_use; i++) {\n\t\tif (i >= MTL_MAX_RX_QUEUES)\n\t\t\tbreak;\n\t\tif (priv->rx_irq[i] == 0)\n\t\t\tcontinue;\n\n\t\tint_name = priv->int_name_rx_irq[i];\n\t\tsprintf(int_name, \"%s:%s-%d\", dev->name, \"rx\", i);\n\t\tret = request_irq(priv->rx_irq[i],\n\t\t\t\t  stmmac_msi_intr_rx,\n\t\t\t\t  0, int_name, &priv->dma_conf.rx_queue[i]);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: alloc rx-%d  MSI %d (error: %d)\\n\",\n\t\t\t\t   __func__, i, priv->rx_irq[i], ret);\n\t\t\tirq_err = REQ_IRQ_ERR_RX;\n\t\t\tirq_idx = i;\n\t\t\tgoto irq_error;\n\t\t}\n\t\tcpumask_clear(&cpu_mask);\n\t\tcpumask_set_cpu(i % num_online_cpus(), &cpu_mask);\n\t\tirq_set_affinity_hint(priv->rx_irq[i], &cpu_mask);\n\t}\n\n\t \n\tfor (i = 0; i < priv->plat->tx_queues_to_use; i++) {\n\t\tif (i >= MTL_MAX_TX_QUEUES)\n\t\t\tbreak;\n\t\tif (priv->tx_irq[i] == 0)\n\t\t\tcontinue;\n\n\t\tint_name = priv->int_name_tx_irq[i];\n\t\tsprintf(int_name, \"%s:%s-%d\", dev->name, \"tx\", i);\n\t\tret = request_irq(priv->tx_irq[i],\n\t\t\t\t  stmmac_msi_intr_tx,\n\t\t\t\t  0, int_name, &priv->dma_conf.tx_queue[i]);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: alloc tx-%d  MSI %d (error: %d)\\n\",\n\t\t\t\t   __func__, i, priv->tx_irq[i], ret);\n\t\t\tirq_err = REQ_IRQ_ERR_TX;\n\t\t\tirq_idx = i;\n\t\t\tgoto irq_error;\n\t\t}\n\t\tcpumask_clear(&cpu_mask);\n\t\tcpumask_set_cpu(i % num_online_cpus(), &cpu_mask);\n\t\tirq_set_affinity_hint(priv->tx_irq[i], &cpu_mask);\n\t}\n\n\treturn 0;\n\nirq_error:\n\tstmmac_free_irq(dev, irq_err, irq_idx);\n\treturn ret;\n}\n\nstatic int stmmac_request_irq_single(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tenum request_irq_err irq_err;\n\tint ret;\n\n\tret = request_irq(dev->irq, stmmac_interrupt,\n\t\t\t  IRQF_SHARED, dev->name, dev);\n\tif (unlikely(ret < 0)) {\n\t\tnetdev_err(priv->dev,\n\t\t\t   \"%s: ERROR: allocating the IRQ %d (error: %d)\\n\",\n\t\t\t   __func__, dev->irq, ret);\n\t\tirq_err = REQ_IRQ_ERR_MAC;\n\t\tgoto irq_error;\n\t}\n\n\t \n\tif (priv->wol_irq > 0 && priv->wol_irq != dev->irq) {\n\t\tret = request_irq(priv->wol_irq, stmmac_interrupt,\n\t\t\t\t  IRQF_SHARED, dev->name, dev);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: ERROR: allocating the WoL IRQ %d (%d)\\n\",\n\t\t\t\t   __func__, priv->wol_irq, ret);\n\t\t\tirq_err = REQ_IRQ_ERR_WOL;\n\t\t\tgoto irq_error;\n\t\t}\n\t}\n\n\t \n\tif (priv->lpi_irq > 0 && priv->lpi_irq != dev->irq) {\n\t\tret = request_irq(priv->lpi_irq, stmmac_interrupt,\n\t\t\t\t  IRQF_SHARED, dev->name, dev);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: ERROR: allocating the LPI IRQ %d (%d)\\n\",\n\t\t\t\t   __func__, priv->lpi_irq, ret);\n\t\t\tirq_err = REQ_IRQ_ERR_LPI;\n\t\t\tgoto irq_error;\n\t\t}\n\t}\n\n\treturn 0;\n\nirq_error:\n\tstmmac_free_irq(dev, irq_err, 0);\n\treturn ret;\n}\n\nstatic int stmmac_request_irq(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tint ret;\n\n\t \n\tif (priv->plat->flags & STMMAC_FLAG_MULTI_MSI_EN)\n\t\tret = stmmac_request_irq_multi_msi(dev);\n\telse\n\t\tret = stmmac_request_irq_single(dev);\n\n\treturn ret;\n}\n\n \nstatic struct stmmac_dma_conf *\nstmmac_setup_dma_desc(struct stmmac_priv *priv, unsigned int mtu)\n{\n\tstruct stmmac_dma_conf *dma_conf;\n\tint chan, bfsize, ret;\n\n\tdma_conf = kzalloc(sizeof(*dma_conf), GFP_KERNEL);\n\tif (!dma_conf) {\n\t\tnetdev_err(priv->dev, \"%s: DMA conf allocation failed\\n\",\n\t\t\t   __func__);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tbfsize = stmmac_set_16kib_bfsize(priv, mtu);\n\tif (bfsize < 0)\n\t\tbfsize = 0;\n\n\tif (bfsize < BUF_SIZE_16KiB)\n\t\tbfsize = stmmac_set_bfsize(mtu, 0);\n\n\tdma_conf->dma_buf_sz = bfsize;\n\t \n\tdma_conf->dma_tx_size = priv->dma_conf.dma_tx_size;\n\tdma_conf->dma_rx_size = priv->dma_conf.dma_rx_size;\n\n\tif (!dma_conf->dma_tx_size)\n\t\tdma_conf->dma_tx_size = DMA_DEFAULT_TX_SIZE;\n\tif (!dma_conf->dma_rx_size)\n\t\tdma_conf->dma_rx_size = DMA_DEFAULT_RX_SIZE;\n\n\t \n\tfor (chan = 0; chan < priv->plat->tx_queues_to_use; chan++) {\n\t\tstruct stmmac_tx_queue *tx_q = &dma_conf->tx_queue[chan];\n\t\tint tbs_en = priv->plat->tx_queues_cfg[chan].tbs_en;\n\n\t\t \n\t\ttx_q->tbs |= tbs_en ? STMMAC_TBS_AVAIL : 0;\n\t}\n\n\tret = alloc_dma_desc_resources(priv, dma_conf);\n\tif (ret < 0) {\n\t\tnetdev_err(priv->dev, \"%s: DMA descriptors allocation failed\\n\",\n\t\t\t   __func__);\n\t\tgoto alloc_error;\n\t}\n\n\tret = init_dma_desc_rings(priv->dev, dma_conf, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tnetdev_err(priv->dev, \"%s: DMA descriptors initialization failed\\n\",\n\t\t\t   __func__);\n\t\tgoto init_error;\n\t}\n\n\treturn dma_conf;\n\ninit_error:\n\tfree_dma_desc_resources(priv, dma_conf);\nalloc_error:\n\tkfree(dma_conf);\n\treturn ERR_PTR(ret);\n}\n\n \nstatic int __stmmac_open(struct net_device *dev,\n\t\t\t struct stmmac_dma_conf *dma_conf)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tint mode = priv->plat->phy_interface;\n\tu32 chan;\n\tint ret;\n\n\tret = pm_runtime_resume_and_get(priv->device);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (priv->hw->pcs != STMMAC_PCS_TBI &&\n\t    priv->hw->pcs != STMMAC_PCS_RTBI &&\n\t    (!priv->hw->xpcs ||\n\t     xpcs_get_an_mode(priv->hw->xpcs, mode) != DW_AN_C73) &&\n\t    !priv->hw->lynx_pcs) {\n\t\tret = stmmac_init_phy(dev);\n\t\tif (ret) {\n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: Cannot attach to PHY (error: %d)\\n\",\n\t\t\t\t   __func__, ret);\n\t\t\tgoto init_phy_error;\n\t\t}\n\t}\n\n\tpriv->rx_copybreak = STMMAC_RX_COPYBREAK;\n\n\tbuf_sz = dma_conf->dma_buf_sz;\n\tmemcpy(&priv->dma_conf, dma_conf, sizeof(*dma_conf));\n\n\tstmmac_reset_queues_param(priv);\n\n\tif (!(priv->plat->flags & STMMAC_FLAG_SERDES_UP_AFTER_PHY_LINKUP) &&\n\t    priv->plat->serdes_powerup) {\n\t\tret = priv->plat->serdes_powerup(dev, priv->plat->bsp_priv);\n\t\tif (ret < 0) {\n\t\t\tnetdev_err(priv->dev, \"%s: Serdes powerup failed\\n\",\n\t\t\t\t   __func__);\n\t\t\tgoto init_error;\n\t\t}\n\t}\n\n\tret = stmmac_hw_setup(dev, true);\n\tif (ret < 0) {\n\t\tnetdev_err(priv->dev, \"%s: Hw setup failed\\n\", __func__);\n\t\tgoto init_error;\n\t}\n\n\tstmmac_init_coalesce(priv);\n\n\tphylink_start(priv->phylink);\n\t \n\tphylink_speed_up(priv->phylink);\n\n\tret = stmmac_request_irq(dev);\n\tif (ret)\n\t\tgoto irq_error;\n\n\tstmmac_enable_all_queues(priv);\n\tnetif_tx_start_all_queues(priv->dev);\n\tstmmac_enable_all_dma_irq(priv);\n\n\treturn 0;\n\nirq_error:\n\tphylink_stop(priv->phylink);\n\n\tfor (chan = 0; chan < priv->plat->tx_queues_to_use; chan++)\n\t\thrtimer_cancel(&priv->dma_conf.tx_queue[chan].txtimer);\n\n\tstmmac_hw_teardown(dev);\ninit_error:\n\tphylink_disconnect_phy(priv->phylink);\ninit_phy_error:\n\tpm_runtime_put(priv->device);\n\treturn ret;\n}\n\nstatic int stmmac_open(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tstruct stmmac_dma_conf *dma_conf;\n\tint ret;\n\n\tdma_conf = stmmac_setup_dma_desc(priv, dev->mtu);\n\tif (IS_ERR(dma_conf))\n\t\treturn PTR_ERR(dma_conf);\n\n\tret = __stmmac_open(dev, dma_conf);\n\tif (ret)\n\t\tfree_dma_desc_resources(priv, dma_conf);\n\n\tkfree(dma_conf);\n\treturn ret;\n}\n\nstatic void stmmac_fpe_stop_wq(struct stmmac_priv *priv)\n{\n\tset_bit(__FPE_REMOVING, &priv->fpe_task_state);\n\n\tif (priv->fpe_wq)\n\t\tdestroy_workqueue(priv->fpe_wq);\n\n\tnetdev_info(priv->dev, \"FPE workqueue stop\");\n}\n\n \nstatic int stmmac_release(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 chan;\n\n\tif (device_may_wakeup(priv->device))\n\t\tphylink_speed_down(priv->phylink, false);\n\t \n\tphylink_stop(priv->phylink);\n\tphylink_disconnect_phy(priv->phylink);\n\n\tstmmac_disable_all_queues(priv);\n\n\tfor (chan = 0; chan < priv->plat->tx_queues_to_use; chan++)\n\t\thrtimer_cancel(&priv->dma_conf.tx_queue[chan].txtimer);\n\n\tnetif_tx_disable(dev);\n\n\t \n\tstmmac_free_irq(dev, REQ_IRQ_ERR_ALL, 0);\n\n\tif (priv->eee_enabled) {\n\t\tpriv->tx_path_in_lpi_mode = false;\n\t\tdel_timer_sync(&priv->eee_ctrl_timer);\n\t}\n\n\t \n\tstmmac_stop_all_dma(priv);\n\n\t \n\tfree_dma_desc_resources(priv, &priv->dma_conf);\n\n\t \n\tstmmac_mac_set(priv, priv->ioaddr, false);\n\n\t \n\tif (priv->plat->serdes_powerdown)\n\t\tpriv->plat->serdes_powerdown(dev, priv->plat->bsp_priv);\n\n\tnetif_carrier_off(dev);\n\n\tstmmac_release_ptp(priv);\n\n\tpm_runtime_put(priv->device);\n\n\tif (priv->dma_cap.fpesel)\n\t\tstmmac_fpe_stop_wq(priv);\n\n\treturn 0;\n}\n\nstatic bool stmmac_vlan_insert(struct stmmac_priv *priv, struct sk_buff *skb,\n\t\t\t       struct stmmac_tx_queue *tx_q)\n{\n\tu16 tag = 0x0, inner_tag = 0x0;\n\tu32 inner_type = 0x0;\n\tstruct dma_desc *p;\n\n\tif (!priv->dma_cap.vlins)\n\t\treturn false;\n\tif (!skb_vlan_tag_present(skb))\n\t\treturn false;\n\tif (skb->vlan_proto == htons(ETH_P_8021AD)) {\n\t\tinner_tag = skb_vlan_tag_get(skb);\n\t\tinner_type = STMMAC_VLAN_INSERT;\n\t}\n\n\ttag = skb_vlan_tag_get(skb);\n\n\tif (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\tp = &tx_q->dma_entx[tx_q->cur_tx].basic;\n\telse\n\t\tp = &tx_q->dma_tx[tx_q->cur_tx];\n\n\tif (stmmac_set_desc_vlan_tag(priv, p, tag, inner_tag, inner_type))\n\t\treturn false;\n\n\tstmmac_set_tx_owner(priv, p);\n\ttx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size);\n\treturn true;\n}\n\n \nstatic void stmmac_tso_allocator(struct stmmac_priv *priv, dma_addr_t des,\n\t\t\t\t int total_len, bool last_segment, u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\tstruct dma_desc *desc;\n\tu32 buff_size;\n\tint tmp_len;\n\n\ttmp_len = total_len;\n\n\twhile (tmp_len > 0) {\n\t\tdma_addr_t curr_addr;\n\n\t\ttx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx,\n\t\t\t\t\t\tpriv->dma_conf.dma_tx_size);\n\t\tWARN_ON(tx_q->tx_skbuff[tx_q->cur_tx]);\n\n\t\tif (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\tdesc = &tx_q->dma_entx[tx_q->cur_tx].basic;\n\t\telse\n\t\t\tdesc = &tx_q->dma_tx[tx_q->cur_tx];\n\n\t\tcurr_addr = des + (total_len - tmp_len);\n\t\tif (priv->dma_cap.addr64 <= 32)\n\t\t\tdesc->des0 = cpu_to_le32(curr_addr);\n\t\telse\n\t\t\tstmmac_set_desc_addr(priv, desc, curr_addr);\n\n\t\tbuff_size = tmp_len >= TSO_MAX_BUFF_SIZE ?\n\t\t\t    TSO_MAX_BUFF_SIZE : tmp_len;\n\n\t\tstmmac_prepare_tso_tx_desc(priv, desc, 0, buff_size,\n\t\t\t\t0, 1,\n\t\t\t\t(last_segment) && (tmp_len <= TSO_MAX_BUFF_SIZE),\n\t\t\t\t0, 0);\n\n\t\ttmp_len -= TSO_MAX_BUFF_SIZE;\n\t}\n}\n\nstatic void stmmac_flush_tx_descriptors(struct stmmac_priv *priv, int queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\tint desc_size;\n\n\tif (likely(priv->extend_desc))\n\t\tdesc_size = sizeof(struct dma_extended_desc);\n\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\tdesc_size = sizeof(struct dma_edesc);\n\telse\n\t\tdesc_size = sizeof(struct dma_desc);\n\n\t \n\twmb();\n\n\ttx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx * desc_size);\n\tstmmac_set_tx_tail_ptr(priv, priv->ioaddr, tx_q->tx_tail_addr, queue);\n}\n\n \nstatic netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct dma_desc *desc, *first, *mss_desc = NULL;\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tint nfrags = skb_shinfo(skb)->nr_frags;\n\tu32 queue = skb_get_queue_mapping(skb);\n\tunsigned int first_entry, tx_packets;\n\tstruct stmmac_txq_stats *txq_stats;\n\tint tmp_pay_len = 0, first_tx;\n\tstruct stmmac_tx_queue *tx_q;\n\tbool has_vlan, set_ic;\n\tu8 proto_hdr_len, hdr;\n\tunsigned long flags;\n\tu32 pay_len, mss;\n\tdma_addr_t des;\n\tint i;\n\n\ttx_q = &priv->dma_conf.tx_queue[queue];\n\ttxq_stats = &priv->xstats.txq_stats[queue];\n\tfirst_tx = tx_q->cur_tx;\n\n\t \n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {\n\t\tproto_hdr_len = skb_transport_offset(skb) + sizeof(struct udphdr);\n\t\thdr = sizeof(struct udphdr);\n\t} else {\n\t\tproto_hdr_len = skb_tcp_all_headers(skb);\n\t\thdr = tcp_hdrlen(skb);\n\t}\n\n\t \n\tif (unlikely(stmmac_tx_avail(priv, queue) <\n\t\t(((skb->len - proto_hdr_len) / TSO_MAX_BUFF_SIZE + 1)))) {\n\t\tif (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, queue))) {\n\t\t\tnetif_tx_stop_queue(netdev_get_tx_queue(priv->dev,\n\t\t\t\t\t\t\t\tqueue));\n\t\t\t \n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: Tx Ring full when queue awake\\n\",\n\t\t\t\t   __func__);\n\t\t}\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tpay_len = skb_headlen(skb) - proto_hdr_len;  \n\n\tmss = skb_shinfo(skb)->gso_size;\n\n\t \n\tif (mss != tx_q->mss) {\n\t\tif (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\tmss_desc = &tx_q->dma_entx[tx_q->cur_tx].basic;\n\t\telse\n\t\t\tmss_desc = &tx_q->dma_tx[tx_q->cur_tx];\n\n\t\tstmmac_set_mss(priv, mss_desc, mss);\n\t\ttx_q->mss = mss;\n\t\ttx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx,\n\t\t\t\t\t\tpriv->dma_conf.dma_tx_size);\n\t\tWARN_ON(tx_q->tx_skbuff[tx_q->cur_tx]);\n\t}\n\n\tif (netif_msg_tx_queued(priv)) {\n\t\tpr_info(\"%s: hdrlen %d, hdr_len %d, pay_len %d, mss %d\\n\",\n\t\t\t__func__, hdr, proto_hdr_len, pay_len, mss);\n\t\tpr_info(\"\\tskb->len %d, skb->data_len %d\\n\", skb->len,\n\t\t\tskb->data_len);\n\t}\n\n\t \n\thas_vlan = stmmac_vlan_insert(priv, skb, tx_q);\n\n\tfirst_entry = tx_q->cur_tx;\n\tWARN_ON(tx_q->tx_skbuff[first_entry]);\n\n\tif (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\tdesc = &tx_q->dma_entx[first_entry].basic;\n\telse\n\t\tdesc = &tx_q->dma_tx[first_entry];\n\tfirst = desc;\n\n\tif (has_vlan)\n\t\tstmmac_set_desc_vlan(priv, first, STMMAC_VLAN_INSERT);\n\n\t \n\tdes = dma_map_single(priv->device, skb->data, skb_headlen(skb),\n\t\t\t     DMA_TO_DEVICE);\n\tif (dma_mapping_error(priv->device, des))\n\t\tgoto dma_map_err;\n\n\ttx_q->tx_skbuff_dma[first_entry].buf = des;\n\ttx_q->tx_skbuff_dma[first_entry].len = skb_headlen(skb);\n\ttx_q->tx_skbuff_dma[first_entry].map_as_page = false;\n\ttx_q->tx_skbuff_dma[first_entry].buf_type = STMMAC_TXBUF_T_SKB;\n\n\tif (priv->dma_cap.addr64 <= 32) {\n\t\tfirst->des0 = cpu_to_le32(des);\n\n\t\t \n\t\tif (pay_len)\n\t\t\tfirst->des1 = cpu_to_le32(des + proto_hdr_len);\n\n\t\t \n\t\ttmp_pay_len = pay_len - TSO_MAX_BUFF_SIZE;\n\t} else {\n\t\tstmmac_set_desc_addr(priv, first, des);\n\t\ttmp_pay_len = pay_len;\n\t\tdes += proto_hdr_len;\n\t\tpay_len = 0;\n\t}\n\n\tstmmac_tso_allocator(priv, des, tmp_pay_len, (nfrags == 0), queue);\n\n\t \n\tfor (i = 0; i < nfrags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tdes = skb_frag_dma_map(priv->device, frag, 0,\n\t\t\t\t       skb_frag_size(frag),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(priv->device, des))\n\t\t\tgoto dma_map_err;\n\n\t\tstmmac_tso_allocator(priv, des, skb_frag_size(frag),\n\t\t\t\t     (i == nfrags - 1), queue);\n\n\t\ttx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des;\n\t\ttx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_frag_size(frag);\n\t\ttx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = true;\n\t\ttx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB;\n\t}\n\n\ttx_q->tx_skbuff_dma[tx_q->cur_tx].last_segment = true;\n\n\t \n\ttx_q->tx_skbuff[tx_q->cur_tx] = skb;\n\ttx_q->tx_skbuff_dma[tx_q->cur_tx].buf_type = STMMAC_TXBUF_T_SKB;\n\n\t \n\ttx_packets = (tx_q->cur_tx + 1) - first_tx;\n\ttx_q->tx_count_frames += tx_packets;\n\n\tif ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en)\n\t\tset_ic = true;\n\telse if (!priv->tx_coal_frames[queue])\n\t\tset_ic = false;\n\telse if (tx_packets > priv->tx_coal_frames[queue])\n\t\tset_ic = true;\n\telse if ((tx_q->tx_count_frames %\n\t\t  priv->tx_coal_frames[queue]) < tx_packets)\n\t\tset_ic = true;\n\telse\n\t\tset_ic = false;\n\n\tif (set_ic) {\n\t\tif (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\tdesc = &tx_q->dma_entx[tx_q->cur_tx].basic;\n\t\telse\n\t\t\tdesc = &tx_q->dma_tx[tx_q->cur_tx];\n\n\t\ttx_q->tx_count_frames = 0;\n\t\tstmmac_set_tx_ic(priv, desc);\n\t}\n\n\t \n\ttx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, priv->dma_conf.dma_tx_size);\n\n\tif (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) {\n\t\tnetif_dbg(priv, hw, priv->dev, \"%s: stop transmitted packets\\n\",\n\t\t\t  __func__);\n\t\tnetif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue));\n\t}\n\n\tflags = u64_stats_update_begin_irqsave(&txq_stats->syncp);\n\ttxq_stats->tx_bytes += skb->len;\n\ttxq_stats->tx_tso_frames++;\n\ttxq_stats->tx_tso_nfrags += nfrags;\n\tif (set_ic)\n\t\ttxq_stats->tx_set_ic_bit++;\n\tu64_stats_update_end_irqrestore(&txq_stats->syncp, flags);\n\n\tif (priv->sarc_type)\n\t\tstmmac_set_desc_sarc(priv, first, priv->sarc_type);\n\n\tskb_tx_timestamp(skb);\n\n\tif (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\n\t\t     priv->hwts_tx_en)) {\n\t\t \n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\tstmmac_enable_tx_timestamp(priv, first);\n\t}\n\n\t \n\tstmmac_prepare_tso_tx_desc(priv, first, 1,\n\t\t\tproto_hdr_len,\n\t\t\tpay_len,\n\t\t\t1, tx_q->tx_skbuff_dma[first_entry].last_segment,\n\t\t\thdr / 4, (skb->len - proto_hdr_len));\n\n\t \n\tif (mss_desc) {\n\t\t \n\t\tdma_wmb();\n\t\tstmmac_set_tx_owner(priv, mss_desc);\n\t}\n\n\tif (netif_msg_pktdata(priv)) {\n\t\tpr_info(\"%s: curr=%d dirty=%d f=%d, e=%d, f_p=%p, nfrags %d\\n\",\n\t\t\t__func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry,\n\t\t\ttx_q->cur_tx, first, nfrags);\n\t\tpr_info(\">>> frame to be transmitted: \");\n\t\tprint_pkt(skb->data, skb_headlen(skb));\n\t}\n\n\tnetdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len);\n\n\tstmmac_flush_tx_descriptors(priv, queue);\n\tstmmac_tx_timer_arm(priv, queue);\n\n\treturn NETDEV_TX_OK;\n\ndma_map_err:\n\tdev_err(priv->device, \"Tx dma map failed\\n\");\n\tdev_kfree_skb(skb);\n\tpriv->xstats.tx_dropped++;\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tunsigned int first_entry, tx_packets, enh_desc;\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tunsigned int nopaged_len = skb_headlen(skb);\n\tint i, csum_insertion = 0, is_jumbo = 0;\n\tu32 queue = skb_get_queue_mapping(skb);\n\tint nfrags = skb_shinfo(skb)->nr_frags;\n\tint gso = skb_shinfo(skb)->gso_type;\n\tstruct stmmac_txq_stats *txq_stats;\n\tstruct dma_edesc *tbs_desc = NULL;\n\tstruct dma_desc *desc, *first;\n\tstruct stmmac_tx_queue *tx_q;\n\tbool has_vlan, set_ic;\n\tint entry, first_tx;\n\tunsigned long flags;\n\tdma_addr_t des;\n\n\ttx_q = &priv->dma_conf.tx_queue[queue];\n\ttxq_stats = &priv->xstats.txq_stats[queue];\n\tfirst_tx = tx_q->cur_tx;\n\n\tif (priv->tx_path_in_lpi_mode && priv->eee_sw_timer_en)\n\t\tstmmac_disable_eee_mode(priv);\n\n\t \n\tif (skb_is_gso(skb) && priv->tso) {\n\t\tif (gso & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6))\n\t\t\treturn stmmac_tso_xmit(skb, dev);\n\t\tif (priv->plat->has_gmac4 && (gso & SKB_GSO_UDP_L4))\n\t\t\treturn stmmac_tso_xmit(skb, dev);\n\t}\n\n\tif (unlikely(stmmac_tx_avail(priv, queue) < nfrags + 1)) {\n\t\tif (!netif_tx_queue_stopped(netdev_get_tx_queue(dev, queue))) {\n\t\t\tnetif_tx_stop_queue(netdev_get_tx_queue(priv->dev,\n\t\t\t\t\t\t\t\tqueue));\n\t\t\t \n\t\t\tnetdev_err(priv->dev,\n\t\t\t\t   \"%s: Tx Ring full when queue awake\\n\",\n\t\t\t\t   __func__);\n\t\t}\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\t \n\thas_vlan = stmmac_vlan_insert(priv, skb, tx_q);\n\n\tentry = tx_q->cur_tx;\n\tfirst_entry = entry;\n\tWARN_ON(tx_q->tx_skbuff[first_entry]);\n\n\tcsum_insertion = (skb->ip_summed == CHECKSUM_PARTIAL);\n\n\tif (likely(priv->extend_desc))\n\t\tdesc = (struct dma_desc *)(tx_q->dma_etx + entry);\n\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\tdesc = &tx_q->dma_entx[entry].basic;\n\telse\n\t\tdesc = tx_q->dma_tx + entry;\n\n\tfirst = desc;\n\n\tif (has_vlan)\n\t\tstmmac_set_desc_vlan(priv, first, STMMAC_VLAN_INSERT);\n\n\tenh_desc = priv->plat->enh_desc;\n\t \n\tif (enh_desc)\n\t\tis_jumbo = stmmac_is_jumbo_frm(priv, skb->len, enh_desc);\n\n\tif (unlikely(is_jumbo)) {\n\t\tentry = stmmac_jumbo_frm(priv, tx_q, skb, csum_insertion);\n\t\tif (unlikely(entry < 0) && (entry != -EINVAL))\n\t\t\tgoto dma_map_err;\n\t}\n\n\tfor (i = 0; i < nfrags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tint len = skb_frag_size(frag);\n\t\tbool last_segment = (i == (nfrags - 1));\n\n\t\tentry = STMMAC_GET_ENTRY(entry, priv->dma_conf.dma_tx_size);\n\t\tWARN_ON(tx_q->tx_skbuff[entry]);\n\n\t\tif (likely(priv->extend_desc))\n\t\t\tdesc = (struct dma_desc *)(tx_q->dma_etx + entry);\n\t\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\tdesc = &tx_q->dma_entx[entry].basic;\n\t\telse\n\t\t\tdesc = tx_q->dma_tx + entry;\n\n\t\tdes = skb_frag_dma_map(priv->device, frag, 0, len,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(priv->device, des))\n\t\t\tgoto dma_map_err;  \n\n\t\ttx_q->tx_skbuff_dma[entry].buf = des;\n\n\t\tstmmac_set_desc_addr(priv, desc, des);\n\n\t\ttx_q->tx_skbuff_dma[entry].map_as_page = true;\n\t\ttx_q->tx_skbuff_dma[entry].len = len;\n\t\ttx_q->tx_skbuff_dma[entry].last_segment = last_segment;\n\t\ttx_q->tx_skbuff_dma[entry].buf_type = STMMAC_TXBUF_T_SKB;\n\n\t\t \n\t\tstmmac_prepare_tx_desc(priv, desc, 0, len, csum_insertion,\n\t\t\t\tpriv->mode, 1, last_segment, skb->len);\n\t}\n\n\t \n\ttx_q->tx_skbuff[entry] = skb;\n\ttx_q->tx_skbuff_dma[entry].buf_type = STMMAC_TXBUF_T_SKB;\n\n\t \n\ttx_packets = (entry + 1) - first_tx;\n\ttx_q->tx_count_frames += tx_packets;\n\n\tif ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && priv->hwts_tx_en)\n\t\tset_ic = true;\n\telse if (!priv->tx_coal_frames[queue])\n\t\tset_ic = false;\n\telse if (tx_packets > priv->tx_coal_frames[queue])\n\t\tset_ic = true;\n\telse if ((tx_q->tx_count_frames %\n\t\t  priv->tx_coal_frames[queue]) < tx_packets)\n\t\tset_ic = true;\n\telse\n\t\tset_ic = false;\n\n\tif (set_ic) {\n\t\tif (likely(priv->extend_desc))\n\t\t\tdesc = &tx_q->dma_etx[entry].basic;\n\t\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\t\tdesc = &tx_q->dma_entx[entry].basic;\n\t\telse\n\t\t\tdesc = &tx_q->dma_tx[entry];\n\n\t\ttx_q->tx_count_frames = 0;\n\t\tstmmac_set_tx_ic(priv, desc);\n\t}\n\n\t \n\tentry = STMMAC_GET_ENTRY(entry, priv->dma_conf.dma_tx_size);\n\ttx_q->cur_tx = entry;\n\n\tif (netif_msg_pktdata(priv)) {\n\t\tnetdev_dbg(priv->dev,\n\t\t\t   \"%s: curr=%d dirty=%d f=%d, e=%d, first=%p, nfrags=%d\",\n\t\t\t   __func__, tx_q->cur_tx, tx_q->dirty_tx, first_entry,\n\t\t\t   entry, first, nfrags);\n\n\t\tnetdev_dbg(priv->dev, \">>> frame to be transmitted: \");\n\t\tprint_pkt(skb->data, skb->len);\n\t}\n\n\tif (unlikely(stmmac_tx_avail(priv, queue) <= (MAX_SKB_FRAGS + 1))) {\n\t\tnetif_dbg(priv, hw, priv->dev, \"%s: stop transmitted packets\\n\",\n\t\t\t  __func__);\n\t\tnetif_tx_stop_queue(netdev_get_tx_queue(priv->dev, queue));\n\t}\n\n\tflags = u64_stats_update_begin_irqsave(&txq_stats->syncp);\n\ttxq_stats->tx_bytes += skb->len;\n\tif (set_ic)\n\t\ttxq_stats->tx_set_ic_bit++;\n\tu64_stats_update_end_irqrestore(&txq_stats->syncp, flags);\n\n\tif (priv->sarc_type)\n\t\tstmmac_set_desc_sarc(priv, first, priv->sarc_type);\n\n\tskb_tx_timestamp(skb);\n\n\t \n\tif (likely(!is_jumbo)) {\n\t\tbool last_segment = (nfrags == 0);\n\n\t\tdes = dma_map_single(priv->device, skb->data,\n\t\t\t\t     nopaged_len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(priv->device, des))\n\t\t\tgoto dma_map_err;\n\n\t\ttx_q->tx_skbuff_dma[first_entry].buf = des;\n\t\ttx_q->tx_skbuff_dma[first_entry].buf_type = STMMAC_TXBUF_T_SKB;\n\t\ttx_q->tx_skbuff_dma[first_entry].map_as_page = false;\n\n\t\tstmmac_set_desc_addr(priv, first, des);\n\n\t\ttx_q->tx_skbuff_dma[first_entry].len = nopaged_len;\n\t\ttx_q->tx_skbuff_dma[first_entry].last_segment = last_segment;\n\n\t\tif (unlikely((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\n\t\t\t     priv->hwts_tx_en)) {\n\t\t\t \n\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t\tstmmac_enable_tx_timestamp(priv, first);\n\t\t}\n\n\t\t \n\t\tstmmac_prepare_tx_desc(priv, first, 1, nopaged_len,\n\t\t\t\tcsum_insertion, priv->mode, 0, last_segment,\n\t\t\t\tskb->len);\n\t}\n\n\tif (tx_q->tbs & STMMAC_TBS_EN) {\n\t\tstruct timespec64 ts = ns_to_timespec64(skb->tstamp);\n\n\t\ttbs_desc = &tx_q->dma_entx[first_entry];\n\t\tstmmac_set_desc_tbs(priv, tbs_desc, ts.tv_sec, ts.tv_nsec);\n\t}\n\n\tstmmac_set_tx_owner(priv, first);\n\n\tnetdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len);\n\n\tstmmac_enable_dma_transmission(priv, priv->ioaddr);\n\n\tstmmac_flush_tx_descriptors(priv, queue);\n\tstmmac_tx_timer_arm(priv, queue);\n\n\treturn NETDEV_TX_OK;\n\ndma_map_err:\n\tnetdev_err(priv->dev, \"Tx DMA map failed\\n\");\n\tdev_kfree_skb(skb);\n\tpriv->xstats.tx_dropped++;\n\treturn NETDEV_TX_OK;\n}\n\nstatic void stmmac_rx_vlan(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct vlan_ethhdr *veth = skb_vlan_eth_hdr(skb);\n\t__be16 vlan_proto = veth->h_vlan_proto;\n\tu16 vlanid;\n\n\tif ((vlan_proto == htons(ETH_P_8021Q) &&\n\t     dev->features & NETIF_F_HW_VLAN_CTAG_RX) ||\n\t    (vlan_proto == htons(ETH_P_8021AD) &&\n\t     dev->features & NETIF_F_HW_VLAN_STAG_RX)) {\n\t\t \n\t\tvlanid = ntohs(veth->h_vlan_TCI);\n\t\tmemmove(skb->data + VLAN_HLEN, veth, ETH_ALEN * 2);\n\t\tskb_pull(skb, VLAN_HLEN);\n\t\t__vlan_hwaccel_put_tag(skb, vlan_proto, vlanid);\n\t}\n}\n\n \nstatic inline void stmmac_rx_refill(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];\n\tint dirty = stmmac_rx_dirty(priv, queue);\n\tunsigned int entry = rx_q->dirty_rx;\n\tgfp_t gfp = (GFP_ATOMIC | __GFP_NOWARN);\n\n\tif (priv->dma_cap.host_dma_width <= 32)\n\t\tgfp |= GFP_DMA32;\n\n\twhile (dirty-- > 0) {\n\t\tstruct stmmac_rx_buffer *buf = &rx_q->buf_pool[entry];\n\t\tstruct dma_desc *p;\n\t\tbool use_rx_wd;\n\n\t\tif (priv->extend_desc)\n\t\t\tp = (struct dma_desc *)(rx_q->dma_erx + entry);\n\t\telse\n\t\t\tp = rx_q->dma_rx + entry;\n\n\t\tif (!buf->page) {\n\t\t\tbuf->page = page_pool_alloc_pages(rx_q->page_pool, gfp);\n\t\t\tif (!buf->page)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (priv->sph && !buf->sec_page) {\n\t\t\tbuf->sec_page = page_pool_alloc_pages(rx_q->page_pool, gfp);\n\t\t\tif (!buf->sec_page)\n\t\t\t\tbreak;\n\n\t\t\tbuf->sec_addr = page_pool_get_dma_addr(buf->sec_page);\n\t\t}\n\n\t\tbuf->addr = page_pool_get_dma_addr(buf->page) + buf->page_offset;\n\n\t\tstmmac_set_desc_addr(priv, p, buf->addr);\n\t\tif (priv->sph)\n\t\t\tstmmac_set_desc_sec_addr(priv, p, buf->sec_addr, true);\n\t\telse\n\t\t\tstmmac_set_desc_sec_addr(priv, p, buf->sec_addr, false);\n\t\tstmmac_refill_desc3(priv, rx_q, p);\n\n\t\trx_q->rx_count_frames++;\n\t\trx_q->rx_count_frames += priv->rx_coal_frames[queue];\n\t\tif (rx_q->rx_count_frames > priv->rx_coal_frames[queue])\n\t\t\trx_q->rx_count_frames = 0;\n\n\t\tuse_rx_wd = !priv->rx_coal_frames[queue];\n\t\tuse_rx_wd |= rx_q->rx_count_frames > 0;\n\t\tif (!priv->use_riwt)\n\t\t\tuse_rx_wd = false;\n\n\t\tdma_wmb();\n\t\tstmmac_set_rx_owner(priv, p, use_rx_wd);\n\n\t\tentry = STMMAC_GET_ENTRY(entry, priv->dma_conf.dma_rx_size);\n\t}\n\trx_q->dirty_rx = entry;\n\trx_q->rx_tail_addr = rx_q->dma_rx_phy +\n\t\t\t    (rx_q->dirty_rx * sizeof(struct dma_desc));\n\tstmmac_set_rx_tail_ptr(priv, priv->ioaddr, rx_q->rx_tail_addr, queue);\n}\n\nstatic unsigned int stmmac_rx_buf1_len(struct stmmac_priv *priv,\n\t\t\t\t       struct dma_desc *p,\n\t\t\t\t       int status, unsigned int len)\n{\n\tunsigned int plen = 0, hlen = 0;\n\tint coe = priv->hw->rx_csum;\n\n\t \n\tif (priv->sph && len)\n\t\treturn 0;\n\n\t \n\tstmmac_get_rx_header_len(priv, p, &hlen);\n\tif (priv->sph && hlen) {\n\t\tpriv->xstats.rx_split_hdr_pkt_n++;\n\t\treturn hlen;\n\t}\n\n\t \n\tif (status & rx_not_ls)\n\t\treturn priv->dma_conf.dma_buf_sz;\n\n\tplen = stmmac_get_rx_frame_len(priv, p, coe);\n\n\t \n\treturn min_t(unsigned int, priv->dma_conf.dma_buf_sz, plen);\n}\n\nstatic unsigned int stmmac_rx_buf2_len(struct stmmac_priv *priv,\n\t\t\t\t       struct dma_desc *p,\n\t\t\t\t       int status, unsigned int len)\n{\n\tint coe = priv->hw->rx_csum;\n\tunsigned int plen = 0;\n\n\t \n\tif (!priv->sph)\n\t\treturn 0;\n\n\t \n\tif (status & rx_not_ls)\n\t\treturn priv->dma_conf.dma_buf_sz;\n\n\tplen = stmmac_get_rx_frame_len(priv, p, coe);\n\n\t \n\treturn plen - len;\n}\n\nstatic int stmmac_xdp_xmit_xdpf(struct stmmac_priv *priv, int queue,\n\t\t\t\tstruct xdp_frame *xdpf, bool dma_map)\n{\n\tstruct stmmac_txq_stats *txq_stats = &priv->xstats.txq_stats[queue];\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\tunsigned int entry = tx_q->cur_tx;\n\tstruct dma_desc *tx_desc;\n\tdma_addr_t dma_addr;\n\tbool set_ic;\n\n\tif (stmmac_tx_avail(priv, queue) < STMMAC_TX_THRESH(priv))\n\t\treturn STMMAC_XDP_CONSUMED;\n\n\tif (likely(priv->extend_desc))\n\t\ttx_desc = (struct dma_desc *)(tx_q->dma_etx + entry);\n\telse if (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\ttx_desc = &tx_q->dma_entx[entry].basic;\n\telse\n\t\ttx_desc = tx_q->dma_tx + entry;\n\n\tif (dma_map) {\n\t\tdma_addr = dma_map_single(priv->device, xdpf->data,\n\t\t\t\t\t  xdpf->len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(priv->device, dma_addr))\n\t\t\treturn STMMAC_XDP_CONSUMED;\n\n\t\ttx_q->tx_skbuff_dma[entry].buf_type = STMMAC_TXBUF_T_XDP_NDO;\n\t} else {\n\t\tstruct page *page = virt_to_page(xdpf->data);\n\n\t\tdma_addr = page_pool_get_dma_addr(page) + sizeof(*xdpf) +\n\t\t\t   xdpf->headroom;\n\t\tdma_sync_single_for_device(priv->device, dma_addr,\n\t\t\t\t\t   xdpf->len, DMA_BIDIRECTIONAL);\n\n\t\ttx_q->tx_skbuff_dma[entry].buf_type = STMMAC_TXBUF_T_XDP_TX;\n\t}\n\n\ttx_q->tx_skbuff_dma[entry].buf = dma_addr;\n\ttx_q->tx_skbuff_dma[entry].map_as_page = false;\n\ttx_q->tx_skbuff_dma[entry].len = xdpf->len;\n\ttx_q->tx_skbuff_dma[entry].last_segment = true;\n\ttx_q->tx_skbuff_dma[entry].is_jumbo = false;\n\n\ttx_q->xdpf[entry] = xdpf;\n\n\tstmmac_set_desc_addr(priv, tx_desc, dma_addr);\n\n\tstmmac_prepare_tx_desc(priv, tx_desc, 1, xdpf->len,\n\t\t\t       true, priv->mode, true, true,\n\t\t\t       xdpf->len);\n\n\ttx_q->tx_count_frames++;\n\n\tif (tx_q->tx_count_frames % priv->tx_coal_frames[queue] == 0)\n\t\tset_ic = true;\n\telse\n\t\tset_ic = false;\n\n\tif (set_ic) {\n\t\tunsigned long flags;\n\t\ttx_q->tx_count_frames = 0;\n\t\tstmmac_set_tx_ic(priv, tx_desc);\n\t\tflags = u64_stats_update_begin_irqsave(&txq_stats->syncp);\n\t\ttxq_stats->tx_set_ic_bit++;\n\t\tu64_stats_update_end_irqrestore(&txq_stats->syncp, flags);\n\t}\n\n\tstmmac_enable_dma_transmission(priv, priv->ioaddr);\n\n\tentry = STMMAC_GET_ENTRY(entry, priv->dma_conf.dma_tx_size);\n\ttx_q->cur_tx = entry;\n\n\treturn STMMAC_XDP_TX;\n}\n\nstatic int stmmac_xdp_get_tx_queue(struct stmmac_priv *priv,\n\t\t\t\t   int cpu)\n{\n\tint index = cpu;\n\n\tif (unlikely(index < 0))\n\t\tindex = 0;\n\n\twhile (index >= priv->plat->tx_queues_to_use)\n\t\tindex -= priv->plat->tx_queues_to_use;\n\n\treturn index;\n}\n\nstatic int stmmac_xdp_xmit_back(struct stmmac_priv *priv,\n\t\t\t\tstruct xdp_buff *xdp)\n{\n\tstruct xdp_frame *xdpf = xdp_convert_buff_to_frame(xdp);\n\tint cpu = smp_processor_id();\n\tstruct netdev_queue *nq;\n\tint queue;\n\tint res;\n\n\tif (unlikely(!xdpf))\n\t\treturn STMMAC_XDP_CONSUMED;\n\n\tqueue = stmmac_xdp_get_tx_queue(priv, cpu);\n\tnq = netdev_get_tx_queue(priv->dev, queue);\n\n\t__netif_tx_lock(nq, cpu);\n\t \n\ttxq_trans_cond_update(nq);\n\n\tres = stmmac_xdp_xmit_xdpf(priv, queue, xdpf, false);\n\tif (res == STMMAC_XDP_TX)\n\t\tstmmac_flush_tx_descriptors(priv, queue);\n\n\t__netif_tx_unlock(nq);\n\n\treturn res;\n}\n\nstatic int __stmmac_xdp_run_prog(struct stmmac_priv *priv,\n\t\t\t\t struct bpf_prog *prog,\n\t\t\t\t struct xdp_buff *xdp)\n{\n\tu32 act;\n\tint res;\n\n\tact = bpf_prog_run_xdp(prog, xdp);\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\tres = STMMAC_XDP_PASS;\n\t\tbreak;\n\tcase XDP_TX:\n\t\tres = stmmac_xdp_xmit_back(priv, xdp);\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\tif (xdp_do_redirect(priv->dev, xdp, prog) < 0)\n\t\t\tres = STMMAC_XDP_CONSUMED;\n\t\telse\n\t\t\tres = STMMAC_XDP_REDIRECT;\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(priv->dev, prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(priv->dev, prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tres = STMMAC_XDP_CONSUMED;\n\t\tbreak;\n\t}\n\n\treturn res;\n}\n\nstatic struct sk_buff *stmmac_xdp_run_prog(struct stmmac_priv *priv,\n\t\t\t\t\t   struct xdp_buff *xdp)\n{\n\tstruct bpf_prog *prog;\n\tint res;\n\n\tprog = READ_ONCE(priv->xdp_prog);\n\tif (!prog) {\n\t\tres = STMMAC_XDP_PASS;\n\t\tgoto out;\n\t}\n\n\tres = __stmmac_xdp_run_prog(priv, prog, xdp);\nout:\n\treturn ERR_PTR(-res);\n}\n\nstatic void stmmac_finalize_xdp_rx(struct stmmac_priv *priv,\n\t\t\t\t   int xdp_status)\n{\n\tint cpu = smp_processor_id();\n\tint queue;\n\n\tqueue = stmmac_xdp_get_tx_queue(priv, cpu);\n\n\tif (xdp_status & STMMAC_XDP_TX)\n\t\tstmmac_tx_timer_arm(priv, queue);\n\n\tif (xdp_status & STMMAC_XDP_REDIRECT)\n\t\txdp_do_flush();\n}\n\nstatic struct sk_buff *stmmac_construct_skb_zc(struct stmmac_channel *ch,\n\t\t\t\t\t       struct xdp_buff *xdp)\n{\n\tunsigned int metasize = xdp->data - xdp->data_meta;\n\tunsigned int datasize = xdp->data_end - xdp->data;\n\tstruct sk_buff *skb;\n\n\tskb = __napi_alloc_skb(&ch->rxtx_napi,\n\t\t\t       xdp->data_end - xdp->data_hard_start,\n\t\t\t       GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\tmemcpy(__skb_put(skb, datasize), xdp->data, datasize);\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\treturn skb;\n}\n\nstatic void stmmac_dispatch_skb_zc(struct stmmac_priv *priv, u32 queue,\n\t\t\t\t   struct dma_desc *p, struct dma_desc *np,\n\t\t\t\t   struct xdp_buff *xdp)\n{\n\tstruct stmmac_rxq_stats *rxq_stats = &priv->xstats.rxq_stats[queue];\n\tstruct stmmac_channel *ch = &priv->channel[queue];\n\tunsigned int len = xdp->data_end - xdp->data;\n\tenum pkt_hash_types hash_type;\n\tint coe = priv->hw->rx_csum;\n\tunsigned long flags;\n\tstruct sk_buff *skb;\n\tu32 hash;\n\n\tskb = stmmac_construct_skb_zc(ch, xdp);\n\tif (!skb) {\n\t\tpriv->xstats.rx_dropped++;\n\t\treturn;\n\t}\n\n\tstmmac_get_rx_hwtstamp(priv, p, np, skb);\n\tstmmac_rx_vlan(priv->dev, skb);\n\tskb->protocol = eth_type_trans(skb, priv->dev);\n\n\tif (unlikely(!coe))\n\t\tskb_checksum_none_assert(skb);\n\telse\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tif (!stmmac_get_rx_hash(priv, p, &hash, &hash_type))\n\t\tskb_set_hash(skb, hash, hash_type);\n\n\tskb_record_rx_queue(skb, queue);\n\tnapi_gro_receive(&ch->rxtx_napi, skb);\n\n\tflags = u64_stats_update_begin_irqsave(&rxq_stats->syncp);\n\trxq_stats->rx_pkt_n++;\n\trxq_stats->rx_bytes += len;\n\tu64_stats_update_end_irqrestore(&rxq_stats->syncp, flags);\n}\n\nstatic bool stmmac_rx_refill_zc(struct stmmac_priv *priv, u32 queue, u32 budget)\n{\n\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];\n\tunsigned int entry = rx_q->dirty_rx;\n\tstruct dma_desc *rx_desc = NULL;\n\tbool ret = true;\n\n\tbudget = min(budget, stmmac_rx_dirty(priv, queue));\n\n\twhile (budget-- > 0 && entry != rx_q->cur_rx) {\n\t\tstruct stmmac_rx_buffer *buf = &rx_q->buf_pool[entry];\n\t\tdma_addr_t dma_addr;\n\t\tbool use_rx_wd;\n\n\t\tif (!buf->xdp) {\n\t\t\tbuf->xdp = xsk_buff_alloc(rx_q->xsk_pool);\n\t\t\tif (!buf->xdp) {\n\t\t\t\tret = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (priv->extend_desc)\n\t\t\trx_desc = (struct dma_desc *)(rx_q->dma_erx + entry);\n\t\telse\n\t\t\trx_desc = rx_q->dma_rx + entry;\n\n\t\tdma_addr = xsk_buff_xdp_get_dma(buf->xdp);\n\t\tstmmac_set_desc_addr(priv, rx_desc, dma_addr);\n\t\tstmmac_set_desc_sec_addr(priv, rx_desc, 0, false);\n\t\tstmmac_refill_desc3(priv, rx_q, rx_desc);\n\n\t\trx_q->rx_count_frames++;\n\t\trx_q->rx_count_frames += priv->rx_coal_frames[queue];\n\t\tif (rx_q->rx_count_frames > priv->rx_coal_frames[queue])\n\t\t\trx_q->rx_count_frames = 0;\n\n\t\tuse_rx_wd = !priv->rx_coal_frames[queue];\n\t\tuse_rx_wd |= rx_q->rx_count_frames > 0;\n\t\tif (!priv->use_riwt)\n\t\t\tuse_rx_wd = false;\n\n\t\tdma_wmb();\n\t\tstmmac_set_rx_owner(priv, rx_desc, use_rx_wd);\n\n\t\tentry = STMMAC_GET_ENTRY(entry, priv->dma_conf.dma_rx_size);\n\t}\n\n\tif (rx_desc) {\n\t\trx_q->dirty_rx = entry;\n\t\trx_q->rx_tail_addr = rx_q->dma_rx_phy +\n\t\t\t\t     (rx_q->dirty_rx * sizeof(struct dma_desc));\n\t\tstmmac_set_rx_tail_ptr(priv, priv->ioaddr, rx_q->rx_tail_addr, queue);\n\t}\n\n\treturn ret;\n}\n\nstatic struct stmmac_xdp_buff *xsk_buff_to_stmmac_ctx(struct xdp_buff *xdp)\n{\n\t \n\treturn (struct stmmac_xdp_buff *)xdp;\n}\n\nstatic int stmmac_rx_zc(struct stmmac_priv *priv, int limit, u32 queue)\n{\n\tstruct stmmac_rxq_stats *rxq_stats = &priv->xstats.rxq_stats[queue];\n\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];\n\tunsigned int count = 0, error = 0, len = 0;\n\tint dirty = stmmac_rx_dirty(priv, queue);\n\tunsigned int next_entry = rx_q->cur_rx;\n\tu32 rx_errors = 0, rx_dropped = 0;\n\tunsigned int desc_size;\n\tstruct bpf_prog *prog;\n\tbool failure = false;\n\tunsigned long flags;\n\tint xdp_status = 0;\n\tint status = 0;\n\n\tif (netif_msg_rx_status(priv)) {\n\t\tvoid *rx_head;\n\n\t\tnetdev_dbg(priv->dev, \"%s: descriptor ring:\\n\", __func__);\n\t\tif (priv->extend_desc) {\n\t\t\trx_head = (void *)rx_q->dma_erx;\n\t\t\tdesc_size = sizeof(struct dma_extended_desc);\n\t\t} else {\n\t\t\trx_head = (void *)rx_q->dma_rx;\n\t\t\tdesc_size = sizeof(struct dma_desc);\n\t\t}\n\n\t\tstmmac_display_ring(priv, rx_head, priv->dma_conf.dma_rx_size, true,\n\t\t\t\t    rx_q->dma_rx_phy, desc_size);\n\t}\n\twhile (count < limit) {\n\t\tstruct stmmac_rx_buffer *buf;\n\t\tstruct stmmac_xdp_buff *ctx;\n\t\tunsigned int buf1_len = 0;\n\t\tstruct dma_desc *np, *p;\n\t\tint entry;\n\t\tint res;\n\n\t\tif (!count && rx_q->state_saved) {\n\t\t\terror = rx_q->state.error;\n\t\t\tlen = rx_q->state.len;\n\t\t} else {\n\t\t\trx_q->state_saved = false;\n\t\t\terror = 0;\n\t\t\tlen = 0;\n\t\t}\n\n\t\tif (count >= limit)\n\t\t\tbreak;\n\nread_again:\n\t\tbuf1_len = 0;\n\t\tentry = next_entry;\n\t\tbuf = &rx_q->buf_pool[entry];\n\n\t\tif (dirty >= STMMAC_RX_FILL_BATCH) {\n\t\t\tfailure = failure ||\n\t\t\t\t  !stmmac_rx_refill_zc(priv, queue, dirty);\n\t\t\tdirty = 0;\n\t\t}\n\n\t\tif (priv->extend_desc)\n\t\t\tp = (struct dma_desc *)(rx_q->dma_erx + entry);\n\t\telse\n\t\t\tp = rx_q->dma_rx + entry;\n\n\t\t \n\t\tstatus = stmmac_rx_status(priv, &priv->xstats, p);\n\t\t \n\t\tif (unlikely(status & dma_own))\n\t\t\tbreak;\n\n\t\t \n\t\trx_q->cur_rx = STMMAC_GET_ENTRY(rx_q->cur_rx,\n\t\t\t\t\t\tpriv->dma_conf.dma_rx_size);\n\t\tnext_entry = rx_q->cur_rx;\n\n\t\tif (priv->extend_desc)\n\t\t\tnp = (struct dma_desc *)(rx_q->dma_erx + next_entry);\n\t\telse\n\t\t\tnp = rx_q->dma_rx + next_entry;\n\n\t\tprefetch(np);\n\n\t\t \n\t\tif (!buf->xdp)\n\t\t\tbreak;\n\n\t\tif (priv->extend_desc)\n\t\t\tstmmac_rx_extended_status(priv, &priv->xstats,\n\t\t\t\t\t\t  rx_q->dma_erx + entry);\n\t\tif (unlikely(status == discard_frame)) {\n\t\t\txsk_buff_free(buf->xdp);\n\t\t\tbuf->xdp = NULL;\n\t\t\tdirty++;\n\t\t\terror = 1;\n\t\t\tif (!priv->hwts_rx_en)\n\t\t\t\trx_errors++;\n\t\t}\n\n\t\tif (unlikely(error && (status & rx_not_ls)))\n\t\t\tgoto read_again;\n\t\tif (unlikely(error)) {\n\t\t\tcount++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (likely(status & rx_not_ls)) {\n\t\t\txsk_buff_free(buf->xdp);\n\t\t\tbuf->xdp = NULL;\n\t\t\tdirty++;\n\t\t\tcount++;\n\t\t\tgoto read_again;\n\t\t}\n\n\t\tctx = xsk_buff_to_stmmac_ctx(buf->xdp);\n\t\tctx->priv = priv;\n\t\tctx->desc = p;\n\t\tctx->ndesc = np;\n\n\t\t \n\t\tbuf1_len = stmmac_rx_buf1_len(priv, p, status, len);\n\t\tlen += buf1_len;\n\n\t\t \n\t\tif (likely(!(status & rx_not_ls))) {\n\t\t\tbuf1_len -= ETH_FCS_LEN;\n\t\t\tlen -= ETH_FCS_LEN;\n\t\t}\n\n\t\t \n\t\tbuf->xdp->data_end = buf->xdp->data + buf1_len;\n\t\txsk_buff_dma_sync_for_cpu(buf->xdp, rx_q->xsk_pool);\n\n\t\tprog = READ_ONCE(priv->xdp_prog);\n\t\tres = __stmmac_xdp_run_prog(priv, prog, buf->xdp);\n\n\t\tswitch (res) {\n\t\tcase STMMAC_XDP_PASS:\n\t\t\tstmmac_dispatch_skb_zc(priv, queue, p, np, buf->xdp);\n\t\t\txsk_buff_free(buf->xdp);\n\t\t\tbreak;\n\t\tcase STMMAC_XDP_CONSUMED:\n\t\t\txsk_buff_free(buf->xdp);\n\t\t\trx_dropped++;\n\t\t\tbreak;\n\t\tcase STMMAC_XDP_TX:\n\t\tcase STMMAC_XDP_REDIRECT:\n\t\t\txdp_status |= res;\n\t\t\tbreak;\n\t\t}\n\n\t\tbuf->xdp = NULL;\n\t\tdirty++;\n\t\tcount++;\n\t}\n\n\tif (status & rx_not_ls) {\n\t\trx_q->state_saved = true;\n\t\trx_q->state.error = error;\n\t\trx_q->state.len = len;\n\t}\n\n\tstmmac_finalize_xdp_rx(priv, xdp_status);\n\n\tflags = u64_stats_update_begin_irqsave(&rxq_stats->syncp);\n\trxq_stats->rx_pkt_n += count;\n\tu64_stats_update_end_irqrestore(&rxq_stats->syncp, flags);\n\n\tpriv->xstats.rx_dropped += rx_dropped;\n\tpriv->xstats.rx_errors += rx_errors;\n\n\tif (xsk_uses_need_wakeup(rx_q->xsk_pool)) {\n\t\tif (failure || stmmac_rx_dirty(priv, queue) > 0)\n\t\t\txsk_set_rx_need_wakeup(rx_q->xsk_pool);\n\t\telse\n\t\t\txsk_clear_rx_need_wakeup(rx_q->xsk_pool);\n\n\t\treturn (int)count;\n\t}\n\n\treturn failure ? limit : (int)count;\n}\n\n \nstatic int stmmac_rx(struct stmmac_priv *priv, int limit, u32 queue)\n{\n\tu32 rx_errors = 0, rx_dropped = 0, rx_bytes = 0, rx_packets = 0;\n\tstruct stmmac_rxq_stats *rxq_stats = &priv->xstats.rxq_stats[queue];\n\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];\n\tstruct stmmac_channel *ch = &priv->channel[queue];\n\tunsigned int count = 0, error = 0, len = 0;\n\tint status = 0, coe = priv->hw->rx_csum;\n\tunsigned int next_entry = rx_q->cur_rx;\n\tenum dma_data_direction dma_dir;\n\tunsigned int desc_size;\n\tstruct sk_buff *skb = NULL;\n\tstruct stmmac_xdp_buff ctx;\n\tunsigned long flags;\n\tint xdp_status = 0;\n\tint buf_sz;\n\n\tdma_dir = page_pool_get_dma_dir(rx_q->page_pool);\n\tbuf_sz = DIV_ROUND_UP(priv->dma_conf.dma_buf_sz, PAGE_SIZE) * PAGE_SIZE;\n\tlimit = min(priv->dma_conf.dma_rx_size - 1, (unsigned int)limit);\n\n\tif (netif_msg_rx_status(priv)) {\n\t\tvoid *rx_head;\n\n\t\tnetdev_dbg(priv->dev, \"%s: descriptor ring:\\n\", __func__);\n\t\tif (priv->extend_desc) {\n\t\t\trx_head = (void *)rx_q->dma_erx;\n\t\t\tdesc_size = sizeof(struct dma_extended_desc);\n\t\t} else {\n\t\t\trx_head = (void *)rx_q->dma_rx;\n\t\t\tdesc_size = sizeof(struct dma_desc);\n\t\t}\n\n\t\tstmmac_display_ring(priv, rx_head, priv->dma_conf.dma_rx_size, true,\n\t\t\t\t    rx_q->dma_rx_phy, desc_size);\n\t}\n\twhile (count < limit) {\n\t\tunsigned int buf1_len = 0, buf2_len = 0;\n\t\tenum pkt_hash_types hash_type;\n\t\tstruct stmmac_rx_buffer *buf;\n\t\tstruct dma_desc *np, *p;\n\t\tint entry;\n\t\tu32 hash;\n\n\t\tif (!count && rx_q->state_saved) {\n\t\t\tskb = rx_q->state.skb;\n\t\t\terror = rx_q->state.error;\n\t\t\tlen = rx_q->state.len;\n\t\t} else {\n\t\t\trx_q->state_saved = false;\n\t\t\tskb = NULL;\n\t\t\terror = 0;\n\t\t\tlen = 0;\n\t\t}\n\nread_again:\n\t\tif (count >= limit)\n\t\t\tbreak;\n\n\t\tbuf1_len = 0;\n\t\tbuf2_len = 0;\n\t\tentry = next_entry;\n\t\tbuf = &rx_q->buf_pool[entry];\n\n\t\tif (priv->extend_desc)\n\t\t\tp = (struct dma_desc *)(rx_q->dma_erx + entry);\n\t\telse\n\t\t\tp = rx_q->dma_rx + entry;\n\n\t\t \n\t\tstatus = stmmac_rx_status(priv, &priv->xstats, p);\n\t\t \n\t\tif (unlikely(status & dma_own))\n\t\t\tbreak;\n\n\t\trx_q->cur_rx = STMMAC_GET_ENTRY(rx_q->cur_rx,\n\t\t\t\t\t\tpriv->dma_conf.dma_rx_size);\n\t\tnext_entry = rx_q->cur_rx;\n\n\t\tif (priv->extend_desc)\n\t\t\tnp = (struct dma_desc *)(rx_q->dma_erx + next_entry);\n\t\telse\n\t\t\tnp = rx_q->dma_rx + next_entry;\n\n\t\tprefetch(np);\n\n\t\tif (priv->extend_desc)\n\t\t\tstmmac_rx_extended_status(priv, &priv->xstats, rx_q->dma_erx + entry);\n\t\tif (unlikely(status == discard_frame)) {\n\t\t\tpage_pool_recycle_direct(rx_q->page_pool, buf->page);\n\t\t\tbuf->page = NULL;\n\t\t\terror = 1;\n\t\t\tif (!priv->hwts_rx_en)\n\t\t\t\trx_errors++;\n\t\t}\n\n\t\tif (unlikely(error && (status & rx_not_ls)))\n\t\t\tgoto read_again;\n\t\tif (unlikely(error)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\tskb = NULL;\n\t\t\tcount++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\n\t\tprefetch(page_address(buf->page) + buf->page_offset);\n\t\tif (buf->sec_page)\n\t\t\tprefetch(page_address(buf->sec_page));\n\n\t\tbuf1_len = stmmac_rx_buf1_len(priv, p, status, len);\n\t\tlen += buf1_len;\n\t\tbuf2_len = stmmac_rx_buf2_len(priv, p, status, len);\n\t\tlen += buf2_len;\n\n\t\t \n\t\tif (likely(!(status & rx_not_ls))) {\n\t\t\tif (buf2_len) {\n\t\t\t\tbuf2_len -= ETH_FCS_LEN;\n\t\t\t\tlen -= ETH_FCS_LEN;\n\t\t\t} else if (buf1_len) {\n\t\t\t\tbuf1_len -= ETH_FCS_LEN;\n\t\t\t\tlen -= ETH_FCS_LEN;\n\t\t\t}\n\t\t}\n\n\t\tif (!skb) {\n\t\t\tunsigned int pre_len, sync_len;\n\n\t\t\tdma_sync_single_for_cpu(priv->device, buf->addr,\n\t\t\t\t\t\tbuf1_len, dma_dir);\n\n\t\t\txdp_init_buff(&ctx.xdp, buf_sz, &rx_q->xdp_rxq);\n\t\t\txdp_prepare_buff(&ctx.xdp, page_address(buf->page),\n\t\t\t\t\t buf->page_offset, buf1_len, true);\n\n\t\t\tpre_len = ctx.xdp.data_end - ctx.xdp.data_hard_start -\n\t\t\t\t  buf->page_offset;\n\n\t\t\tctx.priv = priv;\n\t\t\tctx.desc = p;\n\t\t\tctx.ndesc = np;\n\n\t\t\tskb = stmmac_xdp_run_prog(priv, &ctx.xdp);\n\t\t\t \n\t\t\tsync_len = ctx.xdp.data_end - ctx.xdp.data_hard_start -\n\t\t\t\t   buf->page_offset;\n\t\t\tsync_len = max(sync_len, pre_len);\n\n\t\t\t \n\t\t\tif (IS_ERR(skb)) {\n\t\t\t\tunsigned int xdp_res = -PTR_ERR(skb);\n\n\t\t\t\tif (xdp_res & STMMAC_XDP_CONSUMED) {\n\t\t\t\t\tpage_pool_put_page(rx_q->page_pool,\n\t\t\t\t\t\t\t   virt_to_head_page(ctx.xdp.data),\n\t\t\t\t\t\t\t   sync_len, true);\n\t\t\t\t\tbuf->page = NULL;\n\t\t\t\t\trx_dropped++;\n\n\t\t\t\t\t \n\t\t\t\t\tskb = NULL;\n\n\t\t\t\t\tif (unlikely((status & rx_not_ls)))\n\t\t\t\t\t\tgoto read_again;\n\n\t\t\t\t\tcount++;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else if (xdp_res & (STMMAC_XDP_TX |\n\t\t\t\t\t\t      STMMAC_XDP_REDIRECT)) {\n\t\t\t\t\txdp_status |= xdp_res;\n\t\t\t\t\tbuf->page = NULL;\n\t\t\t\t\tskb = NULL;\n\t\t\t\t\tcount++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (!skb) {\n\t\t\t \n\t\t\tbuf1_len = ctx.xdp.data_end - ctx.xdp.data;\n\n\t\t\tskb = napi_alloc_skb(&ch->rx_napi, buf1_len);\n\t\t\tif (!skb) {\n\t\t\t\trx_dropped++;\n\t\t\t\tcount++;\n\t\t\t\tgoto drain_data;\n\t\t\t}\n\n\t\t\t \n\t\t\tskb_copy_to_linear_data(skb, ctx.xdp.data, buf1_len);\n\t\t\tskb_put(skb, buf1_len);\n\n\t\t\t \n\t\t\tpage_pool_recycle_direct(rx_q->page_pool, buf->page);\n\t\t\tbuf->page = NULL;\n\t\t} else if (buf1_len) {\n\t\t\tdma_sync_single_for_cpu(priv->device, buf->addr,\n\t\t\t\t\t\tbuf1_len, dma_dir);\n\t\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t\tbuf->page, buf->page_offset, buf1_len,\n\t\t\t\t\tpriv->dma_conf.dma_buf_sz);\n\n\t\t\t \n\t\t\tskb_mark_for_recycle(skb);\n\t\t\tbuf->page = NULL;\n\t\t}\n\n\t\tif (buf2_len) {\n\t\t\tdma_sync_single_for_cpu(priv->device, buf->sec_addr,\n\t\t\t\t\t\tbuf2_len, dma_dir);\n\t\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t\tbuf->sec_page, 0, buf2_len,\n\t\t\t\t\tpriv->dma_conf.dma_buf_sz);\n\n\t\t\t \n\t\t\tskb_mark_for_recycle(skb);\n\t\t\tbuf->sec_page = NULL;\n\t\t}\n\ndrain_data:\n\t\tif (likely(status & rx_not_ls))\n\t\t\tgoto read_again;\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\t \n\n\t\tstmmac_get_rx_hwtstamp(priv, p, np, skb);\n\t\tstmmac_rx_vlan(priv->dev, skb);\n\t\tskb->protocol = eth_type_trans(skb, priv->dev);\n\n\t\tif (unlikely(!coe))\n\t\t\tskb_checksum_none_assert(skb);\n\t\telse\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\tif (!stmmac_get_rx_hash(priv, p, &hash, &hash_type))\n\t\t\tskb_set_hash(skb, hash, hash_type);\n\n\t\tskb_record_rx_queue(skb, queue);\n\t\tnapi_gro_receive(&ch->rx_napi, skb);\n\t\tskb = NULL;\n\n\t\trx_packets++;\n\t\trx_bytes += len;\n\t\tcount++;\n\t}\n\n\tif (status & rx_not_ls || skb) {\n\t\trx_q->state_saved = true;\n\t\trx_q->state.skb = skb;\n\t\trx_q->state.error = error;\n\t\trx_q->state.len = len;\n\t}\n\n\tstmmac_finalize_xdp_rx(priv, xdp_status);\n\n\tstmmac_rx_refill(priv, queue);\n\n\tflags = u64_stats_update_begin_irqsave(&rxq_stats->syncp);\n\trxq_stats->rx_packets += rx_packets;\n\trxq_stats->rx_bytes += rx_bytes;\n\trxq_stats->rx_pkt_n += count;\n\tu64_stats_update_end_irqrestore(&rxq_stats->syncp, flags);\n\n\tpriv->xstats.rx_dropped += rx_dropped;\n\tpriv->xstats.rx_errors += rx_errors;\n\n\treturn count;\n}\n\nstatic int stmmac_napi_poll_rx(struct napi_struct *napi, int budget)\n{\n\tstruct stmmac_channel *ch =\n\t\tcontainer_of(napi, struct stmmac_channel, rx_napi);\n\tstruct stmmac_priv *priv = ch->priv_data;\n\tstruct stmmac_rxq_stats *rxq_stats;\n\tu32 chan = ch->index;\n\tunsigned long flags;\n\tint work_done;\n\n\trxq_stats = &priv->xstats.rxq_stats[chan];\n\tflags = u64_stats_update_begin_irqsave(&rxq_stats->syncp);\n\trxq_stats->napi_poll++;\n\tu64_stats_update_end_irqrestore(&rxq_stats->syncp, flags);\n\n\twork_done = stmmac_rx(priv, budget, chan);\n\tif (work_done < budget && napi_complete_done(napi, work_done)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ch->lock, flags);\n\t\tstmmac_enable_dma_irq(priv, priv->ioaddr, chan, 1, 0);\n\t\tspin_unlock_irqrestore(&ch->lock, flags);\n\t}\n\n\treturn work_done;\n}\n\nstatic int stmmac_napi_poll_tx(struct napi_struct *napi, int budget)\n{\n\tstruct stmmac_channel *ch =\n\t\tcontainer_of(napi, struct stmmac_channel, tx_napi);\n\tstruct stmmac_priv *priv = ch->priv_data;\n\tstruct stmmac_txq_stats *txq_stats;\n\tu32 chan = ch->index;\n\tunsigned long flags;\n\tint work_done;\n\n\ttxq_stats = &priv->xstats.txq_stats[chan];\n\tflags = u64_stats_update_begin_irqsave(&txq_stats->syncp);\n\ttxq_stats->napi_poll++;\n\tu64_stats_update_end_irqrestore(&txq_stats->syncp, flags);\n\n\twork_done = stmmac_tx_clean(priv, budget, chan);\n\twork_done = min(work_done, budget);\n\n\tif (work_done < budget && napi_complete_done(napi, work_done)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ch->lock, flags);\n\t\tstmmac_enable_dma_irq(priv, priv->ioaddr, chan, 0, 1);\n\t\tspin_unlock_irqrestore(&ch->lock, flags);\n\t}\n\n\treturn work_done;\n}\n\nstatic int stmmac_napi_poll_rxtx(struct napi_struct *napi, int budget)\n{\n\tstruct stmmac_channel *ch =\n\t\tcontainer_of(napi, struct stmmac_channel, rxtx_napi);\n\tstruct stmmac_priv *priv = ch->priv_data;\n\tint rx_done, tx_done, rxtx_done;\n\tstruct stmmac_rxq_stats *rxq_stats;\n\tstruct stmmac_txq_stats *txq_stats;\n\tu32 chan = ch->index;\n\tunsigned long flags;\n\n\trxq_stats = &priv->xstats.rxq_stats[chan];\n\tflags = u64_stats_update_begin_irqsave(&rxq_stats->syncp);\n\trxq_stats->napi_poll++;\n\tu64_stats_update_end_irqrestore(&rxq_stats->syncp, flags);\n\n\ttxq_stats = &priv->xstats.txq_stats[chan];\n\tflags = u64_stats_update_begin_irqsave(&txq_stats->syncp);\n\ttxq_stats->napi_poll++;\n\tu64_stats_update_end_irqrestore(&txq_stats->syncp, flags);\n\n\ttx_done = stmmac_tx_clean(priv, budget, chan);\n\ttx_done = min(tx_done, budget);\n\n\trx_done = stmmac_rx_zc(priv, budget, chan);\n\n\trxtx_done = max(tx_done, rx_done);\n\n\t \n\tif (rxtx_done >= budget)\n\t\treturn budget;\n\n\t \n\tif (napi_complete_done(napi, rxtx_done)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ch->lock, flags);\n\t\t \n\t\tstmmac_enable_dma_irq(priv, priv->ioaddr, chan, 1, 1);\n\t\tspin_unlock_irqrestore(&ch->lock, flags);\n\t}\n\n\treturn min(rxtx_done, budget - 1);\n}\n\n \nstatic void stmmac_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tstmmac_global_err(priv);\n}\n\n \nstatic void stmmac_set_rx_mode(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tstmmac_set_filter(priv, priv->hw, dev);\n}\n\n \nstatic int stmmac_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tint txfifosz = priv->plat->tx_fifo_size;\n\tstruct stmmac_dma_conf *dma_conf;\n\tconst int mtu = new_mtu;\n\tint ret;\n\n\tif (txfifosz == 0)\n\t\ttxfifosz = priv->dma_cap.tx_fifo_size;\n\n\ttxfifosz /= priv->plat->tx_queues_to_use;\n\n\tif (stmmac_xdp_is_enabled(priv) && new_mtu > ETH_DATA_LEN) {\n\t\tnetdev_dbg(priv->dev, \"Jumbo frames not supported for XDP\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tnew_mtu = STMMAC_ALIGN(new_mtu);\n\n\t \n\tif ((txfifosz < new_mtu) || (new_mtu > BUF_SIZE_16KiB))\n\t\treturn -EINVAL;\n\n\tif (netif_running(dev)) {\n\t\tnetdev_dbg(priv->dev, \"restarting interface to change its MTU\\n\");\n\t\t \n\t\tdma_conf = stmmac_setup_dma_desc(priv, mtu);\n\t\tif (IS_ERR(dma_conf)) {\n\t\t\tnetdev_err(priv->dev, \"failed allocating new dma conf for new MTU %d\\n\",\n\t\t\t\t   mtu);\n\t\t\treturn PTR_ERR(dma_conf);\n\t\t}\n\n\t\tstmmac_release(dev);\n\n\t\tret = __stmmac_open(dev, dma_conf);\n\t\tif (ret) {\n\t\t\tfree_dma_desc_resources(priv, dma_conf);\n\t\t\tkfree(dma_conf);\n\t\t\tnetdev_err(priv->dev, \"failed reopening the interface after MTU change\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tkfree(dma_conf);\n\n\t\tstmmac_set_rx_mode(dev);\n\t}\n\n\tdev->mtu = mtu;\n\tnetdev_update_features(dev);\n\n\treturn 0;\n}\n\nstatic netdev_features_t stmmac_fix_features(struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tif (priv->plat->rx_coe == STMMAC_RX_COE_NONE)\n\t\tfeatures &= ~NETIF_F_RXCSUM;\n\n\tif (!priv->plat->tx_coe)\n\t\tfeatures &= ~NETIF_F_CSUM_MASK;\n\n\t \n\tif (priv->plat->bugged_jumbo && (dev->mtu > ETH_DATA_LEN))\n\t\tfeatures &= ~NETIF_F_CSUM_MASK;\n\n\t \n\tif ((priv->plat->flags & STMMAC_FLAG_TSO_EN) && (priv->dma_cap.tsoen)) {\n\t\tif (features & NETIF_F_TSO)\n\t\t\tpriv->tso = true;\n\t\telse\n\t\t\tpriv->tso = false;\n\t}\n\n\treturn features;\n}\n\nstatic int stmmac_set_features(struct net_device *netdev,\n\t\t\t       netdev_features_t features)\n{\n\tstruct stmmac_priv *priv = netdev_priv(netdev);\n\n\t \n\tif (features & NETIF_F_RXCSUM)\n\t\tpriv->hw->rx_csum = priv->plat->rx_coe;\n\telse\n\t\tpriv->hw->rx_csum = 0;\n\t \n\tstmmac_rx_ipc(priv, priv->hw);\n\n\tif (priv->sph_cap) {\n\t\tbool sph_en = (priv->hw->rx_csum > 0) && priv->sph;\n\t\tu32 chan;\n\n\t\tfor (chan = 0; chan < priv->plat->rx_queues_to_use; chan++)\n\t\t\tstmmac_enable_sph(priv, priv->ioaddr, sph_en, chan);\n\t}\n\n\treturn 0;\n}\n\nstatic void stmmac_fpe_event_status(struct stmmac_priv *priv, int status)\n{\n\tstruct stmmac_fpe_cfg *fpe_cfg = priv->plat->fpe_cfg;\n\tenum stmmac_fpe_state *lo_state = &fpe_cfg->lo_fpe_state;\n\tenum stmmac_fpe_state *lp_state = &fpe_cfg->lp_fpe_state;\n\tbool *hs_enable = &fpe_cfg->hs_enable;\n\n\tif (status == FPE_EVENT_UNKNOWN || !*hs_enable)\n\t\treturn;\n\n\t \n\tif ((status & FPE_EVENT_RVER) == FPE_EVENT_RVER) {\n\t\tif (*lp_state < FPE_STATE_CAPABLE)\n\t\t\t*lp_state = FPE_STATE_CAPABLE;\n\n\t\t \n\t\tif (*hs_enable)\n\t\t\tstmmac_fpe_send_mpacket(priv, priv->ioaddr,\n\t\t\t\t\t\tfpe_cfg,\n\t\t\t\t\t\tMPACKET_RESPONSE);\n\t}\n\n\t \n\tif ((status & FPE_EVENT_TVER) == FPE_EVENT_TVER) {\n\t\tif (*lo_state < FPE_STATE_CAPABLE)\n\t\t\t*lo_state = FPE_STATE_CAPABLE;\n\t}\n\n\t \n\tif ((status & FPE_EVENT_RRSP) == FPE_EVENT_RRSP)\n\t\t*lp_state = FPE_STATE_ENTERING_ON;\n\n\t \n\tif ((status & FPE_EVENT_TRSP) == FPE_EVENT_TRSP)\n\t\t*lo_state = FPE_STATE_ENTERING_ON;\n\n\tif (!test_bit(__FPE_REMOVING, &priv->fpe_task_state) &&\n\t    !test_and_set_bit(__FPE_TASK_SCHED, &priv->fpe_task_state) &&\n\t    priv->fpe_wq) {\n\t\tqueue_work(priv->fpe_wq, &priv->fpe_task);\n\t}\n}\n\nstatic void stmmac_common_interrupt(struct stmmac_priv *priv)\n{\n\tu32 rx_cnt = priv->plat->rx_queues_to_use;\n\tu32 tx_cnt = priv->plat->tx_queues_to_use;\n\tu32 queues_count;\n\tu32 queue;\n\tbool xmac;\n\n\txmac = priv->plat->has_gmac4 || priv->plat->has_xgmac;\n\tqueues_count = (rx_cnt > tx_cnt) ? rx_cnt : tx_cnt;\n\n\tif (priv->irq_wake)\n\t\tpm_wakeup_event(priv->device, 0);\n\n\tif (priv->dma_cap.estsel)\n\t\tstmmac_est_irq_status(priv, priv->ioaddr, priv->dev,\n\t\t\t\t      &priv->xstats, tx_cnt);\n\n\tif (priv->dma_cap.fpesel) {\n\t\tint status = stmmac_fpe_irq_status(priv, priv->ioaddr,\n\t\t\t\t\t\t   priv->dev);\n\n\t\tstmmac_fpe_event_status(priv, status);\n\t}\n\n\t \n\tif ((priv->plat->has_gmac) || xmac) {\n\t\tint status = stmmac_host_irq_status(priv, priv->hw, &priv->xstats);\n\n\t\tif (unlikely(status)) {\n\t\t\t \n\t\t\tif (status & CORE_IRQ_TX_PATH_IN_LPI_MODE)\n\t\t\t\tpriv->tx_path_in_lpi_mode = true;\n\t\t\tif (status & CORE_IRQ_TX_PATH_EXIT_LPI_MODE)\n\t\t\t\tpriv->tx_path_in_lpi_mode = false;\n\t\t}\n\n\t\tfor (queue = 0; queue < queues_count; queue++) {\n\t\t\tstatus = stmmac_host_mtl_irq_status(priv, priv->hw,\n\t\t\t\t\t\t\t    queue);\n\t\t}\n\n\t\t \n\t\tif (priv->hw->pcs &&\n\t\t    !(priv->plat->flags & STMMAC_FLAG_HAS_INTEGRATED_PCS)) {\n\t\t\tif (priv->xstats.pcs_link)\n\t\t\t\tnetif_carrier_on(priv->dev);\n\t\t\telse\n\t\t\t\tnetif_carrier_off(priv->dev);\n\t\t}\n\n\t\tstmmac_timestamp_interrupt(priv, priv);\n\t}\n}\n\n \nstatic irqreturn_t stmmac_interrupt(int irq, void *dev_id)\n{\n\tstruct net_device *dev = (struct net_device *)dev_id;\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\t \n\tif (test_bit(STMMAC_DOWN, &priv->state))\n\t\treturn IRQ_HANDLED;\n\n\t \n\tif (stmmac_safety_feat_interrupt(priv))\n\t\treturn IRQ_HANDLED;\n\n\t \n\tstmmac_common_interrupt(priv);\n\n\t \n\tstmmac_dma_interrupt(priv);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t stmmac_mac_interrupt(int irq, void *dev_id)\n{\n\tstruct net_device *dev = (struct net_device *)dev_id;\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tif (unlikely(!dev)) {\n\t\tnetdev_err(priv->dev, \"%s: invalid dev pointer\\n\", __func__);\n\t\treturn IRQ_NONE;\n\t}\n\n\t \n\tif (test_bit(STMMAC_DOWN, &priv->state))\n\t\treturn IRQ_HANDLED;\n\n\t \n\tstmmac_common_interrupt(priv);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t stmmac_safety_interrupt(int irq, void *dev_id)\n{\n\tstruct net_device *dev = (struct net_device *)dev_id;\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tif (unlikely(!dev)) {\n\t\tnetdev_err(priv->dev, \"%s: invalid dev pointer\\n\", __func__);\n\t\treturn IRQ_NONE;\n\t}\n\n\t \n\tif (test_bit(STMMAC_DOWN, &priv->state))\n\t\treturn IRQ_HANDLED;\n\n\t \n\tstmmac_safety_feat_interrupt(priv);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t stmmac_msi_intr_tx(int irq, void *data)\n{\n\tstruct stmmac_tx_queue *tx_q = (struct stmmac_tx_queue *)data;\n\tstruct stmmac_dma_conf *dma_conf;\n\tint chan = tx_q->queue_index;\n\tstruct stmmac_priv *priv;\n\tint status;\n\n\tdma_conf = container_of(tx_q, struct stmmac_dma_conf, tx_queue[chan]);\n\tpriv = container_of(dma_conf, struct stmmac_priv, dma_conf);\n\n\tif (unlikely(!data)) {\n\t\tnetdev_err(priv->dev, \"%s: invalid dev pointer\\n\", __func__);\n\t\treturn IRQ_NONE;\n\t}\n\n\t \n\tif (test_bit(STMMAC_DOWN, &priv->state))\n\t\treturn IRQ_HANDLED;\n\n\tstatus = stmmac_napi_check(priv, chan, DMA_DIR_TX);\n\n\tif (unlikely(status & tx_hard_error_bump_tc)) {\n\t\t \n\t\tstmmac_bump_dma_threshold(priv, chan);\n\t} else if (unlikely(status == tx_hard_error)) {\n\t\tstmmac_tx_err(priv, chan);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t stmmac_msi_intr_rx(int irq, void *data)\n{\n\tstruct stmmac_rx_queue *rx_q = (struct stmmac_rx_queue *)data;\n\tstruct stmmac_dma_conf *dma_conf;\n\tint chan = rx_q->queue_index;\n\tstruct stmmac_priv *priv;\n\n\tdma_conf = container_of(rx_q, struct stmmac_dma_conf, rx_queue[chan]);\n\tpriv = container_of(dma_conf, struct stmmac_priv, dma_conf);\n\n\tif (unlikely(!data)) {\n\t\tnetdev_err(priv->dev, \"%s: invalid dev pointer\\n\", __func__);\n\t\treturn IRQ_NONE;\n\t}\n\n\t \n\tif (test_bit(STMMAC_DOWN, &priv->state))\n\t\treturn IRQ_HANDLED;\n\n\tstmmac_napi_check(priv, chan, DMA_DIR_RX);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int stmmac_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tstruct stmmac_priv *priv = netdev_priv (dev);\n\tint ret = -EOPNOTSUPP;\n\n\tif (!netif_running(dev))\n\t\treturn -EINVAL;\n\n\tswitch (cmd) {\n\tcase SIOCGMIIPHY:\n\tcase SIOCGMIIREG:\n\tcase SIOCSMIIREG:\n\t\tret = phylink_mii_ioctl(priv->phylink, rq, cmd);\n\t\tbreak;\n\tcase SIOCSHWTSTAMP:\n\t\tret = stmmac_hwtstamp_set(dev, rq);\n\t\tbreak;\n\tcase SIOCGHWTSTAMP:\n\t\tret = stmmac_hwtstamp_get(dev, rq);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int stmmac_setup_tc_block_cb(enum tc_setup_type type, void *type_data,\n\t\t\t\t    void *cb_priv)\n{\n\tstruct stmmac_priv *priv = cb_priv;\n\tint ret = -EOPNOTSUPP;\n\n\tif (!tc_cls_can_offload_and_chain0(priv->dev, type_data))\n\t\treturn ret;\n\n\t__stmmac_disable_all_queues(priv);\n\n\tswitch (type) {\n\tcase TC_SETUP_CLSU32:\n\t\tret = stmmac_tc_setup_cls_u32(priv, priv, type_data);\n\t\tbreak;\n\tcase TC_SETUP_CLSFLOWER:\n\t\tret = stmmac_tc_setup_cls(priv, priv, type_data);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tstmmac_enable_all_queues(priv);\n\treturn ret;\n}\n\nstatic LIST_HEAD(stmmac_block_cb_list);\n\nstatic int stmmac_setup_tc(struct net_device *ndev, enum tc_setup_type type,\n\t\t\t   void *type_data)\n{\n\tstruct stmmac_priv *priv = netdev_priv(ndev);\n\n\tswitch (type) {\n\tcase TC_QUERY_CAPS:\n\t\treturn stmmac_tc_query_caps(priv, priv, type_data);\n\tcase TC_SETUP_BLOCK:\n\t\treturn flow_block_cb_setup_simple(type_data,\n\t\t\t\t\t\t  &stmmac_block_cb_list,\n\t\t\t\t\t\t  stmmac_setup_tc_block_cb,\n\t\t\t\t\t\t  priv, priv, true);\n\tcase TC_SETUP_QDISC_CBS:\n\t\treturn stmmac_tc_setup_cbs(priv, priv, type_data);\n\tcase TC_SETUP_QDISC_TAPRIO:\n\t\treturn stmmac_tc_setup_taprio(priv, priv, type_data);\n\tcase TC_SETUP_QDISC_ETF:\n\t\treturn stmmac_tc_setup_etf(priv, priv, type_data);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic u16 stmmac_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t\t       struct net_device *sb_dev)\n{\n\tint gso = skb_shinfo(skb)->gso_type;\n\n\tif (gso & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6 | SKB_GSO_UDP_L4)) {\n\t\t \n\t\treturn 0;\n\t}\n\n\treturn netdev_pick_tx(dev, skb, NULL) % dev->real_num_tx_queues;\n}\n\nstatic int stmmac_set_mac_address(struct net_device *ndev, void *addr)\n{\n\tstruct stmmac_priv *priv = netdev_priv(ndev);\n\tint ret = 0;\n\n\tret = pm_runtime_resume_and_get(priv->device);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = eth_mac_addr(ndev, addr);\n\tif (ret)\n\t\tgoto set_mac_error;\n\n\tstmmac_set_umac_addr(priv, priv->hw, ndev->dev_addr, 0);\n\nset_mac_error:\n\tpm_runtime_put(priv->device);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_DEBUG_FS\nstatic struct dentry *stmmac_fs_dir;\n\nstatic void sysfs_display_ring(void *head, int size, int extend_desc,\n\t\t\t       struct seq_file *seq, dma_addr_t dma_phy_addr)\n{\n\tint i;\n\tstruct dma_extended_desc *ep = (struct dma_extended_desc *)head;\n\tstruct dma_desc *p = (struct dma_desc *)head;\n\tdma_addr_t dma_addr;\n\n\tfor (i = 0; i < size; i++) {\n\t\tif (extend_desc) {\n\t\t\tdma_addr = dma_phy_addr + i * sizeof(*ep);\n\t\t\tseq_printf(seq, \"%d [%pad]: 0x%x 0x%x 0x%x 0x%x\\n\",\n\t\t\t\t   i, &dma_addr,\n\t\t\t\t   le32_to_cpu(ep->basic.des0),\n\t\t\t\t   le32_to_cpu(ep->basic.des1),\n\t\t\t\t   le32_to_cpu(ep->basic.des2),\n\t\t\t\t   le32_to_cpu(ep->basic.des3));\n\t\t\tep++;\n\t\t} else {\n\t\t\tdma_addr = dma_phy_addr + i * sizeof(*p);\n\t\t\tseq_printf(seq, \"%d [%pad]: 0x%x 0x%x 0x%x 0x%x\\n\",\n\t\t\t\t   i, &dma_addr,\n\t\t\t\t   le32_to_cpu(p->des0), le32_to_cpu(p->des1),\n\t\t\t\t   le32_to_cpu(p->des2), le32_to_cpu(p->des3));\n\t\t\tp++;\n\t\t}\n\t\tseq_printf(seq, \"\\n\");\n\t}\n}\n\nstatic int stmmac_rings_status_show(struct seq_file *seq, void *v)\n{\n\tstruct net_device *dev = seq->private;\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 rx_count = priv->plat->rx_queues_to_use;\n\tu32 tx_count = priv->plat->tx_queues_to_use;\n\tu32 queue;\n\n\tif ((dev->flags & IFF_UP) == 0)\n\t\treturn 0;\n\n\tfor (queue = 0; queue < rx_count; queue++) {\n\t\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];\n\n\t\tseq_printf(seq, \"RX Queue %d:\\n\", queue);\n\n\t\tif (priv->extend_desc) {\n\t\t\tseq_printf(seq, \"Extended descriptor ring:\\n\");\n\t\t\tsysfs_display_ring((void *)rx_q->dma_erx,\n\t\t\t\t\t   priv->dma_conf.dma_rx_size, 1, seq, rx_q->dma_rx_phy);\n\t\t} else {\n\t\t\tseq_printf(seq, \"Descriptor ring:\\n\");\n\t\t\tsysfs_display_ring((void *)rx_q->dma_rx,\n\t\t\t\t\t   priv->dma_conf.dma_rx_size, 0, seq, rx_q->dma_rx_phy);\n\t\t}\n\t}\n\n\tfor (queue = 0; queue < tx_count; queue++) {\n\t\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\n\t\tseq_printf(seq, \"TX Queue %d:\\n\", queue);\n\n\t\tif (priv->extend_desc) {\n\t\t\tseq_printf(seq, \"Extended descriptor ring:\\n\");\n\t\t\tsysfs_display_ring((void *)tx_q->dma_etx,\n\t\t\t\t\t   priv->dma_conf.dma_tx_size, 1, seq, tx_q->dma_tx_phy);\n\t\t} else if (!(tx_q->tbs & STMMAC_TBS_AVAIL)) {\n\t\t\tseq_printf(seq, \"Descriptor ring:\\n\");\n\t\t\tsysfs_display_ring((void *)tx_q->dma_tx,\n\t\t\t\t\t   priv->dma_conf.dma_tx_size, 0, seq, tx_q->dma_tx_phy);\n\t\t}\n\t}\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(stmmac_rings_status);\n\nstatic int stmmac_dma_cap_show(struct seq_file *seq, void *v)\n{\n\tstatic const char * const dwxgmac_timestamp_source[] = {\n\t\t\"None\",\n\t\t\"Internal\",\n\t\t\"External\",\n\t\t\"Both\",\n\t};\n\tstatic const char * const dwxgmac_safety_feature_desc[] = {\n\t\t\"No\",\n\t\t\"All Safety Features with ECC and Parity\",\n\t\t\"All Safety Features without ECC or Parity\",\n\t\t\"All Safety Features with Parity Only\",\n\t\t\"ECC Only\",\n\t\t\"UNDEFINED\",\n\t\t\"UNDEFINED\",\n\t\t\"UNDEFINED\",\n\t};\n\tstruct net_device *dev = seq->private;\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tif (!priv->hw_cap_support) {\n\t\tseq_printf(seq, \"DMA HW features not supported\\n\");\n\t\treturn 0;\n\t}\n\n\tseq_printf(seq, \"==============================\\n\");\n\tseq_printf(seq, \"\\tDMA HW features\\n\");\n\tseq_printf(seq, \"==============================\\n\");\n\n\tseq_printf(seq, \"\\t10/100 Mbps: %s\\n\",\n\t\t   (priv->dma_cap.mbps_10_100) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\t1000 Mbps: %s\\n\",\n\t\t   (priv->dma_cap.mbps_1000) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tHalf duplex: %s\\n\",\n\t\t   (priv->dma_cap.half_duplex) ? \"Y\" : \"N\");\n\tif (priv->plat->has_xgmac) {\n\t\tseq_printf(seq,\n\t\t\t   \"\\tNumber of Additional MAC address registers: %d\\n\",\n\t\t\t   priv->dma_cap.multi_addr);\n\t} else {\n\t\tseq_printf(seq, \"\\tHash Filter: %s\\n\",\n\t\t\t   (priv->dma_cap.hash_filter) ? \"Y\" : \"N\");\n\t\tseq_printf(seq, \"\\tMultiple MAC address registers: %s\\n\",\n\t\t\t   (priv->dma_cap.multi_addr) ? \"Y\" : \"N\");\n\t}\n\tseq_printf(seq, \"\\tPCS (TBI/SGMII/RTBI PHY interfaces): %s\\n\",\n\t\t   (priv->dma_cap.pcs) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tSMA (MDIO) Interface: %s\\n\",\n\t\t   (priv->dma_cap.sma_mdio) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tPMT Remote wake up: %s\\n\",\n\t\t   (priv->dma_cap.pmt_remote_wake_up) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tPMT Magic Frame: %s\\n\",\n\t\t   (priv->dma_cap.pmt_magic_frame) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tRMON module: %s\\n\",\n\t\t   (priv->dma_cap.rmon) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tIEEE 1588-2002 Time Stamp: %s\\n\",\n\t\t   (priv->dma_cap.time_stamp) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tIEEE 1588-2008 Advanced Time Stamp: %s\\n\",\n\t\t   (priv->dma_cap.atime_stamp) ? \"Y\" : \"N\");\n\tif (priv->plat->has_xgmac)\n\t\tseq_printf(seq, \"\\tTimestamp System Time Source: %s\\n\",\n\t\t\t   dwxgmac_timestamp_source[priv->dma_cap.tssrc]);\n\tseq_printf(seq, \"\\t802.3az - Energy-Efficient Ethernet (EEE): %s\\n\",\n\t\t   (priv->dma_cap.eee) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tAV features: %s\\n\", (priv->dma_cap.av) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tChecksum Offload in TX: %s\\n\",\n\t\t   (priv->dma_cap.tx_coe) ? \"Y\" : \"N\");\n\tif (priv->synopsys_id >= DWMAC_CORE_4_00 ||\n\t    priv->plat->has_xgmac) {\n\t\tseq_printf(seq, \"\\tIP Checksum Offload in RX: %s\\n\",\n\t\t\t   (priv->dma_cap.rx_coe) ? \"Y\" : \"N\");\n\t} else {\n\t\tseq_printf(seq, \"\\tIP Checksum Offload (type1) in RX: %s\\n\",\n\t\t\t   (priv->dma_cap.rx_coe_type1) ? \"Y\" : \"N\");\n\t\tseq_printf(seq, \"\\tIP Checksum Offload (type2) in RX: %s\\n\",\n\t\t\t   (priv->dma_cap.rx_coe_type2) ? \"Y\" : \"N\");\n\t\tseq_printf(seq, \"\\tRXFIFO > 2048bytes: %s\\n\",\n\t\t\t   (priv->dma_cap.rxfifo_over_2048) ? \"Y\" : \"N\");\n\t}\n\tseq_printf(seq, \"\\tNumber of Additional RX channel: %d\\n\",\n\t\t   priv->dma_cap.number_rx_channel);\n\tseq_printf(seq, \"\\tNumber of Additional TX channel: %d\\n\",\n\t\t   priv->dma_cap.number_tx_channel);\n\tseq_printf(seq, \"\\tNumber of Additional RX queues: %d\\n\",\n\t\t   priv->dma_cap.number_rx_queues);\n\tseq_printf(seq, \"\\tNumber of Additional TX queues: %d\\n\",\n\t\t   priv->dma_cap.number_tx_queues);\n\tseq_printf(seq, \"\\tEnhanced descriptors: %s\\n\",\n\t\t   (priv->dma_cap.enh_desc) ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tTX Fifo Size: %d\\n\", priv->dma_cap.tx_fifo_size);\n\tseq_printf(seq, \"\\tRX Fifo Size: %d\\n\", priv->dma_cap.rx_fifo_size);\n\tseq_printf(seq, \"\\tHash Table Size: %lu\\n\", priv->dma_cap.hash_tb_sz ?\n\t\t   (BIT(priv->dma_cap.hash_tb_sz) << 5) : 0);\n\tseq_printf(seq, \"\\tTSO: %s\\n\", priv->dma_cap.tsoen ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tNumber of PPS Outputs: %d\\n\",\n\t\t   priv->dma_cap.pps_out_num);\n\tseq_printf(seq, \"\\tSafety Features: %s\\n\",\n\t\t   dwxgmac_safety_feature_desc[priv->dma_cap.asp]);\n\tseq_printf(seq, \"\\tFlexible RX Parser: %s\\n\",\n\t\t   priv->dma_cap.frpsel ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tEnhanced Addressing: %d\\n\",\n\t\t   priv->dma_cap.host_dma_width);\n\tseq_printf(seq, \"\\tReceive Side Scaling: %s\\n\",\n\t\t   priv->dma_cap.rssen ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tVLAN Hash Filtering: %s\\n\",\n\t\t   priv->dma_cap.vlhash ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tSplit Header: %s\\n\",\n\t\t   priv->dma_cap.sphen ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tVLAN TX Insertion: %s\\n\",\n\t\t   priv->dma_cap.vlins ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tDouble VLAN: %s\\n\",\n\t\t   priv->dma_cap.dvlan ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tNumber of L3/L4 Filters: %d\\n\",\n\t\t   priv->dma_cap.l3l4fnum);\n\tseq_printf(seq, \"\\tARP Offloading: %s\\n\",\n\t\t   priv->dma_cap.arpoffsel ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tEnhancements to Scheduled Traffic (EST): %s\\n\",\n\t\t   priv->dma_cap.estsel ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tFrame Preemption (FPE): %s\\n\",\n\t\t   priv->dma_cap.fpesel ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tTime-Based Scheduling (TBS): %s\\n\",\n\t\t   priv->dma_cap.tbssel ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tNumber of DMA Channels Enabled for TBS: %d\\n\",\n\t\t   priv->dma_cap.tbs_ch_num);\n\tseq_printf(seq, \"\\tPer-Stream Filtering: %s\\n\",\n\t\t   priv->dma_cap.sgfsel ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tTX Timestamp FIFO Depth: %lu\\n\",\n\t\t   BIT(priv->dma_cap.ttsfd) >> 1);\n\tseq_printf(seq, \"\\tNumber of Traffic Classes: %d\\n\",\n\t\t   priv->dma_cap.numtc);\n\tseq_printf(seq, \"\\tDCB Feature: %s\\n\",\n\t\t   priv->dma_cap.dcben ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tIEEE 1588 High Word Register: %s\\n\",\n\t\t   priv->dma_cap.advthword ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tPTP Offload: %s\\n\",\n\t\t   priv->dma_cap.ptoen ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tOne-Step Timestamping: %s\\n\",\n\t\t   priv->dma_cap.osten ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tPriority-Based Flow Control: %s\\n\",\n\t\t   priv->dma_cap.pfcen ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tNumber of Flexible RX Parser Instructions: %lu\\n\",\n\t\t   BIT(priv->dma_cap.frpes) << 6);\n\tseq_printf(seq, \"\\tNumber of Flexible RX Parser Parsable Bytes: %lu\\n\",\n\t\t   BIT(priv->dma_cap.frpbs) << 6);\n\tseq_printf(seq, \"\\tParallel Instruction Processor Engines: %d\\n\",\n\t\t   priv->dma_cap.frppipe_num);\n\tseq_printf(seq, \"\\tNumber of Extended VLAN Tag Filters: %lu\\n\",\n\t\t   priv->dma_cap.nrvf_num ?\n\t\t   (BIT(priv->dma_cap.nrvf_num) << 1) : 0);\n\tseq_printf(seq, \"\\tWidth of the Time Interval Field in GCL: %d\\n\",\n\t\t   priv->dma_cap.estwid ? 4 * priv->dma_cap.estwid + 12 : 0);\n\tseq_printf(seq, \"\\tDepth of GCL: %lu\\n\",\n\t\t   priv->dma_cap.estdep ? (BIT(priv->dma_cap.estdep) << 5) : 0);\n\tseq_printf(seq, \"\\tQueue/Channel-Based VLAN Tag Insertion on TX: %s\\n\",\n\t\t   priv->dma_cap.cbtisel ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tNumber of Auxiliary Snapshot Inputs: %d\\n\",\n\t\t   priv->dma_cap.aux_snapshot_n);\n\tseq_printf(seq, \"\\tOne-Step Timestamping for PTP over UDP/IP: %s\\n\",\n\t\t   priv->dma_cap.pou_ost_en ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tEnhanced DMA: %s\\n\",\n\t\t   priv->dma_cap.edma ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tDifferent Descriptor Cache: %s\\n\",\n\t\t   priv->dma_cap.ediffc ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tVxLAN/NVGRE: %s\\n\",\n\t\t   priv->dma_cap.vxn ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tDebug Memory Interface: %s\\n\",\n\t\t   priv->dma_cap.dbgmem ? \"Y\" : \"N\");\n\tseq_printf(seq, \"\\tNumber of Policing Counters: %lu\\n\",\n\t\t   priv->dma_cap.pcsel ? BIT(priv->dma_cap.pcsel + 3) : 0);\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(stmmac_dma_cap);\n\n \nstatic int stmmac_device_event(struct notifier_block *unused,\n\t\t\t       unsigned long event, void *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tif (dev->netdev_ops != &stmmac_netdev_ops)\n\t\tgoto done;\n\n\tswitch (event) {\n\tcase NETDEV_CHANGENAME:\n\t\tif (priv->dbgfs_dir)\n\t\t\tpriv->dbgfs_dir = debugfs_rename(stmmac_fs_dir,\n\t\t\t\t\t\t\t priv->dbgfs_dir,\n\t\t\t\t\t\t\t stmmac_fs_dir,\n\t\t\t\t\t\t\t dev->name);\n\t\tbreak;\n\t}\ndone:\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block stmmac_notifier = {\n\t.notifier_call = stmmac_device_event,\n};\n\nstatic void stmmac_init_fs(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\trtnl_lock();\n\n\t \n\tpriv->dbgfs_dir = debugfs_create_dir(dev->name, stmmac_fs_dir);\n\n\t \n\tdebugfs_create_file(\"descriptors_status\", 0444, priv->dbgfs_dir, dev,\n\t\t\t    &stmmac_rings_status_fops);\n\n\t \n\tdebugfs_create_file(\"dma_cap\", 0444, priv->dbgfs_dir, dev,\n\t\t\t    &stmmac_dma_cap_fops);\n\n\trtnl_unlock();\n}\n\nstatic void stmmac_exit_fs(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tdebugfs_remove_recursive(priv->dbgfs_dir);\n}\n#endif  \n\nstatic u32 stmmac_vid_crc32_le(__le16 vid_le)\n{\n\tunsigned char *data = (unsigned char *)&vid_le;\n\tunsigned char data_byte = 0;\n\tu32 crc = ~0x0;\n\tu32 temp = 0;\n\tint i, bits;\n\n\tbits = get_bitmask_order(VLAN_VID_MASK);\n\tfor (i = 0; i < bits; i++) {\n\t\tif ((i % 8) == 0)\n\t\t\tdata_byte = data[i / 8];\n\n\t\ttemp = ((crc & 1) ^ data_byte) & 1;\n\t\tcrc >>= 1;\n\t\tdata_byte >>= 1;\n\n\t\tif (temp)\n\t\t\tcrc ^= 0xedb88320;\n\t}\n\n\treturn crc;\n}\n\nstatic int stmmac_vlan_update(struct stmmac_priv *priv, bool is_double)\n{\n\tu32 crc, hash = 0;\n\t__le16 pmatch = 0;\n\tint count = 0;\n\tu16 vid = 0;\n\n\tfor_each_set_bit(vid, priv->active_vlans, VLAN_N_VID) {\n\t\t__le16 vid_le = cpu_to_le16(vid);\n\t\tcrc = bitrev32(~stmmac_vid_crc32_le(vid_le)) >> 28;\n\t\thash |= (1 << crc);\n\t\tcount++;\n\t}\n\n\tif (!priv->dma_cap.vlhash) {\n\t\tif (count > 2)  \n\t\t\treturn -EOPNOTSUPP;\n\n\t\tpmatch = cpu_to_le16(vid);\n\t\thash = 0;\n\t}\n\n\treturn stmmac_update_vlan_hash(priv, priv->hw, hash, pmatch, is_double);\n}\n\nstatic int stmmac_vlan_rx_add_vid(struct net_device *ndev, __be16 proto, u16 vid)\n{\n\tstruct stmmac_priv *priv = netdev_priv(ndev);\n\tbool is_double = false;\n\tint ret;\n\n\tret = pm_runtime_resume_and_get(priv->device);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (be16_to_cpu(proto) == ETH_P_8021AD)\n\t\tis_double = true;\n\n\tset_bit(vid, priv->active_vlans);\n\tret = stmmac_vlan_update(priv, is_double);\n\tif (ret) {\n\t\tclear_bit(vid, priv->active_vlans);\n\t\tgoto err_pm_put;\n\t}\n\n\tif (priv->hw->num_vlan) {\n\t\tret = stmmac_add_hw_vlan_rx_fltr(priv, ndev, priv->hw, proto, vid);\n\t\tif (ret)\n\t\t\tgoto err_pm_put;\n\t}\nerr_pm_put:\n\tpm_runtime_put(priv->device);\n\n\treturn ret;\n}\n\nstatic int stmmac_vlan_rx_kill_vid(struct net_device *ndev, __be16 proto, u16 vid)\n{\n\tstruct stmmac_priv *priv = netdev_priv(ndev);\n\tbool is_double = false;\n\tint ret;\n\n\tret = pm_runtime_resume_and_get(priv->device);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (be16_to_cpu(proto) == ETH_P_8021AD)\n\t\tis_double = true;\n\n\tclear_bit(vid, priv->active_vlans);\n\n\tif (priv->hw->num_vlan) {\n\t\tret = stmmac_del_hw_vlan_rx_fltr(priv, ndev, priv->hw, proto, vid);\n\t\tif (ret)\n\t\t\tgoto del_vlan_error;\n\t}\n\n\tret = stmmac_vlan_update(priv, is_double);\n\ndel_vlan_error:\n\tpm_runtime_put(priv->device);\n\n\treturn ret;\n}\n\nstatic int stmmac_bpf(struct net_device *dev, struct netdev_bpf *bpf)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\n\tswitch (bpf->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn stmmac_xdp_set_prog(priv, bpf->prog, bpf->extack);\n\tcase XDP_SETUP_XSK_POOL:\n\t\treturn stmmac_xdp_setup_pool(priv, bpf->xsk.pool,\n\t\t\t\t\t     bpf->xsk.queue_id);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int stmmac_xdp_xmit(struct net_device *dev, int num_frames,\n\t\t\t   struct xdp_frame **frames, u32 flags)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tint cpu = smp_processor_id();\n\tstruct netdev_queue *nq;\n\tint i, nxmit = 0;\n\tint queue;\n\n\tif (unlikely(test_bit(STMMAC_DOWN, &priv->state)))\n\t\treturn -ENETDOWN;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\tqueue = stmmac_xdp_get_tx_queue(priv, cpu);\n\tnq = netdev_get_tx_queue(priv->dev, queue);\n\n\t__netif_tx_lock(nq, cpu);\n\t \n\ttxq_trans_cond_update(nq);\n\n\tfor (i = 0; i < num_frames; i++) {\n\t\tint res;\n\n\t\tres = stmmac_xdp_xmit_xdpf(priv, queue, frames[i], true);\n\t\tif (res == STMMAC_XDP_CONSUMED)\n\t\t\tbreak;\n\n\t\tnxmit++;\n\t}\n\n\tif (flags & XDP_XMIT_FLUSH) {\n\t\tstmmac_flush_tx_descriptors(priv, queue);\n\t\tstmmac_tx_timer_arm(priv, queue);\n\t}\n\n\t__netif_tx_unlock(nq);\n\n\treturn nxmit;\n}\n\nvoid stmmac_disable_rx_queue(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_channel *ch = &priv->channel[queue];\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ch->lock, flags);\n\tstmmac_disable_dma_irq(priv, priv->ioaddr, queue, 1, 0);\n\tspin_unlock_irqrestore(&ch->lock, flags);\n\n\tstmmac_stop_rx_dma(priv, queue);\n\t__free_dma_rx_desc_resources(priv, &priv->dma_conf, queue);\n}\n\nvoid stmmac_enable_rx_queue(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];\n\tstruct stmmac_channel *ch = &priv->channel[queue];\n\tunsigned long flags;\n\tu32 buf_size;\n\tint ret;\n\n\tret = __alloc_dma_rx_desc_resources(priv, &priv->dma_conf, queue);\n\tif (ret) {\n\t\tnetdev_err(priv->dev, \"Failed to alloc RX desc.\\n\");\n\t\treturn;\n\t}\n\n\tret = __init_dma_rx_desc_rings(priv, &priv->dma_conf, queue, GFP_KERNEL);\n\tif (ret) {\n\t\t__free_dma_rx_desc_resources(priv, &priv->dma_conf, queue);\n\t\tnetdev_err(priv->dev, \"Failed to init RX desc.\\n\");\n\t\treturn;\n\t}\n\n\tstmmac_reset_rx_queue(priv, queue);\n\tstmmac_clear_rx_descriptors(priv, &priv->dma_conf, queue);\n\n\tstmmac_init_rx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,\n\t\t\t    rx_q->dma_rx_phy, rx_q->queue_index);\n\n\trx_q->rx_tail_addr = rx_q->dma_rx_phy + (rx_q->buf_alloc_num *\n\t\t\t     sizeof(struct dma_desc));\n\tstmmac_set_rx_tail_ptr(priv, priv->ioaddr,\n\t\t\t       rx_q->rx_tail_addr, rx_q->queue_index);\n\n\tif (rx_q->xsk_pool && rx_q->buf_alloc_num) {\n\t\tbuf_size = xsk_pool_get_rx_frame_size(rx_q->xsk_pool);\n\t\tstmmac_set_dma_bfsize(priv, priv->ioaddr,\n\t\t\t\t      buf_size,\n\t\t\t\t      rx_q->queue_index);\n\t} else {\n\t\tstmmac_set_dma_bfsize(priv, priv->ioaddr,\n\t\t\t\t      priv->dma_conf.dma_buf_sz,\n\t\t\t\t      rx_q->queue_index);\n\t}\n\n\tstmmac_start_rx_dma(priv, queue);\n\n\tspin_lock_irqsave(&ch->lock, flags);\n\tstmmac_enable_dma_irq(priv, priv->ioaddr, queue, 1, 0);\n\tspin_unlock_irqrestore(&ch->lock, flags);\n}\n\nvoid stmmac_disable_tx_queue(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_channel *ch = &priv->channel[queue];\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ch->lock, flags);\n\tstmmac_disable_dma_irq(priv, priv->ioaddr, queue, 0, 1);\n\tspin_unlock_irqrestore(&ch->lock, flags);\n\n\tstmmac_stop_tx_dma(priv, queue);\n\t__free_dma_tx_desc_resources(priv, &priv->dma_conf, queue);\n}\n\nvoid stmmac_enable_tx_queue(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\tstruct stmmac_channel *ch = &priv->channel[queue];\n\tunsigned long flags;\n\tint ret;\n\n\tret = __alloc_dma_tx_desc_resources(priv, &priv->dma_conf, queue);\n\tif (ret) {\n\t\tnetdev_err(priv->dev, \"Failed to alloc TX desc.\\n\");\n\t\treturn;\n\t}\n\n\tret = __init_dma_tx_desc_rings(priv,  &priv->dma_conf, queue);\n\tif (ret) {\n\t\t__free_dma_tx_desc_resources(priv, &priv->dma_conf, queue);\n\t\tnetdev_err(priv->dev, \"Failed to init TX desc.\\n\");\n\t\treturn;\n\t}\n\n\tstmmac_reset_tx_queue(priv, queue);\n\tstmmac_clear_tx_descriptors(priv, &priv->dma_conf, queue);\n\n\tstmmac_init_tx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,\n\t\t\t    tx_q->dma_tx_phy, tx_q->queue_index);\n\n\tif (tx_q->tbs & STMMAC_TBS_AVAIL)\n\t\tstmmac_enable_tbs(priv, priv->ioaddr, 1, tx_q->queue_index);\n\n\ttx_q->tx_tail_addr = tx_q->dma_tx_phy;\n\tstmmac_set_tx_tail_ptr(priv, priv->ioaddr,\n\t\t\t       tx_q->tx_tail_addr, tx_q->queue_index);\n\n\tstmmac_start_tx_dma(priv, queue);\n\n\tspin_lock_irqsave(&ch->lock, flags);\n\tstmmac_enable_dma_irq(priv, priv->ioaddr, queue, 0, 1);\n\tspin_unlock_irqrestore(&ch->lock, flags);\n}\n\nvoid stmmac_xdp_release(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 chan;\n\n\t \n\tnetif_tx_disable(dev);\n\n\t \n\tstmmac_disable_all_queues(priv);\n\n\tfor (chan = 0; chan < priv->plat->tx_queues_to_use; chan++)\n\t\thrtimer_cancel(&priv->dma_conf.tx_queue[chan].txtimer);\n\n\t \n\tstmmac_free_irq(dev, REQ_IRQ_ERR_ALL, 0);\n\n\t \n\tstmmac_stop_all_dma(priv);\n\n\t \n\tfree_dma_desc_resources(priv, &priv->dma_conf);\n\n\t \n\tstmmac_mac_set(priv, priv->ioaddr, false);\n\n\t \n\tnetif_trans_update(dev);\n\tnetif_carrier_off(dev);\n}\n\nint stmmac_xdp_open(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 rx_cnt = priv->plat->rx_queues_to_use;\n\tu32 tx_cnt = priv->plat->tx_queues_to_use;\n\tu32 dma_csr_ch = max(rx_cnt, tx_cnt);\n\tstruct stmmac_rx_queue *rx_q;\n\tstruct stmmac_tx_queue *tx_q;\n\tu32 buf_size;\n\tbool sph_en;\n\tu32 chan;\n\tint ret;\n\n\tret = alloc_dma_desc_resources(priv, &priv->dma_conf);\n\tif (ret < 0) {\n\t\tnetdev_err(dev, \"%s: DMA descriptors allocation failed\\n\",\n\t\t\t   __func__);\n\t\tgoto dma_desc_error;\n\t}\n\n\tret = init_dma_desc_rings(dev, &priv->dma_conf, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tnetdev_err(dev, \"%s: DMA descriptors initialization failed\\n\",\n\t\t\t   __func__);\n\t\tgoto init_error;\n\t}\n\n\tstmmac_reset_queues_param(priv);\n\n\t \n\tfor (chan = 0; chan < dma_csr_ch; chan++) {\n\t\tstmmac_init_chan(priv, priv->ioaddr, priv->plat->dma_cfg, chan);\n\t\tstmmac_disable_dma_irq(priv, priv->ioaddr, chan, 1, 1);\n\t}\n\n\t \n\tsph_en = (priv->hw->rx_csum > 0) && priv->sph;\n\n\t \n\tfor (chan = 0; chan < rx_cnt; chan++) {\n\t\trx_q = &priv->dma_conf.rx_queue[chan];\n\n\t\tstmmac_init_rx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,\n\t\t\t\t    rx_q->dma_rx_phy, chan);\n\n\t\trx_q->rx_tail_addr = rx_q->dma_rx_phy +\n\t\t\t\t     (rx_q->buf_alloc_num *\n\t\t\t\t      sizeof(struct dma_desc));\n\t\tstmmac_set_rx_tail_ptr(priv, priv->ioaddr,\n\t\t\t\t       rx_q->rx_tail_addr, chan);\n\n\t\tif (rx_q->xsk_pool && rx_q->buf_alloc_num) {\n\t\t\tbuf_size = xsk_pool_get_rx_frame_size(rx_q->xsk_pool);\n\t\t\tstmmac_set_dma_bfsize(priv, priv->ioaddr,\n\t\t\t\t\t      buf_size,\n\t\t\t\t\t      rx_q->queue_index);\n\t\t} else {\n\t\t\tstmmac_set_dma_bfsize(priv, priv->ioaddr,\n\t\t\t\t\t      priv->dma_conf.dma_buf_sz,\n\t\t\t\t\t      rx_q->queue_index);\n\t\t}\n\n\t\tstmmac_enable_sph(priv, priv->ioaddr, sph_en, chan);\n\t}\n\n\t \n\tfor (chan = 0; chan < tx_cnt; chan++) {\n\t\ttx_q = &priv->dma_conf.tx_queue[chan];\n\n\t\tstmmac_init_tx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,\n\t\t\t\t    tx_q->dma_tx_phy, chan);\n\n\t\ttx_q->tx_tail_addr = tx_q->dma_tx_phy;\n\t\tstmmac_set_tx_tail_ptr(priv, priv->ioaddr,\n\t\t\t\t       tx_q->tx_tail_addr, chan);\n\n\t\thrtimer_init(&tx_q->txtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\t\ttx_q->txtimer.function = stmmac_tx_timer;\n\t}\n\n\t \n\tstmmac_mac_set(priv, priv->ioaddr, true);\n\n\t \n\tstmmac_start_all_dma(priv);\n\n\tret = stmmac_request_irq(dev);\n\tif (ret)\n\t\tgoto irq_error;\n\n\t \n\tstmmac_enable_all_queues(priv);\n\tnetif_carrier_on(dev);\n\tnetif_tx_start_all_queues(dev);\n\tstmmac_enable_all_dma_irq(priv);\n\n\treturn 0;\n\nirq_error:\n\tfor (chan = 0; chan < priv->plat->tx_queues_to_use; chan++)\n\t\thrtimer_cancel(&priv->dma_conf.tx_queue[chan].txtimer);\n\n\tstmmac_hw_teardown(dev);\ninit_error:\n\tfree_dma_desc_resources(priv, &priv->dma_conf);\ndma_desc_error:\n\treturn ret;\n}\n\nint stmmac_xsk_wakeup(struct net_device *dev, u32 queue, u32 flags)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tstruct stmmac_rx_queue *rx_q;\n\tstruct stmmac_tx_queue *tx_q;\n\tstruct stmmac_channel *ch;\n\n\tif (test_bit(STMMAC_DOWN, &priv->state) ||\n\t    !netif_carrier_ok(priv->dev))\n\t\treturn -ENETDOWN;\n\n\tif (!stmmac_xdp_is_enabled(priv))\n\t\treturn -EINVAL;\n\n\tif (queue >= priv->plat->rx_queues_to_use ||\n\t    queue >= priv->plat->tx_queues_to_use)\n\t\treturn -EINVAL;\n\n\trx_q = &priv->dma_conf.rx_queue[queue];\n\ttx_q = &priv->dma_conf.tx_queue[queue];\n\tch = &priv->channel[queue];\n\n\tif (!rx_q->xsk_pool && !tx_q->xsk_pool)\n\t\treturn -EINVAL;\n\n\tif (!napi_if_scheduled_mark_missed(&ch->rxtx_napi)) {\n\t\t \n\t\tif (likely(napi_schedule_prep(&ch->rxtx_napi)))\n\t\t\t__napi_schedule(&ch->rxtx_napi);\n\t}\n\n\treturn 0;\n}\n\nstatic void stmmac_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 tx_cnt = priv->plat->tx_queues_to_use;\n\tu32 rx_cnt = priv->plat->rx_queues_to_use;\n\tunsigned int start;\n\tint q;\n\n\tfor (q = 0; q < tx_cnt; q++) {\n\t\tstruct stmmac_txq_stats *txq_stats = &priv->xstats.txq_stats[q];\n\t\tu64 tx_packets;\n\t\tu64 tx_bytes;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&txq_stats->syncp);\n\t\t\ttx_packets = txq_stats->tx_packets;\n\t\t\ttx_bytes   = txq_stats->tx_bytes;\n\t\t} while (u64_stats_fetch_retry(&txq_stats->syncp, start));\n\n\t\tstats->tx_packets += tx_packets;\n\t\tstats->tx_bytes += tx_bytes;\n\t}\n\n\tfor (q = 0; q < rx_cnt; q++) {\n\t\tstruct stmmac_rxq_stats *rxq_stats = &priv->xstats.rxq_stats[q];\n\t\tu64 rx_packets;\n\t\tu64 rx_bytes;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&rxq_stats->syncp);\n\t\t\trx_packets = rxq_stats->rx_packets;\n\t\t\trx_bytes   = rxq_stats->rx_bytes;\n\t\t} while (u64_stats_fetch_retry(&rxq_stats->syncp, start));\n\n\t\tstats->rx_packets += rx_packets;\n\t\tstats->rx_bytes += rx_bytes;\n\t}\n\n\tstats->rx_dropped = priv->xstats.rx_dropped;\n\tstats->rx_errors = priv->xstats.rx_errors;\n\tstats->tx_dropped = priv->xstats.tx_dropped;\n\tstats->tx_errors = priv->xstats.tx_errors;\n\tstats->tx_carrier_errors = priv->xstats.tx_losscarrier + priv->xstats.tx_carrier;\n\tstats->collisions = priv->xstats.tx_collision + priv->xstats.rx_collision;\n\tstats->rx_length_errors = priv->xstats.rx_length;\n\tstats->rx_crc_errors = priv->xstats.rx_crc_errors;\n\tstats->rx_over_errors = priv->xstats.rx_overflow_cntr;\n\tstats->rx_missed_errors = priv->xstats.rx_missed_cntr;\n}\n\nstatic const struct net_device_ops stmmac_netdev_ops = {\n\t.ndo_open = stmmac_open,\n\t.ndo_start_xmit = stmmac_xmit,\n\t.ndo_stop = stmmac_release,\n\t.ndo_change_mtu = stmmac_change_mtu,\n\t.ndo_fix_features = stmmac_fix_features,\n\t.ndo_set_features = stmmac_set_features,\n\t.ndo_set_rx_mode = stmmac_set_rx_mode,\n\t.ndo_tx_timeout = stmmac_tx_timeout,\n\t.ndo_eth_ioctl = stmmac_ioctl,\n\t.ndo_get_stats64 = stmmac_get_stats64,\n\t.ndo_setup_tc = stmmac_setup_tc,\n\t.ndo_select_queue = stmmac_select_queue,\n\t.ndo_set_mac_address = stmmac_set_mac_address,\n\t.ndo_vlan_rx_add_vid = stmmac_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid = stmmac_vlan_rx_kill_vid,\n\t.ndo_bpf = stmmac_bpf,\n\t.ndo_xdp_xmit = stmmac_xdp_xmit,\n\t.ndo_xsk_wakeup = stmmac_xsk_wakeup,\n};\n\nstatic void stmmac_reset_subtask(struct stmmac_priv *priv)\n{\n\tif (!test_and_clear_bit(STMMAC_RESET_REQUESTED, &priv->state))\n\t\treturn;\n\tif (test_bit(STMMAC_DOWN, &priv->state))\n\t\treturn;\n\n\tnetdev_err(priv->dev, \"Reset adapter.\\n\");\n\n\trtnl_lock();\n\tnetif_trans_update(priv->dev);\n\twhile (test_and_set_bit(STMMAC_RESETING, &priv->state))\n\t\tusleep_range(1000, 2000);\n\n\tset_bit(STMMAC_DOWN, &priv->state);\n\tdev_close(priv->dev);\n\tdev_open(priv->dev, NULL);\n\tclear_bit(STMMAC_DOWN, &priv->state);\n\tclear_bit(STMMAC_RESETING, &priv->state);\n\trtnl_unlock();\n}\n\nstatic void stmmac_service_task(struct work_struct *work)\n{\n\tstruct stmmac_priv *priv = container_of(work, struct stmmac_priv,\n\t\t\tservice_task);\n\n\tstmmac_reset_subtask(priv);\n\tclear_bit(STMMAC_SERVICE_SCHED, &priv->state);\n}\n\n \nstatic int stmmac_hw_init(struct stmmac_priv *priv)\n{\n\tint ret;\n\n\t \n\tif (priv->plat->flags & STMMAC_FLAG_HAS_SUN8I)\n\t\tchain_mode = 1;\n\tpriv->chain_mode = chain_mode;\n\n\t \n\tret = stmmac_hwif_init(priv);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tpriv->hw_cap_support = stmmac_get_hw_features(priv);\n\tif (priv->hw_cap_support) {\n\t\tdev_info(priv->device, \"DMA HW capability register supported\\n\");\n\n\t\t \n\t\tpriv->plat->enh_desc = priv->dma_cap.enh_desc;\n\t\tpriv->plat->pmt = priv->dma_cap.pmt_remote_wake_up &&\n\t\t\t\t!(priv->plat->flags & STMMAC_FLAG_USE_PHY_WOL);\n\t\tpriv->hw->pmt = priv->plat->pmt;\n\t\tif (priv->dma_cap.hash_tb_sz) {\n\t\t\tpriv->hw->multicast_filter_bins =\n\t\t\t\t\t(BIT(priv->dma_cap.hash_tb_sz) << 5);\n\t\t\tpriv->hw->mcast_bits_log2 =\n\t\t\t\t\tilog2(priv->hw->multicast_filter_bins);\n\t\t}\n\n\t\t \n\t\tif (priv->plat->force_thresh_dma_mode)\n\t\t\tpriv->plat->tx_coe = 0;\n\t\telse\n\t\t\tpriv->plat->tx_coe = priv->dma_cap.tx_coe;\n\n\t\t \n\t\tpriv->plat->rx_coe = priv->dma_cap.rx_coe;\n\n\t\tif (priv->dma_cap.rx_coe_type2)\n\t\t\tpriv->plat->rx_coe = STMMAC_RX_COE_TYPE2;\n\t\telse if (priv->dma_cap.rx_coe_type1)\n\t\t\tpriv->plat->rx_coe = STMMAC_RX_COE_TYPE1;\n\n\t} else {\n\t\tdev_info(priv->device, \"No HW DMA feature register supported\\n\");\n\t}\n\n\tif (priv->plat->rx_coe) {\n\t\tpriv->hw->rx_csum = priv->plat->rx_coe;\n\t\tdev_info(priv->device, \"RX Checksum Offload Engine supported\\n\");\n\t\tif (priv->synopsys_id < DWMAC_CORE_4_00)\n\t\t\tdev_info(priv->device, \"COE Type %d\\n\", priv->hw->rx_csum);\n\t}\n\tif (priv->plat->tx_coe)\n\t\tdev_info(priv->device, \"TX Checksum insertion supported\\n\");\n\n\tif (priv->plat->pmt) {\n\t\tdev_info(priv->device, \"Wake-Up On Lan supported\\n\");\n\t\tdevice_set_wakeup_capable(priv->device, 1);\n\t}\n\n\tif (priv->dma_cap.tsoen)\n\t\tdev_info(priv->device, \"TSO supported\\n\");\n\n\tpriv->hw->vlan_fail_q_en =\n\t\t(priv->plat->flags & STMMAC_FLAG_VLAN_FAIL_Q_EN);\n\tpriv->hw->vlan_fail_q = priv->plat->vlan_fail_q;\n\n\t \n\tif (priv->hwif_quirks) {\n\t\tret = priv->hwif_quirks(priv);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tif (((priv->synopsys_id >= DWMAC_CORE_3_50) ||\n\t    (priv->plat->has_xgmac)) && (!priv->plat->riwt_off)) {\n\t\tpriv->use_riwt = 1;\n\t\tdev_info(priv->device,\n\t\t\t \"Enable RX Mitigation via HW Watchdog Timer\\n\");\n\t}\n\n\treturn 0;\n}\n\nstatic void stmmac_napi_add(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 queue, maxq;\n\n\tmaxq = max(priv->plat->rx_queues_to_use, priv->plat->tx_queues_to_use);\n\n\tfor (queue = 0; queue < maxq; queue++) {\n\t\tstruct stmmac_channel *ch = &priv->channel[queue];\n\n\t\tch->priv_data = priv;\n\t\tch->index = queue;\n\t\tspin_lock_init(&ch->lock);\n\n\t\tif (queue < priv->plat->rx_queues_to_use) {\n\t\t\tnetif_napi_add(dev, &ch->rx_napi, stmmac_napi_poll_rx);\n\t\t}\n\t\tif (queue < priv->plat->tx_queues_to_use) {\n\t\t\tnetif_napi_add_tx(dev, &ch->tx_napi,\n\t\t\t\t\t  stmmac_napi_poll_tx);\n\t\t}\n\t\tif (queue < priv->plat->rx_queues_to_use &&\n\t\t    queue < priv->plat->tx_queues_to_use) {\n\t\t\tnetif_napi_add(dev, &ch->rxtx_napi,\n\t\t\t\t       stmmac_napi_poll_rxtx);\n\t\t}\n\t}\n}\n\nstatic void stmmac_napi_del(struct net_device *dev)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tu32 queue, maxq;\n\n\tmaxq = max(priv->plat->rx_queues_to_use, priv->plat->tx_queues_to_use);\n\n\tfor (queue = 0; queue < maxq; queue++) {\n\t\tstruct stmmac_channel *ch = &priv->channel[queue];\n\n\t\tif (queue < priv->plat->rx_queues_to_use)\n\t\t\tnetif_napi_del(&ch->rx_napi);\n\t\tif (queue < priv->plat->tx_queues_to_use)\n\t\t\tnetif_napi_del(&ch->tx_napi);\n\t\tif (queue < priv->plat->rx_queues_to_use &&\n\t\t    queue < priv->plat->tx_queues_to_use) {\n\t\t\tnetif_napi_del(&ch->rxtx_napi);\n\t\t}\n\t}\n}\n\nint stmmac_reinit_queues(struct net_device *dev, u32 rx_cnt, u32 tx_cnt)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tint ret = 0, i;\n\n\tif (netif_running(dev))\n\t\tstmmac_release(dev);\n\n\tstmmac_napi_del(dev);\n\n\tpriv->plat->rx_queues_to_use = rx_cnt;\n\tpriv->plat->tx_queues_to_use = tx_cnt;\n\tif (!netif_is_rxfh_configured(dev))\n\t\tfor (i = 0; i < ARRAY_SIZE(priv->rss.table); i++)\n\t\t\tpriv->rss.table[i] = ethtool_rxfh_indir_default(i,\n\t\t\t\t\t\t\t\t\trx_cnt);\n\n\tstmmac_set_half_duplex(priv);\n\tstmmac_napi_add(dev);\n\n\tif (netif_running(dev))\n\t\tret = stmmac_open(dev);\n\n\treturn ret;\n}\n\nint stmmac_reinit_ringparam(struct net_device *dev, u32 rx_size, u32 tx_size)\n{\n\tstruct stmmac_priv *priv = netdev_priv(dev);\n\tint ret = 0;\n\n\tif (netif_running(dev))\n\t\tstmmac_release(dev);\n\n\tpriv->dma_conf.dma_rx_size = rx_size;\n\tpriv->dma_conf.dma_tx_size = tx_size;\n\n\tif (netif_running(dev))\n\t\tret = stmmac_open(dev);\n\n\treturn ret;\n}\n\n#define SEND_VERIFY_MPAKCET_FMT \"Send Verify mPacket lo_state=%d lp_state=%d\\n\"\nstatic void stmmac_fpe_lp_task(struct work_struct *work)\n{\n\tstruct stmmac_priv *priv = container_of(work, struct stmmac_priv,\n\t\t\t\t\t\tfpe_task);\n\tstruct stmmac_fpe_cfg *fpe_cfg = priv->plat->fpe_cfg;\n\tenum stmmac_fpe_state *lo_state = &fpe_cfg->lo_fpe_state;\n\tenum stmmac_fpe_state *lp_state = &fpe_cfg->lp_fpe_state;\n\tbool *hs_enable = &fpe_cfg->hs_enable;\n\tbool *enable = &fpe_cfg->enable;\n\tint retries = 20;\n\n\twhile (retries-- > 0) {\n\t\t \n\t\tif (*lo_state == FPE_STATE_OFF || !*hs_enable)\n\t\t\tbreak;\n\n\t\tif (*lo_state == FPE_STATE_ENTERING_ON &&\n\t\t    *lp_state == FPE_STATE_ENTERING_ON) {\n\t\t\tstmmac_fpe_configure(priv, priv->ioaddr,\n\t\t\t\t\t     fpe_cfg,\n\t\t\t\t\t     priv->plat->tx_queues_to_use,\n\t\t\t\t\t     priv->plat->rx_queues_to_use,\n\t\t\t\t\t     *enable);\n\n\t\t\tnetdev_info(priv->dev, \"configured FPE\\n\");\n\n\t\t\t*lo_state = FPE_STATE_ON;\n\t\t\t*lp_state = FPE_STATE_ON;\n\t\t\tnetdev_info(priv->dev, \"!!! BOTH FPE stations ON\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((*lo_state == FPE_STATE_CAPABLE ||\n\t\t     *lo_state == FPE_STATE_ENTERING_ON) &&\n\t\t     *lp_state != FPE_STATE_ON) {\n\t\t\tnetdev_info(priv->dev, SEND_VERIFY_MPAKCET_FMT,\n\t\t\t\t    *lo_state, *lp_state);\n\t\t\tstmmac_fpe_send_mpacket(priv, priv->ioaddr,\n\t\t\t\t\t\tfpe_cfg,\n\t\t\t\t\t\tMPACKET_VERIFY);\n\t\t}\n\t\t \n\t\tmsleep(500);\n\t}\n\n\tclear_bit(__FPE_TASK_SCHED, &priv->fpe_task_state);\n}\n\nvoid stmmac_fpe_handshake(struct stmmac_priv *priv, bool enable)\n{\n\tif (priv->plat->fpe_cfg->hs_enable != enable) {\n\t\tif (enable) {\n\t\t\tstmmac_fpe_send_mpacket(priv, priv->ioaddr,\n\t\t\t\t\t\tpriv->plat->fpe_cfg,\n\t\t\t\t\t\tMPACKET_VERIFY);\n\t\t} else {\n\t\t\tpriv->plat->fpe_cfg->lo_fpe_state = FPE_STATE_OFF;\n\t\t\tpriv->plat->fpe_cfg->lp_fpe_state = FPE_STATE_OFF;\n\t\t}\n\n\t\tpriv->plat->fpe_cfg->hs_enable = enable;\n\t}\n}\n\nstatic int stmmac_xdp_rx_timestamp(const struct xdp_md *_ctx, u64 *timestamp)\n{\n\tconst struct stmmac_xdp_buff *ctx = (void *)_ctx;\n\tstruct dma_desc *desc_contains_ts = ctx->desc;\n\tstruct stmmac_priv *priv = ctx->priv;\n\tstruct dma_desc *ndesc = ctx->ndesc;\n\tstruct dma_desc *desc = ctx->desc;\n\tu64 ns = 0;\n\n\tif (!priv->hwts_rx_en)\n\t\treturn -ENODATA;\n\n\t \n\tif (priv->plat->has_gmac4 || priv->plat->has_xgmac)\n\t\tdesc_contains_ts = ndesc;\n\n\t \n\tif (stmmac_get_rx_timestamp_status(priv, desc, ndesc, priv->adv_ts)) {\n\t\tstmmac_get_timestamp(priv, desc_contains_ts, priv->adv_ts, &ns);\n\t\tns -= priv->plat->cdc_error_adj;\n\t\t*timestamp = ns_to_ktime(ns);\n\t\treturn 0;\n\t}\n\n\treturn -ENODATA;\n}\n\nstatic const struct xdp_metadata_ops stmmac_xdp_metadata_ops = {\n\t.xmo_rx_timestamp\t\t= stmmac_xdp_rx_timestamp,\n};\n\n \nint stmmac_dvr_probe(struct device *device,\n\t\t     struct plat_stmmacenet_data *plat_dat,\n\t\t     struct stmmac_resources *res)\n{\n\tstruct net_device *ndev = NULL;\n\tstruct stmmac_priv *priv;\n\tu32 rxq;\n\tint i, ret = 0;\n\n\tndev = devm_alloc_etherdev_mqs(device, sizeof(struct stmmac_priv),\n\t\t\t\t       MTL_MAX_TX_QUEUES, MTL_MAX_RX_QUEUES);\n\tif (!ndev)\n\t\treturn -ENOMEM;\n\n\tSET_NETDEV_DEV(ndev, device);\n\n\tpriv = netdev_priv(ndev);\n\tpriv->device = device;\n\tpriv->dev = ndev;\n\n\tfor (i = 0; i < MTL_MAX_RX_QUEUES; i++)\n\t\tu64_stats_init(&priv->xstats.rxq_stats[i].syncp);\n\tfor (i = 0; i < MTL_MAX_TX_QUEUES; i++)\n\t\tu64_stats_init(&priv->xstats.txq_stats[i].syncp);\n\n\tstmmac_set_ethtool_ops(ndev);\n\tpriv->pause = pause;\n\tpriv->plat = plat_dat;\n\tpriv->ioaddr = res->addr;\n\tpriv->dev->base_addr = (unsigned long)res->addr;\n\tpriv->plat->dma_cfg->multi_msi_en =\n\t\t(priv->plat->flags & STMMAC_FLAG_MULTI_MSI_EN);\n\n\tpriv->dev->irq = res->irq;\n\tpriv->wol_irq = res->wol_irq;\n\tpriv->lpi_irq = res->lpi_irq;\n\tpriv->sfty_ce_irq = res->sfty_ce_irq;\n\tpriv->sfty_ue_irq = res->sfty_ue_irq;\n\tfor (i = 0; i < MTL_MAX_RX_QUEUES; i++)\n\t\tpriv->rx_irq[i] = res->rx_irq[i];\n\tfor (i = 0; i < MTL_MAX_TX_QUEUES; i++)\n\t\tpriv->tx_irq[i] = res->tx_irq[i];\n\n\tif (!is_zero_ether_addr(res->mac))\n\t\teth_hw_addr_set(priv->dev, res->mac);\n\n\tdev_set_drvdata(device, priv->dev);\n\n\t \n\tstmmac_verify_args();\n\n\tpriv->af_xdp_zc_qps = bitmap_zalloc(MTL_MAX_TX_QUEUES, GFP_KERNEL);\n\tif (!priv->af_xdp_zc_qps)\n\t\treturn -ENOMEM;\n\n\t \n\tpriv->wq = create_singlethread_workqueue(\"stmmac_wq\");\n\tif (!priv->wq) {\n\t\tdev_err(priv->device, \"failed to create workqueue\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto error_wq_init;\n\t}\n\n\tINIT_WORK(&priv->service_task, stmmac_service_task);\n\n\t \n\tINIT_WORK(&priv->fpe_task, stmmac_fpe_lp_task);\n\n\t \n\tif ((phyaddr >= 0) && (phyaddr <= 31))\n\t\tpriv->plat->phy_addr = phyaddr;\n\n\tif (priv->plat->stmmac_rst) {\n\t\tret = reset_control_assert(priv->plat->stmmac_rst);\n\t\treset_control_deassert(priv->plat->stmmac_rst);\n\t\t \n\t\tif (ret == -ENOTSUPP)\n\t\t\treset_control_reset(priv->plat->stmmac_rst);\n\t}\n\n\tret = reset_control_deassert(priv->plat->stmmac_ahb_rst);\n\tif (ret == -ENOTSUPP)\n\t\tdev_err(priv->device, \"unable to bring out of ahb reset: %pe\\n\",\n\t\t\tERR_PTR(ret));\n\n\t \n\tret = stmmac_hw_init(priv);\n\tif (ret)\n\t\tgoto error_hw_init;\n\n\t \n\tif (priv->synopsys_id < DWMAC_CORE_5_20)\n\t\tpriv->plat->dma_cfg->dche = false;\n\n\tstmmac_check_ether_addr(priv);\n\n\tndev->netdev_ops = &stmmac_netdev_ops;\n\n\tndev->xdp_metadata_ops = &stmmac_xdp_metadata_ops;\n\n\tndev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\t    NETIF_F_RXCSUM;\n\tndev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |\n\t\t\t     NETDEV_XDP_ACT_XSK_ZEROCOPY;\n\n\tret = stmmac_tc_init(priv, priv);\n\tif (!ret) {\n\t\tndev->hw_features |= NETIF_F_HW_TC;\n\t}\n\n\tif ((priv->plat->flags & STMMAC_FLAG_TSO_EN) && (priv->dma_cap.tsoen)) {\n\t\tndev->hw_features |= NETIF_F_TSO | NETIF_F_TSO6;\n\t\tif (priv->plat->has_gmac4)\n\t\t\tndev->hw_features |= NETIF_F_GSO_UDP_L4;\n\t\tpriv->tso = true;\n\t\tdev_info(priv->device, \"TSO feature enabled\\n\");\n\t}\n\n\tif (priv->dma_cap.sphen &&\n\t    !(priv->plat->flags & STMMAC_FLAG_SPH_DISABLE)) {\n\t\tndev->hw_features |= NETIF_F_GRO;\n\t\tpriv->sph_cap = true;\n\t\tpriv->sph = priv->sph_cap;\n\t\tdev_info(priv->device, \"SPH feature enabled\\n\");\n\t}\n\n\t \n\tif (priv->plat->host_dma_width)\n\t\tpriv->dma_cap.host_dma_width = priv->plat->host_dma_width;\n\telse\n\t\tpriv->dma_cap.host_dma_width = priv->dma_cap.addr64;\n\n\tif (priv->dma_cap.host_dma_width) {\n\t\tret = dma_set_mask_and_coherent(device,\n\t\t\t\tDMA_BIT_MASK(priv->dma_cap.host_dma_width));\n\t\tif (!ret) {\n\t\t\tdev_info(priv->device, \"Using %d/%d bits DMA host/device width\\n\",\n\t\t\t\t priv->dma_cap.host_dma_width, priv->dma_cap.addr64);\n\n\t\t\t \n\t\t\tif (IS_ENABLED(CONFIG_ARCH_DMA_ADDR_T_64BIT))\n\t\t\t\tpriv->plat->dma_cfg->eame = true;\n\t\t} else {\n\t\t\tret = dma_set_mask_and_coherent(device, DMA_BIT_MASK(32));\n\t\t\tif (ret) {\n\t\t\t\tdev_err(priv->device, \"Failed to set DMA Mask\\n\");\n\t\t\t\tgoto error_hw_init;\n\t\t\t}\n\n\t\t\tpriv->dma_cap.host_dma_width = 32;\n\t\t}\n\t}\n\n\tndev->features |= ndev->hw_features | NETIF_F_HIGHDMA;\n\tndev->watchdog_timeo = msecs_to_jiffies(watchdog);\n#ifdef STMMAC_VLAN_TAG_USED\n\t \n\tndev->features |= NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_STAG_RX;\n\tif (priv->dma_cap.vlhash) {\n\t\tndev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;\n\t\tndev->features |= NETIF_F_HW_VLAN_STAG_FILTER;\n\t}\n\tif (priv->dma_cap.vlins) {\n\t\tndev->features |= NETIF_F_HW_VLAN_CTAG_TX;\n\t\tif (priv->dma_cap.dvlan)\n\t\t\tndev->features |= NETIF_F_HW_VLAN_STAG_TX;\n\t}\n#endif\n\tpriv->msg_enable = netif_msg_init(debug, default_msg_level);\n\n\tpriv->xstats.threshold = tc;\n\n\t \n\trxq = priv->plat->rx_queues_to_use;\n\tnetdev_rss_key_fill(priv->rss.key, sizeof(priv->rss.key));\n\tfor (i = 0; i < ARRAY_SIZE(priv->rss.table); i++)\n\t\tpriv->rss.table[i] = ethtool_rxfh_indir_default(i, rxq);\n\n\tif (priv->dma_cap.rssen && priv->plat->rss_en)\n\t\tndev->features |= NETIF_F_RXHASH;\n\n\tndev->vlan_features |= ndev->features;\n\t \n\tndev->vlan_features &= ~NETIF_F_TSO;\n\n\t \n\tndev->min_mtu = ETH_ZLEN - ETH_HLEN;\n\tif (priv->plat->has_xgmac)\n\t\tndev->max_mtu = XGMAC_JUMBO_LEN;\n\telse if ((priv->plat->enh_desc) || (priv->synopsys_id >= DWMAC_CORE_4_00))\n\t\tndev->max_mtu = JUMBO_LEN;\n\telse\n\t\tndev->max_mtu = SKB_MAX_HEAD(NET_SKB_PAD + NET_IP_ALIGN);\n\t \n\tif ((priv->plat->maxmtu < ndev->max_mtu) &&\n\t    (priv->plat->maxmtu >= ndev->min_mtu))\n\t\tndev->max_mtu = priv->plat->maxmtu;\n\telse if (priv->plat->maxmtu < ndev->min_mtu)\n\t\tdev_warn(priv->device,\n\t\t\t \"%s: warning: maxmtu having invalid value (%d)\\n\",\n\t\t\t __func__, priv->plat->maxmtu);\n\n\tif (flow_ctrl)\n\t\tpriv->flow_ctrl = FLOW_AUTO;\t \n\n\tndev->priv_flags |= IFF_LIVE_ADDR_CHANGE;\n\n\t \n\tstmmac_napi_add(ndev);\n\n\tmutex_init(&priv->lock);\n\n\t \n\tif (priv->plat->clk_csr >= 0)\n\t\tpriv->clk_csr = priv->plat->clk_csr;\n\telse\n\t\tstmmac_clk_csr_set(priv);\n\n\tstmmac_check_pcs_mode(priv);\n\n\tpm_runtime_get_noresume(device);\n\tpm_runtime_set_active(device);\n\tif (!pm_runtime_enabled(device))\n\t\tpm_runtime_enable(device);\n\n\tif (priv->hw->pcs != STMMAC_PCS_TBI &&\n\t    priv->hw->pcs != STMMAC_PCS_RTBI) {\n\t\t \n\t\tret = stmmac_mdio_register(ndev);\n\t\tif (ret < 0) {\n\t\t\tdev_err_probe(priv->device, ret,\n\t\t\t\t      \"%s: MDIO bus (id: %d) registration failed\\n\",\n\t\t\t\t      __func__, priv->plat->bus_id);\n\t\t\tgoto error_mdio_register;\n\t\t}\n\t}\n\n\tif (priv->plat->speed_mode_2500)\n\t\tpriv->plat->speed_mode_2500(ndev, priv->plat->bsp_priv);\n\n\tif (priv->plat->mdio_bus_data && priv->plat->mdio_bus_data->has_xpcs) {\n\t\tret = stmmac_xpcs_setup(priv->mii);\n\t\tif (ret)\n\t\t\tgoto error_xpcs_setup;\n\t}\n\n\tret = stmmac_phy_setup(priv);\n\tif (ret) {\n\t\tnetdev_err(ndev, \"failed to setup phy (%d)\\n\", ret);\n\t\tgoto error_phy_setup;\n\t}\n\n\tret = register_netdev(ndev);\n\tif (ret) {\n\t\tdev_err(priv->device, \"%s: ERROR %i registering the device\\n\",\n\t\t\t__func__, ret);\n\t\tgoto error_netdev_register;\n\t}\n\n#ifdef CONFIG_DEBUG_FS\n\tstmmac_init_fs(ndev);\n#endif\n\n\tif (priv->plat->dump_debug_regs)\n\t\tpriv->plat->dump_debug_regs(priv->plat->bsp_priv);\n\n\t \n\tpm_runtime_put(device);\n\n\treturn ret;\n\nerror_netdev_register:\n\tphylink_destroy(priv->phylink);\nerror_xpcs_setup:\nerror_phy_setup:\n\tif (priv->hw->pcs != STMMAC_PCS_TBI &&\n\t    priv->hw->pcs != STMMAC_PCS_RTBI)\n\t\tstmmac_mdio_unregister(ndev);\nerror_mdio_register:\n\tstmmac_napi_del(ndev);\nerror_hw_init:\n\tdestroy_workqueue(priv->wq);\nerror_wq_init:\n\tbitmap_free(priv->af_xdp_zc_qps);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(stmmac_dvr_probe);\n\n \nvoid stmmac_dvr_remove(struct device *dev)\n{\n\tstruct net_device *ndev = dev_get_drvdata(dev);\n\tstruct stmmac_priv *priv = netdev_priv(ndev);\n\n\tnetdev_info(priv->dev, \"%s: removing driver\", __func__);\n\n\tpm_runtime_get_sync(dev);\n\n\tstmmac_stop_all_dma(priv);\n\tstmmac_mac_set(priv, priv->ioaddr, false);\n\tnetif_carrier_off(ndev);\n\tunregister_netdev(ndev);\n\n#ifdef CONFIG_DEBUG_FS\n\tstmmac_exit_fs(ndev);\n#endif\n\tphylink_destroy(priv->phylink);\n\tif (priv->plat->stmmac_rst)\n\t\treset_control_assert(priv->plat->stmmac_rst);\n\treset_control_assert(priv->plat->stmmac_ahb_rst);\n\tif (priv->hw->pcs != STMMAC_PCS_TBI &&\n\t    priv->hw->pcs != STMMAC_PCS_RTBI)\n\t\tstmmac_mdio_unregister(ndev);\n\tdestroy_workqueue(priv->wq);\n\tmutex_destroy(&priv->lock);\n\tbitmap_free(priv->af_xdp_zc_qps);\n\n\tpm_runtime_disable(dev);\n\tpm_runtime_put_noidle(dev);\n}\nEXPORT_SYMBOL_GPL(stmmac_dvr_remove);\n\n \nint stmmac_suspend(struct device *dev)\n{\n\tstruct net_device *ndev = dev_get_drvdata(dev);\n\tstruct stmmac_priv *priv = netdev_priv(ndev);\n\tu32 chan;\n\n\tif (!ndev || !netif_running(ndev))\n\t\treturn 0;\n\n\tmutex_lock(&priv->lock);\n\n\tnetif_device_detach(ndev);\n\n\tstmmac_disable_all_queues(priv);\n\n\tfor (chan = 0; chan < priv->plat->tx_queues_to_use; chan++)\n\t\thrtimer_cancel(&priv->dma_conf.tx_queue[chan].txtimer);\n\n\tif (priv->eee_enabled) {\n\t\tpriv->tx_path_in_lpi_mode = false;\n\t\tdel_timer_sync(&priv->eee_ctrl_timer);\n\t}\n\n\t \n\tstmmac_stop_all_dma(priv);\n\n\tif (priv->plat->serdes_powerdown)\n\t\tpriv->plat->serdes_powerdown(ndev, priv->plat->bsp_priv);\n\n\t \n\tif (device_may_wakeup(priv->device) && priv->plat->pmt) {\n\t\tstmmac_pmt(priv, priv->hw, priv->wolopts);\n\t\tpriv->irq_wake = 1;\n\t} else {\n\t\tstmmac_mac_set(priv, priv->ioaddr, false);\n\t\tpinctrl_pm_select_sleep_state(priv->device);\n\t}\n\n\tmutex_unlock(&priv->lock);\n\n\trtnl_lock();\n\tif (device_may_wakeup(priv->device) && priv->plat->pmt) {\n\t\tphylink_suspend(priv->phylink, true);\n\t} else {\n\t\tif (device_may_wakeup(priv->device))\n\t\t\tphylink_speed_down(priv->phylink, false);\n\t\tphylink_suspend(priv->phylink, false);\n\t}\n\trtnl_unlock();\n\n\tif (priv->dma_cap.fpesel) {\n\t\t \n\t\tstmmac_fpe_configure(priv, priv->ioaddr,\n\t\t\t\t     priv->plat->fpe_cfg,\n\t\t\t\t     priv->plat->tx_queues_to_use,\n\t\t\t\t     priv->plat->rx_queues_to_use, false);\n\n\t\tstmmac_fpe_handshake(priv, false);\n\t\tstmmac_fpe_stop_wq(priv);\n\t}\n\n\tpriv->speed = SPEED_UNKNOWN;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(stmmac_suspend);\n\nstatic void stmmac_reset_rx_queue(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_rx_queue *rx_q = &priv->dma_conf.rx_queue[queue];\n\n\trx_q->cur_rx = 0;\n\trx_q->dirty_rx = 0;\n}\n\nstatic void stmmac_reset_tx_queue(struct stmmac_priv *priv, u32 queue)\n{\n\tstruct stmmac_tx_queue *tx_q = &priv->dma_conf.tx_queue[queue];\n\n\ttx_q->cur_tx = 0;\n\ttx_q->dirty_tx = 0;\n\ttx_q->mss = 0;\n\n\tnetdev_tx_reset_queue(netdev_get_tx_queue(priv->dev, queue));\n}\n\n \nstatic void stmmac_reset_queues_param(struct stmmac_priv *priv)\n{\n\tu32 rx_cnt = priv->plat->rx_queues_to_use;\n\tu32 tx_cnt = priv->plat->tx_queues_to_use;\n\tu32 queue;\n\n\tfor (queue = 0; queue < rx_cnt; queue++)\n\t\tstmmac_reset_rx_queue(priv, queue);\n\n\tfor (queue = 0; queue < tx_cnt; queue++)\n\t\tstmmac_reset_tx_queue(priv, queue);\n}\n\n \nint stmmac_resume(struct device *dev)\n{\n\tstruct net_device *ndev = dev_get_drvdata(dev);\n\tstruct stmmac_priv *priv = netdev_priv(ndev);\n\tint ret;\n\n\tif (!netif_running(ndev))\n\t\treturn 0;\n\n\t \n\tif (device_may_wakeup(priv->device) && priv->plat->pmt) {\n\t\tmutex_lock(&priv->lock);\n\t\tstmmac_pmt(priv, priv->hw, 0);\n\t\tmutex_unlock(&priv->lock);\n\t\tpriv->irq_wake = 0;\n\t} else {\n\t\tpinctrl_pm_select_default_state(priv->device);\n\t\t \n\t\tif (priv->mii)\n\t\t\tstmmac_mdio_reset(priv->mii);\n\t}\n\n\tif (!(priv->plat->flags & STMMAC_FLAG_SERDES_UP_AFTER_PHY_LINKUP) &&\n\t    priv->plat->serdes_powerup) {\n\t\tret = priv->plat->serdes_powerup(ndev,\n\t\t\t\t\t\t priv->plat->bsp_priv);\n\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\trtnl_lock();\n\tif (device_may_wakeup(priv->device) && priv->plat->pmt) {\n\t\tphylink_resume(priv->phylink);\n\t} else {\n\t\tphylink_resume(priv->phylink);\n\t\tif (device_may_wakeup(priv->device))\n\t\t\tphylink_speed_up(priv->phylink);\n\t}\n\trtnl_unlock();\n\n\trtnl_lock();\n\tmutex_lock(&priv->lock);\n\n\tstmmac_reset_queues_param(priv);\n\n\tstmmac_free_tx_skbufs(priv);\n\tstmmac_clear_descriptors(priv, &priv->dma_conf);\n\n\tstmmac_hw_setup(ndev, false);\n\tstmmac_init_coalesce(priv);\n\tstmmac_set_rx_mode(ndev);\n\n\tstmmac_restore_hw_vlan_rx_fltr(priv, ndev, priv->hw);\n\n\tstmmac_enable_all_queues(priv);\n\tstmmac_enable_all_dma_irq(priv);\n\n\tmutex_unlock(&priv->lock);\n\trtnl_unlock();\n\n\tnetif_device_attach(ndev);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(stmmac_resume);\n\n#ifndef MODULE\nstatic int __init stmmac_cmdline_opt(char *str)\n{\n\tchar *opt;\n\n\tif (!str || !*str)\n\t\treturn 1;\n\twhile ((opt = strsep(&str, \",\")) != NULL) {\n\t\tif (!strncmp(opt, \"debug:\", 6)) {\n\t\t\tif (kstrtoint(opt + 6, 0, &debug))\n\t\t\t\tgoto err;\n\t\t} else if (!strncmp(opt, \"phyaddr:\", 8)) {\n\t\t\tif (kstrtoint(opt + 8, 0, &phyaddr))\n\t\t\t\tgoto err;\n\t\t} else if (!strncmp(opt, \"buf_sz:\", 7)) {\n\t\t\tif (kstrtoint(opt + 7, 0, &buf_sz))\n\t\t\t\tgoto err;\n\t\t} else if (!strncmp(opt, \"tc:\", 3)) {\n\t\t\tif (kstrtoint(opt + 3, 0, &tc))\n\t\t\t\tgoto err;\n\t\t} else if (!strncmp(opt, \"watchdog:\", 9)) {\n\t\t\tif (kstrtoint(opt + 9, 0, &watchdog))\n\t\t\t\tgoto err;\n\t\t} else if (!strncmp(opt, \"flow_ctrl:\", 10)) {\n\t\t\tif (kstrtoint(opt + 10, 0, &flow_ctrl))\n\t\t\t\tgoto err;\n\t\t} else if (!strncmp(opt, \"pause:\", 6)) {\n\t\t\tif (kstrtoint(opt + 6, 0, &pause))\n\t\t\t\tgoto err;\n\t\t} else if (!strncmp(opt, \"eee_timer:\", 10)) {\n\t\t\tif (kstrtoint(opt + 10, 0, &eee_timer))\n\t\t\t\tgoto err;\n\t\t} else if (!strncmp(opt, \"chain_mode:\", 11)) {\n\t\t\tif (kstrtoint(opt + 11, 0, &chain_mode))\n\t\t\t\tgoto err;\n\t\t}\n\t}\n\treturn 1;\n\nerr:\n\tpr_err(\"%s: ERROR broken module parameter conversion\", __func__);\n\treturn 1;\n}\n\n__setup(\"stmmaceth=\", stmmac_cmdline_opt);\n#endif  \n\nstatic int __init stmmac_init(void)\n{\n#ifdef CONFIG_DEBUG_FS\n\t \n\tif (!stmmac_fs_dir)\n\t\tstmmac_fs_dir = debugfs_create_dir(STMMAC_RESOURCE_NAME, NULL);\n\tregister_netdevice_notifier(&stmmac_notifier);\n#endif\n\n\treturn 0;\n}\n\nstatic void __exit stmmac_exit(void)\n{\n#ifdef CONFIG_DEBUG_FS\n\tunregister_netdevice_notifier(&stmmac_notifier);\n\tdebugfs_remove_recursive(stmmac_fs_dir);\n#endif\n}\n\nmodule_init(stmmac_init)\nmodule_exit(stmmac_exit)\n\nMODULE_DESCRIPTION(\"STMMAC 10/100/1000 Ethernet device driver\");\nMODULE_AUTHOR(\"Giuseppe Cavallaro <peppe.cavallaro@st.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}