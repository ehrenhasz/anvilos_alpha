{
  "module_name": "lantiq_xrx200.c",
  "hash_id": "73acfc4caae3abdb549e6ae0ce679c43eacb058a14ac0b654d3232615bc36fe1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/lantiq_xrx200.c",
  "human_readable_source": "\n \n\n#include <linux/etherdevice.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/interrupt.h>\n#include <linux/clk.h>\n#include <linux/delay.h>\n\n#include <linux/if_vlan.h>\n\n#include <linux/of_net.h>\n#include <linux/of_platform.h>\n\n#include <xway_dma.h>\n\n \n#define XRX200_DMA_DATA_LEN\t(SZ_64K - 1)\n#define XRX200_DMA_RX\t\t0\n#define XRX200_DMA_TX\t\t1\n#define XRX200_DMA_BURST_LEN\t8\n\n#define XRX200_DMA_PACKET_COMPLETE\t0\n#define XRX200_DMA_PACKET_IN_PROGRESS\t1\n\n \n#define PMAC_RX_IPG\t\t0x0024\n#define PMAC_RX_IPG_MASK\t0xf\n\n#define PMAC_HD_CTL\t\t0x0000\n \n#define PMAC_HD_CTL_ADD\t\tBIT(0)\n \n#define PMAC_HD_CTL_TAG\t\tBIT(1)\n \n#define PMAC_HD_CTL_AC\t\tBIT(2)\n \n#define PMAC_HD_CTL_AS\t\tBIT(3)\n \n#define PMAC_HD_CTL_RC\t\tBIT(4)\n \n#define PMAC_HD_CTL_RL2\t\tBIT(5)\n \n#define PMAC_HD_CTL_RXSH\tBIT(6)\n \n#define PMAC_HD_CTL_AST\t\tBIT(7)\n \n#define PMAC_HD_CTL_RST\t\tBIT(8)\n \n#define PMAC_HD_CTL_CCRC\tBIT(9)\n \n#define PMAC_HD_CTL_FC\t\tBIT(10)\n\nstruct xrx200_chan {\n\tint tx_free;\n\n\tstruct napi_struct napi;\n\tstruct ltq_dma_channel dma;\n\n\tunion {\n\t\tstruct sk_buff *skb[LTQ_DESC_NUM];\n\t\tvoid *rx_buff[LTQ_DESC_NUM];\n\t};\n\n\tstruct sk_buff *skb_head;\n\tstruct sk_buff *skb_tail;\n\n\tstruct xrx200_priv *priv;\n};\n\nstruct xrx200_priv {\n\tstruct clk *clk;\n\n\tstruct xrx200_chan chan_tx;\n\tstruct xrx200_chan chan_rx;\n\n\tu16 rx_buf_size;\n\tu16 rx_skb_size;\n\n\tstruct net_device *net_dev;\n\tstruct device *dev;\n\n\t__iomem void *pmac_reg;\n};\n\nstatic u32 xrx200_pmac_r32(struct xrx200_priv *priv, u32 offset)\n{\n\treturn __raw_readl(priv->pmac_reg + offset);\n}\n\nstatic void xrx200_pmac_w32(struct xrx200_priv *priv, u32 val, u32 offset)\n{\n\t__raw_writel(val, priv->pmac_reg + offset);\n}\n\nstatic void xrx200_pmac_mask(struct xrx200_priv *priv, u32 clear, u32 set,\n\t\t\t     u32 offset)\n{\n\tu32 val = xrx200_pmac_r32(priv, offset);\n\n\tval &= ~(clear);\n\tval |= set;\n\txrx200_pmac_w32(priv, val, offset);\n}\n\nstatic int xrx200_max_frame_len(int mtu)\n{\n\treturn VLAN_ETH_HLEN + mtu;\n}\n\nstatic int xrx200_buffer_size(int mtu)\n{\n\treturn round_up(xrx200_max_frame_len(mtu), 4 * XRX200_DMA_BURST_LEN);\n}\n\nstatic int xrx200_skb_size(u16 buf_size)\n{\n\treturn SKB_DATA_ALIGN(buf_size + NET_SKB_PAD + NET_IP_ALIGN) +\n\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n}\n\n \nstatic void xrx200_flush_dma(struct xrx200_chan *ch)\n{\n\tint i;\n\n\tfor (i = 0; i < LTQ_DESC_NUM; i++) {\n\t\tstruct ltq_dma_desc *desc = &ch->dma.desc_base[ch->dma.desc];\n\n\t\tif ((desc->ctl & (LTQ_DMA_OWN | LTQ_DMA_C)) != LTQ_DMA_C)\n\t\t\tbreak;\n\n\t\tdesc->ctl = LTQ_DMA_OWN | LTQ_DMA_RX_OFFSET(NET_IP_ALIGN) |\n\t\t\t    ch->priv->rx_buf_size;\n\t\tch->dma.desc++;\n\t\tch->dma.desc %= LTQ_DESC_NUM;\n\t}\n}\n\nstatic int xrx200_open(struct net_device *net_dev)\n{\n\tstruct xrx200_priv *priv = netdev_priv(net_dev);\n\n\tnapi_enable(&priv->chan_tx.napi);\n\tltq_dma_open(&priv->chan_tx.dma);\n\tltq_dma_enable_irq(&priv->chan_tx.dma);\n\n\tnapi_enable(&priv->chan_rx.napi);\n\tltq_dma_open(&priv->chan_rx.dma);\n\t \n\tusleep_range(20, 40);\n\txrx200_flush_dma(&priv->chan_rx);\n\tltq_dma_enable_irq(&priv->chan_rx.dma);\n\n\tnetif_wake_queue(net_dev);\n\n\treturn 0;\n}\n\nstatic int xrx200_close(struct net_device *net_dev)\n{\n\tstruct xrx200_priv *priv = netdev_priv(net_dev);\n\n\tnetif_stop_queue(net_dev);\n\n\tnapi_disable(&priv->chan_rx.napi);\n\tltq_dma_close(&priv->chan_rx.dma);\n\n\tnapi_disable(&priv->chan_tx.napi);\n\tltq_dma_close(&priv->chan_tx.dma);\n\n\treturn 0;\n}\n\nstatic int xrx200_alloc_buf(struct xrx200_chan *ch, void *(*alloc)(unsigned int size))\n{\n\tvoid *buf = ch->rx_buff[ch->dma.desc];\n\tstruct xrx200_priv *priv = ch->priv;\n\tdma_addr_t mapping;\n\tint ret = 0;\n\n\tch->rx_buff[ch->dma.desc] = alloc(priv->rx_skb_size);\n\tif (!ch->rx_buff[ch->dma.desc]) {\n\t\tch->rx_buff[ch->dma.desc] = buf;\n\t\tret = -ENOMEM;\n\t\tgoto skip;\n\t}\n\n\tmapping = dma_map_single(priv->dev, ch->rx_buff[ch->dma.desc],\n\t\t\t\t priv->rx_buf_size, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(priv->dev, mapping))) {\n\t\tskb_free_frag(ch->rx_buff[ch->dma.desc]);\n\t\tch->rx_buff[ch->dma.desc] = buf;\n\t\tret = -ENOMEM;\n\t\tgoto skip;\n\t}\n\n\tch->dma.desc_base[ch->dma.desc].addr = mapping + NET_SKB_PAD + NET_IP_ALIGN;\n\t \n\twmb();\nskip:\n\tch->dma.desc_base[ch->dma.desc].ctl =\n\t\tLTQ_DMA_OWN | LTQ_DMA_RX_OFFSET(NET_IP_ALIGN) | priv->rx_buf_size;\n\n\treturn ret;\n}\n\nstatic int xrx200_hw_receive(struct xrx200_chan *ch)\n{\n\tstruct xrx200_priv *priv = ch->priv;\n\tstruct ltq_dma_desc *desc = &ch->dma.desc_base[ch->dma.desc];\n\tvoid *buf = ch->rx_buff[ch->dma.desc];\n\tu32 ctl = desc->ctl;\n\tint len = (ctl & LTQ_DMA_SIZE_MASK);\n\tstruct net_device *net_dev = priv->net_dev;\n\tstruct sk_buff *skb;\n\tint ret;\n\n\tret = xrx200_alloc_buf(ch, napi_alloc_frag);\n\n\tch->dma.desc++;\n\tch->dma.desc %= LTQ_DESC_NUM;\n\n\tif (ret) {\n\t\tnet_dev->stats.rx_dropped++;\n\t\tnetdev_err(net_dev, \"failed to allocate new rx buffer\\n\");\n\t\treturn ret;\n\t}\n\n\tskb = build_skb(buf, priv->rx_skb_size);\n\tif (!skb) {\n\t\tskb_free_frag(buf);\n\t\tnet_dev->stats.rx_dropped++;\n\t\treturn -ENOMEM;\n\t}\n\n\tskb_reserve(skb, NET_SKB_PAD);\n\tskb_put(skb, len);\n\n\t \n\tif (ctl & LTQ_DMA_SOP) {\n\t\tch->skb_head = skb;\n\t\tch->skb_tail = skb;\n\t\tskb_reserve(skb, NET_IP_ALIGN);\n\t} else if (ch->skb_head) {\n\t\tif (ch->skb_head == ch->skb_tail)\n\t\t\tskb_shinfo(ch->skb_tail)->frag_list = skb;\n\t\telse\n\t\t\tch->skb_tail->next = skb;\n\t\tch->skb_tail = skb;\n\t\tch->skb_head->len += skb->len;\n\t\tch->skb_head->data_len += skb->len;\n\t\tch->skb_head->truesize += skb->truesize;\n\t}\n\n\tif (ctl & LTQ_DMA_EOP) {\n\t\tch->skb_head->protocol = eth_type_trans(ch->skb_head, net_dev);\n\t\tnet_dev->stats.rx_packets++;\n\t\tnet_dev->stats.rx_bytes += ch->skb_head->len;\n\t\tnetif_receive_skb(ch->skb_head);\n\t\tch->skb_head = NULL;\n\t\tch->skb_tail = NULL;\n\t\tret = XRX200_DMA_PACKET_COMPLETE;\n\t} else {\n\t\tret = XRX200_DMA_PACKET_IN_PROGRESS;\n\t}\n\n\treturn ret;\n}\n\nstatic int xrx200_poll_rx(struct napi_struct *napi, int budget)\n{\n\tstruct xrx200_chan *ch = container_of(napi,\n\t\t\t\tstruct xrx200_chan, napi);\n\tint rx = 0;\n\tint ret;\n\n\twhile (rx < budget) {\n\t\tstruct ltq_dma_desc *desc = &ch->dma.desc_base[ch->dma.desc];\n\n\t\tif ((desc->ctl & (LTQ_DMA_OWN | LTQ_DMA_C)) == LTQ_DMA_C) {\n\t\t\tret = xrx200_hw_receive(ch);\n\t\t\tif (ret == XRX200_DMA_PACKET_IN_PROGRESS)\n\t\t\t\tcontinue;\n\t\t\tif (ret != XRX200_DMA_PACKET_COMPLETE)\n\t\t\t\tbreak;\n\t\t\trx++;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (rx < budget) {\n\t\tif (napi_complete_done(&ch->napi, rx))\n\t\t\tltq_dma_enable_irq(&ch->dma);\n\t}\n\n\treturn rx;\n}\n\nstatic int xrx200_tx_housekeeping(struct napi_struct *napi, int budget)\n{\n\tstruct xrx200_chan *ch = container_of(napi,\n\t\t\t\tstruct xrx200_chan, napi);\n\tstruct net_device *net_dev = ch->priv->net_dev;\n\tint pkts = 0;\n\tint bytes = 0;\n\n\tnetif_tx_lock(net_dev);\n\twhile (pkts < budget) {\n\t\tstruct ltq_dma_desc *desc = &ch->dma.desc_base[ch->tx_free];\n\n\t\tif ((desc->ctl & (LTQ_DMA_OWN | LTQ_DMA_C)) == LTQ_DMA_C) {\n\t\t\tstruct sk_buff *skb = ch->skb[ch->tx_free];\n\n\t\t\tpkts++;\n\t\t\tbytes += skb->len;\n\t\t\tch->skb[ch->tx_free] = NULL;\n\t\t\tconsume_skb(skb);\n\t\t\tmemset(&ch->dma.desc_base[ch->tx_free], 0,\n\t\t\t       sizeof(struct ltq_dma_desc));\n\t\t\tch->tx_free++;\n\t\t\tch->tx_free %= LTQ_DESC_NUM;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tnet_dev->stats.tx_packets += pkts;\n\tnet_dev->stats.tx_bytes += bytes;\n\tnetdev_completed_queue(ch->priv->net_dev, pkts, bytes);\n\n\tnetif_tx_unlock(net_dev);\n\tif (netif_queue_stopped(net_dev))\n\t\tnetif_wake_queue(net_dev);\n\n\tif (pkts < budget) {\n\t\tif (napi_complete_done(&ch->napi, pkts))\n\t\t\tltq_dma_enable_irq(&ch->dma);\n\t}\n\n\treturn pkts;\n}\n\nstatic netdev_tx_t xrx200_start_xmit(struct sk_buff *skb,\n\t\t\t\t     struct net_device *net_dev)\n{\n\tstruct xrx200_priv *priv = netdev_priv(net_dev);\n\tstruct xrx200_chan *ch = &priv->chan_tx;\n\tstruct ltq_dma_desc *desc = &ch->dma.desc_base[ch->dma.desc];\n\tu32 byte_offset;\n\tdma_addr_t mapping;\n\tint len;\n\n\tskb->dev = net_dev;\n\tif (skb_put_padto(skb, ETH_ZLEN)) {\n\t\tnet_dev->stats.tx_dropped++;\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tlen = skb->len;\n\n\tif ((desc->ctl & (LTQ_DMA_OWN | LTQ_DMA_C)) || ch->skb[ch->dma.desc]) {\n\t\tnetdev_err(net_dev, \"tx ring full\\n\");\n\t\tnetif_stop_queue(net_dev);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tch->skb[ch->dma.desc] = skb;\n\n\tmapping = dma_map_single(priv->dev, skb->data, len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(priv->dev, mapping)))\n\t\tgoto err_drop;\n\n\t \n\tbyte_offset = mapping % (XRX200_DMA_BURST_LEN * 4);\n\n\tdesc->addr = mapping - byte_offset;\n\t \n\twmb();\n\tdesc->ctl = LTQ_DMA_OWN | LTQ_DMA_SOP | LTQ_DMA_EOP |\n\t\tLTQ_DMA_TX_OFFSET(byte_offset) | (len & LTQ_DMA_SIZE_MASK);\n\tch->dma.desc++;\n\tch->dma.desc %= LTQ_DESC_NUM;\n\tif (ch->dma.desc == ch->tx_free)\n\t\tnetif_stop_queue(net_dev);\n\n\tnetdev_sent_queue(net_dev, len);\n\n\treturn NETDEV_TX_OK;\n\nerr_drop:\n\tdev_kfree_skb(skb);\n\tnet_dev->stats.tx_dropped++;\n\tnet_dev->stats.tx_errors++;\n\treturn NETDEV_TX_OK;\n}\n\nstatic int\nxrx200_change_mtu(struct net_device *net_dev, int new_mtu)\n{\n\tstruct xrx200_priv *priv = netdev_priv(net_dev);\n\tstruct xrx200_chan *ch_rx = &priv->chan_rx;\n\tint old_mtu = net_dev->mtu;\n\tbool running = false;\n\tvoid *buff;\n\tint curr_desc;\n\tint ret = 0;\n\n\tnet_dev->mtu = new_mtu;\n\tpriv->rx_buf_size = xrx200_buffer_size(new_mtu);\n\tpriv->rx_skb_size = xrx200_skb_size(priv->rx_buf_size);\n\n\tif (new_mtu <= old_mtu)\n\t\treturn ret;\n\n\trunning = netif_running(net_dev);\n\tif (running) {\n\t\tnapi_disable(&ch_rx->napi);\n\t\tltq_dma_close(&ch_rx->dma);\n\t}\n\n\txrx200_poll_rx(&ch_rx->napi, LTQ_DESC_NUM);\n\tcurr_desc = ch_rx->dma.desc;\n\n\tfor (ch_rx->dma.desc = 0; ch_rx->dma.desc < LTQ_DESC_NUM;\n\t     ch_rx->dma.desc++) {\n\t\tbuff = ch_rx->rx_buff[ch_rx->dma.desc];\n\t\tret = xrx200_alloc_buf(ch_rx, netdev_alloc_frag);\n\t\tif (ret) {\n\t\t\tnet_dev->mtu = old_mtu;\n\t\t\tpriv->rx_buf_size = xrx200_buffer_size(old_mtu);\n\t\t\tpriv->rx_skb_size = xrx200_skb_size(priv->rx_buf_size);\n\t\t\tbreak;\n\t\t}\n\t\tskb_free_frag(buff);\n\t}\n\n\tch_rx->dma.desc = curr_desc;\n\tif (running) {\n\t\tnapi_enable(&ch_rx->napi);\n\t\tltq_dma_open(&ch_rx->dma);\n\t\tltq_dma_enable_irq(&ch_rx->dma);\n\t}\n\n\treturn ret;\n}\n\nstatic const struct net_device_ops xrx200_netdev_ops = {\n\t.ndo_open\t\t= xrx200_open,\n\t.ndo_stop\t\t= xrx200_close,\n\t.ndo_start_xmit\t\t= xrx200_start_xmit,\n\t.ndo_change_mtu\t\t= xrx200_change_mtu,\n\t.ndo_set_mac_address\t= eth_mac_addr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n};\n\nstatic irqreturn_t xrx200_dma_irq(int irq, void *ptr)\n{\n\tstruct xrx200_chan *ch = ptr;\n\n\tif (napi_schedule_prep(&ch->napi)) {\n\t\tltq_dma_disable_irq(&ch->dma);\n\t\t__napi_schedule(&ch->napi);\n\t}\n\n\tltq_dma_ack_irq(&ch->dma);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int xrx200_dma_init(struct xrx200_priv *priv)\n{\n\tstruct xrx200_chan *ch_rx = &priv->chan_rx;\n\tstruct xrx200_chan *ch_tx = &priv->chan_tx;\n\tint ret = 0;\n\tint i;\n\n\tltq_dma_init_port(DMA_PORT_ETOP, XRX200_DMA_BURST_LEN,\n\t\t\t  XRX200_DMA_BURST_LEN);\n\n\tch_rx->dma.nr = XRX200_DMA_RX;\n\tch_rx->dma.dev = priv->dev;\n\tch_rx->priv = priv;\n\n\tltq_dma_alloc_rx(&ch_rx->dma);\n\tfor (ch_rx->dma.desc = 0; ch_rx->dma.desc < LTQ_DESC_NUM;\n\t     ch_rx->dma.desc++) {\n\t\tret = xrx200_alloc_buf(ch_rx, netdev_alloc_frag);\n\t\tif (ret)\n\t\t\tgoto rx_free;\n\t}\n\tch_rx->dma.desc = 0;\n\tret = devm_request_irq(priv->dev, ch_rx->dma.irq, xrx200_dma_irq, 0,\n\t\t\t       \"xrx200_net_rx\", &priv->chan_rx);\n\tif (ret) {\n\t\tdev_err(priv->dev, \"failed to request RX irq %d\\n\",\n\t\t\tch_rx->dma.irq);\n\t\tgoto rx_ring_free;\n\t}\n\n\tch_tx->dma.nr = XRX200_DMA_TX;\n\tch_tx->dma.dev = priv->dev;\n\tch_tx->priv = priv;\n\n\tltq_dma_alloc_tx(&ch_tx->dma);\n\tret = devm_request_irq(priv->dev, ch_tx->dma.irq, xrx200_dma_irq, 0,\n\t\t\t       \"xrx200_net_tx\", &priv->chan_tx);\n\tif (ret) {\n\t\tdev_err(priv->dev, \"failed to request TX irq %d\\n\",\n\t\t\tch_tx->dma.irq);\n\t\tgoto tx_free;\n\t}\n\n\treturn ret;\n\ntx_free:\n\tltq_dma_free(&ch_tx->dma);\n\nrx_ring_free:\n\t \n\tfor (i = 0; i < LTQ_DESC_NUM; i++) {\n\t\tif (priv->chan_rx.skb[i])\n\t\t\tskb_free_frag(priv->chan_rx.rx_buff[i]);\n\t}\n\nrx_free:\n\tltq_dma_free(&ch_rx->dma);\n\treturn ret;\n}\n\nstatic void xrx200_hw_cleanup(struct xrx200_priv *priv)\n{\n\tint i;\n\n\tltq_dma_free(&priv->chan_tx.dma);\n\tltq_dma_free(&priv->chan_rx.dma);\n\n\t \n\tfor (i = 0; i < LTQ_DESC_NUM; i++)\n\t\tskb_free_frag(priv->chan_rx.rx_buff[i]);\n}\n\nstatic int xrx200_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct device_node *np = dev->of_node;\n\tstruct xrx200_priv *priv;\n\tstruct net_device *net_dev;\n\tint err;\n\n\t \n\tnet_dev = devm_alloc_etherdev(dev, sizeof(struct xrx200_priv));\n\tif (!net_dev)\n\t\treturn -ENOMEM;\n\n\tpriv = netdev_priv(net_dev);\n\tpriv->net_dev = net_dev;\n\tpriv->dev = dev;\n\n\tnet_dev->netdev_ops = &xrx200_netdev_ops;\n\tSET_NETDEV_DEV(net_dev, dev);\n\tnet_dev->min_mtu = ETH_ZLEN;\n\tnet_dev->max_mtu = XRX200_DMA_DATA_LEN - xrx200_max_frame_len(0);\n\tpriv->rx_buf_size = xrx200_buffer_size(ETH_DATA_LEN);\n\tpriv->rx_skb_size = xrx200_skb_size(priv->rx_buf_size);\n\n\t \n\tpriv->pmac_reg = devm_platform_get_and_ioremap_resource(pdev, 0, NULL);\n\tif (IS_ERR(priv->pmac_reg))\n\t\treturn PTR_ERR(priv->pmac_reg);\n\n\tpriv->chan_rx.dma.irq = platform_get_irq_byname(pdev, \"rx\");\n\tif (priv->chan_rx.dma.irq < 0)\n\t\treturn -ENOENT;\n\tpriv->chan_tx.dma.irq = platform_get_irq_byname(pdev, \"tx\");\n\tif (priv->chan_tx.dma.irq < 0)\n\t\treturn -ENOENT;\n\n\t \n\tpriv->clk = devm_clk_get(dev, NULL);\n\tif (IS_ERR(priv->clk)) {\n\t\tdev_err(dev, \"failed to get clock\\n\");\n\t\treturn PTR_ERR(priv->clk);\n\t}\n\n\terr = of_get_ethdev_address(np, net_dev);\n\tif (err)\n\t\teth_hw_addr_random(net_dev);\n\n\t \n\terr = xrx200_dma_init(priv);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = clk_prepare_enable(priv->clk);\n\tif (err)\n\t\tgoto err_uninit_dma;\n\n\t \n\txrx200_pmac_mask(priv, PMAC_RX_IPG_MASK, 0xb, PMAC_RX_IPG);\n\n\t \n\txrx200_pmac_mask(priv, 0,\n\t\t\t PMAC_HD_CTL_RST | PMAC_HD_CTL_AST | PMAC_HD_CTL_RXSH |\n\t\t\t PMAC_HD_CTL_AS | PMAC_HD_CTL_AC | PMAC_HD_CTL_RC,\n\t\t\t PMAC_HD_CTL);\n\n\t \n\tnetif_napi_add(net_dev, &priv->chan_rx.napi, xrx200_poll_rx);\n\tnetif_napi_add_tx(net_dev, &priv->chan_tx.napi,\n\t\t\t  xrx200_tx_housekeeping);\n\n\tplatform_set_drvdata(pdev, priv);\n\n\terr = register_netdev(net_dev);\n\tif (err)\n\t\tgoto err_unprepare_clk;\n\n\treturn 0;\n\nerr_unprepare_clk:\n\tclk_disable_unprepare(priv->clk);\n\nerr_uninit_dma:\n\txrx200_hw_cleanup(priv);\n\n\treturn err;\n}\n\nstatic int xrx200_remove(struct platform_device *pdev)\n{\n\tstruct xrx200_priv *priv = platform_get_drvdata(pdev);\n\tstruct net_device *net_dev = priv->net_dev;\n\n\t \n\tnetif_stop_queue(net_dev);\n\tnetif_napi_del(&priv->chan_tx.napi);\n\tnetif_napi_del(&priv->chan_rx.napi);\n\n\t \n\tunregister_netdev(net_dev);\n\n\t \n\tclk_disable_unprepare(priv->clk);\n\n\t \n\txrx200_hw_cleanup(priv);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id xrx200_match[] = {\n\t{ .compatible = \"lantiq,xrx200-net\" },\n\t{},\n};\nMODULE_DEVICE_TABLE(of, xrx200_match);\n\nstatic struct platform_driver xrx200_driver = {\n\t.probe = xrx200_probe,\n\t.remove = xrx200_remove,\n\t.driver = {\n\t\t.name = \"lantiq,xrx200-net\",\n\t\t.of_match_table = xrx200_match,\n\t},\n};\n\nmodule_platform_driver(xrx200_driver);\n\nMODULE_AUTHOR(\"John Crispin <john@phrozen.org>\");\nMODULE_DESCRIPTION(\"Lantiq SoC XRX200 ethernet\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}