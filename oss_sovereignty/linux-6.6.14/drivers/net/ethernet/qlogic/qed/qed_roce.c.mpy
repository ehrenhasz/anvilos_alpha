{
  "module_name": "qed_roce.c",
  "hash_id": "d1a0885f76b69d89f3284655bf2e8ee5b23e3ab2a24a9338ecafba42054816fa",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/qlogic/qed/qed_roce.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <asm/byteorder.h>\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/errno.h>\n#include <linux/io.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/if_vlan.h>\n#include \"qed.h\"\n#include \"qed_cxt.h\"\n#include \"qed_dcbx.h\"\n#include \"qed_hsi.h\"\n#include \"qed_hw.h\"\n#include \"qed_init_ops.h\"\n#include \"qed_int.h\"\n#include \"qed_ll2.h\"\n#include \"qed_mcp.h\"\n#include \"qed_reg_addr.h\"\n#include <linux/qed/qed_rdma_if.h>\n#include \"qed_rdma.h\"\n#include \"qed_roce.h\"\n#include \"qed_sp.h\"\n\nstatic void qed_roce_free_real_icid(struct qed_hwfn *p_hwfn, u16 icid);\n\nstatic int qed_roce_async_event(struct qed_hwfn *p_hwfn, u8 fw_event_code,\n\t\t\t\t__le16 echo, union event_ring_data *data,\n\t\t\t\tu8 fw_return_code)\n{\n\tstruct qed_rdma_events events = p_hwfn->p_rdma_info->events;\n\tunion rdma_eqe_data *rdata = &data->rdma_data;\n\n\tif (fw_event_code == ROCE_ASYNC_EVENT_DESTROY_QP_DONE) {\n\t\tu16 icid = (u16)le32_to_cpu(rdata->rdma_destroy_qp_data.cid);\n\n\t\t \n\t\tqed_roce_free_real_icid(p_hwfn, icid);\n\t} else if (fw_event_code == ROCE_ASYNC_EVENT_SRQ_EMPTY ||\n\t\t   fw_event_code == ROCE_ASYNC_EVENT_SRQ_LIMIT) {\n\t\tu16 srq_id = (u16)le32_to_cpu(rdata->async_handle.lo);\n\n\t\tevents.affiliated_event(events.context, fw_event_code,\n\t\t\t\t\t&srq_id);\n\t} else {\n\t\tevents.affiliated_event(events.context, fw_event_code,\n\t\t\t\t\t(void *)&rdata->async_handle);\n\t}\n\n\treturn 0;\n}\n\nvoid qed_roce_stop(struct qed_hwfn *p_hwfn)\n{\n\tstruct qed_bmap *rcid_map = &p_hwfn->p_rdma_info->real_cid_map;\n\tint wait_count = 0;\n\n\t \n\twhile (!bitmap_empty(rcid_map->bitmap, rcid_map->max_count)) {\n\t\t \n\t\tif (p_hwfn->cdev->recov_in_prog)\n\t\t\treturn;\n\n\t\tmsleep(100);\n\t\tif (wait_count++ > 20) {\n\t\t\tDP_NOTICE(p_hwfn, \"cid bitmap wait timed out\\n\");\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void qed_rdma_copy_gids(struct qed_rdma_qp *qp, __le32 *src_gid,\n\t\t\t       __le32 *dst_gid)\n{\n\tu32 i;\n\n\tif (qp->roce_mode == ROCE_V2_IPV4) {\n\t\t \n\t\tmemset(src_gid, 0, sizeof(union qed_gid));\n\t\tmemset(dst_gid, 0, sizeof(union qed_gid));\n\t\tsrc_gid[3] = cpu_to_le32(qp->sgid.ipv4_addr);\n\t\tdst_gid[3] = cpu_to_le32(qp->dgid.ipv4_addr);\n\t} else {\n\t\t \n\t\tfor (i = 0; i < ARRAY_SIZE(qp->sgid.dwords); i++) {\n\t\t\tsrc_gid[i] = cpu_to_le32(qp->sgid.dwords[i]);\n\t\t\tdst_gid[i] = cpu_to_le32(qp->dgid.dwords[i]);\n\t\t}\n\t}\n}\n\nstatic enum roce_flavor qed_roce_mode_to_flavor(enum roce_mode roce_mode)\n{\n\tswitch (roce_mode) {\n\tcase ROCE_V1:\n\t\treturn PLAIN_ROCE;\n\tcase ROCE_V2_IPV4:\n\t\treturn RROCE_IPV4;\n\tcase ROCE_V2_IPV6:\n\t\treturn RROCE_IPV6;\n\tdefault:\n\t\treturn MAX_ROCE_FLAVOR;\n\t}\n}\n\nstatic void qed_roce_free_cid_pair(struct qed_hwfn *p_hwfn, u16 cid)\n{\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\tqed_bmap_release_id(p_hwfn, &p_hwfn->p_rdma_info->cid_map, cid);\n\tqed_bmap_release_id(p_hwfn, &p_hwfn->p_rdma_info->cid_map, cid + 1);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n}\n\nint qed_roce_alloc_cid(struct qed_hwfn *p_hwfn, u16 *cid)\n{\n\tstruct qed_rdma_info *p_rdma_info = p_hwfn->p_rdma_info;\n\tu32 responder_icid;\n\tu32 requester_icid;\n\tint rc;\n\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\trc = qed_rdma_bmap_alloc_id(p_hwfn, &p_rdma_info->cid_map,\n\t\t\t\t    &responder_icid);\n\tif (rc) {\n\t\tspin_unlock_bh(&p_rdma_info->lock);\n\t\treturn rc;\n\t}\n\n\trc = qed_rdma_bmap_alloc_id(p_hwfn, &p_rdma_info->cid_map,\n\t\t\t\t    &requester_icid);\n\n\tspin_unlock_bh(&p_rdma_info->lock);\n\tif (rc)\n\t\tgoto err;\n\n\t \n\tif ((requester_icid - responder_icid) != 1) {\n\t\tDP_NOTICE(p_hwfn, \"Failed to allocate two adjacent qp's'\\n\");\n\t\trc = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tresponder_icid += qed_cxt_get_proto_cid_start(p_hwfn,\n\t\t\t\t\t\t      p_rdma_info->proto);\n\trequester_icid += qed_cxt_get_proto_cid_start(p_hwfn,\n\t\t\t\t\t\t      p_rdma_info->proto);\n\n\t \n\trc = qed_cxt_dynamic_ilt_alloc(p_hwfn, QED_ELEM_CXT, responder_icid);\n\tif (rc)\n\t\tgoto err;\n\n\trc = qed_cxt_dynamic_ilt_alloc(p_hwfn, QED_ELEM_CXT, requester_icid);\n\tif (rc)\n\t\tgoto err;\n\n\t*cid = (u16)responder_icid;\n\treturn rc;\n\nerr:\n\tspin_lock_bh(&p_rdma_info->lock);\n\tqed_bmap_release_id(p_hwfn, &p_rdma_info->cid_map, responder_icid);\n\tqed_bmap_release_id(p_hwfn, &p_rdma_info->cid_map, requester_icid);\n\n\tspin_unlock_bh(&p_rdma_info->lock);\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t   \"Allocate CID - failed, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic void qed_roce_set_real_cid(struct qed_hwfn *p_hwfn, u32 cid)\n{\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\tqed_bmap_set_id(p_hwfn, &p_hwfn->p_rdma_info->real_cid_map, cid);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n}\n\nstatic u8 qed_roce_get_qp_tc(struct qed_hwfn *p_hwfn, struct qed_rdma_qp *qp)\n{\n\tu8 pri, tc = 0;\n\n\tif (qp->vlan_id) {\n\t\tpri = (qp->vlan_id & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;\n\t\ttc = qed_dcbx_get_priority_tc(p_hwfn, pri);\n\t}\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_SP,\n\t\t   \"qp icid %u tc: %u (vlan priority %s)\\n\",\n\t\t   qp->icid, tc, qp->vlan_id ? \"enabled\" : \"disabled\");\n\n\treturn tc;\n}\n\nstatic int qed_roce_sp_create_responder(struct qed_hwfn *p_hwfn,\n\t\t\t\t\tstruct qed_rdma_qp *qp)\n{\n\tstruct roce_create_qp_resp_ramrod_data *p_ramrod;\n\tu16 regular_latency_queue, low_latency_queue;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tenum protocol_type proto;\n\tu32 flags = 0;\n\tint rc;\n\tu8 tc;\n\n\tif (!qp->has_resp)\n\t\treturn 0;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", qp->icid);\n\n\t \n\tqp->irq_num_pages = 1;\n\tqp->irq = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t\t     RDMA_RING_PAGE_SIZE,\n\t\t\t\t     &qp->irq_phys_addr, GFP_KERNEL);\n\tif (!qp->irq) {\n\t\trc = -ENOMEM;\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"qed create responder failed: cannot allocate memory (irq). rc = %d\\n\",\n\t\t\t  rc);\n\t\treturn rc;\n\t}\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.cid = qp->icid;\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent, ROCE_RAMROD_CREATE_QP,\n\t\t\t\t PROTOCOLID_ROCE, &init_data);\n\tif (rc)\n\t\tgoto err;\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_ROCE_FLAVOR,\n\t\t  qed_roce_mode_to_flavor(qp->roce_mode));\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_RDMA_RD_EN,\n\t\t  qp->incoming_rdma_read_en);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_RDMA_WR_EN,\n\t\t  qp->incoming_rdma_write_en);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_ATOMIC_EN,\n\t\t  qp->incoming_atomic_en);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_E2E_FLOW_CONTROL_EN,\n\t\t  qp->e2e_flow_control_en);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_SRQ_FLG, qp->use_srq);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_RESERVED_KEY_EN,\n\t\t  qp->fmr_and_reserved_lkey);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_MIN_RNR_NAK_TIMER,\n\t\t  qp->min_rnr_nak_timer);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_XRC_FLAG,\n\t\t  qed_rdma_is_xrc_qp(qp));\n\n\tp_ramrod = &p_ent->ramrod.roce_create_qp_resp;\n\tp_ramrod->flags = cpu_to_le32(flags);\n\tp_ramrod->max_ird = qp->max_rd_atomic_resp;\n\tp_ramrod->traffic_class = qp->traffic_class_tos;\n\tp_ramrod->hop_limit = qp->hop_limit_ttl;\n\tp_ramrod->irq_num_pages = qp->irq_num_pages;\n\tp_ramrod->p_key = cpu_to_le16(qp->pkey);\n\tp_ramrod->flow_label = cpu_to_le32(qp->flow_label);\n\tp_ramrod->dst_qp_id = cpu_to_le32(qp->dest_qp);\n\tp_ramrod->mtu = cpu_to_le16(qp->mtu);\n\tp_ramrod->initial_psn = cpu_to_le32(qp->rq_psn);\n\tp_ramrod->pd = cpu_to_le16(qp->pd);\n\tp_ramrod->rq_num_pages = cpu_to_le16(qp->rq_num_pages);\n\tDMA_REGPAIR_LE(p_ramrod->rq_pbl_addr, qp->rq_pbl_ptr);\n\tDMA_REGPAIR_LE(p_ramrod->irq_pbl_addr, qp->irq_phys_addr);\n\tqed_rdma_copy_gids(qp, p_ramrod->src_gid, p_ramrod->dst_gid);\n\tp_ramrod->qp_handle_for_async.hi = qp->qp_handle_async.hi;\n\tp_ramrod->qp_handle_for_async.lo = qp->qp_handle_async.lo;\n\tp_ramrod->qp_handle_for_cqe.hi = qp->qp_handle.hi;\n\tp_ramrod->qp_handle_for_cqe.lo = qp->qp_handle.lo;\n\tp_ramrod->cq_cid = cpu_to_le32((p_hwfn->hw_info.opaque_fid << 16) |\n\t\t\t\t       qp->rq_cq_id);\n\tp_ramrod->xrc_domain = cpu_to_le16(qp->xrcd_id);\n\n\ttc = qed_roce_get_qp_tc(p_hwfn, qp);\n\tregular_latency_queue = qed_get_cm_pq_idx_ofld_mtc(p_hwfn, tc);\n\tlow_latency_queue = qed_get_cm_pq_idx_llt_mtc(p_hwfn, tc);\n\tDP_VERBOSE(p_hwfn, QED_MSG_SP,\n\t\t   \"qp icid %u pqs: regular_latency %u low_latency %u\\n\",\n\t\t   qp->icid, regular_latency_queue - CM_TX_PQ_BASE,\n\t\t   low_latency_queue - CM_TX_PQ_BASE);\n\tp_ramrod->regular_latency_phy_queue =\n\t    cpu_to_le16(regular_latency_queue);\n\tp_ramrod->low_latency_phy_queue =\n\t    cpu_to_le16(low_latency_queue);\n\n\tp_ramrod->dpi = cpu_to_le16(qp->dpi);\n\n\tqed_rdma_set_fw_mac(p_ramrod->remote_mac_addr, qp->remote_mac_addr);\n\tqed_rdma_set_fw_mac(p_ramrod->local_mac_addr, qp->local_mac_addr);\n\n\tp_ramrod->udp_src_port = cpu_to_le16(qp->udp_src_port);\n\tp_ramrod->vlan_id = cpu_to_le16(qp->vlan_id);\n\tp_ramrod->srq_id.srq_idx = cpu_to_le16(qp->srq_id);\n\tp_ramrod->srq_id.opaque_fid = cpu_to_le16(p_hwfn->hw_info.opaque_fid);\n\n\tp_ramrod->stats_counter_id = RESC_START(p_hwfn, QED_RDMA_STATS_QUEUE) +\n\t\t\t\t     qp->stats_queue;\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\tgoto err;\n\n\tqp->resp_offloaded = true;\n\tqp->cq_prod = 0;\n\n\tproto = p_hwfn->p_rdma_info->proto;\n\tqed_roce_set_real_cid(p_hwfn, qp->icid -\n\t\t\t      qed_cxt_get_proto_cid_start(p_hwfn, proto));\n\n\treturn rc;\n\nerr:\n\tDP_NOTICE(p_hwfn, \"create responder - failed, rc = %d\\n\", rc);\n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t  qp->irq_num_pages * RDMA_RING_PAGE_SIZE,\n\t\t\t  qp->irq, qp->irq_phys_addr);\n\n\treturn rc;\n}\n\nstatic int qed_roce_sp_create_requester(struct qed_hwfn *p_hwfn,\n\t\t\t\t\tstruct qed_rdma_qp *qp)\n{\n\tstruct roce_create_qp_req_ramrod_data *p_ramrod;\n\tu16 regular_latency_queue, low_latency_queue;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tenum protocol_type proto;\n\tu16 flags = 0;\n\tint rc;\n\tu8 tc;\n\n\tif (!qp->has_req)\n\t\treturn 0;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", qp->icid);\n\n\t \n\tqp->orq_num_pages = 1;\n\tqp->orq = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t\t     RDMA_RING_PAGE_SIZE,\n\t\t\t\t     &qp->orq_phys_addr, GFP_KERNEL);\n\tif (!qp->orq) {\n\t\trc = -ENOMEM;\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"qed create requester failed: cannot allocate memory (orq). rc = %d\\n\",\n\t\t\t  rc);\n\t\treturn rc;\n\t}\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.cid = qp->icid + 1;\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t ROCE_RAMROD_CREATE_QP,\n\t\t\t\t PROTOCOLID_ROCE, &init_data);\n\tif (rc)\n\t\tgoto err;\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_REQ_RAMROD_DATA_ROCE_FLAVOR,\n\t\t  qed_roce_mode_to_flavor(qp->roce_mode));\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_REQ_RAMROD_DATA_FMR_AND_RESERVED_EN,\n\t\t  qp->fmr_and_reserved_lkey);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_REQ_RAMROD_DATA_SIGNALED_COMP,\n\t\t  qp->signal_all);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_REQ_RAMROD_DATA_ERR_RETRY_CNT,\n\t\t  qp->retry_cnt);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_REQ_RAMROD_DATA_RNR_NAK_CNT,\n\t\t  qp->rnr_retry_cnt);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_REQ_RAMROD_DATA_XRC_FLAG,\n\t\t  qed_rdma_is_xrc_qp(qp));\n\n\tp_ramrod = &p_ent->ramrod.roce_create_qp_req;\n\tp_ramrod->flags = cpu_to_le16(flags);\n\n\tSET_FIELD(p_ramrod->flags2, ROCE_CREATE_QP_REQ_RAMROD_DATA_EDPM_MODE,\n\t\t  qp->edpm_mode);\n\n\tp_ramrod->max_ord = qp->max_rd_atomic_req;\n\tp_ramrod->traffic_class = qp->traffic_class_tos;\n\tp_ramrod->hop_limit = qp->hop_limit_ttl;\n\tp_ramrod->orq_num_pages = qp->orq_num_pages;\n\tp_ramrod->p_key = cpu_to_le16(qp->pkey);\n\tp_ramrod->flow_label = cpu_to_le32(qp->flow_label);\n\tp_ramrod->dst_qp_id = cpu_to_le32(qp->dest_qp);\n\tp_ramrod->ack_timeout_val = cpu_to_le32(qp->ack_timeout);\n\tp_ramrod->mtu = cpu_to_le16(qp->mtu);\n\tp_ramrod->initial_psn = cpu_to_le32(qp->sq_psn);\n\tp_ramrod->pd = cpu_to_le16(qp->pd);\n\tp_ramrod->sq_num_pages = cpu_to_le16(qp->sq_num_pages);\n\tDMA_REGPAIR_LE(p_ramrod->sq_pbl_addr, qp->sq_pbl_ptr);\n\tDMA_REGPAIR_LE(p_ramrod->orq_pbl_addr, qp->orq_phys_addr);\n\tqed_rdma_copy_gids(qp, p_ramrod->src_gid, p_ramrod->dst_gid);\n\tp_ramrod->qp_handle_for_async.hi = qp->qp_handle_async.hi;\n\tp_ramrod->qp_handle_for_async.lo = qp->qp_handle_async.lo;\n\tp_ramrod->qp_handle_for_cqe.hi = qp->qp_handle.hi;\n\tp_ramrod->qp_handle_for_cqe.lo = qp->qp_handle.lo;\n\tp_ramrod->cq_cid =\n\t    cpu_to_le32((p_hwfn->hw_info.opaque_fid << 16) | qp->sq_cq_id);\n\n\ttc = qed_roce_get_qp_tc(p_hwfn, qp);\n\tregular_latency_queue = qed_get_cm_pq_idx_ofld_mtc(p_hwfn, tc);\n\tlow_latency_queue = qed_get_cm_pq_idx_llt_mtc(p_hwfn, tc);\n\tDP_VERBOSE(p_hwfn, QED_MSG_SP,\n\t\t   \"qp icid %u pqs: regular_latency %u low_latency %u\\n\",\n\t\t   qp->icid, regular_latency_queue - CM_TX_PQ_BASE,\n\t\t   low_latency_queue - CM_TX_PQ_BASE);\n\tp_ramrod->regular_latency_phy_queue =\n\t    cpu_to_le16(regular_latency_queue);\n\tp_ramrod->low_latency_phy_queue =\n\t    cpu_to_le16(low_latency_queue);\n\n\tp_ramrod->dpi = cpu_to_le16(qp->dpi);\n\n\tqed_rdma_set_fw_mac(p_ramrod->remote_mac_addr, qp->remote_mac_addr);\n\tqed_rdma_set_fw_mac(p_ramrod->local_mac_addr, qp->local_mac_addr);\n\n\tp_ramrod->udp_src_port = cpu_to_le16(qp->udp_src_port);\n\tp_ramrod->vlan_id = cpu_to_le16(qp->vlan_id);\n\tp_ramrod->stats_counter_id = RESC_START(p_hwfn, QED_RDMA_STATS_QUEUE) +\n\t\t\t\t     qp->stats_queue;\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\tgoto err;\n\n\tqp->req_offloaded = true;\n\tproto = p_hwfn->p_rdma_info->proto;\n\tqed_roce_set_real_cid(p_hwfn,\n\t\t\t      qp->icid + 1 -\n\t\t\t      qed_cxt_get_proto_cid_start(p_hwfn, proto));\n\n\treturn rc;\n\nerr:\n\tDP_NOTICE(p_hwfn, \"Create requested - failed, rc = %d\\n\", rc);\n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t  qp->orq_num_pages * RDMA_RING_PAGE_SIZE,\n\t\t\t  qp->orq, qp->orq_phys_addr);\n\treturn rc;\n}\n\nstatic int qed_roce_sp_modify_responder(struct qed_hwfn *p_hwfn,\n\t\t\t\t\tstruct qed_rdma_qp *qp,\n\t\t\t\t\tbool move_to_err, u32 modify_flags)\n{\n\tstruct roce_modify_qp_resp_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tu16 flags = 0;\n\tint rc;\n\n\tif (!qp->has_resp)\n\t\treturn 0;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", qp->icid);\n\n\tif (move_to_err && !qp->resp_offloaded)\n\t\treturn 0;\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.cid = qp->icid;\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t ROCE_EVENT_MODIFY_QP,\n\t\t\t\t PROTOCOLID_ROCE, &init_data);\n\tif (rc) {\n\t\tDP_NOTICE(p_hwfn, \"rc = %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_RESP_RAMROD_DATA_MOVE_TO_ERR_FLG,\n\t\t  !!move_to_err);\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_RESP_RAMROD_DATA_RDMA_RD_EN,\n\t\t  qp->incoming_rdma_read_en);\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_RESP_RAMROD_DATA_RDMA_WR_EN,\n\t\t  qp->incoming_rdma_write_en);\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_RESP_RAMROD_DATA_ATOMIC_EN,\n\t\t  qp->incoming_atomic_en);\n\n\tSET_FIELD(flags, ROCE_CREATE_QP_RESP_RAMROD_DATA_E2E_FLOW_CONTROL_EN,\n\t\t  qp->e2e_flow_control_en);\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_RESP_RAMROD_DATA_RDMA_OPS_EN_FLG,\n\t\t  GET_FIELD(modify_flags,\n\t\t\t    QED_RDMA_MODIFY_QP_VALID_RDMA_OPS_EN));\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_RESP_RAMROD_DATA_P_KEY_FLG,\n\t\t  GET_FIELD(modify_flags, QED_ROCE_MODIFY_QP_VALID_PKEY));\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_RESP_RAMROD_DATA_ADDRESS_VECTOR_FLG,\n\t\t  GET_FIELD(modify_flags,\n\t\t\t    QED_ROCE_MODIFY_QP_VALID_ADDRESS_VECTOR));\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_RESP_RAMROD_DATA_MAX_IRD_FLG,\n\t\t  GET_FIELD(modify_flags,\n\t\t\t    QED_RDMA_MODIFY_QP_VALID_MAX_RD_ATOMIC_RESP));\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_RESP_RAMROD_DATA_MIN_RNR_NAK_TIMER_FLG,\n\t\t  GET_FIELD(modify_flags,\n\t\t\t    QED_ROCE_MODIFY_QP_VALID_MIN_RNR_NAK_TIMER));\n\n\tp_ramrod = &p_ent->ramrod.roce_modify_qp_resp;\n\tp_ramrod->flags = cpu_to_le16(flags);\n\n\tp_ramrod->fields = 0;\n\tSET_FIELD(p_ramrod->fields,\n\t\t  ROCE_MODIFY_QP_RESP_RAMROD_DATA_MIN_RNR_NAK_TIMER,\n\t\t  qp->min_rnr_nak_timer);\n\n\tp_ramrod->max_ird = qp->max_rd_atomic_resp;\n\tp_ramrod->traffic_class = qp->traffic_class_tos;\n\tp_ramrod->hop_limit = qp->hop_limit_ttl;\n\tp_ramrod->p_key = cpu_to_le16(qp->pkey);\n\tp_ramrod->flow_label = cpu_to_le32(qp->flow_label);\n\tp_ramrod->mtu = cpu_to_le16(qp->mtu);\n\tqed_rdma_copy_gids(qp, p_ramrod->src_gid, p_ramrod->dst_gid);\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Modify responder, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic int qed_roce_sp_modify_requester(struct qed_hwfn *p_hwfn,\n\t\t\t\t\tstruct qed_rdma_qp *qp,\n\t\t\t\t\tbool move_to_sqd,\n\t\t\t\t\tbool move_to_err, u32 modify_flags)\n{\n\tstruct roce_modify_qp_req_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tu16 flags = 0;\n\tint rc;\n\n\tif (!qp->has_req)\n\t\treturn 0;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", qp->icid);\n\n\tif (move_to_err && !(qp->req_offloaded))\n\t\treturn 0;\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.cid = qp->icid + 1;\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t ROCE_EVENT_MODIFY_QP,\n\t\t\t\t PROTOCOLID_ROCE, &init_data);\n\tif (rc) {\n\t\tDP_NOTICE(p_hwfn, \"rc = %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_REQ_RAMROD_DATA_MOVE_TO_ERR_FLG,\n\t\t  !!move_to_err);\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_REQ_RAMROD_DATA_MOVE_TO_SQD_FLG,\n\t\t  !!move_to_sqd);\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_REQ_RAMROD_DATA_EN_SQD_ASYNC_NOTIFY,\n\t\t  qp->sqd_async);\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_REQ_RAMROD_DATA_P_KEY_FLG,\n\t\t  GET_FIELD(modify_flags, QED_ROCE_MODIFY_QP_VALID_PKEY));\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_REQ_RAMROD_DATA_ADDRESS_VECTOR_FLG,\n\t\t  GET_FIELD(modify_flags,\n\t\t\t    QED_ROCE_MODIFY_QP_VALID_ADDRESS_VECTOR));\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_REQ_RAMROD_DATA_MAX_ORD_FLG,\n\t\t  GET_FIELD(modify_flags,\n\t\t\t    QED_RDMA_MODIFY_QP_VALID_MAX_RD_ATOMIC_REQ));\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_REQ_RAMROD_DATA_RNR_NAK_CNT_FLG,\n\t\t  GET_FIELD(modify_flags,\n\t\t\t    QED_ROCE_MODIFY_QP_VALID_RNR_RETRY_CNT));\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_REQ_RAMROD_DATA_ERR_RETRY_CNT_FLG,\n\t\t  GET_FIELD(modify_flags, QED_ROCE_MODIFY_QP_VALID_RETRY_CNT));\n\n\tSET_FIELD(flags, ROCE_MODIFY_QP_REQ_RAMROD_DATA_ACK_TIMEOUT_FLG,\n\t\t  GET_FIELD(modify_flags,\n\t\t\t    QED_ROCE_MODIFY_QP_VALID_ACK_TIMEOUT));\n\n\tp_ramrod = &p_ent->ramrod.roce_modify_qp_req;\n\tp_ramrod->flags = cpu_to_le16(flags);\n\n\tp_ramrod->fields = 0;\n\tSET_FIELD(p_ramrod->fields,\n\t\t  ROCE_MODIFY_QP_REQ_RAMROD_DATA_ERR_RETRY_CNT, qp->retry_cnt);\n\tSET_FIELD(p_ramrod->fields, ROCE_MODIFY_QP_REQ_RAMROD_DATA_RNR_NAK_CNT,\n\t\t  qp->rnr_retry_cnt);\n\n\tp_ramrod->max_ord = qp->max_rd_atomic_req;\n\tp_ramrod->traffic_class = qp->traffic_class_tos;\n\tp_ramrod->hop_limit = qp->hop_limit_ttl;\n\tp_ramrod->p_key = cpu_to_le16(qp->pkey);\n\tp_ramrod->flow_label = cpu_to_le32(qp->flow_label);\n\tp_ramrod->ack_timeout_val = cpu_to_le32(qp->ack_timeout);\n\tp_ramrod->mtu = cpu_to_le16(qp->mtu);\n\tqed_rdma_copy_gids(qp, p_ramrod->src_gid, p_ramrod->dst_gid);\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Modify requester, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic int qed_roce_sp_destroy_qp_responder(struct qed_hwfn *p_hwfn,\n\t\t\t\t\t    struct qed_rdma_qp *qp,\n\t\t\t\t\t    u32 *cq_prod)\n{\n\tstruct roce_destroy_qp_resp_output_params *p_ramrod_res;\n\tstruct roce_destroy_qp_resp_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tdma_addr_t ramrod_res_phys;\n\tint rc;\n\n\tif (!qp->has_resp) {\n\t\t*cq_prod = 0;\n\t\treturn 0;\n\t}\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", qp->icid);\n\t*cq_prod = qp->cq_prod;\n\n\tif (!qp->resp_offloaded) {\n\t\t \n\t\tu32 cid;\n\n\t\tcid = qp->icid -\n\t\t      qed_cxt_get_proto_cid_start(p_hwfn,\n\t\t\t\t\t\t  p_hwfn->p_rdma_info->proto);\n\t\tqed_roce_free_cid_pair(p_hwfn, (u16)cid);\n\n\t\treturn 0;\n\t}\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.cid = qp->icid;\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t ROCE_RAMROD_DESTROY_QP,\n\t\t\t\t PROTOCOLID_ROCE, &init_data);\n\tif (rc)\n\t\treturn rc;\n\n\tp_ramrod = &p_ent->ramrod.roce_destroy_qp_resp;\n\n\tp_ramrod_res = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t\t\t  sizeof(*p_ramrod_res),\n\t\t\t\t\t  &ramrod_res_phys, GFP_KERNEL);\n\n\tif (!p_ramrod_res) {\n\t\trc = -ENOMEM;\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"qed destroy responder failed: cannot allocate memory (ramrod). rc = %d\\n\",\n\t\t\t  rc);\n\t\tqed_sp_destroy_request(p_hwfn, p_ent);\n\t\treturn rc;\n\t}\n\n\tDMA_REGPAIR_LE(p_ramrod->output_params_addr, ramrod_res_phys);\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\tgoto err;\n\n\t*cq_prod = le32_to_cpu(p_ramrod_res->cq_prod);\n\tqp->cq_prod = *cq_prod;\n\n\t \n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t  qp->irq_num_pages * RDMA_RING_PAGE_SIZE,\n\t\t\t  qp->irq, qp->irq_phys_addr);\n\n\tqp->resp_offloaded = false;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Destroy responder, rc = %d\\n\", rc);\n\nerr:\n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t  sizeof(struct roce_destroy_qp_resp_output_params),\n\t\t\t  p_ramrod_res, ramrod_res_phys);\n\n\treturn rc;\n}\n\nstatic int qed_roce_sp_destroy_qp_requester(struct qed_hwfn *p_hwfn,\n\t\t\t\t\t    struct qed_rdma_qp *qp)\n{\n\tstruct roce_destroy_qp_req_output_params *p_ramrod_res;\n\tstruct roce_destroy_qp_req_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tdma_addr_t ramrod_res_phys;\n\tint rc = -ENOMEM;\n\n\tif (!qp->has_req)\n\t\treturn 0;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", qp->icid);\n\n\tif (!qp->req_offloaded)\n\t\treturn 0;\n\n\tp_ramrod_res = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t\t\t  sizeof(*p_ramrod_res),\n\t\t\t\t\t  &ramrod_res_phys, GFP_KERNEL);\n\tif (!p_ramrod_res) {\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"qed destroy requester failed: cannot allocate memory (ramrod)\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.cid = qp->icid + 1;\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent, ROCE_RAMROD_DESTROY_QP,\n\t\t\t\t PROTOCOLID_ROCE, &init_data);\n\tif (rc)\n\t\tgoto err;\n\n\tp_ramrod = &p_ent->ramrod.roce_destroy_qp_req;\n\tDMA_REGPAIR_LE(p_ramrod->output_params_addr, ramrod_res_phys);\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\tgoto err;\n\n\t \n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t  qp->orq_num_pages * RDMA_RING_PAGE_SIZE,\n\t\t\t  qp->orq, qp->orq_phys_addr);\n\n\tqp->req_offloaded = false;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Destroy requester, rc = %d\\n\", rc);\n\nerr:\n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_ramrod_res),\n\t\t\t  p_ramrod_res, ramrod_res_phys);\n\n\treturn rc;\n}\n\nint qed_roce_query_qp(struct qed_hwfn *p_hwfn,\n\t\t      struct qed_rdma_qp *qp,\n\t\t      struct qed_rdma_query_qp_out_params *out_params)\n{\n\tstruct roce_query_qp_resp_output_params *p_resp_ramrod_res;\n\tstruct roce_query_qp_req_output_params *p_req_ramrod_res;\n\tstruct roce_query_qp_resp_ramrod_data *p_resp_ramrod;\n\tstruct roce_query_qp_req_ramrod_data *p_req_ramrod;\n\tstruct qed_sp_init_data init_data;\n\tdma_addr_t resp_ramrod_res_phys;\n\tdma_addr_t req_ramrod_res_phys;\n\tstruct qed_spq_entry *p_ent;\n\tbool rq_err_state;\n\tbool sq_err_state;\n\tbool sq_draining;\n\tint rc = -ENOMEM;\n\n\tif ((!(qp->resp_offloaded)) && (!(qp->req_offloaded))) {\n\t\t \n\t\tout_params->draining = false;\n\t\tout_params->rq_psn = qp->rq_psn;\n\t\tout_params->sq_psn = qp->sq_psn;\n\t\tout_params->state = qp->cur_state;\n\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"No QPs as no offload\\n\");\n\t\treturn 0;\n\t}\n\n\tif (!(qp->resp_offloaded)) {\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"The responder's qp should be offloaded before requester's\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tp_resp_ramrod_res =\n\t\tdma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t\t   sizeof(*p_resp_ramrod_res),\n\t\t\t\t   &resp_ramrod_res_phys, GFP_KERNEL);\n\tif (!p_resp_ramrod_res) {\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"qed query qp failed: cannot allocate memory (ramrod)\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.cid = qp->icid;\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\trc = qed_sp_init_request(p_hwfn, &p_ent, ROCE_RAMROD_QUERY_QP,\n\t\t\t\t PROTOCOLID_ROCE, &init_data);\n\tif (rc)\n\t\tgoto err_resp;\n\n\tp_resp_ramrod = &p_ent->ramrod.roce_query_qp_resp;\n\tDMA_REGPAIR_LE(p_resp_ramrod->output_params_addr, resp_ramrod_res_phys);\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\tgoto err_resp;\n\n\tout_params->rq_psn = le32_to_cpu(p_resp_ramrod_res->psn);\n\trq_err_state = GET_FIELD(le32_to_cpu(p_resp_ramrod_res->flags),\n\t\t\t\t ROCE_QUERY_QP_RESP_OUTPUT_PARAMS_ERROR_FLG);\n\n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_resp_ramrod_res),\n\t\t\t  p_resp_ramrod_res, resp_ramrod_res_phys);\n\n\tif (!(qp->req_offloaded)) {\n\t\t \n\t\tout_params->sq_psn = qp->sq_psn;\n\t\tout_params->draining = false;\n\n\t\tif (rq_err_state)\n\t\t\tqp->cur_state = QED_ROCE_QP_STATE_ERR;\n\n\t\tout_params->state = qp->cur_state;\n\n\t\treturn 0;\n\t}\n\n\t \n\tp_req_ramrod_res = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t\t\t      sizeof(*p_req_ramrod_res),\n\t\t\t\t\t      &req_ramrod_res_phys,\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!p_req_ramrod_res) {\n\t\trc = -ENOMEM;\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"qed query qp failed: cannot allocate memory (ramrod)\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tinit_data.cid = qp->icid + 1;\n\trc = qed_sp_init_request(p_hwfn, &p_ent, ROCE_RAMROD_QUERY_QP,\n\t\t\t\t PROTOCOLID_ROCE, &init_data);\n\tif (rc)\n\t\tgoto err_req;\n\n\tp_req_ramrod = &p_ent->ramrod.roce_query_qp_req;\n\tDMA_REGPAIR_LE(p_req_ramrod->output_params_addr, req_ramrod_res_phys);\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\tgoto err_req;\n\n\tout_params->sq_psn = le32_to_cpu(p_req_ramrod_res->psn);\n\tsq_err_state = GET_FIELD(le32_to_cpu(p_req_ramrod_res->flags),\n\t\t\t\t ROCE_QUERY_QP_REQ_OUTPUT_PARAMS_ERR_FLG);\n\tsq_draining =\n\t\tGET_FIELD(le32_to_cpu(p_req_ramrod_res->flags),\n\t\t\t  ROCE_QUERY_QP_REQ_OUTPUT_PARAMS_SQ_DRAINING_FLG);\n\n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_req_ramrod_res),\n\t\t\t  p_req_ramrod_res, req_ramrod_res_phys);\n\n\tout_params->draining = false;\n\n\tif (rq_err_state || sq_err_state)\n\t\tqp->cur_state = QED_ROCE_QP_STATE_ERR;\n\telse if (sq_draining)\n\t\tout_params->draining = true;\n\tout_params->state = qp->cur_state;\n\n\treturn 0;\n\nerr_req:\n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_req_ramrod_res),\n\t\t\t  p_req_ramrod_res, req_ramrod_res_phys);\n\treturn rc;\nerr_resp:\n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_resp_ramrod_res),\n\t\t\t  p_resp_ramrod_res, resp_ramrod_res_phys);\n\treturn rc;\n}\n\nint qed_roce_destroy_qp(struct qed_hwfn *p_hwfn, struct qed_rdma_qp *qp)\n{\n\tu32 cq_prod;\n\tint rc;\n\n\t \n\tif ((qp->cur_state != QED_ROCE_QP_STATE_RESET) &&\n\t    (qp->cur_state != QED_ROCE_QP_STATE_ERR) &&\n\t    (qp->cur_state != QED_ROCE_QP_STATE_INIT)) {\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"QP must be in error, reset or init state before destroying it\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (qp->cur_state != QED_ROCE_QP_STATE_RESET) {\n\t\trc = qed_roce_sp_destroy_qp_responder(p_hwfn, qp,\n\t\t\t\t\t\t      &cq_prod);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\t \n\t\trc = qed_roce_sp_destroy_qp_requester(p_hwfn, qp);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nint qed_roce_modify_qp(struct qed_hwfn *p_hwfn,\n\t\t       struct qed_rdma_qp *qp,\n\t\t       enum qed_roce_qp_state prev_state,\n\t\t       struct qed_rdma_modify_qp_in_params *params)\n{\n\tint rc = 0;\n\n\t \n\tif (((prev_state == QED_ROCE_QP_STATE_INIT) ||\n\t     (prev_state == QED_ROCE_QP_STATE_RESET)) &&\n\t    (qp->cur_state == QED_ROCE_QP_STATE_RTR)) {\n\t\t \n\t\trc = qed_roce_sp_create_responder(p_hwfn, qp);\n\t\treturn rc;\n\t} else if ((prev_state == QED_ROCE_QP_STATE_RTR) &&\n\t\t   (qp->cur_state == QED_ROCE_QP_STATE_RTS)) {\n\t\t \n\t\trc = qed_roce_sp_create_requester(p_hwfn, qp);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\t \n\t\trc = qed_roce_sp_modify_responder(p_hwfn, qp, false,\n\t\t\t\t\t\t  params->modify_flags);\n\t\treturn rc;\n\t} else if ((prev_state == QED_ROCE_QP_STATE_RTS) &&\n\t\t   (qp->cur_state == QED_ROCE_QP_STATE_RTS)) {\n\t\t \n\t\trc = qed_roce_sp_modify_responder(p_hwfn, qp, false,\n\t\t\t\t\t\t  params->modify_flags);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\trc = qed_roce_sp_modify_requester(p_hwfn, qp, false, false,\n\t\t\t\t\t\t  params->modify_flags);\n\t\treturn rc;\n\t} else if ((prev_state == QED_ROCE_QP_STATE_RTS) &&\n\t\t   (qp->cur_state == QED_ROCE_QP_STATE_SQD)) {\n\t\t \n\t\trc = qed_roce_sp_modify_requester(p_hwfn, qp, true, false,\n\t\t\t\t\t\t  params->modify_flags);\n\t\treturn rc;\n\t} else if ((prev_state == QED_ROCE_QP_STATE_SQD) &&\n\t\t   (qp->cur_state == QED_ROCE_QP_STATE_SQD)) {\n\t\t \n\t\trc = qed_roce_sp_modify_responder(p_hwfn, qp, false,\n\t\t\t\t\t\t  params->modify_flags);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\trc = qed_roce_sp_modify_requester(p_hwfn, qp, false, false,\n\t\t\t\t\t\t  params->modify_flags);\n\t\treturn rc;\n\t} else if ((prev_state == QED_ROCE_QP_STATE_SQD) &&\n\t\t   (qp->cur_state == QED_ROCE_QP_STATE_RTS)) {\n\t\t \n\t\trc = qed_roce_sp_modify_responder(p_hwfn, qp, false,\n\t\t\t\t\t\t  params->modify_flags);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\trc = qed_roce_sp_modify_requester(p_hwfn, qp, false, false,\n\t\t\t\t\t\t  params->modify_flags);\n\n\t\treturn rc;\n\t} else if (qp->cur_state == QED_ROCE_QP_STATE_ERR) {\n\t\t \n\t\trc = qed_roce_sp_modify_responder(p_hwfn, qp, true,\n\t\t\t\t\t\t  params->modify_flags);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\trc = qed_roce_sp_modify_requester(p_hwfn, qp, false, true,\n\t\t\t\t\t\t  params->modify_flags);\n\t\treturn rc;\n\t} else if (qp->cur_state == QED_ROCE_QP_STATE_RESET) {\n\t\t \n\t\tu32 cq_prod;\n\n\t\t \n\t\trc = qed_roce_sp_destroy_qp_responder(p_hwfn,\n\t\t\t\t\t\t      qp,\n\t\t\t\t\t\t      &cq_prod);\n\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tqp->cq_prod = cq_prod;\n\n\t\trc = qed_roce_sp_destroy_qp_requester(p_hwfn, qp);\n\t} else {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"0\\n\");\n\t}\n\n\treturn rc;\n}\n\nstatic void qed_roce_free_real_icid(struct qed_hwfn *p_hwfn, u16 icid)\n{\n\tstruct qed_rdma_info *p_rdma_info = p_hwfn->p_rdma_info;\n\tu32 start_cid, cid, xcid;\n\n\t \n\tstart_cid = qed_cxt_get_proto_cid_start(p_hwfn, p_rdma_info->proto);\n\tcid = icid - start_cid;\n\txcid = cid ^ 1;\n\n\tspin_lock_bh(&p_rdma_info->lock);\n\n\tqed_bmap_release_id(p_hwfn, &p_rdma_info->real_cid_map, cid);\n\tif (qed_bmap_test_id(p_hwfn, &p_rdma_info->real_cid_map, xcid) == 0) {\n\t\tqed_bmap_release_id(p_hwfn, &p_rdma_info->cid_map, cid);\n\t\tqed_bmap_release_id(p_hwfn, &p_rdma_info->cid_map, xcid);\n\t}\n\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n}\n\nvoid qed_roce_dpm_dcbx(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)\n{\n\tu8 val;\n\n\t \n\tval = qed_rdma_allocated_qps(p_hwfn) ? true : false;\n\tp_hwfn->dcbx_no_edpm = (u8)val;\n\n\tqed_rdma_dpm_conf(p_hwfn, p_ptt);\n}\n\nint qed_roce_setup(struct qed_hwfn *p_hwfn)\n{\n\treturn qed_spq_register_async_cb(p_hwfn, PROTOCOLID_ROCE,\n\t\t\t\t\t qed_roce_async_event);\n}\n\nint qed_roce_init_hw(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)\n{\n\tu32 ll2_ethertype_en;\n\n\tqed_wr(p_hwfn, p_ptt, PRS_REG_ROCE_DEST_QP_MAX_PF, 0);\n\n\tp_hwfn->rdma_prs_search_reg = PRS_REG_SEARCH_ROCE;\n\n\tll2_ethertype_en = qed_rd(p_hwfn, p_ptt, PRS_REG_LIGHT_L2_ETHERTYPE_EN);\n\tqed_wr(p_hwfn, p_ptt, PRS_REG_LIGHT_L2_ETHERTYPE_EN,\n\t       (ll2_ethertype_en | 0x01));\n\n\tif (qed_cxt_get_proto_cid_start(p_hwfn, PROTOCOLID_ROCE) % 2) {\n\t\tDP_NOTICE(p_hwfn, \"The first RoCE's cid should be even\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Initializing HW - Done\\n\");\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}