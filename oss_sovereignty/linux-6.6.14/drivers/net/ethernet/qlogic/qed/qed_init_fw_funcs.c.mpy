{
  "module_name": "qed_init_fw_funcs.c",
  "hash_id": "82e77f7e6dc3defc00790f864dc523e730f2c5f3bafd6a24a6a1a4c4376b0c98",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/qlogic/qed/qed_init_fw_funcs.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/crc8.h>\n#include <linux/delay.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n#include \"qed_hsi.h\"\n#include \"qed_hw.h\"\n#include \"qed_init_ops.h\"\n#include \"qed_iro_hsi.h\"\n#include \"qed_reg_addr.h\"\n\n#define CDU_VALIDATION_DEFAULT_CFG CDU_CONTEXT_VALIDATION_DEFAULT_CFG\n\nstatic u16 con_region_offsets[3][NUM_OF_CONNECTION_TYPES] = {\n\t{400, 336, 352, 368, 304, 384, 416, 352},\t \n\t{528, 496, 416, 512, 448, 512, 544, 480},\t \n\t{608, 544, 496, 576, 576, 592, 624, 560}\t \n};\n\nstatic u16 task_region_offsets[1][NUM_OF_CONNECTION_TYPES] = {\n\t{240, 240, 112, 0, 0, 0, 0, 96}\t \n};\n\n \n#define QM_PQ_MEM_4KB(pq_size)\t(pq_size ? DIV_ROUND_UP((pq_size + 1) *\t\\\n\t\t\t\t\t\t\tQM_PQ_ELEMENT_SIZE, \\\n\t\t\t\t\t\t\t0x1000) : 0)\n#define QM_PQ_SIZE_256B(pq_size)\t(pq_size ? DIV_ROUND_UP(pq_size, \\\n\t\t\t\t\t\t\t\t0x100) - 1 : 0)\n#define QM_INVALID_PQ_ID\t\t0xffff\n\n \n#define QM_MAX_LINK_SPEED               100000\n\n \n#define QM_BYPASS_EN\t1\n#define QM_BYTE_CRD_EN\t1\n\n \n#define QM_INITIAL_VOQ_BYTE_CRD         98304\n \n#define QM_OTHER_PQS_PER_PF\t4\n\n \n#define MAX_NUM_VOQS\t(MAX_NUM_PORTS_K2 * NUM_TCS_4PORT_K2)\n#define VOQS_BIT_MASK\t(BIT(MAX_NUM_VOQS) - 1)\n\n \n\n \n#define QM_PF_WFQ_INC_VAL(weight)       ((weight) * 0x9000)\n\n \n#define QM_PF_WFQ_UPPER_BOUND           62500000\n\n \n#define QM_PF_WFQ_MAX_INC_VAL           ((QM_PF_WFQ_UPPER_BOUND * 7) / 10)\n\n \n#define QM_PF_WFQ_CRD_E5_NUM_VOQS       16\n\n \n#define QM_VP_WFQ_INC_VAL(weight)       ((weight) * QM_VP_WFQ_MIN_INC_VAL)\n\n \n#define QM_VP_WFQ_MIN_INC_VAL           10800\n\n \n#define QM_VP_WFQ_MAX_INC_VAL           0x40000000\n\n \n#define QM_VP_WFQ_BYPASS_THRESH         (QM_VP_WFQ_MIN_INC_VAL - 100)\n\n \n#define QM_VP_RL_CRD_TASK_COST          9700\n\n \n#define QM_VP_WFQ_PQ_VOQ_SHIFT          0\n\n \n#define QM_VP_WFQ_PQ_PF_SHIFT   5\n\n \n\n \n#define QM_RL_PERIOD\t5\n\n \n#define QM_RL_PERIOD_CLK_25M\t(25 * QM_RL_PERIOD)\n\n \n#define QM_RL_INC_VAL(rate)                     ({\t\\\n\t\t\t\t\t\ttypeof(rate) __rate = (rate); \\\n\t\t\t\t\t\tmax_t(u32,\t\t\\\n\t\t\t\t\t\t(u32)(((__rate ? __rate : \\\n\t\t\t\t\t\t100000) *\t\t\\\n\t\t\t\t\t\tQM_RL_PERIOD *\t\t\\\n\t\t\t\t\t\t101) / (8 * 100)), 1); })\n\n \n#define QM_PF_RL_UPPER_BOUND\t62500000\n\n \n#define QM_PF_RL_MAX_INC_VAL\t((QM_PF_RL_UPPER_BOUND * 7) / 10)\n\n \n#define QM_GLOBAL_RL_UPPER_BOUND(speed)         ((u32)max_t( \\\n\t\tu32,\t\t\t\t\t    \\\n\t\t(u32)(((speed) *\t\t\t    \\\n\t\t       QM_RL_PERIOD * 101) / (8 * 100)),    \\\n\t\tQM_VP_RL_CRD_TASK_COST\t\t\t    \\\n\t\t+ 1000))\n\n \n#define QM_OPPOR_LINE_VOQ_DEF\t1\n#define QM_OPPOR_FW_STOP_DEF\t0\n#define QM_OPPOR_PQ_EMPTY_DEF\t1\n\n \n\n \n#define PBF_CMDQ_PURE_LB_LINES\t150\n\n#define PBF_CMDQ_LINES_RT_OFFSET(ext_voq) \\\n\t(PBF_REG_YCMD_QS_NUM_LINES_VOQ0_RT_OFFSET + \\\n\t (ext_voq) * (PBF_REG_YCMD_QS_NUM_LINES_VOQ1_RT_OFFSET - \\\n\t\tPBF_REG_YCMD_QS_NUM_LINES_VOQ0_RT_OFFSET))\n\n#define PBF_BTB_GUARANTEED_RT_OFFSET(ext_voq) \\\n\t(PBF_REG_BTB_GUARANTEED_VOQ0_RT_OFFSET + \\\n\t (ext_voq) * (PBF_REG_BTB_GUARANTEED_VOQ1_RT_OFFSET - \\\n\t\tPBF_REG_BTB_GUARANTEED_VOQ0_RT_OFFSET))\n\n \n#define QM_VOQ_LINE_CRD(pbf_cmd_lines) \\\n\t((((pbf_cmd_lines) - 4) * 2) | QM_LINE_CRD_REG_SIGN_BIT)\n\n \n\n \n#define BTB_JUMBO_PKT_BLOCKS\t38\n\n \n#define BTB_HEADROOM_BLOCKS\tBTB_JUMBO_PKT_BLOCKS\n#define BTB_PURE_LB_FACTOR\t10\n\n \n#define BTB_PURE_LB_RATIO\t7\n\n \n#define QM_STOP_PQ_MASK_WIDTH\t\t32\n#define QM_STOP_CMD_ADDR\t\t2\n#define QM_STOP_CMD_STRUCT_SIZE\t\t2\n#define QM_STOP_CMD_PAUSE_MASK_OFFSET\t0\n#define QM_STOP_CMD_PAUSE_MASK_SHIFT\t0\n#define QM_STOP_CMD_PAUSE_MASK_MASK\t-1\n#define QM_STOP_CMD_GROUP_ID_OFFSET\t1\n#define QM_STOP_CMD_GROUP_ID_SHIFT\t16\n#define QM_STOP_CMD_GROUP_ID_MASK\t15\n#define QM_STOP_CMD_PQ_TYPE_OFFSET\t1\n#define QM_STOP_CMD_PQ_TYPE_SHIFT\t24\n#define QM_STOP_CMD_PQ_TYPE_MASK\t1\n#define QM_STOP_CMD_MAX_POLL_COUNT\t100\n#define QM_STOP_CMD_POLL_PERIOD_US\t500\n\n \n#define QM_CMD_STRUCT_SIZE(cmd)\tcmd ## _STRUCT_SIZE\n#define QM_CMD_SET_FIELD(var, cmd, field, value) \\\n\tSET_FIELD(var[cmd ## _ ## field ## _OFFSET], \\\n\t\t  cmd ## _ ## field, \\\n\t\t  value)\n\n#define QM_INIT_TX_PQ_MAP(p_hwfn, map, pq_id, vp_pq_id, rl_valid,\t      \\\n\t\t\t  rl_id, ext_voq, wrr)\t\t\t\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tu32 __reg = 0;\t\t\t\t\t\t      \\\n\t\t\t\t\t\t\t\t\t      \\\n\t\tBUILD_BUG_ON(sizeof((map).reg) != sizeof(__reg));\t      \\\n\t\tmemset(&(map), 0, sizeof(map));\t\t\t\t      \\\n\t\tSET_FIELD(__reg, QM_RF_PQ_MAP_PQ_VALID, 1);\t      \\\n\t\tSET_FIELD(__reg, QM_RF_PQ_MAP_RL_VALID,\t      \\\n\t\t\t  !!(rl_valid));\t\t\t\t      \\\n\t\tSET_FIELD(__reg, QM_RF_PQ_MAP_VP_PQ_ID, (vp_pq_id)); \\\n\t\tSET_FIELD(__reg, QM_RF_PQ_MAP_RL_ID, (rl_id));\t      \\\n\t\tSET_FIELD(__reg, QM_RF_PQ_MAP_VOQ, (ext_voq));\t      \\\n\t\tSET_FIELD(__reg, QM_RF_PQ_MAP_WRR_WEIGHT_GROUP,      \\\n\t\t\t  (wrr));\t\t\t\t\t      \\\n\t\t\t\t\t\t\t\t\t      \\\n\t\tSTORE_RT_REG((p_hwfn), QM_REG_TXPQMAP_RT_OFFSET + (pq_id),    \\\n\t\t\t     __reg);\t\t\t\t\t      \\\n\t\t(map).reg = cpu_to_le32(__reg);\t\t\t\t      \\\n\t} while (0)\n\n#define WRITE_PQ_INFO_TO_RAM\t1\n#define PQ_INFO_ELEMENT(vp, pf, tc, port, rl_valid, rl) \\\n\t(((vp) << 0) | ((pf) << 12) | ((tc) << 16) | ((port) << 20) | \\\n\t((rl_valid ? 1 : 0) << 22) | (((rl) & 255) << 24) | \\\n\t(((rl) >> 8) << 9))\n\n#define PQ_INFO_RAM_GRC_ADDRESS(pq_id) \\\n\t(XSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM + \\\n\tXSTORM_PQ_INFO_OFFSET(pq_id))\n\nstatic const char * const s_protocol_types[] = {\n\t\"PROTOCOLID_ISCSI\", \"PROTOCOLID_FCOE\", \"PROTOCOLID_ROCE\",\n\t\"PROTOCOLID_CORE\", \"PROTOCOLID_ETH\", \"PROTOCOLID_IWARP\",\n\t\"PROTOCOLID_TOE\", \"PROTOCOLID_PREROCE\", \"PROTOCOLID_COMMON\",\n\t\"PROTOCOLID_TCP\", \"PROTOCOLID_RDMA\", \"PROTOCOLID_SCSI\",\n};\n\nstatic const char *s_ramrod_cmd_ids[][28] = {\n\t{\n\t\"ISCSI_RAMROD_CMD_ID_UNUSED\", \"ISCSI_RAMROD_CMD_ID_INIT_FUNC\",\n\t \"ISCSI_RAMROD_CMD_ID_DESTROY_FUNC\",\n\t \"ISCSI_RAMROD_CMD_ID_OFFLOAD_CONN\",\n\t \"ISCSI_RAMROD_CMD_ID_UPDATE_CONN\",\n\t \"ISCSI_RAMROD_CMD_ID_TERMINATION_CONN\",\n\t \"ISCSI_RAMROD_CMD_ID_CLEAR_SQ\", \"ISCSI_RAMROD_CMD_ID_MAC_UPDATE\",\n\t \"ISCSI_RAMROD_CMD_ID_CONN_STATS\", },\n\t{ \"FCOE_RAMROD_CMD_ID_INIT_FUNC\", \"FCOE_RAMROD_CMD_ID_DESTROY_FUNC\",\n\t \"FCOE_RAMROD_CMD_ID_STAT_FUNC\",\n\t \"FCOE_RAMROD_CMD_ID_OFFLOAD_CONN\",\n\t \"FCOE_RAMROD_CMD_ID_TERMINATE_CONN\", },\n\t{ \"RDMA_RAMROD_UNUSED\", \"RDMA_RAMROD_FUNC_INIT\",\n\t \"RDMA_RAMROD_FUNC_CLOSE\", \"RDMA_RAMROD_REGISTER_MR\",\n\t \"RDMA_RAMROD_DEREGISTER_MR\", \"RDMA_RAMROD_CREATE_CQ\",\n\t \"RDMA_RAMROD_RESIZE_CQ\", \"RDMA_RAMROD_DESTROY_CQ\",\n\t \"RDMA_RAMROD_CREATE_SRQ\", \"RDMA_RAMROD_MODIFY_SRQ\",\n\t \"RDMA_RAMROD_DESTROY_SRQ\", \"RDMA_RAMROD_START_NS_TRACKING\",\n\t \"RDMA_RAMROD_STOP_NS_TRACKING\", \"ROCE_RAMROD_CREATE_QP\",\n\t \"ROCE_RAMROD_MODIFY_QP\", \"ROCE_RAMROD_QUERY_QP\",\n\t \"ROCE_RAMROD_DESTROY_QP\", \"ROCE_RAMROD_CREATE_UD_QP\",\n\t \"ROCE_RAMROD_DESTROY_UD_QP\", \"ROCE_RAMROD_FUNC_UPDATE\",\n\t \"ROCE_RAMROD_SUSPEND_QP\", \"ROCE_RAMROD_QUERY_SUSPENDED_QP\",\n\t \"ROCE_RAMROD_CREATE_SUSPENDED_QP\", \"ROCE_RAMROD_RESUME_QP\",\n\t \"ROCE_RAMROD_SUSPEND_UD_QP\", \"ROCE_RAMROD_RESUME_UD_QP\",\n\t \"ROCE_RAMROD_CREATE_SUSPENDED_UD_QP\", \"ROCE_RAMROD_FLUSH_DPT_QP\", },\n\t{ \"CORE_RAMROD_UNUSED\", \"CORE_RAMROD_RX_QUEUE_START\",\n\t \"CORE_RAMROD_TX_QUEUE_START\", \"CORE_RAMROD_RX_QUEUE_STOP\",\n\t \"CORE_RAMROD_TX_QUEUE_STOP\",\n\t \"CORE_RAMROD_RX_QUEUE_FLUSH\",\n\t \"CORE_RAMROD_TX_QUEUE_UPDATE\", \"CORE_RAMROD_QUEUE_STATS_QUERY\", },\n\t{ \"ETH_RAMROD_UNUSED\", \"ETH_RAMROD_VPORT_START\",\n\t \"ETH_RAMROD_VPORT_UPDATE\", \"ETH_RAMROD_VPORT_STOP\",\n\t \"ETH_RAMROD_RX_QUEUE_START\", \"ETH_RAMROD_RX_QUEUE_STOP\",\n\t \"ETH_RAMROD_TX_QUEUE_START\", \"ETH_RAMROD_TX_QUEUE_STOP\",\n\t \"ETH_RAMROD_FILTERS_UPDATE\", \"ETH_RAMROD_RX_QUEUE_UPDATE\",\n\t \"ETH_RAMROD_RX_CREATE_OPENFLOW_ACTION\",\n\t \"ETH_RAMROD_RX_ADD_OPENFLOW_FILTER\",\n\t \"ETH_RAMROD_RX_DELETE_OPENFLOW_FILTER\",\n\t \"ETH_RAMROD_RX_ADD_UDP_FILTER\",\n\t \"ETH_RAMROD_RX_DELETE_UDP_FILTER\",\n\t \"ETH_RAMROD_RX_CREATE_GFT_ACTION\",\n\t \"ETH_RAMROD_RX_UPDATE_GFT_FILTER\", \"ETH_RAMROD_TX_QUEUE_UPDATE\",\n\t \"ETH_RAMROD_RGFS_FILTER_ADD\", \"ETH_RAMROD_RGFS_FILTER_DEL\",\n\t \"ETH_RAMROD_TGFS_FILTER_ADD\", \"ETH_RAMROD_TGFS_FILTER_DEL\",\n\t \"ETH_RAMROD_GFS_COUNTERS_REPORT_REQUEST\", },\n\t{ \"RDMA_RAMROD_UNUSED\", \"RDMA_RAMROD_FUNC_INIT\",\n\t \"RDMA_RAMROD_FUNC_CLOSE\", \"RDMA_RAMROD_REGISTER_MR\",\n\t \"RDMA_RAMROD_DEREGISTER_MR\", \"RDMA_RAMROD_CREATE_CQ\",\n\t \"RDMA_RAMROD_RESIZE_CQ\", \"RDMA_RAMROD_DESTROY_CQ\",\n\t \"RDMA_RAMROD_CREATE_SRQ\", \"RDMA_RAMROD_MODIFY_SRQ\",\n\t \"RDMA_RAMROD_DESTROY_SRQ\", \"RDMA_RAMROD_START_NS_TRACKING\",\n\t \"RDMA_RAMROD_STOP_NS_TRACKING\",\n\t \"IWARP_RAMROD_CMD_ID_TCP_OFFLOAD\",\n\t \"IWARP_RAMROD_CMD_ID_MPA_OFFLOAD\",\n\t \"IWARP_RAMROD_CMD_ID_MPA_OFFLOAD_SEND_RTR\",\n\t \"IWARP_RAMROD_CMD_ID_CREATE_QP\", \"IWARP_RAMROD_CMD_ID_QUERY_QP\",\n\t \"IWARP_RAMROD_CMD_ID_MODIFY_QP\",\n\t \"IWARP_RAMROD_CMD_ID_DESTROY_QP\",\n\t \"IWARP_RAMROD_CMD_ID_ABORT_TCP_OFFLOAD\", },\n\t{ NULL },  \n\t{ NULL },  \n\t{ \"COMMON_RAMROD_UNUSED\", \"COMMON_RAMROD_PF_START\",\n\t     \"COMMON_RAMROD_PF_STOP\", \"COMMON_RAMROD_VF_START\",\n\t     \"COMMON_RAMROD_VF_STOP\", \"COMMON_RAMROD_PF_UPDATE\",\n\t     \"COMMON_RAMROD_RL_UPDATE\", \"COMMON_RAMROD_EMPTY\", }\n};\n\n \n\n \nstatic u8 qed_get_ext_voq(struct qed_hwfn *p_hwfn,\n\t\t\t  u8 port_id, u8 tc, u8 max_phys_tcs_per_port)\n{\n\tif (tc == PURE_LB_TC)\n\t\treturn NUM_OF_PHYS_TCS * MAX_NUM_PORTS_BB + port_id;\n\telse\n\t\treturn port_id * max_phys_tcs_per_port + tc;\n}\n\n \nstatic void qed_enable_pf_rl(struct qed_hwfn *p_hwfn, bool pf_rl_en)\n{\n\tSTORE_RT_REG(p_hwfn, QM_REG_RLPFENABLE_RT_OFFSET, pf_rl_en ? 1 : 0);\n\tif (pf_rl_en) {\n\t\tu8 num_ext_voqs = MAX_NUM_VOQS;\n\t\tu64 voq_bit_mask = ((u64)1 << num_ext_voqs) - 1;\n\n\t\t \n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_RLPFVOQENABLE_RT_OFFSET,\n\t\t\t     (u32)voq_bit_mask);\n\n\t\t \n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_RLPFPERIOD_RT_OFFSET, QM_RL_PERIOD_CLK_25M);\n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_RLPFPERIODTIMER_RT_OFFSET,\n\t\t\t     QM_RL_PERIOD_CLK_25M);\n\n\t\t \n\t\tif (QM_BYPASS_EN)\n\t\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t\t     QM_REG_AFULLQMBYPTHRPFRL_RT_OFFSET,\n\t\t\t\t     QM_PF_RL_UPPER_BOUND);\n\t}\n}\n\n \nstatic void qed_enable_pf_wfq(struct qed_hwfn *p_hwfn, bool pf_wfq_en)\n{\n\tSTORE_RT_REG(p_hwfn, QM_REG_WFQPFENABLE_RT_OFFSET, pf_wfq_en ? 1 : 0);\n\n\t \n\tif (pf_wfq_en && QM_BYPASS_EN)\n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_AFULLQMBYPTHRPFWFQ_RT_OFFSET,\n\t\t\t     QM_PF_WFQ_UPPER_BOUND);\n}\n\n \nstatic void qed_enable_global_rl(struct qed_hwfn *p_hwfn, bool global_rl_en)\n{\n\tSTORE_RT_REG(p_hwfn, QM_REG_RLGLBLENABLE_RT_OFFSET,\n\t\t     global_rl_en ? 1 : 0);\n\tif (global_rl_en) {\n\t\t \n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_RLGLBLPERIOD_0_RT_OFFSET,\n\t\t\t     QM_RL_PERIOD_CLK_25M);\n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_RLGLBLPERIODTIMER_0_RT_OFFSET,\n\t\t\t     QM_RL_PERIOD_CLK_25M);\n\n\t\t \n\t\tif (QM_BYPASS_EN)\n\t\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t\t     QM_REG_AFULLQMBYPTHRGLBLRL_RT_OFFSET,\n\t\t\t\t     QM_GLOBAL_RL_UPPER_BOUND(10000) - 1);\n\t}\n}\n\n \nstatic void qed_enable_vport_wfq(struct qed_hwfn *p_hwfn, bool vport_wfq_en)\n{\n\tSTORE_RT_REG(p_hwfn, QM_REG_WFQVPENABLE_RT_OFFSET,\n\t\t     vport_wfq_en ? 1 : 0);\n\n\t \n\tif (vport_wfq_en && QM_BYPASS_EN)\n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_AFULLQMBYPTHRVPWFQ_RT_OFFSET,\n\t\t\t     QM_VP_WFQ_BYPASS_THRESH);\n}\n\n \nstatic void qed_cmdq_lines_voq_rt_init(struct qed_hwfn *p_hwfn,\n\t\t\t\t       u8 ext_voq, u16 cmdq_lines)\n{\n\tu32 qm_line_crd = QM_VOQ_LINE_CRD(cmdq_lines);\n\n\tOVERWRITE_RT_REG(p_hwfn, PBF_CMDQ_LINES_RT_OFFSET(ext_voq),\n\t\t\t (u32)cmdq_lines);\n\tSTORE_RT_REG(p_hwfn, QM_REG_VOQCRDLINE_RT_OFFSET + ext_voq,\n\t\t     qm_line_crd);\n\tSTORE_RT_REG(p_hwfn, QM_REG_VOQINITCRDLINE_RT_OFFSET + ext_voq,\n\t\t     qm_line_crd);\n}\n\n \nstatic void\nqed_cmdq_lines_rt_init(struct qed_hwfn *p_hwfn,\n\t\t       u8 max_ports_per_engine,\n\t\t       u8 max_phys_tcs_per_port,\n\t\t       struct init_qm_port_params port_params[MAX_NUM_PORTS])\n{\n\tu8 tc, ext_voq, port_id, num_tcs_in_port;\n\tu8 num_ext_voqs = MAX_NUM_VOQS;\n\n\t \n\tfor (ext_voq = 0; ext_voq < num_ext_voqs; ext_voq++)\n\t\tSTORE_RT_REG(p_hwfn, PBF_CMDQ_LINES_RT_OFFSET(ext_voq), 0);\n\n\tfor (port_id = 0; port_id < max_ports_per_engine; port_id++) {\n\t\tu16 phys_lines, phys_lines_per_tc;\n\n\t\tif (!port_params[port_id].active)\n\t\t\tcontinue;\n\n\t\t \n\t\tphys_lines = port_params[port_id].num_pbf_cmd_lines;\n\t\tphys_lines -= PBF_CMDQ_PURE_LB_LINES;\n\n\t\t \n\t\tnum_tcs_in_port = 0;\n\t\tfor (tc = 0; tc < max_phys_tcs_per_port; tc++)\n\t\t\tif (((port_params[port_id].active_phys_tcs >>\n\t\t\t      tc) & 0x1) == 1)\n\t\t\t\tnum_tcs_in_port++;\n\t\tphys_lines_per_tc = phys_lines / num_tcs_in_port;\n\n\t\t \n\t\tfor (tc = 0; tc < max_phys_tcs_per_port; tc++) {\n\t\t\text_voq = qed_get_ext_voq(p_hwfn,\n\t\t\t\t\t\t  port_id,\n\t\t\t\t\t\t  tc, max_phys_tcs_per_port);\n\t\t\tif (((port_params[port_id].active_phys_tcs >>\n\t\t\t      tc) & 0x1) == 1)\n\t\t\t\tqed_cmdq_lines_voq_rt_init(p_hwfn,\n\t\t\t\t\t\t\t   ext_voq,\n\t\t\t\t\t\t\t   phys_lines_per_tc);\n\t\t}\n\n\t\t \n\t\text_voq = qed_get_ext_voq(p_hwfn,\n\t\t\t\t\t  port_id,\n\t\t\t\t\t  PURE_LB_TC, max_phys_tcs_per_port);\n\t\tqed_cmdq_lines_voq_rt_init(p_hwfn, ext_voq,\n\t\t\t\t\t   PBF_CMDQ_PURE_LB_LINES);\n\t}\n}\n\n \nstatic void\nqed_btb_blocks_rt_init(struct qed_hwfn *p_hwfn,\n\t\t       u8 max_ports_per_engine,\n\t\t       u8 max_phys_tcs_per_port,\n\t\t       struct init_qm_port_params port_params[MAX_NUM_PORTS])\n{\n\tu32 usable_blocks, pure_lb_blocks, phys_blocks;\n\tu8 tc, ext_voq, port_id, num_tcs_in_port;\n\n\tfor (port_id = 0; port_id < max_ports_per_engine; port_id++) {\n\t\tif (!port_params[port_id].active)\n\t\t\tcontinue;\n\n\t\t \n\t\tusable_blocks = port_params[port_id].num_btb_blocks -\n\t\t\t\tBTB_HEADROOM_BLOCKS;\n\n\t\t \n\t\tnum_tcs_in_port = 0;\n\t\tfor (tc = 0; tc < NUM_OF_PHYS_TCS; tc++)\n\t\t\tif (((port_params[port_id].active_phys_tcs >>\n\t\t\t      tc) & 0x1) == 1)\n\t\t\t\tnum_tcs_in_port++;\n\n\t\tpure_lb_blocks = (usable_blocks * BTB_PURE_LB_FACTOR) /\n\t\t\t\t (num_tcs_in_port * BTB_PURE_LB_FACTOR +\n\t\t\t\t  BTB_PURE_LB_RATIO);\n\t\tpure_lb_blocks = max_t(u32, BTB_JUMBO_PKT_BLOCKS,\n\t\t\t\t       pure_lb_blocks / BTB_PURE_LB_FACTOR);\n\t\tphys_blocks = (usable_blocks - pure_lb_blocks) /\n\t\t\t      num_tcs_in_port;\n\n\t\t \n\t\tfor (tc = 0; tc < NUM_OF_PHYS_TCS; tc++) {\n\t\t\tif (((port_params[port_id].active_phys_tcs >>\n\t\t\t      tc) & 0x1) == 1) {\n\t\t\t\text_voq =\n\t\t\t\t\tqed_get_ext_voq(p_hwfn,\n\t\t\t\t\t\t\tport_id,\n\t\t\t\t\t\t\ttc,\n\t\t\t\t\t\t\tmax_phys_tcs_per_port);\n\t\t\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t\t\t     PBF_BTB_GUARANTEED_RT_OFFSET\n\t\t\t\t\t     (ext_voq), phys_blocks);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\text_voq = qed_get_ext_voq(p_hwfn,\n\t\t\t\t\t  port_id,\n\t\t\t\t\t  PURE_LB_TC, max_phys_tcs_per_port);\n\t\tSTORE_RT_REG(p_hwfn, PBF_BTB_GUARANTEED_RT_OFFSET(ext_voq),\n\t\t\t     pure_lb_blocks);\n\t}\n}\n\n \nstatic int qed_global_rl_rt_init(struct qed_hwfn *p_hwfn)\n{\n\tu32 upper_bound = QM_GLOBAL_RL_UPPER_BOUND(QM_MAX_LINK_SPEED) |\n\t\t\t  (u32)QM_RL_CRD_REG_SIGN_BIT;\n\tu32 inc_val;\n\tu16 rl_id;\n\n\t \n\tfor (rl_id = 0; rl_id < MAX_QM_GLOBAL_RLS; rl_id++) {\n\t\tinc_val = QM_RL_INC_VAL(QM_MAX_LINK_SPEED);\n\n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_RLGLBLCRD_RT_OFFSET + rl_id,\n\t\t\t     (u32)QM_RL_CRD_REG_SIGN_BIT);\n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_RLGLBLUPPERBOUND_RT_OFFSET + rl_id,\n\t\t\t     upper_bound);\n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_RLGLBLINCVAL_RT_OFFSET + rl_id, inc_val);\n\t}\n\n\treturn 0;\n}\n\n \nstatic u32 qed_get_vport_rl_upper_bound(enum init_qm_rl_type vport_rl_type,\n\t\t\t\t\tu32 link_speed)\n{\n\tswitch (vport_rl_type) {\n\tcase QM_RL_TYPE_NORMAL:\n\t\treturn QM_INITIAL_VOQ_BYTE_CRD;\n\tcase QM_RL_TYPE_QCN:\n\t\treturn QM_GLOBAL_RL_UPPER_BOUND(link_speed);\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\n \nstatic int qed_vport_rl_rt_init(struct qed_hwfn *p_hwfn,\n\t\t\t\tu16 start_rl,\n\t\t\t\tu16 num_rls,\n\t\t\t\tu32 link_speed,\n\t\t\t\tstruct init_qm_rl_params *rl_params)\n{\n\tu16 i, rl_id;\n\n\tif (num_rls && start_rl + num_rls >= MAX_QM_GLOBAL_RLS) {\n\t\tDP_NOTICE(p_hwfn, \"Invalid rate limiter configuration\\n\");\n\t\treturn -1;\n\t}\n\n\t \n\tfor (i = 0, rl_id = start_rl; i < num_rls; i++, rl_id++) {\n\t\tu32 upper_bound, inc_val;\n\n\t\tupper_bound =\n\t\t    qed_get_vport_rl_upper_bound((enum init_qm_rl_type)\n\t\t\t\t\t\t rl_params[i].vport_rl_type,\n\t\t\t\t\t\t link_speed);\n\n\t\tinc_val =\n\t\t    QM_RL_INC_VAL(rl_params[i].vport_rl ?\n\t\t\t\t  rl_params[i].vport_rl : link_speed);\n\t\tif (inc_val > upper_bound) {\n\t\t\tDP_NOTICE(p_hwfn,\n\t\t\t\t  \"Invalid RL rate - limit configuration\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tSTORE_RT_REG(p_hwfn, QM_REG_RLGLBLCRD_RT_OFFSET + rl_id,\n\t\t\t     (u32)QM_RL_CRD_REG_SIGN_BIT);\n\t\tSTORE_RT_REG(p_hwfn, QM_REG_RLGLBLUPPERBOUND_RT_OFFSET + rl_id,\n\t\t\t     upper_bound | (u32)QM_RL_CRD_REG_SIGN_BIT);\n\t\tSTORE_RT_REG(p_hwfn, QM_REG_RLGLBLINCVAL_RT_OFFSET + rl_id,\n\t\t\t     inc_val);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int qed_tx_pq_map_rt_init(struct qed_hwfn *p_hwfn,\n\t\t\t\t struct qed_ptt *p_ptt,\n\t\t\t\t struct qed_qm_pf_rt_init_params *p_params,\n\t\t\t\t u32 base_mem_addr_4kb)\n{\n\tu32 tx_pq_vf_mask[MAX_QM_TX_QUEUES / QM_PF_QUEUE_GROUP_SIZE] = { 0 };\n\tstruct init_qm_vport_params *vport_params = p_params->vport_params;\n\tu32 num_tx_pq_vf_masks = MAX_QM_TX_QUEUES / QM_PF_QUEUE_GROUP_SIZE;\n\tu16 num_pqs, first_pq_group, last_pq_group, i, j, pq_id, pq_group;\n\tstruct init_qm_pq_params *pq_params = p_params->pq_params;\n\tu32 pq_mem_4kb, vport_pq_mem_4kb, mem_addr_4kb;\n\n\tnum_pqs = p_params->num_pf_pqs + p_params->num_vf_pqs;\n\n\tfirst_pq_group = p_params->start_pq / QM_PF_QUEUE_GROUP_SIZE;\n\tlast_pq_group = (p_params->start_pq + num_pqs - 1) /\n\t\t\tQM_PF_QUEUE_GROUP_SIZE;\n\n\tpq_mem_4kb = QM_PQ_MEM_4KB(p_params->num_pf_cids);\n\tvport_pq_mem_4kb = QM_PQ_MEM_4KB(p_params->num_vf_cids);\n\tmem_addr_4kb = base_mem_addr_4kb;\n\n\t \n\tfor (pq_group = first_pq_group; pq_group <= last_pq_group; pq_group++)\n\t\tSTORE_RT_REG(p_hwfn, QM_REG_PQTX2PF_0_RT_OFFSET + pq_group,\n\t\t\t     (u32)(p_params->pf_id));\n\n\t \n\tSTORE_RT_REG(p_hwfn, QM_REG_MAXPQSIZE_0_RT_OFFSET,\n\t\t     QM_PQ_SIZE_256B(p_params->num_pf_cids));\n\tSTORE_RT_REG(p_hwfn, QM_REG_MAXPQSIZE_1_RT_OFFSET,\n\t\t     QM_PQ_SIZE_256B(p_params->num_vf_cids));\n\n\t \n\tfor (i = 0, pq_id = p_params->start_pq; i < num_pqs; i++, pq_id++) {\n\t\tu16 *p_first_tx_pq_id, vport_id_in_pf;\n\t\tstruct qm_rf_pq_map tx_pq_map;\n\t\tu8 tc_id = pq_params[i].tc_id;\n\t\tbool is_vf_pq;\n\t\tu8 ext_voq;\n\n\t\text_voq = qed_get_ext_voq(p_hwfn,\n\t\t\t\t\t  pq_params[i].port_id,\n\t\t\t\t\t  tc_id,\n\t\t\t\t\t  p_params->max_phys_tcs_per_port);\n\t\tis_vf_pq = (i >= p_params->num_pf_pqs);\n\n\t\t \n\t\tvport_id_in_pf = pq_params[i].vport_id - p_params->start_vport;\n\t\tp_first_tx_pq_id =\n\t\t    &vport_params[vport_id_in_pf].first_tx_pq_id[tc_id];\n\t\tif (*p_first_tx_pq_id == QM_INVALID_PQ_ID) {\n\t\t\tu32 map_val =\n\t\t\t\t(ext_voq << QM_VP_WFQ_PQ_VOQ_SHIFT) |\n\t\t\t\t(p_params->pf_id << QM_VP_WFQ_PQ_PF_SHIFT);\n\n\t\t\t \n\t\t\t*p_first_tx_pq_id = pq_id;\n\n\t\t\t \n\t\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t\t     QM_REG_WFQVPMAP_RT_OFFSET +\n\t\t\t\t     *p_first_tx_pq_id,\n\t\t\t\t     map_val);\n\t\t}\n\n\t\t \n\t\tQM_INIT_TX_PQ_MAP(p_hwfn,\n\t\t\t\t  tx_pq_map,\n\t\t\t\t  pq_id,\n\t\t\t\t  *p_first_tx_pq_id,\n\t\t\t\t  pq_params[i].rl_valid,\n\t\t\t\t  pq_params[i].rl_id,\n\t\t\t\t  ext_voq, pq_params[i].wrr_group);\n\n\t\t \n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_BASEADDRTXPQ_RT_OFFSET + pq_id,\n\t\t\t     mem_addr_4kb);\n\n\t\t \n\t\tif (p_params->is_pf_loading)\n\t\t\tfor (j = 0; j < 2; j++)\n\t\t\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t\t\t     QM_REG_PTRTBLTX_RT_OFFSET +\n\t\t\t\t\t     (pq_id * 2) + j, 0);\n\n\t\t \n\t\tif (WRITE_PQ_INFO_TO_RAM != 0) {\n\t\t\tu32 pq_info = 0;\n\n\t\t\tpq_info = PQ_INFO_ELEMENT(*p_first_tx_pq_id,\n\t\t\t\t\t\t  p_params->pf_id,\n\t\t\t\t\t\t  tc_id,\n\t\t\t\t\t\t  pq_params[i].port_id,\n\t\t\t\t\t\t  pq_params[i].rl_valid,\n\t\t\t\t\t\t  pq_params[i].rl_id);\n\t\t\tqed_wr(p_hwfn, p_ptt, PQ_INFO_RAM_GRC_ADDRESS(pq_id),\n\t\t\t       pq_info);\n\t\t}\n\n\t\t \n\t\tif (is_vf_pq) {\n\t\t\ttx_pq_vf_mask[pq_id /\n\t\t\t\t      QM_PF_QUEUE_GROUP_SIZE] |=\n\t\t\t    BIT((pq_id % QM_PF_QUEUE_GROUP_SIZE));\n\t\t\tmem_addr_4kb += vport_pq_mem_4kb;\n\t\t} else {\n\t\t\tmem_addr_4kb += pq_mem_4kb;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < num_tx_pq_vf_masks; i++)\n\t\tif (tx_pq_vf_mask[i])\n\t\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t\t     QM_REG_MAXPQSIZETXSEL_0_RT_OFFSET + i,\n\t\t\t\t     tx_pq_vf_mask[i]);\n\n\treturn 0;\n}\n\n \nstatic void qed_other_pq_map_rt_init(struct qed_hwfn *p_hwfn,\n\t\t\t\t     u8 pf_id,\n\t\t\t\t     bool is_pf_loading,\n\t\t\t\t     u32 num_pf_cids,\n\t\t\t\t     u32 num_tids, u32 base_mem_addr_4kb)\n{\n\tu32 pq_size, pq_mem_4kb, mem_addr_4kb;\n\tu16 i, j, pq_id, pq_group;\n\n\t \n\tpq_group = pf_id;\n\tpq_size = num_pf_cids + num_tids;\n\tpq_mem_4kb = QM_PQ_MEM_4KB(pq_size);\n\tmem_addr_4kb = base_mem_addr_4kb;\n\n\t \n\tSTORE_RT_REG(p_hwfn, QM_REG_PQOTHER2PF_0_RT_OFFSET + pq_group,\n\t\t     (u32)(pf_id));\n\n\t \n\tSTORE_RT_REG(p_hwfn, QM_REG_MAXPQSIZE_2_RT_OFFSET,\n\t\t     QM_PQ_SIZE_256B(pq_size));\n\n\tfor (i = 0, pq_id = pf_id * QM_PF_QUEUE_GROUP_SIZE;\n\t     i < QM_OTHER_PQS_PER_PF; i++, pq_id++) {\n\t\t \n\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t     QM_REG_BASEADDROTHERPQ_RT_OFFSET + pq_id,\n\t\t\t     mem_addr_4kb);\n\n\t\t \n\t\tif (is_pf_loading)\n\t\t\tfor (j = 0; j < 2; j++)\n\t\t\t\tSTORE_RT_REG(p_hwfn,\n\t\t\t\t\t     QM_REG_PTRTBLOTHER_RT_OFFSET +\n\t\t\t\t\t     (pq_id * 2) + j, 0);\n\n\t\tmem_addr_4kb += pq_mem_4kb;\n\t}\n}\n\n \nstatic int qed_pf_wfq_rt_init(struct qed_hwfn *p_hwfn,\n\t\t\t      struct qed_qm_pf_rt_init_params *p_params)\n{\n\tu16 num_tx_pqs = p_params->num_pf_pqs + p_params->num_vf_pqs;\n\tstruct init_qm_pq_params *pq_params = p_params->pq_params;\n\tu32 inc_val, crd_reg_offset;\n\tu8 ext_voq;\n\tu16 i;\n\n\tinc_val = QM_PF_WFQ_INC_VAL(p_params->pf_wfq);\n\tif (!inc_val || inc_val > QM_PF_WFQ_MAX_INC_VAL) {\n\t\tDP_NOTICE(p_hwfn, \"Invalid PF WFQ weight configuration\\n\");\n\t\treturn -1;\n\t}\n\n\tfor (i = 0; i < num_tx_pqs; i++) {\n\t\text_voq = qed_get_ext_voq(p_hwfn,\n\t\t\t\t\t  pq_params[i].port_id,\n\t\t\t\t\t  pq_params[i].tc_id,\n\t\t\t\t\t  p_params->max_phys_tcs_per_port);\n\t\tcrd_reg_offset =\n\t\t\t(p_params->pf_id < MAX_NUM_PFS_BB ?\n\t\t\t QM_REG_WFQPFCRD_RT_OFFSET :\n\t\t\t QM_REG_WFQPFCRD_MSB_RT_OFFSET) +\n\t\t\text_voq * MAX_NUM_PFS_BB +\n\t\t\t(p_params->pf_id % MAX_NUM_PFS_BB);\n\t\tOVERWRITE_RT_REG(p_hwfn,\n\t\t\t\t crd_reg_offset, (u32)QM_WFQ_CRD_REG_SIGN_BIT);\n\t}\n\n\tSTORE_RT_REG(p_hwfn,\n\t\t     QM_REG_WFQPFUPPERBOUND_RT_OFFSET + p_params->pf_id,\n\t\t     QM_PF_WFQ_UPPER_BOUND | (u32)QM_WFQ_CRD_REG_SIGN_BIT);\n\tSTORE_RT_REG(p_hwfn, QM_REG_WFQPFWEIGHT_RT_OFFSET + p_params->pf_id,\n\t\t     inc_val);\n\n\treturn 0;\n}\n\n \nstatic int qed_pf_rl_rt_init(struct qed_hwfn *p_hwfn, u8 pf_id, u32 pf_rl)\n{\n\tu32 inc_val = QM_RL_INC_VAL(pf_rl);\n\n\tif (inc_val > QM_PF_RL_MAX_INC_VAL) {\n\t\tDP_NOTICE(p_hwfn, \"Invalid PF rate limit configuration\\n\");\n\t\treturn -1;\n\t}\n\n\tSTORE_RT_REG(p_hwfn,\n\t\t     QM_REG_RLPFCRD_RT_OFFSET + pf_id,\n\t\t     (u32)QM_RL_CRD_REG_SIGN_BIT);\n\tSTORE_RT_REG(p_hwfn,\n\t\t     QM_REG_RLPFUPPERBOUND_RT_OFFSET + pf_id,\n\t\t     QM_PF_RL_UPPER_BOUND | (u32)QM_RL_CRD_REG_SIGN_BIT);\n\tSTORE_RT_REG(p_hwfn, QM_REG_RLPFINCVAL_RT_OFFSET + pf_id, inc_val);\n\n\treturn 0;\n}\n\n \nstatic int qed_vp_wfq_rt_init(struct qed_hwfn *p_hwfn,\n\t\t\t      u16 num_vports,\n\t\t\t      struct init_qm_vport_params *vport_params)\n{\n\tu16 vport_pq_id, wfq, i;\n\tu32 inc_val;\n\tu8 tc;\n\n\t \n\tfor (i = 0; i < num_vports; i++) {\n\t\t \n\t\tfor (tc = 0; tc < NUM_OF_TCS; tc++) {\n\t\t\t \n\t\t\tvport_pq_id = vport_params[i].first_tx_pq_id[tc];\n\t\t\tif (vport_pq_id == QM_INVALID_PQ_ID)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\twfq = vport_params[i].wfq;\n\t\t\twfq = wfq ? wfq : vport_params[i].tc_wfq[tc];\n\t\t\tinc_val = QM_VP_WFQ_INC_VAL(wfq);\n\t\t\tif (inc_val > QM_VP_WFQ_MAX_INC_VAL) {\n\t\t\t\tDP_NOTICE(p_hwfn,\n\t\t\t\t\t  \"Invalid VPORT WFQ weight configuration\\n\");\n\t\t\t\treturn -1;\n\t\t\t}\n\n\t\t\t \n\t\t\tSTORE_RT_REG(p_hwfn, QM_REG_WFQVPCRD_RT_OFFSET +\n\t\t\t\t     vport_pq_id,\n\t\t\t\t     (u32)QM_WFQ_CRD_REG_SIGN_BIT);\n\t\t\tSTORE_RT_REG(p_hwfn, QM_REG_WFQVPUPPERBOUND_RT_OFFSET +\n\t\t\t\t     vport_pq_id,\n\t\t\t\t     inc_val | QM_WFQ_CRD_REG_SIGN_BIT);\n\t\t\tSTORE_RT_REG(p_hwfn, QM_REG_WFQVPWEIGHT_RT_OFFSET +\n\t\t\t\t     vport_pq_id, inc_val);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic bool qed_poll_on_qm_cmd_ready(struct qed_hwfn *p_hwfn,\n\t\t\t\t     struct qed_ptt *p_ptt)\n{\n\tu32 reg_val, i;\n\n\tfor (i = 0, reg_val = 0; i < QM_STOP_CMD_MAX_POLL_COUNT && !reg_val;\n\t     i++) {\n\t\tudelay(QM_STOP_CMD_POLL_PERIOD_US);\n\t\treg_val = qed_rd(p_hwfn, p_ptt, QM_REG_SDMCMDREADY);\n\t}\n\n\t \n\tif (i == QM_STOP_CMD_MAX_POLL_COUNT) {\n\t\tDP_VERBOSE(p_hwfn, NETIF_MSG_HW,\n\t\t\t   \"Timeout when waiting for QM SDM command ready signal\\n\");\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool qed_send_qm_cmd(struct qed_hwfn *p_hwfn,\n\t\t\t    struct qed_ptt *p_ptt,\n\t\t\t    u32 cmd_addr, u32 cmd_data_lsb, u32 cmd_data_msb)\n{\n\tif (!qed_poll_on_qm_cmd_ready(p_hwfn, p_ptt))\n\t\treturn false;\n\n\tqed_wr(p_hwfn, p_ptt, QM_REG_SDMCMDADDR, cmd_addr);\n\tqed_wr(p_hwfn, p_ptt, QM_REG_SDMCMDDATALSB, cmd_data_lsb);\n\tqed_wr(p_hwfn, p_ptt, QM_REG_SDMCMDDATAMSB, cmd_data_msb);\n\tqed_wr(p_hwfn, p_ptt, QM_REG_SDMCMDGO, 1);\n\tqed_wr(p_hwfn, p_ptt, QM_REG_SDMCMDGO, 0);\n\n\treturn qed_poll_on_qm_cmd_ready(p_hwfn, p_ptt);\n}\n\n \n\nu32 qed_qm_pf_mem_size(u32 num_pf_cids,\n\t\t       u32 num_vf_cids,\n\t\t       u32 num_tids, u16 num_pf_pqs, u16 num_vf_pqs)\n{\n\treturn QM_PQ_MEM_4KB(num_pf_cids) * num_pf_pqs +\n\t       QM_PQ_MEM_4KB(num_vf_cids) * num_vf_pqs +\n\t       QM_PQ_MEM_4KB(num_pf_cids + num_tids) * QM_OTHER_PQS_PER_PF;\n}\n\nint qed_qm_common_rt_init(struct qed_hwfn *p_hwfn,\n\t\t\t  struct qed_qm_common_rt_init_params *p_params)\n{\n\tu32 mask = 0;\n\n\t \n\tSET_FIELD(mask, QM_RF_OPPORTUNISTIC_MASK_LINEVOQ,\n\t\t  QM_OPPOR_LINE_VOQ_DEF);\n\tSET_FIELD(mask, QM_RF_OPPORTUNISTIC_MASK_BYTEVOQ, QM_BYTE_CRD_EN);\n\tSET_FIELD(mask, QM_RF_OPPORTUNISTIC_MASK_PFWFQ,\n\t\t  p_params->pf_wfq_en ? 1 : 0);\n\tSET_FIELD(mask, QM_RF_OPPORTUNISTIC_MASK_VPWFQ,\n\t\t  p_params->vport_wfq_en ? 1 : 0);\n\tSET_FIELD(mask, QM_RF_OPPORTUNISTIC_MASK_PFRL,\n\t\t  p_params->pf_rl_en ? 1 : 0);\n\tSET_FIELD(mask, QM_RF_OPPORTUNISTIC_MASK_VPQCNRL,\n\t\t  p_params->global_rl_en ? 1 : 0);\n\tSET_FIELD(mask, QM_RF_OPPORTUNISTIC_MASK_FWPAUSE, QM_OPPOR_FW_STOP_DEF);\n\tSET_FIELD(mask,\n\t\t  QM_RF_OPPORTUNISTIC_MASK_QUEUEEMPTY, QM_OPPOR_PQ_EMPTY_DEF);\n\tSTORE_RT_REG(p_hwfn, QM_REG_AFULLOPRTNSTCCRDMASK_RT_OFFSET, mask);\n\n\t \n\tqed_enable_pf_rl(p_hwfn, p_params->pf_rl_en);\n\n\t \n\tqed_enable_pf_wfq(p_hwfn, p_params->pf_wfq_en);\n\n\t \n\tqed_enable_global_rl(p_hwfn, p_params->global_rl_en);\n\n\t \n\tqed_enable_vport_wfq(p_hwfn, p_params->vport_wfq_en);\n\n\t \n\tqed_cmdq_lines_rt_init(p_hwfn,\n\t\t\t       p_params->max_ports_per_engine,\n\t\t\t       p_params->max_phys_tcs_per_port,\n\t\t\t       p_params->port_params);\n\n\t \n\tqed_btb_blocks_rt_init(p_hwfn,\n\t\t\t       p_params->max_ports_per_engine,\n\t\t\t       p_params->max_phys_tcs_per_port,\n\t\t\t       p_params->port_params);\n\n\tqed_global_rl_rt_init(p_hwfn);\n\n\treturn 0;\n}\n\nint qed_qm_pf_rt_init(struct qed_hwfn *p_hwfn,\n\t\t      struct qed_ptt *p_ptt,\n\t\t      struct qed_qm_pf_rt_init_params *p_params)\n{\n\tstruct init_qm_vport_params *vport_params = p_params->vport_params;\n\tu32 other_mem_size_4kb = QM_PQ_MEM_4KB(p_params->num_pf_cids +\n\t\t\t\t\t       p_params->num_tids) *\n\t\t\t\t QM_OTHER_PQS_PER_PF;\n\tu16 i;\n\tu8 tc;\n\n\t \n\tfor (i = 0; i < p_params->num_vports; i++)\n\t\tfor (tc = 0; tc < NUM_OF_TCS; tc++)\n\t\t\tvport_params[i].first_tx_pq_id[tc] = QM_INVALID_PQ_ID;\n\n\t \n\tqed_other_pq_map_rt_init(p_hwfn,\n\t\t\t\t p_params->pf_id,\n\t\t\t\t p_params->is_pf_loading, p_params->num_pf_cids,\n\t\t\t\t p_params->num_tids, 0);\n\n\t \n\tif (qed_tx_pq_map_rt_init(p_hwfn, p_ptt, p_params, other_mem_size_4kb))\n\t\treturn -1;\n\n\t \n\tif (p_params->pf_wfq)\n\t\tif (qed_pf_wfq_rt_init(p_hwfn, p_params))\n\t\t\treturn -1;\n\n\t \n\tif (qed_pf_rl_rt_init(p_hwfn, p_params->pf_id, p_params->pf_rl))\n\t\treturn -1;\n\n\t \n\tif (qed_vp_wfq_rt_init(p_hwfn, p_params->num_vports, vport_params))\n\t\treturn -1;\n\n\t \n\tif (qed_vport_rl_rt_init(p_hwfn, p_params->start_rl,\n\t\t\t\t p_params->num_rls, p_params->link_speed,\n\t\t\t\t p_params->rl_params))\n\t\treturn -1;\n\n\treturn 0;\n}\n\nint qed_init_pf_wfq(struct qed_hwfn *p_hwfn,\n\t\t    struct qed_ptt *p_ptt, u8 pf_id, u16 pf_wfq)\n{\n\tu32 inc_val = QM_PF_WFQ_INC_VAL(pf_wfq);\n\n\tif (!inc_val || inc_val > QM_PF_WFQ_MAX_INC_VAL) {\n\t\tDP_NOTICE(p_hwfn, \"Invalid PF WFQ weight configuration\\n\");\n\t\treturn -1;\n\t}\n\n\tqed_wr(p_hwfn, p_ptt, QM_REG_WFQPFWEIGHT + pf_id * 4, inc_val);\n\n\treturn 0;\n}\n\nint qed_init_pf_rl(struct qed_hwfn *p_hwfn,\n\t\t   struct qed_ptt *p_ptt, u8 pf_id, u32 pf_rl)\n{\n\tu32 inc_val = QM_RL_INC_VAL(pf_rl);\n\n\tif (inc_val > QM_PF_RL_MAX_INC_VAL) {\n\t\tDP_NOTICE(p_hwfn, \"Invalid PF rate limit configuration\\n\");\n\t\treturn -1;\n\t}\n\n\tqed_wr(p_hwfn,\n\t       p_ptt, QM_REG_RLPFCRD + pf_id * 4, (u32)QM_RL_CRD_REG_SIGN_BIT);\n\tqed_wr(p_hwfn, p_ptt, QM_REG_RLPFINCVAL + pf_id * 4, inc_val);\n\n\treturn 0;\n}\n\nint qed_init_vport_wfq(struct qed_hwfn *p_hwfn,\n\t\t       struct qed_ptt *p_ptt,\n\t\t       u16 first_tx_pq_id[NUM_OF_TCS], u16 wfq)\n{\n\tint result = 0;\n\tu16 vport_pq_id;\n\tu8 tc;\n\n\tfor (tc = 0; tc < NUM_OF_TCS && !result; tc++) {\n\t\tvport_pq_id = first_tx_pq_id[tc];\n\t\tif (vport_pq_id != QM_INVALID_PQ_ID)\n\t\t\tresult = qed_init_vport_tc_wfq(p_hwfn, p_ptt,\n\t\t\t\t\t\t       vport_pq_id, wfq);\n\t}\n\n\treturn result;\n}\n\nint qed_init_vport_tc_wfq(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt,\n\t\t\t  u16 first_tx_pq_id, u16 wfq)\n{\n\tu32 inc_val;\n\n\tif (first_tx_pq_id == QM_INVALID_PQ_ID)\n\t\treturn -1;\n\n\tinc_val = QM_VP_WFQ_INC_VAL(wfq);\n\tif (!inc_val || inc_val > QM_VP_WFQ_MAX_INC_VAL) {\n\t\tDP_NOTICE(p_hwfn, \"Invalid VPORT WFQ configuration.\\n\");\n\t\treturn -1;\n\t}\n\n\tqed_wr(p_hwfn, p_ptt, QM_REG_WFQVPCRD + first_tx_pq_id * 4,\n\t       (u32)QM_WFQ_CRD_REG_SIGN_BIT);\n\tqed_wr(p_hwfn, p_ptt, QM_REG_WFQVPUPPERBOUND + first_tx_pq_id * 4,\n\t       inc_val | QM_WFQ_CRD_REG_SIGN_BIT);\n\tqed_wr(p_hwfn, p_ptt, QM_REG_WFQVPWEIGHT + first_tx_pq_id * 4,\n\t       inc_val);\n\n\treturn 0;\n}\n\nint qed_init_global_rl(struct qed_hwfn *p_hwfn,\n\t\t       struct qed_ptt *p_ptt, u16 rl_id, u32 rate_limit,\n\t\t       enum init_qm_rl_type vport_rl_type)\n{\n\tu32 inc_val, upper_bound;\n\n\tupper_bound =\n\t    (vport_rl_type ==\n\t     QM_RL_TYPE_QCN) ? QM_GLOBAL_RL_UPPER_BOUND(QM_MAX_LINK_SPEED) :\n\t    QM_INITIAL_VOQ_BYTE_CRD;\n\tinc_val = QM_RL_INC_VAL(rate_limit);\n\tif (inc_val > upper_bound) {\n\t\tDP_NOTICE(p_hwfn, \"Invalid VPORT rate limit configuration.\\n\");\n\t\treturn -1;\n\t}\n\n\tqed_wr(p_hwfn, p_ptt,\n\t       QM_REG_RLGLBLCRD + rl_id * 4, (u32)QM_RL_CRD_REG_SIGN_BIT);\n\tqed_wr(p_hwfn,\n\t       p_ptt,\n\t       QM_REG_RLGLBLUPPERBOUND + rl_id * 4,\n\t       upper_bound | (u32)QM_RL_CRD_REG_SIGN_BIT);\n\tqed_wr(p_hwfn, p_ptt, QM_REG_RLGLBLINCVAL + rl_id * 4, inc_val);\n\n\treturn 0;\n}\n\nbool qed_send_qm_stop_cmd(struct qed_hwfn *p_hwfn,\n\t\t\t  struct qed_ptt *p_ptt,\n\t\t\t  bool is_release_cmd,\n\t\t\t  bool is_tx_pq, u16 start_pq, u16 num_pqs)\n{\n\tu32 cmd_arr[QM_CMD_STRUCT_SIZE(QM_STOP_CMD)] = { 0 };\n\tu32 pq_mask = 0, last_pq, pq_id;\n\n\tlast_pq = start_pq + num_pqs - 1;\n\n\t \n\tQM_CMD_SET_FIELD(cmd_arr, QM_STOP_CMD, PQ_TYPE, is_tx_pq ? 0 : 1);\n\n\t \n\tfor (pq_id = start_pq; pq_id <= last_pq; pq_id++) {\n\t\t \n\t\tif (!is_release_cmd)\n\t\t\tpq_mask |= BIT((pq_id % QM_STOP_PQ_MASK_WIDTH));\n\n\t\t \n\t\tif ((pq_id == last_pq) ||\n\t\t    (pq_id % QM_STOP_PQ_MASK_WIDTH ==\n\t\t     (QM_STOP_PQ_MASK_WIDTH - 1))) {\n\t\t\tQM_CMD_SET_FIELD(cmd_arr,\n\t\t\t\t\t QM_STOP_CMD, PAUSE_MASK, pq_mask);\n\t\t\tQM_CMD_SET_FIELD(cmd_arr,\n\t\t\t\t\t QM_STOP_CMD,\n\t\t\t\t\t GROUP_ID,\n\t\t\t\t\t pq_id / QM_STOP_PQ_MASK_WIDTH);\n\t\t\tif (!qed_send_qm_cmd(p_hwfn, p_ptt, QM_STOP_CMD_ADDR,\n\t\t\t\t\t     cmd_arr[0], cmd_arr[1]))\n\t\t\t\treturn false;\n\t\t\tpq_mask = 0;\n\t\t}\n\t}\n\n\treturn true;\n}\n\n#define SET_TUNNEL_TYPE_ENABLE_BIT(var, offset, enable) \\\n\tdo { \\\n\t\ttypeof(var) *__p_var = &(var); \\\n\t\ttypeof(offset) __offset = offset; \\\n\t\t*__p_var = (*__p_var & ~BIT(__offset)) | \\\n\t\t\t   ((enable) ? BIT(__offset) : 0); \\\n\t} while (0)\n\n#define PRS_ETH_TUNN_OUTPUT_FORMAT     0xF4DAB910\n#define PRS_ETH_OUTPUT_FORMAT          0xFFFF4910\n\n#define ARR_REG_WR(dev, ptt, addr, arr,\tarr_size) \\\n\tdo { \\\n\t\tu32 i; \\\n\t\t\\\n\t\tfor (i = 0; i < (arr_size); i++) \\\n\t\t\tqed_wr(dev, ptt, \\\n\t\t\t       ((addr) + (4 * i)), \\\n\t\t\t       ((u32 *)&(arr))[i]); \\\n\t} while (0)\n\n \nstatic int qed_dmae_to_grc(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt,\n\t\t\t   __le32 *p_data, u32 addr, u32 len_in_dwords)\n{\n\tstruct qed_dmae_params params = { 0 };\n\tu32 *data_cpu;\n\tint rc;\n\n\tif (!p_data)\n\t\treturn -1;\n\n\t \n\tSET_FIELD(params.flags, QED_DMAE_PARAMS_COMPLETION_DST, 1);\n\n\t \n\trc = qed_dmae_host2grc(p_hwfn, p_ptt,\n\t\t\t       (u64)(uintptr_t)(p_data),\n\t\t\t       addr, len_in_dwords, &params);\n\n\t \n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn,\n\t\t\t   QED_MSG_DEBUG,\n\t\t\t   \"Failed writing to chip using DMAE, using GRC instead\\n\");\n\n\t\t \n\t\tdata_cpu = (__force u32 *)p_data;\n\t\tle32_to_cpu_array(data_cpu, len_in_dwords);\n\n\t\tARR_REG_WR(p_hwfn, p_ptt, addr, data_cpu, len_in_dwords);\n\t\tcpu_to_le32_array(data_cpu, len_in_dwords);\n\t}\n\n\treturn len_in_dwords;\n}\n\nvoid qed_set_vxlan_dest_port(struct qed_hwfn *p_hwfn,\n\t\t\t     struct qed_ptt *p_ptt, u16 dest_port)\n{\n\t \n\tqed_wr(p_hwfn, p_ptt, PRS_REG_VXLAN_PORT, dest_port);\n\n\t \n\tqed_wr(p_hwfn, p_ptt, NIG_REG_VXLAN_CTRL, dest_port);\n\n\t \n\tqed_wr(p_hwfn, p_ptt, PBF_REG_VXLAN_PORT, dest_port);\n}\n\nvoid qed_set_vxlan_enable(struct qed_hwfn *p_hwfn,\n\t\t\t  struct qed_ptt *p_ptt, bool vxlan_enable)\n{\n\tu32 reg_val;\n\tu8 shift;\n\n\t \n\treg_val = qed_rd(p_hwfn, p_ptt, PRS_REG_ENCAPSULATION_TYPE_EN);\n\tSET_FIELD(reg_val,\n\t\t  PRS_REG_ENCAPSULATION_TYPE_EN_VXLAN_ENABLE, vxlan_enable);\n\tqed_wr(p_hwfn, p_ptt, PRS_REG_ENCAPSULATION_TYPE_EN, reg_val);\n\tif (reg_val) {\n\t\treg_val =\n\t\t    qed_rd(p_hwfn, p_ptt, PRS_REG_OUTPUT_FORMAT_4_0);\n\n\t\t \n\t\tif (reg_val == (u32)PRS_ETH_OUTPUT_FORMAT)\n\t\t\tqed_wr(p_hwfn, p_ptt, PRS_REG_OUTPUT_FORMAT_4_0,\n\t\t\t       (u32)PRS_ETH_TUNN_OUTPUT_FORMAT);\n\t}\n\n\t \n\treg_val = qed_rd(p_hwfn, p_ptt, NIG_REG_ENC_TYPE_ENABLE);\n\tshift = NIG_REG_ENC_TYPE_ENABLE_VXLAN_ENABLE_SHIFT;\n\tSET_TUNNEL_TYPE_ENABLE_BIT(reg_val, shift, vxlan_enable);\n\tqed_wr(p_hwfn, p_ptt, NIG_REG_ENC_TYPE_ENABLE, reg_val);\n\n\t \n\tqed_wr(p_hwfn,\n\t       p_ptt, DORQ_REG_L2_EDPM_TUNNEL_VXLAN_EN, vxlan_enable ? 1 : 0);\n}\n\nvoid qed_set_gre_enable(struct qed_hwfn *p_hwfn,\n\t\t\tstruct qed_ptt *p_ptt,\n\t\t\tbool eth_gre_enable, bool ip_gre_enable)\n{\n\tu32 reg_val;\n\tu8 shift;\n\n\t \n\treg_val = qed_rd(p_hwfn, p_ptt, PRS_REG_ENCAPSULATION_TYPE_EN);\n\tSET_FIELD(reg_val,\n\t\t  PRS_REG_ENCAPSULATION_TYPE_EN_ETH_OVER_GRE_ENABLE,\n\t\t  eth_gre_enable);\n\tSET_FIELD(reg_val,\n\t\t  PRS_REG_ENCAPSULATION_TYPE_EN_IP_OVER_GRE_ENABLE,\n\t\t  ip_gre_enable);\n\tqed_wr(p_hwfn, p_ptt, PRS_REG_ENCAPSULATION_TYPE_EN, reg_val);\n\tif (reg_val) {\n\t\treg_val =\n\t\t    qed_rd(p_hwfn, p_ptt, PRS_REG_OUTPUT_FORMAT_4_0);\n\n\t\t \n\t\tif (reg_val == (u32)PRS_ETH_OUTPUT_FORMAT)\n\t\t\tqed_wr(p_hwfn, p_ptt, PRS_REG_OUTPUT_FORMAT_4_0,\n\t\t\t       (u32)PRS_ETH_TUNN_OUTPUT_FORMAT);\n\t}\n\n\t \n\treg_val = qed_rd(p_hwfn, p_ptt, NIG_REG_ENC_TYPE_ENABLE);\n\tshift = NIG_REG_ENC_TYPE_ENABLE_ETH_OVER_GRE_ENABLE_SHIFT;\n\tSET_TUNNEL_TYPE_ENABLE_BIT(reg_val, shift, eth_gre_enable);\n\tshift = NIG_REG_ENC_TYPE_ENABLE_IP_OVER_GRE_ENABLE_SHIFT;\n\tSET_TUNNEL_TYPE_ENABLE_BIT(reg_val, shift, ip_gre_enable);\n\tqed_wr(p_hwfn, p_ptt, NIG_REG_ENC_TYPE_ENABLE, reg_val);\n\n\t \n\tqed_wr(p_hwfn,\n\t       p_ptt,\n\t       DORQ_REG_L2_EDPM_TUNNEL_GRE_ETH_EN, eth_gre_enable ? 1 : 0);\n\tqed_wr(p_hwfn,\n\t       p_ptt, DORQ_REG_L2_EDPM_TUNNEL_GRE_IP_EN, ip_gre_enable ? 1 : 0);\n}\n\nvoid qed_set_geneve_dest_port(struct qed_hwfn *p_hwfn,\n\t\t\t      struct qed_ptt *p_ptt, u16 dest_port)\n{\n\t \n\tqed_wr(p_hwfn, p_ptt, PRS_REG_NGE_PORT, dest_port);\n\n\t \n\tqed_wr(p_hwfn, p_ptt, NIG_REG_NGE_PORT, dest_port);\n\n\t \n\tqed_wr(p_hwfn, p_ptt, PBF_REG_NGE_PORT, dest_port);\n}\n\nvoid qed_set_geneve_enable(struct qed_hwfn *p_hwfn,\n\t\t\t   struct qed_ptt *p_ptt,\n\t\t\t   bool eth_geneve_enable, bool ip_geneve_enable)\n{\n\tu32 reg_val;\n\n\t \n\treg_val = qed_rd(p_hwfn, p_ptt, PRS_REG_ENCAPSULATION_TYPE_EN);\n\tSET_FIELD(reg_val,\n\t\t  PRS_REG_ENCAPSULATION_TYPE_EN_ETH_OVER_GENEVE_ENABLE,\n\t\t  eth_geneve_enable);\n\tSET_FIELD(reg_val,\n\t\t  PRS_REG_ENCAPSULATION_TYPE_EN_IP_OVER_GENEVE_ENABLE,\n\t\t  ip_geneve_enable);\n\tqed_wr(p_hwfn, p_ptt, PRS_REG_ENCAPSULATION_TYPE_EN, reg_val);\n\tif (reg_val) {\n\t\treg_val =\n\t\t    qed_rd(p_hwfn, p_ptt, PRS_REG_OUTPUT_FORMAT_4_0);\n\n\t\t \n\t\tif (reg_val == (u32)PRS_ETH_OUTPUT_FORMAT)\n\t\t\tqed_wr(p_hwfn, p_ptt, PRS_REG_OUTPUT_FORMAT_4_0,\n\t\t\t       (u32)PRS_ETH_TUNN_OUTPUT_FORMAT);\n\t}\n\n\t \n\tqed_wr(p_hwfn, p_ptt, NIG_REG_NGE_ETH_ENABLE,\n\t       eth_geneve_enable ? 1 : 0);\n\tqed_wr(p_hwfn, p_ptt, NIG_REG_NGE_IP_ENABLE, ip_geneve_enable ? 1 : 0);\n\n\t \n\tif (QED_IS_BB_B0(p_hwfn->cdev))\n\t\treturn;\n\n\t \n\tqed_wr(p_hwfn,\n\t       p_ptt,\n\t       DORQ_REG_L2_EDPM_TUNNEL_NGE_ETH_EN_K2,\n\t       eth_geneve_enable ? 1 : 0);\n\tqed_wr(p_hwfn,\n\t       p_ptt,\n\t       DORQ_REG_L2_EDPM_TUNNEL_NGE_IP_EN_K2,\n\t       ip_geneve_enable ? 1 : 0);\n}\n\n#define PRS_ETH_VXLAN_NO_L2_ENABLE_OFFSET      3\n#define PRS_ETH_VXLAN_NO_L2_OUTPUT_FORMAT   0xC8DAB910\n\nvoid qed_set_vxlan_no_l2_enable(struct qed_hwfn *p_hwfn,\n\t\t\t\tstruct qed_ptt *p_ptt, bool enable)\n{\n\tu32 reg_val, cfg_mask;\n\n\t \n\treg_val = qed_rd(p_hwfn, p_ptt, PRS_REG_MSG_INFO);\n\n\t \n\tcfg_mask = BIT(PRS_ETH_VXLAN_NO_L2_ENABLE_OFFSET);\n\n\tif (enable) {\n\t\t \n\t\treg_val |= cfg_mask;\n\n\t\t \n\t\tqed_wr(p_hwfn,\n\t\t       p_ptt,\n\t\t       PRS_REG_OUTPUT_FORMAT_4_0,\n\t\t       (u32)PRS_ETH_VXLAN_NO_L2_OUTPUT_FORMAT);\n\t} else {\n\t\t \n\t\treg_val &= ~cfg_mask;\n\t}\n\n\t \n\tqed_wr(p_hwfn, p_ptt, PRS_REG_MSG_INFO, reg_val);\n}\n\n#define T_ETH_PACKET_ACTION_GFT_EVENTID  23\n#define PARSER_ETH_CONN_GFT_ACTION_CM_HDR  272\n#define T_ETH_PACKET_MATCH_RFS_EVENTID 25\n#define PARSER_ETH_CONN_CM_HDR 0\n#define CAM_LINE_SIZE sizeof(u32)\n#define RAM_LINE_SIZE sizeof(u64)\n#define REG_SIZE sizeof(u32)\n\nvoid qed_gft_disable(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt, u16 pf_id)\n{\n\tstruct regpair ram_line = { 0 };\n\n\t \n\tqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_GFT, 0);\n\n\t \n\n\t \n\tqed_wr(p_hwfn, p_ptt, PRS_REG_GFT_CAM + CAM_LINE_SIZE * pf_id, 0);\n\n\t \n\tqed_dmae_to_grc(p_hwfn, p_ptt, &ram_line.lo,\n\t\t\tPRS_REG_GFT_PROFILE_MASK_RAM + RAM_LINE_SIZE * pf_id,\n\t\t\tsizeof(ram_line) / REG_SIZE);\n}\n\nvoid qed_gft_config(struct qed_hwfn *p_hwfn,\n\t\t    struct qed_ptt *p_ptt,\n\t\t    u16 pf_id,\n\t\t    bool tcp,\n\t\t    bool udp,\n\t\t    bool ipv4, bool ipv6, enum gft_profile_type profile_type)\n{\n\tstruct regpair ram_line;\n\tu32 search_non_ip_as_gft;\n\tu32 reg_val, cam_line;\n\tu32 lo = 0, hi = 0;\n\n\tif (!ipv6 && !ipv4)\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"gft_config: must accept at least on of - ipv4 or ipv6'\\n\");\n\tif (!tcp && !udp)\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"gft_config: must accept at least on of - udp or tcp\\n\");\n\tif (profile_type >= MAX_GFT_PROFILE_TYPE)\n\t\tDP_NOTICE(p_hwfn, \"gft_config: unsupported gft_profile_type\\n\");\n\n\t \n\treg_val = T_ETH_PACKET_MATCH_RFS_EVENTID <<\n\t\t  PRS_REG_CM_HDR_GFT_EVENT_ID_SHIFT;\n\treg_val |= PARSER_ETH_CONN_CM_HDR << PRS_REG_CM_HDR_GFT_CM_HDR_SHIFT;\n\tqed_wr(p_hwfn, p_ptt, PRS_REG_CM_HDR_GFT, reg_val);\n\n\t \n\tqed_wr(p_hwfn, p_ptt, PRS_REG_LOAD_L2_FILTER, 0);\n\n\t \n\tqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_TENANT_ID, 0);\n\n\t \n\tcam_line = 0;\n\tSET_FIELD(cam_line, GFT_CAM_LINE_MAPPED_VALID, 1);\n\n\t \n\tSET_FIELD(cam_line,\n\t\t  GFT_CAM_LINE_MAPPED_PF_ID_MASK,\n\t\t  GFT_CAM_LINE_MAPPED_PF_ID_MASK_MASK);\n\tSET_FIELD(cam_line, GFT_CAM_LINE_MAPPED_PF_ID, pf_id);\n\n\tif (!(tcp && udp)) {\n\t\tSET_FIELD(cam_line,\n\t\t\t  GFT_CAM_LINE_MAPPED_UPPER_PROTOCOL_TYPE_MASK,\n\t\t\t  GFT_CAM_LINE_MAPPED_UPPER_PROTOCOL_TYPE_MASK_MASK);\n\t\tif (tcp)\n\t\t\tSET_FIELD(cam_line,\n\t\t\t\t  GFT_CAM_LINE_MAPPED_UPPER_PROTOCOL_TYPE,\n\t\t\t\t  GFT_PROFILE_TCP_PROTOCOL);\n\t\telse\n\t\t\tSET_FIELD(cam_line,\n\t\t\t\t  GFT_CAM_LINE_MAPPED_UPPER_PROTOCOL_TYPE,\n\t\t\t\t  GFT_PROFILE_UDP_PROTOCOL);\n\t}\n\n\tif (!(ipv4 && ipv6)) {\n\t\tSET_FIELD(cam_line, GFT_CAM_LINE_MAPPED_IP_VERSION_MASK, 1);\n\t\tif (ipv4)\n\t\t\tSET_FIELD(cam_line,\n\t\t\t\t  GFT_CAM_LINE_MAPPED_IP_VERSION,\n\t\t\t\t  GFT_PROFILE_IPV4);\n\t\telse\n\t\t\tSET_FIELD(cam_line,\n\t\t\t\t  GFT_CAM_LINE_MAPPED_IP_VERSION,\n\t\t\t\t  GFT_PROFILE_IPV6);\n\t}\n\n\t \n\tqed_wr(p_hwfn, p_ptt, PRS_REG_GFT_CAM + CAM_LINE_SIZE * pf_id,\n\t       cam_line);\n\tcam_line =\n\t    qed_rd(p_hwfn, p_ptt, PRS_REG_GFT_CAM + CAM_LINE_SIZE * pf_id);\n\n\t \n\n\t \n\tsearch_non_ip_as_gft = 0;\n\n\t \n\tSET_FIELD(lo, GFT_RAM_LINE_TUNNEL_DST_PORT, 1);\n\tSET_FIELD(lo, GFT_RAM_LINE_TUNNEL_OVER_IP_PROTOCOL, 1);\n\n\tif (profile_type == GFT_PROFILE_TYPE_4_TUPLE) {\n\t\tSET_FIELD(hi, GFT_RAM_LINE_DST_IP, 1);\n\t\tSET_FIELD(hi, GFT_RAM_LINE_SRC_IP, 1);\n\t\tSET_FIELD(hi, GFT_RAM_LINE_OVER_IP_PROTOCOL, 1);\n\t\tSET_FIELD(lo, GFT_RAM_LINE_ETHERTYPE, 1);\n\t\tSET_FIELD(lo, GFT_RAM_LINE_SRC_PORT, 1);\n\t\tSET_FIELD(lo, GFT_RAM_LINE_DST_PORT, 1);\n\t} else if (profile_type == GFT_PROFILE_TYPE_L4_DST_PORT) {\n\t\tSET_FIELD(hi, GFT_RAM_LINE_OVER_IP_PROTOCOL, 1);\n\t\tSET_FIELD(lo, GFT_RAM_LINE_ETHERTYPE, 1);\n\t\tSET_FIELD(lo, GFT_RAM_LINE_DST_PORT, 1);\n\t} else if (profile_type == GFT_PROFILE_TYPE_IP_DST_ADDR) {\n\t\tSET_FIELD(hi, GFT_RAM_LINE_DST_IP, 1);\n\t\tSET_FIELD(lo, GFT_RAM_LINE_ETHERTYPE, 1);\n\t} else if (profile_type == GFT_PROFILE_TYPE_IP_SRC_ADDR) {\n\t\tSET_FIELD(hi, GFT_RAM_LINE_SRC_IP, 1);\n\t\tSET_FIELD(lo, GFT_RAM_LINE_ETHERTYPE, 1);\n\t} else if (profile_type == GFT_PROFILE_TYPE_TUNNEL_TYPE) {\n\t\tSET_FIELD(lo, GFT_RAM_LINE_TUNNEL_ETHERTYPE, 1);\n\n\t\t \n\t\tsearch_non_ip_as_gft = 1;\n\t}\n\n\tram_line.lo = cpu_to_le32(lo);\n\tram_line.hi = cpu_to_le32(hi);\n\n\tqed_wr(p_hwfn,\n\t       p_ptt, PRS_REG_SEARCH_NON_IP_AS_GFT, search_non_ip_as_gft);\n\tqed_dmae_to_grc(p_hwfn, p_ptt, &ram_line.lo,\n\t\t\tPRS_REG_GFT_PROFILE_MASK_RAM + RAM_LINE_SIZE * pf_id,\n\t\t\tsizeof(ram_line) / REG_SIZE);\n\n\t \n\tram_line.lo = cpu_to_le32(0xffffffff);\n\tram_line.hi = cpu_to_le32(0x3ff);\n\tqed_dmae_to_grc(p_hwfn, p_ptt, &ram_line.lo,\n\t\t\tPRS_REG_GFT_PROFILE_MASK_RAM + RAM_LINE_SIZE *\n\t\t\tPRS_GFT_CAM_LINES_NO_MATCH,\n\t\t\tsizeof(ram_line) / REG_SIZE);\n\n\t \n\tqed_wr(p_hwfn, p_ptt, PRS_REG_SEARCH_GFT, 1);\n}\n\nDECLARE_CRC8_TABLE(cdu_crc8_table);\n\n \nstatic u8 qed_calc_cdu_validation_byte(u8 conn_type, u8 region, u32 cid)\n{\n\tconst u8 validation_cfg = CDU_VALIDATION_DEFAULT_CFG;\n\tu8 crc, validation_byte = 0;\n\tstatic u8 crc8_table_valid;  \n\tu32 validation_string = 0;\n\t__be32 data_to_crc;\n\n\tif (!crc8_table_valid) {\n\t\tcrc8_populate_msb(cdu_crc8_table, 0x07);\n\t\tcrc8_table_valid = 1;\n\t}\n\n\t \n\tif ((validation_cfg >> CDU_CONTEXT_VALIDATION_CFG_USE_CID) & 1)\n\t\tvalidation_string |= (cid & 0xFFF00000) | ((cid & 0xFFF) << 8);\n\n\tif ((validation_cfg >> CDU_CONTEXT_VALIDATION_CFG_USE_REGION) & 1)\n\t\tvalidation_string |= ((region & 0xF) << 4);\n\n\tif ((validation_cfg >> CDU_CONTEXT_VALIDATION_CFG_USE_TYPE) & 1)\n\t\tvalidation_string |= (conn_type & 0xF);\n\n\t \n\tdata_to_crc = cpu_to_be32(validation_string);\n\tcrc = crc8(cdu_crc8_table, (u8 *)&data_to_crc, sizeof(data_to_crc),\n\t\t   CRC8_INIT_VALUE);\n\n\t \n\tvalidation_byte |=\n\t    ((validation_cfg >>\n\t      CDU_CONTEXT_VALIDATION_CFG_USE_ACTIVE) & 1) << 7;\n\n\tif ((validation_cfg >>\n\t     CDU_CONTEXT_VALIDATION_CFG_VALIDATION_TYPE_SHIFT) & 1)\n\t\tvalidation_byte |= ((conn_type & 0xF) << 3) | (crc & 0x7);\n\telse\n\t\tvalidation_byte |= crc & 0x7F;\n\n\treturn validation_byte;\n}\n\n \nvoid qed_calc_session_ctx_validation(void *p_ctx_mem,\n\t\t\t\t     u16 ctx_size, u8 ctx_type, u32 cid)\n{\n\tu8 *x_val_ptr, *t_val_ptr, *u_val_ptr, *p_ctx;\n\n\tp_ctx = (u8 * const)p_ctx_mem;\n\tx_val_ptr = &p_ctx[con_region_offsets[0][ctx_type]];\n\tt_val_ptr = &p_ctx[con_region_offsets[1][ctx_type]];\n\tu_val_ptr = &p_ctx[con_region_offsets[2][ctx_type]];\n\n\tmemset(p_ctx, 0, ctx_size);\n\n\t*x_val_ptr = qed_calc_cdu_validation_byte(ctx_type, 3, cid);\n\t*t_val_ptr = qed_calc_cdu_validation_byte(ctx_type, 4, cid);\n\t*u_val_ptr = qed_calc_cdu_validation_byte(ctx_type, 5, cid);\n}\n\n \nvoid qed_calc_task_ctx_validation(void *p_ctx_mem,\n\t\t\t\t  u16 ctx_size, u8 ctx_type, u32 tid)\n{\n\tu8 *p_ctx, *region1_val_ptr;\n\n\tp_ctx = (u8 * const)p_ctx_mem;\n\tregion1_val_ptr = &p_ctx[task_region_offsets[0][ctx_type]];\n\n\tmemset(p_ctx, 0, ctx_size);\n\n\t*region1_val_ptr = qed_calc_cdu_validation_byte(ctx_type, 1, tid);\n}\n\n \nvoid qed_memset_session_ctx(void *p_ctx_mem, u32 ctx_size, u8 ctx_type)\n{\n\tu8 *x_val_ptr, *t_val_ptr, *u_val_ptr, *p_ctx;\n\tu8 x_val, t_val, u_val;\n\n\tp_ctx = (u8 * const)p_ctx_mem;\n\tx_val_ptr = &p_ctx[con_region_offsets[0][ctx_type]];\n\tt_val_ptr = &p_ctx[con_region_offsets[1][ctx_type]];\n\tu_val_ptr = &p_ctx[con_region_offsets[2][ctx_type]];\n\n\tx_val = *x_val_ptr;\n\tt_val = *t_val_ptr;\n\tu_val = *u_val_ptr;\n\n\tmemset(p_ctx, 0, ctx_size);\n\n\t*x_val_ptr = x_val;\n\t*t_val_ptr = t_val;\n\t*u_val_ptr = u_val;\n}\n\n \nvoid qed_memset_task_ctx(void *p_ctx_mem, u32 ctx_size, u8 ctx_type)\n{\n\tu8 *p_ctx, *region1_val_ptr;\n\tu8 region1_val;\n\n\tp_ctx = (u8 * const)p_ctx_mem;\n\tregion1_val_ptr = &p_ctx[task_region_offsets[0][ctx_type]];\n\n\tregion1_val = *region1_val_ptr;\n\n\tmemset(p_ctx, 0, ctx_size);\n\n\t*region1_val_ptr = region1_val;\n}\n\n \nvoid qed_enable_context_validation(struct qed_hwfn *p_hwfn,\n\t\t\t\t   struct qed_ptt *p_ptt)\n{\n\tu32 ctx_validation;\n\n\t \n\tctx_validation = CDU_VALIDATION_DEFAULT_CFG << 24;\n\tqed_wr(p_hwfn, p_ptt, CDU_REG_CCFC_CTX_VALID0, ctx_validation);\n\n\t \n\tctx_validation = CDU_VALIDATION_DEFAULT_CFG << 8;\n\tqed_wr(p_hwfn, p_ptt, CDU_REG_CCFC_CTX_VALID1, ctx_validation);\n\n\t \n\tctx_validation = CDU_VALIDATION_DEFAULT_CFG << 8;\n\tqed_wr(p_hwfn, p_ptt, CDU_REG_TCFC_CTX_VALID0, ctx_validation);\n}\n\nconst char *qed_get_protocol_type_str(u32 protocol_type)\n{\n\tif (protocol_type >= ARRAY_SIZE(s_protocol_types))\n\t\treturn \"Invalid protocol type\";\n\n\treturn s_protocol_types[protocol_type];\n}\n\nconst char *qed_get_ramrod_cmd_id_str(u32 protocol_type, u32 ramrod_cmd_id)\n{\n\tconst char *ramrod_cmd_id_str;\n\n\tif (protocol_type >= ARRAY_SIZE(s_ramrod_cmd_ids))\n\t\treturn \"Invalid protocol type\";\n\n\tif (ramrod_cmd_id >= ARRAY_SIZE(s_ramrod_cmd_ids[0]))\n\t\treturn \"Invalid Ramrod command ID\";\n\n\tramrod_cmd_id_str = s_ramrod_cmd_ids[protocol_type][ramrod_cmd_id];\n\n\tif (!ramrod_cmd_id_str)\n\t\treturn \"Invalid Ramrod command ID\";\n\n\treturn ramrod_cmd_id_str;\n}\n\nstatic u32 qed_get_rdma_assert_ram_addr(struct qed_hwfn *p_hwfn, u8 storm_id)\n{\n\tswitch (storm_id) {\n\tcase 0:\n\t\treturn TSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    TSTORM_RDMA_ASSERT_LEVEL_OFFSET(p_hwfn->rel_pf_id);\n\tcase 1:\n\t\treturn MSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    MSTORM_RDMA_ASSERT_LEVEL_OFFSET(p_hwfn->rel_pf_id);\n\tcase 2:\n\t\treturn USEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    USTORM_RDMA_ASSERT_LEVEL_OFFSET(p_hwfn->rel_pf_id);\n\tcase 3:\n\t\treturn XSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    XSTORM_RDMA_ASSERT_LEVEL_OFFSET(p_hwfn->rel_pf_id);\n\tcase 4:\n\t\treturn YSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    YSTORM_RDMA_ASSERT_LEVEL_OFFSET(p_hwfn->rel_pf_id);\n\tcase 5:\n\t\treturn PSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    PSTORM_RDMA_ASSERT_LEVEL_OFFSET(p_hwfn->rel_pf_id);\n\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nvoid qed_set_rdma_error_level(struct qed_hwfn *p_hwfn,\n\t\t\t      struct qed_ptt *p_ptt,\n\t\t\t      u8 assert_level[NUM_STORMS])\n{\n\tu8 storm_id;\n\n\tfor (storm_id = 0; storm_id < NUM_STORMS; storm_id++) {\n\t\tu32 ram_addr = qed_get_rdma_assert_ram_addr(p_hwfn, storm_id);\n\n\t\tqed_wr(p_hwfn, p_ptt, ram_addr, assert_level[storm_id]);\n\t}\n}\n\n#define PHYS_ADDR_DWORDS        DIV_ROUND_UP(sizeof(dma_addr_t), 4)\n#define OVERLAY_HDR_SIZE_DWORDS (sizeof(struct fw_overlay_buf_hdr) / 4)\n\nstatic u32 qed_get_overlay_addr_ram_addr(struct qed_hwfn *p_hwfn, u8 storm_id)\n{\n\tswitch (storm_id) {\n\tcase 0:\n\t\treturn TSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    TSTORM_OVERLAY_BUF_ADDR_OFFSET;\n\tcase 1:\n\t\treturn MSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    MSTORM_OVERLAY_BUF_ADDR_OFFSET;\n\tcase 2:\n\t\treturn USEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    USTORM_OVERLAY_BUF_ADDR_OFFSET;\n\tcase 3:\n\t\treturn XSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    XSTORM_OVERLAY_BUF_ADDR_OFFSET;\n\tcase 4:\n\t\treturn YSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    YSTORM_OVERLAY_BUF_ADDR_OFFSET;\n\tcase 5:\n\t\treturn PSEM_REG_FAST_MEMORY + SEM_FAST_REG_INT_RAM +\n\t\t    PSTORM_OVERLAY_BUF_ADDR_OFFSET;\n\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstruct phys_mem_desc *qed_fw_overlay_mem_alloc(struct qed_hwfn *p_hwfn,\n\t\t\t\t\t       const u32 * const\n\t\t\t\t\t       fw_overlay_in_buf,\n\t\t\t\t\t       u32 buf_size_in_bytes)\n{\n\tu32 buf_size = buf_size_in_bytes / sizeof(u32), buf_offset = 0;\n\tstruct phys_mem_desc *allocated_mem;\n\n\tif (!buf_size)\n\t\treturn NULL;\n\n\tallocated_mem = kcalloc(NUM_STORMS, sizeof(struct phys_mem_desc),\n\t\t\t\tGFP_KERNEL);\n\tif (!allocated_mem)\n\t\treturn NULL;\n\n\t \n\twhile (buf_offset < buf_size) {\n\t\tstruct phys_mem_desc *storm_mem_desc;\n\t\tstruct fw_overlay_buf_hdr *hdr;\n\t\tu32 storm_buf_size;\n\t\tu8 storm_id;\n\n\t\thdr =\n\t\t    (struct fw_overlay_buf_hdr *)&fw_overlay_in_buf[buf_offset];\n\t\tstorm_buf_size = GET_FIELD(hdr->data,\n\t\t\t\t\t   FW_OVERLAY_BUF_HDR_BUF_SIZE);\n\t\tstorm_id = GET_FIELD(hdr->data, FW_OVERLAY_BUF_HDR_STORM_ID);\n\t\tif (storm_id >= NUM_STORMS)\n\t\t\tbreak;\n\t\tstorm_mem_desc = allocated_mem + storm_id;\n\t\tstorm_mem_desc->size = storm_buf_size * sizeof(u32);\n\n\t\t \n\t\tstorm_mem_desc->virt_addr =\n\t\t    dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t\t       storm_mem_desc->size,\n\t\t\t\t       &storm_mem_desc->phys_addr, GFP_KERNEL);\n\t\tif (!storm_mem_desc->virt_addr)\n\t\t\tbreak;\n\n\t\t \n\t\tbuf_offset += OVERLAY_HDR_SIZE_DWORDS;\n\n\t\t \n\t\tmemcpy(storm_mem_desc->virt_addr,\n\t\t       &fw_overlay_in_buf[buf_offset], storm_mem_desc->size);\n\n\t\t \n\t\tbuf_offset += storm_buf_size;\n\t}\n\n\t \n\tif (buf_offset < buf_size) {\n\t\tqed_fw_overlay_mem_free(p_hwfn, &allocated_mem);\n\t\treturn NULL;\n\t}\n\n\treturn allocated_mem;\n}\n\nvoid qed_fw_overlay_init_ram(struct qed_hwfn *p_hwfn,\n\t\t\t     struct qed_ptt *p_ptt,\n\t\t\t     struct phys_mem_desc *fw_overlay_mem)\n{\n\tu8 storm_id;\n\n\tfor (storm_id = 0; storm_id < NUM_STORMS; storm_id++) {\n\t\tstruct phys_mem_desc *storm_mem_desc =\n\t\t    (struct phys_mem_desc *)fw_overlay_mem + storm_id;\n\t\tu32 ram_addr, i;\n\n\t\t \n\t\tif (!storm_mem_desc->virt_addr)\n\t\t\tcontinue;\n\n\t\t \n\t\tram_addr = qed_get_overlay_addr_ram_addr(p_hwfn, storm_id) +\n\t\t\t   sizeof(dma_addr_t) * p_hwfn->rel_pf_id;\n\n\t\t \n\t\tfor (i = 0; i < PHYS_ADDR_DWORDS; i++, ram_addr += sizeof(u32))\n\t\t\tqed_wr(p_hwfn, p_ptt, ram_addr,\n\t\t\t       ((u32 *)&storm_mem_desc->phys_addr)[i]);\n\t}\n}\n\nvoid qed_fw_overlay_mem_free(struct qed_hwfn *p_hwfn,\n\t\t\t     struct phys_mem_desc **fw_overlay_mem)\n{\n\tu8 storm_id;\n\n\tif (!fw_overlay_mem || !(*fw_overlay_mem))\n\t\treturn;\n\n\tfor (storm_id = 0; storm_id < NUM_STORMS; storm_id++) {\n\t\tstruct phys_mem_desc *storm_mem_desc =\n\t\t    (struct phys_mem_desc *)*fw_overlay_mem + storm_id;\n\n\t\t \n\t\tif (storm_mem_desc->virt_addr)\n\t\t\tdma_free_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t\t\t  storm_mem_desc->size,\n\t\t\t\t\t  storm_mem_desc->virt_addr,\n\t\t\t\t\t  storm_mem_desc->phys_addr);\n\t}\n\n\t \n\tkfree(*fw_overlay_mem);\n\t*fw_overlay_mem = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}