{
  "module_name": "qed_rdma.c",
  "hash_id": "aa872f493897e7977bece1eebb4cddb766943f90e21994e856ae294f18b589ca",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/qlogic/qed/qed_rdma.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <asm/byteorder.h>\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/errno.h>\n#include <linux/io.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <net/addrconf.h>\n#include \"qed.h\"\n#include \"qed_cxt.h\"\n#include \"qed_hsi.h\"\n#include \"qed_iro_hsi.h\"\n#include \"qed_hw.h\"\n#include \"qed_init_ops.h\"\n#include \"qed_int.h\"\n#include \"qed_ll2.h\"\n#include \"qed_mcp.h\"\n#include \"qed_reg_addr.h\"\n#include <linux/qed/qed_rdma_if.h>\n#include \"qed_rdma.h\"\n#include \"qed_roce.h\"\n#include \"qed_sp.h\"\n\nint qed_rdma_bmap_alloc(struct qed_hwfn *p_hwfn,\n\t\t\tstruct qed_bmap *bmap, u32 max_count, char *name)\n{\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"max_count = %08x\\n\", max_count);\n\n\tbmap->max_count = max_count;\n\n\tbmap->bitmap = bitmap_zalloc(max_count, GFP_KERNEL);\n\tif (!bmap->bitmap)\n\t\treturn -ENOMEM;\n\n\tsnprintf(bmap->name, QED_RDMA_MAX_BMAP_NAME, \"%s\", name);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"0\\n\");\n\treturn 0;\n}\n\nint qed_rdma_bmap_alloc_id(struct qed_hwfn *p_hwfn,\n\t\t\t   struct qed_bmap *bmap, u32 *id_num)\n{\n\t*id_num = find_first_zero_bit(bmap->bitmap, bmap->max_count);\n\tif (*id_num >= bmap->max_count)\n\t\treturn -EINVAL;\n\n\t__set_bit(*id_num, bmap->bitmap);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"%s bitmap: allocated id %d\\n\",\n\t\t   bmap->name, *id_num);\n\n\treturn 0;\n}\n\nvoid qed_bmap_set_id(struct qed_hwfn *p_hwfn,\n\t\t     struct qed_bmap *bmap, u32 id_num)\n{\n\tif (id_num >= bmap->max_count)\n\t\treturn;\n\n\t__set_bit(id_num, bmap->bitmap);\n}\n\nvoid qed_bmap_release_id(struct qed_hwfn *p_hwfn,\n\t\t\t struct qed_bmap *bmap, u32 id_num)\n{\n\tbool b_acquired;\n\n\tif (id_num >= bmap->max_count)\n\t\treturn;\n\n\tb_acquired = test_and_clear_bit(id_num, bmap->bitmap);\n\tif (!b_acquired) {\n\t\tDP_NOTICE(p_hwfn, \"%s bitmap: id %d already released\\n\",\n\t\t\t  bmap->name, id_num);\n\t\treturn;\n\t}\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"%s bitmap: released id %d\\n\",\n\t\t   bmap->name, id_num);\n}\n\nint qed_bmap_test_id(struct qed_hwfn *p_hwfn,\n\t\t     struct qed_bmap *bmap, u32 id_num)\n{\n\tif (id_num >= bmap->max_count)\n\t\treturn -1;\n\n\treturn test_bit(id_num, bmap->bitmap);\n}\n\nstatic bool qed_bmap_is_empty(struct qed_bmap *bmap)\n{\n\treturn bitmap_empty(bmap->bitmap, bmap->max_count);\n}\n\nstatic u32 qed_rdma_get_sb_id(void *p_hwfn, u32 rel_sb_id)\n{\n\t \n\treturn FEAT_NUM((struct qed_hwfn *)p_hwfn, QED_PF_L2_QUE) + rel_sb_id;\n}\n\nint qed_rdma_info_alloc(struct qed_hwfn *p_hwfn)\n{\n\tstruct qed_rdma_info *p_rdma_info;\n\n\tp_rdma_info = kzalloc(sizeof(*p_rdma_info), GFP_KERNEL);\n\tif (!p_rdma_info)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&p_rdma_info->lock);\n\n\tp_hwfn->p_rdma_info = p_rdma_info;\n\treturn 0;\n}\n\nvoid qed_rdma_info_free(struct qed_hwfn *p_hwfn)\n{\n\tkfree(p_hwfn->p_rdma_info);\n\tp_hwfn->p_rdma_info = NULL;\n}\n\nstatic int qed_rdma_alloc(struct qed_hwfn *p_hwfn)\n{\n\tstruct qed_rdma_info *p_rdma_info = p_hwfn->p_rdma_info;\n\tu32 num_cons, num_tasks;\n\tint rc = -ENOMEM;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Allocating RDMA\\n\");\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn))\n\t\tp_rdma_info->proto = PROTOCOLID_IWARP;\n\telse\n\t\tp_rdma_info->proto = PROTOCOLID_ROCE;\n\n\tnum_cons = qed_cxt_get_proto_cid_count(p_hwfn, p_rdma_info->proto,\n\t\t\t\t\t       NULL);\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn))\n\t\tp_rdma_info->num_qps = num_cons;\n\telse\n\t\tp_rdma_info->num_qps = num_cons / 2;  \n\n\tnum_tasks = qed_cxt_get_proto_tid_count(p_hwfn, PROTOCOLID_ROCE);\n\n\t \n\tp_rdma_info->num_mrs = num_tasks;\n\n\t \n\tp_rdma_info->queue_zone_base = (u16)RESC_START(p_hwfn, QED_L2_QUEUE);\n\tp_rdma_info->max_queue_zones = (u16)RESC_NUM(p_hwfn, QED_L2_QUEUE);\n\n\t \n\tp_rdma_info->dev = kzalloc(sizeof(*p_rdma_info->dev), GFP_KERNEL);\n\tif (!p_rdma_info->dev)\n\t\treturn rc;\n\n\t \n\tp_rdma_info->port = kzalloc(sizeof(*p_rdma_info->port), GFP_KERNEL);\n\tif (!p_rdma_info->port)\n\t\tgoto free_rdma_dev;\n\n\t \n\trc = qed_rdma_bmap_alloc(p_hwfn, &p_rdma_info->pd_map, RDMA_MAX_PDS,\n\t\t\t\t \"PD\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate pd_map, rc = %d\\n\",\n\t\t\t   rc);\n\t\tgoto free_rdma_port;\n\t}\n\n\t \n\trc = qed_rdma_bmap_alloc(p_hwfn, &p_rdma_info->xrcd_map,\n\t\t\t\t QED_RDMA_MAX_XRCDS, \"XRCD\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate xrcd_map,rc = %d\\n\", rc);\n\t\tgoto free_pd_map;\n\t}\n\n\t \n\trc = qed_rdma_bmap_alloc(p_hwfn, &p_rdma_info->dpi_map,\n\t\t\t\t p_hwfn->dpi_count, \"DPI\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate DPI bitmap, rc = %d\\n\", rc);\n\t\tgoto free_xrcd_map;\n\t}\n\n\t \n\trc = qed_rdma_bmap_alloc(p_hwfn, &p_rdma_info->cq_map, num_cons, \"CQ\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate cq bitmap, rc = %d\\n\", rc);\n\t\tgoto free_dpi_map;\n\t}\n\n\t \n\trc = qed_rdma_bmap_alloc(p_hwfn, &p_rdma_info->toggle_bits,\n\t\t\t\t num_cons, \"Toggle\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate toggle bits, rc = %d\\n\", rc);\n\t\tgoto free_cq_map;\n\t}\n\n\t \n\trc = qed_rdma_bmap_alloc(p_hwfn, &p_rdma_info->tid_map,\n\t\t\t\t p_rdma_info->num_mrs, \"MR\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate itids bitmaps, rc = %d\\n\", rc);\n\t\tgoto free_toggle_map;\n\t}\n\n\t \n\trc = qed_rdma_bmap_alloc(p_hwfn, &p_rdma_info->cid_map, num_cons,\n\t\t\t\t \"CID\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate cid bitmap, rc = %d\\n\", rc);\n\t\tgoto free_tid_map;\n\t}\n\n\t \n\trc = qed_rdma_bmap_alloc(p_hwfn, &p_rdma_info->real_cid_map, num_cons,\n\t\t\t\t \"REAL_CID\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate real cid bitmap, rc = %d\\n\", rc);\n\t\tgoto free_cid_map;\n\t}\n\n\t \n\tp_rdma_info->srq_id_offset = p_hwfn->p_cxt_mngr->xrc_srq_count;\n\trc = qed_rdma_bmap_alloc(p_hwfn,\n\t\t\t\t &p_rdma_info->xrc_srq_map,\n\t\t\t\t p_hwfn->p_cxt_mngr->xrc_srq_count, \"XRC SRQ\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate xrc srq bitmap, rc = %d\\n\", rc);\n\t\tgoto free_real_cid_map;\n\t}\n\n\t \n\tp_rdma_info->num_srqs = p_hwfn->p_cxt_mngr->srq_count;\n\trc = qed_rdma_bmap_alloc(p_hwfn, &p_rdma_info->srq_map,\n\t\t\t\t p_rdma_info->num_srqs, \"SRQ\");\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t   \"Failed to allocate srq bitmap, rc = %d\\n\", rc);\n\t\tgoto free_xrc_srq_map;\n\t}\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn))\n\t\trc = qed_iwarp_alloc(p_hwfn);\n\n\tif (rc)\n\t\tgoto free_srq_map;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Allocation successful\\n\");\n\treturn 0;\n\nfree_srq_map:\n\tkfree(p_rdma_info->srq_map.bitmap);\nfree_xrc_srq_map:\n\tkfree(p_rdma_info->xrc_srq_map.bitmap);\nfree_real_cid_map:\n\tkfree(p_rdma_info->real_cid_map.bitmap);\nfree_cid_map:\n\tkfree(p_rdma_info->cid_map.bitmap);\nfree_tid_map:\n\tkfree(p_rdma_info->tid_map.bitmap);\nfree_toggle_map:\n\tkfree(p_rdma_info->toggle_bits.bitmap);\nfree_cq_map:\n\tkfree(p_rdma_info->cq_map.bitmap);\nfree_dpi_map:\n\tkfree(p_rdma_info->dpi_map.bitmap);\nfree_xrcd_map:\n\tkfree(p_rdma_info->xrcd_map.bitmap);\nfree_pd_map:\n\tkfree(p_rdma_info->pd_map.bitmap);\nfree_rdma_port:\n\tkfree(p_rdma_info->port);\nfree_rdma_dev:\n\tkfree(p_rdma_info->dev);\n\n\treturn rc;\n}\n\nvoid qed_rdma_bmap_free(struct qed_hwfn *p_hwfn,\n\t\t\tstruct qed_bmap *bmap, bool check)\n{\n\tunsigned int bit, weight, nbits;\n\tunsigned long *b;\n\n\tif (!check)\n\t\tgoto end;\n\n\tweight = bitmap_weight(bmap->bitmap, bmap->max_count);\n\tif (!weight)\n\t\tgoto end;\n\n\tDP_NOTICE(p_hwfn,\n\t\t  \"%s bitmap not free - size=%d, weight=%d, 512 bits per line\\n\",\n\t\t  bmap->name, bmap->max_count, weight);\n\n\tfor (bit = 0; bit < bmap->max_count; bit += 512) {\n\t\tb =  bmap->bitmap + BITS_TO_LONGS(bit);\n\t\tnbits = min(bmap->max_count - bit, 512U);\n\n\t\tif (!bitmap_empty(b, nbits))\n\t\t\tDP_NOTICE(p_hwfn,\n\t\t\t\t  \"line 0x%04x: %*pb\\n\", bit / 512, nbits, b);\n\t}\n\nend:\n\tbitmap_free(bmap->bitmap);\n\tbmap->bitmap = NULL;\n}\n\nstatic void qed_rdma_resc_free(struct qed_hwfn *p_hwfn)\n{\n\tstruct qed_rdma_info *p_rdma_info = p_hwfn->p_rdma_info;\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn))\n\t\tqed_iwarp_resc_free(p_hwfn);\n\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->cid_map, 1);\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->pd_map, 1);\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->dpi_map, 1);\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->cq_map, 1);\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->toggle_bits, 0);\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->tid_map, 1);\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->srq_map, 1);\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->real_cid_map, 1);\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->xrc_srq_map, 1);\n\tqed_rdma_bmap_free(p_hwfn, &p_hwfn->p_rdma_info->xrcd_map, 1);\n\n\tkfree(p_rdma_info->port);\n\tkfree(p_rdma_info->dev);\n}\n\nstatic void qed_rdma_free_tid(void *rdma_cxt, u32 itid)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"itid = %08x\\n\", itid);\n\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\tqed_bmap_release_id(p_hwfn, &p_hwfn->p_rdma_info->tid_map, itid);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n}\n\nstatic void qed_rdma_free_reserved_lkey(struct qed_hwfn *p_hwfn)\n{\n\tqed_rdma_free_tid(p_hwfn, p_hwfn->p_rdma_info->dev->reserved_lkey);\n}\n\nstatic void qed_rdma_free(struct qed_hwfn *p_hwfn)\n{\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Freeing RDMA\\n\");\n\n\tqed_rdma_free_reserved_lkey(p_hwfn);\n\tqed_cxt_free_proto_ilt(p_hwfn, p_hwfn->p_rdma_info->proto);\n\tqed_rdma_resc_free(p_hwfn);\n}\n\nstatic void qed_rdma_init_events(struct qed_hwfn *p_hwfn,\n\t\t\t\t struct qed_rdma_start_in_params *params)\n{\n\tstruct qed_rdma_events *events;\n\n\tevents = &p_hwfn->p_rdma_info->events;\n\n\tevents->unaffiliated_event = params->events->unaffiliated_event;\n\tevents->affiliated_event = params->events->affiliated_event;\n\tevents->context = params->events->context;\n}\n\nstatic void qed_rdma_init_devinfo(struct qed_hwfn *p_hwfn,\n\t\t\t\t  struct qed_rdma_start_in_params *params)\n{\n\tstruct qed_rdma_device *dev = p_hwfn->p_rdma_info->dev;\n\tstruct qed_dev *cdev = p_hwfn->cdev;\n\tu32 pci_status_control;\n\tu32 num_qps;\n\n\t \n\tdev->vendor_id = cdev->vendor_id;\n\tdev->vendor_part_id = cdev->device_id;\n\tdev->hw_ver = cdev->chip_rev;\n\tdev->fw_ver = (FW_MAJOR_VERSION << 24) | (FW_MINOR_VERSION << 16) |\n\t\t      (FW_REVISION_VERSION << 8) | (FW_ENGINEERING_VERSION);\n\n\taddrconf_addr_eui48((u8 *)&dev->sys_image_guid,\n\t\t\t    p_hwfn->hw_info.hw_mac_addr);\n\n\tdev->node_guid = dev->sys_image_guid;\n\n\tdev->max_sge = min_t(u32, RDMA_MAX_SGE_PER_SQ_WQE,\n\t\t\t     RDMA_MAX_SGE_PER_RQ_WQE);\n\n\tif (cdev->rdma_max_sge)\n\t\tdev->max_sge = min_t(u32, cdev->rdma_max_sge, dev->max_sge);\n\n\tdev->max_srq_sge = QED_RDMA_MAX_SGE_PER_SRQ_WQE;\n\tif (p_hwfn->cdev->rdma_max_srq_sge) {\n\t\tdev->max_srq_sge = min_t(u32,\n\t\t\t\t\t p_hwfn->cdev->rdma_max_srq_sge,\n\t\t\t\t\t dev->max_srq_sge);\n\t}\n\tdev->max_inline = ROCE_REQ_MAX_INLINE_DATA_SIZE;\n\n\tdev->max_inline = (cdev->rdma_max_inline) ?\n\t\t\t  min_t(u32, cdev->rdma_max_inline, dev->max_inline) :\n\t\t\t  dev->max_inline;\n\n\tdev->max_wqe = QED_RDMA_MAX_WQE;\n\tdev->max_cnq = (u8)FEAT_NUM(p_hwfn, QED_RDMA_CNQ);\n\n\t \n\tnum_qps = ROCE_MAX_QPS;\n\tnum_qps = min_t(u64, num_qps, p_hwfn->p_rdma_info->num_qps);\n\tdev->max_qp = num_qps;\n\n\t \n\tdev->max_cq = num_qps * 2;\n\n\t \n\tdev->max_mr = p_hwfn->p_rdma_info->num_mrs - 1;\n\tdev->max_mr_size = QED_RDMA_MAX_MR_SIZE;\n\n\t \n\tif (params->cq_mode == QED_RDMA_CQ_MODE_32_BITS)\n\t\tdev->max_cqe = QED_RDMA_MAX_CQE_32_BIT;\n\telse\n\t\tdev->max_cqe = QED_RDMA_MAX_CQE_16_BIT;\n\n\tdev->max_mw = 0;\n\tdev->max_mr_mw_fmr_pbl = (PAGE_SIZE / 8) * (PAGE_SIZE / 8);\n\tdev->max_mr_mw_fmr_size = dev->max_mr_mw_fmr_pbl * PAGE_SIZE;\n\tif (QED_IS_ROCE_PERSONALITY(p_hwfn))\n\t\tdev->max_pkey = QED_RDMA_MAX_P_KEY;\n\n\tdev->max_srq = p_hwfn->p_rdma_info->num_srqs;\n\tdev->max_srq_wr = QED_RDMA_MAX_SRQ_WQE_ELEM;\n\tdev->max_qp_resp_rd_atomic_resc = RDMA_RING_PAGE_SIZE /\n\t\t\t\t\t  (RDMA_RESP_RD_ATOMIC_ELM_SIZE * 2);\n\tdev->max_qp_req_rd_atomic_resc = RDMA_RING_PAGE_SIZE /\n\t\t\t\t\t RDMA_REQ_RD_ATOMIC_ELM_SIZE;\n\tdev->max_dev_resp_rd_atomic_resc = dev->max_qp_resp_rd_atomic_resc *\n\t\t\t\t\t   p_hwfn->p_rdma_info->num_qps;\n\tdev->page_size_caps = QED_RDMA_PAGE_SIZE_CAPS;\n\tdev->dev_ack_delay = QED_RDMA_ACK_DELAY;\n\tdev->max_pd = RDMA_MAX_PDS;\n\tdev->max_ah = p_hwfn->p_rdma_info->num_qps;\n\tdev->max_stats_queues = (u8)RESC_NUM(p_hwfn, QED_RDMA_STATS_QUEUE);\n\n\t \n\tdev->dev_caps = 0;\n\tSET_FIELD(dev->dev_caps, QED_RDMA_DEV_CAP_RNR_NAK, 1);\n\tSET_FIELD(dev->dev_caps, QED_RDMA_DEV_CAP_PORT_ACTIVE_EVENT, 1);\n\tSET_FIELD(dev->dev_caps, QED_RDMA_DEV_CAP_PORT_CHANGE_EVENT, 1);\n\tSET_FIELD(dev->dev_caps, QED_RDMA_DEV_CAP_RESIZE_CQ, 1);\n\tSET_FIELD(dev->dev_caps, QED_RDMA_DEV_CAP_BASE_MEMORY_EXT, 1);\n\tSET_FIELD(dev->dev_caps, QED_RDMA_DEV_CAP_BASE_QUEUE_EXT, 1);\n\tSET_FIELD(dev->dev_caps, QED_RDMA_DEV_CAP_ZBVA, 1);\n\tSET_FIELD(dev->dev_caps, QED_RDMA_DEV_CAP_LOCAL_INV_FENCE, 1);\n\n\t \n\tpcie_capability_read_dword(cdev->pdev, PCI_EXP_DEVCTL2,\n\t\t\t\t   &pci_status_control);\n\n\tif (pci_status_control & PCI_EXP_DEVCTL2_LTR_EN)\n\t\tSET_FIELD(dev->dev_caps, QED_RDMA_DEV_CAP_ATOMIC_OP, 1);\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn))\n\t\tqed_iwarp_init_devinfo(p_hwfn);\n}\n\nstatic void qed_rdma_init_port(struct qed_hwfn *p_hwfn)\n{\n\tstruct qed_rdma_port *port = p_hwfn->p_rdma_info->port;\n\tstruct qed_rdma_device *dev = p_hwfn->p_rdma_info->dev;\n\n\tport->port_state = p_hwfn->mcp_info->link_output.link_up ?\n\t\t\t   QED_RDMA_PORT_UP : QED_RDMA_PORT_DOWN;\n\n\tport->max_msg_size = min_t(u64,\n\t\t\t\t   (dev->max_mr_mw_fmr_size *\n\t\t\t\t    p_hwfn->cdev->rdma_max_sge),\n\t\t\t\t   BIT(31));\n\n\tport->pkey_bad_counter = 0;\n}\n\nstatic int qed_rdma_init_hw(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)\n{\n\tint rc = 0;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Initializing HW\\n\");\n\tp_hwfn->b_rdma_enabled_in_prs = false;\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn))\n\t\tqed_iwarp_init_hw(p_hwfn, p_ptt);\n\telse\n\t\trc = qed_roce_init_hw(p_hwfn, p_ptt);\n\n\treturn rc;\n}\n\nstatic int qed_rdma_start_fw(struct qed_hwfn *p_hwfn,\n\t\t\t     struct qed_rdma_start_in_params *params,\n\t\t\t     struct qed_ptt *p_ptt)\n{\n\tstruct rdma_init_func_ramrod_data *p_ramrod;\n\tstruct qed_rdma_cnq_params *p_cnq_pbl_list;\n\tstruct rdma_init_func_hdr *p_params_header;\n\tstruct rdma_cnq_params *p_cnq_params;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tu32 cnq_id, sb_id;\n\tu16 igu_sb_id;\n\tint rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Starting FW\\n\");\n\n\t \n\tp_hwfn->p_rdma_info->num_cnqs = params->desired_cnq;\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent, RDMA_RAMROD_FUNC_INIT,\n\t\t\t\t p_hwfn->p_rdma_info->proto, &init_data);\n\tif (rc)\n\t\treturn rc;\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn)) {\n\t\tqed_iwarp_init_fw_ramrod(p_hwfn,\n\t\t\t\t\t &p_ent->ramrod.iwarp_init_func);\n\t\tp_ramrod = &p_ent->ramrod.iwarp_init_func.rdma;\n\t} else {\n\t\tp_ramrod = &p_ent->ramrod.roce_init_func.rdma;\n\t}\n\n\tp_params_header = &p_ramrod->params_header;\n\tp_params_header->cnq_start_offset = (u8)RESC_START(p_hwfn,\n\t\t\t\t\t\t\t   QED_RDMA_CNQ_RAM);\n\tp_params_header->num_cnqs = params->desired_cnq;\n\tp_params_header->first_reg_srq_id =\n\t    cpu_to_le16(p_hwfn->p_rdma_info->srq_id_offset);\n\tp_params_header->reg_srq_base_addr =\n\t    cpu_to_le32(qed_cxt_get_ilt_page_size(p_hwfn, ILT_CLI_TSDM));\n\tif (params->cq_mode == QED_RDMA_CQ_MODE_16_BITS)\n\t\tp_params_header->cq_ring_mode = 1;\n\telse\n\t\tp_params_header->cq_ring_mode = 0;\n\n\tfor (cnq_id = 0; cnq_id < params->desired_cnq; cnq_id++) {\n\t\tsb_id = qed_rdma_get_sb_id(p_hwfn, cnq_id);\n\t\tigu_sb_id = qed_get_igu_sb_id(p_hwfn, sb_id);\n\t\tp_ramrod->cnq_params[cnq_id].sb_num = cpu_to_le16(igu_sb_id);\n\t\tp_cnq_params = &p_ramrod->cnq_params[cnq_id];\n\t\tp_cnq_pbl_list = &params->cnq_pbl_list[cnq_id];\n\n\t\tp_cnq_params->sb_index = p_hwfn->pf_params.rdma_pf_params.gl_pi;\n\t\tp_cnq_params->num_pbl_pages = p_cnq_pbl_list->num_pbl_pages;\n\n\t\tDMA_REGPAIR_LE(p_cnq_params->pbl_base_addr,\n\t\t\t       p_cnq_pbl_list->pbl_ptr);\n\n\t\t \n\t\tp_cnq_params->queue_zone_num =\n\t\t\tcpu_to_le16(p_hwfn->p_rdma_info->queue_zone_base +\n\t\t\t\t    cnq_id);\n\t}\n\n\treturn qed_spq_post(p_hwfn, p_ent, NULL);\n}\n\nstatic int qed_rdma_alloc_tid(void *rdma_cxt, u32 *itid)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tint rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Allocate TID\\n\");\n\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\trc = qed_rdma_bmap_alloc_id(p_hwfn,\n\t\t\t\t    &p_hwfn->p_rdma_info->tid_map, itid);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n\tif (rc)\n\t\tgoto out;\n\n\trc = qed_cxt_dynamic_ilt_alloc(p_hwfn, QED_ELEM_TASK, *itid);\nout:\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Allocate TID - done, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic int qed_rdma_reserve_lkey(struct qed_hwfn *p_hwfn)\n{\n\tstruct qed_rdma_device *dev = p_hwfn->p_rdma_info->dev;\n\n\t \n\tqed_rdma_alloc_tid(p_hwfn, &dev->reserved_lkey);\n\tif (dev->reserved_lkey != RDMA_RESERVED_LKEY) {\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"Reserved lkey should be equal to RDMA_RESERVED_LKEY\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int qed_rdma_setup(struct qed_hwfn *p_hwfn,\n\t\t\t  struct qed_ptt *p_ptt,\n\t\t\t  struct qed_rdma_start_in_params *params)\n{\n\tint rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"RDMA setup\\n\");\n\n\tqed_rdma_init_devinfo(p_hwfn, params);\n\tqed_rdma_init_port(p_hwfn);\n\tqed_rdma_init_events(p_hwfn, params);\n\n\trc = qed_rdma_reserve_lkey(p_hwfn);\n\tif (rc)\n\t\treturn rc;\n\n\trc = qed_rdma_init_hw(p_hwfn, p_ptt);\n\tif (rc)\n\t\treturn rc;\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn)) {\n\t\trc = qed_iwarp_setup(p_hwfn, params);\n\t\tif (rc)\n\t\t\treturn rc;\n\t} else {\n\t\trc = qed_roce_setup(p_hwfn);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\treturn qed_rdma_start_fw(p_hwfn, params, p_ptt);\n}\n\nstatic int qed_rdma_stop(void *rdma_cxt)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tstruct rdma_close_func_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tstruct qed_ptt *p_ptt;\n\tu32 ll2_ethertype_en;\n\tint rc = -EBUSY;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"RDMA stop\\n\");\n\n\tp_ptt = qed_ptt_acquire(p_hwfn);\n\tif (!p_ptt) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Failed to acquire PTT\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tqed_wr(p_hwfn, p_ptt, p_hwfn->rdma_prs_search_reg, 0);\n\tp_hwfn->b_rdma_enabled_in_prs = false;\n\tp_hwfn->p_rdma_info->active = 0;\n\tqed_wr(p_hwfn, p_ptt, PRS_REG_ROCE_DEST_QP_MAX_PF, 0);\n\n\tll2_ethertype_en = qed_rd(p_hwfn, p_ptt, PRS_REG_LIGHT_L2_ETHERTYPE_EN);\n\n\tqed_wr(p_hwfn, p_ptt, PRS_REG_LIGHT_L2_ETHERTYPE_EN,\n\t       (ll2_ethertype_en & 0xFFFE));\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn)) {\n\t\trc = qed_iwarp_stop(p_hwfn);\n\t\tif (rc) {\n\t\t\tqed_ptt_release(p_hwfn, p_ptt);\n\t\t\treturn rc;\n\t\t}\n\t} else {\n\t\tqed_roce_stop(p_hwfn);\n\t}\n\n\tqed_ptt_release(p_hwfn, p_ptt);\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\t \n\trc = qed_sp_init_request(p_hwfn, &p_ent, RDMA_RAMROD_FUNC_CLOSE,\n\t\t\t\t p_hwfn->p_rdma_info->proto, &init_data);\n\tif (rc)\n\t\tgoto out;\n\n\tp_ramrod = &p_ent->ramrod.rdma_close_func;\n\n\tp_ramrod->num_cnqs = p_hwfn->p_rdma_info->num_cnqs;\n\tp_ramrod->cnq_start_offset = (u8)RESC_START(p_hwfn, QED_RDMA_CNQ_RAM);\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\nout:\n\tqed_rdma_free(p_hwfn);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"RDMA stop done, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic int qed_rdma_add_user(void *rdma_cxt,\n\t\t\t     struct qed_rdma_add_user_out_params *out_params)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tu32 dpi_start_offset;\n\tu32 returned_id = 0;\n\tint rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Adding User\\n\");\n\n\t \n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\trc = qed_rdma_bmap_alloc_id(p_hwfn, &p_hwfn->p_rdma_info->dpi_map,\n\t\t\t\t    &returned_id);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n\n\tout_params->dpi = (u16)returned_id;\n\n\t \n\tdpi_start_offset = p_hwfn->dpi_start_offset;\n\n\tout_params->dpi_addr = p_hwfn->doorbells + dpi_start_offset +\n\t\t\t       out_params->dpi * p_hwfn->dpi_size;\n\n\tout_params->dpi_phys_addr = p_hwfn->db_phys_addr +\n\t\t\t\t    dpi_start_offset +\n\t\t\t\t    ((out_params->dpi) * p_hwfn->dpi_size);\n\n\tout_params->dpi_size = p_hwfn->dpi_size;\n\tout_params->wid_count = p_hwfn->wid_count;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Adding user - done, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic struct qed_rdma_port *qed_rdma_query_port(void *rdma_cxt)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tstruct qed_rdma_port *p_port = p_hwfn->p_rdma_info->port;\n\tstruct qed_mcp_link_state *p_link_output;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"RDMA Query port\\n\");\n\n\t \n\tp_link_output = &QED_LEADING_HWFN(p_hwfn->cdev)->mcp_info->link_output;\n\n\tp_port->port_state = p_link_output->link_up ? QED_RDMA_PORT_UP\n\t    : QED_RDMA_PORT_DOWN;\n\n\tp_port->link_speed = p_link_output->speed;\n\n\tp_port->max_msg_size = RDMA_MAX_DATA_SIZE_IN_WQE;\n\n\treturn p_port;\n}\n\nstatic struct qed_rdma_device *qed_rdma_query_device(void *rdma_cxt)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Query device\\n\");\n\n\t \n\treturn p_hwfn->p_rdma_info->dev;\n}\n\nstatic void qed_rdma_cnq_prod_update(void *rdma_cxt, u8 qz_offset, u16 prod)\n{\n\tstruct qed_hwfn *p_hwfn;\n\tu16 qz_num;\n\tu32 addr;\n\n\tp_hwfn = (struct qed_hwfn *)rdma_cxt;\n\n\tif (qz_offset > p_hwfn->p_rdma_info->max_queue_zones) {\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"queue zone offset %d is too large (max is %d)\\n\",\n\t\t\t  qz_offset, p_hwfn->p_rdma_info->max_queue_zones);\n\t\treturn;\n\t}\n\n\tqz_num = p_hwfn->p_rdma_info->queue_zone_base + qz_offset;\n\taddr = GET_GTT_REG_ADDR(GTT_BAR0_MAP_REG_USDM_RAM,\n\t\t\t\tUSTORM_COMMON_QUEUE_CONS, qz_num);\n\n\tREG_WR16(p_hwfn, addr, prod);\n\n\t \n\twmb();\n}\n\nstatic int qed_fill_rdma_dev_info(struct qed_dev *cdev,\n\t\t\t\t  struct qed_dev_rdma_info *info)\n{\n\tstruct qed_hwfn *p_hwfn = QED_AFFIN_HWFN(cdev);\n\n\tmemset(info, 0, sizeof(*info));\n\n\tinfo->rdma_type = QED_IS_ROCE_PERSONALITY(p_hwfn) ?\n\t    QED_RDMA_TYPE_ROCE : QED_RDMA_TYPE_IWARP;\n\n\tinfo->user_dpm_enabled = (p_hwfn->db_bar_no_edpm == 0);\n\n\tqed_fill_dev_info(cdev, &info->common);\n\n\treturn 0;\n}\n\nstatic int qed_rdma_get_sb_start(struct qed_dev *cdev)\n{\n\tint feat_num;\n\n\tif (cdev->num_hwfns > 1)\n\t\tfeat_num = FEAT_NUM(QED_AFFIN_HWFN(cdev), QED_PF_L2_QUE);\n\telse\n\t\tfeat_num = FEAT_NUM(QED_AFFIN_HWFN(cdev), QED_PF_L2_QUE) *\n\t\t\t   cdev->num_hwfns;\n\n\treturn feat_num;\n}\n\nstatic int qed_rdma_get_min_cnq_msix(struct qed_dev *cdev)\n{\n\tint n_cnq = FEAT_NUM(QED_AFFIN_HWFN(cdev), QED_RDMA_CNQ);\n\tint n_msix = cdev->int_params.rdma_msix_cnt;\n\n\treturn min_t(int, n_cnq, n_msix);\n}\n\nstatic int qed_rdma_set_int(struct qed_dev *cdev, u16 cnt)\n{\n\tint limit = 0;\n\n\t \n\tcdev->int_params.fp_initialized = cnt ? true : false;\n\n\tif (cdev->int_params.out.int_mode != QED_INT_MODE_MSIX) {\n\t\tDP_ERR(cdev,\n\t\t       \"qed roce supports only MSI-X interrupts (detected %d).\\n\",\n\t\t       cdev->int_params.out.int_mode);\n\t\treturn -EINVAL;\n\t} else if (cdev->int_params.fp_msix_cnt) {\n\t\tlimit = cdev->int_params.rdma_msix_cnt;\n\t}\n\n\tif (!limit)\n\t\treturn -ENOMEM;\n\n\treturn min_t(int, cnt, limit);\n}\n\nstatic int qed_rdma_get_int(struct qed_dev *cdev, struct qed_int_info *info)\n{\n\tmemset(info, 0, sizeof(*info));\n\n\tif (!cdev->int_params.fp_initialized) {\n\t\tDP_INFO(cdev,\n\t\t\t\"Protocol driver requested interrupt information, but its support is not yet configured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (cdev->int_params.out.int_mode == QED_INT_MODE_MSIX) {\n\t\tint msix_base = cdev->int_params.rdma_msix_base;\n\n\t\tinfo->msix_cnt = cdev->int_params.rdma_msix_cnt;\n\t\tinfo->msix = &cdev->int_params.msix_table[msix_base];\n\n\t\tDP_VERBOSE(cdev, QED_MSG_RDMA, \"msix_cnt = %d msix_base=%d\\n\",\n\t\t\t   info->msix_cnt, msix_base);\n\t}\n\n\treturn 0;\n}\n\nstatic int qed_rdma_alloc_pd(void *rdma_cxt, u16 *pd)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tu32 returned_id;\n\tint rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Alloc PD\\n\");\n\n\t \n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\trc = qed_rdma_bmap_alloc_id(p_hwfn,\n\t\t\t\t    &p_hwfn->p_rdma_info->pd_map, &returned_id);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n\n\t*pd = (u16)returned_id;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Alloc PD - done, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic void qed_rdma_free_pd(void *rdma_cxt, u16 pd)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"pd = %08x\\n\", pd);\n\n\t \n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\tqed_bmap_release_id(p_hwfn, &p_hwfn->p_rdma_info->pd_map, pd);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n}\n\nstatic int qed_rdma_alloc_xrcd(void *rdma_cxt, u16 *xrcd_id)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tu32 returned_id;\n\tint rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Alloc XRCD\\n\");\n\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\trc = qed_rdma_bmap_alloc_id(p_hwfn,\n\t\t\t\t    &p_hwfn->p_rdma_info->xrcd_map,\n\t\t\t\t    &returned_id);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n\tif (rc) {\n\t\tDP_NOTICE(p_hwfn, \"Failed in allocating xrcd id\\n\");\n\t\treturn rc;\n\t}\n\n\t*xrcd_id = (u16)returned_id;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Alloc XRCD - done, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic void qed_rdma_free_xrcd(void *rdma_cxt, u16 xrcd_id)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"xrcd_id = %08x\\n\", xrcd_id);\n\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\tqed_bmap_release_id(p_hwfn, &p_hwfn->p_rdma_info->xrcd_map, xrcd_id);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n}\n\nstatic enum qed_rdma_toggle_bit\nqed_rdma_toggle_bit_create_resize_cq(struct qed_hwfn *p_hwfn, u16 icid)\n{\n\tstruct qed_rdma_info *p_info = p_hwfn->p_rdma_info;\n\tenum qed_rdma_toggle_bit toggle_bit;\n\tu32 bmap_id;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", icid);\n\n\t \n\tbmap_id = icid - qed_cxt_get_proto_cid_start(p_hwfn, p_info->proto);\n\n\tspin_lock_bh(&p_info->lock);\n\ttoggle_bit = !test_and_change_bit(bmap_id,\n\t\t\t\t\t  p_info->toggle_bits.bitmap);\n\tspin_unlock_bh(&p_info->lock);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"QED_RDMA_TOGGLE_BIT_= %d\\n\",\n\t\t   toggle_bit);\n\n\treturn toggle_bit;\n}\n\nstatic int qed_rdma_create_cq(void *rdma_cxt,\n\t\t\t      struct qed_rdma_create_cq_in_params *params,\n\t\t\t      u16 *icid)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tstruct qed_rdma_info *p_info = p_hwfn->p_rdma_info;\n\tstruct rdma_create_cq_ramrod_data *p_ramrod;\n\tenum qed_rdma_toggle_bit toggle_bit;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tu32 returned_id, start_cid;\n\tint rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"cq_handle = %08x%08x\\n\",\n\t\t   params->cq_handle_hi, params->cq_handle_lo);\n\n\t \n\tspin_lock_bh(&p_info->lock);\n\trc = qed_rdma_bmap_alloc_id(p_hwfn, &p_info->cq_map, &returned_id);\n\tspin_unlock_bh(&p_info->lock);\n\n\tif (rc) {\n\t\tDP_NOTICE(p_hwfn, \"Can't create CQ, rc = %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tstart_cid = qed_cxt_get_proto_cid_start(p_hwfn,\n\t\t\t\t\t\tp_info->proto);\n\t*icid = returned_id + start_cid;\n\n\t \n\trc = qed_cxt_dynamic_ilt_alloc(p_hwfn, QED_ELEM_CXT, *icid);\n\tif (rc)\n\t\tgoto err;\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.cid = *icid;\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\t \n\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t RDMA_RAMROD_CREATE_CQ,\n\t\t\t\t p_info->proto, &init_data);\n\tif (rc)\n\t\tgoto err;\n\n\tp_ramrod = &p_ent->ramrod.rdma_create_cq;\n\n\tp_ramrod->cq_handle.hi = cpu_to_le32(params->cq_handle_hi);\n\tp_ramrod->cq_handle.lo = cpu_to_le32(params->cq_handle_lo);\n\tp_ramrod->dpi = cpu_to_le16(params->dpi);\n\tp_ramrod->is_two_level_pbl = params->pbl_two_level;\n\tp_ramrod->max_cqes = cpu_to_le32(params->cq_size);\n\tDMA_REGPAIR_LE(p_ramrod->pbl_addr, params->pbl_ptr);\n\tp_ramrod->pbl_num_pages = cpu_to_le16(params->pbl_num_pages);\n\tp_ramrod->cnq_id = (u8)RESC_START(p_hwfn, QED_RDMA_CNQ_RAM) +\n\t\t\t   params->cnq_id;\n\tp_ramrod->int_timeout = cpu_to_le16(params->int_timeout);\n\n\t \n\ttoggle_bit = qed_rdma_toggle_bit_create_resize_cq(p_hwfn, *icid);\n\n\tp_ramrod->toggle_bit = toggle_bit;\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc) {\n\t\t \n\t\tqed_rdma_toggle_bit_create_resize_cq(p_hwfn, *icid);\n\t\tgoto err;\n\t}\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Created CQ, rc = %d\\n\", rc);\n\treturn rc;\n\nerr:\n\t \n\tspin_lock_bh(&p_info->lock);\n\tqed_bmap_release_id(p_hwfn, &p_info->cq_map, returned_id);\n\tspin_unlock_bh(&p_info->lock);\n\tDP_NOTICE(p_hwfn, \"Create CQ failed, rc = %d\\n\", rc);\n\n\treturn rc;\n}\n\nstatic int\nqed_rdma_destroy_cq(void *rdma_cxt,\n\t\t    struct qed_rdma_destroy_cq_in_params *in_params,\n\t\t    struct qed_rdma_destroy_cq_out_params *out_params)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tstruct rdma_destroy_cq_output_params *p_ramrod_res;\n\tstruct rdma_destroy_cq_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tdma_addr_t ramrod_res_phys;\n\tenum protocol_type proto;\n\tint rc = -ENOMEM;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", in_params->icid);\n\n\tp_ramrod_res =\n\t    dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t       sizeof(struct rdma_destroy_cq_output_params),\n\t\t\t       &ramrod_res_phys, GFP_KERNEL);\n\tif (!p_ramrod_res) {\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"qed destroy cq failed: cannot allocate memory (ramrod)\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.cid = in_params->icid;\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\tproto = p_hwfn->p_rdma_info->proto;\n\t \n\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t RDMA_RAMROD_DESTROY_CQ,\n\t\t\t\t proto, &init_data);\n\tif (rc)\n\t\tgoto err;\n\n\tp_ramrod = &p_ent->ramrod.rdma_destroy_cq;\n\tDMA_REGPAIR_LE(p_ramrod->output_params_addr, ramrod_res_phys);\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\tgoto err;\n\n\tout_params->num_cq_notif = le16_to_cpu(p_ramrod_res->cnq_num);\n\n\tdma_free_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t  sizeof(struct rdma_destroy_cq_output_params),\n\t\t\t  p_ramrod_res, ramrod_res_phys);\n\n\t \n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\n\tqed_bmap_release_id(p_hwfn,\n\t\t\t    &p_hwfn->p_rdma_info->cq_map,\n\t\t\t    (in_params->icid -\n\t\t\t     qed_cxt_get_proto_cid_start(p_hwfn, proto)));\n\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Destroyed CQ, rc = %d\\n\", rc);\n\treturn rc;\n\nerr:\tdma_free_coherent(&p_hwfn->cdev->pdev->dev,\n\t\t\t  sizeof(struct rdma_destroy_cq_output_params),\n\t\t\t  p_ramrod_res, ramrod_res_phys);\n\n\treturn rc;\n}\n\nvoid qed_rdma_set_fw_mac(__le16 *p_fw_mac, const u8 *p_qed_mac)\n{\n\tp_fw_mac[0] = cpu_to_le16((p_qed_mac[0] << 8) + p_qed_mac[1]);\n\tp_fw_mac[1] = cpu_to_le16((p_qed_mac[2] << 8) + p_qed_mac[3]);\n\tp_fw_mac[2] = cpu_to_le16((p_qed_mac[4] << 8) + p_qed_mac[5]);\n}\n\nstatic int qed_rdma_query_qp(void *rdma_cxt,\n\t\t\t     struct qed_rdma_qp *qp,\n\t\t\t     struct qed_rdma_query_qp_out_params *out_params)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tint rc = 0;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", qp->icid);\n\n\t \n\tout_params->mtu = qp->mtu;\n\tout_params->dest_qp = qp->dest_qp;\n\tout_params->incoming_atomic_en = qp->incoming_atomic_en;\n\tout_params->e2e_flow_control_en = qp->e2e_flow_control_en;\n\tout_params->incoming_rdma_read_en = qp->incoming_rdma_read_en;\n\tout_params->incoming_rdma_write_en = qp->incoming_rdma_write_en;\n\tout_params->dgid = qp->dgid;\n\tout_params->flow_label = qp->flow_label;\n\tout_params->hop_limit_ttl = qp->hop_limit_ttl;\n\tout_params->traffic_class_tos = qp->traffic_class_tos;\n\tout_params->timeout = qp->ack_timeout;\n\tout_params->rnr_retry = qp->rnr_retry_cnt;\n\tout_params->retry_cnt = qp->retry_cnt;\n\tout_params->min_rnr_nak_timer = qp->min_rnr_nak_timer;\n\tout_params->pkey_index = 0;\n\tout_params->max_rd_atomic = qp->max_rd_atomic_req;\n\tout_params->max_dest_rd_atomic = qp->max_rd_atomic_resp;\n\tout_params->sqd_async = qp->sqd_async;\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn))\n\t\tqed_iwarp_query_qp(qp, out_params);\n\telse\n\t\trc = qed_roce_query_qp(p_hwfn, qp, out_params);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Query QP, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic int qed_rdma_destroy_qp(void *rdma_cxt, struct qed_rdma_qp *qp)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tint rc = 0;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x\\n\", qp->icid);\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn))\n\t\trc = qed_iwarp_destroy_qp(p_hwfn, qp);\n\telse\n\t\trc = qed_roce_destroy_qp(p_hwfn, qp);\n\n\t \n\tkfree(qp);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"QP destroyed\\n\");\n\treturn rc;\n}\n\nstatic struct qed_rdma_qp *\nqed_rdma_create_qp(void *rdma_cxt,\n\t\t   struct qed_rdma_create_qp_in_params *in_params,\n\t\t   struct qed_rdma_create_qp_out_params *out_params)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tstruct qed_rdma_qp *qp;\n\tu8 max_stats_queues;\n\tint rc;\n\n\tif (!rdma_cxt || !in_params || !out_params ||\n\t    !p_hwfn->p_rdma_info->active) {\n\t\tpr_err(\"qed roce create qp failed due to NULL entry (rdma_cxt=%p, in=%p, out=%p, roce_info=?\\n\",\n\t\t       rdma_cxt, in_params, out_params);\n\t\treturn NULL;\n\t}\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t   \"qed rdma create qp called with qp_handle = %08x%08x\\n\",\n\t\t   in_params->qp_handle_hi, in_params->qp_handle_lo);\n\n\t \n\tmax_stats_queues = p_hwfn->p_rdma_info->dev->max_stats_queues;\n\tif (in_params->stats_queue >= max_stats_queues) {\n\t\tDP_ERR(p_hwfn->cdev,\n\t\t       \"qed rdma create qp failed due to invalid statistics queue %d. maximum is %d\\n\",\n\t\t       in_params->stats_queue, max_stats_queues);\n\t\treturn NULL;\n\t}\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn)) {\n\t\tif (in_params->sq_num_pages * sizeof(struct regpair) >\n\t\t    IWARP_SHARED_QUEUE_PAGE_SQ_PBL_MAX_SIZE) {\n\t\t\tDP_NOTICE(p_hwfn->cdev,\n\t\t\t\t  \"Sq num pages: %d exceeds maximum\\n\",\n\t\t\t\t  in_params->sq_num_pages);\n\t\t\treturn NULL;\n\t\t}\n\t\tif (in_params->rq_num_pages * sizeof(struct regpair) >\n\t\t    IWARP_SHARED_QUEUE_PAGE_RQ_PBL_MAX_SIZE) {\n\t\t\tDP_NOTICE(p_hwfn->cdev,\n\t\t\t\t  \"Rq num pages: %d exceeds maximum\\n\",\n\t\t\t\t  in_params->rq_num_pages);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tqp = kzalloc(sizeof(*qp), GFP_KERNEL);\n\tif (!qp)\n\t\treturn NULL;\n\n\tqp->cur_state = QED_ROCE_QP_STATE_RESET;\n\tqp->qp_handle.hi = cpu_to_le32(in_params->qp_handle_hi);\n\tqp->qp_handle.lo = cpu_to_le32(in_params->qp_handle_lo);\n\tqp->qp_handle_async.hi = cpu_to_le32(in_params->qp_handle_async_hi);\n\tqp->qp_handle_async.lo = cpu_to_le32(in_params->qp_handle_async_lo);\n\tqp->use_srq = in_params->use_srq;\n\tqp->signal_all = in_params->signal_all;\n\tqp->fmr_and_reserved_lkey = in_params->fmr_and_reserved_lkey;\n\tqp->pd = in_params->pd;\n\tqp->dpi = in_params->dpi;\n\tqp->sq_cq_id = in_params->sq_cq_id;\n\tqp->sq_num_pages = in_params->sq_num_pages;\n\tqp->sq_pbl_ptr = in_params->sq_pbl_ptr;\n\tqp->rq_cq_id = in_params->rq_cq_id;\n\tqp->rq_num_pages = in_params->rq_num_pages;\n\tqp->rq_pbl_ptr = in_params->rq_pbl_ptr;\n\tqp->srq_id = in_params->srq_id;\n\tqp->req_offloaded = false;\n\tqp->resp_offloaded = false;\n\tqp->e2e_flow_control_en = qp->use_srq ? false : true;\n\tqp->stats_queue = in_params->stats_queue;\n\tqp->qp_type = in_params->qp_type;\n\tqp->xrcd_id = in_params->xrcd_id;\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn)) {\n\t\trc = qed_iwarp_create_qp(p_hwfn, qp, out_params);\n\t\tqp->qpid = qp->icid;\n\t} else {\n\t\tqp->edpm_mode = GET_FIELD(in_params->flags, QED_ROCE_EDPM_MODE);\n\t\trc = qed_roce_alloc_cid(p_hwfn, &qp->icid);\n\t\tqp->qpid = ((0xFF << 16) | qp->icid);\n\t}\n\n\tif (rc) {\n\t\tkfree(qp);\n\t\treturn NULL;\n\t}\n\n\tout_params->icid = qp->icid;\n\tout_params->qp_id = qp->qpid;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Create QP, rc = %d\\n\", rc);\n\treturn qp;\n}\n\nstatic int qed_rdma_modify_qp(void *rdma_cxt,\n\t\t\t      struct qed_rdma_qp *qp,\n\t\t\t      struct qed_rdma_modify_qp_in_params *params)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tenum qed_roce_qp_state prev_state;\n\tint rc = 0;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"icid = %08x params->new_state=%d\\n\",\n\t\t   qp->icid, params->new_state);\n\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"rc = %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tif (GET_FIELD(params->modify_flags,\n\t\t      QED_RDMA_MODIFY_QP_VALID_RDMA_OPS_EN)) {\n\t\tqp->incoming_rdma_read_en = params->incoming_rdma_read_en;\n\t\tqp->incoming_rdma_write_en = params->incoming_rdma_write_en;\n\t\tqp->incoming_atomic_en = params->incoming_atomic_en;\n\t}\n\n\t \n\tif (GET_FIELD(params->modify_flags, QED_ROCE_MODIFY_QP_VALID_ROCE_MODE))\n\t\tqp->roce_mode = params->roce_mode;\n\tif (GET_FIELD(params->modify_flags, QED_ROCE_MODIFY_QP_VALID_PKEY))\n\t\tqp->pkey = params->pkey;\n\tif (GET_FIELD(params->modify_flags,\n\t\t      QED_ROCE_MODIFY_QP_VALID_E2E_FLOW_CONTROL_EN))\n\t\tqp->e2e_flow_control_en = params->e2e_flow_control_en;\n\tif (GET_FIELD(params->modify_flags, QED_ROCE_MODIFY_QP_VALID_DEST_QP))\n\t\tqp->dest_qp = params->dest_qp;\n\tif (GET_FIELD(params->modify_flags,\n\t\t      QED_ROCE_MODIFY_QP_VALID_ADDRESS_VECTOR)) {\n\t\t \n\t\tqp->traffic_class_tos = params->traffic_class_tos;\n\t\tqp->flow_label = params->flow_label;\n\t\tqp->hop_limit_ttl = params->hop_limit_ttl;\n\n\t\tqp->sgid = params->sgid;\n\t\tqp->dgid = params->dgid;\n\t\tqp->udp_src_port = 0;\n\t\tqp->vlan_id = params->vlan_id;\n\t\tqp->mtu = params->mtu;\n\t\tqp->lb_indication = params->lb_indication;\n\t\tmemcpy((u8 *)&qp->remote_mac_addr[0],\n\t\t       (u8 *)&params->remote_mac_addr[0], ETH_ALEN);\n\t\tif (params->use_local_mac) {\n\t\t\tmemcpy((u8 *)&qp->local_mac_addr[0],\n\t\t\t       (u8 *)&params->local_mac_addr[0], ETH_ALEN);\n\t\t} else {\n\t\t\tmemcpy((u8 *)&qp->local_mac_addr[0],\n\t\t\t       (u8 *)&p_hwfn->hw_info.hw_mac_addr, ETH_ALEN);\n\t\t}\n\t}\n\tif (GET_FIELD(params->modify_flags, QED_ROCE_MODIFY_QP_VALID_RQ_PSN))\n\t\tqp->rq_psn = params->rq_psn;\n\tif (GET_FIELD(params->modify_flags, QED_ROCE_MODIFY_QP_VALID_SQ_PSN))\n\t\tqp->sq_psn = params->sq_psn;\n\tif (GET_FIELD(params->modify_flags,\n\t\t      QED_RDMA_MODIFY_QP_VALID_MAX_RD_ATOMIC_REQ))\n\t\tqp->max_rd_atomic_req = params->max_rd_atomic_req;\n\tif (GET_FIELD(params->modify_flags,\n\t\t      QED_RDMA_MODIFY_QP_VALID_MAX_RD_ATOMIC_RESP))\n\t\tqp->max_rd_atomic_resp = params->max_rd_atomic_resp;\n\tif (GET_FIELD(params->modify_flags,\n\t\t      QED_ROCE_MODIFY_QP_VALID_ACK_TIMEOUT))\n\t\tqp->ack_timeout = params->ack_timeout;\n\tif (GET_FIELD(params->modify_flags, QED_ROCE_MODIFY_QP_VALID_RETRY_CNT))\n\t\tqp->retry_cnt = params->retry_cnt;\n\tif (GET_FIELD(params->modify_flags,\n\t\t      QED_ROCE_MODIFY_QP_VALID_RNR_RETRY_CNT))\n\t\tqp->rnr_retry_cnt = params->rnr_retry_cnt;\n\tif (GET_FIELD(params->modify_flags,\n\t\t      QED_ROCE_MODIFY_QP_VALID_MIN_RNR_NAK_TIMER))\n\t\tqp->min_rnr_nak_timer = params->min_rnr_nak_timer;\n\n\tqp->sqd_async = params->sqd_async;\n\n\tprev_state = qp->cur_state;\n\tif (GET_FIELD(params->modify_flags,\n\t\t      QED_RDMA_MODIFY_QP_VALID_NEW_STATE)) {\n\t\tqp->cur_state = params->new_state;\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"qp->cur_state=%d\\n\",\n\t\t\t   qp->cur_state);\n\t}\n\n\tswitch (qp->qp_type) {\n\tcase QED_RDMA_QP_TYPE_XRC_INI:\n\t\tqp->has_req = true;\n\t\tbreak;\n\tcase QED_RDMA_QP_TYPE_XRC_TGT:\n\t\tqp->has_resp = true;\n\t\tbreak;\n\tdefault:\n\t\tqp->has_req  = true;\n\t\tqp->has_resp = true;\n\t}\n\n\tif (QED_IS_IWARP_PERSONALITY(p_hwfn)) {\n\t\tenum qed_iwarp_qp_state new_state =\n\t\t    qed_roce2iwarp_state(qp->cur_state);\n\n\t\trc = qed_iwarp_modify_qp(p_hwfn, qp, new_state, 0);\n\t} else {\n\t\trc = qed_roce_modify_qp(p_hwfn, qp, prev_state, params);\n\t}\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Modify QP, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic int\nqed_rdma_register_tid(void *rdma_cxt,\n\t\t      struct qed_rdma_register_tid_in_params *params)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tstruct rdma_register_tid_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tenum rdma_tid_type tid_type;\n\tu8 fw_return_code;\n\tu16 flags = 0;\n\tint rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"itid = %08x\\n\", params->itid);\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent, RDMA_RAMROD_REGISTER_MR,\n\t\t\t\t p_hwfn->p_rdma_info->proto, &init_data);\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"rc = %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tif (p_hwfn->p_rdma_info->last_tid < params->itid)\n\t\tp_hwfn->p_rdma_info->last_tid = params->itid;\n\n\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_TWO_LEVEL_PBL,\n\t\t  params->pbl_two_level);\n\n\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_ZERO_BASED,\n\t\t  false);\n\n\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_PHY_MR, params->phy_mr);\n\n\t \n\tif (!(params->tid_type == QED_RDMA_TID_FMR) && !(params->dma_mr))\n\t\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_PAGE_SIZE_LOG,\n\t\t\t  params->page_size_log - 12);\n\n\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_REMOTE_READ,\n\t\t  params->remote_read);\n\n\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_REMOTE_WRITE,\n\t\t  params->remote_write);\n\n\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_REMOTE_ATOMIC,\n\t\t  params->remote_atomic);\n\n\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_LOCAL_WRITE,\n\t\t  params->local_write);\n\n\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_LOCAL_READ,\n\t\t  params->local_read);\n\n\tSET_FIELD(flags, RDMA_REGISTER_TID_RAMROD_DATA_ENABLE_MW_BIND,\n\t\t  params->mw_bind);\n\n\tp_ramrod = &p_ent->ramrod.rdma_register_tid;\n\tp_ramrod->flags = cpu_to_le16(flags);\n\n\tSET_FIELD(p_ramrod->flags1,\n\t\t  RDMA_REGISTER_TID_RAMROD_DATA_PBL_PAGE_SIZE_LOG,\n\t\t  params->pbl_page_size_log - 12);\n\n\tSET_FIELD(p_ramrod->flags2, RDMA_REGISTER_TID_RAMROD_DATA_DMA_MR,\n\t\t  params->dma_mr);\n\n\tswitch (params->tid_type) {\n\tcase QED_RDMA_TID_REGISTERED_MR:\n\t\ttid_type = RDMA_TID_REGISTERED_MR;\n\t\tbreak;\n\tcase QED_RDMA_TID_FMR:\n\t\ttid_type = RDMA_TID_FMR;\n\t\tbreak;\n\tcase QED_RDMA_TID_MW:\n\t\ttid_type = RDMA_TID_MW;\n\t\tbreak;\n\tdefault:\n\t\trc = -EINVAL;\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"rc = %d\\n\", rc);\n\t\tqed_sp_destroy_request(p_hwfn, p_ent);\n\t\treturn rc;\n\t}\n\n\tSET_FIELD(p_ramrod->flags1, RDMA_REGISTER_TID_RAMROD_DATA_TID_TYPE,\n\t\t  tid_type);\n\n\tp_ramrod->itid = cpu_to_le32(params->itid);\n\tp_ramrod->key = params->key;\n\tp_ramrod->pd = cpu_to_le16(params->pd);\n\tp_ramrod->length_hi = (u8)(params->length >> 32);\n\tp_ramrod->length_lo = DMA_LO_LE(params->length);\n\tDMA_REGPAIR_LE(p_ramrod->va, params->vaddr);\n\tDMA_REGPAIR_LE(p_ramrod->pbl_base, params->pbl_ptr);\n\n\t \n\tif (params->dif_enabled) {\n\t\tSET_FIELD(p_ramrod->flags2,\n\t\t\t  RDMA_REGISTER_TID_RAMROD_DATA_DIF_ON_HOST_FLG, 1);\n\t\tDMA_REGPAIR_LE(p_ramrod->dif_error_addr,\n\t\t\t       params->dif_error_addr);\n\t}\n\n\trc = qed_spq_post(p_hwfn, p_ent, &fw_return_code);\n\tif (rc)\n\t\treturn rc;\n\n\tif (fw_return_code != RDMA_RETURN_OK) {\n\t\tDP_NOTICE(p_hwfn, \"fw_return_code = %d\\n\", fw_return_code);\n\t\treturn -EINVAL;\n\t}\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"Register TID, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic int qed_rdma_deregister_tid(void *rdma_cxt, u32 itid)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tstruct rdma_deregister_tid_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data;\n\tstruct qed_spq_entry *p_ent;\n\tstruct qed_ptt *p_ptt;\n\tu8 fw_return_code;\n\tint rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"itid = %08x\\n\", itid);\n\n\t \n\tmemset(&init_data, 0, sizeof(init_data));\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent, RDMA_RAMROD_DEREGISTER_MR,\n\t\t\t\t p_hwfn->p_rdma_info->proto, &init_data);\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"rc = %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tp_ramrod = &p_ent->ramrod.rdma_deregister_tid;\n\tp_ramrod->itid = cpu_to_le32(itid);\n\n\trc = qed_spq_post(p_hwfn, p_ent, &fw_return_code);\n\tif (rc) {\n\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"rc = %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\tif (fw_return_code == RDMA_RETURN_DEREGISTER_MR_BAD_STATE_ERR) {\n\t\tDP_NOTICE(p_hwfn, \"fw_return_code = %d\\n\", fw_return_code);\n\t\treturn -EINVAL;\n\t} else if (fw_return_code == RDMA_RETURN_NIG_DRAIN_REQ) {\n\t\t \n\t\tp_ptt = qed_ptt_acquire(p_hwfn);\n\t\tif (!p_ptt) {\n\t\t\trc = -EBUSY;\n\t\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t\t   \"Failed to acquire PTT\\n\");\n\t\t\treturn rc;\n\t\t}\n\n\t\trc = qed_mcp_drain(p_hwfn, p_ptt);\n\t\tif (rc) {\n\t\t\tqed_ptt_release(p_hwfn, p_ptt);\n\t\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t\t   \"Drain failed\\n\");\n\t\t\treturn rc;\n\t\t}\n\n\t\tqed_ptt_release(p_hwfn, p_ptt);\n\n\t\t \n\t\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t\t RDMA_RAMROD_DEREGISTER_MR,\n\t\t\t\t\t p_hwfn->p_rdma_info->proto,\n\t\t\t\t\t &init_data);\n\t\tif (rc) {\n\t\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t\t   \"Failed to init sp-element\\n\");\n\t\t\treturn rc;\n\t\t}\n\n\t\trc = qed_spq_post(p_hwfn, p_ent, &fw_return_code);\n\t\tif (rc) {\n\t\t\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t\t\t   \"Ramrod failed\\n\");\n\t\t\treturn rc;\n\t\t}\n\n\t\tif (fw_return_code != RDMA_RETURN_OK) {\n\t\t\tDP_NOTICE(p_hwfn, \"fw_return_code = %d\\n\",\n\t\t\t\t  fw_return_code);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"De-registered TID, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic void *qed_rdma_get_rdma_ctx(struct qed_dev *cdev)\n{\n\treturn QED_AFFIN_HWFN(cdev);\n}\n\nstatic struct qed_bmap *qed_rdma_get_srq_bmap(struct qed_hwfn *p_hwfn,\n\t\t\t\t\t      bool is_xrc)\n{\n\tif (is_xrc)\n\t\treturn &p_hwfn->p_rdma_info->xrc_srq_map;\n\n\treturn &p_hwfn->p_rdma_info->srq_map;\n}\n\nstatic int qed_rdma_modify_srq(void *rdma_cxt,\n\t\t\t       struct qed_rdma_modify_srq_in_params *in_params)\n{\n\tstruct rdma_srq_modify_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data = {};\n\tstruct qed_hwfn *p_hwfn = rdma_cxt;\n\tstruct qed_spq_entry *p_ent;\n\tu16 opaque_fid;\n\tint rc;\n\n\tinit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t RDMA_RAMROD_MODIFY_SRQ,\n\t\t\t\t p_hwfn->p_rdma_info->proto, &init_data);\n\tif (rc)\n\t\treturn rc;\n\n\tp_ramrod = &p_ent->ramrod.rdma_modify_srq;\n\tp_ramrod->srq_id.srq_idx = cpu_to_le16(in_params->srq_id);\n\topaque_fid = p_hwfn->hw_info.opaque_fid;\n\tp_ramrod->srq_id.opaque_fid = cpu_to_le16(opaque_fid);\n\tp_ramrod->wqe_limit = cpu_to_le32(in_params->wqe_limit);\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\treturn rc;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"modified SRQ id = %x, is_xrc=%u\\n\",\n\t\t   in_params->srq_id, in_params->is_xrc);\n\n\treturn rc;\n}\n\nstatic int\nqed_rdma_destroy_srq(void *rdma_cxt,\n\t\t     struct qed_rdma_destroy_srq_in_params *in_params)\n{\n\tstruct rdma_srq_destroy_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data = {};\n\tstruct qed_hwfn *p_hwfn = rdma_cxt;\n\tstruct qed_spq_entry *p_ent;\n\tstruct qed_bmap *bmap;\n\tu16 opaque_fid;\n\tu16 offset;\n\tint rc;\n\n\topaque_fid = p_hwfn->hw_info.opaque_fid;\n\n\tinit_data.opaque_fid = opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t RDMA_RAMROD_DESTROY_SRQ,\n\t\t\t\t p_hwfn->p_rdma_info->proto, &init_data);\n\tif (rc)\n\t\treturn rc;\n\n\tp_ramrod = &p_ent->ramrod.rdma_destroy_srq;\n\tp_ramrod->srq_id.srq_idx = cpu_to_le16(in_params->srq_id);\n\tp_ramrod->srq_id.opaque_fid = cpu_to_le16(opaque_fid);\n\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\treturn rc;\n\n\tbmap = qed_rdma_get_srq_bmap(p_hwfn, in_params->is_xrc);\n\toffset = (in_params->is_xrc) ? 0 : p_hwfn->p_rdma_info->srq_id_offset;\n\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\tqed_bmap_release_id(p_hwfn, bmap, in_params->srq_id - offset);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t   \"XRC/SRQ destroyed Id = %x, is_xrc=%u\\n\",\n\t\t   in_params->srq_id, in_params->is_xrc);\n\n\treturn rc;\n}\n\nstatic int\nqed_rdma_create_srq(void *rdma_cxt,\n\t\t    struct qed_rdma_create_srq_in_params *in_params,\n\t\t    struct qed_rdma_create_srq_out_params *out_params)\n{\n\tstruct rdma_srq_create_ramrod_data *p_ramrod;\n\tstruct qed_sp_init_data init_data = {};\n\tstruct qed_hwfn *p_hwfn = rdma_cxt;\n\tenum qed_cxt_elem_type elem_type;\n\tstruct qed_spq_entry *p_ent;\n\tu16 opaque_fid, srq_id;\n\tstruct qed_bmap *bmap;\n\tu32 returned_id;\n\tu16 offset;\n\tint rc;\n\n\tbmap = qed_rdma_get_srq_bmap(p_hwfn, in_params->is_xrc);\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\trc = qed_rdma_bmap_alloc_id(p_hwfn, bmap, &returned_id);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n\n\tif (rc) {\n\t\tDP_NOTICE(p_hwfn,\n\t\t\t  \"failed to allocate xrc/srq id (is_xrc=%u)\\n\",\n\t\t\t  in_params->is_xrc);\n\t\treturn rc;\n\t}\n\n\telem_type = (in_params->is_xrc) ? (QED_ELEM_XRC_SRQ) : (QED_ELEM_SRQ);\n\trc = qed_cxt_dynamic_ilt_alloc(p_hwfn, elem_type, returned_id);\n\tif (rc)\n\t\tgoto err;\n\n\topaque_fid = p_hwfn->hw_info.opaque_fid;\n\n\topaque_fid = p_hwfn->hw_info.opaque_fid;\n\tinit_data.opaque_fid = opaque_fid;\n\tinit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\n\n\trc = qed_sp_init_request(p_hwfn, &p_ent,\n\t\t\t\t RDMA_RAMROD_CREATE_SRQ,\n\t\t\t\t p_hwfn->p_rdma_info->proto, &init_data);\n\tif (rc)\n\t\tgoto err;\n\n\tp_ramrod = &p_ent->ramrod.rdma_create_srq;\n\tDMA_REGPAIR_LE(p_ramrod->pbl_base_addr, in_params->pbl_base_addr);\n\tp_ramrod->pages_in_srq_pbl = cpu_to_le16(in_params->num_pages);\n\tp_ramrod->pd_id = cpu_to_le16(in_params->pd_id);\n\tp_ramrod->srq_id.opaque_fid = cpu_to_le16(opaque_fid);\n\tp_ramrod->page_size = cpu_to_le16(in_params->page_size);\n\tDMA_REGPAIR_LE(p_ramrod->producers_addr, in_params->prod_pair_addr);\n\toffset = (in_params->is_xrc) ? 0 : p_hwfn->p_rdma_info->srq_id_offset;\n\tsrq_id = (u16)returned_id + offset;\n\tp_ramrod->srq_id.srq_idx = cpu_to_le16(srq_id);\n\n\tif (in_params->is_xrc) {\n\t\tSET_FIELD(p_ramrod->flags,\n\t\t\t  RDMA_SRQ_CREATE_RAMROD_DATA_XRC_FLAG, 1);\n\t\tSET_FIELD(p_ramrod->flags,\n\t\t\t  RDMA_SRQ_CREATE_RAMROD_DATA_RESERVED_KEY_EN,\n\t\t\t  in_params->reserved_key_en);\n\t\tp_ramrod->xrc_srq_cq_cid =\n\t\t\tcpu_to_le32((p_hwfn->hw_info.opaque_fid << 16) |\n\t\t\t\t     in_params->cq_cid);\n\t\tp_ramrod->xrc_domain = cpu_to_le16(in_params->xrcd_id);\n\t}\n\trc = qed_spq_post(p_hwfn, p_ent, NULL);\n\tif (rc)\n\t\tgoto err;\n\n\tout_params->srq_id = srq_id;\n\n\tDP_VERBOSE(p_hwfn,\n\t\t   QED_MSG_RDMA,\n\t\t   \"XRC/SRQ created Id = %x (is_xrc=%u)\\n\",\n\t\t   out_params->srq_id, in_params->is_xrc);\n\treturn rc;\n\nerr:\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\tqed_bmap_release_id(p_hwfn, bmap, returned_id);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n\n\treturn rc;\n}\n\nbool qed_rdma_allocated_qps(struct qed_hwfn *p_hwfn)\n{\n\tbool result;\n\n\t \n\tif (!p_hwfn->p_rdma_info->active)\n\t\treturn false;\n\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\tif (!p_hwfn->p_rdma_info->cid_map.bitmap)\n\t\tresult = false;\n\telse\n\t\tresult = !qed_bmap_is_empty(&p_hwfn->p_rdma_info->cid_map);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n\treturn result;\n}\n\nvoid qed_rdma_dpm_conf(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)\n{\n\tu32 val;\n\n\tval = (p_hwfn->dcbx_no_edpm || p_hwfn->db_bar_no_edpm) ? 0 : 1;\n\n\tqed_wr(p_hwfn, p_ptt, DORQ_REG_PF_DPM_ENABLE, val);\n\tDP_VERBOSE(p_hwfn, (QED_MSG_DCB | QED_MSG_RDMA),\n\t\t   \"Changing DPM_EN state to %d (DCBX=%d, DB_BAR=%d)\\n\",\n\t\t   val, p_hwfn->dcbx_no_edpm, p_hwfn->db_bar_no_edpm);\n}\n\nvoid qed_rdma_dpm_bar(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)\n{\n\tp_hwfn->db_bar_no_edpm = true;\n\n\tqed_rdma_dpm_conf(p_hwfn, p_ptt);\n}\n\nstatic int qed_rdma_start(void *rdma_cxt,\n\t\t\t  struct qed_rdma_start_in_params *params)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\tstruct qed_ptt *p_ptt;\n\tint rc = -EBUSY;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\n\t\t   \"desired_cnq = %08x\\n\", params->desired_cnq);\n\n\tp_ptt = qed_ptt_acquire(p_hwfn);\n\tif (!p_ptt)\n\t\tgoto err;\n\n\trc = qed_rdma_alloc(p_hwfn);\n\tif (rc)\n\t\tgoto err1;\n\n\trc = qed_rdma_setup(p_hwfn, p_ptt, params);\n\tif (rc)\n\t\tgoto err2;\n\n\tqed_ptt_release(p_hwfn, p_ptt);\n\tp_hwfn->p_rdma_info->active = 1;\n\n\treturn rc;\n\nerr2:\n\tqed_rdma_free(p_hwfn);\nerr1:\n\tqed_ptt_release(p_hwfn, p_ptt);\nerr:\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"RDMA start - error, rc = %d\\n\", rc);\n\treturn rc;\n}\n\nstatic int qed_rdma_init(struct qed_dev *cdev,\n\t\t\t struct qed_rdma_start_in_params *params)\n{\n\treturn qed_rdma_start(QED_AFFIN_HWFN(cdev), params);\n}\n\nstatic void qed_rdma_remove_user(void *rdma_cxt, u16 dpi)\n{\n\tstruct qed_hwfn *p_hwfn = (struct qed_hwfn *)rdma_cxt;\n\n\tDP_VERBOSE(p_hwfn, QED_MSG_RDMA, \"dpi = %08x\\n\", dpi);\n\n\tspin_lock_bh(&p_hwfn->p_rdma_info->lock);\n\tqed_bmap_release_id(p_hwfn, &p_hwfn->p_rdma_info->dpi_map, dpi);\n\tspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\n}\n\nstatic int qed_roce_ll2_set_mac_filter(struct qed_dev *cdev,\n\t\t\t\t       u8 *old_mac_address,\n\t\t\t\t       const u8 *new_mac_address)\n{\n\tint rc = 0;\n\n\tif (old_mac_address)\n\t\tqed_llh_remove_mac_filter(cdev, 0, old_mac_address);\n\tif (new_mac_address)\n\t\trc = qed_llh_add_mac_filter(cdev, 0, new_mac_address);\n\n\tif (rc)\n\t\tDP_ERR(cdev,\n\t\t       \"qed roce ll2 mac filter set: failed to add MAC filter\\n\");\n\n\treturn rc;\n}\n\nstatic int qed_iwarp_set_engine_affin(struct qed_dev *cdev, bool b_reset)\n{\n\tenum qed_eng eng;\n\tu8 ppfid = 0;\n\tint rc;\n\n\t \n\tif (!cdev->iwarp_cmt)\n\t\treturn -EINVAL;\n\n\tif (b_reset)\n\t\teng = QED_BOTH_ENG;\n\telse\n\t\teng = cdev->l2_affin_hint ? QED_ENG1 : QED_ENG0;\n\n\trc = qed_llh_set_ppfid_affinity(cdev, ppfid, eng);\n\tif (rc) {\n\t\tDP_NOTICE(cdev,\n\t\t\t  \"Failed to set the engine affinity of ppfid %d\\n\",\n\t\t\t  ppfid);\n\t\treturn rc;\n\t}\n\n\tDP_VERBOSE(cdev, (QED_MSG_RDMA | QED_MSG_SP),\n\t\t   \"LLH: Set the engine affinity of non-RoCE packets as %d\\n\",\n\t\t   eng);\n\n\treturn 0;\n}\n\nstatic const struct qed_rdma_ops qed_rdma_ops_pass = {\n\t.common = &qed_common_ops_pass,\n\t.fill_dev_info = &qed_fill_rdma_dev_info,\n\t.rdma_get_rdma_ctx = &qed_rdma_get_rdma_ctx,\n\t.rdma_init = &qed_rdma_init,\n\t.rdma_add_user = &qed_rdma_add_user,\n\t.rdma_remove_user = &qed_rdma_remove_user,\n\t.rdma_stop = &qed_rdma_stop,\n\t.rdma_query_port = &qed_rdma_query_port,\n\t.rdma_query_device = &qed_rdma_query_device,\n\t.rdma_get_start_sb = &qed_rdma_get_sb_start,\n\t.rdma_get_rdma_int = &qed_rdma_get_int,\n\t.rdma_set_rdma_int = &qed_rdma_set_int,\n\t.rdma_get_min_cnq_msix = &qed_rdma_get_min_cnq_msix,\n\t.rdma_cnq_prod_update = &qed_rdma_cnq_prod_update,\n\t.rdma_alloc_pd = &qed_rdma_alloc_pd,\n\t.rdma_dealloc_pd = &qed_rdma_free_pd,\n\t.rdma_alloc_xrcd = &qed_rdma_alloc_xrcd,\n\t.rdma_dealloc_xrcd = &qed_rdma_free_xrcd,\n\t.rdma_create_cq = &qed_rdma_create_cq,\n\t.rdma_destroy_cq = &qed_rdma_destroy_cq,\n\t.rdma_create_qp = &qed_rdma_create_qp,\n\t.rdma_modify_qp = &qed_rdma_modify_qp,\n\t.rdma_query_qp = &qed_rdma_query_qp,\n\t.rdma_destroy_qp = &qed_rdma_destroy_qp,\n\t.rdma_alloc_tid = &qed_rdma_alloc_tid,\n\t.rdma_free_tid = &qed_rdma_free_tid,\n\t.rdma_register_tid = &qed_rdma_register_tid,\n\t.rdma_deregister_tid = &qed_rdma_deregister_tid,\n\t.rdma_create_srq = &qed_rdma_create_srq,\n\t.rdma_modify_srq = &qed_rdma_modify_srq,\n\t.rdma_destroy_srq = &qed_rdma_destroy_srq,\n\t.ll2_acquire_connection = &qed_ll2_acquire_connection,\n\t.ll2_establish_connection = &qed_ll2_establish_connection,\n\t.ll2_terminate_connection = &qed_ll2_terminate_connection,\n\t.ll2_release_connection = &qed_ll2_release_connection,\n\t.ll2_post_rx_buffer = &qed_ll2_post_rx_buffer,\n\t.ll2_prepare_tx_packet = &qed_ll2_prepare_tx_packet,\n\t.ll2_set_fragment_of_tx_packet = &qed_ll2_set_fragment_of_tx_packet,\n\t.ll2_set_mac_filter = &qed_roce_ll2_set_mac_filter,\n\t.ll2_get_stats = &qed_ll2_get_stats,\n\t.iwarp_set_engine_affin = &qed_iwarp_set_engine_affin,\n\t.iwarp_connect = &qed_iwarp_connect,\n\t.iwarp_create_listen = &qed_iwarp_create_listen,\n\t.iwarp_destroy_listen = &qed_iwarp_destroy_listen,\n\t.iwarp_accept = &qed_iwarp_accept,\n\t.iwarp_reject = &qed_iwarp_reject,\n\t.iwarp_send_rtr = &qed_iwarp_send_rtr,\n};\n\nconst struct qed_rdma_ops *qed_get_rdma_ops(void)\n{\n\treturn &qed_rdma_ops_pass;\n}\nEXPORT_SYMBOL(qed_get_rdma_ops);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}