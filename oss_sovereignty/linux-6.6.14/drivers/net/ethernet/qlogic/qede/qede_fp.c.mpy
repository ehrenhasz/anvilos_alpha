{
  "module_name": "qede_fp.c",
  "hash_id": "97f7df8262ae1bfc5327fdd9c34d6dc02b146b4eaf456fc76bc4ce2822118d15",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/qlogic/qede/qede_fp.c",
  "human_readable_source": "\n \n\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/skbuff.h>\n#include <linux/bpf_trace.h>\n#include <net/udp_tunnel.h>\n#include <linux/ip.h>\n#include <net/gro.h>\n#include <net/ipv6.h>\n#include <net/tcp.h>\n#include <linux/if_ether.h>\n#include <linux/if_vlan.h>\n#include <net/ip6_checksum.h>\n#include \"qede_ptp.h\"\n\n#include <linux/qed/qed_if.h>\n#include \"qede.h\"\n \n\nint qede_alloc_rx_buffer(struct qede_rx_queue *rxq, bool allow_lazy)\n{\n\tstruct sw_rx_data *sw_rx_data;\n\tstruct eth_rx_bd *rx_bd;\n\tdma_addr_t mapping;\n\tstruct page *data;\n\n\t \n\tif (allow_lazy && likely(rxq->filled_buffers > 12)) {\n\t\trxq->filled_buffers--;\n\t\treturn 0;\n\t}\n\n\tdata = alloc_pages(GFP_ATOMIC, 0);\n\tif (unlikely(!data))\n\t\treturn -ENOMEM;\n\n\t \n\tmapping = dma_map_page(rxq->dev, data, 0,\n\t\t\t       PAGE_SIZE, rxq->data_direction);\n\tif (unlikely(dma_mapping_error(rxq->dev, mapping))) {\n\t\t__free_page(data);\n\t\treturn -ENOMEM;\n\t}\n\n\tsw_rx_data = &rxq->sw_rx_ring[rxq->sw_rx_prod & NUM_RX_BDS_MAX];\n\tsw_rx_data->page_offset = 0;\n\tsw_rx_data->data = data;\n\tsw_rx_data->mapping = mapping;\n\n\t \n\trx_bd = (struct eth_rx_bd *)qed_chain_produce(&rxq->rx_bd_ring);\n\tWARN_ON(!rx_bd);\n\trx_bd->addr.hi = cpu_to_le32(upper_32_bits(mapping));\n\trx_bd->addr.lo = cpu_to_le32(lower_32_bits(mapping) +\n\t\t\t\t     rxq->rx_headroom);\n\n\trxq->sw_rx_prod++;\n\trxq->filled_buffers++;\n\n\treturn 0;\n}\n\n \nint qede_free_tx_pkt(struct qede_dev *edev, struct qede_tx_queue *txq, int *len)\n{\n\tu16 idx = txq->sw_tx_cons;\n\tstruct sk_buff *skb = txq->sw_tx_ring.skbs[idx].skb;\n\tstruct eth_tx_1st_bd *first_bd;\n\tstruct eth_tx_bd *tx_data_bd;\n\tint bds_consumed = 0;\n\tint nbds;\n\tbool data_split = txq->sw_tx_ring.skbs[idx].flags & QEDE_TSO_SPLIT_BD;\n\tint i, split_bd_len = 0;\n\n\tif (unlikely(!skb)) {\n\t\tDP_ERR(edev,\n\t\t       \"skb is null for txq idx=%d txq->sw_tx_cons=%d txq->sw_tx_prod=%d\\n\",\n\t\t       idx, txq->sw_tx_cons, txq->sw_tx_prod);\n\t\treturn -1;\n\t}\n\n\t*len = skb->len;\n\n\tfirst_bd = (struct eth_tx_1st_bd *)qed_chain_consume(&txq->tx_pbl);\n\n\tbds_consumed++;\n\n\tnbds = first_bd->data.nbds;\n\n\tif (data_split) {\n\t\tstruct eth_tx_bd *split = (struct eth_tx_bd *)\n\t\t\tqed_chain_consume(&txq->tx_pbl);\n\t\tsplit_bd_len = BD_UNMAP_LEN(split);\n\t\tbds_consumed++;\n\t}\n\tdma_unmap_single(&edev->pdev->dev, BD_UNMAP_ADDR(first_bd),\n\t\t\t BD_UNMAP_LEN(first_bd) + split_bd_len, DMA_TO_DEVICE);\n\n\t \n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++, bds_consumed++) {\n\t\ttx_data_bd = (struct eth_tx_bd *)\n\t\t\tqed_chain_consume(&txq->tx_pbl);\n\t\tdma_unmap_page(&edev->pdev->dev, BD_UNMAP_ADDR(tx_data_bd),\n\t\t\t       BD_UNMAP_LEN(tx_data_bd), DMA_TO_DEVICE);\n\t}\n\n\twhile (bds_consumed++ < nbds)\n\t\tqed_chain_consume(&txq->tx_pbl);\n\n\t \n\tdev_kfree_skb_any(skb);\n\ttxq->sw_tx_ring.skbs[idx].skb = NULL;\n\ttxq->sw_tx_ring.skbs[idx].flags = 0;\n\n\treturn 0;\n}\n\n \nstatic void qede_free_failed_tx_pkt(struct qede_tx_queue *txq,\n\t\t\t\t    struct eth_tx_1st_bd *first_bd,\n\t\t\t\t    int nbd, bool data_split)\n{\n\tu16 idx = txq->sw_tx_prod;\n\tstruct sk_buff *skb = txq->sw_tx_ring.skbs[idx].skb;\n\tstruct eth_tx_bd *tx_data_bd;\n\tint i, split_bd_len = 0;\n\n\t \n\tqed_chain_set_prod(&txq->tx_pbl,\n\t\t\t   le16_to_cpu(txq->tx_db.data.bd_prod), first_bd);\n\n\tfirst_bd = (struct eth_tx_1st_bd *)qed_chain_produce(&txq->tx_pbl);\n\n\tif (data_split) {\n\t\tstruct eth_tx_bd *split = (struct eth_tx_bd *)\n\t\t\t\t\t  qed_chain_produce(&txq->tx_pbl);\n\t\tsplit_bd_len = BD_UNMAP_LEN(split);\n\t\tnbd--;\n\t}\n\n\tdma_unmap_single(txq->dev, BD_UNMAP_ADDR(first_bd),\n\t\t\t BD_UNMAP_LEN(first_bd) + split_bd_len, DMA_TO_DEVICE);\n\n\t \n\tfor (i = 0; i < nbd; i++) {\n\t\ttx_data_bd = (struct eth_tx_bd *)\n\t\t\tqed_chain_produce(&txq->tx_pbl);\n\t\tif (tx_data_bd->nbytes)\n\t\t\tdma_unmap_page(txq->dev,\n\t\t\t\t       BD_UNMAP_ADDR(tx_data_bd),\n\t\t\t\t       BD_UNMAP_LEN(tx_data_bd), DMA_TO_DEVICE);\n\t}\n\n\t \n\tqed_chain_set_prod(&txq->tx_pbl,\n\t\t\t   le16_to_cpu(txq->tx_db.data.bd_prod), first_bd);\n\n\t \n\tdev_kfree_skb_any(skb);\n\ttxq->sw_tx_ring.skbs[idx].skb = NULL;\n\ttxq->sw_tx_ring.skbs[idx].flags = 0;\n}\n\nstatic u32 qede_xmit_type(struct sk_buff *skb, int *ipv6_ext)\n{\n\tu32 rc = XMIT_L4_CSUM;\n\t__be16 l3_proto;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn XMIT_PLAIN;\n\n\tl3_proto = vlan_get_protocol(skb);\n\tif (l3_proto == htons(ETH_P_IPV6) &&\n\t    (ipv6_hdr(skb)->nexthdr == NEXTHDR_IPV6))\n\t\t*ipv6_ext = 1;\n\n\tif (skb->encapsulation) {\n\t\trc |= XMIT_ENC;\n\t\tif (skb_is_gso(skb)) {\n\t\t\tunsigned short gso_type = skb_shinfo(skb)->gso_type;\n\n\t\t\tif ((gso_type & SKB_GSO_UDP_TUNNEL_CSUM) ||\n\t\t\t    (gso_type & SKB_GSO_GRE_CSUM))\n\t\t\t\trc |= XMIT_ENC_GSO_L4_CSUM;\n\n\t\t\trc |= XMIT_LSO;\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\tif (skb_is_gso(skb))\n\t\trc |= XMIT_LSO;\n\n\treturn rc;\n}\n\nstatic void qede_set_params_for_ipv6_ext(struct sk_buff *skb,\n\t\t\t\t\t struct eth_tx_2nd_bd *second_bd,\n\t\t\t\t\t struct eth_tx_3rd_bd *third_bd)\n{\n\tu8 l4_proto;\n\tu16 bd2_bits1 = 0, bd2_bits2 = 0;\n\n\tbd2_bits1 |= (1 << ETH_TX_DATA_2ND_BD_IPV6_EXT_SHIFT);\n\n\tbd2_bits2 |= ((((u8 *)skb_transport_header(skb) - skb->data) >> 1) &\n\t\t     ETH_TX_DATA_2ND_BD_L4_HDR_START_OFFSET_W_MASK)\n\t\t    << ETH_TX_DATA_2ND_BD_L4_HDR_START_OFFSET_W_SHIFT;\n\n\tbd2_bits1 |= (ETH_L4_PSEUDO_CSUM_CORRECT_LENGTH <<\n\t\t      ETH_TX_DATA_2ND_BD_L4_PSEUDO_CSUM_MODE_SHIFT);\n\n\tif (vlan_get_protocol(skb) == htons(ETH_P_IPV6))\n\t\tl4_proto = ipv6_hdr(skb)->nexthdr;\n\telse\n\t\tl4_proto = ip_hdr(skb)->protocol;\n\n\tif (l4_proto == IPPROTO_UDP)\n\t\tbd2_bits1 |= 1 << ETH_TX_DATA_2ND_BD_L4_UDP_SHIFT;\n\n\tif (third_bd)\n\t\tthird_bd->data.bitfields |=\n\t\t\tcpu_to_le16(((tcp_hdrlen(skb) / 4) &\n\t\t\t\tETH_TX_DATA_3RD_BD_TCP_HDR_LEN_DW_MASK) <<\n\t\t\t\tETH_TX_DATA_3RD_BD_TCP_HDR_LEN_DW_SHIFT);\n\n\tsecond_bd->data.bitfields1 = cpu_to_le16(bd2_bits1);\n\tsecond_bd->data.bitfields2 = cpu_to_le16(bd2_bits2);\n}\n\nstatic int map_frag_to_bd(struct qede_tx_queue *txq,\n\t\t\t  skb_frag_t *frag, struct eth_tx_bd *bd)\n{\n\tdma_addr_t mapping;\n\n\t \n\tmapping = skb_frag_dma_map(txq->dev, frag, 0,\n\t\t\t\t   skb_frag_size(frag), DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(txq->dev, mapping)))\n\t\treturn -ENOMEM;\n\n\t \n\tBD_SET_UNMAP_ADDR_LEN(bd, mapping, skb_frag_size(frag));\n\n\treturn 0;\n}\n\nstatic u16 qede_get_skb_hlen(struct sk_buff *skb, bool is_encap_pkt)\n{\n\tif (is_encap_pkt)\n\t\treturn skb_inner_tcp_all_headers(skb);\n\n\treturn skb_tcp_all_headers(skb);\n}\n\n \n#if ((MAX_SKB_FRAGS + 2) > ETH_TX_MAX_BDS_PER_NON_LSO_PACKET)\nstatic bool qede_pkt_req_lin(struct sk_buff *skb, u8 xmit_type)\n{\n\tint allowed_frags = ETH_TX_MAX_BDS_PER_NON_LSO_PACKET - 1;\n\n\tif (xmit_type & XMIT_LSO) {\n\t\tint hlen;\n\n\t\thlen = qede_get_skb_hlen(skb, xmit_type & XMIT_ENC);\n\n\t\t \n\t\tif (skb_headlen(skb) > hlen)\n\t\t\tallowed_frags--;\n\t}\n\n\treturn (skb_shinfo(skb)->nr_frags > allowed_frags);\n}\n#endif\n\nstatic inline void qede_update_tx_producer(struct qede_tx_queue *txq)\n{\n\t \n\twmb();\n\tbarrier();\n\twritel(txq->tx_db.raw, txq->doorbell_addr);\n\n\t \n\twmb();\n}\n\nstatic int qede_xdp_xmit(struct qede_tx_queue *txq, dma_addr_t dma, u16 pad,\n\t\t\t u16 len, struct page *page, struct xdp_frame *xdpf)\n{\n\tstruct eth_tx_1st_bd *bd;\n\tstruct sw_tx_xdp *xdp;\n\tu16 val;\n\n\tif (unlikely(qed_chain_get_elem_used(&txq->tx_pbl) >=\n\t\t     txq->num_tx_buffers)) {\n\t\ttxq->stopped_cnt++;\n\t\treturn -ENOMEM;\n\t}\n\n\tbd = qed_chain_produce(&txq->tx_pbl);\n\tbd->data.nbds = 1;\n\tbd->data.bd_flags.bitfields = BIT(ETH_TX_1ST_BD_FLAGS_START_BD_SHIFT);\n\n\tval = (len & ETH_TX_DATA_1ST_BD_PKT_LEN_MASK) <<\n\t       ETH_TX_DATA_1ST_BD_PKT_LEN_SHIFT;\n\n\tbd->data.bitfields = cpu_to_le16(val);\n\n\t \n\tBD_SET_UNMAP_ADDR_LEN(bd, dma + pad, len);\n\n\txdp = txq->sw_tx_ring.xdp + txq->sw_tx_prod;\n\txdp->mapping = dma;\n\txdp->page = page;\n\txdp->xdpf = xdpf;\n\n\ttxq->sw_tx_prod = (txq->sw_tx_prod + 1) % txq->num_tx_buffers;\n\n\treturn 0;\n}\n\nint qede_xdp_transmit(struct net_device *dev, int n_frames,\n\t\t      struct xdp_frame **frames, u32 flags)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tstruct device *dmadev = &edev->pdev->dev;\n\tstruct qede_tx_queue *xdp_tx;\n\tstruct xdp_frame *xdpf;\n\tdma_addr_t mapping;\n\tint i, nxmit = 0;\n\tu16 xdp_prod;\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!netif_running(dev)))\n\t\treturn -ENETDOWN;\n\n\ti = smp_processor_id() % edev->total_xdp_queues;\n\txdp_tx = edev->fp_array[i].xdp_tx;\n\n\tspin_lock(&xdp_tx->xdp_tx_lock);\n\n\tfor (i = 0; i < n_frames; i++) {\n\t\txdpf = frames[i];\n\n\t\tmapping = dma_map_single(dmadev, xdpf->data, xdpf->len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dmadev, mapping)))\n\t\t\tbreak;\n\n\t\tif (unlikely(qede_xdp_xmit(xdp_tx, mapping, 0, xdpf->len,\n\t\t\t\t\t   NULL, xdpf)))\n\t\t\tbreak;\n\t\tnxmit++;\n\t}\n\n\tif (flags & XDP_XMIT_FLUSH) {\n\t\txdp_prod = qed_chain_get_prod_idx(&xdp_tx->tx_pbl);\n\n\t\txdp_tx->tx_db.data.bd_prod = cpu_to_le16(xdp_prod);\n\t\tqede_update_tx_producer(xdp_tx);\n\t}\n\n\tspin_unlock(&xdp_tx->xdp_tx_lock);\n\n\treturn nxmit;\n}\n\nint qede_txq_has_work(struct qede_tx_queue *txq)\n{\n\tu16 hw_bd_cons;\n\n\t \n\tbarrier();\n\thw_bd_cons = le16_to_cpu(*txq->hw_cons_ptr);\n\tif (qed_chain_get_cons_idx(&txq->tx_pbl) == hw_bd_cons + 1)\n\t\treturn 0;\n\n\treturn hw_bd_cons != qed_chain_get_cons_idx(&txq->tx_pbl);\n}\n\nstatic void qede_xdp_tx_int(struct qede_dev *edev, struct qede_tx_queue *txq)\n{\n\tstruct sw_tx_xdp *xdp_info, *xdp_arr = txq->sw_tx_ring.xdp;\n\tstruct device *dev = &edev->pdev->dev;\n\tstruct xdp_frame *xdpf;\n\tu16 hw_bd_cons;\n\n\thw_bd_cons = le16_to_cpu(*txq->hw_cons_ptr);\n\tbarrier();\n\n\twhile (hw_bd_cons != qed_chain_get_cons_idx(&txq->tx_pbl)) {\n\t\txdp_info = xdp_arr + txq->sw_tx_cons;\n\t\txdpf = xdp_info->xdpf;\n\n\t\tif (xdpf) {\n\t\t\tdma_unmap_single(dev, xdp_info->mapping, xdpf->len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\txdp_return_frame(xdpf);\n\n\t\t\txdp_info->xdpf = NULL;\n\t\t} else {\n\t\t\tdma_unmap_page(dev, xdp_info->mapping, PAGE_SIZE,\n\t\t\t\t       DMA_BIDIRECTIONAL);\n\t\t\t__free_page(xdp_info->page);\n\t\t}\n\n\t\tqed_chain_consume(&txq->tx_pbl);\n\t\ttxq->sw_tx_cons = (txq->sw_tx_cons + 1) % txq->num_tx_buffers;\n\t\ttxq->xmit_pkts++;\n\t}\n}\n\nstatic int qede_tx_int(struct qede_dev *edev, struct qede_tx_queue *txq)\n{\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\tstruct netdev_queue *netdev_txq;\n\tu16 hw_bd_cons;\n\tint rc;\n\n\tnetdev_txq = netdev_get_tx_queue(edev->ndev, txq->ndev_txq_id);\n\n\thw_bd_cons = le16_to_cpu(*txq->hw_cons_ptr);\n\tbarrier();\n\n\twhile (hw_bd_cons != qed_chain_get_cons_idx(&txq->tx_pbl)) {\n\t\tint len = 0;\n\n\t\trc = qede_free_tx_pkt(edev, txq, &len);\n\t\tif (rc) {\n\t\t\tDP_NOTICE(edev, \"hw_bd_cons = %d, chain_cons=%d\\n\",\n\t\t\t\t  hw_bd_cons,\n\t\t\t\t  qed_chain_get_cons_idx(&txq->tx_pbl));\n\t\t\tbreak;\n\t\t}\n\n\t\tbytes_compl += len;\n\t\tpkts_compl++;\n\t\ttxq->sw_tx_cons = (txq->sw_tx_cons + 1) % txq->num_tx_buffers;\n\t\ttxq->xmit_pkts++;\n\t}\n\n\tnetdev_tx_completed_queue(netdev_txq, pkts_compl, bytes_compl);\n\n\t \n\tsmp_mb();\n\n\tif (unlikely(netif_tx_queue_stopped(netdev_txq))) {\n\t\t \n\n\t\t__netif_tx_lock(netdev_txq, smp_processor_id());\n\n\t\tif ((netif_tx_queue_stopped(netdev_txq)) &&\n\t\t    (edev->state == QEDE_STATE_OPEN) &&\n\t\t    (qed_chain_get_elem_left(&txq->tx_pbl)\n\t\t      >= (MAX_SKB_FRAGS + 1))) {\n\t\t\tnetif_tx_wake_queue(netdev_txq);\n\t\t\tDP_VERBOSE(edev, NETIF_MSG_TX_DONE,\n\t\t\t\t   \"Wake queue was called\\n\");\n\t\t}\n\n\t\t__netif_tx_unlock(netdev_txq);\n\t}\n\n\treturn 0;\n}\n\nbool qede_has_rx_work(struct qede_rx_queue *rxq)\n{\n\tu16 hw_comp_cons, sw_comp_cons;\n\n\t \n\tbarrier();\n\n\thw_comp_cons = le16_to_cpu(*rxq->hw_cons_ptr);\n\tsw_comp_cons = qed_chain_get_cons_idx(&rxq->rx_comp_ring);\n\n\treturn hw_comp_cons != sw_comp_cons;\n}\n\nstatic inline void qede_rx_bd_ring_consume(struct qede_rx_queue *rxq)\n{\n\tqed_chain_consume(&rxq->rx_bd_ring);\n\trxq->sw_rx_cons++;\n}\n\n \nstatic inline void qede_reuse_page(struct qede_rx_queue *rxq,\n\t\t\t\t   struct sw_rx_data *curr_cons)\n{\n\tstruct eth_rx_bd *rx_bd_prod = qed_chain_produce(&rxq->rx_bd_ring);\n\tstruct sw_rx_data *curr_prod;\n\tdma_addr_t new_mapping;\n\n\tcurr_prod = &rxq->sw_rx_ring[rxq->sw_rx_prod & NUM_RX_BDS_MAX];\n\t*curr_prod = *curr_cons;\n\n\tnew_mapping = curr_prod->mapping + curr_prod->page_offset;\n\n\trx_bd_prod->addr.hi = cpu_to_le32(upper_32_bits(new_mapping));\n\trx_bd_prod->addr.lo = cpu_to_le32(lower_32_bits(new_mapping) +\n\t\t\t\t\t  rxq->rx_headroom);\n\n\trxq->sw_rx_prod++;\n\tcurr_cons->data = NULL;\n}\n\n \nvoid qede_recycle_rx_bd_ring(struct qede_rx_queue *rxq, u8 count)\n{\n\tstruct sw_rx_data *curr_cons;\n\n\tfor (; count > 0; count--) {\n\t\tcurr_cons = &rxq->sw_rx_ring[rxq->sw_rx_cons & NUM_RX_BDS_MAX];\n\t\tqede_reuse_page(rxq, curr_cons);\n\t\tqede_rx_bd_ring_consume(rxq);\n\t}\n}\n\nstatic inline int qede_realloc_rx_buffer(struct qede_rx_queue *rxq,\n\t\t\t\t\t struct sw_rx_data *curr_cons)\n{\n\t \n\tcurr_cons->page_offset += rxq->rx_buf_seg_size;\n\n\tif (curr_cons->page_offset == PAGE_SIZE) {\n\t\tif (unlikely(qede_alloc_rx_buffer(rxq, true))) {\n\t\t\t \n\t\t\tcurr_cons->page_offset -= rxq->rx_buf_seg_size;\n\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdma_unmap_page(rxq->dev, curr_cons->mapping,\n\t\t\t       PAGE_SIZE, rxq->data_direction);\n\t} else {\n\t\t \n\t\tpage_ref_inc(curr_cons->data);\n\t\tqede_reuse_page(rxq, curr_cons);\n\t}\n\n\treturn 0;\n}\n\nvoid qede_update_rx_prod(struct qede_dev *edev, struct qede_rx_queue *rxq)\n{\n\tu16 bd_prod = qed_chain_get_prod_idx(&rxq->rx_bd_ring);\n\tu16 cqe_prod = qed_chain_get_prod_idx(&rxq->rx_comp_ring);\n\tstruct eth_rx_prod_data rx_prods = {0};\n\n\t \n\trx_prods.bd_prod = cpu_to_le16(bd_prod);\n\trx_prods.cqe_prod = cpu_to_le16(cqe_prod);\n\n\t \n\twmb();\n\n\tinternal_ram_wr(rxq->hw_rxq_prod_addr, sizeof(rx_prods),\n\t\t\t(u32 *)&rx_prods);\n}\n\nstatic void qede_get_rxhash(struct sk_buff *skb, u8 bitfields, __le32 rss_hash)\n{\n\tenum pkt_hash_types hash_type = PKT_HASH_TYPE_NONE;\n\tenum rss_hash_type htype;\n\tu32 hash = 0;\n\n\thtype = GET_FIELD(bitfields, ETH_FAST_PATH_RX_REG_CQE_RSS_HASH_TYPE);\n\tif (htype) {\n\t\thash_type = ((htype == RSS_HASH_TYPE_IPV4) ||\n\t\t\t     (htype == RSS_HASH_TYPE_IPV6)) ?\n\t\t\t    PKT_HASH_TYPE_L3 : PKT_HASH_TYPE_L4;\n\t\thash = le32_to_cpu(rss_hash);\n\t}\n\tskb_set_hash(skb, hash, hash_type);\n}\n\nstatic void qede_set_skb_csum(struct sk_buff *skb, u8 csum_flag)\n{\n\tskb_checksum_none_assert(skb);\n\n\tif (csum_flag & QEDE_CSUM_UNNECESSARY)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tif (csum_flag & QEDE_TUNN_CSUM_UNNECESSARY) {\n\t\tskb->csum_level = 1;\n\t\tskb->encapsulation = 1;\n\t}\n}\n\nstatic inline void qede_skb_receive(struct qede_dev *edev,\n\t\t\t\t    struct qede_fastpath *fp,\n\t\t\t\t    struct qede_rx_queue *rxq,\n\t\t\t\t    struct sk_buff *skb, u16 vlan_tag)\n{\n\tif (vlan_tag)\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tag);\n\n\tnapi_gro_receive(&fp->napi, skb);\n}\n\nstatic void qede_set_gro_params(struct qede_dev *edev,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct eth_fast_path_rx_tpa_start_cqe *cqe)\n{\n\tu16 parsing_flags = le16_to_cpu(cqe->pars_flags.flags);\n\n\tif (((parsing_flags >> PARSING_AND_ERR_FLAGS_L3TYPE_SHIFT) &\n\t    PARSING_AND_ERR_FLAGS_L3TYPE_MASK) == 2)\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\n\telse\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\n\n\tskb_shinfo(skb)->gso_size = __le16_to_cpu(cqe->len_on_first_bd) -\n\t\t\t\t    cqe->header_len;\n}\n\nstatic int qede_fill_frag_skb(struct qede_dev *edev,\n\t\t\t      struct qede_rx_queue *rxq,\n\t\t\t      u8 tpa_agg_index, u16 len_on_bd)\n{\n\tstruct sw_rx_data *current_bd = &rxq->sw_rx_ring[rxq->sw_rx_cons &\n\t\t\t\t\t\t\t NUM_RX_BDS_MAX];\n\tstruct qede_agg_info *tpa_info = &rxq->tpa_info[tpa_agg_index];\n\tstruct sk_buff *skb = tpa_info->skb;\n\n\tif (unlikely(tpa_info->state != QEDE_AGG_STATE_START))\n\t\tgoto out;\n\n\t \n\tskb_fill_page_desc(skb, tpa_info->frag_id++,\n\t\t\t   current_bd->data,\n\t\t\t   current_bd->page_offset + rxq->rx_headroom,\n\t\t\t   len_on_bd);\n\n\tif (unlikely(qede_realloc_rx_buffer(rxq, current_bd))) {\n\t\t \n\t\tpage_ref_inc(current_bd->data);\n\t\tgoto out;\n\t}\n\n\tqede_rx_bd_ring_consume(rxq);\n\n\tskb->data_len += len_on_bd;\n\tskb->truesize += rxq->rx_buf_seg_size;\n\tskb->len += len_on_bd;\n\n\treturn 0;\n\nout:\n\ttpa_info->state = QEDE_AGG_STATE_ERROR;\n\tqede_recycle_rx_bd_ring(rxq, 1);\n\n\treturn -ENOMEM;\n}\n\nstatic bool qede_tunn_exist(u16 flag)\n{\n\treturn !!(flag & (PARSING_AND_ERR_FLAGS_TUNNELEXIST_MASK <<\n\t\t\t  PARSING_AND_ERR_FLAGS_TUNNELEXIST_SHIFT));\n}\n\nstatic u8 qede_check_tunn_csum(u16 flag)\n{\n\tu16 csum_flag = 0;\n\tu8 tcsum = 0;\n\n\tif (flag & (PARSING_AND_ERR_FLAGS_TUNNELL4CHKSMWASCALCULATED_MASK <<\n\t\t    PARSING_AND_ERR_FLAGS_TUNNELL4CHKSMWASCALCULATED_SHIFT))\n\t\tcsum_flag |= PARSING_AND_ERR_FLAGS_TUNNELL4CHKSMERROR_MASK <<\n\t\t\t     PARSING_AND_ERR_FLAGS_TUNNELL4CHKSMERROR_SHIFT;\n\n\tif (flag & (PARSING_AND_ERR_FLAGS_L4CHKSMWASCALCULATED_MASK <<\n\t\t    PARSING_AND_ERR_FLAGS_L4CHKSMWASCALCULATED_SHIFT)) {\n\t\tcsum_flag |= PARSING_AND_ERR_FLAGS_L4CHKSMERROR_MASK <<\n\t\t\t     PARSING_AND_ERR_FLAGS_L4CHKSMERROR_SHIFT;\n\t\ttcsum = QEDE_TUNN_CSUM_UNNECESSARY;\n\t}\n\n\tcsum_flag |= PARSING_AND_ERR_FLAGS_TUNNELIPHDRERROR_MASK <<\n\t\t     PARSING_AND_ERR_FLAGS_TUNNELIPHDRERROR_SHIFT |\n\t\t     PARSING_AND_ERR_FLAGS_IPHDRERROR_MASK <<\n\t\t     PARSING_AND_ERR_FLAGS_IPHDRERROR_SHIFT;\n\n\tif (csum_flag & flag)\n\t\treturn QEDE_CSUM_ERROR;\n\n\treturn QEDE_CSUM_UNNECESSARY | tcsum;\n}\n\nstatic inline struct sk_buff *\nqede_build_skb(struct qede_rx_queue *rxq,\n\t       struct sw_rx_data *bd, u16 len, u16 pad)\n{\n\tstruct sk_buff *skb;\n\tvoid *buf;\n\n\tbuf = page_address(bd->data) + bd->page_offset;\n\tskb = build_skb(buf, rxq->rx_buf_seg_size);\n\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tskb_reserve(skb, pad);\n\tskb_put(skb, len);\n\n\treturn skb;\n}\n\nstatic struct sk_buff *\nqede_tpa_rx_build_skb(struct qede_dev *edev,\n\t\t      struct qede_rx_queue *rxq,\n\t\t      struct sw_rx_data *bd, u16 len, u16 pad,\n\t\t      bool alloc_skb)\n{\n\tstruct sk_buff *skb;\n\n\tskb = qede_build_skb(rxq, bd, len, pad);\n\tbd->page_offset += rxq->rx_buf_seg_size;\n\n\tif (bd->page_offset == PAGE_SIZE) {\n\t\tif (unlikely(qede_alloc_rx_buffer(rxq, true))) {\n\t\t\tDP_NOTICE(edev,\n\t\t\t\t  \"Failed to allocate RX buffer for tpa start\\n\");\n\t\t\tbd->page_offset -= rxq->rx_buf_seg_size;\n\t\t\tpage_ref_inc(bd->data);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\tpage_ref_inc(bd->data);\n\t\tqede_reuse_page(rxq, bd);\n\t}\n\n\t \n\tqede_rx_bd_ring_consume(rxq);\n\n\treturn skb;\n}\n\nstatic struct sk_buff *\nqede_rx_build_skb(struct qede_dev *edev,\n\t\t  struct qede_rx_queue *rxq,\n\t\t  struct sw_rx_data *bd, u16 len, u16 pad)\n{\n\tstruct sk_buff *skb = NULL;\n\n\t \n\tif ((len + pad <= edev->rx_copybreak)) {\n\t\tunsigned int offset = bd->page_offset + pad;\n\n\t\tskb = netdev_alloc_skb(edev->ndev, QEDE_RX_HDR_SIZE);\n\t\tif (unlikely(!skb))\n\t\t\treturn NULL;\n\n\t\tskb_reserve(skb, pad);\n\t\tskb_put_data(skb, page_address(bd->data) + offset, len);\n\t\tqede_reuse_page(rxq, bd);\n\t\tgoto out;\n\t}\n\n\tskb = qede_build_skb(rxq, bd, len, pad);\n\n\tif (unlikely(qede_realloc_rx_buffer(rxq, bd))) {\n\t\t \n\t\tpage_ref_inc(bd->data);\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NULL;\n\t}\nout:\n\t \n\tqede_rx_bd_ring_consume(rxq);\n\n\treturn skb;\n}\n\nstatic void qede_tpa_start(struct qede_dev *edev,\n\t\t\t   struct qede_rx_queue *rxq,\n\t\t\t   struct eth_fast_path_rx_tpa_start_cqe *cqe)\n{\n\tstruct qede_agg_info *tpa_info = &rxq->tpa_info[cqe->tpa_agg_index];\n\tstruct sw_rx_data *sw_rx_data_cons;\n\tu16 pad;\n\n\tsw_rx_data_cons = &rxq->sw_rx_ring[rxq->sw_rx_cons & NUM_RX_BDS_MAX];\n\tpad = cqe->placement_offset + rxq->rx_headroom;\n\n\ttpa_info->skb = qede_tpa_rx_build_skb(edev, rxq, sw_rx_data_cons,\n\t\t\t\t\t      le16_to_cpu(cqe->len_on_first_bd),\n\t\t\t\t\t      pad, false);\n\ttpa_info->buffer.page_offset = sw_rx_data_cons->page_offset;\n\ttpa_info->buffer.mapping = sw_rx_data_cons->mapping;\n\n\tif (unlikely(!tpa_info->skb)) {\n\t\tDP_NOTICE(edev, \"Failed to allocate SKB for gro\\n\");\n\n\t\t \n\t\ttpa_info->tpa_start_fail = true;\n\t\tqede_rx_bd_ring_consume(rxq);\n\t\ttpa_info->state = QEDE_AGG_STATE_ERROR;\n\t\tgoto cons_buf;\n\t}\n\n\ttpa_info->frag_id = 0;\n\ttpa_info->state = QEDE_AGG_STATE_START;\n\n\tif ((le16_to_cpu(cqe->pars_flags.flags) >>\n\t     PARSING_AND_ERR_FLAGS_TAG8021QEXIST_SHIFT) &\n\t    PARSING_AND_ERR_FLAGS_TAG8021QEXIST_MASK)\n\t\ttpa_info->vlan_tag = le16_to_cpu(cqe->vlan_tag);\n\telse\n\t\ttpa_info->vlan_tag = 0;\n\n\tqede_get_rxhash(tpa_info->skb, cqe->bitfields, cqe->rss_hash);\n\n\t \n\tqede_set_gro_params(edev, tpa_info->skb, cqe);\n\ncons_buf:  \n\tif (likely(cqe->bw_ext_bd_len_list[0]))\n\t\tqede_fill_frag_skb(edev, rxq, cqe->tpa_agg_index,\n\t\t\t\t   le16_to_cpu(cqe->bw_ext_bd_len_list[0]));\n\n\tif (unlikely(cqe->bw_ext_bd_len_list[1])) {\n\t\tDP_ERR(edev,\n\t\t       \"Unlikely - got a TPA aggregation with more than one bw_ext_bd_len_list entry in the TPA start\\n\");\n\t\ttpa_info->state = QEDE_AGG_STATE_ERROR;\n\t}\n}\n\n#ifdef CONFIG_INET\nstatic void qede_gro_ip_csum(struct sk_buff *skb)\n{\n\tconst struct iphdr *iph = ip_hdr(skb);\n\tstruct tcphdr *th;\n\n\tskb_set_transport_header(skb, sizeof(struct iphdr));\n\tth = tcp_hdr(skb);\n\n\tth->check = ~tcp_v4_check(skb->len - skb_transport_offset(skb),\n\t\t\t\t  iph->saddr, iph->daddr, 0);\n\n\ttcp_gro_complete(skb);\n}\n\nstatic void qede_gro_ipv6_csum(struct sk_buff *skb)\n{\n\tstruct ipv6hdr *iph = ipv6_hdr(skb);\n\tstruct tcphdr *th;\n\n\tskb_set_transport_header(skb, sizeof(struct ipv6hdr));\n\tth = tcp_hdr(skb);\n\n\tth->check = ~tcp_v6_check(skb->len - skb_transport_offset(skb),\n\t\t\t\t  &iph->saddr, &iph->daddr, 0);\n\ttcp_gro_complete(skb);\n}\n#endif\n\nstatic void qede_gro_receive(struct qede_dev *edev,\n\t\t\t     struct qede_fastpath *fp,\n\t\t\t     struct sk_buff *skb,\n\t\t\t     u16 vlan_tag)\n{\n\t \n\tif (unlikely(!skb->data_len)) {\n\t\tskb_shinfo(skb)->gso_type = 0;\n\t\tskb_shinfo(skb)->gso_size = 0;\n\t\tgoto send_skb;\n\t}\n\n#ifdef CONFIG_INET\n\tif (skb_shinfo(skb)->gso_size) {\n\t\tskb_reset_network_header(skb);\n\n\t\tswitch (skb->protocol) {\n\t\tcase htons(ETH_P_IP):\n\t\t\tqede_gro_ip_csum(skb);\n\t\t\tbreak;\n\t\tcase htons(ETH_P_IPV6):\n\t\t\tqede_gro_ipv6_csum(skb);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDP_ERR(edev,\n\t\t\t       \"Error: FW GRO supports only IPv4/IPv6, not 0x%04x\\n\",\n\t\t\t       ntohs(skb->protocol));\n\t\t}\n\t}\n#endif\n\nsend_skb:\n\tskb_record_rx_queue(skb, fp->rxq->rxq_id);\n\tqede_skb_receive(edev, fp, fp->rxq, skb, vlan_tag);\n}\n\nstatic inline void qede_tpa_cont(struct qede_dev *edev,\n\t\t\t\t struct qede_rx_queue *rxq,\n\t\t\t\t struct eth_fast_path_rx_tpa_cont_cqe *cqe)\n{\n\tint i;\n\n\tfor (i = 0; cqe->len_list[i]; i++)\n\t\tqede_fill_frag_skb(edev, rxq, cqe->tpa_agg_index,\n\t\t\t\t   le16_to_cpu(cqe->len_list[i]));\n\n\tif (unlikely(i > 1))\n\t\tDP_ERR(edev,\n\t\t       \"Strange - TPA cont with more than a single len_list entry\\n\");\n}\n\nstatic int qede_tpa_end(struct qede_dev *edev,\n\t\t\tstruct qede_fastpath *fp,\n\t\t\tstruct eth_fast_path_rx_tpa_end_cqe *cqe)\n{\n\tstruct qede_rx_queue *rxq = fp->rxq;\n\tstruct qede_agg_info *tpa_info;\n\tstruct sk_buff *skb;\n\tint i;\n\n\ttpa_info = &rxq->tpa_info[cqe->tpa_agg_index];\n\tskb = tpa_info->skb;\n\n\tif (tpa_info->buffer.page_offset == PAGE_SIZE)\n\t\tdma_unmap_page(rxq->dev, tpa_info->buffer.mapping,\n\t\t\t       PAGE_SIZE, rxq->data_direction);\n\n\tfor (i = 0; cqe->len_list[i]; i++)\n\t\tqede_fill_frag_skb(edev, rxq, cqe->tpa_agg_index,\n\t\t\t\t   le16_to_cpu(cqe->len_list[i]));\n\tif (unlikely(i > 1))\n\t\tDP_ERR(edev,\n\t\t       \"Strange - TPA emd with more than a single len_list entry\\n\");\n\n\tif (unlikely(tpa_info->state != QEDE_AGG_STATE_START))\n\t\tgoto err;\n\n\t \n\tif (unlikely(cqe->num_of_bds != tpa_info->frag_id + 1))\n\t\tDP_ERR(edev,\n\t\t       \"Strange - TPA had %02x BDs, but SKB has only %d frags\\n\",\n\t\t       cqe->num_of_bds, tpa_info->frag_id);\n\tif (unlikely(skb->len != le16_to_cpu(cqe->total_packet_len)))\n\t\tDP_ERR(edev,\n\t\t       \"Strange - total packet len [cqe] is %4x but SKB has len %04x\\n\",\n\t\t       le16_to_cpu(cqe->total_packet_len), skb->len);\n\n\t \n\tskb->protocol = eth_type_trans(skb, edev->ndev);\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t \n\tNAPI_GRO_CB(skb)->count = le16_to_cpu(cqe->num_of_coalesced_segs);\n\n\tqede_gro_receive(edev, fp, skb, tpa_info->vlan_tag);\n\n\ttpa_info->state = QEDE_AGG_STATE_NONE;\n\n\treturn 1;\nerr:\n\ttpa_info->state = QEDE_AGG_STATE_NONE;\n\n\tif (tpa_info->tpa_start_fail) {\n\t\tqede_reuse_page(rxq, &tpa_info->buffer);\n\t\ttpa_info->tpa_start_fail = false;\n\t}\n\n\tdev_kfree_skb_any(tpa_info->skb);\n\ttpa_info->skb = NULL;\n\treturn 0;\n}\n\nstatic u8 qede_check_notunn_csum(u16 flag)\n{\n\tu16 csum_flag = 0;\n\tu8 csum = 0;\n\n\tif (flag & (PARSING_AND_ERR_FLAGS_L4CHKSMWASCALCULATED_MASK <<\n\t\t    PARSING_AND_ERR_FLAGS_L4CHKSMWASCALCULATED_SHIFT)) {\n\t\tcsum_flag |= PARSING_AND_ERR_FLAGS_L4CHKSMERROR_MASK <<\n\t\t\t     PARSING_AND_ERR_FLAGS_L4CHKSMERROR_SHIFT;\n\t\tcsum = QEDE_CSUM_UNNECESSARY;\n\t}\n\n\tcsum_flag |= PARSING_AND_ERR_FLAGS_IPHDRERROR_MASK <<\n\t\t     PARSING_AND_ERR_FLAGS_IPHDRERROR_SHIFT;\n\n\tif (csum_flag & flag)\n\t\treturn QEDE_CSUM_ERROR;\n\n\treturn csum;\n}\n\nstatic u8 qede_check_csum(u16 flag)\n{\n\tif (!qede_tunn_exist(flag))\n\t\treturn qede_check_notunn_csum(flag);\n\telse\n\t\treturn qede_check_tunn_csum(flag);\n}\n\nstatic bool qede_pkt_is_ip_fragmented(struct eth_fast_path_rx_reg_cqe *cqe,\n\t\t\t\t      u16 flag)\n{\n\tu8 tun_pars_flg = cqe->tunnel_pars_flags.flags;\n\n\tif ((tun_pars_flg & (ETH_TUNNEL_PARSING_FLAGS_IPV4_FRAGMENT_MASK <<\n\t\t\t     ETH_TUNNEL_PARSING_FLAGS_IPV4_FRAGMENT_SHIFT)) ||\n\t    (flag & (PARSING_AND_ERR_FLAGS_IPV4FRAG_MASK <<\n\t\t     PARSING_AND_ERR_FLAGS_IPV4FRAG_SHIFT)))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool qede_rx_xdp(struct qede_dev *edev,\n\t\t\tstruct qede_fastpath *fp,\n\t\t\tstruct qede_rx_queue *rxq,\n\t\t\tstruct bpf_prog *prog,\n\t\t\tstruct sw_rx_data *bd,\n\t\t\tstruct eth_fast_path_rx_reg_cqe *cqe,\n\t\t\tu16 *data_offset, u16 *len)\n{\n\tstruct xdp_buff xdp;\n\tenum xdp_action act;\n\n\txdp_init_buff(&xdp, rxq->rx_buf_seg_size, &rxq->xdp_rxq);\n\txdp_prepare_buff(&xdp, page_address(bd->data), *data_offset,\n\t\t\t *len, false);\n\n\tact = bpf_prog_run_xdp(prog, &xdp);\n\n\t \n\t*data_offset = xdp.data - xdp.data_hard_start;\n\t*len = xdp.data_end - xdp.data;\n\n\tif (act == XDP_PASS)\n\t\treturn true;\n\n\t \n\trxq->xdp_no_pass++;\n\n\tswitch (act) {\n\tcase XDP_TX:\n\t\t \n\t\tif (unlikely(qede_alloc_rx_buffer(rxq, true))) {\n\t\t\tqede_recycle_rx_bd_ring(rxq, 1);\n\n\t\t\ttrace_xdp_exception(edev->ndev, prog, act);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (unlikely(qede_xdp_xmit(fp->xdp_tx, bd->mapping,\n\t\t\t\t\t   *data_offset, *len, bd->data,\n\t\t\t\t\t   NULL))) {\n\t\t\tdma_unmap_page(rxq->dev, bd->mapping, PAGE_SIZE,\n\t\t\t\t       rxq->data_direction);\n\t\t\t__free_page(bd->data);\n\n\t\t\ttrace_xdp_exception(edev->ndev, prog, act);\n\t\t} else {\n\t\t\tdma_sync_single_for_device(rxq->dev,\n\t\t\t\t\t\t   bd->mapping + *data_offset,\n\t\t\t\t\t\t   *len, rxq->data_direction);\n\t\t\tfp->xdp_xmit |= QEDE_XDP_TX;\n\t\t}\n\n\t\t \n\t\tqede_rx_bd_ring_consume(rxq);\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\t \n\t\tif (unlikely(qede_alloc_rx_buffer(rxq, true))) {\n\t\t\tqede_recycle_rx_bd_ring(rxq, 1);\n\n\t\t\ttrace_xdp_exception(edev->ndev, prog, act);\n\t\t\tbreak;\n\t\t}\n\n\t\tdma_unmap_page(rxq->dev, bd->mapping, PAGE_SIZE,\n\t\t\t       rxq->data_direction);\n\n\t\tif (unlikely(xdp_do_redirect(edev->ndev, &xdp, prog)))\n\t\t\tDP_NOTICE(edev, \"Failed to redirect the packet\\n\");\n\t\telse\n\t\t\tfp->xdp_xmit |= QEDE_XDP_REDIRECT;\n\n\t\tqede_rx_bd_ring_consume(rxq);\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(edev->ndev, prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(edev->ndev, prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tqede_recycle_rx_bd_ring(rxq, cqe->bd_num);\n\t}\n\n\treturn false;\n}\n\nstatic int qede_rx_build_jumbo(struct qede_dev *edev,\n\t\t\t       struct qede_rx_queue *rxq,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct eth_fast_path_rx_reg_cqe *cqe,\n\t\t\t       u16 first_bd_len)\n{\n\tu16 pkt_len = le16_to_cpu(cqe->pkt_len);\n\tstruct sw_rx_data *bd;\n\tu16 bd_cons_idx;\n\tu8 num_frags;\n\n\tpkt_len -= first_bd_len;\n\n\t \n\tfor (num_frags = cqe->bd_num - 1; num_frags > 0; num_frags--) {\n\t\tu16 cur_size = pkt_len > rxq->rx_buf_size ? rxq->rx_buf_size :\n\t\t    pkt_len;\n\n\t\tif (unlikely(!cur_size)) {\n\t\t\tDP_ERR(edev,\n\t\t\t       \"Still got %d BDs for mapping jumbo, but length became 0\\n\",\n\t\t\t       num_frags);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (unlikely(qede_alloc_rx_buffer(rxq, true)))\n\t\t\tgoto out;\n\n\t\t \n\t\tbd_cons_idx = rxq->sw_rx_cons & NUM_RX_BDS_MAX;\n\t\tbd = &rxq->sw_rx_ring[bd_cons_idx];\n\t\tqede_rx_bd_ring_consume(rxq);\n\n\t\tdma_unmap_page(rxq->dev, bd->mapping,\n\t\t\t       PAGE_SIZE, DMA_FROM_DEVICE);\n\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, bd->data,\n\t\t\t\trxq->rx_headroom, cur_size, PAGE_SIZE);\n\n\t\tpkt_len -= cur_size;\n\t}\n\n\tif (unlikely(pkt_len))\n\t\tDP_ERR(edev,\n\t\t       \"Mapped all BDs of jumbo, but still have %d bytes\\n\",\n\t\t       pkt_len);\n\nout:\n\treturn num_frags;\n}\n\nstatic int qede_rx_process_tpa_cqe(struct qede_dev *edev,\n\t\t\t\t   struct qede_fastpath *fp,\n\t\t\t\t   struct qede_rx_queue *rxq,\n\t\t\t\t   union eth_rx_cqe *cqe,\n\t\t\t\t   enum eth_rx_cqe_type type)\n{\n\tswitch (type) {\n\tcase ETH_RX_CQE_TYPE_TPA_START:\n\t\tqede_tpa_start(edev, rxq, &cqe->fast_path_tpa_start);\n\t\treturn 0;\n\tcase ETH_RX_CQE_TYPE_TPA_CONT:\n\t\tqede_tpa_cont(edev, rxq, &cqe->fast_path_tpa_cont);\n\t\treturn 0;\n\tcase ETH_RX_CQE_TYPE_TPA_END:\n\t\treturn qede_tpa_end(edev, fp, &cqe->fast_path_tpa_end);\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int qede_rx_process_cqe(struct qede_dev *edev,\n\t\t\t       struct qede_fastpath *fp,\n\t\t\t       struct qede_rx_queue *rxq)\n{\n\tstruct bpf_prog *xdp_prog = READ_ONCE(rxq->xdp_prog);\n\tstruct eth_fast_path_rx_reg_cqe *fp_cqe;\n\tu16 len, pad, bd_cons_idx, parse_flag;\n\tenum eth_rx_cqe_type cqe_type;\n\tunion eth_rx_cqe *cqe;\n\tstruct sw_rx_data *bd;\n\tstruct sk_buff *skb;\n\t__le16 flags;\n\tu8 csum_flag;\n\n\t \n\tcqe = (union eth_rx_cqe *)qed_chain_consume(&rxq->rx_comp_ring);\n\tcqe_type = cqe->fast_path_regular.type;\n\n\t \n\tif (unlikely(cqe_type == ETH_RX_CQE_TYPE_SLOW_PATH)) {\n\t\tstruct eth_slow_path_rx_cqe *sp_cqe;\n\n\t\tsp_cqe = (struct eth_slow_path_rx_cqe *)cqe;\n\t\tedev->ops->eth_cqe_completion(edev->cdev, fp->id, sp_cqe);\n\t\treturn 0;\n\t}\n\n\t \n\tif (cqe_type != ETH_RX_CQE_TYPE_REGULAR)\n\t\treturn qede_rx_process_tpa_cqe(edev, fp, rxq, cqe, cqe_type);\n\n\t \n\tbd_cons_idx = rxq->sw_rx_cons & NUM_RX_BDS_MAX;\n\tbd = &rxq->sw_rx_ring[bd_cons_idx];\n\n\tfp_cqe = &cqe->fast_path_regular;\n\tlen = le16_to_cpu(fp_cqe->len_on_first_bd);\n\tpad = fp_cqe->placement_offset + rxq->rx_headroom;\n\n\t \n\tif (xdp_prog)\n\t\tif (!qede_rx_xdp(edev, fp, rxq, xdp_prog, bd, fp_cqe,\n\t\t\t\t &pad, &len))\n\t\t\treturn 0;\n\n\t \n\tflags = cqe->fast_path_regular.pars_flags.flags;\n\tparse_flag = le16_to_cpu(flags);\n\n\tcsum_flag = qede_check_csum(parse_flag);\n\tif (unlikely(csum_flag == QEDE_CSUM_ERROR)) {\n\t\tif (qede_pkt_is_ip_fragmented(fp_cqe, parse_flag))\n\t\t\trxq->rx_ip_frags++;\n\t\telse\n\t\t\trxq->rx_hw_errors++;\n\t}\n\n\t \n\tskb = qede_rx_build_skb(edev, rxq, bd, len, pad);\n\tif (!skb) {\n\t\trxq->rx_alloc_errors++;\n\t\tqede_recycle_rx_bd_ring(rxq, fp_cqe->bd_num);\n\t\treturn 0;\n\t}\n\n\t \n\tif (fp_cqe->bd_num > 1) {\n\t\tu16 unmapped_frags = qede_rx_build_jumbo(edev, rxq, skb,\n\t\t\t\t\t\t\t fp_cqe, len);\n\n\t\tif (unlikely(unmapped_frags > 0)) {\n\t\t\tqede_recycle_rx_bd_ring(rxq, unmapped_frags);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tskb->protocol = eth_type_trans(skb, edev->ndev);\n\tqede_get_rxhash(skb, fp_cqe->bitfields, fp_cqe->rss_hash);\n\tqede_set_skb_csum(skb, csum_flag);\n\tskb_record_rx_queue(skb, rxq->rxq_id);\n\tqede_ptp_record_rx_ts(edev, cqe, skb);\n\n\t \n\tqede_skb_receive(edev, fp, rxq, skb, le16_to_cpu(fp_cqe->vlan_tag));\n\n\treturn 1;\n}\n\nstatic int qede_rx_int(struct qede_fastpath *fp, int budget)\n{\n\tstruct qede_rx_queue *rxq = fp->rxq;\n\tstruct qede_dev *edev = fp->edev;\n\tint work_done = 0, rcv_pkts = 0;\n\tu16 hw_comp_cons, sw_comp_cons;\n\n\thw_comp_cons = le16_to_cpu(*rxq->hw_cons_ptr);\n\tsw_comp_cons = qed_chain_get_cons_idx(&rxq->rx_comp_ring);\n\n\t \n\trmb();\n\n\t \n\twhile ((sw_comp_cons != hw_comp_cons) && (work_done < budget)) {\n\t\trcv_pkts += qede_rx_process_cqe(edev, fp, rxq);\n\t\tqed_chain_recycle_consumed(&rxq->rx_comp_ring);\n\t\tsw_comp_cons = qed_chain_get_cons_idx(&rxq->rx_comp_ring);\n\t\twork_done++;\n\t}\n\n\trxq->rcv_pkts += rcv_pkts;\n\n\t \n\twhile (rxq->num_rx_buffers - rxq->filled_buffers)\n\t\tif (qede_alloc_rx_buffer(rxq, false))\n\t\t\tbreak;\n\n\t \n\tqede_update_rx_prod(edev, rxq);\n\n\treturn work_done;\n}\n\nstatic bool qede_poll_is_more_work(struct qede_fastpath *fp)\n{\n\tqed_sb_update_sb_idx(fp->sb_info);\n\n\t \n\trmb();\n\n\tif (likely(fp->type & QEDE_FASTPATH_RX))\n\t\tif (qede_has_rx_work(fp->rxq))\n\t\t\treturn true;\n\n\tif (fp->type & QEDE_FASTPATH_XDP)\n\t\tif (qede_txq_has_work(fp->xdp_tx))\n\t\t\treturn true;\n\n\tif (likely(fp->type & QEDE_FASTPATH_TX)) {\n\t\tint cos;\n\n\t\tfor_each_cos_in_txq(fp->edev, cos) {\n\t\t\tif (qede_txq_has_work(&fp->txq[cos]))\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\n \nint qede_poll(struct napi_struct *napi, int budget)\n{\n\tstruct qede_fastpath *fp = container_of(napi, struct qede_fastpath,\n\t\t\t\t\t\tnapi);\n\tstruct qede_dev *edev = fp->edev;\n\tint rx_work_done = 0;\n\tu16 xdp_prod;\n\n\tfp->xdp_xmit = 0;\n\n\tif (likely(fp->type & QEDE_FASTPATH_TX)) {\n\t\tint cos;\n\n\t\tfor_each_cos_in_txq(fp->edev, cos) {\n\t\t\tif (qede_txq_has_work(&fp->txq[cos]))\n\t\t\t\tqede_tx_int(edev, &fp->txq[cos]);\n\t\t}\n\t}\n\n\tif ((fp->type & QEDE_FASTPATH_XDP) && qede_txq_has_work(fp->xdp_tx))\n\t\tqede_xdp_tx_int(edev, fp->xdp_tx);\n\n\trx_work_done = (likely(fp->type & QEDE_FASTPATH_RX) &&\n\t\t\tqede_has_rx_work(fp->rxq)) ?\n\t\t\tqede_rx_int(fp, budget) : 0;\n\n\tif (fp->xdp_xmit & QEDE_XDP_REDIRECT)\n\t\txdp_do_flush();\n\n\t \n\tif (rx_work_done < budget || !budget) {\n\t\tif (!qede_poll_is_more_work(fp)) {\n\t\t\tnapi_complete_done(napi, rx_work_done);\n\n\t\t\t \n\t\t\tqed_sb_ack(fp->sb_info, IGU_INT_ENABLE, 1);\n\t\t} else {\n\t\t\trx_work_done = budget;\n\t\t}\n\t}\n\n\tif (fp->xdp_xmit & QEDE_XDP_TX) {\n\t\txdp_prod = qed_chain_get_prod_idx(&fp->xdp_tx->tx_pbl);\n\n\t\tfp->xdp_tx->tx_db.data.bd_prod = cpu_to_le16(xdp_prod);\n\t\tqede_update_tx_producer(fp->xdp_tx);\n\t}\n\n\treturn rx_work_done;\n}\n\nirqreturn_t qede_msix_fp_int(int irq, void *fp_cookie)\n{\n\tstruct qede_fastpath *fp = fp_cookie;\n\n\tqed_sb_ack(fp->sb_info, IGU_INT_DISABLE, 0  );\n\n\tnapi_schedule_irqoff(&fp->napi);\n\treturn IRQ_HANDLED;\n}\n\n \nnetdev_tx_t qede_start_xmit(struct sk_buff *skb, struct net_device *ndev)\n{\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\tstruct netdev_queue *netdev_txq;\n\tstruct qede_tx_queue *txq;\n\tstruct eth_tx_1st_bd *first_bd;\n\tstruct eth_tx_2nd_bd *second_bd = NULL;\n\tstruct eth_tx_3rd_bd *third_bd = NULL;\n\tstruct eth_tx_bd *tx_data_bd = NULL;\n\tu16 txq_index, val = 0;\n\tu8 nbd = 0;\n\tdma_addr_t mapping;\n\tint rc, frag_idx = 0, ipv6_ext = 0;\n\tu8 xmit_type;\n\tu16 idx;\n\tu16 hlen;\n\tbool data_split = false;\n\n\t \n\ttxq_index = skb_get_queue_mapping(skb);\n\tWARN_ON(txq_index >= QEDE_TSS_COUNT(edev) * edev->dev_info.num_tc);\n\ttxq = QEDE_NDEV_TXQ_ID_TO_TXQ(edev, txq_index);\n\tnetdev_txq = netdev_get_tx_queue(ndev, txq_index);\n\n\tWARN_ON(qed_chain_get_elem_left(&txq->tx_pbl) < (MAX_SKB_FRAGS + 1));\n\n\txmit_type = qede_xmit_type(skb, &ipv6_ext);\n\n#if ((MAX_SKB_FRAGS + 2) > ETH_TX_MAX_BDS_PER_NON_LSO_PACKET)\n\tif (qede_pkt_req_lin(skb, xmit_type)) {\n\t\tif (skb_linearize(skb)) {\n\t\t\ttxq->tx_mem_alloc_err++;\n\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t}\n#endif\n\n\t \n\tidx = txq->sw_tx_prod;\n\ttxq->sw_tx_ring.skbs[idx].skb = skb;\n\tfirst_bd = (struct eth_tx_1st_bd *)\n\t\t   qed_chain_produce(&txq->tx_pbl);\n\tmemset(first_bd, 0, sizeof(*first_bd));\n\tfirst_bd->data.bd_flags.bitfields =\n\t\t1 << ETH_TX_1ST_BD_FLAGS_START_BD_SHIFT;\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))\n\t\tqede_ptp_tx_ts(edev, skb);\n\n\t \n\tmapping = dma_map_single(txq->dev, skb->data,\n\t\t\t\t skb_headlen(skb), DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(txq->dev, mapping))) {\n\t\tDP_NOTICE(edev, \"SKB mapping failed\\n\");\n\t\tqede_free_failed_tx_pkt(txq, first_bd, 0, false);\n\t\tqede_update_tx_producer(txq);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tnbd++;\n\tBD_SET_UNMAP_ADDR_LEN(first_bd, mapping, skb_headlen(skb));\n\n\t \n\tif (unlikely((xmit_type & XMIT_LSO) | ipv6_ext)) {\n\t\tsecond_bd = (struct eth_tx_2nd_bd *)\n\t\t\tqed_chain_produce(&txq->tx_pbl);\n\t\tmemset(second_bd, 0, sizeof(*second_bd));\n\n\t\tnbd++;\n\t\tthird_bd = (struct eth_tx_3rd_bd *)\n\t\t\tqed_chain_produce(&txq->tx_pbl);\n\t\tmemset(third_bd, 0, sizeof(*third_bd));\n\n\t\tnbd++;\n\t\t \n\t\ttx_data_bd = (struct eth_tx_bd *)second_bd;\n\t}\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tfirst_bd->data.vlan = cpu_to_le16(skb_vlan_tag_get(skb));\n\t\tfirst_bd->data.bd_flags.bitfields |=\n\t\t\t1 << ETH_TX_1ST_BD_FLAGS_VLAN_INSERTION_SHIFT;\n\t}\n\n\t \n\tif (xmit_type & XMIT_L4_CSUM) {\n\t\t \n\t\tfirst_bd->data.bd_flags.bitfields |=\n\t\t\t1 << ETH_TX_1ST_BD_FLAGS_L4_CSUM_SHIFT;\n\n\t\tif (xmit_type & XMIT_ENC) {\n\t\t\tfirst_bd->data.bd_flags.bitfields |=\n\t\t\t\t1 << ETH_TX_1ST_BD_FLAGS_IP_CSUM_SHIFT;\n\n\t\t\tval |= (1 << ETH_TX_DATA_1ST_BD_TUNN_FLAG_SHIFT);\n\t\t}\n\n\t\t \n\t\tif (unlikely(txq->is_legacy))\n\t\t\tval ^= (1 << ETH_TX_DATA_1ST_BD_TUNN_FLAG_SHIFT);\n\n\t\t \n\t\tif (unlikely(ipv6_ext))\n\t\t\tqede_set_params_for_ipv6_ext(skb, second_bd, third_bd);\n\t}\n\n\tif (xmit_type & XMIT_LSO) {\n\t\tfirst_bd->data.bd_flags.bitfields |=\n\t\t\t(1 << ETH_TX_1ST_BD_FLAGS_LSO_SHIFT);\n\t\tthird_bd->data.lso_mss =\n\t\t\tcpu_to_le16(skb_shinfo(skb)->gso_size);\n\n\t\tif (unlikely(xmit_type & XMIT_ENC)) {\n\t\t\tfirst_bd->data.bd_flags.bitfields |=\n\t\t\t\t1 << ETH_TX_1ST_BD_FLAGS_TUNN_IP_CSUM_SHIFT;\n\n\t\t\tif (xmit_type & XMIT_ENC_GSO_L4_CSUM) {\n\t\t\t\tu8 tmp = ETH_TX_1ST_BD_FLAGS_TUNN_L4_CSUM_SHIFT;\n\n\t\t\t\tfirst_bd->data.bd_flags.bitfields |= 1 << tmp;\n\t\t\t}\n\t\t\thlen = qede_get_skb_hlen(skb, true);\n\t\t} else {\n\t\t\tfirst_bd->data.bd_flags.bitfields |=\n\t\t\t\t1 << ETH_TX_1ST_BD_FLAGS_IP_CSUM_SHIFT;\n\t\t\thlen = qede_get_skb_hlen(skb, false);\n\t\t}\n\n\t\t \n\t\tthird_bd->data.bitfields |=\n\t\t\tcpu_to_le16(1 << ETH_TX_DATA_3RD_BD_HDR_NBD_SHIFT);\n\n\t\t \n\t\tif (unlikely(skb_headlen(skb) > hlen)) {\n\t\t\tDP_VERBOSE(edev, NETIF_MSG_TX_QUEUED,\n\t\t\t\t   \"TSO split header size is %d (%x:%x)\\n\",\n\t\t\t\t   first_bd->nbytes, first_bd->addr.hi,\n\t\t\t\t   first_bd->addr.lo);\n\n\t\t\tmapping = HILO_U64(le32_to_cpu(first_bd->addr.hi),\n\t\t\t\t\t   le32_to_cpu(first_bd->addr.lo)) +\n\t\t\t\t\t   hlen;\n\n\t\t\tBD_SET_UNMAP_ADDR_LEN(tx_data_bd, mapping,\n\t\t\t\t\t      le16_to_cpu(first_bd->nbytes) -\n\t\t\t\t\t      hlen);\n\n\t\t\t \n\t\t\ttxq->sw_tx_ring.skbs[idx].flags |= QEDE_TSO_SPLIT_BD;\n\n\t\t\tfirst_bd->nbytes = cpu_to_le16(hlen);\n\n\t\t\ttx_data_bd = (struct eth_tx_bd *)third_bd;\n\t\t\tdata_split = true;\n\t\t}\n\t} else {\n\t\tif (unlikely(skb->len > ETH_TX_MAX_NON_LSO_PKT_LEN)) {\n\t\t\tDP_ERR(edev, \"Unexpected non LSO skb length = 0x%x\\n\", skb->len);\n\t\t\tqede_free_failed_tx_pkt(txq, first_bd, 0, false);\n\t\t\tqede_update_tx_producer(txq);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\tval |= ((skb->len & ETH_TX_DATA_1ST_BD_PKT_LEN_MASK) <<\n\t\t\t ETH_TX_DATA_1ST_BD_PKT_LEN_SHIFT);\n\t}\n\n\tfirst_bd->data.bitfields = cpu_to_le16(val);\n\n\t \n\t \n\twhile (tx_data_bd && frag_idx < skb_shinfo(skb)->nr_frags) {\n\t\trc = map_frag_to_bd(txq,\n\t\t\t\t    &skb_shinfo(skb)->frags[frag_idx],\n\t\t\t\t    tx_data_bd);\n\t\tif (rc) {\n\t\t\tqede_free_failed_tx_pkt(txq, first_bd, nbd, data_split);\n\t\t\tqede_update_tx_producer(txq);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\tif (tx_data_bd == (struct eth_tx_bd *)second_bd)\n\t\t\ttx_data_bd = (struct eth_tx_bd *)third_bd;\n\t\telse\n\t\t\ttx_data_bd = NULL;\n\n\t\tfrag_idx++;\n\t}\n\n\t \n\tfor (; frag_idx < skb_shinfo(skb)->nr_frags; frag_idx++, nbd++) {\n\t\ttx_data_bd = (struct eth_tx_bd *)\n\t\t\t     qed_chain_produce(&txq->tx_pbl);\n\n\t\tmemset(tx_data_bd, 0, sizeof(*tx_data_bd));\n\n\t\trc = map_frag_to_bd(txq,\n\t\t\t\t    &skb_shinfo(skb)->frags[frag_idx],\n\t\t\t\t    tx_data_bd);\n\t\tif (rc) {\n\t\t\tqede_free_failed_tx_pkt(txq, first_bd, nbd, data_split);\n\t\t\tqede_update_tx_producer(txq);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\t}\n\n\t \n\tfirst_bd->data.nbds = nbd;\n\n\tnetdev_tx_sent_queue(netdev_txq, skb->len);\n\n\tskb_tx_timestamp(skb);\n\n\t \n\ttxq->sw_tx_prod = (txq->sw_tx_prod + 1) % txq->num_tx_buffers;\n\n\t \n\ttxq->tx_db.data.bd_prod =\n\t\tcpu_to_le16(qed_chain_get_prod_idx(&txq->tx_pbl));\n\n\tif (!netdev_xmit_more() || netif_xmit_stopped(netdev_txq))\n\t\tqede_update_tx_producer(txq);\n\n\tif (unlikely(qed_chain_get_elem_left(&txq->tx_pbl)\n\t\t      < (MAX_SKB_FRAGS + 1))) {\n\t\tif (netdev_xmit_more())\n\t\t\tqede_update_tx_producer(txq);\n\n\t\tnetif_tx_stop_queue(netdev_txq);\n\t\ttxq->stopped_cnt++;\n\t\tDP_VERBOSE(edev, NETIF_MSG_TX_QUEUED,\n\t\t\t   \"Stop queue was called\\n\");\n\t\t \n\t\tsmp_mb();\n\n\t\tif ((qed_chain_get_elem_left(&txq->tx_pbl) >=\n\t\t     (MAX_SKB_FRAGS + 1)) &&\n\t\t    (edev->state == QEDE_STATE_OPEN)) {\n\t\t\tnetif_tx_wake_queue(netdev_txq);\n\t\t\tDP_VERBOSE(edev, NETIF_MSG_TX_QUEUED,\n\t\t\t\t   \"Wake queue was called\\n\");\n\t\t}\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\nu16 qede_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t      struct net_device *sb_dev)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tint total_txq;\n\n\ttotal_txq = QEDE_TSS_COUNT(edev) * edev->dev_info.num_tc;\n\n\treturn QEDE_TSS_COUNT(edev) ?\n\t\tnetdev_pick_tx(dev, skb, NULL) % total_txq :  0;\n}\n\n \n#define QEDE_MAX_TUN_HDR_LEN 48\n\nnetdev_features_t qede_features_check(struct sk_buff *skb,\n\t\t\t\t      struct net_device *dev,\n\t\t\t\t      netdev_features_t features)\n{\n\tif (skb->encapsulation) {\n\t\tu8 l4_proto = 0;\n\n\t\tswitch (vlan_get_protocol(skb)) {\n\t\tcase htons(ETH_P_IP):\n\t\t\tl4_proto = ip_hdr(skb)->protocol;\n\t\t\tbreak;\n\t\tcase htons(ETH_P_IPV6):\n\t\t\tl4_proto = ipv6_hdr(skb)->nexthdr;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn features;\n\t\t}\n\n\t\t \n\t\tif (l4_proto == IPPROTO_UDP) {\n\t\t\tstruct qede_dev *edev = netdev_priv(dev);\n\t\t\tu16 hdrlen, vxln_port, gnv_port;\n\n\t\t\thdrlen = QEDE_MAX_TUN_HDR_LEN;\n\t\t\tvxln_port = edev->vxlan_dst_port;\n\t\t\tgnv_port = edev->geneve_dst_port;\n\n\t\t\tif ((skb_inner_mac_header(skb) -\n\t\t\t     skb_transport_header(skb)) > hdrlen ||\n\t\t\t     (ntohs(udp_hdr(skb)->dest) != vxln_port &&\n\t\t\t      ntohs(udp_hdr(skb)->dest) != gnv_port))\n\t\t\t\treturn features & ~(NETIF_F_CSUM_MASK |\n\t\t\t\t\t\t    NETIF_F_GSO_MASK);\n\t\t} else if (l4_proto == IPPROTO_IPIP) {\n\t\t\t \n\t\t\treturn features & ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n\t\t}\n\t}\n\n\treturn features;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}