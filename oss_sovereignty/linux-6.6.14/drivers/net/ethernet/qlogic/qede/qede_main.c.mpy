{
  "module_name": "qede_main.c",
  "hash_id": "e5b3ba387016af22e90a2db7e3a38c4d92e18b92bcf48f3022eaf79f2a4e4e93",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/qlogic/qede/qede_main.c",
  "human_readable_source": "\n \n\n#include <linux/crash_dump.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/device.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/skbuff.h>\n#include <linux/errno.h>\n#include <linux/list.h>\n#include <linux/string.h>\n#include <linux/dma-mapping.h>\n#include <linux/interrupt.h>\n#include <asm/byteorder.h>\n#include <asm/param.h>\n#include <linux/io.h>\n#include <linux/netdev_features.h>\n#include <linux/udp.h>\n#include <linux/tcp.h>\n#include <net/udp_tunnel.h>\n#include <linux/ip.h>\n#include <net/ipv6.h>\n#include <net/tcp.h>\n#include <linux/if_ether.h>\n#include <linux/if_vlan.h>\n#include <linux/pkt_sched.h>\n#include <linux/ethtool.h>\n#include <linux/in.h>\n#include <linux/random.h>\n#include <net/ip6_checksum.h>\n#include <linux/bitops.h>\n#include <linux/vmalloc.h>\n#include \"qede.h\"\n#include \"qede_ptp.h\"\n\nMODULE_DESCRIPTION(\"QLogic FastLinQ 4xxxx Ethernet Driver\");\nMODULE_LICENSE(\"GPL\");\n\nstatic uint debug;\nmodule_param(debug, uint, 0);\nMODULE_PARM_DESC(debug, \" Default debug msglevel\");\n\nstatic const struct qed_eth_ops *qed_ops;\n\n#define CHIP_NUM_57980S_40\t\t0x1634\n#define CHIP_NUM_57980S_10\t\t0x1666\n#define CHIP_NUM_57980S_MF\t\t0x1636\n#define CHIP_NUM_57980S_100\t\t0x1644\n#define CHIP_NUM_57980S_50\t\t0x1654\n#define CHIP_NUM_57980S_25\t\t0x1656\n#define CHIP_NUM_57980S_IOV\t\t0x1664\n#define CHIP_NUM_AH\t\t\t0x8070\n#define CHIP_NUM_AH_IOV\t\t\t0x8090\n\n#ifndef PCI_DEVICE_ID_NX2_57980E\n#define PCI_DEVICE_ID_57980S_40\t\tCHIP_NUM_57980S_40\n#define PCI_DEVICE_ID_57980S_10\t\tCHIP_NUM_57980S_10\n#define PCI_DEVICE_ID_57980S_MF\t\tCHIP_NUM_57980S_MF\n#define PCI_DEVICE_ID_57980S_100\tCHIP_NUM_57980S_100\n#define PCI_DEVICE_ID_57980S_50\t\tCHIP_NUM_57980S_50\n#define PCI_DEVICE_ID_57980S_25\t\tCHIP_NUM_57980S_25\n#define PCI_DEVICE_ID_57980S_IOV\tCHIP_NUM_57980S_IOV\n#define PCI_DEVICE_ID_AH\t\tCHIP_NUM_AH\n#define PCI_DEVICE_ID_AH_IOV\t\tCHIP_NUM_AH_IOV\n\n#endif\n\nenum qede_pci_private {\n\tQEDE_PRIVATE_PF,\n\tQEDE_PRIVATE_VF\n};\n\nstatic const struct pci_device_id qede_pci_tbl[] = {\n\t{PCI_VDEVICE(QLOGIC, PCI_DEVICE_ID_57980S_40), QEDE_PRIVATE_PF},\n\t{PCI_VDEVICE(QLOGIC, PCI_DEVICE_ID_57980S_10), QEDE_PRIVATE_PF},\n\t{PCI_VDEVICE(QLOGIC, PCI_DEVICE_ID_57980S_MF), QEDE_PRIVATE_PF},\n\t{PCI_VDEVICE(QLOGIC, PCI_DEVICE_ID_57980S_100), QEDE_PRIVATE_PF},\n\t{PCI_VDEVICE(QLOGIC, PCI_DEVICE_ID_57980S_50), QEDE_PRIVATE_PF},\n\t{PCI_VDEVICE(QLOGIC, PCI_DEVICE_ID_57980S_25), QEDE_PRIVATE_PF},\n#ifdef CONFIG_QED_SRIOV\n\t{PCI_VDEVICE(QLOGIC, PCI_DEVICE_ID_57980S_IOV), QEDE_PRIVATE_VF},\n#endif\n\t{PCI_VDEVICE(QLOGIC, PCI_DEVICE_ID_AH), QEDE_PRIVATE_PF},\n#ifdef CONFIG_QED_SRIOV\n\t{PCI_VDEVICE(QLOGIC, PCI_DEVICE_ID_AH_IOV), QEDE_PRIVATE_VF},\n#endif\n\t{ 0 }\n};\n\nMODULE_DEVICE_TABLE(pci, qede_pci_tbl);\n\nstatic int qede_probe(struct pci_dev *pdev, const struct pci_device_id *id);\nstatic pci_ers_result_t\nqede_io_error_detected(struct pci_dev *pdev, pci_channel_state_t state);\n\n#define TX_TIMEOUT\t\t(5 * HZ)\n\n \n#define XDP_PI\t11\n\nstatic void qede_remove(struct pci_dev *pdev);\nstatic void qede_shutdown(struct pci_dev *pdev);\nstatic void qede_link_update(void *dev, struct qed_link_output *link);\nstatic void qede_schedule_recovery_handler(void *dev);\nstatic void qede_recovery_handler(struct qede_dev *edev);\nstatic void qede_schedule_hw_err_handler(void *dev,\n\t\t\t\t\t enum qed_hw_err_type err_type);\nstatic void qede_get_eth_tlv_data(void *edev, void *data);\nstatic void qede_get_generic_tlv_data(void *edev,\n\t\t\t\t      struct qed_generic_tlvs *data);\nstatic void qede_generic_hw_err_handler(struct qede_dev *edev);\n#ifdef CONFIG_QED_SRIOV\nstatic int qede_set_vf_vlan(struct net_device *ndev, int vf, u16 vlan, u8 qos,\n\t\t\t    __be16 vlan_proto)\n{\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\n\tif (vlan > 4095) {\n\t\tDP_NOTICE(edev, \"Illegal vlan value %d\\n\", vlan);\n\t\treturn -EINVAL;\n\t}\n\n\tif (vlan_proto != htons(ETH_P_8021Q))\n\t\treturn -EPROTONOSUPPORT;\n\n\tDP_VERBOSE(edev, QED_MSG_IOV, \"Setting Vlan 0x%04x to VF [%d]\\n\",\n\t\t   vlan, vf);\n\n\treturn edev->ops->iov->set_vlan(edev->cdev, vlan, vf);\n}\n\nstatic int qede_set_vf_mac(struct net_device *ndev, int vfidx, u8 *mac)\n{\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\n\tDP_VERBOSE(edev, QED_MSG_IOV, \"Setting MAC %pM to VF [%d]\\n\", mac, vfidx);\n\n\tif (!is_valid_ether_addr(mac)) {\n\t\tDP_VERBOSE(edev, QED_MSG_IOV, \"MAC address isn't valid\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn edev->ops->iov->set_mac(edev->cdev, mac, vfidx);\n}\n\nstatic int qede_sriov_configure(struct pci_dev *pdev, int num_vfs_param)\n{\n\tstruct qede_dev *edev = netdev_priv(pci_get_drvdata(pdev));\n\tstruct qed_dev_info *qed_info = &edev->dev_info.common;\n\tstruct qed_update_vport_params *vport_params;\n\tint rc;\n\n\tvport_params = vzalloc(sizeof(*vport_params));\n\tif (!vport_params)\n\t\treturn -ENOMEM;\n\tDP_VERBOSE(edev, QED_MSG_IOV, \"Requested %d VFs\\n\", num_vfs_param);\n\n\trc = edev->ops->iov->configure(edev->cdev, num_vfs_param);\n\n\t \n\tif ((rc == num_vfs_param) && netif_running(edev->ndev) &&\n\t    !qed_info->b_inter_pf_switch && qed_info->tx_switching) {\n\t\tvport_params->vport_id = 0;\n\t\tvport_params->update_tx_switching_flg = 1;\n\t\tvport_params->tx_switching_flg = num_vfs_param ? 1 : 0;\n\t\tedev->ops->vport_update(edev->cdev, vport_params);\n\t}\n\n\tvfree(vport_params);\n\treturn rc;\n}\n#endif\n\nstatic int __maybe_unused qede_suspend(struct device *dev)\n{\n\tdev_info(dev, \"Device does not support suspend operation\\n\");\n\n\treturn -EOPNOTSUPP;\n}\n\nstatic DEFINE_SIMPLE_DEV_PM_OPS(qede_pm_ops, qede_suspend, NULL);\n\nstatic const struct pci_error_handlers qede_err_handler = {\n\t.error_detected = qede_io_error_detected,\n};\n\nstatic struct pci_driver qede_pci_driver = {\n\t.name = \"qede\",\n\t.id_table = qede_pci_tbl,\n\t.probe = qede_probe,\n\t.remove = qede_remove,\n\t.shutdown = qede_shutdown,\n#ifdef CONFIG_QED_SRIOV\n\t.sriov_configure = qede_sriov_configure,\n#endif\n\t.err_handler = &qede_err_handler,\n\t.driver.pm = &qede_pm_ops,\n};\n\nstatic struct qed_eth_cb_ops qede_ll_ops = {\n\t{\n#ifdef CONFIG_RFS_ACCEL\n\t\t.arfs_filter_op = qede_arfs_filter_op,\n#endif\n\t\t.link_update = qede_link_update,\n\t\t.schedule_recovery_handler = qede_schedule_recovery_handler,\n\t\t.schedule_hw_err_handler = qede_schedule_hw_err_handler,\n\t\t.get_generic_tlv_data = qede_get_generic_tlv_data,\n\t\t.get_protocol_tlv_data = qede_get_eth_tlv_data,\n\t},\n\t.force_mac = qede_force_mac,\n\t.ports_update = qede_udp_ports_update,\n};\n\nstatic int qede_netdev_event(struct notifier_block *this, unsigned long event,\n\t\t\t     void *ptr)\n{\n\tstruct net_device *ndev = netdev_notifier_info_to_dev(ptr);\n\tstruct ethtool_drvinfo drvinfo;\n\tstruct qede_dev *edev;\n\n\tif (event != NETDEV_CHANGENAME && event != NETDEV_CHANGEADDR)\n\t\tgoto done;\n\n\t \n\tif (!ndev || !ndev->ethtool_ops || !ndev->ethtool_ops->get_drvinfo)\n\t\tgoto done;\n\n\tmemset(&drvinfo, 0, sizeof(drvinfo));\n\tndev->ethtool_ops->get_drvinfo(ndev, &drvinfo);\n\tif (strcmp(drvinfo.driver, \"qede\"))\n\t\tgoto done;\n\tedev = netdev_priv(ndev);\n\n\tswitch (event) {\n\tcase NETDEV_CHANGENAME:\n\t\t \n\t\tif (!edev->ops || !edev->ops->common)\n\t\t\tgoto done;\n\t\tedev->ops->common->set_name(edev->cdev, edev->ndev->name);\n\t\tbreak;\n\tcase NETDEV_CHANGEADDR:\n\t\tedev = netdev_priv(ndev);\n\t\tqede_rdma_event_changeaddr(edev);\n\t\tbreak;\n\t}\n\ndone:\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block qede_netdev_notifier = {\n\t.notifier_call = qede_netdev_event,\n};\n\nstatic\nint __init qede_init(void)\n{\n\tint ret;\n\n\tpr_info(\"qede init: QLogic FastLinQ 4xxxx Ethernet Driver qede\\n\");\n\n\tqede_forced_speed_maps_init();\n\n\tqed_ops = qed_get_eth_ops();\n\tif (!qed_ops) {\n\t\tpr_notice(\"Failed to get qed ethtool operations\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tret = register_netdevice_notifier(&qede_netdev_notifier);\n\tif (ret) {\n\t\tpr_notice(\"Failed to register netdevice_notifier\\n\");\n\t\tqed_put_eth_ops();\n\t\treturn -EINVAL;\n\t}\n\n\tret = pci_register_driver(&qede_pci_driver);\n\tif (ret) {\n\t\tpr_notice(\"Failed to register driver\\n\");\n\t\tunregister_netdevice_notifier(&qede_netdev_notifier);\n\t\tqed_put_eth_ops();\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit qede_cleanup(void)\n{\n\tif (debug & QED_LOG_INFO_MASK)\n\t\tpr_info(\"qede_cleanup called\\n\");\n\n\tunregister_netdevice_notifier(&qede_netdev_notifier);\n\tpci_unregister_driver(&qede_pci_driver);\n\tqed_put_eth_ops();\n}\n\nmodule_init(qede_init);\nmodule_exit(qede_cleanup);\n\nstatic int qede_open(struct net_device *ndev);\nstatic int qede_close(struct net_device *ndev);\n\nvoid qede_fill_by_demand_stats(struct qede_dev *edev)\n{\n\tstruct qede_stats_common *p_common = &edev->stats.common;\n\tstruct qed_eth_stats stats;\n\n\tedev->ops->get_vport_stats(edev->cdev, &stats);\n\n\tspin_lock(&edev->stats_lock);\n\n\tp_common->no_buff_discards = stats.common.no_buff_discards;\n\tp_common->packet_too_big_discard = stats.common.packet_too_big_discard;\n\tp_common->ttl0_discard = stats.common.ttl0_discard;\n\tp_common->rx_ucast_bytes = stats.common.rx_ucast_bytes;\n\tp_common->rx_mcast_bytes = stats.common.rx_mcast_bytes;\n\tp_common->rx_bcast_bytes = stats.common.rx_bcast_bytes;\n\tp_common->rx_ucast_pkts = stats.common.rx_ucast_pkts;\n\tp_common->rx_mcast_pkts = stats.common.rx_mcast_pkts;\n\tp_common->rx_bcast_pkts = stats.common.rx_bcast_pkts;\n\tp_common->mftag_filter_discards = stats.common.mftag_filter_discards;\n\tp_common->mac_filter_discards = stats.common.mac_filter_discards;\n\tp_common->gft_filter_drop = stats.common.gft_filter_drop;\n\n\tp_common->tx_ucast_bytes = stats.common.tx_ucast_bytes;\n\tp_common->tx_mcast_bytes = stats.common.tx_mcast_bytes;\n\tp_common->tx_bcast_bytes = stats.common.tx_bcast_bytes;\n\tp_common->tx_ucast_pkts = stats.common.tx_ucast_pkts;\n\tp_common->tx_mcast_pkts = stats.common.tx_mcast_pkts;\n\tp_common->tx_bcast_pkts = stats.common.tx_bcast_pkts;\n\tp_common->tx_err_drop_pkts = stats.common.tx_err_drop_pkts;\n\tp_common->coalesced_pkts = stats.common.tpa_coalesced_pkts;\n\tp_common->coalesced_events = stats.common.tpa_coalesced_events;\n\tp_common->coalesced_aborts_num = stats.common.tpa_aborts_num;\n\tp_common->non_coalesced_pkts = stats.common.tpa_not_coalesced_pkts;\n\tp_common->coalesced_bytes = stats.common.tpa_coalesced_bytes;\n\n\tp_common->rx_64_byte_packets = stats.common.rx_64_byte_packets;\n\tp_common->rx_65_to_127_byte_packets =\n\t    stats.common.rx_65_to_127_byte_packets;\n\tp_common->rx_128_to_255_byte_packets =\n\t    stats.common.rx_128_to_255_byte_packets;\n\tp_common->rx_256_to_511_byte_packets =\n\t    stats.common.rx_256_to_511_byte_packets;\n\tp_common->rx_512_to_1023_byte_packets =\n\t    stats.common.rx_512_to_1023_byte_packets;\n\tp_common->rx_1024_to_1518_byte_packets =\n\t    stats.common.rx_1024_to_1518_byte_packets;\n\tp_common->rx_crc_errors = stats.common.rx_crc_errors;\n\tp_common->rx_mac_crtl_frames = stats.common.rx_mac_crtl_frames;\n\tp_common->rx_pause_frames = stats.common.rx_pause_frames;\n\tp_common->rx_pfc_frames = stats.common.rx_pfc_frames;\n\tp_common->rx_align_errors = stats.common.rx_align_errors;\n\tp_common->rx_carrier_errors = stats.common.rx_carrier_errors;\n\tp_common->rx_oversize_packets = stats.common.rx_oversize_packets;\n\tp_common->rx_jabbers = stats.common.rx_jabbers;\n\tp_common->rx_undersize_packets = stats.common.rx_undersize_packets;\n\tp_common->rx_fragments = stats.common.rx_fragments;\n\tp_common->tx_64_byte_packets = stats.common.tx_64_byte_packets;\n\tp_common->tx_65_to_127_byte_packets =\n\t    stats.common.tx_65_to_127_byte_packets;\n\tp_common->tx_128_to_255_byte_packets =\n\t    stats.common.tx_128_to_255_byte_packets;\n\tp_common->tx_256_to_511_byte_packets =\n\t    stats.common.tx_256_to_511_byte_packets;\n\tp_common->tx_512_to_1023_byte_packets =\n\t    stats.common.tx_512_to_1023_byte_packets;\n\tp_common->tx_1024_to_1518_byte_packets =\n\t    stats.common.tx_1024_to_1518_byte_packets;\n\tp_common->tx_pause_frames = stats.common.tx_pause_frames;\n\tp_common->tx_pfc_frames = stats.common.tx_pfc_frames;\n\tp_common->brb_truncates = stats.common.brb_truncates;\n\tp_common->brb_discards = stats.common.brb_discards;\n\tp_common->tx_mac_ctrl_frames = stats.common.tx_mac_ctrl_frames;\n\tp_common->link_change_count = stats.common.link_change_count;\n\tp_common->ptp_skip_txts = edev->ptp_skip_txts;\n\n\tif (QEDE_IS_BB(edev)) {\n\t\tstruct qede_stats_bb *p_bb = &edev->stats.bb;\n\n\t\tp_bb->rx_1519_to_1522_byte_packets =\n\t\t    stats.bb.rx_1519_to_1522_byte_packets;\n\t\tp_bb->rx_1519_to_2047_byte_packets =\n\t\t    stats.bb.rx_1519_to_2047_byte_packets;\n\t\tp_bb->rx_2048_to_4095_byte_packets =\n\t\t    stats.bb.rx_2048_to_4095_byte_packets;\n\t\tp_bb->rx_4096_to_9216_byte_packets =\n\t\t    stats.bb.rx_4096_to_9216_byte_packets;\n\t\tp_bb->rx_9217_to_16383_byte_packets =\n\t\t    stats.bb.rx_9217_to_16383_byte_packets;\n\t\tp_bb->tx_1519_to_2047_byte_packets =\n\t\t    stats.bb.tx_1519_to_2047_byte_packets;\n\t\tp_bb->tx_2048_to_4095_byte_packets =\n\t\t    stats.bb.tx_2048_to_4095_byte_packets;\n\t\tp_bb->tx_4096_to_9216_byte_packets =\n\t\t    stats.bb.tx_4096_to_9216_byte_packets;\n\t\tp_bb->tx_9217_to_16383_byte_packets =\n\t\t    stats.bb.tx_9217_to_16383_byte_packets;\n\t\tp_bb->tx_lpi_entry_count = stats.bb.tx_lpi_entry_count;\n\t\tp_bb->tx_total_collisions = stats.bb.tx_total_collisions;\n\t} else {\n\t\tstruct qede_stats_ah *p_ah = &edev->stats.ah;\n\n\t\tp_ah->rx_1519_to_max_byte_packets =\n\t\t    stats.ah.rx_1519_to_max_byte_packets;\n\t\tp_ah->tx_1519_to_max_byte_packets =\n\t\t    stats.ah.tx_1519_to_max_byte_packets;\n\t}\n\n\tspin_unlock(&edev->stats_lock);\n}\n\nstatic void qede_get_stats64(struct net_device *dev,\n\t\t\t     struct rtnl_link_stats64 *stats)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tstruct qede_stats_common *p_common;\n\n\tp_common = &edev->stats.common;\n\n\tspin_lock(&edev->stats_lock);\n\n\tstats->rx_packets = p_common->rx_ucast_pkts + p_common->rx_mcast_pkts +\n\t\t\t    p_common->rx_bcast_pkts;\n\tstats->tx_packets = p_common->tx_ucast_pkts + p_common->tx_mcast_pkts +\n\t\t\t    p_common->tx_bcast_pkts;\n\n\tstats->rx_bytes = p_common->rx_ucast_bytes + p_common->rx_mcast_bytes +\n\t\t\t  p_common->rx_bcast_bytes;\n\tstats->tx_bytes = p_common->tx_ucast_bytes + p_common->tx_mcast_bytes +\n\t\t\t  p_common->tx_bcast_bytes;\n\n\tstats->tx_errors = p_common->tx_err_drop_pkts;\n\tstats->multicast = p_common->rx_mcast_pkts + p_common->rx_bcast_pkts;\n\n\tstats->rx_fifo_errors = p_common->no_buff_discards;\n\n\tif (QEDE_IS_BB(edev))\n\t\tstats->collisions = edev->stats.bb.tx_total_collisions;\n\tstats->rx_crc_errors = p_common->rx_crc_errors;\n\tstats->rx_frame_errors = p_common->rx_align_errors;\n\n\tspin_unlock(&edev->stats_lock);\n}\n\n#ifdef CONFIG_QED_SRIOV\nstatic int qede_get_vf_config(struct net_device *dev, int vfidx,\n\t\t\t      struct ifla_vf_info *ivi)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\n\tif (!edev->ops)\n\t\treturn -EINVAL;\n\n\treturn edev->ops->iov->get_config(edev->cdev, vfidx, ivi);\n}\n\nstatic int qede_set_vf_rate(struct net_device *dev, int vfidx,\n\t\t\t    int min_tx_rate, int max_tx_rate)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\n\treturn edev->ops->iov->set_rate(edev->cdev, vfidx, min_tx_rate,\n\t\t\t\t\tmax_tx_rate);\n}\n\nstatic int qede_set_vf_spoofchk(struct net_device *dev, int vfidx, bool val)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\n\tif (!edev->ops)\n\t\treturn -EINVAL;\n\n\treturn edev->ops->iov->set_spoof(edev->cdev, vfidx, val);\n}\n\nstatic int qede_set_vf_link_state(struct net_device *dev, int vfidx,\n\t\t\t\t  int link_state)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\n\tif (!edev->ops)\n\t\treturn -EINVAL;\n\n\treturn edev->ops->iov->set_link_state(edev->cdev, vfidx, link_state);\n}\n\nstatic int qede_set_vf_trust(struct net_device *dev, int vfidx, bool setting)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\n\tif (!edev->ops)\n\t\treturn -EINVAL;\n\n\treturn edev->ops->iov->set_trust(edev->cdev, vfidx, setting);\n}\n#endif\n\nstatic int qede_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\n\tif (!netif_running(dev))\n\t\treturn -EAGAIN;\n\n\tswitch (cmd) {\n\tcase SIOCSHWTSTAMP:\n\t\treturn qede_ptp_hw_ts(edev, ifr);\n\tdefault:\n\t\tDP_VERBOSE(edev, QED_MSG_DEBUG,\n\t\t\t   \"default IOCTL cmd 0x%x\\n\", cmd);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic void qede_fp_sb_dump(struct qede_dev *edev, struct qede_fastpath *fp)\n{\n\tchar *p_sb = (char *)fp->sb_info->sb_virt;\n\tu32 sb_size, i;\n\n\tsb_size = sizeof(struct status_block);\n\n\tfor (i = 0; i < sb_size; i += 8)\n\t\tDP_NOTICE(edev,\n\t\t\t  \"%02hhX %02hhX %02hhX %02hhX  %02hhX %02hhX %02hhX %02hhX\\n\",\n\t\t\t  p_sb[i], p_sb[i + 1], p_sb[i + 2], p_sb[i + 3],\n\t\t\t  p_sb[i + 4], p_sb[i + 5], p_sb[i + 6], p_sb[i + 7]);\n}\n\nstatic void\nqede_txq_fp_log_metadata(struct qede_dev *edev,\n\t\t\t struct qede_fastpath *fp, struct qede_tx_queue *txq)\n{\n\tstruct qed_chain *p_chain = &txq->tx_pbl;\n\n\t \n\tDP_NOTICE(edev,\n\t\t  \"fpid 0x%x sbid 0x%x txqid [0x%x] ndev_qid [0x%x] cos [0x%x] p_chain %p cap %d size %d jiffies %lu HZ 0x%x\\n\",\n\t\t  fp->id, fp->sb_info->igu_sb_id, txq->index, txq->ndev_txq_id, txq->cos,\n\t\t  p_chain, p_chain->capacity, p_chain->size, jiffies, HZ);\n\n\t \n\tDP_NOTICE(edev,\n\t\t  \"hw cons %04x sw_tx_prod=0x%x, sw_tx_cons=0x%x, bd_prod 0x%x bd_cons 0x%x\\n\",\n\t\t  le16_to_cpu(*txq->hw_cons_ptr), txq->sw_tx_prod, txq->sw_tx_cons,\n\t\t  qed_chain_get_prod_idx(p_chain), qed_chain_get_cons_idx(p_chain));\n}\n\nstatic void\nqede_tx_log_print(struct qede_dev *edev, struct qede_fastpath *fp, struct qede_tx_queue *txq)\n{\n\tstruct qed_sb_info_dbg sb_dbg;\n\tint rc;\n\n\t \n\tqede_fp_sb_dump(edev, fp);\n\n\tmemset(&sb_dbg, 0, sizeof(sb_dbg));\n\trc = edev->ops->common->get_sb_info(edev->cdev, fp->sb_info, (u16)fp->id, &sb_dbg);\n\n\tDP_NOTICE(edev, \"IGU: prod %08x cons %08x CAU Tx %04x\\n\",\n\t\t  sb_dbg.igu_prod, sb_dbg.igu_cons, sb_dbg.pi[TX_PI(txq->cos)]);\n\n\t \n\tedev->ops->common->mfw_report(edev->cdev,\n\t\t\t\t      \"Txq[%d]: FW cons [host] %04x, SW cons %04x, SW prod %04x [Jiffies %lu]\\n\",\n\t\t\t\t      txq->index, le16_to_cpu(*txq->hw_cons_ptr),\n\t\t\t\t      qed_chain_get_cons_idx(&txq->tx_pbl),\n\t\t\t\t      qed_chain_get_prod_idx(&txq->tx_pbl), jiffies);\n\tif (!rc)\n\t\tedev->ops->common->mfw_report(edev->cdev,\n\t\t\t\t\t      \"Txq[%d]: SB[0x%04x] - IGU: prod %08x cons %08x CAU Tx %04x\\n\",\n\t\t\t\t\t      txq->index, fp->sb_info->igu_sb_id,\n\t\t\t\t\t      sb_dbg.igu_prod, sb_dbg.igu_cons,\n\t\t\t\t\t      sb_dbg.pi[TX_PI(txq->cos)]);\n}\n\nstatic void qede_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tint i;\n\n\tnetif_carrier_off(dev);\n\tDP_NOTICE(edev, \"TX timeout on queue %u!\\n\", txqueue);\n\n\tfor_each_queue(i) {\n\t\tstruct qede_tx_queue *txq;\n\t\tstruct qede_fastpath *fp;\n\t\tint cos;\n\n\t\tfp = &edev->fp_array[i];\n\t\tif (!(fp->type & QEDE_FASTPATH_TX))\n\t\t\tcontinue;\n\n\t\tfor_each_cos_in_txq(edev, cos) {\n\t\t\ttxq = &fp->txq[cos];\n\n\t\t\t \n\t\t\tqede_txq_fp_log_metadata(edev, fp, txq);\n\n\t\t\tif (qed_chain_get_cons_idx(&txq->tx_pbl) !=\n\t\t\t    qed_chain_get_prod_idx(&txq->tx_pbl))\n\t\t\t\tqede_tx_log_print(edev, fp, txq);\n\t\t}\n\t}\n\n\tif (IS_VF(edev))\n\t\treturn;\n\n\tif (test_and_set_bit(QEDE_ERR_IS_HANDLED, &edev->err_flags) ||\n\t    edev->state == QEDE_STATE_RECOVERY) {\n\t\tDP_INFO(edev,\n\t\t\t\"Avoid handling a Tx timeout while another HW error is being handled\\n\");\n\t\treturn;\n\t}\n\n\tset_bit(QEDE_ERR_GET_DBG_INFO, &edev->err_flags);\n\tset_bit(QEDE_SP_HW_ERR, &edev->sp_flags);\n\tschedule_delayed_work(&edev->sp_task, 0);\n}\n\nstatic int qede_setup_tc(struct net_device *ndev, u8 num_tc)\n{\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\tint cos, count, offset;\n\n\tif (num_tc > edev->dev_info.num_tc)\n\t\treturn -EINVAL;\n\n\tnetdev_reset_tc(ndev);\n\tnetdev_set_num_tc(ndev, num_tc);\n\n\tfor_each_cos_in_txq(edev, cos) {\n\t\tcount = QEDE_TSS_COUNT(edev);\n\t\toffset = cos * QEDE_TSS_COUNT(edev);\n\t\tnetdev_set_tc_queue(ndev, cos, count, offset);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nqede_set_flower(struct qede_dev *edev, struct flow_cls_offload *f,\n\t\t__be16 proto)\n{\n\tswitch (f->command) {\n\tcase FLOW_CLS_REPLACE:\n\t\treturn qede_add_tc_flower_fltr(edev, proto, f);\n\tcase FLOW_CLS_DESTROY:\n\t\treturn qede_delete_flow_filter(edev, f->cookie);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int qede_setup_tc_block_cb(enum tc_setup_type type, void *type_data,\n\t\t\t\t  void *cb_priv)\n{\n\tstruct flow_cls_offload *f;\n\tstruct qede_dev *edev = cb_priv;\n\n\tif (!tc_cls_can_offload_and_chain0(edev->ndev, type_data))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch (type) {\n\tcase TC_SETUP_CLSFLOWER:\n\t\tf = type_data;\n\t\treturn qede_set_flower(edev, f, f->common.protocol);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic LIST_HEAD(qede_block_cb_list);\n\nstatic int\nqede_setup_tc_offload(struct net_device *dev, enum tc_setup_type type,\n\t\t      void *type_data)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tstruct tc_mqprio_qopt *mqprio;\n\n\tswitch (type) {\n\tcase TC_SETUP_BLOCK:\n\t\treturn flow_block_cb_setup_simple(type_data,\n\t\t\t\t\t\t  &qede_block_cb_list,\n\t\t\t\t\t\t  qede_setup_tc_block_cb,\n\t\t\t\t\t\t  edev, edev, true);\n\tcase TC_SETUP_QDISC_MQPRIO:\n\t\tmqprio = type_data;\n\n\t\tmqprio->hw = TC_MQPRIO_HW_OFFLOAD_TCS;\n\t\treturn qede_setup_tc(dev, mqprio->num_tc);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic const struct net_device_ops qede_netdev_ops = {\n\t.ndo_open\t\t= qede_open,\n\t.ndo_stop\t\t= qede_close,\n\t.ndo_start_xmit\t\t= qede_start_xmit,\n\t.ndo_select_queue\t= qede_select_queue,\n\t.ndo_set_rx_mode\t= qede_set_rx_mode,\n\t.ndo_set_mac_address\t= qede_set_mac_addr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_change_mtu\t\t= qede_change_mtu,\n\t.ndo_eth_ioctl\t\t= qede_ioctl,\n\t.ndo_tx_timeout\t\t= qede_tx_timeout,\n#ifdef CONFIG_QED_SRIOV\n\t.ndo_set_vf_mac\t\t= qede_set_vf_mac,\n\t.ndo_set_vf_vlan\t= qede_set_vf_vlan,\n\t.ndo_set_vf_trust\t= qede_set_vf_trust,\n#endif\n\t.ndo_vlan_rx_add_vid\t= qede_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= qede_vlan_rx_kill_vid,\n\t.ndo_fix_features\t= qede_fix_features,\n\t.ndo_set_features\t= qede_set_features,\n\t.ndo_get_stats64\t= qede_get_stats64,\n#ifdef CONFIG_QED_SRIOV\n\t.ndo_set_vf_link_state\t= qede_set_vf_link_state,\n\t.ndo_set_vf_spoofchk\t= qede_set_vf_spoofchk,\n\t.ndo_get_vf_config\t= qede_get_vf_config,\n\t.ndo_set_vf_rate\t= qede_set_vf_rate,\n#endif\n\t.ndo_features_check\t= qede_features_check,\n\t.ndo_bpf\t\t= qede_xdp,\n#ifdef CONFIG_RFS_ACCEL\n\t.ndo_rx_flow_steer\t= qede_rx_flow_steer,\n#endif\n\t.ndo_xdp_xmit\t\t= qede_xdp_transmit,\n\t.ndo_setup_tc\t\t= qede_setup_tc_offload,\n};\n\nstatic const struct net_device_ops qede_netdev_vf_ops = {\n\t.ndo_open\t\t= qede_open,\n\t.ndo_stop\t\t= qede_close,\n\t.ndo_start_xmit\t\t= qede_start_xmit,\n\t.ndo_select_queue\t= qede_select_queue,\n\t.ndo_set_rx_mode\t= qede_set_rx_mode,\n\t.ndo_set_mac_address\t= qede_set_mac_addr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_change_mtu\t\t= qede_change_mtu,\n\t.ndo_vlan_rx_add_vid\t= qede_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= qede_vlan_rx_kill_vid,\n\t.ndo_fix_features\t= qede_fix_features,\n\t.ndo_set_features\t= qede_set_features,\n\t.ndo_get_stats64\t= qede_get_stats64,\n\t.ndo_features_check\t= qede_features_check,\n};\n\nstatic const struct net_device_ops qede_netdev_vf_xdp_ops = {\n\t.ndo_open\t\t= qede_open,\n\t.ndo_stop\t\t= qede_close,\n\t.ndo_start_xmit\t\t= qede_start_xmit,\n\t.ndo_select_queue\t= qede_select_queue,\n\t.ndo_set_rx_mode\t= qede_set_rx_mode,\n\t.ndo_set_mac_address\t= qede_set_mac_addr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_change_mtu\t\t= qede_change_mtu,\n\t.ndo_vlan_rx_add_vid\t= qede_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid\t= qede_vlan_rx_kill_vid,\n\t.ndo_fix_features\t= qede_fix_features,\n\t.ndo_set_features\t= qede_set_features,\n\t.ndo_get_stats64\t= qede_get_stats64,\n\t.ndo_features_check\t= qede_features_check,\n\t.ndo_bpf\t\t= qede_xdp,\n\t.ndo_xdp_xmit\t\t= qede_xdp_transmit,\n};\n\n \n\nstatic struct qede_dev *qede_alloc_etherdev(struct qed_dev *cdev,\n\t\t\t\t\t    struct pci_dev *pdev,\n\t\t\t\t\t    struct qed_dev_eth_info *info,\n\t\t\t\t\t    u32 dp_module, u8 dp_level)\n{\n\tstruct net_device *ndev;\n\tstruct qede_dev *edev;\n\n\tndev = alloc_etherdev_mqs(sizeof(*edev),\n\t\t\t\t  info->num_queues * info->num_tc,\n\t\t\t\t  info->num_queues);\n\tif (!ndev) {\n\t\tpr_err(\"etherdev allocation failed\\n\");\n\t\treturn NULL;\n\t}\n\n\tedev = netdev_priv(ndev);\n\tedev->ndev = ndev;\n\tedev->cdev = cdev;\n\tedev->pdev = pdev;\n\tedev->dp_module = dp_module;\n\tedev->dp_level = dp_level;\n\tedev->ops = qed_ops;\n\n\tif (is_kdump_kernel()) {\n\t\tedev->q_num_rx_buffers = NUM_RX_BDS_KDUMP_MIN;\n\t\tedev->q_num_tx_buffers = NUM_TX_BDS_KDUMP_MIN;\n\t} else {\n\t\tedev->q_num_rx_buffers = NUM_RX_BDS_DEF;\n\t\tedev->q_num_tx_buffers = NUM_TX_BDS_DEF;\n\t}\n\n\tDP_INFO(edev, \"Allocated netdev with %d tx queues and %d rx queues\\n\",\n\t\tinfo->num_queues, info->num_queues);\n\n\tSET_NETDEV_DEV(ndev, &pdev->dev);\n\n\tmemset(&edev->stats, 0, sizeof(edev->stats));\n\tmemcpy(&edev->dev_info, info, sizeof(*info));\n\n\t \n\tif (edev->dev_info.common.wol_support)\n\t\tedev->wol_enabled = true;\n\n\tINIT_LIST_HEAD(&edev->vlan_list);\n\n\treturn edev;\n}\n\nstatic void qede_init_ndev(struct qede_dev *edev)\n{\n\tstruct net_device *ndev = edev->ndev;\n\tstruct pci_dev *pdev = edev->pdev;\n\tbool udp_tunnel_enable = false;\n\tnetdev_features_t hw_features;\n\n\tpci_set_drvdata(pdev, ndev);\n\n\tndev->mem_start = edev->dev_info.common.pci_mem_start;\n\tndev->base_addr = ndev->mem_start;\n\tndev->mem_end = edev->dev_info.common.pci_mem_end;\n\tndev->irq = edev->dev_info.common.pci_irq;\n\n\tndev->watchdog_timeo = TX_TIMEOUT;\n\n\tif (IS_VF(edev)) {\n\t\tif (edev->dev_info.xdp_supported)\n\t\t\tndev->netdev_ops = &qede_netdev_vf_xdp_ops;\n\t\telse\n\t\t\tndev->netdev_ops = &qede_netdev_vf_ops;\n\t} else {\n\t\tndev->netdev_ops = &qede_netdev_ops;\n\t}\n\n\tqede_set_ethtool_ops(ndev);\n\n\tndev->priv_flags |= IFF_UNICAST_FLT;\n\n\t \n\thw_features = NETIF_F_GRO | NETIF_F_GRO_HW | NETIF_F_SG |\n\t\t      NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t      NETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_HW_TC;\n\n\tif (edev->dev_info.common.b_arfs_capable)\n\t\thw_features |= NETIF_F_NTUPLE;\n\n\tif (edev->dev_info.common.vxlan_enable ||\n\t    edev->dev_info.common.geneve_enable)\n\t\tudp_tunnel_enable = true;\n\n\tif (udp_tunnel_enable || edev->dev_info.common.gre_enable) {\n\t\thw_features |= NETIF_F_TSO_ECN;\n\t\tndev->hw_enc_features = NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\t\t\tNETIF_F_SG | NETIF_F_TSO |\n\t\t\t\t\tNETIF_F_TSO_ECN | NETIF_F_TSO6 |\n\t\t\t\t\tNETIF_F_RXCSUM;\n\t}\n\n\tif (udp_tunnel_enable) {\n\t\thw_features |= (NETIF_F_GSO_UDP_TUNNEL |\n\t\t\t\tNETIF_F_GSO_UDP_TUNNEL_CSUM);\n\t\tndev->hw_enc_features |= (NETIF_F_GSO_UDP_TUNNEL |\n\t\t\t\t\t  NETIF_F_GSO_UDP_TUNNEL_CSUM);\n\n\t\tqede_set_udp_tunnels(edev);\n\t}\n\n\tif (edev->dev_info.common.gre_enable) {\n\t\thw_features |= (NETIF_F_GSO_GRE | NETIF_F_GSO_GRE_CSUM);\n\t\tndev->hw_enc_features |= (NETIF_F_GSO_GRE |\n\t\t\t\t\t  NETIF_F_GSO_GRE_CSUM);\n\t}\n\n\tndev->vlan_features = hw_features | NETIF_F_RXHASH | NETIF_F_RXCSUM |\n\t\t\t      NETIF_F_HIGHDMA;\n\tndev->features = hw_features | NETIF_F_RXHASH | NETIF_F_RXCSUM |\n\t\t\t NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HIGHDMA |\n\t\t\t NETIF_F_HW_VLAN_CTAG_FILTER | NETIF_F_HW_VLAN_CTAG_TX;\n\n\tndev->hw_features = hw_features;\n\n\tndev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |\n\t\t\t     NETDEV_XDP_ACT_NDO_XMIT;\n\n\t \n\tndev->min_mtu = ETH_ZLEN - ETH_HLEN;\n\tndev->max_mtu = QEDE_MAX_JUMBO_PACKET_SIZE;\n\n\t \n\teth_hw_addr_set(edev->ndev, edev->dev_info.common.hw_mac);\n\n\tndev->mtu = edev->dev_info.common.mtu;\n}\n\n \nvoid qede_config_debug(uint debug, u32 *p_dp_module, u8 *p_dp_level)\n{\n\t*p_dp_level = QED_LEVEL_NOTICE;\n\t*p_dp_module = 0;\n\n\tif (debug & QED_LOG_VERBOSE_MASK) {\n\t\t*p_dp_level = QED_LEVEL_VERBOSE;\n\t\t*p_dp_module = (debug & 0x3FFFFFFF);\n\t} else if (debug & QED_LOG_INFO_MASK) {\n\t\t*p_dp_level = QED_LEVEL_INFO;\n\t} else if (debug & QED_LOG_NOTICE_MASK) {\n\t\t*p_dp_level = QED_LEVEL_NOTICE;\n\t}\n}\n\nstatic void qede_free_fp_array(struct qede_dev *edev)\n{\n\tif (edev->fp_array) {\n\t\tstruct qede_fastpath *fp;\n\t\tint i;\n\n\t\tfor_each_queue(i) {\n\t\t\tfp = &edev->fp_array[i];\n\n\t\t\tkfree(fp->sb_info);\n\t\t\t \n\t\t\tif (fp->rxq && xdp_rxq_info_is_reg(&fp->rxq->xdp_rxq))\n\t\t\t\txdp_rxq_info_unreg(&fp->rxq->xdp_rxq);\n\t\t\tkfree(fp->rxq);\n\t\t\tkfree(fp->xdp_tx);\n\t\t\tkfree(fp->txq);\n\t\t}\n\t\tkfree(edev->fp_array);\n\t}\n\n\tedev->num_queues = 0;\n\tedev->fp_num_tx = 0;\n\tedev->fp_num_rx = 0;\n}\n\nstatic int qede_alloc_fp_array(struct qede_dev *edev)\n{\n\tu8 fp_combined, fp_rx = edev->fp_num_rx;\n\tstruct qede_fastpath *fp;\n\tint i;\n\n\tedev->fp_array = kcalloc(QEDE_QUEUE_CNT(edev),\n\t\t\t\t sizeof(*edev->fp_array), GFP_KERNEL);\n\tif (!edev->fp_array) {\n\t\tDP_NOTICE(edev, \"fp array allocation failed\\n\");\n\t\tgoto err;\n\t}\n\n\tif (!edev->coal_entry) {\n\t\tedev->coal_entry = kcalloc(QEDE_MAX_RSS_CNT(edev),\n\t\t\t\t\t   sizeof(*edev->coal_entry),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!edev->coal_entry) {\n\t\t\tDP_ERR(edev, \"coalesce entry allocation failed\\n\");\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tfp_combined = QEDE_QUEUE_CNT(edev) - fp_rx - edev->fp_num_tx;\n\n\t \n\tfor_each_queue(i) {\n\t\tfp = &edev->fp_array[i];\n\n\t\tfp->sb_info = kzalloc(sizeof(*fp->sb_info), GFP_KERNEL);\n\t\tif (!fp->sb_info) {\n\t\t\tDP_NOTICE(edev, \"sb info struct allocation failed\\n\");\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (fp_rx) {\n\t\t\tfp->type = QEDE_FASTPATH_RX;\n\t\t\tfp_rx--;\n\t\t} else if (fp_combined) {\n\t\t\tfp->type = QEDE_FASTPATH_COMBINED;\n\t\t\tfp_combined--;\n\t\t} else {\n\t\t\tfp->type = QEDE_FASTPATH_TX;\n\t\t}\n\n\t\tif (fp->type & QEDE_FASTPATH_TX) {\n\t\t\tfp->txq = kcalloc(edev->dev_info.num_tc,\n\t\t\t\t\t  sizeof(*fp->txq), GFP_KERNEL);\n\t\t\tif (!fp->txq)\n\t\t\t\tgoto err;\n\t\t}\n\n\t\tif (fp->type & QEDE_FASTPATH_RX) {\n\t\t\tfp->rxq = kzalloc(sizeof(*fp->rxq), GFP_KERNEL);\n\t\t\tif (!fp->rxq)\n\t\t\t\tgoto err;\n\n\t\t\tif (edev->xdp_prog) {\n\t\t\t\tfp->xdp_tx = kzalloc(sizeof(*fp->xdp_tx),\n\t\t\t\t\t\t     GFP_KERNEL);\n\t\t\t\tif (!fp->xdp_tx)\n\t\t\t\t\tgoto err;\n\t\t\t\tfp->type |= QEDE_FASTPATH_XDP;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\nerr:\n\tqede_free_fp_array(edev);\n\treturn -ENOMEM;\n}\n\n \nvoid __qede_lock(struct qede_dev *edev)\n{\n\tmutex_lock(&edev->qede_lock);\n}\n\nvoid __qede_unlock(struct qede_dev *edev)\n{\n\tmutex_unlock(&edev->qede_lock);\n}\n\n \nstatic void qede_lock(struct qede_dev *edev)\n{\n\trtnl_lock();\n\t__qede_lock(edev);\n}\n\nstatic void qede_unlock(struct qede_dev *edev)\n{\n\t__qede_unlock(edev);\n\trtnl_unlock();\n}\n\nstatic void qede_periodic_task(struct work_struct *work)\n{\n\tstruct qede_dev *edev = container_of(work, struct qede_dev,\n\t\t\t\t\t     periodic_task.work);\n\n\tqede_fill_by_demand_stats(edev);\n\tschedule_delayed_work(&edev->periodic_task, edev->stats_coal_ticks);\n}\n\nstatic void qede_init_periodic_task(struct qede_dev *edev)\n{\n\tINIT_DELAYED_WORK(&edev->periodic_task, qede_periodic_task);\n\tspin_lock_init(&edev->stats_lock);\n\tedev->stats_coal_usecs = USEC_PER_SEC;\n\tedev->stats_coal_ticks = usecs_to_jiffies(USEC_PER_SEC);\n}\n\nstatic void qede_sp_task(struct work_struct *work)\n{\n\tstruct qede_dev *edev = container_of(work, struct qede_dev,\n\t\t\t\t\t     sp_task.work);\n\n\t \n\tif (test_bit(QEDE_SP_DISABLE, &edev->sp_flags))\n\t\treturn;\n\n\t \n\n\tif (test_and_clear_bit(QEDE_SP_RECOVERY, &edev->sp_flags)) {\n\t\tcancel_delayed_work_sync(&edev->periodic_task);\n#ifdef CONFIG_QED_SRIOV\n\t\t \n\t\tif (pci_num_vf(edev->pdev))\n\t\t\tqede_sriov_configure(edev->pdev, 0);\n#endif\n\t\tqede_lock(edev);\n\t\tqede_recovery_handler(edev);\n\t\tqede_unlock(edev);\n\t}\n\n\t__qede_lock(edev);\n\n\tif (test_and_clear_bit(QEDE_SP_RX_MODE, &edev->sp_flags))\n\t\tif (edev->state == QEDE_STATE_OPEN)\n\t\t\tqede_config_rx_mode(edev->ndev);\n\n#ifdef CONFIG_RFS_ACCEL\n\tif (test_and_clear_bit(QEDE_SP_ARFS_CONFIG, &edev->sp_flags)) {\n\t\tif (edev->state == QEDE_STATE_OPEN)\n\t\t\tqede_process_arfs_filters(edev, false);\n\t}\n#endif\n\tif (test_and_clear_bit(QEDE_SP_HW_ERR, &edev->sp_flags))\n\t\tqede_generic_hw_err_handler(edev);\n\t__qede_unlock(edev);\n\n\tif (test_and_clear_bit(QEDE_SP_AER, &edev->sp_flags)) {\n#ifdef CONFIG_QED_SRIOV\n\t\t \n\t\tif (pci_num_vf(edev->pdev))\n\t\t\tqede_sriov_configure(edev->pdev, 0);\n#endif\n\t\tedev->ops->common->recovery_process(edev->cdev);\n\t}\n}\n\nstatic void qede_update_pf_params(struct qed_dev *cdev)\n{\n\tstruct qed_pf_params pf_params;\n\tu16 num_cons;\n\n\t \n\tmemset(&pf_params, 0, sizeof(struct qed_pf_params));\n\n\t \n\tnum_cons = QED_MIN_L2_CONS;\n\n\tpf_params.eth_pf_params.num_cons = (MAX_SB_PER_PF_MIMD - 1) * num_cons;\n\n\t \n\tpf_params.eth_pf_params.num_vf_cons = 48;\n\n\tpf_params.eth_pf_params.num_arfs_filters = QEDE_RFS_MAX_FLTR;\n\tqed_ops->common->update_pf_params(cdev, &pf_params);\n}\n\n#define QEDE_FW_VER_STR_SIZE\t80\n\nstatic void qede_log_probe(struct qede_dev *edev)\n{\n\tstruct qed_dev_info *p_dev_info = &edev->dev_info.common;\n\tu8 buf[QEDE_FW_VER_STR_SIZE];\n\tsize_t left_size;\n\n\tsnprintf(buf, QEDE_FW_VER_STR_SIZE,\n\t\t \"Storm FW %d.%d.%d.%d, Management FW %d.%d.%d.%d\",\n\t\t p_dev_info->fw_major, p_dev_info->fw_minor, p_dev_info->fw_rev,\n\t\t p_dev_info->fw_eng,\n\t\t (p_dev_info->mfw_rev & QED_MFW_VERSION_3_MASK) >>\n\t\t QED_MFW_VERSION_3_OFFSET,\n\t\t (p_dev_info->mfw_rev & QED_MFW_VERSION_2_MASK) >>\n\t\t QED_MFW_VERSION_2_OFFSET,\n\t\t (p_dev_info->mfw_rev & QED_MFW_VERSION_1_MASK) >>\n\t\t QED_MFW_VERSION_1_OFFSET,\n\t\t (p_dev_info->mfw_rev & QED_MFW_VERSION_0_MASK) >>\n\t\t QED_MFW_VERSION_0_OFFSET);\n\n\tleft_size = QEDE_FW_VER_STR_SIZE - strlen(buf);\n\tif (p_dev_info->mbi_version && left_size)\n\t\tsnprintf(buf + strlen(buf), left_size,\n\t\t\t \" [MBI %d.%d.%d]\",\n\t\t\t (p_dev_info->mbi_version & QED_MBI_VERSION_2_MASK) >>\n\t\t\t QED_MBI_VERSION_2_OFFSET,\n\t\t\t (p_dev_info->mbi_version & QED_MBI_VERSION_1_MASK) >>\n\t\t\t QED_MBI_VERSION_1_OFFSET,\n\t\t\t (p_dev_info->mbi_version & QED_MBI_VERSION_0_MASK) >>\n\t\t\t QED_MBI_VERSION_0_OFFSET);\n\n\tpr_info(\"qede %02x:%02x.%02x: %s [%s]\\n\", edev->pdev->bus->number,\n\t\tPCI_SLOT(edev->pdev->devfn), PCI_FUNC(edev->pdev->devfn),\n\t\tbuf, edev->ndev->name);\n}\n\nenum qede_probe_mode {\n\tQEDE_PROBE_NORMAL,\n\tQEDE_PROBE_RECOVERY,\n};\n\nstatic int __qede_probe(struct pci_dev *pdev, u32 dp_module, u8 dp_level,\n\t\t\tbool is_vf, enum qede_probe_mode mode)\n{\n\tstruct qed_probe_params probe_params;\n\tstruct qed_slowpath_params sp_params;\n\tstruct qed_dev_eth_info dev_info;\n\tstruct qede_dev *edev;\n\tstruct qed_dev *cdev;\n\tint rc;\n\n\tif (unlikely(dp_level & QED_LEVEL_INFO))\n\t\tpr_notice(\"Starting qede probe\\n\");\n\n\tmemset(&probe_params, 0, sizeof(probe_params));\n\tprobe_params.protocol = QED_PROTOCOL_ETH;\n\tprobe_params.dp_module = dp_module;\n\tprobe_params.dp_level = dp_level;\n\tprobe_params.is_vf = is_vf;\n\tprobe_params.recov_in_prog = (mode == QEDE_PROBE_RECOVERY);\n\tcdev = qed_ops->common->probe(pdev, &probe_params);\n\tif (!cdev) {\n\t\trc = -ENODEV;\n\t\tgoto err0;\n\t}\n\n\tqede_update_pf_params(cdev);\n\n\t \n\tmemset(&sp_params, 0, sizeof(sp_params));\n\tsp_params.int_mode = QED_INT_MODE_MSIX;\n\tstrscpy(sp_params.name, \"qede LAN\", QED_DRV_VER_STR_SIZE);\n\trc = qed_ops->common->slowpath_start(cdev, &sp_params);\n\tif (rc) {\n\t\tpr_notice(\"Cannot start slowpath\\n\");\n\t\tgoto err1;\n\t}\n\n\t \n\trc = qed_ops->fill_dev_info(cdev, &dev_info);\n\tif (rc)\n\t\tgoto err2;\n\n\tif (mode != QEDE_PROBE_RECOVERY) {\n\t\tedev = qede_alloc_etherdev(cdev, pdev, &dev_info, dp_module,\n\t\t\t\t\t   dp_level);\n\t\tif (!edev) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err2;\n\t\t}\n\n\t\tedev->devlink = qed_ops->common->devlink_register(cdev);\n\t\tif (IS_ERR(edev->devlink)) {\n\t\t\tDP_NOTICE(edev, \"Cannot register devlink\\n\");\n\t\t\trc = PTR_ERR(edev->devlink);\n\t\t\tedev->devlink = NULL;\n\t\t\tgoto err3;\n\t\t}\n\t} else {\n\t\tstruct net_device *ndev = pci_get_drvdata(pdev);\n\t\tstruct qed_devlink *qdl;\n\n\t\tedev = netdev_priv(ndev);\n\t\tqdl = devlink_priv(edev->devlink);\n\t\tqdl->cdev = cdev;\n\t\tedev->cdev = cdev;\n\t\tmemset(&edev->stats, 0, sizeof(edev->stats));\n\t\tmemcpy(&edev->dev_info, &dev_info, sizeof(dev_info));\n\t}\n\n\tif (is_vf)\n\t\tset_bit(QEDE_FLAGS_IS_VF, &edev->flags);\n\n\tqede_init_ndev(edev);\n\n\trc = qede_rdma_dev_add(edev, (mode == QEDE_PROBE_RECOVERY));\n\tif (rc)\n\t\tgoto err3;\n\n\tif (mode != QEDE_PROBE_RECOVERY) {\n\t\t \n\t\tINIT_DELAYED_WORK(&edev->sp_task, qede_sp_task);\n\t\tmutex_init(&edev->qede_lock);\n\t\tqede_init_periodic_task(edev);\n\n\t\trc = register_netdev(edev->ndev);\n\t\tif (rc) {\n\t\t\tDP_NOTICE(edev, \"Cannot register net-device\\n\");\n\t\t\tgoto err4;\n\t\t}\n\t}\n\n\tedev->ops->common->set_name(cdev, edev->ndev->name);\n\n\t \n\tif (!is_vf)\n\t\tqede_ptp_enable(edev);\n\n\tedev->ops->register_ops(cdev, &qede_ll_ops, edev);\n\n#ifdef CONFIG_DCB\n\tif (!IS_VF(edev))\n\t\tqede_set_dcbnl_ops(edev->ndev);\n#endif\n\n\tedev->rx_copybreak = QEDE_RX_HDR_SIZE;\n\n\tqede_log_probe(edev);\n\n\t \n\tif (edev->stats_coal_usecs)\n\t\tschedule_delayed_work(&edev->periodic_task, 0);\n\n\treturn 0;\n\nerr4:\n\tqede_rdma_dev_remove(edev, (mode == QEDE_PROBE_RECOVERY));\nerr3:\n\tif (mode != QEDE_PROBE_RECOVERY)\n\t\tfree_netdev(edev->ndev);\n\telse\n\t\tedev->cdev = NULL;\nerr2:\n\tqed_ops->common->slowpath_stop(cdev);\nerr1:\n\tqed_ops->common->remove(cdev);\nerr0:\n\treturn rc;\n}\n\nstatic int qede_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tbool is_vf = false;\n\tu32 dp_module = 0;\n\tu8 dp_level = 0;\n\n\tswitch ((enum qede_pci_private)id->driver_data) {\n\tcase QEDE_PRIVATE_VF:\n\t\tif (debug & QED_LOG_VERBOSE_MASK)\n\t\t\tdev_err(&pdev->dev, \"Probing a VF\\n\");\n\t\tis_vf = true;\n\t\tbreak;\n\tdefault:\n\t\tif (debug & QED_LOG_VERBOSE_MASK)\n\t\t\tdev_err(&pdev->dev, \"Probing a PF\\n\");\n\t}\n\n\tqede_config_debug(debug, &dp_module, &dp_level);\n\n\treturn __qede_probe(pdev, dp_module, dp_level, is_vf,\n\t\t\t    QEDE_PROBE_NORMAL);\n}\n\nenum qede_remove_mode {\n\tQEDE_REMOVE_NORMAL,\n\tQEDE_REMOVE_RECOVERY,\n};\n\nstatic void __qede_remove(struct pci_dev *pdev, enum qede_remove_mode mode)\n{\n\tstruct net_device *ndev = pci_get_drvdata(pdev);\n\tstruct qede_dev *edev;\n\tstruct qed_dev *cdev;\n\n\tif (!ndev) {\n\t\tdev_info(&pdev->dev, \"Device has already been removed\\n\");\n\t\treturn;\n\t}\n\n\tedev = netdev_priv(ndev);\n\tcdev = edev->cdev;\n\n\tDP_INFO(edev, \"Starting qede_remove\\n\");\n\n\tqede_rdma_dev_remove(edev, (mode == QEDE_REMOVE_RECOVERY));\n\n\tif (mode != QEDE_REMOVE_RECOVERY) {\n\t\tset_bit(QEDE_SP_DISABLE, &edev->sp_flags);\n\t\tunregister_netdev(ndev);\n\n\t\tcancel_delayed_work_sync(&edev->sp_task);\n\t\tcancel_delayed_work_sync(&edev->periodic_task);\n\n\t\tedev->ops->common->set_power_state(cdev, PCI_D0);\n\n\t\tpci_set_drvdata(pdev, NULL);\n\t}\n\n\tqede_ptp_disable(edev);\n\n\t \n\tqed_ops->common->slowpath_stop(cdev);\n\tif (system_state == SYSTEM_POWER_OFF)\n\t\treturn;\n\n\tif (mode != QEDE_REMOVE_RECOVERY && edev->devlink) {\n\t\tqed_ops->common->devlink_unregister(edev->devlink);\n\t\tedev->devlink = NULL;\n\t}\n\tqed_ops->common->remove(cdev);\n\tedev->cdev = NULL;\n\n\t \n\tif (mode != QEDE_REMOVE_RECOVERY) {\n\t\tkfree(edev->coal_entry);\n\t\tfree_netdev(ndev);\n\t}\n\n\tdev_info(&pdev->dev, \"Ending qede_remove successfully\\n\");\n}\n\nstatic void qede_remove(struct pci_dev *pdev)\n{\n\t__qede_remove(pdev, QEDE_REMOVE_NORMAL);\n}\n\nstatic void qede_shutdown(struct pci_dev *pdev)\n{\n\t__qede_remove(pdev, QEDE_REMOVE_NORMAL);\n}\n\n \n\nstatic int qede_set_num_queues(struct qede_dev *edev)\n{\n\tint rc;\n\tu16 rss_num;\n\n\t \n\tif (edev->req_queues)\n\t\trss_num = edev->req_queues;\n\telse\n\t\trss_num = netif_get_num_default_rss_queues() *\n\t\t\t  edev->dev_info.common.num_hwfns;\n\n\trss_num = min_t(u16, QEDE_MAX_RSS_CNT(edev), rss_num);\n\n\trc = edev->ops->common->set_fp_int(edev->cdev, rss_num);\n\tif (rc > 0) {\n\t\t \n\t\tedev->num_queues = rc;\n\t\tDP_INFO(edev, \"Managed %d [of %d] RSS queues\\n\",\n\t\t\tQEDE_QUEUE_CNT(edev), rss_num);\n\t\trc = 0;\n\t}\n\n\tedev->fp_num_tx = edev->req_num_tx;\n\tedev->fp_num_rx = edev->req_num_rx;\n\n\treturn rc;\n}\n\nstatic void qede_free_mem_sb(struct qede_dev *edev, struct qed_sb_info *sb_info,\n\t\t\t     u16 sb_id)\n{\n\tif (sb_info->sb_virt) {\n\t\tedev->ops->common->sb_release(edev->cdev, sb_info, sb_id,\n\t\t\t\t\t      QED_SB_TYPE_L2_QUEUE);\n\t\tdma_free_coherent(&edev->pdev->dev, sizeof(*sb_info->sb_virt),\n\t\t\t\t  (void *)sb_info->sb_virt, sb_info->sb_phys);\n\t\tmemset(sb_info, 0, sizeof(*sb_info));\n\t}\n}\n\n \nstatic int qede_alloc_mem_sb(struct qede_dev *edev,\n\t\t\t     struct qed_sb_info *sb_info, u16 sb_id)\n{\n\tstruct status_block *sb_virt;\n\tdma_addr_t sb_phys;\n\tint rc;\n\n\tsb_virt = dma_alloc_coherent(&edev->pdev->dev,\n\t\t\t\t     sizeof(*sb_virt), &sb_phys, GFP_KERNEL);\n\tif (!sb_virt) {\n\t\tDP_ERR(edev, \"Status block allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trc = edev->ops->common->sb_init(edev->cdev, sb_info,\n\t\t\t\t\tsb_virt, sb_phys, sb_id,\n\t\t\t\t\tQED_SB_TYPE_L2_QUEUE);\n\tif (rc) {\n\t\tDP_ERR(edev, \"Status block initialization failed\\n\");\n\t\tdma_free_coherent(&edev->pdev->dev, sizeof(*sb_virt),\n\t\t\t\t  sb_virt, sb_phys);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic void qede_free_rx_buffers(struct qede_dev *edev,\n\t\t\t\t struct qede_rx_queue *rxq)\n{\n\tu16 i;\n\n\tfor (i = rxq->sw_rx_cons; i != rxq->sw_rx_prod; i++) {\n\t\tstruct sw_rx_data *rx_buf;\n\t\tstruct page *data;\n\n\t\trx_buf = &rxq->sw_rx_ring[i & NUM_RX_BDS_MAX];\n\t\tdata = rx_buf->data;\n\n\t\tdma_unmap_page(&edev->pdev->dev,\n\t\t\t       rx_buf->mapping, PAGE_SIZE, rxq->data_direction);\n\n\t\trx_buf->data = NULL;\n\t\t__free_page(data);\n\t}\n}\n\nstatic void qede_free_mem_rxq(struct qede_dev *edev, struct qede_rx_queue *rxq)\n{\n\t \n\tqede_free_rx_buffers(edev, rxq);\n\n\t \n\tkfree(rxq->sw_rx_ring);\n\n\t \n\tedev->ops->common->chain_free(edev->cdev, &rxq->rx_bd_ring);\n\tedev->ops->common->chain_free(edev->cdev, &rxq->rx_comp_ring);\n}\n\nstatic void qede_set_tpa_param(struct qede_rx_queue *rxq)\n{\n\tint i;\n\n\tfor (i = 0; i < ETH_TPA_MAX_AGGS_NUM; i++) {\n\t\tstruct qede_agg_info *tpa_info = &rxq->tpa_info[i];\n\n\t\ttpa_info->state = QEDE_AGG_STATE_NONE;\n\t}\n}\n\n \nstatic int qede_alloc_mem_rxq(struct qede_dev *edev, struct qede_rx_queue *rxq)\n{\n\tstruct qed_chain_init_params params = {\n\t\t.cnt_type\t= QED_CHAIN_CNT_TYPE_U16,\n\t\t.num_elems\t= RX_RING_SIZE,\n\t};\n\tstruct qed_dev *cdev = edev->cdev;\n\tint i, rc, size;\n\n\trxq->num_rx_buffers = edev->q_num_rx_buffers;\n\n\trxq->rx_buf_size = NET_IP_ALIGN + ETH_OVERHEAD + edev->ndev->mtu;\n\n\trxq->rx_headroom = edev->xdp_prog ? XDP_PACKET_HEADROOM : NET_SKB_PAD;\n\tsize = rxq->rx_headroom +\n\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\t \n\tif (rxq->rx_buf_size + size > PAGE_SIZE)\n\t\trxq->rx_buf_size = PAGE_SIZE - size;\n\n\t \n\tif (!edev->xdp_prog) {\n\t\tsize = size + rxq->rx_buf_size;\n\t\trxq->rx_buf_seg_size = roundup_pow_of_two(size);\n\t} else {\n\t\trxq->rx_buf_seg_size = PAGE_SIZE;\n\t\tedev->ndev->features &= ~NETIF_F_GRO_HW;\n\t}\n\n\t \n\tsize = sizeof(*rxq->sw_rx_ring) * RX_RING_SIZE;\n\trxq->sw_rx_ring = kzalloc(size, GFP_KERNEL);\n\tif (!rxq->sw_rx_ring) {\n\t\tDP_ERR(edev, \"Rx buffers ring allocation failed\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t \n\tparams.mode = QED_CHAIN_MODE_NEXT_PTR;\n\tparams.intended_use = QED_CHAIN_USE_TO_CONSUME_PRODUCE;\n\tparams.elem_size = sizeof(struct eth_rx_bd);\n\n\trc = edev->ops->common->chain_alloc(cdev, &rxq->rx_bd_ring, &params);\n\tif (rc)\n\t\tgoto err;\n\n\t \n\tparams.mode = QED_CHAIN_MODE_PBL;\n\tparams.intended_use = QED_CHAIN_USE_TO_CONSUME;\n\tparams.elem_size = sizeof(union eth_rx_cqe);\n\n\trc = edev->ops->common->chain_alloc(cdev, &rxq->rx_comp_ring, &params);\n\tif (rc)\n\t\tgoto err;\n\n\t \n\trxq->filled_buffers = 0;\n\tfor (i = 0; i < rxq->num_rx_buffers; i++) {\n\t\trc = qede_alloc_rx_buffer(rxq, false);\n\t\tif (rc) {\n\t\t\tDP_ERR(edev,\n\t\t\t       \"Rx buffers allocation failed at index %d\\n\", i);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tedev->gro_disable = !(edev->ndev->features & NETIF_F_GRO_HW);\n\tif (!edev->gro_disable)\n\t\tqede_set_tpa_param(rxq);\nerr:\n\treturn rc;\n}\n\nstatic void qede_free_mem_txq(struct qede_dev *edev, struct qede_tx_queue *txq)\n{\n\t \n\tif (txq->is_xdp)\n\t\tkfree(txq->sw_tx_ring.xdp);\n\telse\n\t\tkfree(txq->sw_tx_ring.skbs);\n\n\t \n\tedev->ops->common->chain_free(edev->cdev, &txq->tx_pbl);\n}\n\n \nstatic int qede_alloc_mem_txq(struct qede_dev *edev, struct qede_tx_queue *txq)\n{\n\tstruct qed_chain_init_params params = {\n\t\t.mode\t\t= QED_CHAIN_MODE_PBL,\n\t\t.intended_use\t= QED_CHAIN_USE_TO_CONSUME_PRODUCE,\n\t\t.cnt_type\t= QED_CHAIN_CNT_TYPE_U16,\n\t\t.num_elems\t= edev->q_num_tx_buffers,\n\t\t.elem_size\t= sizeof(union eth_tx_bd_types),\n\t};\n\tint size, rc;\n\n\ttxq->num_tx_buffers = edev->q_num_tx_buffers;\n\n\t \n\tif (txq->is_xdp) {\n\t\tsize = sizeof(*txq->sw_tx_ring.xdp) * txq->num_tx_buffers;\n\t\ttxq->sw_tx_ring.xdp = kzalloc(size, GFP_KERNEL);\n\t\tif (!txq->sw_tx_ring.xdp)\n\t\t\tgoto err;\n\t} else {\n\t\tsize = sizeof(*txq->sw_tx_ring.skbs) * txq->num_tx_buffers;\n\t\ttxq->sw_tx_ring.skbs = kzalloc(size, GFP_KERNEL);\n\t\tif (!txq->sw_tx_ring.skbs)\n\t\t\tgoto err;\n\t}\n\n\trc = edev->ops->common->chain_alloc(edev->cdev, &txq->tx_pbl, &params);\n\tif (rc)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tqede_free_mem_txq(edev, txq);\n\treturn -ENOMEM;\n}\n\n \nstatic void qede_free_mem_fp(struct qede_dev *edev, struct qede_fastpath *fp)\n{\n\tqede_free_mem_sb(edev, fp->sb_info, fp->id);\n\n\tif (fp->type & QEDE_FASTPATH_RX)\n\t\tqede_free_mem_rxq(edev, fp->rxq);\n\n\tif (fp->type & QEDE_FASTPATH_XDP)\n\t\tqede_free_mem_txq(edev, fp->xdp_tx);\n\n\tif (fp->type & QEDE_FASTPATH_TX) {\n\t\tint cos;\n\n\t\tfor_each_cos_in_txq(edev, cos)\n\t\t\tqede_free_mem_txq(edev, &fp->txq[cos]);\n\t}\n}\n\n \nstatic int qede_alloc_mem_fp(struct qede_dev *edev, struct qede_fastpath *fp)\n{\n\tint rc = 0;\n\n\trc = qede_alloc_mem_sb(edev, fp->sb_info, fp->id);\n\tif (rc)\n\t\tgoto out;\n\n\tif (fp->type & QEDE_FASTPATH_RX) {\n\t\trc = qede_alloc_mem_rxq(edev, fp->rxq);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\tif (fp->type & QEDE_FASTPATH_XDP) {\n\t\trc = qede_alloc_mem_txq(edev, fp->xdp_tx);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\tif (fp->type & QEDE_FASTPATH_TX) {\n\t\tint cos;\n\n\t\tfor_each_cos_in_txq(edev, cos) {\n\t\t\trc = qede_alloc_mem_txq(edev, &fp->txq[cos]);\n\t\t\tif (rc)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\treturn rc;\n}\n\nstatic void qede_free_mem_load(struct qede_dev *edev)\n{\n\tint i;\n\n\tfor_each_queue(i) {\n\t\tstruct qede_fastpath *fp = &edev->fp_array[i];\n\n\t\tqede_free_mem_fp(edev, fp);\n\t}\n}\n\n \nstatic int qede_alloc_mem_load(struct qede_dev *edev)\n{\n\tint rc = 0, queue_id;\n\n\tfor (queue_id = 0; queue_id < QEDE_QUEUE_CNT(edev); queue_id++) {\n\t\tstruct qede_fastpath *fp = &edev->fp_array[queue_id];\n\n\t\trc = qede_alloc_mem_fp(edev, fp);\n\t\tif (rc) {\n\t\t\tDP_ERR(edev,\n\t\t\t       \"Failed to allocate memory for fastpath - rss id = %d\\n\",\n\t\t\t       queue_id);\n\t\t\tqede_free_mem_load(edev);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void qede_empty_tx_queue(struct qede_dev *edev,\n\t\t\t\tstruct qede_tx_queue *txq)\n{\n\tunsigned int pkts_compl = 0, bytes_compl = 0;\n\tstruct netdev_queue *netdev_txq;\n\tint rc, len = 0;\n\n\tnetdev_txq = netdev_get_tx_queue(edev->ndev, txq->ndev_txq_id);\n\n\twhile (qed_chain_get_cons_idx(&txq->tx_pbl) !=\n\t       qed_chain_get_prod_idx(&txq->tx_pbl)) {\n\t\tDP_VERBOSE(edev, NETIF_MSG_IFDOWN,\n\t\t\t   \"Freeing a packet on tx queue[%d]: chain_cons 0x%x, chain_prod 0x%x\\n\",\n\t\t\t   txq->index, qed_chain_get_cons_idx(&txq->tx_pbl),\n\t\t\t   qed_chain_get_prod_idx(&txq->tx_pbl));\n\n\t\trc = qede_free_tx_pkt(edev, txq, &len);\n\t\tif (rc) {\n\t\t\tDP_NOTICE(edev,\n\t\t\t\t  \"Failed to free a packet on tx queue[%d]: chain_cons 0x%x, chain_prod 0x%x\\n\",\n\t\t\t\t  txq->index,\n\t\t\t\t  qed_chain_get_cons_idx(&txq->tx_pbl),\n\t\t\t\t  qed_chain_get_prod_idx(&txq->tx_pbl));\n\t\t\tbreak;\n\t\t}\n\n\t\tbytes_compl += len;\n\t\tpkts_compl++;\n\t\ttxq->sw_tx_cons++;\n\t}\n\n\tnetdev_tx_completed_queue(netdev_txq, pkts_compl, bytes_compl);\n}\n\nstatic void qede_empty_tx_queues(struct qede_dev *edev)\n{\n\tint i;\n\n\tfor_each_queue(i)\n\t\tif (edev->fp_array[i].type & QEDE_FASTPATH_TX) {\n\t\t\tint cos;\n\n\t\t\tfor_each_cos_in_txq(edev, cos) {\n\t\t\t\tstruct qede_fastpath *fp;\n\n\t\t\t\tfp = &edev->fp_array[i];\n\t\t\t\tqede_empty_tx_queue(edev,\n\t\t\t\t\t\t    &fp->txq[cos]);\n\t\t\t}\n\t\t}\n}\n\n \nstatic void qede_init_fp(struct qede_dev *edev)\n{\n\tint queue_id, rxq_index = 0, txq_index = 0;\n\tstruct qede_fastpath *fp;\n\tbool init_xdp = false;\n\n\tfor_each_queue(queue_id) {\n\t\tfp = &edev->fp_array[queue_id];\n\n\t\tfp->edev = edev;\n\t\tfp->id = queue_id;\n\n\t\tif (fp->type & QEDE_FASTPATH_XDP) {\n\t\t\tfp->xdp_tx->index = QEDE_TXQ_IDX_TO_XDP(edev,\n\t\t\t\t\t\t\t\trxq_index);\n\t\t\tfp->xdp_tx->is_xdp = 1;\n\n\t\t\tspin_lock_init(&fp->xdp_tx->xdp_tx_lock);\n\t\t\tinit_xdp = true;\n\t\t}\n\n\t\tif (fp->type & QEDE_FASTPATH_RX) {\n\t\t\tfp->rxq->rxq_id = rxq_index++;\n\n\t\t\t \n\t\t\tif (fp->type & QEDE_FASTPATH_XDP)\n\t\t\t\tfp->rxq->data_direction = DMA_BIDIRECTIONAL;\n\t\t\telse\n\t\t\t\tfp->rxq->data_direction = DMA_FROM_DEVICE;\n\t\t\tfp->rxq->dev = &edev->pdev->dev;\n\n\t\t\t \n\t\t\tWARN_ON(xdp_rxq_info_reg(&fp->rxq->xdp_rxq, edev->ndev,\n\t\t\t\t\t\t fp->rxq->rxq_id, 0) < 0);\n\n\t\t\tif (xdp_rxq_info_reg_mem_model(&fp->rxq->xdp_rxq,\n\t\t\t\t\t\t       MEM_TYPE_PAGE_ORDER0,\n\t\t\t\t\t\t       NULL)) {\n\t\t\t\tDP_NOTICE(edev,\n\t\t\t\t\t  \"Failed to register XDP memory model\\n\");\n\t\t\t}\n\t\t}\n\n\t\tif (fp->type & QEDE_FASTPATH_TX) {\n\t\t\tint cos;\n\n\t\t\tfor_each_cos_in_txq(edev, cos) {\n\t\t\t\tstruct qede_tx_queue *txq = &fp->txq[cos];\n\t\t\t\tu16 ndev_tx_id;\n\n\t\t\t\ttxq->cos = cos;\n\t\t\t\ttxq->index = txq_index;\n\t\t\t\tndev_tx_id = QEDE_TXQ_TO_NDEV_TXQ_ID(edev, txq);\n\t\t\t\ttxq->ndev_txq_id = ndev_tx_id;\n\n\t\t\t\tif (edev->dev_info.is_legacy)\n\t\t\t\t\ttxq->is_legacy = true;\n\t\t\t\ttxq->dev = &edev->pdev->dev;\n\t\t\t}\n\n\t\t\ttxq_index++;\n\t\t}\n\n\t\tsnprintf(fp->name, sizeof(fp->name), \"%s-fp-%d\",\n\t\t\t edev->ndev->name, queue_id);\n\t}\n\n\tif (init_xdp) {\n\t\tedev->total_xdp_queues = QEDE_RSS_COUNT(edev);\n\t\tDP_INFO(edev, \"Total XDP queues: %u\\n\", edev->total_xdp_queues);\n\t}\n}\n\nstatic int qede_set_real_num_queues(struct qede_dev *edev)\n{\n\tint rc = 0;\n\n\trc = netif_set_real_num_tx_queues(edev->ndev,\n\t\t\t\t\t  QEDE_TSS_COUNT(edev) *\n\t\t\t\t\t  edev->dev_info.num_tc);\n\tif (rc) {\n\t\tDP_NOTICE(edev, \"Failed to set real number of Tx queues\\n\");\n\t\treturn rc;\n\t}\n\n\trc = netif_set_real_num_rx_queues(edev->ndev, QEDE_RSS_COUNT(edev));\n\tif (rc) {\n\t\tDP_NOTICE(edev, \"Failed to set real number of Rx queues\\n\");\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic void qede_napi_disable_remove(struct qede_dev *edev)\n{\n\tint i;\n\n\tfor_each_queue(i) {\n\t\tnapi_disable(&edev->fp_array[i].napi);\n\n\t\tnetif_napi_del(&edev->fp_array[i].napi);\n\t}\n}\n\nstatic void qede_napi_add_enable(struct qede_dev *edev)\n{\n\tint i;\n\n\t \n\tfor_each_queue(i) {\n\t\tnetif_napi_add(edev->ndev, &edev->fp_array[i].napi, qede_poll);\n\t\tnapi_enable(&edev->fp_array[i].napi);\n\t}\n}\n\nstatic void qede_sync_free_irqs(struct qede_dev *edev)\n{\n\tint i;\n\n\tfor (i = 0; i < edev->int_info.used_cnt; i++) {\n\t\tif (edev->int_info.msix_cnt) {\n\t\t\tfree_irq(edev->int_info.msix[i].vector,\n\t\t\t\t &edev->fp_array[i]);\n\t\t} else {\n\t\t\tedev->ops->common->simd_handler_clean(edev->cdev, i);\n\t\t}\n\t}\n\n\tedev->int_info.used_cnt = 0;\n\tedev->int_info.msix_cnt = 0;\n}\n\nstatic int qede_req_msix_irqs(struct qede_dev *edev)\n{\n\tint i, rc;\n\n\t \n\tif (QEDE_QUEUE_CNT(edev) > edev->int_info.msix_cnt) {\n\t\tDP_ERR(edev,\n\t\t       \"Interrupt mismatch: %d RSS queues > %d MSI-x vectors\\n\",\n\t\t       QEDE_QUEUE_CNT(edev), edev->int_info.msix_cnt);\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < QEDE_QUEUE_CNT(edev); i++) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct qede_fastpath *fp = &edev->fp_array[i];\n\n\t\tif (edev->ndev->rx_cpu_rmap && (fp->type & QEDE_FASTPATH_RX)) {\n\t\t\trc = irq_cpu_rmap_add(edev->ndev->rx_cpu_rmap,\n\t\t\t\t\t      edev->int_info.msix[i].vector);\n\t\t\tif (rc) {\n\t\t\t\tDP_ERR(edev, \"Failed to add CPU rmap\\n\");\n\t\t\t\tqede_free_arfs(edev);\n\t\t\t}\n\t\t}\n#endif\n\t\trc = request_irq(edev->int_info.msix[i].vector,\n\t\t\t\t qede_msix_fp_int, 0, edev->fp_array[i].name,\n\t\t\t\t &edev->fp_array[i]);\n\t\tif (rc) {\n\t\t\tDP_ERR(edev, \"Request fp %d irq failed\\n\", i);\n#ifdef CONFIG_RFS_ACCEL\n\t\t\tif (edev->ndev->rx_cpu_rmap)\n\t\t\t\tfree_irq_cpu_rmap(edev->ndev->rx_cpu_rmap);\n\n\t\t\tedev->ndev->rx_cpu_rmap = NULL;\n#endif\n\t\t\tqede_sync_free_irqs(edev);\n\t\t\treturn rc;\n\t\t}\n\t\tDP_VERBOSE(edev, NETIF_MSG_INTR,\n\t\t\t   \"Requested fp irq for %s [entry %d]. Cookie is at %p\\n\",\n\t\t\t   edev->fp_array[i].name, i,\n\t\t\t   &edev->fp_array[i]);\n\t\tedev->int_info.used_cnt++;\n\t}\n\n\treturn 0;\n}\n\nstatic void qede_simd_fp_handler(void *cookie)\n{\n\tstruct qede_fastpath *fp = (struct qede_fastpath *)cookie;\n\n\tnapi_schedule_irqoff(&fp->napi);\n}\n\nstatic int qede_setup_irqs(struct qede_dev *edev)\n{\n\tint i, rc = 0;\n\n\t \n\trc = edev->ops->common->get_fp_int(edev->cdev, &edev->int_info);\n\tif (rc)\n\t\treturn rc;\n\n\tif (edev->int_info.msix_cnt) {\n\t\trc = qede_req_msix_irqs(edev);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tedev->ndev->irq = edev->int_info.msix[0].vector;\n\t} else {\n\t\tconst struct qed_common_ops *ops;\n\n\t\t \n\t\tops = edev->ops->common;\n\t\tfor (i = 0; i < QEDE_QUEUE_CNT(edev); i++)\n\t\t\tops->simd_handler_config(edev->cdev,\n\t\t\t\t\t\t &edev->fp_array[i], i,\n\t\t\t\t\t\t qede_simd_fp_handler);\n\t\tedev->int_info.used_cnt = QEDE_QUEUE_CNT(edev);\n\t}\n\treturn 0;\n}\n\nstatic int qede_drain_txq(struct qede_dev *edev,\n\t\t\t  struct qede_tx_queue *txq, bool allow_drain)\n{\n\tint rc, cnt = 1000;\n\n\twhile (txq->sw_tx_cons != txq->sw_tx_prod) {\n\t\tif (!cnt) {\n\t\t\tif (allow_drain) {\n\t\t\t\tDP_NOTICE(edev,\n\t\t\t\t\t  \"Tx queue[%d] is stuck, requesting MCP to drain\\n\",\n\t\t\t\t\t  txq->index);\n\t\t\t\trc = edev->ops->common->drain(edev->cdev);\n\t\t\t\tif (rc)\n\t\t\t\t\treturn rc;\n\t\t\t\treturn qede_drain_txq(edev, txq, false);\n\t\t\t}\n\t\t\tDP_NOTICE(edev,\n\t\t\t\t  \"Timeout waiting for tx queue[%d]: PROD=%d, CONS=%d\\n\",\n\t\t\t\t  txq->index, txq->sw_tx_prod,\n\t\t\t\t  txq->sw_tx_cons);\n\t\t\treturn -ENODEV;\n\t\t}\n\t\tcnt--;\n\t\tusleep_range(1000, 2000);\n\t\tbarrier();\n\t}\n\n\t \n\tusleep_range(1000, 2000);\n\n\treturn 0;\n}\n\nstatic int qede_stop_txq(struct qede_dev *edev,\n\t\t\t struct qede_tx_queue *txq, int rss_id)\n{\n\t \n\tedev->ops->common->db_recovery_del(edev->cdev, txq->doorbell_addr,\n\t\t\t\t\t   &txq->tx_db);\n\n\treturn edev->ops->q_tx_stop(edev->cdev, rss_id, txq->handle);\n}\n\nstatic int qede_stop_queues(struct qede_dev *edev)\n{\n\tstruct qed_update_vport_params *vport_update_params;\n\tstruct qed_dev *cdev = edev->cdev;\n\tstruct qede_fastpath *fp;\n\tint rc, i;\n\n\t \n\tvport_update_params = vzalloc(sizeof(*vport_update_params));\n\tif (!vport_update_params)\n\t\treturn -ENOMEM;\n\n\tvport_update_params->vport_id = 0;\n\tvport_update_params->update_vport_active_flg = 1;\n\tvport_update_params->vport_active_flg = 0;\n\tvport_update_params->update_rss_flg = 0;\n\n\trc = edev->ops->vport_update(cdev, vport_update_params);\n\tvfree(vport_update_params);\n\n\tif (rc) {\n\t\tDP_ERR(edev, \"Failed to update vport\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tfor_each_queue(i) {\n\t\tfp = &edev->fp_array[i];\n\n\t\tif (fp->type & QEDE_FASTPATH_TX) {\n\t\t\tint cos;\n\n\t\t\tfor_each_cos_in_txq(edev, cos) {\n\t\t\t\trc = qede_drain_txq(edev, &fp->txq[cos], true);\n\t\t\t\tif (rc)\n\t\t\t\t\treturn rc;\n\t\t\t}\n\t\t}\n\n\t\tif (fp->type & QEDE_FASTPATH_XDP) {\n\t\t\trc = qede_drain_txq(edev, fp->xdp_tx, true);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\t}\n\n\t \n\tfor (i = QEDE_QUEUE_CNT(edev) - 1; i >= 0; i--) {\n\t\tfp = &edev->fp_array[i];\n\n\t\t \n\t\tif (fp->type & QEDE_FASTPATH_TX) {\n\t\t\tint cos;\n\n\t\t\tfor_each_cos_in_txq(edev, cos) {\n\t\t\t\trc = qede_stop_txq(edev, &fp->txq[cos], i);\n\t\t\t\tif (rc)\n\t\t\t\t\treturn rc;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (fp->type & QEDE_FASTPATH_RX) {\n\t\t\trc = edev->ops->q_rx_stop(cdev, i, fp->rxq->handle);\n\t\t\tif (rc) {\n\t\t\t\tDP_ERR(edev, \"Failed to stop RXQ #%d\\n\", i);\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (fp->type & QEDE_FASTPATH_XDP) {\n\t\t\trc = qede_stop_txq(edev, fp->xdp_tx, i);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\n\t\t\tbpf_prog_put(fp->rxq->xdp_prog);\n\t\t}\n\t}\n\n\t \n\trc = edev->ops->vport_stop(cdev, 0);\n\tif (rc)\n\t\tDP_ERR(edev, \"Failed to stop VPORT\\n\");\n\n\treturn rc;\n}\n\nstatic int qede_start_txq(struct qede_dev *edev,\n\t\t\t  struct qede_fastpath *fp,\n\t\t\t  struct qede_tx_queue *txq, u8 rss_id, u16 sb_idx)\n{\n\tdma_addr_t phys_table = qed_chain_get_pbl_phys(&txq->tx_pbl);\n\tu32 page_cnt = qed_chain_get_page_cnt(&txq->tx_pbl);\n\tstruct qed_queue_start_common_params params;\n\tstruct qed_txq_start_ret_params ret_params;\n\tint rc;\n\n\tmemset(&params, 0, sizeof(params));\n\tmemset(&ret_params, 0, sizeof(ret_params));\n\n\t \n\tif (txq->is_xdp)\n\t\tparams.queue_id = QEDE_TXQ_XDP_TO_IDX(edev, txq);\n\telse\n\t\tparams.queue_id = txq->index;\n\n\tparams.p_sb = fp->sb_info;\n\tparams.sb_idx = sb_idx;\n\tparams.tc = txq->cos;\n\n\trc = edev->ops->q_tx_start(edev->cdev, rss_id, &params, phys_table,\n\t\t\t\t   page_cnt, &ret_params);\n\tif (rc) {\n\t\tDP_ERR(edev, \"Start TXQ #%d failed %d\\n\", txq->index, rc);\n\t\treturn rc;\n\t}\n\n\ttxq->doorbell_addr = ret_params.p_doorbell;\n\ttxq->handle = ret_params.p_handle;\n\n\t \n\ttxq->hw_cons_ptr = &fp->sb_info->sb_virt->pi_array[sb_idx];\n\n\t \n\tSET_FIELD(txq->tx_db.data.params, ETH_DB_DATA_DEST, DB_DEST_XCM);\n\tSET_FIELD(txq->tx_db.data.params, ETH_DB_DATA_AGG_CMD, DB_AGG_CMD_SET);\n\tSET_FIELD(txq->tx_db.data.params, ETH_DB_DATA_AGG_VAL_SEL,\n\t\t  DQ_XCM_ETH_TX_BD_PROD_CMD);\n\ttxq->tx_db.data.agg_flags = DQ_XCM_ETH_DQ_CF_CMD;\n\n\t \n\trc = edev->ops->common->db_recovery_add(edev->cdev, txq->doorbell_addr,\n\t\t\t\t\t\t&txq->tx_db, DB_REC_WIDTH_32B,\n\t\t\t\t\t\tDB_REC_KERNEL);\n\n\treturn rc;\n}\n\nstatic int qede_start_queues(struct qede_dev *edev, bool clear_stats)\n{\n\tint vlan_removal_en = 1;\n\tstruct qed_dev *cdev = edev->cdev;\n\tstruct qed_dev_info *qed_info = &edev->dev_info.common;\n\tstruct qed_update_vport_params *vport_update_params;\n\tstruct qed_queue_start_common_params q_params;\n\tstruct qed_start_vport_params start = {0};\n\tint rc, i;\n\n\tif (!edev->num_queues) {\n\t\tDP_ERR(edev,\n\t\t       \"Cannot update V-VPORT as active as there are no Rx queues\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tvport_update_params = vzalloc(sizeof(*vport_update_params));\n\tif (!vport_update_params)\n\t\treturn -ENOMEM;\n\n\tstart.handle_ptp_pkts = !!(edev->ptp);\n\tstart.gro_enable = !edev->gro_disable;\n\tstart.mtu = edev->ndev->mtu;\n\tstart.vport_id = 0;\n\tstart.drop_ttl0 = true;\n\tstart.remove_inner_vlan = vlan_removal_en;\n\tstart.clear_stats = clear_stats;\n\n\trc = edev->ops->vport_start(cdev, &start);\n\n\tif (rc) {\n\t\tDP_ERR(edev, \"Start V-PORT failed %d\\n\", rc);\n\t\tgoto out;\n\t}\n\n\tDP_VERBOSE(edev, NETIF_MSG_IFUP,\n\t\t   \"Start vport ramrod passed, vport_id = %d, MTU = %d, vlan_removal_en = %d\\n\",\n\t\t   start.vport_id, edev->ndev->mtu + 0xe, vlan_removal_en);\n\n\tfor_each_queue(i) {\n\t\tstruct qede_fastpath *fp = &edev->fp_array[i];\n\t\tdma_addr_t p_phys_table;\n\t\tu32 page_cnt;\n\n\t\tif (fp->type & QEDE_FASTPATH_RX) {\n\t\t\tstruct qed_rxq_start_ret_params ret_params;\n\t\t\tstruct qede_rx_queue *rxq = fp->rxq;\n\t\t\t__le16 *val;\n\n\t\t\tmemset(&ret_params, 0, sizeof(ret_params));\n\t\t\tmemset(&q_params, 0, sizeof(q_params));\n\t\t\tq_params.queue_id = rxq->rxq_id;\n\t\t\tq_params.vport_id = 0;\n\t\t\tq_params.p_sb = fp->sb_info;\n\t\t\tq_params.sb_idx = RX_PI;\n\n\t\t\tp_phys_table =\n\t\t\t    qed_chain_get_pbl_phys(&rxq->rx_comp_ring);\n\t\t\tpage_cnt = qed_chain_get_page_cnt(&rxq->rx_comp_ring);\n\n\t\t\trc = edev->ops->q_rx_start(cdev, i, &q_params,\n\t\t\t\t\t\t   rxq->rx_buf_size,\n\t\t\t\t\t\t   rxq->rx_bd_ring.p_phys_addr,\n\t\t\t\t\t\t   p_phys_table,\n\t\t\t\t\t\t   page_cnt, &ret_params);\n\t\t\tif (rc) {\n\t\t\t\tDP_ERR(edev, \"Start RXQ #%d failed %d\\n\", i,\n\t\t\t\t       rc);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t \n\t\t\trxq->hw_rxq_prod_addr = ret_params.p_prod;\n\t\t\trxq->handle = ret_params.p_handle;\n\n\t\t\tval = &fp->sb_info->sb_virt->pi_array[RX_PI];\n\t\t\trxq->hw_cons_ptr = val;\n\n\t\t\tqede_update_rx_prod(edev, rxq);\n\t\t}\n\n\t\tif (fp->type & QEDE_FASTPATH_XDP) {\n\t\t\trc = qede_start_txq(edev, fp, fp->xdp_tx, i, XDP_PI);\n\t\t\tif (rc)\n\t\t\t\tgoto out;\n\n\t\t\tbpf_prog_add(edev->xdp_prog, 1);\n\t\t\tfp->rxq->xdp_prog = edev->xdp_prog;\n\t\t}\n\n\t\tif (fp->type & QEDE_FASTPATH_TX) {\n\t\t\tint cos;\n\n\t\t\tfor_each_cos_in_txq(edev, cos) {\n\t\t\t\trc = qede_start_txq(edev, fp, &fp->txq[cos], i,\n\t\t\t\t\t\t    TX_PI(cos));\n\t\t\t\tif (rc)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tvport_update_params->vport_id = start.vport_id;\n\tvport_update_params->update_vport_active_flg = 1;\n\tvport_update_params->vport_active_flg = 1;\n\n\tif ((qed_info->b_inter_pf_switch || pci_num_vf(edev->pdev)) &&\n\t    qed_info->tx_switching) {\n\t\tvport_update_params->update_tx_switching_flg = 1;\n\t\tvport_update_params->tx_switching_flg = 1;\n\t}\n\n\tqede_fill_rss_params(edev, &vport_update_params->rss_params,\n\t\t\t     &vport_update_params->update_rss_flg);\n\n\trc = edev->ops->vport_update(cdev, vport_update_params);\n\tif (rc)\n\t\tDP_ERR(edev, \"Update V-PORT failed %d\\n\", rc);\n\nout:\n\tvfree(vport_update_params);\n\treturn rc;\n}\n\nenum qede_unload_mode {\n\tQEDE_UNLOAD_NORMAL,\n\tQEDE_UNLOAD_RECOVERY,\n};\n\nstatic void qede_unload(struct qede_dev *edev, enum qede_unload_mode mode,\n\t\t\tbool is_locked)\n{\n\tstruct qed_link_params link_params;\n\tint rc;\n\n\tDP_INFO(edev, \"Starting qede unload\\n\");\n\n\tif (!is_locked)\n\t\t__qede_lock(edev);\n\n\tclear_bit(QEDE_FLAGS_LINK_REQUESTED, &edev->flags);\n\n\tif (mode != QEDE_UNLOAD_RECOVERY)\n\t\tedev->state = QEDE_STATE_CLOSED;\n\n\tqede_rdma_dev_event_close(edev);\n\n\t \n\tnetif_tx_disable(edev->ndev);\n\tnetif_carrier_off(edev->ndev);\n\n\tif (mode != QEDE_UNLOAD_RECOVERY) {\n\t\t \n\t\tmemset(&link_params, 0, sizeof(link_params));\n\t\tlink_params.link_up = false;\n\t\tedev->ops->common->set_link(edev->cdev, &link_params);\n\n\t\trc = qede_stop_queues(edev);\n\t\tif (rc) {\n#ifdef CONFIG_RFS_ACCEL\n\t\t\tif (edev->dev_info.common.b_arfs_capable) {\n\t\t\t\tqede_poll_for_freeing_arfs_filters(edev);\n\t\t\t\tif (edev->ndev->rx_cpu_rmap)\n\t\t\t\t\tfree_irq_cpu_rmap(edev->ndev->rx_cpu_rmap);\n\n\t\t\t\tedev->ndev->rx_cpu_rmap = NULL;\n\t\t\t}\n#endif\n\t\t\tqede_sync_free_irqs(edev);\n\t\t\tgoto out;\n\t\t}\n\n\t\tDP_INFO(edev, \"Stopped Queues\\n\");\n\t}\n\n\tqede_vlan_mark_nonconfigured(edev);\n\tedev->ops->fastpath_stop(edev->cdev);\n\n\tif (edev->dev_info.common.b_arfs_capable) {\n\t\tqede_poll_for_freeing_arfs_filters(edev);\n\t\tqede_free_arfs(edev);\n\t}\n\n\t \n\tqede_sync_free_irqs(edev);\n\tedev->ops->common->set_fp_int(edev->cdev, 0);\n\n\tqede_napi_disable_remove(edev);\n\n\tif (mode == QEDE_UNLOAD_RECOVERY)\n\t\tqede_empty_tx_queues(edev);\n\n\tqede_free_mem_load(edev);\n\tqede_free_fp_array(edev);\n\nout:\n\tif (!is_locked)\n\t\t__qede_unlock(edev);\n\n\tif (mode != QEDE_UNLOAD_RECOVERY)\n\t\tDP_NOTICE(edev, \"Link is down\\n\");\n\n\tedev->ptp_skip_txts = 0;\n\n\tDP_INFO(edev, \"Ending qede unload\\n\");\n}\n\nenum qede_load_mode {\n\tQEDE_LOAD_NORMAL,\n\tQEDE_LOAD_RELOAD,\n\tQEDE_LOAD_RECOVERY,\n};\n\nstatic int qede_load(struct qede_dev *edev, enum qede_load_mode mode,\n\t\t     bool is_locked)\n{\n\tstruct qed_link_params link_params;\n\tstruct ethtool_coalesce coal = {};\n\tu8 num_tc;\n\tint rc, i;\n\n\tDP_INFO(edev, \"Starting qede load\\n\");\n\n\tif (!is_locked)\n\t\t__qede_lock(edev);\n\n\trc = qede_set_num_queues(edev);\n\tif (rc)\n\t\tgoto out;\n\n\trc = qede_alloc_fp_array(edev);\n\tif (rc)\n\t\tgoto out;\n\n\tqede_init_fp(edev);\n\n\trc = qede_alloc_mem_load(edev);\n\tif (rc)\n\t\tgoto err1;\n\tDP_INFO(edev, \"Allocated %d Rx, %d Tx queues\\n\",\n\t\tQEDE_RSS_COUNT(edev), QEDE_TSS_COUNT(edev));\n\n\trc = qede_set_real_num_queues(edev);\n\tif (rc)\n\t\tgoto err2;\n\n\tif (qede_alloc_arfs(edev)) {\n\t\tedev->ndev->features &= ~NETIF_F_NTUPLE;\n\t\tedev->dev_info.common.b_arfs_capable = false;\n\t}\n\n\tqede_napi_add_enable(edev);\n\tDP_INFO(edev, \"Napi added and enabled\\n\");\n\n\trc = qede_setup_irqs(edev);\n\tif (rc)\n\t\tgoto err3;\n\tDP_INFO(edev, \"Setup IRQs succeeded\\n\");\n\n\trc = qede_start_queues(edev, mode != QEDE_LOAD_RELOAD);\n\tif (rc)\n\t\tgoto err4;\n\tDP_INFO(edev, \"Start VPORT, RXQ and TXQ succeeded\\n\");\n\n\tnum_tc = netdev_get_num_tc(edev->ndev);\n\tnum_tc = num_tc ? num_tc : edev->dev_info.num_tc;\n\tqede_setup_tc(edev->ndev, num_tc);\n\n\t \n\tqede_configure_vlan_filters(edev);\n\n\tset_bit(QEDE_FLAGS_LINK_REQUESTED, &edev->flags);\n\n\t \n\tmemset(&link_params, 0, sizeof(link_params));\n\tlink_params.link_up = true;\n\tedev->ops->common->set_link(edev->cdev, &link_params);\n\n\tedev->state = QEDE_STATE_OPEN;\n\n\tcoal.rx_coalesce_usecs = QED_DEFAULT_RX_USECS;\n\tcoal.tx_coalesce_usecs = QED_DEFAULT_TX_USECS;\n\n\tfor_each_queue(i) {\n\t\tif (edev->coal_entry[i].isvalid) {\n\t\t\tcoal.rx_coalesce_usecs = edev->coal_entry[i].rxc;\n\t\t\tcoal.tx_coalesce_usecs = edev->coal_entry[i].txc;\n\t\t}\n\t\t__qede_unlock(edev);\n\t\tqede_set_per_coalesce(edev->ndev, i, &coal);\n\t\t__qede_lock(edev);\n\t}\n\tDP_INFO(edev, \"Ending successfully qede load\\n\");\n\n\tgoto out;\nerr4:\n\tqede_sync_free_irqs(edev);\nerr3:\n\tqede_napi_disable_remove(edev);\nerr2:\n\tqede_free_mem_load(edev);\nerr1:\n\tedev->ops->common->set_fp_int(edev->cdev, 0);\n\tqede_free_fp_array(edev);\n\tedev->num_queues = 0;\n\tedev->fp_num_tx = 0;\n\tedev->fp_num_rx = 0;\nout:\n\tif (!is_locked)\n\t\t__qede_unlock(edev);\n\n\treturn rc;\n}\n\n \nvoid qede_reload(struct qede_dev *edev,\n\t\t struct qede_reload_args *args, bool is_locked)\n{\n\tif (!is_locked)\n\t\t__qede_lock(edev);\n\n\t \n\tif (edev->state == QEDE_STATE_OPEN) {\n\t\tqede_unload(edev, QEDE_UNLOAD_NORMAL, true);\n\t\tif (args)\n\t\t\targs->func(edev, args);\n\t\tqede_load(edev, QEDE_LOAD_RELOAD, true);\n\n\t\t \n\t\tqede_config_rx_mode(edev->ndev);\n\t} else if (args) {\n\t\targs->func(edev, args);\n\t}\n\n\tif (!is_locked)\n\t\t__qede_unlock(edev);\n}\n\n \nstatic int qede_open(struct net_device *ndev)\n{\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\tint rc;\n\n\tnetif_carrier_off(ndev);\n\n\tedev->ops->common->set_power_state(edev->cdev, PCI_D0);\n\n\trc = qede_load(edev, QEDE_LOAD_NORMAL, false);\n\tif (rc)\n\t\treturn rc;\n\n\tudp_tunnel_nic_reset_ntf(ndev);\n\n\tedev->ops->common->update_drv_state(edev->cdev, true);\n\n\treturn 0;\n}\n\nstatic int qede_close(struct net_device *ndev)\n{\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\n\tqede_unload(edev, QEDE_UNLOAD_NORMAL, false);\n\n\tif (edev->cdev)\n\t\tedev->ops->common->update_drv_state(edev->cdev, false);\n\n\treturn 0;\n}\n\nstatic void qede_link_update(void *dev, struct qed_link_output *link)\n{\n\tstruct qede_dev *edev = dev;\n\n\tif (!test_bit(QEDE_FLAGS_LINK_REQUESTED, &edev->flags)) {\n\t\tDP_VERBOSE(edev, NETIF_MSG_LINK, \"Interface is not ready\\n\");\n\t\treturn;\n\t}\n\n\tif (link->link_up) {\n\t\tif (!netif_carrier_ok(edev->ndev)) {\n\t\t\tDP_NOTICE(edev, \"Link is up\\n\");\n\t\t\tnetif_tx_start_all_queues(edev->ndev);\n\t\t\tnetif_carrier_on(edev->ndev);\n\t\t\tqede_rdma_dev_event_open(edev);\n\t\t}\n\t} else {\n\t\tif (netif_carrier_ok(edev->ndev)) {\n\t\t\tDP_NOTICE(edev, \"Link is down\\n\");\n\t\t\tnetif_tx_disable(edev->ndev);\n\t\t\tnetif_carrier_off(edev->ndev);\n\t\t\tqede_rdma_dev_event_close(edev);\n\t\t}\n\t}\n}\n\nstatic void qede_schedule_recovery_handler(void *dev)\n{\n\tstruct qede_dev *edev = dev;\n\n\tif (edev->state == QEDE_STATE_RECOVERY) {\n\t\tDP_NOTICE(edev,\n\t\t\t  \"Avoid scheduling a recovery handling since already in recovery state\\n\");\n\t\treturn;\n\t}\n\n\tset_bit(QEDE_SP_RECOVERY, &edev->sp_flags);\n\tschedule_delayed_work(&edev->sp_task, 0);\n\n\tDP_INFO(edev, \"Scheduled a recovery handler\\n\");\n}\n\nstatic void qede_recovery_failed(struct qede_dev *edev)\n{\n\tnetdev_err(edev->ndev, \"Recovery handling has failed. Power cycle is needed.\\n\");\n\n\tnetif_device_detach(edev->ndev);\n\n\tif (edev->cdev)\n\t\tedev->ops->common->set_power_state(edev->cdev, PCI_D3hot);\n}\n\nstatic void qede_recovery_handler(struct qede_dev *edev)\n{\n\tu32 curr_state = edev->state;\n\tint rc;\n\n\tDP_NOTICE(edev, \"Starting a recovery process\\n\");\n\n\t \n\tedev->state = QEDE_STATE_RECOVERY;\n\n\tedev->ops->common->recovery_prolog(edev->cdev);\n\n\tif (curr_state == QEDE_STATE_OPEN)\n\t\tqede_unload(edev, QEDE_UNLOAD_RECOVERY, true);\n\n\t__qede_remove(edev->pdev, QEDE_REMOVE_RECOVERY);\n\n\trc = __qede_probe(edev->pdev, edev->dp_module, edev->dp_level,\n\t\t\t  IS_VF(edev), QEDE_PROBE_RECOVERY);\n\tif (rc) {\n\t\tedev->cdev = NULL;\n\t\tgoto err;\n\t}\n\n\tif (curr_state == QEDE_STATE_OPEN) {\n\t\trc = qede_load(edev, QEDE_LOAD_RECOVERY, true);\n\t\tif (rc)\n\t\t\tgoto err;\n\n\t\tqede_config_rx_mode(edev->ndev);\n\t\tudp_tunnel_nic_reset_ntf(edev->ndev);\n\t}\n\n\tedev->state = curr_state;\n\n\tDP_NOTICE(edev, \"Recovery handling is done\\n\");\n\n\treturn;\n\nerr:\n\tqede_recovery_failed(edev);\n}\n\nstatic void qede_atomic_hw_err_handler(struct qede_dev *edev)\n{\n\tstruct qed_dev *cdev = edev->cdev;\n\n\tDP_NOTICE(edev,\n\t\t  \"Generic non-sleepable HW error handling started - err_flags 0x%lx\\n\",\n\t\t  edev->err_flags);\n\n\t \n\tWARN_ON(test_bit(QEDE_ERR_WARN, &edev->err_flags));\n\n\t \n\tif (test_bit(QEDE_ERR_ATTN_CLR_EN, &edev->err_flags))\n\t\tedev->ops->common->attn_clr_enable(cdev, true);\n\n\tDP_NOTICE(edev, \"Generic non-sleepable HW error handling is done\\n\");\n}\n\nstatic void qede_generic_hw_err_handler(struct qede_dev *edev)\n{\n\tDP_NOTICE(edev,\n\t\t  \"Generic sleepable HW error handling started - err_flags 0x%lx\\n\",\n\t\t  edev->err_flags);\n\n\tif (edev->devlink) {\n\t\tDP_NOTICE(edev, \"Reporting fatal error to devlink\\n\");\n\t\tedev->ops->common->report_fatal_error(edev->devlink, edev->last_err_type);\n\t}\n\n\tclear_bit(QEDE_ERR_IS_HANDLED, &edev->err_flags);\n\n\tDP_NOTICE(edev, \"Generic sleepable HW error handling is done\\n\");\n}\n\nstatic void qede_set_hw_err_flags(struct qede_dev *edev,\n\t\t\t\t  enum qed_hw_err_type err_type)\n{\n\tunsigned long err_flags = 0;\n\n\tswitch (err_type) {\n\tcase QED_HW_ERR_DMAE_FAIL:\n\t\tset_bit(QEDE_ERR_WARN, &err_flags);\n\t\tfallthrough;\n\tcase QED_HW_ERR_MFW_RESP_FAIL:\n\tcase QED_HW_ERR_HW_ATTN:\n\tcase QED_HW_ERR_RAMROD_FAIL:\n\tcase QED_HW_ERR_FW_ASSERT:\n\t\tset_bit(QEDE_ERR_ATTN_CLR_EN, &err_flags);\n\t\tset_bit(QEDE_ERR_GET_DBG_INFO, &err_flags);\n\t\t \n\t\tset_bit(QEDE_ERR_IS_RECOVERABLE, &err_flags);\n\t\tbreak;\n\n\tdefault:\n\t\tDP_NOTICE(edev, \"Unexpected HW error [%d]\\n\", err_type);\n\t\tbreak;\n\t}\n\n\tedev->err_flags |= err_flags;\n}\n\nstatic void qede_schedule_hw_err_handler(void *dev,\n\t\t\t\t\t enum qed_hw_err_type err_type)\n{\n\tstruct qede_dev *edev = dev;\n\n\t \n\tif ((test_and_set_bit(QEDE_ERR_IS_HANDLED, &edev->err_flags) ||\n\t     edev->state == QEDE_STATE_RECOVERY) &&\n\t     err_type != QED_HW_ERR_FAN_FAIL) {\n\t\tDP_INFO(edev,\n\t\t\t\"Avoid scheduling an error handling while another HW error is being handled\\n\");\n\t\treturn;\n\t}\n\n\tif (err_type >= QED_HW_ERR_LAST) {\n\t\tDP_NOTICE(edev, \"Unknown HW error [%d]\\n\", err_type);\n\t\tclear_bit(QEDE_ERR_IS_HANDLED, &edev->err_flags);\n\t\treturn;\n\t}\n\n\tedev->last_err_type = err_type;\n\tqede_set_hw_err_flags(edev, err_type);\n\tqede_atomic_hw_err_handler(edev);\n\tset_bit(QEDE_SP_HW_ERR, &edev->sp_flags);\n\tschedule_delayed_work(&edev->sp_task, 0);\n\n\tDP_INFO(edev, \"Scheduled a error handler [err_type %d]\\n\", err_type);\n}\n\nstatic bool qede_is_txq_full(struct qede_dev *edev, struct qede_tx_queue *txq)\n{\n\tstruct netdev_queue *netdev_txq;\n\n\tnetdev_txq = netdev_get_tx_queue(edev->ndev, txq->ndev_txq_id);\n\tif (netif_xmit_stopped(netdev_txq))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void qede_get_generic_tlv_data(void *dev, struct qed_generic_tlvs *data)\n{\n\tstruct qede_dev *edev = dev;\n\tstruct netdev_hw_addr *ha;\n\tint i;\n\n\tif (edev->ndev->features & NETIF_F_IP_CSUM)\n\t\tdata->feat_flags |= QED_TLV_IP_CSUM;\n\tif (edev->ndev->features & NETIF_F_TSO)\n\t\tdata->feat_flags |= QED_TLV_LSO;\n\n\tether_addr_copy(data->mac[0], edev->ndev->dev_addr);\n\teth_zero_addr(data->mac[1]);\n\teth_zero_addr(data->mac[2]);\n\t \n\tnetif_addr_lock_bh(edev->ndev);\n\ti = 1;\n\tnetdev_for_each_uc_addr(ha, edev->ndev) {\n\t\tether_addr_copy(data->mac[i++], ha->addr);\n\t\tif (i == QED_TLV_MAC_COUNT)\n\t\t\tbreak;\n\t}\n\n\tnetif_addr_unlock_bh(edev->ndev);\n}\n\nstatic void qede_get_eth_tlv_data(void *dev, void *data)\n{\n\tstruct qed_mfw_tlv_eth *etlv = data;\n\tstruct qede_dev *edev = dev;\n\tstruct qede_fastpath *fp;\n\tint i;\n\n\tetlv->lso_maxoff_size = 0XFFFF;\n\tetlv->lso_maxoff_size_set = true;\n\tetlv->lso_minseg_size = (u16)ETH_TX_LSO_WINDOW_MIN_LEN;\n\tetlv->lso_minseg_size_set = true;\n\tetlv->prom_mode = !!(edev->ndev->flags & IFF_PROMISC);\n\tetlv->prom_mode_set = true;\n\tetlv->tx_descr_size = QEDE_TSS_COUNT(edev);\n\tetlv->tx_descr_size_set = true;\n\tetlv->rx_descr_size = QEDE_RSS_COUNT(edev);\n\tetlv->rx_descr_size_set = true;\n\tetlv->iov_offload = QED_MFW_TLV_IOV_OFFLOAD_VEB;\n\tetlv->iov_offload_set = true;\n\n\t \n\tetlv->txqs_empty = true;\n\tetlv->rxqs_empty = true;\n\tetlv->num_txqs_full = 0;\n\tetlv->num_rxqs_full = 0;\n\n\t__qede_lock(edev);\n\tfor_each_queue(i) {\n\t\tfp = &edev->fp_array[i];\n\t\tif (fp->type & QEDE_FASTPATH_TX) {\n\t\t\tstruct qede_tx_queue *txq = QEDE_FP_TC0_TXQ(fp);\n\n\t\t\tif (txq->sw_tx_cons != txq->sw_tx_prod)\n\t\t\t\tetlv->txqs_empty = false;\n\t\t\tif (qede_is_txq_full(edev, txq))\n\t\t\t\tetlv->num_txqs_full++;\n\t\t}\n\t\tif (fp->type & QEDE_FASTPATH_RX) {\n\t\t\tif (qede_has_rx_work(fp->rxq))\n\t\t\t\tetlv->rxqs_empty = false;\n\n\t\t\t \n\t\t\tif (le16_to_cpu(*fp->rxq->hw_cons_ptr) -\n\t\t\t    qed_chain_get_cons_idx(&fp->rxq->rx_comp_ring) >\n\t\t\t    RX_RING_SIZE - 100)\n\t\t\t\tetlv->num_rxqs_full++;\n\t\t}\n\t}\n\t__qede_unlock(edev);\n\n\tetlv->txqs_empty_set = true;\n\tetlv->rxqs_empty_set = true;\n\tetlv->num_txqs_full_set = true;\n\tetlv->num_rxqs_full_set = true;\n}\n\n \nstatic pci_ers_result_t\nqede_io_error_detected(struct pci_dev *pdev, pci_channel_state_t state)\n{\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct qede_dev *edev = netdev_priv(dev);\n\n\tif (!edev)\n\t\treturn PCI_ERS_RESULT_NONE;\n\n\tDP_NOTICE(edev, \"IO error detected [%d]\\n\", state);\n\n\t__qede_lock(edev);\n\tif (edev->state == QEDE_STATE_RECOVERY) {\n\t\tDP_NOTICE(edev, \"Device already in the recovery state\\n\");\n\t\t__qede_unlock(edev);\n\t\treturn PCI_ERS_RESULT_NONE;\n\t}\n\n\t \n\tif (IS_VF(edev)) {\n\t\tDP_VERBOSE(edev, QED_MSG_IOV,\n\t\t\t   \"VF recovery is handled by its PF\\n\");\n\t\t__qede_unlock(edev);\n\t\treturn PCI_ERS_RESULT_RECOVERED;\n\t}\n\n\t \n\tnetif_tx_disable(edev->ndev);\n\tnetif_carrier_off(edev->ndev);\n\n\tset_bit(QEDE_SP_AER, &edev->sp_flags);\n\tschedule_delayed_work(&edev->sp_task, 0);\n\n\t__qede_unlock(edev);\n\n\treturn PCI_ERS_RESULT_CAN_RECOVER;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}