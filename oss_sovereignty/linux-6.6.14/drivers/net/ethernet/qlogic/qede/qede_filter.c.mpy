{
  "module_name": "qede_filter.c",
  "hash_id": "d5174cca438e7358aed8187a3daf792e85e40f8d662d1bd958aee064b6b7d258",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/qlogic/qede/qede_filter.c",
  "human_readable_source": "\n \n\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <net/udp_tunnel.h>\n#include <linux/bitops.h>\n#include <linux/vmalloc.h>\n\n#include <linux/qed/qed_if.h>\n#include \"qede.h\"\n\n#define QEDE_FILTER_PRINT_MAX_LEN\t(64)\nstruct qede_arfs_tuple {\n\tunion {\n\t\t__be32 src_ipv4;\n\t\tstruct in6_addr src_ipv6;\n\t};\n\tunion {\n\t\t__be32 dst_ipv4;\n\t\tstruct in6_addr dst_ipv6;\n\t};\n\t__be16  src_port;\n\t__be16  dst_port;\n\t__be16  eth_proto;\n\tu8      ip_proto;\n\n\t \n\tenum qed_filter_config_mode mode;\n\n\t \n\tbool (*ip_comp)(struct qede_arfs_tuple *a, struct qede_arfs_tuple *b);\n\n\t \n\tvoid (*build_hdr)(struct qede_arfs_tuple *t, void *header);\n\n\t \n\tvoid (*stringify)(struct qede_arfs_tuple *t, void *buffer);\n};\n\nstruct qede_arfs_fltr_node {\n#define QEDE_FLTR_VALID\t 0\n\tunsigned long state;\n\n\t \n\tvoid *data;\n\n\t \n\tdma_addr_t mapping;\n\n\t \n\tint buf_len;\n\n\t \n\tstruct qede_arfs_tuple tuple;\n\n\tu32 flow_id;\n\tu64 sw_id;\n\tu16 rxq_id;\n\tu16 next_rxq_id;\n\tu8 vfid;\n\tbool filter_op;\n\tbool used;\n\tu8 fw_rc;\n\tbool b_is_drop;\n\tstruct hlist_node node;\n};\n\nstruct qede_arfs {\n#define QEDE_ARFS_BUCKET_HEAD(edev, idx) (&(edev)->arfs->arfs_hl_head[idx])\n#define QEDE_ARFS_POLL_COUNT\t100\n#define QEDE_RFS_FLW_BITSHIFT\t(4)\n#define QEDE_RFS_FLW_MASK\t((1 << QEDE_RFS_FLW_BITSHIFT) - 1)\n\tstruct hlist_head\tarfs_hl_head[1 << QEDE_RFS_FLW_BITSHIFT];\n\n\t \n\tspinlock_t\t\tarfs_list_lock;\n\tunsigned long\t\t*arfs_fltr_bmap;\n\tint\t\t\tfilter_count;\n\n\t \n\tenum qed_filter_config_mode mode;\n};\n\nstatic void qede_configure_arfs_fltr(struct qede_dev *edev,\n\t\t\t\t     struct qede_arfs_fltr_node *n,\n\t\t\t\t     u16 rxq_id, bool add_fltr)\n{\n\tconst struct qed_eth_ops *op = edev->ops;\n\tstruct qed_ntuple_filter_params params;\n\n\tif (n->used)\n\t\treturn;\n\n\tmemset(&params, 0, sizeof(params));\n\n\tparams.addr = n->mapping;\n\tparams.length = n->buf_len;\n\tparams.qid = rxq_id;\n\tparams.b_is_add = add_fltr;\n\tparams.b_is_drop = n->b_is_drop;\n\n\tif (n->vfid) {\n\t\tparams.b_is_vf = true;\n\t\tparams.vf_id = n->vfid - 1;\n\t}\n\n\tif (n->tuple.stringify) {\n\t\tchar tuple_buffer[QEDE_FILTER_PRINT_MAX_LEN];\n\n\t\tn->tuple.stringify(&n->tuple, tuple_buffer);\n\t\tDP_VERBOSE(edev, NETIF_MSG_RX_STATUS,\n\t\t\t   \"%s sw_id[0x%llx]: %s [vf %u queue %d]\\n\",\n\t\t\t   add_fltr ? \"Adding\" : \"Deleting\",\n\t\t\t   n->sw_id, tuple_buffer, n->vfid, rxq_id);\n\t}\n\n\tn->used = true;\n\tn->filter_op = add_fltr;\n\top->ntuple_filter_config(edev->cdev, n, &params);\n}\n\nstatic void\nqede_free_arfs_filter(struct qede_dev *edev,  struct qede_arfs_fltr_node *fltr)\n{\n\tkfree(fltr->data);\n\n\tif (fltr->sw_id < QEDE_RFS_MAX_FLTR)\n\t\tclear_bit(fltr->sw_id, edev->arfs->arfs_fltr_bmap);\n\n\tkfree(fltr);\n}\n\nstatic int\nqede_enqueue_fltr_and_config_searcher(struct qede_dev *edev,\n\t\t\t\t      struct qede_arfs_fltr_node *fltr,\n\t\t\t\t      u16 bucket_idx)\n{\n\tfltr->mapping = dma_map_single(&edev->pdev->dev, fltr->data,\n\t\t\t\t       fltr->buf_len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(&edev->pdev->dev, fltr->mapping)) {\n\t\tDP_NOTICE(edev, \"Failed to map DMA memory for rule\\n\");\n\t\tqede_free_arfs_filter(edev, fltr);\n\t\treturn -ENOMEM;\n\t}\n\n\tINIT_HLIST_NODE(&fltr->node);\n\thlist_add_head(&fltr->node,\n\t\t       QEDE_ARFS_BUCKET_HEAD(edev, bucket_idx));\n\n\tedev->arfs->filter_count++;\n\tif (edev->arfs->filter_count == 1 &&\n\t    edev->arfs->mode == QED_FILTER_CONFIG_MODE_DISABLE) {\n\t\tedev->ops->configure_arfs_searcher(edev->cdev,\n\t\t\t\t\t\t   fltr->tuple.mode);\n\t\tedev->arfs->mode = fltr->tuple.mode;\n\t}\n\n\treturn 0;\n}\n\nstatic void\nqede_dequeue_fltr_and_config_searcher(struct qede_dev *edev,\n\t\t\t\t      struct qede_arfs_fltr_node *fltr)\n{\n\thlist_del(&fltr->node);\n\tdma_unmap_single(&edev->pdev->dev, fltr->mapping,\n\t\t\t fltr->buf_len, DMA_TO_DEVICE);\n\n\tqede_free_arfs_filter(edev, fltr);\n\n\tedev->arfs->filter_count--;\n\tif (!edev->arfs->filter_count &&\n\t    edev->arfs->mode != QED_FILTER_CONFIG_MODE_DISABLE) {\n\t\tenum qed_filter_config_mode mode;\n\n\t\tmode = QED_FILTER_CONFIG_MODE_DISABLE;\n\t\tedev->ops->configure_arfs_searcher(edev->cdev, mode);\n\t\tedev->arfs->mode = QED_FILTER_CONFIG_MODE_DISABLE;\n\t}\n}\n\nvoid qede_arfs_filter_op(void *dev, void *filter, u8 fw_rc)\n{\n\tstruct qede_arfs_fltr_node *fltr = filter;\n\tstruct qede_dev *edev = dev;\n\n\tfltr->fw_rc = fw_rc;\n\n\tif (fw_rc) {\n\t\tDP_NOTICE(edev,\n\t\t\t  \"Failed arfs filter configuration fw_rc=%d, flow_id=%d, sw_id=0x%llx, src_port=%d, dst_port=%d, rxq=%d\\n\",\n\t\t\t  fw_rc, fltr->flow_id, fltr->sw_id,\n\t\t\t  ntohs(fltr->tuple.src_port),\n\t\t\t  ntohs(fltr->tuple.dst_port), fltr->rxq_id);\n\n\t\tspin_lock_bh(&edev->arfs->arfs_list_lock);\n\n\t\tfltr->used = false;\n\t\tclear_bit(QEDE_FLTR_VALID, &fltr->state);\n\n\t\tspin_unlock_bh(&edev->arfs->arfs_list_lock);\n\t\treturn;\n\t}\n\n\tspin_lock_bh(&edev->arfs->arfs_list_lock);\n\n\tfltr->used = false;\n\n\tif (fltr->filter_op) {\n\t\tset_bit(QEDE_FLTR_VALID, &fltr->state);\n\t\tif (fltr->rxq_id != fltr->next_rxq_id)\n\t\t\tqede_configure_arfs_fltr(edev, fltr, fltr->rxq_id,\n\t\t\t\t\t\t false);\n\t} else {\n\t\tclear_bit(QEDE_FLTR_VALID, &fltr->state);\n\t\tif (fltr->rxq_id != fltr->next_rxq_id) {\n\t\t\tfltr->rxq_id = fltr->next_rxq_id;\n\t\t\tqede_configure_arfs_fltr(edev, fltr,\n\t\t\t\t\t\t fltr->rxq_id, true);\n\t\t}\n\t}\n\n\tspin_unlock_bh(&edev->arfs->arfs_list_lock);\n}\n\n \nvoid qede_process_arfs_filters(struct qede_dev *edev, bool free_fltr)\n{\n\tint i;\n\n\tfor (i = 0; i <= QEDE_RFS_FLW_MASK; i++) {\n\t\tstruct hlist_node *temp;\n\t\tstruct hlist_head *head;\n\t\tstruct qede_arfs_fltr_node *fltr;\n\n\t\thead = &edev->arfs->arfs_hl_head[i];\n\n\t\thlist_for_each_entry_safe(fltr, temp, head, node) {\n\t\t\tbool del = false;\n\n\t\t\tif (edev->state != QEDE_STATE_OPEN)\n\t\t\t\tdel = true;\n\n\t\t\tspin_lock_bh(&edev->arfs->arfs_list_lock);\n\n\t\t\tif ((!test_bit(QEDE_FLTR_VALID, &fltr->state) &&\n\t\t\t     !fltr->used) || free_fltr) {\n\t\t\t\tqede_dequeue_fltr_and_config_searcher(edev,\n\t\t\t\t\t\t\t\t      fltr);\n\t\t\t} else {\n\t\t\t\tbool flow_exp = false;\n#ifdef CONFIG_RFS_ACCEL\n\t\t\t\tflow_exp = rps_may_expire_flow(edev->ndev,\n\t\t\t\t\t\t\t       fltr->rxq_id,\n\t\t\t\t\t\t\t       fltr->flow_id,\n\t\t\t\t\t\t\t       fltr->sw_id);\n#endif\n\t\t\t\tif ((flow_exp || del) && !free_fltr)\n\t\t\t\t\tqede_configure_arfs_fltr(edev, fltr,\n\t\t\t\t\t\t\t\t fltr->rxq_id,\n\t\t\t\t\t\t\t\t false);\n\t\t\t}\n\n\t\t\tspin_unlock_bh(&edev->arfs->arfs_list_lock);\n\t\t}\n\t}\n\n#ifdef CONFIG_RFS_ACCEL\n\tspin_lock_bh(&edev->arfs->arfs_list_lock);\n\n\tif (edev->arfs->filter_count) {\n\t\tset_bit(QEDE_SP_ARFS_CONFIG, &edev->sp_flags);\n\t\tschedule_delayed_work(&edev->sp_task,\n\t\t\t\t      QEDE_SP_TASK_POLL_DELAY);\n\t}\n\n\tspin_unlock_bh(&edev->arfs->arfs_list_lock);\n#endif\n}\n\n \nvoid qede_poll_for_freeing_arfs_filters(struct qede_dev *edev)\n{\n\tint count = QEDE_ARFS_POLL_COUNT;\n\n\twhile (count) {\n\t\tqede_process_arfs_filters(edev, false);\n\n\t\tif (!edev->arfs->filter_count)\n\t\t\tbreak;\n\n\t\tmsleep(100);\n\t\tcount--;\n\t}\n\n\tif (!count) {\n\t\tDP_NOTICE(edev, \"Timeout in polling for arfs filter free\\n\");\n\n\t\t \n\t\tqede_process_arfs_filters(edev, true);\n\t}\n}\n\nint qede_alloc_arfs(struct qede_dev *edev)\n{\n\tint i;\n\n\tif (!edev->dev_info.common.b_arfs_capable)\n\t\treturn -EINVAL;\n\n\tedev->arfs = vzalloc(sizeof(*edev->arfs));\n\tif (!edev->arfs)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&edev->arfs->arfs_list_lock);\n\n\tfor (i = 0; i <= QEDE_RFS_FLW_MASK; i++)\n\t\tINIT_HLIST_HEAD(QEDE_ARFS_BUCKET_HEAD(edev, i));\n\n\tedev->arfs->arfs_fltr_bmap =\n\t\tvzalloc(array_size(sizeof(long),\n\t\t\t\t   BITS_TO_LONGS(QEDE_RFS_MAX_FLTR)));\n\tif (!edev->arfs->arfs_fltr_bmap) {\n\t\tvfree(edev->arfs);\n\t\tedev->arfs = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n#ifdef CONFIG_RFS_ACCEL\n\tedev->ndev->rx_cpu_rmap = alloc_irq_cpu_rmap(QEDE_RSS_COUNT(edev));\n\tif (!edev->ndev->rx_cpu_rmap) {\n\t\tvfree(edev->arfs->arfs_fltr_bmap);\n\t\tedev->arfs->arfs_fltr_bmap = NULL;\n\t\tvfree(edev->arfs);\n\t\tedev->arfs = NULL;\n\t\treturn -ENOMEM;\n\t}\n#endif\n\treturn 0;\n}\n\nvoid qede_free_arfs(struct qede_dev *edev)\n{\n\tif (!edev->arfs)\n\t\treturn;\n\n#ifdef CONFIG_RFS_ACCEL\n\tif (edev->ndev->rx_cpu_rmap)\n\t\tfree_irq_cpu_rmap(edev->ndev->rx_cpu_rmap);\n\n\tedev->ndev->rx_cpu_rmap = NULL;\n#endif\n\tvfree(edev->arfs->arfs_fltr_bmap);\n\tedev->arfs->arfs_fltr_bmap = NULL;\n\tvfree(edev->arfs);\n\tedev->arfs = NULL;\n}\n\n#ifdef CONFIG_RFS_ACCEL\nstatic bool qede_compare_ip_addr(struct qede_arfs_fltr_node *tpos,\n\t\t\t\t const struct sk_buff *skb)\n{\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\tif (tpos->tuple.src_ipv4 == ip_hdr(skb)->saddr &&\n\t\t    tpos->tuple.dst_ipv4 == ip_hdr(skb)->daddr)\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\t} else {\n\t\tstruct in6_addr *src = &tpos->tuple.src_ipv6;\n\t\tu8 size = sizeof(struct in6_addr);\n\n\t\tif (!memcmp(src, &ipv6_hdr(skb)->saddr, size) &&\n\t\t    !memcmp(&tpos->tuple.dst_ipv6, &ipv6_hdr(skb)->daddr, size))\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\t}\n}\n\nstatic struct qede_arfs_fltr_node *\nqede_arfs_htbl_key_search(struct hlist_head *h, const struct sk_buff *skb,\n\t\t\t  __be16 src_port, __be16 dst_port, u8 ip_proto)\n{\n\tstruct qede_arfs_fltr_node *tpos;\n\n\thlist_for_each_entry(tpos, h, node)\n\t\tif (tpos->tuple.ip_proto == ip_proto &&\n\t\t    tpos->tuple.eth_proto == skb->protocol &&\n\t\t    qede_compare_ip_addr(tpos, skb) &&\n\t\t    tpos->tuple.src_port == src_port &&\n\t\t    tpos->tuple.dst_port == dst_port)\n\t\t\treturn tpos;\n\n\treturn NULL;\n}\n\nstatic struct qede_arfs_fltr_node *\nqede_alloc_filter(struct qede_dev *edev, int min_hlen)\n{\n\tstruct qede_arfs_fltr_node *n;\n\tint bit_id;\n\n\tbit_id = find_first_zero_bit(edev->arfs->arfs_fltr_bmap,\n\t\t\t\t     QEDE_RFS_MAX_FLTR);\n\n\tif (bit_id >= QEDE_RFS_MAX_FLTR)\n\t\treturn NULL;\n\n\tn = kzalloc(sizeof(*n), GFP_ATOMIC);\n\tif (!n)\n\t\treturn NULL;\n\n\tn->data = kzalloc(min_hlen, GFP_ATOMIC);\n\tif (!n->data) {\n\t\tkfree(n);\n\t\treturn NULL;\n\t}\n\n\tn->sw_id = (u16)bit_id;\n\tset_bit(bit_id, edev->arfs->arfs_fltr_bmap);\n\treturn n;\n}\n\nint qede_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,\n\t\t       u16 rxq_index, u32 flow_id)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tstruct qede_arfs_fltr_node *n;\n\tint min_hlen, rc, tp_offset;\n\tstruct ethhdr *eth;\n\t__be16 *ports;\n\tu16 tbl_idx;\n\tu8 ip_proto;\n\n\tif (skb->encapsulation)\n\t\treturn -EPROTONOSUPPORT;\n\n\tif (skb->protocol != htons(ETH_P_IP) &&\n\t    skb->protocol != htons(ETH_P_IPV6))\n\t\treturn -EPROTONOSUPPORT;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\tip_proto = ip_hdr(skb)->protocol;\n\t\ttp_offset = sizeof(struct iphdr);\n\t} else {\n\t\tip_proto = ipv6_hdr(skb)->nexthdr;\n\t\ttp_offset = sizeof(struct ipv6hdr);\n\t}\n\n\tif (ip_proto != IPPROTO_TCP && ip_proto != IPPROTO_UDP)\n\t\treturn -EPROTONOSUPPORT;\n\n\tports = (__be16 *)(skb->data + tp_offset);\n\ttbl_idx = skb_get_hash_raw(skb) & QEDE_RFS_FLW_MASK;\n\n\tspin_lock_bh(&edev->arfs->arfs_list_lock);\n\n\tn = qede_arfs_htbl_key_search(QEDE_ARFS_BUCKET_HEAD(edev, tbl_idx),\n\t\t\t\t      skb, ports[0], ports[1], ip_proto);\n\tif (n) {\n\t\t \n\t\tn->next_rxq_id = rxq_index;\n\n\t\tif (test_bit(QEDE_FLTR_VALID, &n->state)) {\n\t\t\tif (n->rxq_id != rxq_index)\n\t\t\t\tqede_configure_arfs_fltr(edev, n, n->rxq_id,\n\t\t\t\t\t\t\t false);\n\t\t} else {\n\t\t\tif (!n->used) {\n\t\t\t\tn->rxq_id = rxq_index;\n\t\t\t\tqede_configure_arfs_fltr(edev, n, n->rxq_id,\n\t\t\t\t\t\t\t true);\n\t\t\t}\n\t\t}\n\n\t\trc = n->sw_id;\n\t\tgoto ret_unlock;\n\t}\n\n\tmin_hlen = ETH_HLEN + skb_headlen(skb);\n\n\tn = qede_alloc_filter(edev, min_hlen);\n\tif (!n) {\n\t\trc = -ENOMEM;\n\t\tgoto ret_unlock;\n\t}\n\n\tn->buf_len = min_hlen;\n\tn->rxq_id = rxq_index;\n\tn->next_rxq_id = rxq_index;\n\tn->tuple.src_port = ports[0];\n\tn->tuple.dst_port = ports[1];\n\tn->flow_id = flow_id;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\tn->tuple.src_ipv4 = ip_hdr(skb)->saddr;\n\t\tn->tuple.dst_ipv4 = ip_hdr(skb)->daddr;\n\t} else {\n\t\tmemcpy(&n->tuple.src_ipv6, &ipv6_hdr(skb)->saddr,\n\t\t       sizeof(struct in6_addr));\n\t\tmemcpy(&n->tuple.dst_ipv6, &ipv6_hdr(skb)->daddr,\n\t\t       sizeof(struct in6_addr));\n\t}\n\n\teth = (struct ethhdr *)n->data;\n\teth->h_proto = skb->protocol;\n\tn->tuple.eth_proto = skb->protocol;\n\tn->tuple.ip_proto = ip_proto;\n\tn->tuple.mode = QED_FILTER_CONFIG_MODE_5_TUPLE;\n\tmemcpy(n->data + ETH_HLEN, skb->data, skb_headlen(skb));\n\n\trc = qede_enqueue_fltr_and_config_searcher(edev, n, tbl_idx);\n\tif (rc)\n\t\tgoto ret_unlock;\n\n\tqede_configure_arfs_fltr(edev, n, n->rxq_id, true);\n\n\tspin_unlock_bh(&edev->arfs->arfs_list_lock);\n\n\tset_bit(QEDE_SP_ARFS_CONFIG, &edev->sp_flags);\n\tschedule_delayed_work(&edev->sp_task, 0);\n\n\treturn n->sw_id;\n\nret_unlock:\n\tspin_unlock_bh(&edev->arfs->arfs_list_lock);\n\treturn rc;\n}\n#endif\n\nvoid qede_udp_ports_update(void *dev, u16 vxlan_port, u16 geneve_port)\n{\n\tstruct qede_dev *edev = dev;\n\n\tif (edev->vxlan_dst_port != vxlan_port)\n\t\tedev->vxlan_dst_port = 0;\n\n\tif (edev->geneve_dst_port != geneve_port)\n\t\tedev->geneve_dst_port = 0;\n}\n\nvoid qede_force_mac(void *dev, u8 *mac, bool forced)\n{\n\tstruct qede_dev *edev = dev;\n\n\t__qede_lock(edev);\n\n\tif (!is_valid_ether_addr(mac)) {\n\t\t__qede_unlock(edev);\n\t\treturn;\n\t}\n\n\teth_hw_addr_set(edev->ndev, mac);\n\t__qede_unlock(edev);\n}\n\nvoid qede_fill_rss_params(struct qede_dev *edev,\n\t\t\t  struct qed_update_vport_rss_params *rss, u8 *update)\n{\n\tbool need_reset = false;\n\tint i;\n\n\tif (QEDE_RSS_COUNT(edev) <= 1) {\n\t\tmemset(rss, 0, sizeof(*rss));\n\t\t*update = 0;\n\t\treturn;\n\t}\n\n\t \n\tfor (i = 0; i < QED_RSS_IND_TABLE_SIZE; i++) {\n\t\tif (edev->rss_ind_table[i] >= QEDE_RSS_COUNT(edev)) {\n\t\t\tneed_reset = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!(edev->rss_params_inited & QEDE_RSS_INDIR_INITED) || need_reset) {\n\t\tfor (i = 0; i < QED_RSS_IND_TABLE_SIZE; i++) {\n\t\t\tu16 indir_val, val;\n\n\t\t\tval = QEDE_RSS_COUNT(edev);\n\t\t\tindir_val = ethtool_rxfh_indir_default(i, val);\n\t\t\tedev->rss_ind_table[i] = indir_val;\n\t\t}\n\t\tedev->rss_params_inited |= QEDE_RSS_INDIR_INITED;\n\t}\n\n\t \n\tfor (i = 0; i < QED_RSS_IND_TABLE_SIZE; i++) {\n\t\tu16 idx = QEDE_RX_QUEUE_IDX(edev, edev->rss_ind_table[i]);\n\n\t\trss->rss_ind_table[i] = edev->fp_array[idx].rxq->handle;\n\t}\n\n\tif (!(edev->rss_params_inited & QEDE_RSS_KEY_INITED)) {\n\t\tnetdev_rss_key_fill(edev->rss_key, sizeof(edev->rss_key));\n\t\tedev->rss_params_inited |= QEDE_RSS_KEY_INITED;\n\t}\n\tmemcpy(rss->rss_key, edev->rss_key, sizeof(rss->rss_key));\n\n\tif (!(edev->rss_params_inited & QEDE_RSS_CAPS_INITED)) {\n\t\tedev->rss_caps = QED_RSS_IPV4 | QED_RSS_IPV6 |\n\t\t    QED_RSS_IPV4_TCP | QED_RSS_IPV6_TCP;\n\t\tedev->rss_params_inited |= QEDE_RSS_CAPS_INITED;\n\t}\n\trss->rss_caps = edev->rss_caps;\n\n\t*update = 1;\n}\n\nstatic int qede_set_ucast_rx_mac(struct qede_dev *edev,\n\t\t\t\t enum qed_filter_xcast_params_type opcode,\n\t\t\t\t const unsigned char mac[ETH_ALEN])\n{\n\tstruct qed_filter_ucast_params ucast;\n\n\tmemset(&ucast, 0, sizeof(ucast));\n\tucast.type = opcode;\n\tucast.mac_valid = 1;\n\tether_addr_copy(ucast.mac, mac);\n\n\treturn edev->ops->filter_config_ucast(edev->cdev, &ucast);\n}\n\nstatic int qede_set_ucast_rx_vlan(struct qede_dev *edev,\n\t\t\t\t  enum qed_filter_xcast_params_type opcode,\n\t\t\t\t  u16 vid)\n{\n\tstruct qed_filter_ucast_params ucast;\n\n\tmemset(&ucast, 0, sizeof(ucast));\n\tucast.type = opcode;\n\tucast.vlan_valid = 1;\n\tucast.vlan = vid;\n\n\treturn edev->ops->filter_config_ucast(edev->cdev, &ucast);\n}\n\nstatic int qede_config_accept_any_vlan(struct qede_dev *edev, bool action)\n{\n\tstruct qed_update_vport_params *params;\n\tint rc;\n\n\t \n\tif (edev->accept_any_vlan == action)\n\t\treturn 0;\n\n\tparams = vzalloc(sizeof(*params));\n\tif (!params)\n\t\treturn -ENOMEM;\n\n\tparams->vport_id = 0;\n\tparams->accept_any_vlan = action;\n\tparams->update_accept_any_vlan_flg = 1;\n\n\trc = edev->ops->vport_update(edev->cdev, params);\n\tif (rc) {\n\t\tDP_ERR(edev, \"Failed to %s accept-any-vlan\\n\",\n\t\t       action ? \"enable\" : \"disable\");\n\t} else {\n\t\tDP_INFO(edev, \"%s accept-any-vlan\\n\",\n\t\t\taction ? \"enabled\" : \"disabled\");\n\t\tedev->accept_any_vlan = action;\n\t}\n\n\tvfree(params);\n\treturn 0;\n}\n\nint qede_vlan_rx_add_vid(struct net_device *dev, __be16 proto, u16 vid)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tstruct qede_vlan *vlan, *tmp;\n\tint rc = 0;\n\n\tDP_VERBOSE(edev, NETIF_MSG_IFUP, \"Adding vlan 0x%04x\\n\", vid);\n\n\tvlan = kzalloc(sizeof(*vlan), GFP_KERNEL);\n\tif (!vlan) {\n\t\tDP_INFO(edev, \"Failed to allocate struct for vlan\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tINIT_LIST_HEAD(&vlan->list);\n\tvlan->vid = vid;\n\tvlan->configured = false;\n\n\t \n\tlist_for_each_entry(tmp, &edev->vlan_list, list) {\n\t\tif (tmp->vid == vlan->vid) {\n\t\t\tDP_VERBOSE(edev, (NETIF_MSG_IFUP | NETIF_MSG_IFDOWN),\n\t\t\t\t   \"vlan already configured\\n\");\n\t\t\tkfree(vlan);\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\t \n\t__qede_lock(edev);\n\tif (edev->state != QEDE_STATE_OPEN) {\n\t\tDP_VERBOSE(edev, NETIF_MSG_IFDOWN,\n\t\t\t   \"Interface is down, VLAN %d will be configured when interface is up\\n\",\n\t\t\t   vid);\n\t\tif (vid != 0)\n\t\t\tedev->non_configured_vlans++;\n\t\tlist_add(&vlan->list, &edev->vlan_list);\n\t\tgoto out;\n\t}\n\n\t \n\tif ((edev->configured_vlans < edev->dev_info.num_vlan_filters) ||\n\t    (vlan->vid == 0)) {\n\t\trc = qede_set_ucast_rx_vlan(edev,\n\t\t\t\t\t    QED_FILTER_XCAST_TYPE_ADD,\n\t\t\t\t\t    vlan->vid);\n\t\tif (rc) {\n\t\t\tDP_ERR(edev, \"Failed to configure VLAN %d\\n\",\n\t\t\t       vlan->vid);\n\t\t\tkfree(vlan);\n\t\t\tgoto out;\n\t\t}\n\t\tvlan->configured = true;\n\n\t\t \n\t\tif (vlan->vid != 0)\n\t\t\tedev->configured_vlans++;\n\t} else {\n\t\t \n\t\tif (!edev->non_configured_vlans) {\n\t\t\trc = qede_config_accept_any_vlan(edev, true);\n\t\t\tif (rc) {\n\t\t\t\tkfree(vlan);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tedev->non_configured_vlans++;\n\t}\n\n\tlist_add(&vlan->list, &edev->vlan_list);\n\nout:\n\t__qede_unlock(edev);\n\treturn rc;\n}\n\nstatic void qede_del_vlan_from_list(struct qede_dev *edev,\n\t\t\t\t    struct qede_vlan *vlan)\n{\n\t \n\tif (vlan->vid != 0) {\n\t\tif (vlan->configured)\n\t\t\tedev->configured_vlans--;\n\t\telse\n\t\t\tedev->non_configured_vlans--;\n\t}\n\n\tlist_del(&vlan->list);\n\tkfree(vlan);\n}\n\nint qede_configure_vlan_filters(struct qede_dev *edev)\n{\n\tint rc = 0, real_rc = 0, accept_any_vlan = 0;\n\tstruct qed_dev_eth_info *dev_info;\n\tstruct qede_vlan *vlan = NULL;\n\n\tif (list_empty(&edev->vlan_list))\n\t\treturn 0;\n\n\tdev_info = &edev->dev_info;\n\n\t \n\tlist_for_each_entry(vlan, &edev->vlan_list, list) {\n\t\tif (vlan->configured)\n\t\t\tcontinue;\n\n\t\t \n\t\tif ((vlan->vid != 0) &&\n\t\t    (edev->configured_vlans == dev_info->num_vlan_filters)) {\n\t\t\taccept_any_vlan = 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tDP_VERBOSE(edev, NETIF_MSG_IFUP, \"Adding vlan %d\\n\", vlan->vid);\n\n\t\trc = qede_set_ucast_rx_vlan(edev, QED_FILTER_XCAST_TYPE_ADD,\n\t\t\t\t\t    vlan->vid);\n\t\tif (rc) {\n\t\t\tDP_ERR(edev, \"Failed to configure VLAN %u\\n\",\n\t\t\t       vlan->vid);\n\t\t\treal_rc = rc;\n\t\t\tcontinue;\n\t\t}\n\n\t\tvlan->configured = true;\n\t\t \n\t\tif (vlan->vid != 0) {\n\t\t\tedev->non_configured_vlans--;\n\t\t\tedev->configured_vlans++;\n\t\t}\n\t}\n\n\t \n\n\tif (accept_any_vlan)\n\t\trc = qede_config_accept_any_vlan(edev, true);\n\telse if (!edev->non_configured_vlans)\n\t\trc = qede_config_accept_any_vlan(edev, false);\n\n\tif (rc && !real_rc)\n\t\treal_rc = rc;\n\n\treturn real_rc;\n}\n\nint qede_vlan_rx_kill_vid(struct net_device *dev, __be16 proto, u16 vid)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tstruct qede_vlan *vlan;\n\tint rc = 0;\n\n\tDP_VERBOSE(edev, NETIF_MSG_IFDOWN, \"Removing vlan 0x%04x\\n\", vid);\n\n\t \n\t__qede_lock(edev);\n\tlist_for_each_entry(vlan, &edev->vlan_list, list)\n\t\tif (vlan->vid == vid)\n\t\t\tbreak;\n\n\tif (list_entry_is_head(vlan, &edev->vlan_list, list)) {\n\t\tDP_VERBOSE(edev, (NETIF_MSG_IFUP | NETIF_MSG_IFDOWN),\n\t\t\t   \"Vlan isn't configured\\n\");\n\t\tgoto out;\n\t}\n\n\tif (edev->state != QEDE_STATE_OPEN) {\n\t\t \n\t\tDP_VERBOSE(edev, NETIF_MSG_IFDOWN,\n\t\t\t   \"Interface is down, removing VLAN from list only\\n\");\n\t\tqede_del_vlan_from_list(edev, vlan);\n\t\tgoto out;\n\t}\n\n\t \n\tif (vlan->configured) {\n\t\trc = qede_set_ucast_rx_vlan(edev, QED_FILTER_XCAST_TYPE_DEL,\n\t\t\t\t\t    vid);\n\t\tif (rc) {\n\t\t\tDP_ERR(edev, \"Failed to remove VLAN %d\\n\", vid);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tqede_del_vlan_from_list(edev, vlan);\n\n\t \n\trc = qede_configure_vlan_filters(edev);\n\nout:\n\t__qede_unlock(edev);\n\treturn rc;\n}\n\nvoid qede_vlan_mark_nonconfigured(struct qede_dev *edev)\n{\n\tstruct qede_vlan *vlan = NULL;\n\n\tif (list_empty(&edev->vlan_list))\n\t\treturn;\n\n\tlist_for_each_entry(vlan, &edev->vlan_list, list) {\n\t\tif (!vlan->configured)\n\t\t\tcontinue;\n\n\t\tvlan->configured = false;\n\n\t\t \n\t\tif (vlan->vid != 0) {\n\t\t\tedev->non_configured_vlans++;\n\t\t\tedev->configured_vlans--;\n\t\t}\n\n\t\tDP_VERBOSE(edev, NETIF_MSG_IFDOWN,\n\t\t\t   \"marked vlan %d as non-configured\\n\", vlan->vid);\n\t}\n\n\tedev->accept_any_vlan = false;\n}\n\nstatic void qede_set_features_reload(struct qede_dev *edev,\n\t\t\t\t     struct qede_reload_args *args)\n{\n\tedev->ndev->features = args->u.features;\n}\n\nnetdev_features_t qede_fix_features(struct net_device *dev,\n\t\t\t\t    netdev_features_t features)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\n\tif (edev->xdp_prog || edev->ndev->mtu > PAGE_SIZE ||\n\t    !(features & NETIF_F_GRO))\n\t\tfeatures &= ~NETIF_F_GRO_HW;\n\n\treturn features;\n}\n\nint qede_set_features(struct net_device *dev, netdev_features_t features)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tnetdev_features_t changes = features ^ dev->features;\n\tbool need_reload = false;\n\n\tif (changes & NETIF_F_GRO_HW)\n\t\tneed_reload = true;\n\n\tif (need_reload) {\n\t\tstruct qede_reload_args args;\n\n\t\targs.u.features = features;\n\t\targs.func = &qede_set_features_reload;\n\n\t\t \n\t\t__qede_lock(edev);\n\t\tif (edev->xdp_prog)\n\t\t\targs.func(edev, &args);\n\t\telse\n\t\t\tqede_reload(edev, &args, true);\n\t\t__qede_unlock(edev);\n\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int qede_udp_tunnel_sync(struct net_device *dev, unsigned int table)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\tstruct qed_tunn_params tunn_params;\n\tstruct udp_tunnel_info ti;\n\tu16 *save_port;\n\tint rc;\n\n\tmemset(&tunn_params, 0, sizeof(tunn_params));\n\n\tudp_tunnel_nic_get_port(dev, table, 0, &ti);\n\tif (ti.type == UDP_TUNNEL_TYPE_VXLAN) {\n\t\ttunn_params.update_vxlan_port = 1;\n\t\ttunn_params.vxlan_port = ntohs(ti.port);\n\t\tsave_port = &edev->vxlan_dst_port;\n\t} else {\n\t\ttunn_params.update_geneve_port = 1;\n\t\ttunn_params.geneve_port = ntohs(ti.port);\n\t\tsave_port = &edev->geneve_dst_port;\n\t}\n\n\t__qede_lock(edev);\n\trc = edev->ops->tunn_config(edev->cdev, &tunn_params);\n\t__qede_unlock(edev);\n\tif (rc)\n\t\treturn rc;\n\n\t*save_port = ntohs(ti.port);\n\treturn 0;\n}\n\nstatic const struct udp_tunnel_nic_info qede_udp_tunnels_both = {\n\t.sync_table\t= qede_udp_tunnel_sync,\n\t.flags\t\t= UDP_TUNNEL_NIC_INFO_MAY_SLEEP,\n\t.tables\t\t= {\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_VXLAN,  },\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_GENEVE, },\n\t},\n}, qede_udp_tunnels_vxlan = {\n\t.sync_table\t= qede_udp_tunnel_sync,\n\t.flags\t\t= UDP_TUNNEL_NIC_INFO_MAY_SLEEP,\n\t.tables\t\t= {\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_VXLAN,  },\n\t},\n}, qede_udp_tunnels_geneve = {\n\t.sync_table\t= qede_udp_tunnel_sync,\n\t.flags\t\t= UDP_TUNNEL_NIC_INFO_MAY_SLEEP,\n\t.tables\t\t= {\n\t\t{ .n_entries = 1, .tunnel_types = UDP_TUNNEL_TYPE_GENEVE, },\n\t},\n};\n\nvoid qede_set_udp_tunnels(struct qede_dev *edev)\n{\n\tif (edev->dev_info.common.vxlan_enable &&\n\t    edev->dev_info.common.geneve_enable)\n\t\tedev->ndev->udp_tunnel_nic_info = &qede_udp_tunnels_both;\n\telse if (edev->dev_info.common.vxlan_enable)\n\t\tedev->ndev->udp_tunnel_nic_info = &qede_udp_tunnels_vxlan;\n\telse if (edev->dev_info.common.geneve_enable)\n\t\tedev->ndev->udp_tunnel_nic_info = &qede_udp_tunnels_geneve;\n}\n\nstatic void qede_xdp_reload_func(struct qede_dev *edev,\n\t\t\t\t struct qede_reload_args *args)\n{\n\tstruct bpf_prog *old;\n\n\told = xchg(&edev->xdp_prog, args->u.new_prog);\n\tif (old)\n\t\tbpf_prog_put(old);\n}\n\nstatic int qede_xdp_set(struct qede_dev *edev, struct bpf_prog *prog)\n{\n\tstruct qede_reload_args args;\n\n\t \n\targs.func = &qede_xdp_reload_func;\n\targs.u.new_prog = prog;\n\tqede_reload(edev, &args, false);\n\n\treturn 0;\n}\n\nint qede_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct qede_dev *edev = netdev_priv(dev);\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn qede_xdp_set(edev, xdp->prog);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int qede_set_mcast_rx_mac(struct qede_dev *edev,\n\t\t\t\t enum qed_filter_xcast_params_type opcode,\n\t\t\t\t unsigned char *mac, int num_macs)\n{\n\tstruct qed_filter_mcast_params mcast;\n\tint i;\n\n\tmemset(&mcast, 0, sizeof(mcast));\n\tmcast.type = opcode;\n\tmcast.num = num_macs;\n\n\tfor (i = 0; i < num_macs; i++, mac += ETH_ALEN)\n\t\tether_addr_copy(mcast.mac[i], mac);\n\n\treturn edev->ops->filter_config_mcast(edev->cdev, &mcast);\n}\n\nint qede_set_mac_addr(struct net_device *ndev, void *p)\n{\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\tstruct sockaddr *addr = p;\n\tint rc = 0;\n\n\t \n\t__qede_lock(edev);\n\n\tif (!is_valid_ether_addr(addr->sa_data)) {\n\t\tDP_NOTICE(edev, \"The MAC address is not valid\\n\");\n\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (!edev->ops->check_mac(edev->cdev, addr->sa_data)) {\n\t\tDP_NOTICE(edev, \"qed prevents setting MAC %pM\\n\",\n\t\t\t  addr->sa_data);\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (edev->state == QEDE_STATE_OPEN) {\n\t\t \n\t\trc = qede_set_ucast_rx_mac(edev, QED_FILTER_XCAST_TYPE_DEL,\n\t\t\t\t\t   ndev->dev_addr);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\teth_hw_addr_set(ndev, addr->sa_data);\n\tDP_INFO(edev, \"Setting device MAC to %pM\\n\", addr->sa_data);\n\n\tif (edev->state != QEDE_STATE_OPEN) {\n\t\tDP_VERBOSE(edev, NETIF_MSG_IFDOWN,\n\t\t\t   \"The device is currently down\\n\");\n\t\t \n\t\tif (IS_VF(edev) && edev->ops->req_bulletin_update_mac)\n\t\t\tedev->ops->req_bulletin_update_mac(edev->cdev,\n\t\t\t\t\t\t\t   ndev->dev_addr);\n\t\tgoto out;\n\t}\n\n\tedev->ops->common->update_mac(edev->cdev, ndev->dev_addr);\n\n\trc = qede_set_ucast_rx_mac(edev, QED_FILTER_XCAST_TYPE_ADD,\n\t\t\t\t   ndev->dev_addr);\nout:\n\t__qede_unlock(edev);\n\treturn rc;\n}\n\nstatic int\nqede_configure_mcast_filtering(struct net_device *ndev,\n\t\t\t       enum qed_filter_rx_mode_type *accept_flags)\n{\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\tunsigned char *mc_macs, *temp;\n\tstruct netdev_hw_addr *ha;\n\tint rc = 0, mc_count;\n\tsize_t size;\n\n\tsize = 64 * ETH_ALEN;\n\n\tmc_macs = kzalloc(size, GFP_KERNEL);\n\tif (!mc_macs) {\n\t\tDP_NOTICE(edev,\n\t\t\t  \"Failed to allocate memory for multicast MACs\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\ttemp = mc_macs;\n\n\t \n\trc = qede_set_mcast_rx_mac(edev, QED_FILTER_XCAST_TYPE_DEL,\n\t\t\t\t   mc_macs, 1);\n\tif (rc)\n\t\tgoto exit;\n\n\tnetif_addr_lock_bh(ndev);\n\n\tmc_count = netdev_mc_count(ndev);\n\tif (mc_count <= 64) {\n\t\tnetdev_for_each_mc_addr(ha, ndev) {\n\t\t\tether_addr_copy(temp, ha->addr);\n\t\t\ttemp += ETH_ALEN;\n\t\t}\n\t}\n\n\tnetif_addr_unlock_bh(ndev);\n\n\t \n\tif ((ndev->flags & IFF_ALLMULTI) || (mc_count > 64)) {\n\t\tif (*accept_flags == QED_FILTER_RX_MODE_TYPE_REGULAR)\n\t\t\t*accept_flags = QED_FILTER_RX_MODE_TYPE_MULTI_PROMISC;\n\t} else {\n\t\t \n\t\trc = qede_set_mcast_rx_mac(edev, QED_FILTER_XCAST_TYPE_ADD,\n\t\t\t\t\t   mc_macs, mc_count);\n\t}\n\nexit:\n\tkfree(mc_macs);\n\treturn rc;\n}\n\nvoid qede_set_rx_mode(struct net_device *ndev)\n{\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\n\tset_bit(QEDE_SP_RX_MODE, &edev->sp_flags);\n\tschedule_delayed_work(&edev->sp_task, 0);\n}\n\n \nvoid qede_config_rx_mode(struct net_device *ndev)\n{\n\tenum qed_filter_rx_mode_type accept_flags;\n\tstruct qede_dev *edev = netdev_priv(ndev);\n\tunsigned char *uc_macs, *temp;\n\tstruct netdev_hw_addr *ha;\n\tint rc, uc_count;\n\tsize_t size;\n\n\tnetif_addr_lock_bh(ndev);\n\n\tuc_count = netdev_uc_count(ndev);\n\tsize = uc_count * ETH_ALEN;\n\n\tuc_macs = kzalloc(size, GFP_ATOMIC);\n\tif (!uc_macs) {\n\t\tDP_NOTICE(edev, \"Failed to allocate memory for unicast MACs\\n\");\n\t\tnetif_addr_unlock_bh(ndev);\n\t\treturn;\n\t}\n\n\ttemp = uc_macs;\n\tnetdev_for_each_uc_addr(ha, ndev) {\n\t\tether_addr_copy(temp, ha->addr);\n\t\ttemp += ETH_ALEN;\n\t}\n\n\tnetif_addr_unlock_bh(ndev);\n\n\t \n\trc = qede_set_ucast_rx_mac(edev, QED_FILTER_XCAST_TYPE_REPLACE,\n\t\t\t\t   edev->ndev->dev_addr);\n\tif (rc)\n\t\tgoto out;\n\n\t \n\tif (ndev->flags & IFF_PROMISC)\n\t\taccept_flags = QED_FILTER_RX_MODE_TYPE_PROMISC;\n\telse\n\t\taccept_flags = QED_FILTER_RX_MODE_TYPE_REGULAR;\n\n\t \n\tif (uc_count < edev->dev_info.num_mac_filters) {\n\t\tint i;\n\n\t\ttemp = uc_macs;\n\t\tfor (i = 0; i < uc_count; i++) {\n\t\t\trc = qede_set_ucast_rx_mac(edev,\n\t\t\t\t\t\t   QED_FILTER_XCAST_TYPE_ADD,\n\t\t\t\t\t\t   temp);\n\t\t\tif (rc)\n\t\t\t\tgoto out;\n\n\t\t\ttemp += ETH_ALEN;\n\t\t}\n\t} else {\n\t\taccept_flags = QED_FILTER_RX_MODE_TYPE_PROMISC;\n\t}\n\n\trc = qede_configure_mcast_filtering(ndev, &accept_flags);\n\tif (rc)\n\t\tgoto out;\n\n\t \n\tif (ndev->flags & IFF_PROMISC) {\n\t\tqede_config_accept_any_vlan(edev, true);\n\t} else if (!edev->non_configured_vlans) {\n\t\t \n\t\tqede_config_accept_any_vlan(edev, false);\n\t}\n\n\tedev->ops->filter_config_rx_mode(edev->cdev, accept_flags);\nout:\n\tkfree(uc_macs);\n}\n\nstatic struct qede_arfs_fltr_node *\nqede_get_arfs_fltr_by_loc(struct hlist_head *head, u64 location)\n{\n\tstruct qede_arfs_fltr_node *fltr;\n\n\thlist_for_each_entry(fltr, head, node)\n\t\tif (location == fltr->sw_id)\n\t\t\treturn fltr;\n\n\treturn NULL;\n}\n\nint qede_get_cls_rule_all(struct qede_dev *edev, struct ethtool_rxnfc *info,\n\t\t\t  u32 *rule_locs)\n{\n\tstruct qede_arfs_fltr_node *fltr;\n\tstruct hlist_head *head;\n\tint cnt = 0, rc = 0;\n\n\tinfo->data = QEDE_RFS_MAX_FLTR;\n\n\t__qede_lock(edev);\n\n\tif (!edev->arfs) {\n\t\trc = -EPERM;\n\t\tgoto unlock;\n\t}\n\n\thead = QEDE_ARFS_BUCKET_HEAD(edev, 0);\n\n\thlist_for_each_entry(fltr, head, node) {\n\t\tif (cnt == info->rule_cnt) {\n\t\t\trc = -EMSGSIZE;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\trule_locs[cnt] = fltr->sw_id;\n\t\tcnt++;\n\t}\n\n\tinfo->rule_cnt = cnt;\n\nunlock:\n\t__qede_unlock(edev);\n\treturn rc;\n}\n\nint qede_get_cls_rule_entry(struct qede_dev *edev, struct ethtool_rxnfc *cmd)\n{\n\tstruct ethtool_rx_flow_spec *fsp = &cmd->fs;\n\tstruct qede_arfs_fltr_node *fltr = NULL;\n\tint rc = 0;\n\n\tcmd->data = QEDE_RFS_MAX_FLTR;\n\n\t__qede_lock(edev);\n\n\tif (!edev->arfs) {\n\t\trc = -EPERM;\n\t\tgoto unlock;\n\t}\n\n\tfltr = qede_get_arfs_fltr_by_loc(QEDE_ARFS_BUCKET_HEAD(edev, 0),\n\t\t\t\t\t fsp->location);\n\tif (!fltr) {\n\t\tDP_NOTICE(edev, \"Rule not found - location=0x%x\\n\",\n\t\t\t  fsp->location);\n\t\trc = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tif (fltr->tuple.eth_proto == htons(ETH_P_IP)) {\n\t\tif (fltr->tuple.ip_proto == IPPROTO_TCP)\n\t\t\tfsp->flow_type = TCP_V4_FLOW;\n\t\telse\n\t\t\tfsp->flow_type = UDP_V4_FLOW;\n\n\t\tfsp->h_u.tcp_ip4_spec.psrc = fltr->tuple.src_port;\n\t\tfsp->h_u.tcp_ip4_spec.pdst = fltr->tuple.dst_port;\n\t\tfsp->h_u.tcp_ip4_spec.ip4src = fltr->tuple.src_ipv4;\n\t\tfsp->h_u.tcp_ip4_spec.ip4dst = fltr->tuple.dst_ipv4;\n\t} else {\n\t\tif (fltr->tuple.ip_proto == IPPROTO_TCP)\n\t\t\tfsp->flow_type = TCP_V6_FLOW;\n\t\telse\n\t\t\tfsp->flow_type = UDP_V6_FLOW;\n\t\tfsp->h_u.tcp_ip6_spec.psrc = fltr->tuple.src_port;\n\t\tfsp->h_u.tcp_ip6_spec.pdst = fltr->tuple.dst_port;\n\t\tmemcpy(&fsp->h_u.tcp_ip6_spec.ip6src,\n\t\t       &fltr->tuple.src_ipv6, sizeof(struct in6_addr));\n\t\tmemcpy(&fsp->h_u.tcp_ip6_spec.ip6dst,\n\t\t       &fltr->tuple.dst_ipv6, sizeof(struct in6_addr));\n\t}\n\n\tfsp->ring_cookie = fltr->rxq_id;\n\n\tif (fltr->vfid) {\n\t\tfsp->ring_cookie |= ((u64)fltr->vfid) <<\n\t\t\t\t\tETHTOOL_RX_FLOW_SPEC_RING_VF_OFF;\n\t}\n\n\tif (fltr->b_is_drop)\n\t\tfsp->ring_cookie = RX_CLS_FLOW_DISC;\nunlock:\n\t__qede_unlock(edev);\n\treturn rc;\n}\n\nstatic int\nqede_poll_arfs_filter_config(struct qede_dev *edev,\n\t\t\t     struct qede_arfs_fltr_node *fltr)\n{\n\tint count = QEDE_ARFS_POLL_COUNT;\n\n\twhile (fltr->used && count) {\n\t\tmsleep(20);\n\t\tcount--;\n\t}\n\n\tif (count == 0 || fltr->fw_rc) {\n\t\tDP_NOTICE(edev, \"Timeout in polling filter config\\n\");\n\t\tqede_dequeue_fltr_and_config_searcher(edev, fltr);\n\t\treturn -EIO;\n\t}\n\n\treturn fltr->fw_rc;\n}\n\nstatic int qede_flow_get_min_header_size(struct qede_arfs_tuple *t)\n{\n\tint size = ETH_HLEN;\n\n\tif (t->eth_proto == htons(ETH_P_IP))\n\t\tsize += sizeof(struct iphdr);\n\telse\n\t\tsize += sizeof(struct ipv6hdr);\n\n\tif (t->ip_proto == IPPROTO_TCP)\n\t\tsize += sizeof(struct tcphdr);\n\telse\n\t\tsize += sizeof(struct udphdr);\n\n\treturn size;\n}\n\nstatic bool qede_flow_spec_ipv4_cmp(struct qede_arfs_tuple *a,\n\t\t\t\t    struct qede_arfs_tuple *b)\n{\n\tif (a->eth_proto != htons(ETH_P_IP) ||\n\t    b->eth_proto != htons(ETH_P_IP))\n\t\treturn false;\n\n\treturn (a->src_ipv4 == b->src_ipv4) &&\n\t       (a->dst_ipv4 == b->dst_ipv4);\n}\n\nstatic void qede_flow_build_ipv4_hdr(struct qede_arfs_tuple *t,\n\t\t\t\t     void *header)\n{\n\t__be16 *ports = (__be16 *)(header + ETH_HLEN + sizeof(struct iphdr));\n\tstruct iphdr *ip = (struct iphdr *)(header + ETH_HLEN);\n\tstruct ethhdr *eth = (struct ethhdr *)header;\n\n\teth->h_proto = t->eth_proto;\n\tip->saddr = t->src_ipv4;\n\tip->daddr = t->dst_ipv4;\n\tip->version = 0x4;\n\tip->ihl = 0x5;\n\tip->protocol = t->ip_proto;\n\tip->tot_len = cpu_to_be16(qede_flow_get_min_header_size(t) - ETH_HLEN);\n\n\t \n\tports[0] = t->src_port;\n\tports[1] = t->dst_port;\n}\n\nstatic void qede_flow_stringify_ipv4_hdr(struct qede_arfs_tuple *t,\n\t\t\t\t\t void *buffer)\n{\n\tconst char *prefix = t->ip_proto == IPPROTO_TCP ? \"TCP\" : \"UDP\";\n\n\tsnprintf(buffer, QEDE_FILTER_PRINT_MAX_LEN,\n\t\t \"%s %pI4 (%04x) -> %pI4 (%04x)\",\n\t\t prefix, &t->src_ipv4, t->src_port,\n\t\t &t->dst_ipv4, t->dst_port);\n}\n\nstatic bool qede_flow_spec_ipv6_cmp(struct qede_arfs_tuple *a,\n\t\t\t\t    struct qede_arfs_tuple *b)\n{\n\tif (a->eth_proto != htons(ETH_P_IPV6) ||\n\t    b->eth_proto != htons(ETH_P_IPV6))\n\t\treturn false;\n\n\tif (memcmp(&a->src_ipv6, &b->src_ipv6, sizeof(struct in6_addr)))\n\t\treturn false;\n\n\tif (memcmp(&a->dst_ipv6, &b->dst_ipv6, sizeof(struct in6_addr)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void qede_flow_build_ipv6_hdr(struct qede_arfs_tuple *t,\n\t\t\t\t     void *header)\n{\n\t__be16 *ports = (__be16 *)(header + ETH_HLEN + sizeof(struct ipv6hdr));\n\tstruct ipv6hdr *ip6 = (struct ipv6hdr *)(header + ETH_HLEN);\n\tstruct ethhdr *eth = (struct ethhdr *)header;\n\n\teth->h_proto = t->eth_proto;\n\tmemcpy(&ip6->saddr, &t->src_ipv6, sizeof(struct in6_addr));\n\tmemcpy(&ip6->daddr, &t->dst_ipv6, sizeof(struct in6_addr));\n\tip6->version = 0x6;\n\n\tif (t->ip_proto == IPPROTO_TCP) {\n\t\tip6->nexthdr = NEXTHDR_TCP;\n\t\tip6->payload_len = cpu_to_be16(sizeof(struct tcphdr));\n\t} else {\n\t\tip6->nexthdr = NEXTHDR_UDP;\n\t\tip6->payload_len = cpu_to_be16(sizeof(struct udphdr));\n\t}\n\n\t \n\tports[0] = t->src_port;\n\tports[1] = t->dst_port;\n}\n\n \nstatic int qede_flow_spec_validate_unused(struct qede_dev *edev,\n\t\t\t\t\t  struct ethtool_rx_flow_spec *fs)\n{\n\tif (fs->flow_type & FLOW_MAC_EXT) {\n\t\tDP_INFO(edev, \"Don't support MAC extensions\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif ((fs->flow_type & FLOW_EXT) &&\n\t    (fs->h_ext.vlan_etype || fs->h_ext.vlan_tci)) {\n\t\tDP_INFO(edev, \"Don't support vlan-based classification\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif ((fs->flow_type & FLOW_EXT) &&\n\t    (fs->h_ext.data[0] || fs->h_ext.data[1])) {\n\t\tDP_INFO(edev, \"Don't support user defined data\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic int qede_set_v4_tuple_to_profile(struct qede_dev *edev,\n\t\t\t\t\tstruct qede_arfs_tuple *t)\n{\n\t \n\tif (t->src_port && t->dst_port && t->src_ipv4 && t->dst_ipv4) {\n\t\tt->mode = QED_FILTER_CONFIG_MODE_5_TUPLE;\n\t} else if (!t->src_port && t->dst_port &&\n\t\t   !t->src_ipv4 && !t->dst_ipv4) {\n\t\tt->mode = QED_FILTER_CONFIG_MODE_L4_PORT;\n\t} else if (!t->src_port && !t->dst_port &&\n\t\t   !t->dst_ipv4 && t->src_ipv4) {\n\t\tt->mode = QED_FILTER_CONFIG_MODE_IP_SRC;\n\t} else if (!t->src_port && !t->dst_port &&\n\t\t   t->dst_ipv4 && !t->src_ipv4) {\n\t\tt->mode = QED_FILTER_CONFIG_MODE_IP_DEST;\n\t} else {\n\t\tDP_INFO(edev, \"Invalid N-tuple\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tt->ip_comp = qede_flow_spec_ipv4_cmp;\n\tt->build_hdr = qede_flow_build_ipv4_hdr;\n\tt->stringify = qede_flow_stringify_ipv4_hdr;\n\n\treturn 0;\n}\n\nstatic int qede_set_v6_tuple_to_profile(struct qede_dev *edev,\n\t\t\t\t\tstruct qede_arfs_tuple *t,\n\t\t\t\t\tstruct in6_addr *zaddr)\n{\n\t \n\tif (t->src_port && t->dst_port &&\n\t    memcmp(&t->src_ipv6, zaddr, sizeof(struct in6_addr)) &&\n\t    memcmp(&t->dst_ipv6, zaddr, sizeof(struct in6_addr))) {\n\t\tt->mode = QED_FILTER_CONFIG_MODE_5_TUPLE;\n\t} else if (!t->src_port && t->dst_port &&\n\t\t   !memcmp(&t->src_ipv6, zaddr, sizeof(struct in6_addr)) &&\n\t\t   !memcmp(&t->dst_ipv6, zaddr, sizeof(struct in6_addr))) {\n\t\tt->mode = QED_FILTER_CONFIG_MODE_L4_PORT;\n\t} else if (!t->src_port && !t->dst_port &&\n\t\t   !memcmp(&t->dst_ipv6, zaddr, sizeof(struct in6_addr)) &&\n\t\t   memcmp(&t->src_ipv6, zaddr, sizeof(struct in6_addr))) {\n\t\tt->mode = QED_FILTER_CONFIG_MODE_IP_SRC;\n\t} else if (!t->src_port && !t->dst_port &&\n\t\t   memcmp(&t->dst_ipv6, zaddr, sizeof(struct in6_addr)) &&\n\t\t   !memcmp(&t->src_ipv6, zaddr, sizeof(struct in6_addr))) {\n\t\tt->mode = QED_FILTER_CONFIG_MODE_IP_DEST;\n\t} else {\n\t\tDP_INFO(edev, \"Invalid N-tuple\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tt->ip_comp = qede_flow_spec_ipv6_cmp;\n\tt->build_hdr = qede_flow_build_ipv6_hdr;\n\n\treturn 0;\n}\n\n \nstatic struct qede_arfs_fltr_node *\nqede_flow_find_fltr(struct qede_dev *edev, struct qede_arfs_tuple *t)\n{\n\tstruct qede_arfs_fltr_node *fltr;\n\tstruct hlist_node *temp;\n\tstruct hlist_head *head;\n\n\thead = QEDE_ARFS_BUCKET_HEAD(edev, 0);\n\n\thlist_for_each_entry_safe(fltr, temp, head, node) {\n\t\tif (fltr->tuple.ip_proto == t->ip_proto &&\n\t\t    fltr->tuple.src_port == t->src_port &&\n\t\t    fltr->tuple.dst_port == t->dst_port &&\n\t\t    t->ip_comp(&fltr->tuple, t))\n\t\t\treturn fltr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void qede_flow_set_destination(struct qede_dev *edev,\n\t\t\t\t      struct qede_arfs_fltr_node *n,\n\t\t\t\t      struct ethtool_rx_flow_spec *fs)\n{\n\tif (fs->ring_cookie == RX_CLS_FLOW_DISC) {\n\t\tn->b_is_drop = true;\n\t\treturn;\n\t}\n\n\tn->vfid = ethtool_get_flow_spec_ring_vf(fs->ring_cookie);\n\tn->rxq_id = ethtool_get_flow_spec_ring(fs->ring_cookie);\n\tn->next_rxq_id = n->rxq_id;\n\n\tif (n->vfid)\n\t\tDP_VERBOSE(edev, QED_MSG_SP,\n\t\t\t   \"Configuring N-tuple for VF 0x%02x\\n\", n->vfid - 1);\n}\n\nint qede_delete_flow_filter(struct qede_dev *edev, u64 cookie)\n{\n\tstruct qede_arfs_fltr_node *fltr = NULL;\n\tint rc = -EPERM;\n\n\t__qede_lock(edev);\n\tif (!edev->arfs)\n\t\tgoto unlock;\n\n\tfltr = qede_get_arfs_fltr_by_loc(QEDE_ARFS_BUCKET_HEAD(edev, 0),\n\t\t\t\t\t cookie);\n\tif (!fltr)\n\t\tgoto unlock;\n\n\tqede_configure_arfs_fltr(edev, fltr, fltr->rxq_id, false);\n\n\trc = qede_poll_arfs_filter_config(edev, fltr);\n\tif (rc == 0)\n\t\tqede_dequeue_fltr_and_config_searcher(edev, fltr);\n\nunlock:\n\t__qede_unlock(edev);\n\treturn rc;\n}\n\nint qede_get_arfs_filter_count(struct qede_dev *edev)\n{\n\tint count = 0;\n\n\t__qede_lock(edev);\n\n\tif (!edev->arfs)\n\t\tgoto unlock;\n\n\tcount = edev->arfs->filter_count;\n\nunlock:\n\t__qede_unlock(edev);\n\treturn count;\n}\n\nstatic int qede_parse_actions(struct qede_dev *edev,\n\t\t\t      struct flow_action *flow_action,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tconst struct flow_action_entry *act;\n\tint i;\n\n\tif (!flow_action_has_entries(flow_action)) {\n\t\tDP_NOTICE(edev, \"No actions received\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!flow_action_basic_hw_stats_check(flow_action, extack))\n\t\treturn -EOPNOTSUPP;\n\n\tflow_action_for_each(i, act, flow_action) {\n\t\tswitch (act->id) {\n\t\tcase FLOW_ACTION_DROP:\n\t\t\tbreak;\n\t\tcase FLOW_ACTION_QUEUE:\n\t\t\tif (act->queue.vf)\n\t\t\t\tbreak;\n\n\t\t\tif (act->queue.index >= QEDE_RSS_COUNT(edev)) {\n\t\t\t\tDP_INFO(edev, \"Queue out-of-bounds\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int\nqede_flow_parse_ports(struct qede_dev *edev, struct flow_rule *rule,\n\t\t      struct qede_arfs_tuple *t)\n{\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_PORTS)) {\n\t\tstruct flow_match_ports match;\n\n\t\tflow_rule_match_ports(rule, &match);\n\t\tif ((match.key->src && match.mask->src != htons(U16_MAX)) ||\n\t\t    (match.key->dst && match.mask->dst != htons(U16_MAX))) {\n\t\t\tDP_NOTICE(edev, \"Do not support ports masks\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tt->src_port = match.key->src;\n\t\tt->dst_port = match.key->dst;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nqede_flow_parse_v6_common(struct qede_dev *edev, struct flow_rule *rule,\n\t\t\t  struct qede_arfs_tuple *t)\n{\n\tstruct in6_addr zero_addr, addr;\n\n\tmemset(&zero_addr, 0, sizeof(addr));\n\tmemset(&addr, 0xff, sizeof(addr));\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_IPV6_ADDRS)) {\n\t\tstruct flow_match_ipv6_addrs match;\n\n\t\tflow_rule_match_ipv6_addrs(rule, &match);\n\t\tif ((memcmp(&match.key->src, &zero_addr, sizeof(addr)) &&\n\t\t     memcmp(&match.mask->src, &addr, sizeof(addr))) ||\n\t\t    (memcmp(&match.key->dst, &zero_addr, sizeof(addr)) &&\n\t\t     memcmp(&match.mask->dst, &addr, sizeof(addr)))) {\n\t\t\tDP_NOTICE(edev,\n\t\t\t\t  \"Do not support IPv6 address prefix/mask\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmemcpy(&t->src_ipv6, &match.key->src, sizeof(addr));\n\t\tmemcpy(&t->dst_ipv6, &match.key->dst, sizeof(addr));\n\t}\n\n\tif (qede_flow_parse_ports(edev, rule, t))\n\t\treturn -EINVAL;\n\n\treturn qede_set_v6_tuple_to_profile(edev, t, &zero_addr);\n}\n\nstatic int\nqede_flow_parse_v4_common(struct qede_dev *edev, struct flow_rule *rule,\n\t\t\tstruct qede_arfs_tuple *t)\n{\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_IPV4_ADDRS)) {\n\t\tstruct flow_match_ipv4_addrs match;\n\n\t\tflow_rule_match_ipv4_addrs(rule, &match);\n\t\tif ((match.key->src && match.mask->src != htonl(U32_MAX)) ||\n\t\t    (match.key->dst && match.mask->dst != htonl(U32_MAX))) {\n\t\t\tDP_NOTICE(edev, \"Do not support ipv4 prefix/masks\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tt->src_ipv4 = match.key->src;\n\t\tt->dst_ipv4 = match.key->dst;\n\t}\n\n\tif (qede_flow_parse_ports(edev, rule, t))\n\t\treturn -EINVAL;\n\n\treturn qede_set_v4_tuple_to_profile(edev, t);\n}\n\nstatic int\nqede_flow_parse_tcp_v6(struct qede_dev *edev, struct flow_rule *rule,\n\t\t     struct qede_arfs_tuple *tuple)\n{\n\ttuple->ip_proto = IPPROTO_TCP;\n\ttuple->eth_proto = htons(ETH_P_IPV6);\n\n\treturn qede_flow_parse_v6_common(edev, rule, tuple);\n}\n\nstatic int\nqede_flow_parse_tcp_v4(struct qede_dev *edev, struct flow_rule *rule,\n\t\t     struct qede_arfs_tuple *tuple)\n{\n\ttuple->ip_proto = IPPROTO_TCP;\n\ttuple->eth_proto = htons(ETH_P_IP);\n\n\treturn qede_flow_parse_v4_common(edev, rule, tuple);\n}\n\nstatic int\nqede_flow_parse_udp_v6(struct qede_dev *edev, struct flow_rule *rule,\n\t\t     struct qede_arfs_tuple *tuple)\n{\n\ttuple->ip_proto = IPPROTO_UDP;\n\ttuple->eth_proto = htons(ETH_P_IPV6);\n\n\treturn qede_flow_parse_v6_common(edev, rule, tuple);\n}\n\nstatic int\nqede_flow_parse_udp_v4(struct qede_dev *edev, struct flow_rule *rule,\n\t\t     struct qede_arfs_tuple *tuple)\n{\n\ttuple->ip_proto = IPPROTO_UDP;\n\ttuple->eth_proto = htons(ETH_P_IP);\n\n\treturn qede_flow_parse_v4_common(edev, rule, tuple);\n}\n\nstatic int\nqede_parse_flow_attr(struct qede_dev *edev, __be16 proto,\n\t\t     struct flow_rule *rule, struct qede_arfs_tuple *tuple)\n{\n\tstruct flow_dissector *dissector = rule->match.dissector;\n\tint rc = -EINVAL;\n\tu8 ip_proto = 0;\n\n\tmemset(tuple, 0, sizeof(*tuple));\n\n\tif (dissector->used_keys &\n\t    ~(BIT_ULL(FLOW_DISSECTOR_KEY_CONTROL) |\n\t      BIT_ULL(FLOW_DISSECTOR_KEY_IPV4_ADDRS) |\n\t      BIT_ULL(FLOW_DISSECTOR_KEY_BASIC) |\n\t      BIT_ULL(FLOW_DISSECTOR_KEY_IPV6_ADDRS) |\n\t      BIT_ULL(FLOW_DISSECTOR_KEY_PORTS))) {\n\t\tDP_NOTICE(edev, \"Unsupported key set:0x%llx\\n\",\n\t\t\t  dissector->used_keys);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (proto != htons(ETH_P_IP) &&\n\t    proto != htons(ETH_P_IPV6)) {\n\t\tDP_NOTICE(edev, \"Unsupported proto=0x%x\\n\", proto);\n\t\treturn -EPROTONOSUPPORT;\n\t}\n\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_BASIC)) {\n\t\tstruct flow_match_basic match;\n\n\t\tflow_rule_match_basic(rule, &match);\n\t\tip_proto = match.key->ip_proto;\n\t}\n\n\tif (ip_proto == IPPROTO_TCP && proto == htons(ETH_P_IP))\n\t\trc = qede_flow_parse_tcp_v4(edev, rule, tuple);\n\telse if (ip_proto == IPPROTO_TCP && proto == htons(ETH_P_IPV6))\n\t\trc = qede_flow_parse_tcp_v6(edev, rule, tuple);\n\telse if (ip_proto == IPPROTO_UDP && proto == htons(ETH_P_IP))\n\t\trc = qede_flow_parse_udp_v4(edev, rule, tuple);\n\telse if (ip_proto == IPPROTO_UDP && proto == htons(ETH_P_IPV6))\n\t\trc = qede_flow_parse_udp_v6(edev, rule, tuple);\n\telse\n\t\tDP_NOTICE(edev, \"Invalid protocol request\\n\");\n\n\treturn rc;\n}\n\nint qede_add_tc_flower_fltr(struct qede_dev *edev, __be16 proto,\n\t\t\t    struct flow_cls_offload *f)\n{\n\tstruct qede_arfs_fltr_node *n;\n\tint min_hlen, rc = -EINVAL;\n\tstruct qede_arfs_tuple t;\n\n\t__qede_lock(edev);\n\n\tif (!edev->arfs) {\n\t\trc = -EPERM;\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (qede_parse_flow_attr(edev, proto, f->rule, &t))\n\t\tgoto unlock;\n\n\t \n\tif ((edev->arfs->filter_count && edev->arfs->mode != t.mode) ||\n\t    edev->arfs->filter_count == QEDE_RFS_MAX_FLTR) {\n\t\tDP_NOTICE(edev,\n\t\t\t  \"Filter configuration invalidated, filter mode=0x%x, configured mode=0x%x, filter count=0x%x\\n\",\n\t\t\t  t.mode, edev->arfs->mode, edev->arfs->filter_count);\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (qede_parse_actions(edev, &f->rule->action, f->common.extack))\n\t\tgoto unlock;\n\n\tif (qede_flow_find_fltr(edev, &t)) {\n\t\trc = -EEXIST;\n\t\tgoto unlock;\n\t}\n\n\tn = kzalloc(sizeof(*n), GFP_KERNEL);\n\tif (!n) {\n\t\trc = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tmin_hlen = qede_flow_get_min_header_size(&t);\n\n\tn->data = kzalloc(min_hlen, GFP_KERNEL);\n\tif (!n->data) {\n\t\tkfree(n);\n\t\trc = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tmemcpy(&n->tuple, &t, sizeof(n->tuple));\n\n\tn->buf_len = min_hlen;\n\tn->b_is_drop = true;\n\tn->sw_id = f->cookie;\n\n\tn->tuple.build_hdr(&n->tuple, n->data);\n\n\trc = qede_enqueue_fltr_and_config_searcher(edev, n, 0);\n\tif (rc)\n\t\tgoto unlock;\n\n\tqede_configure_arfs_fltr(edev, n, n->rxq_id, true);\n\trc = qede_poll_arfs_filter_config(edev, n);\n\nunlock:\n\t__qede_unlock(edev);\n\treturn rc;\n}\n\nstatic int qede_flow_spec_validate(struct qede_dev *edev,\n\t\t\t\t   struct flow_action *flow_action,\n\t\t\t\t   struct qede_arfs_tuple *t,\n\t\t\t\t   __u32 location)\n{\n\tif (location >= QEDE_RFS_MAX_FLTR) {\n\t\tDP_INFO(edev, \"Location out-of-bounds\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (test_bit(location, edev->arfs->arfs_fltr_bmap)) {\n\t\tDP_INFO(edev, \"Location already in use\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (edev->arfs->filter_count &&\n\t    edev->arfs->mode != t->mode) {\n\t\tDP_INFO(edev,\n\t\t\t\"flow_spec would require filtering mode %08x, but %08x is configured\\n\",\n\t\t\tt->mode, edev->arfs->filter_count);\n\t\treturn -EINVAL;\n\t}\n\n\tif (qede_parse_actions(edev, flow_action, NULL))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int qede_flow_spec_to_rule(struct qede_dev *edev,\n\t\t\t\t  struct qede_arfs_tuple *t,\n\t\t\t\t  struct ethtool_rx_flow_spec *fs)\n{\n\tstruct ethtool_rx_flow_spec_input input = {};\n\tstruct ethtool_rx_flow_rule *flow;\n\t__be16 proto;\n\tint err = 0;\n\n\tif (qede_flow_spec_validate_unused(edev, fs))\n\t\treturn -EOPNOTSUPP;\n\n\tswitch ((fs->flow_type & ~FLOW_EXT)) {\n\tcase TCP_V4_FLOW:\n\tcase UDP_V4_FLOW:\n\t\tproto = htons(ETH_P_IP);\n\t\tbreak;\n\tcase TCP_V6_FLOW:\n\tcase UDP_V6_FLOW:\n\t\tproto = htons(ETH_P_IPV6);\n\t\tbreak;\n\tdefault:\n\t\tDP_VERBOSE(edev, NETIF_MSG_IFUP,\n\t\t\t   \"Can't support flow of type %08x\\n\", fs->flow_type);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tinput.fs = fs;\n\tflow = ethtool_rx_flow_rule_create(&input);\n\tif (IS_ERR(flow))\n\t\treturn PTR_ERR(flow);\n\n\tif (qede_parse_flow_attr(edev, proto, flow->rule, t)) {\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\t \n\terr = qede_flow_spec_validate(edev, &flow->rule->action, t,\n\t\t\t\t      fs->location);\nerr_out:\n\tethtool_rx_flow_rule_destroy(flow);\n\treturn err;\n\n}\n\nint qede_add_cls_rule(struct qede_dev *edev, struct ethtool_rxnfc *info)\n{\n\tstruct ethtool_rx_flow_spec *fsp = &info->fs;\n\tstruct qede_arfs_fltr_node *n;\n\tstruct qede_arfs_tuple t;\n\tint min_hlen, rc;\n\n\t__qede_lock(edev);\n\n\tif (!edev->arfs) {\n\t\trc = -EPERM;\n\t\tgoto unlock;\n\t}\n\n\t \n\trc = qede_flow_spec_to_rule(edev, &t, fsp);\n\tif (rc)\n\t\tgoto unlock;\n\n\tif (qede_flow_find_fltr(edev, &t)) {\n\t\trc = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tn = kzalloc(sizeof(*n), GFP_KERNEL);\n\tif (!n) {\n\t\trc = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tmin_hlen = qede_flow_get_min_header_size(&t);\n\tn->data = kzalloc(min_hlen, GFP_KERNEL);\n\tif (!n->data) {\n\t\tkfree(n);\n\t\trc = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tn->sw_id = fsp->location;\n\tset_bit(n->sw_id, edev->arfs->arfs_fltr_bmap);\n\tn->buf_len = min_hlen;\n\n\tmemcpy(&n->tuple, &t, sizeof(n->tuple));\n\n\tqede_flow_set_destination(edev, n, fsp);\n\n\t \n\tn->tuple.build_hdr(&n->tuple, n->data);\n\n\trc = qede_enqueue_fltr_and_config_searcher(edev, n, 0);\n\tif (rc)\n\t\tgoto unlock;\n\n\tqede_configure_arfs_fltr(edev, n, n->rxq_id, true);\n\trc = qede_poll_arfs_filter_config(edev, n);\nunlock:\n\t__qede_unlock(edev);\n\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}