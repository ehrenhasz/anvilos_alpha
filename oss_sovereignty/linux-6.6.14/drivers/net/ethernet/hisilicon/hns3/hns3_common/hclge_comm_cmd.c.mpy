{
  "module_name": "hclge_comm_cmd.c",
  "hash_id": "6ca1b9fe2dd3240d019f6362fc52812df5bdfb67c69e01448dca6b57483e00c1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/hisilicon/hns3/hns3_common/hclge_comm_cmd.c",
  "human_readable_source": "\n\n\n#include \"hnae3.h\"\n#include \"hclge_comm_cmd.h\"\n\nstatic void hclge_comm_cmd_config_regs(struct hclge_comm_hw *hw,\n\t\t\t\t       struct hclge_comm_cmq_ring *ring)\n{\n\tdma_addr_t dma = ring->desc_dma_addr;\n\tu32 reg_val;\n\n\tif (ring->ring_type == HCLGE_COMM_TYPE_CSQ) {\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_BASEADDR_L_REG,\n\t\t\t\t     lower_32_bits(dma));\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_BASEADDR_H_REG,\n\t\t\t\t     upper_32_bits(dma));\n\t\treg_val = hclge_comm_read_dev(hw, HCLGE_COMM_NIC_CSQ_DEPTH_REG);\n\t\treg_val &= HCLGE_COMM_NIC_SW_RST_RDY;\n\t\treg_val |= ring->desc_num >> HCLGE_COMM_NIC_CMQ_DESC_NUM_S;\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_DEPTH_REG, reg_val);\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_HEAD_REG, 0);\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_TAIL_REG, 0);\n\t} else {\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_BASEADDR_L_REG,\n\t\t\t\t     lower_32_bits(dma));\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_BASEADDR_H_REG,\n\t\t\t\t     upper_32_bits(dma));\n\t\treg_val = ring->desc_num >> HCLGE_COMM_NIC_CMQ_DESC_NUM_S;\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_DEPTH_REG, reg_val);\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_HEAD_REG, 0);\n\t\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_TAIL_REG, 0);\n\t}\n}\n\nvoid hclge_comm_cmd_init_regs(struct hclge_comm_hw *hw)\n{\n\thclge_comm_cmd_config_regs(hw, &hw->cmq.csq);\n\thclge_comm_cmd_config_regs(hw, &hw->cmq.crq);\n}\n\nvoid hclge_comm_cmd_reuse_desc(struct hclge_desc *desc, bool is_read)\n{\n\tdesc->flag = cpu_to_le16(HCLGE_COMM_CMD_FLAG_NO_INTR |\n\t\t\t\t HCLGE_COMM_CMD_FLAG_IN);\n\tif (is_read)\n\t\tdesc->flag |= cpu_to_le16(HCLGE_COMM_CMD_FLAG_WR);\n\telse\n\t\tdesc->flag &= cpu_to_le16(~HCLGE_COMM_CMD_FLAG_WR);\n}\n\nstatic void hclge_comm_set_default_capability(struct hnae3_ae_dev *ae_dev,\n\t\t\t\t\t      bool is_pf)\n{\n\tset_bit(HNAE3_DEV_SUPPORT_GRO_B, ae_dev->caps);\n\tif (is_pf) {\n\t\tset_bit(HNAE3_DEV_SUPPORT_FD_B, ae_dev->caps);\n\t\tset_bit(HNAE3_DEV_SUPPORT_FEC_B, ae_dev->caps);\n\t\tset_bit(HNAE3_DEV_SUPPORT_PAUSE_B, ae_dev->caps);\n\t}\n}\n\nvoid hclge_comm_cmd_setup_basic_desc(struct hclge_desc *desc,\n\t\t\t\t     enum hclge_opcode_type opcode,\n\t\t\t\t     bool is_read)\n{\n\tmemset((void *)desc, 0, sizeof(struct hclge_desc));\n\tdesc->opcode = cpu_to_le16(opcode);\n\tdesc->flag = cpu_to_le16(HCLGE_COMM_CMD_FLAG_NO_INTR |\n\t\t\t\t HCLGE_COMM_CMD_FLAG_IN);\n\n\tif (is_read)\n\t\tdesc->flag |= cpu_to_le16(HCLGE_COMM_CMD_FLAG_WR);\n}\n\nint hclge_comm_firmware_compat_config(struct hnae3_ae_dev *ae_dev,\n\t\t\t\t      struct hclge_comm_hw *hw, bool en)\n{\n\tstruct hclge_comm_firmware_compat_cmd *req;\n\tstruct hclge_desc desc;\n\tu32 compat = 0;\n\n\thclge_comm_cmd_setup_basic_desc(&desc, HCLGE_OPC_IMP_COMPAT_CFG, false);\n\n\tif (en) {\n\t\treq = (struct hclge_comm_firmware_compat_cmd *)desc.data;\n\n\t\thnae3_set_bit(compat, HCLGE_COMM_LINK_EVENT_REPORT_EN_B, 1);\n\t\thnae3_set_bit(compat, HCLGE_COMM_NCSI_ERROR_REPORT_EN_B, 1);\n\t\tif (hclge_comm_dev_phy_imp_supported(ae_dev))\n\t\t\thnae3_set_bit(compat, HCLGE_COMM_PHY_IMP_EN_B, 1);\n\t\thnae3_set_bit(compat, HCLGE_COMM_MAC_STATS_EXT_EN_B, 1);\n\t\thnae3_set_bit(compat, HCLGE_COMM_SYNC_RX_RING_HEAD_EN_B, 1);\n\t\thnae3_set_bit(compat, HCLGE_COMM_LLRS_FEC_EN_B, 1);\n\n\t\treq->compat = cpu_to_le32(compat);\n\t}\n\n\treturn hclge_comm_cmd_send(hw, &desc, 1);\n}\n\nvoid hclge_comm_free_cmd_desc(struct hclge_comm_cmq_ring *ring)\n{\n\tint size  = ring->desc_num * sizeof(struct hclge_desc);\n\n\tif (!ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(&ring->pdev->dev, size,\n\t\t\t  ring->desc, ring->desc_dma_addr);\n\tring->desc = NULL;\n}\n\nstatic int hclge_comm_alloc_cmd_desc(struct hclge_comm_cmq_ring *ring)\n{\n\tint size  = ring->desc_num * sizeof(struct hclge_desc);\n\n\tring->desc = dma_alloc_coherent(&ring->pdev->dev,\n\t\t\t\t\tsize, &ring->desc_dma_addr, GFP_KERNEL);\n\tif (!ring->desc)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic __le32 hclge_comm_build_api_caps(void)\n{\n\tu32 api_caps = 0;\n\n\thnae3_set_bit(api_caps, HCLGE_COMM_API_CAP_FLEX_RSS_TBL_B, 1);\n\n\treturn cpu_to_le32(api_caps);\n}\n\nstatic const struct hclge_comm_caps_bit_map hclge_pf_cmd_caps[] = {\n\t{HCLGE_COMM_CAP_UDP_GSO_B, HNAE3_DEV_SUPPORT_UDP_GSO_B},\n\t{HCLGE_COMM_CAP_PTP_B, HNAE3_DEV_SUPPORT_PTP_B},\n\t{HCLGE_COMM_CAP_INT_QL_B, HNAE3_DEV_SUPPORT_INT_QL_B},\n\t{HCLGE_COMM_CAP_TQP_TXRX_INDEP_B, HNAE3_DEV_SUPPORT_TQP_TXRX_INDEP_B},\n\t{HCLGE_COMM_CAP_HW_TX_CSUM_B, HNAE3_DEV_SUPPORT_HW_TX_CSUM_B},\n\t{HCLGE_COMM_CAP_UDP_TUNNEL_CSUM_B, HNAE3_DEV_SUPPORT_UDP_TUNNEL_CSUM_B},\n\t{HCLGE_COMM_CAP_FD_FORWARD_TC_B, HNAE3_DEV_SUPPORT_FD_FORWARD_TC_B},\n\t{HCLGE_COMM_CAP_FEC_B, HNAE3_DEV_SUPPORT_FEC_B},\n\t{HCLGE_COMM_CAP_PAUSE_B, HNAE3_DEV_SUPPORT_PAUSE_B},\n\t{HCLGE_COMM_CAP_PHY_IMP_B, HNAE3_DEV_SUPPORT_PHY_IMP_B},\n\t{HCLGE_COMM_CAP_QB_B, HNAE3_DEV_SUPPORT_QB_B},\n\t{HCLGE_COMM_CAP_TX_PUSH_B, HNAE3_DEV_SUPPORT_TX_PUSH_B},\n\t{HCLGE_COMM_CAP_RAS_IMP_B, HNAE3_DEV_SUPPORT_RAS_IMP_B},\n\t{HCLGE_COMM_CAP_RXD_ADV_LAYOUT_B, HNAE3_DEV_SUPPORT_RXD_ADV_LAYOUT_B},\n\t{HCLGE_COMM_CAP_PORT_VLAN_BYPASS_B,\n\t HNAE3_DEV_SUPPORT_PORT_VLAN_BYPASS_B},\n\t{HCLGE_COMM_CAP_PORT_VLAN_BYPASS_B, HNAE3_DEV_SUPPORT_VLAN_FLTR_MDF_B},\n\t{HCLGE_COMM_CAP_CQ_B, HNAE3_DEV_SUPPORT_CQ_B},\n\t{HCLGE_COMM_CAP_GRO_B, HNAE3_DEV_SUPPORT_GRO_B},\n\t{HCLGE_COMM_CAP_FD_B, HNAE3_DEV_SUPPORT_FD_B},\n\t{HCLGE_COMM_CAP_FEC_STATS_B, HNAE3_DEV_SUPPORT_FEC_STATS_B},\n\t{HCLGE_COMM_CAP_LANE_NUM_B, HNAE3_DEV_SUPPORT_LANE_NUM_B},\n\t{HCLGE_COMM_CAP_WOL_B, HNAE3_DEV_SUPPORT_WOL_B},\n\t{HCLGE_COMM_CAP_TM_FLUSH_B, HNAE3_DEV_SUPPORT_TM_FLUSH_B},\n};\n\nstatic const struct hclge_comm_caps_bit_map hclge_vf_cmd_caps[] = {\n\t{HCLGE_COMM_CAP_UDP_GSO_B, HNAE3_DEV_SUPPORT_UDP_GSO_B},\n\t{HCLGE_COMM_CAP_INT_QL_B, HNAE3_DEV_SUPPORT_INT_QL_B},\n\t{HCLGE_COMM_CAP_TQP_TXRX_INDEP_B, HNAE3_DEV_SUPPORT_TQP_TXRX_INDEP_B},\n\t{HCLGE_COMM_CAP_HW_TX_CSUM_B, HNAE3_DEV_SUPPORT_HW_TX_CSUM_B},\n\t{HCLGE_COMM_CAP_UDP_TUNNEL_CSUM_B, HNAE3_DEV_SUPPORT_UDP_TUNNEL_CSUM_B},\n\t{HCLGE_COMM_CAP_QB_B, HNAE3_DEV_SUPPORT_QB_B},\n\t{HCLGE_COMM_CAP_TX_PUSH_B, HNAE3_DEV_SUPPORT_TX_PUSH_B},\n\t{HCLGE_COMM_CAP_RXD_ADV_LAYOUT_B, HNAE3_DEV_SUPPORT_RXD_ADV_LAYOUT_B},\n\t{HCLGE_COMM_CAP_CQ_B, HNAE3_DEV_SUPPORT_CQ_B},\n\t{HCLGE_COMM_CAP_GRO_B, HNAE3_DEV_SUPPORT_GRO_B},\n};\n\nstatic void\nhclge_comm_capability_to_bitmap(unsigned long *bitmap, __le32 *caps)\n{\n\tconst unsigned int words = HCLGE_COMM_QUERY_CAP_LENGTH;\n\tu32 val[HCLGE_COMM_QUERY_CAP_LENGTH];\n\tunsigned int i;\n\n\tfor (i = 0; i < words; i++)\n\t\tval[i] = __le32_to_cpu(caps[i]);\n\n\tbitmap_from_arr32(bitmap, val,\n\t\t\t  HCLGE_COMM_QUERY_CAP_LENGTH * BITS_PER_TYPE(u32));\n}\n\nstatic void\nhclge_comm_parse_capability(struct hnae3_ae_dev *ae_dev, bool is_pf,\n\t\t\t    struct hclge_comm_query_version_cmd *cmd)\n{\n\tconst struct hclge_comm_caps_bit_map *caps_map =\n\t\t\t\tis_pf ? hclge_pf_cmd_caps : hclge_vf_cmd_caps;\n\tu32 size = is_pf ? ARRAY_SIZE(hclge_pf_cmd_caps) :\n\t\t\t\tARRAY_SIZE(hclge_vf_cmd_caps);\n\tDECLARE_BITMAP(caps, HCLGE_COMM_QUERY_CAP_LENGTH * BITS_PER_TYPE(u32));\n\tu32 i;\n\n\thclge_comm_capability_to_bitmap(caps, cmd->caps);\n\tfor (i = 0; i < size; i++)\n\t\tif (test_bit(caps_map[i].imp_bit, caps))\n\t\t\tset_bit(caps_map[i].local_bit, ae_dev->caps);\n}\n\nint hclge_comm_alloc_cmd_queue(struct hclge_comm_hw *hw, int ring_type)\n{\n\tstruct hclge_comm_cmq_ring *ring =\n\t\t(ring_type == HCLGE_COMM_TYPE_CSQ) ? &hw->cmq.csq :\n\t\t\t\t\t\t     &hw->cmq.crq;\n\tint ret;\n\n\tring->ring_type = ring_type;\n\n\tret = hclge_comm_alloc_cmd_desc(ring);\n\tif (ret)\n\t\tdev_err(&ring->pdev->dev, \"descriptor %s alloc error %d\\n\",\n\t\t\t(ring_type == HCLGE_COMM_TYPE_CSQ) ? \"CSQ\" : \"CRQ\",\n\t\t\tret);\n\n\treturn ret;\n}\n\nint hclge_comm_cmd_query_version_and_capability(struct hnae3_ae_dev *ae_dev,\n\t\t\t\t\t\tstruct hclge_comm_hw *hw,\n\t\t\t\t\t\tu32 *fw_version, bool is_pf)\n{\n\tstruct hclge_comm_query_version_cmd *resp;\n\tstruct hclge_desc desc;\n\tint ret;\n\n\thclge_comm_cmd_setup_basic_desc(&desc, HCLGE_OPC_QUERY_FW_VER, 1);\n\tresp = (struct hclge_comm_query_version_cmd *)desc.data;\n\tresp->api_caps = hclge_comm_build_api_caps();\n\n\tret = hclge_comm_cmd_send(hw, &desc, 1);\n\tif (ret)\n\t\treturn ret;\n\n\t*fw_version = le32_to_cpu(resp->firmware);\n\n\tae_dev->dev_version = le32_to_cpu(resp->hardware) <<\n\t\t\t\t\t HNAE3_PCI_REVISION_BIT_SIZE;\n\tae_dev->dev_version |= ae_dev->pdev->revision;\n\n\tif (ae_dev->dev_version == HNAE3_DEVICE_VERSION_V2) {\n\t\thclge_comm_set_default_capability(ae_dev, is_pf);\n\t\treturn 0;\n\t}\n\n\thclge_comm_parse_capability(ae_dev, is_pf, resp);\n\n\treturn ret;\n}\n\nstatic const u16 spec_opcode[] = { HCLGE_OPC_STATS_64_BIT,\n\t\t\t\t   HCLGE_OPC_STATS_32_BIT,\n\t\t\t\t   HCLGE_OPC_STATS_MAC,\n\t\t\t\t   HCLGE_OPC_STATS_MAC_ALL,\n\t\t\t\t   HCLGE_OPC_QUERY_32_BIT_REG,\n\t\t\t\t   HCLGE_OPC_QUERY_64_BIT_REG,\n\t\t\t\t   HCLGE_QUERY_CLEAR_MPF_RAS_INT,\n\t\t\t\t   HCLGE_QUERY_CLEAR_PF_RAS_INT,\n\t\t\t\t   HCLGE_QUERY_CLEAR_ALL_MPF_MSIX_INT,\n\t\t\t\t   HCLGE_QUERY_CLEAR_ALL_PF_MSIX_INT,\n\t\t\t\t   HCLGE_QUERY_ALL_ERR_INFO };\n\nstatic bool hclge_comm_is_special_opcode(u16 opcode)\n{\n\t \n\tu32 i;\n\n\tfor (i = 0; i < ARRAY_SIZE(spec_opcode); i++)\n\t\tif (spec_opcode[i] == opcode)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic int hclge_comm_ring_space(struct hclge_comm_cmq_ring *ring)\n{\n\tint ntc = ring->next_to_clean;\n\tint ntu = ring->next_to_use;\n\tint used = (ntu - ntc + ring->desc_num) % ring->desc_num;\n\n\treturn ring->desc_num - used - 1;\n}\n\nstatic void hclge_comm_cmd_copy_desc(struct hclge_comm_hw *hw,\n\t\t\t\t     struct hclge_desc *desc, int num)\n{\n\tstruct hclge_desc *desc_to_use;\n\tint handle = 0;\n\n\twhile (handle < num) {\n\t\tdesc_to_use = &hw->cmq.csq.desc[hw->cmq.csq.next_to_use];\n\t\t*desc_to_use = desc[handle];\n\t\t(hw->cmq.csq.next_to_use)++;\n\t\tif (hw->cmq.csq.next_to_use >= hw->cmq.csq.desc_num)\n\t\t\thw->cmq.csq.next_to_use = 0;\n\t\thandle++;\n\t}\n}\n\nstatic int hclge_comm_is_valid_csq_clean_head(struct hclge_comm_cmq_ring *ring,\n\t\t\t\t\t      int head)\n{\n\tint ntc = ring->next_to_clean;\n\tint ntu = ring->next_to_use;\n\n\tif (ntu > ntc)\n\t\treturn head >= ntc && head <= ntu;\n\n\treturn head >= ntc || head <= ntu;\n}\n\nstatic int hclge_comm_cmd_csq_clean(struct hclge_comm_hw *hw)\n{\n\tstruct hclge_comm_cmq_ring *csq = &hw->cmq.csq;\n\tint clean;\n\tu32 head;\n\n\thead = hclge_comm_read_dev(hw, HCLGE_COMM_NIC_CSQ_HEAD_REG);\n\trmb();  \n\n\tif (!hclge_comm_is_valid_csq_clean_head(csq, head)) {\n\t\tdev_warn(&hw->cmq.csq.pdev->dev, \"wrong cmd head (%u, %d-%d)\\n\",\n\t\t\t head, csq->next_to_use, csq->next_to_clean);\n\t\tdev_warn(&hw->cmq.csq.pdev->dev,\n\t\t\t \"Disabling any further commands to IMP firmware\\n\");\n\t\tset_bit(HCLGE_COMM_STATE_CMD_DISABLE, &hw->comm_state);\n\t\tdev_warn(&hw->cmq.csq.pdev->dev,\n\t\t\t \"IMP firmware watchdog reset soon expected!\\n\");\n\t\treturn -EIO;\n\t}\n\n\tclean = (head - csq->next_to_clean + csq->desc_num) % csq->desc_num;\n\tcsq->next_to_clean = head;\n\treturn clean;\n}\n\nstatic int hclge_comm_cmd_csq_done(struct hclge_comm_hw *hw)\n{\n\tu32 head = hclge_comm_read_dev(hw, HCLGE_COMM_NIC_CSQ_HEAD_REG);\n\treturn head == hw->cmq.csq.next_to_use;\n}\n\nstatic u32 hclge_get_cmdq_tx_timeout(u16 opcode, u32 tx_timeout)\n{\n\tstatic const struct hclge_cmdq_tx_timeout_map cmdq_tx_timeout_map[] = {\n\t\t{HCLGE_OPC_CFG_RST_TRIGGER, HCLGE_COMM_CMDQ_TX_TIMEOUT_500MS},\n\t};\n\tu32 i;\n\n\tfor (i = 0; i < ARRAY_SIZE(cmdq_tx_timeout_map); i++)\n\t\tif (cmdq_tx_timeout_map[i].opcode == opcode)\n\t\t\treturn cmdq_tx_timeout_map[i].tx_timeout;\n\n\treturn tx_timeout;\n}\n\nstatic void hclge_comm_wait_for_resp(struct hclge_comm_hw *hw, u16 opcode,\n\t\t\t\t     bool *is_completed)\n{\n\tu32 cmdq_tx_timeout = hclge_get_cmdq_tx_timeout(opcode,\n\t\t\t\t\t\t\thw->cmq.tx_timeout);\n\tu32 timeout = 0;\n\n\tdo {\n\t\tif (hclge_comm_cmd_csq_done(hw)) {\n\t\t\t*is_completed = true;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t\ttimeout++;\n\t} while (timeout < cmdq_tx_timeout);\n}\n\nstatic int hclge_comm_cmd_convert_err_code(u16 desc_ret)\n{\n\tstruct hclge_comm_errcode hclge_comm_cmd_errcode[] = {\n\t\t{ HCLGE_COMM_CMD_EXEC_SUCCESS, 0 },\n\t\t{ HCLGE_COMM_CMD_NO_AUTH, -EPERM },\n\t\t{ HCLGE_COMM_CMD_NOT_SUPPORTED, -EOPNOTSUPP },\n\t\t{ HCLGE_COMM_CMD_QUEUE_FULL, -EXFULL },\n\t\t{ HCLGE_COMM_CMD_NEXT_ERR, -ENOSR },\n\t\t{ HCLGE_COMM_CMD_UNEXE_ERR, -ENOTBLK },\n\t\t{ HCLGE_COMM_CMD_PARA_ERR, -EINVAL },\n\t\t{ HCLGE_COMM_CMD_RESULT_ERR, -ERANGE },\n\t\t{ HCLGE_COMM_CMD_TIMEOUT, -ETIME },\n\t\t{ HCLGE_COMM_CMD_HILINK_ERR, -ENOLINK },\n\t\t{ HCLGE_COMM_CMD_QUEUE_ILLEGAL, -ENXIO },\n\t\t{ HCLGE_COMM_CMD_INVALID, -EBADR },\n\t};\n\tu32 errcode_count = ARRAY_SIZE(hclge_comm_cmd_errcode);\n\tu32 i;\n\n\tfor (i = 0; i < errcode_count; i++)\n\t\tif (hclge_comm_cmd_errcode[i].imp_errcode == desc_ret)\n\t\t\treturn hclge_comm_cmd_errcode[i].common_errno;\n\n\treturn -EIO;\n}\n\nstatic int hclge_comm_cmd_check_retval(struct hclge_comm_hw *hw,\n\t\t\t\t       struct hclge_desc *desc, int num,\n\t\t\t\t       int ntc)\n{\n\tu16 opcode, desc_ret;\n\tint handle;\n\n\topcode = le16_to_cpu(desc[0].opcode);\n\tfor (handle = 0; handle < num; handle++) {\n\t\tdesc[handle] = hw->cmq.csq.desc[ntc];\n\t\tntc++;\n\t\tif (ntc >= hw->cmq.csq.desc_num)\n\t\t\tntc = 0;\n\t}\n\tif (likely(!hclge_comm_is_special_opcode(opcode)))\n\t\tdesc_ret = le16_to_cpu(desc[num - 1].retval);\n\telse\n\t\tdesc_ret = le16_to_cpu(desc[0].retval);\n\n\thw->cmq.last_status = desc_ret;\n\n\treturn hclge_comm_cmd_convert_err_code(desc_ret);\n}\n\nstatic int hclge_comm_cmd_check_result(struct hclge_comm_hw *hw,\n\t\t\t\t       struct hclge_desc *desc,\n\t\t\t\t       int num, int ntc)\n{\n\tbool is_completed = false;\n\tint handle, ret;\n\n\t \n\tif (HCLGE_COMM_SEND_SYNC(le16_to_cpu(desc->flag)))\n\t\thclge_comm_wait_for_resp(hw, le16_to_cpu(desc->opcode),\n\t\t\t\t\t &is_completed);\n\n\tif (!is_completed)\n\t\tret = -EBADE;\n\telse\n\t\tret = hclge_comm_cmd_check_retval(hw, desc, num, ntc);\n\n\t \n\thandle = hclge_comm_cmd_csq_clean(hw);\n\tif (handle < 0)\n\t\tret = handle;\n\telse if (handle != num)\n\t\tdev_warn(&hw->cmq.csq.pdev->dev,\n\t\t\t \"cleaned %d, need to clean %d\\n\", handle, num);\n\treturn ret;\n}\n\n \nint hclge_comm_cmd_send(struct hclge_comm_hw *hw, struct hclge_desc *desc,\n\t\t\tint num)\n{\n\tstruct hclge_comm_cmq_ring *csq = &hw->cmq.csq;\n\tint ret;\n\tint ntc;\n\n\tspin_lock_bh(&hw->cmq.csq.lock);\n\n\tif (test_bit(HCLGE_COMM_STATE_CMD_DISABLE, &hw->comm_state)) {\n\t\tspin_unlock_bh(&hw->cmq.csq.lock);\n\t\treturn -EBUSY;\n\t}\n\n\tif (num > hclge_comm_ring_space(&hw->cmq.csq)) {\n\t\t \n\t\tcsq->next_to_clean =\n\t\t\thclge_comm_read_dev(hw, HCLGE_COMM_NIC_CSQ_HEAD_REG);\n\t\tspin_unlock_bh(&hw->cmq.csq.lock);\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tntc = hw->cmq.csq.next_to_use;\n\n\thclge_comm_cmd_copy_desc(hw, desc, num);\n\n\t \n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_TAIL_REG,\n\t\t\t     hw->cmq.csq.next_to_use);\n\n\tret = hclge_comm_cmd_check_result(hw, desc, num, ntc);\n\n\tspin_unlock_bh(&hw->cmq.csq.lock);\n\n\treturn ret;\n}\n\nstatic void hclge_comm_cmd_uninit_regs(struct hclge_comm_hw *hw)\n{\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_BASEADDR_L_REG, 0);\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_BASEADDR_H_REG, 0);\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_DEPTH_REG, 0);\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_HEAD_REG, 0);\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CSQ_TAIL_REG, 0);\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_BASEADDR_L_REG, 0);\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_BASEADDR_H_REG, 0);\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_DEPTH_REG, 0);\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_HEAD_REG, 0);\n\thclge_comm_write_dev(hw, HCLGE_COMM_NIC_CRQ_TAIL_REG, 0);\n}\n\nvoid hclge_comm_cmd_uninit(struct hnae3_ae_dev *ae_dev,\n\t\t\t   struct hclge_comm_hw *hw)\n{\n\tstruct hclge_comm_cmq *cmdq = &hw->cmq;\n\n\thclge_comm_firmware_compat_config(ae_dev, hw, false);\n\tset_bit(HCLGE_COMM_STATE_CMD_DISABLE, &hw->comm_state);\n\n\t \n\tmsleep(HCLGE_COMM_CMDQ_CLEAR_WAIT_TIME);\n\tspin_lock_bh(&cmdq->csq.lock);\n\tspin_lock(&cmdq->crq.lock);\n\thclge_comm_cmd_uninit_regs(hw);\n\tspin_unlock(&cmdq->crq.lock);\n\tspin_unlock_bh(&cmdq->csq.lock);\n\n\thclge_comm_free_cmd_desc(&cmdq->csq);\n\thclge_comm_free_cmd_desc(&cmdq->crq);\n}\n\nint hclge_comm_cmd_queue_init(struct pci_dev *pdev, struct hclge_comm_hw *hw)\n{\n\tstruct hclge_comm_cmq *cmdq = &hw->cmq;\n\tint ret;\n\n\t \n\tspin_lock_init(&cmdq->csq.lock);\n\tspin_lock_init(&cmdq->crq.lock);\n\n\tcmdq->csq.pdev = pdev;\n\tcmdq->crq.pdev = pdev;\n\n\t \n\tcmdq->csq.desc_num = HCLGE_COMM_NIC_CMQ_DESC_NUM;\n\tcmdq->crq.desc_num = HCLGE_COMM_NIC_CMQ_DESC_NUM;\n\n\t \n\tcmdq->tx_timeout = HCLGE_COMM_CMDQ_TX_TIMEOUT_DEFAULT;\n\n\t \n\tret = hclge_comm_alloc_cmd_queue(hw, HCLGE_COMM_TYPE_CSQ);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"CSQ ring setup error %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hclge_comm_alloc_cmd_queue(hw, HCLGE_COMM_TYPE_CRQ);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"CRQ ring setup error %d\\n\", ret);\n\t\tgoto err_csq;\n\t}\n\n\treturn 0;\nerr_csq:\n\thclge_comm_free_cmd_desc(&hw->cmq.csq);\n\treturn ret;\n}\n\nint hclge_comm_cmd_init(struct hnae3_ae_dev *ae_dev, struct hclge_comm_hw *hw,\n\t\t\tu32 *fw_version, bool is_pf,\n\t\t\tunsigned long reset_pending)\n{\n\tstruct hclge_comm_cmq *cmdq = &hw->cmq;\n\tint ret;\n\n\tspin_lock_bh(&cmdq->csq.lock);\n\tspin_lock(&cmdq->crq.lock);\n\n\tcmdq->csq.next_to_clean = 0;\n\tcmdq->csq.next_to_use = 0;\n\tcmdq->crq.next_to_clean = 0;\n\tcmdq->crq.next_to_use = 0;\n\n\thclge_comm_cmd_init_regs(hw);\n\n\tspin_unlock(&cmdq->crq.lock);\n\tspin_unlock_bh(&cmdq->csq.lock);\n\n\tclear_bit(HCLGE_COMM_STATE_CMD_DISABLE, &hw->comm_state);\n\n\t \n\tif (reset_pending) {\n\t\tret = -EBUSY;\n\t\tgoto err_cmd_init;\n\t}\n\n\t \n\tret = hclge_comm_cmd_query_version_and_capability(ae_dev, hw,\n\t\t\t\t\t\t\t  fw_version, is_pf);\n\tif (ret) {\n\t\tdev_err(&ae_dev->pdev->dev,\n\t\t\t\"failed to query version and capabilities, ret = %d\\n\",\n\t\t\tret);\n\t\tgoto err_cmd_init;\n\t}\n\n\tdev_info(&ae_dev->pdev->dev,\n\t\t \"The firmware version is %lu.%lu.%lu.%lu\\n\",\n\t\t hnae3_get_field(*fw_version, HNAE3_FW_VERSION_BYTE3_MASK,\n\t\t\t\t HNAE3_FW_VERSION_BYTE3_SHIFT),\n\t\t hnae3_get_field(*fw_version, HNAE3_FW_VERSION_BYTE2_MASK,\n\t\t\t\t HNAE3_FW_VERSION_BYTE2_SHIFT),\n\t\t hnae3_get_field(*fw_version, HNAE3_FW_VERSION_BYTE1_MASK,\n\t\t\t\t HNAE3_FW_VERSION_BYTE1_SHIFT),\n\t\t hnae3_get_field(*fw_version, HNAE3_FW_VERSION_BYTE0_MASK,\n\t\t\t\t HNAE3_FW_VERSION_BYTE0_SHIFT));\n\n\tif (!is_pf && ae_dev->dev_version < HNAE3_DEVICE_VERSION_V3)\n\t\treturn 0;\n\n\t \n\tret = hclge_comm_firmware_compat_config(ae_dev, hw, true);\n\tif (ret)\n\t\tdev_warn(&ae_dev->pdev->dev,\n\t\t\t \"Firmware compatible features not enabled(%d).\\n\",\n\t\t\t ret);\n\treturn 0;\n\nerr_cmd_init:\n\tset_bit(HCLGE_COMM_STATE_CMD_DISABLE, &hw->comm_state);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}